Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009652509652509653,"Recent studies highlighted a practical setting of unsupervised anomaly detection
1"
ABSTRACT,0.0019305019305019305,"(UAD) that builds a unified model for multi-class images, serving as an alternative
2"
ABSTRACT,0.0028957528957528956,"to the conventional one-class-one-model setup. Despite various advancements
3"
ABSTRACT,0.003861003861003861,"addressing this challenging task, the detection performance under the multi-class
4"
ABSTRACT,0.004826254826254826,"setting still lags far behind state-of-the-art class-separated models. Our research
5"
ABSTRACT,0.005791505791505791,"aims to bridge this substantial performance gap. In this paper, we introduce a
6"
ABSTRACT,0.006756756756756757,"minimalistic reconstruction-based anomaly detection framework, namely Dino-
7"
ABSTRACT,0.007722007722007722,"maly, which leverages pure Transformer architectures without relying on complex
8"
ABSTRACT,0.008687258687258687,"designs, additional modules, or specialized tricks. Given this powerful frame-
9"
ABSTRACT,0.009652509652509652,"work consisted of only Attentions and MLPs, we found four simple components
10"
ABSTRACT,0.010617760617760617,"that are essential to multi-class anomaly detection: (1) Foundation Transformers
11"
ABSTRACT,0.011583011583011582,"that extracts universal and discriminative features, (2) Noisy Bottleneck where
12"
ABSTRACT,0.012548262548262547,"pre-existing Dropouts do all the noise injection tricks, (3) Linear Attention that nat-
13"
ABSTRACT,0.013513513513513514,"urally cannot focus, and (4) Loose Reconstruction that does not force layer-to-layer
14"
ABSTRACT,0.01447876447876448,"and point-by-point reconstruction. Extensive experiments are conducted across
15"
ABSTRACT,0.015444015444015444,"three popular anomaly detection benchmarks including MVTec-AD, VisA, and the
16"
ABSTRACT,0.016409266409266408,"recently released Real-IAD. Our proposed Dinomaly achieves impressive image
17"
ABSTRACT,0.017374517374517374,"AUROC of 99.6%, 98.7%, and 89.3% on the three datasets respectively, which is
18"
ABSTRACT,0.01833976833976834,"not only superior to state-of-the-art multi-class UAD methods, but also surpasses
19"
ABSTRACT,0.019305019305019305,"the most advanced class-separated UAD records.
20"
INTRODUCTION,0.02027027027027027,"1
Introduction
21"
INTRODUCTION,0.021235521235521235,"Unsupervised anomaly detection (UAD) aims to detect abnormal patterns from normal images and
22"
INTRODUCTION,0.0222007722007722,"further localize the anomalous regions. Because of the diversity of potential anomalies and their
23"
INTRODUCTION,0.023166023166023165,"scarcity, this task is proposed to model the accessible training sets containing only normal samples as
24"
INTRODUCTION,0.02413127413127413,"an unsupervised paradigm. UAD has a wide range of applications, e.g., industrial defect detection,
25"
INTRODUCTION,0.025096525096525095,"medical disease screening, and video surveillance, addressing the difficulty of collecting and labeling
26"
INTRODUCTION,0.026061776061776062,"all possible anomalies in these scenarios.
27"
INTRODUCTION,0.02702702702702703,"Efforts on UAD attempt to learn the distribution of available normal samples. Most advanced methods
28"
INTRODUCTION,0.027992277992277992,"utilize networks pre-trained on large-scale datasets, e.g. ImageNet [1], for extracting discriminative
29"
INTRODUCTION,0.02895752895752896,"and informative feature representations. Specifically, Feature reconstruction [2; 3; 4] and feature
30"
INTRODUCTION,0.029922779922779922,"distillation methods [5; 6] are proposed to reconstruct features of pre-trained encoders, based on
31"
INTRODUCTION,0.03088803088803089,"the hypothesis that the networks trained on normal images can only construct normal regions, but
32"
INTRODUCTION,0.03185328185328185,"fail for unseen anomalous regions. Feature statistics methods [7; 8; 9] memorize and model all
33"
INTRODUCTION,0.032818532818532815,"anomaly-free features extracted from pre-trained networks in training, and compare them with the
34"
INTRODUCTION,0.033783783783783786,"test features during inference. Pseudo-anomaly methods [10; 11] generate pseudo defects or noises
35"
INTRODUCTION,0.03474903474903475,"Figure 1: Setting and Performance of UAD and multi-class UAD (MUAD). (a) Task setting of
class-separated UAD. (b) Task setting of MUAD. (c) Comparison of Dinomaly and previous SoTA
methods [13; 14; 15; 16; 8; 17; 18; 19] on MVTec-AD [20], VisA [21], and Real-IAD [22]."
INTRODUCTION,0.03571428571428571,"on normal images or features to imitate anomalies, converting UAD to supervised classification [11]
36"
INTRODUCTION,0.03667953667953668,"or segmentation tasks [10; 12].
37"
INTRODUCTION,0.037644787644787646,"Conventional works on UAD build a separate model for each object category, as shown in Figure 1(a).
38"
INTRODUCTION,0.03861003861003861,"However, this one-class-one-model setting entails substantial storage overhead for saving models
39"
INTRODUCTION,0.03957528957528957,"[3], especially when the application scenario necessitates a large number of object classes. For
40"
INTRODUCTION,0.04054054054054054,"UAD methods, a compact boundary of normal patterns is vital to distinguish anomalies. Once the
41"
INTRODUCTION,0.041505791505791506,"intra-normal patterns become exceedingly complicated due to various classes, the corresponding
42"
INTRODUCTION,0.04247104247104247,"distribution becomes challenging to measure, consequently harming the detection performance.
43"
INTRODUCTION,0.04343629343629344,"Recently, UniAD [3] and successive studies have been proposed to train a unified model for multi-
44"
INTRODUCTION,0.0444015444015444,"class anomaly detection (MUAD), as shown in Figure 1(b). Under this setting, the ""identity mapping""
45"
INTRODUCTION,0.045366795366795366,"that directly copies the input as the output regardless of normal or anomaly harms the performance of
46"
INTRODUCTION,0.04633204633204633,"conventional methods [3]. This phenomenon is caused by the diversity of multi-class normal patterns
47"
INTRODUCTION,0.0472972972972973,"that drive the network to generalize on unseen patterns.
48"
INTRODUCTION,0.04826254826254826,"Within two years, a number of methods have been proposed to address MUAD, such as neighbor-
49"
INTRODUCTION,0.04922779922779923,"masked attention [3], synthetic anomalies [23], feature jitter [3], vector quantization [24], diffusion
50"
INTRODUCTION,0.05019305019305019,"model [25; 26], and state space model (Mamba) [19]. However, there is still a non-negligible
51"
INTRODUCTION,0.05115830115830116,"performance gap between the state-of-the-art (SoTA) MUAD methods and class-separated UAD
52"
INTRODUCTION,0.052123552123552123,"methods, restricting the practicability of implementing unified models, as shown in Figure 1(c). In
53"
INTRODUCTION,0.05308880308880309,"addition, previous methods employ modules and architectures delicately designed, which may not be
54"
INTRODUCTION,0.05405405405405406,"straightforward, and consequently suffer from limited universality and usability.
55"
INTRODUCTION,0.05501930501930502,"In this work, we aim to catch up with the performance of class-separated anomaly detection models
56"
INTRODUCTION,0.055984555984555984,"using a multi-class unified model, namely Dinomaly. To begin with, we build a reconstruction-based
57"
INTRODUCTION,0.05694980694980695,"UAD framework that consists of only vanilla Transformer blocks [27], i.e. Self-Attentions and
58"
INTRODUCTION,0.05791505791505792,"Multi-Layer Perceptrons (MLPs). Within this framework, we propose four simple but essential
59"
INTRODUCTION,0.05888030888030888,"elements that boost Dinomaly to perform equal to or better than SoTA conventional class-separated
60"
INTRODUCTION,0.059845559845559844,"models. First, we show that self-supervised pre-trained Vision Transformers (ViT) [28], especially the
61"
INTRODUCTION,0.060810810810810814,"DINO family [29; 30], serve as powerful feature encoders to extract discriminative representations as
62"
INTRODUCTION,0.06177606177606178,"reconstruction objects. Second, as an alternative to carefully designed pseudo anomaly and feature
63"
INTRODUCTION,0.06274131274131274,"noise, we propose to activate the out-of-the-box Dropout in an MLP to prevent the network from
64"
INTRODUCTION,0.0637065637065637,"restoring both normal and anomalous patterns, which is previously referred to as identity mapping.
65"
INTRODUCTION,0.06467181467181467,"Third, we propose to utilize the ""side effect"" of Linear Attention (a computation-efficient counterpart
66"
INTRODUCTION,0.06563706563706563,"of Softmax Attention) that makes it hard to focus on local regions, to further alleviate the issue of
67"
INTRODUCTION,0.06660231660231661,"identity mapping. Fourth, previous methods adopt layer-to-layer and region-by-region reconstruction
68"
INTRODUCTION,0.06756756756756757,"schemes, distilling a decoder that can well mimic the behavior of the encoder even for anomalous
69"
INTRODUCTION,0.06853281853281853,"regions. Therefore, we propose to loosen the reconstruction constraints by grouping multiple layers
70"
INTRODUCTION,0.0694980694980695,"as a whole and discarding well-reconstructed regions during optimization.
71"
INTRODUCTION,0.07046332046332046,"To validate the effectiveness of the proposed Dinomaly under MUAD setting, we conduct extensive
72"
INTRODUCTION,0.07142857142857142,"experiments on three widely used industrial defect detection benchmarks, i.e., MVTec AD [20]
73"
INTRODUCTION,0.07239382239382239,"(15 classes), VisA [21] (12 classes), and recently released Real-IAD (30 classes). Notably, we
74"
INTRODUCTION,0.07335907335907337,"achieve unprecedented image-level AUROC of 99.6%, 98.7%, and 89.3% on MVTec AD, VisA, and
75"
INTRODUCTION,0.07432432432432433,"Real-IAD, respectively, which surpasses previous SoTA methods by a large margin.
76"
INTRODUCTION,0.07528957528957529,"Related works are presented in Appendix A.1.
77"
METHOD,0.07625482625482626,"2
Method
78"
DINOMALY FRAMEWORK,0.07722007722007722,"2.1
Dinomaly Framework
79"
DINOMALY FRAMEWORK,0.07818532818532818,"“What I cannot create, I do not understand”——Richer Feynman
80"
DINOMALY FRAMEWORK,0.07915057915057915,"The ability to recognize anomalies from what we know is an innate human capability, serving as a
81"
DINOMALY FRAMEWORK,0.08011583011583012,"vital pathway for us to explore the world. Similarly, we construct a reconstruction-based framework
82"
DINOMALY FRAMEWORK,0.08108108108108109,"that relies on the epistemic characteristic of artificial neural networks. Dinomaly consists of an
83"
DINOMALY FRAMEWORK,0.08204633204633205,"encoder, a bottleneck, and a reconstruction decoder, as shown in Figure 2. Without loss of generality,
84"
DINOMALY FRAMEWORK,0.08301158301158301,"a standard ViT-Base/14 network [28] with 12 Transformer layers is used as the encoder, extracting
85"
DINOMALY FRAMEWORK,0.08397683397683398,"informative feature maps with different semantic scales. The bottleneck is a simple MLP (a.k.a.
86"
DINOMALY FRAMEWORK,0.08494208494208494,"feed-forward network, FFN) that collects the feature representations of the encoder’s 8 middle-level
87"
DINOMALY FRAMEWORK,0.0859073359073359,"layers. The decoder is similar to the encoder, consisting of 8 Transformer layers. During training,
88"
DINOMALY FRAMEWORK,0.08687258687258688,"the decoder learns to reconstruct the middle-level features of the encoder by maximizing the cosine
89"
DINOMALY FRAMEWORK,0.08783783783783784,"similarity between feature maps. During inference, the decoder is expected to reconstruct normal
90"
DINOMALY FRAMEWORK,0.0888030888030888,"regions of feature maps but fails for anomalous regions as it has never seen such samples.
91"
DINOMALY FRAMEWORK,0.08976833976833977,"Foundation Transformers. Foundation models, especially ViTs [28; 31] pre-trained on large-scale
92"
DINOMALY FRAMEWORK,0.09073359073359073,"datasets, serve as a basis and starting point for specific computer vision tasks. Such networks employ
93"
DINOMALY FRAMEWORK,0.0916988416988417,"self-supervised learning schemes such as contrastive learning (MoCov3 [32], DINO [29]), masked
94"
DINOMALY FRAMEWORK,0.09266409266409266,"image modeling (MAE [33], SimMIM [34], BEiT [35]), and their combination (iBOT [36], DINOv2
95"
DINOMALY FRAMEWORK,0.09362934362934362,"[30]), producing universal features suitable for image-level visual tasks (image classification, instance
96"
DINOMALY FRAMEWORK,0.0945945945945946,"retrieval) and pixel-level visual tasks (depth estimation, semantic segmentation).
97"
DINOMALY FRAMEWORK,0.09555984555984556,"Because of the lack of supervision in UAD, most advanced methods adopt pre-trained networks to
98"
DINOMALY FRAMEWORK,0.09652509652509653,"extract discriminative features. Recent works [37; 17; 38] have discovered the advantage of robust
99"
DINOMALY FRAMEWORK,0.09749034749034749,"and universal features of self-supervised models over domain-specific ImageNet features in anomaly
100"
DINOMALY FRAMEWORK,0.09845559845559845,"detection tasks. In this work, we further utilize the up-to-date Transformer foundation, i.e., DINOv2
101"
DINOMALY FRAMEWORK,0.09942084942084942,"with registers [39], as the encoder of Dinomaly.
102"
DINOMALY FRAMEWORK,0.10038610038610038,"2.2
Noisy Bottleneck.
103"
DINOMALY FRAMEWORK,0.10135135135135136,"“Dropout is all you need.”
104"
DINOMALY FRAMEWORK,0.10231660231660232,"Generalization ability is a merit of neural networks, allowing them to perform equally well on unseen
105"
DINOMALY FRAMEWORK,0.10328185328185328,"test sets. However, generalization is not so wanted in the context of unsupervised anomaly detection
106"
DINOMALY FRAMEWORK,0.10424710424710425,"that leverages the epistemic nature of neural networks. With the increasing diversity of images and
107"
DINOMALY FRAMEWORK,0.10521235521235521,"their patterns due to multi-class UAD settings, the decoder can generalize its reconstruction ability to
108"
DINOMALY FRAMEWORK,0.10617760617760617,"unseen anomalous samples, resulting in the failure of anomaly detection using reconstruction error.
109"
DINOMALY FRAMEWORK,0.10714285714285714,"This phenomenon is called ""identity mapping"" in previous works of literature [3; 23; 18].
110"
DINOMALY FRAMEWORK,0.10810810810810811,"A direct solution for identity mapping is to shift ""reconstruction"" to ""restoration"". Specifically, instead
111"
DINOMALY FRAMEWORK,0.10907335907335908,"of directly reconstructing the normal images or features given normal inputs, previous works propose
112"
DINOMALY FRAMEWORK,0.11003861003861004,"Figure 2: The framework of Dinomaly, built by pure Transformer building blocks."
DINOMALY FRAMEWORK,0.111003861003861,"to add perturbations as pseudo anomalies on input images [10; 40; 12] or feature representations
113"
DINOMALY FRAMEWORK,0.11196911196911197,"[3; 25] during network forward propagation; meanwhile, still let the decoder restore anomaly-free
114"
DINOMALY FRAMEWORK,0.11293436293436293,"images or features, formulating a denoising-like framework. However, such methods employ heuristic
115"
DINOMALY FRAMEWORK,0.1138996138996139,"and hand-crafted anomaly generation strategies, that are not universal across domains, datasets, and
116"
DINOMALY FRAMEWORK,0.11486486486486487,"methods.
117"
DINOMALY FRAMEWORK,0.11583011583011583,"In this work, we propose to activate the pre-existing Dropout in an MLP layer. Dropout, a popular
118"
DINOMALY FRAMEWORK,0.1167953667953668,"network element introduced by Hinton et al. [41] in 2014 to prevent overfitting, flourished in nearly
119"
DINOMALY FRAMEWORK,0.11776061776061776,"all neural network architectures to the present day, including Transformers. In Dinomaly, Dropout is
120"
DINOMALY FRAMEWORK,0.11872586872586872,"used to discard neural activations in the MLP bottleneck randomly. Instead of alleviating overfitting,
121"
DINOMALY FRAMEWORK,0.11969111969111969,"the role of Dropout in Dinomaly can be explained as feature noise and pseudo feature anomaly.
122"
DINOMALY FRAMEWORK,0.12065637065637065,"Although the decoder takes noisy features during training, it is encouraged to restore clean features
123"
DINOMALY FRAMEWORK,0.12162162162162163,"from the encoder. Without introducing any novel modules, this paradigm forces the decoder to restore
124"
DINOMALY FRAMEWORK,0.12258687258687259,"normal features given a test image with anomalies, in turn, mitigating identical mapping.
125"
DINOMALY FRAMEWORK,0.12355212355212356,"2.3
Unfocused Linear Attention.
126"
DINOMALY FRAMEWORK,0.12451737451737452,"“One man’s poison is another man’s meat”
127"
DINOMALY FRAMEWORK,0.12548262548262548,"Softmax Attention is the key mechanism of Transformers, allowing the model to attend to different
128"
DINOMALY FRAMEWORK,0.12644787644787644,"parts of its input token sequence. Formally, given an input sequence X ∈RN×d with length
129"
DINOMALY FRAMEWORK,0.1274131274131274,"N, Attention first transforms it into three matrices: the query matrix Q ∈RN×d, the key matrix
130"
DINOMALY FRAMEWORK,0.12837837837837837,"K ∈RN×d, and the value matrix V ∈RN×d:
131"
DINOMALY FRAMEWORK,0.12934362934362933,"Q = XWQ , K = XWK , V = XWV ,
(1)"
DINOMALY FRAMEWORK,0.1303088803088803,"where WQ, WK, WV ∈Rd×d are learnable parameters. By computing the attention map by the
132"
DINOMALY FRAMEWORK,0.13127413127413126,"query-key similarity, the output of Attention is given as: 1
133"
DINOMALY FRAMEWORK,0.13223938223938225,"Attention(Q, K, V) = Softmax(QKT )V .
(2)"
DINOMALY FRAMEWORK,0.13320463320463322,1The full form of Attention is Softmax( QKT √
DINOMALY FRAMEWORK,0.13416988416988418,"d )V. The constant denominator is omitted for narrative simplicity.
The multi-head mechanism that concatenates multiple Attentions is also omitted."
DINOMALY FRAMEWORK,0.13513513513513514,"Figure 3: The decoder attention map (min-max to 0-1 for visualization) of Dinomaly with vanilla
Softmax Attention vs. Linear Attention."
DINOMALY FRAMEWORK,0.1361003861003861,"Because the attention map is obtained by computing the similarity between all query-key pairs
134"
DINOMALY FRAMEWORK,0.13706563706563707,"followed by row-wise Softmax, the computation complexity is O(N 2d).
135"
DINOMALY FRAMEWORK,0.13803088803088803,"Linear Attention was proposed as a promising alternative to reduce the computation complexity of
136"
DINOMALY FRAMEWORK,0.138996138996139,"vanilla Softmax Attention concerning the number of tokens [42]. By substituting Softmax operation
137"
DINOMALY FRAMEWORK,0.13996138996138996,"with a simple activation function ϕ(·) (usually ϕ(x) = elu(x) + 1), we can change the computation
138"
DINOMALY FRAMEWORK,0.14092664092664092,"order from (QKT )V to Q(KT V). Formally, Linear Attention is given as:
139"
DINOMALY FRAMEWORK,0.14189189189189189,"LinearAttention(Q, K, V) = (ϕ(Q)ϕ(KT ))V = ϕ(Q)(ϕ(KT )V) ,
(3)"
DINOMALY FRAMEWORK,0.14285714285714285,"where the computation complexity is reduced to O(Nd2). The trade-off between complexity and
140"
DINOMALY FRAMEWORK,0.1438223938223938,"expressiveness is a dilemma. Previous studies [43; 44] attribute Linear Attention’s performance
141"
DINOMALY FRAMEWORK,0.14478764478764478,"degradation on supervised tasks to its incompetence in focusing. Due to the absence of non-linear
142"
DINOMALY FRAMEWORK,0.14575289575289574,"attention reweighting by Softmax operation, Linear Attention cannot concentrate on important regions
143"
DINOMALY FRAMEWORK,0.14671814671814673,"related to the query, such as foreground and neighbors.
144"
DINOMALY FRAMEWORK,0.1476833976833977,"Back to MUAD, previous methods [3; 24] suggest adopting Attentions instead of Convolutions
145"
DINOMALY FRAMEWORK,0.14864864864864866,"because Convolutions can easily learn identical mappings. Nevertheless, both operations are in
146"
DINOMALY FRAMEWORK,0.14961389961389962,"danger of forming identity mapping by over-concentrating on corresponding input locations for
147"
DINOMALY FRAMEWORK,0.15057915057915058,"producing the outputs:
148"
DINOMALY FRAMEWORK,0.15154440154440155,Conv Kernel =
DINOMALY FRAMEWORK,0.1525096525096525,"""0
0
0
0
1
0
0
0
0 #"
DINOMALY FRAMEWORK,0.15347490347490347,",
Attention Map =  "
DINOMALY FRAMEWORK,0.15444015444015444,"1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1  ."
DINOMALY FRAMEWORK,0.1554054054054054,"In Dinomaly, we turn to leverage the ""unfocusing ability"" of Linear Attention. In order to probe
149"
DINOMALY FRAMEWORK,0.15637065637065636,"how Attentions propagate information, we train two variants of Dinomaly using vanilla Softmax
150"
DINOMALY FRAMEWORK,0.15733590733590733,"Attention or Linear Attention as the spatial mixer in the decoder and visualize their attention maps.
151"
DINOMALY FRAMEWORK,0.1583011583011583,"As shown in Figure 3, Softmax Attention tends to focus on the exact region of the query, while
152"
DINOMALY FRAMEWORK,0.15926640926640925,"Linear Attention spreads its attention across the whole image. This implies that Linear Attention,
153"
DINOMALY FRAMEWORK,0.16023166023166024,"forced by its incompetence to focus, utilizes more long-range information to restore features at each
154"
DINOMALY FRAMEWORK,0.1611969111969112,"position, reducing the chance of passing identical information of unseen patterns to the next layer
155"
DINOMALY FRAMEWORK,0.16216216216216217,"Figure 4: Schemes of reconstruction constraint. (a) Layer-to-layer (sparse). (b) Layer-to-cat-layer.
(c) Layer-to-layer (dense). (d) Loose group-to-group, 1-group (Ours). (e) Loose group-to-group,
2-group (Ours)."
DINOMALY FRAMEWORK,0.16312741312741313,"during reconstruction. Of course, employing Linear Attention also benefits from less computation,
156"
DINOMALY FRAMEWORK,0.1640926640926641,"free of performance drop.
157"
LOOSE RECONSTRUCTION,0.16505791505791506,"2.4
Loose Reconstruction
158"
LOOSE RECONSTRUCTION,0.16602316602316602,"“The tighter you squeeze, the less you have.”
159"
LOOSE RECONSTRUCTION,0.166988416988417,"Loose Constraint. Pioneers of feature-reconstruction/distillation UAD methods [5; 2] are inspired
160"
LOOSE RECONSTRUCTION,0.16795366795366795,"by knowledge distillation [45]. Most reconstruction-based methods distill specific encoder layers
161"
LOOSE RECONSTRUCTION,0.16891891891891891,"(e.g. 3 last layers of 3 ResNet stages) by the corresponding decoder layers [2; 5; 17] (Figure 4(a))
162"
LOOSE RECONSTRUCTION,0.16988416988416988,"or the last decoder layer [3; 4] (Figure 4(b)). Intuitively, with more encoder-decoder feature pairs
163"
LOOSE RECONSTRUCTION,0.17084942084942084,"(Figure 4(c)), UAD model can utilize more information in different layers to discriminate anomalies.
164"
LOOSE RECONSTRUCTION,0.1718146718146718,"However, according to the intuition of knowledge distillation, the student (decoder) can better mimic
165"
LOOSE RECONSTRUCTION,0.17277992277992277,"the behavior of the teacher (encoder) given more layer-to-layer supervision, which is harmful for UAD
166"
LOOSE RECONSTRUCTION,0.17374517374517376,"models that detect anomalies by encoder-decoder discrepancy. This phenomenon is also embodied
167"
LOOSE RECONSTRUCTION,0.17471042471042472,"as identity mapping. Thanks to the top-to-bottom consistency of columnar Transformer layers, we
168"
LOOSE RECONSTRUCTION,0.17567567567567569,"propose to loosen the layer-to-layer constraint by adding up all feature maps of interested layers as
169"
LOOSE RECONSTRUCTION,0.17664092664092665,"a whole group, as shown in Figure 4(d). This scheme can be seen as loosening the layer-to-layer
170"
LOOSE RECONSTRUCTION,0.1776061776061776,"correspondence, so that the decoder is allowed to act much more differently from the encoder when
171"
LOOSE RECONSTRUCTION,0.17857142857142858,"the input pattern is unseen. Because features of shallow layers contain low-level visual characters
172"
LOOSE RECONSTRUCTION,0.17953667953667954,"that are helpful for precise localization, we can further group the features into the low-semantic-level
173"
LOOSE RECONSTRUCTION,0.1805019305019305,"group and high-semantic-level group, as shown in Figure 4(e).
174"
LOOSE RECONSTRUCTION,0.18146718146718147,"Loose Loss. Following the above analysis, we also loosen the point-by-point reconstruction loss
175"
LOOSE RECONSTRUCTION,0.18243243243243243,"function by discarding some points in the feature map. Here, we simply borrow the hard-mining
176"
LOOSE RECONSTRUCTION,0.1833976833976834,"global cosine loss [18] that detaches the gradients of well-restored feature points with low cosine
177"
LOOSE RECONSTRUCTION,0.18436293436293436,"distance during training. Let fE and fD denotes (grouped) feature maps of encoder and decoder:
178"
LOOSE RECONSTRUCTION,0.18532818532818532,"Lglobal−hm = Dcos(F(fE), F(fD)) = 1 −
F(fE)T · F(fD)
∥F(fE)∥∥F(fD)∥,
(4) 179"
LOOSE RECONSTRUCTION,0.18629343629343628,"fD(h, w) =

sg(fD(h, w))0.1, if Dcos(fD(h, w), fE(h, w)) < 90% [Dcos(fD, fE)]batch
fD(h, w), else
, (5)"
LOOSE RECONSTRUCTION,0.18725868725868725,"where F(·) denotes flatten operation, fD(h, w) represents the feature point at (h, w), sg(·)0.1
180"
LOOSE RECONSTRUCTION,0.18822393822393824,"denotes shrink the gradient to one-tenth of the original
2, Dcos(fD(h, w), fE(h, w))
<
181"
LOOSE RECONSTRUCTION,0.1891891891891892,"90% [Dcos(fD, fE)]batch selects 90% feature points with smaller cosine distance within a batch.
182"
LOOSE RECONSTRUCTION,0.19015444015444016,"Total loss is the mean Lglobal−hm of all encoder-decoder feature pairs.
183"
EXPERIMENTS,0.19111969111969113,"3
Experiments
184"
EXPERIMENTAL SETTINGS,0.1920849420849421,"3.1
Experimental Settings
185"
EXPERIMENTAL SETTINGS,0.19305019305019305,"Datasets. MVTec-AD [20] contains 15 objects (5 texture classes and 10 object classes) with a
186"
EXPERIMENTAL SETTINGS,0.19401544401544402,"total of 3,629 normal images as the training set and 1,725 images as the test set (467 normal, 1258
187"
EXPERIMENTAL SETTINGS,0.19498069498069498,"anomalous). VisA [21] contains 12 objects. Training and test sets are split following the official
188"
EXPERIMENTAL SETTINGS,0.19594594594594594,"splitting, resulting in 8,659 normal images in the training set and 2,162 images in the test set (962
189"
EXPERIMENTAL SETTINGS,0.1969111969111969,"normal, 1,200 anomalous). Real-IAD [22] is a large UAD dataset recently released, containing 30
190"
EXPERIMENTAL SETTINGS,0.19787644787644787,"distinct objects. We follow the official splitting that includes all views, resulting in 36,465 normal
191"
EXPERIMENTAL SETTINGS,0.19884169884169883,"images in the training set and 114,585 images in the test set (63,256 normal, 51,329 anomalous).
192"
EXPERIMENTAL SETTINGS,0.1998069498069498,"Metrics. Following prior works [19; 17], we adopt 7 evaluation metrics. Image-level anomaly
193"
EXPERIMENTAL SETTINGS,0.20077220077220076,"detection performance is measured by the Area Under the Receiver Operator Curve (AUROC),
194"
EXPERIMENTAL SETTINGS,0.20173745173745175,"Average Precision (AP), and F1 score under optimal threshold (F1-max). Pixel-level anomaly
195"
EXPERIMENTAL SETTINGS,0.20270270270270271,"localization is measured by AUROC, AP, F1-max and the Area Under the Per-Region-Overlap
196"
EXPERIMENTAL SETTINGS,0.20366795366795368,"(AUPRO). The results of a dataset is the average of all classes.
197"
EXPERIMENTAL SETTINGS,0.20463320463320464,"Implementation Details. ViT-Base/14 (patchsize=14) pre-trained by DINOv2-R [39] is used as
198"
EXPERIMENTAL SETTINGS,0.2055984555984556,"the encoder by default. The drop rate of Noisy Bottleneck is 0.2 by default and increases to 0.4 on
199"
EXPERIMENTAL SETTINGS,0.20656370656370657,"the diverse Real-IAD. Loose constraint with 2 groups is employed, and the anomaly map is given
200"
EXPERIMENTAL SETTINGS,0.20752895752895753,"by the mean per-point cosine distance of the 2 groups. The input image is first resized to 4482
201"
EXPERIMENTAL SETTINGS,0.2084942084942085,"and then center-cropped to 3922, so the feature map (282) is large enough for anomaly localization.
202"
EXPERIMENTAL SETTINGS,0.20945945945945946,"StableAdamW optimizer [46] with AMSGrad [47] (more stable than AdamW [48] in training) is
203"
EXPERIMENTAL SETTINGS,0.21042471042471042,"utilized with lr=2e-3, β=(0.9,0.999) and wd=1e-4. The network is trained for 10,000 iterations
204"
EXPERIMENTAL SETTINGS,0.21138996138996138,"(steps) on MVTec-AD and VisA, and 50,000 iterations on Real-IAD. More details are available in
205"
EXPERIMENTAL SETTINGS,0.21235521235521235,"Appendix A.2.
206"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.2133204633204633,"3.2
Comparison to Multi-Class UAD SoTAs
207"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.21428571428571427,"We compare the proposed Dinomaly with the most advanced methods. Among them, RD4AD [2]
208"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.21525096525096524,"based on feature reconstruction, SimpleNet [13] based on feature-level pseudo-anomaly, and DeST-
209"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.21621621621621623,"Seg [12] based on feature reconstruction & pseudo anomaly are designed for conventional class-
210"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.2171814671814672,"separated UAD settings. UniAD based on feature reconstruction, ReContrast [18] based on contrastive
211"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.21814671814671815,"reconstruction, ViTAD [17] based on feature reconstruction & Transformer, DiAD [49] based on
212"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.21911196911196912,"Diffusion reconstruction, and MambaAD [19] based on feature reconstruction & Mamba are designed
213"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.22007722007722008,"for MUAD settings. Notably, ViTAD and MambaAD are contemporary arxiv preprints released
214"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.22104247104247104,"within months. The intuitive comparison is already presented in Figure 1.
215"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.222007722007722,"Experimental results are presented in Table 1, where Dinomaly surpasses compared methods by a large
216"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.22297297297297297,"margin on all datasets and all metrics. On the most widely used MVTec-AD, Dinomaly produces
217"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.22393822393822393,"image-level performance of 99.6/99.8/99.0 and pixel-level performance of 98.4/69.3/69.2/94.8,
218"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.2249034749034749,"outperforming previous SoTAs by 1.0/0.2/1.2 and 0.7/9.1/7.7/1.6. This result declares that the
219"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.22586872586872586,"image-level performance on the MVTec-AD dataset is nearly saturated under the MUAD setting.
220"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.22683397683397682,"On the popular VisA, Dinomaly achieves image-level performance of 98.7/98.9/96.2 and pixel-level
221"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.2277992277992278,"performance of 98.7/53.2/55.7/94.5, outperforming previous SoTAs by 3.2/2.5/4.2 and 0.2/5.3/5.1/2.6.
222"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.22876447876447875,"On the Real-IAD that contains 30 classes, each with 5 camera views, we produce image-level and
223"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.22972972972972974,"pixel-level performance of 89.3/86.8/80.2 and 98.8/42.8/47.1/93.9, outperforming previous SoTAs by
224"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.2306949806949807,"3.0/2.2/3.2 and 0.3/4.9/5.4/3.4, indicating our scalability to extremely complex scenarios. Per-class
225"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.23166023166023167,"performances and qualitative visualization are presented in Appendix A.5 and A.6. In addition,
226"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.23262548262548263,"adopting a larger backbone further improves the above performances, as presented in Table A2.
227"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.2335907335907336,2Complete stop-gradient causes optimization instability occasionally.
COMPARISON TO MULTI-CLASS UAD SOTAS,0.23455598455598456,Table 1: Performance under multi-class UAD setting (%). †: method designed for MUAD.
COMPARISON TO MULTI-CLASS UAD SOTAS,0.23552123552123552,"Dateset
Method
Image-level
Pixel-level"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.23648648648648649,"AUROC
AP
F1-max
AUROC
AP
F1-max
AUPRO"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.23745173745173745,MVTec-AD [20]
COMPARISON TO MULTI-CLASS UAD SOTAS,0.2384169884169884,"RD4AD [2]
94.6
96.5
95.2
96.1
48.6
53.8
91.1
SimpleNet [13]
95.3
98.4
95.8
96.9
45.9
49.7
86.5
DeSTSeg [12]
89.2
95.5
91.6
93.1
54.3
50.9
64.8
UniAD [3]†
96.5
98.8
96.2
96.8
43.4
49.5
90.7
ReContrast [18]†
98.3
99.4
97.6
97.1
60.2
61.5
93.2
DiAD [49]†
97.2
99.0
96.5
96.8
52.6
55.5
90.7
ViTAD [17]†
98.3
99.4
97.3
97.7
55.3
58.7
91.4
MambaAD [19]†
98.6
99.6
97.8
97.7
56.3
59.2
93.1
Dinomaly (Ours)
99.6
99.8
99.0
98.4
69.3
69.2
94.8"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.23938223938223938,VisA [21]
COMPARISON TO MULTI-CLASS UAD SOTAS,0.24034749034749034,"RD4AD [2]
92.4
92.4
89.6
98.1
38.0
42.6
91.8
SimpleNet [13]
87.2
87.0
81.8
96.8
34.7
37.8
81.4
DeSTSeg [12]
88.9
89.0
85.2
96.1
39.6
43.4
67.4
UniAD [3]†
88.8
90.8
85.8
98.3
33.7
39.0
85.5
ReContrast [18]†
95.5
96.4
92.0
98.5
47.9
50.6
91.9
DiAD [49]†
86.8
88.3
85.1
96.0
26.1
33.0
75.2
ViTAD [17]†
90.5
91.7
86.3
98.2
36.6
41.1
85.1
MambaAD [19]†
94.3
94.5
89.4
98.5
39.4
44.0
91.0
Dinomaly (Ours)
98.7
98.9
96.2
98.7
53.2
55.7
94.5"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.2413127413127413,Real-IAD [22]
COMPARISON TO MULTI-CLASS UAD SOTAS,0.24227799227799227,"RD4AD [2]
82.4
79.0
73.9
97.3
25.0
32.7
89.6
SimpleNet [13]
57.2
53.4
61.5
75.7
2.8
6.5
39.0
DeSTSeg [12]
82.3
79.2
73.2
94.6
37.9
41.7
40.6
UniAD [3]†
83.0
80.9
74.3
97.3
21.1
29.2
86.7
ReContrast [18]†
86.4
84.2
77.4
97.8
31.6
38.2
91.8
DiAD [49]†
75.6
66.4
69.9
88.0
2.9
7.1
58.1
ViTAD [17]†
82.3
79.4
73.4
96.9
26.7
34.9
84.9
MambaAD [19]†
86.3
84.6
77.0
98.5
33.0
38.7
90.5
Dinomaly (Ours)
89.3
86.8
80.2
98.8
42.8
47.1
93.9"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.24324324324324326,Table 2: Performance under conventional class-separated UAD setting (%). n/a: not available.
COMPARISON TO MULTI-CLASS UAD SOTAS,0.24420849420849422,"Method
MVTec-AD [20]
VisA [21]
Real-IAD [22]"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.24517374517374518,"I-AUROC
P-AP
P-AUPRO
I-AUROC
P-AP
P-AUPRO
I-AUROC
P-AP
P-AUPRO"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.24613899613899615,"Dinomaly (MUAD)
99.6
69.3
94.8
98.7
53.2
94.5
89.3
42.8
93.9"
COMPARISON TO MULTI-CLASS UAD SOTAS,0.2471042471042471,"Dinomaly
99.7
68.9
95.0
98.9
50.7
95.1
92.0
45.2
95.1
RD4AD [2]
98.5
58.0
93.9
96.0
27.7
70.9
87.1
n/a
93.8
PatchCore [8]
99.1
56.1
93.5
95.1
40.1
91.2
89.4
n/a
91.5
SimpleNet [13]
99.6
54.8
90.0
96.8
36.3
88.7
88.5
n/a
84.6
EfficientAD [15]
99.1
63.8
93.5
98.1
40.8
94.0
n/a
n/a
n/a"
COMPARISON TO CLASS-SEPARATED UAD SOTAS,0.24806949806949807,"3.3
Comparison to Class-Separated UAD SoTAs
228"
COMPARISON TO CLASS-SEPARATED UAD SOTAS,0.24903474903474904,"We also compare our Dinomaly with class-separated SoTAs, as shown in Table 2. On MVTec-AD and
229"
COMPARISON TO CLASS-SEPARATED UAD SOTAS,0.25,"VisA, our Dinomaly under MUAD setting is comparable to conventional SoTAs that build individual
230"
COMPARISON TO CLASS-SEPARATED UAD SOTAS,0.25096525096525096,"models for each class [2; 13; 8; 15]. In addition, Dinomaly is subjected to nearly no performance
231"
COMPARISON TO CLASS-SEPARATED UAD SOTAS,0.2519305019305019,"drop compared to its class-separated counterpart on these datasets. On the complicated Real-IAD
232"
COMPARISON TO CLASS-SEPARATED UAD SOTAS,0.2528957528957529,"that involves more images, classes, and views, class-separated Dinomaly sets new SoTA records.
233"
COMPARISON TO CLASS-SEPARATED UAD SOTAS,0.25386100386100385,"Multi-class Dinomaly presents moderate performance drop but is still comparable to class-separated
234"
COMPARISON TO CLASS-SEPARATED UAD SOTAS,0.2548262548262548,"SoTAs.
235"
ABLATION STUDY,0.2557915057915058,"3.4
Ablation Study
236"
ABLATION STUDY,0.25675675675675674,"Overall Ablation. We conduct experiments to verify the effectiveness of the proposed elements, i.e.,
237"
ABLATION STUDY,0.2577220077220077,"Noisy Bottleneck (NB), Linear Attention (LA), Loose Constraint (LC), and Loose Loss (LL). The
238"
ABLATION STUDY,0.25868725868725867,"already-powerful baseline is Dinomaly with noiseless MLP bottleneck, Softmax Attention, dense
239"
ABLATION STUDY,0.25965250965250963,"layer-to-layer supervision, and global cosine loss [18]. Results on MVTec-AD and VisA are shown in
240"
ABLATION STUDY,0.2606177606177606,"Table 3 and Table A1, respectively. NB and LL can directly contribute to the model performance. LA
241"
ABLATION STUDY,0.26158301158301156,"and LC boost the performance with the presence of NB. The use of LC is not solely beneficial because
242"
ABLATION STUDY,0.2625482625482625,"Table 3: Ablations of Dinomaly elements on MVTec-AD (%). NB: Noisy Bottleneck. LA: Linear
Attention. LC: Loose Constraint (2 groups). LL: Loose Loss. Results on VisA see Table A1."
ABLATION STUDY,0.2635135135135135,"NB
LA
LC
LL
Image-level
Pixel-level"
ABLATION STUDY,0.2644787644787645,"AUROC
AP
F1-max
AUROC
AP
F1-max
AUPRO"
ABLATION STUDY,0.26544401544401547,"98.41
99.09
97.41
97.18
62.96
63.82
92.95
✓
99.06
99.54
98.31
97.62
66.22
66.70
93.71
✓
98.54
99.21
97.62
97.20
62.94
63.73
93.09
✓
98.35
99.04
97.43
97.10
61.05
62.73
92.60
✓
99.03
99.45
98.19
97.62
64.10
64.96
93.34
✓
✓
99.27
99.62
98.63
97.85
67.36
67.33
94.16
✓
✓
99.50
99.72
98.87
98.14
68.16
68.24
94.23
✓
✓
✓
99.52
99.73
98.92
98.20
68.25
68.34
94.17
✓
✓
✓
99.57
99.78
99.00
98.20
67.93
68.21
94.50
✓
✓
✓
✓
99.60
99.78
99.04
98.35
69.29
69.17
94.79"
ABLATION STUDY,0.26640926640926643,"Table 4: Ablations of Dropout rates in Noisy Bottleneck, conducted on MVTec-AD (%). †: default."
ABLATION STUDY,0.2673745173745174,"Dropout rate
Image-level
Pixel-level"
ABLATION STUDY,0.26833976833976836,"AUROC
AP
F1-max
AUROC
AP
F1-max
AUPRO"
ABLATION STUDY,0.2693050193050193,"0 (noiseless)
98.19
99.55
98.51
97.55
63.11
64.39
93.33
0.1
99.54
99.75
98.90
98.35
69.46
69.19
94.53
0.2 †
99.60
99.78
99.04
98.35
69.29
69.17
94.79
0.3
99.65
99.83
99.16
98.34
68.46
68.81
94.63
0.4
99.64
99.80
99.23
98.22
67.95
68.33
94.57
0.5
99.56
99.81
99.14
98.15
67.43
67.82
94.64"
ABLATION STUDY,0.2702702702702703,"Table 5: Ablations of reconstruction constraint, conduected on MVTec-AD (%). †: default."
ABLATION STUDY,0.27123552123552125,"Constraints
Image-level
Pixel-level"
ABLATION STUDY,0.2722007722007722,"AUROC
AP
F1-max
AUROC
AP
F1-max
AUPRO"
ABLATION STUDY,0.2731660231660232,"layer-to-layer (dense, every 1)
99.39
99.68
98.73
98.12
68.55
68.63
94.28
layer-to-layer (sparse, every 2)
99.52
99.73
98.95
98.16
68.89
68.57
94.40
layer-to-layer (sparse, every 4)
99.54
99.77
99.05
98.04
66.69
67.17
94.07
layer-to-cat-layer (every 2)
99.48
99.71
99.26
97.83
62.29
62.91
93.16
group-to-group (1 group)
99.64
99.80
99.36
98.18
64.79
65.40
93.96
group-to-group (2 groups)†
99.60
99.78
99.04
98.35
69.29
69.17
94.79"
ABLATION STUDY,0.27413127413127414,"LC makes the reconstruction too easy without injected noise. Combining some of the proposed
243"
ABLATION STUDY,0.2750965250965251,"elements boosts the performance of the baseline, while employing them all produces the best results.
244"
ABLATION STUDY,0.27606177606177607,"Noisy Rates. We conduct ablations on the discarding rate of the Dropouts in MLP bottleneck, as
245"
ABLATION STUDY,0.27702702702702703,"shown in Table 4. Experimental results demonstrate that Dinomaly is robust to different levels
246"
ABLATION STUDY,0.277992277992278,"of dropout rate. Reconstruction Constraint. We quantitatively examine different reconstruction
247"
ABLATION STUDY,0.27895752895752896,"schemes presented in Figure 4. As shown in Table 5, group-to-group LC outperforms layer-to-layer
248"
ABLATION STUDY,0.2799227799227799,"supervision. On image-level metrics, 1-group LC with all layers added performs similarly to its
249"
ABLATION STUDY,0.2808880308880309,"2-group counterpart that separates low-level and high-level layers; however, 1-group LC mixes
250"
ABLATION STUDY,0.28185328185328185,"low-level and high-level features which is harmful for anomaly localization. More ablations on
251"
ABLATION STUDY,0.2828185328185328,"scalability, input size, pre-trained foundations, etc., are presented in Appendix A.3.
252"
CONCLUSION,0.28378378378378377,"4
Conclusion
253"
CONCLUSION,0.28474903474903474,"Dinomaly, a minimalistic UAD framework, is proposed to address the under-performed MUAD
254"
CONCLUSION,0.2857142857142857,"models in this paper. We present four key elements in Dinomaly, i.e., Foundation Transformer, Noisy
255"
CONCLUSION,0.28667953667953666,"MLP Bottleneck, Linear Attention, and Loose Reconstruction, that can boost the performance under
256"
CONCLUSION,0.2876447876447876,"the challenging MUAD setting without fancy modules and tricks. Extensive experiments on MVTec
257"
CONCLUSION,0.2886100386100386,"AD, VisA, and Real-IAD demonstrate our superiority over previous model-unified multi-class models
258"
CONCLUSION,0.28957528957528955,"and even recent class-separated models, indicating the feasibility of implementing a unified model in
259"
CONCLUSION,0.2905405405405405,"complicated scenarios free of severe performance degradation.
260"
REFERENCES,0.2915057915057915,"References
261"
REFERENCES,0.2924710424710425,"[1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale
262"
REFERENCES,0.29343629343629346,"hierarchical image database,” in IEEE Conference on Computer Vision and Pattern Recognition,
263"
REFERENCES,0.2944015444015444,"pp. 248–255, 2009.
264"
REFERENCES,0.2953667953667954,"[2] H. Deng and X. Li, “Anomaly detection via reverse distillation from one-class embedding,”
265"
REFERENCES,0.29633204633204635,"in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
266"
REFERENCES,0.2972972972972973,"pp. 9737–9746, 2022.
267"
REFERENCES,0.2982625482625483,"[3] Z. You, L. Cui, Y. Shen, K. Yang, X. Lu, Y. Zheng, and X. Le, “A unified model for multi-class
268"
REFERENCES,0.29922779922779924,"anomaly detection,” arXiv preprint arXiv:2206.03687, 2022.
269"
REFERENCES,0.3001930501930502,"[4] J. Yang, Y. Shi, and Z. Qi, “Dfr: Deep feature reconstruction for unsupervised anomaly
270"
REFERENCES,0.30115830115830117,"segmentation,” arXiv preprint arXiv:2012.07122, 2020.
271"
REFERENCES,0.30212355212355213,"[5] M. Salehi, N. Sadjadi, S. Baselizadeh, M. H. Rohban, and H. R. Rabiee, “Multiresolution
272"
REFERENCES,0.3030888030888031,"knowledge distillation for anomaly detection,” in Proceedings of the IEEE/CVF conference on
273"
REFERENCES,0.30405405405405406,"computer vision and pattern recognition, pp. 14902–14912, 2021.
274"
REFERENCES,0.305019305019305,"[6] G. Wang, S. Han, E. Ding, and D. Huang, “Student-teacher feature pyramid matching for
275"
REFERENCES,0.305984555984556,"anomaly detection,” in The British Machine Vision Conference (BMVC), 2021.
276"
REFERENCES,0.30694980694980695,"[7] T. Defard, A. Setkov, A. Loesch, and R. Audigier, “Padim: a patch distribution modeling
277"
REFERENCES,0.3079150579150579,"framework for anomaly detection and localization,” in International Conference on Pattern
278"
REFERENCES,0.3088803088803089,"Recognition, pp. 475–489, Springer, 2021.
279"
REFERENCES,0.30984555984555984,"[8] K. Roth, L. Pemula, J. Zepeda, B. Schölkopf, T. Brox, and P. Gehler, “Towards total recall in
280"
REFERENCES,0.3108108108108108,"industrial anomaly detection,” in Proceedings of the IEEE/CVF Conference on Computer Vision
281"
REFERENCES,0.31177606177606176,"and Pattern Recognition, pp. 14318–14328, 2022.
282"
REFERENCES,0.3127413127413127,"[9] S. Lee, S. Lee, and B. C. Song, “Cfa: Coupled-hypersphere-based feature adaptation for
283"
REFERENCES,0.3137065637065637,"target-oriented anomaly localization,” IEEE Access, vol. 10, pp. 78446–78454, 2022.
284"
REFERENCES,0.31467181467181465,"[10] V. Zavrtanik, M. Kristan, and D. Skoˇc aj, “Draem-a discriminatively trained reconstruction
285"
REFERENCES,0.3156370656370656,"embedding for surface anomaly detection,” in Proceedings of the IEEE/CVF International
286"
REFERENCES,0.3166023166023166,"Conference on Computer Vision, pp. 8330–8339, 2021.
287"
REFERENCES,0.31756756756756754,"[11] C.-L. Li, K. Sohn, J. Yoon, and T. Pfister, “Cutpaste: Self-supervised learning for anomaly
288"
REFERENCES,0.3185328185328185,"detection and localization,” in Proceedings of the IEEE/CVF Conference on Computer Vision
289"
REFERENCES,0.3194980694980695,"and Pattern Recognition, pp. 9664–9674, 2021.
290"
REFERENCES,0.3204633204633205,"[12] X. Zhang, S. Li, X. Li, P. Huang, J. Shan, and T. Chen, “Destseg: Segmentation guided
291"
REFERENCES,0.32142857142857145,"denoising student-teacher for anomaly detection,” in Proceedings of the IEEE/CVF Conference
292"
REFERENCES,0.3223938223938224,"on Computer Vision and Pattern Recognition, pp. 3914–3923, 2023.
293"
REFERENCES,0.3233590733590734,"[13] Z. Liu, Y. Zhou, Y. Xu, and Z. Wang, “Simplenet: A simple network for image anomaly
294"
REFERENCES,0.32432432432432434,"detection and localization,” arXiv preprint arXiv:2303.15140, 2023.
295"
REFERENCES,0.3252895752895753,"[14] J. Bae, J.-H. Lee, and S. Kim, “Pni: industrial anomaly detection using position and neighbor-
296"
REFERENCES,0.32625482625482627,"hood information,” in Proceedings of the IEEE/CVF International Conference on Computer
297"
REFERENCES,0.32722007722007723,"Vision, pp. 6373–6383, 2023.
298"
REFERENCES,0.3281853281853282,"[15] K. Batzner, L. Heckler, and R. König, “Efficientad: Accurate visual anomaly detection at
299"
REFERENCES,0.32915057915057916,"millisecond-level latencies,” in Proceedings of the IEEE/CVF Winter Conference on Applications
300"
REFERENCES,0.3301158301158301,"of Computer Vision, pp. 128–138, 2024.
301"
REFERENCES,0.3310810810810811,"[16] X. Zhang, M. Xu, and X. Zhou, “Realnet: A feature selection network with realistic synthetic
302"
REFERENCES,0.33204633204633205,"anomaly for anomaly detection,” arXiv preprint arXiv:2403.05897, 2024.
303"
REFERENCES,0.333011583011583,"[17] J. Zhang, X. Chen, Y. Wang, C. Wang, Y. Liu, X. Li, M.-H. Yang, and D. Tao, “Explor-
304"
REFERENCES,0.333976833976834,"ing plain vit reconstruction for multi-class unsupervised anomaly detection,” arXiv preprint
305"
REFERENCES,0.33494208494208494,"arXiv:2312.07495, 2023.
306"
REFERENCES,0.3359073359073359,"[18] J. Guo, S. Lu, L. Jia, W. Zhang, and H. Li, “Recontrast: Domain-specific anomaly detection via
307"
REFERENCES,0.33687258687258687,"contrastive reconstruction,” in Advances in Neural Information Processing Systems (NeurIPS),
308"
REFERENCES,0.33783783783783783,"vol. 36, pp. 10721–10740, 2023.
309"
REFERENCES,0.3388030888030888,"[19] H. He, Y. Bai, J. Zhang, Q. He, H. Chen, Z. Gan, C. Wang, X. Li, G. Tian, and L. Xie,
310"
REFERENCES,0.33976833976833976,"“Mambaad: Exploring state space models for multi-class unsupervised anomaly detection,”
311"
REFERENCES,0.3407335907335907,"arXiv preprint arXiv:2404.06564, 2024.
312"
REFERENCES,0.3416988416988417,"[20] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, “Mvtec ad–a comprehensive real-world
313"
REFERENCES,0.34266409266409265,"dataset for unsupervised anomaly detection,” in Proceedings of the IEEE/CVF conference on
314"
REFERENCES,0.3436293436293436,"computer vision and pattern recognition, pp. 9592–9600, 2019.
315"
REFERENCES,0.34459459459459457,"[21] Y. Zou, J. Jeong, L. Pemula, D. Zhang, and O. Dabeer, “Spot-the-difference self-supervised
316"
REFERENCES,0.34555984555984554,"pre-training for anomaly detection and segmentation,” in European Conference on Computer
317"
REFERENCES,0.3465250965250965,"Vision, pp. 392–408, Springer, 2022.
318"
REFERENCES,0.3474903474903475,"[22] C. Wang, W. Zhu, B.-B. Gao, Z. Gan, J. Zhang, Z. Gu, S. Qian, M. Chen, and L. Ma, “Real-iad:
319"
REFERENCES,0.3484555984555985,"A real-world multi-view dataset for benchmarking versatile industrial anomaly detection,” arXiv
320"
REFERENCES,0.34942084942084944,"preprint arXiv:2403.12580, 2024.
321"
REFERENCES,0.3503861003861004,"[23] Y. Zhao, “Omnial: A unified cnn framework for unsupervised anomaly localization,” in Proceed-
322"
REFERENCES,0.35135135135135137,"ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3924–3933,
323"
REFERENCES,0.35231660231660233,"2023.
324"
REFERENCES,0.3532818532818533,"[24] R. Lu, Y. Wu, L. Tian, D. Wang, B. Chen, X. Liu, and R. Hu, “Hierarchical vector quantized
325"
REFERENCES,0.35424710424710426,"transformer for multi-class unsupervised anomaly detection,” arXiv preprint arXiv:2310.14228,
326"
REFERENCES,0.3552123552123552,"2023.
327"
REFERENCES,0.3561776061776062,"[25] H. Yin, G. Jiao, Q. Wu, B. F. Karlsson, B. Huang, and C. Y. Lin, “Lafite: Latent diffusion
328"
REFERENCES,0.35714285714285715,"model with feature editing for unsupervised multi-class anomaly detection,” arXiv preprint
329"
REFERENCES,0.3581081081081081,"arXiv:2307.08059, 2023.
330"
REFERENCES,0.3590733590733591,"[26] H. He, J. Zhang, H. Chen, X. Chen, Z. Li, X. Chen, Y. Wang, C. Wang, and L. Xie, “Diad: A
331"
REFERENCES,0.36003861003861004,"diffusion-based framework for multi-class anomaly detection,” arXiv preprint arXiv:2312.06607,
332"
REFERENCES,0.361003861003861,"2023.
333"
REFERENCES,0.36196911196911197,"[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
334"
REFERENCES,0.36293436293436293,"I. Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing
335"
REFERENCES,0.3638996138996139,"Systems, pp. 5998–6008, 2017.
336"
REFERENCES,0.36486486486486486,"[28] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
337"
REFERENCES,0.3658301158301158,"M. Minderer, G. Heigold, S. Gelly, et al., “An image is worth 16x16 words: Transformers for
338"
REFERENCES,0.3667953667953668,"image recognition at scale,” arXiv preprint arXiv:2010.11929, 2020.
339"
REFERENCES,0.36776061776061775,"[29] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin, “Emerging
340"
REFERENCES,0.3687258687258687,"properties in self-supervised vision transformers,” in Proceedings of the IEEE/CVF international
341"
REFERENCES,0.3696911196911197,"conference on computer vision, pp. 9650–9660, 2021.
342"
REFERENCES,0.37065637065637064,"[30] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza,
343"
REFERENCES,0.3716216216216216,"F. Massa, A. El-Nouby, et al., “Dinov2: Learning robust visual features without supervision,”
344"
REFERENCES,0.37258687258687256,"arXiv preprint arXiv:2304.07193, 2023.
345"
REFERENCES,0.3735521235521235,"[31] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer:
346"
REFERENCES,0.3745173745173745,"Hierarchical vision transformer using shifted windows,” in IEEE/CVF International Conference
347"
REFERENCES,0.3754826254826255,"on Computer Vision, pp. 10012–10022, 2021.
348"
REFERENCES,0.3764478764478765,"[32] X. Chen, S. Xie, and K. He, “An empirical study of training self-supervised vision transformers,”
349"
REFERENCES,0.37741312741312744,"in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9640–9649,
350"
REFERENCES,0.3783783783783784,"2021.
351"
REFERENCES,0.37934362934362936,"[33] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders are scalable
352"
REFERENCES,0.3803088803088803,"vision learners,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
353"
REFERENCES,0.3812741312741313,"Recognition, pp. 16000–16009, 2022.
354"
REFERENCES,0.38223938223938225,"[34] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu, “Simmim: A simple
355"
REFERENCES,0.3832046332046332,"framework for masked image modeling,” in Proceedings of the IEEE/CVF conference on
356"
REFERENCES,0.3841698841698842,"computer vision and pattern recognition, pp. 9653–9663, 2022.
357"
REFERENCES,0.38513513513513514,"[35] Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei, “Beit v2: Masked image modeling with vector-
358"
REFERENCES,0.3861003861003861,"quantized visual tokenizers,” arXiv preprint arXiv:2208.06366, 2022.
359"
REFERENCES,0.38706563706563707,"[36] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong, “ibot: Image bert pre-training
360"
REFERENCES,0.38803088803088803,"with online tokenizer,” arXiv preprint arXiv:2111.07832, 2021.
361"
REFERENCES,0.388996138996139,"[37] T. Reiss, N. Cohen, E. Horwitz, R. Abutbul, and Y. Hoshen, “Anomaly detection requires better
362"
REFERENCES,0.38996138996138996,"representations,” in European Conference on Computer Vision, pp. 56–68, Springer, 2022.
363"
REFERENCES,0.3909266409266409,"[38] Y. Lee, H. Lim, and H. Yoon, “Selformaly: Towards task-agnostic unified anomaly detection,”
364"
REFERENCES,0.3918918918918919,"arXiv preprint arXiv:2307.12540, 2023.
365"
REFERENCES,0.39285714285714285,"[39] T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski, “Vision transformers need registers,” arXiv
366"
REFERENCES,0.3938223938223938,"preprint arXiv:2309.16588, 2023.
367"
REFERENCES,0.3947876447876448,"[40] Y. Liang, J. Zhang, S. Zhao, R. Wu, Y. Liu, and S. Pan, “Omni-frequency channel-selection
368"
REFERENCES,0.39575289575289574,"representations for unsupervised anomaly detection,” arXiv preprint arXiv:2203.00259, 2022.
369"
REFERENCES,0.3967181467181467,"[41] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, “Im-
370"
REFERENCES,0.39768339768339767,"proving neural networks by preventing co-adaptation of feature detectors,” arXiv preprint
371"
REFERENCES,0.39864864864864863,"arXiv:1207.0580, 2012.
372"
REFERENCES,0.3996138996138996,"[42] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, “Transformers are rnns: Fast autore-
373"
REFERENCES,0.40057915057915056,"gressive transformers with linear attention,” in International conference on machine learning,
374"
REFERENCES,0.4015444015444015,"pp. 5156–5165, PMLR, 2020.
375"
REFERENCES,0.4025096525096525,"[43] D. Han, X. Pan, Y. Han, S. Song, and G. Huang, “Flatten transformer: Vision transformer
376"
REFERENCES,0.4034749034749035,"using focused linear attention,” in Proceedings of the IEEE/CVF International Conference on
377"
REFERENCES,0.40444015444015446,"Computer Vision, pp. 5961–5971, 2023.
378"
REFERENCES,0.40540540540540543,"[44] Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li, “Efficient attention: Attention with linear
379"
REFERENCES,0.4063706563706564,"complexities,” in Proceedings of the IEEE/CVF winter conference on applications of computer
380"
REFERENCES,0.40733590733590735,"vision, pp. 3531–3539, 2021.
381"
REFERENCES,0.4083011583011583,"[45] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” arXiv
382"
REFERENCES,0.4092664092664093,"preprint arXiv:1503.02531, 2015.
383"
REFERENCES,0.41023166023166024,"[46] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt, “Stable and
384"
REFERENCES,0.4111969111969112,"low-precision training for large-scale vision-language models,” Advances in Neural Information
385"
REFERENCES,0.41216216216216217,"Processing Systems, vol. 36, pp. 10271–10298, 2023.
386"
REFERENCES,0.41312741312741313,"[47] S. J. Reddi, S. Kale, and S. Kumar, “On the convergence of adam and beyond,” arXiv preprint
387"
REFERENCES,0.4140926640926641,"arXiv:1904.09237, 2019.
388"
REFERENCES,0.41505791505791506,"[48] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” arXiv preprint
389"
REFERENCES,0.416023166023166,"arXiv:1711.05101, 2017.
390"
REFERENCES,0.416988416988417,"[49] H. He, J. Zhang, H. Chen, X. Chen, Z. Li, X. Chen, Y. Wang, C. Wang, and L. Xie, “A diffusion-
391"
REFERENCES,0.41795366795366795,"based framework for multi-class anomaly detection,” in Proceedings of the AAAI Conference on
392"
REFERENCES,0.4189189189189189,"Artificial Intelligence, vol. 38, pp. 8472–8480, 2024.
393"
REFERENCES,0.4198841698841699,"[50] P. Bergmann, S. Löwe, M. Fauser, D. Sattlegger, and C. Steger, “Improving unsuper-
394"
REFERENCES,0.42084942084942084,"vised defect segmentation by applying structural similarity to autoencoders,” arXiv preprint
395"
REFERENCES,0.4218146718146718,"arXiv:1807.02011, 2018.
396"
REFERENCES,0.42277992277992277,"[51] V. Zavrtanik, M. Kristan, and D. Skocaj, “Reconstruction by inpainting for visual anomaly
397"
REFERENCES,0.42374517374517373,"detection,” Pattern Recognition, vol. 112, p. 107706, 2021.
398"
REFERENCES,0.4247104247104247,"[52] W. Liu, R. Li, M. Zheng, S. Karanam, Z. Wu, B. Bhanu, R. J. Radke, and O. Camps, “Towards
399"
REFERENCES,0.42567567567567566,"visually explaining variational autoencoders,” in Proceedings of the IEEE/CVF Conference on
400"
REFERENCES,0.4266409266409266,"Computer Vision and Pattern Recognition, pp. 8642–8651, 2020.
401"
REFERENCES,0.4276061776061776,"[53] D. Dehaene and P. Eline, “Anomaly localization by modeling perceptual features,” arXiv
402"
REFERENCES,0.42857142857142855,"preprint arXiv:2008.05369, 2020.
403"
REFERENCES,0.4295366795366795,"[54] T. Schlegl, P. Seeböck, S. M. Waldstein, G. Langs, and U. Schmidt-Erfurth, “f-anogan: Fast
404"
REFERENCES,0.4305019305019305,"unsupervised anomaly detection with generative adversarial networks,” Medical image analysis,
405"
REFERENCES,0.4314671814671815,"vol. 54, pp. 30–44, 2019.
406"
REFERENCES,0.43243243243243246,"[55] S. Akcay, A. Atapour-Abarghouei, and T. P. Breckon, “Ganomaly: Semi-supervised anomaly
407"
REFERENCES,0.4333976833976834,"detection via adversarial training,” in Computer Vision–ACCV 2018: 14th Asian Conference on
408"
REFERENCES,0.4343629343629344,"Computer Vision, Perth, Australia, December 2–6, 2018, Revised Selected Papers, Part III 14,
409"
REFERENCES,0.43532818532818535,"pp. 622–637, Springer, 2019.
410"
REFERENCES,0.4362934362934363,"[56] S. Sheynin, S. Benaim, and L. Wolf, “A hierarchical transformation-discriminating genera-
411"
REFERENCES,0.4372586872586873,"tive model for few shot anomaly detection,” in Proceedings of the IEEE/CVF International
412"
REFERENCES,0.43822393822393824,"Conference on Computer Vision, pp. 8495–8504, 2021.
413"
REFERENCES,0.4391891891891892,"[57] J. Liu and F. Wang, “mixed attention auto encoder for multi-class industrial anomaly detection,”
414"
REFERENCES,0.44015444015444016,"arXiv preprint arXiv:2309.12700, 2023.
415"
REFERENCES,0.4411196911196911,"[58] X. Chen and K. He, “Exploring simple siamese representation learning,” in Proceedings of the
416"
REFERENCES,0.4420849420849421,"IEEE/CVF conference on computer vision and pattern recognition, pp. 15750–15758, 2021.
417"
REFERENCES,0.44305019305019305,"[59] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J’egou, “Training data-
418"
REFERENCES,0.444015444015444,"efficient image transformers & distillation through attention,” arXiv preprint arXiv:2012.12877,
419"
REFERENCES,0.444980694980695,"2021.
420"
REFERENCES,0.44594594594594594,"[60] S. Ren, Z. Wang, H. Zhu, J. Xiao, A. Yuille, and C. Xie, “Rejuvenating image-gpt as strong
421"
REFERENCES,0.4469111969111969,"visual representation learners,” arXiv preprint arXiv:2312.02147, 2023.
422"
REFERENCES,0.44787644787644787,"[61] W. Yu, C. Si, P. Zhou, M. Luo, Y. Zhou, J. Feng, S. Yan, and X. Wang, “Metaformer baselines
423"
REFERENCES,0.44884169884169883,"for vision,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.
424"
REFERENCES,0.4498069498069498,"[62] T. Reiss, N. Cohen, L. Bergman, and Y. Hoshen, “Panda: Adapting pretrained features for
425"
REFERENCES,0.45077220077220076,"anomaly detection and segmentation,” in Proceedings of the IEEE/CVF Conference on Computer
426"
REFERENCES,0.4517374517374517,"Vision and Pattern Recognition, pp. 2806–2814, 2021.
427"
REFERENCES,0.4527027027027027,"[63] J. Jeong, Y. Zou, T. Kim, D. Zhang, A. Ravichandran, and O. Dabeer, “Winclip: Zero-/few-shot
428"
REFERENCES,0.45366795366795365,"anomaly classification and segmentation,” arXiv preprint arXiv:2303.14814, 2023.
429"
REFERENCES,0.4546332046332046,"[64] C. Huang, H. Guan, A. Jiang, Y. Zhang, M. Spratling, and Y.-F. Wang, “Registration based few-
430"
REFERENCES,0.4555984555984556,"shot anomaly detection,” in European Conference on Computer Vision, pp. 303–319, Springer,
431"
REFERENCES,0.45656370656370654,"2022.
432"
REFERENCES,0.4575289575289575,"[65] X. Jiang, J. Liu, J. Wang, Q. Nie, K. Wu, Y. Liu, C. Wang, and F. Zheng, “Softpatch: Unsuper-
433"
REFERENCES,0.4584942084942085,"vised anomaly detection with noisy data,” Advances in Neural Information Processing Systems,
434"
REFERENCES,0.4594594594594595,"vol. 35, pp. 15433–15445, 2022.
435"
REFERENCES,0.46042471042471045,"A
Appendix / supplemental material
436"
REFERENCES,0.4613899613899614,"A.1
Related Work
437"
REFERENCES,0.4623552123552124,"Epistemic methods are based on the assumption that the networks respond differently during in-
438"
REFERENCES,0.46332046332046334,"ference between seen input and unseen input. Within this paradigm, pixel reconstruction methods
439"
REFERENCES,0.4642857142857143,"assume that the networks trained on normal images can reconstruct anomaly-free regions well, but
440"
REFERENCES,0.46525096525096526,"poorly for anomalous regions. Auto-encoder (AE) [50; 51], variational auto-encoder (VAE) [52; 53],
441"
REFERENCES,0.46621621621621623,"or generative adversarial network (GAN) [54; 55] are used to restore normal pixels. However, pixel
442"
REFERENCES,0.4671814671814672,"reconstruction models may also succeed in restoring unseen anomalous regions if they resemble
443"
REFERENCES,0.46814671814671815,"normal regions in pixel values or the anomalies are barely noticeable [2]. Therefore, feature recon-
444"
REFERENCES,0.4691119691119691,"struction is proposed to construct features of pre-trained encoders instead of raw pixels [2; 3; 4]. To
445"
REFERENCES,0.4700772200772201,"prevent the whole network from converging to a trivial solution, the parameters of the encoders are
446"
REFERENCES,0.47104247104247104,"frozen during training. In feature distillation [5; 6], the student network is trained from scratch to
447"
REFERENCES,0.472007722007722,"mimic the output features of the pre-trained teacher network with the same input of normal images,
448"
REFERENCES,0.47297297297297297,"also based on the similar hypothesis that the student trained on normal samples only succeed in
449"
REFERENCES,0.47393822393822393,"mimicking features of normal regions.
450"
REFERENCES,0.4749034749034749,"Pseudo-anomaly methods generate handcrafted defects on normal images to imitate anomalies,
451"
REFERENCES,0.47586872586872586,"converting UAD to supervised classification [11] or segmentation tasks [10]. Specifically, CutPaste
452"
REFERENCES,0.4768339768339768,"[11] simulates anomalous regions by randomly pasting cropped patches of normal images. DRAEM
453"
REFERENCES,0.4777992277992278,"[10] constructs abnormal regions using Perlin noise as the mask and another image as the additive
454"
REFERENCES,0.47876447876447875,"anomaly. DeTSeg [12] employs a similar anomaly generation strategy and combines it with feature
455"
REFERENCES,0.4797297297297297,"reconstruction. SimpleNet [13] introduces anomaly by injecting Gaussian noise in the pre-trained
456"
REFERENCES,0.4806949806949807,"feature space. These methods deeply rely on how well the pseudo anomalies match the real anomalies,
457"
REFERENCES,0.48166023166023164,"which makes it hard to generalize to different datasets.
458"
REFERENCES,0.4826254826254826,"Feature statistics methods [7; 8; 56; 9] memorize all normal features (or their modeled distribution)
459"
REFERENCES,0.48359073359073357,"extracted by networks pre-trained on large-scale datasets and match them with test samples during
460"
REFERENCES,0.48455598455598453,"inference. Since these methods require memorizing, processing, and matching nearly all features
461"
REFERENCES,0.4855212355212355,"from training samples, they are computationally expensive in both training and inference, especially
462"
REFERENCES,0.4864864864864865,"when the training set is large.
463"
REFERENCES,0.4874517374517375,"Multi-Class UAD. UniAD [3] first introduced multi-class anomaly detection, aiming to detect
464"
REFERENCES,0.48841698841698844,"anomalies for different classes using a unified model. In this setting, conventional UAD methods
465"
REFERENCES,0.4893822393822394,"often face the challenge of ""identical shortcuts"", where both anomaly-free and anomaly samples can be
466"
REFERENCES,0.49034749034749037,"effectively recovered during inference [3]. It is caused by the diversity of multi-class normal patterns
467"
REFERENCES,0.49131274131274133,"that drive the network to generalize on unseen patterns. This contradicts the fundamental assumption
468"
REFERENCES,0.4922779922779923,"of epistemic methods. Many current researches focus on addressing this challenge [3; 24; 18; 57; 25].
469"
REFERENCES,0.49324324324324326,"UniAD [3] employs a neighbor-masked attention module and a feature-jitter strategy to mitigate these
470"
REFERENCES,0.4942084942084942,"shortcuts. HVQ-Trans [24] proposes a vector quantization (VQ) Transformer model that induces
471"
REFERENCES,0.4951737451737452,"large feature discrepancies for anomalies. LafitE [25] utilizes a latent diffusion model and introduces
472"
REFERENCES,0.49613899613899615,"a feature editing strategy to alleviate this issue. DiAD [26] also employs diffusion models to address
473"
REFERENCES,0.4971042471042471,"multi-class UAD settings. OmniAL [23] focuses on anomaly localization in the unified setting,
474"
REFERENCES,0.4980694980694981,"preventing identical reconstruction by using synthesized pseudo anomalies. ViTAD [58] abstracts a
475"
REFERENCES,0.49903474903474904,"unified feature-reconstruction UAD framework and employ Transformer building blocks. MambaAD
476"
REFERENCES,0.5,"[19] explores the recently proposed State Space Model (SSM), Mamba, in the context of multi-class
477"
REFERENCES,0.500965250965251,"UAD.
478"
REFERENCES,0.5019305019305019,"Scope of Application. In this work, we focus on sensory AD that detects regional or structural
479"
REFERENCES,0.502895752895753,"anomalies (common in practical applications such as industrial inspection, medical disease screening,
480"
REFERENCES,0.5038610038610039,"etc.), which is distinguished from semantic AD. In sensory AD, normal and anomalous samples
481"
REFERENCES,0.5048262548262549,"are the same objects except for anomaly, e.g. good cable vs. spoiled cable. In semantic AD, the
482"
REFERENCES,0.5057915057915058,"class of normal samples and anomalous samples are semantically different, e.g. animals vs. vehicles.
483"
REFERENCES,0.5067567567567568,"Semantic AD methods usually utilize and compare the global representation of images, which
484"
REFERENCES,0.5077220077220077,"generally do not suffer from the issues of multi-class setting discussed in this paper..
485"
REFERENCES,0.5086872586872587,"A.2
Full Implementation Details
486"
REFERENCES,0.5096525096525096,"ViT-Base/14 (patch size=14) pre-trained by DINOv2 with registers (DINOv2-R) [39] is utilized as
487"
REFERENCES,0.5106177606177607,"the encoder. The discard rate of Dropout in Noisy Bottleneck is 0.2 by default, which is increased to
488"
REFERENCES,0.5115830115830116,"0.4 for the diverse Real-IAD. Loose constraint with 2 groups and Lglobal−hm loss are used by default.
489"
REFERENCES,0.5125482625482626,"The input image is first resized to 4482 and then center-cropped to 3922, so that the feature map
490"
REFERENCES,0.5135135135135135,"(282) is large enough for localization. StableAdamW optimizer [46] with AMSGrad [47] is utilized
491"
REFERENCES,0.5144787644787645,"with lr (learning rate)=2e-3, β=(0.9,0.999), wd (weight decay)=1e-4 and eps=1e-10. The network
492"
REFERENCES,0.5154440154440154,"is trained for 10,000 iterations for MVTec-AD and VisA and 50,000 iterations for Real-IAD under
493"
REFERENCES,0.5164092664092664,"MUAD setting. The network is trained for 5,000 iterations on each class under the class-separated
494"
REFERENCES,0.5173745173745173,"UAD setting. The lr warms up from 0 to 2e-3 in the first 100 iterations and cosine anneals to 2e-4
495"
REFERENCES,0.5183397683397684,"throughout the training. The discarding rate in Equation 5 linearly rises from 0% to 90% in the first
496"
REFERENCES,0.5193050193050193,"1,000 iterations as warm-up (500 iters for class-separated setting). The anomaly map is obtained by
497"
REFERENCES,0.5202702702702703,"upsampling the point-wise cosine distance between encoder and decoder feature maps (averaging
498"
REFERENCES,0.5212355212355212,"if more than one pair or group). The mean of the top 1% pixels in an anomaly map is used as the
499"
REFERENCES,0.5222007722007722,"image anomaly score. All experiments are conducted with random seed=1 with cuda deterministic
500"
REFERENCES,0.5231660231660231,"for invariable weight initialization and batch order. Codes are implemented with Python 3.8 and
501"
REFERENCES,0.5241312741312741,"PyTorch 1.12.0 cuda 11.3, and run on NVIDIA GeForce RTX3090 GPUs (24GB).
502"
REFERENCES,0.525096525096525,"A.3
Additional Ablation Studies and Experiments
503"
REFERENCES,0.5260617760617761,"Ablations on VisA. Similar to Table 3 that conduct ablation experiments on MVTec-AD, we
504"
REFERENCES,0.527027027027027,"additionally run them on VisA for further validations. As shown in Table A1, proposed components
505"
REFERENCES,0.527992277992278,"of Dinomaly contribute to the AD performances on VisA as on MVTec-AD.
506"
REFERENCES,0.528957528957529,"Table A1: Ablations of Dinomaly elements on VisA (%). NB: Noisy Bottleneck. LA: Linear
Attention. LC: Loosen Constraint (2 groups). LL: Loosen Loss."
REFERENCES,0.5299227799227799,"NB
LA
LC
LL
Image-level
Pixel-level"
REFERENCES,0.5308880308880309,"AUROC
AP
F1-max
AUROC
AP
F1-max
AUPRO"
REFERENCES,0.5318532818532818,"95.81
96.35
92.06
97.97
47.88
52.55
93.43
✓
97.38
97.74
94.07
97.84
50.42
54.57
93.71
✓
95.74
96.23
91.87
98.01
47.89
52.58
93.34
✓
96.39
97.01
92.54
97.37
46.80
51.66
92.75
✓
96.93
97.26
93.32
98.37
49.52
53.59
94.11
✓
✓
97.52
97.75
94.33
98.06
51.49
55.09
93.75
✓
✓
98.06
98.37
95.18
98.21
51.43
54.89
93.94
✓
✓
✓
98.57
98.77
95.75
98.57
52.29
55.38
94.28
✓
✓
✓
98.22
98.43
95.27
98.51
53.11
55.48
94.24
✓
✓
✓
✓
98.73
98.87
96.18
98.74
53.23
55.69
94.50"
REFERENCES,0.5328185328185329,"Scalability. Previous works [3; 2; 17] reported that AD methods do not follow the model ""scaling
507"
REFERENCES,0.5337837837837838,"law"", i.e., larger models do not necessarily produce better performance. For example, RD4AD [2]
508"
REFERENCES,0.5347490347490348,"found WideResNet50 better than WideResNet101 as the encoder backbone. ViTAD [17] found ViT-
509"
REFERENCES,0.5357142857142857,"Small better than ViT-Base. We conduct experiments to probe the influence of the scale of backbone
510"
REFERENCES,0.5366795366795367,"Transformers in Dinomaly. ViT-Small, ViT-Base (default), and ViT-Large pre-trained by DINOv2-R
511"
REFERENCES,0.5376447876447876,"are used as the encoder, respectively. ViT-Small has 12 layers, so we take the [3,4,5,...10]th layer as
512"
REFERENCES,0.5386100386100386,"the interested 8 middle layers, which is the same as default ViT-Base. ViT-Large has 24 layers, so
513"
REFERENCES,0.5395752895752896,"we take the [5,7,9,...19]th layer as the interested 8 middle layers. The layer hyperparameters of the
514"
REFERENCES,0.5405405405405406,"decoder, such as embedding dimension and numbers of attention heads, follow the hyperparameters
515"
REFERENCES,0.5415057915057915,"of the corresponding encoder. Other training strategies are identical to default. As shown in Table A2,
516"
REFERENCES,0.5424710424710425,"the MUAD performance of Dinomaly follows the ""scaling law"". Dinomaly equipped with ViT-Small
517"
REFERENCES,0.5434362934362934,"already produces state-of-the-art results. ViT-Large further boosts Dinomaly to an unprecedented
518"
REFERENCES,0.5444015444015444,higher record.
REFERENCES,0.5453667953667953,"Table A2: Comparison of different ViT architectures, conducted on MVTec-AD (%). Latency per
image is measured on NVIDIA RTX3090 with batch size=16."
REFERENCES,0.5463320463320464,"Arch.
Parameters
MACs
Latency
Image-level
Pixel-level"
REFERENCES,0.5472972972972973,"AUROC
AP
F1-max
AUROC
AP
F1-max
AUPRO"
REFERENCES,0.5482625482625483,"ViT-Small
37.4M
26.3G
6.8ms
99.26
99.67
98.72
98.07
68.29
67.78
94.36
ViT-Base
148.0M
104.7G
17.2ms
99.60
99.78
99.04
98.35
69.29
69.17
94.79
ViT-Large
275.3M
413.5G
41.3ms
99.77
99.92
99.45
98.54
70.53
70.04
95.09 519"
REFERENCES,0.5492277992277992,"Input Size. The patch size of ViTs (usually 14 × 14 or 16 × 16) is much larger than the stem
520"
REFERENCES,0.5501930501930502,"layer’s down-sampling rate of CNNs (usually 4 × 4), resulting in smaller feature map size. For dense
521"
REFERENCES,0.5511583011583011,"prediction tasks like semantic segmentation, ViTs usually employ a large input image size [30]. This
522"
REFERENCES,0.5521235521235521,"practice holds in anomaly localization as well. In Table A3, we present the results of Dinomaly with
523"
REFERENCES,0.553088803088803,"different input resolutions. Following PatchCore [8], by default, we adopt center-crop preprocessing
524"
REFERENCES,0.5540540540540541,"to reduce the influence of background, which can also cause unreachable anomalies at the edge of
525"
REFERENCES,0.555019305019305,"images. Experimental results demonstrate our robustness to input size. While small image size is
526"
REFERENCES,0.555984555984556,"enough for image-level anomaly detection, larger inputs are beneficial to anomaly localization. All
527"
REFERENCES,0.556949806949807,"experiments evaluate localization performance in a unified size of 256 × 256 for fairness.
528"
REFERENCES,0.5579150579150579,"Table A3: Ablations of input size, conducted on MVTec-AD (%). R4482-C3922 represents first
resizing images to 448×448, then center cropping to 392×392."
REFERENCES,0.5588803088803089,"Image Size
MACs
Image-level
Pixel-level"
REFERENCES,0.5598455598455598,"AUROC
AP
F1-max
AUROC
AP
F1-max
AUPRO"
REFERENCES,0.5608108108108109,"R5122-C4482
136.4G
99.67
99.81
99.12
98.33
69.24
69.47
94.76
R4482
136.4G
99.59
99.77
99.19
98.57
68.09
68.58
95.60
R4482-C3922
104.7G
99.60
99.78
99.04
98.35
69.29
69.17
94.79
R3922
104.7G
99.48
99.74
99.04
98.47
67.02
67.86
95.34
R3842-C3362
77.1G
99.61
99.78
99.22
98.27
67.22
67.77
94.24
R3362
77.1G
99.63
99.84
99.23
98.48
65.46
66.60
95.10
R3202-C2802
53.7G
99.62
99.81
99.07
98.21
65.21
66.34
93.57
R2802
53.7G
99.46
99.75
99.27
98.40
63.28
64.79
94.47"
REFERENCES,0.5617760617760618,"Pre-Trained Foundations. The representation quality of the frozen backbone Transformer is of
529"
REFERENCES,0.5627413127413128,"great significance to unsupervised anomaly detection. We conduct extensive experiments to probe
530"
REFERENCES,0.5637065637065637,"the impact of different pre-training methods, including supervised learning and self-supervised
531"
REFERENCES,0.5646718146718147,"learning. DeiT [59] is trained on ImageNet[1] in a supervised manner by distilling CNNs. MAE [33],
532"
REFERENCES,0.5656370656370656,"BEiTv2 [35], and D-iGPT [60] are based on masked image modeling (MIM). Given input images
533"
REFERENCES,0.5666023166023166,"with masked patches, MAE [33] is optimized to restore raw pixels; BEiTv2 [35] is trained to predict
534"
REFERENCES,0.5675675675675675,"the token index of VQ-GAN and CLIP; D-iGPT [60] is trained to predict the features of CLIP
535"
REFERENCES,0.5685328185328186,"model. DINO [29] is based on positive-pair contrastive learning (CL), which is also referred to as
536"
REFERENCES,0.5694980694980695,"self-distillation. It trains the network to produce similar feature representations given two views
537"
REFERENCES,0.5704633204633205,"(augmentations) of the same image. iBot [36] and DINOv2 [30] combine MIM and CL strategies,
538"
REFERENCES,0.5714285714285714,"marking the SoTA of self-supervised foundation models. DINOv2-R [39] is a variation of DINOv2
539"
REFERENCES,0.5723938223938224,"that employs 4 extra register tokens.
540"
REFERENCES,0.5733590733590733,"Table A4: Comparison between pre-trained ViT foundations, conducted on MVTec-AD (%). All
models are ViT-Base. The patch size of DINOv2 and DINOv2-R is 142; others are 162."
REFERENCES,0.5743243243243243,"Pre-Train
Method
Type
Image
Size
Image-level
Pixel-level"
REFERENCES,0.5752895752895753,"AUROC
AP
F1-max
AUROC
AP
F1-max
AUPRO"
REFERENCES,0.5762548262548263,"DeiT[59]
Supervised
R5122-C4482
98.19
99.24
97.64
97.93
68.98
67.91
91.45
MAE[33]
MIM
R5122-C4482
96.27
98.33
95.44
96.96
62.89
63.32
89.85
D-iGPT[60]
MIM
R5122-C4482
98.75
99.24
97.70
98.30
65.77
66.16
92.34
DINO[29]
CL
R5122-C4482
98.97
99.58
98.14
98.52
70.89
69.02
93.48
iBOT[36]
CL+MIM
R5122-C4482
99.22
99.67
98.57
98.60
70.78
69.92
93.33
DINOv2[30]
CL+MIM
R4482-C3962
99.55
99.81
99.13
98.26
68.35
68.79
94.83
DINOv2-R[39]
CL+MIM
R4482-C3962
99.60
99.78
99.04
98.35
69.29
69.17
94.79"
REFERENCES,0.5772200772200772,"DeiT[59]
Supervised
R2562-C2242
97.65
99.05
97.40
97.80
62.58
63.39
89.98
MAE[33]
MIM
R2562-C2242
97.25
98.84
96.94
97.78
63.00
64.01
90.95
BEiTv2[35]
MIM
R2562-C2242
97.70
99.11
97.39
97.61
59.79
62.53
90.10
D-iGPT[60]
MIM
R2562-C2242
99.21
99.66
98.47
98.08
60.05
63.05
91.78
DINO[29]
CL
R2562-C2242
99.20
99.72
98.77
98.16
64.16
65.07
92.02
iBOT[36]
CL+MIM
R2562-C2242
99.31
99.74
98.77
98.25
64.01
65.37
91.68
DINOv2[30]
CL+MIM
R2562-C2242
99.26
99.70
98.60
97.95
62.27
64.39
92.80
DINOv2-R[39]
CL+MIM
R2562-C2242
99.34
99.73
99.03
98.09
63.04
64.48
92.59"
REFERENCES,0.5781853281853282,"It is noted that most models are pre-trained with the image resolution of 224 × 224, except that
541"
REFERENCES,0.5791505791505791,"DINOv2 [30] and DINOv2-R [39] have extra a high-resolution training phase with 518 × 518.
542"
REFERENCES,0.5801158301158301,"However, directly using the pre-trained weights on a different resolution for UAD without fine-tuning
543"
REFERENCES,0.581081081081081,"like other supervised tasks can cause generalization problems. Therefore, by default, we still keep
544"
REFERENCES,0.582046332046332,"the feature size of all compared models to 28 × 28, i.e., the input size is 392 × 392 for ViT-Base/14
545"
REFERENCES,0.583011583011583,"and 448 × 448 for ViT-Base/16. Additionally, we train Dinomaly with the low-resolution input
546"
REFERENCES,0.583976833976834,"size of 224 × 224. The results are presented in Table A4. Generally speaking, CL+MIM combined
547"
REFERENCES,0.584942084942085,"models outperform MIM and CL models. In addition, MIM-based models do not benefit from
548"
REFERENCES,0.5859073359073359,"higher resolutions but suffer from them, indicating the lack of generalization on a different input
549"
REFERENCES,0.5868725868725869,"size. Methods involving CL can better adapt to a higher resolution as they optimize the global
550"
REFERENCES,0.5878378378378378,"representation of class tokens in pre-training, which is insensitive to input size. As expected, DINOv2
551"
REFERENCES,0.5888030888030888,"and DINOv2-R pre-trained on larger inputs can better benefit from higher resolution in Dinomaly.
552"
REFERENCES,0.5897683397683398,"Because some methods, i.e., D-iGPT, DINO, and iBOT, produce similar results to DINOv2 in
553"
REFERENCES,0.5907335907335908,"224 × 224, we expect that they also have the potential to be as powerful in Dinomaly if they are
554"
REFERENCES,0.5916988416988417,"pre-trained in high-resolution.
555"
REFERENCES,0.5926640926640927,"Attention vs. Convolution. Previous works and this paper have proposed to leverage attentions
556"
REFERENCES,0.5936293436293436,"instead of convolutions in UAD. Here, we conduct experiments substituting the attention in the
557"
REFERENCES,0.5945945945945946,"decoder of Dinomaly by convolutions as the spatial mixers. Following MetaFormer [61], we employ
558"
REFERENCES,0.5955598455598455,"Inverted Bottleneck block that consists of 1 × 1 conv, GELU activation, N × N deep-wise conv,
559"
REFERENCES,0.5965250965250966,"and 1 × 1 conv, sequentially. The results are shown in Table A5, where Attentions outperform
560"
REFERENCES,0.5974903474903475,"Convolutions, especially for pixel-level anomaly localization. In addition, utilizing convolutions in
561"
REFERENCES,0.5984555984555985,"the decoder can still yield SoTA results, demonstrating the universality of the proposed Dinomaly.
562"
REFERENCES,0.5994208494208494,"Neighbour-Masking. Prior method [3] proposed to mask the keys and values in an n × n square
563"
REFERENCES,0.6003861003861004,"centered at each query, in order to alleviate identity mapping in Attention. This mechanism can also
564"
REFERENCES,0.6013513513513513,"be applied to Linear Attention as well. As shown in Table A5, neighbor-masking can further improve
565"
REFERENCES,0.6023166023166023,"Dinomaly with both Softmax Attention and Linear Attention moderately.
566"
REFERENCES,0.6032818532818532,"Table A5: Comparison between Convolutional block, Softmax Attention, and Linear Attention as the
spatial mixer of decoder, conducted on MVTec-AD (%)."
REFERENCES,0.6042471042471043,"Spatial Mixer
Image-level
Pixel-level"
REFERENCES,0.6052123552123552,"AUROC
AP
F1-max
AUROC
AP
F1-max
AUPRO"
REFERENCES,0.6061776061776062,"ConvBlock 3 × 3
99.45
99.63
98.64
98.05
65.35
68.07
94.17
ConvBlock 5 × 5
99.41
99.62
98.86
97.99
66.64
67.47
94.24
ConvBlock 7 × 7
99.42
99.65
98.86
98.01
67.57
67.94
94.45"
REFERENCES,0.6071428571428571,"Softmax Attention
99.52
99.73
98.92
98.20
68.25
68.34
94.17
Softmax Attention w/ Neighbour-Mask n = 1
99.51
99.71
98.90
98.17
67.86
67.92
94.27
Softmax Attention w/ Neighbour-Mask n = 3
99.56
99.76
99.05
98.28
69.26
68.17
94.50"
REFERENCES,0.6081081081081081,"Linear Attention
99.60
99.78
99.04
98.35
69.29
69.17
94.79
Linear Attention w/ Neighbour-Mask n = 1
99.60
99.78
99.04
98.32
68.77
68.72
94.75
Linear Attention w/ Neighbour-Mask n = 3
99.60
99.80
99.14
98.38
69.65
69.38
94.70"
REFERENCES,0.609073359073359,"Feature Noise. Prior method [3] proposed to perturb the encoder features by Feature Jitter, i.e. adding
567"
REFERENCES,0.61003861003861,"Gaussian noise with scale to control the noise magnitude. We evaluate the feature jitter strategy in the
568"
REFERENCES,0.611003861003861,"proposed Dinomaly by placing it at the beginning of Noisy Bottleneck. As shown in Table A6, both
569"
REFERENCES,0.611969111969112,"Dropout and Feature Jitter can be a good noise injector in Noisy Bottleneck. Meanwhile, Dropout is
570"
REFERENCES,0.612934362934363,"more robust to the noisy scale hyperparameter, and more elegant without introducing new modules.
571"
REFERENCES,0.6138996138996139,"Random Seeds. Due to limited computation resources, experiments in this paper are conducted for
572"
REFERENCES,0.6148648648648649,"one run with random seed=1. Here, we conduct 5 runs with 5 random seeds on MVTec-AD. As
573"
REFERENCES,0.6158301158301158,"shown in Table A7, Dinomaly is robust to randomness.
574"
REFERENCES,0.6167953667953668,"A.4
Limitation
575"
REFERENCES,0.6177606177606177,"Vision Transformers are known for their high computation cost, which can be a barrier to low-
576"
REFERENCES,0.6187258687258688,"computation scenarios that require inference speed. Future research can be conducted on the effi-
577"
REFERENCES,0.6196911196911197,"ciency of Transformer-based methods, such as distillation, pruning, and hardware-friendly attention
578"
REFERENCES,0.6206563706563707,"mechanism (such as FlashAttention).
579"
REFERENCES,0.6216216216216216,"Table A6: Dropout vs. feature jitter, conducted on MVTec-AD (%)."
REFERENCES,0.6225868725868726,"Noise type
Image-level
Pixel-level"
REFERENCES,0.6235521235521235,"AUROC
AP
F1-max
AUROC
AP
F1-max
AUPRO"
REFERENCES,0.6245173745173745,"No Noise
98.19
99.55
98.51
97.55
63.11
64.39
93.33"
REFERENCES,0.6254826254826255,"Feature Jitter scale=1
99.23
99.54
98.48
97.58
63.22
64.31
93.55
Feature Jitter scale=5
99.24
99.57
98.55
97.84
65.28
65.81
93.75
Feature Jitter scale=10
99.46
99.73
99.12
98.19
67.59
67.80
94.19
Feature Jitter scale=20
99.59
99.79
99.04
98.23
67.93
68.21
94.40"
REFERENCES,0.6264478764478765,"Dropout p=0.1
99.54
99.75
98.90
98.35
69.46
69.19
94.53
Dropout p=0.2
99.60
99.78
99.04
98.35
69.29
69.17
94.79
Dropout p=0.3
99.65
99.83
99.16
98.34
68.46
68.81
94.63
Dropout p=0.4
99.64
99.80
99.23
98.22
67.95
68.33
94.57"
REFERENCES,0.6274131274131274,Table A7: Results of 5 random seeds on MVTec-AD (%).
REFERENCES,0.6283783783783784,"Random Seed
Image-level
Pixel-level"
REFERENCES,0.6293436293436293,"AUROC
AP
F1-max
AUROC
AP
F1-max
AUPRO"
REFERENCES,0.6303088803088803,"seed=1
99.60
99.78
99.04
98.35
69.29
69.17
94.79
seed=2
99.63
99.79
99.12
98.33
68.73
68.91
94.63
seed=3
99.63
99.79
99.16
98.31
68.70
68.93
94.60
seed=4
99.56
99.74
99.02
98.33
69.04
69.09
94.70
seed=5
99.59
99.77
99.02
98.32
68.64
68.47
94.51
mean±std
99.60±0.03
99.77±0.02
99.07±0.06
98.33±0.01
68.88±0.25
68.91±0.24
94.65±0.09"
REFERENCES,0.6312741312741312,"As discussed in section A.1, Dinomaly is used for sensory AD that aims to detect regional anomalies
580"
REFERENCES,0.6322393822393823,"in normal backgrounds. It is not suitable for semantic AD. Previous works have shown that methods
581"
REFERENCES,0.6332046332046332,"designed for sensory AD usually fail to be competitive under semantic AD tasks [3; 2]. Conversely,
582"
REFERENCES,0.6341698841698842,"methods designed for semantic AD do not perform well on sensory AD tasks [62; 37]. Future work
583"
REFERENCES,0.6351351351351351,"can be conducted to unify these two tasks, but according to the ""no free lunch"" theorem, we believe
584"
REFERENCES,0.6361003861003861,"that methods designed for specific anomaly assumption are likely to be more convincing.
585"
REFERENCES,0.637065637065637,"Other special UAD settings, such as zero-shot UAD (vision-language model based) [63], few-shot
586"
REFERENCES,0.638030888030888,"UAD [64], UAD under noisy training set [65], are not included in this work.
587"
REFERENCES,0.638996138996139,"A.5
Results Per-Category
588"
REFERENCES,0.63996138996139,"For future research, we report the per-class results of MVTec-AD [20], VisA [21], and Real-IAD [22].
589"
REFERENCES,0.640926640926641,"The performance of compared methods is drawn from MambaAD [19]. Thanks for their exhaustive
590"
REFERENCES,0.6418918918918919,"reproducing. The results of image-level anomaly detection and pixel-level anomaly localization on
591"
REFERENCES,0.6428571428571429,"MVTec-AD are presented in Table A8 and Table A9, respectively. The results of image-level anomaly
592"
REFERENCES,0.6438223938223938,"detection and pixel-level anomaly localization on VisA are presented in Table A10 and Table A11,
593"
REFERENCES,0.6447876447876448,"respectively. The results of image-level anomaly detection and pixel-level anomaly localization on
594"
REFERENCES,0.6457528957528957,"Real-IAD are presented in Table A12 and Table A13, respectively.
595"
REFERENCES,0.6467181467181468,"A.6
Qualitative Visualization
596"
REFERENCES,0.6476833976833977,"We visualize the output anomaly maps of Dinomaly on MVTec-AD, VisA, and Real-IAD, as shown
597"
REFERENCES,0.6486486486486487,"in Figure A1, Figure A2, and Figure A3. It is noted that all visualized samples are randomly chosen
598"
REFERENCES,0.6496138996138996,"without artificial selection.
599"
REFERENCES,0.6505791505791506,"Table A8: Per-class performance on MVTec-AD dataset for multi-class anomaly detection with
AUROC/AP/F1-max metrics."
REFERENCES,0.6515444015444015,"Method →
RD4AD [2]
UniAD [3]
SimpleNet [13]
DeSTSeg [12]
DiAD [49]
MambaAD [19]
Dinomaly
Category ↓
CVPR’22
NeurlPS’22
CVPR’23
CVPR’23
AAAI’24
Arxiv’24
Ours"
REFERENCES,0.6525096525096525,Objects
REFERENCES,0.6534749034749034,"Bottle
99.6/99.9/98.4
99.7/100./100.
100./100./100.
98.7/99.6/96.8
99.7/96.5/91.8
100./100./100.
100./100./100.
Cable
84.1/89.5/82.5
95.2/95.9/88.0
97.5/98.5/94.7
89.5/94.6/85.9
94.8/98.8/95.2
98.8/99.2/95.7
100./100./100.
Capsule
94.1/96.9/96.9
86.9/97.8/94.4
90.7/97.9/93.5
82.8/95.9/92.6
89.0/97.5/95.5
94.4/98.7/94.9
97.9/99.5/97.7
Hazelnut
60.8/69.8/86.4
99.8/100./99.3
99.9/99.9/99.3
98.8/99.2/98.6
99.5/99.7/97.3
100./100./100.
100./100./100.
Metal Nut
100./100./99.5
99.2/99.9/99.5
96.9/99.3/96.1
92.9/98.4/92.2
99.1/96.0/91.6
99.9/100./99.5
100./100./100.
Pill
97.5/99.6/96.8
93.7/98.7/95.7
88.2/97.7/92.5
77.1/94.4/91.7
95.7/98.5/94.5
97.0/99.5/96.2
99.1/99.9/98.3
Screw
97.7/99.3/95.8
87.5/96.5/89.0
76.7/90.6/87.7
69.9/88.4/85.4
90.7/99.7/97.9
94.7/97.9/94.0
98.4/99.5/96.1
Toothbrush
97.2/99.0/94.7
94.2/97.4/95.2
89.7/95.7/92.3
71.7/89.3/84.5
99.7/99.9/99.2
98.3/99.3/98.4
100./100./100.
Transistor
94.2/95.2/90.0
99.8/98.0/93.8
99.2/98.7/97.6
78.2/79.5/68.8
99.8/99.6/97.4
100./100./100.
99.0/98.0/96.4
Zipper
99.5/99.9/99.2
95.8/99.5/97.1
99.0/99.7/98.3
88.4/96.3/93.1
95.1/99.1/94.4
99.3/99.8/97.5
100./100./100."
REFERENCES,0.6544401544401545,Textures
REFERENCES,0.6554054054054054,"Carpet
98.5/99.6/97.2
99.8/99.9/99.4
95.7/98.7/93.2
95.9/98.8/94.9
99.4/99.9/98.3
99.8/99.9/99.4
99.8/100./98.9
Grid
98.0/99.4/96.5
98.2/99.5/97.3
97.6/99.2/96.4
97.9/99.2/96.6
98.5/99.8/97.7
100./100./100.
99.9/100./99.1
Leather
100./100./100.
100./100./100.
100./100./100.
99.2/99.8/98.9
99.8/99.7/97.6
100./100./100.
100./100./100.
Tile
98.3/99.3/96.4
99.3/99.8/98.2
99.3/99.8/98.8
97.0/98.9/95.3
96.8/99.9/98.4
98.2/99.3/95.4
100./100./100.
Wood
99.2/99.8/98.3
98.6/99.6/96.6
98.4/99.5/96.7
99.9/100./99.2
99.7/100./100.
98.8/99.6/96.6
99.8/99.9/99.2
Mean
94.6/96.5/95.2
96.5/98.8/96.2
95.3/98.4/95.8
89.2/95.5/91.6
97.2/99.0/96.5
98.6/99.6/97.8
99.6/99.8/99.0"
REFERENCES,0.6563706563706564,"Table A9: Per-class performance on MVTec-AD dataset for multi-class anomaly localization with
AUROC/AP/F1-max/AUPRO metrics."
REFERENCES,0.6573359073359073,"Method →
RD4AD [2]
UniAD [3]
SimpleNet [13]
DeSTSeg [12]
DiAD [49]
MambaAD [19]
Dinomaly
Category ↓
CVPR’22
NeurlPS’22
CVPR’23
CVPR’23
AAAI’24
Arxiv’24
Ours"
REFERENCES,0.6583011583011583,Objects
REFERENCES,0.6592664092664092,"Bottle
97.8/68.2/67.6/94.0
98.1/66.0/69.2/93.1
97.2/53.8/62.4/89.0
93.3/61.7/56.0/67.5
98.4/52.2/54.8/86.6
98.8/79.7/76.7/95.2
99.2/88.6/84.2/96.6
Cable
85.1/26.3/33.6/75.1
97.3/39.9/45.2/86.1
96.7/42.4/51.2/85.4
89.3/37.5/40.5/49.4
96.8/50.1/57.8/80.5
95.8/42.2/48.1/90.3
98.6/72.0/74.3/94.2
Capsule
98.8/43.4/50.0/94.8
98.5/42.7/46.5/92.1
98.5/35.4/44.3/84.5
95.8/47.9/48.9/62.1
97.1/42.0/45.3/87.2
98.4/43.9/47.7/92.6
98.7/61.4/60.3/97.2
Hazelnut
97.9/36.2/51.6/92.7
98.1/55.2/56.8/94.1
98.4/44.6/51.4/87.4
98.2/65.8/61.6/84.5
98.3/79.2/80.4/91.5
99.0/63.6/64.4/95.7
99.4/82.2/76.4/97.0
Metal Nut
94.8/55.5/66.4/91.9
62.7/14.6/29.2/81.8
98.0/83.1/79.4/85.2
84.2/42.0/22.8/53.0
97.3/30.0/38.3/90.6
96.7/74.5/79.1/93.7
96.9/78.6/86.7/94.9
Pill
97.5/63.4/65.2/95.8
95.0/44.0/53.9/95.3
96.5/72.4/67.7/81.9
96.2/61.7/41.8/27.9
95.7/46.0/51.4/89.0
97.4/64.0/66.5/95.7
97.8/76.4/71.6/97.3
Screw
99.4/40.2/44.6/96.8
98.3/28.7/37.6/95.2
96.5/15.9/23.2/84.0
93.8/19.9/25.3/47.3
97.9/60.6/59.6/95.0
99.5/49.8/50.9/97.1
99.6/60.2/59.6/98.3
Toothbrush
99.0/53.6/58.8/92.0
98.4/34.9/45.7/87.9
98.4/46.9/52.5/87.4
96.2/52.9/58.8/30.9
99.0/78.7/72.8/95.0
99.0/48.5/59.2/91.7
98.9/51.5/62.6/95.3
Transistor
85.9/42.3/45.2/74.7
97.9/59.5/64.6/93.5
95.8/58.2/56.0/83.2
73.6/38.4/39.2/43.9
95.1/15.6/31.7/90.0
96.5/69.4/67.1/87.0
93.2/59.9/58.5/77.0
Zipper
98.5/53.9/60.3/94.1
96.8/40.1/49.9/92.6
97.9/53.4/54.6/90.7
97.3/64.7/59.2/66.9
96.2/60.7/60.0/91.6
98.4/60.4/61.7/94.3
99.2/79.5/75.4/97.2"
REFERENCES,0.6602316602316602,Textures
REFERENCES,0.6611969111969112,"Carpet
99.0/58.5/60.4/95.1
98.5/49.9/51.1/94.4
97.4/38.7/43.2/90.6
93.6/59.9/58.9/89.3
98.6/42.2/46.4/90.6
99.2/60.0/63.3/96.7
99.3/68.7/71.1/97.6
Grid
96.5/23.0/28.4/97.0
63.1/10.7/11.9/92.9
96.8/20.5/27.6/88.6/
97.0/42.1/46.9/86.8
96.6/66.0/64.1/94.0
99.2/47.4/47.7/97.0
99.4/55.3/57.7/97.2
Leather
99.3/38.0/45.1/97.4
98.8/32.9/34.4/96.8
98.7/28.5/32.9/92.7
99.5/71.5/66.5/91.1
98.8/56.1/62.3/91.3
99.4/50.3/53.3/98.7
99.4/52.2/55.0/97.6
Tile
95.3/48.5/60.5/85.8
91.8/42.1/50.6/78.4
95.7/60.5/59.9/90.6
93.0/71.0/66.2/87.1
92.4/65.7/64.1/90.7
93.8/45.1/54.8/80.0
98.1/80.1/75.7/90.5
Wood
95.3/47.8/51.0/90.0
93.2/37.2/41.5/86.7
91.4/34.8/39.7/76.3
95.9/77.3/71.3/83.4
93.3/43.3/43.5/97.5
94.4/46.2/48.2/91.2
97.6/72.8/68.4/94.0
Mean
96.1/48.6/53.8/91.1
96.8/43.4/49.5/90.7
96.9/45.9/49.7/86.5
93.1/54.3/50.9/64.8
96.8/52.6/55.5/90.7
97.7/56.3/59.2/93.1
98.4/69.3/69.2/94.8"
REFERENCES,0.6621621621621622,"Table A10:
Per-class performance on VisA dataset for multi-class anomaly detection with
AUROC/AP/F1-max metrics."
REFERENCES,0.6631274131274131,"Method →
RD4AD [2]
UniAD [3]
SimpleNet [13]
DeSTSeg [12]
DiAD [49]
MambaAD
Dinomaly
Category ↓
CVPR’22
NeurlPS’22
CVPR’23
CVPR’23
AAAI’24
Arxiv’24
Ours
pcb1
96.2/95.5/91.9
92.8/92.7/87.8
91.6/91.9/86.0
87.6/83.1/83.7
88.1/88.7/80.7
95.4/93.0/91.6
99.1/99.1/96.6
pcb2
97.8/97.8/94.2
87.8/87.7/83.1
92.4/93.3/84.5
86.5/85.8/82.6
91.4/91.4/84.7
94.2/93.7/89.3
99.3/99.2/97.0
pcb3
96.4/96.2/91.0
78.6/78.6/76.1
89.1/91.1/82.6
93.7/95.1/87.0
86.2/87.6/77.6
93.7/94.1/86.7
98.9/98.9/96.1
pcb4
99.9/99.9/99.0
98.8/98.8/94.3
97.0/97.0/93.5
97.8/97.8/92.7
99.6/99.5/97.0
99.9/99.9/98.5
99.8/99.8/98.0
macaroni1
75.9/ 1.5/76.8
79.9/79.8/72.7
85.9/82.5/73.1
76.6/69.0/71.0
85.7/85.2/78.8
91.6/89.8/81.6
98.0/97.6/94.2
macaroni2
88.3/84.5/83.8
71.6/71.6/69.9
68.3/54.3/59.7
68.9/62.1/67.7
62.5/57.4/69.6
81.6/78.0/73.8
95.9/95.7/90.7
capsules
82.2/90.4/81.3
55.6/55.6/76.9
74.1/82.8/74.6
87.1/93.0/84.2
58.2/69.0/78.5
91.8/95.0/88.8
98.6/99.0/97.1
candle
92.3/92.9/86.0
94.1/94.0/86.1
84.1/73.3/76.6
94.9/94.8/89.2
92.8/92.0/87.6
96.8/96.9/90.1
98.7/98.8/95.1
cashew
92.0/95.8/90.7
92.8/92.8/91.4
88.0/91.3/84.7
92.0/96.1/88.1
91.5/95.7/89.7
94.5/97.3/91.1
98.7/99.4/97.0
chewinggum
94.9/97.5/92.1
96.3/96.2/95.2
96.4/98.2/93.8
95.8/98.3/94.7
99.1/99.5/95.9
97.7/98.9/94.2
99.8/99.9/99.0
fryum
95.3/97.9/91.5
83.0/83.0/85.0
88.4/93.0/83.3
92.1/96.1/89.5
89.8/95.0/87.2
95.2/97.7/90.5
98.8/99.4/96.5
pipe_fryum
97.9/98.9/96.5
94.7/94.7/93.9
90.8/95.5/88.6
94.1/97.1/91.9
96.2/98.1/93.7
98.7/99.3/97.0
99.2/99.7/97.0
Mean
92.4/92.4/89.6
85.5/85.5/84.4
87.2/87.0/81.8
88.9/89.0/85.2
86.8/88.3/85.1
94.3/94.5/89.4
98.7/98.9/96.2"
REFERENCES,0.6640926640926641,"Table A11: Per-class performance on VisA dataset for multi-class anomaly localization with
AUROC/AP/F1-max/AUPRO metrics."
REFERENCES,0.665057915057915,"Method →
RD4AD [2]
UniAD [3]
SimpleNet [13]
DeSTSeg [12]
DiAD [49]
MambaAD
Dinomaly
Category ↓
CVPR’22
NeurlPS’22
CVPR’23
CVPR’23
AAAI’24
Arxiv’24
Ours"
REFERENCES,0.666023166023166,"pcb1
99.4/66.2/62.4/95.8
93.3/ 3.9/ 8.3/64.1
99.2/86.1/78.8/83.6
95.8/46.4/49.0/83.2
98.7/49.6/52.8/80.2
99.8/77.1/72.4/92.8
99.5/87.9/80.5/95.1
pcb2
98.0/22.3/30.0/90.8
93.9/ 4.2/ 9.2/66.9
96.6/ 8.9/18.6/85.7
97.3/14.6/28.2/79.9
95.2/ 7.5/16.7/67.0
98.9/13.3/23.4/89.6
98.0/47.0/49.8/91.3
pcb3
97.9/26.2/35.2/93.9
97.3/13.8/21.9/70.6
97.2/31.0/36.1/85.1
97.7/28.1/33.4/62.4
96.7/ 8.0/18.8/68.9
99.1/18.3/27.4/89.1
98.4/41.7/45.3/94.6
pcb4
97.8/31.4/37.0/88.7
94.9/14.7/22.9/72.3
93.9/23.9/32.9/61.1
95.8/53.0/53.2/76.9
97.0/17.6/27.2/85.0
98.6/47.0/46.9/87.6
98.7/50.5/53.1/94.4"
REFERENCES,0.666988416988417,"macaroni1
99.4/ 2.9/6.9/95.3
97.4/ 3.7/ 9.7/84.0
98.9/ 3.5/8.4/92.0
99.1/ 5.8/13.4/62.4
94.1/10.2/16.7/68.5
99.5/17.5/27.6/95.2
99.6/33.5/40.6/96.4
macaroni2
99.7/13.2/21.8/97.4
95.2/ 0.9/ 4.3/76.6
93.2/ 0.6/ 3.9/77.8
98.5/ 6.3/14.4/70.0
93.6/ 0.9/ 2.8/73.1
99.5/ 9.2/16.1/96.2
99.7/24.7/36.1/98.7
capsules
99.4/60.4/60.8/93.1
88.7/ 3.0/ 7.4/43.7
97.1/52.9/53.3/73.7
96.9/33.2/ 9.1/76.7
97.3/10.0/21.0/77.9
99.1/61.3/59.8/91.8
99.6/65.0/66.6/97.4
candle
99.1/25.3/35.8/94.9
98.5/17.6/27.9/91.6
97.6/ 8.4/16.5/87.6
98.7/39.9/45.8/69.0
97.3/12.8/22.8/89.4
99.0/23.2/32.4/95.5
99.4/43.0/47.9/95.4"
REFERENCES,0.667953667953668,"cashew
91.7/44.2/49.7/86.2
98.6/51.7/58.3/87.9
98.9/68.9/66.0/84.1
87.9/47.6/52.1/66.3
90.9/53.1/60.9/61.8
94.3/46.8/51.4/87.8
97.1/64.5/62.4/94.0
chewinggum
98.7/59.9/61.7/76.9
98.8/54.9/56.1/81.3
97.9/26.8/29.8/78.3
98.8/86.9/81.0/68.3
94.7/11.9/25.8/59.5
98.1/57.5/59.9/79.7
99.1/65.0/67.7/88.1
fryum
97.0/47.6/51.5/93.4
95.9/34.0/40.6/76.2
93.0/39.1/45.4/85.1
88.1/35.2/38.5/47.7
97.6/58.6/60.1/81.3
96.9/47.8/51.9/91.6
96.6/51.6/53.4/93.5
pipe_fryum
99.1/56.8/58.8/95.4
98.9/50.2/57.7/91.5
98.5/65.6/63.4/83.0
98.9/78.8/72.7/45.9
99.4/72.7/69.9/89.9
99.1/53.5/58.5/95.1
99.2/64.3/65.1/95.2"
REFERENCES,0.668918918918919,"Mean
98.1/38.0/42.6/91.8
95.9/21.0/27.0/75.6
96.8/34.7/37.8/81.4
96.1/39.6/43.4/67.4
96.0/26.1/33.0/75.2
98.5/39.4/44.0/91.0
98.7/53.2/55.7/94.5"
REFERENCES,0.6698841698841699,"Table A12: Per-class performance on Real-IAD dataset for multi-class anomaly detection with
AUROC/AP/F1-max metrics."
REFERENCES,0.6708494208494209,"Method →
RD4AD [2]
UniAD [3]
SimpleNet [13]
DeSTSeg [12]
DiAD [49]
MambaAD
Dinomaly
Category ↓
CVPR’22
NeurlPS’22
CVPR’23
CVPR’23
AAAI’24
Arxiv’24
Ours
audiojack
76.2/63.2/60.8
81.4/76.6/64.9
58.4/44.2/50.9
81.1/72.6/64.5
76.5/54.3/65.7
84.2/76.5/67.4
86.8/82.4/72.2
bottle cap
89.5/86.3/81.0
92.5/91.7/81.7
54.1/47.6/60.3
78.1/74.6/68.1
91.6/94.0/87.9
92.8/92.0/82.1
89.9/86.7/81.2
button battery
73.3/78.9/76.1
75.9/81.6/76.3
52.5/60.5/72.4
86.7/89.2/83.5
80.5/71.3/70.6
79.8/85.3/77.8
86.6/88.9/82.1
end cap
79.8/84.0/77.8
80.9/86.1/78.0
51.6/60.8/72.9
77.9/81.1/77.1
85.1/83.4/84.8
78.0/82.8/77.2
87.0/87.5/83.4
eraser
90.0/88.7/79.7
90.3/89.2/80.2
46.4/39.1/55.8
84.6/82.9/71.8
80.0/80.0/77.3
87.5/86.2/76.1
90.3/87.6/78.6
fire hood
78.3/70.1/64.5
80.6/74.8/66.4
58.1/41.9/54.4
81.7/72.4/67.7
83.3/81.7/80.5
79.3/72.5/64.8
83.8/76.2/69.5
mint
65.8/63.1/64.8
67.0/66.6/64.6
52.4/50.3/63.7
58.4/55.8/63.7
76.7/76.7/76.0
70.1/70.8/65.5
73.1/72.0/67.7
mounts
88.6/79.9/74.8
87.6/77.3/77.2
58.7/48.1/52.4
74.7/56.5/63.1
75.3/74.5/82.5
86.8/78.0/73.5
90.4/84.2/78.0
pcb
79.5/85.8/79.7
81.0/88.2/79.1
54.5/66.0/75.5
82.0/88.7/79.6
86.0/85.1/85.4
89.1/93.7/84.0
92.0/95.3/87.0
phone battery
87.5/83.3/77.1
83.6/80.0/71.6
51.6/43.8/58.0
83.3/81.8/72.1
82.3/77.7/75.9
90.2/88.9/80.5
92.9/91.6/82.5
plastic nut
80.3/68.0/64.4
80.0/69.2/63.7
59.2/40.3/51.8
83.1/75.4/66.5
71.9/58.2/65.6
87.1/80.7/70.7
88.3/81.8/74.7
plastic plug
81.9/74.3/68.8
81.4/75.9/67.6
48.2/38.4/54.6
71.7/63.1/60.0
88.7/89.2/90.9
85.7/82.2/72.6
90.5/86.4/78.6
porcelain doll
86.3/76.3/71.5
85.1/75.2/69.3
66.3/54.5/52.1
78.7/66.2/64.3
72.6/66.8/65.2
88.0/82.2/74.1
85.1/73.3/69.6
regulator
66.9/48.8/47.7
56.9/41.5/44.5
50.5/29.0/43.9
79.2/63.5/56.9
72.1/71.4/78.2
69.7/58.7/50.4
85.2/78.9/69.8
rolled strip base
97.5/98.7/94.7
98.7/99.3/96.5
59.0/75.7/79.8
96.5/98.2/93.0
68.4/55.9/56.8
98.0/99.0/95.0
99.2/99.6/97.1
sim card set
91.6/91.8/84.8
89.7/90.3/83.2
63.1/69.7/70.8
95.5/96.2/89.2
72.6/53.7/61.5
94.4/95.1/87.2
95.8/96.3/88.8
switch
84.3/87.2/77.9
85.5/88.6/78.4
62.2/66.8/68.6
90.1/92.8/83.1
73.4/49.4/61.2
91.7/94.0/85.4
97.8/98.1/93.3
tape
96.0/95.1/87.6
97.2/96.2/89.4
49.9/41.1/54.5
94.5/93.4/85.9
73.9/57.8/66.1
96.8/95.9/89.3
96.9/95.0/88.8
terminalblock
89.4/89.7/83.1
87.5/89.1/81.0
59.8/64.7/68.8
83.1/86.2/76.6
62.1/36.4/47.8
96.1/96.8/90.0
96.7/97.4/91.1
toothbrush
82.0/83.8/77.2
78.4/80.1/75.6
65.9/70.0/70.1
83.7/85.3/79.0
91.2/93.7/90.9
85.1/86.2/80.3
90.4/91.9/83.4
toy
69.4/74.2/75.9
68.4/75.1/74.8
57.8/64.4/73.4
70.3/74.8/75.4
66.2/57.3/59.8
83.0/87.5/79.6
85.6/89.1/81.9
toy brick
63.6/56.1/59.0
77.0/71.1/66.2
58.3/49.7/58.2
73.2/68.7/63.3
68.4/45.3/55.9
70.5/63.7/61.6
72.3/65.1/63.4
transistor1
91.0/94.0/85.1
93.7/95.9/88.9
62.2/69.2/72.1
90.2/92.1/84.6
73.1/63.1/62.7
94.4/96.0/89.0
97.4/98.2/93.1
u block
89.5/85.0/74.2
88.8/84.2/75.5
62.4/48.4/51.8
80.1/73.9/64.3
75.2/68.4/67.9
89.7/85.7/75.3
89.9/84.0/75.2
usb
84.9/84.3/75.1
78.7/79.4/69.1
57.0/55.3/62.9
87.8/88.0/78.3
58.9/37.4/45.7
92.0/92.2/84.5
92.0/91.6/83.3
usb adaptor
71.1/61.4/62.2
76.8/71.3/64.9
47.5/38.4/56.5
80.1/74.9/67.4
76.9/60.2/67.2
79.4/76.0/66.3
81.5/74.5/69.4
vcpill
85.1/80.3/72.4
87.1/84.0/74.7
59.0/48.7/56.4
83.8/81.5/69.9
64.1/40.4/56.2
88.3/87.7/77.4
92.0/91.2/82.0
wooden beads
81.2/78.9/70.9
78.4/77.2/67.8
55.1/52.0/60.2
82.4/78.5/73.0
62.1/56.4/65.9
82.5/81.7/71.8
87.3/85.8/77.4
woodstick
76.9/61.2/58.1
80.8/72.6/63.6
58.2/35.6/45.2
80.4/69.2/60.3
74.1/66.0/62.1
80.4/69.0/63.4
84.0/73.3/65.6
zipper
95.3/97.2/91.2
98.2/98.9/95.3
77.2/86.7/77.6
96.9/98.1/93.5
86.0/87.0/84.0
99.2/99.6/96.9
99.1/99.5/96.5"
REFERENCES,0.6718146718146718,"Mean
82.4/79.0/73.9
83.0/80.9/74.3
57.2/53.4/61.5
82.3/79.2/73.2
75.6/66.4/69.9
86.3/84.6/77.0
89.3/86.8/80.2"
REFERENCES,0.6727799227799228,"Table A13: Per-class performance on Real-IAD dataset for multi-class anomaly localization with
AUROC/AP/F1-max/AUPRO metrics."
REFERENCES,0.6737451737451737,"Method →
RD4AD [2]
UniAD [3]
SimpleNet [13]
DeSTSeg [12]
DiAD [49]
MambaAD [19]
Dinomaly
Category ↓
CVPR’22
NeurlPS’22
CVPR’23
CVPR’23
AAAI’24
Arxiv’24
Ours
audiojack
96.6/12.8/22.1/79.6
97.6/20.0/31.0/83.7
74.4/ 0.9/ 4.8/38.0
95.5/25.4/31.9/52.6
91.6/ 1.0/ 3.9/63.3
97.7/21.6/29.5/83.9
98.7/48.1/54.5/91.7
bottle cap
99.5/18.9/29.9/95.7
99.5/19.4/29.6/96.0
85.3/ 2.3/ 5.7/45.1
94.5/25.3/31.1/25.3
94.6/ 4.9/11.4/73.0
99.7/30.6/34.6/97.2
99.7/32.4/36.7/98.1
button battery
97.6/33.8/37.8/86.5
96.7/28.5/34.4/77.5
75.9/ 3.2/ 6.6/40.5
98.3/63.9/60.4/36.9
84.1/ 1.4/ 5.3/66.9
98.1/46.7/49.5/86.2
99.1/46.9/56.7/92.9
end cap
96.7/12.5/22.5/89.2
95.8/ 8.8/17.4/85.4
63.1/ 0.5/ 2.8/25.7
89.6/14.4/22.7/29.5
81.3/ 2.0/ 6.9/38.2
97.0/12.0/19.6/89.4
99.1/26.2/32.9/96.0
eraser
99.5/30.8/36.7/96.0
99.3/24.4/30.9/94.1
80.6/ 2.7/ 7.1/42.8
95.8/52.7/53.9/46.7
91.1/ 7.7/15.4/67.5
99.2/30.2/38.3/93.7
99.5/39.6/43.3/96.4
fire hood
98.9/27.7/35.2/87.9
98.6/23.4/32.2/85.3
70.5/ 0.3/ 2.2/25.3
97.3/27.1/35.3/34.7
91.8/ 3.2/ 9.2/66.7
98.7/25.1/31.3/86.3
99.3/38.4/42.7/93.0
mint
95.0/11.7/23.0/72.3
94.4/ 7.7/18.1/62.3
79.9/ 0.9/ 3.6/43.3
84.1/10.3/22.4/ 9.9
91.1/ 5.7/11.6/64.2
96.5/15.9/27.0/72.6
96.9/22.0/32.5/77.6
mounts
99.3/30.6/37.1/94.9
99.4/28.0/32.8/95.2
80.5/ 2.2/ 6.8/46.1
94.2/30.0/41.3/43.3
84.3/ 0.4/ 1.1/48.8
99.2/31.4/35.4/93.5
99.4/39.9/44.3/95.6
pcb
97.5/15.8/24.3/88.3
97.0/18.5/28.1/81.6
78.0/ 1.4/ 4.3/41.3
97.2/37.1/40.4/48.8
92.0/ 3.7/ 7.4/66.5
99.2/46.3/50.4/93.1
99.3/55.0/56.3/95.7
phone battery
77.3/22.6/31.7/94.5
85.5/11.2/21.6/88.5
43.4/ 0.1/ 0.9/11.8
79.5/25.6/33.8/39.5
96.8/ 5.3/11.4/85.4
99.4/36.3/41.3/95.3
99.7/51.6/54.2/96.8
phone battery
77.3/22.6/31.7/94.5
85.5/11.2/21.6/88.5
43.4/ 0.1/ 0.9/11.8
79.5/25.6/33.8/39.5
96.8/5.3/11.4/85.4
99.4/36.3/41.3/95.3
99.7/51.6/54.2/96.8
plastic nut
98.8/21.1/29.6/91.0
98.4/20.6/27.1/88.9
77.4/ 0.6/ 3.6/41.5
96.5/44.8/45.7/38.4
81.1/ 0.4/ 3.4/38.6
99.4/33.1/37.3/96.1
99.7/41.0/45.0/97.4
plastic plug
99.1/20.5/28.4/94.9
98.6/17.4/26.1/90.3
78.6/ 0.7/ 1.9/38.8
91.9/20.1/27.3/21.0
92.9/ 8.7/15.0/66.1
99.0/24.2/31.7/91.5
99.4/31.7/37.2/96.4
porcelain doll
99.2/24.8/34.6/95.7
98.7/14.1/24.5/93.2
81.8/ 2.0/ 6.4/47.0
93.1/35.9/40.3/24.8
93.1/ 1.4/ 4.8/70.4
99.2/31.3/36.6/95.4
99.3/27.9/33.9/96.0
regulator
98.0/7.8/16.1/88.6
95.5/9.1/17.4/76.1
76.6/0.1/0.6/38.1
88.8/18.9/23.6/17.5
84.2/0.4/1.5/44.4
97.6/20.6/29.8/87.0
99.3/42.2/48.9/95.6
rolled strip base
99.7/31.4/39.9/98.4
99.6/20.7/32.2/97.8
80.5/ 1.7/ 5.1/52.1
99.2/48.7/50.1/55.5
87.7/ 0.6/ 3.2/63.4
99.7/37.4/42.5/98.8
99.7/41.6/45.5/98.5
sim card set
98.5/40.2/44.2/89.5
97.9/31.6/39.8/85.0
71.0/ 6.8/14.3/30.8
99.1/65.5/62.1/73.9
89.9/ 1.7/ 5.8/60.4
98.8/51.1/50.6/89.4
99.0/52.1/52.9/90.9
switch
94.4/18.9/26.6/90.9
98.1/33.8/40.6/90.7
71.7/ 3.7/ 9.3/44.2
97.4/57.6/55.6/44.7
90.5/ 1.4/ 5.3/64.2
98.2/39.9/45.4/92.9
96.7/62.3/63.6/95.9
tape
99.7/42.4/47.8/98.4
99.7/29.2/36.9/97.5
77.5/ 1.2/ 3.9/41.4
99.0/61.7/57.6/48.2
81.7/ 0.4/ 2.7/47.3
99.8/47.1/48.2/98.0
99.8/54.0/55.8/98.8
terminalblock
99.5/27.4/35.8/97.6
99.2/23.1/30.5/94.4
87.0/ 0.8/ 3.6/54.8
96.6/40.6/44.1/34.8
75.5/ 0.1/ 1.1/38.5
99.8/35.3/39.7/98.2
99.8/48.0/50.7/98.8
toothbrush
96.9/26.1/34.2/88.7
95.7/16.4/25.3/84.3
84.7/ 7.2/14.8/52.6
94.3/30.0/37.3/42.8
82.0/ 1.9/ 6.6/54.5
97.5/27.8/36.7/91.4
96.9/38.3/43.9/90.4
toy
95.2/ 5.1/12.8/82.3
93.4/ 4.6/12.4/70.5
67.7/ 0.1/ 0.4/25.0
86.3/ 8.1/15.9/16.4
82.1/ 1.1/ 4.2/50.3
96.0/16.4/25.8/86.3
94.9/22.5/32.1/91.0
toy brick
96.4/16.0/24.6/75.3
97.4/17.1/27.6/81.3
86.5/ 5.2/11.1/56.3
94.7/24.6/30.8/45.5
93.5/ 3.1/ 8.1/66.4
96.6/18.0/25.8/74.7
96.8/27.9/34.0/76.6
transistor1
99.1/29.6/35.5/95.1
98.9/25.6/33.2/94.3
71.7/ 5.1/11.3/35.3
97.3/43.8/44.5/45.4
88.6/ 7.2/15.3/58.1
99.4/39.4/40.0/96.5
99.6/53.5/53.3/97.8
u block
99.6/40.5/45.2/96.9
99.3/22.3/29.6/94.3
76.2/ 4.8/12.2/34.0
96.9/57.1/55.7/38.5
88.8/ 1.6/ 5.4/54.2
99.5/37.8/46.1/95.4
99.5/41.8/45.6/96.8
usb
98.1/26.4/35.2/91.0
97.9/20.6/31.7/85.3
81.1/ 1.5/ 4.9/52.4
98.4/42.2/47.7/57.1
78.0/ 1.0/ 3.1/28.0
99.2/39.1/44.4/95.2
99.2/45.0/48.7/97.5
usb adaptor
94.5/ 9.8/17.9/73.1
96.6/10.5/19.0/78.4
67.9/ 0.2/ 1.3/28.9
94.9/25.5/34.9/36.4
94.0/ 2.3/ 6.6/75.5
97.3/15.3/22.6/82.5
98.7/23.7/32.7/91.0
vcpill
98.3/43.1/48.6/88.7
99.1/40.7/43.0/91.3
68.2/ 1.1/ 3.3/22.0
97.1/64.7/62.3/42.3
90.2/ 1.3/ 5.2/60.8
98.7/50.2/54.5/89.3
99.1/66.4/66.7/93.7
wooden beads
98.0/27.1/34.7/85.7
97.6/16.5/23.6/84.6
68.1/ 2.4/ 6.0/28.3
94.7/38.9/42.9/39.4
85.0/ 1.1/ 4.7/45.6
98.0/32.6/39.8/84.5
99.1/45.8/50.1/90.5
woodstick
97.8/30.7/38.4/85.0
94.0/36.2/44.3/77.2
76.1/ 1.4/ 6.0/32.0
97.9/60.3/60.0/51.0
90.9/ 2.6/ 8.0/60.7
97.7/40.1/44.9/82.7
99.0/50.9/52.1/90.4
zipper
99.1/44.7/50.2/96.3
98.4/32.5/36.1/95.1
89.9/23.3/31.2/55.5
98.2/35.3/39.0/78.5
90.2/12.5/18.8/53.5
99.3/58.2/61.3/97.6
99.3/67.2/66.5/97.8
Mean
97.3/25.0/32.7/89.6
97.3/21.1/29.2/86.7
75.7/ 2.8/ 6.5/39.0
94.6/37.9/41.7/40.6
88.0/ 2.9/ 7.1/58.1
98.5/33.0/38.7/90.5
98.8/42.8/47.1/93.9"
REFERENCES,0.6747104247104247,Figure A1: Anomaly maps visualization on MVTec-AD. All samples are randomly chosen.
REFERENCES,0.6756756756756757,Figure A2: Anomaly maps visualization on VisA. All samples are randomly chosen.
REFERENCES,0.6766409266409267,Figure A3: Anomaly maps visualization on Real-IAD. All samples are randomly chosen.
REFERENCES,0.6776061776061776,"NeurIPS Paper Checklist
600"
REFERENCES,0.6785714285714286,"The checklist is designed to encourage best practices for responsible machine learning research,
601"
REFERENCES,0.6795366795366795,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
602"
REFERENCES,0.6805019305019305,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
603"
REFERENCES,0.6814671814671814,"follow the references and precede the (optional) supplemental material. The checklist does NOT
604"
REFERENCES,0.6824324324324325,"count towards the page limit.
605"
REFERENCES,0.6833976833976834,"Please read the checklist guidelines carefully for information on how to answer these questions. For
606"
REFERENCES,0.6843629343629344,"each question in the checklist:
607"
REFERENCES,0.6853281853281853,"• You should answer [Yes] , [No] , or [NA] .
608"
REFERENCES,0.6862934362934363,"• [NA] means either that the question is Not Applicable for that particular paper or the
609"
REFERENCES,0.6872586872586872,"relevant information is Not Available.
610"
REFERENCES,0.6882239382239382,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
611"
REFERENCES,0.6891891891891891,"The checklist answers are an integral part of your paper submission. They are visible to the
612"
REFERENCES,0.6901544401544402,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
613"
REFERENCES,0.6911196911196911,"(after eventual revisions) with the final version of your paper, and its final version will be published
614"
REFERENCES,0.6920849420849421,"with the paper.
615"
REFERENCES,0.693050193050193,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
616"
REFERENCES,0.694015444015444,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
617"
REFERENCES,0.694980694980695,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
618"
REFERENCES,0.6959459459459459,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
619"
REFERENCES,0.696911196911197,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
620"
REFERENCES,0.6978764478764479,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
621"
REFERENCES,0.6988416988416989,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
622"
REFERENCES,0.6998069498069498,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
623"
REFERENCES,0.7007722007722008,"please point to the section(s) where related material for the question can be found.
624"
REFERENCES,0.7017374517374517,"IMPORTANT, please:
625"
REFERENCES,0.7027027027027027,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
626"
REFERENCES,0.7036679536679536,"• Keep the checklist subsection headings, questions/answers and guidelines below.
627"
REFERENCES,0.7046332046332047,"• Do not modify the questions and only use the provided macros for your answers.
628"
CLAIMS,0.7055984555984556,"1. Claims
629"
CLAIMS,0.7065637065637066,"Question: Do the main claims made in the abstract and introduction accurately reflect the
630"
CLAIMS,0.7075289575289575,"paper’s contributions and scope?
631"
CLAIMS,0.7084942084942085,"Answer: [Yes]
632"
CLAIMS,0.7094594594594594,"Justification: Well reflected.
633"
CLAIMS,0.7104247104247104,"Guidelines:
634"
CLAIMS,0.7113899613899614,"• The answer NA means that the abstract and introduction do not include the claims
635"
CLAIMS,0.7123552123552124,"made in the paper.
636"
CLAIMS,0.7133204633204633,"• The abstract and/or introduction should clearly state the claims made, including the
637"
CLAIMS,0.7142857142857143,"contributions made in the paper and important assumptions and limitations. A No or
638"
CLAIMS,0.7152509652509652,"NA answer to this question will not be perceived well by the reviewers.
639"
CLAIMS,0.7162162162162162,"• The claims made should match theoretical and experimental results, and reflect how
640"
CLAIMS,0.7171814671814671,"much the results can be expected to generalize to other settings.
641"
CLAIMS,0.7181467181467182,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
642"
CLAIMS,0.7191119691119691,"are not attained by the paper.
643"
LIMITATIONS,0.7200772200772201,"2. Limitations
644"
LIMITATIONS,0.721042471042471,"Question: Does the paper discuss the limitations of the work performed by the authors?
645"
LIMITATIONS,0.722007722007722,"Answer: [Yes]
646"
LIMITATIONS,0.722972972972973,"Justification: Presented in Appendix.
647"
LIMITATIONS,0.7239382239382239,"Guidelines:
648"
LIMITATIONS,0.724903474903475,"• The answer NA means that the paper has no limitation while the answer No means that
649"
LIMITATIONS,0.7258687258687259,"the paper has limitations, but those are not discussed in the paper.
650"
LIMITATIONS,0.7268339768339769,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
651"
LIMITATIONS,0.7277992277992278,"• The paper should point out any strong assumptions and how robust the results are to
652"
LIMITATIONS,0.7287644787644788,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
653"
LIMITATIONS,0.7297297297297297,"model well-specification, asymptotic approximations only holding locally). The authors
654"
LIMITATIONS,0.7306949806949807,"should reflect on how these assumptions might be violated in practice and what the
655"
LIMITATIONS,0.7316602316602316,"implications would be.
656"
LIMITATIONS,0.7326254826254827,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
657"
LIMITATIONS,0.7335907335907336,"only tested on a few datasets or with a few runs. In general, empirical results often
658"
LIMITATIONS,0.7345559845559846,"depend on implicit assumptions, which should be articulated.
659"
LIMITATIONS,0.7355212355212355,"• The authors should reflect on the factors that influence the performance of the approach.
660"
LIMITATIONS,0.7364864864864865,"For example, a facial recognition algorithm may perform poorly when image resolution
661"
LIMITATIONS,0.7374517374517374,"is low or images are taken in low lighting. Or a speech-to-text system might not be
662"
LIMITATIONS,0.7384169884169884,"used reliably to provide closed captions for online lectures because it fails to handle
663"
LIMITATIONS,0.7393822393822393,"technical jargon.
664"
LIMITATIONS,0.7403474903474904,"• The authors should discuss the computational efficiency of the proposed algorithms
665"
LIMITATIONS,0.7413127413127413,"and how they scale with dataset size.
666"
LIMITATIONS,0.7422779922779923,"• If applicable, the authors should discuss possible limitations of their approach to
667"
LIMITATIONS,0.7432432432432432,"address problems of privacy and fairness.
668"
LIMITATIONS,0.7442084942084942,"• While the authors might fear that complete honesty about limitations might be used by
669"
LIMITATIONS,0.7451737451737451,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
670"
LIMITATIONS,0.7461389961389961,"limitations that aren’t acknowledged in the paper. The authors should use their best
671"
LIMITATIONS,0.747104247104247,"judgment and recognize that individual actions in favor of transparency play an impor-
672"
LIMITATIONS,0.7480694980694981,"tant role in developing norms that preserve the integrity of the community. Reviewers
673"
LIMITATIONS,0.749034749034749,"will be specifically instructed to not penalize honesty concerning limitations.
674"
THEORY ASSUMPTIONS AND PROOFS,0.75,"3. Theory Assumptions and Proofs
675"
THEORY ASSUMPTIONS AND PROOFS,0.750965250965251,"Question: For each theoretical result, does the paper provide the full set of assumptions and
676"
THEORY ASSUMPTIONS AND PROOFS,0.7519305019305019,"a complete (and correct) proof?
677"
THEORY ASSUMPTIONS AND PROOFS,0.752895752895753,"Answer: [NA] .
678"
THEORY ASSUMPTIONS AND PROOFS,0.7538610038610039,"Justification: No theory.
679"
THEORY ASSUMPTIONS AND PROOFS,0.7548262548262549,"Guidelines:
680"
THEORY ASSUMPTIONS AND PROOFS,0.7557915057915058,"• The answer NA means that the paper does not include theoretical results.
681"
THEORY ASSUMPTIONS AND PROOFS,0.7567567567567568,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
682"
THEORY ASSUMPTIONS AND PROOFS,0.7577220077220077,"referenced.
683"
THEORY ASSUMPTIONS AND PROOFS,0.7586872586872587,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
684"
THEORY ASSUMPTIONS AND PROOFS,0.7596525096525096,"• The proofs can either appear in the main paper or the supplemental material, but if
685"
THEORY ASSUMPTIONS AND PROOFS,0.7606177606177607,"they appear in the supplemental material, the authors are encouraged to provide a short
686"
THEORY ASSUMPTIONS AND PROOFS,0.7615830115830116,"proof sketch to provide intuition.
687"
THEORY ASSUMPTIONS AND PROOFS,0.7625482625482626,"• Inversely, any informal proof provided in the core of the paper should be complemented
688"
THEORY ASSUMPTIONS AND PROOFS,0.7635135135135135,"by formal proofs provided in appendix or supplemental material.
689"
THEORY ASSUMPTIONS AND PROOFS,0.7644787644787645,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7654440154440154,"4. Experimental Result Reproducibility
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7664092664092664,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7673745173745173,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7683397683397684,"of the paper (regardless of whether the code and data are provided or not)?
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7693050193050193,"Answer: [Yes]
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7702702702702703,"Justification: Yes, and code is in supplementary material.
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7712355212355212,"Guidelines:
697"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7722007722007722,"• The answer NA means that the paper does not include experiments.
698"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7731660231660231,"• If the paper includes experiments, a No answer to this question will not be perceived
699"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7741312741312741,"well by the reviewers: Making the paper reproducible is important, regardless of
700"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.775096525096525,"whether the code and data are provided or not.
701"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7760617760617761,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
702"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.777027027027027,"to make their results reproducible or verifiable.
703"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.777992277992278,"• Depending on the contribution, reproducibility can be accomplished in various ways.
704"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.778957528957529,"For example, if the contribution is a novel architecture, describing the architecture fully
705"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7799227799227799,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
706"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7808880308880309,"be necessary to either make it possible for others to replicate the model with the same
707"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7818532818532818,"dataset, or provide access to the model. In general. releasing code and data is often
708"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7828185328185329,"one good way to accomplish this, but reproducibility can also be provided via detailed
709"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7837837837837838,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
710"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7847490347490348,"of a large language model), releasing of a model checkpoint, or other means that are
711"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7857142857142857,"appropriate to the research performed.
712"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7866795366795367,"• While NeurIPS does not require releasing code, the conference does require all submis-
713"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7876447876447876,"sions to provide some reasonable avenue for reproducibility, which may depend on the
714"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7886100386100386,"nature of the contribution. For example
715"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7895752895752896,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
716"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7905405405405406,"to reproduce that algorithm.
717"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7915057915057915,"(b) If the contribution is primarily a new model architecture, the paper should describe
718"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7924710424710425,"the architecture clearly and fully.
719"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7934362934362934,"(c) If the contribution is a new model (e.g., a large language model), then there should
720"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7944015444015444,"either be a way to access this model for reproducing the results or a way to reproduce
721"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7953667953667953,"the model (e.g., with an open-source dataset or instructions for how to construct
722"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7963320463320464,"the dataset).
723"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7972972972972973,"(d) We recognize that reproducibility may be tricky in some cases, in which case
724"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7982625482625483,"authors are welcome to describe the particular way they provide for reproducibility.
725"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7992277992277992,"In the case of closed-source models, it may be that access to the model is limited in
726"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8001930501930502,"some way (e.g., to registered users), but it should be possible for other researchers
727"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8011583011583011,"to have some path to reproducing or verifying the results.
728"
OPEN ACCESS TO DATA AND CODE,0.8021235521235521,"5. Open access to data and code
729"
OPEN ACCESS TO DATA AND CODE,0.803088803088803,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
730"
OPEN ACCESS TO DATA AND CODE,0.8040540540540541,"tions to faithfully reproduce the main experimental results, as described in supplemental
731"
OPEN ACCESS TO DATA AND CODE,0.805019305019305,"material?
732"
OPEN ACCESS TO DATA AND CODE,0.805984555984556,"Answer: [Yes]
733"
OPEN ACCESS TO DATA AND CODE,0.806949806949807,"Justification: Yes, and code is in supplementary material.
734"
OPEN ACCESS TO DATA AND CODE,0.8079150579150579,"Guidelines:
735"
OPEN ACCESS TO DATA AND CODE,0.8088803088803089,"• The answer NA means that paper does not include experiments requiring code.
736"
OPEN ACCESS TO DATA AND CODE,0.8098455598455598,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
737"
OPEN ACCESS TO DATA AND CODE,0.8108108108108109,"public/guides/CodeSubmissionPolicy) for more details.
738"
OPEN ACCESS TO DATA AND CODE,0.8117760617760618,"• While we encourage the release of code and data, we understand that this might not be
739"
OPEN ACCESS TO DATA AND CODE,0.8127413127413128,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
740"
OPEN ACCESS TO DATA AND CODE,0.8137065637065637,"including code, unless this is central to the contribution (e.g., for a new open-source
741"
OPEN ACCESS TO DATA AND CODE,0.8146718146718147,"benchmark).
742"
OPEN ACCESS TO DATA AND CODE,0.8156370656370656,"• The instructions should contain the exact command and environment needed to run to
743"
OPEN ACCESS TO DATA AND CODE,0.8166023166023166,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
744"
OPEN ACCESS TO DATA AND CODE,0.8175675675675675,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
745"
OPEN ACCESS TO DATA AND CODE,0.8185328185328186,"• The authors should provide instructions on data access and preparation, including how
746"
OPEN ACCESS TO DATA AND CODE,0.8194980694980695,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
747"
OPEN ACCESS TO DATA AND CODE,0.8204633204633205,"• The authors should provide scripts to reproduce all experimental results for the new
748"
OPEN ACCESS TO DATA AND CODE,0.8214285714285714,"proposed method and baselines. If only a subset of experiments are reproducible, they
749"
OPEN ACCESS TO DATA AND CODE,0.8223938223938224,"should state which ones are omitted from the script and why.
750"
OPEN ACCESS TO DATA AND CODE,0.8233590733590733,"• At submission time, to preserve anonymity, the authors should release anonymized
751"
OPEN ACCESS TO DATA AND CODE,0.8243243243243243,"versions (if applicable).
752"
OPEN ACCESS TO DATA AND CODE,0.8252895752895753,"• Providing as much information as possible in supplemental material (appended to the
753"
OPEN ACCESS TO DATA AND CODE,0.8262548262548263,"paper) is recommended, but including URLs to data and code is permitted.
754"
OPEN ACCESS TO DATA AND CODE,0.8272200772200772,"6. Experimental Setting/Details
755"
OPEN ACCESS TO DATA AND CODE,0.8281853281853282,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
756"
OPEN ACCESS TO DATA AND CODE,0.8291505791505791,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
757"
OPEN ACCESS TO DATA AND CODE,0.8301158301158301,"results?
758"
OPEN ACCESS TO DATA AND CODE,0.831081081081081,"Answer: [Yes]
759"
OPEN ACCESS TO DATA AND CODE,0.832046332046332,"Justification: Yes.
760"
OPEN ACCESS TO DATA AND CODE,0.833011583011583,"Guidelines:
761"
OPEN ACCESS TO DATA AND CODE,0.833976833976834,"• The answer NA means that the paper does not include experiments.
762"
OPEN ACCESS TO DATA AND CODE,0.834942084942085,"• The experimental setting should be presented in the core of the paper to a level of detail
763"
OPEN ACCESS TO DATA AND CODE,0.8359073359073359,"that is necessary to appreciate the results and make sense of them.
764"
OPEN ACCESS TO DATA AND CODE,0.8368725868725869,"• The full details can be provided either with the code, in appendix, or as supplemental
765"
OPEN ACCESS TO DATA AND CODE,0.8378378378378378,"material.
766"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8388030888030888,"7. Experiment Statistical Significance
767"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8397683397683398,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
768"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8407335907335908,"information about the statistical significance of the experiments?
769"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8416988416988417,"Answer: [Yes]
770"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8426640926640927,"Justification: Results with mean and std are presented in Appendix.
771"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8436293436293436,"Guidelines:
772"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8445945945945946,"• The answer NA means that the paper does not include experiments.
773"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8455598455598455,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
774"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8465250965250966,"dence intervals, or statistical significance tests, at least for the experiments that support
775"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8474903474903475,"the main claims of the paper.
776"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8484555984555985,"• The factors of variability that the error bars are capturing should be clearly stated (for
777"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8494208494208494,"example, train/test split, initialization, random drawing of some parameter, or overall
778"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8503861003861004,"run with given experimental conditions).
779"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8513513513513513,"• The method for calculating the error bars should be explained (closed form formula,
780"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8523166023166023,"call to a library function, bootstrap, etc.)
781"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8532818532818532,"• The assumptions made should be given (e.g., Normally distributed errors).
782"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8542471042471043,"• It should be clear whether the error bar is the standard deviation or the standard error
783"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8552123552123552,"of the mean.
784"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8561776061776062,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
785"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
786"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8581081081081081,"of Normality of errors is not verified.
787"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.859073359073359,"• For asymmetric distributions, the authors should be careful not to show in tables or
788"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.86003861003861,"figures symmetric error bars that would yield results that are out of range (e.g. negative
789"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.861003861003861,"error rates).
790"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.861969111969112,"• If error bars are reported in tables or plots, The authors should explain in the text how
791"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.862934362934363,"they were calculated and reference the corresponding figures or tables in the text.
792"
EXPERIMENTS COMPUTE RESOURCES,0.8638996138996139,"8. Experiments Compute Resources
793"
EXPERIMENTS COMPUTE RESOURCES,0.8648648648648649,"Question: For each experiment, does the paper provide sufficient information on the com-
794"
EXPERIMENTS COMPUTE RESOURCES,0.8658301158301158,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
795"
EXPERIMENTS COMPUTE RESOURCES,0.8667953667953668,"the experiments?
796"
EXPERIMENTS COMPUTE RESOURCES,0.8677606177606177,"Answer: [Yes]
797"
EXPERIMENTS COMPUTE RESOURCES,0.8687258687258688,"Justification: Yes.
798"
EXPERIMENTS COMPUTE RESOURCES,0.8696911196911197,"Guidelines:
799"
EXPERIMENTS COMPUTE RESOURCES,0.8706563706563707,"• The answer NA means that the paper does not include experiments.
800"
EXPERIMENTS COMPUTE RESOURCES,0.8716216216216216,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
801"
EXPERIMENTS COMPUTE RESOURCES,0.8725868725868726,"or cloud provider, including relevant memory and storage.
802"
EXPERIMENTS COMPUTE RESOURCES,0.8735521235521235,"• The paper should provide the amount of compute required for each of the individual
803"
EXPERIMENTS COMPUTE RESOURCES,0.8745173745173745,"experimental runs as well as estimate the total compute.
804"
EXPERIMENTS COMPUTE RESOURCES,0.8754826254826255,"• The paper should disclose whether the full research project required more compute
805"
EXPERIMENTS COMPUTE RESOURCES,0.8764478764478765,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
806"
EXPERIMENTS COMPUTE RESOURCES,0.8774131274131274,"didn’t make it into the paper).
807"
CODE OF ETHICS,0.8783783783783784,"9. Code Of Ethics
808"
CODE OF ETHICS,0.8793436293436293,"Question: Does the research conducted in the paper conform, in every respect, with the
809"
CODE OF ETHICS,0.8803088803088803,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
810"
CODE OF ETHICS,0.8812741312741312,"Answer: [Yes]
811"
CODE OF ETHICS,0.8822393822393823,"Justification: Yes.
812"
CODE OF ETHICS,0.8832046332046332,"Guidelines:
813"
CODE OF ETHICS,0.8841698841698842,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
814"
CODE OF ETHICS,0.8851351351351351,"• If the authors answer No, they should explain the special circumstances that require a
815"
CODE OF ETHICS,0.8861003861003861,"deviation from the Code of Ethics.
816"
CODE OF ETHICS,0.887065637065637,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
817"
CODE OF ETHICS,0.888030888030888,"eration due to laws or regulations in their jurisdiction).
818"
BROADER IMPACTS,0.888996138996139,"10. Broader Impacts
819"
BROADER IMPACTS,0.88996138996139,"Question: Does the paper discuss both potential positive societal impacts and negative
820"
BROADER IMPACTS,0.890926640926641,"societal impacts of the work performed?
821"
BROADER IMPACTS,0.8918918918918919,"Answer: [Yes]
822"
BROADER IMPACTS,0.8928571428571429,"Justification: Yes.
823"
BROADER IMPACTS,0.8938223938223938,"Guidelines:
824"
BROADER IMPACTS,0.8947876447876448,"• The answer NA means that there is no societal impact of the work performed.
825"
BROADER IMPACTS,0.8957528957528957,"• If the authors answer NA or No, they should explain why their work has no societal
826"
BROADER IMPACTS,0.8967181467181468,"impact or why the paper does not address societal impact.
827"
BROADER IMPACTS,0.8976833976833977,"• Examples of negative societal impacts include potential malicious or unintended uses
828"
BROADER IMPACTS,0.8986486486486487,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
829"
BROADER IMPACTS,0.8996138996138996,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
830"
BROADER IMPACTS,0.9005791505791506,"groups), privacy considerations, and security considerations.
831"
BROADER IMPACTS,0.9015444015444015,"• The conference expects that many papers will be foundational research and not tied
832"
BROADER IMPACTS,0.9025096525096525,"to particular applications, let alone deployments. However, if there is a direct path to
833"
BROADER IMPACTS,0.9034749034749034,"any negative applications, the authors should point it out. For example, it is legitimate
834"
BROADER IMPACTS,0.9044401544401545,"to point out that an improvement in the quality of generative models could be used to
835"
BROADER IMPACTS,0.9054054054054054,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
836"
BROADER IMPACTS,0.9063706563706564,"that a generic algorithm for optimizing neural networks could enable people to train
837"
BROADER IMPACTS,0.9073359073359073,"models that generate Deepfakes faster.
838"
BROADER IMPACTS,0.9083011583011583,"• The authors should consider possible harms that could arise when the technology is
839"
BROADER IMPACTS,0.9092664092664092,"being used as intended and functioning correctly, harms that could arise when the
840"
BROADER IMPACTS,0.9102316602316602,"technology is being used as intended but gives incorrect results, and harms following
841"
BROADER IMPACTS,0.9111969111969112,"from (intentional or unintentional) misuse of the technology.
842"
BROADER IMPACTS,0.9121621621621622,"• If there are negative societal impacts, the authors could also discuss possible mitigation
843"
BROADER IMPACTS,0.9131274131274131,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
844"
BROADER IMPACTS,0.9140926640926641,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
845"
BROADER IMPACTS,0.915057915057915,"feedback over time, improving the efficiency and accessibility of ML).
846"
SAFEGUARDS,0.916023166023166,"11. Safeguards
847"
SAFEGUARDS,0.916988416988417,"Question: Does the paper describe safeguards that have been put in place for responsible
848"
SAFEGUARDS,0.917953667953668,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
849"
SAFEGUARDS,0.918918918918919,"image generators, or scraped datasets)?
850"
SAFEGUARDS,0.9198841698841699,"Answer: [NA] .
851"
SAFEGUARDS,0.9208494208494209,"Justification: No risk.
852"
SAFEGUARDS,0.9218146718146718,"Guidelines:
853"
SAFEGUARDS,0.9227799227799228,"• The answer NA means that the paper poses no such risks.
854"
SAFEGUARDS,0.9237451737451737,"• Released models that have a high risk for misuse or dual-use should be released with
855"
SAFEGUARDS,0.9247104247104247,"necessary safeguards to allow for controlled use of the model, for example by requiring
856"
SAFEGUARDS,0.9256756756756757,"that users adhere to usage guidelines or restrictions to access the model or implementing
857"
SAFEGUARDS,0.9266409266409267,"safety filters.
858"
SAFEGUARDS,0.9276061776061776,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
859"
SAFEGUARDS,0.9285714285714286,"should describe how they avoided releasing unsafe images.
860"
SAFEGUARDS,0.9295366795366795,"• We recognize that providing effective safeguards is challenging, and many papers do
861"
SAFEGUARDS,0.9305019305019305,"not require this, but we encourage authors to take this into account and make a best
862"
SAFEGUARDS,0.9314671814671814,"faith effort.
863"
LICENSES FOR EXISTING ASSETS,0.9324324324324325,"12. Licenses for existing assets
864"
LICENSES FOR EXISTING ASSETS,0.9333976833976834,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
865"
LICENSES FOR EXISTING ASSETS,0.9343629343629344,"the paper, properly credited and are the license and terms of use explicitly mentioned and
866"
LICENSES FOR EXISTING ASSETS,0.9353281853281853,"properly respected?
867"
LICENSES FOR EXISTING ASSETS,0.9362934362934363,"Answer: [Yes]
868"
LICENSES FOR EXISTING ASSETS,0.9372586872586872,"Justification: Yes.
869"
LICENSES FOR EXISTING ASSETS,0.9382239382239382,"Guidelines:
870"
LICENSES FOR EXISTING ASSETS,0.9391891891891891,"• The answer NA means that the paper does not use existing assets.
871"
LICENSES FOR EXISTING ASSETS,0.9401544401544402,"• The authors should cite the original paper that produced the code package or dataset.
872"
LICENSES FOR EXISTING ASSETS,0.9411196911196911,"• The authors should state which version of the asset is used and, if possible, include a
873"
LICENSES FOR EXISTING ASSETS,0.9420849420849421,"URL.
874"
LICENSES FOR EXISTING ASSETS,0.943050193050193,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
875"
LICENSES FOR EXISTING ASSETS,0.944015444015444,"• For scraped data from a particular source (e.g., website), the copyright and terms of
876"
LICENSES FOR EXISTING ASSETS,0.944980694980695,"service of that source should be provided.
877"
LICENSES FOR EXISTING ASSETS,0.9459459459459459,"• If assets are released, the license, copyright information, and terms of use in the
878"
LICENSES FOR EXISTING ASSETS,0.946911196911197,"package should be provided. For popular datasets, paperswithcode.com/datasets
879"
LICENSES FOR EXISTING ASSETS,0.9478764478764479,"has curated licenses for some datasets. Their licensing guide can help determine the
880"
LICENSES FOR EXISTING ASSETS,0.9488416988416989,"license of a dataset.
881"
LICENSES FOR EXISTING ASSETS,0.9498069498069498,"• For existing datasets that are re-packaged, both the original license and the license of
882"
LICENSES FOR EXISTING ASSETS,0.9507722007722008,"the derived asset (if it has changed) should be provided.
883"
LICENSES FOR EXISTING ASSETS,0.9517374517374517,"• If this information is not available online, the authors are encouraged to reach out to
884"
LICENSES FOR EXISTING ASSETS,0.9527027027027027,"the asset’s creators.
885"
NEW ASSETS,0.9536679536679536,"13. New Assets
886"
NEW ASSETS,0.9546332046332047,"Question: Are new assets introduced in the paper well documented and is the documentation
887"
NEW ASSETS,0.9555984555984556,"provided alongside the assets?
888"
NEW ASSETS,0.9565637065637066,"Answer: [NA] .
889"
NEW ASSETS,0.9575289575289575,"Justification: No new assets.
890"
NEW ASSETS,0.9584942084942085,"Guidelines:
891"
NEW ASSETS,0.9594594594594594,"• The answer NA means that the paper does not release new assets.
892"
NEW ASSETS,0.9604247104247104,"• Researchers should communicate the details of the dataset/code/model as part of their
893"
NEW ASSETS,0.9613899613899614,"submissions via structured templates. This includes details about training, license,
894"
NEW ASSETS,0.9623552123552124,"limitations, etc.
895"
NEW ASSETS,0.9633204633204633,"• The paper should discuss whether and how consent was obtained from people whose
896"
NEW ASSETS,0.9642857142857143,"asset is used.
897"
NEW ASSETS,0.9652509652509652,"• At submission time, remember to anonymize your assets (if applicable). You can either
898"
NEW ASSETS,0.9662162162162162,"create an anonymized URL or include an anonymized zip file.
899"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9671814671814671,"14. Crowdsourcing and Research with Human Subjects
900"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9681467181467182,"Question: For crowdsourcing experiments and research with human subjects, does the paper
901"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9691119691119691,"include the full text of instructions given to participants and screenshots, if applicable, as
902"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9700772200772201,"well as details about compensation (if any)?
903"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.971042471042471,"Answer: [NA] .
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972007722007722,"Justification: No human subjects.
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972972972972973,"Guidelines:
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739382239382239,"• The answer NA means that the paper does not involve crowdsourcing nor research with
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974903474903475,"human subjects.
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758687258687259,"• Including this information in the supplemental material is fine, but if the main contribu-
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768339768339769,"tion of the paper involves human subjects, then as much detail as possible should be
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9777992277992278,"included in the main paper.
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787644787644788,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9797297297297297,"or other labor should be paid at least the minimum wage in the country of the data
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806949806949807,"collector.
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816602316602316,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
915"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826254826254827,"Subjects
916"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835907335907336,"Question: Does the paper describe potential risks incurred by study participants, whether
917"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845559845559846,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
918"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9855212355212355,"approvals (or an equivalent approval/review based on the requirements of your country or
919"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864864864864865,"institution) were obtained?
920"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874517374517374,"Answer: [NA] .
921"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884169884169884,"Justification: No involving.
922"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893822393822393,"Guidelines:
923"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903474903474904,"• The answer NA means that the paper does not involve crowdsourcing nor research with
924"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913127413127413,"human subjects.
925"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922779922779923,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
926"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932432432432432,"may be required for any human subjects research. If you obtained IRB approval, you
927"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942084942084942,"should clearly state this in the paper.
928"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951737451737451,"• We recognize that the procedures for this may vary significantly between institutions
929"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961389961389961,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
930"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997104247104247,"guidelines for their institution.
931"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980694980694981,"• For initial submissions, do not include any information that would break anonymity (if
932"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.999034749034749,"applicable), such as the institution conducting the review.
933"
