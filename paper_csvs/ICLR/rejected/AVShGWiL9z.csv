Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002004008016032064,"In many scientific disciplines, we are interested in inferring the nonlinear dynamical
system underlying a set of observed time series, a challenging task in the face of
chaotic behavior and noise. Previous deep learning approaches toward this goal
often suffered from a lack of interpretability and tractability. In particular, the high-
dimensional latent spaces often required for a faithful embedding, even when the
underlying dynamics lives on a lower-dimensional manifold, can hamper theoretical
analysis. Motivated by the emerging principles of dendritic computation, we
augment a dynamically interpretable and mathematically tractable piecewise-linear
(PL) recurrent neural network (RNN) by a linear spline basis expansion. We show
that this approach retains all the theoretically appealing properties of the simple
PLRNN, yet boosts its capacity for approximating arbitrary nonlinear dynamical
systems in comparatively low dimensions. We introduce two frameworks for
training the system, one based on fast and scalable variational inference, and another
combining BPTT with teacher forcing. We show that the dendritically expanded
PLRNN achieves better reconstructions with fewer parameters and dimensions on
various dynamical systems benchmarks and compares favorably to other methods,
while retaining a tractable and interpretable structure."
INTRODUCTION,0.004008016032064128,"1
INTRODUCTION"
INTRODUCTION,0.006012024048096192,"For many complex systems in physics, biology, or the social sciences, we do not know or have only
rudimentary knowledge about the dynamical system (DS) that may underlie those quantities that we
can empirically observe or measure. Data-driven approaches aimed at automatically inferring the
generating DS from time-series observations could therefore strongly support the scientific process,
and various such methods have been proposed in recent years (Raissi et al., 2018; Zhu et al., 2021;
Yin et al., 2021; Norcliffe et al., 2021; Mohajerin & Waslander, 2018; Karl et al., 2017; Chen et al.,"
INTRODUCTION,0.008016032064128256,"2018; Strauss, 2020). However, due to the often high-dimensional, complex, chaotic, and inherently
noisy nature of real-world DS, like the brain, weather-, or ecosystems, this remains a formidable
challenge. Moreover, although the true DS may evolve on a lower-dimensional manifold in its state
space, the system used for approximation usually needs to be of higher dimensionality to achieve a
proper embedding (Takens, 1981; Sauer et al., 1991; Kantz & Schreiber, 2004). This is especially
true when the approximating system is of a different functional form than the one that would most
naturally describe the data generation process (but is unknown), for instance, when we attempt to
approximate a system of exponential or trigonometric functions by polynomials."
INTRODUCTION,0.01002004008016032,"In this work we sought to improve the capacity and expressiveness of a specific class of recurrent
neural networks (RNNs), achieving agreeable solutions with fewer dimensions and parameters while
retaining a set of desirable theoretical properties. Specifically, we build on piecewise-linear RNNs
(PLRNNs) based on ReLU activation functions, for which fixed points, periodic orbits, and other
dynamical properties can be derived analytically (Schmidt et al., 2021; Koppe et al., 2019), and for
which dynamically equivalent continuous-time (ordinary differential equation, ODE) systems can
be constructed (Monfared & Durstewitz, 2020b). Inspired by principles of dendritic computation in
biological neurons (Fig. 1), each PLRNN unit was endowed with a set of nonlinear pre-processing
subunits (“dendritic branches”), such that it effectively takes on the role of an equivalent much larger
network. Mathematically, this comes down, in our case, to enhancing each latent unit with a linear
spline basis expansion as popular in statistics (Hastie et al., 2009). Through this trick, we achieve"
INTRODUCTION,0.012024048096192385,"a powerful RNN which provides reconstructions of underlying nonlinear DS in lower-dimensional
latent spaces than were needed by conventional PLRNNs. At the same time, model inference can be
performed within the scalable framework of sequential variational auto-encoders (SVAE) (Archer
et al., 2015; Girin et al., 2020; Krishnan et al., 2017), or with classical Back-Propagation-Through-
Time (BPTT; Rumelhart et al. (1986)) augmented by teacher-forcing (TF; Williams & Zipser (1989);
Pearlmutter (1990)). We further prove that these modifications preserve the mathematical and
dynamical accessibility of the resulting system, e.g., such that fixed points, cycles, and their stability,
can still be computed analytically."
INTRODUCTION,0.014028056112224449,"Besides its effectiveness in capturing complex dynamical systems in fewer dimensions within a
tractable framework, our approach highlights more generally how principles of dendritic signal
processing may be harvested in the design of RNNs. Strongly nonlinear local computations are
known for decades to occur within dendritic trees of biological neurons (Mel, 1994; Poirazi et al.,
2003), but have hardly been exploited so far for machine learning models."
RELATED WORK,0.01603206412825651,"2
RELATED WORK"
RELATED WORK,0.018036072144288578,"One class of DS reconstruction models attempts to discover governing equations from the flow field
estimated from data through differencing the time series. Sparse Identification of Nonlinear Dynamics
(SINDy), for instance, does so by sparsely regressing on a rich library of basis functions using the
least absolute shrinkage and selection operator (LASSO) (Brunton et al., 2016; Rudy et al., 2017;
de Silva et al., 2020). Other methods approximate the flow field using graph reconstruction via
differential equations (Chen et al., 2017), sparse autoencoders (Heim et al., 2019), shallow multi-layer
perceptrons reformulated as RNNs (Trischler & D’Eleuterio, 2016), or deep neural networks (Chen
et al., 2018). Some works aimed at directly learning the system’s underlying Hamiltonian (Chen
et al., 2020; Greydanus et al., 2019). Generally, numerical derivatives obtained from time series tend
to be more noise-prone than the time series observations themselves (Baydin et al., 2018; Chen et al.,
2017; Raissi, 2018). This can be a problem particularly if only comparatively short trajectories were
empirically observed or when the underlying systems are very high-dimensional, as in these cases the
system’s flow field may be (severely) under-sampled. Methods directly based on numerical derivatives
also need to be augmented by other techniques, like delay embeddings (Kantz & Schreiber, 2004) or
deep auto-encoders (Champion et al., 2019), if not all the system’s dimensions were observed."
RELATED WORK,0.02004008016032064,"Various RNN architectures such as Long-Short-Term-Memory networks (LSTMs) (Zheng et al.,
2017), Reservoir Computing (RC) (Pathak et al., 2018), or PLRNNs (Koppe et al., 2019; Schmidt
et al., 2021) have been employed to infer DS directly from the observed time series without going
through numerical derivatives. More recently, transformers (Shalova & Oseledets, 2020a;b) were used
as black box approaches for DS prediction. Except for PLRNNs, however, all these systems, although
optimized for DS reconstruction and prediction, rest on relatively complex model formulations that
are not easy to tackle and analyze from a DS perspective (Fraccaro et al.). The ability to gain deeper
insights into the specific DS properties and mechanisms of the recovered system is, however, often
crucial for its applicability to science and engineering problems. Transformers, unlike RNNs, do not
even constitute DS themselves (as they explicitly forgo any temporal recursions), and therefore are
not directly amenable to DS theory tools. Moreover, most of these models, RC in particular, need
very high-dimensional latent spaces, which further adds to their black-box nature."
RELATED WORK,0.022044088176352707,"Better interpretability and tractability is achieved by using PLRNNs (Koppe et al., 2019; Schmidt
et al., 2021) or by (locally) linearizing nonlinear systems through ideas from Koopman operator theory
(Azencot et al., 2020; Brunton et al., 2017; Yeung et al., 2017). In such systems, certain DS properties
can be analytically accessed (Schmidt et al., 2021; Monfared & Durstewitz, 2020a), or the resulting
equations can be more easily interpreted by a human reader (Heim et al., 2019). On the downside,
usually one needs to move to very high dimensions to represent the DS in question properly. Here
we aim to overcome this limitation by augmenting PLRNNs with linear basis expansions without
altering their analytical accessibility."
RELATED WORK,0.02404809619238477,"Finally, probabilistic (generative) latent variable models such as state space models have been applied
to the problem of posterior inference of latent state paths zt ∼p(zt|x1:T ) of DS given time series
observations {x1:T } (Pandarinath et al., 2018; Ghahramani & Roweis, 1998; Durstewitz, 2017;
Krishnan et al., 2017). The advantage here is that they also account for uncertainty in the model
formulation or latent process itself and yield the full distribution over latent space variables (Karl"
RELATED WORK,0.026052104208416832,"et al., 2017). For DS reconstruction, however, we need to move beyond posterior inference: We
require that samples drawn from the model’s prior distribution p(z) after training exhibit the same
temporal and geometric structure as those produced by the unknown DS."
RELATED WORK,0.028056112224448898,"Here we embed PLRNNs augmented with a linear spline expansion into a fully probabilistic, varia-
tional approach that scales well with system size by employing stochastic gradient variational Bayes
(SGVB; (Kingma & Welling, 2014; Rezende et al., 2014)), thereby combining the advantages of
the two classes of models reviewed above. On the other hand, we show that the model can also be
efficiently trained by BPTT using a specific form of TF (Appx. 6.1)."
MODEL FORMULATION AND THEORETICAL CONSIDERATIONS,0.03006012024048096,"3
MODEL FORMULATION AND THEORETICAL CONSIDERATIONS"
MODEL FORMULATION AND THEORETICAL CONSIDERATIONS,0.03206412825651302,"3.1
PIECEWISE LINEAR RECURRENT NEURAL NETWORK (PLRNN)"
MODEL FORMULATION AND THEORETICAL CONSIDERATIONS,0.03406813627254509,"Our approach builds on PLRNNs (Durstewitz, 2017; Koppe et al., 2019) because of their mathematical
tractability (see Sec. 3.3). PLRNNs are defined by the M-dimensional latent process equation"
MODEL FORMULATION AND THEORETICAL CONSIDERATIONS,0.036072144288577156,"zt = Azt−1 + W ϕ(zt−1) + h + Cst + ϵt,
(1)"
MODEL FORMULATION AND THEORETICAL CONSIDERATIONS,0.03807615230460922,"which describes the temporal evolution of M-dimensional latent state vector zt = (z1t . . . zMt)T .
The self-connections of the units are represented by diagonal matrix A ∈RM×M, whereas the
connections between units are collected in off-diagonal matrix W ∈RM×M, with the nonlinear
activation function ϕ given by the rectified linear unit (ReLU) applied element-wise:"
MODEL FORMULATION AND THEORETICAL CONSIDERATIONS,0.04008016032064128,"ϕ(zt−1) = max(0, zt−1).
(2)"
MODEL FORMULATION AND THEORETICAL CONSIDERATIONS,0.04208416833667335,"Additionally, the PLRNN comprises a bias term h ∈RM, potential external inputs st ∈RK weighted
by C ∈RM×K, and a Gaussian noise term ϵt ∼N(0, Σ) with diagonal covariance Σ. The PLRNN
can be interpreted as a discrete-time neural rate model (Durstewitz, 2017), where the entries of A
stand for the individual neurons’ time constants, W for the synaptic connection strengths between
neurons, and ϕ(z) for a (ReLU-shaped) voltage-to-spike-rate transfer function. The probabilistic
latent RNN Eq. 1 is linked to the N-dimensional observed time series (xt)t=1...T , xt ∈RN, drawn
from an underlying noisy DS, by an observation function (decoder model) which, in the simplest
case, may take the linear Gaussian form"
MODEL FORMULATION AND THEORETICAL CONSIDERATIONS,0.04408817635270541,"xt = Bzt + ηt,
(3)"
MODEL FORMULATION AND THEORETICAL CONSIDERATIONS,0.04609218436873747,"where B ∈RN×M represents a factor loading matrix and ηt ∼N(0, Γ) is Gaussian observation
noise with diagonal covariance Γ ∈RN×N."
DENDRITIC COMPUTATION AND SPLINE BASIS EXPANSION,0.04809619238476954,"3.2
DENDRITIC COMPUTATION AND SPLINE BASIS EXPANSION"
DENDRITIC COMPUTATION AND SPLINE BASIS EXPANSION,0.050100200400801605,"Dendrites have long been known to play an active and important part in neural computation (Mel,
1994; 1999; Koch, 2004). Active, fast voltage-gated ion channels endow dendrites with strongly
nonlinear behavior, giving rise for instance to dendritic Ca2+ spikes that boost synaptic inputs
(Schiller et al., 2000; H¨ausser et al., 2000). It has been suggested previously that different dendritic
branches may constitute rather independent computational sub-units whose outputs are combined at
the soma, as in a 2-layer neural network (Poirazi et al., 2003; Mel, 1993; 1994), an idea that received
strong empirical support especially in recent years (Poirazi & Papoutsi, 2020). Here we mimic
this functional setup by modeling dendritic processing through a linear combination of ReLU-type
threshold-nonlinearities (Fig. 1), replacing Eq. 2 by"
DENDRITIC COMPUTATION AND SPLINE BASIS EXPANSION,0.052104208416833664,"ϕ(zt−1) = B
X"
DENDRITIC COMPUTATION AND SPLINE BASIS EXPANSION,0.05410821643286573,"b=1
αb max(0, zt−1 −hb),
(4)"
DENDRITIC COMPUTATION AND SPLINE BASIS EXPANSION,0.056112224448897796,"with “dendritic input/output” slopes αb ∈R and “activation” thresholds hb ∈RM. As in real
dendrites, where both ion channels and morphological structure are subject to learning (Poirazi &
Papoutsi, 2020; Stemmler & Koch, 1999), we treat these as trainable parameters. We note that Eq. 4
inserted into model Eq. 1 takes the form of a linear spline basis expansion as popular in statistics
(Hastie et al., 2009) for approximating arbitrary functions (Wahba, 1990; Storace & De Feo, 2004) in
regression settings. For instance, such concepts have been frequently employed within data-analytical"
DENDRITIC COMPUTATION AND SPLINE BASIS EXPANSION,0.05811623246492986,"models in neuroscience (Frank et al.; Huang et al.; Qian et al.), but never within the context of DS
reconstruction enabling lower-dimensional solutions in mathematically tractable models."
DENDRITIC COMPUTATION AND SPLINE BASIS EXPANSION,0.06012024048096192,"To emphasize the connection to dendritic computation we call the system Eqs. 1, 3, 4, the dendPLRNN."
DENDRITIC COMPUTATION AND SPLINE BASIS EXPANSION,0.06212424849699399,"Figure 1: Inspired by principles of dendritic computation, our dendPLRNN extends each unit into a
set of nonlinear branches connected to a soma, yielding single unit transfer functions with increased
approximation capabilities. Image of dendrite from https://www.milad.no/blog/ (CC BY-
SA 4.0)."
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.06412825651302605,"3.3
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION"
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.06613226452905811,"Sharp threshold-nonlinearities (like a ReLU) are a reasonable choice from a neurobiological per-
spective, as dendrites naturally give rise to this threshold-type behavior (Mel, 1999; Koch, 2004).
Another important consideration in choosing this particular form, however, was that it preserves all
the theoretically appealing properties of a PLRNN, as we will formally establish below: For PLRNNs
fixed points and cycles can be explicitly computed (Schmidt et al., 2021; Koppe et al., 2019), and
they can be translated into dynamically equivalent continuous-time systems (Monfared & Durstewitz,
2020b), properties which profoundly ease the analysis of trained systems from a DS perspective. This
is crucial for application in the sciences, where we are specifically interested in understanding the
underlying system’s dynamics. For PLRNNs, precise connections between the long-term behavior
of the system and that of its gradients have also been established (Schmidt et al., 2021). Finally,
PLRNNs belong to the class of continuous piecewise-linear (PWL) maps, for which many important
types of bifurcations have been well characterized (Feigin, 1995; Hogan et al., 2007; Patra, 2018)
(cf. (Monfared & Durstewitz, 2020a) for an overview). Bifurcations are essential to understand how
geometrical and topological properties of the system’s state space depend on its parameters or could
be controlled, and hence are also important to characterize or improve the training process itself
(Doya, 1992; Pascanu et al., 2013; Saxe et al., 2014) or to understand properties of trained systems
(Maheswaranathan et al., 2019b;a) ."
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.06813627254509018,"Our first proposition, therefore, assures that by the particular form of basis expansion introduced in
Eq. 4, the system will remain within the class of continuous PWL maps:"
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.07014028056112225,Proposition 1. The model defined through Eq. 1 and Eq. 4 constitutes a continuous PWL map.
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.07214428857715431,"The proof essentially straightforwardly follows from the model’s definition as a linear spline basis
expansion in each unit, but is formally provided in Appx. 6.5.4."
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.07414829659318638,"While Proposition 1 is all we need to ensure we can harvest all previously established results on
PLRNNs in particular, and on continuous PWL maps more generally, it is revealing to note that
any dendPLRNN (Eqs. 1, 4) can be rewritten as a conventional PLRNN, as stated in the following
theorem:"
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.07615230460921844,"Theorem 1. Any M-dimensional dendPLRNN as defined in Eqs. 1, 4, can always be rewritten as a
M × B-dimensional “conventional” PLRNN of the form"
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.0781563126252505,"ˆzt = ˜
Aˆzt−1 + ˜
W max(0, ˆzt−1) + ˆh0 +
˜
Cst + ˜ϵt.
(5)"
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.08016032064128256,"Proof. Straightforward by construction, see Appx. 6.5.5."
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.08216432865731463,"This theorem highlights why the dendPLRNN will allow to reduce the dimensionality of the recon-
structed system, as it suggests we may often be able to reformulate a high-dimensional PLRNN in
terms of an equally powerful lower-dimensional dendPLRNN. In Appx. 6.5.1 we also spell out the
exact computation of fixed points and k-cycles for the dendPLRNN."
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.0841683366733467,"Finally, the unboundedness of the PLRNN’s latent states due to the ReLU function can cause
divergence problems in training. The dendPLRNN, on the other hand, offers a simple and natural
way to contain the latent states without violating the basic model description above, as established in
the following theorem:
Theorem 2. For each basis {αb, hb} in Eq. 4 of a dendPLRNN let us add another basis {α∗
b, h∗
b}
with α∗
b = −αb and h∗
b = 0. Then, for σmax(A) < 1, any orbit of this “clipped” dendPLRNN (Eq.
10) will remain bounded."
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.08617234468937876,Proof. See Appx. 6.5.6.
MATHEMATICAL TRACTABILITY AND DYNAMICAL SYSTEMS INTERPRETATION,0.08817635270541083,"Appx. 6.5 collects further theoretical results, assuring, for instance, that the manifold attractor
regularization employed here (see next section) does not interfere with the results above (Proposition
2)."
TRAINING THE DENDPLRNN,0.09018036072144289,"3.4
TRAINING THE DENDPLRNN"
TRAINING THE DENDPLRNN,0.09218436873747494,"To infer the parameters θ = {A, W , h, C, Σ, B, Γ, {αb, hb}} of the dendPLRNN (Eq. 1, 3, 4)
from observed data, we apply two different training strategies: First, a fast and scalable variational
inference (VI) algorithm which maximizes the Evidence Lower Bound (ELBO) L(θ, ϕ; x) :=
Eqϕ[log(pθ(x|z)] −KL[qϕ(z|x)||pθ(z)] using the reparameterization trick (Kingma & Welling,
2014), and convolutional neural networks (CNNs) for parameterizing the encoder model qϕ(z|x)
(see Appx. 6.1 for details). Furthermore, as proposed in Schmidt et al. (2021), to efficiently capture
DS at multiple time scales, we add a regularization term to the ELBO that encourages the mapping of
slow time constants and long-range dependencies (so-called manifold attractor regularization, see
Eq. 6, with regularization factor λ). Second, we employ “classical” BPTT with a variant of teacher
forcing (TF) (Williams & Zipser, 1989; Pearlmutter, 1990). TF here means that the first N latent
states zk,lτ+1, k ≤N, were replaced by observations xk,lτ+1 at times lτ + 1, l ∈N0, where τ ≥1
is the forcing interval (for details, see Appx. 6.1). All code used in here is made freely available at
[placeholder]."
EXPERIMENTS,0.09418837675350701,"4
EXPERIMENTS"
PERFORMANCE MEASURES,0.09619238476953908,"4.1
PERFORMANCE MEASURES"
PERFORMANCE MEASURES,0.09819639278557114,"In DS reconstruction, we aim to capture invariant properties of the underlying DS like its geometrical
and temporal structure. To evaluate the quality of the reconstructions w.r.t. geometrical properties
we employed a Kullback-Leibler divergence (Dstsp) that quantifies agreement in attractor geometries
(more details in Appx. 6.2), as first suggested in Koppe et al. (2019) (see also Schmidt et al. (2021)).
Specifically, this measure evaluates the overlap between the observed data distribution p(xobs) and
the distribution p(xgen|zgen) generated from model simulations (i.e., with zgen ∼pθ(z) after model
training) across state space (not time!). Since this measure as originally defined in Koppe et al. (2019)
is expensive to compute, for the high-dimensional benchmark DS we used another approximation,
details of which are given in Appx. 6.2. Dstsp is evaluated on a set of 100 trajectories, pulled from the
learned distribution over initial conditions, with 1000 time steps each. To assess the agreement in
temporal structure, a dimension-wise, Gaussian-kernel-smoothed power spectrum correlation (PSC)
between ground truth and model-generated trajectories was used (see Appx. 6.2). Finally, we also
computed a 20-step-ahead prediction error along test set trajectories (see Appx. 6.2), although not of
primary interest in the context of DS reconstruction."
DS BENCHMARKS USED FOR EVALUATION,0.10020040080160321,"4.2
DS BENCHMARKS USED FOR EVALUATION"
DS BENCHMARKS USED FOR EVALUATION,0.10220440881763528,"We evaluated our approach and the specific role of the basis expansion on five different types of
challenging DS benchmarks."
DS BENCHMARKS USED FOR EVALUATION,0.10420841683366733,"First, the famous 3d chaotic Lorenz attractor (Lorenz-63) originally proposed by Lorenz (1963)
(formally defined in Appx. 6.4) has become a popular benchmark for DS reconstruction algorithms.
Fig. 2a (l.h.s.) illustrates true (blue) and reconstructed (orange) time series from this system, while the
r.h.s. illustrates the chaotic attractor’s geometry in its 3d state space for both the ground truth (blue)
and reconstructed (orange) systems. It is important to note that both the time and state space graphs
are not merely ahead predictions from the dendPLRNN but are produced by simulating the trained
dendPLRNN from some initial condition. This illustrates that the dendPLRNN has captured the
temporal and geometrical structure of the original Lorenz-63 system in its own governing equations.
Moreover, computing analytically (see Appx. 6.5.1) the fixed points of the reconstructed system, we
see that their positions in state space agree well with those of the true system."
DS BENCHMARKS USED FOR EVALUATION,0.1062124248496994,"Second, a 3d biophysical model of a bursting neuron (see Eq. 15 in Appx. 6.4; Durstewitz (2009))
highlights another aspect of DS reconstruction: Besides an equation for membrane voltage (V ), the
model consists of one very fast (n) and one slow (h) variable that control the gating of the model’s
ionic conductances. This produces fast spikes that ride on top of a much slower oscillation, making
this system challenging to reconstruct. One such successful dendPLRNN reconstruction is illustrated
in Fig. 2b (orange) together with time graphs and state space representations of the true system (blue)."
DS BENCHMARKS USED FOR EVALUATION,0.10821643286573146,"Figure 2: Examples of low-dimensional model reconstructions: (a) Time series (left) and state
space trajectories (right) for the original Lorenz-63 chaotic attractor and simulations produced by
a dendPLRNN trained with VI (B = 20, M = 15, λ = 1, Mreg/M = 0.5). Dots indicate true and
reconstructed fixed points. (b) Same for the bursting neuron model, produced by a dendPLRNN
trained with TF (B = 47, M = 26, τ = 5). Note that the bursting is a complex limit cycle but
non-chaotic."
DS BENCHMARKS USED FOR EVALUATION,0.11022044088176353,"Third, the Lorenz-96 weather model is an example of a higher-dimensional, spatially organized chaotic
system with local neighborhood interactions that can be extended to arbitrary dimensionality (Eq. 18
in Appx. 6.4). It has also been used more widely for benchmarking DS reconstruction algorithms.
For our experiments we employed a 10-dimensional spatial layout. Fig. 3a illustrates time graphs for
selected dimensions (l.h.s.), the full evolving spatio-temporal pattern (center), and examples of power
spectra (r.h.s.) for both the ground truth system (blue) and an example reconstruction (orange). The
spatio-temporal characteristics of the true and the dendPLRNN-generated time series tightly agree."
DS BENCHMARKS USED FOR EVALUATION,0.11222444889779559,"Fourth, as another high-dimensional example we used a neural population model with structured
connectivity tuned to produce coherent chaos (Landau & Sompolinsky, 2018), from which we
produced 50d observations (see Appx. 6.4 for details). Fig. 3b provides example time series (l.h.s.),
full spatio-temporal patterns (center), and overlaid power spectra (r.h.s.) for time series drawn from
the true system (blue) and those simulated by a trained dendPLRNN (orange). Again there is a tight
agreement, and again we emphasize that - like in all the other examples - these are not mere model
ahead-predictions but fully simulated from some random initial condition."
DS BENCHMARKS USED FOR EVALUATION,0.11422845691382766,"Finally, we studied a real-world dataset consisting of electroencephalogram (EEG) recordings from
human subjects, described in more detail with results (Fig.***) in Appx. ***."
BASIS EXPANSION IMPROVES DS RECONSTRUCTION AND LOWERS PARAMETER COSTS,0.11623246492985972,"4.3
BASIS EXPANSION IMPROVES DS RECONSTRUCTION AND LOWERS PARAMETER COSTS"
BASIS EXPANSION IMPROVES DS RECONSTRUCTION AND LOWERS PARAMETER COSTS,0.11823647294589178,"While, in theory, the dendPLRNN is equivalent to a larger PLRNN without basis expansion, in
practice the smaller basis-expanded models trained more successfully. Fig. 4a summarizes our
observations for the VI algorithm on the impact of the basis expansion using the Lorenz-63 DS
as an example (see Fig. S3 for further examples): Both the 20-step ahead prediction error as well"
BASIS EXPANSION IMPROVES DS RECONSTRUCTION AND LOWERS PARAMETER COSTS,0.12024048096192384,"Figure 3: Examples of high-dimensional model reconstructions: (a) Time series (left), spatio-temporal
evolution (center), and power spectra (right) for the true 10d Lorenz-96 system and for dendPLRNN
simulations (B = 50, M = 30, λ = 1.0, Mreg/M = 1.0). (b) Same for a 50d neural population
model producing coherent chaos (B = 5, M = 12, λ = 1.0, Mreg/M = 0.2)."
BASIS EXPANSION IMPROVES DS RECONSTRUCTION AND LOWERS PARAMETER COSTS,0.12224448897795591,"Figure 4: Effect of basis expansion for dendPLRNN trained by VI. (a) Agreement in attractor
geometries (left) and 20-step ahead prediction error (right) for the Lorenz-63 system as a function of
the number of bases (B) for fixed numbers of total parameters. (b) Agreement in attractor geometries
(left) and 20-step ahead prediction error (right) for the Lorenz-63 system as a function of the number
of bases (B) for different numbers of latent states (M)."
BASIS EXPANSION IMPROVES DS RECONSTRUCTION AND LOWERS PARAMETER COSTS,0.12424849699398798,"as the geometrical reconstruction quality as assessed by Dstsp profoundly improve with the size
B of the basis expansion even for the same total number of trainable model parameters (given
by M(M + 1 + B + N) + B, where N is the dimensionality of the observed data). Hence, as
conjectured in Sec. 3, the basis expansion yields better reconstructions at no additional costs in terms
of numbers of model parameters. Fig. 4b looks at the impact of the basis expansion from the angle
of dimensionality reduction by systematically varying the number of bases B and latent states M
for the Lorenz-63 DS: Following the curves horizontally, it becomes clear that the basis expansion
enables to reduce the model’s overall dimensionality without compromising performance."
MODEL COMPARISONS,0.12625250501002003,"4.4
MODEL COMPARISONS"
MODEL COMPARISONS,0.1282565130260521,"We compared our model to the PLRNN without the dendritic expansion and three other algorithms
purpose-tailored for DS reconstruction: First, SINDy (Brunton et al., 2016) aims to reconstruct the
governing equations by approximating numerical derivatives (obtained by differencing the time series,
and applying a variance regularization to reduce noise) through a large library of polynomial basis
functions. Sparse (LASSO) regression is used to pick out the right terms from the library (we used
the PySINDy implementation (de Silva et al., 2020) with multinomials up to sixth order). Second,
Vlachas et al. (2018) used a hybrid of truncated LSTMs and mean-field stochastic models based on
Ornstein-Uhlenbeck processes (LSTM-MSM) to approximate the true system’s flow estimated from
observed time series. Third, Pathak et al. (2018) built on reservoir computing (RC) for their approach
with reservoir parameters chosen to satisfy the “echo state property” (Jaeger & Haas, 2004). For"
MODEL COMPARISONS,0.13026052104208416,"Table 1: Comparison of dendPLRNN (Ours) trained by VI or BPTT+TF, RC (Pathak et al., 2018),
LSTM-MSM (Vlachas et al., 2018), and SINDy (Brunton et al., 2016) on 4 DS benchmarks and one
experimental dataset (top) and 3 challenging data situations (bottom). Values are mean ± SEM."
MODEL COMPARISONS,0.13226452905811623,"Dataset / Setting
Method
PSC
Dstsp
20-step PE
Dynamical variables
Parameters"
MODEL COMPARISONS,0.1342685370741483,Lorenz
MODEL COMPARISONS,0.13627254509018036,"dendPLRNN VI
0.997 ± 0.001
0.80 ± 0.25
2.1e−3 ± 0.2e−3
22
1032
dendPLRNN TF
0.997 ± 0.002
0.13 ± 0.18
9.2e−5 ± 2.8e−5
22
1032
RC
0.991 ± 0.001
0.24 ± 0.05
1.2e−2 ± 0.1e−3
345
1053
LSTM-MSM
0.985 ± 0.004
0.85 ± 0.07
1.2e−2 ± 0.1e−3
29
1035
SINDy
0.998 ± 0.0003
0.04 ± 0.01
6.8e−5 ± 0.2e−5
3
252"
MODEL COMPARISONS,0.13827655310621242,"Lorenz-
96"
MODEL COMPARISONS,0.1402805611222445,"dendPLRNN VI
0.987 ± 0.001
0.10 ± 0.01
3.1e−1 ± 0.9e−1
42
4384
dendPLRNN TF
0.998 ± 0.0001
0.04 ± 0.01
4.1e−2 ± 0.8e−2
50
4480
RC
0.986 ± 0.008
0.25 ± 0.17
7.1e−1 ± 0.1e−2
440
4400
LSTM-MSM
0.993 ± 0.002
0.23 ± 0.03
8.2e−1 ± 0.3e−2
62
4384
SINDy
0.996 ± 0.001
0.06 ± 0.003
6.3e−2 ± 0.1e−3
10
27410"
MODEL COMPARISONS,0.14228456913827656,"Bursting
Neuron"
MODEL COMPARISONS,0.14428857715430862,"dendPLRNN VI
0.55 ± 0.03
7.5 ± 0.4
6.1e−1 ± 0.1e−1
26
2052
dendPLRNN TF
0.76 ± 0.04
2.9 ± 1.3
6.1e−2 ± 2.2e−2
26
2040
RC
0.51 ± 0.01
5.1 ± 0.6
8.6e−2 ± 0.1e−2
711
2133
LSTM-MSM
0.54 ± 0.02
2.83 ± 0.36
3.9e−2 ± 0.1e−2
45
2166
SINDy
diverging
diverging
diverging
3
252"
MODEL COMPARISONS,0.1462925851703407,"Neural
Popula-
tion
Model"
MODEL COMPARISONS,0.14829659318637275,"dendPLRNN VI
0.45 ± 0.05
0.56 ± 0.05
0.82 ± 0.09
12
821
dendPLRNN TF
0.51 ± 0.01
0.19 ± 0.02
1.53 ± 0.03
75
9990
RC
0.30 ± 0.05
0.95 ± 0.19
1.82 ± 0.82
50
2500
LSTM-MSM
0.45 ± 0.03
0.43 ± 0.02
1.02 ± 0.02
56
848
SINDy
diverging
diverging
diverging
50
66300 EEG"
MODEL COMPARISONS,0.15030060120240482,"dendPLRNN VI
0.80 ± 0.01
27.9 ± 3.6
0.56 ± 0.046
117
27194
dendPLRNN TF
0.936 ± 0.017
4.7 ± 2.7
0.267 ± 0.013
128
27058
RC
0.81 ± 0.01
21.2 ± 2.2
5.4 ± 0.2
448
28672
LSTM-MSM
0.84 ± 0.005
19.9 ± 1.8
2.0 ± 0.5
168
27728
SINDy
diverging
diverging
diverging
64
133120"
MODEL COMPARISONS,0.1523046092184369,"Low
amount of
data"
MODEL COMPARISONS,0.15430861723446893,"dendPLRNN VI
0.967 ± 0.007
4.36 ± 0.10
2.8e−2 ± 0.2e−2
22
1032
dendPLRNN TF
0.97 ± 0.04
6.9 ± 5.3
1.5e−2 ± 0.9e−2
22
1032
RC
0.68 ± 0.05
5.74 ± 0.11
4.1e+5 ± 1.2e+5
345
1053
LSTM-MSM
0.960 ± 0.006
6.06 ± 0.37
2.1e−1 ± 0.3e−2
29
1035
SINDy
0.998 ± 0.0003
0.04 ± 0.01
6.8e−5 ± 0.2e−5
3
252"
MODEL COMPARISONS,0.156312625250501,"Partially
observed"
MODEL COMPARISONS,0.15831663326653306,"dendPLRNN VI
0.940 ± 0.006
12.6 ± 1.0
6.5e−2 ± 1.4e−2
22
1032
dendPLRNN TF
0.993 ± 0.003
0.54 ± 0.16
5.3e−3 ± 0.2e−3
22
1032
RC
0.981 ± 0.001
2.92 ± 0.08
7.6e−3 ± 0.1e−3
345
1053
LSTM-MSM
0.934 ± 0.005
6.06 ± 0.37
2.3e−2 ± 0.3e−2
29
1035
SINDy
0.974 ± 6e −4
17.5 ± 0.4
5.1e−2 ± 0.4e−2
3
252"
MODEL COMPARISONS,0.16032064128256512,"High
noise"
MODEL COMPARISONS,0.1623246492985972,"dendPLRNN VI
0.973 ± 0.006
4.9 ± 0.75
3.5e−2 ± 0.1e−2
22
1032
dendPLRNN TF
0.995 ± 0.002
0.4 ± 0.13
4.6e−3 ± 0.4e−3
22
1032
RC
0.988 ± 0.001
2.33 ± 0.21
3.1e−2 ± 0.2e−2
345
1053
LSTM-MSM
0.967 ± 0.006
1.19 ± 0.27
3.3e−2 ± 0.2e−2
29
1035
SINDy
0.984 ± 0.005
1.01 ± 0.05
2.3e−3 ± 0.1e−4
3
252"
MODEL COMPARISONS,0.16432865731462926,"higher-dimensional systems, a spatially arranged set of reservoirs with local neighborhood relations
is employed. For all these systems, optimized hyper-parameters were used as reported by the authors.
For our own system, the dendPLRNN, we also performed a grid search for optimal hyper-parameters
λreg, τTF, M, and B (see Appx. 6.1 and Table S1 for details). For all four methods, to the degree
possible we tried to ensure roughly the same number of trainable parameters (see Table S2)."
MODEL COMPARISONS,0.16633266533066132,"Results for all four models on all five DS benchmarks employed here are summarized in the upper
part of Table S2, using the temporal and geometrical reconstruction measures introduced in Sec. 4.2
(as well as a 20-step-ahead prediction error for comparison). To produce this table, 100,000 time
steps for both training and testing were simulated from each ground truth system, all dimensions
were standardized to have zero mean and unit variance, and process noise and observation noise (with
1% of the data variance) were added while simulating the (now stochastic) differential equations,
and after drawing the observations, respectively (see Appx. 6.4 for further methodological details).
To produce statistics, each method was run from a total of 20 randomly chosen initial conditions
for the parameters. We also tested all four methods on the real EEG data and on challenging data
situations produced using the Lorenz-63 system (Fig. 2a), with either short time series of just 1000
time steps, only partial observations (just state variable x in Eq.14 in Appx. 6.4), or high process and
high observation noise (drawing from a Gaussian with dϵ ∼N(0, 0.1dt × I) for the process noise
as described in Appx. 6.4, and using 10% of the observation variance, respectively). SINDy cannot
naturally handle missing observations, as it has no latent variables but formulates the model directly
in terms of the observations. Therefore, for the partially observed system, we used delay embedding
(Takens, 1981; Sauer et al., 1991) to create a 3d dataset, adding two time-lagged versions of x as
coordinates.1"
MODEL COMPARISONS,0.1683366733466934,"A general observation is that indeed all four models are quite powerful for reconstructing the
underlying DS. However, in most comparisons the dendPLRNN had an edge over the other methods,"
WE POINT OUT THAT THIS MAY ALREADY IMPOSE A RESTRICTION FOR METHODS LIKE SINDY AS ONE MOVES TO VERY,0.17034068136272545,"1We point out that this may already impose a restriction for methods like SINDy as one moves to very
high-dimensional systems."
WE POINT OUT THAT THIS MAY ALREADY IMPOSE A RESTRICTION FOR METHODS LIKE SINDY AS ONE MOVES TO VERY,0.17234468937875752,"or came out second after SINDy, especially when trained by BPTT+TF. SINDy tends to outperform
the dendPLRNN on the Lorenz-63 DS, but it completely fails on the bursting-neuron and population
model examples, and on the real EEG data, and generally becomes comparatively slow to train on
high-dimensional systems. This can be explained by the fact that SINDy already has the correct
functional form for the Lorenz-63 (and also Lorenz-96) DS: Both of these have a strictly polynomial
form (see Eq. 14 and Eq. 18 in Appx. 6.4), and SINDy works with a set of polynomial library
functions to begin with. Hence, SINDy only needs to pick out the right terms from its expansion to
succeed, giving it a clear advantage on these model systems by design. On the other hand, as revealed
in Table S2, it completely fails on systems which have a different (non-polynomial) functional form
(or when the true form, as in the EEG case, is simply not known). SINDy therefore appears less
suitable as a general framework for DS reconstruction if an appropriate library of basis functions
cannot be specified a priori, unlike the other methods."
WE POINT OUT THAT THIS MAY ALREADY IMPOSE A RESTRICTION FOR METHODS LIKE SINDY AS ONE MOVES TO VERY,0.1743486973947896,"While our conclusion is that essentially all of the three tested models LSTM-MSM, RC, and dend-
PLRNN are suitable for reconstruction of arbitrary DS even in very challenging data situations
(Table S2, bottom), LSTM-MSM and RC performed worse on average and have other profound
disadvantages compared to our method: First, they are quite complex in their architectures and
hence not easily interpretable, i.e. much harder to track and analyze mathematically.2 In contrast, as
summarized in Sec. 3.3, the dendPLRNN is a continuous PWL map and as such comes with a huge
bulk of already existing theoretical results (Schmidt et al., 2021; Monfared & Durstewitz, 2020b;a),
as well as with mathematical tractability (see Fig. 2a and Appx. 6.5.1). On top, the dendPLRNN
achieves reconstruction of all DS in (much) lower dimensions than the RC or LSTM-MSM (see Table
S2), further adding to its better interpretability. Second, by embedding the dendPLRNN within a
SVAE (Archer et al., 2015) framework we also obtain uncertainty estimates on the state trajectories
and can perform posterior inference, features that the other models lack."
CONCLUSIONS,0.17635270541082165,"5
CONCLUSIONS"
CONCLUSIONS,0.17835671342685372,"In this work we augmented PLRNNs (Durstewitz, 2017; Koppe et al., 2019) by a linear spline basis
expansion inspired by principles of dendritic computation. We show mathematically that by doing so
we remain within the theoretical framework of continuous PWL maps and hence can harvest a huge
bulk of existing DS theory (Sec. 3.3), while at the same time achieving better performance with less
parameters and in lower dimensions. Another contribution of this work is transferring the PLRNN
into the framework of SVAEs which allow for fast and scalable inference and training. These are
two key advantages from both a scientific perspective where mechanistic insight and understanding
of the system under study are sought, and a prediction perspective where we are also interested in
uncertainty estimates."
CONCLUSIONS,0.18036072144288579,"We close by pointing out two open issues: First, somewhat surprisingly, the BPTT+TF approach
to model training clearly outperformed the more sophisticated VI approach. This could be rooted
in suboptimal encoder models or in suboptimal sampling from the approximate posterior: While
BPTT+TF assesses longer bits of trajectory during optimization, in VI single time-point samples are
drawn and the temporal consistency is ensured only through the Kullback-Leibler term in the ELBO.
Other more expressive yet still fast to compute encoder models, e.g., based on normalizing flows
(Rezende & Mohamed, 2015), may boost performance. Smart initialization techniques (Talathi &
Vartak, 2016) or specific annealing and curriculum training protocols (as used in Koppe et al. (2019))
are other amendments to consider. Second, we felt that quantitative measures for assessing the quality
of DS reconstructions in high-dimensional, high-noise situations are an interesting research topic
in their own right. It is known that “classical” DS measures like Lyapunov spectra or correlation
dimensions (Kantz & Schreiber, 2004) are very hard to robustly assess for higher-dimensional or
more noisy systems (Schreiber & Kantz, 1996), and are often not even known for comparatively
simple models. Yet, the geometrical and temporal measures employed here come with their own
pitfalls, some of them alluded to in Appx. 6.2."
CONCLUSIONS,0.18236472945891782,ACKNOWLEDGMENTS
CONCLUSIONS,0.1843687374749499,"2This is especially true for RC. Moreover, the fact that only the weights of the linear output layer are trainable
while the recurrent connections within the reservoirs are static, may raise the question of what precisely is learnt
in terms of dynamics if the reservoirs themselves cannot adapt to the DS at hand."
ETHICS STATEMENT,0.18637274549098196,"Ethics statement
The current work performs theoretical analysis and basic research on a novel type
of RNN architecture, which is evaluated exclusively on a set of simulated physical and biological
systems (no human or animal subjects involved, no privacy concerns etc.). Although it is conceivable
that the current models, training algorithms, and results may be used to develop prediction algorithms
in sensitive domains (like medical time series or tracking consumer data), there are no immediate
ethical implications from this work as far as we can see."
REPRODUCIBILITY STATEMENT,0.18837675350701402,"Reproducibility statement
All theoretical results in this paper were carefully and thoroughly
proven, with all proofs and detailed derivations available in the Appendix. Likewise, we will make
available all code used in the empirical section in a way that will allow others to easily reproduce
the results from this paper. This means we will include everything, starting with the code for the
benchmark models and simulations, the simulated time series data used for evaluation themselves,
the code for our own model and training algorithms, up to the meta-files that produce the figures in
this work, on our lab github site. All of this will be clearly documented."
REFERENCES,0.1903807615230461,REFERENCES
REFERENCES,0.19238476953907815,"Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. Black box
variational inference for state space models. arXiv preprint arXiv:1511.07367, 2015. URL
http://arxiv.org/abs/1511.07367."
REFERENCES,0.19438877755511022,"Omri Azencot, N. Benjamin Erichson, Vanessa Lin, and Michael W. Mahoney. Forecasting Se-
quential Data using Consistent Koopman Autoencoders. In Proceedings of the 37th International
Conference on Machine Learning, 2020. URL http://arxiv.org/abs/2003.02236."
REFERENCES,0.1963927855711423,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv:1607.06450
[cs, stat], July 2016. URL http://arxiv.org/abs/1607.06450. arXiv: 1607.06450
version: 1."
REFERENCES,0.19839679358717435,"Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic Differentiation in Machine Learning: a Survey. Journal of Machine Learning Research,
18(153):1–43, 2018. ISSN 1533-7928. URL http://jmlr.org/papers/v18/17-468.
html."
REFERENCES,0.20040080160320642,"Justin Bayer, Maximilian Soelch, Atanas Mirchev, Baris Kayalibay, and Patrick van der Smagt.
Mind the gap when conditioning amortised inference in sequential latent-variable models. URL
http://arxiv.org/abs/2101.07046."
REFERENCES,0.20240480961923848,"David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians.
Journal of the American Statistical Association, 112(518):859–877, Apr 2017. ISSN 1537-274X.
doi: 10.1080/01621459.2017.1285773. URL http://dx.doi.org/10.1080/01621459.
2017.1285773."
REFERENCES,0.20440881763527055,"Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations from
data by sparse identification of nonlinear dynamical systems. Proc Natl Acad Sci U S A, 113
(15):3932–3937, 2016. ISSN 0027-8424. doi: 10.1073/pnas.1517384113. URL https://www.
ncbi.nlm.nih.gov/pmc/articles/PMC4839439/."
REFERENCES,0.20641282565130262,"Steven L. Brunton, Bingni W. Brunton, Joshua L. Proctor, Eurika Kaiser, and J. Nathan Kutz.
Chaos as an intermittently forced linear system. Nat Commun, 8(1):19, 2017. ISSN 2041-
1723. doi: 10.1038/s41467-017-00030-8. URL http://www.nature.com/articles/
s41467-017-00030-8."
REFERENCES,0.20841683366733466,"Kathleen Champion, Bethany Lusch, J. Nathan Kutz, and Steven L. Brunton. Data-driven discovery
of coordinates and governing equations. Proc Natl Acad Sci USA, 116(45):22445–22451, 2019.
ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1906995116. URL http://www.pnas.org/
lookup/doi/10.1073/pnas.1906995116."
REFERENCES,0.21042084168336672,"Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural Ordinary
Differential Equations. In Advances in Neural Information Processing Systems 31, 2018. URL
http://arxiv.org/abs/1806.07366."
REFERENCES,0.2124248496993988,"Shizhe Chen, Ali Shojaie, and Daniela M. Witten. Network Reconstruction From High-Dimensional
Ordinary Differential Equations. Journal of the American Statistical Association, 112(520):
1697–1707, 2017. ISSN 0162-1459. doi: 10.1080/01621459.2016.1229197. URL https:
//doi.org/10.1080/01621459.2016.1229197."
REFERENCES,0.21442885771543085,"Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and L´eon Bottou. Symplectic Recurrent Neural
Networks. In Proceedings of the 8th International Conference on Learning Representations, 2020.
URL http://arxiv.org/abs/1909.13334."
REFERENCES,0.21643286573146292,"Zhicheng Cui, Wenlin Chen, and Yixin Chen. Multi-scale convolutional neural networks for time
series classification.
Computing Research Repository, abs/1603.06995, 2016.
URL http:
//arxiv.org/abs/1603.06995."
REFERENCES,0.218436873747495,"Brian M. de Silva, Kathleen Champion, Markus Quade, Jean-Christophe Loiseau, J. Nathan Kutz,
and Steven L. Brunton. PySINDy: A Python package for the Sparse Identification of Nonlinear
Dynamics from Data. arXiv preprint arXiv:2004.08424, 2020. URL http://arxiv.org/
abs/2004.08424."
REFERENCES,0.22044088176352705,"Kenji Doya. Bifurcations in the learning of recurrent neural networks. In Proceedings of the
1992 IEEE International Symposium on Circuits and Systems, 1992. ISBN 978-0-7803-0593-9.
doi: 10.1109/ISCAS.1992.230622. URL http://ieeexplore.ieee.org/document/
230622/."
REFERENCES,0.22244488977955912,"Daniel Durstewitz. Implications of synaptic biophysics for recurrent network dynamics and ac-
tive memory.
Neural Networks, 22(8):1189–1200, 2009.
ISSN 08936080.
doi: 10.1016/
j.neunet.2009.07.016. URL https://linkinghub.elsevier.com/retrieve/pii/
S0893608009001622."
REFERENCES,0.22444889779559118,"Daniel Durstewitz.
A state space approach for piecewise-linear recurrent neural networks for
identifying computational dynamics from neural measurements. PLoS Comput. Biol., 13(6):
e1005542, 2017. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1005542."
REFERENCES,0.22645290581162325,"Mark I Feigin. The increasingly complex structure of the bifurcation tree of a piecewise-smooth
system. Journal of Applied Mathematics and Mechanics, 59(6):853–863, 1995. ISSN 0021-
8928. doi: 10.1016/0021-8928(95)00118-2. URL https://www.sciencedirect.com/
science/article/pii/0021892895001182."
REFERENCES,0.22845691382765532,"Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, and Ole Winther. Sequential neural models
with stochastic layers. URL http://arxiv.org/abs/1605.07571."
REFERENCES,0.23046092184368738,"Loren M. Frank, Uri T. Eden, Victor Solo, Matthew A. Wilson, and Emery N. Brown. Contrasting
patterns of receptive field plasticity in the hippocampus and the entorhinal cortex: An adaptive
filtering approach. 22(9):3817–3830. ISSN 0270-6474, 1529-2401. doi: 10.1523/JNEUROSCI.
22-09-03817.2002. URL https://www.jneurosci.org/content/22/9/3817. Pub-
lisher: Society for Neuroscience Section: ARTICLE."
REFERENCES,0.23246492985971945,"Zoubin Ghahramani and Sam T Roweis. Learning nonlinear dynamical systems using an EM
algorithm. In Advances in Neural Information Processing Systems 11, 1998."
REFERENCES,0.23446893787575152,"Laurent Girin, Simon Leglaive, Xiaoyu Bie, Julien Diard, Thomas Hueber, and Xavier Alameda-
Pineda.
Dynamical Variational Autoencoders: A Comprehensive Review.
arXiv preprint
arXiv:2008.12595, 2020. URL http://arxiv.org/abs/2008.12595."
REFERENCES,0.23647294589178355,"Sam Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian Neural Networks. In Advances
in Neural Information Processing Systems 32, 2019. URL http://arxiv.org/abs/1906.
01563."
REFERENCES,0.23847695390781562,"Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data
mining, inference, and prediction. Springer Science & Business Media, 2009."
REFERENCES,0.24048096192384769,"Niklas Heim, V´aclav ˇSm´ıdl, and Tom´aˇs Pevn´y. Rodent: Relevance determination in differential
equations. arXiv preprint arXiv:1912.00656, 2019. URL http://arxiv.org/abs/1912.
00656."
REFERENCES,0.24248496993987975,"John R. Hershey and Peder A. Olsen. Approximating the kullback leibler divergence between
gaussian mixture models. 2007 IEEE International Conference on Acoustics, Speech and Signal
Processing - ICASSP ’07, 4:IV–317–IV–320, 2007."
REFERENCES,0.24448897795591182,"Stephen J. Hogan, L. Higham, and T. C. L. Griffin. Dynamics of a piecewise linear map with a gap.
Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 463(2077):
49–65, 2007. doi: 10.1098/rspa.2006.1735. URL https://royalsocietypublishing.
org/doi/abs/10.1098/rspa.2006.1735."
REFERENCES,0.24649298597194388,"Su-Yun Huang, Yi-Ren Yeh, and Shinto Eguchi. Robust kernel principal component analysis. 21(11):
3179–3213. ISSN 0899-7667. doi: 10.1162/neco.2009.02-08-706. Conference Name: Neural
Computation."
REFERENCES,0.24849699398797595,"Michael H¨ausser, Nelson Spruston, and Greg J. Stuart. Diversity and Dynamics of Dendritic Signaling.
Science, 290(5492):739–744, 2000. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.290.5492.
739. URL https://science.sciencemag.org/content/290/5492/739."
REFERENCES,0.250501002004008,"Herbert Jaeger and Harald Haas. Harnessing Nonlinearity: Predicting Chaotic Systems and Sav-
ing Energy in Wireless Communication. Science, 304(5667):78–80, 2004. ISSN 0036-8075,
1095-9203. doi: 10.1126/science.1091277. URL https://science.sciencemag.org/
content/304/5667/78."
REFERENCES,0.25250501002004005,"Holger Kantz and Thomas Schreiber. Nonlinear time series analysis, volume 7. Cambridge university
press, 2004."
REFERENCES,0.2545090180360721,"Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van der Smagt. Deep Variational
Bayes Filters: Unsupervised Learning of State Space Models from Raw Data. In Proceedings
of the 5th International Conference on Learning Representations, 2017. URL http://arxiv.
org/abs/1605.06432."
REFERENCES,0.2565130260521042,"Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings
of the 3rd International Conference on Learning Representations, 2015. URL http://arxiv.
org/abs/1412.6980."
REFERENCES,0.25851703406813625,"Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In Proceedings of the
2nd International Conference on Learning Representations, 2014. URL http://arxiv.org/
abs/1312.6114."
REFERENCES,0.2605210420841683,"Christof Koch. Biophysics of computation: information processing in single neurons. Oxford
university press, 2004."
REFERENCES,0.2625250501002004,"Georgia Koppe, Hazem Toutounji, Peter Kirsch, Stefanie Lis, and Daniel Durstewitz. Identifying
nonlinear dynamical systems via generative recurrent neural networks with applications to fMRI.
PLOS Computational Biology, 15(8):e1007263, 2019. ISSN 1553-7358. doi: 10.1371/journal.
pcbi.1007263. URL https://journals.plos.org/ploscompbiol/article?id=
10.1371/journal.pcbi.1007263."
REFERENCES,0.26452905811623245,"Rahul G. Krishnan, Uri Shalit, and David Sontag. Structured Inference Networks for Nonlinear
State Space Models. In 31st AAAI Conference on Artificial Intelligence, 2017. URL http:
//arxiv.org/abs/1609.09869."
REFERENCES,0.2665330661322645,"Itamar Daniel Landau and Haim Sompolinsky. Coherent chaos in a recurrent neural network with
structured connectivity. PLoS Comput Biol, 14(12):e1006309, 2018. ISSN 1553-7358. doi:
10.1371/journal.pcbi.1006309. URL https://dx.plos.org/10.1371/journal.pcbi.
1006309."
REFERENCES,0.2685370741482966,"Edward N Lorenz. Deterministic nonperiodic flow. Journal of atmospheric sciences, 20(2):130–141,
1963."
REFERENCES,0.27054108216432865,"Edward N Lorenz. Predictability: A problem partly solved. In Proc. Seminar on predictability,
volume 1, 1996."
REFERENCES,0.2725450901803607,"Niru Maheswaranathan, Alex H. Williams, Matthew D. Golub, Surya Ganguli, and David Sussillo.
Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics.
In Advances in neural information processing systems 32, 2019a. URL https://www.ncbi.
nlm.nih.gov/pmc/articles/PMC7416638/."
REFERENCES,0.2745490981963928,"Niru Maheswaranathan, Alex H. Williams, Matthew D. Golub, Surya Ganguli, and David Sussillo.
Universality and individuality in neural dynamics across large populations of recurrent networks.
In Advances in Neural Information Processing Systems 32, 2019b. URL https://www.ncbi.
nlm.nih.gov/pmc/articles/PMC7416639/."
REFERENCES,0.27655310621242485,"Bartlett W. Mel. Synaptic integration in an excitable dendritic tree. Journal of Neurophysiology,
70(3):1086–1101, 1993. ISSN 0022-3077, 1522-1598. doi: 10.1152/jn.1993.70.3.1086. URL
https://www.physiology.org/doi/10.1152/jn.1993.70.3.1086."
REFERENCES,0.2785571142284569,"Bartlett W. Mel. Information Processing in Dendritic Trees. Neural Computation, 6(6):1031–1085,
1994. ISSN 0899-7667. doi: 10.1162/neco.1994.6.6.1031. URL https://doi.org/10.
1162/neco.1994.6.6.1031."
REFERENCES,0.280561122244489,"Bartlett W. Mel.
Why Have Dendrites?
A Computational Perspective.
In Den-
drites. Oxford University Press,
1999.
ISBN 978-0-19-172420-6.
URL https:
//oxford.universitypressscholarship.com/view/10.1093/acprof:
oso/9780198566564.001.0001/acprof-9780198566564-chapter-016."
REFERENCES,0.28256513026052105,"Nima Mohajerin and Steven L. Waslander. Multi-step prediction of dynamic systems with recur-
rent neural networks. arXiv:1806.00526 [cs], 2018. URL http://arxiv.org/abs/1806.
00526."
REFERENCES,0.2845691382765531,"Zahra Monfared and Daniel Durstewitz. Existence of n-cycles and border-collision bifurcations
in piecewise-linear continuous maps with applications to recurrent neural networks. Nonlinear
Dyn, 101(2):1037–1052, 2020a. ISSN 1573-269X. doi: 10.1007/s11071-020-05841-x. URL
https://doi.org/10.1007/s11071-020-05841-x."
REFERENCES,0.2865731462925852,"Zahra Monfared and Daniel Durstewitz. Transformation of ReLU-based recurrent neural networks
from discrete-time to continuous-time. In Proceedings of the 37th International Conference on Ma-
chine Learning, 2020b. URL http://proceedings.mlr.press/v119/monfared20a.
html."
REFERENCES,0.28857715430861725,"Alexander Norcliffe, Cristian Bodnar, Ben Day, Jacob Moss, and Pietro Li`o. Neural ODE Processes.
In Proceedings of the 9th International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=27acGyyI1BY."
REFERENCES,0.2905811623246493,"Chethan Pandarinath, Daniel J. O’Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D. Stavisky,
Jonathan C. Kao, Eric M. Trautmann, Matthew T. Kaufman, Stephen I. Ryu, Leigh R. Hochberg,
Jaimie M. Henderson, Krishna V. Shenoy, L. F. Abbott, and David Sussillo. Inferring single-trial
neural population dynamics using sequential auto-encoders. Nature Methods, 15(10):805–815,
2018. ISSN 1548-7105. doi: 10.1038/s41592-018-0109-9. URL https://www.nature.
com/articles/s41592-018-0109-9."
REFERENCES,0.2925851703406814,"Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In Proceedings of the 30th International Conference on Machine Learning, 2013. URL
http://proceedings.mlr.press/v28/pascanu13.html."
REFERENCES,0.29458917835671344,"Jaideep Pathak, Brian Hunt, Michelle Girvan, Zhixin Lu, and Edward Ott. Model-Free Prediction of
Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach. Phys. Rev.
Lett., 120(2):024102, 2018. ISSN 0031-9007, 1079-7114. doi: 10.1103/PhysRevLett.120.024102.
URL https://link.aps.org/doi/10.1103/PhysRevLett.120.024102."
REFERENCES,0.2965931863727455,"Mahashweta Patra.
Multiple Attractor Bifurcation in Three-Dimensional Piecewise Linear
Maps.
Int. J. Bifurcation Chaos, 28(10):1830032, 2018.
ISSN 0218-1274.
doi:
10.
1142/S021812741830032X.
URL https://www.worldscientific.com/doi/abs/
10.1142/S021812741830032X."
REFERENCES,0.2985971943887776,"Barak Pearlmutter.
Dynamic recurrent neural networks, 1990.
URL https://kilthub.
cmu.edu/articles/journal_contribution/Dynamic_recurrent_neural_
networks/6605018/1."
REFERENCES,0.30060120240480964,"Panayiota Poirazi and Athanasia Papoutsi.
Illuminating dendritic function with compu-
tational models.
Nat Rev Neurosci, 21(6):303–321, 2020.
ISSN 1471-003X, 1471-
0048. doi: 10.1038/s41583-020-0301-7. URL http://www.nature.com/articles/
s41583-020-0301-7."
REFERENCES,0.3026052104208417,"Panayiota Poirazi, Terrence Brannon, and Bartlett W Mel. Pyramidal neuron as two-layer neural
network. Neuron, 37(6):989–999, 2003."
REFERENCES,0.3046092184368738,"Cunle Qian, Xuyun Sun, Shaomin Zhang, Dong Xing, Hongbao Li, Xiaoxiang Zheng, Gang Pan,
and Yiwen Wang. Nonlinear modeling of neural interaction for spike prediction using the staged
point-process model. 30(12):3189–3226. ISSN 0899-7667. doi: 10.1162/neco a 01137. URL
https://doi.org/10.1162/neco_a_01137."
REFERENCES,0.3066132264529058,"Maziar Raissi. Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential
Equations. Journal of Machine Learning Research, 19(25):1–24, 2018. URL http://jmlr.
org/papers/v19/18-046.html."
REFERENCES,0.30861723446893785,"Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Multistep neural networks for data-
driven discovery of nonlinear dynamical systems. arXiv:1801.01236 [nlin, physics:physics, stat],
2018. URL http://arxiv.org/abs/1801.01236."
REFERENCES,0.3106212424849699,"Danilo Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows. In Pro-
ceedings of the 32nd International Conference on Machine Learning, 2015.
URL http:
//proceedings.mlr.press/v37/rezende15.html."
REFERENCES,0.312625250501002,"Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and
Approximate Inference in Deep Generative Models. In Proceedings of the 31st International
Conference on Machine Learning, 2014. URL http://arxiv.org/abs/1401.4082."
REFERENCES,0.31462925851703405,"Samuel H. Rudy, Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Data-driven discovery
of partial differential equations. Science Advances, 3(4):e1602614, 2017. ISSN 2375-2548. doi:
10.1126/sciadv.1602614. URL https://advances.sciencemag.org/content/3/4/
e1602614."
REFERENCES,0.3166332665330661,"David E. Rumelhart, Geoffrey E. Hinton, and R. J. Williams. Learning Internal Representations by
Error Propagation, volume 1, pp. 318–362. Bradford Books, Cambridge MA, 1986."
REFERENCES,0.3186372745490982,"Tim Sauer, James A Yorke, and Martin Casdagli. Embedology. Journal of statistical Physics, 65(3):
579–616, 1991."
REFERENCES,0.32064128256513025,"Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. In Proceedings of the 2nd International Conference on
Learning Representations, 2014. URL http://arxiv.org/abs/1312.6120."
REFERENCES,0.3226452905811623,"Gerwin Schalk, Dennis J. McFarland, Thilo Hinterberger, Niels Birbaumer, and Jonathan R. Wolpaw.
BCI2000: a general-purpose brain-computer interface (BCI) system. 51(6):1034–1043, 2000.
ISSN 0018-9294. doi: 10.1109/TBME.2004.827072."
REFERENCES,0.3246492985971944,"Jackie Schiller, Guy Major, Helmut J. Koester, and Yitzhak Schiller. NMDA spikes in basal dendrites
of cortical pyramidal neurons. Nature, 404(6775):285–289, 2000. ISSN 1476-4687. doi: 10.1038/
35005094. URL https://www.nature.com/articles/35005094."
REFERENCES,0.32665330661322645,"Dominik Schmidt, Georgia Koppe, Zahra Monfared, Max Beutelspacher, and Daniel Durstewitz.
Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies.
In Proceedings of the 9th International Conference on Learning Representations, 2021. URL
http://arxiv.org/abs/1910.03471."
REFERENCES,0.3286573146292585,"Thomas Schreiber and Holger Kantz. Observing and Predicting Chaotic Signals: Is 2% Noise
Too Much?, pp. 43–65. Springer Berlin Heidelberg, Berlin, Heidelberg, 1996. ISBN 978-3-
642-80254-6. doi: 10.1007/978-3-642-80254-6 3. URL https://doi.org/10.1007/
978-3-642-80254-6_3."
REFERENCES,0.3306613226452906,"A. Shalova and I. Oseledets. Deep Representation Learning for Dynamical Systems Modeling. arXiv
preprint arXiv:2002.05111, 2020a."
REFERENCES,0.33266533066132264,"A. Shalova and I. Oseledets. Tensorized Transformer for Dynamical Systems Modeling. arXiv
preprint arXiv:2006.03445, 2020b."
REFERENCES,0.3346693386773547,"Martin Stemmler and Christof Koch. How voltage-dependent conductances can adapt to maximize the
information encoded by neuronal firing rate. Nature Neuroscience, 2(6):521–527, 1999. ISSN 1546-
1726. doi: 10.1038/9173. URL https://www.nature.com/articles/nn0699_521."
REFERENCES,0.3366733466933868,"M. Storace and O. De Feo. Piecewise-linear approximation of nonlinear dynamical systems. IEEE
Transactions on Circuits and Systems I: Regular Papers, 51(4):830–842, April 2004. ISSN 1558-
0806. doi: 10.1109/TCSI.2004.823664. Conference Name: IEEE Transactions on Circuits and
Systems I: Regular Papers."
REFERENCES,0.33867735470941884,"Robert Strauss. Augmenting neural differential equations to model unknown dynamical systems
with incomplete state information.
arXiv:2008.08226 [physics, q-bio], 2020.
URL http:
//arxiv.org/abs/2008.08226."
REFERENCES,0.3406813627254509,"Floris Takens. Detecting strange attractors in turbulence. In Dynamical Systems and Turbulence,
Warwick 1980, volume 898, pp. 366–381. Springer, 1981. ISBN 978-3-540-11171-9 978-3-540-
38945-3. URL http://link.springer.com/10.1007/BFb0091924."
REFERENCES,0.342685370741483,"Sachin S. Talathi and Aniket Vartak. Improving performance of recurrent neural network with relu
nonlinearity. In Proceedings of the 4th International Conference on Learning Representations,
2016. URL http://arxiv.org/abs/1511.03771."
REFERENCES,0.34468937875751504,"Adam P. Trischler and Gabriele M.T. D’Eleuterio. Synthesis of recurrent neural networks for
dynamical system simulation. Neural Networks, 80:67–78, 2016. ISSN 08936080. doi: 10.1016/
j.neunet.2016.04.001. URL https://linkinghub.elsevier.com/retrieve/pii/
S0893608016300314."
REFERENCES,0.3466933867735471,"Pantelis R. Vlachas, Wonmin Byeon, Zhong Y. Wan, Themistoklis P. Sapsis, and Petros Koumout-
sakos. Data-driven forecasting of high-dimensional chaotic systems with long short-term mem-
ory networks.
Proc. R. Soc. A., 474(2213):20170844, 2018.
ISSN 1364-5021, 1471-2946.
doi: 10.1098/rspa.2017.0844. URL https://royalsocietypublishing.org/doi/
10.1098/rspa.2017.0844."
REFERENCES,0.3486973947895792,"Grace Wahba. Spline models for observational data. SIAM, 1990."
REFERENCES,0.35070140280561124,"Ronald J. Williams and David Zipser. A learning algorithm for continually running fully recurrent
neural networks. Neural Computation, 1(2):270–280, June 1989. ISSN 0899-7667, 1530-888X.
doi: 10.1162/neco.1989.1.2.270. URL https://direct.mit.edu/neco/article/1/
2/270-280/5490."
REFERENCES,0.3527054108216433,"Enoch Yeung, Soumya Kundu, and Nathan Hodas. Learning Deep Neural Network Representations
for Koopman Operators of Nonlinear Dynamical Systems. arXiv preprint arXiv:1708.06850, 2017.
doi: 10.23919/ACC.2019.8815339."
REFERENCES,0.35470941883767537,"Yuan Yin, Vincent LE Guen, J´er´emie Dona, Emmanuel de Bezenac, Ibrahim Ayed, Nicolas Thome,
and Patrick Gallinari. Augmenting Physical Models with Deep Networks for Complex Dynamics
Forecasting. In Proceedings of the 9th International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=kmG8vRXTFv."
REFERENCES,0.35671342685370744,"Xun Zheng, Manzil Zaheer, Amr Ahmed, Yuan Wang, Eric P. Xing, and Alexander J. Smola. State
Space LSTM Models with Particle MCMC Inference. arXiv preprint arXiv:1711.11179, 2017.
URL deep."
REFERENCES,0.3587174348697395,"Qunxi Zhu, Yao Guo, and Wei Lin. Neural Delay Differential Equations. In Proceedings of the 9th
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=Q1jmmQz72M2."
APPENDIX,0.36072144288577157,"6
APPENDIX"
FURTHER METHODOLOGICAL DETAILS,0.3627254509018036,"6.1
FURTHER METHODOLOGICAL DETAILS"
FURTHER METHODOLOGICAL DETAILS,0.36472945891783565,"Manifold Attractor Regularization
As proposed in Schmidt et al. (2021), to encourage the
discovery of long-term dependencies and slow time scales in the data, a subset of Mreg ≤M states
was regularized by adding the following term to the ELBO for the VI approach:"
FURTHER METHODOLOGICAL DETAILS,0.3667334669338677,Lreg = λ  
FURTHER METHODOLOGICAL DETAILS,0.3687374749498998,"Mreg
X"
FURTHER METHODOLOGICAL DETAILS,0.37074148296593185,"i=1
(Aii −1)2 +"
FURTHER METHODOLOGICAL DETAILS,0.3727454909819639,"Mreg
X i=1 M
X"
FURTHER METHODOLOGICAL DETAILS,0.374749498997996,"j̸=i
(Wij)2 +"
FURTHER METHODOLOGICAL DETAILS,0.37675350701402804,"Mreg
X"
FURTHER METHODOLOGICAL DETAILS,0.3787575150300601,"i=1
h2
i "
FURTHER METHODOLOGICAL DETAILS,0.3807615230460922,".
(6)"
FURTHER METHODOLOGICAL DETAILS,0.38276553106212424,"This regularization pushes the regularized subset of states toward a continuous set of marginally
stable fixed points that tends to form an attracting manifold in the full state space, which supports the
learning of systems with widely differing time scales, such as the bursting neuron model (cf. Sec.
4). We found that for all datasets this regularization significantly helped to discover the underlying
dynamics. To put it on equal grounds with the regularization term, the ELBO was divided by the
number of time steps T of a given batch. Regularization settings used are summarized in Table S1
along other hyper-parameter settings."
FURTHER METHODOLOGICAL DETAILS,0.3847695390781563,"BPTT-TF
To train a deterministic version of the dendPLRNN, we employ BPTT with a scheduled
version of TF (Williams & Zipser, 1989; Pearlmutter, 1990). To do so, we choose an “identity-
mapping” for the observation model ˆxt = Izt, where I ∈RN×M with Ikk = 1 if k ≤N and zeroes
everywhere else. This allows us to regularly replace latent states with observations to “recalibrate”
the model and break trajectory divergence in case of chaotic dynamics. Consider a time series
{x1, x2, · · · , xT } generated by a DS we want to reconstruct. At times lτ + 1, l ∈N0, where τ ≥1
is the forcing interval, we replace the first N latent states by observations ˆzk,lτ+1 = xk,lτ+1, k ≤N.
The remaining latent states, ˆzk,lτ+1 = zk,lτ+1, k > N, remain unaffected by the forcing. This
means that we optimize the dendPLRNN such that a subspace of the latent space directly maps
to the observed time series variables. The forcing interval τ is a hyperparameter, with optimal
settings varying depending on the dataset. The settings we chose are summarized in Table S1. With
F = {lτ + 1}l∈N0, the dendPLRNN updates can then be written as"
FURTHER METHODOLOGICAL DETAILS,0.3867735470941884,"zt+1 =
dendPLRNN(˜zt)
if t ∈F
dendPLRNN(zt)
else
.
(7)"
FURTHER METHODOLOGICAL DETAILS,0.38877755511022044,"The loss is calculated prior to the forcing, such that Lt = ∥xt −Izt∥2
2 for every time step. To
improve performance we employ a mean-centred dendPLRNN (for details see next paragraph). In
the evaluation phase, the trained dendPLRNN is simulated freely without any forcing. As the model
is deterministic, the initial condition z1 = [x1, Lx1]T is estimated from the first data point x1 with a
matrix L ∈R(M−N)×N which is jointly learned with the other model parameters."
FURTHER METHODOLOGICAL DETAILS,0.3907815631262525,"Mean-Centered dendPLRNN
Layer normalization has recently been developed as a way of
significantly improving RNN training (Ba et al., 2016). Here we adapt the idea of layer normalization
to the piecewise-linear nature of our dendPLRNN. Instead of fully standardizing the latent states at
every time step before applying the activation function, we only mean-center them:
zt = Azt−1 + W ϕ
 
M(zt−1)

+ h0,
(8)"
FURTHER METHODOLOGICAL DETAILS,0.3927855711422846,"where ϕ(·) is given in Eq. 4 and M(zt−1) = zt−1 −µt−1 = zt−1 −1 1 M M
X"
FURTHER METHODOLOGICAL DETAILS,0.39478957915831664,"j=1
zj,t−1, where"
FURTHER METHODOLOGICAL DETAILS,0.3967935871743487,"1 ∈RM is a vector of ones. Note that this mean-centering is linear and can be rewritten as a
matrix-multiplication
M(zt−1) = zt−1 −µt−1 = 1 M  
"
FURTHER METHODOLOGICAL DETAILS,0.39879759519038077,"M −1
−1
· · ·
−1
−1
M −1
· · ·
−1
· · ·
· · ·
· · ·
· · ·
−1
−1
· · ·
M −1 "
FURTHER METHODOLOGICAL DETAILS,0.40080160320641284,"
zt−1 = Mzt−1.
(9)"
FURTHER METHODOLOGICAL DETAILS,0.4028056112224449,"As Remark 1 points out, all results about the tractability of the dendPLRNN also hold for the
mean-centred dendPLRNN."
FURTHER METHODOLOGICAL DETAILS,0.40480961923847697,"State clipping
Since the ReLU function used in the dendPLRNN is non-saturating, states may
diverge to infinity. As Theorem 2 guarantees, there is a simple and natural way to construct a “clipped”
dendPLRNN"
FURTHER METHODOLOGICAL DETAILS,0.40681362725450904,"zt = Azt−1 + W B
X"
FURTHER METHODOLOGICAL DETAILS,0.4088176352705411,"b=1
αb

max(0, zt−1 −hb) −max(0, zt−1)

+ h0.
(10)"
FURTHER METHODOLOGICAL DETAILS,0.41082164328657317,"Note that the results of Theorem 2 also hold true when the manifold attractor regularization is applied.
This is detailed in Proposition 2 further below."
FURTHER METHODOLOGICAL DETAILS,0.41282565130260523,"Approximate posterior and hyperparameter settings
To estimate the true unknown pos-
terior p(z|x), we make a Gaussian assumption for the approximate posterior qϕ(z|x)
=
N(µϕ(x), Σϕ(x)), where mean and covariance are functions of the observations. Without any
simplifying assumptions, the number of parameters in Σϕ(x) ∈RMT ×MT would scale unacceptably
with time series length T. We therefore made a mean field assumption and factorized qϕ(z|x)
across time. Specifically, a time-dependent mean µt,ϕ and covariance Σt,ϕ were parameterized
through stacked convolutional networks which take the observations {xt−w...xt+w} as inputs, with
w given by the largest kernel size. The mean is given by a 4-layer CNN with decreasing kernel
sizes (41, 31, 21 and 11, respectively), with the last layer of the CNN feeding into a fully connected
layer. For the diagonal covariance, the observations are mapped directly onto the logarithms of
the covariance through a single convolutional layer (with a kernel size of 41) feeding into a fully
connected layer. The classical motivation behind using CNNs rests on the assumption that the
data contains translationally invariant patterns, and that this allows the recognition model to embed
potentially meaningful temporal context into the latent representation (see e.g. Cui et al. (2016)). We
note that while the mean-field approximation is computationally highly efficient, it makes potentially
strongly simplifying assumptions (Blei et al., 2017; Bayer et al.) that may limit the ability of the
encoder model to approximate the true posterior."
FURTHER METHODOLOGICAL DETAILS,0.4148296593186373,"To train the dendPLRNN in the VI framework, Adam (Kingma & Ba, 2015), with a batch size of
1000 and learning rate of 1e −3 was used as the optimizer. For the training with BPTT, we used the
Adam optimizer with an initial learning rate of 1e −3 that was iteratively reduced during training
down to 1e −5. For each epoch we randomly sampled sequences of length Tseq = 500 (except for
the Lorenz-63 runs, where Tseq = 200 time steps were sufficient) from the total training data pool of
each dataset, which are then fed into the reconstruction method in batches of size 16. Initial weights
were drawn from a uniform distribution."
FURTHER METHODOLOGICAL DETAILS,0.4168336673346693,"To find optimal hyper-parameters we performed a grid search within λreg ∈{0, 0.01, 0.1, 1, 10}
(VI), τTF ∈{1, 5, 10, 25, 50, 100} (BPTT-TF), M ∈{5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 75, 100},
and B ∈{0, 1, 2, 5, 10, 20, 35, 50}. Hyper-parameters chosen for the benchmarks in Sec. 4 are
reported in Table S1 below (note that these may not fully agree with the ranges initially scanned, as
given above, since we attempted to adjust them further in order to approximately match the number
of parameters among models in Table S2)."
FURTHER METHODOLOGICAL DETAILS,0.4188376753507014,"Table S1: Hyperparameter settings for dendPLRNN VI/TF for the four different data sets from Sec.
4."
FURTHER METHODOLOGICAL DETAILS,0.42084168336673344,"Dataset
M
B
Mreg/M
λreg
τTF
Lorenz-63
22
20
1.0/−
1.0/−
−/25
Lorenz-96
42/50
50/30
1.0/−
1.0/−
−/10
Bursting Neuron
26
50/47
0.5/−
1.5/−
−/5
Neural Population Model
12/75
5/40
0.2/−
1.0/−
−/5
EEG
117/128
50/50
0.8/0.5
1.0/1.0
−/10"
PERFORMANCE MEASURES,0.4228456913827655,"6.2
PERFORMANCE MEASURES"
PERFORMANCE MEASURES,0.4248496993987976,"Geometrical measure
Dstsp used for evaluating attractor geometries (Fig. 4) measures the match
between the ground truth distribution ptrue(x) and the generated distribution pgen(x | z) through the"
PERFORMANCE MEASURES,0.42685370741482964,"discrete binning approximation (Koppe et al., 2019)"
PERFORMANCE MEASURES,0.4288577154308617,"Dstsp (ptrue(x), pgen(x | z)) ≈ K
X"
PERFORMANCE MEASURES,0.4308617234468938,"k=1
ˆp(k)
true(x) log"
PERFORMANCE MEASURES,0.43286573146292584,"ˆp(k)
true(x)"
PERFORMANCE MEASURES,0.4348697394789579,"ˆp(k)
gen(x | z) !"
PERFORMANCE MEASURES,0.43687374749499,",
(11)"
PERFORMANCE MEASURES,0.43887775551102204,"where K is the total number of bins, and ˆp(k)
true (x) and ˆp(k)
gen (x | z) are estimated as relative frequen-
cies through sampling trajectories from the benchmark DS and the trained reconstruction method,
respectively. A range of 2× the data standard deviation on each dimension was partioned into m
bins, yielding a total of K = mN bins, where N is the dimension of the ground truth system. Initial
transients are removed from sampled trajectories to ensure that the system has reached a limiting set.
If the bin size is chosen too large, important geometrical details may be lost, while if it is chosen too
small, noise and (low) sampling artifacts with many empty bins may misguide the approximation
above. Here we chose a bin number of m = 30 per dimension as an optimal compromise that
distinguished well between successful and poor reconstructions."
PERFORMANCE MEASURES,0.4408817635270541,"Since the number of bins needed to cover the relevant (populated) region of state space scales
exponentially with the number of dimensions, for high-dimensional systems evaluating Dstsp as
outlined above is not feasible. We therefore resorted to an approximation of the densities based on
Gaussian Mixture Models (GMMs), similar to a strategy outlined in (Koppe et al., 2019). Specifically,
we approximate the true data distribution by a GMM ptrue(x) ≈1"
PERFORMANCE MEASURES,0.44288577154308617,"T
PT
t=1 N(xt, Σ) with Gaussians
centered on the observed data points {xt} and covariance Σ, which we treat as a hyper-parameter
that determines the granularity of the spatial resolution (similar to the bin size in Eq. 11). We can
generate a likewise distribution by sampling trajectories from the trained models (or one very long
trajectory) and placing Gaussians on the sampled data points, pgen(x|z) ≈1"
PERFORMANCE MEASURES,0.44488977955911824,"L
PL
l=1 N(ˆxl | zl, Σ)
(in the case of VI, rather than sampling, one could also use the model’s distributional assumptions
to build this posterior across the observations). For the Kullback-Leibler divergence between two
GMMs efficient approximations are at hand (Hershey & Olsen, 2007). Here we employ a Monte
Carlo approximation"
PERFORMANCE MEASURES,0.4468937875751503,"eDstsp
 
ptrue(x), pgen(x|z)

≈1 n n
X"
PERFORMANCE MEASURES,0.44889779559118237,"i=1
log 1/T PT
t=1 N(x(i); xt, Σ)"
PERFORMANCE MEASURES,0.45090180360721444,"1/L PL
l=1 N(x(i); ˆxl, Σ)
,
(12)"
PERFORMANCE MEASURES,0.4529058116232465,"where n Monte-Carlo samples x(i) are drawn from the GMM representing ptrue. In practice, we set
the covariance Σ = σ2I equal to a scaled identity matrix, with a single hyperparamter σ2. Scanning
the range σ2 ∈{0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5}, we found that values for σ2 = 0.1 −1.0 to
differentiate best between good and bad reconstructions. We chose σ2 = 1.0 for numerical stability.
For this setting, Dstsp as derived with the binning method and eDstsp computed through the GMMs
also correlated highly on the low-dimensional benchmark systems (see Figure S1)."
PERFORMANCE MEASURES,0.45490981963927857,"Power Spectrum Correlation
The power spectrum correlations (PSC) were obtained by first
sampling time series of 100,000 time steps, standardizing these, and computing dimension-wise Fast
Fourier Transforms (using scipy.fft) for both the ground truth systems and model-simulated time
series. Individual power spectra were then slightly smoothed with a Gaussian kernel, normalized, and
the long, high-frequency tails of the spectra, mainly representing noise, were cut off. Smoothing width
σ and cutoff values scale linearily with the length of the time series used to compute the spectrum,
and were chosen by visual inspection of the individual spectra. Dimension-wise correlations between
smoothed power spectra were then averaged to obtain the reported PSC scores."
PERFORMANCE MEASURES,0.45691382765531063,"Mean Squared Prediction Error
A mean squared prediction error (PE) was computed across test
sets of length T = 1000 by initializing the reconstruction model with the benchmark time series up
to some time point t, from where it was then iterated forward by n time steps to yield a prediction at
time step t + n. The n-step ahead prediction error (PE) is then defined as the MSE between predicted
and true observations:"
PERFORMANCE MEASURES,0.4589178356713427,"PE(n) =
1
(T −n)"
PERFORMANCE MEASURES,0.46092184368737477,"T −n
X"
PERFORMANCE MEASURES,0.46292585170340683,"t=1
||xt+n −ˆxt+n||2
2.
(13)"
PERFORMANCE MEASURES,0.4649298597194389,"Note that for a chaotic system initially close trajectories will exponentially diverge, such that PEs for
too large prediction steps n are not meaningful anymore (in a chaotic system with noise, for large n"
PERFORMANCE MEASURES,0.46693386773547096,"Figure S1: Correlation between the binning approximation (m = 30) and the logarithm of the
GMM approximation (σ2 = 1) to Dstsp on the Lorenz-63 system for different noise realizations
and variances. Similar as reported for the KLZ approximation in Koppe et al. (2019) we observed a
logarithmic relation between the GMM and binning based measures."
PERFORMANCE MEASURES,0.46893787575150303,Figure S2: Example power spectrum for different values of the smoothing factor σ2.
PERFORMANCE MEASURES,0.4709418837675351,"the PE may be high even when estimated from two different runs of the same ground truth model
from the same initial condition; see (Koppe et al., 2019)). How quickly this happens depends on
the rate of exponential divergence as quantified through the system’s maximal Lyapunov exponent
(Kantz & Schreiber, 2004)."
FURTHER RESULTS,0.4729458917835671,"6.3
FURTHER RESULTS"
FURTHER RESULTS,0.4749498997995992,"Figure S3: Effect of basis expansion for dendPLRNN trained by VI. (a) Agreement in attractor
geometries (left) and 20-step ahead prediction error (right) for the bursting neuron system as a
function of the number of bases (B) for fixed numbers of total parameters. (b) Agreement in power
spectrum correlation (left) and 20-step ahead prediction error (right) for the Lorenz-96 system as a
function of the number of bases (B) for different numbers of latent states (M)."
FURTHER RESULTS,0.47695390781563124,"Figure S4: We observed that the dendPLRNN frequently underwent bifurcations between fixed point
or various cyclic solutions until it reaches the chaotic behavior of the ground truth system."
FURTHER RESULTS,0.4789579158316633,"Table S2: Comparison of dendPLRNN (Ours) trained by VI or BPTT+TF, and a standard PLRNN
(Schmidt et al., 2021), trained by VI or BPTT+TF on four DS benchmarks (top) and three challenging
data situations (bottom). Values are mean ± SEM."
FURTHER RESULTS,0.48096192384769537,"Dataset / Setting
Method
PSC
Dstsp
20-step PE
Dynamical variables
Parameters"
FURTHER RESULTS,0.48296593186372744,Lorenz
FURTHER RESULTS,0.4849699398797595,"dendPLRNN VI
0.997 ± 0.001
0.80 ± 0.25
2.1e−3 ± 0.2e−3
22
1032
dendPLRNN TF
0.997 ± 0.002
0.13 ± 0.18
9.2e−5 ± 2.8e−5
22
1032
PLRNN VI
0.94 ± 0.004
16.6 ± 0.4
1.8e−1 ± 0.1e−1
22
1032
PLRNN TF
0.994 ± 0.001
0.4 ± 0.09
4.3e−3 ± 0.2e−3
30
1011"
FURTHER RESULTS,0.48697394789579157,"Lorenz-
96"
FURTHER RESULTS,0.48897795591182364,"dendPLRNN VI
0.987 ± 0.001
0.10 ± 0.01
3.1e−1 ± 0.9e−1
42
4384
dendPLRNN TF
0.998 ± 0.0001
0.04 ± 0.01
4.1e−2 ± 0.8e−2
50
4480
PLRNN VI
0.93 ± 0.002
1.68 ± 0.03
2.1e−3 ± 0.2e−3
60
4260
PLRNN TF
0.996 ± 0.0003
0.05 ± 0.01
2.2e−1 ± 0.2e−1
64
4700"
FURTHER RESULTS,0.4909819639278557,"Bursting
Neuron"
FURTHER RESULTS,0.49298597194388777,"dendPLRNN VI
0.55 ± 0.03
7.5 ± 0.4
6.1e−1 ± 0.1e−1
26
2052
dendPLRNN TF
0.76 ± 0.04
2.9 ± 1.3
6.1e−2 ± 2.2e−2
26
2040
PLRNN VI
0.54 ± 0.01
17.5 ± 0.5
1.17 ± 0.14
42
2021
PLRNN TF
0.72 ± 0.07
3.2 ± 2.0
7.0e−2 ± 2.7e−2
43
2021"
FURTHER RESULTS,0.49498997995991983,"Neural
Popula-
tion
Model"
FURTHER RESULTS,0.4969939879759519,"dendPLRNN VI
0.45 ± 0.05
0.56 ± 0.05
0.82 ± 0.09
12
821
dendPLRNN TF
0.51 ± 0.01
0.19 ± 0.02
1.53 ± 0.03
75
9990
PLRNN VI
0.48 ± 0.01
11.65 ± 1.32
0.68 ± 0.09
13
832
PLRNN TF
0.38 ± 0.15
9.6 ± 7.3
19 ± 23
98
12102"
FURTHER RESULTS,0.49899799599198397,"Low
amount of
data"
FURTHER RESULTS,0.501002004008016,"dendPLRNN VI
0.967 ± 0.007
4.36 ± 0.10
2.8e−2 ± 0.2e−2
22
1032
dendPLRNN TF
0.97 ± 0.04
6.9 ± 5.3
1.5e−2 ± 0.9e−2
22
1032
PLRNN VI
0.96 ± 0.01
18.1 ± 0.10
1.08 ± 0.02
30
1020
PLRNN TF
0.96 ± 0.04
9.0 ± 5.4
1.8e−2 ± 0.5e−2
30
1011"
FURTHER RESULTS,0.503006012024048,"Partially
observed"
FURTHER RESULTS,0.5050100200400801,"dendPLRNN VI
0.940 ± 0.006
12.6 ± 1.0
6.5e−2 ± 1.4e−2
22
1032
dendPLRNN TF
0.993 ± 0.003
0.54 ± 0.16
5.3e−3 ± 0.2e−3
22
1032
PLRNN VI
0.944 ± 0.002
17.2 ± 0.2
2.7e−1 ± 0.03e−1
30
1020
PLRNN TF
0.994 ± 0.003
0.56 ± 0.34
5.0e−3 ± 0.2e−3
30
1011"
FURTHER RESULTS,0.5070140280561122,"High
noise"
FURTHER RESULTS,0.5090180360721442,"dendPLRNN VI
0.973 ± 0.006
4.9 ± 0.75
3.5e−2 ± 0.1e−2
22
1032
dendPLRNN TF
0.995 ± 0.002
0.4 ± 0.13
4.6e−3 ± 0.4e−3
22
1032
PLRNN VI
0.94 ± 0.004
18.2 ± 0.04
6.4e−1 ± 0.1e−1
30
1020
PLRNN TF
0.994 ± 0.002
0.5 ± 0.08
4.3e−3 ± 0.2e−3
22
1032"
FURTHER RESULTS,0.5110220440881763,"EEG Dataset
Electroencephalogram (EEG) data were taken from a study by (Schalk et al.,
2000) available at https://physionet.org/content/eegmmidb/1.0.0/. These are
64-channel EEG data obtained from human subjects during different motor and imagery tasks. We
trained the dendPLRNN using BPTT+TF on the ”eyes open” baseline time series from subject 0,
which had a total of 9760 time steps. The signal was standardized and smoothed with a Hann function,
using numpy.hanning and a window length of 15. Results for ground-truth and freely generated
EEG signals from several brain regions are shown in figure S5."
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5130260521042084,"6.4
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5150300601202404,"Lorenz-63 system
The famous 3d chaotic Lorenz attractor with the butterfly wing shape, originally
proposed in (Lorenz, 1963), is defined by"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5170340681362725,"Figure S5: EEG recordings from frontal, occipital, parietal and temporal lobe vs. freely gener-
ated trajectories, sampled from the dendPLRNN, trained with BPTT (M = 128, B = 50, τ =
10, Mreg/M = 0.5, λ = 1.0). dx"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5190380761523046,"dt = σ(y −x) + dϵ1(t) dt
, dy"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5210420841683366,dt = x(ρ −z) −y + dϵ2(t)
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5230460921843687,"dt
,
(14) dz"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5250501002004008,"dt = xy −βz + dϵ3(t) dt
."
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5270541082164328,"Parameters used for producing ground truth data in the chaotic regime were σ = 10, ρ = 28,
and β = 8/3. Process noise was injected into the system by drawing from a Gaussian term
dϵ ∼N(0, 0.01dt × I)."
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5290581162324649,"Bursting neuron model
The 3d biophysical bursting neuron model was introduced in (Durstewitz,
2009) and is defined by one voltage and two ion channel gating variables (one slow and one fast):"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.531062124248497,"−Cm ˙V = gL (V −EL) + gNam∞(V ) (V −ENa)
+ gKn (V −EK) + gMh (V −EK)"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.533066132264529,"+ gNMDA

1 + .33e−.0625V −1 (V −ENMDA) (15)"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5350701402805611,˙h = h∞(V ) −h τh
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5370741482965932,˙n = n∞(V ) −n τn (16)
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5390781563126252,The limiting values of the ionic gates are given by
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5410821643286573,"{m∞, n∞, h∞} =
h
1 + e({VhNa,VhK,VhM}−V )/{kNa,kK,kM}i−1
.
(17)"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5430861723446894,"We borrowed parameter settings from Schmidt et al. (2021) to place the system into the burst-firing
regime:"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5450901803607214,"Cm = 6µF, gL = 8mS, EL = −80mV, gNa = 20mS, ENa = 60mV, VhNa = −20mV,
kNa = 15, gK = 10mS, EK = −90mV, VhK = −25mV, kK = 5, τn = 1 ms, gM = 25mS
VhM = −15mV, kM = 5, τh = 200 ms, gNMDA = 10.2mS"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5470941883767535,"Lorenz-96 system
The Lorenz-96 is a high-dimensional, spatially extended weather model, also
introduced by Edward Lorenz (Lorenz, 1996): dxi"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5490981963927856,dt = (xi+1 −xi−2)xi−1 −xi + F + +dϵi(t)
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5511022044088176,"dt
, i = 1 . . . N,
(18)"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5531062124248497,"with (constant) forcing term F. F = 8 is a common choice that leads to chaotic behavior. Process
noise was added as for the Lorenz-63 system, dϵ ∼N(0, 0.01dt × I). In our simulations we used
N = 10, but in principle the system allows for arbitrary dimensionality."
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5551102204408818,"Neural population model
A larger-scale neural population model was recently introduced in
Landau & Sompolinsky (2018) to examine the effect of structured connectivity on top of a randomly
initialized network matrix. Specifically, an independently Gaussian distributed weight structure was
combined with a rank-1 component with coupling strength J1. The dynamics of the single unit
currents were defined as dh"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5571142284569138,"dt = −h + Jϕ(h) + J1
√"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.5591182364729459,"N
ξvT ϕ(h),
(19)"
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.561122244488978,"where ϕ(h) = tanh(h(t)). We produced a 50-dimensional chaotic network model based on the
code provided in Landau & Sompolinsky (2018) using J1 = 0.09 and seeding the random number
generator with 35."
DETAILS ON DYNAMICAL SYSTEMS BENCHMARKS,0.56312625250501,"The Lorenz-63 and Lorenz-96 systems were simulated using scipy.integrate, while for the
bursting neuron and neural population model we used the code provided in Schmidt et al. (2021) and
Landau & Sompolinsky (2018), respectively."
THEORETICAL ANALYSIS,0.5651302605210421,"6.5
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.5671342685370742,"Consider the PLRNN with linear spline basis expansion as defined by Eq. 1, Eq. 4, reproduced here
for convenience:"
THEORETICAL ANALYSIS,0.5691382765531062,"zt = Azt−1 + W B
X"
THEORETICAL ANALYSIS,0.5711422845691383,"b=1
αb max(0, zt−1 −hb) + h0 + Cst + ϵt,
(20)"
THEORETICAL ANALYSIS,0.5731462925851704,"where ϵt ∼N(0, Σ), E[ϵt, ϵT
t′] = 0 for t ̸= t′, αb ∈R are scalar weighting factors and hb ∈RM
different ReLU “activation thresholds”, and all other parameters are as in conventional PLRNNs
(Koppe et al., 2019)."
THEORETICAL ANALYSIS,0.5751503006012024,Defining
THEORETICAL ANALYSIS,0.5771543086172345,"D(b)
Ω(t−1)(zt−1 −hb) := max(0, zt−1 −hb),
(21)"
THEORETICAL ANALYSIS,0.5791583166332666,Eq. 20 can be rewritten as
THEORETICAL ANALYSIS,0.5811623246492986,"zt =

A + W B
X"
THEORETICAL ANALYSIS,0.5831663326653307,"b=1
αb D(b)
Ω(t−1)"
THEORETICAL ANALYSIS,0.5851703406813628,"
zt−1 + W B
X"
THEORETICAL ANALYSIS,0.5871743486973948,"b=1
αb D(b)
Ω(t−1) (−hb) + h0 + Cst + ϵt,
(22)"
THEORETICAL ANALYSIS,0.5891783567134269,"where D(b)
Ω(t−1) = diag
 
d(b)
1,t−1, d(b)
2,t−1, · · · , d(b)
M,t−1

are diagonal binary indicator matrices with"
THEORETICAL ANALYSIS,0.591182364729459,"d(b)
m,t−1 = 1 if zm,t−1 > hm,b and 0 otherwise."
THEORETICAL ANALYSIS,0.593186372745491,"6.5.1
FIXED POINTS AND n-CYCLES OF SYSTEM EQ. 22"
THEORETICAL ANALYSIS,0.5951903807615231,Defining
THEORETICAL ANALYSIS,0.5971943887775552,"DB
Ω(t−1) := B
X"
THEORETICAL ANALYSIS,0.5991983967935872,"b=1
αb D(b)
Ω(t−1),"
THEORETICAL ANALYSIS,0.6012024048096193,"hB
Ω(t−1) := B
X"
THEORETICAL ANALYSIS,0.6032064128256514,"b=1
αb D(b)
Ω(t−1)(−hb),
(23)"
THEORETICAL ANALYSIS,0.6052104208416834,"W B
Ω(t−1) := A + W DB
Ω(t−1),"
THEORETICAL ANALYSIS,0.6072144288577155,"and considering the autonomous system (i.e., without external inputs or noise terms), Eq. 22 can be
rewritten as"
THEORETICAL ANALYSIS,0.6092184368737475,"zt = W B
Ω(t−1) zt−1 + W hB
Ω(t−1) + h0.
(24)"
THEORETICAL ANALYSIS,0.6112224448897795,"Fixed points and cycles of Eq. 20, and their eigenvalue spectra, can now be computed in a way
analogous to standard PLRNNs. Specifically, solving the equation F(z∗1) = z∗1, fixed points of the
dendPLRNN are given by"
THEORETICAL ANALYSIS,0.6132264529058116,"z∗1 =

I −W B
Ω(t∗1)
−1h
W hB
Ω(t∗1) + h0
i
,
(25)"
THEORETICAL ANALYSIS,0.6152304609218436,"where z∗1 = zt∗1 = zt∗1−1, and det(I −W B
Ω(t∗1)) = PW B
Ω(t∗1)(1) ̸= 0, i.e. W B
Ω(t∗1) has no"
THEORETICAL ANALYSIS,0.6172344689378757,eigenvalue equal to 1.
THEORETICAL ANALYSIS,0.6192384769539078,"For n > 1, an n-cycle with periodic points {z∗n, F(z∗n), F 2(z∗n), · · · , F n−1(z∗n)} of map F can
be obtained by solving F n(z∗n) = z∗n. Therefore, in order to find the periodic points, we first
compute F n in the following way:"
THEORETICAL ANALYSIS,0.6212424849699398,"zt = F(zt−1) = W B
Ω(t−1) zt−1 + W hB
Ω(t−1) + h0,"
THEORETICAL ANALYSIS,0.6232464929859719,"zt+1 = F 2(zt−1) = F(zt) = W B
Ω(t) W B
Ω(t−1) zt−1 +
 
W B
Ω(t) W hB
Ω(t−1) + W hB
Ω(t)
"
THEORETICAL ANALYSIS,0.625250501002004,"+
 
W B
Ω(t) + I

h0,"
THEORETICAL ANALYSIS,0.627254509018036,"zt+2 = F 3(zt−1) = F(zt+1) = W B
Ω(t+1)W B
Ω(t)W B
Ω(t−1) zt−1 +
 
W B
Ω(t+1)W B
Ω(t)W hB
Ω(t−1)"
THEORETICAL ANALYSIS,0.6292585170340681,"+ W B
Ω(t+1)W hB
Ω(t) + W hB
Ω(t+1)

+
 
W B
Ω(t+1)W B
Ω(t) + W B
Ω(t+1) + I

h0, ..."
THEORETICAL ANALYSIS,0.6312625250501002,"zt+(n−1) = F n(zt−1) = n+1
Y"
THEORETICAL ANALYSIS,0.6332665330661322,"i=2
W B
Ω(t+n−i) zt−1 + n
X j=2"
THEORETICAL ANALYSIS,0.6352705410821643," n−j+2
Y"
THEORETICAL ANALYSIS,0.6372745490981964,"i=2
W B
Ω(t+n−i) W hB
Ω(t+j−3) "
THEORETICAL ANALYSIS,0.6392785571142284,"+ W hB
Ω(t+n−2) +

n
X j=2"
THEORETICAL ANALYSIS,0.6412825651302605,"n−j+2
Y"
THEORETICAL ANALYSIS,0.6432865731462926,"i=2
W B
Ω(t+n−i) + I

h0,
(26) where n+1
Y"
THEORETICAL ANALYSIS,0.6452905811623246,"i=2
W B
Ω(t+n−i) = W B
Ω(t+n−2)W B
Ω(t+n−3) · · · W B
Ω(t−1)."
THEORETICAL ANALYSIS,0.6472945891783567,"Defining t + n −1 =: t∗n, the periodic point z∗n of the n-cycle of F can now be obtained as the
fixed point of the n-times iterated map F n as"
THEORETICAL ANALYSIS,0.6492985971943888,"z∗n =

I − n
Y"
THEORETICAL ANALYSIS,0.6513026052104208,"i=1
W B
Ω(t∗n−i)"
THEORETICAL ANALYSIS,0.6533066132264529,"−1
n
X j=2"
THEORETICAL ANALYSIS,0.655310621242485,"h n−j+1
Y"
THEORETICAL ANALYSIS,0.657314629258517,"i=1
W B
Ω(t∗n−i)W hB
Ω(t∗n−n+j−2)
i
+ W hB
Ω(t∗n−1)"
THEORETICAL ANALYSIS,0.6593186372745491,"+

n
X j=2"
THEORETICAL ANALYSIS,0.6613226452905812,"n−j+1
Y"
THEORETICAL ANALYSIS,0.6633266533066132,"i=1
W B
Ω(t∗n−i) + I

h0"
THEORETICAL ANALYSIS,0.6653306613226453,"
,
(27)"
THEORETICAL ANALYSIS,0.6673346693386774,"where z∗n = zt∗n = zt∗n−n, if (I −Qn
i=1 W B
Ω(t∗n−i)) is invertible, i.e."
THEORETICAL ANALYSIS,0.6693386773547094,"det

I − n
Y"
THEORETICAL ANALYSIS,0.6713426853707415,"i=1
W B
Ω(t∗n−i)"
THEORETICAL ANALYSIS,0.6733466933867736,"
= PQn
i=1 W B
Ω(t∗n−i)(1) ̸= 0,"
THEORETICAL ANALYSIS,0.6753507014028056,"which implies WΩ∗n := Qn
i=1 W B
Ω(t∗n−i) has no eigenvalue equal to 1."
THEORETICAL ANALYSIS,0.6773547094188377,"Remark 1. These results about fixed points and n-cycles also hold for the mean-centred dendPLRNN.
This can easily be seen by defining W B
Ω(t−1) := A + W DB
Ω(t−1) M and noting that the elements"
THEORETICAL ANALYSIS,0.6793587174348698,"of D(b)
Ω(t−1) are now determined by the mean-centred latent states. That is d(b)
m,t−1 = 1 if zm,t−1 −"
"M
PM",0.6813627254509018,"1
M
PM
j=1 zj,t−1 > hm,b and 0 otherwise. The rest of the calculations then proceeds as above."
"M
PM",0.6833667334669339,"6.5.2
SUB-REGIONS AND DISCONTINUITY BOUNDARIES CORRESPONDING TO SYSTEM EQ. 22"
"M
PM",0.685370741482966,"Consider system Eq. 22 without external input and noise terms.
Denoting hb
=
(h1,b, h2,b, · · · , hM,b)T in Eq. 22, for b = 1, 2, · · · , B, we can order the elements hj,1, hj,2, · · · , hj,B
for every j ∈{1, 2, · · · , M}. Without loss of generality, let"
"M
PM",0.687374749498998,"hj,1 < hj,2 < · · · < hj,B,
j = 1, 2, · · · , M.
(28)"
"M
PM",0.6893787575150301,"Then, for every j, we define the intervals Ij,b as follows:"
"M
PM",0.6913827655310621,"Ij,1 := (−∞, hj,1],"
"M
PM",0.6933867735470942,"Ij,b := (hj,b−1, hj,b],
b = 2, 3, · · · , B,
(29)"
"M
PM",0.6953907815631263,"Ij,B+1 := (hj,B, +∞)."
"M
PM",0.6973947895791583,"By definition of D(i)
Ω(t−1) in Eq. 22, the phase space is separated into (B + 1)M sub-regions by
MB(B + 1)M−1 hyper-surfaces as discontinuity boundaries. Every sub-region can be defined by the
thresholds hb as Cartesian product of suitable intervals in Eq. 29 for j ∈{1, 2, · · · , M}. (Note that if
in Eq. 28 we had ” ≤” instead of strict inequalities ” < ”, obviously the number of intervals, hence
sub-regions, would decrease.) In each sub-region the matrices D(b)
Ω(t−1), b = 1, 2, · · · , B, have a
different configuration. Therefore, in Eq. 24 there are (B + 1)M different forms for DB
Ω(t−1), and so
for W B
Ω(t−1) and hB
Ω(t−1) as well. Hence, indexing DB
Ω(t−1), W B
Ω(t−1) and hB
Ω(t−1) as DB
(r), W B
(r)
and hB
(r) for r ∈{1, 2, · · · , (B + 1)M}, Eq. 22 can be written as"
"M
PM",0.6993987975951904,"zt = W B
(r) zt−1 + W hB
(r) + h0.
(30)"
"M
PM",0.7014028056112225,"To visualize the sub-regions and their borders, let for example M = 2 and B = 2. In this case there
are 9 sub-regions divided by 12 borders. As illustrated in Fig. 6.5.2, there are different matrices
D(b)
Ω(t−1), b = 1, 2, and DB
(r) = D2
(r), r = 1, 2, · · · , 9, for each sub-region."
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7034068136272545,"6.5.3
BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7054108216432866,"Proposition 2. The results of Theorem 2 are also true when the manifold-attractor regularization,
Eq. 6, is strictly enforced for the dendPLRNN, Eq. 10."
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7074148296593187,"Proof. Assume A, W , ˜ϕ(zt−1) (see proof of Theorem 2 in Appx. 6.5.6 for the definition) and h0
have the partitioned forms A = "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7094188376753507,"Ireg
OT"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7114228456913828,"O
Anreg "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7134268537074149,",
W = "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7154308617234469,"Oreg
OT"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.717434869739479,"S
Wnreg  ,"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7194388777555111,"Figure S6: Example of different sub-regions and related matrices D(b)
Ω(t−1), b = 1, 2, and DB
(r), r =
1, 2, · · · , 9, for M = 2 and B = 2. Here, it is assumed that the components of h1 = (h1,1, h2,1)T"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7214428857715431,"and h2 = (h1,2, h2,2)T satisfy Eq. 28 with ” < ”. h0 = "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7234468937875751,"hreg
0"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7254509018036072,"hnreg
0 "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7274549098196392,",
˜ϕ(zt−1) = "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7294589178356713,"
˜ϕreg(zt−1)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7314629258517034,˜ϕnreg(zt−1) 
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7334669338677354,",
(31)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7354709418837675,"where IMreg×Mreg =: Ireg ∈RMreg×Mreg, OMreg×Mreg =: Oreg ∈RMreg×Mreg , O, S ∈
R(M−Mreg)×Mreg , the sub-matrices A{Mreg+1:M,Mreg+1:M} =: Anreg ∈R(M−Mreg)×(M−Mreg)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7374749498997996,"and W{Mreg+1:M,Mreg+1:M} =: Wnreg ∈R(M−Mreg)×(M−Mreg) are diagonal and off-diagonal"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7394789579158316,"respectively. Furthermore, hreg
0
, ˜ϕreg(zt−1) ∈RMreg and h{Mreg+1:M,Mreg+1:M}
0
=: hnreg
0
,
˜ϕ{Mreg+1:M,Mreg+1:M}(zt−1) =: ˜ϕnreg(zt−1) ∈RM−Mreg ."
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7414829659318637,"In this case ∥A∥= σmax(A) = max{1, σmax(Anreg)} and"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7434869739478958,"Aj W ˜ϕ(zT −1−j)
 =   
O"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7454909819639278,"Aj
neg S ˜ϕnreg(zt−1) + Aj
neg Wneg ˜ϕnreg(zt−1)   "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7474949899799599,"=
Aj
neg S ˜ϕnreg(zt−1) + Aj
neg Wneg ˜ϕnreg(zt−1)
 ,"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.749498997995992,"Aj W h0
 =   
O"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.751503006012024,"Aj
neg S hnreg
0
+ Aj
neg Wneg hnreg
0   "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7535070140280561,"=
Aj
neg S hnreg
0
+ Aj
neg Wneg hnreg
0
 .
(32)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7555110220440882,"Thus, for σmax(Anreg) < 1"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7575150300601202,∥zT ∥≤∥A∥T −1 ∥z1∥+
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7595190380761523,"T −2
X j=0"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7615230460921844,"Aj W ˜ϕ(zT −1−j)
 +"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7635270541082164,"T −2
X j=0 Aj h0"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7655310621242485,"≤∥z1∥+
 
˜c + ∥h0∥
 
∥S∥+ ∥Wneg∥
 T −2
X"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7675350701402806,"j=0
∥Aneg∥j ="
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7695390781563126," 
˜c + ∥h0∥
 
∥S∥+ ∥Wneg∥
"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7715430861723447,"1 −∥Aneg∥
< ∞.
(33)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7735470941883767,"6.5.4
PROOF OF PROPOSITION 1"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7755511022044088,"Proof. For A = (aij) ∈RM×M, W = (wij) ∈RM×M, ϵt = (ϵ1,t, ϵ2,t, · · · , ϵM,t)T, st =
(s1,t, s2,t, · · · , sM,t)T and C = (cij) ∈RM×M, writing Eq. 22 in scalar form yields"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7775551102204409,"zl,t = M
X"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.779559118236473,"j=1
aljzj,t−1 + M
X"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.781563126252505,"j=1
wlj B
X"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7835671342685371,"b=1
αb d(b)
j,t−1[zj,t−1 −hj,b] + hl,0 + M
X"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7855711422845691,"j=1
clj sj,t + ϵl,t = M
X j=1"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7875751503006012,"
aljzj,t−1 + wlj B
X"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7895791583166333,"b=1
αb d(b)
j,t−1[zj,t−1 −hj,b]

+ hl,0 + M
X"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7915831663326653,"j=1
clj sj,t + ϵl,t =: M
X"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7935871743486974,"j=1
fl,j(zj,t−1) + hl,0 + M
X"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7955911823647295,"j=1
clj sj,t + ϵl,t =: Fl(zt−1),
l = 1, 2, · · · , M.
(34)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7975951903807615,"Using this, we can write Eq. 22 in the vector form"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.7995991983967936,"zt =
 
F1(zt−1), F2(zt−1), · · · , FM(zt−1)
T.
(35)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8016032064128257,"We show that every Fl is continuous and so Eq. 22 is a continuous PWL map. For this purpose, by
Eq. 34, it suffices to prove that every fl,j(zj,t−1) is continuous. According to the definition of the
intervals Ij,b, Eq. 29, for any j ∈{1, 2, · · · , M} we have"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8036072144288577,"zj,t−1 ∈Ij,1
⇒
d(b)
j,t−1 = 0
∀b = 1, 2, · · · , B,"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8056112224448898,"zj,t−1 ∈Ij,s
⇒"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8076152304609219,"


 

"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8096192384769539,"d(b)
j,t−1 = 1, b = 1, 2, · · · , s −1"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.811623246492986,"d(b)
j,t−1 = 0, b = s, s + 1, · · · , B"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8136272545090181,"s = 2, 3, · · · , B,"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8156312625250501,"zj,t−1 ∈Ij,B+1
⇒
d(b)
j,t−1 = 1
∀b = 1, 2, · · · , B.
(36)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8176352705410822,"Hence, for l, j = 1, 2, · · · , M, each function fl,j(zj,t−1) can be stated as"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8196392785571143,"fl,j(zj,t−1) ="
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8216432865731463,"











"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8236472945891784,"










"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8256513026052105,"f (1)
l,j = alj zj,t−1;
zj,t−1 ∈Ij,1"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8276553106212425,"f (2)
l,j = (alj + α1 wlj) zj,t−1 −α1 wljhj,1;
zj,t−1 ∈Ij,2
..."
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8296593186372746,"f (B)
l,j
= (alj + wlj
PB−1
b=1 αb) zj,t−1 −wlj
PB−1
b=1 αb hj,b;
zj,t−1 ∈Ij,B"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8316633266533067,"f (B+1)
l,j
= (alj + wlj
PB
i=1 αb) zj,t−1 −wlj
PB
b=1 αb hj,b;
zj,t−1 ∈Ij,B+1
(37)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8336673346693386,"Since for every b = 1, 2, · · · , B,"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8356713426853707,"lim
zj,t−1→hj,b f (b)
l,j (zj,t−1) =
lim
zj,t−1→hj,b f (b+1)
l,j
(zj,t−1) = f (b)
l,j (hj,b),
(38)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8376753507014028,"each function fl,j(zj,t−1) is continuous. Hence, Eq. 22 is a continuous PWL map in z (but has
discontinuities in its Jacobian matrix across the borders). Because of these properties, all the results
established for standard PLRNNs in Monfared & Durstewitz (2020a;b); Schmidt et al. (2021) apply
to the dendPLRNN as well, only that the sub-regions and discontinuity boundaries are different."
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8396793587174348,"6.5.5
PROOF OF PROPOSITION 1"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8416833667334669,"Proof. Defining ˜zt as B identical copies of zt, ˜zt = "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.843687374749499,"








"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.845691382765531,"˜z1,t
˜z2,t
...
˜zM,t
˜zM+1,t
...
˜zBM,t "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8476953907815631,"








 :=  

"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8496993987975952,"zt
zt
...
zt  

 BM×1 (39)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8517034068136272,and likewise ˜h = 
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8537074148296593,"









"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8557114228456913,"˜h1
˜h2
...
˜hM
˜hM+1
...
˜hBM "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8577154308617234,"









 =  

"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8597194388777555,"h1
h2
...
hB  

 BM×1"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8617234468937875,",
˜h0 = "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8637274549098196,"









"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8657314629258517,"˜h0,1
˜h0,2
...
˜h0,M
˜h0,M+1
...
˜h0,BM "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8677354709418837,"









 =  

"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8697394789579158,"h0
h0
...
h0  

 BM×1"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8717434869739479,"˜
ABM×BM = diag
 
AM×M, AM×M, · · · , AM×M
|
{z
}
B times 
,"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.87374749498998,"˜
WBM×BM = "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.875751503006012,"








"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8777555110220441,"α1WM×M
α2WM×M
. . .
αBWM×M"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8797595190380761,"α1WM×M
α2WM×M
. . .
αBWM×M
...
...
...
..."
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8817635270541082,"α1WM×M
α2WM×M
. . .
αBWM×M "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8837675350701403,"








 ,"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8857715430861723,"˜
Cst = "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8877755511022044,"








"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8897795591182365,"˜cs1,t
˜cs2,t
...
˜csM,t
˜csM+1,t
...
˜csBM,t "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8917835671342685,"








 =  

"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8937875751503006,"Cst
Cst
...
Cst  

 BM×1"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8957915831663327,",
˜ϵt = "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8977955911823647,"








"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.8997995991983968,"˜ϵ1,t
˜ϵ2,t
...
˜ϵM,t
˜ϵM+1,t
...
˜ϵBM,t "
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9018036072144289,"








 =  

"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9038076152304609,"ϵt
ϵt
...
ϵt  

 BM×1 (40)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.905811623246493,one can rewrite the dendPLRNNfrom Eq. 20 as
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9078156312625251,"˜zt = ˜
A˜zt−1 + ˜
W max(0, ˜zt−1 −˜h) + ˜h0 +
˜
Cst + ˜ϵt.
(41)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9098196392785571,Now performing the substitution
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9118236472945892,"∀t
ˆzt ←˜zt −˜h,
(42)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9138276553106213,Eq. 41 can be rewritten as the M × B-dimensional “conventional” PLRNN Eq. 5 with
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9158316633266533,"ˆh0 =
  ˜
A −I
˜h + ˜h0.
(43)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9178356713426854,"6.5.6
PROOF OF THEOREM 2"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9198396793587175,"Proof. It can easily be shown that for every i ∈{1, 2, · · · , M}"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9218436873747495,"αb

max(max(0, zi,t−1 −hi,b) −max(0, zi,t−1)

∈"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9238476953907816,"(
[−αbhib, 0] if sgn(αb) = sgn(hi,b)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9258517034068137,"[0, αbhi,b] else
. (44)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9278557114228457,"By defining B
X"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9298597194388778,"b=1
αb

max(0, zt−1 −hb) −max(0, zt−1)

:= ˜ϕ(zt−1) =

˜ϕ1(zt−1), · · · , ˜ϕM(zt−1)
T
, (45) and"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9318637274549099,"cup
i,b ="
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9338677354709419,"(
0 if sgn(αb) = sgn(hi,b)"
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.935871743486974,"αbhi,b else
,
clow
i,b ="
"BOUNDED ORBITS ARE COMPATIBLE WITH THE MANIFOLD ATTRACTOR
REGULARIZATION",0.9378757515030061,"(
−αbhi,b if sgn(αb) = sgn(hi,b)"
ELSE,0.9398797595190381,"0 else
,
(46)"
ELSE,0.9418837675350702,"we can conclude that
clow
i
≤˜ϕi(zt−1) ≤cup
i ,"
ELSE,0.9438877755511023,"where clow/up
i
= PB
b=1 clow/up
i,b
. For ci = max{|clow
i
|, |cup
i |} we have"
ELSE,0.9458917835671342,"˜ϕi(zt−1)2 ≤c2
i ,"
ELSE,0.9478957915831663,"and so letting c = max{c1, c2, · · · , cM} yields"
ELSE,0.9498997995991983,"˜ϕ(zt−1)
 ="
ELSE,0.9519038076152304,"v
u
u
t M
X i=1"
ELSE,0.9539078156312625," ˜ϕi(zt−1)
2 ≤"
ELSE,0.9559118236472945,"v
u
u
t M
X"
ELSE,0.9579158316633266,"i=1
c2 := ˜c.
(47) Since"
ELSE,0.9599198396793587,"zt = A zt−1 + W ˜ϕ(zt−1) + h0,
(48)"
ELSE,0.9619238476953907,"for T ∈N and t = 2, · · · , T, computing z2, z3, · · · , zT recursively leads to"
ELSE,0.9639278557114228,z2 = A z1 + W ˜ϕ(z1) + h0
ELSE,0.9659318637274549,"z3 = A2 z1 + A W ˜ϕ(z1) + W ˜ϕ(z2) +

A + I

h0
..."
ELSE,0.9679358717434869,zT = AT −1 z1 +
ELSE,0.969939879759519,"T −2
X"
ELSE,0.9719438877755511,"j=0
Aj W ˜ϕ(zT −1−j) +"
ELSE,0.9739478957915831,"T −2
X"
ELSE,0.9759519038076152,"j=0
Aj h0.
(49)"
ELSE,0.9779559118236473,"Therefore, by Eq. 47, for every T ≥2, we have"
ELSE,0.9799599198396793,∥zT ∥≤∥A∥T −1 ∥z1∥+ ˜c ∥W ∥
ELSE,0.9819639278557114,"T −2
X"
ELSE,0.9839679358717435,"j=0
∥A∥j +"
ELSE,0.9859719438877755,"T −2
X"
ELSE,0.9879759519038076,"j=0
∥A∥j ∥h0∥.
(50)"
ELSE,0.9899799599198397,"If σmax(A) < 1, then lim
T →∞∥A∥T −1 = 0 and"
ELSE,0.9919839679358717,"lim
T →∞∥zT ∥≤˜c ∥W ∥ ∞
X"
ELSE,0.9939879759519038,"j=0
∥A∥j + ∞
X"
ELSE,0.9959919839679359,"j=0
∥A∥j ∥h0∥= ˜c ∥W ∥+ ∥h0∥"
ELSE,0.9979959919839679,"1 −∥A∥
< ∞.
(51)"
