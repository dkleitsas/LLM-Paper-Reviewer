Section,Section Appearance Order,Paragraph
UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,0.0,"1University of Illinois at Urbana-Champaign
2Carnegie Mellon University
3Lawrence Livermore National Laboratory
4Amazon AWS AI
{fanw6,linyi2,chejian2,lbo}@illinois.edu
huan@huan-zhang.com
kailkhura1@llnl.gov
kkenthapadi@gmail.com
dingzhao@andrew.cmu.edu
* Equal contribution."
ABSTRACT,0.0010964912280701754,ABSTRACT
ABSTRACT,0.0021929824561403508,"As reinforcement learning (RL) has achieved near human-level performance in a
variety of tasks, its robustness has raised great attention. While a vast body of
research has explored test-time (evasion) attacks in RL and corresponding de-
fenses, its robustness against training-time (poisoning) attacks remains largely
unanswered. In this work, we focus on certifying the robustness of ofﬂine RL
in the presence of poisoning attacks, where a subset of training trajectories could
be arbitrarily manipulated. We propose the ﬁrst certiﬁcation framework, COPA,
to certify the number of poisoning trajectories that can be tolerated regarding dif-
ferent certiﬁcation criteria. Given the complex structure of RL, we propose two
certiﬁcation criteria: per-state action stability and cumulative reward bound. To
further improve the certiﬁcation, we propose new partition and aggregation pro-
tocols to train robust policies. We further prove that some of the proposed certiﬁ-
cation methods are theoretically tight and some are NP-Complete problems. We
leverage COPA to certify three RL environments trained with different algorithms
and conclude: (1) The proposed robust aggregation protocols such as temporal
aggregation can signiﬁcantly improve the certiﬁcations; (2) Our certiﬁcations for
both per-state action stability and cumulative reward bound are efﬁcient and tight;
(3) The certiﬁcation for different training algorithms and environments are dif-
ferent, implying their intrinsic robustness properties. All experimental results are
available at https://copa-leaderboard.github.io."
INTRODUCTION,0.003289473684210526,"1
INTRODUCTION"
INTRODUCTION,0.0043859649122807015,"Reinforcement learning (RL) has been widely applied to a range of applications, including
robotics (Kober et al., 2013; Deisenroth et al., 2013; Polydoros & Nalpantidis, 2017) and au-
tonomous vehicles (Shalev-Shwartz et al., 2016; Sallab et al., 2017). In particular, ofﬂine RL (Levine
et al., 2020) is proposed to leverage previously observed data to train the policy without requiring
expensive interaction with environments or online data collection, and enables the reuse of training
data (Agarwal et al., 2020). However, ofﬂine RL also raises a great safety concern on poisoning
attacks (Kiourti et al., 2020; Wang et al., 2020; 2021). A recent survey of industry reports that data
poisoning is signiﬁcantly more concerning than other threats (Kumar et al., 2020). For ofﬂine RL,
the situation is even worse, and a recent theoretical study shows that the robust ofﬂine RL against
poisoning attacks is a strictly harder problem than online RL (Zhang et al., 2021)."
INTRODUCTION,0.005482456140350877,"Although there are several empirical and certiﬁed defenses for classiﬁcation tasks against poisoning
attacks (Peri et al., 2020; Levine & Feizi, 2021; Weber et al., 2020), it is challenging and shown
ineffective to directly apply them to RL given its complex structure (Kiourti et al., 2020). Thus, ro-
bust ofﬂine RL against poisoning attacks remains largely unexplored with no mention of robustness
certiﬁcation. In addition, though some theoretical analyses provide general robustness bounds for
RL, they either assume a bounded distance between the learned and Bellman optimal policies or are
limited to linear MDPs (Zhang et al., 2021). To the best of our knowledge, there is no robust RL"
INTRODUCTION,0.006578947368421052,Published as a conference paper at ICLR 2022
INTRODUCTION,0.007675438596491228,"method that is able to provide practically computable certiﬁed robustness against poisoning attacks.
In this paper, we tackle this problem by proposing the ﬁrst framework of Certifying robust policies
for general ofﬂine RL against poisoning attacks (COPA)."
INTRODUCTION,0.008771929824561403,"Certiﬁcation Criteria. One critical challenge in certifying robustness for ofﬂine RL is the certiﬁ-
cation criteria, since the prediction consistency is no longer the only goal as in classiﬁcation. We
propose two criteria based on the properties of RL: per-state action stability and cumulative reward
bound. The former guarantees that at a speciﬁc time, the policy learned with COPA will predict the
same action before and after attacks under certain conditions. This is important for guaranteeing the
safety of the policy at critical states, e.g., braking when seeing pedestrians. For cumulative reward
bound, a lower bound of the cumulative reward for the policy learned with COPA is guaranteed
under certain poisoning conditions. This directly guarantees the worst-case overall performance."
INTRODUCTION,0.009868421052631578,"COPA Framework. COPA is composed of two components: policy partition and aggregation
protocol and robustness certiﬁcation method. We propose three policy partition aggregation pro-
tocols: PARL (Per-State Partition Aggregation), TPARL (Temporal Partition Aggregation), and
DPARL (Dynamic Temporal Partition Aggregation), and propose certiﬁcation methods for each of
them corresponding to both proposed certiﬁcation criteria. In addition, for per-state action stability,
we prove that our certiﬁcations for PARL and TPARL are theoretically tight. For cumulative reward
bound, we propose an adaptive search algorithm, where we compute the possible action set for each
state under certain poisoning conditions. Concretely, we propose a novel method to compute the
precise action set for PARL and efﬁcient algorithms to compute a superset of the possible action
set which leads to sound certiﬁcation for TPARL and DPARL. We further prove that for PARL our
certiﬁcation is theoretically tight, for TPARL the theoretically tight certiﬁcation is NP-complete,
and for DPARL it is open whether theoretically tight certiﬁcation exists."
INTRODUCTION,0.010964912280701754,"Technical Contributions. We take the ﬁrst step towards certifying the robustness of ofﬂine RL
against poisoning attacks, and we make contributions on both theoretical and practical fronts."
INTRODUCTION,0.01206140350877193,"• We abstract and formulate the robustness certiﬁcation for ofﬂine RL against poisoning attacks,
and we propose two certiﬁcation criteria: per-state action stability and cumulative reward bound.
• We propose the ﬁrst framework COPA for certifying robustness of ofﬂine RL against poisoning
attacks. COPA includes novel policy aggregation protocols and certiﬁcation methods.
• We prove the tightness of the proposed certiﬁcation methods for the aggregation protocol PARL.
We also prove the computational hardness of the certiﬁcation for TPARL.
• We conduct thorough experimental evaluation for COPA on different RL environments with
three ofﬂine RL algorithms, demonstrating the effectiveness of COPA, together with several
interesting ﬁndings."
RELATED WORK,0.013157894736842105,"2
RELATED WORK"
RELATED WORK,0.01425438596491228,"Poisoning attacks (Nelson et al., 2008; Diakonikolas et al., 2016) are critical threats in machine
learning, which are claimed to be more concerning than other threats (Kumar et al., 2020).
Poisoning attacks widely exist in classiﬁcation (Schwarzschild et al., 2021), and both empirical
defenses (Liu et al., 2018; Chacon et al., 2019; Peri et al., 2020; Steinhardt et al., 2017) and certiﬁed
defenses (Weber et al., 2020; Jia et al., 2020; Levine & Feizi, 2021) have been proposed."
RELATED WORK,0.015350877192982455,"After Kiourti et al. (2020) show the existence of effective backdoor poisoning attacks in RL, a
recent work theoretically and empirically validates the existence of reward poisoning in online
RL (Zhang et al., 2020b). Furthermore, Zhang et al. (2021) theoretically prove that the ofﬂine RL
is more difﬁcult to be robustiﬁed against poisoning than online RL considering linear MDP. From
the defense side, Zhang et al. (2021) propose robust variants of the Least-Square Value Iteration
algorithm that provides probabilistic robustness guarantees under linear MDP assumption.
In
addition, Robust RL against reward poisoning is studied in Banihashem et al. (2021), but robust
RL against general poisoning is less explored. In this background, we aim to provide the certiﬁed
robustness for general ofﬂine RL algorithms against poisoning attacks, which is the ﬁrst work that
achieves the goal. We discuss broader related work in Appendix I."
CERTIFICATION CRITERIA OF COPA,0.01644736842105263,"3
CERTIFICATION CRITERIA OF COPA"
CERTIFICATION CRITERIA OF COPA,0.017543859649122806,"In this section, we propose two robustness certiﬁcation criteria for ofﬂine RL against general poi-
soning attacks: per-state action stability and cumulative reward bound."
CERTIFICATION CRITERIA OF COPA,0.01864035087719298,Published as a conference paper at ICLR 2022
CERTIFICATION CRITERIA OF COPA,0.019736842105263157,"Ofﬂine RL.
We model the RL environment by an episodic ﬁnite-horizon Markov decision pro-
cess (MDP) E = (S, A, R, P, H, d0), where S is the set of states, A is the set of discrete actions,
R : S × A →R is the reward function, P : S × A →P(S) is the stochastic transition function
with P(·) deﬁning the set of probability measures, H is the time horizon, and d0 ∈P(S) is the
distribution of the initial state. At time step t, the RL agent is at state st ∈S. After choosing action
at ∈A, the agent transitions to the next state st+1 ∼P(st, at) and receives reward rt = R(st, at).
After H time steps, the cumulative reward J = PH−1
t=0 rt. We denote a consecutive sequence of all
states between time step l and r as sl:r := [sl, sl+1, . . . , sr]."
CERTIFICATION CRITERIA OF COPA,0.020833333333333332,"Here we focus on ofﬂine RL, for which the threat of poisoning attacks is practical and more chal-
lenging to deal with (Zhang et al., 2021). Concretely, in ofﬂine RL, a training dataset D = {τi}N
i=1
consists of logged trajectories, where each trajectory τ = {(sj, rj, aj, s′
j)}l
j=1 ∈(S × A × R × S)l
consists of multiple tuples denoting the transitions (i.e., starting from state sj, taking the action aj,
receiving reward rj, and transitioning to the next state s′
j)."
CERTIFICATION CRITERIA OF COPA,0.021929824561403508,"Poisoning Attacks.
Training dataset D can be poisoned in the following manner. For each tra-
jectory τ ∈D, the adversary is allowed to replace it with an arbitrary trajectory eτ, generating a
manipulated dataset eD. We denote D ⊖eD = (D\ eD) S( eD\D) as the symmetric difference between
two datasets D and eD. For instance, adding or removing one trajectory causes a symmetric differ-
ence of magnitude 1, while replacing one trajectory with a new one leads to a symmetric difference
of magnitude 2. We refer to the size of the symmetric difference as the poisoning size."
CERTIFICATION CRITERIA OF COPA,0.023026315789473683,"Certiﬁcation Goal.
To provide the robustness certiﬁcation against poisoning attacks introduced
above, we aim to certify the test-time performance of the trained policy in a clean environment.
Speciﬁcally, in the training phase, the RL training algorithm and our aggregation protocol can be
jointly modeled by M : D →(S⋆→A) which provides an aggregated policy, where S⋆denotes
the set of all consecutive state sequences. Our goal is to provide robustness certiﬁcation for the
poisoned aggregated policy ˜π = M( eD), given bounded poisoning size (i.e., |D ⊖eD| ≤K)."
CERTIFICATION CRITERIA OF COPA,0.02412280701754386,"Robustness Certiﬁcation Criteria: Per-State Action Stability.
We ﬁrst aim to certify the ro-
bustness of the poisoned policy in terms of the stability of per-state action during test time."
CERTIFICATION CRITERIA OF COPA,0.025219298245614034,"Deﬁnition 1 (Robustness Certiﬁcation for Per-State Action Stability). Given a clean dataset D, we
deﬁne the robustness certiﬁcation for per-state action stability as that for any eD satisfying |D⊖eD| ≤
K, the action predictions of the poisoned and clean policies for the state (or state sequence) s are the
same, i.e., ˜π = M( eD), π = M(D), ˜π(s) = π(s), under the the tolerable poisoning threshold K.
In an episode, we denote the tolerable poisoning threshold for the state at step t by Kt."
CERTIFICATION CRITERIA OF COPA,0.02631578947368421,"The deﬁnition encodes the requirement that, for a particular state, any poisoned policy will always
give the same action prediction as the clean one, as long as the poisoning size is within K (K
computed in Section 4). In this deﬁnition, s could be either a state or a state sequence, since our ag-
gregated policy (deﬁned in Section 3) may aggregate multiple recent states to make an action choice."
CERTIFICATION CRITERIA OF COPA,0.027412280701754384,"Robustness Certiﬁcation Criteria: Lower Bound of Cumulative Reward.
We also aim to cer-
tify poisoned policy’s overall performance in addition to the prediction at a particular state. Here we
measure the overall performance by the cumulative reward J(π) (formally deﬁned in Appendix A)."
CERTIFICATION CRITERIA OF COPA,0.02850877192982456,Now we are ready to deﬁne the robustness certiﬁcation for cumulative reward bound.
CERTIFICATION CRITERIA OF COPA,0.029605263157894735,"Deﬁnition 2 (Robustness Certiﬁcation for Cumulative Reward Bound). Robustness certiﬁcation for
cumulative reward bound is the lower bound of cumulative reward JK such that JK ≤J(˜π) for any
˜π = M( eD) where |D ⊖eD| ≤K, i.e., ˜π is trained on poisoned dataset eD within poisoning size K."
CERTIFICATION PROCESS OF COPA,0.03070175438596491,"4
CERTIFICATION PROCESS OF COPA"
CERTIFICATION PROCESS OF COPA,0.03179824561403509,"In this section, we introduce our framework COPA, which is composed of training protocols, ag-
gregation protocols, and certiﬁcation methods. The training protocol combined with an ofﬂine RL
training algorithm provides subpolicies. The aggregation protocol aggregates the subpolicies as an
aggregated policy. The certiﬁcation method certiﬁes the robustness of the aggregated policy against
poisoning attacks corresponding to different certiﬁcation criteria provided in Section 3."
CERTIFICATION PROCESS OF COPA,0.03289473684210526,Published as a conference paper at ICLR 2022
CERTIFICATION PROCESS OF COPA,0.03399122807017544,"Table 1: Overview of theoretical results in Section 4. “Certiﬁcation” columns entail our certiﬁcation theorems.
“Analysis” columns entail the analyses of our certiﬁcation bounds, where “tight” means our certiﬁcation is
theoretically tight, “NP-complete” means the tight certiﬁcation problem is NP-complete, and “open” means
the tight certiﬁcation problem is still open. Theorems 8 and 13 and proposition 9 are in Appendices C and F.6.2."
CERTIFICATION PROCESS OF COPA,0.03508771929824561,"Certiﬁcation
Criteria"
CERTIFICATION PROCESS OF COPA,0.03618421052631579,Proposed Aggregation Protocol
CERTIFICATION PROCESS OF COPA,0.03728070175438596,"PARL (πP, Deﬁnition 7)
TPARL (πT, Deﬁnition 8)
DPARL (πD, Deﬁnition 3)
Certiﬁcation
Analysis
Certiﬁcation
Analysis
Certiﬁcation
Analysis"
CERTIFICATION PROCESS OF COPA,0.03837719298245614,"Per-State Action
Theorem 8
tight (Proposition 9)
Theorem 1
tight (Proposition 2)
Theorem 3
open
Cumulative Reward
Theorem 4
tight (Theorem 13)
Theorem 5
NP-complete (Theorem 6)
Theorem 7
open"
CERTIFICATION PROCESS OF COPA,0.039473684210526314,"Overview of Theoretical Results.
In Table 1, we present an overview of our theoretical results:
For each proposed aggregation protocol and certiﬁcation criteria, we provide the corresponding cer-
tiﬁcation method and core theorems, and we also provide the tightness analysis for each certiﬁcation."
PARTITION-BASED TRAINING PROTOCOL,0.04057017543859649,"4.1
PARTITION-BASED TRAINING PROTOCOL"
PARTITION-BASED TRAINING PROTOCOL,0.041666666666666664,"COPA’s training protocol contains two stages: partitioning and training. We denote D as the entire
ofﬂine RL training dataset. We abstract an ofﬂine RL training algorithm (e.g., DQN) by M0 : 2D →
Π, where 2D is the power set of D, and Π = {π : S →A} is the set of trained policies. Each trained
policy in Π is a function mapping a given state to the predicted action."
PARTITION-BASED TRAINING PROTOCOL,0.04276315789473684,"Partitioning Stage.
In this stage, we separate the training dataset D into u partitions {Di}u−1
i=0
that satisfy Su−1
i=0 Di = D and ∀i ̸= j, Di ∩Dj = ∅. Concretely, when performing partitioning,
for each trajectory τ ∈D, we deterministically assign it to one unique partition. The assignment
is only dependent on the trajectory τ itself, and not impacted by any modiﬁcation to other parts of
the training set. One design choice of such a deterministic assignment is using a deterministic hash
function h to compute the assignment, i.e., Di = {τ ∈D | h(τ) ≡i (mod u)}, ∀i ∈[u]."
PARTITION-BASED TRAINING PROTOCOL,0.043859649122807015,"Training Stage.
In this stage, for each training data partition Di, we independently apply an
RL algorithm M0 to train a policy πi = M0(Di). Hereinafter, we call these trained polices as
subpolicies to distinguish from the aggregated policies. Concretely, let [u] := {0, 1, . . . , u−1}. For
these subpolicies, the policy indicator 1i,a : S →{0, 1} is deﬁned by 1i,a(s) := 1[πi(s) = a],
indicating whether subpolicy πi chooses action a at state s. The aggregated action count na : S →
N≥0 is the number of votes across all the subpolicies for action a given state s: na(s) := |{i|πi(s) =
a, i ∈[u]}| = Pu−1
i=0 1i,a(s). Speciﬁcally, we denote na(sl:r) for Pr
j=l na(sj), i.e., the sum of votes
for states between time step l and r. A detailed algorithm of the training protocol is in Appendix E.1."
PARTITION-BASED TRAINING PROTOCOL,0.044956140350877194,"Now we are ready to introduce the proposed aggregation protocols in COPA (PARL, TPARL,
DPARL) that generate aggregated policies based on subpolicies, and corresponding certiﬁcation."
PARTITION-BASED TRAINING PROTOCOL,0.046052631578947366,"4.2
AGGREGATION PROTOCOLS: PARL, TPARL, DPARL"
PARTITION-BASED TRAINING PROTOCOL,0.047149122807017545,"With u learned subpolicies {πi}u−1
i=0 , we propose three different aggregation protocols in COPA to
form three types of aggregated policies for each certiﬁcation criteria: PARL, TPARL, and DPARL."
PARTITION-BASED TRAINING PROTOCOL,0.04824561403508772,"Per-State Partition Aggregation (PARL). Inspired by aggregation in classiﬁcation (Levine &
Feizi, 2021), PARL aggregates subpolicies by choosing actions with the highest votes. We denote
the PARL aggregated policy by πP : S →A. When there are multiple highest voting actions, we
break ties deterministically by returning the “smaller” (<) action, which can be deﬁned by numeri-
cal order, lexicographical order, etc. Throughout the paper, we assume arg max over A always uses
< operator to break ties. The formal deﬁnition of the protocol is in Appendix A."
PARTITION-BASED TRAINING PROTOCOL,0.049342105263157895,"The intuition behind PARL is that the poisoning attack within size K can change at most K sub-
policies. Therefore, as long as the margin between the votes for top and runner-up actions is larger
than 2K for the given state, after poisoning, we can guarantee that the aggregated PARL policy will
not change its action choice. We will formally state the robustness guarantee in Section 4.3."
PARTITION-BASED TRAINING PROTOCOL,0.05043859649122807,"Temporal Partition Aggregation (TPARL).
In the sequential decision making process of RL,
it is likely that certain important states are much more vulnerable to poisoning attacks, which we
refer to as bottleneck states. Therefore, the attacker may just change the action predictions for these
bottleneck states to deteriorate the overall performance, say, the cumulative reward. For example,
in Pong game, we may lose the round when choosing an immediate bad action when the ball is
closely approaching the paddle. Thus, to improve the overall certiﬁed robustness, we need to focus"
PARTITION-BASED TRAINING PROTOCOL,0.051535087719298246,Published as a conference paper at ICLR 2022
PARTITION-BASED TRAINING PROTOCOL,0.05263157894736842,"on improving the tolerable poisoning threshold for these bottleneck states. Given such intuition and
goal, we propose Temporal Partition Aggregation (TPARL) and the aggregated policy is denoted as
πT, which is formally deﬁned in Deﬁnition 8 in Appendix A."
PARTITION-BASED TRAINING PROTOCOL,0.0537280701754386,"TPARL is based on two insights: (1) Bottleneck states have lower tolerable poisoning threshold,
which is because the vote margin between the top and runner-up actions is smaller at such state;
(2) Some RL tasks satisfy temporal continuity (Legenstein et al., 2010; Veerapaneni et al., 2020),
indicating that good action choices are usually similar across states of adjacent time steps, i.e.,
adjacent states. Hence, we leverage the subpolicies’ votes from adjacent states to enlarge the vote
margin, and thus increase the tolerable poisoning threshold. To this end, in TPARL, we predetermine
a window size W, and choose the action with the highest votes across recent W states."
PARTITION-BASED TRAINING PROTOCOL,0.05482456140350877,"Dynamic Temporal Partition Aggregation (DPARL).
The TPARL uses a ﬁxed window size W
across all states. Since the speciﬁcation of the window size W requires certain prior knowledge,
plus that the same ﬁxed window size W may not be suitable for all states, it is preferable to perform
dynamic temporal aggregation by using a ﬂexible window size. Therefore, we propose Dynamic
Temporal Partition Aggregation (DPARL), which dynamically selects the window size W towards
maximizing the tolerable poisoning threshold per step. Intuitively, DPARL selects the window size
W such that the average vote margin over selected states is maximized. To guarantee that only
recent states are chosen, we further constrain the maximum window size (Wmax)."
PARTITION-BASED TRAINING PROTOCOL,0.05592105263157895,"Deﬁnition 3 (Dynamic Temporal Partition Aggregation). Given subpolicies {πi}u−1
i=0 and maximum
window size Wmax, at time step t, the Dynamic Temporal Partition Aggregation (DPARL) deﬁnes
an aggregated policy πD : Smin{t+1,Wmax} →A such that
πD(smax{t−Wmax+1,0}:t) := arg max
a∈A
na(st−W ′+1:t), where W ′ =
arg max
1≤W ≤min{Wmax,t+1}
∆W
t .
(1)"
PARTITION-BASED TRAINING PROTOCOL,0.05701754385964912,"In the above equation, na is deﬁned in Section 3 and ∆W
t
is given by"
PARTITION-BASED TRAINING PROTOCOL,0.0581140350877193,"∆W
t
:= 1"
PARTITION-BASED TRAINING PROTOCOL,0.05921052631578947,"W (na1(st−W +1:t) −na2(st−W +1:t)) ,"
PARTITION-BASED TRAINING PROTOCOL,0.06030701754385965,"where
a1 = arg max
a∈A
na(st−W +1:t), a2 = arg max
a∈A,a̸=a1
na(st−W +1:t).
(2)"
PARTITION-BASED TRAINING PROTOCOL,0.06140350877192982,"In the above deﬁnition, ∆W
t
encodes the average vote margin between top action a1 and runner-up
action a2 if choosing window size W. Thus, W ′ locates the window size with maximum average
vote margin, and its corresponding action is selected. Again, we use the mechanism described in
PARL to break ties. Robustness certiﬁcation methods for DPARL are in Sections 4.3 and 4.4."
PARTITION-BASED TRAINING PROTOCOL,0.0625,"In Appendix B, we present a concrete example to demonstrate how different aggregation protocols
induce different tolerable poisoning thresholds, and illustrate bottleneck and non-bottleneck states."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.06359649122807018,"4.3
CERTIFICATION OF PER-STATE ACTION STABILITY"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.06469298245614036,"In this section, we present our robustness certiﬁcation theorems and methods for per-state action.
For each of the aggregation protocols (PARL, TPARL, and DPARL), at each time step t, we will
compute a valid tolerable poisoning threshold K as deﬁned in Deﬁnition 1, such that the chosen
action at step t does not change as long as the poisoning size K ≤K."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.06578947368421052,"Certiﬁcation for PARL. Due to the space limit, we defer the robustness certiﬁcation method for
PARL to Appendix C. The certiﬁcation method is based on Theorem 8. We further show the theo-
retical tightness of the certiﬁcation in Proposition 9. All the theorem statements are in Appendix C."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.0668859649122807,Certiﬁcation for TPARL. We certify the robustness of TPARL following Theorem 1.
CERTIFICATION OF PER-STATE ACTION STABILITY,0.06798245614035088,"Theorem 1. Let D be the clean training dataset; let πi = M0(Di), 0 ≤i ≤u −1 be the learned
subpolicies according to Section 4.1 from which we deﬁne na (Section 3); and let πT be the Temporal
Partition Aggregation policy: πT = M(D) where M abstracts the whole training-aggregation
process. eD is a poisoned dataset and f
πT is the poisoned policy: f
πT = M( eD)."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.06907894736842106,"For a given state st encountered at time step t during test time, let a := πT(smax{t−W +1,0}:t), then
at time step t the tolerable poisoning threshold (see Deﬁnition 1)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07017543859649122,"Kt =
min
a′̸=a,a′∈A max ( p p
X"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.0712719298245614,"i=1
h(i)
a,a′ ≤δa,a′ ) (3)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07236842105263158,Published as a conference paper at ICLR 2022
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07346491228070176,"where {h(i)
a,a′}u
i=1 is a nonincreasing permutation of

 "
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07456140350877193,"min{W −1,t}
X"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.0756578947368421,"j=0
1i,a(st−j) + min{W, t + 1} −"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07675438596491228,"min{W −1,t}
X"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07785087719298246,"j=0
1i,a′(st−j) 
  u−1 i=0"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07894736842105263,"=: {hi,a,a′}u−1
i=0 ,
(4)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.0800438596491228,"and δa,a′ := na(smax{t−W +1,0}:t) −(na′(smax{t−W +1,0}:t) + 1[a′ < a]). Here, 1i,a(s) =
1[πi(s) = a] (Section 3), and W is the window size.
Remark. We defer the detailed proof to Appendix F.2. The theorem provides a per-state action
certiﬁcation for TPARL. The detailed algorithm is in Algorithm 3 (Appendix E.2). The certiﬁcation
time complexity per state is O(|A|u(W +log u)) and can be further optimized to O(|A|u log u) with
proper preﬁx sum caching across time steps. We prove the certiﬁcation for TPARL is theoretically
tight in Proposition 2 (proof in Appendix F.3). We also prove that directly extending Theorem 8 for
TPARL (Corollary 10) is loose in Appendix F.4.
Proposition 2. Under the same condition as Theorem 1, for any time step t, there exists an
RL learning algorithm M0, and a poisoned dataset eD, such that |D ⊖eD| = Kt + 1, and
f
πT(smax{t−W +1,0}:t) ̸= πT(smax{t−W +1,0}:t)."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08114035087719298,"Certiﬁcation for DPARL. Theorem 3 provides certiﬁcation for DPARL.
Theorem 3. Let D be the clean training dataset; let πi = M0(Di), 0 ≤i ≤u −1 be the learned
subpolicies according to Section 4.1 from which we deﬁne na (see Section 3); and let πD be the
Dynamic Temporal Partition Aggregation: πD = M(D) where M abstracts the whole training-
aggregation process. eD is a poisoned dataset and f
πD is the poisoned policy: f
πD = M( eD)."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08223684210526316,"For a given state st encountered at time step t during test time, let a := πD(smax{t−Wmax+1,0}:t)
and W ′ be the chosen time window (according to Equation (1)), then tolerable poisoning threshold"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08333333333333333,"KD
t = min
"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08442982456140351,"Kt,
min
1≤W ∗≤min{Wmax,t+1},W ∗̸=W ′,a′̸=a,a′′̸=a LW ∗,W ′"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08552631578947369,"a′,a′′ 
(5)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08662280701754387,"where Kt is deﬁned by Equation (3) with W as W ′ and LW ∗,W ′"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08771929824561403,"a,a′′
deﬁned by the below Deﬁnition 4."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08881578947368421,"Deﬁnition 4 (L in Theorem 3). Under the same condition as Theorem 3, for given W ∗, W ′, a, a′, a′′,
we let a# := arg maxa0̸=a′,a0∈A na0(st−W ∗+1:t), then"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08991228070175439,"LW ∗,W ′"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09100877192982457,"a′,a′′
:= max ( p p
X"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09210526315789473,"i=1
g(i) + W ′(nW ∗
a′
−nW ∗"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09320175438596491,"a# ) −W ∗(nW ′
a
−nW ′
a′′ ) −1[a′ > a] < 0 ) (6)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09429824561403509,"where nw
a is a shorthand of na(st−w+1:t) and {g(i)}u
i=1 is a nonincreasing permutation of {gi}u−1
i=0 .
Each gi is deﬁned by gi :="
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09539473684210527,"max{W ∗,W ′}
X"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09649122807017543,"w=0
max
a0∈A σw(a0) −σw(πi(st−w)), where
(7)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09758771929824561,"σw(a0) := W ′1[a0 = a′, w ≤W ∗] −W ′1[a0 = a#, w ≤W ∗] −W ∗1[a0 = a, w ≤W ′] + W ∗1[a0 = a′′, w ≤W ′]."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09868421052631579,"Proof sketch. A successful poisoning attack should change the chosen action from a to an another
a′. If after attack, the chosen window size is still W ′, the poisoning size should be at least larger than
Kt according to Theorem 1. If the chosen window size is not W ′, we ﬁnd out that the poisoning size
is at least mina′′̸=a LW ∗,W ′"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09978070175438597,"a′,a′′
+1 from a greedy-based analysis. Formal proof is in Appendix F.5."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.10087719298245613,"Remark. The theorem provides a valid per-state action certiﬁcation for DPARL policy. The de-
tailed algorithm is in Algorithm 4 (Appendix E.2). The certiﬁcation time complexity per state is
O(W 2
max|A|2u+Wmax|A|2u log u), which in practice adds similar overhead compared with TPARL
certiﬁcation (see Appendix H.3). Unlike certiﬁcation for PARL and TPARL, the certiﬁcation given
by Theorem 3 is not theoretically tight. An interesting future work would be providing a tighter
per-state action certiﬁcation for DPARL."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10197368421052631,"4.4
CERTIFICATION OF CUMULATIVE REWARD BOUND
In this section, we present our robustness certiﬁcation for cumulative reward bound. We assume the
deterministic RL environment throughout the cumulative reward certiﬁcation for convenience, i.e.,
the transition function is P : S×A →S and the initial state is a ﬁxed s0 ∈S. The certiﬁcation goal,
as listed in Deﬁnition 6, is to obtain a lower bound of cumulative reward under poisoning attacks,
given bounded poisoning size K. The cumulative reward certiﬁcation is based on a novel adaptive
search algorithm COPA-SEARCH inspired from Wu et al. (2022); we tailor the algorithm to certify
against poisoning attacks. We defer detailed discussions and complexity analysis to Appendix E.3."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10307017543859649,Published as a conference paper at ICLR 2022
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10416666666666667,"COPA-SEARCH Algorithm Description.
The pseudocode is in Algorithm 5 (Appendix E.3).
The method starts from the base case: when the poisoning threshold Kcur = 0, the lower bound
of cumulative reward JKcur is exactly the reward without poisoning. The method then gradually
increases the poisoning threshold Kcur, by ﬁnding the immediate larger K′ > Kcur that can expand
the possible action set along the trajectory. With the increase of Kcur ←K′, the attack may cause
the poisoned policy ˜π to take different actions at some states, thus resulting in new trajectories. We
need to ﬁgure out a set of all possible actions to exhaustively traverse all possible trajectories. We
will introduce theorems to compute this set of possible actions. With this set, the method effectively
explores these new trajectories by formulating them as expanded branches of a trajectory tree. Once
all new trajectories are explored, the method examines all leaf nodes of the tree and ﬁgures out the
minimum reward among them, which is the new lower bound of cumulative reward JK′ under new
poisoning size K′. We then repeat this process of increasing poisoning size from K′ and expanding
with new trajectories until we reach a predeﬁned threshold for poisoning size K.
Deﬁnition 5 (Possible Action Set). Given previous states s0:t, the subpolicies {πi}u−1
i=0 , the aggre-
gation protocol (PARL, TPARL, or DPARL), and the poisoning size K, the possible action set A
at step t is a subset of action space: A ⊆A, such that for any poisoned policy eπ, as long as the
poisoning size is within K, the chosen action at step t will always be in A, i.e., at = eπ(s0:t) ∈A."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10526315789473684,"Possible Action Set for PARL.
The following theorem gives the possible action set for PARL.
Theorem 4 (Tight PARL Action Set). Under the condition of Deﬁnition 5, suppose the aggregation
protocol is PARL as deﬁned in Deﬁnition 7, then the possible action set at step t"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10635964912280702,AT (K) = (
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1074561403508772,"a ∈A

X"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10855263157894737,"a′∈A
max{na′(st) −na(st) −K + 1[a′ < a], 0} ≤K ) .
(8)"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10964912280701754,"We defer the proof to Appendix F.6.1. Furthermore, in Appendix F.6.2 we show that: 1) The theo-
rem gives theoretically tight possible action set; 2) In contrast, directly extending PARL’s per-state
certiﬁcation gives loose certiﬁcation."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11074561403508772,"Possible Action Set for TPARL.
The following theorem gives the possible action set for TPARL.
Theorem 5. Under the condition of Deﬁnition 5, suppose the aggregation protocol is TPARL as
deﬁned in Deﬁnition 8, then the possible action set at step t"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1118421052631579,"A(K) = ( a ∈A K
X"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11293859649122807,"i=1
h(i)
a′,a > δa′,a, ∀a′ ̸= a ) ,
(9)"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11403508771929824,"where h(i)
a′,a and δa′,a follow the deﬁnition in Theorem 1."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11513157894736842,"We defer the proof to Appendix F.7. The possible action set here is no longer theoretically tight.
Indeed, the problem of computing a possible action set with minimum cardinality for TPARL is
NP-complete as we shown in the following theorem (proved in Appendix F.8), where we reduce
computing theoretically tight possible action set to the set cover problem (Karp, 1972). This result
can be viewed as the hardness of targeted attack. In other words, the optimal untargeted attack
on TPARL can be found in polynomial time, while the optimal targeted attack on TPARL is NP-
complete, which indicates the robustness property of proposed TPARL.
Theorem 6. Under the condition of Deﬁnition 5, suppose we use TPARL (Deﬁnition 8) as the ag-
gregation protocol, then computing a possible action set A(K) such that any possible action set S
satisﬁes |A(K)| ≤|S| is NP-complete.
Possible Action Set for DPARL.
The following theorem gives the possible action set for DPARL.
Theorem 7. Under the condition of Deﬁnition 5, suppose the aggregation protocol is TPARL as
deﬁned in Deﬁnition 3, then the possible action set at step t"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1162280701754386,"A(K) = {at} ∪ 

"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11732456140350878,"

a′ ∈A

min
1≤W ∗≤min{Wmax,t+1},
W ∗̸=W ′,a′′̸=at"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11842105263157894,"LW ∗,W ′"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11951754385964912,"a′,a′′
≤K 

 

∪ ( a ∈A K
X"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1206140350877193,"i=1
h(i)
a′,a > δa′,a, ∀a′ ̸= a ) (10)"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12171052631578948,"where at = πD(smax{t−Wmax+1,0}:t) is the clean policy’s chosen action, W ′ is deﬁned by Equa-"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12280701754385964,"tion (1), LW ∗,W ′"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12390350877192982,"a′,a′′
is deﬁned by Deﬁnition 4 with a being replaced by at, and h(i)
a′,a, δa′,a is deﬁned in
Theorem 1 with W replaced by W ′."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.125,"We prove the theorem in Appendix F.8. As summarized in Table 1, we further prove the tightness or
hardness of certiﬁcation for PARL and TPARL, while for DPARL it is an interesting open problem
on whether theoretical tight certiﬁcation is possible in polynomial time."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12609649122807018,Published as a conference paper at ICLR 2022
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12719298245614036,"PARL
TPARL (W = 4)
DPARL (Wmax = 5)"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12828947368421054,"Freeway, u = 50
Breakout, u = 50
Highway, u = 50"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12938596491228072,"DQN
stability ratio"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13048245614035087,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0 0.2 0.4 0.6 0.8 1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13157894736842105,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13267543859649122,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1337719298245614,"QR-DQN
stability ratio"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13486842105263158,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0 0.2 0.4 0.6 0.8 1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13596491228070176,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13706140350877194,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13815789473684212,"C51
stability ratio"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13925438596491227,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0 0.2 0.4 0.6 0.8 1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.14035087719298245,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.14144736842105263,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1425438596491228,"≥K
≥K
≥K
Figure 1: Robustness certiﬁcation for per-state action stability. We plot the cumulative histogram of the
tolerable poisoning size K for all time steps. We provide the certiﬁcation for different aggregation protocols
(PARL, TPARL, DPARL) on three environments and #partitions u = 50. The results are averaged over 20 runs
with the vertical bar on top denoting the standard deviation."
EXPERIMENTS,0.14364035087719298,"5
EXPERIMENTS"
EXPERIMENTS,0.14473684210526316,"In this section, we present the evaluation for our COPA framework, speciﬁcally, the aggregation
protocols (Section 4.2) and the certiﬁcation methods under different certiﬁcation criteria (Sec-
tions 4.3 and 4.4). We defer the description of the ofﬂine RL algorithms (DQN (Mnih et al., 2013),
QR-DQN (Dabney et al., 2018), and C51 (Bellemare et al., 2017)) used for training the subpolicies
to Appendix G.1, and the concrete experimental procedures to Appendix G.2. As a summary, we
obtain similar conclusions from per-state action certiﬁcation and reward certiﬁcation: 1) QR-DQN
and C51 are oftentimes more certiﬁably robust than DQN; 2) temporal aggregation (TPARL
and DPARL) achieves higher certiﬁcation for environments satisfying temporal continuity, e.g.,
Freeway; 3) larger partition number improves the certiﬁed robustness; 4) Freeway is the most stable
and robust environment among the three. More interesting discussions are deferred to Appendix H."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.14583333333333334,"5.1
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.14692982456140352,We provide the robustness certiﬁcation for per-state action stability based on Section 4.3.
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.14802631578947367,"Experimental Setup and Metrics.
We evaluate the aggregated policies πP, πT, and πD following
Section 4.2. Basically, in each run, we run one trajectory (of maximum length H) using the derived
policy, and compute Kt at each time step t. Given {Kt}H−1
t=0 , we obtain a cumulative histogram—
for each threshold K, we count the time steps that achieve a threshold no smaller than it and then
normalize, i.e.,
PH−1
t=0 1[Kt≥K]/H. We call this quantity stability ratio since it reﬂects the per-state
action stability w.r.t. given poisoning thresholds. We also compute an average tolerable poisoning
thresholds for a trajectory, deﬁned as
PH−1
t=0 Kt/H. More details are deferred to Appendix G.2."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.14912280701754385,"Evaluation Results.
We present the comparison of per-state action certiﬁcation for different RL
methods and certiﬁcation methods in Figure 1. We plot partial poisoning thresholds on the x-axes
here, and omit full results in Appendix H.6, where we also report the average tolerable poisoning
thresholds. We additionally report benign empirical reward and the comparisons with standard
training in Appendices H.1 and H.2, as well as more analytical statistics in Appendices H.3 and H.4."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15021929824561403,"The cumulative histograms in Figure 1 can be compared in different levels. Basically, we compare
the stability ratio at each tolerable poisoning thresholds K—higher ratio at larger poisoning size
indicates stronger certiﬁed robustness. On the RL algorithm level, QR-DQN and C51 consistently
outperform the baseline DQN, and C51 has a substantial advantage particularly in Highway. On the
aggregation protocol level, we observe different behaviors in different environments. On Freeway,
methods with temporal aggregation (TPARL and DPARL) achieve higher robustness, and DPARL
achieves the highest certiﬁed robustness in most cases; while on Breakout and Highway, the single-
step aggregation PARL is oftentimes better. This difference is due to the different properties of
environments. Our temporal aggregation is developed based on the assumption of consistent action"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.1513157894736842,Published as a conference paper at ICLR 2022
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.1524122807017544,"u = 30, PARL
u = 50, PARL"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15350877192982457,"u = 30, TPARL (W = 4)
u = 50, TPARL (W = 4)"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15460526315789475,"u = 30, DPARL (Wmax = 5)
u = 50, DPARL (Wmax = 5)"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15570175438596492,"Freeway, H = 200
Freeway, H = 400
Breakout, H = 50
Breakout, H = 75
Highway, H = 30"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15679824561403508,"DQN
JK"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15789473684210525,"0
5
10
15
20
25
0 1 2 3"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15899122807017543,"0
5
10
15
20
25
0 1 2 3 4 5"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.1600877192982456,"0
3
6
9 12 15 18 21
0 1"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.1611842105263158,"0
5
10
15
20
25
0 1 2"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16228070175438597,"0
5
10
15
20
25 5 10 15 20 25"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16337719298245615,"QR-DQN
JK"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16447368421052633,"0
5
10
15
20
25
0 1 2 3"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16557017543859648,"0
5
10
15
20
25
0 1 2 3 4 5"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16666666666666666,"0
3
6
9 12 15 18 21 24
0 1"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16776315789473684,"0
5
10
15
20
25
0 1 2"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16885964912280702,"0
5
10
15
20
25 5 10 15 20 25"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.1699561403508772,"C51
JK"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17105263157894737,"0
5
10
15
20
25
0 1 2 3"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17214912280701755,"0
5
10
15
20
25
0 1 2 3 4 5"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17324561403508773,"0
3
6
9 12 15 18 21 24 27
0 1"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17434210526315788,"0
5
10
15
20
25
0 1 2"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17543859649122806,"0
5
10
15
20
25 5 10 15 20 25"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17653508771929824,"Poisoning size K
Poisoning size K
Poisoning size K
Poisoning size K
Poisoning size K
Figure 2: Robustness certiﬁcation for cumulative reward. We plot the lower bound of cumulative reward
bound JK w.r.t. poisoning size K under three aggregation protocols (PARL, TPARL (W = 4), DPARL
(Wmax = 5)) with two #partitions u, evaluated on three environments with different horizon lengths H."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17763157894736842,"selection in adjacent time steps. This assumption is true in Freeway while violated in Breakout and
Highway. A more detailed explanation of environment properties is omitted to Appendix H.5. On
the partition number level, a larger partition number generally allows larger tolerable poisoning
thresholds as shown in Appendix H.6. Finally, on the RL environment level, Freeway achieves
much higher certiﬁed robustness for per-state action stability than Highway, followed by Breakout,
implying that Freeway is an environment that accommodates more stable and robust policies."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND,0.1787280701754386,"5.2
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND,0.17982456140350878,We provide the robustness certiﬁcation for cumulative reward bound according to Section 4.4.
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND,0.18092105263157895,"Experimental Setup and Metrics.
We evaluate the aggregated policies πP, πT, and πD follow-
ing Theorem 4, Theorem 5 and Theorem 7. We compute the lower bounds of the cumulative reward
JK w.r.t. the poisoning size K using the COPA-SEARCH algorithm introduced in Section 4.4. We
provide details of the evaluated trajectory length along with the rationales in Appendix G.2."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND,0.18201754385964913,"Evaluation Results.
We present the comparison of reward certiﬁcation for different RL algorithms
and certiﬁcation methods in Figure 2. Essentially, at each poisoning size K, we compare the lower
bound of cumulative reward achieved by different RL algorithms and certiﬁcation methods—higher
value of the lower bound implies stronger certiﬁed robustness. On the RL algorithm level, QR-
DQN and C51 almost invariably outperform the baseline DQN algorithm. On the aggregation
protocol level, methods with temporal aggregation consistently surpass the single-step aggregation
PARL on Freeway but not the other two, as analyzed in Section 5.1. In addition, we note that DPARL
is sometimes not as robust as TPARL. We hypothesize two reasons: 1) the dynamic mechanism is
more susceptible to the attack, e.g., the selected optimal window size is prone to be manipulated;
2) the lower bound is looser for DPARL given the difﬁculty of computing the possible action set
in DPARL (discussed in Theorem 7). On the partition number level, a larger partition number
(u = 50) demonstrates higher robustness. On the horizon length level, the robustness ranking of
different policies is similar under different horizon lengths with slight differences, corresponding
to the property of ﬁnite-horizon RL. On the RL environment level, Freeway can tolerate a larger
poisoning size than Breakout and Highway. More results and discussions are in Appendix H.7."
CONCLUSIONS,0.18311403508771928,"6
CONCLUSIONS"
CONCLUSIONS,0.18421052631578946,"In this paper, we proposed COPA, the ﬁrst framework for certifying robust policies for ofﬂine RL
against poisoning attacks. COPA includes three policy aggregation protocols. For each aggregation
protocol, COPA provides a sound certiﬁcation for both per-state action stability and cumulative
reward bound. Experimental evaluations on different environments and different ofﬂine RL training
algorithms show the effectiveness of our robustness certiﬁcation in a wide range of scenarios."
CONCLUSIONS,0.18530701754385964,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.18640350877192982,ACKNOWLEDGMENTS
CONCLUSIONS,0.1875,"This work is partially supported by the NSF grant No.1910100, NSF CNS 20-46726 CAR, Alfred P.
Sloan Fellowship, the U.S. Department of Energy by the Lawrence Livermore National Laboratory
under Contract No. DE-AC52-07NA27344 and LLNL LDRD Program Project No. 20-ER-014, and
Amazon Research Award."
ETHICS STATEMENT,0.18859649122807018,"Ethics Statement.
In this paper, we prove the ﬁrst framework for certifying robust policies for
ofﬂine RL against poisoning attacks. On the one hand, such framework provides rigorous guaran-
tees in practice RL tasks and thus signiﬁcantly alleviates the security vulnerabilities of ofﬂine RL
algorithms against training-phase poisoning attacks. The evaluation of different RL algorithms also
provides a better understanding about the different degrees of security vulnerabilities across differ-
ent RL algorithms. On the other hand, the robustness guarantee provided by our framework only
holds under speciﬁc conditions of the attack. Speciﬁcally, we require the attack to change only a
bounded number of training trajectories. Therefore, users should be aware of such limitations of
our framework, and should not blindly trust the robustness guarantee when the attack can change a
large number of training instances or modify the training algorithm itself. As a result, we encourage
researchers to understand the potential risks, and evaluate whether our constraints on the attack align
with their usage scenarios when applying our COPA to real-world applications. We do not expect
any ethics issues raised by our work."
REPRODUCIBILITY STATEMENT,0.18969298245614036,"Reproducibility Statement.
All theorem statements are substantiated with rigorous proofs in Ap-
pendix F. In Appendix E, we list the pseudocode for all key algorithms. Our experimental eval-
uation is conducted with publicly available OpenAI Gym toolkit (Brockman et al., 2016). We
introduce all experimental details in both Section 5 and Appendix G. Speciﬁcally, we build the
code upon the open-source code base of Agarwal et al. (2020), and we upload the source code at
https://github.com/AI-secure/COPA for reproducibility purpose."
REFERENCES,0.19078947368421054,REFERENCES
REFERENCES,0.19188596491228072,"Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on ofﬂine
reinforcement learning. In International Conference on Machine Learning, 2020."
REFERENCES,0.19298245614035087,"Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a
robot hand. arXiv preprint arXiv:1910.07113, 2019."
REFERENCES,0.19407894736842105,"Kiarash Banihashem, Adish Singla, and Goran Radanovic. Defense against reward poisoning attacks
in reinforcement learning. arXiv preprint arXiv:2102.05776, 2021."
REFERENCES,0.19517543859649122,"Vahid Behzadan and Arslan Munir. Whatever does not kill deep reinforcement learning, makes it
stronger. arXiv preprint arXiv:1712.09344, 2017."
REFERENCES,0.1962719298245614,"Vahid Behzadan and Arslan Munir. Mitigation of policy manipulation attacks on deep q-networks
with parameter-space noise. In International Conference on Computer Safety, Reliability, and
Security, pp. 406–417. Springer, 2018."
REFERENCES,0.19736842105263158,"Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449–458. PMLR, 2017."
REFERENCES,0.19846491228070176,"Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966."
REFERENCES,0.19956140350877194,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.20065789473684212,"Henry Chacon, Samuel Silva, and Paul Rad. Deep learning poison data attack detection. In 2019
IEEE 31st International Conference on Tools with Artiﬁcial Intelligence (ICTAI), pp. 971–978.
IEEE, 2019."
REFERENCES,0.20175438596491227,"Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310–1320. PMLR, 2019."
REFERENCES,0.20285087719298245,Published as a conference paper at ICLR 2022
REFERENCES,0.20394736842105263,"Will Dabney, Mark Rowland, Marc G Bellemare, and R´emi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018."
REFERENCES,0.2050438596491228,"Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.
Foundations and trends in Robotics, 2(1-2):388–403, 2013."
REFERENCES,0.20614035087719298,"Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Robust estimators in high dimensions without the computational intractability. In 57th Annual
Symposium on Foundations of Computer Science, pp. 655–664. Institute of Electrical and Elec-
tronics Engineers (IEEE), 2016."
REFERENCES,0.20723684210526316,"Michael Everett, Bj¨orn L¨utjens, and Jonathan P How. Certiﬁable robustness to adversarial state
uncertainty in deep reinforcement learning. IEEE Transactions on Neural Networks and Learning
Systems, 2021."
REFERENCES,0.20833333333333334,"Marc Fischer, Matthew Mirman, Steven Stalder, and Martin Vechev. Online robustness training for
deep reinforcement learning. arXiv preprint arXiv:1911.00887, 2019."
REFERENCES,0.20942982456140352,"Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves,
Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration.
arXiv preprint arXiv:1706.10295, 2017."
REFERENCES,0.21052631578947367,"Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. arXiv preprint arXiv:1810.12715, 2018."
REFERENCES,0.21162280701754385,"IEEE Computer Society. Standards Committee. Working group of the Microprocessor Stan-
dards Subcommittee and American National Standards Institute.
IEEE standard for binary
ﬂoating-point arithmetic, volume 754. IEEE, 1985."
REFERENCES,0.21271929824561403,"Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adver-
sarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 15262–15271, 2021."
REFERENCES,0.2138157894736842,"Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017."
REFERENCES,0.2149122807017544,"Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipula-
tions on cost signals. In International Conference on Decision and Game Theory for Security, pp.
217–237. Springer, 2019."
REFERENCES,0.21600877192982457,"Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong.
Certiﬁed robustness of nearest neighbors
against data poisoning attacks. arXiv preprint arXiv:2012.03765, 2020."
REFERENCES,0.21710526315789475,"Richard M Karp. Reducibility among combinatorial problems. In Complexity of computer compu-
tations, pp. 85–103. Springer, 1972."
REFERENCES,0.21820175438596492,"Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl: evaluation of backdoor
attacks on deep reinforcement learning. In 2020 57th ACM/IEEE Design Automation Conference
(DAC), pp. 1–6. IEEE, 2020."
REFERENCES,0.21929824561403508,"Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238–1274, 2013."
REFERENCES,0.22039473684210525,"Jernej Kos and Dawn Song.
Delving into adversarial attacks on deep policies.
arXiv preprint
arXiv:1705.06452, 2017."
REFERENCES,0.22149122807017543,"Ram Shankar Siva Kumar, Magnus Nystr¨om, John Lambert, Andrew Marshall, Mario Goertzel,
Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry per-
spectives. In 2020 IEEE Security and Privacy Workshops (SPW), pp. 69–75. IEEE, 2020."
REFERENCES,0.2225877192982456,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.2236842105263158,Published as a conference paper at ICLR 2022
REFERENCES,0.22478070175438597,"Robert Legenstein, Niko Wilbert, and Laurenz Wiskott. Reinforcement learning on slow features of
high-dimensional input streams. PLoS computational biology, 6(8):e1000894, 2010."
REFERENCES,0.22587719298245615,"Edouard Leurent. An environment for autonomous driving decision-making. https://github.
com/eleurent/highway-env, 2018."
REFERENCES,0.22697368421052633,"Alexander Levine and Soheil Feizi. Deep partition aggregation: Provable defenses against general
poisoning attacks. In International Conference on Learning Representations, 2020."
REFERENCES,0.22807017543859648,"Alexander Levine and Soheil Feizi. Deep partition aggregation: Provable defenses against gen-
eral poisoning attacks. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=YUGG2tFuPM."
REFERENCES,0.22916666666666666,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.23026315789473684,"Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
Fine-pruning: Defending against back-
dooring attacks on deep neural networks. In International Symposium on Research in Attacks,
Intrusions, and Defenses, pp. 273–294. Springer, 2018."
REFERENCES,0.23135964912280702,"Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Xiaojin Zhu. Policy poisoning in batch reinforcement
learning and control. Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.2324561403508772,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017."
REFERENCES,0.23355263157894737,"Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning, pp. 3578–3586.
PMLR, 2018."
REFERENCES,0.23464912280701755,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller.
Playing atari with deep reinforcement learning.
arXiv preprint
arXiv:1312.5602, 2013."
REFERENCES,0.23574561403508773,"Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein,
Udam Saini, Charles Sutton, J Doug Tygar, and Kai Xia. Exploiting machine learning to subvert
your spam ﬁlter. LEET, 8:1–9, 2008."
REFERENCES,0.23684210526315788,"Tuomas Oikarinen, Tsui-Wei Weng, and Luca Daniel. Robust deep reinforcement learning through
adversarial loss. arXiv preprint arXiv:2008.01976, 2020."
REFERENCES,0.23793859649122806,"Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Ro-
bust deep reinforcement learning with adversarial attacks. In 17th International Conference on
Autonomous Agents and Multiagent Systems, AAMAS 2018, pp. 2040–2042. International Foun-
dation for Autonomous Agents and Multiagent Systems (IFAAMAS), 2018."
REFERENCES,0.23903508771929824,"Neehar Peri, Neal Gupta, W Ronny Huang, Liam Fowl, Chen Zhu, Soheil Feizi, Tom Goldstein, and
John P Dickerson. Deep k-nn defense against clean-label data poisoning attacks. In European
Conference on Computer Vision, pp. 55–70. Springer, 2020."
REFERENCES,0.24013157894736842,"Athanasios S Polydoros and Lazaros Nalpantidis. Survey of model-based reinforcement learning:
Applications on robotics. Journal of Intelligent & Robotic Systems, 86(2):153–173, 2017."
REFERENCES,0.2412280701754386,"Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching
via environment poisoning: Training-time adversarial attacks against reinforcement learning. In
International Conference on Machine Learning, pp. 7974–7984. PMLR, 2020."
REFERENCES,0.24232456140350878,"Ahmad EL Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. Deep reinforcement
learning framework for autonomous driving. Electronic Imaging, 2017(19):70–76, 2017."
REFERENCES,0.24342105263157895,"Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and S´ebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classiﬁers. In Pro-
ceedings of the 33rd International Conference on Neural Information Processing Systems, pp.
11292–11303, 2019."
REFERENCES,0.24451754385964913,Published as a conference paper at ICLR 2022
REFERENCES,0.24561403508771928,"Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just
how toxic is data poisoning? a uniﬁed benchmark for backdoor and data poisoning attacks. In
International Conference on Machine Learning, pp. 9389–9398. PMLR, 2021."
REFERENCES,0.24671052631578946,"Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016."
REFERENCES,0.24780701754385964,"Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certiﬁed defenses for data poisoning attacks. In
Proceedings of the 31st International Conference on Neural Information Processing Systems, pp.
3520–3532, 2017."
REFERENCES,0.24890350877192982,"Yanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online
{rl} with unknown dynamics. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=9r30XCjf5Dt."
REFERENCES,0.25,"Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. In
2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23–30.
IEEE, 2017."
REFERENCES,0.25109649122807015,"Rishi Veerapaneni, John D Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu,
Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement
learning. In Conference on Robot Learning, pp. 1439–1456. PMLR, 2020."
REFERENCES,0.25219298245614036,"Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song. Backdoorl: Back-
door attack against competitive reinforcement learning. arXiv preprint arXiv:2105.00579, 2021."
REFERENCES,0.2532894736842105,"Yue Wang, Esha Sarkar, Wenqing Li, Michail Maniatakos, and Saif Eddin Jabari. Stop-and-go: Ex-
ploring backdoor attacks on deep reinforcement learning-based trafﬁc congestion control systems.
arXiv preprint arXiv:2003.07859, 2020."
REFERENCES,0.2543859649122807,"Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992."
REFERENCES,0.25548245614035087,"Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li. Rab: Provable robustness against
backdoor attacks. arXiv preprint arXiv:2003.08904, 2020."
REFERENCES,0.2565789473684211,"Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certiﬁed robustness for relu networks. In
International Conference on Machine Learning, pp. 5276–5285. PMLR, 2018."
REFERENCES,0.2576754385964912,"Fan Wu, Linyi Li, Zijian Huang, Yevgeniy Vorobeychik, Ding Zhao, and Bo Li. Crop: Certify-
ing robust policies for reinforcement learning through functional smoothing. In International
Conference on Learning Representations, 2022."
REFERENCES,0.25877192982456143,"Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games
with limited data. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.2598684210526316,"Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui
Hsieh.
Robust deep reinforcement learning against adversarial perturbations on state obser-
vations.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21024–21037. Curran Asso-
ciates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/
f0eb6568ea114ba6e293f903c34d7488-Paper.pdf."
REFERENCES,0.26096491228070173,"Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks
against reinforcement learning. In International Conference on Machine Learning, pp. 11225–
11234. PMLR, 2020b."
REFERENCES,0.26206140350877194,"Xuezhou Zhang, Yiding Chen, Jerry Zhu, and Wen Sun. Corruption-robust ofﬂine reinforcement
learning. arXiv preprint arXiv:2106.06630, 2021."
REFERENCES,0.2631578947368421,Published as a conference paper at ICLR 2022
REFERENCES,0.2642543859649123,"A
OMITTED DEFINITIONS"
REFERENCES,0.26535087719298245,"A.1
CUMULATIVE REWARD"
REFERENCES,0.26644736842105265,"Deﬁnition 6 (Cumulative Reward). Let P : S × A →P(S) be the transition function with P(·)
deﬁning the set of probability measures. Let R, d0, H be the reward function, initial state distribu-
tion, and time horizon. We denote J(π) as the cumulative reward of the given policy π:"
REFERENCES,0.2675438596491228,"J(π) := E H−1
X"
REFERENCES,0.26864035087719296,"t=0
R(st, at), where at = π(st), st+1 ∼P(st, at), s0 ∼d0,
(11)"
REFERENCES,0.26973684210526316,"It is natural to adapt this deﬁnition when there is a discount factor. If policy π considers recent nt
states instead of only st to make action predictions, we can change at = π(st) to at = π(st−nt+1:t)
in Equation (11)."
REFERENCES,0.2708333333333333,"A.2
PARL (PER-STATE PARTITION AGGREGATION) PROTOCOL"
REFERENCES,0.2719298245614035,"Deﬁnition 7 (Per-State Partition Aggregation). Given subpolicies {πi}u−1
i=0 , the Per-State Partition
Aggregation (PARL) protocol deﬁnes an aggregated policy πP : S →A such that
πP(s) := arg max
a∈A
na(s),
(12)"
REFERENCES,0.2730263157894737,where na(s) is deﬁned in Section 3.
REFERENCES,0.2741228070175439,"As mentioned in Section 4, when the arg max in Equation (12) returns multiple elements, we break
ties deterministically by returning the “smaller” (<) action, which can be deﬁned by numerical
order, lexicographical order, etc."
REFERENCES,0.27521929824561403,"A.3
TPARL (TEMPORAL PARTITION AGGREGATION) PROTOCOL"
REFERENCES,0.27631578947368424,"Deﬁnition 8 (Temporal Partition Aggregation). Given subpolicies {πi}u−1
i=0 and window size W,
at time step t, the Temporal Partition Aggregation (TPARL) deﬁnes an aggregated policy πT :
Smin{t+1,W } →A such that
πT(smax{t−W +1,0}:t) = arg max
a∈A
na(smax{t−W +1,0}:t),
(13)"
REFERENCES,0.2774122807017544,where sl:r and na are deﬁned in Section 3.
REFERENCES,0.27850877192982454,"In the above deﬁnition, at time step t, the input of policy πT contains all states between time step
max{t −W + 1, 0} and t within window size W; and the output is the policy prediction with the
highest votes across these states. Recall that in Section 3, we deﬁne na(sl:r) = Pr
j=l na(sj). We
break ties in the same way as in PARL."
REFERENCES,0.27960526315789475,"B
ILLUSTRATION OF COPA AGGREGATION PROTOCOLS"
REFERENCES,0.2807017543859649,"We present a concrete example to demonstrate how different aggregation protocols induce different
tolerable poisoning thresholds, illustrate bottleneck and non-bottleneck states, and provide addi-
tional discussions."
REFERENCES,0.2817982456140351,"Suppose the action space contains two actions A = {a1, a2}, and there are six subpolicies {πi}u−1
i=0
where u = 6. We are at time step t = 7 now. When there is no poisoning, the subpolicies’ action
predictions for state s0 to s7 are shown by Table 2. After aggregation, all aggregated policies will
choose action a1 at t = 7."
REFERENCES,0.28289473684210525,"PARL (Deﬁnition 7).
The PARL aggregation protocol only uses the current state s7 to aggregate
the votes. On s7, three subpolicies choose action a1 and three others choose action a2. By breaking
the tie with a1 < a2, the PARL policy πP(s7) = a1. The tolerable poisoning threshold Kt = 0,
because one action ﬂipping from a1 to a2 by poisoning only one subpolicy can change the aggregated
policy to a2."
REFERENCES,0.28399122807017546,"TPARL (Deﬁnition 8).
The TPARL aggregation protocol uses window size W = 7. This implies
that, we aggregate all states s1:7 to count the votes and decide the action. Since na1(s1:7) = 36 and"
REFERENCES,0.2850877192982456,Published as a conference paper at ICLR 2022
REFERENCES,0.28618421052631576,"Table 2: A concrete example of action predictions, where “1” means action a1 and “2” means action a2. When
there is no poisoning attack, the corresponding time window spans by PARL, TPARL, and DPARL are shown
by green , blue , and pink respectively. All aggregated policies choose action a1, but have different tolerable
poisoning thresholds as shown in the last column."
REFERENCES,0.28728070175438597,"Action Prediction for
s0
s1
s2
s3
s4
s5
s6
s7
K at s7
π0
1
1
1
1
1
1
1
1
π1
1
1
1
1
1
1
1
1
π2
1
1
1
1
1
1
1
1
π3
1
1
1
1
1
1
1
2
π4
1
1
1
1
1
1
1
2
π5
1
1
1
2
1
2
2
2
PARL (πP, Deﬁnition 7)
1
0
TPARL with W = 7 (πT, Deﬁnition 8)
1
2
DPARL with Wmax = 8 (πD, Deﬁnition 3)
1
1"
REFERENCES,0.2883771929824561,"na2(s1:7) = 6, after poisoning two subpolicies, we still ena1(s1:7) ≥ena2(s1:7). Thus, we achieve an
tolerable poisoning threshold Kt = 2."
REFERENCES,0.2894736842105263,"Compared with PARL, the temporal aggregation in TPARL increases the vote margin between a1
and a2 and thus improve the tolerable poisoning threshold at current state."
REFERENCES,0.2905701754385965,"DPARL (Deﬁnition 3).
The DPARL aggregation protocol chooses the window size W ′ = 8 since
this window size gives the largest average vote margin ∆W ′
t
(deﬁned in Equation (1)). We can easily
ﬁnd out that with poisoning size K = 1, we will still choose f
W ′ = 8 as the window size and the
resulting votes for a1 can be kept higher than votes for a2. However, when it comes to K = 2, if we
totally ﬂip subpolicies π0 and π1 to let them always predict action a2, then the DPARL aggregated
policy will turn to choose W ′ = 1 as the window size and then choose action a2 as the action output.
Thus, we achieve a tolerable poisoning threshold Kt = 1 this time."
REFERENCES,0.2916666666666667,"Illustrations of Bottleneck and Non-Bottleneck States.
According to our illustration in Sec-
tion 4.2, bottleneck states are those where subpolicies vote for diverse actions but the best action
is the same as previous states, and non-bottleneck states are those where subpolicies mostly vote
for the same action. As we can see, s0 to s6 are non-bottleneck states since the subpolicies almost
unanimously vote for one action. In contrast, s7 is a bottleneck state."
REFERENCES,0.29276315789473684,"From the perspectives of different aggregation protocols, we observe that for the bottleneck state
s7, both TPARL and DPARL exploit temporal aggregation to boost the certiﬁed robust poisoning
threshold (from 0 to 2 and from 0 to 1 respectively), thus demonstrating the effectiveness of TPARL
and DPARL on improving certiﬁed robustness. On the other hand, for non-bottleneck states s0 to
s6, we can easily see that different aggregation protocols do not differ much in terms of provided
certiﬁed robustness levels."
REFERENCES,0.29385964912280704,"Explanations for Bottleneck and Non-Bottleneck States.
We provide additional discussions
regarding why bottleneck states are vulnerable and why our temporal aggregation strategy can ef-
fectively alleviate the problem. We ﬁrst explain the existence of the bottleneck states. Given the
property of the bottleneck states that there is high disagreement among different subpolicies on such
states, they may lie close to the decision boundary. This may be a result of poisoning, or simply due
to the intrinsic difﬁculty of the state. On the other hand, such states naturally exist in the rollouts
(like natural adversarial examples (Hendrycks et al., 2021)), and may induce high instability of the
rollouts during testing. We next discuss additional intuitions for our temporal aggregation. Essen-
tially, temporal aggregation effectively leverages the adjacent non-bottleneck states to rectify the
decisions at the bottleneck states, based on the assumption of temporal continuity which has been
revealed and utilized in several previous works (Legenstein et al., 2010; Veerapaneni et al., 2020; Ye
et al., 2021)."
REFERENCES,0.2949561403508772,Published as a conference paper at ICLR 2022
REFERENCES,0.29605263157894735,"C
CERTIFICATION OF PER-STATE ACTION STABILITY FOR PARL"
REFERENCES,0.29714912280701755,"We certify the robustness of PARL following Theorem 8.
Theorem 8. Let D be the clean training dataset; let πi = M0(Di), 0 ≤i ≤u −1 be the learned
subpolicies according to Section 4.1 from which we deﬁne na (see Section 3); and let πP be the Per-
State Partition Aggregation policy: πP = M(D) where M abstracts the whole training-aggregation
process. eD is a poisoned dataset and f
πP is the poisoned policy: f
πP = M( eD)."
REFERENCES,0.2982456140350877,"For a given state st encountered at time step t during test time, let a := πP(st), then"
REFERENCES,0.2993421052631579,"Kt =
na(st) −maxa′̸=a (na′(st) + 1[a′ < a]) 2"
REFERENCES,0.30043859649122806,"
.
(14)"
REFERENCES,0.30153508771929827,"Remark. The theorem provides a valid per-state action certiﬁcation for PARL, since according to
Deﬁnition 1, as long as the poisoning size is smaller or equal to Kt, i.e., |D ⊖eD| ≤Kt, the action
of the poisoned policy is the same as the clean policy: f
πP(st) = πP(st) = a. To compute Kt,
according to Equation (14), we rely on the aggregated action count na for state st from subpolicies.
The a′ < a is the “smaller-than” operator introduced following Deﬁnition 7."
REFERENCES,0.3026315789473684,"Proof of Theorem 8. Suppose the poisoning size is within K, then after poisoning, the aggregated
action count ena(st) ∈[na(st)−K, na(st)+K] where na(st) is such count before poisoning. Thus,
to ensure the chosen action does not change, a necessary condition is that ena(st) ≥ena′(st)+1[a′ <
a] for any other a′ < a. Solving K yields Equation (14)."
REFERENCES,0.30372807017543857,"We use the theorem to compute the per-state action certiﬁcation for each time step t along the tra-
jectory, and the detailed algorithm is in Algorithm 2 (Appendix E). Furthermore, we prove that this
certiﬁcation is theoretically tight as the following proposition shows. The proof is in Appendix F.1.
Proposition 9. Under the same condition as Theorem 8, for any time step t, there exists an RL learn-
ing algorithm M0, and a poisoned dataset eD, such that |D ⊖eD| = Kt + 1, and f
πP(st) ̸= πP(st)."
REFERENCES,0.3048245614035088,"D
A TABLE OF POSSIBLE ACTION SET"
REFERENCES,0.3059210526315789,"Table 3: Expressions of possible action set A(K) given poisoning threshold K for different aggregation
protocols. Full theorem statements are in Theorems 4, 5 and 7 (in Section 4.4)."
REFERENCES,0.30701754385964913,"Protocol
A(K) PARL"
REFERENCES,0.3081140350877193,"n
a ∈A
 P"
REFERENCES,0.3092105263157895,"a′∈A max{na′(st) −na(st) −K + 1[a′ < a], 0} ≤K
o TPARL"
REFERENCES,0.31030701754385964,"n
a ∈A
 PK
i=1 h(i)
a′,a > δa′,a, ∀a′ ̸= a
o"
REFERENCES,0.31140350877192985,"DPARL
{at} ∪

a′ ∈A

min
1≤W ∗≤min{Wmax,t+1},W ∗̸=W ′,a′′̸=at LW ∗,W ′"
REFERENCES,0.3125,"a′,a′′
≤K

∪ ( a ∈A K
X"
REFERENCES,0.31359649122807015,"i=1
h(i)
a′,a > δa′,a, ∀a′ ̸= a )"
REFERENCES,0.31469298245614036,"For all three aggregation protocols, we summarize the expressions used for computing the possible
action set A(K) given tolerable poisoning threshold K in Table 3."
REFERENCES,0.3157894736842105,"E
ALGORITHM PSEUDOCODE AND DISCUSSION"
REFERENCES,0.3168859649122807,"E.1
COPA TRAINING PROTOCOL"
REFERENCES,0.31798245614035087,"Algorithm 1: COPA training protocol.
Input: training dataset D, number of partitions k, deterministic hash function h
Output: COPA subpolicies {πi}k−1
i=0
1 for i ∈[k] do"
REFERENCES,0.3190789473684211,"2
Di ←{τ ∈D | h(τ) ≡i (mod k)}
▷Separate the training data D into k partitions"
FOR EACH PARTITION DI DO,0.3201754385964912,3 for each partition Di do
FOR EACH PARTITION DI DO,0.32127192982456143,"4
πi ←M0(Di)
▷Subpolicy trained on partition Di with offline RL algorithm M0"
FOR EACH PARTITION DI DO,0.3223684210526316,"5 return {πi}k−1
i=0"
FOR EACH PARTITION DI DO,0.32346491228070173,Published as a conference paper at ICLR 2022
FOR EACH PARTITION DI DO,0.32456140350877194,"E.2
COPA PER-STATE ACTION CERTIFICATION"
FOR EACH PARTITION DI DO,0.3256578947368421,Per-State Partition Aggregation (PARL).
FOR EACH PARTITION DI DO,0.3267543859649123,"Algorithm 2: COPA per-state certiﬁcation algorithm for Per-State Partition Aggrega-
tion (PARL)."
FOR EACH PARTITION DI DO,0.32785087719298245,"Input: environment E = (S, A, R, P, H, d0), subpolicies {πi}k−1
i=0
Output: COPA robust size at each time step {Kt}H−1
t=0
1 s0 ∼d0
▷sample initial state"
FOR EACH PARTITION DI DO,0.32894736842105265,2 for t from 0 to H −1 do
FOR EACH PARTITION DI DO,0.3300438596491228,"3
for each a ∈A do"
FOR EACH PARTITION DI DO,0.33114035087719296,"4
Compute na(st) from subpolicies’ {πi}k−1
i=0 decisions
▷na is defined in Section 3"
FOR EACH PARTITION DI DO,0.33223684210526316,"5
Determine the chosen action at ←πP(st) according to PARL (Deﬁnition 7)"
FOR EACH PARTITION DI DO,0.3333333333333333,"6
Compute Kt according to Equation (14) in Theorem 8"
FOR EACH PARTITION DI DO,0.3344298245614035,"7
st+1 ∼P(st, at)"
FOR EACH PARTITION DI DO,0.3355263157894737,"8 return {Kt}H−1
t=0"
FOR EACH PARTITION DI DO,0.3366228070175439,Temporal Partition Aggregation (TPARL).
FOR EACH PARTITION DI DO,0.33771929824561403,"Algorithm 3: COPA per-state certiﬁcation algorithm for Temporal Partition Aggrega-
tion (TPARL)."
FOR EACH PARTITION DI DO,0.33881578947368424,"Input: environment E = (S, A, R, P, H, d0), subpolicies {πi}k−1
i=0 , window size W
Output: COPA robust size at each time step {Kt}H−1
t=0
1 s0 ∼d0
▷sample initial state"
FOR EACH PARTITION DI DO,0.3399122807017544,2 for t from 0 to H −1 do
FOR EACH PARTITION DI DO,0.34100877192982454,"3
for each a ∈A do"
FOR EACH PARTITION DI DO,0.34210526315789475,"4
Compute na(st) from subpolicies’ {πi}k−1
i=0 decisions
▷na is defined in Section 3"
FOR EACH PARTITION DI DO,0.3432017543859649,"5
Determine the chosen action at ←πT(st) according to TPARL (Deﬁnition 8)"
FOR EACH PARTITION DI DO,0.3442982456140351,"6
pmin ←∞"
FOR EACH PARTITION DI DO,0.34539473684210525,"7
for each a′ ∈A, a′ ̸= at do"
FOR EACH PARTITION DI DO,0.34649122807017546,"8
for i from 0 to k −1 do"
FOR EACH PARTITION DI DO,0.3475877192982456,"9
Compute hi,at,a′ according to Equation (4)"
FOR EACH PARTITION DI DO,0.34868421052631576,"10
{h(i)
at,a′}k
i=1 ←sorted({hi,at,a′}k−1
i=0 , reverse = True)"
FOR EACH PARTITION DI DO,0.34978070175438597,"11
Compute δat,a′"
FOR EACH PARTITION DI DO,0.3508771929824561,"12
sum ←0, p ←0"
FOR EACH PARTITION DI DO,0.3519736842105263,"13
for j from 1 to k do"
FOR EACH PARTITION DI DO,0.3530701754385965,"14
if sum + h(j)
at,a′ > δat,a′ then"
FOR EACH PARTITION DI DO,0.3541666666666667,"15
p ←j −1"
BREAK,0.35526315789473684,"16
break"
BREAK,0.35635964912280704,"17
p ←j, sum ←sum + h(j)
at,a′"
BREAK,0.3574561403508772,"18
pmin ←min{pmin, p}"
BREAK,0.35855263157894735,"19
Kt ←pmin"
BREAK,0.35964912280701755,"20
st+1 ∼P(st, at)"
BREAK,0.3607456140350877,"21 return {Kt}H−1
t=0"
BREAK,0.3618421052631579,Published as a conference paper at ICLR 2022
BREAK,0.36293859649122806,Dynamic Temporal Partition Aggregation (DPARL).
BREAK,0.36403508771929827,"Algorithm 4: COPA per-state certiﬁcation algorithm for Dynamic Temporal Partition Aggrega-
tion (DPARL)."
BREAK,0.3651315789473684,"Input: environment E = (S, A, R, P, H, d0), subpolicies {πi}k−1
i=0 , maximum window size Wmax
Output: COPA robust size at each time step {Kt}H−1
t=0
1 s0 ∼d0
▷sample initial state"
BREAK,0.36622807017543857,2 for t from 0 to H −1 do
BREAK,0.3673245614035088,"3
for each a ∈A do"
BREAK,0.3684210526315789,"4
Compute na(st) from subpolicies’ {πi}k−1
i=0 decisions given st ▷na is defined in Section 3"
BREAK,0.36951754385964913,"5
Determine the chosen action at ←πD(st) and chosen window size W ′ according to
DPARL (Deﬁnition 3)"
BREAK,0.3706140350877193,"6
pmin ←∞"
BREAK,0.3717105263157895,"7
Kt ←Algorithm 3
▷use Algorithm 3 with W replaced by W ′ to compute Kt"
KD,0.37280701754385964,"8
KD
t ←Kt"
KD,0.37390350877192985,"9
for each a′ ∈A, a′ ̸= at do"
KD,0.375,"10
for each a′′ ∈A, a′′ ̸= at do"
KD,0.37609649122807015,"11
for W ∗from 1 to min{Wmax, t + 1} do"
KD,0.37719298245614036,"12
a# ←arg maxa0̸=a′,a0∈A na0(st−W ∗+1:t)"
KD,0.3782894736842105,"13
for w from 1 to max{W ′, W ∗} do"
KD,0.3793859649122807,"14
Compute maxa0∈A σw(a0) according to Equation (7)"
KD,0.38048245614035087,"15
for i from 0 to k −1 do"
KD,0.3815789473684211,"16
for w from 1 to max{W ′, W ∗} do"
KD,0.3826754385964912,"17
Compute σw(πi(st−w)) according to Equation (7)"
KD,0.38377192982456143,"18
Compute gi according to Equation (7)"
KD,0.3848684210526316,"19
{g(i)}k
i=1 ←sorted({gi}k−1
i=0 , reverse = True)"
KD,0.38596491228070173,"20
Compute nW ∗
a′ , nW ∗"
KD,0.38706140350877194,"a# , nW ′
at , nW ′
a′′"
KD,0.3881578947368421,"21
tmp ←W ′(nW ∗
a′
−nW ∗"
KD,0.3892543859649123,"a# ) −W ∗(nW ′
at −nW ′
a′′ ) −1[a′ > at]"
KD,0.39035087719298245,"22
sum ←0, p ←0"
KD,0.39144736842105265,"23
for j from 1 to k do"
KD,0.3925438596491228,"24
if sum + tmp ≥0 then"
KD,0.39364035087719296,"25
p ←j −1"
BREAK,0.39473684210526316,"26
break"
BREAK,0.3958333333333333,"27
p ←j, sum ←sum + g(j)"
BREAK,0.3969298245614035,"28
LW ∗,W ′"
BREAK,0.3980263157894737,"a′,a′′
←p, KD
t ←min{KD
t , LW ∗,W ′"
BREAK,0.3991228070175439,"a′,a′′
}"
BREAK,0.40021929824561403,"29
st+1 ∼P(st, at)"
BREAK,0.40131578947368424,"30 return {KD
t }H−1
t=0"
BREAK,0.4024122807017544,"E.3
COPA CUMULATIVE REWARD CERTIFICATION"
BREAK,0.40350877192982454,"COPA-SEARCH alternately executes the procedure of trajectory exploration and expansion and
poisoning threshold growth. In trajectory exploration and expansion, COPA-SEARCH organizes all
possible trajectories in the form of a search tree and progressively grows it. For each node (represent-
ing a state), we leverage Theorems 4, 5 and 7 to compute the Possible Action Set. We then expand
the tree branches corresponding to the actions in the derived set. In poisoning threshold growth,
when all trajectories for the current poisoning threshold are explored, we increase K′ to seek certi-
ﬁcation under larger poisoning sizes, via maintaining a priority queue of all poisoning sizes during
the expansion of the tree. The iterative procedures end when the priority queue becomes empty."
BREAK,0.40460526315789475,Published as a conference paper at ICLR 2022
BREAK,0.4057017543859649,"The highlighted line in the following algorithm needs to inject different algorithms based on the
aggregation protocol: for PARL (πP), use Theorem 4; for TPARL (πT), use Theorem 5; for
DPARL (πD), use Theorem 7."
BREAK,0.4067982456140351,Algorithm 5: COPA-SEARCH: adaptive tree search for cumulative reward certiﬁcation.
BREAK,0.40789473684210525,"Input: environment E = (S, A, R, P, H, d0), subpolicies {πi}k−1
i=0 , aggregated policy πP or πT (with
W) or πT (with Wmax)
Output: a map M that maps poisoning size K to corresponding certiﬁed lower bound of cumulative
reward JK
▷Initialize global variables"
BREAK,0.40899122807017546,"1 p que ←∅
▷initialize an empty priority queue containing tuples of
(state historys0:t, action a, poisoning size K, reward J) sorted by increasing K"
BREAK,0.4100877192982456,"2 Jglobal ←∞
▷initialize global minimum reward"
BREAK,0.41118421052631576,"3 Function GETACTIONS(s0:t, Klim, Jcur):"
BREAK,0.41228070175438597,"4
A ←possibleActions(s0:t, {πi}k−1
i=1 , π∗, Klim)
▷compute the possible action set given
state history, subpolicies, aggregation policy π∗, and poisoning size according to
theorems in Section 4.4"
BREAK,0.4133771929824561,"5
a list ←∅"
BREAK,0.4144736842105263,"6
for each action a ∈A do"
BREAK,0.4155701754385965,"7
if P(s, a) = ⊥then"
CONTINUE,0.4166666666666667,"8
continue
▷game terminate here, no possible larger or lower cumulative reward
to search"
CONTINUE,0.41776315789473684,"9
a list ←a list ∪{a}
▷record possible actions to expand"
CONTINUE,0.41885964912280704,"10
if A ̸= A then"
CONTINUE,0.4199561403508772,"11
K′ ←minpossibleActions(s0:t,{πi}k−1
i=0 ,π∗,K)̸=A K"
CONTINUE,0.42105263157894735,"12
Anew ←possibleActions(s0:t, {πi}k−1
i=1 , π∗, K′) \ A ▷compute the immediate actions that
will possibly be chosen if enlarging the poisoning size"
CONTINUE,0.42214912280701755,"13
for each action a ∈Anew do"
CONTINUE,0.4232456140350877,"14
p que.push((s0:t, a, K′, Jcur))"
RETURN A LIST,0.4243421052631579,"15
return a list"
RETURN A LIST,0.42543859649122806,"16 Procedure EXPAND(s0:t, Klim, Jcur):"
RETURN A LIST,0.42653508771929827,"17
if Jcur ≥Jglobal ∧(step reward is non-negative) then"
RETURN,0.4276315789473684,"18
return
▷pruning"
RETURN,0.42872807017543857,"19
a list ←GETACTIONS(s0:t, {πi}k−1
i=0 , π∗, Klim)
▷compute according to theorems in
Section 4.4"
RETURN,0.4298245614035088,"20
if a list = ∅then"
RETURN,0.4309210526315789,"21
Jglobal ←min{Jglobal, Jcur}"
RETURN,0.43201754385964913,"22
return"
RETURN,0.4331140350877193,"23
for a ∈a list do"
RETURN,0.4342105263157895,"24
st+1 ←P(st, a)"
RETURN,0.43530701754385964,"25
if st+1 = ⊥then"
RETURN,0.43640350877192985,"26
Jglobal ←min{Jglobal, Jcur}"
ELSE,0.4375,"27
else"
ELSE,0.43859649122807015,"28
EXPAND (s0:t+1, Klim, Jcur + R(st, a))"
ELSE,0.43969298245614036,29 M ←∅
ELSE,0.4407894736842105,30 s0 ←d0
ELSE,0.4418859649122807,"▷initial state is s0
31 EX P AND (s0, Klim = 0, Jcur = 0)
▷expand initial trajectory"
WHILE TRUE DO,0.44298245614035087,32 while True do
WHILE TRUE DO,0.4440789473684211,"33
if p que = ∅then"
BREAK,0.4451754385964912,"34
break
▷no state to expand"
BREAK,0.44627192982456143,"35
(s0:t, a, K, J) ←p que.pop()
▷pop out the first element"
BREAK,0.4473684210526316,"36
( , , K′, ) ←p que.top()
▷examine the next element"
BREAK,0.44846491228070173,"37
M(K) ←Jglobal
▷obtain one new point of certification result"
BREAK,0.44956140350877194,"38
st+1 ←P(st, a)"
BREAK,0.4506578947368421,"39
EXPAND (s0:t+1, K′, J + R(st, a))
▷expand the tree from the new node"
RETURN M,0.4517543859649123,"40 return M
▷indeed the algorithm can terminate at any time within the while loop"
RETURN M,0.45285087719298245,Published as a conference paper at ICLR 2022
RETURN M,0.45394736842105265,"Time Complexity.
The complexity of COPA-SEARCH is O(H|Sexplored|(log |Sexplored| +
|A|T)), where |Sexplored| is the number of explored states throughout the search procedure, which is
no larger than cardinality of state set S, H is the horizon length, |A| is the cardinality of action set,
and T is the time complexity of per-state action certiﬁcation. The main bottleneck of the algorithm
is the large number of possible states, which is in the worse case exponential to state dimension.
However, to provide a deterministic worst-case certiﬁcation agnostic to environment properties, ex-
ploring all possible states may be inevitable."
RETURN M,0.4550438596491228,"Relation with Wu et al. (2022).
The COPA-SEARCH algorithm is inspired from Wu et al.
(2022, Algorithm 3), which also leverages tree search to explore all possible states and thus derive
the robustness guarantee. However, the major distinction is that their algorithm tries to derive a
probabilistic guarantee of RL robustness against state perturbations, while COPA-SEARCH derives
deterministic guarantee of RL robustness against poisoning attacks. We think this general tree search
methodology can be further extended to provide certiﬁcation beyond evasion attack (Wu et al., 2022)
and poisoning attack (our work) and we leave it as future work."
RETURN M,0.45614035087719296,"Extension to stochastic MDPs.
In the current version of COPA-SEARCH, the exhaustive search
is enabled by the deterministic MDP assumption. However, we foresee the potential of conveniently
extending COPA-SEARCH to stochastic MDPs and will discuss one concrete method below. In con-
trast to interacting with the environment for one time to obtain the deterministic next state transition
in the deterministic MDP case, in the case of stochastic MDPs, we can leverage sampling (i.e., by
repeatedly taking the same action at the same state) to obtain the set of high probability next state
transitions with high conﬁdence. In this way, our COPA-SEARCH will be able to yield a proba-
bilistic bound, in comparison with the deterministic bound achieved in this paper enabled by the
deterministic MDP assumption."
RETURN M,0.45723684210526316,"F
PROOFS"
RETURN M,0.4583333333333333,"F.1
TIGHTNESS OF PER-STATE ACTION CERTIFICATION IN PARL"
RETURN M,0.4594298245614035,"Proof of Proposition 9. We prove by construction. Given the state st, we ﬁrst locate the subpolicies
whose chosen action is a, and denote the set of them as
B = {i ∈[u] | πi(st) = a = πP(st)}.
We also denote a′ = arg maxa′̸=a na′(st)+1[a′ < a]. According to Kt’s deﬁnition (Equation (14)),
|B| = na(st) > na(st)/2 ≥Kt.
We now pick an arbitrary subset B′ ⊆B such that |B′| = Kt + 1. For each i ∈B′, we locate its
corresponding parititioned dataset Di for training subpolicy πi. We insert one point pi to Di, such
that our chosen learning algorithm M0 can train a subpolicy ˜πi = M0(Di ∪{pi}) that satisﬁes
˜πi(st) = a′. For example, the point could be pi = {(st, a′, s′, ∞)} and M0 learns the action with
maximum reward for the memorized nearest state, where s′ can be adjusted to make sure the point
is hashed to parition i.1 Then, we construct the poisoned dataset eD = ["
RETURN M,0.4605263157894737,"i∈B′
Di ∪{pi} ! ∪  
["
RETURN M,0.4616228070175439,"i∈[u]\B′
Di  ."
RETURN M,0.46271929824561403,"Therefore, we have eD ⊖D = ∪i∈B′pi = |B′| = Kt + 1."
RETURN M,0.46381578947368424,"On this poisoned dataset, we train subpolicies {˜πi}u−1
i=0 and get the aggregated policy f
πP. To study
f
πP(st), we compute the aggregated action count ena on these poisoned subpolicies. We found that"
RETURN M,0.4649122807017544,"1Strictly speaking, we need to choose a determinisitc hash function h for dataset partitioning such that
adjustment on s′ and reward (currently ∞, but can be an arbitrary large enough number) can make the point
being partitioned to i, i.e., h(pi) ≡i (mod u). Since our adjustment space is inﬁnite due to inﬁnite number of
large enough reward, such assumption can be easily achieved. Same applies to other attack constructions in the
following proofs."
RETURN M,0.46600877192982454,Published as a conference paper at ICLR 2022
RETURN M,0.46710526315789475,"ena(st) = na(st) −|B′| and ena′(st) = na′(st) + |B′|. Therefore,
ena(st) −ena′(st) = na(st) −na′(st) −2(Kt + 1)"
RETURN M,0.4682017543859649,"= na(st) −na′(st) −2
jna(st) −na′(st) −1[a′ < a] 2"
RETURN M,0.4692982456140351,"k
+ 1
"
RETURN M,0.47039473684210525,"< na(st) −na′(st) −(na(st) −na′(st) −1[a′ < a]) = 1[a′ < a].
Therefore, if a′ < a, ena(st) ≤ena′(st), and a′ has higher priority to be chosen than a; if a′ > a,
ena(st) ≤ena′(st) −1, and a′ still has higher priority to be chosen than a. Hence, f
πP(st) ̸= a =
πP(st). To this point, we conclude the proof with a feasible construction."
RETURN M,0.47149122807017546,"F.2
PER-STATE ACTION CERTIFICATION IN TPARL"
RETURN M,0.4725877192982456,"Proof of Theorem 1. For ease of notation, we let w = min{W, t+1} so w is the actual window size
used at step t. We let t0 = t −w + 1, i.e., t0 is the actual start time step for TPARL aggregation at
step t. Now we can write the chosen action at step t without poisoning as a = πT(st0:t)."
RETURN M,0.47368421052631576,"We prove the theorem by contradiction: We assume that there is a poisoning attack whose poisoning
size K ≤Kt where Kt is deﬁned by Equation (3) in the theorem, and after poisoning, the chosen
action is aT ̸= a. We denote {˜πi}u−1
i=0 to the poisoned subpolicies and f
πT to the poisoned TPARL
aggregated policy. From the deﬁnition of Kt, we have
K
X"
RETURN M,0.47478070175438597,"i=1
h(i)
a,aT ≤ Kt
X"
RETURN M,0.4758771929824561,"i=1
h(i)
a,aT ≤δa,aT ,
(15)"
RETURN M,0.4769736842105263,"since K ≤Kt, and each h(i)
a,aT is an element of hi′,a,aT for some i′ where"
RETURN M,0.4780701754385965,"hi′,a,aT = w−1
X"
RETURN M,0.4791666666666667,"j=0
1i′,a(st−j) + w − w−1
X"
RETURN M,0.48026315789473684,"j=0
1i′,aT (st−j) ≥0"
RETURN M,0.48135964912280704,"by Equation (4) in the theorem. Since the poisoning attack within threshold K can at most affect K
subpolicies, we let B be the set of affected policies and assume |B| = K without loss of generality.
Formally, B = {i ∈[u] | ∃t′ ∈[t0, t], ˜πi(st′) ̸= πi(st′)}. Therefore, according to the monotonicity
of h(i)
a,aT , from Equation (15),
X"
RETURN M,0.4824561403508772,"i∈B
hi,a,aT ≤ K
X"
RETURN M,0.48355263157894735,"i=1
h(i)
a,aT ≤δa,aT .
(16)"
RETURN M,0.48464912280701755,"According to the assumption of successfully poisoning attack, after attack, the sum of aggregated
vote for aT plus 1[aT < a] should be larger then that of a. Formally, after the poisoning,
enaT (st0:t) + 1[aT < a] > ena(st0:t)
or equivalently
ena(st0:t) −(enaT (st0:t) + 1[aT < a]) < 0.
From the statement of Theorem 1,
δa,aT = na(st0:t) −(naT (st0:t) + 1[aT < a]).
Take the difference of the above two equations, we know that the attack satisﬁes
na(st0:t) −ena(st0:t) −(naT (st0:t) −enaT (st0:t)) > δa,aT .
(17)
Since the attack only changes the subpolicies in B, we have"
RETURN M,0.4857456140350877,"na(st0:t) −ena(st0:t) =
X i∈B "
RETURN M,0.4868421052631579,"
w−1
X"
RETURN M,0.48793859649122806,"j=0
1[πi(st−j) = a] − w−1
X"
RETURN M,0.48903508771929827,"j=0
1[˜πi(st−j) = a]   ≤
X i∈B w−1
X"
RETURN M,0.4901315789473684,"j=0
1i,a(st−j),"
RETURN M,0.49122807017543857,"naT (st0:t) −enaT (st0:t) =
X i∈B "
RETURN M,0.4923245614035088,"
w−1
X"
RETURN M,0.4934210526315789,"j=0
1[πi(st−j) = aT ] − w−1
X"
RETURN M,0.49451754385964913,"j=0
1[˜πi(st−j) = aT ]   ≥
X i∈B "
RETURN M,0.4956140350877193,"
w−1
X"
RETURN M,0.4967105263157895,"j=0
1i,aT (stj) −w  ."
RETURN M,0.49780701754385964,Published as a conference paper at ICLR 2022
RETURN M,0.49890350877192985,"Inject them into Equation (17) yields
X i∈B "
RETURN M,0.5,"
w−1
X"
RETURN M,0.5010964912280702,"j=0
1i,a(st−j) + w − w−1
X"
RETURN M,0.5021929824561403,"j=0
1i,aT (st−j)  "
RETURN M,0.5032894736842105,"|
{z
}
hi,a,aT"
RETURN M,0.5043859649122807,"> δa,aT"
RETURN M,0.5054824561403509,which contradicts Equation (16) and thus concludes the proof.
RETURN M,0.506578947368421,"F.3
TIGHTNESS OF PER-STATE ACTION CERTIFICATION IN TPARL"
RETURN M,0.5076754385964912,"Proof of Proposition 2. For ease of notation, we let w = min{W, t + 1} so w is the actual window
size used at step t. We let t0 = t −w + 1, i.e., t0 is the actual start time step for TPARL aggregation
at step t. Now we can write the chosen action at step t without poisoning as a = πT(st0:t)."
RETURN M,0.5087719298245614,"We prove by construction, i.e., we construct an poisoning attack with poisoning size Kt + 1 that
deviates the prediction of the poisoned policy. Speciﬁcally, we aim to craft a poisoned dataset eD
with poisoning size |D⊖eD| = Kt+1, such that for certain learning algorithm M0, after partitioning
and learning on the poisoned dataset, the poisoned subpolicies ˜πi = M0( eDi) can be aggregated to
produce different action prediction: f
πT(st0:t) ̸= a."
RETURN M,0.5098684210526315,"Before construction, we ﬁrst show that Kt given by Equation (3) satisﬁes Kt < u. If Kt = u, it
means that for arbitrary a′ ̸= a, u
X"
RETURN M,0.5109649122807017,"i=1
h(i)
a,a′ = u−1
X"
RETURN M,0.5120614035087719,"i=0
hi,a,a′ = u−1
X i=0 "
RETURN M,0.5131578947368421,"
w−1
X"
RETURN M,0.5142543859649122,"j=0
1i,a(st−j) + w − w−1
X"
RETURN M,0.5153508771929824,"j=0
1i,a′(st−j) "
RETURN M,0.5164473684210527,= na(st0:t) + uw −na′(st0:t)
RETURN M,0.5175438596491229,"≤δa,a′ = na(st0:t) −na′(st0:t) −1[a′ < a],
which implies uw ≤0 contracting uw > 0. Now, we know Kt < u, so Kt + 1, our poisoning size,
is smaller or equal to u."
RETURN M,0.518640350877193,"We start our construction by choosing an action aT :
aT = arg min
a′̸=a,a′∈A
arg max
Pp
i=1 h(i)
a,a′≤δa,a′
p."
RETURN M,0.5197368421052632,"According to the deﬁnition of Kt in Equation (3), for aT we have"
RETURN M,0.5208333333333334,"Kt+1
X"
RETURN M,0.5219298245614035,"i=1
h(i)
a,aT > δa,aT .
(18)"
RETURN M,0.5230263157894737,"We locate the subpolicies to poison as
B = {i ∈[u] | hi
a,aT is h(j)
a,aT in the nonincreasing permutation in Equation (4), j ≤Kt + 1}
(19)
Therefore, |B| = Kt + 1 and
X"
RETURN M,0.5241228070175439,"i∈B
hi,a,aT > δa,aT
(20)"
RETURN M,0.5252192982456141,"by Equation (18). For each of these subpolicies i ∈B, we locate its corresponding partitioned
dataset Di for training subpolicy πi, and insert one trajectory pi to Di, such that our chosen learning
algorithm M0 can train a subpolicy ˜πi = M(Di ∪{pi}) satisfying ˜πi(st′) = aT for any t′ ∈
[t0, t]. For example, the trajectory could be pi = {(st′, aT , s′, ∞)}t
t′=t0 and M0 learns the action
with maximum reward for the memorized nearest state, where s′ can be adjusted to make sure the
trajectory is hashed to partition i. Then, we construct the poisoned dataset eD = ["
RETURN M,0.5263157894736842,"i∈B′
Di ∪{pi} ! ∪  
["
RETURN M,0.5274122807017544,"i∈[u]\B′
Di  ."
RETURN M,0.5285087719298246,"Therefore, we have eD ⊖D = ∪i∈B′pi = |B′| = Kt + 1. Now, we compare the aggregated votes
for a and aT before and after poisoning:"
RETURN M,0.5296052631578947,"ena(st0:t) −na(st0:t) =
X i∈B "
RETURN M,0.5307017543859649,"
w−1
X"
RETURN M,0.5317982456140351,"j=0
1[eπi(st−j) = a] − w−1
X"
RETURN M,0.5328947368421053,"j=0
1[πi(st−j) = a]  "
RETURN M,0.5339912280701754,"Published as a conference paper at ICLR 2022 = −
X i∈B w−1
X"
RETURN M,0.5350877192982456,"j=0
1[πi(st−j) = a],"
RETURN M,0.5361842105263158,"enaT (st0:t) −naT (st0:t) =
X i∈B "
RETURN M,0.5372807017543859,"
w−1
X"
RETURN M,0.5383771929824561,"j=0
1[eπi(st−j) = aT ] − w−1
X"
RETURN M,0.5394736842105263,"j=0
1[πi(st−j) = aT ]   =
X i∈B  w − w−1
X"
RETURN M,0.5405701754385965,"j=0
1[πi(st−j) = aT ]  ."
RETURN M,0.5416666666666666,"Now we compare the margin between aggregated votes for a and aT after poisoning:
enaT (st0:t) −ena(st0:t) + 1[aT < a]"
RETURN M,0.5427631578947368,=naT (st0:t) −na(st0:t) + 1[aT < a] + enaT (st0:t) −naT (st0:t) −(ena(st0:t) −na(st0:t))
RETURN M,0.543859649122807,"= −δa,aT +
X i∈B "
RETURN M,0.5449561403508771,"
w−1
X"
RETURN M,0.5460526315789473,"j=0
1[πi(st−j) = a] + w − w−1
X"
RETURN M,0.5471491228070176,"j=0
1[πi(st−j) = aT ]  "
RETURN M,0.5482456140350878,"= −δa,aT +
X"
RETURN M,0.5493421052631579,"i∈B
hi,a,aT > 0."
RETURN M,0.5504385964912281,"As a result, after poisoning, aT has higher priority to be chosen than a, i.e., f
πT(st0:t) ̸= a =
πT(st0:t). To this point, we conclude the proof with a feasible attack construction."
RETURN M,0.5515350877192983,"F.4
LOOSE PER-STATE ACTION CERTIFICATION IN TPARL AND COMPARISION WITH TIGHT
ONE"
RETURN M,0.5526315789473685,"The following corollary of Theorem 8 states a loose per-state action certiﬁcation.
Corollary 10. Under the same condition as Theorem 1, Kt ="
RETURN M,0.5537280701754386,"$
na(smax{t−W +1,0}:t) −maxa′̸=a
 
na′(smax{t−W +1,0}:t) + 1[a′ < a]
"
RETURN M,0.5548245614035088,"2 min{W, t + 1} % (21)"
RETURN M,0.555921052631579,"is a tolerable poisoning threshold at time step t in Deﬁnition 1, where W is the window size."
RETURN M,0.5570175438596491,"Proof of Corollary 10. We let w = min{t +1, W} to be the actual aggregation window size at time
step t. After poisoning with size K, at most K subpolicies are affected and each affected policy can
only make ±w changes to the aggregated action count. This implies that, after poisoning, for any
action a′ ∈A, the aggregated vote count ena′(st−w+1:t) ∈[na′(st−w+1:t) −uw, na′(st−w+1:t) +
uw]. Thus, when K ≤Kt where Kt is deﬁned in Theorem 1, for any a′ ̸= a, we have
˜na(st−w+1:t) −˜na′(st−w+1:t) −1[a′ < a]"
RETURN M,0.5581140350877193,=na(st−w+1:t) −na′(st−w+1:t) −1[a′ < a] −2Kw
RETURN M,0.5592105263157895,≥na(st−w+1:t) −na′(st−w+1:t) −1[a′ < a] −2w · Kt
RETURN M,0.5603070175438597,"≥na(st−w+1:t) −na′(st−w+1:t) −1[a′ < a] −

na(st−w+1:t) −max
a′′̸=a(na′′(st−w+1:t) + 1[a′′ < a])
"
RETURN M,0.5614035087719298,"= max
a′′̸=a(na′′(st−w+1:t) + 1[a′′ < a]) −(na′(st−w+1:t) + 1[a′ < a]) ≥0."
RETURN M,0.5625,"From the deﬁnition of TPARL protocol, the poisoned policy still chooses action a, which implies
Kt is a tolerable poisoning threshold."
RETURN M,0.5635964912280702,"In the main text, we mention that the certiﬁcation from Corollary 10 is looser than that from Theo-
rem 1. This assertion is based on the following two facts:"
RETURN M,0.5646929824561403,"1. According to Proposition 2, the certiﬁcation given by Theorem 1 is theoretically tight, which
means that any other certiﬁcation can only be as tight as Theorem 1 or looser than it.
2. There exists examples where the computed Kt from Theorem 1 is larger than that from Corol-
lary 10.
For instance, suppose W = 5, action set A = {a1, a2}, and there are three subpolicies. At time
step t = 4, πi for s0 to s4 are [a1, a1, a1, a1, a2] for all subpolicies (i.e., i ∈[3]). Thus, the"
RETURN M,0.5657894736842105,Published as a conference paper at ICLR 2022
RETURN M,0.5668859649122807,"benign policy πT(s0:4) = a1. By computation, the Kt from Theorem 1 is 1; while the Kt from
Corollary 10 is 0."
RETURN M,0.5679824561403509,"Indeed, Corollary 10 can be viewed as using 2w to upper bound hi,a,a′ = w + Pw−1
j=0 1i,a(st−j) −
Pw−1
j=0 1i,a′(st−j). Intuitively, Corollary 10 assumes every subpolicy can provide 2w vote margin
shrinkage, and Theorem 1 uses hi,a,a′ to capture the precise worse-case margin shrinkage and thus
provides a tighter certiﬁcation."
RETURN M,0.569078947368421,"F.5
PER-STATE ACTION CERTIFICATION IN DPARL"
RETURN M,0.5701754385964912,"Proof of Theorem 3. Without loss of generality, we assume Wmax ≤t + 1 and otherwise we let
Wmax ←min{Wmax, t + 1}. We let t0 = max{t −Wmax + 1, 0} be the start time step of the
maximum possible window. To prove the theorem, our general methodology is to enumerate all
possible cases of a successful attack, and derive the tolerable poisoning threshold for each case
respectively. Taking a minimum over these tolerable poisoning thresholds gives the required result."
RETURN M,0.5712719298245614,"Speciﬁcally, we denote P to the predicate of robustness under poisoning attack: P = [f
πD(st0:t) =
a], and denote K to the poisoning attack size. Therefore, we can decompose P as such:
P = P(W ′) ∧
^"
RETURN M,0.5723684210526315,"1≤W ∗≤Wmax,W ∗̸=W ′ a′̸=a"
RETURN M,0.5734649122807017,"¬Q(W ∗, a′).
(22)"
RETURN M,0.5745614035087719,"Recall that W ′ is the chosen window size by the protocol DPARL with unattacked subpolicies πD
(Equation (1)). In Equation (22), the predicate P(W ′) means that after poisoning attack, whether
the prediction under window size W ′ is still a; the predicate Q(W ∗, a′) means that after poisoning
attack, whether the chosen action is a′ at window size W ∗and average vote margin is larger (or equal
if a′ < a) at window size W ∗compared to W ′. Formally, let ena be the aggregated action count after
poisoning and e∆W
t
be the average vote margin after poisoning at window W (see Equation (2)),"
RETURN M,0.5756578947368421,"P(W ′) =

arg max
a∈A
ena(st−W ′+1:t) = a

,"
RETURN M,0.5767543859649122,"Q(W ∗, a′) =

arg max
a∈A
ena(st−W ∗+1:t) = a′

∧

e∆W ∗
t
≥e∆W ′
t

∧(a′ < a)

∨

e∆W ∗
t
> e∆W ′
t

∧(a′ > a)

. (23)"
RETURN M,0.5778508771929824,"According to Theorem 1,
K ≤Kt =⇒P(W ′)
where Kt is deﬁned by Equation (3) with W replaced by W ′. The following Lemma 11 shows a
sufﬁcient condition for Q(W ∗, a′). We then aggregate these conditions together with minimum to
obtain a sufﬁcient condition for P:"
RETURN M,0.5789473684210527,"K ≤min
"
RETURN M,0.5800438596491229,"Kt,
min
1≤W ∗≤min{Wmax,t+1},W ∗̸=W ′,a′̸=a,a′′̸=a LW ∗,W ′"
RETURN M,0.581140350877193,"a′,a′′ "
RETURN M,0.5822368421052632,and thus conclude the proof.
RETURN M,0.5833333333333334,"Lemma 11. Let Q(W ∗, a′), K, W ′ be the same as deﬁned in proof of Theorem 3, then
K ≤min
a′′̸=a LW ∗,W ′"
RETURN M,0.5844298245614035,"a′,a′′
=⇒¬Q(W ∗, a′),
(24)"
RETURN M,0.5855263157894737,"where LW ∗,W ′"
RETURN M,0.5866228070175439,"a′,a′′
is deﬁned in Deﬁnition 4."
RETURN M,0.5877192982456141,"Proof. We prove the equivalent form:
Q(W ∗, a′) =⇒K > min
a′′̸=a LW ∗,W ′"
RETURN M,0.5888157894736842,"a′,a′′
.
(25)"
RETURN M,0.5899122807017544,"Suppose a poisoning attack can successfully achieve Q(W ∗, a′), we now induce the requirement on
its poisoning size K. First, we notice that

e∆W ∗
t
≥e∆W ′
t

∧(a′ < a)

∨

e∆W ∗
t
> e∆W ′
t

∧(a′ > a)
"
RETURN M,0.5910087719298246,"⇐⇒W ∗W ′ e∆W ∗
t
≥W ∗W ′ e∆W ′
t
+ 1[a′ > a]
(26)"
RETURN M,0.5921052631578947,Published as a conference paper at ICLR 2022
RETURN M,0.5932017543859649,since W ∗W ′ e∆W ∗/W ∗
RETURN M,0.5942982456140351,"t
is an integer by deﬁnition. According to the deﬁnition and Q(W ∗, a′)’s
assumption that arg maxa∈A ena(st−W ∗+1:t) = a′,
W ∗W ′ e∆W ∗
t
≤W ′ (ena′(st−W ∗+1:t) −ena#(st−W ∗+1:t)) ,
where “≤” comes from the fact that the margin in e∆W ∗
t
should be with respect to the runner-up
class after poisoning, and computing with respect to any other class provides an upper bound. Here
we choose a# = arg maxa0̸=a′,a0∈A na0(st−W ∗+1:t) (see Deﬁnition 4), the runner-up class before
poisoning, to empirically shrink the gap between the bound and actual margin. On the other hand,"
RETURN M,0.5953947368421053,"W ∗W ′ e∆W ′
t
≥W ∗

ena(st−W ′+1:t) −max
a′′̸=a ena′′(st−W ′+1:t)

,"
RETURN M,0.5964912280701754,"where “≥” comes from the fact that the margin in e∆W ′
t
should use the top class after poisoning, and
computing with any other class provides a lower bound. Thus, from Equation (26) and the above
two relaxations, we get
Q(W ∗, a′)"
RETURN M,0.5975877192982456,"=⇒W ′ (ena′(st−W ∗+1:t) −ena#(st−W ∗+1:t)) ≥W ∗

ena(st−W ′+1:t) −max
a′′̸=a ena′′(st−W ′+1:t)

+ 1[a′ > a]"
RETURN M,0.5986842105263158,"=⇒∃a′′ ̸= a,"
RETURN M,0.5997807017543859,"W ′ (ena′(st−W ∗+1:t) −ena#(st−W ∗+1:t)) ≥W ∗(ena(st−W ′+1:t) −ena′′(st−W ′+1:t)) + 1[a′ > a].
For each a′′ ̸= a, now we use the last equation as the condition, and show that K > LW ∗,W ′"
RETURN M,0.6008771929824561,"a′,a′′
is a
necessary condition. This proposition is equivalent to
K ≤LW ∗,W ′"
RETURN M,0.6019736842105263,"a′,a′′"
RETURN M,0.6030701754385965,"=⇒W ′ (ena′(st−W ∗+1:t) −ena#(st−W ∗+1:t)) < W ∗(ena(st−W ′+1:t) −ena′′(st−W ′+1:t)) + 1[a′ > a].
(27)"
RETURN M,0.6041666666666666,"Suppose a poisoning attack within poisoning size K changes the subpolicies in set B ⊆[u]. Note
that |B| ≤K. We inspect the objective in Equation (27):
W ′ (ena′(st−W ∗+1:t) −ena#(st−W ∗+1:t)) −W ∗(ena(st−W ′+1:t) −ena′′(st−W ′+1:t)) −1[a′ > a]"
RETURN M,0.6052631578947368,"=W ′ (na′(st−W ∗+1:t) −na#(st−W ∗+1:t)) −W ∗(na(st−W ′+1:t) −na′′(st−W ′+1:t)) −1[a′ > a] +
X i∈B "
RETURN M,0.606359649122807,"W ′
W ∗−1
X"
RETURN M,0.6074561403508771,"w=0
1[˜πi(st−w+1) = a′] −W ′
W ∗−1
X"
RETURN M,0.6085526315789473,"w=0
1[˜πi(st−w+1) = a#]"
RETURN M,0.6096491228070176,"−W ∗
W ′−1
X"
RETURN M,0.6107456140350878,"w=0
1[˜πi(st−w+1) = a] + W ∗
W ′−1
X"
RETURN M,0.6118421052631579,"w=0
1[˜πi(st−w+1) = a′′]   −
X i∈B "
RETURN M,0.6129385964912281,"W ′
W ∗−1
X"
RETURN M,0.6140350877192983,"w=0
1[πi(st−w+1) = a′] −W ′
W ∗−1
X"
RETURN M,0.6151315789473685,"w=0
1[πi(st−w+1) = a#]"
RETURN M,0.6162280701754386,"−W ∗
W ′−1
X"
RETURN M,0.6173245614035088,"w=0
1[πi(st−w+1) = a] + W ∗
W ′−1
X"
RETURN M,0.618421052631579,"w=0
1[πi(st−w+1) = a′′]  "
RETURN M,0.6195175438596491,"=W ′ (na′(st−W ∗+1:t) −na#(st−W ∗+1:t)) −W ∗(na(st−W ′+1:t) −na′′(st−W ′+1:t)) −1[a′ > a] +
X i∈B"
RETURN M,0.6206140350877193,"max{W ∗,W ′}
X"
RETURN M,0.6217105263157895,"w=0
σw(˜πi(st−w)) −σw(πi(st−w))"
RETURN M,0.6228070175438597,"≤W ′ (na′(st−W ∗+1:t) −na#(st−W ∗+1:t)) −W ∗(na(st−W ′+1:t) −na′′(st−W ′+1:t)) −1[a′ > a] +
X i∈B"
RETURN M,0.6239035087719298,"max{W ∗,W ′}
X"
RETURN M,0.625,"w=0
max
a0∈A σw(a0) −σw(πi(st−w))"
RETURN M,0.6260964912280702,"|
{z
}
gi"
RETURN M,0.6271929824561403,"(a)
≤W ′ (na′(st−W ∗+1:t) −na#(st−W ∗+1:t)) −W ∗(na(st−W ′+1:t) −na′′(st−W ′+1:t)) −1[a′ > a] + K
X"
RETURN M,0.6282894736842105,"i=1
g(i)"
RETURN M,0.6293859649122807,Published as a conference paper at ICLR 2022
RETURN M,0.6304824561403509,"(b)
≤W ′ (na′(st−W ∗+1:t) −na#(st−W ∗+1:t)) −W ∗(na(st−W ′+1:t) −na′′(st−W ′+1:t)) −1[a′ > a] +"
RETURN M,0.631578947368421,"LW ∗,W ′"
RETURN M,0.6326754385964912,"a′,a′′
X"
RETURN M,0.6337719298245614,"i=1
g(i)"
RETURN M,0.6348684210526315,"(c)
<0."
RETURN M,0.6359649122807017,"Thus, Equation (27) is proved. Therefore, K > LW ∗,W ′"
RETURN M,0.6370614035087719,"a′,a′′
is a necessary condition for Q(W ∗, a′),
i.e., Equation (25)."
RETURN M,0.6381578947368421,"In the above deriviation, the deﬁnitions of gi, gi, and σw are from Equation (7). (a) comes from
the facts that {g(i)}u
i=1 is a nondecreasing permutation of {gi}u−1
i=0 , gi ≥0, and |B| ≤K. (b)
comes from the assumption K ≤LW ∗,W ′"
RETURN M,0.6392543859649122,"a,a′′
and also g(i) ≥0. (c) comes from the deﬁnition in
Equation (6)."
RETURN M,0.6403508771929824,"F.6
POSSIBLE ACTION SET IN PARL AND COMPARISON"
RETURN M,0.6414473684210527,"F.6.1
CERTIFICATION"
RETURN M,0.6425438596491229,"Proof of Theorem 4. According to the deﬁnition of possible action set, we only need to prove the
contrary: for any a ∈A \ AT (K), within poisoning size K, the poisoned policy cannot choose a:
f
πP(st) ̸= a."
RETURN M,0.643640350877193,"According to Equation (8), any a ∈A \ AT (K) satisﬁes
X"
RETURN M,0.6447368421052632,"a′∈A
max{na′(st) −na(st) −K + 1[a′ < a], 0} > K.
(28)"
RETURN M,0.6458333333333334,"Given poisoning size K, since each poisoning size can affect only one subpolicy, we know
ena(st) ≤na(st) + K
where ena denotes to the poisoned aggregated action count. We suppose the attack could be success-
ful, then for a′ < a, ena′(st) ≤ena(st) −1, and thus na′(st) −ena′(st) ≥na′(st) −na(st) −K + 1.
Similarly, for a′ > a, ea′(st) ≤ena(st), and thus na′(st)−ena′(st) ≥na′(st)−na(st)−K. Also, for
any a′ ̸= a after poisoning na′(st) −ena′(st) ≥0; otherwise deviating the difference subpolicies’
decisions’ from a′ to a is strictly no-worse. Given these facts, the amount of votes that need to be
reduced is the LHS of Equation (28) which is larger than K. However, we only have K poisoning
size, i.e., K votes that can be reduced. As a result, our assumption that the attack could be successful
is falsiﬁed and the poisoned policy cannot choose a."
RETURN M,0.6469298245614035,"Corollary 12 (Loose PARL Action Set). Under the condition of Deﬁnition 5, suppose the aggrega-
tion protocol is PARL as deﬁned in Deﬁnition 7, then the possible action set at step t"
RETURN M,0.6480263157894737,"AL(K) =

a ∈A
 max
a′∈A na′(st) −na(st) ≤2K −1[a > arg max
a′∈A
na′(st)]

.
(29)"
RETURN M,0.6491228070175439,"Proof of Corollary 12. Again, we prove the contrary, for any a ∈A\AL(K), within poisoning size
K, the poisoned policy cannot choose a: f
πP(st) ̸= a."
RETURN M,0.6502192982456141,"According to Equation (29), let am = arg maxa′∈A na′(st), then any a ∈A \ AL(K) satisﬁes
nam(st) −na(st) > 2K −1[a > am].
After poisoning, we thus have
nam(st) −na(st) > −1[a > am] =⇒nam(st) −na(st) ≥1 −1[a > am]
From the deﬁnition, a /∈AL(K) so a ̸= am. If am < a, nam(st) ≥na(st); if am > a, nam(st) >
na(st). In both cases, am has higher priority to be chosen than a, and thus the poisoned policy
cannot choose a."
RETURN M,0.6513157894736842,"F.6.2
COMPARISON"
RETURN M,0.6524122807017544,"Theorem 13. Under the condition of Deﬁnition 5, suppose the aggregation protocol is PARL as de-
ﬁned in Deﬁnition 7, AT (K), AL(K) are deﬁned according to Equations (8) and (29) accordingly,
then"
RETURN M,0.6535087719298246,"1. AT (K) ⊆AL(K); and there are subpolicies {πi}u−1
i=0 and state st such that AT (K) ⊊AL(K)."
RETURN M,0.6546052631578947,Published as a conference paper at ICLR 2022
RETURN M,0.6557017543859649,"2. Given subpolicies {πi}u−1
i=0 and state st, for any a ∈AT (K), there exists a poisoned training set
eD whose poisoning size |D ⊖eD| ≤K and some RL training mechanism, such that f
πP(st) = a
where f
πP is the poisoned PARL policy trained on eD."
RETURN M,0.6567982456140351,Proof of Theorem 13. We prove the two arguments separately.
RETURN M,0.6578947368421053,"1. We ﬁrst prove AT (K) ⊆AL(K). For any a ∈AT (K), let am = arg maxa′∈A na′(st), then
X"
RETURN M,0.6589912280701754,"a′∈A
max{na′(st) −na(st) −K + 1[a′ < a], 0} ≤K"
RETURN M,0.6600877192982456,"=⇒nam(st) −na(st) −K + 1[am < a] ≤K
=⇒nam(st) −na(st) ≤2K −1[a > am]
where the last proposition is exactly the set selector of AL(K) so a ∈AL(K)."
RETURN M,0.6611842105263158,"We then prove that AT (K) ⊊AL(K) can happen by construction. Suppose that there are three
actions in the action space: A = {a1, a2, a3}. We construct subpolicies for current state st such
that the aggregated action counts are
na1(st) = 10, na2(st) = 9, na3(st) = 1.
Given poisoning size K = 5, we ﬁnd that
na1(st) −na3(st) = 9 ≤10 −1[a3 ≥a1] = 9
=⇒a3 ∈aL(K),
(na1(st) −na3(st) −K + 1[a1 < a3]) +"
RETURN M,0.6622807017543859,"(na2(st) −na3(st) −K + 1[a2 < a3]) = 9 > K = 5
=⇒a3 /∈aT (K).
Therefore, AT (K) ⊊AL(K) for these subpolicies and state st."
RETURN M,0.6633771929824561,"2. We prove by construction. For any a ∈AT (K), we construct the set of subpolicies to poison
Ba ⊆[u] such that |Ba| ≤K, then describe the corresponding poisoned dataset eDa, and ﬁnally
prove that the poisoned policy f
πP
a(st) = a."
RETURN M,0.6644736842105263,"For a ∈AT (K), by deﬁnition (Equation (8)), we know
X"
RETURN M,0.6655701754385965,"a′∈A
max{na′(st) −na(st) −K + 1[a′ < a]
|
{z
}
:=ta,a′"
RETURN M,0.6666666666666666,", 0} ≤K.
(30)"
RETURN M,0.6677631578947368,"We now deﬁne a set of actions Ca ⊆A such that
Ca = {a′ ∈A | ta,a′ > 0}.
According to this deﬁnition, a /∈Ca since ta,a ≤0."
RETURN M,0.668859649122807,"Fact F.1. For a ∈AT (K) and any a′ ∈A, na(st) + K −1[a′ < a] ≥0."
RETURN M,0.6699561403508771,"Proof of Fact F.1. Suppose na(st) + K −1[a′ < a] ≤0, then na(st) = K = 0 and a′ < a.
Then
X"
RETURN M,0.6710526315789473,"a′∈A
max{na′(st)−na(st)−K+1[a′ < a], 0} ≥
X"
RETURN M,0.6721491228070176,"a′∈A
na′(st)−na(st)−K =
X"
RETURN M,0.6732456140350878,"a′∈A
na′(st) = u > 0"
RETURN M,0.6743421052631579,which contradicts the requirement that the LHS of the above inequality should be ≤K = 0.
RETURN M,0.6754385964912281,"Give Fact F.1, for any a′ ∈Ca, na′(st) ≥ta,a′. Notice that na′(st) is the number of subpolicies
that vote for action a′ at state st. Therefore, we can pick an arbitrary subset of those subpolicies
whose cardinality is ta,a′. We denote Ba
a′ to such subset:
Ba
a′ ⊆{i ∈[u] | πi(st) = a′}, |Ba
a′| = ta,a′.
Now deﬁne Ba
α: Ba
α = S"
RETURN M,0.6765350877192983,"a′∈Ca Ba
a′. We construct Ba
β to be an arbitrary subset of those subpoli-
cies whose prediction is not a and who are not in Ba
α, and limit the Ba
β’s cardinality:
Ba
β ⊆{i ∈[u] | πi(st) ̸= a} \ Ba
α, |Ba
β| = min{K −|Ba
α|, u −na(st) −|Ba
α|}.
Such Ba
β can be selected, because:"
RETURN M,0.6776315789473685,"• From deﬁnition, Ba
α ⊆{i ∈[u] | πi(st) ̸= a}, where the cardinality of {i ∈[u] | πi(st) ̸=
a} is u −na(st). So 0 ≤u −na(st) −|Ba
α|.
• Since"
RETURN M,0.6787280701754386,"|Ba
α| ≤
X"
RETURN M,0.6798245614035088,"a′∈Ca
|Ba
a′| =
X"
RETURN M,0.680921052631579,"a′∈Ca
ta,a′ =
X"
RETURN M,0.6820175438596491,"a′∈A,ta,a′>0
ta,a′
(∗)
≤K,"
RETURN M,0.6831140350877193,Published as a conference paper at ICLR 2022
RETURN M,0.6842105263157895,"K −|Ba
α| ≥0, and thus |Ba
β| ≥0. Here (∗) is due to Equation (30)."
RETURN M,0.6853070175438597,"• The superset {i ∈[u] | πi(st) ̸= a} \ Ba
α has cardinality u −na(st) −|Ba
α| and |Ba
β| ≤
u −na(st) −|Ba
α|."
RETURN M,0.6864035087719298,"To this point, we can deﬁne the set of subpolicies to poison:
Ba := Ba
α ∪Ba
β,
and |Ba| = |Ba
α| + |Ba
β| ≤K."
RETURN M,0.6875,"In a similar fashion as the attack construction in proof of Proposition 9, for each i ∈Ba, we
locate its corresponding partitioned dataset Di for training subpolicy πa
i . We inset one trajectory
pa
i to Di such that our chosen learning algorithm M0 can train a subpolicy ˜πa
i = M0(Di∪{pa
i })
such that ˜πa
i (st) = a. For example, the trajectory could be pa
i = {(st, a, s′, ∞)} where s′ is an
arbitrary state that guarantees pa
i is hashed to partition i; and M0 learns the action with maximum
reward for the memorized nearest state. Then, the poisoned dataset eDa = ["
RETURN M,0.6885964912280702,"i∈Ba
Di ∪{pa
i } ! ∪  
["
RETURN M,0.6896929824561403,"i∈[u]\B′
Di  ."
RETURN M,0.6907894736842105,"Thus, eDa is |D ⊖eDa| = |Ba| ≤K, i.e., the constructed attack’s poisoning size is within K."
RETURN M,0.6918859649122807,"We now analyze the action prediction of the poisoned policy f
πP. For action a, after poisoning,
ena(st) = na(st)+|Ba| = na(st)+min{K, u−na(st)} = min{na(st)+K, u}. If ena(st) = u,
then all subpolicies vote for a, apparently f
πP
a(st) = a; otherwise, ena(st) = na(st) + K. In
this case, for any action a′ ∈Ca, since we at least choose subpolicies in Ba
a′ and change their
action prediction to a, the aggregated action count after poisoning is ena′(st) ≤na′(st)−|Ba
a′| =
na′(st)−ta,a′ = na′(st)−na′(st)+na(st)+K−1[a′ < a] = na(st)+K−1[a′ < a] = ˜na(st)−
1[a′ < a]. Thus, a′ has lower priority to be chosen than a. For any action a′ /∈Ca and a′ ̸= a,
by the deﬁnition of Ca, ta,a′ = na′(st)−na(st)−K +1[a′ < a] ≤0. After poisoning, the vote
of a′ does not increase, i.e., ena′(st) ≤na′(st) ≤na(st) + K −1[a′ < a] = ena(st) −1[a′ < a].
Thus, a′ also has lower priority to be chosen than a. In conclusion, we have f
πP
a(st) = a."
RETURN M,0.6929824561403509,"To this point, for any a ∈AT (K), we successfully construct the corresponding poisoned dataset
eDa within poisoning size K such that f
πP
a(st) = a, thus concludes the proof."
RETURN M,0.694078947368421,"F.7
POSSIBLE ACTION SET IN TPARL"
RETURN M,0.6951754385964912,"Proof of Theorem 5. For ease of notation, we let w = min{W, t + 1}, so w is the actual window
size used at step t. We let t0 = t −w + 1, i.e., t0 is the actual start time step for TPARL aggregation
at our current step t. Now we can write chosen action at step t without poisoning as πT(st0:t)."
RETURN M,0.6962719298245614,"We only need to prove the contrary: for any a ∈A \ A(K), within poisoning size K, the poisoned
policy cannot choose a: f
πT(st0:t) ̸= a."
RETURN M,0.6973684210526315,"According to Equation (9), for such a, there exists a′ ̸= a such that
K
X"
RETURN M,0.6984649122807017,"i=1
h(i)
a′,a ≤δa′,a = na′(st0:t) −na(st0:t) −1[a < a′].
(31)"
RETURN M,0.6995614035087719,"Suppose there exists such poisoning attack that lets f
πT(st0:t) = a. This implies that for a′, after
poisoning, we have
ena′(st0:t) −ena′(st0:t) −1[a′ < a] < 0,
(32)
where ena is the aggregated action count after poisoning. Since the poisoning size is within K, it
can affect at most K subpolicies. We let B ⊆[u], |B| ≤K to represent the affected subpolicy set.
Therefore,
ena′(st0:t) −ena(st0:t) −1[a′ < a]"
RETURN M,0.7006578947368421,"= na′(st0:t) −na(st0:t) −1[a′ < a]
|
{z
}
δa′,a"
RETURN M,0.7017543859649122,+(ena′(st0:t) −na′(st0:t)) −(ena(st0:t) −na(st0:t))
RETURN M,0.7028508771929824,Published as a conference paper at ICLR 2022
RETURN M,0.7039473684210527,"=δa′,a +
X i∈B "
RETURN M,0.7050438596491229,"
w−1
X"
RETURN M,0.706140350877193,"j=0
1[˜πi(st−j) = a′, πi(st−j) ̸= a′] − w−1
X"
RETURN M,0.7072368421052632,"j=0
1[˜πi(st−j) ̸= a′, πi(st−j) = a′]   −
X i∈B "
RETURN M,0.7083333333333334,"
w−1
X"
RETURN M,0.7094298245614035,"j=0
1[˜πi(st−j) = a, πi(st−j) ̸= a] − w−1
X"
RETURN M,0.7105263157894737,"j=0
1[˜πi(st−j) ̸= a, πi(st−j) = a]  "
RETURN M,0.7116228070175439,"≥δa′,a −
X i∈B "
RETURN M,0.7127192982456141,"
w−1
X"
RETURN M,0.7138157894736842,"j=0
1[˜πi(st−j) ̸= a′, πi(st−j) = a′] + w−1
X"
RETURN M,0.7149122807017544,"j=0
1[˜πi(st−j) = a, πi(st−j) ̸= a]  "
RETURN M,0.7160087719298246,"≥δa′,a −
X i∈B "
RETURN M,0.7171052631578947,"
w−1
X"
RETURN M,0.7182017543859649,"j=0
1[πi(st−j) = a′] + w−1
X"
RETURN M,0.7192982456140351,"j=0
1[πi(st−j) ̸= a]  "
RETURN M,0.7203947368421053,"=δa′,a −
X i∈B "
RETURN M,0.7214912280701754,"
w−1
X"
RETURN M,0.7225877192982456,"j=0
1i,a′(st−j) + w − w−1
X"
RETURN M,0.7236842105263158,"j=0
1i,a(st−j)  "
RETURN M,0.7247807017543859,"=δa′,a −
X"
RETURN M,0.7258771929824561,"i∈B
hi,a′,a"
RETURN M,0.7269736842105263,"(a)
≥δa′,a − |B|
X"
RETURN M,0.7280701754385965,"i=1
h(i)
a′,a"
RETURN M,0.7291666666666666,"(b)
≥δa′,a − K
X"
RETURN M,0.7302631578947368,"i=1
h(i)
a′,a"
RETURN M,0.731359649122807,"(c)
≥0."
RETURN M,0.7324561403508771,"This contradicts with Equation (32), and thus the assumption is falsiﬁed, i.e., there is no such poi-
soning attack that let f
πT(st0:t) = a. In the above equations, (a) is due to the fact that h(i)
a′,a is a"
RETURN M,0.7335526315789473,"nonincreasing permutation of {hi,a′,a}u−1
i=0 . (b) is due to the facts that |B| ≤K and h(i)
a′,a ≥0. (c)
comes from Equation (31)."
RETURN M,0.7346491228070176,"F.8
HARDNESS FOR COMPUTING TIGHT POSSIBLE ACTION SET IN TPARL"
RETURN M,0.7357456140350878,"Proof of Theorem 6. By Deﬁnition 5, the possible action set with minimum cardinality (called min-
imal possible action set hereinafter) is unique. Otherwise, suppose A and B are both minimal
possible action set, but A ̸= B, then A ∩B is a smaller and valid possible action set. Therefore,
the oracle that returns the minimal possible action set, denoted by MINSET, can tell whether any
action a ∈MINSET and thus whether any action a can be chosen by some poisoned policy eπ whose
poisoning size is within K. In other words, the problem of determining whether an action a can
be chosen by some poisoned policy ˜π whose poisoning size is within K, denoted by ATTKACT,
is polynomially equivalent to MINSET: MINSET ≡P ATTKACT. Now, we show a polynomial
reduction from the set cover problem to ATTKACT, which implies that our MINSET problem is an
NP-complete problem."
RETURN M,0.7368421052631579,"The decision version of the set cover problem (Karp, 1972), denoted by SETCOVER, is a well-
known NP-complete problem and is deﬁned as follows. The inputs are"
RETURN M,0.7379385964912281,"1. a universal set of n elements: U = {u1, u2, · · · , un};"
RETURN M,0.7390350877192983,"2. a set of subsets of U: V = {V1, · · · , Vm}, Vi ⊆U, 1 ≤i ≤m, Sm
i=1 Vi = U;"
RETURN M,0.7401315789473685,3. a positive number K ∈R+.
RETURN M,0.7412280701754386,"The output is a boolean variable b, indicating that whether there exists a subset W ⊆V, |W| ≤K,
such that ∀ui ∈U, ∃V ∈W, ui ∈V . Given an oracle to ATTKACT, we need to show SETCOVER
can be solved in polynomial time, i.e., SETCOVER ≤P ATTKACT."
RETURN M,0.7423245614035088,"• If K ≥n:
We scan all sets Vi ∈V. To being with, we have a record set S ←∅, and an answer set W ←∅.
Whenever we encounter a set that contains a new element ui /∈S, we put W ←W ∪{Vj},
and record this element S ←S ∪{ui}. After one scan pass, W covers all elements of U (since
Sm
j=1 Vj = U), and |W| ≤n ≤K. Therefore, W is a valid set cover. Since we can always ﬁnd
such W, we can directly answer true."
RETURN M,0.743421052631579,Published as a conference paper at ICLR 2022
RETURN M,0.7445175438596491,"• If K ≥m:
We can directly return V as a valid set cover, since Sm
j=1 Vj = U and |V| ≤m ≤K. Thus, we
can answer true."
RETURN M,0.7456140350877193,"• If K < min{n, m}:
This is the general case which we need to handle. Now we construct the ATTKACT(K) problem
so that we can trigger its oracle to solve SETCOVER."
RETURN M,0.7467105263157895,"1. The poisoning size is K.
2. The action space A = U ∪{b} ∪Γ where Γ := {#1, . . . , #m2n}. (|A| = n + 1 + m2n.)
The sorting of actions is u1 < u2 < · · · < un < b < #1 < · · · < #m2n.
3. The subpolicies are {πa
j }m
j=1 ∪{πb
i , πc
i,j
| 1 ≤i ≤n, 1 ≤j ≤K −1}, where πa
j
corresponds to Vj ∈V, and πb
i , πc
i,j correspond to ui ∈U. (Number of subpolicies u =
m + Kn ≤m + n2.)
4. The current time step is t = nm, and the window size W = nm.
5. The input action is b, i.e., asking whether b can be chosen by some poisoned TPARL policy,
i.e., f
πT(s1:t) = b, if the poisoning size is within K."
RETURN M,0.7478070175438597,"Now, we construct the states at each step t (1 ≤t ≤nm) so that the subpolicies’ action predictions
at these steps are as follows."
RETURN M,0.7489035087719298,"1. Count the appearing time of each ui in V, and denote it by ci: ci = Pm
j=1 1[ui ∈Vj]. For
each ui, select a Vj0 that contains ui, and in the corresponding πa
j0, assign m −ci + 1 steps
to predict ui; for all other Vj that contains ui, in the corresponding πa
j , assign one step to
predict ui.
2. After this process, each πa
j at least has one time step whose action prediction is ui for each
ui ∈Vj. Among all {πa
j }m
j=1 and all time steps 1 ≤t ≤nm, mn step-action cells are ﬁlled,
and the remaining (m2n −mn) cells are ﬁlled by #l ∈Γ sequentially.
3. For each πb
i , arbitrarily select W −m time steps to assign action prediction as ui; and ﬁll in
other m time steps by remaining #l ∈Γ sequentially.
4. For each πc
i,j, for all time steps, let the action prediction be ui."
RETURN M,0.75,"As we can observe, the number of actions, the number of subpolicies, and the window size are all
bounded by a polynomial of n and m. Therefore, such construction can be done in polynomial
time."
RETURN M,0.7510964912280702,"We then show SETCOVER = true ⇐⇒∃K′, b ∈ATTKACT(K′), 1 ≤K′ ≤K."
RETURN M,0.7521929824561403,"– =⇒:
Suppose the covering set is W ⊆V, we denote K′ to |W|, and construct the ATTKACT
problem with poisoning size K′ as described above.
We can construct a poisoning strategy to let f
πT(s1:nm) = b, The poisoning strategy is to
ﬁnd out πa
j for each Vj ∈W, and to let them predict action b throughout all time steps:
eπa
j (s′
t) = b, 1 ≤t′ ≤nm.
After poisoning, the aggregated action count enb(s1:nm) = |W| × nm = K′nm. Since W
covers every ui ∈U, for each ui ∈U there exists a set Vj ∋ui, whose corresponding eπa
j is
poisoned to predict b. Thus, enui(s1:nm) < nui(s1:nm) = m + (W −m) + W × (K′ −1) =
WK′ = K′nm. For any #l ∈Γ, en#l(s1:nm) ≤n#l(s1:nm) = 1. In summary,
enui(s1:nm) < K′nm, enb(s1:nm) = K′nm, en#l(s1:nm) = 1.
Thus, after TPARL aggregation, the poisoned policy f
πT(s1:nm) = b, and therefore b ∈
ATTKACT(K′).
– ⇐=:
Suppose it is K′ that let b ∈ATTKACT(K′), which implies that there exists such a poisoning
attack within size K′ that misleads the poisoned policy to b: f
πT(s1:nm) = b. Since the
poisoning size is K′, after poisoning the aggregated action count
enb(s1:nm) ≤K′W = K′nm.
For each ui ∈U, since nui(s1:nm) = m + (W −m) + W × (K′ −1) = K′nm, we always
have"
RETURN M,0.7532894736842105,"enui(s1:nm)
(∗)
< enb(s1:nm) ≤nui(s1:nm),
(33)"
RETURN M,0.7543859649122807,Published as a conference paper at ICLR 2022
RETURN M,0.7554824561403509,"where (∗) is due to the condition of successful attack. We denote the set of poisoned subpoli-
cies by Π (|Π| ≤K′). Therefore, Equation (33) implies that for each ui ∈U, there exists at
least one subpolicy
π′
ui ∈Π, π′
ui ∈{πa
j | ui ∈Vj} ∪{πb
i } ∪{πc
i,j | 1 ≤j ≤K −1}
(34)
that is poisoned by the attack, otherwise the aggregated vote enui cannot change.
We partition Γ by Γa and Γbc, where
Γa = Γ ∩{πa
j }m
j=1, Γb = Γ ∩{πb
i , πc
i,j | 1 ≤i ≤n, 1 ≤j ≤K −1}.
We construct additional poisoning set Γa
+ following this process: In the beginning, Γa
+ ←∅.
For each ui ∈U, if Γa ∩{πa
j
| ui ∈Vj} is not empty, skip. Otherwise, according to
Equation (34), Γb ∩{πb
i , πc
i,j | 1 ≤j ≤K −1} is not empty. In this case, we ﬁnd
an arbitrary covering set of ui, namely Vj0 ∋ui, and put πa
j0 into Γa
+. When the process
terminates, we ﬁnd that for each ui ∈U,
(Γa ∪Γa
+) ∩{πa
j | ui ∈Vj} ̸= ∅.
(35)
Following the mapping {πa
j }m
j=1 ←→{Vj | 1 ≤j ≤m} = V, the subset (Γa ∪Γa
+) ⊆
{πa
j }m
j=1 can be mapped to (Wa ∪Wa
+) ⊆V. From Equation (35), (Wa ∪Wa
+) is a valid set
cover for U. We now study the cardinality of this set cover. From the process, we know that
every Vj0 ∈Wa
+ corresponds to a different set in Γb. Thus, |Wa
+| ≤|Γb|, which implies that
|Wa ∪Wa
+| ≤|Wa| + |Wa
+| ≤|Γa| + |Γb| = |Γ| ≤K′.
To this point, we successfully construct a set cover within cardinally K′ ≤K that covers U,
so SETCOVER = true."
RETURN M,0.756578947368421,"Therefore, we can check whether SETCOVER = true by iterating K′ from 1 to K (<
min{n, m} iterations), constructing the ATTKACT(K′) problem, and querying the oracle.
The whole process can be done in polynomial time assumping O(1) computation time of the
ATTKACT(K′) oracle."
RETURN M,0.7576754385964912,"To this point, we have shown SETCOVER ≤P ATTKACT. On the other hand, an undeterminisitic
Turing machine can try different poisoning strategies by branching on whether to poison current
subpolicy and what actions to be assigned to each poisoned subpolicy. The decision of whether the
poisoning is successful can be done in polynomial time and b ∈ATTKACT(K) corresponds to the
existence of successfully attacked branches. Thus, ATTKACT ∈NP. Given that SETCOVER is
an NP-complete problem, so does ATTKACT and MINSET (since MINSET ≡P ATTKACT)."
RETURN M,0.7587719298245614,"F.9
POSSIBLE ACTION SET IN DPARL"
RETURN M,0.7598684210526315,"Proof of Theorem 7. For ease of notation, we assume that Wmax ≤t + 1, and otherwise we let
Wmax ←t + 1. We let t0 = max{t −Wmax + 1, 0} be the start time step of the maximum possible
window."
RETURN M,0.7609649122807017,"We only need to prove the contrary: for any a ∈A \ A(K), within poisoning size K, the poisoned
policy cannot choose a: f
πD(st0:t) ̸= a. We prove by contradiction: we assume that there exists
such a poisoning attack within poisoning size K that lets f
πD(st0:t) = a. From the expression of
A(K) (Equation (10)), a ̸= at = πD(st0:t). Suppose the selected time window before the attack is
W ′ (selected according to Equation (1) based on ∆W
t
and na), and the selected time window after
the attack is f
W ′ (selected according to Equation (1) based on e∆W
t
and ena)."
RETURN M,0.7620614035087719,"• If f
W ′ = W ′:
Suppose we use the TPARL aggregation policy with window size W = W ′ instead of current
DPARL aggregation policy, then we will have πT(st−W ′+1:t) = at and f
πT(st−W ′+1:t) = a.
Thus, according to the deﬁnition of possible action set (Deﬁnition 5), a ∈A(K) where A(K) is
deﬁned by Equation (9) in Theorem 5. This implies that a ∈A(K) where A(K) is deﬁned by
Equation (10), which contradicts the assumption that a ∈A \ A(K)."
RETURN M,0.7631578947368421,"• If f
W ′ ̸= W ′:
According to the deﬁnition in Equation (10),
min
1≤W ∗≤Wmax,W ∗̸=W ′,a′′̸=at LW ∗,W ′"
RETURN M,0.7642543859649122,"a,a′′
> K."
RETURN M,0.7653508771929824,Published as a conference paper at ICLR 2022
RETURN M,0.7664473684210527,"We deﬁne a#
t = arg maxa0̸=at ena0(st−W ′+1:t). Then, the above equation implies that"
RETURN M,0.7675438596491229,"L
f
W ′,W ′"
RETURN M,0.768640350877193,"a,a#
t
> K
and thus
L
f
W ′,W ′"
RETURN M,0.7697368421052632,"a,a#
t
X"
RETURN M,0.7708333333333334,"i=1
g(i) + W ′(n
f
W ′
a
−n
f
W ′
a# ) −f
W ′(nW ′
at −nW ′"
RETURN M,0.7719298245614035,"a#
t ) −1[a > at] < 0."
RETURN M,0.7730263157894737,"Since g(i) ≥0 by deﬁnition (Equation (7)),
K
X"
RETURN M,0.7741228070175439,"i=1
g(i) + W ′(n
f
W ′
a
−n
f
W ′
a# ) −f
W ′(nW ′
at −nW ′"
RETURN M,0.7752192982456141,"a#
t ) −1[a > at] < 0,
(36)"
RETURN M,0.7763157894736842,"where a# = arg maxa0̸=a′,a0∈A na0(st−f
W ′+1:t) and nw
a is a shorthand of na(st−w+1:t)."
RETURN M,0.7774122807017544,"Following the derivation from Equation (27) to (a), we have
K
X"
RETURN M,0.7785087719298246,"i=1
g(i)+W ′(n
f
W ′
a −n
f
W ′
a# )−f
W ′(nW ′
at −nW ′"
RETURN M,0.7796052631578947,"a#
t )−1[a > at] ≥W ′(en
f
W ′
a −en
f
W ′
a# )−f
W ′(enW ′
at −enW ′"
RETURN M,0.7807017543859649,"a#
t )−1[a > at]."
RETURN M,0.7817982456140351,"Combined with Equation (36),
W ′(en
f
W ′
a
−en
f
W ′
a# ) −f
W ′(enW ′
at −enW ′"
RETURN M,0.7828947368421053,"a#
t ) −1[a > at] < 0.
(37)"
RETURN M,0.7839912280701754,"On the other hand, the successful attack assumption, i.e., f
πD(st0:t) = a and πD(st0:t) = at,
implies that"
RETURN M,0.7850877192982456,"e∆
f
W ′
t
= enf
W ′
a
−enf
W ′
a(2)
f
W ′
>
enW ′
aW ′ −enW ′"
RETURN M,0.7861842105263158,"a(2)
W ′
W ′
= e∆W ′
t
,
(38)"
RETURN M,0.7872807017543859,"where “>” is “≥” if a < at. In the above equation,
a(2) = arg max
a0̸=a,a0∈A
en
f
W ′
a0 , aW ′ = arg max
a0∈A
enW ′
a0 , a(2)
W ′ =
arg max
a0̸=aW ′,a0∈A
enW ′
a0 ."
RETURN M,0.7883771929824561,"Intuitively, after poisoning, a(2) is the runner-up action at window f
W ′, aW ′ is the action at window
W ′, and a(2)
W ′ is the runner-up action at window W ′. We rewrite Equation (38) to
W ′(en
f
W ′
a
−en
f
W ′
a(2)) ≥f
W ′(enW ′
aW ′ −enW ′"
RETURN M,0.7894736842105263,"a(2)
W ′) + 1[a > at].
(39)"
RETURN M,0.7905701754385965,We have the following two observations:
ENF,0.7916666666666666,"1. enf
W ′
a
−enf
W ′
a# ≥enf
W ′
a
−enf
W ′
a(2), since a is the top action and a(2) is the runner-up action and their
margin should be the smallest."
ENF,0.7927631578947368,"2. enW ′
aW ′ −enW ′"
ENF,0.793859649122807,"a(2)
W ′ ≥enW ′
at −enW ′"
ENF,0.7949561403508771,"a#
t , because 1) if at = aW ′, LHS equals to RHS; 2) if at ̸= aW ′,"
ENF,0.7960526315789473,LHS ≥0 and RHS ≤0.
ENF,0.7971491228070176,"Plugging these two observations to two sides of Equation (39), we get
W ′(en
f
W ′
a
−en
f
W ′
a# ) ≥f
W ′(enW ′
at −enW ′"
ENF,0.7982456140350878,"a#
t ) + 1[a > at].
(40)
This contradicts with Equation (37)."
ENF,0.7993421052631579,"Since in both cases, we ﬁnd contradictions. Now we can conclude that for any a ∈A\A(K), within
poisoning size K, the poisoned policy cannot choose a: f
πD(st0:t) ̸= a."
ENF,0.8004385964912281,"G
ADDITIONAL EXPERIMENTAL DETAILS"
ENF,0.8015350877192983,"G.1
DETAILS OF THE OFFLINE RL ALGORITHMS AND IMPLEMENTATIONS"
ENF,0.8026315789473685,"We experimented with three ofﬂine RL algorithms: DQN (Mnih et al., 2013), QR-DQN (Dabney
et al., 2018), and C51 (Bellemare et al., 2017). The ﬁrst one is the standard baseline, while the latter
two are distributional RL algorithms which show SOTA results in ofﬂine RL tasks. We ﬁrst brieﬂy
introduce the algorithm ideas, followed by the implementation details."
ENF,0.8037280701754386,Published as a conference paper at ICLR 2022
ENF,0.8048245614035088,"Algorithm Ideas.
The core of Q-learning (Watkins & Dayan, 1992) is the Bellman optimality
equation (Bellman, 1966)
Q⋆(s, a) = ER(s, a) + γEs′∼P max
a′∈A Q⋆(s′, a′) ,
(41)"
ENF,0.805921052631579,"where a parameterized Qθ is adopted to approximate the optimal Q⋆and iteratively improved. In
DQN (Mnih et al., 2013) speciﬁcally, the parameterization is achieved by using a convolutional
neural network (LeCun et al., 1998). In contrast to estimating the mean action value Qπ(s, a) in
DQN, distributional RL algorithms estimate a density over the values of the state-action pairs. The
distributional Bellman optimality can be expressed as follows:
Z⋆(s, a)
D= r + γZ⋆(s′, argmaxa′∈A Q⋆(s′, a′)) where r ∼R(s, a), s′ ∼P(· | s, a).
(42)
Concretely, QR-DQN (Dabney et al., 2018) approximates the density D⋆with a uniform mixture
of K Dirac delta functions, while C51 (Bellemare et al., 2017) approximates the density using a
categorical distribution over a set of anchor points."
ENF,0.8070175438596491,"Implementation Details.
For training the subpolicies using ofﬂine RL training algorithms, we use
the code base of Agarwal et al. (2020). The conﬁguration ﬁles containing the detailed hyperparam-
eters can be found at their public repository https://github.com/google-research/
batch_rl/tree/master/batch_rl/fixed_replay/configs. For the three methods
DQN, QR-DQN, and C51, the names of the conﬁguration ﬁles are dqn.gin, quantile.gin,
and c51.gin, respectively. On each partition, we train the subpolicy for 50 epochs."
ENF,0.8081140350877193,"G.2
CONCRETE EXPERIMENTAL PROCEDURES"
ENF,0.8092105263157895,"We conduct experiments on two Atari 2600 games, Freeway and Breakout from OpenAI
Gym (Brockman et al., 2016), and one autonomous driving environment Highway (Leurent, 2018),
following Section 4."
ENF,0.8103070175438597,"Concretely, in the training stage, we ﬁrst partition the training dataset into u partitions (u = 30, 50,
or 100) by using the hash function h(τ) pre-deﬁned in each environment. Let si be the i-th value in
the state representation and d be the dimensionality of the state. In Atari games where the state is
the game frame, h(τ) is deﬁned as the sum of all pixel values of all frames in the trajectory τ, i.e.,
h(τ) = P s∈τ
P"
ENF,0.8114035087719298,"i∈[d] si. In Highway where the state is a ﬂoating point number scalar containing
the positions and velocities of all vehicles, we deﬁne h(τ) = P s∈τ
P"
ENF,0.8125,"i∈[d] f(si) where f : R →Z
is a deterministic function that maps the given ﬂoat value to integer space. Concretely, we take f(x)
as the the sum of the higher 16 bits and the lower 16 bits of its 32-bit representation under IEEE
754 standard (group of the Microprocessor Standards Subcommittee & Institute, 1985). We then
train the subpolicies on the partitions with ofﬂine RL training algorithm M0 ∈{DQN (Mnih et al.,
2013), QR-DQN (Dabney et al., 2018), C51 (Bellemare et al., 2017)}. The detailed description of
the algorithms can be found in Appendix G.1."
ENF,0.8135964912280702,"In the aggregation stage, we apply the three proposed aggregation protocols (PARL (Theorem 8),
TPARL (Theorem 1), and DPARL (Theorem 3)) on the trained u subpolicies and derive the aggre-
gated policies πP, πT, and πD accordingly."
ENF,0.8146929824561403,"Finally, in the certiﬁcation stage, for each aggregated policy, we provide the per-state action and
cumulative reward certiﬁcation following our theorems. When certifying the per-state action, we
constrain the maximum trajectory length H = 1000 for Atari games and H = 30 for Highway
(which is the full length in Highway) and report results averaged over 20 runs; for the cumulative
reward certiﬁcation, we adopt the trajectory length H = 400 for evaluating Freeway, H = 75 for
Breakout, and H = 30 for Highway."
ENF,0.8157894736842105,"Conﬁguration of Trajectory Length H.
For Atari games, we do not evaluate the full episode
length (up to tens of thousands steps), since our goal is to compare the relative certiﬁed robustness
of different RL algorithms, and the evaluation on relatively short trajectory is sufﬁcient under af-
fordable computation cost. Moreover, different episodes in Atari games are oftentimes of different
lengths; thus it is necessary that we restrict the episode length to enable a fair comparison. For
Highway, we evaluate on the full episode (H = 30) where we can efﬁciently achieve effective
comparisons."
ENF,0.8168859649122807,Published as a conference paper at ICLR 2022
ENF,0.8179824561403509,"Table 4: Benign empirical performance of three aggregation protocols (PARL, TPARL, and DPARL) applied
on subpolicies trained using three ofﬂine RL algorithms (DQN, QR-DQN, and C51), with the number of sub-
policies (i.e., #partitions) u equal to 30 or 50. We report results averaged over 20 runs of varying randomness
in the environment."
ENF,0.819078947368421,"Freeway
u = 30
u = 50"
ENF,0.8201754385964912,"PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)"
ENF,0.8212719298245614,"DQN
10.90 ± 0.77 11.65 ± 0.57 11.25 ± 0.54 12.00 ± 0.45
11.45 ± 0.74
10.95 ± 0.97 11.60 ± 1.02 12.40 ± 0.58 11.65 ± 0.65
11.90 ± 0.89
QR-DQN 11.60 ± 0.66 11.85 ± 1.15 11.25 ± 0.62 11.80 ± 0.87
12.10 ± 0.99
11.50 ± 0.92 11.60 ± 1.02 12.15 ± 0.57 12.80 ± 0.51
11.90 ± 0.77
C51
11.20 ± 0.60 12.45 ± 0.50 12.55 ± 0.50 11.40 ± 0.66
12.40 ± 0.49
11.70 ± 1.27 11.80 ± 0.75 12.70 ± 0.46 11.50 ± 0.87
11.95 ± 0.92"
ENF,0.8223684210526315,"Breakout
u = 30
u = 50"
ENF,0.8234649122807017,"PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)"
ENF,0.8245614035087719,"DQN
58.65 ± 40.83 38.05 ± 10.22 26.00 ± 12.79 13.50 ± 11.41
36.00 ± 13.39
60.25 ± 31.99 37.90 ± 11.55
25.90 ± 9.55
14.95 ± 7.24
45.00 ± 13.44
QR-DQN 76.50 ± 79.76 45.25 ± 42.70
22.05 ± 9.13
19.05 ± 6.91
42.05 ± 15.60
62.80 ± 28.71
32.10 ± 9.27
40.25 ± 52.97 17.30 ± 7.89
41.00 ± 13.87
C51
51.80 ± 10.74 37.60 ± 11.38 24.75 ± 12.77
13.55 ± 7.37
34.75 ± 10.91
60.55 ± 20.44 34.85 ± 11.92
26.15 ± 9.98
19.65 ± 7.30
39.90 ± 14.69"
ENF,0.8256578947368421,"Highway
u = 30
u = 50"
ENF,0.8267543859649122,"PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)"
ENF,0.8278508771929824,"DQN
28.07 ± 3.86 19.16 ± 10.26 16.83 ± 8.35 12.69 ± 6.17
23.55 ± 9.01
29.05 ± 0.62 16.63 ± 9.93 15.85 ± 8.52 13.43 ± 8.29
22.51 ± 8.01
QR-DQN 28.52 ± 1.22
18.63 ± 8.07
15.53 ± 7.46 12.77 ± 6.39
23.11 ± 7.94
27.64 ± 3.01 20.59 ± 7.70 14.94 ± 7.76 14.13 ± 7.54
23.49 ± 7.95
C51
28.86 ± 1.08
20.20 ± 9.21
13.30 ± 8.65
9.36 ± 5.96
21.51 ± 10.04
27.44 ± 4.32 15.12 ± 9.36 16.13 ± 8.46 10.61 ± 6.52
19.94 ± 11.14"
ENF,0.8289473684210527,"H
ADDITIONAL EVALUATION RESULTS AND DISCUSSIONS"
ENF,0.8300438596491229,"H.1
BENIGN EMPIRICAL PERFORMANCE"
ENF,0.831140350877193,"We present the benign empirical cumulative rewards of the three aggregation protocols (PARL,
TPARL, and DPARL) applied on subpolicies trained using three ofﬂine RL algorithms (DQN, QR-
DQN, and C51) in Table 4. The cumulative reward is obtained by running a trajectory of maximum
length H and accumulating the reward achieved at each time step."
ENF,0.8322368421052632,"We discuss the conclusions on multiple levels. We choose H = 1000 for Atari games and H = 30
for Highway environment and report results averaged over 20 runs. On the RL algorithm level,
we see that QR-DQN achieves the highest score in most cases. On the aggregation protocol level,
temporal aggregation enhances the performance on Freeway while dampens the performance on
Breakout and Highway. We particularly note that the results obtained by using temporal aggregation
(TPARL and DPARL) gives signiﬁcantly smaller variance compared to the single-step aggregation
PARL, especially in Breakout. This implies that temporal aggregation can help stabilize the policy.
However, the conclusion does not hold in Highway, which is an interesting phenomenon that is
worth investigating. On the partition number level, a larger partition number can give slightly
better results. On the environment level, Freeway is simpler and more stable than Breakout and
Highway."
ENF,0.8333333333333334,"H.2
COMPARISON OF COPA WITH STANDARD TRAINING"
ENF,0.8344298245614035,"We provide additional experimental results on the comparison between the empirical performance
of our COPA and the standard training in terms of the convergence speed (Figure 3) and policy
quality (Table 5)."
ENF,0.8355263157894737,"In Figure 3, we aim to show the comparison of the convergence speed. For our proposed training,
we plot all training curves of the sampled 5 subpolicies in blue, where each subpolicy is trained on
one partition of the dataset. (We do not plot all 50 curves for visual clarity.) For standard training,
we plot the training curve of the standard policy in red, where the single policy is trained on the
entire dataset. We see that on Freeway, most subpolicies converge slower than the standard policy,
but will reach similar convergence value as the standard training one; while there also exist a few
subpolicies that fail to be trained. On Breakout, within 50 epochs, we observe substantial ﬂuctuation
for the training curves of all the policies, as well as large variance for the achieved reward at the last"
ENF,0.8366228070175439,Published as a conference paper at ICLR 2022
ENF,0.8377192982456141,"0
10
20
30
40
50
# epoch 0 5 10 15 20 25 30"
ENF,0.8388157894736842,reward
ENF,0.8399122807017544,training curve (Freeway)
ENF,0.8410087719298246,"subpolicies
standard policy"
ENF,0.8421052631578947,"0
10
20
30
40
50
# epoch 0 50 100 150 200 250 300"
ENF,0.8432017543859649,reward
ENF,0.8442982456140351,training curve (Breakout)
ENF,0.8453947368421053,"subpolicies
standard policy"
ENF,0.8464912280701754,"0
600
1200
1800
2400
3000
# epoch 5 10 15 20 25 30"
ENF,0.8475877192982456,reward
ENF,0.8486842105263158,training curve (Highway)
ENF,0.8497807017543859,"subpolicies
standard policy"
ENF,0.8508771929824561,"Figure 3: The convergence speed of the proposed partition-based training compared with the standard
training. For our proposed training, we plot all training curves of the sampled 5 subpolicies in blue, where
each subpolicy is trained on one partition of the dataset. (We do not plot all 50 curves for visual clarity). For
standard training, we plot the training curve of the standard policy in red, where the single policy is trained on
the entire dataset. In Atari games, we train each policy (or subpolicy) for a ﬁxed number of 50 epochs, where
each epoch consumes 1M randomly sampled training data. In Highway environment, we train each policy (or
subpolicy) for a ﬁxed number of 3000 epochs, where each epoch consumes 1K randomly sampled training
data. The ofﬂine RL algorithm used is DQN."
ENF,0.8519736842105263,"Table 5: The policy quality measured by empirical cumulative reward of the proposed aggregation proto-
cols (PARL, TPARL, and DPARL) compared with the standard training. In our aggregation, we aggregate
over u subpolicies with u equal to 30 or 50 . We report results averaged over 20 runs of varying randomness
in the environment, where each run is an episode of length at most 1000 for Atari games and 30 for Highway
environment. The ofﬂine RL algorithm used is DQN."
ENF,0.8530701754385965,"standard
trained
policy"
ENF,0.8541666666666666,"u = 30
u = 50"
ENF,0.8552631578947368,"PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)"
ENF,0.856359649122807,"Freeway
12.00 ± 0.89
10.90 ± 0.77
12.00 ± 0.45
11.45 ± 0.74
10.95 ± 0.97
11.65 ± 0.65
11.90 ± 0.89
Breakout 97.60 ± 117.28 58.65 ± 40.83 13.50 ± 11.41
36.00 ± 13.39
60.25 ± 31.99 14.95 ± 7.24
45.00 ± 13.44
Highway
28.90 ± 0.87
28.07 ± 3.86
12.69 ± 6.17
23.55 ± 9.01
29.05 ± 0.62
13.43 ± 8.29
22.51 ± 8.01"
ENF,0.8574561403508771,"epoch. Thus, on these two Atari games, we cannot draw conclusions w.r.t. the convergence. Given
more computational resources to run more epochs, we would be able to draw more informative
conclusions. In contrast, on Highway, all subpolicies display similarly good convergence properties,
showing comparable convergence speed and value with the standard training on the entire dataset."
ENF,0.8585526315789473,"In Table 5, we aim to compare the policy quality of the aggregated policy derived in our COPA
framework (i.e., PARL, TPARL, and DPARL) with the policy obtained from standard training. We
see that on Freeway, our three protocols achieve comparable results with the policy obtained by
standard training on the entire dataset; on Breakout, although the quality of our obtained policies is
lower, our policies are much more stable with signiﬁcantly lower variance than the standard training;
on Highway, only PARL obtains comparable results to the standard training policy, indicating the
lack of temporal continuity in this environment."
ENF,0.8596491228070176,"Table 6: Average window size (i.e., PT
t=1 Wt/T) for the aggregation protocol DPARL (Wmax = 5) ap-
plied on subpolicies trained using three ofﬂine RL algorithms (DQN, QR-DQN, and C51), with the number of
subpolicies (i.e., #partitions) u equal to 30 or 50. We report results averaged over all time steps in 20 runs."
ENF,0.8607456140350878,"u = 30
u = 50"
ENF,0.8618421052631579,"DQN
QR-DQN
C51
DQN
QR-DQN
C51"
ENF,0.8629385964912281,"Freeway
2.69 ± 1.73
2.59 ± 1.71
2.64 ± 1.74
2.79 ± 1.73
2.70 ± 1.73
2.72 ± 1.73
Breakout
2.28 ± 1.46
2.44 ± 1.55
2.32 ± 1.48
2.33 ± 1.48
2.40 ± 1.52
2.39 ± 1.52
Highway 1.97 ± 0.44
2.21 ± 0.27
2.16 ± 0.37
2.23 ± 0.32
2.26 ± 0.24
2.18 ± 0.37"
ENF,0.8640350877192983,Published as a conference paper at ICLR 2022
ENF,0.8651315789473685,"Table 7: Runtime (unit: seconds) of the aggregation protocol DPARL (Wmax = 5) applied on subpolicies
trained using ofﬂine RL algorithm DQN, with the number of subpolicies (i.e., partition number) u equal to 30 or
50. We compare with the standard testing which tests the runtime of a single trained DQN policy without using
our framework. We report results averaged over 20 runs of varying randomness in the environment, where each
run is an episode of length at most 1000."
ENF,0.8662280701754386,"standard testing
DPARL (u = 30)
DPARL (u = 50)"
ENF,0.8673245614035088,"Freeway
2.53 ± 0.50
56.37 ± 1.75
79.05 ± 4.27
Breakout
2.00 ± 0.56
72.05 ± 13.09
108.05 ± 8.71"
ENF,0.868421052631579,"H.3
MORE ANALYTICAL STATISTICS FOR DPARL"
ENF,0.8695175438596491,"Selected Window Size.
We provide the mean and variance for the selected window sizes in
DPARL in Table 6. In our experiments, the maximum window size Wmax = 5, while the aver-
age selected window sizes are all around half of the maximum value. Thus, the average certiﬁcation
time is expected to be much smaller compared with the worst-case time complexity."
ENF,0.8706140350877193,"Running Time.
We provide the running time in Table 7. Speciﬁcally, we compare the running
time of DPARL (Wmax = 5) applied on u = 30 or 50 subpolicies trained using ofﬂine RL algorithm
DQN with the normal testing which tests the runtime of a single trained DQN policy without using
our framework. As we have shown in the remark of Theorem 3 in Section 4.3, the time complexity of
DPARL is O
 
W 2
max|A|2u + Wmax|A|2u log u

. In Table 7, we also see that the runtime of DPARL
scales roughly quasilinearly with the number of subpolicies u, and quadratically with the action set
size |A|, which is 3 for Freeway and 4 for Breakout. We omit the runtime for Highway, since the
horizon length, the environment type, and the neural network size are all different for Highway and
Atari games."
ENF,0.8717105263157895,"H.4
MAXIMUM TOLERABLE POISONING THRESHOLD VS. TOTAL TRAJECTORY NUMBER"
ENF,0.8728070175438597,"We provide the total number of trajectories in the ofﬂine dataset used for training the environments
Freeway, Breakout and Highway below, as well as the corresponding ratio of the maximum tolerable
poisoning threshold w.r.t. the total number of trajectories. For Freeway, the total number of trajec-
tories in the entire ofﬂine dataset is 121,892, and the ratio (max Kt / # total trajectories) is 0.0002;
for Breakout, the number is 209,049, and the ratio is 0.0001. We emphasize that, as shown in previ-
ous work on probable defense against poisoning in supervised learning (Levine & Feizi, 2020), the
number of instances they can certify is also not high especially for challenging tasks (e.g., certifying
20 instances on the dataset GTSRB in Levine & Feizi (2020), attaining the ratio 0.0005), which
may imply the intrinsic difﬁculty of certifying against poisoning attacks. Given such difﬁculty, we
already achieve reasonable certiﬁcation as the ﬁrst work on certiﬁed robust RL against poisoning,
and we hope future works can further improve upon our results."
ENF,0.8739035087719298,"H.5
COMPARISON OF FREEWAY, BREAKOUT, AND HIGHWAY"
ENF,0.875,"Freeway and Breakout are two typical types of games with distinctive game properties. Freeway is a
simple game where the agent aims to cross the road and avoid the trafﬁc. In most of the time steps,
the agent would take the “forward” action; only when there is a need to avoid the trafﬁc will the agent
stop and wait. In comparison, the game Breakout involves more complicated interactions between
the paddle and the environment. The paddle position and velocity both play important roles in order
to catch and bounce the ball at an appropriate angle, which requires a frequent switch of the paddle
moving direction. Similar to Breakout, as an autonomous driving environment, Highway requires
the agent to accurately analyze the rapidly changing environment around it and react quickly, leading
to frequent changes in vehicle’s actions."
ENF,0.8760964912280702,"Given the properties of the environments, we note that in Freeway, adjacent time steps may share
consistent action selections, which is not necessarily true in the Breakout and Highway. Thus, it
would be expected to be beneﬁcial to consider the past history for making the current decision in
Freeway, while the past history may interfere with the action selection in Breakout and Highway."
ENF,0.8771929824561403,Published as a conference paper at ICLR 2022
ENF,0.8782894736842105,"Table 8: Action change ratio (in percentage) of three aggregation protocols (PARL, TPARL, and DPARL)
applied on subpolicies trained in Highway environment using three ofﬂine RL algorithms (DQN, QR-DQN, and
C51), with the number of subpolicies (i.e., #partitions) u equal to 30, 50, or 100. We report results averaged
over 20 runs of varying randomness in the environment."
ENF,0.8793859649122807,"u
RL
Algorithm"
ENF,0.8804824561403509,Aggregation Protocol
ENF,0.881578947368421,"PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)"
DQN,0.8826754385964912,"30
DQN
59.79 ± 8.99
24.89 ± 10.47
33.82 ± 8.75
QR-DQN
61.00 ± 12.74
31.14 ± 6.43
37.80 ± 7.16
C51
59.00 ± 10.39
18.17 ± 11.52
35.44 ± 8.65"
DQN,0.8837719298245614,"50
DQN
55.50 ± 8.84
26.05 ± 11.69
40.07 ± 13.74
QR-DQN
60.37 ± 10.31
28.48 ± 6.81
37.78 ± 12.15
C51
59.39 ± 12.34
28.04 ± 10.71
38.21 ± 11.23"
DQN,0.8848684210526315,"100
DQN
48.67 ± 5.91
22.17 ± 7.65
33.45 ± 9.70
QR-DQN
61.17 ± 8.38
24.12 ± 9.85
36.76 ± 7.87
C51
61.83 ± 10.41
22.91 ± 13.34
34.06 ± 9.19"
DQN,0.8859649122807017,"Comparisons of Bottleneck states in Atari Games.
In Breakout, the game goal is to control
the paddle so that the ball is bounced to hit the brick. There are clearly very different stages in
the Breakout game, e.g., when the ball is ﬂying towards the paddle and when it is bounced back.
An example of the bottleneck state is when the ball is approaching the paddle. The poisoning
behavior may lead to the disastrous effect that the policy learns to control the paddle to slide in the
opposite direction of the ball at such bottleneck states, while other states may not be as vulnerable.
In comparison, in Freeway, the game goal is to control the agent to cross the road while avoiding
the trafﬁc. The agent is faced with similar trafﬁc conditions everywhere on the road, and can make
stable choices regardless of the complex situation. Thus there are very few bottleneck states."
DQN,0.8870614035087719,"A Study on the Frequency of Action Change in Highway.
In Table 8, we present the action
change ratio (# action changes / trajectory length) in Highway environment. Comparing among
the aggregation protocols, we ﬁrst note that the single-step aggregation policy πP frequently alters
actions as the reaction to the rapidly changing driving environment, while the temporal aggregation
protocol πT changes actions less frequently. This is because in TPARL, the agent is explicitly forced
to take stable actions based on a ﬁxed window size. Second, comparing the action change ratio with
the benign empirical performance in Table 4, we observe that πP and πT achieves the highest and
lowest empirical performance respectively, which indicates the correlation between action change
ratio with the achieved empirical reward. Speciﬁcally, in TPARL, the action change is limited as a
result of the enforced window size, leading to the limited benign performance."
DQN,0.8881578947368421,"H.6
FULL RESULTS OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY"
DQN,0.8892543859649122,"As a complete set of results for per-state action certiﬁcation apart from Figure 1 in Section 5.1, we
present the entire cumulative histogram of tolerable poisoning thresholds in Figure 4 and Figure 5,
together with the average tolerable poisoning thresholds in Table 9. The deﬁnitions of the two
metrcis can be found in Section 5.1."
DQN,0.8903508771929824,"Basically, the conclusions in terms of the comparisons on the RL algorithm level, the aggregation
protocol level, the partition number level, and the game level are all similar to that derived in Sec-
tion 5.1."
DQN,0.8914473684210527,"H.7
FULL RESULTS OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND"
DQN,0.8925438596491229,"In addition to the evaluation results provided in Figure 2 in section 5.2, we provide a more compre-
hensive set of evaluation results with more settings of trajectory length H here in Figure 6."
DQN,0.893640350877193,"We draw similar conclusions as discussed in Section 5.2. Speciﬁcally, in Highway, C51 achieves
higher certiﬁed lower bounds than other two when the poisoning size is large, which can be explained
by the larger portion of states that can tolerate large poisoning sizes as shown in Figure 5."
DQN,0.8947368421052632,Published as a conference paper at ICLR 2022
DQN,0.8958333333333334,"PARL
TPARL (W = 4)
DPARL (Wmax = 5)"
DQN,0.8969298245614035,"Freeway, u = 30
Freeway, u = 50"
DQN,0.8980263157894737,"DQN
stability ratio"
DQN,0.8991228070175439,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9002192982456141,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9013157894736842,"QR-DQN
stability ratio"
DQN,0.9024122807017544,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9035087719298246,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9046052631578947,"C51
stability ratio"
DQN,0.9057017543859649,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9067982456140351,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0 ≥K
≥K"
DQN,0.9078947368421053,"(a) Freeway
Breakout, u = 30
Breakout, u = 50"
DQN,0.9089912280701754,"DQN
stability ratio"
DQN,0.9100877192982456,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9111842105263158,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9122807017543859,"QR-DQN
stability ratio"
DQN,0.9133771929824561,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9144736842105263,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9155701754385965,"C51
stability ratio"
DQN,0.9166666666666666,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9177631578947368,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0 ≥K
≥K"
DQN,0.918859649122807,"(b) Breakout
Figure 4: Robustness certiﬁcation for per-state action stability on Atari games (full results). We plot the
cumulative histogram of the tolerable poisoning size K for all time steps in one trajectory. We provide results
on two games (Freeway and Breakout), two partition numbers (u = 30 and u = 50), and a comparison of
three certiﬁcation methods (PARL, TPARL, and DPARL). The results are averaged over 20 runs considering
the randomness in the game environment, and the short vertical bar on top of each bar represents the standard
deviation."
DQN,0.9199561403508771,Published as a conference paper at ICLR 2022
DQN,0.9210526315789473,"PARL
TPARL (W = 4)
DPARL (Wmax = 5)"
DQN,0.9221491228070176,"Highway, u = 30
Highway, u = 50"
DQN,0.9232456140350878,"DQN
stability ratio"
DQN,0.9243421052631579,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9254385964912281,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9265350877192983,"QR-DQN
stability ratio"
DQN,0.9276315789473685,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9287280701754386,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9298245614035088,"C51
stability ratio"
DQN,0.930921052631579,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9320175438596491,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9331140350877193,"≥K
≥K
Highway, u = 100"
DQN,0.9342105263157895,"DQN
stability ratio"
DQN,0.9353070175438597,"0 1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334353637383940414243444546474849
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9364035087719298,"QR-DQN
stability ratio"
DQN,0.9375,"0 1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334353637383940414243444546474849
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9385964912280702,"C51
stability ratio"
DQN,0.9396929824561403,"0 1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334353637383940414243444546474849
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9407894736842105,"≥K
Figure 5: Robustness certiﬁcation for per-state action stability on Highway environment (full results).
We plot the cumulative histogram of the tolerable poisoning size K for all time steps in one trajectory. We
provide results on Highway environment, three partition numbers (u = 30, u = 50 and u = 100), and a
comparison of three certiﬁcation methods (PARL, TPARL, and DPARL). The results are averaged over 20 runs
considering the randomness in the environment, and the short vertical bar on top of each bar represents the
standard deviation."
DQN,0.9418859649122807,"u = 30, PARL
u = 50, PARL"
DQN,0.9429824561403509,"u = 30, TPARL (W = 4)
u = 50, TPARL (W = 4)"
DQN,0.944078947368421,"u = 30, DPARL (Wmax = 5)
u = 50, DPARL (Wmax = 5)"
DQN,0.9451754385964912,"Freeway, H = 100 Freeway, H = 200 Freeway, H = 400
Breakout, H = 50
Breakout, H = 75 Highway, H = 30"
DQN,0.9462719298245614,"DQN
JK"
DQN,0.9473684210526315,"0
5
10
15
20
25
0 1"
DQN,0.9484649122807017,"0
5
10
15
20
25
0 1 2 3"
DQN,0.9495614035087719,"0
5
10
15
20
25
0 1 2 3 4 5"
DQN,0.9506578947368421,"0
3
6
9 12 15 18 21
0 1"
DQN,0.9517543859649122,"0
5
10
15
20
25
0 1 2"
DQN,0.9528508771929824,"0
5
10
15
20
25 5 10 15 20 25"
DQN,0.9539473684210527,"QR-DQN
JK"
DQN,0.9550438596491229,"0
5
10
15
20
25
0 1"
DQN,0.956140350877193,"0
5
10
15
20
25
0 1 2 3"
DQN,0.9572368421052632,"0
5
10
15
20
25
0 1 2 3 4 5"
DQN,0.9583333333333334,"0
3
6
9 12 15 18 21 24
0 1"
DQN,0.9594298245614035,"0
5
10
15
20
25
0 1 2"
DQN,0.9605263157894737,"0
5
10
15
20
25 5 10 15 20 25"
DQN,0.9616228070175439,"C51
JK"
DQN,0.9627192982456141,"0
5
10
15
20
25
0 1"
DQN,0.9638157894736842,"0
5
10
15
20
25
0 1 2 3"
DQN,0.9649122807017544,"0
5
10
15
20
25
0 1 2 3 4 5"
DQN,0.9660087719298246,"0
3
6
9 12 15 18 21 24 27
0 1"
DQN,0.9671052631578947,"0
5
10
15
20
25
0 1 2"
DQN,0.9682017543859649,"0
5
10
15
20
25 5 10 15 20 25"
DQN,0.9692982456140351,"Poisoning size K
Poisoning size K
Poisoning size K
Poisoning size K
Poisoning size K
Poisoning size K
Figure 6: Robustness certiﬁcation for cumulative reward (full results). We plot the lower bound of cumu-
lative reward JK w.r.t. poisoning size K under three different certiﬁcation methods (PARL, TPARL (W = 4),
DPARL (Wmax = 5)) with two partition numbers (u ∈{30, 50}). Each row corresponds to one RL algorithm,
and each column corresponds to one setting of trajectory length H."
DQN,0.9703947368421053,Published as a conference paper at ICLR 2022
DQN,0.9714912280701754,"Table 9:
Average tolerable poisoning thresholds of three aggregation protocols (PARL, TPARL, and
DPARL) applied on subpolicies trained using three ofﬂine RL algorithms (DQN, QR-DQN, and C51), with
the number of subpolicies (i.e., #partitions) u equal to 30 or 50. We report results averaged over 20 runs of
varying randomness in the environment."
DQN,0.9725877192982456,"Freeway
u = 30
u = 50"
DQN,0.9736842105263158,"PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)"
DQN,0.9747807017543859,"DQN
9.90 ± 0.14
10.21 ± 0.09
10.30 ± 0.09
16.78 ± 0.42
17.16 ± 0.29
16.88 ± 0.45
QR-DQN 9.87 ± 0.20
10.11 ± 0.30
10.16 ± 0.34
16.79 ± 0.52
17.31 ± 0.29
17.03 ± 0.40
C51
9.57 ± 0.24
9.83 ± 0.15
10.11 ± 0.19
16.82 ± 0.40
17.08 ± 0.34
17.61 ± 0.15"
DQN,0.9758771929824561,"Breakout
u = 30
u = 50"
DQN,0.9769736842105263,"PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)"
DQN,0.9780701754385965,"DQN
1.08 ± 0.09
1.28 ± 0.15
0.85 ± 0.09
2.07 ± 0.22
1.92 ± 0.30
1.55 ± 0.28
QR-DQN
1.38 ± 0.09
1.33 ± 0.18
1.18 ± 0.10
2.12 ± 0.27
1.93 ± 0.14
1.71 ± 0.19
C51
1.10 ± 0.08
1.42 ± 0.19
0.93 ± 0.13
2.43 ± 0.21
2.37 ± 0.19
1.90 ± 0.19"
DQN,0.9791666666666666,"Highway
u = 30
u = 50"
DQN,0.9802631578947368,"PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)"
DQN,0.981359649122807,"DQN
4.38 ± 0.78
2.99 ± 1.08
3.75 ± 1.85
7.16 ± 1.58
6.27 ± 1.61
6.32 ± 0.98
QR-DQN 4.18 ± 0.91
3.35 ± 1.44
2.84 ± 0.55
6.52 ± 1.17
5.02 ± 1.60
4.61 ± 1.20
C51
4.97 ± 1.09
4.31 ± 1.45
4.12 ± 0.95
7.95 ± 1.46
6.55 ± 2.20
6.69 ± 1.47"
DQN,0.9824561403508771,"We additionally point out that the value JK achieved at poisoning size K = 0 (e.g., JK = 4
for Freeway, 2 for Breakout, and 28.31 for Highway under πP over u = 50 DQN subpolicies)
corresponds to the case where there is no poisoning at all on the training set. Our successive bounds
under larger K are non-vacuous compared to this value."
DQN,0.9835526315789473,"I
A BROADER DISCUSSION ON RELATED WORK"
DQN,0.9846491228070176,"I.1
POISONING ATTACKS IN RL"
DQN,0.9857456140350878,"Below, we provide a brief discussion on the related works on policy poisoning and reward poison-
ing (Ma et al., 2019; Sun et al., 2021; Huang & Zhu, 2019)."
DQN,0.9868421052631579,"Ma et al. (2019) and Huang & Zhu (2019) study reward poisoning where the attacker can modify
the rewards in an ofﬂine dataset or the reward signals during the online interaction. The attacker’s
goal is to force learning a particular target policy, or minimize the agent’s reward in the original
task. Under this framework, Ma et al. (2019) considers two victim learners with speciﬁc assump-
tions on the learner structure and develops attacks for them, while Huang & Zhu (2019) provides a
theoretical analysis on the conditions for successful attacks against a Q-learning agent. In compari-
son to only reward poisoning in Ma et al. (2019) and Huang et al. (2017), Sun et al. (2021) focuses
on general policy poisoning attacks for policy gradient learners in the online RL setting by using
the vulnerability-awareness metric to decide when to poison, and the adversarial critic to guide the
poisoning. Sun et al. (2021) achieve successful poisoning against policy-based agents in various
complex environments in online RL, but it remains an interesting open problem as to how to poison
the ofﬂine RL dataset which is agnostic to the learning algorithm."
DQN,0.9879385964912281,"I.2
EMPIRICALLY ROBUST RL"
DQN,0.9890350877192983,"Robust RL against Evasion Attacks.
We brieﬂy review several categories of RL methods that
demonstrate empirical robustness against evasion attacks."
DQN,0.9901315789473685,Published as a conference paper at ICLR 2022
DQN,0.9912280701754386,"Randomization methods (Tobin et al., 2017; Akkaya et al., 2019) were ﬁrst proposed to encourage
exploration. This type of method was later systematically studied for its potential to improve model
robustness. NoisyNet (Fortunato et al., 2017) adds parametric noise to the network’s weight during
training, providing better resilience to both training-time and test-time attacks (Behzadan & Munir,
2017; 2018), also reducing the transferability of adversarial examples, and enabling quicker recovery
with fewer number of transitions during phase transition."
DQN,0.9923245614035088,"Under the adversarial training framework, Kos & Song (2017) and Behzadan & Munir (2017)
show that re-training with random noise and FGSM perturbations increases the resilience against
adversarial examples. Pattanaik et al. (2018) leverage attacks using an engineered loss function
speciﬁcally designed for RL to signiﬁcant increase the robustness to parameter variations. RS-
DQN (Fischer et al., 2019) is an imitation learning based approach that trains a robust student-DQN
in parallel with a standard DQN in order to incorporate the constrains such as SOTA adversarial
defenses (Madry et al., 2017; Mirman et al., 2018)."
DQN,0.993421052631579,"SA-DQN (Zhang et al., 2020a) is a regularization based method that adds regularizers to the training
loss function to encourage the top-1 action to stay unchanged under perturbation."
DQN,0.9945175438596491,"Built on top of the neural network veriﬁcation algorithms (Gowal et al., 2018; Weng et al., 2018),
Radial-RL (Oikarinen et al., 2020) proposes to minimize an adversarial loss function that incor-
porates the upper bound of the perturbed loss, computed using certiﬁed bounds from veriﬁcation
algorithms. CARRL (Everett et al., 2021) aims to compute the lower bounds of action-values under
potential perturbation and select actions according to the worst-case bound, but it relies on linear
bounds (Weng et al., 2018) and is only suitable for low-dimensional environments."
DQN,0.9956140350877193,"These robust RL methods only provide empirical robustness against perturbed state inputs during
test time, but cannot provide theoretical guarantees for the performance of the trained models under
any bounded perturbations."
DQN,0.9967105263157895,"Robust RL against Poisoning Attacks.
Banihashem et al. (2021) considers the threat model of
reward poisoning attacks proposed by Ma et al. (2019); Rakhsha et al. (2020); Zhang et al. (2020b),
where the attacker aims to force learning a target policy while minimizing the cost of reward manip-
ulation. As shown in Ma et al. (2019); Rakhsha et al. (2020); Zhang et al. (2020b), this optimization
problem is feasible and always has a unique optimal solution. Leveraging this property, Banihashem
et al. (2021) formulates the defense task as another optimization problem which aims to optimize the
agent’s worst case performance among the set of plausible candidates of the true reward function.
They speciﬁcally consider two settings regarding the agent’s knowledge of the attack parameter, and
provide lower bounds on the performance of the defense policy. In contrast to Banihashem et al.
(2021) which focuses on a speciﬁc type of attack, our paper considers the threat model of general
poisoning attacks, where the attacker has the power to manipulate the training trajectories arbitrarily.
Given limited knowledge regarding the attack, our proposed COPA framework is general and appli-
cable to any potential attack. Our robustness is derived from the aggregation over both sub-policy
level and temporal level. One similarity between Banihashem et al. (2021) and our work is that we
both aim to provide lower bounds of the cumulative reward for our proposed method as the provable
guarantee / certiﬁcation criteria."
DQN,0.9978070175438597,"I.3
ROBUSTNESS CERTIFICATION FOR RL"
DQN,0.9989035087719298,"Wu et al. (2022) provide the ﬁrst robustness certiﬁcation for RL against test-time evasion attacks
following the line of work on randomized smoothing (Cohen et al., 2019; Salman et al., 2019). Con-
cretely, they apply per-state smoothing to achieve the certiﬁcation for per-state action stability, as
well as trajectory smoothing to obtain the certiﬁcation for cumulative reward. Notably, Wu et al.
(2022) propose an adaptive tree search algorithm to explore all possible trajectories and thus de-
rive the robustness guarantee for cumulative reward. The relationship between our COPA-SEARCH
and their Algorithm 3 is discussed in detail in Appendix E.3. In comparison, robustness in their
work is derived from smoothing, while our robustness comes from aggregation. We propose aggre-
gation protocols and certiﬁcation methods that leverage the temporal information by dynamically
aggregating over the past time steps, which is not covered in Wu et al. (2022)."
