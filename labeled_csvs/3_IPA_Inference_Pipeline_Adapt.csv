Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022371364653243847,"Efficiently optimizing multi-model inference pipelines for
fast, accurate, and cost-effective inference is a crucial chal-
lenge in ML production systems, given their tight end-to-end
latency requirements. To simplify the exploration of the vast
and intricate trade-off space of accuracy and cost in inference
pipelines, providers frequently opt to consider one of them.
However, the challenge lies in reconciling accuracy and cost
trade-offs.
To address this challenge and propose a solution to effi-
ciently manage model variants in inference pipelines, we
present IPA, an online deep learning Inference Pipeline
Adaptation system that efficiently leverages model variants
for each deep learning task. Model variants are different
versions of pre-trained models for the same deep learning
task with variations in resource requirements, latency, and
accuracy. IPA dynamically configures batch size, replication,
and model variants to optimize accuracy, minimize costs, and
meet user-defined latency SLAs using Integer Programming.
It supports multi-objective settings for achieving different
trade-offs between accuracy and cost objectives while re-
maining adaptable to varying workloads and dynamic traffic
patterns. Extensive experiments on a Kubernetes implemen-
tation with five real-world inference pipelines demonstrate
that IPA improves normalized accuracy by up to 35% with a
minimal cost increase of less than 5%."
INTRODUCTION,0.0044742729306487695,"1
Introduction"
INTRODUCTION,0.006711409395973154,"Noaways, companies run some or all of their ML pipelines
on cloud computing platforms [70]. The efficient deploy-
ment of machine learning models is crucial in contemporary
systems where ML inference services consume more than
90% of datacenter resources dedicated to machine learning
workloads [12, 16]. In various critical applications, such
as healthcare systems [29], recommendation systems [56],
question-answering, and chatbots [20], a range of machine
learning models, including computer vision models [29] and
speech models [45], play an essential role. It is imperative
to deploy these models cost-efficiently while maintaining
system performance and scalability.
Automatic resource allocation is a complex problem that re-
quires careful consideration and has been extensively studied
in various domains, including stream processing [27,31,46],"
INTRODUCTION,0.008948545861297539,Table 1: Comparison with previous works
INTRODUCTION,0.011185682326621925,"System
Pipeline
Cost
Accuracy
Adaptive"
INTRODUCTION,0.013422818791946308,"Rim [42]
✓
✕
✓
✕
INFaaS [60]
✕
✓
✓
✕
Inferline [25]
✓
✓
✕
✕
GPULet [22]
✓
✓
✕
✕
Llama [61]
✓
✓
✕
✕
FA2 [59]
✓
✓
✕
✕
Model Switch [75]
✕
✕
✓
✕
Scrooge [41]
✓
✓
✕
✕
Nexus [64]
✓
✓
✕
✕
Cocktail [36]
✕
✓
✓
✕
InfAdapter [63]
✕
✓
✓
✓
IPA
✓
✓
✓
✓"
INTRODUCTION,0.015659955257270694,"Submitted to the Journal of Systems Research (JSys)
2023"
INTRODUCTION,0.017897091722595078,"IPA
config 2"
INTRODUCTION,0.020134228187919462,"Most
Accurate
Least
Accurate"
INTRODUCTION,0.02237136465324385,"IPA
config 1"
INTRODUCTION,0.024608501118568233,"System
Designer"
INTRODUCTION,0.026845637583892617,"Figure 1: IPA provides a tunable framework for adjusting the
system based on two contradictory cost and accuracy rank
objectives."
INTRODUCTION,0.029082774049217,"on workload, and Model-switching, which employs differ-
ent model variants with different accuracies/latencies to vary
workloads and tasks, enabling finer control over resource allo-
cation and accuracy. The combination of these two techniques
has been advocated to achieve more precise adjustments in
accuracy and cost trade-offs. Previous autoscaling [62] and
model-switching [63,75] works have argued that using both
techniques in conjunction with each other is beneficial by
providing more precise adjustments in terms of accuracy and
cost trade-offs, providing greater flexibility and efficiency in
ML model resource allocation. However, none of the above
works have considered optimizing accuracy and cost jointly
in a multi-stage pipeline setting. Table 1 presents an overview
of related inference serving works. Systems with inference
pipeline serving have often overlooked the presence of multi-
ple model variants for each inference task [25,41,47,59,61].
The heterogeneity of these model variants presents an op-
portunity not only to configure the pipeline to meet latency
objectives but also to opportunistically select the most suit-
able model variant to enhance the accuracy of the pipeline
output.
In this paper, we propose IPA, a system for jointly opti-
mizing accuracy and cost objectives. It can achieve multiple
trade-offs between these two contradictory objectives based
on the pipeline designer’s preference. Figure 1 shows the
premise of IPA that provides a tunable framework for adapt-
ing the inference pipeline to contradictory accuracy and cost
objectives. The choice of models in previous works was lim-
ited to using just one pre-selected model variant. IPA can
broaden this search space by considering all model variants
and dynamically adapting to a suitable choice of models based
on the pipeline designer’s preference.
The main contributions of this paper are as follows:"
INTRODUCTION,0.03131991051454139,"• We revisit the resource management design in inference
pipelines by incorporating model switching, autoscal-
ing, and pipeline reconfiguration (stage batch size). IPA
proposes a new optimization formulation to allow for a
more granular trade-off between the accuracy and cost
objectives. It is also adaptable based on the inference
pipeline designer’s preference for each accuracy and cost"
INTRODUCTION,0.03355704697986577,objective.
INTRODUCTION,0.035794183445190156,"• We propose a new optimization formulation based on the
interaction between model switching, replication, and
batch size. It can find (1) exact resources to allocate to
each stage of the pipeline, (2) variants to decide for each
stage, and (3) dependency between stages that enable
accurate estimation of demand while guaranteeing end-
to-end latency."
INTRODUCTION,0.03803131991051454,"• The full implementation of IPA is built on top of Ku-
bernetes. IPA integrates open-source technologies in its
stack to facilitate seamless integration into production
clusters."
INTRODUCTION,0.040268456375838924,"• Experimental results show that IPA can achieve more
granular trade-offs between the two contradictory objec-
tives of cost and accuracy. In some scenarios, it was able
to provide an improvement of up to 35% in normalized
accuracy with a negligible increase in cost."
LIT REVIEW,0.042505592841163314,"2
Background and motivation"
LIT REVIEW,0.0447427293064877,"This section provides the background of the inference pipeline
and discusses the challenges of reconciling cost and accuracy
trade-offs."
LIT REVIEW,0.04697986577181208,"2.1
Inference Pipeline"
LIT REVIEW,0.049217002237136466,"Traditional ML applications revolve around utilizing a sin-
gular deep neural network (DNN) for executing inference
tasks, such as identifying objects or comprehending natural
language. Conversely, modern ML systems (ML inference
pipelines) are more intricate scenarios, such as digital as-
sistant services like Amazon Alexa, where a series of inter-
connected/chained DNNs (in the form of DAG structures)
is employed to undertake various inference tasks, spanning
speech recognition, question interpretation, question answer-
ing, and text-to-speech conversion, all of which contribute
to fulfilling user queries and requirements [59,61]. As these
systems frequently interact with users, it becomes essential to
have a stringent service level agreement (SLA), which in our
case is end-to-end latency.
Figure 2 illustrates the latency and throughput differences
across different versions of image classification models within
the ResNet family. Notably, there exists an inverse relation-
ship between latency, throughput and accuracy for each
model variant, considering the same number of CPU cores
and fixed batch sizes. This variability adds another dimen-
sion to the trade-off space for selecting model variants within
inference pipelines.
Choosing the best configuration among the highlighted pa-
rameters is non-trivial and subjective to multiple objectives
and constraints, e.g., cost efficiency and SLA requirement
between cloud users and providers. Table 2 demonstrates"
LIT REVIEW,0.05145413870246085,"Submitted to the Journal of Systems Research (JSys)
2023 0 5 10 15"
LIT REVIEW,0.053691275167785234,Throughput (RPS)
LIT REVIEW,0.05592841163310962,"18
34
50
101
152
ResNet variant 0 100 200 300"
LIT REVIEW,0.058165548098434,Latency (ms) 70.0 72.5 75.0 77.5
LIT REVIEW,0.06040268456375839,Accuracy (%) 70.0 72.5 75.0 77.5
LIT REVIEW,0.06263982102908278,Accuracy (%)
LIT REVIEW,0.06487695749440715,Accuracy
LIT REVIEW,0.06711409395973154,"Figure 2: Performance difference across ResNet Family mod-
els for a batch size of one and one CPU core allocation"
LIT REVIEW,0.06935123042505593,"Table 2: Performance difference across ResNet family models
under different CPU allocations for a batch size of one, both
blue and red core/model configuration can respond to 20
RPS and 75 ms throughput and latency requirements but with
different accuracy and costs."
LIT REVIEW,0.07158836689038031,"CPU
Cores
ResNet18
ResNet50"
LIT REVIEW,0.0738255033557047,"Latency
(ms)
Throughput
(RPS)
Latency
(ms)
Throughput
(RPS)"
LIT REVIEW,0.07606263982102908,"1
75
20
135
9
4
23
37
57
21
8
14
62
32
29"
LIT REVIEW,0.07829977628635347,"that under the same incoming throughput of 20 RPS and mu-
tual SLA agreement of 75 ms between the user and service
provider, a user with high accuracy goals will choose a suit-
able configuration of four core assignments under ResNet50
(highlighted in red). But a user with lower accuracy demands
will choose ResNet18 with one core, which can respond to
latency and throughput requirements under lower core alloca-
tions (highlighted in blue)."
LIT REVIEW,0.08053691275167785,"2.2
Configuration Space"
LIT REVIEW,0.08277404921700224,"Batch Size Neural network structure provides parallel compu-
tation capability. Batching multiple requests will leverage the
parallelization capability of neural networks and increase the
utilization of the assigned resources while increasing the total
latency. Previous works [13,25,41] have shown that there is a
relationship between the system utilization and the request la-
tency resulting in a non-trivial trade-off between maximizing
the resource utilization without violating the latency SLA.
Replication There are mainly two resource provisioning tech-
niques, vertical scaling and horizontal scaling. In vertical
scaling, the assigned resources are modified, while in hori-
zontal scaling, the number of replicas with the same amount"
LIT REVIEW,0.08501118568232663,"Accuracy
Latency
Throughput
Cost"
LIT REVIEW,0.087248322147651,"Model
Switch
Batching
Scaling"
LIT REVIEW,0.0894854586129754,Configuration Knob
LIT REVIEW,0.09172259507829977,Metric
LIT REVIEW,0.09395973154362416,Impacts
LIT REVIEW,0.09619686800894854,"Figure 3: Impact of configuration knobs, batching indirectly
affects the cost, e.g., decreasing the throughput will affect the
IPA to more scaling and increase in the cost"
LIT REVIEW,0.09843400447427293,"of resources is adjusted to keep a balance between perfor-
mance and cost. Horizontal scaling allows predictable per-
formance using a similar environment [35]. while vertical
scaling enables a more fine-grained resource allocation of the
ML model. We use horizontal scaling for the current work
similar to [25,59].
Variant Selection Previous works [60,75] have shown that
there is an abundance of ML models for a single ML task.
This brings the opportunity to abstract away the ML task from
the underlying model and opportunistically switch the model
based on the performance needs of the system.
Figure 3 shows the complex relationship between changing
each configuration knob and performance objectives. Chang-
ing the batch size will affect the throughput and latency of
each stage of the pipeline, while changes in the replication fac-
tor will directly impact the pipeline deployment cost. Model
switching will result in changes both in accuracy and cost as
different models have different resource requirements."
LIT REVIEW,0.10067114093959731,"2.3
Challenges"
LIT REVIEW,0.1029082774049217,"The nonlinear dependency between the three configuration
knobs (batch size, replication, and variant selection) and the
pipeline variables introduces a complex decision space be-
tween multiple conflicting goals. Figure 6(a) depicts one of
the evaluated pipelines consisting of two stages, an object
detection stage and an object classifier. A subset of the config-
uration space is presented in Table 3 with different batch size,
accuracy, and cost. We denote cost as the number of replicas
× allocated CPU cores per replica. The chosen configura-
tion at each stage first should support the incoming workload
into the pipeline while guaranteeing SLA, e.g., sum of the la-
tency of both stages should be less than the SLA requirement.
Under the arrival rate of 20 RPS (Request Per Second) and
SLA requirement of 600 milliseconds, both combinations of
(A1, B1) and (A2, B2) can be responsive to the latency and
throughput requirements. However, the first combination can
sustain these requirements under the cost of 2+2 = 4 CPU
cores with a lower accuracy combination while the latter can
sustain the same load with the cost of 10+3 = 13 CPU cores"
LIT REVIEW,0.10514541387024609,"Submitted to the Journal of Systems Research (JSys)
2023"
LIT REVIEW,0.10738255033557047,Table 3: Two stage pipeline tasks options.
LIT REVIEW,0.10961968680089486,"Variant
Scale
Batch
Latency
Cost
Accuracy"
LIT REVIEW,0.11185682326621924,"A1: YOLOv5n
2
1
80
2×1
45.7
A2: YOLOv5m
5
1
347
5×2
64.1
A3: YOLOv5n
2
8
481
2×1
45.7
A4: YOLOv5m
5
8
1654
5×2
64.1
B1: ResNet18
2
1
73
2×1
69.75
B2: ResNet50
3
1
136
3×1
76.13
B3: ResNet18
2
8
383
2×1
69.75
B4: ResNet50
3
8
833
3×1
76.13"
LIT REVIEW,0.11409395973154363,and a higher possible accuracy combination.
LIT REVIEW,0.116331096196868,"Challenge 1 Multiple configurations can satisfy the
latency constraints of the inference pipeline. The
""optimal"" configuration depends on the accuracy and
cost goals."
LIT REVIEW,0.1185682326621924,"The next challenge is that in inference pipelines, the model
selection at an earlier stage of the pipeline will affect the op-
timal model selection at downstream models as the latency
of a model at an earlier stage will affect the end-to-end la-
tency. Consequently, the available options for the downstream
models are more limited. In the similar example of pipeline
Figure 6(a) and Table 3, and under an SLA of 500 ms choos-
ing a high latency and high accuracy configuration of A2 at
the first stage will eliminate the variant B2 from the second
stage’s option."
LIT REVIEW,0.12080536912751678,"Challenge 2 The choice of replication factor, batch
size, and model variant is a joint decision across mul-
tiple stages of the inference pipeline."
IMPLEMENTATION/METHODS,0.12304250559284116,"3
System Design"
IMPLEMENTATION/METHODS,0.12527964205816555,"In this section, we provide a high-level overview of the main
system components in IPA as illustrated in Figure 4.
Model Loader Users submit the models they intend to use
for each stage of a pipeline. These models should provide
a reasonable span of accuracy, latency, and resource foot-
print trade-offs. Model variants can also be generated using
model optimizers such as TensorRT [67], and ONNX graph
optimization by using different quantization level of neural
networks [32] and Neural Architectural Search methods [21].
After the model submission, the profile (discussed in Sec-
tion 4.2) will be executed for each model variant and store
their latency under multiple batch sizes and resource assign-
ments. Furthermore, the optimizer, as discussed in Section 4,
uses the offline profiling to find the optimal solution during
the runtime. Finally, the models are stored in object storage"
IMPLEMENTATION/METHODS,0.12751677852348994,"Load
models"
IMPLEMENTATION/METHODS,0.1297539149888143,Model Loader
IMPLEMENTATION/METHODS,0.1319910514541387,"Object Store
Monitoring"
IMPLEMENTATION/METHODS,0.1342281879194631,External Load
IMPLEMENTATION/METHODS,0.13646532438478748,Adaptive Pipeline
IMPLEMENTATION/METHODS,0.13870246085011187,Adapter
IMPLEMENTATION/METHODS,0.14093959731543623,Online
IMPLEMENTATION/METHODS,0.14317673378076062,Offline
IMPLEMENTATION/METHODS,0.14541387024608501,"Optimiser
Predictor"
IMPLEMENTATION/METHODS,0.1476510067114094,K8S API
IMPLEMENTATION/METHODS,0.14988814317673377,Figure 4: IPA system design
IMPLEMENTATION/METHODS,0.15212527964205816,"to reduce container creation times. In this work, we have used
MinIO object store [5].
Monitoring The monitoring daemon uses the highly-
available time-series database Prometheus [8] underneath
to observe incoming load to the system. It will periodically
monitor the load destined for the pipeline.
Predictor In our load forecasting process, we employ an
LSTM (Long Short-Term Memory), which is a type of re-
current neural network [40]. Our LSTM model is designed
to predict the maximum workload for the next 20 seconds
based on a time series of loads per second collected from
the monitoring component over the past 2 minutes. To train
the LSTM model, we utilized the initial two weeks of the
Twitter-trace dataset [15]. The architecture of our LSTM neu-
ral network consists of a 25-unit LSTM layer followed by a
one-unit dense layer serving as the output layer.
Pipeline System A centralized load balancer distributes the
inference requests between multiple stages of the inference
pipeline. A centralized queue is behind each stage of the
pipeline. Centralized queues help to have a deterministic
queuing behavior and to model its latency efficiently, as de-
scribed in Section 4. A comparison of using distributed and
central queues is provided in Appendix 8. The queues of
each stage then distribute the batched requests between model
replicas. It uses a round-robin policy for load balancing the
batched requests between model replicas. Communications
between multiple stages of the pipeline are implemented us-
ing gRPC [2]. Load balancing between multiple contain-
ers of the same stage is achieved by using Istio [3] sidecar
containers. Each model container is deployed using Docker
containers built from a forked version of MLServer [6] and
Seldon Core [10] for implementing the gRPC web servers
and deployment on Kubernetes.
Pipeline Simulated Model The profiling data gathered by the
profiler provides the information about latency and throughput
of each model variant under different batch sizes. For runtime
decision-making, a discrete event simulator uses this profiling"
IMPLEMENTATION/METHODS,0.15436241610738255,"Submitted to the Journal of Systems Research (JSys)
2023"
IMPLEMENTATION/METHODS,0.15659955257270694,"5l
152"
IMPLEMENTATION/METHODS,0.15883668903803133,"b = 2
b = 2"
IMPLEMENTATION/METHODS,0.1610738255033557,"5n
b = 8
b = 16"
IMPLEMENTATION/METHODS,0.16331096196868009,5n 18 18 18
IMPLEMENTATION/METHODS,0.16554809843400448,Workload (a) (b)
IMPLEMENTATION/METHODS,0.16778523489932887,Workload t t
IMPLEMENTATION/METHODS,0.17002237136465326,"Heavy
Variants"
IMPLEMENTATION/METHODS,0.17225950782997762,"Light
Variants"
IMPLEMENTATION/METHODS,0.174496644295302,"Figure 5: Switching between different configurations under
(a) low and (b) high loads."
IMPLEMENTATION/METHODS,0.1767337807606264,"data to estimate the end-to-end latency and throughput of the
pipeline based on the number of replicas, model variants used,
and batch sizes at each stage. The predicted latencies and
throughputs of the pipeline are then used by the optimizer in
the adapter to the optimal configuration in terms of accuracy
and cost objectives.
Adapter The Adapter is the auto-configuration model that
periodically (1) Fetch incoming load from the monitoring dae-
mon, (2) Predict the next reference load based on observed
historical received load using the LSTM module, (3) Obtain
optimal configuration in terms of used variant, batch size
and number of replicas for the next timestep, and (4) Finally,
the new configuration is applied to the pipeline through Ku-
bernetes Python API [4]. Figure 5 shows a snapshot of the
system with two available options per each model on a video
analysis pipeline, stage one options are {Yolo5l,Yolo5n} and
stage 2 are {ResNet18,ResNet152}. In low loads (a), choos-
ing more accurate models Yolo5l and ResNet152 with small
batch sizes is preferable to ensure low latency. However, in
higher loads, it is preferable to choose lightweight models
like Yolo5n and ResNet18 with more replication and larger
batch sizes to ensure high throughput for the system."
IMPLEMENTATION/METHODS,0.1789709172259508,"4
Problem Formulation"
IMPLEMENTATION/METHODS,0.18120805369127516,"We now present the details of the optimizer discussed in Sec-
tion 3. The problem formulation needs to have a robust
definition of inference pipeline accuracy and also offline la-
tency profiles of the model variants. Subsections 4.1 and 4.2
discuss details of the inference pipeline accuracy definition
and profiling methodology respectively."
IMPLEMENTATION/METHODS,0.18344519015659955,"4.1
Accuracy Definition over Pipeline"
IMPLEMENTATION/METHODS,0.18568232662192394,"To the best of our knowledge, there is no direct way to com-
pute the end-to-end accuracy of a pipeline unless one trains"
IMPLEMENTATION/METHODS,0.18791946308724833,"the labeled data on the pipeline. In this work, we used a
heuristic for ranking the inference pipelines. In this work, we
have only considered linear inference pipelines with one input
and one output stage where a set of consecutive models are
connected. The accuracy of each model is computed offline
and is part of the property of the model. Statically computed
accuracy is sufficient since in this work we do not consider
model drifts [53]. For models with a qualitative performance
measure other than accuracy (e.g., mAP for object detection.
WER for Speech recognition tasks, and ROUGE for NLP
tasks), as long as we have the higher (or lower) means better
specification in the measure, we can substitute the accuracy
with that measure. For defining a metric over the accuracy in
a pipeline, we first sort the accuracy of each stage’s model
variants from lowest to highest. Then, we assign a zero scale
to the least accurate model and one to the most accurate model
(normalizing the accuracy). Intermediate model variants are
assigned scaled accuracy values between zero and one, pro-
portionally aligned with their rankings in the ordered list. For
example, if three model variants exist, the models’ scaled
accuracy is assigned 0, 0.5, and 1. The overall accuracy over
the pipeline is considered as the sum of each stage’s model
variants’ scaled accuracy value. For example, a two-stage
pipeline with three model variants per stage and the second
most accurate model chosen in each pipeline will have an
end-to-end accuracy rank of 0.5 + 0.5 = 1."
IMPLEMENTATION/METHODS,0.19015659955257272,"4.2
Profiler"
IMPLEMENTATION/METHODS,0.19239373601789708,"The joint cost-accuracy problem formulation needs informa-
tion about the latency, accuracy, and throughput of each model
variant. Previous works have discovered that [35,41,59,60]
the latency of a model under a specific resource allocation is
predictable based on the incoming batch sizes. The profiler
will record the latency on the target hardware for different
batch sizes under specific allocations of resources. We found
out in our experiments that assigning memory beyond a cer-
tain value to the models does not have an impact on the
performance; therefore, for memory, we only need to find just
enough memory allocation. This will be the memory require-
ment for running the largest batch size. It is necessary to find
a minimum allocation to containers since we have chosen hor-
izontal scaling for workload adaptation in this work. Previous
works on inference graphs with CPU evaluation [47,59] have
assigned one core to each container. Adapting a similar ap-
proach is not practical in our case as more resource-intensive
models cannot execute inference under given latency require-
ments with one core per container. Therefore, We find a base
configuration for each model variant in terms of the number
of cores that can provide a reasonable base performance. The
solver selects model variants and horizontally scales them
with the chosen base configurations.
Table 4 provides some of the base CPU allocations under
thresholds 5 RPS, 10 RPS, and 15 RPS for two model variants"
IMPLEMENTATION/METHODS,0.19463087248322147,"Submitted to the Journal of Systems Research (JSys)
2023"
IMPLEMENTATION/METHODS,0.19686800894854586,"Table 4: Sample CPU cores base allocation for different Yolo
variants under different RPS thresholds (Capped on maximum
32 cores)"
IMPLEMENTATION/METHODS,0.19910514541387025,"load
Yolov5n
Yolov5s
Yolo5m
Yolov5l
Yolo5x"
IMPLEMENTATION/METHODS,0.20134228187919462,"5
1
1
4
8
16
10
1
2
8
16
✕
15
1
8
16
32
✕"
IMPLEMENTATION/METHODS,0.203579418344519,"of the Object Detector task. Values in the allocation columns
show the minimum number of CPU allocations per container
needed for being responsive to a certain load under a certain
SLA. We refer to these values as the base resource allocation
to a model variant. The base resource allocation for all of the
stages (∀s ∈S) and their corresponding model variants (∀m ∈
Ms) is the minimum number of resources in terms of CPU
cores (Eq. 1a) that can respond to a certain threshold (Eq. 1b)
and be responsive to a predefined per-model base latency SLA
for the largest batch size in our system (Eq. 1c). Following
previous works [34,59], we define the per-task latency SLA
as the average latency of all available variants for the task
for serving batch size one under the base resource allocation
multiplied by 5 as suggested by Swayam [34]. Therefore,
the minimum number of resources per model variant can be
formulated as:"
IMPLEMENTATION/METHODS,0.2058165548098434,"minRm
(1a)"
IMPLEMENTATION/METHODS,0.2080536912751678,"subject to th ≤h(m,Rm)
(1b)"
IMPLEMENTATION/METHODS,0.21029082774049218,"lm(max(bs)) ≤SLAs
(1c)"
IMPLEMENTATION/METHODS,0.21252796420581654,"Where threshold th is fixed, and the base resource alloca-
tion for all models can be found statistically. In the same
Object Detector task with different model variants, as shown
in Table 4, we choose the first configuration row as it supports
the highest RPS with the minimum resource allocation per
container. As a consequence, the base resource allocation is
fixed during the runtime, and the throughput of each stage of
the pipeline h(m,Rm) will be a function of the used model
variant h(m) and the number of replicas ns."
IMPLEMENTATION/METHODS,0.21476510067114093,"We follow the practice of the profiler in [60] and record
latency and throughput on the power of two increments of 1 to
64 batch sizes. Profiling per all batch sizes is costly, therefore
following [59] for each model variant, we fit the observed re-
sults on the profiled batch sizes under the base resource alloca-
tion to a quadratic polynomial function lm(bs) = αb2
s +βbs+γ
that can infer the latency for unmeasured batch sizes. Mul-
tiple model variants are available per stage of the inference
pipelines, and the mentioned approach can decrease the pro-
filing cost by an order of magnitude."
IMPLEMENTATION/METHODS,0.21700223713646533,Table 5: Notations
IMPLEMENTATION/METHODS,0.21923937360178972,"Symbol
Description"
IMPLEMENTATION/METHODS,0.2214765100671141,"P
Inference pipeline
s ∈P
Inference pipeline stage
SLAP
Latency service-level agreement for pipeline P
λP
Request arrival rate of pipeline P
Ms
Sets of available model variants for stage s
m
A model variant
bs
Batch size of stage s
Rm
Resource allocation of variant m
am
Accuracy rank of variant m
ns
Number of replicas of stage s
Is,m
Indicator of activeness of variant m in stage s
qs(bs)
Queuing time of stage s under batch size b
ls,m(bs)
Latency of variant m under batch size bs
hs,m(bs)
Throughput of variant m under batch size bs
AP
End to End accuracy of pipeline P
th
Threshold RPS of base allocation of Rm
α
Accuracy objective weight
β
Resource allocation objective weight
δ
Penalty term for batching"
IMPLEMENTATION/METHODS,0.22371364653243847,"4.3
Optimization Formulation"
IMPLEMENTATION/METHODS,0.22595078299776286,"The goal of IPA is to minimize the accuracy and maximize
the cost while guaranteeing SLAs."
IMPLEMENTATION/METHODS,0.22818791946308725,Objectives =
IMPLEMENTATION/METHODS,0.23042505592841164,"(
Maximizing the Accuracy
Minimizing the Cost
(2)"
IMPLEMENTATION/METHODS,0.232662192393736,"There are |Ms| model variants for each inference stage s ∈P
in the pipeline P. Each m ∈Ms is a different model variant
for doing the same inference tasks (e.g., image classification)
that exhibit different resource requirements, latency, through-
put, and accuracy. Resource requirements of the models are
estimated offline in the profiling step (section 4.2). The pro-
filer provides us with the resource requirements of the model
variants per each task and their latencies under different batch
sizes. The two main goals of IPA are to maximize the accu-
racy over pipelines and minimize the resource cost by using
lighter models. We define Is,m as an indicator of whether a
selected model variant is currently active for task s or not:"
IMPLEMENTATION/METHODS,0.2348993288590604,"Is,m ="
IMPLEMENTATION/METHODS,0.2371364653243848,"(
1
if m is active in stage s
0
Otherwise
(3)"
IMPLEMENTATION/METHODS,0.23937360178970918,"At each point in time, only one model variant m can be
active for each inference stage; therefore, the resource require-
ment of each replica of models is equal to the active variant
resource requirement."
IMPLEMENTATION/METHODS,0.24161073825503357,"Rs = ∑
m∈Ms
Rs,m.Is,m
(4)"
IMPLEMENTATION/METHODS,0.24384787472035793,"Submitted to the Journal of Systems Research (JSys)
2023"
IMPLEMENTATION/METHODS,0.24608501118568232,"Similarly, the latency and throughput of each pipeline stage
are calculated based on the active model variant latency and
throughput for that stage."
IMPLEMENTATION/METHODS,0.2483221476510067,"ls = ∑
m∈Ms
ls,m(bs).Is,m
(5)"
IMPLEMENTATION/METHODS,0.2505592841163311,"hs = ∑
m∈Ms
hs,m(bs).Is,m
(6)"
IMPLEMENTATION/METHODS,0.25279642058165547,"Another contributing factor to the pipeline’s end-to-end
latency is the time spent on the queue of each inference stage.
For queue modeling, we have used the theoretical upper bound
formulation introduced in [59]:"
IMPLEMENTATION/METHODS,0.2550335570469799,"qs(bs) = bs −1 λ
(7)"
IMPLEMENTATION/METHODS,0.25727069351230425,"Equation 7 illustrates the worst-case queuing delay based on
the arrival rate and the batch size. The first arrived request in
a batch should wait for bs −1 additional request before being
sent to the models."
IMPLEMENTATION/METHODS,0.2595078299776286,"The multi-objective goal (8) is to maximize the pipeline’s
end-to-end accuracy and minimize the cost. Based on the
definition described in section 4.1, the pipeline’s end-to-end
accuracy is achieved by summing each stage’s active model
normalized accuracy. The cost objective is achieved in two
ways, using smaller models (models with fewer resource re-
quirements) and using the least number of replicas for them.
The batch size for each model should be chosen carefully as
larger batch sizes will increase utilization and throughput of
the entire load but also increase the per batch latency. We
should find a batch that increases the utilization at a reason-
able scale. Following [59], we have added a small penalty
term for batch size in the objective function δ that tries to
keep the batch sizes at a reasonable amount."
IMPLEMENTATION/METHODS,0.26174496644295303,"f(n,s,I) = α∑
s∈P
( ∑
m∈Ms
as,m.Is,m)"
IMPLEMENTATION/METHODS,0.2639821029082774,"−β∑
s∈P
ns.Rs"
IMPLEMENTATION/METHODS,0.2662192393736018,"−δ∑
s∈P
bs (8)"
IMPLEMENTATION/METHODS,0.2684563758389262,"The two α and β variables adjust the preference level given
to each objective. We now can describe the auto-configuration
of the three online configuration knobs explained in section
2.2 as an IP problem:"
IMPLEMENTATION/METHODS,0.27069351230425054,"max
f(n,s,I)
(9a)"
IMPLEMENTATION/METHODS,0.27293064876957496,"subject to
∑
s∈P
ls(bs)+qs(bs) ≤SLAP,
(9b)"
IMPLEMENTATION/METHODS,0.2751677852348993,"if Is,m = 1, then"
IMPLEMENTATION/METHODS,0.27740492170022374,"ns ·hs(bs) ≥λp,
∀s ∈P
(9c)
∑
m∈Ms
Is,m = 1,
∀s ∈P
(9d)"
IMPLEMENTATION/METHODS,0.2796420581655481,"ns,bs ∈Z+,
Is,m ∈{0,1},
∀s ∈S,∀m ∈Ms
(9e)"
IMPLEMENTATION/METHODS,0.28187919463087246,"The chosen combination of models should be able to meet
the latency (9b) constraint of the pipeline. The pipeline SLA
of the pipeline is the aggregation of per stage SLA as described
in Section 4.2. End-to-end latency of the pipeline is obtained
by summing the inference latency of the chosen model variant
ls(bs) and queuing time of each model server qs(bs) in the
inference path. Also, the sum of the throughput of all replicas
of an active model should be higher than the incoming arrival
rate 9c into the pipeline.
In summary, the objective function tries to find the most ac-
curate combination of models in the inference pipeline while
also trying to allocate the least number of physical resources
based on the pipeline designer’s preference. This trade-off
between the two objectives is configurable by modifying the
α and β weights for accuracy and resource allocation.
The inputs of the optimization formulation are, therefore,
resource requirements R, latency l, accuracy of all models a,
and throughputs h of all model variants m in all stages s under
all possible batch sizes b alongside queuing latency model
q under all batch sizes, the incoming load λ coming to the
pipeline and pipeline’s latency SLA. In the output, it returns
the optimal number of replicas n, batch size b, and the chosen
model variant I per each stage of the pipeline."
IMPLEMENTATION/METHODS,0.2841163310961969,"4.4
Gurobi Solver"
IMPLEMENTATION/METHODS,0.28635346756152125,"IPA has been designed to meet production-level requirements,
(1) guaranteeing an optimal solution, (2) no need for addi-
tional pre-training or re-training costs, and (3) negligible over-
head. Most optimization techniques can be classified into
heuristics, IP, or ML approaches. Heuristics are ad-hoc so-
lutions that are hard to generalize to different systems. ML
approaches do not guarantee the optimal solution and typi-
cally incur long training times (e.g. Reinforcement Learning).
In contrast, IP formulation meets all the requirements for a
production-ready system. In our case, we chose the Gurobi
solver [19] that guarantees the optimal solution; the only
downside of using them is that in case of very large search
spaces, they might take a long time to find the desirable con-
figuration. In our case, Gurobi was able to solve the problem
formulation in Formula 9 in less than a second."
IMPLEMENTATION/METHODS,0.28859060402684567,"Submitted to the Journal of Systems Research (JSys)
2023"
IMPLEMENTATION/METHODS,0.29082774049217003,"Object
Detector"
IMPLEMENTATION/METHODS,0.2930648769574944,"Object
Classifier"
IMPLEMENTATION/METHODS,0.2953020134228188,(a) Video Monitoring
IMPLEMENTATION/METHODS,0.2975391498881432,"Language
Identification"
IMPLEMENTATION/METHODS,0.29977628635346754,"Neural
Machine
Translation"
IMPLEMENTATION/METHODS,0.30201342281879195,"Text
Summariser"
IMPLEMENTATION/METHODS,0.3042505592841163,(e) Natural Language Processing
IMPLEMENTATION/METHODS,0.30648769574944074,(c) Audio Sentiment Analysis
IMPLEMENTATION/METHODS,0.3087248322147651,"Audio to Text
Sentiment"
IMPLEMENTATION/METHODS,0.31096196868008946,Analysis
IMPLEMENTATION/METHODS,0.3131991051454139,"Audio to Text
Question
Answering"
IMPLEMENTATION/METHODS,0.31543624161073824,(b) Audio Question Answering
IMPLEMENTATION/METHODS,0.31767337807606266,"Text
Summariser"
IMPLEMENTATION/METHODS,0.319910514541387,"Question
Answering"
IMPLEMENTATION/METHODS,0.3221476510067114,(d) Summarisation Question Answering
IMPLEMENTATION/METHODS,0.3243847874720358,Audio stages
IMPLEMENTATION/METHODS,0.32662192393736017,NLP stages
IMPLEMENTATION/METHODS,0.3288590604026846,Video stages
IMPLEMENTATION/METHODS,0.33109619686800895,Figure 6: Representative pipelines used in this work
IMPLEMENTATION/METHODS,0.3333333333333333,"4.5
Dropping"
IMPLEMENTATION/METHODS,0.33557046979865773,"High workloads may cause heavy back pressure at the up-
stream queues. If a request has already passed its SLA at any
stage in the inference pipeline, then there might be no point
in continuing to serve it until the last stage and incurring high
pressure on the system. One mechanism we have employed is
to drop a request at any stage of the pipeline if it has already
passed its SLA in the previous steps. We also consider that a
request is dropped if its current latency has exceeded 2× the
SLA to avoid constant back pressure on the queues."
RESULTS/EXPERIMENTS,0.3378076062639821,"5
Evaluation"
RESULTS/EXPERIMENTS,0.3400447427293065,"In this section, we conduct extensive experiments to show-
case the practical efficacy of IPA in real-world scenarios us-
ing a diverse set of workloads. We evaluate IPA using six
physical machines from Chameleon Cloud [48]. Each server
is equipped with 96 Intel(R) Xeon(R) Gold 6240R CPU @
2.40GHz cores and 188 Gb of RAM. The IPA is open-sourced
at Link-obscured-for-double-blind."
RESULTS/EXPERIMENTS,0.3422818791946309,"5.1
Experimental Setup"
RESULTS/EXPERIMENTS,0.34451901565995524,"Most previous works on inference pipelines [18,25,58,61]
have implemented the entire pipeline on their self-made in-
frastructures. We have implemented our frameworks on top of
Kubernetes, the de facto standard in the containerized world
and widely used in industry. This will enable easier access"
RESULTS/EXPERIMENTS,0.34675615212527966,"0
500
1000
0 20"
RESULTS/EXPERIMENTS,0.348993288590604,"40
Bursty"
RESULTS/EXPERIMENTS,0.3512304250559284,"0
500
1000
0 20"
RESULTS/EXPERIMENTS,0.3534675615212528,"40
Steady Low"
RESULTS/EXPERIMENTS,0.35570469798657717,"0
500
1000
0 20"
RESULTS/EXPERIMENTS,0.3579418344519016,"40
Steady High"
RESULTS/EXPERIMENTS,0.36017897091722595,"0
500
1000
0 20"
RESULTS/EXPERIMENTS,0.3624161073825503,"40
Fluctuating"
RESULTS/EXPERIMENTS,0.36465324384787473,"Real
LSTM"
RESULTS/EXPERIMENTS,0.3668903803131991,Time (s)
RESULTS/EXPERIMENTS,0.3691275167785235,Workload (RPS)
RESULTS/EXPERIMENTS,0.3713646532438479,"Figure 7: Representative tested load patterns from the Twitter
trace [15], showing LSTM predictions"
RESULTS/EXPERIMENTS,0.37360178970917224,"Table 6: Per stage and End to end service level agreements of
inference pipelines (in seconds)"
RESULTS/EXPERIMENTS,0.37583892617449666,"Pipelines
Stage 1
Stage 2
Stage 3
E2E"
RESULTS/EXPERIMENTS,0.378076062639821,"Video Monitoring
4.62
2.27
✕
6.89
Audio QA
8.34
0.89
✕
9.23
Audio Sentiment
8.34
1.08
✕
9.42
Sum QA
2.52
1.32
✕
3.84
NLP
0.97
12.76
3.87
17.61"
RESULTS/EXPERIMENTS,0.38031319910514544,"to the framework for future use by developers. IPA is im-
plemented in Python with over 8K lines of code, including
the adapter, simulator, queuing, load balancer, and model
container implementations.
Pipelines We use five descriptive pipelines with a wide
variety of models for each stage as shown in Figure 6. The
pipelines are adapted from previous works and also from in-
dustrial examples. Video monitoring pipeline (pipeline a) is
a commonly used pipeline in previous works [25, 74] and
industry [23] which an object detector sends the cropped im-
ages to a later model for doing classification tasks like license
plate detection or human recognition. Audio and question
answering/sentiment analysis pipelines (pipelines b and c)
are adapted from use cases composing multiple ML model
types [28]. NLP pipelines (pipeline d and e) are representative
examples of emerging use cases of language models [71,72].
For full specification of the used models in each stage of the
pipelines, refer to Appendix 8.
Baselines We compare IPA against variations of two similar
systems, namely FA2 [59] and RIM [42]. FA2 is a recent sys-
tem that achieves cost efficiency using scaling and batching,
however, compared to IPA it does not have model switching
as an optimization angle. RIM, on the other hand, does not
have scaling as a configuration knob but uses model switching
for adapting to dynamic workloads. The original RIM does
not include batching; To have a fair comparison, we also add
batching to RIM. As RIM does not support scaling therefore
we statically set the scaling of each stage of the inference"
RESULTS/EXPERIMENTS,0.3825503355704698,"Submitted to the Journal of Systems Research (JSys)
2023 10 20 Cost"
RESULTS/EXPERIMENTS,0.38478747203579416,(cores)
RESULTS/EXPERIMENTS,0.3870246085011186,Bursty
RESULTS/EXPERIMENTS,0.38926174496644295,"0
50
100
0 1"
RESULTS/EXPERIMENTS,0.39149888143176736,Accuracy 5 10
RESULTS/EXPERIMENTS,0.39373601789709173,Steady Low
RESULTS/EXPERIMENTS,0.3959731543624161,"0
50
100
0 1 10 20 Cost"
RESULTS/EXPERIMENTS,0.3982102908277405,(cores)
RESULTS/EXPERIMENTS,0.4004474272930649,Steady High
RESULTS/EXPERIMENTS,0.40268456375838924,"0
50
100
Time (s) 0 1"
RESULTS/EXPERIMENTS,0.40492170022371365,Accuracy 10 20
RESULTS/EXPERIMENTS,0.407158836689038,Fluctuating
RESULTS/EXPERIMENTS,0.40939597315436244,"0
50
100
Time (s) 0 1"
RESULTS/EXPERIMENTS,0.4116331096196868,"IPA
FA2-low
FA2-high
RIM"
RESULTS/EXPERIMENTS,0.41387024608501116,(a) Temporal analysis under different workloads 0 2 4 6
RESULTS/EXPERIMENTS,0.4161073825503356,Cost (cores) 0.0 0.5 1.0
RESULTS/EXPERIMENTS,0.41834451901565994,Accuracy 0 1 2 3
RESULTS/EXPERIMENTS,0.42058165548098436,SLA Violations (%)
RESULTS/EXPERIMENTS,0.4228187919463087,"IPA
FA2-low
FA2-high
RIM"
RESULTS/EXPERIMENTS,0.4250559284116331,(b) Average analysis on bursty workload
RESULTS/EXPERIMENTS,0.4272930648769575,Figure 8: Performance analysis of the Video pipeline
RESULTS/EXPERIMENTS,0.42953020134228187,"pipeline to a high value. Similarly, FA2 does not support
model switching, therefore we use two versions of it, one
FA2-low which sets the model variants to the lightest mod-
els, and FA2-high which sets the model variants to a heavy
combination of models on each stage 1. All three compared
systems benefit from the LSTM predictor that was explained
in Section 3.
Workload Figure 7 shows excerpts from Twitter trace [15]
that have been used for evaluating the performance of IPA
against four parts of the dataset. It includes bursty, fluctu-
ating, steady low, and steady high types of workloads. The
LSTM predictor is able to predict the workload with a Sym-
metric Mean Absolute Percentage Error (SMAPE) [39] of
6.6% that is comparable to predictors used in systems with
similar context [78]. Furthermore, an asynchronous load
tester was implemented to emulate the behavior of users in
real-world data-center.
SLA Table 6 shows the pipeline SLA of each inference
pipeline that is calculated by summing the per stage SLAs
heuristic that is explained in Section 4.2."
RESULTS/EXPERIMENTS,0.4317673378076063,"1Ideally we should have set the FA2-high to the heaviest models but
due to resource limitations, we set it to models that on average give better
accuracy compared to IPA 0 50 Cost"
RESULTS/EXPERIMENTS,0.43400447427293065,(cores)
RESULTS/EXPERIMENTS,0.436241610738255,Bursty
RESULTS/EXPERIMENTS,0.43847874720357943,"0
50
100
0 1"
RESULTS/EXPERIMENTS,0.4407158836689038,Accuracy 10 20
RESULTS/EXPERIMENTS,0.4429530201342282,Steady Low
RESULTS/EXPERIMENTS,0.4451901565995526,"0
50
100
0 1 0 50 Cost"
RESULTS/EXPERIMENTS,0.44742729306487694,(cores)
RESULTS/EXPERIMENTS,0.44966442953020136,Steady High
RESULTS/EXPERIMENTS,0.4519015659955257,"0
50
100
Time (s) 0 1"
RESULTS/EXPERIMENTS,0.4541387024608501,Accuracy 0 50
RESULTS/EXPERIMENTS,0.4563758389261745,Fluctuating
RESULTS/EXPERIMENTS,0.45861297539149887,"0
50
100
Time (s) 0 1"
RESULTS/EXPERIMENTS,0.4608501118568233,"IPA
FA2-low
FA2-high
RIM"
RESULTS/EXPERIMENTS,0.46308724832214765,(a) Temporal analysis under different workloads 0 20 40
RESULTS/EXPERIMENTS,0.465324384787472,Cost (cores) 0.0 0.5 1.0 1.5
RESULTS/EXPERIMENTS,0.46756152125279643,Accuracy 0 2 4 6
RESULTS/EXPERIMENTS,0.4697986577181208,SLA Violations (%)
RESULTS/EXPERIMENTS,0.4720357941834452,"IPA
FA2-low
FA2-high
RIM"
RESULTS/EXPERIMENTS,0.4742729306487696,(b) Average analysis on bursty workload
RESULTS/EXPERIMENTS,0.47651006711409394,Figure 9: Performance analysis of the Audio-qa pipeline
RESULTS/EXPERIMENTS,0.47874720357941836,"5.2
End-to-End Evaluation"
RESULTS/EXPERIMENTS,0.4809843400447427,"Figure 8 shows the evaluation results for the video pipeline,
on all four bursty, steady high, steady low and fluctuating
workloads. Since FA2-high and FA2-low are always set to the
lightest and heaviest variants they will always provide lowest
and highest possible accuracies despite the load fluctuations.
In the three bursty, steady low and fluctuating workloads IPA
can always achieves a trade-off between the cost objective,
in steady high workload IPA diverge to a configuration that
uses the lowest cost model variants in order to adapt to the
high resource demands of the steady high workload. The
only available adaptation mechanism for RIM is changing the
models, therefore under load variations in bursty and fluctu-
ating workload it trades off accuracy for being responsive to
the load bursts. As expected FA2-low and FA2-high have the
highest and lowest SLA attainment. In video pipeline, IPA
provides the same resource efficiency as FA2-low since the
base allocation (explained in Section 4) of variants used for
the first stage in video pipelines are similar in most cases and
changing the model in favor of latency reduction does not
result in higher computational costs. In total, IPA is able to
show a better balance between the two cost and accuracy ob-
jectives. While both FA2-high and RIM provides the highest"
RESULTS/EXPERIMENTS,0.48322147651006714,"Submitted to the Journal of Systems Research (JSys)
2023 0 50 Cost"
RESULTS/EXPERIMENTS,0.4854586129753915,(cores)
RESULTS/EXPERIMENTS,0.48769574944071586,Bursty
RESULTS/EXPERIMENTS,0.4899328859060403,"0
50
100
0 1"
RESULTS/EXPERIMENTS,0.49217002237136465,Accuracy 10 20
RESULTS/EXPERIMENTS,0.49440715883668906,Steady Low
RESULTS/EXPERIMENTS,0.4966442953020134,"0
50
100
0 1 0 100 Cost"
RESULTS/EXPERIMENTS,0.4988814317673378,(cores)
RESULTS/EXPERIMENTS,0.5011185682326622,Steady High
RESULTS/EXPERIMENTS,0.5033557046979866,"0
50
100
Time (s) 0 1"
RESULTS/EXPERIMENTS,0.5055928411633109,Accuracy 0 50
RESULTS/EXPERIMENTS,0.5078299776286354,Fluctuating
RESULTS/EXPERIMENTS,0.5100671140939598,"0
50
100
Time (s) 0 1"
RESULTS/EXPERIMENTS,0.5123042505592841,"IPA
FA2-low
FA2-high
RIM"
RESULTS/EXPERIMENTS,0.5145413870246085,(a) Temporal analysis under different workloads 0 20 40
RESULTS/EXPERIMENTS,0.5167785234899329,Cost (cores) 0.0 0.5 1.0 1.5
RESULTS/EXPERIMENTS,0.5190156599552572,Accuracy 0 5 10
RESULTS/EXPERIMENTS,0.5212527964205816,SLA Violations (%)
RESULTS/EXPERIMENTS,0.5234899328859061,"IPA
FA2-low
FA2-high
RIM"
RESULTS/EXPERIMENTS,0.5257270693512305,(b) Average analysis on bursty workload
RESULTS/EXPERIMENTS,0.5279642058165548,Figure 10: Performance analysis of the Audio-sent pipeline
RESULTS/EXPERIMENTS,0.5302013422818792,"accuracies, their cost efficiency is compromised as a result of
employing more accurate variants per stage and a high scaling
factor. FA2-low is able to be responsive to SLA requirements
and achieving the same cost efficiency as IPA but it is unable
to improve the accuracy as it is fixed on the lightest vari-
ants. The higher violation rate in FA2-high, RIM, and IPA
compared to FA2-low is due to the reason that the SLAs are
defined using the processing latency of average models and
the SLAs become tighter for more accurate models, resulting
in a higher tail latency violation. Figure 9a and Figure10a
show the same temporal and average results on the audio-qa
and audio-sent inference pipelines. Due to lower number of
used variants in these two pipelines (5×5 = 25 for video and
5×2 and 5×3 on audio-qa and audio-sent pipelines) we ob-
serve less fluctuations in RIM in all the workloads. However,
similar to video pipeline, IPA was able to achieve a trade-off
between the two accuracy and cost objectives.
Compared to the stages used in the three mentioned
pipelines, base allocations for the summarization stage used
in the sum-qa and NLP pipelines provides a larger span of
changes in terms of required CPU cores (see Appendix 8).
For example, the resource difference between the heaviest
and lightest model in the Object Detection stage of the video
pipeline is 8−1 = 7, while it is more than doubled in the sum- 0 100 Cost"
RESULTS/EXPERIMENTS,0.5324384787472036,(cores)
RESULTS/EXPERIMENTS,0.5346756152125279,Bursty
RESULTS/EXPERIMENTS,0.5369127516778524,"0
50
100
0 2"
RESULTS/EXPERIMENTS,0.5391498881431768,Accuracy
RESULTS/EXPERIMENTS,0.5413870246085011,"10
20
30"
RESULTS/EXPERIMENTS,0.5436241610738255,Steady Low
RESULTS/EXPERIMENTS,0.5458612975391499,"0
50
100
0 2 0 100 Cost"
RESULTS/EXPERIMENTS,0.5480984340044742,(cores)
RESULTS/EXPERIMENTS,0.5503355704697986,Steady High
RESULTS/EXPERIMENTS,0.5525727069351231,"0
50
100
Time (s) 0 2"
RESULTS/EXPERIMENTS,0.5548098434004475,Accuracy 0 100
RESULTS/EXPERIMENTS,0.5570469798657718,Fluctuating
RESULTS/EXPERIMENTS,0.5592841163310962,"0
50
100
Time (s) 0 2"
RESULTS/EXPERIMENTS,0.5615212527964206,"IPA
FA2-low
FA2-high
RIM"
RESULTS/EXPERIMENTS,0.5637583892617449,(a) Temporal analysis under different workloads 0 20 40 60
RESULTS/EXPERIMENTS,0.5659955257270693,Cost (cores) 0 1 2
RESULTS/EXPERIMENTS,0.5682326621923938,Accuracy 0.0 2.5 5.0 7.5
RESULTS/EXPERIMENTS,0.5704697986577181,SLA Violations (%)
RESULTS/EXPERIMENTS,0.5727069351230425,"IPA
FA2-low
FA2-high
RIM"
RESULTS/EXPERIMENTS,0.5749440715883669,(b) Average analysis on bursty workload
RESULTS/EXPERIMENTS,0.5771812080536913,Figure 11: Performance analysis of the Sum-qa pipeline
RESULTS/EXPERIMENTS,0.5794183445190156,"marization stage (16−1 = 15). Consequently, we observe a
larger span of differences between FA2-low and FA2-high ap-
proaches in these two pipelines. In both of these approaches,
IPA can adapt to the load by using the second least heavy
models that result in 3x and 4x cost reduction with only 0.5
loss in the normalized accuracy measure."
RESULTS/EXPERIMENTS,0.5816554809843401,"5.3
IPA Scalibility"
RESULTS/EXPERIMENTS,0.5838926174496645,"System Scalibility To examine the effectiveness of the IPA
in real-world systems we included the NLP pipeline in our
evaluations which during bursts scales up to 500 cores (Fig-
ure 12a). Due to using production-grade best practices of ML
deployment like using lightweight containers and Kubernetes
as the backend with the benefit of distributed scheduling and
cluster management, we believe IPA has the potential to scale
to large clusters.
Optimizer Scalibility As mentioned in Section 4 we have
used the Gurobi solver to solve the IP optimization problem.
One critique of using Gurobi is its limitations in the solv-
able problem space in the time constraints of a real-world
autoscaler. To guarantee fast autoscaler adaptation to work-
load fluctuations, the autoscaler should be able to find the
next configuration in less than two seconds to leave enough"
RESULTS/EXPERIMENTS,0.5861297539149888,"Submitted to the Journal of Systems Research (JSys)
2023 0 500 Cost"
RESULTS/EXPERIMENTS,0.5883668903803132,(cores)
RESULTS/EXPERIMENTS,0.5906040268456376,Bursty
RESULTS/EXPERIMENTS,0.5928411633109619,"0
50
100
1 2"
RESULTS/EXPERIMENTS,0.5950782997762863,Accuracy 0 100
RESULTS/EXPERIMENTS,0.5973154362416108,Steady Low
RESULTS/EXPERIMENTS,0.5995525727069351,"0
50
100
1 2 0 250 Cost"
RESULTS/EXPERIMENTS,0.6017897091722595,(cores)
RESULTS/EXPERIMENTS,0.6040268456375839,Steady High
RESULTS/EXPERIMENTS,0.6062639821029083,"0
50
100
Time (s) 1 2"
RESULTS/EXPERIMENTS,0.6085011185682326,Accuracy 0 250
RESULTS/EXPERIMENTS,0.610738255033557,Fluctuating
RESULTS/EXPERIMENTS,0.6129753914988815,"0
50
100
Time (s) 1 2"
RESULTS/EXPERIMENTS,0.6152125279642058,"IPA
FA2-low
FA2-high
RIM"
RESULTS/EXPERIMENTS,0.6174496644295302,(a) Temporal analysis under different workloads 0 100 200
RESULTS/EXPERIMENTS,0.6196868008948546,Cost (cores) 0 1 2
RESULTS/EXPERIMENTS,0.6219239373601789,Accuracy 0 2 4 6
RESULTS/EXPERIMENTS,0.6241610738255033,SLA Violations (%)
RESULTS/EXPERIMENTS,0.6263982102908278,"IPA
FA2-low
FA2-high
RIM"
RESULTS/EXPERIMENTS,0.6286353467561522,(b) Average analysis on bursty workload.
RESULTS/EXPERIMENTS,0.6308724832214765,Figure 12: Performance analysis of the NLP pipeline.
RESULTS/EXPERIMENTS,0.6331096196868009,"room for the adaptation process itself which in our experi-
ments were around the same number of 8 seconds sums up
8+2 = 10 which we used as our adaptation monitoring inter-
val. We ran a set of simulated experiments shown in Figure 13
to examine the decision-making time of the IPA growth with
respect to changing the number of available model variants
and the number of tasks in the inference pipelines (length of
the inference graph). IPA is able to find the optimal config-
uration for inference pipelines with 10 stages each with 10
models in less than two seconds. Having an effective deci-
sion time for inference pipelines beyond these sizes demands
faster optimization solutions. However, most of the existing
inference pipelines [57] rarely go beyond 10 stages, therefore
IPA will be effective for real-world use cases."
RESULTS/EXPERIMENTS,0.6353467561521253,"5.4
IPA Adaptability"
RESULTS/EXPERIMENTS,0.6375838926174496,"The main premise of IPA is to provide an adaptable frame-
work for achieving a trade-off between cost and accuracy
objectives by leveraging the three configuration knobs of
model switching, scaling, and batching. Instead of using a
fixed value for α and β in previous experiments, we examined
the effect of changing the given weights to each objective
by modifying the α and β values for each of the inference"
RESULTS/EXPERIMENTS,0.639821029082774,"Figure 13: Decision time of Gurobi optimizer for IPA formu-
lation with respect to the number of models and tasks on the
inference graph."
RESULTS/EXPERIMENTS,0.6420581655480985,# Models
RESULTS/EXPERIMENTS,0.6442953020134228,1 2 3 4 5 6 7 8 9
RESULTS/EXPERIMENTS,0.6465324384787472,# Tasks
RESULTS/EXPERIMENTS,0.6487695749440716,"1
2
3
4
5
6
7 8 9"
RESULTS/EXPERIMENTS,0.6510067114093959,Time (s)
RESULTS/EXPERIMENTS,0.6532438478747203,"0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00"
RESULTS/EXPERIMENTS,0.6554809843400448,"Figure 14: Comparison of IPA accuracy for different trade-
offs between accuracy and cost objectives, IPA can navigate
effectively between the two cost and accuracy objectives."
RESULTS/EXPERIMENTS,0.6577181208053692,"video
audio-qa audio-sent
sum-qa
nlp
Pipelines 0 25 50 75 100"
RESULTS/EXPERIMENTS,0.6599552572706935,Cost (cores)
RESULTS/EXPERIMENTS,0.6621923937360179,"Accuracy-priorotize
Balance
Resource-priorotize 0.5 1.0 1.5 2.0 2.5"
RESULTS/EXPERIMENTS,0.6644295302013423,Accuracy
RESULTS/EXPERIMENTS,0.6666666666666666,"pipelines. Figure 14 shows a set of experiments conducted
on all five pipelines where in one scenario cost optimization
(resource prioritize) is set as the priority by setting the β to
a larger value and in another scenario accuracy is set as the
system priority by using a larger value for α. It is evident that
IPA provides an adaptable approach to optimize different cost
and accuracy preferences by the inference pipeline designer.
For instance, one can choose a highly accuracy adaptation
scenario for the NLP pipeline with 100 CPU cores and aver-
age accuracy of 2.5 or a lower accurate result of 1 with 46
CPU cores.
Figure 15 shows the latency CDF of end-to-end latencies
over the five tested pipelines to further show the flexibility of
IPA in dynamic workloads. IPA leverages its fast adaptation
by using heavy models only when the load is low and achieves
nearly the same latency efficiency as the FA2-low (with higher
accuracy compared to FA2). Only RIM is able to provide"
RESULTS/EXPERIMENTS,0.668903803131991,"Submitted to the Journal of Systems Research (JSys)
2023"
RESULTS/EXPERIMENTS,0.6711409395973155,"0.00
6.89
0.0 0.5 1.0 CDF video"
RESULTS/EXPERIMENTS,0.6733780760626398,"0.00
9.23"
RESULTS/EXPERIMENTS,0.6756152125279642,audio-qa
RESULTS/EXPERIMENTS,0.6778523489932886,"0.00
9.42"
RESULTS/EXPERIMENTS,0.680089485458613,audio-sent
RESULTS/EXPERIMENTS,0.6823266219239373,"0.00
3.84"
RESULTS/EXPERIMENTS,0.6845637583892618,sum-qa
RESULTS/EXPERIMENTS,0.6868008948545862,"0.0
17.6"
RESULTS/EXPERIMENTS,0.6890380313199105,"nlp
IPA
FA2-low
FA2-high
RIM
SLA"
RESULTS/EXPERIMENTS,0.6912751677852349,"Figure 15: End-to-end latency distribution for the five tested inference pipelines under different approaches. IPA is able to
achieve latency close to the FA2-low with light model variants and only RIM is achieving better latency at the expense of high
resource over-provisioning."
RESULTS/EXPERIMENTS,0.6935123042505593,"Figure 16: Effect of using predictor on reducing SLA viola-
tions on bursty workload, IPA LSTM is able to reduce SLA
violations up to 10x with the same resource usage 0 20 40 60"
RESULTS/EXPERIMENTS,0.6957494407158836,Cost (cores)
RESULTS/EXPERIMENTS,0.697986577181208,"video
audio-qa
audio-sent
sum-qa
nlp
Pipelines 0 5 10"
RESULTS/EXPERIMENTS,0.7002237136465325,SLA violations (%)
RESULTS/EXPERIMENTS,0.7024608501118568,"lstm
reactive"
RESULTS/EXPERIMENTS,0.7046979865771812,"better latency compared to IPA but as shown in the previous
examples (e.g., Figure 9b for Audio-qa pipeline) comes at the
expense of high resource allocations (3x compared to IPA in
the same pipeline)."
RESULTS/EXPERIMENTS,0.7069351230425056,"5.5
IPA Predictor"
RESULTS/EXPERIMENTS,0.70917225950783,"Predictors are effective in reducing SLA violations. Most
of the previous works on inference pipeline serving [25,41,
42, 59] have done reactive auto-configuration. In reactive
approaches, configuration changes happen with live monitor-
ing of the load and in response to load changes. [73,78] for
predicting load prior to load changes. IPA uses a proactive
approach by using an LSTM predictor that leverages histori-
cal data. The ablation analysis provided in Figures 16 shows
that using the LSTM predictor is beneficial in reducing SLA
violations in all the pipelines with negligible difference in
resource consumption. The LSTM module is trained in less
than ten minutes for the 14 days of Twitter traces, therefore
using it is practical in real-world scenarios."
LITE REVIEW,0.7114093959731543,"6
Related Works"
LITE REVIEW,0.7136465324384788,"Single stage inference serving: Several approaches in pre-
vious research have been proposed for improving the perfor-
mance metrics without considering multiple stages inference
models [13,26,34,68,73]. They intend to enhance a set of
performance metrics, e.g., latency and throughput, and reduce
resource utilization through adaptive batching, horizontal and
vertical resource scaling, model switching, queue re-ordering,
and efficient scheduling of models in heterogeneous clusters.
A few works have considered the joint optimization of quali-
tative metrics like accuracy and performance for single-stage
inference serving systems. Model Switching [75] proposes
a quality adaptive framework for image processing applica-
tions. It uses switching between models trained for the same
task configuration knob and switches from heavier models
to lighter models in response to the load spikes. However,
the proposed prototype does not consider the interaction of
model switching between other resource configuration knobs
like autoscaling and batching. INFaaS [60] abstracts away
the selection of model variants in the single stage setting from
the user and automatically selects the best-performing model
within the user-defined SLOs. It also actively loads and un-
loads models based on their usage frequency. InfAdapter [63]
and Cocktail [36] propose joint optimization formulations for
maximizing accuracy and minimizing cost with predictive
autoscaling in single stage inference scenarios."
LITE REVIEW,0.7158836689038032,"Multi-stage inference serving: Several approaches in pre-
vious research have been proposed for improving the perfor-
mance metrics for getting inference on multi-stage inference
serving systems [7,25,37,41,42,44,47,51,58,59,61,65,69]
since changing one model’s configuration affects on the sub-
sequent steps. InferLine [25] reduces the end-to-end latency
of ML serving services by heuristically optimizing config-
urations such as batch sizes and horizontal scaling of each
stage. Llama [61] is a use case-specific pipeline configura-
tion system designed exclusively for video inference systems.
It tries to reduce the end-to-end latency by interactively de-
creasing the latency of each stage in the pipeline. Stages of
the pipeline can be either ML inference or non-ML video"
LITE REVIEW,0.7181208053691275,"Submitted to the Journal of Systems Research (JSys)
2023"
LITE REVIEW,0.7203579418344519,"tasks like decoding. GrandSLAm [47] is a system designed
to minimize latency and ensure compliance with service level
agreement (SLA) requirements in the context of a chain of
microservices dedicated to mainly machine learning (ML)
tasks. The system achieves this by dynamically reordering
incoming requests, prioritizing those with minimal computa-
tional overhead, and batching them to maximize each stage’s
throughput. FA2 [59] papooses a graph transformation and
dynamic programming solution on inference pipelines with
shared models. The graph transformation part breaks the exe-
cution graph to make it solvable in real-time, and the dynamic
programming solution returns the optimal batch size and scal-
ing factor per each DNN stage of the pipeline. Multi-model
and Multi-task inference VR/AR/Metaverse pipelines [17,50]
are other emerging use cases of inference pipelines. However,
unlike IPA, none of the above approaches consider all three
pillars of accuracy, cost, and end-to-end latency/throughput
jointly for multi-stage inference serving systems."
CONCLUSION AND FUTURE WORKS,0.7225950782997763,"7
Conclusion and Future Works"
CONCLUSION/DISCUSSION,0.7248322147651006,"In this work, we introduced IPA, an online auto-configuration
system for jointly improving the resource cost and accuracy
over inference pipelines. IPA uses a combination of of-
fline profiling with online optimization to find the appropriate
model variant, replication factor, and batch sizes for each
step of the inference pipeline. Real-world implementation
of IPA and experiments using real-world traces showed that
it could preserve the same cost efficiency and SLA agree-
ment while also having normalized accuracy improvement up
to 35% over two compared approaches. The followings are
some directions for future works:
Scalability IPA leverages Gurobi solver for finding the suit-
able configurations. This worked fine in our problem setting
as a limited number of model variants were used in each step
of the pipeline. However, one interesting future direction
for IPA is to examine its performance where more model
variants are available for each step of the pipeline and also
cases where we have more complicated larger graphs [7]. The
adapter needs to be able to respond to bursts in less than one
second, which demands either designing new heuristic meth-
ods that can find a good enough but not necessarily optimal
solution or data-driven solutions like some of the methods
that have been used before in similar auto-configuration con-
text like Bayesian Optimization [14], Reinforcement [68]
Learning or Causal Methods [43].
Emerging machine learning deployment paradigms Multi
model serving [12] enables more efficient usage of GPUs.
This feature enables loading several models simultaneously
on the same server instead of spinning up one microservice
per each model like IPA. While the focus of IPA was on CPU
serving, using it on GPUs and containerized platforms is not
straightforward. To our knowledge, there isn’t any built-in
mechanism for sharing GPU on mainstream container orches-"
CONCLUSION/DISCUSSION,0.727069351230425,"tration frameworks like Kubernetes. Making IPA formulation
consistent with GPU sharing and also considering interfer-
ence between multiple models in the scheduler [55] as part of
the IPA is a potential future extension."
REFERENCES,0.7293064876957495,References
REFERENCES,0.7315436241610739,"[1] AI Techniques in Medical Imaging May Lead to False
Positives and False Negatives.
https://tinyurl.
com/628z9tn4. Published on May 12, 2020."
REFERENCES,0.7337807606263982,[2] GRPC. URL: https://grpc.io/.
REFERENCES,0.7360178970917226,[3] Istio. URL: https://istio.io/.
REFERENCES,0.738255033557047,"[4] Kubernetes Python client.
https://github.com/
kubernetes-client/python."
REFERENCES,0.7404921700223713,[5] MinIO. URL: https://min.io/.
REFERENCES,0.7427293064876958,"[6] MLServer. URL: https://github.com/SeldonIO/
MLServer."
REFERENCES,0.7449664429530202,"[7] Nvidia DeepStream.
URL: https://developer.
nvidia.com/deepstream-sdk."
REFERENCES,0.7472035794183445,[8] prometheus. URL: https://prometheus.io/.
REFERENCES,0.7494407158836689,"[9] Reduce
False
Positives
with Machine
Learning.
https://complyadvantage.com/insights/
reduce-false-positives-with-machine-learning/.
Accessed on July 27, 2023."
REFERENCES,0.7516778523489933,"[10] Seldon core. URL: https://github.com/SeldonIO/
seldon-core."
REFERENCES,0.7539149888143176,"[11] Ultralytics yolov5.
URL: https://github.com/
ultralytics/yolov5."
REFERENCES,0.756152125279642,"[12] Sherif Akoush, Andrei Paleyes, Arnaud Van Looveren,
and Clive Cox. Desiderata for next generation of ML
model serving. arXiv preprint arXiv:2210.14665, 2022."
REFERENCES,0.7583892617449665,"[13] Ahsan Ali, Riccardo Pinciroli, Feng Yan, and Evgenia
Smirni.
Batch: Machine learning inference serving
on serverless platforms with adaptive batching.
In
SC20: International Conference for High Performance
Computing, Networking, Storage and Analysis, pages
1–15. IEEE, 2020."
REFERENCES,0.7606263982102909,"[14] Omid Alipourfard, Hongqiang Harry Liu, Jianshu Chen,
Shivaram Venkataraman, Minlan Yu, and Ming Zhang.
Cherrypick: Adaptively unearthing the best cloud con-
figurations for big data analytics. In 14th {USENIX}
Symposium on Networked Systems Design and Imple-
mentation ({NSDI} 17), pages 469–482, 2017."
REFERENCES,0.7628635346756152,"Submitted to the Journal of Systems Research (JSys)
2023"
REFERENCES,0.7651006711409396,"[15] archiveteam.
Archiveteam-twitter-stream-
2021-08.
https://archive.org/details/
archiveteam-twitter-stream-2021-08, 2021."
REFERENCES,0.767337807606264,"[16] Jeff Bar.
Amazon EC2 ML inference.
https://
tinyurl.com/5n8yb5ub, Dec 2019."
REFERENCES,0.7695749440715883,"[17] Giovanni Bartolomeo, Simon Bäurle, Nitinder Mohan,
and Jörg Ott. Oakestra: An orchestration framework for
edge computing. In Proceedings of the SIGCOMM’22
Poster and Demo Sessions, pages 34–36. 2022."
REFERENCES,0.7718120805369127,"[18] Vivek M Bhasi, Jashwant Raj Gunasekaran, Prashanth
Thinakaran, Cyan Subhra Mishra, Mahmut Taylan Kan-
demir, and Chita Das.
Kraken: Adaptive container
provisioning for deploying dynamic DAGs in serverless
platforms. In Proceedings of the ACM Symposium on
Cloud Computing, pages 153–167, 2021."
REFERENCES,0.7740492170022372,"[19] Stephen P Boyd and Lieven Vandenberghe.
Convex
optimization. Cambridge university press, 2004."
REFERENCES,0.7762863534675615,"[20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al.
Language models are few-shot learn-
ers. Advances in neural information processing systems,
33:1877–1901, 2020."
REFERENCES,0.7785234899328859,"[21] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang,
and Song Han. Once for all: Train one network and
specialize it for efficient deployment. In International
Conference on Learning Representations, 2020. URL:
https://arxiv.org/pdf/1908.09791.pdf."
REFERENCES,0.7807606263982103,"[22] Seungbeom Choi, Sunho Lee, Yeonjae Kim, Jongse
Park, Youngjin Kwon, and Jaehyuk Huh. Serving het-
erogeneous machine learning models on multi-GPU
servers with spatio-temporal sharing. In 2022 USENIX
Annual Technical Conference (USENIX ATC 22), pages
199–216, 2022."
REFERENCES,0.7829977628635347,"[23] clarifai.
Clarifai.
URL: https://clarifai.com/
clarifai/main/workflows."
REFERENCES,0.785234899328859,"[24] CNBC.
Tesla crash that killed two men.
https://
tinyurl.com/mr25a5mv. Published on April 18, 2021."
REFERENCES,0.7874720357941835,"[25] Daniel Crankshaw, Gur-Eyal Sela, Xiangxi Mo, Corey
Zumar, Ion Stoica, Joseph Gonzalez, and Alexey Tu-
manov.
InferLine: Latency-aware provisioning and
scaling for prediction serving pipelines. In Proceed-
ings of the 11th ACM Symposium on Cloud Computing,
pages 477–491, 2020."
REFERENCES,0.7897091722595079,"[26] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J
Franklin, Joseph E Gonzalez, and Ion Stoica. Clipper:
A low-latency online prediction serving system.
In"
REFERENCES,0.7919463087248322,"14th {USENIX} Symposium on Networked Systems De-
sign and Implementation ({NSDI} 17), pages 613–627,
2017."
REFERENCES,0.7941834451901566,"[27] Tathagata Das, Yuan Zhong, Ion Stoica, and Scott
Shenker. Adaptive stream processing using dynamic
batch sizing. In ACM Symposium on Cloud Computing
(SoCC), pages 1–13, 2014."
REFERENCES,0.796420581655481,"[28] Ruofei Du, Na Li, Jing Jin, Michelle Carney, Scott
Miles, Maria Kleiner, Xiuxiu Yuan, Yinda Zhang,
Anuva Kulkarni, Xingyu Liu, et al.
Rapsai: Accel-
erating machine learning prototyping of multimedia ap-
plications through visual programming. In Proceedings
of the 2023 CHI Conference on Human Factors in Com-
puting Systems, pages 1–23, 2023."
REFERENCES,0.7986577181208053,"[29] Andre Esteva, Alexandre Robicquet, Bharath Ramsun-
dar, Volodymyr Kuleshov, Mark DePristo, Katherine
Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and
Jeff Dean. A guide to deep learning in healthcare. Na-
ture medicine, 25(1):24–29, 2019."
REFERENCES,0.8008948545861297,"[30] Maria Fazio, Antonio Celesti, Rajiv Ranjan, Chang Liu,
Lydia Chen, and Massimo Villari.
Open issues in
scheduling microservices in the cloud.
IEEE Cloud
Computing, 3(5):81–88, 2016."
REFERENCES,0.8031319910514542,"[31] Avrilia Floratou, Ashvin Agrawal, Bill Graham, Sriram
Rao, and Karthik Ramasamy. Dhalion: Self-regulating
stream processing in Heron. Very Large Data Bases
(PVLDB), 10(12):1825–1836, 2017."
REFERENCES,0.8053691275167785,"[32] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao,
Michael W Mahoney, and Kurt Keutzer.
A survey
of quantization methods for efficient neural network
inference. arXiv preprint arXiv:2103.13630, 2021."
REFERENCES,0.8076062639821029,"[33] Alim Ul Gias, Giuliano Casale, and Murray Woodside.
Atom: Model-driven autoscaling for microservices. In
IEEE International Conference on Distributed Comput-
ing Systems (ICDCS), pages 1994–2004, 2019."
REFERENCES,0.8098434004474273,"[34] Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S
McKinley, and Björn B Brandenburg. Swayam: Dis-
tributed autoscaling to meet SLAs of machine learning
inference services with resource efficiency.
In Pro-
ceedings of the 18th ACM/IFIP/USENIX Middleware
Conference, pages 109–120, 2017."
REFERENCES,0.8120805369127517,"[35] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao,
Antoine Kaufmann, Ymir Vigfusson, and Jonathan
Mace.
Serving DNNs like clockwork: Performance
predictability from the bottom up. In 14th USENIX Sym-
posium on Operating Systems Design and Implementa-
tion (OSDI 20), pages 443–462. USENIX Association,
November 2020. URL: https://www.usenix.org/
conference/osdi20/presentation/gujarati."
REFERENCES,0.814317673378076,"Submitted to the Journal of Systems Research (JSys)
2023"
REFERENCES,0.8165548098434005,"[36] Jashwant Raj Gunasekaran, Cyan Subhra Mishra,
Prashanth Thinakaran, Bikash Sharma, Mahmut Tay-
lan Kandemir, and Chita R Das. Cocktail: A multidi-
mensional optimization for model serving in cloud. In
USENIX NSDI, pages 1041–1057, 2022."
REFERENCES,0.8187919463087249,"[37] Udit Gupta, Samuel Hsia, Jeff Zhang, Mark Wilken-
ing, Javin Pombra, Hsien-Hsin Sean Lee, Gu-Yeon Wei,
Carole-Jean Wu, and David Brooks.
RecPipe: Co-
designing models and hardware to jointly optimize rec-
ommendation quality and performance. In MICRO-54:
54th Annual IEEE/ACM International Symposium on
Microarchitecture, pages 870–884, 2021."
REFERENCES,0.8210290827740492,"[38] Harvard Business Review.
When machine learning
goes off the rails.
https://hbr.org/2021/01/
when-machine-learning-goes-off-the-rails,
January 2021."
REFERENCES,0.8232662192393736,"[39] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 770–778, 2016."
REFERENCES,0.825503355704698,"[40] Sepp Hochreiter and Jürgen Schmidhuber. Long short-
term memory. Neural computation, 9(8):1735–1780,
1997."
REFERENCES,0.8277404921700223,"[41] Yitao Hu, Rajrup Ghosh, and Ramesh Govindan.
Scrooge: A cost-effective deep learning inference sys-
tem. In Proceedings of the ACM Symposium on Cloud
Computing, pages 624–638, 2021."
REFERENCES,0.8299776286353467,"[42] Yitao Hu, Weiwu Pang, Xiaochen Liu, Rajrup Ghosh,
Bongjun Ko, Wei-Han Lee, and Ramesh Govindan.
Rim: Offloading inference to the edge. In Proceedings
of the International Conference on Internet-of-Things
Design and Implementation, pages 80–92, 2021."
REFERENCES,0.8322147651006712,"[43] Md Shahriar Iqbal, Rahul Krishna, Mohammad Ali Ja-
vidian, Baishakhi Ray, and Pooyan Jamshidi.
Uni-
corn: Reasoning about configurable system performance
through the lens of causality. In Proceedings of the Sev-
enteenth European Conference on Computer Systems,
pages 199–217, 2022."
REFERENCES,0.8344519015659956,"[44] Junchen Jiang, Ganesh Ananthanarayanan, Peter Bodik,
Siddhartha Sen, and Ion Stoica. Chameleon: Scalable
adaptation of video analytics.
In Proceedings of the
2018 Conference of the ACM Special Interest Group on
Data Communication, pages 253–266, 2018."
REFERENCES,0.8366890380313199,"[45] Maree Johnson, Samuel Lapkin, Vanessa Long, Paula
Sanchez, Hanna Suominen, Jim Basilakis, and Linda
Dawson. A systematic review of speech recognition
technology in health care. BMC medical informatics
and decision making, 14(1):1–14, 2014."
REFERENCES,0.8389261744966443,"[46] Vasiliki Kalavri, John Liagouris, Moritz Hoffmann,
Desislava Dimitrova, Matthew Forshaw, and Timothy
Roscoe.
Three steps is all you need: Fast, accurate,
automatic scaling decisions for distributed streaming
dataflows. In USENIX Symposium on Operating Sys-
tems Design and Implementation (OSDI), pages 783–
798, 2018."
REFERENCES,0.8411633109619687,"[47] Ram Srivatsa Kannan, Lavanya Subramanian, Ashwin
Raju, Jeongseob Ahn, Jason Mars, and Lingjia Tang.
Grandslam: Guaranteeing SLAs for jobs in microser-
vices execution frameworks. In Proceedings of the Four-
teenth EuroSys Conference 2019, pages 1–16, 2019."
REFERENCES,0.843400447427293,"[48] Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau,
Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran,
Haryadi S. Gunawi, Cody Hammock, Joe Mambretti,
Alexander Barnes, François Halbach, Alex Rocha, and
Joe Stubbs.
Lessons learned from the Chameleon
testbed. In Proceedings of the 2020 USENIX Annual
Technical Conference (USENIX ATC ’20). USENIX
Association, July 2020."
REFERENCES,0.8456375838926175,"[49] Darpan Kulshreshtha.
10 Instances Where AI Went
Wrong.
https://tinyurl.com/2p8ywtpd.
Pub-
lished on LinkedIn."
REFERENCES,0.8478747203579419,"[50] Hyoukjun Kwon, Krishnakumar Nair, Jamin Seo, Ja-
son Yik, Debabrata Mohapatra, Dongyuan Zhan, Jinook
Song, Peter Capak, Peizhao Zhang, Peter Vajda, et al.
XRBench: An extended reality (XR) machine learn-
ing benchmark suite for the metaverse. arXiv preprint
arXiv:2211.08675, 2022."
REFERENCES,0.8501118568232662,"[51] Yunseong Lee, Alberto Scolari, Byung-Gon Chun,
Marco Domenico Santambrogio, Markus Weimer, and
Matteo Interlandi. PRETZEL: Opening the black box
of machine learning prediction serving systems.
In
13th {USENIX} Symposium on Operating Systems De-
sign and Implementation ({OSDI} 18), pages 611–626,
2018."
REFERENCES,0.8523489932885906,"[52] Nima Mahmoudi and Hamzeh Khazaei. Performance
modeling of serverless computing platforms.
IEEE
Transactions on Cloud Computing, pages 1–15, 2020."
REFERENCES,0.854586129753915,"[53] Ankur Mallick, Kevin Hsieh, Behnaz Arzani, and
Gauri Joshi.
Matchmaker: Data drift mitigation
in machine learning for large-scale systems.
In
D. Marculescu, Y. Chi, and C. Wu, editors, Proceed-
ings of Machine Learning and Systems, volume 4,
pages 77–94, 2022.
URL: https://proceedings.
mlsys.org/paper_files/paper/2022/file/
1c383cd30b7c298ab50293adfecb7b18-Paper.pdf."
REFERENCES,0.8568232662192393,"[54] Sébastien Marcel and Yann Rodriguez. Torchvision the
machine-vision package of torch. In Proceedings of"
REFERENCES,0.8590604026845637,"Submitted to the Journal of Systems Research (JSys)
2023"
REFERENCES,0.8612975391498882,"the 18th ACM international conference on Multimedia,
pages 1485–1488, 2010."
REFERENCES,0.8635346756152126,"[55] Daniel Mendoza, Francisco Romero, Qian Li, Neeraja J
Yadwadkar, and Christos Kozyrakis. Interference-aware
scheduling for inference serving. In Proceedings of the
1st Workshop on Machine Learning and Systems, pages
80–88, 2021."
REFERENCES,0.8657718120805369,"[56] Maxim
Naumov,
Dheevatsa
Mudigere,
Hao-
Jun Michael Shi, Jianyu Huang, Narayanan Sun-
daraman, Jongsoo Park, Xiaodong Wang, Udit Gupta,
Carole-Jean Wu, Alisson G Azzolini, et al.
Deep
learning
recommendation
model for personaliza-
tion and recommendation systems.
arXiv preprint
arXiv:1906.00091, 2019."
REFERENCES,0.8680089485458613,"[57] Nvidia.
https://docs.nvidia.com/metropolis/
deepstream/dev-guide/text/DS_Zero_Coding_
Sample_Graphs.html#. Deepstream reference graphs."
REFERENCES,0.8702460850111857,"[58] Alex Poms, Will Crichton, Pat Hanrahan, and Kayvon
Fatahalian.
Scanner:
Efficient video analysis
at scale.
ACM Trans.
Graph., 37(4):138:1–
138:13, July 2018.
URL: http://doi.acm.org/
10.1145/3197517.3201394, https://doi.org/10.
1145/3197517.3201394."
REFERENCES,0.87248322147651,"[59] Kamran Razavi, Manisha Luthra, Boris Koldehofe, Max
Mühlhäuser, and Lin Wang. FA2: Fast, accurate au-
toscaling for serving deep learning inference with SLA
guarantees. In 2022 IEEE 28th Real-Time and Embed-
ded Technology and Applications Symposium (RTAS),
pages 146–159. IEEE, 2022."
REFERENCES,0.8747203579418344,"[60] Francisco Romero, Qian Li, Neeraja J Yadwadkar, and
Christos Kozyrakis. INFaaS: Automated model-less
inference serving. In 2021 USENIX Annual Technical
Conference (USENIX ATC 21), pages 397–411, 2021."
REFERENCES,0.8769574944071589,"[61] Francisco Romero, Mark Zhao, Neeraja J Yadwadkar,
and Christos Kozyrakis. Llama: A heterogeneous &
serverless framework for auto-tuning video analytics
pipelines. arXiv preprint arXiv:2102.01887, 2021."
REFERENCES,0.8791946308724832,"[62] Krzysztof Rzadca, Pawel Findeisen, Jacek Swiderski,
Przemyslaw Zych, Przemyslaw Broniek, Jarek Kus-
mierek, Pawel Nowak, Beata Strack, Piotr Witusowski,
Steven Hand, et al. Autopilot: Workload autoscaling
at Google. In Proceedings of the Fifteenth European
Conference on Computer Systems, pages 1–16, 2020."
REFERENCES,0.8814317673378076,"[63] Mehran Salmani, Saeid Ghafouri, Alireza Sanaee, Kam-
ran Razavi, Max Mühlhäuser, Joseph Doyle, Pooyan
Jamshidi, and Mohsen Sharifi.
Reconciling high ac-
curacy, cost-efficiency, and low latency of inference
serving systems. In Proceedings of the 3rd Workshop
on Machine Learning and Systems, pages 78–86, 2023."
REFERENCES,0.883668903803132,"[64] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu
Zhao, Bingyu Kong, Matthai Philipose, Arvind Krish-
namurthy, and Ravi Sundaram. Nexus: A GPU cluster
engine for accelerating DNN-based video analysis. In
Proceedings of the 27th ACM Symposium on Operating
Systems Principles, pages 322–337, 2019."
REFERENCES,0.8859060402684564,"[65] Vikram Sreekanti, Harikaran Subbaraj, Chenggang Wu,
Joseph E Gonzalez, and Joseph M Hellerstein.
Op-
timizing prediction serving on low-latency serverless
dataflow. arXiv preprint arXiv:2007.05832, 2020."
REFERENCES,0.8881431767337807,"[66] Vikram Sreekanti, Chenggang Wu, Xiayue Charles Lin,
Johann Schleier-Smith, Joseph E. Gonzalez, Joseph M.
Hellerstein, and Alexey Tumanov. Cloudburst: State-
ful functions-as-a-service.
Very Large Data Bases
(PVLDB), 13(12):2438–2452, July 2020.
https:
//doi.org/10.14778/3407790.3407836."
REFERENCES,0.8903803131991052,"[67] NVIDIA TensorRT. Programmable inference accelera-
tor, 2018."
REFERENCES,0.8926174496644296,"[68] Luping Wang, Lingyun Yang, Yinghao Yu, Wei Wang,
Bo Li, Xianchao Sun, Jian He, and Liping Zhang. Mor-
phling: Fast, near-optimal auto-configuration for cloud-
native model serving. In Proceedings of the ACM Sym-
posium on Cloud Computing, pages 639–653, 2021."
REFERENCES,0.8948545861297539,"[69] Wei Wang, Sheng Wang, Jinyang Gao, Meihui Zhang,
Gang Chen, Teck Khim Ng, and Beng Chin Ooi. Rafiki:
Machine learning as an analytics service system. arXiv
preprint arXiv:1804.06087, 2018."
REFERENCES,0.8970917225950783,"[70] Qizhen Weng, Wencong Xiao, Yinghao Yu, Wei Wang,
Cheng Wang, Jian He, Yong Li, Liping Zhang, Wei Lin,
and Yu Ding. MLaaS in the wild: Workload analysis and
scheduling in large-scale heterogeneous GPU clusters.
In 19th {USENIX} Symposium on Networked Systems
Design and Implementation ({NSDI} 22), 2022."
REFERENCES,0.8993288590604027,"[71] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff
Gray, Alejandra Molina, Michael Terry, and Carrie J
Cai. Promptchainer: Chaining large language model
prompts through visual programming. In CHI Confer-
ence on Human Factors in Computing Systems Extended
Abstracts, pages 1–10, 2022."
REFERENCES,0.901565995525727,"[72] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. Ai
chains: Transparent and controllable human-ai interac-
tion by chaining large language model prompts.
In
Proceedings of the 2022 CHI Conference on Human
Factors in Computing Systems, pages 1–22, 2022."
REFERENCES,0.9038031319910514,"[73] Chengliang Zhang, Minchen Yu, Wei Wang, and Feng
Yan.
Mark:
Exploiting cloud services for cost-
effective, SLO-aware machine learning inference serv-
ing. In 2019 {USENIX} Annual Technical Conference
({USENIX}{ATC} 19), pages 1049–1062, 2019."
REFERENCES,0.9060402684563759,"Submitted to the Journal of Systems Research (JSys)
2023"
REFERENCES,0.9082774049217002,"[74] Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik,
Matthai Philipose, Paramvir Bahl, and Michael J Freed-
man. Live video analytics at scale with approximation
and delay-tolerance. In 14th USENIX Symposium on
Networked Systems Design and Implementation (NSDI
17), pages 377–392, 2017."
REFERENCES,0.9105145413870246,"[75] Jeff Zhang, Sameh Elnikety, Shuayb Zarar, Atul Gupta,
and Siddharth Garg. Model-switching: Dealing with
fluctuating workloads in machine-learning-as-a-service
systems. In 12th {USENIX} Workshop on Hot Topics
in Cloud Computing (HotCloud 20), 2020."
REFERENCES,0.912751677852349,"[76] Tianlei Zheng, Xi Zheng, Yuqun Zhang, Yao Deng, ErXi
Dong, Rui Zhang, and Xiao Liu. SmartVM: a SLA-
aware microservice deployment framework. World Wide
Web, 22(1):275–293, 2019."
REFERENCES,0.9149888143176734,"[77] Hao Zhou, Ming Chen, Qian Lin, Yong Wang, Xiaobin
She, Sifan Liu, Rui Gu, Beng Chin Ooi, and Junfeng
Yang. Overload control for scaling WeChat microser-
vices. In ACM Symposium on Cloud Computing (SoCC),
pages 149–161, 2018."
REFERENCES,0.9172259507829977,"[78] Zhuangzhuang Zhou, Yanqi Zhang, and Christina De-
limitrou.
Aquatope: Qos-and-uncertainty-aware re-
source management for multi-stage serverless work-
flows. In Proceedings of the 28th ACM International
Conference on Architectural Support for Programming
Languages and Operating Systems, Volume 1, pages
1–14, 2022."
OTHER,0.9194630872483222,"8
Appendix"
OTHER,0.9217002237136466,"8.1
Pipelines Stages Specifications"
OTHER,0.9239373601789709,"List of used models per each stage of the pipeline with their
specification.
Object Detection
Performance Measure: Mean Average Precision (mAP)
Number of Variants: Five
Source: Ultralytics YoloV5 [11]
Threshold: 4 RPS"
OTHER,0.9261744966442953,Table 7: Object Detection Task Models
OTHER,0.9284116331096197,"Model
Params (M)
Base Allocation
mAP"
OTHER,0.930648769574944,"YOLOv5n
1.9
1
45.7
YOLOv5s
7.2
1
56.8
YOLOv5m
21.2
2
64.1
YOLOv5l
46.5
4
67.3
YOLOv5x
86.7
8
68.9"
OTHER,0.9328859060402684,"Object Classification
Performance Measure: Accuracy
Number of Variants: 5
Source: Torchvision [54]
Threshold: 4 RPS"
OTHER,0.9351230425055929,Table 8: Object Classification Task Models
OTHER,0.9373601789709173,"Model
Params (M)
Base Allocation
Accuracy"
OTHER,0.9395973154362416,"ResNet18
11.7
1
69.75
ResNet34
21.8
1
73.31
ResNet50
25.5
1
76.13
ResNet101
44.54
1
77.37
ResNet52
60.2
2
78.31"
OTHER,0.941834451901566,"Audio
Performance Measure: Word Error Rate (WER)
Number of Variants: Five
Source: HuggingFace
HuggingFace Source: facebook
Threshold: 1 RPS"
OTHER,0.9440715883668904,Table 9: Audio Task Models
OTHER,0.9463087248322147,"Model
Params (M)
Base Allocation
WER"
OTHER,0.9485458612975392,"s2t-small-librispeech
29.5
1
41.28
s2t-medium-librispeech
71.2
2
35.12
wav2vec2-base
94.4
2
33.85
s2t-large-librispeech
267.8
4
33.26
wav2vec2-large
315.5
8
27.65"
OTHER,0.9507829977628636,"Question Answering
Performance Measure: F1 Score"
OTHER,0.9530201342281879,"Submitted to the Journal of Systems Research (JSys)
2023"
OTHER,0.9552572706935123,"Number of Variants: Two
Source: HuggingFace
HuggingFace Source: depeest
Threshold: 1 RPS"
OTHER,0.9574944071588367,Table 10: Question Answering Task Models
OTHER,0.959731543624161,"Model
Params (M)
Base Allocation
F1 Score"
OTHER,0.9619686800894854,"roberta-base
277.45
1
77.14
roberta-large
558.8
1
83.79"
OTHER,0.9642058165548099,"Summarisation
Performance Measure: Recall-Oriented Understudy for
Gisting Evaluation (ROUGE-L)
Number of Variants: Six
Source: HuggingFace
HuggingFace Source: sshleifer
Threshold: 5 RPS"
OTHER,0.9664429530201343,Table 11: Summarisation Task Models
OTHER,0.9686800894854586,"Model
Params (M)
Base Allocation
ROUGE-L"
OTHER,0.970917225950783,"distilbart-1-1
82.9
1
32.26
distilbart-12-1
221.5
2
33.37
distilbart-6-6
229.9
4
35.73
distilbart-12-3
255.1
8
36.39
distilbart-9-6
267.7
8
36.61
distilbart-12-6
305.5
16
36.99"
OTHER,0.9731543624161074,"Sentiment Analysis
Performance Measure: Accuracy
Number of Variants: Three
Source: HuggingFace
HuggingFace Source: Souvikcmsa
Threshold: 1 RPS"
OTHER,0.9753914988814317,Table 12: Sentiment Analysis Task Models
OTHER,0.9776286353467561,"Model
Params (M)
Base Allocation
Accuracy"
OTHER,0.9798657718120806,"DistillBerT
66.9
1
79.6
Bert
109.4
1
79.9
Roberta
355.3
1
83"
OTHER,0.9821029082774049,"Language Identification Task Models
Performance Measure: Accuracy
Number of Variants: One
Source: HuggingFace
HuggingFace Source: dinalzein
Threshold: 4 RPS
Neural Machine Translation
Performance Measure: Bilingual Evaluation Understudy
(BELU)
Number of Variants: Two"
OTHER,0.9843400447427293,Table 13: Language Identification Task Models
OTHER,0.9865771812080537,"Model
Params (M)
Base Allocation
Accuracy"
OTHER,0.9888143176733781,"roberta-base-finetuned
278
1
79.62"
OTHER,0.9910514541387024,"Source: HuggingFace
HuggingFace Source: Helsinki-NLP
Threshold: 4 RPS"
OTHER,0.9932885906040269,Table 14: Neural Machine Translation Task Models
OTHER,0.9955257270693513,"Model
Params (M)
Base Allocation
Accuracy"
OTHER,0.9977628635346756,"opus-mt-fr-en
74.6
4
33.1
opus-mt-tc-big-fr-en
230.6
8
34.4"
