Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004366812227074236,"The study of first-order optimization is sensitive to the assumptions made on the
objective functions. These assumptions induce complexity classes which play a
key role in worst-case analysis, including the fundamental concept of algorithm
optimality. Recent work argues that strong convexity and smoothness—popular
assumptions in literature—lead to a pathological definition of the condition number
[12]. Motivated by this result, we focus on the class of functions satisfying a
lower restricted secant inequality and an upper error bound. On top of being
robust to the aforementioned pathological behavior and including some non-convex
functions, this pair of conditions displays interesting geometrical properties. In
particular, the necessary and sufficient conditions to interpolate a set of points and
their gradients within the class can be separated into simple conditions on each
sampled gradient. This allows the performance estimation problem (PEP, [7, 33])
to be solved analytically, leading to a lower bound on the convergence rate that
proves gradient descent to be exactly optimal on this class of functions among all
first-order algorithms."
INTRODUCTION,0.008733624454148471,"1
Introduction"
INTRODUCTION,0.013100436681222707,"The typical framework to study convergence properties of first-order algorithms in the context of
machine learning is to first establish a class of objective functions to optimize through assumptions
usually bound to a constant, such as L-smoothness and µ-strong convexity. A tuning prescription of
an algorithm is then made based on the constants, (e.g. step size α =
2
µ+L in the case of the gradient
descent method on smooth and strongly convex functions), and finally a worst-case convergence rate
can be derived for this algorithm when using this tuning prescription. In some cases, a lower bound on
the achievable worst-case convergence rate can also be derived, leading to the theoretical optimality
of an algorithm on the considered class of function, for instance the Nesterov accelerated gradient
method [27] is known to be optimal up to a constant on strongly convex and smooth functions."
INTRODUCTION,0.017467248908296942,"However in [12], the authors establish that such framework and its derived results are very sensitive
to the choice of assumptions, and that strong convexity and smoothness can exhibit pathological
behaviors leading to conservative tuning and arbitrarily sub-optimal convergence rates, even when the
resulting algorithm achieves theoretical worst-case optimality on this class of functions. Furthermore,
they propose a set of more robust alternative conditions. In this work, we focus on a specific pair
of such alternative conditions : lower restricted secant inequality (RSI−) and upper error bounds
(EB+). Our main contribution is to show that the gradient descent (GD) method with a certain tuning"
INTRODUCTION,0.021834061135371178,"is exactly optimal on the classes of objective functions induced by these conditions, confirming that
optimality results are highly sensitive to the choice of conditions. Another consequence is that no
algorithm can accelerate on this class of functions, implying that additional assumptions are required
to explain the practical efficiency of accelerated methods."
INTRODUCTION,0.026200873362445413,"Notation
Let F the set of differentiable functions from Rd to R that admit a convex set of global
minima X∗
f. We focus on the problem of optimizing a function f ∈F, i.e. finding x ∈X∗
f. For any
x ∈Rd, we denote x∗
f the orthogonal projection of x on X∗
f. By abuse of notation, when the context
is not ambiguous, we will simply denote x∗
f as x∗."
INTRODUCTION,0.03056768558951965,"We call gradient descent (GD) the standard optimization algorithm based on the following update,
where α is the step size :
xi+1 = xi −α∇f(xi)"
INTRODUCTION,0.034934497816593885,"We call first-order algorithm all A that consider past iterates, function values and gradients and output
a next iterate. Formally, A can be seen as a sequence of functions {An | n ∈N} such that for any
n ∈N, An is a function defined on

Rd × R × Rdn+1 with values in Rd. Under that formalism,
applying A to optimize an objective function f starting in x0 ∈Rd generates a sequence of iterates
(xi)i such that ∀i, xi+1 = Ai

(xj, f(xj), ∇f(xj))j≤i

. Note that we do not require the iterates of
A to lie within the span of observed gradients as it is often the case in the literature."
INTRODUCTION,0.039301310043668124,"Outline
Section 2 introduces RSI−and EB+ and provides some basic properties and motivation.
Section 3 discusses related works in the literature. Section 4 defines and establishes the necessary
and sufficient interpolation conditions for RSI−and EB+, which is a key element of our analysis.
Section 5 proves the lower bound on the convergence rate of first-order algorithms under RSI−and
EB+, and finally Section 6 concludes our work. Detailed proofs are provided in the appendix."
LIT REVIEW,0.043668122270742356,"2
Lower restricted secant inequality and upper error bounds"
LIT REVIEW,0.048034934497816595,We now define RSI−and EB+ and discuss some basic properties.
LIT REVIEW,0.05240174672489083,Definition 2.1 (Lower restricted secant inequality) Let f ∈F and µ > 0
LIT REVIEW,0.056768558951965066,"f ∈RSI−(µ) ⇔∀x ∈Rd, ⟨∇f(x) | x −x∗⟩≥µ ∥x −x∗∥2
2"
LIT REVIEW,0.0611353711790393,"Intuitively, RSI−(µ) enforces that the further x is from X∗
f, the stronger the gradient of f in x must
be in the opposite direction of X∗
f."
LIT REVIEW,0.06550218340611354,"Remark 2.2 RSI−(µ) includes non-convex functions. However, it prevents flat landscapes outside
of X∗
f, and requires f to increase at least quadratically with the distance to X∗
f, as established in
[12] :
f ∈RSI−(µ) ⇒∀x ∈Rd, f(x) −f ∗≥µ"
LIT REVIEW,0.06986899563318777,"2 ∥x −x∗∥2
2"
LIT REVIEW,0.07423580786026202,Definition 2.3 (Upper Error Bounds) Let f ∈F and L > 0
LIT REVIEW,0.07860262008733625,"f ∈EB+(L) ⇔∀x ∈Rd, ∥∇f(x)∥2 ≤L ∥x −x∗∥2"
LIT REVIEW,0.08296943231441048,"EB+(L) thus enforces that the gradient of f is controlled by the distance to X∗
f."
LIT REVIEW,0.08733624454148471,"Remark 2.4 L-smoothness implies EB+(L) and µ-strong convexity implies RSI−(µ). However, one
must be careful before claiming that RSI−and EB+ are respectively weaker than strong convexity
and smoothness : for µ0 > µ1, RSI−(µ0) is neither stronger or weaker than µ1-strong convexity,
and for L0 < L1, EB+(L0) is neither stronger or weaker than L1-smoothness."
LIT REVIEW,0.09170305676855896,"Therefore, even when the objective function is smooth and strongly convex, considering convergence
results under RSI−and EB+ is relevant, as we might obtain better constants µ and L. Many"
LIT REVIEW,0.09606986899563319,"convergence results depend of the condition number κ =
L
µ. Better constants leads to a better
condition number, and thus a potentially better convergence rate (including when the dependence in
the condition number κ is the same or worse). As a consequence, machine learning problems with
strongly convex and smooth objective functions are all potential applications of results under RSI−"
LIT REVIEW,0.10043668122270742,"and EB+, provided we can obtain better constants under these conditions."
LIT REVIEW,0.10480349344978165,"Prop 1 (convergence rate of GD under RSI−and EB+) Let f ∈RSI−(µ)∩EB+(L). Then
gradient descent with learning rate α =
µ
L2 on f will guarantee the following convergence rate:"
LIT REVIEW,0.1091703056768559,"∥xi −x∗
i ∥2
2 ≤

1 −µ2 L2"
LIT REVIEW,0.11353711790393013,"i
∥x0 −x∗
0∥2
2
(1)"
LIT REVIEW,0.11790393013100436,"Moreover, when the learning rate is set to α0 =
1
2µ on the first step, and α =
µ
L2 on every other
step, gradient descent guarantees the following convergence rate:"
LIT REVIEW,0.1222707423580786,"∥xi −x∗
i ∥2
2 ≤∥∇f(x0)∥2
2
4µ2"
LIT REVIEW,0.12663755458515283,"
1 −µ2 L2"
LIT REVIEW,0.13100436681222707,"i−1
(2)"
LIT REVIEW,0.13537117903930132,"Proof. When α =
µ
L2 , we have

xi+1 −x∗
i+1

2
2 ≤∥xi+1 −x∗
i ∥2
2
= ∥xi −α∇f(xi) −x∗
i ∥2
2
= ∥xi −x∗
i ∥2
2 −2α ⟨∇f(xi) | xi −x∗
i ⟩+ α2 ∥∇f(xi)∥2
2
≤

1 −2αµ + L2α2
∥xi −x∗
i ∥2
2"
LIT REVIEW,0.13973799126637554,"=

1 −µ2 L2"
LIT REVIEW,0.14410480349344978,"
∥xi −x∗
i ∥2
2 , (3)"
LIT REVIEW,0.14847161572052403,"which proves (1). To prove (2), we simply note that when using α =
1
2µ, we have:"
LIT REVIEW,0.15283842794759825,"∥x1 −x∗
1∥2
2 ≤∥x0 −x∗
0∥2
2 −2α ⟨∇f(x0) | x0 −x∗
0⟩+ α2 ∥∇f(x0)∥2
2 ≤1"
LIT REVIEW,0.1572052401746725,"µ ⟨∇f(x0) | x0 −x∗
0⟩−1"
LIT REVIEW,0.1615720524017467,"µ ⟨∇f(x0) | x0 −x∗
0⟩+ ∥∇f(x0)∥2
2
4µ2"
LIT REVIEW,0.16593886462882096,"= ∥∇f(x0)∥2
2
4µ2
. (4) ■"
LIT REVIEW,0.1703056768558952,"Interestingly, RSI−and EB+ are direct bounds on the two additional terms obtained by developing
∥xi −α∇f(xi) −x∗
i ∥2
2, leading to an extremely simple proof. On the intuitive level, RSI−lower
bounds the gain from stepping in the direction of X∗
f, while EB+ upper bounds the error coming
from the component of the gradient orthogonal to that direction."
LIT REVIEW,0.17467248908296942,Remark 2.5 The literature gives a worst case convergence rate of gradient descent on µ-strongly
LIT REVIEW,0.17903930131004367,"convex and L-smooth functions of ∥xi −x∗
i ∥2
2 ≤

1 −
2µ
µ+L
2i
∥x0 −x∗
0∥2
2, using the step size"
LIT REVIEW,0.18340611353711792,"α =
2
µ+L [28, 30]. While this rate is better for a fixed µ and L, we again emphasize that the
constants µ and L may be very different depending on the chosen conditions, and thus these two rates
can not directly be compared."
LIT REVIEW,0.18777292576419213,"Motivation: we empirically verify in Appendix A that the optimization path of a ResNet18 [16]
trained for classification on CIFAR10 [20] verifies the interpolation conditions of RSI−∩EB+,
which are introduced in Section 4. This result guarantees the existence of a function in RSI−∩EB+
which exactly interpolates the observed gradients, and thus all convergence guarantees of RSI−∩
EB+ naturally apply in this practical deep learning setting. RSI−∩EB+ therefore provides linear
convergence guarantees while being empirically applicable to neural networks (although with respects
to a local minima), an impressive feat given the highly non-convex nature of neural networks loss
functions."
LIT REVIEW,0.19213973799126638,"3
Related Work"
LIT REVIEW,0.1965065502183406,"Throughout the literature, many choices of assumptions have been used to study first-order optimiza-
tion. Most assumptions fall into one of two categories : lower conditions and upper conditions that
respectively take the form of a lower and an upper bound on properties of the objective function. For
instance, strong convexity lower bounds the curvature of the objective function and is thus a lower
condition. Similarly, smoothness is an upper condition."
LIT REVIEW,0.20087336244541484,"Lower conditions have been the most extensively studied assumptions, such as Polyak-Łojasiewicz
[29], local-quasi-convexity [15], weak quasi-convexity [14], quadratic growth [2, 4, 18], Kurdyka-
Łojasiewicz [21, 3], optimal strong convexity [22, 25, 8], weak strong convexity [19, 26], error bounds
[24]. Some recent works have explored the relations between these lower conditions [19, 38]. In this
work we focus on the restricted secant inequality (RSI−) (which we denote as lower restricted secant
inequality to differentiate it from its upper bound equivalent) which was introduced in [39], and has
been used (along with its convex extension restricted strong convexity) in many recent theoretical
derivations of linear convergence rates [36, 31, 37]."
LIT REVIEW,0.2052401746724891,"On the contrary, because most machine learning objective functions are naturally smooth, fewer works
have explored alternatives to smoothness. However as discussed in Remark 2.4, it is still relevant
to study these alternatives on smooth objectives due to potentially better conditioning. The most
notable ones in the literature are local smoothness [15], restricted smoothness [1], relative smoothness
[23, 13, 40], weak-smoothness [14], expected smoothness [11]. In [12], the authors argue that lower
conditions can be naturally translated into equivalent upper conditions by changing the lower bound
into an upper bound, and vice-versa. Subsequently, they introduce a set of upper equivalent to existing
lower conditions, such as upper error bounds EB+, the natural upper equivalent to error bounds
from [24]. Throughout this work, we focus on EB+ as an upper condition."
LIT REVIEW,0.2096069868995633,"Finally, a key to our analysis are the necessary and sufficient interpolation conditions of RSI−(µ) ∩
EB+(L) (Section 4). The search of these conditions has been largely motivated by Performance
Estimation Problems (PEP), introduced in [33] and with many recent successful applications [17, 32, 6,
35, 34]. PEP is a framework for computer-assisted worst-case convergence analysis, that only requires
necessary and sufficient interpolation conditions for the considered class of objective functions. In the
case of RSI−∩EB+ however, we found the interpolation conditions to be independent (see Section
4), which made the worst-case analysis directly solvable analytically."
IMPLEMENTATION/METHODS,0.21397379912663755,"4
Interpolation conditions"
IMPLEMENTATION/METHODS,0.2183406113537118,"In this section we provide and discuss the necessary and sufficient interpolation conditions for
RSI−∩EB+. Their importance stems from the framework used in PEP and in this work to analyze
worst-case convergence. Given a first-order optimization algorithm A we want to find the slowest
convergence rate of A among all f within a class of objective functions C and starting point x0 ∈
Rd \ X∗
f. That is equivalent to solving the following optimization problem at any step number n :"
IMPLEMENTATION/METHODS,0.22270742358078602,"min
f∈C,(xi)i≤n∈(Rd)n+1
∥x0 −x∗
0∥2
∥xn −x∗n∥2
s.t.
∀i ≤n −1, xi+1 = A ((x0, f(x0), ∇f(x0)) , ..., (xi, f(xi), ∇f(xi)))
(5)"
IMPLEMENTATION/METHODS,0.22707423580786026,"Directly searching for f in the functional space is generally intractable, however if we can explicitly
find the set G of all families (xi, fi, gi)i such that ∃f ∈C, ∀i, ∇f(xi) = gi and f(xi) = fi (we say
that f interpolates (xi, fi, gi)i), then problem (5) can be reduced to:"
IMPLEMENTATION/METHODS,0.2314410480349345,"min
(xi,gi,fi)i≤n∈(Rd×Rd×R)n+1
∥x0 −x∗
0∥2
∥xn −x∗n∥2
s.t.
∀i ≤n −1, xi+1 = A ((x0, f0, g0) , ..., (xi, fi, gi))
and
(xi, fi, gi)i≤n ∈G (6)"
IMPLEMENTATION/METHODS,0.23580786026200873,"In many cases (see Section 3), problem (6) is tractable and becomes a very powerful analysis tool
providing lower bounds, upper bounds, and optimal tuning for different types of algorithms and
assumptions used. A crucial and difficult component of this analysis is to formulate the interpolation"
IMPLEMENTATION/METHODS,0.24017467248908297,"conditions, that is the necessary and sufficient conditions for a family (xi, fi, gi)i to belong in G.
Driven by these motivations, we now establish the interpolation conditions for RSI−∩EB+."
IMPLEMENTATION/METHODS,0.2445414847161572,"In Theorem 1, we introduce the necessary and sufficient conditions to interpolate a family (xi, gi),
without considering the function values (fi)i. This theorem could be used to find the worst case
convergence rate over all first-order algorithms that ignore function values. However, in Corollary 1,
we deduce from Theorem 1 sufficient (but not necessary) conditions to interpolate a family (xi, fi, gi).
These conditions allow us in Section 5 to find a lower bound on the worst-case convergence rate
for all first-order algorithms (including algorithms that have access to function values information),
which we know is tight thanks to Prop 1."
IMPLEMENTATION/METHODS,0.24890829694323144,"Theorem 1 (Interpolation conditions) Let (xi, gi)i≤n ∈

Rd × Rdn+1, such that the xi are
separate points."
IMPLEMENTATION/METHODS,0.25327510917030566,"Then, ∀µ, L > 0:
∃f ∈RSI−(µ) ∩EB+(L), s.t. ∀i, ∇f(xi) = gi
⇕"
IMPLEMENTATION/METHODS,0.2576419213973799,"∃X∗⊆Rd convex, s.t. ∀i,"
IMPLEMENTATION/METHODS,0.26200873362445415,"∥gi∥2 ≤L ∥xi −x∗
i ∥2
and
⟨gi | xi −x∗
i ⟩≥µ ∥xi −x∗
i ∥2
2 ,
(7)"
IMPLEMENTATION/METHODS,0.2663755458515284,"where x∗
i is the orthogonal projection of xi onto X∗."
IMPLEMENTATION/METHODS,0.27074235807860264,"Proof. In order to preserve concision and clarity, we will only present the broad outline of the proof
here. For a complete technical proof, see Appendix B."
IMPLEMENTATION/METHODS,0.27510917030567683,"The direct implication is trivial as it is a direct application of RSI−(µ) and EB+(L) definitions to
f in (xi)i. For the reverse implication, we will construct a function fϵ,β that interpolates (xi, gi)i.
The function fϵ,β is a quadratic everywhere except in the spheres of radius ϵ around each xi, ϵ being
small enough for these spheres to never intersect. Inside the sphere of radius ϵ around a given xi,
fϵ,β will be perturbed by adding a term λ(∥x −xi∥2)h(x) where h is affine in x, and λ is a scaling
term so that λ(ϵ) = 0 at the border of the sphere, and λ(0) = 1 in its center xi."
IMPLEMENTATION/METHODS,0.2794759825327511,The key is to find a function λ that preserves the properties of RSI−(µ) and EB+(L). We use
IMPLEMENTATION/METHODS,0.2838427947598253,"λϵ,β(u) =
1 + cos

π uβ ϵβ
 2
(8)"
IMPLEMENTATION/METHODS,0.28820960698689957,"And our construction fϵ,β is given by:"
IMPLEMENTATION/METHODS,0.2925764192139738,"fϵ,β(x) = ( µ+L"
IMPLEMENTATION/METHODS,0.29694323144104806,"4
∥x −x∗∥2
2
if ∀i, ∥x −xi∥2 ≥ϵ µ+L"
IMPLEMENTATION/METHODS,0.30131004366812225,"4
∥x −x∗∥2
2 + λϵ,β (∥x −xi∥2)
D
gi −µ+L"
IMPLEMENTATION/METHODS,0.3056768558951965,"2 (xi −x∗
i ) | x −xi
E
if ∃i, ∥x −xi∥2 < ϵ (9)"
IMPLEMENTATION/METHODS,0.31004366812227074,"The rest of the proof is to use the Taylor expansions of fϵ,β to show that for sufficiently small ϵ and
β, fϵ,β will belong in RSI−(µ) and EB+(L) (see Appendix B). ■"
IMPLEMENTATION/METHODS,0.314410480349345,"Corollary 1 Let (xi, fi, gi)i≤n ∈

Rd × R × Rdn+1, such that the xi are separate points."
IMPLEMENTATION/METHODS,0.31877729257641924,"Then, ∀µ, L > 0:
∃X∗⊆Rd convex, s.t. ∀i,"
IMPLEMENTATION/METHODS,0.3231441048034934,"∥gi∥2 ≤L ∥xi −x∗
i ∥2
⟨gi | xi −x∗
i ⟩≥µ ∥xi −x∗
i ∥2
2"
IMPLEMENTATION/METHODS,0.32751091703056767,fi = µ + L
IMPLEMENTATION/METHODS,0.3318777292576419,"4
∥xi −x∗
i ∥2
2 (10)"
IMPLEMENTATION/METHODS,0.33624454148471616,"⇓
∃f ∈RSI−(µ) ∩EB+(L), s.t. ∀i, ∇f(xi) = gi
and
f(xi) = fi,"
IMPLEMENTATION/METHODS,0.3406113537117904,"where x∗
i is the orthogonal projection of xi onto X∗."
IMPLEMENTATION/METHODS,0.34497816593886466,"Proof. We simply use the function fϵ,β from the proof of Theorem 1 and note that ∀i, fϵ,β(xi) =
µ+L"
IMPLEMENTATION/METHODS,0.34934497816593885,"4
∥xi −x∗
i ∥2
2. ■"
IMPLEMENTATION/METHODS,0.3537117903930131,Theorem 1 and Corollary 1 are the key elements to prove the optimality of gradient descent on RSI−
IMPLEMENTATION/METHODS,0.35807860262008734,and EB+ among all first-order algorithms (see Section 5).
IMPLEMENTATION/METHODS,0.3624454148471616,"Remark 4.1 The interpolation conditions in Theorem 1 are independent, in the sense that a family
(xi, gi)i admits an interpolation in RSI−(µ) ∩EB+(L) if and only if each {(xi, gi)} admits an
interpolation in RSI−(µ) ∩EB+(L). Similarly, the sufficient interpolation conditions in Corollary 1
are also independent."
IMPLEMENTATION/METHODS,0.36681222707423583,"This property of independent interpolation conditions drastically simplifies convergence analysis and
is the main reason we are able to analytically derive a lower bound in Section 5. Indeed, given an
interpolable family (xi, fi, gi)i≤n for a given set X∗, it is sufficient to show that (xn+1, fn+1, gn+1)
is interpolable with X∗to prove that the entire family (xi, fi, gi)i≤n+1 can be interpolated. It
is thus simple to find the set of interpolable (fn+1, gn+1) given X∗, xn+1, and an interpolable
(xi, fi, gi)i≤n."
IMPLEMENTATION/METHODS,0.37117903930131,"5
Lower bound on the convergence rate"
IMPLEMENTATION/METHODS,0.37554585152838427,In this Section we derive a lower bound on the convergence rate of first-order algorithms on RSI−
IMPLEMENTATION/METHODS,0.3799126637554585,"and EB+. This lower bounds applies under the assumption that the number of steps taken is smaller
than the number of dimension d. This assumption is frequent in the literature (e.g. [5]) and not
constraining for high-dimensional optimization. The observed gradients of the worst-case functions
for optimal algorithms are typically orthogonal to one another (see [6]) which is not possible when
the number of steps becomes larger than the dimension d. We conjecture that when not bounding the
number of steps, it is possible to achieve an asymptotic rate in O

2−n"
IMPLEMENTATION/METHODS,0.38427947598253276,"d 
which would be better than
the usual rates obtained for very ill-conditioned functions, while having little to no practical uses due
to the bad convergence properties on a lower number of steps."
IMPLEMENTATION/METHODS,0.388646288209607,"We now introduce Lemma 1, which is the cornerstone of the proof of Theorem 2:"
IMPLEMENTATION/METHODS,0.3930131004366812,"Lemma 1 Let µ > 0 and L > µ. Let α0 ∈
h
µ
L2 , max

µ
L2 , 1"
IMPLEMENTATION/METHODS,0.39737991266375544,"2µ
i
. For any first-order"
IMPLEMENTATION/METHODS,0.4017467248908297,"optimization algorithm A and starting point x0 ∈Rd, there exists (gi)i≤d−2 ∈Rd, (fi)i≤d−2 ∈
R and Sd−2 ⊆Sd−1 ⊆· · · ⊆S0 ⊆Rd such that:"
IMPLEMENTATION/METHODS,0.40611353711790393,"1. ∀i ≤d −2, there exists a (d −i −1)-dimensional affine space Hi containing Si and"
IMPLEMENTATION/METHODS,0.4104803493449782,"in which Si is a (d −i −2)-sphere of radius ri =
q α0"
IMPLEMENTATION/METHODS,0.4148471615720524,"µ −α2
0 ∥g0∥2

1 −µ2"
IMPLEMENTATION/METHODS,0.4192139737991266,"L2
 i"
IMPLEMENTATION/METHODS,0.42358078602620086,"2 and
center ci ∈Hi."
IMPLEMENTATION/METHODS,0.4279475982532751,"2. Let (xi)i be the iterates generated by A starting from x0 and reading gradients (gi)i
and function values (fi)i, then for any i ≤d−2 and any x ∈Si, there exists a function
f in RSI−(µ) ∩EB+(L) minimized by {x} that interpolates (xj, fj, gj)j≤i."
IMPLEMENTATION/METHODS,0.43231441048034935,"Proof. In order to preserve concision and clarity, we will only present the broad outline of the proof
here. For a complete technical proof, see Appendix C."
IMPLEMENTATION/METHODS,0.4366812227074236,"We construct the sequence iteratively.
For initialisation, we take any non-zero g0, set f0 =
µ+L"
IMPLEMENTATION/METHODS,0.4410480349344978,"4µ α0 ∥g0∥2
2, c0 = x0 −α0g0, and finally"
IMPLEMENTATION/METHODS,0.44541484716157204,"S0 =

x ∈Rd⟨x −c0 | g0⟩= 0
	
∩

x ∈Rd
∥x −c0∥2 =
rα0"
IMPLEMENTATION/METHODS,0.4497816593886463,"µ −α2
0 ∥g0∥2 "
IMPLEMENTATION/METHODS,0.45414847161572053,"Then assuming we have a sequence (fj, gj, Sj)j≤i<(d−2) respecting the conditions of the Lemma,
noting Hi the d −i −1 dimensional affine space in which Si is a (d −i −2) dimensional sphere,
and xi+1 the (i + 1)-th iterate returned by A. Let hi+1 the orthogonal projection of xi+1 into Hi."
IMPLEMENTATION/METHODS,0.4585152838427948,"If hi+1 ̸= ci, let v =
(hi+1−ci)
∥hi+1−ci∥2 . If hi+1 = ci, let s ∈Si and v =
(s−ci)
∥s−ci∥2 ."
IMPLEMENTATION/METHODS,0.462882096069869,We then construct:
IMPLEMENTATION/METHODS,0.4672489082969432,ci+1 = ci −µ Lriv
IMPLEMENTATION/METHODS,0.47161572052401746,fi+1 = µ + L
IMPLEMENTATION/METHODS,0.4759825327510917,"4
(∥xi+1 −ci+1∥2
2 + (1 −µ2"
IMPLEMENTATION/METHODS,0.48034934497816595,"L2 )r2
i )"
IMPLEMENTATION/METHODS,0.4847161572052402,gi+1 = L ∥xi+1 −x∗∥2
IMPLEMENTATION/METHODS,0.4890829694323144,"∥xi+1 −ci+1∥2
(xi+1 −ci+1)"
IMPLEMENTATION/METHODS,0.49344978165938863,"Hi+1 =
n
x ∈Hi | ⟨x −ci | v⟩= −µ Lri
o"
IMPLEMENTATION/METHODS,0.4978165938864629,Si+1 = Si ∩Hi+1
IMPLEMENTATION/METHODS,0.5021834061135371,We verify in Appendix C that this construction respects the properties of Lemma 1. ■
IMPLEMENTATION/METHODS,0.5065502183406113,"We can now introduce Theorem 2 which gives us a lower bound on the worst-case convergence rate
of any first-order algorithm on RSI−(µ) and EB+(L)."
IMPLEMENTATION/METHODS,0.5109170305676856,"Theorem 2 (Lower bound on RSI−∩EB+) Let A be any first-order algorithm on Rd, µ > 0
and L ≥µ. For any x0 ∈Rd, there exists x∗∈Rd and a function f in RSI−(µ) ∩EB+(L)
minimized by X∗= {x∗} such that"
IMPLEMENTATION/METHODS,0.5152838427947598,"∀i ≤d −1, ∥xi −x∗
i ∥2
2 ≥

1 −µ2 L2"
IMPLEMENTATION/METHODS,0.519650655021834,"i
∥x0 −x∗
0∥2
2
(11)"
IMPLEMENTATION/METHODS,0.5240174672489083,"Furthermore, if L µ ≥
√"
IMPLEMENTATION/METHODS,0.5283842794759825,"2, there exists h in RSI−(µ) ∩EB+(L) minimized by X∗= {x∗} such
that"
IMPLEMENTATION/METHODS,0.5327510917030568,"∀i ≤d −1, ∥zi −z∗
i ∥2
2 ≥∥∇h(z0)∥2
2
4µ2"
IMPLEMENTATION/METHODS,0.537117903930131,"
1 −µ2 L2"
IMPLEMENTATION/METHODS,0.5414847161572053,"i−1
(12)"
IMPLEMENTATION/METHODS,0.5458515283842795,where (xi) (resp. (zi)) is the trajectory obtained by applying A to f (resp. h) starting in x0.
IMPLEMENTATION/METHODS,0.5502183406113537,"Note that since X∗is a singleton, ∀i, x∗
i = z∗
i = x∗."
IMPLEMENTATION/METHODS,0.5545851528384279,"Proof. If L = µ, then the inequalities are trivial from the positivity of the norm. If L > µ, let
(gi, fi, Si)i≤(d−2) be the sequence introduced in Lemma 1 for α0 ∈
h
µ
L2 , max

µ
L2 , 1"
IMPLEMENTATION/METHODS,0.5589519650655022,"2µ
i
. Let us"
IMPLEMENTATION/METHODS,0.5633187772925764,"note that for any x ∈S0, ∥x0 −x∥2
2 = α0∥g0∥2
2
µ
(see initialisation in Appendix C)."
IMPLEMENTATION/METHODS,0.5676855895196506,"Let i ∈{1, . . . , d −1}. Si−1 has radius ri−1, thus there exists x∗∈Si−1 such that ∥xi −x∗∥2 ≥
ri−1, and thus :"
IMPLEMENTATION/METHODS,0.5720524017467249,"∥xi −x∗∥2
2 ≥r2
i−1 =
α0"
IMPLEMENTATION/METHODS,0.5764192139737991,"µ −α2
0"
IMPLEMENTATION/METHODS,0.5807860262008734,"
∥g0∥2
2"
IMPLEMENTATION/METHODS,0.5851528384279476,"
1 −µ2 L2"
IMPLEMENTATION/METHODS,0.5895196506550219,"i−1
(13)"
IMPLEMENTATION/METHODS,0.5938864628820961,"When setting α0 =
µ
L2 and observing that x∗∈Si−1 ⊆S0 and thus ∥x0 −x∗∥2
2 = α0∥g0∥2
2
µ
in (13),
we obtain (11). If L µ ≥
√"
IMPLEMENTATION/METHODS,0.5982532751091703,"2, we set α0 =
1
2µ and (13) immediately yields (12). ■"
IMPLEMENTATION/METHODS,0.6026200873362445,"Remark 5.1 Since the lower bounds established in Theorem 2 are exactly matched by the conver-
gence guarantees of gradient descent (see Prop 1), these bounds are tight and gradient descent is
exactly optimal on RSI−(µ) ∩EB+(L). This is a concrete example of the sensitivity of theoretical
optimality to the choice of complexity classes."
IMPLEMENTATION/METHODS,0.6069868995633187,"5.1
Discussion"
CONCLUSION/DISCUSSION ,0.611353711790393,The first bound presented in Theorem 2 gives the optimal solution when trying to solve
CONCLUSION/DISCUSSION ,0.6157205240174672,"min
A max
f,x0
∥xn −x∗
n∥2
∥x0 −x∗
0∥2
(14)"
CONCLUSION/DISCUSSION ,0.6200873362445415,While the second bound gives the optimal solution when trying to solve
CONCLUSION/DISCUSSION ,0.6244541484716157,"min
A max
f,x0
∥xn −x∗
n∥2
∥∇f(x0)∥2
(15)"
CONCLUSION/DISCUSSION ,0.62882096069869,"For general smooth and convex functions, (15) will not have a solution (for any A, the quantity will
not have a worst case upper bound), which is why (14) has historically been the focus of optimization
literature. However in practice, when (15) admits a solution, as is the case for RSI−(µ) ∩EB+(L), it
fits practical motivations better than (14) : when starting from x0 and observing an initial gradient g0,
the solution to (15) is the one that will minimize ∥xn −x∗
n∥2 in the worst case. In comparison, for
a fixed ∥∇f(x0)∥2, the solution to (14) will be faster when ∥x0 −x∗
0∥2 is small, and slower when
∥x0 −x∗
0∥2 is large, leading to a slower worst-case convergence."
CONCLUSION/DISCUSSION ,0.6331877729257642,"When L µ >
√"
CONCLUSION/DISCUSSION ,0.6375545851528385,"2, using a tuning of α0 =
1
2µ on the first step instead of
µ
L2 leads to a worst-case"
CONCLUSION/DISCUSSION ,0.6419213973799127,"convergence of ∥xn −x∗
n∥2 better by a constant c =
L2
2(L2−µ2). While such small constant factor
is often considered not impactful, the number of steps required to make up for this constant factor (a) L µ >
√"
CONCLUSION/DISCUSSION ,0.6462882096069869,"2
(b) L µ =
√"
CONCLUSION/DISCUSSION ,0.6506550218340611,"2
(c) L µ <
√ 2"
CONCLUSION/DISCUSSION ,0.6550218340611353,"Figure 1: 2D representations of the possible positions of x∗given x0 and g0, for three possible values
of L"
CONCLUSION/DISCUSSION ,0.6593886462882096,"µ . x∗must be in CRSI (red) but not in CEB (green). When L µ >
√"
CONCLUSION/DISCUSSION ,0.6637554585152838,"2 (left), doing a larger step to
reach the center of CRSI will minimize worst-case sub-optimality."
CONCLUSION/DISCUSSION ,0.6681222707423581,"is n =
−log(2)"
CONCLUSION/DISCUSSION ,0.6724890829694323,"log

1−µ2"
CONCLUSION/DISCUSSION ,0.6768558951965066,"L2
 −1, which yields n ≈68 for L"
CONCLUSION/DISCUSSION ,0.6812227074235808,µ = 10 and n ≈6930 for L
CONCLUSION/DISCUSSION ,0.6855895196506551,"µ = 100, and can thus"
CONCLUSION/DISCUSSION ,0.6899563318777293,become substantial on ill-conditioned functions.
CONCLUSION/DISCUSSION ,0.6943231441048034,"Finally, we propose a geometric interpretation of the threshold L µ =
√"
CONCLUSION/DISCUSSION ,0.6986899563318777,"2. Given x0 and g0, RSI−(µ)"
CONCLUSION/DISCUSSION ,0.7030567685589519,requires x∗to be within the circle of center x0 −g0
CONCLUSION/DISCUSSION ,0.7074235807860262,2µ and radius ∥g0∥2
CONCLUSION/DISCUSSION ,0.7117903930131004,"2µ , while EB+(L) requires x∗to"
CONCLUSION/DISCUSSION ,0.7161572052401747,not be within the circle of center x0 and radius ∥g0∥2
CONCLUSION/DISCUSSION ,0.7205240174672489,"L
. In Figure 1 we show these circles for different
values of L"
CONCLUSION/DISCUSSION ,0.7248908296943232,"µ. When L µ >
√"
CONCLUSION/DISCUSSION ,0.7292576419213974,"2 (Figure 1(a)), x1 = x0 −
µ
L2 g0 minimizes ∥x1−x∗∥2"
CONCLUSION/DISCUSSION ,0.7336244541484717,∥x0−x∗∥2 over all possible
CONCLUSION/DISCUSSION ,0.7379912663755459,"x∗, while x1 = x0 −g0"
CONCLUSION/DISCUSSION ,0.74235807860262,2µ minimizes ∥x1 −x∗∥2. As L
CONCLUSION/DISCUSSION ,0.7467248908296943,"µ becomes smaller than
√"
CONCLUSION/DISCUSSION ,0.7510917030567685,"2 (Figure 1(b) and
1(c)), the same point x1 = x0 −
µ
L2 g0 minimizes both quantities."
CONCLUSION/DISCUSSION ,0.7554585152838428,"5.2
PEP experiment"
CONCLUSION/DISCUSSION ,0.759825327510917,"Since we have found necessary and sufficient interpolation conditions in Theorem 1, we can use the
PEP framework on RSI−∩EB+ to confirm our results and derive the worst-case convergence rate of
first-order algorithms. In Figure 2 we show the worst-case linear rate of convergence of Heavy Ball
(HB) [29] on RSI−(0.1) ∩EB+(1.0) for regularly sampled learning rate α and momentum β (bright
yellow means no linear convergence), generated with PEPit [9]. We remind the update rule of HB"
CONCLUSION/DISCUSSION ,0.7641921397379913,xn+1 = xn −α∇f(xn) + β(xn −xn−1)
CONCLUSION/DISCUSSION ,0.7685589519650655,"Since gradient descent is a special case of HB where β = 0, we observe as expected that the optimal
rate of convergence is achieved for β = 0 and α = 0.1 =
µ
L2 . Moreover, Figure 2 shows that
momentum does not do well on RSI−∩EB+ but gradient decent benefits from a relative robustness
to the tuning of α : we get similar convergence rates for any α ∈[0.05, 0.15]."
CONCLUSION/DISCUSSION ,0.7729257641921398,"6
Conclusion"
CONCLUSION/DISCUSSION ,0.777292576419214,"Our main result is to prove that for any µ > 0 and L ≥µ, gradient descent is exactly optimal
on the class of functions RSI−(µ) ∩EB+(L) (by exact optimality, we mean that the convergence
guarantees of GD match the lower bound of worst-case performances exactly, without a constant
factor of difference). This result confirms the observation in [12] that optimality is overly sensitive to
the choice of assumptions, and should thus be considered with a lot of caution."
CONCLUSION/DISCUSSION ,0.7816593886462883,"Interestingly, our analysis also identifies two similar notions of optimality, one of which suggests
using a larger step size on the first iteration when the function is not particularly well-conditioned to
improve worst-case convergence speed (see section 5.1)."
CONCLUSION/DISCUSSION ,0.7860262008733624,"We verified empirically that RSI−∩EB+, with respect to the last iterate, are verified on the
optimization paths of simple deep neural networks (c.f. Appendix A). This suggests that unlike usual
alternatives which are known to not be verified on the highly non-convex loss landscapes of neural
networks, convergence guarantees on RSI−∩EB+ realistically apply to deep learning. On the"
CONCLUSION/DISCUSSION ,0.7903930131004366,"Figure 2: Worst-case linear convergence rate of heavy ball on RSI−(0.1) ∩EB+(1) depending of its
hyperparameters α and β, as calculated by PEP. The best rate is achieved for α = 0.1 and β = 0."
CONCLUSION/DISCUSSION ,0.7947598253275109,"other hand, the impossibility to accelerate the convergence rate on RSI−∩EB+ implies that these
assumptions are insufficient to explain the empirical successes of accelerated methods."
CONCLUSION/DISCUSSION ,0.7991266375545851,"For the scope of this work we have focused on worst-case convergence analysis. While in practice
average-case convergence rates are more insightful than their worst-case counterparts, such results
are rare due to the necessity of defining a reasonable distribution on the considered class of functions,
which is generally unfeasible. While such distribution on RSI−∩EB+ is equally difficult to define,
it should be feasible to define instead a reasonable distribution of the observed gradient for a given
sampling point x and nearest minima x∗, e.g. an uniform distribution over the (simple) set of possible
gradients. Such approach is conceivable with RSI−∩EB+ only because the interpolation conditions
are independent (see remark 4.1), and thus we can easily make sure that any set of gradients sampled
from this distribution can be interpolated within the class. While the distribution will necessarily
be arbitrary, we believe such analysis could yield very useful insights and RSI−∩EB+ is a rare
opportunity to follow this approach."
CONCLUSION/DISCUSSION ,0.8034934497816594,"Finally, many alternative conditions have been introduced in the literature (see Section 3), for which
optimality results are still unknown. The PEP framework is a powerful tool to study conditions for
which we can determine sufficient and necessary interpolation conditions, which would improve our
understanding of first-order algorithm properties and tuning on a wide variety of objective function
classes."
CONCLUSION/DISCUSSION ,0.8078602620087336,Acknowledgments and Disclosure of Funding
CONCLUSION/DISCUSSION ,0.8122270742358079,"The authors would like to thank Leonard Boussioux, for useful discussions and feedback. Ioannis
Mitliagkas acknowledges support by an NSERC Discovery grant (RGPIN-2019-06512), a Samsung
grant and a Canada CIFAR AI chair."
REFERENCES,0.8165938864628821,References
REFERENCES,0.8209606986899564,"[1] A. Agarwal, S. N. Negahban, and M. J. Wainwright. Fast global convergence of gradient
methods for high-dimensional statistical recovery. Ann. Statist., 40(5):2452–2482, 2012."
REFERENCES,0.8253275109170306,"[2] M. Anitescu. Degenerate nonlinear programming with a quadratic growth condition. SIAM
Journal on Optimization, 10(4):1116–1135, 2000."
REFERENCES,0.8296943231441049,"[3] J. Bolte, A. Daniilidis, O. Ley, and L. Mazet. Characterizations of łojasiewicz inequalities and
applications. arXiv:0802.0826, 2008."
REFERENCES,0.834061135371179,"[4] J. F. Bonnans and A. D. Ioffe. Second-order sufficiency and quadratic growth for non isolated
minima. PhD thesis, INRIA, 1993."
REFERENCES,0.8384279475982532,"[5] S. Bubeck. Convex optimization: Algorithms and complexity, 2015."
REFERENCES,0.8427947598253275,"[6] Y. Drori and A. B. Taylor. Efficient first-order methods for convex minimization: a con-
structive approach.
Mathematical Programming, 184(1-2):183–220, Jun 2019.
ISSN
1436-4646. doi: 10.1007/s10107-019-01410-2. URL http://dx.doi.org/10.1007/
s10107-019-01410-2."
REFERENCES,0.8471615720524017,"[7] Y. Drori and M. Teboulle. Performance of first-order methods for smooth convex minimization:
a novel approach, 2012."
REFERENCES,0.851528384279476,"[8] P. Gong and J. Ye. Linear convergence of variance-reduced stochastic gradient without strong
convexity. arXiv:1406.1102, 2014."
REFERENCES,0.8558951965065502,"[9] B. Goujaud, C. Moucer, F. Glineur, J. Hendrickx, A. Taylor, and A. Dieuleveut. Pepit: computer-
assisted worst-case analyses of first-order optimization methods in python, 2022."
REFERENCES,0.8602620087336245,"[10] R. M. Gower.
Convergence theorems for gradient descent.
Lecture notes for Statistical
Optimization, 2018."
REFERENCES,0.8646288209606987,"[11] R. M. Gower, O. Sebbouh, and N. Loizou. Sgd for structured nonconvex functions: Learning
rates, minibatching and interpolation. arXiv:2006.1031, 2020."
REFERENCES,0.868995633187773,"[12] C. Guille-Escuret, M. Girotti, B. Goujaud, and I. Mitliagkas. A study of condition numbers for
first-order optimization. In International Conference on Artificial Intelligence and Statistics,
pages 1261–1269. PMLR, 2021."
REFERENCES,0.8733624454148472,"[13] F. Hanzely, P. Richtarik, and L. Xiao. Accelerated Bregman proximal gradient methods for
relatively smooth convex optimization. Technical Report MSR-TR-2018-22, Microsoft, 2018."
REFERENCES,0.8777292576419214,"[14] M. Hardt, T. Ma, and B. Recht. Gradient descent learns linear dynamical systems. Journal of
Machine Learning Research, 19, 2018."
REFERENCES,0.8820960698689956,"[15] E. Hazan, K. Levy, and S. Shalev-Shwartz.
Beyond convexity: Stochastic quasi-convex
optimization. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems 28, pages 1594–1602. Curran Associates,
Inc., 2015."
REFERENCES,0.8864628820960698,"[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition, 2015. URL
https://arxiv.org/abs/1512.03385."
REFERENCES,0.8908296943231441,"[17] B. Hu and L. Lessard. Dissipativity theory for nesterov’s accelerated method, 2017."
REFERENCES,0.8951965065502183,"[18] A. Ioffe. On sensitivity analysis of nonlinear programs in banach spaces: the approach via
composite unconstrained optimization. SIAM Journal on Optimization, 4(1):1–43, 1994."
REFERENCES,0.8995633187772926,"[19] H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. CoRR, abs/1608.04636, 2016. URL http:
//arxiv.org/abs/1608.04636."
REFERENCES,0.9039301310043668,"[20] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.9082969432314411,"[21] K. Kurdyka. On gradients of functions definable in o-minimal structures. Annales de l’institut
Fourier, 48:769–783, 1998."
REFERENCES,0.9126637554585153,"[22] J. Liu and S. J. Wright. Asynchronous stochastic coordinate descent: Parallelism and conver-
gence properties. SIAM J. Optim., 25(1):351—376, 2015."
REFERENCES,0.9170305676855895,"[23] H. Lu, R. M. Freund, and Y. Nesterov. Relatively-smooth convex optimization by first-order
methods, and applications. SIAM J. Optim., 28(1):333–354, 2018."
REFERENCES,0.9213973799126638,"[24] Z.-Q. Luo and P. Tseng. Error bounds and convergence analysis of feasible descent methods: a
general approach. Annals of Operations Research, 46(1):157–178, 1993."
REFERENCES,0.925764192139738,"[25] C. Ma, R. Tappenden, and M. Takàˇc. Linear convergence of the randomized feasible descent
method under the weak strong convexity assumption. Journal of Machine Learning Research,
17(228):1–24, 2016."
REFERENCES,0.9301310043668122,"[26] I. Necoara, Y. Nesterov, and F. Glineur. Linear convergence of first order methods for non-
strongly convex optimization, 2016."
REFERENCES,0.9344978165938864,"[27] Y. Nesterov. A method of solving a convex programming problem with convergence rate
O
 1"
REFERENCES,0.9388646288209607,"k2

. Soviet Mathematics Doklady, 27:372–376, 1983."
REFERENCES,0.9432314410480349,"[28] Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.9475982532751092,"[29] B. T. Polyak. Gradient methods for the minimisation of functionals. USSR Computational
Mathematics and Mathematical Physics, 3(4):864 – 878, 1963."
REFERENCES,0.9519650655021834,"[30] B. T. Polyak. Introduction to optimization. optimization software. Inc., Publications Division,
New York, 1:32, 1987."
REFERENCES,0.9563318777292577,"[31] F. Schöpfer. Linear convergence of descent methods for the unconstrained minimization of
restricted strongly convex functions. SIAM J. Optim., 26:1883–1911, 2016."
REFERENCES,0.9606986899563319,"[32] A. B. Taylor. Convex interpolation and performance estimation of first-order methods for convex
optimization, 2017."
REFERENCES,0.9650655021834061,"[33] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Smooth strongly convex interpolation and exact
worst-case performance of first-order methods, 2016."
REFERENCES,0.9694323144104804,"[34] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Exact worst-case performance of first-order
methods for composite convex optimization. SIAM Journal on Optimization, 27(3):1283–1313,
Jan 2017. ISSN 1095-7189. doi: 10.1137/16m108104x. URL http://dx.doi.org/10.
1137/16M108104X."
REFERENCES,0.9737991266375546,"[35] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Exact worst-case convergence rates of the
proximal gradient method for composite convex minimization, 2020."
REFERENCES,0.9781659388646288,"[36] X. Yi, S. Zhang, T. Yang, K. H. Johansson, and T. Chai. Exponential convergence for distributed
smooth optimization under the restricted secant inequality condition, 2019."
REFERENCES,0.982532751091703,"[37] K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized gradient descent. SIAM J.
Optim., 26:1835–1854, 2016."
REFERENCES,0.9868995633187773,"[38] H. Zhang. The restricted strong convexity revisited: analysis of equivalence to error bound and
quadratic growth. Optimization Letters, 11(4):817–833, 2017."
REFERENCES,0.9912663755458515,"[39] H. Zhang and W. Yin. Gradient methods for convex minimization: better rates under weaker
conditions. Cam report, UCLA, 2013."
REFERENCES,0.9956331877729258,"[40] Y. Zhou, Y. Liang, and L. Shen. A simple convergence analysis of bregman proximal gradient
algorithm. Computational Optimization and Applications, 73(3):903–912, 2019."
