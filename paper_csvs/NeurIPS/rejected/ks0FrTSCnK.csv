Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000847457627118644,"To reduce reliance on labelled data, learning with noisy labels (LNL) has gained
1"
ABSTRACT,0.001694915254237288,"increasing attention. However, prevailing works typically assume that such datasets
2"
ABSTRACT,0.002542372881355932,"are primarily affected by closed-set noise (where the true/clean labels of noisy
3"
ABSTRACT,0.003389830508474576,"samples come from another known category), and ignore therefore the ubiqui-
4"
ABSTRACT,0.00423728813559322,"tous presence of open-set noise (where the true/clean labels of noisy samples
5"
ABSTRACT,0.005084745762711864,"may not belong to any known category). In this paper, we formally refine the
6"
ABSTRACT,0.005932203389830509,"LNL problem setting considering the presence of open-set noise. We theoret-
7"
ABSTRACT,0.006779661016949152,"ically analyze and compare the effects of open-set noise and closed-set noise,
8"
ABSTRACT,0.007627118644067797,"as well as the effects between different open-set noise modes. We also analyze
9"
ABSTRACT,0.00847457627118644,"common open-set noise detection mechanisms based on prediction entropy values.
10"
ABSTRACT,0.009322033898305085,"To empirically validate the theoretical results, we construct two open-set noisy
11"
ABSTRACT,0.010169491525423728,"datasets - CIFAR100-O/ImageNet-O and introduce a novel open-set test set for
12"
ABSTRACT,0.011016949152542373,"the widely used WebVision benchmark. Our work suggests that open-set noise
13"
ABSTRACT,0.011864406779661017,"exhibits qualitatively and quantitatively distinct characteristics, and how to fairly
14"
ABSTRACT,0.012711864406779662,"and comprehensively evaluate models in this condition requires more exploration.
15"
INTRODUCTION,0.013559322033898305,"1
Introduction
16"
INTRODUCTION,0.01440677966101695,"In recent years, the tremendous success of machine learning often relies on the assumption that data
17"
INTRODUCTION,0.015254237288135594,"labels are accurate and free from noise. However, in real-world scenarios, label noise caused by
18"
INTRODUCTION,0.016101694915254237,"factors such as annotation errors and label ambiguity is ubiquitous, posing a pervasive challenge to
19"
INTRODUCTION,0.01694915254237288,"the performance and generalization of models. To address this challenge, various methods have been
20"
INTRODUCTION,0.017796610169491526,"proposed to learn with noisy labels, including noise transition matrix [7, 23], label correction [17, 3],
21"
INTRODUCTION,0.01864406779661017,"robust loss functions [6, 29, 19], and recently dominant sample selection-based approaches [11, 2].
22"
INTRODUCTION,0.019491525423728815,"Most current efforts, however, primarily focus on closed-set noise, where the true labels of noisy
23"
INTRODUCTION,0.020338983050847456,"samples belong to another known class. This includes common noise models like symmetric noise
24"
INTRODUCTION,0.0211864406779661,"(assuming that the labels of samples are randomly flipped with a certain probability to any other
25"
INTRODUCTION,0.022033898305084745,"known classes) or asymmetric noise model (assuming that the probability of label confusion is
26"
INTRODUCTION,0.02288135593220339,"influenced by the classes, such as ’cat’ being more likely to be confused with ’dog’ than with
27"
INTRODUCTION,0.023728813559322035,"’airplane’). Recent advancements have also explored instance-dependent noise models [4, 26], where
28"
INTRODUCTION,0.02457627118644068,"label confusion depends directly on individual instances.
29"
INTRODUCTION,0.025423728813559324,"Unfortunately, unlike the in-depth exploration of closed-set noise, there is noticeably limited research
30"
INTRODUCTION,0.026271186440677965,"on open-set noise, where the true labels of noisy samples may not belong to any known category.
31"
INTRODUCTION,0.02711864406779661,"This gap becomes particularly crucial when considering one of the primary motivations for learning
32"
INTRODUCTION,0.027966101694915254,"with noisy labels: learning with datasets obtained through web crawling. Examining one of the most
33"
INTRODUCTION,0.0288135593220339,"commonly used benchmarks - the WebVision dataset [12], we validate the prevalence of open-set
34"
INTRODUCTION,0.029661016949152543,"noise (fig. 1). In fact, the ‘open-world’ assumption involving open-set samples has received more
35"
INTRODUCTION,0.030508474576271188,"attention in other weakly supervised learning problems, such as open-set recognition and outlier
36"
INTRODUCTION,0.03135593220338983,Tench?
INTRODUCTION,0.03220338983050847,"Figure 1: Example images of class “Tench"" from WebVision dataset. Clean samples are marked in
extcolorgreenGreen, closed-set noise is marked in Blue and open-set noise is marked in Red. See
appendix F for more details."
INTRODUCTION,0.03305084745762712,"detection, but lacks enough exploration in the context of LNL. To this end, we focus on a thorough
37"
INTRODUCTION,0.03389830508474576,"theoretical analysis of open-set noise in this paper. Specifically:
38"
INTRODUCTION,0.03474576271186441,"• Considering the presence of open-set noise, we introduce the concept of a complete noise
39"
INTRODUCTION,0.03559322033898305,"transition matrix and reformulate the LNL problem and label noise definition in this context.
40"
INTRODUCTION,0.036440677966101696,"• To enable offline analysis, we consider two pragmatic cases: fitted case, that the model
41"
INTRODUCTION,0.03728813559322034,"perfectly fits the noisy distribution, and memorized case, that the model completely memorises
42"
INTRODUCTION,0.038135593220338986,"the noisy labels.
43"
INTRODUCTION,0.03898305084745763,"• We analyze and compare the open-set noise vs. closed-set noise on closed-set classification
44"
INTRODUCTION,0.03983050847457627,"accuracy and suggest that open-set noise has a less negative impact in both cases. We also
45"
INTRODUCTION,0.04067796610169491,"analyze and compare the ‘hard’ open-set noise vs. ‘easy’ open-set noise, but find that these
46"
INTRODUCTION,0.04152542372881356,"two different noise modes show opposite trends in two different cases.
47"
INTRODUCTION,0.0423728813559322,"• Since closed-set classification evaluation may be insufficient to fully reflect model perfor-
48"
INTRODUCTION,0.043220338983050846,"mance, we consider introducing an additional open-set detection task and conduct preliminary
49"
INTRODUCTION,0.04406779661016949,"experiments.
50"
INTRODUCTION,0.044915254237288135,"• We derive and analyze the open-set noise detection mechanism based on the entropy values
51"
INTRODUCTION,0.04576271186440678,"of model predictions and suggest that it may be effective only for ‘easy’ open-set noise. We
52"
INTRODUCTION,0.046610169491525424,"also consider two representative LNL methods and combine them with such open-set noise
53"
INTRODUCTION,0.04745762711864407,"detection mechanism for further experiments.
54"
INTRODUCTION,0.048305084745762714,"• For controlled experiments, we construct two novel synthetic open-set noise datasets:
55"
INTRODUCTION,0.04915254237288136,"CIFAR100-O and ImageNet-O. Additionally, we introduce a new open-set test set to the
56"
INTRODUCTION,0.05,"WebVision dataset for the open-set detection task.
57"
RELATED WORKS,0.05084745762711865,"2
Related works
58"
RELATED WORKS,0.051694915254237285,"Methods for learning with noisy labels can be roughly categorized into two main directions. The first
59"
RELATED WORKS,0.05254237288135593,"direction typically focuses on estimating noise transition matrix [4, 26, 23, 7] or designing robust
60"
RELATED WORKS,0.053389830508474574,"loss functions [29, 19, 6], aiming to achieve theoretically risk-consistent or probabilistic-consistent
61"
RELATED WORKS,0.05423728813559322,"models. However, most of these works often assume an ideal scenario where the model can learn to
62"
RELATED WORKS,0.05508474576271186,"fit the sampled distribution well, overlooking the over-fitting issues arising from excessive model
63"
RELATED WORKS,0.05593220338983051,"capacity and insufficient data in practical situations. In this paper, we introduce the concept of
64"
RELATED WORKS,0.05677966101694915,"complete noise transition matrix considering the presence of open-set noise and conduct theoretical
65"
RELATED WORKS,0.0576271186440678,"analyses and experimental validations for both ideal case and over-fitting case, namely fitted case
66"
RELATED WORKS,0.05847457627118644,"and memorized case. The second type is often based on sample selection strategies, involving also
67"
RELATED WORKS,0.059322033898305086,"different regularization terms and off-the-shelf techniques such as semi-supervised learning and
68"
RELATED WORKS,0.06016949152542373,"model co-training, to achieve the state-of-the-art performance. Most sample selection methods are
69"
RELATED WORKS,0.061016949152542375,"based on the model’s current predictions, such as the popular ‘small loss’ mechanism [2, 11, 8, 28,
70"
RELATED WORKS,0.06186440677966102,"10, 17, 13, 27, 24, 30], or model’s feature space [21, 22, 15, 5].
71"
RELATED WORKS,0.06271186440677966,"Especially, the investigation on open-set noise is relatively scarce. Wang et al. [18] utilize Local
72"
RELATED WORKS,0.0635593220338983,"Outlier Factor algorithm to identify open-set noise in feature space, Wu et al. [22] propose to identify
73"
RELATED WORKS,0.06440677966101695,"open-set noise with subgraph connectivity, while both Sachdeva et al. [16] and Albert et al. [1] try to
74"
RELATED WORKS,0.06525423728813559,"identify open-set noise based on entropy-related dynamics. Instead, Feng et al. [5] do not identify
75"
RELATED WORKS,0.06610169491525424,"open-set noise explicitly while avoid relabelling and including open-set noise in the training. More
76"
RELATED WORKS,0.06694915254237288,"closely related to our work, Xia et al. [25] also investigates noise transition matrices involving open-
77"
RELATED WORKS,0.06779661016949153,"set noise but considering all open-set noise belonging to a single meta-class. In this paper, we consider
78"
RELATED WORKS,0.06864406779661017,"that open-set noise may originate from different classes, and based on this premise, we analyze two
79"
RELATED WORKS,0.06949152542372881,"distinct open-set noise modes. Wei et al. [20] propose leveraging open-set noise to mitigate the
80"
RELATED WORKS,0.07033898305084746,"impact of closed-set noise, as it helps alleviating the model’s over-fitting tendency. Instead, we focus
81"
RELATED WORKS,0.0711864406779661,"on a thorough theoretical analysis of the effects with different noise modes, including open-set noise
82"
RELATED WORKS,0.07203389830508475,"versus closed-set noise, and different open-set noise versus each other.
83"
METHODOLOGY,0.07288135593220339,"3
Methodology
84"
METHODOLOGY,0.07372881355932204,"In section 3.1, we briefly introduce the traditional problem formulation of LNL. In section 3.2, we
85"
METHODOLOGY,0.07457627118644068,"reformulate the LNL problem considering open-set noise. In section 3.3, we formalize how label
86"
METHODOLOGY,0.07542372881355933,"noise influences model generalization, particularly, on the proposed error rate inflation metric. In
87"
METHODOLOGY,0.07627118644067797,"section 3.4, we analyze and compare the impact of open-set vs. closed-set noise, as well as ‘easy’
88"
METHODOLOGY,0.07711864406779662,"open-set noise vs. ‘hard’ open-set noise. In section 3.5, we scrutinize the open-set noise detection
89"
METHODOLOGY,0.07796610169491526,"mechanism based on model prediction entropy values.
90"
TRADITIONAL FORMULATION OF LNL,0.0788135593220339,"3.1
Traditional formulation of LNL
91"
TRADITIONAL FORMULATION OF LNL,0.07966101694915254,"Supervised classification learning typically assumes that we sample a certain number of independently
92"
TRADITIONAL FORMULATION OF LNL,0.08050847457627118,"and identically distributed training samples {xk, yk}K
k=1 from a joint distribution P(x, y; y ∈Yin),
93"
TRADITIONAL FORMULATION OF LNL,0.08135593220338982,"i.e., the so-called train set. By default, here all the possible values for yk in the discrete label space
94"
TRADITIONAL FORMULATION OF LNL,0.08220338983050847,"Yin : {1, 2, ..., A} (referred here as inlier classes), are known in advance. With a certain loss function,
95"
TRADITIONAL FORMULATION OF LNL,0.08305084745762711,"given the train set {xk, yk}K
k=1 we aim to train a model f : x →y whose predictions can achieve the
96"
TRADITIONAL FORMULATION OF LNL,0.08389830508474576,"minimum error rate under the whole clean distribution P(x, y; y ∈Yin).
97"
TRADITIONAL FORMULATION OF LNL,0.0847457627118644,"Under LNL problem setting, we believe that the joint distribution P(x, y; y ∈Yin) has been perturbed
98"
TRADITIONAL FORMULATION OF LNL,0.08559322033898305,"to P n(x, y; y ∈Yin); especially, the conditional distribution P n(y|x; y ∈Yin) changes — normally
99"
TRADITIONAL FORMULATION OF LNL,0.08644067796610169,"we assume the sampling prior is free of the label noise (P(x; y ∈Yin) = P n(x; y ∈Yin)), leading
100"
TRADITIONAL FORMULATION OF LNL,0.08728813559322034,"to the presence of noisy labels yn
k in the noisy train set {xk, yn
k }K
k=1 that do not conform to the clean
101"
TRADITIONAL FORMULATION OF LNL,0.08813559322033898,"conditional distribution P(y|x; y ∈Yin).
102"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.08898305084745763,"3.2
Revisiting LNL considering open-set noise
103"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.08983050847457627,"We here formally revisit the problem formulation of learning with noisy labels considering the
104"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.09067796610169492,"existence of open-set noise. Instead of assuming all the possible classes are known (y ∈Yin), we
105"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.09152542372881356,"consider samples from some unknown outlier classes may also exist in the train set. Let us denote
106"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.0923728813559322,"these classes as outlier classes Yout : {A + 1, A + 2, ..., A + B} with B as the number of possible
107"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.09322033898305085,"outlier classes. Then, we expand the support of joint distribution to contain both inlier and outlier
108"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.0940677966101695,"classes, denoted as P(x, y; y ∈Yin ∪Yout) and P n(x, y; y ∈Yin ∪Yout) for the clean and noisy
109"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.09491525423728814,"ones, respectively. For brevity, we denote as Yall ≜Yin ∪Yout. Similarly as above, we still assume
110"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.09576271186440678,"the noisy labelling will not affect the sampling prior (P(x; y ∈Yall) = P n(x; y ∈Yall)). For
111"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.09661016949152543,"subsequent analysis, we first define below complete noise transition matrix:
112"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.09745762711864407,"Definition 3.1 (Complete noise transition matrix). For a specific sample x, we define as T (sample
113"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.09830508474576272,"index omitted here for simplicity) the complete noise transition matrix1:
114 T ="
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.09915254237288136,"""
T in
A×A
0A×B"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.1,"T out
B×A
0B×B # ."
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.10084745762711865,"T in corresponds to the confusion process between inlier classes Yin : {1, 2, ..., A}, and T out
115"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.1016949152542373,"corresponds to the confusion process from outlier classes Yout : {A + 1, A + 2, ..., A + B} to inlier
116"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.10254237288135593,"classes Yin : {1, 2, ..., A}.
117"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.10338983050847457,"For brevity, we denote as Tij ≜P(yn = j|y = i, x = x; yn, y ∈Yall). We have further
118
PA+B
j=1 Tij = 1 for i ∈{1, ..., A + B} - noise transition from each clean class sums to 1 over all
119"
REVISITING LNL CONSIDERING OPEN-SET NOISE,0.10423728813559321,"possible noisy classes. With such a complete noise transition matrix T, we can connect the clean
120"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.10508474576271186,"1The right part of the transition matrix is all-zero as we assume in the noisy labelling process all outlier
classes are confused into inlier classes, i.e., all of its samples been labelled as one of the inlier classes."
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.1059322033898305,"conditional distribution P(y|x = x; y ∈Yall) with the noisy conditional distribution P n(y|x =
121"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.10677966101694915,"x; y ∈Yall) as below:
122"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.10762711864406779,"P n(y = j|x = x; y ∈Yall) = A+B
X"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.10847457627118644,"l=1
P(y = l|x = x; y ∈Yall) · Tlj
(1)"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.10932203389830508,"Label noise
Recent works usually discriminate label noise into closed-set noise and open-set noise.
123"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.11016949152542373,"Before continuing with the further discussion, we feel it is necessary to elucidate these two concepts
124"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.11101694915254237,"here clearly to avoid any ambiguities, as we will try to comparably discriminate and analyze them
125"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.11186440677966102,"later. Specifically, most of recent works define open-set noise as ‘a sample with its true label from
126"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.11271186440677966,"unknown classes but mislabelled with a known label’. Formally, we have:
127"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.1135593220338983,"Definition 3.2 (Label noise). For sample x with clean label y and noisy label yn:
128"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.11440677966101695,"• When y = yn, (x, y, yn) is a clean sample;
129"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.1152542372881356,"• When y ̸= yn and y ∈Yin, (x, y, yn) is a closed-set noise;
130"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.11610169491525424,"• When y ̸= yn and y ∈Yout, (x, y, yn) is an open-set noise.
131"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.11694915254237288,"Specifically, we have y ∼P(y = y|x = x; y ∈Yall) while yn ∼P n(y = yn|x = x; y ∈Yall).
132"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.11779661016949153,"However, we can only identify label noise type with (x, y, yn) — y, yn yet to be sampled even with
133"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.11864406779661017,"known conditional probability. To enable sample-wise analysis on the impact of different label noise,
134"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.11949152542372882,"we further introduce below (Ox, Cx) label noise:
135"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.12033898305084746,"Definition 3.3 ((Ox, Cx) label noise). For sample x with clean conditional probability P(y|x =
136"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.1211864406779661,"x; y ∈Yall) and complete noise transition matrix T:
137 Ox = A+B
X i=A+1 A
X"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.12203389830508475,"j=1
TijP(y = i|x = x; y ∈Yall) = A+B
X"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.1228813559322034,"i=A+1
P(y = i|x = x; y ∈Yall), Cx = A
X i=1 A
X"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.12372881355932204,"j=1,j̸=i
TijP(y = i|x = x; y ∈Yall). (2)"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.12457627118644068,"Here, Ox is the expected open-set noise ratio, Cx is the expected closed-set noise ratio. We then
138"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.12542372881355932,"define sample x as an (Ox, Cx) label noise. Intuitively speaking, sample x is expected to be an
139"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.12627118644067797,"open-set noise with probability as Ox and to be a closed-set noise with probability Cx.
140"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.1271186440677966,"With Definition 3.3, we formalize the concept of noise ratio for the whole distribution, as the
141"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.12796610169491526,"accumulated (Ox, Cx) label noise at all sample points x ∈X:
142 N =
Z"
THE RIGHT PART OF THE TRANSITION MATRIX IS ALL-ZERO AS WE ASSUME IN THE NOISY LABELLING PROCESS ALL OUTLIER,0.1288135593220339,"x
(Ox + Cx) · P(x = x; y ∈Yall)dx
(3)"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.12966101694915255,"3.3
Analyzing classification error rate inflation in LNL
143"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13050847457627118,"In this section, we try to analyze the impact of different label noise. Please note, while the reformulated
144"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13135593220338984,"LNL setting encompasses outlier classes Yout, in both the training and evaluation stage, they are
145"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13220338983050847,"unknown (agnostic); the learned model f is still tailored for the classification of inlier classes Yin.
146"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13305084745762713,"That is to say, the default classification evaluation protocol is still concerned with the classification
147"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13389830508474576,"error rate over the inlier conditional probability, denoted as P f(y|x = x; y ∈Yin).
148"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13474576271186442,"Error rate inflation
With P f(y|x = x; y ∈Yin), in the evaluation phase, for specific sample x
149"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13559322033898305,"we have its prediction as: yf = arg maxkP f(y = k|x = x; y ∈Yin) ∈Yin, and the corresponding
150"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13644067796610168,"expected classification error rate as:
151"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13728813559322034,"Ex =
X"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13813559322033897,"y̸=yf
P(x, y; y ∈Yin) = (1 −P(y = yf)|x; y ∈Yin)) · P(x; y ∈Yin).
(4)"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13898305084745763,"Specifically, we have the Bayes error rate corresponds to the Bayes optimal model f ∗:
152"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.13983050847457626,"E∗
x = (1 −maxkP(y = k|x = x; y ∈Yin)) · P(x = x; y ∈Yin).
(5)"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.14067796610169492,"To measure the negative impacts of noisy labels, we care about how much extra errors have been
153"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.14152542372881355,"introduced, measured by the error rate inflation of learned model f compared to the Bayes optimal
154"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1423728813559322,"model f ∗:
155"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.14322033898305084,"Definition 3.4 (Error rate inflation). With E∗
x as the Bayes error rate, we define the error rate inflation
156"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1440677966101695,"for sample x as: ∆Ex = Ex −E∗
x.
157"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.14491525423728813,"Two pragmatic cases
However, P f(y|x = x; y ∈Yin), as the prediction of the final learned
158"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.14576271186440679,"model f, is affected by many factors (model capacity/dataset size/training hyperparameters such
159"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.14661016949152542,"as training epochs, etc.), which is non-trivial to determine its specific value for an offline analysis2.
160"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.14745762711864407,"Thus, we consider two specific pragmatic cases:
161"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1483050847457627,"• Fitted case: the model perfectly fits the noisy distribution: P f(y|x = x; y ∈Yin) = P n(y|x =
162"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.14915254237288136,"x; y ∈Yin);
163"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.15,"• Memorized case: the model completely memorises the noisy labels: P f(y|x = x; y ∈Yin) =
164"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.15084745762711865,"P yn(y|x = x; y ∈Yin); Here P yn denotes the one-hot encoding of the noisy label yn.
165"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.15169491525423728,"Nonetheless, these two cases are very realistic and important; Empirically, it is highly possible that
166"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.15254237288135594,"the memorized case can correspond to scenarios such as scratch training based on a single-label
167"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.15338983050847457,"dataset with a normal deep neural network - as normally such model has enough capacity to memorize
168"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.15423728813559323,"all the labels, while the fitted case can correspond to scenarios such as fine-tuning a linear classifier
169"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.15508474576271186,"with a pre-trained model - as the pre-trained model already captures good sample representations and
170"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.15593220338983052,"the capacity of a linear classifier is limited.
171"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.15677966101694915,"3.4
Error rate inflation analysis w.r.t different label noise
172"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1576271186440678,"In this section, we focus on analyzing the error rate inflation of different label noise. Let us recall the
173"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.15847457627118644,"clean conditional distribution as P(y|x; y ∈Yall). For ease of analysis, we contemplate a simple
174"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.15932203389830507,"scenario, wherein the entire clean conditional distribution remains unchanged, except only one of the
175"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.16016949152542373,"sample points, say x, is afflicted by label noise:
176"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.16101694915254236,"P n(y|x ̸= x; y ∈Yall) = P(y|x ̸= x; y ∈Yall), P n(y|x = x; y ∈Yall) ̸= P(y|x = x; y ∈Yall).
(6)"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.16186440677966102,"In this condition, we can simplify analyzing the impact of label noise on the whole distribution to
177"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.16271186440677965,"analyzing the error rate inflation of a single sample x. Specifically, we consider two specific sample
178"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1635593220338983,"points x1 and x2, corresponding to two in our later comparative analysis. Let us denote its clean
179"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.16440677966101694,"conditional probability as P(y|x = x1; y ∈Yall) = [p1
1, ..., p1
A, ..., p1
A+B] and P(y|x = x2; y ∈
180"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1652542372881356,"Yall) = [p2
1, ..., p2
A, ..., p2
A+B], and noise transition matrix as T 1 and T 2, respectively. We further
181"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.16610169491525423,"assume:
182"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1669491525423729,"Ox1 + Cx1 = Ox2 + Cx2 = δ.
(7)
We compare the error rate inflation (∆Ex1 vs ∆Ex2) with different label noise given same/fixed
183"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.16779661016949152,"noise ratio for a strictly fair comparison. Note we assume that x1 and x2 hold the same sampling
184"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.16864406779661018,"prior probability: P(x = x1; y ∈Yall) = P(x = x2; y ∈Yall)); so that, we assure that the whole
185"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1694915254237288,"noise ratio N is fixed, and more importantly, sample x1 and x2 can be considered as probabilistic
186"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.17033898305084746,"exchangeable in the dataset collection process.
187"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1711864406779661,"For better clarity, we depict the derivation relations for ∆x in fig. 2. Specifically, for our two
188"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.17203389830508475,"interested cases above, we have corresponding error rate inflation for sample x (sample subscript
189"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.17288135593220338,"omitted for simplicity) as:
190"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.17372881355932204,"• Fitted case:
191"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.17457627118644067,"∆Ex = max[p1, ..., pA] −parg max[PA+B
i=1
piTi1,...,PA+B
i=1
piTiA]
(8)"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.17542372881355933,"• Memorized case:
192"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.17627118644067796,"∆Ex = max[p1, ..., pA] − A
X"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.17711864406779662,"i=1
(pi · A+B
X"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.17796610169491525,"j=1
pjTji)
(9)"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1788135593220339,"We notice that ∆x in both cases are only affected by clean conditional probability P(y|x = x1; y ∈
193"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.17966101694915254,"Yall) and complete noise transition matrix T.
194"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1805084745762712,2The reader may refer to [14] for more discussions about related topics such as model generalization.
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.18135593220338983,Closed-set Noise Ratio:
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.18220338983050846,Clean conditional probability:
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.18305084745762712,"Noisy conditional probability:
Complete Noise Transition Matrix: 
Noisy conditional probability over"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.18389830508474575,inlier classes:
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1847457627118644,Clean conditional probability over
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.18559322033898304,"inlier classes:
Error Rate Inflation: 
Open-set Noise Ratio:"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1864406779661017,Figure 2: All-in-one derivation flowchart. Full details in appendix C.
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.18728813559322033,"3.4.1
How does open-set noise compare to closed-set noise?
195"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.188135593220339,"We first try to elucidate the difference between open-set noise and closed-set noise. Without loss of
196"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.18898305084745762,"generality, we consider:
197"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.18983050847457628,"Ox1 > Ox2 , Cx1 < Cx2.
(10)
Intuitively speaking, we consider sample x1 to be more prone to open-set noise compared to sample
198"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1906779661016949,"x2, thus corresponding to the ‘more open-set noise’ scenario. However, without extra regularizations,
199"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.19152542372881357,"there exist infinite T 1 and T 2 fulfilling eq. (7) and eq. (10) given specific P(y|x = x1; y ∈Yall) and
200"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.1923728813559322,"P(y|x = x2; y ∈Yall) (see toy example below), the analysis on ∆Ex1 vs ∆Ex2 is thus infeasible.
201"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.19322033898305085,"Toy example about agnostic T Assuming a ternary classification, with two known inlier
classes (“0"" and “1"") and one unknown outlier class “2"". Say, we have sample x1 with clean
conditional probability as [0.1, 0.2, 0.7]. Assuming two different noise transition matrices for
T 1 below:"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.19406779661016949,"[0.55, 0.45, 0.0] = [0.1, 0.2, 0.7]"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.19491525423728814,""" 0.5
0.5
0
0.75
0.25
0
0.5
0.5
0 #"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.19576271186440677,"[0.45, 0.55, 0.0] = [0.1, 0.2, 0.7]"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.19661016949152543,"""
0
1
0
0.5
0.5
0
0.5
0.5
0 #"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.19745762711864406,"We have Ox1 = 0.7, Cx1 = 0.2 in both conditions but we arrive at different noisy conditional
probability, similarly for sample x2. 202"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.19830508474576272,"We thus consider a class concentration assumption — in most classification datasets, the majority of
203"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.19915254237288135,"samples belong to specific class exclusively with high probability. In this condition, we have proved:
204"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.2,"Theorem 3.5 (Open-set noise vs closed-set noise). Let us consider sample x1, x2 fulfilling eq. (7)
and eq. (10) - compared to x2, x1 is considered as more prone to open-set noise. Let us denote
a = arg maxi P(y = i|x = x1; y ∈Yall) and b = arg maxi P(y = i|x = x2; y ∈Yall), we
assume (with a high probability): p1
a →1, {p1
i →0}i̸=a and p2
b →1, {p2
b →0}i̸=b. Then, we have:"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.20084745762711864,"∆Ex1 < ∆Ex2
in both Fitted case and Memorized case.
205"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.2016949152542373,"Please refer to appendix D.1 for detailed proof. To summarize, we validate that in most conditions,
206"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.20254237288135593,"open-set noise is less harmful than closed-set noise in both fitted case and memorized case.
207"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.2033898305084746,"3.4.2
How does different open-set noise compare to each other?
208"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.20423728813559322,"We further study how different open-set noise affect the model. Specifically, we consider:
209"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.20508474576271185,"Ox1 = Ox2 , Cx1 = Cx2 = 0.
(11)"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.2059322033898305,"Intuitively speaking, we focus on the impacts of different open-set noise modes given the same/fixed
210"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.20677966101694914,"open-set noise ratio, while excluding the effect of closed-set noise. In this section, we assume
211"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.2076271186440678,"sample x1 and sample x2 holds the same clean conditional probability: [p1
1, ..., p1
A, ..., p1
A+B] =
212"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.20847457627118643,"[p2
1, ..., p2
A, ..., p2
A+B], to only focus on the impact of different open-set noise modes with the same
213"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.2093220338983051,"original sample. It is straightforward that Ox1 = Ox2 always holds since PA+B
i=A+1 p1
i = PA+B
i=A+1 p2
i .
214"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.21016949152542372,"To ensure Cx1 = Cx2 = 0, we simply set T 1
in = T 2
in = I.
215"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.21101694915254238,"Thus, we have the flexibility to explore various forms of Tout — corresponding to different open-set
216"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.211864406779661,"noise modes. Specifically, we consider two distinct open-set noise modes: ‘easy’ open-set noise
217"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.21271186440677967,"when the transition from outlier classes to inlier classes involves completely random flipping, and
218"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.2135593220338983,"‘hard’ open-set noise when there exists an exclusive transition between the outlier class and specific
219"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.21440677966101696,"inlier class. We denote as T easy for ‘easy’ open-set noise and T hard for ‘hard’ open-set noise, with
220"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.21525423728813559,"intuitive explanations below:
221"
ANALYZING CLASSIFICATION ERROR RATE INFLATION IN LNL,0.21610169491525424,T easy =  
A,0.21694915254237288,"1
A
...
1
A
...
...
...
1
A
...
1
A   B×A (12)"
A,0.21779661016949153,"and
222"
A,0.21864406779661016,T hard =
A,0.21949152542372882,""" 0
...
1
...
...
...
1
...
0 #"
A,0.22033898305084745,"B×A
(13)"
A,0.2211864406779661,"Especially, for T easy, we have Tij = 1"
A,0.22203389830508474,"A everywhere; for T hard, we denote as Hi : {argj(T hard
ji
=
223"
A,0.2228813559322034,"1)}A
i=1 the set of corresponding outlier classes j ∈Yout confused to inlier class i ∈Yin. Without
224"
A,0.22372881355932203,"loss of generality, we consider x1 with ‘easy’ open-set noise T easy and x2 with ‘hard’ open-set
225"
A,0.2245762711864407,"noise T hard. Please note, that we no longer require class concentration assumption here as the noise
226"
A,0.22542372881355932,"transition matrix is already known. In this condition, we have proved:
227"
A,0.22627118644067798,"Theorem 3.6 (‘Hard’ open-set noise vs ‘easy’ open-set noise). Let us consider sample x1, x2
228"
A,0.2271186440677966,"fulfilling eq. (7) and eq. (11).
We set the corresponding noise transition matrix as T 1
out =
229"
A,0.22796610169491524,"T easy, T 2
out = T hard, T 1
in = T 2
in = I and denote [p1
1, ..., p1
A, ..., p1
A+B] = [p2
1, ..., p2
A, ..., p2
A+B] =
230"
A,0.2288135593220339,"[p1, ..., pA, ..., pA+B]. Then, we have:
231"
A,0.22966101694915253,"• Fitted case:
∆Ex1 ≤∆Ex2."
A,0.2305084745762712,• Memorized case:
A,0.23135593220338982,"∆Ex1 −∆Ex2 = A
X"
A,0.23220338983050848,"i=1
aibi."
A,0.2330508474576271,"Here, ai = pi, bi = P
j∈Hi pj −1"
A,0.23389830508474577,"A
PA+B
i=A+1 pi.
232"
A,0.2347457627118644,"Please refer to appendix D.2 for detailed proof. Specifically, we further discuss about memo-
233"
A,0.23559322033898306,"rized case here. Since PA
i=1 bi = 0, PA
i=1 ai = 1, we can easily infer max(∆Ex1 −∆Ex2) ≥
234"
A,0.2364406779661017,"0, min(∆Ex1 −∆Ex2) ≤0. With theorem D.3, we know when the ranking of {p1
i }A
i=1 is completely
235"
A,0.23728813559322035,in agreement with the ranking {P
A,0.23813559322033898,"j∈Hi p1
j}A
i=1 (constant term −1"
A,0.23898305084745763,"A
PA+B
i=A+1 p1
i omitted here), we
236"
A,0.23983050847457626,"reach its maximum value with ∆Ex1 −∆Ex2 ≥0. Intuitively speaking, this implies a scenario that
237"
A,0.24067796610169492,"the ‘hard’ open-set noise tends to confuse a sample into the inlier class it primarily belongs to (with
238"
A,0.24152542372881355,"higher semantic similarity), as indicated by its higher probability (the higher the p1
i the higher the
239
P"
A,0.2423728813559322,"j∈Hi p1
j). For example, an outlier ‘tiger’ image is wrongly included as a ‘cat’ rather than a ‘dog’ in
240"
A,0.24322033898305084,"a ‘cat vs dog’ binary classification dataset. As this is more consistent with the common intuition, we
241"
A,0.2440677966101695,"default to such noise mode for ‘hard’ open-set noise — assuming the ranking of {p1
i }A
i=1 is of high
242"
A,0.24491525423728813,agreement with the ranking of {P
A,0.2457627118644068,"j∈Hi p1
j}A
i=1.
243"
A,0.24661016949152542,"To summarize, unlike the general comparison between open-set noise and closed-set noise, the ‘hard’
244"
A,0.24745762711864408,"open-set noise and the ‘easy’ open-set noise exhibit an opposite trend in two different cases. In the
245"
A,0.2483050847457627,"fitted case, ‘easy’ open-set noise appears to be less harmful, while in the memorized case, the impact
246"
A,0.24915254237288137,"of ‘hard’ open-set noise is comparatively smaller.
247"
RETHINKING OPEN-SET NOISE DETECTION,0.25,"3.5
Rethinking open-set noise detection
248"
RETHINKING OPEN-SET NOISE DETECTION,0.25084745762711863,"In this section, we try to investigate a commonly used open-set noise identification mechanism based
249"
RETHINKING OPEN-SET NOISE DETECTION,0.25169491525423726,"on entropy dynamics. Within the sample selection paradigm, several methods [1, 16] have proposed
250"
RETHINKING OPEN-SET NOISE DETECTION,0.25254237288135595,"to further identify open-set noise, based on the empirical phenomenon that samples with relatively
251"
RETHINKING OPEN-SET NOISE DETECTION,0.2533898305084746,"in-confident predictions are usually open-set samples, characterized by its high prediction entropy.
252"
RETHINKING OPEN-SET NOISE DETECTION,0.2542372881355932,"Specifically, we consider original sample x without noise transition, x with T hard and x with T easy
253"
RETHINKING OPEN-SET NOISE DETECTION,0.25508474576271184,"as a clean sample, a ‘hard’ open-set noise and an ‘easy’ open-set noise, respectively. For simplicity,
254"
RETHINKING OPEN-SET NOISE DETECTION,0.2559322033898305,"we omit the subscript.
255"
RETHINKING OPEN-SET NOISE DETECTION,0.25677966101694916,"Empirically, most sample selection method starts from the early training stages after certain epochs
256"
RETHINKING OPEN-SET NOISE DETECTION,0.2576271186440678,"of warm-up training, expecting the model to learn meaningful information before over-fitting. To
257"
RETHINKING OPEN-SET NOISE DETECTION,0.2584745762711864,"analyze the entropy dynamics, we thus consider the model predictions in the fitted case as a pragmatic
258"
RETHINKING OPEN-SET NOISE DETECTION,0.2593220338983051,"proxy. Let us denote as Heasy, Hhard and Hclean the prediction entropy corresponds to these three
259"
RETHINKING OPEN-SET NOISE DETECTION,0.26016949152542374,"conditions, we have3:
260"
RETHINKING OPEN-SET NOISE DETECTION,0.26101694915254237,"Hclean = H([
p1
PA
i=1 pi
, ...,
pA
PA
i=1 pi
])"
RETHINKING OPEN-SET NOISE DETECTION,0.261864406779661,"= H([p1 +
p1
PA
i=1 pi A+B
X"
RETHINKING OPEN-SET NOISE DETECTION,0.2627118644067797,"i=A+1
pi, ..., pA +
pA
PA
i=1 pi A+B
X"
RETHINKING OPEN-SET NOISE DETECTION,0.2635593220338983,"i=A+1
pi]),"
RETHINKING OPEN-SET NOISE DETECTION,0.26440677966101694,"Heasy = H([p1 + 1 A A+B
X"
RETHINKING OPEN-SET NOISE DETECTION,0.2652542372881356,"i=A+1
pi, ..., pA + 1 A A+B
X"
RETHINKING OPEN-SET NOISE DETECTION,0.26610169491525426,"i=A+1
pi]),"
RETHINKING OPEN-SET NOISE DETECTION,0.2669491525423729,"Hhard = H([p1 +
X"
RETHINKING OPEN-SET NOISE DETECTION,0.2677966101694915,"j∈H1
pj, ..., pA +
X"
RETHINKING OPEN-SET NOISE DETECTION,0.26864406779661015,"j∈HA
pj]). (14)"
RETHINKING OPEN-SET NOISE DETECTION,0.26949152542372884,"We note Heasy ≥Hclean4. However, comparing Hhard and Hclean is non-trivial without specific
261"
RETHINKING OPEN-SET NOISE DETECTION,0.27033898305084747,"values for each entry. Thus, we suggest open-set noise detection based on the prediction entropy may
262"
RETHINKING OPEN-SET NOISE DETECTION,0.2711864406779661,"only be effective for ‘easy’ open-set noise.
263"
EXPERIMENTS,0.27203389830508473,"4
Experiments
264"
EXPERIMENTS,0.27288135593220336,"In this section, we try to validate our theoretical findings. In section 4.1, we validate the theoretical
265"
EXPERIMENTS,0.27372881355932205,"comparisons of different label noise. In section 4.2, we validate the entropy dynamics with different
266"
EXPERIMENTS,0.2745762711864407,"label noise. Moreover, in appendix E.1, we revisit the performance of two existing LNL methods
267"
EXPERIMENTS,0.2754237288135593,"involving open-set noise. To conduct more controllable, fair and accurate experiments, we propose
268"
EXPERIMENTS,0.27627118644067794,"two synthetic open-set noisy datasets — CIFAR100-O and ImageNet-O, respectively based on
269"
EXPERIMENTS,0.2771186440677966,"the CIFAR100 and ImageNet datasets. We also consider closed-set noise in some experiments,
270"
EXPERIMENTS,0.27796610169491526,"particularly, the symmetric closed-set noise. Please refer to appendix A for more dataset and
271"
EXPERIMENTS,0.2788135593220339,"implementation details and also details about open-set detection protocol.
272"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.2796610169491525,"4.1
Empirical validation on previous probabilistic findings
273"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.2805084745762712,"In this section, we conduct experiments to validate the theorem 3.5 and theorem 3.6. Since most deep
274"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.28135593220338984,"models have sufficient capacity, we consider direct supervised learning from scratch on the noisy
275"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.28220338983050847,"dataset and consider the final model as the memorized case - as evidenced by nearly 100% train set
276"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.2830508474576271,"accuracy. Conversely, obtaining a model that perfectly fits the data distribution is often challenging;
277"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.2838983050847458,"here, we consider training a single-layer linear classifier upon a frozen pretrained encoder. Due to the
278"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.2847457627118644,"limited capacity of the linear layer, we expect to roughly approach the fitted case.
279"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.28559322033898304,"We show classification accuracy on CIFAR100-O and ImageNet-O datasets under different noise
280"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.2864406779661017,"ratios, as shown in fig. 3(a/b). We find that: 1) in both cases, the presence of open-set noise has
281"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.28728813559322036,"a significantly smaller impact on classification accuracy compared to closed-set noise. 2) ‘hard’
282"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.288135593220339,"open-set noise and ‘easy’ open-set noise show opposite trends in the two different scenarios. These
283"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.2889830508474576,"results align perfectly with our theoretical analysis.
284"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.28983050847457625,"In addition to closed-set classification accuracy, we also report the model’s open-set detection
285"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.29067796610169494,"performance using the maximum prediction value as the indicator [9]) in fig. 3(c/d). We find that, in
286"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.29152542372881357,"both cases, the presence of open-set noise leads to a degraded open-set detection performance, while
287"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.2923728813559322,"conversely, the presence of closed-set noise can often even enhance open-set detection performance.
288"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.29322033898305083,"In light of this contrasting trend, we propose that the open-set detection task, in addition to the default
289"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.2940677966101695,"closed-set classification, may help to offer a more comprehensive evaluation of LNL methods.
290"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.29491525423728815,"3Please refer to appendix D.2 for full derivation, specifically the eq. (36) and eq. (37).
4Please note, empirically the relative minority of open-set samples can also lead to low-confidence predictions,
which is beyond the scope of this work. We leave it to interested readers."
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.2957627118644068,"(a) Closed-set classification in fitted case
(b) Closed-set classification in memorized case"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.2966101694915254,"(c) Open-set detection in fitted case
(d) Open-set detection in memorized case"
EMPIRICAL VALIDATION ON PREVIOUS PROBABILISTIC FINDINGS,0.29745762711864404,Figure 3: Direct supervised training with different noise modes/ratios.
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.2983050847457627,"4.2
Inspecting entropy-based open-set noise detection mechanism
291"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.29915254237288136,"In section 3.5, we briefly analyze the open-set detection mechanism based on the entropy values of
292"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.3,"model predictions and find that it may be effective only for ‘easy’ open-set noise. Here, we again
293"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.3008474576271186,"utilize the CIFAR100-O and ImageNet-O datasets for validation experiments with different open-set
294"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.3016949152542373,"noise ratios and modes. Specifically, we adopt the common warm-up idea used in existing LNL
295"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.30254237288135594,"methods - training with the entire dataset for a certain number of epochs. We report the model’s
296"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.30338983050847457,"predicted entropy values for each sample at the {5th, 10th, 20th, 30th} epoch in fig. 4.
297"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.3042372881355932,"Epoch 5
Epoch 10
Epoch 20
Epoch 30
Epoch 5
Epoch 10
Epoch 20
Epoch 30"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.3050847457627119,"Epoch 5
Epoch 10
Epoch 20
Epoch 30
Epoch 5
Epoch 10
Epoch 20
Epoch 30"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.3059322033898305,(a) CIFAR100-O with 20% 'easy' open-set noise
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.30677966101694915,(c) ImageNet-O with 40% 'easy' open-set noise
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.3076271186440678,(b) CIFAR100-O with 20% 'hard' open-set noise
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.30847457627118646,(d) ImageNet-O with 40% 'hard' open-set noise
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.3093220338983051,Figure 4: Entropy dynamics w.r.t different datasets/noise modes/noise ratios.
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.3101694915254237,"We validate that the entropy dynamics is a more effective indicator for ‘easy’ open-set noise compared
298"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.31101694915254235,"to ‘hard’ open-set noise ((a) vs (b), (c) vs (d) in fig. 4). However, even for ‘easy’ open-set noise, we
299"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.31186440677966104,"also notice that the warm-up epoch matters a lot — too early (5th epoch in fig. 4(c)) or too late (30th
300"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.31271186440677967,"epoch in fig. 4(c)) also make open-set noise difficult to distinguish. We also test with mixed noise
301"
INSPECTING ENTROPY-BASED OPEN-SET NOISE DETECTION MECHANISM,0.3135593220338983,"including both open-set noise and closed-set noise, please refer to appendix B for more discussions.
302"
CONCLUSIONS,0.31440677966101693,"5
Conclusions
303"
CONCLUSIONS,0.3152542372881356,"This paper focuses on exploring how open-set label noise affects the performance of models. While
304"
CONCLUSIONS,0.31610169491525425,"the ‘open world’ setting involving open-set samples has been widely discussed in several other weakly
305"
CONCLUSIONS,0.3169491525423729,"supervised learning settings, its application in the context of learning with noisy labels has been
306"
CONCLUSIONS,0.3177966101694915,"understudied. In light of this, we reconsider the LNL problem, specifically focusing on the impact of
307"
CONCLUSIONS,0.31864406779661014,"open-set noise compared to closed-set noise, and different types of open-set noise compared to each
308"
CONCLUSIONS,0.31949152542372883,"other, on the evaluation performance. In light of the challenges existing testing frameworks face in
309"
CONCLUSIONS,0.32033898305084746,"handling open-set noise, we explore the open-set detection task to address the deficiencies in model
310"
CONCLUSIONS,0.3211864406779661,"evaluation for open-set noise and conducted preliminary experiments. Additionally, we look into
311"
CONCLUSIONS,0.3220338983050847,"the common mechanism for detecting open-set noise based on the model’s prediction entropy. Both
312"
CONCLUSIONS,0.3228813559322034,"theoretical and empirical results highlight the urgent need for a deeper exploration of open-set noise
313"
CONCLUSIONS,0.32372881355932204,"and its complex impact on model performance.
314"
REFERENCES,0.32457627118644067,"References
315"
REFERENCES,0.3254237288135593,"[1] Paul Albert, Diego Ortego, Eric Arazo, Noel E O’Connor, and Kevin McGuinness. Addressing
316"
REFERENCES,0.326271186440678,"out-of-distribution label noise in webly-labelled data. In Proceedings of the IEEE/CVF Winter
317"
REFERENCES,0.3271186440677966,"Conference on Applications of Computer Vision, pages 392–401, 2022. 2, 7
318"
REFERENCES,0.32796610169491525,"[2] Eric Arazo, Diego Ortego, Paul Albert, Noel O’Connor, and Kevin McGuinness. Unsupervised
319"
REFERENCES,0.3288135593220339,"label noise modeling and loss correction. In International Conference on Machine Learning,
320"
REFERENCES,0.32966101694915256,"pages 312–321. PMLR, 2019. 1, 2
321"
REFERENCES,0.3305084745762712,"[3] Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling:
322"
REFERENCES,0.3313559322033898,"Revisiting pseudo-labeling for semi-supervised learning. In Proceedings of the AAAI Conference
323"
REFERENCES,0.33220338983050846,"on Artificial Intelligence, volume 35, pages 6912–6920, 2021. 1
324"
REFERENCES,0.33305084745762714,"[4] Pengfei Chen, Junjie Ye, Guangyong Chen, Jingwei Zhao, and Pheng-Ann Heng. Beyond
325"
REFERENCES,0.3338983050847458,"class-conditional assumption: A primary attempt to combat instance-dependent label noise. In
326"
REFERENCES,0.3347457627118644,"Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11442–11450,
327"
REFERENCES,0.33559322033898303,"2021. 1, 2
328"
REFERENCES,0.3364406779661017,"[5] Chen Feng, Georgios Tzimiropoulos, and Ioannis Patras. Ssr: An efficient and robust framework
329"
REFERENCES,0.33728813559322035,"for learning with unknown label noise. In 33rd British Machine Vision Conference 2022, BMVC
330"
REFERENCES,0.338135593220339,"2022, London, UK, November 21-24, 2022. BMVA Press, 2022. 2, 19, 20
331"
REFERENCES,0.3389830508474576,"[6] Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep
332"
REFERENCES,0.3398305084745763,"neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31,
333"
REFERENCES,0.34067796610169493,"2017. 1, 2
334"
REFERENCES,0.34152542372881356,"[7] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adapta-
335"
REFERENCES,0.3423728813559322,"tion layer. 2016. 1, 2
336"
REFERENCES,0.3432203389830508,"[8] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
337"
REFERENCES,0.3440677966101695,"Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels.
338"
REFERENCES,0.34491525423728814,"arXiv preprint arXiv:1804.06872, 2018. 2
339"
REFERENCES,0.34576271186440677,"[9] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
340"
REFERENCES,0.3466101694915254,"examples in neural networks. In International Conference on Learning Representations, 2016.
341"
REFERENCES,0.3474576271186441,"8, 13
342"
REFERENCES,0.3483050847457627,"[10] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning
343"
REFERENCES,0.34915254237288135,"data-driven curriculum for very deep neural networks on corrupted labels. In International
344"
REFERENCES,0.35,"Conference on Machine Learning, pages 2304–2313. PMLR, 2018. 2, 12
345"
REFERENCES,0.35084745762711866,"[11] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as
346"
REFERENCES,0.3516949152542373,"semi-supervised learning. arXiv preprint arXiv:2002.07394, 2020. 1, 2, 12, 19, 20
347"
REFERENCES,0.3525423728813559,"[12] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database:
348"
REFERENCES,0.35338983050847456,"Visual learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017. 1,
349"
REFERENCES,0.35423728813559324,"12
350"
REFERENCES,0.3550847457627119,"[13] Eran Malach and Shai Shalev-Shwartz. Decoupling"" when to update"" from"" how to update"".
351"
REFERENCES,0.3559322033898305,"arXiv preprint arXiv:1706.02613, 2017. 2
352"
REFERENCES,0.35677966101694913,"[14] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
353"
REFERENCES,0.3576271186440678,"MIT press, 2018. 5
354"
REFERENCES,0.35847457627118645,"[15] Diego Ortego, Eric Arazo, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Multi-
355"
REFERENCES,0.3593220338983051,"objective interpolation training for robustness to label noise. In Proceedings of the IEEE/CVF
356"
REFERENCES,0.3601694915254237,"Conference on Computer Vision and Pattern Recognition, pages 6606–6615, 2021. 2, 12
357"
REFERENCES,0.3610169491525424,"[16] Ragav Sachdeva, Filipe R Cordeiro, Vasileios Belagiannis, Ian Reid, and Gustavo Carneiro.
358"
REFERENCES,0.36186440677966103,"Evidentialmix: Learning with combined open-set and closed-set noisy labels. In Proceedings of
359"
REFERENCES,0.36271186440677966,"the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3607–3615, 2021.
360"
REFERENCES,0.3635593220338983,"2, 7, 12
361"
REFERENCES,0.3644067796610169,"[17] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. Selfie: Refurbishing unclean samples for robust
362"
REFERENCES,0.3652542372881356,"deep learning. In International Conference on Machine Learning, pages 5907–5915. PMLR,
363"
REFERENCES,0.36610169491525424,"2019. 1, 2
364"
REFERENCES,0.36694915254237287,"[18] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao
365"
REFERENCES,0.3677966101694915,"Xia. Iterative learning with open-set noisy labels. In Proceedings of the IEEE conference on
366"
REFERENCES,0.3686440677966102,"computer vision and pattern recognition, pages 8688–8696, 2018. 2
367"
REFERENCES,0.3694915254237288,"[19] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross
368"
REFERENCES,0.37033898305084745,"entropy for robust learning with noisy labels. In Proceedings of the IEEE/CVF International
369"
REFERENCES,0.3711864406779661,"Conference on Computer Vision, pages 322–330, 2019. 1, 2
370"
REFERENCES,0.37203389830508476,"[20] Hongxin Wei, Lue Tao, Renchunzi Xie, and Bo An. Open-set label noise can improve robustness
371"
REFERENCES,0.3728813559322034,"against inherent label noise. Advances in Neural Information Processing Systems, 34:7978–7992,
372"
REFERENCES,0.373728813559322,"2021. 3
373"
REFERENCES,0.37457627118644066,"[21] Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, and Chao Chen. A
374"
REFERENCES,0.37542372881355934,"topological filter for learning with label noise. Advances in neural information processing
375"
REFERENCES,0.376271186440678,"systems, 33:21382–21393, 2020. 2
376"
REFERENCES,0.3771186440677966,"[22] Zhi-Fan Wu, Tong Wei, Jianwen Jiang, Chaojie Mao, Mingqian Tang, and Yu-Feng Li. Ngc: A
377"
REFERENCES,0.37796610169491524,"unified framework for learning with open-world noisy data. arXiv preprint arXiv:2108.11035,
378"
REFERENCES,0.3788135593220339,"2021. 2, 12
379"
REFERENCES,0.37966101694915255,"[23] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi
380"
REFERENCES,0.3805084745762712,"Sugiyama. Are anchor points really indispensable in label-noise learning? Advances in neural
381"
REFERENCES,0.3813559322033898,"information processing systems, 32, 2019. 1, 2
382"
REFERENCES,0.3822033898305085,"[24] Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi
383"
REFERENCES,0.38305084745762713,"Sugiyama. Sample selection with uncertainty of losses for learning with noisy labels. arXiv
384"
REFERENCES,0.38389830508474576,"preprint arXiv:2106.00445, 2021. 2
385"
REFERENCES,0.3847457627118644,"[25] Xiaobo Xia, Bo Han, Nannan Wang, Jiankang Deng, Jiatong Li, Yinian Mao, and Tongliang
386"
REFERENCES,0.3855932203389831,"Liu. Extended T: Learning with mixed closed-set and open-set noisy labels. IEEE Transactions
387"
REFERENCES,0.3864406779661017,"on Pattern Analysis and Machine Intelligence, 45(3):3047–3058, 2022. 2
388"
REFERENCES,0.38728813559322034,"[26] Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. Estimating
389"
REFERENCES,0.38813559322033897,"instance-dependent bayes-label transition matrix using a deep neural network. In International
390"
REFERENCES,0.3889830508474576,"Conference on Machine Learning, pages 25302–25312. PMLR, 2022. 1, 2
391"
REFERENCES,0.3898305084745763,"[27] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels.
392"
REFERENCES,0.3906779661016949,"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
393"
REFERENCES,0.39152542372881355,"pages 7017–7025, 2019. 2
394"
REFERENCES,0.3923728813559322,"[28] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does
395"
REFERENCES,0.39322033898305087,"disagreement help generalization against label corruption? In International Conference on
396"
REFERENCES,0.3940677966101695,"Machine Learning, pages 7164–7173. PMLR, 2019. 2
397"
REFERENCES,0.3949152542372881,"[29] Zhilu Zhang and Mert R Sabuncu. Generalized cross entropy loss for training deep neural
398"
REFERENCES,0.39576271186440676,"networks with noisy labels. arXiv preprint arXiv:1805.07836, 2018. 1, 2
399"
REFERENCES,0.39661016949152544,"[30] Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Robust curriculum learning: from clean label de-
400"
REFERENCES,0.3974576271186441,"tection to noisy label self-correction. In International Conference on Learning Representations,
401"
REFERENCES,0.3983050847457627,"2020. 2
402"
REFERENCES,0.39915254237288134,"A
Experiment details
403"
REFERENCES,0.4,"A.1
Dataset details
404"
REFERENCES,0.40084745762711865,"Previous works involving open-set noise also try to build synthetic noisy datasets, typically treating
405"
REFERENCES,0.4016949152542373,"different datasets as open-set noise for each other to construct synthetic noisy dataset [16, 22]. In
406"
REFERENCES,0.4025423728813559,"this scenario, potential domain gaps could impact a focused analysis of open-set noise. In this
407"
REFERENCES,0.4033898305084746,"work, we propose selecting inlier/outlier classes from the same dataset to avoid this issue. Besides,
408"
REFERENCES,0.40423728813559323,"in previous works, the consideration of open-set noise patterns often focused on random flipping
409"
REFERENCES,0.40508474576271186,"from outlier classes to all possible inlier classes, which is indeed the ‘easy’ open-set noise adopted
410"
REFERENCES,0.4059322033898305,"here. However, both our theoretical analysis and experimental findings demonstrate that ‘easy’
411"
REFERENCES,0.4067796610169492,"open-set noise and ‘hard’ open-set noise exhibit distinct characteristics. Therefore, relying solely
412"
REFERENCES,0.4076271186440678,"on experiments with ‘Easy’ open-set noise is insufficient, emphasizing the necessity to explore and
413"
REFERENCES,0.40847457627118644,"understand the complexities associated with different types of open-set noise. We also evaluate with
414"
REFERENCES,0.40932203389830507,"closed-set noise in some experiments, by default, we consider the common symmetric closed-set
415"
REFERENCES,0.4101694915254237,"noise in this work.
416"
REFERENCES,0.4110169491525424,"CIFAR100-O
For the original CIFAR100 dataset, in addition to the commonly-used 100 fine
417"
REFERENCES,0.411864406779661,"classes, there exist 20 coarse classes each consisting of 5 fine classes. To build CIFAR100-O, we
418"
REFERENCES,0.41271186440677965,"select one fine class from each coarse class as an inlier class (20 classes in total) while considering
419"
REFERENCES,0.4135593220338983,"the remaining classes as outlier classes (80 classes in total). Then, we consider ‘Hard’ and ‘Easy’
420"
REFERENCES,0.41440677966101697,"open-set noise as below:
421"
REFERENCES,0.4152542372881356,"• ‘Hard’: Randomly selected samples from the same coarse category as the target category
422"
REFERENCES,0.4161016949152542,"were introduced as open-set noise.
423"
REFERENCES,0.41694915254237286,"• ‘Easy’: Regardless of the target category, samples from the remaining categories were
424"
REFERENCES,0.41779661016949154,"randomly introduced as open-set noise.
425"
REFERENCES,0.4186440677966102,"ImageNet-O
For a more challenging benchmark, we consider ImageNet-1K datasets - consisting of
426"
REFERENCES,0.4194915254237288,"1,000 classes. Specifically, we randomly select 20 classes and artificially identify another 20 classes
427"
REFERENCES,0.42033898305084744,"similar to each of them:
428"
REFERENCES,0.4211864406779661,"inliers= [’tench’, ’great white shark’, ’cock’, ’indigo bunting’, ’European fire salamander’, ’African
429"
REFERENCES,0.42203389830508475,"crocodile’, ’barn spider’, ’macaw’, ’rock crab’, ’golden retriever’, ’wood rabbit’, ’gorilla’, ’abaya’,
430"
REFERENCES,0.4228813559322034,"’beer bottle’, ’bookcase’, ’cassette player’, ’coffee mug’, ’shopping basket’, ’trifle’, ’meat loaf’]
431"
REFERENCES,0.423728813559322,"outliers= [’goldfish’, ’tiger shark’, ’hen’, ’robin’, ’common newt’, ’American alligator’, ’garden
432"
REFERENCES,0.4245762711864407,"spider’, ’sulphur-crested cockatoo’, ’king crab’, ’Labrador retriever’, ’Angora’, ’chimpanzee’,
433"
REFERENCES,0.42542372881355933,"’academic gown’, ’beer glass’, ’bookshop’, ’CD player’, ’coffeepot’, ’shopping cart’, ’ice cream’,
434"
REFERENCES,0.42627118644067796,"’pizza’]
435"
REFERENCES,0.4271186440677966,"Then, we consider ‘Hard’ and ‘Easy’ open-set noise as below:
436"
REFERENCES,0.4279661016949153,"• ‘Hard’: Randomly select samples from the corresponding similar outlier class as the target
437"
REFERENCES,0.4288135593220339,"category were introduced as open-set noise.
438"
REFERENCES,0.42966101694915254,"• ‘Easy’: Samples from the remaining categories were randomly introduced as open-set noise.
439"
REFERENCES,0.43050847457627117,"For open-set detection, we directly use the corresponding test sets of these classes from the original
440"
REFERENCES,0.43135593220338986,"datasets.
441"
REFERENCES,0.4322033898305085,"WebVision
WebVision [12] is an extensive dataset comprising 1,000 classes of images obtained
442"
REFERENCES,0.4330508474576271,"through web crawling, which thus contains a large amount of open-set noise. In line with previous
443"
REFERENCES,0.43389830508474575,"studies [10, 11, 15], we evaluate our methods using the first 50 classes from the Google Subset of
444"
REFERENCES,0.4347457627118644,"WebVision. To test the performance of open-set detection on the WebVision dataset, we collect a
445"
REFERENCES,0.43559322033898307,"separate test set consisting of open-set images, following the same collection process as the WebVision
446"
REFERENCES,0.4364406779661017,"dataset. Specifically, we utilize the Google search engine with the class names as keywords and
447"
REFERENCES,0.43728813559322033,"identify those open-set samples that haven’t been included in the train set for this test set.
448"
REFERENCES,0.43813559322033896,"A.2
Implementation details
449"
REFERENCES,0.43898305084745765,"Here, we provide detailed implementation specifications for the fitted case and memorized case in
450"
REFERENCES,0.4398305084745763,"section 4.1. We also briefly the applied open-set detection protocol.
451"
REFERENCES,0.4406779661016949,"Fitted case
For the fitted case, we train a randomly initialized classifier - a single linear layer based
452"
REFERENCES,0.44152542372881354,"on the encoder of the ResNet18 model with pretrained weights. In the case of the CIFAR100-O
453"
REFERENCES,0.4423728813559322,"dataset, a weak augmentation strategy involving image padding and random cropping is applied
454"
REFERENCES,0.44322033898305085,"during training, with a batch size of 512. The weight decay (wd) is set to 0.0005, and the model
455"
REFERENCES,0.4440677966101695,"undergoes training for 100 epochs, utilizing a learning rate (lr) of 0.02. The learning rate schedule
456"
REFERENCES,0.4449152542372881,"follows a cosine annealing strategy.
457"
REFERENCES,0.4457627118644068,"For the ImageNet-O dataset, no augmentation is applied during training. The batch size is maintained
458"
REFERENCES,0.44661016949152543,"at 512, with a weight decay (wd) of 0.01. The model is trained for 100 epochs, employing a learning
459"
REFERENCES,0.44745762711864406,"rate (lr) of 0.02. The learning rate schedule for this case also adheres to a cosine annealing strategy.
460"
REFERENCES,0.4483050847457627,"Memorized case
In this case, we train a PreResNet18 model from scratch. For both datasets, a
461"
REFERENCES,0.4491525423728814,"weak augmentation strategy involving image padding and random cropping is applied during training,
462"
REFERENCES,0.45,"with a batch size of 128. The weight decay (wd) is set to 0.0005, and the model undergoes training
463"
REFERENCES,0.45084745762711864,"for 200 epochs, utilizing a learning rate (lr) of 0.02. The learning rate schedule also follows a cosine
464"
REFERENCES,0.4516949152542373,"annealing strategy.
465"
REFERENCES,0.45254237288135596,"Open-set detection protocol
We use the maximum softmax probability in [9] for the open-set
466"
REFERENCES,0.4533898305084746,"detection task. Specifically, assume the trained model f outputs a softmax vector pi for each sample
467"
REFERENCES,0.4542372881355932,"xi. We then choose a threshold value t between 0 and 1. For evaluation, we consider binary labels
468"
REFERENCES,0.45508474576271185,"indicating whether a sample belongs to a known class (closed-set) or the open-set and convert the
469"
REFERENCES,0.4559322033898305,"open-set detection task into a binary classification problem. Samples with a maximum softmax value
470"
REFERENCES,0.45677966101694917,"pmax
i
below the threshold are considered potential open-set samples. This is because a low maximum
471"
REFERENCES,0.4576271186440678,"value indicates the model is less confident in any specific class for that sample.
472"
REFERENCES,0.45847457627118643,"B
Entropy dynamics for mixed label noise
473"
REFERENCES,0.45932203389830506,"In addition to the open-set noise only scenario, we also inspect the entropy dynamics with mixed
474"
REFERENCES,0.46016949152542375,"label noise in fig. 5. Here, we use the notation ‘0.2all_0.5easy’ to represent a scenario where the
475"
REFERENCES,0.4610169491525424,"total noise ratio is 0.2, and within this, half of them are ’easy’ open-set noise. In the presence of
476"
REFERENCES,0.461864406779661,"mixed label noise, the existence of closed-set noise severely interferes with identifying open-set
477"
REFERENCES,0.46271186440677964,"noise. For example, in fig. 5(d), the entropy values of open-set noise even exceed those of clean
478"
REFERENCES,0.4635593220338983,"samples. Though not theoretically analyzed, this further suggests that entropy dynamics based on
479"
REFERENCES,0.46440677966101696,"model predictions, may be fragile, and we need to handle open-set noise more cautiously."
REFERENCES,0.4652542372881356,"Epoch 5
Epoch 10
Epoch 20
Epoch 30"
REFERENCES,0.4661016949152542,(a) CIFAR100-O 0.2all_0.5easy
REFERENCES,0.4669491525423729,"Epoch 5
Epoch 10
Epoch 20
Epoch 30"
REFERENCES,0.46779661016949153,(b) CIFAR100-O 0.2all_0.5hard
REFERENCES,0.46864406779661016,"Epoch 5
Epoch 10
Epoch 20
Epoch 30"
REFERENCES,0.4694915254237288,(c) ImageNet-O 0.4all_0.5easy
REFERENCES,0.4703389830508475,"Epoch 5
Epoch 10
Epoch 20
Epoch 30"
REFERENCES,0.4711864406779661,(d) ImageNet-O 0.4all_0.5hard
REFERENCES,0.47203389830508474,Figure 5: Entropy dynamics w.r.t mixed label noise. 480
REFERENCES,0.4728813559322034,"C
Error rate inflation in two different cases
481"
REFERENCES,0.47372881355932206,"In this section, we present the computation details of error rate inflation in two interested cases - fitted
482"
REFERENCES,0.4745762711864407,"case and memorized case. Specifically, we have:
483"
REFERENCES,0.4754237288135593,"• Fitted case:
484"
REFERENCES,0.47627118644067795,"Ex = (1 −P(y = arg maxkP n(y = k|x = x; y ∈Yin)|x = x; y ∈Yin)) · P(x = x; y ∈Yin).
(15)"
REFERENCES,0.47711864406779664,"• Memorized case:
485"
REFERENCES,0.47796610169491527,"Ex = (1 −P(y = arg maxkP yn(y = k|x = x; y ∈Yin)|x = x; y ∈Yin)) · P(x = x; y ∈Yin) =
X"
REFERENCES,0.4788135593220339,"yn∈Yin
(1 −P(y = yn|x = x; y ∈Yin))P n(y = yn|x = x; y ∈Yin) · P(x = x; y ∈Yin)"
REFERENCES,0.47966101694915253,"= [1 −
X"
REFERENCES,0.48050847457627116,"yn∈Yin
P(y = yn|x = x; y ∈Yin)P n(y = yn|x = x; y ∈Yin)] · P(x = x; y ∈Yin) (16)"
REFERENCES,0.48135593220338985,"While E∗
x denotes the Bayes optimal error rate:
486"
REFERENCES,0.4822033898305085,"E∗
x = (1 −maxkP(y = k|x = x; y ∈Yin)) · P(x = x; y ∈Yin).
(17)"
REFERENCES,0.4830508474576271,"We thus have ∆Ex in both cases as:
487"
REFERENCES,0.48389830508474574,"• Fitted case:
488"
REFERENCES,0.4847457627118644,∆Ex = [maxkP(y = k|x = x; y ∈Yin) −P(y = arg maxkP n(y = k|x = x; y ∈Yin)|x = x; y ∈Yin)]
REFERENCES,0.48559322033898306,"· P(x = x; y ∈Yin).
(18)"
REFERENCES,0.4864406779661017,"• Memorized case:
489"
REFERENCES,0.4872881355932203,"∆Ex = [maxkP(y = k|x = x; y ∈Yin) −
X"
REFERENCES,0.488135593220339,"yn∈Yin
P(y = yn|x = x; y ∈Yin)P n(y = yn|x = x; y ∈Yin)]"
REFERENCES,0.48898305084745763,· P(x = x; y ∈Yin). (19)
REFERENCES,0.48983050847457626,"Details on the derivation of error rate inflation (fig. 2)
Then, we describe the essential concepts
490"
REFERENCES,0.4906779661016949,"depicted in fig. 2 in detail. For better clarity, we here restate the notations in section 3.4. We explicitly
491"
REFERENCES,0.4915254237288136,"consider two specific sample points x1 and x2 being perturbed independently, corresponding to two
492"
REFERENCES,0.4923728813559322,"different label noise modes. Let us assume its clean conditional probability as:
493"
REFERENCES,0.49322033898305084,"P(y|x = x1; y ∈Yall) = [p1
1, ..., p1
A, ..., p1
A+B],"
REFERENCES,0.4940677966101695,"P(y|x = x2; y ∈Yall) = [p2
1, ..., p2
A, ..., p2
A+B],
(20)"
REFERENCES,0.49491525423728816,"and denote its noise transition matrix as T 1 = {T 1
ij}A+B
i,j=1 and T 2 = {T 2
ij}A+B
i,j=1, respectively. Here,
494"
REFERENCES,0.4957627118644068,"{T 1
ij = 0}, {T 2
ij = 0} for all j > A.
495"
REFERENCES,0.4966101694915254,"With eq. (1), we compute the corresponding noisy conditional probability for both samples as:
496"
REFERENCES,0.49745762711864405,"P n(y|x = x1; y ∈Yall) = [ A+B
X"
REFERENCES,0.49830508474576274,"i=1
p1
i T 1
i1, ..., A+B
X"
REFERENCES,0.49915254237288137,"i=1
piT 1
iA, 0, ..., 0],"
REFERENCES,0.5,"P n(y|x = x2; y ∈Yall) = [ A+B
X"
REFERENCES,0.5008474576271187,"i=1
p2
i T 2
i1, ..., A+B
X"
REFERENCES,0.5016949152542373,"i=1
p2
i T 2
iA, 0, ..., 0]. (21)"
REFERENCES,0.502542372881356,"Note that the error rate inflation is dependent on the clean conditional probability over inlier classes,
497"
REFERENCES,0.5033898305084745,"noisy conditional probability over inlier classes and sampling prior over inlier classes as shown in
498"
REFERENCES,0.5042372881355932,"eq. (18) and eq. (19).
499"
REFERENCES,0.5050847457627119,"Specifically, for sample x1, we have:
500"
REFERENCES,0.5059322033898305,"P(y = k|x = x1; y ∈Yin) =
P(y = k|x = x1; y ∈Yall)
P"
REFERENCES,0.5067796610169492,"i∈Yin P(y = i|x = x1; y ∈Yall) =
p1
k
PA
i=1 p1
i
,"
REFERENCES,0.5076271186440678,"P n(y = k|x = x1; y ∈Yin) =
P n(y = k|x = x1; y ∈Yall)
P"
REFERENCES,0.5084745762711864,"i∈Yin P n(y = i|x = x1; y ∈Yall) = A+B
X"
REFERENCES,0.5093220338983051,"i=1
p1
i T 1
ik,"
REFERENCES,0.5101694915254237,P(x = x1; y ∈Yin) =
REFERENCES,0.5110169491525424,"P
y∈Yin P(x = x1, y = y; y ∈Yall)
R P"
REFERENCES,0.511864406779661,"y∈Yin P(x = x, y = y; y ∈Yall)dx ∝
X"
REFERENCES,0.5127118644067796,"y∈Yin
P(x = x1, y = y; y ∈Yall) ∝
X"
REFERENCES,0.5135593220338983,"y∈Yin
P(y = y|x = x1; y ∈Yall)P(x = x1; y ∈Yall)"
REFERENCES,0.514406779661017,"P (x=x1;y∈Yall)=P (x=x2;y∈Yall)=δ
−−−−−−−−−−−−−−−−−−−−−−−−→ ∝
X"
REFERENCES,0.5152542372881356,"y∈Yin
P(y = y|x = x1; y ∈Yall) = A
X"
REFERENCES,0.5161016949152543,"i=1
p1
i . (22)"
REFERENCES,0.5169491525423728,"Simply changing the subscript leads us to the formulations for sample x2. To summarize, wrapping
501"
REFERENCES,0.5177966101694915,"the above together, we have:
502"
REFERENCES,0.5186440677966102,"P(y|x = x; y ∈Yin) = [
p1
PA
i=1 pi
, ...,
pA
PA
i=1 pi
],"
REFERENCES,0.5194915254237288,"P n(y|x = x; y ∈Yin) = [ A+B
X"
REFERENCES,0.5203389830508475,"i=1
piTi1, ..., A+B
X"
REFERENCES,0.5211864406779662,"i=1
piTiA],"
REFERENCES,0.5220338983050847,"P(x = x1; y ∈Yin) = A
X"
REFERENCES,0.5228813559322034,"i=1
pi. (23)"
REFERENCES,0.523728813559322,"We here omit the sample subscript and abbreviate the proportional symbol for simplicity. With
503"
REFERENCES,0.5245762711864407,"eq. (18), eq. (19) and eq. (23), we can then compute and compare ∆Ex in both fitted case and
504"
REFERENCES,0.5254237288135594,"memorized case:
505"
REFERENCES,0.5262711864406779,"∆Ex = max[p1, ..., pA] −parg max[PA+B
i=1
piTi1,...,PA+B
i=1
piTiA]
(Fitted case)
(24) 506"
REFERENCES,0.5271186440677966,"∆Ex = max[p1, ..., pA] − A
X"
REFERENCES,0.5279661016949152,"i=1
(pi · A+B
X"
REFERENCES,0.5288135593220339,"j=1
pjTji)
(Memorized case)
(25)"
REFERENCES,0.5296610169491526,"D
Full proof of theorem 3.5 and theorem 3.6
507"
REFERENCES,0.5305084745762711,"Error rate inflation comparison s.t. same noise ratio
To ensure a fair comparison, in this work,
508"
REFERENCES,0.5313559322033898,"we focus on the impact of different label noise given the same noise ratio - modifying Ox and Cx
509"
REFERENCES,0.5322033898305085,"while analyzing the trend of ∆Ex. Specifically, for above mentioned x1 and x2, we further assume:
510"
REFERENCES,0.5330508474576271,"Ox1 + Cx1 = Ox2 + Cx2 = δ.
(26)
which leads us to:
511 A+B
X"
REFERENCES,0.5338983050847458,"i=A+1
p1
i + A
X i=1 A
X"
REFERENCES,0.5347457627118644,"j=1,j̸=i
T 1
ijp1
i = A+B
X"
REFERENCES,0.535593220338983,"i=A+1
p2
i + A
X i=1 A
X"
REFERENCES,0.5364406779661017,"j=1,j̸=i
T 2
ijp2
i −→ A
X"
REFERENCES,0.5372881355932203,"i=1
T 1
iip1
i = A
X"
REFERENCES,0.538135593220339,"i=1
T 2
iip2
i
(27)"
REFERENCES,0.5389830508474577,"Please note, here the clean conditional probability is considered as known and fixed, while eq. (27)
512"
REFERENCES,0.5398305084745763,"restricts the values of the noise transition matrix T 1 and T 2, given specific clean conditional
513"
REFERENCES,0.5406779661016949,"probability. We then analyze and compare the error rate inflation in both conditions.
514"
REFERENCES,0.5415254237288135,"D.1
Proof of theorem 3.5 — Open-set noise vs Closed-set noise
515"
REFERENCES,0.5423728813559322,"In this section, we try to compare open-set noise and closed-set noise. Without loss of generality, we
516"
REFERENCES,0.5432203389830509,"consider:
517"
REFERENCES,0.5440677966101695,"Ox1 > Ox2.
(28)"
REFERENCES,0.5449152542372881,"Intuitively speaking, sample x1 is more affected by open-set noise compared to sample x2, thus
518"
REFERENCES,0.5457627118644067,"corresponding to the interested ‘open-set noise’.
519"
REFERENCES,0.5466101694915254,"As clarified by the toy example in section 3.4.1, without extra regularizations, the noise transition
520"
REFERENCES,0.5474576271186441,"matrix is not identifiable. We thus consider a simple compromise situation - in most classification
521"
REFERENCES,0.5483050847457627,"problems, the majority of samples (with a high probability) belong to a specific class exclusively with
522"
REFERENCES,0.5491525423728814,"high probability.
523"
REFERENCES,0.55,"Let us denote:
a = arg max
i
P(y = i|x = x1; y ∈Yall)"
REFERENCES,0.5508474576271186,"and
b = arg max
i
P(y = i|x = x2; y ∈Yall)."
REFERENCES,0.5516949152542373,"We assume :
p1
a →1, {p1
i →0}i̸=a, p2
b →1, {p2
i →0}i̸=b,"
REFERENCES,0.5525423728813559,"and we have: Ox1 = A+B
X"
REFERENCES,0.5533898305084746,"i=A+1
p1
i , Ox2 = A+B
X"
REFERENCES,0.5542372881355933,"i=A+1
p2
i ."
REFERENCES,0.5550847457627118,"With eq. (28), we easily infer that: a ∈Yout while b ∈Yin. Intuitively speaking, x1 is an open-set
524"
REFERENCES,0.5559322033898305,"noise, with its clean conditional probability concentrated on one of the outlier classes, and vice versa
525"
REFERENCES,0.5567796610169492,"for x2.
526"
REFERENCES,0.5576271186440678,"With eq. (27), we further have: A
X"
REFERENCES,0.5584745762711865,"i=1
T 1
iip1
i ≈ A
X"
REFERENCES,0.559322033898305,"i=1
T 1
ii × 0 ≈0, A
X"
REFERENCES,0.5601694915254237,"i=1
T 2
iip2
i ≈ A
X"
REFERENCES,0.5610169491525424,"i=1,i̸=b
T 2
ii × 0 + T 2
bb × 1 ≈T 2
bb."
REFERENCES,0.561864406779661,"Thus we have: T 2
bb ≈0, which enables us to analyze and compare ∆Ex1 and ∆Ex2:
527"
REFERENCES,0.5627118644067797,"Fitted case
In this case, according to eq. (24), we have:
528"
REFERENCES,0.5635593220338984,"∆Ex1 = max[p1
1, ..., p1
A] −parg max[PA+B
i=1
p1
i T 1
i1,...,PA+B
i=1
p1
i T 1
iA]"
REFERENCES,0.5644067796610169,"< max[p1
1, ..., p1
A] −min[p1
1, ..., p1
A]"
REFERENCES,0.5652542372881356,"p1
a→1,{p1
i →0}i̸=a,a∈Yout
−−−−−−−−−−−−−−−−→
≈0, (29) 529"
REFERENCES,0.5661016949152542,"∆Ex2 = max[p2
1, ..., p2
A] −parg max[PA+B
i=1
p2
i T 2
i1,...,PA+B
i=1
p2
i T 2
iA]"
REFERENCES,0.5669491525423729,"[PA+B
i=1
p2
i T 2
i1,...,PA+B
i=1
p2
i T 2
iA]≈[T 2
a1,T 2
a2,...,"
REFERENCES,0.5677966101694916,"b
z}|{
0
,...,T 2
aA]
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→"
REFERENCES,0.5686440677966101,"= p2
b −p2
n"
REFERENCES,0.5694915254237288,"p2
b→1,{p2
i →0}i̸=b,b∈Yin,n̸=b
−−−−−−−−−−−−−−−−−−−→
≈1. (30)"
REFERENCES,0.5703389830508474,"Memorized case
In this case, according to eq. (25), we similarly have:
530"
REFERENCES,0.5711864406779661,"∆Ex1 = max[p1
1, ..., p1
A] − A
X"
REFERENCES,0.5720338983050848,"i=1
(p1
i · A+B
X"
REFERENCES,0.5728813559322034,"j=1
p1
jT 1
ji) ≈0,
(31)"
REFERENCES,0.573728813559322,"∆Ex2 = max[p2
1, ..., p2
A] − A
X"
REFERENCES,0.5745762711864407,"i=1
(p2
i · A+B
X"
REFERENCES,0.5754237288135593,"j=1
p2
jT 2
ji) ≈1.
(32)"
REFERENCES,0.576271186440678,"We wrap up above for theorem D.2:
531"
REFERENCES,0.5771186440677966,"Theorem D.1 (Open-set noise vs Closed-set noise). Let us consider sample x1, x2 fulfilling eq. (26)
and eq. (28) - compared to x2, x1 is considered as more prone to open-set noise. Let us denote
a = arg maxi P(y = i|x = x1; y ∈Yall) and b = arg maxi P(y = i|x = x2; y ∈Yall), we
assume (with a high probability): p1
a →1, {p1
i →0}i̸=a and p2
b →1, {p2
b →0}i̸=b. Then, we have:"
REFERENCES,0.5779661016949152,∆Ex1 < ∆Ex2
REFERENCES,0.5788135593220339,"in both fitted case and memorized case.
532"
REFERENCES,0.5796610169491525,"D.2
Derivation of theorem 3.5 — ‘hard’ open-set noise vs ‘easy’ open-set noise
533"
REFERENCES,0.5805084745762712,"In this part, we try to analyze and compare ‘hard’ open-set noise with ‘easy’ open-set noise. For
534"
REFERENCES,0.5813559322033899,"better clarification, we repeat here the essential statements:
535"
REFERENCES,0.5822033898305085,"T 1
out = T easy =  "
A,0.5830508474576271,"1
A
...
1
A
...
...
...
1
A
...
1
A   B×A (33)"
A,0.5838983050847457,"and
536"
A,0.5847457627118644,"T 2
out = T hard ="
A,0.5855932203389831,""" 0
...
1
...
...
...
1
...
0 #"
A,0.5864406779661017,"B×A
(34)"
A,0.5872881355932204,"and
537"
A,0.588135593220339,"T 1
in = T 2
in = I.
(35)"
A,0.5889830508474576,"Especially, for T easy, we have Tij = 1"
A,0.5898305084745763,"A everywhere; for T hard, we denote as Hi : {argj(T hard
ji
=
1)}A
i=1 the set of corresponding outlier classes j ∈Yout confused to inlier class i ∈Yin. We also
have:
[p1
1, ..., p1
A, ..., p1
A+B] = [p2
1, ..., p2
A, ..., p2
A+B] .
538"
A,0.5906779661016949,"Fitted case
In this case, according to eq. (24), for sample x1 with ‘easy’ open-set noise, we have:
539"
A,0.5915254237288136,"∆Ex1 = max[p1
1, ..., p1
A] −parg max[PA+B
i=1
p1
i T 1
i1,...,PA+B
i=1
p1
i T 1
iA]"
A,0.5923728813559322,"= max[p1
1, ..., p1
A] −parg max[p1
1+ 1"
A,0.5932203389830508,"A
PA+B
i=A+1 p1
i ,...,p1
A+ 1"
A,0.5940677966101695,"A
PA+B
i=A+1 p1
i ]
= 0, (36)"
A,0.5949152542372881,"and, for sample x2 with ‘hard’ open-set noise, we have:
540"
A,0.5957627118644068,"∆Ex2 = max[p2
1, ..., p2
A] −parg max[PA+B
i=1
p2
i T 2
i1,...,PA+B
i=1
p2
i T 2
iA]"
A,0.5966101694915255,"= max[p2
1, ..., p2
A] −parg max[p2
1+P"
A,0.597457627118644,"b∈H1 p2
b,...,p2
A+P"
A,0.5983050847457627,"b∈HA p2
b]"
A,0.5991525423728814,"∈[0, max[p2
1, ..., p2
A] −min[p2
1, ..., p2
A]]. (37)"
A,0.6,"Memorized case
In this case, according to eq. (25), for sample x1 with ‘easy’ open-set noise, we
541"
A,0.6008474576271187,"have:
542"
A,0.6016949152542372,"∆Ex1 = max[p1
1, ..., p1
A] − A
X"
A,0.6025423728813559,"i=1
(p1
i · A+B
X"
A,0.6033898305084746,"j=1
p1
jT 1
ji)"
A,0.6042372881355932,"= max[p1
1, ..., p1
A] − A
X"
A,0.6050847457627119,"i=1
p1
i (p1
i + 1 A A+B
X"
A,0.6059322033898306,"i=A+1
p1
i ). (38)"
A,0.6067796610169491,"and, for sample x2 with ‘hard’ open-set noise, we have:
543"
A,0.6076271186440678,"∆Ex2 = max[p2
1, ..., p2
A] − A
X"
A,0.6084745762711864,"i=1
(p2
i · A+B
X"
A,0.6093220338983051,"j=1
p2
jT 2
ji)"
A,0.6101694915254238,"= max[p2
1, ..., p2
A] − A
X"
A,0.6110169491525423,"i=1
p2
i (p2
i +
X"
A,0.611864406779661,"j∈Hi
p2
j) (39)"
A,0.6127118644067797,We further have:
A,0.6135593220338983,"∆Ex1 −∆Ex2 = A
X"
A,0.614406779661017,"i=1
p1
i (
X"
A,0.6152542372881356,"j∈Hi
p1
j −1 A A+B
X"
A,0.6161016949152542,"i=A+1
p1
i )."
A,0.6169491525423729,"Let ai = p1
i , bi = P"
A,0.6177966101694915,"j∈Hi p1
j −1"
A,0.6186440677966102,"A
PA+B
i=A+1 p1
i , we have:"
A,0.6194915254237288,"∆Ex1 −∆Ex2 = A
X"
A,0.6203389830508474,"i=1
aibi."
A,0.6211864406779661,"To summarize, we wrap up the above together:
544"
A,0.6220338983050847,"Theorem D.2 (‘Hard’ open-set noise vs ‘easy’ open-set noise). Let us consider sample x1, x2
545"
A,0.6228813559322034,"fulfilling eq. (26) and eq. (11). We set the corresponding noise transition matrix as in eq. (33), eq. (34)
546"
A,0.6237288135593221,"and eq. (35). We further assume [p1
1, ..., p1
A, ..., p1
A+B] = [p2
1, ..., p2
A, ..., p2
A+B]. Then, we have:
547"
A,0.6245762711864407,"∆Ex1 ≤∆Ex2
in fitted case,"
A,0.6254237288135593,"∆Ex1 −∆Ex2 = A
X"
A,0.6262711864406779,"i=1
aibi"
A,0.6271186440677966,"in memorized case. Here, ai = p1
i , bi = P"
A,0.6279661016949153,"j∈Hi p1
j −1"
A,0.6288135593220339,"A
PA+B
i=A+1 p1
i .
548"
A,0.6296610169491526,"Theorem D.3 (Rearrangement Inequality). For the sequences a1, a2, . . . , an and b1, b2, . . . , bn,
where a1 ≤a2 ≤. . . ≤an and b1 ≤b2 ≤. . . ≤bn, the rearrangement inequality is given by:"
A,0.6305084745762712,a1·b1+a2·b2+. . .+an·bn ≥a1·bσ(1)+a2·bσ(2)+. . .+an·bσ(n) ≥a1·bn+a2·bn−1+. . .+an·b1
A,0.6313559322033898,"Here, σ denotes a permutation of the indices 1, 2, . . . , n. The leftmost expression corresponds to the
549"
A,0.6322033898305085,"case where σ(i) = i (identity permutation), and the rightmost expression corresponds to the case
550"
A,0.6330508474576271,"where σ(i) = n + 1 −i (reverse permutation).
551"
A,0.6338983050847458,"E
Revisiting LNL methods
552"
A,0.6347457627118644,"E.1
Revisiting existing LNL methods with open-set noise
553"
A,0.635593220338983,"In this section, we further investigate the learning effectiveness of existing LNL methods on previously
554"
A,0.6364406779661017,"discussed open-set label noise, especially the dominant ones based on sample selection - these methods
555"
A,0.6372881355932203,"often integrate different regularization terms and off-the-shelf techniques, resulting in state-of-the-art
556"
A,0.638135593220339,"performance. In essence, such methods typically include a sample selection module along with a
557"
A,0.6389830508474577,"robust training module. Here, we briefly denote the clean subset selected by the original method
558"
A,0.6398305084745762,"as Xclean and denote the entire dataset as Xall. Moreover, we consider integrating the previously
559"
A,0.6406779661016949,"mentioned open-set detection mechanism into current LNL methods - we denote as Xin an inlier
560"
A,0.6415254237288136,"subset based on entropy dynamics. Then, maintaining the robust training module unchanged, we
561"
A,0.6423728813559322,"consider below three different variants (the involved LNL method abbreviated as X, the inlier subset
562"
A,0.6432203389830509,"detection method abbreviated as EntSel):
563"
A,0.6440677966101694,"• X: Robust training using Xclean, i.e., the original method;
564"
A,0.6449152542372881,"• EntSel: Robust training using Xin;
565"
A,0.6457627118644068,"• X + EntSel: Robust training using Xin ∩Xclean.
566"
A,0.6466101694915254,"Specifically, we test with two representative LNL methods with well-maintained open-source imple-
567"
A,0.6474576271186441,"mentations: SSR [5] and DivideMix [11]. Please refer to appendix E.2 for more details. In fig. 6, we
568"
A,0.6483050847457628,"show results on CIFAR100-O and ImageNet-O.
569"
A,0.6491525423728813,"0.2
0.2
0.4
0.4
0.2
0.2
0.4
0.4
Easy
Hard ra
ro"
A,0.65,"0.2
0.2
0.4
0.4
0.2
0.2
0.4
0.4
Easy
Hard ra
ro"
A,0.6508474576271186,"0.2
0.2
0.4
0.4
0.2
0.2
0.4
0.4
Easy
Hard ra
ro"
A,0.6516949152542373,"0.2
0.2
0.4
0.4
0.2
0.2
0.4
0.4
Easy
Hard ra
ro"
A,0.652542372881356,"0.2
0.2
0.4
0.4
0.2
0.2
0.4
0.4
Easy
Hard ra
ro"
A,0.6533898305084745,"0.2
0.2
0.4
0.4
0.2
0.2
0.4
0.4
Easy
Hard ra
ro"
A,0.6542372881355932,"0.2
0.2
0.4
0.4
0.2
0.2
0.4
0.4
Easy
Hard ra
ro"
A,0.6550847457627119,"0.2
0.2
0.4
0.4
0.2
0.2
0.4
0.4
Easy
Hard ra
ro"
A,0.6559322033898305,"Figure 6: Evaluation of directly supervised training with different noise modes/ratios. First row:
Closed-set classification accuracy; Second row: Open-set detection ROC AUC."
A,0.6567796610169492,"First, focusing on the classification accuracy of the model, we observe that 1) using EntSel instead of
570"
A,0.6576271186440678,"the original method leads to a reduction in classification accuracy in the mixed noise scenario (SSR
571"
A,0.6584745762711864,"vs EntSel and DivideMix vs EntSel); in pure open-set noise only scenarios, there are no obvious
572"
A,0.6593220338983051,"trends showing differences in different variant models. 2) the classification accuracy for mixed noise
573"
A,0.6601694915254237,"is significantly lower than that of only open-set noise at the same noise ratio, which further confirms
574"
A,0.6610169491525424,"that closed-set noise is more harmful than open-set noise.
575"
A,0.661864406779661,"Furthermore, we demonstrate the performance of this model in detecting open-set samples - the
576"
A,0.6627118644067796,"introduction of EntSel significantly enhances the effectiveness of open-set detection, especially
577"
A,0.6635593220338983,"when the open-set noise is set to ‘easy’ mode. This also further confirms our theoretical analysis in
578"
A,0.6644067796610169,section 3.5 and experimental results in section 4.2.
A,0.6652542372881356,Table 1: Results on WebVision dataset.
A,0.6661016949152543,"Method
Accuracy (%)
ROC AUC (%)"
A,0.6669491525423729,"SSR
77.48
80.84
EntSel
77.08
85.43
SSR + EntSel
76.04
79.90"
A,0.6677966101694915,"DivideMix
74.08
86.39
EntSel
62.96
81.66
DivideMix + EntSel
58.94
83.85 579"
A,0.6686440677966101,"We report results for the WebVision dataset in table 1, reaffirming that combining ‘EntSel’ with ‘SSR’
580"
A,0.6694915254237288,"significantly enhances open-set detection performance. Notably, most open-set noise in WebVision
581"
A,0.6703389830508475,"seems to arise from factors like text co-occurrence rather than semantic similarity, categorizing
582"
A,0.6711864406779661,"it more as ‘easy’ open-set noise. This may explain why EntSel effectively improves open-set
583"
A,0.6720338983050848,"detection in this context. However, when combining EntSel with DivideMix, both classification
584"
A,0.6728813559322034,"accuracy and open-set detection decrease, indicating that the robustness of the EntSel method itself is
585"
A,0.673728813559322,"questionable. Additionally, simply merging SSR/DivideMix with EntSel using subset intersection (X
586"
A,0.6745762711864407,"+ EntSel) also leads to a decrease in both classification accuracy and open-set detection performance.
587"
A,0.6754237288135593,"Finally, it’s worth mentioning that, despite having lower classification accuracy than SSR, DivideMix
588"
A,0.676271186440678,"outperforms SSR in open-set detection ROC AUC scores. All above illustrates that simply evaluating
589"
A,0.6771186440677966,"the classification accuracy may be one-sided.
590"
A,0.6779661016949152,"E.2
Details of involved methods
591"
A,0.6788135593220339,"DivideMix
[11] Denoting as L = {li}N
i=1 the losses of all samples, DivideMix proposes to model
592"
A,0.6796610169491526,"it (after min-max normalization) with a Gaussian Mixture Model. The probabilities {pi}N
i=1 of each
593"
A,0.6805084745762712,"sample belonging to the component with a smaller mean value are then extracted. Samples with
594"
A,0.6813559322033899,"probability pi greater than the threshold θ are then identified as a “clean"" subset. Link to code:
595"
A,0.6822033898305084,"https://github.com/LiJunnan1992/DivideMix.
596"
A,0.6830508474576271,"SSR
[5] In contrast to DivideMix, SSR extracts features for each sample and constructs a neigh-
597"
A,0.6838983050847458,"bourhood graph. By computing the nearest neighbour labels for each sample, a pseudo-label
598"
A,0.6847457627118644,"distribution p is obtained through a KNN voting process. The consistency c = py/pmax between
599"
A,0.6855932203389831,"this voted distribution and the given noisy label y (logit label) is then calculated. Samples with
600"
A,0.6864406779661016,"consistency c greater than the threshold θ are identified as part of the “clean"" subset. Link to code:
601"
A,0.6872881355932203,"https://github.com/MrChenFeng/SSR_BMVC2022.
602"
A,0.688135593220339,"EntSel
We also provide a concise overview of the steps involved in EntSel, following a methodology
603"
A,0.6889830508474576,"similar to DivideMix. Denoting as E = {ei}N
i=1 the entropy of all samples’ predictions, we similarly
604"
A,0.6898305084745763,"model it (after min-max normalization) with a Gaussian Mixture Model. The probabilities {pi}N
i=1
605"
A,0.690677966101695,"of each sample belonging to the component with a smaller mean value are then extracted. Samples
606"
A,0.6915254237288135,"with probability pi greater than the threshold θ′ are then identified as “inlier"" subset.
607"
A,0.6923728813559322,"Generally, we have a closed-set classifier g and an encoder f, and we use it for training based on
608"
A,0.6932203389830508,"the selected subset. Existing sample selection methods usually rely on an estimated prediction
609"
A,0.6940677966101695,"and a threshold to help filter clean samples. Our proposed OpenAdaptor focuses on the difference
610"
A,0.6949152542372882,"between open-set and closed-set samples. When integrating them, we propose two different strategies:
611"
A,0.6957627118644067,"absorption and exclusion.
612"
A,0.6966101694915254,"E.3
Implementation details
613"
A,0.6974576271186441,"Experiment details
For both SSR and DivideMix, we employ model and optimization configu-
614"
A,0.6983050847457627,"rations on the same dataset. Specifically, for CIFAR100-O and ImageNet-O, we utilize the Pres-
615"
A,0.6991525423728814,"ResNet18 model, trained for 300 epochs with a batch size of 128 and a learning rate of 0.02, and a
616"
A,0.7,"cosine annealing schedule was implemented. For the WebVision dataset, we utilize the ResNet18
617"
A,0.7008474576271186,"model, training for 120 epochs with a reduced batch size of 32. The learning rate is set to 0.01 and
618"
A,0.7016949152542373,"controlled by a cosine annealing scheduler too. Additionally, a warm-up training phase of 10 epochs
619"
A,0.7025423728813559,"is implemented in the CIFAR100-O and ImageNet-O experiments, while a 5-epoch warm-up training
620"
A,0.7033898305084746,"phase is utilized in the WebVision experiment.
621"
A,0.7042372881355933,"Hyperparameters
In all experiments, we set the sample selection threshold θ′ = 0.5 for EntSel.
622"
A,0.7050847457627119,"For SSR, we employ a sample selection threshold θ = 1.0 in all experiments. For DivideMix,
623"
A,0.7059322033898305,"the sample selection threshold remains constant at θ = 0.5 across all experiments. Both SSR and
624"
A,0.7067796610169491,"DivideMix incorporate MixUp, and we adhere to the original paper’s choices by setting the MixUp
625"
A,0.7076271186440678,"coefficient to 4 for experiments on CIFAR100-O and ImageNet-O and to 0.5 for experiments on
626"
A,0.7084745762711865,"WebVision. Please note, as exploring and comparing these methods are not our focus, we believe
627"
A,0.7093220338983051,"there exist better hyperparameter settings.
628"
A,0.7101694915254237,"Robustness of EntSel
A smaller θ′ for EntSel leads to better performance on WebVision - especially
629"
A,0.7110169491525423,"when EntSel is used with DivideMix. If we set θ′ = 0.2, our classification accuracy increases from
630"
A,0.711864406779661,"62.96% to 67.2%, ROC AUC increases from 0.8166 to 0.8599 (table 1). However, we are keen to use
631"
A,0.7127118644067797,"fixed hyperparameters in all experiments as we emphasize that the hyperparameter robustness is also
632"
A,0.7135593220338983,"critical for LNL methods.
633"
A,0.714406779661017,"F
More examples of open-set noise in WebVision dataset
634"
A,0.7152542372881356,"In this section, we present additional examples of open-set noise within the ‘Tench’ class of the
635"
A,0.7161016949152542,"WebVision dataset. We trace the origin of web pages containing some open-set noise images.
636"
A,0.7169491525423729,"Remarkably, we identify that the appearance of the term ‘Tench’ or related keywords is prevalent
637"
A,0.7177966101694915,"on the web pages hosting these open-set noise images. We posit that this occurrence is attributed
638"
A,0.7186440677966102,"to the data collection process on the web. Specifically, in the course of keyword searches and
639"
A,0.7194915254237289,"crawling for images, instances were inadvertently included due to the presence of keywords in image
640"
A,0.7203389830508474,"descriptions or accompanying text, such as people with ‘tench’ in the name, or related fishing tools.
641"
A,0.7211864406779661,"As highlighted earlier, the prevalent belief in the current LNL community is that real-world noise
642"
A,0.7220338983050848,"primarily arises from confusion induced by semantic similarity. Consequently, numerous recent
643"
A,0.7228813559322034,"studies have concentrated on instance-dependent noise and related theoretical analysis. However, our
644"
A,0.7237288135593221,"findings here indicate that in real-world scenarios, particularly in web-crawled datasets, noise may
645"
A,0.7245762711864406,"be unrelated to semantics but instead caused by other latent high-dimensional information, such
646"
A,0.7254237288135593,"as accompanying text here. Addressing such real-world noise requires increased attention and
647"
A,0.726271186440678,further exploration.
A,0.7271186440677966,"0lv77S5PaW5vlM.jpg
http://ukscblog.com/ussc-v-uksc/"
A,0.7279661016949153,fIfzvcQnea4zsM.jpg
A,0.7288135593220338,m59l4cCfDgox6M.jpg
A,0.7296610169491525,X051MdLtrztWqM.jpg
A,0.7305084745762712,usK0NyB0Q9VsVM.jpg
A,0.7313559322033898,https://charlyanderic.travellerspoint.com/67/
A,0.7322033898305085,https://www.drennantackle.com/drennan-distance-specialist-tench-bream-12ft-2lb-rod/
A,0.7330508474576272,https://en.wikipedia.org/wiki/You_Should_Be_So_Lucky
A,0.7338983050847457,http://pete777-pete777.blogspot.com/2012/04/tench-rigs-27th-april.html
A,0.7347457627118644,"Image
Source"
A,0.735593220338983,"Figure 7:
Open-set noise examples in class ‘Tench’ of WebVision dataset with path:
/google/q0001/. The source images are resized to fit the layout. Please note that the web links
here are obtained in May 2024, and there is no guarantee that they will always be valid in the future. 648"
A,0.7364406779661017,"NeurIPS Paper Checklist
649"
CLAIMS,0.7372881355932204,"1. Claims
650"
CLAIMS,0.738135593220339,"Question: Do the main claims made in the abstract and introduction accurately reflect the
651"
CLAIMS,0.7389830508474576,"paper’s contributions and scope?
652"
CLAIMS,0.7398305084745763,"Answer: [Yes]
653"
CLAIMS,0.7406779661016949,"Justification: We clearly and briefly describe our method and our contributions in the abstract
654"
CLAIMS,0.7415254237288136,"and introduction sections.
655"
CLAIMS,0.7423728813559322,"Guidelines:
656"
CLAIMS,0.7432203389830508,"• The answer NA means that the abstract and introduction do not include the claims
657"
CLAIMS,0.7440677966101695,"made in the paper.
658"
CLAIMS,0.7449152542372881,"• The abstract and/or introduction should clearly state the claims made, including the
659"
CLAIMS,0.7457627118644068,"contributions made in the paper and important assumptions and limitations. A No or
660"
CLAIMS,0.7466101694915255,"NA answer to this question will not be perceived well by the reviewers.
661"
CLAIMS,0.747457627118644,"• The claims made should match theoretical and experimental results, and reflect how
662"
CLAIMS,0.7483050847457627,"much the results can be expected to generalize to other settings.
663"
CLAIMS,0.7491525423728813,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
664"
CLAIMS,0.75,"are not attained by the paper.
665"
LIMITATIONS,0.7508474576271187,"2. Limitations
666"
LIMITATIONS,0.7516949152542373,"Question: Does the paper discuss the limitations of the work performed by the authors?
667"
LIMITATIONS,0.752542372881356,"Answer: [Yes]
668"
LIMITATIONS,0.7533898305084745,"Justification: In section 5 We specifically discuss the potential and limitations of current
669"
LIMITATIONS,0.7542372881355932,"LNL method in learning with open-set noise.
670"
LIMITATIONS,0.7550847457627119,"Guidelines:
671"
LIMITATIONS,0.7559322033898305,"• The answer NA means that the paper has no limitation while the answer No means that
672"
LIMITATIONS,0.7567796610169492,"the paper has limitations, but those are not discussed in the paper.
673"
LIMITATIONS,0.7576271186440678,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
674"
LIMITATIONS,0.7584745762711864,"• The paper should point out any strong assumptions and how robust the results are to
675"
LIMITATIONS,0.7593220338983051,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
676"
LIMITATIONS,0.7601694915254237,"model well-specification, asymptotic approximations only holding locally). The authors
677"
LIMITATIONS,0.7610169491525424,"should reflect on how these assumptions might be violated in practice and what the
678"
LIMITATIONS,0.761864406779661,"implications would be.
679"
LIMITATIONS,0.7627118644067796,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
680"
LIMITATIONS,0.7635593220338983,"only tested on a few datasets or with a few runs. In general, empirical results often
681"
LIMITATIONS,0.764406779661017,"depend on implicit assumptions, which should be articulated.
682"
LIMITATIONS,0.7652542372881356,"• The authors should reflect on the factors that influence the performance of the approach.
683"
LIMITATIONS,0.7661016949152543,"For example, a facial recognition algorithm may perform poorly when image resolution
684"
LIMITATIONS,0.7669491525423728,"is low or images are taken in low lighting. Or a speech-to-text system might not be
685"
LIMITATIONS,0.7677966101694915,"used reliably to provide closed captions for online lectures because it fails to handle
686"
LIMITATIONS,0.7686440677966102,"technical jargon.
687"
LIMITATIONS,0.7694915254237288,"• The authors should discuss the computational efficiency of the proposed algorithms
688"
LIMITATIONS,0.7703389830508475,"and how they scale with dataset size.
689"
LIMITATIONS,0.7711864406779662,"• If applicable, the authors should discuss possible limitations of their approach to
690"
LIMITATIONS,0.7720338983050847,"address problems of privacy and fairness.
691"
LIMITATIONS,0.7728813559322034,"• While the authors might fear that complete honesty about limitations might be used by
692"
LIMITATIONS,0.773728813559322,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
693"
LIMITATIONS,0.7745762711864407,"limitations that aren’t acknowledged in the paper. The authors should use their best
694"
LIMITATIONS,0.7754237288135594,"judgment and recognize that individual actions in favor of transparency play an impor-
695"
LIMITATIONS,0.7762711864406779,"tant role in developing norms that preserve the integrity of the community. Reviewers
696"
LIMITATIONS,0.7771186440677966,"will be specifically instructed to not penalize honesty concerning limitations.
697"
THEORY ASSUMPTIONS AND PROOFS,0.7779661016949152,"3. Theory Assumptions and Proofs
698"
THEORY ASSUMPTIONS AND PROOFS,0.7788135593220339,"Question: For each theoretical result, does the paper provide the full set of assumptions and
699"
THEORY ASSUMPTIONS AND PROOFS,0.7796610169491526,"a complete (and correct) proof?
700"
THEORY ASSUMPTIONS AND PROOFS,0.7805084745762711,"Answer: [Yes]
701"
THEORY ASSUMPTIONS AND PROOFS,0.7813559322033898,"Justification: We conduct theoretical analysis on the impact of open-set noise and provided
702"
THEORY ASSUMPTIONS AND PROOFS,0.7822033898305085,"a complete proof in appendix D.
703"
THEORY ASSUMPTIONS AND PROOFS,0.7830508474576271,"Guidelines:
704"
THEORY ASSUMPTIONS AND PROOFS,0.7838983050847458,"• The answer NA means that the paper does not include theoretical results.
705"
THEORY ASSUMPTIONS AND PROOFS,0.7847457627118644,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
706"
THEORY ASSUMPTIONS AND PROOFS,0.785593220338983,"referenced.
707"
THEORY ASSUMPTIONS AND PROOFS,0.7864406779661017,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
708"
THEORY ASSUMPTIONS AND PROOFS,0.7872881355932203,"• The proofs can either appear in the main paper or the supplemental material, but if
709"
THEORY ASSUMPTIONS AND PROOFS,0.788135593220339,"they appear in the supplemental material, the authors are encouraged to provide a short
710"
THEORY ASSUMPTIONS AND PROOFS,0.7889830508474577,"proof sketch to provide intuition.
711"
THEORY ASSUMPTIONS AND PROOFS,0.7898305084745763,"• Inversely, any informal proof provided in the core of the paper should be complemented
712"
THEORY ASSUMPTIONS AND PROOFS,0.7906779661016949,"by formal proofs provided in appendix or supplemental material.
713"
THEORY ASSUMPTIONS AND PROOFS,0.7915254237288135,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
714"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7923728813559322,"4. Experimental Result Reproducibility
715"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7932203389830509,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
716"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7940677966101695,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
717"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7949152542372881,"of the paper (regardless of whether the code and data are provided or not)?
718"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7957627118644067,"Answer: [Yes]
719"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7966101694915254,"Justification: All the dataset and implementation details are included in appendix A.
720"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7974576271186441,"Guidelines:
721"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7983050847457627,"• The answer NA means that the paper does not include experiments.
722"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7991525423728814,"• If the paper includes experiments, a No answer to this question will not be perceived
723"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8,"well by the reviewers: Making the paper reproducible is important, regardless of
724"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8008474576271186,"whether the code and data are provided or not.
725"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8016949152542373,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
726"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8025423728813559,"to make their results reproducible or verifiable.
727"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8033898305084746,"• Depending on the contribution, reproducibility can be accomplished in various ways.
728"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8042372881355933,"For example, if the contribution is a novel architecture, describing the architecture fully
729"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8050847457627118,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
730"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8059322033898305,"be necessary to either make it possible for others to replicate the model with the same
731"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8067796610169492,"dataset, or provide access to the model. In general. releasing code and data is often
732"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8076271186440678,"one good way to accomplish this, but reproducibility can also be provided via detailed
733"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8084745762711865,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
734"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.809322033898305,"of a large language model), releasing of a model checkpoint, or other means that are
735"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8101694915254237,"appropriate to the research performed.
736"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8110169491525424,"• While NeurIPS does not require releasing code, the conference does require all submis-
737"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.811864406779661,"sions to provide some reasonable avenue for reproducibility, which may depend on the
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8127118644067797,"nature of the contribution. For example
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8135593220338984,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8144067796610169,"to reproduce that algorithm.
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8152542372881356,"(b) If the contribution is primarily a new model architecture, the paper should describe
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8161016949152542,"the architecture clearly and fully.
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8169491525423729,"(c) If the contribution is a new model (e.g., a large language model), then there should
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8177966101694916,"either be a way to access this model for reproducing the results or a way to reproduce
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8186440677966101,"the model (e.g., with an open-source dataset or instructions for how to construct
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8194915254237288,"the dataset).
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8203389830508474,"(d) We recognize that reproducibility may be tricky in some cases, in which case
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8211864406779661,"authors are welcome to describe the particular way they provide for reproducibility.
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8220338983050848,"In the case of closed-source models, it may be that access to the model is limited in
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8228813559322034,"some way (e.g., to registered users), but it should be possible for other researchers
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.823728813559322,"to have some path to reproducing or verifying the results.
752"
OPEN ACCESS TO DATA AND CODE,0.8245762711864407,"5. Open access to data and code
753"
OPEN ACCESS TO DATA AND CODE,0.8254237288135593,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
754"
OPEN ACCESS TO DATA AND CODE,0.826271186440678,"tions to faithfully reproduce the main experimental results, as described in supplemental
755"
OPEN ACCESS TO DATA AND CODE,0.8271186440677966,"material?
756"
OPEN ACCESS TO DATA AND CODE,0.8279661016949152,"Answer: [No]
757"
OPEN ACCESS TO DATA AND CODE,0.8288135593220339,"Justification: The complete codes will be released upon acceptance.
758"
OPEN ACCESS TO DATA AND CODE,0.8296610169491525,"Guidelines:
759"
OPEN ACCESS TO DATA AND CODE,0.8305084745762712,"• The answer NA means that paper does not include experiments requiring code.
760"
OPEN ACCESS TO DATA AND CODE,0.8313559322033899,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
761"
OPEN ACCESS TO DATA AND CODE,0.8322033898305085,"public/guides/CodeSubmissionPolicy) for more details.
762"
OPEN ACCESS TO DATA AND CODE,0.8330508474576271,"• While we encourage the release of code and data, we understand that this might not be
763"
OPEN ACCESS TO DATA AND CODE,0.8338983050847457,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
764"
OPEN ACCESS TO DATA AND CODE,0.8347457627118644,"including code, unless this is central to the contribution (e.g., for a new open-source
765"
OPEN ACCESS TO DATA AND CODE,0.8355932203389831,"benchmark).
766"
OPEN ACCESS TO DATA AND CODE,0.8364406779661017,"• The instructions should contain the exact command and environment needed to run to
767"
OPEN ACCESS TO DATA AND CODE,0.8372881355932204,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
768"
OPEN ACCESS TO DATA AND CODE,0.838135593220339,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
769"
OPEN ACCESS TO DATA AND CODE,0.8389830508474576,"• The authors should provide instructions on data access and preparation, including how
770"
OPEN ACCESS TO DATA AND CODE,0.8398305084745763,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
771"
OPEN ACCESS TO DATA AND CODE,0.8406779661016949,"• The authors should provide scripts to reproduce all experimental results for the new
772"
OPEN ACCESS TO DATA AND CODE,0.8415254237288136,"proposed method and baselines. If only a subset of experiments are reproducible, they
773"
OPEN ACCESS TO DATA AND CODE,0.8423728813559322,"should state which ones are omitted from the script and why.
774"
OPEN ACCESS TO DATA AND CODE,0.8432203389830508,"• At submission time, to preserve anonymity, the authors should release anonymized
775"
OPEN ACCESS TO DATA AND CODE,0.8440677966101695,"versions (if applicable).
776"
OPEN ACCESS TO DATA AND CODE,0.8449152542372881,"• Providing as much information as possible in supplemental material (appended to the
777"
OPEN ACCESS TO DATA AND CODE,0.8457627118644068,"paper) is recommended, but including URLs to data and code is permitted.
778"
OPEN ACCESS TO DATA AND CODE,0.8466101694915255,"6. Experimental Setting/Details
779"
OPEN ACCESS TO DATA AND CODE,0.847457627118644,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
780"
OPEN ACCESS TO DATA AND CODE,0.8483050847457627,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
781"
OPEN ACCESS TO DATA AND CODE,0.8491525423728814,"results?
782"
OPEN ACCESS TO DATA AND CODE,0.85,"Answer: [Yes]
783"
OPEN ACCESS TO DATA AND CODE,0.8508474576271187,"Justification: All the dataset and implementation details are included in appendix A.
784"
OPEN ACCESS TO DATA AND CODE,0.8516949152542372,"Guidelines:
785"
OPEN ACCESS TO DATA AND CODE,0.8525423728813559,"• The answer NA means that the paper does not include experiments.
786"
OPEN ACCESS TO DATA AND CODE,0.8533898305084746,"• The experimental setting should be presented in the core of the paper to a level of detail
787"
OPEN ACCESS TO DATA AND CODE,0.8542372881355932,"that is necessary to appreciate the results and make sense of them.
788"
OPEN ACCESS TO DATA AND CODE,0.8550847457627119,"• The full details can be provided either with the code, in appendix, or as supplemental
789"
OPEN ACCESS TO DATA AND CODE,0.8559322033898306,"material.
790"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8567796610169491,"7. Experiment Statistical Significance
791"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8576271186440678,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
792"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8584745762711864,"information about the statistical significance of the experiments?
793"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8593220338983051,"Answer: [Yes]
794"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8601694915254238,"Justification: We conduct multiple runs and report averaged results in most experiments.
795"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8610169491525423,"Guidelines:
796"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.861864406779661,"• The answer NA means that the paper does not include experiments.
797"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8627118644067797,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
798"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8635593220338983,"dence intervals, or statistical significance tests, at least for the experiments that support
799"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.864406779661017,"the main claims of the paper.
800"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8652542372881356,"• The factors of variability that the error bars are capturing should be clearly stated (for
801"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8661016949152542,"example, train/test split, initialization, random drawing of some parameter, or overall
802"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8669491525423729,"run with given experimental conditions).
803"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8677966101694915,"• The method for calculating the error bars should be explained (closed form formula,
804"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8686440677966102,"call to a library function, bootstrap, etc.)
805"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8694915254237288,"• The assumptions made should be given (e.g., Normally distributed errors).
806"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8703389830508474,"• It should be clear whether the error bar is the standard deviation or the standard error
807"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8711864406779661,"of the mean.
808"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8720338983050847,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
809"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8728813559322034,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
810"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8737288135593221,"of Normality of errors is not verified.
811"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8745762711864407,"• For asymmetric distributions, the authors should be careful not to show in tables or
812"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8754237288135593,"figures symmetric error bars that would yield results that are out of range (e.g. negative
813"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8762711864406779,"error rates).
814"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8771186440677966,"• If error bars are reported in tables or plots, The authors should explain in the text how
815"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8779661016949153,"they were calculated and reference the corresponding figures or tables in the text.
816"
EXPERIMENTS COMPUTE RESOURCES,0.8788135593220339,"8. Experiments Compute Resources
817"
EXPERIMENTS COMPUTE RESOURCES,0.8796610169491526,"Question: For each experiment, does the paper provide sufficient information on the com-
818"
EXPERIMENTS COMPUTE RESOURCES,0.8805084745762712,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
819"
EXPERIMENTS COMPUTE RESOURCES,0.8813559322033898,"the experiments?
820"
EXPERIMENTS COMPUTE RESOURCES,0.8822033898305085,"Answer: [Yes]
821"
EXPERIMENTS COMPUTE RESOURCES,0.8830508474576271,"Justification: All experiments are conducted on a private server with 3 RX6000 GPUs.
822"
EXPERIMENTS COMPUTE RESOURCES,0.8838983050847458,"Guidelines:
823"
EXPERIMENTS COMPUTE RESOURCES,0.8847457627118644,"• The answer NA means that the paper does not include experiments.
824"
EXPERIMENTS COMPUTE RESOURCES,0.885593220338983,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
825"
EXPERIMENTS COMPUTE RESOURCES,0.8864406779661017,"or cloud provider, including relevant memory and storage.
826"
EXPERIMENTS COMPUTE RESOURCES,0.8872881355932203,"• The paper should provide the amount of compute required for each of the individual
827"
EXPERIMENTS COMPUTE RESOURCES,0.888135593220339,"experimental runs as well as estimate the total compute.
828"
EXPERIMENTS COMPUTE RESOURCES,0.8889830508474577,"• The paper should disclose whether the full research project required more compute
829"
EXPERIMENTS COMPUTE RESOURCES,0.8898305084745762,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
830"
EXPERIMENTS COMPUTE RESOURCES,0.8906779661016949,"didn’t make it into the paper).
831"
CODE OF ETHICS,0.8915254237288136,"9. Code Of Ethics
832"
CODE OF ETHICS,0.8923728813559322,"Question: Does the research conducted in the paper conform, in every respect, with the
833"
CODE OF ETHICS,0.8932203389830509,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
834"
CODE OF ETHICS,0.8940677966101694,"Answer: [Yes]
835"
CODE OF ETHICS,0.8949152542372881,"Justification: We confirm that the conducted reserach conform with the NeurIPS Code of
836"
CODE OF ETHICS,0.8957627118644068,"Ethics.
837"
CODE OF ETHICS,0.8966101694915254,"Guidelines:
838"
CODE OF ETHICS,0.8974576271186441,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
839"
CODE OF ETHICS,0.8983050847457628,"• If the authors answer No, they should explain the special circumstances that require a
840"
CODE OF ETHICS,0.8991525423728813,"deviation from the Code of Ethics.
841"
CODE OF ETHICS,0.9,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
842"
CODE OF ETHICS,0.9008474576271186,"eration due to laws or regulations in their jurisdiction).
843"
BROADER IMPACTS,0.9016949152542373,"10. Broader Impacts
844"
BROADER IMPACTS,0.902542372881356,"Question: Does the paper discuss both potential positive societal impacts and negative
845"
BROADER IMPACTS,0.9033898305084745,"societal impacts of the work performed?
846"
BROADER IMPACTS,0.9042372881355932,"Answer: [NA]
847"
BROADER IMPACTS,0.9050847457627119,"Justification: Not applicable.
848"
BROADER IMPACTS,0.9059322033898305,"Guidelines:
849"
BROADER IMPACTS,0.9067796610169492,"• The answer NA means that there is no societal impact of the work performed.
850"
BROADER IMPACTS,0.9076271186440678,"• If the authors answer NA or No, they should explain why their work has no societal
851"
BROADER IMPACTS,0.9084745762711864,"impact or why the paper does not address societal impact.
852"
BROADER IMPACTS,0.9093220338983051,"• Examples of negative societal impacts include potential malicious or unintended uses
853"
BROADER IMPACTS,0.9101694915254237,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
854"
BROADER IMPACTS,0.9110169491525424,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
855"
BROADER IMPACTS,0.911864406779661,"groups), privacy considerations, and security considerations.
856"
BROADER IMPACTS,0.9127118644067796,"• The conference expects that many papers will be foundational research and not tied
857"
BROADER IMPACTS,0.9135593220338983,"to particular applications, let alone deployments. However, if there is a direct path to
858"
BROADER IMPACTS,0.9144067796610169,"any negative applications, the authors should point it out. For example, it is legitimate
859"
BROADER IMPACTS,0.9152542372881356,"to point out that an improvement in the quality of generative models could be used to
860"
BROADER IMPACTS,0.9161016949152543,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
861"
BROADER IMPACTS,0.9169491525423729,"that a generic algorithm for optimizing neural networks could enable people to train
862"
BROADER IMPACTS,0.9177966101694915,"models that generate Deepfakes faster.
863"
BROADER IMPACTS,0.9186440677966101,"• The authors should consider possible harms that could arise when the technology is
864"
BROADER IMPACTS,0.9194915254237288,"being used as intended and functioning correctly, harms that could arise when the
865"
BROADER IMPACTS,0.9203389830508475,"technology is being used as intended but gives incorrect results, and harms following
866"
BROADER IMPACTS,0.9211864406779661,"from (intentional or unintentional) misuse of the technology.
867"
BROADER IMPACTS,0.9220338983050848,"• If there are negative societal impacts, the authors could also discuss possible mitigation
868"
BROADER IMPACTS,0.9228813559322034,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
869"
BROADER IMPACTS,0.923728813559322,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
870"
BROADER IMPACTS,0.9245762711864407,"feedback over time, improving the efficiency and accessibility of ML).
871"
SAFEGUARDS,0.9254237288135593,"11. Safeguards
872"
SAFEGUARDS,0.926271186440678,"Question: Does the paper describe safeguards that have been put in place for responsible
873"
SAFEGUARDS,0.9271186440677966,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
874"
SAFEGUARDS,0.9279661016949152,"image generators, or scraped datasets)?
875"
SAFEGUARDS,0.9288135593220339,"Answer: [NA]
876"
SAFEGUARDS,0.9296610169491526,"Justification: Not applicable.
877"
SAFEGUARDS,0.9305084745762712,"Guidelines:
878"
SAFEGUARDS,0.9313559322033899,"• The answer NA means that the paper poses no such risks.
879"
SAFEGUARDS,0.9322033898305084,"• Released models that have a high risk for misuse or dual-use should be released with
880"
SAFEGUARDS,0.9330508474576271,"necessary safeguards to allow for controlled use of the model, for example by requiring
881"
SAFEGUARDS,0.9338983050847458,"that users adhere to usage guidelines or restrictions to access the model or implementing
882"
SAFEGUARDS,0.9347457627118644,"safety filters.
883"
SAFEGUARDS,0.9355932203389831,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
884"
SAFEGUARDS,0.9364406779661016,"should describe how they avoided releasing unsafe images.
885"
SAFEGUARDS,0.9372881355932203,"• We recognize that providing effective safeguards is challenging, and many papers do
886"
SAFEGUARDS,0.938135593220339,"not require this, but we encourage authors to take this into account and make a best
887"
SAFEGUARDS,0.9389830508474576,"faith effort.
888"
LICENSES FOR EXISTING ASSETS,0.9398305084745763,"12. Licenses for existing assets
889"
LICENSES FOR EXISTING ASSETS,0.940677966101695,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
890"
LICENSES FOR EXISTING ASSETS,0.9415254237288135,"the paper, properly credited and are the license and terms of use explicitly mentioned and
891"
LICENSES FOR EXISTING ASSETS,0.9423728813559322,"properly respected?
892"
LICENSES FOR EXISTING ASSETS,0.9432203389830508,"Answer: [Yes]
893"
LICENSES FOR EXISTING ASSETS,0.9440677966101695,"Justification: We include all essential information and references to the used datasets in this
894"
LICENSES FOR EXISTING ASSETS,0.9449152542372882,"work.
895"
LICENSES FOR EXISTING ASSETS,0.9457627118644067,"Guidelines:
896"
LICENSES FOR EXISTING ASSETS,0.9466101694915254,"• The answer NA means that the paper does not use existing assets.
897"
LICENSES FOR EXISTING ASSETS,0.9474576271186441,"• The authors should cite the original paper that produced the code package or dataset.
898"
LICENSES FOR EXISTING ASSETS,0.9483050847457627,"• The authors should state which version of the asset is used and, if possible, include a
899"
LICENSES FOR EXISTING ASSETS,0.9491525423728814,"URL.
900"
LICENSES FOR EXISTING ASSETS,0.95,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
901"
LICENSES FOR EXISTING ASSETS,0.9508474576271186,"• For scraped data from a particular source (e.g., website), the copyright and terms of
902"
LICENSES FOR EXISTING ASSETS,0.9516949152542373,"service of that source should be provided.
903"
LICENSES FOR EXISTING ASSETS,0.9525423728813559,"• If assets are released, the license, copyright information, and terms of use in the
904"
LICENSES FOR EXISTING ASSETS,0.9533898305084746,"package should be provided. For popular datasets, paperswithcode.com/datasets
905"
LICENSES FOR EXISTING ASSETS,0.9542372881355933,"has curated licenses for some datasets. Their licensing guide can help determine the
906"
LICENSES FOR EXISTING ASSETS,0.9550847457627119,"license of a dataset.
907"
LICENSES FOR EXISTING ASSETS,0.9559322033898305,"• For existing datasets that are re-packaged, both the original license and the license of
908"
LICENSES FOR EXISTING ASSETS,0.9567796610169491,"the derived asset (if it has changed) should be provided.
909"
LICENSES FOR EXISTING ASSETS,0.9576271186440678,"• If this information is not available online, the authors are encouraged to reach out to
910"
LICENSES FOR EXISTING ASSETS,0.9584745762711865,"the asset’s creators.
911"
NEW ASSETS,0.9593220338983051,"13. New Assets
912"
NEW ASSETS,0.9601694915254237,"Question: Are new assets introduced in the paper well documented and is the documentation
913"
NEW ASSETS,0.9610169491525423,"provided alongside the assets?
914"
NEW ASSETS,0.961864406779661,"Answer: [NA]
915"
NEW ASSETS,0.9627118644067797,"Justification: Not applicable.
916"
NEW ASSETS,0.9635593220338983,"Guidelines:
917"
NEW ASSETS,0.964406779661017,"• The answer NA means that the paper does not release new assets.
918"
NEW ASSETS,0.9652542372881356,"• Researchers should communicate the details of the dataset/code/model as part of their
919"
NEW ASSETS,0.9661016949152542,"submissions via structured templates. This includes details about training, license,
920"
NEW ASSETS,0.9669491525423729,"limitations, etc.
921"
NEW ASSETS,0.9677966101694915,"• The paper should discuss whether and how consent was obtained from people whose
922"
NEW ASSETS,0.9686440677966102,"asset is used.
923"
NEW ASSETS,0.9694915254237289,"• At submission time, remember to anonymize your assets (if applicable). You can either
924"
NEW ASSETS,0.9703389830508474,"create an anonymized URL or include an anonymized zip file.
925"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9711864406779661,"14. Crowdsourcing and Research with Human Subjects
926"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9720338983050848,"Question: For crowdsourcing experiments and research with human subjects, does the paper
927"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9728813559322034,"include the full text of instructions given to participants and screenshots, if applicable, as
928"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737288135593221,"well as details about compensation (if any)?
929"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9745762711864406,"Answer: [NA]
930"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754237288135593,"Justification: Not applicable.
931"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.976271186440678,"Guidelines:
932"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771186440677966,"• The answer NA means that the paper does not involve crowdsourcing nor research with
933"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9779661016949153,"human subjects.
934"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9788135593220338,"• Including this information in the supplemental material is fine, but if the main contribu-
935"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9796610169491525,"tion of the paper involves human subjects, then as much detail as possible should be
936"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805084745762712,"included in the main paper.
937"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813559322033898,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
938"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822033898305085,"or other labor should be paid at least the minimum wage in the country of the data
939"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830508474576272,"collector.
940"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838983050847457,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
941"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847457627118644,"Subjects
942"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985593220338983,"Question: Does the paper describe potential risks incurred by study participants, whether
943"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864406779661017,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
944"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872881355932204,"approvals (or an equivalent approval/review based on the requirements of your country or
945"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988135593220339,"institution) were obtained?
946"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889830508474576,"Answer: [NA]
947"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9898305084745763,"Justification: Not applicable.
948"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9906779661016949,"Guidelines:
949"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915254237288136,"• The answer NA means that the paper does not involve crowdsourcing nor research with
950"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923728813559322,"human subjects.
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932203389830508,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940677966101695,"may be required for any human subjects research. If you obtained IRB approval, you
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949152542372881,"should clearly state this in the paper.
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957627118644068,"• We recognize that the procedures for this may vary significantly between institutions
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966101694915255,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997457627118644,"guidelines for their institution.
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983050847457627,"• For initial submissions, do not include any information that would break anonymity (if
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991525423728813,"applicable), such as the institution conducting the review.
959"
