Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003215434083601286,"We present a novel methodology for repairing neural networks that use ReLU ac-
tivation functions. Unlike existing methods that rely on modifying the weights
of a neural network which can induce a global change in the function space, our
approach applies only a localized change in the function space while still guar-
anteeing the removal of the buggy behavior. By leveraging the piecewise linear
nature of ReLU networks, our approach can efficiently construct a patch network
tailored to the linear region where the buggy input resides, which when combined
with the original network, provably corrects the behavior on the buggy input. Our
method is both sound and complete ‚Äì the repaired network is guaranteed to fix the
buggy input, and a patch is guaranteed to be found for any buggy input. Moreover,
our approach preserves the continuous piecewise linear nature of ReLU networks,
automatically generalizes the repair to all the points including other undetected
buggy inputs inside the repair region, is minimal in terms of changes in the func-
tion space, and guarantees that outputs on inputs away from the repair region are
unaltered. On several benchmarks, we show that our approach significantly out-
performs existing methods in terms of locality and limiting negative side effects."
INTRODUCTION,0.006430868167202572,"1
INTRODUCTION"
INTRODUCTION,0.00964630225080386,"Deep neural networks (DNNs) have demonstrated impressive performances on a wide variety of
applications ranging from transportation Bojarski et al. (2016) to health care Shahid et al. (2019).
However, DNNs are not perfect. In many cases, especially when the DNNs are used in safety-critical
contexts, it is important to correct erroneous outputs of a DNN as they are discovered after training.
For instance, a neural network in charge of giving control advisories to the pilots in an aircraft
collision avoidance system, such as the ACAS Xu network from Julian et al. (2019), may produce an
incorrect advisory for certain situations and cause the aircraft to turn towards the incoming aircraft,
thereby jeopardizing the safety of both airplanes. In this paper, we consider the problem of neural
network repair, i.e. given a trained neural network and a set of buggy inputs (inputs on which the
neural network produces incorrect predictions), repair the network so that the resulting network on
those buggy inputs behave according to some given correctness specification. Ideally, the changes to
the neural network function should be small so that the outputs on other inputs are either unchanged
or altered in a small way. Existing works on neural network repair roughly fall into three categories."
INTRODUCTION,0.012861736334405145,"1. Retraining/fine-tuning. The idea is to retrain or fine-tune the network with the newly identified
buggy inputs and the corresponding corrected outputs. Methods include counterexample-guided
data augmentation Dreossi et al. (2018); Ren et al. (2020), editable training Sinitsin et al. (2020)
and training input selection Ma et al. (2018). One major weakness of these approaches is the lack of
formal guarantees ‚Äì at the end of retraining/fine-tuning, there is no guarantee that the given buggy
inputs are fixed and no new bugs are introduced. In addition, retraining can be very expensive and
requires access to the original training data which is impractical in cases where the neural network
is obtained from a third party or the training data is private. Fine-tuning, on the other hand, often
faces the issue of catastrophic forgetting Kirkpatrick et al. (2017)."
INTRODUCTION,0.01607717041800643,"2. Direct weight modification. These approaches directly manipulate the weights in a neural
network to fix the buggy inputs. The repair problem is typically cast into an optimization problem"
INTRODUCTION,0.01929260450160772,Published as a conference paper at ICLR 2022 ùë•
INTRODUCTION,0.022508038585209004,"Retraining or direct 
weight modification"
INTRODUCTION,0.02572347266881029,"Decoupled DNN
Our approach ùë•
ùë•"
INTRODUCTION,0.028938906752411574,"Figure 1: Comparison of different approaches to the neural network repair problem. The black lines
represent the original neural network function. The red dot represents the buggy input. The colored
lines represent the functions after the repairs are done."
INTRODUCTION,0.03215434083601286,"or a verification problem. For example, Dong et al. (2020) proposes to minimize a loss defined on
the buggy inputs. Goldberger et al. (2020) uses an SMT solver to identify minimal weight changes
to the output layer of the network so that the undesirable behaviors are removed. Usman et al. (2021)
first locates potentially faulty weights in a layer and then uses constraint solving to find a weights
modification that can fix the failures. In general, the optimization-based approach cannot guarantee
removal of the buggy behaviors, and the verification-based approach does not scale beyond networks
of a few hundred neurons. In addition, these approaches can suffer from substantial accuracy drops
on normal inputs since weight changes may be a poor proxy for changes in the function space."
INTRODUCTION,0.03536977491961415,"3. Architecture extension. The third category of approaches extends the given NN architecture,
such as by introducing more weight parameters, to facilitate more efficient repairs. The so-called
Decoupled DNN architecture Sotoudeh & Thakur (2021) is the only work we know that falls into
this category. Their idea is to decouple the activations of the network from values of the network
by augmenting the original network. Their construction allows the formulation of any single-layer
repair as an linear programming (LP) problem. The decoupling, however, causes the repaired net-
work to become discontinuous (in the functional sense). In addition, it still cannot isolate the output
change to a single buggy input from the rest of the inputs."
INTRODUCTION,0.03858520900321544,"In addition to the aforementioned limitations, a common weakness that is shared amongst these
methods is that the induced changes, as a result of either retraining or direct weight modification,
are global. This means that a correct behavior on another input, regardless of how far it is away from
the buggy input, may not be preserved by the repair. Worse still, the repair on a new buggy input can
end up invalidating the repair on a previous buggy input. The fundamental issue here is that limiting
the changes to a few weights or a single layer only poses a structural constraint (often for ease of
computation); it does not limit the changes on the input-output mapping of the neural network. It is
known that even a single weight change can have a global effect on the output of a neural network."
INTRODUCTION,0.04180064308681672,"In this paper, we propose REASSURE, a novel methodology for neural network repair with locality,
minimality, soundness and completeness guarantees. Our methodology targets continuous piecewise
linear (CPWL) neural networks, specifically those that use the ReLU activation functions. The key
idea of our approach is to leverage the CPWL property of ReLU networks to synthesize a patch
network tailored to the linear region where the buggy input resides, which when combined with the
original network, provably corrects the behavior on the buggy input. Our approach is both sound
and complete ‚Äì the repaired network is guaranteed to fix the buggy input, and a patch is guaranteed
to be found for any buggy input. Moreover, our approach preserves the CPWL nature of ReLU
networks, automatically generalizes the repair to all the points including other undetected buggy
inputs inside the repair region, is minimal in terms of changes in the function space, and guarantees
that outputs on inputs away from the repair region are unaltered. Figure 1 provides an illustrative
comparison of our approach with other methods. Table 1 compares our approach with representative
related works in terms of theoretical guarantees. We summarize our contributions below."
INTRODUCTION,0.04501607717041801,"1. We present REASSURE, the first sound and complete repair methodology for ReLU networks
with strong theoretical guarantees."
INTRODUCTION,0.04823151125401929,"2. Our technique synthesizes a patch network, which when combined with the original neural
network, provably corrects the behavior on the buggy input. This approach is a significant departure
from existing methods that rely on retraining or direct weight manipulation."
INTRODUCTION,0.05144694533762058,"3. Across a set of benchmarks, REASSURE can efficiently correct a set of buggy inputs or buggy
areas with little or no change to the accuracy and overall functionality of the network."
INTRODUCTION,0.05466237942122187,Published as a conference paper at ICLR 2022
INTRODUCTION,0.05787781350482315,"REASSURE
Retrain
MDNN
Editable Fine-Tuning
PRDNN
Preservation of CPWL
Yes
Yes
Yes
Yes
No
Soundness
Yes
No
Yes
No
Yes
Completeness
Yes
No
No
No
No
Area Repair
Yes
No
No
No
Yes
Minimal Change
Yes (Function Space)
No
Yes (Weight Space)
No
Yes (Weight Space)
Localized Change
Yes
No
No
No
No
Limited Side Effect
Yes
No
No
No
No"
INTRODUCTION,0.06109324758842444,"Table 1: Comparing REASSURE with representative related works in terms of theoretical guarantees.
CPWL stands for continuous piecewise linearity. Area repair means repairing all the (infinitely
many) points inside an area. Limited side effect means the repair can limit potential adverse effects
on other inputs. MDNN is the verification-based approach from Goldberger et al. (2020). PRDNN
is the Decoupled DNN approach from Sotoudeh & Thakur (2021). REASSURE is the only method
that can provide all the guarantees."
BACKGROUND,0.06430868167202572,"2
BACKGROUND"
DEEP NEURAL NETWORKS,0.06752411575562701,"2.1
DEEP NEURAL NETWORKS"
DEEP NEURAL NETWORKS,0.0707395498392283,"An R-layer feed-forward DNN f = Œ∫R ‚ó¶œÉ ‚ó¶Œ∫R‚àí1 ‚ó¶... ‚ó¶œÉ ‚ó¶Œ∫1 : X ‚ÜíY is a composition of
linear functions Œ∫r, r = 1, 2, ..., R and activation function œÉ, where X ‚äÜRm is a bounded input
domain and Y ‚äÜRn is the output domain. Weights and biases of linear function {Œ∫r}r=1,2,...,R are
parameters of the DNN."
DEEP NEURAL NETWORKS,0.07395498392282958,"We call the first R ‚àí1 layers hidden layers and the R-th layer the output layer. We use zi
j to denote
the i-th neuron (before activation) in the j-th hidden layer."
DEEP NEURAL NETWORKS,0.07717041800643087,"In this paper, we focus on ReLU DNNs, i.e. DNNs that use only the ReLU activation functions.
It is known that an Rm ‚ÜíR function is representable by a ReLU DNN if and only if it is a
continuous piecewise linear (CPWL) function Arora et al. (2016). The ReLU function is defined as
œÉ(x) = max(x, 0). We say that œÉ(x) is activated if œÉ(x) = x."
LINEAR REGIONS,0.08038585209003216,"2.2
LINEAR REGIONS"
LINEAR REGIONS,0.08360128617363344,"A linear region A is the set of inputs that correspond to the same activation pattern in a ReLU DNN
f Serra et al. (2017). Geometrically, this corresponds to a convex polytope, which is an intersection
of half spaces, in the input space X on which f is linear. We use f|A to denote the part of f on A."
CORRECTNESS SPECIFICATION,0.08681672025723473,"2.3
CORRECTNESS SPECIFICATION"
CORRECTNESS SPECIFICATION,0.09003215434083602,"A correctness specification Œ¶ = (Œ¶in, Œ¶out) is a tuple of two polytopes, where Œ¶in is the union
of some linear regions and Œ¶out is a convex polytope. A DNN f is said to meet a specification
Œ¶ = (Œ¶in, Œ¶out), denoted as f |= Œ¶, if and only if ‚àÄx ‚ààŒ¶in, f(x) ‚ààŒ¶out."
CORRECTNESS SPECIFICATION,0.0932475884244373,"Example 1. For a classification problem, we can formally write the specification that ‚Äúthe pre-
diction of any point in an area A is class k‚Äù as Œ¶ = (Œ¶in, Œ¶out), where Œ¶in = A and
Œ¶out = {y ‚ààRn | yk ‚â•yi, ‚àÄi Ã∏= k}1."
PROBLEM DEFINITION,0.09646302250803858,"2.4
PROBLEM DEFINITION"
PROBLEM DEFINITION,0.09967845659163987,"In this paper, we consider the following two repair problems."
PROBLEM DEFINITION,0.10289389067524116,"Definition 1 (Area repair). Given a correctness specification Œ¶ = (Œ¶in, Œ¶out) and a ReLU DNN
f Ã∏|= Œ¶, the area repair problem is to find a modified ReLU DNN bf such that bf |= Œ¶."
PROBLEM DEFINITION,0.10610932475884244,"Note that we do not require bf to have the same structure or parameters as f in this definition. If
Œ¶in contains a single (buggy) linear region, we refer to this as single-region repair. If Œ¶in contains
multiple (buggy) linear regions, we refer to it as multi-region repair."
PROBLEM DEFINITION,0.10932475884244373,1Note that here y is the output of the layer right before the softmax layer in a classification network.
PROBLEM DEFINITION,0.11254019292604502,Published as a conference paper at ICLR 2022
PROBLEM DEFINITION,0.1157556270096463,"Definition 2 (Point-wise repair). Given a set of buggy inputs {ex1, . . . , exL} ‚äÇŒ¶in with their corre-
sponding correct outputs {y1, . . . , yL} and a ReLU DNN f, the point-wise repair problem is to find
a modified ReLU DNN bf such that ‚àÄi, bf(exi) = yi."
PROBLEM DEFINITION,0.1189710610932476,"We call the minimal variants of area repair and point-wise repair minimal area repair and minimal
point-wise repair respectively. Minimality here is defined with respect to the maximum distance
between f and bf over the whole input domain X. A point-wise repair can be generalized to an area
repair through the following result."
FROM BUGGY INPUTS TO BUGGY LINEAR REGIONS,0.12218649517684887,"2.5
FROM BUGGY INPUTS TO BUGGY LINEAR REGIONS"
FROM BUGGY INPUTS TO BUGGY LINEAR REGIONS,0.12540192926045016,"The linear region where an input x resides can be computed as follows.
Lemma 1. Lee et al. (2019) Consider a ReLU DNN f and an input x ‚ààX. For every neuron zi
j, it
induces a feasible set"
FROM BUGGY INPUTS TO BUGGY LINEAR REGIONS,0.12861736334405144,"Ai
j(x) =
{¬Øx ‚ààX|(‚ñΩxzi
j)T ¬Øx + zi
j ‚àí(‚ñΩxzi
j)T x ‚â•0}
if zi
j ‚â•0
{¬Øx ‚ààX|(‚ñΩxzi
j)T ¬Øx + zi
j ‚àí(‚ñΩxzi
j)T x ‚â§0}
if zi
j < 0
(1)"
FROM BUGGY INPUTS TO BUGGY LINEAR REGIONS,0.13183279742765272,"The set A(x) = ‚à©i,jAi
j(x) is the linear region that includes x. Note that A(x) is essentially the
H-representation of the corresponding convex polytope."
REPAIR DESIDERATA,0.13504823151125403,"2.6
REPAIR DESIDERATA"
REPAIR DESIDERATA,0.1382636655948553,We argue that an effective repair algorithm for ReLU DNN should satisfy the following criteria.
REPAIR DESIDERATA,0.1414790996784566,"1. Preservation of CPWL: Given that the original network f models a CPWL function, the repaired
network bf should still model a CPWL function. 2. Soundness: A sound repair should completely
remove the known buggy behaviors, i.e. it is a solution to the point-wise repair problem defined
in Definition 2. 3. Completeness: Ideally, the algorithm should always be able find a repair for
any given buggy input if it exists. 4. Generalization: If there exists another buggy input ex‚Ä≤ in
the neighborhood of ex (e.g. the same linear region), then the repair should also fix it. For example,
suppose we have an ex that violates a specification which requires the output to be within some range.
It is almost guaranteed that there exists another (and infinitely many) ex‚Ä≤ in the same linear region
that also violates the specification. 5. Locality: We argue that a good repair should only induce a
localized change to f in the function space. For example, in the context of ReLU DNN, if a linear
region B does not border the repair region A, i.e. B‚à©A = ‚àÖ, then bf|B(x) = f|B(x). 6. Minimality:
Some notion of distance between f and bf such as max |f ‚àíbf| should be minimized. Note that this is
a significant departure from existing methods that focus on minimizing the change in weights which
has no guarantee on the amount of change in the function space. 7. Limited side effect: Repairing
a buggy point should not adversely affect points that were originally correct. For example, repairing
a buggy input ex in region A should not change another region from correct to incorrect. Formally,
for any linear region C who is a neighbor of A, i.e. C ‚à©A Ã∏= ‚àÖ, if f|C |= Œ¶, then bf|C |= Œ¶. 8.
Efficiency: The repair algorithm should terminate in polynomial time with respect to the size of the
neural network and the number of buggy inputs."
OUR APPROACH,0.14469453376205788,"3
OUR APPROACH"
OUR APPROACH,0.14790996784565916,"We will first describe our approach to single-region repair and then present our approach to multi-
region repair which builds on results obtained from the single-region case."
OUR APPROACH,0.15112540192926044,"Given a linear region A, our overarching approach is to synthesize a patch network hA such that
bf = f + hA and bf |= Œ¶. The patch network hA is a combination of two sub-networks: a support
network gA, which behaves like a characteristic function, to determine whether an input is in A, and
an affine patch function network pA(x) = cx + d to ensure (f + pA) |= Œ¶ on A."
RUNNING EXAMPLE,0.15434083601286175,"3.1
RUNNING EXAMPLE"
RUNNING EXAMPLE,0.15755627009646303,We use the following example to illustrate our idea.
RUNNING EXAMPLE,0.1607717041800643,"Published as a conference paper at ICLR 2022 ùë•! ùë•"" ùëß! ùëß"" ùë¶ 1 ‚àí1 2 2 1 1 ùë•! ùë•"" ùëß! ùëß"" ùë¶ ùëù! ùëî!"
RUNNING EXAMPLE,0.1639871382636656,"Support Network ùëî!
‚Ñé%"
RUNNING EXAMPLE,0.16720257234726688,"Figure 2: Left: the target DNN with buggy inputs. Right: the REASSURE-repaired DNN with the
patch network shown in red. Support network gA is for approximating the characteristic function
on A; Affine patch function pA ensures the satisfaction of Œ¶ on A; The design of the patch network
hA ensures locality for the final patch."
RUNNING EXAMPLE,0.17041800643086816,"Example 2. Consider repairing the ReLU DNN f in Figure 2 according to the correctness specifi-
cation Œ¶ : ‚àÄx ‚àà[0, 1]2, y ‚àà[0, 2]. The DNN consists of a single hidden layer with two neurons z1
and z2, where y = œÉ(z1) + œÉ(z2), z1 = x1 + 2x2 ‚àí1 and z2 = 2x1 ‚àíx2."
RUNNING EXAMPLE,0.17363344051446947,"The only linear region that violates our specification is A = {x | 1 ‚â•x1, 1 ‚â•x2, x1 + 2x2 ‚àí1 ‚â•
0, 2x1 ‚àíx2 ‚â•0}. ( ex = (0.9, 0.9) ‚àà[0, 1]2 but f(ex) = 2.6 /‚àà[0, 2])"
RUNNING EXAMPLE,0.17684887459807075,"The network f(x) on the linear region A is the affine function f|A(x) = 3x1 + x2 ‚àí1. Our
algorithm first sets up an affine function pA(x) that minimally repairs f on A, such that ‚àÄx ‚àà
A, f(x) + pA(x) ‚àà[0, 2]. Later in the paper, we will show pA(x) can be found by solving a LP
problem. The resulting patch function is pA(x) = ‚àí1"
RUNNING EXAMPLE,0.18006430868167203,2x1 ‚àí1 2x2.
RUNNING EXAMPLE,0.1832797427652733,"However, directly apply f(x) + pA(x) as the patch network will have side effects on areas outside
of A. Our strategy is to combine pA(x) with a support network gA(x) which outputs 1 on A and
drops to 0 quickly outside of A. The final repaired network is f(x) + œÉ(pA(x) + gA(x, 10) ‚àí
1) ‚àíœÉ(‚àípA(x) + gA(x, 10) ‚àí1). This structure makes pA almost only active on A and achieve a
localized repair. Observe that this is still a ReLU DNN."
SUPPORT NETWORKS,0.1864951768488746,"3.2
SUPPORT NETWORKS"
SUPPORT NETWORKS,0.18971061093247588,"Support networks are neural networks with a special structure that can approximate the characteristic
function of a convex polytope. They are keys to ensuring localized repairs in our algorithm."
SUPPORT NETWORKS,0.19292604501607716,"Assume that the linear region we need to repair is A = {x|aix ‚â§bi, i ‚ààI}, where |I| is the number
of linear inequalities. The support network of A is defined as:"
SUPPORT NETWORKS,0.19614147909967847,"gA(x, Œ≥) = œÉ(
X"
SUPPORT NETWORKS,0.19935691318327975,"i‚ààI
g(bi ‚àíaix, Œ≥) ‚àí|I| + 1)
(2)"
SUPPORT NETWORKS,0.20257234726688103,"where g(x, Œ≥) = œÉ(Œ≥x + 1) ‚àíœÉ(Œ≥x) and Œ≥ ‚ààR is a parameter of our algorithm that controls how
quickly gA(x, Œ≥) goes to zero outside of A."
SUPPORT NETWORKS,0.2057877813504823,"Remark: For any x ‚ààA, we have gA(x, Œ≥) = 1, i.e. the support network is fully activated. For any
x /‚ààA, if for one of i ‚ààI, we have aix ‚àíbi ‚â§‚àí1/Œ≥, then gA(x, Œ≥) = 0."
SUPPORT NETWORKS,0.2090032154340836,"Observe that gA(x, Œ≥) is not zero when x is very close to A due to the requirement for preserving
CPWL. In Theorem 3, we prove that we can still guarantee limited side effects on the whole input
domain outside of A with this construction."
AFFINE PATCH FUNCTIONS,0.21221864951768488,"3.3
AFFINE PATCH FUNCTIONS"
AFFINE PATCH FUNCTIONS,0.21543408360128619,"We consider an affine patch function pA(x) = cx+d, where matrix c and vector d are undetermined
coefficients. In a later section, the design of patch network will ensure that on the patch area A, the
repaired network is f(x) + pA(x). We will first consider finding appropriate c and d such that
f(x) + pA(x) satisfy the specification on A."
AFFINE PATCH FUNCTIONS,0.21864951768488747,Published as a conference paper at ICLR 2022
AFFINE PATCH FUNCTIONS,0.22186495176848875,"To satisfy the specification Œ¶, we need f(x) + pA(x) ‚ààŒ¶out for all x ‚ààA. To obtain a minimal
repair, we minimize maxx‚ààA |pA(x)|. Thus, we can formulate the following optimization problem
minc,d maxx‚ààA |pA(x)| = |cx + d|
(c, d) ‚àà{(c, d) | f(x) + cx + d ‚ààŒ¶out, ‚àÄx ‚ààA}
(3)"
AFFINE PATCH FUNCTIONS,0.22508038585209003,Notice that this is not an LP since both c and x are variables and we have a cx term in the objective.
AFFINE PATCH FUNCTIONS,0.2282958199356913,"In general, one can solve it by enumerating all the vertices of A. Suppose that {vs|s = 1, 2, ..., S}
is the set of vertices of A. Since Œ¶out is a convex polytope, we have"
AFFINE PATCH FUNCTIONS,0.2315112540192926,"f(x) + pA(x) ‚ààŒ¶out for all x ‚ààA ‚áîf(vs) + pA(vs) ‚ààŒ¶out for s = 1, 2, ..., S.
(4) and"
AFFINE PATCH FUNCTIONS,0.2347266881028939,"maxx‚ààA |cx + d| = maxs=1,2,...,S |cvs + d|
(5)"
AFFINE PATCH FUNCTIONS,0.2379421221864952,"Hence, we can solve the following equivalent LP.
Ô£±
Ô£≤ Ô£≥"
AFFINE PATCH FUNCTIONS,0.24115755627009647,"minc,d H
H ‚â•(cvs + d)i, H ‚â•‚àí(cvs + d)i, for s = 1, 2, ..., S and i = 1, 2, ..., m
f(vs) + pA(vs) ‚ààŒ¶out, for s = 1, 2, ..., S (6)"
AFFINE PATCH FUNCTIONS,0.24437299035369775,"where H ‚ààR and will take maxs=1,2,...,S |cvs + d| when optimal."
AFFINE PATCH FUNCTIONS,0.24758842443729903,"In general, the number of vertices of a convex polytope can be exponential in the size of its H-
representation Henk et al. (1997) and enumerating the vertices of a convex polytope is known to be
expensive especially when the input dimension is large Bremner (1997). In Appendix 7.1, we show
that we can solve programming 3 via LP without vertex enumeration for many useful cases such as
the classification problem in Example 1 and make our algorithm much more efficient."
SINGLE-REGION REPAIRS,0.2508038585209003,"3.4
SINGLE-REGION REPAIRS"
SINGLE-REGION REPAIRS,0.2540192926045016,"With a support network gA and an affine patch function pA, we can synthesize the final patch
network as follows:"
SINGLE-REGION REPAIRS,0.2572347266881029,"hA(x, Œ≥) = œÉ(pA(x) + K ¬∑ gA(x, Œ≥) ‚àíK) ‚àíœÉ(‚àípA(x) + K ¬∑ gA(x, Œ≥) ‚àíK)
(7)"
SINGLE-REGION REPAIRS,0.2604501607717042,where K is a vector where every entry is equal to the upper bound of {|pA(x)|+‚àû|x ‚ààX}.
SINGLE-REGION REPAIRS,0.26366559485530544,"Remark: For x ‚ààA, gA(x, Œ≥) = 1, then we have hA(x, Œ≥) = œÉ(pA(x)) ‚àíœÉ(‚àípA(x)) = pA(x).
For x /‚ààA, gA(x, Œ≥) goes to zero quickly if Œ≥ is large. When gA(x, Œ≥) = 0, we have hA(x, Œ≥) =
œÉ(pA(x) ‚àíK) ‚àíœÉ(‚àípA(x) ‚àíK) = 0."
SINGLE-REGION REPAIRS,0.26688102893890675,"The repaired network bf(x) = f(x) + hA(x, Œ≥). Since f and hA are both ReLU DNNs, we have bf
is also a ReLU DNN. We will give the formal guarantees on correctness in Theorem 1."
MULTI-REGION REPAIRS,0.27009646302250806,"3.5
MULTI-REGION REPAIRS"
MULTI-REGION REPAIRS,0.2733118971061093,"Suppose there are two linear regions, A1 and A2, that need to be repaired, and we have generated
the affine patch function pA1(x) for A1 and pA2(x) for A2."
MULTI-REGION REPAIRS,0.2765273311897106,"If A1 ‚à©A2 = ‚àÖ, then we can repair f(x) with bf(x) = f(x) + hA1(x, Œ≥) + hA2(x, Œ≥) directly, since
hA1(x, Œ≥) and hA2(x, Œ≥) will not be nonzero at the same time when Œ≥ is large enough."
MULTI-REGION REPAIRS,0.2797427652733119,"However, if A1 ‚à©A2 Ã∏= ‚àÖ, for any x ‚ààA1 ‚à©A2, both hA1(x, Œ≥) and hA2(x, Œ≥) will alter the value
of f on x, which will invalidate both repairs and cannot guarantee that the repaired DNN will meet
the specification Œ¶. To avoid such over-repairs, our strategy is to first repair A1 ‚à™A2 with pA1(x),
and then repair A2 with pA2(x) ‚àípA1(x). Figure 3 provides an illustration of a three-region case."
MULTI-REGION REPAIRS,0.2829581993569132,"In general, for multi-region repair, we note {Al}l=1,2,...,L are all the buggy linear regions. Then we
compute the support network gAl(x, Œ≥) and affine patch function pAl(x) for each Al. Note that this
computation can be done in parallel."
MULTI-REGION REPAIRS,0.2861736334405145,Published as a conference paper at ICLR 2022
MULTI-REGION REPAIRS,0.28938906752411575,A1 : f
MULTI-REGION REPAIRS,0.29260450160771706,A2 : f
MULTI-REGION REPAIRS,0.2958199356913183,A3 : f
MULTI-REGION REPAIRS,0.2990353697749196,A3 : f + pA1 ‚áí
MULTI-REGION REPAIRS,0.3022508038585209,A1 : f + pA1
MULTI-REGION REPAIRS,0.3054662379421222,A2 : f + pA1
MULTI-REGION REPAIRS,0.3086816720257235,A3 : f + pA1 ‚áí
MULTI-REGION REPAIRS,0.31189710610932475,A1 : f + pA1
MULTI-REGION REPAIRS,0.31511254019292606,A2 : f + pA2
MULTI-REGION REPAIRS,0.3183279742765273,A3 : f + pA2 ‚áí
MULTI-REGION REPAIRS,0.3215434083601286,A1 : f + pA1
MULTI-REGION REPAIRS,0.3247588424437299,A2 : f + pA2
MULTI-REGION REPAIRS,0.3279742765273312,A3 : f + pA3
MULTI-REGION REPAIRS,0.3311897106109325,"Figure 3: An illustration of multi-region repair with three different repair regions. Left: the original
DNN; Middle Left: repair A1 ‚à™A2 ‚à™A3 with pA1; Middle Right: repair A2 ‚à™A3 with pA2 ‚àípA1;
Right: repair A3 with pA3 ‚àípA2"
MULTI-REGION REPAIRS,0.33440514469453375,"Once we have gAl(x, Œ≥) and pAl(x), we can ‚Äústitch‚Äù multiple local patches into a final patch as
follows."
MULTI-REGION REPAIRS,0.33762057877813506,"h(x, Œ≥) =
X"
MULTI-REGION REPAIRS,0.3408360128617363,"l
[œÉ(pAl(x) ‚àípAl‚àí1(x) + max
j‚â•l {gAj(x, Œ≥)}Kl ‚àíKl)"
MULTI-REGION REPAIRS,0.3440514469453376,"‚àíœÉ(‚àípAl(x) + pAl‚àí1(x) + max
j‚â•l {gAj(x, Œ≥)}Kl ‚àíKl)]
(8)"
MULTI-REGION REPAIRS,0.34726688102893893,where Kl is the upper bound of {|pAl(x) ‚àípAl‚àí1(x)|‚àû|x ‚ààX} and pA0(x) = 0.
MULTI-REGION REPAIRS,0.3504823151125402,"Remark: maxj‚â•l{gAj(x, Œ≥)} is a support function for ‚à™j‚â•lAj and its value is 1 for any x ‚àà
‚à™j‚â•lAj."
THEORETICAL GUARANTEES,0.3536977491961415,"4
THEORETICAL GUARANTEES"
THEORETICAL GUARANTEES,0.35691318327974275,"In this section, we present the theoretical guarantees that REASSURE provides, and point the readers
to proofs of the theorems in the Appendix."
THEORETICAL GUARANTEES,0.36012861736334406,"Theorem 1 (Soundness). The repaired DNN bf returned by REASSURE is guaranteed to satisfy the
specification Œ¶.
Theorem 2 (Completeness). REASSURE can always find a solution to the minimal point-wise repair
or the minimal area repair problem."
THEORETICAL GUARANTEES,0.3633440514469453,"For any A, the support network ensures that the patch network goes to zero quickly when x is away
from A. However, it still makes a small change on the neighbors of A. The following theorem
shows that for a big enough Œ≥, the patch network would not change a correct region into incorrect.
Theorem 3 (Limited Side Effect). Given a correctness property Œ¶ = (Œ¶in, Œ¶out), a patch region
A and the corresponding patch network h(x, Œ≥), there exists a positive number Œì such that for any
Œ≥ ‚â•Œì, we have"
THEORETICAL GUARANTEES,0.3665594855305466,"1. for any linear region B, if B ‚à©A = ‚àÖ, then bf(x, Œ≥) = f(x);"
THEORETICAL GUARANTEES,0.36977491961414793,"2. for any linear region C who is a neighbor of A (C‚à©A Ã∏= ‚àÖ), if f|C |= Œ¶, then bfC(x, Œ≥) |= Œ¶.
Corollary 1 (Incremental Repair). For multiple-region repair, the patch for a new region A‚Ä≤ would
not cause a previous patched region A to become incorrect."
THEORETICAL GUARANTEES,0.3729903536977492,"Theorem 4 (Minimum Repair). For any ReLU DNN Àúf, which is linear on a patch region A and
satisfies the specification Œ¶, there exists a positive number Œì, such that for all Œ≥ ‚â•Œì,"
THEORETICAL GUARANTEES,0.3762057877813505,"max
x‚ààX | Àúf(x) ‚àíf(x)| ‚â•max
x‚ààX |hA(x, Œ≥)|.
(9)"
THEORETICAL GUARANTEES,0.37942122186495175,"Theorem 5 (Polynomial-Time Efficiency). REASSURE terminates in polynomial-time in the size of
the neural network and the number of buggy linear regions when Œ¶out takes the form of {y | ql ‚â§
Py ‚â§qu} where P is a full row rank matrix and ‚àí‚àû‚â§ql[i] ‚â§qu[i] ‚â§+‚àû(ql[i] and qu[i] are the
i-th elements of ql and qu respectively)."
EXPERIMENTS,0.38263665594855306,"5
EXPERIMENTS"
EXPERIMENTS,0.3858520900321543,"In this Section, we compare REASSURE with state-of-the-art methods on both point-wise repairs
and area repairs. The experiments were designed to answer the following questions: (Effectiveness)"
EXPERIMENTS,0.3890675241157556,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.39228295819935693,"REASSURE
Retrain (Requires Training Data)
#P
ND(L‚àû)
ND(L2)
NDP(L‚àû)
NDP(L2)
Acc
ND(L‚àû)
ND(L2)
NDP(L‚àû)
NDP(L2)
Acc
10
0.01%
0.01%
25.75%
24.78%
98.1%
1.13%
1.09%
77.86%
77.60%
98.1%
20
0.03%
0.02%
19.17%
18.70%
98.2%
0.92%
0.89%
77.14%
76.29%
98.4%
50
0.06%
0.06%
24.64%
23.79%
98.5%
0.84%
0.82%
84.17%
82.15%
98.7%
100
0.11%
0.12%
25.40%
24.44%
99.0%
0.84%
0.82%
84.83%
83.05%
99.0%"
EXPERIMENTS,0.3954983922829582,"Fine-Tuning
PRDNN
#P
ND(L‚àû)
ND(L2)
NDP(L‚àû)
NDP(L2)
Acc
ND(L‚àû)
ND(L2)
NDP(L‚àû)
NDP(L2)
Acc
10
2.20%
2.11%
67.61%
65.52%
97.6%
1.41%
1.34%
34.60%
33.59%
97.8%
20
23.19%
22.35%
82.87%
78.55%
78.6%
2.88%
2.74%
43.63%
41.78%
97.1%
50
35.78%
34.04%
84.73%
80.58%
67.0%
4.79%
4.47%
49.37%
46.31%
96.7%
100
23.83%
22.11%
79.73%
76.57%
81.9%
9.16%
8.20%
51.23%
46.34%
96.1%"
EXPERIMENTS,0.3987138263665595,"Table 2: Point-wise Repairs on MNIST. We use the first hidden layer as the repair layer for PRDNN.
The test accuracy of the original DNN is 98.0%. #P: number of buggy points to repair. ND(L‚àû),
ND(L2): average (L‚àû, L2) norm difference on both training and test data. NDP(L‚àû), NDP(L2):
average (L‚àû, L2) norm difference on random sampled points near the buggy points. Acc: accuracy
on test data. Note that REASSURE automatically performs area repairs on 784-dimensional inputs."
EXPERIMENTS,0.40192926045016075,"How effective is a repair in removing known buggy behaviors? (Locality) How much side effect (i.e.
modification outside the patch area in the function space) does a repair produce? (Function Change)
How much does a repair change the original neural network in the function space? (Performance)
Whether and how much does a repair adversely affect the overall performance of the neural network?"
EXPERIMENTS,0.40514469453376206,"We consider the following evaluation criteria:
1. Efficacy (E): % of given buggy points or
buggy linear regions that are repaired.
2. Norm Difference (ND): average normalized norm
(L‚àûor L2) difference between the original DNN and the repaired DNN on a set of inputs (e.g.
training and testing data; more details in the tables). We use ND to measure how a repair change the
original neural network on function space.
3. Norm Difference on Patch Area (NDP): average
normalized norm (L‚àûor L2) difference between the original DNN and the repaired DNN on patch
areas (calculated on random sampled points on patch areas or near the buggy points; details in the
tables). We use NDP to measure the locality of a repair.
4. Accuracy (Acc): accuracy on training
or testing data to measure the extent to which a repair preserves the performance of the original
neural network.
5. Negative Side Effect (NSE): NSE is only for area repair. It is the percentage
of correct linear regions (outside of patch area) that become incorrect after a repair. If a repair has a
nonzero NSE, the new repair may invalidate a previous repair and lead to a circular repair problem."
EXPERIMENTS,0.40836012861736337,"We compared REASSURE with the representative related works in Table 1. REASSURE, MDNN and
PRDNN guarantee to repair all the buggy points (linear regions). Retrain and Fine-Tuning cannot
guarantee 100% efficacy in general and we run them until all the buggy points are repaired."
EXPERIMENTS,0.4115755627009646,"5.1
POINT-WISE REPAIRS: MNIST"
EXPERIMENTS,0.41479099678456594,"We train a ReLU DNN on the MNIST dataset LeCun (1998) as the target DNN. The goal of a repair
is to fix the behaviors of the target DNN on buggy inputs that are found in the test dataset. Thus, the
repaired DNN is expected to produce correct predictions for all the buggy inputs."
EXPERIMENTS,0.4180064308681672,"The results are shown in Table 2. REASSURE achieves almost zero modification outside the patch
area (ND) amongst all four methods. In addition, REASSURE produces the smallest modification on
the patch area (NDP) and preserves the performance of the original DNN (almost no drop on Acc)."
EXPERIMENTS,0.4212218649517685,"5.2
AREA REPAIRS: HCAS"
EXPERIMENTS,0.42443729903536975,"To the best of our knowledge, Sotoudeh & Thakur (2021) is the only other method that supports
area repairs. In this experiment, we compare REASSURE with Sotoudeh & Thakur (2021) on an
experiment where the setting is similar to the 2D Polytope ACAS Xu repair in their paper."
EXPERIMENTS,0.42765273311897106,"Sotoudeh & Thakur (2021) does not include a vertex enumeration tool (which is required for setting
up their LP problem) in their code. We use pycddlib Troffaes (2018) to perform the vertex
enumeration step when evaluating PRDNN. Note that the vertex enumeration tool does not affect
the experimental results except running time."
EXPERIMENTS,0.43086816720257237,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.4340836012861736,"REASSURE
PRDNN
#A
ND(L‚àû)
NDP(L‚àû)
NSE
Acc
T
ND(L‚àû)
NDP(L‚àû)
NSE
Acc
T
10
0.00%
0.0%
0%
98.1%
1.0422
0.10%
31.6%
4%
89.6%
2.90+0.100
20
0.00%
2.2%
0%
98.1%
1.1856
0.15%
37.2%
8%
83.1%
5.81+0.185
50
0.00%
17.6%
0%
98.1%
1.8174
0.15%
38.4%
8%
83.8%
14.54+0.388
87
0.04%
45.9%
0%
97.8%
2.4571
0.14%
46.6%
0%
85.6%
25.30+0.714"
EXPERIMENTS,0.43729903536977494,"Table 3: Area Repairs on HCAS. We use the the first hidden layer as the repair layer for PRDNN.
Results on PRDNN using the last layer (which are inferior to using the first layer) are shown in
Table 6 in the Appendix 7.4. The test accuracy of the original DNN is 97.9%. #A: number of buggy
linear regions to repair. ND(L‚àû): average L‚àûnorm difference on training data. NDP(L‚àû): average
L‚àûnorm difference on random sampled data on input constraints of specification 1. NSE: % of
correct linear regions changed to incorrect by the repair. Acc: accuracy on training data (no testing
data available). T: running time in seconds. For PRDNN, the first running time is for enumerating
all the vertices of the polytopes and the second is for solving the LP problem in PRDNN."
EXPERIMENTS,0.4405144694533762,"REASSURE (feature space)
Retrain (Requires Training Data)
Fine-Tuning
PRDNN
#P
ND(L‚àû)
ND(L2)
Acc
ND(L‚àû)
ND(L2)
Acc
ND(L‚àû)
ND(L2)
Acc
ND(L‚àû)
ND(L2)
Acc
10
0.15%
0.13%
82.5%
43.43%
36.04%
80.1%
29.45%
25.27%
77.9%
22.93%
21.13%
82.1%
20
0.12%
0.11%
81.3%
42.78%
35.69%
82.9%
69.16%
57.47%
68.5%
21.91%
20.03%
80.1%
50
0.79%
0.70%
81.3%
50.23%*
42.86%*
82.1%*
76.69%
63.46%
66.9%
30.96%
26.94%
68.9%"
EXPERIMENTS,0.4437299035369775,"Table 4: Point-wise Repairs on ImageNet. PRDNN uses parameters in the last layer for repair.
The test accuracy for the original DNN is 83.1%. #P: number of buggy points to repair. ND(L‚àû),
ND(L2): average (L‚àû, L2) norm difference on validation data. Acc: accuracy on validation data. *
means Retrain only repair 96% buggy points in 100 epochs."
EXPERIMENTS,0.44694533762057875,"We consider an area repair where the target DNN is the HCAS network (simplified version of ACAS
Xu)2 N1,4 (previous advisory equal to 1 and time to loss of vertical separation equal to 20s) from
Julian & Kochenderfer (2019). We use Specification 1 (details in Appendix 7.4), which is similar
to Property 5 in Katz et al. (2017). We compute all the linear regions for N1,4 in the area Œ¶in of
Specification 1 and 87 buggy linear regions were found. We apply both REASSURE and PRDNN
to repair those buggy linear regions. We use Specification 2 (details in Appendix 7.4), the dual of
Specification 1, to test the negative side effect (NSE) of a repair."
EXPERIMENTS,0.45016077170418006,"The results are shown in Table 3. Both REASSURE and PRDNN successfully repair all the buggy
linear regions. REASSURE produces repairs that are significantly better in terms of locality (ND),
minimality (NDP) and performance preservation (Acc)."
FEATURE-SPACE REPAIRS,0.4533762057877814,"5.3
FEATURE-SPACE REPAIRS"
FEATURE-SPACE REPAIRS,0.4565916398713826,"In general, when repairing a large DNN with a high input dimension, the number of linear constraints
for one patch area A will be huge and pose a challenge to solving the resulting LP."
FEATURE-SPACE REPAIRS,0.45980707395498394,"One advantage of our approach, which can be used to mitigate this problem, is that it allows for
point-wise and area repairs in the feature space in a principled manner, i.e. constructing a patch
network starting from an intermediate layer. This approach still preserves soundness and complete-
ness, and is fundamentally different from just picking a single layer for repair in PRDNN or MDNN.
Experimental results on repairing AlexNet Krizhevsky et al. (2012) for the ImageNet dataset Rus-
sakovsky et al. (2015) in Appendix 7.4 show REASSURE (feature space) is still significantly better
in term of locality(ND) and minimality (NDP)."
CONCLUSION,0.4630225080385852,"6
CONCLUSION"
CONCLUSION,0.4662379421221865,"We have presented a novel approach for repairing ReLU DNNs with strong theoretical guarantees.
Across a set of benchmarks, our approach significantly outperforms existing methods in terms of
efficacy, locality, and limiting negative side effects. Future directions include further investigation
on feature-space repairs and identifying a lower-bound for Œ≥."
THE TECHNIQUE IN PRDNN FOR COMPUTING LINEAR REGIONS DOES NOT SCALE BEYOND TWO DIMENSIONS AS STATED IN,0.4694533762057878,"2The technique in PRDNN for computing linear regions does not scale beyond two dimensions as stated in
their paper. The input space of HCAS is 3D and that of ACAS Xu is 5D so we use HCAS in order to run their
tool in our evaluation of area repairs."
THE TECHNIQUE IN PRDNN FOR COMPUTING LINEAR REGIONS DOES NOT SCALE BEYOND TWO DIMENSIONS AS STATED IN,0.47266881028938906,Published as a conference paper at ICLR 2022
THE TECHNIQUE IN PRDNN FOR COMPUTING LINEAR REGIONS DOES NOT SCALE BEYOND TWO DIMENSIONS AS STATED IN,0.4758842443729904,ACKNOWLEDGEMENT
THE TECHNIQUE IN PRDNN FOR COMPUTING LINEAR REGIONS DOES NOT SCALE BEYOND TWO DIMENSIONS AS STATED IN,0.4790996784565916,"This effort was partially supported by the Intelligence Advanced Research Projects Agency (IARPA)
under the contract W911NF20C0038. The content of this paper does not necessarily reflect the
position or the policy of the Government, and no official endorsement should be inferred."
REFERENCES,0.48231511254019294,REFERENCES
REFERENCES,0.4855305466237942,"Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning your weak-
ness into a strength: Watermarking deep neural networks by backdooring. In 27th {USENIX}
Security Symposium ({USENIX} Security 18), pp. 1615‚Äì1631, 2018. 16"
REFERENCES,0.4887459807073955,"Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. CoRR, abs/1611.01491, 2016. URL http://arxiv.
org/abs/1611.01491. 3, 18"
REFERENCES,0.4919614147909968,"Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao,
and Karol Zieba. End to end learning for self-driving cars. CoRR, abs/1604.07316, 2016. 1"
REFERENCES,0.49517684887459806,"David D Bremner. On the complexity of vertex and facet enumeration for convex polytopes. PhD
thesis, Citeseer, 1997. 6"
REFERENCES,0.4983922829581994,"Guoliang Dong, Jun Sun, Jingyi Wang, Xinyu Wang, and Ting Dai. Towards repairing neural net-
works correctly. arXiv preprint arXiv:2012.01872, 2020. 2"
REFERENCES,0.5016077170418006,"Tommaso Dreossi,
Shromona Ghosh,
Xiangyu Yue,
Kurt Keutzer,
Alberto Sangiovanni-
Vincentelli, and Sanjit A Seshia. Counterexample-guided data augmentation. arXiv preprint
arXiv:1805.06962, 2018. 1"
REFERENCES,0.5048231511254019,"Ben Goldberger, Guy Katz, Yossi Adi, and Joseph Keshet. Minimal modifications of deep neural
networks using verification. In LPAR, volume 2020, pp. 23rd, 2020. 2, 3, 16"
REFERENCES,0.5080385852090032,"Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2021. URL https://www.
gurobi.com. 16"
REFERENCES,0.5112540192926045,"Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adver-
sarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 15262‚Äì15271, 2021. 16"
REFERENCES,0.5144694533762058,"Martin Henk, J¬®urgen Richter-Gebert, and G¬®unter M. Ziegler. Basic properties of convex polytopes.
In HANDBOOK OF DISCRETE AND COMPUTATIONAL GEOMETRY, CHAPTER 13, pp. 243‚Äì
270. CRC Press, Boca, 1997. 6"
REFERENCES,0.5176848874598071,"Kyle D Julian and Mykel J Kochenderfer. Guaranteeing safety for neural network-based aircraft col-
lision avoidance systems. In 2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC),
pp. 1‚Äì10. IEEE, 2019. 9"
REFERENCES,0.5209003215434084,"Kyle D Julian, Mykel J Kochenderfer, and Michael P Owen. Deep neural network compression
for aircraft collision avoidance systems. Journal of Guidance, Control, and Dynamics, 42(3):
598‚Äì608, 2019. 1"
REFERENCES,0.5241157556270096,"Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An
efficient smt solver for verifying deep neural networks. In International Conference on Computer
Aided Verification, pp. 97‚Äì117. Springer, 2017. 9"
REFERENCES,0.5273311897106109,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hass-
abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forget-
ting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521‚Äì3526,
2017. ISSN 0027-8424. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/
content/114/13/3521. 1"
REFERENCES,0.5305466237942122,Published as a conference paper at ICLR 2022
REFERENCES,0.5337620578778135,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097‚Äì1105,
2012. 9, 16, 17"
REFERENCES,0.5369774919614148,"Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. 8"
REFERENCES,0.5401929260450161,"Guang-He Lee, David Alvarez-Melis, and Tommi S Jaakkola. Towards robust, locally linear deep
networks. arXiv preprint arXiv:1907.03207, 2019. 4"
REFERENCES,0.5434083601286174,"Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, and Ananth Grama. Mode: automated
neural network model debugging via state differential analysis and input selection. In Proceed-
ings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, pp. 175‚Äì186, 2018. 1"
REFERENCES,0.5466237942122186,"Xuhong Ren, Bing Yu, Hua Qi, Felix Juefei-Xu, Zhuo Li, Wanli Xue, Lei Ma, and Jianjun Zhao.
Few-shot guided mix for dnn repairing. In 2020 IEEE International Conference on Software
Maintenance and Evolution (ICSME), pp. 717‚Äì721. IEEE, 2020. 1"
REFERENCES,0.5498392282958199,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211‚Äì252, 2015. doi: 10.1007/s11263-015-0816-y. 9, 16"
REFERENCES,0.5530546623794212,"Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting linear
regions of deep neural networks. CoRR, abs/1711.02114, 2017. URL http://arxiv.org/
abs/1711.02114. 3"
REFERENCES,0.5562700964630225,"Nida Shahid, Tim Rappon, and Whitney Berta.
Applications of artificial neural networks in
health care organizational decision-making: A scoping review.
PLOS ONE, 14(2):1‚Äì22, 02
2019. doi: 10.1371/journal.pone.0212356. URL https://doi.org/10.1371/journal.
pone.0212356. 1"
REFERENCES,0.5594855305466238,"Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, and Artem Babenko. Editable
neural networks. arXiv preprint arXiv:2004.00345, 2020. 1"
REFERENCES,0.5627009646302251,"Matthew Sotoudeh and Aditya V Thakur. Provable repair of deep neural networks. In Proceedings
of the 42nd ACM SIGPLAN International Conference on Programming Language Design and
Implementation, pp. 588‚Äì603, 2021. 2, 3, 8"
REFERENCES,0.5659163987138264,"Matthias Troffaes. pycddlib-a python wrapper for komei fukudals cddlib, 2018. 8"
REFERENCES,0.5691318327974276,"Muhammad Usman, Divya Gopinath, Youcheng Sun, Yannic Noller, and Corina S PÀòasÀòareanu. Nn
repair: Constraint-based repair of neural network classifiers.
In International Conference on
Computer Aided Verification, pp. 3‚Äì25. Springer, 2021. 2"
REFERENCES,0.572347266881029,Published as a conference paper at ICLR 2022
APPENDIX,0.5755627009646302,"7
APPENDIX"
REPAIR VIA LINEAR PROGRAMMING,0.5787781350482315,"7.1
REPAIR VIA LINEAR PROGRAMMING"
REPAIR VIA LINEAR PROGRAMMING,0.5819935691318328,"We consider the case Œ¶out can be expressed as {y|ql ‚â§Py ‚â§qu} where P is a full row rank matrix
and ‚àí‚àû‚â§ql[i] ‚â§qu[i] ‚â§+‚àû(ql[i] and qu[i] are the i-th elements of ql and qu respectively)."
REPAIR VIA LINEAR PROGRAMMING,0.5852090032154341,"Consider the following optimization problem.
minT maxx‚ààA |T(f(x) ‚àíf(x)|
ql ‚â§P(T(f(x))) ‚â§qu, ‚àÄx ‚ààA
(10)"
REPAIR VIA LINEAR PROGRAMMING,0.5884244372990354,where T : Rn ‚ÜíRn is a linear transformation on the DNN‚Äôs output space Rn.
REPAIR VIA LINEAR PROGRAMMING,0.5916398713826366,"Theorem 6. On linear region A, we have f|A(x) = f1x + f2 for some matrix f1 and vector f2.
Assuming that f1 is full rank3, the optimization problem in (3) and the optimization problem in (10)
are equivalent."
REPAIR VIA LINEAR PROGRAMMING,0.594855305466238,"Note that ql ‚â§P(T(f(x))) ‚â§qu can be achieved row by row. Thus, we can find a one-dimensional
linear transformation via LPs and combine them into a single linear transformation to solve opti-
mization problem 10."
REPAIR VIA LINEAR PROGRAMMING,0.5980707395498392,"For every row of P, say the i-th row, we can check the lower bound and upper bound of
{P(f(x)) | ‚àÄx ‚ààA} on the i-th dimension by solving the following LP problems"
REPAIR VIA LINEAR PROGRAMMING,0.6012861736334405,"lb[i] = min
x‚ààA P[i](f(x))
ub[i] = max
x‚ààA P[i](f(x))
(11)"
REPAIR VIA LINEAR PROGRAMMING,0.6045016077170418,where P[i] is the i-th row of P.
REPAIR VIA LINEAR PROGRAMMING,0.6077170418006431,"Then for each row i, we take a minimal linear transformation V [i](x) = v1[i](x) + v2[i] to transfer
interval [lb[i], ub[i]] inside interval [ql[i], qu[i]]. We can take v1[i] = 1 if qu[i] ‚àíql[i] > ub[i] ‚àílb[i],
else v1[i] = qu[i]‚àíql[i]"
REPAIR VIA LINEAR PROGRAMMING,0.6109324758842444,"ub[i]‚àílb[i] . And v2[i] = ql[i] ‚àív1[i]lb[i] if |ql[i] ‚àív1[i]lb[i]| ‚â§|v1[i]ub[i] ‚àíqu[i]|, else
v1[i]ub[i] ‚àíqu[i]."
REPAIR VIA LINEAR PROGRAMMING,0.6141479099678456,"Since matrix P is full row rank, we can find a linear transformation T that is equivalent to V :"
REPAIR VIA LINEAR PROGRAMMING,0.617363344051447,"T = ÀÜP ‚àí1V ÀÜP ‚áíP(T(f(x))) = V (P(f(x)))
(12)"
REPAIR VIA LINEAR PROGRAMMING,0.6205787781350482,"where ÀÜP =

P
P ‚ä•"
REPAIR VIA LINEAR PROGRAMMING,0.6237942122186495,"
is an orthogonal extension of P (P and P ‚ä•are orthogonal to each other and"
REPAIR VIA LINEAR PROGRAMMING,0.6270096463022508,ÀÜP is a full rank square matrix).
REPAIR VIA LINEAR PROGRAMMING,0.6302250803858521,"Once we have T, we can obtain an affine patch function pA(x) = T(f(x)) ‚àíf(x)."
THE REASSURE ALGORITHM,0.6334405144694534,"7.2
THE REASSURE ALGORITHM"
THE REASSURE ALGORITHM,0.6366559485530546,"Algorithm 1 REASSURE
Input: A specification Œ¶ = (Œ¶in, Œ¶out), a ReLU DNN f and a set of buggy points {ex1, . . . , exL} ‚äÇ
Œ¶in.
Output: A repaired ReLU DNN bf."
THE REASSURE ALGORITHM,0.639871382636656,"1: for l = 1 to L do
2:
Generate the patch area Al from buggy point exl according to Equation (1);
3:
Generate a support network gA according to Equation (2);
4:
Solve the linear programming problem (6) to find the optimal affine patch network pA.
5: end for
6: Combine all support networks gAl and the corresponding patch networks pAl to get the overall
patch network h according to Equation (8).
7: return bf = f + h"
THE REASSURE ALGORITHM,0.6430868167202572,"3Note that for neural networks that are trained by a stochastic method, with probability one f1 is full rank."
THE REASSURE ALGORITHM,0.6463022508038585,Published as a conference paper at ICLR 2022
PROOFS OF THEOREMS,0.6495176848874598,"7.3
PROOFS OF THEOREMS"
PROOFS OF THEOREMS,0.6527331189710611,"We prove Theorem 1 after Corollary 1, since the proof of Theorem 1 uses the result of Corollary 1.
Theorem 6. On linear region A, we have f|A(x) = f1x + f2 for some matrix f1 and vector f2.
Assuming that f1 is full rank4, the optimization problem in (3) and the optimization problem in (10)
are equivalent."
PROOFS OF THEOREMS,0.6559485530546624,"Proof. On one side, for any c, d, since f1 is full rank, there exists a linear transformation T, such
that T(f(x)) = T(f1x + f2) = (f1 + c)x + (f2 + d) = f(x) + cx + d."
PROOFS OF THEOREMS,0.6591639871382636,"On the other side, for any T, since T(f(x)) ‚àíf(x) is linear, there exist c, d, such that cx + d =
T(f(x)) ‚àíf(x)."
PROOFS OF THEOREMS,0.662379421221865,"Lemma 2. The repaired DNN bf returned by REASSURE is guaranteed to satisfy the specification
Œ¶ on patch area A in single-region repair case."
PROOFS OF THEOREMS,0.6655948553054662,"Proof. By the definition of pA, we have f(x) + pA(x) ‚ààŒ¶out for all x ‚ààA."
PROOFS OF THEOREMS,0.6688102893890675,"For any x ‚ààA, we have gA(x, Œ≥) = 1 and hA(x, Œ≥) = œÉ(pA(x)) ‚àíœÉ(‚àípA(x)) = pA(x). There-
fore,
bf(x) = f(x) + hA(x, Œ≥) = f(x) + pA(x) ‚ààŒ¶out
(13)"
PROOFS OF THEOREMS,0.6720257234726688,"Thus, the patched neural network bf meets the specification Œ¶ on A."
PROOFS OF THEOREMS,0.6752411575562701,"Theorem 2 (Completeness). REASSURE can always find a solution to the minimal point-wise repair
or the minimal area repair problem."
PROOFS OF THEOREMS,0.6784565916398714,"Proof. For every patch area A, we can always find a support network gA. For any Œ¶out and A,
there exists an affine function pA such that pA(x) ‚ààŒ¶out, ‚àÄx ‚ààA. Therefore, the LP (6) is always
feasible and REASSURE can find an affine patch function pA."
PROOFS OF THEOREMS,0.6816720257234726,"Once we have gA and pA for patch area A, REASSURE returns a patch network either by Equation
(7) or by Equation (8)."
PROOFS OF THEOREMS,0.684887459807074,"Theorem 3 (Limited Side Effect). Given a correctness property Œ¶ = (Œ¶in, Œ¶out), a patch region
A and the corresponding patch network h(x, Œ≥), there exists a positive number Œì such that for any
Œ≥ ‚â•Œì, we have"
PROOFS OF THEOREMS,0.6881028938906752,"1. for any linear region B, if B ‚à©A = ‚àÖ, then bf(x, Œ≥) = f(x);"
PROOFS OF THEOREMS,0.6913183279742765,"2. for any linear region C who is a neighbor of A (C‚à©A Ã∏= ‚àÖ), if f|C |= Œ¶, then bfC(x, Œ≥) |= Œ¶."
PROOFS OF THEOREMS,0.6945337620578779,"Proof. Since a multi-region repair is a composition of multiple singe-region repairs according to
Equation (8), we can prove the limited side effect of a multi-region repair by proving the limited
side effect of its constituent singe-region repairs. Below, we prove the limited side effect of a singe-
region repair."
PROOFS OF THEOREMS,0.6977491961414791,"Consider patch area A = {x | aix ‚â§bi, i ‚ààI} and A>0(Œ≥) = {x | h(x, Œ≥) > 0}."
PROOFS OF THEOREMS,0.7009646302250804,"1. Since the number of neighbors for A are finite, we can take a big enough Œ≥, such that for any B,
if B ‚à©A = ‚àÖ, B ‚à©A>0(Œ≥) = ‚àÖ. Thus, we have bf(x, Œ≥) = f(x) on B."
PROOFS OF THEOREMS,0.7041800643086816,"2. For any linear region C who is a neighbor of A, i.e. C Ã∏= A and C ‚à©A Ã∏= ‚àÖ, bf is no longer a
linear function on C, since there are some hyperplanes introduced by our repair that will divide C
into multiple linear regions."
PROOFS OF THEOREMS,0.707395498392283,"Specifically, those hyperplanes are {x | Œ≥(aix ‚àíbi) + 1 = 0} for i ‚ààI, {x | P"
PROOFS OF THEOREMS,0.7106109324758842,"i‚ààI g(aix ‚àíbi, Œ≥) ‚àí
|I| + 1 = 0}, {x | p(x) + K ¬∑ gA(x, Œ≥) ‚àíK = 0} and {x | ‚àíp(x) + K ¬∑ gA(x, Œ≥) ‚àíK = 0}."
PROOFS OF THEOREMS,0.7138263665594855,"For any point x in those hyperplanes, it will fall into one of the following four cases."
PROOFS OF THEOREMS,0.7170418006430869,"4Note that for neural networks that are trained by a stochastic method, with probability one f1 is full rank."
PROOFS OF THEOREMS,0.7202572347266881,Published as a conference paper at ICLR 2022
PROOFS OF THEOREMS,0.7234726688102894,"(a) x ‚àà{x | Œ≥(aix ‚àíbi) + 1 = 0} for some i ‚ààI, then gA(x, Œ≥) = 0, h(x, Œ≥) = 0 and
bf(x) ‚ààŒ¶out;"
PROOFS OF THEOREMS,0.7266881028938906,"(b) x ‚àà{x | P
i‚ààI g(aix ‚àíbi, Œ≥) ‚àí|I| + 1 = 0}, then gA(x, Œ≥) = 0, h(x, Œ≥) = 0 and
bf(x) ‚ààŒ¶out;"
PROOFS OF THEOREMS,0.729903536977492,"(c) x ‚àà{x | p(x) + K ¬∑ gA(x, Œ≥) ‚àíK = 0}, then p(x) = K ‚àíK ¬∑ gA(x, Œ≥) ‚â•0,
‚àíp(x) + K ¬∑ gA(x, Œ≥) ‚àíK ‚â§0, h(x, Œ≥) = 0 and bf(x) ‚ààŒ¶out;"
PROOFS OF THEOREMS,0.7331189710610932,"(d) x ‚àà{x | ‚àíp(x) + K ¬∑ gA(x, Œ≥) ‚àíK}, then p(x) = K ¬∑ gA(x, Œ≥) ‚àíK ‚â§0, p(x) + K ¬∑
gA(x, Œ≥) ‚àíK ‚â§0, h(x, Œ≥) = 0 and bf(x) ‚ààŒ¶out;"
PROOFS OF THEOREMS,0.7363344051446945,"By the above analysis, we have bf(x) ‚ààŒ¶out for the boundary of the new linear regions. Since bf is
linear on the new linear regions and Œ¶out is convex, bf(x) ‚ààŒ¶out for any x ‚ààC."
PROOFS OF THEOREMS,0.7395498392282959,"Remark: By Theorem 3, we have that a patch would not change a correct linear region to an
incorrect one."
PROOFS OF THEOREMS,0.7427652733118971,"Corollary 1 (Incremental Repair). For multiple-region repair, the patch for a new region A‚Ä≤ would
not cause a previous patched region A to become incorrect."
PROOFS OF THEOREMS,0.7459807073954984,"Proof. After applying the patch to linear region A, we have that the resulting network is correct on
A. When applying a new patch to another linear region A‚Ä≤, by Theorem 3, the new patch would not
make a correct linear region A incorrect."
PROOFS OF THEOREMS,0.7491961414790996,"Theorem 1 (Soundness). The repaired DNN bf returned by REASSURE is guaranteed to satisfy the
specification Œ¶."
PROOFS OF THEOREMS,0.752411575562701,Proof. The proof has two parts:
PROOFS OF THEOREMS,0.7556270096463023,"1. to show that bf satisfies the specification Œ¶ on A, and"
PROOFS OF THEOREMS,0.7588424437299035,2. to show that bf satisfies the specification Œ¶ outside of A.
PROOFS OF THEOREMS,0.7620578778135049,Part 1:
PROOFS OF THEOREMS,0.7652733118971061,Lemma (2) shows bf satisfy the specification Œ¶ for single-region repair on A.
PROOFS OF THEOREMS,0.7684887459807074,"For the multi-region case, consider a set of buggy linear regions ‚à™1‚â§l‚â§IAl with the corresponding
support neural network gAl and affine patch function pAl for each Al. For the multi-region repair
construction in Equation (8), we refer to œÉ(pAj ‚àípAj‚àí1 +maxk‚â•j{gAk}Kj ‚àíKj) as the j-th patch
and bfj = f + P"
PROOFS OF THEOREMS,0.7717041800643086,"j‚Ä≤‚â§j œÉ(pAj‚Ä≤ ‚àípAj‚Ä≤‚àí1 + maxk‚â•j‚Ä≤{gAk}Kj ‚àíKj) as the network after the j-th
patch."
PROOFS OF THEOREMS,0.77491961414791,"For any x in patch area ‚à™1‚â§l‚â§IAl, we can find a j such that x ‚ààAj but x /‚ààAk for all k >
j. After the first j patches œÉ(pA1(x) + maxk‚â•1{gAk(x, Œ≥)}K1 ‚àíK1), œÉ(pA2(x) ‚àípA1(x) +
maxk‚â•2{gAk(x, Œ≥)}K2 ‚àíK2), ... , œÉ(pAj(x) ‚àípAj‚àí1(x) + maxk‚â•j{gAk(x, Œ≥)}Kj ‚àíKj), the
DNN‚Äôs output at x becomes bfj(x) = f(x) + pAj(x) which meets our specification Œ¶ at x by the
definition of pAj(x)."
PROOFS OF THEOREMS,0.7781350482315113,"Since x /‚ààAk for all k > j, then by Corollary 1, the rest of the patches would not change a correct
area to an incorrect area. Therefore, we have the final patched neural network bf meets specification
Œ¶ on ‚à™1‚â§l‚â§IAl."
PROOFS OF THEOREMS,0.7813504823151125,Part 2:
PROOFS OF THEOREMS,0.7845659163987139,To show that bf satisfies Œ¶ outside of A.
PROOFS OF THEOREMS,0.7877813504823151,Published as a conference paper at ICLR 2022
PROOFS OF THEOREMS,0.7909967845659164,"For any x outside the patch area ‚à™1‚â§l‚â§IAl, we have x lies on a correct linear region (linear region
that satisfies the specification Œ¶). By Theorem 3, we have either bf(x) = f(x) or bf(x) ‚ààŒ¶out.
Therefore, bf satisfies Œ¶ outside of A."
PROOFS OF THEOREMS,0.7942122186495176,"Theorem 4 (Minimum Repair). For any ReLU DNN Àúf, which is linear on a patch region A and
satisfies the specification Œ¶, there exists a positive number Œì, such that for all Œ≥ ‚â•Œì,"
PROOFS OF THEOREMS,0.797427652733119,"max
x‚ààX | Àúf(x) ‚àíf(x)| ‚â•max
x‚ààX |hA(x, Œ≥)|.
(14)"
PROOFS OF THEOREMS,0.8006430868167203,Proof. We consider the general case where the linear patch function is obtained from Equation (3).
PROOFS OF THEOREMS,0.8038585209003215,"For any DNN Àúf, which is linear on patch region A and satisfies the specification Œ¶, we have
maxx‚ààA | Àúf ‚àíf| ‚â•maxx‚ààA |cx + d| = maxx‚ààA |hA(., Œ≥)| on patch area A by Equation (3)."
PROOFS OF THEOREMS,0.8070739549839229,"Therefore, we only need to show:"
PROOFS OF THEOREMS,0.8102893890675241,"max
x/‚ààA |hA(., Œ≥)| ‚â§max
x‚ààA |hA(., Œ≥)|
(15)"
PROOFS OF THEOREMS,0.8135048231511254,"Since parameter Œ≥ controls the slope of hA(., Œ≥) outside of patch area A, a large Œ≥ means that
hA(., Œ≥) will drop to zero quickly outside of A. Therefore, we can choose a large enough Œì such
that hA(., Œ≥) drops to zero faster than the change of linear patch function cx + d."
PROOFS OF THEOREMS,0.8167202572347267,"Therefore, we have that for any Œ≥ ‚â•Œì,"
PROOFS OF THEOREMS,0.819935691318328,"max
x/‚ààA |hA(., Œ≥)| ‚â§max
x‚ààA |hA(., Œ≥)| = max
x‚ààX |hA(., Œ≥)|"
PROOFS OF THEOREMS,0.8231511254019293,"‚â§max
x‚ààA | Àúf ‚àíf| ‚â§max
x‚ààX | Àúf ‚àíf|
(16)"
PROOFS OF THEOREMS,0.8263665594855305,"Theorem 5 (Polynomial-Time Efficiency). REASSURE terminates in polynomial-time in the size of
the neural network and the number of buggy linear regions when Œ¶out takes the form of {y | ql ‚â§
Py ‚â§qu}, where P is a full row rank matrix and ‚àí‚àû‚â§ql[i] ‚â§qu[i] ‚â§+‚àû(ql[i] and qu[i] are
the i-th elements of ql and qu respectively)."
PROOFS OF THEOREMS,0.8295819935691319,"Proof. We consider the affine patch function solved via Equation (10). Suppose A = {x ‚ààX|aix ‚â§
bi, i ‚ààI}. For the LPs in Equation (11), |I| is the number of constraints and it is polynomial in the
size of the neural network. Thus, REASSURE runs in polynomial time in the size of the neural
network."
PROOFS OF THEOREMS,0.8327974276527331,"In addition, since REASSURE computes the support network gA and affine patch function pA for
each A one by one (see Algorithm 1), the time complexity of REASSURE is linear in the number of
buggy linear regions."
ADDITIONAL EXPERIMENT DETAILS,0.8360128617363344,"7.4
ADDITIONAL EXPERIMENT DETAILS"
ADDITIONAL EXPERIMENT DETAILS,0.8392282958199357,Details on Feature-Space Repairs:
ADDITIONAL EXPERIMENT DETAILS,0.842443729903537,"For an R-layer DNN f, we split f into two submodels f1 and f2 according to a hidden layer, say
the jth hidden layer, where f1 is the first j layers function, f2 is the last R ‚àíj layers function and
f = f2 ‚ó¶f1. We note the output space of f1 the feature space. And for any buggy input ex, we note
f1(ex) the buggy feature."
ADDITIONAL EXPERIMENT DETAILS,0.8456591639871383,"Repairing in a feature space is to repair the behavior of f2 on buggy features {f1(ex1), . . . , f1(exL)}.
Note this will automatically repair the behavior of f on buggy points {ex1, . . . , exL}."
ADDITIONAL EXPERIMENT DETAILS,0.8488745980707395,"Repairing in a feature space has the benefit of making the repair process more computation-friendly
and reducing the parameter overhead of the additional networks, and has the potential to generalize
the repair to undetected buggy inputs with similar features. However, it loses the locality guarantee
in the input space (but still preserves locality in the feature space)."
ADDITIONAL EXPERIMENT DETAILS,0.8520900321543409,Published as a conference paper at ICLR 2022
ADDITIONAL EXPERIMENT DETAILS,0.8553054662379421,"REASSURE
MDNN
#P
ND(L‚àû)
ND(L2)
NDP(L‚àû)
NDP(L2)
Acc
ND(L‚àû)
ND(L2)
NDP(L‚àû)
NDP(L2)
Acc
1
0.0%
0.0%
9.0%
9.1%
96.8%
8.9%
8.9%
7.1%
6.2%
87.5%
5
0.0%
0.0%
12.7%
12.8%
96.8%
48.1%
48.2%
44.3%
39.9%
57.1%
25
0.0%
0.0%
29.9%
29.7%
96.8%
90.4%
90.6%
63.7%
57.8%
6.7%
50
0.0%
0.0%
42.9%
42.6%
96.8%
92.5%
92.8%
82.1%
72.6%
4.8%
100
0.0%
0.0%
46.2%
45.9%
96.8%
95.5%
95.7%
90.9%
76.8%
5.1%"
ADDITIONAL EXPERIMENT DETAILS,0.8585209003215434,"Table 5: Watermark Removal. The test accuracy of the original DNN is 96.8%. #P: number of buggy
points to repair; ND(L‚àû), ND(L2): average (L‚àû, L2) norm difference on both training data and
testing data; NDP(L‚àû), NDP(L2): average (L‚àû, L2) norm difference on random sampled points
near watermark images; Acc: accuracy on test data."
ADDITIONAL EXPERIMENT DETAILS,0.8617363344051447,Point-wise Repair on ImageNet (Feature-Space Repairs)
ADDITIONAL EXPERIMENT DETAILS,0.864951768488746,"We use AlexNet Krizhevsky et al. (2012) on ImageNet dataset Russakovsky et al. (2015) as the
target DNN. The size of image is (224, 224, 3) and the total number of classes for ImageNet is 1000.
We slightly modified AlexNet: we only consider 10 output classes that our buggy images may lie
on and use a multilayer perceptron with three hidden layers (512, 256, 256 nodes respectively) to
mimic the last two layers of AlexNet."
ADDITIONAL EXPERIMENT DETAILS,0.8681672025723473,"The goal of the repair is to fix the behaviors of the target DNN on buggy inputs, which are found
on ImageNet-A Hendrycks et al. (2021). For REASSURE, we construct the patch network starting
from the third from the last hidden layer (i.e. repair in a feature space)."
ADDITIONAL EXPERIMENT DETAILS,0.8713826366559485,"The results are shown in Table 4. REASSURE, PRDNN and Fine-Tuning repair all the buggy points
while Retrain only repair 96% buggy points in 100 epochs. REASSURE achieves almost zero modifi-
cation on validation images to the original DNN. In addition, REASSURE preserves the performance
of the original DNN."
ADDITIONAL EXPERIMENT DETAILS,0.8745980707395499,Watermark Removal
ADDITIONAL EXPERIMENT DETAILS,0.8778135048231511,"We compare REASSURE with MDNN on the watermark removal experiment from their paper. We
were not able to run the code provided in the MDNN Github repository, but we were able to run on
the target DNN models, watermark images, and MDNN-repaired models in the same repository."
ADDITIONAL EXPERIMENT DETAILS,0.8810289389067524,"The target DNN is from Goldberger et al. (2020), which is watermarked by the method proposed in
Adi et al. (2018) on a set of randomly chosen images xi with label f(xi)."
ADDITIONAL EXPERIMENT DETAILS,0.8842443729903537,"The goal is to change the DNN‚Äôs predictions on all watermarks xi to any other label y Ã∏= f(xi) while
preserving the DNN‚Äôs performance on the MNIST test data. For REASSURE, we set the prediction
y = f(xi) ‚àí1 if f(xi) > 1, and y = 10 otherwise."
ADDITIONAL EXPERIMENT DETAILS,0.887459807073955,"The results are shown in Table 5. Both REASSURE and MDNN remove all the watermarks. How-
ever, MDNN introduces significant distortion to the target DNN and as a result the test accuracy
drops rapidly as the number of repair points increases. In comparison, REASSURE removes all the
watermarks with no harm to test accuracy."
ADDITIONAL EXPERIMENT DETAILS,0.8906752411575563,Area Repair: HCAS
ADDITIONAL EXPERIMENT DETAILS,0.8938906752411575,"Table 6 is the comparison with PRDNN using the last layer as the repair layer. All other settings are
the same as those in Section 5.2."
ADDITIONAL EXPERIMENT DETAILS,0.8971061093247589,Experiment Platform
ADDITIONAL EXPERIMENT DETAILS,0.9003215434083601,"All experiments were run on an Intel Core i5 @ 3.4 GHz with 32 GB of memory. We use Gurobi
Gurobi Optimization, LLC (2021) to solve the linear programs."
ADDITIONAL EXPERIMENT DETAILS,0.9035369774919614,Size of Neural Networks:
ADDITIONAL EXPERIMENT DETAILS,0.9067524115755627,"Point-wise Repairs on MNIST: The DNN model is a multilayer perceptron with ReLU activation
functions. It has an input layer with 784 nodes, 2 hidden layers with 256 nodes in each layer, and a
final output layer with 10 nodes."
ADDITIONAL EXPERIMENT DETAILS,0.909967845659164,"Watermark Removal on MNIST: The DNN model has an input layer with 784 nodes, a single hidden
layer with 150 nodes, and a final output layer with 10 nodes."
ADDITIONAL EXPERIMENT DETAILS,0.9131832797427653,Published as a conference paper at ICLR 2022
ADDITIONAL EXPERIMENT DETAILS,0.9163987138263665,"REASSURE
PRDNN (Last Layer)
#A
ND(L‚àû)
NDP(L‚àû)
NSE
Acc
T
ND(L‚àû)
NDP(L‚àû)
NSE
Acc
T
10
2.662e-03%
1.318e-03%
0%
98.1%
1.0422
0.30%
20.5%
16%
71.4%
2.90+0.100
20
2.918e-03%
2.2%
0%
98.1%
1.1856
0.31%
46.7%
66%
70.5%
5.81+0.169
50
8.289e-03%
17.6%
0%
98.1%
1.8174
0.31%
46.7%
66%
70.5%
14.54+0.353
87
0.04%
45.9%
0%
97.8%
2.4571
0.31%
46.7%
66%
70.5%
25.30+0.467"
ADDITIONAL EXPERIMENT DETAILS,0.9196141479099679,"Table 6: Area Repairs on HCAS. We use the the last hidden layer as the repair layer for PRDNN.
The test accuracy of the original DNN is 97.9%. #A: number of buggy linear regions to repair;
ND(L‚àû): average L‚àûnorm difference on training data ; NDP(L‚àû): average L‚àûnorm difference
on random sampled data on input constraints of Specification 1; NSE: % of correct linear regions that
is repaired to incorrect; Acc: accuracy on training data (no testing data available); T: running time
in seconds. For PRDNN, the first running time is for enumerating all the vertices of the polytopes
and the second is for solving the LP problem in PRDNN."
ADDITIONAL EXPERIMENT DETAILS,0.9228295819935691,"Area Repair on HCAS: The DNN model has an input layer with 3 nodes, 5 hidden layers with 25
nodes in each hidden layer, and a final output layer with 5 nodes. DNN outputs one of five possible
control advisories (‚Äôwrong left‚Äô, ‚Äôweak left‚Äô, ‚ÄôClear-of-Conflict‚Äô, ‚Äôweak right‚Äô and ‚Äôwrong right‚Äô)."
ADDITIONAL EXPERIMENT DETAILS,0.9260450160771704,"Point-wise Repairs on ImageNet: modified AlexNet Krizhevsky et al. (2012) has 650k neurons,
consists of five convolutional layers, some of which are followed by max-pooling layers, and five
fully-connected layers."
ADDITIONAL EXPERIMENT DETAILS,0.9292604501607717,Hyperparameters used in Repair:
ADDITIONAL EXPERIMENT DETAILS,0.932475884244373,"We set Œ≥ = 0.5 for Point-wise Repair on MNIST, Œ≥ = 0.02 for Watermark Removal, Œ≥ = 1 for Area
Repair: HCAS and Œ≥ = 0.0005 for Point-wise Repair on ImageNet."
ADDITIONAL EXPERIMENT DETAILS,0.9356913183279743,We set learning rate to 10‚àí3 for Retrain in the point-wise repair experiment.
ADDITIONAL EXPERIMENT DETAILS,0.9389067524115756,"We set learning rate to 10‚àí2 and momentum to 0.9 for Fine-Tuning in the point-wise repair experi-
ment."
ADDITIONAL EXPERIMENT DETAILS,0.9421221864951769,"PRDNN requires specifying a layer for weight modification. We use the first hidden layer as the
repair layer, which has the best performance in our experiment settings, unless otherwise specified."
ADDITIONAL EXPERIMENT DETAILS,0.9453376205787781,Specifications in HCAS:
ADDITIONAL EXPERIMENT DETAILS,0.9485530546623794,"Specification 1. If the intruder is near and approaching from the left, the network advises ‚Äústrong
right.‚Äù"
ADDITIONAL EXPERIMENT DETAILS,0.9517684887459807,"Input constraints: Œ¶in = {(x, y, œà)|10 ‚â§x ‚â§5000, 10 ‚â§y ‚â§5000, ‚àíœÄ ‚â§œà ‚â§‚àí1/2œÄ}. Output
constraint: f(x, y, œà)4 ‚â•f(x, y, œà)i for i = 0, 1, 2, 3."
ADDITIONAL EXPERIMENT DETAILS,0.954983922829582,"We calculate all the linear regions for N1,4 in the area Œ¶in of Specification 1 and totally 165 linear
regions are found, including 87 buggy linear regions (DNN did not meet the specification) and 78
correct linear regions (DNN meet the specification)."
ADDITIONAL EXPERIMENT DETAILS,0.9581993569131833,"Specification 2. If the intruder is near and approaching from the right, the network advises ‚Äústrong
left.‚Äù"
ADDITIONAL EXPERIMENT DETAILS,0.9614147909967846,"Input constraints: Œ¶in = {(x, y, œà)|10 ‚â§x ‚â§5000, ‚àí5000 ‚â§y ‚â§‚àí10, 1/2œÄ ‚â§œà ‚â§œÄ}. Output
constraint: f(x, y, œà)0 ‚â•f(x, y, œà)i for i = 1, 2, 3, 4."
ADDITIONAL EXPERIMENT DETAILS,0.9646302250803859,"Also we calculate all the linear regions in the area Œ¶in of Specification 2. And 79 correct linear
regions are found. We will test if a repair will make those correct linear regions incorrect."
ADDITIONAL EXPERIMENT DETAILS,0.9678456591639871,Point-wise Repairs vs. Area Repairs:
ADDITIONAL EXPERIMENT DETAILS,0.9710610932475884,"REASSURE automatically performs area repair on the point-wise repair experiments. This means
our area repair method scales well to high-dimensional polytopes (the input dimension of MNIST is
784) whereas PRDNN does not scale beyond 2D linear regions/polytopes."
ADDITIONAL EXPERIMENT DETAILS,0.9742765273311897,Parameter Overhead for REASSURE:
ADDITIONAL EXPERIMENT DETAILS,0.977491961414791,"REASSURE introduces an additional network, patch network, and as a result adds new parameters to
the original network. The number of new parameters depends on |I|, which is the number of linear"
ADDITIONAL EXPERIMENT DETAILS,0.9807073954983923,Published as a conference paper at ICLR 2022
ADDITIONAL EXPERIMENT DETAILS,0.9839228295819936,"constraints for the H-representation of A. We can remove redundant constraints in this representa-
tion in polynomial time to make the additional network smaller. For the area repair experiment on
HCAS, the average number of constraints for one linear region is 3.28 and the average number of
new parameters that REASSURE introduces is 66. As a comparison, the number of parameters in
the original network is around 3000 and PRDNN doubles the number of parameters (as a result of
the Decoupled DNN construction) regardless of the number of point-wise or area repairs."
APPLYING REASSURE TO GENERAL CPWL NETWORKS,0.9871382636655949,"7.5
APPLYING REASSURE TO GENERAL CPWL NETWORKS"
APPLYING REASSURE TO GENERAL CPWL NETWORKS,0.9903536977491961,"Recall the result that an Rm ‚ÜíR function is representable by a ReLU DNN if and only if it is
a continuous piecewise linear (CPWL) function Arora et al. (2016). We use convolutional neural
networks as an example to show how REASSURE can be applied to more general CPWL networks.
Convolutional neural networks (CNNs) are neural networks with convolution layers and maxpooling
layers. For simplicity, we assume the CNNs also use ReLU activation functions (but in general other
CPWL activation functions will also work). The convolutional layers can be viewed as special linear
layers. The maxpooling layers can be converted to linear operations with ReLU activation functions
as follows."
APPLYING REASSURE TO GENERAL CPWL NETWORKS,0.9935691318327974,"max(x1, x2, ..., xn) = max(x1, max(x2, x3, ..., xn))
max(xi, xj) = max(xi ‚àíxj, 0) + xj = œÉ(xi ‚àíxj) + xj"
APPLYING REASSURE TO GENERAL CPWL NETWORKS,0.9967845659163987,"where œÉ is the ReLU activation function. Thus, REASSURE can be used to repair CNNs as well."
