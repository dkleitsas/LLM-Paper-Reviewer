Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00196078431372549,"Large-scale pretrained transformers have created milestones in text (GPT-3) and
1"
ABSTRACT,0.00392156862745098,"text-to-image (DALL-E and CogView) generation. Its application to video gen-
2"
ABSTRACT,0.0058823529411764705,"eration is still facing many challenges: The potential huge computation makes
3"
ABSTRACT,0.00784313725490196,"it unafforable for a full training; The scarcity and weak relevance of text-video
4"
ABSTRACT,0.00980392156862745,"datasets hinder the model to understand complex movement semantics. In this
5"
ABSTRACT,0.011764705882352941,"work, we present 9B-parameter transformer CogVideo, trained by inheriting a
6"
ABSTRACT,0.013725490196078431,"pretrained text-to-image model, CogView2. We also propose multi-frame-rate
7"
ABSTRACT,0.01568627450980392,"hierarchical training strategy to better align text and video clips. As (probably)
8"
ABSTRACT,0.01764705882352941,"the ﬁrst open-source large-scale pretrained text-to-video model, CogVideo outper-
9"
ABSTRACT,0.0196078431372549,"forms all publicly available models at a large margin in both machine and human
10"
ABSTRACT,0.021568627450980392,"evaluations.
11"
ABSTRACT,0.023529411764705882,"A lion is 
drinking water."
ABSTRACT,0.025490196078431372,Nightfall in a
ABSTRACT,0.027450980392156862,metropolis.
ABSTRACT,0.029411764705882353,"A couple are 
having dinner."
ABSTRACT,0.03137254901960784,"A woman is 
running on the"
ABSTRACT,0.03333333333333333,"beach in the 
late afternoon."
ABSTRACT,0.03529411764705882,A man is
ABSTRACT,0.03725490196078431,skiing.
ABSTRACT,0.0392156862745098,"A girl is 
dancing."
ABSTRACT,0.041176470588235294,Anime
ABSTRACT,0.043137254901960784,"Figure 1: Samples generated by CogVideo. The actual text inputs are in Chinese. Each sample is a
4-second clip of 32 frames, and here we sample 9 frames uniformly for display purpose."
INTRODUCTION,0.045098039215686274,"1
Introduction
12"
INTRODUCTION,0.047058823529411764,"Autoregressive transformers, e.g. DALL-E [19] and CogView [5], have revolutionized text-to-image
13"
INTRODUCTION,0.049019607843137254,"generation recently. It is natural to investigate the potential of autoregressive transformers on text-
14"
INTRODUCTION,0.050980392156862744,"to-video generation. Previous works followed this basic framework [36, 9], e.g. VideoGPT [37],
15"
INTRODUCTION,0.052941176470588235,"verifying its superiority over GAN-based methods [4, 27], but are still far from satisfaction.
16"
INTRODUCTION,0.054901960784313725,"One common challenge is that the generated video frames tend to gradually deviate from the text
17"
INTRODUCTION,0.056862745098039215,"prompt, making the generated characters hard to perform the desired actions. Vanilla autoregressive
18"
INTRODUCTION,0.058823529411764705,"models might be good at synthesizing videos with regular (e.g. straightly moving cars) or random
19"
INTRODUCTION,0.060784313725490195,"patterns (e.g. speaking by randomly moving lips), but fail on text prompt such as “a lion is drinking
20"
INTRODUCTION,0.06274509803921569,"water”. The main difference between the two cases is that, in the former case the ﬁrst frame already
21"
INTRODUCTION,0.06470588235294118,"provides sufﬁcient information for the subsequent changes, while in the latter the model has to
22"
INTRODUCTION,0.06666666666666667,"precisely understand the action “drink” in order to correctly generate the desired action — the lion
23"
INTRODUCTION,0.06862745098039216,"lifts the glass to its lip, drinks and then puts down the glass.
24"
INTRODUCTION,0.07058823529411765,"Why do the autoregressive transformers well understand the text-image relations, but struggle to
25"
INTRODUCTION,0.07254901960784314,"understand the text-action relations in videos? We hypothesize that the datasets and the way to utilize
26"
INTRODUCTION,0.07450980392156863,"them are the main reasons.
27"
INTRODUCTION,0.07647058823529412,"First, it is possible to collect billions of high-quality text-image pairs from Internet [19], but the
28"
INTRODUCTION,0.0784313725490196,"text-video data are more scarce. The largest annotated text-video dataset, VATEX [32], has only
29"
INTRODUCTION,0.0803921568627451,"41,250 videos. The retrieval-based text-video pairs, e.g. Howto100M [17], are weakly relevant and
30"
INTRODUCTION,0.08235294117647059,"most of them only describe the scene without the temporal information.
31"
INTRODUCTION,0.08431372549019608,"Second, the duration of videos varies a lot. Previous models split the video into many clips with a
32"
INTRODUCTION,0.08627450980392157,"ﬁxed number of frames for training, which destroys the alignment between the text and its temporal
33"
INTRODUCTION,0.08823529411764706,"counterparts in the video. If a “drinking” video is split into four individual clips of “holding a glass”,
34"
INTRODUCTION,0.09019607843137255,"“lifting”, “drinking” and “putting down” with the same text “drinking”, the model will be confused to
35"
INTRODUCTION,0.09215686274509804,"learn the accurate meaning of drinking.
36"
INTRODUCTION,0.09411764705882353,"Present Work. Here we present a large-scale pretrained text-to-video generative model, CogVideo,
37"
INTRODUCTION,0.09607843137254903,"which is of 9.4 billion parameters and trained on 5.4 million text-video pairs. We build CogVideo
38"
INTRODUCTION,0.09803921568627451,"based on a pretrained text-to-image model, CogView2 [6], in order to inherit the knowledge learned
39"
INTRODUCTION,0.1,"from the text-image pretraining. To ensure the alignment between text and its temporal counterparts
40"
INTRODUCTION,0.10196078431372549,"in the video, we propose the multi-frame-rate hierarchical training. The ﬂexibility of the textual
41"
INTRODUCTION,0.10392156862745099,"condition makes it possible to simply prepend a piece of text describing the frame rate to the original
42"
INTRODUCTION,0.10588235294117647,"text prompt for modeling different frame rates. To keep the text-video alignment, we choose a proper
43"
INTRODUCTION,0.10784313725490197,"frame rate description to include the complete action in each training sample. The frame rate token
44"
INTRODUCTION,0.10980392156862745,"also controls the intensity of the changes throughout continuous frames in generation. Speciﬁcally,
45"
INTRODUCTION,0.11176470588235295,"we train a sequential generation model and a frame interpolation model. The former model generates
46"
INTRODUCTION,0.11372549019607843,"key frames according to the text, and the latter recursively ﬁll the middle frames by varying the frame
47"
INTRODUCTION,0.11568627450980393,"rates to make the video coherent. As shown in Figure 1, CogVideo can generate high-resolution
48"
INTRODUCTION,0.11764705882352941,"(480×480) videos. Human evaluation demonstrates that CogVideo outperforms all publicly available
49"
INTRODUCTION,0.11960784313725491,"models at a large margin. Our main contributions can be concluded as follows:
50"
INTRODUCTION,0.12156862745098039,"• We present CogVideo, which is the largest and the ﬁrst open-source pretrained transformer
51"
INTRODUCTION,0.12352941176470589,"for text-to-video generation in the general domain.
52"
INTRODUCTION,0.12549019607843137,"• CogVideo elegantly and efﬁciently ﬁnetunes a text-to-video generative model from a pre-
53"
INTRODUCTION,0.12745098039215685,"trained text-to-image generative model, avoiding the expensive full pretraining from scratch.
54"
INTRODUCTION,0.12941176470588237,"• We propose the multi-frame-rate hierarchical training to better align text-clip pairs, which
55"
INTRODUCTION,0.13137254901960785,"signiﬁcantly improves the generation accuracy, in particular for movements of complex
56"
INTRODUCTION,0.13333333333333333,"semantics. This training strategy endows CogVideo with the capacity of controlling the
57"
INTRODUCTION,0.13529411764705881,"intensity of changes during the generation.
58"
RELATED WORK,0.13725490196078433,"2
Related Work
59"
VIDEO GENERATION,0.1392156862745098,"2.1
Video Generation
60"
VIDEO GENERATION,0.1411764705882353,"Video generation is a long-standing research topic. Most previous works focus on the next-frame
61"
VIDEO GENERATION,0.14313725490196078,"prediction task — forecasting the future frames based on the ﬁrst video frame. Early works, e.g.
62"
VIDEO GENERATION,0.1450980392156863,"CDNA [8] and PredRNN [33], leverage deterministic methods to directly predict the next frame
63"
VIDEO GENERATION,0.14705882352941177,"via CNNs or RNNs. However, these deterministic models are unable to capture the stochastic
64"
VIDEO GENERATION,0.14901960784313725,"temporal patterns and synthesize coherent complex scenes. Generative models, especially Generative
65"
VIDEO GENERATION,0.15098039215686274,"Adversarial Networks [10] (GANs), begin to dominate the area as they can perform unconditional or
66"
VIDEO GENERATION,0.15294117647058825,"class-conditional video synthesis without the ﬁrst frames. VGAN [31] is the ﬁrst one to use GAN
67"
VIDEO GENERATION,0.15490196078431373,"for video generation. It decomposes video to a static background and a moving foreground, and
68"
VIDEO GENERATION,0.1568627450980392,"then generates them with 2D and 3D convolutional networks respectively. TGAN[20] proposes
69"
VIDEO GENERATION,0.1588235294117647,"to separately generate the temporal latent variables and spatial information, and MoCoGAN [27]
70"
VIDEO GENERATION,0.1607843137254902,"similarly decomposes the latent space into context and motion subspaces. DIGAN [38] applies
71"
VIDEO GENERATION,0.1627450980392157,"implicit neural representations for video encoding. Recently, text-to-video generation emerges as a
72"
VIDEO GENERATION,0.16470588235294117,"promising direction. The framework of VQVAE [29] and autoregressive transformers [30, 1] quickly
73"
VIDEO GENERATION,0.16666666666666666,"becomes the mainstream method [35, 36, 9]. Ho et al. [11] proposes video diffusion model along with
74"
VIDEO GENERATION,0.16862745098039217,"a gradient method recently for text-to-video generation. The previous methods are basically trained
75"
VIDEO GENERATION,0.17058823529411765,"on a speciﬁc dataset, e.g. UCF-101 [23], making the trained model domain-speciﬁc. Moreover, most
76"
VIDEO GENERATION,0.17254901960784313,"of these models are not publicly available.
77"
AUTOREGRESSIVE TRANSFORMER,0.17450980392156862,"2.2
Autoregressive Transformer
78"
AUTOREGRESSIVE TRANSFORMER,0.17647058823529413,"Recent years have witnessed the autoregressive transformer emerging as a powerful generative model.
79"
AUTOREGRESSIVE TRANSFORMER,0.1784313725490196,"The autoregressive models become the most prevalent framework for text generation [24]. With
80"
AUTOREGRESSIVE TRANSFORMER,0.1803921568627451,"its prominent capacity of ﬁtting, transformer [30] gradually becomes the standard neural structure
81"
AUTOREGRESSIVE TRANSFORMER,0.18235294117647058,"for text generation. One milestone is GPT-3 [1]. In computer vision, van den Oord et al. [29]
82"
AUTOREGRESSIVE TRANSFORMER,0.1843137254901961,"ﬁrst proposes to train a VQVAE to compress the image into a sequence of tokens from a learned
83"
AUTOREGRESSIVE TRANSFORMER,0.18627450980392157,"dictionary, which can be efﬁciently handled by autoregressive models. VQ-GAN [7] learns a more
84"
AUTOREGRESSIVE TRANSFORMER,0.18823529411764706,"semantic-aware dictionary for unconditional image generation. In the text-to-image generation, pre-
85"
AUTOREGRESSIVE TRANSFORMER,0.19019607843137254,"trained autoregressive transformers such as DALL-E [19] and CogView [5] have shown superiority
86"
AUTOREGRESSIVE TRANSFORMER,0.19215686274509805,"in open-domain image generation. Besides the pure GPT-style generation, CogView2 [6] proposes a
87"
AUTOREGRESSIVE TRANSFORMER,0.19411764705882353,"new language model CogLM for inﬁlling in the image generation.
88"
AUTOREGRESSIVE TRANSFORMER,0.19607843137254902,"Recent autoregressive transformers [18, 37, 35, 36] have also shown their superiority in video
89"
AUTOREGRESSIVE TRANSFORMER,0.1980392156862745,"generation. Among them, GODIVA [35] and NÜWA [36] focus on the open-domain text-to-video
90"
AUTOREGRESSIVE TRANSFORMER,0.2,"generation. However, they simply generate frames or frame blocks one by one in a chronological
91"
AUTOREGRESSIVE TRANSFORMER,0.2019607843137255,"order, and may suffer from poor text-video alignment (Cf. § 1).
92"
METHOD,0.20392156862745098,"3
Method
93"
METHOD,0.20588235294117646,"In this section, we ﬁrst introduce multi-frame-rate hierarchical training to better align text and
94"
METHOD,0.20784313725490197,"video semantics in § 3.1, and then illustrate an efﬁcient method dual-channel attention to inherit
95"
METHOD,0.20980392156862746,"the knowledge in pretrained text-image models for video generation in § 3.2. To overcome the
96"
METHOD,0.21176470588235294,"large memory and time overhead caused by the large model and long sequence, we refer to Swin
97"
METHOD,0.21372549019607842,"Attention [14] and extend it to autoregressive video generation in § 3.3.
98"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.21568627450980393,"3.1
Multi-frame-rate Hierarchical Training
99"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.21764705882352942,"Here we present the multi-frame-rate hierarchical training and generation. We follow the framework
100"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2196078431372549,"of VQVAE [29] and ﬁrst tokenize each frame into image tokens. Each training sample consists
101"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.22156862745098038,"of 5 frame of tokens, but our training method differs in the construction of training sequences and
102"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2235294117647059,"generation process.
103"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.22549019607843138,"Training. The key design is to add a frame-rate token to the text and sample frames at this frame-rate
104"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.22745098039215686,"to compose a ﬁxed-length training sequence. The motivations are two folds:
105"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.22941176470588234,"(1) Directly separating the long video into clips at a ﬁxed frame-rate often leads to semantic mis-
106"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.23137254901960785,"matching. We still use the full text but the truncated clip might only contain incomplete action.
107"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.23333333333333334,"(2) The adjacent frames are usually very similar. A giant change over the previous frame will
108"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.23529411764705882,"probably incur a large loss. This will lead the models less inclined to explore the long-range
109"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2372549019607843,"correlation because to simply copy the previous frame acts like a shortcut.
110"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.23921568627450981,"A lion is 
drinking water. 
Ӟݝሁৼྋࣁࡆ࿜"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2411764705882353,Input Text:
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.24313725490196078,"Flatten
20*20=400 image tokens per frame  *  5 frames
Text tokenization"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.24509803921568626,Transformer   (Stage 2: Recursive Interpolation)
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.24705882352941178,"Text
[B]
Frame-1
Frame-2
Frame-3
Frame-4
Frame-5
Frame Rate"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.24901960784313726,Transformer   (Stage 1: Sequential Generation)
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.25098039215686274,"Text
[B]
Frame-1
Frame-2
Frame-3
Frame-4
Frame-5
Frame Rate
z
}|
{ VQVAE"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2529411764705882,Discretize
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2549019607843137,Image Tokenizer C
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2568627450980392,"z
}|
{"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.25882352941176473,"Sequence 1
Sequence 2"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2607843137254902,"z
}|
{"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2627450980392157,Interpolate
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2647058823529412,frames
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.26666666666666666,Input Frames:
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.26862745098039215,"Figure 2: Multi-frame-rate hierarchical generation framework in CogVideo. Input sequence includes
frame rate, text, frame tokens. [B] (Begin-of-image) is a separator token, inherited from CogView2.
In stage 1, Ts frames are generated sequentially on condition of frame rate and text. Then in stage
2, generated frames are re-input as bidirectional attention regions to recursively interpolate frames.
Frame rate can be adjusted during both stages. Bidirectional attention regions are highlighted in"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.27058823529411763,"blue , and unidirectional regions are highlighted in green ."
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2725490196078431,"Therefore, in each training sample we want the text and the frames match as possible. We predeﬁned
111"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.27450980392156865,"a series of frame-rates, and select the lowest frame-rate for each text-video pair, as long as we can
112"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.27647058823529413,"sample at least 5 frames at this frame-rate in the video.
113"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2784313725490196,"Although the above method increase the alignment of text and video, the generation at a low frame-
114"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2803921568627451,"rate could be incoherent. We train another frame interpolation model to insert transition frames to the
115"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2823529411764706,"generated samples of the sequential generation model. Thanks to the generality of CogLM [6], the
116"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.28431372549019607,"two models can share the same structure and training process only with different attention masks.
117"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.28627450980392155,"Generation The multi-frame-rate hierarchical generation is a recursive process, illustrated in Fig-
118"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.28823529411764703,"ure 2. Speciﬁcally, the generation pipeline consists of a sequential generation stage and a recursive
119"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2901960784313726,"interpolation stage:
120"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.29215686274509806,"(1) Sequentially generate Ts key frames based on a low frame rate and text. The input sequence
121"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.29411764705882354,"is [{Frame Rate}{Text} [B] {Frame1} ...
{Frame Ts}]. In practice, we always set
122"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.296078431372549,"Ts = 5 and the minimum sampling frame rate to 1 fps.
123"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.2980392156862745,"(2) Recursively interpolate frames based on the text, frame rate and known frames. In each round
124"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.3,"of interpolation, we split generated frames into multiple ⌈Ts"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.30196078431372547,"2 ⌉-frame blocks overlapping at the
125"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.30392156862745096,"beginning and the end, and interpolate a frame between the successive frames in each block.
126"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.3058823529411765,"The input sequence is [{Frame Rate}{Text} [B] {Frame1} ...
{Frame Ts}], where
127"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.307843137254902,"Frame 2i(i = 1, 2, ..., ⌊Ts"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.30980392156862746,"2 ⌋) are to be autoregressively generated. By recursively halﬁng {Frame
128"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.31176470588235294,"Rate}, we can conduct ﬁner and ﬁner interpolation to generate videos of many frames.
129"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.3137254901960784,"The effect of CogLM. Tasks such as frame interpolation rely heavily on bidirectional information.
130"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.3156862745098039,"However, most previous works use GPT [35, 37, 36], which is unidirectional. To be aware of the
131"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.3176470588235294,"bidirectional context, we adopt Cross-Modal General Language Model (CogLM) proposed in [6]
132"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.3196078431372549,"which unites bidirectional context-aware mask prediction and autoregressive generation by dividing
133"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.3215686274509804,"tokens into unidirectional and bidirectional attention regions. While bidirectional regions can attend
134"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.3235294117647059,"to all bidirectional regions, unidirectional regions can attend to all bidirectional regions and previous
135"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.3254901960784314,"unidirectional regions. As shown in 2, (1) all frames in stage 1 and the 2nd, 4th frames in stage
136"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.32745098039215687,"2 are in the unidirectional region; (2) {Frame Rate}, {Text} and all other frames belong to the
137"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.32941176470588235,"bidirectional region. In this way, bidirectional attention context is fully exploited in text and given
138"
MULTI-FRAME-RATE HIERARCHICAL TRAINING,0.33137254901960783,"frames without interfering auto-regressive frame prediction.
139"
DUAL-CHANNEL ATTENTION,0.3333333333333333,"3.2
Dual-channel Attention
140"
DUAL-CHANNEL ATTENTION,0.3352941176470588,Layer Norm
DUAL-CHANNEL ATTENTION,0.33725490196078434,Attention-base
DUAL-CHANNEL ATTENTION,0.3392156862745098,(Spatial Channel)
DUAL-CHANNEL ATTENTION,0.3411764705882353,"Attention-plus
(Temporal Channel)"
DUAL-CHANNEL ATTENTION,0.3431372549019608,Addition
DUAL-CHANNEL ATTENTION,0.34509803921568627,Layer Norm FFN
DUAL-CHANNEL ATTENTION,0.34705882352941175,Addition
DUAL-CHANNEL ATTENTION,0.34901960784313724,Dual-channel Attention
DUAL-CHANNEL ATTENTION,0.3509803921568627,"Figure 3: Dual-channel atten-
tion.
We initialize Attention-
plus the same as Attention-base
so that the model behaves ex-
actly the same as CogView2
when it is initialized."
DUAL-CHANNEL ATTENTION,0.35294117647058826,"Large-scale pretraining usually demands a large dataset. For open-
141"
DUAL-CHANNEL ATTENTION,0.35490196078431374,"domain text-to-video generation, ideally we need the dataset to
142"
DUAL-CHANNEL ATTENTION,0.3568627450980392,"cover sufﬁcient text-video pairs to infer both spatial and tem-
143"
DUAL-CHANNEL ATTENTION,0.3588235294117647,"poral correlation between video and text. However, to collect
144"
DUAL-CHANNEL ATTENTION,0.3607843137254902,"high quality text-video pairs is often difﬁcult, expensive and time-
145"
DUAL-CHANNEL ATTENTION,0.3627450980392157,"consuming.
146"
DUAL-CHANNEL ATTENTION,0.36470588235294116,"A natural idea is to make use of the image data to facilitate the
147"
DUAL-CHANNEL ATTENTION,0.36666666666666664,"learning of spatial semantics. Video Diffusion Model [11] and
148"
DUAL-CHANNEL ATTENTION,0.3686274509803922,"NÜWA [36] try to add text-image pairs into text-video training,
149"
DUAL-CHANNEL ATTENTION,0.37058823529411766,"which achieves better results on multiple metrics. However, as
150"
DUAL-CHANNEL ATTENTION,0.37254901960784315,"for training a video-only generation model, adding image data
151"
DUAL-CHANNEL ATTENTION,0.37450980392156863,"will signiﬁcantly increase training cost, especially in large-scale
152"
DUAL-CHANNEL ATTENTION,0.3764705882352941,"pretraining scenarios.
153"
DUAL-CHANNEL ATTENTION,0.3784313725490196,"In this paper, we propose to leverage pretrained image generation
154"
DUAL-CHANNEL ATTENTION,0.3803921568627451,"models instead of image data. Pretrained text-to-image models,
155"
DUAL-CHANNEL ATTENTION,0.38235294117647056,"e.g. CogView2 [6], already have a good command of the text-
156"
DUAL-CHANNEL ATTENTION,0.3843137254901961,"image relations. The coverage of the dataset to train these model
157"
DUAL-CHANNEL ATTENTION,0.3862745098039216,"is also larger than that of videos.
158"
DUAL-CHANNEL ATTENTION,0.38823529411764707,"The proposed technique is dual-channel attention, where we only
159"
DUAL-CHANNEL ATTENTION,0.39019607843137255,"add a new spatial-temporal attention channel to the pretrained CogView2 [6] at each transformer
160"
DUAL-CHANNEL ATTENTION,0.39215686274509803,"layer. All the parameters in the CogView2 are frozen in the training, and only the parameters in the
161"
DUAL-CHANNEL ATTENTION,0.3941176470588235,"newly added attention layer(See the Attention-plus in Figure 3) are trainable.
162"
DUAL-CHANNEL ATTENTION,0.396078431372549,"Here we also emphasize that directly ﬁnetuning CogView2 for text-to-video generation cannot well
163"
DUAL-CHANNEL ATTENTION,0.3980392156862745,"inherit the knowledge, because the temporal attention follows a different attention pattern and quickly
164"
DUAL-CHANNEL ATTENTION,0.4,"ruins the pretrained weights during the initial phase of training with large gradients.
165"
DUAL-CHANNEL ATTENTION,0.4019607843137255,"Speciﬁcally, a Transformer layer with dual-channel attention can be computed as
166"
DUAL-CHANNEL ATTENTION,0.403921568627451,"ˆxl = LayerNorm(xl),
(1)
exl = α · Attention-base( ˆxl) + (1 −α), ·Attention-plus( ˆxl),
(2)
xl+1 = FFN(LayerNorm(xl + exl)),
(3)"
DUAL-CHANNEL ATTENTION,0.40588235294117647,"where xl denotes input features of layer l; Attention-base and Attention-plus denote two attention
167"
DUAL-CHANNEL ATTENTION,0.40784313725490196,"channels; FFN and LayerNorm represent Feed-Forward Networks and LayerNorm respectively; α
168"
DUAL-CHANNEL ATTENTION,0.40980392156862744,"is a vector with length of hidden-size and normalized to (0, 1). The whole structure is the same as
169"
DUAL-CHANNEL ATTENTION,0.4117647058823529,"CogView2 when ignoring Attention-plus.
170"
DUAL-CHANNEL ATTENTION,0.4137254901960784,"Both channels are computed as normal multi-head attention with a certain receptive ﬁeld formulated
171"
DUAL-CHANNEL ATTENTION,0.41568627450980394,"as follows. For token at (t, x, y) in frame block of size (Ts, X, Y ) (where (t, x, y) corresponds to
172"
DUAL-CHANNEL ATTENTION,0.4176470588235294,"coordination along time, height and width dimension), receptive ﬁeld RF is a 3D block with extent
173"
DUAL-CHANNEL ATTENTION,0.4196078431372549,"lt, lx, ly ∈N+:
174"
DUAL-CHANNEL ATTENTION,0.4215686274509804,"RF(t,x,y) = {(k, i, j)
 |x −i| < lx, |y −j| < ly, |t −k| < lt, (k, i, j) /∈Mask(t,x,y)},
(4)"
DUAL-CHANNEL ATTENTION,0.4235294117647059,"where Mask(t,x,y) represents CogLM attention mask for token (t, x, y). For Attention-base, we
175"
DUAL-CHANNEL ATTENTION,0.42549019607843136,"restrict receptive ﬁeld to current frame, i.e, lx = X, ly = Y, lt = 1, to fully use CogView2’s spatial
176"
DUAL-CHANNEL ATTENTION,0.42745098039215684,"modeling ability (therefore referred to as spatial channel). For Attention-plus, which is the only
177"
DUAL-CHANNEL ATTENTION,0.4294117647058823,"new parameters in CogVideo, we set receptive ﬁeld to a 3D local block throughout the whole time
178"
DUAL-CHANNEL ATTENTION,0.43137254901960786,"dimension, i.e. lx = Ax, ly = Ay, lt = Ts (therefore referred to as temporal channel). Ax, Ay
179"
DUAL-CHANNEL ATTENTION,0.43333333333333335,"are hyper-parameters satisfying Ax ≤X, Ay ≤Y . With Ax and Ay, CogVideo is able to ﬂexibly
180"
DUAL-CHANNEL ATTENTION,0.43529411764705883,"trade off between quadratic attention cost and size of receptive ﬁeld. In practice, we use shifted
181"
DUAL-CHANNEL ATTENTION,0.4372549019607843,"window attention [15] as a approximation of 3D block attention and extend it to CogLM scenario, as
182"
DUAL-CHANNEL ATTENTION,0.4392156862745098,"illustrated in subsection 3.3.
183"
DUAL-CHANNEL ATTENTION,0.4411764705882353,"It is worth noting that two channels are fused and share the same FFN in each layer, because FFN
184"
DUAL-CHANNEL ATTENTION,0.44313725490196076,"is a module of heavy parameters containing much vision knowledge. Due to similarity between
185"
DUAL-CHANNEL ATTENTION,0.44509803921568625,"images and videos, bringing its knowledge to temporal channel will facilitate video modeling. Finally,
186"
DUAL-CHANNEL ATTENTION,0.4470588235294118,"sharing FFN can reduce parameters, thus speed up training and reduce memory overhead.
187"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.44901960784313727,"3.3
Shifted Window Attention in Auto-regressive Generation
188"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.45098039215686275,"To overcome large time and memory overhead in temporal channel during training and inference,
189"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.45294117647058824,"we refer to Swin Attention proposed in [14] and extend it to auto-regressive scenario by applying
190"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4549019607843137,"auto-regressive attention mask in shifted windows.
191"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4568627450980392,"t=i
t=i+1
t=i+2"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4588235294117647,"Figure 4: Receptive ﬁeld (in yellow
or green) for the token in red box.
Shifted window size is 2 × 2 in this
example."
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.46078431372549017,"Different from non-autoregressive scenario which original Swin
192"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4627450980392157,"Transformer explores, we propose that Swin Attention can fur-
193"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4647058823529412,"ther accelerate auto-regressive inference because of restricted
194"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4666666666666667,"receptive ﬁeld. As shown in Figure 4, receptive ﬁeld is re-
195"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.46862745098039216,"stricted by
196"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.47058823529411764,"• Auto-regressive mask. A token can only attend to pre-
197"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4725490196078431,"vious frames or tokens before itself in current frame.
198"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4745098039215686,"• Shifted window. Only tokens within distance of win-
199"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4764705882352941,"dow size in both width and height dimension can be
200"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.47843137254901963,"directly attended to.
201"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4803921568627451,"Suppose X,Y is the height and width of each frame, and Ax,Ay
202"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4823529411764706,"are the height and width of shifted window. For two tokens at (t1, x1, y1) and (t2, x2, y2), t1 < t2,
203"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.4843137254901961,"the latter cannot attend to the former either directly or indirectly if
204"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.48627450980392156,"(x1 −x2)Y + (y1 −y2) ≥(t2 −t1 + 1)(AxY + Ay)
(5)"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.48823529411764705,"is satisﬁed. That is to say, the i-th token in frame t1 can be generated with the (i −AxY + Ay)-th
205"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.49019607843137253,"token in frame t1 + 1 in parallel. In this way, we can generate ⌊
XY
AxY +Ay ⌋tokens in parallel at most,
206"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.492156862745098,"thus greatly enhance parallelism and accelerate inference compared to auto-regressive with standard
207"
SHIFTED WINDOW ATTENTION IN AUTO-REGRESSIVE GENERATION,0.49411764705882355,"attention which can only generate one token at a time.
208"
TRAINING,0.49607843137254903,"4
Training
209"
TRAINING,0.4980392156862745,"Based on methods above, the training details of CogVideo are listed as follows:
210"
TRAINING,0.5,"Model. The backbone of CogVideo in both stages is a Transformer with dual-channel attention.
211"
TRAINING,0.5019607843137255,"The Transformer has 48 layers, with the hidden size of 3072 in each attention channel, 48 attention
212"
TRAINING,0.503921568627451,"heads and 9.4 billion parameters in total. Among them, 6 billion parameters are ﬁxed to CogView2’s
213"
TRAINING,0.5058823529411764,"parameters, which includes Position-wise Feed-Forward Networks (FFN), spatial channel of dual-
214"
TRAINING,0.5078431372549019,"channel Attention, ﬁrst frame’s positional embeddings and all image and text vocabulary embeddings.
215"
TRAINING,0.5098039215686274,"The speciﬁc implementation of Transformer structure is almost identical to CogView [5] such as
216"
TRAINING,0.5117647058823529,"using Sandwich LayerNorm and PB-Relax to stablize training. Shifted CogLM attention window is
217"
TRAINING,0.5137254901960784,"adoppted in recursive interpolation model with window size 10 × 10.
218"
TRAINING,0.515686274509804,"Dataset. We pretrain our model on a dataset of 5.4 million captioned videos with a spatial resolution
219"
TRAINING,0.5176470588235295,"of 160x160. For sequential generation model (Stage-1), we adjust frame rate in each sample to
220"
TRAINING,0.5196078431372549,"accomodate the whole video, while the minimum frame rate is set to 1 fps. For recursive interpolation
221"
TRAINING,0.5215686274509804,"model(Stage-2), we split videos into clips of different length to accomodate prediction on multiple
222"
TRAINING,0.5235294117647059,"frame rates including 2,4,8 fps.
223"
TRAINING,0.5254901960784314,"Pretraining. The sequence lengths in both stages are 2065, consisting of 64 text tokens, 5 (frames)
224"
TRAINING,0.5274509803921569,"x 400 (per frame) image tokens, and 1 seperator token. Both text and images are tokenized with
225"
TRAINING,0.5294117647058824,"icetk1.The parameters are updated by Adam with max learning rate = 2×10−4, β1 = 0.9, β2 = 0.95,
226"
TRAINING,0.5313725490196078,"weight decay = 1 × 10−2. See Appendix for pretraining details.
227"
TRAINING,0.5333333333333333,1https://github.com/THUDM/icetk
TRAINING,0.5352941176470588,"Table 1: (Left) Video generation performance on UCF-101. Class labels are used as text inputs.
* denotes the model is trained on the training split of UCF-101 only. (Right) Video generation
performance on Kinetics-600. Metrics are measured on generated videos of 16 frames priming on
ﬁrst 5 frames, following settings in [18]. ** denotes groundtruth used in FVD testing is blurred with
our image tokenizer icetk."
TRAINING,0.5372549019607843,"Method
IS (↑)
FVD (↓)"
TRAINING,0.5392156862745098,"VideoGPT[37]
24.69
-
DVD-GAN[4]
27.38
-
TGANv2[21]*
28.87
1209
MoCoGAN-HD[25]
32.36
838
DIGAN[38]*
29.71
655
DIGAN[38]
32.70
577
TATS-base[9]
79.28
332"
TRAINING,0.5411764705882353,"CogVideo (Ours)
50.46
626
CogVideo (Ours)**
-
545"
TRAINING,0.5431372549019607,"Method
FVD"
TRAINING,0.5450980392156862,"Latent Video Tranformer[18]
224.73
Video Transformer[34]
170
DVD-GAN-FP[4]
69.15
TriVD-GAN-FP[16]
25.74"
TRAINING,0.5470588235294118,"CogVideo (Ours)
109.23
CogVideo (Ours)**
59.55"
EXPERIMENTS,0.5490196078431373,"5
Experiments
228"
MACHINE EVALUATION,0.5509803921568628,"5.1
Machine Evaluation
229"
MACHINE EVALUATION,0.5529411764705883,"Machine evaluation is conducted on two popular benchmarks for video generation, i.e., UCF101 [23]
230"
MACHINE EVALUATION,0.5549019607843138,"and Kinetics-600 [3]. Following Rakhimov et al. [18], Yu et al. [38], we use Fréchet Video Dis-
231"
MACHINE EVALUATION,0.5568627450980392,"tance(FVD) [28] and Inception score(IS) [22] as metrics in the evaluation. FVD is calculated based
232"
MACHINE EVALUATION,0.5588235294117647,"on I3D model[2] trained on Kinetics-400, and IS is based on C3D model [26] which was ﬁrst trained
233"
MACHINE EVALUATION,0.5607843137254902,"with Sports-1M dataset [12] and then ﬁne-tuned on the UCF101 dataset. Our evaluation code is the
234"
MACHINE EVALUATION,0.5627450980392157,"same as the ofﬁcial TGAN-v2 implementation2.
235"
MACHINE EVALUATION,0.5647058823529412,"UCF-101 is a human action dataset consisted of 13,320 videos annotated with 101 action classes.
236"
MACHINE EVALUATION,0.5666666666666667,"Due to the image style and frame rate gap between CogVideo’s training set and UCF-101, we use
237"
MACHINE EVALUATION,0.5686274509803921,"class labels as the input text and ﬁne-tune CogVideo on the whole dataset for 10,000 iterations with
238"
MACHINE EVALUATION,0.5705882352941176,"batch size = 192. During inference, we sample class labels according to the class distribution. FVD
239"
MACHINE EVALUATION,0.5725490196078431,"and IS are evaluated over 2048 and 10,000 samples respectively, following Yu et al. [38]. Results are
240"
MACHINE EVALUATION,0.5745098039215686,"shown in Table 1 (Left).
241"
MACHINE EVALUATION,0.5764705882352941,"Kinetics-600 dataset contains 600 classes of human action videos, with roughly 350k train and
242"
MACHINE EVALUATION,0.5784313725490197,"50k test videos in total. We use the action category as input text, and ﬁne-tune CogVideo on the
243"
MACHINE EVALUATION,0.5803921568627451,"training set for 12,000 iterations with batch size of 640. Following the setup of Weissenborn et al.
244"
MACHINE EVALUATION,0.5823529411764706,"[34], Rakhimov et al. [18], we center-crop and down-sample each frame to 64x64, and measure with
245"
MACHINE EVALUATION,0.5843137254901961,"FVD. Results are shown in Table 1 (Right).
246"
HUMAN EVALUATION,0.5862745098039216,"5.2
Human Evaluation
247"
HUMAN EVALUATION,0.5882352941176471,"To further evaluate CogVideo, we invite 90 anonymous evaluators to rate for CogVideo and other open-
248"
HUMAN EVALUATION,0.5901960784313726,"source baselines including GAN-based model TGANv2 [21] and GPT-based model VideoGPT [37].
249"
HUMAN EVALUATION,0.592156862745098,"30 classes in UCF101 are randomly picked as text conditions, and several aspects are rated (See
250"
HUMAN EVALUATION,0.5941176470588235,"Appendix for details). For VideoGPT, we use the ofﬁcial unconditonal pretrained model3 to generate
251"
HUMAN EVALUATION,0.596078431372549,"samples. For TGANv2, we use the ofﬁcial source code to train an unconditional generation model
252"
HUMAN EVALUATION,0.5980392156862745,"under the same setting as that in Saito et al. [21]. To assign unconditionally generated samples into
253"
HUMAN EVALUATION,0.6,"corresponding categories, we choose TSM [13] as the action recognition model and only samples
254"
HUMAN EVALUATION,0.6019607843137255,"with conﬁdence >80%. Results in Figure 5 show that CogVideo signiﬁcantly outperforms baselines
255"
HUMAN EVALUATION,0.6039215686274509,"on multiple important aspects including frame texture, motion realism and semantice relevance, and
256"
HUMAN EVALUATION,0.6058823529411764,"achieves the top score by overall quality. It can be seen that 49.53% evaluators choose CogVideo as
257"
HUMAN EVALUATION,0.6078431372549019,"the best method, and only 15.42% and 5.6% favor VideoGPT and TGANv2, respectively.
258"
HUMAN EVALUATION,0.6098039215686275,"2https://github.com/pfnet-research/tgan2
3https://github.com/wilson1yan/VideoGPT"
HUMAN EVALUATION,0.611764705882353,"(c) Scores (1-5) on three important aspects. 
(b) Overall scores (1-10) for each method.
(a) Human preference. The percentage"
HUMAN EVALUATION,0.6137254901960785,of being chosen as the best.
HUMAN EVALUATION,0.615686274509804,"Figure 5: Human evaluation results. ""CogVideo 1Stage"" refers to the method in ablation study, which
generates videos sequentially with CogVideo’s Stage-1 Model only by recursively reinserting last 2
generated frames into input and generate future frames."
HUMAN EVALUATION,0.6176470588235294,"Table 2: Ablation study on a 5,000-sample subset of Kinetcis-600’s testset. FVD is evaluated on
generated 11-frame samples priming on 5 frames and ground-truth blurred by our image tokenizer.
The setting column indicates the difference between each method and CogVideo. Models of each
setting are trained on Kinetics-600 trainset for 10,000 iterations with batch size of 320."
HUMAN EVALUATION,0.6196078431372549,"Method
Setting
FVD (↓)"
HUMAN EVALUATION,0.6215686274509804,"CogVideo
None
108.27"
HUMAN EVALUATION,0.6235294117647059,"1-stage Generation(Noverlap = 1)
−hierarchical
137.13
1-stage Generation(Noverlap = 2)
−hierarchical
120.82"
HUMAN EVALUATION,0.6254901960784314,"Initialzed to CogView2
−Pretrain
124.92
Randomly Initialzed
−Pretrain −CogView
166.13"
ABLATION STUDY,0.6274509803921569,"5.3
Ablation Study
259"
ABLATION STUDY,0.6294117647058823,"To verify the effectiveness of hierarchical multi-frame-rate generation and incorporating CogView2,
260"
ABLATION STUDY,0.6313725490196078,"we conduct ablation study quantitatively and qualitatively on Kinetics-600 and UCF-101 datasets.
261"
ABLATION STUDY,0.6333333333333333,"Hierarchical multi-frame-rate generation. In comparison with CogVideo, we ﬁne-tune a 1-stage
262"
ABLATION STUDY,0.6352941176470588,"video generation model on Kinetics-600 from the sequential generation model in CogVideo, which
263"
ABLATION STUDY,0.6372549019607843,"generates long videos by recursively reinserting last Noverlap frames into the input to sample next
264"
ABLATION STUDY,0.6392156862745098,"Ns −Noverlap frames. Larger Noverlap means more previous frames can be utilized during the
265"
ABLATION STUDY,0.6411764705882353,"inference, but will increase time overhead.
266"
ABLATION STUDY,0.6431372549019608,"Dual-channel attention with CogView2’s weights. We additionally train (1) A randomly initialized
267"
ABLATION STUDY,0.6450980392156863,"model; (2) A model incorporating CogView2’s weights but leaving temporal channel randomly
268"
ABLATION STUDY,0.6470588235294118,"initialized and unﬁxed (equivalent to CogVideo without pretraining on videos) on Kinetics-600.
269"
QUANTITATIVE EVALUATION,0.6490196078431373,"5.3.1
Quantitative Evaluation
270"
QUANTITATIVE EVALUATION,0.6509803921568628,Figure 6: Training loss in ablation study.
QUANTITATIVE EVALUATION,0.6529411764705882,"All aforementioned models have been trained for 11,000
271"
QUANTITATIVE EVALUATION,0.6549019607843137,"iterations with batch size of 160. Quantitative results are
272"
QUANTITATIVE EVALUATION,0.6568627450980392,"shown in Table 2. We can see that the hierarchical method
273"
QUANTITATIVE EVALUATION,0.6588235294117647,"is clearly superior to 1-stage generation with different Ns,
274"
QUANTITATIVE EVALUATION,0.6607843137254902,"and model initialized with CogView2’s weights has lower
275"
QUANTITATIVE EVALUATION,0.6627450980392157,"FVD than randomly initialized one.
276"
QUANTITATIVE EVALUATION,0.6647058823529411,"Figure 6 plots the training loss curve of (1) ﬁnetuning
277"
QUANTITATIVE EVALUATION,0.6666666666666666,"CogVideo; (2) training model from random initialization;
278"
QUANTITATIVE EVALUATION,0.6686274509803921,"(3) training model initialized to CogView2 and partially
279"
QUANTITATIVE EVALUATION,0.6705882352941176,"ﬁxed. We can see that CogView2 endows model with a
280"
QUANTITATIVE EVALUATION,0.6725490196078432,(c) Randomly Initialized
QUANTITATIVE EVALUATION,0.6745098039215687,"(d) Finetuned CogVideo, 1-Stage"
QUANTITATIVE EVALUATION,0.6764705882352942,"(e) Finetuned CogVideo, 1-Stage
(b) Initialized with CogView2"
QUANTITATIVE EVALUATION,0.6784313725490196,"(a) Finetuned CogVideo, hierarchical generation"
QUANTITATIVE EVALUATION,0.6803921568627451,Input Text: Lunge
QUANTITATIVE EVALUATION,0.6823529411764706,Given frames:
QUANTITATIVE EVALUATION,0.6843137254901961,"Figure 7: Video samples in ablation study, which are generated priming on class label and ﬁrst 5
frames in Kinetics-600. All samples are down sampled by extracting one in every three frames for
display purpose. (a) Use ﬁne-tuned CogVideo to hierarchically generate samples. (b) Train a model
on Kinetics-600 which is initialized as and partially ﬁxed to CogView2, and hierarchically generate
samples. (c) Train a model on Kinetics-600 which is randomly initialized, and hierarchically generate
samples. (d)(e) Use ﬁne-tuned CogVideo to generate frames in 1 stage with different Noverlap."
QUANTITATIVE EVALUATION,0.6862745098039216,"good initialization point from which the loss function can converge faster to a lower value. Also,
281"
QUANTITATIVE EVALUATION,0.6882352941176471,"ﬁxing part of the parameters to CogView2 reduce optimization cost, which gains more than 2x
282"
QUANTITATIVE EVALUATION,0.6901960784313725,"acceleration when using optimization CPU-ofﬂoad mode in deepspeed.
283"
QUALITATIVE EVALUATION,0.692156862745098,"5.3.2
Qualitative Evaluation
284"
QUALITATIVE EVALUATION,0.6941176470588235,"Qualitative comparison is shown in Figure 7. While model trained from random initialization tends to
285"
QUALITATIVE EVALUATION,0.696078431372549,"produce irrational deformation, model incorporating CogView2 is able to model objects better. And
286"
QUALITATIVE EVALUATION,0.6980392156862745,"samples generated hierarchically performs better on content consistency and motion rationalization.
287"
QUALITATIVE EVALUATION,0.7,"We also conduct human evaluation between 1-stage and hierarchical video generation model under
288"
QUALITATIVE EVALUATION,0.7019607843137254,"the same setting as 5.2. As shown in 5, hierarchical model, i.e. CogVideo, outperforms 1-stage model
289"
QUALITATIVE EVALUATION,0.703921568627451,"on semantic relevance, motion realism as well as texture quality. This is probably because 1-stage
290"
QUALITATIVE EVALUATION,0.7058823529411765,"model tends to constantly generate small movements which make the whole video unrealistic, and if
291"
QUALITATIVE EVALUATION,0.707843137254902,"one generated frame collapses, the subsequent frames often suffer from severe degradation.
292"
CONCLUSION,0.7098039215686275,"6
Conclusion
293"
CONCLUSION,0.711764705882353,"We present CogVideo, to the best of our knowledge, the largest and the ﬁrst open-source pretrained
294"
CONCLUSION,0.7137254901960784,"transformer for text-to-video generation for the general domain. CogVideo is also the ﬁrst attempt
295"
CONCLUSION,0.7156862745098039,"to efﬁciently leverage pretrained text-to-image generative model to text-to-video generation model
296"
CONCLUSION,0.7176470588235294,"without hurting its image generation capacity. With the proposed multi-frame-rate hierarchical
297"
CONCLUSION,0.7196078431372549,"training framework, CogVideo is endowed with better understanding of text-video relation and ability
298"
CONCLUSION,0.7215686274509804,"to control the intensity of changes during generation. We extend swin attention to CogLM, which
299"
CONCLUSION,0.7235294117647059,"achieves acceleration in both training and inference. There are still some limitations in CogVideo, e.g.
300"
CONCLUSION,0.7254901960784313,"restriction on length of the input sequence still exists due to the large scale of model and limitation of
301"
CONCLUSION,0.7274509803921568,"GPU memory, and we leave them for future work.
302"
CONCLUSION,0.7294117647058823,"Broader Impact. This paper aims to advance the open-domain text-to-video generation, which
303"
CONCLUSION,0.7313725490196078,"will ease the effort of short video and digital art creation. The efﬁcient training method transfers
304"
CONCLUSION,0.7333333333333333,"knowledge from text-to-image models to text-to-video models, which helps avoid training from
305"
CONCLUSION,0.7352941176470589,"scratch, and thus reduce the energy consumption and carbon emission. A negative impact is the risk
306"
CONCLUSION,0.7372549019607844,"of misinformation. To alleviate it, we can train an additional classiﬁer to discriminate the fakes. We
307"
CONCLUSION,0.7392156862745098,"believe the beneﬁts outweigh the downsides.
308"
REFERENCES,0.7411764705882353,"References
309"
REFERENCES,0.7431372549019608,"[1] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
310"
REFERENCES,0.7450980392156863,"P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint
311"
REFERENCES,0.7470588235294118,"arXiv:2005.14165, 2020.
312"
REFERENCES,0.7490196078431373,"[2] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics
313"
REFERENCES,0.7509803921568627,"dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
314"
REFERENCES,0.7529411764705882,"pages 6299–6308, 2017.
315"
REFERENCES,0.7549019607843137,"[3] J. Carreira, E. Noland, A. Banki-Horvath, C. Hillier, and A. Zisserman. A short note about
316"
REFERENCES,0.7568627450980392,"kinetics-600. arXiv preprint arXiv:1808.01340, 2018.
317"
REFERENCES,0.7588235294117647,"[4] A. Clark, J. Donahue, and K. Simonyan. Adversarial video generation on complex datasets.
318"
REFERENCES,0.7607843137254902,"arXiv preprint arXiv:1907.06571, 2019.
319"
REFERENCES,0.7627450980392156,"[5] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang,
320"
REFERENCES,0.7647058823529411,"et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural
321"
REFERENCES,0.7666666666666667,"Information Processing Systems, 34, 2021.
322"
REFERENCES,0.7686274509803922,"[6] M. Ding, W. Zheng, W. Hong, and J. Tang. Cogview2: Faster and better text-to-image generation
323"
REFERENCES,0.7705882352941177,"via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022.
324"
REFERENCES,0.7725490196078432,"[7] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis.
325"
REFERENCES,0.7745098039215687,"arXiv preprint arXiv:2012.09841, 2020.
326"
REFERENCES,0.7764705882352941,"[8] C. Finn, I. Goodfellow, and S. Levine. Unsupervised learning for physical interaction through
327"
REFERENCES,0.7784313725490196,"video prediction. Advances in neural information processing systems, 29, 2016.
328"
REFERENCES,0.7803921568627451,"[9] S. Ge, T. Hayes, H. Yang, X. Yin, G. Pang, D. Jacobs, J.-B. Huang, and D. Parikh. Long
329"
REFERENCES,0.7823529411764706,"video generation with time-agnostic vqgan and time-sensitive transformer. arXiv preprint
330"
REFERENCES,0.7843137254901961,"arXiv:2204.03638, 2022.
331"
REFERENCES,0.7862745098039216,"[10] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
332"
REFERENCES,0.788235294117647,"and Y. Bengio. Generative adversarial networks. arXiv preprint arXiv:1406.2661, 2014.
333"
REFERENCES,0.7901960784313725,"[11] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video diffusion models.
334"
REFERENCES,0.792156862745098,"arXiv preprint arXiv:2204.03458, 2022.
335"
REFERENCES,0.7941176470588235,"[12] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video
336"
REFERENCES,0.796078431372549,"classiﬁcation with convolutional neural networks. In Proceedings of the IEEE conference on
337"
REFERENCES,0.7980392156862746,"Computer Vision and Pattern Recognition, pages 1725–1732, 2014.
338"
REFERENCES,0.8,"[13] J. Lin, C. Gan, and S. Han. Tsm: Temporal shift module for efﬁcient video understanding. In
339"
REFERENCES,0.8019607843137255,"Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083–7093,
340"
REFERENCES,0.803921568627451,"2019.
341"
REFERENCES,0.8058823529411765,"[14] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer:
342"
REFERENCES,0.807843137254902,"Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF
343"
REFERENCES,0.8098039215686275,"International Conference on Computer Vision, pages 10012–10022, 2021.
344"
REFERENCES,0.8117647058823529,"[15] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu. Video swin transformer. arXiv
345"
REFERENCES,0.8137254901960784,"preprint arXiv:2106.13230, 2021.
346"
REFERENCES,0.8156862745098039,"[16] P. Luc, A. Clark, S. Dieleman, D. d. L. Casas, Y. Doron, A. Cassirer, and K. Simonyan.
347"
REFERENCES,0.8176470588235294,"Transformation-based adversarial video prediction on large-scale data.
arXiv preprint
348"
REFERENCES,0.8196078431372549,"arXiv:2003.04035, 2020.
349"
REFERENCES,0.8215686274509804,"[17] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic. Howto100m: Learning
350"
REFERENCES,0.8235294117647058,"a text-video embedding by watching hundred million narrated video clips. In Proceedings of
351"
REFERENCES,0.8254901960784313,"the IEEE/CVF International Conference on Computer Vision, pages 2630–2640, 2019.
352"
REFERENCES,0.8274509803921568,"[18] R. Rakhimov, D. Volkhonskiy, A. Artemov, D. Zorin, and E. Burnaev. Latent video transformer.
353"
REFERENCES,0.8294117647058824,"arXiv preprint arXiv:2006.10704, 2020.
354"
REFERENCES,0.8313725490196079,"[19] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever.
355"
REFERENCES,0.8333333333333334,"Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.
356"
REFERENCES,0.8352941176470589,"[20] M. Saito, E. Matsumoto, and S. Saito. Temporal generative adversarial nets with singular
357"
REFERENCES,0.8372549019607843,"value clipping. In Proceedings of the IEEE international conference on computer vision, pages
358"
REFERENCES,0.8392156862745098,"2830–2839, 2017.
359"
REFERENCES,0.8411764705882353,"[21] M. Saito, S. Saito, M. Koyama, and S. Kobayashi. Train sparsely, generate densely: Memory-
360"
REFERENCES,0.8431372549019608,"efﬁcient unsupervised training of high-resolution temporal gan.
International Journal of
361"
REFERENCES,0.8450980392156863,"Computer Vision, 128(10):2586–2606, 2020.
362"
REFERENCES,0.8470588235294118,"[22] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved
363"
REFERENCES,0.8490196078431372,"techniques for training gans. In Proceedings of the 30th International Conference on Neural
364"
REFERENCES,0.8509803921568627,"Information Processing Systems, pages 2234–2242, 2016.
365"
REFERENCES,0.8529411764705882,"[23] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset of 101 human actions classes from
366"
REFERENCES,0.8549019607843137,"videos in the wild. arXiv preprint arXiv:1212.0402, 2012.
367"
REFERENCES,0.8568627450980392,"[24] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent neural networks. In
368"
REFERENCES,0.8588235294117647,"ICML’11, page 1017–1024, 2011.
369"
REFERENCES,0.8607843137254902,"[25] Y. Tian, J. Ren, M. Chai, K. Olszewski, X. Peng, D. N. Metaxas, and S. Tulyakov. A good image
370"
REFERENCES,0.8627450980392157,"generator is what you need for high-resolution video synthesis. arXiv preprint arXiv:2104.15069,
371"
REFERENCES,0.8647058823529412,"2021.
372"
REFERENCES,0.8666666666666667,"[26] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning spatiotemporal features
373"
REFERENCES,0.8686274509803922,"with 3d convolutional networks. In Proceedings of the IEEE international conference on
374"
REFERENCES,0.8705882352941177,"computer vision, pages 4489–4497, 2015.
375"
REFERENCES,0.8725490196078431,"[27] S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz. Mocogan: Decomposing motion and content
376"
REFERENCES,0.8745098039215686,"for video generation. In Proceedings of the IEEE conference on computer vision and pattern
377"
REFERENCES,0.8764705882352941,"recognition, pages 1526–1535, 2018.
378"
REFERENCES,0.8784313725490196,"[28] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly. To-
379"
REFERENCES,0.8803921568627451,"wards accurate generative models of video: A new metric & challenges. arXiv preprint
380"
REFERENCES,0.8823529411764706,"arXiv:1812.01717, 2018.
381"
REFERENCES,0.884313725490196,"[29] A. van den Oord, O. Vinyals, and K. Kavukcuoglu. Neural discrete representation learning. In
382"
REFERENCES,0.8862745098039215,"Proceedings of the 31st International Conference on Neural Information Processing Systems,
383"
REFERENCES,0.888235294117647,"pages 6309–6318, 2017.
384"
REFERENCES,0.8901960784313725,"[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
385"
REFERENCES,0.8921568627450981,"I. Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
386"
REFERENCES,0.8941176470588236,"[31] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating videos with scene dynamics. Advances
387"
REFERENCES,0.8960784313725491,"in neural information processing systems, 29, 2016.
388"
REFERENCES,0.8980392156862745,"[32] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang. Vatex: A large-scale, high-
389"
REFERENCES,0.9,"quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF
390"
REFERENCES,0.9019607843137255,"International Conference on Computer Vision, pages 4581–4591, 2019.
391"
REFERENCES,0.903921568627451,"[33] Y. Wang, M. Long, J. Wang, Z. Gao, and P. S. Yu. Predrnn: Recurrent neural networks for
392"
REFERENCES,0.9058823529411765,"predictive learning using spatiotemporal lstms. Advances in neural information processing
393"
REFERENCES,0.907843137254902,"systems, 30, 2017.
394"
REFERENCES,0.9098039215686274,"[34] D. Weissenborn, O. Täckström, and J. Uszkoreit. Scaling autoregressive video models. arXiv
395"
REFERENCES,0.9117647058823529,"preprint arXiv:1906.02634, 2019.
396"
REFERENCES,0.9137254901960784,"[35] C. Wu, L. Huang, Q. Zhang, B. Li, L. Ji, F. Yang, G. Sapiro, and N. Duan. Godiva: Generating
397"
REFERENCES,0.9156862745098039,"open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806, 2021.
398"
REFERENCES,0.9176470588235294,"[36] C. Wu, J. Liang, L. Ji, F. Yang, Y. Fang, D. Jiang, and N. Duan. N\"" uwa: Visual synthesis
399"
REFERENCES,0.9196078431372549,"pre-training for neural visual world creation. arXiv preprint arXiv:2111.12417, 2021.
400"
REFERENCES,0.9215686274509803,"[37] W. Yan, Y. Zhang, P. Abbeel, and A. Srinivas. Videogpt: Video generation using vq-vae and
401"
REFERENCES,0.9235294117647059,"transformers. arXiv preprint arXiv:2104.10157, 2021.
402"
REFERENCES,0.9254901960784314,"[38] S. Yu, J. Tack, S. Mo, H. Kim, J. Kim, J.-W. Ha, and J. Shin. Generating videos with dynamics-
403"
REFERENCES,0.9274509803921569,"aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571, 2022.
404"
REFERENCES,0.9294117647058824,"Checklist
405"
REFERENCES,0.9313725490196079,"1. For all authors...
406"
REFERENCES,0.9333333333333333,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
407"
REFERENCES,0.9352941176470588,"contributions and scope? [Yes]
408"
REFERENCES,0.9372549019607843,"(b) Did you describe the limitations of your work? [Yes]
409"
REFERENCES,0.9392156862745098,"(c) Did you discuss any potential negative societal impacts of your work? [Yes]
410"
REFERENCES,0.9411764705882353,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
411"
REFERENCES,0.9431372549019608,"them? [Yes]
412"
REFERENCES,0.9450980392156862,"2. If you are including theoretical results...
413"
REFERENCES,0.9470588235294117,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
414"
REFERENCES,0.9490196078431372,"(b) Did you include complete proofs of all theoretical results? [N/A]
415"
REFERENCES,0.9509803921568627,"3. If you ran experiments...
416"
REFERENCES,0.9529411764705882,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
417"
REFERENCES,0.9549019607843138,"mental results (either in the supplemental material or as a URL)? [No] We will release
418"
REFERENCES,0.9568627450980393,"code later.
419"
REFERENCES,0.9588235294117647,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
420"
REFERENCES,0.9607843137254902,"were chosen)? [Yes]
421"
REFERENCES,0.9627450980392157,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
422"
REFERENCES,0.9647058823529412,"ments multiple times)? [No]
423"
REFERENCES,0.9666666666666667,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
424"
REFERENCES,0.9686274509803922,"of GPUs, internal cluster, or cloud provider)? [No]
425"
REFERENCES,0.9705882352941176,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
426"
REFERENCES,0.9725490196078431,"(a) If your work uses existing assets, did you cite the creators? [Yes] See footnotes.
427"
REFERENCES,0.9745098039215686,"(b) Did you mention the license of the assets? [No]
428"
REFERENCES,0.9764705882352941,"(c) Did you include any new assets either in the supplemental material or as a URL? [No]
429"
REFERENCES,0.9784313725490196,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
430"
REFERENCES,0.9803921568627451,"using/curating? [No]
431"
REFERENCES,0.9823529411764705,"(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
432"
REFERENCES,0.984313725490196,"information or offensive content? [No]
433"
REFERENCES,0.9862745098039216,"5. If you used crowdsourcing or conducted research with human subjects...
434"
REFERENCES,0.9882352941176471,"(a) Did you include the full text of instructions given to participants and screenshots, if
435"
REFERENCES,0.9901960784313726,"applicable? [Yes] See supplemental material.
436"
REFERENCES,0.9921568627450981,"(b) Did you describe any potential participant risks, with links to Institutional Review
437"
REFERENCES,0.9941176470588236,"Board (IRB) approvals, if applicable? [N/A]
438"
REFERENCES,0.996078431372549,"(c) Did you include the estimated hourly wage paid to participants and the total amount
439"
REFERENCES,0.9980392156862745,"spent on participant compensation? [Yes]
440"
