Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018484288354898336,"Offline reinforcement learning, where the agent aims to learn the optimal policy
1"
ABSTRACT,0.0036968576709796672,"based on the data collected by a behavior policy, has attracted increasing attention
2"
ABSTRACT,0.005545286506469501,"in recent years. While offline RL with linear function approximation has been
3"
ABSTRACT,0.0073937153419593345,"extensively studied with optimal results achieved under certain assumptions, the the-
4"
ABSTRACT,0.009242144177449169,"oretical understanding of offline RL with non-linear function approximation is still
5"
ABSTRACT,0.011090573012939002,"limited. Specifically, most existing works on offline RL with non-linear function
6"
ABSTRACT,0.012939001848428836,"approximation either have a poor dependency on the function class complexity or
7"
ABSTRACT,0.014787430683918669,"require an inefficient planning phase. In this paper, we propose an oracle-efficient
8"
ABSTRACT,0.0166358595194085,"algorithm PNLSVI for offline RL with non-linear function approximation. Our
9"
ABSTRACT,0.018484288354898338,"algorithmic design comprises three innovative components: (1) a variance-based
10"
ABSTRACT,0.02033271719038817,"weighted regression scheme that can be applied to a wide range of function classes,
11"
ABSTRACT,0.022181146025878003,"(2) a subroutine for variance estimation, and (3) a planning phase that utilizes a
12"
ABSTRACT,0.024029574861367836,"pessimistic value iteration approach. Our algorithm enjoys a regret bound that has
13"
ABSTRACT,0.025878003696857672,"a tight dependency on the function class complexity and achieves minimax optimal
14"
ABSTRACT,0.027726432532347505,"problem-dependent regret when specialized to linear function approximation. Our
15"
ABSTRACT,0.029574861367837338,"theoretical analysis introduces a new coverage assumption for nonlinear Q func-
16"
ABSTRACT,0.031423290203327174,"tion, bridging the minimum-eigenvalue assumption and the uncertainty measure
17"
ABSTRACT,0.033271719038817,"widely used in online nonlinear RL. To the best of our knowledge, this is the first
18"
ABSTRACT,0.03512014787430684,"statistically optimal algorithm for nonlinear offline RL.
19"
INTRODUCTION,0.036968576709796676,"1
Introduction
20"
INTRODUCTION,0.038817005545286505,"Offline reinforcement learning (RL), also known as batch RL, is a learning paradigm where an
21"
INTRODUCTION,0.04066543438077634,"agent learns to make decisions based on a set of pre-collected data, instead of interacting with the
22"
INTRODUCTION,0.04251386321626617,"environment in real-time like online RL. The goal of offline RL is to learn a policy that performs well
23"
INTRODUCTION,0.04436229205175601,"in a given task, based on historical data that was collected from an unknown environment. Recent
24"
INTRODUCTION,0.04621072088724584,"years have witnessed significant progress in developing offline RL algorithms that can leverage large
25"
INTRODUCTION,0.04805914972273567,"amounts of data to learn effective policies. These algorithms often incorporate powerful function
26"
INTRODUCTION,0.04990757855822551,"approximation techniques, such as deep neural networks, to generalize across large state-action
27"
INTRODUCTION,0.051756007393715345,"spaces. They have achieved excellent performances in a wide range of domains, including the games
28"
INTRODUCTION,0.053604436229205174,"of Go and chess (Silver et al., 2017; Schrittwieser et al., 2020), robotics (Gu et al., 2017; Levine et al.,
29"
INTRODUCTION,0.05545286506469501,"2018), and control systems (Degrave et al., 2022).
30"
INTRODUCTION,0.05730129390018484,"Several studies have studied the theoretical guarantees of tabular offline RL and proved near-optimal
31"
INTRODUCTION,0.059149722735674676,"sample complexities in this setting (Xie et al., 2021b; Shi et al., 2022; Li et al., 2022). However, these
32"
INTRODUCTION,0.06099815157116451,"algorithms cannot handle numerous real-world applications with large state spaces. Consequently,
33"
INTRODUCTION,0.06284658040665435,"a significant body of research has shifted its focus to offline RL with function approximation. For
34"
INTRODUCTION,0.06469500924214418,"example, several works have analyzed the sample efficiency of offline RL with linear function
35"
INTRODUCTION,0.066543438077634,"approximation under different MDP models, including linear MDPs and their variants (Jin et al.,
36"
INTRODUCTION,0.06839186691312385,"2021b; Zanette et al., 2021; Min et al., 2021; Yin et al., 2022a). To handle nonlinear function class, a
37"
INTRODUCTION,0.07024029574861368,"recent line of research considered offline RL with general function approximation (Chen and Jiang,
38"
INTRODUCTION,0.07208872458410351,"2019; Xie et al., 2021a; Zhan et al., 2022). While these algorithms have sample efficiency guarantees,
39"
INTRODUCTION,0.07393715341959335,"they often require an inefficient planning phase or have a poor dependency on the function class
40"
INTRODUCTION,0.07578558225508318,"complexity. For example, Xie et al. (2021a) proposed an information-theoretic algorithm that requires
41"
INTRODUCTION,0.07763401109057301,"solving an optimization problem over all potential policy and corresponding version space, which
42"
INTRODUCTION,0.07948243992606285,"includes all functions with lower Bellman error. To overcome this limitation, Xie et al. (2021a)
43"
INTRODUCTION,0.08133086876155268,"proposed a practical implementation, as a cost, the algorithm have a poor dependency on the function
44"
INTRODUCTION,0.08317929759704251,"class complexity. Recently, (Yin et al., 2022b) studied the general differentiable function class
45"
INTRODUCTION,0.08502772643253234,"and propose a computation efficient algorithm (PFQL). However, their result also have an addition
46"
INTRODUCTION,0.08687615526802218,"dependence on the dimension d of the parameter.
47"
INTRODUCTION,0.08872458410351201,"Therefore, a natural question arises:
48"
INTRODUCTION,0.09057301293900184,"Can we design a computationally efficient algorithm that achieves the minimax optimality with respect
49"
INTRODUCTION,0.09242144177449169,"to the complexity of nonlinear function class?
50"
INTRODUCTION,0.09426987060998152,"We give an affirmative answer to the above question in this work. Our contributions are listed as
51"
INTRODUCTION,0.09611829944547134,"follows:
52"
INTRODUCTION,0.09796672828096119,"• We propose a pessimism-based algorithm PNLSVI designed for nonlinear function approximation,
53"
INTRODUCTION,0.09981515711645102,"which strictly generalizes the existing pessimism-based algorithms for both linear and differentiable
54"
INTRODUCTION,0.10166358595194085,"function approximation (Xiong et al., 2022; Yin et al., 2022b). Our algorithm is oracle-efficient,
55"
INTRODUCTION,0.10351201478743069,"i.e., it is computationally efficient when there exists an efficient regression oracle and bonus oracle
56"
INTRODUCTION,0.10536044362292052,"for the function class (e.g., generalized linear function class).
57"
INTRODUCTION,0.10720887245841035,"• We prove a data-dependent regret bound with the widely used D2-divergence in online nonlinear
58"
INTRODUCTION,0.10905730129390019,"RL regime, which is optimal with respect to the function class complexity. Our analysis closes the
59"
INTRODUCTION,0.11090573012939002,"gap to optimality for differentiable function approximation, which was previously an open problem
60"
INTRODUCTION,0.11275415896487985,"(Yin et al., 2022b).
61"
INTRODUCTION,0.11460258780036968,"• We introduce a novel uniform coverage assumption for general function approximation that is
62"
INTRODUCTION,0.11645101663585952,"generalized over the assumption in Yin et al. (2022b). Our assumption bridges between the
63"
INTRODUCTION,0.11829944547134935,"minimum-eigenvalue assumption used in linear models and the generalized dimension for nonlinear
64"
INTRODUCTION,0.12014787430683918,"function class, offering new insights into the function approximation problem in RL.
65"
INTRODUCTION,0.12199630314232902,"Notation: In this work, we use lowercase letters to denote scalars and use lower and uppercase
66"
INTRODUCTION,0.12384473197781885,"boldface letters to denote vectors and matrices respectively. For a vector x ∈Rd and matrix
67"
INTRODUCTION,0.1256931608133087,"Σ ∈Rd×d, we denote by ∥x∥2 the Euclidean norm and ∥x∥Σ =
√"
INTRODUCTION,0.12754158964879853,"x⊤Σx. For two sequences {an}
68"
INTRODUCTION,0.12939001848428835,"and {bn}, we write an = O(bn) if there exists an absolute constant C such that an ≤Cbn, and we
69"
INTRODUCTION,0.13123844731977818,"write an = Ω(bn) if there exists an absolute constant C such that an ≥Cbn. We use eO(·) and eΩ(·)
70"
INTRODUCTION,0.133086876155268,"to further hide the logarithmic factors. For any a ≤b ∈R, x ∈R, let [x][a,b] denote the truncate
71"
INTRODUCTION,0.13493530499075784,"function a · 1(x ≤a) + x · 1(a ≤x ≤b) + b · 1(b ≤x), where 1(·) is the indicator function. For a
72"
INTRODUCTION,0.1367837338262477,"positive integer n, we use [n] = {1, 2, .., n} to denote the set of integers from 1 to n.
73"
RELATED WORK,0.13863216266173753,"2
Related Work
74"
RELATED WORK,0.14048059149722736,"RL with function approximation. As one of the simplest function approximation classes, linear
75"
RELATED WORK,0.1423290203327172,"representation in RL has been extensively studied in recent years (Jiang et al., 2017; Dann et al.,
76"
RELATED WORK,0.14417744916820702,"2018; Yang and Wang, 2019; Jin et al., 2020; Wang et al., 2020c; Du et al., 2019; Sun et al., 2019;
77"
RELATED WORK,0.14602587800369685,"Zanette et al., 2020a,b; Weisz et al., 2021; Yang and Wang, 2020; Modi et al., 2020; Ayoub et al.,
78"
RELATED WORK,0.1478743068391867,"2020; Zhou et al., 2021; He et al., 2021). Several assumptions on the linear structure of the underlying
79"
RELATED WORK,0.14972273567467653,"MDPs have been made in these works, ranging from the linear MDP assumption (Yang and Wang,
80"
RELATED WORK,0.15157116451016636,"2019; Jin et al., 2020; Hu et al., 2022; He et al., 2022; Agarwal et al., 2022) to the low Bellman-rank
81"
RELATED WORK,0.1534195933456562,"assumption (Jiang et al., 2017) and the low inherent Bellman error assumption (Zanette et al., 2020b).
82"
RELATED WORK,0.15526802218114602,"Extending the previous theoretical guarantees to more general problem classes, RL with nonlinear
83"
RELATED WORK,0.15711645101663585,"function classes has garnered increased attention in recent years (Wang et al., 2020b; Jin et al.,
84"
RELATED WORK,0.1589648798521257,"2021a; Foster et al., 2021; Du et al., 2021; Agarwal and Zhang, 2022; Agarwal et al., 2022). Various
85"
RELATED WORK,0.16081330868761554,"complexity measures of function classes have been studied including Bellman rank (Jiang et al.,
86"
RELATED WORK,0.16266173752310537,"2017), Bellman-Eluder dimension (Jin et al., 2021a), Decision-Estimation Coefficient (Foster et al.,
87"
RELATED WORK,0.1645101663585952,"2021) and generalized Eluder dimension (Agarwal et al., 2022). Among these works, the setting in
88"
RELATED WORK,0.16635859519408502,"our paper is most related to Agarwal et al. (2022) where D2-divergence (Gentile et al., 2022) was
89"
RELATED WORK,0.16820702402957485,"introduced in RL to indicate the uncertainty of a sample with respect to a particular sample batch.
90"
RELATED WORK,0.17005545286506468,"Offline tabular RL. There is a line of works integrating the principle of pessimism to develop
91"
RELATED WORK,0.17190388170055454,"statistically efficient algorithms for offline tabular RL setting (Rashidinejad et al., 2021; Yin and
92"
RELATED WORK,0.17375231053604437,"Wang, 2021; Xie et al., 2021b; Shi et al., 2022; Li et al., 2022). More specifically, Xie et al. (2021b)
93"
RELATED WORK,0.1756007393715342,"utilized the variance of transition noise and proposed a nearly optimal algorithm based on pessimism
94"
RELATED WORK,0.17744916820702403,"and Bernstein-type bonus. Subsequently, Li et al. (2022) proposed a model-based approach that
95"
RELATED WORK,0.17929759704251386,"achieves minimax-optimal sample complexity without burn-in cost for tabular MDPs. Shi et al. (2022)
96"
RELATED WORK,0.18114602587800369,"also contributed by proposing the first nearly minimax-optimal model-free offline RL algorithm.
97"
RELATED WORK,0.18299445471349354,"Offline RL with linear function approximation. Jin et al. (2021b) presented the initial theoretical
98"
RELATED WORK,0.18484288354898337,"results on offline linear MDPs. They introduced a pessimism-principled algorithmic framework for
99"
RELATED WORK,0.1866913123844732,"offline RL and proposed an algorithm based on LSVI (Jin et al., 2020). Min et al. (2021) subsequently
100"
RELATED WORK,0.18853974121996303,"considered offline policy evaluation (OPE) in linear MDPs, assuming independence between data
101"
RELATED WORK,0.19038817005545286,"samples across time steps to obtain tighter confidence sets and proposed an algorithm with optimal
102"
RELATED WORK,0.1922365988909427,"d dependence. Yin et al. (2022a) took one step further and considered the policy optimization in
103"
RELATED WORK,0.19408502772643252,"linear MDPs, which implicitly requires the same independence assumption. Zanette et al. (2021)
104"
RELATED WORK,0.19593345656192238,"proposed an actor-critic-based algorithm that establishes pessimism principle by directly perturbing
105"
RELATED WORK,0.1977818853974122,"the parameter vectors in a linear function approximation framework. Recently, Xiong et al. (2022)
106"
RELATED WORK,0.19963031423290203,"proposed a novel uncertainty decomposition technique via a reference function, which leads to a
107"
RELATED WORK,0.20147874306839186,"minimax-optimal sample complexity bound for offline linear MDPs without additional assumptions.
108"
RELATED WORK,0.2033271719038817,"Offline RL with general function approximation.
Chen and Jiang (2019) critically examined
109"
RELATED WORK,0.20517560073937152,"the assumptions underlying value-function approximation methods and established an information-
110"
RELATED WORK,0.20702402957486138,"theoretic lower bound. Xie et al. (2021a) introduced the concept of Bellman-consistent pessimism,
111"
RELATED WORK,0.2088724584103512,"which enables sample-efficient guarantees by relying solely on the Bellman-completeness assumption.
112"
RELATED WORK,0.21072088724584104,"Uehara and Sun (2021) focused on model-based offline RL with function approximation under partial
113"
RELATED WORK,0.21256931608133087,"coverage, demonstrating that realizability in the function class and partial coverage are sufficient for
114"
RELATED WORK,0.2144177449168207,"policy learning. Zhan et al. (2022) proposed an algorithm that achieves polynomial sample complexity
115"
RELATED WORK,0.21626617375231053,"under the realizability and single-policy concentrability assumptions. Nguyen-Tang and Arora (2023)
116"
RELATED WORK,0.21811460258780038,"proposed a method of random perturbations and pessimism for neural function approximation. For
117"
RELATED WORK,0.2199630314232902,"differentiable function classes, Yin et al. (2022b) made advancements by improving the sample
118"
RELATED WORK,0.22181146025878004,"complexity with respect to the stage H. However, their result had an additional dependence on the
119"
RELATED WORK,0.22365988909426987,"dimension d of the parameter space, whereas in linear function approximation, the dependence is
120"
RELATED WORK,0.2255083179297597,"typically on
√"
RELATED WORK,0.22735674676524953,"d.
121"
PRELIMINARIES,0.22920517560073936,"3
Preliminaries
122"
PRELIMINARIES,0.23105360443622922,"In our work, we consider the inhomogeneous episodic Markov Decision Processes (MDP), which can
123"
PRELIMINARIES,0.23290203327171904,"be denoted by a tuple of M
 
S, A, H, {rh}H
h=1, {Ph}H
h=1

. In specific, S is the state space, A is the
124"
PRELIMINARIES,0.23475046210720887,"finite action space, H is the length of each episode. For each stage h ∈[H], rh : S ×A →[0, 1] is the
125"
PRELIMINARIES,0.2365988909426987,"reward function1 and Ph(s′|s, a) is the transition probability function, which denotes the probability
126"
PRELIMINARIES,0.23844731977818853,"for state s to transfer to next state s′ with current action a. A policy π := {πh}H
h=1 is a collection
127"
PRELIMINARIES,0.24029574861367836,"of mappings πh from a state s ∈S to the simplex of action space A. For simplicity, we denote the
128"
PRELIMINARIES,0.24214417744916822,"state-action pair as z := (s, a). For any policy π and stage h ∈[H], we define the value function
129"
PRELIMINARIES,0.24399260628465805,"V π
h (s) and the action-value function Qπ
h(s, a) as the expected cumulative rewards starting at stage h,
130"
PRELIMINARIES,0.24584103512014788,"which can be denoted as follows:
131"
PRELIMINARIES,0.2476894639556377,"Qπ
h(s, a) = rh(s, a) + E

H
X"
PRELIMINARIES,0.24953789279112754,"h′=h+1
rh′ 
sh′, πh′(sh′)
sh = s, ah = a

, V π
h (s) = Qπ
h
 
s, πh(s)

,"
PRELIMINARIES,0.2513863216266174,"where sh′+1 ∼Ph(·|sh′, ah′) denotes the observed state at stage h′ + 1. By this definition, the value
132"
PRELIMINARIES,0.2532347504621072,"function V π
h (s) and action-value function Qπ
h(s, a) are bounded in [0, H]. In addition, we define the
133"
PRELIMINARIES,0.25508317929759705,"optimal value function V ∗
h and the optimal action-value function Q∗
h as V ∗
h (s) = maxπ V π
h (s) and
134"
PRELIMINARIES,0.25693160813308685,"Q∗
h(s, a) = maxπ Qπ
h(s, a). We denote the corresponding optimal policy by π∗. For any function
135"
PRELIMINARIES,0.2587800369685767,"V : S →R, we denote [PhV ](s, a) = Es′∼Ph(·|s,a)V (s′) and [VarhV ](s, a) = [PhV 2](s, a) −
136
 
[PhV ](s, a)
2 for simplicity. For any function f : S →R, we define the Bellman operator Th
137"
PRELIMINARIES,0.26062846580406657,"as Thf(sh, ah) = Esh+1∼Ph(·|sh,ah) [rh(sh, ah) + f(sh+1)], where we use the shorthand f(s) =
138"
PRELIMINARIES,0.26247689463955637,"maxa∈A f(s, a) for simplicity. Based on this definition, for every stage h ∈[H] and policy π, we
139"
PRELIMINARIES,0.2643253234750462,"1While we study the deterministic reward functions for simplicity, it is not difficult to generalize our results
to stochastic reward functions."
PRELIMINARIES,0.266173752310536,"have the following Bellman equation for value functions Qπ
h(s, a) and V π
h (s), as well as the Bellman
140"
PRELIMINARIES,0.2680221811460259,"optimality equation for optimal value functions:
141"
PRELIMINARIES,0.2698706099815157,"Qπ
h(sh, ah) = ThV π
h+1(sh, ah), Q∗
h(sh, ah) = ThV ∗
h+1(sh, ah),"
PRELIMINARIES,0.27171903881700554,"where V π
H+1(s) = V ∗
H+1(s) = 0. We also define the Bellman operator for second moment as
142"
PRELIMINARIES,0.2735674676524954,"T2,hf(sh, ah) = Esh+1∼Ph(·|sh,ah)
h 
rh(sh, ah) + f(sh+1)
2i
. For simplicity, we omit the sub-
143"
PRELIMINARIES,0.2754158964879852,"scripts h in the Bellman operator without causing confusion.
144"
PRELIMINARIES,0.27726432532347506,"Offline Reinforcement Learning:
In offline RL, the agent only have access to a batch-dataset
145"
PRELIMINARIES,0.27911275415896486,"D = {sk
h, ak
h, rk
h : h ∈[H], k ∈[K]}, which is collected by a behavior policy µ, and the agent
146"
PRELIMINARIES,0.2809611829944547,"cannot interact with the environment. Given the batch dataset, the goal of offline RL is finding a
147"
PRELIMINARIES,0.2828096118299446,"near-optimal policy π that minimize the sub-optimality V ∗
1 (s) −V π
1 (s). In addition, for each stage h
148"
PRELIMINARIES,0.2846580406654344,"and behavior policy µ, we denote the induced distribution of the state-action pair as dµ
h.
149"
PRELIMINARIES,0.28650646950092423,"General Function Approximation:
In this work, we focus on a special class of episodic MDPs,
150"
PRELIMINARIES,0.28835489833641403,"where the value function satisfies the following completeness assumption.
151"
PRELIMINARIES,0.2902033271719039,"Assumption 3.1 (ϵ-completeness under general function approximation, Agarwal et al. 2022). Given
152"
PRELIMINARIES,0.2920517560073937,"a general function class {Fh}h∈[H], where each function class Fh is composed of functions fh :
153"
PRELIMINARIES,0.29390018484288355,"S × A →[0, L]. We assume for each stage h ∈[H], and any function V : S →[0, H], there exists
154"
PRELIMINARIES,0.2957486136783734,"functions fh, f2,h ∈Fh such that
155"
PRELIMINARIES,0.2975970425138632,"max
(s,a)∈S×A |fh(s, a) −ThV (s, a)| ≤ϵ, and
max
(s,a)∈S×A |f2,h(s, a) −T2,hV (s, a)| ≤ϵ."
PRELIMINARIES,0.29944547134935307,"In addition, for each stage h ∈[H], we assume there exist a function f ∗
h ∈Fh closed to the optimal
156"
PRELIMINARIES,0.30129390018484287,"value function such that ∥f ∗
h −Q∗
h∥∞≤ϵ. For simplicity, we assume L = O(H) throughout the
157"
PRELIMINARIES,0.3031423290203327,"paper and denote N = maxh∈[H] |Fh|.
158"
PRELIMINARIES,0.3049907578558225,"To deal with general function class F, Agarwal et al. (2022) introduce the following measure to
159"
PRELIMINARIES,0.3068391866913124,"capture the function class complexity for online learning.
160"
PRELIMINARIES,0.30868761552680224,"Definition 3.2 (Generalized Eluder dimension, Agarwal et al. 2022). Given λ > 0, a sequence of
161"
PRELIMINARIES,0.31053604436229204,"state-action pairs Z = {zi}i∈[K] and a sequence of non-negative weights σ = {σi}i∈[K]. Let F be a
162"
PRELIMINARIES,0.3123844731977819,"function class consisting of functions f : S × A →[0, L]. The generalized Eluder dimension of F is
163"
PRELIMINARIES,0.3142329020332717,"given by dimα,K(F) := supZ,σ:|Z|=K,σ≥α dim(F, Z, σ), where
164"
PRELIMINARIES,0.31608133086876156,"dim(F, Z, σ) := K
X"
PRELIMINARIES,0.3179297597042514,"i=1
min

1, 1"
PRELIMINARIES,0.3197781885397412,"σ2
i
D2
F(zi; z[i−1], σ[i−1])

,"
PRELIMINARIES,0.32162661737523107,"D2
F(z; z[k−1], σ[k−1]) :=
sup
f1,f2∈F"
PRELIMINARIES,0.3234750462107209,"(f1(z) −f2(z))2
P"
PRELIMINARIES,0.32532347504621073,"s∈[k−1]
1
σ2s (f1(zs) −f2(zs))2 + λ."
PRELIMINARIES,0.32717190388170053,"Here, the inequality σ ≥α represents that σi ≥α holds for all i ∈[K] and we use the notation
165"
PRELIMINARIES,0.3290203327171904,"z[i−1], σ[i−1] to represent the sequences {zs}i−1
s=1, {σs}i−1
s=1.
166"
PRELIMINARIES,0.33086876155268025,"However, in offline RL, the proposed Generalized Eluder dimension fails to capture the relationship
167"
PRELIMINARIES,0.33271719038817005,"between function class F and the pre-collected dataset D. To generalize this definition to offline
168"
PRELIMINARIES,0.3345656192236599,"environment, for a batch dataset D = {(sk
h, ak
h, rk
h)}H,K
h,k=1 and a function class Fh consisting of
169"
PRELIMINARIES,0.3364140480591497,"functions f : S × A →R. We denote Dh = {(sk
h, ak
h, rk
h)}k∈[K] as the subset of the dataset D that
170"
PRELIMINARIES,0.33826247689463956,"corresponds to the observations collected up to stage h in the MDP. Then for any weight function
171"
PRELIMINARIES,0.34011090573012936,"σh(·, ·) : S × A →R, we introduce the following D2-divergence:
172"
PRELIMINARIES,0.3419593345656192,"D2
Fh(z; Dh; σh) =
sup
f1,f2∈Fh"
PRELIMINARIES,0.3438077634011091,"(f1(z) −f2(z))2
P"
PRELIMINARIES,0.3456561922365989,"k∈[K]
1
(σh(zk
h))2 (f1(zk
h) −f2(zk
h))2 + λ."
PRELIMINARIES,0.34750462107208874,"Data Coverage Assumption:
In offline RL, there exist a discrepancy between the state-action
173"
PRELIMINARIES,0.34935304990757854,"distribution generated by the behavior policy and the distribution from the learned policy. Under this
174"
PRELIMINARIES,0.3512014787430684,"situation, the distribution shift problem can cause the learned policy to perform poorly or even fail in
175"
PRELIMINARIES,0.35304990757855825,"offline RL. Therefore, we propose the following data coverage assumption to control the distribution
176"
PRELIMINARIES,0.35489833641404805,"shift.
177"
PRELIMINARIES,0.3567467652495379,"Algorithm 1 Pessimistic Nonlinear Least-Squares Value Iteration (PNLSVI)
Require: Input confidence parameters β′
1,h, β′
2,h, βh and ϵ > 0."
PRELIMINARIES,0.3585951940850277,"1: Initialize: Split the input dataset into D = {sk
h, ak
h, rk
h}K,H
k,h=1, D′ = {¯sk
h, ¯ak
h, ¯rk
h}K,H
k,h=1 ; Set the"
PRELIMINARIES,0.36044362292051757,"value function bfH+1(·) = bf ′
H+1(·) = 0.
2: for stage h = H, . . . , 1 do"
PRELIMINARIES,0.36229205175600737,"3:
ef ′
h = argminfh∈Fh
P"
PRELIMINARIES,0.36414048059149723,"k∈[K]

fh(¯sk
h, ¯ak
h) −¯rk
h −bf ′
h+1(¯sk
h+1)
2
."
PRELIMINARIES,0.3659889094269871,"4:
eg′
h = argmingh∈Fh
P
k∈[K]"
PRELIMINARIES,0.3678373382624769,"
gh(¯sk
h, ¯ak
h) −

¯rk
h + bf ′
h+1(¯sk
h+1)
22
."
PRELIMINARIES,0.36968576709796674,"5:
Use the bonus oracle (Definition 4.1) to calculate the bonus function
b′
h = B(1, D′
h, Fh, ef ′
h, β′
1,h + β′
2,h, λ, ϵ),"
PRELIMINARIES,0.37153419593345655,"6:
bf ′
h ←{ ef ′
h −b′
h −ϵ}[0,H−h+1];
7:
Construct the variance estimator
bσ2
h(s, a) = max
n
1, eg′
h(s, a) −( ef ′
h(s, a))2 −O
 √log N NbH3 √ Kκ o
."
PRELIMINARIES,0.3733826247689464,"8: end for
9: for stage h = H, . . . , 1 do"
PRELIMINARIES,0.3752310536044362,"10:
efh = argminfh∈Fh
P"
PRELIMINARIES,0.37707948243992606,"k∈[K]
1
bσ2
h(sk
h,ak
h)"
PRELIMINARIES,0.3789279112754159,"
fh(sk
h, ak
h) −rk
h −bfh+1(sk
h+1)
2"
PRELIMINARIES,0.3807763401109057,"11:
Use the bonus oracle (Definition 4.1) to calculate the bonus function
bh = B(bσh, Dh, Fh, efh, βh, λ, ϵ);
12:
bfh ←{ efh −bh −ϵ}[0,H−h+1];"
PRELIMINARIES,0.3826247689463956,"13:
bπh(·|s) = argmaxa bfh(s, a).
14: end for
15: Output: bπ = {bπh}H
h=1."
PRELIMINARIES,0.3844731977818854,"Assumption 3.3 (Uniform Data Coverage). There exists a constant κ > 0, such that for any stage h
178"
PRELIMINARIES,0.38632162661737524,"and functions f1, f2 ∈Fh, the following inequality holds,
179"
PRELIMINARIES,0.38817005545286504,"Eµ,h
 
f1(sh, ah) −f2(sh, ah)
2 
≥κ∥f1 −f2∥2
∞,"
PRELIMINARIES,0.3900184842883549,"where the state-action pair (at stage h) (sh, ah) is stochastic generated from behavior policy µ.
180"
PRELIMINARIES,0.39186691312384475,"Remark 3.4. Data coverage assumption is widely used in offline RL to guarantee that the collected
181"
PRELIMINARIES,0.39371534195933455,"dataset contains enough information of the state-action space to learn an effective policy. In Yin et al.
182"
PRELIMINARIES,0.3955637707948244,"(2022b), they studied the general differentiable function, where the function class is defined as
183"
PRELIMINARIES,0.3974121996303142,"F :=
n
f
 
θ, ϕ(·, ·)

: S × A →R, θ ∈Θ
o
."
PRELIMINARIES,0.39926062846580407,"Under this definition, Yin et al. (2022b) introduce the following coverage assumption (Assumption
184"
PRELIMINARIES,0.4011090573012939,"2.3) such that for all stage h ∈[H], there exists a constant κ,
185"
PRELIMINARIES,0.4029574861367837,"Eµ,h
h 
f(θ1, ϕ(s, a)) −f(θ2, ϕ(s, a))
2i
≥κ∥θ1 −θ2∥2
2, ∀θ1, θ2 ∈Θ;
(∗)"
PRELIMINARIES,0.4048059149722736,"Eµ,h

∇f(θ, ϕ(s, a))∇f(θ, ϕ(s, a))⊤
≻κI, ∀θ ∈Θ.
(∗∗)"
PRELIMINARIES,0.4066543438077634,"We can prove that our assumption is weaker than the first assumption (*). For the second assumption
186"
PRELIMINARIES,0.40850277264325324,"(**), there is no direct counterpart in the general setting.
187"
PRELIMINARIES,0.41035120147874304,"In addition, for the linear function class, the coverage assumption in Yin et al. (2022b) will reduce to
188"
PRELIMINARIES,0.4121996303142329,"the following linear function coverage assumption (Wang et al., 2020a; Min et al., 2021; Yin et al.,
189"
PRELIMINARIES,0.41404805914972276,"2022a; Xiong et al., 2022).
190"
PRELIMINARIES,0.41589648798521256,"λmin(Eµ,h[ϕ(s, a)ϕ(s, a)⊤]) = κ > 0, ∀h ∈[H]."
PRELIMINARIES,0.4177449168207024,"Therefore, our assumption is also weaker than the linear function coverage assumption when dealing
191"
PRELIMINARIES,0.4195933456561922,"with the linear function class. Due to space limitations, we provide the detailed proof in the appendix.
192"
ALGORITHM,0.4214417744916821,"4
Algorithm
193"
ALGORITHM,0.4232902033271719,"In this section, we provide a comprehensive and detailed description of our algorithm (PNLSVI), as
194"
ALGORITHM,0.42513863216266173,"displayed in Algorithm 1. In the sequel, we introduce the key ideas of the proposed algorithm.
195"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.4269870609981516,"4.1
Pessimistic Value Iteration Based Planning
196"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.4288354898336414,"Our algorithm operates in two distinct phases, Variance Estimate Phase and Pessimistic Planning
197"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.43068391866913125,"Phase. At the beginning of the algorithm, the data-set is divided into two disjoint subsets D, D′, and
198"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.43253234750462105,"each assigned to a specific phase.
199"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.4343807763401109,"The basic framework of our algorithm follows the pessimistic value iteration, which was initially
200"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.43622920517560076,"introduced by Jin et al. (2021b). In details, for each stage h ∈[H], we construct the estimator value
201"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.43807763401109057,"function efh by solving the following variance-weighted ridge regression (Line 11):
202"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.4399260628465804,"efh = argmin
fh∈Fh X k∈[K]"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.4417744916820702,"1
bσ2
h(sk
h, ak
h)"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.4436229205175601,"
fh(sk
h, ak
h) −rk
h −bfh+1(sk
h+1)
2
,"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.4454713493530499,"where bσ2
h is the estimated variance and will be discussed in section 4.2. In Line 12, we subtract the
203"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.44731977818853974,"confidence bonus function bh from the estimator value function efh to construct the pessimistic value
204"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.4491682070240296,"function bfh. With the help of the confidence bonus function bh, the pessimistic value function bfh is
205"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.4510166358595194,"almost a lower bound for the optimal value function f ∗
h. The details of the bonus function and bonus
206"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.45286506469500926,"oracle will be discussed in section 4.3.
207"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.45471349353049906,"Based on the pessimistic value function bfh for stage h, we recursively perform the value iteration
208"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.4565619223659889,"for the stage h −1. Finally, we use the pessimistic value function bfh to do planning and output the
209"
PESSIMISTIC VALUE ITERATION BASED PLANNING,0.4584103512014787,"greedy policy with respect to the pessimistic value function bfh (Line 13 - Line 15).
210"
VARIANCE ESTIMATE PHASE,0.4602587800369686,"4.2
Variance Estimate Phase
211"
VARIANCE ESTIMATE PHASE,0.46210720887245843,"In this phase, we provide a estimator for the variance bσh in the weighted ridge regression. According
212"
VARIANCE ESTIMATE PHASE,0.46395563770794823,"to the definition of Bellman operators T and T2, the variance of the function bf ′
h+1 for each state-action
213"
VARIANCE ESTIMATE PHASE,0.4658040665434381,"pair (s, a) can be denoted by
214"
VARIANCE ESTIMATE PHASE,0.4676524953789279,"[Varh bfh+1](s, a) = T2,h bf ′
h+1(s, a) −

Th bf ′
h+1(s, a)
2
."
VARIANCE ESTIMATE PHASE,0.46950092421441775,"Therefore, we need the evaluate the first-order and second-order moments for bf ′
h. We perform
215"
VARIANCE ESTIMATE PHASE,0.4713493530499076,"nonlinear least-squares regression separately for each of these moments. Specifically, in Line 3, we
216"
VARIANCE ESTIMATE PHASE,0.4731977818853974,"conduct regression to estimate the first-order moment.
217"
VARIANCE ESTIMATE PHASE,0.47504621072088726,"ef ′
h = argmin
fh∈Fh X k∈[K]"
VARIANCE ESTIMATE PHASE,0.47689463955637706,"
fh(¯sk
h, ¯ak
h) −¯rk
h −bf ′
h+1(¯sk
h+1)
2
."
VARIANCE ESTIMATE PHASE,0.4787430683918669,"In Line 4, we perform regression for the second-order moment.
218"
VARIANCE ESTIMATE PHASE,0.4805914972273567,"eg′
h = argmin
gh∈Fh X k∈[K]"
VARIANCE ESTIMATE PHASE,0.4824399260628466,"
gh(¯sk
h, ¯ak
h) −

¯rk
h + bf ′
h+1(¯sk
h+1)
22
."
VARIANCE ESTIMATE PHASE,0.48428835489833644,"In this phase, we set the variance function to 1 for each state-action pair (s, a) and derive an estimator
219"
VARIANCE ESTIMATE PHASE,0.48613678373382624,"with confidence radius β′
1,h, β′
2,h. Combing these two regression results and subtracting a confidence
220"
VARIANCE ESTIMATE PHASE,0.4879852125693161,"bonus function b′
h, we create a pessimistic estimator for the variance function (Lines 6 to 7).
221"
NONLINEAR BONUS ORACLE,0.4898336414048059,"4.3
Nonlinear Bonus Oracle
222"
NONLINEAR BONUS ORACLE,0.49168207024029575,"As we discussed in sections 4.1 and 4.2, we introduce a uncertainty bonus function to construct a
223"
NONLINEAR BONUS ORACLE,0.49353049907578556,"pessimistic estimate of the value function. Unfortunately, for a general class, the uncertainty bonus
224"
NONLINEAR BONUS ORACLE,0.4953789279112754,"may varies greatly across different state-action pair. Under this situation, the addition uncertainty
225"
NONLINEAR BONUS ORACLE,0.49722735674676527,"bonus function will highly increase the complexity of the pessimistic function class, which make
226"
NONLINEAR BONUS ORACLE,0.49907578558225507,"it difficult to construct a accurate estimation and may significant deteriorate the final performance.
227"
NONLINEAR BONUS ORACLE,0.5009242144177449,"To address this issue, we assume there exist a function class W with cardinally |W| = Nb and can
228"
NONLINEAR BONUS ORACLE,0.5027726432532348,"approximate the bonus function well. In addition, we assume there exist a nonlinear bonus oracle
229"
NONLINEAR BONUS ORACLE,0.5046210720887245,"(Agarwal and Zhang, 2022), which can output the approximate bonus function in the class W for
230"
NONLINEAR BONUS ORACLE,0.5064695009242144,"each dataset Dh.
231"
NONLINEAR BONUS ORACLE,0.5083179297597042,"Definition 4.1 (Oracle for bonus function). For an offline dataset D = {sk
h, ak
h, rk
h}H,K
h,k=1, given
232"
NONLINEAR BONUS ORACLE,0.5101663585951941,"index h ∈[H], let Dh = {(sk
h, ak
h, rk
h)}k∈[K] denote the subset of the dataset D that corresponds to
233"
NONLINEAR BONUS ORACLE,0.512014787430684,"the observations collected up to stage h in the MDP. bσh(·, ·) : S × A →R is a variance function. Fh
234"
NONLINEAR BONUS ORACLE,0.5138632162661737,"is a function class such that bfh ∈Fh. The parameters βh, λ ≥0, error parameter ϵ ≥0. The bonus
235"
NONLINEAR BONUS ORACLE,0.5157116451016636,"oracle B(bσ, Dh, Fh, bfh, βh, λ, ϵ) outputs a bonus function bh(·) such that
236"
NONLINEAR BONUS ORACLE,0.5175600739371534,"• bh : S × A →R≥0 belongs to function class W.
237"
NONLINEAR BONUS ORACLE,0.5194085027726433,"• bh(zh) ≥max
n
|fh(zh) −bfh(zh)|, fh ∈Fh : P"
NONLINEAR BONUS ORACLE,0.5212569316081331,"k∈[K]
(fh(zk
h)−b
fh(zk
h))2"
NONLINEAR BONUS ORACLE,0.5231053604436229,"(bσh(sk
h,ak
h))2
≤(βh)2o
for any zh ∈S ×A.
238"
NONLINEAR BONUS ORACLE,0.5249537892791127,"• bh(zh) ≤C ·

DFh(zh; Dh; bσh) ·
p"
NONLINEAR BONUS ORACLE,0.5268022181146026,"(βh)2 + λ + ϵβh

for all zh ∈S ×A with constant 0 < C ≤∞.
239"
NONLINEAR BONUS ORACLE,0.5286506469500925,"Remark 4.2. To address the concern of function class complexity, some previous studies (Xie et al.,
240"
NONLINEAR BONUS ORACLE,0.5304990757855823,"2021a) have approached the problem differently. Instead of introducing a pointwise bonus in the
241"
NONLINEAR BONUS ORACLE,0.532347504621072,"estimated value function, they solve a complicated optimization problem to guarantee the optimism
242"
NONLINEAR BONUS ORACLE,0.5341959334565619,"solely in the intial state. This method can prevent the complexity from bonus function, as a cost, they
243"
NONLINEAR BONUS ORACLE,0.5360443622920518,"requires solving an optimization problem over all potential policy and corresponding version space,
244"
NONLINEAR BONUS ORACLE,0.5378927911275416,"which includes all functions with lower Bellman error.
245"
MAIN RESULTS,0.5397412199630314,"5
Main Results
246"
MAIN RESULTS,0.5415896487985212,"In this section we prove an problem-dependent regret bound of Algorithm 1.
247"
MAIN RESULTS,0.5434380776340111,"Theorem 5.1. Under Assumption 3.3, for K ≥eΩ

log(N Nb)H6"
MAIN RESULTS,0.5452865064695009,"κ2

, if we set the parameters
248"
MAIN RESULTS,0.5471349353049908,"β′
1,h, β′
2,h = eO(√log NNbH2) and βh = eO(√log N) in Algorithm 1, then with the probability of
249"
MAIN RESULTS,0.5489833641404805,"at least 1 −δ, for any state s ∈S, we have
250"
MAIN RESULTS,0.5508317929759704,"V ∗
1 (s) −V bπ
1 (s) ≤eO(
p"
MAIN RESULTS,0.5526802218114603,"log N) H
X"
MAIN RESULTS,0.5545286506469501,"h=1
Eπ∗
DFh(zh; Dh; [VhV ∗
h+1](·, ·))|s1 = s

,"
MAIN RESULTS,0.55637707948244,"where [VhV ∗
h+1](s, a) = max{1, [VarhV ∗
h+1](s, a)} is the truncated conditional variance.
251"
MAIN RESULTS,0.5582255083179297,"Remark 5.2. When reduce to the linear MDP environment, the following function classes
252"
MAIN RESULTS,0.5600739371534196,"Flin
h = {⟨ϕh(·, ·), θh⟩: θh ∈Rd, ∥θh∥2 ≤Bh} for any h ∈[H],"
MAIN RESULTS,0.5619223659889094,"satisfy the completeness assumption (Assumption 3.1) (Jin et al., 2020). Let Flin
h (ϵ) be a ϵ-net of the
253"
MAIN RESULTS,0.5637707948243993,"linear function class Flin
h . In this case, the covering number satisfies log |Flin
h (ϵ)| = eO(d) and the
254"
MAIN RESULTS,0.5656192236598891,"dependency of function class will reduce to eO(√log N) = eO(
√"
MAIN RESULTS,0.5674676524953789,"d). For linear function class, Xiong
255"
MAIN RESULTS,0.5693160813308688,"et al. (2022) proposed the following regret guarantee,
256"
MAIN RESULTS,0.5711645101663586,"V ∗
1 (s) −V bπ
1 (s) ≤eO(
√ d) · H
X"
MAIN RESULTS,0.5730129390018485,"h=1
Eπ∗
h
∥ϕ(sh, ah)∥Σ∗−1
h
|s1 = s
i
,"
MAIN RESULTS,0.5748613678373382,"where Σ∗
h = P
k∈[K] ϕ(sk
h, ak
h)ϕ(sk
h, ak
h)⊤/[VhV ∗
h+1](sk
h, ak
h) + λI. In comparison, we can prove
257"
MAIN RESULTS,0.5767097966728281,"the following inequality:
258"
MAIN RESULTS,0.5785582255083179,"DFlin
h (ϵ)(z; Dh; [VhV ∗
h+1](·, ·)) ≤∥ϕh(z)∥Σ∗−1
h
."
MAIN RESULTS,0.5804066543438078,"This shows that Theorem 5.1 matches the optimal result in Xiong et al. (2022) for linear function
259"
MAIN RESULTS,0.5822550831792976,"class.
260"
KEY TECHNIQUES,0.5841035120147874,"6
Key Techniques
261"
KEY TECHNIQUES,0.5859519408502772,"In this section, we provide an overview of the key techniques in our algorithm design and analysis.
262"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.5878003696857671,"6.1
Variance Estimator with Nonlinear Function Class
263"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.589648798521257,"The technique of variance-weighted ridge regression, first introduced in Zhou et al. (2021), has
264"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.5914972273567468,"demonstrated its effectiveness in the online RL setting with linear function approximation. For offline
265"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.5933456561922366,"setting, Xiong et al. (2022) modified the variance-weighted ridge regression technique, and showed
266"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.5951940850277264,"that using an accurate and independent variance estimator can improves the performance of the
267"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.5970425138632163,"pessimistic value iteration (PEVI) algorithm (Jin et al., 2021b).
268"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.5988909426987061,"In our work, we extend this technique to general nonlinear function class F, and use the following
269"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.600739371534196,"nonlinear least-squares regression to estimate the underlying value function:
270"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6025878003696857,"efh = argmin
fh∈Fh X k∈[K]"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6044362292051756,"1
bσ2
h(sk
h, ak
h)"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6062846580406654,"
fh(sk
h, ak
h) −rk
h −bfh+1(sk
h+1)
2
."
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6081330868761553,"For this regression, it is crucial to obtain a reliable evaluation for the variance of the estimated
271"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.609981515711645,"cumulative reward rk
h + bfh+1(sk
h+1). According to the definition of Bellman operators T and T2, the
272"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6118299445471349,"variance of the function bf ′
h+1 for each state-action pair (s, a) can be denoted by
273"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6136783733826248,"[Varh bf ′
h+1](s, a) = T2,h bf ′
h+1(s, a) −

Th bf ′
h+1(s, a)
2
."
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6155268022181146,"To evaluate the first and second moment for the Bellman operator, we perform nonlinear least-squares
274"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6173752310536045,"regression on a separate dataset D′ with uniform weight (bσh(s, a) = 1 for all state-action pair (s, a)).
275"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6192236598890942,"For simplicity, we denote the empirical variance as Bh(s, a) = eg′
h(s, a) −

ef ′
h(s, a)
2
, and the
276"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6210720887245841,"difference between empirical variance Bh(s, a) with actually variance [Varh bf ′
h+1](s, a) is upper
277"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6229205175600739,"bound by
278"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6247689463955638,"Bh(s, a) −[Varh bf ′
h+1](s, a)
 ≤
egh(s, a) −T2,h bf ′
h+1(s, a)
 +


efh(s, a)
2
−

Th bf ′
h+1(s, a)
2 ."
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6266173752310537,"For these nonlinear function estimator, the following Lemmas provide coarse concentration properties
279"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6284658040665434,"for the first and second order Bellman operators.
280"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6303142329020333,"Lemma 6.1. Given a stage h ∈[H], let bf ′
h+1(·, ·) ≤H be the estimated value function constructed in
281"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6321626617375231,"Algorithm 1 Line 6. By utilizing Assumption 3.1, there exists a function ¯f ′
h ∈Fh, such that | ¯f ′
h(zh)−
282"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.634011090573013,"Th bf ′
h+1(zh)| ≤ϵ holds for all state-action pair zh = (sh, ah). Then with the probability of at least
283"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6358595194085028,"1 −δ/4H, it holds that P"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6377079482439926,"k∈[K]

¯f ′
h(¯zk
h) −ef ′
h(¯zk
h)
2
≤(β′
1,h)2, where β′
1,h = eO
 √log NNbH2
,
284"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6395563770794824,"and ef ′
h is the estimated function for first-moment Bellman operator (Line 3 in Algorithm 1).
285"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6414048059149723,"Lemma 6.2. Given a stage h ∈[H], let bf ′
h+1(·, ·) ≤H be the estimated value function constructed in
286"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6432532347504621,"Algorithm 1 Line 6. By utilizing Assumption 3.1, there exists a function ¯g′
h ∈Fh, such that |¯g′
h(zh)−
287"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6451016635859519,"T2,h bf ′
h+1(zh)| ≤ϵ holds for all state-action pair zh = (sh, ah). Then with the probability of at least
288"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6469500924214417,"1 −δ/4H, it holds that P"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6487985212569316,"k∈[K]
 
¯g′
h(¯zk
h) −eg′
h(¯zk
h)
2 ≤(β′
2,h)2, where β′
2,h = eO
 √log NNbH2
,
289"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6506469500924215,"and eg′
h is the estimated function for second-moment Bellman operator (Line 4 in Algorithm 1).
290"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6524953789279113,"Notice that all of the previous analysis focuses on the estimated function bf ′
h+1. By leveraging
291"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6543438077634011,"an induction procedure similar to existing works in the linear case (Jin et al., 2021b; Xiong
292"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6561922365988909,"et al., 2022), we can control the distance between the estimated function bf ′
h+1 and the optimal
293"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6580406654343808,"value function f ∗
h. In details, with high probability, for all stage h ∈[H], the distance is upper
294"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6598890942698706,"bounded by O
√log NNbH3/
√"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6617375231053605,"Kκ

. This result allows us to further bound [Varh bf ′
h+1](s, a) and
295"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6635859519408502,"[Varhf ∗
h+1](s, a).
296"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6654343807763401,"Therefore, the concentration properties in Lemmas 6.1 and 6.2 enable us to construct the pessimistic
297"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.66728280961183,"variance estimator, which satisfies the following property:
298"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6691312384473198,"[VhV ∗
h+1](s, a) −eO
√log NNbH3 √ Kκ"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6709796672828097,"
≤bσ2
h(s, a) ≤[VhV ∗
h+1](s, a).
(6.1)"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6728280961182994,"where [VhV ∗
h+1](s, a) = max{1, [VarhV ∗
h+1](s, a)} is the truncated conditional variance. Compared
299"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6746765249537893,"with the results in the linear function class, we utilize the logarithm of the covering number of the
300"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.6765249537892791,"function class as a substitute for the linear dimension d, which is a common technique in nonlinear
301"
VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS,0.678373382624769,"function approximation.
302"
REFERENCE-ADVANTAGE DECOMPOSITION,0.6802218114602587,"6.2
Reference-Advantage Decomposition
303"
REFERENCE-ADVANTAGE DECOMPOSITION,0.6820702402957486,"The reference-advantage decomposition is a powerful technique to tackle the challenge of additional
304"
REFERENCE-ADVANTAGE DECOMPOSITION,0.6839186691312384,"error from uniform concentration over whole function class Fh. Such an analysis approach has been
305"
REFERENCE-ADVANTAGE DECOMPOSITION,0.6857670979667283,"first studied in the online RL setting Azar et al. (2017); Zhang et al. (2021); Hu et al. (2022); He et al.
306"
REFERENCE-ADVANTAGE DECOMPOSITION,0.6876155268022182,"(2022); Agarwal et al. (2022) and later in the offline environment by Xiong et al. (2022).
307"
REFERENCE-ADVANTAGE DECOMPOSITION,0.6894639556377079,"For offline RL, in the context of nonlinear function classes, without a explicit linear expression,
308"
REFERENCE-ADVANTAGE DECOMPOSITION,0.6913123844731978,"the increased complexity of the function class structure poses a significant obstacle to effectively
309"
REFERENCE-ADVANTAGE DECOMPOSITION,0.6931608133086876,"utilizing this technique. Previous works, such as Yin et al. (2022b), have struggled to adapt the
310"
REFERENCE-ADVANTAGE DECOMPOSITION,0.6950092421441775,"reference-advantage decomposition to their nonlinear function class, resulting in a parameter space
311"
REFERENCE-ADVANTAGE DECOMPOSITION,0.6968576709796673,"dependence that scales with d, instead of the optimal
√"
REFERENCE-ADVANTAGE DECOMPOSITION,0.6987060998151571,"d. We provide detailed insights into this
312"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7005545286506469,"approach as follows:
313"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7024029574861368,"rh(s, a) + bfh+1(s, a) −Th bfh+1(s, a) = rh(s, a) + f ∗
h+1(s, a) −Thf ∗
h+1(s, a)
|
{z
}
Reference uncertainty"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7042513863216266,"+ bfh+1(s, a) −f ∗
h+1(s, a) −([Ph bfh+1](s, a) −[Phf ∗
h+1](s, a))
|
{z
}
Advantage uncertainty ."
REFERENCE-ADVANTAGE DECOMPOSITION,0.7060998151571165,"We decompose the Bellman error into two parts: the Reference uncertainty and the Advantage
314"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7079482439926063,"uncertainty. For the first term, the optimal value function f ∗
h+1 is fixed and not related to the pre-
315"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7097966728280961,"collected dataset, which circumvents additional uniform concentration over the whole function class
316"
REFERENCE-ADVANTAGE DECOMPOSITION,0.711645101663586,"and avoid any dependence on the function class size. For the second term, it is worth to notice that
317"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7134935304990758,"the distance between the estimated function bf ′
h+1 and the optimal value function f ∗
h is decreased as
318"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7153419593345656,"O(1/
√"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7171903881700554,"Kκ). Though, we still need to maintain the uniform convergence guarantee, the Advantage
319"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7190388170055453,"uncertainty is dominated by the Reference uncertainty when the number of episode K is large enough.
320"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7208872458410351,"By integrating these results, we can prove a variance-weighted concentration inequality for Bellman
321"
REFERENCE-ADVANTAGE DECOMPOSITION,0.722735674676525,"operators.
322"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7245841035120147,"Lemma 6.3. For each stage h ∈[H], assuming the variance estimator bσh satisfies (6.1), let
323"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7264325323475046,"bfh+1(·, ·) ≤H be the estimated value function constructed in Algorithm 1 Line 12. By utiliz-
324"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7282809611829945,"ing Assumption 3.1, there exist a function ¯fh ∈Fh, such that | ¯fh(zh) −Th bfh+1(zh)| ≤ϵ holds
325"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7301293900184843,"for all state-action pair zh = (sh, ah). Then with the probability of at least 1 −δ/4H, it holds that
326"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7319778188539742,"P
k∈[K]
1
(bσh(zk
h))2

¯fh(zk
h) −efh(zk
h)
2
≤(βh)2, where βh = eO(√log N) and efh is the estimated
327"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7338262476894639,"function from the weighted ridge regression (Line 10 in Algorithm 1).
328"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7356746765249538,"After controlling the Bellman error, with a similar argument to Jin et al. (2021b); Xiong et al. (2022),
329"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7375231053604436,"we obtain the following lemma, which provide an upper bound for the regret.
330"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7393715341959335,"Lemma 6.4 (Regret Decomposition Property). If |Th bfh+1(z) −efh(z)| ≤bh(z) holds for all stage
331"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7412199630314233,"h ∈[H] and state-action pair z = (s, a) ∈S × A, then the regret of Algorithm 1 can be bounded as
332"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7430683918669131,"V ∗
1 (s) −V bπ
1 (s) ≤2 H
X"
REFERENCE-ADVANTAGE DECOMPOSITION,0.744916820702403,"h=1
Eπ∗[bh (sh, ah) | s1 = s] ."
REFERENCE-ADVANTAGE DECOMPOSITION,0.7467652495378928,"Here, the expectation Eπ∗is with respect to the trajectory induced by π∗in the underlying MDP.
333"
REFERENCE-ADVANTAGE DECOMPOSITION,0.7486136783733827,"Combing the results in Lemmas 6.3 and 6.4, we have proved Theorem 5.1.
334"
CONCLUSION AND FUTURE WORK,0.7504621072088724,"7
Conclusion and Future Work
335"
CONCLUSION AND FUTURE WORK,0.7523105360443623,"In this paper, we present PNLSVI, an oracle-efficient algorithm for offline RL with non-linear function
336"
CONCLUSION AND FUTURE WORK,0.7541589648798521,"approximation. It achieves minimax optimal problem-dependent regret when specialized to linear
337"
CONCLUSION AND FUTURE WORK,0.756007393715342,"function approximation.
338"
CONCLUSION AND FUTURE WORK,0.7578558225508318,"Regarding future work, we observe that instead of using the uniform coverage assumption, a series of
339"
CONCLUSION AND FUTURE WORK,0.7597042513863216,"works, such as (Liu et al., 2020; Xie et al., 2021a; Uehara and Sun, 2021; Zhan et al., 2022), only
340"
CONCLUSION AND FUTURE WORK,0.7615526802218114,"relies on partial coverage assumption. In these works, the offline data distribution only encompasses
341"
CONCLUSION AND FUTURE WORK,0.7634011090573013,"the state-action distribution of a select high-quality comparator policy π∗. It would be of significant
342"
CONCLUSION AND FUTURE WORK,0.7652495378927912,"interest to investigate whether it’s possible to design practical algorithms for nonlinear function
343"
CONCLUSION AND FUTURE WORK,0.767097966728281,"classes under this weaker partial coverage assumption, while still preserving the inherent efficiency
344"
CONCLUSION AND FUTURE WORK,0.7689463955637708,"found in linear function approximation.
345"
REFERENCES,0.7707948243992606,"References
346"
REFERENCES,0.7726432532347505,"AGARWAL, A., JIN, Y. and ZHANG, T. (2022). Vo q l: Towards optimal regret in model-free rl with
347"
REFERENCES,0.7744916820702403,"nonlinear function approximation. arXiv preprint arXiv:2212.06069 .
348"
REFERENCES,0.7763401109057301,"AGARWAL, A. and ZHANG, T. (2022). Model-based rl with optimistic posterior sampling: Structural
349"
REFERENCES,0.7781885397412199,"conditions and sample complexity. arXiv preprint arXiv:2206.07659 .
350"
REFERENCES,0.7800369685767098,"AYOUB, A., JIA, Z., SZEPESVARI, C., WANG, M. and YANG, L. (2020). Model-based reinforcement
351"
REFERENCES,0.7818853974121996,"learning with value-targeted regression. In International Conference on Machine Learning. PMLR.
352"
REFERENCES,0.7837338262476895,"AZAR, M. G., OSBAND, I. and MUNOS, R. (2017). Minimax regret bounds for reinforcement
353"
REFERENCES,0.7855822550831792,"learning. In International Conference on Machine Learning. PMLR.
354"
REFERENCES,0.7874306839186691,"CHEN, J. and JIANG, N. (2019). Information-theoretic considerations in batch reinforcement learning.
355"
REFERENCES,0.789279112754159,"In International Conference on Machine Learning. PMLR.
356"
REFERENCES,0.7911275415896488,"DANN, C., JIANG, N., KRISHNAMURTHY, A., AGARWAL, A., LANGFORD, J. and SCHAPIRE,
357"
REFERENCES,0.7929759704251387,"R. E. (2018). On oracle-efficient pac rl with rich observations. Advances in neural information
358"
REFERENCES,0.7948243992606284,"processing systems 31.
359"
REFERENCES,0.7966728280961183,"DEGRAVE, J., FELICI, F., BUCHLI, J., NEUNERT, M., TRACEY, B., CARPANESE, F., EWALDS,
360"
REFERENCES,0.7985212569316081,"T., HAFNER, R., ABDOLMALEKI, A., DE LAS CASAS, D. ET AL. (2022). Magnetic control of
361"
REFERENCES,0.800369685767098,"tokamak plasmas through deep reinforcement learning. Nature 602 414–419.
362"
REFERENCES,0.8022181146025879,"DU, S., KAKADE, S., LEE, J., LOVETT, S., MAHAJAN, G., SUN, W. and WANG, R. (2021). Bilinear
363"
REFERENCES,0.8040665434380776,"classes: A structural framework for provable generalization in rl. In International Conference on
364"
REFERENCES,0.8059149722735675,"Machine Learning. PMLR.
365"
REFERENCES,0.8077634011090573,"DU, S. S., KAKADE, S. M., WANG, R. and YANG, L. F. (2019). Is a good representation sufficient
366"
REFERENCES,0.8096118299445472,"for sample efficient reinforcement learning? arXiv preprint arXiv:1910.03016 .
367"
REFERENCES,0.8114602587800369,"FOSTER, D. J., KAKADE, S. M., QIAN, J. and RAKHLIN, A. (2021). The statistical complexity of
368"
REFERENCES,0.8133086876155268,"interactive decision making. arXiv preprint arXiv:2112.13487 .
369"
REFERENCES,0.8151571164510166,"GENTILE, C., WANG, Z. and ZHANG, T. (2022). Achieving minimax rates in pool-based batch
370"
REFERENCES,0.8170055452865065,"active learning. In International Conference on Machine Learning. PMLR.
371"
REFERENCES,0.8188539741219963,"GU, S., HOLLY, E., LILLICRAP, T. and LEVINE, S. (2017). Deep reinforcement learning for robotic
372"
REFERENCES,0.8207024029574861,"manipulation with asynchronous off-policy updates. In 2017 IEEE international conference on
373"
REFERENCES,0.822550831792976,"robotics and automation (ICRA). IEEE.
374"
REFERENCES,0.8243992606284658,"HE, J., ZHAO, H., ZHOU, D. and GU, Q. (2022). Nearly minimax optimal reinforcement learning
375"
REFERENCES,0.8262476894639557,"for linear markov decision processes. arXiv preprint arXiv:2212.06132 .
376"
REFERENCES,0.8280961182994455,"HE, J., ZHOU, D. and GU, Q. (2021). Logarithmic regret for reinforcement learning with linear
377"
REFERENCES,0.8299445471349353,"function approximation. In International Conference on Machine Learning. PMLR.
378"
REFERENCES,0.8317929759704251,"HU, P., CHEN, Y. and HUANG, L. (2022). Nearly minimax optimal reinforcement learning with
379"
REFERENCES,0.833641404805915,"linear function approximation. In International Conference on Machine Learning. PMLR.
380"
REFERENCES,0.8354898336414048,"JIANG, N., KRISHNAMURTHY, A., AGARWAL, A., LANGFORD, J. and SCHAPIRE, R. E. (2017).
381"
REFERENCES,0.8373382624768947,"Contextual decision processes with low bellman rank are pac-learnable. In International Conference
382"
REFERENCES,0.8391866913123844,"on Machine Learning. PMLR.
383"
REFERENCES,0.8410351201478743,"JIN, C., LIU, Q. and MIRYOOSEFI, S. (2021a). Bellman eluder dimension: New rich classes of rl
384"
REFERENCES,0.8428835489833642,"problems, and sample-efficient algorithms. Advances in neural information processing systems 34
385"
REFERENCES,0.844731977818854,"13406–13418.
386"
REFERENCES,0.8465804066543438,"JIN, C., YANG, Z., WANG, Z. and JORDAN, M. I. (2020). Provably efficient reinforcement learning
387"
REFERENCES,0.8484288354898336,"with linear function approximation. In Conference on Learning Theory. PMLR.
388"
REFERENCES,0.8502772643253235,"JIN, Y., YANG, Z. and WANG, Z. (2021b). Is pessimism provably efficient for offline rl?
In
389"
REFERENCES,0.8521256931608133,"International Conference on Machine Learning. PMLR.
390"
REFERENCES,0.8539741219963032,"LEVINE, S., PASTOR, P., KRIZHEVSKY, A., IBARZ, J. and QUILLEN, D. (2018). Learning hand-
391"
REFERENCES,0.8558225508317929,"eye coordination for robotic grasping with deep learning and large-scale data collection. The
392"
REFERENCES,0.8576709796672828,"International journal of robotics research 37 421–436.
393"
REFERENCES,0.8595194085027726,"LI, G., SHI, L., CHEN, Y., CHI, Y. and WEI, Y. (2022). Settling the sample complexity of
394"
REFERENCES,0.8613678373382625,"model-based offline reinforcement learning. arXiv preprint arXiv:2204.05275 .
395"
REFERENCES,0.8632162661737524,"LIU, Y., SWAMINATHAN, A., AGARWAL, A. and BRUNSKILL, E. (2020). Provably good batch
396"
REFERENCES,0.8650646950092421,"off-policy reinforcement learning without great exploration. Advances in neural information
397"
REFERENCES,0.866913123844732,"processing systems 33 1264–1274.
398"
REFERENCES,0.8687615526802218,"MIN, Y., WANG, T., ZHOU, D. and GU, Q. (2021). Variance-aware off-policy evaluation with linear
399"
REFERENCES,0.8706099815157117,"function approximation. Advances in neural information processing systems 34 7598–7610.
400"
REFERENCES,0.8724584103512015,"MODI, A., JIANG, N., TEWARI, A. and SINGH, S. (2020). Sample complexity of reinforcement
401"
REFERENCES,0.8743068391866913,"learning using linearly combined model ensembles. In International Conference on Artificial
402"
REFERENCES,0.8761552680221811,"Intelligence and Statistics. PMLR.
403"
REFERENCES,0.878003696857671,"NGUYEN-TANG, T. and ARORA, R. (2023). Viper: Provably efficient algorithm for offline rl
404"
REFERENCES,0.8798521256931608,"with neural function approximation. In The Eleventh International Conference on Learning
405"
REFERENCES,0.8817005545286506,"Representations.
406"
REFERENCES,0.8835489833641405,"RASHIDINEJAD, P., ZHU, B., MA, C., JIAO, J. and RUSSELL, S. (2021). Bridging offline rein-
407"
REFERENCES,0.8853974121996303,"forcement learning and imitation learning: A tale of pessimism. Advances in Neural Information
408"
REFERENCES,0.8872458410351202,"Processing Systems 34 11702–11716.
409"
REFERENCES,0.88909426987061,"SCHRITTWIESER, J., ANTONOGLOU, I., HUBERT, T., SIMONYAN, K., SIFRE, L., SCHMITT, S.,
410"
REFERENCES,0.8909426987060998,"GUEZ, A., LOCKHART, E., HASSABIS, D., GRAEPEL, T. ET AL. (2020). Mastering atari, go,
411"
REFERENCES,0.8927911275415896,"chess and shogi by planning with a learned model. Nature 588 604–609.
412"
REFERENCES,0.8946395563770795,"SHI, L., LI, G., WEI, Y., CHEN, Y. and CHI, Y. (2022).
Pessimistic q-learning for offline
413"
REFERENCES,0.8964879852125693,"reinforcement learning: Towards optimal sample complexity. In International Conference on
414"
REFERENCES,0.8983364140480592,"Machine Learning. PMLR.
415"
REFERENCES,0.9001848428835489,"SILVER, D., SCHRITTWIESER, J., SIMONYAN, K., ANTONOGLOU, I., HUANG, A., GUEZ, A.,
416"
REFERENCES,0.9020332717190388,"HUBERT, T., BAKER, L., LAI, M., BOLTON, A. ET AL. (2017). Mastering the game of go without
417"
REFERENCES,0.9038817005545287,"human knowledge. nature 550 354–359.
418"
REFERENCES,0.9057301293900185,"SUN, W., JIANG, N., KRISHNAMURTHY, A., AGARWAL, A. and LANGFORD, J. (2019). Model-
419"
REFERENCES,0.9075785582255084,"based rl in contextual decision processes: Pac bounds and exponential improvements over model-
420"
REFERENCES,0.9094269870609981,"free approaches. In Conference on learning theory. PMLR.
421"
REFERENCES,0.911275415896488,"UEHARA, M. and SUN, W. (2021). Pessimistic model-based offline reinforcement learning under
422"
REFERENCES,0.9131238447319778,"partial coverage. arXiv preprint arXiv:2107.06226 .
423"
REFERENCES,0.9149722735674677,"WANG, R., FOSTER, D. P. and KAKADE, S. M. (2020a). What are the statistical limits of offline rl
424"
REFERENCES,0.9168207024029574,"with linear function approximation? arXiv preprint arXiv:2010.11895 .
425"
REFERENCES,0.9186691312384473,"WANG, R., SALAKHUTDINOV, R. R. and YANG, L. (2020b). Reinforcement learning with general
426"
REFERENCES,0.9205175600739371,"value function approximation: Provably efficient approach via bounded eluder dimension. Advances
427"
REFERENCES,0.922365988909427,"in Neural Information Processing Systems 33 6123–6135.
428"
REFERENCES,0.9242144177449169,"WANG, Y., WANG, R., DU, S. S. and KRISHNAMURTHY, A. (2020c). Optimism in reinforcement
429"
REFERENCES,0.9260628465804066,"learning with generalized linear function approximation. In International Conference on Learning
430"
REFERENCES,0.9279112754158965,"Representations.
431"
REFERENCES,0.9297597042513863,"WEISZ, G., AMORTILA, P. and SZEPESVÁRI, C. (2021). Exponential lower bounds for planning
432"
REFERENCES,0.9316081330868762,"in mdps with linearly-realizable optimal action-value functions. In Algorithmic Learning Theory.
433"
REFERENCES,0.933456561922366,"PMLR.
434"
REFERENCES,0.9353049907578558,"XIE, T., CHENG, C.-A., JIANG, N., MINEIRO, P. and AGARWAL, A. (2021a). Bellman-consistent
435"
REFERENCES,0.9371534195933456,"pessimism for offline reinforcement learning. Advances in neural information processing systems
436"
REFERENCES,0.9390018484288355,"34 6683–6694.
437"
REFERENCES,0.9408502772643254,"XIE, T., JIANG, N., WANG, H., XIONG, C. and BAI, Y. (2021b). Policy finetuning: Bridging sample-
438"
REFERENCES,0.9426987060998152,"efficient offline and online reinforcement learning. Advances in neural information processing
439"
REFERENCES,0.944547134935305,"systems 34 27395–27407.
440"
REFERENCES,0.9463955637707948,"XIONG, W., ZHONG, H., SHI, C., SHEN, C., WANG, L. and ZHANG, T. (2022). Nearly minimax
441"
REFERENCES,0.9482439926062847,"optimal offline reinforcement learning with linear function approximation: Single-agent mdp and
442"
REFERENCES,0.9500924214417745,"markov game. arXiv preprint arXiv:2205.15512 .
443"
REFERENCES,0.9519408502772643,"YANG, L. and WANG, M. (2019). Sample-optimal parametric q-learning using linearly additive
444"
REFERENCES,0.9537892791127541,"features. In International Conference on Machine Learning.
445"
REFERENCES,0.955637707948244,"YANG, L. and WANG, M. (2020). Reinforcement learning in feature space: Matrix bandit, kernels,
446"
REFERENCES,0.9574861367837338,"and regret bound. In International Conference on Machine Learning. PMLR.
447"
REFERENCES,0.9593345656192237,"YIN, M., DUAN, Y., WANG, M. and WANG, Y.-X. (2022a). Near-optimal offline reinforcement
448"
REFERENCES,0.9611829944547134,"learning with linear representation: Leveraging variance information with pessimism. arXiv
449"
REFERENCES,0.9630314232902033,"preprint arXiv:2203.05804 .
450"
REFERENCES,0.9648798521256932,"YIN, M., WANG, M. and WANG, Y.-X. (2022b). Offline reinforcement learning with differentiable
451"
REFERENCES,0.966728280961183,"function approximation is provably efficient. arXiv preprint arXiv:2210.00750 .
452"
REFERENCES,0.9685767097966729,"YIN, M. and WANG, Y.-X. (2021). Towards instance-optimal offline reinforcement learning with
453"
REFERENCES,0.9704251386321626,"pessimism. Advances in neural information processing systems 34 4065–4078.
454"
REFERENCES,0.9722735674676525,"ZANETTE, A., BRANDFONBRENER, D., BRUNSKILL, E., PIROTTA, M. and LAZARIC, A. (2020a).
455"
REFERENCES,0.9741219963031423,"Frequentist regret bounds for randomized least-squares value iteration. In International Conference
456"
REFERENCES,0.9759704251386322,"on Artificial Intelligence and Statistics. PMLR.
457"
REFERENCES,0.977818853974122,"ZANETTE, A., LAZARIC, A., KOCHENDERFER, M. and BRUNSKILL, E. (2020b). Learning near
458"
REFERENCES,0.9796672828096118,"optimal policies with low inherent bellman error. In International Conference on Machine Learning.
459"
REFERENCES,0.9815157116451017,"PMLR.
460"
REFERENCES,0.9833641404805915,"ZANETTE, A., WAINWRIGHT, M. J. and BRUNSKILL, E. (2021). Provable benefits of actor-critic
461"
REFERENCES,0.9852125693160814,"methods for offline reinforcement learning. Advances in neural information processing systems 34
462"
REFERENCES,0.9870609981515711,"13626–13640.
463"
REFERENCES,0.988909426987061,"ZHAN, W., HUANG, B., HUANG, A., JIANG, N. and LEE, J. (2022). Offline reinforcement learning
464"
REFERENCES,0.9907578558225508,"with realizability and single-policy concentrability. In Conference on Learning Theory. PMLR.
465"
REFERENCES,0.9926062846580407,"ZHANG, Z., JI, X. and DU, S. (2021). Is reinforcement learning more difficult than bandits? a
466"
REFERENCES,0.9944547134935305,"near-optimal algorithm escaping the curse of horizon. In Conference on Learning Theory. PMLR.
467"
REFERENCES,0.9963031423290203,"ZHOU, D., GU, Q. and SZEPESVARI, C. (2021). Nearly minimax optimal reinforcement learning for
468"
REFERENCES,0.9981515711645101,"linear mixture markov decision processes. In Conference on Learning Theory. PMLR.
469"
