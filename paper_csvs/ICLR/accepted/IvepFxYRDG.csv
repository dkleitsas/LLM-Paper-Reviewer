Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0006711409395973154,"Two-player zero-sum Markov game is a fundamental problem in reinforcement
learning and game theory. Although many algorithms have been proposed for
solving zero-sum Markov games in the existing literature, many of them either
require a full knowledge of the environment or are not sample-efﬁcient. In this
paper, we develop a fully decentralized and sample-efﬁcient stochastic policy
extragradient algorithm for solving tabular zero-sum Markov games. In particular,
our algorithm utilizes multiple stochastic estimators to accurately estimate the value
functions involved in the stochastic updates, and leverages entropy regularization
to accelerate the convergence. Speciﬁcally, with a proper entropy-regularization
parameter, we prove that the stochastic policy extragradient algorithm has a sample
complexity of the order e
O(
Amax
µminϵ5.5(1−γ)13.5 ) for ﬁnding a solution that achieves
ϵ-Nash equilibrium duality gap, where Amax is the maximum number of actions
between the players, µmin is the lower bound of state stationary distribution, and γ
is the discount factor. Such a sample complexity result substantially improves the
state-of-the-art complexity result."
INTRODUCTION,0.0013422818791946308,"1
INTRODUCTION"
INTRODUCTION,0.0020134228187919465,"Competitive reinforcement learning (RL) is an emerging and popular framework that has broad
applications in various areas, including market pricing applications (Kononen and Oja, 2004), real-
time strategy-making (Vinyals et al., 2019), board games (Silver et al., 2017; Moerland et al., 2018)
and inverse RL (Zhang et al., 2019). In particular, an important and fundamental formulation of
competitive RL is the two-player zero-sum Markov game, which involves two competing players that
interact with a common environment and receive zero-sum rewards. Both players aim to learn the
optimal policy that achieves the Nash equilibrium of accumulated rewards."
INTRODUCTION,0.0026845637583892616,"Algorithms for solving Markov games are very different from conventional single-agent RL algo-
rithms. In particular, both players must learn to improve their policies based on feedback from the
opponent and the environment, but usually the opponent will not reveal any sensitive information
(e.g., actions or policy) or cooperate with each other. In the existing literature, numerous algorithms
have been developed for solving zero-sum Markov games, including Q-learning (Fan et al., 2020; Zhu
and Zhao, 2020), ﬁtted Q iteration (Zhang et al., 2021), policy gradient (Daskalakis et al., 2020; Zhao
et al., 2021), policy extragradient (Cen et al., 2021), model-based Monte Carlo estimation (Zhang
et al., 2020), optimistic gradient descent ascent (Wei et al., 2021), etc. However, many of these
algorithms require both players to access their opponent’s actions (Wei et al., 2017; Sidford et al.,
2020; Huang et al., 2021; Jafarnia-Jahromi et al., 2021), which causes privacy issues. On the other
hand, some algorithms need to know about the environment transition kernel and reward mapping
(Cen et al., 2021), which are usually unknown a priori in practice. Moreover, other algorithms require
both players to perform asymmetric policy updates using different numbers of iterations, learning
rates or exploration probabilities (Zhao et al., 2021; Daskalakis et al., 2020), which are generally"
INTRODUCTION,0.003355704697986577,Published as a conference paper at ICLR 2022
INTRODUCTION,0.004026845637583893,"hard to coordinate in advance between two competing players. Therefore, it is desired to develop an
algorithm for solving Markov games that avoids all the aforementioned issues."
INTRODUCTION,0.004697986577181208,"Moreover, from a theoretical perspective, the convergence and sample complexity of these existing
algorithms for solving Markov games have not been comprehensively studied. Speciﬁcally, some
studies established the convergence of the algorithms with i.i.d. samples (Zhao et al., 2021; Guo
et al., 2021), which violates the dependent nature of samples collected from the dynamic Markov
decision process. Also, other algorithms suffer from an extremely high sample complexity to achieve
an approximate Nash equilibrium solution (Wei et al., 2021). Hence, we are motivated to answer the
following fundamental question."
INTRODUCTION,0.005369127516778523,"• Q: Can we develop a fully decentralized algorithm that is model-free and takes symmetric and
private policy updates for solving zero-sum Markov games, with provable convergence guarantee
and an improved sample complexity for achieving a Nash equilibrium solution?"
INTRODUCTION,0.006040268456375839,"In this paper, we provide positive and comprehensive answers to this question by developing a fully
decentralized stochastic policy extragradient algorithm. In Table 1 of Appendix F, we compare the
key properties and sample complexity of our algorithm with those of the existing algorithms."
OUR CONTRIBUTIONS,0.006711409395973154,"1.1
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.00738255033557047,"We consider a standard zero-sum Markov game with discounted reward over inﬁnite horizon. To
solve such a Markov game, we propose a stochastic variant of the policy extragradient algorithm
(Cen et al., 2021) that satisﬁes the following amenable properties."
OUR CONTRIBUTIONS,0.008053691275167786,"• Our algorithm uses multiple stochastic estimators to estimate the value functions involved in the
predictive updates for solving entropy-regularized matrix games, and therefore the algorithm does
not rely on any prior knowledge of the environment transition kernel (model-free). Moreover, the
resulting stochastic policy updates of our algorithm for both players are symmetric and do not
involve any sensitive information of the opponent (private)."
OUR CONTRIBUTIONS,0.0087248322147651,"• Compared with the stochastic estimators used in (Wei et al., 2021), our estimators have much
smaller variance that helps improve the estimation accuracy. Speciﬁcally, by developing new
techniques (explained in the next bullet), we establish tight high-probability estimation error
bounds for our stochastic estimators with Markovian samples. Then, with a proper entropy-
regularization parameter, we show that our stochastic policy extragradient algorithm requires a
sample complexity of the order O(
Amax
µminϵ5.5(1−γ)13.5 ) to achieve an ϵ-Nash equilibrium duality gap,
which substantially improves the state-of-the-art complexity result of (Wei et al., 2021)."
OUR CONTRIBUTIONS,0.009395973154362415,"• We develop novel techniques to bound the estimation error of the proposed stochastic estimators,
whose numerator and denominator involve sample average approximations. First, we propose a
special estimation error decomposition that avoids divergence of the bound caused by possibly small
numerical values of the sample average involved in the denominator of the stochastic estimators.
Second, we leverage this error decomposition and the recursive structure of the stochastic estimators
to derive a contraction property of the estimation errors, which eventually leads to tight bounds for
the estimation error. We refer to Section 4 for more elaboration on our technical novelties."
OTHER RELATED WORK,0.010067114093959731,"1.2
OTHER RELATED WORK"
OTHER RELATED WORK,0.010738255033557046,"Other settings of two-player zero-sum Markov games. In this paper, we focus on a standard
setting of two-player zero-sum Markov game with discount and inﬁnite horizon in the discrete time
domain. There are other settings of two-player zero-sum Markov games. For example, Bai et al.
(2020); Huang et al. (2021) studied a two-player zero-sum Markov game with ﬁnite horizon and
without discount, whereas Daskalakis et al. (2020) considered ﬁnite random horizon without discount.
Jafarnia-Jahromi et al. (2021) also considered the setting without discount, and it allows one of the
players to constantly adjust its policy based on the entire history of states and actions. Ghosh et al.
(2021) studied a two-player zero-sum Markov game in the continuous time domain."
OTHER RELATED WORK,0.011409395973154362,"Multi-agent general-sum Markov game. Some works studied multi-agent Markov games, which
extend the two-player zero-sum Markov games to multiple players without the zero-sum constraint
(Wang and Sandholm, 2002; Hu and Wellman, 2003; Deng et al., 2021; Leonardos et al., 2021). More
speciﬁcally, Leonardos et al. (2021) deﬁned and studied Markov potential game which has a potential"
OTHER RELATED WORK,0.012080536912751677,Published as a conference paper at ICLR 2022
OTHER RELATED WORK,0.012751677852348993,"function assumption. Guo et al. (2019); Elie et al. (2020); Gu et al. (2021) studied mean-ﬁeld games
with a large number of players."
OTHER RELATED WORK,0.013422818791946308,"Entropy regularization and value iteration Our algorithm leverages entropy regularization and
value iteration to accelerate convergence. Entropy regularization is a popular technique that has been
widely used in reinforcement learning (Neu et al., 2017; Geist et al., 2019; Mei et al., 2020; Cen
et al., 2020) and Markov game (Mertikopoulos and Sandholm, 2016; Savas et al., 2019; Cen et al.,
2021) to encourage environment exploration and accelerate algorithm convergence. Value iteration is
also a classical method that is widely used in both single-agent reinforcement learning (Ernst et al.,
2005; Tamar et al., 2016; Farahmand and Ghavamzadeh, 2021) and Markov games (Zhu and Zhao,
2020; Cen et al., 2021). With full knowledge of the environment, it exponentially converges to the
ﬁxed point of Bellman operator (Cen et al., 2021). Compared to another similar classical method
called policy iteration, value iteration does not need policy evaluation which involves additional
computation (Sutton and Barto, 2018)."
BACKGROUND OF MARKOV GAME AND ENTROPY REGULARIZATION,0.014093959731543624,"2
BACKGROUND OF MARKOV GAME AND ENTROPY REGULARIZATION"
TWO-PLAYER ZERO-SUM MARKOV GAME,0.01476510067114094,"2.1
TWO-PLAYER ZERO-SUM MARKOV GAME"
TWO-PLAYER ZERO-SUM MARKOV GAME,0.015436241610738255,"In a zero-sum Markov game, two players compete with each other in a common environment.
Throughout, the state space is denoted as S. The action spaces and policies of both players are
denoted as A(1), π(1) and A(2), π(2), respectively. Here, π(1) ∈∆(|A(1)|), π(2) ∈∆(|A(2)|) are
random policies deﬁned over the corresponding simplex sets. The reward function is denoted as
R : S × A(1) × A(2) →[0, 1], and the discount factor is denoted as γ ∈(0, 1)."
TWO-PLAYER ZERO-SUM MARKOV GAME,0.016107382550335572,"At any time t, both players observe state st ∈S of the environment. Then, both players respectively
select their actions following their own policies, i.e., a(1)
t
∼π(1)(·|st) and a(2)
t
∼π(2)(·|st). After
that, the environment state transfers to a new state st+1 following the underlying transition kernel
P(·|st, a(1)
t , a(2)
t ), and both players receive zero-sum rewards, i.e., R(1)
t
= −R(2)
t
= Rt, where
Rt := R(st, a(1)
t , a(2)
t ). With this Markov decision process, we can deﬁne the following state value
function associated with the players’ policies π(1) and π(2) for any environment state s ∈S."
TWO-PLAYER ZERO-SUM MARKOV GAME,0.016778523489932886,"Vπ(1),π(2)(s) = E
h ∞
X"
TWO-PLAYER ZERO-SUM MARKOV GAME,0.0174496644295302,"t=0
γtRt
s0 = s
i
.
(1)"
TWO-PLAYER ZERO-SUM MARKOV GAME,0.018120805369127517,The goal of both players is to compete via the following minimax game in all states s.
TWO-PLAYER ZERO-SUM MARKOV GAME,0.01879194630872483,"min
π(2) max
π(1) Vπ(1),π(2)(s).
(2)"
TWO-PLAYER ZERO-SUM MARKOV GAME,0.019463087248322148,"In particular, it has been shown in (Shapley, 1953) that there exists a Nash equilibrium policy pair
π(1)
∗, π(2)
∗
for zero-sum Markov games, i.e., Vπ(1),π(2)
∗(s) ≤Vπ(1)
∗
,π(2)
∗(s) ≤Vπ(1)
∗
,π(2)(s) holds for"
TWO-PLAYER ZERO-SUM MARKOV GAME,0.020134228187919462,"any other policies π(1), π(2) and for all states s."
ENTROPY-REGULARIZED MARKOV GAME,0.02080536912751678,"2.2
ENTROPY-REGULARIZED MARKOV GAME"
ENTROPY-REGULARIZED MARKOV GAME,0.021476510067114093,"Entropy regularization is a popular technique that has been widely used in reinforcement learning (Neu
et al., 2017; Geist et al., 2019; Mei et al., 2020; Cen et al., 2020) and Markov game (Mertikopoulos
and Sandholm, 2016; Savas et al., 2019; Cen et al., 2021) to encourage environment exploration and
accelerate algorithm convergence."
ENTROPY-REGULARIZED MARKOV GAME,0.02214765100671141,"Speciﬁcally, for the zero-sum Markov game, we can deﬁne an entropy-regularized state value function
by adding entropy regularization to the state value function in (1) as follows (Cen et al., 2021)."
ENTROPY-REGULARIZED MARKOV GAME,0.022818791946308724,"V (τ)
π(1),π(2)(s) :=E
h ∞
X"
ENTROPY-REGULARIZED MARKOV GAME,0.02348993288590604,"t=0
γt
Rt −τ ln π(1)(a(1)
t |st) + τ ln π(2)(a(2)
t |st)
s0 = s
i
,
(3)"
ENTROPY-REGULARIZED MARKOV GAME,0.024161073825503355,"where τ > 0 is called the regularization parameter. With the above deﬁnition, we further deﬁne the
following entropy-regularized state-action value function (also called Q-function) (Cen et al., 2021)."
ENTROPY-REGULARIZED MARKOV GAME,0.024832214765100672,"Q(τ)
π(1),π(2)(s, a(1), a(2)) :=R(s, a(1), a(2)) + γEs′∼P(·|s,a(1),a(2))

V (τ)
π(1),π(2)(s′)

.
(4)"
ENTROPY-REGULARIZED MARKOV GAME,0.025503355704697986,Published as a conference paper at ICLR 2022
ENTROPY-REGULARIZED MARKOV GAME,0.026174496644295303,"In particular, V (τ)
π(1),π(2) can be obtained from Q(τ)
π(1),π(2) as follows."
ENTROPY-REGULARIZED MARKOV GAME,0.026845637583892617,"V (τ)
π(1),π(2)(s) =[π(1)(s)]⊤Q(τ)
π(1),π(2)(s)π(2)(s) + τH
 
π(1)(s)

−τH
 
π(2)(s)
"
ENTROPY-REGULARIZED MARKOV GAME,0.027516778523489934,":=fτ
 
Q(τ)
π(1),π(2)(s), π(1)(s), π(2)(s)

,
(5)"
ENTROPY-REGULARIZED MARKOV GAME,0.028187919463087248,"where H(π) denotes the entropy of policy π, and we deﬁne this mapping as fτ for convenience."
ENTROPY-REGULARIZED MARKOV GAME,0.028859060402684565,"For the entropy regularized Markov game, it has an equilibrium policy pair that solves the minimax
optimization problem minπ(2) maxπ(1) V (τ)
π(1),π(2)(s). Such a policy pair is called the quantal response
equilibrium (QRE). Our goal is to ﬁnd the equilibrium policy pair of the original Markov game in
(2) by solving the entropy-regularized Markov game with a proper regularization parameter τ. In
particular, compared with the equilibrium policy of the Markov game, the QRE tends to have a larger
entropy due to the entropy regularization, which encourages the players to explore and obtain a
better understanding of the environment. Another advantage of considering the entropy-regularized
Markov game is that the entropy regularization makes the minimax problem have a better optimization
geometry that accelerates the convergence of the optimization process."
"STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM FOR
ENTROPY-REGULARIZED MARKOV GAME",0.02953020134228188,"3
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM FOR
ENTROPY-REGULARIZED MARKOV GAME"
"STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM FOR
ENTROPY-REGULARIZED MARKOV GAME",0.030201342281879196,"In this section, we develop a stochastic policy extragradient (SPE) algorithm for solving entropy-
regularized Markov games. First, we recap the policy extragradient (PE) algorithm, which is
introduced in (Cen et al., 2021) to solve entropy-regularized Markov games with full knowledge
of the environment transition kernel and reward mapping. Then, we propose the model-free SPE
algorithm that solves entropy-regularized Markov games using only stochastic samples."
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.03087248322147651,"3.1
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.03154362416107383,"Value iteration is a classical reinforcement learning algorithm that requires full knowledge of the
environment and achieves an exponential convergence rate. In particular, for the entropy-regularized
Markov game, the k-th value iteration update is deﬁned as follows."
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.032214765100671144,"Qk(s, a(1), a(2)) =R(s, a(1), a(2)) + γEs′∼P(·|s,a(1),a(2))

Vk(s′)

,
∀s, a(1), a(2),
(6)"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.032885906040268455,"Vk+1(s) = min
π(2)(s) max
π(1)(s) fτ
 
Qk(s); π(1)(s), π(2)(s)

,
∀s,
(7)"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.03355704697986577,"where we deﬁne Qk(s) := Qk(s, ·, ·) ∈R|A(1)|×|A(2)|. This algorithm alternatively updates all the
entries of the value functions Qk and Vk. Thanks to the entropy regularization in the function fτ (see
(5) for the deﬁnition), the minimax matrix game in (7) is τ-strongly concave in π(1) and τ-strongly
convex in π(2), and therefore it has a unique solution."
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.03422818791946309,"To solve the entropy-regularized minimax matrix game in (7), Cen et al. (2021) proposed a predictive
update (PU) algorithm. Speciﬁcally, with uniform policy initialization, i.e., π(m)
k,0 (s) =
1
|A(m)|, ∀m ∈
{1, 2}, ∀s ∈S, the PU algorithm performs the following policy updates: for t = 0, 1, 2, ... (PU):"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.0348993288590604,"






"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.03557046979865772,"





"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.036241610738255034,"π(1)
k,t+1(a(1)|s) ∝π(1)
k,t(a(1)|s)1−ητ exp
 
ηQ(1)
k,t(s, a(1))
"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.03691275167785235,"π(2)
k,t+1(a(2)|s) ∝π(2)
k,t(a(2)|s)1−ητ exp
 
−ηQ(2)
k,t(s, a(2))
"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.03758389261744966,"π(1)
k,t+1(a(1)|s) ∝π(1)
k,t(a(1)|s)1−ητ exp
 
ηQ
(1)
k,t+1(s, a(1))
"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.03825503355704698,"π(2)
k,t+1(a(2)|s) ∝π(2)
k,t(a(2)|s)1−ητ exp
 
−ηQ
(2)
k,t+1(s, a(2))
 ,
(8)"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.038926174496644296,where we use the following notations (superscript (\m) denotes the opponent of the m-th player.).
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.03959731543624161,"Q(m)
k,t (s, a(m)) := Ea(\m)∼π(\m)
k,t
(s)

Qk(s, a(1), a(2))

,
m ∈{1, 2}
(9)"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.040268456375838924,"Q
(m)
k,t+1(s, a(m)) := Ea(\m)∼π(\m)
k,t+1(s)

Qk(s, a(1), a(2))

,
m ∈{1, 2}.
(10)"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.04093959731543624,Published as a conference paper at ICLR 2022
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.04161073825503356,"Once we obtain the output policy pair (π(1)
k , π(2)
k ) of the PU algorithm, we can obtain an approxi-
mation of Vk+1(s) as V ′
k+1(s) = fτ
 
Qk(s); π(1)
k (s), π(2)
k (s)

, which will be further used in the next
Q-value function update (6) to replace Vk+1(s′). The updates (6), (7) & (8) are referred to as policy
extragradient (PE) algorithm."
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.042281879194630875,"In the PE algorithm, the PU update in (8) allows both players to take symmetric updates without
revealing their private actions, and has been shown to converge to the unique solution of the entropy-
regularized matrix game (7) exponentially fast (Cen et al., 2021). However, the PE algorithm has
several limitations. First, in the PU update, each player m ∈{1, 2} needs to query the quantities"
REVIEW OF POLICY EXTRAGRADIENT ALGORITHM,0.042953020134228186,"Q(m)
k,t (s, a(m)), Q
(m)
k,t (s, a(m)) from its opponent. To compute these quantities, the opponent needs to
multiply the entire Q-table by its own policy vector. This requires both players to coordinate with
each other and share a Q-table. Second, the update of the Q-table in (6) requires full knowledge of the
environment transition kernel P and the reward mapping R, which are unknown a priori in practice.
To overcome these limitations, we develop a fully stochastic PE algorithm in the next subsection."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.0436241610738255,"3.2
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.04429530201342282,"The major challenge of the PE algorithm is computing the quantities Q(m)
k,t , Q
(m)
k,t and V ′
k+1, which
requires coordinating with the opponent and involves the environment information. Here, we develop
a model-free and fully stochastic variant of PE that estimates these key quantities using Markovian
stochastic samples. We refer to this algorithm as stochastic policy extragradient (SPE)."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.04496644295302014,"Speciﬁcally, we ﬁrst estimate the quantity V ′
k+1(s) = fτ
 
Qk(s); π(1)
k (s), π(2)
k (s)

. By deﬁnition of
fτ in (5) and the update of Qk in (6) (now we use V ′
k(s′) instead of Vk(s′)) and using some standard
tricks on random variables (see Lemma 2 in Appendix B for a full proof), we can rewrite V ′
k+1(s) as"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.04563758389261745,"V ′
k+1(s) =
E
h 
R(es, ea(1), ea(2)) + γV ′
k(s′)

1{es = s}
i"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.046308724832214765,"µk(s)
+ τH
 
π(1)
k (s)

−τH
 
π(2)
k (s)

,
(11)"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.04697986577181208,"where µk(s) denotes the stationary state distribution associated with the policy pair (π(1)
k , π(2)
k ), and
the expectation is taken over es ∼µk, ea(1) ∼π(1)
k (s), ea(2) ∼π(2)
k (s), s′ ∼P(·|es, ea(1), ea(2)). To
estimate this quantity, we query a set Nk+1 (with cardinality Nk+1) of samples from the Markov
decision process following the pair of policies (π(1)
k , π(2)
k ). Then, we estimate V ′
k+1(s) as"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.0476510067114094,bVk+1(s) =
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.04832214765100671,"1
Nk+1
P"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.04899328859060403,"i∈Nk+1
 
Ri + γ bVk(si+1)

1{si = s}"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.049664429530201344,"1
Nk+1
P"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.050335570469798654,"i∈Nk+1 1{si = s}
+ τH
 
π(1)
k (s)

−τH
 
π(2)
k (s)

.
(12)"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.05100671140939597,"Intuitively, we use the sample average of Markovian samples to estimate the expectation terms in
(11). Thanks to the concentration phenomenon of dependent samples (Paulin, 2015), these sample
averages converge to the desired expected values provided that the sample size is sufﬁciently large."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.05167785234899329,"Next, we estimate Q(m)
k,t , m ∈1, 2. Leveraging (9) and (6), we obtain the following equivalent
characterization for both players m ∈1, 2 (see Lemma 2 in Appendix B for the proof of equivalence)."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.052348993288590606,"Q(m)
k,t (s, a(m)) =
E
h 
R(es, ea(1), ea(2)) + γV ′
k(s′)

1{es = s, ea(m) = a(m)}
i"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.053020134228187916,"µk,t(s)π(m)
k,t (a(m)|s)
,
(13)"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.053691275167785234,"where 1{·} denotes the indicator function, µk,t denotes the stationary state distribution associated
with the policy pair (π(1)
k,t, π(2)
k,t), and the expectation is taken over es ∼µk,t, ea(1) ∼π(1)
k,t(s), ea(2) ∼"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.05436241610738255,"π(2)
k,t(s), s′ ∼P(·|es, ea(1), ea(2)). To estimate this quantity, we query a set Nk,t (with cardinality Nk,t)"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.05503355704697987,"of samples from the Markov decision process following a pair of smoothed policies π′(m)
k,t (s) ="
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.05570469798657718,"(1 −ϵ′)π(m)
k,t (s) +
ϵ′"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.056375838926174496,"|A(m)|1, where ϵ′ ∈[0, 1] is a small smoothing constant that will be theoretically"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.05704697986577181,"determined later. Then, we estimate Q(m)
k,t (s, a(m)) as follows."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.05771812080536913,"bQ(m)
k,t (s, a(m)) ="
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.05838926174496644,"1
Nk,t
P"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.05906040268456376,"i∈Nk,t
 
Ri + γ bVk(si+1)

1{si = s, a(m)
i
= a(m)}
 
1
Nk,t
P"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.059731543624161075,"i∈Nk,t 1{si = s}

π′(m)
k,t (a(m)|s)
,
(14)"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06040268456375839,Published as a conference paper at ICLR 2022
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.0610738255033557,"where we have replaced the expectations with sample averages, and replaced V ′
k(s′) with bVk(s′).
Here, the Markovian samples are queried following the ϵ′-smoothed policies (π′(1)
k,t , π′(2)
k,t ). On one
hand, ϵ′ should not be too small so that it keeps the denominator of the above estimation away from
zero. On the other hand, ϵ′ should not be too large so that it is sufﬁciently close to the original policy."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06174496644295302,"Similarly, to estimate Q
(m)
k,t , we query another set N k,t (with cardinality N k,t) of samples from the"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06241610738255034,"Markov decision process following a pair of smoothed policies π′(m)
k,t (s) = (1−ϵ′)π(m)
k,t (s)+
ϵ′"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06308724832214765,"|A(m)|1,"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06375838926174497,"where ϵ′ ∈[0, 1] will be theoretically determined later. Then, we estimate Q
(m)
k,t as follows."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06442953020134229,"bQ
(m)"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06510067114093959,"k,t (s, a(m)) ="
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06577181208053691,"1
Nk,t
P"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06644295302013423,"i∈N k,t
 
Ri + γ bVk(si+1)

1{si = s, a(m)
i
= a(m)}
 
1
N k,t
P"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06711409395973154,"i∈N k,t 1{si = s}
"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06778523489932886,"π′(m)
k,t (a(m)|s)
.
(15)"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.06845637583892618,"Remark 1. The estimators (12), (14) & (15) essentially use ratio of sample averages to approximate
ratio of expectations. Estimators with similar structure have been used in (Ortner and Auer, 2007;
Xia et al., 2016; Wei et al., 2021). However, these works analyze the estimation error of their
estimators with independence samples. As a comparison, our analysis of the estimation error bounds
the additional bias induced by the correlated Markovian samples, and achieves an improved sample
complexity in the main Theorem 2."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.0691275167785235,"We summarize our stochastic policy extragradient (SPE) algorithm in Algorithm 1. Speciﬁcally, in
SPE, we estimate the quantities Q(m)
k,t , Q
(m)
k,t , V ′
k using their corresponding stochastic estimators. As
a result, the SPE algorithm is model-free, and the updates for both players are symmetric and private."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.0697986577181208,"Algorithm 1 Stochastic policy extragradient (SPE) for entropy-regularized Markov game
Initialize: V ′
0(s) for all s ∈S.
for value iterations k = 0, 1, . . . , K −1 do"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.07046979865771812,"Initialize π(1)
k,0, π(2)
k,0 with uniform distribution.
for PU iterations t = 0, 1, . . . , Tk −1 do"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.07114093959731543,"Players 1,2 sample Nk,t Markovian samples following smoothed policies π′(1)
k,t , π′(2)
k,t ."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.07181208053691275,"Every player m ∈{1, 2} computes bQ(m)
k,t (s, a(m)) for all s, a(m) using (14)."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.07248322147651007,"Players 1,2 sample N k,t Markovian samples following smoothed policies π′(1)
k,t , π′(2)
k,t ."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.07315436241610739,"Every player m ∈{1, 2} computes bQ
(m)"
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.0738255033557047,"k,t (s, a(m)) for all s, a(m) using (15).
Implement the t-th PU iteration for all s, a(1), a(2) using (8) with estimations (14)&(15).
end
Let π(m)
k
= π(m)
k,Tk, m = 1, 2. Players sample Nk Markovian samples following π(1)
k , π(2)
k ."
STOCHASTIC POLICY EXTRAGRADIENT ALGORITHM,0.07449664429530202,"Compute bVk+1(s) for all s using (12).
end
Output: π(1)
K−1, π(2)
K−1."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.07516778523489932,"4
FINITE-TIME CONVERGENCE ANALYSIS OF SPE"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.07583892617449664,"Throughout our convergence analysis, we adopt the following two standard assumptions."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.07651006711409396,"Assumption 1. Denote Tπ(1),π(2)(s, s′) := inf{t ≥1 : st = s′|s0 = s} as the ﬁrst-visit time under
the policy pair π(1), π(2). We assume that"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.07718120805369127,"sup
s,s′∈S
sup
π(1),π(2) Eπ(1),π(2)

Tπ(1),π(2)(s, s′)

< +∞.
(16)"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.07785234899328859,"Assumption 1 is widely used in the reinforcement learning literature (Ortner and Auer, 2007; Ortner,
2020; Wei et al., 2021; Jafarnia-Jahromi et al., 2021). It ensures that every state will be visited at least
once within a ﬁnite duration of time, thus ensuring that all the states will be visited inﬁnitely often."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.07852348993288591,Published as a conference paper at ICLR 2022
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.07919463087248323,"This guarantees sufﬁcient exploration. In our analysis, we use the following equivalent statement
of Assumption 1 for convenience, which means that the stationary state distribution µπ(1),π(2) has a
uniform lower bound µmin > 0. Their equivalence is based on Theorem 5.5.11 of (Durrett, 2019)."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.07986577181208054,"µmin := inf
s∈S
inf
π(1),π(2) µπ(1),π(2)(s) =
h
sup
s∈S
sup
π(1),π(2) Eπ(1),π(2)Tπ(1),π(2)(s, s)
i−1
> 0.
(17)"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08053691275167785,"Assumption 2. There exists a mixing time tmix ∈N such that for any policy pair π(1), π(2) and its
corresponding stationary state distribution µπ(1),π(2), we have"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08120805369127516,"dTV
 
Pπ(1),π(2)(stmix), µπ(1),π(2)

≤1 4."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08187919463087248,"where Pπ(1),π(2)(stmix) denotes the state distribution under the policy pair π(1), π(2) at time tmix, and
dTV denotes the total variation distance between two probability distributions."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.0825503355704698,"In this subsection, we analyze the ﬁnite-time convergence of Algorithm 1 for solving the entropy-
regularized Markov game (5). We focus on the convergence rate of the following Nash equilibrium
duality gap, which is a standard optimality metric widely adopted in the existing literature (Xu et al.,
2020; Jin and Sidford, 2021; Wei et al., 2021)."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08322147651006712,"D(τ)(π(1), π(2)) := max
s"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08389261744966443,"
max
π
V (τ)
π,π(2)(s) −min
π′ V (τ)
π(1),π′(s)

."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08456375838926175,"In particular, when τ = 0, D(0)(π(1), π(2)) corresponds to the duality gap of the original Markov
game. Throughout, we deﬁne Amax := max{|A(1)|, |A(2)|}, Qmax := 1+γτ ln Amax"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08523489932885905,"1−γ
and Vmax :="
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08590604026845637,1+τ ln Amax
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08657718120805369,"1−γ
. We also require that the batch sizes of Algorithm 1 satisfy the following conditions."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.087248322147651,"Nk,t, N k,t ≥650tmixAmax"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08791946308724832,"µmin
ln
20Tsum|S|Amax"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08859060402684564,δ√µmin
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08926174496644296,"
,
∀k, t,
(18)"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.08993288590604027,"Nk+1 ≥
650tmix
µmin(1 −γ)2 ln

4
δ√µmin"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.09060402684563758,"
,
∀k.
(19)"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.0912751677852349,"Then, we obtain the following convergence result of Algorithm 1, where Tsum := PK−1
k=0 Tk.
Theorem 1 (Finite-time convergence rate). Apply Algorithm 1 to solve the entropy-regularized
Markov game with τ ∈(0, 1]. Choose learning rate η = [2(τ + Qmax)]−1, initialization ∥bV0∥∞≤
Vmax and batch sizes Nk,t, N k,t, Nk+1 that satisfy (18) & (19). Then, the Nash equilibrium duality
gap converges at the following rate with probability at least 1 −δ."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.09194630872483221,"D(τ) 
π(1)
K−1, π(2)
K−1

≤O
Vmax ln Amax 1 −γ K−1
X"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.09261744966442953,"k=0
γK−k(1 −ητ)Tk−1"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.09328859060402685,+ Vmax 1 −γ
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.09395973154362416,htmixAmax
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.09463087248322148,"µmin
ln
Tsum|S|Amax δµmin"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.0953020134228188,"i2/3 K−1
X"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.0959731543624161,"k=0
γK−k−1
Tk−1
X"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.09664429530201342,"t=0
(1 −ητ)Tk−2−t 1"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.09731543624161074,"N 2/3
k,t
+
Vmax"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.09798657718120805,"τN
2/3
k,t+1 "
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.09865771812080537,"+
V 3
maxγK"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.09932885906040269,"τ 2(1 −γ)2 +
tmixV 3
max
τ 2µmin(1 −γ)3 ln
 K|S| δµmin"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.1," K−1
X k=0"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10067114093959731,γK−k−1 Nk+1
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10134228187919463,"
.
(20)"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10201342281879194,"Remark 2. In the proof of Theorem 1, we also prove that the convergence rate of the Q-function
estimation error ∥QK −Q(τ)
∗∥∞is (1−γ) times the convergence rate in (20). Here, QK corresponds
to the Q-function associated with the policy pair (π(1)
K , π(2)
K ) produced by Algorithm 1 in the K-th
iteration, and Q(τ)
∗
corresponds to the optimal Q-function associated with the Nash equilibrium
policy pair π(1)
∗τ , π(2)
∗τ of the entropy-regularized Markov game."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10268456375838926,"Theorem 1 characterizes the convergence of duality gap of the SPE algorithm under general hy-
perparameter scheduling of the batch sizes Nk,t, N k,t, Nk+1 and number of inner iterations Tk.
Speciﬁcally, it can be seen that as the number of outer iterations K and inner iterations Tk increase,
the duality gap converges to an exponentially weighted average of N −2/3
k,t
, N
−2/3
k,t+1 and N −1
k+1, and the"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10335570469798658,Published as a conference paper at ICLR 2022
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.1040268456375839,"gap can be made arbitrarily small by choosing sufﬁciently large batch sizes. To provide an intuitive
understanding, we can set these hyper-parameters as constants, i.e., Tk ≡T, Nk,t ≡N, N k,t ≡N,
Nk+1 ≡N ′, and then the convergence rate in (20) simpliﬁes to"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10469798657718121,"O
 V 3
maxγK"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10536912751677853,τ 2(1 −γ)2 + γVmax ln Amax
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10604026845637583,"(1 −γ)2
(1 −ητ)T −1 +
tmixV 3
max
τ 2µmin(1 −γ)4 ln
 K|S| δµmin  1 N"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10671140939597315,"+
Vmax
ητ(1 −ητ)(1 −γ)2"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10738255033557047,htmixAmax
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10805369127516778,"µmin
ln
Tsum|S|Amax δµmin"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.1087248322147651,i2/3 1
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.10939597315436242,N 2/3 + Vmax
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11006711409395974,"τN
2/3"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11073825503355705,"
.
(21)"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11140939597315436,"To elaborate, the ﬁrst term characterizes the exponential convergence of the K outer value iterations.
This convergence is faster than the sublinear convergence O(
p"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11208053691275167,"ln K/K) established in (Wei et al.,
2021). The second term characterizes the exponential convergence of the T inner PU iterations.
Moreover, the last two terms characterize the estimation errors O(N −1) and O(N −2/3)+O(N
−2/3)
of the estimators (12) and (14,15), respectively. As a comparison, Wei et al. (2021) established a
much larger estimation error O(N −1/3). These improvements, as we show in Theorem 2 later, lead
to a substantially improved overall sample complexity over the state-of-the-art result. In particular,
due to the exponentially weighted average structure in (20), the sample complexity can be further
optimized by an adaptive scheduling of the batch sizes."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11275167785234899,"Technical Novelty. We further comment on the proof of Theorem 1. Note that the analysis of the PE
algorithm in (Cen et al., 2021) requires full knowledge of the environment and does not characterize
the convergence of duality gap. As a comparison, to establish the duality gap convergence rate (20) of
the model-free SPE, we need to make substantial new developments to tightly bound the estimation
errors of the proposed stochastic estimators. We elaborate our technical contributions below."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11342281879194631,"• As we explained in Remark 1, the sample averages involved in our estimators are correlated
Markovian samples. To bound the additional bias induced by these correlated samples, we apply
the concentration inequalities developed in (Paulin, 2015) for dependent samples to establish tight
high-probability estimation error bounds."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11409395973154363,"• We develop a much reﬁned analysis of the state value function estimation error ∥bVk+1 −V ′
k+1∥∞,
which is the key to develop tight bounds for all the other estimation errors. Speciﬁcally, we ﬁrst
propose the following error decomposition for any state s"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11476510067114094,"|bVk+1(s)−V ′
k+1(s)|=
bvk+1(s)"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11543624161073826,bµk(s) −vk+1(s) µk(s)
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11610738255033556,≤|bvk+1(s)−vk+1(s)|
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11677852348993288,"µk(s)
+
bvk+1(s)

µk(s)−bµk(s)"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.1174496644295302,"µk(s)bµk(s) ,"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11812080536912752,"where bvk+1(s), bµk+1(s) are sample average estimators of vk+1(s), µk+1(s), respectively, and we
refer to Appendix B for the deﬁnitions of these terms. The motivation is that the
bvk+1(s)
 in the
second term helps cancel out the estimator bµk(s) in the denominator, and then all the denominators
do not involve any sample average estimators, which may take a small numerical value that causes
divergence and a loose concentration bound. By leveraging this special decomposition and the
recursive structure of the stochastic estimator (12), we are able to establish the following key
contraction property of the estimation error (see (85) in Appendix B)."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11879194630872483,"∥bVk+1 −V ′
k+1∥∞≤γ∥bVk −V ′
k∥∞+ O
 
N −1/2
k+1

.
By telescoping the above contraction bound, we obtain tight estimation error bounds for all the
proposed stochastic estimators. As a comparison, Wei et al. (2021) directly applied the Azuma-
Hoeffding inequality with independent samples to bound the entire estimator and obtain a loose
error bound, and Liu et al. (2021) simply assumed a small upper bound for the estimation error.
• We develop a stochastic predictive update (SPU) algorithm with general inexact value function
estimations and a ﬁnite-time convergence analysis of its duality gap (see Lemma 1 for the SPU
algorithm and its convergence proof). This generalizes the convergence result of the PU algorithm
established in (Cen et al., 2021), which uses exact value functions based on full knowledge of
the environment. Finally, by incorporating our developed tight estimation error bounds into the
ﬁnite-time duality gap bound of SPU, we obtain the desired convergence rate in Theorem 1."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.11946308724832215,"Based on Theorem 1, we obtain the following sample complexity of SPE for achieving an ϵ-Nash
equilibrium duality gap of the original Markov game, i.e., D(0)(π(1)
K−1, π(2)
K−1) ≤ϵ. Here, we adopt
an adaptive batch size scheduling scheme to optimize the complexity order. The overall sample
complexity is given by PK−1
k=0

Nk+1 + 2 PTk−1
t=0 (Nk,t + N k,t+1)

."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12013422818791947,Published as a conference paper at ICLR 2022
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12080536912751678,"Theorem 2 (Sample complexity). Implement Algorithm 1 with η = O
 
1 −γ

, τ = O
  ϵ(1−γ)"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12147651006711409,"ln Amax

,"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.1221476510067114,"K = O
h
1
1−γ ln

ln Amax"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12281879194630872,"ϵ(1−γ)
i
and Tk = 1 +
k ln γ−1"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12348993288590604,ln(1−ητ)−1 . Choose the following adaptive batch sizes.
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12416107382550336,"Nk+1 = e
O
tmix(ln2 Amax)γ−k"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12483221476510067,"2
ϵ2µmin(1 −γ)8"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12550335570469798,"
, Nk,t = N k,tϵ
3
2 (1 −γ)3 = e
O
tmixAmax(1 −ητ)
−3(t+1)"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.1261744966442953,"5
µmin(1 −γ)3 
."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.1268456375838926,"Then, for any ϵ ≤ln Amax"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12751677852348994,"1−γ , the overall sample complexity to achieve D(0)(π(1)
K−1, π(2)
K−1) ≤ϵ is
e
O
 
tmixAmax
µminϵ5.5(1−γ)13.5

. Please refer to (118) in Appendix E for a complete expression."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12818791946308725,"The above complexity result is obtained by choosing a small τ = O
  ϵ(1−γ)"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12885906040268458,"ln Amax

for the convergence rate
result in Theorem 1. Speciﬁcally, we show in Lemma 6 that the duality gap is Lipschitz continuous
with regard to the entropy regularization parameter, i.e.,
D(τ)(π(1), π(2)) −D(0)(π(1), π(2))
 ≤
2τ ln Amax"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.12953020134228188,"1−γ
. Therefore, by choosing a proper small τ, convergence of the duality gap D(τ) of the
entropy-regularized Markov game implies the convergence of the duality gap D(0) of the original
Markov game."
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.13020134228187918,"Remark on Improvement of Sample Complexity. We elaborate on the improvement of sample
complexity in two different levels: population level (with known environment) and stochastic level
(using stochastic samples). First, in the population level, the original policy extragradient (PE)
algorithm in (Cen et al., 2021) already achieves a faster convergence rate than the OGDA-based
algorithm proposed in (Wei et al., 2021). Speciﬁcally, to achieve an ϵ-Nash equilibrium point of
the original Markov game, PE requires O
  Amax"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.13087248322147652,"(1−γ)ϵ ln2 ϵ−1
iterations by choosing a proper entropy"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.13154362416107382,regularization parameter τ = O( ϵ(1−γ)
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.13221476510067115,"ln Amax ). As a comparison, the OGDA-based algorithm requires
e
O

|S|3"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.13288590604026845,"(1−γ)9ϵ2

iterations by substituting ϵ = 0 (no stochastic error) and their choice of step size into"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.13355704697986578,"the Theorem 1 of Wei et al. (2021). This improvement is in the order of e
O(ϵ−1) and is due to the use
of entropy regularization in the PE algorithm, which improves the geometry of the bilinear matrix
game. Second, the rest of the improvement of sample complexity (about e
O(ϵ−1.5)) comes from the
stochastic level. Speciﬁcally, our stochastic PE (SPE) allows to use a large constant learning rate
η = O
 
1 −γ

, whereas the OGDA-based algorithm in (Wei et al., 2021) needs to use a substantially
smaller learning rate η = O(
p"
FINITE-TIME CONVERGENCE ANALYSIS OF SPE,0.1342281879194631,"(1 −γ)5|S|−1), which signiﬁcantly slows down its convergence in
both the population and stochastic levels. Moreover, both the PE and our SPE take O(ln ϵ−1) inner
updates to achieve an accurate solution of the matrix game (7), whereas the OGDA-based algorithm
uses only one inner update to solve the matrix game and hence suffers from a larger optimization error.
Finally, as we explained in the previous technical novelty paragraph, we improve the techniques used
in bounding the estimation errors. Speciﬁcally, Wei et al. (2021) bounds the estimation errors in all
the iterations by a small constant ϵ using independent samples. As a comparison, we establish a key
contraction property of the estimation error over the iterations with correlated Markovian samples.
Such a property allows us to use a growing batch size that bounds the errors loosely in the initial
iterations and achieves tight error bounds in the end."
CONCLUSION,0.1348993288590604,"5
CONCLUSION"
CONCLUSION,0.13557046979865772,"In this paper, we developed a model-free, provably convergent, sample efﬁcient, symmetric and
private stochastic policy extra gradient algorithm for solving two-player zero-sum Markov games.
Our algorithm leverages entropy regularization to facilitate the algorithm convergence and develops
new stochastic estimators to accurately estimate the value functions. We proved that our SPE
algorithm achieved a fast convergence rate in terms of the Nash equilibrium duality gap and moreover,
achieves a substantially improved sample complexity over the state-of-the-art result. We believe our
algorithm deepens the understanding of Markov games from a computation complexity perspective.
In the future study, it is interesting to extend SPE algorithm to the multi-agent setting for solving
general-sum Markov games and competitive games that involve multiple cooperative teams. Another
interesting direction is to improve the algorithm to further reduce the sample complexity to approach
the theoretical lower bound established in (Zhang et al., 2020)."
CONCLUSION,0.13624161073825503,Published as a conference paper at ICLR 2022
CONCLUSION,0.13691275167785236,ACKNOWLEDGMENTS
CONCLUSION,0.13758389261744966,"The work of Ziyi Chen, Shaocong Ma and Yi Zhou was supported in part by U.S. National Science
Foundation under the Grants CCF-2106216 and DMS-2134223."
REFERENCES,0.138255033557047,REFERENCES
REFERENCES,0.1389261744966443,"Bai, Y., Jin, C., and Yu, T. (2020). Near-optimal reinforcement learning with self-play. Proc. Advances
in Neural Information Processing Systems (Neurips), 33."
REFERENCES,0.1395973154362416,"Cen, S., Cheng, C., Chen, Y., Wei, Y., and Chi, Y. (2020). Fast global convergence of natural policy
gradient methods with entropy regularization. ArXiv:2007.06558."
REFERENCES,0.14026845637583893,"Cen, S., Wei, Y., and Chi, Y. (2021). Fast policy extragradient methods for competitive games with
entropy regularization. ArXiv:2105.15186."
REFERENCES,0.14093959731543623,"Daskalakis, C., Foster, D. J., and Golowich, N. (2020). Independent policy gradient methods for
competitive reinforcement learning. In Proc. Advances in Neural Information Processing Systems
(Neurips), volume 33."
REFERENCES,0.14161073825503356,"Deng, X., Li, Y., Mguni, D. H., Wang, J., and Yang, Y. (2021). On the complexity of computing
markov perfect equilibrium in general-sum stochastic games. ArXiv:2109.01795."
REFERENCES,0.14228187919463087,"Durrett, R. (2019). Probability: theory and examples, volume 49. Cambridge university press."
REFERENCES,0.1429530201342282,"Elie, R., Perolat, J., Laurière, M., Geist, M., and Pietquin, O. (2020). On the convergence of model
free learning in mean ﬁeld games. In Proc. the AAAI Conference on Artiﬁcial Intelligence (AAAI),
volume 34, pages 7143–7150."
REFERENCES,0.1436241610738255,"Ernst, D., Geurts, P., and Wehenkel, L. (2005). Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503–556."
REFERENCES,0.14429530201342283,"Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2020). A theoretical analysis of deep q-learning. In Proc.
Learning for Dynamics and Control (L4DC), pages 486–489."
REFERENCES,0.14496644295302014,"Farahmand, A.-M. and Ghavamzadeh, M. (2021). Pid accelerated value iteration algorithm. In Proc.
International Conference on Machine Learning (ICML), pages 3143–3153."
REFERENCES,0.14563758389261744,"Geist, M., Scherrer, B., and Pietquin, O. (2019). A theory of regularized markov decision processes.
In Proc. International Conference on Machine Learning (ICML), pages 2160–2169."
REFERENCES,0.14630872483221477,"Ghosh, M. K., Golui, S., Pal, C., and Pradhan, S. (2021). Zero-sum games for continuous-time
markov decision processes with risk-sensitive average cost criterion. ArXiv:2109.08837."
REFERENCES,0.14697986577181207,"Gu, H., Guo, X., Wei, X., and Xu, R. (2021). Mean-ﬁeld multi-agent reinforcement learning: A
decentralized network approach. ArXiv:2108.02731."
REFERENCES,0.1476510067114094,"Guo, H., Fu, Z., Yang, Z., and Wang, Z. (2021). Decentralized single-timescale actor-critic on
zero-sum two-player stochastic games. In Proc. International Conference on Machine Learning
(ICML), pages 3899–3909."
REFERENCES,0.1483221476510067,"Guo, X., Hu, A., Xu, R., and Zhang, J. (2019). Learning mean-ﬁeld games. Proc. Advances in Neural
Information Processing Systems (Neurips), 32:4966–4976."
REFERENCES,0.14899328859060404,"Hu, J. and Wellman, M. P. (2003). Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039–1069."
REFERENCES,0.14966442953020134,"Huang, B., Lee, J. D., Wang, Z., and Yang, Z. (2021). Towards general function approximation in
zero-sum markov games. ArXiv:2107.14702."
REFERENCES,0.15033557046979865,"Jafarnia-Jahromi, M., Jain, R., and Nayyar, A. (2021). Learning zero-sum stochastic games with
posterior sampling. ArXiv:2109.03396."
REFERENCES,0.15100671140939598,"Jin, Y. and Sidford, A. (2021). Towards tight bounds on the sample complexity of average-reward
mdps. In Proc. International Conference on Machine Learning (ICML), pages 5055–5064."
REFERENCES,0.15167785234899328,Published as a conference paper at ICLR 2022
REFERENCES,0.1523489932885906,"Kononen, V. and Oja, E. (2004). Asymmetric multiagent reinforcement learning in pricing ap-
plications. In Proc. IEEE International Joint Conference on Neural Networks (IEEE Cat. No.
04CH37541), volume 2, pages 1097–1102."
REFERENCES,0.15302013422818792,"Leonardos, S., Overman, W., Panageas, I., and Piliouras, G. (2021). Global convergence of multi-
agent policy gradient in markov potential games. ArXiv:2106.01969."
REFERENCES,0.15369127516778525,"Liu, B., Yang, Z., and Wang, Z. (2021). Policy optimization in zero-sum markov games: Fictitious
self-play provably attains nash equilibria."
REFERENCES,0.15436241610738255,"Mei, J., Xiao, C., Szepesvari, C., and Schuurmans, D. (2020). On the global convergence rates of
softmax policy gradient methods. In Proc. International Conference on Machine Learning (ICML),
pages 6820–6829."
REFERENCES,0.15503355704697985,"Mertikopoulos, P. and Sandholm, W. H. (2016). Learning in games via reinforcement and regulariza-
tion. Mathematics of Operations Research, 41(4):1297–1324."
REFERENCES,0.15570469798657718,"Moerland, T. M., Broekens, J., Plaat, A., and Jonker, C. M. (2018). A0c: Alpha zero in continuous
action space. ArXiv:1805.09613."
REFERENCES,0.1563758389261745,"Neu, G., Jonsson, A., and Gómez, V. (2017). A uniﬁed view of entropy-regularized markov decision
processes. ArXiv:1705.07798."
REFERENCES,0.15704697986577182,"Ortner, P. and Auer, R. (2007). Logarithmic online regret bounds for undiscounted reinforcement
learning. In Proc. Advances in Neural Information Processing Systems (Neurips), volume 19,
page 49."
REFERENCES,0.15771812080536912,"Ortner, R. (2020). Regret bounds for reinforcement learning via markov chain concentration. Journal
of Artiﬁcial Intelligence Research, 67:115–128."
REFERENCES,0.15838926174496645,"Paulin, D. (2015). Concentration inequalities for markov chains by marton couplings and spectral
methods. Electronic Journal of Probability, 20:1–32."
REFERENCES,0.15906040268456376,"Savas, Y., Ahmadi, M., Tanaka, T., and Topcu, U. (2019). Entropy-regularized stochastic games. In
Proc. IEEE 58th Conference on Decision and Control (CDC), pages 5955–5962."
REFERENCES,0.1597315436241611,"Shapley, L. S. (1953).
Stochastic games.
Proceedings of the national academy of sciences,
39(10):1095–1100."
REFERENCES,0.1604026845637584,"Sidford, A., Wang, M., Yang, L., and Ye, Y. (2020). Solving discounted stochastic two-player games
with near-optimal time and sample complexity. In Proc. International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), pages 2992–3002."
REFERENCES,0.1610738255033557,"Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker,
L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge. nature,
550(7676):354–359."
REFERENCES,0.16174496644295303,"Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press."
REFERENCES,0.16241610738255033,"Tamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P. (2016). Value iteration networks. In Proc.
Advances in neural information processing systems (Neurips), pages 2154–2162."
REFERENCES,0.16308724832214766,"Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H.,
Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii using multi-agent
reinforcement learning. Nature, 575(7782):350–354."
REFERENCES,0.16375838926174496,"Wang, X. and Sandholm, T. (2002). Reinforcement learning to play an optimal nash equilibrium in
team markov games. volume 15, pages 1603–1610."
REFERENCES,0.1644295302013423,"Wei, C.-Y., Hong, Y.-T., and Lu, C.-J. (2017). Online reinforcement learning in stochastic games. In
Proc. Advances in neural information processing systems (Neurips), pages 4994–5004."
REFERENCES,0.1651006711409396,"Wei, C.-Y., Lee, C.-W., Zhang, M., and Luo, H. (2021). Last-iterate convergence of decentralized op-
timistic gradient descent/ascent in inﬁnite-horizon competitive markov games. In Proc. Conference
on Learning Theory (COLT)."
REFERENCES,0.1657718120805369,Published as a conference paper at ICLR 2022
REFERENCES,0.16644295302013423,"Xia, Y., Ding, W., Zhang, X.-D., Yu, N., and Qin, T. (2016). Budgeted bandit problems with
continuous random costs. In Proc. Asian conference on machine learning (ACML), pages 317–332."
REFERENCES,0.16711409395973154,"Xu, Y., Deng, Z., Wang, M., Xu, W., So, A. M.-C., and Cui, S. (2020). Voting-based multiagent
reinforcement learning for intelligent iot. IEEE Internet of Things Journal, 8(4):2681–2693."
REFERENCES,0.16778523489932887,"Zhang, K., Kakade, S., Basar, T., and Yang, L. (2020). Model-based multi-agent rl in zero-sum
markov games with near-optimal sample complexity. In Proc. Advances in Neural Information
Processing Systems (Neurips), volume 33."
REFERENCES,0.16845637583892617,"Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2021). Finite-sample analysis for decentralized
batch multi-agent reinforcement learning with networked agents. IEEE Transactions on Automatic
Control."
REFERENCES,0.1691275167785235,"Zhang, X., Zhang, K., Miehling, E., and Basar, T. (2019). Non-cooperative inverse reinforcement
learning. In Proc. Advances in Neural Information Processing Systems (Neurips), volume 32,
pages 9487–9497."
REFERENCES,0.1697986577181208,"Zhao, Y., Tian, Y., Lee, J. D., and Du, S. S. (2021). Provably efﬁcient policy gradient methods for
two-player zero-sum markov games. ArXiv:2102.08903."
REFERENCES,0.1704697986577181,"Zhu, Y. and Zhao, D. (2020). Online minimax q network learning for two-player zero-sum markov
games. IEEE Transactions on Neural Networks and Learning Systems."
REFERENCES,0.17114093959731544,"Zou, S., Xu, T., and Liang, Y. (2019). Finite-sample analysis for sarsa and q-learning with linear
function approximation. ArXiv:1902.02234."
REFERENCES,0.17181208053691274,Published as a conference paper at ICLR 2022
REFERENCES,0.17248322147651007,Appendix
REFERENCES,0.17315436241610738,Table of Contents
REFERENCES,0.1738255033557047,"A Analysis of stochastic PU (SPU) for entropy-regularized matrix game
13"
REFERENCES,0.174496644295302,"B
Estimation error bounds
18"
REFERENCES,0.17516778523489934,"C Properties of the Duality Gap
25"
REFERENCES,0.17583892617449665,"D Proof of Theorem 1
27"
REFERENCES,0.17651006711409395,"E
Proof of Theorem 2
31"
REFERENCES,0.17718120805369128,"F
Summary of Comparison of Sample Complexities
34"
REFERENCES,0.17785234899328858,"G Rationality
35"
REFERENCES,0.17852348993288591,"A
ANALYSIS OF STOCHASTIC PU (SPU) FOR ENTROPY-REGULARIZED
MATRIX GAME"
REFERENCES,0.17919463087248322,"We ﬁrst consider the following zero-sum entropy regularized matrix game, which can be considered
as a simple special case of Markov game with only one state."
REFERENCES,0.17986577181208055,"max
µ∈∆(d1)
min
ν∈∆(d2) hτ(µ, ν) := µ⊤Qν + τH(µ) −τH(ν).
(22)"
REFERENCES,0.18053691275167785,"where Q ∈Rd1×d2 is ﬁxed. The solution (µ∗
τ, ν∗
τ ) of the above problem, also known as the quantal
response equilibrium (QRE), satisﬁes the following condition (Cen et al., 2021)"
REFERENCES,0.18120805369127516,"µ∗
τ ∝exp(Qν∗
τ /τ), ν∗
τ ∝exp(−Q⊤µ∗
τ/τ).
(23)"
REFERENCES,0.1818791946308725,"Our stochastic PU (SPU) algorithm for the above matrix game is written as follows,"
REFERENCES,0.1825503355704698,"µt+1(a) ∝µt(a)1−ητ exp
 
η

Qνt + δ(1)
t
"
REFERENCES,0.18322147651006712,"a

(24)"
REFERENCES,0.18389261744966443,"νt+1(b) ∝νt(b)1−ητ exp
 
−η

Q⊤µt + δ(2)
t
"
REFERENCES,0.18456375838926176,"b

(25)"
REFERENCES,0.18523489932885906,"µt+1(a) ∝µt(a)1−ητ exp
 
η

Qνt+1 + δ
(1)
t
"
REFERENCES,0.18590604026845636,"a

(26)"
REFERENCES,0.1865771812080537,"νt+1(b) ∝νt(b)1−ητ exp
 
−η

Q⊤µt+1 + δ
(2)
t
"
REFERENCES,0.187248322147651,"b

,
(27)"
REFERENCES,0.18791946308724833,"where δ(1)
t
is an additive noise to the underlying true quantity Qνt, and the other noises δ(2)
t
, δ
(1)
t ,"
REFERENCES,0.18859060402684563,"δ
(2)
t
are similar. If all these noises are zero, the above stochastic PU algorithm reduces to the PU
algorithm for matrix game deﬁned in Section 2 of (Cen et al., 2021)."
REFERENCES,0.18926174496644296,"The convergence rate of stochastic PU for matrix game is shown below. The proof logic follows from
that of (Cen et al., 2021)."
REFERENCES,0.18993288590604027,"Lemma 1. Choose learning rate η ≤

2(τ + ∥Q∥∞)
−1 and use the uniform policy initialization
µ0 = 1/d1, ν0 = 1/d2. Then, the SPU algorithm deﬁned by eqs. (24)-(27) for entropy-regularized
matrix game has the following convergence rates."
REFERENCES,0.1906040268456376,"KL(µ∗
τ∥µt) + KL(ν∗
τ ∥νt)"
REFERENCES,0.1912751677852349,≤(1 −ητ)t ln(d1d2)
REFERENCES,0.1919463087248322,"Published as a conference paper at ICLR 2022 + t−1
X"
REFERENCES,0.19261744966442954,"j=0
(1 −ητ)t−1−jh
2η2 
∥δ(1)
j ∥2
∞+ ∥δ(2)
j ∥2
∞

+ 2η"
REFERENCES,0.19328859060402684,"τ
 
∥δ
(1)
j ∥2
∞+ ∥δ
(2)
j ∥2
∞
i
.
(28)"
REFERENCES,0.19395973154362417,"hτ(µ⋆
τ, ν⋆
τ ) −hτ(µt+1, νt+1) ≤4"
REFERENCES,0.19463087248322147,"η (1 −ητ)t ln(d1d2) + t
X"
REFERENCES,0.1953020134228188,"j=0
(1 −ητ)t−1−jh
8η
 
∥δ(1)
j ∥2
∞+ ∥δ(2)
j ∥2
∞

+ 33"
REFERENCES,0.1959731543624161,"4τ
 
∥δ
(1)
j ∥2
∞+ ∥δ
(2)
j ∥2
∞
i
.
(29)"
REFERENCES,0.1966442953020134,"max
µ∈∆(d1) hτ(µ, νt+1) −
min
ν∈∆(d2) hτ(µt+1, ν) ≤2"
REFERENCES,0.19731543624161074,"η (1 −ητ)t ln(d1d2) + t
X"
REFERENCES,0.19798657718120805,"j=0
(1 −ητ)t−1−jh
16η
 
∥δ(1)
j ∥2
∞+ ∥δ(2)
j ∥2
∞

+ 12"
REFERENCES,0.19865771812080538,"τ
 
∥δ
(1)
j ∥2
∞+ ∥δ
(2)
j ∥2
∞
i
.
(30)"
REFERENCES,0.19932885906040268,"Proof. Eq. (26) implies that for some constant c ∈R,"
REFERENCES,0.2,"ln µt+1 = (1 −ητ) ln µt + η(Qνt+1 + δ
(1)
t ) + c1"
REFERENCES,0.20067114093959731,"⇒

ln µt+1 −(1 −ητ) ln µt −η(Qνt+1 + δ
(1)
t ), µt+1 −µ∗
τ

= 0.
(31)"
REFERENCES,0.20134228187919462,"where we used ⟨µt+1 −µ∗
τ, 1⟩= 0. Similarly, eqs. (27)&(23) respectively imply that

ln νt+1 −(1 −ητ) ln νt + η(Q⊤µt+1 + δ
(2)
t ), νt+1 −ν∗
τ

=0
(32)

ln µ∗
τ −Qν∗
τ /τ, µt+1 −µ∗
τ

=0
(33)

ln ν∗
τ + Q⊤µ∗
τ/τ, νt+1 −ν∗
τ

=0.
(34)"
REFERENCES,0.20201342281879195,"Similarly, eqs. (24)&(26) imply that

ln(µt+1/µt+1), µt+1 −µt+1"
REFERENCES,0.20268456375838925,"=

η

Q(νt −νt+1) + δ(1)
t
−δ
(1)
t

, µt+1 −µt+1"
REFERENCES,0.20335570469798658,"≤η∥Q∥∞∥µt+1 −µt+1∥1∥νt −νt+1∥1 + η∥δ(1)
t
−δ
(1)
t ∥∞∥µt+1 −µt+1∥1 ≤η"
REFERENCES,0.2040268456375839,"2∥Q∥∞
 
∥µt+1 −µt+1∥2
1 + ∥νt −νt+1∥2
1

+
η2"
REFERENCES,0.20469798657718122,"2(1 −η∥Q∥∞)∥δ(1)
t
−δ
(1)
t ∥2
∞ + 1"
REFERENCES,0.20536912751677852,"2(1 −η∥Q∥∞)∥µt+1 −µt+1∥2
1"
REFERENCES,0.20604026845637585,"(i)
≤KL(µt+1∥µt+1) + η∥Q∥∞KL(νt+1∥νt) + 2η2 
∥δ(1)
t
∥2
∞+ ∥δ
(1)
t ∥2
∞

,
(35)"
REFERENCES,0.20671140939597316,"where (i) uses the Pinsker’s inequality and η ≤

2(τ + ∥Q∥∞)
−1."
REFERENCES,0.20738255033557046,"Similarly to the above derivation, we can infer from eqs. (25)&(27) that

ln(νt+1/νt+1), νt+1 −νt+1

≤KL(νt+1∥νt+1) + η∥Q∥∞KL(µt+1∥µt)"
REFERENCES,0.2080536912751678,"+ 2η2 
∥δ(2)
t
∥2
∞+ ∥δ
(2)
t ∥2
∞

.
(36)"
REFERENCES,0.2087248322147651,Computing eq. (31)+eq. (32)-ητ[eq. (33)+eq. (34)] yields that
REFERENCES,0.20939597315436242,"η⟨δ
(1)
t , µt+1 −µ∗
τ

−η⟨δ
(2)
t , νt+1 −ν∗
τ"
REFERENCES,0.21006711409395973,"=

ln µt+1 −(1 −ητ) ln µt −ητ ln µ∗
τ −ηQ(νt+1 −ν∗
τ ), µt+1 −µ∗
τ"
REFERENCES,0.21073825503355706,"+

ln νt+1 −(1 −ητ) ln νt −ητ ln ν∗
τ + ηQ⊤(µt+1 −µ∗
τ), νt+1 −ν∗
τ"
REFERENCES,0.21140939597315436,"=

ln(µt+1/µ∗
τ) −(1 −ητ) ln(µt/µ∗
τ), µt+1 −µ∗
τ"
REFERENCES,0.21208053691275167,Published as a conference paper at ICLR 2022
REFERENCES,0.212751677852349,"+

ln(νt+1/ν∗
τ ) −(1 −ητ) ln(νt/ν∗
τ ), νt+1 −ν∗
τ"
REFERENCES,0.2134228187919463,"=KL(µ∗
τ∥µt+1) −(1 −ητ)KL(µ∗
τ∥µt)"
REFERENCES,0.21409395973154363,"+

ln(µt+1/µ∗
τ) −ln(µt+1/µt+1) −(1 −ητ)

ln(µt+1/µ∗
τ) −ln(µt+1/µt)

, µt+1"
REFERENCES,0.21476510067114093,"+ KL(ν∗
τ ∥νt+1) −(1 −ητ)KL(ν∗
τ ∥νt)"
REFERENCES,0.21543624161073827,"+

ln(νt+1/ν∗
τ ) −ln(νt+1/νt+1) −(1 −ητ)

ln(νt+1/ν∗
τ ) −ln(νt+1/νt)

, νt+1"
REFERENCES,0.21610738255033557,"=KL(µ∗
τ∥µt+1) −(1 −ητ)KL(µ∗
τ∥µt) + ητKL(µt+1∥µ∗
τ)
+ (1 −ητ)KL(µt+1∥µt) + KL(µt+1∥µt+1)
+ KL(ν∗
τ ∥νt+1) −(1 −ητ)KL(ν∗
τ ∥νt) + ητKL(νt+1∥ν∗
τ )
+ (1 −ητ)KL(νt+1∥νt) + KL(νt+1∥νt+1)"
REFERENCES,0.21677852348993287,"−

ln(µt+1/µt+1), µt+1 −µt+1

−

ln(νt+1/νt+1), νt+1 −νt+1"
REFERENCES,0.2174496644295302,"(i)
≥KL(µ∗
τ∥µt+1) −(1 −ητ)KL(µ∗
τ∥µt) + ητKL(µt+1∥µ∗
τ) + (1 −ητ −η∥Q∥∞)KL(µt+1∥µt)
+ KL(ν∗
τ ∥νt+1) −(1 −ητ)KL(ν∗
τ ∥νt) + ητKL(νt+1∥ν∗
τ ) + (1 −ητ −η∥Q∥∞)KL(νt+1∥νt)"
REFERENCES,0.2181208053691275,"−2η2 
∥δ(1)
t
∥2
∞+ ∥δ
(1)
t ∥2
∞+ ∥δ(2)
t
∥2
∞+ ∥δ
(2)
t ∥2
∞

,
(37)"
REFERENCES,0.21879194630872484,where (i) uses eqs. (35)&(36). The left side of the above inequality has the following upper bound.
REFERENCES,0.21946308724832214,"η⟨δ
(1)
t , µt+1 −µ∗
τ

−η⟨δ
(2)
t , νt+1 −ν∗
τ"
REFERENCES,0.22013422818791947,"≤η∥δ
(1)
t ∥∞∥µt+1 −µ∗
τ∥1 + η∥δ
(2)
t ∥∞∥νt+1 −ν∗
τ ∥1 ≤η 2"
REFERENCES,0.22080536912751678,"2∥δ
(1)
t ∥2
∞
τ
+ τ"
REFERENCES,0.2214765100671141,"2∥µt+1 −µ∗
τ∥2
1 + 2∥δ
(2)
t ∥2
∞
τ
+ τ"
REFERENCES,0.2221476510067114,"2∥νt+1 −ν∗
τ ∥2
1
 ≤η"
REFERENCES,0.22281879194630871,"τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞

+ ητ"
REFERENCES,0.22348993288590605,"2
 
KL(µt+1∥µ∗
τ) + KL(νt+1∥ν∗
τ )

.
(38)"
REFERENCES,0.22416107382550335,Substituting eq. (38) into eq. (37) and rearranging it yields that
REFERENCES,0.22483221476510068,"KL(µ∗
τ∥µt+1) + KL(ν∗
τ ∥νt+1) + ητ"
REFERENCES,0.22550335570469798,"2

KL(µt+1∥µ∗
τ) + KL(νt+1∥ν∗
τ )
 + 1"
REFERENCES,0.22617449664429531,"2

KL(µt+1∥µt) + KL(νt+1∥νt)
"
REFERENCES,0.22684563758389262,"≤(1 −ητ)

KL(µ∗
τ∥µt) + KL(ν∗
τ ∥νt)

+ 2η2 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞
 + 2η"
REFERENCES,0.22751677852348992,"τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞

,
(39)"
REFERENCES,0.22818791946308725,"where we use η ≤

2(τ + ∥Q∥∞)
−1 ≤(2τ)−1. This further implies that"
REFERENCES,0.22885906040268456,"KL(µ∗
τ∥µt+1) + KL(ν∗
τ ∥νt+1)"
REFERENCES,0.2295302013422819,"≤(1 −ητ)

KL(µ∗
τ∥µt) + KL(ν∗
τ ∥νt)
"
REFERENCES,0.2302013422818792,"+ 2η2 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞

+ 2η"
REFERENCES,0.23087248322147652,"τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞

.
(40)"
REFERENCES,0.23154362416107382,"Consequently, eq. (28) can be proved via iterating the above inequality and using the facts that
KL(µ∗
τ∥µ0) ≤ln d1, KL(ν∗
τ ∥ν0) ≤ln d2 (Since µ0 and ν0 are uniform probability vectors)."
REFERENCES,0.23221476510067113,"Next, we will prove eq. (29)."
REFERENCES,0.23288590604026846,Computing eq. (31) −ητ·eq. (33) yields that
REFERENCES,0.23355704697986576,"⟨Q(νt+1 −ν∗
τ ), µt+1 −µ∗
τ"
REFERENCES,0.2342281879194631,"= η−1
ln µt+1 −(1 −ητ) ln µt −ητ ln µ∗
τ −ηδ
(1)
t , µt+1 −µ∗
τ

.
(41)"
REFERENCES,0.2348993288590604,"Hence, we conclude that"
REFERENCES,0.23557046979865773,"hτ(µt+1, νt+1) −hτ(µ⋆
τ, ν⋆
τ )
≤hτ(µt+1, νt+1) −hτ(µt+1, ν⋆
τ )"
REFERENCES,0.23624161073825503,Published as a conference paper at ICLR 2022
REFERENCES,0.23691275167785236,"= µ⊤
t+1Q(νt+1 −ν⋆
τ ) + τν⊤
t+1 ln νt+1 −τν⋆⊤
τ
ln ν⋆
τ
= (µt+1 −µ⋆
τ)⊤Q(νt+1 −ν⋆
τ ) + µ⋆⊤
τ Q(νt+1 −ν⋆
τ ) + τν⊤
t+1 ln νt+1 −τν⋆⊤
τ
ln ν⋆
τ"
REFERENCES,0.23758389261744967,"(i)
= η−1
ln µt+1 −(1 −ητ) ln µt −ητ ln µ∗
τ −ηδ
(1)
t , µt+1 −µ∗
τ

−τ

ln ν∗
τ , νt+1 −ν∗
τ"
REFERENCES,0.23825503355704697,"+ τν⊤
t+1 ln νt+1 −τν⋆⊤
τ
ln ν⋆
τ"
REFERENCES,0.2389261744966443,"(ii)
= η−1
KL(µ∗
τ∥µt+1) −(1 −ητ)KL(µ∗
τ∥µt) + ητKL(µt+1∥µ∗
τ) + (1 −ητ)KL(µt+1∥µt)"
REFERENCES,0.2395973154362416,"+ KL(µt+1∥µt+1) −

ln(µt+1/µt+1), µt+1 −µt+1

+ τKL(νt+1∥ν⋆
τ ) −⟨δ
(1)
t , µt+1 −µ∗
τ"
REFERENCES,0.24026845637583893,"≤η−1
KL(µ∗
τ∥µt+1) −(1 −ητ)KL(µ∗
τ∥µt) + ητ
 
KL(µt+1∥µ∗
τ) + KL(νt+1∥ν⋆
τ )
"
REFERENCES,0.24093959731543624,"+ (1 −ητ)KL(µt+1∥µt) −KL(µt+1∥µt+1)

+ 1"
REFERENCES,0.24161073825503357,"2τ ∥δ
(1)
t ∥2
∞+ τ"
REFERENCES,0.24228187919463087,"2∥µt+1 −µ∗
τ∥2
1"
REFERENCES,0.24295302013422818,"(iii)
≤η−1
KL(µ∗
τ∥µt+1) + 2ητ
 
KL(µt+1∥µ∗
τ) + KL(νt+1∥ν⋆
τ )

+ KL(µt+1∥µt)

+ 1"
REFERENCES,0.2436241610738255,"4τ ∥δ
(1)
t ∥2
∞"
REFERENCES,0.2442953020134228,"(iv)
≤4η−1h
KL(µ∗
τ∥µt) + KL(ν∗
τ ∥νt) + 2η2 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞

+ 2η"
REFERENCES,0.24496644295302014,"τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞
i + 1"
REFERENCES,0.24563758389261744,"4τ ∥δ
(1)
t ∥2
∞"
REFERENCES,0.24630872483221478,"(v)
≤4η−1
KL(µ∗
τ∥µt) + KL(ν∗
τ ∥νt)
"
REFERENCES,0.24697986577181208,"+ 8η
 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞

+ 33"
REFERENCES,0.24765100671140938,"4τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞

,
(42)"
REFERENCES,0.2483221476510067,"where (i) uses eq. (41)&(34), (ii) follows the derivation of eq. (37), (iii) uses the Pinsker’s inequality,
(iv) uses eq. (39), and (v) uses η ≤

2(τ + ∥Q∥∞)
−1. In a similar way, we can also prove that"
REFERENCES,0.24899328859060402,"hτ(µ⋆
τ, ν⋆
τ ) −hτ(µt+1, νt+1) ≤hτ(µ⋆
τ, νt+1) −hτ(µt+1, νt+1)"
REFERENCES,0.24966442953020135,"≤4η−1
KL(µ∗
τ∥µt) + KL(ν∗
τ ∥νt)
"
REFERENCES,0.25033557046979865,"+ 8η
 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞

+ 33"
REFERENCES,0.25100671140939596,"4τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞

.
(43)"
REFERENCES,0.2516778523489933,"Combining eqs. (42)&(43) yields that
hτ(µ⋆
τ, ν⋆
τ ) −hτ(µt+1, νt+1)"
REFERENCES,0.2523489932885906,"≤4η−1
KL(µ∗
τ∥µt) + KL(ν∗
τ ∥νt)

+ 8η
 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞

+ 33"
REFERENCES,0.2530201342281879,"4τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞
"
REFERENCES,0.2536912751677852,"(i)
≤4η−1(1 −ητ)t ln(d1d2) + t−1
X"
REFERENCES,0.2543624161073825,"j=0
(1 −ητ)t−1−jh
8η
 
∥δ(1)
j ∥2
∞+ ∥δ(2)
j ∥2
∞

+ 8"
REFERENCES,0.2550335570469799,"τ
 
∥δ
(1)
j ∥2
∞+ ∥δ
(2)
j ∥2
∞
i"
REFERENCES,0.2557046979865772,"+ 8η
 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞

+ 33"
REFERENCES,0.2563758389261745,"4τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞
"
REFERENCES,0.2570469798657718,"≤4η−1(1 −ητ)t ln(d1d2) + t
X"
REFERENCES,0.25771812080536916,"j=0
(1 −ητ)t−1−jh
8η
 
∥δ(1)
j ∥2
∞+ ∥δ(2)
j ∥2
∞

+ 33"
REFERENCES,0.25838926174496646,"4τ
 
∥δ
(1)
j ∥2
∞+ ∥δ
(2)
j ∥2
∞
i
,"
REFERENCES,0.25906040268456376,where (i) uses eq. (28). This proves eq. (29).
REFERENCES,0.25973154362416107,"Next, to prove the duality gap (30), we ﬁrst derive an upper bound of KL(µ⋆
τ∥µt+1)+KL(ν⋆
τ ∥νt+1)."
REFERENCES,0.26040268456375837,"KL(ν⋆
τ ∥νt+1)"
REFERENCES,0.2610738255033557,"=KL(ν⋆
τ ∥νt+1) −KL(νt+1∥νt+1) +"
REFERENCES,0.26174496644295303,"νt+1 −ν⋆
τ , ln νt+1 −ln νt+1"
REFERENCES,0.26241610738255033,"≤KL(ν⋆
τ ∥νt+1) +"
REFERENCES,0.26308724832214764,"νt+1 −ν⋆
τ , ηQ⊤(µt+1 −µt) + η(δ
(2)
t
−δ(2)
t
)"
REFERENCES,0.26375838926174494,"≤KL(ν⋆
τ ∥νt+1) + ∥νt+1 −ν⋆
τ ∥1
 
η∥Q∥∞∥µt+1 −µt∥1 + η∥δ
(2)
t
−δ(2)
t
∥∞
"
REFERENCES,0.2644295302013423,Published as a conference paper at ICLR 2022
REFERENCES,0.2651006711409396,"≤KL(ν⋆
τ ∥νt+1) + 1 2"
REFERENCES,0.2657718120805369,"h
η∥Q∥∞+ 1 4"
REFERENCES,0.2664429530201342,"
∥νt+1 −ν⋆
τ ∥2
1 + η∥Q∥∞∥µt+1 −µt∥2
1 + 4η2∥δ
(2)
t
−δ(2)
t
∥2
∞
i"
REFERENCES,0.26711409395973157,"(i)
≤KL(ν⋆
τ ∥νt+1) + 3"
REFERENCES,0.2677852348993289,"4KL(ν⋆
τ ∥νt+1) + 1"
REFERENCES,0.2684563758389262,"2KL(µt+1∥µt) + 4η2 
∥δ(2)
t
∥2
∞+ ∥δ
(2)
t ∥2
∞

,
(44)"
REFERENCES,0.2691275167785235,"where (i) uses the Pinker’s inequality and η ≤

2(τ + ∥Q∥∞)
−1. Rearranging the above inequality
yields that"
REFERENCES,0.2697986577181208,"KL(ν⋆
τ ∥νt+1) ≤4KL(ν⋆
τ ∥νt+1) + 2KL(µt+1∥µt) + 16η2 
∥δ(2)
t
∥2
∞+ ∥δ
(2)
t ∥2
∞

.
(45)"
REFERENCES,0.27046979865771814,"Similarly, we can prove that"
REFERENCES,0.27114093959731544,"KL(µ⋆
τ∥µt+1) ≤4KL(µ⋆
τ∥µt+1) + 2KL(νt+1∥νt) + 16η2 
∥δ(1)
t
∥2
∞+ ∥δ
(1)
t ∥2
∞

.
(46)"
REFERENCES,0.27181208053691275,Summing up eqs. (45)&(46) yields that
REFERENCES,0.27248322147651005,"KL(µ⋆
τ∥µt+1) + KL(ν⋆
τ ∥νt+1)"
REFERENCES,0.2731543624161074,"≤4

KL(µ∗
τ∥µt+1) + KL(ν∗
τ ∥νt+1)

+ 2

KL(µt+1∥µt) + KL(νt+1∥νt)
"
REFERENCES,0.2738255033557047,"+ 16η2 
∥δ(1)
t
∥2
∞+ ∥δ
(1)
t ∥2
∞+ ∥δ(2)
t
∥2
∞+ ∥δ
(2)
t ∥2
∞
"
REFERENCES,0.274496644295302,"(i)
≤4

KL(µ∗
τ∥µt) + KL(ν∗
τ ∥νt)

+ 24η2 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞
 + 16η"
REFERENCES,0.2751677852348993,"τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞

,
(47)"
REFERENCES,0.2758389261744966,"where (i) uses eq. (39) and η ≤

2(τ + ∥Q∥∞)
−1 ≤(2τ)−1."
REFERENCES,0.276510067114094,"Finally, we prove the duality gap bound (30)."
REFERENCES,0.2771812080536913,"hτ(µ⋆
τ, ν⋆
τ ) −hτ(µ, ν⋆
τ ) = (µ⋆
τ −µ)⊤Qν⋆
τ + τµ⊤ln µ −τµ⋆⊤
τ
ln µ⋆
τ"
REFERENCES,0.2778523489932886,"(i)
= τ(⟨µ⋆
τ −µ, ln µ⋆
τ⟩+ µ⊤ln µ −µ⋆⊤
τ
ln µ⋆
τ) = τKL(µ∥µ⋆
τ),
(48)"
REFERENCES,0.2785234899328859,where (i) uses eq. (33).
REFERENCES,0.2791946308724832,"hτ(µ, νt+1) −hτ(µ, ν⋆
τ )"
REFERENCES,0.27986577181208055,"= µ⊤Q(νt+1 −ν⋆
τ ) + τν⊤
t+1 ln νt+1 −τν⋆⊤
τ
ln ν⋆
τ
= (µ −µ⋆
τ)⊤Q(νt+1 −ν⋆
τ ) + µ⋆⊤
τ Q(νt+1 −ν⋆
τ ) + τν⊤
t+1 ln νt+1 −τν⋆⊤
τ
ln ν⋆
τ"
REFERENCES,0.28053691275167786,"(i)
≤∥µ −µ⋆
τ∥1∥Q∥∞∥νt+1 −ν⋆
τ ∥1 −τ

ln ν∗
τ , νt+1 −ν∗
τ

+ τν⊤
t+1 ln νt+1 −τν⋆⊤
τ
ln ν⋆
τ ≤1"
REFERENCES,0.28120805369127516,"2

τ∥µ −µ⋆
τ∥2
1 + ∥Q∥2
∞∥νt+1 −ν⋆
τ ∥2
1/τ

+ τKL(νt+1∥ν⋆
τ )"
REFERENCES,0.28187919463087246,"(ii)
≤τKL(µ∥µ⋆
τ) + τKL(νt+1∥ν⋆
τ ) + τ −1∥Q∥2
∞KL(ν⋆
τ ∥νt+1),
(49)"
REFERENCES,0.2825503355704698,"where (i) uses eq. (34) and (ii) uses the Pinsker’s inequality. Then, eq. (49) & eq. (48) imply that"
REFERENCES,0.2832214765100671,"hτ(µ, νt+1) −hτ(µ⋆
τ, ν⋆
τ ) ≤τKL(νt+1∥ν⋆
τ ) + τ −1∥Q∥2
∞KL(ν⋆
τ ∥νt+1),
(50)"
REFERENCES,0.28389261744966443,"where (i) uses eq. (45), (ii) uses the Pinker’s inequality and η ≤

2(τ + ∥Q∥∞)
−1."
REFERENCES,0.28456375838926173,"Similarly, it can be proved that"
REFERENCES,0.28523489932885904,"hτ(µ⋆
τ, ν⋆
τ ) −hτ(µt+1, ν) ≤τKL(µt+1∥µ⋆
τ) + τ −1∥Q∥2
∞KL(µ⋆
τ∥µt+1).
(51)"
REFERENCES,0.2859060402684564,"Therefore, the duality gap (30) can be proved as follows."
REFERENCES,0.2865771812080537,"max
µ∈∆(d1) hτ(µ, νt+1) −
min
ν∈∆(d2) hτ(µt+1, ν)"
REFERENCES,0.287248322147651,"=
max
µ∈∆(d1),ν∈∆(d2)"
REFERENCES,0.2879194630872483,"
hτ(µ, νt+1) −hτ(µt+1, ν)
"
REFERENCES,0.28859060402684567,Published as a conference paper at ICLR 2022
REFERENCES,0.28926174496644297,"(i)
≤τ[KL(µt+1∥µ⋆
τ) + KL(νt+1∥ν⋆
τ )] + τ −1∥Q∥2
∞[KL(µ⋆
τ∥µt+1) + KL(ν⋆
τ ∥νt+1)]"
REFERENCES,0.28993288590604027,"(ii)
≤τ[KL(µt+1∥µ⋆
τ) + KL(νt+1∥ν⋆
τ )] + τ −1∥Q∥2
∞
h
4

KL(µ∗
τ∥µt) + KL(ν∗
τ ∥νt)

+ 24η2 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞

+ 16η"
REFERENCES,0.2906040268456376,"τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞
i"
REFERENCES,0.2912751677852349,"(iii)
≤max
2"
REFERENCES,0.29194630872483224,"η , 4∥Q∥2
∞
τ "
REFERENCES,0.29261744966442954,"h
KL(µ∗
τ∥µt) + KL(ν∗
τ ∥νt) + 2η2 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞

+ 2η"
REFERENCES,0.29328859060402684,"τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞
i"
REFERENCES,0.29395973154362415,"+ τ −1∥Q∥2
∞
h
24η2 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞

+ 16η"
REFERENCES,0.29463087248322145,"τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞
i"
REFERENCES,0.2953020134228188,"(iv)
≤2 η"
REFERENCES,0.2959731543624161,"
(1 −ητ)t ln(d1d2) + t−1
X"
REFERENCES,0.2966442953020134,"j=0
(1 −ητ)t−1−jh
2η2 
∥δ(1)
j ∥2
∞+ ∥δ(2)
j ∥2
∞

+ 2η"
REFERENCES,0.2973154362416107,"τ
 
∥δ
(1)
j ∥2
∞+ ∥δ
(2)
j ∥2
∞
i"
REFERENCES,0.2979865771812081,"+ 16η
 
∥δ(1)
t
∥2
∞+ ∥δ(2)
t
∥2
∞

+ 12"
REFERENCES,0.2986577181208054,"τ
 
∥δ
(1)
t ∥2
∞+ ∥δ
(2)
t ∥2
∞
 ≤2"
REFERENCES,0.2993288590604027,"η (1 −ητ)t ln(d1d2) + t
X"
REFERENCES,0.3,"j=0
(1 −ητ)t−1−jh
16η
 
∥δ(1)
j ∥2
∞+ ∥δ(2)
j ∥2
∞

+ 12"
REFERENCES,0.3006711409395973,"τ
 
∥δ
(1)
j ∥2
∞+ ∥δ
(2)
j ∥2
∞
i
,
(52)"
REFERENCES,0.30134228187919465,"where (i) adds up eqs. (50) & (51), (ii) uses eq. (47), (iii) uses eq. (39), and (iv) uses eq. (28) and
4τ −1∥Q∥2
∞≤2/η (since η ≤

2(τ + ∥Q∥∞)
−1, τ ≤1). This proves eq. (30)"
REFERENCES,0.30201342281879195,"B
ESTIMATION ERROR BOUNDS"
REFERENCES,0.30268456375838926,"In this section, we derive error bounds for the estimators bVk+1 ≈V ′
k+1, bQ(m)
k,t ≈Q(m)
k,t , bQ
(m)"
REFERENCES,0.30335570469798656,"k,t ≈Q
(m)
k,t
used in Algorithm 1. For convenience, we deﬁne the following additional notations."
REFERENCES,0.3040268456375839,"bµk(s) :=
1
Nk+1 X"
REFERENCES,0.3046979865771812,"i∈Nk+1
1{si = s} ≈µk(s)
(53)"
REFERENCES,0.3053691275167785,"vk+1(s) := Eµk,π(1)
k
,π(2)
k"
REFERENCES,0.30604026845637583,"h
R(es, ea(1), ea(2)) + γV ′
k(s′)

1{es = s}
i
(54)"
REFERENCES,0.30671140939597313,"v′
k+1(s) := Eµk,π(1)
k
,π(2)
k"
REFERENCES,0.3073825503355705,"h
R(es, ea(1), ea(2)) + γ bVk(s′)

1{es = s}
i
(55)"
REFERENCES,0.3080536912751678,"bvk+1(s) :=
1
Nk+1 X"
REFERENCES,0.3087248322147651,i∈Nk+1
REFERENCES,0.3093959731543624,"h
Ri + γ bVk(si+1)

1{si = s}
i
≈v′
k+1(s) ≈vk+1(s)
(56)"
REFERENCES,0.3100671140939597,"Qk(s, a(1), a(2)) := R(s, a(1), a(2)) + γEs′∼P(·|s,a(1),a(2))

V ′
k(s′)

(57)"
REFERENCES,0.31073825503355706,"V ′
k+1(s) = fτ
 
Qk(s); π(1)
k (s), π(2)
k (s)

= vk+1(s)"
REFERENCES,0.31140939597315437,"µk(s) + τH
 
π(1)
k (s)

−τH
 
π(2)
k (s)

(58)"
REFERENCES,0.31208053691275167,bVk+1(s) := bvk+1(s)
REFERENCES,0.312751677852349,"bµk(s) + τH
 
π(1)
k (s)

−τH
 
π(2)
k (s)

≈V ′
k+1(s)
(59)"
REFERENCES,0.31342281879194633,"bµk,t(s) :=
1
Nk,t X"
REFERENCES,0.31409395973154364,"i∈Nk,t
1{si = s} ≈µ′
k,t(s) ≈µk,t(s),
(60)"
REFERENCES,0.31476510067114094,"q(m)
k,t (s, a(m)) := Eµ′
k,t,π′(1)
k,t ,π′(2)
k,t"
REFERENCES,0.31543624161073824,"h
R(es, ea(1), ea(2)) + γV ′
k(s′)

1{es = s, ea(m) = a(m)}
i
(61)"
REFERENCES,0.31610738255033555,"q′(m)
k,t (s, a(m)) := Eµ′
k,t,π′(1)
k,t ,π′(2)
k,t"
REFERENCES,0.3167785234899329,"h
R(es, ea(1), ea(2)) + γ bVk(s′)

1{es = s, ea(m) = a(m)}
i
(62)"
REFERENCES,0.3174496644295302,Published as a conference paper at ICLR 2022
REFERENCES,0.3181208053691275,"bq(m)
k,t (s, a(m)) :=
1
Nk,t X"
REFERENCES,0.3187919463087248,"i∈Nk,t"
REFERENCES,0.3194630872483222,"h
Ri + γ bVk(si+1)

1{si = s, a(m)
i
= a(m)}
i"
REFERENCES,0.3201342281879195,"≈q′(m)
k,t (s, a(m)) ≈q(m)
k,t (s, a(m))
(63)"
REFERENCES,0.3208053691275168,"Q′(m)
k,t (s, a(m)) := Ea(\m)∼π′(\m)
k,t
(s)

Qk(s, a(1), a(2))

=
q(m)
k,t (s, a(m))"
REFERENCES,0.3214765100671141,"µ′
k,t(s)π′(m)
k,t (a(m)|s)
(64)"
REFERENCES,0.3221476510067114,"bQ(m)
k,t (s, a(m)) :=
bq(m)
k,t (s, a(m))"
REFERENCES,0.32281879194630875,"bµk,t(s)π′(m)
k,t (a(m)|s)
≈Q′(m)
k,t (s, a(m)) ≈Q(m)
k,t (s, a(m)),
(65)"
REFERENCES,0.32348993288590605,"where eqs. (53)-(59) and (60)-(65) are introduced for the description and error bound proof of
the estimations bVk+1 ≈V ′
k+1 and bQ(m)
k,t
≈Q(m)
k,t , respectively. Speciﬁcally, eq. (53) estimates"
REFERENCES,0.32416107382550335,"the stationary state distribution µk associated with the policy pair π(1)
k , π(2)
k
using state frequency
bµk(s). In eqs. (54)&(55), the expectation is taken over es ∼µk, ea(1) ∼π(1)
k (s), ea(2) ∼π(2)
k (s), s′ ∼
P(·|es, ea(1), ea(2)). The deﬁnition of Eµ′
k,t,π′(1)
k,t ,π′(2)
k,t in eqs. (61)&(62) is similar. Equations (58)"
REFERENCES,0.32483221476510066,"& (64) give two equivalent deﬁnitions of V ′
k+1 and Q′(m)
k,t (s, a(m)), respectively 1 (We will prove
the equivalence in Lemma 2 below). Equation (60) estimates the stationary state distribution µ′
k,t
associated with the smoothed policy pair π′(1)
k,t , π′(2)
k,t , and thus approximates the stationary state"
REFERENCES,0.32550335570469796,"distribution µk,t associated with the current policy pair π(1)
k,t, π(2)
k,t."
REFERENCES,0.3261744966442953,"To prove the estimation error bounds, we ﬁrst prove the following two lemmas."
REFERENCES,0.3268456375838926,"Lemma 2. Eq. (13) and the second “=” of eqs. (58) & (64) hold for all s, a(1), a(2)."
REFERENCES,0.3275167785234899,Proof. The second “=” of eq. (64) under m = 1 can be proved as follows.
REFERENCES,0.32818791946308723,"q(1)
k,t(s, a(1))"
REFERENCES,0.3288590604026846,"(i)
= Eµ′
k,t,π′(1)
k,t ,π′(2)
k,t"
REFERENCES,0.3295302013422819,"h
R(es, ea(1), ea(2)) + γV ′
k(s′)

1{es = s, ea(m) = a(m)}
i"
REFERENCES,0.3302013422818792,"= Ees∼µ′
k,t,ea(1)∼π′(1)
k,t (s),ea(2)∼π′(2)
k,t (s),s′∼P(·|s,a(1),ea(2))
h
R(s, a(1), ea(2)) + γV ′
k(s′)

1{es = s, ea(1) = a(1)}
i"
REFERENCES,0.3308724832214765,"(ii)
= Ees∼µ′
k,t,ea(1)∼π′(1)
k,t (s)

1{es = s, ea(1) = a(1)}
"
REFERENCES,0.3315436241610738,"Eea(2)∼π′(2)
k,t (s),s′∼P(·|s,a(1),ea(2))

R(s, a(1), ea(2)) + γV ′
k(s′)
"
REFERENCES,0.33221476510067116,"(iii)
= µ′
k,t(s)π′(1)
k,t (a(1)|s)Eea(2)∼π′(2)
k,t (s)

Qk(s, a(1), ea(2))
"
REFERENCES,0.33288590604026846,"(iv)
= µ′
k,t(s)π′(1)
k,t (a(1)|s)Q′
k,t(s, a(1)),"
REFERENCES,0.33355704697986577,"where (i) uses eq. (61) which denotes Eµ′
k,t,π′(1)
k,t ,π′(2)
k,t as expectation over es ∼µ′
k,t, ea(1) ∼π′(1)
k,t (es),"
REFERENCES,0.33422818791946307,"ea(2) ∼π′(2)
k,t (es), s′ ∼P(·|es, ea(1), ea(2)), (ii) uses independence between es ∼µ′
k,t, ea(1) ∼π′(1)
k,t (s)"
REFERENCES,0.33489932885906043,"and ea(2) ∼π′(2)
k,t (s), s′ ∼P(·|s, a(1), ea(2)), (iii) uses eq. (57), and (iv) uses the ﬁrst deﬁnition of eq.
(64). The second “=” of eq. (64) under m = 2 and eq. (13) can be proved in a similar way."
REFERENCES,0.33557046979865773,The second “=” of eq. (58) can be proved as follows.
REFERENCES,0.33624161073825504,vk+1(s)
REFERENCES,0.33691275167785234,"(i)
= Eµk,π(1)
k
,π(2)
k"
REFERENCES,0.33758389261744964,"h
R(es, ea(1), ea(2)) + γV ′
k(s′)

1{es = s}
i"
REFERENCES,0.338255033557047,"= Ees∼µk,ea(1)∼π(1)
k
(s),ea(2)∼π(2)
k
(s),s′∼P(·|s,ea(1),ea(2))"
REFERENCES,0.3389261744966443,"h
R(s, ea(1), ea(2)) + γV ′
k(s′)

1{es = s}
i"
REFERENCES,0.3395973154362416,1The deﬁnition after the second “=” of eq. (58) is the same as eq. (11)
REFERENCES,0.3402684563758389,Published as a conference paper at ICLR 2022
REFERENCES,0.3409395973154362,"(ii)
= Ees∼µk

1{es = s}

Eea(1)∼π(1)
k
(s),ea(2)∼π(2)
k
(s),s′∼P(·|s,ea(1),ea(2))

R(s, ea(1), ea(2)) + γV ′
k(s′)
"
REFERENCES,0.3416107382550336,"(iii)
= µk(s)Eea(1)∼π(1)
k
(s),ea(2)∼π(2)
k
(s)

Qk(s, ea(1), ea(2))
"
REFERENCES,0.3422818791946309,"= µk,t(s)

fτ
 
Qk(s); π(1)
k (s), π(2)
k (s)

−τH
 
π(1)
k (s)

+ τH
 
π(2)
k (s)
"
REFERENCES,0.3429530201342282,"(iv)
= µk(s)

V ′
k+1(s) −τH
 
π(1)
k (s)

+ τH
 
π(2)
k (s)
"
REFERENCES,0.3436241610738255,"where (i) uses eq. (54), (ii) uses independence between s′ ∼µk and ea(1) ∼π(1)
k (s), ea(2) ∼
π(2)
k (s), s′ ∼P(·|s, ea(1), ea(2)), (iii) uses eq. (57), and (iv) uses eq. (58)."
REFERENCES,0.34429530201342284,Lemma 3. If ∥bV0∥∞≤Vmax := 1+τ ln Amax
REFERENCES,0.34496644295302015,"1−γ
, then for all k ≥0, 0 ≤t ≤Tk −1, we have"
REFERENCES,0.34563758389261745,"∥bVk∥∞, ∥V ′
k∥∞, ∥V ∗∥∞≤Vmax,
(66)
|bvk+1(s)| ≤2Vmaxbµk(s), ∀s ∈S,
(67)"
REFERENCES,0.34630872483221475,"max
 
∥Qk∥∞, ∥Q′(m)
k,t ∥∞, ∥Q∗∥∞

≤Qmax,
(68)"
REFERENCES,0.34697986577181206,where Qmax := 1 + γVmax = 1+γτ ln Amax
REFERENCES,0.3476510067114094,"1−γ
with Amax := max
 
|A(1)|, |A(2)|

."
REFERENCES,0.3483221476510067,"Proof. We will ﬁrst prove ∥bVk∥∞≤Vmax in eq. (66) by induction. Suppose ∥bVk′∥∞≤Vmax holds
for a certain k′ ∈N. Then, eq. (59) implies that for all s ∈S,"
REFERENCES,0.348993288590604,"|bVk′+1(s)| ≤
bvk′+1(s)"
REFERENCES,0.3496644295302013,"bµ′
k′(s)"
REFERENCES,0.3503355704697987,"+ τ

H
 
π(1)
k′ (s)

−H
 
π(2)
k′ (s)
 ≤ P"
REFERENCES,0.351006711409396,i∈Nk′+1
REFERENCES,0.3516778523489933,"h
|Ri| + γ|bVk′(si+1)|

1{si = s}
i"
REFERENCES,0.3523489932885906,"P
i∈Nk′+1 1{si = s}
+ τ ln Amax (i)
≤ P"
REFERENCES,0.3530201342281879,i∈Nk′+1
REFERENCES,0.35369127516778526,"h 
1 + γVmax

1{si = s}
i P"
REFERENCES,0.35436241610738256,"i∈Nk′+1 1{si = s}
+ τ ln Amax"
REFERENCES,0.35503355704697986,=1 + γ(1 + τ ln Amax)
REFERENCES,0.35570469798657717,"1 −γ
+ τ ln Amax = Vmax,
(69)"
REFERENCES,0.35637583892617447,"where (i) uses the inequality that 0 ≤H(π(m)) ≤ln |A(m)| ≤ln Amax, ∀π(m) ∈∆(|A(m)|). Since
∥bV0∥∞≤Vmax, ∥bVk∥∞≤Vmax for all k ∈N. A similar induction yields that ∥V ′
k∥∞≤Vmax."
REFERENCES,0.35704697986577183,"Next, we prove ∥V ∗∥∞≤Vmax, ∥Q∗∥∞≤Qmax in eqs. (66) & (68) respectively. Notice that V ∗
and Q∗have the following relation."
REFERENCES,0.35771812080536913,"Q∗(s, a(1), a(2)) = R(s, a(1), a(2)) + γEs′∼P(·|s,a(1),a(2))V ∗(s′),"
REFERENCES,0.35838926174496644,"V ∗(s) = π∗(1)(s)⊤Q∗(s)π∗(2)(s) + τ

H(π∗(1)(s)) −H(π∗(2)(s))

."
REFERENCES,0.35906040268456374,"Hence,"
REFERENCES,0.3597315436241611,"∥Q∗∥∞≤1 + γ∥V ∗∥∞
(70)
∥V ∗∥∞≤∥Q∗∥∞+ τ ln Amax
(71)"
REFERENCES,0.3604026845637584,Substituting eq. (70) into eq. (71) yields that
REFERENCES,0.3610738255033557,∥V ∗∥∞≤1 + γ∥V ∗∥∞+ τ ln Amax ⇒∥V ∗∥∞≤Vmax.
REFERENCES,0.361744966442953,"The proof of eq. (66) is ﬁnished. Then, substituting ∥V ∗∥∞≤Vmax into eq. (70) proves that
∥Q∗∥∞≤Qmax."
REFERENCES,0.3624161073825503,"Equation (67) can be proved as follows.
bvk+1(s)"
REFERENCES,0.36308724832214767,bµk(s)
REFERENCES,0.363758389261745,"(i)
≤|bVk+1(s)| + τ
H
 
π(2)
k (s)

−H
 
π(1)
k (s)
 ≤Vmax + τ ln Amax"
REFERENCES,0.3644295302013423,"(ii)
≤2Vmax,"
REFERENCES,0.3651006711409396,Published as a conference paper at ICLR 2022
REFERENCES,0.36577181208053694,where (i) uses eq. (58) and (ii) uses Vmax := 1+τ ln Amax
REFERENCES,0.36644295302013424,"1−γ
≥τ ln Amax."
REFERENCES,0.36711409395973155,"Next, we prove eq. (68). It can be easily seen from eqs. (57) & (66) that
Qk(s, a(1), a(2))
 ≤
R(s, a(1), a(2))
 + γEs′∼P(·|s,a(1),a(2))
V ′
k(s′)
 ≤1 + γVmax = Qmax,"
REFERENCES,0.36778523489932885,"which implies that ∥Qk∥∞≤Qmax. Hence,"
REFERENCES,0.36845637583892615,"Q′(m)
k,t (s, a(m))

(i)
≤Ea(\m)∼π′(\m)
k,t
(s)
Qk(s, a(1), a(2))
 ≤Qmax"
REFERENCES,0.3691275167785235,where (i) uses eq. (64). This proves eq. (68).
REFERENCES,0.3697986577181208,"With the above two Lemmas, we can derive the estimation error bounds as follows."
REFERENCES,0.3704697986577181,"Lemma 4. Use hyperparameter choices Nk,t, N k,t ≥650tmixAmax"
REFERENCES,0.3711409395973154,"µmin
ln

20Tsum|S|Amax"
REFERENCES,0.3718120805369127,δ√µmin
REFERENCES,0.3724832214765101,"
and Nk+1 ≥"
TMIX,0.3731543624161074,"650tmix
µmin(1−γ)2 ln

4
δ√µmin"
TMIX,0.3738255033557047,"
in Algorithm 1. Then with probability at least 1 −δ, all the following"
TMIX,0.374496644295302,"bounds hold for all 0 ≤k ≤K −1, 0 ≤t ≤Tk −1, m = 1, 2, s ∈S, a(m) ∈A(m), where
Tsum := PK−1
k=0 Tk."
TMIX,0.37516778523489935,"bVk −V ′
k

∞≤2Vmaxγk + 171Vmax 1 −γ s"
TMIX,0.37583892617449666,"tmix
Nk+1µmin
ln
 4K|S|"
TMIX,0.37651006711409396,δ√µmin
TMIX,0.37718120805369126,"
(72)"
TMIX,0.37785234899328857,"bQ(m)
k,t −Q(m)
k,t

∞ ≤Qmax"
TMIX,0.3785234899328859,"""
640tmixAmax"
TMIX,0.37919463087248323,"Nk,tµminϵ′ ln
20Tsum|S|Amax"
TMIX,0.37986577181208053,δ√µmin
TMIX,0.38053691275167784,"
+ 25 s"
TMIX,0.3812080536912752,"tmixAmax
Nk,tµminϵ′ ln
20Tsum|S|Amax"
TMIX,0.3818791946308725,δ√µmin
TMIX,0.3825503355704698,"
+ ϵ′
#"
TMIX,0.3832214765100671,"+ 2
bVk −V ′
k

∞
(73)
bQ
(m)"
TMIX,0.3838926174496644,"k,t −Q
(m)
k,t

∞ ≤Qmax"
TMIX,0.38456375838926177,"""
640tmixAmax"
TMIX,0.38523489932885907,"N k,tµminϵ′ ln
20Tsum|S|Amax"
TMIX,0.3859060402684564,δ√µmin
TMIX,0.3865771812080537,"
+ 25 s"
TMIX,0.387248322147651,"tmixAmax
N k,tµminϵ′ ln
20Tsum|S|Amax"
TMIX,0.38791946308724834,δ√µmin
TMIX,0.38859060402684564,"
+ ϵ′
#"
TMIX,0.38926174496644295,"+ 2
bVk −V ′
k

∞.
(74)"
TMIX,0.38993288590604025,"More speciﬁcally, the upper bounds (73)&(74) can be simpliﬁed respectively as follows when ϵ′ =
h
650tmixAmax"
TMIX,0.3906040268456376,"Nk,tµmin ln

20Tsum|S|Amax"
TMIX,0.3912751677852349,δ√µmin
TMIX,0.3919463087248322,"i1/3
and ϵ′ =
h
650tmixAmax"
TMIX,0.3926174496644295,"N k,tµmin ln

20Tsum|S|Amax"
TMIX,0.3932885906040268,δ√µmin
TMIX,0.3939597315436242,"i1/3
."
TMIX,0.3946308724832215,"bQ(m)
k,t −Q(m)
k,t

∞≤18Qmax
h tmixAmax"
TMIX,0.3953020134228188,"Nk,tµmin
ln
20Tsum|S|Amax"
TMIX,0.3959731543624161,δ√µmin
TMIX,0.39664429530201345,"i1/3
+ 2
bVk −V ′
k

∞,
(75)"
TMIX,0.39731543624161075,"bQ
(m)"
TMIX,0.39798657718120806,"k,t −Q
(m)
k,t

∞≤18Qmax"
TMIX,0.39865771812080536, tmixAmax
TMIX,0.39932885906040266,"N k,tµmin
ln
20Tsum|S|Amax"
TMIX,0.4,δ√µmin
TMIX,0.4006711409395973,"1/3
+ 2
bVk −V ′
k

∞.
(76)"
TMIX,0.40134228187919463,"Proof. If the initial state distribution of the minibatch Nk,t equals µ′
k,t, i.e., sk,t,0 ∼µ′
k,t,"
TMIX,0.40201342281879193,"then Nk,tbq(m)
k,t (s, a(m))
:=
P"
TMIX,0.40268456375838924,"i∈Nk,t g(si, a(1)
i , a(2)
i , si+1) (g(si, a(1)
i , a(2)
i , si+1)
:=

Ri +"
TMIX,0.4033557046979866,"γ bVk(si+1)

1{si = s, a(m)
i
= a(m)}) and its expectated value Nk,tq′(m)
k,t (s, a(m)) satisfy the fol-
lowing concentration bound."
TMIX,0.4040268456375839,"Pµ′
k,t
Nk,tbq(m)
k,t (s, a(m)) −Nk,tq′(m)
k,t (s, a(m))
 ≥u"
TMIX,0.4046979865771812,"(i)
≤2 exp
h
−
u2γps
8(Nk,t + 1/γps)Q2maxµ′
k,t(s)π′(m)
k,t (a(m)|s) + 40uQmax i"
TMIX,0.4053691275167785,"(ii)
≤2 exp
h
−
u2/(2tmix)"
TMIX,0.40604026845637586,"8(Nk,t + 2tmix)Q2maxµ′
k,t(s)π′(m)
k,t (a(m)|s) + 40uQmax"
TMIX,0.40671140939597317,"i
,
(77)"
TMIX,0.40738255033557047,Published as a conference paper at ICLR 2022
TMIX,0.4080536912751678,"where Pµ′
k,t is under the initial state distribution µ′
k,t, (i) uses Theorem 3.4 of (Paulin,"
TMIX,0.4087248322147651,"2015) and the inequalities that
g(si, a(1)
i , a(2)
i , si+1) −Esi∼µ′
k,tg(si, a(1)
i , a(2)
i , si+1)
 ≤2(1 +"
TMIX,0.40939597315436244,"γVmax)
=
2Qmax (Based on Lemma 3) and that varsi∼µ′
k,t

g(si, a(1)
i , a(2)
i , si+1)

≤"
TMIX,0.41006711409395974,"Esi∼µ′
k,t"
TMIX,0.41073825503355704,"h
Ri + γV ′
k(si+1)
21{si = s, a(m)
i
= a(m)}
i
≤(1 + γVmax)2µ′
k,t(s)π′(m)
k,t (a(m)|s) ="
TMIX,0.41140939597315435,"Q2
maxµ′
k,t(s)π′(m)
k,t (a(m)|s), (ii) uses Proposition 3.4 of (Paulin, 2015) which states that the pseudo
spectral gap γps has a lower bound 1/(2tmix) for any uniformly ergodic Markov chain (This condition
holds for our MDP with ﬁnitely many states and actions). Then, for any initial state distribution
st,k,0 ∼ξ, Proposition 3.10 of (Paulin, 2015) implies that"
TMIX,0.4120805369127517,"Pξ
Nk,tbq(m)
k,t (s, a(m)) −Nk,tq′(m)
k,t (s, a(m))
 ≥u ≤ s"
TMIX,0.412751677852349,"Es∼ξ
h ξ(s)"
TMIX,0.4134228187919463,"µ′
k,t(s)"
TMIX,0.4140939597315436,"i
Pµ′
k,t
Nk,tbq(m)
k,t (s, a(m)) −Nk,tq′(m)
k,t (s, a(m))
 ≥u"
TMIX,0.4147651006711409,"(i)
≤
r
1
µmin
exp
h
−
u2/(4tmix)"
TMIX,0.4154362416107383,"8(Nk,t + 2tmix)Q2maxµ′
k,t(s)π′(m)
k,t (a(m)|s) + 40uQmax"
TMIX,0.4161073825503356,"i
,
(78)"
TMIX,0.4167785234899329,"where (i) uses eq. (77) and the inequality that Es∼ξ
h
ξ(s)
µ′
k,t(s)
i
≤
1
µmin ."
TMIX,0.4174496644295302,"In a similar way, the following concentration bound can be proved for Nk,tbµk,t(s)
:=
P
i∈Nk,t 1{si = s} and Nk,tµ′
k,t(s) = Eµ′
k,t

Nk,tbµk,t(s)

."
TMIX,0.4181208053691275,"Pξ
Nk,tbµk,t(s) −Nk,tµ′
k,t(s)
 ≥u
	
≤
r
1
µmin
exp
h
−
u2/(4tmix)
8(Nk,t + 2tmix)µ′
k,t(s) + 40u"
TMIX,0.41879194630872485,"i
, (79)"
TMIX,0.41946308724832215,"where we use the inequalities that varµ′
k,t1{si = s} ≤µ′
k,t(s) and that
1{si = s} −Eµ′
k,t1{si =
s}
 ≤1. Letting the right hand sides of eqs. (78)&(79) be upper bounded by δ/4 and applying the
union bound yields that, with probability at least 1−δ/2, the following two inequalities simultaneously
hold.
bq(m)
k,t (s, a(m)) −q′(m)
k,t (s, a(m))"
TMIX,0.42013422818791946,≤160Qmax
TMIX,0.42080536912751676,"Nk,t
tmix ln

4
δ√µmin "
TMIX,0.4214765100671141,"+ 6Qmax
p Nk,t s"
TMIX,0.4221476510067114,"tmix(1 + 2tmix/Nk,t)µ′
k,t(s)π′(m)
k,t (a(m)|s) ln

4
δ√µmin"
TMIX,0.4228187919463087,"
,
(80)"
TMIX,0.42348993288590603,"bµk,t(s) −µ′
k,t(s) ≤160"
TMIX,0.42416107382550333,"Nk,t
tmix ln

4
δ√µmin  +
6
p Nk,t s"
TMIX,0.4248322147651007,"tmix(1 + 2tmix/Nk,t)µ′
k,t(s) ln

4
δ√µmin"
TMIX,0.425503355704698," (i)
≤1"
TMIX,0.4261744966442953,"2µ′
k,t(s),
(81)"
TMIX,0.4268456375838926,"where (i) holds since Nk,t ≥650tmixAmax"
TMIX,0.42751677852348996,"µmin
ln

20Tsum|S|Amax"
TMIX,0.42818791946308726,δ√µmin
TMIX,0.42885906040268457,"
≥650tmix and µ′
k,t(s) ≥µmin."
TMIX,0.42953020134228187,"Similarly, it can be proved that the following two inequalities holds with probability at least 1 −δ/2.
bvk+1(s) −v′
k+1(s)"
TMIX,0.4302013422818792,≤160Qmax
TMIX,0.43087248322147653,"Nk+1
tmix ln

4
δ√µmin"
TMIX,0.43154362416107384,"
+ 6Qmax
p Nk+1 s"
TMIX,0.43221476510067114,"tmix(1 + 2tmix/Nk+1)µk(s) ln

4
δ√µmin"
TMIX,0.43288590604026844,"
,
(82)"
TMIX,0.43355704697986575,bµk(s) −µk(s)
TMIX,0.4342281879194631,"≤
160
Nk+1
tmix ln

4
δ√µmin"
TMIX,0.4348993288590604,"
+
6
p Nk+1 s"
TMIX,0.4355704697986577,"tmix(1 + 2tmix/Nk+1)µk(s) ln

4
δ√µmin"
TMIX,0.436241610738255,"
.
(83)"
TMIX,0.4369127516778524,Published as a conference paper at ICLR 2022
TMIX,0.4375838926174497,"Hence, eqs. (80)-(83) hold with probability at least 1 −δ. In this case, we have that
bvk+1(s) −vk+1(s)"
TMIX,0.438255033557047,"≤
bvk+1(s) −v′
k+1(s)
 +
v′
k+1(s) −vk+1(s)"
TMIX,0.4389261744966443,"(i)
≤160Qmax"
TMIX,0.4395973154362416,"Nk+1
tmix ln

4
δ√µmin"
TMIX,0.44026845637583895,"
+ 6Qmax
p Nk+1 s"
TMIX,0.44093959731543625,"tmix(1 + 2tmix/Nk+1)µk(s) ln

4
δ√µmin "
TMIX,0.44161073825503355,"+ γEµk,π(1)
k
,π(2)
k"
TMIX,0.44228187919463086,"hbVk(s′) −V ′
k(s′)

1{es = s}
i"
TMIX,0.4429530201342282,≤160Qmax
TMIX,0.4436241610738255,"Nk+1
tmix ln

4
δ√µmin"
TMIX,0.4442953020134228,"
+ 6Qmax
p Nk+1 s"
TMIX,0.4449664429530201,"tmix(1 + 2tmix/Nk+1)µk(s) ln

4
δ√µmin "
TMIX,0.44563758389261743,"+ γ max
s′
bVk(s′) −V ′
k(s′)
Eµk1{es = s}"
TMIX,0.4463087248322148,≤160Qmax
TMIX,0.4469798657718121,"Nk+1
tmix ln

4
δ√µmin"
TMIX,0.4476510067114094,"
+ 6Qmax
p Nk+1 s"
TMIX,0.4483221476510067,"tmix(1 + 2tmix/Nk+1)µk(s) ln

4
δ√µmin "
TMIX,0.448993288590604,"+ γµk(s)
bVk −V ′
k

∞,
(84)
where (i) uses eqs. (54), (55) & (82)."
TMIX,0.44966442953020136,"|bVk+1(s) −V ′
k+1(s)|"
TMIX,0.45033557046979866,"(i)
=
bvk+1(s)"
TMIX,0.45100671140939597,bµk(s) −vk+1(s) µk(s) 
TMIX,0.45167785234899327,"≤µ−1
k (s)
bvk+1(s) −vk+1(s)
 +
bvk+1(s)

µk(s) −bµk(s)"
TMIX,0.45234899328859063,µk(s)bµk(s) 
TMIX,0.45302013422818793,"(ii)
≤µ−1
k (s)
h160Qmax"
TMIX,0.45369127516778524,"Nk+1
tmix ln

4
δ√µmin"
TMIX,0.45436241610738254,"
+ 6Qmax
p Nk+1 s"
TMIX,0.45503355704697984,"tmix(1 + 2tmix/Nk+1)µk(s) ln

4
δ√µmin "
TMIX,0.4557046979865772,"+ γµk(s)
bVk −V ′
k

∞"
TMIX,0.4563758389261745,"i
+ 2Vmaxµ−1
k (s)"
TMIX,0.4570469798657718,"""
160
Nk+1
tmix ln

4
δ√µmin  +
6
p Nk+1 s"
TMIX,0.4577181208053691,"tmix(1 + 2tmix/Nk+1)µk(s) ln

4
δ√µmin  #"
TMIX,0.45838926174496647,"(iii)
≤γ∥bVk −V ′
k∥∞+ Vmax µk(s)"
TMIX,0.4590604026845638,"""
480tmix"
TMIX,0.4597315436241611,"Nk+1
ln

4
δ√µmin  + 18 s"
TMIX,0.4604026845637584,tmixµk(s)
TMIX,0.4610738255033557,"Nk+1
(1 + 2tmix/Nk+1) ln

4
δ√µmin  #"
TMIX,0.46174496644295304,"(iv)
≤γ∥bVk −V ′
k∥∞+ Vmax """
TMIX,0.46241610738255035,"24
40tmix
Nk+1µmin
ln

4
δ√µmin"
TMIX,0.46308724832214765,"
+ 18 s"
TMIX,0.46375838926174495,"1.05tmix
Nk+1µmin
ln

4
δ√µmin  #"
TMIX,0.46442953020134226,"(v)
≤γ∥bVk −V ′
k∥∞+ Vmax "" 24 s"
TMIX,0.4651006711409396,"40tmix
Nk+1µmin
ln

4
δ√µmin"
TMIX,0.4657718120805369,"
+ 18 s"
TMIX,0.4664429530201342,"1.05tmix
Nk+1µmin
ln

4
δ√µmin  #"
TMIX,0.4671140939597315,"≤γ∥bVk −V ′
k∥∞+ 171Vmax s"
TMIX,0.4677852348993289,"tmix
Nk+1µmin
ln

4
δ√µmin"
TMIX,0.4684563758389262,"
,
(85)"
TMIX,0.4691275167785235,"where (i) uses eqs. (58)&(59), (ii) uses eqs. (67), (83)&(84), (iii) uses Qmax ≤Vmax, (iv) uses
µk(s) ≥µmin and Nk+1 ≥40tmix"
TMIX,0.4697986577181208,"µmin ln

4
δ√µmin"
TMIX,0.4704697986577181,"
≥40tmix, and (v) uses Nk+1 ≥40tmix"
TMIX,0.47114093959731546,"µmin ln

4
δ√µmin"
TMIX,0.47181208053691276,"
.
Applying the union bound to the above inequality over all 0 ≤k ≤K −1, s ∈S and taking
maximum over s ∈S, we obtain that with probability at least 1 −δ,"
TMIX,0.47248322147651006,"∥bVk+1 −V ′
k+1∥∞≤γ∥bVk −V ′
k∥∞+ 171Vmax s"
TMIX,0.47315436241610737,"tmix
Nk+1µmin
ln

4
δ√µmin 
."
TMIX,0.4738255033557047,Published as a conference paper at ICLR 2022
TMIX,0.47449664429530203,"Iterating the above inequality yields that with probability at least 1 −δ,"
TMIX,0.47516778523489933,"∥bVk −V ′
k∥∞≤γk∥bV0 −V ′
0∥∞+ 171Vmax 1 −γ s"
TMIX,0.47583892617449663,"tmix
Nk+1µmin
ln
 4K|S|"
TMIX,0.47651006711409394,δ√µmin 
TMIX,0.4771812080536913,"(i)
≤2Vmaxγk + 171Vmax 1 −γ s"
TMIX,0.4778523489932886,"tmix
Nk+1µmin
ln
 4K|S|"
TMIX,0.4785234899328859,δ√µmin
TMIX,0.4791946308724832,"
,
(86)"
TMIX,0.4798657718120805,"where (i) uses the fact that ∥V ′
0∥∞, ∥bV0∥∞≤Vmax."
TMIX,0.48053691275167787,"Hence, with probability at least 1 −2δ, eqs. (80), (81) & (86) hold simultaneously. In this case,"
TMIX,0.4812080536912752,"bQ(m)
k,t (s, a(m)) −Q′(m)
k,t (s, a(m))
 (i)
=

bq(m)
k,t (s, a(m))"
TMIX,0.4818791946308725,"bµk,t(s)π′(m)
k,t (a(m)|s)
−
q(m)
k,t (s, a(m))"
TMIX,0.4825503355704698,"µ′
k,t(s)π′(m)
k,t (a(m)|s) "
TMIX,0.48322147651006714,"(ii)
≤"
TMIX,0.48389261744966444,"bq(m)
k,t (s, a(m)) −q(m)
k,t (s, a(m))"
TMIX,0.48456375838926175,"bµk,t(s)π′(m)
k,t (a(m)|s)"
TMIX,0.48523489932885905,"+ |µ′
k,t(s)π′(m)
k,t (a(m)|s)Q′(m)
k,t (s, a(m))|"
TMIX,0.48590604026845635,"bµk,t(s) −µ′
k,t(s)"
TMIX,0.4865771812080537,"bµk,t(s)µ′
k,t(s)π′(m)
k,t (a(m)|s)"
TMIX,0.487248322147651,"(iii)
≤
2
bq(m)
k,t (s, a(m)) −q′(m)
k,t (s, a(m))
 + 2
q′(m)
k,t (s, a(m)) −q(m)
k,t (s, a(m))"
TMIX,0.4879194630872483,"µ′
k,t(s)π′(m)
k,t (a(m)|s)"
TMIX,0.4885906040268456,"+
2Qmax
bµk,t(s) −µ′
k,t(s)"
TMIX,0.489261744966443,"µ′
k,t(s)"
TMIX,0.4899328859060403,"(iv)
≤
2Qmax
µ′
k,t(s)π′(m)
k,t (a(m)|s)"
TMIX,0.4906040268456376,"""
320
Nk,t
tmix ln

4
δ√µmin "
TMIX,0.4912751677852349,"+
12
p Nk,t s"
TMIX,0.4919463087248322,"tmix(1 + 2tmix/Nk,t)µ′
k,t(s)π′(m)
k,t (a(m)|s) ln

4
δ√µmin  #"
TMIX,0.49261744966442955,"+ 2
bVk −V ′
k

∞"
TMIX,0.49328859060402686,"(v)
≤Qmax"
TMIX,0.49395973154362416,"""
640tmixAmax"
TMIX,0.49463087248322146,"Nk,tµminϵ′ ln

4
δ√µmin"
TMIX,0.49530201342281877,"
+ 25 s"
TMIX,0.4959731543624161,"tmixAmax
Nk,tµminϵ′ ln

4
δ√µmin  #"
TMIX,0.4966442953020134,"+ 2
bVk −V ′
k

∞, (87)"
TMIX,0.49731543624161073,"where (i) uses eqs. (64)&(65), (ii) uses eq. (64), (iii) uses eq. (68) and the inequality that bµk,t(s) ≥
µ′
k,t(s)/2 implied by (i) of eq. (81), (iv) uses eqs. (80)&(81) and the following inequality based"
TMIX,0.49798657718120803,"on eqs. (61)&(62), and (v) uses µ′
k,t(s) ≥µmin (based on Assumption 1), π′(m)
k,t (a(m)|s) = (1 −"
TMIX,0.4986577181208054,"ϵ′)π(m)
k,t (a(m)|s) + ϵ′/|A(m)| ≥ϵ′/Amax and Nk,t ≥650tmix.
q′(m)
k,t (s, a(m)) −q(m)
k,t (s, a(m))"
TMIX,0.4993288590604027,"≤γEµ′
k,t,π′(1)
k,t ,π′(2)
k,t"
TMIX,0.5,"hbVk(s′) −V ′
k(s′)
1{es = s, ea(m) = a(m)}
i"
TMIX,0.5006711409395973,"≤µ′
k,t(s)π′(m)
k,t (s, a(m))
bVk −V ′
k

∞."
TMIX,0.5013422818791946,"Notice that the following inequality always holds.
Q′(m)
k,t (s, a(m)) −Q(m)
k,t (s, a(m)) ≤
X a(\m)"
TMIX,0.5020134228187919,"Qk(s, a(1), a(2))
π′(\m)
k,t
(a(\m)|s) −π(\m)
k,t
(a(\m)|s)"
TMIX,0.5026845637583892,"(i)
≤Qmaxϵ′π(\m)
k,t
(a(\m)|s) −|A(m)|−1 ≤Qmaxϵ′,
(88)"
TMIX,0.5033557046979866,where (i) uses eq. (68).
TMIX,0.5040268456375839,Published as a conference paper at ICLR 2022
TMIX,0.5046979865771812,"Hence, combining eqs. (87)&(88), it can be seen that with probability at least 1 −2δ, the following
inequality holds
 bQ(m)
k,t (s, a(m)) −Q(m)
k,t (s, a(m)) ≤Qmax"
TMIX,0.5053691275167785,"""
640tmixAmax"
TMIX,0.5060402684563758,"Nk,tµminϵ′ ln

4
δ√µmin"
TMIX,0.5067114093959731,"
+ 25 s"
TMIX,0.5073825503355704,"tmixAmax
Nk,tµminϵ′ ln

4
δ√µmin"
TMIX,0.5080536912751678,"
+ ϵ′
#"
TMIX,0.508724832214765,"+ 2
bVk −V ′
k

∞.
(89)"
TMIX,0.5093959731543625,"Similarly, it can be proved that with probability at least 1 −2δ,"
TMIX,0.5100671140939598,"bQ
(m)"
TMIX,0.5107382550335571,"k,t (s, a(m)) −Q
(m)
k,t (s, a(m)) ≤Qmax"
TMIX,0.5114093959731544,"""
640tmixAmax"
TMIX,0.5120805369127517,"N k,tµminϵ′ ln

4
δ√µmin"
TMIX,0.512751677852349,"
+ 25 s"
TMIX,0.5134228187919463,"tmixAmax
N k,tµminϵ′ ln

4
δ√µmin"
TMIX,0.5140939597315436,"
+ ϵ′
#"
TMIX,0.5147651006711409,"+ 2
bVk −V ′
k

∞.
(90)"
TMIX,0.5154362416107383,"Finally, eqs. (72)-(74) are proved by applying a union bound to eq. (86) and to eqs. (89)&(90) over
all 0 ≤k ≤T −1, 0 ≤t ≤Tk −1, m = 1, 2, s ∈S, a(m) ∈A(m)."
TMIX,0.5161073825503356,"Now we consider the hyperparameter choice ϵ′ =
h
650tmixAmax"
TMIX,0.5167785234899329,"Nk,tµmin
ln

20Tsum|S|Amax"
TMIX,0.5174496644295302,δ√µmin
TMIX,0.5181208053691275,"i1/3
. First, this is"
TMIX,0.5187919463087248,"a valid choice, since Nk,t ≥650tmixAmax"
TMIX,0.5194630872483221,"µmin
ln

20Tsum|S|Amax"
TMIX,0.5201342281879194,δ√µmin
TMIX,0.5208053691275167,"
implies that ϵ′ ∈[0, 1]. Then, for this ϵ′,
the upper bound (73) is simpliﬁed as follows.
 bQ(m)
k,t −Q(m)
k,t

∞ ≤Qmax"
TMIX,0.521476510067114,"""
640tmixAmax"
TMIX,0.5221476510067115,"Nk,tµminϵ′ ln
20Tsum|S|Amax"
TMIX,0.5228187919463088,δ√µmin
TMIX,0.5234899328859061,"
+ 25 s"
TMIX,0.5241610738255034,"tmixAmax
Nk,tµminϵ′ ln
20Tsum|S|Amax"
TMIX,0.5248322147651007,δ√µmin
TMIX,0.525503355704698,"
+ ϵ′
#"
TMIX,0.5261744966442953,"+ 2
bVk −V ′
k

∞
(i)
≤Qmax
640"
TMIX,0.5268456375838926,"650ϵ′2 +
25
√"
TMIX,0.5275167785234899,"650ϵ′
+ 2
bVk −V ′
k

∞"
TMIX,0.5281879194630873,"(ii)
≤2Qmaxϵ′ + 2
bVk −V ′
k

∞= 18Qmax
htmixAmax"
TMIX,0.5288590604026846,"Nk,tµmin
ln
20Tsum|S|Amax"
TMIX,0.5295302013422819,δ√µmin
TMIX,0.5302013422818792,"i1/3
+ 2
bVk −V ′
k

∞,"
TMIX,0.5308724832214765,where (i) uses tmixAmax
TMIX,0.5315436241610738,"Nk,tµmin ln

20Tsum|S|Amax"
TMIX,0.5322147651006711,δ√µmin
TMIX,0.5328859060402684,"
= ϵ′3/650, and (ii) uses ϵ′ ∈[0, 1]. This proves eq. (75).
The proof of eq. (76) is similar."
TMIX,0.5335570469798657,"C
PROPERTIES OF THE DUALITY GAP"
TMIX,0.5342281879194631,"In this section, we prove some useful properties of the duality gap"
TMIX,0.5348993288590604,"D(τ)(π(1), π(2)) :=
max
s,π′(1),π′(2)

V (τ)
π′(1),π(2)(s) −V (τ)
π(1),π′(2)(s)

."
TMIX,0.5355704697986577,"Lemma 5. For any policy pair π(1), π(2), it holds that"
TMIX,0.536241610738255,"D(τ)(π(1), π(2)) ≤
1
1 −γ
max
s,π′(1)(s),π′(2)(s)"
TMIX,0.5369127516778524,"h
fτ

Q(τ)
∗(s), π′(1)(s), π(2)(s)

−fτ

Q(τ)
∗(s), π(1)(s), π′(2)(s)
i
."
TMIX,0.5375838926174497,"This Lemma generalized the Lemma 32 of (Wei et al., 2021) to entropy-regularized Markov game."
TMIX,0.538255033557047,"Proof. Throughout this proof, we denote the policy pair (π(1)
∗τ , π(2)
∗τ ) as the Nash equilibrium and
their associated V-function and Q-function are respectively denoted as V (τ)
∗
and Q(τ)
∗."
TMIX,0.5389261744966443,Published as a conference paper at ICLR 2022
TMIX,0.5395973154362416,Note that
TMIX,0.540268456375839,"V (τ)
π′(1),π(2)(s) −V (τ)
∗
(s) =
X"
TMIX,0.5409395973154363,"a(1),a(2)"
TMIX,0.5416107382550336,"
Q(τ)
π′(1),π(2)(s, a(1), a(2))π′(1)(a(1)|s)π(2)(a(2)|s)"
TMIX,0.5422818791946309,"−Q(τ)
∗(s, a(1), a(2))π(1)
∗τ (a(1)|s)π(2)
∗τ (a(2)|s)
"
TMIX,0.5429530201342282,"+ τ

H
 
π′(1)(s)

−H
 
π(2)(s)

−H
 
π(1)
∗τ (s)

+ H
 
π(2)
∗τ (s)
 =
X"
TMIX,0.5436241610738255,"a(1),a(2)"
TMIX,0.5442953020134228,"
Q(τ)
π′(1),π(2)(s, a(1), a(2)) −Q(τ)
∗(s, a(1), a(2))

π′(1)(a(1)|s)π(2)(a(2)|s) +
X"
TMIX,0.5449664429530201,"a(1),a(2)
Q(τ)
∗(s, a(1), a(2))

π′(1)(a(1)|s)π(2)(a(2)|s) −π(1)
∗τ (a(1)|s)π(2)
∗τ (a(2)|s)
"
TMIX,0.5456375838926174,"+ τ

H
 
π′(1)(s)

−H
 
π(2)(s)

−H
 
π(1)
∗τ (s)

+ H
 
π(2)
∗τ (s)
 = γ
X"
TMIX,0.5463087248322148,"a(1),a(2)"
TMIX,0.5469798657718121,"h
π′(1)(a(1)|s)π(2)(a(2)|s)Es′∼P(·|s,a(1),a(2))

V (τ)
π′(1),π(2)(s′) −V (τ)
∗
(s′)
i"
TMIX,0.5476510067114094,"+ π′(1)(s)⊤Q(τ)
∗(s)π(2)(s) −π(1)
∗τ (s)⊤Q(τ)
∗(s)π(2)
∗τ (s)"
TMIX,0.5483221476510067,"+ τ

H
 
π′(1)(s)

−H
 
π(2)(s)

−H
 
π(1)
∗τ (s)

+ H
 
π(2)
∗τ (s)
"
TMIX,0.548993288590604,"≤γ max
s′

V (τ)
π′(1),π(2)(s′) −V (τ)
∗
(s′)
"
TMIX,0.5496644295302013,"+ τfτ
 
Q(τ)
∗(s); π′(1)(s), π(2)(s)

−τfτ
 
Q(τ)
∗(s); π(1)
∗τ (s), π(2)
∗τ (s)
"
TMIX,0.5503355704697986,"(i)
≤γ max
s′

V (τ)
π′(1),π(2)(s′) −V (τ)
∗
(s′)
"
TMIX,0.5510067114093959,"+ τfτ
 
Q(τ)
∗(s); π′(1)(s), π(2)(s)

−τ min
π′(2) fτ
 
Q(τ)
∗(s); π(1)(s), π′(2)(s)

,"
TMIX,0.5516778523489932,"where (i) uses fτ
 
Q(τ)
∗(s); π(1)
∗τ (s), π(2)
∗τ (s)

= maxπ′′(1) minπ′(2) fτ
 
Q(τ)
∗(s); π′′(1)(s), π′(2)(s)

≥
minπ′(2) fτ
 
Q(τ)
∗(s); π(1)(s), π′(2)(s)

."
TMIX,0.5523489932885906,"Applying maxs,π′(1) to both sides of the above inequality and rearranging it yields that"
TMIX,0.553020134228188,"max
s,π′(1)

V (τ)
π′(1),π(2)(s) −V (τ)
∗
(s)
"
TMIX,0.5536912751677853,"≤
1
1 −γ
max
s,π′(1)(s),π′(2)(s)"
TMIX,0.5543624161073826,"h
fτ

Q(τ)
∗(s), π′(1)(s), π(2)(s)

−fτ

Q(τ)
∗(s), π(1)(s), π′(2)(s)
i
. (91)"
TMIX,0.5550335570469799,"Similarly, we can obtain that"
TMIX,0.5557046979865772,"max
s,π′(2)

V (τ)
∗
(s) −V (τ)
π(1),π′(2)(s)
"
TMIX,0.5563758389261745,"≤
1
1 −γ
max
s,π′(1)(s),π′(2)(s)"
TMIX,0.5570469798657718,"h
fτ

Q(τ)
∗(s), π′(1)(s), π(2)(s)

−fτ

Q(τ)
∗(s), π(1)(s), π′(2)(s)
i
. (92)"
TMIX,0.5577181208053691,"Therefore,"
TMIX,0.5583892617449664,"D(τ)(π(1), π(2))"
TMIX,0.5590604026845638,"=
max
s,π′(1),π′(2)

V (τ)
π′(1),π(2)(s) −V (τ)
π(1),π′(2)(s)
"
TMIX,0.5597315436241611,"≤max
s,π′(1)

V (τ)
π′(1),π(2)(s) −V (τ)
∗
(s)

+ max
s,π′(2)

V (τ)
∗
(s) −V (τ)
π(1),π′(2)(s)
"
TMIX,0.5604026845637584,"(i)
≤
2
1 −γ
max
s,π′(1)(s),π′(2)(s)"
TMIX,0.5610738255033557,"h
fτ

Q(τ)
∗(s), π′(1)(s), π(2)(s)

−fτ

Q(τ)
∗(s), π(1)(s), π′(2)(s)
i"
TMIX,0.561744966442953,where (i) uses eqs. (91)&(92).
TMIX,0.5624161073825503,"Lemma 6. D(τ)(π(1), π(2)) is 2 ln Amax"
TMIX,0.5630872483221476,"1−γ
-Lipschitz continuous with regard to τ."
TMIX,0.5637583892617449,Published as a conference paper at ICLR 2022
TMIX,0.5644295302013422,"Proof. The deﬁnition of the state value function V (τ)
π(1),π(2) in (3) can be rewritten as follows."
TMIX,0.5651006711409396,"V (τ)
π(1),π(2)(s) =E
h ∞
X"
TMIX,0.565771812080537,"t=0
γtRt
s0 = s
i
+ ∞
X"
TMIX,0.5664429530201343,"t=0
γtτ[H(π(1)(st)) −H(π(2)(st))].
(93)"
TMIX,0.5671140939597316,"Hence, for any τ, τ ′ ≥0,
V (τ ′)
π(1),π(2)(s) −V (τ)
π(1),π(2)(s) ≤ ∞
X"
TMIX,0.5677852348993289,"t=0
γt|τ ′ −τ||H(π(1)(st)) −H(π(2)(st))| (i)
≤ ∞
X"
TMIX,0.5684563758389262,"t=0
γt|τ ′ −τ| ln Amax ≤ln Amax"
TMIX,0.5691275167785235,"1 −γ |τ ′ −τ|,
(94)"
TMIX,0.5697986577181208,"where (i) uses 0 ≤H(π(m)(st)) ≤ln |A(m)| ≤ln Amax. Hence, this Lemma can be proved as
follows."
TMIX,0.5704697986577181,"|D(τ ′)(π(1), π(2)) −D(τ)(π(1), π(2))|"
TMIX,0.5711409395973155,"≤

max
s,π′(1),π′(2)

V (τ ′)
π′(1),π(2)(s) −V (τ ′)
π(1),π′(2)(s)

−
max
s,π′(1),π′(2)

V (τ)
π′(1),π(2)(s) −V (τ)
π(1),π′(2)(s)
"
TMIX,0.5718120805369128,"≤
max
s,π′(1),π′(2)"
TMIX,0.5724832214765101,"
V (τ ′)
π′(1),π(2)(s) −V (τ ′)
π(1),π′(2)(s)

−

V (τ)
π′(1),π(2)(s) −V (τ)
π(1),π′(2)(s)
"
TMIX,0.5731543624161074,"≤
max
s,π′(1),π′(2)"
TMIX,0.5738255033557047,"hV (τ ′)
π′(1),π(2)(s) −V (τ)
π′(1),π(2)(s)
 +
V (τ)
π(1),π′(2)(s) −V (τ ′)
π(1),π′(2)(s)

i"
TMIX,0.574496644295302,"(i)
≤2 ln Amax"
TMIX,0.5751677852348993,"1 −γ
|τ ′ −τ|,"
TMIX,0.5758389261744966,where (i) uses eq. (94).
TMIX,0.5765100671140939,"D
PROOF OF THEOREM 1"
TMIX,0.5771812080536913,"Theorem 1 (Finite-time convergence rate). Apply Algorithm 1 to solve the entropy-regularized
Markov game with τ ∈(0, 1]. Choose learning rate η = [2(τ + Qmax)]−1, initialization ∥bV0∥∞≤
Vmax and batch sizes Nk,t, N k,t, Nk+1 that satisfy (18) & (19). Then, the Nash equilibrium duality
gap converges at the following rate with probability at least 1 −δ."
TMIX,0.5778523489932886,"D(τ) 
π(1)
K−1, π(2)
K−1

≤O
Vmax ln Amax 1 −γ K−1
X"
TMIX,0.5785234899328859,"k=0
γK−k(1 −ητ)Tk−1"
TMIX,0.5791946308724832,+ Vmax 1 −γ
TMIX,0.5798657718120805,htmixAmax
TMIX,0.5805369127516778,"µmin
ln
Tsum|S|Amax δµmin"
TMIX,0.5812080536912752,"i2/3 K−1
X"
TMIX,0.5818791946308725,"k=0
γK−k−1
Tk−1
X"
TMIX,0.5825503355704698,"t=0
(1 −ητ)Tk−2−t 1"
TMIX,0.5832214765100671,"N 2/3
k,t
+
Vmax"
TMIX,0.5838926174496645,"τN
2/3
k,t+1 "
TMIX,0.5845637583892618,"+
V 3
maxγK"
TMIX,0.5852348993288591,"τ 2(1 −γ)2 +
tmixV 3
max
τ 2µmin(1 −γ)3 ln
 K|S| δµmin"
TMIX,0.5859060402684564," K−1
X k=0"
TMIX,0.5865771812080537,γK−k−1 Nk+1
TMIX,0.587248322147651,"
.
(20)"
TMIX,0.5879194630872483,"Proof. First, consider the SPU iterations that are deﬁned by replacing Q(m)
k,t , Q
(m)
k,t+1 in (8) with bQ(m)
k,t ,"
TMIX,0.5885906040268456,"bQ
(m)"
TMIX,0.5892617449664429,"k,t+1, respectively, for m = 1, 2. We can apply Lemma 1 with the quantities being speciﬁed as
follows."
TMIX,0.5899328859060403,"• Q := Qk(s), hτ(µ, ν) := fτ
 
Qk(s); µ, ν

,"
TMIX,0.5906040268456376,"• µt := π(1)
k,t(s), νt := π(2)
k,t(s), µt+1 := π(1)
k,t+1(s), νt+1 := π(2)
k,t+1(s),"
TMIX,0.5912751677852349,"µ∗
τ := π∗(1)
k
(s), ν∗
τ := π∗(2)
k
(s),"
TMIX,0.5919463087248322,Published as a conference paper at ICLR 2022
TMIX,0.5926174496644295,"• dm := |A(m)|,"
TMIX,0.5932885906040268,"• δ(m)
k,t := bQ(m)
k,t (s) −Q(m)
k,t (s), δ
(m)
k,t := bQ
(m)"
TMIX,0.5939597315436241,"k,t+1(s) −Q
(m)
k,t+1(s),"
TMIX,0.5946308724832214,"where π(1)∗
k
(s), π(2)∗
k
(s) is the solution policy pair to the minimax optimization problem
minπ(2)(s) maxπ(1)(s) fτ
 
Qk(s); π(1)(s), π(2)(s)

."
TMIX,0.5953020134228187,"Consequently, eqs. (29) & (30) with t = Tk −1 imply the following two inequalities, respectively.
fτ
 
Qk(s); π(1)
k,Tk(s), π(1)
k,Tk(s)

−fτ
 
Qk(s); π∗(1)
k
(s), π∗(2)
k
(s)
"
TMIX,0.5959731543624162,"=
V ′
k+1(s) −fτ
 
Qk(s); π∗(1)
k
(s), π∗(2)
k
(s)
 ≤8"
TMIX,0.5966442953020135,η (1 −ητ)Tk−1 ln Amax +
TMIX,0.5973154362416108,"Tk−1
X"
TMIX,0.5979865771812081,"t=0
(1 −ητ)Tk−2−t"
X,0.5986577181208054,"2
X m=1"
X,0.5993288590604027,"h
8η
 bQ(m)
k,t (s) −Q(m)
k,t (s)
2
∞+ 33 4τ"
X,0.6,"bQ
(m)"
X,0.6006711409395973,"k,t+1(s) −Q
(m)
k,t+1(s)
2
∞"
X,0.6013422818791946,"i
,
(95)"
X,0.602013422818792,"max
π(1)(s),π(2)(s)"
X,0.6026845637583893,"
fτ
 
Qk(s); π(1)(s), π(2)
k,Tk(s)

−fτ
 
Qk(s); π(1)
k,Tk(s), π(2)(s)
"
X,0.6033557046979866,"=
max
π(1)(s),π(2)(s)"
X,0.6040268456375839,"
fτ
 
Qk(s); π(1)(s), π(2)
k (s)

−fτ
 
Qk(s); π(1)
k (s), π(2)(s)
 ≤4"
X,0.6046979865771812,η (1 −ητ)Tk−1 ln Amax +
X,0.6053691275167785,"Tk−1
X"
X,0.6060402684563758,"t=0
(1 −ητ)Tk−2−t"
X,0.6067114093959731,"2
X m=1"
X,0.6073825503355704,"h
16η
 bQ(m)
k,t (s) −Q(m)
k,t (s)
2
∞+ 12 τ"
X,0.6080536912751678,"bQ
(m)"
X,0.6087248322147651,"k,t+1(s) −Q
(m)
k,t+1(s)
2
∞"
X,0.6093959731543624,"i
.
(96)"
X,0.6100671140939598,"Then, deﬁne the soft Bellman operator Bτ as follows."
X,0.610738255033557,"Bτ(Q)(s, a(1), a(2))"
X,0.6114093959731544,":= R(s, a(1), a(2)) + γEs′∼P(·|s,a(1),a(2))
h
max
π(1)(s′) min
π(2)(s′) fτ
 
Q(s′); π(1)(s′), π(2)(s′)
i
,
(97)"
X,0.6120805369127517,"where Q : S × A(1) × A(2) →R and Q(s′) ∈R|A(1)|×|A(2)| with each entry given by
[Q(s′)]a(1),a(2) := Q(s, a(1), a(2)). It can be proved that Bτ is a contraction operator and has a"
X,0.612751677852349,"unique ﬁxed point Q(τ)
∗
(Cen et al., 2021), that is,"
X,0.6134228187919463,"∥Bτ(Q′) −Bτ(Q)∥∞≤γ∥Q′ −Q∥∞,
(98)"
X,0.6140939597315436,"Bτ(Q(τ)
∗) =Q(τ)
∗.
(99)"
X,0.614765100671141,"As a result, we obtain that"
X,0.6154362416107383,"∥Qk+1 −Q(τ)
∗∥∞"
X,0.6161073825503356,"(i)
≤∥Qk+1 −Bτ(Qk)∥∞+ ∥Bτ(Qk) −Bτ(Q(τ)
∗)∥∞"
X,0.6167785234899329,"(ii)
≤
max
s,a(1),a(2) |Qk+1(s, a(1), a(2)) −Bτ(Qk)(s, a(1), a(2))| + γ∥Qk −Q(τ)
∗∥∞"
X,0.6174496644295302,"=
max
s,a(1),a(2)"
X,0.6181208053691275,"R(s, a(1), a(2)) + γEs′∼P(·|s,a(1),a(2))V ′
k+1(s′)"
X,0.6187919463087248,"−
h
R(s, a(1), a(2)) + γEs′∼P(·|s,a(1),a(2))fτ
 
Qk(s′); π∗(1)
k
(s′), π∗(2)
k
(s′)
i + γ∥Qk −Q(τ)
∗∥∞"
X,0.6194630872483221,"≤γ max
s′∈S
V ′
k+1(s′) −fτ
 
Qk(s′); π∗(1)
k
(s′), π∗(2)
k
(s′)
 + γ∥Qk −Q(τ)
∗∥∞,
(100)"
X,0.6201342281879194,Published as a conference paper at ICLR 2022
X,0.6208053691275168,"where (i) uses eq. (99) and (ii) uses eq. (98). Throughout the proof, suppose that eqs. (72), (75) &
(76) hold simultaneously which occurs with probability at least 1 −δ. In this case, we have
 bQ(m)
k,t (s) −Q(m)
k,t (s)
2
∞"
X,0.6214765100671141,"(i)
≤648Q2
max
htmixAmax"
X,0.6221476510067114,"Nk,tµmin
ln
20Tsum|S|Amax"
X,0.6228187919463087,δ√µmin
X,0.623489932885906,"i2/3
+ 8
bVk −V ′
k
2
∞"
X,0.6241610738255033,"(ii)
≤648Q2
max
htmixAmax"
X,0.6248322147651006,"Nk,tµmin
ln
20Tsum|S|Amax"
X,0.625503355704698,δ√µmin i2/3
X,0.6261744966442953,"+ 32V 2
maxγ2k +
29241tmixV 2
max
Nk+1µmin(1 −γ)2 ln
 4K|S|"
X,0.6268456375838927,δ√µmin
X,0.62751677852349,"
,
(101)"
X,0.6281879194630873,"where (i) uses eq. (75) and the inequality that (a + b)2 ≤2a2 + 2b2 for any a, b ≥0, (ii) uses eq.
(72) and also (a + b)2 ≤2a2 + 2b2 for any a, b ≥0. Similarly, we obtain that
bQ
(m)"
X,0.6288590604026846,"k,t+1(s) −Q
(m)
k,t+1(s)
2
∞"
X,0.6295302013422819,"≤648Q2
max
h tmixAmax"
X,0.6302013422818792,"N k,t+1µmin
ln
20Tsum|S|Amax"
X,0.6308724832214765,δ√µmin i2/3
X,0.6315436241610738,"+ 32V 2
maxγ2k +
29241tmixV 2
max
Nk+1µmin(1 −γ)2 ln
 4K|S|"
X,0.6322147651006711,δ√µmin
X,0.6328859060402685,"
.
(102)"
X,0.6335570469798658,"Then, iterating eq. (100) yields that"
X,0.6342281879194631,"∥QK−1 −Q(τ)
∗∥∞"
X,0.6348993288590604,"≤γK−1∥Q0 −Q(τ)
∗∥∞+ K−2
X"
X,0.6355704697986577,"k=0
γK−k−1 max
s′∈S
Vk+1(s′) −fτ
 
Qk(s′); π∗(1)
k
(s′), π∗(2)
k
(s′)
"
X,0.636241610738255,"(i)
≤2γK−1Qmax + K−2
X"
X,0.6369127516778523,"k=0
γK−k−1
""
8
η (1 −ητ)Tk−1 ln Amax +"
X,0.6375838926174496,"Tk−1
X"
X,0.6382550335570469,"t=0
(1 −ητ)Tk−2−t"
X,0.6389261744966444,"2
X m=1"
X,0.6395973154362417,"h
8η
 bQ(m)
k,t (s) −Q(m)
k,t (s)
2
∞+ 33 4τ"
X,0.640268456375839,"bQ
(m)"
X,0.6409395973154363,"k,t+1(s) −Q
(m)
k,t+1(s)
2
∞ i#"
X,0.6416107382550336,"(ii)
≤2γK−1Qmax + K−2
X"
X,0.6422818791946309,"k=0
γK−k−1
""
8
η (1 −ητ)Tk−1 ln Amax +"
X,0.6429530201342282,"Tk−1
X"
X,0.6436241610738255,"t=0
(1 −ητ)Tk−2−t"
X,0.6442953020134228,"2
X m=1 "
X,0.6449664429530201,"648Q2
max
htmixAmax"
X,0.6456375838926175,"µmin
ln
20Tsum|S|Amax"
X,0.6463087248322148,δ√µmin
X,0.6469798657718121,i2/3 8η
X,0.6476510067114094,"N 2/3
k,t
+
33"
X,0.6483221476510067,"4τN
2/3
k,t+1 "
X,0.648993288590604,"+

8η + 33 4τ"
X,0.6496644295302013,"h
32V 2
maxγ2k +
29241tmixV 2
max
Nk+1µmin(1 −γ)2 ln
 4K|S|"
X,0.6503355704697986,δ√µmin i!#
X,0.6510067114093959,"(iii)
≤2γK−1Qmax + 8"
X,0.6516778523489933,"η ln Amax K−2
X"
X,0.6523489932885906,"k=0
γK−k−1(1 −ητ)Tk−1"
X,0.6530201342281879,"+ 324Q2
max
htmixAmax"
X,0.6536912751677852,"µmin
ln
20Tsum|S|Amax"
X,0.6543624161073825,"δ√µmin i2/3 K−2
X"
X,0.6550335570469799,"k=0
γK−k−1
Tk−1
X"
X,0.6557046979865772,"t=0
(1 −ητ)Tk−2−t 32η"
X,0.6563758389261745,"N 2/3
k,t
+
33"
X,0.6570469798657718,"τN
2/3
k,t+1"
X,0.6577181208053692,"
+ 1568γK−1V 2
max
ητ 2(1 −γ)"
X,0.6583892617449665,"+ 1432809tmixV 2
max
ητ 2µmin(1 −γ)2 ln
 4K|S|"
X,0.6590604026845638,δ√µmin
X,0.6597315436241611," K−2
X"
X,0.6604026845637584,"k=0
γK−k−1N −1
k+1
(103)"
X,0.6610738255033557,"(iv)
= O """
X,0.661744966442953,"Vmax ln Amax K−2
X"
X,0.6624161073825503,"k=0
γK−k(1 −ητ)Tk−1 + Vmax
htmixAmax"
X,0.6630872483221476,"µmin
ln
Tsum|S|Amax δµmin i2/3"
X,0.663758389261745,"Published as a conference paper at ICLR 2022 K−2
X"
X,0.6644295302013423,"k=0
γK−k−1
Tk−1
X"
X,0.6651006711409396,"t=0
(1 −ητ)Tk−2−t
1"
X,0.6657718120805369,"N 2/3
k,t
+
Vmax"
X,0.6664429530201342,"τN
2/3
k,t+1 "
X,0.6671140939597315,"+ V 3
maxγK"
X,0.6677852348993288,"τ 2(1 −γ) +
tmixV 3
max
τ 2µmin(1 −γ)2 ln
 K|S| δµmin"
X,0.6684563758389261," K−2
X"
X,0.6691275167785234,"k=0
γK−k−1N −1
k+1 # (104)"
X,0.6697986577181209,"where (i) uses eqs. (68)&(95), (ii) uses eqs. (101)&(102), (iii) uses η = [2(τ + Qmax)]−1 ≤1/(2τ),
PTk−1
t=0 (1 −ητ)Tk−2−t ≤
1
ητ(1−ητ) ≤
2
ητ and PK−2
k=0 γK+k−1 ≤γK−1"
X,0.6704697986577182,"1−γ , and (iv) uses Qmax =
O(Vmax) for γ ≈1 and η = [2(τ + Qmax)]−1 = O(Q−1
max) = O(V −1
max) (Since Qmax ≥γτ =
O(τ))."
X,0.6711409395973155,"Using Lemma 5, the convergence rate of the duality gap in eq. (20) can be proved as follows."
X,0.6718120805369128,"D(τ) 
π(1)
K−1, π(2)
K−1
"
X,0.6724832214765101,"(i)
≤
2
1 −γ
max
s,π(1),π(2)"
X,0.6731543624161074,"h
fτ

Q(τ)
∗(s), π(1)(s), π(2)
K−1(s)

−fτ

Q(τ)
∗(s), π(1)
K−1(s), π(2)(s)
i"
X,0.6738255033557047,"≤
2
1 −γ
max
s,π(1),π(2)"
X,0.674496644295302,"h
fτ
 
QK−1(s); π(1)(s), π(2)
K−1(s)

−fτ
 
QK−1(s); π(1)
K−1(s), π(2)(s)
i"
X,0.6751677852348993,"+
2
1 −γ
max
s,π(1),π(2)
fτ
 
Q(τ)
∗(s); π(1)(s), π(2)
K−1(s)

−fτ
 
QK−1(s); π(1)(s), π(2)
K−1(s)
"
X,0.6758389261744966,"+
2
1 −γ
max
s,π(1),π(2)
fτ
 
QK−1(s); π(1)
K−1(s), π(2)(s)

−fτ
 
Q(τ)
∗(s); π(1)
K−1(s), π(2)(s)
"
X,0.676510067114094,"(ii)
≤
8
η(1 −γ)(1 −ητ)TK−1−1 ln Amax +
2
1 −γ"
X,0.6771812080536913,"TK−1−1
X"
X,0.6778523489932886,"t=0
(1 −ητ)TK−1−2−t"
X,0.6785234899328859,"2
X m=1"
X,0.6791946308724832,"h
16η
 bQ(m)
K−1,t(s) −Q(m)
K−1,t(s)
2
∞+ 12 τ"
X,0.6798657718120805,"bQ
(m)"
X,0.6805369127516778,"K−1,t+1(s) −Q
(m)
K−1,t+1(s)
2
∞ i"
X,0.6812080536912751,"+
2
1 −γ max
s,π(1)
π(1)(s)⊤[Q(τ)
∗(s) −QK−1(s)]π(2)
K−1(s)"
X,0.6818791946308724,"+
2
1 −γ max
s,π(2)
π(1)
K−1(s)⊤[QK−1(s) −Q(τ)
∗(s)]π(2)(s)"
X,0.6825503355704698,"(iii)
≤8 ln Amax"
X,0.6832214765100671,"η(1 −γ) (1 −ητ)TK−1−1 +
4
1 −γ"
X,0.6838926174496645,"TK−1−1
X"
X,0.6845637583892618,"t=0
(1 −ητ)TK−1−2−t "
X,0.6852348993288591,"648Q2
max
htmixAmax"
X,0.6859060402684564,"µmin
ln
20Tsum|S|Amax"
X,0.6865771812080537,δ√µmin
X,0.687248322147651,"i2/3
16η"
X,0.6879194630872483,"N 2/3
K−1,t
+
12"
X,0.6885906040268457,"τN
2/3
K−1,t+1 "
X,0.689261744966443,"+

32η + 24 τ"
X,0.6899328859060403,"h
32V 2
maxγ2(K−1) +
29241tmixV 2
max
NKµmin(1 −γ)2 ln
 4K|S|"
X,0.6906040268456376,δ√µmin i!
X,0.6912751677852349,"+
4
1 −γ ∥QK−1 −Q(τ)
∗∥∞"
X,0.6919463087248322,"(iv)
≤8 ln Amax"
X,0.6926174496644295,"η(1 −γ) (1 −ητ)TK−1−1 + 2592Q2
max
1 −γ"
X,0.6932885906040268,htmixAmax
X,0.6939597315436241,"µmin
ln
20Tsum|S|Amax"
X,0.6946308724832215,δ√µmin i2/3
X,0.6953020134228188,"TK−1−1
X"
X,0.6959731543624161,"t=0
(1 −ητ)TK−1−2−t
16η"
X,0.6966442953020134,"N 2/3
K−1,t
+
12"
X,0.6973154362416107,"τN
2/3
K−1,t+1  + 160 ητ 2"
X,0.697986577181208,"h32V 2
max
1 −γ γ2(K−1) +
29241tmixV 2
max
NKµmin(1 −γ)3 ln
 4K|S|"
X,0.6986577181208053,δ√µmin i
X,0.6993288590604027,+ 8γK−1Qmax
X,0.7,"1 −γ
+ 32 ln Amax"
X,0.7006711409395974,"η(1 −γ) K−2
X"
X,0.7013422818791947,"k=0
γK−k−1(1 −ητ)Tk−1"
X,0.702013422818792,Published as a conference paper at ICLR 2022
X,0.7026845637583893,"+ 1296Q2
max
1 −γ"
X,0.7033557046979866,htmixAmax
X,0.7040268456375839,"µmin
ln
20Tsum|S|Amax"
X,0.7046979865771812,"δ√µmin i2/3 K−2
X"
X,0.7053691275167785,"k=0
γK−k−1
Tk−1
X"
X,0.7060402684563758,"t=0
(1 −ητ)Tk−2−t 32η"
X,0.7067114093959731,"N 2/3
k,t
+
33"
X,0.7073825503355705,"τN
2/3
k,t+1"
X,0.7080536912751678,"
+ 6272γK−1V 2
max
ητ 2(1 −γ)2"
X,0.7087248322147651,"+ 5731236tmixV 2
max
ητ 2µmin(1 −γ)3 ln
 4K|S|"
X,0.7093959731543624,δ√µmin
X,0.7100671140939597," K−2
X"
X,0.710738255033557,"k=0
γK−k−1N −1
k+1"
X,0.7114093959731543,≤8γK−1Qmax
X,0.7120805369127516,"1 −γ
+ 32 ln Amax"
X,0.7127516778523489,"η(1 −γ) K−1
X"
X,0.7134228187919464,"k=0
γK−k−1(1 −ητ)Tk−1"
X,0.7140939597315437,"+ 1296Q2
max
1 −γ"
X,0.714765100671141,htmixAmax
X,0.7154362416107383,"µmin
ln
20Tsum|S|Amax"
X,0.7161073825503356,"δ√µmin i2/3 K−1
X"
X,0.7167785234899329,"k=0
γK−k−1
Tk−1
X"
X,0.7174496644295302,"t=0
(1 −ητ)Tk−2−t 32η"
X,0.7181208053691275,"N 2/3
k,t
+
33"
X,0.7187919463087248,"τN
2/3
k,t+1"
X,0.7194630872483222,"
+ 11392γK−1V 2
max
ητ 2(1 −γ)2"
X,0.7201342281879195,"+ 5731236tmixV 2
max
ητ 2µmin(1 −γ)3 ln
 4K|S|"
X,0.7208053691275168,δ√µmin
X,0.7214765100671141," K−1
X"
X,0.7221476510067114,"k=0
γK−k−1N −1
k+1"
X,0.7228187919463087,"(v)
= O"
X,0.723489932885906,"""
Vmax ln Amax 1 −γ K−1
X"
X,0.7241610738255033,"k=0
γK−k(1 −ητ)Tk−1 + Vmax 1 −γ"
X,0.7248322147651006,htmixAmax
X,0.725503355704698,"µmin
ln
Tsum|S|Amax δµmin i2/3 K−1
X"
X,0.7261744966442953,"k=0
γK−k−1
Tk−1
X"
X,0.7268456375838926,"t=0
(1 −ητ)Tk−2−t
1"
X,0.72751677852349,"N 2/3
k,t
+
Vmax"
X,0.7281879194630873,"τN
2/3
k,t+1"
X,0.7288590604026846,"
+
V 3
maxγK"
X,0.7295302013422819,τ 2(1 −γ)2
X,0.7302013422818792,"+
tmixV 3
max
τ 2µmin(1 −γ)3 ln
 K|S| δµmin"
X,0.7308724832214765," K−1
X"
X,0.7315436241610739,"k=0
γK−k−1N −1
k+1 # ,"
X,0.7322147651006712,"where (i) uses Lemma 5, (ii) uses eq. (96) and the deﬁnition of the function fτ, (iii) uses eqs.
(101)&(102), (iv) uses eq. (103) and η = [2(τ + Qmax)]−1 ≤1/(2τ), PTk−1
t=0 (1 −ητ)Tk−2−t ≤
1
ητ(1−ητ) ≤
2
ητ , and (v) uses Qmax = O(Vmax) for γ ≈1 and η = [2(τ +Qmax)]−1 = O(Q−1
max) =
O(V −1
max) (Since Qmax ≥γτ = O(τ))."
X,0.7328859060402685,"E
PROOF OF THEOREM 2"
X,0.7335570469798658,"Theorem 2 (Sample complexity). Implement Algorithm 1 with η = O
 
1 −γ

, τ = O
  ϵ(1−γ)"
X,0.7342281879194631,"ln Amax

,"
X,0.7348993288590604,"K = O
h
1
1−γ ln

ln Amax"
X,0.7355704697986577,"ϵ(1−γ)
i
and Tk = 1 +
k ln γ−1"
X,0.736241610738255,ln(1−ητ)−1 . Choose the following adaptive batch sizes.
X,0.7369127516778523,"Nk+1 = e
O
tmix(ln2 Amax)γ−k"
X,0.7375838926174496,"2
ϵ2µmin(1 −γ)8"
X,0.738255033557047,"
, Nk,t = N k,tϵ
3
2 (1 −γ)3 = e
O
tmixAmax(1 −ητ)
−3(t+1)"
X,0.7389261744966443,"5
µmin(1 −γ)3 
."
X,0.7395973154362416,"Then, for any ϵ ≤ln Amax"
X,0.7402684563758389,"1−γ , the overall sample complexity to achieve D(0)(π(1)
K−1, π(2)
K−1) ≤ϵ is
e
O
 
tmixAmax
µminϵ5.5(1−γ)13.5

. Please refer to (118) in Appendix E for a complete expression."
X,0.7409395973154362,"Proof. Since τ = O
  ϵ(1−γ)"
X,0.7416107382550335,"ln Amax

, we have"
X,0.7422818791946308,Vmax =1 + τ ln Amax
X,0.7429530201342281,"1 −γ
=
1
1 −γ + O(ϵ) = O
 
(1 −γ)−1
.
(105)"
X,0.7436241610738255,Qmax =1 + γτ ln Amax
X,0.7442953020134229,"1 −γ
=
1
1 −γ + O(ϵ) = O
 
(1 −γ)−1
.
(106)"
X,0.7449664429530202,Published as a conference paper at ICLR 2022
X,0.7456375838926175,"Since Tk = 1 +
k ln γ−1"
X,0.7463087248322148,"ln(1−ητ)−1 , we have"
X,0.7469798657718121,"Tsum = K−1
X k=0"
X,0.7476510067114094,"
1 +
k ln γ−1"
X,0.7483221476510067,ln(1 −ητ)−1 
X,0.748993288590604,=K + K(K −1)
O,0.7496644295302013,"2
O
1 −γ ητ "
O,0.7503355704697987,=K + K(K −1)
O,0.751006711409396,"2
O
 ln Amax"
O,0.7516778523489933,ϵ(1 −γ) 
O,0.7523489932885906,"=O
K2 ln Amax"
O,0.7530201342281879,ϵ(1 −γ) 
O,0.7536912751677852,"=O
h ln Amax"
O,0.7543624161073825,ϵ(1 −γ)3 ln2  ln Amax
O,0.7550335570469798,ϵ(1 −γ)
O,0.7557046979865771,"i
.
(107)"
O,0.7563758389261745,"Hence,"
O,0.7570469798657719,"ln
Tsum|S|Amax δµmin"
O,0.7577181208053692,"
= O
h
ln
|S|Amax ln Amax"
O,0.7583892617449665,δµminϵ(1 −γ)3
O,0.7590604026845638,"
+ ln

2 ln
 ln Amax"
O,0.7597315436241611,ϵ(1 −γ) i
O,0.7604026845637584,"= O
h
ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.7610738255033557,"i
.
(108)"
O,0.761744966442953,"Similarly,"
O,0.7624161073825504,"ln
 K|S| δµmin"
O,0.7630872483221477,"
=O
h
ln

|S|
δµmin(1 −γ)"
O,0.763758389261745,"
ln
 ln Amax"
O,0.7644295302013423,ϵ(1 −γ) i
O,0.7651006711409396,"=O
h
ln
|S| ln(ϵ−1 ln Amax)"
O,0.7657718120805369,δµmin(1 −γ)
O,0.7664429530201342,"i
+ O
h
ln

|S|
δµmin(1 −γ)"
O,0.7671140939597315,"
ln

1
1 −γ i"
O,0.7677852348993288,"=O
h
ln
|S| ln(ϵ−1 ln Amax)"
O,0.7684563758389261,δµmin(1 −γ)
O,0.7691275167785235,"i
+ O
h
ln

|S|
δµmin(1 −γ) i"
O,0.7697986577181208,"=O
h
ln
|S| ln(ϵ−1 ln Amax)"
O,0.7704697986577181,δµmin(1 −γ)
O,0.7711409395973154,"i
(109)"
O,0.7718120805369127,where (i) uses ϵ ≤ln Amax
O,0.77248322147651,"1−γ . With the above equalities, we will prove below that the hyperparameter
choices in Theorem 2 are valid and satisfy the conditions of Theorem 1 with proper constants hidden
in O(·)."
O,0.7731543624161074,"η =[2(τ + Qmax)]−1 = O
 
1 −γ

(110)"
O,0.7738255033557047,"τ =O
ϵ(1 −γ)"
O,0.774496644295302,ln Amax
O,0.7751677852348994,"
∈(0, 1]
(111)"
O,0.7758389261744967,"K =
1
ln γ−1 ln
 ln5/2 Amax"
O,0.776510067114094,ϵ5(1 −γ)7.5
O,0.7771812080536913,"
= O
h
1
1 −γ ln
 ln Amax"
O,0.7778523489932886,ϵ(1 −γ)
O,0.7785234899328859,"i
≥1
(112)"
O,0.7791946308724832,"Tk =1 +
k ln γ−1"
O,0.7798657718120805,"ln(1 −ητ)−1 ≥1
(113)"
O,0.7805369127516778,"Nk+1 =O
htmix(ln2 Amax)γ−k/2"
O,0.7812080536912752,"ϵ2µmin(1 −γ)8
ln
|S| ln(ϵ−1 ln Amax)"
O,0.7818791946308725,δµmin(1 −γ) i
O,0.7825503355704698,≥650tmix
O,0.7832214765100671,"µmin
ln

4
δ√µmin"
O,0.7838926174496644,"
= O
h tmix"
O,0.7845637583892617,"µmin
ln

1
δµmin"
O,0.785234899328859,"i
(114)"
O,0.7859060402684563,"Nk,t =O
htmixAmax(1 −ητ)−3(t+1)/5"
O,0.7865771812080536,"µmin(1 −γ)3
ln

|S|Amax
ϵδµmin(1 −γ) i"
O,0.7872483221476511,≥650tmixAmax
O,0.7879194630872484,"µmin
ln
20Tsum|S|Amax"
O,0.7885906040268457,δ√µmin
O,0.789261744966443,"
= O
htmixAmax"
O,0.7899328859060403,"µmin
ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.7906040268456376,"i
(115)"
O,0.7912751677852349,"N k,t =O
htmixAmax(ln3/2 Amax)(1 −ητ)−3(t+1)/5"
O,0.7919463087248322,"µminϵ3/2(1 −γ)6
ln

|S|Amax
ϵδµmin(1 −γ) i"
O,0.7926174496644295,Published as a conference paper at ICLR 2022
O,0.7932885906040269,≥650tmixAmax
O,0.7939597315436242,"µmin
ln
20Tsum|S|Amax"
O,0.7946308724832215,δ√µmin
O,0.7953020134228188,"
= O
htmixAmax"
O,0.7959731543624161,"µmin
ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.7966442953020134,"i
,
(116)"
O,0.7973154362416107,"With these hyperparameters and eqs. (108)&(109), the duality gap bound (20) can be simpliﬁed as
follows,"
O,0.797986577181208,"D(τ) 
π(1)
K−1, π(2)
K−1
 ≤O"
O,0.7986577181208053,"""
Vmax ln Amax 1 −γ K−1
X"
O,0.7993288590604026,"k=0
γK−k(1 −ητ)Tk−1 + Vmax 1 −γ"
O,0.8,htmixAmax
O,0.8006711409395973,"µmin
ln
Tsum|S|Amax δµmin i2/3 K−1
X"
O,0.8013422818791947,"k=0
γK−k−1
Tk−1
X"
O,0.802013422818792,"t=0
(1 −ητ)Tk−2−t
1"
O,0.8026845637583893,"N 2/3
k,t
+
Vmax"
O,0.8033557046979866,"τN
2/3
k,t+1"
O,0.8040268456375839,"
+
V 3
maxγK"
O,0.8046979865771812,τ 2(1 −γ)2
O,0.8053691275167785,"+
tmixV 3
max
τ 2µmin(1 −γ)3 ln
 K|S| δµmin"
O,0.8060402684563759," K−1
X k=0"
O,0.8067114093959732,γK−k−1 Nk+1 #
O,0.8073825503355705,"(i)
≤O"
O,0.8080536912751678,"""
ln Amax
(1 −γ)2 K−1
X"
O,0.8087248322147651,"k=0
γK−kγk"
O,0.8093959731543624,"+
1
(1 −γ)2"
O,0.8100671140939597,htmixAmax
O,0.810738255033557,"µmin
ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.8114093959731543,"i2/3 K−1
X"
O,0.8120805369127517,"k=0
γK−k−1
Tk−1
X"
O,0.812751677852349,"t=0
(1 −ητ)Tk−2−t "
O,0.8134228187919463,"O
h
tmixAmax
µmin(1 −γ)3 ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.8140939597315436,"i−2/3
(1 −ητ)2(t+1)/5"
O,0.8147651006711409,"+ O
 ln Amax"
O,0.8154362416107382,ϵ(1 −γ)2
O,0.8161073825503355,"
O
htmixAmax(ln3/2 Amax)"
O,0.8167785234899329,"µminϵ3/2(1 −γ)6
ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.8174496644295302,"i−2/3
(1 −ητ)2(t+1)/5
!"
O,0.8181208053691276,+ γK(ln2 Amax)
O,0.8187919463087249,"ϵ2(1 −γ)7
+ tmix(ln2 Amax)"
O,0.8194630872483222,"ϵ2µmin(1 −γ)8 ln
|S| ln(ϵ−1 ln Amax)"
O,0.8201342281879195,"δµmin(1 −γ)  K−1
X"
O,0.8208053691275168,"k=0
γK−k−1O
htmix(ln2 Amax)γ−k/2"
O,0.8214765100671141,"ϵ2µmin(1 −γ)8
ln
|S| ln(ϵ−1 ln Amax)"
O,0.8221476510067114,δµmin(1 −γ)
O,0.8228187919463087,"i−1
# ≤O"
O,0.823489932885906,"""
ln Amax
(1 −γ)2 KγK + K−1
X"
O,0.8241610738255034,"k=0
γK−k−1
Tk−1
X"
O,0.8248322147651007,"t=0
(1 −ητ)Tk−1−3(t+1)/5"
O,0.825503355704698,+ γK(ln2 Amax)
O,0.8261744966442953,"ϵ2(1 −γ)7
+ K−1
X"
O,0.8268456375838926,"k=0
γK−k/2−1
#"
O,0.8275167785234899,"(ii)
≤O"
O,0.8281879194630872,"""
ln Amax
(1 −γ)2 KγK + K−1
X"
O,0.8288590604026845,"k=0
γK−1 (1 −ητ)−3/5[(1 −ητ)−3Tk/5 −1]"
O,0.8295302013422818,(1 −ητ)−3/5 −1
O,0.8302013422818791,+ γK(ln2 Amax)
O,0.8308724832214766,"ϵ2(1 −γ)7
+ γK−1 γ−K/2 −1"
O,0.8315436241610739,γ−1/2 −1 #
O,0.8322147651006712,"(iii)
≤O"
O,0.8328859060402685,"""
ln Amax
(1 −γ)2 KγK + K−1
X k=0"
O,0.8335570469798658,γK−1−3k/5
O,0.8342281879194631,1 −(1 −ητ)3/5 + γK(ln2 Amax)
O,0.8348993288590604,"ϵ2(1 −γ)7
+ γ(K−1)/2"
O,0.8355704697986577,1 −γ1/2 #
O,0.836241610738255,"(iv)
≤O"
O,0.8369127516778524,"""
ln Amax
(1 −γ)2 KγK + γK−1(γ−3K/5 −1)"
O,0.8375838926174497,"ητ(γ−3/5 −1)
+ γK(ln2 Amax)"
O,0.838255033557047,"ϵ2(1 −γ)7
+ γK/2 1 −γ #"
O,0.8389261744966443,"(v)
≤O"
O,0.8395973154362416,"""
γK ln Amax"
O,0.8402684563758389,"(1 −γ)3
ln
 ln Amax"
O,0.8409395973154362,ϵ(1 −γ)
O,0.8416107382550335,"
+ γ2K/5 ln Amax"
O,0.8422818791946308,"ϵ(1 −γ)3
+ γK(ln2 Amax)"
O,0.8429530201342282,"ϵ2(1 −γ)7
+ γK/2 1 −γ #"
O,0.8436241610738255,"(vi)
≤O """
O,0.8442953020134228,"ϵ + ϵ3
r"
O,0.8449664429530201,"1 −γ
ln Amax
+ ϵ5/2(1 −γ)2.75"
O,0.8456375838926175,ln5/4 Amax #
O,0.8463087248322148,"= O(ϵ)
(vii)
≤cϵ
(117)"
O,0.8469798657718121,Published as a conference paper at ICLR 2022
O,0.8476510067114094,"where (i), (ii) and (iii) use (1−ητ)Tk−1 = γk based on eq. (113), (i) also uses eqs. (105), (108)-(111)
& (114)-(116), (iv) uses 1 −(1 −ητ)3/5 = O(ητ), 1 −γ1/2 = O(1 −γ), (v) uses eqs. (110)-"
O,0.8483221476510067,"(112), (vi) uses O
h
γK ln Amax"
O,0.8489932885906041,"(1−γ)3
ln

ln Amax"
O,0.8496644295302014,"ϵ(1−γ)
i
≤O
h
γK(ln2 Amax)"
O,0.8503355704697987,"ϵ2(1−γ)7
i
and γK = ϵ5(1−γ)7.5"
O,0.851006711409396,"ln5/2 Amax implied by
eq. (112), and the numeric constant c > 0 in (vii) exists. By replacing ϵ with ϵ/c, we obtain that
Dτ
 
π(1)
K−1, π(2)
K−1

≤ϵ which means (π(1)
K−1, π(2)
K−1) is an ϵ-Nash equilibrium policy pair, and the
orders of all the hyperparameter choices remain the same. In this case, the overall sample complexity
is given by K−1
X k=0"
O,0.8516778523489933,"h
Nk+1 + 2"
O,0.8523489932885906,"Tk−1
X"
O,0.8530201342281879,"t=0
(Nk,t + N k,t+1)
i"
O,0.8536912751677852,"(i)
= O
h tmix(ln2 Amax)"
O,0.8543624161073825,"ϵ2µmin(1 −γ)8 ln
|S| ln(ϵ−1 ln Amax)"
O,0.8550335570469799,δµmin(1 −γ)
O,0.8557046979865772,"i K−1
X"
O,0.8563758389261745,"k=0
γ−k/2"
O,0.8570469798657718,"+ O
htmixAmax(ln3/2 Amax)"
O,0.8577181208053691,"µminϵ3/2(1 −γ)6
ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.8583892617449664,"i K−1
X k=0"
O,0.8590604026845637,"Tk−1
X"
O,0.859731543624161,"t=0
(1 −ητ)−3(t+1)/5"
O,0.8604026845637583,"= O
h tmix(ln2 Amax)"
O,0.8610738255033556,"ϵ2µmin(1 −γ)8 ln
|S| ln(ϵ−1 ln Amax)"
O,0.8617449664429531,δµmin(1 −γ)
O,0.8624161073825504,iγ−K/2 −1
O,0.8630872483221477,γ−1/2 −1
O,0.863758389261745,"+ O
htmixAmax(ln3/2 Amax)"
O,0.8644295302013423,"µminϵ3/2(1 −γ)6
ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.8651006711409396,"i K−1
X k=0"
O,0.8657718120805369,[(1 −ητ)−3Tk/5 −1]
O,0.8664429530201342,1 −(1 −ητ)3/5
O,0.8671140939597315,"(ii)
≤O
h tmix(ln2 Amax)"
O,0.8677852348993289,"ϵ2µmin(1 −γ)8 ln
|S| ln(ϵ−1 ln Amax)"
O,0.8684563758389262,δµmin(1 −γ)
O,0.8691275167785235,"i
γ−K/2"
O,0.8697986577181208,"+ O
htmixAmax(ln3/2 Amax)"
O,0.8704697986577181,"µminϵ3/2(1 −γ)6
ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.8711409395973154, ln Amax
O,0.8718120805369127,ϵ(1 −γ)2
O,0.87248322147651,"i K−1
X"
O,0.8731543624161073,"k=0
γ−3k/5"
O,0.8738255033557047,"≤O
h tmix(ln2 Amax)"
O,0.874496644295302,"ϵ2µmin(1 −γ)8 ln
|S| ln(ϵ−1 ln Amax)"
O,0.8751677852348994,δµmin(1 −γ)
O,0.8758389261744967,"i
γ−K/2"
O,0.876510067114094,"+ O
htmixAmax(ln5/2 Amax)"
O,0.8771812080536913,"µminϵ5/2(1 −γ)8
ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.8778523489932886,i γ−3K/5
O,0.8785234899328859,γ−3/5 −1
O,0.8791946308724832,"(iii)
≤O
h tmix(ln2 Amax)"
O,0.8798657718120806,"ϵ2µmin(1 −γ)8 ln
|S| ln(ϵ−1 ln Amax)"
O,0.8805369127516779,δµmin(1 −γ)
O,0.8812080536912752,"i
ln5/4 Amax
ϵ5/2(1 −γ)3.75"
O,0.8818791946308725,"+ O
htmixAmax(ln5/2 Amax)"
O,0.8825503355704698,"µminϵ5/2(1 −γ)8
ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.8832214765100671,i ln3/2 Amax
O,0.8838926174496644,ϵ3(1 −γ)5.5
O,0.8845637583892617,"(iv)
= O
htmixAmax(ln4 Amax)"
O,0.885234899328859,"µminϵ5.5(1 −γ)13.5 ln

|S|Amax
ϵδµmin(1 −γ)"
O,0.8859060402684564,"i
(118)"
O,0.8865771812080537,"= e
O
 
tmixAmax
µminϵ5.5(1 −γ)13.5
"
O,0.887248322147651,"where (i) uses Nk,t ≤N k,t and eqs. (114)&(116), (ii) uses γ−1/2−1 = O(1−γ), 1−(1−ητ)3/5 =
O(ητ) = O
  ϵ(1−γ)2"
O,0.8879194630872483,"ln Amax

and (1−ητ)Tk−1 = γk, (iii) uses γ−3/5−1 = O(1−γ) and γK = ϵ5(1−γ)7.5"
O,0.8885906040268456,"ln5/2 Amax
implied by the hyperparameter choice (112), and (iv) uses the fact that the second term of (iii) is
larger than the ﬁrst term of (iii)."
O,0.889261744966443,"F
SUMMARY OF COMPARISON OF SAMPLE COMPLEXITIES"
O,0.8899328859060402,"In the column “Model-free”, a ✓means that an algorithm does not need any prior knowledge of the
environment, including reward mapping and transition kernel."
O,0.8906040268456376,"In the column “Private update”, a ✓means that the algorithm updates do not involve the opponent’s
sensitive information, including action and policy."
O,0.8912751677852349,Published as a conference paper at ICLR 2022
O,0.8919463087248322,"Table 1: Summary of sample complexity of algorithms for solving discounted inﬁnite-horizon zero-
sum Markov games. (|S| is the number of states. Amax is the maximum number of actions between
the players. µmin is the lower bound of state stationary distribution. γ is the discount factor.)"
O,0.8926174496644296,"Work
Model
Private
Symmetric
Data
Sample complexity
-free
update
update
type
(duality gap ≤ϵ)
Zou et al. (2019)
✓
×
✓
Markovian
–
Zhao et al. (2021)
✓
✓
×
i.i.d.
–
Guo et al. (2021)
✓
✓
×
i.i.d.
–
Cen et al. (2021)
×
✓
✓
–
–"
O,0.8932885906040269,"Wei et al. (2021)
✓
✓
✓
Markovian
eO
 
A3
max|S|10.5"
O,0.8939597315436242,"ϵ8µmin(1−γ)29.5
"
O,0.8946308724832215,"Our work
✓
✓
✓
Markovian
e
O
 
Amax
ϵ5.5µmin(1−γ)13.5
"
O,0.8953020134228188,"In the column “Symmetric update”, a ✓means that both players perform symmetric updates."
O,0.8959731543624161,"In the column “Data type”, “Markovian” means that the algorithm uses samples queried from the
dependent Markov decision process. The “–” sign means no stochastic samples are used."
O,0.8966442953020134,"The sample complexity is deﬁned as the total number of samples required to achieve an ϵ-duality
gap for the Markov game. A “–” sign in this column means that such type of sample complexity
is unavailable in that paper or its setting is different from ours. We use e
O to hide all the logarithm
factors."
O,0.8973154362416107,"We next explain how the sample complexity of (Wei et al., 2021) is calculated. Corollary 4 of (Wei
et al., 2021) shows that the iterate-average duality gap is smaller than ϵ with the hyper-parameter
choices: number of iterations T = e
O
h
|S|2"
O,0.897986577181208,"η2(1−γ)4ϵ2
i
and number of samples queried per iteration"
O,0.8986577181208054,"L = e
O
h
A3
max|S|6"
O,0.8993288590604027,"(1−γ)13µminη3ϵ6
i
(we replace ξ with ϵ). Hence, the overall sample complexity is"
O,0.9,"LT = e
O
h
A3
max|S|8"
O,0.9006711409395973,(1 −γ)17µminη5ϵ8
O,0.9013422818791946,"i (i)
≥e
O
h
A3
max|S|10.5"
O,0.9020134228187919,"(1 −γ)29.5µminϵ8 i
,"
O,0.9026845637583892,"where (i) uses the choice of learning rate η ≤O
hq"
O,0.9033557046979865,(1−γ)5
O,0.9040268456375838,"|S|
i
required by their Algorithm 1."
O,0.9046979865771813,"G
RATIONALITY"
O,0.9053691275167786,"In this section, we will prove that the policy extragradient (PE) algorithm Cen et al. (2021) is rational.
The following analysis can be directly extended to our stochastic policy extragradient (SPE) algorithm
by applying the estimation error bounds in Appendix B."
O,0.9060402684563759,"An algorithm is called rational if, whenever the player 2 adopts an arbitrary stationary pol-
icy π(2), the policy of the player 1 converges to the best response to π(2), i.e., eπ(1)
∗τ (s) =
arg maxπ(1)(s) V (τ)
π(1),π(2)(s) for all s ∈S. We add tilde in eπ(1)
∗τ (s) to differentiate it from the Nash"
O,0.9067114093959732,"policy pair (π(1)
∗τ (s), π(2)
∗τ (s)) and some consequent notations are similar. Denote eQ(τ)
∗
:= Q(τ)"
O,0.9073825503355705,"eπ(1)
∗τ ,π(2),"
O,0.9080536912751678,"eV (τ)
∗
:= V (τ)"
O,0.9087248322147651,"eπ(1)
∗τ ,π(2) as the optimal value functions and deﬁne the soft Bellman operator as follows"
O,0.9093959731543624,"eBτ(Q)(s, a(1), a(2))"
O,0.9100671140939597,":= R(s, a(1), a(2)) + γEs′∼P(·|s,a(1),a(2))
h
max
π(1)(s′) fτ
 
Q(s′); π(1)(s′), π(2)(s′)
i
.
(119)"
O,0.9107382550335571,"Similar to eqs. (98)&(99), it can be proved that the soft Bellman operator eBτ is also a contraction
operator and has a unique ﬁxed point eQ(τ)
∗, i.e.,"
O,0.9114093959731544,"∥eBτ(Q′) −eBτ(Q)∥∞≤γ∥Q′ −Q∥∞,
(120)"
O,0.9120805369127517,Published as a conference paper at ICLR 2022
O,0.912751677852349,"eBτ( eQ(τ)
∗) = eQ(τ)
∗.
(121)"
O,0.9134228187919463,"To solve modiﬁed Markov game with ﬁxed policy π(2), we develop the single-player-perspective
version of the PE algorithm in Algorithm 2, which basically approximates the value iteration Qk+1 =
eBτ(Qk). The main modiﬁcation from the original PE algorithm is that the new policy update (126)
only keeps the third expression of the PU steps (8) since only that affects the ﬁnal policy of player
1 with ﬁxed π(2). This new policy update (126) aims to ﬁnd π(1) that maximizes the following
single-player Markov game problem with ﬁxed Qk(s) and π(2)(s) for all s ∈S, while the original
PU algorithm seeks the Nash equilibrium (π(1), π(2)) of the following function."
O,0.9140939597315436,"max
π(1)(s) fτ(Qk(s); π(1)(s), π(2)(s))"
O,0.9147651006711409,":= [π(1)(s)]⊤Qk(s)π(2)(s) + τH
 
π(1)(s)

−τH
 
π(2)(s)

.
(122)"
O,0.9154362416107382,"Similar to eq. (23), it can be derived that the solution eπ∗(1)
k
(s) to the above optimization problem
satisﬁes the following condition Cen et al. (2021)."
O,0.9161073825503355,"eπ∗(1)
k
(s) ∝exp(Qk(s)π2(s)/τ).
(123)"
O,0.9167785234899329,"Algorithm 2 Single-player perspective PE algorithm for entropy-regularized Markov game.
Initialize: V0(s) for all s ∈S.
for value iterations k = 0, 1, . . . , K −1 do"
O,0.9174496644295302,"Compute the following Q functions for all (s, a(1), a(2))."
O,0.9181208053691275,"Qk(s, a(1), a(2)) =R(s, a(1), a(2)) + γEs′∼P(s,a(1),a(2))Vk(s′)
(124)"
O,0.9187919463087248,"Q(1)
k (s, a(1)) =Ea(2)∼π(2)(s)Qk(s, a(1), a(2))
(125)"
O,0.9194630872483222,"Initialize π(1)
k,0 with uniform distribution.
for PU iterations t = 0, 1, . . . , T −1 do"
O,0.9201342281879195,"Player 1 implements the t-th policy update for all s, a(1) as follows."
O,0.9208053691275168,"π(1)
k,t+1(a(1)|s) ∝π(1)
k,t(a(1)|s)1−ητ exp
 
ηQ(1)
k (s, a(1))

,
(126)"
O,0.9214765100671141,"end
Let π(1)
k
= π(1)
k,T , and perform the following value iteration for all s"
O,0.9221476510067114,"Vk+1(s) := fτ(Qk(s); π(1)
k (s), π(2)(s))
(127)"
O,0.9228187919463087,"end
Output: π(1)
K−1."
O,0.9234899328859061,We ﬁrst prove that the value functions Vk and Qk in Algorithm 2 are bounded as follows.
O,0.9241610738255034,Lemma 7. If ∥V0∥∞≤Vmax := 1+τ ln Amax
O,0.9248322147651007,"1−γ
in Algorithm 2, then for all k ≥0, 0 ≤t ≤Tk −1, we
have"
O,0.925503355704698,"max
 
∥Vk∥∞, ∥eV (τ)
∗
∥∞

≤Vmax,
(128)"
O,0.9261744966442953,"max
 
∥Qk∥∞, eQ(τ)
∗∥∞∥∞

≤Qmax,
(129)"
O,0.9268456375838926,where Qmax := 1 + γVmax = 1+γτ ln Amax
O,0.9275167785234899,"1−γ
with Amax := max
 
|A(1)|, |A(2)|

."
O,0.9281879194630872,Proof. The proof is similar to that of Lemma 3.
O,0.9288590604026845,"Similar to Lemma 1, we can prove the following lemma about the convergence rates of the modiﬁed
PU steps deﬁned by eq. (126)."
O,0.9295302013422819,Published as a conference paper at ICLR 2022
O,0.9302013422818792,"Lemma 8. Apply Algorithm 2 to solve the entropy regularized game with τ > 0. Choose learning
rate η ≤τ −1 and initialization ∥V0∥∞≤Vmax. Then, the policy update deﬁned by eq. (126) for
maximizing the function (122) has the following convergence rates."
O,0.9308724832214765,"KL(eπ∗(1)
k
(s)∥π(1)
k,T (s)) + KL
 
π(1)
k,T (s)∥eπ∗(1)
k
(s)

≤2Qmax"
O,0.9315436241610738,"τ
(1 −ητ)T
(130)"
O,0.9322147651006711,"fτ(Qk(s); eπ∗(1)
k
(s), π(2)(s)) −Vk+1(s) = τKL
 
π(1)
k,T (s)∥eπ∗(1)
k
(s)

≤2Qmax(1 −ητ)T
(131)"
O,0.9328859060402684,"Proof. Taking logarithm of eq. (126) yields that there exists ck,t ∈R such that"
O,0.9335570469798657,"ln π(1)
k,t+1(s) = (1 −ητ) ln π(1)
k,t(s) + ηQk(s)π2(s) + ck,t1."
O,0.934228187919463,"Iterating the above equality over t = 0, 1, . . . , t yields that there exists c ∈R such that"
O,0.9348993288590604,"ln π(1)
k,T (s) = 1 −(1 −ητ)T"
O,0.9355704697986578,"τ
Qk(s)π2(s) + c1
(132)"
O,0.9362416107382551,"where we use the uniform policy initialization π(1)
k,0(s) = 1/|A(1)|."
O,0.9369127516778524,Taking logarithm of eq. (123) yields that there exists c∗∈R such that
O,0.9375838926174497,"ln eπ∗(1)
k
(s) = Qk(s)π2(s)/τ + c∗1
(133)"
O,0.938255033557047,"Therefore, eq. (130) can be proved as follows."
O,0.9389261744966443,"KL(eπ∗(1)
k
(s)∥π(1)
k,T (s)) + KL(π(1)
k,T (s)∥eπ∗(1)
k
(s))"
O,0.9395973154362416,"=

eπ∗(1)
k
(s), ln eπ∗(1)
k
(s) −ln π(1)
k,T (s)

+

π(1)
k,T (s), ln π(1)
k,T (s) −ln eπ∗(1)
k
(s)"
O,0.9402684563758389,"(i)
=
D
eπ∗(1)
k
(s) −π(1)
k,T (s), (1 −ητ)T"
O,0.9409395973154362,"τ
Qk(s)π2(s) + (c∗−c)1
E"
O,0.9416107382550336,"(ii)
= (1 −ητ)T"
O,0.9422818791946309,"τ
[eπ∗(1)
k
(s) −π(1)
k,T (s)]⊤Qk(s)π2(s)"
O,0.9429530201342282,"(iii)
≤2Qmax"
O,0.9436241610738255,"τ
(1 −ητ)T"
O,0.9442953020134228,"where (i) uses eqs. (132)&(133), (ii) uses ⟨eπ∗(1)
k
(s) −π(1)
k,T (s), 1⟩= 0 and (iii) uses ∥Qk(s)∥∞≤"
O,0.9449664429530201,"Qmax, ∥eπ∗(1)
k
−π(1)
k,T ∥1 ≤2, ∥π2(s)∥1 = 1 and 1 −ητ > 0."
O,0.9456375838926174,"fτ(Qk(s); eπ∗(1)
k
(s), π(2)(s)) −Vk+1(s)"
O,0.9463087248322147,"= fτ(Qk(s); eπ∗(1)
k
(s), π(2)(s)) −fτ(Qk(s); π(1)
k,T (s), π(2)(s))"
O,0.946979865771812,"=

eπ∗(1)
k
(s) −π(1)
k,T (s), Qk(s)π(2)(s)

+ τ

π(1)
k,T (s), ln π(1)
k,T (s)

−τ

eπ∗(1)
k
(s), ln eπ∗(1)
k
(s)"
O,0.9476510067114094,"(i)
=

eπ∗(1)
k
(s) −π(1)
k,T (s), τ ln eπ∗(1)
k
(s)

+ τ

π(1)
k,T (s), ln π(1)
k,T (s)

−τ

eπ∗(1)
k
(s), ln eπ∗(1)
k
(s)"
O,0.9483221476510068,"= τKL
 
π(1)
k,T (s)∥eπ∗(1)
k
(s)
"
O,0.9489932885906041,"(ii)
≤2Qmax(1 −ητ)T"
O,0.9496644295302014,"where (i) uses eq. (133) and

eπ∗(1)
k
(s) −π(1)
k,T (s), 1

= 0 and (ii) uses eq. (130)."
O,0.9503355704697987,"Finally, we prove the convergence rate of Algorithm 2 as follows.
Theorem 3. Apply Algorithm 2 to solve the entropy regularized game with τ > 0. Choose learning
rate η ≤τ −1 and initialization ∥V0∥∞≤Vmax. Then, the Q function estimation error ∥QK−1 −
eQ(τ)
∗∥∞and the function value gap maxs
eV (τ)
∗
(s) −V (τ)"
O,0.951006711409396,"π(1)
K−1,π(2)(s)

converge at the following rates"
O,0.9516778523489933,"∥QK−1 −eQ(τ)
∗∥∞≤2QmaxγK + 2γQmax"
O,0.9523489932885906,"1 −γ (1 −ητ)T ,
(134) max
s"
O,0.9530201342281879,"h
eV (τ)
∗
(s) −V (τ)"
O,0.9536912751677852,"π(1)
K−1,π(2)(s)
i
≤2Qmax 1 −γ"
O,0.9543624161073826,"
2γK−1 + 1 + γ"
O,0.9550335570469799,"1 −γ (1 −ητ)T

.
(135)"
O,0.9557046979865772,Published as a conference paper at ICLR 2022
O,0.9563758389261745,Proof.
O,0.9570469798657718,"∥Qk+1 −eQ(τ)
∗∥∞"
O,0.9577181208053691,"(i)
≤∥Qk+1 −eBτ(Qk)∥∞+ ∥eBτ(Qk) −eBτ( eQ(τ)
∗)∥∞"
O,0.9583892617449664,"(ii)
≤
max
s,a(1),a(2) |Qk+1(s, a(1), a(2)) −eBτ(Qk)(s, a(1), a(2))| + γ∥Qk −eQ(τ)
∗∥∞"
O,0.9590604026845637,"=
max
s,a(1),a(2)"
O,0.959731543624161,"R(s, a(1), a(2)) + γEs′∼P(·|s,a(1),a(2))Vk+1(s′)"
O,0.9604026845637584,"−
h
R(s, a(1), a(2)) + γEs′∼P(·|s,a(1),a(2))fτ
 
Qk(s′); eπ∗(1)
k
(s′), π(2)(s′)
i + γ∥Qk −eQ(τ)
∗∥∞"
O,0.9610738255033557,"≤γ max
s′∈S
Vk+1(s′) −fτ
 
Qk(s′); eπ∗(1)
k
(s′), π(2)(s′)
 + γ∥Qk −eQ(τ)
∗∥∞"
O,0.961744966442953,"(iii)
≤2γQmax(1 −ητ)T + γ∥Qk −eQ(τ)
∗∥∞,"
O,0.9624161073825503,"where (i) uses eq. (121), (ii) uses eq. (120), (iii) uses eq. (131). Iterating the above inequality yields
that"
O,0.9630872483221476,"∥QK−1 −eQ(τ)
∗∥∞≤γK∥Q0 −eQ(τ)
∗∥∞+ 2γQmax(1 −ητ)T 1 −γT 1 −γ"
O,0.963758389261745,≤2QmaxγK + 2γQmax
O,0.9644295302013423,"1 −γ (1 −ητ)T ,"
O,0.9651006711409396,"which proves eq. (134). Finally, eq. (135) can be proved as follows"
O,0.9657718120805369,"eV (τ)
∗
(s) −V (τ)"
O,0.9664429530201343,"π(1)
K−1,π(2)(s) =
X"
O,0.9671140939597316,"a(1),a(2)"
O,0.9677852348993289," eQ(τ)
∗(s, a(1), a(2))eπ(1)
∗τ (a(1)|s) −Q(τ)"
O,0.9684563758389262,"π(1)
K−1,π(2)(s, a(1), a(2))π(1)
K−1(a(1)|s)

π(2)(a(2)|s)"
O,0.9691275167785235,"+ τ

H
 
eπ(1)
∗τ (s)

−H
 
π(1)
K−1(s)
 =
X"
O,0.9697986577181208,"a(1),a(2)"
O,0.9704697986577181," eQ(τ)
∗(s, a(1), a(2)) −Q(τ)"
O,0.9711409395973154,"π(1)
K−1,π(2)(s, a(1), a(2))

π(1)
K−1(a(1)|s)π(2)(a(2)|s) +
X"
O,0.9718120805369127,"a(1),a(2)
eQ(τ)
∗(s, a(1), a(2))

eπ(1)
∗τ (a(1)|s) −π(1)
K−1(a(1)|s)

π(2)(a(2)|s)"
O,0.9724832214765101,"+ τ

H
 
eπ(1)
∗τ (s)

−H
 
π(1)
K−1(s)
 = γ
X"
O,0.9731543624161074,"a(1),a(2)"
O,0.9738255033557047,"h
π(1)
K−1(a(1)|s)π(2)(a(2)|s)Es′∼P(·|s,a(1),a(2))
eV (τ)
∗
(s′) −V (τ)"
O,0.974496644295302,"π(1)
K−1,π(2)(s′)
i"
O,0.9751677852348993,"+
 
eπ(1)
∗τ (s) −π(1)
K−1(s)
⊤eQ(τ)
∗(s)π(2)(s) + τ

H
 
eπ(1)
∗τ (s)

−H
 
π(1)
K−1(s)
"
O,0.9758389261744966,"≤γ max
s′
eV (τ)
∗
(s′) −V (τ)"
O,0.9765100671140939,"π(1)
K−1,π(2)(s′)

+
 
eπ(1)
∗τ (s) −π(1)
K−1(s)
⊤eQ(τ)
∗(s)π(2)(s)"
O,0.9771812080536912,"+ τ

H
 
eπ(1)
∗τ (s)

−H
 
π(1)
K−1(s)

."
O,0.9778523489932885,Applying maxs to both sides of the above inequality and rearranging it yields that
O,0.978523489932886,"max
s
eV (τ)
∗
(s) −V (τ)"
O,0.9791946308724833,"π(1)
K−1,π(2)(s)
"
O,0.9798657718120806,"≤
1
1 −γ max
s"
O,0.9805369127516779," 
eπ(1)
∗τ (s) −π(1)
K−1(s)
⊤eQ(τ)
∗(s)π(2)(s) + τ

H
 
eπ(1)
∗τ (s)

−H
 
π(1)
K−1(s)
"
O,0.9812080536912752,"≤
1
1 −γ max
s"
O,0.9818791946308725,"
fτ
  eQ(τ)
∗(s); eπ(1)
∗τ (s), π(2)(s)

−fτ
  eQ(τ)
∗(s); π(1)
K−1(s), π(2)(s)
"
O,0.9825503355704698,"≤
1
1 −γ max
s"
O,0.9832214765100671,"
fτ
  eQ(τ)
∗(s); eπ(1)
∗τ (s), π(2)(s)

−fτ
 
QK−1(s); eπ∗(1)
K−1, π(2)(s)
"
O,0.9838926174496644,"+
1
1 −γ max
s"
O,0.9845637583892617,"
fτ
 
QK−1(s); eπ∗(1)
K−1(s), π(2)(s)

−fτ
 
QK−1(s); π(1)
K−1(s), π(2)(s)
"
O,0.9852348993288591,Published as a conference paper at ICLR 2022
O,0.9859060402684564,"+
1
1 −γ max
s"
O,0.9865771812080537,"
fτ
 
QK−1(s); π(1)
K−1(s), π(2)(s)

−fτ
  eQ(τ)
∗(s); π(1)
K−1(s), π(2)(s)
"
O,0.987248322147651,"≤
1
1 −γ max
s"
O,0.9879194630872483,"
max
π(1)(s) fτ
  eQ(τ)
∗(s); π(1)(s), π(2)(s)

−

max
π(1)(s) fτ
 
QK−1(s); π(1)(s), π(2)(s)
"
O,0.9885906040268456,"+
1
1 −γ max
s"
O,0.9892617449664429,"
fτ
 
QK−1(s); eπ∗(1)
K−1(s), π(2)(s)

−VK(s)
"
O,0.9899328859060402,"+
1
1 −γ max
s"
O,0.9906040268456375,"fτ
 
QK−1(s); π(1)
K−1(s), π(2)(s)

−fτ
  eQ(τ)
∗(s); π(1)
K−1(s), π(2)(s)
"
O,0.991275167785235,"(i)
≤
2
1 −γ"
O,0.9919463087248322,"
2QmaxγK + 2γQmax"
O,0.9926174496644296,"1 −γ (1 −ητ)T

+ 2Qmax"
O,0.9932885906040269,1 −γ (1 −ητ)T
O,0.9939597315436242,≤2Qmax 1 −γ
O,0.9946308724832215,"
2γK−1 + 1 + γ"
O,0.9953020134228188,"1 −γ (1 −ητ)T

,"
O,0.9959731543624161,"where (i) uses eq. (131) and the following inequality that holds for all s, π(1)(s), π(2)(s), (ii) uses eq.
(134) and VK−1(s) = fτ
 
Q(τ)
K−1(s); π(1)
K−1(s), π(2)(s)

.
fτ
 
QK−1(s); π(1)
K−1(s), π(2)(s)

−fτ
  eQ(τ)
∗(s); π(1)
K−1(s), π(2)(s)
"
O,0.9966442953020134,"≤∥QK−1(s) −eQ(τ)
∗(s)∥∞
eq.(134)
≤
2QmaxγK + 2γQmax"
O,0.9973154362416108,1 −γ (1 −ητ)T .
O,0.9979865771812081,"In Theorem 3, the convergence rates of both the Q function estimation error ∥QK−1−eQ(τ)
∗∥∞and the
function value gap maxs
eV (τ)
∗
(s) −V (τ)"
O,0.9986577181208054,"π(1)
K−1,π(2)(s)

consist of two terms O(γK) and O(1 −ητ)T ,"
O,0.9993288590604027,"which characterize the exponential convergence rates of the K outer value iterations and the T
inner policy updates respectively. Such a convergence result indicates that Algorithm 2 converges
to the optimal solution to the regularized Markov game, and thus can be arbitrarily close to the
optimal solution of the unregularized Markov game with sufﬁciently small τ. As a result, the PU
algorithm Cen et al. (2020) is rational. Similar result can be directly extended to our stochastic policy
extragradient (SPE) algorithm by applying the estimation error bounds in Appendix B."
