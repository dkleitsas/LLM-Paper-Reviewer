Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00129366106080207,"Bayesian optimization is a methodology to optimize black-box functions. Tra-
ditionally, it focuses on the setting where you can arbitrarily query the search
space. However, many real-life problems do not offer this flexibility; in particular,
the search space of the next query may depend on previous ones. Example chal-
lenges arise in the physical sciences in the form of local movement constraints,
required monotonicity in certain variables, and transitions influencing the accuracy
of measurements. Altogether, such transition constraints necessitate a form of
planning. This work extends classical Bayesian optimization via the framework of
Markov Decision Processes. We iteratively solve a tractable linearization of our
utility function using reinforcement learning to obtain a policy that plans ahead
for the entire horizon. This is a parallel to the optimization of an acquisition
function in policy space. The resulting policy is potentially history-dependent
and non-Markovian. We showcase applications in chemical reactor optimization,
informative path planning, machine calibration, and other synthetic examples."
INTRODUCTION,0.00258732212160414,"1
Introduction"
INTRODUCTION,0.0038809831824062097,"Many areas in the natural sciences and engineering deal with optimizing expensive black-box
functions. Bayesian optimization (BayesOpt) [1–3], a method to optimize these problems using a
probabilistic surrogate, has been successfully applied to a myriad of examples, e.g. hyper-parameter
selection [4], robotics [5], battery design [6], laboratory equipment tuning [7], and drug discovery
[8]. However, state-of-the-art algorithms are often ill-suited when physical sciences interact with
potentially dynamic systems [9]. In such circumstances, real-life constraints limit our future decisions
while depending on the prior state of our interaction with the system. This work focuses on transition
constraints influencing future choices depending on the current state of the experiment. In other
words, reaching certain parts of the decision space (search space) requires long-term planning in
our optimization campaign. This effectively means we address a general sequential-decision problem
akin to those studied in reinforcement learning (RL) or optimal control for the task of optimization.
We assume the transition constraints are known a priori to the optimizer."
INTRODUCTION,0.00517464424320828,"Applications with transition constraints include chemical reaction optimization [10–12], environ-
mental monitoring [13–17], lake surveillance with drones [18–20], energy systems [21], vapor
compression systems [22], electron-laser tuning [23] and seabed identification [24]. For example,"
INTRODUCTION,0.00646830530401035,"Figure 1 depicts an application in environmental monitoring where autonomous sensing vehicles must
avoid obstacles (similar to Hitz et al. [18]). Our main focus application are transient flow reactors
[25–27]. Such reactors allow efficient data collection by obtaining semi-continuous time-series data
rather than a single measurement after reaching the steady state of the reactor. As we can only change
the inputs of the reactor continuously and slowly to maintain quasi-steady-state operation, allowing
arbitrary changes, as in conventional BayesOpt, would result in measurement sequences which are
not possible due to physical limitations."
INTRODUCTION,0.007761966364812419,"Problem Statement. More formally, we design an algorithm to identify the optimal configuration
of a physical system governed by a black box function f, namely, x⋆= arg maxx∈X f(x). The set
X summarizes all possible system configurations, the so called search space. We assume that we
can sequentially evaluate the unknown function at specific points x in the search space and obtain
noisy observations, y = f(x) + ϵ(x), where ϵ has a known Gaussian likelihood, which is possibly
heteroscedastic. We assume that f can be modeled probabilistically using a Gaussian process prior
that we introduce later. Importantly, the order of the evaluations is dictated by known, potentially
stochastic, dynamics modeled by a Markov chain that limits our choices of x ∈X."
INTRODUCTION,0.009055627425614488,"BayesOpt with a Markov Decision Processes. The problem of maximizing an unknown function
could be addressed by BayesOpt, which typically chooses to query f(x) by sequentially maximizing
an acquisition function, u:
xt+1 = arg max
x∈X
u(x|Xt),
(1)"
INTRODUCTION,0.01034928848641656,"depending on all the past data at iteration t, Xt. Eq. (1) arises as a greedy one-step approximation
whose overall goal is to minimize e.g. cumulative regret, and assumes that any choice of point in
the search space X is available. However, given transition constraints, we must traverse the search
space according to the system dynamics. This work extends the BayesOpt framework and provides a
method that constructs a potentially non-Markovian policy by myopically optimizing a utility as,"
INTRODUCTION,0.01164294954721863,"πt+1 = arg max
π∈Π
U(π|Xt),
(2)"
INTRODUCTION,0.0129366106080207,"where U is the greedy utility of the policy π and Xt encodes past trajectories through the search
space. In the following sections, we will show how to tractably formulate the overall utility, how to
greedily maximize it, and how to adapt it to admit policies depending on the full optimization history."
INTRODUCTION,0.014230271668822769,"Contributions. We present a BayesOpt framework that tractably plans over the complete experimen-
tation horizon and respects Markov transition constraints, building on active exploration in Markov
chains [17]. Our key contributions include:"
INTRODUCTION,0.015523932729624839,"• We identify a novel utility function for maximum identification as a function of policies, and
greedily optimize it. The optimization is tractable, and does not scale exponentially in the
policy horizon. In many cases, the problem is convex in the natural representation."
INTRODUCTION,0.016817593790426907,"• We provide exact solutions to the optimization problems using convex optimization for
discrete Markov chains. For continuous Markov chains, we propose a reparameterization
by viewing our problem as an instance of model predictive control (MPC) with a non-
convex objective. Interestingly, in both cases, the resulting policies are history-dependent
(non-Markovian)."
INTRODUCTION,0.018111254851228976,"• We analyze the scheme theoretically and empirically demonstrate its practicality on problems
in physical systems, such as electron laser calibration and chemical reactor optimization."
BACKGROUND,0.019404915912031046,"2
Background"
BACKGROUND,0.02069857697283312,"Because our contributions address experimental design of real-life systems by intersecting design
of experiments, BayesOpt and RL, we review each of these of components. Refer to Figure 5 in
Appendix A for a visual overview of how we selected the individual components for tractability
of the entire problem."
BACKGROUND,0.02199223803363519,"Gaussian Processes
To model the unknown function f, we use Gaussian processes (GPs) [28].
GPs are probabilistic models that capture nonlinear relationships and offer well-calibrated uncertainty
estimates. Any finite marginal of a GP, e.g., for inputs (x1, .., xp), the values {f(xj)}p
j=1, are
normally distributed. We adopt a Bayesian approach and assume f is a sample from a GP prior with
a known covariance kernel, k, and zero mean function, f ∼GP(0, k). Under these assumptions,
the posterior of f, given a Gaussian likelihood of data, is a GP that is analytically tractable."
BACKGROUND,0.02328589909443726,"Figure 1: Representative task of finding pollution in a river while following the current. (a) Problem
formulation: The star represents the maximizer and the arrows the Markov dynamics. (b) Objective
formulation: Orange balls represent potential maximizers, with size corresponding to model uncer-
tainty. (c) Optimization: Deploy a potentially stochastic policy that minimizes our objective."
BACKGROUND,0.02457956015523933,"2.1
Maximum Identification: Experiment Design Goal
Classical BayesOpt is naturally myopic in its definition as a greedy one-step update (see (1)), but
has the overall goal to minimize, e.g., the cumulative regret. Therefore u needs to chosen such that
overall non-myopic goals can be achieved, usually defined as balancing an exploration-exploitation
trade-off. In this paper we follow similar ideas; however, we do not focus on regret but instead on
gathering information to maximize our chances to identify x⋆, the maximizer of f."
BACKGROUND,0.0258732212160414,"Maximum Identification via Hypothesis testing. Maximum identification can naturally be expressed
as a multiple hypothesis testing problem, where we need to determine which of the elements in X is
the maximizer. To do so, we require good estimates of the differences (or at least their signs) between
individual queries f(xi) −f(xj); xi, xj ∈X. For example, if f(xi) −f(xj) ≤0, then xi cannot be
a maximizer. Given the current evidence, the set of arms which we cannot rule out are all potential
maximizers, Z ⊂X. At termination we report our best guess for the maximizer as:"
BACKGROUND,0.027166882276843468,"xT = arg max
x∈Z
µT (x),
where µT is the predictive mean at termination time T."
BACKGROUND,0.028460543337645538,"Suppose we are in step t out of T, then let Xt be the set of previous queries, we seek to identify
new Xnew that when evaluated minimize the probability of returning a sub-optimal arm at the
end. For a given function draw f, the probability of returning a wrong maximizer z ̸= x⋆
f is
P(µT (z) −µT (x⋆
f) ≥0|f). We can then consider the worst-case probability across potential
maximizers, and taking expectation over f we obtain a utility through an asymptotic upper-bound on
the log-probability, indeed for large T we obtain: Ef """
BACKGROUND,0.029754204398447608,"sup
z∈Z\{x⋆
f }
log P(µT (z) −µT (x⋆
f) ≥0|f)"
BACKGROUND,0.031047865459249677,"#
·
≤−1"
EF,0.03234152652005175,"2Ef """
EF,0.03363518758085381,"sup
z∈Z\{x⋆
f }"
EF,0.03492884864165589,"(f(z) −f(x⋆
f))2"
EF,0.03622250970245795,"kXt∪Xnew(z, x⋆
f) # (3)"
EF,0.037516170763260026,"The expectation is on the current prior (posterior up to Xt), the kernel k is the posterior kernel given
observations Xt ∪Xnew. Since we consider the probability of an error, it is more appropriate to
talk about minimizing instead of ‘maximizing the utility’ but the treatment is analogous. Further,
note the intuitive interpretation of the bound: the probability of an error will be minimized if the
uncertainty is small or if the values of f(z) and f(x⋆
f) are far apart. The non-trivial distribution of
f(x⋆) [29] renders the utility intractable; therefore we employ a simple and tractable upper bound on
the objective (3) which can be optimized by minimizing the uncertainty among all pairs in Z:"
EF,0.03880983182406209,"U(Xnew) =
max
z′,z∈Z,z̸=z′ Var[f(z) −f(z′)|Xt ∪Xnew].
(4)"
EF,0.040103492884864166,"Such objectives can be solved greedily in a similar way as acquisition functions in Eq. (1) by
minimizing U over Xnew. Note that Fiez et al. [30] derive this objective for the same problem with
linear bandits, albeit they consider the frequentist setting and (surprisingly) a different optimality
criterion: minimizing T for a given failure rate. For their setting, the authors prove that it is an
asymptotically optimal objective to follow. They do not consider any Markov chain structure.
Derivation of the Bayesian utility and its upper bound in Eq.(4) can be found in Appendix C.1–C.2."
EF,0.04139715394566624,"Utility with kernel embeddings. For illustrative purposes, consider a special case where the kernel
k has a low rank due to existence of embeddings Φ(x) ∈Rm, i.e., k(x, y) = Φ(x)⊤Φ(y). Such
embeddings can be, e.g., Nyström features [31] or Fourier features [32, 33]. While not necessary,"
EF,0.042690815006468305,"these formulations make the objectives considered in this work more tractable and easier to expose
to the reader. With the finite rank assumption, the random function f becomes,"
EF,0.04398447606727038,"f(x) = Φ(x)T θ
and
θ ∼N(0, Im×m)
(5)"
EF,0.045278137128072445,where θ are weights with a Gaussian prior. We can then rewrite the objective Eq. (4) as:
EF,0.04657179818887452,"U(Xnew) = max
z,z′∈Z ||Φ(z) −Φ(z′)||2P"
EF,0.047865459249676584,"x∈Xt∪Xnew
Φ(x)Φ(x)⊤"
EF,0.04915912031047866,"σ2
+I
−1.
(6)"
EF,0.050452781371280724,"This reveals an essential observation that the utility depends only on the visited states; not their
order. This suggests a vast simplification, where we do not to model whole trajectories, and Markov
decision processes sufficiently describe our problem. Additionally, numerically, the objective
involves the inversion of an m × m matrix instead of |X| × |X| (see Sec. 4). Appendix D.1 provides
a utility without the finite rank-assumptions that is more involved symbolically and computationally."
MARKOV DECISION PROCESSES,0.0517464424320828,"2.2
Markov Decision Processes
To model the transition constraints, we use the versatile model of Markov Decision processes
(MDPs). We assume an environment with state space X and action space A, where we interact
with an unknown function f : X × A →R by rolling out a policy for H time-steps (horizon) and
obtain a trajectory, τ = (x0, a0, x1, a1, ..., xH−1, aH−1). From the trajectory, we obtain a sequence
of noisy observations y(τ) := {y(x0, a0), ..., y(xH−1, aH−1)} s.t. y(xh) = f(xh, ah) + ϵ(xh, ah),
where ϵ(xh, ah) is zero-mean Gaussian with known variance which is potentially state and action
dependent. The trajectory is generated using a known transition operator P(xh+1|xh, ah). A Markov
policy π(ah|xh) is a mapping that dictates the probability of action ah in state xh. Hence, the
state-to-state transitions are P(xh+1, xh) = P"
MARKOV DECISION PROCESSES,0.05304010349288486,"a∈A πh(a|xh)P(xh+1|xh, a). In fact, an equivalent
description of any Markov policy π is the corresponding distribution giving us the probability of
visiting a state-action pair under the policy, which we denote dπ ∈D, where"
MARKOV DECISION PROCESSES,0.054333764553686936,"D :=
n
∀h ∈[H] dh | dh(x, a) ≥0,
X"
MARKOV DECISION PROCESSES,0.055627425614489,"a,x
dh(x, a) = 1,
X"
MARKOV DECISION PROCESSES,0.056921086675291076,"a
dh(x′, a) =
X"
MARKOV DECISION PROCESSES,0.05821474773609314,"x,a
dh−1(x, a)p(x′|x, a)
o"
MARKOV DECISION PROCESSES,0.059508408796895215,"We will use this polytope to reformulate our optimization problem over trajectories. Any d ∈D
can be realized by a Markov policy π and vice-versa. We work with non-stationary policies, meaning
the policies depend on horizon count h. The execution of deterministic trajectories is only possible
for deterministic transitions. Otherwise, the resulting trajectories are random. In our setup, we repeat
interactions T times (episodes) to obtain the final dataset of the form XT = {τi}T
i=1."
EXPERIMENT DESIGN IN MARKOV CHAINS,0.06080206985769728,"2.3
Experiment Design in Markov Chains
Notice that the utility U in Eq. 6 depends on the states visited and hence states of the trajectory. In
our notation, Xt will now form a set of executed trajectories. With deterministic dynamics, we could
optimize over trajectories, but this would lead to an exponential blowup (i.e. |X|H). In fact, for
stochastic transitions, we cannot pick the trajectories directly, so instead we work in the space of
distributions. For a given policy, through sampling, we are able to create an empirical distribution
of all the state-action pairs visited during policy executions, ˆdπ(x, a), which assigns equal mass
to each state-action visited during our trajectories. This allows us to focus on the expected utility
over the randomness of the policy and the environment, namely,"
EXPERIMENT DESIGN IN MARKOV CHAINS,0.062095730918499355,"U(dπ) := U(Eτ1∼π1,...τt∼πt[ ˆdπ]).
(7)"
EXPERIMENT DESIGN IN MARKOV CHAINS,0.06338939197930142,"This formulation stems from Mutný et al. [17] who try to tractably solve such objectives that arise
in experiment design by performing planning in MDPs. They focus on learning linear operators
of an unknown function, unlike identifying a maximum, as we do here. The key observation they
make is that any policy π induces a distribution over the state-action visitations, dπ. Therefore we
can reformulate the problem of finding the optimal policy, into finding the optimal distribution over
state-action visitations as: mindπ∈D U(dπ), and then construct policy π via marginalization. We refer
to this optimization as the planning problem. The constraint D encodes the dynamics of the MDP."
"ADDITIONAL RELATED WORKS
THE MOST RELEVANT PRIOR WORK TO OURS IS EXPLORATION IN REINFORCEMENT LEARNING THROUGH THE USE OF",0.0646830530401035,"2.4
Additional Related Works
The most relevant prior work to ours is exploration in reinforcement learning through the use of
Markov decision processes as in Mutný et al. [17] and convex reinforcement learning of Hazan et al.
[34], Zahavy et al. [35] which we will use to optimize the objective. Other related works are:"
"ADDITIONAL RELATED WORKS
THE MOST RELEVANT PRIOR WORK TO OURS IS EXPLORATION IN REINFORCEMENT LEARNING THROUGH THE USE OF",0.06597671410090557,"Pure exploration bandits objectives. Similar objectives to ours have been explored for BayesOpt.
Li and Scarlett [36] use the G-allocation variant of our objective for batch BayesOpt, achieving
good theoretical bounds. Zhang et al. [37] and recently Han et al. [38] take advantage of possible
maximizer sets to train localized models, while Salgia et al. [39] show that considering adaptive
maximization sets yields good regret bounds under random sampling. Contrary to them, motivation
and derivation in terms of a Bayesian decision rule do not appear elsewhere according to our best
knowledge. We also recognize that we can relax the objective and optimize it in the space of policies."
"ADDITIONAL RELATED WORKS
THE MOST RELEVANT PRIOR WORK TO OURS IS EXPLORATION IN REINFORCEMENT LEARNING THROUGH THE USE OF",0.06727037516170763,"Optimizing over sequences. Previous work has focused on planning experimental sequences for
minimizing switching costs [11, 21, 40, 41] however they are only able adhere to strict constraints
under truncation heuristics [20, 22, 42]. Recently, Qing et al. [43] also tackle Bayesian optimization
within dynamical systems, with the focus of optimizing initial conditions. Concurrent work of Che
et al. [44] tackles a constrained variant of a similar problem using model predictive control with a
different goal."
"ADDITIONAL RELATED WORKS
THE MOST RELEVANT PRIOR WORK TO OURS IS EXPLORATION IN REINFORCEMENT LEARNING THROUGH THE USE OF",0.0685640362225097,"Regret vs Best-arm identification. Most algorithms in BayesOpt focus on regret minimization.
This work focuses on maximizer identification directly, i.e., to identify the maximum after a certain
number of iterations with the highest confidence. This branch of BayesOpt is mostly addressed in
the bandit literature [45]. Our work builds upon prior works of Soare et al. [46], Yu et al. [47], and
specifically upon the seminal approach of Fiez et al. [30] to design an optimal objective via hypothesis
testing. Novel to our setting is the added difficulty of transition constraints necessitating planning."
"ADDITIONAL RELATED WORKS
THE MOST RELEVANT PRIOR WORK TO OURS IS EXPLORATION IN REINFORCEMENT LEARNING THROUGH THE USE OF",0.06985769728331177,"Non-myopic Bayesian Optimization. Look-ahead BayesOpt [48–54] seeks to improve the greedy
aspect of BayesOpt. Such works also use an MDP problem formulation, however, they define the
state space to include all past observations (e.g. [55, 56]). This comes at the cost of simulating
expensive integrals, and the complexity grows exponentially with the number of look-ahead steps
(usually less than three steps). Our work follows a different path, we maintain the greedy approach
to control computational efficiency (i.e. by optimizing over the space of Markovian policies), and
maintain provable and state-of-art performance. Even though the optimal policy through non-myopic
analysis is non-Markovian, in Sec. 4, we show that adaptive resampling iteratively approximates this
non-myoptic optimal policies in a numerically tractable way via receeding horizon planning. In our
experiments we comfortably plan for over a hundred steps."
TRANSITION CONSTRAINED BAYESOPT,0.07115135834411385,"3
Transition Constrained BayesOpt"
TRANSITION CONSTRAINED BAYESOPT,0.0724450194049159,"This section introduces BayesOpt with transition constraints. We use MDPs to encode constraints.
Namely, the Markov dynamics dictates which inputs we are allowed to query at time-step h + 1
given we previously queried state xh. This mean that the transition operator is P(xh+1|xh, a) = 0
for any transition xh →xh+1 not allowed by the physical constraints."
TRANSITION CONSTRAINED BAYESOPT,0.07373868046571798,"Motivated by our practical experiments with chemical reactors, we distinguish two different types
of feedback. With episodic feedback we can be split the optimization into episodes. At the end of
each episode of length H, we obtain the whole set of noisy observations. On the other hand, instant
feedback is the setting where we obtain a noisy observation immediately after querying the function.
Asynchronous feedback describes a mix of the previous two, where we obtain observations with
unspecific a delay."
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.07503234152652005,"3.1
Expected Utility for Maximizer Identification
In section 2.1 we introduced the utility for maximum identification. Using the same simplifying
assumption (finite rank approximation of GPs in Sec. 2.1, Eq. (4)), we can show that the expected
utility U can be rewritten in terms of the state-action distribution induced by Xnew:"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.07632600258732213,"U(dπ) = max
z,z′∈Z ||Φ(z) −Φ(z′)||2
V(dπ)−1
(8)"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.07761966364812418,"where V(dπ) =
P"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.07891332470892626,"x,a∈X×A
dπ(x,a)Φ(x,a)Φ(x,a)⊤"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.08020698576972833,"σ2(x,a)
+
1
T H I

. The variable dπ(x, a) is a state-action"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.0815006468305304,"visitation, Φ(x) are e.g. Nyström features of the GP. We prove that the function is additive in terms of
state-action pairs in Lemma D.1 in Appendix D, a condition required for the expression as a function
of state-action visitations [17]. Additionally, by rewriting the objective in this form, the dependence
and convexity with respect to the state-action density dπ becomes clear as it is only composition of
a linear function with an inverse operator. Also, notice that the constraint set is a convex polytope.
Therefore we are able to use convex optimization to solve the planning problem (see Sec. 4)."
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.08279430789133248,Algorithm 1 Transition Constrained BayesOpt via MDPs
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.08408796895213454,"Input: Procedure for estimating sets of maximizers, initial point x0, initial set of maximizer
candidates Z0
Initialize the empirical state-action distribution ˆd0 = 0
for t = 0 to T −1 do"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.08538163001293661,for h = 0 to H −1 do
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.08667529107373868,"Ut,h(dπ) ←U(dπ ⊕ˆdt,h|Zt,h, xt,h)
// define the objective, see eq. (8)
πt,h = arg minπ:dπ∈Dt,h Ut,h(dπ)
// solve MDP planning problem
xt,h+1 = πt,h(xt,h)
// deploy policy
if feedback is immediate then"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.08796895213454076,"yt,h+1 = f(xt,h+1) + ϵt,h
// obtain observation
GPt,h, Zt,h ←Update(Xt,h, Yt,h)
// update model and maximizer candidate set
ˆdt,h+1(x) ←ˆdt,h ⊕δ(xt,h+1, x)
// update empirical state-action distribution, see eq. (11)
if feedback is episodic then"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.08926261319534282,"Yt,H = f(Xt,H) + ⃗ϵt,:
// obtain observations
GPt+1,:, Zt+1,: ←Update(Xt,H, Yt,H)
// update model and maximizer candidate set
Return: Estimate of the maximum using the GP posterior’s mean ˆx∗= arg maxx∈X µT (x)"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.09055627425614489,"Set of potential maximizers Z.
The definition of the objective requires the use of a set of
maximizers. In the ideal case, we can say a particular input x, is not the optimum if there exists x′
such that f(x′) > f(x) with high confidence. We formalize this using the GP credible sets (Bayesian
confidence sets) and define:"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.09184993531694696,"Zt = {x ∈X : UCB(f(x)|Xt) ≥sup
x′∈X
LCB(f(x′)|Xt)}
(9)"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.09314359637774904,"where UCB and LCB correspond to the upper and lower confidence bounds of the GP surrogate
with a user specified confidence level defined via the posterior GP with data up to Xt."
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.0944372574385511,"3.2
Discrete vs Continuous MDPs.
Until this point, our formulation focused on discrete S and A for ease of exposition. However, the
framework is compatible with continuous state-action spaces. The probabilistic reformulation of
the objective in Eq. (7) is possible irrespective of whether X (or A) is a discrete or continuous subset
of Rd. In fact, the convexity of the objective in the space of distributions is still maintained. The
difference is that the visitations d are no longer probability mass functions but have to be expressed
as probability density functions dc(x, a). To recover probabilities in the definition of V, we need to
replace sums with integrals i.e. P"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.09573091849935317,"x∈X,a∈A d(x) Φ(x,a)Φ(x,a)⊤"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.09702457956015524,"σ(x,a)2
→
R"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.09831824062095731,"x∈X,a∈A dc(x, a) Φ(x,a)Φ(x,a)⊤"
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.09961190168175937,"σ(x,a)2
."
EXPECTED UTILITY FOR MAXIMIZER IDENTIFICATION,0.10090556274256145,"In the Eq. (8) we need to approximate a maximum over all input pairs in Z. While this can
be enumerated in the discrete case without issues, it poses a non-trivial constrained optimization
problem when X is continuous. As an alternative, we propose approximating the set Z using a
finite approximation of size K which can be built using Thompson Sampling [57, 58] or through
maximization of different UCBs for higher exploitation (see Appendix E.1). In Appendix E.5, we
numerically benchmark reasonable choices of K, and show that the performance is not significantly
affected by them."
GENERAL ALGORITHM AND THEORY,0.10219922380336352,"3.3
General algorithm and Theory
The general algorithm combines the ideas introduced so far. We present it in Algorithm 1. Notice
that apart from constructing the current utility, keeping track of the visited states and updating
our GP model, an essential step is planning, where we need to find a policy that maximizes the
utility. As this forms the core challenge of the algorithm, we devote Sec. 4 to it. In short, it solves
a sequence of dynamic programming problems defined by the steps of the Frank-Wolfe algorithm.
From a theoretical point of view, under the assumption of episodic feedback, the algorithm provably
minimizes the utility as we show in Proposition C.1 in Appendix C.4."
SOLVING THE PLANNING PROBLEM,0.1034928848641656,"4
Solving the planning problem"
SOLVING THE PLANNING PROBLEM,0.10478654592496765,"The planning problem, defined as mindπ∈D U(dπ), can be thought of as analogous to optimizing
an acquisition function in traditional BayesOpt, with the added difficulty of doing it in the space of"
SOLVING THE PLANNING PROBLEM,0.10608020698576973,"policies. See the bottom half of Figure 5 in Appendix A for a breakdown of the different components
of our solution. Following developments in Hazan et al. [34] and Mutný et al. [17], we use the
classical Frank-Wolfe algorithm [59]. It proceeds by decomposing the problem into a series of
linear optimization sub-problems. Each linearization results in a policy, and we build a mixture
policy consisting of optimal policies for each linearization πmix,n = {(αi, πi)}n
i=1, and αi step-sizes
of Frank-Wolfe. Conveniently, after the linearization of U the subproblem on the polytope D
corresponds to an RL problem with reward ∇U for which many efficient solvers exist. Namely, for
a single mixture component we have,"
SOLVING THE PLANNING PROBLEM,0.1073738680465718,"dπn+1 = arg min
d∈D X"
SOLVING THE PLANNING PROBLEM,0.10866752910737387,"x,a,h
∇U(dπmix,n)(x, a)dh(x, a).
(10)"
SOLVING THE PLANNING PROBLEM,0.10996119016817593,"Due to convexity, the state-action distribution follows the convex combination, dπmix,n = Pn
i=1 αidπi.
The optimization produces a Markovian policy due to the subproblem in Eq. (10) being optimized
by one. We now detail how to construct a non-Markovian policies by adaptive resampling."
SOLVING THE PLANNING PROBLEM,0.111254851228978,"4.1
Adaptive Resampling: Non-Markovian policies.
A core contribution of our paper is receding horizon re-planning. This means that we keep track
of the past states visited in the current and past trajectories and adjust the policy at every step h
of the horizon H in each trajectory indexed by t. At h, we construct a Markov policy for a reward
that depends on all past visited states. This makes the resulting policy history dependent. While
in episode t and time-point h we follow a Markov policy for a single step, the overall policy is a
history-dependent non-Markov policy."
SOLVING THE PLANNING PROBLEM,0.11254851228978008,"We define the empirical state-action visitation distribution,"
SOLVING THE PLANNING PROBLEM,0.11384217335058215,"ˆdt,h =
1
tH + h
( t
X j=1 X"
SOLVING THE PLANNING PROBLEM,0.11513583441138421,"x,a∈τj
δx,a"
SOLVING THE PLANNING PROBLEM,0.11642949547218628,"|
{z
}
visited states in past trajectories +
X"
SOLVING THE PLANNING PROBLEM,0.11772315653298836,"x,a∈τt|h
δx,a)"
SOLVING THE PLANNING PROBLEM,0.11901681759379043,"|
{z
}
states at ep. t up to h (11)"
SOLVING THE PLANNING PROBLEM,0.1203104786545925,"where δx,a denotes a delta mass at state-action (x, a). Instead of solving the objective U(d) as in Eq.
(10), we seek to find a correction to the empirical distribution by minimizing,"
SOLVING THE PLANNING PROBLEM,0.12160413971539456,"Ut,h(d) = U
 1 H H −h"
SOLVING THE PLANNING PROBLEM,0.12289780077619664,1 + t d + tH + h
SOLVING THE PLANNING PROBLEM,0.12419146183699871,"1 + t
ˆdt,h"
SOLVING THE PLANNING PROBLEM,0.12548512289780078,"
.
(12)"
SOLVING THE PLANNING PROBLEM,0.12677878395860284,"We use the same Frank-Wolfe machinery to optimize this objective: dπt,h = arg mindπ∈˜
D Ut,h(dπ).
The distribution dπt,h represents the density of the policy to be deployed at trajectory t and horizon
counter h. We now need to solve multiple (n due to FW) RL problems at each horizon counter h.
Despite this, for discrete MDPs, the sub-problem can be solved extremely efficiently to exactness using
dynamic programming. As can be seen in Appendix B.4, our solving times are just a few seconds,
even if planning for very long horizons. The resulting policy π can be found by marginalization
πh(a|x) = dπ,h(x, a)/ P"
SOLVING THE PLANNING PROBLEM,0.12807244501940493,"a dπ,h(x, a), a basic property of MDPs [60]."
SOLVING THE PLANNING PROBLEM,0.129366106080207,"4.2
Continuous MDPs: Model Predictive Control
With continuous search space, the sub-problem can be solved using continuous RL solvers. However,
this can be difficult. The intractable part of the problem is that the distribution dπ needs to be
represented in a computable fashion. We represent the distribution by the sequence of actions taken
{ah}T
h=1 with the linear state-space model, xh+1 = Axh + Bah. While this formalism is not as
general as it could be, it gives us a tractable sub-problem formulation common to control science
scenario [61] that is practical for our experiments and captures a vast array of problems. The optimal
set of actions is solved with the following problem, where we state it for the full horizon H:"
SOLVING THE PLANNING PROBLEM,0.13065976714100905,"arg min
a0,...,aH H
X"
SOLVING THE PLANNING PROBLEM,0.13195342820181113,"h=0
∇Ut,0(dπmix,t) (xh, ah) ,
(13)"
SOLVING THE PLANNING PROBLEM,0.1332470892626132,"such that ||ah|| ≤amax, xh ∈X, and xh+1 = Axh + Bah, where the known dynamics serves
as constraints. Notice that instead of optimizing over the policy dπ, we directly optimize over
the parameterizations of the policy {ah}H
h=1. In fact, this formulation is reminiscent of the model
predictive control (MPC) optimization problem. Conceptually, these are the same. The only caveat
in our case is that unlike in MPC [62], our objective is non-convex and tends to focus on gathering
information rather than stability. Due to the non-convexity in this parameterization, we need to solve
it heuristically. We identify a number of useful heuristics to solve this problem in Appendix G."
SOLVING THE PLANNING PROBLEM,0.13454075032341525,"Figure 2: The Knorr pyrazole synthesis experiment. On the left, we show the quantitative results.
The line plots denote the best prediction regret, while the bar charts denote the percentage of runs that
correctly identify the best arm at the end of each episode. On the right, we show ten paths in different
colours chosen by the algorithm. The underlying black-box function is shown as the contours, and
we can see the discretization as dots. We can see four remaining potential maximizers (in orange),
which includes the true one (star). Notice all paths are non-decreasing in residence time, following
the transition constraints."
EXPERIMENTS,0.13583441138421734,"5
Experiments"
EXPERIMENTS,0.1371280724450194,"Sections 5.1 – 5.3 showcase real-world applications under physical transitions constraints, using the
discrete version of the algorithm. Section 5.4 benchmarks against other algorithms in the continuous
setting, where we consider the additive transition model of Section 4.2 with A = B = I. We include
additional results in Appendix B. For each benchmark, we selected reasonable GP hyper-parameters
and fixed them during the optimization. These are summarized in Appendix E.2. As we are interested
maximizer identification, in discrete problems, we report the proportion of reruns that succeed at
identifying the true maximum. For continuous benchmarks, we report inference regret at each
iteration: Regrett = f(x∗) −f(xµ,t), where xµ,t = arg maxx∈X µt(x). All statistics reported are
over 25 different runs."
EXPERIMENTS,0.1384217335058215,"Baselines. We include a naive baseline that greedily optimizes the immediate reward to showcase a
method with no planning (Greedy-UCB). Likewise, we create a baseline that replaces the gradient in
Eq. (10) with Expected Improvement [63] (MDP-EI), a weak version of planning. In the continuous
settings, we compare against truncated SnAKe (TrSnaKe) [42], which minimizes movement distance,
and against local search region-constrained BayesOpt or LSR [22] for the same task. We compare
two variants for approximating the set of maximizers, one using Thompson Sampling (MDP-BO-TS)
and one using Upper Confidence Bound (MDP-BO-UCB)."
KNORR PYRAZOLE SYNTHESIS,0.13971539456662355,"5.1
Knorr pyrazole synthesis"
KNORR PYRAZOLE SYNTHESIS,0.1410090556274256,"(a) Monitoring Lake Ypacarai.
(b) Free-electron laser tuning."
KNORR PYRAZOLE SYNTHESIS,0.1423027166882277,"Figure 3: Results for Ypacarai and free electron-laser tuning experiments. On the left, the line plots
denote the best prediction regret, while the bar charts denote the percentage of runs that correctly
identify the best arm at the end of each episode. On the right, We plot the regret and compare against
standard BO without accounting for movement-dependent noise."
KNORR PYRAZOLE SYNTHESIS,0.14359637774902975,"(a) Michalewicz 3D. (synch)
(b) Hartmann 3D. (synch)
(c) Hartmann 6D. (synch)"
KNORR PYRAZOLE SYNTHESIS,0.1448900388098318,"(d) Michalewicz 2D. (asynch)
(e) Hartmann 3D. (asynch)
(f) SnAr 4D. (asynch)"
KNORR PYRAZOLE SYNTHESIS,0.1461836998706339,"Figure 4: Results of experiments on the asynchronous and synchronous benchmarks. We plot the
median predictive regret and the 10% and 90% quantiles. For the asynchronous experiments, we can
see that the paths taken by MDP-BO-TS are more consistent, and the final performance is comparable
to TrSnAKe. While in the asynchronous setting, we found creating the maximization set using
Thompson Sampling gave a stronger performance, in the synchronous setting, UCB is preferred. LSR
gives a very strong performance, comparable to MDP-BO-UCB in almost all benchmarks."
KNORR PYRAZOLE SYNTHESIS,0.14747736093143596,"Our chemical reactor benchmark synthetizes Knorr pyrzole in a transient flow reactor. In this
experiment, we can control the flow-rate (residence time) τ and ratio of reactants B in the reactor.
We observe product concentration at discrete time intervals and we can also change inputs at these
intervals. Our goal is to find the best parameters of the reaction subject to natural movement
constraints on B, and τ. In addition, we assume decreasing the flow rate of a reactor can be easily
achieved. However, increasing the flow rate can lead to inaccurate readings [64]. A lower flow rate
leads to higher residence time, so we impose that τ must be non-decreasing."
KNORR PYRAZOLE SYNTHESIS,0.14877102199223805,"The kernel. Schrecker et al. [27] indicate the reaction can be approximately represented by simple
kinetics via a differential equation model. We use this information along with techniques for
representing linear ODE as constraints in GP fitting [65, 66] to create an approximate ODE kernel
kode through the featurization:"
KNORR PYRAZOLE SYNTHESIS,0.1500646830530401,"Φode(τ, B) = (1 −S(B))y(1)(τ, B) + S(B)y(2)(τ, B)"
KNORR PYRAZOLE SYNTHESIS,0.15135834411384216,"where y(i)(τ, B) are equal to: γi(B)"
KNORR PYRAZOLE SYNTHESIS,0.15265200517464425,"λ(i)
2
λ(i)
1 −λ(i)
2
eλ(i)
1 τ −
λ(i)
1
λ(i)
1 −λ(i)
2
eλ(i)
2 τ + 1 !"
KNORR PYRAZOLE SYNTHESIS,0.1539456662354463,"for i = 1, 2, where λ(i)
1
and λ(i)
2
are eigenvalues of the linearized ODE at different stationary points,
γ1(B) = B, γ2(B) = 1 −B, and S(x) := (1 + e−αsig(x−0.5))−1 is a sigmoid function. Appendix
H holds the details and derivations which may be of independent interest. As the above kernel is only
an approximation of the true ODE kernel, which itself is imperfect, we must account for the model
mismatch. Therefore, we add a squared exponential term to the kernel to ensure a non-parametric
correction, i.e.: k(τ, B) = αodekode(τ, B) + αrbf(τ, B)."
KNORR PYRAZOLE SYNTHESIS,0.15523932729624837,"We report the examples of the trajectories in the search space in Figure 2. Notice that all satisfy
the transition constraints. The paths are not space-filling and avoid sub-optimal areas because of
our choice of non-isotropic kernel based on the ODE considerations. We run the experiment with
episodic feedback, for 10 episodes of length 10 each, starting each episode with (τR, B) = (0, 0).
Figure 2 reports quantitative results and shows that the best-performing algorithm is MDP-BO."
MONITORING LAKE YPACARAI,0.15653298835705046,"5.2
Monitoring Lake Ypacarai
Samaniego et al. [20] investigated automatic monitoring of Lake Ypacarai, and Folch et al. [11] and
Yang et al. [40] benchmarked different BayesOpt algorithms for the task of finding the largest con-
tamination source in the lake. We introduce local transition constraints to this benchmark by creating
the lake containing obstacles that limit movement (see Figure 12 in the Appendix). Such obstacles"
MONITORING LAKE YPACARAI,0.15782664941785252,"in environmental monitoring may include islands or protected areas for animals. We add an initial
and final state constraint with the goal of modeling that the boat has to finish at a maintenance port."
MONITORING LAKE YPACARAI,0.1591203104786546,"We focus on episodic feedback, where each episode consists of 50 iterations. Results can be seen in
Figure 3a. MDP-EI struggles to identify the maximum contamination for the first few episodes. On
the other hand, our method correctly identifies the maximum in approximately 50% of the runs by
episode two and achieves better regret."
MONITORING LAKE YPACARAI,0.16041397153945666,"5.3
Free-electron laser: Transition-driven corruption
Apart from hard constraints, we can apply our framework to state-dependent BayesOpt problems
involving transitions. For example, the magnitude of noise ϵ may depend on the transition. This occurs
in systems observing equilibration constraints such as a free-electron laser [23]. Using the simplified
simulator of this laser [67], we use our framework to model heteroscedastic noise depending on the
difference between the current and next state, σ2(x, x′) = s(1 + w||x −x′||2). By choosing A = X,
we rewrite the problem as σ(s, a) = s(1 + w||x −a||2). The larger the move, the more noisy the
observation. This creates a problem, where the BayesOpt needs to balance between informative
actions and movement, which can be directly implemented in the objective (8) via the matrix
V(dπ) = P"
MONITORING LAKE YPACARAI,0.16170763260025872,"x,a∈X dπ(x, a)
1
σ2(x,a)Φ(x)Φ(x)⊤+
1
T H I. Figure 3b reports the comparison between
worst-case stateless BO and our algorithm. Our approach substantially improves performance."
"SYNTHETIC BENCHMARKS
WE BENCHMARK ON A VARIETY OF CLASSICAL BAYESOPT PROBLEMS WHILE IMPOSING LOCAL MOVEMENT",0.1630012936610608,"5.4
Synthetic Benchmarks
We benchmark on a variety of classical BayesOpt problems while imposing local movement
constraints and considering both immediate and asynchronous feedback (by introducing an
observation delay of 25 iterations). We also include the chemistry SnAr benchmark, from Summit
[68], which we treat as asynchronous as per Folch et al. [11]. Results are in Figure 4. In the
synchronous setting, we found using the UCB maximizer criteria for MDP-BO yields the best results
(c.f. Appendix for details of this variant). We also found that LSR performs very competitively on
many benchmarks, frequently matching the performance of MDP-BO. In the asynchronous settings
we achieved better results using MDP-BO with Thompson sampling. TrSnAKe baseline appears
to be competitive in all synthetic benchmarks as well. However, MDP-BO is more robust having
less variance in the chosen paths as seen in the quantiles. It is important to highlight that SnAKe
and LSR are specialist heuristic algorithms for local box-constraints, and therefore it is not surprising
they perform strongly. Our method can be applied to more general settings and therefore it is very
encouraging that MDP-BO is able to match these SOTA algorithms in their specialist domain."
CONCLUSION,0.16429495472186287,"6
Conclusion"
CONCLUSION,0.16558861578266496,"We considered transition-constrained BayesOpt problems arising in physical sciences, such as chemi-
cal reactor optimization, that require careful planning to reach any system configuration. Focusing on
maximizer identification, we formulated the problem with transition constraints using the framework
of Markov decision processes and constructed a tractable algorithm for provably and efficiently
solving these problems using dynamic programming or model predictive control sub-routines. We
showcased strong empirical performance in a large variety of problems with physical transitions, and
achieve state-of-the-art results in classical BayesOpt benchmarks under local movement constraints.
This work takes an important step towards the larger application of Bayesian Optimization to
real-world problems. Further work could address the continuous variant of the framework to deal
with more general transition dynamics, or explore the performance of new objective functions."
CONCLUSION,0.16688227684346701,Acknowledgments
CONCLUSION,0.16817593790426907,"JPF is funded by EPSRC through the Modern Statistics and Statistical Machine Learning (StatML)
CDT (grant no. EP/S023151/1) and by BASF SE, Ludwigshafen am Rhein. RM acknowledges
support from the BASF / Royal Academy of Engineering Research Chair in Data-Driven Optimisation.
This publication was created as part of NCCR Catalysis (grant number 180544), a National Centre
of Competence in Research funded by the Swiss National Science Foundation, and was partially
supported by the European Research Council (ERC) under the European Union’s Horizon 2020
research and Innovation Program Grant agreement no. 815943. We would also like to thank Linden
Schrecker, Ruby Sedgwick, and Daniel Lengyel for providing valuable feedback on the project."
REFERENCES,0.16946959896507116,References
REFERENCES,0.17076326002587322,"[1] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018."
REFERENCES,0.17205692108667528,"[2] Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of
expensive black-box functions. Journal of Global Optimization, 13(4):455–492, 1998."
REFERENCES,0.17335058214747737,"[3] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. Taking
the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104
(1):148–175, 2016."
REFERENCES,0.17464424320827943,"[4] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, editors. Automated Machine Learning -
Methods, Systems, Challenges. Springer, 2019."
REFERENCES,0.1759379042690815,"[5] Alonso Marco, Felix Berkenkamp, Philipp Hennig, Angela P Schoellig, Andreas Krause, Stefan
Schaal, and Sebastian Trimpe. Virtual vs. real: Trading off simulations and physical experiments
in reinforcement learning with Bayesian optimization. In 2017 IEEE International Conference
on Robotics and Automation (ICRA), pages 1557–1563. IEEE, 2017."
REFERENCES,0.17723156532988357,"[6] Jose Pablo Folch, Robert M Lee, Behrang Shafei, David Walz, Calvin Tsay, Mark van der
Wilk, and Ruth Misener. Combining multi-fidelity modelling and asynchronous batch Bayesian
optimization. Computers & Chemical Engineering, 172:108194, 2023."
REFERENCES,0.17852522639068563,"[7] Thomas M Dixon, Jeanine Williams, Maximilian Besenhard, Roger M Howard, James MacGre-
gor, Philip Peach, Adam D Clayton, Nicholas J Warren, and Richard A Bourne. Operator-free
HPLC automated method development guided by Bayesian optimization. Digital Discovery, 3
(8):1591–1601, 2024."
REFERENCES,0.17981888745148772,"[8] Jennifer Brennan, Lalit Jain, Sofia Garman, Ann E Donnelly, Erik Scott Wright, and Kevin
Jamieson. Sample-efficient identification of high-dimensional antibiotic synergy with a normal-
ized diagonal sampling design. PLOS Computational Biology, 18(7):e1010311, 2022."
REFERENCES,0.18111254851228978,"[9] Alexander Thebelt, Johannes Wiebe, Jan Kronqvist, Calvin Tsay, and Ruth Misener. Maximizing
information from chemical engineering data sets: Applications to machine learning. Chemical
Engineering Science, 252:117469, 2022."
REFERENCES,0.18240620957309184,"[10] Pratibha Vellanki, Santu Rana, Sunil Gupta, David Rubin, Alessandra Sutti, Thomas Dorin,
Murray Height, Paul Sanders, and Svetha Venkatesh. Process-constrained batch Bayesian
optimisation. In Advances in Neural Information Processing Systems. Curran Associates, Inc.,
2017."
REFERENCES,0.18369987063389392,"[11] Jose Pablo Folch, Shiqiang Zhang, Robert Lee, Behrang Shafei, David Walz, Calvin Tsay, Mark
van der Wilk, and Ruth Misener. SnAKe: Bayesian optimization with pathwise exploration. In
Advances in Neural Information Processing Systems, volume 35, pages 35226–35239, 2022."
REFERENCES,0.18499353169469598,"[12] Sarah L Boyall, Holly Clarke, Thomas Dixon, Robert WM Davidson, Kevin Leslie, Graeme
Clemens, Frans L Muller, Adam D Clayton, Richard A Bourne, and Thomas W Chamberlain.
Automated optimization of a multistep, multiphase continuous flow process for pharmaceutical
synthesis. ACS Sustainable Chemistry & Engineering, 2024."
REFERENCES,0.18628719275549807,"[13] Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, and Volkan Cevher. Truncated variance
reduction: A unified approach to Bayesian optimization and level-set estimation. Advances in
Neural Information Processing Systems, 29, 2016."
REFERENCES,0.18758085381630013,"[14] Sigrid Passano Hellan, Christopher G Lucas, and Nigel H Goddard. Bayesian optimisation
for active monitoring of air pollution. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 36, pages 11908–11916, 2022."
REFERENCES,0.1888745148771022,"[15] Sigrid Passano Hellan, Christopher G Lucas, and Nigel H Goddard. Bayesian optimisation
against climate change: Applications and benchmarks. Data-centric Machine Learning Research
(DMLR) Workshop at the 40th International Conference on Machine Learning, 2023."
REFERENCES,0.19016817593790428,"[16] Clara Stoddart, Lauren Shrack, Richard Sserunjogi, Usman Abdul-Ganiy, Engineer Baino-
mugisha, Deo Okure, Ruth Misener, Jose Pablo Folch, and Ruby Sedgwick. Gaussian processes
for monitoring air-quality in Kampala. NeurIPS 2023 Workshop: Tackling Climate Change with
Machine Learning, 2023."
REFERENCES,0.19146183699870634,"[17] Mojmír Mutný, Tadeusz Janik, and Andreas Krause. Active Exploration via Experiment
Design in Markov Chains. In Proceedings of The 26th International Conference on Artificial
Intelligence and Statistics, volume 206, pages 7349–7374, 2023."
REFERENCES,0.1927554980595084,"[18] Gregory Hitz, Alkis Gotovos, Marie-Éve Garneau, Cédric Pradalier, Andreas Krause, Roland Y
Siegwart, et al. Fully autonomous focused exploration for robotic environmental monitoring. In
2014 IEEE International Conference on Robotics and Automation (ICRA), pages 2658–2664.
IEEE, 2014."
REFERENCES,0.19404915912031048,"[19] Alkis Gotovos, Nathalie Casati, Gregory Hitz, and Andreas Krause. Active learning for level
set estimation. In IJCAI 2013, 2013."
REFERENCES,0.19534282018111254,"[20] Federico Peralta Samaniego, Daniel Gutiérrez Reina, Sergio L Toral Marín, Mario Arzamendia,
and Derlis O Gregor. A Bayesian optimization approach for water resources monitoring through
an autonomous surface vehicle: The Ypacarai lake case study. IEEE Access, 9:9163–9179,
2021."
REFERENCES,0.19663648124191463,"[21] Shyam Sundhar Ramesh, Pier Giuseppe Sessa, Andreas Krause, and Ilija Bogunovic. Movement
penalized Bayesian optimization with application to wind energy systems. In Advances in
Neural Information Processing Systems, volume 35, pages 27036–27048, 2022."
REFERENCES,0.1979301423027167,"[22] Joel A Paulson, Farshud Sorouifar, Christopher R Laughman, and Ankush Chakrabarty. LSR-
BO: Local search region constrained Bayesian optimization for performance optimization of
vapor compression systems. In 2023 American Control Conference (ACC), pages 576–582.
IEEE, 2023."
REFERENCES,0.19922380336351875,"[23] Johannes Kirschner, Mojmír Mutný, Andreas Krause, Jaime Coello de Portugal, Nicole Hiller,
and Jochem Snuverink. Tuning particle accelerators with safety constraints using Bayesian
optimization. Phys. Rev. Accel. Beams, 25:062802, 2022."
REFERENCES,0.20051746442432083,"[24] Matthew Sullivan, John Gebbie, and John Lipor. Adaptive sampling for seabed identification
from ambient acoustic noise. IEEE CAMSAP, 2023."
REFERENCES,0.2018111254851229,"[25] Sergey Mozharov, Alison Nordon, David Littlejohn, Charlotte Wiles, Paul Watts, Paul Dallin,
and John M Girkin. Improved method for kinetic studies in microreactors using flow manipula-
tion and noninvasive Raman spectrometry. Journal of the American Chemical Society, 133(10):
3601–3608, 2011."
REFERENCES,0.20310478654592498,"[26] Linden Schrecker, Joachim Dickhaut, Christian Holtze, Philipp Staehle, Andy Wieja, Klaus
Hellgardt, and King Kuok Mimi Hii. An efficient multiparameter method for the collection of
chemical reaction data via ‘one-pot’ transient flow. Reaction Chemistry & Engineering, 8(12):
3196–3202, 2023."
REFERENCES,0.20439844760672704,"[27] Linden Schrecker, Joachim Dickhaut, Christian Holtze, Philipp Staehle, Marcel Vranceanu,
Klaus Hellgardt, and King Kuok Mimi Hii. Discovery of unexpectedly complex reaction
pathways for the Knorr pyrazole synthesis via transient flow. Reaction Chemistry & Engineering,
8(1):41–46, 2023."
REFERENCES,0.2056921086675291,"[28] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine
Learning (Adaptive Computation and Machine Learning). The MIT Press, 2005."
REFERENCES,0.2069857697283312,"[29] Philipp Hennig and Christian J Schuler. Entropy search for information-efficient global opti-
mization. Journal of Machine Learning Research, 13(6), 2012."
REFERENCES,0.20827943078913325,"[30] Tanner Fiez, Lalit Jain, Kevin G Jamieson, and Lillian Ratliff. Sequential experimental design
for transductive linear bandits. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.2095730918499353,"[31] Christopher Williams and Matthias Seeger. Using the Nyström method to speed up kernel
machines. Advances in neural information processing systems, 13, 2000."
REFERENCES,0.2108667529107374,"[32] Mojmír Mutný and Andreas Krause. Efficient high dimensional Bayesian optimization with
additivity and quadrature fourier features. Advances in Neural Information Processing Systems,
31, 2018."
REFERENCES,0.21216041397153945,"[33] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances
in neural information processing systems, 20, 2007."
REFERENCES,0.21345407503234154,"[34] Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum
entropy exploration. In International Conference on Machine Learning, pages 2681–2691.
PMLR, 2019."
REFERENCES,0.2147477360931436,"[35] Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is
enough for convex MDPs. Advances in Neural Information Processing Systems, 34:25746–
25759, 2021."
REFERENCES,0.21604139715394566,"[36] Zihan Li and Jonathan Scarlett. Gaussian process bandit optimization with few batches. In
International Conference on Artificial Intelligence and Statistics, pages 92–107, 2022."
REFERENCES,0.21733505821474774,"[37] Fengxue Zhang, Jialin Song, James C Bowden, Alexander Ladd, Yisong Yue, Thomas Desautels,
and Yuxin Chen. Learning regions of interest for Bayesian optimization with adaptive level-set
estimation. In International Conference on Machine Learning, pages 41579–41595. PMLR,
2023."
REFERENCES,0.2186287192755498,"[38] Minbiao Han, Fengxue Zhang, and Yuxin Chen. No-regret learning of Nash equilibrium for
black-box games via Gaussian processes. arXiv preprint arXiv:2405.08318, 2024."
REFERENCES,0.21992238033635186,"[39] Sudeep Salgia, Sattar Vakili, and Qing Zhao. Random exploration in Bayesian optimization:
Order-optimal regret and computational efficiency. In Forty-first International Conference on
Machine Learning, 2024."
REFERENCES,0.22121604139715395,"[40] Adam X Yang, Laurence Aitchison, and Henry B Moss. MONGOOSE: Path-wise smooth
Bayesian optimisation via meta-learning. arXiv preprint arXiv:2302.11533, 2023."
REFERENCES,0.222509702457956,"[41] Qiyuan Chen and Raed Al Kontar. The traveling bandit: A framework for Bayesian optimization
with movement costs. arXiv preprint arXiv:2410.14533, 2024."
REFERENCES,0.2238033635187581,"[42] Jose Pablo Folch, James Odgers, Shiqiang Zhang, Robert M Lee, Behrang Shafei, David Walz,
Calvin Tsay, Mark van der Wilk, and Ruth Misener. Practical path-based Bayesian optimization.
NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real
World, 2023."
REFERENCES,0.22509702457956016,"[43] Jixiang Qing, Becky D Langdon, Robert M Lee, Behrang Shafei, Mark van der Wilk, Calvin
Tsay, and Ruth Misener. System-aware neural ODE processes for few-shot Bayesian optimiza-
tion. arXiv preprint arXiv:2406.02352, 2024."
REFERENCES,0.22639068564036222,"[44] Ethan Che, Jimmy Wang, and Hongseok Namkoong. Planning contextual adaptive experiments
with model predictive control. NeurIPS 2023 Workshop on Adaptive Experimental Design and
Active Learning in the Real World, 2023."
REFERENCES,0.2276843467011643,"[45] Jean-Yves Audibert and Sébastien Bubeck. Best arm identification in multi-armed bandits. In
Conference on Learning Theory, pages 13–p, 2010."
REFERENCES,0.22897800776196636,"[46] Marta Soare, Alessandro Lazaric, and Rémi Munos. Best-arm identification in linear bandits.
Advances in Neural Information Processing Systems, 27, 2014."
REFERENCES,0.23027166882276842,"[47] Kai Yu, Jinbo Bi, and Volker Tresp. Active learning via transductive experimental design. In
Proceedings of the 23rd international conference on Machine learning, pages 1081–1088, 2006."
REFERENCES,0.2315653298835705,"[48] David Ginsbourger and Rodolphe Le Riche. Towards Gaussian process-based optimization with
finite time horizon. In Advances in Model-Oriented Design and Analysis: Proceedings of the
9th International Workshop in Model-Oriented Design and Analysis, pages 89–96. Springer,
2010."
REFERENCES,0.23285899094437257,"[49] Roman Marchant, Fabio Ramos, Scott Sanner, et al. Sequential Bayesian optimisation for
spatial-temporal monitoring. In UAI, pages 553–562, 2014."
REFERENCES,0.23415265200517466,"[50] Remi Lam, Karen Willcox, and David H Wolpert. Bayesian optimization with a finite budget:
An approximate dynamic programming approach. Advances in Neural Information Processing
Systems, 29, 2016."
REFERENCES,0.23544631306597671,"[51] Shali Jiang, Henry Chai, Javier Gonzalez, and Roman Garnett. Binoculars for efficient, nonmy-
opic sequential experimental design. In International Conference on Machine Learning, pages
4794–4803. PMLR, 2020."
REFERENCES,0.23673997412677877,"[52] Eric Hans Lee, David Eriksson, Valerio Perrone, and Matthias Seeger. A nonmyopic approach
to cost-constrained Bayesian optimization. In Uncertainty in Artificial Intelligence, pages
568–577. PMLR, 2021."
REFERENCES,0.23803363518758086,"[53] Joel A Paulson, Farshud Sorouifar, and Ankush Chakrabarty. Efficient multi-step lookahead
Bayesian optimization with local search constraints. In 2022 IEEE 61st Conference on Decision
and Control (CDC), pages 123–129. IEEE, 2022."
REFERENCES,0.23932729624838292,"[54] Mujin Cheon, Haeun Byun, and Jay H Lee. Reinforcement learning based multi-step look-ahead
bayesian optimization. IFAC-PapersOnLine, 55(7):100–105, 2022."
REFERENCES,0.240620957309185,"[55] Shali Jiang, Daniel Jiang, Maximilian Balandat, Brian Karrer, Jacob Gardner, and Roman
Garnett. Efficient nonmyopic bayesian optimization via one-shot multi-step trees. Advances in
Neural Information Processing Systems, 33:18039–18049, 2020."
REFERENCES,0.24191461836998707,"[56] Raul Astudillo, Daniel Jiang, Maximilian Balandat, Eytan Bakshy, and Peter Frazier. Multi-step
budgeted bayesian optimization with unknown evaluation costs. Advances in Neural Information
Processing Systems, 34:20197–20209, 2021."
REFERENCES,0.24320827943078913,"[57] William R Thompson. On the likelihood that one unknown probability exceeds another in view
of the evidence of two samples. Biometrika, 25(3-4):285–294, 1933."
REFERENCES,0.2445019404915912,"[58] Kirthevasan Kandasamy, Akshay Krishnamurthy, Jeff Schneider, and Barnabás Póczos. Paral-
lelised Bayesian optimisation via Thompson sampling. In International Conference on Artificial
Intelligence and Statistics, pages 133–142. PMLR, 2018."
REFERENCES,0.24579560155239327,"[59] Martin Jaggi. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization. In Sanjoy
Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on
Machine Learning, volume 28, pages 427–435. PMLR, 17–19 Jun 2013."
REFERENCES,0.24708926261319533,"[60] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.
John Wiley Sons, 2014."
REFERENCES,0.24838292367399742,"[61] James Blake Rawlings, David Q Mayne, and Moritz Diehl. Model predictive control: theory,
computation, and design, volume 2. Nob Hill Publishing, 2017."
REFERENCES,0.24967658473479948,"[62] Carlos E. García, David M. Prett, and Manfred Morari. Model predictive control: Theory and
practice—A survey. Automatica, 25(3):335–348, 1989."
REFERENCES,0.25097024579560157,"[63] Vydu-nas R Šaltenis. One method of multiextremum optimization. Avtomatika i Vychislitel’naya
Tekhnika (Automatic Control and Computer Sciences), 5(3):33–38, 1971."
REFERENCES,0.2522639068564036,"[64] Linden Schrecker, Joachim Dickhaut, Christian Holtze, Philipp Staehle, Marcel Vranceanu,
Andy Wieja, Klaus Hellgardt, and King Kuok Hii. A comparative study of transient flow rate
steps and ramps for the efficient collection of kinetic data. Reaction Chemistry & Engineering,
2024."
REFERENCES,0.2535575679172057,"[65] Mojmír Mutný and Andreas Krause. Experimental design for linear functionals in reproducing
kernel Hilbert spaces. Advances in Neural Information Processing Systems, 35:20175–20188,
2022."
REFERENCES,0.25485122897800777,"[66] Andreas Besginow and Markus Lange-Hegermann. Constraining Gaussian processes to systems
of linear ordinary differential equations. Advances in Neural Information Processing Systems,
35:29386–29399, 2022."
REFERENCES,0.25614489003880986,"[67] Mojmír Mutný, Johannes Kirschner, and Andreas Krause. Experimental design for optimization
of orthogonal projection pursuit models. In Proceedings of the 34th AAAI Conference on
Artificial Intelligence (AAAI), 2020."
REFERENCES,0.2574385510996119,"[68] Kobi Felton, Jan Rittig, and Alexei Lapkin. Summit: Benchmarking Machine Learning Methods
for Reaction Optimisation. Chemistry Methods, February 2021."
REFERENCES,0.258732212160414,"[69] Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nyström method
vs random Fourier features: A theoretical and empirical comparison. Advances in neural
information processing systems, 25, 2012."
REFERENCES,0.26002587322121606,"[70] Daniel Russo. Simple bayesian algorithms for best arm identification. In Conference on
Learning Theory, pages 1417–1418. PMLR, 2016."
REFERENCES,0.2613195342820181,"[71] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian pro-
cess optimization in the bandit setting: No regret and experimental design. arXiv preprint
arXiv:0912.3995, 2009."
REFERENCES,0.2626131953428202,"[72] Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the
American mathematical society, 39(1):1–49, 2002."
REFERENCES,0.26390685640362227,"[73] Kathryn Chaloner and Isabella Verdinelli. Bayesian Experimental Design: A Review. Statist.
Sci., 10(3):273–304, 08 1995."
REFERENCES,0.2652005174644243,"[74] Friedrich Pukelsheim. Optimal Design of Experiments (Classics in Applied Mathematics) (Clas-
sics in Applied Mathematics, 50). Society for Industrial and Applied Mathematics, Philadelphia,
PA, USA, 2006. ISBN 0898716047."
REFERENCES,0.2664941785252264,"[75] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,
2004."
REFERENCES,0.2677878395860285,"[76] Max A Woodbury. Inverting modified matrices. In Memorandum Rept. 42, Statistical Research
Group, page 4. Princeton Univ., 1950."
REFERENCES,0.2690815006468305,"[77] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive
entropy search for efficient global optimization of black-box functions. Advances in neural
information processing systems, 27, 2014."
REFERENCES,0.2703751617076326,"[78] Ben Tu, Axel Gandy, Nikolas Kantas, and Behrang Shafei. Joint entropy search for multi-
objective Bayesian optimization. Advances in Neural Information Processing Systems, 35:
9922–9938, 2022."
REFERENCES,0.2716688227684347,"[79] Carl Hvarfner, Frank Hutter, and Luigi Nardi. Joint entropy search for maximally-informed
Bayesian optimization. Advances in Neural Information Processing Systems, 35:11494–11506,
2022."
REFERENCES,0.2729624838292367,"[80] Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient Bayesian optimization.
In International Conference on Machine Learning, pages 3627–3635. PMLR, 2017."
REFERENCES,0.2742561448900388,"A
Visual abstract of the algorithm"
REFERENCES,0.2755498059508409,"In Figure 5 we summarize how our algorithm creates non-Markovian policies for maximizer identifi-
cation and the corresponding connections to other works in the literature."
REFERENCES,0.276843467011643,Transition Constrained BO Problem Formulation
REFERENCES,0.278137128072445,"Objective Formulation
(a) Need to plan ahead"
REFERENCES,0.2794307891332471,"Look-ahead Utilities and
averaging out uncertainty
(intractable)"
REFERENCES,0.2807244501940492,Hypothesis Testing
REFERENCES,0.2820181112548512,"(b) Variance reduction
of a trajectory"
REFERENCES,0.2833117723156533,"Unconstrained Linear Bandits
(Soare et al. [46]) (frequentist)"
REFERENCES,0.2846054333764554,"Adaptive Objective with optimality
guarantees for Linear Bandits (Fiez
et al. [30]) (frequentist)
(c) Acquisition function"
REFERENCES,0.2858990944372574,"Acquisition function
optimization
Direct optimization in
the space of trajectories
(intractable)"
REFERENCES,0.2871927554980595,"(d) Relaxation to space of
state-action distributions"
REFERENCES,0.2884864165588616,"(e) Solvable by Frank-Wolfe
Algorithm"
REFERENCES,0.2897800776196636,"Mutný et al. [17],
Hazan et al. [34]"
REFERENCES,0.2910737386804657,"Convergence to optimal
Markovian policy (Mutný
et al. [17])"
REFERENCES,0.2923673997412678,"Efficient RL solvers,
e.g. Dynamic Pro-
gramming"
REFERENCES,0.2936610608020699,"(f) Iterative Reinforcement
Learning sub-problem"
REFERENCES,0.2949547218628719,"Adaptive resampling
at every iteration
(g) Non-Markovian Policies"
REFERENCES,0.296248382923674,"Figure 5: Visual abstract of the work. In black we show the method presented in this paper, with
literature connections shown in blue. In red we show solutions which we did not pursue due to
intractability. The problem creates the (a) need to plan ahead. To do this, we take inspiration
from hypothesis testing and focus on (b) the variance reduction in a set of maximizers, which
leads to our (c) acquisition function. The objective is the same as Fiez et al. [30] introduced in
the linear bandits literature from a frequentist perspective. To optimize it, we follow developments
in Mutný et al. [17], Hazan et al. [34] by (d) relaxing the acquisition function to the space of
state-action distributions and (e) solving the planning problem using the Frank-Wolfe algorithm.
This consists of iteratively solving tractable (f) reinforcement learning sub-problems which give us
optimal Markov policies. We then apply adaptive resampling to obtain (g) non-Markovian policies."
REFERENCES,0.2975420439844761,Figure 6: High noise constrained Ypacari experiment with immediate feedback.
REFERENCES,0.2988357050452781,Figure 7: Knorr pyrazole synthesis with immediate feedback
REFERENCES,0.3001293661060802,"B
Additional Empirical Results"
REFERENCES,0.3014230271668823,"B.1
Constrained Ypacarai
We also run the Ypacarai experiment with immediate feedback. To increase the difficulty, we used
large observation noise, σ2 = 0.01. The results can be seen in Figure 6. The early performance of
MDP-EI is much stronger, however, it gets overtaken by our algorithm from episode three onwards,
and gives the worst result at the end, as it struggles to identify which of the two optima is the global
one."
REFERENCES,0.3027166882276843,"B.2
Knorr pyrazole synthesis
We also include results for the Knorr pyrazole synthesis with immediate feedback. In this case we
observe very strong early performance from MDP-BO, but by the end MDP-EI is comparable. The
greedy method performs very poorly."
REFERENCES,0.3040103492884864,"B.3
Additional synthetic benchmarks
Finally, we also include additional results on more synthetic benchmarks for both synchronous and
asynchronous feedback. The results are shown in Figures 8 and 9. The results back the conclusions
in the main body. All benchmarks do well in 2-dimensions while highlighting further that MDP-
BO-UCB and LSR can be much stronger in the synchronous setting than Thompson Sampling
planning-based approaches (with the one exception of the Levy function)."
REFERENCES,0.3053040103492885,"Table 1: Average acquisition function solving times for each practical benchmark. We give the
solving times to the nearest second, and provide the size of the state-space, |S|, the maximum number
of actions one can take from a specific state, |A(S)|, and the planning horizon. In all benchmarks we
are able to solve the problem in a few seconds."
REFERENCES,0.30659767141009053,"Benchmark
Solve Time
|S|
Maximum |A(S)|
Planning horizon
Knorr pyrazole
1s
100
6
10
Ypacarai
3s
100
8
50
Electron laser
15s
100
100
100"
REFERENCES,0.3078913324708926,"(a) Branin 2D. (asynch)
(b) Michalewicz 3D. (asynch)
(c) Hartmann 6D. (asynch)"
REFERENCES,0.3091849935316947,Figure 8: Additional asynchronous results.
REFERENCES,0.31047865459249674,"(a) Branin 2D.
(b) Michalewicz2D.
(c) Levy4D."
REFERENCES,0.3117723156532988,Figure 9: Additional synchronous results.
REFERENCES,0.3130659767141009,"B.4
Computational study
We include the average acquisition function solving time for each of the discrete problems. For
the continuous case the running time was comparable to Truncated SnAKe [42] since most of the
computational load was to create the set of maximizers using Thompson Sampling. The times
were obtained in a simple 2015 MacBook Pro 2.5 GHz Quad-Core Intel Core i7. The bulk of the
experiments was ran in parallel on a High Performance Computing cluser, equipped with AMD
EPYC 7742 processors and 16GB of RAM."
REFERENCES,0.314359637774903,"B.5
Median plots for Ypacarai and reactor experiments
In Figures 10 and 11 we give the median and quantile plots for the Knorr pyrazole synthesis and the
Ypacarai experiment, which were not included in the main paper to avoid cluttering the graphics."
REFERENCES,0.31565329883570503,"(a) Episodic feedback results
(b) Immediate feedback results"
REFERENCES,0.3169469598965071,Figure 10: Median and 10th/90th quantile plots for Knorr pyrazole synthesis experiment.
REFERENCES,0.3182406209573092,"C
Utility function: Additional Info"
REFERENCES,0.31953428201811124,"We describe the utility function in complete detail using the kernelized variant that allows to extend
the utility beyond the low-rank assumption in the main text."
REFERENCES,0.3208279430789133,"C.1
Derivation of the Bayesian utility
Suppose that our decision rule is to report the best guess of the maximizer after the T steps as,"
REFERENCES,0.3221216041397154,"xT = arg max
x∈Z
µT (x)."
REFERENCES,0.32341526520051744,"(a) Episodic feedback results
(b) Immediate feedback results"
REFERENCES,0.32470892626131953,Figure 11: Median and 10th/90th quantile plots for Ypacarai experiment.
REFERENCES,0.3260025873221216,"We call this the selection the recommendation rule. We focus on this recommendation rule as this
rule is interpretable to the facilitator of the analysis and experimenters. In this derivation we use that
f = θ⊤Φ(x). More commonly, the notation ⟨θ, Φ(x)⟩is used, where the inner product is potentially
infinite dimensional. We use use ⊤notation for simplicity for both cases. Same is true for any other
functional estimates, e.g., for the posterior mean estimate, we use µt(x) = Φ(x)⊤µt. The inner
product is in the reproducing kernel Hilbert space associated with the kernel k."
REFERENCES,0.32729624838292365,"Now, suppose there is a given f (we will take expectation over it later), then there is an x ∈X
achieving optimum value, denoted x⋆
f (suppose unique for this development here). Hence, we would
like to model the risk associated with predicting a fixed z ̸= x⋆
f, which is still in Z at time T. Suppose
we are at time t, we develop the utility to gather additional data Xnew on top of the already acquired
data Xt. These should improve the discrepancy of the true answer, and the reported value the most."
REFERENCES,0.32858990944372574,"Suppose there are two elements in Zsimple = {z, x⋆
f}. We will generalize to a composite hypothesis
later. In two-element case, the probability of the error in incurred due to selecting z is:"
REFERENCES,0.3298835705045278,"P(µT (z) −µT (x⋆
f) ≥0|f)"
REFERENCES,0.3311772315653299,". The randomness here is due to the observations y = f(Xnew) + ϵ that are used to fit the estimator
µT (x). Namely due to ϵ ∼N(0, σ2). Given f (equivalently θ), the distribution of our estimator
(namely the posterior mean) is Gaussian. Hence, given f:"
REFERENCES,0.33247089262613194,"µT ∼N((VT + IH)−1VT θ, σ2(VT + IH)−1VT (VT + IH)−1),"
REFERENCES,0.33376455368693403,"where V = PT
i=1
1
σ2 Φ(xi)Φ(xi)⊤is an operator on the reproducing kernel Hilbert space due to k
as H →H, and IH the identity operator on the same space."
REFERENCES,0.3350582147477361,"This
is
the
posterior
over
the
posterior
mean
as
a
function.
A
posterior
over
the
specific
evaluation
is
µT (z)
−
µT (x⋆
f)
∼
N(θ⊤(VT + IH)−1VT (Φ(z) −Φ(x⋆
f))
|
{z
}
a"
REFERENCES,0.33635187580853815,", σ2(Φ(z) −Φ(x⋆
f)⊤(VT + IH)−1VT (VT + IH)−1(Φ(z) −Φ(x⋆
f))
|
{z
}
b2 )."
REFERENCES,0.33764553686934023,We can now bound the probability of making an error using a Gaussian tail bound inequality:
REFERENCES,0.3389391979301423,P(µT (z) −µT (x∗) ≥0) = P(µT (z) −µT (x∗) ≥az + (−az)) ≤e−a2 2b2
REFERENCES,0.34023285899094435,"with the caveat that the inequality only holds when the az is negative. However note that az →
f(z) −f(x∗) < 0 as T →∞therefore it will hold once T is large enough. From this we can take
logarithms and then the expectation across the randomness in the GP:"
REFERENCES,0.34152652005174644,Ef∼GP [log P(µT (z) −µT (x∗)|f)] ≤−1
REFERENCES,0.3428201811125485,2Ef∼GP
REFERENCES,0.34411384217335056,"a2
z
b2z "
REFERENCES,0.34540750323415265,"which is called the log Bayes’ factor and is expected log failure rate for the set of potential maximizers
Zsimple. The expectation is over the posterior including the evaluations Xt (or prior at the very
beginning of the procedure). In fact, we can think of the posterior as being the new prior for the future"
REFERENCES,0.34670116429495473,"at any time point. Now assuming that Z has more than one additional element, we want to ensure the
failure rate is small for all other failure modes, all other hypothesis. Technically this means, we have
an alternate hypothesis, which is composite – composed of multiple point hypotheses. We take the
worst-case perspective as its common with composite hypotheses. In expectation over the prior, we
want to minimize:"
REFERENCES,0.34799482535575677,"min
Xnext Ef """
REFERENCES,0.34928848641655885,"sup
z∈Z\{x⋆
f }
log P(µT (z) −µT (x⋆
f) ≥0|f) #"
REFERENCES,0.35058214747736094,".
(14)"
REFERENCES,0.351875808538163,"For moderate to large T ≫0, we can upper bound this objective via elegant argument to yield a very
transparent objective:"
REFERENCES,0.35316946959896506,"min
Xnext Ef """
REFERENCES,0.35446313065976714,"sup
z∈Z\{x⋆
f }
log P(µT (z) −µT (x⋆
f) ≥0|f)"
REFERENCES,0.35575679172056923,"#
·
≤−1"
"MIN
XNEXT EF",0.35705045278137126,"2 min
Xnext Ef """
"MIN
XNEXT EF",0.35834411384217335,"sup
z∈Z\{x⋆
f }"
"MIN
XNEXT EF",0.35963777490297544,"(f(z) −f(x⋆
f))2"
"MIN
XNEXT EF",0.36093143596377747,"kXt∪Xnew(z, x⋆
f) #"
"MIN
XNEXT EF",0.36222509702457956,"(15)
where we have used an lower and upper bound on the az and bz, respectively as follows:"
"MIN
XNEXT EF",0.36351875808538164,"a2
z
=
(θ⊤(VT + IH)−1VT (Φ(z) −Φ(x⋆
f)))2"
"MIN
XNEXT EF",0.3648124191461837,"=
(θ⊤(VT + IH)−1(VT + IH −IH)(Φ(z) −Φ(x⋆
f)))2"
"MIN
XNEXT EF",0.36610608020698576,"=
(θ⊤(Φ(z) −Φ(x⋆
f)) −θ⊤(VT + IH)−1(Φ(z) −Φ(x⋆
f)))2"
"MIN
XNEXT EF",0.36739974126778785,"T ≫0
≈
(θ⊤(Φ(z) −Φ(x⋆
f)))2 = (f(z) −f(x⋆
f))2"
"MIN
XNEXT EF",0.36869340232858994,"b2
z
=
σ2(Φ(z) −Φ(x⋆
f)⊤(VT + IH)−1VT (VT + IH)−1(Φ(z) −Φ(x⋆
f))"
"MIN
XNEXT EF",0.36998706338939197,"≤
σ2(Φ(z) −Φ(x⋆
f))⊤(VT + IH)−1(Φ(z) −Φ(x⋆
f)) = kX(z, x⋆
f)."
"MIN
XNEXT EF",0.37128072445019406,"In the last line we have used the same identity as in Eq. (25). We will explain how to eliminate the
expectation in Section C.2"
"MIN
XNEXT EF",0.37257438551099614,"C.2
Upper-bounding the objective: Eliminating Ef for large T.
The objective Eq. (15) is intractable due to the expectation of the prior and which involves expec-
tation over the maximum f(x⋆
f), which is known to be very difficult to estimate. Interestingly, the
denominator is independent of f if we adopt the worst-case perspective over the x⋆
f, and hence the
only dependence is through the set Z as well as the denominator. Given all current prior information,
we can determine Z, and hence split the expectation. Let us now express"
"MIN
XNEXT EF",0.3738680465717982,"At any time point, we can upper-bound the denominator by the minimum as done by Fiez et al. [30].
Even if Z decreases, as we get more information, the worst-case bound is always proportional to the
smallest gap gap(f) between two arms in X. Hence, we can upper bound the objective as: −Ef """
"MIN
XNEXT EF",0.37516170763260026,"sup
z∈Z\{x⋆
f }"
"MIN
XNEXT EF",0.37645536869340235,"(f(z) −f(x⋆
f))2"
"MIN
XNEXT EF",0.3777490297542044,"kX∪Xnew(z, x⋆
f) #"
"MIN
XNEXT EF",0.37904269081500647,"≤
−
sup
z∈Z\{x⋆
f }"
"MIN
XNEXT EF",0.38033635187580855,"Ef [gap(f)]
kX∪Xnew(z, x⋆
f)"
"MIN
XNEXT EF",0.3816300129366106,"≤
−Ef [gap(f)]
sup
z∈Z\{x⋆
f } 1"
"MIN
XNEXT EF",0.3829236739974127,"Var
h
f(z) −f(x⋆
f)|Xt ∪Xnew
i."
"MIN
XNEXT EF",0.38421733505821476,"As the constant in front of the objective does not influence the optimization problem, we do not need
to consider it when defining the utility. Furthermore, in order to minimise the probability of an error
we can just minimise the variance in the denominator instead (since arg minx −g(x) is equivalent to
arg minx
1
g(x) when g(x) > 0). However, the non-trivial distribution of f(x⋆) [29] renders the utility
intractable; therefore we employ a simple and tractable upper bound on the objective by minimising
the uncertainty among all pairs in Z:"
"MIN
XNEXT EF",0.3855109961190168,"U(Xnew) =
max
z′,z∈Z,z̸=z′ Var[f(z) −f(z′)|Xt ∪Xnew].
(16)"
"MIN
XNEXT EF",0.3868046571798189,"Surprisingly, this objective coincides with the objective from Fiez et al. [30] which has been derived
as lower bound to the best-arm identification problem (maximum identification) with linear bandits.
Their perspective is however slightly different as they try to minimize T for a fixed δ failure rate.
Perhaps it should not be surprising that the dual variant, consider here, for fixed T and trying to
minimize the failure rate leads to the same decision for large T when log(bz) can be neglected."
"MIN
XNEXT EF",0.38809831824062097,"C.3
Approximation of Gaussian Processes
Let us now briefly summarize the Nyström approximation [31, 69]. Given a kernel k(·, ·), and a
data-set X, we can choose a sub-sample of the data ˆx1, ..., ˆxm. Using this sample, we can create a
low r-rank approximation of the full kernel matrix"
"MIN
XNEXT EF",0.38939197930142305,ˆKr = Kb ˆK†Kb
"MIN
XNEXT EF",0.3906856403622251,"where Kb = [k(xi, ˆxj)]N×m, ˆK = [k(ˆxi, ˆxj)]m×m and K† denotes the pseudo-inverse operation.
We can then define the Nyström features as:"
"MIN
XNEXT EF",0.39197930142302717,"ϕn(x) = ˆD−1/2
r
ˆV T
r (k(x, x1), ..., k(x, xm))T ,
(17)"
"MIN
XNEXT EF",0.39327296248382926,"where ˆDr is the diagonal matrix of non-zero eigenvalues of ˆKr and ˆVr the corresponding matrix of
eigenvectors. It follows that we obtain a finite-dimensional estimate of the GP:"
"MIN
XNEXT EF",0.3945666235446313,"f(x) ≈Φ(x)T θ
(18)"
"MIN
XNEXT EF",0.3958602846054334,"where Φ(x) = (ϕ1(x), . . . ϕm(x))T , and θ are weights with a Gaussian prior."
"MIN
XNEXT EF",0.39715394566623546,"C.4
Theory: convergence to the optimal policy
The fact that our objective is derived using Bayesian decision theory makes it well-rooted in theory.
In addition to the derivation of Section C.1, we can prove that our scheme is able to converge in terms
of the utility."
"MIN
XNEXT EF",0.3984476067270375,"Notice that the set of potential maximizers is changing over time, and hence we add a time subscript
to Z as Zt. Let us contemplate for a second what could the optimal policy. As the set of Zt is
changing, we follow the line of work of started by Russo [70] and introduce an optimal algorithm
that knows the true x⋆
f for each possible realization of the prior f. In other words, its an algorithm
that any time t, would follow:"
"MIN
XNEXT EF",0.3997412677878396,"d⋆
t = min
d∈D Ef """
"MIN
XNEXT EF",0.40103492884864167,"max
z∈Zt\{x⋆
f } k ˆdt⊕d(z, x⋆
f) # ,"
"MIN
XNEXT EF",0.4023285899094437,"where in the above ˆdt ⊕d represents the weighted sum as in the main Algorithm 1 that scales the
distributions properly according to t and T, so to make the sum of them a valid distribution. Notice
that in contrast to our objective, it does not take the maximum over z′ ∈Z, but fixes it to the value x⋆
f
that the hypothetical algorithm has privileged access to. To eliminate the cumbersome notation, we
will refer to the objectives as U(d|Zt, Zt) as the objective used by our algorithm (real execution) and
U(d|Zt, {x⋆
f}), as the objective that the privileged algorithm is optimizing which serves as theoretical
baseline."
"MIN
XNEXT EF",0.4036222509702458,"The visitation of d⋆
t represents the best possible investment of the resources (of the size T −t) to
execute at time t had we known the x⋆
f instead of only Zt. This is interpreted as if the modeler knows
x⋆
f, and sets up an optimal curriculum that is being shown to an observer in order to convince him/her
of that x⋆
f is the optimal value. He or she is using statistical testing to elucidate it from execution
of the policy. Like the algorithm, the optimal policy changes along the optimization procedure due
to changes in Zt. Hence, our goal is to show that we are closely tracking the performance of these
optimal policies in time t, and eventually there is little difference between our sequence of executed
policies (visitations) ˆdt and the algorithm optimal d⋆
t ."
"MIN
XNEXT EF",0.4049159120310479,"In order to prove the theorem formally, we need to assume that Zt is decreasing. The rate at which
this set is decreasing determines the performance of the algorithm to a large extent. Namely, we
assume that given two points in time, having the same empirical information ˆdt. Given, f, suppose"
"MIN
XNEXT EF",0.40620957309184996,"sup
d∈D
|d⊤(∇U( ˆdt|Zt, Zt) −∇U( ˆdt|Zt, {x⊤
f }))| ≤Ct.
(⋆)"
"MIN
XNEXT EF",0.407503234152652,"As we gather information in our procedure the, {x⋆
f} ⊂Zt ⊆Zt−1, but the exact decrease depends
on how Zt is constructed. We leave the particular choice for Ct to make the above hold for future
work. We conjecture that this is decreasing as Ct ≈
γt
√"
"MIN
XNEXT EF",0.4087968952134541,"t, where γt is the information gain due to
Srinivas et al. [71]. We are now ready to state the formal theorem along with its assumptions."
"MIN
XNEXT EF",0.41009055627425617,"Proposition C.1. Assuming episodic feedback, and suppose that for any Z,"
U IS CONVEX ON D,0.4113842173350582,1. U is convex on D
U IS CONVEX ON D,0.4126778783958603,2. B-locally Lipschitz continuous under || · ||∞norm
U IS CONVEX ON D,0.4139715394566624,"3. locally smooth with constant L, i.e,"
U IS CONVEX ON D,0.4152652005174644,"U(η + αh) ≤U(η) + ∇U(η)⊤h + Lη,α"
U IS CONVEX ON D,0.4165588615782665,"2
∥h∥2
2 .
(19)"
U IS CONVEX ON D,0.4178525226390686,"for α ∈(0, 1) and η, h ∈∆p, L := maxη,α Lη,α
4. condition in (⋆) holds with Bayesian posterior inference,"
U IS CONVEX ON D,0.4191461836998706,"we can show that the Algorithm 1 satisfied for the sequences of iterates { ˆdt}T
t=1:"
T,0.4204398447606727,"1
T"
T,0.4217335058214748,"T −1
X"
T,0.4230271668822768,"t=1
U(dt|Zt, {x⋆
f}) −U(d⋆
t |Zt, {x⋆
f}) ≤O"
T,0.4243208279430789,"1
T"
T,0.425614489003881,"T −1
X"
T,0.4269081500646831,"t=1
Ct + L log T"
T,0.4282018111254851,"T
+ B
√"
T,0.4294954721862872,"T
log
1 δ ! ,"
T,0.4307891332470893,"with probability 1 −δ on the sampling from the Markov chain. The randomness on the confidence set
is captured by Assumption in Eq. (⋆)."
T,0.4320827943078913,"The previous proposition shows that as the budget of the experimental campaign T is increasing, we
are increasingly converging to the optimal allocation of the experimental resources on average also on
the objective that is unknown to us. In other words, our algorithm is becoming approximately optimal
also under the privileged information setting representing the best possible algorithm. Despite having
a limited understanding of potential maximizers at the beginning by following our procedure, we
show that we are competitive to the best possible allocation of the resources. Now, we prove the
Proposition. The proof is an extension of the Theorem 3 in [17]. Whether the objective satisfied the
above conditions depends on the set X. Should the objective not satisfy smoothness, it can be easily
extended by using the Nesterov smoothing technique as explained in the same priorly cited work."
T,0.4333764553686934,"Proof of Proposition C.1. The proof is based on the proof of Frank-Wolfe convergence that appears
Appendix B.4 in Thm. 3. in Mutný et al. [17]."
T,0.4346701164294955,"Let us start by notation. We will use the notation that Ut is the privileged objective U(d|Zt, {xf
t }),
while the original objective will be specified as U(d|Zt, Zt)."
T,0.4359637774902975,"First, what we follow in the algorithm:"
T,0.4372574385510996,"qt = arg min
d∈D
∇U( ˆdt|Zt, Zt)⊤d
(20)"
T,0.4385510996119017,"The executed visitation is simply generated via sampling a trajectory from qt. Let us denote the
empirical visiation of the trajectory as δt,"
T,0.4398447606727037,"δt ∼qt.
(21)"
T,0.4411384217335058,"For the analysis, we also need the best greedy step for the unknown (privileged) objective U as"
T,0.4424320827943079,"zt = arg min
d∈D
∇Ut

ˆdt
⊤
d.
(22)"
T,0.44372574385511,Let us start by considering the one step update:
T,0.445019404915912,"Ut( ˆdt+1)
=
Ut"
T,0.4463130659767141,"
ˆdt +
1
t + 1(δt −ˆdt)
"
T,0.4476067270375162,"L-smooth
≤
Ut( ˆdt) +
1
t + 1∇Ut( ˆdt)⊤(δt −ˆdt) +
L
2(1 + t)2"
T,0.4489003880983182,"δt −ˆdt

2"
T,0.4501940491591203,"U( ˆdt+1)
bounded
≤
Ut( ˆdt) +
1
t + 1∇Ut( ˆdt)⊤(δt −ˆdt) +
L
(1 + t)2"
T,0.4514877102199224,"=
Ut( ˆdt) +
1
t + 1∇Ut( ˆdt)⊤(qt −ˆdt) +
1
t + 1 ∇Ut( ˆdt)⊤(−qt + δt)
|
{z
}
ϵt"
T,0.45278137128072443,"+
L
(1 + t)2"
T,0.4540750323415265,"We will now carefully insert and subtract two set of terms depending on the real objective so that we
can bound them using (⋆):"
T,0.4553686934023286,"=
Ut( ˆdt) +
1
t + 1∇(Ut( ˆdt)⊤−∇Ut( ˆdt|Zt, Zt)⊤)(qt −zt) +
1
1 + tUt( ˆdt)⊤(zt −ˆdt)"
T,0.45666235446313064,"+
1
1 + tϵt +
L
(1 + t)2"
T,0.4579560155239327,"Using ⋆
≤
Ut( ˆdt) + 2
1
1 + tCt +
1
t + 1Ut( ˆdt)⊤(zt −ˆdt) +
1
1 + tϵt +
L
(1 + t)2"
T,0.4592496765847348,"Carrying on,"
T,0.46054333764553684,"Ut( ˆdt+1)
(22)
≤
Ut( ˆdt) +
1
t + 1∇Ut( ˆdt)⊤(d⋆
t −ˆdt) +
1
1 + tϵt +
L
(1 + t)2"
T,0.46183699870633893,"convexity
≤
Ut( ˆdt) −
1
t + 1(Ut( ˆdt) −Ut(η⋆
t )) +
1
1 + tϵt +
L
(1 + t)2 +
1
1 + tCt"
T,0.463130659767141,"Ut( ˆdt+1) −Ut(d⋆
t )
≤
Ut( ˆdt) −Ut(d⋆
t ) −
1
t + 1(Ut( ˆdt) −Ut(d⋆
t )) +
1
1 + tϵt +
L
(1 + t)2 +
1
1 + tCt"
T,0.4644243208279431,"≤
t
1 + t"
T,0.46571798188874514,"
Ut( ˆdt) −Ut(d⋆
t )

+
1
1 + tϵt +
L
(1 + t)2 +
1
1 + tCt"
T,0.4670116429495472,"=
t
1 + t"
T,0.4683053040103493,"
Ut( ˆdt) −Ut(d⋆
t )

+
1
1 + tϵt +
L
(1 + t)2 +
1
1 + tCt"
T,0.46959896507115134,"Now multiplying by t + 1 both sides, and summing on
1
T −1
PT −1
t=1 . Using the shorthand ρt( ˆdt) =
Ut( ˆdt) −Ut(d⋆
t ) we get:"
T,0.47089262613195343,"1
T"
T,0.4721862871927555,"T −1
X"
T,0.47347994825355755,"t=1
(t + 1)ρt+1( ˆdt+1) ≤1 T"
T,0.47477360931435963,"T −1
X"
T,0.4760672703751617,"t=1
tρt( ˆdt) + 1 T"
T,0.47736093143596375,"T −1
X"
T,0.47865459249676584,"t=1
(ϵt + Ct + L/(1 + t))"
T,0.4799482535575679,"First notice that
1
T −1
PT −1
t=1 ϵt ≤
B
√"
T,0.48124191461837,"T log(1/δ) by Lemma in Mutný et al. [17] due to ϵt being
martingale difference sequence. The other term is the sum on
1
T −1
P"
T,0.48253557567917205,t=1 Ct which appears in the
T,0.48382923673997413,"main result. The sum on PT −1
t=1
1
1+t ≤L log T"
T,0.4851228978007762,"T
. The rest is eliminated by the reccurence of the terms,
and using that U(d|Zt, {x⋆
f} ≤U(d|Zt−1, {x⋆
f}) for any d. This is due to set Zt decreasing over
time. We report the result in asymptotic notation as function of T and log(1/δ)."
T,0.48641655886157825,"D
Objective reformulation and linearization"
T,0.48771021992238034,"For the main objective we try to optimize over a subset of T trajectories X = {τi ∈X H}T
i=1. Let
X H be the set of sequences of inputs τ = (x1, ..., xH) where they consist of states in the search
space X. Furthermore, assume there exists, in the deterministic environment, a constraint such that
xh+1 ∈C(xh) for all h = 1, ..., H −1. Then we seek to find the set X∗, consisting of T trajectories
(possibly repeated), such that we solve the constrained optimization problem:"
T,0.4890038809831824,"X∗= arg min
X∈X T H max
z,z′∈Z Var[f(z) −f(z′)|X]
s.t.
xh+1 ∈C(xh)
∀t = 1, ..., h −1
(23)"
T,0.49029754204398446,We define the objective as:
T,0.49159120310478654,"U(X) = max
z,z′∈Z Var [f(z) −f(z′)|X]
(24)"
T,0.49288486416558863,"Our goal is to show that optimization over sequences can be simplified to state-action visitations as
in Mutný et al. [17]. For this, we require that the objective depends additively involving terms x, a
separately. We formalize this in the next result. In order to prove the result, we utilize the theory of
reproducing kernel Hilbert spaces [72]."
T,0.49417852522639066,"Lemma D.1 (Additivity of Best-arm Objective). Let X be a collection of t trajectories of length
H. Assuming that f ∼GP(0, k). Assuming that k has Mercer decomposition as k(x, y) =
P"
T,0.49547218628719275,k λkϕk(x)ϕk(y).
T,0.49676584734799484,"f(x) =
X"
T,0.49805950840879687,"k
ϕk(x)θk
θk ∼N(0, λk)."
T,0.49935316946959896,"Let dX be the visitation of the states-action in the trajectories in X, as dX =
1
T H
PT
t=1
P
x,a∈τt δx,a,
where the δx,a represent delta function supported on x, a. Then optimization of the objective Eq. (23)
can be rewritten as:"
T,0.500646830530401,"U(dX) =
1
TH max
z,z′∈Z ||Φ(z) −Φ(z′)||2
V(dX)−1,"
T,0.5019404915912031,"where V(d) = P i
P"
T,0.5032341526520052,"x,a∈τi d(x, a)Φ(x)Φ(x)⊤+ Iσ2/(TH) is a operator V(d) : Hk →Hk, the
norm is RKHS norm, and Φ(z)k = ϕk(z)."
T,0.5045278137128072,"Proof. Notice
that
the
posterior
GP
of
any
two
points
z, z′
is
(f(z), f(z′))
=
N((µ(z), µ(z′)), Kz,z′), where Kz,z′ is posterior kernel (consult Rasmussen and Williams [28] for
details) defined via a posterior kernel kX(z, z′) = k(z, z′) −k(z, X)(K(X, X) + σ2I)−1k(X, z′)."
T,0.5058214747736093,"Utilizing k(z, z′) = Φ(z)⊤Φ(z) (RKHS inner product) with the Mercer decomposition we know
that kt(z) = Φ(X)ϕ(z). Applying the matrix inversion lemma, the above can be written as using
V = PT
t=1
P"
T,0.5071151358344114,x∈τt Φ(x)Φ(x)⊤+ σ2IH.
T,0.5084087968952135,"kX(z, z′)
=
k(z, z′) −kt(z)⊤(KX,X + σ2I)−1kt(z′)"
T,0.5097024579560155,"Mercer
=
Φ(z)⊤Φ(z′) −Φ(z)⊤Φ(X)⊤(Φ(X)Φ(X)⊤+ σ2I)−1Φ(X)Φ(z′)"
T,0.5109961190168176,"Lemma D.3
=
Φ(z)⊤Φ(z′) −Φ(z)⊤V−1(V −Iσ2)Φ(z′)"
T,0.5122897800776197,"=
Φ(z)⊤V−1VΦ(z′) −Φ(z)⊤V−1(V −Iσ2)Φ(z′)"
T,0.5135834411384217,"=
Φ(z)⊤V−1(V −V + Iσ2)Φ(z′)"
T,0.5148771021992238,Leading finally to:
T,0.5161707632600259,"kX(z, z′) = σ2Φ(z)⊤
 T
X t=1 X"
T,0.517464424320828,"x∈τt
Φ(x)Φ(x)⊤+ σ2IHk !−1"
T,0.51875808538163,"Φ(z′).
(25)"
T,0.5200517464424321,"Let us calculate Var[f(z) −f(z′)|X]. The variance does not depend on the mean. Hence,"
T,0.5213454075032341,"Var [f(z) −f(z′)|X]
=
Var(f(z)) −Var(f(z′)) −2Cov(f(z), f(z′))
=
kX(z, z) + kX(z′, z′) −2kX(z, z′)"
T,0.5226390685640362,"(25)
=
(Φ(z) −Φ(z′)) T
X t=1 X"
T,0.5239327296248383,"x∈τt
Φ(x)Φ(x)⊤+ σ2IHk !−1"
T,0.5252263906856404,(Φ(z) −Φ(z′))
T,0.5265200517464425,"=
(Φ(z) −Φ(z′)) TH
TH T
X t=1 X"
T,0.5278137128072445,"x∈X
#(x ∈τt)Φ(x)Φ(x)⊤+ σ2IHk !−1"
T,0.5291073738680466,(Φ(z) −Φ(z′))
T,0.5304010349288486,to arrive at:
T,0.5316946959896507,Var [f(z) −f(z′)|X] = (Φ(z) −Φ(z′)) TH X
T,0.5329883570504528,"x∈X
d(X)Φ(x)Φ(x)⊤+ σ2"
T,0.5342820181112549,TH IHk !−1
T,0.535575679172057,(Φ(z) −Φ(z′)) (26)
T,0.536869340232859,"The symbol # counts the number of occurrences. Notice that we have been able to show that the
objective decomposes over state-action visitations as dX decomposes over their visitations"
T,0.538163001293661,"Note that the objective equivalence does not imply that optimization problem in Eq. (23) is equivalent
to finding,
d∗= arg min
dπ∈D
max
z,z′∈Z ||Φ(z) −Φ(z′)||V(dπ)−1.
(27)"
T,0.5394566623544631,"In other words, optimization over trajectories and optimization over dπ ∈D is not equivalent. The
latter is merely a continuous relaxation of discrete optimization problems to the space of Markov
policies. It is in line with the classical relaxation approach addressed in experiment design literature
with a rich history, e.g., Chaloner and Verdinelli [73]. For introductory texts on the topic, consider
Pukelsheim [74] for the statistical perspective and Boyd and Vandenberghe [75] for the optimization
perspective. However, as Mutný et al. [17] points out, reducing the relaxed objective does reduce the
objective as Eq. (23) as well. In other words, by optimizing the relaxation with a larger budget of
trajectories or horizons, we are able to decrease Eq. (23) as well."
T,0.5407503234152652,"For completeness, we state the auxiliary lemma. We make use of the Sherman-Morrison-Woodbury
(SMW) formula, [76]:"
T,0.5420439844760673,"Lemma D.2 (Sherman-Morrison-Woodbury (SMW)). Let A ∈Rn×q and D ∈Rq×q then:
(A⊤DA + ρ2I)−1 = ρ−2I −ρ−2AT (D−1ρ2 + AA⊤)−1A.
(28)
Here we do the opposite, and invert an n × n matrix instead of a q × q one."
T,0.5433376455368694,to show the following.
T,0.5446313065976714,"Lemma D.3 (Matrix Inversion Lemma). Let A ∈Rn×q then
A⊤(AA⊤+ ρ2I)−1 = (A⊤A + ρ2I)−1A⊤.
(29)
Note that instead of inverting n × n matrix, we can invert a q × q matrix."
T,0.5459249676584734,Proof.
T,0.5472186287192755,"A⊤(AA⊤+ ρ2I)−1
SMW
=
A⊤(ρ−2I −ρ−2A(ρ2I + A⊤A)−1A⊤)"
T,0.5485122897800776,"=
(ρ−2I −ρ−2A⊤A(ρ2I + A⊤A)−1)A⊤"
T,0.5498059508408797,"=
(ρ−2(ρ2I + A⊤A) −ρ−2A⊤A)(ρ2I + A⊤A)−1A⊤"
T,0.5510996119016818,"=
(A⊤A + ρ2I)−1A⊤"
T,0.5523932729624839,"D.1
Objective formulation for general kernel methods
The previous discussion also allows us to write the objective in terms of the general kernel matrix
instead of relying on finite dimensional embeddings. The modification is very similar and relies again
on Sherman-Mirrison-Woodbury lemma."
T,0.553686934023286,"We now work backwards from (26), and first write the objective in terms of features of arbitrarily
large size. Using the shorthand, ˜σ2 = σ2/TH, let us define a diagonal matrix that describes the
state-action distribution D = diag({dx : x ∈X}) of the size |X|×|X|, and Φ(X) which corresponds
to the unique (possibly infinite-dimensional) embeddings of each element in X, ordered in the same
way as D."
T,0.5549805950840879,"˜σ2
 X"
T,0.55627425614489,"x∈X
d(x)Φ(x)Φ(x) + I˜σ2
!−1"
T,0.5575679172056921,"= ˜σ2  
Φ(X)T DΦ(X) + I˜σ2−1"
T,0.5588615782664942,"= I −Φ(X)T (˜σ2D−1 + Φ(X)Φ(X)⊤)−1Φ(X)
If we then pre-multiply by Φ(z)⊤and Φ(z′) we obtain:
kX(z, z′) = Φ(z)⊤Φ(z′) −Φ(z)⊤Φ(X)⊤(˜σ2D−1 + Φ(X)Φ(X)⊤)−1Φ(X)Φ(z′)
Finally giving:
kX(z, z′) = k(z, z′) −k(z, X)(˜σ2D−1 + k(X, X))−1k(X, z′)
(30)
which allows us to calculate the objective for general kernel methods at the cost of an |X| × |X|
inversion. Upon identifying the z, z′ that maximize the above, we can use them in an optimization
procedure. This holds irrespective of whether the state space is discrete or continuous. In continuous
settings however, we again require a parametrization of the infinite dimensional probability distribu-
tion by some finite means such as claiming that Dθ contains some finite dimensional simplicity. This
is what we do with the linear system example in Section 4.2."
T,0.5601552393272963,"D.2
Linearizing the objective
To apply our method, we find ourselves having to frequently solve RL sub-problems where we try
to maximize P"
T,0.5614489003880984,"x,a d(x, a)∇F(x, a). To approximately solve this problem in higher dimensions, it
becomes very important to understand what the linearized functional looks like."
T,0.5627425614489003,"Remark D.4. Assume the same black-box model as in Lemma D.1, and further assume that we
have a mixture of policies πmix with density dπmix, such that there exists a set Xmix satisfying
dπmix = 1 N
P"
T,0.5640362225097024,x∈Xmix δx for some integer N. Then:
T,0.5653298835705045,"∇F(dπmix)(x, a) ∝−(Cov[f(z∗), f(x)|Xmix] −Cov[f(z′
∗), f(x)|Xmix])2"
T,0.5666235446313066,"where z∗, z′
∗= arg maxz,z′∈Z Var[f(z) −f(z′)]."
T,0.5679172056921087,"Proof. To show this, we begin by defining:"
T,0.5692108667529108,"Σθ,d = X"
T,0.5705045278137129,"x∈X
Φ(x)Φ(x)T d(x) + σ2I !−1"
T,0.5717981888745148,"z∗, z′
∗= arg max
z,z′∈Z
||Φ(z) −Φ(z′)||2
Σθ,d"
T,0.5730918499353169,"˜z∗= Φ(z∗) −Φ(z′
∗)"
T,0.574385510996119,"In the definition above we dropped the constant pre-factors since they do not influence the maximizer
of the gradient as they are related by a constant multiplicative factor."
T,0.5756791720569211,"It then follows, by applying Danskin’s Theorem that:"
T,0.5769728331177232,"∇U(d)(x) = ∇˜zT
∗Σθ,d˜z∗
= ∇Tr

˜z∗˜zT
∗Σθ,d"
T,0.5782664941785253,"= Tr

˜z∗˜zT
∗∇Σθ,d"
T,0.5795601552393272,"= −Tr

˜z∗˜zT
∗Σθ,dΦ(x)Φ(x)T Σθ,d
	
(as ∂K−1 = −K−1(∂K)K−1))"
T,0.5808538163001293,"= −Tr

˜zT
∗Σθ,dΦ(x)Φ(x)T Σθ,d˜z∗"
T,0.5821474773609314,"= −
 
˜zT
∗Σθ,dΦ(x)
  
Φ(x)T Σθ,d˜z∗
"
T,0.5834411384217335,"∝−(Cov[f(z∗), f(x)] −Cov[f(z′
∗), f(x)]) (Cov[f(x), f(z∗)] −Cov[f(x), f(z′
∗)])"
T,0.5847347994825356,"= −(Cov[f(z∗), f(x)] −Cov[f(z′
∗), f(x)])2"
T,0.5860284605433377,"E
Implementation details and Ablation study"
T,0.5873221216041398,"In this section we provide implementation details, and show some studies into the effects of specific
hyper-parameters. We note that the implementation code will be made public after public review."
T,0.5886157826649417,"E.1
Approximating the set of maximizers using Batch BayesOpt
We give details of the two methods used for approximating the set of potential maximizers. In
particular, we first focus on Thompson Sampling [58]:"
T,0.5899094437257438,"Z(T S)
cont =

arg max
x∈Xc
fi(x) : fi ∼GP(µt, σt)
K"
T,0.5912031047865459,"i=1
where K is a new hyper-parameter influencing the accuracy of the approximation of Z. We found
that the algorithm could be too exploratory in certain scenarios. Therefore, we also propose an
alternative that encourages exploitation by guiding the maximization set using BayesOpt through the
UCB acquisition function [71]:"
T,0.592496765847348,"Z(UCB)
cont
=

arg max
x∈Xc
µt(x) + βiσt(x) : βi ∈B
K"
T,0.5937904269081501,"i=1
where B = linspace(0, 2.5, K) which serves as scaling for the size of set Z(UCB)
cont
. Both cases
reduce optimization over Z to enumeration as with discrete cases."
T,0.5950840879689522,"E.2
Benchmark Details
For all benchmarks, aside from the knorr pyrazole synthesis example, we use a standard squared
exponential kernel for the surrogate Gaussian Process:"
T,0.5963777490297542,"krbf(x, x′) = σ2
rbf exp

−||x −x′||2
2
2ℓrbf "
T,0.5976714100905562,"where σ2
rbf is the prior variance of the kernel, and ℓrbf the kernel. We fix the values of all the hyper-
parameters a priori and use the same for all algorithms. The hyper-parameters for each benchmarks
are included in Table 2."
T,0.5989650711513583,"For the knorr pyrazole synthesis example, we further set αode = 0.6, αrbf = 0.001, k1 = 10,
k2 = 874, k3 = 19200, αsig = 5. Recall we are using a finite dimensional estimate of a GP such
that:"
T,0.6002587322121604,"f(x) ≈ωodeΦode(x) + M
X"
T,0.6015523932729625,"i=1
ωrbf,iΦrbf(x)
(31)"
T,0.6028460543337646,"in this case we set a prior to the ODE weight such that ωode ∼N(0.6, 0.0225). This is incorporating
two key pieces of prior knowledge that (a) the product concentration should be positive, and (b) we
expect a maximum product concentration between 0.15 and 0.45."
T,0.6041397153945667,"The number of features for each experiment, M, is set to be M = |X| in the discrete cases and
M = min
 
25+d, 512

where d is the problem dimensionality."
T,0.6054333764553687,"In the case of Local Search Region BayesOpt (LSR) [22] we set the exploration hyper-parameter to
be γ = 0.01 in all benchmarks."
T,0.6067270375161707,"Table 2: Benchmark and hyper-parameter information. ∆max represents the size of the box constraints
in the traditional benchmarks. For the synchronous benchmarks and for SnAr we used a noise level
of σ2 = 0.001. For the asynchronous benchmarks, and the knorr pyrazole example we used
σ2 = 0.0001. For the Ypacarai example we used σ2 = 0.001 and σ2 = 0.01 for the episodic and
immediate feedback respectively."
T,0.6080206985769728,"Benchmark Name
∆max
Variance σrbf
Lengthscale ℓrbf
Knorr pyrazole
–
0.001
0.1
Constrained Ypacarai
–
1
0.2
Branin2D
0.05
0.6
0.15
Hartmann3D
0.1
2.0
0.13849
Hartmann6D
0.2
1.7
0.22
Michalewicz2D
0.05
0.35
0.179485
Michalewicz3D
0.1
0.85
0.179485
Levy4D
0.1
0.6
0.14175
SnAr
0.1
0.8
0.2"
T,0.6093143596377749,"E.3
Ypacarai Lake
Samaniego et al. [20] investigated the use of Bayesian Optimization for monitoring the lake quality
in Lake Ypacarai in Paraguay. We extend the benchmark to include additional transition constraints,
as well as initial and end-point constraints. These are all shown in Figure 12."
T,0.610608020698577,"E.4
Free-electron Laser
We use the simulator from Mutný et al. [67] that optimizes quadrupole magnet orientations for our
experiment with varying noise levels. We use a 2-dimensional variant of the simulator. We discretize
the system on 10 × 10 grid and assume that the planning horizon H = 100. The simulator itself is
a GP fit with γ = 0.4, hence we use this value. Then we make a choice that the noise variance is
proportional to the change made as σ2(x, a) = s(1 + w||x −a||2), where s = 0.01 and w = 20.
Note that x ∈[−0.5, 0.5]2 in this modeling setup. This means that local steps are indeed very
desired. We showcase the difference to classical BayesOpt, which uses the worst-case variance
σ = supx,a s(1 + w||x −a||2) for modeling as it does not take into account the state in which the
system is. We see that the absence of state modeling leads to a dramatic decrease in performance as
indicated by much higher inference regret in Figure 3b."
T,0.6119016817593791,"Figure 12: Lake Ypacari with the added movement constraints. We show one local optimum and one
global one. The constraints of the problem requiring beginning and ending the optimization in the
dark square."
T,0.6131953428201811,"E.5
Ablation Study"
T,0.6144890038809832,"E.5.1
Number of mixture components
We investigate the effect number of components used in the mixture policy when optimizing the
Frank-Wolfe algorithm. We tested on the four real-world problem using N = 1, 10 and 25. In
the Ypacari example (see Figure 14) we see very little difference in the results, while in the Knorr
pyrazole synthesis (see Figure 13) we observe a much bigger difference. A single component gives a
much stronger performance than multiple ones – we conjecture this is because the optimum is on the
edge of the search space, and adding more components makes the policy stochastic and less likely
to reach the boarder (given episodes are of length ten and ten right-steps are required to reach the
boarder)."
T,0.6157826649417852,"Overall, it seems the performance of a single component is better or at worst comparable as using
multiple components. This is most likely due to the fact that we only follow the Markovian policies
for a single time-step before recalculating, making the overall impact of mixture policies smaller.
Based on this, we only present the single-component variant in the main paper."
T,0.6170763260025873,"(a) Episodic feedback results
(b) Immediate feedback results"
T,0.6183699870633894,"Figure 13: Ablation study on the number of mixture components on the Knorr pyrazole synthesis
benchmark"
T,0.6196636481241915,"E.5.2
Size of batch for approximating the set of maximizers"
T,0.6209573091849935,"We explore the effect of the number of maximizers, K, in the maximization sets Z(T S)
cont and Z(UCB)
cont
.
Overall we found the performance of the algorithm to be fairly robust to the size of the set in all
benchmarks, with a higher K generally leading to a little less spread in the performance."
T,0.6222509702457956,"(a) Episodic feedback results
(b) Immediate feedback results"
T,0.6235446313065977,Figure 14: Ablation study on the number of mixture components on the Ypacarai benchmark
T,0.6248382923673997,"Figure 15: Ablation study into the size of the Thompson Sampling maximization set in the asyn-
chronous Hartmann3D function. We can see that the performance of the algorithm is very similar for
all values of K = 25, 50, 100."
T,0.6261319534282018,"(a) Hartmann6D (asynch).
(b) Hartmann3D.
(c) Levy4D."
T,0.6274256144890039,"Figure 16: Ablation study into the size of the UCB maximization set in a variety of benchmarks. We
can see that the performance of the algorithm is very similar for all values of K = 10, 25, 100."
T,0.628719275549806,"F
XY-allocation vs G-allocation"
T,0.630012936610608,"Our objective is motivated by hypothesis testing between different arms (options) z and z′. In
particular,
U(d) = max
z′,z∈Z Var[f(z) −f(z′)|dX].
(32)"
T,0.6313065976714101,"One could maximize the information of the location of the optimum, as it has a Bayesian interpretation.
This is at odds in frequentist setting, where such interpretation does not exists. Optimization of
information about the maximum has been explored before, in particular via information-theoretic
acquisition functions [29, 77–79]. However, good results (in terms of regret) have been achieved by
focusing only on yet another surrogate to this, namely, the value of the maximum [80]. This is chiefly
due to problem of dealing with the distribution of f(x⋆). Defining a posterior value for f(z) is easy."
T,0.6326002587322122,"Figure 17: Comparison of using XY-allocation against G-allocation as the basis for the objective. In
both cases the maximization sets were created using Thompson Sampling. Overall the performances
were often similar, however in a few examples, such as Branin2D which we showcase here, G-
allocation performed very poorly. This is consistent with what we can expect from the bandits
literature."
T,0.6338939197930142,"Using, this and the worst-case perspective, an alternative way to approximate the best-arm objective,
could be:"
T,0.6351875808538163,"˜U(d) = max
z∈Z Var[f(z)|Xd].
(33)"
T,0.6364812419146184,"What are we losing by not considering the differences? The original objective corresponds to the
XY-allocation in the bandits literature. The modified objective will, in turn, correspond to the G-
allocation, which has been argued can perform arbitrarily worse as it does not consider the differences,
e.g. see Appendix A in Soare et al. [46]. We nonetheless implemented the algorithm with objective
(33), and found the results to be as expected: performance was very similar in general, however in
some cases not considering the differences led to much poorer performance. As an example, see
Figure 17 for results on the synchronous Branin2D benchmark."
T,0.6377749029754204,"G
Practical Planning for Continuous MDPs"
T,0.6390685640362225,"From remark D.4 it becomes clear that for decreasing covariance functions, such as the squared
exponential, ∇F will consist of two modes around z∗and z′
∗The sub-problem seems to find a
sequence that maximizes the sum of gradients, therefore the optimal solution will try to reach one of
the two modes as quickly as possible. For shorter time horizons, the path will reach whichever mode
is closest, and for large enough horizons, the sum will be maximized by reaching the larger of the
two modes."
T,0.6403622250970246,"Therefore we can approximately solve the problem by checking the value of the sub-problem objective
in (13) for the shortest paths from xt−1 →z∗and xt−1 →z′
∗, which are trivial to find under the
constraints in (13). Note that the paths might not necessarily be optimal, as they may be improved by
small perturbations, e.g., there might be a small deviation that allows us to visit the smaller mode on
the way to the larger mode increasing the overall value of the sum of gradients, however, they give us
a good and quick approximation."
T,0.6416558861578266,"H
Kernel for ODE Knorr pyrazole synthesis"
T,0.6429495472186287,"The kernel is based on the following ODE model, which is well known in the chemistry literature and
given in [27]."
T,0.6442432082794308,"R1 = k1y2y3 −k2y4y5
(34)
R2 = k3y4
(35)"
T,0.6455368693402329,"and then:
dy1"
T,0.6468305304010349,dt = R2 dy2
T,0.648124191461837,dt = −R1 dy3
T,0.6494178525226391,dt = −R1 dy4
T,0.6507115135834411,dt = R1 −R2 dy5
T,0.6520051746442432,dt = R1 + R2
T,0.6532988357050453,"Our main goal is to optimize the product concentration of the reaction, which is given by y1. We
do this by sequentially querying the reaction, where we select the residence time, and the initial
conditions of the ODE, in the form y0 = [0, A, B, 0, 0], where A = 1 −B."
T,0.6545924967658473,"Due to the non-linearity in Eq. (34) we are unable to fit a GP to the process directly. Instead, we first
linearize the ODE around two equilibrium points. The set of points of equilibrium are given by:"
T,0.6558861578266494,"S1 = {y1 = a1, y2 = b1, y3 = 0, y4 = 0, y5 = c1|a1, b1, c1 ∈R}
S2 = {y1 = a2, y2 = 0, y3 = b2, y4 = 0, y5 = c2|a1, b1, c1 ∈R}"
T,0.6571798188874515,And the Jacobian of the system is: J =  
T,0.6584734799482536,"0
0
0
k3
0
0
−k1y3
−k1y2
k2y5
k2y4
0
−k1y3
−k1y2
k2y5
k2y4
0
k1y3
k1y2
−k2y5 −k3
−k2y4
0
k1y3
k1y2
−k2y5 + k3
−k2y4  "
T,0.6597671410090556,Giving:
T,0.6610608020698577,J1 = J|S1 =  
T,0.6623544631306598,"0
0
0
k3
0
0
0
−k1b1
k2c1
0
0
0
−k1b1
k2c1
0
0
0
k1b1
−k2c1 −k3
0
0
0
k1b1
−k2c1 + k3
0  "
T,0.6636481241914618,J2 = J|S2 =  
T,0.6649417852522639,"0
0
0
k3
0
0
−k1b2
0
k2c2
0
0
−k1b2
0
k2c2
0
0
k1b2
0
−k2c2 −k3
0
0
k1b2
0
−k2c2 + k3
0  "
T,0.666235446313066,"Unfortunately, since the matrices are singular, we do not get theoretical results on the quality of the
linearization. However, linearization is still possible, with the linear systems given by: d⃗y"
T,0.6675291073738681,"dt = J1⃗y
d⃗y"
T,0.6688227684346701,dt = J2⃗y
T,0.6701164294954722,We focus on the first system for now. The matrix has the following eigenvalues:
T,0.6714100905562742,"λ1,2 = −1 2"
T,0.6727037516170763,"
b1k1 + c1k2 + k3 ±
q"
T,0.6739974126778784,"b2
1k2
1 + c2
1k2
2 + k2
3 + 2b1c1k1k2 −2k3(b1k1 −c1k2)
"
T,0.6752910737386805,"λ3,4,5 = 0"
T,0.6765847347994826,"Note that the three eigenvalues give us the corresponding solution based on their (linearly separable)
eigenvectors:"
T,0.6778783958602846,"v3 = [1
0
0
0
0] ,
v4 = [0
1
0
0
0] ,
v5 = [0
0
0
0
1]"
T,0.6791720569210866,⃗y(t) = p3v3 + p4v4 + p5v5
T,0.6804657179818887,"where pi are constants. The behaviour of the ODE when this is not the case will depend on whether
the remaining eigenvalues will be real or not. However, note:"
T,0.6817593790426908,"b2
1k2
1 + c2
1k2
2 + k2
3 + 2b1c1k1k2 −2k3(b1k1 −c1k2) ≥b2
1k2
1 + k2
3 −2b1k1k3 = (b1k1 −k3)2 ≥0"
T,0.6830530401034929,and therefore all eigenvalues will always be real. Therefore we can write down the solution as:
T,0.684346701164295,⃗y(t) = p1v1eλ1t + p2v2eλ2t + p3v3 + p4v4 + p5v5
T,0.685640362225097,"where we ignore the case of repeated eigenvalues for simplicity (this is the case where b2
1k2
1 +
2b1c1k1k2 + c2
1k2
2 −2k3(b1k1 −c1k2) + k2
3 is exactly equal to zero). We further note that the
eigenvalues will be non-negative as:"
T,0.6869340232858991,"b1k1 + c1k2 + k3 =
p"
T,0.6882276843467011,"(bk1 + c1k2 + k3)2 =
q"
T,0.6895213454075032,"b2
1k2
1 + c2k2
2 + k2
3 + 2b1c1k1k2 + 2c1k2k2 + 2b1k1k3 ≥
q"
T,0.6908150064683053,"b2
1k2
1 + c2
1k2
2 + k2
3 + 2b1c1k1k2 + 2c1k2k2 −2b1k1k3"
T,0.6921086675291074,"therefore:
λ1 ≤λ2 ≤0"
T,0.6934023285899095,"which means the solutions will always be a linear combination of exponentially decaying functions
of time plus constants."
T,0.6946959896507116,The eigenvectors have the closed form: v1 = 
T,0.6959896507115135,"






 1,"
T,0.6972833117723156,"1
2

b1k1 + c1k2 −k3 +
p"
T,0.6985769728331177,"b2
1k2
1 + 2b1c1k1k2 + c2
1k2
2 −2(b1k1 −c1k2)k3 + k2
3

/k3,"
T,0.6998706338939198,"1
2

b1k1 + c1k2 −k3 +
p"
T,0.7011642949547219,"b2
1k2
1 + 2b1c1k1k2 + c2
1k2
2 −2(b1k1 −c1k2)k3 + k2
3

/k3, −1"
T,0.702457956015524,"2

b1k1 + c1k2 + k3 +
p"
T,0.703751617076326,"b2
1k2
1 + 2b1c1k1k2 + c2
1k2
2 −2(b1k1 −c1k2)k3 + k2
3

/k3, −1"
T,0.705045278137128,"2

b1k1 + c1k2 −3k3 +
p"
T,0.7063389391979301,"b2
1k2
1 + 2b1c1k1k2 + c2
1k2
2 −2(b1k1 −c1k2)k3 + k2
3

/k3 "
T,0.7076326002587322,"






 = "
T,0.7089262613195343,"


"
T,0.7102199223803364,"1,
−λ1/k3 −1,
−λ1/k3 −1,
λ1/k3,
λ1/k3 + 2 "
T,0.7115135834411385,"


 v2 = "
T,0.7128072445019404,"






 1,"
T,0.7141009055627425,"1
2

b1k1 + c1k2 −k3 −
p"
T,0.7153945666235446,"b2
1k2
1 + 2b1c1k1k2 + c2
1k2
2 −2(b1k1 −c1k2)k3 + k2
3

/k3,"
T,0.7166882276843467,"1
2

b1k1 + c1k2 −k3 −
p"
T,0.7179818887451488,"b2
1k2
1 + 2b1c1k1k2 + c2
1k2
2 −2(b1k1 −c1k2)k3 + k2
3

/k3, −1"
T,0.7192755498059509,"2

b1k1 + c1k2 + k3 −
p"
T,0.720569210866753,"b2
1k2
1 + 2b1c1k1k2 + c2
1k2
2 −2(b1k1 −c1k2)k3 + k2
3

/k3, −1"
T,0.7218628719275549,"2

b1k1 + c1k2 −3k3 −
p"
T,0.723156532988357,"b2
1k2
1 + 2b1c1k1k2 + c2
1k2
2 −2(b1k1 −c1k2)k3 + k2
3

/k3 "
T,0.7244501940491591,"






 = "
T,0.7257438551099612,"


"
T,0.7270375161707633,"1,
−λ2/k3 −1,
−λ2/k3 −1,
λ2/k3,
λ2/k3 + 2 "
T,0.7283311772315654,"


"
T,0.7296248382923674,"We are optimizing over initial set of conditions y0 = [0, A, B, 0, 0], so solving for the specific values
of the constants gives:"
T,0.7309184993531694,"p1 =
λ2
λ1 −λ2
B"
T,0.7322121604139715,"p2 = −
λ1
λ1 −λ2
B"
T,0.7335058214747736,"p3 = B
p4 = A −B
p5 = 2B"
T,0.7347994825355757,"Finally, since we are setting A = 1 −B and we are only optimizing the first component of ⃗y we can
obtain it in closed form:"
T,0.7360931435963778,"y1(t, B) =
λ2
λ1 −λ2
Beλ1t −
λ1
λ1 −λ2
Beλ2t + B"
T,0.7373868046571799,"= B

λ2
λ1 −λ2
eλ1t −
λ1
λ1 −λ2
eλ2t + 1

(36)"
T,0.7386804657179818,"0
2
4
6
8
10
Time 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Yield"
T,0.7399741267787839,"numerical
exact"
T,0.741267787839586,Figure 18: Comparing the numerical solution against the solutions found in equation (36).
T,0.7425614489003881,"The second ODE is very similar to the first, recall it depends has the following matrix:"
T,0.7438551099611902,J2 = J|S2 =  
T,0.7451487710219923,"0
0
0
k3
0
0
−k1b2
0
k2c2
0
0
−k1b2
0
k2c2
0
0
k1b2
0
−k2c2 −k3
0
0
k1b2
0
−k2c2 + k3
0  "
T,0.7464424320827943,The resulting ODE is symmetric to the alternate linearization giving the same solution:
T,0.7477360931435963,y(t) = p1v1eλ1t + p2v2eλ2t + p3v3 + p4v4 + p5v5
T,0.7490297542043984,with the only difference being the eigenvectors now are:
T,0.7503234152652005,"v3 = [1
0
0
0
0] ,
v4 = [0
0
1
0
0] ,
v5 = [0
0
0
0
1]"
T,0.7516170763260026,which in turn leads to solutions of the form:
T,0.7529107373868047,"y1(t, B) =
λ2
λ1 −λ2
Aeλ1t −
λ1
λ1 −λ2
Aeλ2t + A"
T,0.7542043984476067,"= A

λ2
λ1 −λ2
eλ1t −
λ1
λ1 −λ2
eλ2t + 1
"
T,0.7554980595084088,"where A = 1−B. Note that we now have four different eigenvalues, which depend on the linearization
points:"
T,0.7567917205692108,"λ(1)
1,2 = −1 2"
T,0.7580853816300129,"
b1k1 + c1k2 + k3 ±
q"
T,0.759379042690815,"b2
1k2
1 + c2
1k2
2 + k2
3 + 2b1c1k1k2 −2k3(b1k1 −c1k2)
"
T,0.7606727037516171,"λ(2)
1,2 = −1 2"
T,0.7619663648124192,"
b2k1 + c2k2 + k3 ±
q"
T,0.7632600258732212,"b2
2k2
1 + c2
2k2
2 + k2
3 + 2b2c2k1k2 −2k3(b2k1 −c2k2)
"
T,0.7645536869340233,Giving solutions:
T,0.7658473479948253,"y(1)
1 (t, B) = B"
T,0.7671410090556274,"λ(1)
2
λ(1)
1
−λ(1)
2
eλ(1)
1
t −
λ(1)
1
λ(1)
1
−λ(1)
2
eλ(1)
2
t + 1 !"
T,0.7684346701164295,"y(2)
1 (t, B) = A"
T,0.7697283311772316,"λ(2)
2
λ(2)
1
−λ(2)
2
eλ(2)
1
t −
λ(2)
1
λ(2)
1
−λ(2)
2
eλ(2)
2
t + 1 !"
T,0.7710219922380336,"Due to the length of the derivation, we confirm that our analysis is correct by comparing the numerical
solution of the ODE to the exact solution we found in Figure 18. Finally, we look at interpolating
between the two solutions; so given the solutions y(1)(t, B) and y(2)(t, B) corresponding to the
linearization with stationary point in S1 and S2 respectively, we consider a solution of the form:"
T,0.7723156532988357,"y(t, B|k1, k2, k3, α) = (1 −S(B))y(1)(t, B) + S(B)y(2)(t, B)
(37)"
T,0.7736093143596378,"where S(x) := (1 + e−αsig(x−0.5))−1 is a sigmoid function centered at B = 0.5 and where we
have introduced a new hyper-parameter αsig. Finally, given Eq. (37) we can obtain the kernel. In
particular, we want (37) to be a feature we are predicting on; therefore the kernel is simply the (dot)
product of the features therefore:"
T,0.7749029754204398,"kode((t, B), (t′, B′)) = y(t, B|k1, k2, k3, α) × y(t′, B′|k1, k2, k3, α)"
T,0.7761966364812419,"And because we know we are simply approximating the data we can simply correct the model by
adding an Gaussian Process correction; giving us the final kernel:"
T,0.777490297542044,"kjoint((t, B), (t′, B′)) = αodekode((t, B), (t′, B′)) + αrbfkrbf((t, B), (t′, B′))"
T,0.7787839586028461,"where αode and αrbf are parameters we can learn, e.g. using the marginal likelihood."
T,0.7800776196636481,NeurIPS Paper Checklist
T,0.7813712807244502,"The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit."
T,0.7826649417852523,"Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:"
T,0.7839586028460543,"• You should answer [Yes] , [No] , or [NA] ."
T,0.7852522639068564,"• [NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available."
T,0.7865459249676585,• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
T,0.7878395860284605,"The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper."
T,0.7891332470892626,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
proper justification is given (e.g., ""error bars are not reported because it would be too computationally
expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found."
T,0.7904269081500647,"IMPORTANT, please:"
T,0.7917205692108668,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"","
T,0.7930142302716688,"• Keep the checklist subsection headings, questions/answers and guidelines below."
T,0.7943078913324709,• Do not modify the questions and only use the provided macros for your answers.
CLAIMS,0.795601552393273,1. Claims
CLAIMS,0.796895213454075,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.7981888745148771,Answer: [Yes]
CLAIMS,0.7994825355756792,Justification: All claims in the abstract are backed up by and developed in the paper.
CLAIMS,0.8007761966364813,Guidelines:
CLAIMS,0.8020698576972833,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper."
CLAIMS,0.8033635187580854,"• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers."
CLAIMS,0.8046571798188874,"• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings."
CLAIMS,0.8059508408796895,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8072445019404916,2. Limitations
LIMITATIONS,0.8085381630012937,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8098318240620958,Answer: [Yes]
LIMITATIONS,0.8111254851228978,"Justification: We discuss limitations in the conclusion: we are restricted to a specific
parameterization in the continuous state-space case."
LIMITATIONS,0.8124191461836999,Guidelines:
LIMITATIONS,0.8137128072445019,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper."
LIMITATIONS,0.815006468305304,"• The authors are encouraged to create a separate ""Limitations"" section in their paper."
LIMITATIONS,0.8163001293661061,"• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be."
LIMITATIONS,0.8175937904269082,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated."
LIMITATIONS,0.8188874514877102,"• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon."
LIMITATIONS,0.8201811125485123,"• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size."
LIMITATIONS,0.8214747736093143,"• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness."
LIMITATIONS,0.8227684346701164,"• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8240620957309185,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8253557567917206,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8266494178525227,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.8279430789133247,"Justification: All proofs are in the Appendix, where we provide full assumptions and proofs,
or cite the relevant results. Specifically see Appendix C – D."
THEORY ASSUMPTIONS AND PROOFS,0.8292367399741267,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8305304010349288,• The answer NA means that the paper does not include theoretical results.
THEORY ASSUMPTIONS AND PROOFS,0.8318240620957309,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced."
THEORY ASSUMPTIONS AND PROOFS,0.833117723156533,• All assumptions should be clearly stated or referenced in the statement of any theorems.
THEORY ASSUMPTIONS AND PROOFS,0.8344113842173351,"• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition."
THEORY ASSUMPTIONS AND PROOFS,0.8357050452781372,"• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material."
THEORY ASSUMPTIONS AND PROOFS,0.8369987063389392,• Theorems and Lemmas that the proof relies upon should be properly referenced.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8382923673997412,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8395860284605433,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8408796895213454,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8421733505821475,Justification: We include all the method and computational details in G.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8434670116429496,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8447606727037517,• The answer NA means that the paper does not include experiments.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8460543337645536,"• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8473479948253557,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8486416558861578,"• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8499353169469599,"• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.851228978007762,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8525226390685641,"(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8538163001293662,"(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset)."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8551099611901681,"(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8564036222509702,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8576972833117723,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8589909443725744,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8602846054333765,"Justification: The code will be made public upon acceptance, and an anonymized version is
included for the review process."
OPEN ACCESS TO DATA AND CODE,0.8615782664941786,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8628719275549805,• The answer NA means that paper does not include experiments requiring code.
OPEN ACCESS TO DATA AND CODE,0.8641655886157826,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details."
OPEN ACCESS TO DATA AND CODE,0.8654592496765847,"• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark)."
OPEN ACCESS TO DATA AND CODE,0.8667529107373868,"• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details."
OPEN ACCESS TO DATA AND CODE,0.8680465717981889,"• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc."
OPEN ACCESS TO DATA AND CODE,0.869340232858991,"• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why."
OPEN ACCESS TO DATA AND CODE,0.8706338939197931,"• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable)."
OPEN ACCESS TO DATA AND CODE,0.871927554980595,"• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8732212160413971,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8745148771021992,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8758085381630013,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8771021992238034,Justification: We include all the method and computational details in G.
OPEN ACCESS TO DATA AND CODE,0.8783958602846055,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8796895213454075,• The answer NA means that the paper does not include experiments.
OPEN ACCESS TO DATA AND CODE,0.8809831824062095,"• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them."
OPEN ACCESS TO DATA AND CODE,0.8822768434670116,"• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8835705045278137,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8848641655886158,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8861578266494179,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.88745148771022,"Justification: We provide regret quantiles in our experiments, reproduce on 25 random seeds
and use a variety of different benchmarks."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.888745148771022,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.890038809831824,• The answer NA means that the paper does not include experiments.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8913324708926261,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8926261319534282,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8939197930142303,"• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8952134540750324,"• The assumptions made should be given (e.g., Normally distributed errors)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8965071151358344,"• It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8978007761966365,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8990944372574385,"• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9003880983182406,"• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9016817593790427,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9029754204398448,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9042690815006468,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9055627425614489,Justification: See section B.4.
EXPERIMENTS COMPUTE RESOURCES,0.906856403622251,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.908150064683053,• The answer NA means that the paper does not include experiments.
EXPERIMENTS COMPUTE RESOURCES,0.9094437257438551,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage."
EXPERIMENTS COMPUTE RESOURCES,0.9107373868046572,"• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute."
EXPERIMENTS COMPUTE RESOURCES,0.9120310478654593,"• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9133247089262613,9. Code Of Ethics
CODE OF ETHICS,0.9146183699870634,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9159120310478654,Answer: [Yes]
CODE OF ETHICS,0.9172056921086675,"Justification: Upon reading the guidelines, the paper seems to conform to every point in the
NeurIPS Code of Ethics."
CODE OF ETHICS,0.9184993531694696,Guidelines:
CODE OF ETHICS,0.9197930142302717,• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
CODE OF ETHICS,0.9210866752910737,"• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics."
CODE OF ETHICS,0.9223803363518758,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9236739974126779,10. Broader Impacts
BROADER IMPACTS,0.92496765847348,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.926261319534282,Answer: [NA]
BROADER IMPACTS,0.9275549805950841,"Justification: There are no broader impacts to discuss, outside of those already established
by research into the area of design of experiments."
BROADER IMPACTS,0.9288486416558862,Guidelines:
BROADER IMPACTS,0.9301423027166882,• The answer NA means that there is no societal impact of the work performed.
BROADER IMPACTS,0.9314359637774903,"• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact."
BROADER IMPACTS,0.9327296248382924,"• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
BROADER IMPACTS,0.9340232858990944,"• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster."
BROADER IMPACTS,0.9353169469598965,"• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology."
BROADER IMPACTS,0.9366106080206986,"• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9379042690815006,11. Safeguards
SAFEGUARDS,0.9391979301423027,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9404915912031048,Answer: [NA]
SAFEGUARDS,0.9417852522639069,Justification: There are no risks for misuse of our work.
SAFEGUARDS,0.943078913324709,Guidelines:
SAFEGUARDS,0.944372574385511,• The answer NA means that the paper poses no such risks.
SAFEGUARDS,0.9456662354463131,"• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters."
SAFEGUARDS,0.9469598965071151,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images."
SAFEGUARDS,0.9482535575679172,"• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9495472186287193,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9508408796895214,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9521345407503234,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9534282018111255,Justification: All exisiting assests are cited and the corresponding licenses respected.
LICENSES FOR EXISTING ASSETS,0.9547218628719275,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9560155239327296,• The answer NA means that the paper does not use existing assets.
LICENSES FOR EXISTING ASSETS,0.9573091849935317,• The authors should cite the original paper that produced the code package or dataset.
LICENSES FOR EXISTING ASSETS,0.9586028460543338,"• The authors should state which version of the asset is used and, if possible, include a
URL."
LICENSES FOR EXISTING ASSETS,0.9598965071151359,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset."
LICENSES FOR EXISTING ASSETS,0.9611901681759379,"• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided."
LICENSES FOR EXISTING ASSETS,0.96248382923674,"• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset."
LICENSES FOR EXISTING ASSETS,0.963777490297542,"• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9650711513583441,"• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9663648124191462,13. New Assets
NEW ASSETS,0.9676584734799483,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9689521345407504,Answer: [Yes]
NEW ASSETS,0.9702457956015524,"Justification: The only new asset will be the code implementation which will be published
upon acceptance."
NEW ASSETS,0.9715394566623544,Guidelines:
NEW ASSETS,0.9728331177231565,• The answer NA means that the paper does not release new assets.
NEW ASSETS,0.9741267787839586,"• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc."
NEW ASSETS,0.9754204398447607,"• The paper should discuss whether and how consent was obtained from people whose
asset is used."
NEW ASSETS,0.9767141009055628,"• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780077619663649,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793014230271668,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805950840879689,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981888745148771,Justification: No crowd-sourcing or human subjected were used.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831824062095731,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844760672703752,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857697283311773,"• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9870633893919794,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883570504527813,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896507115135834,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9909443725743855,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922380336351876,Justification: No human subjects were used in this research.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935316946959897,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948253557567918,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961190168175937,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974126778783958,"• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9987063389391979,"• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
