Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029940119760479044,"Equivariance has emerged as a desirable property of representations of objects
subject to identity-preserving transformations that constitute a group, such as
translations and rotations.
However, the expressivity of a representation con-
strained by group equivariance is still not fully understood. We address this gap
by providing a generalization of Cover’s Function Counting Theorem that quanti-
ﬁes the number of linearly separable and group-invariant binary dichotomies that
can be assigned to equivariant representations of objects. We ﬁnd that the frac-
tion of separable dichotomies is determined by the dimension of the space that is
ﬁxed by the group action. We show how this relation extends to operations such
as convolutions, element-wise nonlinearities, and global and local pooling. While
other operations do not change the fraction of separable dichotomies, local pool-
ing decreases the fraction, despite being a highly nonlinear operation. Finally, we
test our theory on intermediate representations of randomly initialized and fully
trained convolutional neural networks and ﬁnd perfect agreement."
INTRODUCTION,0.005988023952095809,"1
INTRODUCTION"
INTRODUCTION,0.008982035928143712,"The ability to robustly categorize objects under conditions and transformations that preserve the
object categories is essential to animal intelligence, and to pursuits of practical importance such as
improving computer vision systems. However, for general-purpose understanding and geometric
reasoning, invariant representations of these objects in sensory processing circuits are not enough.
Perceptual representations must also accurately encode their transformation properties."
INTRODUCTION,0.011976047904191617,"One such property is that of exhibiting equivariance to transformations of the object. When such
transformations are restricted to be an algebraic group, the resulting equivariant representations have
found signiﬁcant success in machine learning starting with classical convolutional neural networks
(CNNs) (Denker et al., 1989; LeCun et al., 1989) and recently being generalized by the inﬂuen-
tial work of Cohen & Welling (2016). Such representations have elicited burgeoning interest as
they capture many transformations of practical interest such as translations, permutations, rotations,
and reﬂections. Furthermore, equivariance to these transformations can be easily “hard-coded” into
neural networks. Indeed, a new breed of CNN architectures that explicitly account for such transfor-
mations are seeing diverse and rapidly growing applications (Townshend et al., 2021; Baek et al.,
2021; Satorras et al., 2021; Anderson et al., 2019; Bogatskiy et al., 2020; Klicpera et al., 2020;
Winkels & Cohen, 2019; Gordon et al., 2020; Sosnovik et al., 2021; Eismann et al., 2020). In ad-
dition, equivariant CNNs have been shown to capture response properties of neurons in the primary
visual cortex beyond classical G´abor ﬁlter models (Ecker et al., 2018)."
INTRODUCTION,0.014970059880239521,∗These authors contributed equally.
INTRODUCTION,0.017964071856287425,Published as a conference paper at ICLR 2022
INTRODUCTION,0.020958083832335328,"While it is clear that equivariance imposes a strong constraint on the geometry of representations and
thus of perceptual manifolds (Seung & Lee, 2000; DiCarlo & Cox, 2007) that are carved out by these
representations as the objects transform, the implications of such constraints on their expressivity
are not well understood. In this work we take a step towards addressing this gap. Our starting
point is the classical notion of the perceptron capacity (sometimes also known as the fractional
memory/storage capacity) – a quantity fundamental to the task of object categorization and closely
related to VC dimension (Vapnik & Chervonenkis, 1968). Deﬁned as the maximum number of
points for which all (or 1-δ fraction of) possible binary label assignments (i.e. dichotomies) afford
a hyperplane that separates points with one label from the points with the other, it can be seen to
offer a quantiﬁcation of the expressivity of a representation."
INTRODUCTION,0.023952095808383235,"Classical work on perceptron capacity focused on points in general position (Wendel, 1962; Cover,
1965; Schl¨aﬂi, 1950; Gardner, 1987; 1988). However, understanding the perceptron capacity when
the inputs are not merely points, but are endowed with richer structure, has only recently started to
attract attention. For instance, work by Chung et al. (2018); Pastore et al. (2020); Rotondo et al.
(2020); Cohen et al. (2020) considered general perceptual manifolds and examined the role of their
geometry to obtain extensions to the perceptron capacity results. However, such work crucially
relied on the assumption that each manifold is oriented randomly, a condition which is strongly
violated by equivariant representations."
INTRODUCTION,0.02694610778443114,"With these motivations, our particular contributions in this paper are the following:"
INTRODUCTION,0.029940119760479042,"• We extend Cover’s function counting theorem and VC dimension to equivariant representations,
ﬁnding that both scale with the dimension of the subspace ﬁxed by the group action."
INTRODUCTION,0.03293413173652695,"• We demonstrate the applicability of our result to G-convolutional network layers, including pool-
ing layers, through theory and verify through simulation."
RELATED WORKS,0.03592814371257485,"1.1
RELATED WORKS
Work most related to ours falls along two major axes. The ﬁrst follows the classical perceptron
capacity result on the linear separability of points (Schl¨aﬂi, 1950; Wendel, 1962; Cover, 1965;
Gardner, 1987; 1988). This result initiated a long history of investigation in theoretical neuroscience,
e.g.
(Brunel et al., 2004; Chapeton et al., 2012; Rigotti et al., 2013; Brunel, 2016; Rubin et al.,
2017; Pehlevan & Sengupta, 2017), where it is used to understand the memory capacity of neuronal
architectures. Similarly, in machine learning, the perceptron capacity and its variants, including
notions for multilayer perceptrons, have been fundamental to a fruitful line of study in the context of
ﬁnite sample expressivity and generalization (Baum, 1988; Kowalczyk, 1997; Sontag, 1997; Huang,
2003; Yun et al., 2019; Vershynin, 2020). Work closest in spirit to ours comes from theoretical
neuroscience and statistical physics (Chung et al., 2018; Pastore et al., 2020; Rotondo et al., 2020;
Cohen et al., 2020), which considered general perceptual manifolds, albeit oriented randomly, and
examined the role of their geometry to obtain extensions to the perceptron capacity result."
RELATED WORKS,0.038922155688622756,"The second line of relevant literature is that on group-equivariant convolutional neural networks
(GCNNs). The main inspiration for such networks comes from the spectacular success of classical
CNNs (LeCun et al., 1989) which directly built in translational symmetry into the network architec-
ture. In particular, the internal representations of a CNN are approximately1 translation equivariant:
if the input image is translated by an amount t, the feature map of each internal layer is translated
by the same amount. Furthermore, an invariant read-out on top ensures that a CNN is translation
invariant.
Cohen & Welling (2016) observed that a viable approach to generalize CNNs to other
data types could involve considering equivariance to more general transformation groups. This idea
has been used to construct networks equivariant to a wide variety of transformations such as pla-
nar rotations (Worrall et al., 2017; Weiler et al., 2018b; Bekkers et al., 2018; Veeling et al., 2018;
Smets et al., 2020), 3D rotations (Cohen et al., 2018; Esteves et al., 2018; Worrall & Brostow, 2018;
Weiler et al., 2018a; Kondor et al., 2018a; Perraudin et al., 2019), permutations (Zaheer et al., 2017;
Hartford et al., 2018; Kondor et al., 2018b; Maron et al., 2019a; 2020), general Euclidean isome-
tries (Weiler et al., 2018a; Weiler & Cesa, 2019; Finzi et al., 2020), scaling (Marcos et al., 2018;
Worrall & Welling, 2019; Sosnovik et al., 2020) and more exotic symmetries (Bogatskiy et al., 2020;
Shutty & Wierzynski, 2020; Finzi et al., 2021) etc."
SOME OPERATIONS SUCH AS MAX POOLING AND BOUNDARY EFFECTS OF THE CONVOLUTIONS TECHNICALLY BREAK STRICT,0.041916167664670656,"1Some operations such as max pooling and boundary effects of the convolutions technically break strict
equivariance, as well as the ﬁnal densely connected layers."
SOME OPERATIONS SUCH AS MAX POOLING AND BOUNDARY EFFECTS OF THE CONVOLUTIONS TECHNICALLY BREAK STRICT,0.04491017964071856,Published as a conference paper at ICLR 2022
SOME OPERATIONS SUCH AS MAX POOLING AND BOUNDARY EFFECTS OF THE CONVOLUTIONS TECHNICALLY BREAK STRICT,0.04790419161676647,"A quite general theory of equivariant/invariant networks has also emerged. Kondor & Trivedi (2018)
gave a complete description of GCNNs for scalar ﬁelds on homogeneous spaces of compact groups.
This was generalized further to cover the steerable case in (Cohen et al., 2019b) and to general gauge
ﬁelds in (Cohen et al., 2019a; Weiler et al., 2021). This theory also includes universal approxima-
tion results (Yarotsky, 2018; Keriven & Peyr´e, 2019; Sannai et al., 2019b; Maron et al., 2019b; Segol
& Lipman, 2020; Ravanbakhsh, 2020). Nevertheless, while beneﬁts of equivariance/invariance in
terms of improved sample complexity and ease of training are quoted frequently, a ﬁrm theoretical
understanding is still largely missing. Some results however do exist, going back to (Shawe-Taylor,
1991). Abu-Mostafa (1993) made the argument that restricting a classiﬁer to be invariant can not
increase its VC dimension. Sokolic et al. (2017) extend this idea to derive generalization bounds
for invariant classiﬁers, while Sannai et al. (2019a) do so speciﬁcally working with the permutation
group. Elesedy & Zaidi (2021) show a strict generalization beneﬁt for equivariant linear models,
showing that the generalization gap between a least squares model and its equivariant version de-
pends on the dimension of the space of anti-symmetric linear maps. Some beneﬁts of related ideas
such as data augmentation and invariant averaging are formally shown in (Lyle et al., 2020; Chen
et al., 2020). Here we focus on the limits to expressivity enforced by equivariance."
PROBLEM FORMULATION,0.05089820359281437,"2
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.05389221556886228,"Suppose x abstractly represents an object and let r(x) ∈RN be some feature map of x to an
N-dimensional space (such as an intermediate layer of a deep neural network). We consider trans-
formations of this object, such that they form a group in the algebraic sense of the word. We denote
the abstract transformation of x by element g ∈G as gx. Groups G may be represented by invert-
ible matrices, which act on a vector space V (which themselves form the group GL(V ) of invertible
linear transformations on V ). We are interested in feature maps r which satisfy the following group
equivariance condition:
r(gx) = π(g)r(x),"
PROBLEM FORMULATION,0.05688622754491018,"where π : G →GL(RN) is a linear representation of G which acts on feature map r(x). Note
that many representations of G are possible, including the trivial representation: π(g) = I for all g."
PROBLEM FORMULATION,0.059880239520958084,"We are interested in perceptual object manifolds generated by the actions of G. Each of the P
manifolds can be written as a set of points {π(g)rµ : g ∈G} where µ ∈[P] ≡{1, 2, . . . , P}; that
is, these manifolds are orbits of the point rµ ≡r(xµ) under the action of π. We will refer to such
manifolds as π-manifolds.2"
PROBLEM FORMULATION,0.06287425149700598,"Each of these π-manifolds represents a single object under the transformation encoded by π; hence,
each of the points in a π-manifold is assigned the same class label. A perceptron endowed with a
set of linear readout weights w will attempt to determine the correct class of every point in every
manifold. The condition for realizing (i.e. linearly separating) the dichotomy {yµ}µ can be written
as yµw⊤π(g)rµ > 0 for all g ∈G and µ ∈[P], where yµ = +1 if the µth manifold belongs to the
ﬁrst class and yµ = −1 if the µth manifold belongs to the second class. The perceptron capacity is
the fraction of dichotomies that can be linearly separated; that is, separated by a hyperplane."
PROBLEM FORMULATION,0.0658682634730539,"For concreteness, one might imagine that each of the rµ is the neural representation for an image of
a dog (if yµ = +1) or of a cat (if yµ = −1). The action π(g) could, for instance, correspond to the
image shifting to the left or right, where the size of the shift is given by g. Different representations
of even the same group can have different coding properties, an important point for investigating
biological circuits and one that we leverage to construct a new GCNN architecture in Section 5."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.0688622754491018,"3
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS"
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.0718562874251497,"Here we ﬁrst state and prove our results in the general case of representations of compact groups over
RN. The reader is encouraged to read the proofs, as they are relatively simple and provide valuable
intuition. We consider applications to speciﬁc group representations and GCNNs in Sections 4 and
5 that follow."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.0748502994011976,"2Note that for a ﬁnite group G, each manifold will consist of a ﬁnite set of points. This violates the technical
mathematical deﬁnition of “manifold”, but we abuse the deﬁnition here for the sake of consistency with related
work (Chung et al., 2018; Cohen et al., 2019b); to be more mathematically precise, one could instead refer to
these “manifolds” as π-orbits."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.07784431137724551,Published as a conference paper at ICLR 2022
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.08083832335329341,"3.1
SEPARABILITY OF π-MANIFOLDS
We begin with a lemma which states that classifying the P π-manifolds can be reduced to the
problem of classifying their P centroids. For the rest of Section 3, we let π : G →GL(RN) be
an arbitrary linear representation of a compact group G.3 We denote the average of π over G with
respect to the Haar measure by ⟨π(g)⟩g∈G; for ﬁnite G this is simply
1
|G|
P"
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.08383233532934131,"g∈G π(g) where |G| is the
order (i.e. number of elements) of G. For ease of notation we will generally write ⟨π⟩≡⟨π(g)⟩g∈G
when the group G being averaged over is clear.
Lemma 1. A dataset {(π(g)rµ, yµ)}g∈G,µ∈[P ] consisting of P π-manifolds with labels yµ is lin-
early separable if and only if the dataset {(⟨π⟩rµ, yµ)}µ∈[P ] consisting of the P centroids ⟨π⟩rµ
with the same labels is linearly separable. Formally,"
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.08682634730538923,"∃w ∀g ∈G, µ ∈[P] : yµw⊤π(g)rµ > 0 ⇐⇒∃w ∀µ ∈[P] : yµw⊤⟨π⟩rµ > 0.
Proof. The forward implication is obvious: if there exists a w which linearly separates the P mani-
folds according to an assignment of labels yµ, that same w must necessarily separate the centroids
of these manifolds. This can be seen by averaging each of the quantities yµw⊤π(g)rµ over g ∈G.
Since each of these quantities is positive, the average must be positive."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.08982035928143713,"For the reverse implication, suppose yµw⊤⟨π⟩rµ > 0, and deﬁne ˜w = ⟨π⟩⊤w. We will show that
˜w separates the P π-manfolds since"
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.09281437125748503,yµ ˜w⊤π(g)rµ = yµw⊤⟨π⟩π(g)rµ (Deﬁnition of ˜w)
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.09580838323353294,= yµw⊤⟨π(g′)π(g)⟩g′∈Grµ (Deﬁnition of ⟨π⟩and linearity of π(g))
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.09880239520958084,"= yµw⊤⟨π⟩rµ (Invariance of the Haar Measure µ(Sg) = µ(S) for set S)
> 0
(Assumption that w separates centroids)
Thus, all that is required to show that ˜w separates the π-orbits are basic properties of a group
representation and invariance of the Haar measure to G-transformations."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.10179640718562874,"3.2
RELATIONSHIP WITH COVER’S THEOREM AND VC DIMENSION
The fraction f of linearly separable dichotomies on a dataset of size P for datapoints in general
position 4 in N dimensions was computed by Cover (1965) and takes the form"
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.10479041916167664,"f(P, N) = 21−P
N−1
X k=0"
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.10778443113772455,"P −1
k 
(1)"
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.11077844311377245,"where we take
  n
m

= 0 for m > n. Taking α = P/N, f is a nonincreasing sigmoidal func-
tion of α that takes the value 1 when α ≤1, 1/2 when α = 2, and approaches 0 as α →∞.
In the thermodynamic limit of P, N →∞and α = O(1), f(α) converges to a step function
¯f(α) = limP,N→∞,α=P/N f(P, N) = Θ(2 −α) (Gardner, 1987; 1988; Shcherbina & Tirozzi,
2003), indicating that in this limit all dichotomies are realized if α < 2 and no dichotomies can be
realized if α > 2, making αc = 2 a critical point in this limit."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.11377245508982035,"The VC dimension (Vapnik & Chervonenkis, 1968) of the perceptron trained on points in dimension
N is deﬁned to be the largest number of points P such that all dichotomies are linearly separable
for some choice of the P points. These points can be taken to be in general position.5 Because
f(P, N) = 1 precisely when P ≤N, the VC dimension is always N = P for any ﬁnite N and
P (see Abu-Mostafa et al. (2012)), even while the asymptotics of f reveal that f(P, N) approaches
1 at any N > P/2 as N becomes large. In this sense the perceptron capacity yields strictly more
information than the VC dimension, and becomes comparatively more descriptive of the expressivity
as N becomes large."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.11676646706586827,"In our G-invariant storage problem, the relevant scale is α = P/N0 where N0 is the dimension of
the ﬁxed point subspace (deﬁned below). We are now ready to state and prove our main theorem."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.11976047904191617,"3Note that our results extend to more general vector spaces than RN, under the condition that the group be
semi-simple.
4A set of P points is in general position in N-space if every subset of N or fewer points is linearly
independent. This says that the points are “generic” in the sense that there aren’t any prescribed special linear
relationships between them beyond lying in an N-dimensional space. Points drawn from a Gaussian distribution
with full-rank covariance are in general position with probability one.
5This is because linear dependencies decrease the number of separable dichotomies (see Hertz et al. (2018))."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.12275449101796407,Published as a conference paper at ICLR 2022
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.12574850299401197,"Theorem 1. Suppose the points ⟨π⟩rµ for µ ∈[P] lie in general position in the subspace V0 =
range(⟨π⟩) = {⟨π⟩x : x ∈V }. Then V0 is the ﬁxed point subspace of π, and the fraction of
linearly separable dichotomies on the P π-manifolds {π(g)rµ : g ∈G} is f(P, N0), where N0 =
dim V0. Equivalently, N0 is the number of trivial irreducible representations that appear in the
decomposition of π into irreducible representations."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.12874251497005987,"The ﬁxed point subspace is the subspace W = {w ∈V |gw = w ,∀g ∈G}. The theorem and its
proof use the notion of irreducible representations (irreps), which are in essence the fundamental
building blocks for general representations. Concretely, any representation of a compact group over
a real vector space decomposes into a direct sum of irreps (Naimark & Stern, 1982; Bump, 2004).
A representation π : G →GL(V ) is irreducible if V is not 0 and if no vector subspace of V is
stable under G, other than 0 and V (which are always stable under G). A subspace W being stable
(or invariant) under G means that π(g)w ∈W for all w ∈W and g ∈G. The condition that the
points ⟨π⟩rµ be in general position essentially means that there is no prescribed special relationship
between the rµ and between the rµ and ⟨π⟩. Taking the rµ to be drawn from a full-rank Gaussian
distribution is sufﬁcient to satisfy this condition."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.1317365269461078,"Proof. By the theorem of complete reducibility (see Fulton & Harris (2004)), π admits a decomposi-
tion into a direct sum of irreps π ∼= πk1⊕πk2⊕...⊕πkM acting on vector space V = V1⊕V2⊕...VM,
where ∼= denotes equality up to similarity transformation. The indices kj indicate the type of irrep
corresponding to invariant subspace Vj. The ﬁxed point subspace V0 is the direct sum of subspaces
where trivial kj = 0 irreps act: V0 = L
n:kn=0 Vn. By the Grand Orthogonality Theorem of irreps
(see Liboff (2004)) all non-trivial irreps average to zero ⟨πk,ij(g)⟩g∈G ∝δk,0δi,j. Then, the matrix
⟨π⟩simply projects the data to V0. By Lemma 1 the fraction of separable dichotomies on the π-
manifolds is the same as that of their centroids ⟨π⟩rµ. Since, by assumption, the P points ⟨π⟩rµ lie
in general position in V0, the fraction of separable dichotomies is f(P, dim V0) by Equation 1."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.1347305389221557,"Remark: A main idea used in the proof is that only nontrivial irreps average to zero. This will be
illustrated with examples in Section 4 below."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.1377245508982036,"Remark: For ﬁnite groups, N0 can be easily computed by averaging the trace of π, also known as
the character, over G: N0 = ⟨Tr(π(g))⟩g∈G = Tr(⟨π⟩) (see Serre (2014))."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.1407185628742515,"Remark: If the perceptron readout has a bias term b i.e. the output of the perceptron is w⊤π(g)r+b,
then this can be thought of as adding an invariant dimension. This is because the output can be
written ˜w⊤˜π(g)˜r where ˜w = (w, b), ˜r = (r, 1), and ˜π(g) = π(g) ⊕1 is π with an extra row
and column added with a 1 in the last position and zeros everywhere else. Hence the fraction of
separable dichotomies is f(P, N0 + 1)."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.1437125748502994,These results extend immediately to a notion of VC-dimension of group-invariant perceptrons.
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.1467065868263473,"Corollary 1. Let the VC dimension for G-invariant perceptrons with representation π, N π
V C,
denote the maximum number P, such that there exist P anchor points {rµ}P
µ=1 so that
{(π(g)rµ, yµ)}µ∈[P ],g∈G is separable for all possible binary dichotomies {yµ}µ∈[P ]. Then N π
V C =
dim(V0)."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.1497005988023952,"Proof. By theorem 1 and Equation 1, all possible dichotomies are realizable for P ≤dim(V0)
provided the points ⟨π⟩rµ are in general position."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.15269461077844312,"Our theory also extends immediately to subgroups. For example, strided convolutions are equivari-
ant to subgroups of the regular representation."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.15568862275449102,"Corollary 2. A solution to the the G-invariant classiﬁcation problem necessarily solves the G′-
invariant problem where G′ is a subgroup of G."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.15868263473053892,Proof. Assume that yµw⊤r(gxµ) > 0 for g ∈G. G′ ⊆G =⇒yµw⊤r(gxµ) > 0 ∀g ∈G′.
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.16167664670658682,"Consequently, the G′-invariant capacity is always higher than the capacity for the G-invariant clas-
siﬁcation problem."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.16467065868263472,Published as a conference paper at ICLR 2022
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.16766467065868262,"(a)
(b)
(c)"
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.17065868263473055,"Figure 1: π-manifolds for different π, illustrating that only the ﬁxed point subspace contributes to
capacity. In each panel two manifolds are plotted, with color denoting class label. Stars indicate the
random points rµ for µ ∈{1, 2} where the orbits begin, and closed circles denote the other points in
the π-manifold. For (a) and (b) the group being represented is G = Z4 and for (c) G = Z3. (a) Here
π(g) is the 2 × 2 rotation matrix R(2πg/4). The open blue circle denotes the ﬁxed point subspace
{0}. (b) Here π(g) is the 3 × 3 block-diagonal matrix with the ﬁrst 2 × 2 block being R(2πg/4)
and second 1 × 1 block being 1. The blue line denotes the ﬁxed point subspace span{(0, 0, 1)}. (c)
Here π(g) is the 3 × 3 matrix that cyclically shifts entries of length-3 vectors by g places. The blue
line denotes the ﬁxed point subspace span{(1, 1, 1)}."
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.17365269461077845,"4
EXAMPLE APPLICATION: THE CYCLIC GROUP Zm"
PERCEPTRON CAPACITY OF GROUP-GENERATED MANIFOLDS,0.17664670658682635,"In this section we illustrate the theory in the case of the cyclic group G = Zm on m elements. This
group is isomorphic to the group of integers {0, 1, ..., m −1} under addition modulo m, and this
is the form of the group that we will consider. An example of this group acting on an object is an
image that is shifted pixel-wise to the left and right, with periodic boundaries. In Appendix A.3 we
show an application of our theory to the non-abelian Lie group SO(3)."
ROTATION MATRICES,0.17964071856287425,"4.1
ROTATION MATRICES"
ROTATION MATRICES,0.18263473053892215,"The 2 × 2 discrete rotation matrices R(θg) ≡

cos(θg)
−sin(θg)
sin(θg)
cos(θg)"
ROTATION MATRICES,0.18562874251497005,"
where θg = 2πg/m and"
ROTATION MATRICES,0.18862275449101795,"g ∈Zm, are one possible representation of Zm; in this case V = R2. This representation is
irreducible and nontrivial, which implies that the dimension of the ﬁxed point subspace is 0 (only
the origin is mapped to itself by R for all g). Hence the fraction of linearly separable dichotomies of
the π-manifolds by Theorem 1 is f(P, 0). This result can be intuitively seen by plotting the orbits,
as in Figure 1a for m = 4. Here it is apparent that it is impossible to linearly separate two or more
manifolds with different class labels, and that the nontrivial irrep R averages to the zero matrix."
ROTATION MATRICES,0.19161676646706588,"The representation can be augmented by appending trivial irreps, deﬁning π : G →GL(RN) by"
ROTATION MATRICES,0.19461077844311378,"π(g) = R(θg) ⊕I ≡

R(θg)
0
0
I"
ROTATION MATRICES,0.19760479041916168,"
where I is an (N −2) × (N −2)-dimensional identity matrix."
ROTATION MATRICES,0.20059880239520958,"The number of trivial irreps is N −2, so that the capacity is f(P, N −2). This is illustrated in
Figure 1b for the case N = 3. Here we can also see that the trivial irrep, which acts on the subspace
span{(0, 0, 1)}, is the only irrep in the decomposition of π that does not average to zero. This ﬁgure
also makes intuitive the result of Lemma 1 that dichotomies are realizable on the π-manifolds if and
only if the dichotomies are realizable on the centroids of the manifolds."
THE REGULAR REPRESENTATION OF ZM,0.20359281437125748,"4.2
THE REGULAR REPRESENTATION OF Zm
Suppose π : Zm →GL(V ) is the representation of Zm consisting of the cyclic shift permutation
matrices (this is called the regular representation of Zm). In this case V = Rm and π(g) is the
matrix that cyclically shifts the entries of a length-m vector g places. For instance, if m = 3 and
v = (1, 2, 3) then π(2)v = (2, 3, 1)."
THE REGULAR REPRESENTATION OF ZM,0.20658682634730538,"In Appendix A.2 we derive the irreps of this representation, which consist of rotation matrices of
different frequencies. There is one copy of the trivial irrep π0(g) ≡1 corresponding with the ﬁxed
point subspace span{1m} where 1m is the length-m all-ones vector. Hence the fraction of separable"
THE REGULAR REPRESENTATION OF ZM,0.20958083832335328,Published as a conference paper at ICLR 2022
THE REGULAR REPRESENTATION OF ZM,0.2125748502994012,"dichotomies is f(P, 1). This is illustrated in Figure 1c in the case where m = 3. The average of the
regular representation matrix is ⟨π⟩=
1
|G|1m1⊤
m, indicating that ⟨π⟩projects data along 1m."
DIRECT SUMS OF REGULAR REPRESENTATIONS,0.2155688622754491,"4.3
DIRECT SUMS OF REGULAR REPRESENTATIONS
For our last example we deﬁne a representation using the isomorphism Zm ∼= Zm1 ⊕Zm2 for
m = m1m2 and m1 and m2 coprime6. Let π(1) : Zm1 →GL(Rm1) and π(2) : Zm2 →GL(Rm2)
be the cyclic shift representations (i.e. the regular representations) of Zm1 and Zm2, respectively.
Consider the representation π(1)⊕π(2) : Zm →GL(Rm1+m2) deﬁned by (π(1)⊕π(2))(g) ≡π(1)(g
mod m1)⊕π(2)(g mod m2), the block-diagonal matrix with π(1)(g mod m1) being the ﬁrst and
π(2)(g mod m2) the second block."
DIRECT SUMS OF REGULAR REPRESENTATIONS,0.218562874251497,"There are two copies of the trivial representation in the decomposition of π(1) ⊕π(2), correspond-
ing to the one-dimensional subspaces span{(1m1, 0m2)} and span{(0m1, 1m2)}, where 0m1 is the
length-k vector of all zeros. Hence the fraction of separable dichotomies is f(P, 2). This reasoning
extends simply to direct sums of arbitrary length ℓ, yielding a fraction of f(P, ℓ).7 These representa-
tions are used to build and test a novel G-convolutional layer architecture with higher capacity than
standard CNN layers in Section 5."
DIRECT SUMS OF REGULAR REPRESENTATIONS,0.2215568862275449,"These representations are analogous to certain formulations of grid cell representations as found in
entorhinal cortex of rats (Hafting et al., 2005), which have desirable qualities in comparison to place
cell representations (Sreenivasan & Fiete, 2011).8 Precisely, a collection {Zmk ×Zmk}k of grid cell
modules encodes a large 2-dimensional spatial domain Zm × Zm where m = Q k mk."
G-EQUIVARIANT NEURAL NETWORKS,0.2245508982035928,"5
G-EQUIVARIANT NEURAL NETWORKS"
G-EQUIVARIANT NEURAL NETWORKS,0.2275449101796407,"The proposed theory can shed light on the feature spaces induced by G-CNNs. Consider a single
convolutional layer feature map for a ﬁnite9 group G with the following activation"
G-EQUIVARIANT NEURAL NETWORKS,0.23053892215568864,"ai,k(x) = φ(w⊤
i g−1
k x) , gk ∈G , i ∈{1, ..., N}
(2)"
G-EQUIVARIANT NEURAL NETWORKS,0.23353293413173654,"for some nonlinear function φ. For each ﬁlter i, and under certain choices of π, the feature map
ai(x) ∈R|G| exhibits the equivariance property ai(gkx) = π(gk)ai(x). We will let a(x) ∈R|G|N
denote a ﬂattened vector for this feature map."
G-EQUIVARIANT NEURAL NETWORKS,0.23652694610778444,"In a traditional periodic convolutional layer applied to inputs of width W and length L, the feature
map of a single ﬁlter ai(x) ∈R|G| is equivariant with the regular representation of the group
G = ZW ×ZL (the representation that cyclically shifts the entries of W ×L matrices). Here the order
of the group is |G| = WL. Crucially, our theory shows that this representation contributes exactly
one trivial irrep per ﬁlter (see Appendix A.4.1). Since the dimension of the entire collection of N
feature maps a(x) is N|G| for ﬁnite groups G, one might naively expect capacity to be P ∼2N|G|
for large N from Gardner (1987). However, Theorem 1 shows that for G-invariant classiﬁcation,
only the trivial irreps contribute to the classiﬁer capacity. Since the number of trivial irreps present
in the representation is equal to the number of ﬁlters N, we have P ∼2N."
G-EQUIVARIANT NEURAL NETWORKS,0.23952095808383234,"We show in Figure 2 that our prediction for f(P, N) matches that empirically measured by training
logistic regression linear classiﬁers on the representation. We perform this experiment on both
(a) a random convolutional network and (b) VGG-11 (Simonyan & Zisserman, 2015) pretrained
on CIFAR-10 (Krizhevsky, 2009) by (liukuang, 2017). For these models we vary α by ﬁxing the
number of input samples and varying the number of output channels by simply removing channels
from the output tensor. The convolutions in these networks are modiﬁed to have periodic boundary
conditions while keeping the actual ﬁlters the same – see Appendix A.4.1 and Figure A.1 for more
information and the result of using non-periodic convolutions, which impact the capacity but not the
overall scaling with N0."
G-EQUIVARIANT NEURAL NETWORKS,0.24251497005988024,"6Two numbers are coprime if they have no common prime factor.
7Our results do not actually require that the mk be coprime, but rather that none of the mk divide one of
the others. To see this, take ˜m and ˜mk, to be m and the mk after being divided by all divisors common among
them. Then Z ˜
m ∼= ⊕ℓ
k=1Z ˜
mk and provided none of the ˜mk are 1, one still gets a fraction of f(P, ℓ).
8Place cells are analogous to standard convolutional layers.
9We consider ﬁnite groups here for simplicity, but the theory extends to compact Lie groups."
G-EQUIVARIANT NEURAL NETWORKS,0.24550898203592814,Published as a conference paper at ICLR 2022
G-EQUIVARIANT NEURAL NETWORKS,0.24850299401197604,"Other GCNN architectures can have different capacities. For instance, a convolutional layer equiv-
ariant to the direct sum representation of Section 4.3 has double the capacity with P ∼4N (Fig-
ure 2c), since each output channel contributes two trivial irreps. See Appendix A.4.4 for an explicit
construction of such a convolutional layer and a derivation of the capacity."
POOLING OPERATIONS,0.25149700598802394,"5.1
POOLING OPERATIONS
In convolutional networks, local pooling is typically applied to the feature maps which result from
each convolution layer. In this section, we describe how our theory can be adapted for codes which
contain such pooling operations. We will ﬁrst assume that π is an N-dimensional representation of
G. Let P(r) : RN →RN/k be a pooling operation which reduces the dimension of the feature map.
The condition that a given dichotomy {yµ} is linearly separable on a pooled code is"
POOLING OPERATIONS,0.25449101796407186,"∃w ∈RN/k ∀µ ∈[P], g ∈G : yµw⊤P(π(g)rµ) > 0
(3)"
POOLING OPERATIONS,0.25748502994011974,"We will ﬁrst analyze the capacity when P(·) is a linear function (average pooling) before studying
the more general case of local non-linear pooling on one and two-dimensional signals."
LOCAL AVERAGE POOLING,0.26047904191616766,"5.1.1
LOCAL AVERAGE POOLING"
LOCAL AVERAGE POOLING,0.2634730538922156,"In the case of average pooling, the pooling function P(·) is a linear map, represented with matrix
P which averages a collection of feature maps over local windows. Using an argument similar to
Lemma 1 (Lemma 2 and Theorem 2 in Appendix A.4.2), we prove that the capacity of a standard
CNN is not changed by local average pooling: for a network with N ﬁlters, local average pooling
preserves the one trivial dimension for each of the N ﬁlters. Consequently the fraction of separable
dichotomies is f(P, N)."
LOCAL NONLINEAR POOLING,0.26646706586826346,"5.1.2
LOCAL NONLINEAR POOLING"
LOCAL NONLINEAR POOLING,0.2694610778443114,"Often, nonlinear pooling operations are applied to downsample feature maps. For concreteness, we
will focus on one-dimensional signals in this section and relegate the proofs for two-dimensional
signals (images) to Appendix A.4.2. Let r(x) ∈RN×D represent a feature map with N ﬁlters and
length-D signals. Consider a pooling operation P(·) which maps the D pixels in each feature map
into new vectors of size D/k for some integer k. Note that the pooled code is equivariant to the
subgroup H = ZD/k, in the sense that P(π(h)r) = ρ(h)P(r) for any h ∈H. The representation
ρ is the regular representation of the subgroup H. We thus decompose G into cosets of size D/k:
g = jh, where j ∈Zk and h ∈ZD/k. The condition that a vector w separates the dataset is"
LOCAL NONLINEAR POOLING,0.27245508982035926,"∀µ ∈[P], j ∈Zk, h ∈H : yµw⊤ρ(h)P (π(j)rµ) > 0.
(4)"
LOCAL NONLINEAR POOLING,0.2754491017964072,"We see that there are effectively k points belonging to each of the P orbits in the pooled code. Since
P (·) is nonlinear, the averaging trick utilized in Lemma 1 is no longer available. However, we
can obtain a lower bound on the capacity, f(P, ⌊N/k⌋), from a simple extension of Cover’s original
proof technique as we show in Appendix A.4.3. Alternatively, an upper bound f ≤f(P, N) persists
since a w which satisﬁes Equation 4 must separate ⟨ρ(h)⟩h∈H P(rµ). This upper bound is tight
when all k points ⟨ρ(h)⟩h∈H P(π(j)rµ) coincide for each µ, giving capacity f(P, N). This is what
occurs in the average pooling case where the upper bound f ≤f(P, N) is tight. Further, if we
are only interested in the H-invariant capacity problem, the fraction of separable dichotomies is
f(P, N), since ρ is a regular representation of H as we show in 4."
GLOBAL POOLING,0.27844311377245506,"5.1.3
GLOBAL POOLING"
GLOBAL POOLING,0.281437125748503,"Lemma 1 shows that linear separability of π-manifolds is equivalent to linear separability after
global average pooling (i.e. averaging the representation over the group). This is relevant to CNNs,
which typically perform global average pooling following the ﬁnal convolutional layer, which is
typically justiﬁed by a desire to achieve translation invariance. This strategy is efﬁcient: Lemma 1
implies that it allows the learning of optimal linear readout weights following the pooling operation,
given only a single point from each π-manifold (i.e. no need for data augmentation). Global average
pooling reduces the problem of linearly classifying |G|P points in a space of dimension N to that
of linearly classifying P points in a space of dimension dim V0. Note that by an argument similar to
Section 5.1.2, global max pooling may in contrast reduce capacity."
GLOBAL POOLING,0.2844311377245509,Published as a conference paper at ICLR 2022
GLOBAL POOLING,0.2874251497005988,"1
2
3
4
α = P/N0 0.0 0.2 0.4 0.6 0.8 1.0"
GLOBAL POOLING,0.2904191616766467,capacity
GLOBAL POOLING,0.2934131736526946,"0.5
1.0
1.5
2.0
2.5
α = P/N0 0.0 0.2 0.4 0.6 0.8 1.0 f(α) layer 2
3
6"
GLOBAL POOLING,0.2964071856287425,"0.5
1.0
1.5
2.0
2.5
3.0
α = P/N0 0.0 0.2 0.4 0.6 0.8 1.0 f(α) layer 1
2"
GLOBAL POOLING,0.2994011976047904,"(a)
(b)"
GLOBAL POOLING,0.3023952095808383,"grid cell conv
+ ReLU"
GLOBAL POOLING,0.30538922155688625,"grid cell conv
+ ReLU theory"
GLOBAL POOLING,0.3083832335329341,"(c)
conv + ReLU"
GLOBAL POOLING,0.31137724550898205,"conv + ReLU
+ maxpool"
GLOBAL POOLING,0.3143712574850299,"conv + ReLU
theory"
GLOBAL POOLING,0.31736526946107785,conv block 1
GLOBAL POOLING,0.3203592814371258,"conv block 1
+ maxpool"
GLOBAL POOLING,0.32335329341317365,"conv block 1
theory"
GLOBAL POOLING,0.3263473053892216,conv block 2
GLOBAL POOLING,0.32934131736526945,"Figure 2: Capacity of GCNN representations. Solid lines denote the empirically measured fraction
f(α) of 100 random dichotomies for which a logistic regression classiﬁer ﬁnds a separating hyper-
plane, where α = P/N0. Dotted lines denote theoretical predictions. Shaded regions depict 95%
conﬁdence intervals over random choice of inputs, as well as network weights in (a) and (c). (a)
f(α) of a random periodic convolutional layer after ReLU (blue line) and followed by 2x2 max pool
(orange line), with P = 40 and N0 = # output channels. Max pooling reduces capacity by a factor
between 1/4 and 1 as predicted by our theory. (b) f(α) of VGG-11 pretrained on CIFAR-10 after a
periodic convolution, batchnorm, and ReLU (blue line), followed by a 2x2 maxpool (orange line),
and then another set of convolution, batchnorm, and ReLU (green line), with P = 20 and N0 = #
output channels. Max pooling reduces capacity as predicted. (c) f(α) after a random convolutional
layer equivariant to the direct sum representation of Z10 ⊕Z8 as deﬁned in Section 4.3, with P = 16
and N0 = 2 (# output channels)."
"INDUCED REPRESENTATIONS
INDUCED REPRESENTATIONS ARE A FUNDAMENTAL INGREDIENT IN THE CONSTRUCTION OF GENERAL EQUIVARIANT NEU-",0.3323353293413174,"5.2
INDUCED REPRESENTATIONS
Induced representations are a fundamental ingredient in the construction of general equivariant neu-
ral network architectures (Cohen et al., 2019b). Here we state our result, and relegate a formal
deﬁnition of induced representations and the proof of the result to Appendix A.4.5."
"INDUCED REPRESENTATIONS
INDUCED REPRESENTATIONS ARE A FUNDAMENTAL INGREDIENT IN THE CONSTRUCTION OF GENERAL EQUIVARIANT NEU-",0.33532934131736525,"Proposition 1. Let π be a representation of a ﬁnite group induced from ρ. Then the fraction of
separable dichotomies of π-manifolds is equal to that of the ρ-manifolds."
DISCUSSION AND CONCLUSION,0.3383233532934132,"6
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.3413173652694611,"Equivariance has emerged as a powerful framework to build and understand representations that re-
ﬂect the structure of the world in useful ways. In this work we take the natural step of quantifying
the expressivity of these representations through the well-established formalism of perceptron ca-
pacity. We ﬁnd that the number of “degrees of freedom” available for solving the classiﬁcation task
is the dimension of the space that is ﬁxed by the group action. This has the immediate implication
that capacity scales with the number of output channels in standard CNN layers, a fact we illustrate
in simulations. However, our results are also very general, extending to virtually any equivariant
representation of practical interest – in particular, they are immediately applicable to GCNNs built
around more general equivariance relations, as we illustrate with an example of a GCNN equivariant
to direct sum representations. We also calculate the capacity of induced representations, a standard
tool in building GCNNs, and show how local and global pooling operations inﬂuence capacity. The
measures of expressivity explored here could prove valuable for ensuring that models with general
equivariance relations have high enough capacity to support the tasks being learned, either by using
more ﬁlters or by using representations with intrinsically higher capacity. We leave this to future
work."
DISCUSSION AND CONCLUSION,0.344311377245509,"While the concept of perceptron capacity has played an essential role in the development of machine
learning systems (Sch¨olkopf & Smola, 2002; Cohen et al., 2020) and the understanding of biolog-
ical circuit computations (Brunel et al., 2004; Chapeton et al., 2012; Rigotti et al., 2013; Brunel,
2016; Rubin et al., 2017; Pehlevan & Sengupta, 2017; Lanore et al., 2021; Froudarakis et al., 2020),
more work is wanting in linking it to other computational attributes of interest such as generaliza-
tion. A comprehensive picture of the computational attributes of equivariant or otherwise structured
representations of artiﬁcial and biological learning systems will likely combine multiple measures,
including perceptron capacity. There is much opportunity, and indeed already signiﬁcant recent
work (e.g. Sokolic et al. (2017))."
DISCUSSION AND CONCLUSION,0.3473053892215569,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.3502994011976048,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.3532934131736527,"Code
for
the
simulations
can
be
found
at
https://github.com/msf235/
group-invariant-perceptron-capacity.
This code includes an environment.yml
ﬁle that can be used to create a python environment identical to the one used by the authors. The
code generates all of the plots in the paper."
REPRODUCIBILITY STATEMENT,0.3562874251497006,"While the main text is self-contained with the essential proofs, proofs of additional results and
further discussion can be found in the Appendices below. These include"
REPRODUCIBILITY STATEMENT,0.3592814371257485,Appendix A.1 a glossary of deﬁnitions and notation.
REPRODUCIBILITY STATEMENT,0.36227544910179643,Appendix A.2 a derivation of the irreps for the regular representation of the cyclic group Zm.
REPRODUCIBILITY STATEMENT,0.3652694610778443,Appendix A.3 a description of the irreps for the group SO(3) and the resulting capacity.
REPRODUCIBILITY STATEMENT,0.36826347305389223,"Appendix A.4 a description of the construction of the GCNNs used in the paper, the methods for
empirically measuring the fraction of linearly separable dichotomies, a description of local
pooling, and complete formal proofs of the fraction of linearly separable dichotomies for
these network representations (including with local pooling). This appendix also includes
more description of the induced representation and a formal proof of the fraction of linearly
separable dichotomies."
REPRODUCIBILITY STATEMENT,0.3712574850299401,"Appendix A.4.1 also contains an additional ﬁgure, Figure A.1."
REPRODUCIBILITY STATEMENT,0.37425149700598803,ACKNOWLEDGEMENTS
REPRODUCIBILITY STATEMENT,0.3772455089820359,"MF and CP are supported by the Harvard Data Science Initiative. MF thanks the Swartz Foundation
for support. BB acknowledges the support of the NSF-Simons Center for Mathematical and Statis-
tical Analysis of Biology at Harvard (award #1764269) and the Harvard Q-Bio Initiative. ST was
partially supported by the NSF under grant No. DMS-1439786."
REPRODUCIBILITY STATEMENT,0.38023952095808383,Published as a conference paper at ICLR 2022
REFERENCES,0.38323353293413176,REFERENCES
REFERENCES,0.38622754491017963,"Yaser S. Abu-Mostafa. Hints and the VC Dimension. Neural Computation, 5(2):278–288, 03 1993.
ISSN 0899-7667. doi: 10.1162/neco.1993.5.2.278. URL https://doi.org/10.1162/
neco.1993.5.2.278."
REFERENCES,0.38922155688622756,"Yaser S Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data, volume 4.
AMLBook New York, NY, USA:, 2012."
REFERENCES,0.39221556886227543,"Brandon Anderson, Truong Son Hy, and Risi Kondor.
Cormorant: Covariant molecular neural
networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlch´e-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,
2019."
REFERENCES,0.39520958083832336,"Minkyung Baek, Frank Dimaio, Ivan V. Anishchenko, Justas Dauparas, Sergey Ovchinnikov,
Gyu Rie Lee, Jue Wang, Qian Cong, Lisa N. Kinch, R. Dustin Schaeffer, Claudia Mill´an, Hahn-
beom Park, Carson Adams, Caleb R. Glassman, Andy M. DeGiovanni, Jose H. Pereira, An-
dria V. Rodrigues, Alberdina Aike van Dijk, Ana C Ebrecht, Diederik Johannes Opperman, Theo
Sagmeister, Christoph Buhlheller, Tea Pavkov-Keller, Manoj K. Rathinaswamy, Udit Dalwadi,
Calvin K. Yip, John E. Burke, K. Christopher Garcia, Nick V. Grishin, Paul D. Adams, Randy J.
Read, and David Baker. Accurate prediction of protein structures and interactions using a three-
track neural network. Science, 373:871 – 876, 2021."
REFERENCES,0.39820359281437123,"Eric B. Baum. On the capabilities of multilayer perceptrons. J. Complex., 4:193–215, 1988."
REFERENCES,0.40119760479041916,"Erik J. Bekkers, Maxime W. Lafarge, Mitko Veta, Koen A. J. Eppenhof, Josien P. W. Pluim, and
Remco Duits.
Roto-translation covariant convolutional networks for medical image analysis.
ArXiv, abs/1804.03393, 2018."
REFERENCES,0.4041916167664671,"Alexander Bogatskiy, Brandon Anderson, Marwah Roussi, David Miller, and Risi Kondor. Lorentz
group equivariant neural network for particle physics. In International Conference on Machine
Learning, pp. 992–1002. PMLR, 2020."
REFERENCES,0.40718562874251496,"Nicolas Brunel, Vincent Hakim, Philippe Isope, Jean-Pierre Nadal, and Boris Barbour. Optimal
information storage and the distribution of synaptic weights: perceptron versus purkinje cell.
Neuron, 43(5):745–757, 2004."
REFERENCES,0.4101796407185629,"Nicolas J.-B. Brunel. Is cortical connectivity optimized for storing information?
Nature Neuro-
science, 19:749–755, 2016."
REFERENCES,0.41317365269461076,"Daniel Bump. Lie groups, volume 8. Springer, New York, 2004. ISBN 9781475740943."
REFERENCES,0.4161676646706587,"Julio Chapeton, Tarec Fares, Darin LaSota, and Armen Stepanyants. Efﬁcient associative memory
storage in cortical circuits of inhibitory and excitatory neurons.
Proceedings of the National
Academy of Sciences, 109:E3614 – E3622, 2012."
REFERENCES,0.41916167664670656,"Shuxiao Chen, Edgar Dobriban, and Jane Lee.
A group-theoretic framework for data augmen-
tation.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21321–21333. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
f4573fc71c731d5c362f0d7860945b88-Paper.pdf."
REFERENCES,0.4221556886227545,"SueYeon Chung, Daniel D. Lee, and Haim Sompolinsky. Classiﬁcation and Geometry of General
Perceptual Manifolds. Physical Review X, 8(3):031003, July 2018. doi: 10.1103/PhysRevX.8.
031003."
REFERENCES,0.4251497005988024,"Taco Cohen and Max Welling. Group equivariant convolutional networks. In ICML, 2016."
REFERENCES,0.4281437125748503,"Taco Cohen, Mario Geiger, Jonas K¨ohler, and Max Welling. Spherical cnns. ArXiv, abs/1801.10130,
2018."
REFERENCES,0.4311377245508982,"Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolu-
tional networks and the icosahedral cnn. In ICML, 2019a."
REFERENCES,0.4341317365269461,Published as a conference paper at ICLR 2022
REFERENCES,0.437125748502994,"Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant CNNs on ho-
mogeneous spaces. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019b."
REFERENCES,0.44011976047904194,"Uri Cohen, SueYeon Chung, Daniel D. Lee, and Haim Sompolinsky. Separability and geometry of
object manifolds in deep neural networks. Nature Communications, 11(1):746, February 2020.
ISSN 2041-1723. doi: 10.1038/s41467-020-14578-5."
REFERENCES,0.4431137724550898,"T. Cover. Geometrical and statistical properties of systems of linear inequalities with applications in
pattern recognition. IEEE Trans. Electron. Comput., 14:326–334, 1965."
REFERENCES,0.44610778443113774,"John Denker, W. Gardner, Hans Graf, Donnie Henderson, R. Howard, W. Hubbard, L. D. Jackel,
Henry Baird, and Isabelle Guyon. Neural network recognizer for hand-written zip code digits.
In D. Touretzky (ed.), Advances in Neural Information Processing Systems, volume 1. Morgan-
Kaufmann, 1989.
URL https://proceedings.neurips.cc/paper/1988/file/
a97da629b098b75c294dffdc3e463904-Paper.pdf."
REFERENCES,0.4491017964071856,"James J DiCarlo and David D Cox. Untangling invariant object recognition. Trends in cognitive
sciences, 11(8):333–341, 2007."
REFERENCES,0.45209580838323354,"Alexander S Ecker, Fabian H Sinz, Emmanouil Froudarakis, Paul G Fahey, Santiago A Cadena,
Edgar Y Walker, Erick Cobos, Jacob Reimer, Andreas S Tolias, and Matthias Bethge.
A
rotation-equivariant convolutional neural network model of primary visual cortex. arXiv preprint
arXiv:1809.10504, 2018."
REFERENCES,0.4550898203592814,"Stephan Eismann, Raphael J. L. Townshend, Nathaniel Thomas, Milind Jagota, Bowen Jing, and
Ron O. Dror. Hierarchical, rotation-equivariant neural networks to select structural models of
protein complexes. Proteins: Structure, 89:493 – 501, 2020."
REFERENCES,0.45808383233532934,"Bryn Elesedy and Sheheryar Zaidi. Provably strict generalisation beneﬁt for equivariant models.
In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of
Machine Learning Research, pp. 2959–2969. PMLR, 2021. URL http://proceedings.
mlr.press/v139/elesedy21a.html."
REFERENCES,0.46107784431137727,"Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning so(3)
equivariant representations with spherical cnns. In ECCV, 2018."
REFERENCES,0.46407185628742514,"Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convo-
lutional neural networks for equivariance to lie groups on arbitrary continuous data. In ICML,
2020."
REFERENCES,0.46706586826347307,"Marc Finzi, Max Welling, and Andrew Gordon Wilson. A practical method for constructing equiv-
ariant multilayer perceptrons for arbitrary matrix groups. ArXiv, abs/2104.09459, 2021."
REFERENCES,0.47005988023952094,"Emmanouil Froudarakis, Uri Cohen, Maria Diamantaki, Edgar Y Walker, Jacob Reimer, Philipp
Berens, Haim Sompolinsky, and Andreas S Tolias. Object manifold geometry across the mouse
cortical visual hierarchy. bioRxiv, 2020."
REFERENCES,0.47305389221556887,"William Fulton and Joe Harris. Representation Theory: A First Course. Readings in Mathematics.
Springer-Verlag New York, New York, 2004. ISBN 9780387974958."
REFERENCES,0.47604790419161674,"E. Gardner. Maximum storage capacity in neural networks. EPL, 4:481–485, 1987."
REFERENCES,0.47904191616766467,"E. Gardner. The space of interactions in neural network models. Journal of Physics A, 21:257–270,
1988."
REFERENCES,0.4820359281437126,"Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. Permutation equivari-
ant models for compositional generalization in language. In ICLR, 2020."
REFERENCES,0.48502994011976047,"Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, and Edvard I. Moser. Microstruc-
ture of a spatial map in the entorhinal cortex. Nature, 436(7052):801–806, August 2005. ISSN
0028-0836, 1476-4687. doi: 10.1038/nature03721."
REFERENCES,0.4880239520958084,Published as a conference paper at ICLR 2022
REFERENCES,0.49101796407185627,"Jason S. Hartford, Devon R. Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models
of interactions across sets. In ICML, 2018."
REFERENCES,0.4940119760479042,"John Hertz, Anders Krogh, and Richard G Palmer. Introduction to the theory of neural computation.
CRC Press, 2018."
REFERENCES,0.49700598802395207,"Guangbin Huang. Learning capability and storage capacity of two-hidden-layer feedforward net-
works. IEEE transactions on neural networks, 14 2:274–81, 2003."
REFERENCES,0.5,"Nicolas Keriven and Gabriel Peyr´e.
Universal invariant and equivariant graph neural net-
works.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
ea9268cb43f55d1d12380fb6ea5bf572-Paper.pdf."
REFERENCES,0.5029940119760479,"Johannes Klicpera, Janek Groß, and Stephan G¨unnemann. Directional message passing for molec-
ular graphs. ArXiv, abs/2003.03123, 2020."
REFERENCES,0.5059880239520959,"Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural
networks to the action of compact groups. In ICML, 2018."
REFERENCES,0.5089820359281437,"Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch-gordan nets: a fully fourier space spherical
convolutional neural network. In NeurIPS, 2018a."
REFERENCES,0.5119760479041916,"Risi Kondor, Hy Truong Son, Horace Pan, Brandon M. Anderson, and Shubhendu Trivedi. Covariant
compositional networks for learning graphs. ArXiv, abs/1801.02144, 2018b."
REFERENCES,0.5149700598802395,"A. Kowalczyk. Estimates of storage capacity of multilayer perceptron with threshold logic hidden
units. Neural networks : the ofﬁcial journal of the International Neural Network Society, 10 8:
1417–1433, 1997."
REFERENCES,0.5179640718562875,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.5209580838323353,"Frederic Lanore, N. Alex Cayco-Gajic, Harsha Gurnani, Diccon Coyle, and R. Angus Silver. Cere-
bellar granule cell axons support high-dimensional representations. Nature Neuroscience, June
2021. ISSN 1097-6256, 1546-1726. doi: 10.1038/s41593-021-00873-x."
REFERENCES,0.5239520958083832,"Yann Andr´e LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard,
Wayne E. Hubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code
recognition. Neural Computation, 1:541–551, 1989."
REFERENCES,0.5269461077844312,"Richard L. Liboff. Primer for Point and Space Groups. Undergraduate Texts in Contemporary
Physics. Springer, New York, 2004. ISBN 9780387402482."
REFERENCES,0.5299401197604791,"liukuang. pytorch-cifar. https://github.com/kuangliu/pytorch-cifar, 2017."
REFERENCES,0.5329341317365269,"Clare Lyle, Mark van der Wilk, Marta Z. Kwiatkowska, Yarin Gal, and Benjamin Bloem-Reddy. On
the beneﬁts of invariance in neural networks. ArXiv, abs/2005.00178, 2020."
REFERENCES,0.5359281437125748,"George W. Mackey.
Induced Representations of Locally Compact Groups and Applications,
pp. 132–166.
Springer Berlin Heidelberg, Berlin, Heidelberg, 1970.
ISBN 978-3-642-
48272-4.
doi:
10.1007/978-3-642-48272-4 6.
URL https://doi.org/10.1007/
978-3-642-48272-4_6."
REFERENCES,0.5389221556886228,"Diego Marcos, Benjamin Kellenberger, Sylvain Lobry, and Devis Tuia. Scale equivariance in cnns
with vector ﬁelds. ArXiv, abs/1807.11783, 2018."
REFERENCES,0.5419161676646707,"Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. ArXiv, abs/1812.09902, 2019a."
REFERENCES,0.5449101796407185,"Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invari-
ant networks. In Proceedings of the 36th International Conference on Machine Learning, vol-
ume 97, pp. 4363–4371. PMLR, 2019b. URL https://proceedings.mlr.press/v97/
maron19a.html."
REFERENCES,0.5479041916167665,Published as a conference paper at ICLR 2022
REFERENCES,0.5508982035928144,"Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements.
ArXiv, abs/2002.08599, 2020."
REFERENCES,0.5538922155688623,"Mark Aronovich Naimark and Aleksandr Isaakovich Stern. Theory of group representations, volume
246. Springer, New York, 1982. ISBN 9781461381440."
REFERENCES,0.5568862275449101,"Mauro Pastore, Pietro Rotondo, Vittorio Erba, and Marco Gherardi. Statistical learning theory of
structured data. Physical Review E: Statistical Physics, Plasmas, Fluids, and Related Interdisci-
plinary Topics, 102(3):032119, September 2020. doi: 10.1103/PhysRevE.102.032119."
REFERENCES,0.5598802395209581,"Cengiz Pehlevan and Anirvan M. Sengupta.
Resource-efﬁcient perceptron has sparse synaptic
weight distribution. 2017 25th Signal Processing and Communications Applications Conference
(SIU), pp. 1–4, 2017."
REFERENCES,0.562874251497006,"Nathanael Perraudin, Micha¨el Defferrard, Tomasz Kacprzak, and Raphael Sgier. Deepsphere: Efﬁ-
cient spherical convolutional neural network with healpix sampling for cosmological applications.
Astron. Comput., 27:130–146, 2019."
REFERENCES,0.5658682634730539,"Siamak Ravanbakhsh. Universal equivariant multilayer perceptrons. In Hal Daum´e III and Aarti
Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research, pp. 7996–8006. PMLR, 13–18 Jul 2020.
URL https://proceedings.mlr.press/v119/ravanbakhsh20a.html."
REFERENCES,0.5688622754491018,"Mattia Rigotti, Omri Barak, Melissa R Warden, Xiao-Jing Wang, Nathaniel D Daw, Earl K Miller,
and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks. Nature, 497
(7451):585–590, 2013."
REFERENCES,0.5718562874251497,"Pietro Rotondo, Marco Cosentino Lagomarsino, and Marco Gherardi. Counting the learnable func-
tions of geometrically structured data. Physical Review Research, 2(2):023169, May 2020. doi:
10.1103/PhysRevResearch.2.023169."
REFERENCES,0.5748502994011976,"Ran Rubin, L. F. Abbott, and Haim Sompolinsky. Balanced excitation and inhibition are required
for high-capacity, noise-robust neuronal selectivity.
Proceedings of the National Academy of
Sciences, 114:E9366 – E9375, 2017."
REFERENCES,0.5778443113772455,"Akiyoshi Sannai, M. Imaizumi, and M. Kawano. Improved generalization bounds of group invariant
/ equivariant deep networks via quotient feature spaces. 2019a."
REFERENCES,0.5808383233532934,"Akiyoshi Sannai, Yuuki Takai, and Matthieu Cordonnier. Universal approximations of permutation
invariant/equivariant functions by deep neural networks. ArXiv, abs/1903.01939, 2019b."
REFERENCES,0.5838323353293413,"Victor Garcia Satorras, E. Hoogeboom, F. Fuchs, I. Posner, and M. Welling. E(n) equivariant nor-
malizing ﬂows for molecule generation in 3d. ArXiv, abs/2105.09016, 2021."
REFERENCES,0.5868263473053892,"Ludwig Schl¨aﬂi. Theorie der vielfachen Kontinuit¨at, pp. 167–387. Springer Basel, Basel, 1950.
ISBN 978-3-0348-4118-4. doi: 10.1007/978-3-0348-4118-4 13. URL https://doi.org/
10.1007/978-3-0348-4118-4_13."
REFERENCES,0.5898203592814372,"Bernhard Sch¨olkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. Adaptive Computation and Machine Learning. MIT
Press, 2002. ISBN 978-0-262-19475-4."
REFERENCES,0.592814371257485,"Nimrod Segol and Yaron Lipman. On universal equivariant set networks. ArXiv, abs/1910.02421,
2020."
REFERENCES,0.5958083832335329,"Jean-Pierre Serre.
Linear Representations of Finite Groups.
Graduate Texts in Mathematics.
Springer, New York, 2014. ISBN 978-1-4684-9460-0 978-1-4684-9458-7."
REFERENCES,0.5988023952095808,"H Sebastian Seung and Daniel D Lee. The manifold ways of perception. science, 290(5500):2268–
2269, 2000."
REFERENCES,0.6017964071856288,"John Shawe-Taylor. Threshold network learning in the presence of equivalences. In NIPS, 1991."
REFERENCES,0.6047904191616766,Published as a conference paper at ICLR 2022
REFERENCES,0.6077844311377245,"Mariya Shcherbina and Brunello Tirozzi.
Rigorous Solution of the Gardner Problem.
Com-
munications in Mathematical Physics, 234(3):383–422, March 2003. ISSN 0010-3616, 1432-
0916. doi: 10.1007/s00220-002-0783-3. URL http://link.springer.com/10.1007/
s00220-002-0783-3."
REFERENCES,0.6107784431137725,"N. Shutty and Casimir Wierzynski. Learning irreducible representations of noncommutative lie
groups. ArXiv, abs/2006.00724, 2020."
REFERENCES,0.6137724550898204,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2015."
REFERENCES,0.6167664670658682,"Bart Smets, Jim Portegies, Erik J. Bekkers, and Remco Duits. Pde-based group equivariant convo-
lutional neural networks. ArXiv, abs/2001.09046, 2020."
REFERENCES,0.6197604790419161,"Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization Error of In-
variant Classiﬁers. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International
Conference on Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of Machine Learn-
ing Research, pp. 1094–1103. PMLR, 20–22 Apr 2017. URL https://proceedings.mlr.
press/v54/sokolic17a.html."
REFERENCES,0.6227544910179641,"Eduardo Sontag. Shattering all sets of k points in general position requires (k 1)/2 parameters.
Neural Computation, 9:337–348, 1997."
REFERENCES,0.625748502994012,"I. Sosnovik, A. Moskalev, and A. Smeulders. Scale equivariance improves siamese tracking. 2021
IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 2764–2773, 2021."
REFERENCES,0.6287425149700598,"Ivan Sosnovik, Michal Szmaja, and Arnold W. M. Smeulders. Scale-equivariant steerable networks.
ArXiv, abs/1910.11093, 2020."
REFERENCES,0.6317365269461078,"Sameet Sreenivasan and Ila Fiete. Grid cells generate an analog error-correcting code for singularly
precise neural computation. Nature Neuroscience, 14(10):1330–1337, October 2011. ISSN 1097-
6256, 1546-1726. doi: 10.1038/nn.2901."
REFERENCES,0.6347305389221557,"Raphael J. L. Townshend, Stephan Eismann, Andrew M. Watkins, Ramya Rangan, Maria Karelina,
Rhiju Das, and Ron O. Dror. Geometric deep learning of rna structure. Science, 373:1047 – 1051,
2021."
REFERENCES,0.6377245508982036,"Vladimir N. Vapnik and Alexey Y. Chervonenkis. On the uniform convergence of relative frequen-
cies of events to their probabilities. Dokl. Akad. Nauk., 181(4), 1968."
REFERENCES,0.6407185628742516,"Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv-
ariant cnns for digital pathology. ArXiv, abs/1806.03962, 2018."
REFERENCES,0.6437125748502994,"Roman Vershynin. Memory capacity of neural networks with threshold and rectiﬁed linear unit
activations. SIAM J. Math. Data Sci., 2:1004–1033, 2020."
REFERENCES,0.6467065868263473,"Maurice Weiler and Gabriele Cesa. General e(2)-equivariant steerable cnns. ArXiv, abs/1911.08251,
2019."
REFERENCES,0.6497005988023952,"Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3d steerable cnns:
Learning rotationally equivariant features in volumetric data. In NeurIPS, 2018a."
REFERENCES,0.6526946107784432,"Maurice Weiler, Fred A. Hamprecht, and Martin Storath. Learning steerable ﬁlters for rotation
equivariant cnns. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
849–858, 2018b."
REFERENCES,0.655688622754491,"Maurice Weiler, Patrick Forr´e, Erik P. Verlinde, and Max Welling. Coordinate independent convolu-
tional networks - isometry and gauge equivariant convolutions on riemannian manifolds. ArXiv,
abs/2106.06020, 2021."
REFERENCES,0.6586826347305389,"J. G. Wendel. A problem in geometric probability. Mathematica Scandinavica, 11:109–112, 1962."
REFERENCES,0.6616766467065869,"Eugene Wigner. Gruppentheorie Und Ihre Anwendung Auf Die Quantenmechanik Der Atomspek-
tren. Vieweg+Teubner Verlag, Wiesbaden, ﬁrst edition, 1931. ISBN 978-3-663-00642-8."
REFERENCES,0.6646706586826348,Published as a conference paper at ICLR 2022
REFERENCES,0.6676646706586826,"Marysia Winkels and Taco Cohen. Pulmonary nodule detection in ct scans with equivariant cnns.
Medical image analysis, 55:15–26, 2019."
REFERENCES,0.6706586826347305,"Daniel E. Worrall and Gabriel J. Brostow. Cubenet: Equivariance to 3d rotation and translation. In
ECCV, 2018."
REFERENCES,0.6736526946107785,"Daniel E. Worrall and Max Welling.
Deep scale-spaces:
Equivariance over scale.
ArXiv,
abs/1905.11697, 2019."
REFERENCES,0.6766467065868264,"Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow. Harmonic
networks: Deep translation and rotation equivariance. 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 7168–7177, 2017."
REFERENCES,0.6796407185628742,"Dmitry Yarotsky.
Universal approximations of invariant maps by neural networks.
ArXiv,
abs/1804.10306, 2018."
REFERENCES,0.6826347305389222,"Chulhee Yun, S. Sra, and A. Jadbabaie. Small relu networks are powerful memorizers: a tight
analysis of memorization capacity. In NeurIPS, 2019."
REFERENCES,0.6856287425149701,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab´as P´oczos, Ruslan Salakhutdinov, and
Alex Smola. Deep sets. In NIPS, 2017."
REFERENCES,0.688622754491018,Published as a conference paper at ICLR 2022
REFERENCES,0.6916167664670658,"A
APPENDIX"
REFERENCES,0.6946107784431138,"A.1
NOTATION AND GLOSSARY"
REFERENCES,0.6976047904191617,• x: an abstract notation for an input object
REFERENCES,0.7005988023952096,• r(x): a feature map of the input to an N dimensional vector space.
REFERENCES,0.7035928143712575,"• π: N dimensional linear representation of group G. For each g ∈G, π(g) ∈GL(RN) is
an N × N invertible real matrix."
REFERENCES,0.7065868263473054,• Equivariance property: r(gx) = π(g)r(x) for all g ∈G and all x.
REFERENCES,0.7095808383233533,"• Invariant measure: a measure µ : G →R+ on G with µ(gS) = µ(S) = µ(Sg). For
ﬁnite groups, the uniform distribution. For locally compact topological groups, the Haar
measure."
REFERENCES,0.7125748502994012,• ⟨·⟩g∈G: an average over the invariant measure of G
REFERENCES,0.7155688622754491,"• Irreducible representation (irrep): an irreducible representation ρ on vector space V satis-
ﬁes ρ(g)v ∈V for all v ∈V, g ∈G."
REFERENCES,0.718562874251497,• Character χ(g): the trace of the representation χ(g) = Tr π(g).
REFERENCES,0.7215568862275449,• Fixed point subspace: the subspace V0 for which π(g)v ∈V0 for all v ∈V0.
REFERENCES,0.7245508982035929,"• General position: a collection of P points in general position in an N dimensional vector
space have the property that any subset of k ≤N points are linearly independent. These
points are generic in the sense that they satisfy no more linear relationships than they must."
REFERENCES,0.7275449101796407,• Dichotomy: a particular binary labeling {yµ} of P points {xµ}.
REFERENCES,0.7305389221556886,"• f(P, N): fraction of linearly separable dichotomies given by Cover’s function counting
theorem (Cover, 1965)."
REFERENCES,0.7335329341317365,"• VC dimension: the largest possible integer P such that there exist P points where all
possible dichotomies {yµ} can be realized by the model Abu-Mostafa et al. (2012)."
REFERENCES,0.7365269461077845,"• Capacity: the largest possible ratio αc = P/N where P points in general position can be
linearly separated by a N dimensional perceptron with probability 1 in an asymptotic limit
where P, N →∞with P/N = ON,P (1). The classical result is αc = 2 (Gardner, 1987;
Shcherbina & Tirozzi, 2003)."
REFERENCES,0.7395209580838323,• P(·): a local pooling operation.
REFERENCES,0.7425149700598802,"A.2
IRREPS FOR THE CYCLIC GROUP"
REFERENCES,0.7455089820359282,"Here we compute the irreps of representations π : Zm →GL(V ) of the cyclic group Zm over
a real vector space V (see (Serre, 2014) for a derivation of the irreps when V is a complex vec-
tor space). To ﬁnd the irreps, one can use the form for the eigenvalues and eigenvectors for cir-
culant matrices, since all the π(g) are circulant. This results in the simultaneous diagonalization
π(g) = V (1 ⊕R(2πg/m) ⊕R(4πg/m) ⊕· · · ⊕R((m −1)πg/m)) V ⊤where V is the real-
valued version of the discrete Fourier transform matrix (the columns are proportional to cosines and
sines of varying frequencies, along with a column proportional to 1m)."
REFERENCES,0.7485029940119761,"Note that the 2x2 rotation matrices R(2πgk/m) are irreps for k ̸= m/2 and k ̸= 0, since there
is no one-dimensional subspace of R2 that is invariant to R(2πgk/m) for all g. The exception
for k = m/2 if m is even, gives R(2πgk/m) = (−1)gI, which corresponds to rotation of 180
degrees. The subspace span{(1, 0)} is invariant to this action, so that R(2πgk/m) is not an irrep.
This representation can thus be reduced to an action on a one-dimensional subspace, represented by
(−1)g. The case k = 0 gives the trivial representation."
REFERENCES,0.7514970059880239,"A.3
SO(3): A NON-ABELIAN LIE GROUP"
REFERENCES,0.7544910179640718,"The special orthogonal group SO(3) on 3 dimensions (rotation group), the 3 × 3 orthogonal ma-
trices with determinant +1, can also be analyzed within our theory. G-convolutional neural net-
works that are equivariant to SO(3) rotations have become of high interest in the physical sciences
and computer vision where the objects of interest often respect these symmetries (Anderson et al.,"
REFERENCES,0.7574850299401198,Published as a conference paper at ICLR 2022
REFERENCES,0.7604790419161677,"2019; Cohen et al., 2018; Esteves et al., 2018; Kondor et al., 2018a) The irreducible representations
have the form Bkm where Bkm are (2km + 1) × (2km + 1) block matrices, known as Wigner D-
matrices (Wigner, 1931). The trivial irreps correspond to the one-dimensional irreps with k = 0.
Thus, the SO(3)-invariant classiﬁcation capacity merely counts the number of trivial irreps which
have N0 = P"
REFERENCES,0.7634730538922155,"m δkm,0. The capacity is again f(P, N0)."
REFERENCES,0.7664670658682635,"A.4
G-EQUIVARIANT CONVOLUTIONAL LAYERS"
REFERENCES,0.7694610778443114,"A.4.1
STANDARD CONVOLUTIONAL LAYERS"
REFERENCES,0.7724550898203593,"A convolutional layer consists of a set of N k × k′ ﬁlters Fi that are convolved (technically cross-
correlated) with a stack of M W × L input tensors. Here M is the number of input channels and
N the number of output channels. The convolution runs each ﬁlter (i.e. takes the dot product at
all possible positions) over each of the W × L input tensors, and the result is averaged across the
M input channels to produce the output of one output channel. In the positions where the ﬁlters
approach the edges of the input tensor, different choices can be made about how to handle these
edge conditions. The standard choice is to pad the edges with some number of zeros depending on
the desired shape of the output tensor and run the convolution out to the end of the padded image.
Another possible choice is to loop the edges of the input tensor together, so that the ﬁlter is applied to
the other side of the input tensor as it runs off the edge. This periodic boundary condition allows us
to write the convolution formally in terms of group actions, and to apply our theory directly. When
convolutions are not periodic, the resulting capacity increases somewhat but still follows the P/N0
scaling of the periodic convolutions (Figure A.1)."
REFERENCES,0.7754491017964071,"For the random convolutional layers of Figure 2a, the input tensors are size 10 × 10 and the number
of input channels are M = 3, as for standard color images. Each entry of these tensors is normally
distributed with mean 0. The ﬁlters are also of size 10 × 10 with periodic boundary conditions,
and are initialized according to a normal Xavier distribution with parameters that are the default for
Pytorch 1.9. The convolution is implemented via the Pytorch 1.9 implementation of Conv2d with
padding mode=“circular” and padding=0 in the case of periodic boundary conditions. The bias term
of the convolution is set to zero and the convolution is followed by a ReLU nonlinearity (blue line).
The resulting ﬁgures do not change appreciably for different choices of input tensor size, number of
input channels, or size of ﬁlters (though note that the nonlinearity is essential for satisfying the gen-
eral position condition of Theorem 1; otherwise, the capacity would be determined by the number
of input channels rather than number of output channels). The output of this convolution is then fed
through a 2×2 max pooling layer (orange line in Figure 2a), provided by Pytorch 1.9’s MaxPool2d."
REFERENCES,0.7784431137724551,"The pretrained VGG-11 layers used in Figure 2b and Figure A.1 are taken from liukuang (2017).
The ﬁrst convolutional block (blue line) consists of 3 × 3 pretrained ﬁlters applied to CIFAR-10
image tensors randomly selected from the validation set and normalized in the same way they are
normalized during training (see liukuang (2017) for details). These images are of size 32 × 32 and
have M = 3 input channels, followed by a batch normalization layer in evaluation mode (ﬁxed
parameters), and then followed by a ReLU nonlinearity. The boundary conditions of these convolu-
tions are set to be periodic in Figure 2b and nonperiodic with a zero padding sizes of 1 in Figure A.1,
and the bias term is set to zero. The batch normalization is an element-wise operation and so equiv-
ariant to the representations we consider – thus this operation is not expected and is not observed
to affect the perceptron capacity. This convolutional block is then followed by a 2 × 2 max pooling
layer (orange line). Finally, another convolutional block of 3 × 3 ﬁlters, batch normalization, and
ReLU nonlinearity are applied (green line)."
REFERENCES,0.781437125748503,"The fraction of linearly separable dichotomies is measured empirically by using the scikit-learn
LogisticRegression implementation of logistic regression, with a tolerance value of tol=1e-18 and
an inverse regularization value of C=1e8. The maximum number of iterations is set to 500. An
intercept (i.e. bias) is not used for this ﬁt."
REFERENCES,0.7844311377245509,"To formally prove that the fraction of separable dichotomies is f(P, N) for standard periodic con-
volutional layers, ﬁrst note that the convolution is equivariant with respect to cyclic permutation of
the inputs and of the outputs. The representation for cyclically permutating the output tensor can
be written LN
k=1 π where π is the representation that cyclically permutes the entries of W × L
matrices. Since each copy of π contains one trivial irrep in its decomposition into a direct sum of"
REFERENCES,0.7874251497005988,Published as a conference paper at ICLR 2022
REFERENCES,0.7904191616766467,"0.5
1.0
1.5
2.0
2.5
α = P/N0 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7934131736526946,capacity
REFERENCES,0.7964071856287425,conv block 1
REFERENCES,0.7994011976047904,"conv block 1
+ maxpool"
REFERENCES,0.8023952095808383,"conv block 1
theory"
REFERENCES,0.8053892215568862,conv block 2
REFERENCES,0.8083832335329342,"Figure A.1: The fraction of realizable dichotomies of non-periodic convolutional layers is higher
than periodic convolutional layers, but still obeys the same scaling. Details are exactly as in Fig-
ure 2b, but using non-periodic convolutions with a zero padding of size 1. Here the theory line refers
to the theory for periodic convolutions."
REFERENCES,0.811377245508982,"irreps, the direct sum LN
k=1 π contains N trivial irreps in its decomposition. The ﬁnal step to use
Theorem 1 is to argue that the centroids of the manifolds are in general position. Since a nonlinearity
(ReLU) is applied to the output of the convolution, and since there is no particular structure in the
convolutional ﬁlters beyond possibly sparsity, we can generically expect this to be the case."
REFERENCES,0.8143712574850299,"A.4.2
LOCAL POOLING"
REFERENCES,0.8173652694610778,"First we prove an extension of Lemma 1 to equivariant linear maps. This will be used to show that
average pooling does not affect the capacity of the regular representation of Zm.
Lemma 2. Let π be a representation of the group G and suppose the matrix P is equivariant with
respect to the restriction of π to a subgroup H ⊆G, so that for all h ∈H P π(h) = ρ(h)P for some
representation ρ of H. Let R denote a set of representatives of G/H. Then we have the following
equivalence."
REFERENCES,0.8203592814371258,"∃w ∀µ ∈[P], g ∈G : yµw⊤P π(g)rµ > 0"
REFERENCES,0.8233532934131736,⇐⇒∃w ∀µ ∈[P] ∀g′ ∈R : yµw⊤P ⟨π(h)⟩h∈Hπ(g′)rµ > 0.
REFERENCES,0.8263473053892215,"Proof. For the forward implication, we write the coset decomposition g = hg′ of g and average
over H to ﬁnd"
REFERENCES,0.8293413173652695,"∀g ∈G : yµw⊤P π(g)rµ > 0 ⇐⇒∀h ∈H, g′ ∈R : yµw⊤P π(h)π(g′)rµ > 0"
REFERENCES,0.8323353293413174,=⇒∀g′ ∈R : yµw⊤P ⟨π(h)⟩h∈Hπ(g′)rµ > 0.
REFERENCES,0.8353293413173652,"For the backward implication, suppose yµw⊤P ⟨π(h)⟩h∈Hπ(g′)rµ > 0 for all representatives g′ ∈
R, and deﬁne ˜w = ⟨ρ(h)⟩⊤
h∈Hw. For any g ∈G, take a coset decomposition g = hg′ for h ∈H
and g′ ∈R. We then have"
REFERENCES,0.8383233532934131,yµ ˜w⊤P π(g)rµ = yµ ˜w⊤P π(h)π(g′)rµ (Coset decomposition)
REFERENCES,0.8413173652694611,= yµ ˜w⊤ρ(h)P π(g′)rµ (P is H-equivariant)
REFERENCES,0.844311377245509,= yµw⊤⟨ρ(h′)⟩h′∈H ρ(h)P π(g′)rµ (Deﬁnition of ˜w)
REFERENCES,0.8473053892215568,= yµw⊤⟨ρ(h)⟩h∈H P π(g′)rµ (Invariance of measure)
REFERENCES,0.8502994011976048,= yµw⊤P ⟨π(h)⟩h∈H π(g′)rµ (P is linear and H-equivariant)
REFERENCES,0.8532934131736527,"> 0 (By assumption).
(5)"
REFERENCES,0.8562874251497006,The implication follows.
REFERENCES,0.8592814371257484,Published as a conference paper at ICLR 2022
REFERENCES,0.8622754491017964,"Lemma 3. For the regular representation of G = ZD, a local average pooling over windows of
size k generates a matrix P which is equivariant with respect to the subgroup H = ZD/k with the
property that"
REFERENCES,0.8652694610778443,"P ⟨π(h)⟩h∈H = aP ⟨π(g)⟩g∈G
(6)"
REFERENCES,0.8682634730538922,where a > 0 is a positive constant.
REFERENCES,0.8712574850299402,"Proof. First, note that the new pooled code is the regular representation of H since shifts of size
mk in the original feature map corresponds to shifts of length m in the pooled code. Thus P is
equivariant to H = ZD/k. Next we note the following two facts"
REFERENCES,0.874251497005988,"P ⟨π(h)⟩h∈H = a′1D/k1⊤
D
(7)"
REFERENCES,0.8772455089820359,"P ⟨π(h)⟩g∈G = a′′1D/k1⊤
D
(8)"
REFERENCES,0.8802395209580839,"a′ and a′′ are positive constants and 1D and 1D/k are the D and D/k dimensional vector of all ones,
respectively. Thus, we have that P ⟨π(h)⟩h∈H = aP ⟨π(g)⟩g∈G where a is a positive constant."
REFERENCES,0.8832335329341318,"Theorem 2. The fraction of linearly separable dichotomies of a CNN pooling layer with N ﬁlters
after average pooling from feature maps with the size of the input image W × L to pooled feature
maps of size W/k × L/k is f(P, N), i.e. no capacity is lost due to local average pooling."
REFERENCES,0.8862275449101796,"Proof. The CNN layer before pooling is a regular representation π of the full group G = ZW ×ZL,
applied to each of the M input channels via a direct a sum LM
j=1 π. The layer after pooling is a
regular representation ρ of the subgroup H = ZW/k × ZL/k, also applied to the output channels via
a direct sum LN
j=1 ρ. Let R be a set of representatives of G/H. Since P is equivariant to π and ρ
over H we have by the previous two lemmas that"
REFERENCES,0.8892215568862275,∀g ∈G : yµw⊤P π(g)rµ > 0 ⇐⇒∀g′ ∈R : yµw⊤P ⟨π(h)⟩h∈Hπ(g′)rµ > 0
REFERENCES,0.8922155688622755,⇐⇒∀g′ ∈R : yµw⊤P ⟨π(g)⟩g∈Gπ(g′)rµ > 0
REFERENCES,0.8952095808383234,⇐⇒yµw⊤P ⟨π(g)⟩g∈Grµ > 0
REFERENCES,0.8982035928143712,⇐⇒yµw⊤P ⟨π(h)⟩h∈Hrµ > 0
REFERENCES,0.9011976047904192,⇐⇒yµw⊤⟨ρ(h)⟩h∈HP rµ > 0
REFERENCES,0.9041916167664671,"Thus the capacity is determined by the rank of ⟨ρ(h)⟩h∈H, assuming the ⟨ρ(h)⟩h∈HP rµ are
in general position.
Since each ρ is a copy of the regular representation for H, the rank of
⟨LN
j=1 ρ(h)⟩h∈H is merely N. Thus the fraction of linearly separable dichotomies is f(P, N),
the same as the capacity before pooling."
REFERENCES,0.907185628742515,"Now we prove local pooling operations in a standard CNN preserve a regular representation of a
subgroup of the cyclic group."
REFERENCES,0.9101796407185628,"Lemma 4. Suppose P is a local pooling operation on two-dimensional signals (CNN feature maps),
and that π is the regular representation of a group G = ZW × ZL on code a(x). A pooled feature
map r = P(a) which acts on k × k windows of a is a regular representation of the subgroup
H = ZW/k × ZL/k."
REFERENCES,0.9131736526946108,"Proof. Suppose an equivariant feature map a(x) ∈RW ×L×N has corresponding regular represen-
tation of the group G = ZW × ZL for each of the N ﬁlters. Consider any local pooling operation
P (·) (such as average or maximum) which acts on k × k patches where k divides both W and L."
REFERENCES,0.9161676646706587,"rij,h(x) = P ({ai′,j′,h(x) | i′ ∈[ik, (i + 1)k], j′ ∈[jk, (j + 1)k]})
(9)"
REFERENCES,0.9191616766467066,"Note that for k > 1, r(x) is no longer equivariant to G since the representation does not satisfy
the homomorphism property for shifts with length ℓnot divisible by k. However, the new code is
equivariant to a subgroup H = ZW/k × ZL/k, namely vertical and horizontal shifts with length
divisible by k. Let πx
nk,mk represent a vertical shift of the image x by nk pixels and horizontal shift"
REFERENCES,0.9221556886227545,Published as a conference paper at ICLR 2022
REFERENCES,0.9251497005988024,"by mk pixels. Note that aij,h(πx
nk,mkx) = ai+nk,j+mk(x) since a(x) is equivariant. Then, the
h-th pooled feature map transforms as"
REFERENCES,0.9281437125748503,"rij,h(πx
nk,mkx) = P
 
{ai′,j′,h(πx
nk,mkx) | i′ ∈[ik, (i + 1)k], j′ ∈[jk, (j + 1)k]}
"
REFERENCES,0.9311377245508982,"= P ({ai′+nk,j′+mk,h(x) | i′ ∈[ik, (i + 1)k], j′ ∈[jk, (j + 1)k]})
(a is Equivariant to πx)"
REFERENCES,0.9341317365269461,"= P ({ai′,j′,h(x) | i′ ∈[(n + i)k, (n + i + 1)k], j′ ∈[(m + j)k, (m + j + 1)k]}) ,
(k divides W, H)
= ri+n,j+m,h(x) , (Deﬁnition of r)"
REFERENCES,0.937125748502994,"We thus ﬁnd that the code is a regular representation of the subgroup of the cyclic translations
H = {(nk, mk)}n∈[H/k],m∈[W/k]. This new group G′ has dimension |H| =
1
k2 |G|."
REFERENCES,0.9401197604790419,"A.4.3
LOWER BOUND AND UPPER BOUND ON CAPACITY FOR NONLINEAR POOLING"
REFERENCES,0.9431137724550899,"Theorem 3. Suppose a code which is equivariant to a ﬁnite group G is pooled to a new code which
is equivariant to a ﬁnite subgroup H ⊆G. Suppose the number of trivial dimensions in the original
G-equivariant code is N0. Then the fraction of linearly separable dichotomies on the G-invariant
problem for the pooled code is at least f(P, ⌊N0/k⌋) where k = |G/H|. Similarly the fraction is at
most f(P, N0)."
REFERENCES,0.9461077844311377,"Proof. The pooled code, by assumption, has the property P(π(hg′)r) = ρ(h)P (π(g′)r) for any
h ∈H and g′ ∈R, where R is a set of representatives of G/H. The G-invariant separability
condition amounts to the proposition"
REFERENCES,0.9491017964071856,"∃w∀µ ∈[P] ∀h ∈H, g′ ∈R : yµw⊤ρ(h)P(π(g′)rµ) > 0
(10)"
REFERENCES,0.9520958083832335,"⇐⇒∃w∀µ ∈[P] ∀g′ ∈R : yµw⊤⟨ρ(h)⟩h∈H P(π(g′)rµ) > 0;
(11)"
REFERENCES,0.9550898203592815,"in other words, a solution on the right hand side affords a solution over all of the manifolds generated
in the input space. We see that, this requires considering if this particular dichotomy is linearly
separble on the Pk anchor points ⟨ρ(h)⟩h∈H P(π(g′)rµ). The simplest strategy to obtain an upper
bound is to consider what happens when a single new manifold is added. We see that when a
single new base point r is added it corresponds to k new points in the orbit ⟨ρ⟩P(π(g′)r) for all
g′ ∈R. Suppose that ⟨ρ(h)⟩h∈H has rank N0. Let C(P, N0) represent the number of linearly
separable dichotomies for P G-orbits in N0 trivial dimensions. Upon the addition of the k new
points (P →P + 1), we ﬁnd that some of the pre-existing separable dichotomies give a new
separable dichotomy. This can be guaranteed to occur when a w separates the old dichotomy and
has w⊤⟨ρ⟩· P(π(g′)r) = 0 for the new anchor point r (but this condition is not necessary for a
new dichotomy to be separable). This condition means that the original dichotomy is separable in
the N0 −k dimensional subspace {w : w · P(π(g′)r) = 0}. By making inﬁnitesimal adjustment
to this w the correct label on this new orbit can be achieved without altering the labels on any other
dichotomy. Since this argument gives a sufﬁcient but not necessary condition to generate a new
dichotomy, we obtain the following inequality"
REFERENCES,0.9580838323353293,"C(P + 1, N0) ≥C(P, N0) + C(P, N0 −k).
(12)"
REFERENCES,0.9610778443113772,"Solving this recursion gives the capacity fP ooled(P, N0) ≥fCover(P, ⌊N0/k⌋). The greatest ca-
pacity occurs in the special case where ⟨ρ⟩P(π(g′)r) = ⟨ρ⟩P(r). In this case, the usual counting
theorem applies giving a fraction of separable dichotomies of f(P, N0). This is achieved, for in-
stance in average pooling as we showed in Theorem 2."
REFERENCES,0.9640718562874252,"A.4.4
DIRECT SUM EQUIVARIANT CONVOLUTIONAL LAYERS"
REFERENCES,0.9670658682634731,"Here we describe how to build a convolutional layer architecture that is equivariant with respect
to the regular representation in the input space and the direct sum representations introduced in
Section 4.3 in the output space. For the following we assume that m1 and m2 are coprime, though
see the footnote in Section 4.3 for a discussion of how to loosen this requirement."
REFERENCES,0.9700598802395209,"The input data is a W × L × M tensor where M is the number of input channels. The ﬁrst step
is to simply take the output of a standard convolution (in our simulations we also apply a ReLU"
REFERENCES,0.9730538922155688,Published as a conference paper at ICLR 2022
REFERENCES,0.9760479041916168,"nonlinearity) applied to this input with periodic boundary conditions, resulting in a W ×L×N tensor
where N is the number of output channels. The next step is to, for each of the output channels, take
an average (or maximum) between entries spaced m1 entries apart horizontally or vertically in the
matrix, resulting in an m1 ×m1 matrix. In our simulations we took averages rather than maximums.
This is repeated for the other number m2, resulting in an m2 ×m2 matrix. This is repeated for every
output channel, resulting in N matrices of size m1 × m1 and N matrices of size m2 × m2. Finally,
the resulting matrices are ﬂattened and appended into an (m2
1 + m2
2) × N matrix, and the result is
passed through a nonlinearity (ReLU)."
REFERENCES,0.9790419161676647,"As the input tensor is cyclically permuted according to a regular representation π of Zm1m2, the
output of this equivariant convolutional layer permutes according to the representation π(1) ⊕π(2)"
REFERENCES,0.9820359281437125,where π(1) is the regular representation of Zm1 and π(2) is the regular representation of Zm2.
REFERENCES,0.9850299401197605,"The proof that the fraction of separable dichotomies is given by f(P, 2N) follows the same proof as
for the standard periodic convolutions in Appendix A.4.1. Instead of a direct sum LN
k=1 π we get a
direct sum LN
k=1(π(1)⊕π(2)). Each of the π(1)⊕π(2) contain two trivial irreps in its decomposition,
so that the ﬁnal fraction is f(P, 2N)."
REFERENCES,0.9880239520958084,"A.4.5
INDUCED REPRESENTATIONS"
REFERENCES,0.9910179640718563,"First we state the deﬁnition of an induced representation. Let H be a subgroup of a ﬁnite group
G and let ρ : H →GL(W) be a representation of H. Let V be the vector space of functions
f : G →W such that f(gh) = ρ(h)f(g) for all h ∈H and g ∈G. We now deﬁne the induced
representation π : G →GL(V ) to be the representation which satisﬁes (π(g′)f)(g) = f(gg′)."
REFERENCES,0.9940119760479041,"For intuition, note that every element of g can be written g = rh where r is a representative for a
coset in G/H and h ∈H. This is because the cosets G/H partition G and the action of H stays
within a coset; hence r selects out the coset, and h goes to the desired element of the coset: g = rh.
With this decomposition, the action of π is then π(rh)f(g) = ρ(h)f(gr). Hence the r component
under π has the effect of permuting to the new coset that gr belongs in, and the h component under
π then has the effect ρ(h) on the resulting vector f(gr) that we originally speciﬁed. This is the most
natural way to get a representation of G from a representation of H. In the case of ﬁnite groups, one
can think of the r component as permuting a set of isomorphic copies of V , each copy corresponding
to a different coset."
REFERENCES,0.9970059880239521,"To compute the capacity of induced representations, and so prove Proposition 1, we use Frobenius
reciprocity of characters. Recall that the character θ of a representation π : G →GL(V ) is the map
θ : G →C induced by the trace: θ(g) = Tr(π(g)). Now let θ be the character of ρ : H →GL(V )
and let θG be the character of the induced representation. Then ⟨θG(g)⟩g∈G = ⟨θ(h)⟩h∈H by
Frobenius reciprocity of characters (Mackey, 1970). The average of the character is the number
of trivial representations contained in the decomposition of the representation (see Serre (2014)).
Hence the capacity of the induced representation is equal to the capacity of ρ. The existence of
extensions beyond ﬁnite groups is not clear to the authors, but we welcome information if such
exists."
