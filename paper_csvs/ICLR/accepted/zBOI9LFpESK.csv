Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003676470588235294,"How to learn an effective reinforcement learning-based model for control tasks
from high-level visual observations is a practical and challenging problem. A key to
solving this problem is to learn low-dimensional state representations from observa-
tions, from which an effective policy can be learned. In order to boost the learning
of state encoding, recent works are focused on capturing behavioral similarities
between state representations or applying data augmentation on visual observations.
In this paper, we propose a novel meta-learner-based framework for representation
learning regarding behavioral similarities for reinforcement learning. Specifically,
our framework encodes the high-dimensional observations into two decomposed
embeddings regarding reward and dynamics in a Markov Decision Process (MDP).
A pair of meta-learners are developed, one of which quantifies the reward similarity
and the other quantifies dynamics similarity over the correspondingly decomposed
embeddings. The meta-learners are self-learned to update the state embeddings by
approximating two disjoint terms in on-policy bisimulation metric. To incorporate
the reward and dynamics terms, we further develop a strategy to adaptively balance
their impacts based on different tasks or environments. We empirically demonstrate
that our proposed framework outperforms state-of-the-art baselines on several
benchmarks, including conventional DM Control Suite, Distracting DM Control
Suite and a self-driving task CARLA."
INTRODUCTION,0.007352941176470588,"1
INTRODUCTION"
INTRODUCTION,0.011029411764705883,"Designing effective reinforcement learning algorithms for learning to control from high-dimensional
visual observations is crucial and has attracted more and more attention (Yarats et al., 2021; Laskin
et al., 2020a; Schwarzer et al., 2021; Lesort et al., 2018). To learn a policy efficiently from high-
dimensional observations, prior approaches first learn an encoder to map high-dimensional obser-
vations, e.g., images, to low-dimensional representations, and subsequently train a policy from
low-dimensional representations to actions based on various RL algorithms. Therefore, how to learn
low-dimensional representations, which are able to provide semantic abstraction for high-dimensional
observations, plays a key role."
INTRODUCTION,0.014705882352941176,"Early works on deep reinforcement learning train encoders based on various reconstruction
losses (Lange & Riedmiller, 2010; Watter et al., 2015; Wahlstr¨om et al., 2015; Higgins et al.,
2017), which aim to enforce the learned low-dimensional representations to reconstruct the original
high-dimensional observations after decoding. Promising results have been achieved in some ap-
plication domains, such as playing video games, simulated tasks, etc. However, the policy learned
on state representations trained with a reconstruction loss may not generalize well to complex en-
vironments, which are even though semantically similar to the source environment. The reason
is that a reconstruction loss is computed over all the pixels, which results in all the details of the
high-dimensional images tending to be preserved in the low-dimensional representations. However,
some of the observed details, such as complex background objects in an image, are task-irrelevant
and highly environment-dependent. Encoding such details in representation learning makes the
learned representation less effective for a downstream reinforcement learning task of interest and less
generalizable to new environments. To address the aforementioned issue, some data augmentation"
INTRODUCTION,0.01838235294117647,Published as a conference paper at ICLR 2022
INTRODUCTION,0.022058823529411766,"techniques have been proposed to make the learned representations more robust (Yarats et al., 2021;
Lee et al., 2020b; Laskin et al., 2020b). However, these approaches rarely consider the properties of a
Markov Decision Process (MDP), such as conditional transition probabilities between states given an
action, in the representation learning procedure."
INTRODUCTION,0.025735294117647058,"Recent research (Zhang et al., 2021; Agarwal et al., 2021; Castro, 2020; Ferns et al., 2004) has
shown that the bisimulation metric and its variants are potentially effective to be exploited to learn
a more generalizable reinforcement learning agent across semantically similar environments. The
bisimulation metric measures the “behavioral similarity” between two states based on two terms: 1)
reward difference that considers the difference in immediate task-specific reward signals between two
states, and 2) dynamics difference that considers the similarity of the long-term behaviors between
two states. A general idea of these approaches is to learn an encoder to map observations to a latent
space such that the distance or similarity between two states in the latent space approximates their
bisimulation metric. In this way, the learned representations (in the latent space) are task-relevant
and invariant to environmental details. However, manually specifying a form of distance, e.g., the L1
norm as used in (Zhang et al., 2021), in the latent space may limit the approximation precision for the
bisimulation metric and potentially discard some state information that is useful for policy learning.
Moreover, existing approaches rarely explore how to learn an adaptive combination of reward and
dynamics differences in the bisimulation metric, which may vary in different tasks or environments."
INTRODUCTION,0.029411764705882353,"We propose a novel framework for learning generalizable state representations for RL, named Adap-
tive Meta-learner of Behavioral Similarities (AMBS). In this framework, we design a network with
two encoders that map the high-dimensional observations to two decomposed representations regard-
ing rewards and dynamics. For the purpose of learning behavioral similarity on state representations,
we introduce a pair of meta-learners that learn similarities in order to measure the reward and the
dynamics similarity between two states over the corresponding decomposed state representations,
respectively. The meta-learners are self-learned by approximating the reward difference and the
dynamics difference in the bisimulation metric. Then the meta-learners update the state representa-
tions according to their behavioral distance to the other state representations. Previous approaches
with a hand-crafted form of distance/similarity evaluating state encoding in the L1 space are diffi-
cult to minimize the approximation error for the bisimulation metric, which may lead to important
side information being discarded, e.g., information regarding to Q-value but not relevant to states
distance/similarity. Instead, our learned similarities measure two states representations via a neural ar-
chitecture, where side information can be preserved for policy learning. Our experiments also showed
that a smaller approximation loss for similarity learning can be obtained by using the meta-learners.
This demonstrates that the proposed meta-learners can overcome the approximation precision issue
introduced by the L1 distance in previous approaches and provide more stable gradients for robust
learning of state representations for deep RL. Moreover, we explore the impact between the reward
and the dynamics terms in the bisimulation metric. We propose a learning-based adaptive strategy to
balance the effect between reward and dynamics in different tasks or environments by introducing a
learnable importance parameter, which is jointly learned with the state-action value function. Finally,
we use a simple but effective data augmentation strategy to accelerate the RL procedure and learn
more robust state representations."
INTRODUCTION,0.03308823529411765,"The main contributions of our work are 3-fold: 1) we propose a meta-learner-based framework to
learn task-relevant and environment-details-invariant state representations; 2) we propose a network
architecture that decomposes each state into two different types of representations for measuring the
similarities in terms of reward and dynamics, respectively, and design a learnable adaptive strategy
to balance them to estimate the bisimulation metric between states; 3) we verify our proposed
framework on extensive experiments and demonstrate new state-of-the-art results on background-
distraction DeepMind control suite (Tassa et al., 2018; Zhang et al., 2018; Stone et al., 2021) and
other visual-based RL tasks."
RELATED WORK,0.03676470588235294,"2
RELATED WORK"
RELATED WORK,0.04044117647058824,"Representation learning for reinforcement learning from pixels
Various existing deep RL
methods have been proposed to address the sample-efficiency and the generalization problems for
conventional RL from pixel observation. In end-to-end deep RL, neural networks learn representations
implicitly by optimizing some RL objective (Mnih et al., 2015; Espeholt et al., 2018). Watter et al."
RELATED WORK,0.04411764705882353,Published as a conference paper at ICLR 2022
RELATED WORK,0.04779411764705882,"(2015) and Wahlstr¨om et al. (2015) proposed to learn an encoder with training a dynamics model
jointly to produce observation representations. Lee et al. (2020a), Hafner et al. (2019), Hafner et al.
(2020) and Zhang et al. (2019) aimed to learn an environment dynamics model with the reconstruction
loss to compact pixels to latent representations. Gelada et al. (2019) proposed to learn representations
by predicting the dynamics model along with the reward, and analyzed its theoretical connection to
the bisimulation metric. Agarwal et al. (2021) proposed to learn representations by state distances
based on policy distribution. Kemertas & Aumentado-Armstrong (2021) modified the learning of
bisimulation metric with intrinsic rewards and inverse dynamics regularization. Castro et al. (2021)
replaced the Wasserstein distance in bisimulation metric with a parametrized metric."
RELATED WORK,0.051470588235294115,"Data Augmentation in reinforcement learning
Laskin et al. (2020b) and Ye et al. (2020b) explored
various data augmentation techniques for deep RL, which are limited to transformations on input
images. Cobbe et al. (2019) applied data augmentation to domain transfer. Ye et al. (2020a) studied
data augmentation on game environments for zero-shot generalization. RandConv (Lee et al., 2020b)
proposed a randomized convolutional neural network to generate randomized observations in order to
perform data augmentation. A recent work DrQ (Yarats et al., 2021) performs random crop on image
observations and provides regularized formulation for updating Q-function. Raileanu et al. (2021)
proposed to use data augmentation for regularizing on-policy actor-critic RL objectives. Pitis et al.
(2020) proposed a counterfactual data augmentation technique by swapping observed trajectory pairs."
PRELIMINARIES,0.05514705882352941,"3
PRELIMINARIES"
PRELIMINARIES,0.058823529411764705,"We define an environment as a Markov Decision Process (MDP) described by a tuple M =
(S, A, P, R, γ), where S is the high-dimensional state space (e.g., images), A is the action space,
P(s′|s, a) is the transition dynamics model that captures the probability of transitioning to next
state s′ ∈S given current state s ∈S and action a ∈A, R is the reward function yielding
a reward signal r = R(s, a) ∈R, and γ ∈[0, 1) is the discounting factor. The goal of rein-
forcement learning is to learn a policy π(a|s) that maximizes the expected cumulative rewards:
maxπ E[P"
PRELIMINARIES,0.0625,"t γtr(st, at)|at ∼π(·|st), s′
t ∼P(s′|s, a)]. In the scope of this paper, we do not consider
partial observability, and use stacked consecutive frames as the fully observed states."
PRELIMINARIES,0.0661764705882353,"Soft Actor-Critic (SAC)
is a widely used off-policy model-free reinforcement learning al-
gorithm (Haarnoja et al., 2018).
SAC aims to maximize the entropy objective (Ziebart,
2010) which is the reinforcement learning objective augmented with an entropy term J(π) =
P"
PRELIMINARIES,0.06985294117647059,"t E [r(st, at) + αH (π(·|st))]. In order to maximize the objective, SAC learns a state-value func-
tion Qθ(s, a), a stochastic policy πψ(a|s) and the temperature α, where θ and ψ are the parameters of
Qθ and πψ, respectively. The Q-function is trained by minimizing the squared soft Bellman residual"
PRELIMINARIES,0.07352941176470588,"JQ(θ) = E(st,at)∼D 1"
PRELIMINARIES,0.07720588235294118,"2
 
Qθ(st, at) −
 
r(st, at) + γEst+1∼P(·|st,at)[V¯θ(st+1)]
2

,
(1)"
PRELIMINARIES,0.08088235294117647,"where D is the dataset or replay buffer storing the transitions, and V¯θ is the value function parameter-
ized by ¯θ. The parameters ψ of policy is learned by maximizing the following objective"
PRELIMINARIES,0.08455882352941177,"Jπ(ψ) = Est∼D

Eat∼πψ[α log(πψ(a|s)) −Qθ(st, at)]

.
(2)"
PRELIMINARIES,0.08823529411764706,"The Bisimulation Metric
defines a pseudometric d : S × S →R,1 where d quantifies the
behavioral similarity of two discrete states (Ferns et al., 2004). An extension to both continuous
and discrete state spaces has also been developed (Ferns et al., 2011). A variant of the bisimulation
metric proposed in Castro (2020) defines a metric w.r.t. a policy π, which is known as the on-policy
bisimulation metric. It removes the requirement of matching actions in the dynamics model but
focuses on the policy π, which is able to better capture behavioral similarity for a specific task.
Because of this property, in this paper, we focus on the π-bisimulation metric (Castro, 2020):"
PRELIMINARIES,0.09191176470588236,"F π(d)(si, sj) = (1 −c)|Rπ
si −Rπ
sj| + cW1(d)(Pπ
si, Pπ
sj).
(3)"
PRELIMINARIES,0.09558823529411764,"where Rπ
s := P"
PRELIMINARIES,0.09926470588235294,"a π(a|s)R(s, a) and Pπ
s := P"
PRELIMINARIES,0.10294117647058823,"a π(a|s)P(·|s, a). The mapping F π : met →met,
where met denotes the set of all pseudometrics in state space S and W1(d)(·, ·) denotes the 1-
Wasserstein distance given the pseudometric d. Then F π has a least fixed point denoted by d∗. Deep"
PRELIMINARIES,0.10661764705882353,"1If the pseudometric d of two states is 0, then the two states belong to an equivalence relation."
PRELIMINARIES,0.11029411764705882,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.11397058823529412,Encoder
PRELIMINARIES,0.11764705882352941,Dynamics Model
PRELIMINARIES,0.1213235294117647,Dynamics Model Q Net
PRELIMINARIES,0.125,"Actor
Encoder"
PRELIMINARIES,0.12867647058823528,Encoder
PRELIMINARIES,0.1323529411764706,"Figure 1: Architecture of our AMBS framework. The dotted arrow represents the regression target
and the dash arrow means stop gradient. Left: the learning process of meta-learner. Right: the model
architecture for SAC with adaptive weight c which is jointly learned with SAC objective."
PRELIMINARIES,0.13602941176470587,"Bisimulation for Control (DBC) (Zhang et al., 2021) is to learn latent representations such that the L1
distances in the latent space are equal to the π-bisimulation metric in the state space:"
PRELIMINARIES,0.13970588235294118,"arg min
ϕ"
PRELIMINARIES,0.14338235294117646,"
∥ϕ(si) −ϕ(sj)∥1 −|Rπ
si −Rπ
sj| −γW2(d)(Pπ
si, Pπ
sj)
2
,"
PRELIMINARIES,0.14705882352941177,"where ϕ is the state encoder, and W2(d)(·, ·) denotes the 2-Wasserstein distance. DBC is combined
with the reinforcement learning algorithm SAC, where ϕ(s) is the input for SAC."
ADAPTIVE META-LEARNER OF BEHAVIORAL SIMILARITIES,0.15073529411764705,"4
ADAPTIVE META-LEARNER OF BEHAVIORAL SIMILARITIES"
ADAPTIVE META-LEARNER OF BEHAVIORAL SIMILARITIES,0.15441176470588236,"In this section, we propose a framework named Adaptive Meta-learner of Behavioral Similarities
(AMBS) to learn generalizable states representation regarding the π-bisimulation metric. The learning
procedure is demonstrated in Figure 1. Observe that the π-bisimulation metric is composed of two
terms: |Rπ
si −Rπ
sj|, which computes the difference of rewards between states, and W2(d)(Pπ
si, Pπ
sj),
which computes the difference of the outputs of dynamics model between states. We propose a
network architecture which contains encoders to transform the high-dimensional visual observations
to two decomposed encodings regarding rewards and dynamics. We develop two meta-learners,
one of which quantifies the reward difference on reward representations and the other captures
dynamics distance (Section 4.1). Each meta-learner is self-learned to update the corresponding state
representations by learning to approximate a term in the π-bisimulation metric, respectively. Rather
than enforcing the L1 distance between embedded states to be equal to the π-bisimulation metric,
our meta-learners are able to use a more flexible form of similarity, i.e., a well-designed non-linear
neural network, with each similarity evaluates the reward difference or dynamics difference between
states beyond the original Euclidean space. Moreover, we propose a strategy for learning to combine
the outputs of the two learned similarities in a specific environment (Section 4.2). We introduce a
learnable weight for the combination and such weight is adaptively learned together with the policy
learning procedure (in this work we use SAC as the base reinforcement learning algorithm). In
addition, we also use a data augmentation technique to make the learned representations more robust
(Section 4.3). The whole learning procedure is trained in an end-to-end manner."
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.15808823529411764,"4.1
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS"
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.16176470588235295,"We design a network architecture with two encoders, ϕr and ϕd, to encode decomposed features from
visual observations as shown in the left side of Figure 1. Each encoder maps a high-dimensional
observation to a low-dimensional representation, i.e., ϕr : S →Zr capturing reward-relevant features
and ϕd : S →Zd capturing dynamics-relevant features. We design two meta-learners, fr and fd,
where fr learns to measure reward difference and fd aims to measure dynamics difference between
two state representations. Specifically, each meta-learner takes two embedded states as input and
outputs the corresponding similarity. The procedure is summarized as follows."
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.16544117647058823,"si, sj →ϕ∗(si), ϕ∗(sj) →f∗(ϕ∗(si), ϕ∗(sj)), where ∗∈{r, d}."
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.16911764705882354,"Note that the aggregation of the outputs of the two encoders ϕr and ϕd of each observation (i.e.,
state)is also fed into SAC to learn a policy (the right side of Figure 1). Each self-learned meta-
learner learns to capture similarity by approximating a distance term in the π-bisimulation metric,
respectively, and provides stable gradients for updating ϕr or ϕd according to the distance to the other
state representations. The details of network architecture can be found in Appendix B.2."
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.17279411764705882,Published as a conference paper at ICLR 2022
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.17647058823529413,"As discussed in Section 1, restricting the form of distance to be the L1/L2 norm in the latent space
may limit the approximation precision. As shown in Figure 2, the L1 and the L2 norm for measuring
the distance of two latent representations ϕ(si) and ϕ(sj) lead to large regression losses during RL
training, which destabilize the representation learning and consequently decrease final performance.
Besides, such hand-crafted distances make semantically similar states encoding close to each other in
terms of the L1 norm in Euclidean space, which however may lose part of useful information, e.g.,
policy-relevant information but not immediately regarding to the distance."
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.1801470588235294,"To overcome the aforementioned limitations, we propose to exploit meta-learners fr and fd to learn
similarities. By learning with a regression target, the similarity learned by f∗is easier to converge to
the target and leads to faster descent tendency of regression loss (as shown in Figure 2 where y-axis
is log-scale). Such a property provides more stable gradients for state representation comparing
to the L1/L2 norm, and therefore the meta-learner f∗is able to guide the process of updating the
state encoder. Besides, f∗is a non-linear transformation that evaluates the state similarity in a more
flexible space instead of the original Euclidean space. Consequently, it is able to preserve more
task-relevant information in state representation that is required for further SAC policy learning."
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.18382352941176472,"Figure 2:
The regres-
sion loss among different
forms of distances."
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.1875,"As the goal of meta-learners is to approximate different types of simi-
larities in the π-bisimulation metric, we design the loss functions for the
meta-learners using the mean squared error:"
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.19117647058823528,"ℓ(fr, ϕr)
=

fr(ϕr(si), ϕr(sj)) −|Rπ
si −Rπ
sj|
2
,
(4)"
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.1948529411764706,"ℓ(fd, ϕd)
=

fd(ϕd(si), ϕd(sj)) −W2(Pπ
si, Pπ
sj)
2
.
(5)"
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.19852941176470587,"To make the resultant optimization problems compatible with SGD up-
dates with transitions sampled from replay buffer, we replace the reward
metric term |Rπ
si −Rπ
sj| by the distance of rewards in the sampled transi-"
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.20220588235294118,"tions in (4), and use a learned parametric dynamics model ˆP to approximate the true dynamics model
Pπ
s in (5). We also use 2-Wasserstein distance W2 in (5) (as in DBC) because it has a closed form for
Gaussian distributions. Denote by (si, ai, ri, s′
i) and (sj, aj, rj, s′
j) two transitions sampled from the
replay buffer. The loss functions (4) and (5) can be revised as follows,"
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.20588235294117646,"ℓ(fr, ϕr)
=
(fr(ϕr(si), ϕr(sj)) −|ri −rj|)2 ,
(6)"
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.20955882352941177,"ℓ(fd, ϕd)
=

fd(ϕd(si), ϕd(sj)) −W2( ˆP(·|ϕd(si), ai), ˆP(·|ϕd(sj), aj))
2
,
(7)"
META-LEARNERS FOR DECOMPOSED REPRESENTATIONS,0.21323529411764705,"where ˆP(·|ϕd(s∗), a∗) is a learned probabilistic dynamics model, which is implemented as a neural
network that takes the dynamics representation ϕd(s∗) and an action a∗as input, and predicts the
distribution of dynamics representation of next state s′
∗. The details of ˆP can be found Appendix A."
BALANCING IMPACT OF REWARD AND DYNAMICS,0.21691176470588236,"4.2
BALANCING IMPACT OF REWARD AND DYNAMICS"
BALANCING IMPACT OF REWARD AND DYNAMICS,0.22058823529411764,"The π-bisimulation metric (3) is defined as a linear combination of the reward and the dynamics
difference. Rather than considering the combination weight as a hyper-parameter, which needs to be
tuned in advance, we introduce a learnable parameter c ∈(0, 1) to adaptively balance the impact of
the reward and the dynamics in different environments and tasks."
BALANCING IMPACT OF REWARD AND DYNAMICS,0.22426470588235295,"As the impact factor c should be automatically determined when learning in a specific task or
environment, we integrate the learning of c into the update of the Q-function in SAC such that
the value of c is learned from the state-action value which is highly relevant to a specific task and
environment. To be specific, we concatenate the low-dimensional representations ϕr(s) and ϕd(s)
weighted by 1 −c and c, where 1 −c and c are output of a softmax to ensure c ∈(0, 1). The loss for
the Q-function is revised as follows, which takes ϕr(s), ϕd(s), weight c and action a as input:"
BALANCING IMPACT OF REWARD AND DYNAMICS,0.22794117647058823,"JQ(θ, c, ϕr, ϕd) = E(s,a,s′)∼D 1"
BALANCING IMPACT OF REWARD AND DYNAMICS,0.23161764705882354,"2 (Qθ(ϕr(s), ϕd(s), c, a) −(r(s, a) + γV (s′)))2

,
(8)"
BALANCING IMPACT OF REWARD AND DYNAMICS,0.23529411764705882,where Qθ is the Q-function parameterized by θ and V (·) is the target value function.
BALANCING IMPACT OF REWARD AND DYNAMICS,0.23897058823529413,"Moreover, to balance the learning of the two meta-learners fr and fd, we jointly minimize the
regression losses of fr and fd ((6) and (7), respectively) with the balance factor c. Thus, the loss"
BALANCING IMPACT OF REWARD AND DYNAMICS,0.2426470588235294,Published as a conference paper at ICLR 2022
BALANCING IMPACT OF REWARD AND DYNAMICS,0.24632352941176472,"function is modified as follows,"
BALANCING IMPACT OF REWARD AND DYNAMICS,0.25,"ℓ(Θ) = (1 −c)ℓ(fr, ϕr) + cℓ(fd, ϕd), where Θ = {fr, fd, ϕr, ϕd}.
(9)"
BALANCING IMPACT OF REWARD AND DYNAMICS,0.2536764705882353,"Note that c is already learned along with the Q-function, we stop gradient of c when minimizing
the loss (9). This is because if we perform gradient descent w.r.t. c, it may only capture which loss
(ℓ(fr, ϕr) or ℓ(fd, ϕd)) is larger/smaller and fail to learn the quantities of their importance. We also
stop gradient for the dynamics model ˆP since ˆP only predicts one-step dynamics."
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.25735294117647056,"4.3
OVERALL OBJECTIVE WITH DATA AUGMENTATION"
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.2610294117647059,"In this section, we propose to integrate a data augmentation strategy into our proposed framework
to learn more robust state representations. We follow the notations of an existing work on data
augmentation for reinforcement learning, DrQ (Yarats et al., 2021), and define a state transformation
h : S × T →S that maps a state to a data-augmented state, where T is the space of parameters
of h. In practice, we use a random crop as the transformation h in this work. Then T contains all
possible crop positions and v ∈T is one crop position drawn from T . We apply DrQ to regularize
the objective of the Q-function (see Appendix for the full objective of Q-function)."
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.2647058823529412,"Besides, by using the augmentation transformation h, we rewrite the loss of representation learning
and similarity approximation in (9) as follows,"
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.26838235294117646,"ℓ(Θ) =(1 −c)

fr

ϕr(s(1)
i ), ϕr(s(1)
j )

−|ri −rj|
2"
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.27205882352941174,"+ c

fd

ϕd(s(1)
i ), ϕd(s(1)
j )

−W2

ˆP(·|ϕd(s(1)
i ), ai), ˆP(·|ϕd(s(1)
j ), aj)
2"
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.2757352941176471,"+ (1 −c)

fr

ϕr(s(2)
j ), ϕr(s(2)
i )

−|ri −rj|
2"
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.27941176470588236,"+ c

fd

ϕd(s(2)
j ), ϕd(s(2)
i )

−W2

ˆP(·|ϕd(s(1)
i ), ai), ˆP(·|ϕd(s(1)
j ), aj)
2
, (10)"
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.28308823529411764,Algorithm 1 AMBS+SAC
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.2867647058823529,"1: Input: Replay Buffer D, initialized Θ = {fr, ϕr, fd, ϕd} , Q
network Qθ, actor piψ, target Q network Q¯
θ,
2: Sample a batch with size B: {(si, ai, ri, s′
i)}B
b=1 ∼D.
3: Shuffle batch {(si, ai, ri, s′
i)}B
b=1 to {(sj, aj, rj, s′
j)}B
b=1.
4: Update Q network by (8) with DrQ augmentation.
5: update actor network by (11).
6: update alpha α.
7: update encoder Θ by (10)
8: update dynamics model ˆ
P."
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.29044117647058826,"where s(k)
∗
= h(s∗, v(k)
∗) is the transformed
state with parameters v(k)
∗. Specifically, s(1)
i ,
s(1)
j , s(2)
i
and s(2)
j
are transformed states with"
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.29411764705882354,"parameters of v(1)
i , v(1)
j , v(2)
i
and v(2)
j
respec-
tively, which are all drawn from T indepen-
dently. The first two terms in (10) are similar
to the two terms in (9), while the observation
s∗in (9) is transformed to s(1)
∗. For the last two
terms in (10), the observation is transformed by
a different parameter v(2)
∗
except for the last term, where the parameter v(1)
∗
does not change. The
reason why the parameter v(1)
∗
is used in the last term is that we expect the meta-learner fd is able to
make a consensus prediction on s(1)
∗
and s(2)
∗
as they are transformed from a same state s∗. Another
major difference between the first two terms and the last two terms is the order of the subscripts i and
j: i is followed by j in first two terms while j is followed by i in last two terms. The reason behind
this is that we aim to make fr and fd to be symmetric."
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.2977941176470588,"The loss of the actor of SAC, which is based on the output of the encoder, is"
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.3014705882352941,"Jπ(ψ) = E(s,a)∼D
h
α log(πψ(a|ˆϕ(s))) −Qθ(ϕ(s), a)
i
,
(11)"
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.30514705882352944,"where ˆϕ denotes the convolutional layers of ϕ and Qθ(ϕ(s), a) is a convenient form of the Q-function
with c-weighted state representations input. We also stop gradients for ˆϕ and ϕ (ϕr and ϕd) regarding
the actor loss. In other words, the loss in (11) only calculates the gradients w.r.t. ψ."
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.3088235294117647,"The overall learning algorithm is summarized in Algorithm 1. Note that due to limit of space, more
details can be found in Appendix A. In summary, as illustrated in Figure 1, in AMBS, the convnet
is shared by the encoders ϕr and ϕd, which consists of 4 convolutional layers. The final output of"
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.3125,Published as a conference paper at ICLR 2022
OVERALL OBJECTIVE WITH DATA AUGMENTATION,0.3161764705882353,"the convnet is fed into two branches: 1) one is a fully-connected layer to form reward representation
ϕr(s), and 2) the other is also a fully-connected layer but forms dynamics representation ϕd(s). The
meta-learners fr and fd are both MLPs with two layers. More implementation details are provided at
Appendix B. Source code is available at https://github.com/jianda-chen/AMBS."
EXPERIMENTS,0.31985294117647056,"5
EXPERIMENTS"
EXPERIMENTS,0.3235294117647059,"The major goal of our proposed AMBS is to learn a generalizable representation for RL from
high-dimensional raw data. We evaluate AMBS on two environments: 1) the DeepMind Control
(DMC) suite (Tassa et al., 2018) with background distraction; and 2) autonomous driving task on
CARLA (Dosovitskiy et al., 2017) simulator. Several baselines methods are compared with AMBS: 1)
Deep Bisimulation Control (DBC) (Zhang et al., 2021) which is recent research on RL representation
learning by using the L1 distance to approximate bisimulation metric; 2) PSEs (Agarwal et al., 2021)
which proposes a new state-pair metric called policy similarity metric for representation learning and
learns state embedding by incorporating a contrastive learning method SimCLR (Chen et al., 2020);
3) DrQ (Yarats et al., 2021), a state-of-the-art data augmentation method for deep RL; 4) Stochastic
Latent Actor-Critic (SLAC) (Lee et al., 2020a), a state-of-the-art approach for partial observability
on controlling by learning a sequential latent model with a reconstruction loss; 5) Soft Actor-Critic
(SAC) (Haarnoja et al., 2018), a commonly used off-policy deep RL method, upon which above
baselines and our approach are built."
DMC SUITE WITH BACKGROUND DISTRACTION,0.3272058823529412,"5.1
DMC SUITE WITH BACKGROUND DISTRACTION"
DMC SUITE WITH BACKGROUND DISTRACTION,0.33088235294117646,"DeepMind Control (DMC) suite (Tassa et al., 2018) is an environment that provides high dimensional
pixel observations for RL tasks. DMC suite with background distraction (Zhang et al., 2018;
Stone et al., 2021) replaces the background with natural videos which are task-irrelevant to the RL
tasks. We perform the comparison experiments in two settings: original background and nature
video background. For each setting we evaluate AMBS and baselines in 4 environments, cartpole-
swingup, finger-spin, cheetah-run and walker-walk. Results of additional environments are shown in
Appendix D. (a)"
DMC SUITE WITH BACKGROUND DISTRACTION,0.33455882352941174,"(b)
Figure 3: (a) Pixel observations of DMC suite with original background. (b) Pixel observations of
DMC suite with natural video background. Videos are sampled from Kinetics (Kay et al., 2017)."
DMC SUITE WITH BACKGROUND DISTRACTION,0.3382352941176471,"Original Background.
Figure 3a shows the DMC observation of original background which is
static and clean. The experiment result is shown in Figure 4. Our method AMBS has comparable
learning efficiency and converge scores comparing to state-of-the-art pixel-level RL methods DrQ
and SLAC. Note that DBC performs worse in original background setting."
DMC SUITE WITH BACKGROUND DISTRACTION,0.34191176470588236,Figure 4: Training curves of AMBS and comparison methods. Each curve is average on 5 runs.
DMC SUITE WITH BACKGROUND DISTRACTION,0.34558823529411764,Published as a conference paper at ICLR 2022
DMC SUITE WITH BACKGROUND DISTRACTION,0.3492647058823529,"Figure 5: Training curves of AMBS and comparison methods on natural video background setting.
Videos are sampled from Kinetics (Kay et al., 2017) dataset. Each curve is average on 5 runs."
DMC SUITE WITH BACKGROUND DISTRACTION,0.35294117647058826,"Natural Video Background.
Figure 3b shows the DMC observation with natural video background.
The background video is sampled from the Kinetics (Kay et al., 2017) dataset under label “driving
car”. By following the experimental configuration of DBC, we sample 1,000 continuous frames as
background video for training, and sample another 1,000 continuous frames for evaluation. The
experimental result is shown in Figure 5. Our method AMBS outperforms other baselines in terms
of both efficiency and scores. It converges to the scores that are similar to the scores on original
background setting within 1 million steps. DrQ can only converge to the same scores on cartpole-
swingup and finger-spin but the learning is slightly slower than AMBS. The sequential latent model
method SLAC performs badly on this setting. This experiment demonstrates that AMBS is robust to
the background task-irrelevant information and the AMBS objectives improve the performance when
comparing to other RL representation learning methods, e.g., DBC."
ABLATION STUDY,0.35661764705882354,"5.2
ABLATION STUDY"
ABLATION STUDY,0.3602941176470588,"Our approach AMBS consists of several major components: meta-learners, a factor to balance the
impact between reward and dynamics, and data augmentation on representation learning. To evaluate
the importance of each component, we eliminate or replace each component by baseline methods.
We conduct experiments on cheetah-run and walker-walk under the natural video background setting.
Figure 6 shows the performance of AMBS, variants of AMBS and baseline methods. AMBS w/o
data aug is constructed by removing data augmentation in AMBS. AMBS w/o f is referred to as
replacing the meta-learners fr and fd by the L1 distance, as in DBC. AMBS w/o c removes the
component of balancing impact between reward and dynamics but encodes the observation into
a single embedding. It uses single meta-learner to approximate the sum of reward and dynamics
distance. DBC + DrQ is DBC with data augmentation applied on representation learning and SAC.
AMBS c = 0.5 denotes AMBS with a fixed weight c = 0.5. AMBS w/ (1, γ) replaces c in AMBS
by freezing the reward weight to 1 and the dynamics weight to γ. Such a setting of weights is used
in DBC. Figure 6 demonstrates that AMBS performs better than any of its variant. Comparing
to DBC that does not utilize data augmentation, our variant AMBS w/o data aug still performs
better. This comparison shows that using meta-learners fr and fd to learn reward- and dynamics-
relevant representations is able to improve RL performance compared with using the L1 distance to
approximate the whole π-bisimulation metric."
TRANSFER OVER REWARD FUNCTIONS,0.3639705882352941,"5.3
TRANSFER OVER REWARD FUNCTIONS"
TRANSFER OVER REWARD FUNCTIONS,0.36764705882352944,"The environments walker-walk, walker-run and walker-stand share the same dynamics but have
different reward functions according to the moving speed. To evaluate the transfer ability of AMBS
over reward functions, we transfer the learned model from walker-walk to walker-run and walker-
stand. We train agents on walker-run and walker-stand with frozen convolutional layers of encoders
that are well trained in walker-walk. The experiments are done under the natural video setting"
TRANSFER OVER REWARD FUNCTIONS,0.3713235294117647,"Figure 6: Ablation Study on cheetah-run (left)
and walker-walk (right) in natural video setting."
TRANSFER OVER REWARD FUNCTIONS,0.375,"Figure 7: Transfer from walker-walk to (left)
walker-run and (right) walker-stand."
TRANSFER OVER REWARD FUNCTIONS,0.3786764705882353,Published as a conference paper at ICLR 2022
TRANSFER OVER REWARD FUNCTIONS,0.38235294117647056,"compared with DBC as shown in Figure 7. It shows that the transferring encoder converges faster
than training from scratch. Besides our approach has a better transfer ability than DBC."
GENERALIZATION OVER BACKGROUND VIDEO,0.3860294117647059,"5.4
GENERALIZATION OVER BACKGROUND VIDEO"
GENERALIZATION OVER BACKGROUND VIDEO,0.3897058823529412,"We evaluate the generalization ability of our method when background video is changed. We follow
the experiment setting of PSE (Agarwal et al., 2021)n. We pick 2 videos, ‘bear’ and ‘bmx-bumps’,
from the training set of DAVIS 2017 (Pont-Tuset et al., 2018) for training. We randomly sample
one video and a frame at each episode start, and then play the video forward and backward until
episode ends. The RL agent is evaluated on validation environment where the background video is
sampled from validation set of DAVIS 2017. Those validation videos are unseen during the training.
The evaluation scores at 500K environment interaction steps is shown in Table 1. The comparison
method DrQ+PSEs is data augmented by DrQ. AMBS outperforms DrQ+PSEs on all the 4 tasks. It
demonstrates that AMBS is generalizable on unseen background videos."
GENERALIZATION OVER BACKGROUND VIDEO,0.39338235294117646,"Methods
C-swingup
F-spin
C-run
W-walk"
GENERALIZATION OVER BACKGROUND VIDEO,0.39705882352941174,"DrQ + PSEs
749 ± 19
779 ± 49 308 ± 12 789 ± 28
AMBS
807 ± 41
933 ± 96 332 ± 27 893 ± 85"
GENERALIZATION OVER BACKGROUND VIDEO,0.4007352941176471,"Table 1: Generalization to unseen background videos sampled from DAVIS 2017 (Pont-Tuset et al.,
2018). Scores of DrQ+PSEs are reported from Agarwal et al. (2021)."
GENERALIZATION OVER BACKGROUND VIDEO,0.40441176470588236,"(a)
(b)
Figure 8: (a) Illustration of a third-person view in “Town4” scenario of CARLA. (b) A first-person
observation for RL agent. It concatenates five cameras for 300 degrees view ."
AUTONOMOUS DRIVING TASK ON CARLA,0.40808823529411764,"5.5
AUTONOMOUS DRIVING TASK ON CARLA"
AUTONOMOUS DRIVING TASK ON CARLA,0.4117647058823529,Figure 9: CARLA simulation.
AUTONOMOUS DRIVING TASK ON CARLA,0.41544117647058826,"CARLA is an autonomous driving simulator that provides a 3D
simulation for realistic on-road scenario. In real world cases
or in realistic simulation, the learning of RL agents may suffer
from complex background that contains task-irrelevant details.
To argue that AMBS can address this issue, we perform exper-
iments with CARLA. We follow the setting of DBC to create
an autonomous driving task on map “Town04” for controlling a
car to drive as far as possible in 1,000 frames. The environment
contains a highway with 20 moving vehicles. The reward function prompts the driving distance
and penalizes collisions. The observation shape is 84 × 420 pixels which consists of five 84 × 84
cameras. Each camera has 60 degrees view, together they produce a 300 degrees first-person view, as
shown in Figure 8. The weather, clouds and brightness may change slowly during the simulation.
Figure 9 shows the training curves of AMBS and other comparison methods. AMBS performs the
best, which provides empirical evidence that AMBS can potentially generalize over task-irrelevant
details, e.g., weather and clouds, in real world scenarios."
CONCLUSION,0.41911764705882354,"6
CONCLUSION"
CONCLUSION,0.4227941176470588,"This paper presents a novel framework AMBS with meta-learners for learning task-relevant and
environment-detail-invariant state representation for deep RL. In AMBS, we propose state encoders
that decompose an observation into rewards and dynamics representations for measuring behavioral
similarities by two self-learned meta-learners, and design a learnable strategy to balance the two
types of representations. AMBS archives new state-of-the-art results in various environments and
demonstrates its transfer ability on RL tasks. Experimental results verify that our framework is able
to effectively learn a generalizable state representation."
CONCLUSION,0.4264705882352941,Published as a conference paper at ICLR 2022
CONCLUSION,0.43014705882352944,ACKNOWLEDGEMENT
CONCLUSION,0.4338235294117647,This work is supported by Microsoft Research Asia collaborative research grant 2020.
REFERENCES,0.4375,REFERENCES
REFERENCES,0.4411764705882353,"Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive
behavioral similarity embeddings for generalization in reinforcement learning. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=qda7-sVg84."
REFERENCES,0.44485294117647056,"Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic markov
decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 10069–
10076, 2020."
REFERENCES,0.4485294117647059,"Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. Mico: Improved rep-
resentations via sampling-based state similarity for markov decision processes. In A. Beygelzimer,
Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing
Systems, 2021. URL https://openreview.net/forum?id=wFp6kmQELgu."
REFERENCES,0.4522058823529412,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Hal Daum´e III and Aarti Singh (eds.), Proceedings
of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp. 1597–1607. PMLR, 13–18 Jul 2020. URL http://proceedings.
mlr.press/v119/chen20j.html."
REFERENCES,0.45588235294117646,"Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization
in reinforcement learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings
of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 1282–1289. PMLR, 09–15 Jun 2019. URL http://proceedings.
mlr.press/v97/cobbe19a.html."
REFERENCES,0.45955882352941174,"Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA:
An open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning,
pp. 1–16, 2017."
REFERENCES,0.4632352941176471,"Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron,
Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable
distributed deep-RL with importance weighted actor-learner architectures. In Jennifer Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 1407–1416. PMLR, 10–15 Jul 2018."
REFERENCES,0.46691176470588236,"Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. In
Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, UAI ’04, pp. 162–169,
Arlington, Virginia, USA, 2004. AUAI Press. ISBN 0974903906."
REFERENCES,0.47058823529411764,"Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous markov
decision processes. SIAM J. Comput., 40(6):1662–1714, December 2011. ISSN 0097-5397. doi:
10.1137/10080484X. URL https://doi.org/10.1137/10080484X."
REFERENCES,0.4742647058823529,"Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Bellemare. DeepMDP:
Learning continuous latent space models for representation learning. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2170–2179. PMLR,
09–15 Jun 2019. URL http://proceedings.mlr.press/v97/gelada19a.html."
REFERENCES,0.47794117647058826,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 1861–1870. PMLR, 10–15 Jul 2018. URL
http://proceedings.mlr.press/v80/haarnoja18b.html."
REFERENCES,0.48161764705882354,Published as a conference paper at ICLR 2022
REFERENCES,0.4852941176470588,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 2555–2565. PMLR, 09–15 Jun
2019. URL http://proceedings.mlr.press/v97/hafner19a.html."
REFERENCES,0.4889705882352941,"Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=S1lOTC4tDS."
REFERENCES,0.49264705882352944,"Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew
Botvinick, Charles Blundell, and Alexander Lerchner. DARLA: Improving zero-shot transfer
in reinforcement learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 1480–1490. PMLR, 06–11 Aug 2017.
URL http://proceedings.mlr.
press/v70/higgins17a.html."
REFERENCES,0.4963235294117647,"Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,
Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman.
The kinetics human action video dataset. CoRR, abs/1705.06950, 2017."
REFERENCES,0.5,"Mete Kemertas and Tristan Ty Aumentado-Armstrong. Towards robust bisimulation metric learning.
In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural
Information Processing Systems, 2021. URL https://openreview.net/forum?id=
ySFGlFjgIfN."
REFERENCES,0.5036764705882353,"Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning.
In The 2010 International Joint Conference on Neural Networks (IJCNN), pp. 1–8, 2010."
REFERENCES,0.5073529411764706,"Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised repre-
sentations for reinforcement learning.
In Hal Daum´e III and Aarti Singh (eds.), Proceed-
ings of the 37th International Conference on Machine Learning, volume 119 of Proceed-
ings of Machine Learning Research, pp. 5639–5650. PMLR, 13–18 Jul 2020a. URL http:
//proceedings.mlr.press/v119/laskin20a.html."
REFERENCES,0.5110294117647058,"Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Re-
inforcement learning with augmented data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
19884–19895. Curran Associates, Inc., 2020b. URL https://proceedings.neurips.
cc/paper/2020/file/e615c82aba461681ade82da2da38004a-Paper.pdf."
REFERENCES,0.5147058823529411,"Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 741–752. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/
paper/2020/file/08058bf500242562c0d031ff830ad094-Paper.pdf."
REFERENCES,0.5183823529411765,"Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network randomization: A simple technique
for generalization in deep reinforcement learning. In International Conference on Learning
Representations, 2020b. URL https://openreview.net/forum?id=HJgcvJBFvB."
REFERENCES,0.5220588235294118,"Timothee Lesort, Natalia D´ıaz-Rodr´ıguez, Jean-Franois Goudou, and David Filliat. State representa-
tion learning for control: An overview. Neural Networks, 108:379–392, 2018."
REFERENCES,0.5257352941176471,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hassabis.
Human-level control through deep rein-
forcement learning.
Nature, 518(7540):529–533, February 2015.
ISSN 00280836.
URL
http://dx.doi.org/10.1038/nature14236."
REFERENCES,0.5294117647058824,Published as a conference paper at ICLR 2022
REFERENCES,0.5330882352941176,"Marek Petrik and Bruno Scherrer.
Biasing approximate dynamic programming with a
lower discount factor.
In D. Koller,
D. Schuurmans,
Y. Bengio,
and L. Bottou
(eds.), Advances in Neural Information Processing Systems, volume 21. Curran Asso-
ciates, Inc., 2009. URL https://proceedings.neurips.cc/paper/2008/file/
08c5433a60135c32e34f46a71175850c-Paper.pdf."
REFERENCES,0.5367647058823529,"Silviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual data augmentation using locally
factored dynamics. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 3976–3990. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
294e09f267683c7ddc6cc5134a7e68a8-Paper.pdf."
REFERENCES,0.5404411764705882,"Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel´aez, Alex Sorkine-Hornung, and
Luc Van Gool. The 2017 davis challenge on video object segmentation. CoRR, abs/1704.00675,
2018."
REFERENCES,0.5441176470588235,"Roberta Raileanu, Max Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic data
augmentation for generalization in deep reinforcement learning, 2021."
REFERENCES,0.5477941176470589,"Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bach-
man. Data-efficient reinforcement learning with self-predictive representations. In International
Conference on Learning Representations, 2021."
REFERENCES,0.5514705882352942,"Austin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski. The distracting control suite –
a challenging benchmark for reinforcement learning from pixels. CoRR, abs/2101.02722, 2021."
REFERENCES,0.5551470588235294,"Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller.
Deepmind control suite. CoRR, abs/1801.00690, 2018."
REFERENCES,0.5588235294117647,"Niklas Wahlstr¨om, Thomas B. Sch¨on, and Marc Peter Deisenroth. From pixels to torques: Policy
learning with deep dynamical models. CoRR, abs/1502.02251, 2015."
REFERENCES,0.5625,"Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:
A locally linear latent dynamics model for control from raw images. In C. Cortes, N. Lawrence,
D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/
paper/2015/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf."
REFERENCES,0.5661764705882353,"Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep
reinforcement learning from pixels. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=GY6-6sTvGaf."
REFERENCES,0.5698529411764706,"Chang Ye, Ahmed Khalifa, Philip Bontrager, and Julian Togelius. Rotation, translation, and cropping
for zero-shot generalization. In 2020 IEEE Conference on Games (CoG), pp. 57–64, 2020a. doi:
10.1109/CoG47356.2020.9231907."
REFERENCES,0.5735294117647058,"Chang Ye, Ahmed Khalifa, Philip Bontrager, and Julian Togelius. Rotation, translation, and cropping
for zero-shot generalization. In 2020 IEEE Conference on Games (CoG), pp. 57–64, 2020b. doi:
10.1109/CoG47356.2020.9231907."
REFERENCES,0.5772058823529411,"Amy Zhang, Yuxin Wu, and Joelle Pineau. Natural environment benchmarks for reinforcement
learning. CoRR, abs/1811.06032, 2018."
REFERENCES,0.5808823529411765,"Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
invariant representations for reinforcement learning without reconstruction. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=-2FCwDKRREu."
REFERENCES,0.5845588235294118,"Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, and Sergey Levine. SO-
LAR: Deep structured representations for model-based reinforcement learning. In Kamalika Chaud-
huri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7444–7453. PMLR, 09–15
Jun 2019. URL http://proceedings.mlr.press/v97/zhang19m.html."
REFERENCES,0.5882352941176471,Published as a conference paper at ICLR 2022
REFERENCES,0.5919117647058824,"Brian D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal
Entropy. PhD thesis, USA, 2010. AAI3438449."
REFERENCES,0.5955882352941176,Published as a conference paper at ICLR 2022
REFERENCES,0.5992647058823529,"A
OBJECTIVES OF ADAPTIVE META-LEARNER OF BEHAVIORAL
SIMILARITIES (AMBS)"
REFERENCES,0.6029411764705882,"We denote two encoders ϕr and ϕd, where each encoder maps a high-dimensional observation to a
low-dimensional representation. The encoder ϕr : S →Zr captures reward-relevant features and
the encoder ϕd : S →Zd captures dynamics-relevant features, where Zr and Zd are representation
spaces. We design two meta-learners fr : Zr × Zr →R and fd : Zd × Zd →R to output reward
difference and dynamics difference, respectively. We follow DrQ (Yarats et al., 2021) to define the
state transformation h : S × T →S that maps a state to a data-augmented state, where T is the
space of parameters of h. In practice, we use random crop as transformation h."
REFERENCES,0.6066176470588235,"With augmentation transformation h, the loss of representation learning is,"
REFERENCES,0.6102941176470589,"ℓ(Θ) =(1 −c)

fr(ϕr(s(1)
i ), ϕr(s(1)
j )) −|ri −rj|
2"
REFERENCES,0.6139705882352942,"+ c

fd(ϕd(s(1)
i ), ϕd(s(1)
j )) −W2( ˆP(·|ϕd(s(1)
i ), ai), ˆP(·|ϕd(s(1)
j ), aj))
2"
REFERENCES,0.6176470588235294,"+ (1 −c)

fr(ϕr(s(2)
j ), ϕr(s(2)
i )) −|ri −rj|
2"
REFERENCES,0.6213235294117647,"+ c

fd(ϕd(s(2)
j ), ϕd(s(2)
i )) −W2( ˆP(·|ϕd(s(1)
i ), ai), ˆP(·|ϕd(s(1)
j ), aj))
2
, (12)"
REFERENCES,0.625,"where Θ = {fr, fd, ϕr, ϕd}, s(l)
∗
= h(s∗, v(l)
∗) is the transformed state with parameters v(l)
∗. Specif-
ically, s(1)
i , s(1)
j , s(2)
i
and s(2)
j
are transformed states with parameters of v(1)
i , v(1)
j , v(2)
i
and v(2)
j
respectively, which are all drawn from T independently. ˆP(·|ϕd(s∗), a∗) is a learned probabilistic
dynamics model, which outputs a Gaussian distribution over Zd for predicting dynamics represen-
tation of next state s′
∗. W2 denotes the 2-Wasserstein distance which has closed-form for Gaussian"
REFERENCES,0.6286764705882353,"distributions: W2(N(µi, Σi), N(µj, Σj))2 = ∥µi −µj∥2
2 + ∥Σ"
REFERENCES,0.6323529411764706,"1
2
i −Σ"
REFERENCES,0.6360294117647058,"1
2
j ∥2
F, where µ∗∈R|Zd| are
the mean vectors of Gaussian distribution, Σ∗∈R|Zd| are the diagonals of covariance matrices, and
∥· ∥F is Frobenius norm. Note that we stop gradients for c as mentioned in Section 4.2."
REFERENCES,0.6397058823529411,"The adaptive weight c and (1 −c) are the softmax output of corresponding learnable parameter
ηc ∈R2. It can be formulated as
[c : (1 −c)] = softmax(ηc).
(13)
where the operation : denotes concatenation of two vectors/scalars,"
REFERENCES,0.6433823529411765,"We integrate the learning of c into the update of Q-function of SAC. The objective of the Q-function
with DrQ augmentation and clipped double trick is"
REFERENCES,0.6470588235294118,"JQ(θk, ηc) = E(s,a,s′)∼D 1 2"
REFERENCES,0.6507352941176471,"
Qθk([(1 −c)ϕr(s(1)) : cϕd(s(1))], a) −(r(s, a) + γV (s′))
2 +1 2"
REFERENCES,0.6544117647058824,"
Qθk([(1 −c)ϕr(s(2)) : cϕd(s(2))], a) −(r(s, a) + γV (s′))
2
,
(14)"
REFERENCES,0.6580882352941176,"where s(l) = h(s, v(l)), v(l) are all drawn from T independently, and {Qθk}2
k=1 are the double Q
networks. The target value function with DrQ augmentation is"
REFERENCES,0.6617647058823529,V (s′) = 1 2
REFERENCES,0.6654411764705882,"
min
k=1,2 Q¯θk

[(1 −¯c)¯ϕr(s′(1)) : ¯c¯ϕd(s′(1))], a′(1)
+ α log πψ(a′(1)|s′(1))"
REFERENCES,0.6691176470588235,"+ min
k=1,2 Q¯θk

[(1 −¯c)¯ϕr(s′(2)) : ¯c¯ϕd(s′(2))], a′(2)
+ α log πψ(a′(2)|s′(2))

,
(15)"
REFERENCES,0.6727941176470589,"where s′(l) = h(s′, v′(l)), a′(l) ∼πψ(a′(l)|s′(l)), ¯θk is the set of parameters of the target Q network,
¯ϕr and ¯ϕd are target encoders specifically for the target Q network and ¯c is the adaptive weight for
target encoders. The parameters ¯θk, ¯ϕr, ¯ϕd and ¯ηc are softly updated."
REFERENCES,0.6764705882352942,"The loss of actor of SAC is,"
REFERENCES,0.6801470588235294,Jπ(ψ) = E(s)∼D
REFERENCES,0.6838235294117647,"
Ea∼πψ[α log(πψ(a|ˆϕ(s(1)))) −min
k=1,2 Qθk(ϕ(s(1)), a)]

.
(16)"
REFERENCES,0.6875,Published as a conference paper at ICLR 2022
REFERENCES,0.6911764705882353,"The loss of α of SAC is,"
REFERENCES,0.6948529411764706,"J(α) = E(s)∼D
h
Ea∼πψ[α log(πψ(a|ˆϕ(s(1)))) −α ¯H]
i
,
(17)"
REFERENCES,0.6985294117647058,where ¯H ∈R is the target entropy hyper-parameter which in practice is ¯H = −|A|.
REFERENCES,0.7022058823529411,The loss for updating of dynamics model ˆP is
REFERENCES,0.7058823529411765,"ℓ( ˆP) = E(s,a,s′)∼D  "
REFERENCES,0.7095588235294118,"ϕd(s′(1)) −µ( ˆP(·|ϕd(s(1)), a))"
REFERENCES,0.7132352941176471,"2σ( ˆP(·|ϕd(s(1)), a)) !2"
REFERENCES,0.7169117647058824,",
(18)"
REFERENCES,0.7205882352941176,"where µ( ˆP(·)) and σ( ˆP(·)) are mean and standard deviation of ˆP output Gaussian distribution,
respectively."
REFERENCES,0.7242647058823529,Algorithm 2 shows the algorithm at each learning step.
REFERENCES,0.7279411764705882,Algorithm 2 AMBS+SAC
REFERENCES,0.7316176470588235,"1: Input: Replay Buffer D, initialized Θ = {fr, ϕr, fd, ϕd} , Q network Qθk, actor piψ, target Q
network Q¯θk,
2: Sample a batch with size B: {(si, ai, ri, s′
i)}B
b=1 ∼D.
3: Shuffle batch {(si, ai, ri, s′
i)}B
b=1 to {(sj, aj, rj, s′
j)}B
b=1.
4: Update Q network by equation 14 with DrQ augmentation.
5: Update actor network by equation 16 .
6: Update alpha α by equation 17 .
7: Update encoder Θ by equation 12
8: Update dynamics model ˆP by equation 18.
9: Softly update target Q network: ¯θk = τQθk + (1 −τQ)¯θk .
10: Softly update target encoder: ¯ϕ = τϕϕ + (1 −τϕ)¯ϕ .
11: Softly update target ¯c: ¯ηc = τϕηc + (1 −τϕ)¯ηc ."
REFERENCES,0.7352941176470589,"B
IMPLEMENTATION DETAILS"
REFERENCES,0.7389705882352942,"B.1
PIXELS PROCESSING"
REFERENCES,0.7426470588235294,"We stack 3 consecutive frames as observation, where each frame is 84×84 RGB images in DeepMind
control suite. Each pixel is divided by 255 to down scale to [0, 1]. We consider the stacked frames as
fully observed state."
REFERENCES,0.7463235294117647,"B.2
NETWORK ARCHITECTURE"
REFERENCES,0.75,"We share the convnet for encoder ϕr and encoder ϕd. The shared convnet consists of four convolu-
tional layers with 3 × 3 kernels and 32 output channels. We set stride to 1 everywhere, except for the
first convolutional layer, which has stride 2. We use ReLU activation for each convolutional layer
output. The final output of convnet is fed into two branches: 1) one is a fully-connected layer with 50
dimensions to form reward representation ϕr(s), and 2) the other one is also a fully-connected layer
with 50 dimensions but form dynamics representation ϕd(s). The meta-learners fr and fd are both
MLPs with two layers with 50 hidden dimensions."
REFERENCES,0.7536764705882353,"The critic network takes ϕr(s), ϕd(s) and action as input, and feeds them into three stacked fully-
connected layers with 1024 hidden dimensions. The actor network takes the shared convnet output as
input, and feeds it into four fully-connected layers where the first layer has 100 hidden dimensions
and other layers have 1024 hidden dimensions. The dynamics model is MLPs with two layers with
512 hidden dimensions. ReLU activations are used in every hidden layer."
REFERENCES,0.7573529411764706,"B.3
HYPERPARAMETERS"
REFERENCES,0.7610294117647058,The hyperparameters used in our algorithm are listed in Table 2.
REFERENCES,0.7647058823529411,Published as a conference paper at ICLR 2022
REFERENCES,0.7683823529411765,"Parameter name
Value"
REFERENCES,0.7720588235294118,"Replay buffer capacity
106
Discount factor γ
0.99
Minibatch size
128
Optimizer
Adam
Learning rate for α
10−4"
REFERENCES,0.7757352941176471,"Learning rate except α
5 × 10−4
Target update frequency
2
Actor update frequency
2
Actor log stddev bounds
[−10, 2]
τQ
0.01
τϕ
0.05
Init temperature
0.1"
REFERENCES,0.7794117647058824,Table 2: Hyperparameters used in our algorithm.
REFERENCES,0.7830882352941176,"B.4
ACTION REPEAT FOR DMC"
REFERENCES,0.7867647058823529,"We use different action repeat hyper-parameters for each task in DeepMind control suite, which are
listed in Table 3."
REFERENCES,0.7904411764705882,"Task name
Action repeat"
REFERENCES,0.7941176470588235,"Cartpole Swingup
8
Finger Spin
2
Cheetah Run
4
Walker Walk
2
Reacher Easy
4
Ball In Cup Catch
4"
REFERENCES,0.7977941176470589,Table 3: Action repeat used for each task in DeepMind control suite.
REFERENCES,0.8014705882352942,"B.5
SOURCE CODES"
REFERENCES,0.8051470588235294,Source code of AMBS is available at https://github.com/jianda-chen/AMBS.
REFERENCES,0.8088235294117647,"C
ANALYSIS"
REFERENCES,0.8125,"C.1
REGRESSION LOSSES OF APPROXIMATING BISIMULATION METRIC"
REFERENCES,0.8161764705882353,"We compare the regression losses of approximating the bisimulation metric over RL environment
steps among DBC and our AMBS. DBC uses the L1 norm to calculate the distances between two
states embeddings. AMBS performs meta-learners on state representations to measure the distance
with learned similarities. Figure 10 demonstrates that AMBS has smaller regression loss and also
faster descent tendency compared to DBC. Meta-learners in AMBS are able to provide more stable
gradients to update the state representations."
REFERENCES,0.8198529411764706,Published as a conference paper at ICLR 2022
REFERENCES,0.8235294117647058,Figure 10: The regression losses over RL environment steps among DBC and AMBS.
REFERENCES,0.8272058823529411,"C.2
VISUALIZATIONS OF COMBINATION WEIGHT C"
REFERENCES,0.8308823529411765,"Figure 11 shows the values of combination weight c in AMBS trained on DMC with origin background
and Figure 11 is on DMC with video background setting. The values of c at 1M steps vary in different
tasks. The common trend is that they increase at the beginning of training. In original background
setting c changes slowly after the beginning. In natural video background setting , c has a fast drop
after the beginning. In the beginning, agent learns a large weight c for dynamics feature that may
boost the learning of RL agent. Then it drops for natural video background setting because it learns
that the video background is distracting the agent. Lower weight is learned for the dynamics features."
REFERENCES,0.8345588235294118,"Figure 11: The value of combination weight c
over environment steps. AMBS is trained on
DMC on Original background."
REFERENCES,0.8382352941176471,"Figure 12: The value of combination weight c
over environment steps. AMBS is trained on
DMC on video background."
REFERENCES,0.8419117647058824,"C.3
NORMS OF STATE EMBEDDINGS"
REFERENCES,0.8455882352941176,"We record the L1 norms of reward representation ϕr(s) and dynamics representation ϕd(s) during
training. We record
1
nr ||ϕr(s)||1 for reward representation where nr is the number of dimensions of
ϕr(s), and
1
nd ||ϕd(s)||1 for dynamics representation where nr is the number of dimensions of ϕd(s).
Figure 13 shows the values averaged on each sampled training batch. Although the L1 norm of ϕd(s)
decreases during training, it converges around 0.4-0.5. The L1 norm of ϕr(s) also decreases in the
training procedure. It can verify that dynamics representation ϕd(s) will not converge to all zeros."
REFERENCES,0.8492647058823529,"Figure 13: Norms of state embeddings:
1
nr ||ϕr(s)||1 and
1
nd ||ϕd(s)||1."
REFERENCES,0.8529411764705882,Published as a conference paper at ICLR 2022
REFERENCES,0.8566176470588235,"C.4
SHARING ENCODERS BETWEEN ACTOR AND CRITIC"
REFERENCES,0.8602941176470589,"In AMBS, we share only the convolutional layers ˆϕ of encoders between actor and critic network.
We compare to sharing the whole encoders ϕr and ϕd in this experiment. Figure 14 shows the raining
curves on Walker-Walk with natural video background. Sharing full encoder ϕr and ϕd is worse than
sharing only CNN ˆϕ."
REFERENCES,0.8639705882352942,"Figure 14: Training curves of different ways of sharing encoder. Experiments on Walker-Walk with
natural video background."
REFERENCES,0.8676470588235294,"D
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.8713235294117647,"D.1
ADDITIONAL RESULT OF DMC SUITE WITH AND W/O BACKGROUND DISTRACTION"
REFERENCES,0.875,"Figure 15 shows the training curves on original background setting. Figure 16 shows the training
curves on Kinetics (Kay et al., 2017) video background setting."
REFERENCES,0.8786764705882353,Figure 15: Training curves of AMBS and comparison methods. Each curve is average on 3 runs.
REFERENCES,0.8823529411764706,Published as a conference paper at ICLR 2022
REFERENCES,0.8860294117647058,"Figure 16: Training curves of AMBS and comparison methods on natural video background setting.
Videos are sampled from Kinetics (Kay et al., 2017) dataset. Each curve is average on 3 runs."
REFERENCES,0.8897058823529411,"D.2
ADDITIONAL RESULT OF GENERALIZATION OVER BACKGROUND VIDEO"
REFERENCES,0.8933823529411765,"Table 4 shows the scores on DAVIS video background setting comparing to PSEs (Agarwal et al.,
2021)."
REFERENCES,0.8970588235294118,"Methods
C-swingup
F-spin
C-run
W-walk
R-easy
BiC-catch"
REFERENCES,0.9007352941176471,"DrQ + PSEs
749 ± 19
779 ± 49 308 ± 12 789 ± 28 955 ± 10 821 ± 17
AMBS
807 ± 41
933 ± 96 332 ± 27 893 ± 85 980 ± 11 944 ± 59"
REFERENCES,0.9044117647058824,"Table 4: Generalization with unseen background videos sampled from DAVIS 2017 (Pont-Tuset et al.,
2018) dataset. We evaluate the agents at 500K environment steps. Scores of DrQ + PSEs are reported
from Agarwal et al. (2021)."
REFERENCES,0.9080882352941176,"D.3
ADDITIONAL RESULT ON CARLA"
REFERENCES,0.9117647058823529,"The CARLA experiment is to learn an autonomous driving RL agent to avoid collisions with other
moving vehicles and drive as far as possible on a highway within 1000 frames. Those vehicles are
randomly generated, including vehicles’ types, colors and initial positions. We raise the number of
generated vehicles from 20 to 40 so that the road becomes more crowded for RL agent. Besides, we
also extend the training steps to 2e5 for AMBS and DBC. Figure 17 shows the training curve on
CARLA. AMBS and DBC run on the precious setting of CARLA, while AMBS40 and DBC40 run
on the highway with 40 generated vehicles. When increasing the number of vehicles from 20 to 40,
DBC becomes more difficult to learn but AMBS is just slightly slower. DBC still doesn’t achieve
high scores after 1e5 steps."
REFERENCES,0.9154411764705882,Published as a conference paper at ICLR 2022
REFERENCES,0.9191176470588235,Figure 17: Training curve on CARLA environment.
REFERENCES,0.9227941176470589,"E
PROOFS"
REFERENCES,0.9264705882352942,"E.1
VALUE-FUNCTION BOUND"
REFERENCES,0.9301470588235294,"Theorem E.1 (Value function difference bound for different discounting factors (Petrik & Scherrer,
2009; Kemertas & Aumentado-Armstrong, 2021)). Consider two identical MDPs except for different
discounting factors γ1 and γ2, where γ1 ≤γ2 and reward r ∈[0, 1]. Let V π
γ denotes the value
function for MDP with discounting factor γ given policy π.
V π
γ1(s) −V π
γ2(s)
 ≤
γ2 −γ1
(1 −γ1)(1 −γ2)
(19)"
REFERENCES,0.9338235294117647,"Definition E.1 (c-weighted on policy bisimulation metric). Given a policy π, and c ∈(0, 1), the
on-policy bisimulation metric exists,"
REFERENCES,0.9375,"d(si, sj) = (1 −c)|Rπ
si −Rπ
sj| + cW1(d)(Pπ
si, Pπ
sj).
(20)"
REFERENCES,0.9411764705882353,"Theorem E.2 (Value difference bound). For any c ∈[γ, 1), given two state si and sj,"
REFERENCES,0.9448529411764706,"(1 −c) |V π(si) −V π(sj)| ≤d(si, sj)
(21)"
REFERENCES,0.9485294117647058,proof. Lemma 6 of Kemertas & Aumentado-Armstrong (2021).
REFERENCES,0.9522058823529411,"Theorem E.3 (Generalized value difference bound). For any c ∈(0, 1), given two states si and sj,"
REFERENCES,0.9558823529411765,"(1 −c)|V π(si) −V π(sj)| ≤d(si, sj) + 2(1 −c)(γ −min(c, γ))"
REFERENCES,0.9595588235294118,"(1 −γ)(1 −c)
(22)"
REFERENCES,0.9632352941176471,"proof. We follow the proof of Theorem 1 in Kemertas & Aumentado-Armstrong (2021). Suppose
another MDP with discounting factor γ′ = c exists. From Theorem E.2 we have"
REFERENCES,0.9669117647058824,"(1 −c)|V π
γ′(si) −V π
γ′(sj)| ≤d(si, sj).
(23) Then,"
REFERENCES,0.9705882352941176,"(1 −c)|V π(si) −V π(sj)|
=(1 −c)|V π(si) −V π
γ′(si) + V π
γ′(si) −V π(sj) + V π
γ′(sj) −V π
γ′(sj)|"
REFERENCES,0.9742647058823529,"≤(1 −c)(|V π
γ′(si) −V π
γ′(sj)| + |V π(si) −V π
γ′(si)| + |V π(sj) −V π
γ′(sj)|)"
REFERENCES,0.9779411764705882,"≤d(si, sj) + (1 −c)(|V π(si) −V π
γ′(si)| + |V π(sj) −V π
γ′(sj)|)"
REFERENCES,0.9816176470588235,"≤d(si, sj) + 2(1 −c)(γ −min(γ′, γ))"
REFERENCES,0.9852941176470589,(1 −γ)(1 −γ′) (24)
REFERENCES,0.9889705882352942,"Replace γ′ by c, finally we have"
REFERENCES,0.9926470588235294,"(1 −c)|V π(si) −V π(sj)| ≤d(si, sj) + 2(1 −c)(γ −min(c, γ))"
REFERENCES,0.9963235294117647,"(1 −γ)(1 −c)
.
(25)"
