Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000819000819000819,"This work investigates the offline formulation of the contextual bandit problem,
where the goal is to leverage past interactions collected under a behavior policy to
evaluate, select, and learn new, potentially better-performing, policies. Motivated
by critical applications, we move beyond point estimators. Instead, we adopt the
principle of pessimism where we construct upper bounds that assess a policy’s worst-
case performance, enabling us to confidently select and learn improved policies.
Precisely, we introduce novel, fully empirical concentration bounds for a broad
class of importance weighting risk estimators. These bounds are general enough to
cover most existing estimators and pave the way for the development of new ones.
In particular, our pursuit of the tightest bound within this class motivates a novel
estimator (LS), that logarithmically smooths large importance weights. The bound
for LS is provably tighter than its competitors, and naturally results in improved
policy selection and learning strategies. Extensive policy evaluation, selection, and
learning experiments highlight the versatility and favorable performance of LS."
INTRODUCTION,0.001638001638001638,"1
Introduction"
INTRODUCTION,0.002457002457002457,"In decision-making under uncertainty, offline contextual bandit [16] presents a practical framework
for leveraging past interactions with an environment to optimize future decisions. This comes into
play when we possess logged data summarizing an agent’s past interactions [10]. These interactions,
typically captured as context-action-reward tuples, hold valuable insights into the underlying dynamics
of the environment. Each tuple represents a single round of interaction, where the agent observes
a context (including relevant features), takes an action according to its current policy, often called
behavior policy, and receives a reward that depends on both the observed context and the taken
action. This framework is prevalent in interactive systems like online advertising, music streaming,
and video recommendation. In online advertising, for instance, the user’s profile is the context, the
recommended product is the action, and the click-through rate (CTR) is the expected reward. By
learning from past interactions, the recommender system tailors product suggestions to individual
preferences, maximizing engagement and ultimately, business success."
INTRODUCTION,0.003276003276003276,"To optimize future decisions without requiring real-time deployments, this framework presents us
with three tasks: off-policy evaluation (OPE) [16], off-policy selection (OPS) [32], and off-policy
learning (OPL) [55]. OPE estimates the risk: the negative of expected reward that a target policy
would achieve, essentially predicting its performance if deployed. OPS selects the best-performing"
INTRODUCTION,0.004095004095004095,"policy from a finite set of options, and OPL finds the optimal policy within an infinite class of policies.
In general, OPE is an intermediary step for OPS and OPL since its primary goal is policy comparison."
INTRODUCTION,0.004914004914004914,"A significant amount of research in OPE has centered around Inverse Propensity Scoring (IPS)
estimators [24, 16–18, 60, 19, 54, 38, 32, 45]. These estimators rely on importance weighting to
address the discrepancy between the target and behavior policies. While unbiased under some
conditions, IPS induces high variance. To mitigate this, regularization techniques have been proposed
for IPS [10, 38, 54, 5, 21] trading some bias for reduced variance. However, these estimators
can still deviate from the true risk, undermining their reliability for decision-making, especially in
critical applications. In such scenarios, practitioners need estimates that cover the true risk with high
confidence. To address this, several approaches focused on constructing either asymptotic [10, 48, 15]
or finite sample [32, 21], high probability, empirical upper bounds on the risk. These bounds evaluate
the performance of a policy in the worst-case scenario, adopting the principle of pessimism [27]."
INTRODUCTION,0.005733005733005733,"If this principle is used in OPE, it is central in OPS and OPL, where strategies are inspired by, or
directly derived from, upper bounds on the risk [55, 35, 32, 49, 5, 59, 21]. Examples for OPS include
Kuzborskij et al. [32] who employed an Efron-Stein bound for self-normalized IPS, or Gabbianelli
et al. [21] that based their analysis on an upper bound constructed with the Implicit Exploration
estimator. Focusing on OPL, Swaminathan and Joachims [55] exploited the empirical Bernstein
bound [36] alongside the Clipping estimator to motivate sample variance penalization. This work
was recently improved by either modifying the penalization [59] or analyzing the problem from the
PAC-Bayesian lens [35]. The latter direction was further explored by Sakhi et al. [49], Aouali et al.
[5, 7], Gabbianelli et al. [21] resulting in tight PAC-Bayesian bounds that can be directly optimized."
INTRODUCTION,0.006552006552006552,"Existing pessimistic OPE, OPS, and OPL approaches involve analyzing the concentration properties
of a pre-defined risk estimator, often chosen to simplify the analysis. We propose a different approach:
we derive general concentration bounds applicable to a broad class of regularized IPS estimators and
then identify the estimator within this class that achieves the tightest concentration bound. This leads
to a tailored estimator, named Logarithmic Smoothing (LS). LS enjoys several desirable properties.
It concentrates at a sub-Gaussian rate, and has a finite variance without being necessarily bounded.
Its concentration upper bound allows us to evaluate the worst-case risk of any policy, enables us to
derive a simple OPS strategy that directly minimizes our estimator akin to Gabbianelli et al. [21],
and achieves state-of-the-art learning guarantees for OPL when analyzed within the PAC-Bayesian
framework akin to [35, 49, 5, 7, 21]."
INTRODUCTION,0.007371007371007371,"This paper is structured as follows. Section 2 introduces the necessary background. In Section 3,
we provide unified risk bounds for a broad class of regularized IPS estimators, for which LS enjoys
the tightest upper bound. In Section 4, we analyze LS for OPS and OPL, and we further extend
the analysis within the PAC-Bayesian framework. Extensive experiments in Section 5 highlight the
favorable performance of LS, and Section 6 provides concluding remarks."
SETTING AND BACKGROUND,0.00819000819000819,"2
Setting and background"
SETTING AND BACKGROUND,0.009009009009009009,"Offline contextual bandit. Let X ⊂Rd be the context space, which is a compact subset of Rd,
and let A = [K] be a finite action set. An agent’s actions are guided by a stochastic and stationary
policy π ∈Π within a policy space Π. Given a context x ∈X, π(·|x) is a probability distribution
over the action set A; π(a|x) is the probability that the agent selects action a in context x. Then,
an agent interacts with a contextual bandit over n rounds. In round i ∈[n], the agent observes a
context xi ∼ν where ν is a distribution with support X. After this, the agent selects an action
ai ∼π0(·|xi), where π0 is the behavior policy of the agent. Finally, the agent receives a stochastic
cost ci ∈[−1, 0] that depends on the observed context xi and the taken action ai. This cost ci is
sampled from a cost distribution p(·|xi, ai). This leads to n-sized logged data, Dn = (xi, ai, ci)i∈[n],
where tuples (xi, ai, ci) for i ∈[n] are i.i.d. The expected cost of taking action a in context x is
c(x, a) = Ec∼p(·|x,a) [c], and the costs are negative because they are interpreted as the negative of
rewards. The performance of a policy π ∈Π is evaluated through its risk, which aggregates the
expected costs c(x, a) over all possible contexts x ∈X and taken actions a ∈A by policy π, such as"
SETTING AND BACKGROUND,0.009828009828009828,"R(π) = Ex∼ν,a∼π(·|x),c∼p(·|x,a) [c] = Ex∼ν,a∼π(·|x) [c(x, a)] .
(1)"
SETTING AND BACKGROUND,0.010647010647010647,"The main goal is to use logged dataset Dn to enhance future decision-making without necessitating
live deployments. This often entails three tasks: OPE, OPS, and OPL. First, OPE is concerned"
SETTING AND BACKGROUND,0.011466011466011465,"with constructing an estimator ˆRn(π) of the risk R(π) of a fixed target policy π and study its
deviation, aspiring for ˆRn(π) to concentrate well around R(π). Second, OPS focuses on selecting
the best performing policy ˆπS
n from a predefined and finite collection of target policies {π1, . . . , πm},
effectively seeking to determine argmink∈[m] R(πk). Third, OPL aims to find a policy ˆπL
n within
the potentially infinite policy space Π that achieves the lowest risk, essentially aiming to find
argminπ∈Π R(π). In general, both OPS and OPL rely on OPE’s initial estimation of the risk."
SETTING AND BACKGROUND,0.012285012285012284,"Regularized IPS. Our work focuses on the inverse propensity scoring (IPS) estimator [24]. IPS
approximates the risk of a policy π, R(π), by adjusting the contribution of each sample in logged
data according to its importance weight (IW), which is the ratio of the probability of an action under
the target policy π to its probability under the behavior policy π0,"
SETTING AND BACKGROUND,0.013104013104013105,"ˆRn(π) = 1 n n
X"
SETTING AND BACKGROUND,0.013923013923013924,"i=1
wπ(xi, ai)ci ,
(2)"
SETTING AND BACKGROUND,0.014742014742014743,"where for any (x, a) ∈X × A , wπ(x, a) = π(a|x)/π0(a|x) are the IWs. IPS is unbiased under
the coverage assumption (see for example Owen [39, Chapter 9]). However, it can suffer high
variance, which tends to scale linearly with IWs [57]. This issue becomes pronounced when there is
a significant discrepancy between the target policy π and the behavior policy π0. To mitigate this,
a common strategy consists in applying a regularization function h : [0, 1]2 × [−1, 0] →(−∞, 0]
to π(a|x), π0(a|x) and c. This function is designed to reduce the estimator’s variance at the cost of
introducing some bias. Formally, the function h needs to satisfy the condition (C1), that is defined by
h satisfies (C1) ⇐⇒∀(p, q, c) ∈[0, 1]2 × [−1, 0],
pc/q ≤h(p, q, c) ≤0.
(C1)
With such function h, the regularized IPS estimator reads"
SETTING AND BACKGROUND,0.015561015561015561,"ˆRh
n(π) = 1 n n
X"
SETTING AND BACKGROUND,0.01638001638001638,"i=1
h (π(ai|xi), π0(ai|xi), ci) = 1 n n
X"
SETTING AND BACKGROUND,0.0171990171990172,"i=1
hi ,
(3)"
SETTING AND BACKGROUND,0.018018018018018018,"where hi = h (π(ai|xi), π0(ai|xi), ci). We recover standard IPS in (2) when h(p, q, c) = pc/q.
Numerous regularization functions h were studied in the literature. For example,
h(p, q, c) = min(p/q, M)c , M ∈R+ =⇒Clipping [10] ,
(4)
h(p, q, c) = pc/qα , α ∈[0, 1] =⇒Exponential Smoothing [5] ,
h(p, q, c) = pc/(q + γ) , γ ≥0 =⇒Implicit Exploration [21] .
Other IW regularizations include Harmonic [38] and Shrinkage [54]. With h satisfying (C1), we can
derive our core result: a family of high-probability bounds that hold for regularized IPS."
PESSIMISTIC OFF-POLICY EVALUATION,0.018837018837018837,"3
Pessimistic off-policy evaluation"
PESSIMISTIC OFF-POLICY EVALUATION,0.019656019656019656,"Standard OPE directly uses estimates of the risk, without capturing their associated uncertainty. This
limits its effectiveness in critical applications. Pessimistic OPE addresses this issue by relying on
finite sample, high-probability upper bounds to assess any policy’s worst-case risk [15, 32]. This
section contributes to this effort and focuses on providing novel, finite sample, tight upper bounds on
the risk. This is achieved by deriving general bounds applicable to regularized IPS in (3)."
PRELIMINARIES AND UNIFIED RISK BOUNDS,0.020475020475020474,"3.1
Preliminaries and unified risk bounds"
PRELIMINARIES AND UNIFIED RISK BOUNDS,0.021294021294021293,"Let λ > 0, π ∈Π, and h satisfying (C1), we define"
PRELIMINARIES AND UNIFIED RISK BOUNDS,0.022113022113022112,"ˆ
Mh,ℓ
n (π) = 1 n n
X"
PRELIMINARIES AND UNIFIED RISK BOUNDS,0.02293202293202293,"i=1
hℓ
i ,
and
ψλ(x) = 1"
PRELIMINARIES AND UNIFIED RISK BOUNDS,0.02375102375102375,"λ (1 −exp(−λx)) , ∀x ∈R ,
(5)"
PRELIMINARIES AND UNIFIED RISK BOUNDS,0.02457002457002457,"where
ˆ
Mh,ℓ
n (π) is the empirical ℓ-th moment of regularized IPS ˆRh
n(π), and ψλ : R →R is a
contraction function satisfying ψλ(x) ≤x for any x ∈R. Then, we state our first result.
Proposition 1 (Empirical moments risk bound). Let π ∈Π, L ≥1, δ ∈(0, 1], λ > 0, and h
satisfying (C1). Then it holds with probability at least 1 −δ that"
PRELIMINARIES AND UNIFIED RISK BOUNDS,0.025389025389025387,"R(π) ≤U λ,h
L (π) ,
with
U λ,h
L (π) = ψλ

ˆRh
n(π) +"
"L
X",0.02620802620802621,"2L
X ℓ=2 λℓ−1"
"L
X",0.02702702702702703,"ℓ
ˆ
Mh,ℓ
n (π) + ln(1/δ) λn"
"L
X",0.027846027846027847,"
,
(6)"
"L
X",0.028665028665028666,"where ψλ and ˆ
Mh,ℓ
n (π) are both defined in (5), and recall that ψλ(x) ≤x."
"L
X",0.029484029484029485,"In Appendix F.1, we provide detailed proof, leveraging Chernoff bounds with a careful analysis of the
moment-generating function. This results in the first empirical, high-order moment bound for offline
contextual bandits, with several advantages. First, the bound applies to any regularization function h
that satisfies the mild condition (C1), enabling the design of a tailored h that minimizes the bound.
Second, it relies solely on empirical moments, without assuming the existence of theoretical moments.
Third, the bound is fully empirical and tractable, facilitating efficient implementation of pessimism.
Lastly, the parameter L controls the number of moments used, allowing a balance between bound
tightness and computational cost. Specifically, for sufficiently small values of λ, higher values of
L yield tighter bounds, though potentially at the cost of increased computational complexity as we
would need to compute higher order moments. This is formally stated as follows.
Proposition 2 (Impact of L). Let π ∈Π, δ ∈(0, 1], λ > 0, L ≥1, and h satisfying (C1). Then,"
"L
X",0.030303030303030304,"λ ≤min
i∈[n]"
"L
X",0.031122031122031123,"
2L + 2
(2L + 1)|hi|"
"L
X",0.03194103194103194,"
=⇒U λ,h
L+1(π) ≤U λ,h
L (π) .
(7)"
"L
X",0.03276003276003276,"From (7), the bound U λ,h
L (π) in (6) becomes a decreasing function of L when λ ≤mini∈[n](1/|hi|),
suggesting that for sufficiently small λ, the tightest bound is achieved as L →∞. This condition on
λ also depends on the values of h, highlighting the importance of the regularizer choice h. In fact,
once we evaluate our bounds at their optimal regularizer function h, this condition on λ becomes
unnecessary when comparing some of the optimal bounds. Specifically, we demonstrate in the
following proposition that the bound with L = 1 can be always improved by increasing L.
Proposition 3 (Comparison of our bounds). Let π ∈Π, and λ > 0, we define"
"L
X",0.03357903357903358,"U λ
L(π) = min
h U λ,h
L (π) ,
and
h∗,L = argmin
h
U λ,h
L (π) ,
(8)"
"L
X",0.0343980343980344,"with the minimum taken over h satisfying (C1). Then, for any λ > 0, it holds that for any L > 1,
U λ
L(π) ≤U λ
1 (π). In particular, for any λ > 0,"
"L
X",0.03521703521703522,"U λ
∞(π) ≤U λ
1 (π) .
(9)"
"L
X",0.036036036036036036,"Proposition 3 shows that, irrespective of the value of λ, the bound with L = 1 can be always improved
by bounds of increased moment order L, evaluated at their optimal regularizer h∗,L. This result
encourages us to study bounds with high moment order L, especially if we can derive their optimal
regularizers h∗,L. To this end, we examine two cases: L = 1, which results in an empirical second-
moment bound, and L →∞, yielding a tight bound that does not require computing high-order
moments. For each case, we identify the function h that minimizes the bound. If the minimizer for
L = 1 is a variant of the clipping estimator [10], minimizing L →∞motivates a novel logarithmic
smoothing estimator. We begin by analyzing our empirical moment risk bound at L = 1."
GLOBAL CLIPPING,0.036855036855036855,"3.2
Global clipping"
GLOBAL CLIPPING,0.03767403767403767,"Corollary 4 (Empirical second-moment risk bound with L = 1). Let π ∈Π, δ ∈(0, 1], λ > 0, and
h satisfying (C1). Then it holds with probability at least 1 −δ that"
GLOBAL CLIPPING,0.03849303849303849,"R(π) ≤ψλ

ˆRh
n(π) + λ"
GLOBAL CLIPPING,0.03931203931203931,"2
ˆ
Mh,2
n (π) + ln(1/δ) λn"
GLOBAL CLIPPING,0.04013104013104013,"
.
(10)"
GLOBAL CLIPPING,0.04095004095004095,"This is a direct consequence of (6) when L = 1. The bound holds for any h satisfying (C1). Thus we
search for a function h∗,1 that minimizes bound in (10). This function h∗,1 writes
h∗,1(p, q, c) = −min(p|c|/q, 1/λ) .
(11)
In particular, if we assume that costs are binary, c ∈{−1, 0}, then h∗,1 corresponds to clipping
in (4) with parameter M = 1/λ. This is because −min(|c|p/q, 1/λ) = min
 
p/q, 1"
GLOBAL CLIPPING,0.04176904176904177,"λ

c when c is
binary. This motivates the widely used clipping estimator [10]. However, this also suggests that the
standard way of clipping (as in (4)) is only optimal1 for binary costs. In general, the cost should also
be clipped (as in (11)). Finally, with a suitable choice of λ = O(1/√n), our bound in Corollary 4,
using clipping (i.e., h = h∗,1), outperforms the existing empirical Bernstein bound [55], which was
specifically derived for clipping. This confirms the strength of our general bound, as minimizing it
results in a bound with tighter concentration than specialized bounds. Appendix F.4 gives the the
proof to find h∗,1 and formal comparisons with empirical Bernstein are provided in Appendix F.5. In
the next section, we study our general bound when we set L →∞."
GLOBAL CLIPPING,0.042588042588042586,"1Here, optimality of a function h is defined with respect to our bound with L = 1 (Corollary 4)."
LOGARITHMIC SMOOTHING,0.043407043407043405,"3.3
Logarithmic smoothing"
LOGARITHMIC SMOOTHING,0.044226044226044224,"Corollary 5 (Empirical infinite-moment bound with L →∞). Let π ∈Π, δ ∈(0, 1], λ > 0, and h
satisfying (C1). Then it holds with probability at least 1 −δ that"
LOGARITHMIC SMOOTHING,0.04504504504504504,"R(π) ≤ψλ

−1 n n
X i=1"
LOGARITHMIC SMOOTHING,0.04586404586404586,"1
λ log (1 −λhi) + ln(1/δ) λn"
LOGARITHMIC SMOOTHING,0.04668304668304668,"
.
(12)"
LOGARITHMIC SMOOTHING,0.0475020475020475,"Appendix F.6 provides detailed proof. Setting L →∞in (6) results in the bound in Corollary 5,
which has different properties than Corollary 4. The resulting bound has a simple expression that does
not require computing high order moments. This means that we can obtain the best of both worlds, a
tight concentration bound with no additional computational complexity. As the bound is increasing
in h, the function h∗,∞that minimizes this bound is h∗,∞(p, q, c) = pc/q. This corresponds to the
standard IPS in (2). This differs from the L = 1 bound in Corollary 4 that favored clipping. This
shows the impact of the moment order L on the optimal function h. For any π ∈Π, applying the
bound in Corollary 5 with the optimal h∗,∞leads to U λ
∞(π), of the following expression:"
LOGARITHMIC SMOOTHING,0.04832104832104832,"U λ
∞(π) = ψλ

ˆRλ
n(π) + ln(1/δ) λn"
LOGARITHMIC SMOOTHING,0.04914004914004914,"
.
(13)"
LOGARITHMIC SMOOTHING,0.049959049959049956,"Even if we set h∗,∞(p, q, c) = pc/q (without IW regularization), U λ
∞(π) can be seen as a risk upper
bound of a novel regularized IPS estimator (satisfying (C1)), called Logarithmic Smoothing (LS):"
LOGARITHMIC SMOOTHING,0.050778050778050775,"ˆRλ
n(π) = −1 n n
X i=1"
LOGARITHMIC SMOOTHING,0.051597051597051594,"1
λ log (1 −λwπ(xi, ai)ci) .
(14)"
LOGARITHMIC SMOOTHING,0.05241605241605242,"0
10
20
30
40
50
Importance Weight w¼"
LOGARITHMIC SMOOTHING,0.05323505323505324,"IPS
Clipping, M = 20
LS, ¸ = 0.1"
LOGARITHMIC SMOOTHING,0.05405405405405406,"LS, ¸ = 0.05"
LOGARITHMIC SMOOTHING,0.054873054873054876,"LS, ¸ = 0.01"
LOGARITHMIC SMOOTHING,0.055692055692055695,Figure 1: LS with different λs.
LOGARITHMIC SMOOTHING,0.056511056511056514,"The LS estimator in (14) is defined for any non-negative λ ≥0,
with its bound in (13) holding for any positive λ > 0. No-
tably, λ = 0 retrieves the standard IPS estimator in (2), while
λ > 0 introduces a bias-variance trade-off by logarithmically
smoothing the IWs (Figure 1). This estimator acts as a soft,
differentiable variant of clipping with parameter 1/λ. A Taylor
expansion of our estimator around λ = 0 yields"
LOGARITHMIC SMOOTHING,0.05733005733005733,"ˆRλ
n(π) = ˆRn(π) + ∞
X ℓ=2 λℓ−1 ℓ  1 n n
X"
LOGARITHMIC SMOOTHING,0.05814905814905815,"i=1
(wπ(xi, ai)ci)ℓ
."
LOGARITHMIC SMOOTHING,0.05896805896805897,"Thus, LS is a pessimistic estimator by design, implicitly im-
plementing a form of Sample All Moments Penalization, which generalizes the Sample Variance
Penalization [55]. To examine the statistical properties of our estimator, we introduce"
LOGARITHMIC SMOOTHING,0.05978705978705979,"Sλ(π) = E

(wπ(x, a)c)2"
LOGARITHMIC SMOOTHING,0.06060606060606061,"(1 −λwπ(x, a)c)"
LOGARITHMIC SMOOTHING,0.06142506142506143,"
,
(15)"
LOGARITHMIC SMOOTHING,0.062244062244062245,"which quantifies the discrepancy between π and π0. Notably, Sλ(π) is always smaller than the
second moment of the IW, effectively interpolating between a weighted first moment (λ ≫1) and the
second moment (λ = 0) of IPS. This quantity Sλ characterizes the concentration properties of the LS
estimator akin to the coverage ratio for IX estimator [21]. With Sλ defined, we proceed by bounding
the mean squared error (MSE) of our estimator, specifically bounding its bias and variance.
Proposition 6 (Bias-variance trade-off). Let π ∈Π and λ ≥0. Let Bλ(π) and Vλ(π) be respectively
the bias and the variance of the LS estimator. Then we have that"
LOGARITHMIC SMOOTHING,0.06306306306306306,"0 ≤Bλ(π) ≤λSλ(π) ,
and
Vλ(π) ≤Sλ(π) n
."
LOGARITHMIC SMOOTHING,0.06388206388206388,"Moreover, it holds that for any λ > 0, the variance is finite as Vλ(π) ≤|R(π)|/λn ≤1/λn."
LOGARITHMIC SMOOTHING,0.0647010647010647,"We observe that both the bias and variance are controlled by Sλ(π). Particularly, λ = 0 recovers the
IPS estimator in (2), with zero bias and a variance bounded by E

w2(x, a)c2
/n. When λ > 0, a
bias-variance trade-off emerges. The bias is always non-negative and is capped at λSλ(π), which
diminishes to zero when λ is small and goes to |R(π)| as λ increases. Conversely, the variance
decreases with a higher λ. Notably, λ > 0 ensures finite variance bounded by 1/λn, despite
the estimator being unbounded. This is different from previous estimators that relied on bounded
functions to ensure finite variance. We also prove in the following that a good choice of λ = O(1/√n)
ensures that our LS estimator enjoys a sub-Gaussian concentration [38]."
LOGARITHMIC SMOOTHING,0.06552006552006552,"Proposition 7 (Sub-Gaussianity and comparison with Metelli et al. [38]). Let π ∈Π, δ ∈(0, 1] and
λ > 0. Then the following inequalities holds with probability at least 1 −δ:"
LOGARITHMIC SMOOTHING,0.06633906633906633,"R(π) −ˆRλ
n(π) ≤ln(2/δ)"
LOGARITHMIC SMOOTHING,0.06715806715806716,"λn
,
and
ˆRλ
n(π) −R(π) ≤λSλ(π) + ln(2/δ) λn
."
LOGARITHMIC SMOOTHING,0.06797706797706797,"In particular, setting λ = λ∗=
p"
LOGARITHMIC SMOOTHING,0.0687960687960688,"ln(2/δ)/nE [wπ(x, a)2c2] yields that"
LOGARITHMIC SMOOTHING,0.06961506961506962,"|R(π) −ˆRλ∗
n (π)| ≤
p"
LOGARITHMIC SMOOTHING,0.07043407043407043,"2σ2 ln(2/δ) ,
where σ2 = 2E

wπ(x, a)2c2
/n .
(16)"
LOGARITHMIC SMOOTHING,0.07125307125307126,"Thus, a particular choice of λ∗ensures that ˆRλ∗
n (π) is sub-Gaussian, with a variance proxy σ2 that
improves on that obtained for the Harmonic estimator of Metelli et al. [38]. We refer the interested
reader to Appendix E.2 for further discussions and proofs."
LOGARITHMIC SMOOTHING,0.07207207207207207,"Next, we focus on the tightness of the LS upper bound in (13) as it will motivate our selection and
learning strategies. Proposition 3 already showed that U λ
∞(π), the bound of LS is tighter than U λ
1 (π),
the bound in Corollary 4 evaluated at the Global clipping function h∗,1. In this section, we compare
the LS bound to the already tight IX bound presented by Gabbianelli et al. [21] and demonstrate in
the following that the LS bound dominates it in all scenarios.
Proposition 8 (Comparison with IX of Gabbianelli et al. [21]). Let π ∈Π, δ ∈]0, 1] and λ > 0, the
IX bound from [21] states that we have with probability at least 1 −δ"
LOGARITHMIC SMOOTHING,0.0728910728910729,"R(π) ≤ˆRλ-IX
n
(π) + ln(1/δ)"
LOGARITHMIC SMOOTHING,0.07371007371007371,"λn
,
with
ˆRλ-IX
n
(π) = 1 n n
X i=1"
LOGARITHMIC SMOOTHING,0.07452907452907453,"π(ai|xi)
π0(ai|xi) + λ/2ci.
(17)"
LOGARITHMIC SMOOTHING,0.07534807534807535,"Let U λ
IX(π) be the upper bound of (17), we have for any λ > 0:"
LOGARITHMIC SMOOTHING,0.07616707616707617,"U λ
∞(π) ≤U λ
IX(π) .
(18)"
LOGARITHMIC SMOOTHING,0.07698607698607698,"This result states that no matter the scenario, for any evaluated policy π, and any chosen λ > 0, the
LS bound will be always tighter than IX. The gap between the LS and IX bounds increases when n is
small, or when the evaluated policy π is stochastic, as demonstrated and developed in Appendix F.8.
These findings further validate the effectiveness of our approach, enabling us to identify the LS
estimator, with an empirical bound that improves upon the tightest existing bounds. Consequently,
we leverage the LS bound in the next section to derive our pessimistic OPS and OPL strategies."
OFF-POLICY SELECTION AND LEARNING,0.07780507780507781,"4
Off-policy selection and learning"
OFF-POLICY SELECTION,0.07862407862407862,"4.1
Off-policy selection"
OFF-POLICY SELECTION,0.07944307944307945,"Let ΠS = {π1, ..., πm} be a finite set of policies. In OPS, the goal is to find πS
∗∈ΠS that satisfies"
OFF-POLICY SELECTION,0.08026208026208026,"πS
∗= argmin
π∈ΠS
R(π) = argmin
k∈[m]
R(πk) .
(19)"
OFF-POLICY SELECTION,0.08108108108108109,"As we do not have access to the true risk, we use a data-driven selection strategy that guarantees the
identification of policies of performance close to that of πS
∗. Precisely, for λ > 0, we search for"
OFF-POLICY SELECTION,0.0819000819000819,"ˆπS
n = argmin
π∈ΠS
ˆRλ
n(π) = argmin
k∈[m]
ˆRλ
n(πk) .
(20)"
OFF-POLICY SELECTION,0.08271908271908272,"To derive our strategy in (20), we minimize the bound of LS in (13), employing pessimism [27].
Fortunately, in our case, this boils down to minimizing ˆRλ
n(π), since the other terms in the bound are
independent of the target policy π. This allows us to avoid computing complex statistics [55, 32] and
does not require access to the behavior policy π0. As we show next, it also ensures low suboptimality.
Proposition 9 (Suboptimality of our selection strategy in (20)). Let λ > 0 and δ ∈(0, 1]. Then, it
holds with probability at least 1 −δ that"
OFF-POLICY SELECTION,0.08353808353808354,"0 ≤R(ˆπS
n) −R(πS
∗) ≤λSλ(πS
∗) + 2 ln(2|ΠS|/δ)"
OFF-POLICY SELECTION,0.08435708435708436,"λn
,
(21)"
OFF-POLICY SELECTION,0.08517608517608517,"where Sλ(π), πS
∗and ˆπS
n are defined in (15), (19) and (20)."
OFF-POLICY SELECTION,0.085995085995086,"The derived suboptimality bound only requires coverage of the optimal actions (support of the optimal
policy πs
∗), and improves on IX suboptimality [21], matching the minimax suboptimality lower bound
of pessimistic methods [34, 27, 28]. Appendix G.1 provides proof of this suboptimality bound, and
we discuss how this suboptimality improves upon existing strategies in Appendix E.3. By selecting
λs
n =
p"
OFF-POLICY SELECTION,0.08681408681408681,"2 ln(2|ΠS|/δ)/n for LS, we achieve a suboptimality scaling of O(1/√n),"
OFF-POLICY SELECTION,0.08763308763308764,"0 ≤R(ˆπS
n) −R(πS
∗) ≤
 
1 + Sλsn(πS
∗)
 p"
OFF-POLICY SELECTION,0.08845208845208845,"2 ln(2|ΠS|/δ)/n,
(22)"
OFF-POLICY SELECTION,0.08927108927108927,"which ensures finding the optimal policy with sufficient samples. Additionally, the multiplicative
constant is smaller when π0 is close to πS
∗, confirming the known observation that it is easier to
identify the best policy if it is similar to the behavior policy π0."
OFF-POLICY LEARNING,0.09009009009009009,"4.2
Off-policy learning"
OFF-POLICY LEARNING,0.09090909090909091,"Similar to how we extended the evaluation bound in Corollary 5 (which applies to a single fixed
target policy) to OPS (where it applies to a finite set of target policies), we can further derive bounds
for an infinite policy class Π, enabling OPL. Several approaches have been proposed in previous
work, primarily based on replacing the finite union bound over policies with more sophisticated
uniform-convergence arguments. This was used by [55], which derived a variance-sensitive bound
scaling with the covering number [61]. Since these approaches incorporate a complexity term that
depends only on the policy class, the resulting pessimistic learning strategy (which minimizes the
upper bound) would be similar to the selection strategy adopted earlier, leading, for a fixed λ, to"
OFF-POLICY LEARNING,0.09172809172809172,"ˆπL
n = argmin
π∈Π
ˆRλ
n(π) + C(Π)"
OFF-POLICY LEARNING,0.09254709254709255,"λn
= argmin
π∈Π
ˆRλ
n(π).
(23)"
OFF-POLICY LEARNING,0.09336609336609336,"where C(Π) is a complexity measure [61]. This learning strategy is straightforward because it involves
a smooth estimator that can be optimized using first-order methods and does not require second-order
statistics. However, analyzing this approach is more challenging because the complexity measure
C(Π) varies depending on the policy class considered, is often intractable [49] and can only be upper
bounded with problem dependent constants [28]."
OFF-POLICY LEARNING,0.09418509418509419,"Instead of the method described above, we derive PAC-Bayesian generalization bounds [37, 11] that
apply to arbitrary policy classes. This framework has been shown to provide strong performance
guarantees for OPL in practical scenarios [49, 5]. The PAC-Bayesian framework analyzes the
performance of policies by viewing them as randomized predictors [35]. Specifically, let F(Θ) =
{fθ : X →[K], θ ∈Θ} be a set of parameterized predictors that associate the context x with the
action fθ(x) ∈[K]. Let P(Θ) be the set of all probability distributions on Θ. Each distribution
Q ∈P(Θ) defines a policy πQ by setting the probability of action a given context x as the probability
that a random predictor fθ ∼Q maps x to action a, that is,"
OFF-POLICY LEARNING,0.095004095004095,"πQ(a|x) = Eθ∼Q [1 [fθ(x) = a]] ,
∀(x, a) ∈X × A .
(24)"
OFF-POLICY LEARNING,0.09582309582309582,"This characterization is not restrictive as any policy can be represented in this form [49]. Deriving
PAC-Bayesian generalization bounds with this policy definition requires the regularized IPS to be
linear in the target policy π [35, 5, 21]. Our estimator LS in (14) is non-linear in π. Therefore, for
this PAC-Bayesian analysis, we introduce a linearized variant of LS, called LS-LIN, and defined as"
OFF-POLICY LEARNING,0.09664209664209664,"ˆRλ-LIN
n
(π) = −1 n n
X i=1"
OFF-POLICY LEARNING,0.09746109746109746,π(ai|xi)
OFF-POLICY LEARNING,0.09828009828009827,"λ
log

1 −
λci
π0(ai|xi)"
OFF-POLICY LEARNING,0.0990990990990991,"
,
(25)"
OFF-POLICY LEARNING,0.09991809991809991,"which smooths the impact of the behavior propensity π0 instead of the IWs π/π0. We provide in the
following a core result of this section, the PAC-Bayesian bound that defines our learning strategy."
OFF-POLICY LEARNING,0.10073710073710074,"Proposition 10 (PAC-Bayes learning bound for ˆRλ-LIN
n
). Given a prior P ∈P(Θ), δ ∈(0, 1] and
λ > 0, the following holds with probability at least 1 −δ:"
OFF-POLICY LEARNING,0.10155610155610155,"∀Q ∈P(Θ),
R(πQ) ≤ψλ

ˆRλ-LIN
n
(πQ) + KL(Q||P) + ln 1 δ
λn"
OFF-POLICY LEARNING,0.10237510237510238,"
,
(26)"
OFF-POLICY LEARNING,0.10319410319410319,where KL(Q||P) is the Kullback-Leibler divergence from P to Q.
OFF-POLICY LEARNING,0.10401310401310401,"PAC-Bayes bounds hold uniformly for all distributions Q ∈P(Θ) and replace the complexity
measure C(Π) with the divergence KL(Q||P) from a reference prior distribution P. Extensive
research focuses on identifying the best strategies for choosing this prior P [40]. While these bounds
hold for any fixed prior P, in practice, it is typically set to the distribution inducing the behavior
policy π0, meaning P satisfies π0 = πP . This leads to an intuitive learning principle: by minimizing
the upper bound, we seek policies with good empirical risk that do not deviate significantly from π0."
OFF-POLICY LEARNING,0.10483210483210484,"Our bound can also be obtained using the truncation method from Alquier [1, Corollary 2.5]. This
bound surpasses the already tight PAC-Bayesian bounds derived for Clipping [49], Exponential
Smoothing [5], and Implicit Exploration [21], resulting in the tightest known generalization bound in
OPL. Appendix G.2 gives formal proof of this bound and comparisons with existing PAC-Bayesian
bounds can be found in Appendix E.4. For a fixed λ and a fixed prior P, we derive a learning strategy
that minimizes the upper bound for a subset L(Θ) ⊆P(Θ) of distributions, seeking"
OFF-POLICY LEARNING,0.10565110565110565,"Qn = argmin
Q∈L(Θ)"
OFF-POLICY LEARNING,0.10647010647010648,"
ˆRλ-LIN
n
(πQ) + KL(Q||P) λn"
OFF-POLICY LEARNING,0.10728910728910729,"
,
and setting ˆπL
n = πQn .
(27)"
OFF-POLICY LEARNING,0.10810810810810811,"(27) is tractable and can be efficiently optimized for various policy classes [49, 5]. Below, we analyze
its suboptimality compared to the best policy in the chosen class, πQ∗= argminQ∈L(Θ) R(πQ)."
OFF-POLICY LEARNING,0.10892710892710893,"Proposition 11 (Suboptimality of the learning strategy in (27)). Let λ > 0, P ∈L(Θ) and δ ∈(0, 1].
Then, it holds with probability at least 1 −δ that"
OFF-POLICY LEARNING,0.10974610974610975,"0 ≤R(ˆπL
n) −R(πQ∗) ≤λS LIN
λ (πQ∗) + 2 (KL(Q∗||P) + ln(2/δ))"
OFF-POLICY LEARNING,0.11056511056511056,"λn
,
(28)"
OFF-POLICY LEARNING,0.11138411138411139,"where S LIN
λ (π) = E

π(a|x)c2/(π2
0(a|x) −λπ0(a|x)c)

and ˆπL
n is defined in (27)."
OFF-POLICY LEARNING,0.1122031122031122,"Our suboptimality bound only requires coverage of the support of the optimal policy πQ∗. This
bound matches the minimax suboptimality lower bound of pessimistic learning with deterministic
policies [28]. Appendix G.3 provides a proof of Proposition 11, while Appendix E.5 discusses the
suboptimality bound further and proves that it improves on the IX learning strategy of [21, Section 5].
Setting λl
n = 2/√n guarantees us a suboptimality that scales with O(1/√n) as"
OFF-POLICY LEARNING,0.11302211302211303,"0 ≤R(ˆπL
n) −R(πQ∗) ≤(2S LIN
λln (πQ∗) + KL(Q∗||P) + ln(2/δ))/√n."
OFF-POLICY LEARNING,0.11384111384111384,"By setting the reference P to the distribution inducing π0, we find that the learning suboptimality
is reduced when the behavior policy π0 is close to the optimal policy πQ∗. This is similar to the
suboptimality for our selection strategy. The suboptimality upper bound reflects a common intuition
in the OPL literature: pessimistic learning algorithms converge faster when π0 is close to πQ∗."
EXPERIMENTS,0.11466011466011466,"5
Experiments"
EXPERIMENTS,0.11547911547911548,"Our experimental setup follows the standard multiclass-to-bandit conversion used in prior studies
[18, 55]. Each multi-class dataset has features and labels and we convert it to contextual bandit
problems where contexts correspond to features and actions to labels. Precisely, the reward r for
taking action (label) a with context (features) x is modeled as Bernoulli with probability px =
ϵ + 1 [a = ρ(x)] (1 −2ϵ), where ρ(x) be the true label of features x, and ϵ is a noise parameter. In
particular, the true label ρ(x) represents the action with the highest average reward for context x. This
setup ensures an average reward of 1 −ϵ for the optimal action ρ(x) and ϵ for all others, constructing
a logged bandit feedback dataset in the form {xi, ai, ci}i∈[n], where ci = −ri is the associated cost."
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.1162981162981163,"5.1
Off-policy evaluation and selection experiments"
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.11711711711711711,"For both evaluation and selection, we adopt the same experimental design as [32] to facilitate the
comparison. We consider exponential target policies π(a|x) ∝exp( 1"
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.11793611793611794,"τ f(a, x)), with τ a temperature
controlling the policy’s entropy and f(a, x) the score of the item a for the context x. We use this
to define ideal policies as πideal(a|x) ∝exp( 1"
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.11875511875511875,"τ I{ρ(x) = a}), and also create faulty, mismatching
policies for which the peak is shifted to another, wrong action for a set of faulty actions F ⊂[K]. To
recreate real world scenarios, we also consider policies directly learned from logged bandit feedback,
of the form πθIPS(a|x) ∝exp( 1"
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.11957411957411958,"τ xtθIPS
a ) and πθSN(a|x) ∝exp( 1"
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.12039312039312039,"τ xtθSN
a ), with their parameters learned"
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.12121212121212122,Table 1: Bound’s tightness (|U(π)/R(π) −1|) with varying number of samples of the kropt dataset.
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.12203112203112203,"Number of samples
SN-ES
cIPS-EB
IX
cIPS-L=1 (Ours)
LS (Ours)"
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.12285012285012285,"28
1.000
0.917
0.373
0.364
0.362
29
1.000
0.732
0.257
0.289
0.236
210
0.794
0.554
0.226
0.240
0.213
211
0.649
0.441
0.171
0.197
0.159
212
0.472
0.327
0.126
0.147
0.117
213
0.374
0.204
0.062
0.077
0.054
214
0.257
0.138
0.041
0.049
0.035"
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.12366912366912367,"by respectively minimizing the IPS [24] and SN [56] empirical risks. More details on the definition
of the different policies are given in Appendix H. Finally, 11 real multiclass classification datasets are
chosen from the UCI ML Repository [8] (See Table 3 in Appendix H.1.1) with various number of
samples, dimensions and action space sizes to conduct our experiments2."
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.12448812448812449,"(OPE) Tightness of the bounds. Evaluating the worst case performance of a policy is done through
evaluating risk upper bounds [10, 32]. This means that a better evaluation will solely depend on the
tightness of the bounds used. To this end, given a policy π, we are interested in bounds U(π) with a
small relative radius |U(π)/R(π) −1|. We compare our newly derived bounds (cIPS-L=1 for U λ
1
and LS for U λ
∞both with λ = 1/√n) to empirical evaluation bounds of the literature: SN-ES: the
Efron Stein bound for Self Normalized IPS [32], cIPS-EB: Empirical Bernstein for Clipping [55]
and the recent IX: Implicit Exploration bound [21]. The first experiment uses the kropt dataset
with ϵ = 0.2, collects bandit feedback with faulty behavior policy (with τ = 0.25) to evaluate
an ideal policy (τ = 0.1), and explores how the relative radiuses of the considered bounds shrink
while varying the number of datapoints. Table 1 compiles the results of the experiments and suggest
that the LS bound is tighter than its competitors no matter the size of the feedback collected. The
second experiments uses all 11 datasets, with different behavior policies (τ0 ∈{0.2, 0.25, 0.3})
and different noise levels (ϵ ∈{0., 0.1, 0.2}) to evaluate ideal policies with different temperatures
(τ ∈{0.1, 0.2, 0.3, 0.4, 0.5}), defining ∼500 different scenarios to validate our findings. We plot in
Figure 2 the cumulative distribution of the relative radius of the considered bounds. We observe that
while cIPS-L=1 and IX can be comparable, the LS bound is tighter than all its competitors. We also
provide detailed results in Appendix H.1.2 that further confirm the superiority of the LS bound."
OFF-POLICY EVALUATION AND SELECTION EXPERIMENTS,0.12530712530712532,"(OPS) Find the best, avoid the worst policy. Policy selection aims at identifying the best policy
among a set of finite candidates. In practice, we are interested in finding policies that improve on
π0 and avoid policies that perform worse than π0. To replicate real world scenarios, we design
an experiment where π0 is a faulty policy (τ = 0.2), that collects noisy (ϵ = 0.2) interaction
data, some of which is used to learn πθIPS, πθSN, and that we add to our discrete set of policies
Πk=4 = {π0, πideal, πθIPS, πθSN}. The goal is to measure the ability of our selection strategies to
choose from Πk=4, better performing policies than π0. We thus define three possible outcomes:
a strategy can select worse performing policies, better performing or the best policy. Our goal in
these experiments is to empirically validate the pitfalls of point estimators while confirming the
benefits of using the pessimism principle. To this end, we compare pessimistic selection strategies
to policy selection using the classical point estimators IPS [24] and SN [56]. The comparison is
conducted on the 11 UCI datasets with 10 different seeds resulting in 110 scenarios. We plot in
Figure 2 the percentage of time each method selected the best policy, a better or a worse policy than
π0. While risk estimators can identify the best policy, they are unreliable as they can choose worse
performing policies than π0, a catastrophic outcome in critical applications. Pessimistic selection is
more conservative, as it avoids poor performing policies completely and empirically confirms that
tighter upper bounds result in better selection strategies: LS upper bound is less conservative and
finds best policies the most (comparable to SN) while never selecting poor performing policies. Fine
grained results (for each dataset) can be found in Appendix H.1.3."
OFF-POLICY LEARNING EXPERIMENTS,0.12612612612612611,"5.2
Off-policy learning experiments"
OFF-POLICY LEARNING EXPERIMENTS,0.12694512694512694,"We follow the successful off policy learning paradigm based on directly minimizing PAC-Bayesian
risk generalization bounds [49, 5] as it comes with guarantees of improvement and avoids hyper-"
OFF-POLICY LEARNING EXPERIMENTS,0.12776412776412777,2The code can be found at https://github.com/otmhi/offpolicy_ls.
OFF-POLICY LEARNING EXPERIMENTS,0.1285831285831286,"0.0
0.2
0.4
0.6
0.8
1.0
Tightness of the Upper Bound (Relative Radius) 0.0 0.2 0.4 0.6 0.8 1.0"
OFF-POLICY LEARNING EXPERIMENTS,0.1294021294021294,Cumulative Probability
OFF-POLICY LEARNING EXPERIMENTS,0.13022113022113022,OPE: Comparing Tightness of the Bounds of Pessimistic Methods
OFF-POLICY LEARNING EXPERIMENTS,0.13104013104013104,"SN-ES
cIPS-EB
IX
cIPS-L=1 (Ours)
LS (Ours)"
OFF-POLICY LEARNING EXPERIMENTS,0.13185913185913187,"IPS
SN
SN-ES cIPS-EB
IX
cIPS-L=1
LS
Selection Strategy (cIPS-L=1 and LS are Ours) 0 20 40 60 80 100"
OFF-POLICY LEARNING EXPERIMENTS,0.13267813267813267,"Percentage of Worse, Better and Best"
OFF-POLICY LEARNING EXPERIMENTS,0.1334971334971335,OPS: Performance of Selected Policies Compared to Logging Policies
OFF-POLICY LEARNING EXPERIMENTS,0.13431613431613432,"Worse
Better
Best"
OFF-POLICY LEARNING EXPERIMENTS,0.13513513513513514,Figure 2: Results for OPE and OPS experiments.
OFF-POLICY LEARNING EXPERIMENTS,0.13595413595413594,"cIPS
cvcIPS
ES
IX
LS-LIN (Ours)"
OFF-POLICY LEARNING EXPERIMENTS,0.13677313677313677,"rI(U(ˆπL
n))
14.48%
21.28%
7.78%
24.74%
26.31%
rI(R(ˆπL
n))
28.13%
33.64%
29.44%
36.70%
36.76%"
OFF-POLICY LEARNING EXPERIMENTS,0.1375921375921376,Table 2: OPL: Relative Improvement of guaranteed risk and true risk averaged over 200 scenarios.
OFF-POLICY LEARNING EXPERIMENTS,0.13841113841113842,"parameter tuning. For comparable results, we use the same 4 datasets (described in Appendix H.2,
Table 7) as in [49, 5] and adopt the LGP: Linear Gaussian Policies [49] as our class of parametrized
policies. For each dataset, we use behavior policies trained on a small fraction of the data in a super-
vised fashion, combined with different inverse temperature parameters α ∈{0.1, 0.3, 0.5, 0.7, 1.}
to cover cases of diffused and peaked behavior policies. These policies generate for 10 different
seeds, 10 logged bandit feedback datasets resulting in 200 different scenarios to test our learning
approaches. In the PAC-Bayesian OPL paradigm, we minimize the empirical upper bounds U(π)
directly and obtain the learned policy as the bound’s minimizer ˆπL
n (as in (27)). With ˆπL
n obtained, we
are interested in two quantities: The guaranteed risk by the bound, which is the value of the bound
U(ˆπL
n) at its minimizer. This quantity reflects the worst case performance of the learned policy, a
lower value implies stronger performance guarantees. We are also interested in the true risk of the
minimizer of the bound R(ˆπL
n) as it translates the performance of the obtained policy acting on unseen
data. As this learning paradigm is based on optimizing tractable, generalization bounds, we only
compare our approach to methods that provide them. Precisely, we compare our LS-LIN learning
strategy in (27) to strategies based on minimizing off-policy PAC Bayesian bounds from the literature:
clipped IPS (cIPS) and Control Variate clipped IPS (cvcIPS) [49], Exponential Smoothing (ES) [5]
and Implicit Exploration (IX) [21]. The results are summarized in Table 2 where we compute:
rI(x) = (R(π0) −x)/(R(π0) −R(π∗)) = (R(π0) −x)/(R(π0) + 1) ,
the improvement over R(π0) achieved by minimizing the different bounds in terms of x ∈{U, R}
(guaranteed risk and true risk respectively), relative to an ideal improvement. This metric helps us
normalize the results, and we report its average over 200 different scenarios, with results in bold
being significantly better. Fine grained results can be found in Appendix H.2.4. We observe that the
LS-LIN PAC-Bayesian bound improves substantially on its competitors in terms of the guaranteed
risk, and also obtains the best performing policies (on par with the IX PAC-Bayesian bound)."
CONCLUSION,0.13923013923013924,"6
Conclusion"
CONCLUSION,0.14004914004914004,"Motivated by the pessimism principle, we have derived novel, empirical risk upper bounds tailored
for the regularized IPS family of estimators. Minimizing these bounds within this family unveiled
Logarithmic Smoothing, a simple estimator with good concentration properties. With its tight upper
bound, LS confidently evaluates a policy, and shows provably better guarantees for both selecting and
learning policies than all competitors. Our upper bounds remain broadly applicable, only requiring
negative costs. While this condition does not impact importance weighting estimators, it does not
hold for doubly robust estimators. Extending our approach to derive empirical bounds for this type
of estimators presents a nontrivial, yet interesting task to explore in future work. Another potential
extension would be to relax the i.i.d. assumption of the contextual bandit problem to address, the
general offline Reinforcement Learning setting. This direction will introduce a more challenging
estimation task and requires developing new concentration bounds."
REFERENCES,0.14086814086814087,References
REFERENCES,0.1416871416871417,"[1] Pierre Alquier. Transductive and Inductive Adaptative Inference for Regression and Density Esti-
mation. Theses, ENSAE ParisTech, December 2006. URL https://pastel.hal.science/
tel-00119593."
REFERENCES,0.14250614250614252,"[2] Pierre Alquier. User-friendly introduction to PAC-Bayes bounds. Foundations and Trends® in
Machine Learning, 17(2), 2024."
REFERENCES,0.14332514332514332,"[3] Imad Aouali, Amine Benhalloum, Martin Bompaire, Achraf Ait Sidi Hammou, Sergey Ivanov,
Benjamin Heymann, David Rohde, Otmane Sakhi, Flavian Vasile, and Maxime Vono. Reward
Optimizing Recommendation using Deep Learning and Fast Maximum Inner Product Search. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
KDD ’22, page 4772–4773, New York, NY, USA, 2022. Association for Computing Machinery.
ISBN 9781450393850. doi: 10.1145/3534678.3542622. URL https://doi.org/10.1145/
3534678.3542622."
REFERENCES,0.14414414414414414,"[4] Imad Aouali, Achraf Ait Sidi Hammou, Sergey Ivanov, Otmane Sakhi, David Rohde, and
Flavian Vasile. Probabilistic Rank and Reward: A Scalable Model for Slate Recommendation,
2022."
REFERENCES,0.14496314496314497,"[5] Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Exponential Smoothing
for Off-Policy Learning. In Proceedings of the 40th International Conference on Machine
Learning, pages 984–1017. PMLR, 2023."
REFERENCES,0.1457821457821458,"[6] Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Bayesian off-policy
evaluation and learning for large action spaces. arXiv preprint arXiv:2402.14664, 2024."
REFERENCES,0.1466011466011466,"[7] Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Unified PAC-Bayesian
Study of Pessimism for Offline Policy Learning with Regularized Importance Sampling. In The
40th Conference on Uncertainty in Artificial Intelligence, 2024. URL https://openreview.
net/forum?id=d7W4H0sTXU."
REFERENCES,0.14742014742014742,"[8] A. Asuncion and D. J. Newman. UCI machine learning repository, 2007. URL http://www.
ics.uci.edu/$\sim$mlearn/{MLR}epository.html."
REFERENCES,0.14823914823914824,"[9] Heejung Bang and James M Robins. Doubly robust estimation in missing data and causal
inference models. Biometrics, 61(4):962–973, 2005."
REFERENCES,0.14905814905814907,"[10] Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X Charles, D Max Chickering,
Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and
learning systems: The example of computational advertising. Journal of Machine Learning
Research, 14(11), 2013."
REFERENCES,0.14987714987714987,"[11] Olivier Catoni. PAC-Bayesian supervised classification: The thermodynamics of statistical
learning. IMS Lecture Notes Monograph Series, page 1–163, 2007. ISSN 0749-2170. doi: 10.
1214/074921707000000391. URL http://dx.doi.org/10.1214/074921707000000391."
REFERENCES,0.1506961506961507,"[12] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H. Chi. Top-K
Off-Policy Correction for a REINFORCE Recommender System. In Proceedings of the Twelfth
ACM International Conference on Web Search and Data Mining, WSDM ’19, page 456–464,
New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450359405. doi:
10.1145/3289600.3290999. URL https://doi.org/10.1145/3289600.3290999."
REFERENCES,0.15151515151515152,"[13] Victor Chernozhukov, Mert Demirer, Greg Lewis, and Vasilis Syrgkanis. Semi-parametric
efficient policy learning with continuous actions. Advances in Neural Information Processing
Systems, 32, 2019."
REFERENCES,0.15233415233415235,"[14] Matej Cief, Jacek Golebiowski, Philipp Schmidt, Ziawasch Abedjan, and Artur Bekasov.
Learning action embeddings for off-policy evaluation. In European Conference on Information
Retrieval, pages 108–122. Springer, 2024."
REFERENCES,0.15315315315315314,"[15] Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, and Dale Schu-
urmans.
Coindice:
Off-policy confidence interval estimation.
In H. Larochelle,
M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural In-
formation Processing Systems, volume 33, pages 9398–9411. Curran Associates, Inc.,
2020.
URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
6aaba9a124857622930ca4e50f5afed2-Paper.pdf."
REFERENCES,0.15397215397215397,"[16] Miroslav Dudík, John Langford, and Lihong Li. Doubly robust policy evaluation and learning.
In Proceedings of the 28th International Conference on International Conference on Machine
Learning, ICML’11, page 1097–1104, 2011."
REFERENCES,0.1547911547911548,"[17] Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li. Sample-efficient nonstationary
policy evaluation for contextual bandits. In Proceedings of the Twenty-Eighth Conference on
Uncertainty in Artificial Intelligence, UAI’12, page 247–254, Arlington, Virginia, USA, 2012.
AUAI Press."
REFERENCES,0.15561015561015562,"[18] Miroslav Dudik, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation
and optimization. Statistical Science, 29(4):485–511, 2014."
REFERENCES,0.15642915642915642,"[19] Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. In International Conference on Machine Learning, pages 1447–1456.
PMLR, 2018."
REFERENCES,0.15724815724815724,"[20] Hamish Flynn, David Reeb, Melih Kandemir, and Jan Peters. PAC-Bayes Bounds for Bandit
Problems: A Survey and Experimental Comparison. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 45(12):15308–15327, 2023. doi: 10.1109/TPAMI.2023.3305381."
REFERENCES,0.15806715806715807,"[21] Germano Gabbianelli, Gergely Neu, and Matteo Papini. Importance-weighted offline learning
done right. In Proceedings of The 35th International Conference on Algorithmic Learning
Theory, volume 237 of Proceedings of Machine Learning Research, pages 614–634. PMLR,
25–28 Feb 2024. URL https://proceedings.mlr.press/v237/gabbianelli24a.html."
REFERENCES,0.1588861588861589,"[22] Alexandre Gilotte, Clément Calauzènes, Thomas Nedelec, Alexandre Abraham, and Simon
Dollé. Offline A/B testing for recommender systems. In Proceedings of the Eleventh ACM
International Conference on Web Search and Data Mining, pages 198–206, 2018."
REFERENCES,0.1597051597051597,"[23] Torben Hagerup and Christine Rüb. A Guided Tour of Chernoff Bounds. Inf. Process. Lett., 33
(6):305–308, 1990. URL http://dblp.uni-trier.de/db/journals/ipl/ipl33.html#
HagerupR90."
REFERENCES,0.16052416052416052,"[24] Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement
from a finite universe. Journal of the American statistical Association, 47(260):663–685, 1952."
REFERENCES,0.16134316134316135,"[25] Edward L Ionides. Truncated importance sampling. Journal of Computational and Graphical
Statistics, 17(2):295–311, 2008."
REFERENCES,0.16216216216216217,"[26] Olivier Jeunen and Bart Goethals. Pessimistic reward models for off-policy learning in recom-
mendation. In Fifteenth ACM Conference on Recommender Systems, pages 63–74, 2021."
REFERENCES,0.16298116298116297,"[27] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning, pages 5084–5096. PMLR, 2021."
REFERENCES,0.1638001638001638,"[28] Ying Jin, Zhimei Ren, Zhuoran Yang, and Zhaoran Wang. Policy learning ""without” overlap:
Pessimism and generalized empirical Bernstein’s inequality, 2023."
REFERENCES,0.16461916461916462,"[29] Nathan Kallus and Angela Zhou. Policy evaluation and optimization with continuous treatments.
In International conference on artificial intelligence and statistics, pages 1243–1251. PMLR,
2018."
REFERENCES,0.16543816543816545,"[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.16625716625716624,"[31] Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang. Contextual
bandits with continuous actions: Smoothing, zooming, and adapting. Journal of Machine
Learning Research, 21(137):1–45, 2020."
REFERENCES,0.16707616707616707,"[32] Ilja Kuzborskij, Claire Vernade, Andras Gyorgy, and Csaba Szepesvári. Confident off-policy
evaluation and selection through self-normalized importance weighting.
In International
Conference on Artificial Intelligence and Statistics, pages 640–648. PMLR, 2021."
REFERENCES,0.1678951678951679,"[33] Tor Lattimore and Csaba Szepesvari. Bandit Algorithms. Cambridge University Press, 2019."
REFERENCES,0.16871416871416872,"[34] Lihong Li, Remi Munos, and Csaba Szepesvari. Toward Minimax Off-policy Value Estimation.
In Guy Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the Eighteenth International
Conference on Artificial Intelligence and Statistics, volume 38 of Proceedings of Machine
Learning Research, pages 608–616, San Diego, California, USA, 09–12 May 2015. PMLR.
URL https://proceedings.mlr.press/v38/li15b.html."
REFERENCES,0.16953316953316952,"[35] Ben London and Ted Sandler. Bayesian counterfactual risk minimization. In International
Conference on Machine Learning, pages 4125–4133. PMLR, 2019."
REFERENCES,0.17035217035217035,"[36] Andreas Maurer and Massimiliano Pontil. Empirical Bernstein bounds and sample variance
penalization. arXiv preprint arXiv:0907.3740, 2009."
REFERENCES,0.17117117117117117,"[37] David A. McAllester. Some PAC-Bayesian theorems. In Proceedings of the Eleventh Annual
Conference on Computational Learning Theory, COLT’ 98, page 230–234, New York, NY, USA,
1998. Association for Computing Machinery. ISBN 1581130570. doi: 10.1145/279943.279989.
URL https://doi.org/10.1145/279943.279989."
REFERENCES,0.171990171990172,"[38] Alberto Maria Metelli, Alessio Russo, and Marcello Restelli. Subgaussian and differentiable
importance sampling for off-policy evaluation and learning. Advances in Neural Information
Processing Systems, 34:8119–8132, 2021."
REFERENCES,0.17280917280917282,"[39] Art B. Owen. Monte Carlo theory, methods and examples. https://artowen.su.domains/
mc/, 2013."
REFERENCES,0.17362817362817362,"[40] Emilio Parrado-Hernández, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PAC-
Bayes Bounds with Data Dependent Priors. Journal of Machine Learning Research, 13(112):
3507–3531, 2012. URL http://jmlr.org/papers/v13/parrado12a.html."
REFERENCES,0.17444717444717445,"[41] Jie Peng, Hao Zou, Jiashuo Liu, Shaoming Li, Yibao Jiang, Jian Pei, and Peng Cui. Offline
policy evaluation in large action spaces via outcome-oriented action grouping. In Proceedings
of the ACM Web Conference 2023, pages 1220–1230, 2023."
REFERENCES,0.17526617526617527,"[42] James M Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression
models with missing data. Journal of the American Statistical Association, 90(429):122–129,
1995."
REFERENCES,0.1760851760851761,"[43] Noveen Sachdeva, Yi Su, and Thorsten Joachims. Off-policy bandits with deficient support. In
Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pages 965–975, 2020."
REFERENCES,0.1769041769041769,"[44] Noveen Sachdeva, Lequn Wang, Dawen Liang, Nathan Kallus, and Julian McAuley. Off-
policy evaluation for large action spaces via policy convolution. In Proceedings of the ACM
Web Conference 2024, WWW ’24, page 3576–3585, New York, NY, USA, 2024. Association
for Computing Machinery. ISBN 9798400701719. doi: 10.1145/3589334.3645501. URL
https://doi.org/10.1145/3589334.3645501."
REFERENCES,0.17772317772317772,"[45] Yuta Saito and Thorsten Joachims. Off-policy evaluation for large action spaces via embeddings.
In Proceedings of the 39th International Conference on Machine Learning, volume 162 of
Proceedings of Machine Learning Research, pages 19089–19122. PMLR, 17–23 Jul 2022. URL
https://proceedings.mlr.press/v162/saito22a.html."
REFERENCES,0.17854217854217855,"[46] Yuta Saito, Qingyang Ren, and Thorsten Joachims. Off-policy evaluation for large action
spaces via conjunct effect modeling. In international conference on Machine learning, pages
29734–29759. PMLR, 2023."
REFERENCES,0.17936117936117937,"[47] Otmane Sakhi, Stephen Bonner, David Rohde, and Flavian Vasile. BLOB: A Probabilistic
model for recommendation that combines organic and bandit signals. In Proceedings of the
26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages
783–793, 2020."
REFERENCES,0.18018018018018017,"[48] Otmane Sakhi, Louis Faury, and Flavian Vasile. Improving Offline Contextual Bandits with
Distributional Robustness, 2020."
REFERENCES,0.180999180999181,"[49] Otmane Sakhi, Pierre Alquier, and Nicolas Chopin. PAC-Bayesian Offline Contextual Bandits
with Guarantees. In International Conference on Machine Learning, pages 29777–29799.
PMLR, 2023."
REFERENCES,0.18181818181818182,"[50] Otmane Sakhi, David Rohde, and Nicolas Chopin. Fast Slate Policy Optimization: Going
Beyond Plackett-Luce. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.
URL https://openreview.net/forum?id=f7a8XCRtUu."
REFERENCES,0.18263718263718265,"[51] Otmane Sakhi, David Rohde, and Alexandre Gilotte. Fast Offline Policy Optimization for Large
Scale Recommendation. Proceedings of the AAAI Conference on Artificial Intelligence, 37
(8):9686–9694, Jun. 2023. doi: 10.1609/aaai.v37i8.26158. URL https://ojs.aaai.org/
index.php/AAAI/article/view/26158."
REFERENCES,0.18345618345618345,"[52] Yevgeny Seldin, Nicolò Cesa-Bianchi, Peter Auer, François Laviolette, and John Shawe-Taylor.
PAC-Bayes-Bernstein Inequality for Martingales and its Application to Multiarmed Bandits. In
Dorota Glowacka, Louis Dorard, and John Shawe-Taylor, editors, Proceedings of the Workshop
on On-line Trading of Exploration and Exploitation 2, volume 26 of Proceedings of Machine
Learning Research, pages 98–111, Bellevue, Washington, USA, 02 Jul 2012. PMLR. URL
https://proceedings.mlr.press/v26/seldin12a.html."
REFERENCES,0.18427518427518427,"[53] Anshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for Sublinear Time Maximum
Inner Product Search (MIPS). In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q.
Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran
Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/
2014/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf."
REFERENCES,0.1850941850941851,"[54] Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudík. Doubly robust
off-policy evaluation with shrinkage. In International Conference on Machine Learning, pages
9167–9176. PMLR, 2020."
REFERENCES,0.18591318591318592,"[55] Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback
through counterfactual risk minimization. The Journal of Machine Learning Research, 16(1):
1731–1755, 2015."
REFERENCES,0.18673218673218672,"[56] Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual
learning. advances in neural information processing systems, 28, 2015."
REFERENCES,0.18755118755118755,"[57] Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miro Dudik, John Langford,
Damien Jose, and Imed Zitouni. Off-policy evaluation for slate recommendation. Advances in
Neural Information Processing Systems, 30, 2017."
REFERENCES,0.18837018837018837,"[58] Muhammad Faaiz Taufiq, Arnaud Doucet, Rob Cornish, and Jean-Francois Ton. Marginal
density ratio for off-policy evaluation in contextual bandits. Advances in Neural Information
Processing Systems, 36, 2024."
REFERENCES,0.1891891891891892,"[59] Lequn Wang, Akshay Krishnamurthy, and Aleksandrs Slivkins. Oracle-efficient pessimism:
Offline policy optimization in contextual bandits. arXiv preprint arXiv:2306.07923, 2023."
REFERENCES,0.19000819000819,"[60] Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudık. Optimal and adaptive off-policy evalua-
tion in contextual bandits. In International Conference on Machine Learning, pages 3589–3597.
PMLR, 2017."
REFERENCES,0.19082719082719082,"[61] Ding-Xuan Zhou. The covering number in learning theory. J. Complex., 18(3):739–767, sep
2002. ISSN 0885-064X. doi: 10.1006/jcom.2002.0635. URL https://doi.org/10.1006/
jcom.2002.0635."
REFERENCES,0.19164619164619165,Table of Contents for Supplementary Material
REFERENCES,0.19246519246519248,"A Limitations
16"
REFERENCES,0.19328419328419327,"B
Broader impact
16"
REFERENCES,0.1941031941031941,"C Extended related work
16"
REFERENCES,0.19492219492219492,"D Useful lemmas
18"
REFERENCES,0.19574119574119575,"E Additional results and discussions
19"
REFERENCES,0.19656019656019655,"E.1
Plots of the empirical moments bounds (Proposition 1) . . . . . . . . . . . . . . .
19"
REFERENCES,0.19737919737919737,"E.2
The study of Logarithmic Smoothing estimator and proofs
. . . . . . . . . . . . .
19"
REFERENCES,0.1981981981981982,"E.3
OPS: Formal comparison with IX suboptimality . . . . . . . . . . . . . . . . . . .
23"
REFERENCES,0.19901719901719903,"E.4
OPL: Formal comparison of PAC-Bayesian bounds . . . . . . . . . . . . . . . . .
24"
REFERENCES,0.19983619983619982,"E.5
OPL: Formal comparison with IX PAC-Bayesian learning suboptimality . . . . . .
27"
REFERENCES,0.20065520065520065,"F
Proofs of OPE
29"
REFERENCES,0.20147420147420148,"F.1
Proof of high order empirical moments bound (Proposition 1) . . . . . . . . . . . .
29"
REFERENCES,0.2022932022932023,"F.2
Proof of the impact of L on the bound’s tightness (Proposition 2) . . . . . . . . . .
31"
REFERENCES,0.2031122031122031,"F.3
Comparisons of the bounds U λ
L (Proposition 3)
. . . . . . . . . . . . . . . . . . .
31"
REFERENCES,0.20393120393120392,"F.4
Proof of the optimality of global clipping for Corollary 4 . . . . . . . . . . . . . .
32"
REFERENCES,0.20475020475020475,"F.5
Comparison with empirical Bernstein
. . . . . . . . . . . . . . . . . . . . . . . .
34"
REFERENCES,0.20556920556920558,"F.6
Proof of the L →∞bound (Corollary 5)
. . . . . . . . . . . . . . . . . . . . . .
35"
REFERENCES,0.20638820638820637,"F.7
Proof of the optimality of IPS for Corollary 5 . . . . . . . . . . . . . . . . . . . .
36"
REFERENCES,0.2072072072072072,"F.8
Comparison with the IX bound (Proposition 8)
. . . . . . . . . . . . . . . . . . .
36"
REFERENCES,0.20802620802620803,"G Proofs of OPS and OPL
37"
REFERENCES,0.20884520884520885,"G.1
OPS: Proof of suboptimality bound (Proposition 9) . . . . . . . . . . . . . . . . .
37"
REFERENCES,0.20966420966420968,"G.2
OPL: Proof of PAC-Bayesian LS-LIN bound (Proposition 10) . . . . . . . . . . . .
38"
REFERENCES,0.21048321048321048,"G.3
OPL: Proof of PAC-Bayesian suboptimality bound (Proposition 11)
. . . . . . . .
39"
REFERENCES,0.2113022113022113,"H Experimental design and detailed experiments
40"
REFERENCES,0.21212121212121213,"H.1
Off-policy evaluation and selection . . . . . . . . . . . . . . . . . . . . . . . . . .
40"
REFERENCES,0.21294021294021295,"H.1.1
Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40"
REFERENCES,0.21375921375921375,"H.1.2
(OPE) Tightness of the bounds . . . . . . . . . . . . . . . . . . . . . . . .
40"
REFERENCES,0.21457821457821458,"H.1.3
(OPS) Find the best, avoid the worst policy . . . . . . . . . . . . . . . . .
41"
REFERENCES,0.2153972153972154,"H.2
Off-policy learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42"
REFERENCES,0.21621621621621623,"H.2.1
Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42"
REFERENCES,0.21703521703521703,"H.2.2
Policy class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42"
REFERENCES,0.21785421785421785,"H.2.3
Detailed hyperparameters
. . . . . . . . . . . . . . . . . . . . . . . . . .
42"
REFERENCES,0.21867321867321868,"H.2.4
Detailed results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43"
REFERENCES,0.2194922194922195,"A
Limitations"
REFERENCES,0.2203112203112203,"This work develops theoretically grounded and practical pessimistic approaches for the offline
contextual bandit setting. Even if the proposed algorithms are general, and provably better than
competitors, they still suffer from the intrinsic limitations of importance weighting estimators.
Specifically, our method, as presented, will perform poorly in extremely large action spaces. However,
these limitations can be mitigated by incorporating additional structure as in Saito and Joachims
[45], Saito et al. [46]. Another limitation arises from the offline contextual bandit setting itself, which
assumes i.i.d. observations. While this assumption is valid in simple scenarios, it becomes unsuitable
once we want to capture the long term effect of interventions. Extending our results to the more
general, reinforcement learning setting would be an interesting research direction as it comes with a
challenging estimation task and will require developing new concentration bounds."
REFERENCES,0.22113022113022113,"B
Broader impact"
REFERENCES,0.22194922194922195,"Our work contributes to the development of theoretically grounded and practical pessimistic ap-
proaches for the offline contextual bandit setting. The derived algorithms can improve the robustness
of decision-making processes by prioritizing safety and minimizing uncertainty associated risks. By
leveraging pessimistic strategies, we ensure that decisions are made with a conservative bias, thereby
potentially improving outcomes in high-stakes environments where the cost of errors is substantial.
Although our framework and algorithms have broad, potentially good applications, their specific
social impacts will solely depend on the chosen application domain."
REFERENCES,0.22276822276822278,"C
Extended related work"
REFERENCES,0.22358722358722358,"Offline contextual bandits. Contextual bandit is a widely adopted framework for online learning
in uncertain environments [33]. However, some real-world applications present challenges for
existing online algorithms, and thus offline methods that leverage historical data to optimize decision-
making have gained traction [10]. Fortunately, large datasets summarizing past interactions are
often available, allowing agents to improve their policies offline [55]. Our work explores this
offline approach, known as offline (or off-policy) contextual bandits [16]. In this setting, off-policy
evaluation (OPE) estimates policy performance using historical data, mimicking real-time evaluations.
Depending on the application, the goal might be to find the best policy within a predefined finite set
(off-policy selection (OPS)) or the optimal policy overall (off-policy learning (OPL))."
REFERENCES,0.2244062244062244,"Off-policy evaluation. In recent years, OPE has experienced a noticeable surge of interest, with
numerous significant contributions [16–18, 60, 19, 54, 38, 32, 45, 47, 26]. The literature on OPE
can be broadly classified into three primary approaches. The first, referred to as the direct method
(DM) [26, 6], involves the development of a model designed to approximate expected costs for
any context-action pair. This model is subsequently employed to estimate the performance of the
policies. This approach is often designed for specific applications such as large-scale recommender
systems [47, 26, 4]. The second approach, known as inverse propensity scoring (IPS) [24, 17], aims to
estimate the costs associated with the evaluated policies by correcting for the inherent preference bias
of the behavior policy within the dataset. While IPS maintains its unbiased nature when operating
under the assumption that the evaluation policy is absolutely continuous with respect to the behavior
policy, it can be susceptible to high variance and substantial bias when this assumption is violated
[43]. In response to the variance issue, various techniques have been introduced, including clipping
[25, 10], shrinkage [54], power-mean correction [38], implicit exploration [21], self-normalization
[56], among others [22]. The third approach, known as doubly robust (DR) [42, 9, 16, 18, 19],
combines elements from both the direct method (DM) and inverse propensity scoring (IPS). This
work focuses on regularized IPS."
REFERENCES,0.22522522522522523,"Off-policy selection and learning. as in OPE, three key approaches dominate: DM, IPS and DR
in OPS and OPL. In OPS, all these methods share the same core objective: identifying the policy
with the highest estimated reward from a finite set of candidates. However, they differ in their
reward estimation techniques, as discussed in the OPE section above. In contrast, in OPL, DM either
deterministically selects the action with the highest estimated reward or constructs a distribution
based on these estimates. IPS and DR, on the other hand, employ gradient descent for policy learning
[55], updating a parameterized policy denoted by πθ as θt+1 ←θt −∇θR(πθ) for each iteration t."
REFERENCES,0.22604422604422605,"Since the true risk R is unknown, ∇θR(πθ) is unknown and needs to be estimated using techniques
like IPS or DR."
REFERENCES,0.22686322686322685,"Pessimism in offline contextual bandits. Most OPE studies directly use their point estimators of
the risk in OPE, OPS and OPL. However, point estimators can deviate from the true value of the
risk, rendering them unreliable for decision-making. Therefore, and to increase safety, alternative
approaches focus on constructing bounds on the risk. These bounds, either asymptotic [10, 48, 15] or
finite sample [32, 21], aim to evaluate a policy’s worst-case performance, adhering to the principle
of pessimism in face of uncertainty [27]. The principle of pessimism transcends OPE, influencing
both OPS and OPL. In these domains, strategies are predominantly inspired by, or directly derived
from, upper bounds on the true risk [55, 35, 32, 49, 5, 59]. Consider OPS: [32] leveraged an Efron-
Stein bound for the self-normalized IPS estimator, while [21] anchored their analysis on a bound
constructed with the Implicit Exploration estimator. Shifting focus to OPL, [55] combined the
empirical Bernstein bound [36] with the clipping estimator, motivating sample variance penalization
for policy learning. Recent advancements include modifications to the penalization term [59] to be
scalable and efficient."
REFERENCES,0.22768222768222768,"PAC-Bayes extension. The PAC-Bayesian paradigm [37, 11] (see Alquier [2] for a recent intro-
duction) provides a rich set of tools to prove generalization bounds for different statistical learning
problems. The classical (online) contextual bandit problem received a lot of attention from the
PAC-Bayesian community with the seminal work of Seldin et al. [52]. It is just recently that these
tools were adapted to the offline contextual bandit setting, with [35] that introduced a clean and
scalable PAC-Bayesian perspective to OPL. This perspective was further explored by [20, 49, 5, 7, 21],
leading to the development of tight, tractable PAC-Bayesian bounds suitable for direct optimization."
REFERENCES,0.2285012285012285,"Large action space extension. While regularization techniques can improve IPS properties, they
often fall short when dealing with extremely large action spaces. Additional assumptions regarding
the structure of the contextual bandit problem become necessary. For example, Saito and Joachims
[45] introduced the Marginalized IPS (MIPS) framework and estimator. MIPS leverages auxiliary
information about the actions in the form of action embeddings. Roughly speaking, MIPS assumes
access to embeddings ei within logged data and defines the risk estimator as"
REFERENCES,0.22932022932022933,"ˆRMIPS
n
(π) = 1 n n
X i=1"
REFERENCES,0.23013923013923013,"π (ei | xi)
π0 (ei | xi)ci = 1 n n
X"
REFERENCES,0.23095823095823095,"i=1
w (xi, ei) ci ,"
REFERENCES,0.23177723177723178,"where the logged data Dn = {(xi, ai, ei, ri)}n
i=1 now includes action embeddings for each data point.
The marginal importance weight"
REFERENCES,0.2325962325962326,"w(x, e) = π(e | x)"
REFERENCES,0.2334152334152334,"π0(e | x) =
P"
REFERENCES,0.23423423423423423,"a p(e | x, a)π(a | x)
P"
REFERENCES,0.23505323505323505,"a p(e | x, a)π0(a | x)"
REFERENCES,0.23587223587223588,"is a key component of this approach. Compared to IPS and DR, MIPS achieves significantly lower
variance in large action spaces [45] while maintaining unbiasedness if the action embeddings directly
influence costs c. This necessitates informative embeddings that capture the causal effects of actions
on costs. However, high-dimensional embeddings can still lead to high variance for MIPS, similar
to IPS. Additionally, high bias can arise if the direct effect assumption is violated and embeddings
fail to capture these causal effects. This bias is particularly present when performing action feature
selection for dimensionality reduction. Recent work proposes learning such embeddings directly
from logged data [41, 44, 14], or loosen this assumption [58, 46]. Our proposed importance weight
regularization can be potentially combined with these estimators under their respective assumptions
on the underlying structure of the contextual bandit problem, extending our approach to large action
spaces, and we posit that this will be beneficial when, for example, the action embedding dimension
is high. Another line of research in large action spaces is more interested with the learning problem,
precisely solving the optimization issues arising from policies defined on large action spaces. Indeed,
naive optimization tends to be slow and scales linearly with the number of actions K [12]. Recent
work [51, 50] solve this by leveraging fast maximum inner product search [53, 3] in the training loop,
reducing the optimization complexity to logarithmic in the action space size. These methods however
require a linear objective on the target policy. Luckily, our PAC-Bayesian learning objective is linear
in the policy and its optimization is amenable to such acceleration."
REFERENCES,0.23669123669123668,"Continuous action space extension. While research has predominantly focused on discrete action
spaces, a limited number of studies have tackled the continuous case [29, 13, 59]. For example, [29]"
REFERENCES,0.2375102375102375,"explored non-parametric evaluation and learning of continuous action policies using kernel smoothing,
while [13] investigated the semi-parametric setting. Recently, [59] leveraged the smoothing approach
from [31] to extend their discrete OPL method to continuous actions. Our work can either use the
densities directly, or be similarly extended to continuous actions through a well-defined discretization
of the space. Imagine a scenario with infinitely many actions, where policies are defined by density
functions. For any context x, π(a | x) represents the density function that maps actions a to probabil-
ities. The discretization process transforms the original contextual bandit problem characterized by
the density-based policy class Π into an OPL problem defined by a discrete, mass-based policy class
ΠK (for a finite number of actions K). Each policy within ΠK approximates a policy in Π through a
smoothing process."
REFERENCES,0.23832923832923833,"D
Useful lemmas"
REFERENCES,0.23914823914823916,"In the following, and for any quantity Z, all expectations are computed w.r.t to the distribution of the
data when playing actions under the behaviour policy π0, as in:"
REFERENCES,0.23996723996723995,"E [Z] = Ex∼ν,a∼π0(·|x),c∼p(·|x,a) [Z] ."
REFERENCES,0.24078624078624078,"A lot of the results derived in the paper are based on the use of the well known Chernoff Inequality,
that we state below for a sum of i.i.d. random variables:"
REFERENCES,0.2416052416052416,"Lemma 12 (Chernoff Inequality for a sum of i.i.d. random variables.). Let a ∈R, n ∈N∗
and {Xi, i ∈[n]} a collection of n i.i.d. random variables. The following concentration
bounds on the right tail of P"
REFERENCES,0.24242424242424243,i∈[n] Xi hold for any λ ≥0: P  X
REFERENCES,0.24324324324324326,"i∈[n]
Xi > a "
REFERENCES,0.24406224406224405,≤(E [exp (λX1)])n exp(−λa)
REFERENCES,0.24488124488124488,"This result is classical in the literature [23] and we omit its proof. We will also need the following
lemma, that states the monotonous nature of a key function in our analysis, and that we take the time
to prove."
REFERENCES,0.2457002457002457,Lemma 13. Let L ≥1 and fL be the following function:
REFERENCES,0.24651924651924653,"fL(x) = log(1 + x) −PL
ℓ=1
(−1)ℓ−1 ℓ
xℓ"
REFERENCES,0.24733824733824733,"(−1)LxL+1
."
REFERENCES,0.24815724815724816,We have that fL is a decreasing function in R+ for all L ∈N∗.
REFERENCES,0.24897624897624898,Proof. Let L ≥1 and fL be the following function:
REFERENCES,0.2497952497952498,"fL(x) = log(1 + x) −PL
ℓ=1
(−1)ℓ−1 ℓ
xℓ"
REFERENCES,0.25061425061425063,"(−1)LxL+1
."
REFERENCES,0.25143325143325146,"Let x ∈R+, we have the following identity holding ∀t > 0 and ∀n ≥0:"
REFERENCES,0.25225225225225223,1 + (−1)ntn+1
REFERENCES,0.25307125307125306,"1 + t
= n
X"
REFERENCES,0.2538902538902539,"k=0
(−1)ktk ⇐⇒
1
1 + t = n
X"
REFERENCES,0.2547092547092547,"k=0
(−1)ktk + (−1)n+1tn+1"
REFERENCES,0.25552825552825553,"1 + t
.
(29)"
REFERENCES,0.25634725634725636,Recall the integral form of the log function:
REFERENCES,0.2571662571662572,"log(1 + x) =
Z x 0"
REFERENCES,0.257985257985258,"1
1 + tdt."
REFERENCES,0.2588042588042588,We integrate both sides of the Equality (29) and show that the numerator of fL(x) is equal to:
REFERENCES,0.2596232596232596,"log(1 + x) − K
X k=1"
REFERENCES,0.26044226044226043,(−1)k−1
REFERENCES,0.26126126126126126,"k
xk = (−1)K
Z x 0 tK"
REFERENCES,0.2620802620802621,1 + tdt.
REFERENCES,0.2628992628992629,This result enables us to rewrite the function fL as:
REFERENCES,0.26371826371826373,"fL(x) =
1
xL+1 Z x 0 tL"
REFERENCES,0.26453726453726456,1 + tdt.
REFERENCES,0.26535626535626533,"Using the change of variable t = ux, we obtain:"
REFERENCES,0.26617526617526616,"fL(x) =
Z 1 0 uL"
REFERENCES,0.266994266994267,1 + xudt
REFERENCES,0.2678132678132678,which is clearly decreasing for in R+. This ends the proof.
REFERENCES,0.26863226863226863,"Finally, we also state the important change of measure lemma:"
REFERENCES,0.26945126945126946,"Lemma 14 (Change of measure). Let g be a function of the parameter θ and data Dn, for
any distribution Q that is P continuous, for any δ ∈(0, 1], we have with probability 1 −δ :"
REFERENCES,0.2702702702702703,"Eθ∼Q[g(θ, Dn)] ≤KL(Q||P) + ln Ψg"
REFERENCES,0.2710892710892711,"δ
(30)"
REFERENCES,0.2719082719082719,"with Ψg = EDnEθ∼P [eg(θ,Dn)]."
REFERENCES,0.2727272727272727,"Lemma 14 is the backbone of a multitude of PAC-Bayesian bounds. It is proven in many references,
see for example [2] or Lemma 1.1.3 in [11]. With this result, the recipe of constructing a generalization
bound reduces to choosing an adequate function g for which we can control Ψg."
REFERENCES,0.27354627354627353,"E
Additional results and discussions"
REFERENCES,0.27436527436527436,"E.1
Plots of the empirical moments bounds (Proposition 1)"
REFERENCES,0.2751842751842752,"For any π ∈Π, let U λ,h
L (π) be the upper bound of Proposition 1:"
REFERENCES,0.276003276003276,"U λ,h
L (π) = ψλ"
REFERENCES,0.27682227682227684,"ˆRh
n(π) + ln(1/δ) λn
+"
"L
X",0.27764127764127766,"2L
X ℓ=2 λℓ−1"
"L
X",0.2784602784602785,"ℓ
ˆ
Mh,ℓ
n (π) ! ."
"L
X",0.27927927927927926,"One can observe that the bound U λ,h
L
depends on three parameters, the regularized IPS function h,
the free parameter λ and the moment order L. We choose a dataset (balance-scale) with n = 612, and
evaluate a policy π with R(π) = −0.93 to evaluate our bound for different parameters. We fix λ =
p"
"L
X",0.2800982800982801,"1/n and plot the value of U λ,h
L
for different values of the moment order L ∈{1, 2, 3, 4, 6, 8, ∞}
and for 4 different regularization functions, namely IPS, clipped IPS (M = √n), Implicit Exploration
(IX) (λ =
p"
"L
X",0.2809172809172809,"1/n) and Exponential Smoothing (ES) (α = 1 −
p"
"L
X",0.28173628173628174,"1/n). The results are shown in
Figure 3. One can observe from the plot that The decreasing nature of U λ,h
L
depends on λ and the
regularization function h. Indeed, Proposition 2 states that λ < mini∈[n] 1/|hi| implies that the
bound is decreasing w.r.t L. Which means that once this condition is not verified, we do not know
if the bound will keep decreasing with L. If the bound seems decreasing for CIPS and IX, One
can observe that for both IPS and ES, the bound increased from L = 4 to L = 8, but achieved its
minimum at L = ∞, with IPS being optimal for this value. This highlights the connection between
L, the value of λ and the regularizer h."
"L
X",0.28255528255528256,"E.2
The study of Logarithmic Smoothing estimator and proofs"
"L
X",0.2833742833742834,"Recall the form of the Logarithmic Smoothing estimator, defined for any λ ≥0:"
"L
X",0.2841932841932842,"ˆRλ
n(π) = −1 n n
X i=1"
"L
X",0.28501228501228504,"1
λ log (1 −λwπ(xi, ai)ci) .
(31)"
"L
X",0.2858312858312858,"Our estimator ˆRλ
n(π), is defined for a non-negative λ ≥0. In particular, λ = 0 recovers the unbiased
IPS estimator in (2) and λ > 0 introduces a bias variance trade-off. This estimator can be interpreted"
"L
X",0.28665028665028663,"1
2
3
4
6
8
+1
Values of L −0.72 −0.68 −0.64 −0.60 −0.56 −0.52"
"L
X",0.28746928746928746,"U ¸; h
L
(¼); ¸ = 1=
pn"
"L
X",0.2882882882882883,"R(¼) = -0.93, n = 612"
"L
X",0.2891072891072891,"IPS
CIPS
IX
ES"
"L
X",0.28992628992628994,Figure 3: Proposition 1 for different values of L and with different regularized IPS h.
"L
X",0.29074529074529076,"as Logarithmic Soft Clipping, and have a similar behavior than Clipping of Bottou et al. [10]. Indeed,
1/λ plays a similar role to the clipping parameter M, as for any i ∈[n], we have:"
"L
X",0.2915642915642916,"wπ(xi, ai)ci ≪1"
"L
X",0.29238329238329236,λ =⇒−1
"L
X",0.2932022932022932,"λ log (1 −λwπ(xi, ai)ci) ≈wπ(xi, ai)ci."
"L
X",0.294021294021294,"wπ(xi, ai)ci < M =⇒min (wπ(xi, ai), M) ci = wπ(xi, ai)ci."
"L
X",0.29484029484029484,"LS can be seen as a smooth, differentiable version of clipping. We plot the graph of the two functions
in Figure 4. One can observe that once λ > 0, LS exhibits a bias-variance trade-off, with a declining
bias with λ →0. This is different than Clipping as no bias is suffered once M is bigger than
the support of wπ, this comes however with the price of suffering the full variance of IPS. In the
following, we study the bias-variance trade-off that emerges with the new Logarithmic Smoothing
estimator."
"L
X",0.29565929565929566,"0
10
20
30
40
50
Importance Weight w¼"
"L
X",0.2964782964782965,"IPS
Clipping, M = 20
LS, ¸ = 0.1"
"L
X",0.2972972972972973,"LS, ¸ = 0.05"
"L
X",0.29811629811629814,"LS, ¸ = 0.01"
"L
X",0.2989352989352989,Figure 4: Comparison of Logarithmic Smoothing and Clipping.
"L
X",0.29975429975429974,"We begin by defining the bias and variance of ˆRλ
n(π):"
"L
X",0.30057330057330056,"Bλ(π) = E
h
ˆRλ
n(π)
i
−R(π) ,
Vλ(π) = E

ˆRλ
n(π) −E
h
ˆRλ
n(π)
i2
.
(32)"
"L
X",0.3013923013923014,"Moreover, for any λ ≥0, we define the following quantity"
"L
X",0.3022113022113022,"Sλ(π) = E

wπ(x, a)2c2"
"L
X",0.30303030303030304,"1 −λwπ(x, a)c"
"L
X",0.30384930384930386,"
,
(33)"
"L
X",0.3046683046683047,"that will be essential in studying the properties of this estimator akin to the coverage ratio used for
the IX-estimator [21]. In the following, we study the properties of our estimator ˆRλ
n(π) in (14). We
start with bounding its mean squared error (MSE), which involves bounding its bias and variance."
"L
X",0.30548730548730546,Proposition (Bias-variance trade-off). Let π ∈Π and λ ≥0. Then we have that
"L
X",0.3063063063063063,"0 ≤Bλ(π) ≤λSλ(π) ,
and
Vλ(π) ≤Sλ(π)/n ."
"L
X",0.3071253071253071,"Moreover, it holds that for any λ > 0:"
"L
X",0.30794430794430794,"Vλ(π) ≤|R(π)| nλ
≤1 nλ."
"L
X",0.30876330876330876,Proof. Let us start with bounding the bias. We have for any λ ≥0:
"L
X",0.3095823095823096,"Bλ(π) = E
h
ˆRλ
n(π)
i
−R(π)"
"L
X",0.3104013104013104,"= E

−1"
"L
X",0.31122031122031124,"λ log(1 −λwπ(x, a)c) −wπ(x, a)c

(IPS is unbiased)."
"L
X",0.31203931203931207,"Using log(1 + x) ≤x for any x ≥0 proves that the bias is positive. For its upper bound, we use the
following inequality log(1 + x) ≥
x
1+x holding for x ≥0:"
"L
X",0.31285831285831284,"Bλ(π) = E

−1"
"L
X",0.31367731367731366,"λ log(1 −λwπ(x, a)c) −wπ(x, a)c
"
"L
X",0.3144963144963145,"≤E

wπ(x, a)c
1 −λwπ(x, a)c −wπ(x, a)c

= λE
 (wπ(x, a)c)2"
"L
X",0.3153153153153153,"1 −λwπ(x, a)c"
"L
X",0.31613431613431614,"
= λSλ(π)."
"L
X",0.31695331695331697,"Now focusing on the variance, we have:"
"L
X",0.3177723177723178,"Vλ(π) = E

ˆRλ
n(π) −E
h
ˆRλ
n(π)
i2"
"L
X",0.3185913185913186,"≤
1
nλ2 E

log(1 −λwπ(x, a)c)2
."
"L
X",0.3194103194103194,We use the following inequality log(1 + x) ≤x/√x + 1 holding for x ≥0 to obtain our result:
"L
X",0.3202293202293202,Vλ(π) ≤1
"L
X",0.32104832104832104,nSλ(π).
"L
X",0.32186732186732187,"Notice that once λ > 0, we have:"
"L
X",0.3226863226863227,"Sλ(π) = E

wπ(x, a)2c2"
"L
X",0.3235053235053235,"1 −λwπ(x, a)c 
≤1"
"L
X",0.32432432432432434,"λE [wπ(x, a)|c|] = |R(π)| λ
,"
"L
X",0.32514332514332517,resulting in a finite variance whenever λ > 0:
"L
X",0.32596232596232594,"Vλ(π) ≤|R(π)| nλ
≤1 nλ."
"L
X",0.32678132678132676,"λ = 0 recovers the IPS estimator in (2), with zero bias and variance bounded by E

w2(x, a)c2
/n.
When λ > 0, a bias-variance trade-off emerges. The bias is always non-negative as we still recover
an estimator that verifies (C1). The bias is capped at λSλ(π), which diminishes to zero when λ is
small and goes to |R(π)| as λ increases. Conversely, the variance decreases with a higher λ. Notably,
λ > 0 ensures finite variance bounded by 1/λn, despite the estimator being unbounded. This is
different from previous regularizations that relied on bounded functions to ensure finite variance."
"L
X",0.3276003276003276,"While prior evaluations of estimators often relied on bias and variance analysis, Metelli et al. [38]
argued for studying the non-asymptotic concentration rate of the estimators, advocating for sub-
Gaussianity as a desired property. Even if our estimator is not bounded, we prove in the following
that it is sub-Gaussian."
"L
X",0.3284193284193284,"Proposition (Sub-Gaussianity). Let π ∈Π, δ ∈(0, 1] and λ > 0. Then the following
inequalities holds with probability at least 1 −δ:"
"L
X",0.32923832923832924,"R(π) −ˆRλ
n(π) ≤ln(2/δ)"
"L
X",0.33005733005733007,"λn
,
and
ˆRλ
n(π) −R(π) ≤λSλ(π) + ln(2/δ) λn
."
"L
X",0.3308763308763309,"In particular, setting λ = λ∗=
p"
"L
X",0.3316953316953317,"ln(2/δ)/nE [wπ(x, a)2c2] yields that"
"L
X",0.3325143325143325,"|R(π) −ˆRλ∗
n (π)| ≤
p"
"L
X",0.3333333333333333,"2σ2 ln(2/δ) ,
where σ2 = 2E

wπ(x, a)2c2
/n .
(34)"
"L
X",0.33415233415233414,"Proof. Let π ∈Π, λ > 0 and δ > 0. To prove sub-Gaussianity, we need both upper bounds and
lower bounds on R(π) using ˆRλ
n(π). For the upper bound, we can use the bound of Corollary 5, and
recall that ψλ(x) ≤x for all x. We then obtain with a probability 1 −δ:"
"L
X",0.33497133497133497,R(π) ≤ψλ
"L
X",0.3357903357903358,"
ˆRλ
n(π) + ln(1/δ) λn"
"L
X",0.3366093366093366,"
=⇒R(π) −ˆRλ
n(π) ≤ln(1/δ) λn
."
"L
X",0.33742833742833744,"For the lower bound on the risk, we go back to our Chernoff Lemma 12, and use the collection of
i.i.d. random variable, that for any i ∈[n], are defined as:"
"L
X",0.33824733824733827,¯Xi = −1
"L
X",0.33906633906633904,"λ log (1 −λwπ(xi, ai)ci) ."
"L
X",0.33988533988533987,This gives for a ∈R: P  X
"L
X",0.3407043407043407,"i∈[n]
¯Xi > a "
"L
X",0.3415233415233415,"≤
 
E

exp
 
λ ¯X1
n exp(−λa) P  X"
"L
X",0.34234234234234234,"i∈[n]
¯Xi > a "
"L
X",0.34316134316134317,"≤

E

1
1 −λwπ(x, a)c"
"L
X",0.343980343980344,"n
exp(−λa)"
"L
X",0.3447993447993448,"Solving for δ =

E
h
1
1−λwπ(x,a)c
in
exp(−λa), we get: P  1 n X"
"L
X",0.34561834561834565,"i∈[n]
¯Xi > 1"
"L
X",0.3464373464373464,"λ log

E

1
1 −λwπ(x, a)c"
"L
X",0.34725634725634724,"
+ ln(1/δ) λn  ≤δ"
"L
X",0.34807534807534807,The complementary event holds with at least probability 1 −δ:
"L
X",0.3488943488943489,"ˆRλ
n(π) ≤1"
"L
X",0.3497133497133497,"λ log

E

1
1 −λwπ(x, a)c"
"L
X",0.35053235053235055,"
+ ln(1/δ) λn
,"
"L
X",0.35135135135135137,which implies using the inequality log(x) ≤x −1 for all x > 0:
"L
X",0.3521703521703522,"ˆRλ
n(π) −R(π) ≤1"
"L
X",0.35298935298935297,"λ log

E

1
1 −λwπ(x, a)c"
"L
X",0.3538083538083538,"
−R(π) + ln(1/δ) λn ≤1 λ"
"L
X",0.3546273546273546,"
E

1
1 −λwπ(x, a)c"
"L
X",0.35544635544635544,"
−1

−R(π) + ln(1/δ) λn"
"L
X",0.35626535626535627,"≤E

wπ(x, a)c
1 −λwπ(x, a)c −wπ(x, a)c

+ ln(1/δ) λn"
"L
X",0.3570843570843571,"≤λE

wπ(x, a)2c2"
"L
X",0.3579033579033579,"1 −λwπ(x, a)c"
"L
X",0.35872235872235875,"
+ ln(1/δ)"
"L
X",0.3595413595413595,"λn
= λSλ(π) + ln(1/δ) λn
,"
"L
X",0.36036036036036034,"which proves the lower bound on the risk. As both results hold with high probability, we use a union
argument to have them both holding for probability at least 1 −δ:"
"L
X",0.36117936117936117,"R(π) −ˆRλ
n(π) ≤ln(2/δ)"
"L
X",0.361998361998362,"λn
,
and
ˆRλ
n(π) −R(π) ≤λSλ(π) + ln(2/δ) λn
,"
"L
X",0.3628173628173628,which implies that:
"L
X",0.36363636363636365,"|R(π) −ˆRλ
n(π)| ≤λSλ(π) + ln(2/δ)"
"L
X",0.36445536445536447,"λn
≤λE

wπ(x, a)2c2
+ ln(2/δ) λn
."
"L
X",0.3652743652743653,"This means that setting λ = λ∗=
p"
"L
X",0.36609336609336607,"ln(2/δ)/nE [wπ(x, a)2c2] yields a sub-Gaussian concentration:"
"L
X",0.3669123669123669,"|R(π) −ˆRλ∗
n (π)| ≤2 r"
"L
X",0.3677313677313677,"E [wπ(x, a)2c2] ln(2/δ) n
."
"L
X",0.36855036855036855,This ends the proof.
"L
X",0.36936936936936937,"From (34), ˆRλ∗
n (π) is sub-Gaussian with variance proxy σ2 = 2E

ω(x, a)2c2
/n, which is lower
that the variance proxy of the Harmonic estimator of Metelli et al. [38]. Indeed, the Harmonic
estimator has a slightly worse variance proxy of σ2
H = (2+
√ 3)2"
E,0.3701883701883702,"3
E

ω(x, a)2c2
/n, giving σ2 < σ2
H."
E,0.371007371007371,"E.3
OPS: Formal comparison with IX suboptimality"
E,0.37182637182637185,"Let us begin by stating results from the IX work [21]. Recall that the IX estimator is defined for any
λ > 0, by:"
E,0.3726453726453726,"ˆRλ-IX
n
(π) = 1 n n
X i=1"
E,0.37346437346437344,"π(ai|xi)
π0(ai|xi) + λ/2ci."
E,0.37428337428337427,"Let ΠS = {π1, ..., πm} be a finite set of predefined policies. In OPS, the goal is to find πS
∗∈ΠS that
satisfies"
E,0.3751023751023751,"πS
∗= argminπ∈ΠS R(π) = argmink∈[m] R(πk) ."
E,0.3759213759213759,"for λ > 0, the selection strategy suggested in Gabbianelli et al. [21] was to search for:"
E,0.37674037674037675,"ˆπS, IX
n
= argmin
π∈ΠS
ˆRλ-IX
n
(π) = argmin
k∈[m]
ˆRλ-IX
n
(π) .
(35)"
E,0.3775593775593776,"Proposition 15 (Suboptimality of the IX selection strategy). Let λ > 0 and δ ∈(0, 1]. Then,
it holds with probability at least 1 −δ that"
E,0.3783783783783784,"0 ≤R(ˆπS, IX
n
) −R(πS
∗) ≤λCλ/2(πS
∗) + 2 ln(2|ΠS|/δ)"
E,0.37919737919737917,"λn
,
(36) where"
E,0.38001638001638,"Cλ(π) = E

π(a|x)
π2
0(a|x) + λπ0(a|x)|c|

."
E,0.3808353808353808,"Both suboptimalities (LS and IX) have the same form, they only depend on two different quantities
(Sλ and Cλ respectively). For a π ∈Π and λ > 0, If we can identify when Sλ(π) ≤Cλ/2(π), then
we can prove that the sub-optimality of LS selection strategy is better than the one of IX. Luckily, this
is always the case, and it is stated formally below."
E,0.38165438165438165,Proposition 16. Let π ∈Π and λ > 0. We have:
E,0.3824733824733825,"Sλ(π) ≤Cλ/2(π).
(37)"
E,0.3832923832923833,"Proof. Let π ∈Π and λ > 0, we have:"
E,0.3841113841113841,Cλ/2(π) −Sλ(π) = E
E,0.38493038493038495,"""
π(a|x)
π2
0(a|x) + λ"
E,0.3857493857493858,"2 π0(a|x)|c| −
wπ(x, a)2c2"
E,0.38656838656838655,"1 −λwπ(x, a)c # = E"
E,0.38738738738738737,"""
π(a|x)
π2
0(a|x) + λ"
E,0.3882063882063882,"2 π0(a|x)|c| −
π(a|x)2c2"
E,0.389025389025389,"π2
0(a|x) −λπ0(a|x)π(a|x)c # = E """
E,0.38984438984438985,π(a|x)|c|
E,0.3906633906633907,"1
π2
0(a|x) + λ"
E,0.3914823914823915,"2 π0(a|x) −
π(a|x)|c|
π2
0(a|x) + λπ0(a|x)π(a|x)|c| !# = E """
E,0.3923013923013923,π(a|x)|c|
E,0.3931203931203931,"π2
0(a|x) (1 −π(a|x)|c|) + λ"
E,0.3939393939393939,2 π0(a|x)π(a|x)|c|
E,0.39475839475839475,"(π2
0(a|x) + λ"
E,0.3955773955773956,"2 π0(a|x))(π2
0(a|x) + λπ0(a|x)π(a|x)|c|) !# ≥0."
E,0.3963963963963964,"This means that the suboptimality of LS selection strategy is better bounded than the one of IX. Our
experiments confirm that the LS selection strategy is better than IX in practical scenarios."
E,0.3972153972153972,"Minimax optimality of our selection strategy.
As discussed in Gabbianelli et al. [21], pessimistic
algorithms tend to have the property that their regret scales with the minimax sample complexity
of estimating the value of the optimal policy [27]. For the case of multi-armed bandit (one con-
text x), this estimation minimax sample complexity is proved by Li et al. [34] and is of the rate
O(E[wπ∗(x, a)2c2]), with π∗being the optimal policy. Our bound matches the lower bound proved
by Li et al. [34], as:"
E,0.39803439803439805,"Sλ(π∗) = E

wπ∗(x, a)2c2"
E,0.3988533988533989,"1 −λwπ∗(x, a)c"
E,0.39967239967239965,"
≤E

wπ∗(x, a)2c2
,"
E,0.4004914004914005,"which is not the case for the suboptimality of IX, that only matches it in the deterministic setting with
binary costs, as:"
E,0.4013104013104013,"Cλ(π∗) = E

π∗(a|x)
π2
0(a|x) + λπ0(a|x)|c|

≤E
π∗(a|x)"
E,0.4021294021294021,"π2
0(a|x)|c|

= E"
E,0.40294840294840295,"""π∗(a|x)"
E,0.4037674037674038,π0(a|x)
E,0.4045864045864046,"2
c2
# ,"
E,0.40540540540540543,"with the last inequality only holding when π∗is deterministic and the costs are binary. For deter-
ministic policies and the general contextual bandit, we invite the reader to see a formal proof of the
minimax lower bound of pessimism in Jin et al. [28, Theorem 4.4], matched for both IX and LS."
E,0.4062244062244062,"E.4
OPL: Formal comparison of PAC-Bayesian bounds"
E,0.407043407043407,"As it is easier to work with linear estimators within the PAC-Bayesian framework, we define the
following estimator of the risk ˆRp−LIN
n
(π), with the help of a function p : R →R as:"
E,0.40786240786240785,"ˆRp−LIN
n
(π) = 1 n n
X i=1"
E,0.4086814086814087,"π(ai|xi)
p(π0(ai|xi))ci"
E,0.4095004095004095,"with the only condition on p to be {CLIN
1
: ∀x, p(x) ≥x}. This condition helps us control the
impact of actions with low probabilities under π0. This risk estimator encompasses well known risk
estimators depending on the choice of p."
E,0.4103194103194103,"Now that we defined the family of estimators covered by our analysis, we attack the problem of
deriving generalization bounds. We derive our empirical high order bound expressed in the following:"
E,0.41113841113841115,"Proposition 17 (Empirical High Order PAC-Bayes bound). Let L ≥1. Given a prior P on
FΘ, δ ∈(0, 1] and λ > 0, the following bound holds with probability at least 1 −δ uniformly
for all distribution Q over FΘ:"
E,0.411957411957412,R(πQ) ≤ψλ
E,0.41277641277641275,"ˆRp−LIN
n
(πQ) + KL(Q||P) + ln 1"
E,0.4135954135954136,"δ
λn
+"
"L
X",0.4144144144144144,"2L
X ℓ=2 λℓ−1"
"L
X",0.4152334152334152,"ℓ
ˆ
Mp−LIN,ℓ
n
(πQ) ! (38) with:"
"L
X",0.41605241605241605,"ˆ
Mp−LIN,ℓ
n
(πQ) = 1 n n
X i=1"
"L
X",0.4168714168714169,"πQ(ai|xi)
p(π0(ai|xi))ℓcℓ
i"
"L
X",0.4176904176904177,"ψλ = x :→1 −exp(−λx) λ
."
"L
X",0.41850941850941853,"Proof. Let L ≥1, we have from Lemma 13, and for any positive random variable X ≥0 and λ > 0:"
"L
X",0.41932841932841936,f2L−1(0) = 1
"L
X",0.4201474201474201,"2L ≥f2L−1(λX) = −log(1 + λX) −P2L−1
ℓ=1
(−1)ℓ−1"
"L
X",0.42096642096642095,"ℓ
(λX)ℓ"
"L
X",0.4217854217854218,(λX)2L
"L
X",0.4226044226044226,which is equivalent to:
"L
X",0.42342342342342343,"2L
X ℓ=1"
"L
X",0.42424242424242425,(−1)ℓ−1
"L
X",0.4250614250614251,"ℓ
(λX)ℓ≤log(1 + λX) ⇐⇒exp"
"L
X",0.4258804258804259,"2L
X ℓ=1"
"L
X",0.4266994266994267,(−1)ℓ−1
"L
X",0.4275184275184275,"ℓ
(λX)ℓ
!"
"L
X",0.4283374283374283,"≤1 + λX =⇒E "" exp"
"L
X",0.42915642915642915,"2L
X ℓ=1"
"L
X",0.42997542997543,(−1)ℓ−1
"L
X",0.4307944307944308,"ℓ
(λX)ℓ
!#"
"L
X",0.43161343161343163,"≤1 + E [λX] =⇒E "" exp"
"L
X",0.43243243243243246,"2L
X ℓ=1"
"L
X",0.4332514332514332,(−1)ℓ−1
"L
X",0.43407043407043405,"ℓ
(λX)ℓ
!#"
"L
X",0.4348894348894349,"≤exp (log(1 + E [λX])) ,"
"L
X",0.4357084357084357,"which implies that: E "" exp "
"L
X",0.43652743652743653,λ(X −1
"L
X",0.43734643734643736,λ log(1 + E [λX])) +
"L
X",0.4381654381654382,"2L
X ℓ=2"
"L
X",0.438984438984439,(−1)ℓ−1
"L
X",0.4398034398034398,"ℓ
(λX)ℓ
!# ≤1."
"L
X",0.4406224406224406,"For any X ≤0, we can inject −X ≥0 to obtain:"
"L
X",0.44144144144144143,"∀X ≤0,
E "" exp "
"L
X",0.44226044226044225,"λ

−1"
"L
X",0.4430794430794431,"λ log(1 + E [λX]) −X

−"
"K
X",0.4438984438984439,"2K
X k=2"
"K
X",0.44471744471744473,"1
k (λX)k
!#"
"K
X",0.44553644553644556,"≤1.
(39) Let:"
"K
X",0.44635544635544633,"dθ(a|x) = 1 [fθ(x) = a] , ∀(x, a) ∈X × A ,"
"K
X",0.44717444717444715,it means that:
"K
X",0.447993447993448,"πQ(a|x) = Eθ∼Q [dθ(a|x)] ,∀(x, a) ∈X × A ."
"K
X",0.4488124488124488,Let λ > 0. The adequate function g we are going to use in combination with Lemma 14 is:
"K
X",0.44963144963144963,"g(θ, Dn) = n
X"
"K
X",0.45045045045045046,"i=1
λ

−1"
"K
X",0.4512694512694513,"λ log(1 + λRp−LIN(dθ)) −
dθ(ai|xi)
p(π0(ai|xi))ci 
−"
"L
X",0.4520884520884521,"2L
X ℓ=2 1
ℓ"
"L
X",0.45290745290745293,"
λ dθ(ai|xi)"
"L
X",0.4537264537264537,"p(π0(ai|xi))ci ℓ = n
X"
"L
X",0.45454545454545453,"i=1
λ

−1"
"L
X",0.45536445536445536,"λ log(1 + λRp−LIN(dθ)) −
dθ(ai|xi)
p(π0(ai|xi))ci 
−"
"L
X",0.4561834561834562,"2L
X ℓ=2"
"L
X",0.457002457002457,dθ(ai|xi) ℓ
"L
X",0.45782145782145783,"
λ
p(π0(ai|xi))ci ℓ
."
"L
X",0.45864045864045866,"By exploiting the i.i.d. nature of the data and exchanging the order of expectations (P is independent
of Dn), we can naturally prove using (39) that:"
"L
X",0.4594594594594595,"Ψg = EP "" n
Y i=1
E "" exp "
"L
X",0.46027846027846026,"λ

−1"
"L
X",0.4610974610974611,"λ log(1 + λRp−LIN(dθ)) −Xi(θ)

−"
"K
X",0.4619164619164619,"2K
X k=2"
"K
X",0.46273546273546273,"1
k (λXi(θ))k
!## ≤1,"
"K
X",0.46355446355446356,as we have :
"K
X",0.4643734643734644,"Xi(θ) =
dθ(ai|xi)
p(π0(ai|xi))ci ≤0
∀i."
"K
X",0.4651924651924652,"Injecting Ψg in Lemma 14, rearranging terms and using that ˆRp−LIN
n
(π) has positive bias concludes
the proof."
"K
X",0.46601146601146604,"Similarly to the OPE section, we use this general bound to obtain a PAC-Bayesian Empirical Second
Moment bound and the PAC-Bayesian LS-LIN bound. That we state directly below:"
"K
X",0.4668304668304668,"Empirical second moment bound.
With L = 1, we obtain the following:"
"K
X",0.46764946764946763,"Corollary 18 (Second Moment Upper bound). Given a prior P on FΘ, δ ∈(0, 1] and λ > 0.
The following bound holds with probability at least 1 −δ uniformly for all distribution Q
over FΘ:"
"K
X",0.46846846846846846,R(πQ) ≤ψλ
"K
X",0.4692874692874693,"
ˆRp
n(πQ) + KL(Q||P) + ln 1"
"K
X",0.4701064701064701,"δ
λn
+ λ"
"K
X",0.47092547092547093,"2
ˆ
Mp−LIN,2
n
(πQ)

.
(40)"
"K
X",0.47174447174447176,"Log Smoothing PAC-Bayesian Bound.
With L →∞, we obtain the following:"
"K
X",0.4725634725634726,"Proposition 19 ( ˆRλ−LIN
n
PAC-Bayes bound). Given a prior P on FΘ, δ ∈(0, 1] and λ > 0,
the following bound holds with probability at least 1 −δ uniformly for all distribution Q over
FΘ:"
"K
X",0.47338247338247336,R(πQ) ≤ψλ
"K
X",0.4742014742014742,"
ˆRλ−LIN
n
(πQ) + KL(Q||P) + ln 1 δ
λn"
"K
X",0.475020475020475,"
.
(41) with:"
"K
X",0.47583947583947583,"ˆRλ-LIN
n
(π) = −1 n n
X i=1"
"K
X",0.47665847665847666,π(ai|xi)
"K
X",0.4774774774774775,"λ
log

1 −
λci
π0(ai|xi) 
."
"K
X",0.4782964782964783,"Following the same proof schema as of the OPE section, we can demonstrate that the Log Smoothing
PAC-Bayesian bound dominates the Empirical Second moment PAC-Bayesian bound L = 1. How-
ever, we use the bound of L = 1 as an intermediary to state the dominance of the Log Smoothing
PAC-Bayesian bound."
"K
X",0.47911547911547914,"Indeed, we can easily compare the result obtained with L = 1 to previously derived PAC-Bayesian
bounds for off-policy learning. We start by writing down the conditional Bernstein bound of Sakhi
et al. [49] holding for the (linear) cIPS (p : x →max(x, τ)). For a policy πQ and a λ > 0, we have:"
"K
X",0.4799344799344799,"R(πQ) ≤ˆRτ
n(πQ) + s"
"K
X",0.48075348075348073,KL(Q||P) + ln 4√n
"K
X",0.48157248157248156,"δ
2n
+ KL(Q||P) + ln 2"
"K
X",0.4823914823914824,"δ
λn
+ λg (λ/τ) Vτ
n(πQ)."
"K
X",0.4832104832104832,"R(πQ) ≤ˆRτ
n(πQ) + KL(Q||P) + ln 1"
"K
X",0.48402948402948404,"δ
λn
+ λ"
"K
X",0.48484848484848486,"2
ˆSτ
n(πQ).
(L = 1)"
"K
X",0.4856674856674857,"We can observe that the previously derived conditional Bernstein bound has several terms that make
it less tight:"
"K
X",0.4864864864864865,"• It has an additional, strictly positive square root KL divergence term."
"K
X",0.4873054873054873,"• The multiplicative factor g(λ/τ) is always bigger than 1/2, and diverges when τ →0."
"K
X",0.4881244881244881,"• With enough data (n ≫1), we also have:"
"K
X",0.48894348894348894,"ˆSτ
n(πQ) ≈E

πQ(a|x)
max{π0(a|x), τ}2 c(a, x)2

≤E

πQ(a|x)
max{π0(a|x), τ}2"
"K
X",0.48976248976248976,"
≈Vτ
n(πQ)."
"K
X",0.4905814905814906,"These observations confirm that the new bound derived with L = 1 is tighter than what was previously
proposed for cIPS, especially when n ≫1. As our bound can work for other estimators, we also
compare it to a recently proposed PAC-Bayes bound in Aouali et al. [5] for the exponentially-smoothed
estimator (p : x →xα) with α ∈[0, 1]:"
"K
X",0.4914004914004914,"R(πQ) ≤ˆRα
n(πQ) + s"
"K
X",0.49221949221949224,KL(Q||P) + ln 4√n
"K
X",0.49303849303849306,"δ
2n
+ KL(Q||P) + ln 2"
"K
X",0.49385749385749383,"δ
λn
+ λ 2 "
"K
X",0.49467649467649466,"Vα
n (πQ) + ˆSα
n (πQ)

."
"K
X",0.4954954954954955,"R(πQ) ≤ˆRα
n(πQ) + KL(Q||P) + ln 1"
"K
X",0.4963144963144963,"δ
λn
+ λ"
"K
X",0.49713349713349714,"2
ˆSα
n (πQ).
(L = 1)"
"K
X",0.49795249795249796,"We can clearly see that the previously proposed bound for the exponentially smoothed estimator has
two additional positive quantities that makes it less tight than our bound. In addition, computing our
bound does not rely on expectations under π0 (contrary to the previous bounds that have Vn) which
alleviates the need to access the logging policy and reduce the computations."
"K
X",0.4987714987714988,"This demonstrates the superiority of L = 1 compared to existing variance sensitive PAC-Bayesian
bounds. It means that L →∞is even better. We can also prove that the Log smoothing PAC-Bayesian
Bound is better than the one of IX in Gabbianelli et al. [21]. Indeed, using log(1 + x) ≥
x
1+x/2 for
all x ≥0, we have for any P, Q ∈P(Θ) and λ > 0: ψλ"
"K
X",0.4995904995904996,"
ˆRλ−LIN
n
(πQ) + KL(Q||P) + ln 1 δ
λn"
"K
X",0.5004095004095004,"
≤ˆRλ−LIN
n
(πQ) + KL(Q||P) + ln 1 δ
λn ≤−1 n n
X i=1"
"K
X",0.5012285012285013,πQ(ai|xi)
"K
X",0.502047502047502,"λ
log

1 −
λci
π0(ai|xi)"
"K
X",0.5028665028665029,"
+ KL(Q||P) + ln 1 δ
λn ≤1 n n
X i=1"
"K
X",0.5036855036855037,"πQ(ai|xi)
π0(ai|xi) −λci/2 + KL(Q||P) + ln 1 δ
λn"
"K
X",0.5045045045045045,"≤ˆRλ−IX
n
(πQ) + KL(Q||P) + ln 1"
"K
X",0.5053235053235053,"δ
λn
,
(IX-bound)"
"K
X",0.5061425061425061,"with the last implication leveraging that the cost is always bigger than −1/ This proves that our bound
is better than the IX bound. This means that our PAC-Bayesian bound is better than all existing
PAC-Bayesian off-policy learning bounds."
"K
X",0.506961506961507,"E.5
OPL: Formal comparison with IX PAC-Bayesian learning suboptimality"
"K
X",0.5077805077805078,"Let us begin by stating results from the IX work [21]. Recall that the IX estimator is defined for any
λ > 0, by:"
"K
X",0.5085995085995086,"ˆRIX−λ
n
(π) = 1 n n
X i=1"
"K
X",0.5094185094185094,"π(ai|xi)
π0(ai|xi) + λ/2ci,"
"K
X",0.5102375102375102,"and that we used the linearized version of the LS estimator, LS-LIN defined as:"
"K
X",0.5110565110565111,"ˆRλ-LIN
n
(π) = −1 n n
X i=1"
"K
X",0.5118755118755118,π(ai|xi)
"K
X",0.5126945126945127,"λ
log

1 −
λci
π0(ai|xi) 
."
"K
X",0.5135135135135135,"Let Θ be a parameter space and P(Θ) be the set of all probability distribution on Θ. Our goal is to
find the best policy in a chosen class L(Θ) ⊂P(Θ):"
"K
X",0.5143325143325144,"πQ∗= argmin
Q∈L(Θ)
R(πQ)."
"K
X",0.5151515151515151,"For λ > 0 and a prior P ∈P(Θ), the PAC-Bayesian learning strategy suggested in Gabbianelli et al.
[21] is to find in L(Θ) ⊂P(Θ):"
"K
X",0.515970515970516,"ˆπIX
Qn = argmin
Q∈L(Θ)"
"K
X",0.5167895167895168,"
ˆRIX−λ
n
(πQ) + KL(Q||P) λn 
."
"K
X",0.5176085176085176,This learning strategy suffers from a suboptimality bounded in the result below:
"K
X",0.5184275184275184,"Proposition 20 (Suboptimality of the IX PAC-Bayesian learning strategy from [21]). Let
λ > 0 and δ ∈(0, 1]. Then, it holds with probability at least 1 −δ that"
"K
X",0.5192465192465192,"0 ≤R(ˆπIX
Qn) −R(πQ∗) ≤λCλ/2(πQ∗) + 2 (KL(Q∗||P) + ln(2/δ)) λn
, where"
"K
X",0.5200655200655201,"Cλ(π) = E

π(a|x)
π2
0(a|x) + λπ0(a|x)|c|

."
"K
X",0.5208845208845209,"Similarly for PAC-Bayesian learning, both suboptimalities (LS and IX) have the same form, they only
depend on two different quantities (SLIN
λ
and Cλ respectively). For a π ∈Π and λ > 0, If we can
identify when SLIN
λ
(π) ≤Cλ/2(π), then we can prove that the sub-optimality of LS PAC-Bayesian
learning strategy is better than the one of IX in certain cases. Luckily, this is always the case, and it is
stated formally below."
"K
X",0.5217035217035217,Proposition 21. Let π ∈Π and λ > 0. We have:
"K
X",0.5225225225225225,"SLIN
λ
(π) ≤Cλ/2(π).
(42)"
"K
X",0.5233415233415234,"Proof. Let π ∈Π and λ > 0, and recall that:"
"K
X",0.5241605241605242,"SLIN
λ (π) = E

π(a|x)c2"
"K
X",0.5249795249795249,"π2
0(a|x) −λπ0(a|x)c 
."
"K
X",0.5257985257985258,We have:
"K
X",0.5266175266175266,"Cλ/2(π) −SLIN
λ
(π) = E"
"K
X",0.5274365274365275,"""
π(a|x)
π2
0(a|x) + λ"
"K
X",0.5282555282555282,"2 π0(a|x)|c| −
π(a|x)c2"
"K
X",0.5290745290745291,"π2
0(a|x) −λπ0(a|x)c # = E """
"K
X",0.5298935298935299,π(a|x)|c|
"K
X",0.5307125307125307,"1
π2
0(a|x) + λ"
"K
X",0.5315315315315315,"2 π0(a|x) −
|c|
π2
0(a|x) + λπ0(a|x)|c| !# = E """
"K
X",0.5323505323505323,π(a|x)|c|
"K
X",0.5331695331695332,"π2
0(a|x) (1 −|c|) + λ"
"K
X",0.533988533988534,2 π0(a|x)|c|
"K
X",0.5348075348075348,"(π2
0(a|x) + λ"
"K
X",0.5356265356265356,"2 π0(a|x))(π2
0(a|x) + λπ0(a|x)|c|) !# ≥0."
"K
X",0.5364455364455365,"Similarly, this means that the suboptimality of LS-LIN PAC-Bayesian learning strategy is also, better
bounded than the one of IX."
"K
X",0.5372645372645373,"Minimax optimality of our learning strategy.
From Jin et al. [28, Theorem 4.4] we can state that
the minimax suboptimality lower bound, in the case of deterministic optimal policies is of the rate
O(1/
√"
"K
X",0.538083538083538,"nC∗) with infx∈X π0(π∗(x)|x) > C∗. Our bound as well as IX bound match this minimax
lower bound, as:"
"K
X",0.5389025389025389,"SLIN
λ (π∗) = Ex,c 
c2"
"K
X",0.5397215397215397,"π0(π∗(x)|x) −λc 
≤1 C∗"
"K
X",0.5405405405405406,"Cλ(π∗) = Ex,c"
"K
X",0.5413595413595413,"
|c|
π0(π∗(x)|x) + λ 
≤1 C∗."
"K
X",0.5421785421785422,"One can see that for both, selecting a λ∗= r"
"K
X",0.542997542997543,"2 (KL(Q∗||P) + ln(2/δ)) C∗ n
,"
"K
X",0.5438165438165438,"gets you the desired bound, matching this minimax rate."
"K
X",0.5446355446355446,"F
Proofs of OPE"
"K
X",0.5454545454545454,"F.1
Proof of high order empirical moments bound (Proposition 1)"
"K
X",0.5462735462735463,"Proposition (Empirical moments risk bound). Let π ∈Π, L ≥1, δ ∈(0, 1], λ > 0 and h
satisfying (C1). Then it holds with probability at least 1 −δ that"
"K
X",0.5470925470925471,"R(π) ≤ψλ

ˆRh
n(π) +"
"L
X",0.547911547911548,"2L
X ℓ=2 λℓ−1"
"L
X",0.5487305487305487,"ℓ
ˆ
Mh,ℓ
n (π) + ln(1/δ) λn 
,"
"L
X",0.5495495495495496,"where ψλ and ˆ
Mh,ℓ
n (π) are defined in (5), respectively, and recall that ψλ(x) ≤x."
"L
X",0.5503685503685504,"Proof. Let L ∈N∗, λ > 0 and X ≥0 a positive random variable. We have 2L −1 ≥1, and with
the decreasing nature of f(2L−1) (Lemma 13), we also have:"
"L
X",0.5511875511875511,"f(2L−1)(0) ≥f2L−1(λX) ⇐⇒
1
2L ≥−log(1 + λX) −P2L−1
l=1
(−1)ℓ−1"
"L
X",0.552006552006552,"k
(λX)ℓ"
"L
X",0.5528255528255528,(λX)2L ⇐⇒
"L
X",0.5536445536445537,"2L
X ℓ=1"
"L
X",0.5544635544635544,(−1)ℓ−1
"L
X",0.5552825552825553,"k
(λX)ℓ≤log(1 + λX) ⇐⇒exp"
"L
X",0.5561015561015561,"2L
X ℓ=1"
"L
X",0.556920556920557,(−1)ℓ−1
"L
X",0.5577395577395577,"ℓ
(λX)ℓ
!"
"L
X",0.5585585585585585,"≤1 + λX =⇒E "" exp"
"L
X",0.5593775593775594,"2L
X ℓ=1"
"L
X",0.5601965601965602,(−1)ℓ−1
"L
X",0.561015561015561,"ℓ
(λX)ℓ
!#"
"L
X",0.5618345618345618,"≤1 + λE [X] =⇒E "" exp"
"L
X",0.5626535626535627,"2L
X ℓ=1"
"L
X",0.5634725634725635,(−1)ℓ−1
"L
X",0.5642915642915642,"ℓ
(λX)ℓ
!#"
"L
X",0.5651105651105651,"≤exp ((log(1 + λE [X])) =⇒E "" exp "
"L
X",0.5659295659295659,λ(X −1
"L
X",0.5667485667485668,λ log (1 + λE [X])) +
"L
X",0.5675675675675675,"2L
X ℓ=2"
"L
X",0.5683865683865684,(−1)ℓ−1
"L
X",0.5692055692055692,"ℓ
(λX)ℓ
!# ≤1."
"L
X",0.5700245700245701,"For any X ≤0, we can inject −X ≥0 to obtain:"
"L
X",0.5708435708435708,"∀X ≤0,
E "" exp "
"L
X",0.5716625716625716,"λ

−1"
"L
X",0.5724815724815725,"λ log (1 −λE [X]) −X

−"
"L
X",0.5733005733005733,"2L
X ℓ=2"
"L
X",0.5741195741195741,"1
ℓ(λX)ℓ
!#"
"L
X",0.5749385749385749,"≤1.
(43)"
"L
X",0.5757575757575758,"The result in Equation (43) will be combined with Chernoff Inequality (Lemma 12) to finally prove
our bound. Let λ > 0, for our problem, we define the random variable Xi to use in the Chernoff
Inequality as:"
"L
X",0.5765765765765766,Xi = −1
"L
X",0.5773955773955773,λ log (1 −λE [h]) −hi −
"L
X",0.5782145782145782,"2L
X ℓ=2"
"L
X",0.579033579033579,"1
ℓ(λhi)ℓ."
"L
X",0.5798525798525799,"For any a ∈R, this gives us the following: P  X"
"L
X",0.5806715806715806,"i∈[n]
Xi > a "
"L
X",0.5814905814905815,≤(E [exp (λX1)])n exp(−λa) P  −n
"L
X",0.5823095823095823,"λ log (1 −λE [h]) −
X i∈[n]  hi +"
"L
X",0.5831285831285832,"2L
X ℓ=2"
"L
X",0.583947583947584,"1
ℓ(λhi)ℓ
! > a "
"L
X",0.5847665847665847,≤(E [exp (λX1)])n exp(−λa) P  −n
"L
X",0.5855855855855856,"λ log (1 −λE [h]) −
X i∈[n]  hi +"
"L
X",0.5864045864045864,"2L
X ℓ=2"
"L
X",0.5872235872235873,"1
ℓ(λhi)ℓ
! > a "
"L
X",0.588042588042588,"≤exp(−λa)
(Use of Equation (43))"
"L
X",0.5888615888615889,"Solving for δ = exp(−λa), we get: P  −n"
"L
X",0.5896805896805897,"λ log (1 −λE [h]) −
X i∈[n]  hi +"
"L
X",0.5904995904995906,"2L
X ℓ=2"
"L
X",0.5913185913185913,"1
ℓ(λhi)ℓ
!"
"L
X",0.5921375921375921,> ln(1/δ) λ  ≤δ P  −1
"L
X",0.592956592956593,λ log (1 −λE [h]) −1 n X i∈[n]  hi +
"L
X",0.5937755937755937,"2L
X ℓ=2 λℓ ℓhℓ
i !"
"L
X",0.5945945945945946,> ln(1/δ) λn  ≤δ P  −1
"L
X",0.5954135954135954,"λ log (1 −λE [h]) −ˆRh
n(π) −"
"L
X",0.5962325962325963,"2L
X ℓ=2 λℓ−1"
"L
X",0.597051597051597,"ℓ
ˆ
Mh,ℓ
n (π) > ln(1/δ) λn ! ≤δ P  −1"
"L
X",0.5978705978705978,"λ log (1 −λE [h]) > ˆRh
n(π) +"
"L
X",0.5986895986895987,"2L
X ℓ=2 λℓ−1"
"L
X",0.5995085995085995,"ℓ
ˆ
Mh,ℓ
n (π)ln(1/δ) λn ! ≤δ."
"L
X",0.6003276003276004,"This means that the following, complementary event will hold with probability at least 1 −δ: −1"
"L
X",0.6011466011466011,"λ log (1 −λE [h]) ≤ˆRh
n(π) +"
"L
X",0.601965601965602,"2L
X ℓ=2 λℓ−1"
"L
X",0.6027846027846028,"ℓ
ˆ
Mh,ℓ
n (π)ln(1/δ) λn
."
"L
X",0.6036036036036037,"ψλ being a non-decreasing function, applying it to the two sides of this inequality gives us:"
"L
X",0.6044226044226044,"E [h] ≤ψλ

ˆRh
n(π) +"
"L
X",0.6052416052416052,"2L
X ℓ=2 λℓ−1"
"L
X",0.6060606060606061,"ℓ
ˆ
Mh,ℓ
n (π) + ln(1/δ) λn 
."
"L
X",0.6068796068796068,"Finally, h satisfies (C1), this means that the bound is also an upper bound on the true risk, giving:"
"L
X",0.6076986076986077,"R(π) ≤ψλ

ˆRh
n(π) +"
"L
X",0.6085176085176085,"2L
X ℓ=2 λℓ−1"
"L
X",0.6093366093366094,"ℓ
ˆ
Mh,ℓ
n (π) + ln(1/δ) λn 
,"
"L
X",0.6101556101556102,which concludes the proof.
"L
X",0.6109746109746109,"F.2
Proof of the impact of L on the bound’s tightness (Proposition 2)"
"L
X",0.6117936117936118,"Proposition (Impact of L on the bound’s tightness). Let π ∈Π, δ ∈(0, 1], λ > 0, L ≥1
and h satisfying (C1). Let"
"L
X",0.6126126126126126,"U λ,h
L (π) = ψλ"
"L
X",0.6134316134316135,"ˆRh
n(π) + ln(1/δ) λn
+"
"L
X",0.6142506142506142,"2L
X ℓ=2 λℓ−1"
"L
X",0.6150696150696151,"ℓ
ˆ
Mh,ℓ
n (π) !"
"L
X",0.6158886158886159,"be the upper bound in Equation (6). Then,"
"L
X",0.6167076167076168,"λ ≤min
i∈[n]"
"L
X",0.6175266175266175,"
2L + 2
(2L + 1)|hi|"
"L
X",0.6183456183456183,"
=⇒U λ,h
L+1(π) ≤U λ,h
L (π) .
(44)"
"L
X",0.6191646191646192,which implies that:
"L
X",0.61998361998362,"λ ≤min
i∈[n]  1 |hi|"
"L
X",0.6208026208026208,"
=⇒U λ,h
L (π) is a decreasing function w.r.t L."
"L
X",0.6216216216216216,"Proof. We want to prove the implication (44) from which the condition on the decreasing nature of
our bound will follow. Indeed, Let us suppose that (44) is true, we have:"
"L
X",0.6224406224406225,"λ ≤min
i∈[n]  1 |hi|"
"L
X",0.6232596232596233,"
=⇒∀L ≥1,
λ ≤min
i∈[n]"
"L
X",0.6240786240786241,"
2L + 2
(2L + 1)|hi| "
"L
X",0.6248976248976249,"=⇒∀L ≥1,
U λ,h
L+1(π) ≤U λ,h
L (π)
(Using (44))"
"L
X",0.6257166257166257,"=⇒U λ,h
L (π) is a decreasing function w.r.t L."
"L
X",0.6265356265356266,Now let us prove the implication in (44). We have for any L ≥1:
"L
X",0.6273546273546273,"U λ,h
L+1(π) ≤U λ,h
L (π) ⇐⇒"
"L
X",0.6281736281736282,"2L+2
X"
"L
X",0.628992628992629,ℓ=2L+1 λℓ−1
"L
X",0.6298116298116299,"ℓ
ˆ
Mh,ℓ
n (π) ≤0 ⇐⇒λ2L n n
X"
"L
X",0.6306306306306306,"i=1
h2L+1
i"
"L
X",0.6314496314496314,"
1
2L + 1 +
λhi
2L + 2 
≤0"
"L
X",0.6322686322686323,"As hi ≤0, we can ensure this inequality by choosing a λ that verifies:"
"L
X",0.633087633087633,"∀i ∈[n],
λ ≤

2L + 2
(2L + 1)|hi|"
"L
X",0.6339066339066339,"
⇐⇒λ ≤min
i∈[n]"
"L
X",0.6347256347256347,"
2L + 2
(2L + 1)|hi| "
"L
X",0.6355446355446356,which concludes the proof.
"L
X",0.6363636363636364,"F.3
Comparisons of the bounds U λ
L (Proposition 3)"
"L
X",0.6371826371826372,"We compare the bounds evaluated in their optimal regularisation function h. We start by stating the
proposition and proving it."
"L
X",0.638001638001638,"Proposition. Let π ∈Π, and λ > 0, we define:"
"L
X",0.6388206388206388,"U λ
L(π) = min
h U λ,h
L (π)."
"L
X",0.6396396396396397,"Then, for any λ > 0, it holds that for any L > 1:"
"L
X",0.6404586404586404,"U λ
L(π) ≤U λ
1 (π)."
"L
X",0.6412776412776413,"In particular, ∀λ > 0:"
"L
X",0.6420966420966421,"U λ
∞(π) ≤U λ
1 (π) ,
(45)"
"L
X",0.642915642915643,"Proof. Let π ∈Π, λ > 0 and
U λ
L(π) = min
h U λ,h
L (π)."
"L
X",0.6437346437346437,We can prove (see Appendix F.4) that:
"L
X",0.6445536445536445,"U λ
1 (π) = U λ,h∗,1
1
(π) = ψλ

ˆRh∗,1
n
(π) + λ"
"L
X",0.6453726453726454,"2
ˆ
Mh∗,1,2
n
(π) + ln(1/δ) λn "
"L
X",0.6461916461916462,"with:
h∗,1(p, q, c) = −min(|c|p/q, 1/λ),"
"L
X",0.647010647010647,and that (see Appendix F.7):
"L
X",0.6478296478296478,"U λ
∞(π) = ψλ

ˆRλ
n(π) + ln(1/δ) λn 
."
"L
X",0.6486486486486487,"From Proposition 2, we have that for any h:"
"L
X",0.6494676494676495,"λ ≤min
i∈[n]  1 |hi|"
"L
X",0.6502866502866503,"
=⇒U λ,h
L (π) is a decreasing function w.r.t L."
"L
X",0.6511056511056511,"It appears that the optimal function h∗,1 respects this condition, as by definition:"
"L
X",0.6519246519246519,"min
i∈[n]"
"L
X",0.6527436527436528,"
1
|(h∗,1)i| 
≥λ,"
"L
X",0.6535626535626535,"meaning that:
U λ,h∗,1
L
(π) is a decreasing function w.r.t L."
"L
X",0.6543816543816544,"This result suggests that the Empirical Second Moment bound, evaluated in its optimal function h∗,1,
is always bigger than bounds with additional moments (evaluated in the same h∗,1). This leads us to
the result wanted, as for any L > 1:"
"L
X",0.6552006552006552,"U λ
L(π) = min
h U λ,h
L (π) ≤U λ,h∗,1
L
(π) ≤U λ,h∗,1
1
(π) = U λ
1 (π)."
"L
X",0.6560196560196561,"In particular, we get:"
"L
X",0.6568386568386568,"U λ
∞(π) ≤U λ
1 (π),"
"L
X",0.6576576576576577,which ends the proof.
"L
X",0.6584766584766585,"This means that U λ
∞is tighter than U λ
1 , and thus can also be tighter than empirical Bernstein."
"L
X",0.6592956592956593,"F.4
Proof of the optimality of global clipping for Corollary 4"
"L
X",0.6601146601146601,"Proposition (Optimal h for L = 1). Let λ > 0. The function h that minimizes the bound for
L = 1, giving the tightest result is:"
"L
X",0.6609336609336609,"∀i,
hi = h(π(ai|xi), π0(ai|xi), ci)) = −min
 π(ai|xi)"
"L
X",0.6617526617526618,"π0(ai|xi)|ci|, 1 λ "
"L
X",0.6625716625716626,"This means that when the costs are binary, we obtain the classical Clipping estimator of
parameter 1/λ:"
"L
X",0.6633906633906634,"hi = min
 π(ai|xi)"
"L
X",0.6642096642096642,"π0(ai|xi), 1 λ 
ci."
"L
X",0.665028665028665,"Proof. We want to look for the value of h that minimizes the bound. Formally, by fixing all variables
of the bound, this problem reduces to:"
"L
X",0.6658476658476659,"argmin
h∈(C1)
ˆRh
n(π) + λ"
"L
X",0.6666666666666666,"2
ˆ
Mh,2
n (π) = argmin
h∈(C1)"
N,0.6674856674856675,"1
n n
X i=1"
N,0.6683046683046683,"
hi + λ"
N,0.6691236691236692,"2 h2
i 
."
N,0.6699426699426699,"The objective decomposes across data points, so we can solve it for every hi independently. Let us
fix a j ∈[n], the following problem:"
N,0.6707616707616708,"argmin
hj∈R
ˆRh
n(π) + λ"
N,0.6715806715806716,"2
ˆ
Mh,2
n (π) = argmin
hj∈R"
N,0.6723996723996724,"
hj + λ"
N,0.6732186732186732,"2 h2
j "
N,0.674037674037674,"subject to
hj ≥π(aj|xj)"
N,0.6748566748566749,π0(aj|xj)cj
N,0.6756756756756757,"is strongly convex in hj. We write the KKT conditions for hj to be optimal; there exists α∗that
verifies:"
N,0.6764946764946765,"1 + λhj −α∗= 0
(46)
α∗≥0
(47)"
N,0.6773136773136773,"α∗
 π(aj|xj)"
N,0.6781326781326781,π0(aj|xj)cj −hj
N,0.678951678951679,"
= 0
(48)"
N,0.6797706797706797,hj ≥π(aj|xj)
N,0.6805896805896806,"π0(aj|xj)cj
(49)"
N,0.6814086814086814,We study the two following two cases:
N,0.6822276822276823,"Case 1:
hj ≤−1"
N,0.683046683046683,"λ :
we have α∗= 1 + λhj ≤0 =⇒α∗= 0, meaning that:"
N,0.6838656838656839,hj = −1 λ
N,0.6846846846846847,"Case 2:
hj > −1 λ :"
N,0.6855036855036855,"we have α∗= 1 + λhj > 0, which combined to condition (36) gives:"
N,0.6863226863226863,hj = π(aj|xj)
N,0.6871416871416871,π0(aj|xj)cj.
N,0.687960687960688,The two results combined mean that we always have:
N,0.6887796887796888,hj ≥−1
N,0.6895986895986896,"λ, and whenever hj > −1"
N,0.6904176904176904,λ =⇒hj = π(aj|xj)
N,0.6912366912366913,π0(aj|xj)cj.
N,0.6920556920556921,We deduce that hj has the following form:
N,0.6928746928746928,"hj = h(π(aj|xj), π0(aj|xj), cj) = −min
 π(aj|xj)"
N,0.6936936936936937,"π0(aj|xj) |cj| , 1 λ"
N,0.6945126945126945,"
(50)"
N,0.6953316953316954,"α∗= 1 −λ min
 π(aj|xj)"
N,0.6961506961506961,"π0(aj|xj) |cj| , 1 λ"
N,0.696969696969697,"
(51)"
N,0.6977886977886978,"These values verify the KKT conditions. As the problem is strongly convex, hj has a unique possible
value and must be equal to equation (38). The form of hj is a global clipping that includes the cost in
the function as well. In the case where the cost function c is binary:"
N,0.6986076986076986,"∀i
ci ∈{−1, 0},"
N,0.6994266994266994,we recover the classical Clipping with parameter 1/λ as an optimal solution for h:
N,0.7002457002457002,"hj = min
 π(aj|xj)"
N,0.7010647010647011,"π0(aj|xj), 1 λ 
cj."
N,0.7018837018837019,"F.5
Comparison with empirical Bernstein"
N,0.7027027027027027,"We begin by comparing the Second Moment Bound with Swaminathan and Joachims [55]’s bound as
they both manipulate similar quantities. The bound of [55] uses the Empirical Bernstein bound of
[36] applied to the Clipping Estimator. We recall its expression below for a parameter M > 0:"
N,0.7035217035217035,"ˆRM
n (π) = 1 n n
X"
N,0.7043407043407044,"i=1
min
 π(ai|xi)"
N,0.7051597051597052,"π0(ai|xi), M

ci."
N,0.7059787059787059,We also give below the Empirical Bernstein Bound applied to this estimator:
N,0.7067977067977068,"Proposition (Empirical Bernstein for Clipping of [55]). Let π ∈Π, δ ∈(0, 1] and M > 0.
Then it holds with probability at least 1 −δ that"
N,0.7076167076167076,"R(π) ≤ˆRM
n (π) + s"
N,0.7084357084357085,"2 ˆV M
n (π) ln(2/δ)"
N,0.7092547092547092,"n
+ 7M ln(2/δ)"
N,0.7100737100737101,"3(n −1)
,
(52)"
N,0.7108927108927109,"with ˆV M
n (π) the empirical variance of the clipping estimator."
N,0.7117117117117117,"We are usually interested in the case where π and π0 are different, leading to substantial importance
weights. In this practical scenario, the variance and the second moment are of the same magnitude of
M. Indeed, one can see it from the following equality:"
N,0.7125307125307125,"ˆV M
n (π)
| {z }
O(M)"
N,0.7133497133497133,"= ˆ
MM,2
n
(π)
|
{z
}
O(M)"
N,0.7141687141687142,"−

ˆRM
n (π)
2"
N,0.714987714987715,"|
{z
}
O(¯c2)"
N,0.7158067158067158,"≈ˆ
MM,2
n
(π)
|
{z
}
O(M)"
N,0.7166257166257166,(M ≫¯c2 = o(1).)
N,0.7174447174447175,"This means that in practical scenarios, the empirical variance and the empirical second moment are
approximately the same. Recall that the Second Moment Bound works for any regularizer h, As
Clipping satisfies (C1), we give the Second Moment Upper of Corollary 4 with Clipping below:"
N,0.7182637182637183,"ψλ

ˆRM
n (π) + λ"
N,0.719082719082719,"2
ˆ
MM,2
n
(π) + ln(1/δ) λn"
N,0.7199017199017199,"
≤ˆRM
n (π) + λ"
N,0.7207207207207207,"2
ˆ
MM,2
n
(π) + ln(1/δ)"
N,0.7215397215397216,"λn
(ψλ(x) ≤x, ∀x)"
N,0.7223587223587223,"≤ˆRM
n (π) + λ"
N,0.7231777231777232,"2
ˆ
MM,2
n
(π) + ln(1/δ) λn
."
N,0.723996723996724,"Choosing a λ ≈
q"
N,0.7248157248157249,"2 ln(1/δ)/(n ˆ
MM,2
n
(π)) gives us an upper bound that is close to:"
N,0.7256347256347256,"ˆRM
n (π) + λ"
N,0.7264537264537264,"2
ˆ
MM,2
n
(π) + ln(1/δ)"
N,0.7272727272727273,"λn
≈ˆRM
n (π) + s"
N,0.7280917280917281,"2 ˆ
MM,2
n
(π) ln(1/δ) n"
N,0.7289107289107289,"≈ˆRM
n (π) + s"
N,0.7297297297297297,"2 ˆV M
n (π) ln(1/δ) n"
N,0.7305487305487306,"≤ˆRM
n (π) + s"
N,0.7313677313677314,"2 ˆV M
n (π) ln(2/δ)"
N,0.7321867321867321,"n
+ 7M ln(2/δ)"
N,0.733005733005733,3(n −1) .
N,0.7338247338247338,"This means that in practical scenarios, and with a good choice of λ ∼O(1/√n), the Second Moment
bound would be better than the Empirical Bernstein bound, and this difference will be even greater
when M ≫1. This is aligned with our experiments, where we see that the new Second Moment
bound is much tighter in practice. This also confirms that the Logarithmic smoothing bound is even
tighter, because it is smaller than the Second Moment bound as stated in Proposition 3."
N,0.7346437346437347,"F.6
Proof of the L →∞bound (Corollary 5)"
N,0.7354627354627354,"Proposition (Empirical Logarithmic Smoothing bound with L →∞). Let π ∈Π, δ ∈(0, 1]
and λ > 0. Then it holds with probability at least 1 −δ that"
N,0.7362817362817363,"R(π) ≤ψλ

−1 n n
X i=1"
N,0.7371007371007371,"1
λ log (1 −λhi) + ln(1/δ) λn 
."
N,0.737919737919738,"Taking the limit of L naively recovers this form of the bound, but imposes a condition on λ for the
bound to converge. We instead, take another path of proof that does not impose any condition on λ,
developed below. The main idea is to take the limit of L to recover the variable to use along Chernoff."
N,0.7387387387387387,"Proof. Recall that for the proof of the Empirical moments bounds, we used the following random
variable defined with λ > 0:"
N,0.7395577395577395,Xi = −1
N,0.7403767403767404,λ log (1 −λE [h]) −hi −
"L
X",0.7411957411957412,"2L
X ℓ=2"
"L
X",0.742014742014742,"1
ℓ(λhi)ℓ,"
"L
X",0.7428337428337428,"combined with Chernoff Inequality (Lemma 12) to prove our bound. If we take the limit L →∞for
our random variable, we obtain the following random variable:"
"L
X",0.7436527436527437,˜Xi = −1
"L
X",0.7444717444717445,λ log (1 −λE [h]) + 1
"L
X",0.7452907452907452,λ log (1 −λhi) = 1
"L
X",0.7461097461097461,"λ log
 1 −λhi"
"L
X",0.7469287469287469,"1 −λE [h] 
."
"L
X",0.7477477477477478,"We use the random variable ˜Xi with the Chernoff Inequality. For any a ∈R, we have: P  X"
"L
X",0.7485667485667485,"i∈[n]
˜Xi > a "
"L
X",0.7493857493857494,"≤

E
h
exp

λ ˜X1
in
exp(−λa) P  −n"
"L
X",0.7502047502047502,"λ log (1 −λE [h]) +
X i∈[n]  1"
"L
X",0.7510237510237511,"λ log (1 −λhi)

> a "
"L
X",0.7518427518427518,"≤

E
h
exp

λ ˜X1
in
exp(−λa)"
"L
X",0.7526617526617526,"On the other hand, we have:"
"L
X",0.7534807534807535,"E
h
exp

λ ˜X1
i
= E [1 −λhi]"
"L
X",0.7542997542997543,1 −λE [h] = 1.
"L
X",0.7551187551187551,"Using this equality and solving for δ = exp(−λa), we get: P  −n"
"L
X",0.7559377559377559,"λ log (1 −λE [h]) +
X i∈[n]  1"
"L
X",0.7567567567567568,"λ log (1 −λhi)

> ln(1/δ) λ  ≤δ P  −1"
"L
X",0.7575757575757576,λ log (1 −λE [h]) + 1 n X i∈[n]
"L
X",0.7583947583947583,"1
λ log (1 −λhi) > ln(1/δ) λn  ≤δ"
"L
X",0.7592137592137592,"This means that the following, complementary event will hold with probability at least 1 −δ: −1"
"L
X",0.76003276003276,"λ log (1 −λE [h]) ≤−1 n n
X i=1"
"L
X",0.7608517608517609,"1
λ log (1 −λhi) + ln(1/δ) λn
."
"L
X",0.7616707616707616,"ψλ being a non-decreasing function, applying it to the two sides of this inequality gives us:"
"L
X",0.7624897624897625,"E [h] ≤ψλ

−1 n n
X i=1"
"L
X",0.7633087633087633,"1
λ log (1 −λhi) + ln(1/δ) λn 
."
"L
X",0.7641277641277642,"As h satisfies (C1), we obtain the required inequality:"
"L
X",0.764946764946765,"R(π) ≤ψλ

−1 n n
X i=1"
"L
X",0.7657657657657657,"1
λ log (1 −λhi) + ln(1/δ) λn 
."
"L
X",0.7665847665847666,and conclude the proof.
"L
X",0.7674037674037674,"F.7
Proof of the optimality of IPS for Corollary 5"
"L
X",0.7682227682227682,"Proposition (Optimal h for L →∞). Let λ > 0. The function h that minimizes the bound
for L →∞, giving the tightest result is:"
"L
X",0.769041769041769,"∀i,
hi = h(π(ai|xi), π0(ai|xi), ci)) = π(ai|xi)"
"L
X",0.7698607698607699,π0(ai|xi)ci
"L
X",0.7706797706797707,Proof. The proof of this proposition is quite simple. The function:
"L
X",0.7714987714987716,f(x) = −log (1 −λx)
"L
X",0.7723177723177723,"is increasing. This means that the lowest possible value of hi ensures the tightest result. As our
variables hi verifies (C1), we recover IPS as an optimal choice for this bound."
"L
X",0.7731367731367731,"F.8
Comparison with the IX bound (Proposition 8)"
"L
X",0.773955773955774,"We now attack the recently derived IX bound in Gabbianelli et al. [21] and show that our newly
proposed bound dominates it in all scenarios."
"L
X",0.7747747747747747,"Proposition (Comparison with IX [21]). Let π ∈Π, δ ∈]0, 1] and λ > 0, the IX bound from
[21] states that we have with at least probability 1 −δ:"
"L
X",0.7755937755937756,"R(π) ≤ˆRλ-IX
n
(π) + ln(1/δ)"
"L
X",0.7764127764127764,"λn
(53) with:"
"L
X",0.7772317772317773,"ˆRλ-IX
n
(π) = 1 n n
X i=1"
"L
X",0.778050778050778,"π(ai|xi)
π0(ai|xi) + λ/2ci."
"L
X",0.7788697788697788,"Let U λ
IX(π) be the IX upper bound defined above, we have for any λ > 0:"
"L
X",0.7796887796887797,"U λ
∞(π) ≤U λ
IX(π) .
(54)"
"L
X",0.7805077805077805,"Proof. Let π ∈Π, δ ∈]0, 1] and λ > 0. Recall that U λ
∞(π) = ψλ

ˆRλ
n(π) + ln(1/δ)"
"L
X",0.7813267813267813,"λn

. We have:"
"L
X",0.7821457821457821,"ψλ

ˆRλ
n(π) + ln(1/δ) λn"
"L
X",0.782964782964783,"
≤ˆRλ
n(π) + ln(1/δ)"
"L
X",0.7837837837837838,"λn
(∀x, ψλ(x) ≤x) ≤−1 n n
X i=1"
"L
X",0.7846027846027847,"1
λ log (1 −λwπ(xi, ai)ci) + ln(1/δ) λn
."
"L
X",0.7854217854217854,"Using the inequality log(1 + x) ≥
x
1+x/2 for all x > 0, we get:"
"L
X",0.7862407862407862,"U λ
∞(π) ≤−1 n n
X i=1"
"L
X",0.7870597870597871,"1
λ log (1 −λwπ(xi, ai)ci) + ln(1/δ) λn ≤1 n n
X i=1"
"L
X",0.7878787878787878,"wπ(xi, ai)
1 −λwπ(xi, ai)ci/2ci + ln(1/δ) λn"
"L
X",0.7886977886977887,"
log(1 + x) ≥
x
1 + x/2  ≤1 n n
X i=1"
"L
X",0.7895167895167895,"π(ai|xi)
π0(ai|xi) −λπ(ai|xi)ci/2ci + ln(1/δ) λn ≤1 n n
X i=1"
"L
X",0.7903357903357904,"π(ai|xi)
π0(ai|xi) + λ/2ci + ln(1/δ)"
"L
X",0.7911547911547911,"λn
(−π(ai|xi)ci ≤1 and ci ≤0)"
"L
X",0.7919737919737919,"≤ˆRIX−λ
n
(π) + ln(1/δ)"
"L
X",0.7927927927927928,"λn
= U λ
IX(π),"
"L
X",0.7936117936117936,which ends the proof.
"L
X",0.7944307944307945,"The result states the dominance of the LS bound compared to IX. The proof of this result also gives
us insight on when the LS bound will be much tighter than IX. Indeed, to obtain the IX bound, LS
bound is loosened through 3 steps:"
"L
X",0.7952497952497952,"1. The use of ψλ(x) ≤x, ∀x."
"L
X",0.7960687960687961,"2. The use of log(1 + λx) ≥
λx
1+λx/2, ∀x ≥0."
"L
X",0.7968877968877969,"3. The use of −π(ai|xi)ci ≤1, ∀i ∈[n]."
"L
X",0.7977067977067978,"The two first inequalities are loose when λ ∼1/√n is not too small, which means that LS will be
much better in problems with few samples. The third inequality is loose when π is not a peaked
policy or the cost is way less than 1. Even if LS bound is always smaller than IX, LS will give way
better result if the number of samples is small, and/or the policy evaluated is diffused."
"L
X",0.7985257985257985,"G
Proofs of OPS and OPL"
"L
X",0.7993447993447993,"G.1
OPS: Proof of suboptimality bound (Proposition 9)"
"L
X",0.8001638001638002,"Proposition (Suboptimality of our selection strategy in (20)). Let λ > 0 and δ ∈(0, 1]. Then,
it holds with probability at least 1 −δ that"
"L
X",0.800982800982801,"0 ≤R(ˆπS
n) −R(πS
∗) ≤λSλ(πS
∗) + 2 ln(2|ΠS|/δ) λn
,"
"L
X",0.8018018018018018,"where πS
∗and ˆπS
n are defined in (19) and (20), and"
"L
X",0.8026208026208026,"Sλ(π) = E

(wπ(x, a)c)2/ (1 −λwπ(x, a)c)

."
"L
X",0.8034398034398035,"In addition, our upper bound is always finite as:"
"L
X",0.8042588042588042,"λSλ(π) = λE
 (wπ(x, a)c)2"
"L
X",0.8050778050778051,"1 −λwπ(x, a)c"
"L
X",0.8058968058968059,"
≤min

|R(π)|, λE

(wπ(x, a)c)2	
≤|R(π)|."
"L
X",0.8067158067158067,"Proof. To prove this bound on the suboptimality of our selection method, we need both an upper
bound and a lower bound on the true risk using the LS estimator. Luckily, we already have derived
them in ?? . For a fixed λ, taking a union of the two bounds over the cardinal of the finite policy class
|Πs|, we get the following holding with probability at least 1 −δ for all π ∈Πs:"
"L
X",0.8075348075348076,"R(π) −ˆRλ
n(π) ≤ln(2|Πs|/δ)"
"L
X",0.8083538083538083,"λn
,
and
ˆRλ
n(π) −R(π) ≤λSλ(π) + ln(2|Πs|/δ) λn
."
"L
X",0.8091728091728092,"As ˆπS
n ∈Πs and by definition of ˆπS
n (minimizer of ˆRλ
n(π)), we have:"
"L
X",0.80999180999181,"R(ˆπS
n) ≤ˆRλ
n(ˆπS
n) + ln(2|Πs|/δ)"
"L
X",0.8108108108108109,"λn
≤ˆRλ
n(ˆπS
∗) + ln(2|Πs|/δ) λn
."
"L
X",0.8116298116298116,"Using the lower bound on the risk of R(ˆπS
∗), we have:"
"L
X",0.8124488124488124,"R(ˆπS
n) ≤ˆRλ
n(ˆπS
∗) + ln(2|Πs|/δ) λn"
"L
X",0.8132678132678133,"≤R(ˆπS
∗) + λSλ(ˆπS
∗) + 2 ln(2|Πs|/δ) λn
."
"L
X",0.814086814086814,which gives us the suboptimality upper bound:
"L
X",0.8149058149058149,"0 ≤R(ˆπS
n) −R(πS
∗) ≤λSλ(πS
∗) + 2 ln(2|ΠS|/δ) λn
."
"L
X",0.8157248157248157,Note that:
"L
X",0.8165438165438166,"λSλ(π) = λE
 (wπ(x, a)c)2"
"L
X",0.8173628173628174,"1 −λwπ(x, a)c"
"L
X",0.8181818181818182,"
≤min

|R(π)|, λE

(wπ(x, a)c)2	
,"
"L
X",0.819000819000819,always ensuring a finite bound.
"L
X",0.8198198198198198,"G.2
OPL: Proof of PAC-Bayesian LS-LIN bound (Proposition 10)"
"L
X",0.8206388206388207,"Proposition (PAC-Bayes learning bound for ˆRλ−LIN
n
). Given a prior P ∈P(Θ), δ ∈(0, 1]
and λ > 0, the following holds with probability at least 1 −δ:"
"L
X",0.8214578214578214,"∀Q ∈P(Θ),
R(πQ) ≤ψλ"
"L
X",0.8222768222768223,"
ˆRλ−LIN
n
(πQ) + KL(Q||P) + ln 1 δ
λn "
"L
X",0.8230958230958231,"Proof. To prove this proposition, we can either take the path of High Order Empirical moments as
for Pessimistic OPE, or we can prove it directly. We provide here a simple proof of this proposition
using ideas from Alquier [1, Corollary 2.5]. Let:"
"L
X",0.823914823914824,"dθ(a|x) = 1 [fθ(x) = a] , ∀(x, a) ∈X × A ,
(55)
it means that:
πQ(a|x) = Eθ∼Q [dθ(a|x)] ,∀(x, a) ∈X × A ."
"L
X",0.8247338247338247,"Recall that to prove a PAC-Bayesian generalization bound, one can rely on the Change of measure
Lemma (Lemma 14). For any λ > 0, the adequate function g to consider is:"
"L
X",0.8255528255528255,"g(θ, Dn) = n
X i=1"
"L
X",0.8263718263718264,"
−log(1 −λR(dθ)) + log

1 −λdθ(ai|xi)ci"
"L
X",0.8271908271908271,"π0(ai|xi)  = n
X"
"L
X",0.828009828009828,"i=1
log "
"L
X",0.8288288288288288,1 −λ dθ(ai|xi)ci
"L
X",0.8296478296478297,"π0(ai|xi)
1 −λR(dθ)  ."
"L
X",0.8304668304668305,"By exploiting the i.i.d. nature of the data and exchanging the order of expectations (P is independent
of Dn), we can naturally prove that:"
"L
X",0.8312858312858313,"Ψg = EP  
n
Y i=1
E  exp  log "
"L
X",0.8321048321048321,1 −λ dθ(ai|xi)ci
"L
X",0.8329238329238329,"π0(ai|xi)
1 −λR(dθ)         = EP  
n
Y i=1
E "
"L
X",0.8337428337428338,1 −λ dθ(ai|xi)ci
"L
X",0.8345618345618345,"π0(ai|xi)
1 −λR(dθ)     = EP "" n
Y i=1"
"L
X",0.8353808353808354,"1 −λR(dθ)
1 −λR(dθ) # = 1."
"L
X",0.8361998361998362,"Injecting Ψg in Lemma 14, gives:"
"L
X",0.8370188370188371,"Eθ∼Q [−log(1 −λR(dθ)] ≤1 n n
X"
"L
X",0.8378378378378378,"i=1
Eθ∼Q"
"L
X",0.8386568386568387,"
−log

1 −λdθ(ai|xi)ci"
"L
X",0.8394758394758395,π0(ai|xi)
"L
X",0.8402948402948403,"
+ KL(Q||P) + ln 1 δ
n ≤1 n n
X"
"L
X",0.8411138411138411,"i=1
Eθ∼Q"
"L
X",0.8419328419328419,"
−dθ(ai|xi) log

1 −λ
ci
π0(ai|xi)"
"L
X",0.8427518427518428,"
+ KL(Q||P) + ln 1 δ
n ≤−1 n n
X"
"L
X",0.8435708435708436,"i=1
πQ(ai|xi) log

1 −λ
ci
π0(ai|xi)"
"L
X",0.8443898443898444,"
+ KL(Q||P) + ln 1 δ
n"
"L
X",0.8452088452088452,"≤λ ˆRλ−LIN
n
(πQ) + KL(Q||P) + ln 1 δ
n
."
"L
X",0.846027846027846,"From the convexity of x →−log(1 + x), we have: −1"
"L
X",0.8468468468468469,λ log (1 −λR(πQ)) ≤1
"L
X",0.8476658476658476,"λEθ∼Q [−log(1 −λR(dθ)] ≤ˆRλ−LIN
n
(πQ) + KL(Q||P) + ln 1"
"L
X",0.8484848484848485,"δ
λn
."
"L
X",0.8493038493038493,Applying the increasing function ψλ of Equation (5) to both sides concludes the proof.
"L
X",0.8501228501228502,"G.3
OPL: Proof of PAC-Bayesian suboptimality bound (Proposition 11)"
"L
X",0.8509418509418509,"Proposition (Suboptimality of the learning strategy in (27)). Let λ > 0, P ∈L(Θ) and
δ ∈(0, 1]. Then, it holds with probability at least 1 −δ that"
"L
X",0.8517608517608518,"0 ≤R(ˆπQn) −R(πQ∗) ≤λSLIN
λ (πQ∗) + 2 (KL(Q∗||P) + ln(2/δ)) λn
, where"
"L
X",0.8525798525798526,"SLIN
λ (π) = E

π(a|x)c2"
"L
X",0.8533988533988534,"π2
0(a|x) −λπ0(a|x)c 
."
"L
X",0.8542178542178542,"In addition, our upper bound is always finite as:"
"L
X",0.855036855036855,"λSLIN
λ (π) ≤min

|R(π)|, λE
π(a|x)c2"
"L
X",0.8558558558558559,"π2
0(a|x)"
"L
X",0.8566748566748567,"
≤|R(π)|."
"L
X",0.8574938574938575,"Proof. To prove this bound on the suboptimality of our learning strategy, we need both a PAC-
Bayesian upper bound and a lower bound on the true risk using the LS-LIN estimator. Luckily, we
already have derived an upper bound in Proposition 10, that we linearize here as ψλ(x) ≤x:"
"L
X",0.8583128583128583,"∀Q ∈P(Θ),
R(πQ) ≤ˆRλ−LIN
n
(πQ) + KL(Q||P) + ln 1"
"L
X",0.8591318591318591,"δ
λn
."
"L
X",0.85995085995086,"For the lower bound, we rely a second time on the Change of measure Lemma (Lemma 14). For any
λ > 0, we choose the following function g:"
"L
X",0.8607698607698607,"g(θ, Dn) = n
X i=1 
−1"
"L
X",0.8615888615888616,"λ log

1 −λdθ(ai|xi)ci"
"L
X",0.8624078624078624,π0(ai|xi)
"L
X",0.8632268632268633,"
−R(dθ) −λSLIN
λ (dθ)

."
"L
X",0.864045864045864,"By exploiting the i.i.d. nature of the data and exchanging the order of expectations (P is independent
of Dn), we can prove that:"
"L
X",0.8648648648648649,"Ψg = EP  
n
Y i=1 "
"L
X",0.8656838656838657,"exp
 
−λ(R(dθ) + λSLIN
λ (dθ))

E  
1"
"L
X",0.8665028665028665,1 −λ dθ(a|x)c
"L
X",0.8673218673218673,"π0(a|x)       ≤EP  
n
Y i=1  exp "
"L
X",0.8681408681408681,"−λ(R(dθ) + λSLIN
λ (dθ)) + E  
1"
"L
X",0.868959868959869,1 −λ dθ(a|x)c
"L
X",0.8697788697788698,"π0(a|x)  −1       ≤EP "" n
Y i=1"
"L
X",0.8705978705978706,"
exp

−λ(R(dθ) + λSLIN
λ (dθ)) + E

λdθ(a|x)c
π0(a|x) −λdθ(a|x)c # ≤EP "" n
Y i=1"
"L
X",0.8714168714168714,"
exp

−λ(R(dθ) + λSLIN
λ (dθ)) + E
 λdθ(a|x)c"
"L
X",0.8722358722358723,π0(a|x) −λc #
"L
X",0.8730548730548731,"(dθ is binary.) ≤EP "" n
Y i=1"
"L
X",0.8738738738738738,"
exp

−λ2SLIN
λ (dθ) + E
 λdθ(a|x)c"
"L
X",0.8746928746928747,π0(a|x) −λc −λdθ(a|x)c
"L
X",0.8755118755118755,"π0(a|x) # ≤EP "" n
Y i=1"
"L
X",0.8763308763308764," 
exp
 
−λ2SLIN
λ (dθ) + λ2SLIN
λ (dθ)

# ≤1,"
"L
X",0.8771498771498771,"giving by rearranging terms, the following PAC-Bayesian bound:"
"L
X",0.877968877968878,"∀Q ∈P(Θ),
ˆRλ−LIN
n
(πQ) ≤R(πQ) + λSLIN
λ (πQ) + KL(Q||P) + ln(2/δ) λn
."
"L
X",0.8787878787878788,"Now we take a union of the the two bounds, for them to hold with probability at least 1 −δ for all Q.
By definition of ˆπQn (minimizer of the upper bound), we have:"
"L
X",0.8796068796068796,"R(ˆπQn) ≤ˆRλ−LIN
n
(ˆπQn) + KL(Qn||P) + ln(2/δ)"
"L
X",0.8804258804258804,"λn
≤ˆRλ−LIN
n
(πQ∗) + KL(Q∗||P) + ln(2/δ) λn
."
"L
X",0.8812448812448812,"Using the lower bound on the risk of R(πQ∗), we have:"
"L
X",0.8820638820638821,"R(ˆπQn) ≤ˆRλ−LIN
n
(πQ∗) + KL(Q∗||P) + ln(2/δ) λn"
"L
X",0.8828828828828829,"≤R(πQ∗) + λSLIN
λ (πQ∗) + KL(Q∗||P) + ln(2/δ) λn
."
"L
X",0.8837018837018837,which gives us the PAC-Bayesian suboptimality upper bound:
"L
X",0.8845208845208845,"0 ≤R(ˆπQn) −R(πQ∗) ≤λSLIN
λ (πQ∗) + 2 (KL(Q∗||P) + ln(2/δ)) λn
."
"L
X",0.8853398853398854,Concluding the proof.
"L
X",0.8861588861588862,"H
Experimental design and detailed experiments"
"L
X",0.8869778869778869,"All our experiments were conducted on a machine with 16 CPUs. The PAC-Bayesian learning
experiments require a moderate amount of computation due to the handling of medium-sized datasets.
However, our experiments remain reproducible with minimal computational resources."
"L
X",0.8877968877968878,"H.1
Off-policy evaluation and selection"
"L
X",0.8886158886158886,"H.1.1
Datasets"
"L
X",0.8894348894348895,"For both our OPE and OPS experiments, we use 11 UCI datasets with different sizes, action spaces
and number of features. The statistics of all these datasets are described in Table 3."
"L
X",0.8902538902538902,Table 3: OPE and OPS: 11 Datasets used from OpenML [8].
"L
X",0.8910728910728911,"Datasets
N
K
p
ecoli
336
8
7
arrhythmia
452
13
279
micro-mass
571
20
1300
balance-scale
625
3
4
eating
945
7
6373
vehicle
846
4
18
yeast
1484
10
8
page-blocks
5473
5
10
optdigits
5620
10
64
satimage
6430
6
36
kropt
28 056
18
6"
"L
X",0.8918918918918919,"H.1.2
(OPE) Tightness of the bounds"
"L
X",0.8927108927108927,"Additional details.
For these experiments, as we only use oracle policies (faulty policies to log data
and we evaluate ideal policies), we use the full 11 datasets without splitting them. The faulty policies
are defined exactly as described in the experiments of Kuzborskij et al. [32]. For each datapoint, the
behavior (faulty) policy plays an action and we record a cost. The triplets datapoint, action and cost
constitute our logged bandit dataset, with which we can compute our estimates and bounds. As we
have access to the true label, the original dataset can be used to compute the true risk of any policy."
"L
X",0.8935298935298935,"Detailed results.
Evaluating the worst case performance of a policy is done through evaluating risk
upper bounds [10, 32]. This means that a better evaluation will solely depend on the tightness of the
bounds used. To this end, given a policy π, we are interested in bounds with a small relative radius
|U(π)/R(π)−1|. We compare our newly derived bounds (cIPS-L=1 for U λ
1 and LS for U λ
∞both with
λ = 1/√n) to SNIPS-ES: the Efron Stein bound for Self Normalized IPS [32], cIPS-EB: Empirical
Bernstein for Clipping [55] and the recent IX: Implicit Exploration bound [21]. We use all 11 datasets,
with different behavior policies (τ0 ∈{0.2, 0.25, 0.3}) and different noise levels (ϵ ∈{0., 0.1, 0.2})
to evaluate ideal policies with different temperatures (τ ∈{0.1, 0.2, 0.3, 0.4, 0.5}), defining ∼500
different scenarios to validate our findings. In addition to the cumulative distribution of the relative
radius of the considered bounds of Figure 2. We give two tables in the following: the average relative"
"L
X",0.8943488943488943,"Table 4: OPE: Average relative radius for each datasets
Datasets
SN-ES
cIPS-EB
IX
cIPS-L=1
LS
ecoli
1.00
1.00
0.676
0.752
0.573
arrhythmia
1.00
1.00
0.677
0.707
0.548
micro-mass
0.962
0.840
0.394
0.346
0.311
balance-scale
1.00
0.950
0.469
0.550
0.422
eating
0.930
0.734
0.318
0.337
0.265
vehicle
0.981
0.867
0.409
0.482
0.358
yeast
0.861
0.660
0.307
0.311
0.254
page-blocks
0.760
0.547
0.371
0.447
0.312
optdigits
0.468
0.323
0.148
0.139
0.113
satimage
0.506
0.336
0.171
0.184
0.140
kropt
0.224
0.161
0.087
0.066
0.060"
"L
X",0.8951678951678952,"radius of our bounds for each dataset, compiled in Table 4, and the average relative radius of our
bounds for each policy evaluated, compiled in Table 5. One can observe that LS always gives the
best results no matter the projection. However, the cIPS-L=1 bound is sometimes better than IX,
especially when it comes to evaluating diffused policies, see Table 5."
"L
X",0.895986895986896,Table 5: OPE: Average relative radiuses for each target policies (ideal policies with different τ)
"L
X",0.8968058968058968,"τ
SN-ES
cIPS-EB
IX
cIPS-L=1
LS
τ = 0.1
0.783
0.630
0.332
0.400
0.308
τ = 0.2
0.781
0.630
0.326
0.390
0.295
τ = 0.3
0.782
0.668
0.353
0.389
0.297
τ = 0.4
0.793
0.706
0.385
0.385
0.301
τ = 0.5
0.810
0.735
0.432
0.397
0.323"
"L
X",0.8976248976248976,"H.1.3
(OPS) Find the best, avoid the worst policy"
"L
X",0.8984438984438985,"Policy selection aims at identifying the best policy among a set of finite candidates. In practice,
we are interested in finding policies that improve on π0 and avoid policies that perform worse
than π0. To replicate real world scenarios, we design an experiment where π0 is a faulty policy
(τ = 0.2), that collects noisy (ϵ = 0.2) interaction data, some of which is used to learn πθIPS, πθSN,
and that we add to our discrete set of policies Πk=4 = {π0, πideal, πθIPS, πθSN}. The splits for these
experiments are the following: 70% of the data is used to create bandit feedback (20% is used to train
πθIPS, πθSN and 50% is used to evaluate policies based on estimators/upper bounds.) the rest is used to
evaluate the true value of the policies. The goal is to measure the ability of our selection strategies
to choose from Πk=4, better performing policies than π0. We thus define three possible outcomes:
a strategy can select worse performing policies, better performing or the best policy. We compare
selection strategies based on upper bounds to the commonly used estimators IPS and SNIPS. The
hyperparameters of all bounds (the clipping parameter M and λ) are set to 1/√n. The comparison is
conducted on the 11 datasets with 10 different seeds resulting in 110 scenarios. In addition to the
plot in Figure 2, we collect the number of times each method selected the best policy (πS
∗), a better
(B) or a worse (W) policy than π0 for all datasets in Table 6. We can see that risk estimators can be
unreliable, especially in small sample datasets, as they can choose worse performing policies than π0,
a catastrophic outcome in highly sensitive applications. Selecting policies based on upper bounds
is more conservative, as it avoids completely poor performing policies. In addition, the tighter the
bound, the better its percentage of time it selects the best policy: LS upper bound is less conservative
and can find best policies more than any other bound, while never selecting poor performing policies."
"L
X",0.8992628992628993,"Table 6: OPS: Number of times the worst, better or best policy was selected for each dataset."
"L
X",0.9000819000819,"Dataset
IPS
SNIPS
SN-ES
cIPS-EB
IX
cIPS-L=1
LS
W
B
πS
∗
W
B
πS
∗
W
B
πS
∗
W
B
πS
∗
W
B
πS
∗
W
B
πS
∗
W
B
πS
∗
ecoli
2
6
2
4
1
5
0
10
0
0
10
0
0
7
3
0
10
0
0
6
4
arrhythmia
0
10
0
0
10
0
0
10
0
0
10
0
0
7
3
0
10
0
0
5
5
micro-mass
3
0
7
1
0
9
0
10
0
0
10
0
0
0
10
0
0
10
0
0
10
balance-scale
0
3
7
0
2
8
0
10
0
0
10
0
0
4
6
0
10
0
0
3
7
eating
3
2
5
2
1
7
0
10
0
0
10
0
0
4
6
0
8
2
0
4
6
vehicle
3
0
7
1
1
8
0
10
0
0
10
0
0
5
5
0
10
0
0
3
7
yeast
0
2
8
2
0
8
0
10
0
0
10
0
0
2
8
0
7
3
0
2
8
page-blocks
0
0
10
0
0
10
0
10
0
0
10
0
0
0
10
0
10
0
0
0
10
optdigits
0
1
9
0
0
10
0
10
0
0
10
0
0
1
9
0
3
7
0
1
9
satimage
0
0
10
0
0
10
0
10
0
0
10
0
0
0
10
0
7
3
0
0
10
kropt
0
0
10
0
0
10
0
10
0
0
0
10
0
0
10
0
0
10
0
0
10"
"L
X",0.9009009009009009,"H.2
Off-policy learning"
"L
X",0.9017199017199017,"H.2.1
Datasets"
"L
X",0.9025389025389026,"As described in the experiments section, we follow exactly the experimental design of Sakhi et al.
[49], Aouali et al. [5] to conduct our PAC-Bayesian Off-Policy learning experiments. We however
take the time to explain it in details. In this procedure, we need three splits: Dl (of size nl) to train the
logging policy π0, another split Dc (of size nc) to generate the logging feedback with π0, and finally
a test split Dtest (of size ntest) to compute the true risk R(π) of any policy π. In our experiments,
we split the training split Dtrain (of size N) of the four datasets considered into Dl (nl = 0.05N)
and Dc (nc = 0.95N) and use their test split Dtest. The detailed statistics of the different splits can
be found in Table 7. Recall that K is the number of actions and p the number of features."
"L
X",0.9033579033579033,"Table 7: OPL: Detailed statistics of the splits used.
Datasets
N
nl
nc
ntest
K
p
MNIST
60 000
3000
57 000
10 000
10
784
FashionMNIST
60 000
3000
57 000
10 000
10
784
EMNIST-b
112 800
5640
107 160
18 800
47
784
NUS-WIDE-128
161 789
8089
153 700
107 859
81
128"
"L
X",0.9041769041769042,"H.2.2
Policy class"
"L
X",0.904995904995905,"In the PAC-Bayesian Learning paradigm, we are interested in the definition of policies as mixtures of
decision rules:
πQ(a|x) = Efθ∼Q [1 [fθ(x) = a]] ,
∀(x, a) ∈X × A .
(56)
We use the Linear Gaussian Policy of Sakhi et al. [49]. To obtain these policies, we restrict fθ to:
∀x ∈X,
fθ(x) = argmax
a′∈A"
"L
X",0.9058149058149059,"
xtθa′	
(57)"
"L
X",0.9066339066339066,"This results in a parameter θ of dimension d = p × K with p the dimension of the features ϕ(x)
and K the number of actions. We also restrict the family of distributions Qd+1 = {Qµ,σ =
N(µ, σ2Id), µ ∈Rd, σ > 0} to independent Gaussians with shared scale. Estimating the propensity
of a given x reduces the computation to a one dimensional integral:"
"L
X",0.9074529074529074,"πµ,σ(a|x) = Eϵ∼N(0,1)  Y"
"L
X",0.9082719082719083,"a′̸=a
Φ

ϵ + ϕ(x)T (µa −µa′)"
"L
X",0.9090909090909091,σ||ϕ(x)||  
"L
X",0.9099099099099099,with Φ the cumulative distribution function of the standard normal.
"L
X",0.9107289107289107,"H.2.3
Detailed hyperparameters"
"L
X",0.9115479115479116,"Contrary to previous work, our method does not require tuning any loss function hyperparameter
over a hold out set. We do however need to choose parameters to optimize the policies."
"L
X",0.9123669123669124,"The logging policy π0.
π0 is trained on Dl (supervised manner) with the following parameters:
We use L2 regularization of 10−4. This is used to prevent the logging policy π0 from being close
to deterministic, allowing efficient learning with importance sampling. We use Adam [30] with a
learning rate of 10−1 for 10 epochs."
"L
X",0.9131859131859131,"Parameters of the bounds.
cIPS and cvcIPS: The clipping parameter τ is fixed to 1/K with K
the action size of the dataset and cvcIPS is used with ξ = −0.5 (the values used in Sakhi et al. [49]).
ES: The exponential smoothing parameter α is fixed to 1 −1/K."
"L
X",0.914004914004914,"Optimizing the bounds.
We use Adam [30] with a learning rate of 10−3 for 100 epochs. The
gradient of LIG policies is a one dimensional integral, and is approximated using S = 32 samples."
"L
X",0.9148239148239148,"πµ,σ(a|x) = Eϵ∼N(0,1)  Y"
"L
X",0.9156429156429157,"a′̸=a
Φ

ϵ + ϕ(x)T (µa −µa′)"
"L
X",0.9164619164619164,"σ||ϕ(x)||   ≈1 S S
X s=1 Y"
"L
X",0.9172809172809173,"a′̸=a
Φ

ϵs + ϕ(x)T (µa −µa′)"
"L
X",0.9180999180999181,σ||ϕ(x)||
"L
X",0.918918918918919,"
ϵ1, ..., ϵS ∼N(0, 1)."
"L
X",0.9197379197379197,"For all bounds, instead of fixing λ, we take a union bound over a discretized space of possible
parameters Λ of size nΛ = 100 and for each iteration j of the optimization procedure, we take λj ∈Λ
that minimizes the estimated bound and proceed to compute the gradient w.r.t µ and σ with λj."
"L
X",0.9205569205569205,"H.2.4
Detailed results"
"L
X",0.9213759213759214,"In addition to the results of Table 2, we also provide a more detailed view of the results here. For
each α and dataset, we average both {GR, R} over the 10 seeds and plot them in Figure 6 and
Figure 5. Note that the error bars are too small σ/
√"
"L
X",0.9221949221949222,"10 ≈0.001 and all our results in these graphs are
significant. We observe that the LS PAC-Bayesian bound improves substantially on its competitors
in terms of the guaranteed risk, especially on MNIST and FashionMNIST and also obtains the best
performing policies, on par with the IX bound in the majority of scenarios."
"L
X",0.923013923013923,"0.2
0.4
0.6
0.8
1.0
Inverse temperature parameter 0.8 0.7 0.6 0.5 0.4 0.3"
"L
X",0.9238329238329238,Guaranteed Risk of Different Bounds MNIST
"L
X",0.9246519246519247,"0.2
0.4
0.6
0.8
1.0
Inverse temperature parameter 0.7 0.6 0.5 0.4 0.3"
"L
X",0.9254709254709255,FashionMNIST
"L
X",0.9262899262899262,"0.2
0.4
0.6
0.8
1.0
Inverse temperature parameter 0.4 0.3 0.2 0.1 0.0"
"L
X",0.9271089271089271,EMNIST
"L
X",0.9279279279279279,"0.2
0.4
0.6
0.8
1.0
Inverse temperature parameter 0.30 0.25 0.20 0.15 0.10 0.05 0.00"
"L
X",0.9287469287469288,nuswide
"L
X",0.9295659295659295,"cIPS
ES
cvcIPS
IX
LS-LIN"
"L
X",0.9303849303849304,OPL: Guaranteed Risk Given by different Bounds
"L
X",0.9312039312039312,"Figure 5: OPL: Guaranteed Risk given by the different bounds. We observe that our LS-LIN
dominates all other bounds. IX comes close, especially on EMNIST and nuswide"
"L
X",0.9320229320229321,"0.2
0.4
0.6
0.8
1.0
Inverse temperature parameter 0.85 0.80 0.75 0.70 0.65"
"L
X",0.9328419328419328,True Risk of Obtained Policies MNIST
"L
X",0.9336609336609336,"0.2
0.4
0.6
0.8
1.0
Inverse temperature parameter 0.75 0.70 0.65 0.60 0.55"
"L
X",0.9344799344799345,FashionMNIST
"L
X",0.9352989352989353,"0.2
0.4
0.6
0.8
1.0
Inverse temperature parameter 0.5 0.4 0.3 0.2 0.1"
"L
X",0.9361179361179361,EMNIST
"L
X",0.9369369369369369,"0.2
0.4
0.6
0.8
1.0
Inverse temperature parameter 0.35 0.30 0.25 0.20 0.15 0.10 0.05"
"L
X",0.9377559377559378,nuswide
"L
X",0.9385749385749386,"cIPS
ES
cvcIPS
IX
LS-LIN"
"L
X",0.9393939393939394,OPL: True Risk of obtained policies
"L
X",0.9402129402129402,"Figure 6: OPL: True risk of obtained policies after minimizing the PAC-Bayesian bounds. We observe
that LS-LIN and IX are hardly distinguishable, they both give the best policies in the majority of
scenarios."
"L
X",0.941031941031941,NeurIPS Paper Checklist
CLAIMS,0.9418509418509419,1. Claims
CLAIMS,0.9426699426699426,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction scopes our work in the offline contextual bandit
setting, describes our method and claims its superiority compared to existing work. We
provide both strong theoretical and empirical evidence in the paper to defend the claim.
Guidelines:"
CLAIMS,0.9434889434889435,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.9443079443079443,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We provide the limitations of our method in Appendix A and discuss ways to
mitigate them in future work.
Guidelines:"
CLAIMS,0.9451269451269452,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9459459459459459,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
CLAIMS,0.9467649467649467,Answer: [Yes]
CLAIMS,0.9475839475839476,Justification: All our results are proven in the paper and all assumptions are discussed.
CLAIMS,0.9484029484029484,Guidelines:
CLAIMS,0.9492219492219492,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.95004095004095,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9508599508599509,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9516789516789517,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9524979524979525,"Justification: Our paper follows the classical off-policy experimental design, and all details
to reproduce them are given in Appendix H."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9533169533169533,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9541359541359541,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.954954954954955,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9557739557739557,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9565929565929566,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9574119574119574,"Justification: All data used is accessible UCI Repository, the code is also given in the
supplementary material to reproduce all experimental results."
OPEN ACCESS TO DATA AND CODE,0.9582309582309583,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.959049959049959,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9598689598689598,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9606879606879607,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9615069615069615,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9623259623259623,Justification: The experimental settings are detailed in Section 5 and Appendix H.
OPEN ACCESS TO DATA AND CODE,0.9631449631449631,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.963963963963964,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9647829647829648,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9656019656019657,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9664209664209664,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9672399672399672,"Justification: All our experiments are run with multiple seeds and for different scenarios
and datasets. Some graphs do not need error bars (cumulative distributions or selection
strategies), for the other results, we have very small error bars (Appendix H.2), making our
results significant."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9680589680589681,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9688779688779688,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9696969696969697,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9705159705159705,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9713349713349714,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9721539721539721,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.972972972972973,"Justification: Our experiments can be conducted in small machines and do not require heavy
compute, this is detailed in Appendix H."
EXPERIMENTS COMPUTE RESOURCES,0.9737919737919738,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9746109746109746,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9754299754299754,9. Code Of Ethics
CODE OF ETHICS,0.9762489762489762,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9770679770679771,Answer: [Yes]
CODE OF ETHICS,0.9778869778869779,"Justification: Our paper is of theoretical nature, presents ideas to increase safety in decision
making, uses publicly available data for the experiments and conforms to the NeurIPS Code
of Ethics."
CODE OF ETHICS,0.9787059787059788,Guidelines:
CODE OF ETHICS,0.9795249795249795,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9803439803439803,10. Broader Impacts
BROADER IMPACTS,0.9811629811629812,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9819819819819819,Answer: [Yes]
BROADER IMPACTS,0.9828009828009828,Justification: The paper discusses both the positive and negative impacts in Appendix B.
BROADER IMPACTS,0.9836199836199836,Guidelines:
BROADER IMPACTS,0.9844389844389845,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9852579852579852,11. Safeguards
SAFEGUARDS,0.9860769860769861,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9868959868959869,Answer: [NA]
SAFEGUARDS,0.9877149877149877,"Justification: Our paper tackles theoretical questions for decision-making, the data used for
the experiments is openly accessible in UCI repository. We do not believe that our work
poses such risks."
SAFEGUARDS,0.9885339885339886,Guidelines:
SAFEGUARDS,0.9893529893529893,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9901719901719902,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.990990990990991,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9918099918099919,"Answer: [Yes]
Justification: Our experimental design is inspired from the code base of some papers that
we cite, and all data used is openly accessible in UCI repository."
LICENSES FOR EXISTING ASSETS,0.9926289926289926,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9934479934479934,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL."
LICENSES FOR EXISTING ASSETS,0.9942669942669943,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.995085995085995,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We do not release new assests, but we give the code to reproduce the experi-
ments in the supplementary material.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9959049959049959,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9967239967239967,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9975429975429976,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9983619983619983,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
LICENSES FOR EXISTING ASSETS,0.9991809991809992,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
