Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002044989775051125,"We study stochastic Cubic Newton methods for solving general possibly non-
1"
ABSTRACT,0.00408997955010225,"convex minimization problems. We propose a new framework, which we call
2"
ABSTRACT,0.006134969325153374,"the helper framework, that provides a unified view of the stochastic and variance-
3"
ABSTRACT,0.0081799591002045,"reduced second-order algorithms equipped with global complexity guarantees. It
4"
ABSTRACT,0.010224948875255624,"can also be applied to learning with auxiliary information. Our helper framework
5"
ABSTRACT,0.012269938650306749,"offers the algorithm designer high flexibility for constructing and analysis of the
6"
ABSTRACT,0.014314928425357873,"stochastic Cubic Newton methods, allowing arbitrary size batches, and the use
7"
ABSTRACT,0.016359918200409,"of noisy and possibly biased estimates of the gradients and Hessians, incorporat-
8"
ABSTRACT,0.018404907975460124,"ing both the variance reduction and the lazy Hessian updates. We recover the
9"
ABSTRACT,0.02044989775051125,"best-known complexities for the stochastic and variance-reduced Cubic Newton,
10"
ABSTRACT,0.022494887525562373,"under weak assumptions on the noise and avoiding artificial logarithms. A direct
11"
ABSTRACT,0.024539877300613498,"consequence of our theory is the new lazy stochastic second-order method, which
12"
ABSTRACT,0.026584867075664622,"significantly improves the arithmetic complexity for large dimension problems. We
13"
ABSTRACT,0.028629856850715747,"also establish complexity bounds for the classes of gradient-dominated objectives,
14"
ABSTRACT,0.03067484662576687,"that include convex and strongly convex problems. For Auxiliary Learning, we
15"
ABSTRACT,0.032719836400818,"show that using a helper (auxiliary function) can outperform training alone if a
16"
ABSTRACT,0.034764826175869123,"given similarity measure is small.
17"
INTRODUCTION,0.03680981595092025,"1
Introduction
18"
INTRODUCTION,0.03885480572597137,"In many fields of machine learning, it is common to optimize a function f(x) that can be expressed
19"
INTRODUCTION,0.0408997955010225,"as a finite sum:
20"
INTRODUCTION,0.04294478527607362,"min
x∈Rd"
INTRODUCTION,0.044989775051124746,"n
f(x)
=
1
n nP"
INTRODUCTION,0.04703476482617587,"i=1
fi(x)
o
,
(1)"
INTRODUCTION,0.049079754601226995,"or, more generally, as an expectation over some given probability distribution: f(x) = Eζ

f(x, ζ)

.
21"
INTRODUCTION,0.05112474437627812,"When f is non-convex, this problem is especially difficult, since finding a global minimum is NP-hard
22"
INTRODUCTION,0.053169734151329244,"in general [14]. Hence, the reasonable goal is to look for approximate solutions. The most prominent
23"
INTRODUCTION,0.05521472392638037,"family of algorithms for solving large-scale problems of the form (1) are the first-order methods, such
24"
INTRODUCTION,0.05725971370143149,"as the Stochastic Gradient Descent (SGD) [25, 16]. They employ only stochastic gradient information
25"
INTRODUCTION,0.05930470347648262,"about the objective f(x) and guarantee the convergence to a stationary point, which is a point with a
26"
INTRODUCTION,0.06134969325153374,"small gradient norm.
27"
INTRODUCTION,0.06339468302658487,"Nevertheless, when the objective function is non-convex, a stationary point may be a saddle point or
28"
INTRODUCTION,0.065439672801636,"even a local maximum, which is not desirable. Another common issue is that first-order methods
29"
INTRODUCTION,0.06748466257668712,"typically have a slow convergence rate, particularly when the problem is ill-conditioned. Therefore,
30"
INTRODUCTION,0.06952965235173825,"they may not be suitable when high precision for the solution is required.
31"
INTRODUCTION,0.07157464212678936,"To address these challenges, we can take into account second-order information (the Hessian matrix)
32"
INTRODUCTION,0.0736196319018405,"and apply Newton’s method (see, e.g. [19]). Among the many versions of this algorithm, the Cubic
33"
INTRODUCTION,0.07566462167689161,"Newton method [20] is one of the most theoretically established. With the Cubic Newton method, we
34"
INTRODUCTION,0.07770961145194274,"can guarantee global convergence to an approximate second-order stationary point (in contrast, the
35"
INTRODUCTION,0.07975460122699386,"pure Newton method without regularization can even diverge when it starts far from a neighborhood
36"
INTRODUCTION,0.081799591002045,"of the solution). For a comprehensive historical overview of the different variants of Newton’s
37"
INTRODUCTION,0.08384458077709611,"method, see [24]. Additionally, the rate of convergence of the Cubic Newton is provably better than
38"
INTRODUCTION,0.08588957055214724,"those for the first-order methods.
39"
INTRODUCTION,0.08793456032719836,"Therefore, theoretical guarantees of the Cubic Newton method seem to be very appealing for practical
40"
INTRODUCTION,0.08997955010224949,"applications. However, the basic version of the Cubic Newton requires the exact gradient and Hessian
41"
INTRODUCTION,0.09202453987730061,"information in each step, which can be very expensive to compute in the large scale setting. To
42"
INTRODUCTION,0.09406952965235174,"overcome this issue, several techniques have been proposed:
43"
INTRODUCTION,0.09611451942740286,"• One popular approach is to use inexact stochastic gradient and Hessian estimates with sub-
44"
INTRODUCTION,0.09815950920245399,"sampling [33, 17, 32, 21, 12, 6, 1]. This technique avoids using the full oracle information,
45"
INTRODUCTION,0.10020449897750511,"but typically it has a slower convergence rate compared to the exact Cubic Newton.
46"
INTRODUCTION,0.10224948875255624,"• Variance reduction techniques [35, 29] combine the advantages of stochastic and exact
47"
INTRODUCTION,0.10429447852760736,"methods, achievieng an improved rates by recomputing the full gradient and Hessian
48"
INTRODUCTION,0.10633946830265849,"information at some iterations.
49"
INTRODUCTION,0.1083844580777096,"• Lazy Hessian updates [26, 9] utilize a simple idea of reusing an old Hessian for several
50"
INTRODUCTION,0.11042944785276074,"iterations of a second-order scheme. Indeed, since the cost of computing one Hessian is
51"
INTRODUCTION,0.11247443762781185,"usually much more expensive than one gradient, it can improve the arithmetic complexity of
52"
INTRODUCTION,0.11451942740286299,"our methods.
53"
INTRODUCTION,0.1165644171779141,"• In addition, exploiting the special structure of the function f (if known) can also be helpful.
54"
INTRODUCTION,0.11860940695296524,"For instance, some studies [20, 18] consider gradient-dominated objectives, a subclass
55"
INTRODUCTION,0.12065439672801637,"of non-convex functions that have improved convergence rates and can even be shown to
56"
INTRODUCTION,0.12269938650306748,"converge to the global minimum. Examples of such objectives include convex and star-
57"
INTRODUCTION,0.12474437627811862,"convex functions, uniformly convex functions, and functions satisfying the PL condition
58"
INTRODUCTION,0.12678936605316973,"[23] as a special case.
59"
INTRODUCTION,0.12883435582822086,"In this work, we revise the current state-of-the-art convergence theory for the stochastic Cubic
60"
INTRODUCTION,0.130879345603272,"Newton method and propose a unified and improved complexity guarantees for different versions of
61"
INTRODUCTION,0.1329243353783231,"the method, which combine all the advanced techniques listed above.
62"
INTRODUCTION,0.13496932515337423,"Our developments are based on the new helper framework for the second-order optimization, that we
63"
INTRODUCTION,0.13701431492842536,"present in Section 3. For the first-order optimization, a similar in-spirit techniques called learning
64"
INTRODUCTION,0.1390593047034765,"with auxiliary information was developed recently in [7, 30]. Thus, our results can also be seen as a
65"
INTRODUCTION,0.1411042944785276,"generalization of the Auxiliary Learning paradigm to the second-order optimization. However, note
66"
INTRODUCTION,0.14314928425357873,"that in our second-order case, we have more freedom for choosing the ""helper functions"" (namely, we
67"
INTRODUCTION,0.14519427402862986,"use one for the gradients and one for the Hessians). That brings more flexibility into our methods and
68"
INTRODUCTION,0.147239263803681,"it allows, for example, to use the lazy Hessian updates.
69"
INTRODUCTION,0.1492842535787321,"Our new helper framework provides us with a unified view of the stochastic and variance-reduced
70"
INTRODUCTION,0.15132924335378323,"methods and can be used by an algorithm designed to construct new methods. Thus, we show how to
71"
INTRODUCTION,0.15337423312883436,"recover already known versions of the stochastic Cubic Newton with the best convergence rates, as
72"
INTRODUCTION,0.1554192229038855,"well as present the new Lazy Stochastic Second-Order Method, which significantly improves the total
73"
INTRODUCTION,0.1574642126789366,"arithmetic complexity for large-dimension problems.
74"
INTRODUCTION,0.15950920245398773,"Contributions.
75"
INTRODUCTION,0.16155419222903886,"• We introduce the helper framework which we argue encompasses multiple methods in a
76"
INTRODUCTION,0.16359918200409,"unified way. Such methods include stochastic methods, variance reduction, Lazy methods,
77"
INTRODUCTION,0.1656441717791411,"core sets, and semi-supervised learning.
78"
INTRODUCTION,0.16768916155419222,"• This framework covers previous versions of the variance-reduced stochastic Cubic Newton
79"
INTRODUCTION,0.16973415132924335,"methods with known rates. Moreover, it provides us with new algorithms that employ Lazy
80"
INTRODUCTION,0.17177914110429449,"Hessian updates and significantly improves the arithmetic complexity (for high dimensions),
81"
INTRODUCTION,0.1738241308793456,"by using the same Hessian snapshot for several steps of the method.
82"
INTRODUCTION,0.17586912065439672,"• In the case of Auxiliary learning we provably show a benefit from using auxiliary tasks
83"
INTRODUCTION,0.17791411042944785,"as helpers in our framework. In particular, we can replace the smoothness constant by a
84"
INTRODUCTION,0.17995910020449898,"similarity constant which might be smaller.
85"
INTRODUCTION,0.18200408997955012,"• Moreover, our analysis works both for the general class of non-convex functions, as well as
86"
INTRODUCTION,0.18404907975460122,"for the class of gradient-dominated problems, that includes convex and uniformly convex
87"
INTRODUCTION,0.18609406952965235,"functions. Hence, in particular, we are the first to establish the convergence rates of the
88"
INTRODUCTION,0.18813905930470348,"stochastic Cubic Newton algorithms with variance reduction for the gradient-dominated
89"
INTRODUCTION,0.1901840490797546,"case.
90"
NOTATION AND ASSUMPTIONS,0.19222903885480572,"2
Notation and Assumptions
91"
NOTATION AND ASSUMPTIONS,0.19427402862985685,"For simplicity, we consider the finite-sum optimization problem (1), while it can be also possible
92"
NOTATION AND ASSUMPTIONS,0.19631901840490798,"to generalize our results to arbitrary expectations. We assume that our objective f is bounded
93"
NOTATION AND ASSUMPTIONS,0.1983640081799591,"from below and denote f ⋆:= inf
x f(x), and use the following notation: F0 := f(x0) −f ⋆, for
94"
NOTATION AND ASSUMPTIONS,0.20040899795501022,"some initial x0 ∈Rd. We denote by ∥x∥:= ⟨x, x⟩1/2, x ∈Rd, the standard Euclidean norm for
95"
NOTATION AND ASSUMPTIONS,0.20245398773006135,"vectors, and the spectral norm for symmetric matrices, ∥H∥:= max{λmax(H), −λmin(H)}, where
96"
NOTATION AND ASSUMPTIONS,0.20449897750511248,"H = H⊤∈Rd×d. We will also use x ∧y to denote min(x, y).
97"
NOTATION AND ASSUMPTIONS,0.2065439672801636,"Throughout this work, we make the following smothness assumption on the objective f :
98"
NOTATION AND ASSUMPTIONS,0.2085889570552147,"Assumption 1 (Lipschitz Hessian) The Hessian of f is Lipschitz continuous, for some L > 0:"
NOTATION AND ASSUMPTIONS,0.21063394683026584,"∥∇2f(x) −∇2f(y)∥
≤
L∥x −y∥,
∀x, y ∈Rd"
NOTATION AND ASSUMPTIONS,0.21267893660531698,"Our goal is to explore the potential of using the Cubically regularized Newton methods to solve
99"
NOTATION AND ASSUMPTIONS,0.2147239263803681,"problem (1). At each iteration, being at a point x ∈Rd, we compute the next point x+ by solving
100"
NOTATION AND ASSUMPTIONS,0.2167689161554192,"the subproblem of the form
101"
NOTATION AND ASSUMPTIONS,0.21881390593047034,"x+ ∈arg min
y∈Rd"
NOTATION AND ASSUMPTIONS,0.22085889570552147,"n
ΩM,g,H(y, x) := ⟨g, y −x⟩+ 1"
NOTATION AND ASSUMPTIONS,0.2229038854805726,"2⟨H(y −x), y −x⟩+ M"
NOTATION AND ASSUMPTIONS,0.2249488752556237,"6 ∥y −x∥3 o
.
(2)"
NOTATION AND ASSUMPTIONS,0.22699386503067484,"Here, g and H are estimates of the gradient ∇f(x) and the Hessian ∇2f(x), respectively. Note that
102"
NOTATION AND ASSUMPTIONS,0.22903885480572597,"solving (2) can be done efficiently even for non-convex problems (see [8, 20, 5]). Generally, the cost
103"
NOTATION AND ASSUMPTIONS,0.2310838445807771,"of computing x+ is O(d3) arithmetic operations, which are needed for evaluating an appropriate
104"
NOTATION AND ASSUMPTIONS,0.2331288343558282,"factorization of H. Hence, it is of a similar order as the cost of the classical Newton’s step.
105"
NOTATION AND ASSUMPTIONS,0.23517382413087934,"We will be interested to find a second-order stationary point to (1). We call (ε, c)-approximate
second-order local minimum a point x that satisfies:"
NOTATION AND ASSUMPTIONS,0.23721881390593047,"∥∇f(x)∥
≤
ε
and
λmin(∇2f(x))
≥
−c√ε,"
NOTATION AND ASSUMPTIONS,0.2392638036809816,"where ε, c > 0 are given tolerance parameters. Let us define the following accuracy measure (see
[20]):
µc(x)
:=
max

∥∇f(x)∥3/2, −λmin(∇2f(x))3"
NOTATION AND ASSUMPTIONS,0.24130879345603273,"c3/2

,
x ∈Rd, c > 0."
NOTATION AND ASSUMPTIONS,0.24335378323108384,"Note that this definition implies that if µc(x) ≤ε3/2 then x is an (ε, c)-approximate local minimum.
106"
NOTATION AND ASSUMPTIONS,0.24539877300613497,"Computing gradients and Hessians.
It is clear that computing the Hessian matrix can be
107"
NOTATION AND ASSUMPTIONS,0.2474437627811861,"much more expensive than computing the gradient vector. We denote the corresponding arith-
108"
NOTATION AND ASSUMPTIONS,0.24948875255623723,"metic complexities by HessCost and GradCost. We will make and follow the convention that
109"
NOTATION AND ASSUMPTIONS,0.25153374233128833,"HessCost = d × GradCost, where d is the dimension of the problem. For example, this is known to
110"
NOTATION AND ASSUMPTIONS,0.25357873210633947,"hold for neural networks using the backpropagation algorithm [15]. However, if the Hessian has a
111"
NOTATION AND ASSUMPTIONS,0.2556237218813906,"sparse structure, the cost of computing the Hessian can be cheaper [22]. Then, we can replace d with
112"
NOTATION AND ASSUMPTIONS,0.25766871165644173,the effective dimension deff := HessCost
NOTATION AND ASSUMPTIONS,0.25971370143149286,"GradCost ≤d.
113"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.261758691206544,"3
Second-Order Optimization with Helper Functions
114"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.26380368098159507,"In this section, we extend the helper framework previously introduced in [7] for first-order optimiza-
115"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.2658486707566462,"tion methods to second-order optimization.
116"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.26789366053169733,"General principle.
The general idea is the following: imagine that, besides the objective function f
117"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.26993865030674846,"we have access to a help function h that we think is similar in some sense (that will be defined later)
118"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.2719836400817996,"to f and thus it should help to minimize it.
119"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.2740286298568507,"Note that many optimization algorithms can be framed in the following sequential way. For a current
120"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.27607361963190186,"state x, we compute the next state x+ as:
121"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.278118609406953,"x+
∈
arg min
y∈Rd"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.28016359918200406,"n
ˆfx(y) + Mrx(y)
o
,"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.2822085889570552,"where ˆfx(·) is an approximation of f around current point x, and rx(y) is a regularizer that encodes
122"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.2842535787321063,"how accurate the approximation is, and M > 0 is a regularization parameter. In this work, we
123"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.28629856850715746,"are interested in cubically regularized second-order models of the form (2) and we use rx(y) :=
124"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.2883435582822086,"1
6∥y −x∥3.
125"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.2903885480572597,"Now let us look at how we can use a helper h to construct the approximation ˆf. We notice that we
126"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.29243353783231085,"can write
127"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.294478527607362,"f(y)
:=
h(y)
|{z}
cheap"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.2965235173824131,"+ f(y) −h(y)
|
{z
}
expensive
We discuss the actual practical choices of the helper function h below. We assume now that we can
128"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.2985685071574642,"afford the second-order approximation for the cheap part h around the current point x. However,
129"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3006134969325153,"approximating the part f −h can be expensive (as for example when the number of elements n in
130"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.30265848670756645,"finite sum (1) is huge), or even impossible (due to lack of data). Thus, we would prefer to approximate
131"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3047034764826176,"the expensive part less frequently. For this reason, let us introduce an extra snapshot point ˆx that
132"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3067484662576687,"is updated less often than x. Then, we use it to approximate f −h. Another question that we still
133"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.30879345603271985,"need to ask is what order should we use for the approximation of f −h? We will see that order 0
134"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.310838445807771,"(approximating by a constant) leads as to the basic stochastic methods, while for orders 1 and 2 we
135"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3128834355828221,"equip our methods with the variance reduction.
136"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3149284253578732,"Combining the two approximations for h and f −h we get the following model of our objective f:
137"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3169734151329243,"ˆfx,˜x(y)
=
C(x, ˜x) + ⟨G(h, x, ˜x), y −x⟩+ 1"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.31901840490797545,"2⟨H(h, x, ˜x)(y −x), y −x⟩,
(3)
where C(x, ˜x) is a constant, G(h, x, ˜x) is a linear term, and H(h, x, ˜x) is a matrix. Note that if
138"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3210633946830266,"˜x ≡x, then the best second-order model of the form (3) is the Taylor polynomial of degree two for
139"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3231083844580777,"f around x, and that would give us the exact Newton-type method. However, when the points x and
140"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.32515337423312884,"˜x are different, we obtain much more freedom in constructing our models.
141"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.32719836400818,"For using this model in our cubically regularized method (2), we only need to define the gradient
142"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3292433537832311,"g = G(h, x, ˜x) and the Hessian estimates H = H(h, x, ˜x), and we can also treat them differently
143"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3312883435582822,"(using two different helpers h1 and h2, correspondingly). Thus we come to the following general
144"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3333333333333333,"second-order (meta)algorithm. We perform S rounds, the length of each round is m ≥1, which is
145"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.33537832310838445,our key parameter:
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3374233128834356,Algorithm 1 Cubic Newton with helper functions
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3394683026584867,"Input: x0 ∈Rd, S, m ≥1, M > 0."
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.34151329243353784,"1: for t = 0, . . . , Sm −1 do
2:
if t mod m = 0 then
3:
Update ˜xt (using previous states xi≤t)
4:
else
5:
˜xt = ˜xt−1
6:
Form helper functions h1, h2
7:
Compute the gradient gt = G(h1, xt, ˜xt), and the Hessian Ht = H(h2, xt, ˜xt)
8:
Compute the cubic step xt+1 ∈arg miny∈Rd ΩM,gt,Ht(y, xt)
return xout using the history (xi)0≤i≤Sm 146"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.34355828220858897,"In Algorithm 1 we update the snapshot ˜x regularly every m iterations. The two possible options are
147"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3456032719836401,"˜xt
=
xt mod m
(use the last iterate)
(4)
or
148"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3476482617586912,"˜xt
=
arg min
i∈{t−m+1,...,t}
f(xi)
(use the best iterate)
(5)"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3496932515337423,"Clearly, option (5) is available only in case we can efficiently estimate the function values. However,
149"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.35173824130879344,"we will see that it serves us with better global convergence guarantees, for the gradient-dominated
150"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3537832310838446,"functions.
151"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.3558282208588957,"It remains only to specify how we choose the helpers h1 and h2. We need to assume that they are
152"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.35787321063394684,"somehow similar to f. Let us present several efficient choices that lead to implementable second-order
153"
SECOND-ORDER OPTIMIZATION WITH HELPER FUNCTIONS,0.35991820040899797,"schemes.
154"
BASIC STOCHASTIC METHODS,0.3619631901840491,"3.1
Basic Stochastic Methods
155"
BASIC STOCHASTIC METHODS,0.36400817995910023,"If the objective function f is very ""expensive"" (for example of the form (1) with n →∞), one option
is to ignore the part f −h i.e. to approximate it by a zeroth-order approximation: f(y) −h(y) ≈
f(˜x) −h(˜x). Since it is just a constant, we do not need to update ˜x. In this case, we have:"
BASIC STOCHASTIC METHODS,0.3660531697341513,"G(h1, x, ˜x)
:=
∇h1(x),
H(h2, x, ˜x)
:=
∇2h2(x) ."
BASIC STOCHASTIC METHODS,0.36809815950920244,"To treat this choice of the helpers and motivated by the form of the errors in Lemma 5, we assume the
156"
BASIC STOCHASTIC METHODS,0.37014314928425357,"following similarity assumptions:
157"
BASIC STOCHASTIC METHODS,0.3721881390593047,"Assumption 2 (Bounded similarity) Let for some δ1, δ2 ≥0, it holds"
BASIC STOCHASTIC METHODS,0.37423312883435583,"Eh1[∥G(h1, x, ˜x)−∇f(x)∥3/2] ≤δ3/2
1
, Eh2[∥H(h2, x, ˜x)−∇2f(x)∥3] ≤δ3
2,
∀x, ˜x ∈Rd."
BASIC STOCHASTIC METHODS,0.37627811860940696,"Under this assumption, we prove the following theorem:
158"
BASIC STOCHASTIC METHODS,0.3783231083844581,"Theorem 1 Under Assumptions 1 and 2, and M ≥L, for an output of Algorithm 1 xout chosen
uniformly at random from (xi)0≤i≤Sm, we have:"
BASIC STOCHASTIC METHODS,0.3803680981595092,"E[µM(xout)]
=
O
 √"
BASIC STOCHASTIC METHODS,0.3824130879345603,"MF0
Sm
+
δ3
2
M 3/2 + δ3/2
1

."
BASIC STOCHASTIC METHODS,0.38445807770961143,"We see that according to this result, we can get E[µM(xout)] ≤ε3/2 only for ε > δ1. In other words,
159"
BASIC STOCHASTIC METHODS,0.38650306748466257,"we can converge only to a certain neighbourhood around a stationary point, that is determined by the
160"
BASIC STOCHASTIC METHODS,0.3885480572597137,"error δ1 of the stochastic gradients.
161"
BASIC STOCHASTIC METHODS,0.39059304703476483,"However, as we will show next, this seemingly pessimistic dependence leads to the same rate of
162"
BASIC STOCHASTIC METHODS,0.39263803680981596,"classical subsampled Cubic Newton methods discovered in [17, 32, 33].
163"
BASIC STOCHASTIC METHODS,0.3946830265848671,"Let us discuss now the specific case of stochastic optimization, where f has the specific form (1),
164"
BASIC STOCHASTIC METHODS,0.3967280163599182,"with n potentially being very large. In this case, it is customary to sample batches at random and
165"
BASIC STOCHASTIC METHODS,0.3987730061349693,"assume the noise to be bounded in expectation. Precisely speaking, if we assume the standard
166"
BASIC STOCHASTIC METHODS,0.40081799591002043,"assumption that for one index sampled uniformly at random, we have Ei∥∇f(x) −∇fi(x)∥2 ≤σ2
g
167"
BASIC STOCHASTIC METHODS,0.40286298568507156,"and Ei∥∇2f(x) −∇2fi(x)∥3 ≤σ3
h , then it is possible to show that for
168"
BASIC STOCHASTIC METHODS,0.4049079754601227,"h1
=
1
bg
P"
BASIC STOCHASTIC METHODS,0.4069529652351738,"i∈Bg fi
and
h2 =
1
bh
P"
BASIC STOCHASTIC METHODS,0.40899795501022496,"i∈Bh fi,
(6)"
BASIC STOCHASTIC METHODS,0.4110429447852761,"for batches Bg, Bh ⊆[n] sampled uniformly at random and of sizes bg and bh respectively, Assump-
169"
BASIC STOCHASTIC METHODS,0.4130879345603272,"tion 2 is satisfied with [27]: δ1 =
σg
√"
BASIC STOCHASTIC METHODS,0.41513292433537835,"bg and δ2 = ˜O( σh
√bh ). Note that we can use the same random
170"
BASIC STOCHASTIC METHODS,0.4171779141104294,"subsets of indices Bg, Bh for all iterations.
171"
BASIC STOCHASTIC METHODS,0.41922290388548056,"Corollary 1 In Algorithm 1, let us choose M = L and m = 1, with basic helpers (6). Then,
according to Theorem 1, for any ε > 0, to reach an (ε, L)-approximate second-order local minimum,
we need at most S =
√"
BASIC STOCHASTIC METHODS,0.4212678936605317,"LF0
ε3/2 iterations with bg =
  σg"
BASIC STOCHASTIC METHODS,0.4233128834355828,"ε
2 and bh = σ2
h
ε . Therefore, the total arithmetic
complexity of the method becomes"
BASIC STOCHASTIC METHODS,0.42535787321063395,"O
 σ2
g
ε7/2 +
σ2
h
ε5/2 deff

× GradCost."
BASIC STOCHASTIC METHODS,0.4274028629856851,It improves upon the complexity O( 1
BASIC STOCHASTIC METHODS,0.4294478527607362,"ε4 ) × GradCost of the first-order SGD for non-convex optimiza-
172"
BASIC STOCHASTIC METHODS,0.43149284253578735,"tion [11], unless deff >
1
ε3/2 (high cost of computing the Hessians).
173"
LET THE OBJECTIVE GUIDE US,0.4335378323108384,"3.2
Let the Objective Guide Us
174"
LET THE OBJECTIVE GUIDE US,0.43558282208588955,"If the objective f is such that we can afford to access its gradients and Hessians from time to time
175"
LET THE OBJECTIVE GUIDE US,0.4376278118609407,"(functions of the form (1) with n < ∞and “reasonable""), then we can do better than the previous
176"
LET THE OBJECTIVE GUIDE US,0.4396728016359918,"chapter. In this case, we can afford to use a better approximation of the term f(y) −h(y). From a
177"
LET THE OBJECTIVE GUIDE US,0.44171779141104295,"theoretical point of view, we can treat the case when f is only differentiable once, and thus we can
178"
LET THE OBJECTIVE GUIDE US,0.4437627811860941,"only use a first-order approximation of f −h, in this case, we will only be using the hessian of the
179"
LET THE OBJECTIVE GUIDE US,0.4458077709611452,"helper h but only gradients of f. However, in our case, if we assume we have access to gradients then
180"
LET THE OBJECTIVE GUIDE US,0.44785276073619634,"we can also have access to the Hessians of f as well (from time to time). For this reason, we consider
181"
LET THE OBJECTIVE GUIDE US,0.4498977505112474,"a second-order approximation of the term f −h, if we follow the procedure that we described above
182"
LET THE OBJECTIVE GUIDE US,0.45194274028629855,"we find:
183"
LET THE OBJECTIVE GUIDE US,0.4539877300613497,"G(h1, x, ˜x)
:= ∇h1(x) −∇h1(˜x) + ∇f(˜x) + (∇2f(˜x) −∇2h1(˜x))(x −˜x)
(7)"
LET THE OBJECTIVE GUIDE US,0.4560327198364008,"H(h2, x, ˜x)
:= ∇2h2(x) −∇2h2(˜x) + ∇2f(˜x)
(8)"
LET THE OBJECTIVE GUIDE US,0.45807770961145194,"We see that there is an explicit dependence on the snapshot ˜x and thus we need to address the question
184"
LET THE OBJECTIVE GUIDE US,0.4601226993865031,"of how this snapshot point should be updated in Algorithm 1. In general, we can update it with a
185"
LET THE OBJECTIVE GUIDE US,0.4621676891615542,"certain probability p ∼
1
m, and we can use more advanced combinations of past iterates (like the
186"
LET THE OBJECTIVE GUIDE US,0.46421267893660534,"average). However, for our purposes, we simply choose option 4 (i.e. the last iterate), thus it is only
187"
LET THE OBJECTIVE GUIDE US,0.4662576687116564,"updated once every m iterations.
188"
LET THE OBJECTIVE GUIDE US,0.46830265848670755,"We also need to address the question of the measure of similarity in this case. Since we are using a
189"
LET THE OBJECTIVE GUIDE US,0.4703476482617587,"second-order approximation of f −h, it is very logical to compare them using the difference between
190"
LET THE OBJECTIVE GUIDE US,0.4723926380368098,"their third derivatives or equivalently, the Hessian Lipschitz constant of their difference. Precisely we
191"
LET THE OBJECTIVE GUIDE US,0.47443762781186094,"make the following similarity assumption :
192"
LET THE OBJECTIVE GUIDE US,0.47648261758691207,"Assumption 3 (Lipschitz similarity) Let for some δ1, δ2 ≥0, it holds, ∀x, ˜x ∈Rd:"
LET THE OBJECTIVE GUIDE US,0.4785276073619632,"Eh1[∥G(h1, x, ˜x) −∇f(x)∥3/2]
≤
δ3/2
1
∥x −˜x∥3,"
LET THE OBJECTIVE GUIDE US,0.48057259713701433,"Eh2[∥H(h2, x, ˜x) −∇2f(x)∥3]
≤
δ3
2∥x −˜x∥3."
LET THE OBJECTIVE GUIDE US,0.48261758691206547,"In particular, if f −h1 and f −h2 have δ1 and δ2 Lipschitz Hessians respectively then h1 and h2
193"
LET THE OBJECTIVE GUIDE US,0.48466257668711654,"satisfy Assumption 3.
194"
LET THE OBJECTIVE GUIDE US,0.4867075664621677,"Under this assumption, we show that the errors resulting from the use of the snapshot can be
195"
LET THE OBJECTIVE GUIDE US,0.4887525562372188,"successfully balanced by choosing M satisfying:
196"
LET THE OBJECTIVE GUIDE US,0.49079754601226994,"4
  δ1"
LET THE OBJECTIVE GUIDE US,0.49284253578732107,"M
3/2 + 73
  δ2"
LET THE OBJECTIVE GUIDE US,0.4948875255623722,"M
3
≤
1
24m3 .
(9)"
LET THE OBJECTIVE GUIDE US,0.49693251533742333,"And we have the following theorem.
197"
LET THE OBJECTIVE GUIDE US,0.49897750511247446,"Theorem 2 For f, h1, h2 verifying Assumptions 1,3. For a regularization parameter M chosen
such that M ≥L and (9) is satisfied. For an output of Algorithm 1 xout chosen uniformly at
random from (xi)0≤i≤Sm:=T , we have:"
LET THE OBJECTIVE GUIDE US,0.5010224948875256,"E[µM(xout)]
=
O
 √"
LET THE OBJECTIVE GUIDE US,0.5030674846625767,"MF0
Sm

,"
LET THE OBJECTIVE GUIDE US,0.5051124744376279,"In particular, we can choose M = max(L, 32δ1m2, 16δ2m) which gives
198"
LET THE OBJECTIVE GUIDE US,0.5071574642126789,"E[µM(xout)]
=
O
 √"
LET THE OBJECTIVE GUIDE US,0.50920245398773,"LF0
Sm +
√δ2F0"
LET THE OBJECTIVE GUIDE US,0.5112474437627812,"S√m +
√δ1F0"
LET THE OBJECTIVE GUIDE US,0.5132924335378323,"S

.
(10)"
LET THE OBJECTIVE GUIDE US,0.5153374233128835,"Based on the choices of the helpers h1 and h2 we can have many algorithms. We discuss these in
199"
LET THE OBJECTIVE GUIDE US,0.5173824130879345,"the following sections. We start by discussing variance reduction and Lazy Hessians which rely on
200"
LET THE OBJECTIVE GUIDE US,0.5194274028629857,"sampling batches randomly, then move to core-sets which try to find, more intelligently, representative
201"
LET THE OBJECTIVE GUIDE US,0.5214723926380368,"weighted batches of data, after this, we discuss semi-supervised learning and how unlabeled data can
202"
LET THE OBJECTIVE GUIDE US,0.523517382413088,"be used to engineer the helpers. More generally, auxiliary learning tries to leverage auxiliary tasks in
203"
LET THE OBJECTIVE GUIDE US,0.5255623721881391,"training a given main task, the auxiliary tasks can be treated as helpers.
204"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5276073619631901,"3.3
Variance Reduction and Lazy Hessians
205"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5296523517382413,"The following lemma demonstrates that we can create helper functions h with lower similarity to the
206"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5316973415132924,"main function f of the form (1) by employing sampling and averaging.
207"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5337423312883436,Lemma 1 Let f = 1
VARIANCE REDUCTION AND LAZY HESSIANS,0.5357873210633947,"n
Pn
i=1 fi such that all fi are twice differentiable and have L-Lipschitz Hes-
sians. Let B ⊂{1, · · · , n} be of size b and sampled with replacement uniformly at random, and"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5378323108384458,"define hB = 1 b
P"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5398773006134969,"i∈B fi, then hB satisfies Assumption 3 with δ1 =
L
√"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5419222903885481,"b and δ2 = O(
√"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5439672801635992,"log(d)L
√ b
)."
VARIANCE REDUCTION AND LAZY HESSIANS,0.5460122699386503,"Choice of the parameter m in Algorithm 1.
Minimizing the total arithmetic cost, we choose
208"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5480572597137015,"m = arg minm #Grad(m, ε) + d#Hess(m, ε), where #Grad(m, ε) and #Hess(m, ε) denote
209"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5501022494887525,"the number of gradients and Hessians required to find an ε stationary point.
210"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5521472392638037,"Now we are ready to discuss several special cases that are direct consequences from Theorem 2.
211"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5541922290388548,"First, note that choosing h1 = h2 = f gives the classical Cubic Newton method [20], whereas
212"
VARIANCE REDUCTION AND LAZY HESSIANS,0.556237218813906,"choosing h1 = f and h2 = 0, gives the Lazy Cubic Newton [9]. In both cases, we recuperate the
213"
VARIANCE REDUCTION AND LAZY HESSIANS,0.558282208588957,"known rates of convergence.
214 215"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5603271983640081,"General variance reduction.
If we sample batches Bg and Bh of sizes bg and bh consecutively at
random and choose"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5623721881390593,"h1
=
1
bg
P"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5644171779141104,"i∈Bg fi
and
h2 =
1
bh
P"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5664621676891616,"i∈Bh fi,"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5685071574642127,"and use these helpers along with the estimates (7), (8), we obtain the Variance Reduced Cubic
216"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5705521472392638,"Newton algorithm [35, 29]. According to Lemma 1, this choice corresponds to δ1 =
L
√"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5725971370143149,"bg and
217"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5746421267893661,"δ2 = ˜O(
L
√bh ). For bg ∼m4 ∧n, bh ∼m2 ∧n and M = L, we have the non-convex convergence
218"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5766871165644172,"rate O
  √"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5787321063394683,"LF0
Sm

, which is the same as that of the cubic Newton algorithm but with a smaller cost per
219"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5807770961145194,"iteration. Minimizing the total arithmetic cost, we can choose m = arg min
m"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5828220858895705,dn+d(m3∧nm)+(m5∧nm)
VARIANCE REDUCTION AND LAZY HESSIANS,0.5848670756646217,"m
.
220"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5869120654396728,"Let us denote by gV R(n, d) the corresponding optimal value. Then we reach an (ε, L)-approximate
221"
VARIANCE REDUCTION AND LAZY HESSIANS,0.588957055214724,"second-order local minimum in at most O( gV R(n,d)"
VARIANCE REDUCTION AND LAZY HESSIANS,0.591002044989775,"ε3/2
) × GradCost arithmetic operations.
222"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5930470347648262,"Variance reduction with Lazy Hessians.
We can also use lazy updates for Hessians combined
with variance-reduced gradients. This corresponds to choosing"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5950920245398773,"h1
=
1
bg
P"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5971370143149284,"i∈Bg fi
and
h2 = 0,"
VARIANCE REDUCTION AND LAZY HESSIANS,0.5991820040899796,"which implies (according to Lemma 1) that δ1 =
L
√"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6012269938650306,"bg and δ2 = L. In this case, we need bg ∼m2
223"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6032719836400818,"to obtain a convergence rate of O
  √"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6053169734151329,"LF0
S√m

, which matches the convergence rate of the Lazy Cubic
224"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6073619631901841,"Newton method while using stochastic gradients. We choose this time m = arg min
m"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6094069529652352,"nd+(m3∧mn)
√m
,
225"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6114519427402862,"as before. Let us denote gLazy(n, d) the corresponding minimum. Then we guarantee to reach an
226"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6134969325153374,"(ε, mL)-approximate second-order local minimum in at most O( gLazy(n,d)"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6155419222903885,"ε3/2
) × GradCost operations.
227"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6175869120654397,"To be lazy or not to be? We show that gLazy(n, d) ∼(nd)5/6 ∧n
√"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6196319018404908,"d and gV R(n, d) ∼(nd)4/5 ∧
228"
VARIANCE REDUCTION AND LAZY HESSIANS,0.621676891615542,"(n2/3d + n). In particular, for d ≥n2/3 we have gLazy(n, d) ≤gV R(n, d) and thus for d ≥n2/3
229"
VARIANCE REDUCTION AND LAZY HESSIANS,0.623721881390593,"it is better to use Lazy Hessians than variance-reduced Hessians from a gradient equivalent cost
230"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6257668711656442,"perspective. We note also that for the Lazy approach, we can keep a factorization of the Hessian (this
231"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6278118609406953,"factorization induces most of the cost of solving the cubic subproblem) and thus it is as if we only
232"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6298568507157464,"need to solve the subproblem once every m iterations, so the Lazy approach has a big advantage
233"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6319018404907976,"compared to the general approach, and the advantage becomes even bigger for the case of large
234"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6339468302658486,"dimensions.
235"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6359918200408998,"Note that according to the theory, we could use the same random batches Bg, Bh ⊆[n] generated
236"
VARIANCE REDUCTION AND LAZY HESSIANS,0.6380368098159509,"once for all iterations. However, using the resampled batches can lead to a more stable convergence.
237"
OTHER APPLICATIONS,0.6400817995910021,"3.4
Other Applications
238"
OTHER APPLICATIONS,0.6421267893660532,"The result in (10) is general enough that it can include many other applications that are only limited
239"
OTHER APPLICATIONS,0.6441717791411042,"by our imagination. To cite a few such applications there are:
240"
OTHER APPLICATIONS,0.6462167689161554,"Core sets. [3] The idea of core sets is simple: can we summarize a potentially big data set using
241"
OTHER APPLICATIONS,0.6482617586912065,"only a few (weighted) important examples? Many reasons such as redundancy make the answer yes.
242"
OTHER APPLICATIONS,0.6503067484662577,"Devising approaches to find such core sets is outside of the scope of this work, but in general, we
243"
OTHER APPLICATIONS,0.6523517382413088,"can see from (10) that if we have batches Bg, Bh such that they are (δ1, 1) and (δ2, 2) similar to f
244"
OTHER APPLICATIONS,0.65439672801636,"respectively, then we can keep reusing the same batch Bg for at least
q"
OTHER APPLICATIONS,0.656441717791411,"L
δ1 times, and Bh for L"
OTHER APPLICATIONS,0.6584867075664622,"δ2 all
245"
OTHER APPLICATIONS,0.6605316973415133,"the while guaranteeing an improved rate. So then if we can design such small batches with small δ1
246"
OTHER APPLICATIONS,0.6625766871165644,"and δ2 then we can keep reusing them, and joy the improved rate without needing large batches.
247"
OTHER APPLICATIONS,0.6646216768916156,"Auxiliary learning. [4, 2, 31] study how a given task f can be trained in the presence of auxiliary
248"
OTHER APPLICATIONS,0.6666666666666666,"(related) tasks. Our approach can be indeed used for auxiliary learning by treating the auxiliaries as
249"
OTHER APPLICATIONS,0.6687116564417178,"helpers. If we compare (10) to the rate that we obtained without the use of the helpers: O(
√ LF0"
OTHER APPLICATIONS,0.6707566462167689,"S
), we
250"
OTHER APPLICATIONS,0.6728016359918201,see that we have a better rate using the helpers/auxiliary tasks when 1
OTHER APPLICATIONS,0.6748466257668712,"m +
√δ2
√"
OTHER APPLICATIONS,0.6768916155419223,"mL +
√δ1
√"
OTHER APPLICATIONS,0.6789366053169734,"L ≤1.
251"
OTHER APPLICATIONS,0.6809815950920245,"Semi-supervised learning.[34] Semi-supervised learning is a machine learning approach that com-
252"
OTHER APPLICATIONS,0.6830265848670757,"bines the use of both labeled data and unlabeled data during training. In general, we can use the
253"
OTHER APPLICATIONS,0.6850715746421268,"unlabeled data to construct the helpers, we can start for example by using random labels for the
254"
OTHER APPLICATIONS,0.6871165644171779,"helpers and improving the labels with training. There are at least two special cases where our theory
255"
OTHER APPLICATIONS,0.689161554192229,"implies improvement by only assigning random labels to the unlabeled data. In fact, for both regular-
256"
OTHER APPLICATIONS,0.6912065439672802,"ized least squares and logistic regression, we notice that the Hessian is independent of the labels (only
257"
OTHER APPLICATIONS,0.6932515337423313,"depends on inputs) and thus if the unlabeled data comes from the same distribution as the labeled
258"
OTHER APPLICATIONS,0.6952965235173824,"data, then we can use it to construct helpers which, at least theoretically, have δ1 = δ2 = 0. Because
259"
OTHER APPLICATIONS,0.6973415132924335,"the Hessian is independent of the labels, we can technically endow the unlabeled data with random
260"
OTHER APPLICATIONS,0.6993865030674846,"labels. Theorem 2 would imply in this case E[µL(xout)] = O(
√"
OTHER APPLICATIONS,0.7014314928425358,"LF0
Sm ), where S is the number of
261"
OTHER APPLICATIONS,0.7034764826175869,"times we use labeled data and S(m −1) is the number of unlabeled data.
262"
GRADIENT-DOMINATED FUNCTIONS,0.7055214723926381,"4
Gradient-Dominated Functions
263"
GRADIENT-DOMINATED FUNCTIONS,0.7075664621676891,"We consider now the class of gradient-dominated functions defined below.
264"
GRADIENT-DOMINATED FUNCTIONS,0.7096114519427403,"Assumption 4 (τ, α)-gradient dominated. A function f is called gradient dominated on set if
it holds, for some α ≥1 and τ > 0:"
GRADIENT-DOMINATED FUNCTIONS,0.7116564417177914,"f(x) −f ⋆
≤
τ∥∇f(x)∥α,
∀x ∈Rd.
(11)"
GRADIENT-DOMINATED FUNCTIONS,0.7137014314928425,"Examples of functions satisfying this assumption are convex functions (α = 1) and strongly convex
265"
GRADIENT-DOMINATED FUNCTIONS,0.7157464212678937,"functions (α = 2), see Appendix D.1. For such functions, we can guarantee convergence (in
266"
GRADIENT-DOMINATED FUNCTIONS,0.7177914110429447,"expectation) to a global minimum, i.e. we can find a point x such that f(x) −f ⋆≤ε.
267"
GRADIENT-DOMINATED FUNCTIONS,0.7198364008179959,"The Gradient-dominance property is interesting because many non-convex functions have been shown
268"
GRADIENT-DOMINATED FUNCTIONS,0.721881390593047,"to satisfy it [28, 13, 18]. Furthermore, besides convergence to a global minimum, we get accelerated
269"
GRADIENT-DOMINATED FUNCTIONS,0.7239263803680982,"rates.
270"
GRADIENT-DOMINATED FUNCTIONS,0.7259713701431493,"We note that for α > 3/2 (and only for this case), we needed to assume the following (stronger)
271"
GRADIENT-DOMINATED FUNCTIONS,0.7280163599182005,"inequality:
272"
GRADIENT-DOMINATED FUNCTIONS,0.7300613496932515,"Ef(xt) −f ⋆
≤
τE

∥∇f(xt)∥
α,
(12)"
GRADIENT-DOMINATED FUNCTIONS,0.7321063394683026,"where the expectation is taken with respect to the iterates (xt) of our algorithms. This is a stronger
273"
GRADIENT-DOMINATED FUNCTIONS,0.7341513292433538,"assumption than (11). To avoid using this stronger assumption, we can assume that the iterates belong
274"
GRADIENT-DOMINATED FUNCTIONS,0.7361963190184049,"to some compact set Q ⊂Rd and that the gradient norm is uniformly bounded: ∀x ∈Q : ∥∇f(x)∥≤
275"
GRADIENT-DOMINATED FUNCTIONS,0.7382413087934561,"G. Then, a (τ, α)-gradient dominated on set Q function is also a (τGα−3/2, 3/2)-gradient dominated
276"
GRADIENT-DOMINATED FUNCTIONS,0.7402862985685071,"on this set for any α > 3/2.
277"
GRADIENT-DOMINATED FUNCTIONS,0.7423312883435583,"In Theorem 3 we extend the results of Theorem 1 to gradient-dominated functions.
278"
GRADIENT-DOMINATED FUNCTIONS,0.7443762781186094,"Theorem 3 Under Assumptions 1,2,4, for M ≥L and T := Sm we have:
- For 1 ≤α ≤3/2:
E[f(xT )] −f ⋆
=
O
  α
√"
GRADIENT-DOMINATED FUNCTIONS,0.7464212678936605,"Mτ 3/(2α)
(3−2α)T

2α
3−2α + τ δ2α
2
M α + τδα
1

."
GRADIENT-DOMINATED FUNCTIONS,0.7484662576687117,"- For 3/2 < α ≤2 , let h0 = O(
F0 (
√"
GRADIENT-DOMINATED FUNCTIONS,0.7505112474437627,"Mτ
3
2α )
2α
3−2α ), then for T ≥t0 = O(h 3−2α"
GRADIENT-DOMINATED FUNCTIONS,0.7525562372188139,"2α
0
log(h0)) we have:"
GRADIENT-DOMINATED FUNCTIONS,0.754601226993865,"E[f(xT )] −f ⋆
=
O

(
√"
GRADIENT-DOMINATED FUNCTIONS,0.7566462167689162,"Mτ
3
2α )
2α
3−2α   1"
GRADIENT-DOMINATED FUNCTIONS,0.7586912065439673,"2
( 2α"
GRADIENT-DOMINATED FUNCTIONS,0.7607361963190185,"3 )T −t0 + τ δ2α
2
M α + τδα
1

."
GRADIENT-DOMINATED FUNCTIONS,0.7627811860940695,"Theorem 3 shows (up to the noise level) for 1 ≤α < 3/2 a sublinear rate, for α = 3/2 a linear rate
279"
GRADIENT-DOMINATED FUNCTIONS,0.7648261758691206,"(obtained by taking the limit α →3/2) and a superlinear rate for α > 3/2.
280"
GRADIENT-DOMINATED FUNCTIONS,0.7668711656441718,"We do the same thing for Theorem 2 which we extend in Theorem 4. In this case, we need to set the
281"
GRADIENT-DOMINATED FUNCTIONS,0.7689161554192229,"snapshot line 3 in Algorithm 1) as in 5 i.e. the snapshot corresponds to the state with the smallest
282"
GRADIENT-DOMINATED FUNCTIONS,0.7709611451942741,"value of f during the last m iterations.
283"
GRADIENT-DOMINATED FUNCTIONS,0.7730061349693251,"Theorem 4 Under Assumptions 1,3,4, for M = max(L, 34δ1m2, 11δ2m) , we have:"
GRADIENT-DOMINATED FUNCTIONS,0.7750511247443763,"- For 1 ≤α ≤3/2 : E[f(xSm)] −f ⋆= O
  α
√"
GRADIENT-DOMINATED FUNCTIONS,0.7770961145194274,"Mτ 3/(2α)
(3−2α)Sm

2α
3−2α

."
GRADIENT-DOMINATED FUNCTIONS,0.7791411042944786,"- For 3/2 < α ≤2, let h0 = O(
F0 (
√"
GRADIENT-DOMINATED FUNCTIONS,0.7811860940695297,"M
m τ
3
2α )
2α
3−2α ), then for S ≥s0 = O(h 3−2α"
GRADIENT-DOMINATED FUNCTIONS,0.7832310838445807,"2α
0
log(h0)) we"
GRADIENT-DOMINATED FUNCTIONS,0.7852760736196319,"have:
E[f(xSm)] −f ⋆
=

(
√"
GRADIENT-DOMINATED FUNCTIONS,0.787321063394683,"M
m τ
3
2α )
2α
3−2α   1"
GRADIENT-DOMINATED FUNCTIONS,0.7893660531697342,"2
( 2α"
GRADIENT-DOMINATED FUNCTIONS,0.7914110429447853,3 )S−s0
GRADIENT-DOMINATED FUNCTIONS,0.7934560327198364,"Again, the same behavior is observed as for Theorem 3 but this time without noise (variance reduction
284"
GRADIENT-DOMINATED FUNCTIONS,0.7955010224948875,"is working). To the best of our knowledge, this is the first time such analysis is made. As a direct
285"
GRADIENT-DOMINATED FUNCTIONS,0.7975460122699386,"consequence of our results, we obtain new global complexities for the variance-reduced and lazy
286"
GRADIENT-DOMINATED FUNCTIONS,0.7995910020449898,"variance-reduced Cubic Newton methods on the class of gradient-dominated functions.
287"
GRADIENT-DOMINATED FUNCTIONS,0.8016359918200409,"To compare the statements of Theorems 3 and 4, for convex functions (i.e. α = 1), Theorem 3
288"
GRADIENT-DOMINATED FUNCTIONS,0.803680981595092,"guarantees convergence to a ε−global minimum in at most O(
1
ε5/2 +
d
ε3/2 ) GradCost, whereas
289"
GRADIENT-DOMINATED FUNCTIONS,0.8057259713701431,"Theorem 4 only needs O( g(n,d)
√ε ) GradCost, where g(n, d) is either gLazy(n, d) = (nd)5/6 ∧n
√ d
290"
GRADIENT-DOMINATED FUNCTIONS,0.8077709611451943,"or gV R(n, d) = (nd)4/5 ∧(n2/3d + n). See the Appendix D.3 for more details.
291"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8098159509202454,"5
Limitations and possible extensions
292"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8118609406952966,"Estimating similarity between the helpers and the main function. While we show in this work
293"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8139059304703476,"that we can have an improvement over training alone, this supposes that we know the similarity
294"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8159509202453987,"constants δ1, δ2, hence it will be interesting to have approaches that can adapt to such constants.
295"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8179959100204499,"Engineering helper functions. Building helper task with small similarities is also an interesting idea.
296"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.820040899795501,"Besides the examples in supervised learning and core-sets that we provide, it is not evident how to do
297"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8220858895705522,"it in a generalized way.
298"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8241308793456033,"Using the helper to regularize the cubic subproblem. We note that while we proposed to approxi-
299"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8261758691206544,"mate the “cheap"" part as well in Section 3, one other theoretically viable approach is to keep it intact
300"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8282208588957055,"and approximately solve a “proximal type"" problem involving h, this will lead to replacing L by δ,
301"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8302658486707567,"but the subproblem is even more difficult to solve. However our theory suggests that we don’t need
302"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8323108384458078,"to solve this subproblem exactly, we only need m ≥L"
LIMITATIONS AND POSSIBLE EXTENSIONS,0.8343558282208589,"δ . We do not treat this case here.
303"
CONCLUSION,0.83640081799591,"6
Conclusion
304"
CONCLUSION,0.8384458077709611,"In this work, we proposed a general theory for using auxiliary information in the context of the
305"
CONCLUSION,0.8404907975460123,"cubically regularized Newton’s method. Our theory encapsulates the classical stochastic methods as
306"
CONCLUSION,0.8425357873210634,"well as variance reduction and Lazy methods. For auxiliary learning, we showed a provable benefit
307"
CONCLUSION,0.8445807770961146,"compared to training alone. Besides studying the convergence for general non-convex functions
308"
CONCLUSION,0.8466257668711656,"for which we show convergence to approximate local minima, we also study gradient-dominated
309"
CONCLUSION,0.8486707566462167,"functions, for which convergence is accelerated and is to approximate global minima.
310"
REFERENCES,0.8507157464212679,"References
311"
REFERENCES,0.852760736196319,"[1] A. Agafonov, D. Kamzolov, P. Dvurechensky, A. Gasnikov, and M. Takáˇc. Inexact tensor meth-
312"
REFERENCES,0.8548057259713702,"ods and their application to stochastic convex optimization. arXiv preprint arXiv:2012.15636,
313"
REFERENCES,0.8568507157464212,"2020.
314"
REFERENCES,0.8588957055214724,"[2] N. Aviv, A. Idan, M. Haggai, C. Gal, and F. Ethan. Auxiliary learning by implicit differentiation.
315"
REFERENCES,0.8609406952965235,"ICLR 2021.
316"
REFERENCES,0.8629856850715747,"[3] O. Bachem, M. Lucic, and K. Andreas. Practical coreset constructions for machine learning.
317"
REFERENCES,0.8650306748466258,"arXiv:1703.06476 [stat.ML]https://arxiv.org/abs/1703.06476, 2017.
318"
REFERENCES,0.8670756646216768,"[4] S. Baifeng, H. Judy, S. Kate, D. Trevor, and X. Huijuan. Auxiliary task reweighting for
319"
REFERENCES,0.869120654396728,"minimum-data learning. 34th Conference on Neural Information Processing Systems (NeurIPS
320"
REFERENCES,0.8711656441717791,"2020), Vancouver, Canada.
321"
REFERENCES,0.8732106339468303,"[5] C. Cartis, N. I. Gould, and P. L. Toint. Adaptive cubic regularisation methods for uncon-
322"
REFERENCES,0.8752556237218814,"strained optimization. Part I: motivation, convergence and numerical results. Mathematical
323"
REFERENCES,0.8773006134969326,"Programming, 127(2):245–295, 2011.
324"
REFERENCES,0.8793456032719836,"[6] C. Cartis and K. Scheinberg. Global convergence rate analysis of unconstrained optimization
325"
REFERENCES,0.8813905930470347,"methods based on probabilistic models. Mathematical Programming, 169:337–375, 2018.
326"
REFERENCES,0.8834355828220859,"[7] E. M. Chayti and S. P. Karimireddy.
Optimization with access to auxiliary information.
327"
REFERENCES,0.885480572597137,"arXiv:2206.00395 [cs.LG], 2022.
328"
REFERENCES,0.8875255623721882,"[8] A. R. Conn, N. I. Gould, and P. L. Toint. Trust region methods. SIAM, 2000.
329"
REFERENCES,0.8895705521472392,"[9] N. Doikov, E. M. Chayti, and M. Jaggi.
Second-order optimization with lazy hessians.
330"
REFERENCES,0.8916155419222904,"arXiv:2212.00781 [math.OC], 2022.
331"
REFERENCES,0.8936605316973415,"[10] N. Doikov and Y. Nesterov. Minimizing uniformly convex functions by cubic regularization of
332"
REFERENCES,0.8957055214723927,"newton method. Journal of Optimization Theory and Applications 189:317–339, 2021.
333"
REFERENCES,0.8977505112474438,"[11] S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
334"
REFERENCES,0.8997955010224948,"programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.
335"
REFERENCES,0.901840490797546,"[12] S. Ghadimi, H. Liu, and T. Zhang. Second-order methods with cubic regularization under
336"
REFERENCES,0.9038854805725971,"inexact information. arXiv preprint arXiv:1710.05782, 2017.
337"
REFERENCES,0.9059304703476483,"[13] M. Hardt and T. Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.
338"
REFERENCES,0.9079754601226994,"[14] C. J. Hillar and L.-H. Lim. Most tensor problems are np-hard. Journal of the ACM (JACM) 60
339"
REFERENCES,0.9100204498977505,"45., 2013.
340"
REFERENCES,0.9120654396728016,"[15] H. J. Kelley. Gradient theory of optimal flight paths. Ars Journal, 30(10):947–954, 1960.
341"
REFERENCES,0.9141104294478528,"[16] J. Kiefer and J. Wolfowitz. Stochastic estimation of the maximum of a regression function. Ann.
342"
REFERENCES,0.9161554192229039,"Math. Statist. Volume 23, Number 3, 462-466, 1952.
343"
REFERENCES,0.918200408997955,"[17] J. M. Kohler and A. Lucchi. Sub-sampled cubic regularization for non-convex optimization.
344"
REFERENCES,0.9202453987730062,"arXiv preprint arXiv:1705.05933, 2017.
345"
REFERENCES,0.9222903885480572,"[18] S. Masiha, S. Salehkaleybar, N. He, N. Kiyavash, and P. Thiran. Stochastic second-order
346"
REFERENCES,0.9243353783231084,"methods improve best-known sample complexity of sgd for gradient-dominated functions.
347"
REFERENCES,0.9263803680981595,"In NeurIPS 2022 - Advances in Neural Information Processing Systems, volume 35, pages
348"
REFERENCES,0.9284253578732107,"10862–10875, 2022.
349"
REFERENCES,0.9304703476482618,"[19] Y. Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
350"
REFERENCES,0.9325153374233128,"[20] Y. Nesterov and B. Polyak. Cubic regularization of Newton’s method and its global performance.
351"
REFERENCES,0.934560327198364,"Mathematical Programming, 108(1):177–205, 2006.
352"
REFERENCES,0.9366053169734151,"[21] T. Nilesh, S. Mitchell, J. Chi, R. Jeffrey, and J. Michael I. Stochastic cubic regularization for
353"
REFERENCES,0.9386503067484663,"fast nonconvex optimization. Part of Advances in Neural Information Processing Systems 31,
354"
REFERENCES,0.9406952965235174,"2018.
355"
REFERENCES,0.9427402862985685,"[22] J. Nocedal and S. Wright. Numerical optimization. Springer Science & Business Media, 2006.
356"
REFERENCES,0.9447852760736196,"[23] B. T. Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi Matematiki
357"
REFERENCES,0.9468302658486708,"i Matematicheskoi Fiziki, 3(4):643–653,, 1963.
358"
REFERENCES,0.9488752556237219,"[24] B. T. Polyak. Newton’s method and its use in optimization. European Journal of Operational
359"
REFERENCES,0.950920245398773,"Research, 181(3):1086–1096, 2007.
360"
REFERENCES,0.9529652351738241,"[25] H. Robbins and S. Monro. A stochastic approximation method the annals of mathematical
361"
REFERENCES,0.9550102249488752,"statistics. Vol. 22, No. 3. pp. 400-407, 1951.
362"
REFERENCES,0.9570552147239264,"[26] V. Shamanskii.
A modification of Newton’s method.
Ukrainian Mathematical Journal,
363"
REFERENCES,0.9591002044989775,"19(1):118–122, 1967.
364"
REFERENCES,0.9611451942740287,"[27] J. A. Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends®
365"
REFERENCES,0.9631901840490797,"in Machine Learning, 8(1-2):1–230, 2015.
366"
REFERENCES,0.9652351738241309,"[28] L. uanzhi and Y. Yang. Convergence analysis of two-layer neural networks with relu activation.
367"
REFERENCES,0.967280163599182,"Advances in neural information processing systems, 30, 2017.
368"
REFERENCES,0.9693251533742331,"[29] Z. Wang, Z. Yi, L. Yingbin, and L. Guanghui. Stochastic variance-reduced cubic regularization
369"
REFERENCES,0.9713701431492843,"for nonconvex optimization. AISTATS, 2019.
370"
REFERENCES,0.9734151329243353,"[30] B. Woodworth, K. Mishchenko, and F. Bach. Two losses are better than one: Faster optimization
371"
REFERENCES,0.9754601226993865,"using a cheaper proxy. arXiv preprint arXiv:2302.03542, 2023.
372"
REFERENCES,0.9775051124744376,"[31] L. Xingyu, S. B. Harjatin, K. George, and H. David. Adaptive auxiliary task weighting for
373"
REFERENCES,0.9795501022494888,"reinforcement learning. 33rd Conference on Neural Information Processing Systems (NeurIPS
374"
REFERENCES,0.9815950920245399,"2019), Vancouver, Canada.
375"
REFERENCES,0.983640081799591,"[32] P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Newton-type methods for non-convex
376"
REFERENCES,0.9856850715746421,"optimization under inexact Hessian information. arXiv preprint arXiv:1708.07164 ., 2017.
377"
REFERENCES,0.9877300613496932,"[33] P. Xu, J. Yang, F. Roosta-Khorasani, and M. W. Mahoney. Sub-sampled newton methods with
378"
REFERENCES,0.9897750511247444,"non-uniform sampling. 2016.
379"
REFERENCES,0.9918200408997955,"[34] X. Yang, Z. Song, I. King, and Z. Xu. A survey on deep semi-supervised learning. Technical
380"
REFERENCES,0.9938650306748467,"report, 2021.
381"
REFERENCES,0.9959100204498977,"[35] D. Zhou, P. Xu, and Q. Gu. Stochastic variance-reduced cubic regularization methods. Journal
382"
REFERENCES,0.9979550102249489,"of Machine Learning Research 20 1-47, 2019.
383"
