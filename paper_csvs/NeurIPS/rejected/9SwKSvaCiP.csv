Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001893939393939394,"We propose SING (StabIlized and Normalized Gradient), a plug-and-play tech-
1"
ABSTRACT,0.003787878787878788,"nique that improves the stability and generalization of the Adam(W) optimizer.
2"
ABSTRACT,0.005681818181818182,"SING is straightforward to implement and has minimal computational overhead,
3"
ABSTRACT,0.007575757575757576,"requiring only a layer-wise standardization of the gradients fed to Adam(W) with-
4"
ABSTRACT,0.00946969696969697,"out introducing additional hyper-parameters. We support the effectiveness and
5"
ABSTRACT,0.011363636363636364,"practicality of the proposed approach by showing improved results on a wide
6"
ABSTRACT,0.013257575757575758,"range of architectures, problems (such as image classification, depth estimation,
7"
ABSTRACT,0.015151515151515152,"and natural language processing), and in combination with other optimizers. We
8"
ABSTRACT,0.017045454545454544,"provide a theoretical analysis of the convergence of the method, and we show that
9"
ABSTRACT,0.01893939393939394,"by virtue of the standardization, SING can escape local minima narrower than a
10"
ABSTRACT,0.020833333333333332,"threshold that is inversely proportional to the network’s depth.
11"
INTRODUCTION,0.022727272727272728,"1
Introduction
12"
INTRODUCTION,0.02462121212121212,"Neural network training is a highly non-convex and stochastic optimization problem, complicated by
13"
INTRODUCTION,0.026515151515151516,"hidden dynamics between the optimization algorithm and the network architecture. Several common
14"
INTRODUCTION,0.028409090909090908,"pitfalls have been identified, such as bad initialization, vanishing and exploding gradients [3, 32],
15"
INTRODUCTION,0.030303030303030304,"abrupt shifts in the distribution of layer inputs (the so-called internal covariate shift [19]). Significant
16"
INTRODUCTION,0.032196969696969696,"progress has been made by tackling these issues either by architectural improvements [1, 17] or with
17"
INTRODUCTION,0.03409090909090909,"better optimizers [21, 28, 39].
18"
INTRODUCTION,0.03598484848484849,"The Adam(W) optimizer [21, 26] is widely adopted for neural network training due to its ability to
19"
INTRODUCTION,0.03787878787878788,"combine first and second-order moments of the gradient, mitigating the sensitivity to the learning
20"
INTRODUCTION,0.03977272727272727,"rate, and providing adaptability to gradient updates of different magnitude or sparsity. It is applicable
21"
INTRODUCTION,0.041666666666666664,"to widely different architectures, from convolutional to transformers, and application domains.
22"
INTRODUCTION,0.043560606060606064,"Nonetheless, it has shown instabilities in specific scenarios, such as large-scale problems [5, 29] or,
23"
INTRODUCTION,0.045454545454545456,"as we demonstrate in this work, some image-to-image tasks. These instabilities manifest as spikes in
24"
INTRODUCTION,0.04734848484848485,"the training loss which might involve a prolonged recovery periods - if it recovers.
25"
INTRODUCTION,0.04924242424242424,"Contributions. In this work we propose a simple layer-wise gradient standardization as a technique
26"
INTRODUCTION,0.05113636363636364,"to improve the stability of existing optimizers. Our technique, SING, is plug-and-play: by simply
27"
INTRODUCTION,0.05303030303030303,"changing the gradient fed to AdamW (or any other “host” optimizer) it integrates seamlessly without
28"
INTRODUCTION,0.054924242424242424,"introducing any additional hyperparameters. As such, it does not require any additional fine-tuning
29"
INTRODUCTION,0.056818181818181816,"apart from that of the host optimization framework. In this way, SING preserves the desirable
30"
INTRODUCTION,0.058712121212121215,"properties of the host optimizer but with increased stability.
31"
INTRODUCTION,0.06060606060606061,"We notably theoretically show that the optimizer is capable to escape narrow local minima within a
32"
INTRODUCTION,0.0625,"single step, given a sufficiently high learning rate (see Theorem 3.1). Moreover, the magnitude of
33"
INTRODUCTION,0.06439393939393939,"this learning rate is inversely proportional to the depth of the network i.e. for a fixed learning rate,
34"
INTRODUCTION,0.06628787878787878,"the higher the number of layers of the network, the lower the learning rate must be to escape local
35"
INTRODUCTION,0.06818181818181818,"minima. This highlights the compatibility of our technique with deep neural networks. Since narrow
36"
INTRODUCTION,0.07007575757575757,"local minima are often associated with poor generalization in non-convex optimization landscapes
37"
INTRODUCTION,0.07196969696969698,"−10
−5
0
5 x 0 1 2 3 F(x)"
INTRODUCTION,0.07386363636363637,Initial point
INTRODUCTION,0.07575757575757576,Iterates of SGD
INTRODUCTION,0.07765151515151515,Iterates of SING + SGD
INTRODUCTION,0.07954545454545454,Final point of SGD
INTRODUCTION,0.08143939393939394,Final point of SING + SGD
INTRODUCTION,0.08333333333333333,"Figure 1: Optimization of the function with three local minima. A gradient descent (noted SGD) with
the proposed gradient standardization SING can escape narrow local minima (Theorem 3.1). SING
steps jump over narrow local minima in one step. Conversely, SGD without SING steps jump over
the first local minimum but stays blocked in the second one because the gradient amplitude is too
small. The learning rate is reduced using a cosine decay."
INTRODUCTION,0.08522727272727272,"[6, 15, 16, 20, 33], it is crucial for an optimizer to avoid such pitfalls. We capitalize on this theoretical
38"
INTRODUCTION,0.08712121212121213,"result and stabilize the gradient using several techniques [18, 57, 40] to reach a step size as large as
39"
INTRODUCTION,0.08901515151515152,"possible, and thus escape from as many local minima as possible.
40"
INTRODUCTION,0.09090909090909091,"Although our initial focus was to address Adam’s limitations, our technique demonstrates robust gen-
41"
INTRODUCTION,0.0928030303030303,"eralization even in tasks where Adam performs well. Notably, we show improvements over AdamW
42"
INTRODUCTION,0.0946969696969697,"on several tasks such as image classification, depth estimation and natural language processing.
43"
INTRODUCTION,0.09659090909090909,"The paper is organized as follows. Section 2 describes the proposed method, followed by its theoretical
44"
INTRODUCTION,0.09848484848484848,"analysis in Section 3 where the properties of SING are presented. We discuss related work in Section
45"
INTRODUCTION,0.10037878787878787,"4. In Section 5 we provide extensive numerical experiments which demonstrate the effectiveness of
46"
INTRODUCTION,0.10227272727272728,"the proposed method on a variety of tasks in natural language processing and computer vision.
47"
ALGORITHM,0.10416666666666667,"2
Algorithm
48"
ALGORITHM,0.10606060606060606,"We seek to approximate the solution to the following optimization problem,
49"
ALGORITHM,0.10795454545454546,"min
x∈Rp F(x).
(1)"
ALGORITHM,0.10984848484848485,"We assume there exists a random function f : Rp →R such that E[∇f(x)] = ∇F(x) for all x ∈Rp,
50"
ALGORITHM,0.11174242424242424,"and that we have access to an oracle providing i.i.d. samples (fn)n∈N [9].
51"
ALGORITHM,0.11363636363636363,"In the case of neural networks, the optimization variable x represents the parameters of the network,
52"
ALGORITHM,0.11553030303030302,"F is the oracle loss and f is the empirical loss evaluated on a random mini-batch. The parameters
53"
ALGORITHM,0.11742424242424243,"of a neural network have a specific structure. They are made of the concatenation of the parameter
54"
ALGORITHM,0.11931818181818182,"tensors from each layer of the network. We use D ∈N to denote the number of these parameters
55"
ALGORITHM,0.12121212121212122,"tensors and define (Ik)k∈[[1,D]] such that xIk = {xi : i ∈Ik} represents the k-th parameter tensor. As
56"
ALGORITHM,0.12310606060606061,"an example, let’s consider the neural network N(·; A, b) = A · +b. In this case, the network has two
57"
ALGORITHM,0.125,"parameter tensors, hence D = 2. The first parameter tensor is xI1 = A and the second is xI2 = b.
58"
ALGORITHM,0.1268939393939394,"Our objective is to endow an optimizer with several key properties: 1) stability 2) capacity to
59"
ALGORITHM,0.12878787878787878,"escape narrow local minima 3) adaptability to geometry of the energy landscape, and 4) convergence.
60"
ALGORITHM,0.13068181818181818,"Importantly, we want to achieve all of these properties without adding any additional hyper-parameters
61"
ALGORITHM,0.13257575757575757,"and with minimal computational overhead. We achieve the first property – stability – by dividing the
62"
ALGORITHM,0.13446969696969696,"gradient by its norm. This prevents vanishing and exploding gradients, which can lead to unstable
63"
ALGORITHM,0.13636363636363635,"training. This also allows to use fixed gradient steps, which enables the algorithm to move past
64"
ALGORITHM,0.13825757575757575,"narrow local minima, as we show in the next section.
65"
ALGORITHM,0.14015151515151514,"def optim_step(model, lr, beta, weight_decay, ϵ):"
ALGORITHM,0.14204545454545456,for p in model.parameters():
ALGORITHM,0.14393939393939395,"# Standardization
p.grad = centralize(p.grad)
p.grad = normalize(p.grad, ϵ)
# Weight decay
p = p * (1 - lr * weight_decay)
# Optimizer
update = optimizer(p.grad, beta)
# Parameter update
p = p - lr * update"
ALGORITHM,0.14583333333333334,def centralize(grad):
ALGORITHM,0.14772727272727273,"if grad.dim() > 1:
dims = tuple(range(1, grad.dim()))
mean = grad.mean(dims, keepdim=True)
grad = grad - mean
return grad"
ALGORITHM,0.14962121212121213,"def normalize(grad, ϵ):
grad = grad / (grad.norm() + ϵ)
return grad"
ALGORITHM,0.15151515151515152,"Algorithm 2: PYTORCH implementation of our algorithm. The Γ operator is implemented by
NORMALIZE and ϕ by CENTRALIZE. Our technique can be used within any existing first order
method i.e. the OPTIMIZER function can be any optimizer (see Table 3 for a comparison)."
ALGORITHM,0.1534090909090909,"We define the steps taken by our optimizer by
66"
ALGORITHM,0.1553030303030303,xt+1 = xt −η ϕ(∇f(xt))
ALGORITHM,0.1571969696969697,"Γ(ϕ(∇f(xt)),
(2)"
ALGORITHM,0.1590909090909091,"where ϕ is the gradient centralization operation [49] and the division is applied element-wise. The
67"
ALGORITHM,0.16098484848484848,"operator Γ corresponds to the parameter-wise normalization i.e.
68"
ALGORITHM,0.16287878787878787,"Γ(x)i = ∥xIk∥2,
where k ∈[[1, D]] and i ∈Ik.
(3)"
ALGORITHM,0.16477272727272727,"In theory, there could be a division by zero in (2). To avoid this we can add ϵ = 10−8 to the
69"
ALGORITHM,0.16666666666666666,"denominator although it is not strictly necessary because the gradient norm is large in practice.
70"
ALGORITHM,0.16856060606060605,"This parametrization naturally arises in usual Deep Learning frameworks, see Algorithm 2 for the
71"
ALGORITHM,0.17045454545454544,"PYTORCH implementation.
72"
ALGORITHM,0.17234848484848486,"Our setting differs from regular normalized gradient descent in two ways: we center the gradients
73"
ALGORITHM,0.17424242424242425,"before normalizing and we perform the normalization on a parameter-wise basis. This is particularly
74"
ALGORITHM,0.17613636363636365,"important for large networks where the norm of the full gradient can be very large, making it nearly
75"
ALGORITHM,0.17803030303030304,"impossible to train the network effectively.
76"
THEORETICAL ANALYSIS,0.17992424242424243,"3
Theoretical Analysis
77"
THEORETICAL ANALYSIS,0.18181818181818182,"This section analyzes the key properties of our technique. Theorem 3.1 demonstrates how normaliza-
78"
THEORETICAL ANALYSIS,0.18371212121212122,"tion techniques aid in escaping local minima. Theorem 3.2 establishes stability results, including
79"
THEORETICAL ANALYSIS,0.1856060606060606,"several invariance properties of the algorithm. Moreover, Theorems 3.3 and 3.4 provide insights
80"
THEORETICAL ANALYSIS,0.1875,"into the rate of convergence of our algorithm in a stochastic setting, under mild assumptions. For
81"
THEORETICAL ANALYSIS,0.1893939393939394,"complete proofs and technical details, please refer to Appendix A.
82"
ESCAPING FROM NARROW LOCAL MINIMA,0.19128787878787878,"3.1
Escaping from narrow local minima
83"
ESCAPING FROM NARROW LOCAL MINIMA,0.19318181818181818,"One of the key properties of our algorithm is its ability to escape from narrow local minima. This is
84"
ESCAPING FROM NARROW LOCAL MINIMA,0.19507575757575757,"crucial because the stochasticity of the optimization landscape often leads to the creation of artificial
85"
ESCAPING FROM NARROW LOCAL MINIMA,0.19696969696969696,"local minima, generally associated with poor generalization performance [6, 15, 16, 20, 33]. To
86"
ESCAPING FROM NARROW LOCAL MINIMA,0.19886363636363635,"achieve this we normalize the gradient to take fixed-size steps during training, where the learning rate
87"
ESCAPING FROM NARROW LOCAL MINIMA,0.20075757575757575,"controls the step size. Doing so allows the escape from narrow local minima provided the steps are
88"
ESCAPING FROM NARROW LOCAL MINIMA,0.20265151515151514,"large enough. This property is central to our algorithm and leads to better generalization performance.
89"
ESCAPING FROM NARROW LOCAL MINIMA,0.20454545454545456,"For simplicity, we assume a deterministic setting in this section. We show that the normalization
90"
ESCAPING FROM NARROW LOCAL MINIMA,0.20643939393939395,"procedure helps the optimizer to escape narrow local minima. To formalize this observation, we first
91"
ESCAPING FROM NARROW LOCAL MINIMA,0.20833333333333334,"define the basin of attraction of a critical point of F.
92"
ESCAPING FROM NARROW LOCAL MINIMA,0.21022727272727273,"Definition 3.1. Let x∗be a critical point of F. The basin of atraction of x∗is defined to be the set
93"
ESCAPING FROM NARROW LOCAL MINIMA,0.21212121212121213,"W(x∗) such that
94"
ESCAPING FROM NARROW LOCAL MINIMA,0.21401515151515152,"W(x∗)def
= {x ∈Rp : ⟨∇F(x), x −x∗⟩≥0}.
Moreover, we write B(x∗) to be the largest ball contained within W(x∗), and r its radius.
95"
ESCAPING FROM NARROW LOCAL MINIMA,0.2159090909090909,"In the previous definition, if x∗is a saddle point, A(x∗) = {x∗} and r = 0.
96"
ESCAPING FROM NARROW LOCAL MINIMA,0.2178030303030303,"Theorem 3.1 (Escaping from narrow local minima). Let xt be the sequence of iterates defined by (2)
97"
ESCAPING FROM NARROW LOCAL MINIMA,0.2196969696969697,"and yt the sequence of iterates of gradient descent,
98"
ESCAPING FROM NARROW LOCAL MINIMA,0.2215909090909091,"yt+1 = yt −ηGD∇F(yt).
(4)"
ESCAPING FROM NARROW LOCAL MINIMA,0.22348484848484848,"Assume that xt ∈B(x∗) (resp. yt ∈B(x∗)) i.e. the ball contained in the basin of attraction of x∗,
99"
ESCAPING FROM NARROW LOCAL MINIMA,0.22537878787878787,"defined in Definition 3.1. Also, assume that xt (resp. yt) is not a critical point i.e. ∇F(xt) ̸= 0 (resp.
100"
ESCAPING FROM NARROW LOCAL MINIMA,0.22727272727272727,"∇F(yt) ̸= 0). If the stepsize is sufficiently large,
101"
ESCAPING FROM NARROW LOCAL MINIMA,0.22916666666666666,"ηSING ≥2r
√"
ESCAPING FROM NARROW LOCAL MINIMA,0.23106060606060605,"D
,
ηGD ≥
2r
∥∇F(yt)∥2
,
(5)"
ESCAPING FROM NARROW LOCAL MINIMA,0.23295454545454544,"then the iterates xt+1 (resp. yt+1) is outside the set B(x∗). See Figure 1 for an illustration.
102"
ESCAPING FROM NARROW LOCAL MINIMA,0.23484848484848486,"We see that GD struggles to escape local minima: under mild assumptions on ∇F, the closer yt is
103"
ESCAPING FROM NARROW LOCAL MINIMA,0.23674242424242425,"to x∗the higher the learning rate must be to escape from A(x∗). Indeed, for GD there is no finite
104"
ESCAPING FROM NARROW LOCAL MINIMA,0.23863636363636365,"step-size ηGD that guarantees escaping A(x∗). In contrast, Theorem 3.1 tells us that our algorithm
105"
ESCAPING FROM NARROW LOCAL MINIMA,0.24053030303030304,"escapes A(x∗) in a single step, provided the learning rate is sufficiently large. Furthermore, as the
106"
ESCAPING FROM NARROW LOCAL MINIMA,0.24242424242424243,"number of parameter tensors in the model increases, it becomes easier to escape from A(x∗). This is
107"
ESCAPING FROM NARROW LOCAL MINIMA,0.24431818181818182,"an important advantage of our algorithm over GD, especially for large models where the optimization
108"
ESCAPING FROM NARROW LOCAL MINIMA,0.24621212121212122,"landscape can be highly complex and difficult to navigate.
109"
ESCAPING FROM NARROW LOCAL MINIMA,0.2481060606060606,"When the Hessian at x∗is well conditioned, escaping from A(x∗) is roughly equivalent to escaping
110"
ESCAPING FROM NARROW LOCAL MINIMA,0.25,"from the local minimum. Therefore, it is crucial to use the highest possible learning rate. However,
111"
ESCAPING FROM NARROW LOCAL MINIMA,0.2518939393939394,"using a high learning rate can be problematic as the gradients are unstable and tend to oscillate leading
112"
ESCAPING FROM NARROW LOCAL MINIMA,0.2537878787878788,"to suboptimal convergence. To address this issue, several methods have been proposed to stabilize the
113"
ESCAPING FROM NARROW LOCAL MINIMA,0.2556818181818182,"gradients and allow for larger learning rates. Such methods include gradient centralization, LookA-
114"
ESCAPING FROM NARROW LOCAL MINIMA,0.25757575757575757,"head [54], different momentum strategies such as Adam [21], AdaBelief [57] or even AdaFactor
115"
ESCAPING FROM NARROW LOCAL MINIMA,0.25946969696969696,"[37] and larger batch sizes, among others. For this reason, the final implementation of our algorithm
116"
ESCAPING FROM NARROW LOCAL MINIMA,0.26136363636363635,"incorporated within AdamW features LookAhead and softplus calibration [40]. Note however that it
117"
ESCAPING FROM NARROW LOCAL MINIMA,0.26325757575757575,"does not introduce any additional hyper-parameters as the parameters of these stabilization methods
118"
ESCAPING FROM NARROW LOCAL MINIMA,0.26515151515151514,"are fixed once and for all.
119"
INVARIANCE PROPERTIES,0.26704545454545453,"3.2
Invariance properties
120"
INVARIANCE PROPERTIES,0.2689393939393939,"In this section, the setting is considered deterministic for simplicity. This section examines the
121"
INVARIANCE PROPERTIES,0.2708333333333333,"invariance properties of the technique.
122"
INVARIANCE PROPERTIES,0.2727272727272727,"Firstly, we show that a rescaling of the objective function,
123"
INVARIANCE PROPERTIES,0.2746212121212121,"min
x∈Rp ˜F(x)def
= αF (x) ,
α > 0.
(6)"
INVARIANCE PROPERTIES,0.2765151515151515,"does not affect the updates. This property is desirable as the network’s performance is unaffected
124"
INVARIANCE PROPERTIES,0.2784090909090909,"by a scaling of the loss. A similar invariance property applies to changes during training that cause
125"
INVARIANCE PROPERTIES,0.2803030303030303,"a rescaling of the gradients of a layer. If during training, the output of one layer of the network is
126"
INVARIANCE PROPERTIES,0.2821969696969697,"rescaled, it won’t affect the update of the previous layers, thus allievating part of the problem of
127"
INVARIANCE PROPERTIES,0.2840909090909091,"internal covariate shift [19].
128"
INVARIANCE PROPERTIES,0.2859848484848485,"Second, the algorithm presented in this paper preserves the mean i.e.
129"
INVARIANCE PROPERTIES,0.2878787878787879,"Pp
i=1[xt+1]i = Pp
i=1[xt]i,
(7)"
INVARIANCE PROPERTIES,0.2897727272727273,"where [x]i corresponds to the i-th component of the vector x.
130"
INVARIANCE PROPERTIES,0.2916666666666667,"Theorem 3.2. The iterates defined by (2) are invariant w.r.t. transformation (6), and preserve the
131"
INVARIANCE PROPERTIES,0.2935606060606061,"mean (7).
132"
INVARIANCE PROPERTIES,0.29545454545454547,"The property of preserving the mean has been demonstrated to improves the stability of the optimiza-
133"
INVARIANCE PROPERTIES,0.29734848484848486,"tion process in deep neural networks [49]. Moreover, it is motivated by the observation that many
134"
INVARIANCE PROPERTIES,0.29924242424242425,"non-linear layers demonstrate a mean-shift behavior [19], which alters their behavior based on the
135"
INVARIANCE PROPERTIES,0.30113636363636365,"sign of input values. This mean-shift behavior is mitigated by the presence of normalization layers,
136"
INVARIANCE PROPERTIES,0.30303030303030304,"that re-scale and shift the weights. Preserving the mean enhances the stability of the optimization
137"
INVARIANCE PROPERTIES,0.30492424242424243,"dynamics when normalization layers are present.
138"
INVARIANCE PROPERTIES,0.3068181818181818,"Furthermore, normalizing the centered gradients mitigates a potential pathological scenario where the
139"
INVARIANCE PROPERTIES,0.3087121212121212,"gradient signal is diminished. Indeed, the mean of the gradient can hinder the important signal when
140"
INVARIANCE PROPERTIES,0.3106060606060606,"the mean is too large compared to the centered gradient [49]. However, in such case the amplitude of
141"
INVARIANCE PROPERTIES,0.3125,"the centered gradient can be relatively small, preventing efficient updates. Normalizing the gradient
142"
INVARIANCE PROPERTIES,0.3143939393939394,"solves this issue by preserving its amplitude.
143"
CONVERGENCE,0.3162878787878788,"3.3
Convergence
144"
CONVERGENCE,0.3181818181818182,"In this section, two theorems of convergence are provided. In the first one, the normalization is
145"
CONVERGENCE,0.32007575757575757,"studied without the centralization. Under mild assumptions, we show the ℓ2-norm of the gradient
146"
CONVERGENCE,0.32196969696969696,"can be reduced to any desired precision. In the second one, we consider the full setting and show the
147"
CONVERGENCE,0.32386363636363635,"same result for the ϕ-norm (which is a pseudo-norm). We assume that the stochastic gradient has a
148"
CONVERGENCE,0.32575757575757575,"σ-bounded variance (σ > 0) i.e.
149"
CONVERGENCE,0.32765151515151514,"∀x ∈Rp, E

∥∇F(x) −∇f(x)∥2
2

≤σ2,
(8)"
CONVERGENCE,0.32954545454545453,"and the objective function F is positive and L-smooth,
150"
CONVERGENCE,0.3314393939393939,"∀x, y ∈Rd, ∥∇F(x) −∇F(y)∥2 ≤L∥x −y∥2.
(9)"
CONVERGENCE,0.3333333333333333,"Theorem 3.3 (Convergence without gradient centralization). Let assumptions (8) and (9) hold.
151"
CONVERGENCE,0.3352272727272727,Assume the gradient is computed across a mini-batch of size B = σ2
CONVERGENCE,0.3371212121212121,"ϵ2 . Let xt be the sequence of
152"
CONVERGENCE,0.3390151515151515,"iterates (2) with ϕ = I. Then, we have
153"
T,0.3409090909090909,"1
T"
T,0.3428030303030303,"T −1
X"
T,0.3446969696969697,"t=0
E[∥∇F(xt)∥2] ≤F(x0)"
T,0.3465909090909091,"ηT
+ (1 +
√"
T,0.3484848484848485,D)ϵ + ηLD
T,0.3503787878787879,"2
.
(10)"
T,0.3522727272727273,"If we set τ ∼U([[0, T −1]]), η = 2ϵ"
T,0.3541666666666667,L and T = LF (x0)
T,0.3560606060606061,"2ϵ2
, we obtain E[∥∇F(xτ)∥2] ≤(2 +
√"
T,0.35795454545454547,"D + D)ϵ.
154"
T,0.35984848484848486,"Therefore, the iteration complexity and computation complexity to achieve an ϵ-stationary point are
155"
T,0.36174242424242425,"O(1/ϵ2) and O(1/ϵ4), respectively.
156"
T,0.36363636363636365,"Theorem 3.4 (Convergence with gradient centralization). Let assumptions (8) and (9) hold. Assume
157"
T,0.36553030303030304,the gradient is computed across a mini-batch of size B = σ2
T,0.36742424242424243,"ϵ2 . Let xt be the sequence of iterates (2).
158"
T,0.3693181818181818,"Then we have
159"
T,0.3712121212121212,"1
T"
T,0.3731060606060606,"T −1
X"
T,0.375,"t=0
E[∥∇F(xt)∥ϕ] ≤F(x0)"
T,0.3768939393939394,"ηT
+ (1 +
√"
T,0.3787878787878788,D)ϵ + ηLD
T,0.3806818181818182,"2
,
(11)"
T,0.38257575757575757,"where ∥· ∥2
ϕ = ⟨·, ϕ(·)⟩2 is a pseudo-norm. If we set τ ∼U([[0, T −1]]), η = 2ϵ"
T,0.38446969696969696,L and T = LF (x0)
T,0.38636363636363635,"2ϵ2
,
160"
T,0.38825757575757575,"we obtain E[∥∇F(xτ)∥ϕ] ≤(2 +
√"
T,0.39015151515151514,"D + D)ϵ. Therefore, the iteration complexity and computation
161"
T,0.39204545454545453,"complexity to achieve an (ϵ, ϕ)-stationary point are O(1/ϵ2) and O(1/ϵ4), respectively.
162"
T,0.3939393939393939,"Note that Theorem 3.4 only gives us an (ϵ, ϕ)-stationary point i.e. in the limit ϵ →0, ∇F(xτ)
163"
T,0.3958333333333333,"converges to a point in Ker(ϕ) = Span((1, . . . , 1)T ). Indeed, applying ϕ to the gradients amounts to
164"
T,0.3977272727272727,"do a projected gradient descent onto the set of weights with the same mean as the initial weights.
165"
T,0.3996212121212121,"We argue that this “weaker” convergence result it is not problematic. Reaching a point in Ker(ϕ)
166"
T,0.4015151515151515,"means that the optimization process cannot go any further without violating the constraint. However,
167"
T,0.4034090909090909,"since neural networks have lots of parameters, adding one constraint to the solution is not likely to
168"
T,0.4053030303030303,"lead to worse performance [49].
169"
RELATED WORK,0.4071969696969697,"4
Related Work
170"
RELATED WORK,0.4090909090909091,"The most-used optimizer nowadays is Adam [21], which computes the entrywise first and second
171"
RELATED WORK,0.4109848484848485,"order moments of the gradient, and uses them to adaptively normalize the gradient. In contrast, SING
172"
RELATED WORK,0.4128787878787879,"first removes to the gradient its mean and divides it by its norm (standardization) prior to any further
173"
RELATED WORK,0.4147727272727273,"computations at the layer level. Furthermore, in Adam the first and second orders are averaged
174"
RELATED WORK,0.4166666666666667,"temporally while ours is not. Numerous variants of Adam have been proposed, mainly focusing on
175"
RELATED WORK,0.4185606060606061,"stabilizing the iterations of Adam. Notably, the popular AdamW [26] optimizer corrects how weight
176"
RELATED WORK,0.42045454545454547,"decay is applied in Adam, yielding a more robust method training larger models, for instance for
177"
RELATED WORK,0.42234848484848486,"SGD
AdamW [26]
W+GC [49]
AdaBelief [57]
W+SING"
RELATED WORK,0.42424242424242425,"ImageNet
ResNet18
72.44%
72.58%
72.31%
72.49%
72.93%
ImageNet
ResNet34
75.36%
75.29%
75.26%
75.49%
75.67%"
RELATED WORK,0.42613636363636365,"CIFAR100
ResNet18
75.63%
77.95%
-
-
78.24%"
RELATED WORK,0.42803030303030304,"Table 1: Top-1 accuracy classification results of ResNet [17] on CIFAR100 [22] and ImageNet [10]
when trained from scratch. W+GC stands for the combination of AdamW [26] and Gradient
Centralization [49] and W+SING stands for AdamW and SING. For CIFAR100, the results are
averaged across five runs. The standard deviations (reported in the appendix) are sufficiently low to
say that W+SING has a clear edge on AdamW. We do not report the mean and standard deviation on
ImageNet-1K because we only launched each experiment once due to computational constraints but
we can reasonably expect our results to be significant (see Figure 3 for a motivation)."
RELATED WORK,0.42992424242424243,"0
20
40
60
80"
RELATED WORK,0.4318181818181818,Epochs 10 20 30 40 50 60 70
RELATED WORK,0.4337121212121212,"T
op-1 Accuracy"
RELATED WORK,0.4356060606060606,W+SING AdamW
RELATED WORK,0.4375,"Figure 3: Evolution of the accuracy throughout
training on ImageNet-1K with a ResNet18. The
almost-periodic oscillation of the metric is typical
of SING, and could be explained by the large
steps taken by the optimizer. As illustrated in
Figure 1, at the beginning the learning rate is very
high to avoid local minima and is slowly reduced
to reach convergence."
RELATED WORK,0.4393939393939394,"0
20
40
60
80
100"
RELATED WORK,0.4412878787878788,Epochs 10 −4 10 −3 10 −2 10 −1 10 0 10 1
RELATED WORK,0.4431818181818182,"T
rain loss"
RELATED WORK,0.44507575757575757,"W+SING, ViT
-S"
RELATED WORK,0.44696969696969696,"W+SING, ViT
-B"
RELATED WORK,0.44886363636363635,"AdamW, ViT
-S"
RELATED WORK,0.45075757575757575,"AdamW, ViT
-B"
RELATED WORK,0.45265151515151514,"Figure 4: An illustration of a failure case of Adam
when trained on ViT-small. The train loss sud-
denly spikes during training, reducing the final
performance. The learning rate scheduler is a
cosine decay, hence the learning rate is already
small when the explosion occurs."
RELATED WORK,0.45454545454545453,"training (visual) Transformers in practice [41]. Also in the panel of corrections to Adam, RAdam [24]
178"
RELATED WORK,0.4564393939393939,"proposes to fix the variance of adaptive learning rate by rewriting the update term, AdaBound [27]
179"
RELATED WORK,0.4583333333333333,"and AdaMod [12] clip the update, and AdaNorm [14] directly clips the gradient instead. Conversely,
180"
RELATED WORK,0.4602272727272727,"EAdam [52] and SAdam [40] target improving the ϵ term in the denominator. AdaBelief [57] is
181"
RELATED WORK,0.4621212121212121,"another variant of Adam. It computes an estimate of the standard deviation instead of the second order
182"
RELATED WORK,0.4640151515151515,"moment. AdaFactor [37] factorizes the elements of the gradient to reduce the memory consumption
183"
RELATED WORK,0.4659090909090909,"of the optimizer. The authors propose as well an analysis of the instabilities of Adam, and fixes. In
184"
RELATED WORK,0.4678030303030303,"this work, we also target reducing Adam’s instabilities via gradient standardization.
185"
RELATED WORK,0.4696969696969697,"The works most closely related to ours are LARS [50] and LAMB [51]. Indeed, both optimizers
186"
RELATED WORK,0.4715909090909091,"normalize the gradient in a layer-wise fashion like us. However, both methods multiply the normalized
187"
RELATED WORK,0.4734848484848485,"gradient by the weight norm. This multiplication is undesired in our case as it would tame down
188"
RELATED WORK,0.4753787878787879,"our main theoretical result in Section 3 (Theorem 3.1) which is central to our work. Indeed, this
189"
RELATED WORK,0.4772727272727273,"theorem is the keystone to building a stable optimizer able to escape from narrow local minima using
190"
RELATED WORK,0.4791666666666667,"larger learning rates, whereas these methods leverage very large batch size to improve performance.
191"
RELATED WORK,0.4810606060606061,"Additionally, our method is hyperparameter-free in contrast to those of [50, 51]. Furthermore, these
192"
RELATED WORK,0.48295454545454547,"methods are new optimizers to be used as a replacement for Adam(W) whereas SING is a technique
193"
RELATED WORK,0.48484848484848486,"that can be used within any optimizer.
194"
RELATED WORK,0.48674242424242425,"Other approaches leverage standardization to better train neural networks: Weight Standardiza-
195"
RELATED WORK,0.48863636363636365,"tion [34] and Weight Normalization [18, 36] parameterize the weights of the network to allow for a
196"
RELATED WORK,0.49053030303030304,"smoother training. While this affects the gradients, this approach is orthogonal to ours and could be
197"
RELATED WORK,0.49242424242424243,"used with our technique.
198"
RELATED WORK,0.4943181818181818,"Maximum LR
AdamW [26]
AdamW + SING"
RELATED WORK,0.4962121212121212,"ViT-S [13]
0.05
78.13%
96.56%
ViT-S [13] (± Normalization)
NC
93.00% (+14.87%)
NC
ViT-S [13] (± GC [49])
0.01 (1/5)
77.46% (-0.67%)
93.86% (-2.70%)
ViT-S [13] (± LookAhead [54])
0.01 (1/5)
54.79% (-23.35%)
95.63% (-0.93%)
ViT-S [13] (± Softplus [40])
0.005 (1/10)
NC
89.38% (-7.18%)"
RELATED WORK,0.4981060606060606,"ViT-B [13]
0.05
NC
97.15%"
RELATED WORK,0.5,"Table 2: Ablation study of the different components of SING using a ViT-S on RDE. The reported
metric is the accuracy. Each component allow for a higher learning rate. For AdamW, we added
the component and studied the convergence. For AdamW + SING, we removed it. As the theory
suggests, the higher the learning rate, the higher the final performance. NC stands for no convergence
i.e. the loss could not be stabilized throughout the iterations. The maximum LR reported corresponds
to the one for AdamW + SING. As displayed in Figure 4, the training of ViT-B spiked resulting in
irrelevant performance. Note that for AdamW, the very unstable nature of the training largely widens
the gaps in performance when adding a component. In all cases, the best learning rate using AdamW
alone was 10−3."
RELATED WORK,0.5018939393939394,"Another part of the literature focuses on improving the stability of training processes to ensure
199"
RELATED WORK,0.5037878787878788,"smoother convergence. Notably, techniques such as LookAhead [54] adopt an approach where
200"
RELATED WORK,0.5056818181818182,"weights computed over the previous k iterations are averaged. Similarly, Gradient Centralization [49]
201"
RELATED WORK,0.5075757575757576,"involves subtracting the mean of the gradient, effectively reducing its ℓ2 norm. In our work, we draw
202"
RELATED WORK,0.509469696969697,"upon these techniques, but it is important to highlight that our approach is distinct and independent
203"
RELATED WORK,0.5113636363636364,"from this line of research.
204"
RELATED WORK,0.5132575757575758,"Lastly, it is common in non-convex optimization to normalize the gradient descent algorithm [8, 30,
205"
RELATED WORK,0.5151515151515151,"56]. This line of work supports that the standardization strategies is a simple way to find a better
206"
RELATED WORK,0.5170454545454546,"minimizer. In this work, we translate this strategy to deep learning.
207"
EXPERIMENTS,0.5189393939393939,"5
Experiments
208"
EXPERIMENTS,0.5208333333333334,"In this section, we evaluate SING on classification, depth estimation and natural language processing.
209"
EXPERIMENTS,0.5227272727272727,"We run all the experiments on a single Tesla V100 GPU with 32GB of VRAM. The code to reproduce
210"
EXPERIMENTS,0.5246212121212122,"the results will be made available upon publication.
211"
IMAGE CLASSIFICATION,0.5265151515151515,"5.1
Image classification
212"
IMAGE CLASSIFICATION,0.5284090909090909,"We evaluate our technique on the large-scale ImageNet-1K dataset [10] which consists of 1.28 million
213"
IMAGE CLASSIFICATION,0.5303030303030303,"images for training and 50K images for validation from 1000 categories. We use the FFCV library
214"
IMAGE CLASSIFICATION,0.5321969696969697,"[23] and its recipe: the data augmentation consists in random horizontal flips and random resized
215"
IMAGE CLASSIFICATION,0.5340909090909091,"crops. Notably, the downsampling layers are replaced by BlurPool [55]. The size of the images is
216"
IMAGE CLASSIFICATION,0.5359848484848485,"192 × 192 during training and 224 × 224 at evaluation [42]. Our networks are trained for 88 epochs
217"
IMAGE CLASSIFICATION,0.5378787878787878,"with a batch size of 1024. The loss to be minimized is the cross-entropy with a label smoothing [38]
218"
IMAGE CLASSIFICATION,0.5397727272727273,"of 0.1. For all networks, there is a 5-epoch linear warmup and a cosine decaying schedule afterward.
219"
IMAGE CLASSIFICATION,0.5416666666666666,"We carefully design our hyper-parameter tuning strategy to ensure a fair comparison. First, we
220"
IMAGE CLASSIFICATION,0.5435606060606061,"tune the learning rate among limited values: {5 × 10−4, 10−3, 5 × 10−3, 10−2} for AdamW and
221"
IMAGE CLASSIFICATION,0.5454545454545454,"{5 × 10−3, 10−2, 5 × 10−2, 10−1} for SING used within AdamW. In the rare cases where the best
222"
IMAGE CLASSIFICATION,0.5473484848484849,"learning rate found is one of the extreme value of the set, additional learning rates were tested. For
223"
IMAGE CLASSIFICATION,0.5492424242424242,"all networks and optimizers the best learning rate found is the last one before the training explodes.
224"
IMAGE CLASSIFICATION,0.5511363636363636,"Then, we tune the weight decay using the best learning rate found. The values assessed for the weight
225"
IMAGE CLASSIFICATION,0.553030303030303,"decay are {5 × 10−4, 5 × 10−3, 5 × 10−2, 5 × 10−1}. Finally, the results are reported in Table 1. We
226"
IMAGE CLASSIFICATION,0.5549242424242424,"notice that SING combined with AdamW systematically outperforms AdamW. The evolution of the
227"
IMAGE CLASSIFICATION,0.5568181818181818,"accuracy throughout training can be seen in Figure 3. SING seems to outperform AdamW during
228"
IMAGE CLASSIFICATION,0.5587121212121212,"the entire training, but seem to loose its edge at the end of the training. We leave the study of this
229"
IMAGE CLASSIFICATION,0.5606060606060606,"phenomena for future works.
230"
IMAGE CLASSIFICATION,0.5625,"SGD
AdamW [26]
AdaBelief [57]
AdaFactor [37]"
IMAGE CLASSIFICATION,0.5643939393939394,"w/o SING
0.25%
78.13%
60.26%
74.98%
w/ SING
94.25% (+94.23%)
96.56% (+18.43%)
96.70% (+36.44%)
76.26% (+1.28%)"
IMAGE CLASSIFICATION,0.5662878787878788,"Table 3: Combination of SING with other optimizers for training a ViT-S [13] model on RDE. Note
that SGD barely works on this task and model despite the hyper-parameter tuning. We argue that its
performance could be further improved by tuning the momentum hyper-parameter. See the Appendix
for more details. We notice that for three out of four optimizers, incorporating SING helps improve
the performance. See the Appendix for more details."
IMAGE CLASSIFICATION,0.5681818181818182,"Additionally, we trained a ResNet18 on CIFAR100 [22], which consists of 50K training images and
231"
IMAGE CLASSIFICATION,0.5700757575757576,"10K testing images from 100 classes. The images are of size 32 × 32. The network was trained for
232"
IMAGE CLASSIFICATION,0.571969696969697,"300 epochs using a batch size of 128. The learning rate scheduler and the tuning strategy are the same
233"
IMAGE CLASSIFICATION,0.5738636363636364,"than for ImageNet. The results are visible in Table 1. We see that even in this challenging setting, the
234"
IMAGE CLASSIFICATION,0.5757575757575758,"combination of AdamW and SING outperforms AdamW and SGD.
235"
DEPTH ESTIMATION,0.5776515151515151,"5.2
Depth Estimation
236"
DEPTH ESTIMATION,0.5795454545454546,"In this section, we investigate the performance of our optimizer on a depth estimation task using a
237"
DEPTH ESTIMATION,0.5814393939393939,"synthetic dataset. The RDE dataset [7] consists of 50K 128 × 128 images of rectangles randomly
238"
DEPTH ESTIMATION,0.5833333333333334,"placed within an image. Depth naturally arises as rectangles are placed on top of each other. The
239"
DEPTH ESTIMATION,0.5852272727272727,"goal is to predict the depth for each pixel in the image, depending on which rectangle it belongs too.
240"
DEPTH ESTIMATION,0.5871212121212122,"This task is interesting because although there exists a simple algorithm that computes the desired
241"
DEPTH ESTIMATION,0.5890151515151515,"depth with 100% accuracy, neural networks struggle to get good performance. Notably, we found
242"
DEPTH ESTIMATION,0.5909090909090909,"training a ViT-small [13] on this task in an image-to-image fashion to be particularly challenging
243"
DEPTH ESTIMATION,0.5928030303030303,"using AdamW. For usual learning rates, the loss spikes randomly during training, largely lowering
244"
DEPTH ESTIMATION,0.5946969696969697,"the final performance. See Figure 4 for more details. For very small learning rates, the training
245"
DEPTH ESTIMATION,0.5965909090909091,"loss doesn’t decrease fast enough to get results in a reasonable amount of time. In this case, we
246"
DEPTH ESTIMATION,0.5984848484848485,"found using SING with AdamW to be a good choice as the normalization prevents the gradient from
247"
DEPTH ESTIMATION,0.6003787878787878,"exploding during training. As a result, the combination of AdamW and SING outperformed AdamW
248"
DEPTH ESTIMATION,0.6022727272727273,"by a large margin. The larger the assessed model, the worse the instabilities. ViT-big [13] does not
249"
DEPTH ESTIMATION,0.6041666666666666,"converge when using AdamW. We tried several sets of hyper-parameters to draw this conclusion.
250"
DEPTH ESTIMATION,0.6060606060606061,"We used the same hyper-parameter tuning strategy and learning rate scheduler as for ImageNet-1K.
251"
DEPTH ESTIMATION,0.6079545454545454,"The network was trained for 100 epochs using a batch size of 512. The loss we minimized was
252"
DEPTH ESTIMATION,0.6098484848484849,"the MSE. See Table 2 for the results and an ablation study. The ablation study shows that each
253"
DEPTH ESTIMATION,0.6117424242424242,"component of SING helps to achieve a higher learning rate and therefore higher performance. Notably,
254"
DEPTH ESTIMATION,0.6136363636363636,"softplus [40] seems to largely help SING while it is detrimental for AdamW. The normalization seems
255"
DEPTH ESTIMATION,0.615530303030303,"to be a determining factor for reaching convergence although it does not fully explain the success of
256"
DEPTH ESTIMATION,0.6174242424242424,"SING. We also studied the impact of SING when combined with other optimizers. The results are
257"
DEPTH ESTIMATION,0.6193181818181818,"visible in Table 3. We used all methods with their default hyper-parameters except for SGD where
258"
DEPTH ESTIMATION,0.6212121212121212,"we tried different values for the momentum. We see that for three out of the four assessed optimizers,
259"
DEPTH ESTIMATION,0.6231060606060606,"the variant with SING significantly outperforms its counterpart. For AdaFactor [37] there is barely
260"
DEPTH ESTIMATION,0.625,"any performance gain. We claim this is due to the numerous tweaks within the optimizer that have
261"
DEPTH ESTIMATION,0.6268939393939394,"been tailored for a gradient descent without SING.
262"
NATURAL LANGUAGE PROCESSING,0.6287878787878788,"5.3
Natural language processing
263"
NATURAL LANGUAGE PROCESSING,0.6306818181818182,"In this section, we evaluate the performance of our optimizer on natural language processing tasks.
264"
NATURAL LANGUAGE PROCESSING,0.6325757575757576,"First, we trained a Transformer with pre-norm convention [45] on the IWSLT14 German-to-English
265"
NATURAL LANGUAGE PROCESSING,0.634469696969697,"(De-En) dataset [4] using the FAIRSEQ [31] library. We used the code of AdaHessian [48] as is but
266"
NATURAL LANGUAGE PROCESSING,0.6363636363636364,"surprisingly we were not able to reproduce the results reported for AdamW. Instead, we used the
267"
NATURAL LANGUAGE PROCESSING,0.6382575757575758,"hyper-parameters reported in [46] and found them to be better, but still below the announced results.
268"
NATURAL LANGUAGE PROCESSING,0.6401515151515151,"Then, we used Hugging Face TRANSFORMERS library [44] to fine-tune Bert [11] on the SQuAD
269"
NATURAL LANGUAGE PROCESSING,0.6420454545454546,"dataset [35] and RoBERTa on SWAG [53]. The results are reported in Table 4. In all cases, the
270"
NATURAL LANGUAGE PROCESSING,0.6439393939393939,"combination of AdamW and SING outperforms a well-tuned AdamW.
271"
NATURAL LANGUAGE PROCESSING,0.6458333333333334,"AdamW [26]
AdamW + SING"
NATURAL LANGUAGE PROCESSING,0.6477272727272727,"IWSLT14
From scratch
Transformer [43]
34.76
35.41"
NATURAL LANGUAGE PROCESSING,0.6496212121212122,"SQuAD1
Fine-tuning
Bert [11]
80.53% / 88.39%
81.00% / 88.34%"
NATURAL LANGUAGE PROCESSING,0.6515151515151515,"SWAG1
Fine-tuning
RoBERTa [25]
80.45%
83.33%"
NATURAL LANGUAGE PROCESSING,0.6534090909090909,"Table 4: First line: BLUE score on the IWSL14 task, when training a SMALL Transformer [31] from
scratch. Second line: Fine-tuning results on the SQuAD dataset [35]; the reported values are the
proportion of exact matches and the F1 score. Third line: Fine-tuning results on the SWAG dataset
[53]; the reported value is the accuracy."
NATURAL LANGUAGE PROCESSING,0.6553030303030303,"We noticed that the gradient norm was increasing throughout training. After investigation, it turned
272"
NATURAL LANGUAGE PROCESSING,0.6571969696969697,"out the culprits were the weights and biases of the Layer Normalization [2] layers. We decided to
273"
NATURAL LANGUAGE PROCESSING,0.6590909090909091,"disable the learning of these weights and found the performance of both optimizers to be improved.
274"
NATURAL LANGUAGE PROCESSING,0.6609848484848485,"We claim doing so is not problematic in practice as disabling the learning of these parameters have
275"
NATURAL LANGUAGE PROCESSING,0.6628787878787878,"been pointed out as beneficial in the literature [47].
276"
LIMITATIONS,0.6647727272727273,"6
Limitations
277"
LIMITATIONS,0.6666666666666666,"We tried SING on other tasks such as image denoising, where the models trained with SING attained
278"
LIMITATIONS,0.6685606060606061,"the same performance of AdamW, but did not result in improved results. This suggests SING’s
279"
LIMITATIONS,0.6704545454545454,"effectiveness can vary depending on the task and architecture. Additionally, we found our optimizer to
280"
LIMITATIONS,0.6723484848484849,"not work well when used in conjunction with LayerNorm [2] or LayerScale [41]. While a simple fix
281"
LIMITATIONS,0.6742424242424242,"is to disable the learning of these weights, it raises the question of why and calls for a better solution.
282"
LIMITATIONS,0.6761363636363636,"Finally, we propose a convergence proof of the iterates defined in 2, which does not incorporate
283"
LIMITATIONS,0.678030303030303,"AdamW even though we mainly used their combination in the paper.
284"
CONCLUSION,0.6799242424242424,"7
Conclusion
285"
CONCLUSION,0.6818181818181818,"We introduced SING, a plug-and-play technique that improves the stability and generalization of the
286"
CONCLUSION,0.6837121212121212,"Adam(W) optimizer in deep learning, and can be used with any optimizer. By leveraging layer-wise
287"
CONCLUSION,0.6856060606060606,"gradient standardization, SING enhances the performance of the optimizer without introducing addi-
288"
CONCLUSION,0.6875,"tional hyperparameters. Extensive experimentation across various tasks demonstrates its effectiveness
289"
CONCLUSION,0.6893939393939394,"compared to the original AdamW optimizer. Theoretical analysis reveals that SING enables the
290"
CONCLUSION,0.6912878787878788,"optimizer to escape narrow local minima within a single step, with the required learning rate inversely
291"
CONCLUSION,0.6931818181818182,"proportional to the network’s depth. This compatibility with deep neural networks highlights its
292"
CONCLUSION,0.6950757575757576,"practicality. The analysis also provides valuable insights into the behavior of SING, such as its
293"
CONCLUSION,0.696969696969697,"convergence rate or stability, and its advantages over traditional optimization techniques.
294"
CONCLUSION,0.6988636363636364,"In conclusion, our proposed SING technique offers a practical and effective upgrade to the Adam(W)
295"
CONCLUSION,0.7007575757575758,"optimizer in deep learning. By enhancing stability and generalization, it contributes to the improve-
296"
CONCLUSION,0.7026515151515151,"ment of optimization algorithms in neural network training. The results of our research, combined
297"
CONCLUSION,0.7045454545454546,"with the theoretical analysis, open avenues for further exploration, including investigating the com-
298"
CONCLUSION,0.7064393939393939,"patibility of SING with other optimization frameworks and addressing the challenges associated with
299"
CONCLUSION,0.7083333333333334,"specific normalization techniques.
300"
CONCLUSION,0.7102272727272727,"1We took the code of Hugging Face (https://huggingface.co/transformers/v2.3.0/examples.html) as is and
launched it, and found the performance to be lower than announced. More details in Appendix."
REFERENCES,0.7121212121212122,"References
301"
REFERENCES,0.7140151515151515,"[1] Abien Fred Agarap.
Deep learning using rectified linear units (relu).
arXiv preprint
302"
REFERENCES,0.7159090909090909,"arXiv:1803.08375, 2018.
303"
REFERENCES,0.7178030303030303,"[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
304"
REFERENCES,0.7196969696969697,"arXiv:1607.06450, 2016.
305"
REFERENCES,0.7215909090909091,"[3] Sunitha Basodi, Chunyan Ji, Haiping Zhang, and Yi Pan. Gradient amplification: An efficient
306"
REFERENCES,0.7234848484848485,"way to train deep neural networks. Big Data Mining and Analytics, 3(3):196–207, 2020.
307"
REFERENCES,0.7253787878787878,"[4] Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa Bentivogli, and Marcello Federico. Report
308"
REFERENCES,0.7272727272727273,"on the 11th iwslt evaluation campaign. In Proceedings of the 11th International Workshop on
309"
REFERENCES,0.7291666666666666,"Spoken Language Translation: Evaluation Campaign, pages 2–17, 2014.
310"
REFERENCES,0.7310606060606061,"[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
311"
REFERENCES,0.7329545454545454,"Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
312"
REFERENCES,0.7348484848484849,"Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
313"
REFERENCES,0.7367424242424242,"[6] Yaim Cooper. The loss landscape of overparameterized neural networks. arXiv preprint
314"
REFERENCES,0.7386363636363636,"arXiv:1804.10200, 2018.
315"
REFERENCES,0.740530303030303,"[7] Adrien Courtois, Jean-Michel Morel, and Pablo Arias. Investigating neural architectures by
316"
REFERENCES,0.7424242424242424,"synthetic dataset design. In Proceedings of the IEEE/CVF Conference on Computer Vision and
317"
REFERENCES,0.7443181818181818,"Pattern Recognition, pages 4890–4899, 2022.
318"
REFERENCES,0.7462121212121212,"[8] Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International
319"
REFERENCES,0.7481060606060606,"conference on machine learning, pages 2260–2268. PMLR, 2020.
320"
REFERENCES,0.75,"[9] Alexandre Défossez, Léon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence
321"
REFERENCES,0.7518939393939394,"proof of adam and adagrad. arXiv preprint arXiv:2003.02395, 2020.
322"
REFERENCES,0.7537878787878788,"[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
323"
REFERENCES,0.7556818181818182,"scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
324"
REFERENCES,0.7575757575757576,"recognition, pages 248–255. Ieee, 2009.
325"
REFERENCES,0.759469696969697,"[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
326"
REFERENCES,0.7613636363636364,"deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
327"
REFERENCES,0.7632575757575758,"2018.
328"
REFERENCES,0.7651515151515151,"[12] Jianbang Ding, Xuancheng Ren, Ruixuan Luo, and Xu Sun. An adaptive and momental bound
329"
REFERENCES,0.7670454545454546,"method for stochastic learning. arXiv preprint arXiv:1910.12249, 2019.
330"
REFERENCES,0.7689393939393939,"[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
331"
REFERENCES,0.7708333333333334,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
332"
REFERENCES,0.7727272727272727,"An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
333"
REFERENCES,0.7746212121212122,"arXiv:2010.11929, 2020.
334"
REFERENCES,0.7765151515151515,"[14] Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Adanorm: Adaptive
335"
REFERENCES,0.7784090909090909,"gradient norm correction based optimizer for cnns. In Proceedings of the IEEE/CVF Winter
336"
REFERENCES,0.7803030303030303,"Conference on Applications of Computer Vision, pages 5284–5293, 2023.
337"
REFERENCES,0.7821969696969697,"[15] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural
338"
REFERENCES,0.7840909090909091,"network optimization problems. arXiv preprint arXiv:1412.6544, 2014.
339"
REFERENCES,0.7859848484848485,"[16] Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and flat local
340"
REFERENCES,0.7878787878787878,"minima. Advances in neural information processing systems, 32, 2019.
341"
REFERENCES,0.7897727272727273,"[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
342"
REFERENCES,0.7916666666666666,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
343"
REFERENCES,0.7935606060606061,"pages 770–778, 2016.
344"
REFERENCES,0.7954545454545454,"[18] Lei Huang, Xianglong Liu, Yang Liu, Bo Lang, and Dacheng Tao. Centered weight normaliza-
345"
REFERENCES,0.7973484848484849,"tion in accelerating training of deep neural networks. In Proceedings of the IEEE International
346"
REFERENCES,0.7992424242424242,"Conference on Computer Vision, pages 2803–2811, 2017.
347"
REFERENCES,0.8011363636363636,"[19] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
348"
REFERENCES,0.803030303030303,"by reducing internal covariate shift. In International conference on machine learning, pages
349"
REFERENCES,0.8049242424242424,"448–456. pmlr, 2015.
350"
REFERENCES,0.8068181818181818,"[20] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
351"
REFERENCES,0.8087121212121212,"Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.
352"
REFERENCES,0.8106060606060606,"arXiv preprint arXiv:1609.04836, 2016.
353"
REFERENCES,0.8125,"[21] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
354"
REFERENCES,0.8143939393939394,"Bengio and Yann LeCun, editors, Int. Conf. on Learning Representations, 2015.
355"
REFERENCES,0.8162878787878788,"[22] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images,
356"
REFERENCES,0.8181818181818182,"2009.
357"
REFERENCES,0.8200757575757576,"[23] Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and
358"
REFERENCES,0.821969696969697,"Aleksander Madry.
FFCV: Accelerating training by removing data bottlenecks.
https:
359"
REFERENCES,0.8238636363636364,"//github.com/libffcv/ffcv/, 2022. commit 2544abd.
360"
REFERENCES,0.8257575757575758,"[24] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao,
361"
REFERENCES,0.8276515151515151,"and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint
362"
REFERENCES,0.8295454545454546,"arXiv:1908.03265, 2019.
363"
REFERENCES,0.8314393939393939,"[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
364"
REFERENCES,0.8333333333333334,"Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
365"
REFERENCES,0.8352272727272727,"approach. arXiv preprint arXiv:1907.11692, 2019.
366"
REFERENCES,0.8371212121212122,"[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
367"
REFERENCES,0.8390151515151515,"arXiv:1711.05101, 2017.
368"
REFERENCES,0.8409090909090909,"[27] Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
369"
REFERENCES,0.8428030303030303,"bound of learning rate. arXiv preprint arXiv:1902.09843, 2019.
370"
REFERENCES,0.8446969696969697,"[28] Agnes Lydia and Sagayaraj Francis. Adagrad—an optimizer for stochastic gradient descent. Int.
371"
REFERENCES,0.8465909090909091,"J. Inf. Comput. Sci, 6(5):566–568, 2019.
372"
REFERENCES,0.8484848484848485,"[29] Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal,
373"
REFERENCES,0.8503787878787878,"Punit Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, et al. A theory on adam
374"
REFERENCES,0.8522727272727273,"instability in large-scale machine learning. arXiv preprint arXiv:2304.09871, 2023.
375"
REFERENCES,0.8541666666666666,"[30] Ryan Murray, Brian Swenson, and Soummya Kar. Revisiting normalized gradient descent: Fast
376"
REFERENCES,0.8560606060606061,"evasion of saddle points. IEEE Transactions on Automatic Control, 64(11):4818–4824, 2019.
377"
REFERENCES,0.8579545454545454,"[31] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
378"
REFERENCES,0.8598484848484849,"and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint
379"
REFERENCES,0.8617424242424242,"arXiv:1904.01038, 2019.
380"
REFERENCES,0.8636363636363636,"[32] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent
381"
REFERENCES,0.865530303030303,"neural networks. In International conference on machine learning, pages 1310–1318. Pmlr,
382"
REFERENCES,0.8674242424242424,"2013.
383"
REFERENCES,0.8693181818181818,"[33] Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random
384"
REFERENCES,0.8712121212121212,"matrix theory. In International Conference on Machine Learning, pages 2798–2806. PMLR,
385"
REFERENCES,0.8731060606060606,"2017.
386"
REFERENCES,0.875,"[34] Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Micro-batch training with
387"
REFERENCES,0.8768939393939394,"batch-channel normalization and weight standardization. arXiv preprint arXiv:1903.10520,
388"
REFERENCES,0.8787878787878788,"2019.
389"
REFERENCES,0.8806818181818182,"[35] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions
390"
REFERENCES,0.8825757575757576,"for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.
391"
REFERENCES,0.884469696969697,"[36] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to
392"
REFERENCES,0.8863636363636364,"accelerate training of deep neural networks. Advances in neural information processing systems,
393"
REFERENCES,0.8882575757575758,"29, 2016.
394"
REFERENCES,0.8901515151515151,"[37] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory
395"
REFERENCES,0.8920454545454546,"cost. In International Conference on Machine Learning, pages 4596–4604. PMLR, 2018.
396"
REFERENCES,0.8939393939393939,"[38] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-
397"
REFERENCES,0.8958333333333334,"thinking the inception architecture for computer vision. In Proceedings of the IEEE conference
398"
REFERENCES,0.8977272727272727,"on computer vision and pattern recognition, pages 2818–2826, 2016.
399"
REFERENCES,0.8996212121212122,"[39] T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop, coursera: Neural networks for machine
400"
REFERENCES,0.9015151515151515,"learning. Technical report, 2012.
401"
REFERENCES,0.9034090909090909,"[40] Qianqian Tong, Guannan Liang, and Jinbo Bi. Calibrating the adaptive learning rate to improve
402"
REFERENCES,0.9053030303030303,"convergence of adam. Neurocomputing, 481:333–356, 2022.
403"
REFERENCES,0.9071969696969697,"[41] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Go-
404"
REFERENCES,0.9090909090909091,"ing deeper with image transformers. In Proceedings of the IEEE/CVF International Conference
405"
REFERENCES,0.9109848484848485,"on Computer Vision, pages 32–42, 2021.
406"
REFERENCES,0.9128787878787878,"[42] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test
407"
REFERENCES,0.9147727272727273,"resolution discrepancy. Advances in neural information processing systems, 32, 2019.
408"
REFERENCES,0.9166666666666666,"[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
409"
REFERENCES,0.9185606060606061,"Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-
410"
REFERENCES,0.9204545454545454,"tion Processing Systems, 2017.
411"
REFERENCES,0.9223484848484849,"[44] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
412"
REFERENCES,0.9242424242424242,"Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
413"
REFERENCES,0.9261363636363636,"Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
414"
REFERENCES,0.928030303030303,"Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-
415"
REFERENCES,0.9299242424242424,"art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods
416"
REFERENCES,0.9318181818181818,"in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020.
417"
REFERENCES,0.9337121212121212,"Association for Computational Linguistics.
418"
REFERENCES,0.9356060606060606,"[45] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,
419"
REFERENCES,0.9375,"Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.
420"
REFERENCES,0.9393939393939394,"In International Conference on Machine Learning, pages 10524–10533. PMLR, 2020.
421"
REFERENCES,0.9412878787878788,"[46] Haoran Xu, Benjamin Van Durme, and Kenton Murray. Bert, mbert, or bibert? a study on
422"
REFERENCES,0.9431818181818182,"contextualized embeddings for neural machine translation. arXiv preprint arXiv:2109.04588,
423"
REFERENCES,0.9450757575757576,"2021.
424"
REFERENCES,0.946969696969697,"[47] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and
425"
REFERENCES,0.9488636363636364,"improving layer normalization. Advances in Neural Information Processing Systems, 32, 2019.
426"
REFERENCES,0.9507575757575758,"[48] Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney.
427"
REFERENCES,0.9526515151515151,"Adahessian: An adaptive second order optimizer for machine learning. In proceedings of the
428"
REFERENCES,0.9545454545454546,"AAAI conference on artificial intelligence, volume 35, pages 10665–10673, 2021.
429"
REFERENCES,0.9564393939393939,"[49] Hongwei Yong, Jianqiang Huang, Xiansheng Hua, and Lei Zhang. Gradient centralization:
430"
REFERENCES,0.9583333333333334,"A new optimization technique for deep neural networks. In Computer Vision–ECCV 2020:
431"
REFERENCES,0.9602272727272727,"16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, pages
432"
REFERENCES,0.9621212121212122,"635–652. Springer, 2020.
433"
REFERENCES,0.9640151515151515,"[50] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks.
434"
REFERENCES,0.9659090909090909,"arXiv preprint arXiv:1708.03888, 2017.
435"
REFERENCES,0.9678030303030303,"[51] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,
436"
REFERENCES,0.9696969696969697,"Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for
437"
REFERENCES,0.9715909090909091,"deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.
438"
REFERENCES,0.9734848484848485,"[52] Wei Yuan and Kai-Xin Gao.
Eadam optimizer: How ϵ impact adam.
arXiv preprint
439"
REFERENCES,0.9753787878787878,"arXiv:2011.02150, 2020.
440"
REFERENCES,0.9772727272727273,"[53] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial
441"
REFERENCES,0.9791666666666666,"dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326, 2018.
442"
REFERENCES,0.9810606060606061,"[54] Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps
443"
REFERENCES,0.9829545454545454,"forward, 1 step back. Advances in neural information processing systems, 32, 2019.
444"
REFERENCES,0.9848484848484849,"[55] Richard Zhang. Making convolutional networks shift-invariant again. In International confer-
445"
REFERENCES,0.9867424242424242,"ence on machine learning, pages 7324–7334. PMLR, 2019.
446"
REFERENCES,0.9886363636363636,"[56] Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li. On the convergence and improvement of stochastic
447"
REFERENCES,0.990530303030303,"normalized gradient descent. Science China Information Sciences, 64:1–13, 2021.
448"
REFERENCES,0.9924242424242424,"[57] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon
449"
REFERENCES,0.9943181818181818,"Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in
450"
REFERENCES,0.9962121212121212,"observed gradients. Advances in neural information processing systems, 33:18795–18806,
451"
REFERENCES,0.9981060606060606,"2020.
452"
