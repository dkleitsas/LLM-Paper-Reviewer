Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0018018018018018018,"Discovering causal structures from data is a challenging inference problem of fun-
damental importance in all areas of science. The appealing scaling properties of
neural networks have recently led to a surge of interest in differentiable neural
network-based methods for learning causal structures from data. So far, differen-
tiable causal discovery has focused on static datasets of observational or interven-
tional origin. In this work, we introduce an active intervention-targeting mecha-
nism which enables quick identiﬁcation of the underlying causal structure of the
data-generating process. Our method signiﬁcantly reduces the required number
of interactions compared with random intervention targeting and is applicable for
both discrete and continuous optimization formulations of learning the underly-
ing directed acyclic graph (DAG) from data. We examine the proposed method
across multiple frameworks in a wide range of settings and demonstrate superior
performance on multiple benchmarks from simulated to real-world data."
INTRODUCTION,0.0036036036036036037,"1
INTRODUCTION"
INTRODUCTION,0.005405405405405406,"Learning causal structure from data is a challenging but important task that lies at the heart of
scientiﬁc reasoning and accompanying progress in many disciplines (Sachs et al., 2005; Hill et al.,
2016; Lauritzen & Spiegelhalter, 1988; Korb & Nicholson, 2010). While there exists a plethora of
methods for the task, computationally and statistically more efﬁcient algorithms are highly desirable
(Heinze-Deml et al., 2018). As a result, there has been a surge in interest in differentiable structure
learning and the combination of deep learning and causal inference (Sch¨olkopf et al., 2021). Such
methods deﬁne a structural causal model with smoothly differentiable parameters that are adjusted
to ﬁt observational data (Zheng et al., 2018; Yu et al., 2019; Zheng et al., 2020; Bengio et al.,
2019; Lorch et al., 2021; Annadani et al., 2021), although some methods can accept interventional
data, thereby signiﬁcantly improving the identiﬁcation of the underlying data-generating process
(Ke et al., 2019; Brouillard et al., 2020; Lippe et al., 2021). However, the improvement critically
depends on the experiments and interventions available to the learner."
INTRODUCTION,0.007207207207207207,"Despite advances in high-throughput methods for interventional data in speciﬁc ﬁelds (Dixit et al.,
2016), the acquisition of interventional samples in general settings tends to be costly, technically
impossible or even unethical for speciﬁc interventions. There is, therefore, a need for efﬁcient usage
of the available interventional samples and efﬁcient experimental design to keep the number of
interventions to a minimum."
INTRODUCTION,0.009009009009009009,"A signiﬁcant amount of prior work exists in causal structure learning that leverages active learn-
ing and experimental design to improve identiﬁability in a sequential manner. These approaches
are either graph theoretical (He & Geng, 2008; Eberhardt, 2012; Hyttinen et al., 2013; Hauser &
B¨uhlmann, 2014; Shanmugam et al., 2015; Kocaoglu et al., 2017b;a; Lindgren et al., 2018; Ghas-
sami et al., 2018; 2019; Greenewald et al., 2019; Squires et al., 2020), Bayesian (Murphy, 2001;
Tong & Koller, 2001; Masegosa & Moral, 2013; Cho et al., 2016; Ness et al., 2017; Agrawal et al.,
2019; Zemplenyi & Miller, 2021) or rely on Invariant Causal Prediction (Gamella & Heinze-Deml,
2020). These methods are typically computationally very expensive and do not scale well with re-
spect to the number of variables or dataset size (Heinze-Deml et al., 2018). A promising alternative
is the use of active learning in a continuous optimization framework for causal structure learning
from joint data. However, since the applicability of existing scores / heuristics for selecting inter-"
INTRODUCTION,0.010810810810810811,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.012612612612612612,"vention targets is limited for existing frameworks (see §A.1), current approaches rely on random and
independent interventions and do not leverage the acquired evidence from processed experiments."
INTRODUCTION,0.014414414414414415,"We thus propose a novel method of active selection of intervention targets that can easily be in-
corporated into many differentiable causal discovery algorithms. Since most of these algorithms
treat the adjacency matrix of the causal graph as a learned soft-adjacency, it is readily available for
parametrized sampling of different hypothesis graphs. Our method looks for an intervention target
that gives maximum disagreement between post-interventional sample distributions under these hy-
pothesis graphs. We conjecture that interventions on such nodes will contain more information about
the causal structure and hence enable more efﬁcient learning. To the best of our knowledge, our pa-
per is the ﬁrst approach to combine both a continuous optimization framework and active causal
structure learning from observational and interventional data. We summarize our contributions as
follows:"
INTRODUCTION,0.016216216216216217,"• We propose a novel approach for selecting interventions (single and multi-target) which identify
the underlying graph efﬁciently and can be used for any differentiable causal discovery method.
• We introduce a novel, scalable two-phase DAG sampling procedure which efﬁciently generates
hypothesis DAGs based on a soft-adjacency matrix.
• We examine the proposed intervention-targeting method across multiple differentiable causal
discovery frameworks in a wide range of settings and demonstrate superior performance against
established competitive baselines on multiple benchmarks from simulated to real-world data.
• We provide empirical insights on the distribution of selected intervention targets and its con-
nection to the (causal) topological order of the variables in the underlying system."
PRELIMINARIES,0.018018018018018018,"2
PRELIMINARIES"
PRELIMINARIES,0.01981981981981982,"Structural Causal Model. An SCM (Peters et al., 2017) is deﬁned over a set of random variables
X1, . . . , XM or just X for short and a directed acyclic graph (DAG) G = (V, E) over variable nodes
V = {1, . . . M}. The random variables are connected by edges in E via functions fi and jointly
independent noise variables Ui through Xi = fi(Xpa(i), Ui) where Xpa(i) are Xi’s parents in G,
and directed edges in the graph represent direct causation. The conditionals P(Xi|Xpa(i)) deﬁne
the conditional distribution of Xi given its parents."
PRELIMINARIES,0.021621621621621623,"Interventions. Interventions on Xi change the conditional distribution of P(Xi|Xpa(i)) to a dif-
ferent distribution, hence affecting the outcome of Xi. Interventions can be perfect (hard) or im-
perfect (soft). Hard interventions entirely remove the dependencies of a variable Xi on its parents
Xpa(i), hence deﬁning the conditional probability distribution of Xi by some ˜P(Xi) rather than
P(Xi|Xpa(i)). A more general form of intervention is the soft intervention, where the interven-
tion changes the effect of the parents of Xi on itself by modifying the conditional distribution from
Pi(Xi|Xpa(i)) to an alternative ˜Pi(Xi|Xpa(i))."
PRELIMINARIES,0.023423423423423424,"Dependency Structure Discovery from Interventions (DSDI). We evaluate the proposed method
under multiple continuous-optimization causal learning frameworks from fused (observational and
interventional) data (Bareinboim & Pearl, 2016), one of them being DSDI (Ke et al., 2019). The
work of DSDI reformulates the problem of causal discovery from discrete data as a continuous
optimization problem using neural networks. The framework proposes to learn the causal graph
adjacency matrix as a matrix parameter γ of a neural network, and is trained using a 3-stage it-
erative procedure. The ﬁrst stage involves sampling graphs under the model’s current belief in the
graph structure and then training functional parameters P specifying the conditionals of the sampled
graphs using observational data. Note how sampling a graph from γ can specify element-wise mul-
tiplications by 0 or 1 in the ﬁrst layer of neural networks for the conditionals, to remove unallowed
edges, with other network parameters P applicable to any sampled graph. The next stage is to eval-
uate the sampled graphs under interventional data and score these graphs accordingly. The ﬁnal step
is to update the learned adjacency matrix with the scores from stage 2. This method performs com-
petitively compared to many other methods. However, all intervention targets in stage 2 of DSDI
are random and independent, a strategy that scales poorly to larger graphs. A better approach would
have been active intervention targeting, elaborated in the next section."
PRELIMINARIES,0.025225225225225224,"Differentiable Causal Discovery from Interventional Data (DCDI). We also consider the work
of DCDI (Brouillard et al., 2020), which addresses causal discovery from continuous data as a
continuous-constrained optimization problem using neural networks to model parameters of Gaus-"
PRELIMINARIES,0.02702702702702703,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.02882882882882883,(a) AIT procedure for intervention target Ik VBG
PRELIMINARIES,0.03063063063063063,"0.0
0.2
0.4
0.6
0.8
1.0 VWG"
PRELIMINARIES,0.032432432432432434,"0.0
0.2
0.4
0.6
0.8
1.0"
PRELIMINARIES,0.03423423423423423,log10 Dk 3 2 1 0 1 2 3
PRELIMINARIES,0.036036036036036036,(b) Discrepancy score Dk landscape
PRELIMINARIES,0.03783783783783784,"Figure 1: (a) Procedure: Start by sampling a set of hypothetical graphs G given the current graph
beliefs γ, and apply an intervention on targets Ik on the functional parameters P which results in
partially altered parameters Pk. Continue by sampling sets of hypothetical interventional experi-
ments under Pk for every graph in G and compute the corresponding discrepancy score Dk as a
ratio of variance-between-graphs VBGk and variance-within-graphs VWGk. (b) Landscape of the
discrepancy score in terms of VBGk and VWGk, in logarithmic scale."
PRELIMINARIES,0.03963963963963964,"sian distributions or normalizing ﬂows (Rezende & Mohamed, 2015) which represent conditional
distributions. Unlike DSDI’s iterative training of the structural and functional parameters, DCDI
optimizes the causal adjacency matrix and functional parameters jointly over the fused data space.
But like DSDI, DCDI uses random and independent interventions."
ACTIVE INTERVENTION TARGETING,0.04144144144144144,"3
ACTIVE INTERVENTION TARGETING"
ACTIVE INTERVENTION TARGETING,0.043243243243243246,"We present a score-based intervention design strategy, called Active Intervention Targeting (AIT),
which is applicable to many discrete and continuous optimization formulations of causal structure
learning algorithms. Furthermore, we show how our proposed method can be integrated into recent
differentiable causal discovery frameworks for guided exploration using interventional data."
ACTIVE INTERVENTION TARGETING,0.04504504504504504,"Assumptions. The proposed method assumes access to a belief state γ in the graph structure (e.g.,
in the form of a distribution over graphs, probabilistic adjacency matrix or a set of hypothetical
graphs) and functional parameters P characterizing the conditional relationships between variables
(constrained by the graphs sampled from the belief speciﬁed by γ). The proposed model does not
have to assume causal sufﬁciency per se. However, it inherits the assumptions of the selected base
framework, and this may include causal sufﬁciency depending on the base algorithm of choice. In
case the underlying framework can handle unobserved variables and offers a generative method for
interventional samples, then our method is also applicable"
A SCORE FOR INTERVENTION TARGETING,0.04684684684684685,"3.1
A SCORE FOR INTERVENTION TARGETING"
A SCORE FOR INTERVENTION TARGETING,0.04864864864864865,"Algorithm 1 Active Intervention Targeting (AIT)
Input: Functional Parameters P, Graph Belief State γ,
Interventional Target Space I
Output: Intervention Target Ik∗"
A SCORE FOR INTERVENTION TARGETING,0.05045045045045045,"1: G ←Sample a set of hypothesis graphs from γ
2: for each intervention target Ik in I do
3:
Pk ←Perform intervention Ik on P
4:
for each graph Gi in G do
5:
Sk,i ←Draw samples from Pk on Gi
6:
Sk,i ←Set variables in Ik to 0
7:
end for
8:
Compute: Dk ←"
A SCORE FOR INTERVENTION TARGETING,0.05225225225225225,"P
i(µk
i −µk)2
P
i
P
j(Sk,i
j
−µk
i )2
9: end for
10: Target Intervention Ik∗←arg maxk(Dk)"
A SCORE FOR INTERVENTION TARGETING,0.05405405405405406,"Given a graph belief state γ with its cor-
responding functional parameters P, and
a possible set of intervention targets I
(single-node and multi-node intervention
targets), we wish to select the most in-
formative intervention target(s) Ik∗∈I
with respect to identiﬁability of the un-
derlying structure.
Such target(s) pre-
sumably yield relatively high discrepan-
cies between samples drawn under differ-
ent hypothesis graphs, indicating larger
uncertainty about the target’s relation to
its parents and/or children."
A SCORE FOR INTERVENTION TARGETING,0.055855855855855854,"We thus construct an F-test-inspired score
to determine the Ik∗exhibiting the high-
est discrepancies between post-interventional sample distributions generated by likely graph struc-
tures under ﬁxed functional parameters P. In order to compare sample distributions over different
graphs, we distinguish between two sources of variation: Variance between graphs (VBG) and vari-
ance within graphs (VWG). While VBG characterizes the variance of sample means over multiple"
A SCORE FOR INTERVENTION TARGETING,0.05765765765765766,Under review as a conference paper at ICLR 2022
A SCORE FOR INTERVENTION TARGETING,0.05945945945945946,"graphs, VWG accounts for the sample variance when a speciﬁc graph is ﬁxed. As in DSDI and
DCDI, we mask the contribution of the intervened variables Ik to VBG and VWG, and construct our
discrepancy score D as a ratio D = VBG VWG."
A SCORE FOR INTERVENTION TARGETING,0.06126126126126126,"This discrepancy score attains high values for intervention targets of particular interest (see Figure
1b for a landscape visualization). While VBG itself indicates for which intervention targets the model
is unsettled about, an extension to the proposed variance ratio enables more control over the region
of interest. Given a ﬁxed set of graphs G and a ﬁxed interventional sample size across all graphs, let
us assume a scenario where multiple intervention targets attain high VBG. Assessing VWG allows us
to distinguish between two extreme cases: (a) targets with sample populations that exhibit large VWG
(b) targets with sample populations that exhibit low VWG. While high VBG in (a) might be induced
by an insufﬁcient sample size due to high variance in the interventional distribution itself, (b) clearly
indicates high discrepancy between graphs and should be preferentially studied."
A SCORE FOR INTERVENTION TARGETING,0.06306306306306306,"Computational Details.
We begin by sampling a set of graphs G = {Gi}, i = 1, 2, 3, . . . from
our graph structure belief state γ, however parametrized. This G will remain ﬁxed for all considered
interventions for the current experimental round. Then, we ﬁx an intervention target Ik and apply
the corresponding intervention to P, resulting in partially altered functional parameters Pk where
some conditionals have been changed. Next, we draw interventional samples Sk,i from Pk on every
graph Gi ∈G and set variables in Ik to zero to mask of their contribution to the variance. Having
collected all samples over the considered graphs for the speciﬁc intervention target Ik, we compute
VBGk and VWGk as:"
A SCORE FOR INTERVENTION TARGETING,0.06486486486486487,"VBGk =
X"
A SCORE FOR INTERVENTION TARGETING,0.06666666666666667,"i
(µk
i −µk)2
and
VWGk =
X i X"
A SCORE FOR INTERVENTION TARGETING,0.06846846846846846,"j
(Sk,i
j
−µk
i )2"
A SCORE FOR INTERVENTION TARGETING,0.07027027027027027,"where µk is a vector of the same dimension as any sample in S and denotes the overall sample-mean
of the interventional setting, µk
i the corresponding mean for a speciﬁc graph Gi and Sk,i
j
is the j-th
sample of the i-th graph conﬁguration. Finally, we construct the discrepancy score Dk of Ik as:"
A SCORE FOR INTERVENTION TARGETING,0.07207207207207207,Dk ←VBGk
A SCORE FOR INTERVENTION TARGETING,0.07387387387387387,"VWGk
."
A SCORE FOR INTERVENTION TARGETING,0.07567567567567568,"In contrast to the original deﬁnition of the F-Score, we can ignore the normalization constants due
to equal group size and degree-of-freedoms. Although the dependence between the variables is
apparent from the connected causal structure, we approximate the variance of the multidimensional
samples as the trace over the covariance matrix by assuming that the variables are independent. An
outline of the method is provided in Algorithm 1."
TWO-PHASE DAG SAMPLING,0.07747747747747748,"3.2
TWO-PHASE DAG SAMPLING"
TWO-PHASE DAG SAMPLING,0.07927927927927927,"Embedding AIT into recent differentiable causal discovery frameworks requires a graph sampler
which generates a set of likely graph conﬁgurations under the current graph belief state. However,
drawing samples from unconstrained graphs (e.g. partially undirected graphs or cyclic directed
graphs) is an expensive multi-pass process. Here, we thus constrain our graph sampling space to
DAGs. Since most differentiable causal structure learning algorithms learn edge beliefs in the form
of a soft-adjacency matrix, we present a scalable, two-stage DAG sampling procedure which exploits
structural information of the soft-adjacency matrix beyond independent edge conﬁdences (see Figure
2 for a visual illustration). More precisely, we start by sampling topological node orderings from
an iterative reﬁned score and construct DAGs in the constrained space by independent Bernoulli
draws over possible edges. We can thus guarantee DAGness by construction and do not have to rely
on expensive, non-scalable techniques such as rejection sampling or Gibbs sampling. The overall
method is inspired by topological sorting algorithms of DAGs where we iteratively identify nodes
with no incoming edges, remove them from the graph and repeat until all nodes are processed."
TWO-PHASE DAG SAMPLING,0.08108108108108109,"Soft-Adjacency. Given a learnable graph structure γ ∈RN×N of a graph over N variables, the soft-
adjacency matrix is given as σ(γ) ∈[0, 1]N×N such that σ(γij) ∈[0, 1] encodes the probabilistic
belief in random variable Xj being a direct cause of Xi, where σ(x) = (1 + exp(−x))−1 denotes
the sigmoid function. For the ease of notation, we deﬁne A = σ(γ) and Ak denotes the considered
soft-adjacency σ(γ) at iteration k. Note that the shape of Ak changes through the iterations."
TWO-PHASE DAG SAMPLING,0.08288288288288288,"Sample node orderings. For the iterative root sampling procedure, we start at iteration k = 0
with an initial soft-adjacency Ak = A and apply the following routine for N iterations. We take"
TWO-PHASE DAG SAMPLING,0.08468468468468468,Under review as a conference paper at ICLR 2022
TWO-PHASE DAG SAMPLING,0.08648648648648649,"Figure 2: Two-Stage DAG Sampling: Based on a soft-adjacency σ(γ), we sample a topological
node ordering from an iterative reﬁned score which is repeatedly computed until we have processed
all nodes of the graph. We proceed by permuting σ(γ) according to the drawn node ordering and
constrain the upper triangular part to ensure DAGness. Finally, we take independent Bernoulli draws
of the unconstrained edge beliefs and arrive at a sampled DAG."
TWO-PHASE DAG SAMPLING,0.08828828828828829,"the maximum over rows of Ak, resulting in a vector of independent probabilities pk
c, where pk
c(i)
denotes the maximal probability of variable Xi being a child of any other variable at the current
belief state. After taking the complement pk
r = 1 −pk
c, we arrive at pk
r where pk
r(i) denotes the
approximated probability of variable Xi being a root node in the current state. In order to arrive at a
normalized distribution to sample a root node, we apply a temperature-scaled softmax:"
TWO-PHASE DAG SAMPLING,0.09009009009009009,"pk
s(i) = softmax(pk
r/t)i =
exp
pk
r(i)/t

P"
TWO-PHASE DAG SAMPLING,0.0918918918918919,"j exp
pkr(j)/t
"
TWO-PHASE DAG SAMPLING,0.0936936936936937,"where t denotes the temperature. The introduction of temperature-scaling allows to control the
distribution over nodes and account for the entropy of the structural belief. We proceed by sampling
a (root) node as rk ∼Categorical(pk
s) and delete all corresponding rows and columns from Ak and
arrive at a shrinked soft-adjacency Ak+1 ∈[0, 1](N−k−1)×(N−k−1) over the remaining variables.
We repeat the procedure until we have processed all nodes and have a resulting topological node
ordering ≺of [r0, ..., rN−1]."
TWO-PHASE DAG SAMPLING,0.09549549549549549,"Sample DAGs based on node orderings. Given a node ordering ≺, we permute the soft-adjacency
A accordingly and constrain the upper triangular part by setting values to 0 to ensure DAGness by
construction (as shown in Figure 2). Finally, we sample a DAG by independent Bernoulli draws of
the edge beliefs, as proposed in Ke et al. (2019)."
APPLICABILITY TO DSDI,0.0972972972972973,"3.3
APPLICABILITY TO DSDI"
APPLICABILITY TO DSDI,0.0990990990990991,"Figure 3: Adapted workﬂow of DSDI
with Active Intervention Targeting"
APPLICABILITY TO DSDI,0.1009009009009009,"Before integrating our method into the DSDI framework,
we must choose/design a graph sampler based on DSDI’s
graph belief characterization and deﬁne a sampling rou-
tine to generate interventional samples under a given state
of the structural and functional parameters."
APPLICABILITY TO DSDI,0.10270270270270271,"DSDI offers a learnable graph structure over N variables
with γ ∈RN×N such that σ(γ) ∈[0, 1]N×N encodes the
soft-adjacency matrix. This formulation naturally sug-
gests the application of the introduced two-phase DAG
sampling to generate hypothetical DAGs under current
beliefs. Under these acyclic graph conﬁgurations, one
may then apply an intervention to DSDI functional pa-
rameters P and sample data using ancestral sampling."
APPLICABILITY TO DSDI,0.1045045045045045,"DSDI’s architectural choices allow a seamless integration of our proposed active intervention tar-
geting into stage 2 of DSDI, where graphs are evaluated using interventional data. See Figure 3
for an illustrative description of the adapted workﬂow and §2 for a compact description of the base
framework."
APPLICABILITY TO DSDI,0.1063063063063063,Under review as a conference paper at ICLR 2022
APPLICABILITY TO DCDI,0.10810810810810811,"3.4
APPLICABILITY TO DCDI"
APPLICABILITY TO DCDI,0.10990990990990991,"DCDI also offers access to γ ∈RN×N which allows the same setup as with DSDI. In order to gener-
ate interventional samples under the hypothetical graphs, we alter the conditionals of the intervened
variables and perform ancestral sampling based on the model’s learned conditional densities."
APPLICABILITY TO DCDI,0.11171171171171171,"Embedding AIT into DCDI allows us to predict an interventional target space instead of relying on
random interventional samples chosen out of the full target space. In contrast to the unconstrained
target space of the original formulation, we estimate a target space of constrained size using AIT
and reevaluate it after a ﬁxed number of gradient steps (see §A.7.1 for technical details)."
EXPERIMENTS,0.11351351351351352,"4
EXPERIMENTS"
EXPERIMENTS,0.11531531531531532,"We evaluate the proposed active intervention targeting mechanism on single-target interventions un-
der two different settings: DSDI (Ke et al., 2019) and DCDI (Brouillard et al., 2020). We investigate
the impact of AIT under both settings for identiﬁability, sample complexity, and convergence be-
haviour compared to random targeting where the next intervention target is chosen independent of
the current evidence. In a further line of experiments, we analyze the targeting dynamics with respect
to convergence behaviour and the distribution of target node selections. This section will highlight
our results on DSDI while also including key results with respect to DCDI (structural discovery and
identiﬁability). However, further analysis of DCDI results have been shifted to the Appendix."
EXPERIMENTS,0.11711711711711711,"Evaluation Setup.
A huge variety of SCMs and their induced DAGs exist, each of which can
stress causal structure discovery algorithms in different ways. We perform a systematic evaluation
over a selected set of synthetic and non-synthetic SCMs (and datasets). We distinguish between
synthetic structured graphs and random graphs, the latter generated from the Erd˝os–R´enyi (ER)
model with varying edge densities (see §A.3 for a detailed description of the setup). For conciseness,
we only report results on 15-node graphs in this section for the noise-free synthetic setting for AIT
on DSDI and on 10-node graphs for the noisy setting for AIT on DSDI (discrete data). In addition,
we report key results on 10-node graphs for AIT on DCDI (continuous data) in the main text and
provide further results and ablation studies in Appendix. We complete the setup with the Sachs ﬂow
cytometry dataset (Sachs et al., 2005) and the Asia network (Lauritzen & Spiegelhalter, 1988) to
evaluate the proposed method on well-known real-world datasets for causal structure discovery. 1"
EXPERIMENTS,0.11891891891891893,"Key Findings.
(a) We report strong results for active-targeted structure discovery on both dis-
crete and continuous-valued datasets, outperforming random targeting in all experiments. (b) The
proposed intervention targeting mechanism signiﬁcantly reduces sample complexity with strong
beneﬁts for graphs of increasing size and density. (c) The distribution of target selections during
graph exploration is strongly connected to the topology of the underlying graph. (d) Our method
is capable of identifying informative targets. (e) Undesirable interventions are drastically reduced.
(f) When monitoring structured Hamming distance (SHD) throughout the procedure, an “elbow”
point appears approximately when the Markov equivalence class (MEC) has been isolated. (g) AIT
introduces desirable properties such as improved recovery of erroneously converging edges. (h) AIT
signiﬁcantly improves robustness in noise-perturbed environments."
EXPERIMENTS,0.12072072072072072,"Structure discovery: Synthetic datasets. We evaluate accuracy in terms of Structural Hamming
Distance (SHD) (Acid & de Campos, 2003) on a diverse set of synthetic non-linear datasets under
both DSDI and DCDI, adopting their respective evaluation setups."
EXPERIMENTS,0.12252252252252252,"The results of DSDI with AIT are reported in Table 1. DSDI with active intervention targeting
outperforms all baselines and DSDI with random intervention targeting over all presented datasets.
It enables almost perfect identiﬁability on all structured graphs of size 15 except for the full15
graph, and signiﬁcantly improves structure discovery of random graphs with varying densities. As
the size or density of the underlying causal graphs increases, the beneﬁt of the selection policy be-
comes more apparent (see Figure 4). We also examine the effectiveness of our proposed method for
DCDI (Brouillard et al., 2020) on non-linear data from random graphs of size 10. Active Interven-
tion Targeting improves the identiﬁcation in terms of sample complexity and structural identiﬁabil-
ity compared with random exploration (see Figure 5 and §A.7 for further analyses). We observe the"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.12432432432432433,"1The real-world datasets are available through a Creative Commons Attribution-Share Alike License in the bnlearn R package and most baseline implementations
are available for Python in the causal discovery toolbox (Kalainathan & Goudet, 2019) with an MIT license. A-ICP is provided at https://github.com/
juangamella/aicp but without a license."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.12612612612612611,Under review as a conference paper at ICLR 2022
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.12792792792792793,"0
10000
20000
30000
40000
50000
Steps 0 25 50 75 SHD"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.12972972972972974,(a) ER-1:
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.13153153153153152,"0
10000
20000
30000
40000
50000
Steps 0 25 50 75 SHD"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.13333333333333333,(b) ER-2:
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.13513513513513514,"0
10000
20000
30000
40000
50000
Steps 0 25 50 75 SHD"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.13693693693693693,(c) ER-4:
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.13873873873873874,"Figure 4: DSDI with active intervention targeting (orange) leads to superior performance over ran-
dom intervention targeting (blue) on random graphs of size 15. The performance gap becomes more
signiﬁcant with increasing edges density. The plot shows average performance in terms of SHD.
Error bands were estimated using 10 random ER graphs per setting."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.14054054054054055,"Table 1: SHD (lower is better) on various 15-variable synthetic datasets. Structured graphs are
sorted in ascending order according to their edge density.
(∗) denotes average SHD over 10 graphs."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.14234234234234233,"Structured Graphs
Random Graphs"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.14414414414414414,"Chain
Collider
Tree
Bidiag
Jungle
Full
ER-1(∗)
ER-2(∗)
ER-4(∗)"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.14594594594594595,"GES (Chickering, 2002)
13
1
12
14
14
69
8.3 (±1.9)
17.6 (±4.6)
39.4 (±6.7)
GIES (Hauser & B¨uhlmann, 2012)
13
6
10
17
23
60
10.9 (±4.2)
18.1 (±4.3)
39.3 (±5.6)
ICP (Peters et al., 2016)
14
14
14
27
26
105
16.2 (±3.6)
31.1 (±3.4)
60.1 (±3.9)
A-ICP (Gamella & Heinze-Deml, 2020)
14
14
14
27
26
105
16.2 (±3.6)
31.1 (±3.4)
60.1 (±3.9)"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.14774774774774774,"NOTEARS (Zheng et al., 2018)
22
21
26
33
35
93
23.7 (±4.0)
35.8 (±5.2)
59.5 (±3.7)
DAG-GNN (Yu et al., 2019)
11
14
15
27
25
97
16.0 (±3.7)
30.6 (±3.4)
59.7 (±4.1)"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.14954954954954955,"DSDI (Random) (Ke et al., 2019)
0
0
2
3
7
24
1.4 (±1.6)
2.1 (±2.3)
7.2 (±2.7)
DSDI (AIT)
0
0
0
0
0
7
0.0 (±0.0)
0.0 (±0.0)
0.0 (±0.0)"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.15135135135135136,"clear impact of the targeting mechanisms, which control the order and frequency of interventional
targets presented to the model. Further experimental results for DCDI can be found in Appendix."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.15315315315315314,"Table 2: SHD (lower is better) on two
real-world datasets"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.15495495495495495,"Sachs
Asia"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.15675675675675677,"GES (Chickering, 2002)
19
4
GIES (Hauser & B¨uhlmann, 2012)
16
11
ICP (Peters et al., 2016)
17
8
A-ICP (Gamella & Heinze-Deml, 2020)
17
8"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.15855855855855855,"NOTEARS (Zheng et al., 2018)
22
14
DAG-GNN (Yu et al., 2019)
19
10"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.16036036036036036,"DSDI (Random) (Ke et al., 2019)
6
0
DSDI (AIT)
6
0"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.16216216216216217,"Structure discovery: ﬂow cytometry and asia dataset.
While the synthetic datasets systematically explore the
strengths and weaknesses of causal structure discovery
methods, we further evaluate their capabilities on the
real-world ﬂow cytometry dataset (also known as Sachs
network)(Sachs et al., 2005) and the Asia network (Lau-
ritzen & Spiegelhalter, 1988) from the BnLearn Repos-
itory. DSDI with active intervention targeting outper-
forms all measured baselines and achieves the same re-
sult as random targeting in terms of SHD, but with re-
duced sample complexity. Despite AIT deviating only
by 6 undirected edges from the (concensus) ground truth structure of Sachs et al. (Sachs et al., 2005),
there is some concern about the correctness of this graph and the different assumptions associated
with the dataset (Mooij et al., 2020; Zemplenyi & Miller, 2021). Therefore, perfect identiﬁcation
may not be achievable by any method in practice in the Sachs setting."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.16396396396396395,"Effect of intervention targeting on sample complexity.
Aside from the signiﬁcantly improved
identiﬁcation of underlying causal structures, our method allows for a substantial reduction in in-
terventional sample complexity. After reaching the “elbow” point in terms of structural Hamming
distance, random intervention targeting requires a fairly long time to converge to a solution within
the MEC. In contrast, our proposed technique continues to select informative intervention targets
beyond the elbow point and more quickly converges to the correct graph within the MEC. The
continued effectiveness of our method directly translates to increased sample-efﬁciency and conver-
gence speed, and is apparent for all examined datasets (see Figure 4)."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.16576576576576577,"Distribution of intervention targets. The careful study of the behaviour of the proposed method
under our chosen synthetic graphs enable us to reason about the method’s underlying dynamics.
Analysing the dynamics of intervention targeting reveals that the distribution of target node selec-
tions is linked to the topology of the underlying graph. More speciﬁcally, the number of selections of
a given target node strongly correlates with its out-degree and number of descendants in the underly-
ing ground-truth graph structure (see Figure 7). That our method prefers interventions on nodes with"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.16756756756756758,Under review as a conference paper at ICLR 2022
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.16936936936936936,"Figure 5: AIT-guided DCDI (orange) allows a more rapid discovery of the causal structure compared
to DSDI relying on random interventions (blue). The distribution of selected intervention targets
shows its correlated connection to the topology of the underlying graph where nodes of greater
impact on the overall system are preferentially studied and nodes without children are rarely chosen."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.17117117117117117,"Figure 6: DSDI: Dynamics and target distribution of AIT for a structured jungle graph of size 15.
The graphs’ nodes are sorted in topological order, root node ﬁrst. The graph is binary-tree-like with
4 levels. For the dense jungle15, the multi-level structure characteristic of the tree-like graph is
readily apparent even before the elbow point. Nodes without children are very rarely chosen.
greater (downstream) impact on the overall system can be most clearly observed in the distribution
of target selection on the example of the synthetic jungle graph in Figure 6."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.17297297297297298,"Selection of informative targets. Apart our strong results in the discovery of the underlying causal
graph, we demonstrate AIT’s general ability of detecting informative intervention targets in a careful
designed empirical study in Appendix §A.6.7."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.17477477477477477,"Reduction of undesirable interventions.
An intervention destroys the original causal inﬂuence
of other variables on the intervened target variable Ik, so its samples cannot be used to determine
the causal parents of Ik in the undisturbed system. Therefore, if a variable without children is
detected, interventions upon it should be avoided since they effectively result in redundant obser-
vational samples of the remaining variables that are of no beneﬁt for causal structure discovery.
Active intervention targeting leads to the desirable property that interventions on such variables are
drastically reduced (see Figure 5 and 6)."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.17657657657657658,"Identiﬁcation of Markov equivalence class. Investigating the evolution of the intervention tar-
get distribution over time reveals that the causal discovery seems to be divided into two phases
of exploration: Phase 1 lasts until the elbow point in terms of SHD, and Phase 2 from the elbow
point until convergence (see Figure 4). We observed over multiple experiments that phase 1 tends
to quickly discover the underlying skeleton (removing superﬂuous connections while keeping some
edges undirected), until a belief state γelbow is reached representing a MEC, or a class of graphs very
close to a MEC. Phase 2 is predominantly operating on the partially directed skeleton and directed
on the remaining edges."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.1783783783783784,"Recovery of erroneously converging edges. Recovery of incorrectly-converging edges critically
depends on adapting the order of interventions, which a random intervention policy does not. In
sharp contrast, intervention targeting signiﬁcantly promotes early recovery from incorrect assign-
ment of an edge. In contrast, the observed edge dynamics and the corresponding graph belief states
indicate that the random policy can lock itself into unfavorable belief states from which recovery is
extremely difﬁcult, while AIT provides an escape hatch throughout learning."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.18018018018018017,"AIT improves robustness in noise-perturbed environments. Considering that noise signiﬁcantly
impairs the performance of causal discovery, we examine the performance of active intervention
targeting in noise-perturbed environments with respect to SHD and convergence speed and compare
it with random intervention targeting. We conduct experiments under different noise levels in the
setting of binary data generated from structured and random graphs of varying density. A noise level"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.18198198198198198,Under review as a conference paper at ICLR 2022
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.1837837837837838,Out-Degree
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.18558558558558558,Descendants
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.1873873873873874,In-Degree
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.1891891891891892,Ancestors
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.19099099099099098,Topological-Order
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.1927927927927928,CHAIN15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.1945945945945946,COLLIDER15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.1963963963963964,TREE15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.1981981981981982,BIDIAG15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.2,JUNGLE15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.2018018018018018,FULL15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.2036036036036036,ER-1(*)
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.20540540540540542,ER-2(*)
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.2072072072072072,ER-4(*)
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.209009009009009,"0.02
0.36
-0.59
-0.36
-0.36"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.21081081081081082,"0.04
0.04
-0.04
-0.04
-0.04"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.2126126126126126,"0.22
0.70
-0.61
-0.60
-0.60"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.21441441441441442,"-0.01
0.36
-0.60
-0.36
-0.36"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.21621621621621623,"0.61
0.69
-0.77
-0.59
-0.59"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.218018018018018,"0.37
0.37
-0.37
-0.37
-0.37"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.21981981981981982,"0.22
0.25
-0.13
-0.09
-0.13"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.22162162162162163,"0.28
0.38
-0.26
-0.26
-0.30"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.22342342342342342,"0.30
0.38
-0.29
-0.31
-0.36"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.22522522522522523,(a) Random Targeting
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.22702702702702704,Out-Degree
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.22882882882882882,Descendants
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.23063063063063063,In-Degree
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.23243243243243245,Ancestors
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.23423423423423423,Topological-Order
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.23603603603603604,CHAIN15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.23783783783783785,COLLIDER15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.23963963963963963,TREE15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.24144144144144145,BIDIAG15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.24324324324324326,JUNGLE15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.24504504504504504,FULL15
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.24684684684684685,ER-1(*)
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.24864864864864866,ER-2(*)
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.25045045045045045,ER-4(*)
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.25225225225225223,"0.46
0.98
-0.51
-0.98
-0.98"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.25405405405405407,"0.38
0.38
-0.38
-0.38
-0.38"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.25585585585585585,"0.96
0.77
-0.41
-0.92
-0.92"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.25765765765765763,"0.67
0.97
-0.51
-0.97
-0.97"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.2594594594594595,"0.98
0.85
-0.79
-0.97
-0.97"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.26126126126126126,"0.93
0.93
-0.93
-0.93
-0.93"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.26306306306306304,"0.79
0.79
-0.48
-0.54
-0.59"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.2648648648648649,"0.91
0.90
-0.60
-0.73
-0.76"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.26666666666666666,"0.96
0.94
-0.78
-0.87
-0.88"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.26846846846846845,(b) Active Intervention Targeting
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.2702702702702703,"Figure 7: Correlation scores between the number of individual target selections and different topo-
logical properties of those targets. AIT shows strong correlations with the measured properties over
all graphs, which indicates a controlled discovery of the underlying structure through preferential
targeting of nodes with greater (downstream) impact on the overall system."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.27207207207207207,"(a) η = 0
(b) η = 0.01
(c) η = 0.02
(d) η = 0.05"
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.27387387387387385,"Figure 8: Convergence Behaviour in terms of SHD for random ER-4 graphs over 10 variables under
different noise levels η, where Active Intervention Targeting (orange) clearly outperforms Random
Targeting (blue) over all noise levels. The performance gap becomes of larger magnitude as the
noise level increases. Error bands were estimated using 3 random ER graphs per setting."
THE REAL-WORLD DATASETS ARE AVAILABLE THROUGH A CREATIVE COMMONS ATTRIBUTION-SHARE ALIKE LICENSE IN THE BNLEARN R PACKAGE AND MOST BASELINE IMPLEMENTATIONS,0.2756756756756757,"η denotes the probability of ﬂipping a random variable and applying it to all measured variables of
observational and interventional samples. Through all examined settings, we observe that active in-
tervention targeting signiﬁcantly improves identiﬁability in contrast to random targeting (see §A.6.6
for detailed results). Active intervention targeting perfectly identiﬁes all structured graphs, except
for the collider and full graph, up to a noise level of η = 0.05, i.e. where every 20th variable
is ﬂipped. The observed performance boost is even more noticeable in the convergence speed, as
shown in Fig. 8 for ER-4 graphs spanning over 10 variables. While the convergence-gap gets more
signiﬁcant with an increasing noise level, random targeting does not converge to the ground-truth
graphs for a noise level higher than η = 0.02. In contrast, AIT still converges to the correct graph
and shows even a convergence tendency for η = 0.05. These ﬁndings support our observation from
different experiments that active intervention targeting leads to a more controlled and robust graph
discovery. Further experimental results in noise-perturbed environments can be found in Appendix."
CONCLUSION,0.2774774774774775,"5
CONCLUSION"
CONCLUSION,0.27927927927927926,"Promising results have driven the recent surge of interest in continuous optimization methods for
Bayesian network structure learning from observational and interventional data. In this work, we
propose an active learning method to choose interventions that help to identify the underlying graph
efﬁciently in the setting of differentiable causal discovery. We show via a detailed empirical study
that active intervention targeting not only improves sample efﬁciency but also the identiﬁcation of
the underlying causal structures compared to random targeting of interventions."
CONCLUSION,0.2810810810810811,"While our method shows signiﬁcant improvements with respect to sample efﬁciency and graph re-
covery over existing methods across multiple noise-free and noise-perturbed datasets, the number
of interventions is not yet optimal (Atkinson & Fedorov, 1975; Eberhardt et al., 2012) and can po-
tentially be reduced in future work. Further, in this work, the interventional samples were presented
to the evaluated frameworks according to a ﬁxed learning schema (e.g. ﬁxed number of samples for
evaluated interventions in graph scoring). It would be interesting to see if the information discovered
by AIT could be used for a more adaptive learning procedure to further improve sample efﬁciency."
CONCLUSION,0.2828828828828829,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY,0.28468468468468466,"6
REPRODUCIBILITY"
REPRODUCIBILITY,0.2864864864864865,"We encourage the reproduction and extension of our presented work by disclosing all relevant infor-
mation. From a methodological point of view, we provide detailed information including algorithm
outlines for the proposed score-based intervention targeting mechanism in section section 3.1 and
similarly for the proposed two-stage DAG sampling technique in section 3.2 and appendix A.2.
Regarding the embedding of the proposed AIT framework into existing differentiable causal dis-
covery frameworks, we introduce the seamless embedding of our proposed method into DSDI (Ke
et al., 2019) in section 3.3 and discuss the embedding including relevant structural changes of DCDI
(Brouillard et al., 2020) in section 3.3 and appendix A.7.1. Moreover, we report all hyperparameters
in appendix A.5 and provide a link to an anonymous repository for reviewers and area chairs once
the discussion forums open. The fully documented code will be soon publicly released to the entire
audience."
REFERENCES,0.2882882882882883,REFERENCES
REFERENCES,0.29009009009009007,"Silvia Acid and Luis M de Campos. Searching for bayesian network structures in the space of
restricted acyclic partially directed graphs. Journal of Artiﬁcial Intelligence Research, 18:445–
490, 2003."
REFERENCES,0.2918918918918919,"Raj Agrawal, Chandler Squires, Karren Yang, Karthikeyan Shanmugam, and Caroline Uhler. Abcd-
strategy: Budgeted experimental design for targeted causal structure discovery.
In The 22nd
International Conference on Artiﬁcial Intelligence and Statistics, pp. 3400–3409. PMLR, 2019."
REFERENCES,0.2936936936936937,"Yashas Annadani, Jonas Rothfuss, Alexandre Lacoste, Nino Scherrer, Anirudh Goyal, Yoshua Ben-
gio, and Stefan Bauer. Variational causal networks: Approximate bayesian inference over causal
structures. arXiv preprint arXiv:2106.07635, 2021."
REFERENCES,0.2954954954954955,"Anthony C Atkinson and Valerii Vadimovich Fedorov. Optimal design: Experiments for discrimi-
nating between several models. Biometrika, 62(2):289–303, 1975."
REFERENCES,0.2972972972972973,"Elias Bareinboim and Judea Pearl. Causal inference and the data-fusion problem. Proceedings of
the National Academy of Sciences, 113(27):7345–7352, 2016."
REFERENCES,0.2990990990990991,"Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, S´ebastien Lachapelle, Olexa Bila-
niuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle
causal mechanisms. arXiv preprint arXiv:1901.10912, 2019."
REFERENCES,0.3009009009009009,"Philippe Brouillard, S´ebastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre
Drouin. Differentiable causal discovery from interventional data. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
volume 33, pp. 21865–21877. Curran Associates, Inc., 2020."
REFERENCES,0.3027027027027027,"David Maxwell Chickering. Optimal structure identiﬁcation with greedy search. Journal of machine
learning research, 3(Nov):507–554, 2002."
REFERENCES,0.3045045045045045,"Hyunghoon Cho, Bonnie Berger, and Jian Peng. Reconstructing causal biological networks through
active learning. PloS one, 11(3):e0150611, 2016."
REFERENCES,0.3063063063063063,"Atray Dixit, Oren Parnas, Biyu Li, Jenny Chen, Charles P Fulco, Livnat Jerby-Arnon, Nemanja D
Marjanovic, Danielle Dionne, Tyler Burks, Raktima Raychowdhury, et al. Perturb-seq: dissecting
molecular circuits with scalable single-cell rna proﬁling of pooled genetic screens. Cell, 167(7):
1853–1866, 2016."
REFERENCES,0.3081081081081081,"Frederick Eberhardt.
Almost optimal intervention sets for causal discovery.
arXiv preprint
arXiv:1206.3250, 2012."
REFERENCES,0.3099099099099099,"Frederick Eberhardt and Richard Scheines. Interventions and causal inference. Philosophy of Sci-
ence, 74(5):981–995, 2007."
REFERENCES,0.3117117117117117,"Frederick Eberhardt, Clark Glymour, and Richard Scheines. On the number of experiments sufﬁcient
and in the worst case necessary to identify all causal relations among n variables. arXiv preprint
arXiv:1207.1389, 2012."
REFERENCES,0.31351351351351353,Under review as a conference paper at ICLR 2022
REFERENCES,0.3153153153153153,"Juan L Gamella and Christina Heinze-Deml. Active invariant causal prediction: Experiment selec-
tion through stability. arXiv preprint arXiv:2006.05690, 2020."
REFERENCES,0.3171171171171171,"AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, and Elias Bareinboim. Budgeted ex-
periment design for causal structure learning. In International Conference on Machine Learning,
pp. 1724–1733. PMLR, 2018."
REFERENCES,0.31891891891891894,"AmirEmad Ghassami, Saber Salehkaleybar, and Negar Kiyavash. Interventional experiment design
for causal structure learning. arXiv preprint arXiv:1910.05651, 2019."
REFERENCES,0.3207207207207207,"Kristjan Greenewald, Dmitriy Katz, Karthikeyan Shanmugam, Sara Magliacane, Murat Kocaoglu,
Enric Boix Adsera, and Guy Bresler. Sample efﬁcient active learning of causal trees. 2019."
REFERENCES,0.3225225225225225,"Alain Hauser and Peter B¨uhlmann. Characterization and greedy learning of interventional markov
equivalence classes of directed acyclic graphs. The Journal of Machine Learning Research, 13
(1):2409–2464, 2012."
REFERENCES,0.32432432432432434,"Alain Hauser and Peter B¨uhlmann. Two optimal strategies for active learning of causal models from
interventional data. International Journal of Approximate Reasoning, 55(4):926–939, 2014."
REFERENCES,0.3261261261261261,"Yang-Bo He and Zhi Geng. Active learning of causal networks with intervention experiments and
optimal designs. Journal of Machine Learning Research, 9(Nov):2523–2547, 2008."
REFERENCES,0.3279279279279279,"Christina Heinze-Deml, Marloes H Maathuis, and Nicolai Meinshausen. Causal structure learning.
Annual Review of Statistics and Its Application, 5:371–391, 2018."
REFERENCES,0.32972972972972975,"Steven M Hill, Laura M Heiser, Thomas Cokelaer, Michael Unger, Nicole K Nesser, Daniel E Carlin,
Yang Zhang, Artem Sokolov, Evan O Paull, Chris K Wong, et al. Inferring causal molecular
networks: empirical assessment through a community-based effort. Nature methods, 13(4):310–
318, 2016."
REFERENCES,0.33153153153153153,"Antti Hyttinen, Frederick Eberhardt, and Patrik O Hoyer. Experiment selection for causal discovery.
Journal of Machine Learning Research, 14:3041–3071, 2013."
REFERENCES,0.3333333333333333,"Diviyan Kalainathan and Olivier Goudet. Causal discovery toolbox: Uncover causal relationships
in python. arXiv preprint arXiv:1903.02278, 2019."
REFERENCES,0.33513513513513515,"Diviyan Kalainathan, Olivier Goudet, Isabelle Guyon, David Lopez-Paz, and Mich`ele Sebag. Sam:
Structural agnostic model, causal discovery and penalized adversarial learning. arXiv preprint
arXiv:1803.04929, 2018."
REFERENCES,0.33693693693693694,"Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard
Sch¨olkopf, Michael C Mozer, Chris Pal, and Yoshua Bengio. Learning neural causal models
from unknown interventions. arXiv preprint arXiv:1910.01075, 2019."
REFERENCES,0.3387387387387387,"Murat Kocaoglu, Alex Dimakis, and Sriram Vishwanath. Cost-optimal learning of causal graphs. In
International Conference on Machine Learning, pp. 1875–1884. PMLR, 2017a."
REFERENCES,0.34054054054054056,"Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Experimental design for learning
causal graphs with latent variables. In Advances in Neural Information Processing Systems, pp.
7018–7028, 2017b."
REFERENCES,0.34234234234234234,"Kevin B Korb and Ann E Nicholson. Bayesian artiﬁcial intelligence. CRC press, 2010."
REFERENCES,0.3441441441441441,"S´ebastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-based
neural dag learning. In International Conference on Learning Representations, 2020."
REFERENCES,0.34594594594594597,"Steffen L Lauritzen and David J Spiegelhalter. Local computations with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical Society: Series
B (Methodological), 50(2):157–194, 1988."
REFERENCES,0.34774774774774775,"Erik M Lindgren, Murat Kocaoglu, Alexandros G Dimakis, and Sriram Vishwanath. Experimental
design for cost-aware learning of causal graphs. arXiv preprint arXiv:1810.11867, 2018."
REFERENCES,0.34954954954954953,Under review as a conference paper at ICLR 2022
REFERENCES,0.35135135135135137,"Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efﬁcient neural causal discovery without acyclic-
ity constraints. arXiv preprint arXiv:2107.10483, 2021."
REFERENCES,0.35315315315315315,"Lars Lorch, Jonas Rothfuss, Bernhard Sch¨olkopf, and Andreas Krause.
Dibs: Differentiable
bayesian structure learning. arXiv preprint arXiv:2105.11839, 2021."
REFERENCES,0.35495495495495494,"R. Duncan Luce. Individual Choice Behavior: A Theoretical analysis. Wiley, New York, NY, USA,
1959."
REFERENCES,0.3567567567567568,"Andr´es R Masegosa and Seraf´ın Moral. An interactive approach for bayesian network learning using
domain/expert knowledge. International Journal of Approximate Reasoning, 54(8):1168–1181,
2013."
REFERENCES,0.35855855855855856,"Joris M. Mooij, Sara Magliacane, and Tom Claassen. Joint causal inference from multiple con-
texts. Journal of Machine Learning Research, 21(99):1–108, 2020. URL http://jmlr.org/
papers/v21/17-123.html."
REFERENCES,0.36036036036036034,Kevin P Murphy. Active learning of causal bayes net structure. 2001.
REFERENCES,0.3621621621621622,"Robert Osazuwa Ness, Karen Sachs, Parag Mallick, and Olga Vitek. A bayesian active learning
experimental design for inferring signaling networks. In International Conference on Research
in Computational Molecular Biology, pp. 134–156. Springer, 2017."
REFERENCES,0.36396396396396397,"Ignavier Ng, Shengyu Zhu, Zhitang Chen, and Zhuangyan Fang. A graph autoencoder approach to
causal structure learning. arXiv preprint arXiv:1911.07420, 2019."
REFERENCES,0.36576576576576575,"Jonas Peters, Peter B¨uhlmann, and Nicolai Meinshausen. Causal inference by using invariant pre-
diction: identiﬁcation and conﬁdence intervals. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 78(5):947–1012, 2016."
REFERENCES,0.3675675675675676,"Jonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf. Elements of causal inference: foundations
and learning algorithms. The MIT Press, 2017."
REFERENCES,0.36936936936936937,"Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical Society: Series C
(Applied Statistics), 24(2):193–202, 1975."
REFERENCES,0.37117117117117115,"Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. arXiv
preprint arXiv:1505.05770, 2015."
REFERENCES,0.372972972972973,"Karen Sachs, Omar Perez, Dana Pe’er, Douglas A Lauffenburger, and Garry P Nolan.
Causal
protein-signaling networks derived from multiparameter single-cell data. Science, 308(5721):
523–529, 2005."
REFERENCES,0.3747747747747748,"Bernhard Sch¨olkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
Anirudh Goyal, and Yoshua Bengio.
Toward causal representation learning.
Proceedings of
the IEEE, 109(5):612–634, 2021."
REFERENCES,0.37657657657657656,"Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G. Dimakis, and Sriram Vishwanath. Learn-
ing causal graphs with small interventions. In NIPS, pp. 3195–3203, 2015."
REFERENCES,0.3783783783783784,"Peter Spirtes, Clark N Glymour, Richard Scheines, David Heckerman, Christopher Meek, Gregory
Cooper, and Thomas Richardson. Causation, prediction, and search. MIT press, 2000."
REFERENCES,0.3801801801801802,"Chandler Squires, Sara Magliacane, Kristjan Greenewald, Dmitriy Katz, Murat Kocaoglu, and
Karthikeyan Shanmugam. Active structure learning of causal dags via directed clique tree. arXiv
preprint arXiv:2011.00641, 2020."
REFERENCES,0.38198198198198197,"Simon Tong and Daphne Koller. Active learning for structure in bayesian networks. In International
joint conference on artiﬁcial intelligence, volume 17, pp. 863–869. Citeseer, 2001."
REFERENCES,0.3837837837837838,"Matthew J Vowels, Necati Cihan Camgoz, and Richard Bowden. D’ya like dags? a survey on
structure learning and causal discovery. arXiv preprint arXiv:2103.02582, 2021."
REFERENCES,0.3855855855855856,"Yue Yu, Jie Chen, Tian Gao, and Mo Yu.
Dag-gnn: Dag structure learning with graph neural
networks. In International Conference on Machine Learning, pp. 7154–7163. PMLR, 2019."
REFERENCES,0.38738738738738737,Under review as a conference paper at ICLR 2022
REFERENCES,0.3891891891891892,"Michele Zemplenyi and Jeffrey W Miller. Bayesian optimal experimental design for inferring causal
structure. arXiv preprint arXiv:2103.15229, 2021."
REFERENCES,0.390990990990991,"Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. DAGs with NO TEARS:
Continuous optimization for structure learning. In Advances in Neural Information Processing
Systems, volume 31, pp. 9472–9483, 2018."
REFERENCES,0.3927927927927928,"Xun Zheng, Chen Dan, Bryon Aragam, Pradeep Ravikumar, and Eric Xing. Learning sparse non-
parametric dags. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 3414–
3425. PMLR, 2020."
REFERENCES,0.3945945945945946,"Shengyu Zhu, Ignavier Ng, and Zhitang Chen. Causal discovery with reinforcement learning. In
International Conference on Learning Representations, 2020."
REFERENCES,0.3963963963963964,Under review as a conference paper at ICLR 2022
REFERENCES,0.3981981981981982,"A
APPENDIX"
REFERENCES,0.4,CONTENTS
REFERENCES,0.4018018018018018,"A.1
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15"
REFERENCES,0.4036036036036036,"A.2
Two-Stage DAG Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16"
REFERENCES,0.40540540540540543,"A.2.1
Algorithm Outline
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16"
REFERENCES,0.4072072072072072,"A.2.2
Connection to Plackett-Luce distribution (Luce, 1959; Plackett, 1975) . . .
16"
REFERENCES,0.409009009009009,"A.3
Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17"
REFERENCES,0.41081081081081083,"A.3.1
Synthetic Datasets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17"
REFERENCES,0.4126126126126126,"A.3.2
Real-World Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18"
REFERENCES,0.4144144144144144,"A.4
Availability of Used (Existing) Assets . . . . . . . . . . . . . . . . . . . . . . . .
18"
REFERENCES,0.41621621621621624,"A.5
Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19"
REFERENCES,0.418018018018018,"A.6
Discrete Setting: Additional Experiments / Results
. . . . . . . . . . . . . . . . .
20"
REFERENCES,0.4198198198198198,"A.6.1
Evaluation (SHD) on graphs of varying size and density
. . . . . . . . . .
20"
REFERENCES,0.42162162162162165,"A.6.2
Evaluation of convergence speed on graphs of varying size and density
. .
21"
REFERENCES,0.42342342342342343,"A.6.3
Target selection analysis for graphs of varying size and density . . . . . . .
22"
REFERENCES,0.4252252252252252,"A.6.4
Visualization of target distribution on structured graphs of size 5 . . . . . .
23"
REFERENCES,0.42702702702702705,"A.6.5
Extended Analysis of Edge Dynamics . . . . . . . . . . . . . . . . . . . .
24"
REFERENCES,0.42882882882882883,"A.6.6
Improved robustness with DSDI+AIT in noise perturbed environments
. .
26"
REFERENCES,0.4306306306306306,"A.6.7
Identiﬁcation of informative intervention targets
. . . . . . . . . . . . . .
27"
REFERENCES,0.43243243243243246,"A.6.8
Limited Intervention Targets . . . . . . . . . . . . . . . . . . . . . . . . .
28"
REFERENCES,0.43423423423423424,"A.7
Continuous Setting: Technical Details and further Results . . . . . . . . . . . . . .
31"
REFERENCES,0.436036036036036,"A.7.1
Integration of AIT into DCDI
. . . . . . . . . . . . . . . . . . . . . . . .
31"
REFERENCES,0.43783783783783786,"A.7.2
Evaluation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31"
REFERENCES,0.43963963963963965,Under review as a conference paper at ICLR 2022
REFERENCES,0.44144144144144143,"A.1
RELATED WORK"
REFERENCES,0.44324324324324327,"Causal induction can use either observational and (or) interventional data. With purely observa-
tional data, the causal graph is only identiﬁable up to a Markov equivalence class (MEC) (Spirtes
et al., 2000), interventions are needed in order to identify the underlying causal graph (Eberhardt &
Scheines, 2007). Our work focuses on causal induction from interventional data."
REFERENCES,0.44504504504504505,"Causal Structure Learning. There exists several approaches for causal induction from interven-
tional data: score-based, constraint-based, conditional independence test based and continuous op-
timization. We refer to (Heinze-Deml et al., 2018; Vowels et al., 2021) for recent overviews. While
most algorithms perform heuristic, guided searches through the discrete space of DAGs, Zheng et al.
(2018) reformulates it as a continuous optimization problem constrained to the zero level set of the
adjacency matrix exponential. This important result has driven recent work in the ﬁeld and showed
promising results (Kalainathan et al., 2018; Yu et al., 2019; Ng et al., 2019; Lachapelle et al., 2020;
Zheng et al., 2020; Zhu et al., 2020). Due to the limitations of purely observational data, Ke et al.
(2019) and Brouillard et al. (2020) extend the continuous optimization framework to make use of
interventional data. Lippe et al. (2021) scales in a concurrent work with ours the work of (Ke
et al., 2019) to higher dimensions by splitting structural edge parameters in separate orientation and
likelihood parameters and leveraging it in an adapted gradient formulation with lower variance. In
contrast to (Brouillard et al., 2020; Ke et al., 2019) and our work, they require interventional data on
every variable."
REFERENCES,0.44684684684684683,"Active Causal Structure Learning. Interventions are usually hard to perform and in some cases
even impossible (Peters et al., 2017). Minimizing the number of interventions performed is desir-
able. Active causal structure learning addresses this problem, and a number of approaches have been
proposed in the literature. These approaches can be divided into those that select intervention targets
using graph-theoretic frameworks, and those using Bayesian methods and information gain."
REFERENCES,0.4486486486486487,"Graph-theoretic frameworks usually proceed from a pre-speciﬁed MEC or CPDAG (completed par-
tially directed acyclic graph) and either investigate special graph substructures (He & Geng, 2008)
such as cliques (Eberhardt, 2012; Squires et al., 2020), trees (Greenewald et al., 2019), or they prune
and orient edges until a satisfactory solution is reached (Ghassami et al., 2018; 2019; Hyttinen et al.,
2013), perhaps under a cost budget (Kocaoglu et al., 2017a; Lindgren et al., 2018). Their chief limi-
tation is that an incorrect starting CPDAG can prevent reaching the correct graph structure even with
an optimal choice of interventions."
REFERENCES,0.45045045045045046,"The other popular set of techniques involve sampling graphs from the posterior distribution in a
Bayesian framework using MCMC and then selecting the interventions which maximize the in-
formation gain on discrete (Murphy, 2001; Tong & Koller, 2001) or Gaussian (Cho et al., 2016)
variables. The drawbacks of these techniques are poor scaling and the difﬁculty of integrating them
with non-Bayesian methods, except perhaps by bootstrapping (Agrawal et al., 2019)."
REFERENCES,0.45225225225225224,"In contrast to existing work, our base frameworks do not start from a pre-speciﬁed MEC or CPDAG
and existing graph-theoretical approaches are hence not applicable unless we pre-initalize them with
a known skeleton. However, in the case we offer access to a predeﬁned structure in the form of a
MEC or CPDAG, a previously directed edge is likely to be inverted during the ongoing process
which contradicts with the underlying assumptions of existing approaches. Further, we build atop
non-Bayesian frameworks and are therefore limited in applying methods based on information gain
which require access to a posterior distribution over graph structures. While bootstrapping would
allow us to approximate the posterior distribution over graph structures in our non-Bayesian setting,
it is not guaranteed to achieve full support over all graphs since the support is limited to graphs
estimated in the bootstrap procedure (Agrawal et al., 2019). Furthermore, the computational burden
of bootstrap would limit us in scaling to graphs of larger size."
REFERENCES,0.4540540540540541,Under review as a conference paper at ICLR 2022
REFERENCES,0.45585585585585586,"A.2
TWO-STAGE DAG SAMPLING"
REFERENCES,0.45765765765765765,"A.2.1
ALGORITHM OUTLINE"
REFERENCES,0.4594594594594595,"We present an outline of the proposed two-stage DAG sampling procedure which exploits structural
information of the soft-adjacency beyond independent edge conﬁdences. The routine is based on
a graph belief state γ where σ(γ) denotes a soft-adjacency characterization. We start by sampling
topological node orderings from an iterative reﬁned score and construct DAGs in the constrained
space by independent Bernoulli draws over possible edges. We can therefore guarantee DAGness
by construction."
REFERENCES,0.46126126126126127,"The temperature parameter t > 0 of the temperature-scaled softmax can be used to account for
the entropy of the graph belief state. However, in the general setting we suggest to initialize the
parameter to t = 0.1. Note that initializing t →0 results in always picking the maximizing argument
and t →∞results in an uniform distribution."
REFERENCES,0.46306306306306305,"Algorithm 2 Two-Stage DAG Sampling
Input: Graph Belief State σ(γ) in the form of a soft-adjacency matrix
Output: DAG Adjacency Matrix ADag"
REFERENCES,0.4648648648648649,"▷Phase 1: Sample Node Ordering ≺
1: A0 ←σ(γ)
2: nodes ←[0, ..., N −1]
3: for k = 0 to N −1 do
4:
pk
c(i) ←max Ak[i, :]
5:
pk
r(i) ←1 −pk
c(i)"
REFERENCES,0.4666666666666667,"6:
pk
s(i) ←
exp[pk
r(i)/t]
P"
REFERENCES,0.46846846846846846,"j exp[pkr(j)/t]
7:
rk ←nodes[idxk] where idxk ∼Categorical(pk
s)
8:
Remove rk from nodes
9:
Ak+1 ←Ak[nodes, nodes]
10: end for
11: ≺= [r0, ..., rN−1]"
REFERENCES,0.4702702702702703,▷Phase 2: Sample DAG based on node ordering ≺
REFERENCES,0.4720720720720721,"12: AP erm ←Permute σ(γ) according to ≺
13: AP erm ←Constrain upper diagonal part by setting values to 0
14: ABer ←Bernoulli(AP erm)
15: ADag ←Apply inverse permutation of ≺to ABer"
REFERENCES,0.47387387387387386,"A.2.2
CONNECTION TO PLACKETT-LUCE DISTRIBUTION (LUCE, 1959; PLACKETT, 1975)"
REFERENCES,0.4756756756756757,"Our proposed node ordering sampling routine can be regarded as an extension of the Placket-Luce
distribution over node permutations. In contrast, we reﬁne scores in an iterative fashion rather than
setting them apriori as we account for previously drawn nodes to estimate the probability of a node
being the root node in the current iteration."
REFERENCES,0.4774774774774775,Under review as a conference paper at ICLR 2022
REFERENCES,0.47927927927927927,"A.3
EXPERIMENTAL SETUP"
REFERENCES,0.4810810810810811,"A huge variety of SCMs and their induced DAGs exist, each of which can stress causal structure
discovery algorithms in different ways. In this work, We perform a systematic evaluation over a
selected set of synthetic and non-synthetic SCMs. We distinguish between discrete (based on DSDI
(Ke et al., 2019)) or continuous (based on DCDI (Brouillard et al., 2020)) valued random variables.
Through all experiment, we limit us to 1000 samples per intervention."
REFERENCES,0.4828828828828829,"A.3.1
SYNTHETIC DATASETS"
REFERENCES,0.4846846846846847,"Graph Structure. We adopt the structured graphs (see Fig. 9) proposed in the work of DSDI
(Ke et al., 2019) as they adequately represent topological diversity of possible DAGs in a compact
fashion. They can be split up in a set of graphs without cycles in the undirected skeletons, and one
group with cycles. Extending the setup with random graphs with varying edge densities, generated
from the Erd˝os–R´enyi (ER) model, allows us to assess the generalized performance of the proposed
method from sparse to dense DAGs."
REFERENCES,0.4864864864864865,"Discrete Data Generation. We adopt the generative setup of DSDI (Ke et al., 2019) and model the
SCMs using two-layer MLPs with Leaky ReLU activations between layers. For every variable Xi,
a seperate MLP models the conditional relationship P(Xi|Xpa(i)). The MLP parameters are initial-
ized orthogonally within the range of [−2.5, 2.5] and biases uniformly in the range of [−1.1, 1.1]."
REFERENCES,0.4882882882882883,"Continuous Data Generation. For the evaluation of the adapted DCDI framework, we adopt their
generative setup as described in (Brouillard et al., 2020) and use the existing non-linear datasets."
REFERENCES,0.4900900900900901,Graphs with acyclic skeletons:
REFERENCES,0.4918918918918919,"(a) Chain
(b) Collider
(c) Tree"
REFERENCES,0.4936936936936937,Graphs with cyclic skeletons:
REFERENCES,0.4954954954954955,"(d) Bidiag
(e) Jungle
(f) Full"
REFERENCES,0.4972972972972973,Figure 9: Visualization of Structured Graphs as proposed in Ke et al. (2019) - adapted illustration
REFERENCES,0.4990990990990991,Under review as a conference paper at ICLR 2022
REFERENCES,0.5009009009009009,"A.3.2
REAL-WORLD DATASETS"
REFERENCES,0.5027027027027027,"Besides the many synthetic graphs, we evaluate our method on real-world datasets provided by the
BnLearn data repository. Namely on the Asia (Lauritzen & Spiegelhalter, 1988) and the Sachs
(Sachs et al., 2005) datasets (see Fig. 10 for a visualization of their underlying ground-truth struc-
ture). Sachs (Sachs et al., 2005) represents a systems biology dataset which exhibits non-linearity,
confounding and complex structure."
REFERENCES,0.5045045045045045,"(a) Asia
(b) Sachs"
REFERENCES,0.5063063063063064,"Figure 10:
Ground-truth structure of the evaluated real-world datasets provided by the Bn-
Learn data repository - Illustration from: https://www.bnlearn.com/bnrepository/
discrete-small.html"
REFERENCES,0.5081081081081081,"A.4
AVAILABILITY OF USED (EXISTING) ASSETS"
REFERENCES,0.5099099099099099,Base Frameworks.
REFERENCES,0.5117117117117117,"• DSDI (Ke et al., 2019): https://github.com/nke001/causal_learning_unknown_
interventions"
REFERENCES,0.5135135135135135,"• DCDI (Brouillard et al., 2020): https://github.com/slachapelle/dcdi"
REFERENCES,0.5153153153153153,Baseline Methods.
REFERENCES,0.5171171171171172,"• GES (Chickering, 2002) and GIES (Hauser & B¨uhlmann, 2012):
www.github.com/
FenTechSolutions/CausalDiscoveryToolbox (Kalainathan & Goudet, 2019)"
REFERENCES,0.518918918918919,"• ICP (Peters et al., 2016): https://github.com/juangamella/aicp"
REFERENCES,0.5207207207207207,"• A-ICP (Gamella & Heinze-Deml, 2020): https://github.com/juangamella/aicp"
REFERENCES,0.5225225225225225,"• NOTEARS (Zheng et al., 2018): https://github.com/xunzheng/notears"
REFERENCES,0.5243243243243243,"• DAG-GNN (Yu et al., 2019): https://github.com/fishmoon1234/DAG-GNN"
REFERENCES,0.5261261261261261,Datasets.
REFERENCES,0.527927927927928,• BnLearn Data Repository: https://www.bnlearn.com/bnrepository/
REFERENCES,0.5297297297297298,Under review as a conference paper at ICLR 2022
REFERENCES,0.5315315315315315,"A.5
HYPER-PARAMETERS"
REFERENCES,0.5333333333333333,"We used a similar set of hyperparameters for our AIT + DSDI and AIT + DCDI models as those
used in the original paper (Ke et al., 2019; Brouillard et al., 2020). The speciﬁc hyperparamters we
used are stated as follows. DSDI."
REFERENCES,0.5351351351351351,"Table 3: Hyperparameters for DSDI including the corresponding
AIT parameters"
REFERENCES,0.5369369369369369,"Number of iterations
1000
Batch size
256
Sparsity Regularizer
0.1
DAG Regularizer
0.5
Functional parameter training iterations
10000
Number of interventions per phase 2
25
Number of data batches for scoring
10
Number of graph conﬁgurations for scoring
- Graph Size 5:
10
- Graph Size 10:
20
- Graph Size 15
40"
REFERENCES,0.5387387387387388,"AIT:
- Number of graph conﬁgurations
100
- Number of interventional samples per graph & target
256 DCDI."
REFERENCES,0.5405405405405406,"Table 4: Hyperparameters for DCDI including the corresponding
AIT parameters"
REFERENCES,0.5423423423423424,"µ0
10−8
γ0
0
η
2
δ
0.9
Augmented Lagrangian Thresh
10−8"
REFERENCES,0.5441441441441441,"Learning rate
10−3
Nr. of hidden units
16
Nr. of hidden layers
2"
REFERENCES,0.5459459459459459,"AIT:
- Number of graph conﬁgurations
100
- Number of interventional samples per graph & target
256"
REFERENCES,0.5477477477477477,Under review as a conference paper at ICLR 2022
REFERENCES,0.5495495495495496,"A.6
DISCRETE SETTING: ADDITIONAL EXPERIMENTS / RESULTS"
REFERENCES,0.5513513513513514,"In this section, we show further results and visualizations of experiments on discrete data and
single-target interventions in various settings (such as graphs of varying size, noise-free vs. noise-
perturbed, limited intervention targets). All experiments are based on the framework DSDI."
REFERENCES,0.5531531531531532,"A.6.1
EVALUATION (SHD) ON GRAPHS OF VARYING SIZE AND DENSITY"
REFERENCES,0.554954954954955,"Table 5: SHD (lower is better) on various 5-variable synthetic datasets. Structured graphs are sorted
in ascending order according to their edge density.
(∗) denotes average SHD over 10 random
graphs.,
†ER-2 graphs on 5 results in the full5 graph and ER-4 graphs on 5 node graphs are non-
existing"
REFERENCES,0.5567567567567567,"Structured Graphs
Random Graphs"
REFERENCES,0.5585585585585585,"Chain
Collider
Tree
Bidiag
Jungle
Full
ER-1(∗)
ER-2(∗)
ER-4(∗)"
REFERENCES,0.5603603603603604,"GES (Chickering, 2002)
3
0
4
6
4
9
4.3 (±1.0)
†
†
GIES (Hauser & B¨uhlmann, 2012)
3
4
2
6
5
10
4.7 (±1.6)
†
†
ICP (Peters et al., 2016)
4
4
4
7
6
10
5.4 (±1.4)
†
†
A-ICP (Gamella & Heinze-Deml, 2020)
4
4
4
7
6
10
5.4 (±1.4)
†
†"
REFERENCES,0.5621621621621622,"NOTEARS (Zheng et al., 2018)
5
3
6
5
7
9
6.1 (±1.7)
†
†
DAG-GNN (Yu et al., 2019)
4
4
3
4
6
9
5.1 (±1.4)
†
†"
REFERENCES,0.563963963963964,"DSDI (Random) (Ke et al., 2019)
0
0
0
0
0
0
0.0 (±0.0)
†
†
DSDI (AIT)
0
0
0
0
0
0
0.0 (±0.0)
†
†"
REFERENCES,0.5657657657657658,"Table 6: SHD (lower is better) on various 10-variable synthetic datasets. Structured graphs are
sorted in ascending order according to their edge density.
(∗) denotes average SHD over 10 random
graphs."
REFERENCES,0.5675675675675675,"Structured Graphs
Random Graphs"
REFERENCES,0.5693693693693693,"Chain
Collider
Tree
Bidiag
Jungle
Full
ER-1(∗)
ER-2(∗)
ER-4(∗)"
REFERENCES,0.5711711711711712,"GES (Chickering, 2002)
9
2
6
8
10
35
7.0 (±1.6)
10.7 (±3.8)
26.7 (±2.9)
GIES (Hauser & B¨uhlmann, 2012)
12
6
13
16
9
20
12.2 (±5.1)
14.1 (±4.7)
26.1 (±4.4)
ICP (Peters et al., 2016)
9
9
9
17
16
45
10.6 (±2.5)
20.7 (±3.3)
39.8 (±1.9)
A-ICP (Gamella & Heinze-Deml, 2020)
9
9
9
17
16
45
10.6 (±2.5)
20.7 (±3.3)
39.8 (±1.9)"
REFERENCES,0.572972972972973,"NOTEARS (Zheng et al., 2018)
13
16
12
21
21
42
16.4 (±3.4)
22.9 (±2.9)
36.6 (±2.6)
DAG-GNN (Yu et al., 2019)
8
7
6
15
13
38
10.3 (±2.8)
20.1 (±3.5)
38.4 (±1.9)"
REFERENCES,0.5747747747747748,"DSDI (Random) (Ke et al., 2019)
0
0
0
0
0
0
0.0 (±0.0)
0.0 (±0.0)
0.0 (±0.0)
DSDI (AIT)
0
0
0
0
0
0
0.0 (±0.0)
0.0 (±0.0)
0.0 (±0.0)"
REFERENCES,0.5765765765765766,"Table 7: SHD (lower is better) on various 15-variable synthetic datasets. Structured graphs are
sorted in ascending order according to their edge density.
(∗) denotes average SHD over 10 random
graphs."
REFERENCES,0.5783783783783784,"Structured Graphs
Random Graphs"
REFERENCES,0.5801801801801801,"Chain
Collider
Tree
Bidiag
Jungle
Full
ER-1(∗)
ER-2(∗)
ER-4(∗)"
REFERENCES,0.581981981981982,"GES (Chickering, 2002)
13
1
12
14
14
69
8.3 (±1.9)
17.6 (±4.6)
39.4 (±6.7)
GIES (Hauser & B¨uhlmann, 2012)
13
6
10
17
23
60
10.9 (±4.2)
18.1 (±4.3)
39.3 (±5.6)
ICP (Peters et al., 2016)
14
14
14
27
26
105
16.2 (±3.6)
31.1 (±3.4)
60.1 (±3.9)
A-ICP (Gamella & Heinze-Deml, 2020)
14
14
14
27
26
105
16.2 (±3.6)
31.1 (±3.4)
60.1 (±3.9)"
REFERENCES,0.5837837837837838,"NOTEARS (Zheng et al., 2018)
22
21
26
33
35
93
23.7 (±4.0)
35.8 (±5.2)
59.5 (±3.7)
DAG-GNN (Yu et al., 2019)
11
14
15
27
25
97
16.0 (±3.7)
30.6 (±3.4)
59.7 (±4.1)"
REFERENCES,0.5855855855855856,"DSDI (Random) (Ke et al., 2019)
0
0
2
3
7
24
1.4 (±1.6)
2.1 (±2.3)
7.2 (±2.7)
DSDI (AIT)
0
0
0
0
0
7
0.0 (±0.0)
0.0 (±0.0)
0.0 (±0.0)"
REFERENCES,0.5873873873873874,Under review as a conference paper at ICLR 2022
REFERENCES,0.5891891891891892,"A.6.2
EVALUATION OF CONVERGENCE SPEED ON GRAPHS OF VARYING SIZE AND DENSITY"
REFERENCES,0.590990990990991,"While we have shown the effectiveness of AIT on random ER graphs of size 15 in §4, we observe
similar effects on ER graphs of size 10 (see Figure 11). Overall, the results indicate a greater impact
of our proposed targeting mechanisms on graphs of bigger size compared to random intervention
targeting which poorly scales to graphs of larger size."
REFERENCES,0.5927927927927928,"0
10000
20000
30000
Steps 0 20 40 SHD"
REFERENCES,0.5945945945945946,"0
10000
20000
30000
Steps 0.4 0.6 0.8 1.0"
REFERENCES,0.5963963963963964,% of correct identified GT edges
REFERENCES,0.5981981981981982,(a) ER-1:
REFERENCES,0.6,"0
10000
20000
30000
Steps 0 20 40 SHD"
REFERENCES,0.6018018018018018,"0
10000
20000
30000
Steps 0.4 0.6 0.8 1.0"
REFERENCES,0.6036036036036037,% of correct identified GT edges
REFERENCES,0.6054054054054054,(b) ER-2:
REFERENCES,0.6072072072072072,"0
10000
20000
30000
Steps 0 20 40 SHD"
REFERENCES,0.609009009009009,"0
10000
20000
30000
Steps 0.4 0.6 0.8 1.0"
REFERENCES,0.6108108108108108,% of correct identified GT edges
REFERENCES,0.6126126126126126,(c) ER-4:
REFERENCES,0.6144144144144145,"Figure 11: DSDI with AIT (orange) leads to superior performance over random intervention tar-
geting (blue) on random graphs of size 10 of varying edge densities. Error bands were estimated
using 10 random ER graphs per setting."
REFERENCES,0.6162162162162163,"0
10000
20000
30000
40000
50000
Steps 0 25 50 75 SHD"
REFERENCES,0.618018018018018,"0
10000
20000
30000
40000
50000
Steps 0.4 0.6 0.8 1.0"
REFERENCES,0.6198198198198198,% of correct identified GT edges
REFERENCES,0.6216216216216216,(a) ER-1:
REFERENCES,0.6234234234234234,"0
10000
20000
30000
40000
50000
Steps 0 25 50 75 SHD"
REFERENCES,0.6252252252252253,"0
10000
20000
30000
40000
50000
Steps 0.4 0.6 0.8 1.0"
REFERENCES,0.6270270270270271,% of correct identified GT edges
REFERENCES,0.6288288288288288,(b) ER-2:
REFERENCES,0.6306306306306306,"0
10000
20000
30000
40000
50000
Steps 0 25 50 75 SHD"
REFERENCES,0.6324324324324324,"0
10000
20000
30000
40000
50000
Steps 0.4 0.6 0.8 1.0"
REFERENCES,0.6342342342342342,% of correct identified GT edges
REFERENCES,0.6360360360360361,(c) ER-4:
REFERENCES,0.6378378378378379,"Figure 12: DSDI with AIT (orange) leads to superior performance over random intervention tar-
geting (blue) on random graphs of size 15 of varying edge densities. Error bands were estimated
using 10 random ER graphs per setting."
REFERENCES,0.6396396396396397,Under review as a conference paper at ICLR 2022
REFERENCES,0.6414414414414414,"A.6.3
TARGET SELECTION ANALYSIS FOR GRAPHS OF VARYING SIZE AND DENSITY"
REFERENCES,0.6432432432432432,"We evaluate the distribution of target node selections over multiple DAGs of varying size to
investigate the behaviour of our proposed method. Over all performed experiments, our method
prefers interventions on nodes with greater (downstream) impact on the overall system, i.e. nodes
of higher topological rank in the underlying DAG."
REFERENCES,0.645045045045045,Out-Degree
REFERENCES,0.6468468468468469,Descendants
REFERENCES,0.6486486486486487,In-Degree
REFERENCES,0.6504504504504505,Ancestors
REFERENCES,0.6522522522522523,Topological-Order
REFERENCES,0.654054054054054,CHAIN5
REFERENCES,0.6558558558558558,BIDIAG5
REFERENCES,0.6576576576576577,COLLIDER5 TREE5
REFERENCES,0.6594594594594595,JUNGLE5 FULL5
REFERENCES,0.6612612612612613,ER-1(*)
REFERENCES,0.6630630630630631,"0.83
0.34
0.31
-0.34
-0.34"
REFERENCES,0.6648648648648648,"0.65
0.34
0.06
-0.34
-0.34"
REFERENCES,0.6666666666666666,"0.82
0.82
-0.82
-0.82
-0.82"
REFERENCES,0.6684684684684684,"0.10
-0.13
0.37
-0.02
-0.02"
REFERENCES,0.6702702702702703,"-0.09
-0.09
-0.07
-0.07
-0.07"
REFERENCES,0.6720720720720721,"0.32
0.32
-0.32
-0.32
-0.32"
REFERENCES,0.6738738738738739,"0.33
0.23
-0.24
-0.28
-0.22"
REFERENCES,0.6756756756756757,Out-Degree
REFERENCES,0.6774774774774774,Descendants
REFERENCES,0.6792792792792792,In-Degree
REFERENCES,0.6810810810810811,Ancestors
REFERENCES,0.6828828828828829,Topological-Order
REFERENCES,0.6846846846846847,CHAIN10
REFERENCES,0.6864864864864865,COLLIDER10
REFERENCES,0.6882882882882883,TREE10
REFERENCES,0.69009009009009,BIDIAG10
REFERENCES,0.6918918918918919,JUNGLE10
REFERENCES,0.6936936936936937,FULL10
REFERENCES,0.6954954954954955,ER-1(*)
REFERENCES,0.6972972972972973,ER-2(*)
REFERENCES,0.6990990990990991,ER-4(*)
REFERENCES,0.7009009009009008,"-0.27
0.37
-0.27
-0.37
-0.37"
REFERENCES,0.7027027027027027,"-0.28
-0.28
0.28
0.28
0.28"
REFERENCES,0.7045045045045045,"0.53
0.38
-0.28
-0.41
-0.41"
REFERENCES,0.7063063063063063,"-0.03
0.38
-0.24
-0.38
-0.38"
REFERENCES,0.7081081081081081,"0.38
0.37
-0.48
-0.41
-0.41"
REFERENCES,0.7099099099099099,"0.37
0.37
-0.37
-0.37
-0.37"
REFERENCES,0.7117117117117117,"0.11
0.15
-0.27
-0.22
-0.21"
REFERENCES,0.7135135135135136,"0.21
0.30
-0.24
-0.26
-0.27"
REFERENCES,0.7153153153153153,"0.39
0.38
-0.42
-0.38
-0.37"
REFERENCES,0.7171171171171171,Out-Degree
REFERENCES,0.7189189189189189,Descendants
REFERENCES,0.7207207207207207,In-Degree
REFERENCES,0.7225225225225225,Ancestors
REFERENCES,0.7243243243243244,Topological-Order
REFERENCES,0.7261261261261261,CHAIN15
REFERENCES,0.7279279279279279,COLLIDER15
REFERENCES,0.7297297297297297,TREE15
REFERENCES,0.7315315315315315,BIDIAG15
REFERENCES,0.7333333333333333,JUNGLE15
REFERENCES,0.7351351351351352,FULL15
REFERENCES,0.736936936936937,ER-1(*)
REFERENCES,0.7387387387387387,ER-2(*)
REFERENCES,0.7405405405405405,ER-4(*)
REFERENCES,0.7423423423423423,"0.02
0.36
-0.59
-0.36
-0.36"
REFERENCES,0.7441441441441441,"0.04
0.04
-0.04
-0.04
-0.04"
REFERENCES,0.745945945945946,"0.22
0.70
-0.61
-0.60
-0.60"
REFERENCES,0.7477477477477478,"-0.01
0.36
-0.60
-0.36
-0.36"
REFERENCES,0.7495495495495496,"0.61
0.69
-0.77
-0.59
-0.59"
REFERENCES,0.7513513513513513,"0.37
0.37
-0.37
-0.37
-0.37"
REFERENCES,0.7531531531531531,"0.22
0.25
-0.13
-0.09
-0.13"
REFERENCES,0.7549549549549549,"0.28
0.38
-0.26
-0.26
-0.30"
REFERENCES,0.7567567567567568,"0.30
0.38
-0.29
-0.31
-0.36"
REFERENCES,0.7585585585585586,(a) Random Targeting
REFERENCES,0.7603603603603604,Out-Degree
REFERENCES,0.7621621621621621,Descendants
REFERENCES,0.7639639639639639,In-Degree
REFERENCES,0.7657657657657657,Ancestors
REFERENCES,0.7675675675675676,Topological-Order
REFERENCES,0.7693693693693694,CHAIN5
REFERENCES,0.7711711711711712,BIDIAG5
REFERENCES,0.772972972972973,COLLIDER5 TREE5
REFERENCES,0.7747747747747747,JUNGLE5 FULL5
REFERENCES,0.7765765765765765,ER-1(*)
REFERENCES,0.7783783783783784,"0.99
0.78
-0.30
-0.78
-0.78"
REFERENCES,0.7801801801801802,"0.99
0.90
-0.60
-0.90
-0.90"
REFERENCES,0.781981981981982,"1.00
1.00
-1.00
-1.00
-1.00"
REFERENCES,0.7837837837837838,"1.00
0.90
-0.58
-0.79
-0.79"
REFERENCES,0.7855855855855856,"0.98
0.98
-0.86
-0.86
-0.86"
REFERENCES,0.7873873873873873,"0.99
0.99
-0.99
-0.99
-0.99"
REFERENCES,0.7891891891891892,"0.99
0.94
-0.73
-0.74
-0.77"
REFERENCES,0.790990990990991,Out-Degree
REFERENCES,0.7927927927927928,Descendants
REFERENCES,0.7945945945945946,In-Degree
REFERENCES,0.7963963963963964,Ancestors
REFERENCES,0.7981981981981981,Topological-Order
REFERENCES,0.8,CHAIN10
REFERENCES,0.8018018018018018,COLLIDER10
REFERENCES,0.8036036036036036,TREE10
REFERENCES,0.8054054054054054,BIDIAG10
REFERENCES,0.8072072072072072,JUNGLE10
REFERENCES,0.809009009009009,FULL10
REFERENCES,0.8108108108108109,ER-1(*)
REFERENCES,0.8126126126126126,ER-2(*)
REFERENCES,0.8144144144144144,ER-4(*)
REFERENCES,0.8162162162162162,"0.76
0.95
-0.46
-0.95
-0.95"
REFERENCES,0.818018018018018,"0.98
0.98
-0.98
-0.98
-0.98"
REFERENCES,0.8198198198198198,"0.98
0.84
-0.49
-0.82
-0.82"
REFERENCES,0.8216216216216217,"0.92
0.90
-0.42
-0.90
-0.90"
REFERENCES,0.8234234234234235,"0.99
0.94
-0.89
-0.91
-0.91"
REFERENCES,0.8252252252252252,"0.96
0.96
-0.96
-0.96
-0.96"
REFERENCES,0.827027027027027,"0.95
0.92
-0.64
-0.72
-0.74"
REFERENCES,0.8288288288288288,"0.98
0.93
-0.72
-0.82
-0.83"
REFERENCES,0.8306306306306306,"0.96
0.95
-0.91
-0.94
-0.95"
REFERENCES,0.8324324324324325,Out-Degree
REFERENCES,0.8342342342342343,Descendants
REFERENCES,0.836036036036036,In-Degree
REFERENCES,0.8378378378378378,Ancestors
REFERENCES,0.8396396396396396,Topological-Order
REFERENCES,0.8414414414414414,CHAIN15
REFERENCES,0.8432432432432433,COLLIDER15
REFERENCES,0.8450450450450451,TREE15
REFERENCES,0.8468468468468469,BIDIAG15
REFERENCES,0.8486486486486486,JUNGLE15
REFERENCES,0.8504504504504504,FULL15
REFERENCES,0.8522522522522522,ER-1(*)
REFERENCES,0.8540540540540541,ER-2(*)
REFERENCES,0.8558558558558559,ER-4(*)
REFERENCES,0.8576576576576577,"0.46
0.98
-0.51
-0.98
-0.98"
REFERENCES,0.8594594594594595,"0.38
0.38
-0.38
-0.38
-0.38"
REFERENCES,0.8612612612612612,"0.96
0.77
-0.41
-0.92
-0.92"
REFERENCES,0.863063063063063,"0.67
0.97
-0.51
-0.97
-0.97"
REFERENCES,0.8648648648648649,"0.98
0.85
-0.79
-0.97
-0.97"
REFERENCES,0.8666666666666667,"0.93
0.93
-0.93
-0.93
-0.93"
REFERENCES,0.8684684684684685,"0.79
0.79
-0.48
-0.54
-0.59"
REFERENCES,0.8702702702702703,"0.91
0.90
-0.60
-0.73
-0.76"
REFERENCES,0.872072072072072,"0.96
0.94
-0.78
-0.87
-0.88"
REFERENCES,0.8738738738738738,(a) Active Intervention Targeting
REFERENCES,0.8756756756756757,"Figure 13: Correlation scores over graphs of varying size and density between the number of in-
dividual target selections and different topological properties of those targets. AIT shows strong
correlations with the measured properties over all graphs, which indicates a controlled discovery of
the underlying structure through preferential targeting of nodes with greater (downstream) impact
on the overall system."
REFERENCES,0.8774774774774775,Under review as a conference paper at ICLR 2022
REFERENCES,0.8792792792792793,"A.6.4
VISUALIZATION OF TARGET DISTRIBUTION ON STRUCTURED GRAPHS OF SIZE 5"
REFERENCES,0.8810810810810811,"(a) Random Targeting
(b) Active Intervention Targeting"
REFERENCES,0.8828828828828829,"Figure 14: Visualization of target selection on structured graphs of size 5 - bigger node size denotes
more selection of the node. While Random Targeting acts as we expect and selects every node an
uniform amount, AIT prefers targeting of nodes with greater (downstream) impact on the overall
system, i.e. nodes of higher topological order."
REFERENCES,0.8846846846846846,Under review as a conference paper at ICLR 2022
REFERENCES,0.8864864864864865,"A.6.5
EXTENDED ANALYSIS OF EDGE DYNAMICS"
REFERENCES,0.8882882882882883,"We show all edge dynamics of all structured graphs over 15 variables and compare the dynamics of
random targeting to active intervention targeting in a noise-free setting where we have access to all
possible single-target interventions."
REFERENCES,0.8900900900900901,(a) Graph: Chain
REFERENCES,0.8918918918918919,(b) Graph: Tree
REFERENCES,0.8936936936936937,(c) Graph: Collider
REFERENCES,0.8954954954954955,"Figure 15: Edge Dynamics of the examined structured graphs spanning over 15 variables - Part
1: The upper part shows the dynamics of random targeting and the lower of active intervention
targeting."
REFERENCES,0.8972972972972973,Under review as a conference paper at ICLR 2022
REFERENCES,0.8990990990990991,(d) Graph: Bidiag
REFERENCES,0.9009009009009009,(e) Graph: Jungle
REFERENCES,0.9027027027027027,(f) Graph: Full
REFERENCES,0.9045045045045045,"Figure 16: Edge Dynamics of the examined structured graphs spanning over 15 variables - Part
2: The upper part shows the dynamics of random targeting and the lower of active intervention
targeting."
REFERENCES,0.9063063063063063,Under review as a conference paper at ICLR 2022
REFERENCES,0.9081081081081082,"A.6.6
IMPROVED ROBUSTNESS WITH DSDI+AIT IN NOISE PERTURBED ENVIRONMENTS"
REFERENCES,0.9099099099099099,"While section §4 highlights our key ﬁndings in noise-perturbed systems, we examine the impact of
AIT in noise perturbed environments more thoroughly in this section. Therefore, we systematically
analyze experiments under different noise levels in the setting of binary data generated from random
graphs of varying densities. A noise level η denotes the probability of ﬂipping a random variable
and apply it to all measured variables of observational and interventional samples."
REFERENCES,0.9117117117117117,"Evaluating convergence on various ER graphs of varying densities over 10 variables under different
noise levels reveals that the impact of AIT becomes of larger magnitude as the density of the graph
and the noise level increases."
REFERENCES,0.9135135135135135,"Table 8: Performance evaluation (SHD) under different noise level η for structured and random
graphs
(∗) denotes average SHD over 3 random graphs."
REFERENCES,0.9153153153153153,"Chain10
Collider10
Tree10
Bidiag10
Jungle10
Full10
ER-1(∗)
ER-2(∗)
ER-4(∗)"
REFERENCES,0.9171171171171171,"η = 0.0
Random
0
0
0
0
0
0
0.0 (±0.0)
0.0 (±0.0)
0.0 (±0.0)
AIT
0
0
0
0
0
0
0.0 (±0.0)
0.0 (±0.0)
0.0 (±0.0)"
REFERENCES,0.918918918918919,"η = 0.01
Random
0
0
0
0
0
3
0.0 (±0.0)
0.0 (±0.0)
0.6 (±0.5)
AIT
0
0
0
0
0
0
0.0 (±0.0)
0.0 (±0.0)
0.0 (±0.0)"
REFERENCES,0.9207207207207208,"η = 0.02
Random
0
4
0
0
0
12
0.0 (±0.0)
0.0 (±0.0)
6.0 (±1.6)
AIT
0
0
0
0
0
3
0.0 (±0.0)
0.0 (±0.0)
0.0 (±0.0)"
REFERENCES,0.9225225225225225,"η = 0.05
Random
1
9
0
2
1
33
1.3 (±0.5)
8.0 (±2.2)
27.0 (±0.5)
AIT
0
7
0
0
0
23
0.0 (±0.0)
1.3 (±0.5)
18.7 (±1.2)"
REFERENCES,0.9243243243243243,"η = 0.1
Random
9
9
9
16
16
45
11.0 (±0.8)
20.7 (±0.5)
40.0 (±0.8)
AIT
7
9
6
16
15
44
10.3 (±0.5)
20.0 (±0.8)
39.3 (±1.2)"
REFERENCES,0.9261261261261261,(i) ER-1:
REFERENCES,0.9279279279279279,(ii) ER-2:
REFERENCES,0.9297297297297298,(iii) ER-4:
REFERENCES,0.9315315315315316,"(a) η = 0
(b) η = 0.01
(c) η = 0.02
(d) η = 0.05"
REFERENCES,0.9333333333333333,"Figure 17: Convergence behaviour in terms of SHD for random ER graphs of various densities over
10 variables under different noise levels η. Overall, Active Intervention Targeting (orange) clearly
outperforms Random Targeting (blue) over all densities under all noise levels. The performance gap
becomes of larger magnitude as density of the graph and the noise level increases. Error bands were
estimated using 3 random ER graphs per setting."
REFERENCES,0.9351351351351351,Under review as a conference paper at ICLR 2022
REFERENCES,0.9369369369369369,"A.6.7
IDENTIFICATION OF INFORMATIVE INTERVENTION TARGETS"
REFERENCES,0.9387387387387387,"Our proposed method aims to select most informative intervention target(s) Ik∗∈I with respect
to identiﬁability of the underlying structure. We conjecture that such targets yield relatively high
discrepancy between samples drawn under different hypothesis graphs, indicating larger uncertainty
about the target node’s relation to its parents and/or children."
REFERENCES,0.9405405405405406,"In order to evaluate our methods capability of detecting informative intervention targets, we per-
form multiple experiments on structured graph structures (chain5, tree5 and full5) where we
preinitalize the structural belief to the ground-truth structure structure but keeping one edge between
a pair of nodes (i,j) undirected, i.e. σ(γi,j) = σ(γj,i) = 0.5. Throughout the experiments, we vary
the position of the undirected edge and analyze which nodes are targeted by our method."
REFERENCES,0.9423423423423424,"Over all evaluated settings, we can observe how AIT preferentially targets the pair of nodes cor-
responding to the undirected edge, with small preferences towards the source nodes of the correct
directed edge (see in Figure 18 and Figure 19). This observation is in line with our conjecture that
AIT preferentially targets nodes with larger uncertainty about the target node’s relation to its parents
and/or children."
REFERENCES,0.9441441441441442,"Figure 18: AIT chooses informative intervention targets by preferentially identifying and targeting
the pair of nodes corresponding to the undirected edge (nodes are marked red in the distribution of
selected target nodes and edges is visualized red in the graph on the right). - First set of experiments
based on the structured graphs chain5 and tree5."
REFERENCES,0.9459459459459459,Under review as a conference paper at ICLR 2022
REFERENCES,0.9477477477477477,"Figure 19: AIT chooses informative intervention targets by preferentially identifying and targeting
the pair of nodes corresponding to the undirected edge (nodes are marked red in the distribution
of selected target nodes and edges is visualized red in the graph on the right). - Second set of
experiments based on the structured graph full5."
REFERENCES,0.9495495495495495,"A.6.8
LIMITED INTERVENTION TARGETS"
REFERENCES,0.9513513513513514,"While we allow access to all possible single-target interventions in all other experiments, real world
settings are usually more restrictive. Speciﬁc interventions might be either technically impossible
or even unethical, or the experiments might want to prevent interventions upon speciﬁc target nodes
due to increased experiment costs. In order to test the capability of AIT, we limit the set of possible
intervention targets in the following experiments and analyze the resulting behaviour based on DSDI.
We examine speed of convergence and the effect on the target distribution under different scenarios
on structured graphs using DSDI with AIT based on single-target interventions."
REFERENCES,0.9531531531531532,"Scenario 1: We perform experiments on a Chain5 graph where we restrict us on intervening upon
a different node in ﬁve experiment and once allow access to all targets as a comparison."
REFERENCES,0.954954954954955,"Throughout the experiments, we observe that blocking interventions on nodes of a higher topological
level results in greater degradation of the convergence speed compared to blocked intervention on
lower levels (see Figure 20). Furthermore, the distribution of selected targets indicates that our
method preferentially chooses neighboring nodes of a blocked target node in the restricted setting."
REFERENCES,0.9567567567567568,Under review as a conference paper at ICLR 2022
REFERENCES,0.9585585585585585,"Figure 20: Limited intervention targets on Chain5: The impact of the restricted target node (red
circled node) is clearly observable in the convergence speed (left) and distribution of target selections
(middle). The speed of convergence indicates a dependence on the topological characteristic of the
restricted intervention target."
REFERENCES,0.9603603603603603,Under review as a conference paper at ICLR 2022
REFERENCES,0.9621621621621622,"Scenario 2: We perform multiple experiments on a Tree5 graph where we restrict access to dif-
ferent subsets of nodes (e.g. root node, set of all sink nodes) for single-target interventions."
REFERENCES,0.963963963963964,"Similar to the experiments on Chain5, we observe a clear impact of the available intervention
targets on the convergence speed and identiﬁability of the underlying structure (see Figure 21).
While preventing interventions on all sink nodes (node 2, 3 and 4) results in improved convergence
towards the underlying structure, restricted access to the set of nodes which act as causes of other
nodes (node 0 and 1) prevents us from identifying the correct underlying structure."
REFERENCES,0.9657657657657658,"Figure 21: Limited intervention targets on Tree5: The impact of the restricted target nodes (red cir-
cled nodes) is clearly observable in the convergence speed (left) and distribution of target selections
(middle)."
REFERENCES,0.9675675675675676,Under review as a conference paper at ICLR 2022
REFERENCES,0.9693693693693693,"A.7
CONTINUOUS SETTING: TECHNICAL DETAILS AND FURTHER RESULTS"
REFERENCES,0.9711711711711711,"While the original framework of DCDI (Brouillard et al., 2020) proposes a joint-optimization over
the observational and interventional sample space by selecting samples at random, we adapt their
framework to the setting of active causal discovery where we acquire interventional sample in an
adaptive manner. We hypothesize that a controlled selection of informative intervention targets
allows a more rapid and controlled discovery of the underlying causal structure."
REFERENCES,0.972972972972973,"A.7.1
INTEGRATION OF AIT INTO DCDI"
REFERENCES,0.9747747747747748,"Instead of demanding the full interventional target space during the complete optimization as in the
original approach, we split the optimization procedure into different episodes, where AIT is used to
estimate a target space I of size K for each episode. This is done by computing the discrepancy
scores over all possible intervention targets and selecting the K highest scoring targets. During an
episode, we continue by performing L gradient steps using the ﬁxed target space I and reevaluate it
afterwards for the next episode. We visualize the adaption in the following high-level outline of the
individual methodologies."
REFERENCES,0.9765765765765766,Algorithm 3 DCDI
REFERENCES,0.9783783783783784,"1: I ←Full target space of size K = N
2: Run DCDI on I until convergence
⇒"
REFERENCES,0.9801801801801802,Algorithm 4 DCDI + AIT
REFERENCES,0.9819819819819819,"1: for episode e = 0 until convergence do
2:
I ←Estimate target space of size K using AIT
3:
Run L gradient steps of DCDI on I
4: end for"
REFERENCES,0.9837837837837838,"A.7.2
EVALUATION"
REFERENCES,0.9855855855855856,"We evaluate the effectiveness of AIT in the base framework of DCDI in the setting of non-linear,
continuous data generated from random graphs over N = 10 variables and show the potential of our
proposed method."
REFERENCES,0.9873873873873874,"Structural Identiﬁcation / Convergence: Despite their joint optimization formulation is not apriori
designed for the setting of experimental design, an AIT guided version shows superior/competitive
performance in terms of structural identiﬁcation and sample complexity over the original formula-
tion (see Figure 22)."
REFERENCES,0.9891891891891892,"Distribution of Intervention Targets: As in DSDI, we observe strong correlation of the number
of target selections with the measured topological properties of the speciﬁc nodes. This indicates a
controlled discovery of the underlying causal structure through preferential targeting of nodes with
greater (downstream) impact on the overall system. In addition, interventions on variables without
children are drastically reduced (see also §4 for equivalent observations in DSDI)."
REFERENCES,0.990990990990991,"Effect of Target Space Size K: While the original formulation assumes K = N for the com-
plete optimization procedure (i.e. L = 1) and relies on random samples out of the full target
space, our adapted AIT-guided version of DCDI constrains the target space to a subset of targets
for each episode. An ablation study on the size of the target space shows that for all choices of
K ∈{2, 4, 6, 8}, our approach outperforms the original formulation in terms of sample complexity
while achieving same or better performance in terms of SHD."
REFERENCES,0.9927927927927928,Under review as a conference paper at ICLR 2022
REFERENCES,0.9945945945945946,"Figure 22: While DCDI Vanilla assumes access to the full interventional target space through the
complete optimization, the AIT guided DCDI approach reevaluates its interventional target space of
size K = 6 every L = 1000 gradient steps. Among the above evaluated graphs (ground-truth on the
left), DCDI+AIT demonstrates a more rapid identiﬁcation of the underlying causal structure while
achieving same or better performance in terms of SHD. The distribution of selected single-node
intervention targets reveals again its connection to the topological properties of the corresponding
nodes."
REFERENCES,0.9963963963963964,"(a) K = 2
(b) K = 4
(d) K = 6
(d) K = 8"
REFERENCES,0.9981981981981982,"Figure 23: All evaluated target space sizes K ∈{2, 4, 6, 8} show that DCDI+AIT (orange) out-
performs DCDI (blue) in terms of sample complexity while achieving same or better performance.
Error bands were estimated using 10 random ER graphs per setting."
