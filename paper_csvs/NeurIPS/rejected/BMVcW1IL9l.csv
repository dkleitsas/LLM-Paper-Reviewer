Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017761989342806395,"We present a novel confidence refinement scheme that enhances pseudo-labels in
1"
ABSTRACT,0.003552397868561279,"semi-supervised semantic segmentation. Unlike current leading methods, which
2"
ABSTRACT,0.0053285968028419185,"filter pixels with low-confidence teacher predictions in isolation, our approach
3"
ABSTRACT,0.007104795737122558,"leverages the spatial correlation of labels in segmentation maps by grouping
4"
ABSTRACT,0.008880994671403197,"neighboring pixels and considering their pseudo-labels collectively. With this
5"
ABSTRACT,0.010657193605683837,"contextual information, our method, named S4MC, increases the amount of
6"
ABSTRACT,0.012433392539964476,"unlabeled data used during training while maintaining the quality of the pseudo-
7"
ABSTRACT,0.014209591474245116,"labels, all with negligible computational overhead. Through extensive experiments
8"
ABSTRACT,0.015985790408525755,"on standard benchmarks, we demonstrate that S4MC outperforms existing state-
9"
ABSTRACT,0.017761989342806393,"of-the-art semi-supervised learning approaches, offering a promising solution for
10"
ABSTRACT,0.019538188277087035,"reducing the cost of acquiring dense annotations. For example, S4MC achieves
11"
ABSTRACT,0.021314387211367674,"a substantial 6.34 mIoU improvement over the prior state-of-the-art method
12"
ABSTRACT,0.023090586145648313,"on PASCAL VOC 12 with 92 annotated images. The code to reproduce our
13"
ABSTRACT,0.02486678507992895,"experiments is available at https://s4mcontext.github.io/.
14"
INTRODUCTION,0.02664298401420959,"1
Introduction
15"
INTRODUCTION,0.028419182948490232,"Supervised learning has been the driving force behind advancements in modern computer vision,
16"
INTRODUCTION,0.03019538188277087,"including classification (Krizhevsky et al., 2012; Dai et al., 2021), object detection (Girshick, 2015;
17"
INTRODUCTION,0.03197158081705151,"Zong et al., 2022), and segmentation (Zagoruyko et al., 2016; Chen et al., 2018a; Li et al., 2022;
18"
INTRODUCTION,0.03374777975133215,"Kirillov et al., 2023). However, it requires extensive amounts of labeled data, which can be costly and
19"
INTRODUCTION,0.035523978685612786,"time-consuming to obtain. In many practical scenarios, while there is no shortage of available data,
20"
INTRODUCTION,0.037300177619893425,"only a fraction can be labeled due to resource constraints. This challenge has led to the development
21"
INTRODUCTION,0.03907637655417407,"of semi-supervised learning (SSL) (Rasmus et al., 2015; Berthelot et al., 2019b; Sohn et al., 2020a;
22"
INTRODUCTION,0.04085257548845471,"Yang et al., 2022a), a methodology that leverages both labeled and unlabeled data for model training.
23"
INTRODUCTION,0.04262877442273535,"This paper focuses on applying SSL to semantic segmentation, which has applications in various
24"
INTRODUCTION,0.04440497335701599,"areas such as perception for autonomous vehicles (Bartolomei et al., 2020), mapping (Van Etten et al.,
25"
INTRODUCTION,0.046181172291296625,"2018) and agriculture (Milioto et al., 2018). SSL is particularly appealing for segmentation tasks, as
26"
INTRODUCTION,0.047957371225577264,"manual labeling can be prohibitively expensive.
27"
INTRODUCTION,0.0497335701598579,"A widely adopted approach for SSL is pseudo-labeling (Lee, 2013; Arazo et al., 2020). This
28"
INTRODUCTION,0.05150976909413854,"technique dynamically assigns supervision targets to unlabeled data during training based on the
29"
INTRODUCTION,0.05328596802841918,"model’s predictions. To generate a meaningful training signal, it is essential to adapt the predictions
30"
INTRODUCTION,0.055062166962699825,"before integrating them into the learning process. Several techniques have been proposed for that,
31"
INTRODUCTION,0.056838365896980464,"such as using a teacher network to generate supervision to a student network (Hinton et al., 2015).
32"
INTRODUCTION,0.0586145648312611,"The teacher network can be made more powerful during training by applying a moving average to
33"
INTRODUCTION,0.06039076376554174,"the student network’s weights (Tarvainen and Valpola, 2017). Additionally, the teacher may undergo
34"
INTRODUCTION,0.06216696269982238,"weaker augmentations than the student (Berthelot et al., 2019b), simplifying the teacher’s task.
35"
INTRODUCTION,0.06394316163410302,"Figure 1: Confidence refinement. Left: pseudo-labels generated by the teacher network without
refinement. Middle: pseudo-labels obtained from the same model after refinement with marginal
contextual information. Right Top: predicted probabilities of the top two classes of the pixel
highlighted by the red square before, and Bottom: after refinement. S4MC allows additional correct
pseudo labels to propagate."
INTRODUCTION,0.06571936056838366,"However, pseudo-labeling is intrinsically susceptible to confirmation bias, which tends to reinforce
36"
INTRODUCTION,0.0674955595026643,"the model predictions instead of improving the student model. Mitigating confirmation bias becomes
37"
INTRODUCTION,0.06927175843694494,"particularly important when dealing with erroneous predictions made by the teacher network.
38"
INTRODUCTION,0.07104795737122557,"One popular technique to address this issue is confidence-based filtering (Sohn et al., 2020a). This
39"
INTRODUCTION,0.07282415630550622,"approach assigns pseudo-labels only when the model’s confidence surpasses a specified threshold,
40"
INTRODUCTION,0.07460035523978685,"thereby reducing the number of incorrect pseudo-labels. Though simple, this strategy was proven
41"
INTRODUCTION,0.0763765541740675,"effective and inspired multiple improvements in semi-supervised classification (Zhang et al., 2021;
42"
INTRODUCTION,0.07815275310834814,"Rizve et al., 2021), segmentation (Wang et al., 2022), and object detection in images (Sohn et al.,
43"
INTRODUCTION,0.07992895204262877,"2020b; Liu et al., 2021) and 3D scenes (Zhao et al., 2020; Wang et al., 2021). However, the strict
44"
INTRODUCTION,0.08170515097690942,"filtering of the supervision signal leads to extended training periods and, potentially, to overfitting
45"
INTRODUCTION,0.08348134991119005,"when the labeled instances used are insufficient to represent the entire sample distribution. Lowering
46"
INTRODUCTION,0.0852575488454707,"the threshold would allow for higher training volumes at the cost of reduced quality, further hindering
47"
INTRODUCTION,0.08703374777975133,"the performance (Sohn et al., 2020a).
48"
INTRODUCTION,0.08880994671403197,"In response to these challenges, we introduce a novel confidence refinement scheme for the teacher
49"
INTRODUCTION,0.0905861456483126,"network predictions in segmentation tasks, designed to increase the availability of pseudo-labels
50"
INTRODUCTION,0.09236234458259325,"without sacrificing their accuracy. Drawing on the observation that labels in segmentation maps
51"
INTRODUCTION,0.0941385435168739,"exhibit strong spatial correlation, we propose to group neighboring pixels and collectively consider
52"
INTRODUCTION,0.09591474245115453,"their pseudo-labels. When considering pixels in spatial groups, we asses the event-union probability,
53"
INTRODUCTION,0.09769094138543517,"which is the probability that at least one pixel belongs to a given class. We assign a pseudo-label if
54"
INTRODUCTION,0.0994671403197158,"this probability is sufficiently larger that the event-union probability of any other class. By taking
55"
INTRODUCTION,0.10124333925399645,"context into account, our approach Semi-Supervised Semantic Segmentation via Marginal Contextual
56"
INTRODUCTION,0.10301953818827708,"Information (S4MC), enables a relaxed filtering criterion which increases the number of unlabeled
57"
INTRODUCTION,0.10479573712255773,"pixels utilized for learning while maintaining high-quality labeling, as demonstrated in Fig. 1.
58"
INTRODUCTION,0.10657193605683836,"We evaluated S4MC on multiple semi-supervised segmentation benchmarks.
S4MC achieves
59"
INTRODUCTION,0.108348134991119,"significant improvements in performance over previous state-of-the-art methods. In particular,
60"
INTRODUCTION,0.11012433392539965,"we observed a remarkable increase of +6.34 mIoU on PASCAL VOC 12 (Everingham et al., 2010)
61"
INTRODUCTION,0.11190053285968028,"using only 92 annotated images and an increase of +1.85 mIoU on Cityscapes (Cordts et al., 2016)
62"
INTRODUCTION,0.11367673179396093,"using only 186 annotated images. These findings highlight the effectiveness of S4MC in producing
63"
INTRODUCTION,0.11545293072824156,"high-quality segmentation results with minimal labeled data.
64"
RELATED WORK,0.1172291296625222,"2
Related Work
65"
SEMI-SUPERVISED LEARNING,0.11900532859680284,"2.1
Semi-Supervised Learning
66"
SEMI-SUPERVISED LEARNING,0.12078152753108348,"Pseudo-labeling (Lee, 2013) is a popular and effective technique in SSL, where labels are assigned to
67"
SEMI-SUPERVISED LEARNING,0.12255772646536411,"unlabeled data based on model predictions. To make the most of these labels during training, it is
68"
SEMI-SUPERVISED LEARNING,0.12433392539964476,"essential to refine them (Laine and Aila, 2016; Berthelot et al., 2019b,a; Xie et al., 2020). One way to
69"
SEMI-SUPERVISED LEARNING,0.1261101243339254,"achieve this is through consistency regularization (Laine and Aila, 2016; Tarvainen and Valpola, 2017;
70"
SEMI-SUPERVISED LEARNING,0.12788632326820604,"Miyato et al., 2018), which ensures consistent predictions between different views of the unlabeled
71"
SEMI-SUPERVISED LEARNING,0.12966252220248667,"data. Alternatively, a teacher model can be used to obtain pseudo-labels, which are then used to train
72"
SEMI-SUPERVISED LEARNING,0.13143872113676733,"a student model. To ensure that the pseudo-labels are useful, the temperature of the prediction (soft
73"
SEMI-SUPERVISED LEARNING,0.13321492007104796,"pseudo-labels; Berthelot et al., 2019b) can be increased, or the label can be assigned to samples with
74"
SEMI-SUPERVISED LEARNING,0.1349911190053286,"high confidence (hard pseudo-labels; Xie et al., 2020; Sohn et al., 2020a; Zhang et al., 2021). In this
75"
SEMI-SUPERVISED LEARNING,0.13676731793960922,"work we follow the hard pseudo-label assignment approach and improve upon previous methods by
76"
SEMI-SUPERVISED LEARNING,0.13854351687388988,"proposing a confidence refinement scheme.
77"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.14031971580817051,"2.2
Semi-Supervised Semantic Segmentation
78"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.14209591474245115,"In semantic segmentation, most SSL methods rely on a combination of consistency regularization
79"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.1438721136767318,"and the development of augmentation strategies compatible with segmentation tasks. Given the
80"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.14564831261101244,"uneven distribution of labels typically encountered in segmentation maps, techniques such as adaptive
81"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.14742451154529307,"sampling, augmentation, and loss re-weighting are commonly employed (Hu et al., 2021). Feature
82"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.1492007104795737,"perturbations on unlabeled data (Ouali et al., 2020; Zou et al., 2021; Liu et al., 2022b) are also used
83"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.15097690941385436,"to enhance consistency, along with the application of virtual adversarial training (Liu et al., 2022b).
84"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.152753108348135,"Curriculum learning strategies that incrementally increase the proportion of data used over time
85"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.15452930728241562,"are beneficial in exploiting more unlabeled data (Yang et al., 2022b; Wang et al., 2022). A recent
86"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.15630550621669628,"approach introduced by (Wang et al., 2022) cleverly utilizes unreliable predictions by employing
87"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.15808170515097691,"contrastive loss with the least confident classes predicted by the model. However, most existing
88"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.15985790408525755,"works primarily focus on individual pixel label predictions. In contrast, we delve into the contextual
89"
SEMI-SUPERVISED SEMANTIC SEGMENTATION,0.16163410301953818,"information offered by spatial predictions on unlabeled data.
90"
CONTEXTUAL INFORMATION,0.16341030195381884,"2.3
Contextual Information
91"
CONTEXTUAL INFORMATION,0.16518650088809947,"Contextual information encompasses environmental cues that assist in interpreting and extracting
92"
CONTEXTUAL INFORMATION,0.1669626998223801,"meaningful insights from visual perception (Toussaint, 1978; Elliman and Lancaster, 1990).
93"
CONTEXTUAL INFORMATION,0.16873889875666073,"Incorporating spatial context explicitly has been proven beneficial in segmentation tasks, for example,
94"
CONTEXTUAL INFORMATION,0.1705150976909414,"by encouraging smoothness like in the Conditional Random Fields (CRF) method (Chen et al.,
95"
CONTEXTUAL INFORMATION,0.17229129662522202,"2018a) and attention mechanisms (Vaswani et al., 2017; Dosovitskiy et al., 2021; Wang et al., 2020).
96"
CONTEXTUAL INFORMATION,0.17406749555950266,"Combating dependence on context has shown to be useful by Nekrasov et al. (2021). In this work,
97"
CONTEXTUAL INFORMATION,0.17584369449378331,"we leverage the context from neighboring pixel predictions to enhance pseudo-label propagation.
98"
METHOD,0.17761989342806395,"3
Method
99"
OVERVIEW,0.17939609236234458,"3.1
Overview
100"
OVERVIEW,0.1811722912966252,"In semi-supervised semantic segmentation, we are given a labeled training set of images Dℓ=
101

(xℓ
i, yi)
	Nℓ
i=1, and an unlabeled set Du = {xu
i }Nu
i=1 sampled from the same distribution, i.e.,
102

xℓ
i, xu
i
	
∼Dx. Here, y are 2D tensors of shape H × W, assigning a semantic label to each
103"
OVERVIEW,0.18294849023090587,"pixel of x. We aim to train a neural network fθ to predict the semantic segmentation of unseen images
104"
OVERVIEW,0.1847246891651865,"sampled from Dx.
105"
OVERVIEW,0.18650088809946713,"We follow a teacher–student approach (Tarvainen and Valpola, 2017) and train two networks fθs
106"
OVERVIEW,0.1882770870337478,"and fθt that share the same architecture but update their parameters separately. The student network
107"
OVERVIEW,0.19005328596802842,"fθs is trained using supervision from the labeled samples and pseudo-labels created by the teacher’s
108"
OVERVIEW,0.19182948490230906,"predictions for unlabeled ones. The teacher model fθt is updated as an exponential moving average
109"
OVERVIEW,0.1936056838365897,"(EMA) of the student weights. fθs(xi) and fθt(xi) denote the predictions of the student and teacher
110"
OVERVIEW,0.19538188277087035,"models for the xi sample, respectively. At each training step, a batch of Bℓand Bu images is sampled
111"
OVERVIEW,0.19715808170515098,"from Dℓand Du, respectively. The optimization objective can be written as the following loss:
112"
OVERVIEW,0.1989342806394316,"L = Ls + λLu
(1)"
OVERVIEW,0.20071047957371227,Ls = 1 Ml X
OVERVIEW,0.2024866785079929,"xℓ
i,yi∈Bl
ℓCE(fθs(xℓ
i), yi)
(2)"
OVERVIEW,0.20426287744227353,"Lu =
1
Mu X"
OVERVIEW,0.20603907637655416,"xu
i ∈Bu
ℓCE(fθs(xu
i ), ˆyi),
(3)"
OVERVIEW,0.20781527531083482,"where Ls and Lu are the losses over the labeled and unlabeled data correspondingly, λ is a
113"
OVERVIEW,0.20959147424511546,"hyperparameter controlling their relative weight, and ˆyi is the pseudo-label for the i-th unlabeled
114"
OVERVIEW,0.2113676731793961,Unlabeled data EMA
OVERVIEW,0.21314387211367672,Labeled data
OVERVIEW,0.21492007104795738,Student network
OVERVIEW,0.216696269982238,Teacher network DPA
OVERVIEW,0.21847246891651864,"Pseudo Label
Reﬁnement"
OVERVIEW,0.2202486678507993,Pseudo Label Reﬁnement > <
OVERVIEW,0.22202486678507993,"Figure 2: Left: S4MC employs a teacher-student paradigm for semi-supervised segmentation.
Labeled images are used to supervise the student network directly. Both teacher and student networks
process unlabeled images. Predictions from the teacher network are refined and used to evaluate the
margin value, which is then thresholded to produce pseudo-labels that guide the student network.
The threshold, denoted as γt, is dynamically adjusted based on the teacher network’s predictions.
Right: Our confidence refinement module exploits neighboring pixels to adjust per-class predictions,
as detailed in Section 3.2.1. The class distribution of the pixel marked by the yellow circle on the left
is changed. Before refinement, the margin surpasses the threshold and erroneously assigns the blue
class (dog) as a pseudo-label. However, after refinement, the margin significantly reduces, thereby
preventing the propagation of this error."
OVERVIEW,0.22380106571936056,"image. Not every pixel of xi has a corresponding label or pseudo-label, and Ml and Mu denote the
115"
OVERVIEW,0.2255772646536412,"number of pixels with label and assigned pseudo-label in the image batch, respectively.
116"
PSEUDO-LABEL PROPAGATION,0.22735346358792186,"3.1.1
Pseudo-label Propagation
117"
PSEUDO-LABEL PROPAGATION,0.2291296625222025,"For a given image xi, we denote by xi
j,k the pixel in the j-th row and k-th column. We adopt a
118"
PSEUDO-LABEL PROPAGATION,0.23090586145648312,"thresholding-based criterion inspired by (Sohn et al., 2020a). By establishing a score, denoted as κ,
119"
PSEUDO-LABEL PROPAGATION,0.23268206039076378,"which is based on the class distribution predicted by the teacher network, we assign a pseudo-label to
120"
PSEUDO-LABEL PROPAGATION,0.2344582593250444,"a pixel if its score exceeds a threshold γt:
121"
PSEUDO-LABEL PROPAGATION,0.23623445825932504,"ˆyi
j,k =
arg maxc{pc(xi
j,k)}
if κ(xi
j,k; θt) > γt,
ignore
otherwise,
,
(4)"
PSEUDO-LABEL PROPAGATION,0.23801065719360567,"where pc(xi
j,k) is the pixel probability of class c. A commonly used score is given by κ(xi
j,k; θt) =
122"
PSEUDO-LABEL PROPAGATION,0.23978685612788633,"maxc{pc(xi
j,k)}. However, we found that using a pixel-wise margin, inspired by the work of Scheffer
123"
PSEUDO-LABEL PROPAGATION,0.24156305506216696,"et al. (2001) and Shin et al. (2021), produces more stable results. This approach calculates the margin
124"
PSEUDO-LABEL PROPAGATION,0.2433392539964476,"as the difference between the highest and the second-highest values of the probability vector:
125"
PSEUDO-LABEL PROPAGATION,0.24511545293072823,"κmargin(xi
j,k) = max
c {pc(xi
j,k)} −max2
c
{pc(xi
j,k)}.
(5)"
PSEUDO-LABEL PROPAGATION,0.2468916518650089,"3.1.2
Dynamic Partition Adjustment (DPA)
126"
PSEUDO-LABEL PROPAGATION,0.24866785079928952,"Following U2PL (Wang et al., 2022), we use a decaying threshold γt. DPA replaces the fixed threshold
127"
PSEUDO-LABEL PROPAGATION,0.25044404973357015,"with a quantile-based threshold that decreases with time. At each iteration, we set γt as the αt-th
128"
PSEUDO-LABEL PROPAGATION,0.2522202486678508,"quantile of κmargin over all pixels of all images in the batch. αt is defined as follows:
129"
PSEUDO-LABEL PROPAGATION,0.2539964476021314,"αt = α0 · (1 −t/iterations).
(6)"
PSEUDO-LABEL PROPAGATION,0.2557726465364121,"As the model predictions improve with each iteration, gradually lowering the threshold increases the
130"
PSEUDO-LABEL PROPAGATION,0.25754884547069273,"number of propagated pseudo-labels without compromising their quality.
131"
MARGINAL CONTEXTUAL INFORMATION,0.25932504440497334,"3.2
Marginal Contextual Information
132"
MARGINAL CONTEXTUAL INFORMATION,0.261101243339254,"Utilizing contextual information (Section 2.3), we look at surrounding predictions (predictions on
133"
MARGINAL CONTEXTUAL INFORMATION,0.26287744227353466,"neighboring pixels) to refine the semantic map at each pixel. We introduce the concept of “Marginal
134"
MARGINAL CONTEXTUAL INFORMATION,0.26465364120781526,"Contextual Information,” which involves integrating additional information to enhance predictions
135"
MARGINAL CONTEXTUAL INFORMATION,0.2664298401420959,"across all classes. At the same time, reliability-based pseudo-label methods focus on the dominant
136"
MARGINAL CONTEXTUAL INFORMATION,0.2682060390763766,"class only (Sohn et al., 2020a; Wang et al., 2023). Section 3.2.1 describes our confidence refinement,
137"
MARGINAL CONTEXTUAL INFORMATION,0.2699822380106572,"followed by our thresholding strategy and a description of S4MC methodology.
138"
CONFIDENCE MARGIN REFINEMENT,0.27175843694493784,"3.2.1
Confidence Margin Refinement
139"
CONFIDENCE MARGIN REFINEMENT,0.27353463587921845,"We refine the predicted pseudo-label of each pixel by considering the predictions of its neighboring
140"
CONFIDENCE MARGIN REFINEMENT,0.2753108348134991,"pixels. Given a pixel xi
j,k with a corresponding per-class prediction pc(xi
j,k), we examine neighboring
141"
CONFIDENCE MARGIN REFINEMENT,0.27708703374777977,"pixels xi
ℓ,m within an N × N pixel neighborhood surrounding it. We then calculate the probability
142"
CONFIDENCE MARGIN REFINEMENT,0.27886323268206037,"that at least one of the two pixels belongs to class c:
143"
CONFIDENCE MARGIN REFINEMENT,0.28063943161634103,"˜pc(xi
j,k) = pc(xi
j,k) + pc(xi
ℓ,m) −pc(xi
j,k, xi
ℓ,m),
(7)"
CONFIDENCE MARGIN REFINEMENT,0.2824156305506217,"where pc(xi
j,k, xi
ℓ,m) denote the joint probability of both xi
j,k and xi
ℓ,m belonging to the same class c.
144"
CONFIDENCE MARGIN REFINEMENT,0.2841918294849023,"While the model does not predict joint probabilities, it is reasonable to assume a non-negative
145"
CONFIDENCE MARGIN REFINEMENT,0.28596802841918295,"correlation between the probabilities of neighboring pixels. This is largely due to the nature of
146"
CONFIDENCE MARGIN REFINEMENT,0.2877442273534636,"segmentation maps, which are typically piecewise constant. Consequently, any information regarding
147"
CONFIDENCE MARGIN REFINEMENT,0.2895204262877442,"the model’s prediction of neighboring pixels belonging to a specific class should not lead to a reduction
148"
CONFIDENCE MARGIN REFINEMENT,0.2912966252220249,"in the posterior probability of the given pixel also falling into that class. The joint probability can
149"
CONFIDENCE MARGIN REFINEMENT,0.29307282415630553,"thus be bounded from below by assuming independence: pc(xi
j,k, xi
ℓ,m) ⩾pc(xi
j,k) · pc(xi
ℓ,m). By
150"
CONFIDENCE MARGIN REFINEMENT,0.29484902309058614,"substituting this into Eq. (7), we obtain an upper bound for the event union probability:
151"
CONFIDENCE MARGIN REFINEMENT,0.2966252220248668,"˜pc(xi
j,k) ≤pc(xi
j,k) + pc(xi
ℓ,m) −pc(xi
j,k) · pc(xi
ℓ,m).
(8)"
CONFIDENCE MARGIN REFINEMENT,0.2984014209591474,"This formulation allows us to filter out confidence margins that do not exceed the threshold.
152"
CONFIDENCE MARGIN REFINEMENT,0.30017761989342806,"For each class c, we select the neighbor with the maximal information gain using Eq. (8):
153"
CONFIDENCE MARGIN REFINEMENT,0.3019538188277087,"˜pN
c (xi
j,k) = max
ℓ,m ˜pc(xi
j,k).
(9)"
CONFIDENCE MARGIN REFINEMENT,0.3037300177619893,"Computing the event union over all classes employs neighboring predictions to amplify differences
154"
CONFIDENCE MARGIN REFINEMENT,0.30550621669627,"in ambiguous cases. Consider, for instance, an uncertain pixel prediction with a 0.5 probability of
155"
CONFIDENCE MARGIN REFINEMENT,0.30728241563055064,"belonging to one of two classes. If a neighboring pixel has a 0.7 probability of belonging to the
156"
CONFIDENCE MARGIN REFINEMENT,0.30905861456483125,"first class and only a 0.3 probability of belonging to the second, this results in a significant event
157"
CONFIDENCE MARGIN REFINEMENT,0.3108348134991119,"union probabilities margin of 0.2. Similarly, this prediction refinement prevents the creation of
158"
CONFIDENCE MARGIN REFINEMENT,0.31261101243339257,"over-confident predictions that is not supported by additional spatial evidence and helps in reducing
159"
CONFIDENCE MARGIN REFINEMENT,0.31438721136767317,"confirmation bias. The refinement is visualized in Fig. 1. In our experiments, we used a neighborhood
160"
CONFIDENCE MARGIN REFINEMENT,0.31616341030195383,"size of 3 × 3. To determine whether the incorporation of contextual information could be enhanced
161"
CONFIDENCE MARGIN REFINEMENT,0.31793960923623443,"with larger neighborhoods, we conducted an ablation study focusing on the neighborhood size and
162"
CONFIDENCE MARGIN REFINEMENT,0.3197158081705151,"the neighbor selection criterion, as detailed in Table 4a. For larger neighborhoods, we decrease the
163"
CONFIDENCE MARGIN REFINEMENT,0.32149200710479575,"probability contribution of the neighboring pixels with a distance-dependent factor:
164"
CONFIDENCE MARGIN REFINEMENT,0.32326820603907636,"˜pc(xi
j,k) = pc(xi
j,k) + βℓ,m

pc(xi
ℓ,m) −pc(xi
j,k, xi
ℓ,m)

,
(10)"
CONFIDENCE MARGIN REFINEMENT,0.325044404973357,"where βℓ,m = exp
 
−1"
CONFIDENCE MARGIN REFINEMENT,0.3268206039076377,"2(|ℓ−j| + |m −k|)

is a spatial weighting function. Empirically, contextual
165"
CONFIDENCE MARGIN REFINEMENT,0.3285968028419183,"information refinement affects mainly the most probable one or two classes. This aligns well with
166"
CONFIDENCE MARGIN REFINEMENT,0.33037300177619894,"our choice to use the margin confidence (5).
167"
CONFIDENCE MARGIN REFINEMENT,0.3321492007104796,"Considering more than two events (more than one neighbor), one can use the formulation for three or
168"
CONFIDENCE MARGIN REFINEMENT,0.3339253996447602,"four event-union. In practice, we find two event-union using Eq. (10), assign it as pc(xi
j,k), find the
169"
CONFIDENCE MARGIN REFINEMENT,0.33570159857904086,"next desired event using Eq. (9) with the remaining neighbors, and repeat the process.
170"
THRESHOLD SETTING,0.33747779751332146,"3.2.2
Threshold Setting
171"
THRESHOLD SETTING,0.3392539964476021,"Setting a high threshold can mitigate confirmation bias from the teacher model’s “beliefs” transferring
172"
THRESHOLD SETTING,0.3410301953818828,"to the student model. However, this comes at the expense of learning from fewer examples, potentially
173"
THRESHOLD SETTING,0.3428063943161634,"resulting in a less comprehensive model. Dynamic Partition Adjustment (DPA; Wang et al., 2022)
174"
THRESHOLD SETTING,0.34458259325044405,"attempt to address this issue by setting a threshold that decreases over time. We adopt this method in
175"
THRESHOLD SETTING,0.3463587921847247,"determining the threshold from the teacher predictions pre-refinement pc(xi
j,k), but we filter values
176"
THRESHOLD SETTING,0.3481349911190053,"based on ˜pc(xi
j,k). Consequently, more pixels pass the threshold that remains unaffected. We set
177"
THRESHOLD SETTING,0.34991119005328597,"α0 = 0.4, i.e., 60% of raw predictions pass the threshold at t = 0, as this value demonstrated superior
178"
THRESHOLD SETTING,0.35168738898756663,"performance in our experiments. An ablation study for α0 is provided in Table 4b.
179"
THRESHOLD SETTING,0.35346358792184723,"Figure 3: Qualitative results of S4MC. The outputs of two trained models and the annotated ground
truth. The segmentation map predicted by S4MC (Ours) compared to the segmentation map using
no refinement module (Baseline) and to the ground truth. Heat map represents the uncertainty of
the model as κ−1, showing a more confident prediction over certain areas, yielding to a smother
segmentation maps (compared in the red boxes)."
PUTTING IT ALL TOGETHER,0.3552397868561279,"3.3
Putting it All Together
180"
PUTTING IT ALL TOGETHER,0.35701598579040855,"We perform semi-supervised for semantic segmentation by pseudo-labeling pixels using their
181"
PUTTING IT ALL TOGETHER,0.35879218472468916,"neighbors’ contextual information. Labeled images are only fed into the student model, producing the
182"
PUTTING IT ALL TOGETHER,0.3605683836589698,"supervised loss (Eq. (2)). Unlabeled images are fed into the student and teacher models. We sort the
183"
PUTTING IT ALL TOGETHER,0.3623445825932504,"margin based κmargin (Eq. (5)) values of teacher predictions and set γt as described in Section 3.2.2.
184"
PUTTING IT ALL TOGETHER,0.3641207815275311,"The per-class teacher predictions are refined using the weighted union event relaxation, as defined in
185"
PUTTING IT ALL TOGETHER,0.36589698046181174,"Eq. (10). Pixels with higher margin values than γt are assigned with pseudo-labels as described in
186"
PUTTING IT ALL TOGETHER,0.36767317939609234,"Eq. (4), producing the unsupervised loss (Eq. (3)). A visualization of the entire pipeline is depicted in
187"
PUTTING IT ALL TOGETHER,0.369449378330373,"Fig. 2.
188"
PUTTING IT ALL TOGETHER,0.37122557726465366,"The impact of S4MC is demonstrated in Fig. 4, which compares the fraction of pixels that pass the
189"
PUTTING IT ALL TOGETHER,0.37300177619893427,"threshold with and without refinement. (a) Our method makes greater use of unlabeled data during
190"
PUTTING IT ALL TOGETHER,0.3747779751332149,"most of the training process, (b) while the refinement ensures high-quality pseudo-labels. Qualitative
191"
PUTTING IT ALL TOGETHER,0.3765541740674956,"results are presented in Fig. 3, where one can see both the confidence heatmap and the pseudo-labels
192"
PUTTING IT ALL TOGETHER,0.3783303730017762,"with and without the impact of S4MC.
193"
EXPERIMENTS,0.38010657193605685,"4
Experiments
194"
EXPERIMENTS,0.38188277087033745,"This section presents our experimental results. The setup for the different datasets and partition
195"
EXPERIMENTS,0.3836589698046181,"protocols is detailed in Section 4.1. Section 4.2 compares our method against existing approaches and
196"
EXPERIMENTS,0.38543516873889877,"Section 4.3 provides the ablation study. Further implementation details are given in the Appendix.
197"
SETUP,0.3872113676731794,"4.1
Setup
198"
SETUP,0.38898756660746003,"Datasets
In our experiments, we use PASCAL VOC 2012 (Everingham et al., 2010) and Cityscapes
199"
SETUP,0.3907637655417407,"(Cordts et al., 2016) datasets.
200"
SETUP,0.3925399644760213,"The PASCAL VOC dataset comprises 20 object classes (plus background). The dataset includes
201"
SETUP,0.39431616341030196,"2,913 annotated images, divided into a training set of 1,464 images and a validation set of 1,449
202"
SETUP,0.3960923623445826,"images. In addition, the dataset includes 9,118 coarsely annotated training images (Hariharan et al.,
203"
SETUP,0.3978685612788632,"2011), in which only a subset of the pixels are labeled. Following previous research, we conduct two
204"
SETUP,0.3996447602131439,"sets of experiments. The ’classic’ experiment utilizes only the original training set (Wang et al., 2022;
205"
SETUP,0.40142095914742454,"Zou et al., 2021), while the ’coarse’ experiment uses all available data (Wang et al., 2022; Chen et al.,
206"
SETUP,0.40319715808170514,"2021; Hu et al., 2021).
207"
SETUP,0.4049733570159858,"The Cityscapes (Cordts et al., 2016) dataset includes urban scenes from 50 different cities with
208"
SETUP,0.4067495559502664,"30 classes, of which only 19 are typically used for evaluation (Chen et al., 2018a,b). Similarly
209"
SETUP,0.40852575488454707,"to PASCAL, in addition to 2,975 training and 500 validation images, the dataset includes 19,998
210"
SETUP,0.4103019538188277,"coarsely annotated images, which we do not use in our experiment.
211"
SETUP,0.41207815275310833,"0
20
40
60
80
Epoch 0.6 0.7 0.8 0.9 1.0"
SETUP,0.413854351687389,% Data passes threshold
SETUP,0.41563055062166965,"After refinement
Before refinement"
SETUP,0.41740674955595025,"(a) Data fraction that passes the threshold. The
baseline model has a fixed percentage, as it is based
on DPA. Our method increases the number of pixels
assigned pseudo-label, mostly in the early stage of the
training when the model is under-confident."
SETUP,0.4191829484902309,"0
20
40
60
80
Epoch 0.90 0.92 0.94 0.96 0.98 1.00"
SETUP,0.42095914742451157,% Correct pseudo-labels
SETUP,0.4227353463587922,"Before refinement
After refinement"
SETUP,0.42451154529307283,"(b) Fraction of correct pseudo-labels the assigned
pseudo-labels with the correct class divided by the total
assigned pseudo-label. S4MC produces more quality
pseudo-labels during the training process, most notably
at the early stages."
SETUP,0.42628774422735344,"Figure 4: Pseudo-label quantity and quality on PASCAL VOC 2012 (Everingham et al., 2010) with
366 labeled images using our margin (5) confidence function."
SETUP,0.4280639431616341,"Table 1: Comparison between our method and prior art on the PASCAL VOC 2012 val on different
partition protocols. the caption describes the share of the training set used as labeled data and, in
parentheses, the actual number of labeled images. Larger improvement can be observed for partitions
of extremely low annotated data, where other methods suffer from starvation due to poor teacher
generalization."
SETUP,0.42984014209591476,"Method
1/16 (92)
1/8 (183)
1/4 (366)
1/2 (732)
Full (1464)"
SETUP,0.43161634103019536,"Supervised Only
45.77
54.92
65.88
71.69
72.50"
SETUP,0.433392539964476,"CutMix-Seg (French et al., 2020)
52.16
63.47
69.46
73.73
76.54
PseudoSeg (Zou et al., 2021)
57.60
65.50
69.14
72.41
73.23
PC2Seg (Zhong et al., 2021)
57.00
66.28
69.78
73.05
74.15
CPS (Chen et al., 2021)
64.10
67.40
71.70
75.90
-
ReCo (Liu et al., 2022a)
64.80
72.0
73.10
74.70
-
ST++ (Yang et al., 2022b)
65.2
71.0
74.6
77.3
79.1
U2PL (Wang et al., 2022)
67.98
69.15
73.66
76.16
79.49
PS-MT (Liu et al., 2022b)
65.8
69.6
76.6
78.4
80.0"
SETUP,0.4351687388987567,"S4MC + CutMix-Seg (Ours)
70.96
71.69
75.41
77.73
80.58
S4MC + FixMatch (Ours)
74.32
75.62
77.84
79.72
81.51"
SETUP,0.4369449378330373,"Implementation details
We implement S4MC on top of two framework variants: CutMix-Seg
212"
SETUP,0.43872113676731794,"(French et al., 2020) and FixMatch (Sohn et al., 2020a). Both use DeepLabv3+ (Chen et al., 2018b)
213"
SETUP,0.4404973357015986,"with a Imagenet-pre-trained (Russakovsky et al., 2015) ResNet-101 (He et al., 2016). The teacher
214"
SETUP,0.4422735346358792,"parameters θt are updated via an exponential moving average (EMA) of the student parameters
215"
SETUP,0.44404973357015987,"Tarvainen and Valpola (2017): θη
t = τθη−1
t
+ (1 −τ)θη
s, where 0 ≤τ ≤1 defines how close the
216"
SETUP,0.44582593250444047,"teacher is to the student and η denotes the training iteration. We used τ = 0.99. Additional details
217"
SETUP,0.44760213143872113,"are provided in Appendix D.
218"
SETUP,0.4493783303730018,"Evaluation
We compare S4MC with baselines under the common partition protocols – using 1/2,
219"
SETUP,0.4511545293072824,"1/4, 1/8, and 1/16 of the training data as labeled data. For the ’classic’ setting of the PASCAL
220"
SETUP,0.45293072824156305,"experiment, we additionally compare using all the finely annotated images. We follow standard
221"
SETUP,0.4547069271758437,"protocols and use mean Intersection over Union (mIoU) as our evaluation metric. We use the data
222"
SETUP,0.4564831261101243,"split published by Wang et al. (2022) when available to ensure a fair comparisons. For the ablation
223"
SETUP,0.458259325044405,"studies, we use PASCAL VOC 2012 val with 1/4 partition.
224"
SETUP,0.46003552397868563,"Methods in comparison
We compare against popular SSL segmentation methods: CutMix-Seg
225"
SETUP,0.46181172291296624,"(French et al., 2020), CCT (Ouali et al., 2020), GCT (Ke et al., 2020), PseudoSeg (Zou et al., 2021),
226"
SETUP,0.4635879218472469,"CPS (Chen et al., 2021), PC2Seg (Zhong et al., 2021), AEL (Hu et al., 2021), U2PL (Wang et al.,
227"
SETUP,0.46536412078152756,"Table 2: Comparison between our method and prior art on the ’coarse’ PASCAL VOC 2012 val
dataset under different partition protocols, using additional unlabeled data from (Hariharan et al.,
2011). For each partition ratio we included the number of labeled images in parentheses. As in 1,
larger improvements are observed for partitions with less annotated data."
SETUP,0.46714031971580816,"Method
1/16 (662)
1/8 (1323)
1/4 (2646)
1/2 (5291)"
SETUP,0.4689165186500888,"Supervised Only
67.87
71.55
75.80
77.13"
SETUP,0.4706927175843694,"CutMix-Seg (French et al., 2020)
71.66
75.51
77.33
78.21
CCT (Ouali et al., 2020)
71.86
73.68
76.51
77.40
GCT (Ke et al., 2020)
70.90
73.29
76.66
77.98
CPS (Chen et al., 2021)
74.48
76.44
77.68
78.64
AEL (Hu et al., 2021)
77.20
77.57
78.06
80.29
PS-MT (Liu et al., 2022b)
75.5
78.2
78.7
-
U2PL (Wang et al., 2022)
77.21
79.01
79.3
80.50"
SETUP,0.4724689165186501,"S4MC + CutMix-Seg (Ours)
78.49
79.67
79.85
81.11
S4MC + FixMatch (Ours)
80.77
81.9
82.3
83.3"
SETUP,0.47424511545293074,"Table 3: Comparison between our method and prior art on the Cityscapes val dataset under different
partition protocols. Labeled and unlabeled images are selected from the Cityscapes training
dataset.For each partition protocol, the caption gives the share of the training set used as labeled data,
in parentheses, the number of labeled images."
SETUP,0.47602131438721135,"Method
1/16 (186)
1/8 (372)
1/4 (744)
1/2 (1488)"
SETUP,0.477797513321492,"Supervised Only
62.96
69.81
74.08
77.46"
SETUP,0.47957371225577267,"CutMix-Seg (French et al., 2020)
69.03
72.06
74.20
78.15
CCT (Ouali et al., 2020)
69.32
74.12
75.99
78.10
GCT (Ke et al., 2020)
66.75
72.66
76.11
78.34
CPS (Chen et al., 2021)
69.78
74.31
74.58
76.81
AEL (Hu et al., 2021)
74.45
75.55
77.48
79.01
U2PL (Wang et al., 2022)
70.30
74.37
76.47
79.05
PS-MT (Liu et al., 2022b)
-
76.89
77.6
79.09"
SETUP,0.48134991119005327,"S4MC + CutMix-Seg (Ours)
75.03
77.02
78.78
78.86
S4MC + FixMatch (Ours)
76.3
78.25
78.95
79.13"
SETUP,0.48312611012433393,"2022), PS-MT (Liu et al., 2022b), and ST++ (Yang et al., 2022b). “Supervised Only” stands for
228"
SETUP,0.4849023090586146,"supervised training without using any unlabeled data. As a baseline, we use CutMix-Seg (French
229"
SETUP,0.4866785079928952,"et al., 2020).
230"
RESULTS,0.48845470692717585,"4.2
Results
231"
RESULTS,0.49023090586145646,"PASCAL VOC 2012.
Table 1 compares our method with state-of-the-art baselines on the PASCAL
232"
RESULTS,0.4920071047957371,"VOC 2012 dataset. While Table 2 shows the comparison results on the PASCAL VOC 2012 dataset
233"
RESULTS,0.4937833037300178,"with additional coarsely annotated data from SBD (Hariharan et al., 2011). In both setups, S4MC
234"
RESULTS,0.4955595026642984,"outperform all the compared methods in standard partition protocols, both when using labels only for
235"
RESULTS,0.49733570159857904,"the original PASCAL VOC 12 dataset and when using SBD annotations as well. Qualitative results
236"
RESULTS,0.4991119005328597,"are shown in Fig. 3. As can be seen our refinement procedure aids in both adding falsely filtered
237"
RESULTS,0.5008880994671403,"pseduo-labels as well as removing erroneous ones.
238"
RESULTS,0.5026642984014209,"Cityscapes.
Table 3 Presents the comparison results on the Cityscapes val dataset. Table 3
239"
RESULTS,0.5044404973357016,"compares our method with other state-of-the-art methods on the Cityscapes (Cordts et al., 2016)
240"
RESULTS,0.5062166962699822,"dataset under various partition protocols. S4MC outperforms the compared methods in most partitions,
241"
RESULTS,0.5079928952042628,"except for the 1/2 setting, and combined with Fixmatch scheme, S4MC outperforms compared
242"
RESULTS,0.5097690941385435,"approaches across all partitions.
243"
RESULTS,0.5115452930728241,Table 4: The effect of neighborhood size and neighbor selection criterion.
RESULTS,0.5133214920071048,(a) Neighborhood choice.
RESULTS,0.5150976909413855,"Selection criterion
Neighborhood size N"
RESULTS,0.5168738898756661,"3 × 3
5 × 5
7 × 7"
RESULTS,0.5186500888099467,"Random neighbor
73.25
71.1
70.41
Max neighbor
75.41
75.18
74.89
Min neighbor
74.54
74.11
70.28
Two max neighbors
74.14
75.15
74.36"
RESULTS,0.5204262877442274,"(b) α0 in Eq. (6), which controls the initial proportion
of confidence pixels"
RESULTS,0.522202486678508,"20%
30%
40%
50%
60%"
RESULTS,0.5239786856127886,"74.45
73.85
75.41
74.56
74.31"
RESULTS,0.5257548845470693,"Contextual information at inference.
Given that our margin refinement scheme operates through
244"
RESULTS,0.5275310834813499,"prediction adjustments, we explored whether it could be employed at inference time to further enhance
245"
RESULTS,0.5293072824156305,"performance. The results reveal a negligible improvement in the DeepLab-V3-plus model, from an
246"
RESULTS,0.5310834813499112,"85.7 mIOU to 85.71. This underlines that the performance advantage of S4MC primarily derives
247"
RESULTS,0.5328596802841918,"from the adjusted margin, as the most confident class is rarely swapped. A heatmap of the prediction
248"
RESULTS,0.5346358792184724,"over several samples is presented in Fig. 3 and Appendix E.
249"
ABLATION STUDY,0.5364120781527532,"4.3
Ablation Study
250"
ABLATION STUDY,0.5381882770870338,"We ablate different components of our method using the CutMix-Seg framework variant, and evaluated
251"
ABLATION STUDY,0.5399644760213144,"using the Pascal VOC 12 dataset with a partition protocol of 1/4 labeled images.
252"
ABLATION STUDY,0.5417406749555951,"Neighborhood size and neighbor selection criterion.
Our prediction refinement scheme employs
253"
ABLATION STUDY,0.5435168738898757,"event-union probability with neighboring pixels, which depends on the chosen neighbor to pair with
254"
ABLATION STUDY,0.5452930728241563,"the current pixel. To assess this, we tested varying neighborhood sizes (N = 3, 5, 7) and criteria
255"
ABLATION STUDY,0.5470692717584369,"for selecting the neighboring pixel: (a) random, (b) maximal class probability, (c) minimal class
256"
ABLATION STUDY,0.5488454706927176,"probability, and (d) two neighbors, as described in Section 3.2.1. As shown in Table 4a, a small 3 × 3
257"
ABLATION STUDY,0.5506216696269982,"neighborhood with one neighboring pixel of the highest class probability proved most efficient in our
258"
ABLATION STUDY,0.5523978685612788,"experiments.
259"
ABLATION STUDY,0.5541740674955595,"Threshold parameter tuning
As outlined in Section 3.1.2, we utilize a dynamic threshold that
260"
ABLATION STUDY,0.5559502664298401,"depends on an initial value, α0. In Table 4b, we examine the effect of different initial quantiles
261"
ABLATION STUDY,0.5577264653641207,"to establish this threshold. A smaller α0 would propagate too many errors, leading to significant
262"
ABLATION STUDY,0.5595026642984015,"confirmation bias. In contrast, a larger α0 would mask most of the data, resulting in insufficient label
263"
ABLATION STUDY,0.5612788632326821,"propagation, rendering the semi-supervised learning process lengthy and inefficient. We found that
264"
ABLATION STUDY,0.5630550621669627,"an α0 of 40% yields the best performance.
265"
CONCLUSION,0.5648312611012434,"5
Conclusion
266"
CONCLUSION,0.566607460035524,"In this paper, we introduce S4MC, a novel approach for incorporating spatial contextual information
267"
CONCLUSION,0.5683836589698046,"in semi-supervised segmentation. This strategy refines confidence levels and enables us to leverage a
268"
CONCLUSION,0.5701598579040853,"larger portion of unlabeled data. S4MC outperforms existing approaches and achieves state-of-the-art
269"
CONCLUSION,0.5719360568383659,"results on multiple popular benchmarks under various data partition protocols, such as Cityscapes
270"
CONCLUSION,0.5737122557726465,"and Pascal VOC 12. While we believe S4MC offers a good solution to lowering the annotation
271"
CONCLUSION,0.5754884547069272,"requirement, it has several limitations. First, the event-union relaxation is relevant in problems
272"
CONCLUSION,0.5772646536412078,"where spatial coherency is expected. The generalization of our framework to other dense prediction
273"
CONCLUSION,0.5790408525754884,"tasks would necessitate an assessment of whether this relaxation is applicable. Furthermore, our
274"
CONCLUSION,0.5808170515097691,"method employs a fixed shape neighborhood without considering the structure of objects. It would
275"
CONCLUSION,0.5825932504440497,"be intriguing to investigate the use of segmented regions to define new neighborhoods, and this is a
276"
CONCLUSION,0.5843694493783304,"direction we plan to explore in the future.
277"
REFERENCES,0.5861456483126111,"References
278"
REFERENCES,0.5879218472468917,"Eric Arazo, Diego Ortego, Paul Albert, Noel E. O’Connor, and Kevin McGuinness. Pseudo-labeling and
279"
REFERENCES,0.5896980461811723,"confirmation bias in deep semi-supervised learning. In International Joint Conference on Neural Networks,
280"
REFERENCES,0.5914742451154529,"pages 1–8, 2020. doi: 10.1109/IJCNN48605.2020.9207304. URL https://arxiv.org/abs/1908.02983.
281"
REFERENCES,0.5932504440497336,"(cited on p. 1)
282"
REFERENCES,0.5950266429840142,"Luca Bartolomei, Lucas Teixeira, and Margarita Chli. Perception-aware path planning for UAVs using semantic
283"
REFERENCES,0.5968028419182948,"segmentation. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages
284"
REFERENCES,0.5985790408525755,"5808–5815, 2020. doi: 10.1109/IROS45743.2020.9341347. (cited on p. 1)
285"
REFERENCES,0.6003552397868561,"David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin A. Raffel.
286"
REFERENCES,0.6021314387211367,"ReMixMatch: semi-supervised learning with distribution alignment and augmentation anchoring. arXiv
287"
REFERENCES,0.6039076376554174,"preprint, November 2019a. URL https://arxiv.org/abs/1911.09785. (cited on p. 2)
288"
REFERENCES,0.605683836589698,"David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A. Raffel.
289"
REFERENCES,0.6074600355239786,"MixMatch: a holistic approach to semi-supervised learning. In H. Wallach, H. Larochelle, A. Beygelzimer,
290"
REFERENCES,0.6092362344582594,"F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems,
291"
REFERENCES,0.61101243339254,"volume 32. Curran Associates, Inc., 2019b. URL https://proceedings.neurips.cc/paper/2019/
292"
REFERENCES,0.6127886323268206,"hash/1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html. (cited on pp. 1, 2, and 3)
293"
REFERENCES,0.6145648312611013,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. DeepLab:
294"
REFERENCES,0.6163410301953819,"semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs.
295"
REFERENCES,0.6181172291296625,"IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848, 2018a. doi: 10.1109/
296"
REFERENCES,0.6198934280639432,"TPAMI.2017.2699184. URL https://arxiv.org/abs/1412.7062. (cited on pp. 1, 3, and 6)
297"
REFERENCES,0.6216696269982238,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with
298"
REFERENCES,0.6234458259325044,"atrous separable convolution for semantic image segmentation. In European Conference on Computer
299"
REFERENCES,0.6252220248667851,"Vision (ECCV), September 2018b.
URL https://openaccess.thecvf.com/content_ECCV_2018/
300"
REFERENCES,0.6269982238010657,"html/Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper.html. (cited on pp.
301"
REFERENCES,0.6287744227353463,"6 and 7)
302"
REFERENCES,0.6305506216696269,"Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang. Semi-supervised semantic segmentation with
303"
REFERENCES,0.6323268206039077,"cross pseudo supervision. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
304"
REFERENCES,0.6341030195381883,"pages 2613–2622, June 2021.
URL https://openaccess.thecvf.com/content/CVPR2021/html/
305"
REFERENCES,0.6358792184724689,"Chen_Semi-Supervised_Semantic_Segmentation_With_Cross_Pseudo_Supervision_CVPR_
306"
REFERENCES,0.6376554174067496,"2021_paper.html. (cited on pp. 6, 7, and 8)
307"
REFERENCES,0.6394316163410302,"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
308"
REFERENCES,0.6412078152753108,"Uwe Franke, Stefan Roth, and Bernt Schiele.
The Cityscapes dataset for semantic urban scene
309"
REFERENCES,0.6429840142095915,"understanding.
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
310"
REFERENCES,0.6447602131438721,"2016.
URL https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Cordts_
311"
REFERENCES,0.6465364120781527,"The_Cityscapes_Dataset_CVPR_2016_paper.html. (cited on pp. 2, 6, and 8)
312"
REFERENCES,0.6483126110124334,"Zihang Dai, Hanxiao Liu, Quoc V. Le, and Mingxing Tan.
CoAtNet:
marrying convolution and
313"
REFERENCES,0.650088809946714,"attention for all data sizes.
In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wort-
314"
REFERENCES,0.6518650088809946,"man Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 3965–
315"
REFERENCES,0.6536412078152753,"3977. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc//paper/2021/hash/
316"
REFERENCES,0.655417406749556,"20568692db622456cc42a2e853ca21f8-Abstract.html. (cited on p. 1)
317"
REFERENCES,0.6571936056838366,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
318"
REFERENCES,0.6589698046181173,"Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An
319"
REFERENCES,0.6607460035523979,"image is worth 16x16 words: Transformers for image recognition at scale, 2021. (cited on p. 3)
320"
REFERENCES,0.6625222024866785,"Dave G. Elliman and Ian T. Lancaster. A review of segmentation and contextual analysis techniques for
321"
REFERENCES,0.6642984014209592,"text recognition.
Pattern Recognition, 23(3):337–346, 1990.
ISSN 0031-3203.
doi: https://doi.org/
322"
REFERENCES,0.6660746003552398,"10.1016/0031-3203(90)90021-C.
URL https://www.sciencedirect.com/science/article/pii/
323"
REFERENCES,0.6678507992895204,"003132039090021C. (cited on p. 3)
324"
REFERENCES,0.6696269982238011,"Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. The Pascal
325"
REFERENCES,0.6714031971580817,"visual object classes (VOC) challenge. International Journal of Computer Vision, 88(2):303–338, June 2010.
326"
REFERENCES,0.6731793960923623,"doi: 10.1007/s11263-009-0275-4. URL https://doi.org/10.1007/s11263-009-0275-4. (cited on pp.
327"
REFERENCES,0.6749555950266429,"2, 6, 7, 15, and 16)
328"
REFERENCES,0.6767317939609236,"Geoffrey French, Samuli Laine, Timo Aila, Michal Mackiewicz, and Graham D. Finlayson. Semi-supervised
329"
REFERENCES,0.6785079928952042,"semantic segmentation needs strong, varied perturbations. In British Machine Vision Conference. BMVA
330"
REFERENCES,0.6802841918294849,"Press, 2020. URL https://www.bmvc2020-conference.com/assets/papers/0680.pdf. (cited on pp.
331"
REFERENCES,0.6820603907637656,"7 and 8)
332"
REFERENCES,0.6838365896980462,"Ross Girshick. Fast R-CNN. In IEEE International Conference on Computer Vision (ICCV), December
333"
REFERENCES,0.6856127886323268,"2015. URL https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_
334"
REFERENCES,0.6873889875666075,"ICCV_2015_paper.html. (cited on p. 1)
335"
REFERENCES,0.6891651865008881,"Bharath Hariharan, Pablo Arbeláez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours
336"
REFERENCES,0.6909413854351687,"from inverse detectors. In International Conference on Computer Vision, pages 991–998, 2011. doi:
337"
REFERENCES,0.6927175843694494,"10.1109/ICCV.2011.6126343. (cited on pp. 6 and 8)
338"
REFERENCES,0.69449378330373,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image
339"
REFERENCES,0.6962699822380106,"recognition.
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
340"
REFERENCES,0.6980461811722913,"2016. URL https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_
341"
REFERENCES,0.6998223801065719,"Residual_Learning_CVPR_2016_paper.html. (cited on p. 7)
342"
REFERENCES,0.7015985790408525,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
343"
REFERENCES,0.7033747779751333,"arXiv:1503.02531, 2015. (cited on p. 1)
344"
REFERENCES,0.7051509769094139,"Hanzhe Hu, Fangyun Wei, Han Hu, Qiwei Ye, Jinshi Cui, and Liwei Wang.
Semi-supervised semantic
345"
REFERENCES,0.7069271758436945,"segmentation via adaptive equalization learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang,
346"
REFERENCES,0.7087033747779752,"and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages
347"
REFERENCES,0.7104795737122558,"22106–22118. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/
348"
REFERENCES,0.7122557726465364,"file/b98249b38337c5088bbc660d8f872d6a-Paper.pdf. (cited on pp. 3, 6, 7, and 8)
349"
REFERENCES,0.7140319715808171,"Zhanghan Ke, Di Qiu, Kaican Li, Qiong Yan, and Rynson W. H. Lau. Guided collaborative training for pixel-
350"
REFERENCES,0.7158081705150977,"wise semi-supervised learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
351"
REFERENCES,0.7175843694493783,"editors, European Conference on Computer Vision, pages 429–445, Cham, 2020. Springer International
352"
REFERENCES,0.7193605683836589,"Publishing. ISBN 978-3-030-58601-0. URL https://www.ecva.net/papers/eccv_2020/papers_
353"
REFERENCES,0.7211367673179396,"ECCV/html/1932_ECCV_2020_paper.php. (cited on pp. 7 and 8)
354"
REFERENCES,0.7229129662522202,"Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer
355"
REFERENCES,0.7246891651865008,"Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv
356"
REFERENCES,0.7264653641207816,"preprint arXiv:2304.02643, 2023. (cited on p. 1)
357"
REFERENCES,0.7282415630550622,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional
358"
REFERENCES,0.7300177619893428,"neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural
359"
REFERENCES,0.7317939609236235,"Information Processing Systems, volume 25. Curran Associates, Inc., 2012. URL https://papers.nips.
360"
REFERENCES,0.7335701598579041,"cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html. (cited on p. 1)
361"
REFERENCES,0.7353463587921847,"Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In International Conference
362"
REFERENCES,0.7371225577264654,"on Learning Representations, 2016. URL https://openreview.net/forum?id=BJ6oOfqge. (cited on p.
363"
REFERENCES,0.738898756660746,"2)
364"
REFERENCES,0.7406749555950266,"Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural
365"
REFERENCES,0.7424511545293073,"networks. ICML 2013 Workshop: Challenges in Representation Learning (WREPL), July 2013. URL
366"
REFERENCES,0.7442273534635879,"http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf. (cited on
367"
REFERENCES,0.7460035523978685,"pp. 1 and 2)
368"
REFERENCES,0.7477797513321492,"Feng Li, Hao Zhang, Huaizhe xu, Shilong Liu, Lei Zhang, Lionel M. Ni, and Heung-Yeung Shum. Mask DINO:
369"
REFERENCES,0.7495559502664298,"towards a unified transformer-based framework for object detection and segmentation. arXiv preprint, June
370"
REFERENCES,0.7513321492007105,"2022. URL https://arxiv.org/abs/2206.02777. (cited on p. 1)
371"
REFERENCES,0.7531083481349912,"Shikun Liu, Shuaifeng Zhi, Edward Johns, and Andrew J. Davison. Bootstrapping semantic segmentation
372"
REFERENCES,0.7548845470692718,"with regional contrast. In International Conference on Learning Representations, 2022a. URL https:
373"
REFERENCES,0.7566607460035524,"//openreview.net/forum?id=6u6N8WWwYSM. (cited on p. 7)
374"
REFERENCES,0.7584369449378331,"Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira,
375"
REFERENCES,0.7602131438721137,"and Peter Vajda. Unbiased teacher for semi-supervised object detection. In International Conference on
376"
REFERENCES,0.7619893428063943,"Learning Representations, 2021. URL https://openreview.net/forum?id=MJIve1zgR_. (cited on p.
377"
REFERENCES,0.7637655417406749,"2)
378"
REFERENCES,0.7655417406749556,"Yuyuan Liu, Yu Tian, Yuanhong Chen, Fengbei Liu, Vasileios Belagiannis, and Gustavo Carneiro.
379"
REFERENCES,0.7673179396092362,"Perturbed and strict mean teachers for semi-supervised semantic segmentation.
In IEEE/CVF
380"
REFERENCES,0.7690941385435168,"Conference on Computer Vision and Pattern Recognition (CVPR), pages 4258–4267, June 2022b.
381"
REFERENCES,0.7708703374777975,"URL https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Perturbed_and_Strict_
382"
REFERENCES,0.7726465364120781,"Mean_Teachers_for_Semi-Supervised_Semantic_Segmentation_CVPR_2022_paper.html. (cited
383"
REFERENCES,0.7744227353463587,"on pp. 3, 7, and 8)
384"
REFERENCES,0.7761989342806395,"Andres Milioto, Philipp Lottes, and Cyrill Stachniss. Real-time semantic segmentation of crop and weed for
385"
REFERENCES,0.7779751332149201,"precision agriculture robots leveraging background knowledge in CNNs. In IEEE International Conference
386"
REFERENCES,0.7797513321492007,"on Robotics and Automation (ICRA), pages 2229–2235, 2018. doi: 10.1109/ICRA.2018.8460962. URL
387"
REFERENCES,0.7815275310834814,"https://arxiv.org/abs/1709.06764. (cited on p. 1)
388"
REFERENCES,0.783303730017762,"Takeru Miyato, Shin-Ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization
389"
REFERENCES,0.7850799289520426,"method for supervised and semi-supervised learning. IEEE Transactions on Pattern Analysis and Machine
390"
REFERENCES,0.7868561278863233,"Intelligence, 41(8):1979–1993, 2018. doi: 10.1109/TPAMI.2018.2858821. URL https://ieeexplore.
391"
REFERENCES,0.7886323268206039,"ieee.org/abstract/document/8417973. (cited on p. 2)
392"
REFERENCES,0.7904085257548845,"Alexey Nekrasov, Jonas Schult, Or Litany, Bastian Leibe, and Francis Engelmann. Mix3d: Out-of-context data
393"
REFERENCES,0.7921847246891652,"augmentation for 3d scenes. 3DV 2021, 2021. (cited on p. 3)
394"
REFERENCES,0.7939609236234458,"Yassine Ouali, Céline Hudelot, and Myriam Tami.
Semi-supervised semantic segmentation with
395"
REFERENCES,0.7957371225577264,"cross-consistency training.
In IEEE/CVF Conference on Computer Vision and Pattern Recognition
396"
REFERENCES,0.7975133214920072,"(CVPR), June 2020.
URL https://openaccess.thecvf.com/content_CVPR_2020/html/Ouali_
397"
REFERENCES,0.7992895204262878,"Semi-Supervised_Semantic_Segmentation_With_Cross-Consistency_Training_CVPR_2020_
398"
REFERENCES,0.8010657193605684,"paper.html. (cited on pp. 3, 7, and 8)
399"
REFERENCES,0.8028419182948491,"Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised learning
400"
REFERENCES,0.8046181172291297,"with ladder networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in
401"
REFERENCES,0.8063943161634103,"Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://papers.
402"
REFERENCES,0.8081705150976909,"nips.cc/paper/2015/hash/378a063b8fdb1db941e34f4bde584c7d-Abstract.html. (cited on p. 1)
403"
REFERENCES,0.8099467140319716,"Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S. Rawat, and Mubarak Shah. In defense of pseudo-labeling: An
404"
REFERENCES,0.8117229129662522,"uncertainty-aware pseudo-label selection framework for semi-supervised learning. In International Conference
405"
REFERENCES,0.8134991119005328,"on Learning Representations, 2021. URL https://openreview.net/forum?id=-ODN6SbiUU. (cited on
406"
REFERENCES,0.8152753108348135,"p. 2)
407"
REFERENCES,0.8170515097690941,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
408"
REFERENCES,0.8188277087033747,"Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual
409"
REFERENCES,0.8206039076376554,"recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015. doi: 10.1007/
410"
REFERENCES,0.822380106571936,"s11263-015-0816-y. (cited on p. 7)
411"
REFERENCES,0.8241563055062167,"Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden Markov models for information
412"
REFERENCES,0.8259325044404974,"extraction. In Frank Hoffmann, David J. Hand, Niall Adams, Douglas Fisher, and Gabriela Guimaraes, editors,
413"
REFERENCES,0.827708703374778,"Advances in Intelligent Data Analysis, pages 309–318, Berlin, Heidelberg, 2001. Springer Berlin Heidelberg.
414"
REFERENCES,0.8294849023090586,"ISBN 978-3-540-44816-7. URL https://link.springer.com/chapter/10.1007/3-540-44816-0_
415"
REFERENCES,0.8312611012433393,"31. (cited on pp. 4 and 15)
416"
REFERENCES,0.8330373001776199,"Gyungin Shin, Weidi Xie, and Samuel Albanie. All you need are a few pixels: Semantic segmentation with
417"
REFERENCES,0.8348134991119005,"pixelpick. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops,
418"
REFERENCES,0.8365896980461812,"pages 1687–1697, October 2021.
URL https://openaccess.thecvf.com/content/ICCV2021W/
419"
REFERENCES,0.8383658969804618,"ILDAV/html/Shin_All_You_Need_Are_a_Few_Pixels_Semantic_Segmentation_With_ICCVW_
420"
REFERENCES,0.8401420959147424,"2021_paper.html. (cited on pp. 4 and 15)
421"
REFERENCES,0.8419182948490231,"Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A. Raffel, Ekin Do-
422"
REFERENCES,0.8436944937833037,"gus Cubuk, Alexey Kurakin, and Chun-Liang Li.
FixMatch:
simplifying semi-supervised learn-
423"
REFERENCES,0.8454706927175843,"ing with consistency and confidence.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
424"
REFERENCES,0.8472468916518651,"and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 596–
425"
REFERENCES,0.8490230905861457,"608. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/hash/
426"
REFERENCES,0.8507992895204263,"06964dce9addb1c5cb5d6e3d9838f733-Abstract.html. (cited on pp. 1, 2, 3, 4, 5, and 7)
427"
REFERENCES,0.8525754884547069,"Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-
428"
REFERENCES,0.8543516873889876,"supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020b. (cited on p.
429"
REFERENCES,0.8561278863232682,"2)
430"
REFERENCES,0.8579040852575488,"Antti Tarvainen and Harri Valpola.
Mean teachers are better role models: Weight-averaged consistency
431"
REFERENCES,0.8596802841918295,"targets improve semi-supervised deep learning results. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
432"
REFERENCES,0.8614564831261101,"R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems,
433"
REFERENCES,0.8632326820603907,"volume 30. Curran Associates, Inc., 2017.
URL https://proceedings.neurips.cc/paper/2017/
434"
REFERENCES,0.8650088809946714,"hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html. (cited on pp. 1, 2, 3, and 7)
435"
REFERENCES,0.866785079928952,"Godfried T. Toussaint. The use of context in pattern recognition. Pattern Recognition, 10(3):189–204, 1978. ISSN
436"
REFERENCES,0.8685612788632326,"0031-3203. doi: https://doi.org/10.1016/0031-3203(78)90027-4. URL https://www.sciencedirect.
437"
REFERENCES,0.8703374777975134,"com/science/article/pii/0031320378900274.
The Proceedings of the IEEE Computer Society
438"
REFERENCES,0.872113676731794,"Conference. (cited on p. 3)
439"
REFERENCES,0.8738898756660746,"Adam Van Etten, Dave Lindenbaum, and Todd M. Bacastow. SpaceNet: a remote sensing dataset and challenge
440"
REFERENCES,0.8756660746003553,"series. arXiv preprint, June 2018. URL https://arxiv.org/abs/1807.01232. (cited on p. 1)
441"
REFERENCES,0.8774422735346359,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
442"
REFERENCES,0.8792184724689165,"and Illia Polosukhin. Attention is all you need, 2017. (cited on p. 3)
443"
REFERENCES,0.8809946714031972,"He Wang, Yezhen Cong, Or Litany, Yue Gao, and Leonidas J. Guibas.
3DIoUMatch: leveraging IoU
444"
REFERENCES,0.8827708703374778,"prediction for semi-supervised 3D object detection.
In Proceedings of the IEEE/CVF Conference
445"
REFERENCES,0.8845470692717584,"on Computer Vision and Pattern Recognition (CVPR), pages 14615–14624, June 2021.
URL
446"
REFERENCES,0.8863232682060391,"https://openaccess.thecvf.com/content/CVPR2021/html/Wang_3DIoUMatch_Leveraging_
447"
REFERENCES,0.8880994671403197,"IoU_Prediction_for_Semi-Supervised_3D_Object_Detection_CVPR_2021_paper.html.
(cited
448"
REFERENCES,0.8898756660746003,"on p. 2)
449"
REFERENCES,0.8916518650088809,"Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu. Eca-net: Efficient channel
450"
REFERENCES,0.8934280639431617,"attention for deep convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer
451"
REFERENCES,0.8952042628774423,"vision and pattern recognition, pages 11534–11542, 2020. (cited on p. 3)
452"
REFERENCES,0.8969804618117229,"Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides,
453"
REFERENCES,0.8987566607460036,"Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, and Xing Xie. FreeMatch: self-adaptive thresholding
454"
REFERENCES,0.9005328596802842,"for semi-supervised learning.
In International Conference on Learning Representations, 2023.
URL
455"
REFERENCES,0.9023090586145648,"https://openreview.net/forum?id=PDrUPTXJI_A. (cited on p. 5)
456"
REFERENCES,0.9040852575488455,"Yuchao Wang,
Haochen Wang,
Yujun Shen,
Jingjing Fei,
Wei Li,
Guoqiang Jin,
Liwei Wu,
457"
REFERENCES,0.9058614564831261,"Rui Zhao, and Xinyi Le.
Semi-supervised semantic segmentation using unreliable pseudo la-
458"
REFERENCES,0.9076376554174067,"bels.
In IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR),
459"
REFERENCES,0.9094138543516874,"2022. URL https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Semi-Supervised_
460"
REFERENCES,0.911190053285968,"Semantic_Segmentation_Using_Unreliable_Pseudo-Labels_CVPR_2022_paper.html. (cited on
461"
REFERENCES,0.9129662522202486,"pp. 2, 3, 4, 5, 6, 7, and 8)
462"
REFERENCES,0.9147424511545293,"Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc V. Le.
Unsupervised data aug-
463"
REFERENCES,0.91651865008881,"mentation for consistency training.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
464"
REFERENCES,0.9182948490230906,"and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6256–
465"
REFERENCES,0.9200710479573713,"6268. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
466"
REFERENCES,0.9218472468916519,"44feb0096faa8326192570788b38c1d1-Abstract.html. (cited on pp. 2 and 3)
467"
REFERENCES,0.9236234458259325,"Fan Yang, Kai Wu, Shuyi Zhang, Guannan Jiang, Yong Liu, Feng Zheng, Wei Zhang, Chengjie
468"
REFERENCES,0.9253996447602132,"Wang,
and Long Zeng.
Class-aware contrastive semi-supervised learning.
In IEEE/CVF
469"
REFERENCES,0.9271758436944938,"Conference on Computer Vision and Pattern Recognition (CVPR), pages 14421–14430,
June
470"
REFERENCES,0.9289520426287744,"2022a.
URL https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Class-Aware_
471"
REFERENCES,0.9307282415630551,"Contrastive_Semi-Supervised_Learning_CVPR_2022_paper.html. (cited on p. 1)
472"
REFERENCES,0.9325044404973357,"Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao. ST++: make self-training work better for
473"
REFERENCES,0.9342806394316163,"semi-supervised semantic segmentation.
In IEEE/CVF Conference on Computer Vision and Pattern
474"
REFERENCES,0.9360568383658969,"Recognition (CVPR), pages 4268–4277, June 2022b.
URL https://openaccess.thecvf.com/
475"
REFERENCES,0.9378330373001776,"content/CVPR2022/html/Yang_ST_Make_Self-Training_Work_Better_for_Semi-Supervised_
476"
REFERENCES,0.9396092362344582,"Semantic_Segmentation_CVPR_2022_paper.html. (cited on pp. 3, 7, and 8)
477"
REFERENCES,0.9413854351687388,"Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon
478"
REFERENCES,0.9431616341030196,"Yoo.
CutMix:
regularization strategy to train strong classifiers with localizable features.
In
479"
REFERENCES,0.9449378330373002,"IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.
URL https:
480"
REFERENCES,0.9467140319715808,"//openaccess.thecvf.com/content_ICCV_2019/html/Yun_CutMix_Regularization_Strategy_
481"
REFERENCES,0.9484902309058615,"to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.html. (cited on
482"
REFERENCES,0.9502664298401421,"p. 16)
483"
REFERENCES,0.9520426287744227,"Sergey Zagoruyko, Adam Lerer, Tsung-Yi Lin, Pedro O. Pinheiro, Sam Gross, Soumith Chintala, and Piotr
484"
REFERENCES,0.9538188277087034,"Dollár. A MultiPath network for object detection. In Edwin R. Hancock Richard C. Wilson and William A. P.
485"
REFERENCES,0.955595026642984,"Smith, editors, Proceedings of the British Machine Vision Conference (BMVC), pages 15.1–15.12. BMVA
486"
REFERENCES,0.9573712255772646,"Press, September 2016. ISBN 1-901725-59-6. doi: 10.5244/C.30.15. URL https://dx.doi.org/10.
487"
REFERENCES,0.9591474245115453,"5244/C.30.15. (cited on p. 1)
488"
REFERENCES,0.9609236234458259,"Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki.
489"
REFERENCES,0.9626998223801065,"Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. In M. Ranzato, A. Beygelz-
490"
REFERENCES,0.9644760213143873,"imer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing
491"
REFERENCES,0.9662522202486679,"Systems, volume 34, pages 18408–18419. Curran Associates, Inc., 2021. URL https://proceedings.
492"
REFERENCES,0.9680284191829485,"neurips.cc/paper/2021/hash/995693c15f439e3d189b06e89d145dd5-Abstract.html. (cited on
493"
REFERENCES,0.9698046181172292,"pp. 2 and 3)
494"
REFERENCES,0.9715808170515098,"Na Zhao, Tat-Seng Chua, and Gim Hee Lee. SESS: self-ensembling semi-supervised 3D object detection.
495"
REFERENCES,0.9733570159857904,"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
496"
REFERENCES,0.9751332149200711,"June
2020.
URL
https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_SESS_
497"
REFERENCES,0.9769094138543517,"Self-Ensembling_Semi-Supervised_3D_Object_Detection_CVPR_2020_paper.html. (cited on p.
498"
REFERENCES,0.9786856127886323,"2)
499"
REFERENCES,0.9804618117229129,"Yuanyi Zhong, Bodi Yuan, Hong Wu, Zhiqiang Yuan, Jian Peng, and Yu-Xiong Wang. Pixel contrastive-consistent
500"
REFERENCES,0.9822380106571936,"semi-supervised semantic segmentation. In IEEE/CVF International Conference on Computer Vision (ICCV),
501"
REFERENCES,0.9840142095914742,"pages 7273–7282, October 2021. URL https://openaccess.thecvf.com/content/ICCV2021/html/
502"
REFERENCES,0.9857904085257548,"Zhong_Pixel_Contrastive-Consistent_Semi-Supervised_Semantic_Segmentation_ICCV_
503"
REFERENCES,0.9875666074600356,"2021_paper.html. (cited on p. 7)
504"
REFERENCES,0.9893428063943162,"Zhuofan Zong, Guanglu Song, and Yu Liu. DETRs with collaborative hybrid assignments training. arXiv
505"
REFERENCES,0.9911190053285968,"preprint, November 2022. URL https://arxiv.org/abs/2211.12860. (cited on p. 1)
506"
REFERENCES,0.9928952042628775,"Yuliang Zou, Zizhao Zhang, Han Zhang, Chun-Liang Li, Xiao Bian, Jia-Bin Huang, and Tomas Pfister.
507"
REFERENCES,0.9946714031971581,"PseudoSeg: designing pseudo labels for semantic segmentation. In International Conference on Learning
508"
REFERENCES,0.9964476021314387,"Representations, 2021. URL https://openreview.net/forum?id=-TwO99rbVRu. (cited on pp. 3, 6,
509"
REFERENCES,0.9982238010657194,"and 7)
510"
