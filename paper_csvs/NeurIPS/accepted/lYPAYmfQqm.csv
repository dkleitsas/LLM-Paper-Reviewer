Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000819000819000819,"Recent research has shown that Transformers with linear attention are capable
of in-context learning (ICL) by implementing a linear estimator through gradient
descent steps. However, the existing results on the optimization landscape apply
under stylized settings where task and feature vectors are assumed to be IID and
the attention weights are fully parameterized. In this work, we develop a stronger
characterization of the optimization and generalization landscape of ICL through
contributions on architectures, low-rank parameterization, and correlated designs:
(1) We study the landscape of 1-layer linear attention and 1-layer H3, a state-
space model. Under a suitable correlated design assumption, we prove that both
implement 1-step preconditioned gradient descent. We show that thanks to its native
convolution filters, H3 also has the advantage of implementing sample weighting
and outperforming linear attention in suitable settings. (2) By studying correlated
designs, we provide new risk bounds for retrieval augmented generation (RAG)
and task-feature alignment which reveal how ICL sample complexity benefits from
distributional alignment. (3) We derive the optimal risk for low-rank parameterized
attention weights in terms of covariance spectrum. Through this, we also shed light
on how LoRA can adapt to a new distribution by capturing the shift between task
covariances. Experimental results corroborate our theoretical findings. Overall, this
work explores the optimization and risk landscape of ICL in practically meaningful
settings and contributes to a more thorough understanding of its mechanics."
ABSTRACT,0.001638001638001638,a) Correlated features
ABSTRACT,0.002457002457002457,"b) Task-feature alignment: Task and 
feature vectors are ùõº-correlated"
ABSTRACT,0.003276003276003276,"Query
features"
ABSTRACT,0.004095004095004095,Database
ABSTRACT,0.004914004914004914,"Relevant
examples"
ABSTRACT,0.005733005733005733,"Ex: Retrieval Augmented Generation
ATTN
(ùëÑùêæ!ùëâ)"
ABSTRACT,0.006552006552006552,"Linear Attention
H3 (SSM) √ó SSM √ó"
ABSTRACT,0.007371007371007371,"Distributional Alignment
Architecture Choice
Low-rank Parameterization"
ABSTRACT,0.00819000819000819,Pretrained
ABSTRACT,0.009009009009009009,Weights
ABSTRACT,0.009828009828009828,"rank ùëü
+ ùëä! ùëä"""
ABSTRACT,0.010647010647010647,a) Low-rank attention weights:
ABSTRACT,0.011466011466011465,"rank ùëä!, ùëä"" ‚â§ùëü
b) LoRA adaptation"
ABSTRACT,0.012285012285012284,"0
10
20
30
40
50
# in-context samples 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
ABSTRACT,0.013104013104013105,Test risk
ABSTRACT,0.013923013923013924,"Linear Att
H3
Theorem 1"
ABSTRACT,0.014742014742014743,(a) Linear attention=H3
ABSTRACT,0.015561015561015561,"0
10
20
30
40
50
# in-context samples 0.2 0.4 0.6 0.8"
ABSTRACT,0.01638001638001638,Test risk
ABSTRACT,0.0171990171990172,"= 0
= 0.2
= 0.4
= 0.6"
ABSTRACT,0.018018018018018018,(b) RAG with Œ± correlation
ABSTRACT,0.018837018837018837,"0
10
20
30
40
50
# in-context samples 0.1 0.3 0.5 0.7 0.9"
ABSTRACT,0.019656019656019656,Test risk
ABSTRACT,0.020475020475020474,"r = 1
r = 5
r = 10
r = 20"
ABSTRACT,0.021294021294021293,"(c) LoRA
Figure 1: We investigate the optimization landscape of in-context learning from the lens of archi-
tecture choice, the role of distributional alignment, and low-rank parameterization. The empirical
performance (solid curves) are aligned with our theoretical results (dotted curves) from Section 3.
More experimental details and discussion are deferred to Section 4."
INTRODUCTION,0.022113022113022112,"1
Introduction"
INTRODUCTION,0.02293202293202293,"Modern language models exhibit the remarkable ability to learn novel tasks or solve complex problems
from the demonstrations provided within their context window [Brown et al., 2020, GeminiTeam
et al., 2023, OpenAI, 2023, Touvron et al., 2023]. Such in-context learning (ICL) offers a novel and
effective alternative to traditional fine-tuning techniques and has become an important feature of LLM
with its applications spanning retrieval-augmented generation [Lewis et al., 2020], and reasoning via
advanced prompting techniques, such as chain-of-thought [Wei et al., 2022]."
INTRODUCTION,0.02375102375102375,"ICL ability presents an important research avenue to develop stronger theoretical and mechanistic
understanding of large language models. To this aim, there has been significant recent interest in
demystifying ICL through the lens of function approximation [Liu et al., 2023a], Bayesian inference
[M√ºller et al., 2021, Xie et al., 2022, Han et al., 2023], and learning and optimization theory [Ahn
et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Duraisamy, 2024]. The latter is concerned
with understanding the optimization landscape of ICL, which is also crucial for understanding the
generalization properties of the model. A notable result in this direction is the observation that
linear attention models [Schlag et al., 2021, Von Oswald et al., 2023, Ahn et al., 2023] implement
preconditioned gradient descent (PGD) during ICL [Ahn et al., 2023, Mahdavi et al., 2024]. While
this line of works provide a fresh perspective to ICL, the existing studies do not address many
questions arising from real-life applications nor provide guiding principles for various ICL setups
motivated by practical considerations."
INTRODUCTION,0.02457002457002457,"To this aim, we revisit the theoretical exploration of ICL with linear data model where we feed an
in-context prompt containing n examples (xi, yi = x‚ä§
i Œ≤ + Œæi)n
i=1 ‚äÇRd √ó R and a test instance or query
xn+1 ‚ààRd to the model, with d being the feature dimension, Œ≤ ‚ààRd being the task weight vector,
and (Œæi)n
i=1 denoting the noise in individual labels. Given the in-context prompt, the model is tasked
to predict ÀÜyn+1 ‚Äì an estimate for yn+1 = x‚ä§
n+1Œ≤ + Œæn+1. We aim to provide answers to the following
questions by exploring the loss landscape of ICL:"
INTRODUCTION,0.025389025389025387,"(Q1) Is the ability to implement gradient-based ICL unique to (linear) attention? Can alternative
sequence models implement richer algorithms beyond PGD?
(Q2) In language modeling, ICL often works well with few-shot samples whereas standard linear
estimation typically requires O (d) samples. How can we reconcile this discrepancy between
classical learning and ICL?
(Q3) To our knowledge, existing works assume linear-attention is fully parameterized, i.e., key and
query projections Wk,Wq ‚ààRd√ód. What happens when they are low-rank? What happens when
there is distribution shift between training and test in-context prompts and we use LoRA [Hu
et al., 2022] for adaptation?"
INTRODUCTION,0.02620802620802621,"In this work, we conduct a careful investigation of these questions. Specifically, we focus on ICL
with 1-layer models and make the following contributions:"
INTRODUCTION,0.02702702702702703,"(A1) We jointly investigate the landscape of linear attention and H3 [Fu et al., 2023], a widely popu-
lar state-space model (SSM). We prove that under correlated design, both models implement
1-step PGD (c.f. Proposition 1) and the alignments in Fig. 1a verify that where the dotted
curve represents the theoretical PGD result derived from Theorem 1. Our analysis reveals
that the gating mechanism in H3 imitates attention. We also empirically show that H3 has the
advantage of implementing sample-weighting which allows it to outperform linear attention in
temporally-heterogeneous problem settings in Appendix D.
(A2) Proposition 1 allows for task and features to be correlated to each other as long as odd moments
are zero. Through this, we can assess the impact of distributional alignment on the sample
complexity of ICL. Specifically, we characterize the performance of Retrieval Augmented
Generation (RAG) (c.f. Theorem 2 and Fig. 1b) and Task-Feature Alignment (c.f. Theorem 3),
where the in-context examples are Œ±-correlated with either the query or the task vector. For
both settings, we prove that alignment amplifies the effective sample size of ICL by a factor of
Œ±2d + 1, highlighting that aligned data are crucial for the success of ICL in few-shot settings.
(A3) We show that, under low-rank parameterization, optimal attention-weights still implements
PGD according to the truncated eigenspectrum of the fused task-feature covariance (see
Section 3.2). We similarly derive risk upper bounds for LoRA adaptation (c.f. Eq. (14) and
Fig. 1c), and show that, these bounds accurately predict the empirical performance."
PROBLEM SETUP AND PRELIMINARIES,0.027846027846027847,"2
Problem Setup and Preliminaries"
PROBLEM SETUP AND PRELIMINARIES,0.028665028665028666,"We begin with a short note on notation. Let bold lowercase and uppercase letters (e.g., x and X)
represent vectors and matrices, respectively. The symbol ‚äôis defined as the element-wise (Hadamard)
product, and ‚àódenotes the convolution operator. 1d and 0d denote the d-dimensional all-ones and
all-zeros vectors, respectively; and Id denotes the identity matrix of dimension d √ó d. Additionally,
let tr (W) denote the trace of the square matrix W."
PROBLEM SETUP AND PRELIMINARIES,0.029484029484029485,"As mentioned earlier, we study the optimization landscapes of 1-layer linear attention [Katharopoulos
et al., 2020, Schlag et al., 2021] and H3 [Fu et al., 2023] models when training with prompts
containing in-context data following a linear model. We construct the input in-context prompt similar
to Ahn et al. [2023], Mahankali et al. [2024], Zhang et al. [2024] as follows."
PROBLEM SETUP AND PRELIMINARIES,0.030303030303030304,"Linear data distribution. Let (x, y) ‚ààRd √ó R be a (feature, label) pair generated by a d-dimensional
linear model parameterized by Œ≤ ‚ààRd, i.e., y = x‚ä§Œ≤ + Œæ, where x and Œ≤ are feature and task vectors,
and Œæ is the label noise. Given demonstrations (xi, yi)n+1
i=1 sampled from a single Œ≤, define the input
in-context prompt"
PROBLEM SETUP AND PRELIMINARIES,0.031122031122031123,"Z = [z1 . . . zn zn+1]‚ä§=
""
x1
. . .
xn
xn+1
y1
. . .
yn
0"
PROBLEM SETUP AND PRELIMINARIES,0.03194103194103194,"#‚ä§
‚ààR(n+1)√ó(d+1).
(1)"
PROBLEM SETUP AND PRELIMINARIES,0.03276003276003276,"Here, we set zi =
""
xi
yi"
PROBLEM SETUP AND PRELIMINARIES,0.03357903357903358,"#
for i ‚â§n and the last/query token zn+1 =
""
xn+1
0"
PROBLEM SETUP AND PRELIMINARIES,0.0343980343980344,"#
. Then, given Z, the goal of"
PROBLEM SETUP AND PRELIMINARIES,0.03521703521703522,"the model is to predict the correct label yn+1 corresponding to xn+1. For cleaner notation, when it
is clear from context, we drop the subscript n + 1 and set x = xn+1, z = zn+1. Different from the
previous work [Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Mahdavi et al., 2024]
where (xi)n+1
i=1 and Œ≤ are assumed to be independent, our analysis focuses on a more general linear
setting that captures the dependency between (xi)n+1
i=1 and Œ≤."
PROBLEM SETUP AND PRELIMINARIES,0.036036036036036036,"Model architectures. To start with, we first review the architectures of both Transformer and
state-space model (SSM). Similar to the previous work [Von Oswald et al., 2023, Ahn et al., 2023,
Mahankali et al., 2024, Zhang et al., 2024] and to simplify the model structure, we focus on single-
layer models and omit the nonlinearity, e.g., softmax operation and MLP activation, from the
Transformer. Given the input prompt Z ‚ààR(n+1)√ó(d+1) in (1), which can be treated as a sequence of
(d + 1)-dimensional tokens, the single-layer linear attention ATT and H3-like single-layer SSM SSM
are denoted by"
PROBLEM SETUP AND PRELIMINARIES,0.036855036855036855,"ATT(Z) = (ZWqW‚ä§
k Z‚ä§)ZWv
(2a)"
PROBLEM SETUP AND PRELIMINARIES,0.03767403767403767,"SSM(Z) =

(ZWq) ‚äô((ZWk ‚äôZWv) ‚àóf)

(2b)"
PROBLEM SETUP AND PRELIMINARIES,0.03849303849303849,"where Wk, Wq, Wv ‚ààR(d+1)√ó(d+1) denote the key, query and value weight matrices, respectively.
In (2b), the parameter f ‚ààRn+1 is a 1-D convolutional filter that mixes tokens. The Hadamard
product ‚äôis the gating mechanism [Dauphin et al., 2017] between key and query channels, which is
crucial for attention-like feature creation. Thus, (2b) is more generally a gated-convolution layer. For
f only, we use indexing f = [f0 . . . fn]‚ä§‚ààRn+1 and given any vector a, denote convolution output
(a ‚àóf)i = Pi
j=1 fi‚àíja j. Note that our notation slightly differs from the original H3 model [Fu et al.,
2023] in two ways:"
PROBLEM SETUP AND PRELIMINARIES,0.03931203931203931,"1. SSMs provide efficient parameterization of f which would otherwise grow with sequence length.
In essence, H3 utilizes a linear state-space model si = Asi‚àí1 + Bui and yi = Csi with parameters
(A ‚ààRd√ód, B ‚ààRd√ó1, C ‚ààR1√ód) from which the filter f is obtained via the impulse response
fi = CAiB for i ‚â•0. Here d is the state dimension and, in practice, A is chosen to be diagonal.
Observe that, setting d = 1 and A = œÅ, C = B = 1, SSM reduces to the exponential smoothing
fi = œÅi for i ‚â•0. Thus, H3 also captures the all-ones filter as a special instance. As we show in
Proposition 1, this simple filter is optimal under independent data model and exactly imitates
linear attention. Note that, utilizing a filter f as in (2b) is strictly more expressive than the SSM
as it captures all possible impulse responses."
PROBLEM SETUP AND PRELIMINARIES,0.04013104013104013,"2. H3 also applies a shift SSM to the key embeddings to enable the retrieval of the local context
around associative recall hits. We opted not to incorporate this shift operator in our model. This
is because unless the features of the neighboring tokens are correlated (which is not the case for"
PROBLEM SETUP AND PRELIMINARIES,0.04095004095004095,"the typical independent data model), the entry-wise products between values and shifted keys
will have zero mean and be redundant for the final prediction."
PROBLEM SETUP AND PRELIMINARIES,0.04176904176904177,"We note that we conduct all empirical evaluations with the original H3 model, which displays exact
agreement with our theory formalized for (6b), further validating our modeling choice."
IN-CONTEXT LINEAR ESTIMATION,0.042588042588042586,"2.1
In-context Linear Estimation"
IN-CONTEXT LINEAR ESTIMATION,0.043407043407043405,"We will next study the algorithms that can be implemented by the single-layer attention and state-
space models. Through this, we will show that training ATT and SSM with linear ICL data is equivalent
to the prediction obtained from one step of optimally-preconditioned gradient descent (PGD) and
sample-weighted preconditioned gradient descent (WPGD), respectively. We will further show that
under mild assumption, the optimal sample weighting for SSM (e.g., f) is an all-ones vector and
therefore, establishing the equivalence among PGD, ATT, and SSM."
IN-CONTEXT LINEAR ESTIMATION,0.044226044226044224,"Background: 1-step gradient descent. Consider minimizing squared loss and solving linear
regression using one step of PGD and WPGD. Given n samples (xi, yi)n
i=1, define"
IN-CONTEXT LINEAR ESTIMATION,0.04504504504504504,"X = [x1 ¬∑ ¬∑ ¬∑ xn]‚ä§‚ààRn√ód
and
y = [y1 ¬∑ ¬∑ ¬∑ yn]‚ä§‚ààRn."
IN-CONTEXT LINEAR ESTIMATION,0.04586404586404586,"Starting from Œ≤0 = 0d and letting Œ∑ = 1/2 be the step size, a single-step GD preconditioned with
weights W returns prediction"
IN-CONTEXT LINEAR ESTIMATION,0.04668304668304668,"ÀÜy = x‚ä§WX‚ä§y := gPGD(Z),
(3)"
IN-CONTEXT LINEAR ESTIMATION,0.0475020475020475,and a single-step sample-weighted GD given weights œâ ‚ààRn and W ‚ààRd√ód returns prediction
IN-CONTEXT LINEAR ESTIMATION,0.04832104832104832,"ÀÜy = x‚ä§WX‚ä§(œâ ‚äôy) := gWPGD(Z),
(4)"
IN-CONTEXT LINEAR ESTIMATION,0.04914004914004914,"where Z is defined in (1) consisting of X, y and x. Our goal is to find the optimal W, as well as œâ in
(4) that minimize the population risks defined as follows."
IN-CONTEXT LINEAR ESTIMATION,0.049959049959049956,"min
W LPGD(W)
where
LPGD(W) = E
h
(y ‚àígPGD(Z))2i
,
(5a)"
IN-CONTEXT LINEAR ESTIMATION,0.050778050778050775,"min
W,œâ LWPGD(W)
where
LWPGD(W) = E
h
(y ‚àígWPGD(Z))2i
.
(5b)"
IN-CONTEXT LINEAR ESTIMATION,0.051597051597051594,"Here, the expectation is over the randomness in (xi, Œæi)n+1
i=1 and Œ≤, and we use W to represent the set
of corresponding trainable parameters. The search spaces for œâ and W are Rn and Rd√ód, respectively."
IN-CONTEXT LINEAR ESTIMATION,0.05241605241605242,"As per (2), given input prompt Z ‚ààR(n+1)√ó(d+1), either of the underlying models outputs a (n+1)-length
sequence. Note that the label for the query x = xn+1 is excluded from the prompt Z. Similar to Ahn
et al. [2023], Mahankali et al. [2024], we consider a training objective with a causal mask to ensure
inputs cannot attend to their own labels and training can be parallelized. Let Z0 = [z1 . . . zn 0]‚ä§be
the features post-causal masking at time/index n + 1. Given weights Wk,Wq,Wv and the filter f for"
IN-CONTEXT LINEAR ESTIMATION,0.05323505323505324,"SSM, predictions at the query token z =
""
x
0"
IN-CONTEXT LINEAR ESTIMATION,0.05405405405405406,"#
take the following forms following sequence-to-sequence"
IN-CONTEXT LINEAR ESTIMATION,0.054873054873054876,mappings in (2):
IN-CONTEXT LINEAR ESTIMATION,0.055692055692055695,"gATT(Z) = (z‚ä§WqW‚ä§
k Z‚ä§
0 )Z0Wvv,"
IN-CONTEXT LINEAR ESTIMATION,0.056511056511056514,"gSSM(Z) =

(z‚ä§Wq)‚ä§‚äô((Z0Wk ‚äôZ0Wv) ‚àóf)n+1

v,"
IN-CONTEXT LINEAR ESTIMATION,0.05733005733005733,"where v ‚ààRd+1 is the linear prediction head and ((Z0Wk ‚äôZ0Wv) ‚àóf)n+1 returns the last row of the
convolution output. Note that SSM can implement the mask by setting f0 = 0. Now consider the meta
learning setting and select loss function to be the squared loss, same as in (5). Thus, the objectives
for both models take the following forms."
IN-CONTEXT LINEAR ESTIMATION,0.05814905814905815,"min
Wk,Wq,Wv,v LATT(W)
where
LATT(W) = E
h
(y ‚àígATT(Z))2i
,
(6a)"
IN-CONTEXT LINEAR ESTIMATION,0.05896805896805897,"min
Wk,Wq,Wv,v, f LSSM(W)
where
LSSM(W) = E
h
(y ‚àígSSM(Z))2i
.
(6b)"
IN-CONTEXT LINEAR ESTIMATION,0.05978705978705979,"Here, similarly, the expectation subsumes the randomness of (xi, Œæi)n+1
i=1 and Œ≤ and W represents the
set of trainable parameters. The search space for matrices Wk, Wq, Wv is R(d+1)√ó(d+1), for head v is
Rd+1, and for f is Rn+1."
IN-CONTEXT LINEAR ESTIMATION,0.06060606060606061,"Note that for all the optimization methods (c.f. (5), (6)), to simplify the analysis, we train the models
without capturing additional bias terms. Therefore, in the following, we introduce the centralized
data assumptions such that the models are trained to make unbiased predictions."
IN-CONTEXT LINEAR ESTIMATION,0.06142506142506143,"To begin with, a cross moment of random variables is defined as the expectation of a monomial of
these variables, with the order of the cross moment being the same as order of the monomial. For
example, E[x‚ä§WŒ≤] is a sum of cross-moments of order 2. Then, it motivates the following data
assumptions."
IN-CONTEXT LINEAR ESTIMATION,0.062244062244062245,"Assumption 1 All cross moments of the entries of (xi)n+1
i=1 and Œ≤ with odd orders are zero."
IN-CONTEXT LINEAR ESTIMATION,0.06306306306306306,"Assumption 2 The label noise (Œæi)n+1
i=1 are independent of (xi)n+1
i=1 and Œ≤, and their cross moments
with odd orders are zero."
IN-CONTEXT LINEAR ESTIMATION,0.06388206388206388,"Note that compared to Ahn et al. [2023], Mahankali et al. [2024], Zhang et al. [2024], Assumption 1 is
more general which also subsumes the dependent distribution settings. In this work, we consider the
following three linear models (omitting noise) satisfying Assumption 1. Let Œ£Œ≤, Œ£x ‚ààRd√ód represent
the task and feature covariance matrices for independent data, and let 0 ‚â§Œ± ‚â§1 be the correlation
level when considering data dependency. More specific discussions are deferred to Section 3."
IN-CONTEXT LINEAR ESTIMATION,0.0647010647010647,"‚Ä¢ Independent task and data: Œ≤ ‚àºN(0, Œ£Œ≤), xi ‚àºN(0, Œ£x), for all 1 ‚â§i ‚â§n + 1."
IN-CONTEXT LINEAR ESTIMATION,0.06552006552006552,"‚Ä¢ Retrieval augmented generation: Œ≤, x ‚àºN(0, Id), xi
 x ‚àºN(Œ±x, (1 ‚àíŒ±2)Id), for all 1 ‚â§i ‚â§n."
IN-CONTEXT LINEAR ESTIMATION,0.06633906633906633,"‚Ä¢ Task-feature alignment: Œ≤ ‚àºN(0, Id), xi
 Œ≤ ‚àºN(Œ±Œ≤, Id), for all 1 ‚â§i ‚â§n + 1."
IN-CONTEXT LINEAR ESTIMATION,0.06715806715806716,"Next, we introduce the following result which establishes the equivalence among optimizing 1-layer
linear attention (c.f. (6a)), 1-layer H3 (c.f. (6b)), and 1-step gradient descent (c.f. (5))."
IN-CONTEXT LINEAR ESTIMATION,0.06797706797706797,"Proposition 1 Suppose Assumptions 1 and 2 hold. Consider the objectives as defined in (5) and (6),
and let L‚ãÜ
PGD, L‚ãÜ
WPGD, L‚ãÜ
ATT, and L‚ãÜ
SSM be their optimal risks, respectively. Then,"
IN-CONTEXT LINEAR ESTIMATION,0.0687960687960688,"L‚ãÜ
PGD = L‚ãÜ
ATT
and
L‚ãÜ
WPGD = L‚ãÜ
SSM."
IN-CONTEXT LINEAR ESTIMATION,0.06961506961506962,"Additionally, if the examples (xi, yi)n
i=1 follow the same distribution and are conditionally independent
given x, Œ≤, then SSM/H3 can achieve the optimal loss using the all-ones filter and L‚ãÜ
PGD = L‚ãÜ
SSM."
IN-CONTEXT LINEAR ESTIMATION,0.07043407043407043,"We defer the proof to Appendix A.1. Proposition 1 establishes that analyzing the optimization
landscape of ICL for both single-layer linear attention and the H3 model can be effectively reduced
to examining the behavior of a one-step PGD algorithm. Notably, under the independent, RAG and
task-feature alignment data settings discussed above, examples (xi, yi)n
i=1 are independently sampled
given x and Œ≤, and we therefore conclude that L‚ãÜ
PGD = L‚ãÜ
ATT = L‚ãÜ
SSM. Leveraging this result, the
subsequent section of the paper concentrate on addressing (5a), taking into account various linear
data distributions."
IN-CONTEXT LINEAR ESTIMATION,0.07125307125307126,"While Proposition 1 demonstrates the equivalence of optimal losses, we also study the uniqueness
and equivalence of optimal prediction functions. To this end, we analyze the strong convexity of
LPGD(W) and derive the subsequent lemmas."
IN-CONTEXT LINEAR ESTIMATION,0.07207207207207207,"Lemma 1 Suppose Assumption 2 holds and let Œæ = [Œæ1 Œæ2 ¬∑ ¬∑ ¬∑ Œæn]‚ä§. Then the loss LPGD(W) in (5a)
is strongly-convex if and only if E[(x‚ä§WX‚ä§XŒ≤)2] + E[(x‚ä§WX‚ä§Œæ)2] is strongly-convex. Additionally,
let g‚ãÜ
PGD, g‚ãÜ
ATT be the optimal prediction functions of (5a) and (6a). Then under the conditions of
Assumptions 1 and 2, and the strong convexity, g‚ãÜ
PGD = g‚ãÜ
ATT."
IN-CONTEXT LINEAR ESTIMATION,0.0728910728910729,"Lemma 2 Suppose that the label noise (Œæi)n
i=1 are i.i.d., zero-mean, variance œÉ2 and independent of
everything else, and that there is a decomposition x = x1 + x2, X = X1 + X2, and Œ≤ = Œ≤1 + Œ≤2 such
that either of the following holds"
IN-CONTEXT LINEAR ESTIMATION,0.07371007371007371,"‚Ä¢ œÉ > 0, and (x1, X1) have full rank covariance and are independent of each other and (x2, X2)."
IN-CONTEXT LINEAR ESTIMATION,0.07452907452907453,"‚Ä¢ (x1, Œ≤1, X1) have full rank covariance and are independent of each other and (x2, Œ≤2, X2)."
IN-CONTEXT LINEAR ESTIMATION,0.07534807534807535,"Then, the loss LPGD(W) in (5a) is strongly-convex."
IN-CONTEXT LINEAR ESTIMATION,0.07616707616707617,"As mentioned above, in this work, we study three specific linear models: with general independent,
RAG-related, and task-feature alignment data. Note that for all the three cases, according to Proposi-
tion 1, we have L‚ãÜ
PGD = L‚ãÜ
ATT = L‚ãÜ
SSM. Additionally, the second claim in Lemma 2 holds, and LPGD(W)
is strongly convex. Therefore, following Lemma 1, we have g‚ãÜ
PGD = g‚ãÜ
ATT. Thanks to the equivalence
among PGD, ATT, and SSM, in the next section, we focus on the solution of objective (5a) under
different scenarios, which will reflect the optimization landscapes of ATT and SSM models."
MAIN RESULTS,0.07698607698607698,"3
Main Results"
MAIN RESULTS,0.07780507780507781,"In light of Proposition 1, optimizing a single layer linear-attention or H3 model is equivalent to
solving the objective (5a). Therefore, in this section, we examine the properties of the one-step PGD
in (5a). To this end, we consider multiple problem settings, including distinct data distributions
and low-rank training. The latter refers to the scenario where the key and query matrices have
rank restrictions, e.g., Wk,Wq ‚ààR(d+1)√ór, as well as LoRA-tuning when adapting the model under
distribution shift."
ANALYSIS OF LINEAR DATA MODELS,0.07862407862407862,"3.1
Analysis of Linear Data Models"
ANALYSIS OF LINEAR DATA MODELS,0.07944307944307945,We first consider the standard independent data setting. We will then examine correlated designs.
ANALYSIS OF LINEAR DATA MODELS,0.08026208026208026,"Independent data model. Let Œ£x and Œ£Œ≤ be the covariance matrices of the input feature and task
vectors, respectively, and œÉ ‚â•0 be the noise level. We assume"
ANALYSIS OF LINEAR DATA MODELS,0.08108108108108109,"Œ≤ ‚àºN(0, Œ£Œ≤),
xi ‚àºN(0, Œ£x),
Œæi ‚àºN(0, œÉ2),
1 ‚â§i ‚â§n + 1
(7)
and the label is obtained via yi = x‚ä§
i Œ≤ + Œæi. Our following result characterizes the optimal solution of
(5a). Note that the data generated from (7) satisfies the conditions in Proposition 1. Therefore, the
same results can be applied to both linear-attention and H3 models."
ANALYSIS OF LINEAR DATA MODELS,0.0819000819000819,"Theorem 1 Consider independent linear data as defined in (7), and suppose the covariance matrices
Œ£x, Œ£Œ≤ are full rank. Recap the objective from (5a) and let W‚ãÜ:= arg minW LPGD(W), and L‚ãÜ=
LPGD(W‚ãÜ). Additionally, let Œ£ = Œ£1/2
x Œ£Œ≤Œ£1/2
x
and M = tr (Œ£) + œÉ2. Then W‚ãÜand L‚ãÜsatisfy"
ANALYSIS OF LINEAR DATA MODELS,0.08271908271908272,"W‚ãÜ= Œ£‚àí1/2
x
¬ØW‚ãÜŒ£‚àí1/2
x
and
L‚ãÜ= M ‚àíntr

Œ£ ¬ØW‚ãÜ

,
(8)"
ANALYSIS OF LINEAR DATA MODELS,0.08353808353808354,"where we define ¬ØW‚ãÜ=

(n + 1)Id + MŒ£‚àí1‚àí1."
ANALYSIS OF LINEAR DATA MODELS,0.08435708435708436,"Corollary 1 Consider noiseless i.i.d. linear data where Œ£x = Œ£Œ≤ = Id and œÉ = 0. Then, the objective
in (5a) returns"
ANALYSIS OF LINEAR DATA MODELS,0.08517608517608517,"W‚ãÜ=
1
n + d + 1 Id
and
L‚ãÜ= d ‚àí
nd
n + d + 1."
ANALYSIS OF LINEAR DATA MODELS,0.085995085995086,"See Appendix B.2 for proofs. Note that Theorem 1 is consistent with prior work [Ahn et al., 2023,
Theorem 1] when specialized to isotropic task covariance, i.e., Œ£Œ≤ = Id. However, their result is
limited as the features and task are assumed to be independent. This prompts us to ask: What is the
optimization landscape with correlated in-context samples? Toward this, we consider the following
RAG-inspired and task-feature alignment models, where Assumptions 1 and 2 continue to hold and
Proposition 1 applies."
ANALYSIS OF LINEAR DATA MODELS,0.08681408681408681,"Retrieval augmented generation. To provide a statistical model of the practical RAG approaches,
given the query vector xn+1 = x, we propose to draw ICL demonstrations that are similar to x with
the same shared task vector Œ≤. Modeling feature similarity through the cosine angle, RAG should
sample the ICL examples xi, i ‚â§n, from the original feature distribution conditioned on the event
cos(xi, x) ‚â•Œ± where Œ± is the similarity threshold. As an approximate proxy, under the Gaussian
distribution model, we assume that Œ≤ ‚àºN(0, Id), x ‚àºN(0, Id) and that RAG samples Œ±-correlated
demonstrations (xi, yi)n
i=1 as follows:"
ANALYSIS OF LINEAR DATA MODELS,0.08763308763308764,"xi
 x ‚àºN(Œ±x, (1 ‚àíŒ±2)Id),
Œæi ‚àºN(0, œÉ2)
and
yi = x‚ä§
i Œ≤ + Œæi,
1 ‚â§i ‚â§n.
(9)
Note that the above normalization ensures that the marginal feature distribution remains N(0, Id).
The full analysis of RAG is provides in Appendix B.3. Specifically, when we carry out the analysis
by assuming Œ± = O

1/
‚àö"
ANALYSIS OF LINEAR DATA MODELS,0.08845208845208845,"d

and d/n = O (1) where O (¬∑) denotes proportionality, our derivation leads
to the following result:"
ANALYSIS OF LINEAR DATA MODELS,0.08927108927108927,"Theorem 2 Consider linear model as defined in (9). Recap the objective from (5a) and let W‚ãÜ:=
arg minW LPGD(W), and L‚ãÜ= LPGD(W‚ãÜ). Additionally, let Œ∫ = Œ±2d + 1 and suppose Œ± = O

1/
‚àö"
ANALYSIS OF LINEAR DATA MODELS,0.09009009009009009,"d

,
d/n = O (1) and d is sufficiently large. Then W‚ãÜand L‚ãÜhave approximate forms"
ANALYSIS OF LINEAR DATA MODELS,0.09090909090909091,"W‚ãÜ‚âà
1
Œ∫n + d + œÉ2 Id
and
L‚ãÜ‚âàd + œÉ2 ‚àí
Œ∫nd
Œ∫n + d + œÉ2 .
(10)"
ANALYSIS OF LINEAR DATA MODELS,0.09172809172809172,"Here, (10) is reminiscent of Corollary 1 and has a surprisingly clean message. Observe that, Œ±2d + 1
is the dominant multiplier ahead of n in both equations. Thus, we deduce that, RAG model follows
the same error bound as the independent data model, however, its sample size is amplified by a factor
of Œ±2d + 1. Œ± = 0 reduces to the result of Corollary 1 whereas we need to set Œ± = O

1/
‚àö"
ANALYSIS OF LINEAR DATA MODELS,0.09254709254709255,"d

for
constant amplification. When Œ± = 1, RAG achieves the approximate risk L‚ãÜ‚âà2 + œÉ2, where the
constant bias is due to the higher order moments (e.g., the 4‚Äôth and 6‚Äôth moments) of the standard
Gaussian distribution. As d increases, the normalized loss L‚ãÜ/d ‚Üí0. The full analysis of its optimal
solution W‚ãÜand loss L‚ãÜare deferred Theorem 4 in Appendix B.3."
ANALYSIS OF LINEAR DATA MODELS,0.09336609336609336,"Task-feature alignment. We also consider another dependent data setting where task and feature
vectors are assumed to be correlated. This dataset model has the following motivation: In general, an
LLM can generate any token within the vocabulary. However, once we specify the task (e.g. domain of
the prompt), the LLM output becomes more deterministic and there are much fewer token candidates.
For instance, if the task is ‚ÄúCountry‚Äù, ‚ÄúFrance‚Äù is a viable output compared to ‚ÄúHelium‚Äù and vice
versa when the task is ‚ÄúChemistry‚Äù. Formally speaking, this can be formalized as the input x having
a diverse distribution whereas it becomes more predictable conditioned on Œ≤. Therefore, it can be
captured through a linear model by making the conditional covariance of x
 Œ≤ to be approximately
low-rank. This formalism can be viewed as a spectral alignment between input and task, which is
also well-established in deep learning both empirically and theoretically [Li et al., 2020, Arora et al.,
2019, Canatar et al., 2021, Cao et al., 2019]. Here, we consider such a setting where the shared task
vector is sampled as standard Gaussian distribution Œ≤ ‚àºN(0, Id) and letting Œ∫ = Œ±2d + 1, we sample
the Œ±-correlated ICL demonstrations (xi, yi)n+1
i=1 as follows:"
ANALYSIS OF LINEAR DATA MODELS,0.09418509418509419,"xi
 Œ≤ ‚àºN(Œ±Œ≤, Id),
Œæi ‚àºN(0, œÉ2)
and
yi = Œ∫‚àí1/2x‚ä§
i Œ≤ + Œæi,
1 ‚â§i ‚â§n + 1.
(11)"
ANALYSIS OF LINEAR DATA MODELS,0.095004095004095,"Above, Œ∫‚àí1/2 is a normalization factor to ensure that label variance remains invariant to Œ±. To keep
the exposition cleaner, we defer the full analysis of its optimal solution W‚ãÜand loss L‚ãÜto Theorem 5
in Appendix B.4. Similar to the RAG setting, by assuming Œ± = O

1/
‚àö"
ANALYSIS OF LINEAR DATA MODELS,0.09582309582309582,"d

and d/n = O (1), we obtain
the following results for the optimal parameter and risk."
ANALYSIS OF LINEAR DATA MODELS,0.09664209664209664,"Theorem 3 Consider linear model as defined in (11). Recap the objective from (5a) and let W‚ãÜ:=
arg minW LPGD(W), and L‚ãÜ= LPGD(W‚ãÜ). Additionally, given Œ∫ = Œ±2d + 1 and suppose Œ± = O

1/
‚àö"
ANALYSIS OF LINEAR DATA MODELS,0.09746109746109746,"d

,
d/n = O (1) and d is sufficiently large. Then W‚ãÜand L‚ãÜhave approximate forms"
ANALYSIS OF LINEAR DATA MODELS,0.09828009828009827,"W‚ãÜ‚âà
1
Œ∫n + (d + œÉ2)/Œ∫ Id
and
L‚ãÜ‚âàd + œÉ2 ‚àí
Œ∫nd
Œ∫n + (d + œÉ2)/Œ∫.
(12)"
ANALYSIS OF LINEAR DATA MODELS,0.0990990990990991,"Similar to (10), (12) contains Œ∫ = Œ±2 + 1 multiplier ahead of n, which reduces the in-context sample
complexity and setting Œ± = 0 reduces to the results of Corollary 1."
LOW-RANK PARAMETERIZATION AND LORA,0.09991809991809991,"3.2
Low-rank Parameterization and LoRA"
LOW-RANK PARAMETERIZATION AND LORA,0.10073710073710074,"In this section, we investigate training low-rank models, which assume Wk,Wq ‚ààR(d+1)√ór where r is
the rank restriction. Equivalently, we consider objective (5a) under condition rank (W) = r."
LOW-RANK PARAMETERIZATION AND LORA,0.10155610155610155,"Lemma 3 Consider independent linear data as defined in (7). Recap the objective from (5a) and
enforce rank (W) ‚â§r and W‚ä§= W. Let Œ£ = Œ£1/2
x Œ£Œ≤Œ£1/2
x
and M = tr (Œ£) + œÉ2. Denoting Œªi to be the
i‚Äôth largest eigenvalue of Œ£, we have that"
LOW-RANK PARAMETERIZATION AND LORA,0.10237510237510238,"min
rank(W)‚â§r,W=W‚ä§L(W) = M ‚àí rX i=1"
LOW-RANK PARAMETERIZATION AND LORA,0.10319410319410319,"nŒª2
i
(n + 1)Œªi + M .
(13)"
LOW-RANK PARAMETERIZATION AND LORA,0.10401310401310401,"0
10
20
30
40
50
# in-context samples 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
LOW-RANK PARAMETERIZATION AND LORA,0.10483210483210484,Test risk
LOW-RANK PARAMETERIZATION AND LORA,0.10565110565110565,"Linear Att
H3
Theorem 1"
LOW-RANK PARAMETERIZATION AND LORA,0.10647010647010648,(a) Œ£x = Œ£Œ≤ = Id and œÉ = 0
LOW-RANK PARAMETERIZATION AND LORA,0.10728910728910729,"0
10
20
30
40
50
# in-context samples 0.4 0.6 0.8 1.0 1.2"
LOW-RANK PARAMETERIZATION AND LORA,0.10810810810810811,Test risk
LOW-RANK PARAMETERIZATION AND LORA,0.10892710892710893,2/d = 0
LOW-RANK PARAMETERIZATION AND LORA,0.10974610974610975,2/d = 0.1
LOW-RANK PARAMETERIZATION AND LORA,0.11056511056511056,2/d = 0.2
LOW-RANK PARAMETERIZATION AND LORA,0.11138411138411139,2/d = 0.3
LOW-RANK PARAMETERIZATION AND LORA,0.1122031122031122,(b) Noisy label
LOW-RANK PARAMETERIZATION AND LORA,0.11302211302211303,"0
10
20
30
40
50
# in-context samples 0.2 0.4 0.6 0.8"
LOW-RANK PARAMETERIZATION AND LORA,0.11384111384111384,Test risk = 0
LOW-RANK PARAMETERIZATION AND LORA,0.11466011466011466,"= 0.3
= 0.6 = 0.9"
LOW-RANK PARAMETERIZATION AND LORA,0.11547911547911548,(c) Non-isotropic task
LOW-RANK PARAMETERIZATION AND LORA,0.1162981162981163,"Figure 2: Empirical evidence validates Theorem 1 and Proposition 1. We train 1-layer linear attention
and H3 models with prompts containing independent demonstrations following a linear model, and
dotted curves are the theory curves following Eq. (8). (a): We consider noiseless i.i.d. setting where
Œ£x = Œ£Œ≤ = Id and œÉ = 0, with results presented in red (attention) and blue (H3) solid curves. (b):
We conduct noisy label experiments by choosing œÉ , 0. (c): Consider non-isotropic task by setting
Œ£Œ≤ = Œ≥11‚ä§+ (1 ‚àíŒ≥)Id. Solid and dashed curves in (b) and (c) represent attention and H3 results,
respectively. The alignments in (a), (b) and (c) show the equivalence between attention and H3,
validating Theorem 1 and Proposition 1. More experimental details are discussed in Section 4."
LOW-RANK PARAMETERIZATION AND LORA,0.11711711711711711,"Note that tr (Œ£) = Pd
i=1 Œªi. Removing the rank constraint and considering noiseless data setting, this
reduces to the following optimal risk L‚ãÜ= Pd
i=1
Œªi+M
n+1+M/Œªi . See Appendix C.1 for more details."
LOW-RANK PARAMETERIZATION AND LORA,0.11793611793611794,"Impact of LoRA: Based on the above lemma, we consider the impact of LoRA for adapting the
pretrained model to a new task distribution under jointly-diagonalizable old and new eigenvalues of
Œ£, Œ£new, (Œªi)d
i=1, (Œªnew
i
)d
i=1. Consider adapting LoRA matrix to the combined key and value weights
in attention, which reflects minimizing the population loss ÀúL(Wlora) := L(W + Wlora) in (5a) with
fixed W. Suppose tr (Œ£) = tr (Œ£new) = M, œÉ = 0 and W is jointly diagonalizable with Œ£, Œ£new, then
LoRA‚Äôs risk is upper-bounded by"
LOW-RANK PARAMETERIZATION AND LORA,0.11875511875511875,"min
rank(Wlora)‚â§r
ÀúL(Wlora) ‚â§
min
|I|‚â§r,I‚äÇ[d]"
LOW-RANK PARAMETERIZATION AND LORA,0.11957411957411958,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
X i<I"
LOW-RANK PARAMETERIZATION AND LORA,0.12039312039312039,"Œªi + M
n + 1 + M/Œªi
+
X i‚ààI"
LOW-RANK PARAMETERIZATION AND LORA,0.12121212121212122,"Œªnew
i
+ M
n + 1 + M/Œªnew
i"
LOW-RANK PARAMETERIZATION AND LORA,0.12203112203112203,"Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏.
(14)"
LOW-RANK PARAMETERIZATION AND LORA,0.12285012285012285,"Note that, the right hand side is provided assuming the optimal LoRA-updated model Wlora is also
jointly diagonalizable with covariances Œ£, Œ£new, and W."
EXPERIMENTS,0.12366912366912367,"4
Experiments"
EXPERIMENTS,0.12448812448812449,"We now conduct synthetic experiments to support our theoretical findings and further explore the
behavior of different models of interest under different conditions. The experiments are designed to
investigate various scenarios, including independent data, retrieval-augmented generation (RAG),
task-feature alignment, low-rank parameterization, and LoRA adaption."
EXPERIMENTS,0.12530712530712532,"Experimental setting. We train 1-layer attention and H3 models for solving the linear regression ICL.
As described in Section 2, we consider meta-learning setting where task parameter Œ≤ is randomly
generated for each training sequence. In all experiments, we set the dimension d = 20. Depending
on the in-context length (n), different models are trained to make in-context predictions. We train
each model for 10000 iterations with batch size 128 and Adam optimizer with learning rate 10‚àí3.
Since our study focuses on the optimization landscape, and experiments are implemented via gradient
descent, we repeat 20 model trainings from different initialization and results are presented as the
minimal test risk among those 20 trails. In all the plots, theoretical predictions are obtained via the
corresponding formulae presented in Section 3 and the test risks are normalized by the dimension d."
EXPERIMENTS,0.12612612612612611,"‚Ä¢ Equivalence among L‚ãÜ
PGD, L‚ãÜ
ATT and L‚ãÜ
SSM (Figure 2). To verify Proposition 1 as well as Theorem 1,
we run random linear regression instances where in-context samples are generated obeying (7).
Fig. 2a is identical to Fig. 1a where we set Œ£x = Œ£Œ≤ = Id and œÉ = 0. In Fig. 2b, set Œ£x = Œ£Œ≤ = I
and vary noise level œÉ2 from 0 to 0.3 √ó d. In Fig. 2c, we consider noiseless labels, œÉ = 0, isotropic
feature distribution Œ£x = Id and set task covariance to be Œ£Œ≤ = Œ≥11‚ä§+ (1 ‚àíŒ≥)Id by choosing Œ≥ in
{0, 0.3, 0.6, 0.9}. Note that in Fig. 2c, we train a sufficient number of models (greater than 20) to
ensure the optimal model is obtained. In all the figures, solid and dashed curves correspond to the"
EXPERIMENTS,0.12694512694512694,"0
10
20
30
40
50
# in-context samples 0.2 0.4 0.6 0.8"
EXPERIMENTS,0.12776412776412777,Test risk
EXPERIMENTS,0.1285831285831286,"= 0
= 0.2
= 0.4
= 0.6"
EXPERIMENTS,0.1294021294021294,(a) RAG
EXPERIMENTS,0.13022113022113022,"0
10
20
30
40
50
# in-context samples 0.2 0.4 0.6 0.8"
EXPERIMENTS,0.13104013104013104,Test risk
EXPERIMENTS,0.13185913185913187,"= 0
= 0.2
= 0.4
= 0.6"
EXPERIMENTS,0.13267813267813267,(b) Task-feature alignment
EXPERIMENTS,0.1334971334971335,"0
10
20
30
40
50
# in-context samples 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
EXPERIMENTS,0.13431613431613432,Test risk
EXPERIMENTS,0.13513513513513514,"r = 1
r = 5
r = 10
r = 20"
EXPERIMENTS,0.13595413595413594,(c) Low-rank attention
EXPERIMENTS,0.13677313677313677,"0
10
20
30
40
50
# in-context samples 0.1 0.3 0.5 0.7 0.9"
EXPERIMENTS,0.1375921375921376,Test risk
EXPERIMENTS,0.13841113841113842,"r = 1
r = 5
r = 10
r = 20"
EXPERIMENTS,0.13923013923013924,(d) LoRA adaptation
EXPERIMENTS,0.14004914004914004,"Figure 3: Distributional alignment and low-rank parameterization experiments. (a) and (b) show
the ICL results using data generated via (9) and (11), respectively, by changing Œ± from 0 to 0.6.
In (c), we train low-rank linear attention models by setting Wk,Wq ‚ààR(d+1)√ór and in (d), we apply
the low-rank LoRA adaptor, Wlora := WupW‚ä§
down where Wup,Wdown ‚ààR(d+1)√ór, to pretrained linear
attention models and adjust the LoRA parameters under different task distribution. Solid and dotted
curves correspond to the linear attention and theoretical results (c.f. Section 3), respectively, and the
alignments validate our theorems in Section 3. More experimental details are discussed in Section 4."
EXPERIMENTS,0.14086814086814087,"ICL results from training 1-layer ATT and SSM models, respectively, and dotted curves are obtained
from (8) in Theorem 1. The alignment of solid, dashed and dotted curves validates our Proposition 1
and Theorem 1."
EXPERIMENTS,0.1416871416871417,"‚Ä¢ Distributional alignment experiments (Figs. 3a&3b). In Figs. 3a and 3b, we generate RAG and
task-feature alignment data following (9) and (11), respectively, by setting œÉ = 0 and varying Œ± from
0 to 0.6. Attention training results are displayed in solid curves, and we generate theory curve (dotted)
via the L‚ãÜformula as described in (36) in Appendix B.3 and (42) in Appendix B.4. The empirical
alignments corroborate Theorems 4 and 5, further confirming that Proposition 1 is applicable to a
broader range of real-world distributional alignment data."
EXPERIMENTS,0.14250614250614252,"‚Ä¢ Low-rank (Fig. 3c) and LoRA (Fig. 3d) experiments. We also run simulations to verify our
theoretical findings in Section 3.2. Consider the independent data setting as described in (7). In Fig. 3c,
we set Œ£x = Id, œÉ = 0 and task covariance to be diagonal with diagonal entries c[1 2‚àí1 ¬∑ ¬∑ ¬∑ d‚àí1]‚ä§for
some normalization constant c = d/ Pd
i=1 i‚àí1, and parameterize the attention model using matrices
Wk,Wq ‚ààR(d+1)√ór and vary r across the set {1, 5, 10, 20}. Results show that empirical (solid) and
theoretical (dotted, c.f. (13)) curves overlap. In Fig. 3d, we implement two phases of training. Phase
1: Setting Œ£x = Œ£Œ≤ = Id and œÉ = 0, we pretrain the model with full rank parameters and obtain
weights ÀÜWk, ÀÜWq, ÀÜWv ‚ààR(d+1)√ó(d+1). Phase 2: We generate new examples with task covariance Œ£Œ≤
being a diagonal matrix with diagonal entries c‚Ä≤[2‚àí1 2‚àí2 ¬∑ ¬∑ ¬∑ 2‚àíd]‚ä§for some normalization constant
c‚Ä≤ = d/ Pd
i=1 2‚àíi. Given the rank restriction r, we train additional LoRA parameters Wup,Wdown ‚àà
R(d+1)√ór where Wlora := WupW‚ä§
down and (2a) becomes ATT(Z) = (Z( ÀÜWq ÀÜW‚ä§
k + WupW‚ä§
down)Z‚ä§)Z ÀÜWv.
Fig. 3d presents the results after two phases of training where dotted curves are drawn from the right
hand side of (14) directly. Here, note that since Œ£, Œ£new are diagonal, the right hand side of (14)
returns the exact optimal risk of LoRA and the alignments verify it."
RELATED WORK,0.14332514332514332,"5
Related Work"
RELATED WORK,0.14414414414414414,"There is growing interest in understanding the mechanisms behind ICL [Brown et al., 2020, Liu et al.,
2023b, Rae et al., 2021] in LLMs due to its success in continuously enabling novel applications for
LLMs [GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023]. In the previous work, Garg
et al. [2022] explored ICL ability of Transformers. In particular, they considered in-context prompts
where each in-context example is labeled by a target function from a given function class, including
linear models. A number of works have studied this and related settings to develop a theoretical
understanding of ICL [von Oswald et al., 2023, Gatmiry et al., Collins et al., 2024, Lin and Lee,
2024, Li et al., 2024, Bai et al., 2024, Aky√ºrek et al., 2023, Zhang et al., 2023, Du et al., 2023].
Aky√ºrek et al. [2023] focus on linear regression and provide a construction of Transformer weights
that can enable a single step of GD based on in-context examples. Along the similar line, Von Oswald
et al. [2023] provide a construction of weights in linear attention-only Transformers that can emulate
GD steps on in-context examples for a linear regression task. Similar to this line of work, Dai et al.
[2023] argue that pre-trained language models act as meta-optimizer which utilize attention to apply
meta-gradients to the original language model based on the in-context examples."
RELATED WORK,0.14496314496314497,"Building on these primarily empirical studies, Zhang et al. [2024], Mahankali et al. [2024], Ahn
et al. [2023], Duraisamy [2024] focus on developing a theoretical understanding of Transformers
trained to perform ICL. For single-layer linear attention model trained on independent in-context
prompts for random linear regression tasks, Mahankali et al. [2024], Ahn et al. [2023] show that the
resulting model implements a single step of PGD on in-context examples in a test prompt, thereby
corroborating the findings of [Von Oswald et al., 2023]. Zhang et al. [2024] study the optimization
dynamics of gradient flow while training a single-layer linear attention model on in-context prompts
for random linear regression tasks. Similar to Mahankali et al. [2024], Ahn et al. [2023], they show
that the trained model implements a single step of GD and PGD for isotropic and anisotropic Gaussian
features, respectively. In addition, they also characterize the test-time prediction error for the trained
model while highlighting its dependence on train and test prompt lengths."
RELATED WORK,0.1457821457821458,"While our work shares similarities with this line of works, as discussed in our contributions in the
introduction, we expand the theoretical understanding of ICL along multiple novel dimensions,
which includes the first study of LoRA adaptation for ICL in the presence of a distributional shift.
Furthermore, we strive to capture the effect of retrieval augmentation [Lewis et al., 2020, Nakano
et al., 2021] on ICL through our analysis. Retrieval augmentation allows for selecting most relevant
demonstration out of a large collection for a test instance, e.g., via a dense retrieval model [Izacard
et al., 2023], which can significantly outperform the typical ICL setup where fixed task-specific
demonstrations are provided as in-context examples [Wang et al., 2022, Basu et al., 2023]. Through
a careful modeling of retrieval augmentation via correlated design, we show that it indeed has
a desirable amplification effect where the effective number in-context examples becomes larger
with higher correlation which corresponds to preforming a successful retrieval of query-relevant
demonstrations in a practical retrieval augmented setup."
RELATED WORK,0.1466011466011466,"Recently, state space models (SSMs) [Gu et al., 2021b,a, Fu et al., 2023, Gu and Dao, 2023] have
appeared as potential alternatives to Transformer architecture, with more efficient scaling to input
sequence length. Recent studies demonstrate that such SSMs can also perform ICL for simple
non-language tasks [Park et al., 2024, Grazzi et al., 2024] as well as complex NLP tasks [Grazzi et al.,
2024]. That said, a rigorous theoretical understanding of ICL for SSMs akin to Zhang et al. [2024],
Mahankali et al. [2024], Ahn et al. [2023] is missing from the literature. In this work, we provide the
first such theoretical treatment for ICL with SSMs. Focusing on H3 architecture [Fu et al., 2023], we
highlight its advantages over linear attention in specific ICL settings."
DISCUSSION,0.14742014742014742,"6
Discussion"
DISCUSSION,0.14823914823914824,"In this work, we revisited the loss landscape of in-context learning with 1-layer sequence models. We
have established a general connection between ICL and gradient methods that accounts for correlated
data, non-attention architectures (specifically SSMs), and the impact of low-rank parameterization
including LoRA adaptation. Our results elucidate two central findings: (1) The functions learned by
different sequence model architectures exhibit a strong degree of universality and (ii) Dataset and
prompt design, such as RAG, can substantially benefit ICL performance."
DISCUSSION,0.14905814905814907,"Future directions and limitations. The results of this work fall short of being a comprehensive theory
for ICL in LLMs and can be augmented in multiple directions. First, while the exact equivalence
between H3 and linear attention is remarkable, we should examine whether it extends to other
SSMs. Secondly, while empirically predictive, our RAG and LoRA analyses are not precise and
fully formal. Thirdly, it is desirable to develop a deeper understanding of multilayer architectures
and connect to iterative GD methods as in [Ahn et al., 2023, Von Oswald et al., 2023]. Finally, we
have studied the population risk of ICL training whereas one can also explore the sample complexity
of pretraining [Wu et al., 2023, Lu et al., 2024]. Moving beyond the theoretically tractable setup
of this work, our simplified models are trained on in-context prompts from random initialization.
Therefore, this theoretical study doesn‚Äôt address more challenging in-context learning tasks, such as
question answering, where both in-context demonstration and general knowledge from pretraining
are required. Future work in this area could also shed light on how certain contexts might elicit
undesirable behaviors acquired by an LLM during pretraining, an aspect not covered in our current
analysis. This work also studies a theoretical model for retrieval augmentation-based ICL. In a
real-life retrieval augmentation-based ICL, one needs to account for the quality of the collection of
the retrievable demonstrations and its (negative) impacts on the final predictions."
DISCUSSION,0.14987714987714987,Acknowledgements
DISCUSSION,0.1506961506961507,"This work was supported in part by the National Science Foundation grants CCF-2046816, CCF-
2403075, the Office of Naval Research award N000142412289, an Adobe Data Science Research
award, and a gift by Google Research."
REFERENCES,0.15151515151515152,References
REFERENCES,0.15233415233415235,"Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement
preconditioned gradient descent for in-context learning. Advances in Neural Information Processing
Systems, 36, 2023."
REFERENCES,0.15315315315315314,"Ekin Aky√ºrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning
algorithm is in-context learning? investigations with linear models. In The Eleventh International
Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=
0g0X4H8yN4I."
REFERENCES,0.15397215397215397,"Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pages 322‚Äì332. PMLR, 2019."
REFERENCES,0.1547911547911548,"Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:
Provable in-context learning with in-context algorithm selection. Advances in neural information
processing systems, 36, 2024."
REFERENCES,0.15561015561015562,"Soumya Basu, Ankit Singh Rawat, and Manzil Zaheer. A statistical perspective on retrieval-based
models. In International Conference on Machine Learning, pages 1852‚Äì1886. PMLR, 2023."
REFERENCES,0.15642915642915642,"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877‚Äì1901, 2020."
REFERENCES,0.15724815724815724,"Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model align-
ment explain generalization in kernel regression and infinitely wide neural networks. Nature
communications, 12(1):2914, 2021."
REFERENCES,0.15806715806715807,"Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019."
REFERENCES,0.1588861588861589,"Liam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, and Sanjay Shakkottai. In-context
learning with transformers: Softmax attention adapts to function lipschitzness. arXiv preprint
arXiv:2402.11639, 2024."
REFERENCES,0.1597051597051597,"Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can
GPT learn in-context? language models secretly perform gradient descent as meta-optimizers.
In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Associa-
tion for Computational Linguistics: ACL 2023, pages 4005‚Äì4019, Toronto, Canada, July 2023.
Association for Computational Linguistics.
doi: 10.18653/v1/2023.findings-acl.247.
URL
https://aclanthology.org/2023.findings-acl.247."
REFERENCES,0.16052416052416052,"Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. In International conference on machine learning, pages 933‚Äì941. PMLR,
2017."
REFERENCES,0.16134316134316135,"Zhe Du, Haldun Balim, Samet Oymak, and Necmiye Ozay. Can transformers learn optimal filtering
for unknown systems? IEEE Control Systems Letters, 7:3525‚Äì3530, 2023."
REFERENCES,0.16216216216216217,"Karthik Duraisamy. Finite sample analysis and bounds of generalization error of gradient descent in
in-context linear regression. arXiv preprint arXiv:2405.02462, 2024."
REFERENCES,0.16298116298116297,"Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re.
Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh
International Conference on Learning Representations, 2023. URL https://openreview.net/
forum?id=COZDy0WYGg."
REFERENCES,0.1638001638001638,"Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn
in-context? a case study of simple function classes. Advances in Neural Information Processing
Systems, 35:30583‚Äì30598, 2022."
REFERENCES,0.16461916461916462,"Khashayar Gatmiry, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, and Sanjiv Kumar. Can
looped transformers learn to implement multi-step gradient descent for in-context learning? In
Forty-first International Conference on Machine Learning."
REFERENCES,0.16543816543816545,"GeminiTeam, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodal models. arXiv preprint arXiv:2312.11805, 2023."
REFERENCES,0.16625716625716624,"Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, and Frank Hutter. Is mamba capable
of in-context learning? arXiv preprint arXiv:2402.03170, 2024."
REFERENCES,0.16707616707616707,"Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv
preprint arXiv:2312.00752, 2023."
REFERENCES,0.1678951678951679,"Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured
state spaces. In International Conference on Learning Representations, 2021a."
REFERENCES,0.16871416871416872,"Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R√©.
Combining recurrent, convolutional, and continuous-time models with linear state space layers.
Advances in neural information processing systems, 34:572‚Äì585, 2021b."
REFERENCES,0.16953316953316952,"Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. In-context learning of large language models explained
as kernel regression. arXiv preprint arXiv:2305.12766, 2023."
REFERENCES,0.17035217035217035,"Noah Hollmann, Samuel M√ºller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A transformer
that solves small tabular classification problems in a second. arXiv preprint arXiv:2207.01848,
2022."
REFERENCES,0.17117117117117117,"Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International
Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=
nZeVKeeFYf9."
REFERENCES,0.171990171990172,"Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning
with retrieval augmented language models. Journal of Machine Learning Research, 24(251):1‚Äì43,
2023."
REFERENCES,0.17280917280917282,"Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In International conference on machine
learning, pages 5156‚Äì5165. PMLR, 2020."
REFERENCES,0.17362817362817362,"Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick. Exploring the relationship between model
architecture and in-context learning ability. arXiv preprint arXiv:2310.08049, 2023."
REFERENCES,0.17444717444717445,"Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. Retrieval-augmented genera-
tion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:
9459‚Äì9474, 2020."
REFERENCES,0.17526617526617527,"Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. In International conference
on artificial intelligence and statistics, pages 4313‚Äì4324. PMLR, 2020."
REFERENCES,0.1760851760851761,"Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers
as algorithms: Generalization and stability in in-context learning. In International Conference on
Machine Learning, pages 19565‚Äì19594. PMLR, 2023."
REFERENCES,0.1769041769041769,"Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and Samet Oymak.
Dissecting chain-of-thought: Compositionality through in-context filtering and learning. Advances
in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.17772317772317772,"Ziqian Lin and Kangwook Lee. Dual operating modes of in-context learning. arXiv preprint
arXiv:2402.18819, 2024."
REFERENCES,0.17854217854217855,"Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers
learn shortcuts to automata. In The Eleventh International Conference on Learning Representations,
2023a. URL https://openreview.net/forum?id=De4FYqjFueZ."
REFERENCES,0.17936117936117937,"Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.
Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language
processing. ACM Computing Surveys, 55(9):1‚Äì35, 2023b."
REFERENCES,0.18018018018018017,"Yue Lu, Mary I. Letey, Jacob A. Zavatone-Veth, Anindita Maiti, and Cengiz Pehlevan. Asymptotic
theory of in-context learning by linear attention. arXiv preprint arXiv:2310.08391, 2024."
REFERENCES,0.180999180999181,"Arvind V. Mahankali, Tatsunori Hashimoto, and Tengyu Ma. One step of gradient descent is provably
the optimal in-context learner with one layer of linear self-attention. In The Twelfth International
Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=
8p3fu56lKc."
REFERENCES,0.18181818181818182,"Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. Revisiting the equivalence of in-context
learning and gradient descent: The impact of data distribution. In ICASSP 2024-2024 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7410‚Äì7414.
IEEE, 2024."
REFERENCES,0.18263718263718265,"Samuel M√ºller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter.
Transformers can do bayesian inference. arXiv preprint arXiv:2112.10510, 2021."
REFERENCES,0.18345618345618345,"Samuel M√ºller, Matthias Feurer, Noah Hollmann, and Frank Hutter. Pfns4bo: In-context learning for
bayesian optimization. In International Conference on Machine Learning, pages 25444‚Äì25470.
PMLR, 2023."
REFERENCES,0.18427518427518427,"Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021."
REFERENCES,0.1850941850941851,"Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,
Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.
arXiv preprint arXiv:2209.11895, 2022."
REFERENCES,0.18591318591318592,"OpenAI. Gpt-4 technical report. arXiv preprintarXiv:2303.08774, 2023."
REFERENCES,0.18673218673218672,"Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kang-
wook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on
in-context learning tasks. International Conference on Machine Learning, 2024."
REFERENCES,0.18755118755118755,"Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021."
REFERENCES,0.18837018837018837,"Imanol Schlag, Kazuki Irie, and J√ºrgen Schmidhuber. Linear transformers are secretly fast weight
programmers. In International Conference on Machine Learning, pages 9355‚Äì9366. PMLR, 2021."
REFERENCES,0.1891891891891892,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e
Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023."
REFERENCES,0.19000819000819,"Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo√£o Sacramento, Alexander Mordvintsev,
Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In
International Conference on Machine Learning, pages 35151‚Äì35174. PMLR, 2023."
REFERENCES,0.19082719082719082,"Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet,
Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering
mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858, 2023."
REFERENCES,0.19164619164619165,"Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and
Michael Zeng. Training data is more valuable than you think: A simple and effective method by
retrieving from training data. arXiv preprint arXiv:2203.08773, 2022."
REFERENCES,0.19246519246519248,"Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
neural information processing systems, 35:24824‚Äì24837, 2022."
REFERENCES,0.19328419328419327,"Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett.
How many pretraining tasks are needed for in-context learning of linear regression? arXiv preprint
arXiv:2310.08391, 2023."
REFERENCES,0.1941031941031941,"Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context
learning as implicit bayesian inference. In International Conference on Learning Representations,
2022. URL https://openreview.net/forum?id=RdJVFCHjUMI."
REFERENCES,0.19492219492219492,"Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context.
arXiv preprint arXiv:2306.09927, 2023."
REFERENCES,0.19574119574119575,"Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context.
Journal of Machine Learning Research, 25(49):1‚Äì55, 2024."
REFERENCES,0.19656019656019655,"Nicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes Von Oswald, Maxime Larcher, Angelika
Steger, and Joao Sacramento. Gated recurrent neural networks discover attention. arXiv preprint
arXiv:2309.01775, 2023."
REFERENCES,0.19737919737919737,Appendix
REFERENCES,0.1981981981981982,Table of Contents
REFERENCES,0.19901719901719903,"A Equivalence among Gradient Descent, Attention, and State-Space Models
15
A.1
Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.2
Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
A.3
Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21"
REFERENCES,0.19983619983619982,"B Analysis of General Data Distribution
21
B.1
Supporting Results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
B.2
Independent Data with General Covariance
. . . . . . . . . . . . . . . . . . . .
24
B.3
Retrieval Augmented Generation with Œ± Correlation . . . . . . . . . . . . . . . .
25
B.4
Task-feature Alignment with Œ± Correlation . . . . . . . . . . . . . . . . . . . . .
28"
REFERENCES,0.20065520065520065,"C Analysis of Low-Rank Parameterization
31
C.1
Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31"
REFERENCES,0.20147420147420148,"D Additional Experiments
32"
REFERENCES,0.2022932022932023,"E
Extended Related Work
33"
REFERENCES,0.2031122031122031,"A
Equivalence among Gradient Descent, Attention, and State-Space Models"
REFERENCES,0.20393120393120392,"In this section, we present the proofs related to Section 2. Recap that given data"
REFERENCES,0.20475020475020475,"X = [x1 ¬∑ ¬∑ ¬∑ xn]‚ä§‚ààRn√ód,"
REFERENCES,0.20556920556920558,"Œæ = [Œæ1 ¬∑ ¬∑ ¬∑ Œæn]‚ä§‚ààRn,"
REFERENCES,0.20638820638820637,"y = [y1 ¬∑ ¬∑ ¬∑ yn]‚ä§= XŒ≤ + Œæ ‚ààRn,"
REFERENCES,0.2072072072072072,"Z0 = [z1 . . . zn 0d+1]‚ä§=
""
x1
. . .
xn
0d
y1
. . .
yn
0"
REFERENCES,0.20802620802620803,"#‚ä§
‚ààR(n+1)√ó(d+1),"
REFERENCES,0.20884520884520885,and corresponding prediction functions
REFERENCES,0.20966420966420968,"gPGD(Z) = x‚ä§WX‚ä§y,
(15a)"
REFERENCES,0.21048321048321048,"gWPGD(Z) = x‚ä§WX‚ä§(œâ ‚äôy),
(15b)"
REFERENCES,0.2113022113022113,"gATT(Z) = (z‚ä§WqW‚ä§
k Z‚ä§
0 )Z0Wvv,
(15c)"
REFERENCES,0.21212121212121213,"gSSM(Z) =

(z‚ä§Wq)‚ä§‚äô((Z0Wk ‚äôZ0Wv) ‚àóf)n+1

v,
(15d)"
REFERENCES,0.21294021294021295,we have objectives
REFERENCES,0.21375921375921375,"min
W LPGD(W)
where
LPGD(W) = E
h
(y ‚àígPGD(Z))2i
,
(16a)"
REFERENCES,0.21457821457821458,"min
W,œâ LWPGD(W)
where
LWPGD(W) = E
h
(y ‚àígWPGD(Z))2i
,
(16b)"
REFERENCES,0.2153972153972154,"min
Wk,Wq,Wv,v LATT(W)
where
LATT(W) = E
h
(y ‚àígATT(Z))2i
,
(16c)"
REFERENCES,0.21621621621621623,"min
Wk,Wq,Wv,v,f LSSM(W)
where
LSSM(W) = E
h
(y ‚àígSSM(Z))2i
.
(16d)"
REFERENCES,0.21703521703521703,"Here, the expectation is over the randomness in (xi, Œæi)n
i=1 and Œ≤, and the search space for W is Rd√ód,
for œâ is Rn, for Wk,Wq,Wv is R(d+1)√ó(d+1), for v is Rd+1, and for f is Rn+1."
REFERENCES,0.21785421785421785,"A.1
Proof of Proposition 1"
REFERENCES,0.21867321867321868,"Consider the problem setting as discussed in Section 2, Proposition 1 can be proven by the following
two lemmas."
REFERENCES,0.2194922194922195,"Lemma 4 Suppose Assumptions 1 and 2 hold. Then, given the objectives (16a) and (16c), we have"
REFERENCES,0.2203112203112203,"min
Wq,Wk,Wv,v LATT(W) = min
W LPGD(W)."
REFERENCES,0.22113022113022113,Proof. Recap the linear attention estimator from (15c) and denote
REFERENCES,0.22194922194922195,"WqW‚ä§
k =
"" ¬ØW
w1
w‚ä§
2
w"
REFERENCES,0.22276822276822278,"#
and
Wvv =
""
v1
v #
,"
REFERENCES,0.22358722358722358,"where ¬ØW ‚ààRd√ód, w1, w2, v1 ‚ààRd, and w, v ‚ààR. Then we have"
REFERENCES,0.2244062244062244,"gATT(Z) = (z‚ä§WqW‚ä§
k Z‚ä§
0 )Z0Wvv"
REFERENCES,0.22522522522522523,"= [x‚ä§0]
"" ¬ØW
w1
w‚ä§
2
w"
REFERENCES,0.22604422604422605,"# ""
X‚ä§
0d
y‚ä§
0"
REFERENCES,0.22686322686322685,"# ""
X
y
0‚ä§
d
0"
REFERENCES,0.22768222768222768,"# ""
v1
v #"
REFERENCES,0.2285012285012285,= (x‚ä§¬ØWX‚ä§+ x‚ä§w1y‚ä§)(Xv1 + yv)
REFERENCES,0.22932022932022933,"= x‚ä§(v ¬ØW)X‚ä§y + x‚ä§w1y‚ä§Xv1 + x‚ä§ ¬ØWX‚ä§Xv1 + v ‚à•y‚à•2
‚Ñì2 w1
"
REFERENCES,0.23013923013923013,"= x‚ä§(v ¬ØW + w1v‚ä§
1 )X‚ä§y + x‚ä§ ¬ØWX‚ä§Xv1 + v ‚à•y‚à•2
‚Ñì2 w1
"
REFERENCES,0.23095823095823095,"= x‚ä§ÀúWX‚ä§y
|     {z     }
ÀúgATT(Z)"
REFERENCES,0.23177723177723178,"+ x‚ä§ ¬ØWX‚ä§Xv1 + v ‚à•y‚à•2
‚Ñì2 w1
"
REFERENCES,0.2325962325962326,"|                            {z                            }
Œµ"
REFERENCES,0.2334152334152334,",
(17)"
REFERENCES,0.23423423423423423,"where ÀúW := v ¬ØW + w1v‚ä§
1 ."
REFERENCES,0.23505323505323505,"We first show that for any given parameters Wk,Wq,Wv, v,"
REFERENCES,0.23587223587223588,"E
h
(gATT(Z) ‚àíy)2i
‚â•E
h
(ÀúgATT(Z) ‚àíy)2i
.
(18)"
REFERENCES,0.23669123669123668,"To this goal, we have"
REFERENCES,0.2375102375102375,"E
h
(gATT(Z) ‚àíy)2i
‚àíE
h
(ÀúgATT(Z) ‚àíy)2i
= E
h
(ÀúgATT(Z) + Œµ ‚àíy)2i
‚àíE
h
(ÀúgATT(Z) ‚àíy)2i"
REFERENCES,0.23832923832923833,"= E[Œµ2] + 2 E[(ÀúgATT(Z) ‚àíy)Œµ]
(19)"
REFERENCES,0.23914823914823916,where we have decomposition
REFERENCES,0.23996723996723995,"(ÀúgATT(Z) ‚àíy)Œµ = (x‚ä§ÀúWX‚ä§y ‚àíy)x‚ä§ ¬ØWX‚ä§Xv1 + v ‚à•y‚à•2
‚Ñì2 w1
"
REFERENCES,0.24078624078624078,"= y‚ä§X ÀúW‚ä§xx‚ä§ ¬ØWX‚ä§Xv1 + v ‚à•y‚à•2
‚Ñì2 w1

‚àíyx‚ä§ ¬ØWX‚ä§Xv1 + v ‚à•y‚à•2
‚Ñì2 w1
"
REFERENCES,0.2416052416052416,"= y‚ä§X ÀúW‚ä§xx‚ä§¬ØWX‚ä§Xv1
|                      {z                      }
(a)"
REFERENCES,0.24242424242424243,"+ v ‚à•y‚à•2
‚Ñì2 y‚ä§X ÀúW‚ä§xx‚ä§w1
|                      {z                      }
(b)"
REFERENCES,0.24324324324324326,"‚àíyx‚ä§¬ØWX‚ä§Xv1
|          {z          }
(c)"
REFERENCES,0.24406224406224405,"‚àívy ‚à•y‚à•2
‚Ñì2 x‚ä§w1
|          {z          }
(d) ."
REFERENCES,0.24488124488124488,"In the following, we consider the expectations of (a), (b), (c), (d) sequentially, which return zeros
under Assumptions 1 and 2. Note that since Assumption 1 holds, expectation of any odd order of
monomial of the entries of X, x, Œ≤ returns zero, i.e., order of x‚ä§Œ≤x is 3 and therefore E[x‚ä§Œ≤x] = 0d."
REFERENCES,0.2457002457002457,"(a) :
E
h
y‚ä§X ÀúW‚ä§xx‚ä§¬ØWX‚ä§Xv1
i"
REFERENCES,0.24651924651924653,"= E
h
(XŒ≤ + Œæ)‚ä§X ÀúW‚ä§xx‚ä§¬ØWX‚ä§Xv1
i"
REFERENCES,0.24733824733824733,"= E
h
Œ≤‚ä§X‚ä§X ÀúW‚ä§xx‚ä§¬ØWX‚ä§Xv1
i
+ E
h
Œæ‚ä§X ÀúW‚ä§xx‚ä§¬ØWX‚ä§Xv1
i = 0."
REFERENCES,0.24815724815724816,"(b) :
E
h
v ‚à•y‚à•2
‚Ñì2 y‚ä§X ÀúW‚ä§xx‚ä§w1
i"
REFERENCES,0.24897624897624898,"= E
h
v(XŒ≤ + Œæ)‚ä§(XŒ≤ + Œæ)(XŒ≤ + Œæ)‚ä§X ÀúW‚ä§xx‚ä§w1
i"
REFERENCES,0.2497952497952498,"= E
h
v ‚à•Œæ‚à•2
‚Ñì2 Œæ‚ä§X ÀúW‚ä§xx‚ä§w1
i = 0."
REFERENCES,0.25061425061425063,"(c) :
E
h
yx‚ä§¬ØWX‚ä§Xv1
i"
REFERENCES,0.25143325143325146,"= E
h
(x‚ä§Œ≤ + Œæ)x‚ä§¬ØWX‚ä§Xv1
i"
REFERENCES,0.25225225225225223,"= E
h
Œ≤‚ä§xx‚ä§¬ØWX‚ä§Xv1
i
+ E
h
Œæx‚ä§¬ØWX‚ä§Xv1
i = 0."
REFERENCES,0.25307125307125306,"(d) :
E
h
vy ‚à•y‚à•2
‚Ñì2 x‚ä§w1
i"
REFERENCES,0.2538902538902539,"= v E
h
(Œ≤‚ä§x + Œæ)(XŒ≤ + Œæ)‚ä§(XŒ≤ + Œæ)x‚ä§w1
i"
REFERENCES,0.2547092547092547,"= v E
h
Œæ ‚à•Œæ‚à•2
‚Ñì2 x‚ä§w1
i"
REFERENCES,0.25552825552825553,"= 0.
Combining the results with (19) returns that"
REFERENCES,0.25634725634725636,"E
h
(gATT(Z) ‚àíy)2i
‚àíE
h
(ÀúgATT(Z) ‚àíy)2i
= E[Œµ2] ‚â•0
(20)"
REFERENCES,0.2571662571662572,"which completes the proof of (18). Therefore, we obtain"
REFERENCES,0.257985257985258,"min
Wq,Wk,Wv,v E
h
(gATT(Z) ‚àíy)2i
‚â•min
ÀúW E
h
(ÀúgATT(Z) ‚àíy)2i
= min
W E
h
(gPGD(Z) ‚àíy)2i
."
REFERENCES,0.2588042588042588,"We conclude the proof of this lemma by showing that for any W ‚ààRd√ód in gPGD, there exist
Wk,Wq,Wv, v such that gATT(Z) = gPGD(Z). Let"
REFERENCES,0.2596232596232596,"Wk = Wv = Id+1,
Wq =
""
W
0d
0‚ä§
d
0"
REFERENCES,0.26044226044226043,"#
,
and
v =
""
0d
1 #
."
REFERENCES,0.26126126126126126,"Then we obtain
gATT(Z) = x‚ä§WX‚ä§y = gPGD(Z),
(21)
which completes the proof."
REFERENCES,0.2620802620802621,"Lemma 5 Suppose Assumptions 1 and 2 hold. Then, given the objectives in (16), we have
min
Wq,Wk,Wv,v, f LSSM(W) = min
W,œâ LWPGD(W).
(22)"
REFERENCES,0.2628992628992629,"Additionally, if the examples (xi, yi)n
i=1 follow the same distribution and are conditionally independent
given x and Œ≤, then SSM/H3 can achieve the optimal loss using the all-ones filter and
min
W,œâ LWPGD(W) = min
W LPGD(W).
(23)"
REFERENCES,0.26371826371826373,Proof. Recap the SSM estimator from (15d) and let
REFERENCES,0.26453726453726456,"Wq =
h
wq1
wq2
¬∑ ¬∑ ¬∑
wq,d+1
i
,"
REFERENCES,0.26535626535626533,"Wk =
h
wk1
wk2
¬∑ ¬∑ ¬∑
wk,d+1
i
,"
REFERENCES,0.26617526617526616,"Wv =
h
wv1
wv2
¬∑ ¬∑ ¬∑
wv,d+1
i
,"
REFERENCES,0.266994266994267,"where wqj, wk j, wvj ‚ààRd+1 for j ‚â§d + 1, and let v ="
REFERENCES,0.2678132678132678,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.26863226863226863,"v1
v2
¬∑ ¬∑ ¬∑
vd+1"
REFERENCES,0.26945126945126946,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
and
f ="
REFERENCES,0.2702702702702703,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.2710892710892711,"f0
f1
¬∑ ¬∑ ¬∑
fn"
REFERENCES,0.2719082719082719,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
."
REFERENCES,0.2727272727272727,"Then we have
gSSM(Z) =

(z‚ä§Wq)‚ä§‚äô((Z0Wk ‚äôZ0Wv) ‚àóf)n+1

v = n
X"
REFERENCES,0.27354627354627353,"i=1
fn+1‚àíi ¬∑ v‚ä§"
REFERENCES,0.27436527436527436,Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
REFERENCES,0.2751842751842752,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.276003276003276,"w‚ä§
q1z
¬∑ ¬∑ ¬∑
w‚ä§
q,d+1z"
REFERENCES,0.27682227682227684,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª‚äô
REFERENCES,0.27764127764127766,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.2784602784602785,"w‚ä§
k1ziw‚ä§
v1zi
¬∑ ¬∑ ¬∑
w‚ä§
k,d+1ziw‚ä§
v,d+1zi"
REFERENCES,0.27927927927927926,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.2800982800982801,"Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏ = n
X"
REFERENCES,0.2809172809172809,"i=1
fn+1‚àíi ¬∑ v‚ä§"
REFERENCES,0.28173628173628174,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.28255528255528256,"w‚ä§
q1zw‚ä§
k1ziw‚ä§
v1zi
¬∑ ¬∑ ¬∑
w‚ä§
q,d+1zw‚ä§
k,d+1ziw‚ä§
v,d+1zi"
REFERENCES,0.2833742833742834,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª.
REFERENCES,0.2841932841932842,"Next for all j ‚â§d + 1, let"
REFERENCES,0.28501228501228504,"wqj =
""
¬Øwq j
wq j"
REFERENCES,0.2858312858312858,"#
,
wk j =
""
¬Øwk j
wk j"
REFERENCES,0.28665028665028663,"#
,
wv j =
""
¬Øwv j
wv j #"
REFERENCES,0.28746928746928746,"where ¬Øwqj, ¬Øwk j, ¬Øwvj ‚ààRd and wqj, wk j, wv j ‚ààR. Then we have"
REFERENCES,0.2882882882882883,"w‚ä§
qjzw‚ä§
k jziw‚ä§
vjzi =

¬Øw‚ä§
qjx
 
¬Øw‚ä§
k jxi + wk jyi
 
¬Øw‚ä§
v jxi + wv jyi
"
REFERENCES,0.2891072891072891,"= x‚ä§¬Øwqj

wvj ¬Øw‚ä§
k j + wk j ¬Øw‚ä§
v j

xiyi +

¬Øw‚ä§
q jx
 
¬Øw‚ä§
k jxi
 
¬Øw‚ä§
v jxi

+

wk jwv j ¬Øw‚ä§
q jxy2
i
"
REFERENCES,0.28992628992628994,"= x‚ä§W‚Ä≤
jxiyi + Œ¥ j(x, xi, xi) + w‚Ä≤
j
‚ä§xy2
i
where"
REFERENCES,0.29074529074529076,"W‚Ä≤
j := ¬Øwq j

wv j ¬Øw‚ä§
k j + wk j ¬Øw‚ä§
v j

‚ààRd√ód,"
REFERENCES,0.2915642915642916,"w‚Ä≤
j := wk jwv j ¬Øwq j ‚ààRd,"
REFERENCES,0.29238329238329236,"Œ¥ j(x, xi, xi) :=

¬Øw‚ä§
q jx
 
¬Øw‚ä§
k jxi
 
¬Øw‚ä§
v jxi

‚ààR. Then"
REFERENCES,0.2932022932022932,"gSSM(Z) = n
X"
REFERENCES,0.294021294021294,"i=1
fn+1‚àíi ¬∑ d+1
X"
REFERENCES,0.29484029484029484,"j=1
v j

x‚ä§W‚Ä≤
jxiyi + Œ¥ j(x, xi, xi) + w‚Ä≤
j
‚ä§xy2
i
"
REFERENCES,0.29565929565929566,"= x‚ä§
Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d+1
X"
REFERENCES,0.2964782964782965,"j=1
vjW‚Ä≤
j"
REFERENCES,0.2972972972972973,"Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏X(y ‚äôÀúf) + n
X"
REFERENCES,0.29811629811629814,"i=1
fn+1‚àíi ¬∑ d+1
X"
REFERENCES,0.2989352989352989,"j=1
vj ¬∑ Œ¥ j(x, xi, xi) +"
REFERENCES,0.29975429975429974,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d+1
X"
REFERENCES,0.30057330057330056,"j=1
v jw‚Ä≤
j
‚ä§
Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏xy‚ä§(y ‚äôÀúf)"
REFERENCES,0.3013923013923014,"= x‚ä§ÀúWXÀúy
|   {z   }
ÀúgSSM(Z)"
REFERENCES,0.3022113022113022,"+ ÀúŒ¥(x, X, X)
|      {z      }
Œµ1"
REFERENCES,0.30303030303030304,"+ Àúw‚ä§xy‚ä§Àúy
|   {z   }
Œµ2 . where"
REFERENCES,0.30384930384930386,"Àúf := [ fn ¬∑ ¬∑ ¬∑ f1]‚ä§‚ààRn,"
REFERENCES,0.3046683046683047,"Àúy := y ‚äôÀúf ‚ààRn, ÀúW := d+1
X"
REFERENCES,0.30548730548730546,"j=1
v jW‚Ä≤
j ‚ààRd√ód, Àúw := d+1
X"
REFERENCES,0.3063063063063063,"j=1
vjw‚Ä≤
j ‚ààRd,"
REFERENCES,0.3071253071253071,"ÀúŒ¥(x, X, X) := n
X"
REFERENCES,0.30794430794430794,"i=1
fn+1‚àíi ¬∑ d+1
X"
REFERENCES,0.30876330876330876,"j=1
v j ¬∑ Œ¥ j(x, xi, xi) ‚ààR."
REFERENCES,0.3095823095823096,"Next we will show that for any Wk,Wq,Wv, v,"
REFERENCES,0.3104013104013104,"E
h
(gSSM(Z) ‚àíy)2i
‚â•E
h
(ÀúgSSM(Z) ‚àíy)2i
."
REFERENCES,0.31122031122031124,"To start with, we obtain"
REFERENCES,0.31203931203931207,"E
h
(gSSM(Z) ‚àíy)2i
= E
h
(ÀúgSSM(Z) + Œµ1 + Œµ2 ‚àíy)2i"
REFERENCES,0.31285831285831284,"= E
h
(ÀúgSSM(Z) ‚àíy)2i
+ E
h
(Œµ1 + Œµ2)2i
+ 2 E (ÀúgSSM(Z) ‚àíy)(Œµ1 + Œµ2)
(24)"
REFERENCES,0.31367731367731366,where there is decomposition
REFERENCES,0.3144963144963145,"(ÀúgSSM(Z) ‚àíy)(Œµ1 + Œµ2) = ÀúŒ¥(x, X, X) ¬∑ x‚ä§ÀúWXÀúy
|                   {z                   }
(a)"
REFERENCES,0.3153153153153153,"‚àíÀúŒ¥(x, X, X)y
|       {z       }
(b)"
REFERENCES,0.31613431613431614,"+ Àúw‚ä§xy‚ä§Àúy ¬∑ x‚ä§ÀúWXÀúy
|                 {z                 }
(c)"
REFERENCES,0.31695331695331697,"‚àíy ¬∑ Àúw‚ä§xy‚ä§Àúy
|       {z       }
(d) ."
REFERENCES,0.3177723177723178,"In the following, similar to the proof of Lemma 4, we consider the expectations of (a), (b), (c), (d)
sequentially, which return zeros under Assumptions 1 and 2. Note that Œ¥ j(x, xi, xi)‚Äôs and ÀúŒ¥(x, X, X)
are summation of monomials of entries of (x, X, Œ≤) with order 3, and entries of y and y are summation"
REFERENCES,0.3185913185913186,"of monomials of entries of (x, X, Œ≤) with even orders: e.g., y = x‚ä§Œ≤ + Œæ where Œæ is of oder 0 and x‚ä§Œ≤
is of order 2."
REFERENCES,0.3194103194103194,"(a) :
E
hÀúŒ¥(x, X, X) ¬∑ x‚ä§ÀúWXÀúy
i"
REFERENCES,0.3202293202293202,"= E
hÀúŒ¥(x, X, X) ¬∑ x‚ä§ÀúWX(XŒ≤ ‚äôÀúf)
i
+ E
hÀúŒ¥(x, X, X) ¬∑ x‚ä§ÀúWX(Œæ ‚äôÀúf)
i"
REFERENCES,0.32104832104832104,"= E
hÀúŒ¥(x, X, X) ¬∑ x‚ä§ÀúWX
i
E
h
Œæ ‚äôÀúf
i = 0."
REFERENCES,0.32186732186732187,"(b) :
E
hÀúŒ¥(x, X, X)y
i"
REFERENCES,0.3226863226863227,"= E
hÀúŒ¥(x, X, X)(x‚ä§Œ≤ + Œæ)
i"
REFERENCES,0.3235053235053235,"= E
hÀúŒ¥(x, X, X)x‚ä§Œ≤
i
+ E
hÀúŒ¥(x, X, X)Œæ
i = 0."
REFERENCES,0.32432432432432434,"(c) :
E
h
Àúw‚ä§xy‚ä§Àúy ¬∑ x‚ä§ÀúWXÀúy
i"
REFERENCES,0.32514332514332517,"= E
h
Àúw‚ä§x(XŒ≤ + Œæ)‚ä§(XŒ≤ ‚äôÀúf + Œæ ‚äôÀúf) ¬∑ x‚ä§ÀúWX(XŒ≤ ‚äôÀúf + Œæ ‚äôÀúf)
i = 0."
REFERENCES,0.32596232596232594,"(d) :
E
h
y ¬∑ Àúw‚ä§xy‚ä§Àúy
i"
REFERENCES,0.32678132678132676,"= E
h
(x‚ä§Œ≤ + Œæ) ¬∑ Àúw‚ä§x(XŒ≤ + Œæ)‚ä§(XŒ≤ ‚äôÀúf + Œæ ‚äôÀúf)
i = 0."
REFERENCES,0.3276003276003276,Combining the results with (24) results that
REFERENCES,0.3284193284193284,"E
h
(gSSM(Z) ‚àíy)2i
‚àíE
h
(ÀúgSSM(Z) ‚àíy)2i
= E
h
(Œµ1 + Œµ2)2i
‚â•0."
REFERENCES,0.32923832923832924,"Therefore we obtain,"
REFERENCES,0.33005733005733007,"min
Wq,Wk,Wv,v, f E
h
(gSSM(Z) ‚àíy)2i
‚â•min
ÀúW, Àúf E
h
(ÀúgSSM(Z) ‚àíy)2i
= min
W,œâ E
h
(gWPGD(Z) ‚àíy)2i
."
REFERENCES,0.3308763308763309,"Next we show that for any choices of W and œâ in gWPGD, there are Wq,k,v, v, f such that gSSM ‚â°gWPGD.
To this end, given œâ = [œâ1 . . . œân]‚ä§, let"
REFERENCES,0.3316953316953317,"Wq = Id+1,
Wk =
""
W‚ä§
0d
0‚ä§
d
0"
REFERENCES,0.3325143325143325,"#
,
Wv =
""
0d√ód
0d
1‚ä§
d
0"
REFERENCES,0.3333333333333333,"#
,
v =
""
1d
0"
REFERENCES,0.33415233415233414,"#
and
f ="
REFERENCES,0.33497133497133497,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.3357903357903358,"0
œân
¬∑ ¬∑ ¬∑
œâ1"
REFERENCES,0.3366093366093366,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
."
REFERENCES,0.33742833742833744,Then we get
REFERENCES,0.33824733824733827,"((Z0Wk ‚äôZ0Wv) ‚àóf)n+1 =
  ""
XW‚ä§
0n
0d
0"
REFERENCES,0.33906633906633904,"#
‚äô
""
y1‚ä§
d
0n
0d
0"
REFERENCES,0.33988533988533987,"#!
‚àóf
! n+1"
REFERENCES,0.3407043407043407,"=
""Pn
i=1 œâi ¬∑ yiWxi
0 #"
REFERENCES,0.3415233415233415,"=
""
WX‚ä§(y ‚äôœâ)
0 #
,"
REFERENCES,0.34234234234234234,"and therefore
gSSM(Z) = x‚ä§WX‚ä§(y ‚äôœâ) = gWPGD(Z),"
REFERENCES,0.34316134316134317,which completes the proof of (22).
REFERENCES,0.343980343980344,"Next, to show (23), for any W ‚ààRd√ód, let L(œâ) = E
h x‚ä§WX‚ä§(y ‚äôœâ) ‚àíy2i
. Then we have ‚àÇL(œâ)"
REFERENCES,0.3447993447993448,"‚àÇœâi
= E"
REFERENCES,0.34561834561834565,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞2
REFERENCES,0.3464373464373464,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠x‚ä§W n
X"
REFERENCES,0.34725634725634724,"j=1
œâjyjx j ‚àíy"
REFERENCES,0.34807534807534807,"Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏

x‚ä§Wyixi
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = 2 n
X"
REFERENCES,0.3488943488943489,"j=1
œâj E
h
(x‚ä§Wy jxj)(x‚ä§Wyixi)
i
‚àí2 E
h
yx‚ä§Wyixi
i
."
REFERENCES,0.3497133497133497,"Here since (xi, yi)n
i=1 follow the same distribution and are conditionally independent given x
and Œ≤, for any i , j , j‚Ä≤, E
h
(x‚ä§Wyixi)2i
= E
h
(x‚ä§Wyjx j)2i
and E
h
(x‚ä§Wyjx j)(x‚ä§Wyixi)
i
="
REFERENCES,0.35053235053235055,"E
h
(x‚ä§Wy j‚Ä≤ x j‚Ä≤)(x‚ä§Wyixi)
i
. Then let"
REFERENCES,0.35135135135135137,"E
h
(x‚ä§Wy jxj)(x‚ä§Wyixi)
i
=
(c1,
i , j
c2,
i = j
and
E
h
yx‚ä§Wyixi
i
= c3,"
REFERENCES,0.3521703521703522,"where (c1, c2, c3) := (c1(W), c2(W), c3(W)). We get ‚àÇL(œâ)"
REFERENCES,0.35298935298935297,"‚àÇœâi
= 2c1œâ‚ä§1n + 2(c2 ‚àíc1)œâi ‚àí2c3."
REFERENCES,0.3538083538083538,"If c2‚àíc1 = 0, then ‚àÇL(œâ)"
REFERENCES,0.3546273546273546,"‚àÇœâi
‚â°2c1œâ‚ä§1n‚àí2c3 for all i ‚â§n and any œâ ‚ààRn achieves the same performance."
REFERENCES,0.35544635544635544,"If c2 ‚àíc1 , 0, setting ‚àÇL(œâ)"
REFERENCES,0.35626535626535627,"‚àÇœâi
= 0 returns"
REFERENCES,0.3570843570843571,"œâi =
c3 ‚àíc1
Pn
j=1 œâj
c2 ‚àíc1
:= C
for all i ‚â§n."
REFERENCES,0.3579033579033579,"Therefore the optimal loss is achieved via setting œâ = C1n. Without loss of generality, we can update
W ‚ÜíCW. Then œâ = 1n, and we obtain"
REFERENCES,0.35872235872235875,"min
W,œâ E

x‚ä§WX‚ä§(y ‚äôœâ) ‚àíy
2
= min
W E
h
(x‚ä§WX‚ä§y ‚àíy)2i"
REFERENCES,0.3595413595413595,which completes the proof of (23).
REFERENCES,0.36036036036036034,"A.2
Proof of Lemma 1"
REFERENCES,0.36117936117936117,"Proof. Recap the loss LPGD(W) in (16a) and prediction gPGD(Z) in (15a), we have"
REFERENCES,0.361998361998362,LPGD(W) = E[(y ‚àígPGD(Z))2]
REFERENCES,0.3628173628173628,"= E

x‚ä§Œ≤ + Œæ ‚àíx‚ä§WX‚ä§(XŒ≤ + Œæ)
2"
REFERENCES,0.36363636363636365,"= E
h
(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2 + 2(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)(Œæ ‚àíx‚ä§WX‚ä§Œæ) + (Œæ ‚àíx‚ä§WX‚ä§Œæ)2i"
REFERENCES,0.36445536445536447,"= E
h
(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2 + (Œæ ‚àíx‚ä§WX‚ä§Œæ)2i
+ 2 E[(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)(Œæ ‚àíx‚ä§WX‚ä§Œæ)]"
REFERENCES,0.3652743652743653,"= E
h
(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2 + (Œæ ‚àíx‚ä§WX‚ä§Œæ)2i
(25)"
REFERENCES,0.36609336609336607,"= E
h
(x‚ä§WX‚ä§XŒ≤)2 + (x‚ä§WX‚ä§Œæ)2i"
REFERENCES,0.3669123669123669,"|                                     {z                                     }
f1(W)"
REFERENCES,0.3677313677313677,"‚àí2 E[Œ≤‚ä§xx‚ä§WX‚ä§XŒ≤ + Œæx‚ä§WX‚ä§Œæ]
|                                         {z                                         }
f2(W)"
REFERENCES,0.36855036855036855,"+ E[(x‚ä§Œ≤)2 + Œæ2]
|            {z            }
constant"
REFERENCES,0.36936936936936937,"where (25) follows Assumption 2. Since f2(W) is convex, LPGD(W) is strongly-convex if and only if
f1(W) is strongly-convex, which completes the proof of strong convexity."
REFERENCES,0.3701883701883702,"Next, (20) and (21) in the proof of Lemma 4 demonstrate that the optimal loss is achievable and is
achieved at Œµ = 0. Subsequently, (17) indicates that g‚ãÜ
ATT has the same form as g‚ãÜ
PGD. Under the strong
convexity assumption, g‚ãÜ
PGD is unique, which leads to the conclusion that g‚ãÜ
PGD = g‚ãÜ
ATT."
REFERENCES,0.371007371007371,"A.3
Proof of Lemma 2"
REFERENCES,0.37182637182637185,"Proof. According to Lemma 1, LPGD(W) is strongly-convex as long as either E[(x‚ä§WX‚ä§XŒ≤)2] or
E[(x‚ä§WX‚ä§Œæ)2] is strongly-convex. Therefore, in this lemma, the two claims correspond to the strong
convexity of E[(x‚ä§WX‚ä§Œæ)2] and E[(x‚ä§WX‚ä§XŒ≤)2] terms, respectively."
REFERENCES,0.3726453726453726,"Suppose the decomposition claim holds. Without losing generality, we may assume (x1, Œ≤1, X1)
are zero-mean because we can allocate the mean component to (x2, Œ≤2, X2) without changing the
covariance."
REFERENCES,0.37346437346437344,"‚Ä¢ Claim 1: Let ¬ØŒ£x = E[x1x‚ä§
1 ], ¬ØŒ£Œ≤ = E[Œ≤1Œ≤‚ä§
1 ], and ¬ØŒ£X = E[X‚ä§
1 X1]. If the first claim holds, using
independence, observe that we can write"
REFERENCES,0.37428337428337427,"E[(x‚ä§WX‚ä§Œæ)2] = E[(x‚ä§
1 WX‚ä§
1 Œæ)2] + E[(x‚ä§
1 WX‚ä§
2 Œæ)2] + E[(x‚ä§
2 WX‚ä§
1 Œæ)2] + E[(x‚ä§
2 WX‚ä§
2 Œæ)2],"
REFERENCES,0.3751023751023751,where the last three terms of the right hand side are convex and the first term obeys
REFERENCES,0.3759213759213759,"E[(x‚ä§
1 WX‚ä§
1 Œæ)2] = œÉ2 E[x‚ä§
1 WX‚ä§
1 X1W‚ä§x1]"
REFERENCES,0.37674037674037675,"= œÉ2tr

E[x1x‚ä§
1 WX‚ä§
1 X1W‚ä§]
"
REFERENCES,0.3775593775593776,"= œÉ2tr
 ¬ØŒ£xW ¬ØŒ£XW‚ä§ = œÉ2 q"
REFERENCES,0.3783783783783784,"¬ØŒ£xW
q ¬ØŒ£X  2 F
."
REFERENCES,0.37919737919737917,"Since noise level œÉ > 0, using the full-rankness of covariance matrices ¬ØŒ£x and ¬ØŒ£X, we conclude with
strong convexity of E[(x‚ä§WX‚ä§Œæ)2]."
REFERENCES,0.38001638001638,"‚Ä¢ Claim 2: Now recall that ¬ØŒ£X = E[X‚ä§
1 X1] and set A = X‚ä§
1 X1 ‚àí¬ØŒ£X and B = X‚ä§
2 X2 + ¬ØŒ£X. Observe
that E[A] = 0. If the second claim holds, E[X‚ä§X] = E[A + B]. Note that (A, Œ≤1, x1) are independent
of each other and (B, Œ≤2, x2). Using independence and E[A] = 0, similarly write"
REFERENCES,0.3808353808353808,E[(x‚ä§WX‚ä§XŒ≤)2] = E[(x‚ä§W AŒ≤)2] + E[(x‚ä§WBŒ≤)2].
REFERENCES,0.38165438165438165,"Now using E[Œ≤1] = E[x1] = 0 and their independence from rest, these terms obeys"
REFERENCES,0.3824733824733825,"E[(x‚ä§W AŒ≤)2] = E[(x‚ä§
1 W AŒ≤1)2] + E[(x‚ä§
1 W AŒ≤2)2] + E[(x‚ä§
2 W AŒ≤1)2] + E[(x‚ä§
2 W AŒ≤2)2]"
REFERENCES,0.3832923832923833,"E[(x‚ä§WBŒ≤)2] = E[(x‚ä§
1 WBŒ≤1)2] + E[(x‚ä§
1 WBŒ≤2)2] + E[(x‚ä§
2 WBŒ≤1)2] + E[(x‚ä§
2 WBŒ≤2)2]."
REFERENCES,0.3841113841113841,"In both equations, the last three terms of the right hand side are convex. To proceed, we focus on the
first terms. Using independence and setting Œ£X = E[X‚ä§X] ‚™∞¬ØŒ£X ‚âª0, we note that"
REFERENCES,0.38493038493038495,"E[(x‚ä§
1 W AŒ≤1)2] + E[(x‚ä§
1 WBŒ≤1)2] = E[(x‚ä§
1 WX‚ä§XŒ≤1)2]"
REFERENCES,0.3857493857493858,"where x1, Œ≤1, X are independent and full-rank covariance. To proceed, note that"
REFERENCES,0.38656838656838655,"E[(x‚ä§
1 WX‚ä§XŒ≤1)2] = E[(x‚ä§
1 WŒ£XŒ≤1)2] + E[(x‚ä§
1 W(X‚ä§X ‚àíŒ£X)Œ≤1)2]."
REFERENCES,0.38738738738738737,"Observing the convexity of the right hand side and focusing on the first term, we get"
REFERENCES,0.3882063882063882,"E[(x‚ä§
1 WŒ£XŒ≤1)2] = tr
 ¬ØŒ£xWŒ£X ¬ØŒ£Œ≤Œ£XW‚ä§
=  q"
REFERENCES,0.389025389025389,"¬ØŒ£xWŒ£X
q ¬ØŒ£Œ≤  2 F
."
REFERENCES,0.38984438984438985,"Using the fact that covariance matrices, ¬ØŒ£x, Œ£X, ¬ØŒ£Œ≤, are full rank concludes the strong convexity proof
of E[(x‚ä§WX‚ä§XŒ≤)2]."
REFERENCES,0.3906633906633907,"B
Analysis of General Data Distribution"
REFERENCES,0.3914823914823915,"In this section, we provide the proofs in Section 3, which focuses on solving Objective (5a). For the
sake of clean notation, let L(W) := LPGD(W) and g := gPGD in this section."
REFERENCES,0.3923013923013923,"B.1
Supporting Results"
REFERENCES,0.3931203931203931,We begin by deriving the even moments of random variables.
REFERENCES,0.3939393939393939,"‚Ä¢ 2n‚Äôth moment of a normally distributed variable: Let u ‚àºN(0, œÉ2). Then we have"
REFERENCES,0.39475839475839475,"E[u2n] = œÉ2n(2n ‚àí1)!!.
(26)"
REFERENCES,0.3955773955773956,"‚Ä¢ 4‚Äôth moment: Let u ‚àºN(0, Id). Then for any W,W‚Ä≤ ‚ààRd√ód, we have"
REFERENCES,0.3963963963963964,"E
h
(u‚ä§Wu)(u‚ä§W‚Ä≤u)
i = E"
REFERENCES,0.3972153972153972,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.39803439803439805,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.3988533988533989,"i,j=1
Wijuiuj"
REFERENCES,0.39967239967239965,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.4004914004914005,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.4013104013104013,"i, j=1
W‚Ä≤
ijuiuj"
REFERENCES,0.4021294021294021,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.40294840294840295,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = E
REFERENCES,0.4037674037674038,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.4045864045864046,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.40540540540540543,"i=1
Wiiu2
i"
REFERENCES,0.4062244062244062,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.407043407043407,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.40786240786240785,"i=1
W‚Ä≤
iiu2
i"
REFERENCES,0.4086814086814087,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.4095004095004095,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ E
REFERENCES,0.4103194103194103,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.41113841113841115,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
X"
REFERENCES,0.411957411957412,"i, j
Wi juiuj"
REFERENCES,0.41277641277641275,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.4135954135954136,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
X"
REFERENCES,0.4144144144144144,"i,j
W‚Ä≤
i juiuj"
REFERENCES,0.4152334152334152,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.41605241605241605,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = d
X"
REFERENCES,0.4168714168714169,"i=1
WiiW‚Ä≤
ii E
h
u4
i
i
+
X"
REFERENCES,0.4176904176904177,"i, j
WiiW‚Ä≤
jj E[u2
i ] E[u2
j] +
X"
REFERENCES,0.41850941850941853,"i, j
Wi jW‚Ä≤
i j E[u2
i ] E[u2
j] +
X"
REFERENCES,0.41932841932841936,"i,j
Wi jW‚Ä≤
ji E[u2
i ] E[u2
j] = 3 d
X"
REFERENCES,0.4201474201474201,"i=1
WiiW‚Ä≤
ii +
X"
REFERENCES,0.42096642096642095,"i, j
WiiW‚Ä≤
jj +
X"
REFERENCES,0.4217854217854218,"i,j
Wi jW‚Ä≤
i j +
X"
REFERENCES,0.4226044226044226,"i,j
Wi jW‚Ä≤
ji = d
X"
REFERENCES,0.42342342342342343,"i, j=1
WiiW‚Ä≤
jj + d
X"
REFERENCES,0.42424242424242425,"i,j=1
WijW‚Ä≤
ij + d
X"
REFERENCES,0.4250614250614251,"i,j=1
Wi jW‚Ä≤
ji"
REFERENCES,0.4258804258804259,"= tr (W) tr  W‚Ä≤ + tr

W‚Ä≤W‚ä§
+ tr  WW‚Ä≤ .
(27)"
REFERENCES,0.4266994266994267,"‚Ä¢ 4‚Äôth cross-moment: Let u, v ‚àºN(0, Id) and for any W ‚ààRd√ód, let ŒõW = W ‚äôId. Then we have"
REFERENCES,0.4275184275184275,"E
h
(u‚ä§Wvv‚ä§u)2i = E"
REFERENCES,0.4283374283374283,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.42915642915642915,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.42997542997543,"i,j=1
Wijuivj"
REFERENCES,0.4307944307944308,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.43161343161343163,"2 Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.43243243243243246,"i=1
uivi"
REFERENCES,0.4332514332514332,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.43407043407043405,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = E
REFERENCES,0.4348894348894349,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.4357084357084357,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.43652743652743653,"i,j=1
W2
i ju2
i v2
j +
X"
REFERENCES,0.43734643734643736,"i,i‚Ä≤
Wi jWi‚Ä≤ juiui‚Ä≤v2
j +
X"
REFERENCES,0.4381654381654382,"j, j‚Ä≤
Wi jWi j‚Ä≤u2
i v jvj‚Ä≤ +
X"
REFERENCES,0.438984438984439,"i‚Ä≤,i, j‚Ä≤,j
Wi jWi‚Ä≤ j‚Ä≤uiui‚Ä≤vjv j‚Ä≤"
REFERENCES,0.4398034398034398,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.4406224406224406,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.44144144144144143,"i=1
u2
i v2
i +
X"
REFERENCES,0.44226044226044225,"i, j
uiujviv j"
REFERENCES,0.4430794430794431,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.4438984438984439,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = E
REFERENCES,0.44471744471744473,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.44553644553644556,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.44635544635544633,"i,j=1
W2
iju2
i v2
j"
REFERENCES,0.44717444717444715,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.447993447993448,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.4488124488124488,"i=1
u2
i v2
i"
REFERENCES,0.44963144963144963,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏+
REFERENCES,0.45045045045045046,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
X"
REFERENCES,0.4512694512694513,"i,j
Wi jW jiu2
i u2
jv2
i v2
j"
REFERENCES,0.4520884520884521,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.45290745290745293,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = E
REFERENCES,0.4537264537264537,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.45454545454545453,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.45536445536445536,"i=1
W2
iiu2
i v2
i +
X"
REFERENCES,0.4561834561834562,"i, j
W2
iju2
i v2
j"
REFERENCES,0.457002457002457,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.45782145782145783,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.45864045864045866,"i=1
u2
i v2
i"
REFERENCES,0.4594594594594595,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.46027846027846026,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+
X"
REFERENCES,0.4610974610974611,"i,j
Wi jW ji = E"
REFERENCES,0.4619164619164619,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.46273546273546273,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.46355446355446356,"i=1
W2
iiu4
i v4
i +
X"
REFERENCES,0.4643734643734644,"i,j
W2
iiu2
i v2
i u2
jv2
j"
REFERENCES,0.4651924651924652,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.46601146601146604,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ E
REFERENCES,0.4668304668304668,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.46764946764946763,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
X"
REFERENCES,0.46846846846846846,"i,j
W2
i ju4
i v2
jv2
i +
X"
REFERENCES,0.4692874692874693,"i,j
W2
i ju2
i v4
ju2
j +
X"
REFERENCES,0.4701064701064701,"i,j,k
W2
i ju2
i v2
ju2
kv2
k"
REFERENCES,0.47092547092547093,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.47174447174447176,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+
X"
REFERENCES,0.4725634725634726,"i,j
Wi jW ji = 9 d
X"
REFERENCES,0.47338247338247336,"i=1
W2
ii + (d ‚àí1) d
X"
REFERENCES,0.4742014742014742,"i=1
W2
ii + 6
X"
REFERENCES,0.475020475020475,"i, j
W2
i j + (d ‚àí2)
X"
REFERENCES,0.47583947583947583,"i,j
W2
i j +
X"
REFERENCES,0.47665847665847666,"i, j
Wi jW ji = 3 d
X"
REFERENCES,0.4774774774774775,"i=1
W2
ii + (d + 4) d
X"
REFERENCES,0.4782964782964783,"i,j=1
W2
ij + d
X"
REFERENCES,0.47911547911547914,"i,j=1
Wi jW ji"
REFERENCES,0.4799344799344799,"= 3tr

Œõ2
W

+ (d + 4)tr

WW‚ä§
+ tr

W2
.
(28)"
REFERENCES,0.48075348075348073,"‚Ä¢ 6‚Äôth moment: Let u ‚àºN(0, Id). Then for any W,W‚Ä≤ ‚ààRd√ód, we have"
REFERENCES,0.48157248157248156,"E
h
(u‚ä§Wu)(u‚ä§W‚Ä≤u) ‚à•u‚à•2
‚Ñì2
i = E"
REFERENCES,0.4823914823914824,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.4832104832104832,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.48402948402948404,"i,j=1
Wijuiu j"
REFERENCES,0.48484848484848486,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.4856674856674857,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.4864864864864865,"i,j=1
W‚Ä≤
ijuiu j"
REFERENCES,0.4873054873054873,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.4881244881244881,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.48894348894348894,"i=1
u2
i"
REFERENCES,0.48976248976248976,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.4905814905814906,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = E
REFERENCES,0.4914004914004914,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.49221949221949224,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.49303849303849306,"i=1
Wiiu2
i"
REFERENCES,0.49385749385749383,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.49467649467649466,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.4954954954954955,"i=1
W‚Ä≤
iiu2
i"
REFERENCES,0.4963144963144963,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.49713349713349714,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.49795249795249796,"i=1
u2
i"
REFERENCES,0.4987714987714988,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.4995904995904996,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ E
REFERENCES,0.5004095004095004,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.5012285012285013,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
X"
REFERENCES,0.502047502047502,"i,j
Wi juiuj"
REFERENCES,0.5028665028665029,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5036855036855037,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
X"
REFERENCES,0.5045045045045045,"i, j
W‚Ä≤
i juiu j"
REFERENCES,0.5053235053235053,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5061425061425061,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.506961506961507,"i=1
u2
i"
REFERENCES,0.5077805077805078,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5085995085995086,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = d
X"
REFERENCES,0.5094185094185094,"i=1
WiiW‚Ä≤
ii E"
REFERENCES,0.5102375102375102,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u4
i"
REFERENCES,0.5110565110565111,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5118755118755118,"i‚Ä≤=1
u2
i‚Ä≤"
REFERENCES,0.5126945126945127,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5135135135135135,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+
X"
REFERENCES,0.5143325143325144,"i,j
WiiW‚Ä≤
j j E"
REFERENCES,0.5151515151515151,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u2
i u2
j"
REFERENCES,0.515970515970516,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5167895167895168,"i‚Ä≤=1
u2
i‚Ä≤"
REFERENCES,0.5176085176085176,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5184275184275184,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª +
X"
REFERENCES,0.5192465192465192,"i, j
WijW‚Ä≤
ij E"
REFERENCES,0.5200655200655201,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u2
i u2
j"
REFERENCES,0.5208845208845209,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5217035217035217,"i‚Ä≤=1
u2
i‚Ä≤"
REFERENCES,0.5225225225225225,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5233415233415234,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+
X"
REFERENCES,0.5241605241605242,"i,j
Wi jW‚Ä≤
ji E"
REFERENCES,0.5249795249795249,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u2
i u2
j"
REFERENCES,0.5257985257985258,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5266175266175266,"i‚Ä≤=1
u2
i‚Ä≤"
REFERENCES,0.5274365274365275,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5282555282555282,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.5290745290745291,= (d + 4)
REFERENCES,0.5298935298935299,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠3 d
X"
REFERENCES,0.5307125307125307,"i=1
WiiW‚Ä≤
ii +
X"
REFERENCES,0.5315315315315315,"i, j
WiiW‚Ä≤
jj +
X"
REFERENCES,0.5323505323505323,"i, j
Wi jW‚Ä≤
i j +
X"
REFERENCES,0.5331695331695332,"i, j
Wi jW‚Ä≤
ji"
REFERENCES,0.533988533988534,"Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
(29)"
REFERENCES,0.5348075348075348,= (d + 4)
REFERENCES,0.5356265356265356,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5364455364455365,"i,j=1
WiiW‚Ä≤
jj + d
X"
REFERENCES,0.5372645372645373,"i, j=1
WijW‚Ä≤
ij + d
X"
REFERENCES,0.538083538083538,"i, j=1
Wi jW‚Ä≤
ji"
REFERENCES,0.5389025389025389,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5397215397215397,"= (d + 4)

tr (W) tr  W‚Ä≤ + tr

W‚Ä≤W‚ä§
+ tr  WW‚Ä≤
,
(30)"
REFERENCES,0.5405405405405406,where (29) is obtained by following E
REFERENCES,0.5413595413595413,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u4
i"
REFERENCES,0.5421785421785422,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.542997542997543,"i‚Ä≤=1
u2
i‚Ä≤"
REFERENCES,0.5438165438165438,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5446355446355446,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª= E[u6] + (d ‚àí1) E[u4] E[u2] = 3(d + 4), E"
REFERENCES,0.5454545454545454,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u2
i u2
j"
REFERENCES,0.5462735462735463,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5470925470925471,"i‚Ä≤=1
u2
i‚Ä≤"
REFERENCES,0.547911547911548,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5487305487305487,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª= 2 E[u4] E[u2] + (d ‚àí2) E[u2] E[u2] E[u2] = d + 4.
REFERENCES,0.5495495495495496,"‚Ä¢ 8‚Äôth moment: Let u ‚àºN(0, Id). Then for any W,W‚Ä≤ ‚ààRd√ód, we have"
REFERENCES,0.5503685503685504,"E
h
(u‚ä§Wu)(u‚ä§W‚Ä≤u) ‚à•u‚à•4
‚Ñì2
i = E"
REFERENCES,0.5511875511875511,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.552006552006552,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5528255528255528,"i, j=1
Wijuiuj"
REFERENCES,0.5536445536445537,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5544635544635544,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5552825552825553,"i, j=1
W‚Ä≤
ijuiuj"
REFERENCES,0.5561015561015561,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.556920556920557,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5577395577395577,"i,j=1
u2
i u2
j"
REFERENCES,0.5585585585585585,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5593775593775594,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = E
REFERENCES,0.5601965601965602,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.561015561015561,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5618345618345618,"i=1
Wiiu2
i"
REFERENCES,0.5626535626535627,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5634725634725635,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5642915642915642,"i=1
W‚Ä≤
iiu2
i"
REFERENCES,0.5651105651105651,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5659295659295659,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5667485667485668,"i=1
u4
i +
X"
REFERENCES,0.5675675675675675,"i, j
u2
i u2
j"
REFERENCES,0.5683865683865684,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5692055692055692,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ E
REFERENCES,0.5700245700245701,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.5708435708435708,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
X"
REFERENCES,0.5716625716625716,"i,j
Wi juiuj"
REFERENCES,0.5724815724815725,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5733005733005733,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
X"
REFERENCES,0.5741195741195741,"i,j
W‚Ä≤
i juiu j"
REFERENCES,0.5749385749385749,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5757575757575758,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5765765765765766,"i=1
u4
i +
X"
REFERENCES,0.5773955773955773,"i,j
u2
i u2
j"
REFERENCES,0.5782145782145782,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.579033579033579,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = d
X"
REFERENCES,0.5798525798525799,"i=1
WiiW‚Ä≤
ii E"
REFERENCES,0.5806715806715806,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u4
i"
REFERENCES,0.5814905814905815,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5823095823095823,"i‚Ä≤=1
u4
i‚Ä≤ +
X"
REFERENCES,0.5831285831285832,"i‚Ä≤, j‚Ä≤
u2
i‚Ä≤u2
j‚Ä≤"
REFERENCES,0.583947583947584,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5847665847665847,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+
X"
REFERENCES,0.5855855855855856,"i, j
WiiW‚Ä≤
j j E"
REFERENCES,0.5864045864045864,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u2
i u2
j"
REFERENCES,0.5872235872235873,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.588042588042588,"i‚Ä≤=1
u4
i‚Ä≤ +
X"
REFERENCES,0.5888615888615889,"i‚Ä≤, j‚Ä≤
u2
i‚Ä≤u2
j‚Ä≤"
REFERENCES,0.5896805896805897,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5904995904995906,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª +
X"
REFERENCES,0.5913185913185913,"i, j
WijW‚Ä≤
ij E"
REFERENCES,0.5921375921375921,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u2
i u2
j"
REFERENCES,0.592956592956593,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5937755937755937,"i‚Ä≤=1
u4
i‚Ä≤ +
X"
REFERENCES,0.5945945945945946,"i‚Ä≤, j‚Ä≤
u2
i‚Ä≤u2
j‚Ä≤"
REFERENCES,0.5954135954135954,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.5962325962325963,"Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+
X"
REFERENCES,0.597051597051597,"i, j
Wi jW‚Ä≤
ji E"
REFERENCES,0.5978705978705978,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u2
i u2
j"
REFERENCES,0.5986895986895987,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.5995085995085995,"i‚Ä≤=1
u4
i‚Ä≤ +
X"
REFERENCES,0.6003276003276004,"i‚Ä≤,j‚Ä≤
u2
i‚Ä≤u2
j‚Ä≤"
REFERENCES,0.6011466011466011,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.601965601965602,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.6027846027846028,= (d + 4)(d + 6)
REFERENCES,0.6036036036036037,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠3 d
X"
REFERENCES,0.6044226044226044,"i=1
WiiW‚Ä≤
ii +
X"
REFERENCES,0.6052416052416052,"i,j
WiiW‚Ä≤
j j +
X"
REFERENCES,0.6060606060606061,"i,j
Wi jW‚Ä≤
i j +
X"
REFERENCES,0.6068796068796068,"i,j
Wi jW‚Ä≤
ji"
REFERENCES,0.6076986076986077,"Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
(31)"
REFERENCES,0.6085176085176085,= (d + 4)(d + 6)
REFERENCES,0.6093366093366094,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.6101556101556102,"i, j=1
WiiW‚Ä≤
jj + d
X"
REFERENCES,0.6109746109746109,"i, j=1
Wi jW‚Ä≤
i j + d
X"
REFERENCES,0.6117936117936118,"i, j=1
Wi jW‚Ä≤
ji"
REFERENCES,0.6126126126126126,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.6134316134316135,"= (d + 4)(d + 6)

tr (W) tr  W‚Ä≤ + tr

W‚Ä≤W‚ä§
+ tr  WW‚Ä≤
.
(32)"
REFERENCES,0.6142506142506142,where (31) is obtained by following E
REFERENCES,0.6150696150696151,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u4
i"
REFERENCES,0.6158886158886159,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.6167076167076168,"i‚Ä≤=1
u4
i‚Ä≤ +
X"
REFERENCES,0.6175266175266175,"i‚Ä≤, j‚Ä≤
u2
i‚Ä≤u2
j‚Ä≤"
REFERENCES,0.6183456183456183,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.6191646191646192,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.61998361998362,"= E[u8] + (d ‚àí1) E[u4] E[u4] + 2(d ‚àí1) E[u6] E[u2] + (d ‚àí1)(d ‚àí2) E[u4] E[u2] E[u2]
= 105 + 9(d ‚àí1) + 30(d ‚àí1) + 3(d ‚àí1)(d ‚àí2)
= 3(d + 4)(d + 6), E"
REFERENCES,0.6208026208026208,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞u2
i u2
j"
REFERENCES,0.6216216216216216,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ d
X"
REFERENCES,0.6224406224406225,"i‚Ä≤=1
u4
i‚Ä≤ +
X"
REFERENCES,0.6232596232596233,"i‚Ä≤,j‚Ä≤
u2
i‚Ä≤u2
j‚Ä≤"
REFERENCES,0.6240786240786241,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.6248976248976249,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.6257166257166257,= 2 E[u6] E[u2] + (d ‚àí2) E[u4](E[u2])2 + 2 E[u4] E[u4] + 4(d ‚àí2) E[u4](E[u2])2 + (d ‚àí2)(d ‚àí3)(E[u2])4
REFERENCES,0.6265356265356266,"= 30 + 3(d ‚àí2) + 18 + 12(d ‚àí2) + (d ‚àí2)(d ‚àí3)
= (d + 4)(d + 6)."
REFERENCES,0.6273546273546273,"B.2
Independent Data with General Covariance"
REFERENCES,0.6281736281736282,"Proof of Theorem 1. Consider a general independent linear model as defined in (7) where Œ£x and
Œ£Œ≤ are full-rank feature and task convariance matrices and"
REFERENCES,0.628992628992629,"x ‚àºN(0, Œ£x),
Œ≤ ‚àºN(0, Œ£Œ≤),
Œæ ‚àºN(0, œÉ2),
and
y = x‚ä§Œ≤ + Œæ."
REFERENCES,0.6298116298116299,"Let
X = [x1 ¬∑ ¬∑ ¬∑ xn]‚ä§,
Œæ = [Œæ1 ¬∑ ¬∑ ¬∑ Œæn]‚ä§,
and
y = [y1 ¬∑ ¬∑ ¬∑ yn]‚ä§= XŒ≤ + Œæ."
REFERENCES,0.6306306306306306,"To simplify and without loss of generality, let ¬Øx = Œ£‚àí1/2
x
x, ¬ØX = XŒ£‚àí1/2
x
, ¬ØŒ≤ = Œ£1/2
x Œ≤ where we have"
REFERENCES,0.6314496314496314,"¬Øx ‚àºN(0, I),
¬ØŒ≤ ‚àºN(0, Œ£1/2
x Œ£Œ≤Œ£1/2
x )"
REFERENCES,0.6322686322686323,"and
y = ¬Øx‚ä§¬ØŒ≤ + Œæ,
y = ¬ØX ¬ØŒ≤ + Œæ."
REFERENCES,0.633087633087633,"Then recap the loss from (5a), and we obtain"
REFERENCES,0.6339066339066339,"L(W) = E
h
(y ‚àíg(Z))2i"
REFERENCES,0.6347256347256347,"= E

x‚ä§Œ≤ + Œæ ‚àíx‚ä§WX‚ä§(XŒ≤ + Œæ)
2"
REFERENCES,0.6355446355446356,"= E
h
(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2 + 2(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)(Œæ ‚àíx‚ä§WX‚ä§Œæ) + (Œæ ‚àíx‚ä§WX‚ä§Œæ)2i"
REFERENCES,0.6363636363636364,"= E
h
(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2i
+ E
h
(x‚ä§WX‚ä§Œæ)2i
+ œÉ2,
(33)"
REFERENCES,0.6371826371826372,"where the last equality comes from the independence of label noise Œæ, Œæ."
REFERENCES,0.638001638001638,We first consider the following term
REFERENCES,0.6388206388206388,"E
h
(x‚ä§WX‚ä§Œæ)2i
= E
h
(¬Øx‚ä§(Œ£1/2
x WŒ£1/2
x ) ¬ØX‚ä§Œæ)2i
= nœÉ2 ¬∑ tr
 ¬ØW ¬ØW‚ä§"
REFERENCES,0.6396396396396397,"where we define ¬ØW = Œ£1/2
x WŒ£1/2
x . Next, focus on the following"
REFERENCES,0.6404586404586404,"E
h
(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2i
= E
h
(¬Øx‚ä§¬ØŒ≤ ‚àí¬Øx‚ä§¬ØW ¬ØX‚ä§¬ØX ¬ØŒ≤)2i"
REFERENCES,0.6412776412776413,"= E

¬Øx‚ä§
I ‚àí¬ØW ¬ØX‚ä§¬ØX
 ¬ØŒ≤
2"
REFERENCES,0.6420966420966421,"= tr

E

I ‚àí¬ØW ¬ØX‚ä§¬ØX

Œ£

I ‚àí¬ØW ¬ØX‚ä§¬ØX
‚ä§"
REFERENCES,0.642915642915643,"= tr (Œ£) ‚àítr

Œ£( ¬ØW + ¬ØW‚ä§) E[ ¬ØX‚ä§¬ØX]

+ tr
 ¬ØW‚ä§¬ØW E[ ¬ØX‚ä§¬ØXŒ£ ¬ØX‚ä§¬ØX]
"
REFERENCES,0.6437346437346437,"= tr (Œ£) ‚àí2n ¬∑ tr

Œ£ ¬ØW

+ tr
 ¬ØW‚ä§¬ØW E[ ¬ØX‚ä§¬ØXŒ£ ¬ØX‚ä§¬ØX]

,"
REFERENCES,0.6445536445536445,"where Œ£ := Œ£1/2
x Œ£Œ≤Œ£1/2
x ."
REFERENCES,0.6453726453726454,"Let ¬Øxi ‚ààRn be the i‚Äôth column of ¬ØX and Œ£i j be the (i, j)‚Äôth entry of Œ£. Then the (i, j) entry of matrix
¬ØX‚ä§¬ØXŒ£ ¬ØX‚ä§¬ØX is"
REFERENCES,0.6461916461916462,"( ¬ØX‚ä§¬ØXŒ£ ¬ØX‚ä§¬ØX)i j = d
X k=1 d
X"
REFERENCES,0.647010647010647,"p=1
Œ£kp ¬Øx‚ä§
i ¬Øxk ¬Øx‚ä§
p ¬Øxj."
REFERENCES,0.6478296478296478,Then we get
REFERENCES,0.6486486486486487,"i , j :
E
 ¬ØX‚ä§¬ØXŒ£ ¬ØX‚ä§¬ØX
 ij"
REFERENCES,0.6494676494676495,"
= Œ£i j E[¬Øx‚ä§
i ¬Øxi ¬Øx‚ä§
j ¬Øxj] + Œ£ ji E[¬Øx‚ä§
i ¬Øxj ¬Øx‚ä§
i ¬Øx j] = n2Œ£i j + nŒ£ji"
REFERENCES,0.6502866502866503,"i = j :
E
h ¬ØX‚ä§¬ØXŒ£ ¬ØX‚ä§¬ØX
"
REFERENCES,0.6511056511056511,"ii
i
= Œ£ii E
h
¬Øx‚ä§
i ¬Øxi ¬Øx‚ä§
i ¬Øxi
i
+
X"
REFERENCES,0.6519246519246519,"j,i
Œ£j j E
h
¬Øx‚ä§
i ¬Øx j ¬Øx‚ä§
j ¬Øxi
i"
REFERENCES,0.6527436527436528,"= Œ£ii E
h
(x2
i1 + ¬∑ ¬∑ ¬∑ + x2
in)2i
+ n
X"
REFERENCES,0.6535626535626535,"j,i
Œ£ j j"
REFERENCES,0.6543816543816544,"= Œ£ii(3n + n(n ‚àí1)) + n
X"
REFERENCES,0.6552006552006552,"j,i
Œ£ j j = n"
REFERENCES,0.6560196560196561,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠Œ£ii(n + 1) + d
X"
REFERENCES,0.6568386568386568,"j=1
Œ£ j j"
REFERENCES,0.6576576576576577,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.6584766584766585,= n (Œ£ii(n + 1) + tr (Œ£)) .
REFERENCES,0.6592956592956593,"Therefore
E[ ¬ØX‚ä§¬ØXŒ£ ¬ØX‚ä§¬ØX] = n(n + 1)Œ£ + n ¬∑ tr (Œ£) I.
Combining all together results in"
REFERENCES,0.6601146601146601,"L(W) = tr (Œ£) ‚àí2ntr

Œ£ ¬ØW

+ n(n + 1)tr

Œ£ ¬ØW‚ä§¬ØW

+ n(tr (Œ£) + œÉ2)tr
 ¬ØW ¬ØW‚ä§
+ œÉ2,"
REFERENCES,0.6609336609336609,"= M ‚àí2ntr

Œ£ ¬ØW

+ n(n + 1)tr

Œ£ ¬ØW‚ä§¬ØW

+ nMtr
 ¬ØW ¬ØW‚ä§
,
(34)"
REFERENCES,0.6617526617526618,where M := tr (Œ£) + œÉ2. Setting ‚àá¬ØWL(W) = 0 returns
REFERENCES,0.6625716625716626,"‚àí2n ¬∑ Œ£ + 2n(n + 1) ¬∑ Œ£ ¬ØW + 2nM ¬ØW = 0 =‚áí¬ØW‚ãÜ=

(n + 1)I + MŒ£‚àí1‚àí1 ."
REFERENCES,0.6633906633906634,"Then we have
W‚ãÜ= Œ£‚àí1/2
x

(n + 1)I + MŒ£‚àí1‚àí1 Œ£‚àí1/2
x
and
L‚ãÜ= L(W‚ãÜ) = M ‚àíntr

((n + 1)Œ£‚àí1 + MŒ£‚àí2)‚àí1
."
REFERENCES,0.6642096642096642,"B.3
Retrieval Augmented Generation with Œ± Correlation"
REFERENCES,0.665028665028665,"In this section, we consider the retrieval augmented generation (RAG) linear model similar to (9),
where we first draw the query vector x and task vector Œ≤ via"
REFERENCES,0.6658476658476659,"x ‚àºN(0, I)
and
Œ≤ ‚àºN(0, I)."
REFERENCES,0.6666666666666666,"We then draw data (xi)n
i=1 to be used in-context according to the rule corr_coef(x, xi) ‚â•Œ± ‚â•0. Hence,
for i ‚â§n we sample"
REFERENCES,0.6674856674856675,"xi
 x ‚àºN(Œ±x, Œ≥2I),
Œæi ‚àºN(0, œÉ2)
and
yi = x‚ä§
i Œ≤ + Œæi,
(35)"
REFERENCES,0.6683046683046683,which results in (9) by setting Œ≥2 = 1 ‚àíŒ±2.
REFERENCES,0.6691236691236692,"Theorem 4 (Extended version of Theorem 2) Consider linear model as defined in (35). Recap the
objective from (5a) and let W‚ãÜ:= arg minW LPGD(W), and L‚ãÜ= LPGD(W‚ãÜ). Then W‚ãÜand L‚ãÜsatisfy"
REFERENCES,0.6699426699426699,"W‚ãÜ= cI
and
L‚ãÜ= d + œÉ2 ‚àícnd(Œ±2(d + 2) + Œ≥2)
(36) where"
REFERENCES,0.6707616707616708,"c =
Œ±2(d + 2) + Œ≥2"
REFERENCES,0.6715806715806716,Œ±4n(d + 2)(d + 4) + Œ±2Œ≥2(d + 2)(d + 2n + 3) + Œ≥4(d + n + 1) + œÉ2(Œ±2(d + 2) + Œ≥2).
REFERENCES,0.6723996723996724,"Suppose Œ± = O

1/
‚àö"
REFERENCES,0.6732186732186732,"d

, d/n = O (1) and d is sufficiently large. Let Œ∫ = Œ±2d + 1 and Œ≥2 = 1 ‚àíŒ±2.
Then W‚ãÜand L‚ãÜhave approximate forms"
REFERENCES,0.674037674037674,"W‚ãÜ‚âà
1
Œ∫n + d + œÉ2 I
and
L‚ãÜ‚âàd + œÉ2 ‚àí
Œ∫nd
Œ∫n + d + œÉ2 .
(37)"
REFERENCES,0.6748566748566749,"Proof. Here, for clean notation and without loss of generality, we define and rewrite (35) via"
REFERENCES,0.6756756756756757,"gi ‚àºN(0, I),
Œæi ‚àºN(0, œÉ2)
and
xi = Œ±x + Œ≥gi,
yi = (Œ±x + Œ≥gi)‚ä§Œ≤ + Œæi."
REFERENCES,0.6764946764946765,Then we obtain
REFERENCES,0.6773136773136773,"L(W) = E
h
(y ‚àíg(Z))2i"
REFERENCES,0.6781326781326781,"= E

x‚ä§Œ≤ + Œæ ‚àíx‚ä§WX‚ä§(XŒ≤ + Œæ)
2"
REFERENCES,0.678951678951679,"= E
h
(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2 + 2(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)(Œæ ‚àíx‚ä§WX‚ä§Œæ) + (Œæ ‚àíx‚ä§WX‚ä§Œæ)2i"
REFERENCES,0.6797706797706797,"= E
h
(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2i
+ E
h
(x‚ä§WX‚ä§Œæ)2i
+ œÉ2.
(38)"
REFERENCES,0.6805896805896806,"To begin with, let"
REFERENCES,0.6814086814086814,"N1 = tr (W)2 + tr

WW‚ä§
+ tr

W2
,
N2 = tr

WW‚ä§
,
and
N3 = tr (W) ."
REFERENCES,0.6822276822276823,We first focus on the second term in (38)
REFERENCES,0.683046683046683,"E
h
(x‚ä§WX‚ä§Œæ)2i
= E"
REFERENCES,0.6838656838656839,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.6846846846846847,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.6855036855036855,"i=1
Œæix‚ä§W(Œ±x + Œ≥gi)"
REFERENCES,0.6863226863226863,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.6871416871416871,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.687960687960688,"= nœÉ2 E
h
x‚ä§W(Œ±x + Œ≥g)(Œ±x + Œ≥g)‚ä§W‚ä§x
i"
REFERENCES,0.6887796887796888,"= nœÉ2 
Œ±2 E[x‚ä§Wxx‚ä§W‚ä§x] + Œ≥2 E[x‚ä§W gg‚ä§W‚ä§x]
"
REFERENCES,0.6895986895986896,"= nœÉ2 
Œ±2N1 + Œ≥2N2

.
(It follows (27) and independence of x, g.)"
REFERENCES,0.6904176904176904,"Next, the first term in (38) can be decomposed into"
REFERENCES,0.6912366912366913,"E
h
(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2i
= E
h
(x‚ä§Œ≤)2i"
REFERENCES,0.6920556920556921,"|      {z      }
(a)"
REFERENCES,0.6928746928746928,"+ E
h
(x‚ä§WX‚ä§XŒ≤)2i"
REFERENCES,0.6936936936936937,"|                {z                }
(b)"
REFERENCES,0.6945126945126945,"‚àí2E
h
x‚ä§Œ≤x‚ä§WX‚ä§XŒ≤
i"
REFERENCES,0.6953316953316954,"|                  {z                  }
(c) ."
REFERENCES,0.6961506961506961,"In the following, we consider solving (a)-(c) sequentially."
REFERENCES,0.696969696969697,"(a) :
E
h
(x‚ä§Œ≤)2i
= d."
REFERENCES,0.6977886977886978,"(b) :
E
h
(x‚ä§WX‚ä§XŒ≤)2i = E"
REFERENCES,0.6986076986076986,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.6994266994266994,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠x‚ä§W n
X"
REFERENCES,0.7002457002457002,"i=1
(Œ±x + Œ≥gi)(Œ±x + Œ≥gi)‚ä§Œ≤"
REFERENCES,0.7010647010647011,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.7018837018837019,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = E
REFERENCES,0.7027027027027027,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.7035217035217035,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.7043407043407044,"i=1
x‚ä§W(Œ±2xx‚ä§+ Œ≥2gi g‚ä§
i + Œ±Œ≥xg‚ä§
i + Œ±Œ≥gix‚ä§)Œ≤"
REFERENCES,0.7051597051597052,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.7059787059787059,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.7067977067977068,"= Œ±4n2 E
h
(x‚ä§Wxx‚ä§Œ≤)2i
+ Œ≥4 E"
REFERENCES,0.7076167076167076,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.7084357084357085,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.7092547092547092,"i=1
x‚ä§W gig‚ä§
i Œ≤"
REFERENCES,0.7100737100737101,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.7108927108927109,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ Œ±2Œ≥2 E
REFERENCES,0.7117117117117117,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.7125307125307125,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.7133497133497133,"i=1
x‚ä§Wxg‚ä§
i Œ≤"
REFERENCES,0.7141687141687142,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.714987714987715,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ Œ±2Œ≥2 E
REFERENCES,0.7158067158067158,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.7166257166257166,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.7174447174447175,"i=1
x‚ä§W gix‚ä§Œ≤"
REFERENCES,0.7182637182637183,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.719082719082719,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.7199017199017199,"+2Œ±2Œ≥2n2 E
h
x‚ä§Wxx‚ä§Œ≤Œ≤‚ä§gg‚ä§W‚ä§x
i
+ 2Œ±2Œ≥2n E
h
x‚ä§Wxg‚ä§Œ≤x‚ä§W gx‚ä§Œ≤
i ="
REFERENCES,0.7207207207207207,"
Œ±4n2(d + 4)N1 + Œ≥4n(d + n + 1)N2

+

Œ±2Œ≥2ndN1 + Œ±2Œ≥2n(d + 2)N2

+

2Œ±2Œ≥2n2N1 + 2Œ±2Œ≥2nN1
 ="
REFERENCES,0.7215397215397216,"
Œ±4n2(d + 4) + Œ±2Œ≥2n(2n + d + 2)

N1 +

Œ±2Œ≥2n(d + 2) + Œ≥4n(d + n + 1)

N2
= A1N1 + A2N2."
REFERENCES,0.7223587223587223,"(c) :
E
h
x‚ä§Œ≤x‚ä§WX‚ä§XŒ≤
i
= E"
REFERENCES,0.7231777231777232,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ n
X"
REFERENCES,0.723996723996724,"i=1
x‚ä§Œ≤x‚ä§W(Œ±x + Œ≥gi)(Œ±x + Œ≥gi)‚ä§Œ≤"
REFERENCES,0.7248157248157249,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª = E
REFERENCES,0.7256347256347256,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ n
X"
REFERENCES,0.7264537264537264,"i=1
x‚ä§Œ≤x‚ä§W(Œ±2xx‚ä§+ Œ≥2 gig‚ä§
i + Œ±Œ≥xg‚ä§
i + Œ±Œ≥gix‚ä§)Œ≤"
REFERENCES,0.7272727272727273,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.7280917280917281,"= Œ±2n E
h
x‚ä§Œ≤x‚ä§Wxx‚ä§Œ≤
i
+ Œ≥2n E
h
x‚ä§Œ≤x‚ä§W gg‚ä§Œ≤
i"
REFERENCES,0.7289107289107289,= Œ±2n(d + 2)tr (W) + Œ≥2ntr (W)
REFERENCES,0.7297297297297297,"=

Œ±2n(d + 2) + Œ≥2n

N3
= A3N3.
Here, (b) utilizes the 4‚Äôth and 6‚Äôth moment results (27) and (30) and we define
A1 = Œ±4n2(d + 4) + Œ±2Œ≥2n(2n + d + 2)"
REFERENCES,0.7305487305487306,A2 = Œ±2Œ≥2n(d + 2) + Œ≥4n(d + n + 1)
REFERENCES,0.7313677313677314,"A3 = Œ±2n(d + 2) + Œ≥2n.
Then combining all together results in
L(W) = A1N1 + A2N2 ‚àí2A3N3 + nœÉ2(Œ±2N1 + Œ≥2N2) + d + œÉ2.
To find the optimal solution, set ‚àáL(W) = 0 and we obtain
A1‚àáN1 + A2‚àáN2 ‚àí2A3‚àáN3 + nœÉ2(Œ±2‚àáN1 + Œ≥2‚àáN2) = 0.
(39)
Note that we have
‚àáN1 = ‚àá

tr (W)2 + tr

WW‚ä§
+ tr

W2
= 2tr (W) I + 2W + 2W‚ä§"
REFERENCES,0.7321867321867321,"‚àáN2 = ‚àátr

WW‚ä§
= 2W"
REFERENCES,0.733005733005733,"‚àáN3 = ‚àátr (W) = I.
Therefore, (39) returns"
REFERENCES,0.7338247338247338,"2A1

tr (W) I + W + W‚ä§
+ 2A2W ‚àí2A3 + 2nœÉ2(Œ±2(tr (W) I + W + W‚ä§) + Œ≥2W)I = 0,
(40)
which implies that the optimal solution W‚ãÜhas the form of cI for some constant c. Then suppose
W‚ãÜ= cI, we have tr (W) = cd and (40) returns
2A1(d + 2)cI + 2A2cI ‚àí2A3I + 2nœÉ2(Œ±2(d + 2)cI + Œ≥2cI) = 0"
REFERENCES,0.7346437346437347,"=‚áíc =
A3
A1(d + 2) + A2 + nœÉ2(Œ±2(d + 2) + Œ≥2)"
REFERENCES,0.7354627354627354,"=
Œ±2(d + 2) + Œ≥2"
REFERENCES,0.7362817362817363,Œ±4n(d + 2)(d + 4) + Œ±2Œ≥2(d + 2)(d + 2n + 3) + Œ≥4(d + n + 1) + œÉ2(Œ±2(d + 2) + Œ≥2).
REFERENCES,0.7371007371007371,"Then the optimal loss is obtained by setting W‚ãÜ= cI and
L‚ãÜ= L(W‚ãÜ) = A1c2d(d + 2) + A2c2d ‚àí2A3cd + nœÉ2c2d(Œ±2(d + 2) + Œ≥2) + d + œÉ2"
REFERENCES,0.737919737919738,"= c2d

A1(d + 2) + A2 + nœÉ2(Œ±2(d + 2) + Œ≥2)

‚àí2A3cd + d + œÉ2"
REFERENCES,0.7387387387387387,= d + œÉ2 ‚àíA3cd.
REFERENCES,0.7395577395577395,"It completes the proof of (36). Now if assuming Œ± = O

1/
‚àö"
REFERENCES,0.7403767403767404,"d

, d/n = O (1) and sufficiently large
dimension d, we have the approximate"
REFERENCES,0.7411957411957412,"c ‚âà
Œ±2d + 1
Œ±4d2n + Œ±2d(d + 2n) + (d + n) + œÉ2(Œ±2d + 1)"
REFERENCES,0.742014742014742,"=
Œ±2d + 1
(Œ±2d + 1)2n + (Œ±2d + 1)d + œÉ2(Œ±2d + 1)"
REFERENCES,0.7428337428337428,"=
1
(Œ±2d + 1)n + d + œÉ2
and"
REFERENCES,0.7436527436527437,"L‚ãÜ‚âàd + œÉ2 ‚àí
(Œ±2d + 1)nd
(Œ±2d + 1)n + d + œÉ2 ."
REFERENCES,0.7444717444717445,"B.4
Task-feature Alignment with Œ± Correlation"
REFERENCES,0.7452907452907452,"In this section, we consider the task-feature alignment data model similar to (11), where we first draw
task vector Œ≤ via
Œ≤ ‚àºN(0, I).
Then we generate examples (xi, yi)n+1
i=1 according to the rule corr_coef(xi, Œ≤) ‚â•Œ± ‚â•0 via"
REFERENCES,0.7461097461097461,"xi
 Œ≤ ‚àºN(Œ±Œ≤, I),
Œæi ‚àºN(0, œÉ2)
and
yi = Œ≥ ¬∑ x‚ä§
i Œ≤ + Œæi,
(41)"
REFERENCES,0.7469287469287469,which results in (11) by setting Œ≥2 = 1/(Œ±2d + 1).
REFERENCES,0.7477477477477478,"Theorem 5 (Extended version of Theorem 3) Consider linear model as defined in (41). Recap the
objective from (5a) and let W‚ãÜ:= arg minW LPGD(W), and L‚ãÜ= LPGD(W‚ãÜ). Then W‚ãÜand L‚ãÜsatisfy"
REFERENCES,0.7485667485667485,"W‚ãÜ= cI
and
L‚ãÜ= dŒ≥2(‚àÜ0Œ±2 + 1) + œÉ2 ‚àícndŒ≥2(‚àÜ1Œ±4 + 2‚àÜ0Œ±2 + 1)
(42) where"
REFERENCES,0.7493857493857494,"c =
‚àÜ1Œ±4 + 2‚àÜ0Œ±2 + 1
‚àÜ2Œ±6 + ‚àÜ3Œ±4 + ‚àÜ4Œ±2 + (d + n + 1) + œÉ2(‚àÜ0Œ±4 + 2Œ±2 + 1)/Œ≥2"
REFERENCES,0.7502047502047502,"and
Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥"
REFERENCES,0.7510237510237511,"‚àÜ0 = d + 2
‚àÜ1 = (d + 2)(d + 4)
‚àÜ2 = (d + 2)(d + 4)(d + 6)n
‚àÜ3 = (d + 2)(d + 4)(3n + 4)
‚àÜ4 = (d + 2)(3n + d + 3) + (d + 8)."
REFERENCES,0.7518427518427518,"Suppose Œ± = O

1/
‚àö"
REFERENCES,0.7526617526617526,"d

, d/n = O (1) and d is sufficiently large. Let Œ∫ = Œ±2d + 1 and Œ≥2 = 1/Œ∫. Then
W‚ãÜand L‚ãÜhave approximate forms"
REFERENCES,0.7534807534807535,"W‚ãÜ‚âà
1
Œ∫n + (d + œÉ2)/Œ∫
and
L‚ãÜ‚âàd + œÉ2 ‚àí
Œ∫nd
Œ∫n + (d + œÉ2)/Œ∫.
(43)"
REFERENCES,0.7542997542997543,"Proof. Here, for clean notation and without loss of generality, we define and rewrite (41) via"
REFERENCES,0.7551187551187551,"gi ‚àºN(0, I),
Œæi ‚àºN(0, œÉ2)
and
xi = Œ±Œ≤ + gi,
yi = Œ≥x‚ä§
i Œ≤ + Œæi = Œ≥ ¬∑ (Œ±Œ≤ + gi)‚ä§Œ≤ + Œæi."
REFERENCES,0.7559377559377559,"Recap the loss function from (5a), we obtain"
REFERENCES,0.7567567567567568,"L(W) = E
h
(y ‚àíg(Z))2i"
REFERENCES,0.7575757575757576,"= E

Œ≥x‚ä§Œ≤ + Œæ ‚àíx‚ä§WX‚ä§(Œ≥XŒ≤ + Œæ)
2"
REFERENCES,0.7583947583947583,"= E
h
Œ≥2(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2 + 2Œ≥(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)(Œæ ‚àíx‚ä§WX‚ä§Œæ) + (Œæ ‚àíx‚ä§WX‚ä§Œæ)2i"
REFERENCES,0.7592137592137592,"= Œ≥2 E
h
(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2i
+ E
h
(x‚ä§WX‚ä§Œæ)2i
+ œÉ2.
(44)"
REFERENCES,0.76003276003276,"Similar to Appendix B.3, to begin with, let"
REFERENCES,0.7608517608517609,"N1 = tr (W)2 + tr

WW‚ä§
+ tr

W2
,
N2 = tr

WW‚ä§
,
and
N3 = tr (W) ,"
REFERENCES,0.7616707616707616,"and additionally, given ŒõW = W ‚äôI, let"
REFERENCES,0.7624897624897625,"N4 = 3tr

Œõ2
W

+ (d + 4)tr

WW‚ä§
+ tr

W2
."
REFERENCES,0.7633087633087633,We first focus on the second term in (44)
REFERENCES,0.7641277641277642,"E
h
(x‚ä§WX‚ä§Œæ)2i
= E"
REFERENCES,0.764946764946765,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.7657657657657657,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠(Œ±Œ≤ + g)‚ä§W n
X"
REFERENCES,0.7665847665847666,"i=1
Œæi(Œ±Œ≤ + gi)"
REFERENCES,0.7674037674037674,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.7682227682227682,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.769041769041769,"= nœÉ2 E

(Œ±Œ≤ + g)‚ä§W(Œ±Œ≤ + g‚Ä≤)
2"
REFERENCES,0.7698607698607699,"= nœÉ2 
Œ±4 E
h
(Œ≤‚ä§WŒ≤)2i
+ 2Œ±2 E
h
(Œ≤‚ä§W g‚Ä≤)2i
+ E
h
(g‚ä§W g‚Ä≤)2i"
REFERENCES,0.7706797706797707,"= nœÉ2 
Œ±4 
tr (W)2 + tr

W2
+ tr

WW‚ä§
+ (2Œ±2 + 1)tr

WW‚ä§"
REFERENCES,0.7714987714987716,"= nœÉ2 
Œ±4N1 + (2Œ±2 + 1)N2

.
(It follows (27) and independence of Œ≤, g, g‚Ä≤.)"
REFERENCES,0.7723177723177723,"Next, the first term of (44) (omitting Œ≥2) returns the following decomposition:"
REFERENCES,0.7731367731367731,"E
h
(x‚ä§Œ≤ ‚àíx‚ä§WX‚ä§XŒ≤)2i
= E
h
((Œ±Œ≤ + g)‚ä§(Œ≤ ‚àíWX‚ä§XŒ≤))2i"
REFERENCES,0.773955773955774,"= E

Œ±Œ≤‚ä§Œ≤ ‚àíŒ±Œ≤‚ä§WX‚ä§XŒ≤ + g‚ä§Œ≤ ‚àíg‚ä§WX‚ä§XŒ≤
2"
REFERENCES,0.7747747747747747,= Œ±2 E[(Œ≤‚ä§Œ≤)2] + Œ±2 E[(Œ≤‚ä§WX‚ä§XŒ≤)2] + E[(g‚ä§Œ≤)2] + E[(g‚ä§WX‚ä§XŒ≤)2]
REFERENCES,0.7755937755937756,‚àí2Œ±2 E[Œ≤‚ä§Œ≤Œ≤‚ä§WX‚ä§XŒ≤] ‚àí2 E[Œ≤‚ä§gg‚ä§WX‚ä§XŒ≤]
REFERENCES,0.7764127764127764,"= Œ±2d(d + 2) + Œ±2E[(Œ≤‚ä§WX‚ä§XŒ≤)2]
|                {z                }
(a)"
REFERENCES,0.7772317772317773,"+ d + E[(g‚ä§WX‚ä§XŒ≤)2]
|                {z                }
(b)"
REFERENCES,0.778050778050778,"‚àí2Œ±2E[Œ≤‚ä§Œ≤Œ≤‚ä§WX‚ä§XŒ≤]
|                  {z                  }
(c)"
REFERENCES,0.7788697788697788,"‚àí2E[Œ≤‚ä§gg‚ä§WX‚ä§XŒ≤]
|                  {z                  }
(d) ."
REFERENCES,0.7796887796887797,Consider solving (a)-(d) sequentially as follows:
REFERENCES,0.7805077805077805,"To begin with, we use the following decomposition for all (a)-(d):"
REFERENCES,0.7813267813267813,"X‚ä§XŒ≤ = n
X"
REFERENCES,0.7821457821457821,"i=1
xix‚ä§
i Œ≤ = n
X"
REFERENCES,0.782964782964783,"i=1
(Œ±Œ≤ + gi)(Œ±Œ≤ + gi)‚ä§Œ≤ = n
X"
REFERENCES,0.7837837837837838,"i=1
Œ±2Œ≤Œ≤‚ä§Œ≤ + Œ±Œ≤g‚ä§
i Œ≤ + Œ±giŒ≤‚ä§Œ≤ + gig‚ä§
i Œ≤."
REFERENCES,0.7846027846027847,"Then, we have"
REFERENCES,0.7854217854217854,"(a) :
E[(Œ≤‚ä§WX‚ä§XŒ≤)2] = E"
REFERENCES,0.7862407862407862,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.7870597870597871,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.7878787878787878,"i=1
Œ±2Œ≤‚ä§WŒ≤Œ≤‚ä§Œ≤ + Œ±Œ≤‚ä§WŒ≤g‚ä§
i Œ≤ + Œ±Œ≤‚ä§W giŒ≤‚ä§Œ≤ + Œ≤‚ä§W gig‚ä§
i Œ≤"
REFERENCES,0.7886977886977887,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.7895167895167895,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.7903357903357904,"= Œ±4n2 E

Œ≤‚ä§WŒ≤Œ≤‚ä§Œ≤
2
+ Œ±2 E"
REFERENCES,0.7911547911547911,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.7919737919737919,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.7927927927927928,"i=1
Œ≤‚ä§WŒ≤g‚ä§
i Œ≤"
REFERENCES,0.7936117936117936,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.7944307944307945,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ Œ±2 E
REFERENCES,0.7952497952497952,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.7960687960687961,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.7968877968877969,"i=1
Œ≤‚ä§W giŒ≤‚ä§Œ≤"
REFERENCES,0.7977067977067978,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.7985257985257985,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ E
REFERENCES,0.7993447993447993,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.8001638001638002,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.800982800982801,"i=1
Œ≤‚ä§W gig‚ä§
i Œ≤"
REFERENCES,0.8018018018018018,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.8026208026208026,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.8034398034398035,+2Œ±2n E
REFERENCES,0.8042588042588042,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ n
X"
REFERENCES,0.8050778050778051,"i=1
Œ≤‚ä§WŒ≤Œ≤‚ä§Œ≤Œ≤‚ä§W gig‚ä§
i Œ≤"
REFERENCES,0.8058968058968059,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ 2Œ±2 E
REFERENCES,0.8067158067158067,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ n
X"
REFERENCES,0.8075348075348076,"i=1
Œ≤‚ä§WŒ≤g‚ä§
i Œ≤Œ≤‚ä§W giŒ≤‚ä§Œ≤"
REFERENCES,0.8083538083538083,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.8091728091728092,"= Œ±4n2 E

Œ≤‚ä§WŒ≤Œ≤‚ä§Œ≤
2
+ Œ±2n E

Œ≤‚ä§WŒ≤g‚Ä≤‚ä§Œ≤
2
+ Œ±2n E

Œ≤‚ä§W g‚Ä≤Œ≤‚ä§Œ≤
2
+ E"
REFERENCES,0.80999180999181,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.8108108108108109,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.8116298116298116,"i=1
Œ≤‚ä§W gi g‚ä§
i Œ≤"
REFERENCES,0.8124488124488124,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.8132678132678133,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.814086814086814,"+2Œ±2n2 E
h
Œ≤‚ä§WŒ≤Œ≤‚ä§Œ≤Œ≤‚ä§W g‚Ä≤g‚Ä≤‚ä§Œ≤
i
+ 2Œ±2n E
h
Œ≤‚ä§WŒ≤g‚ä§
i Œ≤Œ≤‚ä§W giŒ≤‚ä§Œ≤
i"
REFERENCES,0.8149058149058149,"= Œ±4n2(d + 4)(d + 6)N1 + Œ±2n(d + 4)N1 + Œ±2n(d + 2)(d + 4)N2
(45)
+ n(n ‚àí1)N1 + nN4
(46)"
REFERENCES,0.8157248157248157,"+ 2Œ±2n2(d + 4)N1 + 2Œ±2n(d + 4)N1
(47)"
REFERENCES,0.8165438165438166,"=

Œ±2n(d + 4)(Œ±2n(d + 6) + 2n + 3) + n(n ‚àí1)

N1 + Œ±2n(d + 2)(d + 4)N2 + nN4
(48)"
REFERENCES,0.8173628173628174,"= B1N1 + B2N2 + nN4,"
REFERENCES,0.8181818181818182,"where (45) and (47) utilize (30) and (32), and (46) is obtained via E"
REFERENCES,0.819000819000819,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.8198198198198198,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.8206388206388207,"i=1
Œ≤‚ä§W gig‚ä§
i Œ≤"
REFERENCES,0.8214578214578214,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.8222768222768223,"2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª= n E

Œ≤‚ä§W g‚Ä≤g‚Ä≤‚ä§Œ≤
2
+ n(n ‚àí1) E
h
Œ≤‚ä§W g‚Ä≤ g‚Ä≤‚ä§Œ≤Œ≤‚ä§W g‚Ä≤‚Ä≤g‚Ä≤‚Ä≤‚ä§Œ≤
i"
REFERENCES,0.8230958230958231,"= nN4 + n(n ‚àí1)N1,"
REFERENCES,0.823914823914824,which follows (27) and (28).
REFERENCES,0.8247338247338247,"(b) :
E
h
(g‚ä§WX‚ä§XŒ≤)2i = E"
REFERENCES,0.8255528255528255,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.8263718263718264,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.8271908271908271,"i=1
Œ±2g‚ä§WŒ≤Œ≤‚ä§Œ≤ + Œ±g‚ä§WŒ≤g‚ä§
i Œ≤ + Œ±g‚ä§W giŒ≤‚ä§Œ≤ + g‚ä§W gi g‚ä§
i Œ≤"
REFERENCES,0.828009828009828,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.8288288288288288,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.8296478296478297,"= Œ±4n2 E

g‚ä§WŒ≤Œ≤‚ä§Œ≤
2
+ Œ±2 E"
REFERENCES,0.8304668304668305,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.8312858312858313,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.8321048321048321,"i=1
g‚ä§WŒ≤g‚ä§
i Œ≤"
REFERENCES,0.8329238329238329,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.8337428337428338,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ Œ±2 E
REFERENCES,0.8345618345618345,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.8353808353808354,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.8361998361998362,"i=1
g‚ä§W giŒ≤‚ä§Œ≤"
REFERENCES,0.8370188370188371,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.8378378378378378,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ E
REFERENCES,0.8386568386568387,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.8394758394758395,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.8402948402948403,"i=1
g‚ä§W gig‚ä§
i Œ≤"
REFERENCES,0.8411138411138411,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.8419328419328419,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.8427518427518428,+2Œ±2n E
REFERENCES,0.8435708435708436,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ n
X"
REFERENCES,0.8443898443898444,"i=1
g‚ä§WŒ≤Œ≤‚ä§Œ≤g‚ä§W gig‚ä§
i Œ≤"
REFERENCES,0.8452088452088452,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+ 2Œ±2 E
REFERENCES,0.846027846027846,"Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ n
X"
REFERENCES,0.8468468468468469,"i=1
g‚ä§WŒ≤g‚ä§
i Œ≤g‚ä§W giŒ≤‚ä§Œ≤"
REFERENCES,0.8476658476658476,Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.8484848484848485,"= Œ±4n2 E

g‚ä§WŒ≤Œ≤‚ä§Œ≤
2
+ Œ±2n E

g‚ä§WŒ≤g‚Ä≤‚ä§Œ≤
2
+ Œ±2n E

g‚ä§W g‚Ä≤Œ≤‚ä§Œ≤
2
+ E"
REFERENCES,0.8493038493038493,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.8501228501228502,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.8509418509418509,"i=1
g‚ä§W gig‚ä§
i Œ≤"
REFERENCES,0.8517608517608518,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.8525798525798526,2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
REFERENCES,0.8533988533988534,"+2Œ±2n2 E
h
g‚ä§WŒ≤Œ≤‚ä§Œ≤g‚ä§W g‚Ä≤g‚Ä≤‚ä§Œ≤
i
+ 2Œ±2n E
h
g‚ä§WŒ≤g‚ä§
i Œ≤g‚ä§W giŒ≤‚ä§Œ≤
i"
REFERENCES,0.8542178542178542,"= Œ±4n2(d + 2)(d + 4)N2 + Œ±2n(d + 2)N2 + Œ±2nd(d + 2)N2 + n(d + n + 1)N2
(49)"
REFERENCES,0.855036855036855,"+ 2Œ±2n2(d + 2)N2 + 2Œ±2n(d + 2)N2
(50)"
REFERENCES,0.8558558558558559,"=

Œ±2n(d + 2)(Œ±2n(d + 4) + 2n + d + 3) + n(d + n ‚àí1)

N2
= B3N2,
where (49) and (50) are obtained using (27), (30) and E"
REFERENCES,0.8566748566748567,Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
REFERENCES,0.8574938574938575,"Ô£´Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠ n
X"
REFERENCES,0.8583128583128583,"i=1
g‚ä§W gig‚ä§
i Œ≤"
REFERENCES,0.8591318591318591,Ô£∂Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
REFERENCES,0.85995085995086,"2Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª= n E

g‚ä§W g‚Ä≤g‚Ä≤‚ä§Œ≤
2
+ n(n ‚àí1) E
h
g‚ä§W g‚Ä≤g‚Ä≤‚ä§Œ≤g‚ä§W g‚Ä≤‚Ä≤g‚Ä≤‚Ä≤‚ä§Œ≤
i"
REFERENCES,0.8607698607698607,= n(d + 2)N2 + n(n ‚àí1)N2 = n(n + d + 1)N2.
REFERENCES,0.8615888615888616,"(c) :
E
h
Œ≤‚ä§Œ≤Œ≤‚ä§WX‚ä§XŒ≤
i"
REFERENCES,0.8624078624078624,"= n E
h
Œ≤‚ä§Œ≤Œ≤‚ä§W(Œ±Œ≤ + g‚Ä≤)(Œ±Œ≤ + g‚Ä≤)‚ä§Œ≤
i"
REFERENCES,0.8632268632268633,"= Œ±2n E
h
Œ≤‚ä§Œ≤Œ≤‚ä§WŒ≤Œ≤‚ä§Œ≤
i
+ n E
h
Œ≤‚ä§Œ≤Œ≤‚ä§W g‚Ä≤ g‚Ä≤‚ä§Œ≤
i"
REFERENCES,0.864045864045864,= Œ±2n(d + 2)(d + 4)tr (W) + n(d + 2)tr (W)
REFERENCES,0.8648648648648649,"=

Œ±2n(d + 2)(d + 4) + n(d + 2)

N3
= B4N3."
REFERENCES,0.8656838656838657,"(d) :
E
h
Œ≤‚ä§gg‚ä§WX‚ä§XŒ≤
i"
REFERENCES,0.8665028665028665,"= n E
h
Œ≤‚ä§gg‚ä§W(Œ±Œ≤ + g‚Ä≤)(Œ±Œ≤ + g‚Ä≤)‚ä§Œ≤
i"
REFERENCES,0.8673218673218673,"= Œ±2n E
h
Œ≤‚ä§gg‚ä§WŒ≤Œ≤‚ä§Œ≤
i
+ n E
h
Œ≤‚ä§gg‚ä§W g‚Ä≤ g‚Ä≤‚ä§Œ≤
i"
REFERENCES,0.8681408681408681,= Œ±2n(d + 2)tr (W) + ntr (W)
REFERENCES,0.868959868959869,"=

Œ±2n(d + 2) + n

N3
= B5N3.
Here we define
B1 = Œ±2n(d + 4)(Œ±2n(d + 6) + 2n + 3) + n(n ‚àí1)"
REFERENCES,0.8697788697788698,B2 = Œ±2n(d + 2)(d + 4)
REFERENCES,0.8705978705978706,B3 = Œ±2n(d + 2)(Œ±2n(d + 4) + 2n + d + 3) + n(d + n ‚àí1)
REFERENCES,0.8714168714168714,B4 = Œ±2n(d + 2)(d + 4) + n(d + 2)
REFERENCES,0.8722358722358723,"B5 = Œ±2n(d + 2) + n.
Then combining all together results in
L(W) = Œ≥2 
Œ±2d(d + 2) + d + Œ±2(B1N1 + B2N2 + nN4) + B3N2 ‚àí2Œ±2B4N3 ‚àí2B5N3

+ nœÉ2(Œ±4N1 + (2Œ±2 + 1)N2) + œÉ2"
REFERENCES,0.8730548730548731,"= Œ≥2 
Œ±2B1N1 + (Œ±2B2 + B3)N2 ‚àí2(Œ±2B4 + B5)N3 + Œ±2nN4

+ nœÉ2(Œ±4N1 + (2Œ±2 + 1)N2) + Œ≥2d

Œ±2(d + 2) + 1

+ œÉ2"
REFERENCES,0.8738738738738738,and differentiating it results in
REFERENCES,0.8746928746928747,"‚àáL(W) = Œ≥2 
Œ±2B1‚àáN1 + (Œ±2B2 + B3)‚àáN2 ‚àí2(Œ±2B4 + B5)‚àáN3 + Œ±2n‚àáN4

+nœÉ2(Œ±4‚àáN1+(2Œ±2+1)‚àáN2)."
REFERENCES,0.8755118755118755,"Similar to the proof in Appendix B.3, W‚ãÜhas the form of W‚ãÜ= cI and we have"
REFERENCES,0.8763308763308764,"‚àáN1 = ‚àá

tr (W)2 + tr

WW‚ä§
+ tr

W2
= 2tr (W) I + 2W + 2W‚ä§= 2c(d + 2)I"
REFERENCES,0.8771498771498771,"‚àáN2 = ‚àátr

WW‚ä§
= 2W = 2cI"
REFERENCES,0.877968877968878,‚àáN3 = ‚àátr (W) = I
REFERENCES,0.8787878787878788,"‚àáN4 = ‚àá

3tr

Œõ2
W

+ (d + 4)tr

WW‚ä§
+ tr

W2"
REFERENCES,0.8796068796068796,= 6 ¬∑ diag (ŒõW) + 2(d + 4)W + 2W‚ä§
REFERENCES,0.8804258804258804,= 2c(d + 8)I.
REFERENCES,0.8812448812448812,"Therefore, setting ‚àáL(W) = 0 returns"
REFERENCES,0.8820638820638821,"Œ≥2 
2c(d + 2)Œ±2B1 + 2c(Œ±2B2 + B3) ‚àí2(Œ±2B4 + B5) + 2c(d + 8)Œ±2n

+2cnœÉ2(Œ±4(d+2)+2Œ±2+1) = 0"
REFERENCES,0.8828828828828829,"=‚áíc =
Œ±2B4 + B5
(d + 2)Œ±2B1 + (Œ±2B2 + B3) + (d + 8)Œ±2n + nœÉ2(Œ±4(d + 2) + 2Œ±2 + 1)/Œ≥2"
REFERENCES,0.8837018837018837,"=
Œ±4n(d + 2)(d + 4) + 2Œ±2n(d + 2) + n
Œ±6n2(d + 2)(d + 4)(d + 6) + Œ±4n(d + 2)(d + 4)(3n + 4) + Œ±2n((d + 2)(3n + d + 3) + (d + 8)) + n(d + n + 1) + nœÉ2(Œ±4(d + 2) + 2Œ±2 + 1)/Œ≥2"
REFERENCES,0.8845208845208845,"=
Œ±4(d + 2)(d + 4) + 2Œ±2(d + 2) + 1
Œ±6n(d + 2)(d + 4)(d + 6) + Œ±4(d + 2)(d + 4)(3n + 4) + Œ±2((d + 2)(3n + d + 3) + (d + 8)) + (d + n + 1) + œÉ2(Œ±4(d + 2) + 2Œ±2 + 1)/Œ≥2."
REFERENCES,0.8853398853398854,Then the optimal loss is obtained by setting W‚ãÜ= cI and
REFERENCES,0.8861588861588862,L‚ãÜ= L(W‚ãÜ) = Œ≥2d(Œ±2(d + 2) + 1) + œÉ2 ‚àíŒ≥2(Œ±2B4 + B5)cd.
REFERENCES,0.8869778869778869,"It completes the proof of (42). Now if assuming Œ± = O

1/
‚àö"
REFERENCES,0.8877968877968878,"d

, d/n = O (1), Œ≥2 = 1/(Œ±2d + 1) and
sufficiently large dimension d, we have the approximate"
REFERENCES,0.8886158886158886,"c ‚âà
Œ±4d2 + 2Œ±2d + 1
nŒ±6d3 + 3nŒ±4d2 + (3n + d)Œ±2d + d + n + œÉ2(Œ±4d + 2Œ±2 + 1)/Œ≥2"
REFERENCES,0.8894348894348895,"‚âà
(Œ±2d + 1)2"
REFERENCES,0.8902538902538902,n(Œ±2d + 1)3 + d(Œ±2d + 1) + œÉ2(Œ±2d + 1)
REFERENCES,0.8910728910728911,"‚âà
1
(Œ±2d + 1)n + (d + œÉ2)/(Œ±2d + 1) and"
REFERENCES,0.8918918918918919,"L‚ãÜ‚âàŒ≥2d(Œ±2d + 1) + œÉ2 ‚àí
Œ≥2(Œ±2d + 1)2nd
(Œ±2d + 1)n + (d + œÉ2)/(Œ±2d + 1)"
REFERENCES,0.8927108927108927,"= d + œÉ2 ‚àí
(Œ±2d + 1)nd
(Œ±2d + 1)n + (d + œÉ2)/(Œ±2d + 1)."
REFERENCES,0.8935298935298935,"C
Analysis of Low-Rank Parameterization"
REFERENCES,0.8943488943488943,"C.1
Proof of Lemma 3"
REFERENCES,0.8951678951678952,Proof. Recall the loss function from (34)
REFERENCES,0.895986895986896,"L(W) = M ‚àí2ntr

Œ£ ¬ØW

+ n(n + 1)tr

Œ£ ¬ØW‚ä§¬ØW

+ nMtr
 ¬ØW ¬ØW‚ä§"
REFERENCES,0.8968058968058968,"where ¬ØW = Œ£1/2
x WŒ£1/2
x , Œ£ = Œ£1/2
x Œ£Œ≤Œ£1/2
x
and M = tr (Œ£) + œÉ2. For any ¬ØW, let us parameterize
¬ØW = UEU‚ä§where U ‚ààRd√ór denotes the eigenvectors of ¬ØW and E ‚ààRr√ór is a symmetric square"
REFERENCES,0.8976248976248976,"0
10
20
30
40
50
60
70
80
# in-context samples 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.8984438984438985,Test risk
REFERENCES,0.8992628992628993,"nmax = 30
nmax = 50
nmax = 80"
REFERENCES,0.9000819000819,(a) Linear attention
REFERENCES,0.9009009009009009,"0
10
20
30
40
50
60
70
80
# in-context samples 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.9017199017199017,Test risk
REFERENCES,0.9025389025389026,"nmax = 30
nmax = 50
nmax = 80"
REFERENCES,0.9033579033579033,(b) H3
REFERENCES,0.9041769041769042,"30
40
50
60
70
80
nmax 0.45 0.50 0.55 0.60"
REFERENCES,0.904995904995905,Averaged test risk
REFERENCES,0.9058149058149059,"Linear Att
H3"
REFERENCES,0.9066339066339066,(c) Averaged risk
REFERENCES,0.9074529074529074,"0
10
20
30
40
50
# in-context samples 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.9082719082719083,Test risk
REFERENCES,0.9090909090909091,"Linear Att
H3"
REFERENCES,0.9099099099099099,(d) Evolving Œ≤
REFERENCES,0.9107289107289107,"Figure 4: Further comparison for linear attention and H3. In (a) and (b), given maximum context
lengths nmax, we train linear attention and H3 models to minimize the average loss across all positions
n from 1 to nmax. Averaged test risks are presented in (c). In (d), the task vector Œ≤ evolves gradually
over the context positions i ‚â§n via Œ≤i = (i/n)Œ≤1 + (1 ‚àíi/n)Œ≤2. In both scenarios, H3 outperforms
linear attention benefiting from its additional convolutional filter (c.f. f in (2b)). Implementation
details are discussed in Section 4."
REFERENCES,0.9115479115479116,"matrix. We will first treat U as fixed and optimize E. We will then optimize U. Fixing U, setting
¬ØŒ£ = U‚ä§Œ£U, we obtain"
REFERENCES,0.9123669123669124,"L(E) = M ‚àí2ntr
 ¬ØŒ£E

+ n(n + 1)tr
 ¬ØŒ£E2
+ nMtr

E2
."
REFERENCES,0.9131859131859131,"Differentiating, we obtain"
REFERENCES,0.914004914004914,0.5n‚àí1‚àáL(E) = ‚àí¬ØŒ£ + (n + 1) ¬ØŒ£E + ME.
REFERENCES,0.9148239148239148,Setting ‚àáL(E) = 0 returns
REFERENCES,0.9156429156429157,"E‚ãÜ= (MI + (n + 1) ¬ØŒ£)‚àí1 ¬ØŒ£.
(51)"
REFERENCES,0.9164619164619164,"Let ¬ØŒªi denote the i‚Äôth largest eigenvalue of ¬ØŒ£. Plugging in this value, we obtain the optimal risk as a
function of U is given by"
REFERENCES,0.9172809172809173,"L‚ãÜ(U) = M ‚àín ¬∑ tr
 ¬ØŒ£E‚ãÜ

= M ‚àín ¬∑ tr

(MI + (n + 1) ¬ØŒ£)‚àí1 ¬ØŒ£2
(52)"
REFERENCES,0.9180999180999181,= M ‚àín rX i=1
REFERENCES,0.918918918918919,"¬ØŒª2
i
(n + 1)¬ØŒªi + M = M ‚àín rX i=1"
REFERENCES,0.9197379197379197,"¬ØŒªi
n + 1 + M¬ØŒª‚àí1
i
.
(53)"
REFERENCES,0.9205569205569205,"Now observe that, the right hand side is strictly decreasing function of the eigenvalues ¬ØŒªi of ¬ØŒ£ =
U‚ä§Œ£U. Thus, to minimize L‚ãÜ(U), we need to maximize Pr
i=1
¬ØŒªi
n+1+M¬ØŒª‚àí1
i . It follows from Cauchy"
REFERENCES,0.9213759213759214,"interlacing theorem that ¬ØŒª j ‚â§Œªi where Œªi is the i‚Äôth largest eigenvalue of Œ£ since ¬ØŒ£ is an orthogonal
projection of Œ£ on U. Consequently, we find the desired bound where"
REFERENCES,0.9221949221949222,L‚ãÜ= M ‚àín rX i=1
REFERENCES,0.923013923013923,"Œªi
n + 1 + MŒª‚àí1
i
."
REFERENCES,0.9238329238329238,"The equality holds by setting U to be the top-r eigenvectors of Œ£ and E = E‚ãÜ(U) to be the diagonal
matrix according to (51)."
REFERENCES,0.9246519246519247,"D
Additional Experiments"
REFERENCES,0.9254709254709255,"In this section, we present additional experiments demonstrating that the H3 model can outperform
the linear attention model under different training or data settings. The implementation details are
consistent with those outlined in Section 4."
REFERENCES,0.9262899262899262,"‚Ä¢ H3 outperforms linear attention (Figure 4). Until now, our analysis has established the equiva-
lence between linear attention and H3 models in solving linear ICL problem. Furthermore, we also
investigate settings where H3 could outperform linear attention due to its sample weighting ability.
In Figs. 4a and 4b, instead of training separate models to fit the different context lengths, we train
a single model with fixed max-length nmax and loss is evaluated as the average loss given samples
from 1 to nmax. Such setting has been wildly studied in the previous ICL work [Garg et al., 2022,"
REFERENCES,0.9271089271089271,"Aky√ºrek et al., 2023, Li et al., 2023]. We generate data according to (7) with Œ£x = Œ£Œ≤ = Id and
œÉ = 0, and train 1-layer linear attention (Fig. 4a) and H3 (Fig. 4b) models with different max-lengths
nmax = 30, 50, 80. Comparison between Fig. 4a and 4b shows that 1-layer attention and H3 implement
different algorithms in solving the averaged linear regression problem and H3 is more consistent
in generalizing to longer context lengths. In Fig. 4c, we plot the averaged risks for each model and
H3 outperforms linear attention. Furthermore, in Fig. 4d, we focus on the setting where in-context
examples are generated using evolving task vector Œ≤. Specifically, consider that each sequence corre-
sponds to two individual task parameters Œ≤(1) ‚àºN(0, Id) and Œ≤(2) ‚àºN(0, Id). Then the i‚Äôth sample is
generated via xi ‚àºN(0, Id) and yi = Œ≤‚ä§
i xi where Œ≤i = ŒªiŒ≤(1) + (1 ‚àíŒªi)Œ≤(2) and Œªi = i/n. The results
are reported in Fig. 4d which again shows that H3 achieves better performance compared to linear
attention, as H3 may benefit from the additional convolutional filter (c.f. f in (2b)). Here, dotted
curve represent the theoretical results under i.i.d. and noiseless setting, derived from Corollary 1."
REFERENCES,0.9279279279279279,"E
Extended Related Work"
REFERENCES,0.9287469287469288,"There is growing interest in understanding the mechanisms behind ICL [Brown et al., 2020, Liu
et al., 2023b, Rae et al., 2021] in large language models (LLMs) due to its success in continuously
enabling novel applications for LLMs [GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023].
Towards this, Xie et al. [2022] explain ICL by language model‚Äôs ability to perform implicit Bayesian
inference where, under specific assumptions on the pre-training data distribution, the model infers a
shared latent concept among the in-context examples and leverages the concept to make a prediction.
M√ºller et al. [2021], Hollmann et al. [2022], M√ºller et al. [2023] introduce prior-data fitted network
(PFN) to approximate Bayesian inference on synthetic datasets and use it to perform downstream
tasks such as tabular dataset classification. On the other hand, Olsson et al. [2022] posit induction
heads as the key mechanism enabling ICL in Transformers. Park et al. [2024] study how various
distributional properties of training data aid in the emergence of ICL in Transformers."
REFERENCES,0.9295659295659295,"In the previous work, Garg et al. [2022] explored ICL ability of Transformers. In particular, they
considered in-context prompts where each in-context example is labeled by a target function from
a given function class, including linear models. A number of works have studied this and related
settings to develop a theoretical understanding of ICL [von Oswald et al., 2023, Gatmiry et al., Collins
et al., 2024, Lin and Lee, 2024, Li et al., 2024, Bai et al., 2024, Aky√ºrek et al., 2023, Zhang et al.,
2023, Du et al., 2023]. Aky√ºrek et al. [2023] focus on linear regression and provide a construction
of Transformer weights that can enable a single step of GD based on in-context examples. They
further show that Transformers trained on in-context prompts exhibit behaviors similar to the models
recovered via explicit learning algorithm on the in-context examples in a prompt. Along the similar
line, Von Oswald et al. [2023] provide a construction of weights in linear attention-only Transformers
that can emulate GD steps on in-context examples for a linear regression task. Interestingly, they find
similarity between their constructed networks and the networks resulting from training on in-context
prompts corresponding to linear regression tasks. Similar to this line of work, Dai et al. [2023] argue
that pre-trained language models act as meta-optimizer which utilize attention to apply meta-gradients
to the original language model based on the in-context examples. Focusing on various NLP tasks,
they further connect it to a specific form of explicit fine-tuning that performs gradient updates to
the attention-related parameters. Inspired by the connection between linear attention and GD, they
developed a novel attention mechanism that mirrors the behavior of GD with momentum. Beyond
Transformers, existing work [Lee et al., 2023, Zucchet et al., 2023, Grazzi et al., 2024] demonstrate
that other model architectures, such as SSM and RNNs, are also capable of in-context learning (ICL)."
REFERENCES,0.9303849303849304,"Building on these primarily empirical studies, Zhang et al. [2024], Mahankali et al. [2024], Ahn
et al. [2023], Duraisamy [2024] focus on developing a theoretical understanding of Transformers
trained to perform ICL. For single-layer linear attention model trained on in-context prompts for
random linear regression tasks with isotropic Gaussian features and isotropic Gaussian weight vectors,
Mahankali et al. [2024], Ahn et al. [2023] show that the resulting model implements a single step
of GD on in-context examples in a test prompt, thereby corroborating the findings of [Von Oswald
et al., 2023]. They also show that the learned model implements a PGD step, when faced with
anisotropic Gaussian features, with Mahankali et al. [2024] also considering anisotropic Gaussian
weight vectors. Ahn et al. [2023] further study multi-layer model and show that the trained model can
implement a generalization of GD++ algorithm, supporting an empirical observation in Von Oswald
et al. [2023]. On the other hand, Mahankali et al. [2024] extend their single-layer setup to consider"
REFERENCES,0.9312039312039312,"suitable non-linear target functions, showing that learned Transformer again implements a single
step of GD on lineare regression objective. For a single-layer linear attention model, Zhang et al.
[2024] study the optimization dynamics of gradient flow while training such a model on in-context
prompts for random linear regression tasks. Despite the non-convexity of the underlying problem,
they show the convergence to the global minimum of the population objective. Similar to Mahankali
et al. [2024], Ahn et al. [2023], they show that the trained model implements a single step of GD and
PGD for isotropic and anisotropic Gaussian features, respectively. In addition, they also characterize
the test-time prediction error for the trained model while highlighting its dependence on train and test
prompt lengths. Interestingly, Zhang et al. [2024] further explore the effect of various distributional
shifts, including the shift in task weight vector distributions between train and test time as well
as the covariate shifts among train and test in-context prompts. Interestingly, they find that while
linear-attention models are robust to most shifts, they exhibit brittleness to the covariate shifts."
REFERENCES,0.9320229320229321,"While our work shares similarities with this line of works, as discussed in our contributions in the
introduction, we expand the theoretical understanding of ICL along multiple novel dimensions,
which includes the first study of LoRA adaptation for ICL in the presence of a distributional shift.
Furthermore, we strive to capture the effect of retrieval augmentation [Lewis et al., 2020, Nakano
et al., 2021] on ICL through our analysis. Retrieval augmentation allows for selecting most relevant
demonstration out of a large collection for a test instance, e.g., via a dense retrieval model [Izacard
et al., 2023], which can significantly outperform the typical ICL setup where fixed task-specific
demonstrations are provided as in-context examples [Wang et al., 2022, Basu et al., 2023]. Through
a careful modeling of retrieval augmentation via correlated design, we show that it indeed has
a desirable amplification effect where the effective number in-context examples becomes larger
with higher correlation which corresponds to preforming a successful retrieval of query-relevant
demonstrations in a practical retrieval augmented setup."
REFERENCES,0.9328419328419328,"Recently, state space models (SSMs) [Gu et al., 2021b,a, Fu et al., 2023, Gu and Dao, 2023] have
appeared as potential alternatives to Transformer architecture, with more efficient scaling to input
sequence length. Recent studies demonstrate that such SSMs can also perform ICL for simple
non-language tasks [Park et al., 2024, Grazzi et al., 2024] as well as complex NLP tasks [Grazzi et al.,
2024]. That said, a rigorous theoretical understanding of ICL for SSMs akin to Zhang et al. [2024],
Mahankali et al. [2024], Ahn et al. [2023] is missing from the literature. In this work, we provide the
first such theoretical treatment for ICL with SSMs. Focusing on H3 architecture [Fu et al., 2023], we
highlight its advantages over linear attention in specific ICL settings."
REFERENCES,0.9336609336609336,NeurIPS Paper Checklist
CLAIMS,0.9344799344799345,1. Claims
CLAIMS,0.9352989352989353,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: All the theoretical contributions claimed in the abstract and introduction
along with the underlying data model are presented in Section 2 and Section 3.
Guidelines:"
CLAIMS,0.9361179361179361,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.9369369369369369,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: This is a theoretical study which (similar to prior studies in the field)
relies on a precise but simplified data model to draw quantitatively precise conclusions.
All the assumptions on the data model are clearly stated in Section 2 and Section 3.
We have also added a paragraph after the conclusion to specifically highlight various
limitations of our work.
Guidelines:"
CLAIMS,0.9377559377559378,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9385749385749386,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
CLAIMS,0.9393939393939394,Answer: [Yes]
CLAIMS,0.9402129402129402,"Justification: As discussed above, for all of our theoretical results and proofs we state
the precise setup and assumptions in Section 2 and Section 3. Due to page limit, proofs
are deferred to the supplemental material."
CLAIMS,0.941031941031941,Guidelines:
CLAIMS,0.9418509418509419,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9426699426699426,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9434889434889435,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9443079443079443,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9451269451269452,"Justification: This is primarily a theoretical work where detailed synthetic experiments
(on the same data model studied in our theoretical analysis) have been conducted to
corroborate our theoretical findings. We provide sufficient details in Section 4 for
reproducing these experiments."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9459459459459459,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9467649467649467,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset)."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9475839475839476,"(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9484029484029484,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9492219492219492,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.95004095004095,Answer: [No]
OPEN ACCESS TO DATA AND CODE,0.9508599508599509,"Justification: As discussed above, this paper conducts small scale synthetic experiments
to corroborate our theoretical findings. We have provided sufficient details to reproduce
these experiments in Section 4."
OPEN ACCESS TO DATA AND CODE,0.9516789516789517,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9524979524979525,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9533169533169533,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9541359541359541,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.954954954954955,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9557739557739557,"Justification: All the relevant details for our small scale experiments are provided in
Section 4."
OPEN ACCESS TO DATA AND CODE,0.9565929565929566,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9574119574119574,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9582309582309583,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.959049959049959,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9598689598689598,Answer: [No]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9606879606879607,"Justification: This work is a theoretical work studying the optimization landscape of
linear attention/H3 under population risk. Then our goal of simulations is to find the
optimal solution corresponding to the minimal risks. Therefore, we do not report the
error bars and we have included the discussion in the experiment section."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9615069615069615,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9623259623259623,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9631449631449631,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.963963963963964,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9647829647829648,Answer: [No]
EXPERIMENTS COMPUTE RESOURCES,0.9656019656019657,"Justification: Our work only focuses on 1-layer attention/H3 model training with hidden
dimension 21 and maximal context length < 100, which can be implemented easily."
EXPERIMENTS COMPUTE RESOURCES,0.9664209664209664,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9672399672399672,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper)."
CODE OF ETHICS,0.9680589680589681,9. Code Of Ethics
CODE OF ETHICS,0.9688779688779688,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9696969696969697,Answer: [Yes]
CODE OF ETHICS,0.9705159705159705,"Justification: Yes, the authors confirm that the research conducted in the paper conform
wiht the NeurIPS Code of Ethics."
CODE OF ETHICS,0.9713349713349714,Guidelines:
CODE OF ETHICS,0.9721539721539721,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics."
CODE OF ETHICS,0.972972972972973,"‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9737919737919738,10. Broader Impacts
BROADER IMPACTS,0.9746109746109746,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9754299754299754,Answer: [Yes]
BROADER IMPACTS,0.9762489762489762,"Justification: In its current form, we don‚Äôt see any specific negative impacts of our
theoretical study. However, we have discussed potential broader impacts of the future
extensions of this work, e.g., the ones tied to eliciting undesirable behavior of LLMs with
in-context learning, while discussing the limitations of the work after the conclusion
section."
BROADER IMPACTS,0.9770679770679771,Guidelines:
BROADER IMPACTS,0.9778869778869779,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9787059787059788,11. Safeguards
SAFEGUARDS,0.9795249795249795,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9803439803439803,Answer: [NA]
SAFEGUARDS,0.9811629811629812,Justification: The synthetic setup studied in the paper does not pose such risks.
SAFEGUARDS,0.9819819819819819,Guidelines:
SAFEGUARDS,0.9828009828009828,"‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9836199836199836,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9844389844389845,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9852579852579852,Answer: [NA]
LICENSES FOR EXISTING ASSETS,0.9860769860769861,Justification: The paper does not rely on existing assets.
LICENSES FOR EXISTING ASSETS,0.9868959868959869,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9877149877149877,"‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators."
NEW ASSETS,0.9885339885339886,13. New Assets
NEW ASSETS,0.9893529893529893,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9901719901719902,Answer:[NA]
NEW ASSETS,0.990990990990991,"Justification: The paper does not release new assets such as code, data, or models."
NEW ASSETS,0.9918099918099919,Guidelines:
NEW ASSETS,0.9926289926289926,"‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934479934479934,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942669942669943,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995085995085995,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959049959049959,"Justification: The paper does not involve crowdsourcing nor research with human
subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967239967239967,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9975429975429976,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983619983619983,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human
subjects.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991809991809992,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
