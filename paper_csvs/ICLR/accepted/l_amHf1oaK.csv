Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004405286343612335,"State-of-the-art neural network veriﬁers are fundamentally based on one of two
paradigms: either encoding the whole veriﬁcation problem via tight multi-neuron
convex relaxations or applying a Branch-and-Bound (BaB) procedure leveraging
imprecise but fast bounding methods on a large number of easier subproblems.
The former can capture complex multi-neuron dependencies but sacriﬁces com-
pleteness due to the inherent limitations of convex relaxations. The latter en-
ables complete veriﬁcation but becomes increasingly ineffective on larger and
more challenging networks. In this work, we present a novel complete veriﬁer
which combines the strengths of both paradigms: it leverages multi-neuron re-
laxations to drastically reduce the number of subproblems generated during the
BaB process and an efﬁcient GPU-based dual optimizer to solve the remaining
ones. An extensive evaluation demonstrates that our veriﬁer achieves a new state-
of-the-art on both established benchmarks as well as networks with signiﬁcantly
higher accuracy than previously considered. The latter result (up to 28% certi-
ﬁcation gains) indicates meaningful progress towards creating veriﬁers that can
handle practically relevant networks."
INTRODUCTION,0.00881057268722467,"1
INTRODUCTION"
INTRODUCTION,0.013215859030837005,"Recent years have witnessed substantial interest in methods for certifying properties of neural net-
works, ranging from stochastic approaches (Cohen et al., 2019) which construct a robust model from
an underlying base classiﬁer to deterministic ones (Gehr et al., 2018; Katz et al., 2017; Xu et al.,
2020) that analyze a given network as is (the focus of our work)."
INTRODUCTION,0.01762114537444934,"Key Challenge: Scalable and Precise Non-Linearity Handling
Deterministic veriﬁcation meth-
ods can be categorized as complete or incomplete. Recent incomplete veriﬁcation methods based
on propagating and reﬁning a single convex region (Müller et al., 2022; Dathathri et al., 2020; Tjan-
draatmadja et al., 2020) are limited in precision due to fundamental constraints imposed by convex
relaxations. Traditional complete veriﬁcation approaches based on SMT solvers (Ehlers, 2017) or
a single mixed-integer linear programming encoding of a property (Tjeng et al., 2019; Katz et al.,
2017) suffer from worst-case exponential complexity and are often unable to compute sound bounds
in reasonable time-frames. To address this issue, a Branch-and-Bound approach (Bunel et al., 2020)
has been popularized recently: anytime-valid bounds are computed by recursively splitting the prob-
lem domain into easier subproblems and deriving bounds on each of these via cheap and less precise
methods (Xu et al., 2021; Wang et al., 2021; Palma et al., 2021; Henriksen & Lomuscio, 2021).
This approach has proven effective on (smaller) networks where there are relatively few unstable
activations and splitting a problem simpliﬁes it substantially. However, for larger networks or those
not regularized to be amenable to certiﬁcation this strategy becomes increasingly ineffective as the
larger number of unstable activations makes individual splits less effective, which is exacerbated by
the relatively loose underlying bounding methods."
INTRODUCTION,0.022026431718061675,"This Work: Branch-and-Bound guided by Multi-Neuron Constraints
In this work, we propose
a novel certiﬁcation method and veriﬁer, called Multi-Neuron Constraint Guided BaB (MN-BAB),
which aims to combine the best of both worlds: it leverages the tight multi-neuron constraints pro-
posed by Müller et al. (2022) within a BaB framework to yield an efﬁcient GPU-based dual method."
INTRODUCTION,0.02643171806167401,Published as a conference paper at ICLR 2022
INTRODUCTION,0.030837004405286344,"The key insight is that the signiﬁcantly increased precision of the underlying bounding method
substantially reduces the number of domain splits (carrying exponential cost) required to certify a
property. This improvement is especially pronounced for larger and less regularized networks where
additional splits of the problem domain yield diminishing returns. We release all code and scripts to
reproduce our experiments at https://github.com/eth-sri/mn-bab."
INTRODUCTION,0.03524229074889868,Main Contributions:
INTRODUCTION,0.039647577092511016,"• We present a novel veriﬁcation framework, MN-BAB, which leverages tight multi-neuron
constraints and a GPU-based dual solver in a BaB approach.
• We develop a novel branching heuristic, ACS, based on information obtained from analyz-
ing our multi-neuron constraints.
• We propose a new class of branching heuristics, CAB, applicable to all BaB-based veri-
ﬁers, that correct the expected bound improvement of a branching decision for the incurred
computational cost.
• Our extensive empirical evaluation demonstrates that we improve on the state of the art in
terms of certiﬁed accuracy by as much as 28% on challenging networks."
BACKGROUND,0.04405286343612335,"2
BACKGROUND"
BACKGROUND,0.048458149779735685,"In this section, we review the necessary background for our method (discussed next)."
NEURAL NETWORK VERIFICATION,0.05286343612334802,"2.1
NEURAL NETWORK VERIFICATION"
NEURAL NETWORK VERIFICATION,0.05726872246696035,"The neural network veriﬁcation problem can be deﬁned as follows: given a network f : X →Y, an
input region D ⊆X, and a linear property P ⊆Y over the output neurons y ∈Y, prove f(x) ∈P,
∀x ∈D. We instantiate this problem with the challenging ℓ∞-norm bounded perturbations and set
D to the ℓ∞ball around an input point x0 of radius ϵ: Dϵ(x0) = {x ∈X | ||x −x0||∞≤ϵ}."
NEURAL NETWORK VERIFICATION,0.06167400881057269,"For ease of presentation, we consider neural networks of L fully-connected layers with ReLU acti-
vation functions (we note that MN-BAB can handle a wide range of layers including convolutional,
residual, batch-normalization, and average-pooling layers). We focus on ReLU networks as the BaB
framework only yields complete veriﬁers for piecewise linear activation functions but remark that
our approach is applicable to a wide class of activations including ReLU, Sigmoid, Tanh, MaxPool,
and others. We denote the number of neurons in the ith layer as di and the corresponding weights and
biases as W (i) ∈Rdi×di−1 and b(i) ∈Rdi for i ∈{1, ..., L}. Further, the neural network is deﬁned
as f(x) := ˆz(L)(x) where ˆz(i)(x) := W (i)z(i−1)(x) + b(i) and z(i)(x) := max(0, ˆz(i)(x)). For
readability, we omit the dependency of intermediate activations on x."
NEURAL NETWORK VERIFICATION,0.06607929515418502,"Since we can encode any linear property over output neurons into an additional afﬁne layer, we can
simplify the general formulation f(x) ∈P to f(x) > 0. The property can now be veriﬁed by
proving that a lower bound to the following optimization problem is greater 0:"
NEURAL NETWORK VERIFICATION,0.07048458149779736,"min
x∈Dϵ(x0)
f(x) = ˆz(L)"
NEURAL NETWORK VERIFICATION,0.07488986784140969,"s.t.
ˆz(i) = W (i)z(i−1) + b(i)"
NEURAL NETWORK VERIFICATION,0.07929515418502203,"z(i) = max(0, ˆz(i)) (1)"
NEURAL NETWORK VERIFICATION,0.08370044052863436,"A method is called sound if every property it proves actually holds (no false positives). A method is
called complete if it can prove every property that actually holds (no false negatives)."
BRANCH-AND-BOUND FOR VERIFICATION,0.0881057268722467,"2.2
BRANCH-AND-BOUND FOR VERIFICATION"
BRANCH-AND-BOUND FOR VERIFICATION,0.09251101321585903,"Bunel et al. (2020) successfully applied the Branch-and-Bound (BaB) approach (lan, 1960) to neural
network veriﬁcation. It consists of a bounding method that computes sound upper and lower bounds
on the optimization objective of Eq. (1) and a branching method that recursively splits the problem
into subproblems with added constraints, allowing for increasingly tighter bounds. If an upper bound
(primal solution) < 0 is found, this represents a counterexample and allows to terminate the proce-
dure. If a lower bound > 0 is obtained, the property is veriﬁed on the corresponding (sub-)domain."
BRANCH-AND-BOUND FOR VERIFICATION,0.09691629955947137,Published as a conference paper at ICLR 2022
BRANCH-AND-BOUND FOR VERIFICATION,0.1013215859030837,"If a lower bound > 0 is derived on all subdomains, the property is veriﬁed. An ideal splitting pro-
cedure minimizes the total time required for bounding, which is often well approximated by the
number of considered subproblems. A simple approach is to split the input domain, however, this is
inefﬁcient for high-dimensional input spaces. Splitting a ReLU activation node into its positive and
negative phases has been shown to be far more efﬁcient (Bunel et al., 2020) and ultimately yields a
complete veriﬁer (Wang et al., 2021). Hence, we focus solely on ReLU branching strategies."
LINEAR CONSTRAINTS,0.10572687224669604,"2.3
LINEAR CONSTRAINTS x y lx ux"
LINEAR CONSTRAINTS,0.11013215859030837,"y ≤
ux
ux−lx (x −lx) y ≥αx"
LINEAR CONSTRAINTS,0.1145374449339207,"y = max(0, x)"
LINEAR CONSTRAINTS,0.11894273127753303,"Figure 1:
Illustration of the DEEP-
POLY relaxation of a ReLU activation
y = max(x, 0) given the neuron-wise
bounds x ∈[lx, ux] and parametrized
by α ∈[0, 1]."
LINEAR CONSTRAINTS,0.12334801762114538,"The key challenge in neural network veriﬁcation Eq. (1)
is handling the non-linear activations. Stable ReLUs, i.e.,
those which we can show to be always active (ˆz ≥0) or in-
active (ˆz ≤0), can be replaced by linear functions. Unsta-
ble ReLUs, i.e., those that can be either active or inactive
depending on the exact x ∈D, have to be approximated
using a convex relaxation of their input-output set. We
build on the convex relaxation introduced in DEEPPOLY
(Singh et al., 2019b) and shown in Fig. 1. Its key property
is the single linear upper and lower bound, which allows
for efﬁcient bound computation."
MULTI-NEURON CONSTRAINTS,0.1277533039647577,"2.4
MULTI-NEURON CONSTRAINTS"
"-NEURON
SINGLE-NEURON",0.13215859030837004,"2-neuron
single-neuron x1 x2 y2 x1 x2 y2"
"-NEURON
SINGLE-NEURON",0.13656387665198239,"Figure
2:
Comparison
of
multi-
neuron and single-neuron constraints
projected into y2-x1-x2-space. Repro-
duced from Müller et al. (2022)."
"-NEURON
SINGLE-NEURON",0.14096916299559473,"All convex relaxations that consider ReLU neurons in-
dividually are fundamentally limited in their precision
by the so-called (single-neuron) convex relaxation barrier
(Salman et al., 2019). It can be overcome by considering
multiple neurons jointly (Singh et al., 2019a; Tjandraat-
madja et al., 2020; Müller et al., 2022; Palma et al., 2021),
thereby capturing interactions between these neurons and
obtaining tighter bounds, illustrated in Fig. 2. We leverage
the multi-neuron constraints from Müller et al. (2022), ex-
pressed as a conjunction of linear constraints over the joint
input and output space of a ReLU layer."
CONSTRAINED OPTIMIZATION VIA LAGRANGE MULTIPLIERS,0.14537444933920704,"2.5
CONSTRAINED OPTIMIZATION VIA LAGRANGE MULTIPLIERS"
CONSTRAINED OPTIMIZATION VIA LAGRANGE MULTIPLIERS,0.14977973568281938,"To express constraints as part of the optimization problem, we use the technique of Lagrange mul-
tipliers. Given a constrained minimization problem minx f(x), s.t. g(x) ≤0, we can bound the
objective function with:
min
x f(x) ≥min
x max
λ≥0 f(x) + λg(x)"
CONSTRAINED OPTIMIZATION VIA LAGRANGE MULTIPLIERS,0.15418502202643172,"If a constraint is satisﬁed, i.e., g(x)j ≤0, λj = 0 maximizes the (inner) objective, else, i.e.,
g(x)j > 0, increasing λj allows the objective to grow unboundedly, shifting the minimum over x
until the constraint is satisﬁed. Hence, if λj > 0 after optimization, the constraint is active, i.e., it is
actively enforced and currently satisﬁed with equality."
A MULTI-NEURON RELAXATION BASED BAB FRAMEWORK,0.15859030837004406,"3
A MULTI-NEURON RELAXATION BASED BAB FRAMEWORK"
A MULTI-NEURON RELAXATION BASED BAB FRAMEWORK,0.16299559471365638,"In this section, we describe the two key components of MN-BAB: (i) an efﬁcient bounding method
leveraging multi-neuron constraints, as well as constrained optimization via lagrange multipliers
(§3.1), and (ii) a branching method tailored to it (§3.2)."
EFFICIENT MULTI-NEURON BOUNDING,0.16740088105726872,"3.1
EFFICIENT MULTI-NEURON BOUNDING"
EFFICIENT MULTI-NEURON BOUNDING,0.17180616740088106,"We build on the approach of Singh et al. (2019b), extended by Wang et al. (2021) of deriving a
lower bound f as a function of the network inputs and a set of optimizable parameters. Crucially,
we tighten these relatively loose bounds signiﬁcantly by enforcing precise multi-neuron constraints"
EFFICIENT MULTI-NEURON BOUNDING,0.1762114537444934,Published as a conference paper at ICLR 2022
EFFICIENT MULTI-NEURON BOUNDING,0.18061674008810572,"via Lagrange multipliers. To enable this, we develop a method capable of integrating any linear
constraint over arbitrary neurons anywhere in the network into the optimization objective. At a high
level, we derive linear upper and lower bounds of the form z(i) ≶−az(i−1) + c for every layer’s out-
put in terms of its inputs z(i−1). Then, starting with a linear expression in the last layer’s outputs
z(L) which we aim to bound, we use the linear bounds derived above, to replace z(L) with symbolic
bounds depending only on the previous layer’s values z(L−1). We proceed in this manner recur-
sively until we obtain an expression only in terms of the networks inputs. Below, we describe this
backsubstitution process for ReLU and afﬁne layers."
EFFICIENT MULTI-NEURON BOUNDING,0.18502202643171806,"Afﬁne Layer
Assume any afﬁne layer ˆz(i) = W(i)z(i−1) +b(i) and a lower bound f = ˆa(i) ˆz(i) +
ˆc(i) with respect to its outputs. We then substitute the afﬁne expression for ˆz(i) to obtain:"
EFFICIENT MULTI-NEURON BOUNDING,0.1894273127753304,"f = ˆa(i)W(i)
|
{z
}
a(i−1)
z(i−1) + ˆa(i)b(i) + ˆc(i)
|
{z
}
c(i−1)
= a(i−1)z(i−1) + c(i−1)
(2)"
EFFICIENT MULTI-NEURON BOUNDING,0.19383259911894274,"ReLU Layer
Let f = a(i)z(i) + c(i) be a lower bound with respect to the output of a ReLU
layer z(i) = max(0, ˆz(i)) and l(i) and u(i) bounds on its input s.t. l(i) ≤ˆz(i) ≤u(i), obtained by
recursively applying this bounding procedure or using a cheaper but less precise bounding method
(Singh et al., 2018; Gowal et al., 2018). The backsubstitution through a ReLU layer now consists
of three distinct steps: 1) enforcing multi-neuron constraints, 2) enforcing single-neuron constraints
to replace the dependence on z(i) by ˆz(i), and 3) enforcing split constraints, which we describe in
detail below."
EFFICIENT MULTI-NEURON BOUNDING,0.19823788546255505,"Enforcing Multi-Neuron Constraints We compute multi-neuron constraints as described in Müller
et al. (2022), although our approach is applicable to any linear constraints in the input-output space
of ReLU activations, written as:"
EFFICIENT MULTI-NEURON BOUNDING,0.2026431718061674,"
P (i)
ˆP (i)
−p(i)
"
EFFICIENT MULTI-NEURON BOUNDING,0.20704845814977973,"
z(i)"
EFFICIENT MULTI-NEURON BOUNDING,0.21145374449339208,"ˆz(i)
1 "
EFFICIENT MULTI-NEURON BOUNDING,0.21585903083700442,"≤0.
(3)"
EFFICIENT MULTI-NEURON BOUNDING,0.22026431718061673,"where ˆz(i) are the pre- and z(i) the post-activation values and P (i), ˆP (i), and −p(i) the constraint
parameters. We enforce these constraints using Lagrange multipliers (see §2.5), yielding sound
lower bounds for all γ(i) ∈(R≥0)
ei, where ei denotes the number of multi-neuron constraints in
layer i."
EFFICIENT MULTI-NEURON BOUNDING,0.22466960352422907,"a(i)z(i) + c(i) ≥max
γ(i)≥0 a(i)z(i) + c(i) + γ(i)⊤(P (i)z(i) + ˆP (i) ˆz(i) −p(i))"
EFFICIENT MULTI-NEURON BOUNDING,0.2290748898678414,"= max
γ(i)≥0 (a(i) + γ(i)⊤P (i))
|
{z
}
a′(i)"
EFFICIENT MULTI-NEURON BOUNDING,0.23348017621145375,"z(i) + γ(i)⊤ˆP (i) ˆz(i) + γ(i)⊤(−p(i)) + c(i)
|
{z
}
c′(i)"
EFFICIENT MULTI-NEURON BOUNDING,0.23788546255506607,"Note that this approach can be easily extended to linear constraints over any activations in arbitrary
layers if applied in the last afﬁne layer at the very beginning of the backsubstitution process."
EFFICIENT MULTI-NEURON BOUNDING,0.2422907488986784,"Enforcing Single-Neuron Constraints We now apply the single-neuron DEEPPOLY relaxation with
parametrized slopes α collected in D (see below):"
EFFICIENT MULTI-NEURON BOUNDING,0.24669603524229075,"max
γ(i)≥0 a′(i)z(i) + c′(i) ≥
max
0≤α(i)≤1
γ(i)≥0"
EFFICIENT MULTI-NEURON BOUNDING,0.2511013215859031,a′(i)(D(i) ˆz(i) + b(i)) + c′(i)
EFFICIENT MULTI-NEURON BOUNDING,0.2555066079295154,"=
max
0≤α(i)≤1
γ(i)≥0"
EFFICIENT MULTI-NEURON BOUNDING,0.2599118942731278,"a′(i)D(i)
|
{z
}
a′′(i)
ˆz(i) + a′(i)b(i) + c′(i)
|
{z
}
c′′(i)"
EFFICIENT MULTI-NEURON BOUNDING,0.2643171806167401,The intercept vector b and the diagonal slope matrix D are deﬁned as:
EFFICIENT MULTI-NEURON BOUNDING,0.2687224669603524,"Dj,j ="
EFFICIENT MULTI-NEURON BOUNDING,0.27312775330396477,"



"
EFFICIENT MULTI-NEURON BOUNDING,0.2775330396475771,"


"
EFFICIENT MULTI-NEURON BOUNDING,0.28193832599118945,"1
if lj ≥0 or node j is positively split
0
if uj ≤0 or node j is negatively split
αj
if lj < 0 < uj and aj ≥0
uj
uj−lj
if lj < 0 < uj and aj < 0"
EFFICIENT MULTI-NEURON BOUNDING,0.28634361233480177,Published as a conference paper at ICLR 2022 bj =
EFFICIENT MULTI-NEURON BOUNDING,0.2907488986784141,"(
−ujlj"
EFFICIENT MULTI-NEURON BOUNDING,0.29515418502202645,"uj−lj
if lj < 0 < uj and aj < 0
0
otherwise"
EFFICIENT MULTI-NEURON BOUNDING,0.29955947136563876,"Where we drop the layer index i for readability and αj is the lower bound slope parameter illustrated
in Fig. 1. Note how, depending on whether the sensitivity a(i)
j
of f w.r.t. z(i)
j
has positive or negative"
EFFICIENT MULTI-NEURON BOUNDING,0.3039647577092511,"sign, we substitute z(i)
j
for its lower or upper bound, respectively."
EFFICIENT MULTI-NEURON BOUNDING,0.30837004405286345,"Enforcing Split Constraints We encode split constraints of the form ˆz(i)
j
≥0 or ˆz(i)
j
≤0 using the
diagonal split matrix S as follows, again dropping the layer index i:"
EFFICIENT MULTI-NEURON BOUNDING,0.31277533039647576,"Sj,j = 
 "
EFFICIENT MULTI-NEURON BOUNDING,0.31718061674008813,"−1
positive split: ˆzj ≥0
1
negative split: ˆzj < 0
0
no split"
EFFICIENT MULTI-NEURON BOUNDING,0.32158590308370044,S(i) ˆz(i) ≤0
EFFICIENT MULTI-NEURON BOUNDING,0.32599118942731276,We again enforce these constraints using Lagrange multipliers:
EFFICIENT MULTI-NEURON BOUNDING,0.3303964757709251,"max
0≤α(i)≤1
γ(i)≥0"
EFFICIENT MULTI-NEURON BOUNDING,0.33480176211453744,"a′′(i) ˆz(i) + c′′(i) ≥
max
0≤α(i)≤1
β(i)≥0
γ(i)≥0"
EFFICIENT MULTI-NEURON BOUNDING,0.3392070484581498,"(a′′(i) + β(i)⊤S(i))
|
{z
}
a′′′(i)"
EFFICIENT MULTI-NEURON BOUNDING,0.3436123348017621,"ˆz(i) + c′′(i)
|{z}
c′′′(i)"
EFFICIENT MULTI-NEURON BOUNDING,0.34801762114537443,"Putting everything together, the backsubstitution operation through a ReLU layer is:"
EFFICIENT MULTI-NEURON BOUNDING,0.3524229074889868,"min
x∈D a(i)z(i) + c(i) ≥min
x∈D
max
0≤α(i)≤1
β(i)≥0
γ(i)≥0"
EFFICIENT MULTI-NEURON BOUNDING,0.3568281938325991,"((a(i) + γ(i)⊤P (i))D(i) + β(i)⊤S(i) + γ(i)⊤ˆP (i))
|
{z
}
ˆa(i) ˆz(i)"
EFFICIENT MULTI-NEURON BOUNDING,0.36123348017621143,"+ (a(i) + γ(i)⊤P (i))′b(i) + γ(i)⊤(−p(i)) + c(i)
|
{z
}
ˆc(i) (4)"
EFFICIENT MULTI-NEURON BOUNDING,0.3656387665198238,Full backsubstitution through all layers leads to an optimizable lower bound on f:
EFFICIENT MULTI-NEURON BOUNDING,0.3700440528634361,"min
x∈D f(x) ≥min
x∈D max
0≤α≤1
0≤β
0≤γ"
EFFICIENT MULTI-NEURON BOUNDING,0.3744493392070485,"a(0)x + c(0) ≥max
0≤α≤1
0≤β
0≤γ"
EFFICIENT MULTI-NEURON BOUNDING,0.3788546255506608,"min
x∈D a(0)x + c(0)"
EFFICIENT MULTI-NEURON BOUNDING,0.3832599118942731,"where the second inequality holds due to weak duality. We denote all α(i)
j
from every layer of the
backsubstitution process with α and deﬁne β and γ analogously. The inner minimization over x
has a closed form solution if D is an lp-ball of radius ϵ around x0, given by Hölder’s inequality:"
EFFICIENT MULTI-NEURON BOUNDING,0.3876651982378855,"max
0≤α≤1
0≤β
0≤γ"
EFFICIENT MULTI-NEURON BOUNDING,0.3920704845814978,"min
x∈D a(0)x + c(0) ≥max
0≤α≤1
0≤β
0≤γ"
EFFICIENT MULTI-NEURON BOUNDING,0.3964757709251101,"a(0)x0 −||a(0)⊤||q ϵ + c(0)
(5)"
EFFICIENT MULTI-NEURON BOUNDING,0.4008810572687225,where q is deﬁned s.t. 1 p + 1
EFFICIENT MULTI-NEURON BOUNDING,0.4052863436123348,"q = 1. Since these bounds are sound for any 0 ≤α ≤1, and β, γ ≥0,
we tighten them by using (projected) gradient ascent to optimize these parameters."
EFFICIENT MULTI-NEURON BOUNDING,0.40969162995594716,"We compute all intermediate bounds using the same bounding procedure, leading to two full param-
eter sets for every neuron in the network. To reduce memory requirements, we share parameter sets
between all neurons in the same layer, but keep separate sets for upper and lower bounds."
EFFICIENT MULTI-NEURON BOUNDING,0.41409691629955947,"Upper Bounding the Minimum Objective
Showing an upper bound on the minimum optimiza-
tion objective precluding veriﬁcation (f < 0) allows us to terminate the BaB process early. Prop-
agating any input x ∈D through the network yields a valid upper bound, hence, we use the input
that minimizes Eq. (5): xi ="
EFFICIENT MULTI-NEURON BOUNDING,0.4185022026431718,"(
(x0)i + ϵ
if a(0)
i
< 0
(x0)i −ϵ
if a(0)
i
≥0 ."
EFFICIENT MULTI-NEURON BOUNDING,0.42290748898678415,Published as a conference paper at ICLR 2022
MULTI-NEURON CONSTRAINT GUIDED BRANCHING,0.42731277533039647,"3.2
MULTI-NEURON CONSTRAINT GUIDED BRANCHING"
MULTI-NEURON CONSTRAINT GUIDED BRANCHING,0.43171806167400884,"Generally, the BaB approach is based on recursively splitting an optimization problem into easier
subproblems to derive increasingly tighter bounds. However, the beneﬁt of different splits can vary
widely, making an effective branching heuristic which chooses splits that minimize the total number
of required subproblems a key component of any BaB framework (Bunel et al., 2020). Typically, a
score is assigned based on the expected bound improvement and the most promising split is chosen.
Consequently, the better this score captures the actual bound improvement, the better the resulting
decision. Both the commonly used BABSR (Bunel et al., 2020) and the more recent FSB (De Palma
et al., 2021) approximate improvements under a DEEPPOLY style backsubstitution procedure. As
neither considers the impact of multi-neuron constraints, the scores they compute might not be
suitable proxies for the bound improvements obtained with our method. To overcome this issue,
we propose a novel branching heuristic, Active Constraint Score Branching (ACS), considering
multi-neuron constraints. Further, we introduce a branching heuristic framework which corrects the
expected bound improvement with the potentially signiﬁcantly varying expected computational cost."
MULTI-NEURON CONSTRAINT GUIDED BRANCHING,0.43612334801762115,"Active Constraint Score Branching
The value of a Lagrange parameter γ provides valuable in-
formation about the constraint it enforces. Concretely, γ > 0 indicates that a constraint is active,
i.e., the optimal solution fulﬁlls it with equality. Further, for a constraint g(x) ≤0, a larger ∂xγg(x)
indicates a larger sensitivity of the ﬁnal bound to violations of this constraint. We compute this
sensitivity for our multi-neuron constraints with respect to both ReLU outputs and inputs as γ⊤P
and γ⊤ˆP , respectively, where P and ˆP are the multi-neuron constraint parameters. We then deﬁne
our branching score for a neuron j in layer i as the sum over all sensitivities with respect to its input
or output:
si,j = |γ(i)⊤P (i)|j + |γ(i)⊤ˆP (i)|j,
(6)"
MULTI-NEURON CONSTRAINT GUIDED BRANCHING,0.44052863436123346,"Intuitively, splitting the node with the highest cumulative sensitivity makes its encoding exact and
effectively tightens all of these high sensitivity constraints. We can efﬁciently compute those scores
without an additional backsubstitution pass."
MULTI-NEURON CONSTRAINT GUIDED BRANCHING,0.44493392070484583,"Cost Adjusted Branching
Any complete method will decide every property eventually. Hence,
its runtime is a key performance metric. Existing branching heuristics ignore this aspect and only
consider the expected improvement in bound-tightness, but not the sometimes considerable differ-
ences in computational cost. We propose Cost Adjusted Branching (CAB), scaling the expected
bound improvement (approximated with the branching score) with the inverse of the cost expected
for the split, and then picking the branching decision yielding the highest expected bound improve-
ment per cost. The true cost of a split consists of the direct cost of the next branching step and the
change of cumulative cost for all consecutive steps. We ﬁnd a local approximation considering just
the former component, similar to only considering the one-step bound improvement, to be a good
proxy. We approximate this direct cost by the number of ﬂoating-point operations required to com-
pute the new bounds, refer to App. B for a more detailed description. Note that any approximation
of the expected cost can be used to instantiate CAB."
MULTI-NEURON CONSTRAINT GUIDED BRANCHING,0.44933920704845814,"Enforcing Splits
Once the ReLU node to split on has been determined, two subproblems are
generated. One where a non-negative ˆz ≥0 and one where non-positive ˆz ≤0 pre-activation value
has to be enforced. This is accomplished by setting the corresponding entries in the split matrix
S, used during the bounding process, to −1 and 1, respectively. As the intermediate bounds for all
layers up to and including the one that was split remain unchanged, we do not recompute them."
SOUNDNESS AND COMPLETENESS,0.45374449339207046,"3.3
SOUNDNESS AND COMPLETENESS"
SOUNDNESS AND COMPLETENESS,0.4581497797356828,"The soundness of the BaB approach follows directly from the soundness of the underlying bounding
method discussed in Section 3.1. To show completeness, it is sufﬁcient to consider the limit case
where all ReLU nodes are split and the network becomes linear, making DEEPPOLY relaxations
exact. To also obtain exact bounds, all split constraints have to be enforced. This can be done by
computing optimal β for the now convex optimization problem (Wang et al., 2021). It follows that
a property holds if and only if the exact bounds thus obtained are positive on all subproblems. We
conclude that MN-BAB is a complete veriﬁer."
SOUNDNESS AND COMPLETENESS,0.46255506607929514,Published as a conference paper at ICLR 2022
SOUNDNESS AND COMPLETENESS,0.4669603524229075,"Table 1: Natural accuracy [%] (Acc.), veriﬁed accuracy [%] (Ver.) and its empirical upper bound
[%] and average runtime [s] of the ﬁrst 1000 images of the test set."
SOUNDNESS AND COMPLETENESS,0.4713656387665198,"Dataset
Model
Acc.
ϵ
ERAN
OVAL
β-CROWN
MN-BAB (ours)
BABSR +CAB
MN-BAB (ours)
ACS +CAB
Upper
Bound
Ver.
Time
Ver.
Time
Ver.
Time
Ver.
Time
Ver.
Time"
SOUNDNESS AND COMPLETENESS,0.47577092511013214,"MNIST
ConvSmall
98.0
0.12
73.2
38.4
69.8
26.2
71.6†
46
71.0
21.3
70.3
26.2
73.2"
SOUNDNESS AND COMPLETENESS,0.4801762114537445,"ConvBig
92.9
0.30
78.6
6.0
−
−
77.7†
78
78.3
20.8
77.2
46.2
78.6
ConvSuper
97.7
0.18
0.5
142.0
−
−
−
−
19.2
86.2
17.6
90.9
37.3"
SOUNDNESS AND COMPLETENESS,0.4845814977973568,"CIFAR10
ConvSmall
63.0
2/255 47.2
54.4
46.2
17.7
46.3†
18
46.1
16.4
45.8
18.0
48.1"
SOUNDNESS AND COMPLETENESS,0.4889867841409692,"ConvBig
63.1
2/255 48.2
128.1
50.6
42.0
50.3†
55
49.4
49.5
51.5
37.0
55.0"
SOUNDNESS AND COMPLETENESS,0.4933920704845815,"ResNet6-A ‡
84.0
1/255 45.0
114.6
•
•
52.0
263.4
48.0
202.7
55.0
170.7
75.0"
SOUNDNESS AND COMPLETENESS,0.4977973568281938,"ResNet6-B ‡
79.0
1/255 66.0
48.6
•
•
67.0
105.7
65.0
51.1
67.0
37.8
71.0"
SOUNDNESS AND COMPLETENESS,0.5022026431718062,"ResNet8-A ‡
83.0
1/255 11.0
362.5
•
•
18.0
497.3
19.0
390.7
23.0
371.0
70.0"
SOUNDNESS AND COMPLETENESS,0.5066079295154186,"† We report numbers from Wang et al. (2021). −Errors prevent reporting reliable numbers. ‡ Due to long runtimes, we evaluated only on the
ﬁrst 100 samples of the test set. • OVAL does not support ResNet architectures."
EXPERIMENTAL EVALUATION,0.5110132158590308,"4
EXPERIMENTAL EVALUATION"
EXPERIMENTAL EVALUATION,0.5154185022026432,"We now present an extensive evaluation of our method. First, and perhaps surprisingly, we ﬁnd that
existing MILP-based veriﬁcation tools (Singh et al., 2019c; Müller et al., 2022) are more effective in
verifying robustness on many established benchmark networks than what is considered state-of-the-
art. This highlights that next-generation veriﬁers should focus on and be benchmarked using less
regularized and more accurate networks. We take a step in this direction by proposing and comparing
on such networks, before analyzing the effectiveness of the different components of MN-BAB in an
extensive ablation study."
EXPERIMENTAL EVALUATION,0.5198237885462555,"Experimental Setup
We implement a GPU-based version of MN-BAB in PyTorch (Paszke et al.,
2019) and evaluate all benchmarks using a single NVIDIA RTX 2080Ti, 64 GB of memory, and
16 CPU cores. We attempt to falsify every property with an adversarial attack, before beginning
certiﬁcation. For every subproblem, we ﬁrst lower-bound the objective using DEEPPOLY and then
compute reﬁned bounds using the method described in §3."
EXPERIMENTAL EVALUATION,0.5242290748898678,"Benchmarks
We benchmark MN-BAB on a wide range of networks (see Table 3 in App. A) on
the MNIST (Deng, 2012) and CIFAR10 datasets (Krizhevsky et al., 2009). We also consider three
new residual networks, ResNet6-A, ResNet6-B, and ResNet8-A. ResNet6-A and ResNet6-B have the
same architecture but differ in regularization strength while ResNet8-A has an additional residual
block. All three were trained with adversarial training (Madry et al., 2018) using PGD, the GAMA
loss (Sriramanan et al., 2020) and MixUp data augmentation (Zhang et al., 2021). ResNet6-A and
ResNet8-A were trained using 8-steps and ϵ = 4/255, whereas 20-steps and ϵ = 8/255 were used
for ResNet6-B.We compare against β-CROWN (Wang et al., 2021), a BaB-based state-of-the-art
complete veriﬁer, OVAL (Palma et al., 2021; De Palma et al., 2021; Bunel et al., 2020), a BaB frame-
work based on a different class of multi-neuron constraints, and ERAN Singh et al. (2019c); Müller
et al. (2022) combining the incomplete veriﬁer PRIMA, whose tight multi-neuron constraints we
leverage in our method, and a (partial) MILP encoding."
EXPERIMENTAL EVALUATION,0.5286343612334802,"Comparison to State-of-the-Art Methods
In Table 1, we compare the veriﬁed accuracy and run-
time of MN-BAB with that of state-of-the-art tools ERAN, β-CROWN, and OVAL. Perhaps sur-
prisingly, we ﬁnd that both MNIST and the smallest CIFAR10 benchmark network established over
the last years (Singh et al., 2019b) can be veriﬁed completely or almost completely in less than 50s
per sample using ERAN, making them less relevant as benchmarks for new veriﬁcation tools. On
these networks, all three BaB methods (including ours) are outperformed to a similar degree, with
the reservation that we could not evaluate OVAL on MNIST ConvBig due to runtime errors. On
the remaining unsolved networks where complete veriﬁcation via a MILP encoding does not scale,
MN-BAB consistently obtains the highest certiﬁed accuracy and for all but one also the lowest run-
time. On ConvSuper we were not able to ﬁnd conﬁgurations for OVAL and β-CROWN that did not
run out of memory. Comparing the BaB methods, we observe an overall trend that more precise
but also expensive underlying bounding methods are most effective on larger networks, where addi-
tional splits are less efﬁcient, and complex inter-neuron interactions can be captured by the precise
multi-neuron constraints. On these networks, the more complex interactions also lead to more active
Multi-Neuron Constraints (MNCs) and hence more informative ACS scores."
EXPERIMENTAL EVALUATION,0.5330396475770925,Published as a conference paper at ICLR 2022
EXPERIMENTAL EVALUATION,0.5374449339207048,"0.0
0.5
1.0
Quantile 100 101 102 103"
SUBPROBLEM COUNT RATIO,0.5418502202643172,"104
Subproblem count ratio mean mean"
SUBPROBLEM COUNT RATIO,0.5462555066079295,"ResNet6-A
ResNet6-B"
SUBPROBLEM COUNT RATIO,0.5506607929515418,"Figure 3: Ratio of subprob-
lems required per property
without and with MNCs."
SUBPROBLEM COUNT RATIO,0.5550660792951542,"0.0
0.5
1.0
Quantile 100 101 102"
SUBPROBLEM COUNT RATIO,0.5594713656387665,"103
Subproblem count ratio mean mean"
SUBPROBLEM COUNT RATIO,0.5638766519823789,"ResNet6-A
ResNet6-B"
SUBPROBLEM COUNT RATIO,0.5682819383259912,"Figure 4: Ratio of subprob-
lems required per property
with BABSR and ACS."
SUBPROBLEM COUNT RATIO,0.5726872246696035,"0
150
300
450
Time [s] ACS 0 150 300"
SUBPROBLEM COUNT RATIO,0.5770925110132159,Time [s] ACS+CAB
SUBPROBLEM COUNT RATIO,0.5814977973568282,"ResNet6-A
ResNet6-B"
SUBPROBLEM COUNT RATIO,0.5859030837004405,"Figure 5: Effect of Cost Ad-
justed Branching on mean
veriﬁcation time with ACS."
SUBPROBLEM COUNT RATIO,0.5903083700440529,"Table 2: Veriﬁed accuracy [%] (Ver.) and avg. runtime
[s] on the ﬁrst 100 images of the test set for ϵ = 1/255)."
SUBPROBLEM COUNT RATIO,0.5947136563876652,"Model
Acc.
Upper
Bound
Branching
Method
MNCs
MN-BAB"
SUBPROBLEM COUNT RATIO,0.5991189427312775,"Ver
Time"
SUBPROBLEM COUNT RATIO,0.6035242290748899,"ResNet6-A
84
75
No Branching
no
30
0.4
No Branching
yes
39
13.2
BABSR
no
42
247.1
BABSR +CAB
no
46
219.3
FSB
yes
45
239.7
BABSR
yes
47
212.8
BABSR +CAB
yes
48
202.7
ACS
yes
51
186.4
ACS +CAB
yes
55
170.7"
SUBPROBLEM COUNT RATIO,0.6079295154185022,"ResNet6-B
79
71
No Branching
no
58
0.7
No Branching
yes
61
4.1
BABSR
no
61
78.3
BABSR +CAB
no
63
64.9
FSB
yes
64
67.0
BABSR
yes
63
65.5
BABSR +CAB
yes
65
51.1
ACS
yes
65
52.0
ACS +CAB
yes
67
37.8"
SUBPROBLEM COUNT RATIO,0.6123348017621145,"Ablation Study
To analyze the indi-
vidual components of MN-BaB, we con-
sider the weakly and heavily regular-
ized ResNet6-A and ResNet6-B, respec-
tively, with identical architecture. Con-
cretely,
we show the effect different
bounding procedures and branching ap-
proaches have in the two settings in Ta-
ble 2. As veriﬁed accuracy and mean run-
time are very coarse performance metrics,
we also analyze the ratio of runtimes and
number of subproblems required for ver-
iﬁcation on a per-property level, ﬁltering
out those where both methods verify be-
fore any branching occurred (Figs. 3–5).
Overall, we observe that the number of
visited subproblems required for certiﬁca-
tion can be reduced by two to three orders
of magnitude by leveraging precise multi-
neuron constraints and then again by another one to two orders of magnitude by our novel branching
heuristic, ACS. Our cost-adjusted branching yields an additional speed up of around 50%."
SUBPROBLEM COUNT RATIO,0.6167400881057269,"0
150
300
450
Time [s] wo. MNC BaBSR 0 150 300"
SUBPROBLEM COUNT RATIO,0.6211453744493393,Time [s] MNC ACS+CAB
SUBPROBLEM COUNT RATIO,0.6255506607929515,"ResNet6-A
ResNet6-B"
SUBPROBLEM COUNT RATIO,0.6299559471365639,"Figure 6: Per property veriﬁcation
times using MN-BAB over those
without MNCs and using BABSR."
SUBPROBLEM COUNT RATIO,0.6343612334801763,"The trend of MN-BAB succeeding on more challenging net-
works is conﬁrmed here. Leveraging MNCs enables us to ver-
ify 20% more samples while being around 22% faster (see Ta-
ble 2) on ResNet6-A while on the more heavily regularized
ResNet6-B we only verify 6% more samples. When analyzing
on a per-property level, shown in Fig. 6, the trend is even more
pronounced. For easy problems, leveraging MNCs and ACS
has little impact (points in the lower left-hand corner). How-
ever, it completely dominates on the harder properties where
only using single-neuron constraints and BABSR takes up to
33 times longer (points below the diagonal)."
SUBPROBLEM COUNT RATIO,0.6387665198237885,"Effectiveness of Multi-Neuron Constraints
In Fig. 3, we
show the ratio between the number of subproblems required to prove the same lower bound on the
ﬁnal objective (either 0, if both methods certify, or the smaller of the two lower bounds at termina-
tion) with and without MNCs over the quantile of properties for ResNet6-A (blue) and ResNet6-B
(orange). We observe that using MNCs reduces the number of subproblems for both networks by
between two and three orders of magnitude. Despite the higher per bounding-step cost, this leads to
the use of MNCs increasing the number of veriﬁed samples by up to 12% while reducing average
certiﬁcation times (see Table 2)."
SUBPROBLEM COUNT RATIO,0.6431718061674009,"Effectiveness of ACS Branching
In Fig. 4, we show the ratio between the number of subprob-
lems considered when using BABSR vs. ACS over the quantile of properties. We observe that
ACS yields signiﬁcantly fewer subproblems on most (75%) or all properties on ResNet6-A and"
SUBPROBLEM COUNT RATIO,0.6475770925110133,Published as a conference paper at ICLR 2022
SUBPROBLEM COUNT RATIO,0.6519823788546255,"ResNet6-B, respectively, leading to an additional reduction by between one and two orders of mag-
nitude and showing the effectiveness of our novel ACS branching heuristic. Average veriﬁcation
times are reduced by 12% and 21% on ResNet6-A and ResNet6-B, respectively. Note that the rel-
atively small improvements in timings are due to timeouts for both methods yielding equally high
runtimes which dominate the mean. FSB is consistently outperformed by ACS, certifying 12% less
samples on ResNet6-A."
SUBPROBLEM COUNT RATIO,0.6563876651982379,"Effectiveness of Cost Adjusted Branching
In Fig. 5, we show the per property veriﬁcation times
with ACS + CAB over those with ACS. Using CAB is faster (points below the dashed line) for all
properties, sometimes signiﬁcantly so, leading to an average speedup of around 50%. Analyzing
the results in Table 2, we observe that CAB is particularly effective in combination with the ACS
scores and multi-neuron constraints, where bounding costs vary more signiﬁcantly."
RELATED WORK,0.6607929515418502,"5
RELATED WORK"
RELATED WORK,0.6651982378854625,"Neural Network Veriﬁcation Beyond the Single Neuron Convex Barrier
After the so-called
(Single Neuron) Convex Barrier has been described by Salman et al. (2019) for incomplete
relaxation-based methods, a range of approaches has been proposed that consider multiple neu-
rons jointly to obtain tighter relaxations. Singh et al. (2019a) and Müller et al. (2022) derive joint
constraints over the input-output space of groups of neurons and reﬁne their relaxation using the
intersection of these constraints. Tjandraatmadja et al. (2020) merge the ReLU and preceding afﬁne
layer to consider multiple inputs but only one output at a time. While the two approaches are theo-
retically incomparable, the former yields empirically better results (Müller et al., 2022)."
RELATED WORK,0.6696035242290749,"Early complete veriﬁcation methods relied on off-the-shelf SMT (Katz et al., 2017; Ehlers, 2017)
or MILP solvers (Dutta et al., 2018; Tjeng et al., 2019). However, these methods do not scale
beyond small networks. In order to overcome these limitations, Bunel et al. (2020) formulated a
BaB style framework for complete veriﬁcation and showed it contains many previous methods as
special cases. The basic idea is to recursively split the veriﬁcation problem into easier subproblems
on which cheap incomplete methods can show robustness. Since then, a range of partially (Xu
et al., 2021) or fully (Wang et al., 2021; Palma et al., 2021) GPU-based BaB frameworks have been
proposed. The most closely related, Palma et al. (2021), leverages the multi-neuron constraints from
Tjandraatmadja et al. (2020) but yields an optimization problem of different structure, as constraints
only ever include single output neurons."
RELATED WORK,0.6740088105726872,"Branching
Most ReLU branching methods proposed to date use the bound improvement of the
two child subproblems as the metric to decide which node to branch on next. Full strong branching
(Applegate et al., 1995) exhaustively evaluates this for all possible branching decisions. However,
this is intractable for all but the smallest networks. Lu & Kumar (2020) train a GNN to imitate
the behavior of full strong branching at a fraction of the cost, but transferability remains an open
question and collecting training data to imitate is costly. Bunel et al. (2020) proposed an efﬁciently
computable heuristic score, locally approximating the bound improvement of a branching decision
using the method of Wong & Kolter (2018). Henriksen & Lomuscio (2021) reﬁne this approach
by additionally approximating the indirect effect of the branching decision, however, this requires
using two different bounding procedures. De Palma et al. (2021) introduced ﬁltered-smart-branching
(FSB), using BaBSR to select branching candidates and then computing a more accurate heuristic
score only for the selected candidates. Instead of targeting the largest bound improvement, Kouvaros
& Lomuscio (2021) aim to minimize the number of unstable neurons by splitting the ReLU node
with the most other ReLU nodes depending on it."
CONCLUSION,0.6784140969162996,"6
CONCLUSION"
CONCLUSION,0.6828193832599119,"We propose the complete neural network veriﬁer MN-BAB. Building on the Branch-and-Bound
methodology, MN-BAB leverages tight multi-neuron constraints, a novel branching heuristic and an
efﬁcient dual solver, able to utilize massively parallel hardware accelerators, to enable the veriﬁca-
tion of particularly challenging networks. Our thorough empirical evaluation shows how MN-BAB
is particularly effective in verifying challenging networks with high natural accuracy and practical
relevance, reaching a new state-of-the-art in several settings."
CONCLUSION,0.6872246696035242,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.6916299559471366,"7
ETHICS STATEMENT"
ETHICS STATEMENT,0.6960352422907489,"Most machine learning based systems can be both employed with ethical as well as malicious pur-
poses. Methods such as ours that enable the certiﬁcation of robustness properties of neural networks
are a step towards more safe and trustworthy AI systems and can hence amplify any such usage.
Further, malicious actors might aim to convince regulators that the proposed approach is sufﬁcient
to show robustness to perturbations encountered during real world application, which could lead to
insufﬁcient regulation in safety critical domains."
REPRODUCIBILITY STATEMENT,0.7004405286343612,"8
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.7048458149779736,"We will make all code and trained networks required to reproduce our experiments available during
the review process as supplementary material and provide instructions on how to run them. Upon
publication, we will also release them publicly. We explain the basic experimental setup in Section 4
and provide more details in Section A. All datasets used in the experiments are publicly available.
Random seeds are ﬁxed where possible and provided in the supplementary material."
REPRODUCIBILITY STATEMENT,0.7092511013215859,Published as a conference paper at ICLR 2022
REFERENCES,0.7136563876651982,REFERENCES
REFERENCES,0.7180616740088106,"An automatic method of solving discrete programming problems. Econometrica, 28(3), 1960. ISSN
00129682, 14680262."
REFERENCES,0.7224669603524229,"David Applegate, Robert Bixby, Vašek Chvátal, and William Cook.
Finding cuts in the tsp (a
preliminary report). Technical report, Citeseer, 1995."
REFERENCES,0.7268722466960352,"Rudy Bunel, P Mudigonda, Ilker Turkaslan, P Torr, Jingyue Lu, and Pushmeet Kohli. Branch and
bound for piecewise linear neural network veriﬁcation. Journal of Machine Learning Research,
21(2020), 2020."
REFERENCES,0.7312775330396476,"Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certiﬁed adversarial robustness via random-
ized smoothing. In Proc. of ICML, volume 97, 2019."
REFERENCES,0.73568281938326,"Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Ue-
sato, Rudy Bunel, Shreya Shankar, Jacob Steinhardt, Ian J. Goodfellow, Percy Liang, and
Pushmeet Kohli. Enabling certiﬁcation of veriﬁcation-agnostic networks via memory-efﬁcient
semideﬁnite programming. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020."
REFERENCES,0.7400881057268722,"Alessandro De Palma, Rudy Bunel, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet Kohli,
Philip HS Torr, and M Pawan Kumar. Improved branch and bound for neural network veriﬁcation
via lagrangian decomposition. ArXiv preprint, abs/2104.06718, 2021."
REFERENCES,0.7444933920704846,"Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE
Signal Processing Magazine, 29(6), 2012."
REFERENCES,0.748898678414097,"Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. Output range analysis
for deep feedforward neural networks. In NASA Formal Methods Symposium. Springer, 2018."
REFERENCES,0.7533039647577092,"Ruediger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In Interna-
tional Symposium on Automated Technology for Veriﬁcation and Analysis. Springer, 2017."
REFERENCES,0.7577092511013216,"Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Mar-
tin Vechev. Ai2: Safety and robustness certiﬁcation of neural networks with abstract interpreta-
tion. In 2018 IEEE Symposium on Security and Privacy (SP). IEEE, 2018."
REFERENCES,0.762114537444934,"Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. ArXiv preprint, abs/1810.12715, 2018."
REFERENCES,0.7665198237885462,"Patrick Henriksen and Alessio Lomuscio. Deepsplit: An efﬁcient splitting method for neural net-
work veriﬁcation via indirect effect analysis. In Proceedings of the 30th International Joint Con-
ference on Artiﬁcial Intelligence (IJCAI21), To appear, 2021."
REFERENCES,0.7709251101321586,"Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An
efﬁcient smt solver for verifying deep neural networks. In International Conference on Computer
Aided Veriﬁcation. Springer, 2017."
REFERENCES,0.775330396475771,"Panagiotis Kouvaros and Alessio Lomuscio. Towards scalable complete veriﬁcation of relu neu-
ral networks via dependency-based branching. In Proceedings of the 30th International Joint
Conference on Artiﬁcial Intelligence (IJCAI21), To Appear, 2021."
REFERENCES,0.7797356828193832,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.7841409691629956,"Jingyue Lu and M. Pawan Kumar. Neural network branching for neural network veriﬁcation. In
Proc. of ICLR, 2020."
REFERENCES,0.788546255506608,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In Proc. of ICLR, 2018."
REFERENCES,0.7929515418502202,Published as a conference paper at ICLR 2022
REFERENCES,0.7973568281938326,"Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev.
Prima: General and precise neural network certiﬁcation via scalable convex hull approximations.
Proc. ACM Program. Lang., 6(POPL), jan 2022. doi: 10.1145/3498704. URL https://doi.
org/10.1145/3498704."
REFERENCES,0.801762114537445,"Alessandro De Palma, Harkirat S. Behl, Rudy R. Bunel, Philip H. S. Torr, and M. Pawan Kumar.
Scaling the convex barrier with active sets. In Proc. of ICLR, 2021."
REFERENCES,0.8061674008810573,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada, 2019."
REFERENCES,0.8105726872246696,"Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation
barrier to tight robustness veriﬁcation of neural networks. In Advances in Neural Information
Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019."
REFERENCES,0.8149779735682819,"Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin T. Vechev. Fast
and effective robustness certiﬁcation. In Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December
3-8, 2018, Montréal, Canada, 2018."
REFERENCES,0.8193832599118943,"Gagandeep Singh, Rupanshu Ganvir, Markus Püschel, and Martin T. Vechev. Beyond the single neu-
ron convex barrier for neural network certiﬁcation. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada, 2019a."
REFERENCES,0.8237885462555066,"Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. An abstract domain for certi-
fying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL), 2019b."
REFERENCES,0.8281938325991189,"Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev. Boosting robustness certi-
ﬁcation of neural networks. In Proc. of ICLR, 2019c."
REFERENCES,0.8325991189427313,"Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, and Venkatesh Babu R. Guided adversarial
attack for evaluating and enhancing adversarial defenses. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.8370044052863436,"Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, and Juan Pablo
Vielma. The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural
network veriﬁcation. In Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020."
REFERENCES,0.8414096916299559,"Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In Proc. of ICLR, 2019."
REFERENCES,0.8458149779735683,"Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.
Beta-crown: Efﬁcient bound propagation with per-neuron split constraints for neural network
robustness veriﬁcation. In ICML 2021 Workshop on Adversarial Machine Learning, 2021."
REFERENCES,0.8502202643171806,"Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In Proc. of ICML, volume 80, 2018."
REFERENCES,0.8546255506607929,"Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya
Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certiﬁed
robustness and beyond. In Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020."
REFERENCES,0.8590308370044053,Published as a conference paper at ICLR 2022
REFERENCES,0.8634361233480177,"Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast
and complete: Enabling complete neural network veriﬁcation with rapid and massively parallel
incomplete veriﬁers. In Proc. of ICLR, 2021."
REFERENCES,0.8678414096916299,"Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup
help with robustness and generalization? In Proc. of ICLR, 2021."
REFERENCES,0.8722466960352423,Published as a conference paper at ICLR 2022
REFERENCES,0.8766519823788547,Table 3: Overview of the experimental conﬁguration for every network.
REFERENCES,0.8810572687224669,"Dataset
Model
Training
Timeout
Batch sizes
#Activation
Layers
#Activation
Nodes"
REFERENCES,0.8854625550660793,"MNIST
ConvSmall
NOR
360
[100, 200, 400]
3
3 604
ConvBig
DiffAI
2000
[2, 2, 4, 8, 12, 20]
6
34 688
ConvSuper
DiffAI
360
[1, 2, 2, 3, 4, 8]
6
88 500"
REFERENCES,0.8898678414096917,"CIFAR10
ConvSmall
PGD
360
[100, 150, 250]
3
4 852
ConvBig
PGD
500
[3, 3, 6, 6, 8, 16]
6
62 464
ResNet6-A
PGD
600
[4, 4, 8, 12, 16, 100]
6
10 340
ResNet6-B
PGD
600
[4, 8, 32, 64, 128, 256]
6
10 340
ResNet8-A
PGD
600
[2, 2, 2, 4, 8, 16, 32, 64]
8
11 364"
REFERENCES,0.8942731277533039,"A
EXPERIMENT DETAILS"
REFERENCES,0.8986784140969163,"In Table 3 we show the per-sample timeout and the batch sizes that were used in the experiments.
The timeouts for the ﬁrst 4 networks were chosen to approximately match the average runtimes
reported by Wang et al. (2021), to facilitate comparability."
REFERENCES,0.9030837004405287,"Since we keep intermediate bounds of neurons before the split layer ﬁxed, as described in Sec-
tion 3.2, the memory requirements for splitting at different layers can vary. We exploit this fact and
choose batch sizes for our bounding procedure depending on the layer where the split occurred."
REFERENCES,0.9074889867841409,"In order to falsify properties more quickly, we run a strong adversarial attack with the following
parameters before attempting certiﬁcation: We apply two targeted versions (towards all classes) of
PGD (Madry et al., 2018) using margin loss (Gowal et al., 2018) and GAMA loss (Sriramanan et al.,
2020), both with 5 restarts, 50 steps, and 10 step output diversiﬁcation (?)."
REFERENCES,0.9118942731277533,"A.1
ARCHITECTURES"
REFERENCES,0.9162995594713657,"In this section, we provide an overview of all the architectures evaluated in Section 4. The ar-
chitectures of the convolutional networks for MNIST and CIFAR10 are detailed in Table 4. The
architectures of both ResNet6-A and ResNet6-B are given in Table 5"
REFERENCES,0.920704845814978,"Table 4: Network architectures of the convolutional networks for CIFAR10 and MNIST. All layers
listed below are followed by an activation layer. The output layer is omitted. ‘CONV c h×w/s/p’
corresponds to a 2D convolution with c output channels, an h×w kernel size, a stride of s in both
dimensions, and an all-around zero padding of p."
REFERENCES,0.9251101321585903,"ConvSmall
ConvBig
ConvSuper"
REFERENCES,0.9295154185022027,"CONV 16 4×4/2/0
CONV 32 3×3/1/1
CONV 32 3×3/1/0
CONV 32 4×4/2/0
CONV 32 4×4/2/1
CONV 32 4×4/1/0
FC 100
CONV 64 3×3/1/1
CONV 64 3×3/1/0
CONV 64 4×4/2/1
CONV 64 4×4/1/0
FC 512
FC 512
FC 512
FC 512"
REFERENCES,0.933920704845815,"B
SPLIT-COST COMPUTATION FOR CAB"
REFERENCES,0.9383259911894273,"Recall that for CAB, we normalize the branching score obtained with an arbitrary branching heuris-
tic with the (approximate) cost of the corresponding split. The true cost of a split consists of the
direct cost of the next branching step and the change of cumulative cost for all consecutive steps. As
a local approximation, we just consider the former component."
REFERENCES,0.9427312775330396,"We approximate this direct cost by the number of ﬂoating-point operations required to compute
the new bounds. This is computed as the sum of ﬂoating-point operations required for bounding
all intermediate bounds that are recomputed. Our bounding approach only enforces constraints"
REFERENCES,0.947136563876652,Published as a conference paper at ICLR 2022
REFERENCES,0.9515418502202643,"Table 5: Network architecture of the ResNet6 and ResNet8. All layers listed below are followed by a
ReLU activation layer, except if they are followed by a RESADD layer. The output layer is omitted.
‘CONV c h×w/s/p’ corresponds to a 2D convolution with c output channels, an h×w kernel size, a
stride of s in both dimensions, and an all-around zero padding of p."
REFERENCES,0.9559471365638766,"ResNet6
ResNet8"
REFERENCES,0.960352422907489,"CONV 16 3×3/1/1
CONV 16 3×3/2/1
CONV 32 1×1/2/0
CONV 32 3×3/2/1
CONV 32 1×1/2/0
CONV 32 3×3/2/1
CONV 32 3×3/1/1
CONV 32 3×3/1/1
RESADD
RESADD
CONV 64 1×1/2/0
CONV 64 3×3/2/1
CONV 64 1×1/2/0
CONV 64 3×3/2/1
CONV 64 3×3/1/1
CONV 64 3×3/1/1
RESADD
RESADD
FC 100
CONV 128 1×1/2/0
CONV 128 3×3/2/1
CONV 128 3×3/1/1
RESADD
FC 100"
REFERENCES,0.9647577092511013,"on neurons preceding the neurons included in the bounding objective. Hence, we only recompute
intermediate bounds for layers after the layer where the split occurs, as discussed in §3.2."
REFERENCES,0.9691629955947136,"To compute the cost of recomputing the lower (or upper) bound of one intermediate node, we add up
all ﬂoating point operations needed to perform the backsubstitution described in §3.1. As backsub-
stitution is just a series of matrix multiplications, the number of required ﬂoating point operations
can be deduced from the sizes of the multiplied matrices."
REFERENCES,0.973568281938326,"Thus if we split on layer k of an L layer network and the cost of backsubstituion from layer i is Ci
and the number of nodes is di, the ﬁnal cost of the split is: L
X"
REFERENCES,0.9779735682819384,"i=k+1
2diCi"
REFERENCES,0.9823788546255506,"Where the factor 2 comes from the fact that for intermediate bounds we need to recompute both
lower and upper bounds. The cost Ci of a full backsubstitution from layer i can be computed as the
sum over the cost of backsubstituting through all preceding layers Ci = Pi−1
j=0 cj, where the cost
for a single layer can be computed as follows:"
REFERENCES,0.986784140969163,"• ReLU layer: cj = dj + pj, where pj is the number of multi-neuron constraints."
REFERENCES,0.9911894273127754,"• Linear layer: cj = #Wj, where #Wj is the number of elements in the weight matrix."
REFERENCES,0.9955947136563876,"• Conv layer: cj = djk2
j, where kj is the kernel size."
