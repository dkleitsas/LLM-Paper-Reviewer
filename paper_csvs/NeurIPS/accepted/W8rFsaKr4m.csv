Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0021929824561403508,"The state space models, employing recursively propagated features, demonstrate
strong representation capabilities comparable to Transformer models and supe-
rior efficiency. However, constrained by the inherent geometric constraints of
sequences, it still falls short in modeling long-range dependencies. To address
this issue, we propose the MambaTree network, which first dynamically generates
a tree topology based on spatial relationships and input features. Then, feature
propagation is performed based on this graph, thereby breaking the original se-
quence constraints to achieve stronger representation capabilities. Additionally,
we introduce a linear complexity dynamic programming algorithm to enhance
long-range interactions without increasing computational cost. MambaTree is a
versatile multimodal framework that can be applied to both visual and textual tasks.
Extensive experiments demonstrate that our method significantly outperforms
existing structured state space models on image classification, object detection
and segmentation. Besides, by fine-tuning large language models, our approach
achieves consistent improvements in multiple textual tasks at minor training cost.
Code is available at https://github.com/EasonXiao-888/GrootVL."
INTRODUCTION,0.0043859649122807015,"1
Introduction"
INTRODUCTION,0.006578947368421052,"Mainstream fundamental models are primarily based on CNN [30, 62, 44, 32, 13] and Transformer
architectures [15, 43, 42, 59, 14], which dominate in visual and language tasks. However, the small
receptive field of CNNs and the high complexity of Transformers make it challenging to strike a good
balance between effectiveness and efficiency. The state space models (SSMs) [22, 24, 52] attempt
to disrupt this impasse, which model sequences in a recurrent form. Different from the previous
recurrent neural networks [31, 7], these approaches draw inspiration from control systems, leveraging
structural parameter initialization to attain stable optimization and superior computing performance.
Nevertheless, it remains susceptible to the intrinsic flaw shared by recurrent neural networks, i.e., a
deficiency in capturing long-range dependencies."
INTRODUCTION,0.008771929824561403,"Recently, an improved selection mechanism known as Mamba [19] is proposed to mitigate the
challenges of SSMs. This approach introduces weight modulation during the propagation process,
which substantially enlarges the effective receptive field and achieves impressive performance in NLP
tasks. Besides, numerous studies aim to extend Mamba into computer vision, by employing various
pre-defined strategies to map 2D image features into 1D sequences. ViM [77] and VMamba [41]
utilize a multi-directional raster-scanning strategy, while LocalMamba [34] further confines its"
INTRODUCTION,0.010964912280701754,∗Equal contribution. † Work done during an internship at Tencent. B Corresponding author.
INTRODUCTION,0.013157894736842105,"propagation range within a local window. They have successfully adapted Mamba to image inputs.
Nevertheless, as shown in Fig. 1(a), both raster-scanning and local-scanning strategies introduce
spatial discontinuities between adjacent pixels, and feature transformations in Mamba rely on the
feature relationships, thereby impeding the effective information flow in a sequence. Additionally,
PlainMamba [69] introduces a continuous scanning strategy, aiming to alleviate this issue by simply
adjusting the propagation direction at discontinuous positions. However, all these methods rely on
fixed propagation trajectories, which ignore the inherent spatial structure and cannot dynamically
adjust the topology based on input. Therefore, this paper endeavors to explore a new perspective:
introducing an input-aware topological network for feature propagation in state space models."
INTRODUCTION,0.015350877192982455,"To achieve it, we develop a tree state space model and propose a new framework, termed MambaTree,
which adaptively generates a tree topology based on the input feature and then performs feature
propagation on it. Specifically, two sub-networks, MambaTreeV and MambaTreeL, are designed for
visual and language tasks respectively, which are illustrated in Fig. 1(b) and Fig. 1(d). For visual
tasks, motivated by [71, 54], we first utilize the dissimilarity between adjacent features to construct
a minimum spanning tree on a four-connected planner graph. This process can adaptively encode
the spatial and semantic information into a tree graph [71, 54]. Then, we iteratively traverse each
pixel, considering it as the root vertex, and aggregate the features of other pixels using the state
transition function of Mamba. Intuitively, this operation requires two levels of traversal across the
entire pixel set, resulting in an unacceptable quadratic complexity relative to the number of pixels.
However, given that the tree graph is acyclic, we propose a dynamic programming algorithm to
achieve linear complexity propagation. With such an input-aware tree topology, our approach enables
more effective long-range interactions while maintaining consistent linear complexity with Mamba.
Furthermore, our method can also be applied to language tasks by constructing a tree topology based
on the dissimilarity between token features, which overcomes the geometrical constraints of the
text sequence. Using a similar aggregation process as MambaTreeV, MambaTreeL can significantly
enhance the language representation of a pre-trained Large Language Model [19]."
INTRODUCTION,0.017543859649122806,"We conduct extensive experiments to validate the effectiveness of MambaTreeV on multiple visual
benchmarks, i.e. image classification on ImageNet [12], object detection and instance segmentation
on MSCOCO [39] as well as semantic segmentation on ADE20K [75]. Results show that our method
notably outperforms existing SSM-based methods for all benchmarks and achieves competitive
performance with CNN and Transformer-based approaches. Moreover, with LoRA finetuning [33],
MambaTreeL demonstrates consistent improvements for a pre-trained large language model at minor
training cost."
RELATED WORK,0.019736842105263157,"2
Related Work"
CONVENTIONAL VISION FOUNDATION MODELS,0.021929824561403508,"2.1
Conventional Vision Foundation Models"
CONVENTIONAL VISION FOUNDATION MODELS,0.02412280701754386,"The evolution of deep neural networks has been a significant catalyst in machine vision perception.
CNN-based models [30, 51, 35, 25, 61, 72, 38, 55, 73] firstly emerge as pivotal landmarks, with
ResNet [30] notably standing out for its inventive residual connection module, garnering widespread
adoption across diverse domains of visual recognition. Furthermore, more efficient convolution
operations are formulated, such as depth-wise convolutions introduced by MobileNet [32], paving the
way for lightweight models. Additionally, deformable convolution [10] has been proposed to enhance
the receptive field. Subsequently, ViT [15] has significantly improved the vision recognition paradigm.
It reformulates the architecture design and training mechanism by combining transformer architecture
in natural language processing, aiming to improve computational efficiency and broaden the scope of
applications. After research discourse is centred on hierarchical ViTs [43, 42, 11, 63, 14, 56, 5] which
design networks by decreasing feature resolution across the backbone gradually. Furthermore, recent
research built on CNN serves to re-emphasize the capabilities of convolutional networks. For example,
InternImage [62] presents a large model based on deformable CNN, while UniRepLKNet [13] exhibits
significant performance through large kernel convolution."
EXPLORATIONS ABOUT STATE SPACE MODELS,0.02631578947368421,"2.2
Explorations about State Space Models"
EXPLORATIONS ABOUT STATE SPACE MODELS,0.02850877192982456,"State space models (SSMs) have emerged as a novel class of models within the deep learning
paradigm, showing significant potential for sequence transforming [23, 22, 52]. These methods have
attracted significant attention due to their linear scalability with sequence length. The early method,"
EXPLORATIONS ABOUT STATE SPACE MODELS,0.03070175438596491,"Continuous Scan
Local Scan
Raster Scan
Tree Topology Scan"
EXPLORATIONS ABOUT STATE SPACE MODELS,0.03289473684210526,shortcrust ……
EXPLORATIONS ABOUT STATE SPACE MODELS,0.03508771929824561,"ingredients
do
I
need
make"
EXPLORATIONS ABOUT STATE SPACE MODELS,0.03728070175438596,"a
shortcrust
for
……
mini"
EXPLORATIONS ABOUT STATE SPACE MODELS,0.039473684210526314,"to
What pies"
EXPLORATIONS ABOUT STATE SPACE MODELS,0.041666666666666664,"What 
ingredients do
I"
EXPLORATIONS ABOUT STATE SPACE MODELS,0.043859649122807015,"need
to
make a for"
EXPLORATIONS ABOUT STATE SPACE MODELS,0.046052631578947366,"mini
pies"
EXPLORATIONS ABOUT STATE SPACE MODELS,0.04824561403508772,"(a) Previous Visual SSMs
(b) MambaTreeV"
EXPLORATIONS ABOUT STATE SPACE MODELS,0.05043859649122807,"(c) Previous Textual SSMs
(d) MambaTreeL"
EXPLORATIONS ABOUT STATE SPACE MODELS,0.05263157894736842,"Figure 1: Comparison of different propagation strategies for multi-modal tasks. For visual tasks,
the previous strategies (a) are based on fixed patterns, while our method can adaptively generate the
propagation topology according to input features. For textual tasks, compared to previous methods
(c), our approach (d) can break the inherent constraints of text sequences, facilitating the effective
transmission of long-range information."
EXPLORATIONS ABOUT STATE SPACE MODELS,0.05482456140350877,"LSSL [23], draws inspiration from continuous state space models in control systems and attempts to
address the long-range dependency problem through a combination with HIPPO [20] initialization.
S4 [22] proposes to normalize the parameters into a diagonal matrix, prompting a subsequent series
of research on structured SSMs [24, 21, 26, 19]. Recently, the Selective State Space Model [19],
known as Mamba, strikes a balance between effectiveness and efficiency through the design of an
input-dependent parameter initialization strategy, which has emerged as a formidable competitor
to both transformer and CNN structures. In addition to showcasing superior outcomes in sequence
modeling, Mamba has been seamlessly incorporated into the visual domain [77, 41, 34, 69, 68].
These studies often rely on handcrafted fixed scanning mechanisms to mitigate the execution bias of
the selective state space model on 2D non-causal images. However, such simplistic approaches cannot
effectively capture spatial relationships in an input-dependent paradigm. To address this limitation,
we propose an effective framework MambaTree in this work to enhance long-range modeling for
both vision and language tasks by introducing an input-aware tree-based topological structure."
METHOD,0.05701754385964912,"3
Method"
METHOD,0.05921052631578947,"In this section, we first revisit the selective state space model [19] and then elaborate on our input-
aware topology scanning algorithm for state space modeling. With this superior algorithm, we develop
a tree SSM and propose a novel framework called MambaTree, which consists of two sub-networks:
MambaTreeV for visual tasks and MambaTreeL for fine-tuning a pre-trained language model [19]."
REVISITING SELECTIVE STATE SPACE MODEL,0.06140350877192982,"3.1
Revisiting Selective State Space Model"
REVISITING SELECTIVE STATE SPACE MODEL,0.06359649122807018,"State Space Models (SSMs) are commonly regarded as continuous linear time-invariant systems [64]
that map input stimulation x(t) ∈R1×D to output signal y(t) ∈R1×D through a state vector
h(t) ∈R1×N, where t, D and N indicate the time step, channel number of the signal and state size,
respectively. These models can be formulated as the following linear ordinary differential equations:"
REVISITING SELECTIVE STATE SPACE MODEL,0.06578947368421052,"h′(t) = Ah(t) + Bx(t),
y(t) = Ch(t) + Dx(t),
(1)"
REVISITING SELECTIVE STATE SPACE MODEL,0.06798245614035088,"where A ∈RN×N, B ∈RN×D, C ∈RN×D and feedthrough coefficient D ∈RD."
REVISITING SELECTIVE STATE SPACE MODEL,0.07017543859649122,"Discretization.
Although SSM serves as a powerful tool in systems and control engineering, its
time-continuous nature poses challenges for integration into deep learning architectures. To alleviate
this issue, most methods utilize the zero-order hold rule [19] to discretize the continuous system
described by Eq. (1) and convert continuous variables (A, B, C, D) into corresponding discrete TSA SiLU"
REVISITING SELECTIVE STATE SPACE MODEL,0.07236842105263158,Building a 4-connected Graph !
REVISITING SELECTIVE STATE SPACE MODEL,0.07456140350877193,"!"", !$, %, &
Minimum Spanning Tree"
REVISITING SELECTIVE STATE SPACE MODEL,0.07675438596491228,"Linear
Linear SiLU LN ×"
REVISITING SELECTIVE STATE SPACE MODEL,0.07894736842105263,Linear
REVISITING SELECTIVE STATE SPACE MODEL,0.08114035087719298,Pruning (
REVISITING SELECTIVE STATE SPACE MODEL,0.08333333333333333,Parameters Generator
REVISITING SELECTIVE STATE SPACE MODEL,0.08552631578947369,"!"", !$, %, &"
REVISITING SELECTIVE STATE SPACE MODEL,0.08771929824561403,"Linear
Embedding """
REVISITING SELECTIVE STATE SPACE MODEL,0.08991228070175439,SoftPlus
REVISITING SELECTIVE STATE SPACE MODEL,0.09210526315789473,"!"" = +', -""
!$ = -$ $ - %
&"
REVISITING SELECTIVE STATE SPACE MODEL,0.09429824561403509,Linear
REVISITING SELECTIVE STATE SPACE MODEL,0.09649122807017543,Linear
REVISITING SELECTIVE STATE SPACE MODEL,0.09868421052631579,"Tree Scanning Algorithm
Tree State Space Model"
REVISITING SELECTIVE STATE SPACE MODEL,0.10087719298245613,DWConv
REVISITING SELECTIVE STATE SPACE MODEL,0.10307017543859649,Parameters
REVISITING SELECTIVE STATE SPACE MODEL,0.10526315789473684,"Generator '
' ( '"
REVISITING SELECTIVE STATE SPACE MODEL,0.1074561403508772,"Figure 2: Illustration of Tree State Space Model. With an image feature map x, we perform
Tree Scanning Algorithm (TSA) to construct a 4-connected graph with edge weights measured by
dissimilarity between pixels. Then, we obtain an MST with vertices set Ωthrough a pruning algorithm
and perform the state transition for each vertex in this topology (detailed in Sec. 3.2). Red arrows
describe the propagation source of vertex i."
REVISITING SELECTIVE STATE SPACE MODEL,0.10964912280701754,"parameters ( ¯A, ¯B, ¯C, ¯D) over the specified sampling time-scale ∆∈RD:"
REVISITING SELECTIVE STATE SPACE MODEL,0.1118421052631579,"¯A = e∆A,
¯B =
 
e∆A −I

A−1B,
¯C = C,
¯D = D
(2)"
REVISITING SELECTIVE STATE SPACE MODEL,0.11403508771929824,"In addition, many improved methods [41, 19] use an approximation of ¯B based on the first-order
Taylor Series:
¯B =
 
e∆A −I

A−1B ≈(∆A)(∆A)−1∆B = ∆B
(3)"
REVISITING SELECTIVE STATE SPACE MODEL,0.1162280701754386,"Selective Mechanism .
Previous SSMs store information through finite states and inherent time-
invariance, which limits their effectiveness. Therefore, Mamba [19] introduces a dynamic mechanism
to selectively filter out input into a sequential state. Specifically, it utilizes Linear Projection to
calculate the parameters {Bi}L
i=1, {Ci}L
i=1 and {∆i}L
i=1 from the input sequence {xi}L
i=1 with
xi ∈R1×D directly to improve the context-aware ability. Then the output sequence {yi}L
i=1 can be
computed with those input-adaptive discretized parameters as follows:"
REVISITING SELECTIVE STATE SPACE MODEL,0.11842105263157894,"hi = ¯Aihi−1 + ¯Bixi,
yi = Cihi + Dxi
(4)"
TREE STATE SPACE MODEL,0.1206140350877193,"3.2
Tree State Space Model"
TREE STATE SPACE MODEL,0.12280701754385964,"Mamba [19] has showcased remarkable performance in modeling the dependencies of consecutive
words in a sequence. However, its applicability in long-context tasks, especially visual modeling,
still poses certain challenges. For visual tasks, many methods attempt to address this problem by
employing fixed scanning strategies, such as multi-directional raster scan [41, 77], local scan [34],
and continuous scan [69]. However, these handcrafted scanning methods fail to effectively preserve
the 2D structural information of images."
TREE STATE SPACE MODEL,0.125,"Following the design in Mamba [19], we construct a transform block as a tree state space model, which
is presented in Fig. 2. The only difference between our block and Mamba lies in the replacement of the
structured state space block with the proposed tree scanning algorithm. In the tree scanning algorithm,
we generate a tree topology and then propagate the state of each vertex along the topological path to
obtain strong feature representations. In addition, our algorithm can effectively enhance language
representations by incorporating such a tree topology during text processing, which overcomes
the geometrical constraints of text sequences. In the following, we elaborate on the proposed tree
scanning algorithm and its applications for multi-modal tasks."
TREE STATE SPACE MODEL,0.12719298245614036,"Tree Scanning Algorithm.
Given an input feature X = {xi}L
i=1 where L is the sequence length
(or the number of input pixels), we construct an undirected m-connected graph G = (V, E) for the Stem !×#×3"
TREE STATE SPACE MODEL,0.12938596491228072,"STAGE 2
Basic Block ×""%"
TREE STATE SPACE MODEL,0.13157894736842105,"STAGE 3
Basic Block ×""&"
TREE STATE SPACE MODEL,0.1337719298245614,"STAGE 4
Basic Block ×""'"
TREE STATE SPACE MODEL,0.13596491228070176,Down Sampling
TREE STATE SPACE MODEL,0.13815789473684212,Down Sampling
TREE STATE SPACE MODEL,0.14035087719298245,Down Sampling
TREE STATE SPACE MODEL,0.1425438596491228,"STAGE 1
Basic Block ×""( ! 4 × #"
TREE STATE SPACE MODEL,0.14473684210526316,"4 ×*
! 8 × #"
TREE STATE SPACE MODEL,0.14692982456140352,"8 ×2*
!
16 × #"
TREE STATE SPACE MODEL,0.14912280701754385,"16 ×4*
!
32 × #"
TREE STATE SPACE MODEL,0.1513157894736842,32 ×8* Head
TREE STATE SPACE MODEL,0.15350877192982457,"LN
Tree State
Space Model
FFN
LN
+
+ Stem"
TREE STATE SPACE MODEL,0.15570175438596492,"Conv: 3×3, S2, P1"
TREE STATE SPACE MODEL,0.15789473684210525,"LN + GELU
Conv: 3×3, S2, P1 LN"
TREE STATE SPACE MODEL,0.1600877192982456,Basic Block
TREE STATE SPACE MODEL,0.16228070175438597,Down Sampling
TREE STATE SPACE MODEL,0.16447368421052633,"Conv: 3×3, S2, P1 LN"
TREE STATE SPACE MODEL,0.16666666666666666,"Head
Cls. Det. Seg."
TREE STATE SPACE MODEL,0.16885964912280702,"Figure 3: Overview of MambaTreeV. LN means LayerNorm and FFN is a feed-forward network in
the basic block. S2 and P1 denote stride of 2 and padding size of 1 in convolution, respectively."
TREE STATE SPACE MODEL,0.17105263157894737,"feature. m is a hyper-parameter that indicates the number of adjacent tokens. Following [71, 54],
we set m = 4 for visual tasks, meaning each pixel is connected to its four neighboring pixels. For
language tasks, we set m = 3 by default, meaning each token is connected to the previous three
tokens. In addition, the vertices V represent the pixel (or token) embeddings, and the E indicates
the edges of the graph. The edge weight is calculated by the feature dissimilarity between adjacent
vertices. Besides, the metric of dissimilarity uses cosine distance by default, and the comparison with
other metrics refers to Table 5."
TREE STATE SPACE MODEL,0.17324561403508773,"We use the Contractive Boruvka algorithm [2] to prune the edges with significant dissimilarity, which
generates a minimum spanning tree (MST) GT whose sum of dissimilarity weights is minimum out of
all spanning trees. In the propagation process, we iteratively traverse each vertex, treating it as the root,
and aggregate the features of the remaining vertices. Intuitively, applying state propagation within
such a geometric configuration makes its preferential interactions among vertices with small spatial
and feature distances. Following the Mamba, we employ the data-dependent transition matrix for
state propagation. For a vertex k, we denote the transition matrix with its parent as ¯Ak. Furthermore,
following the Eq. (4), the state aggregation process for the i-th vertex can be formulated as:"
TREE STATE SPACE MODEL,0.17543859649122806,"hi =
X"
TREE STATE SPACE MODEL,0.17763157894736842,"∀j∈Ω
S(Eij) ¯Bjxj,
S(Eij) =
Y k∈Nij"
TREE STATE SPACE MODEL,0.17982456140350878,"¯Ak,
(5)"
TREE STATE SPACE MODEL,0.18201754385964913,"where Ωdenotes the index set of all vertices in the tree. S(Eij) represents the path weight of
hyperedge Eij traced from j-th vertex to i-th vertex in the tree GT , and Nij indicates the index set
of all vertices on this hyperedge. For visual tasks, we iterate over each vertex, treating it as the
root of the spanning tree GT , and aggregate the states from the other vertices, thereby obtaining
the transformed states {hi}L
i=1. For textual tasks, because of the causal prediction manner in large
language models, we only take the last token as root and aggregate from other tokens. To achieve
end-to-end training, we derive the derivative of the output hidden state hi to the input variables ¯Ak,
¯Bj and xj as follows:
∂hi
∂xj
= S (Eij) ¯Bj,
∂hi
∂¯Bj
= S (Eij) xj
(6)"
TREE STATE SPACE MODEL,0.18421052631578946,"∂hi
∂¯Ak
=
X"
TREE STATE SPACE MODEL,0.18640350877192982,"∀j∈Ci
k"
TREE STATE SPACE MODEL,0.18859649122807018,"¯BjxjS(Ekj)S(Ein),
(7)"
TREE STATE SPACE MODEL,0.19078947368421054,"where Ci
k indicates the children of vertex k in tree GT whose root is the vertex i, and n denotes the
parent of vertex k in Eq. (7). Finally, the output feature Y can be formulated as:"
TREE STATE SPACE MODEL,0.19298245614035087,"Y = C ⊙Norm(H) + D ⊙X,
(8)"
TREE STATE SPACE MODEL,0.19517543859649122,"where Y , H and X indicate the stack of {yi}L
i=1, {hi}L
i=1 and {x}L
i=1 respectively. ⊙denotes the
element-wise multiplication."
TREE STATE SPACE MODEL,0.19736842105263158,"Efficient Implementation for Multi-Modality.
For visual tasks, the tree scanning algorithm
requires two levels of traversal across the entire pixel set, resulting in an unacceptable quadratic
complexity relative to the number of pixels O(L2). To alleviate this issue, we utilize a dynamic"
TREE STATE SPACE MODEL,0.19956140350877194,Algorithm 1 Vision Tree Scanning
TREE STATE SPACE MODEL,0.20175438596491227,"Input: Input feature {xi}L
i=1; Input matrix { ¯Bi}L
i=1; State matrix { ¯Ai}L
i=1; Gradient of loss to
hidden states { ∂Loss"
TREE STATE SPACE MODEL,0.20394736842105263,"∂hi }L
i=1; Minimum Spanning Tree GT .
Traverse Path: Root, . . . , Leaf ←BFS(GT )
▷Breadth-first topological order of GT
Forward:"
TREE STATE SPACE MODEL,0.20614035087719298,"Initialization: {ξi}L
i=1 ←{xi}L
i=1
2: for i ←Leaf to Root do"
TREE STATE SPACE MODEL,0.20833333333333334,ξi = ¯Bixi + P
TREE STATE SPACE MODEL,0.21052631578947367,"∀j∈{t|Par(t)=i} ξj ¯Aj
4: end for"
TREE STATE SPACE MODEL,0.21271929824561403,"for i ←Root to Leaf do
6:
if i is Root then"
TREE STATE SPACE MODEL,0.2149122807017544,"hi = ξi
8:
else"
TREE STATE SPACE MODEL,0.21710526315789475,"hi = ¯Ai(hPar(i) −¯Aiξi) + ξi = (1 −¯A2
i )ξi + ¯AihPar(i)
10:
end if
end for
Backward:
12: Initialization: {ηi}L
i=1 ←{ ∂Loss"
TREE STATE SPACE MODEL,0.21929824561403508,"∂hi }L
i=1
for i ←Leaf to Root do
14:
ηi = ¯Bi ∂Loss"
TREE STATE SPACE MODEL,0.22149122807017543,"∂hi
+ P"
TREE STATE SPACE MODEL,0.2236842105263158,"∀j∈{t|Par(t)=i} ηj ¯Aj
end for
16: for i ←Root to Leaf do"
TREE STATE SPACE MODEL,0.22587719298245615,"if i is Root then
18:
∂Loss"
TREE STATE SPACE MODEL,0.22807017543859648,"∂xi
= ηi ¯Bi ,
∂Loss"
TREE STATE SPACE MODEL,0.23026315789473684,"∂¯Bi
= ηixi ,
∂Loss"
TREE STATE SPACE MODEL,0.2324561403508772,"∂¯Ai = 0
else
20:
∂Loss"
TREE STATE SPACE MODEL,0.23464912280701755,"∂xi
= (1 −¯A2
i )ηi ¯Bi + ¯Ai ∂Loss"
TREE STATE SPACE MODEL,0.23684210526315788,"∂xPar(i) ¯Bi ,
∂Loss"
TREE STATE SPACE MODEL,0.23903508771929824,"∂¯Bi
= (1 −¯A2
i )ηixi + ¯Ai ∂Loss"
TREE STATE SPACE MODEL,0.2412280701754386,∂¯BPar(i) xi ∂Loss
TREE STATE SPACE MODEL,0.24342105263157895,∂¯Ai = ηi ∗(hi −¯Aiξi) + ξi ∗( ∂Loss
TREE STATE SPACE MODEL,0.24561403508771928,"∂xi
−¯Aiηi) = ηihi + ξi ∂Loss"
TREE STATE SPACE MODEL,0.24780701754385964,"∂xi
−2ηiξi ¯Ai
22:
end if
end for
Output: Hidden states {hi}L
i=1; Grad. of loss to input feature { ∂Loss"
TREE STATE SPACE MODEL,0.25,"∂xi }L
i=1; Grad. of loss to input
matrix { ∂Loss"
TREE STATE SPACE MODEL,0.25219298245614036,"∂¯Bi }L
i=1; Grad. of loss to state matrix { ∂Loss"
TREE STATE SPACE MODEL,0.2543859649122807,"∂¯Ai }L
i=1."
TREE STATE SPACE MODEL,0.2565789473684211,"programming procedure to accelerate the inference and training processes as elaborated in Algo-
rithm 1, which results in linear complexity O(L). For textual tasks, we perform a unidirectional
aggregation approach (shown in Algorithm 2 of Appendix B) in adherence to the causal nature of
language. Moreover, we provide the back-propagation process for both Vision Tree Scanning and
Language Tree Scanning processes, whose detailed proofs refer to Appendix C."
APPLICATION FOR VISION AND LANGUAGE,0.25877192982456143,"3.3
Application for Vision and Language"
APPLICATION FOR VISION AND LANGUAGE,0.26096491228070173,"MambaTreeV
Given an image with a shape of H × W × 3, our goal is to obtain high-quality
visual features for downstream tasks. To this end, we propose an effective vision architecture
MambaTreeV which consists of a stem module, several basic blocks and downsampling layers to
generate hierarchical representations illustrated in Fig. 3. Overall, our MambaTreeV comprises four
stages similar to previous general vision backbones [44, 43, 62, 41]. We integrate the stem module
before the first stage to decrease the resolution of the input image signal by a factor of 4, resulting in a
feature map with a shape of H 4 × W"
APPLICATION FOR VISION AND LANGUAGE,0.2631578947368421,"4 ×C. It includes two convolutions, two Layer Normalization (LN)
layers and one GELU activation function. The kernel size for both convolutions is 3 with a stride of 2
and padding of 1. Similarly, a downsampling layer consists of a 3 × 3 convolution with a stride of 2
and padding of 1 and an LN layer. Positioned between two stages, it serves to downsample the input
feature map by a factor of 2. Motivated by [62, 41], we devise a residual block with skip connections
to integrate our fundamental Tree State Space Model in Sec. 3.2. In detail, we first normalize the input
features with LN layer. Spatial priors and long-range dependencies are then obtained through our
tree scanning algorithm with residual connections established alongside the input features. Finally, a
feedforward neural network is utilized to project the normalized features to output signals as shown"
APPLICATION FOR VISION AND LANGUAGE,0.26535087719298245,"Method
Type #Param. #FLOPs Top-1 Acc."
APPLICATION FOR VISION AND LANGUAGE,0.2675438596491228,"Deit-S [59]
T
22M
4.6G
79.9
Swin-T [43]
T
28M
4.6G
81.3
CoAtNet-0 [11]
T
25M
4.0G
81.6
SG-Former-S [50]
T
23M
4.8G
83.2
ConvNeXt-T [44]
C
29M
4.5G
82.1
SLaK-T [40]
C
30M
5.0G
82.5
UniRepLKNet-T [13]
C
31M
4.9G
83.2
InternImage-T [62]
C
30M
5.0G
83.5
ViM-S [77]
S
26M
5.1G
80.5
LocalViM-S [34]
S
28M
4.8G
81.2
PlainMamba-L2 [69]
S
25M
8.1G
81.6
Mamba-2D-S [37]
S
24M
-
81.7
S4ND-ConvNeXt-T [48]
S
30M
-
82.2
VMamba-T [41]
S
31M
4.9G
82.5
LocalVMamba-T [34]
S
26M
5.7G
82.7
MambaTreeV-T (Ours)
S
30M
4.8G
83.4"
APPLICATION FOR VISION AND LANGUAGE,0.26973684210526316,"Swin-S [43]
T
50M
8.7G
83.0
CoAtNet-1 [11]
T
42M
8.0G
83.3"
APPLICATION FOR VISION AND LANGUAGE,0.2719298245614035,"Method
Type #Param. #FLOPs Top-1 Acc."
APPLICATION FOR VISION AND LANGUAGE,0.2741228070175439,"ConvNeXt-S [44]
C
50M
8.7G
83.1
SLaK-S [40]
C
55M
9.8G
83.8
UniRepLKNet-S [13]
C
56M
9.1G
83.9
InternImage-S [62]
C
50M
8.0G
84.2
HyenaViT-B [17]
S
88M
-
78.5
S4ND-ViT-B [48]
S
89M
-
80.4
PlainMamba-L3 [69]
S
50M
14.4G
82.3
VMamba-S [41]
S
50M
8.7G
83.6
LocalVMamba-S [34]
S
50M
11.4G
83.7
MambaTreeV-S (Ours)
S
51M
8.5G
84.2"
APPLICATION FOR VISION AND LANGUAGE,0.27631578947368424,"Deit-B [59]
T
86M
55.4G
83.1
Swin-B [43]
T
88M
15.4G
83.5
CoAtNet-2 [11]
T
75M
16.0G
84.1
ConvNeXt-B [44]
C
89M
15.4G
83.8
SLaK-B [40]
C
95M
17.0G
84.0
Mamba-2D-B [37]
S
92M
-
83.0
VMamba-B [41]
S
89M
15.4G
83.9
MambaTreeV-B (Ours)
S
91M
15.1G
84.8"
APPLICATION FOR VISION AND LANGUAGE,0.27850877192982454,"Table 1: Image classification performance on the ImageNet-1K validation set. T, C and S indicate
the model type of Transformer, CNN and SSM, respectively. All models take a scale of 2242 as input."
APPLICATION FOR VISION AND LANGUAGE,0.2807017543859649,"in Fig. 3. Based on the above origin components, we develop our MambaTreeV in three scales, i.e.,
MambaTreeV-Tiny, MambaTreeV-Small and MambaTreeV-Base."
APPLICATION FOR VISION AND LANGUAGE,0.28289473684210525,"MambaTreeL
Recurrent neural networks rely on fixed memory to preserve past information, which
poses limitations when handling long contexts where relevant words are distant from the current
moment. While Mamba [19] employs a selection mechanism to enhance context awareness, its fixed
memory size cannot expand over time, resulting in restricted state space. Therefore, the ability to
extrapolate decreases during scrolling as the prompt extends. To mitigate this issue, we propose an
effective fine-tuning paradigm. Specifically, the tree-based topology branch is built upon one-way
scrolling with a scaling factor, enabling state transitions within such a structure. This arrangement
facilitates the preferential interaction of semantically related tokens. It is noteworthy that this
paradigm does not introduce any additional training parameters. Instead, it utilizes pretrained state
transformation parameters to conduct semantic aggregation by incorporating topological structures.
Experimental results demonstrate the effectiveness of our approach."
EXPERIMENTS,0.2850877192982456,"4
Experiments"
EXPERIMENTS,0.28728070175438597,"We conduct extensive experiments to evaluate the effectiveness of MambaTreeV and compare it with
advanced CNN-based, Transformer-based, and SSM-based models covering various downstream
tasks, including image classification, object detection and semantic segmentation. Furthermore, we
validate the capability of MambaTreeL in the field of natural language understanding."
IMAGE CLASSIFICATION,0.2894736842105263,"4.1
Image Classification"
IMAGE CLASSIFICATION,0.2916666666666667,"Settings.
We assess the classification performance of MambaTreeV on the ImageNet-1k dataset [12].
Following previous practices [43, 44, 62, 41], all MambaTreeV models are trained for 300 epochs
from scratch using AdamW optimizer with a warm-up strategy of 20 epochs. During training, we
utilize a Cosine Scheduler with an initial learning rate of 1 × 10−3 and weight decay of 0.05. In
addition, the exponential moving average (EMA) is also applied."
IMAGE CLASSIFICATION,0.29385964912280704,"Results.
The comparison results summarized in Table 1 show MambaTreeV leading all SSM-based
models and competitive with advanced CNNs and Transformers across tiny, small, and base scales.
Specifically, MambaTreeV-T achieves 83.4% Top-1 Acc. boosting ViM-S by 2.9%, LocalVim-S
by 2.2%, PlainMamba-L2 by 1.8% and VMamba-T by 0.9% with similar FLOPs. Additionally, it
surpasses ConvNeXt-T by 1.3% and Swin-T by 2.2%, demonstrating the effectiveness of our method."
IMAGE CLASSIFICATION,0.29605263157894735,"(a) Input Signal
(b) TP Scan
(c) Raster Scan"
IMAGE CLASSIFICATION,0.2982456140350877,"Figure 4: Visualization of affinity maps in the
specific position. The Location is marked by the
red cross in each input (a). TP is our tree topol-
ogy scanning algorithm (b), which captures more
detailed structural information and has a larger re-
ceptive field compared to raster scanning (c)."
IMAGE CLASSIFICATION,0.30043859649122806,"Method
Type #FLOPs mIoU mIoU SS
MS"
IMAGE CLASSIFICATION,0.3026315789473684,"Swin-T [43]
T
945G
44.5
45.8
ConvNeXt-T [44]
C
939G
46.0
46.7
SLaK-T [40]
C
936G
47.6
-
InternImage-T [62]
C
944G
47.9
48.1
UniRepLKNet-T [13]
C
946G
48.6
49.1
ViM-S [77]
S
-
44.9
-
LocalViM-S [34]
S
297G
46.4
47.5
PlainMamba-L2 [69]
S
285G
46.8
-
VMamba-T [41]
S
964G
47.3
48.3
LocalVMamba-T [41]
S
970G
47.9
49.1
MambaTreeV-T (Ours)
S
941G
48.5
49.4"
IMAGE CLASSIFICATION,0.3048245614035088,"Swin-S [43]
T
1038G
47.6
49.5
ConvNeXt-S [44]
C
1027G
48.7
49.6
SLaK-S [40]
C
1028G
49.4
-
InternImage-S [62]
C
1017G
50.1
50.9
UniRepLKNet-S [13]
C
1036G
50.5
51.0
PlainMamba-L3 [69]
S
419G
49.1
-
VMamba-S [41]
S
1081G
49.5
50.5
LocalVMamba-S [34]
S
1095G
50.0
51.0
MambaTreeV-S (Ours)
S
1019G
50.7
51.7"
IMAGE CLASSIFICATION,0.30701754385964913,"Table 2:
Semantic segmentation perfor-
mance on ADE20K val set. The crop size
is all set to 5122. SS and MS denote single-
scale and multi-scale testing, respectively."
OBJECT DETECTION,0.3092105263157895,"4.2
Object Detection"
OBJECT DETECTION,0.31140350877192985,"Settings.
We verify the detection performance of MambaTreeV on the MSCOCO 2017 dataset [39]
with MMDetection library [3]. We follow previous works [41, 62, 43, 34, 53, 55, 74, 70, 6] to validate
object detection and instance segmentation tasks with Mask-RCNN [29]. Specifically, We adopt the
AdamW optimizer with a learning rate of 1 × 10−4 and batch size of 16 to optimize the model built
upon our pre-trained classification backbones on ImageNet-1K. The training schedules include 1×
(12 epochs) and 3× (36 epochs) with multi-scale data augmentation."
OBJECT DETECTION,0.31359649122807015,"Results.
As depicted in Table 8 (in Appendix A.), our method outperforms existing methods on
most evaluation metrics, especially for instance segmentation. Under 1× schedule, MambaTreeV-T
achieves 47.0 in box mAP (APb), which is 1.1 points higher than ViM-S and 0.5 points higher
than VMamba-T. It is worth noting that MambaTreeV-T outperforms ViM-S by 1.7 points with 1×
schedule and LocalVMamba-T by 0.4 points with 3× schedule in mask mAP (APm). Moreover,
the best APb 50.1 and APm 44.6 are obtained by MambaTreeV-S in 3× schedule with multi-scale
training."
SEMANTIC SEGMENTATION,0.3157894736842105,"4.3
Semantic Segmentation"
SEMANTIC SEGMENTATION,0.31798245614035087,"Settings.
To evaluate the semantic segmentation performance of our MambaTreeV series, we train
our models with UperNet [65] initialized by pre-trained classification weights on ADE20K[75] for
160k iterations, following common practices without additional augmentations for fair comparison."
SEMANTIC SEGMENTATION,0.3201754385964912,"Results.
Our method performs exceptionally well on segmentation tasks shown in Table 2.
MambaTreeV-T yields a clear improvement of +3.6 in single-scale mIoU compared to ViM-S
and +1.9 in multi-scale mIoU compared to LocalViM-S. Furthermore, MambaTreeV-S boosts
InternImage-S by 0.6 and 0.8 in single-scale and multi-scale respectively. We consider the preserva-
tion of intricate structural details through tree topology scanning to be particularly advantageous for
segmentation tasks that require pixel-level perception."
SEMANTIC SEGMENTATION,0.3223684210526316,"Method
PIQA ↑
Arc-E ↑
SST ↑
WG ↑
L-ppl ↓
Race ↑
BQA ↑
Average↑
Acc."
SEMANTIC SEGMENTATION,0.32456140350877194,"Mamba [19]
64.5
48.0
65.6
51.8
16.1
27.4
16.8
45.7
+ LoRA [33]
64.7
48.3
65.1
52.2
17.7
28.6
17.8
46.1
+ MambaTreeL (Ours)
65.0
49.8
69.5
51.1
15.9
28.9
19.2
47.2"
SEMANTIC SEGMENTATION,0.3267543859649123,"Table 3: Evaluation on language model benchmarks. Arc-E, WG, L-ppl and BQA indicate Arc-
easy [8], WinoGrande, LAMBADA [49] and Openbookqa [47] benchmark, respectively."
SEMANTIC SEGMENTATION,0.32894736842105265,"Scanning Strategy
Acc"
SEMANTIC SEGMENTATION,0.33114035087719296,"Raster Scan
82.6
Cross Scan
83.1
Tree Topology Scan
83.4"
SEMANTIC SEGMENTATION,0.3333333333333333,"Table 4: Effectiveness of our
algorithm."
SEMANTIC SEGMENTATION,0.3355263157894737,"Distance Metric
Acc."
SEMANTIC SEGMENTATION,0.33771929824561403,"Manhattan
82.9
Euclidean
83.2
Cosine
83.4"
SEMANTIC SEGMENTATION,0.3399122807017544,"Table 5: Impact of different
distance Metrics."
SEMANTIC SEGMENTATION,0.34210526315789475,"Root Setting
Acc."
SEMANTIC SEGMENTATION,0.3442982456140351,"First vertex
82.9
Last vertex
83.0
All vertices
83.4"
SEMANTIC SEGMENTATION,0.34649122807017546,"Table
6:
Superiority
of
traversing all vertices."
LANGUAGE UNDERSTANDING,0.34868421052631576,"4.4
Language Understanding"
LANGUAGE UNDERSTANDING,0.3508771929824561,"We regard Mamba [19] with 130M parameters as the base model. To verify the effectiveness of our
MambaTreeL in nature language understanding, we first fine-tune pre-trained Mamba via LoRA [33]
and MambaTreeL under the same setting with the Alpaca data [58], which contains 52000 instruction
tuning data for supervised fine-tuning. Then we utilize popular language benchmarks provided in the
open-sourced lm-evaluation-harness project [18] for evaluation, including PIQA [1], AI2-ARC [8],
SST [60], WinoGrande, LAMBADA [49], Race [36] and Openbookqa [47]. The results in Table 3
demonstrate that our MambaTreeL provides a benefit of +1.1% in average Acc. compared to LoRA.
Since the short prompt length of WinoGrande dataset, the performance degrades with a marginal gap."
ABLATION STUDY & QUALITATIVE RESULTS,0.3530701754385965,"4.5
Ablation Study & Qualitative Results"
ABLATION STUDY & QUALITATIVE RESULTS,0.35526315789473684,"In this section, we conduct analysis experiments on ImageNet-1K dataset and present some visual
results to illustrate the effectiveness of our algorithm."
ABLATION STUDY & QUALITATIVE RESULTS,0.3574561403508772,"Scanning Strategy.
We conduct a head-to-head comparison of different scanning strategies, as
shown in Table 4. The tree topology scanning outperforms previous strategies by 0.8% and 0.3%,
highlighting the superiority of our algorithm in vision recognition."
ABLATION STUDY & QUALITATIVE RESULTS,0.35964912280701755,"Distance Metric.
Before generating a minimum spanning tree from a connected graph, it is
important to measure the edge weights between vertices. Therefore, we validate several distance
metrics as illustrated in Table 5. The results indicate that Cosine distance most effectively represents
the relationship between vertices, performing 0.5% better than Manhattan and 0.2% better than
Euclidean."
ABLATION STUDY & QUALITATIVE RESULTS,0.3618421052631579,"Root Setting.
We traverse all vertices, treating each as a root, and perform state transitions along
the topological path from the other vertices toward the root. This traversal ensures that each vertex
captures long-range dependencies. To verify the effectiveness of this operation, we consider only
the first and last vertices as the root in Table 6. The results show reductions of 0.5% and 0.4%,
respectively."
ABLATION STUDY & QUALITATIVE RESULTS,0.36403508771929827,"Inference speed comparison.
As shown in Table 7, we report the inference throughputs of our
method on an Nvidia V100 GPU. MambaTreeV-T∗refers to each stage sharing the same tree topology
structure, which enhances efficiency without compromising accuracy. To achieve better practical
inference speed, we also introduce a cuda implementation optimized for GPUs. Compared with other
counterparts, our approach exhibits superior effectiveness and faster inference speed."
ABLATION STUDY & QUALITATIVE RESULTS,0.36622807017543857,"Qualitative Results.
To better illustrate the superiority of our scanning strategy, we visualize the
affinity maps of different positions marked by the red cross in each input image. For example, we"
ABLATION STUDY & QUALITATIVE RESULTS,0.3684210526315789,"Method
Throughput
GPU Memory
FLOPs
#Params.
Acc. (Top1.)
(224x224)
(img/s)"
ABLATION STUDY & QUALITATIVE RESULTS,0.3706140350877193,"PlainMamba-L2 [69]
363
4204M
8.1G
25M
81.6
VMamba-T [41]
374
8646M
4.9G
31M
82.5
LocalVMamba-T [34]
311
11298M
5.7G
26M
82.7
MambaTreeV-T(one root)
283
6012M
4.8G
30M
83.0
MambaTreeV-T
281
6471M
4.8G
30M
83.4
MambaTreeV-T∗
392
4800M
4.8G
30M
83.4"
ABLATION STUDY & QUALITATIVE RESULTS,0.37280701754385964,Table 7: Runtime comparison on an Nvidia V100 GPU during inference.
ABLATION STUDY & QUALITATIVE RESULTS,0.375,"set the anchor point in the upper left corner of the sky as shown in the second row of in Fig. 4(a).
Our method can easily identify white houses, flagpoles, and the sky, which raster scanning fails to
achieve. This demonstrates the capability of our algorithm to preserve detailed structural information.
More comparisons can be seen in Fig. 6 (in Appendix D.)"
CONCLUSION & LIMITATIONS,0.37719298245614036,"5
Conclusion & Limitations"
CONCLUSION & LIMITATIONS,0.3793859649122807,"In this paper, we propose a tree state space model to perform feature propagation on an input-aware
topology. Besides, we introduce a linear complexity dynamic programming algorithm to enhance
long-range interactions without increasing computational cost. With the proposed techniques, we
establish the general multi-modal networks to break the original sequence constraints and achieve
stronger representation capabilities. Extensive experiments demonstrate the effectiveness of our
method in both visual and language tasks. The limitation of our method is that the tree structure is
not a common paradigm, and it needs to be specifically optimized according to the hardware device."
CONCLUSION & LIMITATIONS,0.3815789473684211,Acknowledgments and Disclosure of Funding
CONCLUSION & LIMITATIONS,0.38377192982456143,This work was supported by the STI 2030-Major Projects under Grant 2021ZD0201404.
REFERENCES,0.38596491228070173,References
REFERENCES,0.3881578947368421,"[1] Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al.: Piqa: Reasoning about physical commonsense in
natural language. In: AAAI. pp. 7432–7439 (2020) 9"
REFERENCES,0.39035087719298245,"[2] Bor˚uvka, O.: O jistém problému minimálním (1926) 5"
REFERENCES,0.3925438596491228,"[3] Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., et al.:
Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155
(2019) 8"
REFERENCES,0.39473684210526316,"[4] Chen, Z., Duan, Y., Wang, W., He, J., Lu, T., Dai, J., Qiao, Y.: Vision transformer adapter for
dense predictions. arXiv preprint arXiv:2205.08534 (2022) 16"
REFERENCES,0.3969298245614035,"[5] Cheng, C., Song, L., Xue, R., Wang, H., Sun, H., Ge, Y., Shan, Y.: Meta-adapter: An online
few-shot learner for vision-language model. arXiv preprint arXiv:2311.03774 (2023) 2"
REFERENCES,0.3991228070175439,"[6] Cheng, T., Song, L., Ge, Y., Liu, W., Wang, X., Shan, Y.: Yolo-world: Real-time open-
vocabulary object detection. arXiv preprint arXiv:2401.17270 (2024) 8"
REFERENCES,0.40131578947368424,"[7] Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recurrent neural
networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014) 1"
REFERENCES,0.40350877192982454,"[8] Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord, O.: Think
you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint
arXiv:1803.05457 (2018) 9"
REFERENCES,0.4057017543859649,"[9] Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmenta-
tion strategies from data. In: CVPR. pp. 113–123 (2019) 15"
REFERENCES,0.40789473684210525,"[10] Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convolutional networks.
In: ICCV. pp. 764–773 (2017) 2"
REFERENCES,0.4100877192982456,"[11] Dai, Z., Liu, H., Le, Q.V., Tan, M.: Coatnet: Marrying convolution and attention for all data
sizes. NeurIPS 34, 3965–3977 (2021) 2, 7"
REFERENCES,0.41228070175438597,"[12] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical
image database. In: CVPR. pp. 248–255. Ieee (2009) 2, 7"
REFERENCES,0.4144736842105263,"[13] Ding, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal
perception large-kernel convnet for audio, video, point cloud, time-series and image recognition.
CVPR (2023) 1, 2, 7, 8"
REFERENCES,0.4166666666666667,"[14] Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer:
A general vision transformer backbone with cross-shaped windows. In: CVPR. pp. 12124–12134
(2022) 1, 2, 16"
REFERENCES,0.41885964912280704,"[15] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,
M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16
words: Transformers for image recognition at scale. In: ICLR (2021) 1, 2"
REFERENCES,0.42105263157894735,"[16] Fang, C., He, C., Xiao, F., Zhang, Y., Tang, L., Zhang, Y., Li, K., Li, X.: Real-world image
dehazing with coherence-based label generator and cooperative unfolding network. arXiv
preprint arXiv:2406.07966 (2024) 15"
REFERENCES,0.4232456140350877,"[17] Fu, D., Arora, S., Grogan, J., Johnson, I., Eyuboglu, E.S., Thomas, A., Spector, B., Poli, M.,
Rudra, A., Ré, C.: Monarch mixer: A simple sub-quadratic gemm-based architecture. NeurIPS
36 (2023) 7"
REFERENCES,0.42543859649122806,"[18] Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu,
J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L.,
Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., Zou, A.:
A framework for few-shot language model evaluation (12 2023) 9"
REFERENCES,0.4276315789473684,"[19] Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv
preprint arXiv:2312.00752 (2023) 1, 2, 3, 4, 7, 9"
REFERENCES,0.4298245614035088,"[20] Gu, A., Dao, T., Ermon, S., Rudra, A., Ré, C.: Hippo: Recurrent memory with optimal
polynomial projections. NeurIPS 33, 1474–1487 (2020) 3"
REFERENCES,0.43201754385964913,"[21] Gu, A., Goel, K., Gupta, A., Ré, C.: On the parameterization and initialization of diagonal state
space models. NeurIPS 35, 35971–35983 (2022) 3"
REFERENCES,0.4342105263157895,"[22] Gu, A., Goel, K., Ré, C.: Efficiently modeling long sequences with structured state spaces. In:
ICLR (2022) 1, 2, 3"
REFERENCES,0.43640350877192985,"[23] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., Ré, C.: Combining recurrent,
convolutional, and continuous-time models with linear state space layers. NeurIPS 34, 572–585
(2021) 2, 3"
REFERENCES,0.43859649122807015,"[24] Gupta, A., Gu, A., Berant, J.: Diagonal state spaces are as effective as structured state spaces.
NeurIPS 35, 22982–22994 (2022) 1, 3"
REFERENCES,0.4407894736842105,"[25] Han, K., Wang, Y., Xu, C., Guo, J., Xu, C., Wu, E., Tian, Q.: Ghostnets on heterogeneous
devices via cheap operations. IJCV 130(4), 1050–1069 (2022) 2"
REFERENCES,0.44298245614035087,"[26] Hasani, R., Lechner, M., Wang, T.H., Chahine, M., Amini, A., Rus, D.: Liquid structural
state-space models. arXiv preprint arXiv:2209.12951 (2022) 3"
REFERENCES,0.4451754385964912,"[27] He, C., Li, K., Zhang, Y., Xu, G., Tang, L., Zhang, Y., Guo, Z., Li, X.: Weakly-supervised
concealed object segmentation with sam-based pseudo labeling and multi-scale feature grouping.
Advances in Neural Information Processing Systems 36 (2024) 15"
REFERENCES,0.4473684210526316,"[28] He, C., Shen, Y., Fang, C., Xiao, F., Tang, L., Zhang, Y., Zuo, W., Guo, Z., Li, X.: Diffusion
models in low-level vision: A survey. arXiv preprint arXiv:2406.11138 (2024) 15"
REFERENCES,0.44956140350877194,"[29] He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask r-cnn. In: ICCV. pp. 2961–2969 (2017) 8"
REFERENCES,0.4517543859649123,"[30] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR.
pp. 770–778 (2016) 1, 2"
REFERENCES,0.45394736842105265,"[31] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9(8), 1735–1780
(1997) 1"
REFERENCES,0.45614035087719296,"[32] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M.,
Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications.
arXiv preprint arXiv:1704.04861 (2017) 1, 2"
REFERENCES,0.4583333333333333,"[33] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora:
Low-rank adaptation of large language models. In: ICLR (2022) 2, 9"
REFERENCES,0.4605263157894737,"[34] Huang, T., Pei, X., You, S., Wang, F., Qian, C., Xu, C.: Localmamba: Visual state space model
with windowed selective scan. arXiv preprint arXiv:2403.09338 (2024) 1, 3, 4, 7, 8, 10, 15, 16"
REFERENCES,0.46271929824561403,"[35] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional
neural networks. NeurIPS 25 (2012) 2"
REFERENCES,0.4649122807017544,"[36] Lai, G., Xie, Q., Liu, H., Yang, Y., Hovy, E.H.: RACE: large-scale reading comprehension
dataset from examinations. In: EMNLP. pp. 785–794. Association for Computational Linguistics
(2017) 9"
REFERENCES,0.46710526315789475,"[37] Li, S., Singh, H., Grover, A.: Mamba-nd: Selective state space modeling for multi-dimensional
data. arXiv preprint arXiv:2402.05892 (2024) 7"
REFERENCES,0.4692982456140351,"[38] Li, Y., Song, L., Chen, Y., Li, Z., Zhang, X., Wang, X., Sun, J.: Learning dynamic routing for
semantic segmentation. In: CVPR (2020) 2"
REFERENCES,0.47149122807017546,"[39] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.:
Microsoft coco: Common objects in context. In: ECCV. pp. 740–755. Springer (2014) 2, 8"
REFERENCES,0.47368421052631576,"[40] Liu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., Kärkkäinen, T., Pechenizkiy, M.,
Mocanu, D., Wang, Z.: More convnets in the 2020s: Scaling up kernels beyond 51x51 using
sparsity. arXiv preprint arXiv:2207.03620 (2022) 7, 8"
REFERENCES,0.4758771929824561,"[41] Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state
space model. arXiv preprint arXiv:2401.10166 (2024) 1, 3, 4, 6, 7, 8, 10, 15, 16"
REFERENCES,0.4780701754385965,"[42] Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L., et al.:
Swin transformer v2: Scaling up capacity and resolution. In: CVPR. pp. 12009–12019 (2022)
1, 2"
REFERENCES,0.48026315789473684,"[43] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer:
Hierarchical vision transformer using shifted windows. In: ICCV. pp. 10012–10022 (2021) 1, 2,
6, 7, 8, 15, 16"
REFERENCES,0.4824561403508772,"[44] Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In:
CVPR. pp. 11976–11986 (2022) 1, 6, 7, 8, 16"
REFERENCES,0.48464912280701755,"[45] Loshchilov, I.,
Hutter,
F.:
Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017) 15"
REFERENCES,0.4868421052631579,"[46] Luo, Z., Xiao, Y., Liu, Y., Li, S., Wang, Y., Tang, Y., Li, X., Yang, Y.: Soc: Semantic-
assisted object cluster for referring video object segmentation. Advances in Neural Information
Processing Systems 36 (2024) 15"
REFERENCES,0.48903508771929827,"[47] Mihaylov, T., Clark, P., Khot, T., Sabharwal, A.: Can a suit of armor conduct electricity? A
new dataset for open book question answering. In: EMNLP. pp. 2381–2391. Association for
Computational Linguistics (2018) 9"
REFERENCES,0.49122807017543857,"[48] Nguyen, E., Goel, K., Gu, A., Downs, G.W., Shah, P., Dao, T., Baccus, S.A., Ré, C.: S4nd:
Modeling images and videos as multidimensional signals using state spaces. arXiv preprint
arXiv:2210.06583 (2022) 7"
REFERENCES,0.4934210526315789,"[49] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are
unsupervised multitask learners. OpenAI blog 1(8), 9 (2019) 9, 19"
REFERENCES,0.4956140350877193,"[50] Ren, S., Yang, X., Liu, S., Wang, X.: Sg-former: Self-guided transformer with evolving token
reallocation. In: ICCV. pp. 6003–6014 (2023) 7"
REFERENCES,0.49780701754385964,"[51] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recogni-
tion. In: Bengio, Y., LeCun, Y. (eds.) ICLR (2015) 2"
REFERENCES,0.5,"[52] Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for sequence
modeling. arXiv preprint arXiv:2208.04933 (2022) 1, 2"
REFERENCES,0.5021929824561403,"[53] Song, L., Li, Y., Jiang, Z., Li, Z., Sun, H., Sun, J., Zheng, N.: Fine-grained dynamic head for
object detection. NIPS (2020) 8"
REFERENCES,0.5043859649122807,"[54] Song, L., Li, Y., Li, Z., Yu, G., Sun, H., Sun, J., Zheng, N.: Learnable tree filter for structure-
preserving feature transform. NeurIPS 32 (2019) 2, 5"
REFERENCES,0.506578947368421,"[55] Song, L., Zhang, S., Yu, G., Sun, H.: Tacnet: Transition-aware context network for spatio-
temporal action detection. In: CVPR (2019) 2, 8"
REFERENCES,0.5087719298245614,"[56] Song, L., Zhang, S., Liu, S., Li, Z., He, X., Sun, H., Sun, J., Zheng, N.: Dynamic grained
encoder for vision transformers. NIPS (2021) 2"
REFERENCES,0.5109649122807017,"[57] Tang, L., Li, K., He, C., Zhang, Y., Li, X.: Source-free domain adaptive fundus image seg-
mentation with class-balanced mean teacher. In: International Conference on Medical Image
Computing and Computer-Assisted Intervention. pp. 684–694. Springer (2023) 15"
REFERENCES,0.5131578947368421,"[58] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., Hashimoto, T.B.:
Stanford alpaca: An instruction-following llama model (2023) 9"
REFERENCES,0.5153508771929824,"[59] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H.: Training data-efficient
image transformers & distillation through attention. In: ICML. pp. 10347–10357. PMLR (2021)
1, 7"
REFERENCES,0.5175438596491229,"[60] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: GLUE: A multi-task
benchmark and analysis platform for natural language understanding. In: ICLR (2019) 9"
REFERENCES,0.5197368421052632,"[61] Wang, J., Song, L., Li, Z., Sun, H., Sun, J., Zheng, N.: End-to-end object detection with fully
convolutional network. In: CVPR (2021) 2"
REFERENCES,0.5219298245614035,"[62] Wang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.:
Internimage: Exploring large-scale vision foundation models with deformable convolutions. In:
CVPR. pp. 14408–14419 (2023) 1, 2, 6, 7, 8, 15, 16"
REFERENCES,0.5241228070175439,"[63] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid
vision transformer: A versatile backbone for dense prediction without convolutions. In: ICCV.
pp. 568–578 (2021) 2"
REFERENCES,0.5263157894736842,"[64] Williams, R.L., Lawrence, D.A., et al.: Linear state-space control systems. John Wiley & Sons
(2007) 3"
REFERENCES,0.5285087719298246,"[65] Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Unified perceptual parsing for scene understanding.
In: ECCV. pp. 418–434 (2018) 8"
REFERENCES,0.5307017543859649,"[66] Xiao, Y., Luo, Z., Liu, Y., Ma, Y., Bian, H., Ji, Y., Yang, Y., Li, X.: Bridging the gap: A unified
video comprehension framework for moment retrieval and highlight detection. CVPR (2024) 15"
REFERENCES,0.5328947368421053,"[67] Xiao, Y., Ma, Y., Li, S., Zhou, H., Liao, R., Li, X.: Semanticac: Semantics-assisted framework
for audio classification. In: ICASSP 2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP). pp. 1–5. IEEE (2023) 15"
REFERENCES,0.5350877192982456,"[68] Xu, Z., Lin, Y., Han, H., Yang, S., Li, R., Zhang, Y., Li, X.: Mambatalk: Efficient holistic
gesture synthesis with selective state space models. arXiv preprint arXiv:2403.09471 (2024) 3"
REFERENCES,0.5372807017543859,"[69] Yang, C., Chen, Z., Espinosa, M., Ericsson, L., Wang, Z., Liu, J., Crowley, E.J.: Plainmamba:
Improving non-hierarchical mamba in visual recognition. arXiv preprint arXiv:2403.17695
(2024) 2, 3, 4, 7, 8, 10"
REFERENCES,0.5394736842105263,"[70] Yang, J., Song, L., Liu, S., Li, Z., Li, X., Sun, H., Sun, J., Zheng, N.: Dbq-ssd: Dynamic ball
query for efficient 3d object detection. arXiv preprint arXiv:2207.10909 (2022) 8"
REFERENCES,0.5416666666666666,"[71] Yang, Q.: Stereo matching using tree filtering. IEEE TPAMI 37(4), 834–846 (2014) 2, 5"
REFERENCES,0.543859649122807,"[72] Yang, R., Song, L., Ge, Y., Li, X.: Boxsnake: Polygonal instance segmentation with box
supervision. In: Proceedings of the IEEE/CVF International Conference on Computer Vision
(2023) 2"
REFERENCES,0.5460526315789473,"[73] Zhang, S., Song, L., Gao, C., Sang, N.: Glnet: Global local network for weakly supervised
action localization. IEEE Transactions on Multimedia 22(10), 2610–2622 (2019) 2"
REFERENCES,0.5482456140350878,"[74] Zhang, S., Song, L., Liu, S., Ge, Z., Li, Z., He, X., Sun, J.: Workshop on autonomous
driving at cvpr 2021: Technical report for streaming perception challenge. arXiv preprint
arXiv:2108.04230 (2021) 8"
REFERENCES,0.5504385964912281,"[75] Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through
ade20k dataset. In: CVPR. pp. 633–641 (2017) 2, 8"
REFERENCES,0.5526315789473685,"[76] Zhou, H., Yang, R., Zhang, Y., Duan, H., Huang, Y., Hu, R., Li, X., Zheng, Y.: Unihead:
unifying multi-perception for detection heads. TNNLS (2023) 15"
REFERENCES,0.5548245614035088,"[77] Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual
representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417
(2024) 1, 3, 4, 7, 8, 16"
REFERENCES,0.5570175438596491,Appendix
REFERENCES,0.5592105263157895,"A
Detailed Training Settings and Results"
REFERENCES,0.5614035087719298,"A.1
Image Classification."
REFERENCES,0.5635964912280702,"We follow the previous works [62, 41, 43] to conduct the experiments. The models are trained with
thirty-two 32GB V100 GPUs by default. We set betas and momentum of the AdamW [45, 76, 66]
optimizer with (0.9, 0.999) and 0.9, respectively. During training, we utilize a Cosine Scheduler with
an initial learning rate of 1×10−3 and weight decay of 0.05. We adopt the common training data aug-
mentation strategies following [34, 62], including AutoAugment [9] with rand-m9-mstd0.5-inc1.
A MixUp strategy with a ratio of 0.8 is also adopted in each batch. Horizontal flip and Random
resized crop strategy are both used in the process of training."
REFERENCES,0.5657894736842105,"6
8
10
12
14
FLOPs 81 82 83 84 85"
REFERENCES,0.5679824561403509,Top-1 Accuracy
REFERENCES,0.5701754385964912,MambaTreeV-T
REFERENCES,0.5723684210526315,"MambaTreeV-S
MambaTreeV-B"
REFERENCES,0.5745614035087719,PlainMamba-L2
REFERENCES,0.5767543859649122,"PlainMamba-L3
VMamba-T"
REFERENCES,0.5789473684210527,"VMamba-S
VMamba-B ViM-S"
REFERENCES,0.581140350877193,LocalMamba-T
REFERENCES,0.5833333333333334,LocalMamba-S
REFERENCES,0.5855263157894737,"MambaTreeV-T
PlainMamba-L2
VMamba-T
ViM-S
LocalMamba-T"
REFERENCES,0.5877192982456141,Figure 5: Classification performance comparison among SSM-based vision foundation models.
REFERENCES,0.5899122807017544,"Performance Comparison.
We compare various SSM-based visual foundation models as shown
in Fig. 5, with different colors representing different models and different shapes indicating different
model scales. The size of each shape indicates the number of model parameters. The horizontal
axis denotes FLOPs and the vertical axis represents the Top-1 accuracy of the corresponding method
on ImageNet-1K val dataset. Fig. 5 demonstrates that MambaTreeV is the best choice in terms of
efficiency and effectiveness."
REFERENCES,0.5921052631578947,"A.2
Object Detection."
REFERENCES,0.5942982456140351,"For a fair comparison, we conduct the evaluation following common practice [62, 41, 43]. The models
are trained with eight 32GB V100 GPUs by default. The input image is resized so that the shorter
side is 800 pixels, while the longer side does not exceed 1333 pixels during the 1× schedule. The
number of warmup steps is set to 500 in the 1× schedule. For 3× schedule, the shorter side is resized
to 480-800 pixels and the longer side does not exceed 1333 pixels. The number of warmup steps is
set to 1000 in 3× schedule. Results shown in Table 8 demonstrate the effectiveness of MambaTreeV
in object detection and instance segmentation on COCO val2017."
REFERENCES,0.5964912280701754,"A.3
Semantic Segmentation."
REFERENCES,0.5986842105263158,"We optimize our MambaTreeV-T/S using AdamW optimizer with an initial learning rate of 6 × 10−5
which is decayed by a rate of 1.0 with the polynomial decay schedule following [62, 27, 57, 16, 28].
The number of warmup iters is set to 1600 with an initial learning rate of 1 × 10−6 [41, 34, 46, 67].
The default input resolution is 512 × 512 as well as FLOPs are calculated with an input size of
512 × 2048. The models are trained with eight 32GB V100 GPUs by default."
REFERENCES,0.6008771929824561,"Method
#FLOPs.
Mask R-CNN 1× Schedule
Mask R-CNN 3× MS Schedule
APb APb
50 APb
75 APm APm
50 APm
75 APb APb
50 APb
75 APm APm
50 APm
75"
REFERENCES,0.6030701754385965,"Swin-T [43]
267G
42.7 65.2
46.8
39.3
62.2
42.2
46.0 68.1
50.3
41.6
65.1
44.9
ConvNeXt-T [44]
262G
44.2 66.6
48.3
40.1
63.3
42.8
46.2 67.9
50.8
41.7
65.0
44.9
CSWin-T [14]
279G
46.7 68.6
51.3
42.2
65.6
45.4
49.0 70.7
53.7
43.6
67.9
46.6
ViM-S [77]
218G
44.9 67.1
49.3
41.0
64.2
44.1
-
-
-
-
-
-
VMamba-T [41]
286G
46.5 68.5
50.7
42.1
65.5
45.3
48.5 69.9
52.9
43.2
66.8
46.3
L-Vmamba-T [34]
291G
46.7 68.7
50.8
42.2
65.7
45.5
48.7 70.1
53.0
43.4
67.0
46.4
MambaTreeV-T (Ours)
265G
47.0 69.4
51.5
42.7
66.4
46.0
49.0 70.8
54.0
43.8
67.6
47.1"
REFERENCES,0.6052631578947368,"Vit-Adapter-S [4]
403G
44.7 65.8
48.3
39.9
62.5
42.8
48.2 69.7
52.5
42.8
66.4
45.9
Swin-S [43]
354G
44.8 66.6
48.9
40.9
63.4
44.2
48.2 69.8
52.8
43.2
67.0
46.1
ConvNeXt-T [44]
348G
45.4 67.9
50.0
41.8
65.2
45.1
47.9 70.0
52.7
42.9
66.9
46.2
InternImage-S [62]
340G
47.8 69.8
52.8
43.3
67.1
46.7
49.7 71.1
54.5
44.5
68.5
47.8
VMamba-S [41]
400G
48.2 69.7
52.5
43.0
66.6
46.4
49.7 70.4
54.2
44.0
67.6
47.3
L-Vmamba-S [34]
414G
48.4 69.9
52.7
43.2
66.7
46.5
49.9 70.5
54.4
44.1
67.8
47.4
MambaTreeV-S (Ours)
341G
48.6 70.3
53.5
43.6
67.5
47.1
50.1 71.2
54.9
44.6
68.7
47.8"
REFERENCES,0.6074561403508771,"Table 8: Object detection and instance segmentation performance on COCO val2017. APb and
APm indicate the mAP of detection and segmentation, respectively. MS indicates the multi-scale
training strategy."
REFERENCES,0.6096491228070176,"B
Language Tree Topology Scanning Operator"
REFERENCES,0.6118421052631579,Algorithm 2 Language Tree Scanning
REFERENCES,0.6140350877192983,"Input: Input feature {xi}L
i=1; Input matrix { ¯Bi}L
i=1; State matrix { ¯Ai}L
i=1; Gradient of loss to
hidden states { ∂Loss"
REFERENCES,0.6162280701754386,"∂hi }L
i=1; Minimum Spanning Tree GT .
Traverse Path: Root, . . . , Leaf ←BFS(GT )
▷Breadth-first topological order of GT
Forward:"
REFERENCES,0.618421052631579,"Initialization: {ξi}L
i=1 ←{xi}L
i=1
2: for i ←Leaf to Root do"
REFERENCES,0.6206140350877193,ξi = ¯Bixi + P
REFERENCES,0.6228070175438597,"∀j∈{t|Par(t)=i} ξj ¯Aj
4: end for
Backward:"
REFERENCES,0.625,"for i ←Root to Leaf do
6:
if i is Root then ∂Loss"
REFERENCES,0.6271929824561403,"∂xi
= ηi ¯Bi ,
∂Loss"
REFERENCES,0.6293859649122807,"∂¯Bi
= ηixi,
∂Loss"
REFERENCES,0.631578947368421,"∂¯Ai = 0
8:
else ∂Loss"
REFERENCES,0.6337719298245614,"∂xi
= ∂Loss"
REFERENCES,0.6359649122807017,∂hi ¯Bi + ¯Ai ∂Loss
REFERENCES,0.6381578947368421,"∂xPar(i) ¯Bi ,
∂Loss"
REFERENCES,0.6403508771929824,"∂¯Bi
= ∂Loss"
REFERENCES,0.6425438596491229,∂hi xi + ¯Ai ∂Loss
REFERENCES,0.6447368421052632,∂¯BPar(i) xi
REFERENCES,0.6469298245614035,"10:
∂Loss"
REFERENCES,0.6491228070175439,"∂¯Ai =
∂Loss
∂x′
P ar(i) hi
end if
12: end for
Output: Hidden states {hi}L
i=1; Grad. of loss to input feature { ∂Loss"
REFERENCES,0.6513157894736842,"∂xi }L
i=1; Grad. of loss to input
matrix { ∂Loss"
REFERENCES,0.6535087719298246,"∂¯Bi }L
i=1; Grad. of loss to state matrix { ∂Loss"
REFERENCES,0.6557017543859649,"∂¯Ai }L
i=1."
REFERENCES,0.6578947368421053,"C
Algorithm Proof"
REFERENCES,0.6600877192982456,"In this section, we present detailed proofs for our tree scanning algorithm. The definitions of symbols
are consistent with those in the main paper."
REFERENCES,0.6622807017543859,"C.1
Proof for Algorithm 1."
REFERENCES,0.6644736842105263,"We randomly take a vertex in the MST GT as the root. According to the definition of the tree scanning
algorithm introduced in Sec. 3.2, we can derive hroot as follows:"
REFERENCES,0.6666666666666666,"hroot =
X"
REFERENCES,0.668859649122807,"∀j∈Croot
S(Eroot,j) ¯Bjxj,
S(Eroot,j) =
Y"
REFERENCES,0.6710526315789473,"k∈Nroot,j"
REFERENCES,0.6732456140350878,"¯Ak,
(9)"
REFERENCES,0.6754385964912281,"which shows a process of aggregation from all leaf vertices to the root. Therefore, each vertex is
only related to its child in this period. Taking vertex m as an example, the Aggrm can be derived as:"
REFERENCES,0.6776315789473685,"Aggrm(x) = ¯Bmxm +
X"
REFERENCES,0.6798245614035088,"∀k∈{t|Par(t)=i}
Aggrk(x) ¯Ak.
(10)"
REFERENCES,0.6820175438596491,We assume that one of the child of m is n and hn can be derived as following:
REFERENCES,0.6842105263157895,"hn = Aggrn(x) + ¯An]
Aggrm(x),
(11)"
REFERENCES,0.6864035087719298,"where ]
Aggrm(x) indicates the aggregation value from the vertices ∈Ω\Croot
m
to vertex m. Therefore,
we can obtain the propagation relationship between the hidden state of parent m and child n:"
REFERENCES,0.6885964912280702,"hn = Aggrn(x) + ¯An]
Aggrm(x)"
REFERENCES,0.6907894736842105,= Aggrn(x) + ¯An(hm −¯AnAggrn(x))
REFERENCES,0.6929824561403509,"= ¯Anhm + (1 −¯A2
n)Aggrn(x) (12)"
REFERENCES,0.6951754385964912,"Through the above derivation, we can calculate {hi}L
i=1 with only two traversals (i.e., the aggre-
gation from leaf to root and the propagation from root to leaf) in the forward process as shown
in Algorithm 1, thereby reducing the computational complexity from O(L2) to O(L)."
REFERENCES,0.6973684210526315,"Next, we analyze the backpropagation process in Algorithm 1. According to the chain rule, we can
easily calculate the derivative of loss with respect to xi: ∂loss"
REFERENCES,0.6995614035087719,"∂xi
=
X j∈Ω ∂loss ∂hj"
REFERENCES,0.7017543859649122,"∂hj
∂xi"
REFERENCES,0.7039473684210527,"= ¯Bi
X"
REFERENCES,0.706140350877193,"j∈Ω
S (Eji) ∂loss ∂hj (13)"
REFERENCES,0.7083333333333334,"Similarly, the derivative of loss with respect to ¯Bi is: ∂loss"
REFERENCES,0.7105263157894737,"∂¯Bi
=
X j∈Ω ∂loss ∂hj"
REFERENCES,0.7127192982456141,"∂hj
∂¯Bi"
REFERENCES,0.7149122807017544,"= xi
X"
REFERENCES,0.7171052631578947,"j∈Ω
S (Eji) ∂loss ∂hj (14)"
REFERENCES,0.7192982456140351,The above formulas are equivalent to replacing the input x with ∂loss
REFERENCES,0.7214912280701754,"∂h
during the forward process."
REFERENCES,0.7236842105263158,"Subsequently, we assume that the vertex k is the child of vertex l and define Ck
l indicates the children
of vertex l with the root of vertex k. ∂loss"
REFERENCES,0.7258771929824561,∂¯Ak is formulated as follows: ∂loss
REFERENCES,0.7280701754385965,"∂¯Ak
=
X j∈Ω ∂loss ∂hj"
REFERENCES,0.7302631578947368,"∂hj
∂¯Ak =
X j∈Ω ∂loss ∂hj X p∈Ω"
REFERENCES,0.7324561403508771,"∂S(Ejp) ¯Bpx′
p
∂¯Ak =
X"
REFERENCES,0.7346491228070176,"j∈Ck
l ∂loss ∂hj X"
REFERENCES,0.7368421052631579,"p∈Cl
k"
REFERENCES,0.7390350877192983,"S(Ekp)S(Ejl) ¯Bpx′
p +
X"
REFERENCES,0.7412280701754386,"j∈Cl
k ∂loss ∂hj X"
REFERENCES,0.743421052631579,"p∈Ck
l"
REFERENCES,0.7456140350877193,"S(Ekj)S(Epl) ¯Bpx′
p =
X"
REFERENCES,0.7478070175438597,"j∈Ck
l"
REFERENCES,0.75,S(Ejl)∂loss ∂hj X
REFERENCES,0.7521929824561403,"p∈Cl
k"
REFERENCES,0.7543859649122807,"S(Ekp) ¯Bpx′
p +
X"
REFERENCES,0.756578947368421,"j∈Cl
k"
REFERENCES,0.7587719298245614,S(Ekj)∂loss ∂hj X
REFERENCES,0.7609649122807017,"p∈Ck
l"
REFERENCES,0.7631578947368421,"S(Epl) ¯Bpx′
p"
REFERENCES,0.7653508771929824,= (∂Loss
REFERENCES,0.7675438596491229,"∂xk
−¯AkAggrk(∂loss"
REFERENCES,0.7697368421052632,∂h )) ∗Aggrk(x) + Aggrk(∂loss
REFERENCES,0.7719298245614035,∂h ) ∗(hk −¯AkAggrk(x))
REFERENCES,0.7741228070175439,= ∂Loss
REFERENCES,0.7763157894736842,"∂xk
Aggrk(x) + Aggrk(∂loss"
REFERENCES,0.7785087719298246,∂h )hk −2 ¯AkAggrk(∂loss
REFERENCES,0.7807017543859649,∂h )Aggrk(x)
REFERENCES,0.7828947368421053,≜∂Loss
REFERENCES,0.7850877192982456,"∂xk
ξk + ηkhk −2 ¯Akηkξk
(definition in Algorithm 1)"
REFERENCES,0.7872807017543859,"(15)
So far we have completed the proof of forward and back-propagation of Algorithm 1."
REFERENCES,0.7894736842105263,"C.2
Proof for Algorithm 2."
REFERENCES,0.7916666666666666,"We only take the last token as root and replace the transition source from Ωto Ci in sequence
modeling tasks like nature language understanding to ensure causality. Therefore, only one traversal
(from leaf to root) is required for the forward process, and another traversal (from root to leaf) is
needed for the backpropagation process. The proof is similar to the Algorithm 1. (a) (b) (c) (d)"
REFERENCES,0.793859649122807,"Input
TP
Raster
Cross
TP
Raster
Cross"
REFERENCES,0.7960526315789473,"Figure 6: Visualization of affinity maps in the specific position. The Location is marked by the red
cross in each affinity map. TP represents our Tree Scanning Algorithm."
REFERENCES,0.7982456140350878,"D
More Qualitative Results"
REFERENCES,0.8004385964912281,"Fig. 6 displays additional qualitative comparisons between our algorithm and previous scanning
strategies (e.g., cross-scanning and raster-scanning), which shows our advanced capability to perceive
detailed structural information and capture long-range dependencies."
REFERENCES,0.8026315789473685,"E
Statistical Significance"
REFERENCES,0.8048245614035088,"Method
PIQA
Arc-Easy
SST
WinoGrande
LAM-ppl
Race
Openbookqa"
REFERENCES,0.8070175438596491,"MambaTreeL (Ours)
0.011
0.010
0.016
0.014
0.553
0.014
0.018"
REFERENCES,0.8092105263157895,"Table 9: Standard error on language model benchmarks. LAM-ppl indicates LAMBADA [49].
We calculate the standard deviation of our MambaTreeL on language model benchmarks in the
open-sourced lm-evaluation-harness project as shown in Table 9."
REFERENCES,0.8114035087719298,NeurIPS Paper Checklist
CLAIMS,0.8135964912280702,1. Claims
CLAIMS,0.8157894736842105,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The claims in the abstract and introduction accurately reflect our motivation
and contribution.
Guidelines:"
CLAIMS,0.8179824561403509,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.8201754385964912,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We have discussed the limitations of our work.
Guidelines:"
CLAIMS,0.8223684210526315,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.8245614035087719,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]"
CLAIMS,0.8267543859649122,"Justification: We provide detailed proofs for our tree topology scanning algorithm in both
the main paper and the appendix."
CLAIMS,0.8289473684210527,Guidelines:
CLAIMS,0.831140350877193,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8333333333333334,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8355263157894737,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8377192982456141,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8399122807017544,"Justification: We describe the experimental details both in the main paper and the appendix.
We will make our code publicly available, along with detailed instructions for reproduction."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8421052631578947,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8442982456140351,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8464912280701754,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8486842105263158,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8508771929824561,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8530701754385965,Justification: We provide the core part of our code in the supplementary material.
OPEN ACCESS TO DATA AND CODE,0.8552631578947368,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8574561403508771,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8596491228070176,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8618421052631579,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8640350877192983,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8662280701754386,"Justification: The complete experiment details are introduced in the main paper and the
appendix."
OPEN ACCESS TO DATA AND CODE,0.868421052631579,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8706140350877193,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8728070175438597,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.875,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8771929824561403,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8793859649122807,Justification: We provide the statistical analysis results in the appendix.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.881578947368421,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8837719298245614,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8859649122807017,"• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.8881578947368421,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.8903508771929824,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.8925438596491229,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.8947368421052632,"Justification: We have described the resources required to perform our experiments in the
appendix."
EXPERIMENTS COMPUTE RESOURCES,0.8969298245614035,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.8991228070175439,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9013157894736842,9. Code Of Ethics
CODE OF ETHICS,0.9035087719298246,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9057017543859649,Answer: [Yes]
CODE OF ETHICS,0.9078947368421053,Justification: Our work conforms with the NeurIPS Code of Ethics in every respect
CODE OF ETHICS,0.9100877192982456,Guidelines:
CODE OF ETHICS,0.9122807017543859,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9144736842105263,10. Broader Impacts
BROADER IMPACTS,0.9166666666666666,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.918859649122807,Answer: [No]
BROADER IMPACTS,0.9210526315789473,"Justification: Our work mainly focuses on the research of basic neural network architecture.
We have identified no potential negative social impacts."
BROADER IMPACTS,0.9232456140350878,Guidelines:
BROADER IMPACTS,0.9254385964912281,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact."
BROADER IMPACTS,0.9276315789473685,"• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9298245614035088,11. Safeguards
SAFEGUARDS,0.9320175438596491,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9342105263157895,Answer: [NA]
SAFEGUARDS,0.9364035087719298,Justification: Our paper poses no such risks.
SAFEGUARDS,0.9385964912280702,Guidelines:
SAFEGUARDS,0.9407894736842105,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9429824561403509,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9451754385964912,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9473684210526315,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9495614035087719,"Justification: The creators or original owners of assets (e.g., code, data, models), used in the
paper, are properly credited as well as the license and terms of use explicitly are mentioned
and properly respected."
LICENSES FOR EXISTING ASSETS,0.9517543859649122,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9539473684210527,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided."
LICENSES FOR EXISTING ASSETS,0.956140350877193,"• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9583333333333334,13. New Assets
NEW ASSETS,0.9605263157894737,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9627192982456141,Answer: [NA]
NEW ASSETS,0.9649122807017544,Justification: No new assets introduced.
NEW ASSETS,0.9671052631578947,Guidelines:
NEW ASSETS,0.9692982456140351,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714912280701754,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9736842105263158,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758771929824561,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780701754385965,Justification: Our work does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802631578947368,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824561403508771,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846491228070176,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868421052631579,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890350877192983,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912280701754386,Justification: Our work does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.993421052631579,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956140350877193,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978070175438597,"• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
