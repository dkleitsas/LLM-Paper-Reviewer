Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.005291005291005291,"We study the regret of Thompson sampling (TS) algorithms for exponential family
bandits, where the reward distribution is from a one-dimensional exponential fam-
ily, which covers many common reward distributions including Bernoulli, Gaussian,
Gamma, Exponential, etc. We propose a Thompson sampling algorithm, termed
ExpTS, which uses a novel sampling distribution to avoid the under-estimation
of the optimal arm. We provide a tight regret analysis for ExpTS, which simulta-
neously yields both the ﬁnite-time regret bound as well as the asymptotic regret
bound. In particular, for a K-armed bandit with exponential family rewards, Ex-
pTS over a horizon T is sub-UCB (a strong criterion for the ﬁnite-time regret that
is problem-dependent), minimax optimal up to a factor √log K, and asymptoti-
cally optimal, for exponential family rewards. Moreover, we propose ExpTS+, by
adding a greedy exploitation step in addition to the sampling distribution used in
ExpTS, to avoid the over-estimation of sub-optimal arms. ExpTS+ is an anytime
bandit algorithm and achieves the minimax optimality and asymptotic optimality
simultaneously for exponential family reward distributions. Our proof techniques
are general and conceptually simple and can be easily applied to analyze standard
Thompson sampling with speciﬁc reward distributions."
INTRODUCTION,0.010582010582010581,"1
Introduction"
INTRODUCTION,0.015873015873015872,"The Multi-Armed Bandit (MAB) problem is centered around a fundamental model for balancing
the exploration versus exploitation trade-off in many online decision problems. In this problem,
the agent is given an environment with a set of K arms [K] = {1, 2, · · · , K}. At each time step
t, the agent pulls an arm At ∈[K] based on observations of previous t −1 time steps, and then a
reward rt is revealed at the end of the step. In real-world applications, reward distributions often
have different forms such as Bernoulli, Gaussian, etc. As suggested by Auer et al. [8, 9], Agrawal
and Goyal [5], Lattimore [29], Garivier et al. [16], a good bandit strategy should be general enough
to cover a sufﬁciently rich family of reward distributions. In this paper, we assume the reward rt is
independently generated from some canonical one-parameter exponential family of distributions with
a mean value µAt. It is a rich family that covers many common distributions including Bernoulli,
Gaussian, Gamma, Exponential, and others."
INTRODUCTION,0.021164021164021163,"The goal of a bandit strategy is usually to maximize the cumulative reward over T time steps, which
is equivalent to minimizing the regret, deﬁned as the expected cumulative difference between playing"
INTRODUCTION,0.026455026455026454,"the best arm and playing the arm according to the strategy: Rµ(T) = T · maxi∈[K] µi −E[PT
t=1 rt].
We assume, without loss of generality, µ1 = maxi∈[K] µi is the best arm throughout this paper. For a
ﬁxed bandit instance (i.e., mean rewards µ1, · · · , µK are ﬁxed), Lai and Robbins [26] shows that for
distributions that are continuously parameterized by their means,"
INTRODUCTION,0.031746031746031744,"lim
T →∞
Rµ(T)"
INTRODUCTION,0.037037037037037035,"log T
≥
X i>1"
INTRODUCTION,0.042328042328042326,"µ1 −µi
kl(µi, µ1),
(1.1)"
INTRODUCTION,0.047619047619047616,"where kl(µi, µ1) is the Kullback-Leibler divergence between two distributions with mean µi and µ1.
A bandit strategy satisfying limT →∞Rµ(T)/ log T = P"
INTRODUCTION,0.05291005291005291,"i>1
µ1−µi
kl(µi,µ1) is said to be asymptotically
optimal or achieve the asymptotic optimality in regret. The asymptotic optimality is one of the most
important statistical properties in regret minimization, which shows that an algorithm is consistently
good when it is played for inﬁnite steps and thus should be a basic theoretical requirement of any
good bandit strategy [8]."
INTRODUCTION,0.0582010582010582,"In practice, we can only run the bandit algorithm for a ﬁnite number T steps, which is the time
horizon of interest in real-world applications. Therefore, the ﬁnite-time regret is the ultimate property
of a practical bandit strategy in regret minimization problems. A strong notion of ﬁnite-time regret
bounds is called the sub-UCB criteria [29]. An algorithm is sub-UCB if there exist universal constants
C1, C2 > 0 such that for any problem instances,"
INTRODUCTION,0.06349206349206349,"Rµ(T) = C1
X"
INTRODUCTION,0.06878306878306878,"i∈[K]:∆i>0
∆i + C2
X"
INTRODUCTION,0.07407407407407407,i∈[K]:∆i>0 log T
INTRODUCTION,0.07936507936507936,"∆i
,
(1.2)"
INTRODUCTION,0.08465608465608465,"where ∆i = µ1 −µi is the sub-optimal gap between arm 1 and arm i. Note that the regret bound in
(1.2) is a problem-dependent bound since it depends on the bandit instance and the sub-optimal gaps.
Sub-UCB is an important metric for ﬁnite-time regret bound and has been adopted by recent work of
Lattimore [29], Bian and Jun [10]. Another special type of ﬁnite-time bounds is called the worst-case
regret, which is deﬁned as the ﬁnite-time regret of an algorithm on any possible bandit instance
within a bandit class. Speciﬁcally, for a ﬁnite time horizon T, Auer et al. [8] proves that any strategy
has at least worst-case regret Ω(
√"
INTRODUCTION,0.08994708994708994,"KT) for a K-armed bandit. We say the strategy that achieves a
worst-case regret O(
√"
INTRODUCTION,0.09523809523809523,"KT) is minimax optimal or achieves the minimax optimality. Different from
the asymptotic optimality, the minimax optimality characterizes the worst-case performance of the
bandit strategy in ﬁnite steps."
INTRODUCTION,0.10052910052910052,"A vast body of literature in multi-armed bandits [6, 5, 22, 32, 16, 29] have been pursing the afore-
mentioned theoretical properties of bandit algorithms: generality, asymptotic optimality, problem-
dependent ﬁnite-time regret, and minimax optimality. However, most of them focus on one or two
properties and sacriﬁce the others. Moreover, many of existing theoretical analyses of bandit strategies
are for optimism-based algorithm. The theoretical analysis of Thompson sampling (TS) is much less
understood until recently, which has been shown to exhibit superior practical performances compared
to the state-of-the-art methods [13, 34]. Speciﬁcally, its ﬁnite-time regret, asymptotic optimality, and
near minimax optimality have been studied by Agrawal and Goyal [3, 4, 5] for Bernoulli rewards. Jin
et al. [20] proved the minimax optimality of TS for sub-Gaussian rewards. For exponential family
reward distributions, the asymptotic optimality is shown by Korda et al. [25], but no ﬁnite-time regret
of TS is provided. See Table 1 for a comprehensive comparison of these results."
INTRODUCTION,0.10582010582010581,"In this paper, we study the regret of Thompson sampling for exponential family reward distributions
and address all the theoretical properties of TS. We propose a variant of TS algorithm with a general
sampling distribution and a tight analysis for frequentist regret bounds. Our analysis simultaneously
yields both the ﬁnite-time regret bound and the asymptotic regret bound.
Speciﬁcally, the main contributions of this paper are summarized as follows:"
INTRODUCTION,0.1111111111111111,"• We propose ExpTS, a general variant of Thompson sampling, that uses a novel sampling distribution
with a tight anti-concentration bound to avoid the under-estimation of the optimal arm and a tight
concentration bound to avoid the over-estimation of sub-optimal arms. For exponential family of
reward distributions, we prove that ExpTS is the ﬁrst Thompson sampling algorithm achieving the
sub-UCB criteria, which is a strong notion of problem-dependent ﬁnite-time regret bounds. We
further show that ExpTS is also simultaneously minimax optimal up to a factor of √log K, as well
as asymptotically optimal, where K is the number of arms.
• We also propose ExpTS+, which explores between the sample generated in ExpTS and the em-
pirical mean reward for each arm, to get rid of the extra √log K factor in the worst-case regret."
INTRODUCTION,0.1164021164021164,"Table 1: Comparisons of different Thompson sampling algorithms on K-armed bandits over a horizon
T. For any algorithm, Asym. Opt is the indicator whether it is asymptotically optimal, minimax ratio
is the scaling of its worst-case regret w.r.t. the minimax optimal regret O(
√"
INTRODUCTION,0.12169312169312169,"V KT), where V is the
variance of reward distributions, and sub-UCB is the indicator if it satisﬁes the sub-UCB criteria."
INTRODUCTION,0.12698412698412698,"Algorithm
Reward Type
Asym.
Finite-Time Regret
Anytime
Reference
Opt
Minimax Ratio
Sub-UCB"
INTRODUCTION,0.13227513227513227,"TS
Bernoulli
yes
√log T
–*
yes
[4]
TS
Bernoulli
–
√log K
–*
yes
[5]
TS
Exponential Family
yes
–
–
yes
[25]
MOTS
sub-Gaussian
no
1
no
no
[20]
MOTS-J
Gaussian
yes
1
no
no
[20]
ExpTS
Exponential Family
yes
√log K
yes
yes
This paper
ExpTS+
Exponential Family
yes
1
no
yes
This paper"
INTRODUCTION,0.13756613756613756,"* [4, 5] did not explicitly show that their regret bounds are sub-UCB. However, the intermediate results
in their proofs might imply sub-UCB regret bounds."
INTRODUCTION,0.14285714285714285,"Thus ExpTS+ is the ﬁrst Thompson sampling algorithm that is simultaneously minimax and
asymptotically optimal for exponential family of reward distributions.
• Our regret analysis of ExpTS can be easily extended to analyze standard Thompson sampling with
common reward distributions. We prove that standard Thompson sampling without inﬂating the
posterior distribution1 is minimax optimal up to a factor of √log K, which matches the regret lower
bound for standard Thompson sampling in Agrawal and Goyal [5]. Similar to the idea of ExpTS+,
we can add a greedy exploration step to the posterior distributions used in these variants of TS, and
then the algorithms are simultaneously minimax and asymptotically optimal."
INTRODUCTION,0.14814814814814814,"Our techniques are novel and conceptually simple. First, we introduce a lower conﬁdence bound
in the regret decomposition to avoid the under-estimation of the optimal arm, which is important
in obtaining the ﬁnite-time regret bound. Speciﬁcally, Jin et al. [20] (Lemma 5 in their paper)
shows that for Gaussian reward distributions, Gaussian-TS has a regret bound at least in the order
of Ω(√KT log T) if the standard regret decomposition in existing analysis of Thompson sampling
[5, 30, 20] is adopted. With our new regret decomposition that is conditioned on the lower conﬁdence
bound introduced in this paper, we improve the worst-case regret of Gaussian-TS for Gaussian reward
distributions to O(√KT log K)."
INTRODUCTION,0.15343915343915343,"Second, we do not require the closed form of the reward distribution, but only make use of the
corresponding concentration bounds. This means our results can be readily extended to other reward
distributions. For example, we can extend ExpTS+ to sub-Gaussian reward distributions and the
algorithm is simultaneously minimax and asymptotically optimal2, which improve the results of
MOTS proposed by Jin et al. [20] (see Table 1)."
INTRODUCTION,0.15873015873015872,"Third, the idea of ExpTS+ is simple and can be used to remove the extra √log K factor in the
worst-case regret. We note that MOTS [20] can also achieve the minimax optimal via the clipped
Gaussian. However, it is not clear how to generalize the clipping idea to the exponential family
of reward distribution. Moreover, it uses the MOSS [6] index for clipping, which needs to know
the horizon T in advance and thus cannot be extended to the anytime setting, while ExpTS+ is an
anytime bandit algorithm which does not need to know the horizon length in advance."
INTRODUCTION,0.164021164021164,"Notations. We let T be the total number of time steps, K be the total number of arms, and
[K] = {1, 2, · · · , K}. For simplicity, we assume arm 1 is the optimal throughout this paper, i.e.,
µ1 = maxi∈[K] µi. We denote log+(x) = max{0, log x} and ∆i := µ1 −µi, i ∈[K] \ {1} for the"
INTRODUCTION,0.1693121693121693,"1This is a common trick in the literature. In particular, for Bernoulli rewards, instead of using Beta posterior,
Agrawal and Goyal [5] consider Thompson sampling with Gaussian posterior, whose variance is larger. Moreover,
Jin et al. [20] inﬂate the variance of the sampling distribution by a factor 1/ρ, where ρ < 1. However, both of
these methods lose the asymptotic optimality.
2Note that sub-Gaussian is a non-parametric family and thus the lower bound (1.1) by Lai and Robbins [26]
does not directly apply to a general sub-Gaussian distribution. Following similar work in the literature [20], in
this paper, when we say an algorithm achieves the asymptotic optimality for sub-Gaussian rewards, we mean its
regret matches the asymptotic lower bound for Gaussian rewards, which is a stronger notion."
INTRODUCTION,0.1746031746031746,"gap between arm 1 and arm i. We let Ti(t) := Pt
j=1 1{At = i} be the number of pulls of arm i at
the time step t, bµi(t) := 1/Ti(t) Pt
j=1

rj · 1{At = i}

be the average reward of arm i at the time
step t, and bµis be the average reward of arm i after its s-th pull. We reserve notations C1, C2, · · · to
be positive universal constants that are independent of problem parameters."
RELATED WORK,0.17989417989417988,"2
Related Work"
RELATED WORK,0.18518518518518517,"There are series of works pursuing the asymptotic regret bound and worst-case regret bound for
MAB. For asymptotic optimality, UCB algorithms [15, 31, 5, 29], Thompson sampling [23, 25, 5, 20],
Bayes-UCB [22], and other methods [21, 10] are all shown to be asymptotically optimal. Among
them, only a few [15, 12, 25] can be extended to exponential families of distributions. One notable
result in Cappé et al. [12] shows that for [0, 1] bounded distribution, there exists an algorithm that has
regret P"
RELATED WORK,0.19047619047619047,"i>1 ∆i log T/kl(µi, µ1) + O(P"
RELATED WORK,0.19576719576719576,"i>1(log T)4/5 log log T · ∆i)3, which is better than (1.2).
It is an interesting problem whether we can achieve such regret for unbounded reward distributions.
For the worst-case regret, MOSS [6] is the ﬁrst algorithm proved to be minimax optimal. Later,
KL-UCB++ [5], AdaUCB [29], MOTS [20] also join the family. The anytime version of MOSS
is studied by Degenne and Perchet [14]. There are also some works that focus on the near optimal
problem-dependent regret bound [27, 28]. As far as we know, no algorithm has been proved to
achieve the sub-UCB criteria, asymptotic optimality, and minimax optimality simultaneously for
exponential family reward distributions."
RELATED WORK,0.20105820105820105,"For Thompson sampling, Russo and Van Roy [33] studied the Bayesian regret. They show that
the Bayesian regret of Thompson sampling is never worse than the regret of UCB. Bubeck and
Liu [11] further showed the Bayesian regret of Thompson sampling is optimal using the regret
analysis of MOSS. There are also a line of works focused on the frequentist regret of TS. Agrawal
and Goyal [3] proposed the ﬁrst ﬁnite time regret analysis for TS. Kaufmann et al. [23], Agrawal
and Goyal [4] proved that TS with Beta posteriors is asymptotically optimal for Bernoulli reward
distributions. Korda et al. [25] extended the asymptotic optimality to the exponential family of reward
distributions. Subsequently, for Bernoulli rewards, Agrawal and Goyal [5] proved that TS with Beta
prior is asymptotically optimal and has worst-case regret O(√KT log T). Besides, they showed that
TS with Gaussian posteriors can achieve a better worst-case regret bound O(√KT log K). They
also proved that for Bernoulli rewards, TS with Gaussian posteriors has a worst-case regret at least
Ω(√KT log K). Very recently, Jin et al. [20] proposed the MOTS algorithm that can achieve the
minimax optimal regret O(
√"
RELATED WORK,0.20634920634920634,"KT) for multi-armed bandits with sub-Gaussian rewards but at the
cost of losing the asymptotic optimality by a multiplicative factor of 1/ρ, where 0 < ρ < 1 is an
arbitrarily ﬁxed constant. For bandits with Gaussian rewards, Jin et al. [20] proved that MOTS
combined with a Rayleigh distribution can achieve the minimax optimality and the asymptotic
optimality simultaneously. We refer readers to Tables 1 and 2 for more details."
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.21164021164021163,"3
Preliminary on Exponential Family Distributions"
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.21693121693121692,"A one-dimensional canonical exponential family [15, 17, 32] is a parametric set of probability
distributions with respect to some reference measure, with the density function given by"
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.2222222222222222,"pθ(x) = exp(xθ −b(θ) + c(x)),"
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.2275132275132275,"where θ is the model parameter, and c is a real function. Denote the measure of pθ(x) as νθ. Then,
the above deﬁnition can be rewritten as
dνθ"
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.2328042328042328,"dρ (x) = exp(xθ −b(θ)),"
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.23809523809523808,"for some measure ρ and b(θ) = log(
R
exθdρ(x)). We make the classic assumption used by Garivier
and Cappé [15], Ménard and Garivier [32] that b(θ) is twice differentiable with a continuous second
derivative. Then, we can verify that exponential families have the following properties:"
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.24338624338624337,"b′(θ) = E[νθ]
and
b′′(θ) = Var[νθ] > 0."
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.24867724867724866,"3Cappé et al. [12] used a more general notation Kinf(·, ·) for any distribution supported in [0, 1], which is
equivalent to the kl(·, ·) notation for the one-exponential family distribution studied in our paper."
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.25396825396825395,"Let µ = E[νθ]. The above equality means that the mapping between the mean value µ of ν(θ) and
the parameter θ is one-to-one. Hence, exponential family of distributions can also be parameterized
by the mean value µ = b′(θ). Note that b′′(θ) > 0 for all θ, which implies b′(·) is invertible and its
inverse function b′−1 satisﬁes θ = b′−1(µ). In this paper, we will use the notion of Kullback-Leibler
(KL) divergence. The KL divergence between two exponential family distributions with parameter θ
and θ′ respectively is deﬁned as follows:
KL(νθ, νθ′) = b(θ′) −b(θ) −b′(θ)(θ′ −θ).
(3.1)
Recall that the mapping θ 7→µ is one-to-one. We can deﬁne an equivalent notion of the KL divergence
between random variables νθ and νθ′ as a function of the mean values µ and µ′ respectively:
kl(µ, µ′) = KL(νθ, νθ′),
where E[νθ] = µ and E[νθ′] = µ′. Similarly, we deﬁne V (µ) = Var(νb′−1(µ)) as the variance of an
exponential family random variable νθ with mean µ. We assume the variances of exponential family
distributions used in this paper are bounded by a constant V > 0: 0 < V (µ) ≤V < +∞. We have
the following property of the KL divergence between exponential family distributions.
Proposition 3.1 (Harremoës [17]). Let µ and µ′ be the mean values of two exponential family
distributions. The Kullback-Leibler divergence between them can be calculated as follows:"
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.25925925925925924,"kl(µ, µ′) =
Z µ′ µ x −µ"
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.26455026455026454,"V (x) dx.
(3.2)"
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.2698412698412698,"Based on Proposition 3.1, we can also verify the following properties.
Proposition 3.2 (Jin et al. [19]). For all µ and µ′, we have
kl(µ, µ′) ≥(µ −µ′)2/(2V ).
(3.3)
In addition, for ϵ > 0 and µ ≤µ′ −ϵ, we can obtain that
kl(µ, µ′) ≥kl(µ, µ′ −ϵ)
and
kl(µ, µ′) ≤kl(µ −ϵ, µ′).
(3.4)"
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.2751322751322751,"Exponential families cover many of the most common distributions used in practice such as Bernoulli,
exponential, Gamma, and Gaussian distributions. In particular, for two Gaussian distributions with
the same known variance σ2 but different means µ and µ′, we can choose V (·) = σ2, and it holds
that kl(µ, µ′) = (µ −µ′)2/(2σ2). For two Bernoulli distributions with means µ and µ′ respectively,
the variance upper bound is set as V = 1/4. Thus we can recover the result in Proposition 3.1 as
kl(µ, µ′) = µ log(µ/µ′) + (1 −µ) log((1 −µ)/(1 −µ′)). For exponential and Gamma distributions,
it sufﬁces to ensure the variance id bounded as long as we assume the mean value is bounded."
PRELIMINARY ON EXPONENTIAL FAMILY DISTRIBUTIONS,0.2804232804232804,"The deﬁnition of one-dimensional exponential family in our paper is pθ(x) = exp(xθ −b(θ) + c(x)),
which is the same as that used by Garivier and Cappé [15], Harremoës [17], Jin et al. [19], Ménard
and Garivier [32] as well as Cappé et al. [12]. The one-dimensional exponential family considered in
Korda et al. [25] (pθ(x) = exp(T(x)θ−b(θ)+c(x))) is more general than that in the aforementioned
papers (see page 4 in Korda et al. [25]). Lai and Robbins [26] considers parametric distributions that
satisﬁes some mild conditions, which is also more general than ours. Moreover, Cappé et al. [12]
also considered the general reward distributions supported in [0, 1], which is not compatible to ours."
THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARD DISTRIBUTIONS,0.2857142857142857,"4
Thompson Sampling for Exponential Family Reward Distributions"
THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARD DISTRIBUTIONS,0.291005291005291,"We present a general variant of Thompson sampling for exponential family rewards in Algorithm 1,
named as ExpTS. At round t, ExpTS maintains an estimate of a sampling distribution for each arm,
denoted as P. The algorithm generates a sample parameter θi(t) for each arm i independently from
their sampling distribution and chooses the arm that attains the largest sample parameter. For each
arm i ∈[K], the sampling distribution P is usually deﬁned as a function of the total number of pulls
Ti(t) and the empirical average reward bµi(t). After pulling the chosen arm, the algorithm updates
Ti(t) and bµi(t) for each arm based on the reward rt it receives and proceeds to the next round."
THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARD DISTRIBUTIONS,0.2962962962962963,"Since we study the frequentist regret bound of Algorithm 1, ExpTS is not restricted as a Bayesian
method. It has been shown [1, 20, 24, 36] that the sampling distribution does not have to be a
posterior distribution derived from a pre-deﬁned prior distribution. Therefore, we call P the sampling
distribution instead of the posterior distribution as in Bayesian regret analysis of Thompson sampling
[33, 11]. To obtain the ﬁnite-time regret bound of ExpTS for exponential family rewards, we will
discuss the choice of a general sampling distribution and a new proof technique."
THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARD DISTRIBUTIONS,0.30158730158730157,Algorithm 1 Exponential Family Thompson Sampling (ExpTS)
THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARD DISTRIBUTIONS,0.30687830687830686,"1: Input: Arm set [K]
2: Initialization: Play each arm once and set Ti(K) = 1; let bµi(K) be the observed reward of
playing arm i
3: for t = K + 1, K + 2, · · · do
4:
For all i ∈[K], sample θi(t) independently from P(bµi(t), Ti(t))
5:
Play arm At = arg maxi∈[K] θi(t) and observe the reward rt
6:
For all i ∈[K], update the mean reward estimator and the number of pulls:"
THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARD DISTRIBUTIONS,0.31216931216931215,ˆµi(t) = Ti(t −1) · bµi(t −1) + rt 1{i = At}
THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARD DISTRIBUTIONS,0.31746031746031744,"Ti(t −1) + 1{i = At}
,
Ti(t) = Ti(t −1) + 1{i = At}"
THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARD DISTRIBUTIONS,0.32275132275132273,7: end for
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.328042328042328,"4.1
Challenges in Regret Analysis for Exponential Family Bandits"
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.3333333333333333,"Before we choose a speciﬁc sampling distribution P for ExpTS, we ﬁrst discuss the main challenges
in the ﬁnite-time regret analysis of Thompson sampling, which is the main motivation for our design
of P in the next subsection."
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.3386243386243386,"Under-Estimation of the Optimal Arm. Denote bµis as the average reward of arm i after its s-th
pull, Ti(t) as the number of pulls of arm i at time t, and P(bµis, s) as the sampling distribution of arm
i after its s-th pull. The regret of the algorithm contributed by pulling arm i is ∆iE[Ti(T)], where
Ti(T) is the total number of pulls of arm i. All existing analyses of ﬁnite-time regret bounds for
TS [3–5, 20] decompose this regret term as ∆iE[Ti(T)] ≤Di + hi(∆i, T, θi(1), . . . , θi(T)), where
hi() is a quantity characterizing the over-estimation of arm i which can be easily dealt with by some
concentration properties of the sampling distribution (see Lemma A.3 for more details). The term Di
characterizes the under-estimation of the optimal arm 1, which is usually bounded as follows."
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.3439153439153439,"Di = ∆i T
X"
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.3492063492063492,"s=1
Ebµ1s"
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.3544973544973545,"
1
G1s(ϵ) −1

,
(4.1)"
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.35978835978835977,"where G1s(ϵ) = 1 −F1s(µ1 −ϵ), F1s is the CDF of the sampling distribution P(bµ1s, s), and
ϵ = Θ(∆i). In other words, G1s(ϵ) = P(θ1(t) > µ1 −ϵ) is the probability that the best arm will not
be under-estimated from the mean reward by a margin ϵ. Furthermore, we can interpret the quantity
in (4.1) as the result of a union bound indicating how many samples TS requires to ensure that at least
one sample of the best arm {θ1(t)}T
t=1 is larger than µ1 −ϵ. If G1s(ϵ) is too small, arm 1 could be
signiﬁcantly under-estimated, and thus Di will be unbounded. In fact, as shown in Lemma 5 by Jin
et al. [20], for MAB with Gaussian rewards, TS using Gaussian posteriors will unavoidably suffer
from the lower bound of K · Di = Ω(√KT log T)."
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.36507936507936506,"To address the above issue, we introduce a lower conﬁdence bound for measuring the under-estimation
problem. We use a new decomposition of the regret that bounds Di with the following term ∆i T
X"
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.37037037037037035,"s=1
Ebµ1s"
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.37566137566137564,"
1
G1s(ϵ) −1

· 1{bµ1s ≥Lows}

,
(4.2)"
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.38095238095238093,"where Lows is a lower conﬁdence bound of bµ1s. Intuitively, due to the concentration of arm 1’s
rewards, the probability of bµ1s ≤Lows is very small. Thus, even when G1s(ϵ) is small, the overall
regret can be well controlled."
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.3862433862433862,"In the regret analysis of TS, we can bound (4.2) from two facets: (1) the lower conﬁdence bound can
be proved using the concentration property of the reward distribution; and (2) the term G1s(ϵ) =
P(θ1(t) > µ1 −ϵ) can be upper bounded by the anti-concentration property for the sampling
distribution P. To achieve an optimal regret, one needs to carefully balance the interplay between
these two bounds. For a speciﬁc reward distribution (e.g., Gaussian, Bernoulli) as is studied by
Agrawal and Goyal [5], Jin et al. [20], there are already tight anti-concentration inequalities for the
reward distribution, and thus the lower conﬁdence bound is tight. Therefore, by choosing Gaussian or
Bernoulli as the prior (which leads to a Gaussian or Beta sampling distribution P), we can use existing
anti-concentration bounds for Gaussian [2, Formula 7.1.13] or Beta [18, Prop. A.4] distributions to
obtain a tight bound of G1s(ϵ)."
CHALLENGES IN REGRET ANALYSIS FOR EXPONENTIAL FAMILY BANDITS,0.3915343915343915,"In this paper, we study the general exponential family of reward distributions, which has no closed
form. Thus we cannot obtain a tight concentration bound for ˆµ1s as in special cases such as Gaussian
or Bernoulli rewards. This increases the hardness of tightly bounding term (4.2) and it is imperative
for us to design a sampling distribution P with a tight anti-concentration bound that can carefully
control G1s(ϵ) without any knowledge of the closed form distribution of the average reward ˆµ1s. Due
to the generality of exponential family distributions, it is challenging and nontrivial to ﬁnd such a
sampling distribution to obtain a tight ﬁnite-time regret bound."
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.3968253968253968,"4.2
Sampling Distribution Design in Exponential Family Bandits"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.4021164021164021,"In this subsection, we show how to choose a sampling distribution P that has a tight anti-concentration
bound to overcome the under-estimation of the optimal arm and concentration bound to overcome
the over-estimation of the suboptimal arms."
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.4074074074074074,"For the simplicity of notation, we denote P(µ, n) as the sampling distribution, where µ and n are
some input parameters. In particular, for ExpTS, we will choose µ = bµi(t) and n = Ti(t) for arm
i ∈[K] at round t. We deﬁne P(µ, n) as a distribution with PDF"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.4126984126984127,"f(x; µ, n) = 1/2|(nbn · kl(µ, x))′|e−nbn·kl(µ,x) = nbn · |x −µ|"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.41798941798941797,"2V (x)
e−nbn·kl(µ,x),
(4.3)"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.42328042328042326,"where (kl(µ, x))′ denotes the derivative of kl(µ, x) with respect to x, and bn is a function of n and
will be chosen later."
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.42857142857142855,"We assume the reward is supported in [Rmin, Rmax]. Note that Rmin = 0, and Rmax = 1 for
Bernoulli rewards, and Rmin = −∞, and Rmax = ∞for Gaussian rewards. Let p(x) and q(x) be
the density functions of two exponential family distributions with mean values µp and µq respectively.
By the deﬁnition in Section 3, we have kl(µp, µq) = KL(p(x), q(x)) =
R Rmax
Rmin p(x) log p(x)"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.43386243386243384,q(x)dx.
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.43915343915343913,"Proposition 4.1. If the mean reward of q(x) is equal to the maximum value in its support, i.e.,
µq = Rmax, we will have kl(µ, Rmax) = ∞for any µ < Rmax."
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.4444444444444444,"Proof. First consider the case that Rmax < ∞. Since the mean value concentrates on the maximum
value, we must have q(x) = 0 for all x < Rmax, which immediately implies kl(µ, Rmax) = ∞for
any µ < Rmax. For the case that Rmax = ∞, from (3.3) and the assumption that V < ∞, we also
have kl(µ, ∞) = (∞−µ)2/V = ∞."
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.4497354497354497,"Similarly, we can also prove that kl(µ, Rmin) = ∞for µ > Rmin. Based on these properties, we
can easily verify that a sample from the proposed sampling distribution θ ∼P has the following tail
bounds: for z ∈[µ, Rmax), it holds that"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.455026455026455,"P(θ ≥z) =
Z Rmax"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.4603174603174603,"z
f(x; µ, n)dx = −1/2e−nbn·kl(µ,x) Rmax"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.4656084656084656,"z
= 1/2e−nbn·kl(µ,z),
(4.4)"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.4708994708994709,"and for z ∈(Rmin, µ], it holds that"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.47619047619047616,"P(θ ≤z) =
Z z"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.48148148148148145,"Rmin
f(x; µ, n)dx = 1/2e−nbn·kl(µ,x) z"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.48677248677248675,"Rmin
= 1/2e−nbn·kl(µ,z).
(4.5)"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.49206349206349204,"Note that
R Rmax
Rmin f(x; µ, n)dx =
R µ
Rmin f(x; µ, n)dx +
R Rmax
µ
f(x; µ, n)dx = 1, which indicates the
PDF of P is well-deﬁned."
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.4973544973544973,"Intuition for the Design of the Sampling Distribution. The tail bounds in (4.4) and (4.5) provide
proper anti-concentration and concentration bounds for the sampling distribution P as long as we
have corresponding lower and upper bounds of e−nbn·kl(µ,z). When n is large, we will choose bn to
be close to 1, and thus (4.4) and (4.5) ensure that the sample of the corresponding arm concentrates
in the interval (µ −ϵ, µ + ϵ) with an exponentially small probability e−nkl(µ−ϵ,µ+ϵ), which is crucial
for achieving a tight ﬁnite-time regret."
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.5026455026455027,"How to Sample from P. We show that sampling from P is tractable when the CDF of P is invertible.
In particular, according to (4.4) and (4.5), the CDF of P(µ, n) is"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.5079365079365079,"F(x) =
1 −1/2e−nbn·kl(µ,x)
x ≥µ,
1/2e−nbn·kl(µ,x)
x ≤µ."
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.5132275132275133,"Table 2: Comparisons of different algorithms on K-armed bandits over a horizon T. For any
algorithm, Asym. Opt is the indicator whether it is asymptotically optimal, minimax ratio is the
scaling of its worst-case regret w.r.t. the minimax optimal regret O(
√"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.5185185185185185,"V KT), sub-UCB indicates
whether it satisﬁes the sub-UCB criteria, and Anytime indicates whether it needs the knowledge of
the horizon length T in advance."
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.5238095238095238,"Algorithm
Reward Type
Asym.
Finite-Time Regret
Anytime
References
Opt
Minimax Ratio
Sub-UCB"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.5291005291005291,"MOSS
[0, 1]
no
1
no
no
[6]
Anytime MOSS
[0, 1]
no
1
no
yes
[14]
KL-UCB++
Exponential Family
yes
1
no
no
[32]
OCUCB
sub-Gaussian
no
√log log T
yes
yes
[28]
AdaUCB
Gaussian
yes
1
yes
no
[29]
MS
sub-Gaussian
yes
√log K
yes
yes
[10]
ExpTS
Exponential Family
yes
√log K
yes
yes
This paper
ExpTS+
Exponential Family
yes
1
no
yes
This paper"
SAMPLING DISTRIBUTION DESIGN IN EXPONENTIAL FAMILY BANDITS,0.5343915343915344,"To sample from P(µ, n), we can ﬁrst pick y uniformly random from [0, 1]. Then, for y ≥1/2,
we solve the equation y = 1 −1/2e−nbn·kl(µ,x) for x (x ≥µ), which is equivalent to solving
log(1/(2(1 −y)))/(nbn) = kl(µ, x). For y ≤1/2, we solve the equation y = 1/2e−nbn·kl(µ,x) for
x (x ≤µ), which is equivalent to solving log(1/(2y))/(nbn) = kl(µ, x). If b(θ) is reversible and the
mapping θ 7→µ is given4, then according to (3.1), kl(µ, x) is also reversible for x. We can obtain an
exact sample from distribution P by solving kl(µ, x) = log(1/(2y))/(nbn). Alternatively, we can
also use approximate sampling methods such as Monte Carlo Markov Chain and Hastings-Metropolis
[25] or gradient based Langevin Monte Carlo [35] to obtain samples from the target distribution."
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.5396825396825397,"4.3
Regret Analysis of ExpTS for Exponential Family Rewards"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.544973544973545,"Now we present the regret bound of ExpTS for general exponential family bandits. The sampling
distribution used in Algorithm 1 is deﬁned in (4.3)."
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.5502645502645502,"Theorem 4.2. Let bn = (n −1)/n. Let P be the sampling distribution deﬁned in Section 4.2. There
exist universal constants C0, C1 > 0 such that the regret of Algorithm 1 satisﬁes"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.5555555555555556,"Rµ(T) ≤C0 
X"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.5608465608465608,"i∈[K]:∆i>λ
∆i + V log(T∆2
i /V )
∆i"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.5661375661375662,"
+
max
i∈[K],∆i≤λ ∆i · T,
(4.6)"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.5714285714285714,"Rµ(T) ≤C1  K
X"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.5767195767195767,"i=2
∆i +
p"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.582010582010582,"V KT log K

,
(4.7)"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.5873015873015873,"where λ ≥16
p"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.5925925925925926,"V/T, and also satisﬁes the following asymptotic bound simultaneously:"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.5978835978835979,"lim
T →∞
Rµ(T)"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.6031746031746031,"log T
= K
X i=2"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.6084656084656085,"∆i
kl(µi, µ1).
(4.8)"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.6137566137566137,"Remark 4.3. Similar to the argument by Auer and Ortner [7], we can see that the logarithm term
in (4.6) is the main term for suitable λ. For instance, if we choose λ = 16
p"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.6190476190476191,"V/T, we will have
maxi∈[K],∆i≤λ ∆iT ≤
√"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.6243386243386243,"V T, which is in the order of O(V/∆i) due to ∆i ≤λ. Thus it is obvious
to see that the regret in (4.6) satisﬁes the sub-UCB criteria."
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.6296296296296297,"It is worth highlighting that ExpTS is an anytime algorithm and simultaneously satisﬁes the sub-UCB
criteria in (1.2), the minimax optimal regret up to a factor √log K, and the asymptotically optimal
regret. ExpTS is also the ﬁrst Thompson sampling algorithm that provides ﬁnite-time regret bounds
for exponential family of rewards. Compared with state-of-the-art MAB algorithms listed in Table 2,
ExpTS is comparable to the best known UCB algorithms that work for exponential family of reward
distributions and no algorithms can dominate ExpTS. In particular, compared with MS [10] and"
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.6349206349206349,"4This is true for distributions such as Gaussian with known variance, exponential distribution, and Bernoulli."
REGRET ANALYSIS OF EXPTS FOR EXPONENTIAL FAMILY REWARDS,0.6402116402116402,"OCUCB [28], ExpTS is asymptotically optimal for exponential family of rewards, while MS is only
asymptotically optimal for sub-Gaussian rewards and OCUCB is not asymptotically optimal. We note
that Exponential Family does not cover the sub-Gaussian rewards. However, since we only use the
tail bound to approximate the reward distribution, ExpTS can also be extended to solve sub-Gaussian
reward bandits, which we leave as a future open direction."
SIMPLE VARIANTS FOR GAUSSIAN AND BERNOULLI REWARD DISTRIBUTIONS,0.6455026455026455,"4.4
Simple Variants for Gaussian and Bernoulli Reward Distributions"
SIMPLE VARIANTS FOR GAUSSIAN AND BERNOULLI REWARD DISTRIBUTIONS,0.6507936507936508,"The choice of P in (4.3) seems complicated for a general exponential family reward distribution,
even though we only need the sampling distribution to satisfy a nice tail bound derived from this
reward distribution. When the reward distribution has a closed form such as Gaussian and Bernoulli
distributions, we can replace P with the posterior in standard Thompson sampling and obtain the
asymptotic and ﬁnite-time regrets in the previous section.
Theorem 4.4. If the reward follows a Gaussian distribution with a known variance V , we can set
the sampling distribution in Algorithm 1 as N(bµi(t), V/Ti(t)). The resulting algorithm (denoted as
Gaussian-TS) enjoys the same regret bounds presented in Theorem 4.2.
Remark 4.5. Lemma 5 in Jin et al. [20] shows for Gaussian rewards, Gaussian-TS has a regret
bound at least Ω(√V KT log T) if the standard regret decomposition discussed in Section 4.1 is
adopted in the proof [5, 30, 20]. With our new regret decomposition and the lower conﬁdence bound
introduced in (4.2), we improve it to O(√V KT log K)."
SIMPLE VARIANTS FOR GAUSSIAN AND BERNOULLI REWARD DISTRIBUTIONS,0.656084656084656,"Jin et al. [20] also shows that their algorithms MOTS/MOTS-J can overcome the under-estimation
issue of (4.1). However, they are either at the cost of sacriﬁcing the asymptotic optimality or not
generalizable to exponential family bandits. In speciﬁc, (1) For Gaussian rewards, MOTS [20]
enlarges the variance of Gaussian posterior by a factor of 1/ρ, where ρ ∈(0, 1), which loses
the asymptotic optimality by a factor of 1/ρ resultantly. (2) For Gaussian rewards, MOTS-J
[20] introduces the Rayleigh posterior to overcome the under-estimation while maintaining the
asymptotic optimality. However, it is not clear whether the idea can be generalized to exponential
family rewards. Interestingly, their experimental results show that compared with Rayleigh posterior,
Gaussian posterior actually has a smaller regret empirically. Therefore, to use a Gaussian sampling
distribution, the new regret decomposition and the novel lower conﬁdence bound in our paper is a
better way to overcome the under-estimation issue of Gaussian-TS.
Theorem 4.6. If the reward distribution is Bernoulli, we can set the sampling distribution P in
Algorithm 1 as Beta posterior B(Si(t)+1, Ti(t)−Si(t)+1), where Si(t) is the number of successes
among the Ti(t) plays of arm i. We denote the resulting algorithm as Bernoulli-TS, which enjoys the
same regret bounds as in Theorem 4.2."
SIMPLE VARIANTS FOR GAUSSIAN AND BERNOULLI REWARD DISTRIBUTIONS,0.6613756613756614,"Agrawal and Goyal [5] proved that for Bernoulli rewards, Thompson sampling with Beta posterior
is asymptotically optimal and has a worst-case regret in the order of O(√KT log T). Our regret
analysis improves the worst-case regret to O(√KT log K). They also proved that Gaussian-TS
applied to the Bernoulli reward setting has a regret O(√KT log K). However, no asymptotic regret
was guaranteed in this setting."
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.6666666666666666,"5
Minimax Optimal Thompson Sampling for Exponential Family Rewards"
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.671957671957672,"In this section, in order to remove the extra logarithm term in the worst-case regret of ExpTS, we
introduce a new sampling distribution that adds a greedy exploration step to the sampling distribution
used in ExpTS. Speciﬁcally, the new algorithm ExpTS+ is the same as ExpTS but uses a new
sampling distribution P+(µ, n). A sample θ is generated from P+(µ, n) in the following way: θ = µ
with probability 1 −1/K and θ ∼P(µ, n) with probability 1/K."
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.6772486772486772,"Over-Estimation of Sub-Optimal Arms. We ﬁrst elaborate the over-estimation issue of sub-optimal
arms, which results in the extra √log K term in the worst-case regret of Thompson sampling. To
explain, suppose that the sample of each arm i has a probability p = P(θi(t) ≥θ1(t)) to become
larger than the sample of arm 1. Note that when this event happens, the algorithm chooses the wrong
arm and thus incurs a regret. Intuitively, the probability of making a mistake will be K −1 times
larger due to the union bound over K −1 sub-optimal arms, which leads to an additional √log K
factor in the worst-case regret. To reduce the probability P(θi(t) ≥θ1(t)), ExpTS+ adds a greedy
step that chooses the ExpTS sample with probability 1/K and chooses the arm with the largest"
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.6825396825396826,"empirical average reward with probability 1 −1/K. Then we can prove that for sufﬁciently large
s, with high probability we have ˆµis < θ1(t) and in this case it holds that P(θi(t) ≥θ1(t)) = p/K.
Thus the extra factor √log K in regret is removed."
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.6878306878306878,"In speciﬁc, we have the following theorem showing that ExpTS+ is asymptotically optimal and
minimax optimal simultaneously.
Theorem 5.1. Let bn = (n −1)/n. There exists a constant C1 > 0 such that ExpTS+ satisﬁes"
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.6931216931216931,"Rµ(T) ≤C1  K
X"
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.6984126984126984,"i=2
∆i +
√"
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.7037037037037037,"V KT

,
and
lim
T →∞
Rµ(T)"
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.708994708994709,"log T
= K
X i=2"
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.7142857142857143,"∆i
kl(µi, µ1)."
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.7195767195767195,"This is the ﬁrst time that the Thompson sampling algorithm achieves the minimax and asymptotically
optimal regret for exponential family of reward distributions. Moreover, ExpTS+ is also an anytime
algorithm since it does not need to know the horizon T in advance.
Remark 5.2 (Sub-Gaussian Rewards). In the proof of Theorem 5.1, we do not need the strict form of
the PDF of the empirical mean reward bµis, but only need the maximal inequality (Lemma H.1). This
means that the proof can be straightforwardly extended to sub-Gaussian reward distributions, where
similar maximal inequality holds [21]."
MINIMAX OPTIMAL THOMPSON SAMPLING FOR EXPONENTIAL FAMILY REWARDS,0.7248677248677249,"It is worth noting that MOTS proposed by [20] (Thompson sampling with a clipped Gaussian
posterior) also achieves the minimax optimal regret for sub-Gaussian rewards, but it can not keep
the asymptotic optimality simultaneously with the same algorithm parameters. In particular, to
achieve the minimax optimality, MOTS will have an additional 1/ρ factor in the asymptotic regret
with 0 < ρ < 1. Moreover, different from ExpTS+, MOTS is only designed for ﬁxed T setting and
thus is not an anytime algorithm.
Remark 5.3 (Gaussian and Bernoulli Rewards). Following the idea in Section 4.4, we can derive
new algorithms Gaussian-TS+ and Bernoulli-TS+ for Gaussian and Bernoulli rewards by replacing
the sampling distribution in ExpTS+. However, the posterior distribution does not fully satisfy the
properties shown in Section 4.2. In particular, the factor bn < 1 in Theorem 5.1 is an essential
requirement for the asymptotic analyses whereas the posterior distribution does not have this factor.
Due to these extra challenges, the proof techniques used for Theorem 5.1 can not be directly applied to
these two new algorithms, and it is interesting to further investigate whether they are simultaneously
minimax and asymptotically optimal."
CONCLUSIONS,0.7301587301587301,"6
Conclusions"
CONCLUSIONS,0.7354497354497355,"We studied Thompson sampling for exponential family of reward distributions. We proposed the
ExpTS algorithm and proved it satisﬁes the sub-UCB criteria for problem-dependent ﬁnite-time
regret, as well as achieves the asymptotic optimality and the minimax optimality up to a factor of
√log K for exponential family rewards. Furthermore, we proposed a variant of ExpTS, dubbed
ExpTS+, that adds a greedy exploration step to balance between the sample generated in ExpTS
and the empirical mean reward for each arm. We proved that ExpTS+ is simultaneously minimax
and asymptotically optimal. We also extended our proof techniques to standard Thompson sampling
with common posterior distributions and improved existing results. This work is mainly focused on
the theoretical optimality of Thompson sampling type algorithms. It would be an interesting future
direction to investigate the empirical performance of ExpTS and ExpTS+."
CONCLUSIONS,0.7407407407407407,Acknowledgement
CONCLUSIONS,0.746031746031746,"We thank the anonymous reviewers and the area chair for their helpful comments. This research is
supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG
Award No: AISG-PhD/2021-01-004[T]), by ASTAR, Singapore under Grant A19E3b0099, and by
Bren Named Chair Professorship at Caltech. In particular, T. Jin is supported by the National Research
Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG-PhD/2021-01-
004[T]). X. Xiao is supported by ASTAR, Singapore under Grant A19E3b0099. A. Anandkumar
is partially supported by Bren Named Chair Professorship at Caltech. The views and conclusions
contained in this paper are those of the authors and should not be interpreted as representing any
funding agencies."
REFERENCES,0.7513227513227513,References
REFERENCES,0.7566137566137566,"[1] Marc Abeille and Alessandro Lazaric. Linear thompson sampling revisited. In Artiﬁcial
Intelligence and Statistics, pages 176–184. PMLR, 2017. (p. 5.)"
REFERENCES,0.7619047619047619,"[2] Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions with formulas,
graphs, and mathematical tables, volume 55. US Government printing ofﬁce, 1964. (pp. 6
and 41.)"
REFERENCES,0.7671957671957672,"[3] Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit
problem. In Conference on learning theory, pages 39–1, 2012. (pp. 2, 4, and 6.)"
REFERENCES,0.7724867724867724,"[4] Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In
Artiﬁcial intelligence and statistics, pages 99–107, 2013. (pp. 2, 3, and 4.)"
REFERENCES,0.7777777777777778,"[5] Shipra Agrawal and Navin Goyal. Near-optimal regret bounds for thompson sampling. Journal
of the ACM (JACM), 64(5):30, 2017. (pp. 1, 2, 3, 4, 6, 9, 27, and 29.)"
REFERENCES,0.783068783068783,"[6] Jean-Yves Audibert and Sébastien Bubeck. Minimax policies for adversarial and stochastic
bandits. In COLT, pages 217–226, 2009. (pp. 2, 3, 4, and 8.)"
REFERENCES,0.7883597883597884,"[7] Peter Auer and Ronald Ortner. Ucb revisited: Improved regret bounds for the stochastic
multi-armed bandit problem. Periodica Mathematica Hungarica, 61(1-2):55–65, 2010. (p. 8.)"
REFERENCES,0.7936507936507936,"[8] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed
bandit problem. Machine learning, 47(2-3):235–256, 2002. (pp. 1 and 2.)"
REFERENCES,0.798941798941799,"[9] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic
multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. (p. 1.)"
REFERENCES,0.8042328042328042,"[10] Jie Bian and Kwang-Sung Jun. Maillard sampling: Boltzmann exploration done optimally.
arXiv preprint arXiv:2111.03290, 2021. (pp. 2, 4, and 8.)"
REFERENCES,0.8095238095238095,"[11] Sébastien Bubeck and Che-Yu Liu. Prior-free and prior-dependent regret bounds for thompson
sampling. In Advances in Neural Information Processing Systems, pages 638–646, 2013. (pp. 4
and 5.)"
REFERENCES,0.8148148148148148,"[12] Olivier Cappé, Aurélien Garivier, Odalric-Ambrym Maillard, Rémi Munos, and Gilles Stoltz.
Kullback-leibler upper conﬁdence bounds for optimal sequential allocation. The Annals of
Statistics, pages 1516–1541, 2013. (pp. 4 and 5.)"
REFERENCES,0.8201058201058201,"[13] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances
in neural information processing systems, pages 2249–2257, 2011. (p. 2.)"
REFERENCES,0.8253968253968254,"[14] Rémy Degenne and Vianney Perchet. Anytime optimal algorithms in stochastic multi-armed
bandits. In International Conference on Machine Learning, pages 1587–1595. PMLR, 2016.
(pp. 4 and 8.)"
REFERENCES,0.8306878306878307,"[15] Aurélien Garivier and Olivier Cappé. The kl-ucb algorithm for bounded stochastic bandits and
beyond. In Proceedings of the 24th annual conference on learning theory, pages 359–376, 2011.
(pp. 4 and 5.)"
REFERENCES,0.8359788359788359,"[16] Aurélien Garivier, Hédi Hadiji, Pierre Menard, and Gilles Stoltz. Kl-ucb-switch: optimal
regret bounds for stochastic bandits from both a distribution-dependent and a distribution-free
viewpoints. arXiv preprint arXiv:1805.05071, 2018. (pp. 1 and 2.)"
REFERENCES,0.8412698412698413,"[17] Peter Harremoës.
Bounds on tail probabilities in exponential families.
arXiv preprint
arXiv:1601.05179, 2016. (pp. 4 and 5.)"
REFERENCES,0.8465608465608465,"[18] Emil Jeˇrábek. Dual weak pigeonhole principle, boolean complexity, and derandomization.
Annals of Pure and Applied Logic, 129(1-3):1–37, 2004. (pp. 6 and 27.)"
REFERENCES,0.8518518518518519,"[19] Tianyuan Jin, Jing Tang, Pan Xu, Keke Huang, Xiaokui Xiao, and Quanquan Gu. Almost
optimal anytime algorithm for batched multi-armed bandits. In International Conference on
Machine Learning, pages 5065–5073. PMLR, 2021. (p. 5.)"
REFERENCES,0.8571428571428571,"[20] Tianyuan Jin, Pan Xu, Jieming Shi, Xiaokui Xiao, and Quanquan Gu. MOTS: Minimax Optimal
Thompson Sampling. In International Conference on Machine Learning, pages 5074–5083.
PMLR, 2021. (pp. 2, 3, 4, 5, 6, 9, and 10.)"
REFERENCES,0.8624338624338624,"[21] Tianyuan Jin, Pan Xu, Xiaokui Xiao, and Quanquan Gu. Double explore-then-commit: Asymp-
totic optimality and beyond. In Conference on Learning Theory, pages 2584–2633. PMLR,
2021. (pp. 4 and 10.)"
REFERENCES,0.8677248677248677,"[22] Emilie Kaufmann. On bayesian index policies for sequential resource allocation. arXiv preprint
arXiv:1601.01190, 2016. (pp. 2 and 4.)"
REFERENCES,0.873015873015873,"[23] Emilie Kaufmann, Nathaniel Korda, and Rémi Munos. Thompson sampling: An asymptotically
optimal ﬁnite-time analysis. In International conference on algorithmic learning theory, pages
199–213. Springer, 2012. (p. 4.)"
REFERENCES,0.8783068783068783,"[24] Wonyoung Kim, Gi-soo Kim, and Myunghee Cho Paik. Doubly robust thompson sampling
with linear payoffs. Advances in Neural Information Processing Systems, 34, 2021. (p. 5.)"
REFERENCES,0.8835978835978836,"[25] Nathaniel Korda, Emilie Kaufmann, and Remi Munos. Thompson sampling for 1-dimensional
exponential family bandits. In Advances in neural information processing systems, pages
1448–1456, 2013. (pp. 2, 3, 4, 5, and 8.)"
REFERENCES,0.8888888888888888,"[26] Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Ad-
vances in applied mathematics, 6(1):4–22, 1985. (pp. 2, 3, and 5.)"
REFERENCES,0.8941798941798942,"[27] Tor Lattimore. Optimally conﬁdent ucb: Improved regret for ﬁnite-armed bandits. arXiv
preprint arXiv:1507.07880, 2015. (p. 4.)"
REFERENCES,0.8994708994708994,"[28] Tor Lattimore. Regret analysis of the ﬁnite-horizon gittins index strategy for multi-armed
bandits. In Conference on Learning Theory, pages 1214–1245, 2016. (pp. 4, 8, and 9.)"
REFERENCES,0.9047619047619048,"[29] Tor Lattimore. Reﬁning the conﬁdence level for optimistic bandit strategies. The Journal of
Machine Learning Research, 19(1):765–796, 2018. (pp. 1, 2, 4, and 8.)"
REFERENCES,0.91005291005291,"[30] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
(pp. 3, 9, and 16.)"
REFERENCES,0.9153439153439153,"[31] Odalric-Ambrym Maillard, Rémi Munos, and Gilles Stoltz. A ﬁnite-time analysis of multi-
armed bandits problems with kullback-leibler divergences. In Proceedings of the 24th annual
Conference On Learning Theory, pages 497–514, 2011. (p. 4.)"
REFERENCES,0.9206349206349206,"[32] Pierre Ménard and Aurélien Garivier. A minimax and asymptotically optimal algorithm for
stochastic bandits. In International Conference on Algorithmic Learning Theory, pages 223–237,
2017. (pp. 2, 4, 5, 8, and 41.)"
REFERENCES,0.9259259259259259,"[33] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics
of Operations Research, 39(4):1221–1243, 2014. (pp. 4 and 5.)"
REFERENCES,0.9312169312169312,"[34] Siwei Wang and Wei Chen. Thompson sampling for combinatorial semi-bandits. In International
Conference on Machine Learning, pages 5114–5122, 2018. (p. 2.)"
REFERENCES,0.9365079365079365,"[35] Pan Xu, Hongkai Zheng, Eric V Mazumdar, Kamyar Azizzadenesheli, and Animashree Anand-
kumar. Langevin monte carlo for contextual bandits. In International Conference on Machine
Learning, pages 24830–24850. PMLR, 2022. (p. 8.)"
REFERENCES,0.9417989417989417,"[36] Tong Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning.
arXiv preprint arXiv:2110.00871, 2021. (p. 5.)"
REFERENCES,0.9470899470899471,Checklist
REFERENCES,0.9523809523809523,1. For all authors...
REFERENCES,0.9576719576719577,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes]"
REFERENCES,0.9629629629629629,"(c) Did you discuss any potential negative societal impacts of your work? [N/A]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results..."
REFERENCES,0.9682539682539683,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
(b) Did you include complete proofs of all theoretical results? [Yes] All the proofs are
provided in the appendix.
3. If you ran experiments..."
REFERENCES,0.9735449735449735,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [N/A]
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [N/A]
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [N/A]
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [N/A]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9788359788359788,"(a) If your work uses existing assets, did you cite the creators? [N/A]
(b) Did you mention the license of the assets? [N/A]"
REFERENCES,0.9841269841269841,(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
REFERENCES,0.9894179894179894,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9947089947089947,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
