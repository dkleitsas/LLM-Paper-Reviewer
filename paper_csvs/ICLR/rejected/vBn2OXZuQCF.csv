Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0045045045045045045,"Pre-training on massive unlabeled datasets greatly improves accuracy under
distribution shifts. As a ﬁrst step toward understanding this, we study a popular
pre-training method, contrastive learning, in the unsupervised domain adaptation
(UDA) setting where we only have labeled data from a source domain and unlabeled
data from a target domain. We begin by showing on 4 benchmark vision datasets that
out-of-the-box contrastive pre-training (even without large-scale unlabeled data) is
competitive with other UDA methods. Intuitions from classical UDA methods such
as domain adversarial training focus on bringing the domains together in feature
space to improve generalization from source to target. Surprisingly, we ﬁnd that
contrastive pre-training learns features that are very far apart between the source
and target domains. How then does contrastive learning improve robustness to
distribution shift? We develop a conceptual model for contrastive learning under
domain shifts, where data augmentations form connections between classes and
domains that can be far apart. We propose a new measure of connectivity —the rel-
ative connection strengths between same and different classes across domains—that
governs the success of contrastive pre-training for domain adaptation in a simple
example and strongly correlates with our results on benchmark vision datasets."
INTRODUCTION,0.009009009009009009,"1
INTRODUCTION"
INTRODUCTION,0.013513513513513514,"In applications such as image recognition for self-driving cars (Yu et al., 2020; Sun et al., 2020) or
medical image diagnosis (AlBadawy et al., 2018; Dai & Gool, 2018), machine learning models often
fail on examples drawn from a different distribution than training. Pre-training on large-scale unlabeled
data (Chen et al., 2020; He et al., 2020; Radford et al., 2021) can substantially boost accuracy under
distribution shift (Radford et al., 2021; Hendrycks et al., 2019; Fisch et al., 2019; Yogatama et al., 2019;
Devlin et al., 2019). For example, CLIP (Radford et al., 2021), which is trained on internet-scale image
and text data, has shown impressive robustness beneﬁts on ImageNet variants such as ImageNetV2.
Similarly, pre-training methods such as BERT (Devlin et al., 2019) have improved robustness in NLP
— for example, multilingual pre-training signiﬁcantly improves performance on unseen languages (Liu
et al., 2020). While other robustness methods that use unlabeled data such as self-training (Prabhu
et al., 2021; Sohn et al., 2020; Berthelot et al., 2021) and domain adversarial training (Shu et al., 2018;
Ganin et al., 2016) jointly optimize a labeled and unlabeled objective, pre-training is particularly
suitable for large unlabeled datasets since we can generically pre-train once on unlabeled data and
then specialize the model to downstream labeled data via ﬁne-tuning. How does pre-training improve
robustness to distribution shift when the pre-training method is not tailored to any downstream task?"
INTRODUCTION,0.018018018018018018,"In this paper, we run controlled experiments to understand the robustness beneﬁts of a popular
pre-training method, contrastive pre-training (Chen et al., 2020; He et al., 2020; Caron et al., 2020),
even without large-scale unlabeled data. We directly apply contrastive pre-training to unsupervised
visual domain adaptation (UDA), where unlabeled data from a shifted target domain is used to adapt
models trained on labeled source dataset. We compare contrastive pre-training (SwAV (Caron et al.,
2020)) to UDA methods such as SENTRY (Prabhu et al., 2021) (self-training) and DIRT-T (Shu et al.,
2018) (domain adversarial training) where all methods use the same unlabeled data (source and target).
We ﬁnd that SwAV achieves comparable or better results on the DomainNet, BREEDS Living-17,
and BREEDS Entity-30 benchmarks. Furthermore, we show that pre-training once on unlabeled data
from many domains further improves the average target accuracy over the consituent source-target
pairs, showing that extra unlabeled data improves contrastive pre-training for domain adaptation."
INTRODUCTION,0.02252252252252252,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02702702702702703,"Figure 1: (Left) Edges between (class, domain) pairs indicate how “connected” they are by
augmentations (e.g., cropping, colorization). Small crop sizes in contrastive learning can increase the
connectivity between disparate domains. For example, α denotes the probability that augmentations
connect examples of the same class across different domains. (Right) When α and β, which measures
the connectivity between images of the same class or same domain respectively, are larger than γ
(different domain and different class connecivity), contrastive pre-training produces a feature space
where training on labeled data from the source domain (Real — green, ﬁlled) achieves high accuracy
on the target domain (Sketch — gray, hollow)."
INTRODUCTION,0.03153153153153153,"Given the strong performance of contrastive pre-training, we seek to understand why contrastive
pre-training works for domain adaptation. Domain adversarial training and self-supervision algorithms
are commonly based on the intuition that domains get merged in feature space (Tzeng et al., 2014;
Ganin et al., 2016; Tzeng et al., 2017; Sun et al., 2019; Ben-David et al., 2010; Shu et al., 2018; Wang
et al., 2021; Thota & Leontidis, 2021). This is support by theoretical notions such as H∆H-divergence,
where domains that can be discriminated well and results in worse generalization bounds for target
accuracy (Ben-David et al., 2010). A natural hypothesis is that contrastive pre-training brings domains
together by bringing the representations of strong augmentations of the same image together (Sun
et al., 2019)."
INTRODUCTION,0.036036036036036036,"Surprisingly, we ﬁnd that the source and target domains are still far apart in the feature space learned
by contrastive pre-training. A domain classiﬁer trained on the pre-trained representations of strongly
augmented images from the sketch and painting domains in DomainNet has 94% accuracy, which
is even higher than the accuracy of a domain classiﬁer on the original (augmented) input space (75%)."
INTRODUCTION,0.04054054054054054,"We study this phenomenon from the lens of the recently proposed augmentation graph (HaoChen
et al., 2021), which forms connections between examples if they have similar augmentations (e.g.,
a small crop + grayscale can connect sketch and real photos, Figure 1). These connections are central
to the contrastive objective, which seeks to bring neighbors in the graph closer together while pushing
apart non-neighbors. From both simulations and real datasets, we deﬁne a critical connectivity quantity
which seems to govern whether domain adaptation will be successful (Figure 1). In particular, the
connection strength between the same classes across different domains should be relatively greater
than the connection between different classes across different domains. Importantly, the absolute
connectivity between domains can be small and the domains can be far apart in input space. This
suggests that contrastive learning circumvents the separation of domains through the connectivity
structure of the (general-purpose) augmentations used."
INTRODUCTION,0.04504504504504504,"Using our understanding, we ablate contrastive learning by removing subsets of unlabeled data that
contribute most to connecting the two domains. We do this by training a domain classiﬁer to distinguish
between the source and target and removing examples which are most uncertain under the classiﬁer.
This consistently worsens the target accuracy in comparison to randomly removing the same number of
data points (by 2–3% on DomainNet), showing the importance of domain connectivity for adaptation."
INTRODUCTION,0.04954954954954955,Under review as a conference paper at ICLR 2022
SETUP,0.05405405405405406,"2
SETUP"
SETUP,0.05855855855855856,We consider a classiﬁcation problem from an input space X ⊆Rd to a label space Y =[K].
SETUP,0.06306306306306306,"Data.
Let PS and PT denote the source and target distributions of (¯x,y) pairs, respectively, where
¯x ∈X and y ∈Y. The labeled source dataset S consists of nS input-output pairs from PS. We also
have access to an unlabeled target dataset T with nT unlabeled inputs from PT , and we assume the
label sets for the source and target are identical. In some cases, we also consider an additional related
dataset R with nR unlabeled inputs from a related distribution R."
SETUP,0.06756756756756757,"Model and metrics.
The goal is to learn a classiﬁer h:X →Y with low classiﬁcation error on the
target distribution L0−1(h)=E¯x,y∼PT [1[h(¯x)̸=y]]. The pre-training algorithms we consider consist
of two steps: ﬁrst, we train an encoder function f :X →Z into a representation space Z ⊆Rd′ (where
d′ denotes the representation dimension), then we train a classiﬁcation head g : Z →Y. The ﬁnal
classiﬁer h=g◦f composes the encoder with the classiﬁcation head."
CONTRASTIVE LEARNING,0.07207207207207207,"2.1
CONTRASTIVE LEARNING
Contrastive learning (He et al., 2020; Chen et al., 2020; Caron et al., 2020) trains representations to
be similar between augmented views of the same example and dissimilar between augmented views of
random examples. Example image augmentations include random cropping, color jitter, and Gaussian
blur (Chen et al., 2020). The basic assumption is that the semantic content of the data is invariant under
the augmentations."
CONTRASTIVE LEARNING,0.07657657657657657,"Though there are several variants of contrastive learning, we focus on the following objective
from HaoChen et al. (2021) for our conceptual understanding. Let X denote the “augmented” space
containing all data-augmented views of natural examples (e.g., all random crops of natural ImageNet
images). Data augmentation is deﬁned as a random function A:X →X, where P[A(x)=x] denotes
the probability that x∈X augments to x∈X. A pair of augmented views is called a positive pair and
denoted x,x+ if a natural point x is ﬁrst drawn and then x,x+ are the results of two calls to A(x). On the
other hand, a negative pair x,x−arises from two natural data points x,x′ when x=A(x), x−=A(x′).
Minimizing the following loss function simultaneously brings together the representations of positive
pairs x,x+ and repels the representations of negative pairs x,x−:"
CONTRASTIVE LEARNING,0.08108108108108109,"L(f)=−2·Ex,x+
f(x)⊤f(x+)

+Ex,x−
h 
f(x)⊤f(x−)
2i
.
(1)"
CONTRASTIVE LEARNING,0.08558558558558559,Here the ﬁrst expectations are taken over the marginal distribution over X and the randomness of A.
CONSTRASTIVE PRE-TRAINING FOR DOMAIN ADAPTATION,0.09009009009009009,"2.2
CONSTRASTIVE PRE-TRAINING FOR DOMAIN ADAPTATION
Let D denote the unlabeled input data used for contrastive pre-training (e.g., some combination of
the source unlabeled data S, target unlabeled data T, and/or additional unlabeled data R). We use
the following simple algorithm to apply contrastive pre-training to domain adaptation:"
CONSTRASTIVE PRE-TRAINING FOR DOMAIN ADAPTATION,0.0945945945945946,1. (Pre-train) Learn an encoder function f :X →Z via contrastive learning on D.
CONSTRASTIVE PRE-TRAINING FOR DOMAIN ADAPTATION,0.0990990990990991,"2. (Fine-tune) On labeled source data S, train a classiﬁer h≜g◦f :X →Y as the composition
of the pre-trained encoder f and a classiﬁer head g. In this paper, we take “ﬁne-tuning” to
mean updating both the parameters of f and g."
CONTRASTIVE PRE-TRAINING IS A STRONG DOMAIN ADAPTATION METHOD,0.1036036036036036,"3
CONTRASTIVE PRE-TRAINING IS A STRONG DOMAIN ADAPTATION METHOD"
CONTRASTIVE PRE-TRAINING IS A STRONG DOMAIN ADAPTATION METHOD,0.10810810810810811,"We evaluate contrastive learning and relevant baselines on 4 benchmark vision datasets and observe
that contrastive learning achieves comparable or better performance in all cases."
DATASETS,0.11261261261261261,"3.1
DATASETS
BREEDS (Santurkar et al., 2020).
BREEDS is a subpopulation shift benchmark derived from
ImageNet by constructing a hierarchical tree structure of classes from WordNet. Nodes at a speciﬁed
depth of the tree become the labels for the classiﬁcation task, and descendant nodes are treated as
subpopulations that can be randomly partitioned into source and target domains."
DATASETS,0.11711711711711711,"DomainNet (Peng et al., 2019).
DomainNet is a large unsupervised domain adaptation task,
consisting of approximately 600,000 images and 345 classes in 6 domains. For our experiments we
utilize the same ﬁltered version of DomainNet from Prabhu et al. (2021), which uses 40 of the 345
classes and the sketch, painting, photograph, and clipart domains."
DATASETS,0.12162162162162163,Under review as a conference paper at ICLR 2022
DATASETS,0.12612612612612611,"Source
Real
Sketch
Painting
Clipart
Avg."
DATASETS,0.13063063063063063,"Target
Sketch
Painting
Clipart
Real
Painting
Clipart
Real
Sketch
Clipart
Real
Sketch
Painting"
DATASETS,0.13513513513513514,"ERM
25.67
37.09
44.80
25.72
18.59
31.87
29.13
14.25
20.48
19.34
12.71
10.79
24.20
ERM (SA)
42.76
47.26
48.26
26.74
18.56
37.56
40.32
32.01
29.82
26.89
22.13
14.30
32.22
SENTRY
41.18
51.66
59.65
56.73
42.55
55.01
30.13
13.33
20.97
10.52
5.33
6.60
32.81
DANN
44.97
48.88
57.36
46.26
39.60
52.28
47.77
32.22
35.64
32.98
25.46
15.67
39.92
DANN (SA)
52.52
50.91
57.42
45.32
37.02
56.74
48.46
49.39
40.22
33.97
32.34
23.58
43.99
SwAV
43.76
54.55
53.27
55.48
34.99
40.59
67.08
39.64
32.98
57.13
34.30
25.09
44.91"
DATASETS,0.13963963963963963,"SwAV+
44.64
57.27
54.20
58.10
46.75
53.46
69.03
48.68
41.33
59.38
46.22
41.66
51.73"
DATASETS,0.14414414414414414,"ERM
SENTRY
DANN
SwAV (S)
SwAV (T)
SwAV (S+T)
SwAV+"
DATASETS,0.14864864864864866,"Living-17
59.00
68.76
66.88
61.89
68.53
73.59
81.82
Entity-30
50.97
62.80
54.62
52.33
60.33
62.03
65.90"
DATASETS,0.15315315315315314,"ERM
Dirt-T
SimCLR (T)"
DATASETS,0.15765765765765766,"63.6
75.3
76.1"
DATASETS,0.16216216216216217,"Table 1: (Top) Test accuracy (%) of ERM with standard and SwAV augmentations, SENTRY, DANN
with standard and SwAV augmentations, SwAV, and SwAV + extra (abbreviated to SwAV+) on all
domain pairs of DomainNet. Here, SwAV is run on source + target, and SwAV+ is separated from
the other algorithms as it uses additional data and is therefore not directly comparable. (Middle)
Test accuracy (%) of ERM, SENTRY, DANN, SwAV (with various splits), and SwAV+ on BREEDS.
(Bottom) Test accuracy (%) of ERM, Dirt-T, and SimCLR on STL-10 →CIFAR-10. ERM and Dirt-T
numbers are as reported from Shu et al. (2018)."
DATASETS,0.16666666666666666,"STL-10 →CIFAR-10.
STL-10 and CIFAR-10 are two classic image classiﬁcation datasets, each
consisting of 10 classes (Coates et al., 2011; Krizhevsky, 2009). Each dataset has a class label that
is not present in the other, and hence we follow the procedure of French et al. (2018) and ﬁlter out
the examples in the non-overlapping classes, resulting in a 9-class classiﬁcation problem."
METHODS,0.17117117117117117,"3.2
METHODS"
METHODS,0.17567567567567569,"Baselines.
We run empirical risk minimization (ERM) on the source dataset for each task, i.e., a
“source-only” baseline. In addition, for BREEDS and DomainNet we use these ERM models as weight
initialization for SENTRY, a self-training algorithm for domain adaptation that achieved state of the art
results on several domains of DomainNet when initialized with ImageNet-pretrained models (Prabhu
et al., 2021). For STL →CIFAR, we use Dirt-T (Shu et al., 2018), the current state of the art algorithm
for this task."
METHODS,0.18018018018018017,"Contrastive pre-training.
For BREEDS and DomainNet we use SwAV (Caron et al., 2020), a
contrastive learning algorithm that achieved state-of-the-art linear evaluation accuracy on ImageNet.
SwAV contrasts examples in feature space rather than input space and uses a novel multi-crop
data augmentation strategy using several crops of different sizes. For the lower-resolution task
STL →CIFAR we use SimCLR (Chen et al., 2020), which uses only 2 crops and therefore scales
straightforwardly to smaller input dimensions."
RESULTS,0.18468468468468469,"3.3
RESULTS"
RESULTS,0.1891891891891892,"Main comparison.
For all 4 tasks, contrastive pre-training is competitive when compared to ERM
and SOTA unsupervised domain adaptation baselines. On DomainNet (top of Table 1), contrastive
pre-training on source and target unlabeled data achieves higher target accuracy than all joint-training
baselines both on 5 of 12 domain pairs and on average over all pairs (44.91% vs. the second best,
43.99%). On Living-17 (middle of Table 1), contrastive learning on source and target unlabeled data
improves over SENTRY over 4%, while on Entity-30, contrastive learning (source + target) is within
0.3% of the target accuracy of SENTRY. On STL→CIFAR, SimCLR on the target unlabeled data
achieves 0.8% higher target accuracy than Dirt-T."
RESULTS,0.19369369369369369,"Extra unlabeled data from related domains.
We also consider adding extra unlabeled data from
other (related) domains in DomainNet, Living-17, and Entity-30. In particular, we can pre-train once
on an unlabeled dataset that consists of a superset of the domains we want to adapt to, and ﬁne-tune
on the available source data. While this is not a fair comparison to other domain adaptation methods,
the ability to scale to large unlabeled datasets is a natural advantage of pre-training."
RESULTS,0.1981981981981982,Under review as a conference paper at ICLR 2022
RESULTS,0.20270270270270271,"Feature space
Different class,
Different class,
Same class,
Different class,
learned by
same domain, source
same domain, target
different domain
different domain"
RESULTS,0.2072072072072072,"Living-17
Input space
20.94
21.91
36.58
20.00
SwAV
1.85
2.26
12.67
1.26"
RESULTS,0.21171171171171171,"DomainNet
Input space
32.88
–
27.36
25.12
ERM
15.16
16.85
18.34
10.69
DANN
10.59
11.96
16.59
7.30
SwAV
7.03
–
7.54
2.46"
RESULTS,0.21621621621621623,"Table 2: Average separation error of Living-17 and DomainNet class-domain pairs. On both datasets,
the classes become very distinguishable in the feature space learned by contrastive learning and on
DomainNet, the domains additionally remain far apart in the pre-trained feature space. Classiﬁers were
trained on augmented images to distinguish class-domain pairs as a proxy for connectivity (higher
separation errors suggest greater connectivity)."
RESULTS,0.22072072072072071,"On DomainNet, we consider contrastive pre-training with SwAV on unlabeled data from all the
domains at once, which we denote as SwAV + extra. The top Table 1 shows that adding extra unlabeled
data improves the target accuracy of contrastive pre-training on all pairs and improves the average
by almost 6% (44.91% to 50.75%). On Living-17 and Entity-30 (middle of Table 1), we additionally
consider contrastive pre-training with SwAV with 1) only source, 2) only target, and 3) with all of
ImageNet as the unlabeled data. We ﬁnd that increasing the amount of unlabeled data steadily increases
the target accuracy, and pre-training on all of ImageNet improves over source + target by 8% and 4%
respectively for Living-17 and Entity-30."
RESULTS,0.22522522522522523,"Ease of hyperparmeter tuning.
The SENTRY self-training baseline requires extensive hyper-
parameter tuning (e.g., ﬁnding optimal balancing coefﬁcients for the joint losses, or learning rate
schedules) for every transfer task and is difﬁcult with access only to source labels. In contrast to the
baseline methods, for contrastive pre-training we did not need to sweep over hyperparameters or use
target labels for model selection. We used nearly identical hyperparmeters for both BREEDS datasets
(Living-17 and Entity-30) and all DomainNet pairs (details in Appendix A)."
RESULTS,0.22972972972972974,"No explicit knowledge of distribution shift.
We note that SENTRY uses explicit knowledge of
label imbalance within its algorithm, on top of being designed for the domain adaptation task of
adapting from source to target. In contrast, we used off-the-shelf contrastive pre-training, which was
designed for general representation learning, with no additional information about distribution shift."
CONNECTIVITY MODEL FOR DOMAIN ADAPTATION,0.23423423423423423,"4
CONNECTIVITY MODEL FOR DOMAIN ADAPTATION"
CONNECTIVITY MODEL FOR DOMAIN ADAPTATION,0.23873873873873874,"How does contrastive pre-training improve target accuracy with unlabeled data? Conventional
intuitions and theory for domain adaptation are based on reducing the distance between the source
and target domains (Ben-David et al., 2010; Ganin et al., 2016).However, we see empirically that
contrastive pre-training keep the domains very separated in representation space. Towards building
an understanding for how contrastive pre-training connects domains without merging them, we extend
a conceptual framework based on connectivity via data augmentations (HaoChen et al., 2021) for
distribution shifts. We deﬁne a crucial measure of domain connectivity that relates properties of the
data distribution and contrastive augmentations to the success of contrastive learning for domain shifts
on a simulated data model, DomainNet, Living-17, and Entity-30."
CONTRASTIVE PRE-TRAINING KEEPS DOMAINS APART,0.24324324324324326,"4.1
CONTRASTIVE PRE-TRAINING KEEPS DOMAINS APART
Contrary to common intuitions for domain adaptation methods, we ﬁnd that contrastive learning does
not bring the domains together in feature space. Table 2 shows the average test error of classiﬁers
trained to discriminate between examples from the same class but different domains on DomainNet.
We train and test the classiﬁers on images using the same augmentations as SwAV and ﬁnd that
contrastive pre-training learns features that are actually more discriminable between domains than
in the original input space (7.5% vs. 27.3% error on average over all pairs). These estimates for the
baselines and for individual pairs are provided in table 3."
CONTRASTIVE PRE-TRAINING KEEPS DOMAINS APART,0.24774774774774774,"Next, we measure the separability in feature space and input space between classes within a single
domain (e.g., 3% vs. 24% error in the real domain, table 3). We note that the separability between
domains in feature space is comparable to the separability between classes, suggesting that contrastive
learning may cluster both domains and classes in separate axes."
CONTRASTIVE PRE-TRAINING KEEPS DOMAINS APART,0.25225225225225223,Under review as a conference paper at ICLR 2022
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.25675675675675674,"4.2
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.26126126126126126,"In this section, we set up our conceptual model of connectivity used to study contrastive learning."
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.26576576576576577,"Connectivity via augmentations in domain adaptation.
HaoChen et al. (2021) proposed viewing
contrastive learning as operating on an augmentation graph. The augmented inputs from X constitute
the vertices and the edge weight between x,x′ represents how likely they are to be generated from the
same natural example (i.e., how likely they are to be chosen as a positive example during contrastive
pre-training). HaoChen et al. (2021) show that when examples from different classes x,x′ ∈X are
sparsely connected via the augmentation graph (i.e., rarely augment into the same augmented example),
and the connectivity within a class is strong, the contrastive pre-training leads to features that obtain
good accuracy on the downstream task."
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.2702702702702703,"Extension to domain adaptation.
Motivated by the augmentation graph framework, we deﬁne
a notion of connectivity for domain adaptation on a graph where class-domain pairs are vertices. We
denote the set of examples belonging to class c and domain d (which we refer to as the class-domain
pair (c,d)) as P(c,d). We formalize this notion in the following deﬁnition:"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.2747747747747748,"Deﬁnition 1 (Connectivity). The connectivity between two class-domain pairs (i,d) and (j,d′) is"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.27927927927927926,"conn((i,d),(j,d′))≜P[A(x)=A(x′)],"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.28378378378378377,"where x∼P(i,d) and x′ ∼P(j,d′)."
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.2882882882882883,"Intuitively, the connectivity between two class-domain pairs is the probability that two randomly
sampled points from each class-domain pair are augmented to the same point."
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.2927927927927928,"Deﬁnition 1 is deﬁned for two arbitrary class-domain pairs, which then allows us to build further
deﬁnitions of connectivity involving multiple class-domain pairs. Such connectivity deﬁnitions would
allow us to more rigorously describe (for instance) our observation in the DomainNet dataset that
points of the same class but different domains are more discriminable than points in different classes
within the same domain. We deﬁne these instances of connectivity using Deﬁnition 1 below:"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.2972972972972973,"Deﬁnition 2 ((α,β,γ)9connectivity). A source-target pair of distributions S and T over X (with
domain labels dS, dT , respectively) satisﬁes (α,β,γ)9connectivity if"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.30180180180180183,"1. Ei∈[K][conn((i,dS),(i,dT ))]=α. (same-class-different-domain)"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.3063063063063063,"2. Ei,j∈[K][conn((i,d),(j,d))]=β. (different-class-same-domain, d∈{dS,dT } )"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.3108108108108108,"3. Ei,j∈[K][conn((i,dS),(j,dT ))]=γ. (different-class-different-domain)"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.3153153153153153,"4.3
ILLUSTRATIVE TOY EXAMPLE: BINARY CLASSIFICATION"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.31981981981981983,"As an illustrative example, consider the simple case of binary classiﬁcation where each class-domain
pair consists of a single data point. This toy setting pinpoints a simple connectivity condition such
that linear classiﬁcation on the pre-trained features results in good target accuracy."
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.32432432432432434,"Setting (Figure 2).
Assume a uniform marginal distribution over the natural inputs, and suppose
data augmentation transforms each data point directly into another with a certain probability, so that
the entire augmentation graph consists of the 4 points (2 classes, 2 domains) each identiﬁed by their
class and domain. Formally, the example (i,d) (of class i and domain d) has augmentation given by"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.32882882882882886,"A((i,d))="
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.3333333333333333,"


 

"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.33783783783783783,"(i,d)
with probability ζ
(i,d′)
with probability (α/2ζ)
(j,d)
with probability (β/2ζ)
(j,d′)
with probability (γ/2ζ)"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.34234234234234234,"where j ̸=i, d′ ̸=d. We also assume ζ >max{α,β,γ} (i.e., data augmentation is most likely to map
each data point to itself) and appropriately constrain α+β +γ = 2ζ(1−ζ) so that the probabilities
sum to 1. Note that the speciﬁc transition probabilites are selected so that the distribution shift satisﬁes
(α,β,γ)9connectivity. We also number the points 1−4 in the following order: class 1 (source), class
1 (target), class 2 (source), class 2 (target)."
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.34684684684684686,Under review as a conference paper at ICLR 2022
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.35135135135135137,"Figure 2: (Left) Illustrative toy example with 2 domains and 2 classes,
where each class-domain pair is a single node in the graph. Edge weights
denote connectivities (probability of two class-domain pairs augmenting
into the same point). (Middle) When α (same-class-different-domain
connectivity) is greater than γ (connectivity across different domains
and classes), the domain representation are oriented so that linear clas-
siﬁcation on the source classiﬁes the target accurately. (Right) When
α<γ, no linear classiﬁer can label both the source and target accurately."
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.35585585585585583,"0
5
10
15
Same Class, Different Domain Connectivity 0.0 0.2 0.4 0.6 0.8 1.0"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.36036036036036034,Target Test Accuracy
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.36486486486486486,"Figure 3: Scatter plot of
random graph simulations,
where ζ = 16,β = 4, and
γ =2. Shown on the x-axis
is α and nontrivial target
accuracy is not possible
until α
>
γ.
Target
accuracy also increases
with the ratio α/β."
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.36936936936936937,"Pre-training.
HaoChen et al. (2021) show that minimizing the spectral contrastive loss (given in
Eq. 1) is equivalent to computing the spectral decomposition of the adjacency matrix of the population
augmentationgraph.Theleadingeigenvectorsoftheresultingdecompositionarethenusedasthelearned
representations, so that the representation of example i is the concatenation of the ith components
of those eigenvectors. Because the population here is ﬁnite, we compute the spectral decomposition
algebraically and express the learned representations as a function of connectivity. Agnostic of the
ordering of the relevant parameters, the unordered eigenvectors of this toy example are:1"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.3738738738738739,"v1 =[ 1 1 1 1]⊤,
v2 =[ 1 −1 1 −1]⊤,
v3 =[ 1 1 −1 −1]⊤,
v4 =[ 1 −1 −1 1]⊤"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.3783783783783784,with associated eigenvalues
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.38288288288288286,"λ1 =ζ+β+α+γ,
λ2 =ζ+β−α−γ,
λ3 =ζ−β+α−γ,
λ4 =ζ−β−α+γ"
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.38738738738738737,"We discard the eigenvector v1, as it assigns the same feature to every point, and use the top 2 remaining
eigenvectors as the learned features. For example, if v2 and v3 are the top remaining eigenvectors, then
the features for data point 2 would be the 2nd component of v2 and v3 (namely, (−1,1))."
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.3918918918918919,"Fine-tuning.
We consider learning a linear classiﬁer upon the pre-trained features. The middle panel
of Figure 2 illustrates the fortunate case, in which α>γ and β >γ (but possibly α≤β). In this case,
λ2 >λ4 and λ3 >λ4 and therefore the learned features use v2 and v3 and are (1,1),(1,−1) for class
1 and (−1,1),(−1,−1) for class 2. Then, a linear classiﬁer trained on the source uses only the class
information, which is contained in the second index, and labels the target accurately."
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.3963963963963964,"Note that the source classiﬁer labels the target correctly if and only if it uses solely the features given by
v3 for prediction. Two possible failure modes occur when α is smallest (and therefore v3 is discarded)
and when β is smallest (and therefore the source classiﬁer can use features from either v3 or v4)."
USING CONNECTIVITY TO CHARACTERIZE DOMAIN SHIFT,0.4009009009009009,"Importantly in this toy setting, whether or not the features of the target examples are oriented correctly
depends only on whether α and β are both > γ. Consequently, the classes within each domain can
be very connected–more so than across domains–and target accuracy can be high. To illustrate the
importance of this connectivity ratio on a more ﬁne-grained setting, we run simulations and observe
sharp thresholding behavior when γ surpasses α (Figure 3, simulation details in Appendix C)."
CONNECTIVITY GOVERNS THE SUCCESS OF PRE-TRAINING FOR DOMAIN ADAPTATION,0.40540540540540543,"4.4
CONNECTIVITY GOVERNS THE SUCCESS OF PRE-TRAINING FOR DOMAIN ADAPTATION
Connectivity on benchmark datasets.
We estimate the input space domain connectivity on
DomainNet, Living-17, and Entity-30 by training classiﬁers between various pairs of class-domain
pairs (test errors provided in Table 2, full details provided in Section B). We ﬁnd that all the datasets
satisfy α>γ–the necessary condition for the good case in the toy example–but α<β for DomainNet
while α>β for BREEDS. Intuitively, on natural distribution shifts α is likely to be higher than γ, since
the data augmentation should be more likely to connect class-domain pairs where only the domain
differs than to bridge differences across both the domain and the class."
CONNECTIVITY GOVERNS THE SUCCESS OF PRE-TRAINING FOR DOMAIN ADAPTATION,0.4099099099099099,"1We leave the eigenvectors unnormalized for ease of exposition; the normalized variants would multiply the
entire vector by 1/2."
CONNECTIVITY GOVERNS THE SUCCESS OF PRE-TRAINING FOR DOMAIN ADAPTATION,0.4144144144144144,Under review as a conference paper at ICLR 2022
CONNECTIVITY GOVERNS THE SUCCESS OF PRE-TRAINING FOR DOMAIN ADAPTATION,0.4189189189189189,"1.05
1.10
1.15
1.20
Connectivity measure 0 10 20 30"
CONNECTIVITY GOVERNS THE SUCCESS OF PRE-TRAINING FOR DOMAIN ADAPTATION,0.42342342342342343,Target accuracy gain
CONNECTIVITY GOVERNS THE SUCCESS OF PRE-TRAINING FOR DOMAIN ADAPTATION,0.42792792792792794,(over worst pair with same target)
CONNECTIVITY GOVERNS THE SUCCESS OF PRE-TRAINING FOR DOMAIN ADAPTATION,0.43243243243243246,"r = 0.84
real-clipart
sketch-clipart
painting-clipart
real-painting
sketch-painting
clipart-painting
sketch-real
painting-real
clipart-real
real-sketch
painting-sketch
clipart-sketch"
CONNECTIVITY GOVERNS THE SUCCESS OF PRE-TRAINING FOR DOMAIN ADAPTATION,0.4369369369369369,"Figure 4: Plot of our measure of connectivity
ratios against target accuracy of contrastive
pre-training on DomainNet. Our quantity highly
correlates r=0.84 with target accuracy."
K,0.44144144144144143,"2k
4k
6k
8k
10k
12k
Subsampled Dataset Size 54 56 58 60 62"
K,0.44594594594594594,Target Test Accuracy
K,0.45045045045045046,"Confidence
Random"
K,0.45495495495495497,"Figure 5: Ablation of connectivity by removing
unlabeled data points near the margin of a
domain classiﬁer. Removing these points from
the pre-training dataset reduces target accuracy
more than removing a random subset of the
same size."
K,0.4594594594594595,"Domain and class classiﬁers are nearly orthogonal.
We check the extent to which the domain and
class information are separated by training 3 linear classiﬁers on the pre-trained representation space:
1) Source classiﬁer (which discriminates classes in source) 2) Target classiﬁer (which discriminates
classes in target)2 3) Domain classiﬁer (discriminates between source or target domains). We compare
the linear weights of these classiﬁers by computing the cosine similarity. We ﬁnd that"
K,0.46396396396396394,"1. The source and target classiﬁers are very similar; on average over the classes, the cosine
similarity of linear weights for the same class from the source and target classiﬁers is high
(around 0.20 for DomainNet, 0.34 for Living-17)."
K,0.46846846846846846,"2. The domain classiﬁer is nearly orthogonal to the source and target classiﬁers. The average
cosine similarity between the weights of any class from the source or target classiﬁers and
the weights of the domain classiﬁer is 0.01."
K,0.47297297297297297,"This suggests that the class information and domain information are contained in different directions of
the feature space, and gives an explanation for how both classes and domains can be easily separated in
feature space while maintaining the ability to generalize from source to target. Learning to discriminate
classes results in a classiﬁer with weights that are nearly orthogonal to the weights for domain
classiﬁcation, suggesting a natural amount of ‘domain agnosticity” despite not removing domain
information from the features. Detailed results on individual pairs are provided in Appendix D."
K,0.4774774774774775,"Connectivity ratios correlate with target accuracy.
We construct a scalar connectivity quantity
and ﬁnd that it correlates with target accuracy of contrastive learning on DomainNet. In the simple
example, contrastive pre-training for domain adaptation works when α > γ and β > γ. The ﬁrst
condition (α > γ) is relatively more important since if β (connectivity between different classes in
the same domain) is very high, then the classes within a domain are hard to discriminate and the
classiﬁcation task itself will be hard. Thus, we deﬁne the quantity"
K,0.481981981981982,"(α/γ)·(β/γ)ϵ
(2)"
K,0.4864864864864865,"where we take ϵ = 0.1 to “discount” the importance of the second ratio. We multiply the two ratios
to express the logical “and”. The larger this quantity is, the better the target accuracy should be."
K,0.49099099099099097,"Figure 4 shows a plot of the quantity with target accuracy of contrastive learning on DomainNet pairs.
Because different target domains can have different intrinsic errors, we “debias” the target accuracy
of each source-target pair by subtracting the worst accuracy over all pairs with the same target domain.
We ﬁnd a strong correlation between the connectivity and target accuracy (Pearson r of 0.87 for S+T
pre-training) (Figure 4)."
K,0.4954954954954955,"Ablating connectivity.
We ﬁnd that the connectivity between examples of the same class but different
domain is also very important, as those examples “bridge” the domains. We verify this intuition by
systematically ablating the connectivity on a subset of Living-17 by removing those examples that
contribute most to domain connectivity. Speciﬁcally, for each class we trained discriminators between
the source and target examples of that class and removed the n pre-training data points from each"
K,0.5,"2Although this cannot be done in practice, we only use the target labels here for exploratory analysis;"
K,0.5045045045045045,Under review as a conference paper at ICLR 2022
K,0.509009009009009,"class-domain pair on which the discriminator was least conﬁdent; these examples can be seen as
the ones contributing most to domain connectivity. For subsample sizes of {1176,2352,5880,11760}
we consistently reduce target accuracy as compared to random subset removal (Figure 5; conﬁdence
subsampling increases error relatively by 6% on average compared to random)."
RELATED WORK,0.5135135135135135,"5
RELATED WORK"
RELATED WORK,0.5180180180180181,"Domain adaptation.
Ben-David et al. (2010) prove generalization bounds in the distribution shift
setting. Their bounds rely on a notion of distance between two data distributions called H∆H
divergence being small, which many methods such as domain adversarial training are inspired by (Ganin
et al., 2016; Tzeng et al., 2014; 2017). Our conceptual model based around connectivity provides a
way to explain how contrastive pre-training can work for domain adaptation even when the H∆H
divergence between domains is large."
RELATED WORK,0.5225225225225225,"Domain adaptation with self-supervision.
Prior works have explored the application of self-
supervision to domain adaptation. Sun et al. (2019) propose optimizing on hand-crafted self-supervised
tasks such as predicting the angle of rotation of a rotated image. These tasks are jointly optimized
on the source and target data along with the labeled source loss. As part of their comparisons, they
found that ﬁrst pre-training on target (or source and target) data and then ﬁne-tuning on source data
separately (instead of jointly) was highly non-competitive with their other methods. In contrast, we
ﬁnd that pre-training with a general, off-the-shelf contrastive learning objective is competitive with
other state-of-the-art domain adaptation techniques."
RELATED WORK,0.527027027027027,"Contrastive objective for domain adaptation.
Contrastive objectives have also been applied to do-
main adaptation problems. Wang et al. (2021) jointly train the contrastive loss on source and target data.
These methods rely on explicit anchor pairs that encourage domain alignment (Wang et al., 2021), or still
employ some form of explicit domain alignment (Thota & Leontidis, 2021). We consider pre-training
instead of joint training and do not use any domain knowledge about the distribution shift problem."
RELATED WORK,0.5315315315315315,"Self-training.
Berthelot et al. (2021) use self-training and consistency regularization with a distri-
bution alignment method, but require estimating the target label distribution and tuning a conﬁdence
thresholding parameter (tuned using target test labels). Cai et al. (2021) give a label propagation
analysis for self-training in domain adaptation.They have only one parameter which governs both the
different-class-same-domain and different-class-different-domain connections, requiring both to be
small. In our framework, we allow the different-class-same-domain connectivity to not be small, in
accordance with our observation that domains can be well separated in contrastive feature space. They
give experiments on self-training when operating on a contrastive pre-trained representation space,
but their focus is on self-training while the pre-trained representations are an empirical detail. One
inﬂexible characteristic of standard self-training is that it relies on the unlabeled data having the same
classes as the labeled data (in order to pseudolabel unlabeled points), which may not allow them to
utilize diverse sources of extra unlabled data like many pre-training methods can."
CONCLUSION,0.536036036036036,"6
CONCLUSION"
CONCLUSION,0.5405405405405406,"In this work we focused on evaluating and understanding contrastive pre-training under domain
adaptation. Pre-training has the advantage of amortizing the cost of processing a large unlabeled
dataset into a pre-training step that is done only once. While pre-training loses the advantage of
seeing labeled data (and thus the downstream task) to adapt its usage of unlabeled data, we ﬁnd
that contrastive pre-training is still competitive with other domain adaptation algorithms. We hope
that our connectivity model can give insights to improve pre-training data selection, develop better
augmentations for pre-training with distribution shift in mind, and improve ﬁne-tuning methods that
exploit the properties of the pre-trained feature space."
CONCLUSION,0.545045045045045,ETHICS
CONCLUSION,0.5495495495495496,"Our experiments use standard benchmark vision datasets in domain adaptation, which is publicly
available data. In general, however, large-scale unsupervised learning can involve data scraped from
the internet which may lead to privacy concerns. Care should be taken when curating datasets for
large scale contrastive pre-training in practice."
CONCLUSION,0.5540540540540541,Under review as a conference paper at ICLR 2022
CONCLUSION,0.5585585585585585,REPRODUCIBILITY
CONCLUSION,0.5630630630630631,"We detail the hyperparameters and experimental details in the paper and appendix. We have uploaded
a zipped code archive and will publish the code on Github upon acceptance."
REFERENCES,0.5675675675675675,REFERENCES
REFERENCES,0.5720720720720721,"EA AlBadawy, A Saha, and MA Mazurowski. Deep learning for segmentation of brain tumors: Impact
of cross-institutional training and testing. Med Phys., 45, 2018."
REFERENCES,0.5765765765765766,"Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine Learning, 79(1):151–175, 2010."
REFERENCES,0.581081081081081,"David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch:
A uniﬁed approach to semi-supervised learning and domain adaptation.
arXiv preprint
arXiv:2106.04732, 2021."
REFERENCES,0.5855855855855856,"Tianle Cai, Ruiqi Gao, Jason Lee, and Qi Lei. A theory of label propagation for subpopulation shift. In
Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine
Learning, volume 139 of Proceedings of Machine Learning Research, pp. 1170–1182. PMLR,
18–24 Jul 2021. URL https://proceedings.mlr.press/v139/cai21b.html."
REFERENCES,0.5900900900900901,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In Advances in Neural
Information Processing Systems (NeurIPS), volume 33, pp. 9912–9924, 2020."
REFERENCES,0.5945945945945946,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning
(ICML), pp. 1597–1607, 2020."
REFERENCES,0.5990990990990991,"Adam Coates, Andrew Ng, and Honlak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence
and Statistics, volume 15, pp. 215–223, 2011."
REFERENCES,0.6036036036036037,"Dengxin Dai and Luc Van Gool. Dark model adaptation: Semantic image segmentation from daytime
to nighttime. In 2018 21st International Conference on Intelligent Transportation Systems (ITSC),
pp. 3819–3824, 2018. doi: 10.1109/ITSC.2018.8569387."
REFERENCES,0.6081081081081081,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Association for Computational Linguistics
(ACL), pp. 4171–4186, 2019."
REFERENCES,0.6126126126126126,"Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness
(python library). https://github.com/MadryLab/robustness, 2019."
REFERENCES,0.6171171171171171,"Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. MRQA 2019
shared task: Evaluating generalization in reading comprehension. In Workshop on Machine Reading
for Question Answering (MRQA), 2019."
REFERENCES,0.6216216216216216,"Geoff French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation.
In International Conference on Learning Representations, 2018."
REFERENCES,0.6261261261261262,"Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks.
Journal of Machine Learning Research (JMLR), 17, 2016."
REFERENCES,0.6306306306306306,"Jeff Z. HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised
deep learning with spectral contrastive loss. arXiv preprint arXiv:2106.04156, 2021."
REFERENCES,0.6351351351351351,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsu-
pervised visual representation learning. In Computer Vision and Pattern Recognition (CVPR), 2020."
REFERENCES,0.6396396396396397,"Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness
and uncertainty. In International Conference on Machine Learning (ICML), 2019."
REFERENCES,0.6441441441441441,Under review as a conference paper at ICLR 2022
REFERENCES,0.6486486486486487,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009."
REFERENCES,0.6531531531531531,"Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions
of the Association for Computational Linguistics, 8:726–742, 2020. doi: 10.1162/tacl a 00343.
URL https://aclanthology.org/2020.tacl-1.47."
REFERENCES,0.6576576576576577,"Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In International Conference on Computer Vision (ICCV), 2019."
REFERENCES,0.6621621621621622,"Viraj Prabhu, Shivam Khare, Deeksha Karthik, and Judy Hoffman. Selective entropy optimization
via committee consistency for unsupervised domain adaptation. In International Conference on
Computer Vision (ICCV), 2021."
REFERENCES,0.6666666666666666,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
Learning transferable visual models from natural language supervision. In International Conference
on Machine Learning (ICML), volume 139, pp. 8748–8763, 2021."
REFERENCES,0.6711711711711712,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition
challenge. International Journal of Computer Vision, 115(3):211–252, 2015."
REFERENCES,0.6756756756756757,"Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation
shift. arXiv, 2020."
REFERENCES,0.6801801801801802,"Rui Shu, Hung H. Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-T approach to unsupervised
domain adaptation. In International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.6846846846846847,"Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk,
Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with
consistency and conﬁdence. arXiv, 2020."
REFERENCES,0.6891891891891891,"Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James
Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang
Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Sheng Zhao,
Shuyang Cheng, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability
in perception for autonomous driving: Waymo open dataset, 2020."
REFERENCES,0.6936936936936937,"Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A. Efros. Unsupervised domain adaptation through
self-supervision. arXiv, 2019."
REFERENCES,0.6981981981981982,"Shuhan Tan, Xingchao Peng, and Kate Saenko. Class-imbalanced domain adaptation: An empirical
odyssey. arXiv preprint arXiv:1910.10320, 2020."
REFERENCES,0.7027027027027027,"Mamatha Thota and Georgios Leontidis. Contrastive domain adaptation. arXiv, 2021."
REFERENCES,0.7072072072072072,"Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:
Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014."
REFERENCES,0.7117117117117117,"Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Computer Vision and Pattern Recognition (CVPR), 2017."
REFERENCES,0.7162162162162162,"Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain
contrastive learning for unsupervised domain adaptation. arXiv, 2021."
REFERENCES,0.7207207207207207,"Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski,
Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. Learning and evaluating
general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019."
REFERENCES,0.7252252252252253,"Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan,
and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020."
REFERENCES,0.7297297297297297,Under review as a conference paper at ICLR 2022
REFERENCES,0.7342342342342343,"A
ADDITIONAL EXPERIMENTAL DETAILS"
REFERENCES,0.7387387387387387,"A.1
DATASETS
BREEDS.
We use the dataset creation functions deﬁned in the robustness Python library to
generate the Living-17 and Entity-30 tasks from the original ImageNet dataset (Engstrom et al., 2019;
Russakovsky et al., 2015). The Living-17 dataset is an animal classiﬁcation task which consists of
nodes in the subtree rooted at the “living thing” node in the WordNet hierarchy. An example of a label
is “ape” with subpopulations of gibbons, orangutans, gorillas, and chimpanzees. The Entity-30 dataset
is a much more general task, incorporating nodes in the “entity” subtree. Labels include “building” and
“home appliance”. The trailing number in the dataset name is the total number of classes in that dataset."
REFERENCES,0.7432432432432432,"DomainNet.
We use the ofﬁcial SENTRY repo at https://github.com/virajprabhu/
SENTRY, which ﬁlters the original DomainNet dataset automatically as described in Section 3. This
reﬁnement is done to eliminate much of the noise present in the original DomainNet dataset (Tan et al.,
2020)."
REFERENCES,0.7477477477477478,"STL →CIFAR.
CIFAR-10 consists of 32×32 images from the former TinyImages dataset, and
STL-10 is derived from ImageNet and contains images with resolution 96×96. We resize the STL-10
images to 32×32 to match the resolution of CIFAR10. The two non-overlapping classes (“monkey”
in CIFAR-10 and “frog” in STL10) are removed from both datasets before training."
REFERENCES,0.7522522522522522,"A.2
BASELINES AND HYPERPARAMETER TUNING
ERM (source-only)."
REFERENCES,0.7567567567567568,"• BREEDS: We use the same hyperparameters as Santurkar et al. (2020). For Entity30, we train
for 300 epochs and divided the learning rate by 10 every 100 epochs. On Living17, we train
for 450 epochs and divided the learning rate by 10 every 150 epochs. For both datasets we
use a ResNet50 with a learning rate of 0.1, a weight decay of 10−4, and a batch size of 128."
REFERENCES,0.7612612612612613,"• DomainNet: The SENTRY algorithm runs ERM with class balancing (starting with
ImageNet-pretrained initialization) prior to beginning entropy minimization, and therefore
the SENTRY repository contains an ERM implementation and hyperparameters for
DomainNet. We run ERM for 150 epochs and multiply the initial learning rate by 10x,
keeping all other hyperparameters constant."
REFERENCES,0.7657657657657657,• STL10 →CIFAR10: We report the exact results from Shu et al. (2018).
REFERENCES,0.7702702702702703,"DIRT-T.
DIRT-T (Shu et al., 2018) is a domain adaptation method that addresses two ﬂaws of domain
adversarial neural networks (Ganin et al., 2016): 1) distribution matching is a weak constraint, and 2)
in some domain adaptation settings there does not exist a good joint classiﬁer on both source and target.
The authors address the ﬁrst shortcoming by adding a conditional entropy regularization term so that the
model’s decision boundaries do not overlap high-density regions of data. This is inspired by the cluster
assumption, which states that the input space is divided into well-separated clusters, one for each class in
the label space. The lack of a good classiﬁer on both source and target is then addressed via self-training
on the unlabeled target data. We report the STL10 →CIFAR10 results from Shu et al. (2018)."
REFERENCES,0.7747747747747747,"SENTRY.
For each pair of domains on DomainNet we conduct a hyperparameter search through
λsrc ∈{0.5,1.0,1.5} (the weight on the supervised classiﬁcation loss) and learning rates ∈{0.01,0.001}
and report the model results with the highest source evaluation accuracy."
REFERENCES,0.7792792792792793,"On BREEDS we keep the same default hyperparameters from the SENTRY repo and search over 3 learn-
ing rates {0.001,0.01,0.1}, choosing the model with the highest target evaluation accuracy to report."
REFERENCES,0.7837837837837838,"SENTRY underperforms the ERM models when the source domain is clipart images (from DomainNet),
which could be the result of not using ImageNet-pretrained classiﬁers for initialization as Prabhu et al.
(2021) do. Therefore, as the joint-training baseline for DomainNet we take the maximum of ERM
and SENTRY."
REFERENCES,0.7882882882882883,"A.3
SIMCLR
We
use
the
ofﬁcial
SimCLR
repository
from
Google
for
our
experiments
(https:
//github.com/google-research/simclr).
We use a ResNet18 with a batch size
of 256, a learning rate of 0.2, and weight decay 10−4. The projection head has two layers and an output"
REFERENCES,0.7927927927927928,Under review as a conference paper at ICLR 2022
REFERENCES,0.7972972972972973,"dimension of 64. We pre-train the model for 400 epochs with square-root learning rate scaling and
we train the linear probe for 100 epochs on batches of size 512 and a learning rate of 0.1."
REFERENCES,0.8018018018018018,"A.4
SWAV
We use the public SwAV repository available at https://github.com/facebookresearch/
swav and kept almost all of the hyperparameters provided by the original paper for 400 epoch,
256-batch-size training on ImageNet. However, we used a batch size of 512 and additionally made
the following changes based on the Github issues answered by the original authors:"
REFERENCES,0.8063063063063063,"1. We avoid using a queue on DomainNet and the subsampled variant of Living-17 in order to
stabilize training. For pre-training on the full Living-17 and Entity-30 datasets, we introduce
the queue at epoch 60."
REFERENCES,0.8108108108108109,"2. We set the number of prototypes to be 10 times the number of classes (170, 300, and 400
for Living-17, Entity-30 and DomainNet, respectively)."
REFERENCES,0.8153153153153153,3. We set ϵ=0.03.
REFERENCES,0.8198198198198198,"4. We set the base learning rate to 0.6, following a linear scaling rule based on batch size."
REFERENCES,0.8243243243243243,We always ﬁne-tuned from the ﬁnal iterate of SwAV pre-training (400 epochs).
REFERENCES,0.8288288288288288,"On DomainNet, we ﬁne-tuned SwAV models for 50 epochs using the ERM implementation in the SEN-
TRY repository (without running any joint-training algorithm), keeping all hyperparameters other than
the number of epochs constant. We report the target test accuracy of the ﬁnal model (after 50 epochs)."
REFERENCES,0.8333333333333334,"On BREEDS Living-17, we ﬁne-tuned SwAV models for 100 epochs with a cosine learning rate
schedule without restarts. We use SGD with initial learning rate 0.1 for the classiﬁer head and 0.01
for the encoder, momentum 0.9, and weight decay 0.0001. We use a batch size of 96, and once again
report the target test accuracy of the ﬁnal model (after 100 epochs)."
REFERENCES,0.8378378378378378,"On BREEDS Entity-30, which contains 300K examples, we instead linear probe."
REFERENCES,0.8423423423423423,"B
RESULTS AND PROTOCOL FOR ESTIMATING (α,β,γ)9CONNECTIVITY"
REFERENCES,0.8468468468468469,"Table 3 reports the connectivity estimates for the input space and the feature spaces learned by ERM,
DANN, and SwAV for all pairs of DomainNet domains."
REFERENCES,0.8513513513513513,"To estimate the connectivity between 2 class-domain pairs (i,d) and (j,d′), we use the following
algorithm:"
REFERENCES,0.8558558558558559,"1. Label all training examples of class and domain (i,d) as 0 and all training examples of (j,d′)
as 1. Discard the remaining examples."
REFERENCES,0.8603603603603603,"2. Train a ResNet50 for 100 epochs using SwAV augmentations and cosine learning rate. We
did not exhaustively tune this training step, but we kept the hyperparameters constant for
all the pairs."
REFERENCES,0.8648648648648649,"3. Collect the test set analogously to step 1, and evaluate the classiﬁer on augmented data."
REFERENCES,0.8693693693693694,"Each domain in DomainNet-S has a unique label distribution (all of which are far from uniform), and
therefore in computing the average connectivity we compute the weighted mean, where each pair
of (class, domain) pairs is weighted by the ratio of the less to more frequent label (0 or 1)."
REFERENCES,0.8738738738738738,"C
SIMULATION DETAILS"
REFERENCES,0.8783783783783784,"To obtain an even more ﬁne-grained picture, we run simulations on toy data settings, where we can
precisely vary each type of connectivity, and observe:"
REFERENCES,0.8828828828828829,1. Sharp thresholding behavior when α and γ cross over.
REFERENCES,0.8873873873873874,2. α must be nontrivial.
REFERENCES,0.8918918918918919,"The simulation is on a ﬁnite population so that we can study the spectral decomposition directly; how-
ever, later we show empirically that these ideas still hold in real-world, continuous-population settings.
For each domain-class pair dc, we generate N points in augmentation space. These augmented points
inherit the domain-class assignments of their corresponding natural points (we have to ensure that all"
REFERENCES,0.8963963963963963,Under review as a conference paper at ICLR 2022
REFERENCES,0.9009009009009009,"Input space
Different class,
Different class,
Same class,
Different class,
same domain, source
same domain, target
different domain
different domain"
REFERENCES,0.9054054054054054,"Real ↔Sketch
24.49
38.12
21.47
20.51
Real ↔Painting
24.49
33.71
33.62
28.07
Real ↔Clipart
24.49
35.20
23.12
21.18
Sketch ↔Painting
38.12
33.71
24.34
23.16
Sketch ↔Clipart
38.12
35.20
30.23
27.72
Painting ↔Clipart
33.71
35.20 31.36
30.09"
REFERENCES,0.9099099099099099,"Avg.
30.57
35.19
27.36
25.12"
REFERENCES,0.9144144144144144,"SwAV pre-trained
Different class,
Different class,
Same class,
Different class,
space
same domain, source
same domain, target
different domain
different domain"
REFERENCES,0.918918918918919,"Real ↔Sketch
3.08
6.92
4.73
1.74
Real ↔Painting
3.07
7.43
11.79
2.23
Real ↔Clipart
3.02
6.34
8.93
1.73
Sketch ↔Painting
8.50
10.65
5.55
2.66
Sketch ↔Clipart
8.54
8.15
8.10
3.29
Painting ↔Clipart
10.22
8.40
6.13
3.12"
REFERENCES,0.9234234234234234,"Avg.
6.07
7.98
7.54
2.46"
REFERENCES,0.9279279279279279,"ERM feature
Different class,
Different class,
Same class,
Different class,
space
same domain, source
same domain, target
different domain
different domain"
REFERENCES,0.9324324324324325,"Real →Sketch
8.40
13.32
9.99
6.43
Real →Painting
8.40
16.24
22.23
8.87
Real →Clipart
8.40
13.15
14.16
6.63
Sketch →Real
12.46
11.75
16.36
8.45
Sketch →Painting
12.46
18.57
16.51
9.38
Sketch →Clipart
12.46
14.45
22.30
10.90
Painting →Real
22.59
13.45
26.50
14.10
Painting →Sketch
22.59
20.43
14.03
11.28
Painting →Clipart
22.59
18.73
19.55
14.20
Clipart →Real
17.17
15.35
21.18
11.48
Clipart →Sketch
17.17
21.45
17.47
12.03
Clipart →Painting
17.17
25.30
19.80
14.52"
REFERENCES,0.9369369369369369,"Avg.
15.16
16.85
18.34
10.69"
REFERENCES,0.9414414414414415,"DANN feature
Different class,
Different class,
Same class,
Different class,
space
same domain, source
same domain, target
different domain
different domain"
REFERENCES,0.9459459459459459,"Real →Sketch
6.09
9.98
9.87
4.53
Real →Painting
6.27
13.07
21.55
6.72
Real →Clipart
6.36
9.82
14.37
4.98
Sketch →Real
8.50
7.53
15.38
5.71
Sketch →Painting
8.75
13.24
16.61
6.91
Sketch →Clipart
8.80
10.43
21.15
7.76
Painting →Real
14.93
7.70
22.14
7.46
Painting →Sketch
16.80
14.57
15.49
9.76
Painting →Clipart
14.78
11.31
13.24
7.35
Clipart →Real
11.89
10.64
17.32
7.08
Clipart →Sketch
12.40
16.18
16.09
8.90
Clipart →Painting
11.51
19.06
15.90
10.44"
REFERENCES,0.9504504504504504,"Avg.
10.59
11.96
16.59
7.30"
REFERENCES,0.954954954954955,"Table 3: Empirical estimates of the different parameters of connectivity in the input space (top)
and feature spaces computed by SwAV (second), ERM (third), and DANN (bottom). The numbers
provided are the separation error."
REFERENCES,0.9594594594594594,"the augmented points are distinct). Then for any new natural point x∈X that we want to augment, we
simply choose one of our existing augmented points x at random and designate x as the augmentation
of x. Since we have assigned domain-class memberships to this base set of augmentations, we can
deﬁne the connectivity of two domain-class pairs (d1c1,d2c2) to be the probability that a natural
point from d1c1 is “augmented into” an augmented point in d2c2 (and vice-versa). This probability
is something we can easily change in the simulations to vary this notion of connectivity."
REFERENCES,0.963963963963964,"D
ADDITIONAL DETAILS FOR SECTION 4.4"
REFERENCES,0.9684684684684685,"As another method for evaluating the connectivity measures (in addition to Figure 4), we inspect
the weights learned from a linear regression model from 3 features [α,β,γ] to the target accuracy
(de-biased). The weights are [−6.94,5.61,0.87], which gives a large negative weight to α (different-"
REFERENCES,0.972972972972973,Under review as a conference paper at ICLR 2022
REFERENCES,0.9774774774774775,"class-different-domain connecitivity), large positive weight to γ (same-class-different-domain), and
a small positive weight to β (different-class-same-domain) as expected from our intuitions."
REFERENCES,0.9819819819819819,"The following table contains more detailed results on the cosine similarity between domain and class
classiﬁers."
REFERENCES,0.9864864864864865,"Class (domain 1)
Class (domain 1)
Class (domain 2)
vs. Class (domain 2)
vs. Domain
vs. Domain"
REFERENCES,0.990990990990991,"Living-17
Source ↔Target
0.3971
0.0125
0.0158
DomainNet
Real ↔Sketch
0.1999
0.0143
0.0165
Real ↔Painting
0.2280
0.0157
0.0166
Real ↔Clipart
0.2222
0.0148
0.0149
Sketch ↔Painting
0.1758
0.0146
0.0200
Sketch ↔Clipart
0.1590
0.0155
0.0178
Painting ↔Clipart
0.1383
0.0178
0.0221"
REFERENCES,0.9954954954954955,"Table 4: Cosine similarity of class and domain classiﬁers trained on SwAV representations (average
over all classes). Class classiﬁers trained on the source and target individually learn similar linear
weights, evidenced by the cosine similarity in the range 0.13 to 0.33. However, domain classiﬁers
learn linear weights that are nearly orthogonal to the class classiﬁer weights, suggesting that SwAV
pre-training learns features containing domain and class information in somewhat separate directions."
