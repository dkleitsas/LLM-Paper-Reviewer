Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0022222222222222222,"Bootstrap is a principled and powerful frequentist statistical tool for uncertainty
quantiﬁcation. Unfortunately, standard bootstrap methods are computationally
intensive due to the need of drawing a large i.i.d. bootstrap sample to approximate
the ideal bootstrap distribution; this largely hinders their application in large-scale
machine learning, especially deep learning problems. In this work, we propose
an efﬁcient method to explicitly optimize a small set of high quality “centroid”
points to better approximate the ideal bootstrap distribution. We achieve this by
minimizing a simple objective function that is asymptotically equivalent to the
Wasserstein distance to the ideal bootstrap distribution. This allows us to provide
an accurate estimation of uncertainty with a small number of bootstrap centroids,
outperforming the naive i.i.d. sampling approach. Empirically, we show that our
method can boost the performance of bootstrap in a variety of applications."
INTRODUCTION,0.0044444444444444444,"1
INTRODUCTION"
INTRODUCTION,0.006666666666666667,"Bootstrap is a simple and principled frequentist uncertainty quantiﬁcation tool and can be ﬂexibly
applied to obtain data uncertainty estimation with strong theoretical guarantees (Hall et al., 1988;
Austern & Syrgkanis, 2020; Chatterjee et al., 2005; Cheng et al., 2010). In particular, when combined
with the maximum likelihood estimator or more general M-estimators, bootstrap provides a general-
purpose, plug-and-play non-parametric inference framework for general probabilistic models without
case-by-case derivations; this makes it a promising frequentist alternative to Bayesian inference."
INTRODUCTION,0.008888888888888889,"However, the standard bootstrap inference is highly expensive in both computation and memory as it
typically requires drawing a large number1 of i.i.d. bootstrap particles (samples) to obtain an accurate
uncertainty estimation. In the context of this paper, as each bootstrap particle/sample/centroid is a
machine learning model, we might directly call a model as particle/sample/centroid. With a small
number of particles, bootstrap may perform poorly. As a consequence, when applied to deep learning,
we need to store a large number of neural networks and feed the input into a tremendous number
of networks every time we make inference, which can be quite expensive and even unaffordable
for deep learning problems with huge models2. For example, in autonomous driving applications,
our device can only store a limited number of models and we need to make decisions within a short
time, which makes the standard bootstrap with a large number of models no more feasible. Typical
ensemble methods in deep learning, such as Lakshminarayanan et al. (2017); Huang et al. (2017);
Vyas et al. (2018); Maddox et al. (2019); Liu & Wang (2016), can only afford to use a small number
(e.g., less than 20) of models."
INTRODUCTION,0.011111111111111112,"Therefore, to make bootstrap more accessible in modern machine learning, it is essential to develop
new approaches that break the key computation and memory barriers mentioned above. This paper
aims to improve the bootstrap when the resource at inference is limited. We are motivated to consider
the following problem:"
INTRODUCTION,0.013333333333333334,How to improve the accuracy of bootstrap inference when the number of particles is limited?
INTRODUCTION,0.015555555555555555,"We attack this challenge by presenting an efﬁcient centroid approximation for bootstrap. Our method
replaces the i.i.d. bootstrap particles with a set of carefully optimized centroid particles that are"
INTRODUCTION,0.017777777777777778,"1For example, thousands of, as suggested by Statistics textbooks such as Wasserman (2013).
2While training cost is an extra burden, it is small compared with the cost of making prediction as we only
need to train the model once but make countless predictions at deployment."
INTRODUCTION,0.02,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.022222222222222223,"guaranteed to provide an accurate and compact approximation to the ideal bootstrap distribution so
that only a smaller number of particles is needed to obtain good performance."
INTRODUCTION,0.024444444444444446,"Our method is based on minimizing a specially designed objective function that is asymptotically
equivalent to the Wasserstein distance between the ideal bootstrap distribution and the particle
distribution formed by the learned centroids. During the training, each centroid adjusts its location
being aware of the locations of the others so that centroids are diversiﬁed and well distributed on
the domain. Our method is similar to doing K-means on the ideal bootstrap distribution, ﬁnding
K representative centroids that well represent K separate parts of the target distribution’s domain
in an optimal way. As centroids are optimized to better approximate the distribution, our approach
naturally improves over the vanilla bootstrap with i.i.d. particles. See Figure 1 for illustration."
INTRODUCTION,0.02666666666666667,"Empirically, we apply the centroid approximation method to various applications, including conﬁ-
dence interval estimation (DiCiccio et al., 1996), bootstrap method for contextual bandit (Riquelme
et al., 2018), bootstrap deep Q-network (Osband et al., 2016) and bagging method (Breiman, 1996)
for neural networks. We ﬁnd that our method consistently improves over the standard bootstrap."
INTRODUCTION,0.028888888888888888,"Figure 1: The solid lines represent the density of
the target distribution. Left ﬁgure: Typical i.i.d.
particles that are randomly distributed on the do-
main. Right ﬁgure: The learned diversiﬁed cen-
troids that are well distributed on the domain. The
centroids partition the domain into several disjoint
regions (separated by the dashed lines in the ﬁgure)
and each centroid can be viewed as the ‘K-means’
center of the region it belongs to."
INTRODUCTION,0.03111111111111111,"Notation
We use ∥·∥to represent the ℓ2 norm
for a vector and the operator norm for a matrix.
We denote the integer set {1, 2, ...., N} by [N].
Given any m, we deﬁne the probability simplex
Cm := {[v1, ..., vm] ∈Rm : vi ≥0, ∀i ∈
[m] and P"
INTRODUCTION,0.03333333333333333,"i∈[m] vi = 1}. For a symmetric ma-
trix M, we denote its minimal eigenvalue by
λmin(M). For a positive-deﬁnite matrix M, if
M = A⊤A, then we denote A by M 1/2. We de-
note the Wasserstein distance between two distri-
bution ρ1 and ρ2 by W2[ρ1, ρ2]. We use O and
o to denote the conventional big-O and small-
o notation and use Op to denote the stochastic"
INTRODUCTION,0.035555555555555556,"boundedness. We use
d→to denote convergence
in distribution."
BACKGROUND,0.03777777777777778,"2
BACKGROUND"
BACKGROUND,0.04,"Suppose we have a model fθ parameterized by θ in a parameter space Θ ⊆Rd. Let {xi}n
i=1 ⊂X be
a training set with n data points on X. Assume ℓ(x, fθ) is the negative log-likelihood of data point x
with model fθ. A standard approach to estimate θ is maximum likelihood estimator (MLE), which
minimizes the negative log-likelihood function (loss) over the training set"
BACKGROUND,0.042222222222222223,"ˆθ = arg min
θ∈Θ L(θ),
L(θ) = Pn
i=1ℓ(xi, fθ)/n."
BACKGROUND,0.044444444444444446,"Here the MLE ˆθ provides a point estimation without any information on the data uncertainty. Bootstrap
is a simple and effective frequentist method to quantify the uncertainty. The bootstrap loss is a
randomly perturbed loss deﬁned as"
BACKGROUND,0.04666666666666667,"Lw(θ) = Pn
i=1wiℓ(xi, fθ)/n,"
BACKGROUND,0.04888888888888889,"where w = [w1, ..., wn]⊤is a set of random weights of data points drawn from some distribution π.
A typical choice of π is the multinomial distribution with uniform probability, which corresponds to
resampling on the training set with replacement. Given w, one can calculate its associated bootstrap
particle by minimizing the bootstrap loss:"
BACKGROUND,0.051111111111111114,"ˆθw = arg min
θ∈Θ Lw(θ).
(1)"
BACKGROUND,0.05333333333333334,"Let ρπ be the distribution of ˆθw when w ∼π. Bootstrap theory indicates that we can quantify the
data uncertainty of θ or any function g(θ) using ρπ. We call ρπ the ideal bootstrap distribution and it
is the main object we want to approximate."
BACKGROUND,0.05555555555555555,Under review as a conference paper at ICLR 2022
BACKGROUND,0.057777777777777775,"Denote δθ as the delta measure centered at θ. Standard bootstrap method approximates ρπ by
the particle distribution ˆρπ(·) = Pm
j=1δˆθwj (·)/m formed by m i.i.d. particles {ˆθwj}m
j=1, which"
BACKGROUND,0.06,"can be obtained by drawing m i.i.d. weights {wj}m
j=1 from π and calculating each ˆθwj based on
(1). However, for deep learning applications, as discussed in the introduction, storing and making
inference using a large number m of bootstrap particles can be quite expensive. On the other hand,
if m is small, ˆρπ tends to be a poor approximation of ρπ. In this paper, we aim to improve the
approximation of the particle distribution when m is small."
METHOD,0.06222222222222222,"3
METHOD"
METHOD,0.06444444444444444,"Our idea is simple. Instead of using i.i.d. particles, in which the location of each particle is
independent from that of the others, we try to actively optimize the location of each particle so
that particles are diversiﬁed, better distributed and eventually providing a particle distribution with
improved approximation accuracy. A natural way to achieve this goal is to explicitly optimize a set
of points {θj}m
j=1 (called centroids) jointly such that the Wasserstein distance between ρπ and the
induced particle distribution is minimized:"
METHOD,0.06666666666666667,"{θ∗
j , v∗
j }m
j=1 = arg
min
θ1,...,θm∈Θ, [v1,...,vm]∈Cm W2
hPm
j=1vjδθj, ρπ
i
.
(2)"
METHOD,0.06888888888888889,"Here we consider a Wasserstein distance W2 equipped with a special data-dependent distance metric
|| · ||D that will be introduced later in (5). We note that here we also optimize the probability weights
{vj}m
j=1 of the centroids. Finding the optimal centroids and probability weights can be decomposed
into two steps: the centroid learning phase and the probability weights learning phase, based on the
facts in (3,4). W2
2 P"
METHOD,0.07111111111111111,"j∈[m]v∗
j δθ∗
j , ρπ"
METHOD,0.07333333333333333,"
= Jπ({θ∗
j }m
j=1), where Jπ({θj}m
j=1):=Ew∼π
h
minj∈[m]||θj −ˆθw||2
D
i
. (3)"
METHOD,0.07555555555555556,"Here (3) implies that, to ﬁnd the optimal particle distribution in (2), we can start with the centroid
learning phase where we only need to optimize the centroids. It can be achieved by minimizing
Jπ({θj}m
j=1), which is the averaged distance of bootstrap particles to their closest centroid. After we
obtain the optimal centroids, the optimal probability weights can be learned by (4):"
METHOD,0.07777777777777778,"v∗
j = ˜v∗
j /P"
METHOD,0.08,"s∈[m] ˜v∗
s, where ˜v∗
j = Pw∼π

j = arg minj∈[m]||θ∗
j −ˆθw||2
D

.
(4)"
METHOD,0.08222222222222222,"Here v∗
j is the proportion of bootstrap particles that are closest to the centroid j. We emphasize that
the optimal solution to two-stage learning is guaranteed to be the global minimizer of the loss in (2)
(see Lemma 3.1 and 3.2 in Canas & Rosasco (2012))."
METHOD,0.08444444444444445,"However, the key issue is that the losses in both (2, 3) can not be computed in practice, as they require
us to access ρπ (i.e., obtain ˆθw ﬁrst in order to calculate the loss). To handle this issue, we seek an
easy-to-compute surrogate loss. Our idea is based on the following observation. Assuming the size
of training data is large, which is usually the case in deep learning, we can expect that θw will be
centered around a small region3. It implies that we should search the centroid in this small region.
Notice that when θ is close to ˆθw, based on Taylor approximation, we have"
METHOD,0.08666666666666667,"Lw(θ) ≈Lw(ˆθw) + ∇⊤
θ Lw(ˆθw)(θ −ˆθw) + 1/2(θ −ˆθw)⊤∇2
θLw(ˆθw)(θ −ˆθw)"
METHOD,0.08888888888888889,"≈L∞(θ0) + 1/2||θ −ˆθw||2
D, where ∥V ∥2
D := V ⊤∇2
θL∞(θ0)V.
(5)"
METHOD,0.09111111111111111,"Here L∞(θ) := Exℓ(x, fθ) denotes the population loss; θ0 is the minimizer of L∞(θ). In (5), we use
the facts4 that ∇⊤
θ Lw(ˆθw) = 0; and with large training set, the empirical distribution Pn
i=1δxi/n
well approximates the whole data population, and hence the bootstrap resampling distribution, i.e.,
Pn
i=1wiδxi/n on the empirical distribution also well approximates the whole data population. This
implies that Lw(·) ≈L∞(·) and ∇2
θLw(·) ≈∇2
θL∞(·). As the loss are close to each other, their
minimizers are also close ˆθw ≈θ0. Since L∞(θ0) is some (unknown) constant independent with θ,
we can replace the ||θj −ˆθw||2
D in (3) by Lw(θj) as it only adds some constant into the loss."
METHOD,0.09333333333333334,"3This can be formally characterized by central limit theorem as discussed in Section 4.
4We defer the detailed analysis to Section 4."
METHOD,0.09555555555555556,Under review as a conference paper at ICLR 2022
METHOD,0.09777777777777778,"Intuitively, we can expect that the centroid closest to ˆθw is the one that gives the smallest loss on Lw.
It motivates us to learn the centroids via the modiﬁed centroid learning phase:"
METHOD,0.1,"{θ∗
j }m
j=1 = arg
min
θ1,...,θm∈Θ Ew∼π

minj∈[m]Lw(θj)

.
(6)"
METHOD,0.10222222222222223,"Similarly, the optimal probability weights can be learned via the modiﬁed weight learning phase:"
METHOD,0.10444444444444445,"v∗
j = ˜v∗
j /P
s∈[m] ˜v∗
s, where ˜v∗
j = Pw∼π
 
j ∈arg minj∈[m]Lw(θ∗
j )

.
(7)"
METHOD,0.10666666666666667,"We note that here we slightly abuse the notation of θ∗
j and v∗
j in (3,4) and (6,7) for simpliﬁcation. In
the later context, θ∗
j and v∗
j are used based on their deﬁnitions in (6,7)."
METHOD,0.10888888888888888,"Connection to K-means By viewing the target distribution as a set of particles that we want to
cluster, in K-means clustering, each centroid (i.e., K-means center) represents one of the K disjoint
groups5 of particles, which is formed by assigning each particle in the whole set to the closest centroid
among all the K centroids. K-means learns the optimal K centroids in the way that they can best
approximate the whole set. The ‘closeness’ for assigning the particles is measured by the distance
between the two points. As pointed out by Canas & Rosasco (2012), K-means essentially searches
the optimal particle distribution formed by the K centroids that minimizes its Wasserstein distance to
the target distribution. Our centroid approximation idea follows the same fashion of clustering but
our key innovation is to measure the ‘closeness’ by examining the bootstrap loss of the centroids so
that we can still learn the optimal centroids without obtaining the i.i.d. bootstrap particles ﬁrst. We
also point out that, while we share the same objective as K-means, the optimization algorithms differ.
The Expectation-Maximization type of algorithm used by K-means is not applicable to our scenario."
METHOD,0.1111111111111111,"Comparing with Other Particle Improving Approach Intuitively, from a high level abstracted
perspective, we provide an approach to use K-means type of idea to improve the particle quality
without accessing to the true target distribution. This is the key differentiator of this work to other
approaches that improve the particle quality, as they all require to access the target distribution. For
example Claici et al. (2018) requires that sampling from target distribution is cheap and easy. Chen
et al. (2012; 2018a); Campbell & Beronov (2019) need to access the logarithm of the probability
density function of the target distribution. In our problem, neither sampling from the target distribution
is cheap nor the logarithm of the probability density function is available, making those approaches
not more applicable."
TRAINING,0.11333333333333333,"3.1
TRAINING"
TRAINING,0.11555555555555555,"The optimization of (6) can be solved by gradient descent. Suppose θ∗
j (t) is the j-th centroid at
iteration t. We initialize {θ∗
j (0)}m
j=1 by sampling from ρπ and at iteration t, we update θ∗
t by applying
the gradient descent on the loss in (6), which yields"
TRAINING,0.11777777777777777,"θ∗
j (t + 1) ←θ∗
j (t) −ϵtg(θ∗
j (t)),"
TRAINING,0.12,"g(θ∗
j (t)) = ∇θEw∼π

I{j ∈uw(t)}Lw(θ∗
j (t))

/v∗
j (t),
(8)"
TRAINING,0.12222222222222222,"where we deﬁne the index of the closest centroid to particle ˆθw as uw(t) = arg minj∈[m] Lw(θ∗
j (t))
and v∗
j (t) = Pw∼π (j ∈uw(t)) denotes the probability that centroid j is the one that gives the lowest
bootstrap loss. The denominator v∗
j (t) in g(θ∗
j (t)) is optional. However, notice that the magnitude of
numerator in g(θ∗
k(t)) decays with larger m, which might require an adjustment of the learning rate
when m changes. This adjustment can be avoided by rescaling with v∗
k(t)."
TRAINING,0.12444444444444444,"We note that {θ∗
j (0)}m
j=1 is just m i.i.d. bootstrap particles which is not optimal for approximation
and our algorithm can be viewed as an approach for reﬁning the m particles by solving (6). In
practice, we ﬁnd that we can simply use random initialization (e.g., draw θ from some Gaussian
distribution) instead."
TRAINING,0.12666666666666668,"Centroid Degeneration Phenomenon Naively applying the updating rule (8) may cause a degenera-
tion phenomenon: When a centroid happens to give considerably worse performance than others,
which can be caused by the stochasticity of gradient or worse initialization, the performance of this
centroid will remain considerably worse throughout the optimization. The reason is simple. As this"
TRAINING,0.1288888888888889,5i.e. the regions separated by the dashed lines in the right plot of Figure 1.
TRAINING,0.13111111111111112,Under review as a conference paper at ICLR 2022
TRAINING,0.13333333333333333,"centroid (e.g. θ∗
j (t)) gives a considerably worse performance, the probability that it gives the lowest
bootstrap loss, i.e., v∗
j (t), is small. As a consequence, the gradient that updates this centroid is only
based on aggregating information from a small low-density region of π and hence can be unstable and
further degrades this centroid. Note that this mechanism is self-reinforced since when this centroid
cannot be effectively improved in the current iteration, it faces the same issue in the next one. As a
result, this centroid is always signiﬁcantly worse than the others."
TRAINING,0.13555555555555557,"We call this undesirable phenomenon centroid degeneration and we want to prevent this phenomenon
because when it happens, we have a centroid that is not representative and contributes less to
approximating ρπ. We solve this issue with a simple solution and here is the intuition. The reason
that a centroid degenerates lies in that this centroid is far from the good region where it gives a good
performance. And when this happens, we should push the centroid to move towards this good region,
which can be achieved by using the common gradient over the whole training data. Speciﬁcally, we
deﬁne a threshold γ, indicating centroid j is degenerated if v∗
j (t) ≤γ. And when it happens, we
update using the common gradient over the whole data:"
TRAINING,0.13777777777777778,"θ∗
j (t + 1) ←θ∗
j (t) −ϵt∇θL(θ∗
j (t)).
(9)"
TRAINING,0.14,"In section 4, we give a theoretical analysis on why this modiﬁcation is important and is able to solve
the centroid degeneration issue."
TRAINING,0.14222222222222222,"Practical Algorithm In practice, we estimate the gradient by replacing the expectation over w ∼π
in (8) with averaging over M i.i.d. samples {wh}M
h=1 drawn from π:"
TRAINING,0.14444444444444443,"ˆg(θ∗
j (t)) ="
TRAINING,0.14666666666666667,"PM
h=1

I{j ∈uwh(t)}∇θLwh(θ∗
j (t))
"
TRAINING,0.14888888888888888,"PM
h=1 I{j ∈uwh(t)}
.
(10)"
TRAINING,0.1511111111111111,"We emphasize that here uwh and Lwh for all wh can be computed very cheaply, enabling us to use a
very large M to reduce the error of gradient estimation. Speciﬁcally, at iteration t, for each j ∈[m],
we ﬁrst calculate"
TRAINING,0.15333333333333332,"L(θ∗
j (t)) = [ℓ(x1, fθ∗
j (t)), ...ℓ(xn, fθ∗
j (t))]⊤∈Rn,
(11)"
TRAINING,0.15555555555555556,"which is the vector encodes the loss of centroid j at each data point. This procedure does not introduce
any extra computational overhead compared with standard gradient descent. After that, the bootstrap
loss of centroid j can be computed cheaply by Lw(θ∗
j (t)) = w⊤L(θ∗
j (t)). Similarly, it is cheap to
obtain uwh by"
TRAINING,0.15777777777777777,"uwh(t) = arg min
j∈[m] w⊤
h L(θ∗
j (t)).
(12)"
TRAINING,0.16,"Taking the modiﬁed updating rule introduced to prevent the centroid degeneration phenomenon into
account, we update θ∗
j (t) by θ∗
j (t + 1) ←θ∗
j (t) −ϵtφ(θ∗
j (t)), where"
TRAINING,0.1622222222222222,"φ(θ∗
j ) ="
TRAINING,0.16444444444444445,"(
ˆg(θ∗
j (t))
if P"
TRAINING,0.16666666666666666,"h∈[M] I{uwh(t) = j}/M > γ
∇θL(θ∗
j (t))
otherwise.
(13)"
TRAINING,0.1688888888888889,"Notice that as as L(θ∗
j (t)) is pre-computed, calculating Lwh(θ∗
j (t)) for many (e.g., M) different
wh is almost free, since it only requires a simple matrix multiplication with O(nM) complexity.
Similarly, calculating uwh(t) is also very cheap.
In practical implementation, as uwh(t) do not
change much within a few iterations, we can update uwh(t) every a few iterations (e.g., every epoch).
We can also replace the ∇θLwh(θ∗
j (t)) or ∇θL(θ∗
j (t)) in (13) using a mini-batch of data instead of
the whole data, which leads to a stochastic gradient version of our algorithm. We refer readers to
Algorithm 1 for the ideal updating and Algorithm 2 for the practical implementation in Appendix B
for more details."
THEORY,0.1711111111111111,"4
THEORY"
THEORY,0.17333333333333334,"Recall that, as discussed in (5), our approach relies on the intuition that bootstrap particles are nested
in a small region so that we can approximate the distance between the centroid and a bootstrap particle
by the bootstrap loss of that centroid. The main goal of this section is to give a formal theoretical
justiﬁcation of this intuition."
THEORY,0.17555555555555555,Under review as a conference paper at ICLR 2022
THEORY,0.17777777777777778,"Before we proceed, we clarify several important setups for establishing and interpreting the theoretical
result. As discussed in the introduction, we are mainly interested in the scenerio that the number of
available particles/centroids m is small while the number of training data n is large, which motivates
us to establish theoretical result in the region of small m and large n. This is signiﬁcantly different
from conventional asymptotic analysis in which we aim to show the behavior when m →∞. We
consider the setting that the parameter dimension d is ﬁxed and does not scale with n."
THEORY,0.18,"We are mainly interested in characterizing the approximation of the proposed loss in (6) to the ideal
loss in (3), given any small and ﬁxed number m of centroids when n →∞. This justiﬁes why the
proposed centroid approximation method can be viewed as minimizing the Wasserstein distance
between the particle distribution ρ∗
π and the target bootstrap distribution ρπ."
THEORY,0.18222222222222223,"For simplicity, we build our analysis assuming the ideal update rule (8,9) is used. We start with the
following main assumptions."
THEORY,0.18444444444444444,"Assumption 1 (Smoothness and boundedness) Assume that the following quantities are upper
bounded by some constant c < ∞:"
MAX,0.18666666666666668,"1.
max
i,j,k∈[d]
sup
θ∈Θ,x∈X"
MAX,0.18888888888888888,"∂3ℓ(x, fθ)
∂iθi∂θj∂θk
; 2.
sup
θ1,θ2∈Θ
sup
x∈X"
MAX,0.19111111111111112,"||∇2
θℓ(x, fθ1) −∇2
θℓ(x, fθ2)||
||θ1 −θ2||
;"
SUP,0.19333333333333333,"3.
sup
x∈X,θ∈Θ"
SUP,0.19555555555555557,"∇2
θℓ(x, fθ)
 ;
4. sup
θ∈Θ
∥θ∥."
SUP,0.19777777777777777,Assumption 1 is a standard regularity condition on the boundness and smoothness of the problem.
SUP,0.2,"Assumption 2 (Asymptotic normality) Assume √n

ˆθw −ˆθ

d→N (0, A) and √n

ˆθ −θ0

d→"
SUP,0.20222222222222222,"N (0, A) as n →∞, where A is a positive-deﬁnite matrix with the largest eigenvalue bounded."
SUP,0.20444444444444446,"Assumption 2 is a higher level assumption on the asymptotic normality of the estimators. Such result
is classic and can be derived with some weak and technical regularity conditions. See examples in
Chatterjee et al. (2005); Cheng et al. (2010)."
SUP,0.20666666666666667,"Assumption 3 (On the global minimizer) Suppose that λmin
 
∇2
θL∞(θ0)

> 0."
SUP,0.2088888888888889,Assumption 3 is also standard showing the locally strongly convexity of the loss around the truth θ0.
SUP,0.2111111111111111,Assumption 4 (On the learning rate) Suppose that maxt ϵt = O(n−1).
SUP,0.21333333333333335,"Assumption 4 assumes that the learning rate of the algorithm is sufﬁciently small such that its induced
discretization error is not the dominating term."
SUP,0.21555555555555556,"The key challenge of our analysis is to show that our dynamics is B(θ0, r)-stable (deﬁned below in
Deﬁnition 1) for some small r, saying that {θ∗
j (t)}m
j=1 stay in a small region that is close to θ0 for any
iteration t. Combined with the property6 that ˆθw are also close to θ0, the centroids and the bootstrap
particles are close to each other and thus our approximation in (5) holds for all t ≥0. In this way,
optimizing the centroids by minimizing our loss is almost equivalent to optimizing the centroids by
minimizing the Wasserstein distance."
SUP,0.21777777777777776,"Deﬁnition 1 (B(θ, r)-stable) Given some θ ∈Θ and r ≥0, we say our dynamics is B(θ, r)-stable
if ∀t ≥0 and ∀j ∈[m], θ∗
j (t) ∈B(θ, r), where B(θ, r) := {θ′ : ∥θ′ −θ∥≤r, θ′ ∈Θ} is the ball
with radius r centered at θ."
SUP,0.22,"The key intuition to establish such B(θ0, r)-stable result is to characterize that our optimization
dynamics is implicitly self-controlled: when some centroid approaches the boundary of B(θ0, r), the
updating mechanism automatically start to push the centroid to move towards the center of the region.
Thus, if all the centroids are within B(θ0, r) at initialization, they will alway stay in this region."
SUP,0.2222222222222222,"Thanks to assumption 2, 3, when the dataset is large, the landscape of our loss is locally strongly
convex around θ0. When a centroid j is at the boundary of B(θ0, r), it has v∗
j (t) < γ and thus the"
SUP,0.22444444444444445,6This is implied by the asymptotic normality in assumption 2.
SUP,0.22666666666666666,Under review as a conference paper at ICLR 2022
SUP,0.2288888888888889,"m = 20
m = 50
m = 100
m = 200"
SUP,0.2311111111111111,α = 0.9
SUP,0.23333333333333334,"Normal
Bootstrap
0.029 ± 0.010
0.031 ± 0.011
0.021 ± 0.010
0.017 ± 0.010
Centroid
0.027 ± 0.010
0.001 ± 0.009
0.012 ± 0.010
0.016 ± 0.010"
SUP,0.23555555555555555,"Percentile
Bootstrap
0.101 ± 0.013
0.036 ± 0.011
0.021 ± 0.010
0.014 ± 0.010
Centroid
0.081 ± 0.012
0.021 ± 0.010
0.020 ± 0.010
0.015 ± 0.010"
SUP,0.23777777777777778,"Pivotal
Bootstrap
0.106 ± 0.013
0.045 ± 0.011
0.025 ± 0.010
0.023 ± 0.010
Centroid
0.046 ± 0.011
0.013 ± 0.009
0.011 ± 0.010
0.020 ± 0.010"
SUP,0.24,"Table 1: Centroid approximation for conﬁdence interval. The numbers in the table represent |α −ˆα|,
where ˆα is the estimated coverage probability. The errors bar is the standard deviation."
SUP,0.24222222222222223,"updating direction is the gradient of loss L. By the convexity, such gradient will push the centroid
move towards the center of B(θ0, r) where the empirical minimizer locates at. On the other hand, for
centroid j with v∗
j (t) ≥γ, its updating direction aggregates information from sufﬁcient data point
and thus behaves similarly to that of the common gradient, pushing centroid to move towards the
center with the centroid is not close to the center."
SUP,0.24444444444444444,"Theorem 1 Under Assumptions 1-4 and suppose that we initialize θ∗
j (0), j ∈[m] by sampling from
ρπ, given any m < ∞and γ > 0, when n is sufﬁciently large, we have"
SUP,0.24666666666666667,"max
j∈[m] sup
t≥0"
SUP,0.24888888888888888,"θ∗
j (t) −θ0
 = Op(
p"
SUP,0.2511111111111111,(log n)/n).
SUP,0.25333333333333335,Here the probability is taken w.r.t. training data.
SUP,0.25555555555555554,"Theorem 1 implies our dynamics is B(θ0, rn)-stable with rn = O(
p"
SUP,0.2577777777777778,"log n/n). The condition that
θ∗
j (0) ∼ρπ i.i.d. can be replaced by the condition that θ∗
j (0) is sufﬁciently close to θ0. We need such
condition as we uniformly bound the distance between θ∗
j (t) and θ0 at any iteration including the ﬁrst
one. Theorem 1 implies that the approximation stated in (5) holds with high probability and hence
the proposed loss in (6) is ‘almost as good as’ the ideal loss in (3)."
SUP,0.26,"Theorem 2 Under the same assumptions as Theorem 1, given any m < ∞and γ > 0, when n is
sufﬁciently large, we have"
SUP,0.26222222222222225,"sup
t≥0"
SUP,0.2644444444444444,"Ew∼π[ min
j∈[m] Lw(θ∗
j (t))] −B −Ew∼π[ min
j∈[m] ||θ∗
j (t) −ˆθw||2
D]/2
 = Op(
q"
SUP,0.26666666666666666,(log n)/n3/2).
SUP,0.2688888888888889,"Here the probability is taken w.r.t. training data and B is some constant independent from θ∗
j (t) for
any t ≥0 and j ∈[m]."
SUP,0.27111111111111114,"Asymptotics when m also grows
Although our main interest is the asymptotics with a small, ﬁxed
m and growing n, we discuss here on asymptotics when m also grows. As shown in Section 3 and
introduction, our method can be viewed as an ‘approximated’ K-means on the target distribution. From
Theorem 5.2 in Canas & Rosasco (2012), the particle distribution formed by the optimal centroids
learned by K-means gives improved O(m−1/d) convergence to any general target distribution in
terms of Wasserstein distance, where d is data dimension. In comparison, the particle distribution of
i.i.d. sample only gives O(m−1/(2d+4)) from Theorem 5.1 in Canas & Rosasco (2012). This implies
that our approach potentially also has such a rate improvement. Note that the results in Canas &
Rosasco (2012) are for general target distribution without any n involves. To rigorously establish the
large m asymptotic result for our problem, we need to study the joint limit of n and m. This is indeed
very non-trivial: as discussed in Weed et al. (2019) (i.e. Proposition 14), when n →∞, the target
distribution ρπ becomes a sharp Gaussian and the convergence rate of i.i.d. bootstrap particles will
gradually improve to O(m−1/2) (in a way that depends on n). It implies that when n ≫m →∞,
our improvement may become only constant level. We ﬁnd establishing such a theory is out the scope
of this conference paper and leave it as future work."
EXPERIMENT,0.2733333333333333,"5
EXPERIMENT"
EXPERIMENT,0.27555555555555555,"As discussed in the introduction, our main goal is to improve the quality of the particle distribution
when only a limited number of particles/centroids is allowed, so that we can use less particles at"
EXPERIMENT,0.2777777777777778,Under review as a conference paper at ICLR 2022
EXPERIMENT,0.28,"m = 3
m = 4
m = 5
m = 10"
EXPERIMENT,0.2822222222222222,"Mushroom
Bootstrap
3282.1 ± 72.82
3307.9 ± 69.2
3311.6 ± 79.3
3397.4 ± 51.4
Centroid
3702.7 ± 89.76
3723.1 ± 78.7
3799.6 ± 84.2
3796.9 ± 36.1"
EXPERIMENT,0.28444444444444444,"Statlog
Bootstrap
1864.3 ± 6.4
1869.2 ± 5.2
1877.2 ± 4.1
1877.0 ± 2.7
Centroid
1893.6 ± 6.0
1892.6 ± 3.6
1891.3 ± 3.5
1892.6 ± 2.8"
EXPERIMENT,0.2866666666666667,"Financial
Bootstrap
2255.77 ± 58.45
2265.42 ± 58.17
2269.33 ± 56.36
2281.35 ± 56.65
Centroid
2313.29 ± 56.45
2315.32 ± 56.75
2323.88 ± 56.73
2325.54 ± 56.05"
EXPERIMENT,0.28888888888888886,"Table 2: Results on the contextual bandit experiment. The numbers in the table represent the averaged
reward with its standard deviation."
EXPERIMENT,0.2911111111111111,"deployment, which reduces the memory consumption and the computational cost for making predic-
tion. Thus, our experiment design will be focusing on comparing the testing performance of vanilla
bootstrap and our centroid based approach when the same and a small number of particles/centroids is
used. We apply our method to four applications: conﬁdence interval construction, bootstrap method
in contextual bandit, bootstrapped deep Q-network and bagging. Due to space limit, we refer to
Appendix C.4 for the bagging experiment, Appendix C.5 for ablation study on the importance of
modifying the gradient to overcome the centroid degeneration phenomenon. Although we are less
interested in the computational cost of training, as discussed in Section 3, our method actually only
introduces a little training computation overhead, which is another advantage of our method. We
draw analysis on this aspect in Appendix C.6."
BOOTSTRAP CONFIDENCE INTERVAL,0.29333333333333333,"5.1
BOOTSTRAP CONFIDENCE INTERVAL"
BOOTSTRAP CONFIDENCE INTERVAL,0.29555555555555557,"20
50
100
200
Num particles 0.05 0.10 0.15 0.20"
BOOTSTRAP CONFIDENCE INTERVAL,0.29777777777777775,Wasserstein Dist
BOOTSTRAP CONFIDENCE INTERVAL,0.3,"Bootstrap
Centroid"
BOOTSTRAP CONFIDENCE INTERVAL,0.3022222222222222,"Figure 2: Wasserstein distance between the particle
distribution and the true bootstrap distribution w.r.t.
the number of particles."
BOOTSTRAP CONFIDENCE INTERVAL,0.30444444444444446,"We start with a classic application of bootstrap:
conﬁdence interval estimation for linear model
with parameter θ. Fix conﬁdence level α, we
consider three ways to construct (two-sided)
bootstrap conﬁdence interval of θ: the Normal
interval, the percentile interval and the pivotal
interval. And we test m = 20, 50, 100, 200. For
all experiments, we repeat with 1000 indepen-
dent random trials. We consider the standard
bootstrap as baseline. Detailed experimental
setup are included in Appendix C.1."
BOOTSTRAP CONFIDENCE INTERVAL,0.30666666666666664,"Figure 2 shows the Wasserstein distance be-
tween the true target distribution ρπ and the empirical distributions obtained by (a) i.i.d. sampling
ˆρπ, (b) the proposed centroid approximation ρ∗
π. The centroid approximation signiﬁcantly reduces
the Wasserstein distance by a large margin. We then compare the quality of obtained conﬁdence
intervals, which is measured by the difference between the estimated coverage probability and the
true conﬁdence level, i.e., |ˆα −α| (the lower the better). Here we only consider conﬁdence intervals
of the ﬁrst coordinate of θ: θ1. Table 1 summarizes the result with α = 0.9. We see that using more
particles is generally able to improve the constructed conﬁdence intervals. We also compare with
two variants of standard bootstrap: Bayesian bootstrap (Rubin, 1981) and residual bootstrap (Efron,
1992). And we consider varying α = 0.8, 0.95. These results are included in Appendix C.1."
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT,0.3088888888888889,"5.2
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT"
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT,0.3111111111111111,"Contextual bandit is a classic task in sequential decision making, in which accurately quantifying
the model uncertainty is important in order to achieve good exploration-exploitation trade-off. As
shown in Riquelme et al. (2018), tracking the model uncertainty using bootstrap is a strong method
for contextual bandit. However, it is costly to maintain a large number of bootstrap models and thus
the number of models is typically within 10 (Osband et al., 2016). We ﬁnd that applying the proposed
centroid approximation here can signiﬁcantly improve the performance. Riquelme et al. (2018)
uses m = 3 bootstrap models and we give a more comprehensive evaluation with m = 3, 4, 5, 10.
We consider three datasets: Mushroom, Statlog and Financial. We set γ = 0.5/m. We randomly
generate 20 different context sequences, apply all the methods and report the averaged cumulative
reward and its standard deviation. Table 2 summarizes the result and note that a large part of variance"
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT,0.31333333333333335,Under review as a conference paper at ICLR 2022
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT,0.31555555555555553,"100
150
200
250
300
350
400
Episodes 100 50 0 50 100 150 200 250"
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT,0.31777777777777777,Moving avg reward
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT,0.32,"Bootstrap 2-head
Centroid-2-head
Bootstrap-5-head
Centroid-5-head"
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT,0.32222222222222224,"30
40
50
60
70
80
90
Episodes 0 10 20 30 40 50"
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT,0.3244444444444444,Moving avg reward
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT,0.32666666666666666,"Bootstrap 2-head
Centroid-2-head
Bootstrap 5-head
Centroid-5-head"
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT,0.3288888888888889,"Figure 3: Results for Bootstrap DQN with centroid approximation experiment. Left: LunarLander-v2;
Right: Catcher-v0."
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT,0.33111111111111113,"can be explained by different context sequences. All results in Table 2 are statistically signiﬁcant
under signiﬁcant level 5% using matched pair t-test. Table 2 shows that using more bootstrap models
generally improves the accumulated reward. And when using the same number of models, the
proposed centroid approximation method consistently improves over standard bootstrap method. We
refer readers to appendix C.2 for more information on the background and experiment."
CENTROID APPROXIMATION FOR BOOTSTRAP DQN,0.3333333333333333,"5.3
CENTROID APPROXIMATION FOR BOOTSTRAP DQN"
CENTROID APPROXIMATION FOR BOOTSTRAP DQN,0.33555555555555555,"“Efﬁcient exploration is a major challenge for reinforcement learning (RL). Common dithering strate-
gies such as ϵ-greedy do not carry out temporally-extended exploration, which leads to exponentially
larger data requirements” (Osband et al., 2016). To tackle this issue, Osband et al. (2016) proposes
the Bootstrapped Deep Q-Network (DQN). We apply our centroid approximation to improve Boot-
strapped DQN. We consider m = 2, 5 and similar to the experimental setting in contextual bandit,
we set γ = 0.5/m. We consider two benchmark environments: LunarLander-v2 and Catcher-v0
from GYM (Brockman et al., 2016) and PyGame learning environment (Tasﬁ, 2016). We conduct
the experiment with 5 independent random trails and report the averaged result with its standard
deviation. We refer readers to Appendix C.3 for more background and other experiment details.
Figure 3 summarizes the result. For LunarLander-v2, Bootstrap DQN with 2 and 5 heads give similar
performance but both converge to a less optimal model compared with the centroid approximation
method. Centroid approximation method with 2 and 5 heads performs similarly at convergence but
centroid approximation method with 5 heads is able to converge faster than 2-head model and thus
has lower regret. For Catcher-v0, adding more heads to the model is able to improve the performance
for both methods. The proposed centroid approximation consistently improves over baselines."
RELATED WORK,0.3377777777777778,"6
RELATED WORK"
RELATED WORK,0.34,"Bootstrap is an classical statistical inference method, which was developed by Efron (1992) and
generalized by, i.e., Mammen (1993); Shao (2010); Efron (2012) (just to name a few). Bootstrap can
be widely applied to various statistical inference problem, such as conﬁdence interval estimation
(DiCiccio et al., 1996), model selection (Shao, 1996), high-dimensional inference (Chen et al., 2018b;
El Karoui & Purdom, 2018; Nie & Roˇckov´a, 2020), off-policy evaluation (Hanna et al., 2017),
distributed inference (Yu et al., 2020) and inference for ensemble model (Kim et al., 2020), etc."
RELATED WORK,0.3422222222222222,"Despite its wide applications and nice theoretical properties, there has been very few works on
discussing and improving the approximation efﬁciency in the region of small bootstrap sample size,
beyond the i.i.d. sampling paradigm. While the m-out-of-n bootstrap (Bickel et al., 2012) and the
bag of little bootstrap (Kleiner et al., 2014) are designed to reduce the computational cost with the
subsampling techniques in the big data settings (large n), they still require a large bootstrap sample
size and thus are still not scalable to large deep learning applications."
RELATED WORK,0.34444444444444444,"Bayesian Inference is a different approach to quantify the model uncertainty. Different from
frequentists’ method, Bayesian assumes a prior over the model and the uncertainty can be captured by
the posterior. Bayesian inference have been largely popularized in machine learning, largely thanks
to the recent development in scalable sampling method (Welling & Teh, 2011; Chen et al., 2014;
Seita et al., 2018; Wu et al., 2020), variational inference (Blei et al., 2017; Liu & Wang, 2016), and
other approximation methods such as Gal & Ghahramani (2016); Lee et al. (2018). In comparison,"
RELATED WORK,0.3466666666666667,Under review as a conference paper at ICLR 2022
RELATED WORK,0.3488888888888889,"bootstrap has been much less widely used in modern machine learning and deep learning. We believe
this is largely attributed to the lack of similarly efﬁcient computational methods in the small sample
size m region, which is the very problem that we aim to address with our new centroid approximation
method."
RELATED WORK,0.3511111111111111,"Uncertainty in Deep Learning In additional to the applications considered in this paper, uncertainty
in deep learning model can also be applied to problems including calibration (Guo et al., 2017)
and out-of-distribution detection (Nguyen et al., 2015). The deﬁnition of uncertainty of neural
network is quite generalized (e.g., Gal & Ghahramani (2016); Ovadia et al. (2019); Maddox et al.
(2019); Van Amersfoort et al. (2020)) and can be quite different from the uncertainty that bootstrap
inference want to quantify and can be approached with various methods including drop out (Gal &
Ghahramani, 2016; Durasov et al., 2020), label smoothing (Qin et al., 2020), designing new modules
in the model (Kivaranovic et al., 2020), adversarial training (Lakshminarayanan et al., 2017) and
Bayesian modeling (Blundell et al., 2015), etc. This paper focuses on improving the bootstrap method
and thus is orthogonal to those previous works. Pearce et al. (2018); Salem et al. (2020) also try to
reﬁne the ensemble models to improve the quality of prediction interval. Compare with our method,
their method can only be applied to prediction interval and does not have theoretical guarantee."
CONCLUSION,0.35333333333333333,"7
CONCLUSION"
CONCLUSION,0.35555555555555557,"We propose a centroid approximation method to learn an improved particle distribution that better
approximates the target bootstrap distribution, especially in the region with small particle size.
Theoretically, when the size of training data is large, our objective function is surrogate to the
Wasserstein distance between the particle distribution and target distribution. Thus, compared with
standard bootstrap, the proposed centroid approximation method actively optimizes the distance
between particle distribution and target distribution. The proposed method is simple and can be
ﬂexibly used for applications of bootstrap with negligible extra computational cost."
CONCLUSION,0.35777777777777775,Under review as a conference paper at ICLR 2022
REFERENCES,0.36,REFERENCES
REFERENCES,0.3622222222222222,"Morgane Austern and Vasilis Syrgkanis. Asymptotics of the empirical bootstrap method beyond
asymptotic normality. arXiv preprint arXiv:2011.11248, 2020."
REFERENCES,0.36444444444444446,"Peter J Bickel, Friedrich G¨otze, and Willem R van Zwet. Resampling fewer than n observations:
gains, losses, and remedies for losses. In Selected works of Willem van Zwet, pp. 267–297. Springer,
2012."
REFERENCES,0.36666666666666664,"David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
Journal of the American statistical Association, 112(518):859–877, 2017."
REFERENCES,0.3688888888888889,"Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613–1622. PMLR, 2015."
REFERENCES,0.3711111111111111,"Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996."
REFERENCES,0.37333333333333335,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.37555555555555553,"Trevor Campbell and Boyan Beronov. Sparse variational inference: Bayesian coresets from scratch.
Advances in Neural Information Processing Systems, 32:11461–11472, 2019."
REFERENCES,0.37777777777777777,"Guillermo Canas and Lorenzo Rosasco. Learning probability measures with respect to optimal
transport metrics. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances
in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012."
REFERENCES,0.38,"Snigdhansu Chatterjee, Arup Bose, et al. Generalized bootstrap for estimating equations. The Annals
of Statistics, 33(1):414–436, 2005."
REFERENCES,0.38222222222222224,"Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In
International conference on machine learning, pp. 1683–1691. PMLR, 2014."
REFERENCES,0.3844444444444444,"Wilson Ye Chen, Lester Mackey, Jackson Gorham, Franc¸ois-Xavier Briol, and Chris Oates. Stein
points. In International Conference on Machine Learning, pp. 844–853. PMLR, 2018a."
REFERENCES,0.38666666666666666,"Xiaohui Chen et al. Gaussian and bootstrap approximations for high-dimensional u-statistics and
their applications. The Annals of Statistics, 46(2):642–678, 2018b."
REFERENCES,0.3888888888888889,"Yutian Chen, Max Welling, and Alex Smola. Super-samples from kernel herding. arXiv preprint
arXiv:1203.3472, 2012."
REFERENCES,0.39111111111111113,"Guang Cheng, Jianhua Z Huang, et al. Bootstrap consistency for general semiparametric m-estimation.
Annals of Statistics, 38(5):2884–2915, 2010."
REFERENCES,0.3933333333333333,"Sebastian Claici, Aude Genevay, and Justin Solomon. Wasserstein measure coresets. arXiv preprint
arXiv:1805.07412, 2018."
REFERENCES,0.39555555555555555,"Thomas J DiCiccio, Bradley Efron, et al. Bootstrap conﬁdence intervals. Statistical science, 11(3):
189–228, 1996."
REFERENCES,0.3977777777777778,"Nikita Durasov, Timur Bagautdinov, Pierre Baque, and Pascal Fua. Masksembles for uncertainty
estimation. arXiv preprint arXiv:2012.08334, 2020."
REFERENCES,0.4,"Bradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics, pp.
569–593. Springer, 1992."
REFERENCES,0.4022222222222222,"Bradley Efron. Bayesian inference and the parametric bootstrap. The annals of applied statistics, 6
(4):1971, 2012."
REFERENCES,0.40444444444444444,"Noureddine El Karoui and Elizabeth Purdom. Can we trust the bootstrap in high-dimensions? the
case of linear models. The Journal of Machine Learning Research, 19(1):170–235, 2018."
REFERENCES,0.4066666666666667,"Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059.
PMLR, 2016."
REFERENCES,0.4088888888888889,Under review as a conference paper at ICLR 2022
REFERENCES,0.4111111111111111,"Alex Graves. Practical variational inference for neural networks. In Advances in neural information
processing systems, pp. 2348–2356. Citeseer, 2011."
REFERENCES,0.41333333333333333,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017."
REFERENCES,0.41555555555555557,"Peter Hall et al. Rate of convergence in bootstrap approximations. The Annals of Probability, 16(4):
1665–1684, 1988."
REFERENCES,0.4177777777777778,"Josiah Hanna, Peter Stone, and Scott Niekum. Bootstrapping with models: Conﬁdence intervals for
off-policy evaluation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 31,
2017."
REFERENCES,0.42,"Botao Hao, Yasin Abbasi Yadkori, Zheng Wen, and Guang Cheng. Bootstrapping upper conﬁdence
bound. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,
2019."
REFERENCES,0.4222222222222222,"Jiri Hron, Alexander G de G Matthews, and Zoubin Ghahramani. Variational gaussian dropout is not
bayesian. arXiv preprint arXiv:1711.02989, 2017."
REFERENCES,0.42444444444444446,"Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger.
Snapshot ensembles: Train 1, get m for free. International Conference on Learning Representations,
2017."
REFERENCES,0.4266666666666667,"Byol Kim, Chen Xu, and Rina Foygel Barber. Predictive inference is free with the jackknife+-after-
bootstrap. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.4288888888888889,"Danijel Kivaranovic, Kory D Johnson, and Hannes Leeb. Adaptive, distribution-free prediction
intervals for deep networks. In International Conference on Artiﬁcial Intelligence and Statistics,
pp. 4346–4356. PMLR, 2020."
REFERENCES,0.4311111111111111,"Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, and Michael I Jordan. A scalable bootstrap
for massive data. Journal of the Royal Statistical Society: Series B: Statistical Methodology, pp.
795–816, 2014."
REFERENCES,0.43333333333333335,"Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. Advances in neural information processing systems,
30, 2017."
REFERENCES,0.43555555555555553,"Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on
Learning Representations, 2018."
REFERENCES,0.43777777777777777,"Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. Advances in Neural Information Processing Systems, 29, 2016."
REFERENCES,0.44,"Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson.
A simple baseline for bayesian uncertainty in deep learning. Advances in Neural Information
Processing Systems, 32:13153–13164, 2019."
REFERENCES,0.44222222222222224,"Enno Mammen. Bootstrap and wild bootstrap for high dimensional linear models. The annals of
statistics, pp. 255–285, 1993."
REFERENCES,0.4444444444444444,"Benedict C May, Nathan Korda, Anthony Lee, and David S Leslie. Optimistic bayesian sampling in
contextual-bandit problems. Journal of Machine Learning Research, 13:2069–2106, 2012."
REFERENCES,0.44666666666666666,"Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence
predictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 427–436, 2015."
REFERENCES,0.4488888888888889,"Lizhen Nie and Veronika Roˇckov´a.
Bayesian bootstrap spike-and-slab lasso.
arXiv preprint
arXiv:2011.14279, 2020."
REFERENCES,0.45111111111111113,Under review as a conference paper at ICLR 2022
REFERENCES,0.4533333333333333,"Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. Advances in neural information processing systems, 29:4026–4034, 2016."
REFERENCES,0.45555555555555555,"Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. Advances in Neural Information Processing Systems, 32:
13991–14002, 2019."
REFERENCES,0.4577777777777778,"Tim Pearce, Alexandra Brintrup, Mohamed Zaki, and Andy Neely. High-quality prediction intervals
for deep learning: A distribution-free, ensembled approach. In International Conference on
Machine Learning, pp. 4075–4084. PMLR, 2018."
REFERENCES,0.46,"Yao Qin, Xuezhi Wang, Alex Beutel, and Ed H Chi. Improving uncertainty estimates through the
relationship with adversarial robustness. arXiv preprint arXiv:2006.16375, 2020."
REFERENCES,0.4622222222222222,"Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical
comparison of bayesian deep networks for thompson sampling. In International Conference on
Learning Representations, 2018."
REFERENCES,0.46444444444444444,"Donald B Rubin. The bayesian bootstrap. The annals of statistics, pp. 130–134, 1981."
REFERENCES,0.4666666666666667,"T´arik S Salem, Helge Langseth, and Heri Ramampiaro. Prediction intervals: Split normal mixture
from quality-driven deep ensembles. In Conference on Uncertainty in Artiﬁcial Intelligence, pp.
1179–1187. PMLR, 2020."
REFERENCES,0.4688888888888889,"Daniel Seita, Xinlei Pan, Haoyu Chen, and John Canny. An efﬁcient minibatch acceptance test
for metropolis-hastings. In Proceedings of the 27th International Joint Conference on Artiﬁcial
Intelligence, pp. 5359–5363, 2018."
REFERENCES,0.4711111111111111,"Jun Shao. Bootstrap model selection. Journal of the American statistical Association, 91(434):
655–665, 1996."
REFERENCES,0.47333333333333333,"Xiaofeng Shao. The dependent wild bootstrap. Journal of the American Statistical Association, 105
(489):218–235, 2010."
REFERENCES,0.47555555555555556,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.4777777777777778,"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine
learning research, 15(1):1929–1958, 2014."
REFERENCES,0.48,"Norman Tasﬁ.
Pygame learning environment.
https://github.com/ntasfi/
PyGame-Learning-Environment, 2016."
REFERENCES,0.4822222222222222,"William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285–294, 1933."
REFERENCES,0.48444444444444446,"Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a
single deep deterministic neural network. In International Conference on Machine Learning, pp.
9690–9700. PMLR, 2020."
REFERENCES,0.4866666666666667,"Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016."
REFERENCES,0.4888888888888889,"Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das, Bharat Kaul, and Theodore L
Willke. Out-of-distribution detection using an ensemble of self supervised leave-out classiﬁers. In
Proceedings of the European Conference on Computer Vision (ECCV), pp. 550–564, 2018."
REFERENCES,0.4911111111111111,"Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &
Business Media, 2013."
REFERENCES,0.49333333333333335,"Jonathan Weed, Francis Bach, et al. Sharp asymptotic and ﬁnite-sample rates of convergence of
empirical measures in wasserstein distance. Bernoulli, 25(4A):2620–2648, 2019."
REFERENCES,0.4955555555555556,Under review as a conference paper at ICLR 2022
REFERENCES,0.49777777777777776,"Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681–688.
Citeseer, 2011."
REFERENCES,0.5,"Tung-Yu Wu, YX Rachel Wang, and Wing H Wong. Mini-batch metropolis–hastings with reversible
sgld proposal. Journal of the American Statistical Association, pp. 1–9, 2020."
REFERENCES,0.5022222222222222,Jeremy Wyatt. Exploration and inference in learning from reinforcement. 1998.
REFERENCES,0.5044444444444445,"Yang Yu, Shih-Kang Chao, and Guang Cheng. Simultaneous inference for massive data: Distributed
bootstrap. In International Conference on Machine Learning, pp. 10892–10901. PMLR, 2020."
REFERENCES,0.5066666666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.5088888888888888,"A
PROOF"
REFERENCES,0.5111111111111111,"We also show Theorem 3, which gives a formal characterization of the Taylor approximation intuition
introduced in (5)."
REFERENCES,0.5133333333333333,"Theorem 3 Under Assumption 1 and 2, when n is sufﬁciently large, we have"
REFERENCES,0.5155555555555555,Lw(θ) = Lw(ˆθw) + 1
REFERENCES,0.5177777777777778,"2

θ −ˆθw
⊤
∇2
θL∞(θ0)

θ −ˆθw
"
REFERENCES,0.52,"+Op

||θ −ˆθw||2(n−1/2 + ||θ −ˆθw||)

."
REFERENCES,0.5222222222222223,Here the stochastic boundedness is taken w.r.t. the training data and w.
REFERENCES,0.5244444444444445,"In the proof, we may use c to represent some absolute constant, which may vary in different lines."
REFERENCES,0.5266666666666666,"A.1
PROOF OF THEOREM 3"
REFERENCES,0.5288888888888889,"With the fact that ∇wL(ˆθw) = 0 and under assumption 1, using Taylor expansion, we have"
REFERENCES,0.5311111111111111,Lw(θ) = Lw(ˆθw) + 1 2
REFERENCES,0.5333333333333333,"
θ −ˆθw
⊤
∇2
θLw(ˆθw)

θ −ˆθw

+ O
θ −ˆθw

3
."
REFERENCES,0.5355555555555556,"Notice that


θ −ˆθw
⊤
∇2
θLw(ˆθw) −∇2
θL∞(θ0)
 
θ −ˆθw
"
REFERENCES,0.5377777777777778,"≤
θ −ˆθw

2 ∇2
θLw(ˆθw) −∇2
θL∞(θ0)"
REFERENCES,0.54,"≤
θ −ˆθw

2 ∇2
θLw(ˆθw) −∇2
θLw(θ0)
 +
∇2
θLw(θ0) −∇2
θL∞(θ0)

"
REFERENCES,0.5422222222222223,"≤
θ −ˆθw

2 
C
ˆθw −θ0
 +
∇2
θLw(θ0) −∇2
θL∞(θ0)

F 
,"
REFERENCES,0.5444444444444444,"where we denote the Frobenius norm as ∥·∥F . With assumption 2, we have
ˆθw −θ0
 = Op(n−1/2)."
REFERENCES,0.5466666666666666,"By applying centroid limit theorem and delta method to
∇2
θijLw(θ0) −∇2
θijL∞(θ0)
 for every pair"
REFERENCES,0.5488888888888889,"i, j ∈[d], we have
∇2
θLw(θ0) −∇2
θL∞(θ0)

F = Op(n−1/2). Thus we obtained the desired result."
REFERENCES,0.5511111111111111,"A.2
PROOF OF THEOREM 1"
REFERENCES,0.5533333333333333,"Given any radius r and ϵ > 0, with sufﬁciently large n, we have"
REFERENCES,0.5555555555555556,"P
ˆθw −θ0
 ≥r

≤exp(−λnr2)+ϵ/m,"
REFERENCES,0.5577777777777778,for λ = 1
REFERENCES,0.56,"4λmax(A)−1. Here the probability is the jointly probability of bootstrap weight and training
data. Thus, given any r, under the assumption that θ∗
j (0) is initialized via sampling ˆθw, then we have"
REFERENCES,0.5622222222222222,"P
 
∪j∈[m]
θ∗
j (0) −θ0
 ≥r
	
≤
X"
REFERENCES,0.5644444444444444,"j∈[m]
P
 θ∗
j (0) −θ0
 ≥r

≤m exp(−λnr2) + ϵ."
REFERENCES,0.5666666666666667,"We proof by induction. Given any {θj}m
j=1, deﬁne"
REFERENCES,0.5688888888888889,"Rk,r = I

w ∈supp(π) : arg min
j∈[m] Lw(θj) = k and
ˆθw −θ0
 ≤r

."
REFERENCES,0.5711111111111111,"Suppose at iteration t, we have ∥θ∗
k(t) −θ0∥≤cα√log n"
REFERENCES,0.5733333333333334,"n
λ0γ
for some constant c and λ0, which we
denote as the minimum eigenvalue of ∇2
θL∞(fθ0). Now at iteration t, we have two cases."
REFERENCES,0.5755555555555556,Under review as a conference paper at ICLR 2022
REFERENCES,0.5777777777777777,"Case 1: EπRk,∞≥γ
Suppose that at iteration t, for k such that EπRk,∞≥γ, and ∥θ∗
k(t) −θ0∥=
qk, we have the following property:"
REFERENCES,0.58,"∥θ∗
k(t + 1) −θ0∥2 =
θ∗
k(t) −
ϵt
EπRk,∞
Eπ [∇θLw(θ∗
k(t))Rk,∞] −θ0  2"
REFERENCES,0.5822222222222222,"= ∥θ∗
k(t) −θ0∥2 −
2ϵt
EπRk,∞
⟨θ∗
k(t) −θ0, Eπ [∇θLw(θ∗
k(t))Rk,∞]⟩+ ϵ2
t ∥Eπ [∇θLw(θ∗
k(t))Rk,∞]∥2 ."
REFERENCES,0.5844444444444444,Notice that
REFERENCES,0.5866666666666667,"Eπ [∇θLw(θ∗
k(t))Rk,qk]
(1)
= Eπ
h
∇2
θLw(ˆθw)(θ∗
k(t) −ˆθw)Rk,qk
i
+ o
 
q2
k
"
REFERENCES,0.5888888888888889,"(2)
= Eπ
h
∇2
θLw(ˆθw)(θ∗
k(t) −θ0)Rk,qk
i
+ O
 
q2
k
"
REFERENCES,0.5911111111111111,"(3)
= Eπ

∇2
θLw(θ0)(θ∗
k(t) −θ0)Rk,qk

+ O
 
q2
k
"
REFERENCES,0.5933333333333334,"Here (1) is obtained via applying Taylor expansion on ∇θLw(θ∗
k(t)) at ˆθw. (2) is by assumption 1
and 2. (3) is by assumption 1. We thus have"
REFERENCES,0.5955555555555555,"−⟨θ∗
k(t) −θ0, Eπ [∇θLw(θ∗
k(t))Rk,∞]⟩
≤−⟨θ∗
k(t) −θ0, Eπ [∇θLw(θ∗
k(t))Rk,qk]⟩+ ∥θ∗
k(t) −θ0∥∥Eπ∇θLw(θ∗
k(t))(1 −Rk,qk)∥"
REFERENCES,0.5977777777777777,"≤−⟨θ∗
k(t) −θ0, Eπ [∇θLw(θ∗
k(t))Rk,qk]⟩+ cqk exp(−λnq2
k)"
REFERENCES,0.6,"≤−EπRk,qk(θ∗
k(t) −θ0)⊤∇2
θLw(θ0)(θ∗
k(t) −θ0) + cqk exp(−λnq2
k) + O
 
q3
k

."
REFERENCES,0.6022222222222222,"Notice that with sufﬁciently large n, with central limit theorem, we have"
REFERENCES,0.6044444444444445,"−EπRk,qk(θ∗
k(t) −θ0)⊤∇2
θLw(θ0)(θ∗
k(t) −θ0)"
REFERENCES,0.6066666666666667,"≤∥θ∗
k(t) −θ0∥2 Eπ
∇2
θLw(θ0) −∇2
θLw(θ0)
 −EπRk,qk(θ∗
k(t) −θ0)⊤∇2
θL∞(θ0)(θ∗
k(t) −θ0)"
REFERENCES,0.6088888888888889,"= −EπRk,qk(θ∗
k(t) −θ0)⊤∇2
θL∞(θ0)(θ∗
k(t) −θ0) + O(n−1/2)."
REFERENCES,0.6111111111111112,This gives that
REFERENCES,0.6133333333333333,"−⟨θ∗
k(t) −θ0, Eπ [∇θLw(θ∗
k(t))Rk,∞]⟩"
REFERENCES,0.6155555555555555,"≤−EπRk,qk(θ∗
k(t) −θ0)⊤∇2
θL∞(θ0)(θ∗
k(t) −θ0) + cqk exp(−λnq2
k) + O

q3
k + qkn−1/2"
REFERENCES,0.6177777777777778,"≤−λ0EπRk,qk ∥θ∗
k(t) −θ0∥2 + cqk exp(−λnq2
k) + O

q3
k + qkn−1/2
."
REFERENCES,0.62,"Use the above estimation, we have"
REFERENCES,0.6222222222222222,"∥θ∗
k(t + 1) −θ0∥2"
REFERENCES,0.6244444444444445,"≤∥θ∗
k(t) −θ0∥2 −2ϵtλmin
EπRk,qk
EπRk,∞
∥θ∗
k(t) −θ0∥2"
REFERENCES,0.6266666666666667,"+2cϵtqk exp(−λnq2
k)/EπRk,∞+ O

ϵt(q3
k + qkn−1/2)/EπRk,∞+ ϵ2
t
"
REFERENCES,0.6288888888888889,"≤∥θ∗
k(t) −θ0∥2 +
ϵt
EπRk,∞"
REFERENCES,0.6311111111111111,"
−2λ0EπRk,qk ∥θ∗
k(t) −θ0∥2 −2cqk exp(−λnq2
k) + O

q3
k + ϵt + qkn−1/2
."
REFERENCES,0.6333333333333333,"Notice that by choosing α >
p"
REFERENCES,0.6355555555555555,"1/(2λ) and ϵt = O(n−1), with sufﬁciently large n, when"
REFERENCES,0.6377777777777778,"∥θ∗
k(t) −θ0∥≥
cα
q log n"
REFERENCES,0.64,"n
λ0EπRk,qk"
REFERENCES,0.6422222222222222,"for some constant c, we have"
REFERENCES,0.6444444444444445,"∥θ∗
k(t + 1) −θ0∥≤∥θ∗
k(t) −θ0∥."
REFERENCES,0.6466666666666666,"Thus ∥θ∗
k(t + 1) −θ0∥≤
cα√log n"
REFERENCES,0.6488888888888888,"n
λ0EπRk,∞≤cα√log n"
REFERENCES,0.6511111111111111,"n
λ0γ
for some constant c."
REFERENCES,0.6533333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.6555555555555556,"Case 2: EπRk,∞≤γ
In this case, we have"
REFERENCES,0.6577777777777778,"∥θ∗
k(t + 1) −θ0∥2 =
θ∗
k(t) −ϵt∇θL(fθ∗
k(t)) −θ0

2"
REFERENCES,0.66,"= ∥θ∗
k(t) −θ0∥2 −2ϵt ⟨θ∗
k(t) −θ0, ∇θL(θ∗
k(t))⟩+ ϵ2
t
∇θL(fθ∗
k(t))

2
."
REFERENCES,0.6622222222222223,"Notice that
−⟨θ∗
k(t) −θ0, ∇θL(θ∗
k(t))⟩≤−

θ∗
k(t) −θ0, ∇2
θL(θ0) (θ∗
k(t) −θ0)

−⟨θ∗
k(t) −θ0, ∇θL(fθ0)⟩+ o(||θ∗
k(t) −θ0||3)"
REFERENCES,0.6644444444444444,"= −(θ∗
k(t) −θ0)⊤∇2
θL∞(θ0) (θ∗
k(t) −θ0) + o(||θ∗
k(t) −θ0||3) + Op(n−1/2)||θ∗
k(t) −θ0||.
This gives that"
REFERENCES,0.6666666666666666,"∥θ∗
k(t + 1) −θ0∥2 ≤∥θ∗
k(t) −θ0∥2−2ϵtλ0 ∥θ∗
k(t) −θ0∥2+o(ϵt||θ∗
k(t)−θ0||3+ϵ2
t)+Op(n−1/2)ϵt||θ∗
k(t)−θ0||."
REFERENCES,0.6688888888888889,"With ϵt = O(n−1) and sufﬁciently large n, when"
REFERENCES,0.6711111111111111,"∥θ∗
k(t) −θ0∥≥
cα
q log n"
REFERENCES,0.6733333333333333,"n
λ0γ
,"
REFERENCES,0.6755555555555556,"we have ∥θ∗
k(t + 1) −θ0∥2 ≤∥θ∗
k(t) −θ0∥2."
REFERENCES,0.6777777777777778,"Combine this two cases, we conclude that ∥θ∗
k(t + 1) −θ0∥≤
cα√log n"
REFERENCES,0.68,"n
λ0γ
for any t, when"
REFERENCES,0.6822222222222222,"∥θ∗
k(0) −θ0∥≤
cα√log n"
REFERENCES,0.6844444444444444,"n
λ0γ
. We thus conclude that, for any α >
p"
REFERENCES,0.6866666666666666,"1/(2λ) and ϵ > 0, when n"
REFERENCES,0.6888888888888889,"is sufﬁciently large, with probability at least 1 −m exp

−λ cα2 log n"
REFERENCES,0.6911111111111111,"λ2
0γ2

−ϵ, we have"
REFERENCES,0.6933333333333334,"max
j∈[m] sup
t"
REFERENCES,0.6955555555555556,"θ∗
j (t) −θ0
 ≤
cα
q log n"
REFERENCES,0.6977777777777778,"n
λ0γ
."
REFERENCES,0.7,"A.3
PROOF FOR THEOREM 2"
REFERENCES,0.7022222222222222,Notice that
REFERENCES,0.7044444444444444,"Lw(θ∗
j (t)) −Lw(ˆθw) = 1 2"
REFERENCES,0.7066666666666667,"
θ∗
j (t) −ˆθw
⊤
∇2
θLw(ˆθw)

θ∗
j (t) −ˆθw

+ O(
θ∗
j (t) −ˆθw

3
) = 1 2"
REFERENCES,0.7088888888888889,"
θ∗
j (t) −ˆθw
⊤
∇2
θLw(θ0)

θ∗
j (t) −ˆθw

+ O(
θ∗
j (t) −ˆθw

3
) + O(
θ∗
j (t) −ˆθw

2 ˆθw −θ0
) = 1 2"
REFERENCES,0.7111111111111111,"θ∗
j (t) −ˆθw

2"
REFERENCES,0.7133333333333334,"D +
∇2
θLw(θ0) −∇2
θL∞(θ0)

θ∗
j (t) −ˆθw

2
+ O(
θ∗
j (t) −ˆθw

3
)"
REFERENCES,0.7155555555555555,"+ O(
θ∗
j (t) −ˆθw

2 ˆθw −θ0
)"
REFERENCES,0.7177777777777777,"Given w, we deﬁne uw = arg minj∈[m]
θ∗
j (t) −ˆθw

2"
REFERENCES,0.72,"D. For any α >
p"
REFERENCES,0.7222222222222222,"1/(2λ) and ϵ > 0, when n"
REFERENCES,0.7244444444444444,"is sufﬁciently large, with probability at least 1 −m exp

−λ cα2 log n"
REFERENCES,0.7266666666666667,"λ2
0γ2

−ϵ, we have"
REFERENCES,0.7288888888888889,"1
2Ew∼π"
REFERENCES,0.7311111111111112,"
min
j∈[m]"
REFERENCES,0.7333333333333333,"θ∗
j (t) −ˆθw

2 D  =1 2Ew∼π"
REFERENCES,0.7355555555555555,"θ∗
uw −ˆθw

2 D  ≥Ew∼π"
REFERENCES,0.7377777777777778,"
Lw(θ∗
uw) −Lw(ˆθw) −c
θ∗
uw −ˆθw

2 ˆθw −θ0
 +
∇2
θLw(θ0) −∇2
θL∞(θ0)
 +
θ∗
uw −ˆθw

"
REFERENCES,0.74,"≥Ew∼π

Lw(θ∗
uw)

−Ew∼π
h
Lw(ˆθw)
i
−c
α√log n"
REFERENCES,0.7422222222222222,λ0γn3/2  =Ew∼π
REFERENCES,0.7444444444444445,"
min
j∈[m] Lw(θ∗
j (t))

−Ew∼π
h
Lw(ˆθw)
i
−c
α√log n"
REFERENCES,0.7466666666666667,"λ0γn3/2 
."
REFERENCES,0.7488888888888889,Under review as a conference paper at ICLR 2022
REFERENCES,0.7511111111111111,"Algorithm 1 Ideal algorithm for centroid approximation with full-batch gradient used and wh
updated every iteration."
REFERENCES,0.7533333333333333,"1: Initialize θ∗
j (0), j ∈[m] by i.i.d. sampling from ρπ or other distribution such as Gaussian.
2: for t ∈iterations do
3:
∀j ∈[m], calculate L(θ∗
j (t)) deﬁned in (11)
4:
Sample {wh}M
h=1, i.i.d. from π.
5:
∀h ∈[M] and j ∈[m], calculate Lwh(θ∗
j (t)) = wT
h L(θ∗
j (t)).
6:
∀h ∈[M], calculate uwh deﬁned in (12) for each h.
7:
∀j ∈[m], update θ∗
j by (13).
8: end for"
REFERENCES,0.7555555555555555,"Similarly, we also have, with probability at least 1 −m exp

−λ cα2 log n"
REFERENCES,0.7577777777777778,"λ2
0γ2

, Ew∼π"
REFERENCES,0.76,"
min
j∈[m] Lw(θ∗
j (t))

−Ew∼π
h
Lw(ˆθw)
i
≥1 2Ew∼π"
REFERENCES,0.7622222222222222,"
min
j∈[m]"
REFERENCES,0.7644444444444445,"θ∗
j (t) −ˆθw

2 D"
REFERENCES,0.7666666666666667,"
−c
α√log n"
REFERENCES,0.7688888888888888,"λ0γn3/2 
."
REFERENCES,0.7711111111111111,"Notice that the above bound holds uniformly for all j ∈[m] and any iteration t, which implies that"
REFERENCES,0.7733333333333333,"with probability at least 1 −2m exp

−λ cα2 log n"
REFERENCES,0.7755555555555556,"λ2
0γ2

−2ϵ, we have"
REFERENCES,0.7777777777777778,"sup
t≥0"
REFERENCES,0.78,"Ew∼π[ min
j∈[m] Lw(θ∗
j (t))] −B −Ew∼π[ min
j∈[m] ||θ∗
j (t) −ˆθw||2
D]/2
 ≤c
α√log n"
REFERENCES,0.7822222222222223,"λ0γn3/2 
."
REFERENCES,0.7844444444444445,"B
ALGORITHM BOX"
REFERENCES,0.7866666666666666,"We provide pseudo algorithm for the ideal centroid approximation algorithm in Algorithm 1. In
practical implementation, we do not need to update wh every iteration and can also replace the
full-batch gradient by stochastic gradient. Speciﬁcally, notice that
ˆ
g(θ∗
j ) deﬁned in (10) can be
alternative represented as"
REFERENCES,0.7888888888888889,"ˆg(θ∗
j (t)) ="
REFERENCES,0.7911111111111111,"PM
h=1
Pn
i=1 [I{j ∈uwh(t)}] wh,i∇θℓ(xi, fθ∗
j (t))/n
PM
h=1 [I{j ∈uwh}]
= 1 n n
X"
REFERENCES,0.7933333333333333,"i=1
qi,j∇θℓ(xi, fθ∗
j (t)), (14)"
REFERENCES,0.7955555555555556,"where qi,j is deﬁned by"
REFERENCES,0.7977777777777778,"qij :=
PM
h=1
Pn
i=1 [I{j ∈uwh(t)}] wh,i
PM
h=1 [I{j ∈uwh(t)}]
.
(15)"
REFERENCES,0.8,This allows us to use a stochastic gradient version of gradient
REFERENCES,0.8022222222222222,"ˆgsgd(θ∗
j ) =
1
|B| X"
REFERENCES,0.8044444444444444,"i∈[B]
qi,j∇θℓ(xi, fθ∗
j ),
(16)"
REFERENCES,0.8066666666666666,where B is the set of mini-batch data. The detailed algorithm is summarized in Algorithm
REFERENCES,0.8088888888888889,"C
ADDITIONAL EXPERIMENT DETAILS"
REFERENCES,0.8111111111111111,"C.1
BOOTSTRAP CONFIDENCE INTERVAL"
REFERENCES,0.8133333333333334,"Given a model fθ parameterized by θ and a training set with n data points i.i.d. sampled from
population, our goal is to construct conﬁdence interval for θ. Let ˜ρπ be an empirical distribution
approximating ρπ, which could be obtained by i.i.d. sampling, or by our centroid method. Denote
by Q[α, ˜ρπ] the α-quantile function of ˜ρπ with some α ∈[0, 1]. We consider the following three
ways to construct (two-sided) bootstrap conﬁdence interval of θ with conﬁdence level α: the Normal
interval, the percentile interval and the pivotal interval which are deﬁned below."
REFERENCES,0.8155555555555556,Under review as a conference paper at ICLR 2022
REFERENCES,0.8177777777777778,"Algorithm 2 Practical implementation of centroid approximation with less frequent updating of wh
and stochastic gradient enabled."
REFERENCES,0.82,"1: Initialize θ∗
j (0), j ∈[m] by i.i.d. sampling from ρπ or other distribution such as Gaussian.
2: for t ∈iterations do
3:
// Update wh only every a few iterations.
4:
if t mod freq == 0 then
5:
∀j ∈[m], calculate L(θ∗
j (t)) deﬁned in (11)
6:
Sample {wh}M
h=1, i.i.d. from π.
7:
∀h ∈[M] and j ∈[m], calculate Lwh(θ∗
j (t)) = wT
h L(θ∗
j (t)).
8:
∀h ∈[M], calculate uwh(t) deﬁned in (12) for each h.
9:
else
10:
uwh(t) = uwh(t −1)
11:
end if
12:
∀j ∈[m], update θ∗
j (t) by (13). (May use mini-batch gradient deﬁned in (16)).
13: end for"
REFERENCES,0.8222222222222222,"Methods to construct conﬁdence interval
The methods we used to construct conﬁdence interval
are – The Normal interval:"
REFERENCES,0.8244444444444444,"[ˆθ −z((1 + α)/2) ˆseboot, ˆθ + z((1 + α)/2) ˆseboot],"
REFERENCES,0.8266666666666667,"where z(·) is the inverse cumulative distribution function of standard Normal distribution. And ˆseboot
is the standard deviation estimated from ˜ρπ."
REFERENCES,0.8288888888888889,– The percentile intervals:
REFERENCES,0.8311111111111111,"[Q[(1 −α)/2, ˜ρπ], Q[(1 + α)/2, ˜ρπ]]."
REFERENCES,0.8333333333333334,– The pivotal interval:
REFERENCES,0.8355555555555556,"[2ˆθ −Q[(1 + α)/2, ˜ρπ], 2ˆθ −Q[(1 −α)/2, ˜ρπ]]."
REFERENCES,0.8377777777777777,"We consider the following simple linear regression: x ∼N(0, I), y | x ∼N(θ⊤x, I), where the
features x ∈R4 and we set the true parameter to be θ0 = [1, −1, 1, −1]. We consider n = 50 and the
number of particles m = 20, 50, 100, 200. We compare the coverage probability and the conﬁdence
level α to measure the quality:"
REFERENCES,0.84,"Measuring the quality of conﬁdence interval
With a large number N of independently generated
training data (we use N = 1000), we are able to obtain the corresponding conﬁdence intervals
{CI(α)s}N
s=1and thus obtain the probability that the true parameter falls into the conﬁdence intervals,
which is the estimated coverage probability"
REFERENCES,0.8422222222222222,"ˆα = 1 N N
X"
REFERENCES,0.8444444444444444,"s=1
I{θ0 ∈CI(α)s}."
REFERENCES,0.8466666666666667,"A good conﬁdence interval should have ˆα close to α. Thus we measure the performance by calculating
the difference |α −ˆα|."
REFERENCES,0.8488888888888889,"As ˆθw is the least square estimator of the bootstrapped dataset, it has analytic solution and thus can
be obtained via some matrix multiplications. θ∗
w is initialized using ˆθw and then updated for 2000
steps. For this experiment, we ﬁnd that adding the threshold γ does not gives further improvement
for this experiment and thus we simply set γ = 0 and use M = 1. We approximate the true bootstrap
distribution by sampling 10000 i.i.d. samples."
REFERENCES,0.8511111111111112,"More experiment result
Table 3 all the result we have varying α = 0.8, 0.9, 0.95, m =
20, 50, 100, 200 and three different approaches for constructing conﬁdence interval. As we can
see, centroid approximation gives the best performance in most cases compared with the other three
baselines."
REFERENCES,0.8533333333333334,Under review as a conference paper at ICLR 2022
REFERENCES,0.8555555555555555,"Num Particle
20
50
100
200"
REFERENCES,0.8577777777777778,α = 0.8
REFERENCES,0.86,Normal
REFERENCES,0.8622222222222222,"Bootstrap
0.033 ± 0.013
0.028 ± 0.013
0.026 ± 0.013
0.031 ± 0.013
Bayesian
0.084 ± 0.014
0.076 ± 0.014
0.082 ± 0..014
0.086 ± 0.014
Residual
0.033 ± 0.013
0.037 ± 0.013
0.029 ± 0.013
0.024 ± 0.013
Centroid
0.036 ± 0.013
0.003 ± 0.012
0.017 ± 0.013
0.030 ± 0.013"
REFERENCES,0.8644444444444445,Percentile
REFERENCES,0.8666666666666667,"Bootstrap
0.096 ± 0.014
0.050 ± 0.014
0.044 ± 0.013
0.024 ± 0.013
Bayesian
0.114 ± 0.015
0.079 ± 0.014
0.074 ± 0.014
0.071 ± 0.014
Residual
0.079 ± 0.014
0.032 ± 0.013
0.017 ± 0.013
0.010 ± 0.013
Centroid
0.066 ± 0.014
0.008 ± 0.013
0.019 ± 0.013
0.020 ± 0.013"
REFERENCES,0.8688888888888889,Pivotal
REFERENCES,0.8711111111111111,"Bootstrap
0.101 ± 0.015
0.053 ± 0.014
0.045 ± 0.014
0.033 ± 0.013
Bayesian
0.158 ± 0.015
0.110 ± 0.110
0.088 ± 0.014
0.078 ± 0.014
Residual
0.087 ± 0.014
0.044 ± 0.013
0.030 ± 0.013
0.023 ± 0.013
Centroid
0.026 ± 0.013
0.030 ± 0.012
0.018 ± 0.013
0.030 ± 0.013"
REFERENCES,0.8733333333333333,α = 0.9
REFERENCES,0.8755555555555555,Normal
REFERENCES,0.8777777777777778,"Bootstrap
0.029 ± 0.010
0.031 ± 0.011
0.021 ± 0.010
0.017 ± 0.010
Bayesian
0.076 ± 0.012
0.054 ± 0.011
0.048 ± 0.011
0.045 ± 0.011
Residual
0.043 ± 0.011
0.023 ± 0.010
0.025 ± 0.010
0.020 ± 0.010
Centroid
0.027 ± 0.010
0.001 ± 0.009
0.012 ± 0.010
0.016 ± 0.010"
REFERENCES,0.88,Percentile
REFERENCES,0.8822222222222222,"Bootstrap
0.101 ± 0.013
0.036 ± 0.011
0.021 ± 0.010
0.014 ± 0.010
Bayesian
0.129 ± 0.013
0.077 ± 0.012
0.059 ± 0.012
0.054 ± 0.011
Residual
0.098 ± 0.013
0.030 ± 0.011
0.033 ± 0.011
0.025 ± 0.010
Centroid
0.081 ± 0.012
0.021 ± 0.010
0.020 ± 0.010
0.015 ± 0.010"
REFERENCES,0.8844444444444445,Pivotal
REFERENCES,0.8866666666666667,"Bootstrap
0.106 ± 0.013
0.045 ± 0.011
0.025 ± 0.010
0.023 ± 0.010
Bayesian
0.149 ± 0.014
0.093 ± 0.012
0.073 ± 0.012
0.056 ± 0.011
Residual
0.100 ± 0.013
0.044 ± 0.011
0.030 ± 0.011
0.023 ± 0.010
Centroid
0.046 ± 0.011
0.013 ± 0.009
0.011 ± 0.010
0.020 ± 0.010"
REFERENCES,0.8888888888888888,α = 0.95
REFERENCES,0.8911111111111111,Normal
REFERENCES,0.8933333333333333,"Bootstrap
0.018 ± 0.008
0.014 ± 0.008
0.012 ± 0.008
0.006 ± 0.007
Bayesian
0.053 ± 0.010
0.038 ± 0.009
0.031 ± 0.009
0.037 ± 0.009
Residual
0.036 ± 0.009
0.019 ± 0.008
0.011 ± 0.008
0.008 ± 0.007
Centroid
0.018 ± 0.008
0.005 ± 0.006
0.009 ± 0.007
0.005 ± 0.007"
REFERENCES,0.8955555555555555,Percentile
REFERENCES,0.8977777777777778,"Bootstrap
0.081 ± 0.010
0.047 ± 0.009
0.030 ± 0.008
0.017 ± 0.008
Bayesian
0.126 ± 0.012
0.072 ± 0.010
0.056 ± 0.010
0.042 ± 0.009
Residual
0.100 ± 0.011
0.040 ± 0.009
0.037 ± 0.009
0.021 ± 0.008
Centroid
0.077 ± 0.010
0.029 ± 0.008
0.020 ± 0.008
0.016 ± 0.008"
REFERENCES,0.9,Pivotal
REFERENCES,0.9022222222222223,"Bootstrap
0.089 ± 0.011
0.043 ± 0.009
0.027 ± 0.008
0.015 ± 0.008
Bayesian
0.127 ± 0.012
0.091 ± 0.011
0.064 ± 0.010
0.056 ± 0.010
Residual
0.085 ± 0.011
0.051 ± 0.009
0.036 ± 0.009
0.029 ± 0.008
Centroid
0.046 ± 0.009
0.002 ± 0.007
0.014 ± 0.008
0.009 ± 0.007"
REFERENCES,0.9044444444444445,"Table 3: Complete result on comparing centroid approximation with various bootstrap methods. The
bold number shows the best approach."
REFERENCES,0.9066666666666666,Under review as a conference paper at ICLR 2022
REFERENCES,0.9088888888888889,Algorithm 3 Algorithm for Centroid Approximation Applied to Contextual Bandit.
REFERENCES,0.9111111111111111,"1: Obtain a randomly initialized θ∗
j (0), j ∈[m].
2: Initialize a common replay buffer Rc = ∅recording all the observed contexts.
3: For each model, initialize its own replay buffer Rj = ∅that is used for training.
4: for t ∈number of total steps do
5:
Obtain the t-th context xt.
6:
Sampling one model based on probability {v∗
j (t)}m
j=1 to make action at and get reward rt.
7:
Update the common replay buffer by Rc ←Rc ∪{(xt, at, rt)}
8:
// Update wh and Rj and model every a few iterations.
9:
if t mod freq == 0 then
10:
∀j ∈[m], calculate L(θ∗
j (t)) deﬁned in (11) for all the contexts in Rc. // L(θ∗
j (t)) ∈R|Rc|.
11:
Generate M sets of random weights {wh}M
h=1 of contexts in Rc from π.
12:
∀h ∈[M] and j ∈[m], calculate Lwh(θ∗
j (t)) = wT
h L(θ∗
j (t)).
13:
∀h ∈[M], calculate uwh(t) deﬁned in (12) for each h.
14:
∀i ∈[|Rc|] and j ∈[m], calculate qi,j by (15)
15:
∀j ∈[m], update v∗
j (t) based on (7).
16:
∀j ∈[m], if v∗
j (t) ≤γ, construct Rj = Rc, else, construct Rj by sample |Rc| contexts"
REFERENCES,0.9133333333333333,"in Rc. The probability that context i is being sampled is qi,j/ P|Rc|
i=1 qi,j.
17:
∀j ∈[m], train model j using the data in Rj for several iterations.
18:
end if
19: end for"
REFERENCES,0.9155555555555556,"C.2
CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT"
REFERENCES,0.9177777777777778,"C.2.1
MORE BACKGROUND"
REFERENCES,0.92,"Contextual bandit is a classic task in sequential decision making problem in which at time t = 1, ..., n,
a new context xt arrives and is observed by an algorithm. Based on its internal model, the algorithm
selects an actions at and receives a reward rt(xt, at) related to the context and action. During this
process, the algorithm may update its internal model with the newly received data. At the end of
this process, the cumulative reward of the algorithm is calculated by r = Pn
t=1 rt and the goal for
the algorithm is to improve the cumulative reward r. The exploration-exploitation dilemma is a
fundamental aspect in sequential decision making problem such as contextual bandit: the algorithm
needs to trade-off between the best expected action returned by the internal model at the moment
(i.e., exploitation) with potentially sub-optimal exploratory actions. Thompson sampling (Thompson,
1933; Wyatt, 1998; May et al., 2012) is an elegant and effective approach to tackle the exploration-
exploitation dilemma using the model uncertainty, which can be approached with various methods
including Bayesian posterior (Graves, 2011; Welling & Teh, 2011), dropout uncertainty (Srivastava
et al., 2014; Hron et al., 2017) and Bootstrap (Osband et al., 2016; Hao et al., 2019). The ability
to accurately assess the uncertainty is a key to improve the cumulative reward. Bootstrap method
for contextual bandit maintains m bootstrap models trained with different bootstrapped training set.
When conducting an action, the algorithm uniformly samples a model and then selects the best action
returned by the sampled model."
REFERENCES,0.9222222222222223,"C.2.2
MORE EXPERIMENT SETUP DETAILS"
REFERENCES,0.9244444444444444,"We set all the experimental setting including data preprocessing, network architecture and training
pipeline exactly the same as the one used in Riquelme et al. (2018) and adopt its open source code
repository."
REFERENCES,0.9266666666666666,"Network architecture Following Riquelme et al. (2018), we consider a fully connected feed forward
network with two hidden layers with 50 hidden units and ReLU activations. The input and output
dimensions depends on the dimension of context and number of possible actions."
REFERENCES,0.9288888888888889,"Training For each dataset, we randomly generate 2000 contexts, and for each algorithm, we update
the replay memory buffer for each model every 50 contexts, and each model is also updated every
50 contexts. For the standard bootstrap, when updating the replay buffer of each model, we sample
50 i.i.d. contexts with uniform probability from the latest 50 contexts (each model have different"
REFERENCES,0.9311111111111111,Under review as a conference paper at ICLR 2022
REFERENCES,0.9333333333333333,"realizations of the samples) and add the newly sampled contexts to each model’s replay buffer. For
the centroid approximation, we update the replay buffer of each model by applying resampling on all
the observed contexts up to the current steps. The resampling probability of each context for each
model is different and determined by the algorithm. We refer readers to Algorithm 3 for the detailed
procedures. Here we choose freq = 50 and M = 100. When at model updating, each model is trained
for 100 iterations with batch size 512 using the data from its replay buffer. Following Riquelme et al.
(2018), we use RMSprop optimizer with learning rate 0.1 for optimizing. When making actions,
we sample the prediction head according to v∗
k(t) obtained using the examples between the last two
model updates."
REFERENCES,0.9355555555555556,"Notice that in the implementation, we only need to maintain one common replay buffer and the
replay buffer for each model can be implemented by maintaining the number of each context. Thus
when sampling batches of context, we simply need to sample the index of the context and refer to the
common replay buffer to get the actual data."
REFERENCES,0.9377777777777778,"C.3
CENTROID APPROXIMATION FOR BOOTSTRAP DQN"
REFERENCES,0.94,"C.3.1
MORE BACKGROUND"
REFERENCES,0.9422222222222222,"Similar to the bootstrap method in contextual bandit problem, Bootstrap DQN explores using the
model uncertainty, which can be assessed via maintaining several models trained with bootstrapped
training set. Maintaining several independent models can be very expensive in RL and to reduce the
computational cost, Bootstrap DQN uses a multi-head network with a shared base. Each head in the
network corresponds to a bootstrap model and the common shared base is thus trained via the union
of the bootstrap training set of each head. We train the Bootstrap DQN with standard updating rule
for DQN and use Double-DQN (Van Hasselt et al., 2016) to reduce the overestimate issue. Notice
that our centroid approximation method only changes the memory buffer for each head and thus
introduces no conﬂict to other possible techniques that can be applied to Bootstrap DQN."
REFERENCES,0.9444444444444444,"C.3.2
MORE EXPERIMENT SETUP DETAILS"
REFERENCES,0.9466666666666667,"Network Architecture Following Osband et al. (2016), we considered multi-head network structure
with a shared base layer to save the memory. Speciﬁcally, we use a fully connected layer with 256
hidden neurons as the shared base and stack two fully connected layers each with 256 hidden neurons
as head. Each head in the model can be viewed as one bootstrap particles and in computation, all the
bootstrap particles use the same base layer."
REFERENCES,0.9488888888888889,"Training and Evaluation For LunarLander-v2, we train the model for 450 episodes with the ﬁrst
50 episodes used to initialize the common memory buffer. The maximum number of steps within
each episode is set to 1000 and we report the moving average reward with window width 100. For
Catcher-v0, we train the model for 100 episodes with the ﬁrst 10 episodes used to initialize the
common memory buffer. We set the maximum number of steps within each episodes 2000 and report
the moving average reward with window width 25."
REFERENCES,0.9511111111111111,"For training the Bootstrap DQN, given the current state xt, we sample one particle based on {v∗
j }m
j=1
and use its policy network to make an action at and get the reward rt and next state xt+1. The
Q-value of the state action pair Q(xt, at) is estimated by rt + λ ∗ˆQ(xt+1), where ˆQ(xt+1) is the
predicted state value by the target network of the sampled particle and λ is the discount factor set
to be 0.99. At each step, the policy network of all particles are updated using one step gradient
descent with Adam optimizer (β = (0.9, 0.999) and learning rate 0.001) and mini-batch data (size
64) sampled from its replay buffer. We update target model, each particle’s replay buffer and v∗
j s
every 1000 steps for LunarLander-v2 and every 200 steps for Catcher-v0. The update scheme for
replay buffers of each particles and v∗
j s is the same as the one in contextual bandit experiment. As
the model see signiﬁcantly larger number of contexts than that in the contextual bandit experiment, to
reduce the memory consumption, we set the max capacity of the common replay buffer to 50000
(the oldest data point will be pop out when the size reaches maximum and new data comes in). For
training the shared base, following Osband et al. (2016), we adds up all the gradient comes from each
head and normalizes it by the number of heads. Algorithm 4 summarizes the whole training pipeline."
REFERENCES,0.9533333333333334,Under review as a conference paper at ICLR 2022
REFERENCES,0.9555555555555556,Algorithm 4 Algorithm for Centroid Approximation Applied to DQN.
REFERENCES,0.9577777777777777,"1: Obtain a randomly initialized θ∗
j (0), j ∈[m]. (For the j-th particle, both of its target and policy
network use the same initialization.)
2: Initialize a common replay buffer Rc = ∅recording all the observed contexts.
3: For each head, initialize its own replay buffer Rj = ∅that is used for training.
4: for t ∈number of total episodes do
5:
while not at terminal state and the number of steps does not exceed the threshold do
6:
Obtain the t-th context xt.
7:
Sample an head based on {v∗
j } to make action at and get Q(xt, at) using the reward rt
and the prediction of the corresponding target network.
8:
Update the common replay buffer by Rc ←Rc ∪{(xt, Q(xt, at))}
9:
∀j ∈[m], update its policy network by one step gradient descent using the data from its
reply buffer.
10:
// Update wh and Rj and target network every a few iterations.
11:
if t mod freq == 0 then
12:
∀j ∈[m], calculate L(θ∗
j (t)) deﬁned in (11) for all the contexts in Rc.
13:
Generate M sets of random weights {wh}M
h=1 of contexts in Rc from π.
14:
∀h ∈[M] and j ∈[m], calculate Lwh(θ∗
j (t)) = wT
h L(θ∗
j (t)).
15:
∀h ∈[M], calculate uwh(t) deﬁned in (12) for each h.
16:
∀i ∈[|Rc|] and j ∈[m], calculate qi,j by (15)
17:
∀j ∈[m], update v∗
j (t) based on (7).
18:
∀j ∈[m], if v∗
j (t) ≤γ, construct Rj = Rc, else, construct Rj by sample |Rc|"
REFERENCES,0.96,"contexts in Rc. The probability that context i is being sampled is qi,j/ P|Rc|
i=1 qi,j.
19:
∀j ∈[m], update the j-th target network by loading the weights of the j-th policy
network.
20:
end if
21:
end while
22: end for"
REFERENCES,0.9622222222222222,"3
4
5
6
7
8
9
10
Num particles 71 72 73 74 75 76 Top-1"
REFERENCES,0.9644444444444444,"Centroid
Bootstrap"
REFERENCES,0.9666666666666667,"3
4
5
6
7
8
9
10
Num particles 91.0 91.5 92.0 92.5 Top-5"
REFERENCES,0.9688888888888889,"Centroid
Bootstrap"
REFERENCES,0.9711111111111111,Figure 4: Results on ensemble modeling with bootstrap using Vgg16 on CIFAR-100.
REFERENCES,0.9733333333333334,"C.4
BOOTSTRAP ENSEMBLE MODEL"
REFERENCES,0.9755555555555555,"Ensemble of deep neural networks have been successfully used to boost predictive performance
(Lakshminarayanan et al., 2017). In this experiment, we consider using an ensemble of deep neural
network trained on different bootstrapped training set, which is also known as a popular strategy
called bagging."
REFERENCES,0.9777777777777777,"We consider image classiﬁcation task on CIFAR-100 and use standard VGG-16 (Simonyan &
Zisserman, 2014) with batch normalization. We apply a standard training pipeline. We train the
bootstrap model for 160 epochs using SGD optimizer with 0.9 momentum and batchsize 128. The
learning rate is initialized to be 0.1 and is decayed by a factor of 10 at epoch 80 and 120. We start
to apply the centroid approximation at epoch 120 (thus the centroid is initialized with 120 epochs’
training). We generate the bootstrap training set for each centroid every epoch using the proposed
centroid approximation method. We consider m = 3, 4, 5, 10 ensembles and use γ = 0.5/m. We"
REFERENCES,0.98,Under review as a conference paper at ICLR 2022
REFERENCES,0.9822222222222222,"#Particle
γ = 0
γ = 0.5
γ = m
3
3480.0 ± 120
3702.7 ± 89.8
3467.7 ± 115
4
3461.92 ± 126
3723.1 ± 78.7
3600.0 ± 69.3
5
3586.5 ± 64.5
3799.6 ± 84.2
3647.3 ± 64.5
10
3785.0 ± 59.1
3796.9 ± 36.1
3742.7 ± 86.8"
REFERENCES,0.9844444444444445,Table 4: Ablation study.
REFERENCES,0.9866666666666667,"repeat the experiment for 3 random trials and report the averaged top1 and top5 accuracy with the
standard deviation of the mean estimator. Algorithm"
REFERENCES,0.9888888888888889,"Figure 4 summarizes the result. Overall, increasing m is able to improve the predictive performance
and with the same number of models, our centroid approximation consistently improves over standard
bootstrap ensembles."
REFERENCES,0.9911111111111112,"C.5
ABLATION STUDY"
REFERENCES,0.9933333333333333,"We study the effectiveness of using (8) to modify the gradient of centroid with v∗
k(t) ≤γ. We
consider the setting γ = 0 (no modiﬁcation) and γ = m (always modify, equivalent to no bootstrap
uncertainty) and applied the method on the mushroom dataset in the contextual bandit problem. Table
4 shows that (i) modifying the gradient of centroid with small v∗
k(t) using do improve the overall
result; (2) bootstrap uncertainty is important for exploration."
REFERENCES,0.9955555555555555,"C.6
COMPUTATION OVERHEAD"
REFERENCES,0.9977777777777778,"Our main goal is not to decrease the training cost but improve the quality of bootstrap partical
distribution so that we can use less models at deployment and hence reduce the memory cost and
the computational cost for inference. Actually, as discussed in Section 3, our method actually only
introduces a little computation overhead while much improves the quality of the particles, which
is another advantage of our method. For example, in bandit problem on mushroom dataset, when
m = 3, 10, vanilla bootstrap takes 33s, 101s while our approach takes 35s, 108s per run. For the
bagging, when m = 3, 10, vanilla bootstrap takes 11200s, 34240s while ours take 12000s, 36200s.
Results are based on the average of 3 runs. Our method only introduces about 7% computational
overhead even with an naive implementation."
