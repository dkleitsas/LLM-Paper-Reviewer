Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003952569169960474,"Diffusion models have emerged as an expressive family of generative models ri-
valing GANs in sample quality and autoregressive models in likelihood scores.
Standard diffusion models typically require hundreds of forward passes through
the model to generate a single high-ﬁdelity sample. We introduce Differentiable
Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any
pre-trained diffusion model by differentiating through sample quality scores. We
present Generalized Gaussian Diffusion Models (GGDM), a family of ﬂexible
non-Markovian samplers for diffusion models. We show that optimizing the de-
grees of freedom of GGDM samplers by maximizing sample quality scores via
gradient descent leads to improved sample quality. Our optimization procedure
backpropagates through the sampling process using the reparametrization trick
and gradient rematerialization. DDSS achieves strong results on unconditional im-
age generation across various datasets (e.g., FID scores on LSUN church 128x128
of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and
14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any
pre-trained diffusion model without ﬁne-tuning or re-training required."
INTRODUCTION,0.007905138339920948,"1
INTRODUCTION"
INTRODUCTION,0.011857707509881422,"Denoising Diffusion Probabilistic Models (DDPM) (Sohl-Dickstein et al., 2015; Song & Ermon,
2019; Ho et al., 2020) have emerged as a powerful family of generative models, capable of synthe-
sizing high-quality images, audio, and 3D shapes (Ho et al., 2020; 2021; Chen et al., 2021a;b; Cai
et al., 2020; Luo & Hu, 2021). Recent work (Dhariwal & Nichol, 2021; Ho et al., 2021) shows that
DDPMs can outperform Generative Adversarial Networks (GAN) (Goodfellow et al., 2014; Brock
et al., 2018) in generation quality, but unlike GANs, DDPMs admit likelihood computation and
much more stable training dynamics (Arjovsky et al., 2017; Gulrajani et al., 2017)."
STEPS,0.015810276679841896,"5 steps
10 steps
20 steps"
STEPS,0.019762845849802372,1000 steps
STEPS,0.023715415019762844,DDSS (ours) Real DDIM
STEPS,0.02766798418972332,"Figure 1: Non-cherrypicked samples from DDSS (ours) and strongest DDIM(η = 0) baseline for uncondi-
tional DDPMs trained on LSUN churches 128×128. All samples are generated with the same random seed.
Original DDPM samples (1000 steps) and training images are shown on the left."
STEPS,0.03162055335968379,∗Work done as part of the Google AI Residency.
STEPS,0.03557312252964427,Published as a conference paper at ICLR 2022
STEPS,0.039525691699604744,"However, GANs are typically much more efﬁcient than DDPMs at generation time, often requiring
a single forward pass through the generator network, whereas DDPMs require hundreds of forward
passes through a U-Net model. Instead of learning a generator directly, DDPMs learn to convert
noisy data to less noisy data starting from pure noise, which leads to a wide variety of feasible
strategies for sampling (Song et al., 2021b). In particular, at inference time, DDPMs allow con-
trolling the number of forward passes (a.k.a. inference steps) through the denoising network (Song
et al., 2020; Nichol & Dhariwal, 2021)."
STEPS,0.043478260869565216,"It has been shown both empirically and mathematically that, for any sufﬁciently good DDPM, more
inference steps leads to better log-likelihood and sample quality (Nichol & Dhariwal, 2021; Kingma
et al., 2021). In practice, the minimum number of inference steps to achieve competitive sample
quality is highly problem-dependent, e.g., depends on the complexity of the dataset, and the strength
of the conditioning signal if the task is conditional. Given the importance of generation speed, recent
work (Song et al., 2020; Chen et al., 2021a; Watson et al., 2021) has explored reducing the number
of steps required for high quality sampling with pretrained diffusion models. See Section 7 for a
more complete review of prior work on few-step sampling."
STEPS,0.04743083003952569,"This paper treats the design of fast samplers for diffusion models as a differentiable optimization
problem, and proposes Differentiable Diffusion Sampler Search (DDSS). Our key observation is
that one can unroll the sampling chain of a diffusion model and use reparametrization trick (Kingma
& Welling, 2013) and gradient rematerialization (Kumar et al., 2019a) to optimize over a class of
parametric few-step samplers with respect to a global objective function. Our class of parameteric
samplers, which we call Generalized Gaussian Diffusion Model (GGDM), includes Denoising Dif-
fusion Implicit Models (DDIM) (Song et al., 2020) as a special case and is motivated by the success
of DDIM on fast sampling of diffusion models."
STEPS,0.05138339920948617,"An important challenge for fast DDPM sampling is the mismatch between the training objective
(e.g., ELBO or weighted ELBO) and sample quality. Prior work (Watson et al., 2021; Song et al.,
2021a) ﬁnds that samplers that are optimal with respect to ELBO often lead to worse sample quality
and Fr´echet Inception Distance (FID) scores (Heusel et al., 2017), especially with few inference
steps. We propose the use of a perceptual loss within the DDSS framework to ﬁnd high-ﬁdelity
diffusion samplers, motivated by prior work showing that their optimization leads to solutions that
correlate better with human perception of quality. We empirically ﬁnd that using DDSS with the
Kernel Inception Distance (KID) (Bi´nkowski et al., 2018) as the perceptual loss indeed leads to
fast samplers with signiﬁcantly better image quality than prior work (see Figure 1). Moreover, our
method is robust to different choices of kernels for KID."
STEPS,0.05533596837944664,Our main contributions are as follows:
STEPS,0.05928853754940711,"1. We propose Differentiable Diffusion Sampler Search (DDSS), which uses the reparametrization
trick and gradient rematerialization to optimize over a parametric family of fast samplers for
diffusion models.
2. We identify a parametric family of Generalized Gaussian Diffusion Model (GGDM) that admits
high-ﬁdelity fast samplers for diffusion models.
3. We show that using DDSS to optimize samplers by minimizing the Kernel Inception Distance
leads to fast diffusion model samplers with state-of-the-art sample quality scores."
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.06324110671936758,"2
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.06719367588932806,"We start with a brief review on DDPM (Ho et al., 2020) and DDIM (Song et al., 2020). DDPMs
pre-specify a Markovian forward diffusion process, which gradually adds noise to data in T steps.
Following the notation of Ho et al. (2020),"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.07114624505928854,"q(x0, ..., xT )
=
q(x0) T
Y"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.07509881422924901,"t=1
q(xt|xt−1)
(1)"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.07905138339920949,"q(xt|xt−1)
=
N(xt|
p"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.08300395256916997,"1 −βtxs, βtI),
(2)"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.08695652173913043,"where q(x0) is the data distribution, and βt is the variance of Gaussian noise added at step t. To be
able to gradually convert noise to data, DDPMs learn to invert (1) with a model pθ(xt−1|xt), which"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.09090909090909091,Published as a conference paper at ICLR 2022
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.09486166007905138,"is trained by maximizing a (possibly reweighted) evidence lower bound (ELBO): Eq """
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.09881422924901186,"DKL[q(xT |x0)∥p(xT )] + T
X"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.10276679841897234,"t=2
DKL[q(xt−1|xt, x0)∥pθ(xt−1|xt)] −log pθ(x0|x1) # (3)"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.1067193675889328,DDPMs speciﬁcally choose the model to be parametrized as
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.11067193675889328,"pθ(xt−1|xt) = q

xt−1"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.11462450592885376,"xt, ˆx0 =
1
√¯αt
(xt −
√"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.11857707509881422,"1 −¯αtϵθ(xt, t))
"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.1225296442687747,"= N

xt−1"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.12648221343873517,"1
√1 −βt"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.13043478260869565,"
xt −
βt
√1 −¯αt
ϵθ(xt, t)

, 1 −¯αt"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.13438735177865613,"1 −¯αt−1
βtId 
(4)"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.1383399209486166,"where ¯αt = Qt
s=1(1−βt) for each t. With this parametrization, maximizing the ELBO is equivalent
to minimizing a weighted sum of denoising score matching objectives (Vincent, 2011)."
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.1422924901185771,"The seminal work of Song et al. (2020) presents Denoising Diffusion Implicit Models (DDIM):
a family of evidence lower bounds (ELBOs) with corresponding forward diffusion processes and
samplers. All of these ELBOs share the same marginals as DDPM, but allow arbitrary choices of
posterior variances. Speciﬁcally, Song et al. (2020) note that is it possible to construct alternative
ELBOs with only a subsequence of the original timesteps S ⊂{1, ..., T} that shares the same
marginals as the construction above (i.e., qS(xt|x0) = q(xt|x0) for every t ∈S, so qS deﬁnes a
faster sampler compatible with the pre-trained model) by simply using the new contiguous timesteps
in the equations above. They also show it is also possible to construct an inﬁnite family of non-
Markovian processes {qσ : σ ∈[0, 1]T −1} where each qσ also shares the same marginals as the
original forward process with:"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.14624505928853754,"qσ(x0, ..., xt) = q(x0)q(xT |x0)"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.15019762845849802,"T −1
Y"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.1541501976284585,"t=1
qσ(xt|xt+1, x0)
(5)"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.15810276679841898,and where the posteriors are deﬁned as
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.16205533596837945,"qσ(xt−1|xt, x0) = N

xt−1"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.16600790513833993,"√¯αt−1x0 +
q"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.16996047430830039,"1 −¯αt−1 −σ2
t · xt −√¯αtx0
√1 −¯αt
, σ2
t Id 
(6)"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.17391304347826086,"Song et al. (2020) empirically ﬁnd that the extreme case of using all-zero variances (a.k.a.
DDIM(η = 0)) consistently helps with sample quality in the few-step regime. Combined with a
good selection of timesteps to evaluate the modeled score function (a.k.a. strides), DDIM(η = 0)
establishes the current state-of-the-art for few-step diffusion model sampling with the smallest in-
ference step budgets. Our key contribution that allows improving sample quality signiﬁcantly by
optimizing sampler families is constructing a family that generalizes DDIM. See Section 4 for a
more complete treatment of our novel GGDM family."
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.17786561264822134,"3
DIFFERENTIABLE DIFFUSION SAMPLER SEARCH (DDSS)"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.18181818181818182,"We now describe DDSS, our approach to search for fast high-ﬁdelity samplers with a limited budget
of K < T steps. Our key observation is that one can backpropagate through the sampling process of
a diffusion model via the reparamterization trick (Kingma & Welling, 2013). Equipped with this, we
can now use stochastic gradient descent to learn fast samplers by optimizing any given differentiable
loss function over a minibatch of model samples."
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.1857707509881423,"We begin with a pre-trained DDPM and a family of K-step samplers that we wish to optimize for
the given DDPM. We parametrize this family’s degrees of freedom as simple transformations of
trainable variables. We experiment with the following families in this paper, but emphasize that
DDSS is applicable to any other family where model samples are differentiable with respect to the
trainable variables:"
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.18972332015810275,"• DDIM: we parametrize the posterior variances with σt = √1 −¯αt−1 sigmoid(vt), where
v1, ..., vK are trainable variables (the √1 −¯αt−1 constant ensures real-valued mean coef-
ﬁcients; see the square root in Equation 6)."
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.19367588932806323,Published as a conference paper at ICLR 2022
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.1976284584980237,"Figure 2: Illustration of GGDM. To improve sample quality, our novel family of samplers combines
information from all previous (noisier) images at every denoising step."
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.2015810276679842,"• VARS: we parametrize the marginal variances of a DDPM as cumsum(softmax([v; 1]))t
instead of ﬁxing them to 1 −¯αt. This ensures they are monotonically increasing with
respect to t (appending a one to ensure K degrees of freedom).
• GGDM: a new family of non-Markovian samplers for diffusion models with more degrees
of freedom illustrated in Figure 2 and deﬁned in Section 4. We parametrize µtu and σt of
a GGDM for all t as sigmoid functions of trainable variables.
• GGDM +PRED: we parametrize all the µtu and σt identically to GGDM, but also learn
the marginal coefﬁcients with a cumsum ◦softmax parametrization (identical to VARS)
instead of computing them via Theorem 1, as well as the coefﬁcients that predict x0 from
atxt −btϵ with 1 + softplus and softplus parametrizations.
• [family]+TIME: for any sampler family, we additionally parametrize the timesteps used
to query the score model with a cumsum ◦softmax parametrization (identical to VARS)."
BACKGROUND ON DENOISING DIFFUSION IMPLICIT MODELS,0.20553359683794467,"As we will show in the experiments, despite the fact that our pre-trained DDPMs are trained with
discrete timesteps, learning the timesteps is still helpful. In principle, this should only be possible
for DDPMs trained with continuous time sampling (Chen et al., 2021a; Song et al., 2021b; Kingma
et al., 2021), but in practice we ﬁnd that DDPMs trained with continuously embedded discrete
timesteps are still well-behaved when applied at timesteps not present during training. We think this
is due to the regularity of the sinusoidal positional encodings Vaswani et al. (2017) used in these
model architectures and training with a sufﬁciently large number of timesteps T."
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.20948616600790515,"3.1
DIFFERENTIABLE SAMPLE QUALITY SCORES"
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.2134387351778656,"We can differentiate through a stochastic sampler using the reparameterization trick, but the question
of which objective to optimize still remains. Prior work has shown that optimizing log-likelihood
can actually worsen sample quality and FID scores in the few-step regime (Watson et al., 2021; Song
et al., 2021a). Thus, we instead design a perceptual loss which simply compares mean statistics be-
tween model samples and real samples in the neural network feature space. These types of objectives
have been shown in prior work to better correlate with human perception of sample quality (Johnson
et al., 2016; Heusel et al., 2017), which we also conﬁrm in our experiments."
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.21739130434782608,"We rely on the representations of the penultimate layer of a pre-trained InceptionV3 classiﬁer
(Szegedy et al., 2016) and optimize the Kernel Inception Distance (KID) (Bi´nkowski et al., 2018).
Let φ(x) denote the inception features of an image x and pψ represent a diffusion sampler with
trainable parameters ψ. For a linear kernel, which works best in our experiments, the objective is:"
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.22134387351778656,"L(ψ) =
E
xp∼pψ
E
x′p∼pψ
φ(xp)⊤φ(x′
p) −
E
xp∼pψ
E
xq∼q φ(xp)⊤φ(xq)
(7)"
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.22529644268774704,"More generally, KID can be expressed as:"
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.22924901185770752,"LKID(ψ) =

E
xp∼pψ
f ∗(xp) −
E
xq∼q f ∗(xq) 2"
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.233201581027668,"2
,
(8)"
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.23715415019762845,"where f ∗(x) = Ex′
p∼pψkφ(x, x′
p) −Ex′q∼q(x0)kφ(x, x′
q) is the witness function for any differen-
tiable, positive deﬁnite kernel k, and kφ(x, y) = k(φ(x), φ(y)). Note that f ∗attains the supremum
of the MMD. To enable stochastic gradient descent, we use an unbiased estimator of KID using a
minibatch of n model samples x(1)
p
. . . x(n)
p
∼pψ and n real samples x(1)
q
. . . x(n)
q
∼q:"
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.24110671936758893,"1
n(n −1) n
X"
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.2450592885375494,"i̸=j
kφ(x(i)
p , x(j)
p ) −2 n2 n
X i=1 n
X"
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.2490118577075099,"j=1
kφ(x(i)
p , x(j)
q ) + c ,
(9)"
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.25296442687747034,Published as a conference paper at ICLR 2022
DIFFERENTIABLE SAMPLE QUALITY SCORES,0.25691699604743085,"where c is constant in ψ. Since the sampling chain of any Gaussian diffusion process admits using
the reparametrization trick, our loss function is fully differentiable with respect to the trainable vari-
ables ψ. We empirically ﬁnd that using the perceptual features is crucial; i.e., by trying φ(x) = x to
compare images directly on pixel space rather than neural network feature space (as above), we ob-
serve that our method makes samples consistently worsen in apparent quality as training progresses."
GRADIENT REMATERIALIZATION,0.2608695652173913,"3.2
GRADIENT REMATERIALIZATION"
GRADIENT REMATERIALIZATION,0.2648221343873518,"In order for backpropagation to be feasible under reasonable memory constraints, one ﬁnal problem
must be addressed: since we are taking gradients with respect to model samples, the cost in memory
to maintain the state of the forward pass scales linearly with the number of inference steps, which
can quickly become unfeasible considering the large size of typical DDPM architectures. To address
this issue, we use gradient rematerialization (Kumar et al., 2019b). Instead of storing a particular
computation’s output from the forward pass required by the backward pass computation, we recom-
pute it on the ﬂy. To trade O(K) memory cost for O(K) computation time, we simply rematerialize
calls to the pre-trained DDPM (i.e., the estimated score function), but keep in memory all the pro-
gressively denoised images from the sampling chain. In JAX (Bradbury et al., 2018), this is trivial
to implement by simply wrapping the score function calls with jax.remat."
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.26877470355731226,"4
GENERALIZED GAUSSIAN DIFFUSION MODELS"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.2727272727272727,"We now present Generalized Gaussian Diffusion Model (GGDM), our novel family of Gaussian
diffusion processes that includes DDIM as a special case mentioned in section 3. We deﬁne a joint
distribution with no independence assumptions"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.2766798418972332,"qµ,σ(x0, ..., xT ) = q(x0)q(xT |x0)"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.28063241106719367,"T −1
Y"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.2845849802371542,"t=1
qµ,σ(xt|x>t, x0)
(10)"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.2885375494071146,where the new factors are deﬁned as
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.2924901185770751,"qµ,σ(xt|x>t, x0) = N  xt  X"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.2964426877470356,"u∈St
µtuxu, σ2
t Id ! (11)"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.30039525691699603,"(letting St = {0, ..., T} \ {1, ..., t} for notation compactness), with σt and µtu free parameters
∀t ∈{1, ..., T}, u ∈St. In other words, when predicting the next, less noisy image, the sampler can
take into account all the previous, noisier images in the sampling chain, and similarly to DDIM, we
can also control the sampler’s variances. As we prove in the appendix (A.2), this construction admits
Gaussian marginals, and we can differentiably compute the marginal coefﬁcients from arbitrary
choices of µ and σ:"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.30434782608695654,"Theorem 1. Given some t ∈{1, ..., T}, let a(1)
tu
= µtu ∀u ∈St and v(1)
t
= σ2
t . For each
i ∈{1, ..., T −t}, recursively deﬁne"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.308300395256917,"a(i+1)
tu
= a(i)
t,t+iµt+i,u + a(i)
tu ∀u ∈St+i
and
v(i+1)
t
= v(i)
t
+

a(i)
t,t+iσt+i
2
."
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.31225296442687744,"Then, it follows that"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.31620553359683795,"qµ,σ(xt|x>t+i, x0) = N  xt  X"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.3201581027667984,"u∈St+i
a(i+1)
tu
xu, v(i+1)
t
Id "
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.3241106719367589,".
(12)"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.32806324110671936,"In other words, instead of letting the βt (or equivalently, the ¯αt) deﬁne the forward process as done
by a usual DDPM, the GGDM family lets the µtu and σt deﬁne the process. In particular, an
immediate corollary of Theorem 1 is that the marginal coefﬁcients are given by"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.33201581027667987,"qµ,σ(xt|x0) = N

xt
a(T −t+1)
t0
x0, v(T −t+1)
t
Id

(13)"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.3359683794466403,"The reverse process is thus deﬁned as p(xT ) QT
t=1 p(xt−1|xt) with p(xT ) ∼N(0, Id)) and"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.33992094861660077,"pθ(xt|x>t) = qµ,σ "
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.3438735177865613,"xt|x>t, ˆx0 =
1"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.34782608695652173,"a(T −t+1)
t0"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.35177865612648224,"
xt −
q"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.3557312252964427,"v(T −t+1)
t
ϵθ(xt, t)
!"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.35968379446640314,".
(14)"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.36363636363636365,Published as a conference paper at ICLR 2022
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.3675889328063241,"Table 1: FID and IS scores for DDSS against baseline methods for a DDPM trained on CIFAR10
with the Lsimple objective proposed by (Ho et al., 2020). FID scores (lower is better) are the numbers
at the left of each entry, and IS scores (higher is better) are at the right."
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.3715415019762846,"Sampler \ K
5
10
15
20
25
DDPM (linear stride)
84.27 / 5.396
43.39 / 7.034
31.40 / 7.609
25.94 / 7.879
22.60 / 8.043
DDPM (quadratic stride)
76.25 / 5.435
42.03 / 6.965
27.78 / 7.714
20.225 / 8.128
16.17 / 8.350
DDIM (linear stride)
44.41 / 6.750
19.11 / 7.965
14.06 / 8.190
11.82 / 8.420
10.52 / 8.512
DDIM (quadratic stride)
32.66 / 7.090
13.62 / 8.190
9.318 / 8.495
7.500 / 8.641
6.560 / 8.759
GGDM +PRED+TIME
13.77 / 8.520
8.227 / 8.903
6.115 / 9.050
4.722 / 9.261
4.250 / 9.186"
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.37549407114624506,"Table 2: FID / IS scores for DDSS against baseline methods for a DDPM trained on ImageNet 64x64
with the Lhybrid objective proposed by Nichol & Dhariwal (2021)."
GENERALIZED GAUSSIAN DIFFUSION MODELS,0.3794466403162055,"Sampler \ K
5
10
15
20
25
DDPM (linear stride)
122.0 / 5.878
58.78 / 10.67
39.30 / 13.22
31.36 / 14.72
26.36 / 15.71
DDPM (quadratic stride)
394.8 / 1.351
129.5 / 5.997
80.10 / 9.595
61.34 / 11.60
49.60 / 13.01
DDIM (linear stride)
135.4 / 5.898
40.70 / 12.225
28.54 / 13.99
24.225 / 14.75
22.13 / 15.16
DDIM (quadratic stride)
409.1 / 1.380
148.6 / 5.533
67.65 / 9.842
45.60 / 11.99
36.11 / 13.225
GGDM +PRED+TIME
55.14 / 12.90
37.32 / 14.76
24.69 / 17.225
20.69 /17.92
18.40 / 18.12"
IGNORING THE MATCHING MARGINALS CONDITION,0.383399209486166,"4.1
IGNORING THE MATCHING MARGINALS CONDITION"
IGNORING THE MATCHING MARGINALS CONDITION,0.38735177865612647,"Unlike DDIM, the GGDM family does not guarantee that the marginals of the new forward
process match that of the original DDPM. We empirically ﬁnd, however, that this condition
can often be too restrictive and better samplers exist where the marginals qµ,σ(xt|x0)
="
IGNORING THE MATCHING MARGINALS CONDITION,0.391304347826087,"N

xt
a(T −t+1)
t0
x0, v(T −t+1)
t
Id

of the new forward process differ from the original DDPM’s
marginals. We verify this empirically by applying DDSS to both the family of DDIM sigmas and
DDPM variances (“VARS” in Section 3): both sampler families have the same number of parame-
ters (the reverse process variances), but the latter does not adjust the mean coefﬁcients like DDIM to
ensure matching marginals and still achieves similar or better scores than the former across sample
quality metrics (and even outperforms the DDIM(η = 0) baseline); see Section 5.2."
EXPERIMENTS,0.3952569169960474,"5
EXPERIMENTS"
EXPERIMENTS,0.39920948616600793,"In order to emphasize that our method is compatible with any pre-trained DDPM, we apply our
method on pre-trained DDPM checkpoints from prior work. Speciﬁcally, we experiment with the
DDPM trained by Ho et al. (2020) with Lsimple on CIFAR10, as well as a DDPM following the exact
conﬁguration of Nichol & Dhariwal (2021) trained on ImageNet 64x64 (Deng et al., 2009) with their
Lhybrid objective (with the only difference being that we trained the latter ourselves for 3M rather
than 1.5M steps). Both of these models utilize adaptations of the UNet architecture (Ronneberger
et al., 2015) that incorporate self-attention layers (Vaswani et al., 2017)."
EXPERIMENTS,0.4031620553359684,"We evaluate all of our models on both FID and Inception Score (IS) (Salimans et al., 2016), compar-
ing the samplers discovered by DDSS against DDPM and DDIM baselines with linear and quadratic
strides. As previously mentioned, more recent methods for fast sampling are outperformed by DDIM
when the budget of inference steps is as small as those we utilize in this work (5, 10, 15, 20, 25). All
reported results on both of these approximate sample quality metrics were computed by comparing
50K model and training data samples, as is standard in the literature. Also as is standard, IS scores
are computed 10 times, each time on 5K samples, and then averaged."
EXPERIMENTS,0.40711462450592883,"In all of our experiments, we optimize the DDSS objective presented in Section 3.1 with the follow-
ing hyperparameters:"
EXPERIMENTS,0.41106719367588934,"1. For every family of models we search over, we initialize the degrees of freedom such that
training begins with a sampler matching DDPM with K substeps following Song et al.
(2020); Nichol & Dhariwal (2021)."
EXPERIMENTS,0.4150197628458498,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.4189723320158103,"Table 3: FID / IS scores for the KID kernel ablation on CIFAR10. When not learning the timesteps,
we ﬁx them to a quadratic stride, as Table 1 shows this performs best on CIFAR10."
EXPERIMENTS,0.42292490118577075,"Sampler \ K
5
10
15
20
25
DDSS (linear kernel)"
EXPERIMENTS,0.4268774703557312,"GGDM +PRED+TIME
13.77 / 8.520
8.227 / 8.903
6.115 / 9.050
4.722 / 9.261
4.250 / 9.186
GGDM +PRED
14.26 / 8.406
8.617 / 8.842
5.939 / 9.035
4.893 / 9.153
4.574 / 9.145
GGDM +TIME
12.85 / 8.383
7.858 / 8.895
6.265 / 9.075
5.367 / 9.136
4.887 / 9.229
GGDM)
14.45 / 8.281
8.154 / 8.892
7.045 / 8.939
5.477 / 9.183
4.815 / 9.189
DDSS (cubic kernel)"
EXPERIMENTS,0.4308300395256917,"GGDM +PRED+TIME
14.41 / 8.527
8.2257 / 9.007
5.895 / 9.036
4.932 / 9.092
4.278/ 9.286
GGDM +PRED
14.39 / 8.401
8.977 / 8.870
6.517 / 8.970
4.915 / 9.132
4.471 / 9.247
GGDM +TIME
12.35 / 8.406
7.879 / 8.852
6.682 / 8.999
5.639 / 9.058
4.631 / 9.189
GGDM
14.57 / 8.297
8.2252 / 8.836
6.727 / 8.904
5.569 / 9.177
4.668 / 9.192"
EXPERIMENTS,0.43478260869565216,"2. We apply gradient updates using the Adam optimizer (Kingma & Ba, 2015). We sweeped
over the learning rate and used λ = 0.0005. We did not sweep over other Adam hyperpa-
rameters and kept β1 = 0.9, β2 = 0.999, and ϵ = 1 × 10−8.
3. We tried batch sizes of 128 and 512 and opted for the latter, ﬁnding that it leads to better
sample quality upon inspection. Since the loss depends on averages over examples as our
experiments are on unconditional generation, this choice was expected.
4. We run all of our experiments for 50K training steps and evaluate the discovered samplers
at this exact number of training steps. We did not sweep over this value."
EXPERIMENTS,0.43873517786561267,"We include our main results in Table 1 for CIFAR10 and Table 2 for ImageNet 64x64, compar-
ing DDSS applied to GGDM +PRED+TIME against DDPM and DDIM baselines with linear and
quadratic strides. All models use a linear kernel, i.e., kφ(x, y) = φ(x)⊤φ(y), which we found
to perform slightly better than the cubic kernel used by Bi´nkowski et al. (2018) (we ablate this in
section 5.1). We omit the use of the learned variances of the ImageNet 64x64 model (i.e., following
Nichol & Dhariwal (2021)), as we search for the variances ourselves via DDSS. We include samples
for 5, 10 and 25 steps comparing the strongest DDIM baselines to DDSS + GGDM with a learned
stride; see Figures 1 and 3. We include additional ImageNet 64x64 samples (A.1) and results for
larger resolution datasets (A.4) in the appendix."
ABLATIONS FOR KID KERNEL AND GGDM VARIANTS,0.4426877470355731,"5.1
ABLATIONS FOR KID KERNEL AND GGDM VARIANTS"
ABLATIONS FOR KID KERNEL AND GGDM VARIANTS,0.44664031620553357,"As our approach is compatible with any choice of KID kernel, we experiment with different choices
of kernels. Namely, we try the simplest possible linear kernel, kφ(x, y) = φ(x)⊤φ(y), as well as
the cubic kernel kφ(x, y) =
  1"
ABLATIONS FOR KID KERNEL AND GGDM VARIANTS,0.4505928853754941,"dφ(x)⊤φ(y) + 1
3 used by Bi´nkowski et al. (2018). We compare the
performance of these two kernels, as well as different variations of GGDM (i.e., with and without
TIME and PRED as deﬁned in Section 3). Results are included for CIFAR10 across all budgets
in Table 3. We also include a smaller version of this ablation for ImageNet 64x64 in the appendix
(A.3)."
ABLATIONS FOR KID KERNEL AND GGDM VARIANTS,0.45454545454545453,"The results in this ablation show that the contributions of the linear kernel, timestep learning, and
the empirical choice of learning the coefﬁcients that predict x0 all slightly contribute to better FID
and IS scores. Importantly, however, removing any of these additions still allows us to comfortably
outperform the strongest baselines. See also the results on LSUN in the appendix A.4, which also
do not include these additional trainable variables."
SEARCH SPACE ABLATION,0.45849802371541504,"5.2
SEARCH SPACE ABLATION"
SEARCH SPACE ABLATION,0.4624505928853755,"Now, in order to further demonstrate the key importance of optimizing our GGDM family to ﬁnd
high-ﬁdelity samplers, we also apply DDSS to the less general DDIM and VARS families. We show
that, while we still attain better scores than a regular DDPM, searching these less ﬂexible families of
samplers does not yield improvements as signiﬁcant as with out novel GGDM family. In particular,
optimizing the DDIM sigma coefﬁcients does not outperform the corresponding DDIM(η = 0)
baseline on CIFAR10, which is not a surprising result as Song et al. (2020) show empirically that
most choices of the σt degrees of freedom lead to worse FID scores than setting them all to 0. These"
SEARCH SPACE ABLATION,0.466403162055336,Published as a conference paper at ICLR 2022
SEARCH SPACE ABLATION,0.47035573122529645,"Table 4: FID / IS scores for the DDSS search space ablation on CIFAR10. All runs ﬁx the timesteps
to a quadratic stride and use a linear kernel except for the last row (we only include the GGDM
results for ease of comparison)."
SEARCH SPACE ABLATION,0.4743083003952569,"Sampler \ K
5
10
15
20
25
DDIM(η = 0)
32.66 / 7.090
13.62 / 8.190
9.318 / 8.495
7.500 / 8.641
6.560 / 8.759
DDSS"
SEARCH SPACE ABLATION,0.4782608695652174,"VARS
33.08 / 7.096
15.33 / 8.559
9.693 / 8.845
7.297 / 8.924
6.172 / 9.057
DDIM
32.61 / 7.084
16.29 / 7.966
11.31 / 8.372
9.120 / 8.563
7.853 / 8.644
GGDM
14.45 / 8.281
8.154 / 8.892
7.045 / 8.939
5.477 / 9.183
4.815 / 9.189
GGDM +PRED+TIME
13.77 / 8.520
8.227 / 8.903
6.115 / 9.050
4.722 / 9.261
4.250 / 9.186"
SEARCH SPACE ABLATION,0.48221343873517786,"results also show that optimizing the VARS can outperform DDSS applied to the DDIM family, and
even the strongest DDIM(η = 0) baselines for certain budgets, justifying our choice of not enforcing
the marginals to match (as discussed in Section 4.1)."
DISCUSSION,0.48616600790513836,"6
DISCUSSION"
DISCUSSION,0.4901185770750988,"When applied to a sufﬁciently ﬂexible family (such as the GGDM family proposed in this work),
DDSS consistently ﬁnds samplers that achieve better image generation quality than the strongest
baselines in the literature for very few steps. This is qualitatively apparent in non-cherrypicked sam-
ples (e.g., DDIM(η = 0) tends to generate blurrier images and with less background details as the
budget decreases), and multiple quantitative sample quality metrics (FID and IS) also reﬂect these
results. Still, we observe limitations to our method. Finding samplers with inference step budgets
as small as K < 10 that have little apparent loss in quality remains challenging with our proposed
search family. And, while on CIFAR10 the metrics indicate signiﬁcant relative improvement over
sample quality metrics, the relative improvement on ImageNet 64x64 is less pronounced. We hy-
pothesize that this is an inherent difﬁculty of ImageNet due to its high diversity of samples, and
that in order to retain sample quality and diversity, it might be impossible to escape some minimum
number of inference steps with score-based models as they might be crucial to mode-breaking."
DISCUSSION,0.49407114624505927,"Beyond the empirical gains of applying our procedure, our ﬁndings shed further light into properties
of pre-trained score-based generative models. First, we show that without ﬁne-tuning a DDPM’s
parameters, these models are already capable of producing high-quality samples with very few in-
ference steps, though the default DDPM sampler in this regime is usually suboptimal when using
a few-step sampler. We further show that better sampling paths exist, and interestingly, these are
determined by alternative variational lower bounds to the data distribution that make use of the
score-based model but do not necessarily share the same marginals as the original DDPM forward
process. Our ﬁndings thus suggest that enforcing this marginal-sharing constraint is unnecessary
and can be too restrictive in practice."
OTHER RELATED WORK,0.4980237154150198,"7
OTHER RELATED WORK"
OTHER RELATED WORK,0.5019762845849802,"Besides DDIM (Song et al., 2020), there have been more recent attempts at reducing the number of
inference steps for DDPMs. Jolicoeur-Martineau et al. (2021) proposed a dynamic step size SDE
solver that can reduce the number of calls to the modeled score function to ∼150 on CIFAR10
(Krizhevsky et al., 2009) with minimal cost in FID scores, but quickly falls behind DDIM(η = 0)
with as many as 50 steps. Watson et al. (2021) proposed a dynamic programming algorithm that
chooses log-likelihood optimal strides, but ﬁnd that log-likelihood reduction has a mismatch with
FID scores, particularly with in the very few step regime, also falling behind DDIM(η = 0) in this
front. Other methods that have been shown to help sample quality in the few-step regime include
non-Gaussian variants of diffusion models (Nachmani et al., 2021) and adaptively adjusting the
sampling path by introducing a noise level estimating network (San-Roman et al., 2021), but more
thorough evaluation of sample quality achieved by these approaches is needed with budgets as small
as those considered in this work."
OTHER RELATED WORK,0.5059288537549407,"Other approaches to sampling DDPMs have also been recently proposed, though not for the explicit
purpose of efﬁcient sampling. Song et al. (2021b) derive a reverse SDE that, when discretized, uses"
OTHER RELATED WORK,0.5098814229249012,Published as a conference paper at ICLR 2022
STEPS,0.5138339920948617,"5 steps
10 steps
20 steps
Reference"
STEPS,0.5177865612648221,"DDIM
Ours"
STEPS,0.5217391304347826,"Figure 3: Non-cherrypicked samples for a DDPM trained on CIFAR10, comparing the strongest
DDIM(η = 0) baseline and our approach. All samples were generated with the same random seeds.
For reference, we include DDPM samples using all 1,000 steps (top right) and real images (bottom
right)."
STEPS,0.525691699604743,"different coefﬁcients than the ancestral samplers considered in this work. The same authors also
derive “corrector” steps, which introduce additional calls to the pre-trained DDPM as a form of
gradient ascent (Langevin dynamics) that help with quality but introduce computation cost, as well
as an alternative sampling procedure using a probability ﬂow ODE that shares the same marginals
as the DDPM’s original forward process. Huang et al. (2021) generalize this family of samplers
to a “plug-in reverse SDE” that interpolates between a probability ﬂow ODE and the reverse SDE,
similarly to how the DDIM η interpolates between an implicit probabilistic model and a stochastic
reverse process. Our proposed search family includes discretizations of most of these cases for
Gaussian processes, notably missing corrector steps, where reusing a single timestep is considered."
CONCLUSION AND FUTURE WORK,0.5296442687747036,"8
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.5335968379446641,"We propose Differentiable Diffusion Sampler Search (DDSS), a method for ﬁnding few-step sam-
plers for Denoising Diffusion Probabilistic Models. We show how to optimize a perceptual loss over
a space of diffusion processes that makes use of a pre-trained DDPM’s samples by leveraging the
reparametrization trick and gradient rematerialization. Our results qualitatively and quantitatively
show that DDSS is able to signiﬁcantly improve sample quality for unconditional image generation
over prior methods on efﬁcient DDPM sampling. The success of our method hinges on searching
a novel, wider family of Generalized Gaussian Diffusion Model (GGDM) than those identiﬁed in
prior work (Song et al., 2020). DDSS does not ﬁne-tune the pre-trained DDPM, only needs to be
applied once, has few hyperparameters, and does not require re-training the DDPM."
CONCLUSION AND FUTURE WORK,0.5375494071146245,"Our ﬁndings suggest future directions to further reduce the number of inference steps while retaining
high ﬁdelity in generated samples. For instance, it is plausible to use different representations for the
perceptual loss instead of those of a classiﬁer, e.g., use representations from an unsupervised model
such as SimCLR (Chen et al., 2020), to using internal representations learned by the pre-trained
DDPM itself, which would eliminate the burden of additional computation. Moreover, considering
the demonstrated beneﬁts of applying DDSS to our proposed GGDM family of samplers (as opposed
to narrower families like DDIM), we motivate future work on identifying more general families of
samplers and investigating whether they help uncover even better samplers or lead to overﬁtting.
Finally, identifying other variants of perceptual losses (e.g., that do not sample from the model),
or alternative optimization strategies (e.g., gradient-free methods) that lead to similar results is im-
portant future work. This would make DDSS itself a more efﬁcient procedure, as gradient-based
optimization of our proposed loss requires extensive memory or computation requirements to back-
propagate through the whole sampling chain."
CONCLUSION AND FUTURE WORK,0.541501976284585,Published as a conference paper at ICLR 2022
REFERENCES,0.5454545454545454,REFERENCES
REFERENCES,0.549407114624506,"Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein GAN. In arXiv, 2017."
REFERENCES,0.5533596837944664,"Mikołaj Bi´nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd
gans. arXiv preprint arXiv:1801.01401, 2018."
REFERENCES,0.5573122529644269,"James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax."
REFERENCES,0.5612648221343873,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity
natural image synthesis. arXiv preprint arXiv:1809.11096, 2018."
REFERENCES,0.5652173913043478,"Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and
Bharath Hariharan. Learning Gradient Fields for Shape Generation. In ECCV, 2020."
REFERENCES,0.5691699604743083,"Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wave-
Grad: Estimating Gradients for Waveform Generation. In ICLR, 2021a."
REFERENCES,0.5731225296442688,"Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, Najim Dehak, and William
Chan. WaveGrad 2: Iterative Reﬁnement for Text-to-Speech Synthesis . In INTERSPEECH,
2021b."
REFERENCES,0.5770750988142292,"Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big Self-
Supervised Models are Strong Semi-Supervised Learners. In NeurIPS, 2020."
REFERENCES,0.5810276679841897,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.5849802371541502,"Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. arXiv preprint
arXiv:2105.05233, 2021."
REFERENCES,0.5889328063241107,"Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014."
REFERENCES,0.5928853754940712,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017."
REFERENCES,0.5968379446640316,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. arXiv preprint
arXiv:1706.08500, 2017."
REFERENCES,0.6007905138339921,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. NeurIPS,
2020."
REFERENCES,0.6047430830039525,"Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim
Salimans.
Cascaded diffusion models for high ﬁdelity image generation.
arXiv preprint
arXiv:2106.15282, 2021."
REFERENCES,0.6086956521739131,"Chin-Wei Huang, Jae Hyun Lim, and Aaron Courville. A variational perspective on diffusion-based
generative models and score matching. arXiv preprint arXiv:2106.02808, 2021."
REFERENCES,0.6126482213438735,"Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In European conference on computer vision, pp. 694–711. Springer, 2016."
REFERENCES,0.616600790513834,"Alexia Jolicoeur-Martineau, Ke Li, R´emi Pich´e-Taillefer, Tal Kachman, and Ioannis Mitliagkas.
Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080,
2021."
REFERENCES,0.6205533596837944,"Diederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2015."
REFERENCES,0.6245059288537549,Published as a conference paper at ICLR 2022
REFERENCES,0.6284584980237155,"Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In ICLR, 2013."
REFERENCES,0.6324110671936759,"Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.
arXiv preprint arXiv:2107.00630, 2021."
REFERENCES,0.6363636363636364,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical Report, 2009."
REFERENCES,0.6403162055335968,"Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo,
Alexandre de Brebisson, Yoshua Bengio, and Aaron Courville. MelGAN: Generative Adversarial
Networks for Conditional Waveform Synthesis. In NeurIPS, 2019a."
REFERENCES,0.6442687747035574,"Ravi Kumar, Manish Purohit, Zoya Svitkina, Erik Vee, and Joshua R Wang. Efﬁcient remateri-
alization for deep networks.
In Proceedings of the 33rd International Conference on Neural
Information Processing Systems, pp. 15172–15181, 2019b."
REFERENCES,0.6482213438735178,"Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2837–2845,
2021."
REFERENCES,0.6521739130434783,"Eliya Nachmani, Robin San Roman, and Lior Wolf. Non gaussian denoising diffusion models. arXiv
preprint arXiv:2106.07582, 2021."
REFERENCES,0.6561264822134387,"Alex Nichol and Prafulla Dhariwal.
Improved denoising diffusion probabilistic models.
arXiv
preprint arXiv:2102.09672, 2021."
REFERENCES,0.6600790513833992,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234–241. Springer, 2015."
REFERENCES,0.6640316205533597,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. Advances in neural information processing systems, 29:
2234–2242, 2016."
REFERENCES,0.6679841897233202,"Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion mod-
els. arXiv preprint arXiv:2104.02600, 2021."
REFERENCES,0.6719367588932806,"Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learn-
ing, pp. 2256–2265. PMLR, 2015."
REFERENCES,0.6758893280632411,"Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502, 2020."
REFERENCES,0.6798418972332015,"Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribu-
tion. NeurIPS, 2019."
REFERENCES,0.6837944664031621,"Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-
based diffusion models. arXiv e-prints, pp. arXiv–2101, 2021a."
REFERENCES,0.6877470355731226,"Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In ICLR,
2021b."
REFERENCES,0.691699604743083,"Markus Svens´en and Christopher M Bishop. Pattern recognition and machine learning, 2007."
REFERENCES,0.6956521739130435,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-
thinking the Inception Architecture for Computer Vision. In CVPR, 2016."
REFERENCES,0.6996047430830039,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In NIPS, 2017."
REFERENCES,0.7035573122529645,"Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Compu-
tation, 23(7):1661–1674, 2011."
REFERENCES,0.7075098814229249,Published as a conference paper at ICLR 2022
REFERENCES,0.7114624505928854,"Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efﬁciently sam-
ple from diffusion probabilistic models. arXiv preprint arXiv:2106.03802, 2021."
REFERENCES,0.7154150197628458,"Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015."
REFERENCES,0.7193675889328063,Published as a conference paper at ICLR 2022
REFERENCES,0.7233201581027668,"A
APPENDIX"
REFERENCES,0.7272727272727273,"A.1
ADDITIONAL IMAGENET 64X64 SAMPLES"
REFERENCES,0.7312252964426877,"We provide additional samples for our results on ImageNet 64x64. The DDPM and DDIM(η = 0)
samples (left and middle, respectively) use a linear stride, while our DDSS + GGDM samples (right)
use a learned stride."
REFERENCES,0.7351778656126482,"DDPM
DDIM
Ours"
STEPS,0.7391304347826086,"5 steps
10 steps
20 steps
Reference"
STEPS,0.7430830039525692,"Figure A.1: Additional samples on ImageNet 64x64. For reference, we include DDPM samples
with all 4,000 steps (bottom left) and real samples (bottom middle)."
STEPS,0.7470355731225297,Published as a conference paper at ICLR 2022
STEPS,0.7509881422924901,"A.2
PROOF FOR THEOREM 1"
STEPS,0.7549407114624506,"Theorem 1. Given some t ∈{1, ..., T}, let a(1)
tu
= µtu ∀u ∈St and v(1)
t
= σ2
t . For each
i ∈{1, ..., T −t}, recursively deﬁne"
STEPS,0.758893280632411,"• a(i+1)
tu
= a(i)
t,t+iµt+i,u + a(i)
tu ∀u ∈St+i"
STEPS,0.7628458498023716,"• v(i+1)
t
= v(i)
t
+

a(i)
t,t+iσt+i
2"
STEPS,0.766798418972332,"Then, it follows that"
STEPS,0.7707509881422925,"qµ,σ(xt|x>t+i, x0) = N  xt  X"
STEPS,0.7747035573122529,"u∈St+i
a(i+1)
tu
xu, v(i+1)
t
Id  "
STEPS,0.7786561264822134,"Proof. Let us prove this result with mathematical induction. Note that, for each such t, we have by
deﬁnition that"
STEPS,0.782608695652174,"qµ,σ(xt+1|x>t+1, x0) = N

xt
P"
STEPS,0.7865612648221344,"u∈St+1 µt+1,uxu, σ2
t+1Id
 and"
STEPS,0.7905138339920948,"qµ,σ(xt|xt+1, x>t+1, x0) = N
 
xt
P"
STEPS,0.7944664031620553,"u∈St µtuxu, σ2
t Id

= N

xt
P"
STEPS,0.7984189723320159,"u∈St a(1)
tu xu, v(1)
t
Id
"
STEPS,0.8023715415019763,"Therefore, following Svens´en & Bishop (2007) (2.115), by prior conjugacy it follows that"
STEPS,0.8063241106719368,"qµ,σ(xt|x>t+1, x0) = N

xt
a(1)
t,t+1
P"
STEPS,0.8102766798418972,"u∈St+1 µt+1,uxu + P"
STEPS,0.8142292490118577,"u∈St+1 a(1)
tu ,

v(1)
t
+ a(1)
t,t+1σ2
t+1a(1)
t,t+1

Id
"
STEPS,0.8181818181818182,"= N

xt
P"
STEPS,0.8221343873517787,u∈St+1
STEPS,0.8260869565217391,"
a(1)
t,t+1µt+1,u + a(1)
tu

xu, v(2)
t
Id
"
STEPS,0.8300395256916996,"= N

xt
P"
STEPS,0.83399209486166,"u∈St+1 a(2)
tu xu, v(2)
t
Id

."
STEPS,0.8379446640316206,"This proves the base case for our induction argument. Now, let us prove the inductive step. Suppose
there exists some integer j ∈{1, ..., T −t + 1} such that"
STEPS,0.841897233201581,"qµ,σ(xt|x>t+j, x0) = N

xt
P"
STEPS,0.8458498023715415,"u∈St+j a(j+1)
tu
xu, v(j+1)
t
Id

."
STEPS,0.849802371541502,"By deﬁnition, we already know q(xt+j+1|x>t+j+1, x0), so we have"
STEPS,0.8537549407114624,"qµ,σ(xt+j+1|x>t+j+1, x0) = N

xt+j+1

P"
STEPS,0.857707509881423,"u∈St+j+1 µt+j+1,uxu, σ2
t+j+1Id
"
STEPS,0.8616600790513834,and (rewriting the inductive hypothesis)
STEPS,0.8656126482213439,"qµ,σ(xt|xt+j+1, x>t+j+1, x0) = N

xt

P"
STEPS,0.8695652173913043,"u∈St+j a(j+1)
tu
xu, v(j+1)
t
Id

."
STEPS,0.8735177865612648,"Therefore, by prior conjugacy again, it follows that"
STEPS,0.8774703557312253,"qµ,σ(xt|x>t+j+1, x0)"
STEPS,0.8814229249011858,"=N

xt
a(j+1)
t,t+j+1
P"
STEPS,0.8853754940711462,"u∈St+j µt+j+1,uxu + P"
STEPS,0.8893280632411067,"u∈St+j a(j+1)
tu
,

v(j+1)
t
+ a(j+1)
t,t+j+1σ2
t+j+1a(j+1)
t,t+j+1

Id
"
STEPS,0.8932806324110671,"=N

xt
P"
STEPS,0.8972332015810277,u∈St+j
STEPS,0.9011857707509882,"
a(j+1)
t,t+j+1µt+j+1,u + a(j+1)
tu

xu, v(j+2)
t
Id
"
STEPS,0.9051383399209486,"=N

xt
P"
STEPS,0.9090909090909091,"u∈St+j+1 a(j+2)
tu
xu, v(j+2)
t
Id

."
STEPS,0.9130434782608695,"This concludes the proof of the inductive step. Hence, we have proven the result for any i ∈
{1, ..., T −t}. In particular,"
STEPS,0.9169960474308301,"qµ,σ(xt|x0) = N

xt
a(T −t+1)
t0
x0, v(T −t+1)
t
Id

.
□"
STEPS,0.9209486166007905,Published as a conference paper at ICLR 2022
STEPS,0.924901185770751,"A.3
ADDITIONAL ABLATION OF KID KERNEL AND GGDM VARIANTS FOR IMAGENET
64X64"
STEPS,0.9288537549407114,"We also ran a smaller version of the ablation results presented in Section 5.1, but for ImageNet
64x64 instead of CIFAR10, as these are more computationally intensive to do a full grid search.
Results for a step budget K = 15 are included below. When not learning the timesteps, we ﬁx them
to a linear stride, as Table 2 shows this performs best on ImageNet 64x64."
STEPS,0.932806324110672,"Sampler \ K
15
DDSS (linear kernel)"
STEPS,0.9367588932806324,"GGDM +PRED+TIME
24.69 / 17.225
GGDM +PRED
27.08 / 16.44
GGDM +TIME
25.73 / 17.27
GGDM
28.34 / 16.63
DDSS (cubic kernel)"
STEPS,0.9407114624505929,"GGDM +PRED+TIME
26.52 / 16.29
GGDM +PRED
27.82 / 16.3
GGDM +TIME
26.87 / 16.99
GGDM
28.83 / 16.32"
STEPS,0.9446640316205533,"A.4
RESULTS ON LARGER RESOLUTION DATASETS"
STEPS,0.9486166007905138,"We include results for LSUN (Yu et al., 2015) bedrooms and churches at the 128x128 resolution.
We trained the models for 400K and 200K steps (respectively), and all other hyperparameters are
identical: we use the Adam optimizer with learning rate 0.0003 (linearly warmed up for the ﬁrst
1000 training steps), batch size 2048, gradient clipping at norms over 1.0, dropout of 0.1, and EMA
over the weights with decay rate 0.9999. We train the models using a linear stride of 1000 evenly-
spaced timesteps, ﬁxing the log-signal-to-noise-ratio schedule to a cosine function monotonically
decreasing from 20 to -20. The ELBO is reweighted with Lsimple following Ho et al. (2020), but we
additionally reweight each term by max(1, SNR) which we found to be slightly helpful in resulting
FID scores (note this is equivalent to minimizing the worst mean squared error between either the
x0 or ϵ). The UNet employs ﬁve down/up-sampling resolutions with 768 × (1, 2, 4, 6, 8) respective
channels, 3 ResNet blocks per resolution, and spatial self-attention at the 3 smallest resolutions, i.e.,
8, 16, and 32."
STEPS,0.9525691699604744,"After training the models, we run DDSS using just the GGDM model family for simplicity (i.e., we
don’t use the +PRED and +TIME we experiment with in the paper) at 5, 10 and 20 evenly-spaced
inference steps. DDSS training occurs for 50K steps, using the Adam optimizer with learning rate
of 0.0005 and batch size 512, optimizing the linear kernel for the KID loss. We compare against the
usual DDPM and DDIM(η = 0) baselines at the same inference budgets and include the FID scores
with all 1000 steps for reference. Results and samples are included below."
STEPS,0.9565217391304348,"Sampler \ K
5
10
20
1000
LSUN Bedroom"
STEPS,0.9604743083003953,"DDPM
95.38
44.84
16.88
2.547
DDIM(η = 0)
168.7
56.33
9.527
–
DDSS (GGDM)
29.15
11.01
4.817
–
LSUN Church"
STEPS,0.9644268774703557,"DDPM
96.67
51.05
16.53
2.718
DDIM(η = 0)
133.1
54.39
14.96
–
DDSS (GGDM)
30.24
11.59
6.736
–"
STEPS,0.9683794466403162,Published as a conference paper at ICLR 2022
STEPS,0.9723320158102767,"DDPM
DDIM
Ours"
STEPS,0.9762845849802372,"5 steps
10 steps
20 steps
Reference"
STEPS,0.9802371541501976,"Figure A.2: Non-cherrypicked samples for a DDPM trained on LSUN bedroom 128x128, comparing
DDPM and DDIM(η = 0) to our approach. All samples were generated with the same random seeds
and a linear stride. For reference, we include DDPM samples using all 1,000 steps (bottom left) and
real images (bottom middle)."
STEPS,0.9841897233201581,Published as a conference paper at ICLR 2022
STEPS,0.9881422924901185,"DDPM
DDIM
Ours"
STEPS,0.9920948616600791,"5 steps
10 steps
20 steps
Reference"
STEPS,0.9960474308300395,"Figure A.3: Non-cherrypicked samples for a DDPM trained on LSUN church 128x128, comparing
DDPM and DDIM(η = 0) to our approach. All samples were generated with the same random seeds
and a linear stride. For reference, we include DDPM samples using all 1,000 steps (bottom left) and
real images (bottom middle)."
