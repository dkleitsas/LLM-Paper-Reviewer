Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029069767441860465,"In many real-world multi-agent cooperative tasks, due to high cost and risk, agents
cannot continuously interact with the environment and collect experiences during
learning, but have to learn from ofﬂine datasets. However, the transition probabil-
ities calculated from the dataset can be much different from the transition prob-
abilities induced by the learned policies of other agents, creating large errors in
value estimates. Moreover, the experience distributions of agents’ datasets may
vary wildly due to diverse behavior policies, causing large difference in value
estimates between agents. Consequently, agents will learn uncoordinated subop-
timal policies. In this paper, we propose MABCQ, which exploits value deviation
and transition normalization to modify the transition probabilities. Value devia-
tion optimistically increases the transition probabilities of high-value next states,
and transition normalization normalizes the biased transition probabilities of next
states. They together encourage agents to discover potential optimal and coordi-
nated policies. Mathematically, we prove the convergence of Q-learning under the
non-stationary transition probabilities after modiﬁcation. Empirically, we show
that MABCQ greatly outperforms baselines and reduces the difference in value
estimates between agents."
INTRODUCTION,0.005813953488372093,"1
INTRODUCTION"
INTRODUCTION,0.00872093023255814,"In multi-agent cooperative tasks, agents learn from the experiences generated by continuously in-
teracting with the environment to maximize the cumulative shared reward. Recently, multi-agent
reinforcement learning (MARL) has been applied to real-world cooperative systems (Bhalla et al.,
2020; Xu et al., 2021). However, in many industrial applications, continuously interacting with the
environment and collecting the experiences during learning is costly, risky, and time-consuming.
One way to address this is ofﬂine RL, where the agent can only access a ﬁxed dataset of experiences
and learn the policy without further interacting with the environment. However, in multi-agent envi-
ronments, the dataset of each agent is often pre-collected individually by different behavior policies,
which are not necessary to be expert, and each dataset contains the individual action of the agent
instead of the joint action of all agents, e.g., autonomous driving dataset. Therefore, the dataset does
not satisfy the paradigm of centralized training, and the agent has to learn the coordinated policy in
an ofﬂine and fully decentralized way."
INTRODUCTION,0.011627906976744186,"The main challenge of ofﬂine RL is the extrapolation error, an error in value estimate incurred by the
mismatch between the experience distributions of the learned policy and the dataset (Fujimoto et al.,
2019), e.g., the distance of the learned action distribution to the behavior action distribution, and the
bias of the transition dynamics estimated from the dataset to the true transition dynamics. Recently,
almost all ofﬂine RL methods (Fujimoto et al., 2019; Levine et al., 2020; Jaques et al., 2019) focus
on constraining the learned policy to be close to the behavior policy to avoid the overestimate of the
values of out-of-distribution actions, but ignore to correct the transition bias since the deviation of
estimated transition dynamics would not be too large if the single-agent environment is stationary."
INTRODUCTION,0.014534883720930232,"However, in decentralized multi-agent environments, from the perspective of each agent, other
agents are a part of the environment, and the transition dynamics experienced by each agent de-
pend on the policies of other agents. Even in a stationary environment, the experienced transition
dynamics of each agent will change as other agents update their policies (Foerster et al., 2017).
Since the behavior policies of other agents would be inconsistent with their learned policies which"
INTRODUCTION,0.01744186046511628,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.020348837209302327,"are unknowable in decentralized multi-agent environments, the transition dynamics estimated from
the dataset by each agent would be different from the transition dynamics induced by the learned
policies of other agents, causing large errors in value estimates. The extrapolation error would
lead to suboptimal policies. Moreover, trained on different distributions of experiences collected by
various behavior policies, the estimated values of the same state might be much different between
agents, which causes that the learned policies cannot coordinate with each other."
INTRODUCTION,0.023255813953488372,"To overcome the suboptimum and miscoordination caused by transition bias in decentralized learn-
ing, we introduce value deviation and transition normalization to deliberately modify the estimated
transition probabilities from the dataset. During data collection, if one agent takes an optimal action
while other agents take suboptimal actions at a state, the transition probabilities of low-value next
states will become large. Thus, the Q-value of the optimal action will be underestimated, and the
agents will fall into suboptimum. Since the other agents are also trained, the learned policies of other
agents would become better than the behavior policies. For each agent, the transition probabilities
of high-value next states induced by the learned policies would be larger than those estimated from
the dataset. Therefore, we let each agent be optimistic toward other agents and multiply the tran-
sition probabilities by the deviation of the value of next state from the expected value over all next
states, to make the estimated transition probabilities close to the transition probabilities induced by
the learned policies of other agents."
INTRODUCTION,0.02616279069767442,"Value deviation could decrease the extrapolation error and help the agents escape from the subopti-
mum. However, in some cases, the behavior policies of other agents might be highly deterministic,
which makes the distribution of experiences unbalanced. If the transition probabilities of high-value
next states are extremely low, value deviation may not remedy the underestimate. Moreover, due
to the diversity in experience distributions of agents, the value of the same state might be overes-
timated by some agents while underestimated by others, which results in miscoordination of the
learned policies. To address these two problems, we normalize the transition probability estimated
from the dataset to be uniform. Transition normalization balances the extremely biased distribu-
tion of experiences and builds the consensus about value estimate. By combining value deviation
and transition normalization, the agents would learn high-performing and coordinated policies in an
ofﬂine and fully decentralized way."
INTRODUCTION,0.029069767441860465,"Although value deviation and transition normalization make transition dynamics non-stationary, we
mathematically prove the convergence of Q-learning under such non-stationary transition dynamics.
By importance sampling, value deviation and transition normalization take effect only as the weights
of the objective function, which makes our method easy to implement. We instantiate the proposed
method on BCQ, termed MABCQ, to additionally avoid out-of-distribution actions. Nevertheless,
it can also be implemented on other ofﬂine RL methods. We build the ofﬂine datasets and evaluate
MABCQ in four multi-agent mujoco scenarios (Todorov et al., 2012). Experimental results show
that MABCQ greatly outperforms BCQ, and ablation studies demonstrate the effectiveness of value
deviation and transition normalization. To the best of our knowledge, MABCQ is the ﬁrst method
for ofﬂine and fully decentralized multi-agent reinforcement learning."
RELATED WORK,0.03197674418604651,"2
RELATED WORK"
RELATED WORK,0.03488372093023256,"MARL. Many MARL methods have been proposed for learning to solve cooperative tasks in an
online manner. Some methods (Lowe et al., 2017; Foerster et al., 2018; Iqbal & Sha, 2019) extend
policy gradient into multi-agent cases. Value factorization methods (Sunehag et al., 2018; Rashid
et al., 2018; Son et al., 2019) decompose the joint value function into individual value functions.
Communication methods (Das et al., 2019; Ding et al., 2020) share information between agents for
better cooperation. All these methods follow centralized learning and decentralized execution, where
the agents could access the information from other agents during centralized training. However,
in our ofﬂine and decentralized setting, the datasets of agents are different; each dataset contains
individual actions instead of joint actions; and the agents cannot be trained in a centralized way."
RELATED WORK,0.0377906976744186,"For decentralized learning, the key challenge is the obsolete experiences in replay buffer. Finger-
prints (Foerster et al., 2017) deals with obsolete experience problems by conditioning the value
function on a ﬁngerprint that disambiguates the age of the sampled data. Lenient-DQN (Palmer
et al., 2018) extends the leniency concept and introduces optimism in the value function update by
forgiving suboptimal actions. Concurrent experience replay (Omidshaﬁei et al., 2017) induces cor-"
RELATED WORK,0.040697674418604654,Under review as a conference paper at ICLR 2022
RELATED WORK,0.0436046511627907,"relations in local policy updates, making agents tend to converge to the same equilibrium. However,
these methods require additional information, e.g., training iteration number, exploration rate, and
timestamp, which often are not provided by the ofﬂine dataset."
RELATED WORK,0.046511627906976744,"Ofﬂine RL. Ofﬂine RL requires the agent to learn from a ﬁxed batch of data {(s, a, s′, r)}, con-
sisting of single-step transitions without exploration. Unlike imitation learning, ofﬂine RL does
not assume that the ofﬂine data is provided by a high-performing expert but has to handle the data
generated by suboptimal or multi-modal behavior policies. Most ofﬂine RL methods consider the
out-of-distribution action (Levine et al., 2020) as the fundamental challenge, which is the main
cause of the extrapolation error (Fujimoto et al., 2019) in value estimate in the single-agent environ-
ment. To minimize the extrapolation error, some recent methods introduce constraints to enforce the
learned policy to be close to the behavior policy, which could be direct action constraint (Fujimoto
et al., 2019), kernel MMD (Kumar et al., 2019), Wasserstein distance (Wu et al., 2019), and KL
divergence (Peng et al., 2019). Some methods train a Q-function pessimistic to out-of-distribution
actions to avoid overestimation by adding a reward penalty quantiﬁed by the learned environment
model (Yu et al., 2020), by minimizing the Q-values of out-of-distribution actions (Kumar et al.,
2020; Yu et al., 2021), or by weighting the update of Q-function via Monte Carlo dropout (Wu et al.,
2021). A ﬁnite-sample analysis for ofﬂine MARL (Zhang et al., 2018) has been studied, but the
agents are assumed to get individual rewards instead of the shared reward, and be connected by
communication networks, which is not the fully decentralized setting. All these methods do not
consider the extrapolation error introduced by the transition bias, which is a fatal problem in ofﬂine
and decentralized MARL."
METHOD,0.04941860465116279,"3
METHOD"
PRELIMINARIES,0.05232558139534884,"3.1
PRELIMINARIES"
PRELIMINARIES,0.055232558139534885,"We consider N
agents in multi-agent MDP (Oliehoek & Amato, 2016) Menv
=<
S, A, R, Penv, γ > with the state space S and the joint action space A. At each timestep, each
agent i gets state s and performs an individual action ai, and the environment transitions to the next
state s′ by taking the joint action ⃗a with the transition probability Penv (s′|s,⃗a). The agents would
get a shared reward r = R (s), which is simpliﬁed to just depending on state (Schulman et al., 2015).
The agents learn to maximize the expected return E PT
t=0 γtrt, where γ is a discount factor and T
is the time horizon of the episode. However, in the fully decentralized learning, Menv is partially
observable to the agent since the agent cannot observe the joint action ⃗a. During execution, from the
perspective of each agent i, there is a viewed MDP MEi =< S, Ai, R, PEi, γ > with the individual
action space Ai and the transition probability"
PRELIMINARIES,0.05813953488372093,"PEi (s′|s, ai) =
X"
PRELIMINARIES,0.061046511627906974,"⃗a−i
Penv (s′|s,⃗a) N
Y"
PRELIMINARIES,0.06395348837209303,"j̸=i
πj(aj|s),"
PRELIMINARIES,0.06686046511627906,"where ⃗a−i denotes the joint action of all agents except agent i, and π denotes the policy of the
agent. As the transition probability depends on the policies of other agents, if other agents are also
updating their policies, PEi becomes non-stationary. Moreover, if the agent cannot interact with the
environment, PEi is unknown. Since we only investigate the inﬂuence of other agents’ policies on
PEi, we assume Penv to be deterministic."
PRELIMINARIES,0.06976744186046512,"In ofﬂine and decentralized settings, each agent i could only access a ﬁxed ofﬂine dataset Bi, which
is pre-collected by behavior policies and contains the tuples (s, ai, r, s′). As deﬁned in BCQ (Fu-
jimoto et al., 2019), the visible MDP MBi =< S, Ai, R, PBi, γ > is constructed on Bi, which has
the transition probability1"
PRELIMINARIES,0.07267441860465117,"PBi (s′|s, ai) =
num(s, ai, s′)
P"
PRELIMINARIES,0.0755813953488372,"ˆs′ num(s, ai, ˆs′),"
PRELIMINARIES,0.07848837209302326,"where num(s, ai, s′) is the number of times the tuple (s, ai, s′) is observed in Bi. However, since the
learned policies of other agents might be greatly different from the behavior policies, PBi would be
biased from PEi, which creates large extrapolation errors and differences in value estimates between
agents, and eventually leads to uncoordinated suboptimal policies."
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.08139534883720931,"1The transition probability in the following sections means the one calculated from Bi unless otherwise
stated."
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.08430232558139535,Under review as a conference paper at ICLR 2022
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.0872093023255814,Table 1: The matrix game.
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.09011627906976744,"Agent 2
a1 (0.4)
a2 (0.6)
a1 (0.8)
1
5
Agent 1"
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.09302325581395349,"a2 (0.2)
6
1"
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.09593023255813954,Table 2: Transition probabilities and expected returns calculated in the dataset.
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.09883720930232558,"action
transition
expected return
p(1|a1) = 0.4
a1
p(5|a1) = 0.6 3.4"
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.10174418604651163,p(6|a2) = 0.4
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.10465116279069768,Agent 1
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.10755813953488372,"a2
p(1|a2) = 0.6 3.0"
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.11046511627906977,"action
transition
expected return
p(1|a1) = 0.8
a1
p(6|a1) = 0.2 2.0"
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.11337209302325581,p(5|a2) = 0.8
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.11627906976744186,Agent 2
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.11918604651162791,"a2
p(1|a2) = 0.2 4.2"
THE TRANSITION PROBABILITY IN THE FOLLOWING SECTIONS MEANS THE ONE CALCULATED FROM BI UNLESS OTHERWISE,0.12209302325581395,"To intuitively illustrate the miscoordination caused by the transition bias, we devise ofﬂine datasets
in a matrix game for two agents, with the payoff depicted in Table 1. The action distributions of
the behavior polices of the two agents are [0.8, 0.2] and [0.4, 0.6], respectively. Table 2 shows the
transition probabilities and expected returns calculated by the independent agents in the datasets.
Since the datasets are collected by poor behavior policies, when one agent chooses the optimal
action, the other agent would choose the suboptimal action with a high probability, which leads to
low transition probabilities of high-value next states. Thus, the agents underestimate the optimal
actions and converge to the suboptimal policies (a1, a2), rather than the optimal policies (a2, a1)."
IMPORTANCE WEIGHTS,0.125,"3.2
IMPORTANCE WEIGHTS"
VALUE DEVIATION,0.12790697674418605,"3.2.1
VALUE DEVIATION"
VALUE DEVIATION,0.1308139534883721,"If the behavior policies of some agents are low-performing during data collection, they usually
take suboptimal actions to cooperate with the optimal actions of other agents, which leads to high
transition probabilities of low-value next states. When agent i performs Q-learning with the dataset
Bi, the Bellman operator T is approximated by the transition probability PBi (s′|s, ai) to estimate
the expectation over s′:"
VALUE DEVIATION,0.13372093023255813,"T Qi(s, ai) = Es′∼PBi(s′|s,ai)"
VALUE DEVIATION,0.13662790697674418,"
r + γ max
ˆai Qi (s′, ˆai)

."
VALUE DEVIATION,0.13953488372093023,"If PBi of a high-value s′ is lower than PEi, the Q-value of this (s, ai) pair is underestimated, which
would cause large extrapolation error and guide the agent to the convergence of suboptimal policy."
VALUE DEVIATION,0.14244186046511628,"As discussed before, during execution, the transition probability viewed by agent i is PEi and the en-
vironment is deterministic, thus PEi (s′|s, ai) only depends on the learned policies of other agents,
which are unavailable. However, since the policies of other agents are also updating towards max-
imizing the Q-values, PEi of high-value next states would grow higher than PBi. Based on this
intuition, we let each agent be optimistic towards other agents and modify PBi as"
VALUE DEVIATION,0.14534883720930233,"PBi (s′|s, ai) ∗(1 + V ∗
i (s′) −Eˆs′V ∗
i (ˆs′)
|Eˆs′V ∗
i (ˆs′)|
|
{z
}
value deviation ) ∗1"
VALUE DEVIATION,0.14825581395348839,"zvd
i
,"
VALUE DEVIATION,0.1511627906976744,"where the state value V ∗
i (s) = maxai Qi(s, ai), 1 + V ∗
i (s′)−Eˆs′V ∗
i (ˆs′)
|Eˆs′V ∗
i (ˆs′)|
is the deviation of the
value of next state from the expected value over all next states, which increases the transition
probabilities of the high-value next states and decreases those of the low-value next states, and
zvd
i
= P"
VALUE DEVIATION,0.15406976744186046,"s′ PBi (s′|s, ai) ∗(1 + V ∗
i (s′)−Eˆs′V ∗
i (ˆs′)
|Eˆs′V ∗
i (ˆs′)|
) is a normalization term to make sure the sum
of the transition probabilities is one. Value deviation makes the transition probability to be close
to PEi and hence decreases the extrapolation error. The optimism towards other agents helps the
agents escape from local optima and discover potential optimal actions which are hidden by the poor
behavior policies."
VALUE DEVIATION,0.1569767441860465,Under review as a conference paper at ICLR 2022
TRANSITION NORMALIZATION,0.15988372093023256,"3.2.2
TRANSITION NORMALIZATION"
TRANSITION NORMALIZATION,0.16279069767441862,"In real-world applications, the action distribution of behavior policy might be unbalanced, which
makes the transition probabilities PBi biased, e.g., the transition probabilities of Agent 2 (i.e., action
distribution of Agent 1) in Table 1. If the transition probability of a high-value next state is extremely
low, value deviation cannot correct the underestimate. Moreover, since Bi of each agent is individu-
ally collected by different behavior policies, the diversity in transition probabilities of agents leads to
that the value of the same state s will be overestimated by some agents, while be underestimated by
others. Since the agents are trained to reach high-value states, the large divergences on state values
will cause miscoordination of the learned policies. To overcome these problems, we normalize the
biased transition probability PBi to be uniform over next states,"
TRANSITION NORMALIZATION,0.16569767441860464,"PBi (s′|s, ai) ∗
1
PBi (s′|s, ai)
|
{z
}
transition normalization ∗1"
TRANSITION NORMALIZATION,0.1686046511627907,"ztn
i
,"
TRANSITION NORMALIZATION,0.17151162790697674,"where ztn
i
is a normalization term that is the number of different s′ given (s, ai) in Bi. Transition
normalization enforces that each agent has the same PBi when it acts the learned action a∗
i on the
same state s, and we have the following proposition.
Proposition 1. In episodic environments, if each agent i performs Q-learning on Bi, all agents will
converge to the same V ∗if they have the same transition probability on any state where each agent
i acts the learned action a∗
i ."
TRANSITION NORMALIZATION,0.1744186046511628,Proof. The proof is provided in Appendix A.
TRANSITION NORMALIZATION,0.17732558139534885,"However, to satisfy PB1 (s′|s, a∗
1) = PB2 (s′|s, a∗
2) = . . . = PBN (s′|s, a∗
N) for all s′ ∈S, the
agents should have the same set of s′ at (s, a∗), which is a strong assumption. In practice, although
the assumption is not strictly satisﬁed, transition normalization could still normalize the biased
transition distribution, encouraging the estimated state value V ∗to be close to each other."
OPTIMIZATION OBJECTIVE,0.18023255813953487,"3.2.3
OPTIMIZATION OBJECTIVE"
OPTIMIZATION OBJECTIVE,0.18313953488372092,"We combine value deviation (1 + V ∗
i (s′)−Eˆs′V ∗
i (ˆs′)
|Eˆs′V ∗
i (ˆs′)|
), which is denoted as λvdi, and transition nor-"
OPTIMIZATION OBJECTIVE,0.18604651162790697,"malization
1
PBi(s′|s,ai), which is denoted as λtni, and modify PBi as,"
OPTIMIZATION OBJECTIVE,0.18895348837209303,"ˆPBi (s′|s, ai) = PBi (s′|s, ai) ∗λtniλvdi zi
,"
OPTIMIZATION OBJECTIVE,0.19186046511627908,where zi = P
OPTIMIZATION OBJECTIVE,0.19476744186046513,"s′(1 + V ∗
i (s′)−Eˆs′V ∗
i (ˆs′)
|Eˆs′V ∗
i (ˆs′)|
) is the normalization term. In a sense, ˆPBi makes the ofﬂine
learning on Bi similar to the online decentralized MARL. In the initial stage, λvdi is close to 1 since
Qi(s, ai) is not updated, and the transition probabilities are uniform, meaning other agents are act-
ing randomly. During training, the transition probabilities of high-value states gradually grow under
value deviation, which is an analogy of that other agents are improving their policies in the online
learning. Starting from the normalized transition probabilities and changing the transition proba-
bilities following the same optimism principle, the agents increase the values of potential optimal
actions optimistically and unanimously, and build consensuses about value estimate. Therefore tran-
sition normalization and value deviation encourage the agents to learn high-performing policies and
improve coordination. Moreover, although ˆPBi is non-stationary (i.e., λvdi changes over updates of
Q-value), we have the following theorem about the convergence of Bellman operator T under ˆPBi,"
OPTIMIZATION OBJECTIVE,0.19767441860465115,"T Qi(s, ai) = Es′∼ˆ
PBi(s′|s,ai)"
OPTIMIZATION OBJECTIVE,0.2005813953488372,"
r + γ max
ˆai Qi (s′, ˆai)

."
OPTIMIZATION OBJECTIVE,0.20348837209302326,"Theorem 1. Under the non-stationary transition probability ˆPBi, the Bellman operator T is a
contraction and converges to a unique ﬁxed point when γ <
rmin
2rmax−rmin , if the reward is bounded
by the positive region [rmin, rmax]."
OPTIMIZATION OBJECTIVE,0.2063953488372093,Proof. The proof is provided in Appendix A.
OPTIMIZATION OBJECTIVE,0.20930232558139536,Under review as a conference paper at ICLR 2022
OPTIMIZATION OBJECTIVE,0.21220930232558138,"As positive afﬁne transformation of the reward function does not change the optimal policy in the
environments with ﬁxed horizon (Zhang et al., 2021), Theorem 1 holds in general. We could rescale
the reward to make rmin arbitrarily close to rmax so as to obtain a high upper bound of γ."
OPTIMIZATION OBJECTIVE,0.21511627906976744,"In deep reinforcement learning, directly modifying the transition probability is infeasible. However,
we could modify the sampling probability to achieve the same effect. The optimization objective
of decentralized deep Q-learning EpBi(s,ai,s′)|Qi(s, ai) −yi|2 is calculated by sampling the batch
from Bi according to the sampling probability pBi(s, ai, s′). By factorizing pBi(s, ai, s′), we have"
OPTIMIZATION OBJECTIVE,0.2180232558139535,"pBi(s, ai, s′)
|
{z
}
sampling probability"
OPTIMIZATION OBJECTIVE,0.22093023255813954,"= pBi(s, ai) ∗PBi(s′|s, ai)
|
{z
}
transition probability ."
OPTIMIZATION OBJECTIVE,0.2238372093023256,"Therefore, we can modify the transition probability as
λtniλvdi"
OPTIMIZATION OBJECTIVE,0.22674418604651161,"zi
PBi (s′|s, ai) and scale pBi(s, ai)
with zi. Then, the sampling probability can be re-written as"
OPTIMIZATION OBJECTIVE,0.22965116279069767,"λtniλvdipBi(s, ai, s′)
|
{z
}
modiﬁed sampling probability"
OPTIMIZATION OBJECTIVE,0.23255813953488372,"= zipBi(s, ai) ∗λtniλvdi"
OPTIMIZATION OBJECTIVE,0.23546511627906977,"zi
PBi(s′|s, ai)
|
{z
}
modiﬁed transition probability ."
OPTIMIZATION OBJECTIVE,0.23837209302325582,"Since zi is independent from s′, it could be regarded as a scale factor on pBi(s, ai).
Scaling
pBi(s, ai) will not change the expected target value yi, so sampling batches for update accord-
ing to the modiﬁed sampling probability could achieve the same effect of modifying the transition
probability. Using importance sampling, the modiﬁed optimization objective is"
OPTIMIZATION OBJECTIVE,0.24127906976744187,"EλtniλvdipBi(s,ai,s′)|Qi(s, ai) −yi|2 = EpBi(s,ai,s′)
λtniλvdipBi(s, ai, s′)"
OPTIMIZATION OBJECTIVE,0.2441860465116279,"pBi(s, ai, s′)
|Qi(s, ai) −yi|2"
OPTIMIZATION OBJECTIVE,0.24709302325581395,"= EpBi(s,ai,s′)λtniλvdi|Qi(s, ai) −yi|2,"
OPTIMIZATION OBJECTIVE,0.25,where λtni and λvdi could be seen as the weights of the objective function.
IMPLEMENTATION,0.25290697674418605,"3.3
IMPLEMENTATION"
IMPLEMENTATION,0.2558139534883721,"We implement our method on BCQ (Fujimoto et al., 2019), termed MABCQ. To make it adapt to
high-dimensional continuous spaces, for each agent i, we train a Q-network Qi, a perturbation net-
work ξi, and a conditional VAE G1
i = {E1
i
 
µ1, σ1|s, a

, D1
i
 
a|s, z1 ∼
 
µ1, σ1
}. In execution,
each agent i generates n actions by G1
i , adds small perturbations ∈[−Φ, Φ] on the actions using ξi,
and then selects the action with the highest value in Qi. The policy can be written as"
IMPLEMENTATION,0.25872093023255816,"πi(s) =
argmax
aj
i +ξi(s,aj
i)
Qi

s, aj
i + ξ

s, aj
i

,
where
n
aj
i ∼G1
i (s)
on j=1 ."
IMPLEMENTATION,0.2616279069767442,Qi is updated by minimizing
IMPLEMENTATION,0.26453488372093026,"EpBi(s,ai,s′)λtniλvdi|Qi(s, ai) −yi|2,
where yi = r + γ ˆQi(s′, ˆπi(s′)).
(1)"
IMPLEMENTATION,0.26744186046511625,"yi is calculated by the target networks ˆQi and ˆξi, where ˆπi is correspondingly the policy induced by
ˆQi and ˆξi."
IMPLEMENTATION,0.2703488372093023,ξi is updated by maximizing
IMPLEMENTATION,0.27325581395348836,"EpBi(s,ai,s′)λtniλvdiQi (s, ai + ξi (s, ai)) .
(2)"
IMPLEMENTATION,0.2761627906976744,"To estimate λvdi, we need V ∗
i (s′) = ˆQi(s′, ˆπi(s′)) and Es′[V ∗
i (s′)] =
1
γ ( ˆQi(s, ai) −r), which
can be estimated from the sample without actually going through all s′. We estimate λvdi using the
target networks to stabilize λvdi along with the updates of Qi and ξi. To avoid extreme values, we
clip λvdi to the region [1 −ϵ, 1 + ϵ], where ϵ is the optimism level."
IMPLEMENTATION,0.27906976744186046,"To estimate λtni, we train a VAE G2
i = {E2
i
 
µ2, σ2|s, a, s′
, D2
i
 
a|s, s′, z2 ∼
 
µ2, σ2
}. Since
the latent variable of VAE follows the Gaussian distribution, we use the mean as the encoding of
the input and estimate the probability density functions: ρi(s, a) ≈ρN(0,1)(µ1
i ) and ρi(s, a, s′) ≈"
IMPLEMENTATION,0.2819767441860465,Under review as a conference paper at ICLR 2022
IMPLEMENTATION,0.28488372093023256,Algorithm 1 MABCQ
IMPLEMENTATION,0.2877906976744186,"1: for i ∈N do
2:
Initialize the conditional VAEs:
G1
i = {E1
i
 
µ1, σ1|s, a

, D1
i
 
a|s, z1
}, G2
i = {E2
i
 
µ2, σ2|s, a, s′
, D2
i
 
a|s, s′, z2
}."
IMPLEMENTATION,0.29069767441860467,"3:
Initialize Q-network Qi, perturbation network ξi, and their target networks ˆQi and ˆξi.
4:
Fit the VAEs G1
i and G2
i using Bi.
5:
for t = 1, . . . , max update do
6:
Sample a mini-batch from Bi.
7:
Update Qi by minimizing (1).
8:
Update ξi by maximizing (2).
9:
Update the target networks ˆQi and ˆξi.
10:
end for
11: end for"
IMPLEMENTATION,0.2936046511627907,"ρN(0,1)(µ2
i ), where ρN(0,1) is the density of unit Gaussian distribution. The conditional density is"
IMPLEMENTATION,0.29651162790697677,"ρi(s′|a, s) ≈
ρN (0,1)(µ2
i )
ρN (0,1)(µ1
i ) and the transition probability is PBi(s′|s, ai) ≈
R s′+ 1"
IMPLEMENTATION,0.29941860465116277,"2 δS
s′−1"
IMPLEMENTATION,0.3023255813953488,"2 δS ρi(s′|s, a)ds′ ≈
ρi(s′|s, a) ∥δS∥when the integral interval ∥δS∥is a small constant. Approximately, we have"
IMPLEMENTATION,0.30523255813953487,"λtni = ρN(0,1)(µ1
i )
ρN(0,1)(µ2
i ),"
IMPLEMENTATION,0.3081395348837209,"and the constant ∥δS∥is considered in zi. In practice, we ﬁnd that λtni falls into the region [0.2, 1.4]
for almost all samples. For completeness, we summarize the training procedure of MABCQ in
Algorithm 1."
EXPERIMENTS,0.311046511627907,"4
EXPERIMENTS"
MATRIX GAME,0.313953488372093,"4.1
MATRIX GAME"
MATRIX GAME,0.3168604651162791,"We perform MABCQ on the matrix game in Table 1. As shown in Table 3, if we only use λvd
without considering transition normalization, as the transition probabilities of high-value next states
have been increased, for agent 1 the value of a2 becomes higher than that of a1. However, due
to the unbalanced action distribution of agent 1, the initial transition probabilities of agent 2 are
extremely biased. With λvd, agent 2 still underestimates the value of a1 and learns the action a2.
The agents arrive at the joint action (a2, a2), which is a worse solution than the initial one (Table 2).
Normalizing the biased distribution by λtn, the agents could learn the optimal solution (a2, a1) and
build the consensus about the values of learned actions, as shown in Table 4."
MATRIX GAME,0.31976744186046513,Table 3: Transition probabilities and expected returns calculated in the dataset using only λvd.
MATRIX GAME,0.3226744186046512,"action
transition
expected return
p(1|a1) = 0.12
a1
p(5|a1) = 0.88 4.52"
MATRIX GAME,0.32558139534883723,p(6|a2) = 0.8
MATRIX GAME,0.32848837209302323,Agent 1
MATRIX GAME,0.3313953488372093,"a2
p(1|a2) = 0.2 5"
MATRIX GAME,0.33430232558139533,"action
transition
expected return
p(1|a1) = 0.4
a1
p(6|a1) = 0.6 4"
MATRIX GAME,0.3372093023255814,p(5|a2) = 0.95
MATRIX GAME,0.34011627906976744,Agent 2
MATRIX GAME,0.3430232558139535,"a2
p(1|a2) = 0.05 4.8"
MATRIX GAME,0.34593023255813954,Table 4: Transition probabilities and expected returns calculated in the dataset using λtn and λvd.
MATRIX GAME,0.3488372093023256,"action
transition
expected return
p(1|a1) = 0.17
a1
p(5|a1) = 0.83 4.33"
MATRIX GAME,0.35174418604651164,p(6|a2) = 0.86
MATRIX GAME,0.3546511627906977,Agent 1
MATRIX GAME,0.35755813953488375,"a2
p(1|a2) = 0.14 5.29"
MATRIX GAME,0.36046511627906974,"action
transition
expected return
p(1|a1) = 0.14
a1
p(6|a1) = 0.86 5.29"
MATRIX GAME,0.3633720930232558,p(5|a2) = 0.83
MATRIX GAME,0.36627906976744184,Agent 2
MATRIX GAME,0.3691860465116279,"a2
p(1|a2) = 0.17 4.33"
MATRIX GAME,0.37209302325581395,Under review as a conference paper at ICLR 2022
MATRIX GAME,0.375,"(a) HalfCheetah
(b) Walker
(c) Hopper
(d) Ant"
MATRIX GAME,0.37790697674418605,Figure 1: Illustrations of the scenarios. Different colors indicate different agents.
MATRIX GAME,0.3808139534883721,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 600 800 1000 1200 1400 1600 1800 2000 2200"
MATRIX GAME,0.38372093023255816,Reward
MATRIX GAME,0.3866279069767442,"MABCQ
MABCQ w/o λtn
MABCQ w/o λvd
BCQ
DDPG
Behavior"
MATRIX GAME,0.38953488372093026,(a) HalfCheetah
MATRIX GAME,0.39244186046511625,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 0 500 1000 1500 2000 2500 3000 3500"
MATRIX GAME,0.3953488372093023,Reward
MATRIX GAME,0.39825581395348836,"MABCQ
MABCQ w/o λtn
MABCQ w/o λvd
BCQ
DDPG
Behavior"
MATRIX GAME,0.4011627906976744,(b) Walker
MATRIX GAME,0.40406976744186046,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 0 500 1000 1500 2000"
MATRIX GAME,0.4069767441860465,Reward
MATRIX GAME,0.40988372093023256,"MABCQ
MABCQ w/o λtn
MABCQ w/o λvd
BCQ
DDPG
Behavior"
MATRIX GAME,0.4127906976744186,(c) Hopper
MATRIX GAME,0.41569767441860467,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 500 1000 1500 2000 2500 3000"
MATRIX GAME,0.4186046511627907,Reward
MATRIX GAME,0.42151162790697677,"MABCQ
MABCQ w/o λtn
MABCQ w/o λvd
BCQ
DDPG
Behavior"
MATRIX GAME,0.42441860465116277,(d) Ant
MATRIX GAME,0.4273255813953488,"Figure 2: Learning curves of MABCQ and the baselines in HalfCheetah, Walker, Hopper, and Ant. The curves
are plotted based the mean and standard deviation of ﬁve runs with difference random seeds."
ENVIRONMENTS AND DATASETS,0.43023255813953487,"4.2
ENVIRONMENTS AND DATASETS"
ENVIRONMENTS AND DATASETS,0.4331395348837209,"To evaluate the effectiveness of MABCQ in high-dimensional complex environments, we adopt
multi-agent mujoco (de Witt et al., 2020), which splits the original action space of the mujoco tasks
(Todorov et al., 2012; Brockman et al., 2016) into several sub-spaces. We consider four tasks, which
are HalfCheetah, Walker, Hopper, and Ant. As illustrated in Figure 1, different colors indicate
different agents. Each agent independently controls one or some joints of the robot and could get
the state and reward of the robot, which are deﬁned in the original tasks."
ENVIRONMENTS AND DATASETS,0.436046511627907,"For each environment, we collect N datasets for the N agents. Each dataset contains 1 million
transitions (s, ai, r, s′, done). For data collection, we train an intermediate policy and an expert
policy for each agent using SAC algorithm (Haarnoja et al., 2018) provided by OpenAI Spinning
Up (Achiam, 2018). The ofﬂine dataset Bi is a mixture of four parts: 20% transitions are split from
the experiences generated by the SAC agent at the early training, 35% transitions are generated from
that the agent i acts the intermediate policy while other agents act the expert policies, 35% transitions
are generated from that agent i performs the expert policy while other agents act the intermediate
policies, 10% transitions are generated from that all agents perform the expert policies. For the last
three parts, we add a small noise to the policies to increase the diversity of the dataset."
ENVIRONMENTS AND DATASETS,0.438953488372093,We compare MABCQ against the following methods:
ENVIRONMENTS AND DATASETS,0.4418604651162791,"• MABCQ w/o λtn. Removing λtn from MABCQ.
• MABCQ w/o λvd. Removing λvd from MABCQ.
• BCQ. Removing both λtn and λvd from MABCQ.
• DDPG (Lillicrap et al., 2016). Each agent i is trained using independent DDPG on the
ofﬂine Bi without action constraint and transition probability modiﬁcation.
• Behavior. Each agent i takes the action generated from the VAE G1
i ."
ENVIRONMENTS AND DATASETS,0.44476744186046513,"The baselines have the same neural network architectures and hyperparameters as MABCQ. All the
models are trained for ﬁve runs with different random seeds. All the learning curves are plotted using
mean and standard deviation. More details about hyperparameters are available in Appendix C."
PERFORMANCE AND ABLATION,0.4476744186046512,"4.3
PERFORMANCE AND ABLATION"
PERFORMANCE AND ABLATION,0.45058139534883723,"Figure 2 shows the learning curves of all the methods in the four tasks. Without action constraint
and transition probability modiﬁcation, DDPG severely suffers from the large extrapolation error and
can hardly improve the performance throughout the training. BCQ outperforms the behavior policies"
PERFORMANCE AND ABLATION,0.45348837209302323,Under review as a conference paper at ICLR 2022
PERFORMANCE AND ABLATION,0.4563953488372093,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 50 100 150 200 250 300"
PERFORMANCE AND ABLATION,0.45930232558139533,"V ∗
max −V ∗
min"
PERFORMANCE AND ABLATION,0.4622093023255814,"MABCQ
MABCQ w/o λtn"
PERFORMANCE AND ABLATION,0.46511627906976744,(a) HalfCheetah
PERFORMANCE AND ABLATION,0.4680232558139535,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 0 20 40 60 80 100 120"
PERFORMANCE AND ABLATION,0.47093023255813954,"V ∗
max −V ∗
min"
PERFORMANCE AND ABLATION,0.4738372093023256,"MABCQ
MABCQ w/o λtn"
PERFORMANCE AND ABLATION,0.47674418604651164,(b) Walker
PERFORMANCE AND ABLATION,0.4796511627906977,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 10 15 20 25 30 35"
PERFORMANCE AND ABLATION,0.48255813953488375,"V ∗
max −V ∗
min"
PERFORMANCE AND ABLATION,0.48546511627906974,"MABCQ
MABCQ w/o λtn"
PERFORMANCE AND ABLATION,0.4883720930232558,(c) Hopper
PERFORMANCE AND ABLATION,0.49127906976744184,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 20 40 60 80 100"
PERFORMANCE AND ABLATION,0.4941860465116279,"V ∗
max −V ∗
min"
PERFORMANCE AND ABLATION,0.49709302325581395,"MABCQ
MABCQ w/o λtn"
PERFORMANCE AND ABLATION,0.5,(d) Ant
PERFORMANCE AND ABLATION,0.502906976744186,"Figure 3: Difference in value estimates among agents along with the training in HalfCheetah, Walker, Hopper,
and Ant. It is shown that transition normalization indeed reduces the difference in value estimates."
PERFORMANCE AND ABLATION,0.5058139534883721,Table 5: Extrapolation errors of MABCQ and BCQ.
PERFORMANCE AND ABLATION,0.5087209302325582,"HalfCheetah
Walker
Hopper
Ant"
PERFORMANCE AND ABLATION,0.5116279069767442,"MABCQ
98.4 ± 31.3
55.0 ± 9.6
28.1 ± 3.4
180.2 ± 22.2
BCQ
97.2 ± 29.1
91.5 ± 35.4
65.8 ± 6.4
231.3 ± 47"
PERFORMANCE AND ABLATION,0.5145348837209303,"but only arrives at the mediocre performance. In Figure 2(a) and Figure 2(c), the performance of
BCQ even descends in the later stage of learning. During the collection of Bi, when agent i takes
a “good” action, other agents usually take “bad” actions, making BCQ underestimate the “good”
actions, especially in the latter stage. The learning curves of MABCQ w/o λvd are similar to those
of BCQ in the ﬁrst three tasks. That is because other agents’ policies are assumed to be random using
only transition normalization, which is far from the learned policies and leads to large extrapolation
errors. But in Ant, MABCQ w/o λvd outperforms BCQ in the later stage, which is attributed to the
value consensus built by the normalized transition probabilities. By optimistically increasing the
transition probabilities of high-value next states, MABCQ w/o λtn encourages the agents to learn
potential optimal actions and obviously boosts the performance. MABCQ combines the advantages
of both value deviation and transition normalization and outperforms other baselines."
PERFORMANCE AND ABLATION,0.5174418604651163,"To interpret the effectiveness of transition normalization, we uniformly sample a subset from the
union of all agents’ states and calculate the difference in value estimates, maxi V ∗
i −mini V ∗
i ,
on this subset, where V ∗
i is calculated as Qi(s, πi(s)). The results are illustrated in Figure 3. The
maxi V ∗
i −mini V ∗
i of MABCQ is lower than that of MABCQ w/o λtn, which veriﬁes that transition
normalization could decrease the difference in value estimates among agents. If there is a consensus
among agents about which states are high-value, the agents would select the actions that most likely
lead to the common high-value states. This promotes the coordination of policies and helps MABCQ
outperform MABCQ w/o λtn."
PERFORMANCE AND ABLATION,0.5203488372093024,"In Table 5, we present the extrapolation errors of MABCQ and BCQ, | 1 N
P"
PERFORMANCE AND ABLATION,0.5232558139534884,"i Qi(s, ai) −R|, where
R is the true value evaluated by Monte Carlo return. Although MABCQ greatly outperforms BCQ
(i.e., higher return), it still achieves smaller extrapolation errors than BCQ in Walker, Hopper, and
Ant, which empirically veriﬁes our claim that the proposed weights could decrease the extrapolation
error."
PERFORMANCE AND ABLATION,0.5261627906976745,"We also investigate the effect of optimism level ϵ, the computation efﬁciency of MABCQ, and the
performance of the proposed weights on other datasets and another ofﬂine RL algorithm, i.e., CQL
(Kumar et al., 2020). Due to the space limit, these results are given in Appendix B."
CONCLUSION,0.5290697674418605,"5
CONCLUSION"
CONCLUSION,0.5319767441860465,"In this paper, we proposed MABCQ for ofﬂine and fully decentralized multi-agent reinforcement
learning. MABCQ modiﬁes the transition probability by value deviation that increases the transition
probabilities of high-value next states, and by transition normalization that normalizes the biased
transition probabilities. Mathematically, we show that under the non-stationary transition probability
after modiﬁcation, ofﬂine decentralized Q-learning converges to a unique ﬁxed point. Empirically,
we show that MABCQ could help the agents escape from the suboptimum, learn coordinated poli-
cies, and greatly outperform the baselines in a variety of multi-agent ofﬂine datasets."
CONCLUSION,0.5348837209302325,Under review as a conference paper at ICLR 2022
REFERENCES,0.5377906976744186,REFERENCES
REFERENCES,0.5406976744186046,Joshua Achiam. Spinning Up in Deep Reinforcement Learning. 2018.
REFERENCES,0.5436046511627907,"Sushrut Bhalla, Sriram Ganapathi Subramanian, and Mark Crowley. Deep multi agent reinforcement
learning for autonomous driving. In Canadian Conference on Artiﬁcial Intelligence (Canadian
AI), 2020."
REFERENCES,0.5465116279069767,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016."
REFERENCES,0.5494186046511628,"Abhishek Das, Th´eophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and
Joelle Pineau. Tarmac: Targeted multi-agent communication. In International Conference on
Machine Learning (ICML), 2019."
REFERENCES,0.5523255813953488,"Christian Schroeder de Witt, Bei Peng, Pierre-Alexandre Kamienny, Philip Torr, Wendelin B¨ohmer,
and Shimon Whiteson. Deep multi-agent reinforcement learning for decentralized continuous
cooperative control. arXiv preprint arXiv:2003.06709, 2020."
REFERENCES,0.5552325581395349,"Ziluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for
multi-agent cooperation. In Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.5581395348837209,"Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet
Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement
learning. In International Conference on Machine Learning (ICML), 2017."
REFERENCES,0.561046511627907,"Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon White-
son. Counterfactual multi-agent policy gradients. In AAAI Conference on Artiﬁcial Intelligence
(AAAI), 2018."
REFERENCES,0.563953488372093,"Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.5668604651162791,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning (ICML), 2019."
REFERENCES,0.5697674418604651,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning (ICML), 2018."
REFERENCES,0.5726744186046512,"Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning (ICML), 2019."
REFERENCES,0.5755813953488372,"Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019."
REFERENCES,0.5784883720930233,"Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-
learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems
(NeurIPS), 2019."
REFERENCES,0.5813953488372093,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.5843023255813954,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.5872093023255814,"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Inter-
national Conference on Learning Representations (ICLR), 2016."
REFERENCES,0.5901162790697675,"Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems (NeurIPS), 2017."
REFERENCES,0.5930232558139535,Under review as a conference paper at ICLR 2022
REFERENCES,0.5959302325581395,"Frans A Oliehoek and Christopher Amato.
A concise introduction to decentralized POMDPs.
Springer, 2016."
REFERENCES,0.5988372093023255,"Shayegan Omidshaﬁei, Jason Pazis, Christopher Amato, Jonathan P How, and John Vian. Deep
decentralized multi-task multi-agent reinforcement learning under partial observability. In Inter-
national Conference on Machine Learning (ICML), 2017."
REFERENCES,0.6017441860465116,"Gregory Palmer, Karl Tuyls, Daan Bloembergen, and Rahul Savani. Lenient multi-agent deep rein-
forcement learning. In International Conference on Autonomous Agents and MultiAgent Systems
(AAMAS), 2018."
REFERENCES,0.6046511627906976,"Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019."
REFERENCES,0.6075581395348837,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foer-
ster, and Shimon Whiteson. Qmix: monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning (ICML), 2018."
REFERENCES,0.6104651162790697,"John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning (ICML), 2015."
REFERENCES,0.6133720930232558,"Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning (ICML), 2019."
REFERENCES,0.6162790697674418,"Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In International Conference
on Autonomous Agents and Multiagent Systems (AAMAS), 2018."
REFERENCES,0.6191860465116279,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2012."
REFERENCES,0.622093023255814,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.625,"Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and
Hanlin Goh. Uncertainty weighted actor-critic for ofﬂine reinforcement learning. arXiv preprint
arXiv:2105.08140, 2021."
REFERENCES,0.627906976744186,"Bingyu Xu, Yaowei Wang, Zhaozhi Wang, Huizhu Jia, and Zongqing Lu. Hierarchically and co-
operatively learning trafﬁc signal control. In AAAI Conference on Artiﬁcial Intelligence (AAAI),
2021."
REFERENCES,0.6308139534883721,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. Mopo: Model-based ofﬂine policy optimization. Advances in Neural Information
Processing Systems (NeurIPS), 2020."
REFERENCES,0.6337209302325582,"Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
Combo: Conservative ofﬂine model-based policy optimization. arXiv preprint arXiv:2102.08363,
2021."
REFERENCES,0.6366279069767442,"Chi Zhang, Sanmukh Rao Kuppannagari, and Viktor Prasanna. Brac+: Going deeper with behavior
regularized ofﬂine reinforcement learning, 2021."
REFERENCES,0.6395348837209303,"Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Bas¸ar. Finite-sample analysis for
decentralized batch multi-agent reinforcement learning with networked agents. arXiv preprint
arXiv:1812.02783, 2018."
REFERENCES,0.6424418604651163,Under review as a conference paper at ICLR 2022
REFERENCES,0.6453488372093024,"A
PROOFS"
REFERENCES,0.6482558139534884,"Proposition 1. In episodic environments, if each agent i performs Q-learning on Bi, all agents will
converge to the same V ∗if they have the same transition probability on any state where each agent
i acts the learned action a∗
i ."
REFERENCES,0.6511627906976745,"Proof. Considering the two-agent case, we deﬁne δ(s) as the difference in the V ∗."
REFERENCES,0.6540697674418605,"δ(s) = V ∗
1 (s) −V ∗
2 (s) =
X"
REFERENCES,0.6569767441860465,"s′
PB1 (s′|s, a∗
1) (r + γV ∗
1 (s′)) −
X"
REFERENCES,0.6598837209302325,"s′
PB2 (s′|s, a∗
2) (r + γV ∗
2 (s′)) =
X"
REFERENCES,0.6627906976744186,"s′
PB1 (s′|s, a∗
1) (r + γV ∗
2 (s′) + γV ∗
1 (s′) −γV ∗
2 (s′)) −
X"
REFERENCES,0.6656976744186046,"s′
PB2 (s′|s, a∗
2) (r + γV ∗
2 (s′)) =
X"
REFERENCES,0.6686046511627907,"s′
(PB1 (s′|s, a∗
1) −PB2 (s′|s, a∗
2)) (r + γV ∗
2 (s′)) + γPB1 (s′|s, a∗
1) δ (s′)"
REFERENCES,0.6715116279069767,"For the terminal state send, we have δ(send) = 0. If PB1 (s′|s, a∗
1) = PB2 (s′|s, a∗
2) , ∀s′ ∈S,
recursively expanding the δ term, we arrive at δ(s) = 0 + γ0 + γ20 + ... + 0 = 0. We can easily
show that it also holds in the N-agent case."
REFERENCES,0.6744186046511628,"Theorem 1.
Under the non-stationary transition probability ˆPBi, the Bellman operator T is a
contraction and converges to a unique ﬁxed point when γ <
rmin
2rmax−rmin , if the reward is bounded
by the positive region [rmin, rmax]."
REFERENCES,0.6773255813953488,"Proof. We initialize the Q-value to be ηrmin, where η denotes
1−γT +1"
REFERENCES,0.6802325581395349,"1−γ
.
Since the reward is
bounded by the positive region [rmin, rmax], the Q-value under the operator T is bounded to
[ηrmin, ηrmax]. Based on the deﬁnition of ˆPBi (s′|s, ai), it can be written as
V ∗
i (s′)
P"
REFERENCES,0.6831395348837209,"ˆs′ V ∗
i (s′), where
V ∗
i (s′) = maxˆai Qi (s′, ˆai). Then, we have the following,
T Q1
i −T Q2
i

∞"
REFERENCES,0.686046511627907,"= max
s,ai  X"
REFERENCES,0.688953488372093,"s′∈S
ˆP 1
Bi (s′|s, ai)

r + γ max
ˆai Q1
i (s′, ˆai)

−
X"
REFERENCES,0.6918604651162791,"s′∈S
ˆP 2
Bi (s′|s, ai)

r + γ max
ˆai Q2
i (s′, ˆai)
"
REFERENCES,0.6947674418604651,"= max
s,ai γ  P"
REFERENCES,0.6976744186046512,"s′∈S(V ∗1
i (s′))2
P"
REFERENCES,0.7005813953488372,"s′∈S V ∗1
i (s′)
−
P"
REFERENCES,0.7034883720930233,"s′∈S(V ∗2
i (s′))2
P"
REFERENCES,0.7063953488372093,"s′∈S V ∗2
i (s′) "
REFERENCES,0.7093023255813954,"= max
s,ai γ  P"
REFERENCES,0.7122093023255814,"s′∈S(V ∗1
i (s′))2 −(V ∗2
i (s′))2
P"
REFERENCES,0.7151162790697675,"s′∈S V ∗1
i (s′)
−
X"
REFERENCES,0.7180232558139535,"s′∈S
(V ∗2
i (s′))2
 
1
P"
REFERENCES,0.7209302325581395,"s′∈S V ∗2
i (s′) −
1
P"
REFERENCES,0.7238372093023255,"s′∈S V ∗1
i (s′) !"
REFERENCES,0.7267441860465116,"= max
s,ai γ  P"
REFERENCES,0.7296511627906976,"s′∈S(V ∗1
i (s′) −V ∗2
i (s′))(V ∗1
i (s′) + V ∗2
i (s′))
P"
REFERENCES,0.7325581395348837,"s′∈S V ∗1
i (s′)
−
X"
REFERENCES,0.7354651162790697,"s′∈S
(V ∗2
i (s′))2
P"
REFERENCES,0.7383720930232558,"s′∈S V ∗1
i (s′) −V ∗2
i (s′)
P"
REFERENCES,0.7412790697674418,"s′ V ∗1
i (s′) P"
REFERENCES,0.7441860465116279,"s′∈S V ∗2
i (s′) "
REFERENCES,0.747093023255814,"≤max
s,ai γ  X"
REFERENCES,0.75,"s′∈S
(V ∗1
i (s′) −V ∗2
i (s′))"
REFERENCES,0.752906976744186,"∗
1
P
s′∈S V ∗1
i (s′) ∗max"
REFERENCES,0.7558139534883721,"(V ∗1
i (s′) + V ∗2
i (s′)) −
P"
REFERENCES,0.7587209302325582,"s′∈S(V ∗2
i (s′))2
P
s′∈S V ∗2
i (s′) "
REFERENCES,0.7616279069767442,"≤γ|S|
Q1
i −Q2
i

∞∗
1
|S|ηrmin
∗η(2rmax −rmin)"
REFERENCES,0.7645348837209303,= γ(2rmax
REFERENCES,0.7674418604651163,"rmin
−1)
Q1
i −Q2
i

∞."
REFERENCES,0.7703488372093024,"The third term of the penultimate line is because: if V ∗1
i (s′) + V ∗2
i (s′) > P"
REFERENCES,0.7732558139534884,"s′∈S(V ∗2
i
(s′))2
P"
REFERENCES,0.7761627906976745,"s′∈S V ∗2
i
(s′) ,"
REFERENCES,0.7790697674418605,"V ∗1
i (s′)+V ∗2
i (s′)−
P"
REFERENCES,0.7819767441860465,"s′∈S(V ∗2
i (s′))2
P"
REFERENCES,0.7848837209302325,"s′∈S V ∗2
i (s′)
≤V ∗1
i (s′)+V ∗2
i (s′)−
P"
REFERENCES,0.7877906976744186,"s′∈S(V ∗2
i (s′)) ∗ηrmin
P"
REFERENCES,0.7906976744186046,"s′∈S V ∗2
i (s′)
≤2ηrmax−ηrmin,"
REFERENCES,0.7936046511627907,Under review as a conference paper at ICLR 2022
REFERENCES,0.7965116279069767,"else,
P"
REFERENCES,0.7994186046511628,"s′∈S(V ∗2
i (s′))2
P"
REFERENCES,0.8023255813953488,"s′∈S V ∗2
i (s′)
−(V ∗1
i (s′) + V ∗2
i (s′)) ≤
P"
REFERENCES,0.8052325581395349,"s′∈S(V ∗2
i (s′)) ∗ηrmax
P"
REFERENCES,0.8081395348837209,"s′∈S V ∗2
i (s′)
≤ηrmax."
REFERENCES,0.811046511627907,"Since 2ηrmax −ηrmin ≥ηrmax, we have"
REFERENCES,0.813953488372093,"|(V ∗1
i (s′) + V ∗2
i (s′)) −
P"
REFERENCES,0.8168604651162791,"s′∈S(V ∗2
i (s′))2
P"
REFERENCES,0.8197674418604651,"s′∈S V ∗2
i (s′) | ≤2ηrmax −ηrmin."
REFERENCES,0.8226744186046512,"Therefore, if γ <
rmin
2rmax−rmin , the operator T is a contraction. By contraction mapping theorem, T
converges to a unique ﬁxed point."
REFERENCES,0.8255813953488372,"B
ADDITIONAL RESULTS"
REFERENCES,0.8284883720930233,"To clearly present the performance gain of MABCQ over BCQ, we summarize the mean value and
standard deviation after 5 × 104 updates in Table 6, where we can see that MABCQ performs more
than one standard deviation better than BCQ."
REFERENCES,0.8313953488372093,Table 6: Comparison between MABCQ and BCQ.
REFERENCES,0.8343023255813954,"HalfCheetah
Walker
Hopper
Ant"
REFERENCES,0.8372093023255814,"MABCQ
1905 ± 126
2498 ± 256
1383 ± 443
3027 ± 178
BCQ
1384 ± 360
1324 ± 662
566 ± 89
2573 ± 637"
REFERENCES,0.8401162790697675,"To evaluate MABCQ in low-quality dataset, we carried out an additional experiment on 2-agent
Swimmer with another data collection approach, where each dataset only contains the experiences
at the early training and the experiences generated by intermediate policies. As shown in Figure 4,
after removing the expert experiences, MABCQ greatly outperforms BCQ, and BCQ even cannot
outperform the behavior policies."
REFERENCES,0.8430232558139535,"0
2 × 102
4 × 102
6 × 102
8 × 102
10 × 102
Updates −20 −10 0 10 20 30 40 50"
REFERENCES,0.8459302325581395,Reward
REFERENCES,0.8488372093023255,"MABCQ
MABCQ w/o β1
MABCQ w/o β2
BCQ
DDPG
Behavior"
REFERENCES,0.8517441860465116,Figure 4: Learning curves of MABCQ and the baselines in Swimmer.
REFERENCES,0.8546511627906976,"The optimism level ϵ controls the strength of value deviation. If ϵ is too small, value deviation has
weak effects on the objective function. But if ϵ is too large, the agent will be over optimistic about
other agents’ learned policies, which could result in large extrapolation errors and uncoordinated
policies. Figure 5 shows the learning curves of MABCQ with different ϵ. It is commonly observed
that increasing ϵ elevates the performance, especially in HalfCheetah. However, in Walker and Ant,"
REFERENCES,0.8575581395348837,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 600 800 1000 1200 1400 1600 1800 2000 2200"
REFERENCES,0.8604651162790697,Reward
REFERENCES,0.8633720930232558,"ϵ = 0.8
ϵ = 0.64
ϵ = 0.48
ϵ = 0.32
ϵ = 0"
REFERENCES,0.8662790697674418,(a) HalfCheetah
REFERENCES,0.8691860465116279,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 500 1000 1500 2000 2500 3000 3500"
REFERENCES,0.872093023255814,Reward
REFERENCES,0.875,"ϵ = 0.8
ϵ = 0.64
ϵ = 0.48
ϵ = 0.32
ϵ = 0"
REFERENCES,0.877906976744186,(b) Walker
REFERENCES,0.8808139534883721,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 0 250 500 750 1000 1250 1500 1750 2000"
REFERENCES,0.8837209302325582,Reward
REFERENCES,0.8866279069767442,"ϵ = 0.8
ϵ = 0.64
ϵ = 0.48
ϵ = 0.32
ϵ = 0"
REFERENCES,0.8895348837209303,(c) Hopper
REFERENCES,0.8924418604651163,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 500 1000 1500 2000 2500 3000 3500"
REFERENCES,0.8953488372093024,Reward
REFERENCES,0.8982558139534884,"ϵ = 0.8
ϵ = 0.64
ϵ = 0.48
ϵ = 0.32
ϵ = 0"
REFERENCES,0.9011627906976745,(d) Ant
REFERENCES,0.9040697674418605,"Figure 5: Learning curves of MABCQ with different ϵ in HalfCheetah, Walker, Hopper, and Ant. It is shown
that with any positive ϵ, MABCQ does not underperform that without value deviation."
REFERENCES,0.9069767441860465,Under review as a conference paper at ICLR 2022
REFERENCES,0.9098837209302325,Table 7: Average time taken by one update.
REFERENCES,0.9127906976744186,"HalfCheetah
Walker
Hopper
Ant"
REFERENCES,0.9156976744186046,"MABCQ
18 ms
18 ms
16 ms
20 ms
BCQ
10 ms
10 ms
9 ms
11 ms"
REFERENCES,0.9186046511627907,"if we set a large ϵ (0.8), the performance slightly drops due to overoptimism. Even so, with any
positive ϵ, MABCQ does not underperform that without value deviation."
REFERENCES,0.9215116279069767,"To demonstrate the computation efﬁciency of our method, we record the average time taken by one
update in Table 7. The experiments are carried out on Intel i7-8700 CPU and NVIDIA GTX 1080Ti
GPU. Since λvd and λtn could be calculated from the sampled experience without actually going
through all next states, MABCQ additionally needs only two forward passes for computing λvd and
λtn in the update and costs less than twice the training time of BCQ. Moreover, since MABCQ is
fully decentralized, the learning of agents can be entirely parallelized. Therefore, MABCQ scales
well with the number of agents."
REFERENCES,0.9244186046511628,"0
1 × 104
2 × 104
3 × 104
4 × 104
5 × 104
Updates 0 1000 2000 3000 4000 5000"
REFERENCES,0.9273255813953488,Reward
REFERENCES,0.9302325581395349,"MACQL
CQL"
REFERENCES,0.9331395348837209,(a) HalfCheetah
REFERENCES,0.936046511627907,"0
2 × 104
4 × 104
6 × 104
8 × 104
10 × 104
Updates 0 500 1000 1500 2000 2500"
REFERENCES,0.938953488372093,Reward
REFERENCES,0.9418604651162791,"MACQL
CQL"
REFERENCES,0.9447674418604651,(b) Hopper
REFERENCES,0.9476744186046512,"0
3 × 104
6 × 104
9 × 104
12 × 104
Updates 0 200 400 600 800 1000"
REFERENCES,0.9505813953488372,Reward
REFERENCES,0.9534883720930233,"MACQL
CQL"
REFERENCES,0.9563953488372093,(c) Walker
REFERENCES,0.9593023255813954,Figure 6: Learning curves of MACQL in D4RL medium-replay datasets.
REFERENCES,0.9622093023255814,"The two proposed weights λvd and λtn could also be extended to other ofﬂine RL methods. We in-
troduce the two weights to CQL (Kumar et al., 2020), as MACQL, and test them in D4RL medium-
replay datasets (Fu et al., 2020). CQL augments the standard Bellman error objective with con-
servative regularizer, thus the learned Q-value may not represent the original meaning and thus not
theoretically match value deviation. However, MACQL still outperforms CQL, as shown in Figure 6."
REFERENCES,0.9651162790697675,"C
EXPERIMENTAL SETTINGS AND HYPERPARAMETERS"
REFERENCES,0.9680232558139535,The experimental settings and hyperparameters are summarized in Table 8.
REFERENCES,0.9709302325581395,Table 8: Experimental settings and hyperparameters
REFERENCES,0.9738372093023255,"Hyperparameter
HalfCheetah
Walker
Hopper
Ant"
REFERENCES,0.9767441860465116,"agent number (N)
2
2
3
4
state space
17
17
11
27
action space
3
3
1
2
horizon (T)
1000
discount (γ)
0.99
Bi size
106"
REFERENCES,0.9796511627906976,"batch size
1024
MLP units
(64, 64)
MLP activation
ReLU
learning rate of Q
10−3"
REFERENCES,0.9825581395348837,"learning rate of ξ
10−4"
REFERENCES,0.9854651162790697,"learning rate of G
10−4"
REFERENCES,0.9883720930232558,"ϵ
0.80
0.48
0.80
0.64
Φ
0.05
n
10
VAE hidden space
10"
REFERENCES,0.9912790697674418,Under review as a conference paper at ICLR 2022
REFERENCES,0.9941860465116279,"D
LIMITATION AND FUTURE WORK"
REFERENCES,0.997093023255814,"Although we consider the setting where each agent can get the state of the environment, MABCQ
could also be potentially applied to partially observable environments. However, if the partial ob-
servability is too limited, Q-value may not be accurately estimated from the observation. Many
MARL methods adopt recurrent neural networks to utilize the historical information, which how-
ever is impractical if the timestamp is not included in the ofﬂine dataset. Moreover, if a state is
estimated as a high-value state by an agent but not included in the datasets of other agents, the other
agents cannot learn corresponding optimal actions to cooperate with that agent at that state, and
thus miscoordination may occur. In this case, each agent should conservatively estimate the values
of states that are absent from the datasets of other agents to avoid miscoordination. We leave the
observation limitation and the state absence to future work."
