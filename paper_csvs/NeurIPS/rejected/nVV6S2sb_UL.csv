Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017331022530329288,"Secure aggregation is a critical component in federated learning (FL), which
1"
ABSTRACT,0.0034662045060658577,"enables the server to learn the aggregate model of the users without observing
2"
ABSTRACT,0.005199306759098787,"their local models. Conventionally, secure aggregation algorithms focus only on
3"
ABSTRACT,0.006932409012131715,"ensuring the privacy of individual users in a single training round. We contend that
4"
ABSTRACT,0.008665511265164644,"such designs can lead to signiﬁcant privacy leakages over multiple training rounds,
5"
ABSTRACT,0.010398613518197574,"due to partial user selection/participation at each round of FL. In fact, we show that
6"
ABSTRACT,0.012131715771230503,"the conventional random user selection strategies in FL may lead to leaking users’
7"
ABSTRACT,0.01386481802426343,"individual models within a number of rounds that is linear in the number of users.
8"
ABSTRACT,0.01559792027729636,"To address this challenge, we introduce a secure aggregation framework, Multi-
9"
ABSTRACT,0.01733102253032929,"RoundSecAgg, with multi-round privacy guarantees. In particular, we introduce a
10"
ABSTRACT,0.019064124783362217,"new metric to quantify the privacy guarantees of FL over multiple training rounds,
11"
ABSTRACT,0.02079722703639515,"and develop a structured user selection strategy that guarantees the long-term
12"
ABSTRACT,0.022530329289428077,"privacy of each user (over any number of training rounds). Our framework also
13"
ABSTRACT,0.024263431542461005,"carefully accounts for the fairness and the average number of participating users at
14"
ABSTRACT,0.025996533795493933,"each round. Our experiments on MNIST, CIFAR-10 and CIFAR-100 datasets in
15"
ABSTRACT,0.02772963604852686,"the IID and the non-IID settings demonstrate the performance improvement over
16"
ABSTRACT,0.029462738301559793,"the baselines, both in terms of privacy protection and test accuracy.
17"
INTRODUCTION,0.03119584055459272,"1
Introduction
18"
INTRODUCTION,0.03292894280762565,"Figure 1: A qualitative comparison of the reconstructed
images in two settings is shown.
The ﬁrst setting
corresponds to the case that model privacy with random
user selection (e.g., FedAvg [25]) is protected by
conventional secure aggregation schemes as [4] at each
round.
In the second setting, our proposed method
ensures the long-term privacy of individual models
over any number of rounds, and hence model inversion
attack cannot work well. This reconstruction process is
described in detail in Appendix H."
INTRODUCTION,0.03466204506065858,"Federated learning (FL) enables collaborative
19"
INTRODUCTION,0.036395147313691506,"training of machine learning models over the
20"
INTRODUCTION,0.038128249566724434,"data collected and stored locally by multiple
21"
INTRODUCTION,0.03986135181975736,"data-owners. The training in FL is typically
22"
INTRODUCTION,0.0415944540727903,"coordinated by a central server who maintains a
23"
INTRODUCTION,0.043327556325823226,"global model that is updated locally by the users.
24"
INTRODUCTION,0.045060658578856154,"The local updates are then aggregated by the
25"
INTRODUCTION,0.04679376083188908,"server to update the global model. Throughout
26"
INTRODUCTION,0.04852686308492201,"the training process, the users never share their
27"
INTRODUCTION,0.05025996533795494,"data with the server, i.e., the data is always kept
28"
INTRODUCTION,0.05199306759098787,"on device, rather, they only share their local
29"
INTRODUCTION,0.053726169844020795,"updates. However, as has been shown recently,
30"
INTRODUCTION,0.05545927209705372,"the local models may still reveal substantial
31"
INTRODUCTION,0.05719237435008666,"information about the local datasets, and the
32"
INTRODUCTION,0.058925476603119586,"private training data can be reconstructed from
33"
INTRODUCTION,0.060658578856152515,"the local models through inference or inversion
34"
INTRODUCTION,0.06239168110918544,"attacks (see e.g., [11, 26, 42, 12]).
35"
INTRODUCTION,0.06412478336221837,"To prevent such information leakage, secure aggregation protocols are proposed (e.g., [4, 31, 15,
36"
INTRODUCTION,0.0658578856152513,"40, 2, 38, 30]) to protect the privacy of the local models, both from the server and the other users,
37"
INTRODUCTION,0.06759098786828423,"while still allowing the server to learn their aggregate. More speciﬁcally, the secure aggregation
38"
INTRODUCTION,0.06932409012131716,"protocols ensure that, at any given round, the server can only learn the aggregate model of the users,
39"
INTRODUCTION,0.07105719237435008,"and beyond that no further information is revealed about the individual model.
40"
INTRODUCTION,0.07279029462738301,"Secure aggregation protocols, however, only ensure the privacy of the individual users in a single
41"
INTRODUCTION,0.07452339688041594,"training round, and do not consider their privacy over multiple training rounds [4, 2, 31, 32]. On
42"
INTRODUCTION,0.07625649913344887,"the other hand, due to partial user selection [7, 5, 6, 28], the server may be able to reconstruct the
43"
INTRODUCTION,0.0779896013864818,"individual models of some users using the aggregated models from the previous rounds. In fact, we
44"
INTRODUCTION,0.07972270363951472,"show that after a sufﬁcient number of rounds, all local models can be recovered with a high accuracy
45"
INTRODUCTION,0.08145580589254767,"in Fig.1, performing model inversion attack [12] with the recovered local models yields reconstructed
47"
INTRODUCTION,0.0831889081455806,"images with a similar quality as the original images.
48"
INTRODUCTION,0.08492201039861352,"Contributions.
As such motivated, we study long-term user privacy in FL. Speciﬁcally, our
49"
INTRODUCTION,0.08665511265164645,"contributions are as follows.
50"
WE INTRODUCE A NEW METRIC TO CAPTURE LONG-TERM PRIVACY GUARANTEES FOR SECURE AGGREGATION PROTOCOLS,0.08838821490467938,"1. We introduce a new metric to capture long-term privacy guarantees for secure aggregation protocols
51"
WE INTRODUCE A NEW METRIC TO CAPTURE LONG-TERM PRIVACY GUARANTEES FOR SECURE AGGREGATION PROTOCOLS,0.09012131715771231,"in FL for the ﬁrst time. This long-term privacy requires that the server cannot reconstruct any
52"
WE INTRODUCE A NEW METRIC TO CAPTURE LONG-TERM PRIVACY GUARANTEES FOR SECURE AGGREGATION PROTOCOLS,0.09185441941074524,"individual model using the aggregated models from any number of training rounds. Using this
53"
WE INTRODUCE A NEW METRIC TO CAPTURE LONG-TERM PRIVACY GUARANTEES FOR SECURE AGGREGATION PROTOCOLS,0.09358752166377816,"metric, we show that the conventional random selection schemes can result in leaking the local
54"
WE INTRODUCE A NEW METRIC TO CAPTURE LONG-TERM PRIVACY GUARANTEES FOR SECURE AGGREGATION PROTOCOLS,0.09532062391681109,"models after a sufﬁcient number of rounds, even if secure aggregation is employed at each round.
55"
WE INTRODUCE A NEW METRIC TO CAPTURE LONG-TERM PRIVACY GUARANTEES FOR SECURE AGGREGATION PROTOCOLS,0.09705372616984402,"2. We propose Multi-RoundSecAgg, a privacy-preserving structured user selection strategy that
56"
WE INTRODUCE A NEW METRIC TO CAPTURE LONG-TERM PRIVACY GUARANTEES FOR SECURE AGGREGATION PROTOCOLS,0.09878682842287695,"ensures the long-term privacy of the individual users over any number of training rounds. This
57"
WE INTRODUCE A NEW METRIC TO CAPTURE LONG-TERM PRIVACY GUARANTEES FOR SECURE AGGREGATION PROTOCOLS,0.10051993067590988,"strategy also takes into account the fairness of the selection process and the average number of
58"
WE INTRODUCE A NEW METRIC TO CAPTURE LONG-TERM PRIVACY GUARANTEES FOR SECURE AGGREGATION PROTOCOLS,0.1022530329289428,"participating users at each round.
59"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.10398613518197573,"3. We demonstrate that Multi-RoundSecAgg creates a trade-off between the long-term privacy
60"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.10571923743500866,"guarantee and the average number of participating users. In particular, as the average number of
61"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.10745233968804159,"participating users increases, the long-term privacy guarantee becomes weaker.
62"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.10918544194107452,"4. We provide the convergence analysis of Multi-RoundSecAgg, which shows that the long-term
63"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.11091854419410745,"privacy guarantee and the average number of participating users control the convergence rate. The
64"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.11265164644714037,"convergence rate is maximized when the average number of participating users is maximized.
65"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.11438474870017332,"(e.g., the random user selection strategy maximizes the average number of participating users at
66"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.11611785095320624,"the expense of not providing long-term privacy guarantees). As we require stronger long-term
67"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.11785095320623917,"privacy guarantees, the average number of participating users decreases and a larger number of
68"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.1195840554592721,"training rounds is required to achieve the same level of accuracy as the random selection strategy.
69"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.12131715771230503,"5. Finally, our experiments in both IID and non-IID settings on MNIST, CIFAR-10 and CIFAR-100
70"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.12305025996533796,"datasets demonstrate that Multi-RoundSecAgg achieves almost the same test accuracy compared
71"
WE DEMONSTRATE THAT MULTI-ROUNDSECAGG CREATES A TRADE-OFF BETWEEN THE LONG-TERM PRIVACY,0.12478336221837089,"to the random selection scheme while providing better long-term privacy guarantees.
72"
RELATED WORK,0.1265164644714038,"2
Related Work
73"
RELATED WORK,0.12824956672443674,"The underlying principle of the secure aggregation protocol in [4] is that each pair of users exchange a
74"
RELATED WORK,0.12998266897746968,"pairwise secret key which they can use to mask their local models before sharing them with the server.
75"
RELATED WORK,0.1317157712305026,"The pairwise masks cancel out when the server aggregates the masked models, allowing the server to
76"
RELATED WORK,0.13344887348353554,"aggregate the local models. These masks also ensure that the local models are kept private, i.e., no
77"
RELATED WORK,0.13518197573656845,"further information is revealed beyond the aggregate of the local models. This protocol, however,
78"
RELATED WORK,0.1369150779896014,"incurs a signiﬁcant communication cost due to exchanging and reconstructing the pairwise keys.
79"
RELATED WORK,0.1386481802426343,"Recently, several works have developed computation and communication-efﬁcient protocols [31, 15,
80"
RELATED WORK,0.14038128249566725,"2, 35, 8, 10, 38], which are complementary to and can be combined with our work. Another line of
81"
RELATED WORK,0.14211438474870017,"work focused on designing partial user selection strategies to overcome the communication bottleneck
82"
RELATED WORK,0.1438474870017331,"in FL while speeding up the convergence by selecting the users based on their local loss [7, 5, 6, 28].
83"
RELATED WORK,0.14558058925476602,"Previous works, either on secure aggregation or on partial user selection, however, do not consider
84"
RELATED WORK,0.14731369150779897,"mitigating the potential privacy leakage as a result of partial user participation and the server observing
85"
RELATED WORK,0.14904679376083188,"the aggregated models across multiple training rounds. While [27] pointed out to the privacy leakage
86"
RELATED WORK,0.15077989601386482,"of secure aggregation, mitigating this leakage has not been considered and our work is the ﬁrst secure
87"
RELATED WORK,0.15251299826689774,"aggregation protocol to address this challenge.
88"
RELATED WORK,0.15424610051993068,"Differential privacy (DP), in which each user adds artiﬁcial noises to the local models, can be one of
89"
RELATED WORK,0.1559792027729636,"the potential solution to protect the privacy leakage over the multiple rounds [9, 1, 37, 3, 16]. In DP,
90"
RELATED WORK,0.15771230502599654,"however, the privacy guarantee sacriﬁces the model performance, which is known as a privacy-utility
91"
RELATED WORK,0.15944540727902945,"trade-off. It is worth noting that secure aggregation and DP are complementary, i.e., all the beneﬁts
92"
RELATED WORK,0.1611785095320624,"of DP can be applied to our approach by adding noise to the local models [3]. In this paper, our
93"
RELATED WORK,0.16291161178509533,"objective is to understand the secure aggregation itself.
94"
SYSTEM MODEL,0.16464471403812825,"3
System Model
95"
SYSTEM MODEL,0.1663778162911612,"In this section, we ﬁrst describe the basic federated learning model in Section 3.1. Next, we introduce
96"
SYSTEM MODEL,0.1681109185441941,"the multi-round secure aggregation problem for federated learning and deﬁne the key metrics to
97"
SYSTEM MODEL,0.16984402079722705,"evaluate the performance of a multi-round secure aggregation protocol in Section 3.2.
98"
BASIC FEDERATED LEARNING MODEL,0.17157712305025996,"3.1
Basic Federated Learning Model
99"
BASIC FEDERATED LEARNING MODEL,0.1733102253032929,"We consider a cross-device federated learning setup consisting of a server and 푁users. User 푖∈[푁]
100"
BASIC FEDERATED LEARNING MODEL,0.17504332755632582,"has a local dataset D푖consisting of 푚푖= |D푖| data samples. The users are connected to each other
101"
BASIC FEDERATED LEARNING MODEL,0.17677642980935876,"through the server, i.e., all communications between the users goes through the server [24, 4, 17].
102"
BASIC FEDERATED LEARNING MODEL,0.17850953206239167,"The goal is to collaboratively learn a global model x with dimension 푑, using the local datasets that
103"
BASIC FEDERATED LEARNING MODEL,0.18024263431542462,"are generated, stored, and processed locally by the users. The training task can be represented by
104"
BASIC FEDERATED LEARNING MODEL,0.18197573656845753,"minimizing a global loss function,
105"
BASIC FEDERATED LEARNING MODEL,0.18370883882149047,"min
x
퐿(x) s.t. 퐿(x) =
1
Í푁
푖=1 푤푖 푁
Õ"
BASIC FEDERATED LEARNING MODEL,0.1854419410745234,"푖=1
푤푖퐿푖(x),
(1)"
BASIC FEDERATED LEARNING MODEL,0.18717504332755633,"where 퐿푖is the loss function of user 푖and 푤푖≥0 is a weight parameter assigned to user 푖to specify
106"
BASIC FEDERATED LEARNING MODEL,0.18890814558058924,"the relative impact of that user. A common choice for the weight parameters is 푤푖= 푚푖[17]. We
107"
BASIC FEDERATED LEARNING MODEL,0.19064124783362218,"deﬁne the optimal model parameters x∗and x∗
푖as x∗= arg minx∈R푑퐿(x) and x∗
푖= arg minx∈R푑퐿푖(x).
108"
BASIC FEDERATED LEARNING MODEL,0.1923743500866551,"Federated Averaging with Partial User Participation. To solve (1), the most common algorithm
109"
BASIC FEDERATED LEARNING MODEL,0.19410745233968804,"is the FedAvg (federated averaging) algorithm [24]. FedAvg is an iterative algorithm, where the model
110"
BASIC FEDERATED LEARNING MODEL,0.19584055459272098,"training is done by repeatedly iterating over individual local updates. At the beginning of training
111"
BASIC FEDERATED LEARNING MODEL,0.1975736568457539,"round 푡, the server sends the current state of the global model, denoted by x(푡), to the users. Each
112"
BASIC FEDERATED LEARNING MODEL,0.19930675909878684,"round consists of two phases, local training and aggregation. In the local training phase, user 푖∈[푁]
113"
BASIC FEDERATED LEARNING MODEL,0.20103986135181975,"updates the global model by carrying out 퐸(≥1) local stochastic gradient descent (SGD) steps and
114"
BASIC FEDERATED LEARNING MODEL,0.2027729636048527,"sends the updated local model x(푡)
푖
to the server. One of key features of cross-device FL is partial
115"
BASIC FEDERATED LEARNING MODEL,0.2045060658578856,"device participation. Due to various reasons such as unreliable wireless connectivity, or battery issues,
116"
BASIC FEDERATED LEARNING MODEL,0.20623916811091855,"at any given round, only a fraction of the users are available to participate in the protocol. We refer
117"
BASIC FEDERATED LEARNING MODEL,0.20797227036395147,"to such users as available users throughout the paper. In the aggregation phase, the server selects
118"
BASIC FEDERATED LEARNING MODEL,0.2097053726169844,"퐾≤푁users among the available users if this is possible and aggregates their local updates. The
119"
BASIC FEDERATED LEARNING MODEL,0.21143847487001732,"server updates the global model as follows
120"
BASIC FEDERATED LEARNING MODEL,0.21317157712305027,"x(푡+1) =
Õ"
BASIC FEDERATED LEARNING MODEL,0.21490467937608318,"푖∈S(푡)
푤′
푖x(푡)
푖
= X(푡)⊤p(푡),
(2)"
BASIC FEDERATED LEARNING MODEL,0.21663778162911612,"where S(푡) is the set of participating users at round 푡, 푤′
푖=
푤푖
Í
푖∈S(푡) 푤푖, and p(푡) ∈{0, 1}푁is the
121"
BASIC FEDERATED LEARNING MODEL,0.21837088388214904,"corresponding characteristic vector. That is, p(푡) denotes a participation vector at round 푡whose 푖-th
122"
BASIC FEDERATED LEARNING MODEL,0.22010398613518198,"entry is 0 when user 푖is not selected and 1 otherwise. X(푡) denotes the concatenation of the weighted
123"
BASIC FEDERATED LEARNING MODEL,0.2218370883882149,"local models at round 푡, i.e., X(푡) =

푤′
1x(푡)
1 , . . . , 푤′
푁x(푡)
푁
⊤∈R푁×푑. Finally, the server broadcasts
124"
BASIC FEDERATED LEARNING MODEL,0.22357019064124783,"the updated global model x(푡+1) to the users for the next round.
125"
BASIC FEDERATED LEARNING MODEL,0.22530329289428075,"Threat Model. Similar to the prior works on secure aggregation as [4, 15, 31], we consider the
126"
BASIC FEDERATED LEARNING MODEL,0.2270363951473137,"honest-but-curious model. All participants follow the protocol honestly in this model, but try to learn
127"
BASIC FEDERATED LEARNING MODEL,0.22876949740034663,"as much as possible about the users. At each round, the privacy of individual model x(푡)
푖
in (2) is
128"
BASIC FEDERATED LEARNING MODEL,0.23050259965337955,"protected by secure aggregation such that the server only learns the aggregated model Í
푖∈S(푡) 푤′
푖x(푡)
푖.
129 130"
MULTI-ROUND SECURE AGGREGATION,0.2322357019064125,"3.2
Multi-round Secure Aggregation
131"
MULTI-ROUND SECURE AGGREGATION,0.2339688041594454,"Conventional secure aggregation protocols only consider the privacy guarantees over a single training
132"
MULTI-ROUND SECURE AGGREGATION,0.23570190641247835,"round. While secure aggregation protocols have provable privacy guarantees at any single round,
133"
MULTI-ROUND SECURE AGGREGATION,0.23743500866551126,"in the sense that no information is leaked beyond the aggregate model at each round, the privacy
134"
MULTI-ROUND SECURE AGGREGATION,0.2391681109185442,"guarantees do not extend to attacks that span multiple training rounds. Speciﬁcally, by using the
135"
MULTI-ROUND SECURE AGGREGATION,0.24090121317157712,"aggregate models and participation information across multiple rounds, an individual model may be
136"
MULTI-ROUND SECURE AGGREGATION,0.24263431542461006,"reconstructed. For instance, consider the following user participation strategy across three training
137"
MULTI-ROUND SECURE AGGREGATION,0.24436741767764297,"rounds, p(1) = [1, 1, 0]⊤, p(2) = [0, 1, 1]⊤, and p(3) = [1, 0, 1]⊤. Assume a scenario where the local
138"
MULTI-ROUND SECURE AGGREGATION,0.24610051993067592,"updates do not change signiﬁcantly over time (e.g., models start to converge, or the server ﬁxes the
139"
MULTI-ROUND SECURE AGGREGATION,0.24783362218370883,"global model over consecutive rounds), i.e., x푖= x(푡)
푖
for all 푖∈[3] and 푡∈[3]. Then, the server can
140"
MULTI-ROUND SECURE AGGREGATION,0.24956672443674177,"single out individual model, e.g., x1 = (x(1) + x(3) −x(2))/2. Similarly, the server can single out all
141"
MULTI-ROUND SECURE AGGREGATION,0.2512998266897747,"individual models x푖, even if a secure aggregation protocol is employed at each round.
142"
MULTI-ROUND SECURE AGGREGATION,0.2530329289428076,"In this paper, we study secure aggregation protocols with long-term privacy guarantees (which we
143"
MULTI-ROUND SECURE AGGREGATION,0.25476603119584057,"term multi-round secure aggregation) for the cross-device FL setup which has not been studied before.
144"
MULTI-ROUND SECURE AGGREGATION,0.2564991334488735,"We assume that user 푖∈[푁] drops from the protocol at each round with probability 푝푖. U (푡) denotes
145"
MULTI-ROUND SECURE AGGREGATION,0.2582322357019064,"the index set of available users at round 푡and u(푡) ∈{0, 1}푁is a vector indicating the available users
146"
MULTI-ROUND SECURE AGGREGATION,0.25996533795493937,"such that {u(푡)} 푗= 1{ 푗∈U (푡)}, where {u} 푗is 푗-th entry of u and 1{·} is the indicator function.
147"
MULTI-ROUND SECURE AGGREGATION,0.2616984402079723,"The server selects 퐾users from U (푡), if |U (푡)| ≥퐾, based on the history of selected users in previous
148"
MULTI-ROUND SECURE AGGREGATION,0.2634315424610052,"rounds. If |U (푡)| < 퐾, the server skips this round. The local models of the selected users are then
149"
MULTI-ROUND SECURE AGGREGATION,0.2651646447140381,"aggregated via a secure aggregation protocol (i.e., by communicating masked models), at the end of
150"
MULTI-ROUND SECURE AGGREGATION,0.2668977469670711,"which the server learns the aggregate of the local models of the selected users. Our goal is to design a
151"
MULTI-ROUND SECURE AGGREGATION,0.268630849220104,"user selection algorithm A (푡) : {0, 1}푡×푁× {0, 1}푁→{0, 1}푁,
152"
MULTI-ROUND SECURE AGGREGATION,0.2703639514731369,"A (푡)  P(푡), u(푡) = p(푡) such that ∥p(푡) ∥0 ∈{0, 퐾},
(3)"
MULTI-ROUND SECURE AGGREGATION,0.2720970537261698,"to prevent the potential information leakage over multiple rounds, where p(푡) ∈{0, 1}푁is the
153"
MULTI-ROUND SECURE AGGREGATION,0.2738301559792028,"participation vector deﬁned in (2), ∥x∥0 denotes the 퐿0-“norm” of a vector x and 퐾denotes the
154"
MULTI-ROUND SECURE AGGREGATION,0.2755632582322357,"number of selected users. We note that A (푡) can be a random function. P(푡) is a matrix representing
155"
MULTI-ROUND SECURE AGGREGATION,0.2772963604852686,"the user participation information up to round 푡, and is termed the participation matrix, given by
156"
MULTI-ROUND SECURE AGGREGATION,0.27902946273830154,"P(푡) =

p(0), p(1), . . . , p(푡−1)⊤∈{0, 1}푡×푁.
(4)"
MULTI-ROUND SECURE AGGREGATION,0.2807625649913345,"Key Metrics. A multi-round secure aggregation protocol can be represented by A = {A (푡)}푡∈[퐽],
157"
MULTI-ROUND SECURE AGGREGATION,0.2824956672443674,"where A (푡) is the user selection algorithm at round 푡deﬁned in (3) and 퐽is the total number of rounds.
158"
MULTI-ROUND SECURE AGGREGATION,0.28422876949740034,"The inputs of A (푡) are a random vector u(푡), which indicates the available users at round 푡, and the
159"
MULTI-ROUND SECURE AGGREGATION,0.28596187175043325,"participation matrix P(푡) deﬁned in (4) which can be a random matrix. Given the participation matrix
160"
MULTI-ROUND SECURE AGGREGATION,0.2876949740034662,"P(퐽), we evaluate the performance of the corresponding multi-round secure aggregation protocol
161"
MULTI-ROUND SECURE AGGREGATION,0.28942807625649913,"through the following metrics.
162"
MULTI-ROUND SECURE AGGREGATION,0.29116117850953205,"1. Multi-round Privacy Guarantee. The secure aggregation protocols ensure that the server can
163"
MULTI-ROUND SECURE AGGREGATION,0.292894280762565,"only learn the sum of the local models of some users in each single round, but they do not consider
164"
MULTI-ROUND SECURE AGGREGATION,0.29462738301559793,"what the server can learn over the long run. Our multi-round privacy deﬁnition extends the
165"
MULTI-ROUND SECURE AGGREGATION,0.29636048526863085,"guarantees of the secure aggregation protocols from one round to all rounds by requiring that the
166"
MULTI-ROUND SECURE AGGREGATION,0.29809358752166376,"server can only learn a sum of the local models even if the server exploits the aggregate models
167"
MULTI-ROUND SECURE AGGREGATION,0.29982668977469673,"of all rounds. That is, our multi-round privacy guarantee is a natural extension of the privacy
168"
MULTI-ROUND SECURE AGGREGATION,0.30155979202772965,"guarantee provided by the secure aggregation protocols considering a single training round.
169"
MULTI-ROUND SECURE AGGREGATION,0.30329289428076256,"Speciﬁcally, a multi-round privacy guarantee 푇requires that any non-zero partial sum of the
170"
MULTI-ROUND SECURE AGGREGATION,0.3050259965337955,"local models that the server can reconstruct, through any linear combination X⊤P(퐽)⊤z, where
171"
MULTI-ROUND SECURE AGGREGATION,0.30675909878682844,"z ∈R퐽\ {0}, must be of the form1
172"
MULTI-ROUND SECURE AGGREGATION,0.30849220103986136,"X⊤P(퐽)⊤z =
Õ"
MULTI-ROUND SECURE AGGREGATION,0.31022530329289427,"푖∈[푛]
푎푖
Õ"
MULTI-ROUND SECURE AGGREGATION,0.3119584055459272,"푗∈S푖
x푗= 푎1
Õ"
MULTI-ROUND SECURE AGGREGATION,0.31369150779896016,"푗∈S1
x푗+ 푎2
Õ"
MULTI-ROUND SECURE AGGREGATION,0.31542461005199307,"푗∈S2
x푗+ · · · + 푎푛
Õ"
MULTI-ROUND SECURE AGGREGATION,0.317157712305026,"푗∈S푛
x푗,
(5)"
MULTI-ROUND SECURE AGGREGATION,0.3188908145580589,"where |S푖| ≥푇, 푎푖≠0, ∀푖∈[푛] and 푛∈Z+. Here all the sets S푖, the number of sets 푛, and each
173"
MULTI-ROUND SECURE AGGREGATION,0.32062391681109187,"푎푖could all depend on z. In equation (5), we consider the worst-case scenario, where the local
174"
MULTI-ROUND SECURE AGGREGATION,0.3223570190641248,"models do not change over the rounds. That is, X(푡) = X, ∀푡∈[퐽]. Intuitively, this guarantee
175"
MULTI-ROUND SECURE AGGREGATION,0.3240901213171577,"ensures that the best that the server can do is to reconstruct a partial sum of 푇local models which
176"
MULTI-ROUND SECURE AGGREGATION,0.32582322357019067,"corresponds to the case where 푛= 1. When 푇≥2, this condition implies that the server cannot
177"
MULTI-ROUND SECURE AGGREGATION,0.3275563258232236,"get any user model from the aggregate models of all training rounds (the best it can obtain is the
178"
MULTI-ROUND SECURE AGGREGATION,0.3292894280762565,"sum of two local models).
179"
MULTI-ROUND SECURE AGGREGATION,0.3310225303292894,"Remark 1. (Weaker Privacy Notion). It is worth noting that, a weaker privacy notion would
180"
MULTI-ROUND SECURE AGGREGATION,0.3327556325823224,"require that ∥P(퐽)⊤z∥0 ≥푇when P(퐽)⊤z ≠0. When 푇= 2, this deﬁnition requires that the server
181"
MULTI-ROUND SECURE AGGREGATION,0.3344887348353553,"cannot reconstruct any individual model (the best it can do is to obtain a linear combination of
182"
MULTI-ROUND SECURE AGGREGATION,0.3362218370883882,"two local models). This notion, however, allows constructions in the form of 푎x푖+ 푏x 푗for any
183"
MULTI-ROUND SECURE AGGREGATION,0.3379549393414211,"푎, 푏∈R \ {0}. When 푎≫푏, however, this is almost the same as recovering x푖perfectly, hence
184"
MULTI-ROUND SECURE AGGREGATION,0.3396880415944541,"this privacy criterion is weaker than that of (5).
185"
MULTI-ROUND SECURE AGGREGATION,0.341421143847487,"Remark 2. (Multi-round Privacy of Random Selection). In Section 6, we empirically show that a
186"
MULTI-ROUND SECURE AGGREGATION,0.3431542461005199,"random selection strategy in which 퐾available users are selected uniformly at random at each
187"
MULTI-ROUND SECURE AGGREGATION,0.34488734835355284,"round does not ensure multi-round privacy even with respect to the weaker deﬁnition of Remark
188"
MULTI-ROUND SECURE AGGREGATION,0.3466204506065858,"1. Speciﬁcally, the local models can be reconstructed within a number of rounds that is linear in
189"
MULTI-ROUND SECURE AGGREGATION,0.3483535528596187,"푁. We also show theoretically in Appendix H that when min(푁−퐾, 퐾) ≥푐푁, where 푐> 0 is a
190"
MULTI-ROUND SECURE AGGREGATION,0.35008665511265163,"constant, then the probability that the server can reconstruct all local models after 푁rounds is
191"
MULTI-ROUND SECURE AGGREGATION,0.35181975736568455,1We assume that 푤푖= 1
MULTI-ROUND SECURE AGGREGATION,0.3535528596187175,"푁, ∀푖∈[푁] in this paper."
MULTI-ROUND SECURE AGGREGATION,0.35528596187175043,"at least 1 −2푒−푐′푁for a constant 푐′ that depends on 푐. Finally, we show that a random selection
192"
MULTI-ROUND SECURE AGGREGATION,0.35701906412478335,"scheme in which the users are selected in an i.i.d fashion according to Bern(
퐾
푁(1−푝) ) reveals all
193"
MULTI-ROUND SECURE AGGREGATION,0.3587521663778163,"local models after 푁rounds with probability that converges to 1 exponentially fast.
194"
MULTI-ROUND SECURE AGGREGATION,0.36048526863084923,"Remark 3. (Worst-Case Assumption). In (5), we considered the worst-case assumption where
195"
MULTI-ROUND SECURE AGGREGATION,0.36221837088388215,"the models do not change over time. When the local models change over rounds, the multi-round
196"
MULTI-ROUND SECURE AGGREGATION,0.36395147313691506,"privacy guarantee becomes even stronger as the number of unknowns increases. In Fig. 1 and
197"
MULTI-ROUND SECURE AGGREGATION,0.36568457538994803,"Appendix H, we empirically show that the conventional secure aggregation schemes leak extensive
198"
MULTI-ROUND SECURE AGGREGATION,0.36741767764298094,"information of training data even in the realistic settings where the models change over the rounds.
199"
MULTI-ROUND SECURE AGGREGATION,0.36915077989601386,"2. Aggregation Fairness Gap. The average aggregation fairness gap quantiﬁes the largest gap
200"
MULTI-ROUND SECURE AGGREGATION,0.3708838821490468,"between any two users in terms of the expected relative number of rounds each user has participated
201"
MULTI-ROUND SECURE AGGREGATION,0.37261698440207974,"in training. Formally, the average aggregation fairness gap is deﬁned as follows
202"
MULTI-ROUND SECURE AGGREGATION,0.37435008665511266,"퐹= max
푖∈[푁] lim sup
퐽→∞"
MULTI-ROUND SECURE AGGREGATION,0.37608318890814557,"1
퐽E
h 퐽−1
Õ"
MULTI-ROUND SECURE AGGREGATION,0.3778162911611785,"푡=0
1

{p(푡)}푖= 1
	i
−min
푖∈[푁] lim inf
퐽→∞
1
퐽E
h 퐽−1
Õ"
MULTI-ROUND SECURE AGGREGATION,0.37954939341421146,"푡=0
1

{p(푡)}푖= 1
	i
,
(6)"
MULTI-ROUND SECURE AGGREGATION,0.38128249566724437,"where {p(푡)}푖is 푖-th entry of the vector p(푡) and the expectation is over the randomness of the user
203"
MULTI-ROUND SECURE AGGREGATION,0.3830155979202773,"selection algorithm A and the user availability. The main intuition behind this deﬁnition is that
204"
MULTI-ROUND SECURE AGGREGATION,0.3847487001733102,"when 퐹= 0, all users participate on average on the same number of rounds. This is important to
205"
MULTI-ROUND SECURE AGGREGATION,0.38648180242634317,"take the different users into consideration equally and our experiments show that the accuracy of
206"
MULTI-ROUND SECURE AGGREGATION,0.3882149046793761,"the schemes with small 퐹are much higher than the schemes with high 퐹.
207"
MULTI-ROUND SECURE AGGREGATION,0.389948006932409,"3. Average Aggregation Cardinality. The aggregation cardinality quantiﬁes the expected number
208"
MULTI-ROUND SECURE AGGREGATION,0.39168110918544197,"of models to be aggregated per round. Formally, it is deﬁned as
209"
MULTI-ROUND SECURE AGGREGATION,0.3934142114384749,"퐶= lim inf
퐽→∞
E
 Í퐽−1
푡=0 ∥p(푡) ∥0
"
MULTI-ROUND SECURE AGGREGATION,0.3951473136915078,"퐽
,
(7)"
MULTI-ROUND SECURE AGGREGATION,0.3968804159445407,"where the expectation is over the randomness in A and the user availability. Intuitively, less
210"
MULTI-ROUND SECURE AGGREGATION,0.3986135181975737,"number of rounds are needed to converge as more users participate in the training. In fact, as we
211"
MULTI-ROUND SECURE AGGREGATION,0.4003466204506066,"show in Section 5.2, 퐶directly controls the convergence rate.
212"
BASELINE SCHEMES,0.4020797227036395,"3.3
Baseline Schemes
213"
BASELINE SCHEMES,0.4038128249566724,"In this subsection, we introduce three baseline schemes for multi-round secure aggregation.
214"
BASELINE SCHEMES,0.4055459272097054,"Random Selection. In this scheme, at each round, the server selects 퐾users at random from the set
215"
BASELINE SCHEMES,0.4072790294627383,"of available users if this is possible.
216"
BASELINE SCHEMES,0.4090121317157712,"Random Weighted Selection. This scheme is a modiﬁed version of random selection to reduce 퐹
217"
BASELINE SCHEMES,0.41074523396880414,"when the dropout probabilities of the users are not equal. Speciﬁcally, 퐾users are selected at random
218"
BASELINE SCHEMES,0.4124783362218371,"from the available users with the minimum frequency of participation in the previous rounds.
219"
BASELINE SCHEMES,0.41421143847487,"User Partitioning (Grouping). In this scheme, the users are partitioned into 퐺= 푁/퐾equal-sized
220"
BASELINE SCHEMES,0.41594454072790293,"groups denoted as G1, G2, · · · , G퐺. At each round, the server selects one of the groups if none of
221"
BASELINE SCHEMES,0.41767764298093585,"the users in this group has dropped out. If multiple groups are available, to reduce the aggregation
222"
BASELINE SCHEMES,0.4194107452339688,"fairness gap, the server selects a group including a user with the minimum frequency of participation
223"
BASELINE SCHEMES,0.42114384748700173,"in previous rounds. If no group is available, the server skips this round.
224"
BASELINE SCHEMES,0.42287694974003465,"4
Proposed Scheme: Multi-RoundSecAgg
225"
BASELINE SCHEMES,0.4246100519930676,"In this section, we present Multi-RoundSecAgg, which has two components as follows.
226"
BASELINE SCHEMES,0.42634315424610053,"• The ﬁrst component designs a family of sets of users that satisfy the multi-round privacy requirement.
227"
BASELINE SCHEMES,0.42807625649913345,"The inputs of the ﬁrst component are the number of users (푁), the number of selected users at each
228"
BASELINE SCHEMES,0.42980935875216636,"round (퐾), and the desired multi-round privacy guarantee (푇). The output is a family of sets of 퐾
229"
BASELINE SCHEMES,0.43154246100519933,"users satisfying the multi-round privacy guarantee 푇, termed as a privacy-preserving family. This
230"
BASELINE SCHEMES,0.43327556325823224,"family is represented by a matrix B, where the rows are the characteristic vectors of these user sets.
231"
BASELINE SCHEMES,0.43500866551126516,"• The second component selects a set from this designed family to satisfy the fairness guarantee. The
232"
BASELINE SCHEMES,0.43674176776429807,"inputs to the second component are the family B, the set of available users at round 푡, U (푡), and the
233"
BASELINE SCHEMES,0.43847487001733104,"frequency of participation of each user. The output is the set of users that will participate at round 푡.
234"
BASELINE SCHEMES,0.44020797227036396,"We now describe these two components in detail.
235"
BASELINE SCHEMES,0.44194107452339687,"Component 1 (Batch Partitioning (BP) of the users to guarantee multi-round privacy). The
236"
BASELINE SCHEMES,0.4436741767764298,"ﬁrst component designs a family of 푅BP sets, where 푅BP is the size of the set, satisfying the multi-
237"
BASELINE SCHEMES,0.44540727902946275,"round privacy requirement 푇. We denote the 푅BP × 푁binary matrix corresponding to these sets by
238"
BASELINE SCHEMES,0.44714038128249567,"B = [b1, · · · , b푅BP]⊤, where ∥b푖∥0 = 퐾, ∀푖∈[푅BP]. That is, the rows of B are the characteristic
239"
BASELINE SCHEMES,0.4488734835355286,"vectors of those sets. The main idea of our scheme is to restrict certain sets of users of size 푇, denoted
240"
BASELINE SCHEMES,0.4506065857885615,"as batches, to either participate together or not participate at all. This guarantees a multi-round privacy
241"
BASELINE SCHEMES,0.45233968804159447,"푇as we show in Section 5.
242"
BASELINE SCHEMES,0.4540727902946274,"To construct a family of sets with this property, the users are ﬁrst partitioned into 푁/푇
243"
BASELINE SCHEMES,0.4558058925476603,"batches. At any given round, either all or none of the users of a particular batch participate
244"
BASELINE SCHEMES,0.45753899480069327,"in training.
The server can choose 퐾/푇batches to participate in training, provided that all
245"
BASELINE SCHEMES,0.4592720970537262,"users in any given selected batch are available.
Since there are  푁/푇
퐾/푇
 possible sets with
246"
BASELINE SCHEMES,0.4610051993067591,"this property, then the size of this privacy-preserving family of sets is given by 푅BP
def=  푁/푇
퐾/푇
2.
247"
BASELINE SCHEMES,0.462738301559792,"Figure 2: Example of our construction
with 푁= 8, 퐾= 4 and 푇= 2."
BASELINE SCHEMES,0.464471403812825,"In the extreme case of 푇= 1, this strategy specializes to random
248"
BASELINE SCHEMES,0.4662045060658579,"selection where the server can choose any 퐾possible users. In
249"
BASELINE SCHEMES,0.4679376083188908,"the other extreme case of 푇= 퐾, this strategy specializes to
250"
BASELINE SCHEMES,0.4696707105719237,"the partitioning strategy where there are 푁/퐾possible sets. We
251"
BASELINE SCHEMES,0.4714038128249567,"next provide an example to illustrate the construction of B.
252"
BASELINE SCHEMES,0.4731369150779896,"Example 1 (푁= 8, 퐾= 4,푇= 2). In this example, the users
253"
BASELINE SCHEMES,0.4748700173310225,"are partitioned into 4 batches as G1 = {1, 2}, G2 = {3, 4}, G3 =
254"
BASELINE SCHEMES,0.47660311958405543,"{5, 6} and G4 = {7, 8} as given in Fig. 2. The server can choose any two batches out of these 4
255"
BASELINE SCHEMES,0.4783362218370884,"batches, hence we have 푅BP =  4
2
 = 6 possible sets. This ensures a multi-round privacy 푇= 2.
256"
BASELINE SCHEMES,0.4800693240901213,"Component 2 (Available batch selection to guarantee fairness). At round 푡, user 푖∈[푁] is
257"
BASELINE SCHEMES,0.48180242634315423,"available to participate in the protocol with a probability 1−푝푖∈(0, 1]. The frequency of participation
258"
BASELINE SCHEMES,0.48353552859618715,"of user 푖before round 푡is denoted by 푓(푡)
푖
def= Í푡−1
푗=0 1

{p( 푗)}푖= 1
	
. Given the set of available users at
259"
BASELINE SCHEMES,0.4852686308492201,"round 푡, U (푡), and the frequencies of participation f (푡−1) = ( 푓(푡−1)
1
, · · · , 푓(푡−1)
푁
), the server selects
260"
BASELINE SCHEMES,0.48700173310225303,"퐾users. To do so, the server ﬁrst ﬁnds the submatrix of B denoted by B(푡) corresponding to U (푡).
261"
BASELINE SCHEMES,0.48873483535528595,"Speciﬁcally, the 푖-th row of B denoted by b⊤
푖is included in B(푡) provided that supp(bi) ⊆U (푡). If
262"
BASELINE SCHEMES,0.4904679376083189,"B(푡) is an empty matrix, then the server skips this round. Otherwise, the server selects a row from B(푡)
263"
BASELINE SCHEMES,0.49220103986135183,"uniformly at random if 푝푖= 푝, ∀푖∈[푁]. If the users have different 푝푖, the server selects a row from
264"
BASELINE SCHEMES,0.49393414211438474,"B(푡) that includes the user with the minimum frequency of participation ℓ(푡−1)
min
def= arg min푖∈U (푡) 푓(푡−1)
푖
.
265"
BASELINE SCHEMES,0.49566724436741766,"If there are many such rows, then the server selects one of them uniformly at random.
266"
BASELINE SCHEMES,0.49740034662045063,"Remark 4. (Necessity of the Second Component). The second component is necessary to guarantee
267"
BASELINE SCHEMES,0.49913344887348354,"that the aggregation fairness gap goes to zero as we show in Theorem 1 and Section 6.
268"
BASELINE SCHEMES,0.5008665511265165,"Overall, the algorithm ﬁrst designs a privacy-preserving family of sets to ensure the multi-round
269"
BASELINE SCHEMES,0.5025996533795494,"privacy guarantee 푇. Then speciﬁc sets are selected from this family to ensure fairness. We describe
270"
BASELINE SCHEMES,0.5043327556325823,"the two components of Multi-RoundSecAgg in detail in Algorithm 1 and Algorithm 2 in Appendix D.
271"
THEORETICAL RESULTS,0.5060658578856152,"5
Theoretical Results
272"
THEORETICAL RESULTS,0.5077989601386482,"In this section, we provide the theoretical guarantees of Multi-RoundSecAgg in Section 5.1 and the
273"
THEORETICAL RESULTS,0.5095320623916811,"convergence analysis of Multi-RoundSecAgg in Section 5.2.
274"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.511265164644714,"5.1
Theoretical Guarantees of Multi-RoundSecAgg
275"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.512998266897747,"In this subsection, we establish the theoretical guarantees of Multi-RoundSecAgg in terms of the
276"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5147313691507799,"multi-round privacy guarantee, the aggregation fairness gap and the average aggregation cardinality.
277"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5164644714038128,"Theorem 1. Multi-RoundSecAgg with parameters 푁, 퐾,푇ensures a multi-round privacy guarantee
278"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5181975736568457,"of 푇, an aggregation fairness gap 퐹= 0, and an average aggregation cardinality given by
279"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5199306759098787,"퐶= 퐾©­
«
1 − 푁/푇
Õ"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5216637781629117,푖=푁/푇−퐾/푇+1
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5233968804159446,"푁/푇
푖"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5251299826689775,"
푞푖(1 −푞)푁/푇−푖ª®
¬
,"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5268630849220104,2We assume for simplicity that 푁/푇and 퐾/푇are integers.
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5285961871750433,"where 푞= 1 −(1 −푝)푇, when all users have the dropout probability 푝.
280"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5303292894280762,"We provide the proof of Theorem 1 in Appendix A.
281"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5320623916811091,"Remark 5. (Trade-off between “Multi-round Privacy Guarantee” and “Average Aggregation
282"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5337954939341422,"Cardinality”). Theorem 1 indicates a trade-off between the multi-round privacy and the average
283"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5355285961871751,"aggregation cardinality since as 푇increases, 퐶decreases which slows down the convergence as we
284"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.537261698440208,"show in Sec. 5.2. We show this trade-off in Fig. 3.
285"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5389948006932409,"Remark 6. (Necessity of Batch Partitioning (BP)). We show that any strategy that satisﬁes the privacy
286"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5407279029462738,"guarantee in Equation (5) must have a batch partitioning structure, and for given 푁, 퐾,푇, 퐾≤푁/2,
287"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5424610051993067,"the largest number of distinct user sets in any strategy is at most  푁/푇
퐾/푇
, which is achieved in our
288"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5441941074523396,"design in Section 4. We provide the proof in Appendix C.
289"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5459272097053726,"Remark 7. (Non-linear Reconstructions of Aggregated Models). The privacy criterion in Eq. (5)
290"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5476603119584056,"considers linear reconstructions of the aggregated models. One may also consider more general
291"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5493934142114385,"non-linear reconstructions. The long-term privacy guarantees of batch partitioning hold even under
292"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5511265164644714,"such reconstructions as the users in the same batch always participate together or do not participate
293"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5528596187175043,"at all. Hence, the server cannot separate individual models within the same batch even through
294"
THEORETICAL GUARANTEES OF MULTI-ROUNDSECAGG,0.5545927209705372,"non-linear operations.
295"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5563258232235702,"5.2
Convergence Analysis of Multi-RoundSecAgg
296"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5580589254766031,"0
2
4
6
8
10
12
0 2 4 6 8 10 12"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5597920277296361,"Figure 3: An illustration of the trade-off between
the multi-round privacy guarantee 푇and the
average aggregation cardinality 퐶. In this example,
푁= 120 and 퐾= 12."
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.561525129982669,"For convergence analysis of Multi-RoundSecAgg, we
297"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5632582322357019,"ﬁrst introduce a few common assumptions [23, 39].
298"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5649913344887348,"Assumption 1. 퐿1, . . . , 퐿푁in (1) are all 휌-smooth:
299"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5667244367417678,"for all a, b ∈R푑and 푖∈[푁], 퐿푖(a) ≤퐿푖(b) + (a −
300"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5684575389948007,b)⊤∇퐿푖(b) + 휌
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5701906412478336,"2 ∥a −b∥2.
301"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5719237435008665,"Assumption 2. 퐿1, . . . , 퐿푁in (1) are all 휇-strongly
302"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5736568457538995,"convex: for all a, b ∈R푑and 푖∈[푁], 퐿푖(a) ≥
303"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5753899480069324,퐿푖(b) + (a −b)⊤∇퐿푖(b) + 휇
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5771230502599654,"2 ∥a −b∥2.
304"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5788561525129983,"Assumption 3. Let 휉(푡)
푖
be a sample uniformly selected from the dataset D푖. The variance of the
305"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5805892547660312,"stochastic gradients at each user is bounded, i.e., E∥∇퐿푖(x(푡)
푖, 휉(푡)
푖
) −∇퐿푖(x(푡)
푖)∥2 ≤휎2
푖for 푖∈[푁].
306"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5823223570190641,"Assumption 4. The expected squared norm of the stochastic gradients is uniformly bounded, i.e.,
307"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.584055459272097,"E∥∇퐿푖(x(푡)
푖, 휉(푡)
푖
)∥2 ≤퐺2 for all 푖∈[푁].
308"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.58578856152513,"We now state the convergence guarantees of Multi-RoundSecAgg.
309"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.587521663778163,"Theorem 2. Consider a FL setup with 푁users to train a machine learning model from (1). Assume
310"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5892547660311959,"퐾users are selected by Multi-RoundSecAgg with average aggregation cardinality 퐶deﬁned in (7) to
311"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5909878682842288,"update the global model from (2), and all users have the same dropout rate, hence Multi-RoundSecAgg
312"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5927209705372617,"selects a random set of 퐾users uniformly from the set of available user sets at each round. Then, the
313"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5944540727902946,"following is satisﬁed
314"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5961871750433275,"E[퐿(x(퐽))] −퐿∗≤
휌 훾+ 퐶 퐾퐸퐽−1"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5979202772963604,2(훼+ 훽)
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.5996533795493935,"휇2
+ 훾"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6013864818024264,"2 E∥x(0) −x∗∥2

,
(8)"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6031195840554593,where 훼= 1
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6048526863084922,"푁
Í푁
푖=1 휎2
푖+6휌Γ+8(퐸−1)2퐺2, 훽= 4(푁−퐾)퐸2퐺2"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6065857885615251,"퐾(푁−1)
, Γ = 퐿∗−Í푁
푖=1 퐿∗
푖, and 훾= max
n
8휌"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.608318890814558,"휇, 퐸
o
.
315"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.610051993067591,"We provide the proof of Theorem 2 in Appendix B.
316"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6117850953206239,"Remark 8. (The average aggregation cardinality controls the convergence rate.) Theorem 2 shows
317"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6135181975736569,"how the average aggregation cardinality affects the convergence. When the average aggregation
318"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6152512998266898,"cardinality is maximized, i.e., 퐶= 퐾, the convergence rate in Theorem 2 equals that of the random
319"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6169844020797227,"selection algorithm provided in Theorem 3 of [23]. In (8), we have the additional term 퐸(number of
320"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6187175043327556,"local epochs) in front of 퐽compared to Theorem 3 of [23] as we use global round index 푡instead of
321"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6204506065857885,"using step index of local SGD. As the average aggregation cardinality decreases, a greater number of
322"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6221837088388215,"training rounds is required to achieve the same level of accuracy.
323"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6239168110918544,"Remark 9. (General Convex and Non-Convex Convergence Rates). Theorem 2 considers the
324"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6256499133448874,"strongly-convex case, but we consider the general convex and the non-convex cases in Appendix I.
325"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6273830155979203,"Remark 10. (Different Dropout Rates). When the dropout probabilities of the users are not the same,
326"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6291161178509532,"characterizing the convergence guarantees of Multi-RoundSecAgg is challenging. This is due to the
327"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6308492201039861,"fact that batch selection based on the frequency of participation breaks the conditional unbiasedness
328"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.6325823223570191,"of the user selection, which is required for the convergence guarantee. In experiments, however, we
329"
CONVERGENCE ANALYSIS OF MULTI-ROUNDSECAGG,0.634315424610052,"empirically show that Multi-RoundSecAgg guarantees the convergence with different dropout rates.
330"
EXPERIMENTS,0.6360485268630849,"6
Experiments
331"
EXPERIMENTS,0.6377816291161178,"Our experiments consist of two parts. We ﬁrst numerically demonstrate the performance of Multi-
332"
EXPERIMENTS,0.6395147313691508,"RoundSecAgg compared to the baselines of Section 3.3 in terms of the key metrics of Section 3.2.
333"
EXPERIMENTS,0.6412478336221837,"Next, we implement convolutional neural networks (CNNs) for image classiﬁcation with MNIST [21],
334"
EXPERIMENTS,0.6429809358752167,"CIFAR-10, and CIFAR-100 [20] to investigate how the key metrics affect the test accuracy.
335"
EXPERIMENTS,0.6447140381282496,"Setup. We consider a FL setting with 푁= 120 users, where the server aims to choose 퐾= 12 users
336"
EXPERIMENTS,0.6464471403812825,"at every round. We study two settings for partitioning the CIFAR-100 dataset across the users.
337"
EXPERIMENTS,0.6481802426343154,"• IID Setting. 50000 training samples are shufﬂed and partitioned uniformly across 푁= 120 users.
338"
EXPERIMENTS,0.6499133448873483,"• Non-IID Setting. We distribute the dataset using a Dirichlet distribution [13], which samples
339"
EXPERIMENTS,0.6516464471403813,"d푐∼Dir(훽= 0.5) which specifying the prior class distribution over 100 classes, and allocate a
340"
EXPERIMENTS,0.6533795493934142,"portion 푑푐,푖of the class 푐to user 푖. The parameter 훽controls the heterogeneity of the distributions
341"
EXPERIMENTS,0.6551126516464472,"at each user, where 훽→∞results in IID setting.
342"
EXPERIMENTS,0.6568457538994801,"We implement a VGG-11 [29], which is sufﬁcient for our needs, as our goal is to evaluate various
343"
EXPERIMENTS,0.658578856152513,"schemes, not to achieve the best accuracy. The hyperparameters are provided in Appendix F.
344"
EXPERIMENTS,0.6603119584055459,"Modeling dropouts. To model heterogeneous system, users have different dropout probability 푝푖
345"
EXPERIMENTS,0.6620450606585788,"selected from {0.1, 0.2, 0.3, 0.4, 0.5}. At each round, user 푖∈[푁] drops with probability 푝푖.
346"
EXPERIMENTS,0.6637781629116117,"Scheme
Family size (= 푅)"
EXPERIMENTS,0.6655112651646448,"Random selection
∼1016"
EXPERIMENTS,0.6672443674176777,"Weighted random selection
∼1016
User partition
10
Multi-RoundSecAgg, T=6
190
Multi-RoundSecAgg, T=4
4060
Multi-RoundSecAgg, T=3
91389"
EXPERIMENTS,0.6689774696707106,"Table 1: Family size with 푁= 120, 퐾= 12."
EXPERIMENTS,0.6707105719237435,"Implemented Schemes.
For the benchmarks, we
347"
EXPERIMENTS,0.6724436741767764,"implement the three baselines introduced in Sec. 3.3,
348"
EXPERIMENTS,0.6741767764298093,"referred to as Random, Weighted Random, and Partition.
349"
EXPERIMENTS,0.6759098786828422,"For Multi-RoundSecAgg, we construct three privacy-
350"
EXPERIMENTS,0.6776429809358753,"preserving families with different target multi-round
351"
EXPERIMENTS,0.6793760831889082,"privacy guarantees, 푇= 6, 푇= 4, and 푇= 3 which
352"
EXPERIMENTS,0.6811091854419411,"we refer to as Multi-RoundSecAgg (푇= 6), Multi-
353"
EXPERIMENTS,0.682842287694974,"RoundSecAgg (푇= 4), and Multi-RoundSecAgg (푇=
354"
EXPERIMENTS,0.6845753899480069,"3), respectively. One can view the Random and Partition as extreme cases of Multi-RoundSecAgg
355"
EXPERIMENTS,0.6863084922010398,"with 푇= 1 and 푇= 퐾, respectively. Table 1 summarizes the family size 푅deﬁned in Section 4.
356"
EXPERIMENTS,0.6880415944540728,"Key Metrics. To numerically demonstrate the performance of the six schemes in terms of the key
357"
EXPERIMENTS,0.6897746967071057,"metrics deﬁned in Sec. 3.2, at each round, we measure the following metrics.
358"
EXPERIMENTS,0.6915077989601387,"• For the multi-round privacy guarantee, we measure the number of models in the partial sum that
359"
EXPERIMENTS,0.6932409012131716,"the server can reconstruct, which is given by 푇(푡) B minz∈R퐽} ∥z⊤P(푡) ∥0, s.t. P(푡)⊤z ≠0. This
360"
EXPERIMENTS,0.6949740034662045,"corresponds to the weaker privacy deﬁnition of Remark 1. We use this weaker privacy deﬁnition
361"
EXPERIMENTS,0.6967071057192374,"as the random selection and the random weighted selection strategies provide the worst privacy
362"
EXPERIMENTS,0.6984402079722704,"guarantee even with this weaker deﬁnition, as demonstrated later. On the other hand, Multi-
363"
EXPERIMENTS,0.7001733102253033,"RoundSecAgg provides better privacy guarantees with both the strong and the weaker deﬁnitions.
364"
EXPERIMENTS,0.7019064124783362,"• For the aggregation fairness gap, we measure the instantaneous fairness gap, 퐹(푡)
B
365"
EXPERIMENTS,0.7036395147313691,"max푖∈[푁] 퐹(푡)
푖
−min푖∈[푁] 퐹(푡)
푖
where 퐹(푡)
푖
=
1
푡+1
Í푡
푙=0 1

{p(푙)}푖= 1
	
.
366"
EXPERIMENTS,0.7053726169844021,"• We measure the instantaneous aggregation cardinality as 퐶(푡) B
1
푡+1
Í푡
푙=0 ∥p(푙) ∥0.
367"
EXPERIMENTS,0.707105719237435,"We demonstrate these key metrics in Figure 4. We make the following key observations.
368"
EXPERIMENTS,0.708838821490468,"• Multi-RoundSecAgg achieves better multi-round privacy guarantee than both the random selection
369"
EXPERIMENTS,0.7105719237435009,"and random weighted selection strategies, while user partitioning achieves the best multi-round
370"
EXPERIMENTS,0.7123050259965338,"privacy guarantee, 푇= 퐾= 12. However, the partitioning strategy has the worst aggregation
371"
EXPERIMENTS,0.7140381282495667,"cardinality, which results in the lowest convergence rate as demonstrated later.
372"
EXPERIMENTS,0.7157712305025996,"• Figure 5 demonstrates the trade-off between the multi-round privacy guarantee 푇and the average
373"
EXPERIMENTS,0.7175043327556326,"aggregation cardinality 퐶. Interestingly, Multi-RoundSecAgg when 푇= 3 or 푇= 4 achieves better
374"
EXPERIMENTS,0.7192374350086655,"multi-round privacy guarantee than both the random selection and the weighted random selection
375"
EXPERIMENTS,0.7209705372616985,"strategies while achieving almost the same average aggregation cardinality.
376"
EXPERIMENTS,0.7227036395147314,"0
50
100
150
200
# Rounds 0 2 4 6 8 10 12"
EXPERIMENTS,0.7244367417677643,The number of Partial Sum
EXPERIMENTS,0.7261698440207972,(a) Multi-round privacy guarantee.
EXPERIMENTS,0.7279029462738301,"0
50
100
150
200
# Rounds 0.0 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.729636048526863,Aggregation Fairness Gap
EXPERIMENTS,0.7313691507798961,"Random
Weighted Random
Partition
Multi-RoundSecAgg, T=6
Multi-RoundSecAgg, T=4
Multi-RoundSecAgg, T=3"
EXPERIMENTS,0.733102253032929,(b) Aggregation fairness gap.
EXPERIMENTS,0.7348353552859619,"0
50
100
150
200
# Rounds 0 2 4 6 8 10 12"
EXPERIMENTS,0.7365684575389948,Average Aggregation Cardinality
EXPERIMENTS,0.7383015597920277,(c) Average aggregation cardinality.
EXPERIMENTS,0.7400346620450606,"Figure 4: The key metrics with 푁= 120 (number of users), 퐾= 12 (number of selected users at each round)."
EXPERIMENTS,0.7417677642980935,"0
2
4
6
8
10
12
Average Aggregation Cardinality 0 2 4 6 8 10 12"
EXPERIMENTS,0.7435008665511266,Multi-Round Privacy Guarantee
EXPERIMENTS,0.7452339688041595,"Random
Weighted Random
Partition
Multi-RoundSecAgg, T=6
Multi-RoundSecAgg, T=4
Multi-RoundSecAgg, T=3"
EXPERIMENTS,0.7469670710571924,"Figure 5:
Trade-off between
multi-round privacy and average
aggregation cardinality with 푁=
120, 퐾= 12."
EXPERIMENTS,0.7487001733102253,"(a) IID data distribution.
(b) Non-IID data distribution."
EXPERIMENTS,0.7504332755632582,"Figure 6: Training rounds versus test accuracy of VGG11 in [29] on
the CIFAR-100 with 푁= 120 and 퐾= 12."
EXPERIMENTS,0.7521663778162911,"Remark 11. (Multi-round Privacy of Random and Weighted Random). The multi-round privacy
377"
EXPERIMENTS,0.7538994800693241,"guarantees of Random and Weighted Random drop sharply as shown in Fig. 4(a) as the participating
378"
EXPERIMENTS,0.755632582322357,"matrix P(푡) ∈{0, 1}푡×푁becomes full rank with high probability when 푡≥푁, and hence the server can
379"
EXPERIMENTS,0.75736568457539,"reconstruct the individual models by utilizing a pseudo inversion of the matrix P(푡). More precisely,
380"
EXPERIMENTS,0.7590987868284229,"Theorem 3 in Appendix H shows this thresholding phenomenon, where the probability that the server
381"
EXPERIMENTS,0.7608318890814558,"can reconstruct individual models after certain number of rounds converges to 1 exponentially fast.
382"
EXPERIMENTS,0.7625649913344887,"Key Metrics versus Test Accuracy. To investigate how the key metrics affect the test accuracy, we
383"
EXPERIMENTS,0.7642980935875217,"measure the test accuracy of the six schemes in the two settings, the IID and the non-IID settings.
384"
EXPERIMENTS,0.7660311958405546,"Our results are demonstrated in Figure 6. We make the following key observations.
385"
EXPERIMENTS,0.7677642980935875,"• In the IID setting, the Multi-RoundSecAgg schemes show test accuracies that are comparable to the
386"
EXPERIMENTS,0.7694974003466204,"random selection and random weighted selection schemes while the Multi-RoundSecAgg schemes
387"
EXPERIMENTS,0.7712305025996534,"provide higher levels of privacy. Speciﬁcally, the Multi-RoundSecAgg schemes achieve 푇= 3, 4, 6
388"
EXPERIMENTS,0.7729636048526863,"based on the privacy-preserving family design while the random selection and random weighted
389"
EXPERIMENTS,0.7746967071057193,"selection schemes have 푇= 1, i.e., the server can learn an individual local model.
390"
EXPERIMENTS,0.7764298093587522,"• In the non-IID setting, Multi-RoundSecAgg not only outperforms the random selection scheme but
391"
EXPERIMENTS,0.7781629116117851,"also achieves a smaller aggregation fairness gap as demonstrated in Fig. 4(b).
392"
EXPERIMENTS,0.779896013864818,"• In both IID and non-IID settings, the user partitioning scheme has the worst accuracy as its average
393"
EXPERIMENTS,0.7816291161178509,"aggregation cardinality is much smaller than the other schemes as demonstrated in Fig. 4(c).
394"
EXPERIMENTS,0.7833622183708839,"We also implement additional experiments on MNIST and CIFAR-10 datasets in Appendix E and
395"
EXPERIMENTS,0.7850953206239168,"present ablation study for various settings of (푁, 퐾,푇) in Appendix G
396"
CONCLUSION,0.7868284228769498,"7
Conclusion
397"
CONCLUSION,0.7885615251299827,"Partial user participation may breach user privacy in federated learning, even if secure aggregation is
398"
CONCLUSION,0.7902946273830156,"employed at every training round. To address this challenge, we introduced the notion of long-term
399"
CONCLUSION,0.7920277296360485,"privacy, which ensures that the privacy of individual models are protected over all training rounds. We
400"
CONCLUSION,0.7937608318890814,"developed Multi-RoundSecAgg, a structured user selection strategy that guarantees long-term privacy
401"
CONCLUSION,0.7954939341421143,"while taking into account the fairness in user selection and average number of participating users,
402"
CONCLUSION,0.7972270363951474,"and showed that Multi-RoundSecAgg provides a trade-off between long-term privacy and average
403"
CONCLUSION,0.7989601386481803,"number of participating users (hence the convergence rate). Our experiments on the CIFAR-100,
404"
CONCLUSION,0.8006932409012132,"CIFAR-10, and MNIST datasets on both the IID and non-IID settings show that Multi-RoundSecAgg
405"
CONCLUSION,0.8024263431542461,"achieves comparable accuracy to the random selection strategy (which does not ensure long-term
406"
CONCLUSION,0.804159445407279,"privacy), while ensuring long-term privacy guarantees.
407"
REFERENCES,0.8058925476603119,"References
408"
REFERENCES,0.8076256499133448,"[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar,
409"
REFERENCES,0.8093587521663779,"and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
410"
REFERENCES,0.8110918544194108,"conference on computer and communications security, pages 308–318, 2016.
411"
REFERENCES,0.8128249566724437,"[2] James Henry Bell, Kallista A Bonawitz, Adrià Gascón, Tancrède Lepoint, and Mariana Raykova.
412"
REFERENCES,0.8145580589254766,"Secure single-server aggregation with (poly) logarithmic overhead. In Proceedings of the 2020
413"
REFERENCES,0.8162911611785095,"ACM SIGSAC Conference on Computer and Communications Security, pages 1253–1269, 2020.
414"
REFERENCES,0.8180242634315424,"[3] Kallista Bonawitz, Peter Kairouz, Brendan McMahan, and Daniel Ramage. Federated learning
415"
REFERENCES,0.8197573656845754,"and privacy: Building privacy-preserving systems for machine learning and data science on
416"
REFERENCES,0.8214904679376083,"decentralized data. Queue, 19(5):87–114, 2021.
417"
REFERENCES,0.8232235701906413,"[4] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan,
418"
REFERENCES,0.8249566724436742,"Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for
419"
REFERENCES,0.8266897746967071,"privacy-preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on
420"
REFERENCES,0.82842287694974,"Computer and Communications Security, pages 1175–1191, 2017.
421"
REFERENCES,0.830155979202773,"[5] Wenlin Chen, Samuel Horvath, and Peter Richtarik. Optimal client sampling for federated
422"
REFERENCES,0.8318890814558059,"learning. arXiv preprint arXiv:2010.13723, 2020.
423"
REFERENCES,0.8336221837088388,"[6] Yae Jee Cho, Samarth Gupta, Gauri Joshi, and Osman Ya˘gan. Bandit-based communication-
424"
REFERENCES,0.8353552859618717,"efﬁcient client selection strategies for federated learning. arXiv preprint arXiv:2012.08009,
425"
REFERENCES,0.8370883882149047,"2020.
426"
REFERENCES,0.8388214904679376,"[7] Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence
427"
REFERENCES,0.8405545927209706,"analysis and power-of-choice selection strategies. arXiv preprint arXiv:2010.01243, 2020.
428"
REFERENCES,0.8422876949740035,"[8] Beongjun Choi, Jy-yong Sohn, Dong-Jun Han, and Jaekyun Moon.
Communication-
429"
REFERENCES,0.8440207972270364,"computation efﬁcient secure aggregation for federated learning.
arXiv preprint
430"
REFERENCES,0.8457538994800693,"arXiv:2012.05433, 2020.
431"
REFERENCES,0.8474870017331022,"[9] Cynthia Dwork, Aaron Roth, et al.
The algorithmic foundations of differential privacy.
432"
REFERENCES,0.8492201039861352,"Foundations and Trends® in Theoretical Computer Science, 9(3–4):211–407, 2014.
433"
REFERENCES,0.8509532062391681,"[10] Ahmed Roushdy Elkordy and A Salman Avestimehr. Secure aggregation with heterogeneous
434"
REFERENCES,0.8526863084922011,"quantization in federated learning. arXiv preprint arXiv:2009.14388, 2020.
435"
REFERENCES,0.854419410745234,"[11] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit
436"
REFERENCES,0.8561525129982669,"conﬁdence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC
437"
REFERENCES,0.8578856152512998,"Conference on Computer and Communications Security, pages 1322–1333, 2015.
438"
REFERENCES,0.8596187175043327,"[12] Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. Inverting gradients–
439"
REFERENCES,0.8613518197573656,"how easy is it to break privacy in federated learning?
arXiv preprint arXiv:2003.14053,
440"
REFERENCES,0.8630849220103987,"2020.
441"
REFERENCES,0.8648180242634316,"[13] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical
442"
REFERENCES,0.8665511265164645,"data distribution for federated visual classiﬁcation. arXiv preprint arXiv:1909.06335, 2019.
443"
REFERENCES,0.8682842287694974,"[14] Vishesh Jain, Ashwin Sah, and Mehtaab Sawhney. Singularity of discrete random matrices ii.
444"
REFERENCES,0.8700173310225303,"arXiv preprint arXiv:2010.06554, 2020.
445"
REFERENCES,0.8717504332755632,"[15] Swanand Kadhe, Nived Rajaraman, O Ozan Koyluoglu, and Kannan Ramchandran. Fastsecagg:
446"
REFERENCES,0.8734835355285961,"Scalable secure aggregation for privacy-preserving federated learning.
arXiv preprint
447"
REFERENCES,0.8752166377816292,"arXiv:2009.11248, 2020.
448"
REFERENCES,0.8769497400346621,"[16] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis,
449"
REFERENCES,0.878682842287695,"Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings,
450"
REFERENCES,0.8804159445407279,"et al. Advances and open problems in federated learning. Foundations and Trends® in Machine
451"
REFERENCES,0.8821490467937608,"Learning, 14(1–2):1–210, 2021.
452"
REFERENCES,0.8838821490467937,"[17] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis,
453"
REFERENCES,0.8856152512998267,"Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings,
454"
REFERENCES,0.8873483535528596,"et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977,
455"
REFERENCES,0.8890814558058926,"2019.
456"
REFERENCES,0.8908145580589255,"[18] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
457"
REFERENCES,0.8925476603119584,"Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
458"
REFERENCES,0.8942807625649913,"International Conference on Machine Learning, pages 5132–5143. PMLR, 2020.
459"
REFERENCES,0.8960138648180243,"[19] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
460"
REFERENCES,0.8977469670710572,"and Neil Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision–
461"
REFERENCES,0.8994800693240901,"ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part
462"
REFERENCES,0.901213171577123,"V 16, pages 491–507. Springer, 2020.
463"
REFERENCES,0.902946273830156,"[20] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
464"
REFERENCES,0.9046793760831889,"Technical report, Citeseer, 2009.
465"
REFERENCES,0.9064124783362218,"[21] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. http://yann.
466"
REFERENCES,0.9081455805892548,"lecun. com/exdb/mnist, 2010.
467"
REFERENCES,0.9098786828422877,"[22] Yann LeCun, Patrick Haffner, Léon Bottou, and Yoshua Bengio. Object recognition with
468"
REFERENCES,0.9116117850953206,"gradient-based learning. In Shape, contour and grouping in computer vision, pages 319–345.
469"
REFERENCES,0.9133448873483535,"Springer, 1999.
470"
REFERENCES,0.9150779896013865,"[23] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence
471"
REFERENCES,0.9168110918544194,"of fedavg on non-iid data. In International Conference on Learning Representations, 2019.
472"
REFERENCES,0.9185441941074524,"[24] H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
473"
REFERENCES,0.9202772963604853,"Communication-efﬁcient learning of deep networks from decentralized data. In Int. Conf. on
474"
REFERENCES,0.9220103986135182,"Artiﬁcial Int. and Stat. (AISTATS), pages 1273–1282, 2017.
475"
REFERENCES,0.9237435008665511,"[25] H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially
476"
REFERENCES,0.925476603119584,"private recurrent language models. Int. Conf. on Learning Representations (ICLR), 2018.
477"
REFERENCES,0.9272097053726169,"[26] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep
478"
REFERENCES,0.92894280762565,"learning: Passive and active white-box inference attacks against centralized and federated
479"
REFERENCES,0.9306759098786829,"learning. In 2019 IEEE symposium on security and privacy (SP), pages 739–753. IEEE, 2019.
480"
REFERENCES,0.9324090121317158,"[27] Balázs Pejó and Gergely Biczók. Quality inference in federated learning with secure aggregation.
481"
REFERENCES,0.9341421143847487,"arXiv preprint arXiv:2007.06236, 2020.
482"
REFERENCES,0.9358752166377816,"[28] Monica Ribero and Haris Vikalo. Communication-efﬁcient federated learning via optimal client
483"
REFERENCES,0.9376083188908145,"sampling. arXiv preprint arXiv:2007.15197, 2020.
484"
REFERENCES,0.9393414211438474,"[29] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
485"
REFERENCES,0.9410745233968805,"image recognition. arXiv preprint arXiv:1409.1556, 2014.
486"
REFERENCES,0.9428076256499134,"[30] Jinhyun So, Ramy E Ali, Ba¸sak Güler, and A Salman Avestimehr. Secure aggregation for
487"
REFERENCES,0.9445407279029463,"buffered asynchronous federated learning. arXiv preprint arXiv:2110.02177, 2021.
488"
REFERENCES,0.9462738301559792,"[31] Jinhyun So, Ba¸sak Güler, and A Salman Avestimehr. Turbo-aggregate: Breaking the quadratic
489"
REFERENCES,0.9480069324090121,"aggregation barrier in secure federated learning. IEEE Journal on Selected Areas in Information
490"
REFERENCES,0.949740034662045,"Theory, 2(1):479–489, 2021.
491"
REFERENCES,0.951473136915078,"[32] Jinhyun So, Corey J Nolet, Chien-Sheng Yang, Songze Li, Qian Yu, Ramy E Ali, Basak Guler,
492"
REFERENCES,0.9532062391681109,"and Salman Avestimehr. Lightsecagg: a lightweight and versatile design for secure aggregation
493"
REFERENCES,0.9549393414211439,"in federated learning. Proceedings of Machine Learning and Systems, 4:694–720, 2022.
494"
REFERENCES,0.9566724436741768,"[33] Sebastian U Stich.
Local sgd converges fast and communicates little.
arXiv preprint
495"
REFERENCES,0.9584055459272097,"arXiv:1805.09767, 2018.
496"
REFERENCES,0.9601386481802426,"[34] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural
497"
REFERENCES,0.9618717504332756,"networks. In International Conference on Machine Learning, pages 6105–6114. PMLR, 2019.
498"
REFERENCES,0.9636048526863085,"[35] Minxue Tang, Xuefei Ning, Yitu Wang, Yu Wang, and Yiran Chen. Fedgp: Correlation-based
499"
REFERENCES,0.9653379549393414,"active client selection for heterogeneous federated learning. arXiv preprint arXiv:2103.13822,
500"
REFERENCES,0.9670710571923743,"2021.
501"
REFERENCES,0.9688041594454073,"[36] Tuan Tran. The smallest singular value of random combinatorial matrices. arXiv preprint
502"
REFERENCES,0.9705372616984402,"arXiv:2007.06318, 2020.
503"
REFERENCES,0.9722703639514731,"[37] Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS
504"
REFERENCES,0.9740034662045061,"Quek, and H Vincent Poor. Federated learning with differential privacy: Algorithms and
505"
REFERENCES,0.975736568457539,"performance analysis. IEEE Transactions on Information Forensics and Security, 15:3454–
506"
REFERENCES,0.9774696707105719,"3469, 2020.
507"
REFERENCES,0.9792027729636048,"[38] Chien-Sheng Yang, Jinhyun So, Chaoyang He, Songze Li, Qian Yu, and Salman Avestimehr.
508"
REFERENCES,0.9809358752166378,"Lightsecagg:
Rethinking secure aggregation in federated learning.
arXiv preprint
509"
REFERENCES,0.9826689774696707,"arXiv:2109.14236, 2021.
510"
REFERENCES,0.9844020797227037,"[39] Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less
511"
REFERENCES,0.9861351819757366,"communication: Demystifying why model averaging works for deep learning. In Proceedings
512"
REFERENCES,0.9878682842287695,"of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 5693–5700, 2019.
513"
REFERENCES,0.9896013864818024,"[40] Yizhou Zhao and Hua Sun. Information theoretic secure aggregation with user dropouts. arXiv
514"
REFERENCES,0.9913344887348353,"preprint arXiv:2101.07750, 2021.
515"
REFERENCES,0.9930675909878682,"[41] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
516"
REFERENCES,0.9948006932409013,"learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.
517"
REFERENCES,0.9965337954939342,"[42] Ligeng Zhu and Song Han. Deep leakage from gradients. In Federated Learning, pages 17–31.
518"
REFERENCES,0.9982668977469671,"Springer, 2020.
519"
