Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00425531914893617,"Parametric and non-parametric classiﬁers often have to deal with real-world data,
where corruptions like noise, occlusions, and blur are unavoidable – posing signiﬁ-
cant challenges. We present a probabilistic approach to classify strongly corrupted
data and quantify uncertainty, despite the model only having been trained with
uncorrupted data. A semi-supervised autoencoder trained on uncorrupted data is
the underlying architecture. We use the decoding part as a generative model for
realistic data and extend it by convolutions, masking, and additive Gaussian noise
to describe imperfections. This constitutes a statistical inference task in terms of
the optimal latent space activations of the underlying uncorrupted datum. We solve
this problem approximately with Metric Gaussian Variational Inference (MGVI).
The supervision of the autoencoder’s latent space allows us to classify corrupted
data directly under uncertainty with the statistically inferred latent space activa-
tions. Furthermore, we demonstrate that the model uncertainty strongly depends
on whether the classiﬁcation is correct or wrong, setting a basis for a statistical ""lie
detector"" of the classiﬁcation. Independent of that, we show that the generative
model can optimally restore the uncorrupted datum by decoding the inferred latent
space activations."
INTRODUCTION AND MOTIVATION,0.00851063829787234,"1
INTRODUCTION AND MOTIVATION"
INTRODUCTION AND MOTIVATION,0.01276595744680851,"Many real-world applications of data-driven classiﬁers, e.g., neural networks, involve corruptions
that pose signiﬁcant challenges to the pretrained classiﬁers. Often, the corruption must previously be
included, and thus already be known, during the process of training. For instance, noise (e.g., due to
sensor imperfections) and convolutions (e.g., due to lens ﬂares or unfocused images) are inevitable in
image processing systems and may occur spontaneously and irregularly. The same holds for masking,
which may occur when a foreign object occludes the actual object of interest (e.g., water droplets or
dirt or scratches on the camera lens)."
INTRODUCTION AND MOTIVATION,0.01702127659574468,"Hence, we aim to answer the following question in this paper: How can we classify corrupted data
with a parametric classiﬁer trained exclusively on uncorrupted data? As classifying corrupted data
naturally demands a measure of uncertainty for validation (corruption may, in the worst case, lead to
a total loss of information), we include both model uncertainty δm and reconstruction uncertainty δr
in the classiﬁcation. We refer to δm as the model’s conﬁdence on the classiﬁcation itself. In contrast,
we refer to δr as the conﬁdence of the process on reconstructing the latent space activations given
some corrupted datum. An overview of the proposed method is illustrated in Figure 1."
"CLASSIFICATION AND UNCERTAINTY QUANTIFICATION OF CORRUPTED
DATA",0.02127659574468085,"2
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION OF CORRUPTED
DATA"
METHODOLOGY OVERVIEW AND RELATED WORK,0.02553191489361702,"2.1
METHODOLOGY OVERVIEW AND RELATED WORK"
METHODOLOGY OVERVIEW AND RELATED WORK,0.029787234042553193,"To address the challenge of classiﬁcation and uncertainty quantiﬁcation of corrupted data, we propose
the following core approach, illustrated in Figure 2."
METHODOLOGY OVERVIEW AND RELATED WORK,0.03404255319148936,Under review as a conference paper at ICLR 2022 −5 0 5
METHODOLOGY OVERVIEW AND RELATED WORK,0.03829787234042553,H ± δr & f(x) 0 10 20
METHODOLOGY OVERVIEW AND RELATED WORK,0.0425531914893617,δm ± δr
METHODOLOGY OVERVIEW AND RELATED WORK,0.04680851063829787,0 1 2 3 4 5 6 7 8 9 −5 0 5 10 y
METHODOLOGY OVERVIEW AND RELATED WORK,0.05106382978723404,0 1 2 3 4 5 6 7 8 9 0 5 10 y
METHODOLOGY OVERVIEW AND RELATED WORK,0.05531914893617021,"g(H)
x
d = mCx + n"
METHODOLOGY OVERVIEW AND RELATED WORK,0.059574468085106386,"Figure 1: From left to right: Ground truth image x in the data space, corrupted image d in the data
space (random masking m, Gaussian blur C, additive white Gaussian noise n), posterior mean H
in the latent space with reconstruction uncertainty δr, model uncertainty δm and the restored image
g(H) (decoded posterior mean) in the data space. We have included the encoding of the uncorrupted
data f(x) (illustrated by the shaded white bars in the third column). Top row: data sample from the
MNIST-dataset (ground truth label: 4). Bottom row: data sample from the Fashion-MNIST-dataset
(ground truth label: 2 (pullover)). We can classify d using the posterior mean H as the autoencoder’s
latent space is supervised (note the highlighted max. activation responsible for classiﬁcation). We
are able to classify and quantify model uncertainty δm with the Mahalanobis-distance in the latent
space (note the highlighted min. activation responsible for classiﬁcation). Strong overlapping for the
Fashion-MNIST-example of the 1 · σ error bars of δr across different classes indicates that no reliable
and conﬁdent classiﬁcation is possible due to heavy corruption."
METHODOLOGY OVERVIEW AND RELATED WORK,0.06382978723404255,"1 In the ﬁrst step, we train a semi-supervised autoencoder (Le et al., 2018) that is: (a) capable
of classifying the input data with its latent space activations h, and (b) capable of decoding the
(supervised) latent space activations to generate higher-dimensional data, targeting it to be identical
to the input. Except for these two constraints (a) and (b), we do not impose any further restrictions on
the autoencoder and train it as a standard feedforward neural network."
METHODOLOGY OVERVIEW AND RELATED WORK,0.06808510638297872,"2 In the second step, we decouple the decoder g from the autoencoder and treat the decoder as a
ﬁxed generative function g. Neither retraining nor further modifying of g is done in the following
steps."
METHODOLOGY OVERVIEW AND RELATED WORK,0.07234042553191489,"3 In the third step, we include g in an ADDITIVE WHITE GAUSSIAN NOISE (AWGN) channel-
model d = mCg(h) + n. This AWGN channel model additionally involves heavy corruption like
convolution C and masking m."
METHODOLOGY OVERVIEW AND RELATED WORK,0.07659574468085106,"4 In the ﬁnal step, we approximate the posterior probability distribution P(h|d) in the latent space,
and derive the mean and standard deviation, corresponding optimally to some uncorrupted datum
g(h), given the corrupted datum d. Due to supervision at the latent space, this reconstruction enables
a direct classiﬁcation of d including model and reconstruction uncertainty quantiﬁcation, even though
the decoding function was trained on uncorrupted data."
METHODOLOGY OVERVIEW AND RELATED WORK,0.08085106382978724,"We use a set of samples H from the approximate posterior probability distribution to determine the
sampling mean mean(H) = H as well as the set’s reconstruction uncertainty δr with the sampling
standard deviation std(H). Samples are statistically inferred by METRIC GAUSSIAN VARIATIONAL
INFERENCE (MGVI) (Knollmüller & Enßlin, 2020)."
METHODOLOGY OVERVIEW AND RELATED WORK,0.0851063829787234,"In addition to reconstruction uncertainty δr, we determine the model uncertainty by calculating the
MAHALANOBIS-distance (M-distance) (De Maesschalck et al., 2000) in the latent space represen-
tation, slightly different to Lee et al. (2018). For details of our implementation, see Algorithm 2
in the appendix. We here distinguish between reconstruction uncertainty δr and model uncertainty
δm to evaluate the conﬁdence of the process of inferring h and to evaluate the conﬁdence of the
classiﬁcation given by the supervised latent space, respectively."
METHODOLOGY OVERVIEW AND RELATED WORK,0.08936170212765958,Under review as a conference paper at ICLR 2022 [784] x [512] [256]
METHODOLOGY OVERVIEW AND RELATED WORK,0.09361702127659574,"[128] [10] h [128] [256] [512] [784] g(h) [784] n
+
= [784] d 1
2 3 4"
METHODOLOGY OVERVIEW AND RELATED WORK,0.09787234042553192,P(h|d)
METHODOLOGY OVERVIEW AND RELATED WORK,0.10212765957446808,"H = {˜h1, ˜h2, . . . , ˜hn}"
METHODOLOGY OVERVIEW AND RELATED WORK,0.10638297872340426,VARIATIONAL INFERENCE
METHODOLOGY OVERVIEW AND RELATED WORK,0.11063829787234042,"Figure 2: Concept visualization: Steps involved in classifying corrupted data and quantifying
uncertainty of the reconstruction, as described in Section 2.1. The arrow from d to P(h|d) graphically
illustrates the process of statistically inferring the latent space activations from corrupted data d using
MGVI. H includes all samples drawn from the posterior distribution in the latent space. Subsequently,
the sampling mean and sampling standard deviation are determined from H to classify corrupted
data samples and quantify their reconstruction uncertainty. Model uncertainty is determined via the
Mahalanobis distance in the latent space. We do not depict convolution C and masking m in the
ﬁgure above for simplicity."
METHODOLOGY OVERVIEW AND RELATED WORK,0.1148936170212766,"Similarly to our approach, Böhm et al. (2019) and Böhm & Seljak (2020) have shown that the
reconstruction of the latent space by posterior inference and by using generative models ((Adler
& Öktem, 2018), (Seljak & Yu, 2019), (Wu et al., 2018)) for a corrupted datum can lead to an
optimal image restoration with uncertainty quantiﬁcation. These methods do, however, not focus on
classifying the corrupted datum in the latent space, nor using supervised autoencoder structures."
METHODOLOGY OVERVIEW AND RELATED WORK,0.11914893617021277,"In the ﬁeld of quantifying uncertainties of classiﬁcations there exist several methods. Predominantly
BAYESIAN NEURAL NETWORKS (BNNs) including neural network ensembling ((Depeweg et al.,
2017), (Neal, 1995), (Pearce et al., 2020)) and MONTE CARLO-dropout (MC dropout) (Gal &
Ghahramani, 2016) have lately shown success. More recently, EVIDENTIAL DEEP LEARNING
(Sensoy et al., 2018) was introduced as yet another probabilistic method to quantify classiﬁcation
uncertainty. The latter two methods will be compared to our method in Section 3."
METHODOLOGY OVERVIEW AND RELATED WORK,0.12340425531914893,"Finally, various methods to perform image restoration exist in the literature (see (Dong et al., 2012),
(Lehtinen et al., 2018), (Mao et al., 2016a), (Mao et al., 2016b), (Zoran & Weiss, 2011)). Similar
to the well-known denoising autoencoder (Vincent et al., 2008), almost all methods require prior
knowledge of the corruption to be included in the training data. We argue that these methods lack
ﬂexibility, as they deal with one speciﬁc type of corruption. Once the model is trained, one cannot
include other corruption types without retraining."
METHODOLOGY OVERVIEW AND RELATED WORK,0.1276595744680851,"Moreover, many methods focus on either classiﬁcation or eliminating corruption, but none of the
named approaches combine both."
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.13191489361702127,"2.2
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS"
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.13617021276595745,"The ﬁrst step of our method is to train a semi-supervised autoencoder. The autoencoder involves
the encoding function f (mapping data x ∈Rp to the latent space representation with activations
h ∈Rz, z ∈N) as well as the decoding function g (mapping h to the data space representation
ˆx ∈Rp, p ∈N, p ≫z). Parameters of f : Rp →Rz and g : Rz →Rp are optimized via a
combination of two loss terms Lgf (representing reconstruction loss in the data space) and Lf
(representing classiﬁcation loss in the latent space):"
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.14042553191489363,"LSAE = Lgf(g(f(x)), x) + Lf(f(x)j, y) = Lgf(ˆx, x) + Lf(hj, y).
(1)"
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.14468085106382977,"where j denotes the number of dimensions of h that are supervised, i.e., hj = [h1, . . . , hj]. The
number of classes to be classiﬁed equals j. After normalizing all data samples (i.e., pixel values range"
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.14893617021276595,Under review as a conference paper at ICLR 2022
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.15319148936170213,"in between [0, 1]), we use the corresponding cross-entropy for each respective loss term to penalize
false classiﬁcations in the latent space and inaccurate reconstructions in the data space. The binary
cross-entropy represents the reconstruction loss, while we use the sparse categorical cross-entropy
on integer labels y = [0, 1, . . . , 9] to represent the classiﬁcation loss. Note that for training, we
process the latent space activations h through the softmax-function before feeding them to the sparse
categorical cross-entropy. However, the softmax-function is not included as an activation function
in our neural network. We minimize the general loss function Equation (1) using Adam optimizer
(Kingma & Ba, 2015)1."
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.1574468085106383,"Once the training procedure has converged, we decouple the decoding function g from the autoencoder
and extend it to model different types of corruption (this is necessary as the decoder is trained on
uncorrupted data). Without loss of generality, we use an AWGN model including the nonlinearity
g(h), which additionally involves masking m and convolution C on g:"
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.16170212765957448,"d = mCg(h) + n.
(2)"
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.16595744680851063,"In the data space, additive white Gaussian noise, n ∈Rp ∼N(0, Σn), is applied to the decoded
latent space signal g(h), which yields the corrupted data d ∈Rp. Note for the implementation of
h = Aξ + µh, the reparametrization trick Kingma & Welling (2014) is applied.2"
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.1702127659574468,"In addition to AWGN, we include corruptions of masking m and convolutions C, which are both
linear operations. See the Appendix A, for details on the implementation of n, m and C."
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.17446808510638298,"Since we are interested in reconstructing the latent space activation h from d alongside uncertainty
quantiﬁcation, the goal is to determine the posterior distribution P(h|d) ∝P(d|h)P(h). The
log-probability distribution reads"
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.17872340425531916,−ln P(h|d) = 1 2
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.1829787234042553," 
d −mCg(h)
TΣ−1
n (d −mCg(h)) +
 
hTΣ−1
h h

+ const.,
(3)"
GENERATIVE MODEL AND BAYESIAN INFERENCE WITH NEURAL NETWORKS,0.18723404255319148,"where (·)T denotes the matrix transpose. Since we are ﬁnally interested in the analytically intractable
mean of h, ⟨h⟩P(h|d) =
R
hP(h|d)dh, we approximately determine mean and variance of P(h|d)
by applying MGVI. Similar to other variational inference methods (Kingma & Welling (2014),
Kucukelbir et al. (2017)), MGVI approximates the distribution by a simper, but tractable distribution
from within a variational family, Q(h). The parameters of Q(h), i.e., mean η and covariance ∆, are
obtained by the minimization of the variational lower bound. The size of a full variational covariance
scales quadratically with the number of latent variables. Taking these limitations into account, we
employ MGVI, which locally approximates the target distribution using the inverse Fisher metric as
an uncertainty estimate around the variational mean η, for which we optimize. The approximation is
represented by an ensemble of samples H = {˜h1, ˜h2, . . . , ˜hn} with ˜hn ∈Rz, which we use for our
analysis. ˜(·) refers to the inferred sample. We here call H the posterior mean and δr the posterior
standard deviation, or, the reconstruction uncertainty."
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION,0.19148936170212766,"2.3
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION"
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION,0.19574468085106383,"The supervision of the latent space allows us to classify the input d in a straightforward manner by
evaluating the sampling mean and sampling standard deviation of the set H. While the sampling
mean of the set mean(H) = H gives the class of the most likely classiﬁcation, the sampling standard
deviation reﬂects the reconstruction uncertainty δr of the latent space posterior distribution. δr
depends on the type and magnitude of the corruption as well as the prior probability distribution we
include in the channel model (Equation (3)). We visualize this dependency with various experiments,
see Figure 4 and Figure 10."
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION,0.2,"The straightforward classiﬁcation does not yet provide information about the model uncertainty on the
classiﬁcation. Since we are additionally interested in the uncertainty of the model, δm, we evaluate
the M-distance of all samples in H to all class conditional distributions in the latent space. The
closest class conditional distribution to a single sample ˜hn corresponds to the most likely class. The
absolute value of the M-distance to the closest class conditional distribution serves as a measure of the"
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION,0.20425531914893616,"1Test accuracy of [98, 6%; 89, 4%] on the encoding function f with [MNIST; Fashion-MNIST].
2Σh = cov(f(XVal)), Σh = AAT, Σh ∈Rz×z, ξ ∼N(0, I), ξ ∈Rz, µh = mean(XVal), µh ∈Rz"
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION,0.20851063829787234,Under review as a conference paper at ICLR 2022
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION,0.2127659574468085,"−5
0
5
10 −5 0 5 H PC-1 PC-2"
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION,0.2170212765957447,"0
1
2
3
4
5
6
7
8
9 δm0 δm1 δm2 δm3 δm4 δm5 δm6 δm7"
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION,0.22127659574468084,"δm8
δm9"
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION,0.225531914893617,"Figure 3: Illustration of the latent space structure of a semi-supervised autoencoder and the M-
distance as a classiﬁer based on MNIST. For this visualization, the 10-dimensional latent space
activations are mapped to a two-dimensional subspace using a Principal Component (PC) analysis
(Wold et al., 1987). For an arbitrary corrupted datum d, the inferred posterior mean H in the latent
space is marked accordingly. To classify H, the M-distance is computed to every cluster in the latent
space to obtain δmi for all ten classes. The shortest distance arg min(δm) serves as the classiﬁcation.
In this concrete example, the given posterior mean H would likely be classiﬁed to digit 1. The
absolute value of δm reﬂects at the same time the model uncertainty w.r.t. each class in the latent
space."
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION,0.2297872340425532,"model uncertainty δm. In this work, all class conditional distributions in the latent space are assumed
to follow multivariate Gaussian distributions with covariance Σi and mean µi. We determine the
parameters of these class conditional distributions by passing the uncorrupted data samples from
an independent (i.e., independent of training and testing) dataset XVal (see Section 3) through the
encoder f. This method is an implementation slightly different to Lee et al. (2018), where it was
shown that the Mahalanobis metric is not only an accurate classiﬁer in this context but also a reliable
out-of-distribution detector reﬂecting the model uncertainty. Lee et al. (2018) uses tied covariance
matrices instead of individual covariance matrices for each class conditional distribution, as done in
our method."
CLASSIFICATION AND UNCERTAINTY QUANTIFICATION,0.23404255319148937,"Concretely, we calculate the M-distance of all samples in H to all class-conditional clusters Ck within
the latent space, each characterized by µCk and ΣCk for K-classes. We then determine sampling
mean δm and sampling standard deviation δr of the M-distances of all samples. This way, we can
represent reconstruction uncertainty by the sampling standard deviation (resulting directly from
the shape of the inferred posterior distribution) and model uncertainty by the absolute value of the
M-distance. For a graphical illustration, refer to Figure 3. The pseudo-code is given in Algorithm 2
in the appendix."
SUMMARY AND LIMITATIONS,0.23829787234042554,"2.4
SUMMARY AND LIMITATIONS"
SUMMARY AND LIMITATIONS,0.2425531914893617,"We summarize our proposed methodology (Algorithm 1) to classify a corrupted datum d including
uncertainty quantiﬁcation, which requires the following input in addition to d:"
SUMMARY AND LIMITATIONS,0.24680851063829787,"m, C: Without loss of generality, here we assume corruption by masking and convolution represented
by m and C in the AWGN channel-model, as written in Equation (2). The convolution C can be
determined with methods proposed by, e.g., Herbel et al. (2018), Hu & de Haan (2006) and Schlecht
et al. (2006), occlusion m can be modeled by, e.g., Li et al. (2013) and Rosales & Sclaroff (1998).
Σn: Noise covariance matrix. Noise is drawn from a Gaussian distribution with covariance Σn and
mean µn = 0 and applied additively to the data d. Various methods exist to extract Σn given d
(e.g. (Gravel et al., 2004), (Liu et al., 2012), (Russo, 2003)). Σh: Sampling covariance matrix of all
(uncorrupted) latent space activations processed by the encoding function f. We use the assumption
that an autoencoder can represent an inherent, sub-dimensional structure of the data in its latent space
and assume this sub-dimensional structure to sufﬁciently follow a multivariate Gaussian probability
distribution."
SUMMARY AND LIMITATIONS,0.251063829787234,Under review as a conference paper at ICLR 2022
SUMMARY AND LIMITATIONS,0.2553191489361702,"Algorithm 1: Classiﬁcation and Uncertainty Quantiﬁcation of Corrupted Data
Input: Decoder g : Rz →Rp; corrupted datum d; noise covariance Σn ∈Rp×p; corruption
models m and C; latent space covariance Σh ∈Rz×z
Output: Classiﬁcation ˆyd, reconstruction uncertainty δr, model uncertainty δm; reconstruction
of uncorrupted datum g(H)"
SUMMARY AND LIMITATIONS,0.25957446808510637,"1 Decouple decoder g from semi-supervised autoencoder, pretrained on uncorrupted data"
SUMMARY AND LIMITATIONS,0.26382978723404255,"2 Deﬁne data model, d = mCg(h) + n"
SUMMARY AND LIMITATIONS,0.2680851063829787,"3 Approximate P(h|d) with MGVI and store samples in H = {˜h1, ˜h2, . . . , ˜hn}"
SUMMARY AND LIMITATIONS,0.2723404255319149,"4 Determine the sampling mean of H (i.e., H) and classify datum directly or via M-distance to
obtain ˆyd"
QUANTIFY RECONSTRUCTION UNCERTAINTY WITH SAMPLING STANDARD DEVIATION FROM H,0.2765957446808511,5 Quantify reconstruction uncertainty with sampling standard deviation from H
QUANTIFY MODEL UNCERTAINTY USING M-DISTANCE IN THE LATENT SPACE,0.28085106382978725,6 Quantify model uncertainty using M-distance in the latent space
QUANTIFY MODEL UNCERTAINTY USING M-DISTANCE IN THE LATENT SPACE,0.2851063829787234,7 Generate reconstruction of uncorrupted datum g(H)
EXPERIMENTS,0.28936170212765955,"3
EXPERIMENTS"
EXPERIMENTS,0.2936170212765957,"To experimentally validate our method of classifying corrupted data with a supervised autoencoder
trained on uncorrupted data, we conduct several experiments3 on the MNIST (LeCun, 1998) and the
Fashion-MNIST (Xiao et al., 2017) dataset (both MIT-licenses, https://opensource.org/
licenses/MIT). We evaluate the performance on various corruptions types and magnitudes and
perform a comparison to MC dropout (Gal & Ghahramani, 2016) and EDL (Sensoy et al., 2018).
The following architecture is used for the supervised autoencoder (we use the same architecture for
both datasets): A feedforward neural network is built with dimensions 784{0} −512{1} −256{2} −
128{3} −10{4} −128{5} −256{6} −512{7} −784{8}, where layers {0} −{2} and {4} −{7}
use the SeLU activation function (Klambauer et al., 2017), layer {3} linear and layer {8} sigmoid
activations. Note that in our case, for simplicity, the number of latent space dimensions z is equal to
the number of supervised classes j, although j ≤z holds generally. For experiments, we train the
neural network with the architecture above on two different datasets, MNIST and Fashion-MNIST.
We split each dataset into three subsets, XTrain (48 · 103 samples, used for training), XTest (10 · 103
samples, used for testing and experiments) and XVal (12 · 103 samples, used for determining Σh
and ΣCk . . . ΣCK). We use Tensorﬂow-Keras (Chollet et al., 2015) (Apache License, version 2.0,
http://www.apache.org/licenses/LICENSE-2.0) to implement the neural networks
and the MGVI implementation of NIFTy (Selig et al., 2013) (General Public License, version 3.0,
https://www.gnu.org/licenses/gpl-3.0.en.html) to perform the inference."
CLASSIFICATION,0.2978723404255319,"3.1
CLASSIFICATION"
CLASSIFICATION,0.3021276595744681,"We visualize experiments (1) – (3) in Figure 4. In the ﬁrst experiment (1), we classify data from
an independent test set of the MNIST-dataset corrupted by different noise levels with the proposed
method. We compare the accuracy of our method to the baseline of processing corrupted data through
the encoder of the pretrained autoencoder. We show that we signiﬁcantly improve the accuracy of
classifying corrupted data in comparison to the straightforward classiﬁcation by f(d). For the second
experiment (2), we use the same data samples as for (1) with the exception that we now additionally
corrupt the data with window masking (see Appendix for visualization and details) at a constant noise
level of α = 0.1. Again, we compare the accuracy of our method to the baseline of processing the
same data samples through the encoder."
CLASSIFICATION,0.30638297872340425,"In the third experiment (3), we corrupt the data by convolving them with a Gaussian blur kernel with
a ﬁlter size of 7 × 7 and different magnitudes γ at a constant noise level of α = 0.1."
CLASSIFICATION,0.31063829787234043,"Experiments (1), (2), and (3) lead to the following conclusions:"
CLASSIFICATION,0.3148936170212766,"• The reconstruction uncertainty δrTrue of correct classiﬁcations is approximately equivalent
to δrFalse of wrong classiﬁcations. It is thus independent of the classiﬁcation."
CLASSIFICATION,0.3191489361702128,3Code to be found  here.
CLASSIFICATION,0.32340425531914896,Under review as a conference paper at ICLR 2022
CLASSIFICATION,0.3276595744680851,"0.01
0.1
1
10
0 0.2 0.4 0.6 0.8 1"
CLASSIFICATION,0.33191489361702126,Noise level α
CLASSIFICATION,0.33617021276595743,"ACCh
(1) 0 0.5 1"
CLASSIFICATION,0.3404255319148936,"1.5
δr"
CLASSIFICATION,0.3446808510638298,"ACCg
ACCf
Random"
CLASSIFICATION,0.34893617021276596,"δr
δrFalse
δrTrue"
CLASSIFICATION,0.35319148936170214,"0.01
0.1
1
10
0 0.2 0.4 0.6 0.8 1"
CLASSIFICATION,0.3574468085106383,Noise level α ACCδm 5 10 δm
CLASSIFICATION,0.3617021276595745,"ACCg
ACCf
Random"
CLASSIFICATION,0.3659574468085106,"δm
δmFalse
δmTrue"
CLASSIFICATION,0.3702127659574468,"2
4
6
8
10
12
14
0 0.2 0.4 0.6 0.8 1"
CLASSIFICATION,0.37446808510638296,Masking level β
CLASSIFICATION,0.37872340425531914,"ACCh
(2) 0 0.5 1 1.5 2 δr"
CLASSIFICATION,0.3829787234042553,"2
4
6
8
10
12
14
0 0.2 0.4 0.6 0.8 1"
CLASSIFICATION,0.3872340425531915,Masking level β ACCδm 5 10 15 δm
CLASSIFICATION,0.39148936170212767,"10−1
100
101
0 0.2 0.4 0.6 0.8 1"
CLASSIFICATION,0.39574468085106385,Convolution level γ
CLASSIFICATION,0.4,"ACCh
(3) 0 0.5 1 δr"
CLASSIFICATION,0.40425531914893614,"10−1
100
101
0 0.2 0.4 0.6 0.8 1"
CLASSIFICATION,0.4085106382978723,Convolution level γ ACCδm 5 10 15 δm
CLASSIFICATION,0.4127659574468085,"Figure 4: Accuracy and uncertainty (reconstruction uncertainty δr and model uncertainty δm) of
classiﬁcations of data samples of the MNIST dataset (for Fashion-MNIST see Figure 10 in the
Appendix A) at different noise levels (left column), different masking levels (middle column), and
different convolution levels (right column) exploiting the supervised latent space structure (top row)
and the M-distance (bottom row) as classifying features.
ACCf serves as the baseline and is the accuracy of the plain encoding function f classifying corrupted
data. ACCg corresponds to the accuracy of the method proposed in Section 2.1. Additionally, we
distinguish between uncertainties of correct classiﬁcations δrTrue, δmTrue and of false classiﬁcations
δrFalse, δmFalse.
The M-distance and thus δm strongly depends on the quality of the classiﬁcation; δr depends on the
the level of corruption and is independent of the classiﬁcation result. The plot was generated with
1000 test samples for each data point (accordingly corrupted). The standard deviation is averaged
over all 1000 samples."
CLASSIFICATION,0.41702127659574467,"• Opposed to δr, the model uncertainty δm strongly depends on the correct/wrong clas-
siﬁcation of the corrupted datum: δm is signiﬁcantly and consistently higher for false
classiﬁcations than for true classiﬁcations. This delta in the model uncertainty corresponds
to our method’s concept of capturing the class conditional distribution density with the
M-distance: For wrong classiﬁcations, the M-distance is larger because the corrupted sample
is dissimilar to samples from its underlying class conditional distribution. This feature sets
the basis for a statistical ""lie detector"" (see section 3.2) of classiﬁcation and thus allows the
possibility of various applications: One could pass data of high reconstruction uncertainty to
further processing methods, as the process of inference is in this case likely faulty. Moreover,
it is possible to pass detected false classiﬁcations with high model uncertainties to human
veriﬁcation. Fields of application could be the validation of neural networks in, e.g., medical
imaging and other safety-critical applications."
CLASSIFICATION,0.42127659574468085,"• Classifying corrupted data through the decoder (rather than the encoder) with a suitable
channel model considering the corruption signiﬁcantly improves the model’s accuracy
without the necessity of retraining the autoencoder. Only for very low noise levels, the direct
processing of the corrupted data through f performs better than our method. We observe a
slight loss of accuracy in the process of inference."
CLASSIFICATION,0.425531914893617,"• Both uncertainties δr and δm rise with increasing levels of corruption, as expected. Also,
the absolute value of the reconstruction uncertainty δr correlates inversely with the corre-
sponding accuracy across all three experiments."
CLASSIFICATION,0.4297872340425532,Under review as a conference paper at ICLR 2022
DETECTION OF FALSE CLASSIFICATIONS,0.4340425531914894,"3.2
DETECTION OF FALSE CLASSIFICATIONS"
DETECTION OF FALSE CLASSIFICATIONS,0.43829787234042555,"Finally, in the experiment (4) (see Figure 5), motivated by the results of experiments so far, we
validate the model uncertainty of our method by introducing the uncertainty based Receiver Operating
Characteristics (U-ROC) curve of detecting false classiﬁcations with the M-distance. We evaluate
the binary classiﬁcation task of the two classes ""The neural network correctly classiﬁes a corrupted
datum"" (POSITIVE CLASS) and ""The neural network falsely classiﬁes a corrupted datum"" (NEGATIVE
CLASS). Based on the model uncertainty of our method, we aim to predict the two classes without
further knowledge, providing the initially proposed ""lie detector"". The U-ROC curve is built from the
TRUE POSITIVE RATE and the FALSE POSITIVE RATE."
DETECTION OF FALSE CLASSIFICATIONS,0.4425531914893617,"We detect a false classiﬁcation if the minimum M-distance of a reconstructed sample in the latent
space is above some threshold value. On the contrary, we detect a correct classiﬁcation if the
minimum M-distance is below the threshold. These threshold values vary for the plot depicted in
Figure 5 between 0 and 12, being conﬁdent at 0 and uncertain at 12. We show by Figure 5 that
the model uncertainty of our methodology truly reﬂects the conﬁdence on a speciﬁc classiﬁcation,
verifying that high model uncertainty correlates with false classiﬁcations."
DETECTION OF FALSE CLASSIFICATIONS,0.44680851063829785,"We compare our U-ROC curve with the U-ROC curve of the MC dropout method (Gal & Ghahramani,
2016) and with the U-ROC curve of EDL (Sensoy et al., 2018), feeding all methods with the identical
input of a datum corrupted by noise at α = [0.1, 0.5, 1.0]. Kindly note that this comparison uses the
optimized neural network architectures presented in the respective publications, which is different
from our simplistic proof-of-concept architecture: Both EDL and MC dropout use the LeNet (Lecun
et al., 1998) with custom modiﬁcations4,5, while we use a signiﬁcantly simpler feedforward neural
network, see Figure 2. Note that our method is not limited to feedforward neural networks and
applicable to convolutional neural networks, as well. MC dropout exploits weight-dropout in a neural
network to achieve statistically varying outputs of their classifying neural network at the same input
over several forward passes. They argue that overlapping output samples indicate high uncertainty
in the classiﬁcation – we use the number of overlapping samples as the metric for detecting false
classiﬁcations (applying 50 repetitions per sample). On the contrary, EDL trains the neural network
to learn parameters of a Dirichlet distribution, instead of softmax probabilities. By replacing the
standard output of a classiﬁcation network (softmax) with the parameters of a Dirichlet density, EDL
represents the predictions of the neural network as a distribution over possible softmax outputs, rather
than the point estimate of a softmax output. The output of EDL equals the range of the possible
entropies, i.e., [0, log(10)]. To create the U-ROC curve of EDL, we thus use different thresholds
ranging from 0 to 1."
DETECTION OF FALSE CLASSIFICATIONS,0.451063829787234,"We make the following conclusions from experiment (4), Figure 5:"
DETECTION OF FALSE CLASSIFICATIONS,0.4553191489361702,"• Our method seems to outperform MC dropout and EDL to detect false classiﬁcations given
the same data samples at the input for α = 0.1 and α = 0.5. One reason for this might be
that the M-distance serves as a reliable out-of-distribution detector, exploiting the inherent
latent space structure of uncorrupted data as a reference, as opposed to MC dropout and
EDL. For α = 1.0, both EDL and our method outperform MC dropout, while the AUC
of EDL is largest. Here, it should be noted that EDL cannot classify the corrupted data at
this noise level (accuracy: 8.9%), resulting in only few samples to test the cases of TRUE
POSITIVES and FALSE POSITIVES."
DETECTION OF FALSE CLASSIFICATIONS,0.4595744680851064,"• All three methods provide reliable results for detecting false classiﬁcations for low noise
levels."
DETECTION OF FALSE CLASSIFICATIONS,0.46382978723404256,"• The model uncertainty δm truly reﬂects the conﬁdence of the classiﬁcation, i.e., a high value
of δm correlates empirically with a higher probability of false classiﬁcation."
DETECTION OF FALSE CLASSIFICATIONS,0.46808510638297873,"• U-ROC curve combined with the accuracy indicates that EDL seems to overestimate un-
certainties, leading to a very robust U-ROC curve for high noise levels, but simultaneously
leading in presence of data corruption to a severe drop in the accuracy of the model."
DETECTION OF FALSE CLASSIFICATIONS,0.4723404255319149,"4MC dropout: dropout is applied before the last fully connected inner-product layer
5EDL: LeNet trained to learn parameters of a Dirichlet distribution, instead of softmax probabilities"
DETECTION OF FALSE CLASSIFICATIONS,0.4765957446808511,Under review as a conference paper at ICLR 2022
DETECTION OF FALSE CLASSIFICATIONS,0.4808510638297872,"0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1"
DETECTION OF FALSE CLASSIFICATIONS,0.4851063829787234,False Positive Rate
DETECTION OF FALSE CLASSIFICATIONS,0.48936170212765956,True Positive Rate
DETECTION OF FALSE CLASSIFICATIONS,0.49361702127659574,U-ROC-Curve α = 0.1
DETECTION OF FALSE CLASSIFICATIONS,0.4978723404255319,"0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1"
DETECTION OF FALSE CLASSIFICATIONS,0.502127659574468,False Positive Rate
DETECTION OF FALSE CLASSIFICATIONS,0.5063829787234042,True Positive Rate
DETECTION OF FALSE CLASSIFICATIONS,0.5106382978723404,U-ROC-Curve α = 0.5
DETECTION OF FALSE CLASSIFICATIONS,0.5148936170212766,"0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1"
DETECTION OF FALSE CLASSIFICATIONS,0.5191489361702127,False Positive Rate
DETECTION OF FALSE CLASSIFICATIONS,0.5234042553191489,True Positive Rate
DETECTION OF FALSE CLASSIFICATIONS,0.5276595744680851,U-ROC-Curve α = 1.0
DETECTION OF FALSE CLASSIFICATIONS,0.5319148936170213,"Our method
MC dropout"
DETECTION OF FALSE CLASSIFICATIONS,0.5361702127659574,"EDL
Random"
DETECTION OF FALSE CLASSIFICATIONS,0.5404255319148936,"α
METHOD
ACC
AUC"
OUR METHOD,0.5446808510638298,"0.1
Our Method
0.862
0.904
MC dropout
0.915
0.745
EDL
0.666
0.746"
OUR METHOD,0.548936170212766,"0.5
Our Method
0.753
0.881
MC dropout
0.755
0.555
EDL
0.125
0.717"
OUR METHOD,0.5531914893617021,"1.0
Our Method
0.580
0.828
MC dropout
0.570
0.472
EDL
0.089
0.885"
OUR METHOD,0.5574468085106383,"Figure 5: Uncertainty based Receiver Operator Characteristics (U-ROC) of the proposed identiﬁer of
false classiﬁcations for different noise levels α of our method in comparison with MC dropout and
with EDL. In this experiment, the formulation of the e.g. ""TRUE NEGATIVE"" case would be: Based
on the uncertainty value, the sample is correctly detected as a false classiﬁcation – the ""lie detector""
works. Samples are taken from the MNIST-dataset. Top left: corrupted datum at α = 0.1. Bottom
left: corrupted datum at α = 1.0. The irregularity in the U-ROC-Curve of the dropout model is due
to the stochastic nature of MC dropout. Bottom right: Evaluation of Accuracy (ACC) for all given
noise values α and the Area under the Curve (AUC) for all U-ROCs."
SUMMARY AND FUTURE RESEARCH,0.5617021276595745,"4
SUMMARY AND FUTURE RESEARCH"
SUMMARY AND FUTURE RESEARCH,0.5659574468085107,"We present a novel approach to classify heavily corrupted data with parametric classiﬁers trained
on uncorrupted data. As we build our procedure on a probabilistic architecture, we quantify both
classiﬁcation and model uncertainty, allowing for a reliable detection of false classiﬁcations. We
see our method as a highly ﬂexible template that can be applied to any generative neural network to
improve performance on corrupted data signiﬁcantly. If the generative neural network comes with a
supervised encoded space, it can classify the data directly. We have shown that the M-distance can
independently be used to classify data. Limitations of our method include that the corruption type
needs to be modeled."
SUMMARY AND FUTURE RESEARCH,0.5702127659574469,"Future research is planned in more realistic, real-time, and complex scenarios with strong corruptions,
e.g., medical imaging, autonomous driving, or astronomy. Here, potential applications of our method
could be image segmentation and object detection via bounding boxes. The method described in
Section 2.1 can provide uncertainties of the bounding box and thus of the position of the detected
object. This might be useful, e.g., if the object of interest is occluded. A typical situation would be a
car visually blocking pedestrians at a crossing."
ACKNOWLEDGEMENTS,0.574468085106383,"5
ACKNOWLEDGEMENTS"
ACKNOWLEDGEMENTS,0.5787234042553191,Anonymous.
ACKNOWLEDGEMENTS,0.5829787234042553,Under review as a conference paper at ICLR 2022
REFERENCES,0.5872340425531914,REFERENCES
REFERENCES,0.5914893617021276,"Jonas Adler and Ozan Öktem. Deep Bayesian Inversion. CoRR, abs/1811.05910, 2018. URL
http://arxiv.org/abs/1811.05910."
REFERENCES,0.5957446808510638,"Vanessa Böhm and Uros Seljak. Probabilistic Auto-Encoder. CoRR, abs/2006.05479, 2020. URL
https://arxiv.org/abs/2006.05479."
REFERENCES,0.6,"Vanessa Böhm, Francois Lanusse, and Uros Seljak. Uncertainty Quantiﬁcation with Generative
Models. NeurIPS, 4th workshop on Bayesian Deep Learning, 2019. doi: arXiv:1910.10046. URL
https://arxiv.org/abs/1910.10046."
REFERENCES,0.6042553191489362,"François Chollet et al. Keras. https://keras.io, 2015."
REFERENCES,0.6085106382978723,"Roy De Maesschalck, Delphine Jouan-Rimbaud, and Desire L Massart. The Mahalanobis Distance.
Chemometrics and Intelligent Laboratory Systems, 50(1):1–18, 2000. ISSN 0169-7439."
REFERENCES,0.6127659574468085,"Stefan Depeweg, Jose-Miguel Hernandez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Decom-
position of Uncertainty in Bayesian deep Learning for Efﬁcient and Risk-Sensitive Learning. In
International Conference on Machine Learning, pp. 1184–1193. PMLR, 2017. ISBN 2640-3498."
REFERENCES,0.6170212765957447,"Weisheng Dong, Lei Zhang, Guangming Shi, and Xin Li. Nonlocally Centralized Sparse Represen-
tation for Image Restoration. IEEE Transactions on Image Processing, 22(4):1620–1630, 2012.
ISSN 1057-7149."
REFERENCES,0.6212765957446809,"Brion Douglas. Code - Evidential Deep Learning to Quantify Classiﬁcation Uncertainty, 2021. URL
https://github.com/dougbrion/pytorch-classification-uncertainty."
REFERENCES,0.625531914893617,"Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing Model
Uncertainty in Deep Learning, 2016."
REFERENCES,0.6297872340425532,"Pierre Gravel, Gilles Beaudoin, and Jacques A De Guise. A method for modeling noise in medical
images. IEEE Transactions on medical imaging, 23(10):1221–1232, 2004."
REFERENCES,0.6340425531914894,"Jörg Herbel, Tomasz Kacprzak, Adam Amara, Alexandre Refregier, and Aurelien Lucchi. Fast point
spread function modeling with deep learning. Journal of Cosmology and Astroparticle Physics,
2018(07):054, 2018. ISSN 1475-7516."
REFERENCES,0.6382978723404256,"H. Hu and G. de Haan. Low Cost Robust Blur Estimator. In 2006 International Conference on Image
Processing, pp. 617–620, 2006. doi: 10.1109/ICIP.2006.312411."
REFERENCES,0.6425531914893617,"Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980."
REFERENCES,0.6468085106382979,"Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In Yoshua Bengio and
Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.
URL http:
//arxiv.org/abs/1312.6114."
REFERENCES,0.6510638297872341,"Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-Normalizing
Neural Networks. In Advances in Neural Information Processing Systems, pp. 971–980, 2017."
REFERENCES,0.6553191489361702,"Jakob Knollmüller and Torsten Enßlin. Metric Gaussian Variational Inference. arXiv pre-print server,
2020. doi: arXiv:1901.11033. URL https://arxiv.org/abs/1901.11033."
REFERENCES,0.6595744680851063,"Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. Automatic
Differentiation Variational Inference. The Journal of Machine Learning Research, 18(1):430–474,
2017. ISSN 1532-4435."
REFERENCES,0.6638297872340425,"Lei Le, Andrew Patterson, and Martha White. Supervised Autoencoders: Improving Generalization
Performance with Unsupervised Regularizers. In Advances in Neural Information Processing
Systems, pp. 107–117, 2018."
REFERENCES,0.6680851063829787,Under review as a conference paper at ICLR 2022
REFERENCES,0.6723404255319149,"Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791."
REFERENCES,0.676595744680851,"Yann LeCun. The MNIST database of Handwritten Digits. http://yann. lecun. com/exdb/mnist/, 1998."
REFERENCES,0.6808510638297872,"Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A Simple Uniﬁed Framework for Detect-
ing out-of-Distribution Samples and Adversarial Attacks. In Advances in Neural Information
Processing Systems, pp. 7167–7177, 2018."
REFERENCES,0.6851063829787234,"Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala,
and Timo Aila. Noise2Noise: Learning Image Restoration without Clean Data. In Jennifer G.
Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80
of Proceedings of Machine Learning Research, pp. 2971–2980. PMLR, 2018. URL http:
//proceedings.mlr.press/v80/lehtinen18a.html."
REFERENCES,0.6893617021276596,"Bo Li, Wenze Hu, Tianfu Wu, and Song-Chun Zhu. Modeling occlusion by discriminative and-
or structures. In Proceedings of the IEEE International Conference on Computer Vision, pp.
2560–2567, 2013."
REFERENCES,0.6936170212765957,"Xinhao Liu, Masayuki Tanaka, and Masatoshi Okutomi. Noise Level Estimation Using Weak
Textured Patches of a Single Noisy Image. In 2012 19th IEEE International Conference on Image
Processing, pp. 665–668. IEEE, 2012. ISBN 1467325333."
REFERENCES,0.6978723404255319,"Xiao-Jiao Mao, Chunhua Shen, and Yu-Bin Yang.
Image Restoration Using Very Deep Con-
volutional Encoder-Decoder Networks with Symmetric Skip Connections.
In Daniel D.
Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.),
Advances in Neural Information Processing Systems 29:
Annual Conference on Neu-
ral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp.
2802–2810, 2016a. URL https://proceedings.neurips.cc/paper/2016/hash/
0ed9422357395a0d4879191c66f4faa2-Abstract.html."
REFERENCES,0.7021276595744681,"Xiao Jiao Mao, Chunhua Shen, and Yu Bin Yang. Image Restoration Using Convolutional Auto-
encoders with Symmetric Skip Connections. CoRR, abs/1606.08921, 2016b. URL http://
arxiv.org/abs/1606.08921."
REFERENCES,0.7063829787234043,"Radford M Neal. Bayesian Learning for Neural Networks. Springer, 1995."
REFERENCES,0.7106382978723405,"Chanwoo Park.
Code:
Dropout as a Bayesian Approximation, 2019.
URL https:
//github.com/cpark321/uncertainty-deep-learning/blob/master/
02.%20Dropout%20as%20a%20Bayesian%20Approximation.ipynb."
REFERENCES,0.7148936170212766,"Tim Pearce, Felix Leibfried, and Alexandra Brintrup. Uncertainty in Neural Networks: Approximately
Bayesian Ensembling. In Silvia Chiappa and Roberto Calandra (eds.), The 23rd International
Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online
[Palermo, Sicily, Italy], volume 108 of Proceedings of Machine Learning Research, pp. 234–244.
PMLR, 2020. URL http://proceedings.mlr.press/v108/pearce20a.html."
REFERENCES,0.7191489361702128,"Romer Rosales and Stan Sclaroff. Improved Tracking of Multiple Humans with Trajectory Prediction
and Occlusion Modeling. Technical report, Boston University Computer Science Department,
1998."
REFERENCES,0.723404255319149,"Fabrizio Russo.
A Method for Estimation and Filtering of Gaussian Noise in Images.
IEEE
Transactions on Instrumentation and Measurement, 52(4):1148–1154, 2003. ISSN 0018-9456."
REFERENCES,0.7276595744680852,"Joseph Schlecht, Kobus Barnard, and Barry Pryor. Statistical Inference of Biological Structure
and Point Spread Functions in 3D Microscopy. In Third International Symposium on 3D Data
Processing, Visualization, and Transmission (3DPVT’06), pp. 373–380, 2006. doi: 10.1109/
3DPVT.2006.131."
REFERENCES,0.7319148936170212,"Marco Selig, Michael R Bell, Henrik Junklewitz, Niels Oppermann, Martin Reinecke, Maksim
Greiner, Carlos Pachajoa, and Torsten A Enßlin. NIFTY–Numerical Information Field Theory-A
versatile PYTHON library for signal inference. Astronomy & Astrophysics, 554:A26, 2013. ISSN
0004-6361."
REFERENCES,0.7361702127659574,Under review as a conference paper at ICLR 2022
REFERENCES,0.7404255319148936,"Uros Seljak and Byeonghee Yu. Posterior Inference Unchained with EL_2O. CoRR, abs/1901.04454,
2019. URL http://arxiv.org/abs/1901.04454."
REFERENCES,0.7446808510638298,"Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential Deep Learning to Quantify Classiﬁ-
cation Uncertainty. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
a981f2b708044d6fb4a71a1463242520-Paper.pdf."
REFERENCES,0.7489361702127659,"Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Inproceedings of the 25th International
Conference on Machine learning, pp. 1096–1103, 2008."
REFERENCES,0.7531914893617021,"Svante Wold, Kim Esbensen, and Paul Geladi. Principal Component Analysis. Chemometrics and
Intelligent Laboratory Systems, 2(1-3):37–52, 1987. ISSN 0169-7439."
REFERENCES,0.7574468085106383,"Ga Wu, Justin Domke, and Scott Sanner. Conditional Inference in Pre-trained Variational Autoen-
coders via Cross-coding. CoRR, abs/1805.07785, 2018. URL http://arxiv.org/abs/
1805.07785."
REFERENCES,0.7617021276595745,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: A Novel Image Dataset for Bench-
marking Machine Learning Algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.7659574468085106,"Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole image
restoration. In 2011 International Conference on Computer Vision, pp. 479–486, 2011. doi:
10.1109/ICCV.2011.6126278."
REFERENCES,0.7702127659574468,Under review as a conference paper at ICLR 2022
REFERENCES,0.774468085106383,"A
APPENDIX"
REFERENCES,0.7787234042553192,"A.1
CORRUPTION LAYOUT"
REFERENCES,0.7829787234042553,"Here we visualize the effect of the different corruptions, i.e., convolution as Gaussian blur C, Figure 8,
masking m (Figure 7, Figure 9), and noise n, (Figure 6). We evaluate the effect of different corruption
levels on accuracy and conﬁdence, where"
REFERENCES,0.7872340425531915,"• α corresponds to the standard deviation of the Gaussian distribution from which noise is
drawn, i.e., Σn = I · α (I ∈Rp×p is the identity matrix)
• β corresponds to the number of columns and rows of pixels set to 0, counted from outside
to inside (i.e., β = 0 means no masking, β = 14 means full image is masked), see ﬁgure
Figure 7,
• γ corresponds to the standard deviation of the Gaussian blur kernel with a ﬁlter size of 7 × 7
pixels."
REFERENCES,0.7914893617021277,"Note that to the experiments conducted on the effect on convolution (Figure 4, right), we added noise
at α = 0.1 . For visualization purposes, we do not add noise for Figure 8."
REFERENCES,0.7957446808510639,"α = 0.01
α = 0.1
α = 1
α = 10"
REFERENCES,0.8,"Figure 6: Exemplary visualization of corruption through noise. Experiments cover the entire noise
range from α = 0.01 to α = 10."
REFERENCES,0.8042553191489362,"β = 3
β = 6
β = 9
β = 12"
REFERENCES,0.8085106382978723,"Figure 7: Exemplary visualization of isolated masking. Experiments cover the entire masking range
from β = 0 to β = 14 and additional noise at α = 0.1. The experiment layout of masking is adopted
from"
REFERENCES,0.8127659574468085,"γ = 1
γ = 1.5
γ = 2
γ = 10"
REFERENCES,0.8170212765957446,"Figure 8: Exemplary visualization of corruption through convolution. Experiments cover the entire
convolution range from γ = 0.1 to γ = 10."
REFERENCES,0.8212765957446808,Under review as a conference paper at ICLR 2022
REFERENCES,0.825531914893617,"α = 0.1, β = 3
α = 0.1, β = 6
α = 0.1, β = 9
α = 0.1, β = 12"
REFERENCES,0.8297872340425532,"Figure 9: Exemplary visualization of different masking levels applied in experiments with additive
noise α = 0.1 on top. Experiments cover the entire masking range from β = 0 to β = 14."
REFERENCES,0.8340425531914893,"A.2
DETERMINATION OF THE MAHALANOBIS-DISTANCE"
REFERENCES,0.8382978723404255,"In addition to the methodology outlined in Section 2.3, we present with Algorithm 2 the pseudo-
code of calculating the M-distance of the inferred latent space samples H to determine the model
uncertainty δm."
REFERENCES,0.8425531914893617,Algorithm 2: Classiﬁcation and Model Uncertainty by Mahalanobis Distance
REFERENCES,0.8468085106382979,"Input: Set of posterior samples H = {˜h1, . . . , ˜hn}; µk, . . . , µK; ΣCk, . . . , ΣCK
Output: Label of Classiﬁed Class y, model uncertainty δm"
REFERENCES,0.851063829787234,1 for n ←0 to N:
REFERENCES,0.8553191489361702,"2
for k ←0 to K:"
REFERENCES,0.8595744680851064,"3
δm[k]n =
q"
REFERENCES,0.8638297872340426,"[˜hn −µk]Σ−1
Ck [˜hn −µk]T"
REFERENCES,0.8680851063829788,"4
D{n} = δmn
5 y = argmin(mean(D))"
REFERENCES,0.8723404255319149,6 δm = mean(D)
REFERENCES,0.8765957446808511,"7 return δm, y"
REFERENCES,0.8808510638297873,Under review as a conference paper at ICLR 2022
REFERENCES,0.8851063829787233,"A.3
EXPERIMENTS ON FASHION-MNIST DATA"
REFERENCES,0.8893617021276595,"Finally, we show in Figure 10 the results of the identical experiment layout of Figure 4 with Fashion-
MNIST data. Note that we trained the neural network with the identical architecture as for MNIST
data. Besides the general loss in the accuracy due to the retraining of the identical neural network on
the more complex Fashion-MNIST dataset, the results are mostly coherent to Figure 4. In column 2,
row 1, we observe that the accuracy does not further decrease for β = 8 through β = 10. We assume
that this is due to the nature of the Fashion-MNIST dataset, where the window mask does not take
away much information for the affected pixels: Classes such as T-shirt/top, Pullover, Dress, and Coat
all usually exhibit the same structure in the affected area of the image."
REFERENCES,0.8936170212765957,"0.01
0.1
1
10
0 0.2 0.4 0.6 0.8 1"
REFERENCES,0.8978723404255319,Noise level α
REFERENCES,0.902127659574468,"ACCh
(1) 0 0.5 1 1.5 2
δr"
REFERENCES,0.9063829787234042,"ACCg
ACCf
Random"
REFERENCES,0.9106382978723404,"δr
δrFalse
δrTrue"
REFERENCES,0.9148936170212766,"0.01
0.1
1
10
0 0.2 0.4 0.6 0.8 1"
REFERENCES,0.9191489361702128,Noise level α ACCδm 5 10 δm
REFERENCES,0.9234042553191489,"ACCg
ACCf
Random"
REFERENCES,0.9276595744680851,"δm
δmFalse
δmTrue"
REFERENCES,0.9319148936170213,"2
4
6
8
10 12 14
0 0.2 0.4 0.6 0.8 1"
REFERENCES,0.9361702127659575,Masking level β
REFERENCES,0.9404255319148936,"ACCh
(2) 0 1 2 δr"
REFERENCES,0.9446808510638298,"2
4
6
8
10 12 14
0 0.2 0.4 0.6 0.8 1"
REFERENCES,0.948936170212766,Masking level β ACCδm 5 10 15 δm
REFERENCES,0.9531914893617022,"10−1
100
101
0 0.2 0.4 0.6 0.8 1"
REFERENCES,0.9574468085106383,Convolution level γ
REFERENCES,0.9617021276595744,"ACCh
(3) 0 0.5 1 1.5 2
δr"
REFERENCES,0.9659574468085106,"10−1
100
101
0 0.2 0.4 0.6 0.8 1"
REFERENCES,0.9702127659574468,Convolution level γ ACCδm 6 8 10 12 14 δm
REFERENCES,0.9744680851063829,"Figure 10: Accuracy and uncertainty (reconstruction uncertainty δr and model uncertainty δm) of
classiﬁcations of data samples of the Fashion-MNIST dataset (for MNIST see Figure 4) at different
noise levels (left column), different masking levels (middle column), and different convolution levels
(right column) exploiting the supervised latent space structure (top row) and the M-distance (bottom
row) as classifying features.
ACCf serves as the baseline and is the accuracy of the plain encoding function f classifying corrupted
data. ACCg corresponds to the accuracy of the method proposed in Section 2.1. Additionally, we
distinguish between uncertainties of correct classiﬁcations δrTrue, δmTrue and of false classiﬁcations
δrFalse, δmFalse."
REFERENCES,0.9787234042553191,"A.4
EXPERIMENT LAYOUT FOR U-ROC CURVE"
REFERENCES,0.9829787234042553,We use the following threshold range for the displayed methods:
REFERENCES,0.9872340425531915,"• The range for U-ROC-Curve of Mahalanobis-Distance: [0, 12]
• The range for U-ROC-Curve of overlapping samples of MC dropout-model: [0, 50]"
REFERENCES,0.9914893617021276,"• The range for the U-ROC-Curve for EDL: [0, 1]."
REFERENCES,0.9957446808510638,"Noise is applied at α = [0.1; 0.5; 1.0] without further corruption. The AUC is calculated using the
trapezoidal rule. Implementation is inspired by Park (2019) for MC dropout and by Douglas (2021)
for EDL."
