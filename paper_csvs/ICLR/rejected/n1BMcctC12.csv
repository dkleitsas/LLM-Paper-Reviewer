Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0013020833333333333,"The large-scale linearly constrained nonsmooth nonconvex optimization ﬁnds
wide applications in machine learning, including non-PSD Kernel SVM, linearly
constrained Lasso with nonsmooth nonconvex penalty, etc. To tackle this class
of optimization problems, we propose an efﬁcient algorithm called Nonconvex
Randomized Primal-Dual Coordinate (N-RPDC) method. At each iteration, this
method only randomly selects a block of primal variables to update rather than
updating all the variables, which is suitable for large-scale problems. We provide
two types of convergence results for N-RPDC. We ﬁrst show that any cluster point
of the sequence of iterates generated by N-RPDC is almost surely (i.e., with proba-
bility 1) a stationary point. In addition, we also provide an almost sure asymptotic
convergence rate of O(1/
√"
ABSTRACT,0.0026041666666666665,"k). Next, we establish the expected O(ε−2) iteration
complexity of N-RPDC in order to drive a natural stationarity measure below ε
in expectation. The fundamental aspect to establishing the aforementioned con-
vergence results is a surrogate stationarity measure we discovered for analyzing
N-RPDC. Finally, we conduct a set of experiments to show the efﬁcacy of N-
RPDC."
INTRODUCTION,0.00390625,"1
INTRODUCTION"
INTRODUCTION,0.005208333333333333,"Many large scale problems arising in machine learning amounts to solving the following linearly
constrained nonsmooth nonconvex optimization problem:
min
x∈X
F(x) = f(x) + g(x)"
INTRODUCTION,0.006510416666666667,"s.t
Ax −b = 0,
(P)"
INTRODUCTION,0.0078125,"where A ∈Rn×d and b ∈Rn. Throughout this paper, we impose the following assumptions on
problem (P): the function f : Rd →R is possibly nonconvex and continuously differentiable with
its gradient ∇f being Lf-Lipschitz continuous, X is a convex and compact set of Rd, i.e., there
exits a positive number M such that M = maxx,x′∈X ∥x −x′∥, and g : Rd →R is nonsmooth and
nonconvex. More precisely, g is assumed to be lower semicontinuous (l.s.c) ρg-weakly convex and
has bounded subgradients over X, i.e., there exists Lg > 0 such that ∥s∥≤Lg, ∀s ∈∂g(x), x ∈X,
where ∂g is the subdifferential of g (see (4) for deﬁnition). Let F ∗be the optimal value of (P)."
INTRODUCTION,0.009114583333333334,Recall that a function g is said to be ρg-weakly convex if g(·) + ρg
INTRODUCTION,0.010416666666666666,"2 ∥· ∥2 is convex for some constant
ρg ≥0 (Vial, 1983). It is worth mentioning that a wide class of nonsmooth nonconvex functions
belong to the weakly convex class; see, e.g., (Vial, 1983; Davis & Drusvyatskiy, 2019) for more
discussions on weak convexity."
INTRODUCTION,0.01171875,"We further assume g(x) = PN
i=1 gi(xi) is block separable with respect to the space decomposition"
INTRODUCTION,0.013020833333333334,"X = X1 × X2 × · · · × XN,
xi ∈Xi ⊂Rdi,
and N
X"
INTRODUCTION,0.014322916666666666,"i=1
di = d.
(1)"
INTRODUCTION,0.015625,"Let A = (A1, A2, . . . , AN) ∈Rn×d be the corresponding partition of A, where Ai ∈Rn×di. When
N = d, each Xi ⊂R corresponds to a box constraint on the coordinate xi for i = 1, . . . , d."
INTRODUCTION,0.016927083333333332,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.018229166666666668,"Motivations. Our motivation for studying large scale problem (P) stems from the fact that this
problem class ﬁnds wide applications in machine learning. To be more speciﬁc, we will present
several practical examples below that give rise to (P)."
INTRODUCTION,0.01953125,"Application 1: Non-PSD kernel support vector machine. Support vector machine (SVM) is a widely
utilized technique for supervised learning (Boser et al., 1992; Cortes & Vapnik, 1995). In order
to improve the interpretability and enhance the robustness, some non-PSD kernels are used in the
kernalized SVM method, such as the sigmoid kernel (Lin & Lin, 2003), the jittering kernel (DeCoste
& Sch¨olkopf, 2002), the tangent distance kernel (Haasdonk & Keysers, 2002, August), to name a
few. Formally, the kernalized SVM problem can be written as
min
x∈[0,c]d
1
2x⊤Qx −1⊤
d x"
INTRODUCTION,0.020833333333333332,"s.t.
y⊤x = 0,
(2)"
INTRODUCTION,0.022135416666666668,"where Q is a d × d non-PSD matrix, c ∈R is the upper bound of all variables, y ∈{−1, 1}d is the
vector of labels, and 1d is an d-dimensional vector of all 1s."
INTRODUCTION,0.0234375,"Application 2: Linearly constrained Lasso with nonsmooth nonconvex penalty. The Lasso is one of the
most popular methods for variable selection. The standard Lasso uses ℓ1-norm to select important
variables. Some works propose to utilize nonsmooth nonconvex regularizers to further improve the
performance of the Lasso; see, e.g., (Breheny & Huang, 2011; 2015; Rakotomamonjy et al., 2019).
If additional prior information is available, the works (Deng et al., 2020, May; Gaines et al., 2018;
James et al., 2013; Won et al., 2019) propose the constrained Lasso model. The linearly constrained
LASSO with nonsmooth nonconvex penalty can be formulated as follows:"
INTRODUCTION,0.024739583333333332,"min
x∈X
1
2∥Ax −b∥2 +
dP"
INTRODUCTION,0.026041666666666668,"i=1
φ(xi)"
INTRODUCTION,0.02734375,"s.t.
Bx −c = 0
(3)"
INTRODUCTION,0.028645833333333332,"where x = (x1, ..., xd)⊤, X ⊂Rd, A ∈Rn×d is the design matrix, b ∈Rn is the response vector,
B ∈Rm×d and c ∈Rm are given constraints, and φ is a nonsmooth nonconvex regularizer such as
MCP (Zhang et al., 2010) or SCAD (Fan & Li, 2001) penalty."
INTRODUCTION,0.029947916666666668,"There are plenty of other applications that give rise to (P), e.g., the robust M-estimators, distributed
learning, etc. Due to the limitation of space, we will not expand them in details here."
INTRODUCTION,0.03125,"Related works. Linearly constrained nonconvex optimization. Perhaps the most widely utilized class
of algorithms for solving constrained optimization problems are the Primal-dual methods. Previous
works on single-loop primal-dual methods mainly consider linearly contained convex optimization
problems. The work (Bot¸ & Nguyen, 2020) considers a nonconvex instance with a special linear
constraint Ax −z = 0, where both x and z are decision variables and the matrix A is assumed to be
surjective. Under these assumptions, convergence results were established for a proximal alternat-
ing direction method of multipliers (ADMM). The recent works (Zhang & Luo, 2020a;b) provide
convergence results of ADMM using gradient-based updates for linearly constrained smooth non-
convex optimization problems with a general linear constraint Ax = b, i.e., problem (P) with g ≡0.
The main insights of their results are a construction of an proximally regularized auxiliary problem
that dates back to (Bertsekas, 1979) and a properly constructed Lyapunov function. Then, they es-
tablished descent property on the Lyapunov function based on the dual error bound condition (see
also (Hong & Luo, 2017)), which leads to iteration complexity results. The method introduced in
(Zhu et al., 2020) can be used to solve problem (P) with speciﬁc linear constraints. This method
is based on the auxiliary problem principle of augmented Lagrangian (APP-AL) method (Cohen &
Zhu, 1984; Zhao & Zhu, 2019; 2021), which can be seen as a forward-backward splitting method
and applies a Jacobian updating strategy.1 However, both ADMM-type and APP-AL-type methods
compute the full gradient of primal variables and update all primal variables at each iteration. There-
fore, the computation complexity of one iteration of these two types of methods are expensive for
large-scale problems (See Remark 2.1)."
INTRODUCTION,0.032552083333333336,"Randomized coordinate methods for unconstrained optimization. In the past decade, big data applica-
tions are ubiquitous in machine learning. The formulated optimization problems often involve very
large datasets, and hence computing the function value or the gradient can be very expensive. These"
INTRODUCTION,0.033854166666666664,"1ADMM applies a Gauss-Seidel like minimization strategy, which is related to the Douglas-Rachford split-
ting method."
INTRODUCTION,0.03515625,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.036458333333333336,"observations motivate the study of the randomized coordinate methods. The researches on this type
of methods can be traced back to (Nesterov, 2012; 2014). In (Nesterov, 2012), Nesterov studied
the iteration complexity of the randomized coordinate gradient (RCG) descent method for smooth
convex optimization. Later, in (Nesterov, 2014), the same author analyzed the randomized coordi-
nate subgradient method for a set of piece-wise linear nonsmooth convex optimization problems.
The works (Richt´arik & Tak´aˇc, 2014) and (Lu & Xiao, 2015) extends Nesterov’s results to convex
additive composite optimization. Iteration complexity and linear convergence of RCG were studied
in (Patrascu & Necoara, 2015) and (Karimi et al., 2016) for nonsmooth nonconvex optimization
and smooth nonconvex optimization, respectively. Moreover, by employing an asynchronous point
to evaluate the gradient in each iteration, The works (Liu & Wright, 2015) and (Liu et al., 2014)
establish the iteration complexity results of asynchronous RCG for convex smooth optimization and
additive composite optimization, respectively."
INTRODUCTION,0.037760416666666664,"A Few results on randomized coordinate methods for linearly constrained optimization. Though the
randomized coordinate-type methods for unconstrained optimization are extensively studied, there
are only a few results for this type of methods concerning coupled linearly constrained optimization
problems. The works (Gao et al., 2019; Wang et al., 2014) study the randomized primal-dual co-
ordinate (RPDC) method and provides iteration complexity results for linearly constrained convex
optimization. Better complexity results of RPDC were obtained for linearly constrained strongly
convex optimization (Xu & Zhang, 2018). The recent works (Zhu & Zhao, 2020; Latafat et al.,
2019; Fercoq & Bianchi, 2019; Alacaoglu et al., 2020) provide almost sure (i.e., with probability
1) asymptotic convergence results of RPDC-type methods for linearly constrained convex optimiza-
tion. It is worth emphasizing that the works (Latafat et al., 2019; Fercoq & Bianchi, 2019; Alacaoglu
et al., 2020) establish sequential convergence results (i.e., the convergence of the whole sequence of
iterates)."
INTRODUCTION,0.0390625,"However, the above mentioned works only considers either ADMM-type and APP-AL-type meth-
ods for linearly constrained smooth nonconvex optimization problem or the randomized coordinate
methods for linearly constrained convex optimization. To the best of our knowledge, no previous re-
sult concerns convergence and iterate complexity of the randomized coordinate methods for linearly
constrained nonsmooth nonconvex optimization, which will be the main focus of this paper."
INTRODUCTION,0.040364583333333336,"Main contributions. In this paper, we aim to solve the large-scale linearly constrained nonsmooth
nonconvex optimization problem (P). To tackle it, we propose the Nonconvex Randomized Primal-
Dual Coordinate (N-RPDC) method based on an auxiliary problem (see Section 2). Relying on the
mild local uniform metric subregularity property (see Section 3.3), we provide two types of conver-
gence results for N-RPDC (see Section 4). We ﬁrst show that any cluster point of the sequence of
iterates generated by N-RPDC is almost surely (i.e., with probability 1) a stationary point of (P). In
addition, we also provide an almost sure asymptotic convergence rate of O(1/
√"
INTRODUCTION,0.041666666666666664,"k), where k repre-
sents iteration number. Next, we establish the expected iteration complexity of N-RPDC. Namely,
N-RPDC needs at most O(ε−2) number of iterations in order to drive a surrogate stationarity mea-
sure below ε in expectation."
INTRODUCTION,0.04296875,"The fundamental aspect to our convergence analysis is a surrogate stationarity measure we discov-
ered for analyzing N-RPDC (see Section 3.1), which is one of the main contributions of this work.
The standard stationarity measure (i.e., checking KKT conditions) is not directly applicable here
due to the stochastic nature of N-RPDC. Instead, we deﬁne the notion of reference point and use its
distance to the iterate generated by N-RPDC as a surrogate stationarity measure. Then, such a surro-
gate stationarity measure is clariﬁed by showing that it is an upper bound of the standard stationarity
measure at the reference point up to a numerical constant (see Proposition 3.1 and Remark 3.1)."
INTRODUCTION,0.044270833333333336,"There are also some other interesting techniques that we utilized for establishing the above conver-
gence results. For example, the utilization of the local uniform metric subregularity is crucial in
order to deal with the nonsmooth nonconvex term g in problem (P), as it is far from obvious how to
prove the previously used error bound condition if g is not null (see Section 3.3)."
INTRODUCTION,0.045572916666666664,"Notations. We use ⟨·, ·⟩and ∥· ∥to denote the Euclidean inner product and the Euclidean norm,
respectively. For a matrix A, its minimum eigenvalue is denoted by λmin(A). The spectral norm of
a matrix A is denoted by ∥A∥. Let S be a subset of Rd. We use projS(z) to denote the orthogonal
projector onto S and dist(z, S) := infx∈S ∥x −z∥to denote the distance between x and S. When"
INTRODUCTION,0.046875,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.048177083333333336,"S = ∅, we set dist(z, S) = +∞. IS(z) =

0,
z ∈S
+∞,
z /∈S
represents the indicator function of the"
INTRODUCTION,0.049479166666666664,"set S. If S is a convex set, then the limiting normal cone to S is deﬁned as NS(z) = {ξ : ⟨ξ, ζ −z⟩≤
0, ∀ζ ∈S}."
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.05078125,"2
AUXILIARY PROBLEM AND PROPOSED ALGORITHM"
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.052083333333333336,"In this section, we will deﬁne an auxiliary problem of (P). An important feature of the auxiliary
problem is that it has exactly the same set of KKT points to the original problem (P). Furthermore,
the auxiliary problem has a better geometric structure, which makes it easier to solve. Finally, we
will propose a primal-dual coordinate method based on the auxiliary problem."
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.053385416666666664,"Let us ﬁrst present some preliminaries on the subdifferential and KKT conditions. Due to the fact
that problem (P) is highly nonsmooth and nonconvex, our general purpose is to design an efﬁcient
algorithm for ﬁnding a stationary point rather than a globally optimal solution. Hence, we will deﬁne
certain suitable stationarity measures in this subsection."
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.0546875,"Recall that the function g in problem (P) is τ-weakly convex. By (Vial, 1983, Proposition 4.6), we
have
∂g(x) = ∂h(x) −τx,
(4)"
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.055989583333333336,where h is the associated convex function such that g(x) = h(x) −τ
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.057291666666666664,"2∥x||2 (see (Vial, 1983, Propo-
sition 4.3) for the guarantee of the existence of such a function h) and ∂h(x) is the usual convex
subdifferential. Thus, the subdifferential of a weakly convex function is always well deﬁned."
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.05859375,The Lagrangian of problem (P) can be written as
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.059895833333333336,"L(x, p) = f(x) + g(x) + ⟨p, Ax −b⟩,"
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.061197916666666664,"where p is the dual variable. Then, we have the following KKT conditions for problem (P):"
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.0625,"0 ∈∂L(x, p) :=

∇f(x) + ∂g(x) + NX(x) + A⊤p
Ax −b"
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.06380208333333333,"
.
(5)"
AUXILIARY PROBLEM AND PROPOSED ALGORITHM,0.06510416666666667,"A feasible point (x, p) ∈X × Rn is called a stationary point of (P) if it satisﬁes the above KKT
conditions, i.e., it satisﬁes dist (0, ∂L(x, p)) = 0."
THE AUXILIARY PROBLEM,0.06640625,"2.1
THE AUXILIARY PROBLEM"
THE AUXILIARY PROBLEM,0.06770833333333333,"Thanks to the construction used in (Bertsekas, 1979), we introduce an auxiliary problem to (P) in
the following:
min
x∈X,z∈Rd
F +(x, z) = f(x) + g(x) + σ"
THE AUXILIARY PROBLEM,0.06901041666666667,2 ∥x −z∥2
THE AUXILIARY PROBLEM,0.0703125,"s.t
Ax −b = 0,
(P+)"
THE AUXILIARY PROBLEM,0.07161458333333333,where σ > Lf + ρg is a regularization parameter.
THE AUXILIARY PROBLEM,0.07291666666666667,"We now characterize the set of stationary points of (P+). Towards that end, let w = (x, z, p) ∈
X × Rd × Rn. The Lagrangian of (P+) is given by"
THE AUXILIARY PROBLEM,0.07421875,"L+(w) = L+(x, z, p) = f(x) + g(x) + σ"
THE AUXILIARY PROBLEM,0.07552083333333333,"2 ∥x −z∥2 + ⟨p, Ax −b⟩."
THE AUXILIARY PROBLEM,0.07682291666666667,"Thus, we have the following KKT conditions for problem (P+):"
THE AUXILIARY PROBLEM,0.078125,"0 ∈∂L+(w) = ∂L+(x, z, p) := "
THE AUXILIARY PROBLEM,0.07942708333333333,"
∇f(x) + ∂g(x) + σ(x −z) + NX(x) + A⊤p
σ(z −x)
Ax −b "
THE AUXILIARY PROBLEM,0.08072916666666667,".
(6)"
THE AUXILIARY PROBLEM,0.08203125,"A point w = (x, z, p) ∈X × Rd × Rn satisfying the above KKT conditions is called a stationary
point of (P+). One crucial feature of the auxiliary problem is that it has exactly the same set of
stationary points as that of the original problem (P), which is presented in the following lemma."
THE AUXILIARY PROBLEM,0.08333333333333333,"Lemma 2.1 (relation between stationary points of (P) and (P+)). The following two statements are
equivalent: (a) (x∗, p∗) is a stationary point of problem (P); and (b) (x∗, x∗, p∗) is a stationary
point of problem (P+)."
THE AUXILIARY PROBLEM,0.08463541666666667,Under review as a conference paper at ICLR 2022
THE AUXILIARY PROBLEM,0.0859375,"Algorithm 1 N-RPDC: Nonconvex Randomized Primal-Dual Coordinate Method
Initialization: set x0 ∈X, z0 ∈Rd, and p0 ∈Null(A⊤); Step sizes η, αx, and αz."
THE AUXILIARY PROBLEM,0.08723958333333333,"1: for k = 0, 1, . . . do
2:
Update p as pk+1 = pk + η(Axk −b).
3:
Choose i(k) from {1, ..., N} uniformly at random;
4:
Update x and z through coordinate steps:"
THE AUXILIARY PROBLEM,0.08854166666666667,"xk+1 = arg min
x∈X

∇i(k)f(xk) + σ(xk −zk)i(k), xi(k)

+ gi(k)(xi(k))"
THE AUXILIARY PROBLEM,0.08984375,"+

pk+1 + γ(Axk −b), Ai(k)xi(k)

+ D(x, xk) αx
,"
THE AUXILIARY PROBLEM,0.09114583333333333,"zk+1 = arg min
z∈Rd

σ(zk −xk)i(k), zi(k)

+
1
2αz
∥z −zk∥2."
THE AUXILIARY PROBLEM,0.09244791666666667,5: end for
THE AUXILIARY PROBLEM,0.09375,"Proof. We ﬁrst prove that part (a) implies part (b). Since (x∗, p∗) is a stationary point of (P), we
have 0 ∈∂L(x∗, p∗). Upon plugging z = x∗in the L+(x∗, z, p∗) shows that 0 ∈∂L+(x∗, x∗, p∗).
The inverse direction is proved by a similar argument."
THE AUXILIARY PROBLEM,0.09505208333333333,"The auxiliary problem can be viewed as a quadratically regularized version of the original problem
(P) with an additional variable z. Recall that f has Lf-Lipschitz gradient and g is ρg-weakly convex.
We have the following benign properties of the auxiliary problem (P+), whose derivations can be
found in Appendix A.2:"
THE AUXILIARY PROBLEM,0.09635416666666667,(1) f(x) + σ
THE AUXILIARY PROBLEM,0.09765625,"2 ∥x −z∥2 is continuously differentiable in (x, z) with Lipschitz continuous gradient;"
THE AUXILIARY PROBLEM,0.09895833333333333,"(2) F +(x, z) = f(x) + g(x) + σ"
THE AUXILIARY PROBLEM,0.10026041666666667,"2 ∥x −z∥2 is weakly convex in (x, z) with parameter Lf + ρg;"
THE AUXILIARY PROBLEM,0.1015625,"(3) F +(x, z) is bi-strongly convex, i.e., F +(x, z) is strongly convex in x for every ﬁxed z and
F +(x, z) is strongly convex in z for every ﬁxed x."
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.10286458333333333,"2.2
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.10416666666666667,"As we established in the last subsection, problem (P+) has the same set of stationary points as that
of the original problem (P) and the former has a series of benign properties. These observations
motivate us to design algorithm for solving the original problem by targeting on (P+). Similar
to standard primal-dual methods, our algorithm for solving problem (P+) builds on the following
augmented Lagrangian of (P+):"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.10546875,"L+
γ (x, z, p) = f(x) + g(x) + σ"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.10677083333333333,"2 ∥x −z∥2 + ⟨p, Ax −b⟩+ γ"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.10807291666666667,"2 ∥Ax −b∥2.
(7)"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.109375,"With the augmented Lagrangian, our algorithm is designed by performing an APP-AL update over
(x, z, p) at each iteration. Furthermore, the updates for variables x and z are achieved through
randomized coordinate steps. We depict our algorithmic procedures—i.e., the N-RPDC method—
in Algorithm 1."
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.11067708333333333,"The proximal term D(x, xk) used in N-RPDC is the so-called Bregman distance function. Let βK
be the strong convexity parameter of the Bregman distance function D(x, xk). It is safe for the
readers to think D(x, xk) = 1"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.11197916666666667,"2∥x −xk∥2 for now. We put the more general choices of D(x, xk) in
Appendix A.3."
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.11328125,"It is worth mentioning that the subproblem for updating xk+1 in N-RPDC has a closed form solution
once the Bregman distance function D(x, xk) = 1"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.11458333333333333,"2∥x −xk∥2 and gi(xi) is SCAD, MCP, quadratic,
or ℓν-norms with ν ∈{1, 2, ∞}."
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.11588541666666667,"Remark 2.1 (computational complexity). It is claimed in (Nesterov, 2012; Wright, 2015) that a co-
ordinate method can have a much lower computational complexity than its full counterpart. Let us
take the second motivating application (i.e., nonsmooth nonconvex constrained Lasso, problem (3)
in Application 2) in Section 1 as an illustrative example to show that the computational complexity
of one iteration of N-RPDC with N = d can be much cheaper than the full primal-dual counterpart
(ADMM-type or APP-type method). Note that in this case, each Xi is a box constraint on the coordi-"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.1171875,Under review as a conference paper at ICLR 2022
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.11848958333333333,"nate xi. Suppose we set D(x, xk) = 1"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.11979166666666667,"2∥x −xk∥2. Then, the update of wk+1 = (xk+1, zk+1, pk+1)"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.12109375,"consists of: 1) pk+1 = pk + ηsk. 2) xk+1
i(k) = proxαx,IXi(k)+φ

xk
i(k) −αx(
 
Ai(k)
⊤rk + σ(xk −"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.12239583333333333,"zk)i(k) +
 
Bi(k)
⊤(pk+1 + γsk))

and xk+1
j̸=i(k) = xk
j̸=i(k), where IXi(k) is the indicator function"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.12369791666666667,"of Xi(k). 3) zk+1
i(k) = zk
i(k) −αzσ(zk −xk)i(k) with zk+1
j̸=i(k) = zk
j̸=i(k). We also update the residual"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.125,"vectors as rk+1 = rk+Ai(k)(xk+1
i(k) −xk
i(k)) and sk+1 = sk+Bi(k)(xk+1
i(k) −xk
i(k)). If we precompute
r0 = Ax0 −b and s0 = Bx0 −c, and the proximal mapping proxαx,IXi(k)+φ admits a closed form
solution (which is the case when φ is the SCAD or MCP penalty), then the above updates have a
total complexity O(max{m, n}). Therefore, the complexity of N-RPDC is d× cheaper than its full
counterpart (which has a computational complexity of O(d max{m, n}) in one iteration)."
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.12630208333333334,"3
STATIONARITY MEASURE, LYAPUNOV FUNCTION, AND UNIFORM METRIC
SUBREGULARITY"
NONCONVEX RANDOMIZED PRIMAL-DUAL COORDINATE METHOD,0.12760416666666666,"In this section, we will establish a series of important results, which serve as the foundations for our
later convergence analysis."
THE STATIONARITY MEASURE,0.12890625,"3.1
THE STATIONARITY MEASURE"
THE STATIONARITY MEASURE,0.13020833333333334,"Consider the current iterate wk. If wk+1 is updated by the full primal-dual counterpart to N-RPDC
(i.e., N = 1 in Algorithm 1), then the step length ∥wk −wk+1∥is a natural stationarity measure
due to the facts that dist(0, ∂L+(wk+1)) = O(∥wk −wk+1∥) with some step sizes hidden in
the big-O and limk→+∞∥wk −wk+1∥= 0 can be established by an easy argument based on an
appropriate Lyapunov function. However, if wk+1 is updated by our N-RPDC with N ̸= 1, which
is a stochastic algorithm that only updates part of wk to wk+1, it is no longer obvious to show
limk→+∞∥wk −wk+1∥= 0. Therefore, due to the stochastic nature of N-RPDC, the step length
∥wk −wk+1∥cannot play the role of stationarity measure. This observation motivates us to work
with a surrogate stationarity measure. Towards that end, for a given point w = (x, z, p), we deﬁne
the reference point of w as T(w) := (Tx(w), Tz(w), Tp(w)) and





"
THE STATIONARITY MEASURE,0.13151041666666666,"



"
THE STATIONARITY MEASURE,0.1328125,Tp(w) = p + η(Ax −b);
THE STATIONARITY MEASURE,0.13411458333333334,"Tx(w) = arg min
y∈X ⟨∇f(x) + σ(x −z), y⟩+ g(y) + ⟨Tp(w) + γ(Ax −b), Ay⟩+ D(y, x) αx
;"
THE STATIONARITY MEASURE,0.13541666666666666,"Tz(w) = arg min
y∈Rd ⟨σ(z −x), y⟩+
1
2αz
∥y −z∥2. (8)"
THE STATIONARITY MEASURE,0.13671875,"One possible way to understanding the reference point is to let wk = (xk, zk, pk), then T(wk) is
nothing else other than the next iterate wk+1 if the full primal-dual method is utilized, i.e., N = 1 in
N-RPDC. However, it is worthing mentioning in the case where N > 1, we do not need to compute
the reference point T(wk) explicitly. Instead, N-RPDC only use a block coordinate for updating."
THE STATIONARITY MEASURE,0.13802083333333334,"The next proposition provides us a valid stationary measure, which is crucial for our later conver-
gence analysis."
THE STATIONARITY MEASURE,0.13932291666666666,"Proposition 3.1 (surrogate stationarity measure). For all w = (x, z, p) ∈X × Rd × Rn, we have"
THE STATIONARITY MEASURE,0.140625,"dist
 
0, ∂L+(T(w))

≤c∥w −T(w)∥=: Φ(w)."
THE STATIONARITY MEASURE,0.14192708333333334,"Here, T(w) is the reference point of w deﬁned in (8) and c :=
√"
"MAX
N
LK",0.14322916666666666,"3 max
n
LK"
"MAX
N
LK",0.14453125,"αx +Lf +2σ+∥A∥, 1 αz +"
"MAX
N
LK",0.14583333333333334,"2σ, γ∥A∥+1"
"MAX
N
LK",0.14713541666666666,"η
o
is a positive constant."
"MAX
N
LK",0.1484375,"Remark 3.1 (interpretation of the surrogate stationarity measure). The mapping Φ(w) will serve
as the surrogate stationarity measure for analyzing the convergence of N-RPDC. Particularly, it is
clear that dist
 
0, ∂L+
γ (T(w))

= 0 when Φ(w) = 0. In order to characterize the iteration com-
plexity of N-RPDC, we focusing on achieving Φ(w) ≤ε since it implies that w is ε-close to the
reference point T(w) which is an ε-stationary point (here, T(w) is deﬁned to be ε-stationary since"
"MAX
N
LK",0.14973958333333334,Under review as a conference paper at ICLR 2022
"MAX
N
LK",0.15104166666666666,"dist (0, ∂L+(T(w))) ≤ε). These interpretations clarify our surrogate stationarity measure. Note
that similar notion of surrogate stationarity measure dates back to Ekeland’s variational principle
(Ekeland, 1974) and also appeared in recent advances of the analysis of the subgradient-type meth-
ods; see, e.g., (Davis & Drusvyatskiy, 2019; Li et al., 2021). It is interesting to note that the main
difference between our approach and the existing ones lies in the choice of the reference point. Let
us take the approach used in (Davis & Drusvyatskiy, 2019) for analyzing subgradient-type method
as an example. The authors deﬁne the proximal mapping of the objective function as the reference
point, while we utilize the update of the full primal-dual method (i.e., N = 1 in Algorithm 1) as
the reference point. Such a main difference comes from different sources of difﬁculties. The main
difﬁculty for the analysis of subgradient-type method in (Davis & Drusvyatskiy, 2019) comes from
the fact that this algorithm is intrinsically not a descent method on the objective function, while our
main difﬁculty is due to the stochastic nature of N-RPDC."
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.15234375,"3.2
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.15364583333333334,"Recall that the function F + deﬁned in (P+) is strongly convex in x for every ﬁxed z (see our analysis
in Section 2.1). Let us deﬁne"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.15494791666666666,"ν(z) =
min
x∈X, Ax−b=0 F +(x, z)
and
x(z) =
arg min
x∈X, Ax−b=0
F +(x, z).
(9)"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.15625,"In addition, we also deﬁne"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.15755208333333334,"ψγ(z, p) = min
x∈X L+
γ (x, z, p)
and
x(z, p) = arg min
x∈X
L+
γ (x, z, p),
(10)"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.15885416666666666,"where L+
γ is deﬁned in (7)."
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.16015625,"Next, mimicking the construction of the Lyapunov function used in (Zhang & Luo, 2020b), we
deﬁne
Λ(w) = L+
γ (w) + 2 (ν(z) −ψγ(z, p))
(11)"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.16145833333333334,"as the Lyapunov function for our convergence analysis. The ﬁrst part of Λ(·) is augmented La-
grangian L+
γ (w) of (P+). The second part is dual gap of (P+) with ﬁxed z. Additionally, by
the deﬁnition of ψγ(z, p), we have that L+
γ (w) ≥ψγ(z, p).
By the strong duality, we have
ν(z) = max
p
ψγ(z, p). Therefore, we have the fact that Λ(w) ≥ν(z) ≥F ∗."
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.16276041666666666,"Standard analysis for primal-dual methods develops certain descent property on the Lyapunov func-
tion. Before that, we ﬁrst present a preliminary one-step analysis for N-RPDC. Note that indices
i(k), k = 0, 1, 2, . . . in N-RPDC are random variables. Thus, it generates a random output. We use"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.1640625,"Fk := {i(0), i(1), . . . , i(k)}"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.16536458333333334,"to denote the ﬁltration generated by the random variables i(0), i(1), . . . , i(k). Clearly, we have
Fk ⊂Fk+1. We use Ei(k) and EFk to denote the expectations taken over the random variable i(k)
and the ﬁltration Fk, respectively. The following lemma provides estimations for ν(z) −ψγ(z, p)
and L+
γ (w) after one-step execution of N-RPDC."
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.16666666666666666,"Lemma 3.1 (one-step analysis of N-RPDC). Suppose 0 < αx ≤βK/

Lf + 2σ + γ∥A∥2 + 5

and"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.16796875,"0 < η < 1/ """
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.16927083333333334,"2N∥A∥2

Lf +σ+γ∥A∥2+ LK"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.17057291666666666,"αx
σ−Lf −ρg
+ 1
2#"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.171875,". For all k ≥0, we have"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.17317708333333334,Λ(wk) −Ei(k)Λ(wk+1) ≥2
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.17447916666666666,N ∥xk −Tx(wk)∥2 + 1 N  1
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.17578125,"αz
−2σ −σ"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.17708333333333334,"λ −
3σ2"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.17838541666666666,σ −Lf −ρg
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.1796875,"
∥zk −Tz(wk)∥2"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.18098958333333334,"+ η∥Ax(zk, Tp(wk)) −b∥2 −σλ∥x(zk, Tp(wk)) −x(zk)∥2. (12)"
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.18229166666666666,where σ is deﬁned in (P+) and λ is any positive constant.
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.18359375,"Toward establishing certain decent property on the Lyapunov function in terms of the stationar-
ity measure Φ(wk), we have to connect all the terms on the right-hand side of the inequality of
Lemma 3.1 to Φ2(wk) = ∥wk−T(wk)∥2. Thus, it remains to upper bound ∥x(zk, Tp(wk))−x(zk)∥
by ∥Ax(zk, Tp(wk)) −b∥, which needs the so-called uniform metric subregularity property."
THE LYAPUNOV FUNCTION AND ONE-STEP ANALYSIS,0.18489583333333334,Under review as a conference paper at ICLR 2022
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.18619791666666666,"3.3
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE"
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.1875,Let us deﬁne the parameter set-valued mapping
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.18880208333333334,"∂Lz
γ(x, p) =

∇f(x) + ∂g(x) + σ(x −z) + NX(x) + A⊤(p + γ(Ax −b))
Ax −b"
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.19010416666666666,"
(13)"
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.19140625,"as the subdifferential of L+
γ with respect to (x, p) with ﬁxed z. Clearly, we have ∂Lz
γ(x(z, p), p) =

0
Ax(z, p) −b"
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.19270833333333334,"
due to the optimality of x(z, p) in (10). To proceed, we introduce the notion of"
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.19401041666666666,"local uniform metric subregularity property for the parameter set-valued mapping.
Deﬁnition 3.1 (local uniform metric subregularity, cf. (Kruger & Duy Cuong, 2021)). Let Hz(u) :
U ⇒V be a set-valued mapping. We call that Hz(u) satisﬁes the locally metric subregularity
property with parameter (δ, κ) uniformly over all z ∈Z if"
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.1953125,"dist
 
u, H−1
z (0)

≤κ · dist (0, Hz(u)) ,
∀z ∈Z,
(14)"
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.19661458333333334,"for any u ∈{u′ ∈U : dist(0, Hz(u′)) ≤δ}, where δ, κ > 0 are uniform constants."
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.19791666666666666,"This local error bound condition gives the following result.
Lemma 3.2. Suppose the set-valued mapping ∂Lz
γ(x, p) deﬁned in (13) satisﬁes the local uniform
metric subregularity property, then there is κ > 0 (independent of z) such that"
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.19921875,"∥x(z, p) −x(z)∥≤κ∥Ax(z, p) −b∥,
whenever ∥Ax(z, p) −b∥≤δ."
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.20052083333333334,"Equipped with this lemma,
we are able to upper bound ∥x(zk, Tp(wk)) −x(zk)∥by
∥Ax(zk, Tp(wk)) −b∥locally in order to establish the descent property on the Lyapunov func-
tion. We will discuss this descent property in the next section. Now, let us provide the conditions on
problem (P) in order to ensure the local uniform metric subregularity assumed in Lemma 3.2.
Lemma 3.3 (sufﬁcient condition for local uniform metric subregularity). Consider problem (P).
Suppose that ∇f(x) is piecewise afﬁne and ∂g(x) is polyhedral multifunction and X is a polyhedral
set, then ∂Lz
γ(x, p) deﬁned in (13) satisﬁes the local uniform metric subregularity property.
Remark 3.2 (generality of local uniform metric subregularity). This sufﬁcient condition ensures
that ∂Lz
γ(x, p) must satisfy the local uniform metric subregularity property whenever f is quadratic
(can be nonconvex, i.e., its Hessian is not required to be positive semideﬁnite), g can be written as
∥· ∥1 −ρg"
UNIFORM METRIC SUBREGULARITY AND ITS CONSEQUENCE,0.20182291666666666,"2 ∥· ∥2, ℓ2-norm, ℓ∞-norm structure, and X is a polyhedron, which covers all the two
motivating applications listed in Section 1.
Remark 3.3 (comparison to the error bound condition utilized in (Zhang & Luo, 2020b)). Recall
that the work (Zhang & Luo, 2020b) considers problem (P) with g ≡0. They use the so-called dual
error bound condition (see (Hong & Luo, 2017)) instead of the local uniform metric subregularity
property. However, we argue that it is far form obvious how to extend their technique to cover
our nonsmooth nonconvex case, as it is not clear whether one can establish the dual error bound
condition if g is not null.
Remark 3.4 (further comments on the surrogate stationarity measure Φ(w) deﬁned in Proposi-
tion 3.1). It is interesting that the sufﬁcient condition in Lemma 3.3 also ensures that ∂L+(w)
satisﬁes the local metric subregularity when Φ(w) ≤ε with sufﬁcient small ε (Robinson, 1981),
i.e., dist(w, W) ≤κ′ dist(0, ∂L+(w)) for some κ′ > 0, where W is the set of stationary points of
(P+). Then, we have dist(w, W) ≤(1/c + κ′)ε which follows from dist(T(w), W) ≤κ′ε (due to
Φ(w) ≤ε) and the triangle inequality."
CONVERGENCE AND ITERATION COMPLEXITY OF N-RPDC,0.203125,"4
CONVERGENCE AND ITERATION COMPLEXITY OF N-RPDC"
CONVERGENCE AND ITERATION COMPLEXITY OF N-RPDC,0.20442708333333334,"Equipped with all the machineries developed in Section 3, we are now ready to establish the conver-
gence properties of N-RPDC."
DESCENT PROPERTY ON THE LYAPNOV FUNCTION,0.20572916666666666,"4.1
DESCENT PROPERTY ON THE LYAPNOV FUNCTION"
DESCENT PROPERTY ON THE LYAPNOV FUNCTION,0.20703125,"Upon invoking Lemma 3.2 in Lemma 3.1, we can derive the descent property in expectation on the
Lyapnov Function."
DESCENT PROPERTY ON THE LYAPNOV FUNCTION,0.20833333333333334,Under review as a conference paper at ICLR 2022
DESCENT PROPERTY ON THE LYAPNOV FUNCTION,0.20963541666666666,"Lemma 4.1 (expected sufﬁcient decrease for Λk). Suppose that Assumptions of Lemma 3.1 hold.
If the set-valued mapping ∂Lz
γ(x, p) deﬁned in (13) satisﬁes the local uniform metric subregularity
property (see Deﬁnition 3.1). Let λ = min{
δη
2σM 2+1,
η
σκ2+1}, the parameter αz satisfy 0 < αz <"
DESCENT PROPERTY ON THE LYAPNOV FUNCTION,0.2109375,"1/
h
2σ + σ"
DESCENT PROPERTY ON THE LYAPNOV FUNCTION,0.21223958333333334,"λ +
3σ2
σ−Lf −ρg + 1
i
, and c1 = min
n
1,
N
2η(σκ2+2)+1
o
. Then, we have"
DESCENT PROPERTY ON THE LYAPNOV FUNCTION,0.21354166666666666,Λ(wk) −Ei(k)Λ(wk+1) ≥c1
DESCENT PROPERTY ON THE LYAPNOV FUNCTION,0.21484375,N ∥wk −T(wk)∥2.
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.21614583333333334,"4.2
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY"
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.21744791666666666,"In this subsection, we establish the almost sure (i.e., with probability 1) convergence results for
N-RPDC."
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.21875,"Theorem 4.1 (almost sure convergence result). Under the setting of Lemma 4.1, then"
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.22005208333333334,(a) limk→+∞Φ(wk) = 0 almost surely.
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.22135416666666666,(b) The sequence {wk} is almost surely bounded.
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.22265625,(c) Any cluster point of the sequence {wk} is almost surely a stationary point of (P).
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.22395833333333334,"In addition, we can also provide the almost surely asymptotic rate of convergence in the sense of
limit inferior for our N-RPDC method."
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.22526041666666666,"Theorem 4.2 (almost sure O(1/
√"
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.2265625,"k) convergence rate). Under the setting of Lemma 4.1, we have"
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.22786458333333334,"lim inf
k→+∞ √"
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.22916666666666666,"k + 1 · Φ(wk) = 0
almost surely"
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.23046875,"i.e., the asymptotic O(1/
√"
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.23177083333333334,k) convergence rate holds almost surely.
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.23307291666666666,"The almost sure convergence rate established above is of asymptotic nature, which concerns the
behavior of N-RPDC when k →∞. Next, we also provide the iteration complexity results of
N-RPDC in expectation."
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.234375,"Theorem 4.3 (expected iteration complexity). Under the setting of Lemma 4.1. The sequence {wk}
is generated by N-RPDC. Then"
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.23567708333333334,"min
0≤k≤t EFtΦ(wk) ≤ s"
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.23697916666666666,Nc2(Λ(w0) −F ∗)
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.23828125,"c1(t + 1)
."
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.23958333333333334,"Consequently, to achieve EFtΦ(w¯t) ≤ε for some 0 ≤¯t ≤t, N-RPDC needs at most t =
Nc2(Λ(w0)−F ∗)"
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.24088541666666666,"c1ε2
number of iterations."
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.2421875,"Remark 4.1. Note that the closely related work (Zhang & Luo, 2020b) provides a similar itera-
tion complexity result (i.e., O(ε2)) if g ≡0. By contrast, our results can deal with the additional
nonsmooth nonconvex term g. In addition, we also provide the almost sure convergence results,
complementing the results in (Zhang & Luo, 2020b). Let us also emphasize that the main moti-
vation of our algorithm lies in that N-RPDC is a randomized coordinate method, which is more
suitable for modern large-scale problems."
ALMOST SURE CONVERGENCE RESULTS AND EXPECTED ITERATION COMPLEXITY,0.24348958333333334,"Remark 4.2. The recent work (Zhang & Luo, 2020a) establishes a global dual error bound condi-
tion, which allows them to avoid the compactness assumption on X. Due to the existence of g, we
utilize the local uniform metric subregularity rather than this dual error bound condition. To further
relax the compactness assumption on X in problem (P), one possible direction is to establish a global
version of the utilized local uniform metric subregularity. We leave this direction as a future work."
NUMERICAL EXPERIMENTS,0.24479166666666666,"5
NUMERICAL EXPERIMENTS"
NUMERICAL EXPERIMENTS,0.24609375,"Because of the limitation of space, we put the numerical experiments in Appendix A.1. We can
observe that N-RPDC with larger N slightly outperforms that with smaller N in terms of conver-
gence speed. We also compared our N-RPDC to the algorithm proposed in (Zhang & Luo, 2020b);
see Figure 1. We can observe that N-RPDC slightly outperforms their algorithm. Note that the
main motivation of N-RPDC is that it is more suitable for modern large-scale problems due to its
reasonably fast convergence speed and low computational complexity at each iteration."
NUMERICAL EXPERIMENTS,0.24739583333333334,Under review as a conference paper at ICLR 2022
REFERENCES,0.24869791666666666,REFERENCES
REFERENCES,0.25,"Ahmet Alacaoglu, Olivier Fercoq, and Volkan Cevher. Random extrapolation for primal-dual coor-
dinate descent. In International conference on machine learning, pp. 191–201. PMLR, 2020."
REFERENCES,0.2513020833333333,"D.P. Bertsekas. Convexiﬁcation procedures and decomposition methods for nonconvex optimization
problems. Journal of Optimization Theory and Applications, 29(2):169–197, 1979."
REFERENCES,0.2526041666666667,"Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal
margin classiﬁers. In Proceedings of the ﬁfth annual workshop on Computational learning theory,
pp. 144–152. ACM, 1992."
REFERENCES,0.25390625,"Radu Ioan Bot¸ and Dang-Khoa Nguyen. The proximal alternating direction method of multipliers
in the nonconvex setting: convergence analysis and rates. Mathematics of Operations Research,
45(2):682–712, 2020."
REFERENCES,0.2552083333333333,"P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with
applications to biological feature selection. The annals of applied statistics, 5(1):232, 2011."
REFERENCES,0.2565104166666667,"P. Breheny and J. Huang. Group descent algorithms for nonconvex penalized linear and logistic
regression models with grouped predictors. Statistics and computing, 25(2):173–187, 2015."
REFERENCES,0.2578125,"Pierre Carpentier, Guy Cohen, Jean-Philippe Chancelier, and Michel De Lara. Stochastic multi-
stage optimization. At the Crossroads between Discrete Time Stochastic Control and Stochastic
Programming Springer-Verlag, Berlin, 2015."
REFERENCES,0.2591145833333333,"Guy Cohen and Daoli Zhu. Decomposition and coordination methods in large scale optimization
problems: The nondifferentiable case and the use of augmented lagrangians. Adv. in Large Scale
Systems, 1:203–266, 1984."
REFERENCES,0.2604166666666667,"Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273–297,
1995."
REFERENCES,0.26171875,"Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex
functions. SIAM Journal on Optimization, 29(1):207–239, 2019."
REFERENCES,0.2630208333333333,"D. DeCoste and B. Sch¨olkopf. Training invariant support vector machines. Machine learning, 46
(1):161–190, 2002."
REFERENCES,0.2643229166666667,"Z. Deng, M. C. Yue, and A. M. C. So. An efﬁcient augmented lagrangian-based method for linear
equality-constrained lasso. In ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 5760–5764. IEEE, 2020, May."
REFERENCES,0.265625,"Marie Duﬂo. Random iterative models, volume 34. Springer Science & Business Media, 2013."
REFERENCES,0.2669270833333333,"I. Ekeland. On the variational principle. Journal of Mathematical Analysis and Applications, 47(2):
324–353, 1974."
REFERENCES,0.2682291666666667,"Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle
properties. Journal of the American statistical Association, 96(456):1348–1360, 2001."
REFERENCES,0.26953125,"Olivier Fercoq and Pascal Bianchi. A coordinate-descent primal-dual algorithm with large step size
and possibly nonseparable functions. SIAM Journal on Optimization, 29(1):100–134, 2019."
REFERENCES,0.2708333333333333,"B. R. Gaines, J. Kim, and H. Zhou. Algorithms for ﬁtting the constrained lasso. Journal of Compu-
tational and Graphical Statistics, 27(4):861–871, 2018."
REFERENCES,0.2721354166666667,"Xiang Gao, Yang-Yang Xu, and Shu-Zhong Zhang. Randomized primal–dual proximal block coor-
dinate updates. Journal of the Operations Research Society of China, 7(2):205–250, 2019."
REFERENCES,0.2734375,"B. Haasdonk and D. Keysers. Tangent distance kernels for support vector machines. In Object
recognition supported by user interaction for service robots, volume 2, pp. 864–868. IEEE, 2002,
August."
REFERENCES,0.2747395833333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.2760416666666667,"M. Hong and Z. Q. Luo. On the linear convergence of the alternating direction method of multipliers.
Mathematical Programming, 162(1-2):165–199, 2017."
REFERENCES,0.27734375,"G. M. James, C. Paulson, and P. Rusmevichientong. Penalized and constrained regression. Unpub-
lished manuscript, 2013."
REFERENCES,0.2786458333333333,"Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-łojasiewicz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pp. 795–811. Springer, 2016."
REFERENCES,0.2799479166666667,"A. Y. Kruger and N. Duy Cuong. Uniform regularity of set-valued mappings and stability of implicit
multifunctions. Journal of Nonsmooth Analysis and Optimization, 2, 2021."
REFERENCES,0.28125,"Puya Latafat, Nikolaos M Freris, and Panagiotis Patrinos. A new randomized block-coordinate
primal-dual proximal algorithm for distributed optimization. IEEE Transactions on Automatic
Control, 64(10):4050–4065, 2019."
REFERENCES,0.2825520833333333,"Xiao Li, Shixiang Chen, Zengde Deng, Qing Qu, Zhihui Zhu, and Anthony Man-Cho So. Weakly
convex optimization over stiefel manifold using riemannian subgradient-type methods. SIAM
Journal on Optimization, 31(3):1605–1634, 2021."
REFERENCES,0.2838541666666667,"H. T. Lin and C. J. Lin. A study on sigmoid kernels for svm and the training of non-psd kernels by
smo-type methods. Unpublished manuscript, 2003."
REFERENCES,0.28515625,"Ji Liu and Stephen J Wright. Asynchronous stochastic coordinate descent: Parallelism and conver-
gence properties. SIAM Journal on Optimization, 25(1):351–376, 2015."
REFERENCES,0.2864583333333333,"Ji Liu, Steve Wright, Christopher R´e, Victor Bittorf, and Srikrishna Sridhar. An asynchronous par-
allel stochastic coordinate descent algorithm. In International Conference on Machine Learning,
pp. 469–477, 2014."
REFERENCES,0.2877604166666667,"Zhaosong Lu and Lin Xiao. On the complexity analysis of randomized block-coordinate descent
methods. Mathematical Programming, 152(1-2):615–642, 2015."
REFERENCES,0.2890625,"Yu Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM
Journal on Optimization, 22(2):341–362, 2012."
REFERENCES,0.2903645833333333,"Yu Nesterov. Subgradient methods for huge-scale optimization problems. Mathematical Program-
ming, 146(1-2):275–297, 2014."
REFERENCES,0.2916666666666667,"Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.29296875,"Andrei Patrascu and Ion Necoara. Efﬁcient random coordinate descent algorithms for large-scale
structured nonconvex optimization. Journal of Global Optimization, 61(1):19–46, 2015."
REFERENCES,0.2942708333333333,"A. Rakotomamonjy, G. Gasso, and J. Salmon. Screening rules for lasso with non-convex sparse
regularizers. In International Conference on Machine Learning, pp. 5341–5350. PMLR, 2019."
REFERENCES,0.2955729166666667,"Peter Richt´arik and Martin Tak´aˇc. Iteration complexity of randomized block-coordinate descent
methods for minimizing a composite function. Mathematical Programming, 144(1-2):1–38, 2014."
REFERENCES,0.296875,"Herbert Robbins and David Siegmund.
A convergence theorem for non negative almost super-
martingales and some applications. In Optimizing methods in statistics, pp. 233–257. Elsevier,
1971."
REFERENCES,0.2981770833333333,"S. M. Robinson. Some continuity properties of polyhedral multifunctions. Mathematical Program-
ming at Oberwolfach, pp. 206–214, 1981."
REFERENCES,0.2994791666666667,"Jean-Philippe Vial. Strong and weak convexity of sets and functions. Mathematics of Operations
Research, 8(2):231–259, 1983."
REFERENCES,0.30078125,"H. Wang, A. Banerjee, and Z. Q. Luo. Parallel direction method of multipliers. arXiv preprint
arXiv:1406.4064., 2014."
REFERENCES,0.3020833333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.3033854166666667,"J. H. Won, J. Xu, and K. Lange. Projection onto minkowski sums with application to constrained
learning. In International Conference on Machine Learning, pp. 3642–3651. PMLR, 2019."
REFERENCES,0.3046875,"Stephen J Wright. Coordinate descent algorithms. Mathematical Programming, 151(1):3–34, 2015."
REFERENCES,0.3059895833333333,"Yangyang Xu and Shuzhong Zhang. Accelerated primal–dual proximal block coordinate updating
methods for constrained convex optimization. Computational Optimization and Applications, 70
(1):91–128, 2018."
REFERENCES,0.3072916666666667,"Cun-Hui Zhang et al. Nearly unbiased variable selection under minimax concave penalty. The
Annals of statistics, 38(2):894–942, 2010."
REFERENCES,0.30859375,"J. Zhang and Z. Luo.
A global dual error bound and its application to the analysis of linearly
constrained nonconvex optimization. arXiv preprint arXiv:2006.16440, 2020a."
REFERENCES,0.3098958333333333,"J. Zhang and Z. Q. Luo. A proximal alternating direction method of multiplier for linearly con-
strained nonconvex minimization. SIAM journal on Optimization, 30(3):2272–2302, 2020b."
REFERENCES,0.3111979166666667,"L. Zhao and D. Zhu. First-order primal-dual method for nonlinear convex cone programs. arXiv
preprint arXiv:1801.00261v5, 2019."
REFERENCES,0.3125,"L. Zhao and D. Zhu. On iteration complexity of a ﬁrst-order primal-dual method for nonlinear
convex cone programming. Journal of the Operations Research Society of China, pp. 1–35, 2021."
REFERENCES,0.3138020833333333,"D. Zhu, L. Zhao, and S. Zhang. A ﬁrst-order primal-dual method for nonconvex constrained opti-
mization based on the augmented lagrangian. arXiv preprint arXiv:2007.12219, 2020."
REFERENCES,0.3151041666666667,"Daoli Zhu and Lei Zhao. Linear convergence of randomized primal-dual coordinate method for
large-scale linear constrained convex programming. In International Conference on Machine
Learning, pp. 11619–11628. PMLR, 2020."
REFERENCES,0.31640625,"A
APPENDIX"
REFERENCES,0.3177083333333333,"The Appendix is organized as follows. Subsection A.1 discusses implementation details for N-
RPDC and presents numerical experiments on non-PSD kernel SVM and sum to zero constrained
LASSO problem with SCAD penalty. Some properties of the auxiliary problem (P+) are given in
subsection A.2. Some examples of Bregman distance function are given in A.3. Subsection A.4
shows the proof of Proposition 3.1. Subsection A.5 provides the proof of Lemma 3.1. Proof of
Lemma 3.2 is given in subsection A.6. Subsection A.7 provides the proof of Lemma 3.3. Sub-
section A.8 shows the proof of Lemma 4.1. Subsection A.9 provides the proof of Theorem 4.1.
Subsection A.10 gives the proof of Theorem 4.2. The proof of Theorem 4.3 is given in subsec-
tion A.11."
REFERENCES,0.3190104166666667,"A.1
NUMERICAL EXAMPLES"
REFERENCES,0.3203125,"This subsection discusses experiments conducted using MATLAB R2020a on a personal computer
with Intel Core i5-6200U CPU (2.40GHz) and 8.00 GB RAM."
REFERENCES,0.3216145833333333,"A.1.1
NON-PSD KERNEL SUPPORT VECTOR MACHINE PROBLEM"
REFERENCES,0.3229166666666667,"Consider the non-PSD kernel support vector machine problem,"
REFERENCES,0.32421875,"min
x∈[0,c]d
1
2x⊤Qx −1⊤
d x"
REFERENCES,0.3255208333333333,"s.t.
y⊤x = 0
,"
REFERENCES,0.3268229166666667,"where x ∈Rd are the decision variables, and Q ∈Rd×d is a matrix, possibly non-PSD. Let Q =
(Q⊤
1 , Q⊤
2 , · · · , Q⊤
N)⊤∈Rd×d be an appropriate partition of matrix Q and Qi be an di × d matrix.
L+
γ of Non-PSD kernel SVM can be written as"
REFERENCES,0.328125,"L+
γ (w) = L+
γ (x, z, p) = 1"
REFERENCES,0.3294270833333333,"2x⊤Qx −1⊤
d x + σ"
REFERENCES,0.3307291666666667,"2 ∥x −z∥2 + ⟨p, y⊤x⟩+ γ"
REFERENCES,0.33203125,2 ∥y⊤x∥2.
REFERENCES,0.3333333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.3346354166666667,"0
200
400
600
0 5 10 15 20"
REFERENCES,0.3359375,"25
heart_scale"
REFERENCES,0.3372395833333333,"nADMM
N=10
N=90"
REFERENCES,0.3385416666666667,"0
200
400
600
0 10 20"
IONOSPHERE,0.33984375,"30
ionosphere"
IONOSPHERE,0.3411458333333333,"nADMM
N=10
N=70"
IONOSPHERE,0.3424479166666667,"Figure 1: Evolution of min0≤k≤t Φ(wk) versus epoch counts for several different choices of N (i.e.,
the number of blocks in N-RPDC), where nADMM refers to the algorithm proposed in (Zhang &
Luo, 2020b). Left: dataset heart scale. Right: dataset ionosphere. The results are obtained
by averaging 50 independent trials."
IONOSPHERE,0.34375,"Thanks of Proposition 3.3, for problem Non-PSD kernel SVM, ∂Lz
γ is locally metric subregular in
(x, z, p) uniformly in z at point 0. N-RPDC with D(x, xk) =
1
2∥x −xk∥2 for non-PSD kernel
support vector machine problem is"
IONOSPHERE,0.3450520833333333,"pk+1 ←pk + ηy⊤xk;
Choose i(k) from {1, 2, . . . , N} with equal probability"
IONOSPHERE,0.3463541666666667,"xk+1 ←
min
x∈[0,c]d⟨Qi(k)xk + σ(xk −zk)i(k) −1di(k) + yi(k)
 
pk+1 + γy⊤xk
, xi(k)⟩ + 1"
IONOSPHERE,0.34765625,"2αx
∥x −xk∥2;"
IONOSPHERE,0.3489583333333333,"zk+1 ←min
z∈Rd⟨σ(zk −xk)i(k), zi(k)⟩+
1
2αz
∥z −zk∥2."
IONOSPHERE,0.3502604166666667,"Thus, the primal subproblem of N-RPDC has the closed form

"
IONOSPHERE,0.3515625,"
xk+1
i(k) = Π[0,c]
di(k)"
IONOSPHERE,0.3528645833333333,"
xk
i(k) −αx"
IONOSPHERE,0.3541666666666667,"
Qi(k)xk + σ(xk −zk)i(k) −1ni(k) + (pk + γy⊤xk)yi(k) 
,"
IONOSPHERE,0.35546875,"xk+1
j̸=i(k) = xk
j̸=i(k);"
IONOSPHERE,0.3567708333333333,"and
(
zk+1
i(k) = zk
i(k) −αzσ(zk −xk)i(k),
zk+1
j̸=i(k) = zk
j̸=i(k)."
IONOSPHERE,0.3580729166666667,"We used LIBSVM dataset heart scale and ionosphere in the experiment. Q was generated
using the sigmoid kernel (Lin & Lin, 2003), and we selected c = 1. We compared two algorithms:
nonconvex ADMM in (Zhang & Luo, 2020b) (nADMM), and N-RPDC from this paper on these
two datasets."
IONOSPHERE,0.359375,"For N-RPDC algorithm, we partitioned the variables N = 10, 90 blocks for heart scale and
N = 10, 70 blocks for ionosphere."
IONOSPHERE,0.3606770833333333,"In order to verify the theoretical results of this paper, we compute the sequence of reference point
{T(wk)} along with the sequence {wk} generated by N-RPDC. In the real world applications of
N-RPDC, the reference point sequence is not necessary to compute. In Figure 1, the left graph
show the number of blocks and min0≤k≤t Φ(wk) with respect to epoch count for heart scale;
the right graph show the number of blocks and min0≤k≤t Φ(wk) with respect to epoch count for
ionosphere. For both datasets, min0≤k≤t Φ(wk) of each epoch of N-RPDC is the average value
of 50 times result. Moreover, we can draw the conclusion that N-RPDC is slightly outperforms
nADMM and larger N is slightly outperforms small N."
IONOSPHERE,0.3619791666666667,Under review as a conference paper at ICLR 2022
IONOSPHERE,0.36328125,"A.1.2
SUM TO ZERO CONSTRAINED LASSO PROBLEM WITH SCAD PENALTY"
IONOSPHERE,0.3645833333333333,Consider the sum to zero constrained LASSO problem with SCAD penalty:
IONOSPHERE,0.3658854166666667,"(CLASSO-SCAD)
min
x∈[−1,1]d
1
2∥Ax −b∥2 +
nP"
IONOSPHERE,0.3671875,"i=1
φ(xi)"
IONOSPHERE,0.3684895833333333,"s.t.
1⊤
n x = 0;
,"
IONOSPHERE,0.3697916666666667,"where x ∈Rd is the decision variables and A ∈Rn×d is the regressors matrix.
Let A =
(A1, A2, · · · , AN) ∈Rn×d be an appropriate partition of matrix A and Ai be an n × di matrix.
b ∈Rn is the response vector. 1d is an d-dimensional vector of 1s. φ(·) is the SCAD penalty:"
IONOSPHERE,0.37109375,"φ(|t|) = 
 "
IONOSPHERE,0.3723958333333333,"|t|
|t| ≤1
−t2+2θ|t|−1"
IONOSPHERE,0.3736979166666667,"2(θ−1)
1 < |t| ≤θ
1+θ"
IONOSPHERE,0.375,"2
|t| > θ.
,
t ∈R"
IONOSPHERE,0.3763020833333333,"L+
γ of (CLASSO-SCAD) can be written as"
IONOSPHERE,0.3776041666666667,"L+
γ (w) = L+
γ (x, z, p) = 1"
IONOSPHERE,0.37890625,"2∥Ax −b∥2 + n
X"
IONOSPHERE,0.3802083333333333,"i=1
φ(xi) + σ"
IONOSPHERE,0.3815104166666667,"2 ∥x −z∥2 + ⟨p, 1⊤
n x⟩+ γ"
IONOSPHERE,0.3828125,"2 ∥1⊤
n x∥2."
IONOSPHERE,0.3841145833333333,"Thanks of Proposition 3.3, for problem (CLASSO-SCAD), ∂Lz
γ is locally metric subregular in
(x, z, p) uniformly in z at point 0. We take D(x, xk) = 1"
IONOSPHERE,0.3854166666666667,"2∥x −xk∥2 and construct N-RPDC al-
gorithm for (CLASSO-SCAD) as following:"
IONOSPHERE,0.38671875,"pk+1 ←pk + η(1⊤
d xk),
Choose i(k) from {1, 2, . . . , N} with equal probability"
IONOSPHERE,0.3880208333333333,"xk+1 ←
min
x∈[−1,1]d⟨A⊤
i(k)(Axk −b) + σ(xk −zk)i(k) + 1di(qk), xi(k)⟩ +"
IONOSPHERE,0.3893229166666667,"Pi(k)
j=1 dj
X"
IONOSPHERE,0.390625,"i=1+Pi(k)−1
j=1
dj"
IONOSPHERE,0.3919270833333333,"φ(xi) +
1
2αx
∥x −xk∥2;"
IONOSPHERE,0.3932291666666667,"zk+1 ←min
z∈Rd⟨σ(zk −xk)i(k), zi(k)⟩+
1
2αz
∥z −zk∥2;"
IONOSPHERE,0.39453125,"where qk = pk+1 + γ(1⊤
d xk). Thus, the primal subproblem of RPDC has the closed form




"
IONOSPHERE,0.3958333333333333,"


"
IONOSPHERE,0.3971354166666667,"xk+1
i(k) = 

 
"
IONOSPHERE,0.3984375,"Π[−1,1]
di(k){sign(ζk) ⊙max(0, |ζk| −µ1di(k))}
|ζk| ≤1 + µ
Π[−1,1]
di(k){
1
θ−1−µ[(θ −1)ζk −µθsign(ζk)]}
1 + µ < |ζk| ≤θ
Π[−1,1]
di(k){ζk}
|ζk| > θ
xk+1
j̸=i(k) = xk
j̸=i(k);"
IONOSPHERE,0.3997395833333333,"with ζk = xk
i(k) −αx[A⊤
i(k)(Axk −b) + σ(xk −zk)i(k) + qk1ni(k)] and µ = αx.
(
zk+1
i(k) = zk
i(k) −αzσ(zk −xk)i(k);
zk+1
j̸=i(k) = zk
j̸=i(k)."
IONOSPHERE,0.4010416666666667,"In this experiment, the elements of A ∈Rn×d are selected i.i.d. from a Gaussian N(0, 1) distribu-
tion. To construct a sparse true solution x∗∈Rd, given the dimension d and sparsity s, we select
s entries of x∗at random to be nonzero and N(0, 1) normally distributed, and set the rest to zero.
The measurement vector b ∈Rn is obtained by b = Ax∗+ δ with the elements of the noise vector
δ ∈Rn are i.i.d. N(0, 0.001). We choose θ = 2.3 with n = 360, d = 1280, s = 8 for the ﬁrst
dataset and n = 720, d = 2560, s = 16 for the second dataset."
IONOSPHERE,0.40234375,"We partitioned the variables N = 10, 40, 80 blocks. Thus di = 128, 32, 16 for the ﬁrst dataset and
di = 256, 64, 32 for the second dataset."
IONOSPHERE,0.4036458333333333,"In order to verify the theoretical results of this paper, we compute the sequence of reference point
{T(wk)} along with the sequence {wk} generated by N-RPDC. In the real world applications of
N-RPDC, the reference point sequence is not necessary to compute. In Figure 2, the left graph"
IONOSPHERE,0.4049479166666667,Under review as a conference paper at ICLR 2022
IONOSPHERE,0.40625,"0
50
100
0 0.02 0.04 0.06 0.08"
IONOSPHERE,0.4075520833333333,"0.1
d=1280,n=360,s=8"
IONOSPHERE,0.4088541666666667,"N=10
N=40
N=80"
IONOSPHERE,0.41015625,"0
50
100
0 0.1 0.2 0.3 0.4"
IONOSPHERE,0.4114583333333333,"0.5
d=1280,n=360,s=8"
IONOSPHERE,0.4127604166666667,"N=10
N=40
N=80"
IONOSPHERE,0.4140625,"Figure 2: Evolution of min0≤k≤t Φ(wk) versus epoch counts for several different choices of N (i.e.,
the number of blocks in N-RPDC) on synthetic dataset. Left: n = 360, d = 1280, s = 8. Right:
n = 720, d = 2560, s = 16. The results are obtained by averaging 50 independent trials."
IONOSPHERE,0.4153645833333333,"show the number of blocks and min
0≤k≤t Φ(wk) with respect to epoch count for the ﬁrst dataset; the"
IONOSPHERE,0.4166666666666667,"right graph show the number of blocks and min
0≤k≤t Φ(wk) with respect to epoch count for the second"
IONOSPHERE,0.41796875,"dataset. For both datasets, min
0≤k≤t Φ(wk) for each epoch of N-RPDC is the average value of 50 times"
IONOSPHERE,0.4192708333333333,"result. Additionally, we can draw the conclusion that larger N is slightly outperforms small N."
IONOSPHERE,0.4205729166666667,"A.2
SOME PROPERTIES OF THE AUXILIARY PROBLEM P+"
IONOSPHERE,0.421875,"The auxiliary problem can be viewed as a quadratically regularized version of the original problem
(P) with an additional variable z. Recall that f has Lf-Lipschitz gradient and g is ρg-weakly convex.
We have the following benign properties of the auxiliary problem (P+):"
IONOSPHERE,0.4231770833333333,(1) f(x) + σ
IONOSPHERE,0.4244791666666667,"2 ∥x −z∥2 is continuously differentiable in (x, z) with Lipschitz continuous gradient;"
IONOSPHERE,0.42578125,"(2) F +(x, z) = f(x) + g(x) + σ"
IONOSPHERE,0.4270833333333333,"2 ∥x −z∥2 is weakly convex in (x, z) with parameter Lf + ρg;"
IONOSPHERE,0.4283854166666667,"(3) F +(x, z) is bi-strongly convex, i.e., F +(x, z) is strongly convex in x for every ﬁxed z and
F +(x, z) is strongly convex in z for every ﬁxed x."
IONOSPHERE,0.4296875,"The ﬁrst property is trivially true. To see the second property, we note that the Lf-Lipschitz gradient
property of f implies that f(x) ≥f(y) + ⟨s, x −y⟩−Lf"
IONOSPHERE,0.4309895833333333,"2 ∥x −y∥2, ∀x, y ∈Rd (Nesterov, 2003,
Lemma 1.2.3). This, together with (Vial, 1983, Proposition 4.8), establishes that f is Lf-weakly
convex2. By (Vial, 1983, Proposition 4.1), we have that f + g is weakly convex with parameter
Lf + ρg. We have shown the second property y recognizing that (x, z) 7→σ"
IONOSPHERE,0.4322916666666667,"2 ∥x −z∥2 is a convex
mapping. The third property is a direct consequence of the deﬁnition of weak convexity, which
yields that f(x) + g(x) + σ"
IONOSPHERE,0.43359375,2 ∥x −z∥2 is σ −(Lf + ρg) strongly convex in x.
IONOSPHERE,0.4348958333333333,"A.3
SOME EXAMPLES OF BREGMAN DISTANCE FUNCTION"
IONOSPHERE,0.4361979166666667,"The proximal term D(x, xk) used in N-RPDC is the so-called Bregman distance function, which is
deﬁned as
D(x, xk) := K(x) −K(xk) −

∇K(xk), x −xk
."
IONOSPHERE,0.4375,"Here, K : Rd →R is the core function, which is βK-strongly convex and has LK-Lipschitz
continuous gradient on Rd. One popular choice of the core function is K(x) = 1"
IONOSPHERE,0.4388020833333333,"2∥x∥2, which yields
D(x, xk) = 1"
IONOSPHERE,0.4401041666666667,"2∥x −xk∥2. However, more generally, the core function can also be selected as the
Q-quadratic proximal regularization term K(x) = 1"
IONOSPHERE,0.44140625,"2∥x∥2
Q, where Q is a positive deﬁnite matrix.
This choice covers some second-order methods as we can set Q to be the regularized Hessian matrix."
IONOSPHERE,0.4427083333333333,"2Actually, this derivation veriﬁes the fact that any continuously differentiable function with L-Lipschitz
continuous gradient is L-weakly convex."
IONOSPHERE,0.4440104166666667,Under review as a conference paper at ICLR 2022
IONOSPHERE,0.4453125,"A.4
PROOF OF PROPOSITION 3.1"
IONOSPHERE,0.4466145833333333,Proof. The optimal condition of
IONOSPHERE,0.4479166666666667,"Tx(wk) = arg min
x∈X⟨∇f(xk) + σ(xk −zk), x⟩+ g(x) + ⟨Tp(wk) + γ(Axk −b), Ax⟩+ D(x,xk) αx"
IONOSPHERE,0.44921875,"and Tz(wk) = arg min
z∈Rd⟨σ(zk −xk), z⟩+
1
2αz ∥z −zk∥2 yields that"
IONOSPHERE,0.4505208333333333,0 ∈∇f(xk) + ∂g(Tx(wk)) + σ(xk −zk) + NX(Tx(wk)) + A⊤[Tp(wk) + γ(Axk −b)] + 1
IONOSPHERE,0.4518229166666667,"αx
[∇K(Tx(wk)) −∇K(xk)];"
IONOSPHERE,0.453125,0 = σ(zk −xk) + 1
IONOSPHERE,0.4544270833333333,"αz
(Tz(wk) −zk). Let"
IONOSPHERE,0.4557291666666667,"ξx
=
∇f(Tx(wk)) −∇f(xk) + σ(Tx(wk) −Tz(wk)) −σ(xk −zk) −γA⊤(Axk −b) −1"
IONOSPHERE,0.45703125,"αx
[∇K(Tx(wk)) −∇K(xk)]"
IONOSPHERE,0.4583333333333333,"=
∇f(Tx(wk)) −∇f(xk) + σ(Tx(wk) −Tz(wk)) −σ(xk −zk) −γ"
IONOSPHERE,0.4596354166666667,η A⊤(Tp(wk) −pk) −1
IONOSPHERE,0.4609375,"αx
[∇K(Tx(wk)) −∇K(xk)];
(15)"
IONOSPHERE,0.4622395833333333,"ξz
=
σ(Tz(wk) −Tx(wk)) −σ(zk −xk) −1"
IONOSPHERE,0.4635416666666667,"αz
(Tz(wk) −zk);
(16) and"
IONOSPHERE,0.46484375,ξp = 1
IONOSPHERE,0.4661458333333333,"η (Tp(wk) −pk) + A(Tx(wk) −xk).
(17)"
IONOSPHERE,0.4674479166666667,Then we have that
IONOSPHERE,0.46875,ξx ∈∇f(Tx(wk)) + ∂g(Tx(wk)) + σ(Tx(wk) −Tz(wk)) + NX(Tx(wk)) + A⊤Tp(wk);
IONOSPHERE,0.4700520833333333,ξz = σ(Tz(wk) −Tx(wk));
IONOSPHERE,0.4713541666666667,ξp = ATx(wk) −b;
IONOSPHERE,0.47265625,"By
the
expression
(15-17)
of
ξ
="
IONOSPHERE,0.4739583333333333,"ξx
ξz
ξp !"
IONOSPHERE,0.4752604166666667,",
αx,
αz
and
η
are
given
posi-"
IONOSPHERE,0.4765625,"tive
numbers
and
Assumptions
of
problem
(P),
there
exists
positive
number
c
≥
√"
"MAX
N
LK",0.4778645833333333,"3 max
n
LK"
"MAX
N
LK",0.4791666666666667,"αx + Lf + 2σ + ∥A∥, 1"
"MAX
N
LK",0.48046875,"αz + 2σ, γ∥A∥+1"
"MAX
N
LK",0.4817708333333333,"η
o
and the following inequality holds"
"MAX
N
LK",0.4830729166666667,∥ξ∥≤c∥wk −T(wk)∥.
"MAX
N
LK",0.484375,Combining ξ ∈∂L+(T(wk)) and it follow the desired statement.
"MAX
N
LK",0.4856770833333333,"A.5
PROOF OF LEMMA 3.1"
"MAX
N
LK",0.4869791666666667,"In order to provide the Lemma 3.1, we need the following two propositions as preparation."
"MAX
N
LK",0.48828125,"Proposition A.1 (Lipschitz properties of x(z) and x(z, p)). Suppose σ > Lf + ρg. Then"
"MAX
N
LK",0.4895833333333333,"(i) ∥x(z) −x(z′)∥≤
σ
σ−(Lf +ρg)∥z −z′∥;"
"MAX
N
LK",0.4908854166666667,"(ii) ∥x(z, p) −x(z′, p)∥≤
σ
σ−(Lf +ρg)∥z −z′∥."
"MAX
N
LK",0.4921875,Under review as a conference paper at ICLR 2022
"MAX
N
LK",0.4934895833333333,"Proof. (i) Let φ(x, z) = f(x) + g(x) + σ"
"MAX
N
LK",0.4947916666666667,"2 ∥x −z∥2 + I˜X(x) with ˜X = {x ∈X|Ax −b = 0}. Since
σ > Lf + ρg, we have φ(x, z) is σ −(Lf + ρg)-strongly convex in x. Therefore"
"MAX
N
LK",0.49609375,"φ (x(z), z′) −φ (x(z′), z′)
=
[φ (x(z), z′) −φ (x(z), z)] + [φ (x(z), z) −φ (x(z′), z)] + [φ (x(z′), z) −φ (x(z′), z′)] =
σ"
"MAX
N
LK",0.4973958333333333,"2 [∥x(z) −z′∥2 −∥x(z) −z∥2] + [φ (x(z), z) −φ (x(z′), z)] +σ"
"MAX
N
LK",0.4986979166666667,2 [∥x(z′) −z∥2 −∥x(z′) −z′∥2]
"MAX
N
LK",0.5,"=
[φ (x(z), z) −φ (x(z′), z)] + σ⟨z′ −z, x(z′) −x(z)⟩"
"MAX
N
LK",0.5013020833333334,"≤
−σ −(Lf + ρg)"
"MAX
N
LK",0.5026041666666666,"2
∥x(z) −x(z′)∥2 + σ⟨z′ −z, x(z′) −x(z)⟩"
"MAX
N
LK",0.50390625,"Again using the strongly convex of φ(x, z) in x, we have that"
"MAX
N
LK",0.5052083333333334,"φ (x(z), z′) −φ (x(z′), z′) ≥σ −(Lf + ρg)"
"MAX
N
LK",0.5065104166666666,"2
∥x(z) −x(z′)∥2."
"MAX
N
LK",0.5078125,Therefore
"MAX
N
LK",0.5091145833333334,"∥x(z) −x(z′)∥≤
σ
σ −(Lf + ρg)∥z −z′∥."
"MAX
N
LK",0.5104166666666666,"(ii) Let ϕ(x, z, p) = L+
γ (x, z, p) + IX(x). Since σ > Lf + ρg, function ϕ(x, z, p) also is σ −(Lf +
ρg)-strongly convex in x. Therefore"
"MAX
N
LK",0.51171875,"ϕ (x(z, p), z′, p) −ϕ (x(z′, p), z′, p)
=
[ϕ (x(z, p), z′, p) −ϕ (x(z, p), z, p)] + [ϕ (x(z, p), z, p) −ϕ (x(z′, p), z, p)]
+[ϕ (x(z′, p), z, p) −ϕ (x(z′, p), z′, p)] =
σ"
"MAX
N
LK",0.5130208333333334,"2 [∥x(z, p) −z′∥2 −∥x(z, p) −z∥2] + [ϕ (x(z, p), z, p) −ϕ (x(z′, p), z, p)] +σ"
"MAX
N
LK",0.5143229166666666,"2 [∥x(z′, p) −z∥2 −∥x(z′, p) −z′∥2]"
"MAX
N
LK",0.515625,"=
[ϕ (x(z, p), z, p) −ϕ (x(z′, p), z, p)] + σ⟨z′ −z, x(z′, p) −x(z, p)⟩"
"MAX
N
LK",0.5169270833333334,"≤
−σ −(Lf + ρg)"
"MAX
N
LK",0.5182291666666666,"2
∥x(z, p) −x(z′, p)∥2 + σ⟨z′ −z, x(z′, p) −x(z, p)⟩"
"MAX
N
LK",0.51953125,"Again using the strongly convex of ϕ(x, z, p) in x, we have that"
"MAX
N
LK",0.5208333333333334,"ϕ (x(z, p), z′, p) −ϕ (x(z′, p), z′, p) ≥σ −(Lf + ρg)"
"MAX
N
LK",0.5221354166666666,"2
∥x(z, p) −x(z′, p)∥2."
"MAX
N
LK",0.5234375,"Therefore, ∥x(z, p) −x(z′, p)∥≤
σ
σ−(Lf +ρg)∥z −z′∥."
"MAX
N
LK",0.5247395833333334,"Proposition A.2. Suppose σ > Lf + ρg. Let {wk} be generated by N-RPDC. For given wk, the
primal output (xk+1, zk+1) are random variables, and the following assertion hold:"
"MAX
N
LK",0.5260416666666666,"∥xk −x(zk, Tp(wk))∥≤"
"MAX
N
LK",0.52734375,Lf + σ + γ∥A∥2 + LK
"MAX
N
LK",0.5286458333333334,"αx
σ −Lf −ρg
+ 1 !"
"MAX
N
LK",0.5299479166666666,"∥xk −Tx(wk)∥.
(18)"
"MAX
N
LK",0.53125,"Proof. Since x(zk, Tp(wk)) = arg min
x∈X L+
γ (x, zk, Tp(wk)) and L+
γ (x, z, p) is σ −Lf −ρg-strongly"
"MAX
N
LK",0.5325520833333334,"convex in x, then ∀ζ ∈∂g(Tx(wk)) + NX(Tx(wk)) we have that"
"MAX
N
LK",0.5338541666666666,"(σ −Lf −ρg)∥Tx(wk) −x(zk, Tp(wk))∥2"
"MAX
N
LK",0.53515625,"≤
⟨∇f(Tx(wk)) + σ(Tx(wk) −zk) + ζ, Tx(wk) −x(zk, Tp(wk))⟩"
"MAX
N
LK",0.5364583333333334,"+⟨A⊤[Tp(wk) + γ(ATx(wk) −b)], Tx(wk) −x(zk, Tp(wk))⟩."
"MAX
N
LK",0.5377604166666666,It follows that
"MAX
N
LK",0.5390625,"(σ −Lf −ρg)∥Tx(wk) −x(zk, Tp(wk))∥"
"MAX
N
LK",0.5403645833333334,"≤
∥∇f(Tx(wk)) + σ(Tx(wk) −zk) + ζ + A⊤[Tp(wk) + γ(ATx(wk) −b)]∥."
"MAX
N
LK",0.5416666666666666,Again using the optimal condition of
"MAX
N
LK",0.54296875,Under review as a conference paper at ICLR 2022
"MAX
N
LK",0.5442708333333334,"Tx(wk) = arg min
x∈X⟨∇f(xk) + σ(xk −zk), x⟩+ g(x) + ⟨Tp(wk) + γ(Axk −b), Ax⟩+ D(x,xk) αx
,"
"MAX
N
LK",0.5455729166666666,we have that
"MAX
N
LK",0.546875,"0
∈
∇f(xk) + ∂g(Tx(wk)) + σ(xk −zk) + NX(Tx(wk)) + A⊤[Tp(wk) + γ(Axk −b)] + 1"
"MAX
N
LK",0.5481770833333334,"αx
[∇K(Tx(wk)) −∇K(xk)]."
"MAX
N
LK",0.5494791666666666,Therefore
"MAX
N
LK",0.55078125,"ξ
∈
∇f(Tx(wk)) + σ(Tx(wk) −zk) + ∂g(Tx(wk)) + NX(Tx(wk))"
"MAX
N
LK",0.5520833333333334,"+A⊤[Tp(wk) + γ(ATx(wk) −b)], with"
"MAX
N
LK",0.5533854166666666,"ξ = ∇f(Tx(wk)) −∇f(xk) + σ(Tx(wk) −xk) + γA⊤A(Tx(wk) −xk) −
1
αx [∇K(Tx(wk)) −
∇K(xk)]."
"MAX
N
LK",0.5546875,"By Assumptions on (P), we obtain that"
"MAX
N
LK",0.5559895833333334,∥ξ∥≤(Lf + σ + γ∥A∥2 + LK
"MAX
N
LK",0.5572916666666666,"αx
)∥xk −Tx(wk)∥, and"
"MAX
N
LK",0.55859375,"∥Tx(wk) −x(zk, Tp(wk))∥≤
Lf + σ + γ∥A∥2 + LK"
"MAX
N
LK",0.5598958333333334,"αx
σ −Lf −ρg
∥xk −Tx(wk)∥."
"MAX
N
LK",0.5611979166666666,"Then by the triangle inequality, above inequality yields the desired result."
"MAX
N
LK",0.5625,"With these two propositions in hands, we can provide the technical proof of Lemma 3.1."
"MAX
N
LK",0.5638020833333334,Proof of Lemma 3.1:
"MAX
N
LK",0.5651041666666666,"Step 1: Estimate the term L+
γ (wk) −Ei(k)L+
γ (wk+1).
By the dual update of N-RPDC, we have
that"
"MAX
N
LK",0.56640625,"L+
γ (xk, zk, pk) −L+
γ (xk, zk, pk+1) = ⟨pk −pk+1, Axk −b⟩= −1"
"MAX
N
LK",0.5677083333333334,"η ∥pk −pk+1∥2.
(19)"
"MAX
N
LK",0.5690104166666666,"Since xk+1 is the solution of subproblem of x in N-RPDC scheme, we have that"
"MAX
N
LK",0.5703125,"⟨∇i(k)f(xk) + σ(xk −zk)i(k), (x −xk+1)i(k)⟩+ gi(k)(xi(k)) −gi(k)(xk+1
i(k) )"
"MAX
N
LK",0.5716145833333334,"+⟨pk+1 + γ(Axk −b), Ai(k)(x −xk+1)i(k)⟩+ 1"
"MAX
N
LK",0.5729166666666666,"αx
D(x, xk) −1"
"MAX
N
LK",0.57421875,"αx
D(xk+1, xk) ≥0
(20)"
"MAX
N
LK",0.5755208333333334,"Take x = xk in (20), by the fact that gi(k)(xk
i(k)) −gi(k)(xk+1
i(k) ) = g(xk) −g(xk+1) and"
"MAX
N
LK",0.5768229166666666,"⟨∇i(k)f(xk) + σ(xk −zk)i(k) + (Ai(k))⊤[pk+1 + γ(Axk −b)], (xk −xk+1)i(k)⟩"
"MAX
N
LK",0.578125,"=
⟨∇f(xk) + σ(xk −zk) + A⊤[pk+1 + γ(Axk −b)], xk −xk+1⟩,"
"MAX
N
LK",0.5794270833333334,we have that
"MAX
N
LK",0.5807291666666666,"⟨∇f(xk) + σ(xk −zk), xk −xk+1⟩+ g(xk) −g(xk+1) + ⟨pk+1 + γ(Axk −b), A(xk −xk+1)⟩ −1"
"MAX
N
LK",0.58203125,"αx
D(xk+1, xk) ≥0"
"MAX
N
LK",0.5833333333333334,It yields that
"MAX
N
LK",0.5846354166666666,"F +(xk, zk) −F +(xk+1, zk) + ⟨pk+1 + γ(Axk −b), A(xk −xk+1)⟩"
"MAX
N
LK",0.5859375,"≥
1
αx
D(xk+1, xk) −[f(xk+1) −f(xk) −⟨∇f(xk), xk+1 −xk⟩] −σ"
"MAX
N
LK",0.5872395833333334,2 ∥xk −xk+1∥2
"MAX
N
LK",0.5885416666666666,"≥
βK −αx(Lf + σ)"
"MAX
N
LK",0.58984375,"2αx
∥xk −xk+1∥2"
"MAX
N
LK",0.5911458333333334,Under review as a conference paper at ICLR 2022
"MAX
N
LK",0.5924479166666666,"Since ⟨γ(Axk −b), A(xk −xk+1)⟩=
γ
2
 
∥Axk −b∥2 −∥Axk+1 −b∥2 + ∥A(xk −xk+1)∥2
,
therefore"
"MAX
N
LK",0.59375,"L+
γ (xk, zk, pk+1) −L+
γ (xk+1, zk, pk+1) ≥βK −αx(Lf + σ + γ∥A∥2)"
"MAX
N
LK",0.5950520833333334,"2αx
∥xk −xk+1∥2.
(21)"
"MAX
N
LK",0.5963541666666666,"Additionally, since zk+1 is the solution of subproblem of z in N-RPDC, we have that"
"MAX
N
LK",0.59765625,"⟨σ(zk −xk)i(k), (z −zk+1)i(k)⟩+ 1"
"MAX
N
LK",0.5989583333333334,"αz
⟨zk+1 −zk, z −zk+1⟩≥0.
(22)"
"MAX
N
LK",0.6002604166666666,"Take z = zk in (22), by the fact that ⟨σ(zk −xk)i(k), (zk −zk+1)i(k)⟩= ⟨σ(zk −xk), zk −zk+1⟩,
we have that"
"MAX
N
LK",0.6015625,"⟨σ(zk −xk), zk −zk+1⟩≥1"
"MAX
N
LK",0.6028645833333334,"αz
∥zk −zk+1∥2. Since"
"MAX
N
LK",0.6041666666666666,"⟨σ(zk −xk), zk −zk+1⟩
=
⟨σ(zk −xk+1), zk −zk+1⟩+ ⟨σ(xk+1 −xk), zk −zk+1⟩ =
σ"
"MAX
N
LK",0.60546875,"2
 
∥xk+1 −zk∥2 −∥xk+1 −zk+1∥2 + ∥zk −zk+1∥2"
"MAX
N
LK",0.6067708333333334,"+⟨σ(xk+1 −xk), zk −zk+1⟩ ≤
σ"
"MAX
N
LK",0.6080729166666666,"2
 
∥xk+1 −zk∥2 −∥xk+1 −zk+1∥2 + ∥zk −zk+1∥2 +σ"
"MAX
N
LK",0.609375,"2
 
∥xk −xk+1∥2 + ∥zk −zk+1∥2 =
σ"
"MAX
N
LK",0.6106770833333334,"2
 
∥xk+1 −zk∥2 −∥xk+1 −zk+1∥2
+ σ∥zk −zk+1∥2 +σ"
"MAX
N
LK",0.6119791666666666,"2 ∥xk −xk+1∥2,"
"MAX
N
LK",0.61328125,above inequality yields that
"MAX
N
LK",0.6145833333333334,"L+
γ (xk+1, zk, pk+1) −L+
γ (xk+1, zk+1, pk+1)
=
σ"
"MAX
N
LK",0.6158854166666666,"2
 
∥xk+1 −zk∥2 −∥xk+1 −zk+1∥2 ≥
( 1"
"MAX
N
LK",0.6171875,"αz
−σ)∥zk −zk+1∥2 −σ"
"MAX
N
LK",0.6184895833333334,2 ∥xk −xk+1∥2. (23)
"MAX
N
LK",0.6197916666666666,"By the combination of (19), (21) and (23), we obtain that"
"MAX
N
LK",0.62109375,"L+
γ (xk, zk, pk) −L+
γ (xk+1, zk+1, pk+1) ≥ βK"
"MAX
N
LK",0.6223958333333334,αx −(Lf + 2σ + γ∥A∥2)
"MAX
N
LK",0.6236979166666666,"2
∥xk −xk+1∥2 + ( 1"
"MAX
N
LK",0.625,"αz
−σ)∥zk −zk+1∥2 −1"
"MAX
N
LK",0.6263020833333334,η ∥pk −pk+1∥2.
"MAX
N
LK",0.6276041666666666,"Take expectation with respect to i(k) on both side of above inequality, we have that"
"MAX
N
LK",0.62890625,"L+
γ (xk, zk, pk) −Ei(k)L+
γ (xk+1, zk+1, pk+1) ≥ βK"
"MAX
N
LK",0.6302083333333334,αx −(Lf + 2σ + γ∥A∥2)
"MAX
N
LK",0.6315104166666666,"2
Ei(k)∥xk −xk+1∥2 + ( 1"
"MAX
N
LK",0.6328125,"αz
−σ)Ei(k)∥zk −zk+1∥2 −1"
"MAX
N
LK",0.6341145833333334,η ∥pk −pk+1∥2.
"MAX
N
LK",0.6354166666666666,By the fact that Ei(k)∥xk −xk+1∥2 = 1
"MAX
N
LK",0.63671875,"N ∥xk −Tx(wk)∥2, Ei(k)∥zk −zk+1∥2 = 1"
"MAX
N
LK",0.6380208333333334,N ∥zk −Tz(wk)∥2
"MAX
N
LK",0.6393229166666666,"and pk+1 = Tp(wk), above inequality follows that"
"MAX
N
LK",0.640625,"L+
γ (xk, zk, pk) −Ei(k)L+
γ (xk+1, zk+1, pk+1) ≥ βK"
"MAX
N
LK",0.6419270833333334,αx −(Lf + 2σ + γ∥A∥2)
N,0.6432291666666666,"2N
∥xk −Tx(wk)∥2 + 1 N ( 1"
N,0.64453125,"αz
−σ)∥zk −Tz(wk)∥2 −1"
N,0.6458333333333334,η ∥pk −Tp(wk)∥2.
N,0.6471354166666666,Under review as a conference paper at ICLR 2022
N,0.6484375,"Step 2: Estimate the term ν(zk) −ν(zk+1).
By the Danskin’s theorem, we have that ∇ν(z) =
σ (z −x(z)). By statement (i) of Proposition A.1, we have that"
N,0.6497395833333334,"∥∇ν(zk) −∇ν(zk+1)∥≤

σ2"
N,0.6510416666666666,"σ −Lf −ρg
+ σ

∥zk −zk+1∥."
N,0.65234375,The gradient Lipschitz property of ν follows that
N,0.6536458333333334,"ν(zk) −ν(zk+1) ≥σ⟨zk −x(zk), zk −zk+1⟩−

σ2"
N,0.6549479166666666,2(σ −Lf −ρg) + σ 2
N,0.65625,"
∥zk −zk+1∥2.
(24)"
N,0.6575520833333334,"Step
3:
Estimate
the
term
ψγ(zk+1, pk+1) −ψγ(zk, pk).
Since
x(zk, pk)
=
arg min
x∈X L+
γ (x, zk, pk), we have"
N,0.6588541666666666,"ψγ(zk+1, pk+1) −ψγ(zk, pk+1)"
N,0.66015625,"=
L+
γ (x(zk+1, pk+1), zk+1, pk+1) −L+
γ (x(zk, pk+1), zk, pk+1)"
N,0.6614583333333334,"≥
L+
γ (x(zk+1, pk+1), zk+1, pk+1) −L+
γ (x(zk+1, pk+1), zk, pk+1) =
σ"
N,0.6627604166666666,"2
 
∥x(zk+1, pk+1) −zk+1∥2 −∥x(zk+1, pk+1) −zk∥2"
N,0.6640625,"≥
σ⟨zk −x(zk+1, pk+1), zk+1 −zk⟩.
(25)"
N,0.6653645833333334,"By the concavity of ψγ(z, ·) and ∇ψγ(z, p) = Ax(z, p) −b, we have"
N,0.6666666666666666,"ψγ(zk, pk+1) −ψγ(zk, pk)
≥
⟨pk+1 −pk, Ax(zk, pk+1) −b⟩"
N,0.66796875,"=
1
2η ∥pk −pk+1∥2 + η"
N,0.6692708333333334,"2∥Ax(zk, pk+1) −b∥2 −1"
N,0.6705729166666666,"2η ∥pk+1 −pk −η[Ax(zk, pk+1) −b]∥2."
N,0.671875,"By the dual update of N-RPDC, the above inequality yields that"
N,0.6731770833333334,"ψγ(zk, pk+1) −ψγ(zk, pk)"
N,0.6744791666666666,"≥
1
2η ∥pk −pk+1∥2 + η"
N,0.67578125,"2∥Ax(zk, pk+1) −b∥2 −1"
N,0.6770833333333334,"2η ∥ηA[xk −x(zk, pk+1)]∥2"
N,0.6783854166666666,"≥
1
2η ∥pk −pk+1∥2 + η"
N,0.6796875,"2∥Ax(zk, pk+1) −b∥2 −η"
N,0.6809895833333334,"2∥A∥2 · ∥xk −x(zk, pk+1)∥2.
(26)"
N,0.6822916666666666,"By Proposition A.2, (26) guarantees that"
N,0.68359375,"ψγ(zk, pk+1) −ψγ(zk, pk)
≥
1
2η ∥pk −pk+1∥2 + η"
N,0.6848958333333334,"2∥Ax(zk, pk+1) −b∥2 −η"
N,0.6861979166666666,"2∥A∥2
 
Lf + σ + γ∥A∥2 + LK"
N,0.6875,"αx
σ −Lf −ρg
+ 1 !2"
N,0.6888020833333334,∥xk −Tx(wk)∥2. (27)
N,0.6901041666666666,"By the combination of (25) and (27), we obtain that"
N,0.69140625,"ψγ(zk+1, pk+1) −ψγ(zk, pk)"
N,0.6927083333333334,"≥
σ⟨zk −x(zk+1, pk+1), zk+1 −zk⟩+ 1"
N,0.6940104166666666,2η ∥pk −pk+1∥2 +η
N,0.6953125,"2∥Ax(zk, pk+1) −b∥2 −η"
N,0.6966145833333334,"2∥A∥2
 
Lf + σ + γ∥A∥2 + LK"
N,0.6979166666666666,"αx
σ −Lf −ρg
+ 1 !2"
N,0.69921875,∥xk −Tx(wk)∥2 (28)
N,0.7005208333333334,Under review as a conference paper at ICLR 2022
N,0.7018229166666666,"Step 4: Estimate the term [ν(zk) −ψγ(zk, pk)] −Ei(k)[ν(zk+1) −ψγ(zk+1, pk+1)].
Summing
statement (24) and (28), we obtain the variation of dual gap:"
N,0.703125,"[ν(zk) −ψγ(zk, pk)] −[ν(zk+1) −ψγ(zk+1, pk+1)]"
N,0.7044270833333334,"≥
σ⟨x(zk+1, pk+1) −x(zk), zk −zk+1⟩−

σ2"
N,0.7057291666666666,2(σ −Lf −ρg) + σ 2
N,0.70703125,"
∥zk −zk+1∥2 + 1"
N,0.7083333333333334,2η ∥pk −pk+1∥2 + η
N,0.7096354166666666,"2∥Ax(zk, pk+1) −b∥2 −η"
N,0.7109375,"2∥A∥2
 
Lf + σ + γ∥A∥2 + LK"
N,0.7122395833333334,"αx
σ −Lf −ρg
+ 1 !2"
N,0.7135416666666666,"∥xk −Tx(wk)∥2.
(29)"
N,0.71484375,"By Proposition A.1, we have that"
N,0.7161458333333334,"σ⟨x(zk+1, pk+1) −x(zk), zk −zk+1⟩"
N,0.7174479166666666,"=
σ[⟨x(zk, pk+1) −x(zk), zk −zk+1⟩+ ⟨x(zk+1, pk+1) −x(zk, pk+1), zk −zk+1⟩] ≥
−σ"
N,0.71875,2λ∥zk −zk+1∥2 −σλ
N,0.7200520833333334,"2 ∥x(zk, pk+1) −x(zk)∥2"
N,0.7213541666666666,"−σ∥zk −zk+1∥· ∥x(zk, pk+1) −x(zk+1, pk+1)∥"
N,0.72265625,"≥
−( σ"
N,0.7239583333333334,"2λ +
σ2"
N,0.7252604166666666,"σ −Lf −ρg
)∥zk −zk+1∥2 −σλ"
N,0.7265625,"2 ∥x(zk, pk+1) −x(zk)∥2,
(30)"
N,0.7278645833333334,"with λ > 0 is any positive number. By the combination of (29) and (30), we have that"
N,0.7291666666666666,"[ν(zk) −ψγ(zk, pk)] −[ν(zk+1) −ψγ(zk+1, pk+1)]"
N,0.73046875,"≥
−
 σ"
N,0.7317708333333334,"2λ +
3σ2"
N,0.7330729166666666,2(σ −Lf −ρg) + σ 2
N,0.734375,"
∥zk −zk+1∥2 −σλ"
N,0.7356770833333334,"2 ∥x(zk, pk+1) −x(zk)∥2 + 1"
N,0.7369791666666666,2η ∥pk −pk+1∥2 + η
N,0.73828125,"2∥Ax(zk, pk+1) −b∥2 −η"
N,0.7395833333333334,"2∥A∥2
 
Lf + σ + γ∥A∥2 + LK"
N,0.7408854166666666,"αx
σ −Lf −ρg
+ 1 !2"
N,0.7421875,"∥xk −Tx(wk)∥2.
(31)"
N,0.7434895833333334,"Take expectation with respect to i(k) on both side of above inequality, we have that"
N,0.7447916666666666,"[ν(zk) −ψγ(zk, pk)] −Ei(k)[ν(zk+1) −ψγ(zk+1, pk+1)]"
N,0.74609375,"≥
−
 σ"
N,0.7473958333333334,"2λ +
3σ2"
N,0.7486979166666666,2(σ −Lf −ρg) + σ 2
N,0.75,"
Ei(k)∥zk −zk+1∥2 −σλ"
N,0.7513020833333334,"2 ∥x(zk, pk+1) −x(zk)∥2 + 1"
N,0.7526041666666666,2η ∥pk −pk+1∥2 + η
N,0.75390625,"2∥Ax(zk, pk+1) −b∥2 −η"
N,0.7552083333333334,"2∥A∥2
 
Lf + σ + γ∥A∥2 + LK"
N,0.7565104166666666,"αx
σ −Lf −ρg
+ 1 !2"
N,0.7578125,∥xk −Tx(wk)∥2.
N,0.7591145833333334,"By the fact that Ei(k)∥zk −zk+1∥2 =
1
N ∥zk −Tz(wk)∥2 and pk+1 = Tp(wk), above inequality
follows that"
N,0.7604166666666666,"[ν(zk) −ψγ(zk, pk)] −Ei(k)[ν(zk+1) −ψγ(zk+1, pk+1)] ≥
−1 N  σ"
N,0.76171875,"2λ +
3σ2"
N,0.7630208333333334,2(σ −Lf −ρg) + σ 2
N,0.7643229166666666,"
∥zk −Tz(wk)∥2 −σλ"
N,0.765625,"2 ∥x(zk, Tp(wk)) −x(zk)∥2 + 1"
N,0.7669270833333334,2η ∥pk −Tp(wk)∥2 + η
N,0.7682291666666666,"2∥Ax(zk, Tp(wk)) −b∥2 −η"
N,0.76953125,"2∥A∥2
 
Lf + σ + γ∥A∥2 + LK"
N,0.7708333333333334,"αx
σ −Lf −ρg
+ 1 !2"
N,0.7721354166666666,"∥xk −Tx(wk)∥2.
(32)"
N,0.7734375,Under review as a conference paper at ICLR 2022
N,0.7747395833333334,"Step 5: Estimate the term Λ(wk) −Ei(k)Λ(wk+1).
By the combination of Step 1 and Step 4, we
obtain that"
N,0.7760416666666666,Λ(wk) −Ei(k)Λ(wk+1) ≥   βK
N,0.77734375,αx −(Lf + 2σ + γ∥A∥2)
N,0.7786458333333334,"2N
−η∥A∥2
 
Lf + σ + γ∥A∥2 + LK"
N,0.7799479166666666,"αx
σ −Lf −ρg
+ 1 !2"
N,0.78125,∥xk −Tx(wk)∥2 + 1 N  1
N,0.7825520833333334,"αz
−2σ −σ"
N,0.7838541666666666,"λ −
3σ2"
N,0.78515625,σ −Lf −ρg
N,0.7864583333333334,"
∥zk −Tz(wk)∥2"
N,0.7877604166666666,"+η∥Ax(zk, Tp(wk)) −b∥2 −σλ∥x(zk, Tp(wk)) −x(zk)∥2.
(33)"
N,0.7890625,"Since 0 < αx ≤βK/[Lf + 2σ + γ∥A∥2 + 5], then βK"
N,0.7903645833333334,"αx −(Lf + 2σ + γ∥A∥2) ≥5. Moreover,"
N,0.7916666666666666,"0 < η ≤1/ """
N,0.79296875,"2N∥A∥2

Lf +σ+γ∥A∥2+ LK"
N,0.7942708333333334,"αx
σ−Lf −ρg
+ 1
2#"
N,0.7955729166666666,implies
N,0.796875,Λ(wk) −Ei(k)Λ(wk+1)
N,0.7981770833333334,"≥
2
N ∥xk −Tx(wk)∥2 + 1 N  1"
N,0.7994791666666666,"αz
−2σ −σ"
N,0.80078125,"λ −
3σ2"
N,0.8020833333333334,σ −Lf −ρg
N,0.8033854166666666,"
∥zk −Tz(wk)∥2"
N,0.8046875,"+η∥Ax(zk, Tp(wk)) −b∥2 −σλ∥x(zk, Tp(wk)) −x(zk)∥2.
(34)"
N,0.8059895833333334,"A.6
PROOF OF LEMMA 3.2"
N,0.8072916666666666,"Proof. The result is directly by the fact that dist
 
0, ∂Lz
γ (x(z, p), p)

= ∥Ax(z, p) −b∥."
N,0.80859375,"A.7
PROOF OF LEMMA 3.3"
N,0.8098958333333334,"Proof. The claim is provided by the results of Proposition 1 and Corollary of (Robinson, 1981)."
N,0.8111979166666666,"A.8
PROOF OF LEMMA 4.1"
N,0.8125,"Proof. Taking λ = min{
δη
2σM 2+1,
η
σκ2+1} and 0 < αz < 1/
h
2σ + σ"
N,0.8138020833333334,"λ +
3σ2
σ−Lf −ρg + 1
i
in
Lemma 3.1, we have that"
N,0.8151041666666666,Λ(wk) −Ei(k)Λ(wk+1)
N,0.81640625,"≥
2
N ∥xk −Tx(wk)∥2 + 1"
N,0.8177083333333334,"N ∥zk −Tz(wk)∥2 + η∥Ax(zk, Tp(wk)) −b∥2"
N,0.8190104166666666,"−σ min{
δη
2σM 2 + 1,
η
σκ2 + 1}∥x(zk, Tp(wk)) −x(zk)∥2.
(35)"
N,0.8203125,"For the last two terms of above inequality, we have two cases to discuss."
N,0.8216145833333334,"• Case 1: ∥Ax(zk, Tp(wk)) −b∥≤2σλM2 η
≤δ."
N,0.8229166666666666,"Since ∂Lz
γ(x, p) is locally metric subregular in (x, p) uniformly in z over Rd at point 0
with parameters κ, δ > 0, (35) yields that"
N,0.82421875,"Λ(wk) −Ei(k)Λ(wk+1)
≥
2
N ∥xk −Tx(wk)∥2 + 1"
N,0.8255208333333334,N ∥zk −Tz(wk)∥2
N,0.8268229166666666,"+
η
σκ2 + 1∥Ax(zk, Tp(wk)) −b∥2.
(36)"
N,0.828125,"• Case 2: ∥Ax(zk, Tp(wk)) −b∥> 2σλM2 η
."
N,0.8294270833333334,Under review as a conference paper at ICLR 2022
N,0.8307291666666666,"Since M = max
x,x′∈X ∥x −x′∥, we have that"
N,0.83203125,"η∥Ax(zk, Tp(wk)) −b∥2 −σλ∥x(zk, Tp(wk)) −x∗(zk)∥2"
N,0.8333333333333334,"≥
η
2∥Ax(zk, Tp(wk)) −b∥2 + η"
N,0.8346354166666666,2 · 2σλM 2
N,0.8359375,"η
−σλM 2"
N,0.8372395833333334,"=
η
2∥Ax(zk, Tp(wk)) −b∥2.
(37)"
N,0.8385416666666666,"For both case, (35) yields that"
N,0.83984375,Λ(wk) −Ei(k)Λ(wk+1)
N,0.8411458333333334,"≥
2
N ∥xk −Tx(wk)∥2 + 1"
N,0.8424479166666666,"N ∥zk −Tz(wk)∥2 +
η
σκ2 + 2∥Ax(zk, Tp(wk)) −b∥2.
(38)"
N,0.84375,"By Proposition A.2, above inequality yields that"
N,0.8450520833333334,Λ(wk) −Ei(k)Λ(wk+1)
N,0.8463541666666666,"≥
1
N ∥xk −Tx(wk)∥2 +
1"
N,0.84765625,"N

Lf +σ+γ∥A∥2+ LK"
N,0.8489583333333334,"αx
σ−Lf −ρg
+ 1
2 ∥xk −x(zk, Tp(wk)∥2 + 1"
N,0.8502604166666666,"N ∥zk −Tz(wk)∥2 +
η
σκ2 + 2∥Ax(zk, Tp(wk)) −b∥2"
N,0.8515625,"≥
1
N ∥xk −Tx(wk)∥2 +
1"
N,0.8528645833333334,"N∥A∥2

Lf +σ+γ∥A∥2+ LK"
N,0.8541666666666666,"αx
σ−Lf −ρg
+ 1
2 ∥A(xk −x(zk, Tp(wk))∥2 + 1"
N,0.85546875,"N ∥zk −Tz(wk)∥2 +
η
σκ2 + 2∥Ax(zk, Tp(wk)) −b∥2"
N,0.8567708333333334,"≥
1
N ∥xk −Tx(wk)∥2 + 1"
N,0.8580729166666666,N ∥zk −Tz(wk)∥2
N,0.859375,"+
η
2(σκ2 + 2)
 
2∥A(xk −x(zk, Tp(wk))∥2 + 2∥Ax(zk, Tp(wk)) −b∥2"
N,0.8606770833333334,"≥
1
N ∥xk −Tx(wk)∥2 + 1"
N,0.8619791666666666,"N ∥zk −Tz(wk)∥2 +
η
2(σκ2 + 2)∥Axk −b∥2"
N,0.86328125,"=
1
N ∥xk −Tx(wk)∥2 + 1"
N,0.8645833333333334,"N ∥zk −Tz(wk)∥2 +
1
2η(σκ2 + 2)∥pk −Tp(wk)∥2"
N,0.8658854166666666,"≥
c1
N ∥wk −T(wk)∥2,
(39)"
N,0.8671875,"with c1 = min
n
1,
N
2η(σκ2+2)+1
o
."
N,0.8684895833333334,"A.9
PROOF OF THEOREM 4.1"
N,0.8697916666666666,"In order to provide the Theorem 4.1, we need the famous Robbins-Siegmund theorem."
N,0.87109375,"Theorem A.1. (Robbins-Siegmund Theorem, Robbins & Siegmund (1971), Theorem 1.3.12
in Duﬂo (2013), Theorem 2.27 in Carpentier et al. (2015)) Let {Λk}k∈N, {µk}k∈N, {νk}k∈N
and {ηk}k∈N be four positive sequences of real-valued random variables adapted to the ﬁltration
{ξk}k∈N. Assume that"
N,0.8723958333333334,"EξkΛk+1 ≤(1 + µk)Λk + νk −ηk,
∀k ∈N,"
N,0.8736979166666666,"and that
X"
N,0.875,"k∈N
µk < +∞
and
X"
N,0.8763020833333334,"k∈N
νk < +∞,
a.s.."
N,0.8776041666666666,Under review as a conference paper at ICLR 2022
N,0.87890625,"Then, the sequence {Λk}k∈N almost surely converges to a ﬁnite3 random variable Λ∞, and P"
N,0.8802083333333334,"k∈N
ηk <"
N,0.8815104166666666,"+∞, a.s.."
N,0.8828125,Then we can derive the technical proof of this theorem.
N,0.8841145833333334,Proof of Theorem 4.1:
N,0.8854166666666666,"(a) By Lemma 4.1 and the fact that Λ(w) ≥F ∗, we have that"
N,0.88671875,Ei(k)[Λ(wk+1) −F ∗] ≤[Λ(wk) −F ∗] −c1
N,0.8880208333333334,N ∥wk −T(wk)∥2.
N,0.8893229166666666,"By the Robbins-Siegmund Theorem (Theorem A.1), we obtain that,
lim
k→+∞Λ(wk) is almost"
N,0.890625,"surely exists and
+∞
P"
N,0.8919270833333334,"k=0
∥wk −T(wk)∥2 < +∞a.s.. Therefore, we have limk→+∞Φ(wk) = 0;"
N,0.8932291666666666,"(b) By the compact of X, we have the sequences {xk} and {x(zk)} are bounded.
By statement (a), we have
lim
k→+∞Λ(wk) is almost surely exists. It follows that the sequence"
N,0.89453125,"{Λ(wk)} is almost surely bounded. By the deﬁnition of Λ(wk), we also have that Λ(wk) ≥
ν(zk) ≥F(x(zk)) ≥F ∗. Therefore the sequence {ν(zk)} is almost surely bounded.
Now
we
show
the
almost
surely
boundness
of
the
sequence
{zk}
by
contra-
diction.
Suppose
P

∥zk∥→+∞
	
>
0.
Since
the
sequence
{x(zk)}
is
bounded,
there exists a subsequence {zk′} such that x(zk′)
→
¯x.
Therefore"
N,0.8958333333333334,"P
n
ν(zk′) = F(x(zk′)) + σ"
N,0.8971354166666666,"2 ∥(x(zk′)) −zk′∥2 →+∞
o
> 0, which follows a contradiction"
N,0.8984375,"with that {ν(zk′)} is almost surely bounded. Therefore, we obtain the almost surely boundness
of the sequence {zk}.
Next we propose the boundness of {pk}. By the optimal condition of problem of Tx(wk) =
arg min
x∈X⟨∇f(xk) + σ(xk −zk), x⟩+ g(x) + ⟨Tp(wk) + γ(Axk −b), Ax⟩+ D(x,xk)"
N,0.8997395833333334,"αx
, we have"
N,0.9010416666666666,0 ∈∇f(xk) + σ(xk −zk) + ∂g(Tx(wk)) + NX(Tx(wk)) + A⊤[Tp(wk) + γ(Axk −b)] + 1
N,0.90234375,"αx
[∇K(Tx(wk)) −∇K(xk)]."
N,0.9036458333333334,"Since Tx(wk) = NEi(k)xk+1 −(N −1)xk and the boundness of the sequence {xk}, we
have the sequence {Tx(wk)} is bounded. By bounded of subgradient of g, the boundness of
{xk}, {Tx(wk)} and the almost surely boundness of {zk}, it follows that ∥A⊤Tp(wk)∥< +∞,
∀k > 0, a.s.. By the fact that pk+1 = Tp(wk), we have that"
N,0.9049479166666666,"∥A⊤pk+1∥< +∞,
∀k > 0,
a.s."
N,0.90625,"By update of p in N-RPDC, we have that"
N,0.9075520833333334,"pk+1 = p0 + ˆpk+1
when
ˆpk+1 = η k
X"
N,0.9088541666666666,"j=1
(Axj −b) = ηA k
X"
N,0.91015625,"j=1
(xj −x∗) ∈Im(A),"
N,0.9114583333333334,"with x∗be the optimal solution of (P). Since p0 ∈Null(A⊤), we have A⊤p0 = 0 and
∥A⊤ˆpk+1∥< +∞, a.s.. Then we have two cases to discuss."
N,0.9127604166666666,"• Case 1: rank(A) = n. Obviously, we have that"
N,0.9140625,"∥ˆpk+1∥≤
∥A⊤ˆpk+1∥
p"
N,0.9153645833333334,"λmin(AA⊤)
< +∞,"
N,0.9166666666666666,"with λmin(AA⊤) be the smallest eigenvalue of matrix AA⊤.
• Case 2: rank(A) = r < n. Without loss of generality, assume the ﬁrst r rows of A
(denoted by Ar ∈Rr×d) are linearly independent, we have"
N,0.91796875,"A =

Ar
BAr"
N,0.9192708333333334,"
=

Ir×r
B 
Ar,"
N,0.9205729166666666,3A random variable X is ﬁnite if P ({ω ∈Ω|X(ω) = +∞}) = 0.
N,0.921875,Under review as a conference paper at ICLR 2022
N,0.9231770833333334,"where Ir×r ∈Rr×r is the indentity matrix and B ∈R(n−r)×r. Let Q := Ir×r + B⊤B.
It is easy to show that Q is symmetric and positive deﬁnite. By the fact that ˆpk+1 =
ηA Pk
j=1(xj −x∗), we have that"
N,0.9244791666666666,"∥A⊤ˆpk+1∥2
="
N,0.92578125,"(Ar)⊤ 
Ir×r, B⊤
η

Ir×r
B"
N,0.9270833333333334,"
Ar
k
X"
N,0.9283854166666666,"j=1
(xj −x∗)  2 ="
N,0.9296875,"(Ar)⊤QηAr
k
X"
N,0.9309895833333334,"j=1
(xj −x∗)  2"
N,0.9322916666666666,"≥
λmin
 
Ar(Ar)⊤

QηAr
k
X"
N,0.93359375,"j=1
(xj −x∗)  2"
N,0.9348958333333334,"≥
λmin
 
Ar(Ar)⊤
λmin(Q⊤Q)∥ηAr
k
X"
N,0.9361979166666666,"j=1
(xj −x∗)∥2,"
N,0.9375,"where λmin
 
Ar(Ar)⊤
is the smallest eigenvalue of positive semideﬁnite matrix Ar(Ar)⊤"
N,0.9388020833333334,"and λmin(Q⊤Q) is the smallest eigenvalue of positive semideﬁnite matrix Q⊤Q. Using the
fact that"
N,0.9401041666666666,"∥ˆpk+1∥2
=
∥ηA k
X"
N,0.94140625,"j=1
(xj −x∗)∥2"
N,0.9427083333333334,"=
∥η

Ir×r
B"
N,0.9440104166666666,"
Ar
k
X"
N,0.9453125,"j=1
(xj −x∗)∥2"
N,0.9466145833333334,"≤
λmax (Q) ∥ηAr
k
X"
N,0.9479166666666666,"j=1
(xj −x∗)∥2,"
N,0.94921875,"where λmax(Q) is the largest eigenvalue of matrix Q. Therefore, we have that"
N,0.9505208333333334,"∥ˆpk+1∥2 ≤
λmax (Q)
λmin (Ar(Ar)⊤) λmin(Q⊤Q)∥A⊤ˆpk+1∥2."
N,0.9518229166666666,"Therefore, for both cases, there exists d > 0 such that
∥ˆpk+1∥≤d∥A⊤ˆpk+1∥< +∞, a.s."
N,0.953125,"which yields the almost surely boundness of {pk} by the triangle inequality. Therefore, the
sequence {wk} is almost surely bounded."
N,0.9544270833333334,"(c) By
lim
k→+∞∥wk −T(wk)∥= 0 a.s. in statment (a), Lemma 2.1, Proposition 3.1, the almost"
N,0.9557291666666666,"surely boundness of {wk}, and the closedness of ∂L+
γ (·), we obtain the desired result."
N,0.95703125,"A.10
PROOF OF THEOREM 4.2"
N,0.9583333333333334,"Proof. Suppose this statement does not hold, that is,"
N,0.9596354166666666,"P

lim inf
k→+∞ √"
N,0.9609375,"k + 1Φ(wk) ≥δ

> 0,"
N,0.9622395833333334,"for some d > 0. Then for k0 large enough, for all k ≥k0 we have"
N,0.9635416666666666,"P

Φ(wk) ≥
d
√ k + 1"
N,0.96484375,"
> 0."
N,0.9661458333333334,Therefore P
N,0.9674479166666666,"( +∞
X"
N,0.96875,"k=k0
(Φ(wk))2 ≥ +∞
X"
N,0.9700520833333334,"k=k0
(Φ(wk))2 ≥ +∞
X k=k0 d2 k + 1 ) > 0."
N,0.9713541666666666,Under review as a conference paper at ICLR 2022
N,0.97265625,"Since the fact that
+∞
P k=k0"
N,0.9739583333333334,"d2
k+1 = +∞, we have that above inequality is contradicted with the fact that +∞
P"
N,0.9752604166666666,"k=0
(Φ(wk))2 < +∞a.s. in the proof of Theorem 4.1. Therefore, we obtain the desired result."
N,0.9765625,"A.11
PROOF OF THEOREM 4.3"
N,0.9778645833333334,"Proof. Recalling the inequality of Lemma 4.1 and the fact that Λ(wk) ≥ν(zk) ≥F ∗, we have"
N,0.9791666666666666,Ei(k)[Λ(wk+1) −F ∗] ≤[Λ(wk) −F ∗] −c1
N,0.98046875,c2N (Φ(wk))2.
N,0.9817708333333334,"Taking expectation with respect to Ft, t > k for above inequality, we obtain that"
N,0.9830729166666666,EFt[Λ(wk+1) −F ∗] ≤EFt[Λ(wk) −F ∗] −c1
N,0.984375,c2N EFt(Φ(wk))2.
N,0.9856770833333334,"By the fact Λ(wk) ≥ν(zk) ≥F ∗, it follows"
N,0.9869791666666666,"c1
c2N t
X"
N,0.98828125,"k=0
EFt(Φ(wk))2 ≤Λ(w0) −F ∗."
N,0.9895833333333334,"By the Jensen’s inequality and the convexity of function h(x) = x2, x ∈R, it follows that"
N,0.9908854166666666,"c1
c2N t
X k=0"
N,0.9921875," 
EFtΦ(wk)
2 ≤Λ(w0) −F ∗."
N,0.9934895833333334,"By the fact that EFtΦ(wk) ≥0 and h(x) = x2, x ∈R is monotonic increasing in x on [0, +∞), we
have that
c1
c2N (t + 1)

min
0≤k≤t EFtΦ(wk)
2
≤Λ(w0) −F ∗."
N,0.9947916666666666,Here comes that
N,0.99609375,"min
0≤k≤t EFtΦ(wk) ≤ s c2N"
N,0.9973958333333334,c1 (Λ(w0) −F ∗)
N,0.9986979166666666,"t + 1
."
