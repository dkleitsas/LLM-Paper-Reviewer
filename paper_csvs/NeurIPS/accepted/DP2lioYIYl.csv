Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001277139208173691,"Neural networks are capable of translating between languages—in some cases
even between two languages where there is little or no access to parallel trans-
lations, in what is known as Unsupervised Machine Translation (UMT). Given
this progress, it is intriguing to ask whether machine learning tools can ultimately
enable understanding animal communication, particularly that of highly intelligent
animals. We propose a theoretical framework for analyzing UMT when no parallel
translations are available and when it cannot be assumed that the source and target
corpora address related subject domains or posses similar linguistic structure. We
exemplify this theory with two stylized models of language, for which our frame-
work provides bounds on necessary sample complexity; the bounds are formally
proven and experimentally verified on synthetic data. These bounds show that the
error rates are inversely related to the language complexity and amount of common
ground. This suggests that unsupervised translation of animal communication may
be feasible if the communication system is sufficiently complex."
INTRODUCTION,0.002554278416347382,"1
Introduction"
INTRODUCTION,0.0038314176245210726,"Recent interest in translating animal communication [2, 3, 9] has been motivated by breakthrough
performance of Language Models (LMs). Empirical work has succeeded in unsupervised translation
between human-language pairs such as English–French [23, 5] and programming languages such as
Python–Java [33]. Key to this feasibility seems to be the fact that language statistics, captured by a
LM (a probability distribution over text), encapsulate more than just grammar. For example, even
though both are grammatically correct, The calf nursed from its mother is more than 1,000 times
more likely than The calf nursed from its father.2"
INTRODUCTION,0.005108556832694764,"Given this remarkable progress, it is natural to ask whether it is possible to collect and analyze
animal communication data, aiming towards translating animal communication to a human language
description. This is particularly interesting when the source language may be of highly social and
intelligent animals, such as whales, and the target language is a human language, such as English."
INTRODUCTION,0.006385696040868455,"Challenges. The first and most basic challenge is understanding the goal, a question with a rich
history of philosophical debate [38]. To define the goal, we consider a hypothetical ground-truth
translator. As a thought experiment, consider a “mermaid” fluent in English and the source language"
INTRODUCTION,0.007662835249042145,"∗Authors listed alphabetically.
2Probabilities computed using the GPT-3 API https://openai.com/api/ text-davinci-02 model. A
B
C"
INTRODUCTION,0.008939974457215836,"Have you seen any orcas today?
I just got back from the reef.
p(A1) ≈10−18"
INTRODUCTION,0.010217113665389528,"Have you seen mom?
I just
returned from the ocean basin.
p(B1) ≈10−18"
INTRODUCTION,0.011494252873563218,"Hat out hat dsjgh!!!
p(C1) ≈10−48"
INTRODUCTION,0.01277139208173691,"At the reef, there were a lot of sea
turtles. p(A) ≈10−22
At the reef, there were a lot of sea
turtles. p(B) ≈10−26
bicycle
OMG
and.
p(C) ≈10−72"
INTRODUCTION,0.0140485312899106,"Figure 1: LMs identify incoherent text. The probabilities of three two-paragraph texts computed using
the GPT-3 API. The probabilities of just the first paragraphs A1, B1, C1 are also shown. Although
p(A1) ≈p(B1) and the second paragraphs of A and B are identical, overall p(A) ≫p(B) due to
coherence between the paragraphs. C is gibberish."
INTRODUCTION,0.01532567049808429,"(e.g. sperm whale communication). Such a mermaid could translate whale vocalizations that English
naturally expresses. An immediate worry arises: what about communications that the source language
may have about topics for which English has no specific words? For example, sperm whales have a
sonar sense which they use to perform echolocation. In that case, lacking a better alternative, the
mermaid may translate such a conversation as (something about echolocation).3"
INTRODUCTION,0.016602809706257982,"Thus, we formally define the goal to be to achieve translations similar to those that would be output
by a hypothetical ground-truth translator. While this does not guarantee functional utility, it brings the
general task of unsupervised translation and the specific task of understanding animal communication
into the familiar territory of supervised translation, where one can use existing error metrics to define
(hypothetical) error rates."
INTRODUCTION,0.017879948914431672,"The second challenge is that animal communication is unlikely to share much, if any, linguistic
structure with human languages. Indeed, our theory will make no assumption on the source language
other than that it is presented in a textual format. That said, one of our instantiations of the general
theory (the knowledge graph) shows that translation is easier between compositional languages."
INTRODUCTION,0.019157088122605363,"The third challenge is domain gap, i.e., that ideal translations of animal communications into English
would be semantically different from existing English text, and we have no precise prior model of
this semantic content. (In contrast, the distribution of English translations of French text would
resemble the distribution of English text.) Simply put: whales do not “talk” about smartphones.
Instead, we assume the existence of a broad prior that models plausible English translations of animal
communication. LMs assign likelihood to an input text based not only on grammatical correctness,
but also on agreement with the training data. In particular, LMs trained on massive and diverse
data, including some capturing facts about animals, may be able to reason about the plausibility of a
candidate translation. See Figure 1 and the discussion in Appendix H."
FRAMEWORK AND RESULTS,0.020434227330779056,"1.1
Framework and results"
FRAMEWORK AND RESULTS,0.021711366538952746,"A translator4 is a function f : X →Y that translates source text x ∈X into the target language
f(x) ∈Y. We focus on the easier-to-analyze case of lossless translation, where f is invertible
(one-to-one) denoted by fθ : X ,→Y. See Appendix I.1 for an extension to lossy translation."
FRAMEWORK AND RESULTS,0.022988505747126436,"We will consider a parameterized family of translators {fθ : X →Y}θ∈Θ, with the goal being to learn
the parameters θ ∈Θ of the most accurate translator. Accuracy (defined shortly) is measured with
respect to a hypothetical ground-truth translator denoted by f⋆: X →Y. We make a realizability
assumption that the ground-truth translator can be represented in our family, i.e., ⋆∈Θ."
FRAMEWORK AND RESULTS,0.024265644955300127,"The source language is defined as a distribution µ over x ∈X, where µ(x) is the likelihood that
text x occurs in the source language. The error of a model θ ∈Θ will be measured in terms of
err(θ) := Prx∼µ[fθ(x) ̸= f⋆(x)], or at times a general bounded loss function L(θ). Given µ and f⋆,
it will be useful to consider the translated language distribution τ over Y by taking f⋆(x) for x ∼µ."
FRAMEWORK AND RESULTS,0.02554278416347382,"In the case of similar source and target domains, one may assume that the target language distribution
ν over Y is close to τ. This is a common intuition given for the “magic” behind why UMT sometimes"
FRAMEWORK AND RESULTS,0.02681992337164751,"3A better description might be possible. Consider the fact that some people who are congenitally blind
comprehend vision-related verbs as accurately as sighted people [8].
4In this work, a translator refers to the function f : X →Y while translation refers to an output y = f(x)."
FRAMEWORK AND RESULTS,0.0280970625798212,"works: for complex asymmetric distributions, there may a nearly unique transformation in {fθ}θ∈Θ
that maps µ to something close ν (namely f⋆which maps µ to τ). A common approach in UMT is to
embed source and target text as high-dimensional vectors and learn a low-complexity transformation,
e.g., a rotation between these Euclidean spaces. Similarly, translator complexity will also play an
important role in our analysis."
FRAMEWORK AND RESULTS,0.02937420178799489,"Priors.
Rather than assuming that the target distribution ν is similar to the translated distribution τ,
we will instead assume access to a broad prior ρ over Y meant to capture how plausible a translation
y is, with larger ρ(y) indicating more natural and plausible translation. Appendix H discusses one
way a prior oracle can be created, starting with an LM ≈ν learned from many examples in the target
domain, and combined with a prompt, in the target language, describing the source domain."
FRAMEWORK AND RESULTS,0.03065134099616858,"We define the problem of unsupervised machine translation (with a prior) to be finding an accurate
θ ∈Θ given m iid unlabeled source texts x1, . . . , xm ∼µ and oracle access to prior ρ."
FRAMEWORK AND RESULTS,0.031928480204342274,"MLE.
Our focus is on the Maximum-Likelihood Estimator (MLE), which selects model parameters
θ ∈Θ that (approximately) maximize the likelihood of translations Q"
FRAMEWORK AND RESULTS,0.033205619412515965,"i ρ
 
fθ(xi)

."
FRAMEWORK AND RESULTS,0.034482758620689655,"Definition 1.1 (MLE). Given input a translator family {fθ : X →Y}θ∈Θ, samples x1, . . . , xm ∈X
and a distribution ρ over Y, the MLE outputs"
FRAMEWORK AND RESULTS,0.035759897828863345,"MLEρ(x1, x2, . . . , xm) := argmin
θ∈Θ m
X"
FRAMEWORK AND RESULTS,0.037037037037037035,"i=1
log
1
ρ(fθ(xi))"
FRAMEWORK AND RESULTS,0.038314176245210725,"If multiple θ have equal empirical loss, it breaks ties, say, lexicographically."
FRAMEWORK AND RESULTS,0.03959131545338442,"We note that heuristics for MLE have proven extremely successful in training the breakthrough LMs,
even though MLE optimization is intractable in the worst case."
FRAMEWORK AND RESULTS,0.04086845466155811,"Next, we analyze the efficacy of MLE in two complementary models of language: one that is
highly structured (requiring compositional language) and one that is completely unstructured. These
analyses both make strong assumptions on the target language, but make few assumptions about
the source language itself. In both cases, the source distributions are uniform over subsets of X,
which (informally) is the “difficult” case for UMT as the learner cannot benefit from similarity in
text frequency across languages. Both models are parameterized by the amount of “common ground”
between the source language and the prior, and both are randomized. Note that these models are not
intended to accurately capture natural language. Rather, they illustrate how our theory can be used to
study the effect of language similarity and complexity on data requirements for UMT."
FRAMEWORK AND RESULTS,0.0421455938697318,"Knowledge graph model.
Our first model consists of a pair of related knowledge graphs, in which
edges encode knowledge of binary relations. Each edge yields a text that described the knowledge it
encodes. For example, in Figure 2 edges encode which animal A eats which other animal B, and text
is derived as a simple description A eats B.5"
FRAMEWORK AND RESULTS,0.04342273307790549,"Formally, there are two Erd˝os–Rényi random digraphs. The target graph is assumed to contain n
nodes, while the source graph has r ≤n nodes corresponding to an (unknown) subset of the n target
nodes. The model has two parameters: the average degree d in the (randomly-generated) target
language graph, and the agreement α ∈(0, 1] between the source and the target graphs. Here α = 1
is complete agreement on edges in the subgraph, while α = 0 is complete independence. We assume
the languages use a compositional encoding for edges, meaning that they encode a given edge by
encoding both nodes, so we may consider only |Θ| = n!/(n−r)! translators consistently mapping the
r source nodes to the n target nodes, which is many fewer than the number of functions f : X →Y
mapping the |X| = O(r2) source edges into the Y = O(n2) target edges. Human languages as well
as the communication systems of several animals are known to be compositional [39].6"
FRAMEWORK AND RESULTS,0.04469987228607918,We analyze how the error of the learned translator depends on this “common knowledge” α:
FRAMEWORK AND RESULTS,0.04597701149425287,"Theorem 1.2 (Theorem 3.2, simplified). Consider a source language µ and a prior ρ generated by
the knowledge graph model over r source nodes, n target nodes, average degree d and agreement"
FRAMEWORK AND RESULTS,0.04725415070242656,"5In a future work, it would be interesting to consider k-ary relations (hypergraphs) or multiple relations.
6A system is compositional if the meaning of an expression is determined by the meaning of its parts."
FRAMEWORK AND RESULTS,0.04853128991060025,"Figure 2: An illustration of the knowledge graph model. In this example, the Welsh graph is an exact
subgraph of the English knowledge graph, but our model allows for differences."
FRAMEWORK AND RESULTS,0.04980842911877394,"parameter α. Then, with at least 99% probability, when given m source sentences x1, . . . , xm and
access to a prior ρ, MLE outputs a translator ˆθ with error"
FRAMEWORK AND RESULTS,0.05108556832694764,err(ˆθ) ≤O log n
FRAMEWORK AND RESULTS,0.05236270753512133,α2d + 1 α r
FRAMEWORK AND RESULTS,0.05363984674329502,r log n m ! .
FRAMEWORK AND RESULTS,0.05491698595146871,"The second term decreases to 0 at a O(m−1/2) rate, similar to (noisy) generalization bounds [30].
Note that the first term does not decrease with the number of samples. The average degree d is a
rough model of language complexity capturing per-node knowledge, while the agreement parameter
α captures the amount of common ground. Thus, more complex languages can be translated (within
a given error rate) with less common ground. Even with m = ∞source data, there could still be
errors in the mapping. For instance, there could be multiple triangles in the source and target graphs
that lead to ambiguities. However, for complex knowledge relations (degree d ≫1/α2), there will
be few such ambiguities."
FRAMEWORK AND RESULTS,0.0561941251596424,"Figure 2 illustrates an example of four English sentences and three sentences (corresponding to
an unknown subset of three of the English sentences) in Welsh. For UMT, one might hypothesize
that bwytaodd means eat because they both appear in every sentence. One might predict that siarc
means shark because the word siarc appears twice in a single Welsh sentence and only the word
shark appears twice in an English sentence. Next, note that eog may mean salmon because they
are the only other words occurring with siarc and shark. Similar logic suggests that bennog means
herring. Furthermore, the word order is consistently permuted, with subject-verb-object in English
and verb-subject-object in Welsh. This translation is indeed roughly correct. This information is
encoded in the directed graphs as shown, where each node corresponds to an animal species and an
edge between two nodes is present if the one species eats the other."
FRAMEWORK AND RESULTS,0.05747126436781609,"“Common nonsense” model.
The second model, the common nonsense model, assumes no
linguistic structure on the source language. Here, we set out to capture the fact that the translated
language τ = µ ◦f⋆and the prior ρ share some common ground through the fact that the laws of
nature may exclude common “nonsense” outside both distributions’ support."
FRAMEWORK AND RESULTS,0.05874840357598978,"Earlier work has justified alignment for UMT under the intuition that the target language distribution
ν is approximated by a nearly unique simple transformation, e.g., a rotation, of the source distribution
µ. However, for a prior ρ, our work suggests that UMT may also be possible if there is nearly a
unique simple transformation that maps τ so that it is contained in ρ. Figure 3 illustrates such a
nearly unique rotation—the UMT “puzzle” of finding a transformation fθ of µ which is contained
within ρ is subtly different from finding an alignment."
FRAMEWORK AND RESULTS,0.06002554278416347,"In the common nonsense model, τ and ρ are uniform over arbitrary sets T ⊆P ⊆Y from which a
common α ∈(0, 1/2] fraction of text is removed (hence the name “common nonsense”). Specifically,
τ and ρ are defined to be uniform over ˜T = T \ S, ˜P = P \ S, respectively, for a set S sampled by
including each y ∈Y with probability α."
FRAMEWORK AND RESULTS,0.06130268199233716,"We analyze the error of the learned translator as a function of the amount of common nonsense:
Theorem 1.3 (Theorem 3.4, simplified). Consider source language µ and a prior ρ generated by the
common nonsense model over |T| source texts and common-nonsense parameter α, and a translator
family parameterized by |Θ|. Then, with at least 99% probability, when given m source sentences
x1, . . . , xm and access to a prior ρ, MLE outputs a translator ˆθ with error"
FRAMEWORK AND RESULTS,0.06257982120051085,"err
 ˆθ
 := Pr
x∈X [fθ(x) ̸= f⋆(x)] = O

ln |Θ|
α min(m, |T|) 
."
FRAMEWORK AND RESULTS,0.06385696040868455,"Figure 3: The previous intuition behind UMT has the distributions of target language ν (middle)
close to ground-truth translations τ, which is assumed to be a low-complexity transformation (in
this example a rotation) of the source language µ (left). When source and target are not aligned,
restricting to prior ρ region (right) allows for translation, as long as there are enough “nonsense” texts
(black regions) so that there is a nearly unique rotation of µ that is contained in ρ. For example, both
distributions may assign negligible probability to nonsensical texts such as I died 3 times tomorrow.
(In this toy example, µ is uniform over a two-dimensional shape that happens to look like a whale.)"
FRAMEWORK AND RESULTS,0.06513409961685823,"Theorem 3.5 gives a nearly matching lower bound. Let us unpack the relevant quantities. First, we
think of α as measuring the amount of agreement or common ground required, which might be a
small constant. Second, note that |T| is a coarse measure of the complexity of the source language,
which requires a total of ˜O(|T|) bits to encode. Thus, the bound suggests that accurate UMT requires
the translator to be simple, with a description length that is an α-factor of the language description
length, and again α captures the agreement between τ, ρ. Thus, even with limited common ground,
one may be able to translate from a source language that is sufficiently complex. Third, for simplicity,
we require X, Y ⊂{0, 1}∗to be finite sets of binary strings, so WLOG Θ may be also assumed to be
finite. Thus, log2 |Θ| is the description length, a coarse but useful complexity measure that equals the
number of bits required to describe any model. (Neural network parameters can be encoded using
a constant number of bits per parameter.) Appendix I.2 discusses how this can be generalized to
continuous parameters."
FRAMEWORK AND RESULTS,0.06641123882503193,"Importantly, we note that (supervised) neural machine translators typically use far fewer parameters
than LMs.7 To see why, consider the example of the nursing calf (page 1) and the fact that a translator
needs not know that calves nurse from mothers. On the other hand, such knowledge is essential to
generate realistic text. Similarly, generating realistic text requires maintaining coherence between
paragraphs, while translation can often be done at the paragraph level."
FRAMEWORK AND RESULTS,0.06768837803320563,"As a warm-up, we include a simplified version of the common nonsense model, called the tree-based
model (Appendix B.1), in which texts are constructed word-by-word based on a tree structure."
FRAMEWORK AND RESULTS,0.06896551724137931,"Comparison to supervised classification.
Consider the dependency on m, the number of training
examples. Note that the classic Occam bound O
  1"
FRAMEWORK AND RESULTS,0.070242656449553,"m log |Θ|

is what one gets for noiseless supervised
classification, that is, when one is also given labels yi = f⋆(xi) at training time, which is similar to
Theorem 1.3, and give ˜O(m−1/2) bounds for noisy classification as in Theorem 1.2. Furthermore,
these bounds apply to translation, which can be viewed as a special case of classification with many
classes Y. Thus, in both cases, the data dependency on m is quite similar to that of classification."
FRAMEWORK AND RESULTS,0.07151979565772669,"Experiments.
We validate our theorems generating synthetic data from randomly-generated lan-
guages according to each model, and evaluating translator error as a function of the number of
samples and amount of common ground. The knowledge graph model (Figure 4, left) is used to
generate a source graph (language) on r = 9 nodes to a target graph (language) on n = 10 nodes and
average degree d ≈5, while varying the agreement parameter α. We also vary r (Figure 4, right)
supporting our main message: more complex languages can be translated more accurately. For the
common nonsense model (Figure 5) we simulate translation of a source language of size |T| = 105
while varying the fraction of common nonsense α. Appendix E contains details and code."
FRAMEWORK AND RESULTS,0.07279693486590039,"7For example, a multilingual model achieves state-of-the-art performance using only 5 billion parameters
[37], compared to 175 billion for GPT-3 [11]."
FRAMEWORK AND RESULTS,0.07407407407407407,"r
Accuracy
4
0.10 ± 0.05
7
0.74 ± 0.08
10
1.00 ± 0.00"
FRAMEWORK AND RESULTS,0.07535121328224777,"Figure 4: Knowledge Graph model experiments, each run on twenty seeds with standard errors
shown. Left: error of the top-scoring translator vs. number of source samples m. Right: effect of
source language complexity (number of source nodes r) on translator accuracy in the knowledge
graph model. We report the accuracy of the top-scoring translator after all source edges were input to
the learning algorithm, i.e., as the number of samples m →∞."
FRAMEWORK AND RESULTS,0.07662835249042145,"Figure 5: Common Nonsense model. The X-axis is the number of source samples m, and the Y-axis
is the average error among plausible translators (that have not been ruled-out so far). Each experiment
was run on five seeds, with standard error depicted by the shaded area."
FRAMEWORK AND RESULTS,0.07790549169859515,"Contributions.
The first contribution of this work is formalizing and analyzing a model of UMT. As
an initial work, its value is in the opportunities which it opens for further work more than the finality
and tightness/generality of its bounds. Our model applies even to low-resource source languages with
massive domain gap and linguistic distance. We emphasize that this work is only a first step in the
theoretical analysis of UMT (indeed, there is little theoretical work on machine translation in general).
Second, we exhibit two simple complementary models for which we prove that: (a) more complex
languages require less common ground, and (b) data requirements may not be significantly greater
than those of supervised translation (which tends to use less data than training a large LM). These
findings may have implications for the quantity and type of communication data that is collected for
deciphering animal communication and for UMT more generally. They also give theoretical evidence
that UMT can be successful and worth pursuing, in lieu of parallel (supervised) data, in the case
of sufficiently complex languages. All of that said, we note that our sample complexity bounds are
information theoretic, that is, they do not account for the computational complexity of optimizing the
translator. Finally, animal communication aside, to the best of our knowledge this work is the first
theoretical treatment of UMT, and may also shed light on translation between human languages."
FRAMEWORK AND RESULTS,0.07918263090676884,"Organization.
The framework is formally described in Section 2, and is instantiated with models
of language in Section 3 (proofs, experiments, and other details deferred to Appendices A to E). Key
takeaways from this work are presented in Section 4. Related and future work is discussed in Appen-
dices F and G. We illustrate how prompting LMs may give priors in Appendix H. Appendix I sketches"
FRAMEWORK AND RESULTS,0.08045977011494253,"a generalization of our framework to the settings of lossy translation and infinite translator families.
Lastly, Appendix J proves sample complexity bounds for the settings of supervised translation."
THE GENERAL FRAMEWORK,0.08173690932311622,"2
The General Framework"
THE GENERAL FRAMEWORK,0.08301404853128991,"We use f : X ,→Y to denote a 1–1 function, in which f(x) ̸= f(x′) for all x ̸= x′. For S ⊆X, we
write f(S) := {f(x) | x ∈S}. The indicator 1P is 1 if the predicate P holds, and 0 otherwise. The
uniform distribution over a set S is denoted by U(S), and log = log2 denotes base-2 logarithm."
THE GENERAL FRAMEWORK,0.0842911877394636,"Language and Prior.
A source language is a distribution µ over a set of possible texts X. Similarly,
a target language is a distribution ν over a set of possible texts Y. When clear from context, we
associate each language with its corresponding set of possible texts. A prior distribution ρ over
translations Y aims to predict the probability of observing each translation. One could naively take
ρ = ν, but Appendix H describes how better priors can focus on the domain of interest. Intuitively,
ρ(y) measures how “plausible” a translation y is. For simplicity, we assume that X, Y ⊆{0, 1}∗are
finite, non-empty sets of binary strings. Appendix I.2 discusses extensions to infinite sets."
THE GENERAL FRAMEWORK,0.08556832694763729,"Translators.
A translator is a mapping f : X ,→Y. There is a known set of 1–1 functions
{fθ : X ,→Y | θ ∈Θ} with parameter set Θ. Since parameters are assumed to be known and fixed,
we will omit them from the theorem statements and algorithm inputs, for ease of presentation. Like
X, Y, the set Θ is assumed to be finite. Appendix I.1 considers translators that are not 1–1."
THE GENERAL FRAMEWORK,0.08684546615581099,"Divergence.
A translator fθ and a distribution µ induce a distribution over y = fθ(x), which we
denote by fθ ◦µ. The divergence between this distribution and ρ is quantified using the Kullback–
Leibler (KL) divergence,"
THE GENERAL FRAMEWORK,0.08812260536398467,"D(θ) := KL(fθ ◦µ ∥ρ) = E
x∼µ """
THE GENERAL FRAMEWORK,0.08939974457215837,"log
µ(x)
ρ
 
fθ(x)
 # =
X"
THE GENERAL FRAMEWORK,0.09067688378033206,"x
µ(x) log
µ(x)
ρ
 
fθ(x)
 ≥0."
THE GENERAL FRAMEWORK,0.09195402298850575,"Note that since D(θ) = Ex∼µ

−1"
THE GENERAL FRAMEWORK,0.09323116219667944,"m
P log ρ(fθ(xi))

−H(µ), and H(µ) is a constant independent
of θ, the MLE of Definition 1.1 approximately minimizes divergence."
THE GENERAL FRAMEWORK,0.09450830140485313,"Ground truth.
In order to define semantic loss, we consider a ground-truth translator f⋆for some
⋆∈Θ. We can then define the (ground-truth) translated language τ = f⋆◦µ over Y, obtained
by taking f⋆(x) for x ∼µ. This is similar to the standard realizability assumption, and some of
our bounds resemble Occam bounds with training labels yi = f⋆(xi). Of course, the ground-truth
translator ⋆is not known to the unsupervised learning algorithm. In our setting, we further require
that ground-truth translations never have 0 probability under ρ:"
THE GENERAL FRAMEWORK,0.09578544061302682,"Definition 2.1 (Realizable prior). Prx∼µ[ρ(f⋆(x)) = 0] = 0, or equivalently D(⋆) < ∞."
THE GENERAL FRAMEWORK,0.0970625798212005,"Semantic loss.
The semantic loss of a translator is defined with respect to a semantic difference
function ℓ: Y × Y →[0, 1]. This function, unknown to the learner, measures the difference between
two texts from the target language Y, with ℓ(y, y) = 0 for all y. For a given semantic difference ℓ
and ground-truth translator f⋆, we define the semantic loss of a translator fθ by"
THE GENERAL FRAMEWORK,0.0983397190293742,"L(θ) := E
x∼µ

ℓ(f⋆(x), fθ(x))

."
THE GENERAL FRAMEWORK,0.09961685823754789,"Of particular interest to us is the semantic error err(·, ·), obtained when ℓis taken to be the 0-1
difference ℓ01 = (y, y′) = 1 for all y′ ̸= y. Note that since any semantic difference ℓis upper
bounded by 1, the semantic error upper-bounds any other semantic loss L. That is,"
THE GENERAL FRAMEWORK,0.10089399744572158,"L(θ) ≤err(θ) := Pr
x∼µ[fθ(x) ̸= f⋆(x)]."
THE GENERAL FRAMEWORK,0.10217113665389528,"Section 3 analyzes this error err(θ), which thus directly implies bounds on L(θ)."
THE GENERAL FRAMEWORK,0.10344827586206896,"3
Models of Language: Instantiating the General Framework"
RANDOM KNOWLEDGE GRAPHS,0.10472541507024266,"3.1
Random knowledge graphs"
RANDOM KNOWLEDGE GRAPHS,0.10600255427841634,"In this section, we define a model in which each text represents an edge between a pair of nodes in a
knowledge graph. Both languages have knowledge graphs, with the source language weakly agreeing
with an unknown subgraph of the target language."
RANDOM KNOWLEDGE GRAPHS,0.10727969348659004,"We fix X = X × X = X2 and Y = Y × Y = Y 2 with r := |X| and n := |Y |. The set of translators
considered is all mappings from the r source nodes to the n target nodes, namely ΘXY = {θ : X ,→
Y } and fθ
 
(u, v)
 :=
 
θ(u), θ(v)

. The random knowledge graph is parametrized by the number of
source nodes r, target node set Y, an edge density parameter p ∈(0, 1) representing the expected
fraction of edges present in each graph, and an agreement parameter α ∈(0, 1] representing the
correlation between these edges. In particular, α = 1 corresponds to the case where both graphs
agree on all edges, and α = 0 corresponds to the case where edges in the graphs are completely
independent. These parameters are unknown to the learner, who only knows X and Y (and thus
X = X2, Y = Y 2).
Definition 3.1 (Random knowledge graph). For a natural number r ≤|Y |, the KG = KG(Y, r, p, α)
model determines a distribution over sets T, P ⊆Y (which determine distributions ρ and µ). The
sets T and P are sampled as follows:"
RANDOM KNOWLEDGE GRAPHS,0.10855683269476372,"1. Set P ⊆Y is chosen by including each edge y ∈Y with probability p, independently."
RANDOM KNOWLEDGE GRAPHS,0.10983397190293742,2. Set S ⊆Y of size |S| = r is chosen uniformly at random.
RANDOM KNOWLEDGE GRAPHS,0.1111111111111111,"3. Set T ⊆S2 is chosen as follows. For each edge y ∈S2, independently,"
RANDOM KNOWLEDGE GRAPHS,0.1123882503192848,"(a) With probability α, y ∈T if and only if y ∈P.
(b) With probability 1−α, toss another p-biased coin and add y to T if it lands on “heads”;
that is, y ∈T with probability p, independently."
RANDOM KNOWLEDGE GRAPHS,0.1136653895274585,"It is easy to see that T ⊆S2 and P ⊆Y 2 marginally represent the edges of Erd˝os–Rényi random
graphs Gr,p and Gn,p, respectively. Moreover, the event that y ∈T is positively correlated with
y ∈P: for each y ∈S2, since with probability α > 0 they are identical and otherwise they are
independent. Formally, the equations below describe the probability of y ∈T for each y ∈S2 after
we fix S and choosing T ⊆S2. Letting q := (1 −p), for each y ∈S2:"
RANDOM KNOWLEDGE GRAPHS,0.11494252873563218,"Pr[y ∈T] = Pr[y ∈P] = p
(1)
Pr[y ∈T \ P] = Pr[y ∈P \ T] = (1 −α)pq
(2)"
RANDOM KNOWLEDGE GRAPHS,0.11621966794380588,Pr[y /∈P | y ∈T] = Pr[y /∈T | y ∈P] = (1 −α)pq
RANDOM KNOWLEDGE GRAPHS,0.11749680715197956,"p
= (1 −α)q
(3)"
RANDOM KNOWLEDGE GRAPHS,0.11877394636015326,"The last equality, shows that the probability of excluding a random y ∈T from P is smaller than the
probability of excluding a random “incorrect translation” y′ ̸= y, Pr[y′ /∈P] = q > (1 −α)q."
RANDOM KNOWLEDGE GRAPHS,0.12005108556832694,"We now describe how ρ, τ, are determined from T, P and how µ, ⋆may be chosen to complete the
model description. The ground-truth target translated distribution τ := U(T) is uniform over T. The
prior ρ is uniform over P, and then “smoothed” over the rest of the domain Y. Formally,"
RANDOM KNOWLEDGE GRAPHS,0.12132822477650064,ρ(y) :=
RANDOM KNOWLEDGE GRAPHS,0.12260536398467432,"(
1
2 ·

1
|P | +
1
|Y|

if y ∈P"
RANDOM KNOWLEDGE GRAPHS,0.12388250319284802,"1
2|Y|
if y /∈P."
RANDOM KNOWLEDGE GRAPHS,0.1251596424010217,"The ground-truth translator ⋆∈Θ is obtained by sampling a uniformly random ⋆: X ,→S. Lastly,
we take µ = U(f −1
⋆(T)), which agrees with the definition of τ.8"
RANDOM KNOWLEDGE GRAPHS,0.12643678160919541,"Next, we state the main theorem for this model, formalizing Theorem 1.2 from the introduction."
RANDOM KNOWLEDGE GRAPHS,0.1277139208173691,"8Formally, the KG model outputs T, P which may not determine S if some nodes have zero edges. In that
case, we choose ⋆randomly among θ such that fθ(X) ⊇T. In the exponentially unlikely event that either S or
T is empty, we define both τ, ρ to be the singleton distribution concentrated on (y, y) for the lexicographically
smallest y ∈Y and µ to concentrated on (x, x) for x = f −1
⋆(y). It is not difficult to see that MLE selects a
translator with 0 error."
RANDOM KNOWLEDGE GRAPHS,0.12899106002554278,"Theorem 3.2 (Translatability in the KG model). Fix any m ≥1, ∅̸= S ⊆Y, δ, α, p ∈(0, 1), and
let r := |S|, n := |Y |, q = 1 −p. Then, with probability ≥1 −δ over T, P from KG(S, Y, p, α),"
RANDOM KNOWLEDGE GRAPHS,0.13026819923371646,"err
 ˆθ

≤max"
RANDOM KNOWLEDGE GRAPHS,0.13154533844189017,"64
α2pq2r2 ln 6nr δ , 2 αq r"
RANDOM KNOWLEDGE GRAPHS,0.13282247765006386,"2
m ln 6nr δ ! ,"
RANDOM KNOWLEDGE GRAPHS,0.13409961685823754,"where ˆθ = MLEρ(x1, x2, . . . , xm) is from Definition 1.1. Simply, for p < 0.99, with probability
≥0.99,"
RANDOM KNOWLEDGE GRAPHS,0.13537675606641125,err(θ) = O
RANDOM KNOWLEDGE GRAPHS,0.13665389527458494,"log n
α2pr + 1 α r"
RANDOM KNOWLEDGE GRAPHS,0.13793103448275862,r log n m ! .
RANDOM KNOWLEDGE GRAPHS,0.1392081736909323,"The proof, given in Appendix D, requires generalizing our theory to priors that have full support.
Experimental validation of the theorem is described in Appendix E."
COMMON NONSENSE MODEL,0.140485312899106,"3.2
Common nonsense model"
COMMON NONSENSE MODEL,0.1417624521072797,"We next perform a “smoothed analysis” of arbitrary LMs µ, ρ that are uniform over sets that share a
small amount of randomness, i.e., a small common random set has been removed from both. This
shared randomness captures the fact that some texts are implausible in both languages and that this
set has some complex structure determined by the laws of nature, which we model as random."
COMMON NONSENSE MODEL,0.14303959131545338,"The α-common-nonsense distribution is a meta-distribution over pairs (µ, ρ) which themselves are
uniform distributions over perturbed versions of P, T. This is inspired by Smoothed Analysis [36].
Recall that U(S) denotes the uniform distribution over the set S."
COMMON NONSENSE MODEL,0.14431673052362706,"Definition 3.3 (Common nonsense). The α-common-nonsense distribution DP,T
α
with respect to
nonempty sets T ⊆P ⊆Y is the distribution over
 
ρ = U(P ∩S), τ = U(T ∩S)

where S ⊆Y is
formed by removing each y ∈Y with probability α, independently.9"
COMMON NONSENSE MODEL,0.14559386973180077,"To make this concrete in terms of a distribution µ on X, for any ground-truth translator f⋆: X ,→Y,
we similarly define a distribution DP,T
α,⋆over (µ, ρ) where µ := U(f −1
⋆(T ∩S)) is the uniform
distribution over the subset of X that translates into τ. We now state the formal version of Theorem 1.3."
COMMON NONSENSE MODEL,0.14687100893997446,"Theorem 3.4 (Translatability in the CN model). Let {fθ : X ,→Y | θ ∈Θ} a family of translators,
⋆∈Θ, α, δ ∈(0, 1/2], T ⊆P ⊆Y, and m ≥1. Then with probability ≥1 −δ, MLE run on ρ and
m ≥1 iid samples from µ outputs ˆθ with,"
COMMON NONSENSE MODEL,0.14814814814814814,err(ˆθ) ≤6
COMMON NONSENSE MODEL,0.14942528735632185,"α max
 1 m, 16 |T|"
COMMON NONSENSE MODEL,0.15070242656449553,"
· ln 6|Θ| δ
."
COMMON NONSENSE MODEL,0.15197956577266922,"Note that the probability is over both (µ, ρ) drawn from DP,T
α,⋆, and the m iid samples from µ. More
simply, with probability ≥0.99,"
COMMON NONSENSE MODEL,0.1532567049808429,"err(ˆθ) = O

log |Θ|
α min(m, |T|) 
."
COMMON NONSENSE MODEL,0.1545338441890166,"When the amount of shared randomness α is a constant, then this decreases asymptotically like the
bound of supervised translation (Theorem J.1) up until a constant, similar to Theorem 3.2. For very
large m, each extra bit describing the translator (increase by 1 in log |Θ|) amounts to a constant
number of mistranslated x’s out of all X. The proof is deferred to Appendix C."
COMMON NONSENSE MODEL,0.1558109833971903,We also prove the following lower-bound that is off by a constant factor of the upper bound.
COMMON NONSENSE MODEL,0.15708812260536398,"Theorem 3.5 (CN lower-bound). There exists constants c1, c2 ≥1 such that: for any set T ⊆Y, for
any m ≥1, any α ∈(0, 1/2], and any Θ with c1 ≤log |Θ| ≤α min(m, |T|), there exists Θ′ of size"
COMMON NONSENSE MODEL,0.1583652618135377,"9Again, in the exponentially unlikely event that either P ∩S or T ∩S is empty, we define both τ, ρ to be the
singleton distribution concentrated on the lexicographically smallest element of Y, so MLE outputs a 0-error
translator."
COMMON NONSENSE MODEL,0.15964240102171137,"|Θ′| ≤|Θ| such that, for any P ⊇T and any algorithm Aρ : X m →Θ′, with probability ≥0.99
over ⋆∼U(Θ′) and (µ, ρ) drawn from DP,T
α,⋆and x1, . . . , xm ∼µ,"
COMMON NONSENSE MODEL,0.16091954022988506,"err
 ˆθ

≥
log |Θ|
c2α min(m, |T|),"
COMMON NONSENSE MODEL,0.16219667943805874,"where ˆθ = Aρ(x1, x2, . . . , xm)."
COMMON NONSENSE MODEL,0.16347381864623245,"The only purpose of Θ in the above theorem is to upper-bound the description length of translators, as
we replace it with an entirely different (possibly smaller) translator family Θ′ that still has the lower
bound using log |Θ| ≥log |Θ′|. Since U(Θ′) is the uniform distribution over Θ′, the ground-truth
classifier is uniformly random from Θ′. A requirement of the form log |Θ| = O
 
α min(m, |T|)

is
inherent as otherwise one would have an impossible right-hand side error lower-bound greater than 1,
though the constants could be improved."
COMMON NONSENSE MODEL,0.16475095785440613,"The proof of this theorem is given in Appendix C.2, and creates a model with O(log n) independent
“ambiguities” that cannot be resolved, with high probability over S, x1, x2, . . . , xm. Experimental
validation of the theorem is described in Appendix E."
DISCUSSION,0.16602809706257982,"4
Discussion"
DISCUSSION,0.1673052362707535,"We have given a framework for unsupervised translation and instantiated it in two stylized models.
Roughly speaking, in both models, the error rate is inversely related to the amount of samples,
common ground, and the language complexity. The first two relations are intuitive, while the last is
perhaps more surprising. All error bounds were information-theoretic, meaning that they guarantee a
learnable accurate translator, but learning this translator might be computationally intensive."
DISCUSSION,0.1685823754789272,"In both models, the translators are restricted. In the knowledge graph, the translators must operate
node-by-node following an assumed compositional language structure.10 In the common nonsense
model, the restriction is based on the translator description bit length log |Θ|. To illustrate how such
restrictions can be helpful, consider block-by-block translators which operate on limited contexts
(e.g., by paragraph). Consider again the hypothetical example of Figure 1. Suppose the three texts
are outputs of three translators Θ = {A, B, C}. Let us suppose that translator A always produces
accurate and natural translations, and further that all translators work paragraph-by-paragraph, as
modern translation algorithms operate within some limited context window. In fact, one can imagine
the translators of different paragraphs as a set of isolated adversaries where each adversary is trying
to mistranslate a paragraph, knowing the ground-truth translation of their paragraph, while attempting
to maintain the plausibility of the entire translation. If only the first-paragraph adversary mistranslates
reef to ocean basin, then the translation lacks coherence and is unlikely. If the adversaries are in
cahoots and coordinate to all translate reef to ocean basin, they would generate: Have you seen
mom? I just returned from the ocean basin. At the basin, there were a lot of sea turtles. which has
low probability ≈10−25, presumably because encoded in GPT-3’s training data is the knowledge
that there are no turtles deep in the ocean near the basin. While the adversary could also decide to
change the word turtle to something else when it appears near basin, eventually it would get caught
in its “web of deceit.” The intuition is that, across sufficiently many translations, the prior will not
“rule out” the ground-truth translations while very incorrect translators will be ruled out."
DISCUSSION,0.1698595146871009,"Judging success.
Our analysis sheds some light on whether it is even possible to tell if translation
without parallel data (UMT) is successful. A positive sign would be if millions of translations are
fluent English accounts that are consistent over time across translations. In principle, however, this is
what LM likelihood should measure (excluding consistencies across translations which sufficiently
powerful LMs may be able to measure better than humans). We also considered a statistical distance
(KL divergence) between the translations fˆθ(x) for x ∼µ and the prior y ∼ρ, and µ could be
estimated given enough samples. If this distance is close to zero, then one can have predictive accuracy
regardless of whether the translations are correct. This raises a related philosophical quandary: a
situation in which two beings are communicating via an erroneous translator, but both judge the
conversation to be natural."
DISCUSSION,0.17113665389527458,"10That is, we assume that each translator has a latent map from nodes in the source graph into nodes in
the target graph, and edges are mapped from the source to target graphs in the natural way. The study of
compositional communication systems, among humans and animals, has played a central role in linguistics [39]."
DISCUSSION,0.1724137931034483,Acknowledgments and Disclosure of Funding
DISCUSSION,0.17369093231162197,"We thank Madhu Sudan, Yonatan Belinkov and the entire Project CETI team, especially Pratyusha
Sharma, Jacob Andreas, Gašper Beguš, Michael Bronstein, and Dan Tchernov for illuminating
discussions. This study was funded by Project CETI via grants from Dalio Philanthropies and Ocean
X; Sea Grape Foundation; Rosamund Zander/Hansjorg Wyss, Chris Anderson/Jacqueline Novogratz
through The Audacious Project: a collaborative funding initiative housed at TED."
REFERENCES,0.17496807151979565,References
REFERENCES,0.17624521072796934,"[1] S. Ahyong, C.B. Boyko, N. Bailly, J. Bernot, R. Bieler, S.N. Brandão, M. Daly, S. De Grave,
S. Gofas, F. Hernandez, L. Hughes, T.A. Neubauer, G. Paulay, W. Decock, S. Dekeyzer,
L. Vandepitte, B. Vanhoorne, R. Adlard, S. Agatha, K.J. Ahn, N. Akkari, B. Alvarez, V. Amorim,
A. Anderberg, G. Anderson, S. Andrés Sánchez, Y. Ang, D. Antic, L.S.. Antonietto, C. Arango,
T. Artois, S. Atkinson, K. Auffenberg, B.G. Baldwin, R. Bank, A. Barber, J.P. Barbosa, I. Bartsch,
D. Bellan-Santini, N. Bergh, A. Berta, T.N. Bezerra, S. Blanco, I. Blasco-Costa, ..., and
A. Zullini. World register of marine species (worms). =https://www.marinespecies.org, 2022.
URL https://www.marinespecies.org. Accessed: 2022-10-22."
REFERENCES,0.17752234993614305,"[2] Jacob Andreas, Gašper Beguš, Michael M. Bronstein, Roee Diamant, Denley Delaney, Shane
Gero, Shafi Goldwasser, David F. Gruber, Sarah de Haas, Peter Malkin, Nikolay Pavlov, Roger
Payne, Giovanni Petri, Daniela Rus, Pratyusha Sharma, Dan Tchernov, Pernille Tønnesen,
Antonio Torralba, Daniel Vogt, and Robert J. Wood. Toward understanding the communication
in sperm whales. iScience, 25(6):104393, 2022. ISSN 2589-0042. doi: https://doi.org/10.1016/
j.isci.2022.104393. URL https://www.sciencedirect.com/science/article/
pii/S2589004222006642."
REFERENCES,0.17879948914431673,"[3] Emily
Anthes.
The
animal
translators.
The
New
York
Times,
Aug
2022.
URL
https://www.nytimes.com/2022/08/30/science/
translators-animals-naked-mole-rats.html."
REFERENCES,0.18007662835249041,"[4] Mikel Artetxe, Gorka Labaka, and Eneko Agirre. A robust self-learning method for fully
unsupervised cross-lingual mappings of word embeddings. In Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
789–798, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi:
10.18653/v1/P18-1073. URL https://aclanthology.org/P18-1073."
REFERENCES,0.18135376756066413,"[5] Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Unsupervised neural machine translation, a
new paradigm solely based on monolingual text. Proces. del Leng. Natural, 63:151–154,
2019.
URL http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/
article/view/6107."
REFERENCES,0.1826309067688378,"[6] László Babai, Paul Erdo˝s, and Stanley M. Selkow. Random graph isomorphism. SIAM Journal
on Computing, 9(3):628–635, 1980. doi: 10.1137/0209047. URL https://doi.org/10.
1137/0209047."
REFERENCES,0.1839080459770115,"[7] Christos Baziotis, Barry Haddow, and Alexandra Birch. Language model prior for low-resource
neural machine translation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors,
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2020, Online, November 16-20, 2020, pages 7622–7634. Association for Computational
Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.615. URL https://doi.org/10.
18653/v1/2020.emnlp-main.615."
REFERENCES,0.18518518518518517,"[8] Marina Bedny, Jorie Koster-Hale, Giulia Elli, Lindsay Yazzolino, and Rebecca Saxe. There’s
more to “sparkle” than meets the eye: Knowledge of vision and light verbs among congenitally
blind and sighted individuals. Cognition, 189:105–115, 2019. ISSN 0010-0277. doi: https:
//doi.org/10.1016/j.cognition.2019.03.017. URL https://www.sciencedirect.com/
science/article/pii/S0010027719300721."
REFERENCES,0.18646232439335889,"[9] Mélissa Berthet, Camille Coye, Guillaume Dezecache, and Jeremy Kuhn. Animal linguistics:
a primer. Biological Reviews, n/a(n/a), 2022. doi: https://doi.org/10.1111/brv.12897. URL
https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12897."
REFERENCES,0.18773946360153257,"[10] Thorsten Brants, Ashok C. Popat, Peng Xu, Franz Josef Och, and Jeffrey Dean. Large language
models in machine translation. In Jason Eisner, editor, EMNLP-CoNLL 2007, Proceedings
of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning, June 28-30, 2007, Prague, Czech Republic, pages
858–867. ACL, 2007. URL https://aclanthology.org/D07-1090/."
REFERENCES,0.18901660280970625,"[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, ..., and Dario Amodei. Language models are few-shot learners.
In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran
Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf."
REFERENCES,0.19029374201787994,"[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,
Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope,
James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, ..., and Noah Fiedel. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/
2204.02311."
REFERENCES,0.19157088122605365,"[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018."
REFERENCES,0.19284802043422733,"[14] Benjamin Doerr.
Probabilistic tools for the analysis of randomized optimization heuris-
tics.
In Natural Computing Series, pages 1–87. Springer International Publishing, nov
2019.
doi:
10.1007/978-3-030-29414-4_1.
URL https://doi.org/10.1007%
2F978-3-030-29414-4_1."
REFERENCES,0.194125159642401,"[15] Daniel Edmiston, Phillip Keung, and Noah A. Smith. Domain mismatch doesn’t always prevent
cross-lingual transfer learning. In Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid
Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard,
Joseph Mariani, Hélène Mazo, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the
Thirteenth Language Resources and Evaluation Conference, LREC 2022, Marseille, France,
20-25 June 2022, pages 892–899. European Language Resources Association, 2022. URL
https://aclanthology.org/2022.lrec-1.94."
REFERENCES,0.19540229885057472,"[16] Shane Gero, Hal Whitehead, and Luke Rendell. Individual, unit and vocal clan level identity
cues in sperm whale codas. Royal Society Open Science, 3(1):150372, 2016."
REFERENCES,0.1966794380587484,"[17] Oded Goldreich, Brendan Juba, and Madhu Sudan. A theory of goal-oriented communication.
J. ACM, 59(2):8:1–8:65, 2012. doi: 10.1145/2160158.2160161. URL https://doi.org/
10.1145/2160158.2160161."
REFERENCES,0.1979565772669221,"[18] Jesse Michael Han, Igor Babuschkin, Harrison Edwards, Arvind Neelakantan, Tao Xu, Stanislas
Polu, Alex Ray, Pranav Shyam, Aditya Ramesh, Alec Radford, and Ilya Sutskever. Unsupervised
neural machine translation with generative language models only. CoRR, abs/2110.05448, 2021.
URL https://arxiv.org/abs/2110.05448."
REFERENCES,0.19923371647509577,"[19] Zhiwei He, Xing Wang, Rui Wang, Shuming Shi, and Zhaopeng Tu. Bridging the data gap
between training and inference for unsupervised neural machine translation. In Smaranda
Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022,
Dublin, Ireland, May 22-27, 2022, pages 6611–6623. Association for Computational Linguistics,
2022. doi: 10.18653/v1/2022.acl-long.456. URL https://doi.org/10.18653/v1/
2022.acl-long.456."
REFERENCES,0.20051085568326948,"[20] Yaofang Hu, Wanjie Wang, and Yi Yu. Graph matching beyond perfectly-overlapping erd˝os-
rényi random graphs. Stat. Comput., 32(1):19, 2022. doi: 10.1007/s11222-022-10079-1. URL
https://doi.org/10.1007/s11222-022-10079-1."
REFERENCES,0.20178799489144317,"[21] Brendan Juba and Madhu Sudan. Universal semantic communication I. In Cynthia Dwork,
editor, Proceedings of the 40th Annual ACM Symposium on Theory of Computing, Victoria,
British Columbia, Canada, May 17-20, 2008, pages 123–132. ACM, 2008. doi: 10.1145/
1374376.1374397. URL https://doi.org/10.1145/1374376.1374397."
REFERENCES,0.20306513409961685,"[22] Yunsu Kim, Miguel Graça, and Hermann Ney. When and why is unsupervised neural machine
translation useless? In Mikel L. Forcada, André Martins, Helena Moniz, Marco Turchi, Arianna
Bisazza, Joss Moorkens, Ana Guerberof Arenas, Mary Nurminen, Lena Marg, Sara Fumega,
Bruno Martins, Fernando Batista, Luísa Coheur, Carla Parra Escartín, and Isabel Trancoso,
editors, Proceedings of the 22nd Annual Conference of the European Association for Machine
Translation, EAMT 2020, Lisboa, Portugal, November 3-5, 2020, pages 35–44. European
Association for Machine Translation, 2020. URL https://aclanthology.org/2020.
eamt-1.5/."
REFERENCES,0.20434227330779056,"[23] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsuper-
vised machine translation using monolingual corpora only. In 6th International Conference
on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,
Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/
forum?id=rkYTTf-AZ."
REFERENCES,0.20561941251596424,"[24] Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ran-
zato. Phrase-based & neural unsupervised machine translation. In Ellen Riloff, David Chi-
ang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -
November 4, 2018, pages 5039–5049. Association for Computational Linguistics, 2018. URL
https://aclanthology.org/D18-1549/."
REFERENCES,0.20689655172413793,"[25] Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ran-
zato. Phrase-based & neural unsupervised machine translation. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, pages 5039–5049, Brus-
sels, Belgium, October-November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-1549. URL https://aclanthology.org/D18-1549."
REFERENCES,0.2081736909323116,"[26] Lorenzo Livi and Antonello Rizzi.
The graph matching problem.
Pattern Analysis and
Applications, 16:253–283, 08 2013. doi: 10.1007/s10044-012-0284-8."
REFERENCES,0.20945083014048532,"[27] André Lynum, Erwin Marsi, Lars Bungum, and Björn Gambäck. Disambiguating word trans-
lations with target language models. In Petr Sojka, Ales Horák, Ivan Kopecek, and Karel
Pala, editors, Text, Speech and Dialogue - 15th International Conference, TSD 2012, Brno,
Czech Republic, September 3-7, 2012. Proceedings, volume 7499 of Lecture Notes in Com-
puter Science, pages 378–385. Springer, 2012. doi: 10.1007/978-3-642-32790-2\_46. URL
https://doi.org/10.1007/978-3-642-32790-2_46."
REFERENCES,0.210727969348659,"[28] Kelly Marchisio, Kevin Duh, and Philipp Koehn. When does unsupervised machine translation
work?
In Loïc Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-
jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Yvette Graham, Paco Guzman,
Barry Haddow, Matthias Huck, Antonio Jimeno-Yepes, Philipp Koehn, André Martins, Makoto
Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, and Matteo Negri, editors,
Proceedings of the Fifth Conference on Machine Translation, WMT@EMNLP 2020, Online,
November 19-20, 2020, pages 571–583. Association for Computational Linguistics, 2020. URL
https://aclanthology.org/2020.wmt-1.68/."
REFERENCES,0.2120051085568327,"[29] Antonio Valerio Miceli Barone.
Towards cross-lingual distributed representations with-
out parallel text trained with adversarial autoencoders.
In Proceedings of the 1st Work-
shop on Representation Learning for NLP, pages 121–126, Berlin, Germany, August 2016.
Association for Computational Linguistics. doi: 10.18653/v1/W16-1614. URL https:
//aclanthology.org/W16-1614."
REFERENCES,0.21328224776500637,"[30] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018."
REFERENCES,0.21455938697318008,"[31] Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi Shekhar, Mehreen
Alam, and Rishemjit Kaur. Neural machine translation for low-resource languages: A survey.
arXiv preprint arXiv:2106.15115, 2021."
REFERENCES,0.21583652618135377,"[32] Sujith Ravi and Kevin Knight. Deciphering foreign language. In Dekang Lin, Yuji Matsumoto,
and Rada Mihalcea, editors, The 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011,
Portland, Oregon, USA, pages 12–21. The Association for Computer Linguistics, 2011. URL
https://aclanthology.org/P11-1002/."
REFERENCES,0.21711366538952745,"[33] Baptiste Rozière, Jie Zhang, François Charton, Mark Harman, Gabriel Synnaeve, and Guillaume
Lample. Leveraging automated unit tests for unsupervised code translation. In The Tenth Inter-
national Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.
OpenReview.net, 2022. URL https://openreview.net/forum?id=cmt-6KtR4c4."
REFERENCES,0.21839080459770116,"[34] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. Auto-
Prompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts.
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 4222–4235, Online, November 2020. Association for Computational Lin-
guistics. doi: 10.18653/v1/2020.emnlp-main.346. URL https://aclanthology.org/
2020.emnlp-main.346."
REFERENCES,0.21966794380587484,"[35] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: masked sequence to se-
quence pre-training for language generation. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019,
9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning
Research, pages 5926–5936. PMLR, 2019. URL http://proceedings.mlr.press/
v97/song19d.html."
REFERENCES,0.22094508301404853,"[36] Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis: An attempt to explain the
behavior of algorithms in practice. Commun. ACM, 52(10):76–84, oct 2009. ISSN 0001-
0782. doi: 10.1145/1562764.1562785. URL https://doi.org/10.1145/1562764.
1562785."
REFERENCES,0.2222222222222222,"[37] Chau Tran, Shruti Bhosale, James Cross, Philipp Koehn, Sergey Edunov, and Angela Fan.
Facebook ai’s WMT21 news translation task submission. In Loïc Barrault, Ondrej Bojar,
Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel,
Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry
Haddow, Matthias Huck, Antonio Jimeno-Yepes, Philipp Koehn, Tom Kocmi, André Martins,
Makoto Morishita, and Christof Monz, editors, Proceedings of the Sixth Conference on Machine
Translation, WMT@EMNLP 2021, Online Event, November 10-11, 2021, pages 205–215.
Association for Computational Linguistics, 2021. URL https://aclanthology.org/
2021.wmt-1.19."
REFERENCES,0.22349936143039592,"[38] Ludwig Wittgenstein. Philosophical Investigations. Basil Blackwell, Oxford, 1953. ISBN
0631119000."
REFERENCES,0.2247765006385696,"[39] Klaus Zuberbühler. Syntax and compositionality in animal communication. Philosophical
Transactions of the Royal Society B, 375(1789):20190062, 2020."
REFERENCES,0.2260536398467433,"A
Translator Revisions and Plausible Ambiguities"
REFERENCES,0.227330779054917,"Even though ⋆is unknown, it will be convenient to define, for each θ ∈Θ, a revision of f⋆which is
the permutation π⋆
θ : Y ,→Y between fθ and f⋆, π⋆
θ(y) := fθ(f −1
⋆(y)).11 We write πθ = π⋆
θ when ⋆
is clear from context, and π⋆(y) ≡y is the identity revision."
REFERENCES,0.22860791826309068,"Note that the divergence D(θ) and semantic loss L(θ) can equivalently be defined using revisions,"
REFERENCES,0.22988505747126436,"D(θ) := KL(fθ ◦µ ∥ρ) = KL(τ ∥π−1
θ
◦ρ) = E
y∼τ

log
τ(y)
ρ(πθ(y))

,"
REFERENCES,0.23116219667943805,"L(θ) := E
x∼µ

ℓ(f⋆(x), fθ(x))

= E
y∼τ

ℓ(y, πθ(y))

."
REFERENCES,0.23243933588761176,"To relate divergence and loss, it is helpful to define a notion of plausible ambiguities which are θ
whose revisions πθ(y ∼τ) are not too unlikely under the prior. These may also be of independent
interest as they capture certain types of ambiguities that can only be resolved with supervision.
Definition A.1 (γ-Plausible ambiguities). For any γ ∈[0, 1], ⋆∈Θ, and distributions τ, ρ over Y,
the set of γ-plausible ambiguities Aγ = A⋆,τ,ρ
γ
⊆Θ is:"
REFERENCES,0.23371647509578544,"Aγ :=

θ ∈Θ
 Pr
y∼τ[ρ(πθ(y)) = 0] ≤γ

, and εγ := max
θ∈Aγ L(θ)."
REFERENCES,0.23499361430395913,"For example, left↔right would constitute a γ-plausible ambiguity if one could swap the two words,
making a ≤γ fraction of the translations have 0 probability. Such a revision would have low loss if
such swaps are considered similar in meaning. Condition 2.1 is equivalent to ⋆∈A0."
REFERENCES,0.23627075351213284,"The quantity of interest is εγ, the maximum loss of any γ-plausible ambiguity.12 Next, we prove that
the loss of the translator output by the Maximum Likelihood Estimator is not greater than εγ given
m ≥1"
REFERENCES,0.23754789272030652,γ ln |Θ|
REFERENCES,0.2388250319284802,δ examples.
REFERENCES,0.24010217113665389,"Theorem A.2. Let µ, ρ be probability distributions over X, Y, resp., and f⋆satisfy Condition 2.1:
Prµ[ρ(f⋆(x)) = 0] = 0. Fix any δ ∈(0, 1), m ≥1, and let γ ≥1"
REFERENCES,0.2413793103448276,m ln |Θ|
REFERENCES,0.24265644955300128,"δ . Then,"
REFERENCES,0.24393358876117496,"Pr
x1,...,xm∼µ"
REFERENCES,0.24521072796934865,"h
L(ˆθ) ≤εγ
i
> 1 −δ,"
REFERENCES,0.24648786717752236,"where ˆθ = MLEρ(x1, . . . , xm) and εγ is from Definition A.1."
REFERENCES,0.24776500638569604,"Since εγ is non-decreasing in γ, as the number of examples increases the loss bound εγ decreases to
approach ε0. This bound and its proof in Appendix A are analogous to the realizable Occam bound
of supervised classification, see Appendix J."
REFERENCES,0.24904214559386972,"Proof of Theorem A.2. MLE minimizes ˆv(θ) :=
1
m
P
i≤m log
1
ρ(πθ(yi)) over θ ∈Θ, where yi =
fθ(xi). By the realizable prior assumption ˆv(⋆) < ∞. Thus, for the algorithm to fail, there must be
a “bad” ˆθ ∈B := {θ ∈Θ | L(θ) > εγ} with ˆv(θ) ≤ˆv(⋆) < ∞. If ρ(πθ(yi)) = 0 for any i, then
ˆv(πθ) = ∞. Note that by Definition A.1, B ∩Aγ = ∅, thus for all θ ∈B: Pr[ρ(πθ(y)) = 0] > γ
and,"
REFERENCES,0.2503192848020434,"Pr [ˆv(θ) < ∞] < (1 −γ)m ≤e−γm ≤
δ
|Θ|."
REFERENCES,0.2515964240102171,"By the union bound over θ ∈B, Pr[∃θ ∈B ˆv(θ) < ∞] < δ since |B| ≤|Θ|."
REFERENCES,0.25287356321839083,"B
The Tree-based Model"
REFERENCES,0.2541507024265645,"B.1
A tree-based probabilistic model of language"
REFERENCES,0.2554278416347382,"The last model, which we view as secondary to the previous two, is a simpler version of the common
nonsense model. It can be viewed as a “warm-up” leading up to that more general model, and we"
REFERENCES,0.2567049808429119,"11Formally, fθ ◦f −1
⋆
: f⋆(X) ,→Y is only defined on f⋆(X) but can be extended to a full permutation on Y
in many ways. For concreteness, we take the lexicographically smallest extension on Y ⊆{0, 1}∗.
12For intuition, note that εγ can be bounded using γ:
γ
εγ ≥minθ∈Θ Pry∼τ

ρ(πθ(y)) = 0 | πθ(y) ̸= y

."
REFERENCES,0.25798212005108556,"Figure 6: An example of a language tree of plausible texts and the subtree of ground-truth translations
illustrated in green."
REFERENCES,0.25925925925925924,"hope it helps illuminate that model by instantiating the language therein with a simple tree-based
syntax."
REFERENCES,0.26053639846743293,"In the tree-based model, the nodes of a tree are labeled with random words, and plausible texts
(according to the prior ρ) correspond to paths from root to leaf. A translated language τ (or
equivalently, a source language µ) is then derived from root-to-leaf paths in a random subtree
H ⊆G. We prove that, with high probability, a prior ρ and translated language τ sampled by this
process satisfy Condition 2.1 and semantic error εγ = O
 
1/ min(m, an)

. Therefore, MLE yields a
semantically accurate translator for the sampled language with high probability, for sufficiently large
n."
REFERENCES,0.26181353767560667,"The random tree language (RT) model is a randomized process for generating a tree-based source
language µ, translated language τ, and prior ρ. It is parameterized by a finite vocabulary set W, depth
n ∈N, and arities a, b ∈N such that 1 ≤a ≤b ≤|W|/4. For simplicity, the possible target texts are
taken to be the set of all possible n-grams over W, namely, Y := Wn. Again for simplicity, we also
assume |X| = |Y| so that each fθ : X ,→Y is a bijection. To generate an RT, first a full b-ary, depth
n + 1 tree G is constructed. Labeled edges shall correspond to words, and paths shall correspond to
texts (sentences), as follows:"
REFERENCES,0.26309067688378035,"1. Starting from the root node and proceeding in level-order traversal, for each node v: sample
b words w1, . . . , wb ∈W uniformly at random and without replacement; label each of v’s
child edges with one of the sampled words."
REFERENCES,0.26436781609195403,"2. The labels on each path from the root to a leaf (y1, . . . , yn) corresponds to a plausible text,
giving a set of plausible texts"
REFERENCES,0.2656449553001277,"P := {y | y = (y1, . . . , yn) labels of a path in G} ."
REFERENCES,0.2669220945083014,"3. A subtree H ⊆G is obtained by sampling uniformly at random a out of b of the children
at each level of the tree, in level-order traversal. The set of translated texts is analogously
defined as
T := {y | y = (y1, . . . , yn) labels of a path in H} ⊆P."
REFERENCES,0.2681992337164751,"The prior ρ = U(P) is uniform over plausible texts P, while µ = is uniform over f −1
⋆(T). We let
(µ, ρ) ∼RT(W, n, a, b) denote the sampling of a prior and translated language obtained via this
randomized process. When the parameters W, n, a, b are clear from context we simply write RT."
REFERENCES,0.26947637292464877,"Note that Θ and f⋆∈Θ may be arbitrary, and by definition of τ, µ = U(f −1
⋆(T)) is uniform over
T’s preimage. Since we assumed |X| = |Y| above, f⋆is invertible."
REFERENCES,0.2707535121328225,"Next, we will argue that a random tree language sampled in this process satisfies realizability
(Condition 2.1) and Definition A.1 with appropriate choice of parameters. While we make no
assumptions on Θ, it is important that the plausible texts P are sampled after the possible permutations
(ambiguities) Π are specified. Otherwise, if an adversary were able to choose a permutation π: Y →"
REFERENCES,0.2720306513409962,"Y based on P, then they could arbitrarily permute P, resulting in a permutation π with high expected
loss but no change in likelihood according to the prior ρ—thereby violating Definition A.1."
REFERENCES,0.27330779054916987,"You can think of Theorem B.1 as pointing to the required text length n and an upper bound on the
number of parameters |Θ| for which Condition 2.1 and Definition A.1 hold with high probability. As
we know from Theorem A.2, and state clearly next, these in turn imply translatability of a random
tree language.
Theorem B.1 (Translatability in the RT model). Fix any m ≥1, δ ∈(0, 1) vocabulary set W, and
tree arities a ≤b ≤|W|/4. Then, for any Θ and any ⋆∈Θ, with probability at least 1 −δ over
(µ, ρ) ∼RT(W, n, a, b) and iid samples x1, x2, . . . , xm ∼µ,"
REFERENCES,0.27458492975734355,"err(ˆθ) ≤16 max
 1 m, 4 an"
REFERENCES,0.27586206896551724,"
· ln 6|Θ| δ
,"
REFERENCES,0.2771392081736909,"where ˆθ = MLEρ(x1, x2, . . . , xm) and MLE is from Definition 1.1"
REFERENCES,0.2784163473818646,"Note that the probability in the corollary is over both (µ, ρ) ∼RT and the m iid training samples.
Again, note how the first term is similar to the 1"
REFERENCES,0.2796934865900383,m log |Θ|
REFERENCES,0.280970625798212,"δ term from realizable supervised learning
(see Theorem J.1), and the second term is additional due to the unsupervised case. The proof, given in
Appendix B, uses Theorem A.2 and a lemma stating that εγ ≤16γ for γ that are not too small. The
main challenge is that there are dependencies between the paths, so one cannot directly use standard
concentration inequalities."
REFERENCES,0.2822477650063857,"To better understand Theorem B.1, let us consider two possible families of translators and suppose
we are trying to apply Theorem B.1 to get bounds on the sample complexity m needed to translate
(µ, ρ) ∼RT(W, n, a, b) with small constant loss and small constant failure probability."
REFERENCES,0.2835249042145594,"First, since |X| = |Y| = |W|n, the above bound is meaningless (bigger than 1) if Θ is the family of
all translators, as the set of all translators has log(|X|!) = O(|W|n · n log |W|) parameters which is
much larger than an. In other words, no free lunch."
REFERENCES,0.2848020434227331,"On the other hand, let us consider the family of word-for-word translators Θw, which work by translat-
ing each word in a text separately, ignoring the surrounding context. The number of such translators
is the same as the number of word-to-word permutations, i.e., |Θw| = |W|!. So Theorem B.1 gives
a sample complexity bound of m = O(log W!) = O(|W| log |W|). The number of words in the
English language is about N ≈105. The quantity a is equal to what is called the perplexity of µ,
the effective number of words that may typically appear after a random prefix. For a constant a, the
minimum text (or communication) length n = O(log N) needed is thus logarithmic in the vocabulary
size. While word-for-word translators are poor, this analysis still sheds some light on the possible
asymptotic behavior (barring computational constraints) of translation, at least in a simplistic model."
REFERENCES,0.28607918263090676,"Common tree analysis.
The tree model helps us demonstrate the generality of the common
nonsense model. Instead of the fixed arities a ≤b in the tree model, one can consider an arbitrary
language tree over plausible texts P and an arbitrary subset T ⊆P corresponding to a subtree of
ground-truth translations. The common nonsense model implies that if the sets are perturbed by
removing, say, a α = 0.01 common fraction of translations, then with high probability MLE’s error
decreases at an ˜O(1/ min(m, |T|)) rate. Note that Theorem 3.4 does not directly apply to the random
tree LM because in that LM, the choice of which branches to include are not independent due to the
fixed a-arity constraint."
REFERENCES,0.28735632183908044,"B.2
Proof of Theorem B.1"
REFERENCES,0.2886334610472541,"The proof of Theorem B.1 follows from Lemma B.2 below. In this section, we will prove this lemma
and then derive the theorem from it.
Lemma B.2 (RT conditions). Consider any vocabulary set W, tree arities a ≤b ≤|W|/4, tree
depth n ∈N. Then, for any δ ∈(0, 1) and γ ≥4a−n ln(4|Θ|/δ), with probability ≥1 −δ, (µ, ρ)
give,
Pr
(µ,ρ)∼RT [εγ ≤16γ] ≥1 −δ.
(4)"
REFERENCES,0.28991060025542786,"Moreover, any µ, ρ sampled from RT(W, n, a, b) satisfy Prx∼µ[ρ(f⋆(x)) = 0] = 0 as needed for the
realizability Condition 2.1."
REFERENCES,0.29118773946360155,"The proof of Lemma B.2 requires two additional lemmas, stated and proved next. The first is a known
variant of the Chernoff bound for so-called 0-negatively correlated random variables.
Lemma B.3 (Chernoff bound for 0-negatively correlated random variables). Let Z1, . . . , Zn be
0-negatively correlated Boolean random variables, that is, they satisfy"
REFERENCES,0.29246487867177523,"∀I ⊆[n]
Pr [∀i ∈I Zi = 0] ≤
Y"
REFERENCES,0.2937420178799489,"i∈I
Pr [Zi = 0] ."
REFERENCES,0.2950191570881226,"Then, letting Z := Pn
i=1 Zi it holds that"
REFERENCES,0.2962962962962963,"Pr

Z ≤E[Z] 2"
REFERENCES,0.29757343550446996,"
≤e−E[Z] 8"
REFERENCES,0.2988505747126437,"Lemma B.3 follows from [14, Theorem 1.10.24(a)] with δ := 1/2, ai := 0, bi := 1 and Yi := Xi.
That theorem, in turn, is simply Theorem 1.10.10 for 0-negatively correlated random variables."
REFERENCES,0.3001277139208174,The second lemma used for the proof of Lemma B.2 is a combinatorial argument that we prove below.
REFERENCES,0.30140485312899107,"Lemma B.4. Given any π : Y ,→Y, it is possible to partition A = {y ∈Y | π(y) ̸= y} into four
disjoint sets A = A1 ∪A2 ∪A3 ∪A4 such that, for each i = 1, 2, 3, 4, the following two conditions
hold:"
REFERENCES,0.30268199233716475,π(Ai) ∩Ai = ∅
REFERENCES,0.30395913154533843,∀z ∈Wn−1 |{y ∈Ai | y begins with z}| ≤|W| 2
REFERENCES,0.3052362707535121,"Proof. of Lemma B.4. We will partition A greedily to achieve the two conditions. Begin with four
empty sets A1 = A2 = A3 = A4 = ∅. For each y ∈A in turn, assign it to one of the 4 sets as
follows."
REFERENCES,0.3065134099616858,"1. Let i be the index of the set Ai such that π(y) ∈Ai has already been assigned to. If π(y))
has not yet been assigned, let i = 1."
REFERENCES,0.30779054916985954,"2. Similarly, if π(y) ∈Aj has been assigned, let j be its index otherwise j = 1."
REFERENCES,0.3090676883780332,"3. Let z be the first n −1 elements of x. Let k be the index of the set Ak such that |{y ∈Ai |
y begins with z}| ≥|W|/2 if there is such a set, otherwise k = 1. Note that there can be at
most one such k because there are at most |W| −1 other elements beginning with z that
have been assigned already."
REFERENCES,0.3103448275862069,"Thus S = {1, 2, 3, 4} \ {i, j, k} ̸= ∅and we can assign x to any (say the minimum) element in that
set. By induction, we preserve the two properties stated in the lemma."
REFERENCES,0.3116219667943806,"With Lemmas B.3 and B.4 in hand, we are ready to prove Lemma B.2."
REFERENCES,0.3128991060025543,"Proof. of Lemma B.2. The realizability Condition 2.1, Prx∼µ[ρ(f⋆(x)) = 0] = 0, follows immedi-
ately from the fact that ρ is uniform over P and that τ is supported on T ⊆P. To prove the lemma, it
suffices to show that for any γ ≥4a−n ln(4|Θ|/δ),"
REFERENCES,0.31417624521072796,"Pr
(µ,ρ)∼RT"
REFERENCES,0.31545338441890164,"
∃θ ∈Θ err(θ) > 16γ, Pr
y∼τ[ρ(πθ(y)) = 0] ≤γ

≤δ."
REFERENCES,0.3167305236270754,"Note that err(θ) = Pry∼τ[y ̸= πθ(y)], and that ρ(πθ(y)) = 0 if and only if πθ(y) /∈P. Therefore,
by the union bound over θ ∈Θ, it suffices to show that, for each θ ∈Θ,"
REFERENCES,0.31800766283524906,"Pr
(µ,ρ)∼RT"
REFERENCES,0.31928480204342274,"
Pr
y∼τ[y ̸= πθ(y)] > 16γ, Pr
y∼τ[πθ(y) /∈P] ≤γ

≤
δ
|Θ|.
(5)"
REFERENCES,0.3205619412515964,"We will show that, more generally, for any permutation π : Y ,→Y. Equation (5) can be restated
in terms of ambiguous texts A := {y ∈Y | π(y) ̸= y} and implausible ambiguities B := {y ∈Y |
π(y) /∈P} :"
REFERENCES,0.3218390804597701,"Pr
(µ,ρ)∼RT [τ(A) > 16γ, τ(B) ≤γ] <
δ
|Θ|."
REFERENCES,0.3231162196679438,"Using Lemma B.4, partition A into four disjoint sets A = A1 ∪A2 ∪A3 ∪A4 such that for each
i ∈[4], the following two conditions hold:"
REFERENCES,0.3243933588761175,"π(Ai) ∩Ai = ∅
(6)"
REFERENCES,0.32567049808429116,"∀z ∈Wn−1
|{y ∈Ai | y begins with z}| ≤|W|"
REFERENCES,0.3269476372924649,"2 .
(7)"
REFERENCES,0.3282247765006386,"By the union bound, it suffices to show that,"
REFERENCES,0.32950191570881227,"∀i ∈[4]
Pr
(µ,ρ)∼RT [τ(Ai) > 4γ, τ(B) ≤γ] <
δ
4|Θ|,"
REFERENCES,0.33077905491698595,"because τ(A) = P4
i=1 τ(Ai), so if τ(A) > 16γ then it must follow that τ(Ai) > 16γ/4 for some i.
It suffices to show that this holds conditioned on any value of Ai ∩T:"
REFERENCES,0.33205619412515963,"∀V ⊆Ai
Pr
(µ,ρ)∼RT [τ(Ai) > 4γ, τ(B) ≤γ | Ai ∩T = V ] ≤
δ
4|Θ|.
(8)"
REFERENCES,0.3333333333333333,"Thus, fix any V ⊆Ai. We proceed by analyzing two cases, based on |V | versus |T| = an.13"
REFERENCES,0.334610472541507,"Case 1:
|V | ≤4γ|T|. Then Equation (8) holds with probability 0, because conditioning on
Ai ∩T = V , we have"
REFERENCES,0.33588761174968074,τ(Ai) := |Ai ∩T|
REFERENCES,0.3371647509578544,"|T|
= |V |"
REFERENCES,0.3384418901660281,|T| ≤4γ.
REFERENCES,0.3397190293742018,"Case 2:
|V | > 4γ|T|. For each v ∈V we define a Boolean random variable Zv = 1v∈B, i.e.,"
REFERENCES,0.34099616858237547,"Zv =
1
v ∈B
0
v /∈B."
REFERENCES,0.34227330779054915,"We claim that the Zv’s satisfy the condition of Lemma B.3, which follows inductively from the fact
any subset of Zv’s being 0 only makes it less likely for another Zv to be 0, because the way that
the Zv’s are chosen is such that there are exactly b many 1’s among the Zv’s (and other symmetric
random variables which we have not named)."
REFERENCES,0.34355044699872284,"Therefore, if we define Z := P"
REFERENCES,0.3448275862068966,"v∈V Zv and ζ := E[Z | Ai ∩T = V ], Lemma B.3 gives,"
REFERENCES,0.34610472541507026,"Pr

Z ≤ζ 2"
REFERENCES,0.34738186462324394,"Ai ∩T = V

≤e−ζ/8.
(9)"
REFERENCES,0.3486590038314176,We conclude by bounding both sides of Equation (9) to obtain Equation (8).
REFERENCES,0.3499361430395913,"• For the right-hand side of eq. (9), we claim that Pr [Zv = 1 | Ai ∩T = V ] ≥1/2 for each
v ∈V : Fix v and let"
REFERENCES,0.351213282247765,Q = {y ∈Y | y and π(v) agree on the first n −1 words}.
REFERENCES,0.3524904214559387,"So |Q| = |W| and |Q ∩P| = b. Equation (6) implies that π(v) /∈Ai, and eq. (7) implies
that |Q \ Ai| ≥|W|/2. Thus, there are ≥|W|/2 elements in Q \ Ai that, by symmetry, are
as likely to be in Q ∩P as π(v) is, and Q ∩P contains exactly b ≤|W|/4 elements, thus
the conditional probability that π(v) is in Q ∩P (equivalently, π(v) ∈P) is at most:"
REFERENCES,0.3537675606641124,"|W|/4
|W|/2 ≤1 2."
REFERENCES,0.3550446998722861,"Since π(v) /∈P is equivalent to Zv = 1, this means that Pr [Zv = 1 | Ai ∩T = V ] ≥1/2,
and hence:"
REFERENCES,0.3563218390804598,ζ = |V | · E[Zv | Ai ∩T = V ] = |V | · Pr [Zv = 1 | Ai ∩T = V ] ≥|V | 2 .
REFERENCES,0.35759897828863346,"13Note that while T is randomly sampled, |T| = an always, and therefore the case-analysis is valid."
REFERENCES,0.35887611749680715,"Because we are in the case that |V | > 4γ|T|, we have"
REFERENCES,0.36015325670498083,"ζ ≥|V | 2
> 1"
REFERENCES,0.3614303959131545,"2 · 4γ · |T| = 2γ · an.
(10)"
REFERENCES,0.36270753512132825,Therefore the right-hand side of eq. (9) satisfies
REFERENCES,0.36398467432950193,"e−ζ/8 ≤e−γan/4 ≥
δ
4 |Θ|,"
REFERENCES,0.3652618135376756,where the rightmost inequality holds because by our assumption that γ ≥4a−n ln(4|Θ|/δ).
REFERENCES,0.3665389527458493,"• For the left-hand side of eq. (9), note that"
REFERENCES,0.367816091954023,"Z :=
X"
REFERENCES,0.36909323116219667,"v∈V
Zv :=
X"
REFERENCES,0.37037037037037035,"v∈V
1v∈B = |V ∩B|."
REFERENCES,0.3716475095785441,"and therefore, using eq. (10), the left-hand side of eq. (9) satisfies"
REFERENCES,0.37292464878671777,"Pr

Z < ζ 2"
REFERENCES,0.37420178799489145,"Ai ∩T = V

≥Pr
|V ∩B|"
REFERENCES,0.37547892720306514,"|T|
≤γ
 Ai ∩T = V

."
REFERENCES,0.3767560664112388,"We claim that V ∩B ⊆B∩T: Due to the conditional event, we have V ∩B = Ai∩T ∩B ⊆
A ∩T ∩B. However, we argue that A ∩T ∩B ⊆T ∩B, i.e., that T ∩B ⊆A. Indeed,
by definition, if y ∈T ∩B then y ∈T but π(y) /∈P, and since P ⊇T, this implies that
π(y) ̸= y; that is, that y ∈A, as needed."
REFERENCES,0.3780332056194125,"We have just shown that V ∩B ⊆B ∩T, and therefore"
REFERENCES,0.3793103448275862,"Pr
|V ∩B|"
REFERENCES,0.38058748403575987,"|T|
≤γ
 Ai ∩T = V

≥Pr
|B ∩T|"
REFERENCES,0.3818646232439336,"|T|
≤γ
 Ai ∩T = V
"
REFERENCES,0.3831417624521073,= Pr [τ(B) ≤γ | Ai ∩T = V ] .
REFERENCES,0.384418901660281,"This shows that the left-hand side of eq. (9) upper bounds that of eq. (8), and concludes the
proof."
REFERENCES,0.38569604086845466,"Lastly, we prove Theorem B.1 using Lemma B.2"
REFERENCES,0.38697318007662834,"Proof. of Theorem B.1. Let γ be,"
REFERENCES,0.388250319284802,"γ := max
 1 m, 4 an"
REFERENCES,0.3895274584929757,"
ln 6|Θ| δ
."
REFERENCES,0.39080459770114945,"Lemma B.2 below shows that with probability ≥1 −2δ/3 over µ, ρ, we have that εγ ≤16γ.
Theorem A.2 implies that with probability ≥1 −δ/3 over x1, . . . , xm, we have err(ˆθ) ≤εγ. Thus,
by the union bound, with probability ≥1 −δ we have"
REFERENCES,0.39208173690932313,"err(ˆθ) ≤εγ ≤16γ = 16 max
 1 m, 4 an"
REFERENCES,0.3933588761174968,"
ln 6|Θ| δ
."
REFERENCES,0.3946360153256705,"C
Proofs for the Common Nonsense Model"
REFERENCES,0.3959131545338442,"C.1
Upper bound in the common nonsense model"
REFERENCES,0.39719029374201786,"C.1.1
Proof overview"
REFERENCES,0.39846743295019155,"To convey intuition, we think of the case of say δ = 99%, and we omit constants in the following
discussion. Recall that Theorem A.2 asserts that with high probability, the learned translator ˆθ has
error at most εγ for γ = log |Θ|/ min(m, |T|) ≥log |Θ|/m. As such, the main technical challenge"
REFERENCES,0.3997445721583653,"in this proof is to show that, with high probability over the generated source language µ and the prior
ρ, it holds that
εγ ≲γ α."
REFERENCES,0.40102171136653897,"By definition of εγ, we ought to show that w.h.p over µ and ρ, any θ ∈Θ with large semantic error
must have many translations fθ(x) deemed implausible by ρ. Slightly more formally: Since any
y ∈Y is implausible (ρ(y) = 0) only if y /∈S, a union bound over θ ∈Θ means that is suffices to
show that"
REFERENCES,0.40229885057471265,"Pr
S,µ,ρ"
REFERENCES,0.40357598978288634,"
Pr
x∼µ[fθ(x) ̸= f⋆(x)] ≲γ"
REFERENCES,0.40485312899106,"α, Pr
x∼µ [fθ(x) /∈S] ≳γ

≲exp(−γ|T|)) ≤
1
|Θ|,"
REFERENCES,0.4061302681992337,"where the right inequality is by choice of γ := log |Θ|/ min(m, |T|). The above inequality “looks
like” it could be proven by a Chernoff bound, but a closer look reveals a subtle flaw with this
argument."
REFERENCES,0.4074074074074074,"To use a Chernoff bound, we’d first want to fix (i.e., condition on) each supp(µ) := f −1
⋆(S ∩T)
and then use Chernoff over the conditional random variables 1fθ(x)/∈S for each x ∈supp(µ) such
that fθ(x) ̸= f⋆(x). Unfortunately, these conditional random variables are not independent. To see
this, consider the case that fθ(x) = f⋆(x′) for two different x ̸= x′. Then, since we are considering
x′ ∈supp(µ), we have fθ(x) = f⋆(x′) ∈S with probability 1."
REFERENCES,0.4086845466155811,"To avoid this dependency, we prove a combinatorial lemma showing that it is possible to partition the
set
A := {x ∈X | fθ(x) ̸= f⋆(x)}
into three parts A = A1 ∪A2 ∪A3 such that fθ(Ai) ∩f⋆(Ai) = ∅for each i ∈[3]. This resolves
the dependency issue demonstrated above. We then proceed by applying a Chernoff bound separately
for each Ai, which suffices since a union bound (over i ∈[3]) loses only a constant factor in the
upper-bound."
REFERENCES,0.4099616858237548,"C.1.2
The full proof"
REFERENCES,0.4112388250319285,"The proof of Theorem 3.4 follows from the following main lemma. We first prove this lemma, and
then show how the theorem follows from it.
Lemma C.1. Let α, δ ∈(0, 1) and T ⊆P ⊆Y. Then, for any γ ≥
8
(1−α)·|T | ln 4|Θ| δ :"
REFERENCES,0.4125159642401022,"Pr
(µ,ρ)∼DP,T
α"
REFERENCES,0.41379310344827586,"
εγ ≤6γ α"
REFERENCES,0.41507024265644954,"
≥1 −δ."
REFERENCES,0.4163473818646232,"The proof of Lemma C.1 relies on a simple combinatorial proposition. This proposition is a special
case of Lemma B.4, but since it is much simpler we give a self-contained proof.
Proposition C.2. For any finite set Y and any π : Y ,→Y, it is possible to partition {y ∈Y|π(y) ̸=
y} = A1 ∪A2 ∪A2 into three sets such that, π(Ai) ∩Ai = ∅for i = 1, 2, 3."
REFERENCES,0.41762452107279696,"Proof. Let S := {y ∈Y | π(y) ̸= y}. We proceed iteratively, dividing each (non-trivial) cycle of π
separately into the three Ai’s: Fix a cycle {s1, s2, . . . , sn} such that π(si) = si+1 and π(sn) = s1.
If n is even we can just partition it into two sets: put the si for even i’s into A1 and odd i’s into A2.
If n is odd, we can do the same except put the last element sn into A3."
REFERENCES,0.41890166028097064,We can now prove Lemma C.1.
REFERENCES,0.42017879948914433,"Proof. of Lemma C.1. Note that the lemma holds trivially for any γ > 1/6 because we always have
εγ ≤1. Assume that γ ≤1/6. Let a := 6"
REFERENCES,0.421455938697318,"αγ. The probabilities in this proof are over the choice of
S ⊆Y. It suffices to show that, Pr
S"
REFERENCES,0.4227330779054917,"h
∃θ ∈Θ Pr
τ [πθ(y) ̸= y] > a ∧Pr
τ [πθ(y) /∈S] ≤γ
i
≤δ,
(11)"
REFERENCES,0.4240102171136654,"because ρ(πθ(y)) = 0 whenever πθ(y) /∈S, thus Aγ = {θ ∈Θ | Prτ[πθ(y) /∈S] ≤γ}. The set
S determines the perturbed sets ˜T := T ∩S and ˜P := P ∩S and the distributions τ = U( ˜T) and
ρ = U( ˜P)."
REFERENCES,0.42528735632183906,"Define β := 1 −α. Since E

| ˜T|

= β|T|, a multiplicative Chernoff bound14 gives Pr
S"
REFERENCES,0.42656449553001274,"
| ˜T| ≤β"
REFERENCES,0.4278416347381865,"2 |T|

≤exp

−β"
REFERENCES,0.42911877394636017,"8 |T|

≤exp

−β 8 · 8"
REFERENCES,0.43039591315453385,βγ ln 4|Θ| δ
REFERENCES,0.43167305236270753,"
<
δ
4|Θ| ≤δ 4."
REFERENCES,0.4329501915708812,"Thus, to show eq. (11), it suffices to show that"
REFERENCES,0.4342273307790549,"Pr

| ˜T| ≥β"
REFERENCES,0.4355044699872286,"2 |T| ∧∃θ ∈Θ Pr
τ [πθ(y) ̸= y] > a ∧Pr
τ [πθ(y) /∈S] ≤γ

≤3δ"
REFERENCES,0.4367816091954023,"4 .
(12)"
REFERENCES,0.438058748403576,"By the union bound, to show eq. (12) it thus suffices to show that for any π : Y ,→Y,"
REFERENCES,0.4393358876117497,"Pr

| ˜T| ≥β"
REFERENCES,0.44061302681992337,"2 |T| ∧Pr
τ [π(y) ̸= y] > a ∧Pr
τ [π(y) /∈S] ≤γ

≤
3δ
4|Θ|.
(13)"
REFERENCES,0.44189016602809705,"Fix any π : Y ,→Y. By Lemma C.2, we can partition {y ∈Y | π(y) ̸= y} = A1 ∪A2 ∪A3
such that π(Ai) ∩Ai = ∅, and hence Prτ[π(y) ̸= y] = P"
REFERENCES,0.44316730523627074,"i τ(Ai). So if Prτ[π(y) ̸= y] > a, then
τ(Ai) > a/3 for some i. Therefore, it suffices to show that"
REFERENCES,0.4444444444444444,"Pr

| ˜T| ≥β"
REFERENCES,0.44572158365261816,2 |T| ∧∃i ∈[3] τ(Ai) > a
REFERENCES,0.44699872286079184,"3 ∧Pr
τ [π(y) /∈S] ≤γ

≤
3δ
4|Θ|."
REFERENCES,0.4482758620689655,"With a union bound over i ∈[3], it suffices to show that for each i ∈[3]"
REFERENCES,0.4495530012771392,"Pr

| ˜T| ≥β"
REFERENCES,0.4508301404853129,2 |T| ∧τ(Ai) > a
REFERENCES,0.4521072796934866,"3 ∧Pr
τ [π(y) /∈S] ≤γ

≤
δ
4|Θ|.
(14)"
REFERENCES,0.45338441890166026,"Thus, now in addition to fixing π, we fix i ≤3, thus fixing Ai. To continue, imagine we are picking
S by first selecting Ai ∩S, and subsequently selecting S \ Ai. We will show that Equation (14)
holds when conditioning on each possible value for the first selection, that is, each possible Ai ∩S.
Formally, we fix V ⊆Ai and condition on Ai ∩S = V , claiming that"
REFERENCES,0.454661558109834,"Pr

| ˜T| ≥β"
REFERENCES,0.4559386973180077,2 |T| ∧τ(Ai) > a
REFERENCES,0.45721583652618136,"3 ∧Pr
τ [π(y) /∈S] ≤γ
V = Ai ∩S

≤
δ
4|Θ|.
(15)"
REFERENCES,0.45849297573435505,"First, observe that"
REFERENCES,0.45977011494252873,τ(Ai) = |Ai ∩˜T|
REFERENCES,0.4610472541507024,"| ˜T|
= |Ai ∩S|"
REFERENCES,0.4623243933588761,"| ˜T|
= |V |"
REFERENCES,0.46360153256704983,"| ˜T|
,"
REFERENCES,0.4648786717752235,"therefore if |V | < βa|T|/6 then Equation (15) holds with probability 0 (due to the first two events in
the conjunction). Thus, we can assume that |V | ≥βa|T|/6."
REFERENCES,0.4661558109833972,"Note that τ(V ) = τ(Ai), and that τ(Ai) ≥a/3 implies"
REFERENCES,0.4674329501915709,"Pr
y∼τ[π(y) /∈S] ≥Pr
y∈V [π(y) /∈S] · τ(V ) ≥Pr
y∈V [π(y) /∈S] · a 3,"
REFERENCES,0.46871008939974457,therefore the left-hand side of Equation (15) is upper-bounded by
REFERENCES,0.46998722860791825,"Pr

Pr
y∈V [π(y) /∈S] ≤3 · γ a"
REFERENCES,0.47126436781609193,"V = Ai ∩S

= Pr

Pr
y∈V [π(y) /∈S] ≤a 2"
REFERENCES,0.4725415070242657,"V = Ai ∩S

.
(16)"
REFERENCES,0.47381864623243936,"We conclude the proof by upper-bounding Equation (16) with a Chernoff bound. Consider the
random variables Zy = 1π(y)/∈S for y ∈V . These random variables are independent by definition of
α-common-nonsense. Furthermore, due to the fact that π(Ai) ∩Ai = ∅, they remain independent
even when conditioning on the event Ai ∩S = V . By linearity of expectation, E  X"
REFERENCES,0.47509578544061304,"y∈V
Zy"
REFERENCES,0.4763729246487867,V = Ai ∩S 
REFERENCES,0.4776500638569604,= α|V |.
REFERENCES,0.4789272030651341,"14Specifically, that the probability that a sum of binary random variables is less than half its mean β|T| is at
most exp(−β|T|/8)."
REFERENCES,0.48020434227330777,"Using the same Chernoff bound as above, we have Pr
P"
REFERENCES,0.48148148148148145,y∈V Zy
REFERENCES,0.4827586206896552,"|V |
≤α 2"
REFERENCES,0.4840357598978289,"V = Ai ∩S

≤exp(−α|V |/8)"
REFERENCES,0.48531289910600256,"Noting that P Zy/|V | = Pry∈V [π(y) /∈S], we conclude that that Equation (16) is upper-bounded
by"
REFERENCES,0.48659003831417624,"exp

−α|V | 8"
REFERENCES,0.4878671775223499,"
≤exp

−αβa|T| 48"
REFERENCES,0.4891443167305236,"
≤
δ
4|Θ|."
REFERENCES,0.4904214559386973,"This proves the inequality in Equation (15), thereby concluding the proof."
REFERENCES,0.49169859514687103,"Finally, with the main technical lemma in hand, we prove Theorem 3.4."
REFERENCES,0.4929757343550447,"Proof. of Theorem 3.4. The proof is very similar to that in Appendix B. First, the realizability
Condition 2.1, Prx∼µ[ρ(f⋆(x)) = 0] = 0, follows immediately from the fact that µ, ρ are uniform
distributions with f⋆(supp(µ)) ⊆supp(ρ) which follows from the fact that T ⊆P and the definitions
of µ, ρ."
REFERENCES,0.4942528735632184,"Let β := 1 −α and define γ by,"
REFERENCES,0.4955300127713921,"γ := max
 1"
REFERENCES,0.49680715197956576,"m,
8
β|T|"
REFERENCES,0.49808429118773945,"
ln 6|Θ| δ
."
REFERENCES,0.49936143039591313,"Lemma C.1 below shows that with probability ≥1 −2δ/3 over µ, ρ, we have that εγ ≤6γ/α.
Theorem A.2 implies that with probability ≥1 −δ/3 over x1, . . . , xm, we have err(ˆθ) ≤εγ. Thus,
by the union bound, with probability ≥1 −δ we have"
REFERENCES,0.5006385696040868,err(ˆθ) ≤εγ ≤6γ α = 6
REFERENCES,0.5019157088122606,"α max
 1"
REFERENCES,0.5031928480204342,"m,
8
β|T|"
REFERENCES,0.5044699872286079,"
ln 6|Θ| δ
."
REFERENCES,0.5057471264367817,"C.2
Lower bound in the common nonsense model"
REFERENCES,0.5070242656449553,"The proof of the lower bound works by creating log n candidate “plausible ambiguities“ and arguing
that a constant fraction of survive the random removal of elements."
REFERENCES,0.508301404853129,"Proof. of Theorem 3.5. The constants c1, c2 will also be determined through this proof to be large
enough to satisfy multiple conditions defined below. No effort has been made to minimize the
constants in this proof."
REFERENCES,0.5095785440613027,Let n := |Θ|.
REFERENCES,0.5108556832694764,"We will lay out two a × b grids X ⊆X and Y ⊆Y, for:"
REFERENCES,0.51213282247765,"a := ⌊log n⌋, b :=
 1"
REFERENCES,0.5134099616858238,"α max

1,
|T|
105m 
."
REFERENCES,0.5146871008939975,"For integer t, denote [t] := {1, 2, . . . , t}. For i ∈[a], j ∈[b], choose distinct elements xij ∈X and
yij ∈T. To ensure this is even possible, we must make sure ab ≤|T|, which holds because we
assumed log n ≤α min(m, |T|) thus,"
REFERENCES,0.5159642401021711,"ab ≤α min(m, |T|) 1"
REFERENCES,0.5172413793103449,"α max

1, |T| m"
REFERENCES,0.5185185185185185,"
= min(m, |T|) · max
 1"
REFERENCES,0.5197956577266922,"|T|, 1 m"
REFERENCES,0.5210727969348659,"
|T| = |T|."
REFERENCES,0.5223499361430396,"Let X := {xij | i ∈[a], j ∈[b]} and Y := {yij | i ∈[a], j ∈[b]}. Let h : X \ X ,→Y \ Y be a
fixed 1–1 mapping, say the lexicographically smallest. The parametrized translator family is defined
by,"
REFERENCES,0.5236270753512133,"Θ′ = Θm,n,Y := {−1, 1}a, fθ(xij) :=
yij,
θi = 1
yi(j+1 mod b),
θi = −1 , ∀x /∈X fθ(x) = h(x)."
REFERENCES,0.524904214559387,"Clearly |Θ′| = 2a ≤n = |Θ| as needed. Let x = (x1, x2, . . . , xm). It suffices to show:"
REFERENCES,0.5261813537675607,"Pr
S,x"
REFERENCES,0.5274584929757343,"
err
 ˆθ

≥
log n
c3α min(105m, |T|)"
REFERENCES,0.5287356321839081,"
≥0.99,"
REFERENCES,0.5300127713920817,"for some constant c3 sufficiently large, because we can set c2 = 105c3. The above equation is
equivalent to the following two cases based on m:"
REFERENCES,0.5312899106002554,"Case 1. 105m > |T| =⇒Pr
S,x"
REFERENCES,0.5325670498084292,"
err
 ˆθ

≥log n"
REFERENCES,0.5338441890166028,c3α|T|
REFERENCES,0.5351213282247765,"
≥0.99
(17)"
REFERENCES,0.5363984674329502,"Case 2. 105m ≤|T| =⇒Pr
S,x"
REFERENCES,0.5376756066411239,"
err
 ˆθ

≥
log n
c3α105m"
REFERENCES,0.5389527458492975,"
≥0.99
(18)"
REFERENCES,0.5402298850574713,"In both cases, it will be convenient to notice that, for any z ≥2, ⌊z⌋≥z −1 ≥z"
REFERENCES,0.541507024265645,"2. Since b ≥2
(because α ≤1/2), we therefore have
1
2α max

1,
|T|
105m"
REFERENCES,0.5427841634738186,"
≤b ≤1"
REFERENCES,0.5440613026819924,"α max

1,
|T|
105m"
REFERENCES,0.545338441890166,"
(19)"
REFERENCES,0.5466155810983397,"Case 1:
105m > T. In this case
1
2α ≤b ≤1"
REFERENCES,0.5478927203065134,"α by Equation (19). We will show Equation (17). Now,
consider the “full rows”:
C(S) := {i ∈[a] | ∀j ∈[b] yij ∈S}.
These rows will be useful to consider because nothing has been removed from the entire row, no
information about θi has been revealed and (on average) one cannot achieve error < 1/2 on these
examples, because one cannot distinguish between the two permutations on this row."
REFERENCES,0.5491698595146871,"Note that the membership of different i, i′ ∈C(S) is independent since S is chosen independently,
and by definition of C and S:"
REFERENCES,0.5504469987228607,"E[|C(S)|] = (1 −α)ba ≥(1 −α)1/αa ≥a 4,"
REFERENCES,0.5517241379310345,"since (1 −α)1/α is decreasing in α and α ≤1/2. Thus, by multiplicative Chernoff bounds
(specifically, Pr[Z ≤E[Z]/2] ≤e−E[Z]/8), Pr
S"
REFERENCES,0.5530012771392082,"h
|C(S)| ≤a 8"
REFERENCES,0.5542784163473818,"i
≤e−a/32 ≤e−⌊c1⌋/32 ≤0.001,
(20)"
REFERENCES,0.5555555555555556,"for sufficiently large c1. Thus, PrS[|C(S)| > a/8] ≥0.999. Let C′(S, x) ⊆C(S) be those i which
ˆθi ̸= ⋆i,
C′(S, x) := {i ∈C(S) | ˆθi ̸= ⋆i}.
Clearly, for any algorithm and any C(S), Ex[|C′(S, x)| | S] = |C(S)|/2 because no information
whatsoever has been revealed about θi for any i ∈C. Thus, by the same Chernoff bound, we have:"
REFERENCES,0.5568326947637292,"Pr
S,x"
REFERENCES,0.558109833971903,"
|C′(S, x)| ≤1"
REFERENCES,0.5593869731800766,"4|C(S)|
 |C(S)| > a 8"
REFERENCES,0.5606641123882503,"
≤e−a"
REFERENCES,0.561941251596424,16 · 1
REFERENCES,0.5632183908045977,"8 ≤0.001,"
REFERENCES,0.5644955300127714,"for sufficiently large c1, because a ≥c1. By the union bound over this and Equation (20),"
REFERENCES,0.565772669220945,"Pr
S,x"
REFERENCES,0.5670498084291188,"h
|C′(S, x)| ≥a 32"
REFERENCES,0.5683269476372924,"i
≥0.998."
REFERENCES,0.5696040868454662,"Since each row i ∈C′ incurs b errors on examples x, one for each j because f⋆(xij) ∈S:"
REFERENCES,0.5708812260536399,"err
 ˆθ

≥b ·
C′(S, x) |T|
. Thus,"
REFERENCES,0.5721583652618135,"Pr
S,x"
REFERENCES,0.5734355044699873,"
err
 ˆθ

≥
ba
32α|T|"
REFERENCES,0.5747126436781609,"
≥0.998."
REFERENCES,0.5759897828863346,"Now, a ≥1"
REFERENCES,0.5772669220945083,"2 log n for sufficiently large c1 and as mentioned b ≥
1
2α. Thus,"
REFERENCES,0.578544061302682,"Pr
S,x"
REFERENCES,0.5798212005108557,"
err
 ˆθ

≥
log n
128α|T|"
REFERENCES,0.5810983397190294,"
≥0.998."
REFERENCES,0.5823754789272031,This establishes Equation (17) as long as c3 ≥128.
REFERENCES,0.5836526181353767,It remains to prove Equation (18).
REFERENCES,0.5849297573435505,"Case 2:
105m ≤T. In this case
|T |
2α105m ≤b ≤
|T |
α105m by Equation (19). Next, consider the set of
rows with at least 1/2 of the elements in S:"
REFERENCES,0.5862068965517241,"D(S) :=

i ∈[a]

{j ∈[b] | yij ∈S}
 ≥b 2 
."
REFERENCES,0.5874840357598978,"Intuitively, any row i ∈D(S) is “dangerous” in the sense that if ˆθi ̸= ⋆i, then it causes errors on
b/2 different x’s in the support of µ, i.e., for which f⋆(xij) ∈S. Observe that E[|D(S)|] ≥a/2
since each size s =
{j ∈[b] | yij ∈S}
 ≥b/2 is at least as likely as the size b −s, since α ≤1/2.
And also, membership of i, i′ ∈D(S) since S is independent. Thus, by the same Chernoff bound as
above, for sufficiently large c1, Pr
S"
REFERENCES,0.5887611749680716,"h
|D(S)| ≤a 4"
REFERENCES,0.5900383141762452,"i
≤e−a/16 ≤e−⌊c1⌋/16 ≤0.001.
(21)"
REFERENCES,0.5913154533844189,"Let −θ := (−θ1, −θ2, . . . , −θa). This makes it convenient to define the giveaways G(S) ⊆X to be,"
REFERENCES,0.5925925925925926,"G(S) := {xij | i ∈[a], j ∈[b], f⋆(xij) ∈S, f−⋆(xij) /∈S}."
REFERENCES,0.5938697318007663,"These are the points xij which we might observe f⋆(xij) which would imply that θi = ⋆i (and not
its negative). Also let,
ˆG(S, x) = {x1, x2, . . . , xm} ∩G(S)."
REFERENCES,0.5951468710089399,"(Note that if for a give row i, we do not have any xij ∈ˆG(S, x), then we have no information about
θi. As a preview to what is to come, we now argue that with high probability | ˆG(S, x)| < a/8 which
will mean that, if |D(S)| > a/4, then we have no information about θi for at least a/8 of the rows
i ∈D(S).)"
REFERENCES,0.5964240102171137,"For any fixed i, j, observe that Pr[xij ∈G(S)] = α(1 −α) so E[|G(S)|] = α(1 −α)ab ≤αab. By
the Chernoff bound that Pr[Z ≥2 E[Z]] ≤e−E[Z]/3,"
REFERENCES,0.5977011494252874,"Pr
S [|G(S)| ≥2αab] ≤e−αab/3 ≤e−a/3 ≤0.001,"
REFERENCES,0.598978288633461,"for sufficiently large c1. (We have used the fact that the above probability is smaller than if E[|G(S)|]
were actually αab.)"
REFERENCES,0.6002554278416348,"Also, ES[|T ∩S|] ≥|T|/2 since α ≤1/2. So, by the Chernoff bound Pr[Z ≤E[Z]/2] ≤e−E[Z]/8, Pr
S"
REFERENCES,0.6015325670498084,"
|T ∩S| ≤|T| 4"
REFERENCES,0.6028097062579821,"
≤e−|T |/16 ≤0.001,"
REFERENCES,0.6040868454661558,for sufficiently large c1 since |T| ≥log n ≥c1.
REFERENCES,0.6053639846743295,"Thus, by the union bound: Pr
S"
REFERENCES,0.6066411238825032,"
|T ∩S| ≤|T|"
REFERENCES,0.6079182630906769,"4 ∨|G(S)| ≥2αab

≤0.002.
(22) Also, E
x"
REFERENCES,0.6091954022988506,"h
| ˆG(S, x)| | S
i
≤m |G(S)|"
REFERENCES,0.6104725415070242,|T ∩S|.
REFERENCES,0.611749680715198,"Thus, using Markov’s inequality in the second line below, E
x,S"
REFERENCES,0.6130268199233716,"
| ˆG(S, x)|
 |T ∩S| > |T|"
REFERENCES,0.6143039591315453,"4 , |G(S)| ≤2αab

≤m 2αab"
REFERENCES,0.6155810983397191,"|T|/4 = 8αabm |T|
,"
REFERENCES,0.6168582375478927,"Pr
x,S"
REFERENCES,0.6181353767560664,"
| ˆG(S, x)| > 8000αabm |T|"
REFERENCES,0.6194125159642401,|T ∩S| > |T|
REFERENCES,0.6206896551724138,"4 , |G(S)| ≤2αab

≤0.001."
REFERENCES,0.6219667943805874,"By the union bound over the above and Equation (22), since Pr[E] ≤Pr[E|F] + Pr[¬F]"
REFERENCES,0.6232439335887612,"Pr
x,S"
REFERENCES,0.6245210727969349,"
| ˆG(S, x)| > 8000αabm |T|"
REFERENCES,0.6257982120051085,"
≤0.001 + 0.002."
REFERENCES,0.6270753512132823,"Finally, since"
REFERENCES,0.6283524904214559,"b ≤
|T|
α105m
8000αabm"
REFERENCES,0.6296296296296297,"|T|
≤0.08 · a ≤a 8"
REFERENCES,0.6309067688378033,"Pr
x,S"
REFERENCES,0.632183908045977,"h
| ˆG(S, x)| > a 8"
REFERENCES,0.6334610472541508,"i
≤0.003"
REFERENCES,0.6347381864623244,"By the union bound with Equation (21),"
REFERENCES,0.6360153256704981,"Pr
x,S"
REFERENCES,0.6372924648786717,"h
|D(S)| ≤a"
REFERENCES,0.6385696040868455,"4 ∨| ˆG(S, x)| > a 8"
REFERENCES,0.6398467432950191,"i
≤0.004"
REFERENCES,0.6411238825031929,"Let
F(S, x) := {i ∈D(S) | ∀j ∈[b] xij /∈ˆG(S)(S, x)} ⊆D."
REFERENCES,0.6424010217113666,"Clearly |F(S, x)| ≥|D(S)| −| ˆG(S, x)| because each x ∈ˆG(S, x) can remove at most one row
from D(S). Thus,"
REFERENCES,0.6436781609195402,"Pr
x,S"
REFERENCES,0.644955300127714,"h
|F(S, x)| ≤a 8"
REFERENCES,0.6462324393358876,"i
≤0.004.
(23)"
REFERENCES,0.6475095785440613,"F(S, x) will function exactly like C in the analysis above of Equation (17). We repeat this analysis
for completeness, replacing C by F. Let F ′(S, x) ⊆F(S, x) be those i which ˆθi ̸= ⋆i,"
REFERENCES,0.648786717752235,"F ′(S, x) := {i ∈F(S, x) | ˆθi ̸= ⋆i}."
REFERENCES,0.6500638569604087,"For any algorithm and any F(S, x), Ex[|F ′(S, x)| | S] = |F(S, x)|/2 because no information
whatsoever has been revealed about θi for any i ∈F. Thus, by the same Chernoff bound, we have:"
REFERENCES,0.6513409961685823,"Pr
S,x"
REFERENCES,0.6526181353767561,"
|F ′(S, x)| ≤1"
REFERENCES,0.6538952745849298,"4|F(S, x)|
 |F(S, x)| > a 8"
REFERENCES,0.6551724137931034,"
≤e−a"
REFERENCES,0.6564495530012772,16 · 1
REFERENCES,0.6577266922094508,"8 ≤0.001,"
REFERENCES,0.6590038314176245,"for sufficiently large c1. By the union bound over this and Equation (23),"
REFERENCES,0.6602809706257982,"Pr
S,x"
REFERENCES,0.6615581098339719,"h
|F ′(S, x)| ≥a 32"
REFERENCES,0.6628352490421456,"i
≥0.995."
REFERENCES,0.6641123882503193,"Since each row i ∈F ′ incurs ≥b/2 errors on examples x by definition of F ′ and D, since F ′ ⊆D
and thus at least b/2 errors on j for which f⋆(xij) ∈S. Thus,"
REFERENCES,0.665389527458493,"err
 ˆθ

≥b ·
F ′(S, x)"
REFERENCES,0.6666666666666666,"2|T|
. Thus,"
REFERENCES,0.6679438058748404,"Pr
S,x"
REFERENCES,0.669220945083014,"
err
 ˆθ

≥
ba
64α|T|"
REFERENCES,0.6704980842911877,"
≥0.995."
REFERENCES,0.6717752234993615,"Now, a ≥1"
REFERENCES,0.6730523627075351,"2 log n for sufficiently large c1 and we also have b ≥
|T |
2α105m by Equation (19) since
|T |
α105m ≥2 since α ≤1/2 and we are in the case where 105m ≤|T|. Thus,"
REFERENCES,0.6743295019157088,"Pr
S,x"
REFERENCES,0.6756066411238825,"
err
 ˆθ

≥
log n
256α105m"
REFERENCES,0.6768837803320562,"
≥0.995."
REFERENCES,0.6781609195402298,This establishes Equation (18) for c3 ≥256 × 105.
REFERENCES,0.6794380587484036,"D
Proofs for random knowledge graph"
REFERENCES,0.6807151979565773,"Our goal in this section is to prove Theorem 3.2. The proof is based on the following main lemma.
We first state and prove this lemma, and then derive the theorem from it. Recall that the sets
T, P ⊆Y = Y × Y = Y 2 represent the edges of the two knowledge graphs."
REFERENCES,0.6819923371647509,"Lemma D.1. Fix ∅̸= S ⊆Y , π : Y 2 ,→Y 2, δ, p, α ∈(0, 1), q := 1 −p, and"
REFERENCES,0.6832694763729247,"ϵ ≥32 ·
ln(1/δ)
pα2q2|S|2 ."
REFERENCES,0.6845466155810983,"For any (T, P) ∼KG(S, Y, p, α) chosen from the random knowledge graph distribution, we define"
REFERENCES,0.685823754789272,"A := {y ∈T | π(y) ̸= y}
B := {y ∈A | π(y) /∈P}
C := {y ∈A | y /∈P}"
REFERENCES,0.6871008939974457,"Then,
Pr
T,P"
REFERENCES,0.6883780332056194,"h
|A| ≥ϵ|T| ∧|B| −|C| ≤αq"
REFERENCES,0.6896551724137931,"2 |A|
i
≤5δ."
REFERENCES,0.6909323116219668,"Proof. Let A′ := {y ∈S2 | π(y) ̸= y}, so A ⊆A′. If π is the identity then the lemma holds trivially,
therefore we can assume that A′ ̸= ∅. If ϵ > 1 then the lemma holds trivially as well, because A ⊆T
and therefore |A| ≤|T|, so we assume ϵ ∈(0, 1]."
REFERENCES,0.6922094508301405,"For each y ∈A′, Equations (1) and (2) under Definition 3.1 imply that Pr[y ∈C] = Pr[y ∈T \P] =
(1 −α)pq and Pr[y ∈A] = Pr[y ∈T] = p, thus Bayes rule gives"
REFERENCES,0.6934865900383141,∀y ∈A′ Pr[y ∈C | y ∈A] = (1 −α)pq
REFERENCES,0.6947637292464879,"p
= (1 −α)q."
REFERENCES,0.6960408684546615,"Now suppose we fix V ⊆A′ and condition on A := A′ ∩T = V . Note that the event y ∈C is
independent for different y ∈V , therefore for any y ∈V it holds that"
REFERENCES,0.6973180076628352,"Pr
T,P [y ∈C | A = V ] = Pr
T,P [y ∈C | y ∈A] = (1 −α)q."
REFERENCES,0.698595146871009,"Therefore, E[|C| | A = V ] = (1 −α)q|A|, and so a Chernoff bound gives"
REFERENCES,0.6998722860791826,"∀V ⊆A′
Pr
T,P """
REFERENCES,0.7011494252873564,|C| ≤(1 −α)q|A| + r
REFERENCES,0.70242656449553,"1
2|A| ln 1 δ A = V #"
REFERENCES,0.7037037037037037,≥1 −δ.
REFERENCES,0.7049808429118773,"(Normally, Chernoff bounds would give the tight inequality that Pr[|C| < . . .] ≥1 −δ, but the ≤
in the above is necessary for the case in which A = ∅in which case Chernoff bounds do not apply
because it would be over |A| = 0 coin flips.) Since this holds for every V , we have:"
REFERENCES,0.7062579821200511,"Pr
T,P """
REFERENCES,0.7075351213282248,|C| ≤(1 −α)q|A| + r
REFERENCES,0.7088122605363985,"1
2|A| ln 1 δ #"
REFERENCES,0.7100893997445722,"≥1 −δ.
(24)"
REFERENCES,0.7113665389527458,"By Lemma C.2, we can partition A′ into three disjoint sets,"
REFERENCES,0.7126436781609196,"A′ = A′
1 ∪A′
2 ∪A′
3 such that π(A′
i) ∩A′
i = ∅."
REFERENCES,0.7139208173690932,"As above, we are going to condition on the value of Ai := A′
i ∩T. Also, define,"
REFERENCES,0.7151979565772669,Bi := {y ∈Ai | π(y) /∈P} = B ∩Ai.
REFERENCES,0.7164750957854407,"Now, fix any i ∈[3] and any set V ⊆A′
i. We now claim that for all i ∈[3], V ∈A′
i, and y ∈V"
REFERENCES,0.7177522349936143,"Pr
T,P [y ∈Bi | Ai = V ] = Pr
T,P [π(y) /∈P | y ∈T] = q."
REFERENCES,0.719029374201788,"The rightmost equality follows from the fact that π(y) ̸= y so π(y) /∈P is independent of y ∈T.
The leftmost equality follows similarly: Since π(A′
i) ∩A′
i = ∅, the event Ai = V is independent of
π(y) /∈P. Thus, again by Chernoff bounds we have"
REFERENCES,0.7203065134099617,"∀i ∈[3] ∀V ⊆A′
i
Pr
T,P """
REFERENCES,0.7215836526181354,|Bi| ≥q|Ai| − r
REFERENCES,0.722860791826309,"1
2|Ai| ln 1 δ"
REFERENCES,0.7241379310344828,Ai = V #
REFERENCES,0.7254150702426565,≥1 −δ.
REFERENCES,0.7266922094508301,"Since this holds for all V , it holds unconditionally, and by the union bound it follows that"
REFERENCES,0.7279693486590039,"Pr
T,P """
REFERENCES,0.7292464878671775,∀i ∈[3] |Bi| ≥q|Ai| − r
REFERENCES,0.7305236270753512,"1
2|Ai| ln 1 δ #"
REFERENCES,0.7318007662835249,"≥1 −3δ.
(25)"
REFERENCES,0.7330779054916986,"Now, since the sets Bi partition B and Ai partition A, we have |B| = P
i |Bi| , |A| = P
i |Ai|
, and also P3
i=1
p"
REFERENCES,0.7343550446998723,"|Ai| ≤
p"
REFERENCES,0.735632183908046,"3|A| by Cauchy–Schwartz. Thus, summing the three equations in
Equation (25) probability implies"
REFERENCES,0.7369093231162197,"Pr
T,P """
REFERENCES,0.7381864623243933,|B| ≥q|A| − r
REFERENCES,0.7394636015325671,"3
2|A| ln 1 δ #"
REFERENCES,0.7407407407407407,≥1 −3δ.
REFERENCES,0.7420178799489144,"Combining with Equation (24) gives, by the union bound,"
REFERENCES,0.7432950191570882,"Pr
T,P """
REFERENCES,0.7445721583652618,|B| −|C| ≥q|A| − r
REFERENCES,0.7458492975734355,"3
2|A| ln 1"
REFERENCES,0.7471264367816092,δ −(1 −α)q|A| − r
REFERENCES,0.7484035759897829,"1
2|A| ln 1 δ #"
REFERENCES,0.7496807151979565,≥1 −4δ.
REFERENCES,0.7509578544061303,"Since
p"
REFERENCES,0.7522349936143039,"3/2 +
p"
REFERENCES,0.7535121328224776,"1/2 ≤2, this implies:"
REFERENCES,0.7547892720306514,"Pr
T,P """
REFERENCES,0.756066411238825,|B| −|C| ≥αq|A| −2 r
REFERENCES,0.7573435504469987,|A| ln 1 δ #
REFERENCES,0.7586206896551724,≥1 −4δ.
REFERENCES,0.7598978288633461,"Or equivalently,"
REFERENCES,0.7611749680715197,"Pr
T,P """
REFERENCES,0.7624521072796935,|B| −|C| < αq|A| −2 r
REFERENCES,0.7637292464878672,|A| ln 1 δ # ≤4δ.
REFERENCES,0.7650063856960408,"Since adding additional restrictions can only reduce a probability, we have:"
REFERENCES,0.7662835249042146,"Pr
T,P"
REFERENCES,0.7675606641123882,"""
p|S|2"
REFERENCES,0.768837803320562,"2
≤|T| ≤|A|"
REFERENCES,0.7701149425287356,ϵ ∧|B| −|C| < αq|A| −2 r
REFERENCES,0.7713920817369093,|A| ln 1 δ # ≤4δ.
REFERENCES,0.7726692209450831,But if p|S|2
REFERENCES,0.7739463601532567,"2
≤|T| ≤|A|"
REFERENCES,0.7752234993614304,"ϵ then 2|A| ≥ϵp|S|2 and then, since ϵ ≥
32
α2q2p|S|2 ln 1 δ : 2 r"
REFERENCES,0.776500638569604,|A| ln 1 δ ≤2 s
REFERENCES,0.7777777777777778,"2|A|
ϵp|S|2 · |A| ln 1"
REFERENCES,0.7790549169859514,δ ≤2|A| s
REFERENCES,0.7803320561941252,"2
p|S|2
32
α2q2p|S|2 ln 1"
REFERENCES,0.7816091954022989,"δ
ln 1"
REFERENCES,0.7828863346104725,δ = αq
REFERENCES,0.7841634738186463,"2 |A|. Thus,"
REFERENCES,0.7854406130268199,"Pr
T,P"
REFERENCES,0.7867177522349936,p|S|2
REFERENCES,0.7879948914431673,"2
≤|T| ≤|A|"
REFERENCES,0.789272030651341,ϵ ∧|B| −|C| < αq
REFERENCES,0.7905491698595147,"2 |A|

≤4δ."
REFERENCES,0.7918263090676884,"Since, in general, for any two events X and Y it holds that Pr[Y ] ≤Pr[X, Y ] + Pr[X], we have"
REFERENCES,0.7931034482758621,"Pr
T,P"
REFERENCES,0.7943805874840357,"
|T| ≤|A|"
REFERENCES,0.7956577266922095,ϵ ∧|B| −|C| < αq
REFERENCES,0.7969348659003831,"2 |A|

≤Pr
T,P"
REFERENCES,0.7982120051085568,p|S|2
REFERENCES,0.7994891443167306,"2
≤|T| ≤|A|"
REFERENCES,0.8007662835249042,ϵ ∧|B| −|C| < αq
REFERENCES,0.8020434227330779,"2 |A|

+"
REFERENCES,0.8033205619412516,"+ Pr
T,P"
REFERENCES,0.8045977011494253,p|S|2
REFERENCES,0.8058748403575989,"2
> |T|
"
REFERENCES,0.8071519795657727,"≤4δ + Pr
T,P"
REFERENCES,0.8084291187739464,p|S|2
REFERENCES,0.80970625798212,"2
> |T|
"
REFERENCES,0.8109833971902938,"≤4δ + δ,"
REFERENCES,0.8122605363984674,"which is equivalent to the statement in the lemma. To see the last step above, note that E[|T|] = p|S|2
and thus by multiplicative Chernoff bounds,"
REFERENCES,0.8135376756066411,"Pr

|T| < p|S|2 2"
REFERENCES,0.8148148148148148,"
≤exp

−p|S|2 8"
REFERENCES,0.8160919540229885,"
≤exp

−p|S|2 8
· 1"
REFERENCES,0.8173690932311622,"ϵ ·
32
α2q2p|S|2 ln 1 δ 
= δ"
REFERENCES,0.8186462324393359,"4
ϵα2q2 ≤δ."
REFERENCES,0.8199233716475096,"In the last step we have utilized the fact that α, q, δ ∈(0, 1], and the fact (observed in the first
paragraph of this proof) that we may assume that ϵ ∈(0, 1] else the lemma holds trivially."
REFERENCES,0.8212005108556832,"Using the above lemma, we now prove our main theorem regarding knowledge graphs."
REFERENCES,0.822477650063857,"Proof. of Theorem 3.2. Let q := 1 −p and,"
REFERENCES,0.8237547892720306,ϵ := max
REFERENCES,0.8250319284802043,"64
α2pq2|S|2 ln 6n|S| δ
, 2 αq r"
REFERENCES,0.8263090676883781,"2
m ln 6n|S| δ ! ."
REFERENCES,0.8275862068965517,"For any θ ∈Θ define,"
REFERENCES,0.8288633461047255,"Aθ := {y ∈T | πθ(y) ̸= y}
Bθ := {y ∈Aθ | πθ(y) /∈P}
Cθ := {y ∈Aθ | y /∈P}"
REFERENCES,0.8301404853128991,"Note that since err(θ) = |Aθ|/|T|, our goal is to show that, with probability ≥1 −δ, we will not
output any θ with |Aθ| ≥ϵ|T|."
REFERENCES,0.8314176245210728,"Recall that |Θ| ≤n|S|. By Lemma D.1 substituting δ′ =
1
6n|S| δ and the union bound over θ ∈Θ
which is of size |Θ| ≤n|S|,"
REFERENCES,0.8326947637292464,"Pr
T,P"
REFERENCES,0.8339719029374202,"h
∃θ ∈Θ |Aθ| ≥ϵ|T| ∧|Bθ| −|Cθ| ≤αq"
REFERENCES,0.8352490421455939,"2 |Aθ|
i
≤5δ 6 ."
REFERENCES,0.8365261813537676,"Using err(θ) = |Aθ|/|T|, this implies,"
REFERENCES,0.8378033205619413,"Pr
T,P"
REFERENCES,0.8390804597701149,"
∃θ ∈Θ err(θ) ≥ϵ ∧|Bθ| −|Cθ| ≤αqϵ|T| 2 
≤5δ 6"
REFERENCES,0.8403575989782887,"Pr
T,P"
REFERENCES,0.8416347381864623,"
∃θ ∈Θ err(θ) ≥ϵ ∧|Bθ|"
REFERENCES,0.842911877394636,|T| −|Cθ|
REFERENCES,0.8441890166028098,"|T| ≤αqϵ 2 
≤5δ"
REFERENCES,0.8454661558109834,"6
(26)"
REFERENCES,0.8467432950191571,"Finally, define the empirical “errors” for any θ to be,"
REFERENCES,0.8480204342273308,ˆeθ = 1
REFERENCES,0.8492975734355045,m{i | fθ(xi) /∈P}.
REFERENCES,0.8505747126436781,"It is not difficult to see that the algorithm outputs a θ with minimal ˆeθ, and thus it will not output any
θ with ˆeθ −ˆe⋆> 0. Now, it is also not difficult to see that ˆeθ −ˆe⋆is the mean of m random variables
in {−1, 0, 1} and"
REFERENCES,0.8518518518518519,"E[ˆeθ −ˆe⋆] = Pr
y∼τ [πθ(y) /∈P] −Pr
y∼τ [y /∈P] = |Bθ|"
REFERENCES,0.8531289910600255,|T| −|Cθ| |T| .
REFERENCES,0.8544061302681992,"The last step above follows because π⋆is the identity, and because if y = πθ(y) then y ∈P ⇐⇒
πθ(y) ∈P. (Formally, one may define Eθ := {y ∈T | πθ(y) /∈P} and observe that Bθ ⊆Eθ,
Cθ ⊆E⋆and Eθ \ Bθ = E⋆\ Cθ). Thus, by Chernoff bounds,"
REFERENCES,0.855683269476373,"∀θ ∈Θ
Pr
x1,...,xm """
REFERENCES,0.8569604086845466,ˆeθ −ˆe⋆≤|Bθ|
REFERENCES,0.8582375478927203,|T| −|Cθ| |T| + r
REFERENCES,0.859514687100894,"2
m ln 6|Θ| δ #"
REFERENCES,0.8607918263090677,"≤
δ
6|Θ|."
REFERENCES,0.8620689655172413,"By the union bound over θ ∈Θ,"
REFERENCES,0.8633461047254151,"Pr
x1,...,xm """
REFERENCES,0.8646232439335888,∃θ ∈Θ ˆeθ −ˆe⋆≤|Bθ|
REFERENCES,0.8659003831417624,|T| −|Cθ| |T| + r
REFERENCES,0.8671775223499362,"2
m ln 6|Θ| δ # ≤δ 6."
REFERENCES,0.8684546615581098,"Combining with Equation (26) gives,"
REFERENCES,0.8697318007662835,"Pr
T,P """
REFERENCES,0.8710089399744572,"∃θ ∈Θ err(θ) ≥ϵ ∧ˆeθ −ˆe⋆≤αqϵ 2
− r"
REFERENCES,0.8722860791826309,"2
m ln 6|Θ| δ # ≤5δ 6 + δ"
REFERENCES,0.8735632183908046,6 = δ.
REFERENCES,0.8748403575989783,"Since, for our choice of ϵ ≥
2
αq q"
REFERENCES,0.876117496807152,"2
m ln 6|Θ| δ ,"
REFERENCES,0.8773946360153256,"Pr
T,P [∃θ ∈Θ err(θ) ≥ϵ ∧ˆeθ −ˆe⋆≤0] ≤δ."
REFERENCES,0.8786717752234994,"Put another way,
Pr
T,P [∀θ ∈Θ err(θ) ≤ϵ ∨ˆeθ −ˆe⋆> 0] ≥1 −δ."
REFERENCES,0.879948914431673,"We claim that we are done: Observe that if MLE outputs some θ ̸= ⋆then, ˆbθ ≤ˆb⋆. To see this,
recall the definition of the prior ρ,"
REFERENCES,0.8812260536398467,ρ(fθ(x)) :=
REFERENCES,0.8825031928480205,"(
1
2 ·

1
|P | +
1
|Y|

if fθ(x) ∈P"
REFERENCES,0.8837803320561941,"1
2|Y|
if fθ(x) /∈P."
REFERENCES,0.8850574712643678,"and therefore the objective function minimized by MLE, namely,
1
m
Pm
i=1 −log(ρ(fθ(xi))), is
strictly monotonic in ˆbθ:"
M,0.8863346104725415,"1
m m
X"
M,0.8876117496807152,"i=1
−log(ρ(fθ(xi))) = ˆbθ · log
2
1/|Y| + (1 −ˆbθ) log
2
1/|P| + 1/|Y|"
M,0.8888888888888888,"= log
2
1/|P| + 1/|Y| + ˆbθ · log 1/|P| + 1/|Y| 1/|Y|"
M,0.8901660280970626,so the θ output by MLE necessarily minimizes bθ.
M,0.8914431673052363,"Finally, for the simplification in the theorem, note that for p < 0.99, 1/q < 100 is at most a constant
and note that a maximum is never more than a sum."
M,0.89272030651341,"It is interesting to note that it is possible to prove the same theorem using a generalization of Plausible
Ambiguities, though we use the shorter proof above here because it is somewhat more involved.
This generalization may be useful for other priors of full support. Many LMs, in practice, assign
non-zero probability to every string due to softmax distributions or a process called “smoothing.” A
full-support prior ρ has full support, then Aγ = Θ and so the parameter εγ becomes too large to
be meaningful even for γ = 0. To address this, we refine our definition of plausible ambiguities as
follows. For generality, we state them in terms of arbitrary loss L, though we only use them for the
semantic error L = err.
Definition D.2 ((γ, κ)-plausible ambiguities). For any γ, κ ∈[0, 1], the set of (γ, κ)-plausible
ambiguities is:"
M,0.8939974457215837,"Aγ,κ :=

θ ∈Θ
 Pr
y∼τ[ρ(π⋆
θ(y)) ≤κ] ≤γ

, and εγ,κ := max
θ∈Aγ L(θ)."
M,0.8952745849297573,"Furthermore, Aγ = Aγ,0 and εγ = εγ,0."
M,0.896551724137931,"E
Experiments"
M,0.8978288633461047,"For the experiments used to generate Figures 4 and 5, we sampled random languages according to ei-
ther the knowledge graph model or the common nonsense model, and then used a brute-force learning
algorithm to find the optimal translator given an increasing amount of samples. A detailed description
follows, and code can be found at https://github.com/orrp/theory-of-umt."
M,0.8991060025542784,"E.1
Experiments in the knowledge graph model"
M,0.9003831417624522,"Recall that in the knowledge graph model, text describes relations between nodes in a directed
graph. Due to computational constraints, we consider ten nodes, each corresponding to a different
word in the target language. To generate edges corresponding to the target language P, two nodes
are connected with a directed edge independently, with probability 0.5. We then consider source
languages with r ≤10 words. Given a ground-truth translator f⋆: [r] →[10], the source language
graph T is obtained by choosing a random subset of nodes S of size r, taking the pre-image of graph
induced on S under f⋆, and (3) adding noise by redrawing each edge with probability 1 −α for a
fixed agreement coefficient α ∈(0, 1)."
M,0.9016602809706258,"The prior ρ is derived from the edges of P, and the source language µ is derived from the (noisy)
permuted subgraph T. We consider the translator family {fθ|θ ∈Θ} of all node-to-node (word-
to-word) injective translators, of which one is secretly chosen to be ground-truth. Similarly to the"
M,0.9029374201787995,"Name
Symbol
Value
Number of source nodes
r
1, 4, 7, 10
Number of target nodes
n
10
Number of training data
m
1, 2, . . . up to all edges
Edge density (probability of including an edge)
p
0.5
Agreement parameter
α
0, 0.33, 0.66, 1"
M,0.9042145593869731,"Figure 7: Parameters for experiments in the knowledge graph model (Figure 4). For ablations on r
we take α = 0.5, and for ablations on α we take r = 9. The experiments were run in parallel on an
AWS r6i.4xlarge for a total of two and a half CPU-hours."
M,0.9054916985951469,"Name
Symbol
Value"
M,0.9067688378033205,"Number of source sentences
|T|
105"
M,0.9080459770114943,"Number of target sentences
|P|
106
Number of training data
m
1, . . . , 100
Number of validation data
1000
Fraction of common nonsense
α
0, 0.1, . . . , 0.8"
M,0.909323116219668,"Figure 8: Parameters for experiments in the common nonsense model (Figure 5). The experiments
were run in parallel on an AWS r6i.4xlarge for a total of four CPU-hours."
M,0.9106002554278416,"previous setting, we train an MLE algorithm on randomly chosen edges from T, which correspond to
sentences in the source language. For each sampled edge (x1, x2), we increase the ""score"" of each
translator that agrees with the edge, that is, that (fθ(x1), fθ(x2)) is an edge in the graph P."
M,0.9118773946360154,"To show how common ground affects translatability, we ablate the parameter α determines the
fraction of edges on which the source language graph T and the target language graph P agree.
Figure 4 validates the intuition that increased agreement results in lower translation error, and that as
the number of samples increases, the error of the top-scoring translator decreases."
M,0.913154533844189,"To show how language complexity affects translatability, we ablate r, which is the size of the subgraph
corresponding to the source language. Figure 4 (right) validates the intuition that a larger subgraph
results in lower translation error."
M,0.9144316730523627,"The error of a translator is computed as the fraction of edges whose labels are different than the
ground-truth. The values with which the model is instantiated are detailed in Figure 7."
M,0.9157088122605364,"E.2
Experiments in the common nonsense model"
M,0.9169859514687101,"Since in this model the structure of sentences is arbitrary, we represent sentences by integer IDs,
[105] = 1, 2, . . . , 105 and [106] for the target language. We generate a prior ρ from the common
nonsense model by taking the target sentence ids [106] and labeling a random α-fraction of them as
nonsense; the remaining sentences are called sensical S. Given a ground-truth translator f⋆: [105] →
[106], the source language then distributes uniformly over the back-translation of sensical sentences,
f −1
⋆(S)."
M,0.9182630906768838,"The translator family {fθ|θ ∈Θ} is taken to be a set of 105 random one-to-one translators, of which
one is secretely chosen to be ground-truth f⋆. We then train an MLE algorithm on random samples
from the source language: Each sample x ∼µ rules-out a subset of translators, namely, all θ ∈Θ
such that fθ(x) /∈S, i.e., is nonsensical."
M,0.9195402298850575,"Figure 5 shows that as the number of samples increases, the average error over the plausible translators
(that have not been ruled-out) decreases. To show how language complexity / common ground affect
translatability, we ablate the parameter α which determines the fraction of common nonsense. Our
experiments validate the intuition that increased common nonsense results in lower translation error.
The error of a translator is computed as the fraction of disagreements with the ground-truth on a
hold-out validation set of size 1000. The values with which the model is instantiated are detailed in
Figure 8."
M,0.9208173690932312,"F
Related work"
M,0.9220945083014048,"Project CETI.
The sperm whale data collection effort began with a longitudinal dataset from a
community of whales off the coast of Dominica that revealed interesting communication findings,
such as dialects and vocal clans [16]. A recent effort by the Cetacean Translation Initiative (Project
CETI) has been to collect custom-built passive bioacoustic arrays (installed in Fall 2022) covering a
20×20 kilometer area where these whale families reside (collecting over 3 TB/month) in tandem with
on-whale robotic acoustic and video tags, underwater (robotic swimming fish) and aerial drones as
well as other observation techniques in effort to augment rich contextual communication data. CETI’s
scientific team consists of specialists in machine learning, robotics, natural language processing,
marine biology, linguistics, cryptography, signal processing and bio-acoustics. Andreas et al. [2]
present CETI’s initial scientific roadmap for understanding sperm whale communication, identifying
the potential for unsupervised translation to be applied to whale communication. That roadmap
suggests training a full generative LM for whale communication (often using trillions of bits for
parameters [11, 12]). In contrast, our analysis suggests that the data requirements for translation may
be similar to those of supervised translation, which is often several orders of magnitude smaller [37]."
M,0.9233716475095786,"With this setting in mind, our requirements from source and target language are not symmetric: it
would be unreasonable (and unnecessary) for our framework to assume that any sentence in the target
language could be expressed in the source language. Put simply: whales need not understand what
a smartphone is for us to gain some understanding of their communication. Also note, regarding
domain gap, that some (although not all) knowledge can be inferred by training data from, e.g., online
catalogs of hundreds of thousands of marine species [1]). Of course, there are also data-collection
and transcription challenges, a challenge also present in the setting of low-resource (human) language
translation [31]. While these challenges are outside the scope of this paper, our theoretical bounds
on the data requirements may inform how much and what types of data are collected. For instance,
it is less expensive to acquire textual data alone than both textual and video data. Therefore, if it is
believed that an adequate translation is statistically possible using textual data alone, then greater
effort may be placed on collecting this data and on UMT algorithms."
M,0.9246487867177522,"Unsupervised translation.
In unsupervised machine translation [32], a translator between two
languages is learned based only on monolingual corpora from each language. A body of work on
UMT uses neural networks [29, 23, 5, 24, 35] or statistical methods [25, 4] for this task. Empirical
evaluation of UMT found that it is outperformed by supervised machine translation, even when
UMT is trained on several orders of magnitude more data [28, 22]. Among the key barriers for
UMT identified in these evaluations are the domain gap and the data gap, and recent works propose
techniques for bridging these gaps [15, 19]. Our theory suggests that sample complexity should
remain roughly the same between the supervised and unsupervised settings, barring computational
constraints. This, we hope, will encourage practitioners to bridge the remaining gaps."
M,0.9259259259259259,"Language models (LMs).
In recent years, LMs such as GPT [11], BERT [13] and PaLM [12] were
shown to achieve state-of-the-art performance on many tasks in natural language processing (NLP)
such as text generation, summarization, or (supervised) MT. These models are indeed large, with
hundreds of billions of parameters, and are pre-trained on hundreds of billions of tokens."
M,0.9272030651340997,"LMs are useful for machine translation in a variety of ways (e.g. [10, 18]). Of particular relevance
are empirical works that use target LMs as priors to improve machine translation [27, 7]. To our
knowledge, our work is the first theoretical work formally proving error bounds for prior-assisted
translation. Appendix H discusses the use of LMs to establish priors for translation."
M,0.9284802043422733,"Goal-oriented communication.
It is interesting to contrast our work with the work on goal-oriented
communication, which was introduced by [21] and extended in [17]. They study the setting of two
communicating parties (one of which is trying to achieve a verifiable goal) using each a language
completely unknown to the other. They put forward a theory of goal-oriented communication,
where communication is not an end in itself, but rather a means to achieving some goals of the
communicating parties. Focusing on goals provides a way to address “misunderstanding” during
communication, as in when one can verify whether the goal is (or is not) achieved. Their theory
shows how to overcome any initial misunderstanding between parties towards achieving a given goal.
Our setting is different: Informally, rather than be a participant in a communication with someone"
M,0.929757343550447,"Sentence
Probability"
M,0.9310344827586207,"I just ate a giant squid.
1.5 × 10−9"
M,0.9323116219667944,"I just ate a giant cheeseburger.
1.9 × 10−8"
M,0.933588761174968,"A sperm whale said: I just ate a giant squid.
6.8 × 10−14"
M,0.9348659003831418,"A sperm whale said: I just ate a giant cheeseburger.
1.2 × 10−17"
M,0.9361430395913155,"Figure 9: Without using a prompt, the sentence I just ate a giant cheeseburger is more likely, but
using the prompt A sperm whale said:, the sentence I just ate a giant squid is much more likely.
Probabilities are from the GPT-3 API."
M,0.9374201787994891,"speaking a different language, we wish to translate communications between two external parties
speaking in a language unknown to us and there is no verifiable goal to aid us in this process."
M,0.9386973180076629,"Subgraph isomoprhism.
For simplicity, we model the knowledge graphs of Section 3.1 as a pair
of correlated Erd˝os–Rényi (ER) graphs. The computational problem of identifying a subgraph of an
ER graph has been studied by Erd˝os and others [6, 26, 20]. In particular, [20] consider a model in
which two correlated graphs P, T are derived from a “parent graph” G by independently deleting
rows and edges G, and then applying a permutation π∗to the vertices of T. Although their model
differs from our knowledge graph model,15 they propose efficient algorithms for recovering the latent
permutation π∗and provide an empirical evaluation on synthetic and real-world data. Given the
similarity between our models, it would be interesting to see if their algorithm can be adapted to our
setting, which would nicely complement our formally-proven-yet-inefficient algorithm."
M,0.9399744572158365,"G
Future Work"
M,0.9412515964240102,"Our initial exploration leaves plenty of room for future work. In particular, we propose the following
possible directions:"
M,0.9425287356321839,"1. In our lossless models, the target language subsumes the source language in the sense that
everything that is representable in the source language can also be represented in the target
language. It would be interesting to extend our work to the partially-overlapping case."
M,0.9438058748403576,"2. The language distribution in our models are all uniform. It would be interesting to examine
non-uniform distributions such as Zipfian or power-law distributions."
M,0.9450830140485313,"3. As stated earlier, our analysis is purely information-theoretic and leaves the question of the
efficiency of UMT open."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.946360153256705,"4. A good starting point for the efficiency question would be to design efficient UMT algorithms
for one of the randomized models of language presented in this paper."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9476372924648787,"H
Good Priors through Prompts"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9489144316730523,"The most direct way to improve a prior is to train (or fine-tune) the LM on a dataset that includes a
large number of articles relevant to the source language, e.g., volumes of oceanic research, for whales.
In addition, training on a wide variety of sources including multiple languages, diverse sources, and
encoding systems may be helpful. It is possible that a system that has how to transfer knowledge
between hundreds of languages and even programming languages, may have a better prior."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9501915708812261,"Another general strategy for creating a prior is to use prompting: Given a prompt string s, one can
define ρ(y) ∝ν(s y), that is the prior distribution of text that that LM generates conditioned on the
text beginning with s. Figure 9 illustrates some toy examples of how even a simple prompt like A
sperm whale said: can help focus on translations that are more likely for a sperm whale to say, and
eliminate irrelevant translations."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9514687100893997,"15In the knowledge graph (a) the vertices of T are always a subset of the vertices of P, and (b) the deleted
vertices are fixed rather than randomly chosen"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9527458492975734,"Background prompts.
There is a natural and potentially powerful idea that an unsupervised
translator, in addition to outputting translations, would automatically generate a background prompt
that increases the intelligibility of many translations.16 Suppose, for example, across numerous
communications, the unsupervised translator determines that sperm whales measure time in “number
of naps” and that typical nap duration varies by age. Then, rather than having to repeatedly explain
this in each translation, it can be explained once in a background prompt that is automatically inserted
before each translation. For example, following the prompt"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9540229885057471,"s = Sperm whales measure time in numbers of naps, but each whale’s typical nap duration depends
on their age. So if a whale’s nap duration is 9 minutes, then 4 naps is about 36 minutes
(though whales tend to exaggerate times). A sperm whale said:"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9553001277139208,"would make translations like Wow, I just dove for 6 naps or How many naps ago was that? both more
likely and more understandable."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9565772669220945,"I
Generalizations of the framework"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9578544061302682,"I.1
Lossy Translation"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9591315453384419,"Many things can be included in textual transcriptions suitable for translation. For instance, one can
distinguish the speakers, e.g., “Whale 1: ... Whale 2: ... Whale 1: ...” if the source data is annotated
with speaker identifiers. Some aspects of the way one is speaking can be transcribed, e.g., “Whale 1
(fast tempo clicking): ... Whale 2 (slow, loud clicking): ...” It may be possible to encode these textually
in x."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9604086845466155,"However, if x is encoded in a more flexible format, such as arbitrary binary files, one can include as
much raw information as possible, including the source audio recording, to provide the translator
with as much context as possible. In that case, lossless translation will no longer be possible, because
one cannot compute the raw x from a textual translation."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9616858237547893,"Given the possible benefits of such annotations, we propose an extension of our theory to the lossy
setting. A natural generalization of the maximum-likelihood approach is as follows:"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9629629629629629,"min
θ∈Θ
1
m m
X"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9642401021711366,"i=1
−log ρ(fθ(xi)) −1"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9655172413793104,λ log ϕθ(xi | y = fθ(xi)).
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.966794380587484,"Here ϕθ : X × Y →[0, 1] is a probabilistic inverse (“randomized back translation”) of f whose
parameters are also encoded in θ. Note that the family {(fθ, ϕθ) | θ ∈Θ} must satisfy that for all
y ∈Y, P"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9680715197956578,"x∈f −1(y) ϕθ(x | y) = 1, though it is no longer required that fθ be 1–1."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9693486590038314,"As λ decreases to 0, the optimal solution would assign infinite loss to any fθ : X ,→Y and ϕθ
that are not perfect inverses, where there is some x (with positive probability under µ) such that
ϕθ(x | y = fθ(x)) < 1. Thus, for sufficiently small λ, the algorithm is exactly the minimum cross-
entropy (maximum likelihood) algorithm of Definition 1.1. For arbitrarily large λ, the algorithm
will collapse to always outputting the most likely y ∈Y under ρ. For example, everything could be
translated to Hello regardless of its contents."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9706257982120051,"For intermediate values of λ, the chosen translator trades off naturalness in the form of ρ(fθ(x))
versus information loss which is inversely related to ϕθ(xi | y = fθ(xi)). This trade-off makes
it challenging to define and analyze the success of a lossy unsupervised translation algorithm.
Nonetheless, the algorithm is intuitive."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9719029374201787,"I.2
Infinite parameter sets Θ"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9731800766283525,"In some cases, one can learn with many fewer examples, which is important when parameters are
real-valued and |Θ| = ∞or when the model is over-parameterized. However, one can analyze
even these cases in our model using the following “trick.” Suppose one has a supervised translation
algorithm SUPER that takes m labeled examples (xi, yi) as input and outputs θ ∈Θ. A simple"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9744572158365262,"16In general, the problem of AI-based prompt generation has recently attracted attention, e.g., [34]."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9757343550446999,"observation in these cases is that one could use an initial set of m unlabeled examples, x1, . . . , xm,
to define a subset of translators:"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9770114942528736,"Θ :=

SUPER
 
(x1, y1), (x2, y2), . . . , (xm, ym)

| y1, y2, . . . , ym ∈Y
	
."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9782886334610472,"That is, we restrict attention to the set of possible translators that we could output for any given
ground-truth translations, then it is not difficult to see log |Θ| ≤m log |Y|. If we assume that one
of these is accurate and natural, then restricting the attention of MLE to this set will suffice, and
Theorem A.2 means that the number of examples required is O(log |Θ|) = O(m log |Y|) which is a
linear blowup in the number of examples m used for supervised translation. To make this formal, one
would start from only assuming that one of the translators had negligible error—this is left to future
work."
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.979565772669221,"J
Bounds for Supervised Translation"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9808429118773946,"In this work, for ease of presentation, we have stated that error decreases like O( 1"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9821200510855683,m log |Θ|
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9833971902937421,"δ ) for
noiseless supervised translation. This is based in the classic Occam bound for supervised learning:
Theorem J.1 (Occam bounds). Let X, Y, be sets, D be a joint distribution over X × Y, ℓ: Y ×
Y →[0, 1] be a loss, and fθ : X →Y be a family of functions parameterized by θ ∈Θ, and
L(θ) := E(x,y)∼D[ℓ(y, fθ(x))]. For any δ > 0,"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9846743295019157,"Pr
(x1,y1),...,(xm,ym)∼Dm"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9859514687100894,"
L
 ˆθ

≤1"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9872286079182631,m ln |Θ| δ
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9885057471264368,"
≥1 −δ
if L(⋆) = 0,"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9897828863346104,"Pr
(x1,y1),...,(xm,ym)∼Dm """
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9910600255427842,"L
 ˆθ

≤L(⋆) + r"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9923371647509579,"1
m ln |Θ| δ #"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9936143039591315,"≥1 −δ
if L(⋆) ̸= 0,"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9948914431673053,"for ˆθ := argminθ∈Θ
P"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9961685823754789,"i ℓ(yi, fθ(xi)).17"
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9974457215836526,Note that supervised translation is simply classification with many classes Y.
A GOOD STARTING POINT FOR THE EFFICIENCY QUESTION WOULD BE TO DESIGN EFFICIENT UMT ALGORITHMS,0.9987228607918263,"17As in MLE, ties can be broken arbitrarily, e.g., lexicographically. Our bounds, like the Occam bound,
hold simultaneously for all minimizers. In the realizable case, the Empirical Risk Minimizer ˆθ will have
P ℓ(yi, fθ(xi)) = 0."
