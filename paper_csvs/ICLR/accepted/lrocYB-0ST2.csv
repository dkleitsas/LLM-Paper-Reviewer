Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001694915254237288,"The empirical success of deep convolutional networks on tasks involving high-
dimensional data such as images or audio suggests that they can efﬁciently ap-
proximate certain functions that are well-suited for such tasks. In this paper, we
study this through the lens of kernel methods, by considering simple hierarchical
kernels with two or three convolution and pooling layers, inspired by convolutional
kernel networks. These achieve good empirical performance on standard vision
datasets, while providing a precise description of their functional space that yields
new insights on their inductive bias. We show that the RKHS consists of additive
models of interaction terms between patches, and that its norm encourages spatial
similarities between these terms through pooling layers. We then provide general-
ization bounds which illustrate how pooling and patches yield improved sample
complexity guarantees when the target function presents such regularities."
INTRODUCTION,0.003389830508474576,"1
INTRODUCTION"
INTRODUCTION,0.005084745762711864,"Deep convolutional models have been at the heart of the recent successes of deep learning in problems
where the data consists of high-dimensional signals, such as image classiﬁcation or speech recognition.
Convolution and pooling operations have notably contributed to the practical success of these models,
yet our theoretical understanding of how they enable efﬁcient learning is still limited."
INTRODUCTION,0.006779661016949152,"One key difﬁculty for understanding such models is the curse of dimensionality: due to the high-
dimensionality of the input data, it is hopeless to learn arbitrary functions from samples. For instance,
classical non-parametric regression techniques for learning generic target functions typically require
either low dimension or very high degrees of smoothness in order to obtain good generalization (e.g.,
Wainwright, 2019), which makes them impractical for dealing with high-dimensional signals. Thus,
further assumptions on the target function are needed to make the problem tractable, and we seek
assumptions that make convolutions a useful modeling tool. Various works have studied approxima-
tion beneﬁts of depth with models that resemble deep convolutional architectures (Cohen & Shashua,
2017; Mhaskar & Poggio, 2016; Schmidt-Hieber et al., 2020). Nevertheless, while such function
classes may provide improved statistical efﬁciency in theory, it is unclear if there exist efﬁcient
algorithms to learn such models, and hence, whether they might correspond to what convolutional
networks learn in practice. To overcome this issue, we consider instead function classes based on
kernel methods (Schölkopf & Smola, 2001; Wahba, 1990), which are known to be learnable with
efﬁcient (polynomial-time) algorithms, such as kernel ridge regression or gradient descent."
INTRODUCTION,0.00847457627118644,"We consider “deep” structured kernels known as convolutional kernels, which yield good empirical
performance on standard computer vision benchmarks (Li et al., 2019; Mairal, 2016; Mairal et al.,
2014; Shankar et al., 2020), and are related to over-parameterized convolutional networks (CNNs) in
so-called “kernel regimes” (Arora et al., 2019; Bietti & Mairal, 2019b; Daniely et al., 2016; Garriga-
Alonso et al., 2019; Jacot et al., 2018; Novak et al., 2019; Yang, 2019). Such regimes may be seen as
providing a ﬁrst-order description of what common deep models trained with gradient methods may
learn. Studying the corresponding function spaces (reproducing kernel Hilbert spaces, or RKHS)
may then provide insight into the beneﬁts of various architectural choices. For fully-connected
architectures, such kernels are rotation-invariant, and the corresponding RKHSs are well understood
in terms of regularity properties on the sphere (Bach, 2017a; Smola et al., 2001), but do not show
any major differences between deep and shallow kernels (Bietti & Bach, 2021; Chen & Xu, 2021;"
INTRODUCTION,0.010169491525423728,Published as a conference paper at ICLR 2022
INTRODUCTION,0.011864406779661017,"Geifman et al., 2020). In contrast, in this work we show that even in the kernel setting, multiple layers
of convolution and pooling operations can be crucial for efﬁcient learning of functions with speciﬁc
structures that are well-suited for natural signals. Our work paves the way for further studies of the
inductive bias of optimization algorithms on deep convolutional networks beyond kernel regimes,
for instance by incorporating adaptivity to low-dimensional structure (Bach, 2017a; Chizat & Bach,
2020; Wei et al., 2019) or hierarchical learning (Allen-Zhu & Li, 2020)."
INTRODUCTION,0.013559322033898305,We make the following contributions:
INTRODUCTION,0.015254237288135594,"• We revisit convolutional kernel networks (Mairal, 2016), ﬁnding that simple two or three layers
models with Gaussian pooling and polynomial kernels of degree 2-4 at higher layers provide
competitive performance with state-of-the-art convolutional kernels such as Myrtle kernels (Shankar
et al., 2020) on Cifar10.
• For such kernels, we provide an exact description of the RKHS functions and their norm, illustrating
representation beneﬁts of multiple convolutional and pooling layers for capturing additive and
interaction models on patches with certain spatial regularities among interaction terms.
• We provide generalization bounds that illustrate the beneﬁts of architectural choices such as pooling
and patches for learning additive interaction models with spatial invariance in the interaction terms,
namely, improvements in sample complexity by polynomial factors in the size of the input signal."
INTRODUCTION,0.01694915254237288,"Related work.
Convolutional kernel networks were introduced by Mairal et al. (2014); Mairal
(2016). Empirically, they used kernel approximations to improve computational efﬁciency, while we
evaluate the exact kernels in order to assess their best performance, as in (Arora et al., 2019; Li et al.,
2019; Shankar et al., 2020). Bietti & Mairal (2019a;b) show invariance and stability properties of
its RKHS functions, and provide upper bounds on the RKHS norm for some speciﬁc functions (see
also Zhang et al., 2017); in contrast, we provide exact characterizations of the RKHS norm, and
study generalization beneﬁts of certain architectures. Scetbon & Harchaoui (2020) study statistical
properties of simple convolutional kernels without pooling, while we focus on the role of architecture
choices with an emphasis on pooling. Cohen & Shashua (2016; 2017); Mhaskar & Poggio (2016);
Poggio et al. (2017) study expressivity and approximation with models that resemble CNNs, showing
beneﬁts thanks to hierarchy or local interactions, but such models are not known to be learnable with
tractable algorithms, while we focus on (tractable) kernels. Regularization properties of convolutional
models were also considered in (Gunasekar et al., 2018; Heckel & Soltanolkotabi, 2020), but in
different regimes or architectures than ours. Li et al. (2021); Malach & Shalev-Shwartz (2021) study
beneﬁts of convolutional networks with efﬁcient algorithms, but do not study the gains of pooling.
Du et al. (2018) study sample complexity of learning CNNs, focusing on parametric rather than non-
parametric models. Mei et al. (2021) study statistical beneﬁts of global pooling for learning invariant
functions, but only consider one layer with full-size patches. Concurrently to our work, Favero et al.
(2021); Misiakiewicz & Mei (2021) study beneﬁts of local patches, but focus on one-layer models."
DEEP CONVOLUTIONAL KERNELS,0.01864406779661017,"2
DEEP CONVOLUTIONAL KERNELS"
DEEP CONVOLUTIONAL KERNELS,0.020338983050847456,"In this section, we recall the construction of multi-layer convolutional kernels on discrete signals,
following most closely the convolutional kernel network (CKN) architectures studied by Mairal
(2016); Bietti & Mairal (2019a). These architectures rely crucially on pooling layers, typically with
Gaussian ﬁlters, which make them empirically effective even with just two convolutional layers. These
kernels deﬁne function spaces that will be the main focus of our theoretical study of approximation
and generalization in the next sections. In particular, when learning a target function of the form
f ∗(x) = P"
DEEP CONVOLUTIONAL KERNELS,0.022033898305084745,"i fi(x), we will show that they are able to efﬁciently exploit two useful properties of f ∗:
(locality) each fi may depend on only one or a few small localized patches of the signal; (invariance)
many different terms fi may involve the same function applied to different input patches. We provide
further background and motivation in Appendix A."
DEEP CONVOLUTIONAL KERNELS,0.023728813559322035,"For simplicity, we will focus on discrete 1D input signals, though one may easily extend our results
to 2D or higher-dimensional signals. We will assume periodic signals in order to avoid difﬁculties
with border effects, or alternatively, a cyclic domain Ω= Z/|Ω|Z. A convolutional kernel of depth L
may then be deﬁned for input signals x, x′ ∈L2(Ω, Rp) by KL(x, x′) = ⟨Ψ(x), Ψ(x′)⟩, through the
explicit feature map
Ψ(x) = ALMLPL · · · A1M1P1x.
(1)"
DEEP CONVOLUTIONAL KERNELS,0.025423728813559324,Published as a conference paper at ICLR 2022
DEEP CONVOLUTIONAL KERNELS,0.02711864406779661,"Here, Pℓ, Mℓand Aℓare linear or non-linear operators corresponding to patch extraction, kernel
mapping and pooling, respectively, and are described below. They operate on feature maps in L2(Ωℓ)
(with Ω0 =Ω) with values in different Hilbert spaces, starting from H0 := Rp, and are deﬁned below.
An illustration of this construction is given in Figure 1. xℓ−1"
DEEP CONVOLUTIONAL KERNELS,0.0288135593220339,"Pℓxℓ−1[u] ∈H|Sℓ|
ℓ−1"
DEEP CONVOLUTIONAL KERNELS,0.030508474576271188,MℓPℓxℓ−1
DEEP CONVOLUTIONAL KERNELS,0.03220338983050847,dot-product kernel
DEEP CONVOLUTIONAL KERNELS,0.03389830508474576,linear pooling
DEEP CONVOLUTIONAL KERNELS,0.03559322033898305,downsampling
DEEP CONVOLUTIONAL KERNELS,0.03728813559322034,xℓ= AℓMℓPℓxℓ−1
DEEP CONVOLUTIONAL KERNELS,0.03898305084745763,Figure 1: Convolutional kernel.
DEEP CONVOLUTIONAL KERNELS,0.04067796610169491,"Patch extraction.
Given a patch shape Sℓ⊂Ωℓ−1, such
as Sℓ= [−1, 0, 1] for one-dimensional patches of size 3,
the operator Pℓis deﬁned for x ∈L2(Ωℓ−1, Hℓ−1) by1"
DEEP CONVOLUTIONAL KERNELS,0.0423728813559322,"Pℓx[u] = (x[u + v])v∈Sℓ∈H|Sℓ|
ℓ−1."
DEEP CONVOLUTIONAL KERNELS,0.04406779661016949,"Kernel mapping.
The operators Mℓperform a non-
linear embedding of patches into a new Hilbert space
using dot-product kernels. We consider homogeneous
dot-product kernels given for z, z′ ∈H|Sℓ|
ℓ−1 by"
DEEP CONVOLUTIONAL KERNELS,0.04576271186440678,"kℓ(z, z′) = ∥z∥∥z′∥κℓ"
DEEP CONVOLUTIONAL KERNELS,0.04745762711864407," ⟨z, z′⟩"
DEEP CONVOLUTIONAL KERNELS,0.04915254237288136,∥z∥∥z′∥
DEEP CONVOLUTIONAL KERNELS,0.05084745762711865,"
= ⟨ϕℓ(z), ϕℓ(z′)⟩Hℓ,"
DEEP CONVOLUTIONAL KERNELS,0.05254237288135593,"(2)
where ϕℓ: H|Sℓ|
ℓ−1 →Hℓis a feature map for the kernel. The kernel functions take the form κℓ(u) =
P"
DEEP CONVOLUTIONAL KERNELS,0.05423728813559322,"j≥0 bjuj with bj ≥0. This includes the exponential kernel κ(u) = eα(u−1) (i.e., Gaussian kernel
on the sphere) and the arc-cosine kernel arising from random ReLU features (Cho & Saul, 2009), for
which our construction is equivalent to that of the conjugate or NNGP kernel for an inﬁnite-width
random ReLU network with the same architecture. The operator Mℓis then deﬁned pointwise by"
DEEP CONVOLUTIONAL KERNELS,0.05593220338983051,"Mℓx[u] = ϕℓ(x[u]).
(3)"
DEEP CONVOLUTIONAL KERNELS,0.0576271186440678,"At the ﬁrst layer on image patches, these kernels lead to functional spaces consisting of homogeneous
functions with varying degrees of smoothness on the sphere, depending on the properties of the
kernel (Bach, 2017a; Smola et al., 2001). At higher layers, our theoretical analysis will also consider
simple polynomial kernels such as kℓ(z, z′) = (⟨z, z′⟩)r, in which case the feature map may be
explicitly written in terms of tensor products. For instance, r = 2 gives ϕℓ(z) = z ⊗z and Hℓ=
(H|Sℓ|
ℓ−1)⊗2 = (Hℓ−1 ⊗Hℓ−1)|Sℓ|×|Sℓ|. See Appendix A.2 for more background on dot-product
kernels, their tensor products, and their regularization properties."
DEEP CONVOLUTIONAL KERNELS,0.059322033898305086,"Pooling.
Finally, the pooling operators Aℓperform local averaging through convolution with a
ﬁlter hℓ[u], which we may consider to be symmetric (hℓ[−u]=:¯hℓ[u]=hℓ[u]). In practice, the pooling
operation is often followed by downsampling by a factor sℓ, in which case the new signal Aℓx is
deﬁned on a new domain Ωℓwith |Ωℓ| = |Ωℓ−1|/sℓ, and we may write for x ∈L2(Ωℓ−1) and u ∈Ωℓ,"
DEEP CONVOLUTIONAL KERNELS,0.061016949152542375,"Aℓx[u] =
X"
DEEP CONVOLUTIONAL KERNELS,0.06271186440677966,"v∈Ωℓ−1
hℓ[sℓu −v]x[v].
(4)"
DEEP CONVOLUTIONAL KERNELS,0.06440677966101695,"Our experiments consider Gaussian pooling ﬁlters with a size and bandwidth proportional to the
downsampling factor sℓ, following Mairal (2016), namely, size 2sℓ+ 1 and bandwidth
√"
DEEP CONVOLUTIONAL KERNELS,0.06610169491525424,"2sℓ. In
Section 3, we will often assume no downsampling for simplicity, in which case we may see the ﬁlter
bandwidth as increasing with the layers."
DEEP CONVOLUTIONAL KERNELS,0.06779661016949153,"Links with other convolutional kernels.
We note that our construction closely resembles kernels
derived from inﬁnitely wide convolutional networks, known as conjugate or NNGP kernels (Garriga-
Alonso et al., 2019; Novak et al., 2019), and is also related to convolutional neural tangent ker-
nels (Arora et al., 2019; Bietti & Mairal, 2019b; Yang, 2019). The Myrtle family of kernels (Shankar
et al., 2020) also resembles our models, but they use small average pooling ﬁlters instead of Gaussian
ﬁlters, which leads to deeper architectures due to smaller receptive ﬁelds."
DEEP CONVOLUTIONAL KERNELS,0.06949152542372881,"1L2(Ω, H) denotes the space of H-valued signals x such that ∥x∥2
L2(Ω,H) := P"
DEEP CONVOLUTIONAL KERNELS,0.0711864406779661,"u∈Ω∥x[u]∥2
H < ∞."
DEEP CONVOLUTIONAL KERNELS,0.07288135593220339,Published as a conference paper at ICLR 2022
DEEP CONVOLUTIONAL KERNELS,0.07457627118644068,"3
APPROXIMATION WITH (DEEP) CONVOLUTIONAL KERNELS"
DEEP CONVOLUTIONAL KERNELS,0.07627118644067797,"In this section, we present our main results on the approximation properties of convolutional kernels,
by characterizing functions in the RKHS as well as their norms. We begin with the one-layer case,
which does not capture interactions between patches but highlights the role of pooling, before moving
multiple layers, where interaction terms play an important role. Proofs are given in Appendix E."
THE ONE-LAYER CASE,0.07796610169491526,"3.1
THE ONE-LAYER CASE"
THE ONE-LAYER CASE,0.07966101694915254,"We begin by considering the case of a single convolutional layer, which can already help us illustrate
the role of patches and pooling. Here, the kernel is given by"
THE ONE-LAYER CASE,0.08135593220338982,"K1(x, x′) = ⟨AΦ(x), AΦ(x′)⟩L2(Ω,H),"
THE ONE-LAYER CASE,0.08305084745762711,"with Φ(x)[u] = ϕ(xu), where we use the shorthand xu = Px[u] for the patch at position u. We now
characterize the RKHS of K1, showing that it consists of additive models of functions in H deﬁned
on patches, with spatial regularities among the terms, induced by the pooling operator A. (Notation:
A∗and A† denote the adjoint and pseudo-inverse of an operator A, respectively.)
Proposition 1 (RKHS for 1-layer CKN.). The RKHS of K1 consists of functions f(x) =
⟨G, Φ(x)⟩L2(Ω,H) = P"
THE ONE-LAYER CASE,0.0847457627118644,"u∈ΩG[u](xu), with G ∈Range(A∗), and with RKHS norm"
THE ONE-LAYER CASE,0.08644067796610169,"∥f∥2
HK1 =
inf
G∈L2(Ω,H)∥A†∗G∥2
L2(Ω,H)
s.t.
f(x) =
X"
THE ONE-LAYER CASE,0.08813559322033898,"u∈Ω
G[u](xu)
(5)"
THE ONE-LAYER CASE,0.08983050847457627,"Note that if A∗is not invertible (for instance in the presence of downsampling), the constraint G ∈
Range(A∗) is active and A†∗is its pseudo-inverse. In the extreme case of global average pooling, we
have A = (1, . . . , 1) ⊗Id : L2(Ω, H) →H, so that G ∈Range(A∗) is equivalent to G[u] = g for
all u, for some ﬁxed g ∈H. In this case, the penalty in (5) is simply the squared RKHS norm ∥g∥2
H."
THE ONE-LAYER CASE,0.09152542372881356,"In order to understand the norm (5) for general pooling, recall that A is a convolution operator with
ﬁlter h1, hence its inverse (which we now assume exists for simplicity) may be easily computed in
the Fourier basis. In particular, for a patch z ∈Rp|S1|, deﬁning the scalar signal gz[u] = G[u](z), we
may write the following using the reproducing property and linearity:"
THE ONE-LAYER CASE,0.09322033898305085,"A†∗G[u](z) = (A−1)⊤gz[u] = F−1 diag(F¯h1)−1Fgz[u],"
THE ONE-LAYER CASE,0.09491525423728814,"where ¯h1[u] := h1[−u] arises from transposition, F is the discrete Fourier transform, and both F
and A are viewed here as |Ω| × |Ω| matrices. From this expression, we see that by penalizing the
RKHS norm of f, we are implicitly penalizing the high frequencies of the signals gz[u] for any z,
and this regularization is stronger when the pooling ﬁlter h1 has a fast spectral decay. For instance,
as the spatial bandwidth of h1 increases (approaching a global pooling operation), Fh1 decreases
more rapidly, which encourages gz[u] to be more smooth as a function of u, and thus prevents f
from relying too much on the location of patches. If instead h1 is very localized in space (e.g., a
Dirac ﬁlter, which corresponds to no pooling), gz[u] may vary much more rapidly as a function of u,
which then allows f to discriminate differently depending on the spatial location. This provides a
different perspective on the invariance properties induced by pooling. If we denote ˜G[u] = A†∗G[u],
the penalty writes
∥˜G∥2
L2(Ω,H) =
X"
THE ONE-LAYER CASE,0.09661016949152543,"u∈Ω
∥˜G[u]∥2
H."
THE ONE-LAYER CASE,0.09830508474576272,"Here, the RKHS norm ∥· ∥H also controls smoothness, but this time for functions ˜G[u](·) deﬁned on
input patches. For homogeneous dot-product kernels of the form (2), the norm takes the form ∥g∥H =
∥T −1"
THE ONE-LAYER CASE,0.1,"2 g∥L2(Sd−1), where the regularization operator T −1"
IS THE SELF-ADJOINT INVERSE SQUARE ROOT OF,0.1016949152542373,"2 is the self-adjoint inverse square root of
the integral operator for the patch kernel restricted to L2(Sd−1). For instance, when the eigenvalues
of T decay polynomially, as for arc-cosine kernels, T −1"
BEHAVES LIKE A POWER OF THE SPHERICAL,0.10338983050847457,"2 behaves like a power of the spherical
Laplacian (see Bach, 2017a, and Appendix A.2). Then we may write"
BEHAVES LIKE A POWER OF THE SPHERICAL,0.10508474576271186,"∥˜G∥2
L2(Ω,H) = ∥((A−1)⊤⊗T −1"
BEHAVES LIKE A POWER OF THE SPHERICAL,0.10677966101694915,"2 )G∥2
L2(Ω)⊗L2(Sd−1),"
BEHAVES LIKE A POWER OF THE SPHERICAL,0.10847457627118644,which highlights that the norm applies two regularization operators (A−1)⊤and T −1
INDEPENDENTLY,0.11016949152542373,"2 independently
on the spatial variable and the patch variable of (u, z) 7→G[u](z), viewed here as an element
of L2(Ω) ⊗L2(Sd−1)."
INDEPENDENTLY,0.11186440677966102,Published as a conference paper at ICLR 2022
INDEPENDENTLY,0.1135593220338983,"Table 1: Cifar10 test accuracy with 2-layer convolutional kernels with 3x3 patches and pool-
ing/downsampling sizes [2,5], with different choices of patch kernels κ1 and κ2. The last model is
similar to a 1-layer convolutional kernel. See Section 5 for experimental details."
INDEPENDENTLY,0.1152542372881356,"κ1-κ2
Exp-Exp
Exp-Poly3
Exp-Poly2
Poly2-Exp
Poly2-Poly2
Exp-Lin
Test acc.
87.9%
87.7%
86.9%
85.1%
82.2%
80.9%"
THE MULTI-LAYER CASE,0.11694915254237288,"3.2
THE MULTI-LAYER CASE"
THE MULTI-LAYER CASE,0.11864406779661017,"We now study the case of convolutional kernels with more than one convolutional layer. While the
patch kernels used at higher layers are typically similar to the ones from the ﬁrst layer, we show
empirically on Cifar10 that they may be replaced by simple polynomial kernels with little loss in
accuracy. We then proceed by studying the RKHS of such simpliﬁed models, highlighting the role of
depth for capturing interactions between different patches via kernel tensor products."
THE MULTI-LAYER CASE,0.12033898305084746,"An empirical study.
Table 1 shows the performance of a given 2-layer convolutional kernel
architecture, with different choices of patch kernels κ1 and κ2. The reference model uses exponential
kernels in both layers, following the construction in Mairal (2016). We ﬁnd that replacing the second
layer kernel by a simple polynomial kernel of degree 3, κ2(u) = u3, leads to roughly the same test
accuracy. By changing κ2 to κ2(u) = u2, the test accuracy is only about 1% lower, while doing
the same for the ﬁrst layer decreases it by about 3%. The shallow kernel with a single non-linear
convolutional layer (shown in the last line of Table 1) performs signiﬁcantly worse. This suggests
that the approximation properties described in Section 3.1 may not be sufﬁcient for this task, while
even a simple polynomial kernel of order 2 at the second layer may substantially improve things by
capturing interactions, in a way that we describe below."
THE MULTI-LAYER CASE,0.12203389830508475,"Two-layers with a quadratic kernel.
Motivated by the above experiments, we now study the
RKHS of a two-layer kernel K2(x, x′) = ⟨Ψ(x), Ψ(x′)⟩L2(Ω2,H2) with Ψ as in (1) with L = 2,
where the second-layer uses a quadratic kernel2 on patches k2(z, z′) = (⟨z, z′⟩)2. An explicit
feature map for k2 is given by ϕ2(z) = z ⊗z. Denoting by H the RKHS of k1, the patches z lie
in H|S2|, thus we may view ϕ2 as a feature map into a Hilbert space H2 = (H ⊗H)|S2|×|S2| (by
isomorphism to H|S2| ⊗H|S2|). The following result characterizes the RKHS of such a 2-layer
convolutional kernel, showing that it consists of additive models of interaction functions in H ⊗H
on pairs of patches, with different spatial regularities on the interaction terms induced by the two
pooling operations. (Notation: we use the notations diag(M)[u] = M[u, u] for M ∈L2(Ω2),
diag(x)[u, v] = 1{u=v}x[u] for x ∈L2(Ω), and Lc is the translation operator Lcx[u] = x[u −c].)
Proposition 2 (RKHS of 2-layer CKN with quadratic k2). Let Φ(x) = (ϕ1(xu) ⊗ϕ1(xv))u,v∈Ω∈
L2(Ω2, H ⊗H). The RKHS of K2 when k2(z, z′) = (⟨z, z′⟩)2 consists of functions of the form"
THE MULTI-LAYER CASE,0.12372881355932204,"f(x) =
X"
THE MULTI-LAYER CASE,0.12542372881355932,"p,q∈S2
⟨Gpq, Φ(x)⟩=
X"
THE MULTI-LAYER CASE,0.1271186440677966,"p,q∈S2 X"
THE MULTI-LAYER CASE,0.1288135593220339,"u,v∈Ω
Gpq[u, v](xu, xv),"
THE MULTI-LAYER CASE,0.13050847457627118,"where Gpq ∈L2(Ω2, H ⊗H) obeys the constraints Gpq ∈Range(Epq) and diag((LpA1 ⊗
LqA1)†∗Gpq) ∈Range(A∗
2). Here, Epq : L2(Ω1) →L2(Ω2) is a linear operator given by"
THE MULTI-LAYER CASE,0.13220338983050847,"Epqx = (LpA1 ⊗LqA1)∗diag(x).
(6)"
THE MULTI-LAYER CASE,0.13389830508474576,"The squared RKHS norm ∥f∥2
HK2 is then equal to the minimum over such decompositions of the
quantity
X"
THE MULTI-LAYER CASE,0.13559322033898305,"p,q∈S2
∥A†∗
2 diag((LpA1 ⊗LqA1)†∗Gpq)∥2
L2(Ω2,H⊗H).
(7)"
THE MULTI-LAYER CASE,0.13728813559322034,"As discussed in the one-layer case, the inverses should be replaced by pseudo-inverses if needed, e.g.,
when using downsampling. In particular, if A∗
2 is singular, the second constraint plays a similar role
to the one-layer case. In order to understand the ﬁrst constraint, we show in Figure 2 the outputs"
THE MULTI-LAYER CASE,0.13898305084745763,"2For simplicity we study the quadratic kernel instead of the homogeneous version used in the experiments,
noting that it still performs well (78.0% instead of 79.4% on 10k examples)."
THE MULTI-LAYER CASE,0.14067796610169492,Published as a conference paper at ICLR 2022
THE MULTI-LAYER CASE,0.1423728813559322,"Figure 2: Display of the 2D response Epqx ∈L2(Ω2) of the operator in (6) for various 1D inputs x ∈
L2(Ω). (left/center) Dirac inputs x = δu centered at two different locations u; (right) Constant
input x = 1. The responses are localized on the p−q diagonal, corresponding to interactions between
two patches at distance around p −q. Here, we took (p, q) = (4, 0), with a signal size |Ω| = 20."
THE MULTI-LAYER CASE,0.1440677966101695,"of Epqx for Dirac delta signals x[v] = δu[v]. We can see that if the pooling ﬁlter h1 has a small
support of size m, then Gpq[u −p, v −q] must be zero when |u −v| > m, which highlights that the
functions in Gpq may only capture interactions between pairs of patches where the (signed) distance
between the ﬁrst and the second is close to p −q."
THE MULTI-LAYER CASE,0.14576271186440679,"The penalty then involves operators LpA1 ⊗LqA1, which may be seen as separable 2D convolutions
on the “images” Gpq[u, v]. Then, if z, z′ ∈Rp|S1| are two ﬁxed patches, deﬁning gz,z′[u, v] =
Gpq[u, v](z, z′), we have, assuming A1 is invertible and symmetric,"
THE MULTI-LAYER CASE,0.14745762711864407,"(LpA1 ⊗LqA1)†∗Gpq[u, v](z, z′) = (A1 ⊗A1)−1gz,z′[u −p, v −q]"
THE MULTI-LAYER CASE,0.14915254237288136,"= F−1
2
diag(F2(h1 ⊗h1))−1F2gz,z′[u −p, v −q],"
THE MULTI-LAYER CASE,0.15084745762711865,"where F2 = F ⊗F is the 2D discrete Fourier transform. Thus, this penalizes the variations of gz,z′
in both dimensions, encouraging the interaction functions to not rely too strongly on the speciﬁc
positions of the two patches. This regularization is stronger when the spatial bandwidth of h1 is
large, since this leads to a more localized ﬁlter in the frequency domain, with stronger penalties on
high frequencies. In addition to this 2D smoothness, the penalty in Proposition 2 also encourages
smoothness along the p −q diagonal of this resulting 2D image using the pooling operator A2. This
has a similar behavior to the one-layer case, where the penalty prevents the functions from relying
too much on the absolute position of the patches. Since A2 typically has a larger bandwidth than A1,
interaction functions Gpq[u, u+r] are allowed to vary with r more rapidly than with u. The regularity
of the resulting “smoothed” interaction terms as a function of the input patches is controlled by the
RKHS norm of the tensor product kernel k1 ⊗k1 as described in Appendix A.2."
THE MULTI-LAYER CASE,0.15254237288135594,"Extensions.
When using a polynomial kernel k2(z, z′) = (⟨z, z′⟩)α with α > 2, we obtain a
similar picture as above, with higher-order interaction terms. For example, if α = 3, the RKHS
contains functions with interaction terms of the form Gpqr[u, v, w](xu, xv, xw), with a penalty
X"
THE MULTI-LAYER CASE,0.15423728813559323,"p,q,r∈S2
∥A†∗
2 diag((A1p ⊗A1q ⊗A1r)†∗Gpqr)∥2
L2(Ω2,H⊗3),"
THE MULTI-LAYER CASE,0.15593220338983052,"where A1c = LcA1. Similarly to the quadratic case, the ﬁrst-layer pooling operator encourages
smoothness with respect to relative positions between patches, while the second-layer pooling
penalizes dependence on the global location. One may extend this further to higher orders to capture
more complex interactions, and our experiments suggest that a two-layer kernel of this form with a
degree-4 polynomial at the second layer may achieve state-of-the-art accuracy for kernel methods
on Cifar10 (see Table 2). We note that such ﬁxed-order choices for κ2 lead to convolutional kernels
that lower-bound richer kernels with, e.g., an exponential kernel at the second layer, in the Loewner
order on positive-deﬁnite kernels. This imples in particular that the RKHS of these “richer” kernels
also contains the functions described above. For more than two layers with polynomial kernels,
one similarly obtains higher-order interactions, but with different regularization properties (see
Appendix D)."
GENERALIZATION PROPERTIES,0.1576271186440678,"4
GENERALIZATION PROPERTIES"
GENERALIZATION PROPERTIES,0.15932203389830507,"In this section, we study generalization properties of the convolutional kernels studied in Section 3,
and show improved sample complexity guarantees for architectures with pooling and small patches
when the problem exhibits certain invariance properties."
GENERALIZATION PROPERTIES,0.16101694915254236,Published as a conference paper at ICLR 2022
GENERALIZATION PROPERTIES,0.16271186440677965,"Learning setting.
We consider a non-parametric regression setting with data distribution ρ
over (x, y), where the goal is to minimize R(f) = E(x,y)∼ρ[(y −f(x))2]. We denote by f ∗=
Eρ[y|x] = arg minf R(f) the regression function, and assume f ∗∈H for some RKHS H with
kernel K. Without any further assumptions on the kernel, we have the following generalization bound
on the excess risk for the kernel ridge regression (KRR) estimator, denoted ˆfn (see Proposition 7 in
Appendix E):"
GENERALIZATION PROPERTIES,0.16440677966101694,E[R( ˆfn) −R(f ∗)] ≤C∥f ∗∥H r
GENERALIZATION PROPERTIES,0.16610169491525423,"τ 2ρ Ex∼ρX[K(x, x)]"
GENERALIZATION PROPERTIES,0.16779661016949152,"n
,
(8)"
GENERALIZATION PROPERTIES,0.1694915254237288,"where ρX is the marginal distribution of ρ on inputs x, C is an absolute constant, and τ 2
ρ is an upper
bound on the conditional noise variance Var[y|x]. We note that this 1/√n rate is optimal if no further
assumptions are made on the kernel (Caponnetto & De Vito, 2007). The quantity Ex∼ρX[K(x, x)]
corresponds to the trace of the covariance operator, and thus provides a global control of eigenvalues
through their sum, which will already highlight the gains that pooling can achieve. Faster rates can be
achieved, e.g., when assuming certain eigenvalue decays on the covariance operator, or when further
restricting f ∗. We discuss in Appendix F.2 how similar gains to those described in this section can
extend to fast rate settings under speciﬁc scenarios."
GENERALIZATION PROPERTIES,0.1711864406779661,"One-layer CKN with invariance.
As discussed in Section 3, the RKHS of 1-layer CKNs consists
of sums of functions that are localized on patches, each belonging to the RKHS H of the patch
kernel k1. The next result illustrates the beneﬁts of pooling when f ∗is translation invariant.
Proposition 3 (Generalization for 1-layer CKN.). Assume f ∗(x) = P"
GENERALIZATION PROPERTIES,0.17288135593220338,"u∈Ωg(xu) with g ∈H of
minimal norm, and assume Ex∼ρX[k1(xu, xv)] ≤σ2
u−v for some (σ2
r)r∈Ω. For a 1-layer CKN K1
with any pooling ﬁlter h ≥0 with ∥h∥1 = 1, we have ∥f ∗∥HK1 =
p"
GENERALIZATION PROPERTIES,0.17457627118644067,"|Ω|∥g∥H, and KRR satisﬁes"
GENERALIZATION PROPERTIES,0.17627118644067796,"E R( ˆfn) −R(f∗) ≲|Ω|∥g∥H
√n s"
GENERALIZATION PROPERTIES,0.17796610169491525,"τ 2ρ
X"
GENERALIZATION PROPERTIES,0.17966101694915254,"r∈Ω
⟨h, Lrh⟩σ2r.
(9)"
GENERALIZATION PROPERTIES,0.18135593220338983,"The quantities σ2
r can be interpreted as auto-correlations between patches at distance r from each
other. Note that if h is a Dirac ﬁlter, then ⟨h, Lrh⟩= 1{r = 0} (recall Lrh[u] = h[u −r]), thus
only σ2
0 plays a role in the bound, while if h is an average pooling ﬁlter, we have ⟨h, Lrh⟩= 1/|Ω|,
so that σ2
0 is replaced by the average ¯σ2 := P"
GENERALIZATION PROPERTIES,0.18305084745762712,"r σ2
r/|Ω|. Natural signals commonly display a decay
in their auto-correlation functions, suggesting that a similar decay may be present in σ2
r as a function
of r. In this case, ¯σ2 may be much smaller than σ2
0, which in turn yields an improved sample
complexity guarantee for learning such an f ∗with global pooling, by a factor up to |Ω| in the extreme
case where σ2
r vanishes for r ≥1 (since ¯σ2 = σ2
0/|Ω| in this case). In Appendix F.1, we provide
simple models where this can be quantiﬁed. For more general ﬁlters, such as local averaging or
Gaussian ﬁlters, and assuming σ2
r ≈0 for r ̸= 0, the bound interpolates between no pooling and
global pooling through the quantity ∥h∥2
2. While this yields a worse bound than global pooling on
invariant functions, such ﬁlters enable learning functions that are not fully invariant, but exhibit some
smoothness along the translation group, more efﬁciently than with no pooling. It should also be noted
that the requirement that g belongs to an RKHS H is much weaker when the patches are small, as
this typically implies that g admits more than p|S|/2 derivatives, a condition which becomes much
stronger as the patch size grows. In the fast rate setting that we study in Appendix F.2, this also
leads to better rates that only depend on the dimension of the patch instead of the full dimension (see
Theorem 8 in Appendix F.2)."
GENERALIZATION PROPERTIES,0.1847457627118644,"Two layers.
When using two layers with polynomial kernels at the second layer, we saw in Section 3
that the RKHS of CKNs consists of additive models of interaction terms of the order of the polynomial
kernel used. The next proposition illustrates how pooling ﬁlters and patch sizes at the second layer
may affect generalization on a simple target function consisting of order-2 interactions.
Proposition 4 (Generalization for 2-layer CKN.). Consider a 2-layer CKN K2 with quadratic k2,
as in Proposition 2, and pooling ﬁlters h1, h2 with ∥h1∥1 = ∥h2∥1 = 1. Assume that ρX satis-
ﬁes Ex∼ρX[k1(xu, xu′)k1(xv, xv′)] ≤ϵ if u ̸= u′ or v ̸= v′, and ≤1 otherwise. We have"
GENERALIZATION PROPERTIES,0.1864406779661017,"Ex∼ρX[K2(x, x)] ≤|S2|2|Ω| X"
GENERALIZATION PROPERTIES,0.188135593220339,"v
⟨h2, Lvh2⟩⟨h1, Lvh1⟩2 + ϵ !"
GENERALIZATION PROPERTIES,0.18983050847457628,".
(10)"
GENERALIZATION PROPERTIES,0.19152542372881357,Published as a conference paper at ICLR 2022
GENERALIZATION PROPERTIES,0.19322033898305085,"As an example, consider f ∗(x) = P"
GENERALIZATION PROPERTIES,0.19491525423728814,"u,v g(xu, xv) for g ∈H ⊗H of minimal norm. The following"
GENERALIZATION PROPERTIES,0.19661016949152543,"table illustrates the obtained generalization bounds R( ˆfn) −R(f ∗) for KRR with various two-layer
architectures (δ: Dirac ﬁlter; 1: global average pooling):"
GENERALIZATION PROPERTIES,0.19830508474576272,"h1
h2
|S2|
∥f ∗∥K2
Ex∼ρX[K2(x, x)]
Bound (ϵ = 0, τ 2
ρ = 1)
δ
δ
|Ω|
|Ω|∥g∥
|Ω|3 + ϵ|Ω|3
∥g∥|Ω|2.5/√n
δ
1
|Ω|
|Ω|∥g∥
|Ω|2 + ϵ|Ω|3
∥g∥|Ω|2/√n
1
1
|Ω| p"
GENERALIZATION PROPERTIES,0.2,"|Ω|∥g∥
|Ω| + ϵ|Ω|3
∥g∥|Ω|/√n
1
δ or 1
1 p"
GENERALIZATION PROPERTIES,0.2016949152542373,"|Ω|∥g∥
|Ω|−1 + ϵ|Ω|
∥g∥/√n"
GENERALIZATION PROPERTIES,0.2033898305084746,"The above result shows that the two-layer model allows for a much wider range of behav-
iors than the one-layer case, between approximation (through the norm ∥f ∗∥K2) and estimation
(through Ex∼ρX[K2(x, x)]), depending on the choice of architecture. Choosing the right architecture
may lead to large improvements in sample complexity when the target functions has a speciﬁc
structure, for instance here by a factor up to |Ω|2.5. In Appendix F.1, we discuss simple possible
models where we may have a small ϵ. Note that choosing ﬁlters that are less localized than Dirac
impulses, but more than global average pooling, will again lead to different “variance” terms (10),
while providing more ﬂexibility in terms of approximation compared to global pooling. This result
may be easily extended to higher-order polynomials at the second layer, by increasing the exponents
on |S2| and ⟨h1, Lvh1⟩to the degree of the polynomial. Other than the gains in sample complexity
due to pooling, the bound also presents large gains compared to a “fully-connected” architecture, as
in the one-layer case, since it only grows with the norm of a local interaction function in H ⊗H that
depends on two patches, which may then be small even when this function has low smoothness."
NUMERICAL EXPERIMENTS,0.20508474576271185,"5
NUMERICAL EXPERIMENTS"
NUMERICAL EXPERIMENTS,0.20677966101694914,"In this section, we provide additional experiments illustrating numerical properties of the con-
volutional kernels considered in this paper. We focus here on the Cifar10 dataset, and on CKN
architectures based on the exponential kernel. Additional results are given in Appendix B."
NUMERICAL EXPERIMENTS,0.20847457627118643,"Experimental setup on Cifar10.
We consider classiﬁcation on Cifar10 dataset, which consists of
50k training images and 10k test images with 10 different output categories. We pre-process the
images using a whitening/ZCA step at the patch level, which is commonly used for such kernels
on images (Mairal, 2016; Shankar et al., 2020; Thiry et al., 2021). This may help reduce the
effective dimensionality of patches, and better align the dominant eigen directions to the target
function, a property which may help kernel methods (Ghorbani et al., 2020). Our convolutional kernel
evaluation code is written in C++ and leverages the Eigen library for hardware-accelerated numerical
computations. The computation of kernel matrices is distributed on up to 1000 cores on a cluster
consisting of Intel Xeon processors. Computing the full Cifar10 kernel matrix typically takes around
10 hours when running on all 1000 cores. Our results use kernel ridge regression in a one-versus-all
approach, where each class uses labels 0.9 for the correct label and −0.1 for the other labels. We report
the test accuracy for a ﬁxed regularization parameter λ = 10−8 (we note that the performance typically
remains the same for smaller values of λ). The exponential kernel always refers to κ(u) = e
1
σ2 (u−1)"
NUMERICAL EXPERIMENTS,0.21016949152542372,with σ = 0.6. Code is available at https://github.com/albietz/ckn_kernel.
NUMERICAL EXPERIMENTS,0.211864406779661,"Varying the kernel architecture.
Table 2 shows test accuracies for different architectures com-
pared to Table 1, including 3-layer models and 2-layer models with larger patches. In both cases,
the full models with exponential kernels outperform the 2-layer architecture of Table 1, and provide
comparable accuracy to the Myrtle10 kernel of Shankar et al. (2020), with an arguably simpler
architecture. We also see that using degree-3 or 4 polynomial kernels at the second second layer of
the two-layer model essentially provides the same performance to the exponential kernel, and that
degree-2 at the second and third layer of the 3-layer model only results in a 0.3% accuracy drop. The
two-layer model with degree-2 at the second layer loses about 1% accuracy, suggesting that certain
Cifar10 images may require capturing interactions between at least 3 different patches in the image
for good classiﬁcation, though even with only second-order interactions, these models signiﬁcantly
outperform single-layer models. While these results are encouraging, computing such kernels is
prohibitively costly, and we found that applying the Nyström approach of Mairal (2016) to these"
NUMERICAL EXPERIMENTS,0.2135593220338983,Published as a conference paper at ICLR 2022
NUMERICAL EXPERIMENTS,0.21525423728813559,"Table 2: Cifar10 test accuracy for two-layer architectures with larger second-layer patches, or three
layer architectures. κ denote the patch kernels used at each layer, ‘conv’ the patch sizes, and ‘pool’
the downsampling factors for Gaussian pooling ﬁlters. We include the Myrtle10 convolutional
kernel Shankar et al. (2020), which consists of 10 layers including Exp kernels on 3x3 patches and
2x2 average pooling."
NUMERICAL EXPERIMENTS,0.21694915254237288,"κ
conv
pool
Test acc. (10k)
Test acc. (50k)
(Exp,Exp)
(3,5)
(2,5)
81.1%
88.3%
(Exp,Poly4)
(3,5)
(2,5)
81.3%
88.3%
(Exp,Poly3)
(3,5)
(2,5)
81.1%
88.2%
(Exp,Poly2)
(3,5)
(2,5)
80.1%
87.4%
(Exp,Exp,Exp)
(3,3,3)
(2,2,2)
80.7%
88.2%
(Exp,Poly2,Poly2)
(3,3,3)
(2,2,2)
80.5%
87.9%
Myrtle10 Shankar et al. (2020)
-
-
-
88.2%"
NUMERICAL EXPERIMENTS,0.21864406779661016,"101
102
103
104 n"
NUMERICAL EXPERIMENTS,0.22033898305084745,"4 × 10
2"
NUMERICAL EXPERIMENTS,0.22203389830508474,"6 × 10
2 mse"
NUMERICAL EXPERIMENTS,0.22372881355932203,average MSE
NUMERICAL EXPERIMENTS,0.22542372881355932,"2-layer poly2
2-layer poly3
2-layer exp
3-layer exp"
NUMERICAL EXPERIMENTS,0.2271186440677966,"100
101
102
103 10
3 10
2 10
1 100 101 102"
NUMERICAL EXPERIMENTS,0.2288135593220339,eigenvalues
NUMERICAL EXPERIMENTS,0.2305084745762712,"spectral decay, 1/2/3 layers"
NUMERICAL EXPERIMENTS,0.23220338983050848,"3-layer (3,3,3) (2,2,2)
2-layer (3,3) (2,5)
1-layer (3) (5)
1-layer (5) (8)"
NUMERICAL EXPERIMENTS,0.23389830508474577,"100
101
102
103 10
3 10
2 10
1 100 101"
NUMERICAL EXPERIMENTS,0.23559322033898306,eigenvalues
NUMERICAL EXPERIMENTS,0.23728813559322035,gaussian vs strided pooling
NUMERICAL EXPERIMENTS,0.23898305084745763,"gaussian
strided"
NUMERICAL EXPERIMENTS,0.24067796610169492,"Figure 3: (left) Mean squared error of kernel ridge regression on Cifar10 for different kernels with
3x3 patches as a function of sample size (averaged over the 10 classes). (center) Eigenvalue decays of
kernel matrices (with Exp kernels) on 1000 Cifar images, for different depths, patch sizes (3 or 5) and
pooling sizes. (right) decays for 2-layer architecture, Gaussian pooling ﬁlter vs strided convolutions
(i.e., no pooling). The plots illustrate that pooling is essential for reducing effective dimensionality."
NUMERICAL EXPERIMENTS,0.2423728813559322,"kernels with more layers or larger patches requires larger models than for the architecture of Table 1
for a similar accuracy. Figure 3(left) shows learning curves for different architectures, with slightly
better convergence rates for more expressive models involving higher-order kernels or more layers;
this suggests that their approximation properties may be better suited for these datasets."
NUMERICAL EXPERIMENTS,0.2440677966101695,"Role of pooling.
Figure 3 shows the spectral decays of the empirical kernel matrix on 1000
Cifar images, which may help assess the “effective dimensionality” of the data, and are related to
generalization properties (Caponnetto & De Vito, 2007). While multi-layer architectures with pooling
seem to provide comparable decays for various depths, removing pooling leads to signiﬁcantly
slower decays, and hence much larger RKHSs. In particular, the “strided pooling” architecture (i.e.,
with Dirac pooling ﬁlters and downsampling) shown in Figure 3(right), which resembles the kernel
considered in (Scetbon & Harchaoui, 2020), obtains less than 40% accuracy on 10k examples. This
suggests that the regularization properties induced by pooling, studied in Section 3, are crucial for
efﬁcient learning on these problems, as shown in Section 4. Appendix B provides more empirics on
different pooling conﬁgurations."
DISCUSSION AND CONCLUDING REMARKS,0.2457627118644068,"6
DISCUSSION AND CONCLUDING REMARKS"
DISCUSSION AND CONCLUDING REMARKS,0.24745762711864408,"In this paper, we studied approximation and generalization properties of convolutional kernels,
showing how multi-layer models with convolutional architectures may effectively break the curse of
dimensionality on problems where the input consists of high-dimensional natural signals, by modeling
localized functions on patches and interactions thereof. We also show how pooling induces additional
smoothness constraints on how interaction terms may or may not vary with global and relative spatial
locations. An important question for future work is how optimization of deep convolutional networks
may further improve approximation properties compared to what is captured by the kernel regime
presented here, for instance by selecting well-chosen convolution ﬁlters at the ﬁrst layer, or interaction
patterns in subsequent layers, perhaps in a hierarchical manner."
DISCUSSION AND CONCLUDING REMARKS,0.24915254237288137,Published as a conference paper at ICLR 2022
DISCUSSION AND CONCLUDING REMARKS,0.25084745762711863,ACKNOWLEDGMENTS
DISCUSSION AND CONCLUDING REMARKS,0.25254237288135595,"The author would like to thank Francis Bach, Alessandro Rudi, Joan Bruna, and Julien Mairal for
helpful discussions."
REFERENCES,0.2542372881355932,REFERENCES
REFERENCES,0.2559322033898305,"Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep
learning. arXiv preprint arXiv:2001.04413, 2020."
REFERENCES,0.2576271186440678,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an inﬁnitely wide neural net. In Advances in Neural Information Processing
Systems (NeurIPS), 2019."
REFERENCES,0.2593220338983051,"Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine
Learning Research (JMLR), 18(1):629–681, 2017a."
REFERENCES,0.26101694915254237,"Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions.
Journal of Machine Learning Research (JMLR), 18(1):714–751, 2017b."
REFERENCES,0.2627118644067797,"Francis Bach. Learning Theory from First Principles (draft). 2021. URL https://www.di.ens.
fr/~fbach/ltfp_book.pdf."
REFERENCES,0.26440677966101694,"Gregory Beylkin and Martin J Mohlenkamp. Numerical operator calculus in higher dimensions.
Proceedings of the National Academy of Sciences, 99(16):10246–10251, 2002."
REFERENCES,0.26610169491525426,"Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. In
Proceedings of the International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.2677966101694915,"Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complexity of
deep convolutional representations. Journal of Machine Learning Research (JMLR), 20(25):1–49,
2019a."
REFERENCES,0.26949152542372884,"Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In Advances in
Neural Information Processing Systems (NeurIPS), 2019b."
REFERENCES,0.2711864406779661,"Joan Bruna and Stéphane Mallat. Invariant scattering convolution networks. IEEE Transactions on
Pattern Analysis and Machine Intelligence (PAMI), 35(8):1872–1886, 2013."
REFERENCES,0.27288135593220336,"Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331–368, 2007."
REFERENCES,0.2745762711864407,"Lin Chen and Sheng Xu. Deep neural tangent kernel and laplace kernel have the same rkhs. In
Proceedings of the International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.27627118644067794,"Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard Socher.
Towards understanding hierarchical learning: Beneﬁts of neural representations. In Advances in
Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.27796610169491526,"Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In Conference on Learning Theory, 2020."
REFERENCES,0.2796610169491525,"Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.28135593220338984,"Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in Neural
Information Processing Systems (NIPS), 2009."
REFERENCES,0.2830508474576271,"Carlo Ciliberto, Francis Bach, and Alessandro Rudi. Localized structured prediction. In Advances in
Neural Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.2847457627118644,"Nadav Cohen and Amnon Shashua. Convolutional rectiﬁer networks as generalized tensor de-
compositions. In Proceedings of the International Conference on Machine Learning (ICML),
2016."
REFERENCES,0.2864406779661017,Published as a conference paper at ICLR 2022
REFERENCES,0.288135593220339,"Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling
geometry. In Proceedings of the International Conference on Learning Representations (ICLR),
2017."
REFERENCES,0.28983050847457625,"Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the
American mathematical society, 39(1):1–49, 2002."
REFERENCES,0.29152542372881357,"Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances in Neural Information
Processing Systems (NIPS), 2016."
REFERENCES,0.29322033898305083,"Simon S Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Ruslan Salakhutdinov, and Aarti
Singh. How many samples are needed to estimate a convolutional neural network? In Advances in
Neural Information Processing Systems (NeurIPS), 2018."
REFERENCES,0.29491525423728815,"Costas Efthimiou and Christopher Frye. Spherical harmonics in p dimensions. World Scientiﬁc,
2014."
REFERENCES,0.2966101694915254,"Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimen-
sionality in convolutional teacher-student scenarios. In Advances in Neural Information Processing
Systems (NeurIPS), 2021."
REFERENCES,0.2983050847457627,"Adrià Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional
networks as shallow gaussian processes. In Proceedings of the International Conference on
Learning Representations (ICLR), 2019."
REFERENCES,0.3,"Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On
the similarity between the laplace and neural tangent kernels. In Advances in Neural Information
Processing Systems (NeurIPS), 2020."
REFERENCES,0.3016949152542373,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? In Advances in Neural Information Processing Systems
(NeurIPS), 2020."
REFERENCES,0.30338983050847457,"Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on
linear convolutional networks. In Advances in Neural Information Processing Systems (NeurIPS),
2018."
REFERENCES,0.3050847457627119,"Wolfgang Hackbusch and Stefan Kühn. A new scheme for the tensor representation. Journal of
Fourier analysis and applications, 15(5):706–722, 2009."
REFERENCES,0.30677966101694915,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2016."
REFERENCES,0.30847457627118646,"Reinhard Heckel and Mahdi Soltanolkotabi. Denoising and regularization via exploiting the structural
bias of convolutional generators. In Proceedings of the International Conference on Learning
Representations (ICLR), 2020."
REFERENCES,0.3101694915254237,"Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems (NIPS),
2018."
REFERENCES,0.31186440677966104,"Hervé Jégou, Florent Perronnin, Matthijs Douze, Jorge Sánchez, Patrick Pérez, and Cordelia Schmid.
Aggregating local image descriptors into compact codes. IEEE Transactions on Pattern Analysis
and Machine Intelligence (PAMI), 34(9):1704–1716, 2011."
REFERENCES,0.3135593220338983,"Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and
Jascha Sohl-Dickstein. Finite versus inﬁnite neural networks: an empirical study. In Advances in
Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.3152542372881356,"Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev
Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019."
REFERENCES,0.3169491525423729,Published as a conference paper at ICLR 2022
REFERENCES,0.31864406779661014,"Zhiyuan Li, Yi Zhang, and Sanjeev Arora. Why are convolutional nets more sample-efﬁcient than
fully-connected nets? In Proceedings of the International Conference on Learning Representations
(ICLR), 2021."
REFERENCES,0.32033898305084746,"Yi Lin. Tensor product space anova models. Annals of Statistics, 28(3):734–755, 2000."
REFERENCES,0.3220338983050847,"David G Lowe. Object recognition from local scale-invariant features. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 1999."
REFERENCES,0.32372881355932204,"Julien Mairal. End-to-End Kernel Learning with Supervised Convolutional Kernel Networks. In
Advances in Neural Information Processing Systems (NIPS), 2016."
REFERENCES,0.3254237288135593,"Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks.
In Advances in Neural Information Processing Systems (NIPS), 2014."
REFERENCES,0.3271186440677966,"Eran Malach and Shai Shalev-Shwartz. Computational separation between convolutional and fully-
connected networks. In Proceedings of the International Conference on Learning Representations
(ICLR), 2021."
REFERENCES,0.3288135593220339,"Stéphane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65
(10):1331–1398, 2012."
REFERENCES,0.3305084745762712,"Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random
features and kernel models. In Conference on Learning Theory (COLT), 2021."
REFERENCES,0.33220338983050846,"Hrushikesh N Mhaskar and Tomaso Poggio. Deep vs. shallow networks: An approximation theory
perspective. Analysis and Applications, 14(06):829–848, 2016."
REFERENCES,0.3338983050847458,"Ha Quang Minh, Partha Niyogi, and Yuan Yao. Mercer’s theorem, feature maps, and smoothing. In
Conference on Learning Theory (COLT), 2006."
REFERENCES,0.33559322033898303,"Theodor Misiakiewicz and Song Mei. Learning with convolution and pooling operations in kernel
methods. arXiv preprint arXiv:2111.08308, 2021."
REFERENCES,0.33728813559322035,"Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Jiri Hron, Daniel A Abolaﬁa,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. In Proceedings of the International Conference on Learning
Representations (ICLR), 2019."
REFERENCES,0.3389830508474576,"Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and
when can deep-but not shallow-networks avoid the curse of dimensionality: a review. International
Journal of Automation and Computing, 14(5):503–519, 2017."
REFERENCES,0.34067796610169493,"Saburou Saitoh. Integral transforms, reproducing kernels and their applications, volume 369. CRC
Press, 1997."
REFERENCES,0.3423728813559322,"Jorge Sánchez, Florent Perronnin, Thomas Mensink, and Jakob Verbeek. Image classiﬁcation with
the ﬁsher vector: Theory and practice. International Journal of Computer Vision (IJCV), 105(3):
222–245, 2013."
REFERENCES,0.3440677966101695,"Meyer Scetbon and Zaid Harchaoui. Harmonic decompositions of convolutional networks. In
Proceedings of the International Conference on Machine Learning (ICML), 2020."
REFERENCES,0.34576271186440677,"Johannes Schmidt-Hieber et al. Nonparametric regression using deep neural networks with relu
activation function. Annals of Statistics, 48(4):1875–1897, 2020."
REFERENCES,0.3474576271186441,"Bernhard Schölkopf and Alexander J Smola. Learning with kernels: support vector machines,
regularization, optimization, and beyond. 2001."
REFERENCES,0.34915254237288135,"Vaishaal Shankar, Alex Fang, Wenshuo Guo, Sara Fridovich-Keil, Jonathan Ragan-Kelley, Ludwig
Schmidt, and Benjamin Recht. Neural kernels without tangents. In Proceedings of the International
Conference on Machine Learning (ICML), 2020."
REFERENCES,0.35084745762711866,Published as a conference paper at ICLR 2022
REFERENCES,0.3525423728813559,"Winfried Sickel and Tino Ullrich. Tensor products of sobolev–besov spaces and applications to
approximation from the hyperbolic cross. Journal of Approximation Theory, 161(2):748–786,
2009."
REFERENCES,0.35423728813559324,"Alex J Smola, Zoltan L Ovari, and Robert C Williamson. Regularization with dot-product kernels. In
Advances in Neural Information Processing Systems (NIPS), 2001."
REFERENCES,0.3559322033898305,"Louis Thiry, Michael Arbel, Eugene Belilovsky, and Edouard Oyallon. The unreasonable effectiveness
of patches in deep convolutional kernels methods. In Proceedings of the International Conference
on Learning Representations (ICLR), 2021."
REFERENCES,0.3576271186440678,"Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.
Journal of Machine Learning Research (JMLR), 5(Jun):669–695, 2004."
REFERENCES,0.3593220338983051,"Grace Wahba. Spline models for observational data, volume 59. Siam, 1990."
REFERENCES,0.3610169491525424,"Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019."
REFERENCES,0.36271186440677966,"Colin Wei, Jason Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. In Advances in Neural Information Processing
Systems (NeurIPS), 2019."
REFERENCES,0.3644067796610169,"Thomas Wiatowski and Helmut Bölcskei. A mathematical theory of deep convolutional neural
networks for feature extraction. IEEE Transactions on Information Theory, 64(3):1845–1866,
2018."
REFERENCES,0.36610169491525424,"Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019."
REFERENCES,0.3677966101694915,"Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
Proceedings of the European Conference on Computer Vision (ECCV), 2014."
REFERENCES,0.3694915254237288,"Y. Zhang, P. Liang, and M. J. Wainwright. Convexiﬁed convolutional neural networks. In International
Conference on Machine Learning (ICML), 2017."
REFERENCES,0.3711864406779661,"A
FURTHER BACKGROUND"
REFERENCES,0.3728813559322034,"This section provides further background on the problem of approximation of functions deﬁned on
signals, as well as on the kernels considered in the paper. We begin by introducing and motivating
the problem of learning functions deﬁned on signals such as images, which captures tasks such as
image classiﬁcation where deep convolutional networks are predominant. We then recall properties
of dot-product kernels and kernel tensor products, which are key to our study of approximation."
REFERENCES,0.37457627118644066,"A.1
NATURAL SIGNALS AND CURSE OF DIMENSIONALITY"
REFERENCES,0.376271186440678,"We consider learning problems consisting of labeled examples (x, y) ∼ρ from a data distribution ρ,
where x is a discrete signal x[u] with u ∈Ωdenoting the position (e.g., pixel location in an image) in
a domain Ω, x[u] ∈Rp (e.g., p = 3 for RGB pixels), and y ∈R is a target label. In a non-parametric
setup, statistical learning may be framed as trying to approximate the regression function"
REFERENCES,0.37796610169491524,f ∗(x) = Eρ[y|x]
REFERENCES,0.37966101694915255,"using samples from the data distribution ρ. If f ∗is only assumed to be Lipschitz, learning requires a
number of samples that scales exponentially in the dimension (see, e.g., von Luxburg & Bousquet
(2004); Wainwright (2019)), a phenomenon known as the curse of dimensionality. In the case of
natural signals, the dimension d = p|Ω| scales with the size of the domain |Ω| (e.g., the number of
pixels), which is typically very large and thus makes this intractable. One common way to alleviate
this is to assume that f ∗is smooth, however the order of smoothness typically needs to be of the order
of the dimension in order for the problem to become tractable, which is a very strong assumption
here when d is very large. This highlights the need for more structured assumptions on f ∗which
may help overcome the curse of dimensionality."
REFERENCES,0.3813559322033898,Published as a conference paper at ICLR 2022
REFERENCES,0.38305084745762713,"Insufﬁciency of invariance and stability.
Two geometric properties that have been successful for
studying the beneﬁts of convolutional architectures are (near-)translation invariance and stability to
deformations. Various works have shown that certain convolutional models f yield good invariance
and stability Mallat (2012); Bruna & Mallat (2013); Bietti & Mairal (2019a), in the sense that
when ˜x is a translation or a small deformation of x, then |f(˜x) −f(x)| is small. Nevertheless, one
can show that for band-limited signals (such as discrete signals), ∥˜x −x∥2 can be controlled in a
similar way (though with worse constants, see (Wiatowski & Bölcskei, 2018, Proposition 5)), so that
Lipschitz functions on such signals obey such stability properties. Thus, deformation stability is not
a much stronger assumption than Lipschitzness, and is insufﬁcient by itself to escape the curse of
dimensionality."
REFERENCES,0.3847457627118644,"Spatial localization.
One successful strategy for learning image recognition models which predates
deep learning is to rely on simple aggregations of local features. These may be extracted using
hand-crafted procedures (Lowe, 1999; Sánchez et al., 2013; Jégou et al., 2011), or using learned
feature extractors, either through learned ﬁlters in the early layers of a CNN (Zeiler & Fergus, 2014),
or other procedures (e.g., Thiry et al. (2021)). One simpliﬁed example that encodes such a prior
is if the target function f ∗only depends on the input image through a localized part of the input
such as a patch xu = (x[u + v])v∈S ∈Rp|S|, where S is a small box centered around 0, that is,
f ∗(x) = g∗(xu). Then, if g∗is assumed to be Lipschitz, we would like a sample complexity that
only scales exponentially in the dimension of a patch p|S|, which is much smaller than dimension of
the entire image p|Ω|. This is indeed the case if we use a kernel deﬁned on such patches, such as"
REFERENCES,0.3864406779661017,"K(x, x′) =
X"
REFERENCES,0.38813559322033897,"u
k(xu, x′
u),"
REFERENCES,0.3898305084745763,"where k is a “simple” kernel such as a dot-product kernel, as discussed in Appendix C. In contrast,
if K is a dot-product kernel on the entire image, corresponding to an inﬁnite-width limit of a fully-
connected network, then approximation is more difﬁcult and is generally cursed by the full dimension
(see Appendix C). While some models of wide fully-connected networks provide some adaptivity to
low-dimensional structures such as the variables in a patch (Bach, 2017a), no tractable algorithms are
currently known to achieve such behavior provably, and it is reasonable to instead encode such prior
information in a convolutional architecture."
REFERENCES,0.39152542372881355,"Modeling interactions.
Modeling interactions between elements of a system at different scales,
possibly hierarchically, is important in physics and complex systems, in order to efﬁciently handle
systems with large numbers of variables (Beylkin & Mohlenkamp, 2002; Hackbusch & Kühn, 2009).
As an example, one may consider target functions f ∗(x) that consist of interaction functions of
the form g(xp, xq), where p, q denote locations of the corresponding patches, and higher-order
interactions may also be considered. In the context of image recognition, while functions of a single
patch may capture local texture information such as edges or color, such an interaction function
may also respond to speciﬁc spatial conﬁgurations of relevant patches, which could perhaps help
identify properties related to the “shape” of an object, for instance. If such functions g are too general,
then the curse of dimensionality may kick in again when one considers more than a handful of
patches. Certain idealized models of approximation may model such interactions more efﬁciently
through hierarchical compositions (e.g., Poggio et al. (2017)) or tensor decompositions (Cohen &
Shashua, 2016; 2017), though no tractable algorithms are known to ﬁnd such models. In this work,
we tackle this in a tractable way using multi-layer convolutional kernels. We show that they can
model interactions through kernel tensor products, which deﬁne functional spaces that are typically
much smaller and more structured than for a generic kernel on the full vector (xp, xq)."
REFERENCES,0.39322033898305087,"A.2
DOT-PRODUCT KERNELS AND THEIR TENSOR PRODUCTS"
REFERENCES,0.3949152542372881,"In this section, we review some properties of dot-product kernels, their induced RKHS and regular-
ization properties. We then recall the notion of tensor product of kernels, which allows us to describe
the RKHS of products of kernels in terms of that of individual kernels."
REFERENCES,0.39661016949152544,"Dot-product kernels.
The rotation-invariance of dot-product kernels provides a natural description
of their RKHS in terms of harmonic decompositions of functions on the sphere using spherical
harmonics (Smola et al., 2001; Bach, 2017a). This leads to natural connections with regularity"
REFERENCES,0.3983050847457627,Published as a conference paper at ICLR 2022
REFERENCES,0.4,"properties of functions deﬁned on the sphere. For instance, if the kernel integral operator on L2(Sd−1)
has a polynomially decaying spectral decay, as is the case for kernels arising from the ReLU
activation (Bach, 2017a; Bietti & Mairal, 2019b), then the RKHS contains functions g ∈L2(Sd−1)
with an RKHS norm equivalent to"
REFERENCES,0.4016949152542373,"∥∆β/2
Sd−1g∥L2(Sd−1),
(11)"
REFERENCES,0.4033898305084746,"for some β that depends on the decay exponent and must be larger than (d −1)/2, with ∆Sd−1
the Laplace-Beltrami operator on the sphere. This resembles a Sobolev norm of order β, and the
RKHS contains functions with bounded derivatives up to order β. When d is small (e.g., at the ﬁrst
layer with small images patches), the space contains functions that need not be too regular, and may
thus be quite discriminative, while for large d (e.g., for a fully-connected network), the functions
must be highly smooth in order to be in the RKHS, and large norms are necessary to approach
non-smooth functions. For kernels with decays faster than polynomial, such as the Gaussian kernel,
the RKHS contains smooth functions, but may still provide good approximation to non-smooth
functions, particularly with small d and when using small bandwidth parameters. The homogeneous
case (2) leads to functions f(x) = ∥x∥g( x"
REFERENCES,0.40508474576271186,"∥x∥) with g deﬁned on the sphere, with a norm given by the
same penalty (11) on the function g (Bietti & Mairal, 2019b)."
REFERENCES,0.4067796610169492,"Kernel tensor products.
For more than one layer, the convolutional kernels we study in Section 3
can be expressed in terms of products of kernels on patches, of the form"
REFERENCES,0.40847457627118644,"K((x1, . . . , xm), (x′
1, . . . , x′
m)) = m
Y"
REFERENCES,0.4101694915254237,"j=1
k(xj, x′
j),
(12)"
REFERENCES,0.411864406779661,"where x1, . . . , xm, x′
1, . . . , x′
m ∈Rd are patches which may come from different signal locations.
If ϕ : Rd →H is the feature map into the RKHS H of k, then"
REFERENCES,0.4135593220338983,"ψ(x1, . . . , xm) = ϕ(x1) ⊗· · · ⊗ϕ(xm)"
REFERENCES,0.4152542372881356,"is a feature map for K, and the corresponding RKHS, denoted H⊗m = H ⊗· · · ⊗H, contains all
functions"
REFERENCES,0.41694915254237286,"f(x1, . . . , xm) = n
X"
REFERENCES,0.4186440677966102,"i=1
gi,1(x1) . . . gi,m(xm),"
REFERENCES,0.42033898305084744,"for some n, with gi,m ∈H for i ∈[n] and j ∈[m] (see,e.g., (Wainwright, 2019, Section 12.4.2)
for a precise construction). The resulting RKHS is often much smaller than for a more generic
kernel on Rd×m; for instance, if H is a Sobolev space of order β in dimension d, then H⊗m is
much smaller than the Sobolev space of order β in d × m dimensions, and corresponds to stronger,
mixed regularity conditions (see, e.g., Bach, 2017b; Sickel & Ullrich, 2009, for the d = 1 case).
This can yield improved generalization properties if the target function has such a structure (Lin,
2000). Kernels of the form (12) and sums of such kernels have been useful tools for avoiding the
curse of dimensionality by encoding interactions between variables that are relevant to the problem
at hand (Wahba, 1990, Chapter 10). In what follows, we show how patch extraction and pooling
operations shape the properties of such interactions between patches in convolutional kernels through
additional spatial regularities."
REFERENCES,0.42203389830508475,"B
ADDITIONAL EXPERIMENTS"
REFERENCES,0.423728813559322,"In this section, we provide additional experiments to those presented in Section 5, using different
patch kernels, patch sizes, pooling ﬁlters, preprocessings, and datasets."
REFERENCES,0.42542372881355933,"Three-layer architectures with different patch kernels.
Table 3 provides more results on 3-layer
architectures compared to Table 2, including different changes in the degrees of polynomial kernels
at the second and third layer. In particular we see that the architecture with degree-2 kernels at both
layers, which captures interactions of order 4, also outperforms the simpler ones using degree-4
kernels at either layer, suggesting that a deeper architecture may better model relevant interactions
terms on this problem."
REFERENCES,0.4271186440677966,Published as a conference paper at ICLR 2022
REFERENCES,0.4288135593220339,"Table 3: Cifar10 test accuracy with 3-layer convolutional kernels with 3x3 patches and pool-
ing/downsampling sizes [2,2,2], with different choices of patch kernels κ1, κ2 and κ3. The last
model is similar to a 1-layer convolutional kernel. Due to high computational cost, we use 10k
training images instead of the full training set (50k images) in most cases."
REFERENCES,0.43050847457627117,"κ1
κ2
κ3
Test ac. (10k)
Test ac. (50k)
Exp
Exp
Exp
80.7%
88.2%
Exp
Poly2
Poly2
80.5%
87.9%
Exp
Poly4
Lin
80.2%
-
Exp
Lin
Poly4
79.2%
-
Exp
Lin
Lin
74.1%
-"
REFERENCES,0.4322033898305085,"Table 4: Cifar10 test accuracy on 10k examples with 2-layer convolutional kernels with 3x3 patches
at the ﬁrst layer, pooling/downsampling sizes [2,5] and patch kernels [Exp,Poly2], with different
patch sizes at the second layer."
REFERENCES,0.43389830508474575,"|S2|
1x1
3x3
5x5
7x7
9x9
11x11
Test acc. (10k)
76.3%
79.4%
80.1%
80.1%
80.1%
79.9%"
REFERENCES,0.43559322033898307,"Varying the second layer patch size.
Table 4 shows the variations in test performance when
changing the size of the second patches at the second layer. We see that intermediate sizes between
3x3 and 9x9 work best, but that performance degrades when using patches that are too large or too
small. For very large patches, this may be due to the large variance in (10), or perhaps instability (Bietti
& Mairal, 2019a). For |S2| =1x1, note that while pooling after the ﬁrst layer allows even 1x1 patches
to capture interactions across different input image patches, these may be limited to short range
interactions when the pooling ﬁlter is localized (see Proposition 2), which may limit the expressivity
of the model."
REFERENCES,0.43728813559322033,"Table 5: Cifar10 test accuracy with patch kernels that are either arc-cosine kernels (denoted ReLU) or
polynomial kernels. The 2-layer architectures use 3x3 patches and [2,5] downsampling/pooling as in
Table 1. “ReLU-NTK” indicates that we consider the neural tangent kernel for a ReLU network with
similar architecture, instead of the conjugate kernel."
REFERENCES,0.43898305084745765,"κ1
κ2
Test ac. (10k)
Test ac. (50k)
ReLU
ReLU
78.5%
86.6%
ReLU-NTK
ReLU-NTK
79.2%
87.2%
ReLU
Poly2
77.2%
-
ReLU
Lin
71.5%
-"
REFERENCES,0.4406779661016949,"Arc-cosine kernel.
In Table 5, we consider 2-layer convolutional kernels with a similar architecture
to those considered in Table 1, but where we use arc-cosine kernels arising from ReLU activations
instead of the exponential kernel used in Section 5, given by"
REFERENCES,0.4423728813559322,κ(u) = 1
REFERENCES,0.4440677966101695,"π (u · (π −arccos(u)) +
p"
REFERENCES,0.4457627118644068,1 −u2).
REFERENCES,0.44745762711864406,"The obtained convolutional kernel then corresponds to the conjugate kernel or NNGP kernel arising
from an inﬁnite-width convolutional network with the ReLU activation (Daniely et al., 2016; Garriga-
Alonso et al., 2019; Novak et al., 2019). We may also consider the neural tangent kernel (NTK) for
the same architecture, which additionally involves arc-cosine kernels of degree 0, which correspond
to random feature kernels for step activations u 7→1{u ≥0}. We ﬁnd that the NTK performs slightly
better than the conjugate kernel, but both kernels achieve lower accuracy compared to the Exponential
kernel shown in Table 1. Nevertheless, we observe a similar pattern regarding the use of polynomial
kernels at the second layer, namely, the drop in accuracy is much smaller when using a quadratic
kernel compared to a linear kernel, suggesting that non-linear kernels on top of the ﬁrst layer, and the
interactions they may capture, are crucial on this dataset for good accuracy."
REFERENCES,0.4491525423728814,Published as a conference paper at ICLR 2022
REFERENCES,0.45084745762711864,"Table 6: Cifar10 test accuracy for one-layer architectures with larger patches of size 6x6, exponential
kernels, and different downsampling/pooling sizes (using Gaussian pooling ﬁlters with bandwidth
and size of ﬁlters proportional to the downsampling factor). The results are for 10k training samples."
REFERENCES,0.45254237288135596,"Pooling
2
4
6
8
10
Test acc. (10k)
67.6%
73.3%
75.5%
75.8%
75.5%"
REFERENCES,0.4542372881355932,"One-layer architectures and larger initial patches.
Table 6 shows the accuracy for one-layer
convolutional kernels with 6x6 patches3 and various pooling sizes, with a highest accuracy of 75.8%
for a pooling size of 8. While this improves on the accuracy obtained with 3x3 patches (slightly
above 74% for the architectures in Tables 1 and 3 with a single non-linear kernel at the ﬁrst layer),
these accuracies remain much lower than those achieved by two-layer architectures with even
quadratic kernels at the second layer. While using larger patches may allow capturing patterns that
are less localized compared to small 3x3 patches, the neighborhoods that they model need to remain
small in order to avoid the curse of dimensionality when using dot-product kernels, as discussed in
Section 2. Instead, the multi-layer architecture may model information at larger scales with a much
milder dependence on the size of the neighborhood, thanks to the structure imposed by tensor product
kernels (see Section A.2) and the additional regularities induced by pooling."
REFERENCES,0.4559322033898305,"We also found that larger patches at the ﬁrst layer may hurt performance in multi-layer models: when
considering the architecture of Table 1 with exponential kernels, using 5x5 patches instead of 3x3 at
the ﬁrst layer yields an accuracy of 79.6% instead of 80.5% on Cifar10 when training on the same
10k images. This again reﬂects the beneﬁts of using small patches at the ﬁrst layer for allowing
better approximation on small neighborhoods, while modeling larger scales using interaction models
according to the structure of the architecture. We note nevertheless that for standard deep networks,
larger patches are often used at the ﬁrst layer (e.g., He et al., 2016), as the feature selection capabilities
of SGD may alleviate the dependence on dimension, e.g., by ﬁnding Gabor-like ﬁlters."
REFERENCES,0.4576271186440678,"Gaussian vs average pooling.
Table 7 shows the differences in performance between two or three
layer architectures considered in Table 2, when Gaussian pooling ﬁlters are replaced by average
pooling ﬁlters. For both architectures considered, average pooling leads to a signiﬁcant performance
drop. This suggests that one may need deeper architectures in order for such average pooling ﬁlters to
work well, as in (Shankar et al., 2020), either with multiple 3x3 convolutional layers before applying
pooling, or by applying multiple average pooling layers in a row as in certain Myrtle kernels. Note
that iterating multiple average pooling layers in a row is equivalent to using a larger and more smooth
pooling ﬁlter (with one more order of smoothness at each layer), which may then be more comparable
to our Gaussian pooling ﬁlters."
REFERENCES,0.45932203389830506,Table 7: Gaussian vs average pooling for two models from Table 2.
REFERENCES,0.4610169491525424,"Model
Gaussian
Average
κ: (Exp,Exp), conv: (3,5), pool: (2,5)
88.3%
75.9%
κ: (Exp,Exp,Exp), conv: (3,3,3), pool: (2,2,2)
88.2%
72.4%"
REFERENCES,0.46271186440677964,"Table 8: SVHN test accuracy for a two-layer convolutional kernel network with Nyström approxima-
tion (Mairal, 2016) with patch size 3x3, pooling sizes [2,5], and ﬁlters [256, 4096]."
REFERENCES,0.46440677966101696,"κ1
κ2
Test acc. (full with Nyström)
Exp
Exp
89.5%
Exp
Poly3
89.3%
Exp
Poly2
88.6%
Poly2
Exp
87.1%
Poly2
Poly2
86.6%
Exp
Lin
78.5%"
REFERENCES,0.4661016949152542,3Note that in this case the ZCA/whitening step is applied on these larger 6x6 patches.
REFERENCES,0.46779661016949153,Published as a conference paper at ICLR 2022
REFERENCES,0.4694915254237288,"SVHN dataset.
We now consider the SVHN dataset, which consists of 32x32 images of digits from
Google Street View images, 73 257 for training and 26 032 for testing. Due to the larger dataset size,
we only consider the kernel approximation approach of Mairal (2016) based on the Nyström method,
which projects the patch kernel feature maps at each layer to ﬁnite-dimensional subspaces generated
by a set of anchor points (playing the role of convolutional ﬁlters), themselves computed via a
K-means clustering of patches.4 We train one-versus-all classiﬁers on the resulting ﬁnite-dimensional
representations using regularized ERM with the squared hinge loss, and simply report the best test
accuracy over a logarithmic grid of choices for the regularization parameter, ignoring model selection
issues in order to assess approximation properties. We use the same ZCA preprocessing as on Cifar10
and the same architecture as in Table 1, with a relatively small number of ﬁlters (256 at the ﬁrst layer,
4096 at the second layer, leading to representations of dimension 65 536), noting that the accuracy can
further improve when increasing this number. Our observations are similar to those for the Cifar10
dataset: using a degree-3 polynomial kernel at the second layer reaches very similar accuracy to the
exponential kernel; using a degree-2 polynomial leads to a slight drop, but a smaller drop than when
making this same change at the ﬁrst layer; using a linear kernel at the second layer leads to a much
larger drop. This again highlights the importance of using non-linear kernels on top of the ﬁrst layer
in order to capture interactions at larger scales than the scale of a single patch."
REFERENCES,0.4711864406779661,"Local versus global whitening.
Recall that our pre-processing is based on a patch-level whitening
or ZCA on each image, following Mairal (2016). In practice, this is achieved by whitening extracted
patches from each image, and reconstructing the image from whitened patches via averaging. In
contrast, other approaches use global whitening of the entire image Lee et al. (2020); Shankar et al.
(2020). For the 2-layer model shown in Table 2 with 5x5 patches at the second layer, we found global
ZCA to provide signiﬁcantly worse performance, with a drop from 88.3% to about 80%."
REFERENCES,0.4728813559322034,"Finite networks and comparison to Shankar et al. (2020).
The work Shankar et al. (2020)
introduces Myrtle kernels but also consider similar architectures for usual CNNs with ﬁnite-width,
trained with stochastic gradient descent. Obtaining competitive architectures for the ﬁnite-width
case is not the goal of our work, which focuses on good architectures for the kernel setup, yet
it remains interesting to consider this question. In the case of Shankar et al. (2020), training the
ﬁnite-width networks yields better accuracy compared to their “inﬁnite-width” kernel counterparts, a
commonly observed phenomenon which may be due to better “adaptivity” of optimization algorithms
compared to kernel methods, which have a ﬁxed representation and thus may not learn representations
adapted to the data (see, e.g., Allen-Zhu & Li, 2020; Bach, 2017a; Chizat et al., 2019). Nevertheless,
we found that for the two-layer architecture considered in Table 1, which has many fewer layers
compared to the Myrtle architectures of Shankar et al. (2020), using a ﬁnite-width ReLU network
yields poorer performance compared to the kernel (around 83% at best, compared to 87.9%). This
may suggest that for convolutional networks, deeper networks may have additional advantages when
using optimization algorithms, in terms of adapting to possibly relevant structure of the problem,
such as hierarchical representations (see, e.g., Allen-Zhu & Li (2020); Chen et al. (2020); Poggio
et al. (2017) for theoretical justiﬁcations of the beneﬁts of depth in non-kernel regimes)."
REFERENCES,0.4745762711864407,"C
COMPLEXITY OF SPATIALLY LOCALIZED FUNCTIONS"
REFERENCES,0.47627118644067795,"In this section, we brieﬂy elaborate on our discussion in Section A.1 on how simple convolutional
structure may improve complexity when target functions are spatially localized. We assume f ∗(x) =
g∗(xu) with xu = (x[u + v])v∈S ∈Rp|S| a patch of size |S|, where g∗is a Lipschitz function."
REFERENCES,0.47796610169491527,"If we deﬁne the kernel Ku(x, x′) = k(xu, x′
u), where k is a dot-product kernel arising from a
one-hidden layer network with positively-homogeneous activation such as the ReLU, and further
assume patches to be bounded and g∗to be bounded, then the uniform approximation error bound
of Bach (2017a, Proposition 6) together with a simple O(1/√n) Rademacher complexity bound on
estimation error shows that we may achieve a generalization bound with a rate that only depends on
the patch dimension p|S| rather than p|Ω| in this setup (i.e., a sample complexity that is exponential
in p|S|, which is much smaller than p|Ω|)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.47966101694915253,"4We
use
the
PyTorch
implementation
available
at
https://github.com/claying/
CKN-Pytorch-image."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.48135593220338985,Published as a conference paper at ICLR 2022
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.4830508474576271,"If we consider the kernel K(x, x′) = P"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.4847457627118644,"u∈Ωk(xu, x′
u), the RKHS contains all functions in the
RKHS of Ku for all u ∈Ω, with the same norm (this may be seen as an application of Theorem 6
with a feature map given by concatenating the kernel maps of each Ku), so that we may achieve the
same approximation error as above, and thus a similar generalization bound that is not cursed by
dimension. This kernel also allows us to obtain similar generalization guarantees when f ∗consists of
linear combinations of such spatially localized functions on different patches within the image."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.4864406779661017,"In contrast, when using a similar dot-product kernel on the full signal, corresponding to using a
fully-connected network in a kernel regime, one may construct functions f ∗(x) = g∗(xu) with g∗
Lipschitz where an RKHS norm that is exponentially large in the (full) dimension p|Ω| is needed for
a small approximation error (see Bach, 2017a, Appendix D.5)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.488135593220339,"Related to this, Malach & Shalev-Shwartz (2021) show a separation in the different setting of learning
certain parity functions on the hypercube using gradient methods; their upper bound for convolutional
networks is based on a similar kernel regime as above. We note that kernels that exploit such a
localized structure have also been considered in the context of structured prediction for improved
statistical guarantees (Ciliberto et al., 2019)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.48983050847457626,"D
EXTENSIONS TO MORE LAYERS"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.4915254237288136,"In this section, we study the RKHS for convolutional kernels with more than 2 convolutional layers,
by considering the simple example of a 3-layer convolutional kernel K3 deﬁned by the feature map"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.49322033898305084,"Ψ(x) = A3M3P3A2M2P2A1M1P1x,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.49491525423728816,"with quadratic kernels at the second and third layer, i.e., k2(z, z′) = (⟨z, z′⟩)2 and k3(z, z′) =
(⟨z, z′⟩)2. By isomorphism, we may consider the sequence of Hilbert spaces Hℓto be H1 = H,
H2 = (H ⊗H)|S2|×|S2|, and H3 = (H⊗4)(|S3|×|S2|×|S2|)2. For some domain Ω, we deﬁne the
operators diag2 : L2(Ω4) →L2(Ω2) and its adjoint diag2 : L2(Ω2) →L2(Ω4) by"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.4966101694915254,"diag2(M)[u, v] = M[u, u, v, v]
for M ∈L2(Ω4)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.49830508474576274,"diag2(M)[u1, u2, u3, u4] = 1{u1 = u2} 1{u3 = u4}M[u1, u3]
for M ∈L2(Ω2)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5,"We may then describe the RKHS as follows.
Proposition 5 (RKHS of 3-layer CKN with quadratic k2/3). The RKHS of K3 when k2 and k3 are
quadratic kernels (⟨·, ·⟩)2 consists of functions of the form"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5016949152542373,"f(x) =
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5033898305084745,α∈(S3×S2×S2)2 X
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5050847457627119,"u1,u2,u3,uv∈Ω
Gα[u1, u2, u3, u4](xu1, xu2, xu3, xu4),
(13)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5067796610169492,"where Gα ∈L2(Ω4, H⊗4) obeys the constraint"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5084745762711864,"Gα ∈Range(Eα),
(14)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5101694915254237,"where the linear operator Eα : L2(Ω3) →L2(Ω4) for α = (p, q, r, p′, q′, r′) (with p, p′ ∈S3
and q, r, q′, r′ ∈S2) is deﬁned by"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.511864406779661,"Eαx = A∗
1,α diag2(A∗
2,α diag(A∗
3x))."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5135593220338983,"The operators A1,α and A2,α denote:"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5152542372881356,"A1,α = LqA1 ⊗LrA1 ⊗Lq′A1 ⊗Lr′A1
A2,α = LpA2 ⊗Lp′A2."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5169491525423728,"The squared RKHS norm ∥f∥2
HK3 is then equal to the minimum over decompositions (13) of the
quantity
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5186440677966102,"α
∥A†∗
3 diag(A†∗
2,α diag2(A†∗
1,αGα))∥2
L2(Ω3).
(15)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5203389830508475,"The constraint (14) and penalty (15) resemble the corresponding constraint/penalty in the two-
layer case for an order-4 polynomial kernel at the second layer, but provide more structure on the"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5220338983050847,Published as a conference paper at ICLR 2022
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.523728813559322,"interactions, using a multi-scale structure that may model interactions between certain pairs of patches
((xu1, xu2) and (xu3, xu4) in (13)) more strongly than those between all four patches. In addition to
localizing the interactions Gα around certain diagonals, the kernel also promotes spatial regularities:
assuming that the spatial bandwidths of Aℓincrease with ℓ, the functions (u, v, w1, w2) 7→Gα[u, u +
w1, u+v, u+v +w2] may vary quickly with w1 or w2 (distances between patches in each of the two
pairs), but should vary more slowly with v (distance between the two pairs) and even more slowly
with u (a global position)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5254237288135594,"E
PROOFS"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5271186440677966,"We recall the following result about reproducing kernel Hilbert spaces, which characterizes the RKHS
of kernels deﬁned by explicit Hilbert space features maps (see, e.g., Saitoh, 1997, §2.1).
Theorem 6 (RKHS from explicit feature map). Let H be some Hilbert space, ψ : X →H a
feature map, and K(x, x′) = ⟨ψ(x), ψ(x′)⟩H a kernel on X. The RKHS H of K consists of
functions f = ⟨g, ψ(·)⟩H, with norm"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5288135593220339,"∥f∥H = inf{∥g′∥H : g′ ∈H s.t. f = ⟨g′, ψ(·)⟩H}
(16)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5305084745762711,"We also state here the generalization bound for kernel ridge regression used in Section 4, adapted
from Bach (2021, Proposition 7.1).
Proposition 7 (Generalization bound for kernel ridge regression). Denote f ∗(x) = Eρ[y|x]. As-
sume f ∗∈H, Varρ[y|x] ≤σ2, K(x, x) ≤1 a.s., and deﬁne"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5322033898305085,"ˆfλ := arg min
f∈H
1
n n
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5338983050847458,"i=1
(yi −f(xi))2 + λ∥f∥2
H,
(17)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.535593220338983,"for i.i.d. data (xi, yi) ∼ρ, i = 1, . . . , n. Let"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5372881355932203,"n ≥max
∥f ∗∥∞"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5389830508474577,"σ2
,
∥f ∗∥2
H
σ2 EρX[K(x, x)], σ2 EρX[K(x, x)]"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5406779661016949,"∥f ∗∥2
H 
."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5423728813559322,"For λ =
p"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5440677966101695,"σ2 EρX[K(x, x)]/n∥f ∗∥2
H, we have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5457627118644067,E[R( ˆfλ) −R(f ∗)] ≤C∥f ∗∥H r
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5474576271186441,"σ2 EρX[K(x, x)]"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5491525423728814,"n
,
(18)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5508474576271186,where C is an absolute constant.
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5525423728813559,"Proof. Under the conditions of the theorem, we may apply (Bach, 2021, Proposition 7.1), which
states that for λ ≤1 and n ≥5"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5542372881355933,"λ(1 + log(1/λ)), we have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5559322033898305,E[R( ˆfλ) −R(f ∗)] ≤16σ2
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5576271186440678,"n Tr((Σ + λI)−1Σ) + 16λ⟨f ∗, (Σ + λI)−1Σf ∗⟩H + 24"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.559322033898305,"n2 ∥f ∗∥2
∞,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5610169491525424,"where Σ = EρX[K(x, ·) ⊗K(x, ·)] is the covariance operator. We conclude by using the inequalities"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5627118644067797,Tr((Σ + λI)−1Σ) ≤Tr(Σ)
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5644067796610169,"λ
= EρX[K(x, x)]"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5661016949152542,"λ
⟨f ∗, (Σ + λI)−1Σf ∗⟩H ≤∥f ∗∥2
H,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5677966101694916,and optimizing for λ.
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5694915254237288,"E.1
PROOF OF PROPOSITION 1 (RKHS OF ONE-LAYER CONVOLUTIONAL KERNEL)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5711864406779661,"Proof. From Theorem 6, the RKHS contains functions of the form"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5728813559322034,"f(x) = ⟨F, AΦ(x)⟩L2(Ω1,H),"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5745762711864407,"with RKHS norm equal to the minimum of ∥F∥L2(Ω1,H) over such decompositions."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.576271186440678,"We may alternatively write f(x) = ⟨G, Φ(x)⟩L2(Ω,H) with G = A∗F. The mapping from F to G is
one-to-one if G ∈Range(A∗). Then, we obtain that equivalently, the RKHS contains functions of
this form, with G ∈Range(A∗), and with RKHS norm equal to the minimum of ∥A†∗G∥L2(Ω1,H)
over such decompositions."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5779661016949152,Published as a conference paper at ICLR 2022
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5796610169491525,"E.2
PROOF OF PROPOSITION 2 (RKHS OF 2-LAYER CKN WITH QUADRATIC k2)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5813559322033899,"Proof. From Theorem 6, the RKHS contains functions of the form"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5830508474576271,"f(x) = ⟨F, A2M2P2A1Φ1(x)⟩L2(Ω2,H2),"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5847457627118644,"with RKHS norm equal to the minimum of ∥F∥L2(Ω2,H2) over such decompositions. Here, Φ1(x) ∈
L2(Ω, H) is given by Φ1(x)[u] = ϕ1(xu), so that Φ(x) in the statement is given by Φ(x) =
Φ1(x) ⊗Φ1(x). We also have that H2 = (H ⊗H)|S2|×|S2|, so that we may write F = (Fpq)p,q∈S2
with Fpq ∈L2(Ω2, H ⊗H)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5864406779661017,"For p, q ∈S2, denoting by Lc the translation operator Lcx[u] = x[u −c], we have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.588135593220339,"(M2P2A1Φ1(x)[u])pq = LpA1Φ1(x)[u] ⊗LqA1Φ1(x)[u]
= diag(LpA1Φ1(x) ⊗LqA1Φ1(x))[u]
= diag((LpA1 ⊗LqA1)Φ(x))[u]."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5898305084745763,"Then, we have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5915254237288136,"⟨Fpq, (A2M2P2A1Φ1(x))pq⟩L2(Ω2,H⊗H) = ⟨Fpq, A2 diag((LpA1 ⊗LqA1)Φ(x))⟩L2(Ω2,H⊗H)
= ⟨A∗
2Fpq, diag((LpA1 ⊗LqA1)Φ(x))⟩L2(Ω1,H⊗H)
= ⟨diag(A∗
2Fpq), (LpA1 ⊗LqA1)Φ(x)⟩L2(Ω2
1,H⊗H)
= ⟨(LpA1 ⊗LqA1)∗diag(A∗
2Fpq), Φ(x)⟩L2(Ω2,H⊗H)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5932203389830508,"We may then write this as ⟨Gpq, Φ(x)⟩L2(Ω2,H⊗H) with"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5949152542372881,"Gpq = (LpA1 ⊗LqA1)∗diag(A∗
2Fpq),"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5966101694915255,"and the mapping between Fpq ∈L2(Ω2, H ⊗H) and Gpq ∈L2(Ω2, H ⊗H) is one-to-one
if Gpq ∈Range((LpA1 ⊗LqA1)∗), and diag((LpA1 ⊗LqA1)†∗Gpq) ∈Range(A∗
2). We may
then equivalently write the RKHS norm as the minimum over Gpq satisfying such constraints for
all p, q ∈S2, of the quantity
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.5983050847457627,"p,q∈S2
∥Fpq∥2 =
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6,"p,q∈S2
∥A†∗
2 diag((LpA1 ⊗LqA1)†∗Gpq)∥2
L2(Ω2,H⊗H)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6016949152542372,"E.3
PROOF OF PROPOSITION 5 (RKHS OF 3-LAYER CKN WITH QUADRATIC k2/3)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6033898305084746,"Proof. Let Φ(x) = (ϕ1(xu))u ∈L2(Ω, H), so that we may write
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6050847457627119,"u1,u2,u3,uv∈Ω
G[u1, u2, u3, u4](xu1, xu2, xu3, xu4) = ⟨G, Φ(x)⊗4⟩L2(Ω4,H⊗4),"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6067796610169491,"for some G ∈L2(Ω4, H⊗4)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6084745762711864,"From Theorem 6, the RKHS contains functions of the form"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6101694915254238,"f(x) = ⟨F, A3M3P3A2Φ2(x)⟩L2(Ω3,H3),
(19)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.611864406779661,"with RKHS norm equal to the minimum of ∥F∥L2(Ω3,H3) over such decompositions. Here, Φ2(x) ∈
L2(Ω1, H2) = L2(Ω1, (H ⊗H)|S2|×|S2|) is given as in the proof of Proposition 2, by"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6135593220338983,"Φ2,qr(x)[u] = diag((LqA1 ⊗LrA1)(Φ(x) ⊗Φ(x)))[u],"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6152542372881356,"for q, r ∈S2. A patch P3A2Φ2(x)[u] is then given by"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6169491525423729,"P3A2Φ2(x)[u] = (LpA2Φ2,qr(x)[u])p∈S3,q,r∈S2 ∈(H ⊗H)|S3|×|S2|×|S2|."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6186440677966102,"Applying the quadratic feature map given by ϕ3(z) = z ⊗z ∈(H⊗4)(|S3|×|S2|×|S2|)2 for z ∈
(H ⊗H)|S3|×|S2|×|S2|, we obtain for α = (p, q, r, p′, q′, r′) ∈(S3 × S2 × S2)2,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6203389830508474,"(M3P3A2Φ2(x)[u])α = LpA2Φ2,qr(x)[u] ⊗Lp′A2Φ2,q′r′(x)[u]
= diag(A2,α(Φ2,qr(x) ⊗Φ2,q′r′(x)))[u],"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6220338983050847,Published as a conference paper at ICLR 2022
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6237288135593221,"where
A2,α = LpA2 ⊗Lp′A2.
Now, one can check that we have the following relation:"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6254237288135593,"Φ2,qr(x) ⊗Φ2,q′r′(x) = diag((LqA1 ⊗LrA1)(Φ(x) ⊗Φ(x))) ⊗diag((Lq′A1 ⊗Lr′A1)(Φ(x) ⊗Φ(x)))"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6271186440677966,"= diag2(A1,αΦ(x)⊗4),"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6288135593220339,"with
A1,α = LqA1 ⊗LrA1 ⊗Lq′A1 ⊗Lr′A1."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6305084745762712,"Since H3 = (H⊗4)(|S3|×|S2|×|S2|)2, we may write F = (Fα)α∈(S3×S2×S2)2, with each Fα ∈
L2(Ω3, H⊗4). We then have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6322033898305085,"⟨Fα,(A3M3P3A2Φ2(x))α⟩L2(Ω3)
= ⟨Fα, A3 diag(A2,α diag2(A1,αΦ(x)⊗4))⟩L2(Ω3)
= ⟨diag(A∗
3Fα), A2,α diag2(A1,αΦ(x)⊗4)⟩L2(Ω2
2)
= ⟨diag2(A∗
2,α diag(A∗
3Fα)), A1,αΦ(x)⊗4⟩L2(Ω4
1)
= ⟨A∗
1,α diag2(A∗
2,α diag(A∗
3Fα)), Φ(x)⊗4⟩L2(Ω4)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6338983050847458,"We may write this as ⟨Gα, Φ(x)⊗4⟩L2(Ω4,H⊗4), with"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.635593220338983,"Gα = A∗
1,α diag2(A∗
2,α diag(A∗
3Fα))."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6372881355932203,"The mapping from Fα to Gα is bijective if Gα is constrained to the lie in the range of the operator Eα.
If Gα satisﬁes this constraint, we may write"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6389830508474577,"Fα = A†∗
3 diag(A†∗
2,α diag2(A†∗
1,αGα))."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6406779661016949,"Then, the resulting penalty on Gα is as desired."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6423728813559322,"E.4
PROOF OF PROPOSITION 3 (GENERALIZATION FOR ONE-LAYER CKN)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6440677966101694,Proof. Note that we have
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6457627118644068,"Ex[K1(x, x)] =
X u X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6474576271186441,"v,r
h[u −v]h[u −v −r] Ex[k1(xv, xv−r)] ≤
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6491525423728813,"v,r
⟨h, Lrh⟩σ2
r"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6508474576271186,"= |Ω|
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.652542372881356,"r
⟨h, Lrh⟩σ2
r."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6542372881355932,"It remains to verify that ∥f ∗∥HK1 =
p"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6559322033898305,"|Ω|∥g∥H. Note that if we denote G = (g)u∈Ω∈L2(Ω, H),
then we have A∗G = G, since P
v h[v−u]G[v] = (P
v h[v−u])g = g. This implies that A∗†G = G,
regardless of which pooling ﬁlter is used. Then we have, by (5) that ∥f ∗∥2 ≤|Ω|∥g∥2. Further,
since g is of minimal norm, no other G ∈L2(Ω, H) may lead to a smaller norm, so that we can
conclude ∥f ∗∥2 = |Ω|∥g∥2."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6576271186440678,"E.5
PROOF OF PROPOSITION 4 (GENERALIZATION FOR TWO-LAYER CKN WITH
QUADRATIC k2)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6593220338983051,"Proof. We begin by studying the “variance” quantity Ex[K2(x, x)]. By expanding the construction
of the kernel K2, we may write"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6610169491525424,"K2(x, x) =
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6627118644067796,"p,q∈S2 X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6644067796610169,"u,v,v′
h2[u−v]h2[u−v′]× X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6661016949152543,"w1,w2
w′
1,w′
2"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6677966101694915,"h1[v−w1]h1[v−w2]h1[v′−w′
1]h1[v′−w′
2]k1(xw1−p, xw′
1−p)k1(xw2−q, xw′
2−q)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6694915254237288,Published as a conference paper at ICLR 2022
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6711864406779661,"Upper bounding the quantity E[k1(xw1−p, xw′
1−p)k1(xw2−q, xw′
2−q)] by 1 when w1 = w′
1 and w2 =
w′
2, and by ϵ otherwise, the sum of the coefﬁcients in front of 1 can be bounded as follows:
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6728813559322034,"p,q∈S2 X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6745762711864407,"u,v,v′,w1,w2
h2[u−v]h2[u−v′]h1[v−w1]h1[v−w2]h1[v′−w1]h1[v′−w2]"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.676271186440678,= |S2|2 X
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6779661016949152,"u,v,v′ X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6796610169491526,"v
h2[u−v]h2[u−v]⟨Lvh1, Lv′h1⟩2"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6813559322033899,= |S2|2 X
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6830508474576271,"v,v′
⟨Lvh2, Lv′h2⟩⟨Lvh1, Lv′h1⟩2"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6847457627118644,= |Ω||S2|2 X
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6864406779661016,"v
⟨h2, Lvh2⟩⟨h1, Lvh1⟩2."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.688135593220339,"while the sum of coefﬁcients bounded by ϵ is upper bounded by |Ω||S2|2. Overall, this yields"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6898305084745763,"Ex[K2(x, x)] ≤|S2|2|Ω| X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6915254237288135,"v
⟨h2, Lvh2⟩⟨h1, Lvh1⟩2 + ϵ !"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6932203389830508,".
(20)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6949152542372882,"The bounds obtained for the example function f ∗(x) = P
u,v g(xu, xv) rely on plugging in
the values of the Dirac ﬁlter h[u] = δ(u = 0) or average pooling ﬁlters h[u] = 1/|Ω| in
the expression of Ex[K2(x, x)], and on computing the norm ∥f ∗∥K2 for different architectures
using Eq. (7) in Proposition 2.
Computing Ex[K2(x, x)] is immediate using the expression
above. Bounding ∥f ∗∥K2 is more involved, and requires ﬁnding appropriate decompositions of
the form f ∗(x) = P p,q
P"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6966101694915254,"u,v Gpq[u, v](xu, xv) in order to leverage Proposition 2:"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.6983050847457627,"• When using a Dirac ﬁlter at the ﬁrst layer, we need |S2| = |Ω| in order to capture log-
range interaction terms, and we represent f ∗as a sum of Gp that are non-zero and equal
to g only on the p-th diagonal, i.e., Gp[u, v] = g when v = u + p, and zero otherwise.
We then verify P p
P"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7,"u,v Gp[u, v](xu, xv) = P p
P"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7016949152542373,"u g(xu, xu+p) = f ∗(x). Then, the
expression (7) on this decomposition yields |S2||Ω|∥g∥2
H⊗H = |Ω|2∥g∥2
H⊗H, for any choice
of pooling h2 such that ∥h2∥1 = 1 (using similar arguments to the proof of Proposition 3).
This is then equal to the squared norm of f ∗due to the minimality of ∥g∥H⊗H."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7033898305084746,"• When using average pooling at the ﬁrst layer and |S2| = |Ω|, we may use a single
term G[u, v] with all entries equal to g, i.e., a decomposition f ∗(x) = P"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7050847457627119,"u,v G[u, v](xu, xv).
Using (7), we obtain an upper bound |Ω|∥g∥2 on the squared norm. The same decomposition
can be used when |S2| = 1, leading to the same bound."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7067796610169491,"F
GENERALIZATION GAINS UNDER SPECIFIC DATA MODELS"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7084745762711865,"In this section, we consider simple models of architectures and data distribution where we may
quantify more precisely the improvements in sample complexity guarantees thanks to pooling."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7101694915254237,"We consider architectures with non-overlapping patches, and a data distribution where the patches are
independent and uniformly distributed on the sphere Sd−1 sphere in d := p|S1| dimensions. Further,
we consider a dot-product kernel on patches of the form k1(z, z′) = κ(⟨z, z′⟩) for z, z′ ∈Sd−1, with
the common normalization κ(1) = 1."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.711864406779661,"In the construction of Section 2, using non-overlapping patches corresponds to taking a downsampling
factor s1 = |S1| at the ﬁrst layer, and a corresponding pooling ﬁlter such that h1[u] = 0 for u ̸= 0
mod |S1|. Alternatively, we may more simply denote patches by xu with u ∈Ωby considering a
modiﬁed signal with more channels (xu = x[u] of dimension pe instead of p, where e is the patch size)
so that extracting patches of size 1 actually corresponds to a patch of size e of the underlying signal.
We then have that the patches xu in a signal x are i.i.d., uniformly distributed on the sphere Sd−1.
We denote the uniform measure on Sd−1 by dτ."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7135593220338983,Published as a conference paper at ICLR 2022
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7152542372881356,"We note that when patches are in high dimension, overlapping patches may become near-orthogonal,
which could allow extensions of our arguments below to the case with overlap, yet this may require
different tools similar to Mei et al. (2021). We leave these questions to future work."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7169491525423729,"F.1
QUANTIFYING THE TRACE OF THE COVARIANCE OPERATOR E[K(x, x)]"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7186440677966102,"In this section, we focus on the “variance” term E[K(x, x)] which is used in the generalization results
of Section 4."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7203389830508474,"One layer.
In the one-layer case, we clearly have σ2
0 := E[κ(⟨xu, xu⟩)] = κ(1) = 1. For u ̸= v,
since patches xu and xv are independent and i.i.d., σ2
u−v is a constant independent of u, v, which
may be computed by integration on the sphere as:"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7220338983050848,"σ2
u−v = E[κ(⟨xu, xv⟩)] = Exu∼τ[Exv∼τ[κ(⟨xu, xv⟩)|xu]] = ωd−2 ωd−1 Z 1"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7237288135593221,"−1
κ(t)(1 −t2)
d−3"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7254237288135593,"2 dt, (21)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7271186440677966,"where we used a standard change of variable t = ⟨xu, xv⟩when integrating xv over Sd−1 (see,
e.g., Efthimiou & Frye, 2014), with ωp−1 = 2πp/2/Γ(p/2) the surface measure of the sphere Sp−1.
Note that the integral in (21) corresponds to the constant component in the Legendre decomposition
of κ, which is known for common kernels as consider in various works studying spectral properties
of dot-product kernels (Bach, 2017a; Minh et al., 2006). For instance, for the exponential kernel"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7288135593220338,"(or Gaussian on the sphere) κ(⟨x, y⟩) = e−∥x−y∥2"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7305084745762712,"2σ2
= e
1
σ2 (⟨x,y⟩−1), which is used in most of our
experiments with σ = 0.6, we have (Minh et al., 2006, Theorem 2):"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7322033898305085,"ωd−2
ωd−1 Z 1"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7338983050847457,"−1
κ(t)(1 −t2)
d−3"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.735593220338983,"2 dt = e−1/σ2(2σ2)(d−2)/2Id/2−1(1/σ2)Γ(d/2),"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7372881355932204,"where I denotes the modiﬁed Bessel function of the ﬁrst kind. For arc-cosine kernels, it may be
obtained by leveraging the random feature expansion of the kernel (Bach, 2017a). More generally,
we also note that when the patch dimension d is large, we have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7389830508474576,"ωd−2
ωd−1 Z 1"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7406779661016949,"−1
κ(t)(1 −t2)
d−3"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7423728813559322,"2 dt →κ(0),
as d →∞,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7440677966101695,since ωd−2
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7457627118644068,"ωd−1 (1 −t2)
d−3"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.747457627118644,"2
is a probability density that converges weakly to a Dirac mass at 0. In"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7491525423728813,"particular, for the exponential kernel with σ = 0.6, we have κ(0) = e−1/σ2 ≈0.06. When learning
a translation-invariant function, the bound in Prop. 3 then shows that global average pooling yields an
improvement w.r.t. no pooling of order |Ω|/(1+0.06|Ω|). Note that removing the constant component
of κ, i.e., using the kernel κ(u)−κ(0)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7508474576271187,"κ(1)−κ(0) , may further improve this bound, leading to a denominator very
close to 1 when d is large, and hence an improvement in sample complexity of order |Ω|. We also
remark that the dependence on κ(0) may be removed by using a ﬁner generalization analysis beyond
uniform convergence that leverages spectral properties of the kernel (see Section F.2)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.752542372881356,"Two layers with quadratic k2.
For the two-layer case,
we may obtain expressions
of E[k1(xu, xu′)k1(xv, xv′)] as above. Denote ϵ := Ez,z′∼τ[k1(z, z′)] where z, z′ are independent,
which is given in (21). We may have the following cases:"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7542372881355932,"• If u = u′ and v = v′, we have, trivially, E[k1(xu, xu′)k1(xv, xv′)] = 1.
• If u = u′ and v ̸= v′, we have E[k1(xu, xu′)k1(xv, xv′)] = E[k1(xv, xv′)] = ϵ, since xv and xv′
are independent. The same holds if u ̸= u′ and v = v′.
• If |{u, v, u′, v′}| = 4, we have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7559322033898305,"E[k1(xu, xu′)k1(xv, xv′)] = E[k1(xu, xu′)] E[k1(xv, xv′)] = ϵ2."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7576271186440678,"• If u = v and |{u, u′, v′}| = 3, then we have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7593220338983051,"E[k1(xu, xu′)k1(xv, xv′)] = Exu[Exu′[k1(xu, xu′)|xu] Ex′v[k1(xu, xv′)|xu]] = ϵ2,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7610169491525424,"by using Exu′[k1(xu, xu′)|xu] = Ez,z′∼τ[k1(z, z′)], which holds by rotational invariance."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7627118644067796,Published as a conference paper at ICLR 2022
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.764406779661017,"• If u = v′ and u′ = v, we have E[k1(xu, xu′)k1(xv, xv′)] = Ez,z′∼τ[k1(z, z′)2] =: ˜ϵ. This takes
the same form as (21), but with a different kernel function κ2 instead of κ. Note that in the case
of the Exponential kernel, κ2 is also an exponential kernel with different bandwidth."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7661016949152543,"Overall, when ϵ and ˜ϵ are small compared to 1, we can see that the quantity E[k1(xu, xu′)k1(xv, xv′)]
is small compared to 1 unless u = u′ and v = v′, thus satisfying the assumptions in Prop. 4. As
described above, we may obtain expressions of ϵ and ˜ϵ in various cases, and in particular these vanish
in high dimension when using a kernel with κ(0) = 0."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7677966101694915,"F.2
FAST RATES"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7694915254237288,"In this section, we derive spectral decompositions of 1-layer CKN architectures with non-overlapping
patches under the product of spheres distribution described in the previous section. This allows
us to derive fast rates that depend on the complexity of the target functions on patches, and shows
similar improvement factors to those derived in Section F.1, without the κ(0) term, which in fact turns
out to only be due to a single eigenspace, namely constant functions. We note that our derivation
extends (Favero et al., 2021) to the case of generic pooling ﬁlters, and considers a different data
distribution."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7711864406779662,"Before studying the 1-layer case, we remark that while it may seem natural to extend such decom-
positions to the 2-layer case using tensor products of spherical harmonics, as done by Scetbon &
Harchaoui (2020) in the case without pooling, it appears that pooling may make it more challenging
to ﬁnd an eigenbasis since subspaces consisting of tensor products of spherical harmonics with ﬁxed
total degree are no longer left stable by the kernel.5 We thus leave such a study to future work."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7728813559322034,We begin by considering the following Mercer decomposition of the patch kernel
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7745762711864407,"k1(z, z′) = κ(⟨z, z′⟩) = ∞
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7762711864406779,"k=0
µk"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7779661016949152,"N(d,k)
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7796610169491526,"j=1
Yk,j(z)Yk,j(z′),"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7813559322033898,"where Yk,j for k ≥0 and j = 1, . . . , N(d, k) are spherical harmonic polynomials of degree k
forming an orthonormal basis of L2(dτ). The µk here are Legendre/Gegenbauer coefﬁcients of the
function κ (see, e.g., Bach, 2017a; Smola et al., 2001)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7830508474576271,Note that the 1-layer kernel may be written as
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7847457627118644,"Kh(x, y) =
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7864406779661017,"u,v∈Ω
h ⊛¯h[u −v]κ(⟨xu, yv⟩),"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.788135593220339,"where ⊛denotes circular convolution and ¯h[u] := h[−u]. We denote by ˜ew[u] = exp(2iπwu/|Ω|),
w = 0, . . . , |Ω|−1, the DFT basis vectors, and by ew = ˜ew/
p"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7898305084745763,"|Ω| the normalized DFT basis vectors,
which satisfy ⟨ew, ew⟩= P"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7915254237288135,"u ew[u]e∗
w[u] = 1, where z∗is the complex conjugate of z. Deﬁne the
Fourier coefﬁcients
ˆh[w] = ⟨h, ˜ew⟩=
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7932203389830509,"u
h[u]e−2iπwu/|Ω|,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7949152542372881,"and let λw := \
h ⊛¯h[w] = |ˆh[w]|2. Note that when the ﬁlter is normalized s.t. ∥h∥1 = 1, we
have λ0 = ˆh[0] = 1. We will also use the Parseval identity ∥h∥2
2 = (P"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7966101694915254,"w λw)/|Ω|. Using the inverse
DFT, it holds"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.7983050847457627,h ⊛¯h[u −v] = 1 |Ω|
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8,"|Ω|−1
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8016949152542373,"w=0
λw˜ew[u −u] = 1 |Ω|"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8033898305084746,"|Ω|−1
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8050847457627118,"w=0
λw˜ew[u]˜e∗
w[v] ="
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8067796610169492,"|Ω|−1
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8084745762711865,"w=0
λwew[u]e∗
w[v]."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8101694915254237,"Then, we have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.811864406779661,"Kh(x, y) ="
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8135593220338984,"|Ω|−1
X w=0 X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8152542372881356,"k≥0
λwµk"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8169491525423729,"N(d,k)
X j=1 X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8186440677966101,"u
ew[u]Yk,j(xu) !  X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8203389830508474,"u
e∗
w[u]Yk,j(yu) ! ."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8220338983050848,"5For instance, the term k1(xw, yu)k1(xw, yv), which may only appear in the presence of pooling, maps the
polynomial Yk(xu)Yk(xv) of degree 2k to a polynomial µ2
kYk(xw)2 which is not necessarily orthogonal to all
spherical harmonics tensor products of degree smaller than 2k."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.823728813559322,Published as a conference paper at ICLR 2022
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8254237288135593,"Note that when k = 0, we have Y0,1(xu) = 1 for all u, hence
 X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8271186440677966,"u
ew[u]Y0,1(xu) !  X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8288135593220339,"u
e∗
w[u]Y0,1(yu) ! = (
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8305084745762712,"u
ew[u])(
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8322033898305085,"v
e∗
w[v]) = |Ω| 1{w = 0},"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8338983050847457,"since e0 = |Ω|−1/2(1, . . . , 1) and P"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8355932203389831,"u ew[u] = 0 for w > 0. Then, we may write"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8372881355932204,"Kh(x, y) = |Ω|λ0µ0φ0(x)φ∗
0(y) +"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8389830508474576,"|Ω|−1
X w=0 X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8406779661016949,"k≥1
λwµk"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8423728813559322,"N(d,k)
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8440677966101695,"j=1
φw,k,j(x)φ∗
w,k,j(y),"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8457627118644068,"with φ0(x) = 1, φw,k,j(x) = P"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.847457627118644,"u ew[u]Yk,j(xu), and φ∗denotes the complex conjugate of φ."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8491525423728814,"It is then easy to check that the φ0 and φw,k,j form an orthonormal basis of L2(dτ ⊗|Ω|). We thus have
obtained a Mercer decomposition of the kernel Kh w.r.t. the data distribution, so that its eigenvalues
are also the eigenvalues of the covariance operator (Caponnetto & De Vito, 2007), and control
generalization performance, typically through the degrees of freedom"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8508474576271187,"N(λ) = Tr((Σ + λI)−1Σ) =
X m≥0"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8525423728813559,"ξm
λ + ξm
,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8542372881355932,"where Σ is the covariance operator, and (ξm)m is the collection of its eigenvalues. In particular, if we
have a decay ξm ≍m−α (with α < 1), then we have N(λ) ≤O(λ−1/α), which then leads to a fast
rate of n−α/(α+1) on the excess risk when optimizing for λ in kernel ridge regression (Bach, 2021;
Caponnetto & De Vito, 2007). In our case, the degrees of freedom takes the form"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8559322033898306,"Nh(λ) =
|Ω|λ0µ0
λ + |Ω|λ0µ0
+"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8576271186440678,"|Ω|−1
X w=0 X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8593220338983051,"k≥1
N(d, k)
λwµk
λ + λwµk
(22)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8610169491525423,We make a few remarks:
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8627118644067797,"• Given that the number of spatial frequencies w is ﬁxed, the asymptotic decay rate of eigen-
values associated to the φw,k,j (that is, λwµk, each with multiplicity N(d, k)) is the same as
that of the eigenvalues associated to φw0,k,j for some ﬁxed w0, which in turn corresponds
to the decay for the corresponding dot-product kernel on the sphere. For instance, if κ is
the arc-cosine kernel, we have ξm ≍m−α with α = d+2"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.864406779661017,"d−1, and more generally α =
2s
d−1
for a kernel resembling a Sobolev space with s bounded derivatives. This then leads to
a rate n2s/(2s+(d−1)), which only depends on the dimension d of patches, rather than the
full dimension d|Ω|. One may also add more general assumption on the smoothness of the
localized components of f ∗(such as g in Proposition 3) in order to get rates that depend
explicitly on the order of smoothness s of such components (as in Caponnetto & De Vito,
2007)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8661016949152542,"• The eigenvalue associated to φ0 plays a minor role as by itself as it only contributes at most
τ 2
ρ/n to the excess risk, which is negligible compared to the rest of the eigenvalues which lead
to a slower n−α/(α+1) rate."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8677966101694915,"• With no pooling (h is a Dirac delta), we have λw = |ˆh[w]|2 = 1 for all w. We then have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8694915254237288,"Nh(λ) ≤1 + |Ω|Nκ(λ),"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8711864406779661,"where we deﬁned Nκ(λ) := P
k≥1 N(d, k)µk/(λ + µk)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8728813559322034,"• With global pooling (h = 1/|Ω| is constant), we have λ0 = 1, and λw = 0 for w > 0. This
yields
Nh(λ) ≤1 + Nκ(λ).
(23)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8745762711864407,"This then yields an improvement by a factor |Ω| in sample complexity guarantees compared
to the scenario above with no pooling, namely the dominant term in the excess risk bound will
be C(1/n)α/(α+1) compared to C(|Ω|/n)α/(α+1)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8762711864406779,Published as a conference paper at ICLR 2022
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8779661016949153,"• For more general pooling, one may exploit speciﬁc decays of λw to obtain ﬁner bounds. We
may also obtain the following bound by Jensen’s inequality"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8796610169491526,"Nh(λ) ≤1 + |Ω|
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8813559322033898,"k≥1
N(d, k)
¯λµk
λ + ¯λµk"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8830508474576271,≤1 + |Ω|Nκ
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8847457627118644,"
λ
∥h∥2
2 "
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8864406779661017,where we used that ¯λ = (P
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.888135593220339,"w λw)/|Ω| = ∥h∥2
2, by Parseval’s identity. When Nκ(λ) ≤
Cκλ−1/α, we get a bound"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8898305084745762,"Nh(λ) ≤1 + Cκ|Ω|∥h∥2/α
2
λ−1/α.
(24)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8915254237288136,"instead of a bound N(λ) ≤1+C|Ω|λ−1/α for the case of no pooling, that is, the improvement
is again controlled by ∥h∥2
2, which goes from 1/|Ω| for global pooling, to 1 for no pooling."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8932203389830509,"The following result provides an example of a generalization bound for invariant target functions,
which illustrates that there is no curse of dimensionality in the rate if the patch dimension is much
smaller than the full dimension (i.e., d ≪d|Ω|), as well as the beneﬁts of pooling.
Theorem 8 (Fast rates for one-layer CKN on invariant targets). Consider an invariant target of the
form f ∗(x) = P"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8949152542372881,"u∈Ωg∗(xu), with Ez∼dτ[g∗(z)] = 0, and assume:"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8966101694915254,"• (capacity condition) Nκ(λ) ≤Cκλ−1/α,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.8983050847457628,"• (source condition) g∗= T r
κg0 and ∥g0∥L2(dτ) ≤C∗, where Tκ is the integral operator of
the kernel κ on L2(dτ), with r > α−1 2α ."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9,"Then, kernel ridge regression with the one-layer CKN kernel Kh with pooling ﬁlter h (with ∥h∥1 = 1)
satisﬁes, for n large enough,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9016949152542373,E[R( ˆfn)] −R(f ∗) ≤C|Ω|
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9033898305084745,"∥h∥2/α
2
n"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9050847457627119,"!
2αr
2αr+1
,
(25)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9067796610169492,"where C is independent of |Ω| and h. For global pooling, the factor ∥h∥2/α
2
= |Ω|−1/α can be
improved to |Ω|−1. In contrast, with no pooling we have ∥h∥2/α
2
= 1, i.e., n needs to be |Ω| times
larger for the same guarantee. Note that for α →1 and r = 1/2, the resulting bound resembles that
of Proposition 3."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9084745762711864,"We note that if g∗is assumed to be s-smooth on the sphere, the source condition with 2αr =
2s
d−1
corresponds to a Sobolev condition of order s, and leads to the bound"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9101694915254237,E[R( ˆfn)] −R(f ∗) ≤C|Ω|
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.911864406779661,"∥h∥2/α
2
n"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9135593220338983,"!
2s
2s+d−1
,
(26)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9152542372881356,which highlights that the rate only depends on the patch dimension d instead of the full dimension d|Ω|.
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9169491525423729,"Proof. Under the conditions of the theorem, we may apply (Bach, 2021, Proposition 7.2), which
states that for λ ≤1 and n ≥5"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9186440677966101,"λ(1 + log(1/λ)), we have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9203389830508475,"E[R( ˆfλ)] −R(f ∗) ≤16τ 2
ρ
n Nh(λ) + 16Ah(λ, f ∗) + 24"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9220338983050848,"n2 ∥f ∗∥2
∞,
(27)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.923728813559322,"where the degrees of freedom Nh(λ) is given in (22) and satisﬁes the upper bound (24), and the
approximation error is given by"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9254237288135593,"Ah(λ, f ∗) = min
f∈HK ∥f −f ∗∥2
L2(dτ ⊗|Ω|) + λ∥f∥2
HKh,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9271186440677966,where HKh is the RKHS of Kh. Denote by
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9288135593220339,"f = a0φ0 +
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9305084745762712,"w,k,j
aw,k,jφw,k,j,
f ∗= a∗
0φ0 +
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9322033898305084,"w,k,j
a∗
w,k,jφw,k,j"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9338983050847458,Published as a conference paper at ICLR 2022
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9355932203389831,the decompositions of f and f ∗in the orthonormal basis deﬁned above. If g∗= P
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9372881355932203,"k,j gk,jYk,j is the
spherical harmonic decomposition of g∗, then we have"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9389830508474576,"a∗
0 = E[f ∗(x)] = 0"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.940677966101695,"a∗
0,k,j = E[f ∗(x)φ∗
0,k,j(x)] = gk,j
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9423728813559322,"u
e∗
0[u] =
p"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9440677966101695,"|Ω|gk,j"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9457627118644067,"a∗
w,k,j = 0
for w ̸= 0."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9474576271186441,This yields
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9491525423728814,"Ah(λ, f ∗) =
min
a0,a0,k,j(a0 −a∗
0)2 + λ
a2
0
|Ω|λ0µ0
+
X k≥1"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9508474576271186,"N(d,k)
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9525423728813559,"j=1
(a0,k,j −a∗
0,k,j)2 + λ
a2
0,k,j
λ0µk"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9542372881355933,"= min
bk,j X k≥1"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9559322033898305,"N(d,k)
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9576271186440678,"j=1
|Ω|(bk,j −gk,j)2 + λ|Ω|
b2
k,j
λ0µk"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9593220338983051,"= |Ω| min
bk,j X k≥0"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9610169491525423,"N(d,k)
X"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9627118644067797,"j=1
(bk,j −gk,j)2 + λ
b2
k,j
µk"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.964406779661017,"= |Ω| min
g∈H ∥g −g∗∥2
L2(dτ) + λ∥g∥2
H = |Ω|Aκ(λ, g∗)"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9661016949152542,"where the second line uses a∗
0 = 0 and considers bk,j = a0,k,j/
p"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9677966101694915,"|Ω|, while the third line uses g0,1 =
0 and the fact that λ0 = 1 regardless of the choice of pooling ﬁlter h. Thus, Ah(λ, f ∗) does not
depend on h, and corresponds to the approximation error Aκ(λ, g∗) of the patch kernel κ on the
sphere (up to a factor |Ω|). Under the source condition, we then have, by (Cucker & Smale, 2002,
Theorem 3, p.33),"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9694915254237289,"Ah(λ, f ∗) = |Ω|Aκ(λ, g∗) ≤|Ω|C2
∗λ2r."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9711864406779661,"with a∗
0,k,j = a∗
0. Combining with (24) and plugging this into (27), we obtain"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9728813559322034,"E[R( ˆfλ)] −R(f ∗) ≲τ 2
ρ
n + τ 2
ρCκ|Ω|∥h∥2/α
2
λ−1/α"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9745762711864406,"n
+ |Ω|C2
∗λ2r + ∥f ∗∥2
∞
n2
."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.976271186440678,For the choice λn =
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9779661016949153,"τ 2
ρCκ|Ω|∥h∥2/α
2
rα|Ω|C2∗n"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9796610169491525,"!
α
2αr+1
,"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9813559322033898,we have
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9830508474576272,"E[R( ˆfλ)] −R(f ∗) ≲(|Ω|C2
∗)
1
2αr+1"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9847457627118644,"Cκτ 2
ρ|Ω|∥h∥2/α
2
n"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9864406779661017,"!
2αr
2αr+1
+ τ 2
ρ
n + ∥f ∗∥2 n2"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.988135593220339,"= (Cκτ 2
ρ)
2αr
2αr+1 C"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9898305084745763,"2
2αr+1
∗
|Ω|"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9915254237288136,"∥h∥2/α
2
n"
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9932203389830508,"!
2αr
2αr+1
+ τ 2
ρ
n + ∥f ∗∥2 n2
."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9949152542372881,"In the case of global pooling, the term in parentheses scales as (|Ω|−1/α/n)
2αr
2αr+1 , but can be
improved to (1/|Ω|n)
2αr
2αr+1 by using the bound (23) on Nh(λ) instead of (24)."
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9966101694915255,When r > α−1
"WE
USE
THE
PYTORCH
IMPLEMENTATION
AVAILABLE
AT",0.9983050847457627,"2α , we can choose n large enough so that n ≥
5
λn (1 + log(1/λn)) is satisﬁed, and the
higher order terms in n are negligible."
