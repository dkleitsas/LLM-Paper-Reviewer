Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015267175572519084,"Game-based decision-making involves reasoning over both world dynamics and
1"
ABSTRACT,0.0030534351145038168,"strategic interactions among the agents. Typically, empirical models capturing these
2"
ABSTRACT,0.004580152671755725,"respective aspects are learned and used separately. We investigate the potential gain
3"
ABSTRACT,0.0061068702290076335,"from co-learning these elements: a world model for dynamics and an empirical
4"
ABSTRACT,0.007633587786259542,"game for strategic interactions. Empirical games drive world models toward a
5"
ABSTRACT,0.00916030534351145,"broader consideration of possible game dynamics induced by a diversity of strategy
6"
ABSTRACT,0.010687022900763359,"profiles. Conversely, world models guide empirical games to efficiently discover
7"
ABSTRACT,0.012213740458015267,"new strategies through planning. We demonstrate these benefits first independently,
8"
ABSTRACT,0.013740458015267175,"then in combination as realized by a new algorithm, Dyna-PSRO, that co-learns
9"
ABSTRACT,0.015267175572519083,"an empirical game and a world model. When compared to PSRO—a baseline
10"
ABSTRACT,0.016793893129770993,"empirical-game building algorithm, Dyna-PSRO is found to compute lower regret
11"
ABSTRACT,0.0183206106870229,"solutions on partially observable general-sum games. In our experiments, Dyna-
12"
ABSTRACT,0.01984732824427481,"PSRO also requires substantially fewer experiences than PSRO, a key algorithmic
13"
ABSTRACT,0.021374045801526718,"advantage for settings where collecting player-game interaction data is a cost-
14"
ABSTRACT,0.022900763358778626,"limiting factor.
15"
INTRODUCTION,0.024427480916030534,"1
Introduction
16"
INTRODUCTION,0.025954198473282442,"Even seemingly simple games can actually embody a level of complexity rendering them intractable
17"
INTRODUCTION,0.02748091603053435,"to direct reasoning. This complexity stems from the interplay of two sources: dynamics of the
18"
INTRODUCTION,0.02900763358778626,"game environment, and strategic interactions among the game’s players. As an alternative to direct
19"
INTRODUCTION,0.030534351145038167,"reasoning, models have been developed to facilitate reasoning over these distinct aspects of the game.
20"
INTRODUCTION,0.03206106870229008,"Empirical games capture strategic interactions in the form of payoff estimates for joint policies [80].
21"
INTRODUCTION,0.03358778625954199,"World models represent a game’s transition dynamics and reward signal directly [69, 19]. Whereas
22"
INTRODUCTION,0.035114503816793895,"each of these forms of model have been found useful for game reasoning, typical use in prior work
23"
INTRODUCTION,0.0366412213740458,"has focused on one or the other, learned and employed in isolation from its natural counterpart.
24"
INTRODUCTION,0.03816793893129771,"Co-learning both models presents an opportunity to leverage their complementary strengths as a
25"
INTRODUCTION,0.03969465648854962,"means to improve each other. World models predict successor states and rewards given a game’s
26"
INTRODUCTION,0.04122137404580153,"current state and action(s). However, their performance depends on coverage of their training data,
27"
INTRODUCTION,0.042748091603053436,"which is limited by the range of strategies considered during learning. Empirical games can inform
28"
INTRODUCTION,0.044274809160305344,"training of world models by suggesting a diverse set of salient strategies, based on game-theoretic
29"
INTRODUCTION,0.04580152671755725,"reasoning [80]. These strategies can expose the world model to a broader range of relevant dynamics.
30"
INTRODUCTION,0.04732824427480916,"Moreover, as empirical games are estimated through simulation of strategy profiles, this same
31"
INTRODUCTION,0.04885496183206107,"simulation data can be reused as training data for the world model.
32"
INTRODUCTION,0.050381679389312976,"Strategic diversity through empirical games, however, comes at a cost. In the popular framework
33"
INTRODUCTION,0.051908396946564885,"of Policy-Space Response Oracles (PSRO) [38], empirical normal-form game models are built
34"
INTRODUCTION,0.05343511450381679,"iteratively, at each step expanding a restricted strategy set by computing best-response policies to
35"
INTRODUCTION,0.0549618320610687,"the current game’s solution. As computing an exact best-response is generally intractable, PSRO
36"
INTRODUCTION,0.05648854961832061,"uses Deep Reinforcement Learning (DRL) to compute approximate response policies. However,
37"
INTRODUCTION,0.05801526717557252,"each application of DRL can be considerably resource-intensive, necessitating the generation of
38"
INTRODUCTION,0.059541984732824425,"a vast amount of gameplays for learning. Whether gameplays, or experiences, are generated via
39"
INTRODUCTION,0.061068702290076333,"simulation [48] or from real-world interactions [24], their collection poses a major limiting factor in
40"
INTRODUCTION,0.06259541984732825,"DRL and by extension PSRO. World models present one avenue to reduce this cost by transferring
41"
INTRODUCTION,0.06412213740458016,"previously learned game dynamics across response computations.
42"
INTRODUCTION,0.06564885496183206,"World
Model
Empirical
Game"
INTRODUCTION,0.06717557251908397,Dyna-PSRO
INTRODUCTION,0.06870229007633588,Strategic Diversity
INTRODUCTION,0.07022900763358779,Planning
INTRODUCTION,0.0717557251908397,"Figure 1: Dyna-PSRO co-learns a world model and
empirical game. Empirical games offer world mod-
els strategically diverse game dynamics. World
models offer empirical games more efficient strat-
egy discovery through planning."
INTRODUCTION,0.0732824427480916,"We investigate the mutual benefits of co-learning
43"
INTRODUCTION,0.07480916030534351,"a world model and an empirical game by first
44"
INTRODUCTION,0.07633587786259542,"verifying the potential contributions of each
45"
INTRODUCTION,0.07786259541984733,"component independently. We then show how
46"
INTRODUCTION,0.07938931297709924,"to realize the combined effects in a new algo-
47"
INTRODUCTION,0.08091603053435115,"rithm, Dyna-PSRO, that co-learns a world model
48"
INTRODUCTION,0.08244274809160305,"and an empirical game (illustrated in Figure 1).
49"
INTRODUCTION,0.08396946564885496,"Dyna-PSRO extends PSRO to learn a world
50"
INTRODUCTION,0.08549618320610687,"model concurrently with empirical game expan-
51"
INTRODUCTION,0.08702290076335878,"sion, and applies this world model to reduce the
52"
INTRODUCTION,0.08854961832061069,"computational cost of computing new policies.
53"
INTRODUCTION,0.0900763358778626,"This is implemented by a Dyna-based reinforce-
54"
INTRODUCTION,0.0916030534351145,"ment learner [67, 68] that integrates planning,
55"
INTRODUCTION,0.09312977099236641,"acting, and learning in parallel. Dyna-PSRO
56"
INTRODUCTION,0.09465648854961832,"is evaluated against PSRO on a collection of
57"
INTRODUCTION,0.09618320610687023,"partially observable general-sum games. In our
58"
INTRODUCTION,0.09770992366412214,"experiments, Dyna-PSRO found lower-regret
59"
INTRODUCTION,0.09923664122137404,"solutions while requiring substantially fewer cu-
60"
INTRODUCTION,0.10076335877862595,"mulative experiences.
61"
INTRODUCTION,0.10229007633587786,"The main points of novelty of this paper are as follows: (1) empirically demonstrate that world models
62"
INTRODUCTION,0.10381679389312977,"benefit from the strategic diversity induced by an empirical game; (2) empirically demonstrate that a
63"
INTRODUCTION,0.10534351145038168,"world model can be effectively transferred and used in planning with new other-players. The major
64"
INTRODUCTION,0.10687022900763359,"contribution of this work is a new algorithm, Dyna-PSRO, that co-learns an empirical game and
65"
INTRODUCTION,0.1083969465648855,"world model finding a stronger solution at less cost than the baseline, PSRO.
66"
RELATED WORK,0.1099236641221374,"2
Related Work
67"
RELATED WORK,0.11145038167938931,"Empirical Game Theoretic Analysis (EGTA).
The core idea of EGTA [80] is to reason over
68"
RELATED WORK,0.11297709923664122,"approximate game models (empirical games) estimated by simulation over a restricted strategy set.
69"
RELATED WORK,0.11450381679389313,"This basic approach was first demonstrated by Walsh et al. [77], in a study of pricing and bidding
70"
RELATED WORK,0.11603053435114503,"games. Phelps et al. [51] introduced the idea of extending a strategy set automatically through
71"
RELATED WORK,0.11755725190839694,"optimization, employing genetic search over a policy space. Schvartzman & Wellman [58] proposed
72"
RELATED WORK,0.11908396946564885,"using RL to derive new strategies that are approximate best responses (BRs) to the current empirical
73"
RELATED WORK,0.12061068702290076,"game’s Nash equilibrium. The general question of which strategies to add to an empirical game
74"
RELATED WORK,0.12213740458015267,"has been termed the strategy exploration problem [31]. PSRO [38] generalized the target for BR
75"
RELATED WORK,0.12366412213740458,"beyond NE, and introduced DRL for BR computation in empirical games. Many further variants and
76"
RELATED WORK,0.1251908396946565,"extensions of EGTA have been proposed, for example those using structured game representations
77"
RELATED WORK,0.1267175572519084,"such as extensive-form [43, 34]. Some prior work has considered transfer learning across BR
78"
RELATED WORK,0.1282442748091603,"computations in EGTA, specifically by reusing elements of policies and value functions [64, 65].
79"
RELATED WORK,0.1297709923664122,"Model-Based Reinforcement Learning (MBRL).
Model-Based RL algorithms construct or use
80"
RELATED WORK,0.13129770992366413,"a model of the environment (henceforth, world model) in the process of learning a policy or value
81"
RELATED WORK,0.13282442748091602,"function [69]. World models may either predict successor observations directly (e.g., at pixel
82"
RELATED WORK,0.13435114503816795,"level [76, 79]), or in a learned latent space [18, 17]. The world models can be either used for
83"
RELATED WORK,0.13587786259541984,"background planning by rolling out model-predicted trajectories to train a policy, or by decision-
84"
RELATED WORK,0.13740458015267176,"time planning where the world model is used to evaluate the current state by planning into the
85"
RELATED WORK,0.13893129770992366,"future. Talvitie [71] demonstrated that even in small Markov decision processes (MDP) [52], model-
86"
RELATED WORK,0.14045801526717558,"prediction errors tend to compound—rendering long-term planning at the abstraction of observations
87"
RELATED WORK,0.14198473282442747,"ineffective. A follow-up study demonstrated that for imperfect models, short-term planning was
88"
RELATED WORK,0.1435114503816794,"no better than repeatedly training on previously collected real experiences; however, medium-term
89"
RELATED WORK,0.1450381679389313,"planning offered advantages even with an imperfect model [27]. Parallel studies hypothesized that
90"
RELATED WORK,0.1465648854961832,"these errors are a result of insufficient data for that transition to be learned [36, 8]. To remedy
91"
RELATED WORK,0.1480916030534351,"the data insufficiency, ensembles of world models were proposed to account for world model
92"
RELATED WORK,0.14961832061068703,"uncertainty [8, 36, 84], and another line of inquiry used world model uncertainty to guide exploration
93"
RELATED WORK,0.15114503816793892,"in state-action space [3, 59]. This study extends this problem into the multiagent setting, where
94"
RELATED WORK,0.15267175572519084,"now other-agents may preclude transitions from occurring. The proposed remedy is to leverage the
95"
RELATED WORK,0.15419847328244274,"strategy exploration process of building an empirical game to guide data generation.
96"
RELATED WORK,0.15572519083969466,"Multiagent Reinforcement Learning (MARL).
Previous research intersecting MARL and MBRL
97"
RELATED WORK,0.15725190839694655,"has primarily focused on modeling the opponent, particularly in scenarios where the opponent is fixed
98"
RELATED WORK,0.15877862595419848,"and well-defined. Within specific game sub-classes, like cooperative games and two-player zero-sum
99"
RELATED WORK,0.16030534351145037,"games, it has been theoretically shown that opponent modeling reduces the sample complexity of
100"
RELATED WORK,0.1618320610687023,"RL [73, 85]. Opponent models can either explicitly [46, 15] or implicitly [4, 29] model the behavior
101"
RELATED WORK,0.1633587786259542,"of the opponent. Additionally, these models can either construct a single model of opponent behavior,
102"
RELATED WORK,0.1648854961832061,"or learn a set of models [12, 21]. While opponent modeling details are beyond the scope of this
103"
RELATED WORK,0.166412213740458,"study, readers can refer to Albrecht & Stone’s survey [1] for a comprehensive review on this subject.
104"
RELATED WORK,0.16793893129770993,"Instead, we consider the case where the learner has explicit access to the opponent’s policy during
105"
RELATED WORK,0.16946564885496182,"training, as is the case in empirical-game building. A natural example is that of Self-Play, where all
106"
RELATED WORK,0.17099236641221374,"agents play the same policy; therefore, a world model can be learned used to evaluate the quality of
107"
RELATED WORK,0.17251908396946564,"actions with Monte-Carlo Tree Search [60, 62, 72, 56]. Li et al. [41] expands on this by building a
108"
RELATED WORK,0.17404580152671756,"population of candidate opponent policies through PSRO to augment the search procedure. Krupnik
109"
RELATED WORK,0.17557251908396945,"et al. [35] demonstrated that a generative world model could be useful in multi-step opponent-action
110"
RELATED WORK,0.17709923664122137,"prediction. Sun et al. [66] examined modeling stateful game dynamics from observations when
111"
RELATED WORK,0.17862595419847327,"the agents’ policies are stationary. Chockalingam et al. [11] explored learning world models for
112"
RELATED WORK,0.1801526717557252,"homogeneous agents with a centralized controller in a cooperative game. World models may also be
113"
RELATED WORK,0.18167938931297709,"shared by independent reinforcement learners in cooperative games [81, 86].
114"
CO-LEARNING BENEFITS,0.183206106870229,"3
Co-Learning Benefits
115"
CO-LEARNING BENEFITS,0.18473282442748093,"We begin by specifying exactly what we mean by world model and empirical game. This requires
116"
CO-LEARNING BENEFITS,0.18625954198473282,"defining some primitive elements. Let t ∈T denote time in the real game, with st ∈S the
117"
CO-LEARNING BENEFITS,0.18778625954198475,"information state and ht ∈H the game state at time t. The information state st ≡(mπ,t, ot)
118"
CO-LEARNING BENEFITS,0.18931297709923664,"is composed of the agent’s memory mπ ∈Mπ , or recurrent state, and the current observation
119"
CO-LEARNING BENEFITS,0.19083969465648856,"o ∈O. Subscripts denote a player-specific component si, negative subscripts denote all but the
120"
CO-LEARNING BENEFITS,0.19236641221374046,"player s−i, and boldface denote the joint of all players s. The transition dynamics p : H × A →
121"
CO-LEARNING BENEFITS,0.19389312977099238,"∆(H) × ∆(R) define the game state update and reward signal. The agent experiences transitions, or
122"
CO-LEARNING BENEFITS,0.19541984732824427,"experiences, (st, at, rt+1, st+1) of the game; where, sequences of transitions are called trajectories τ
123"
CO-LEARNING BENEFITS,0.1969465648854962,"and trajectories ending in a terminal game state are episodes.
124"
CO-LEARNING BENEFITS,0.1984732824427481,"At the start of an episode, all players sample their current policy π from their strategy σ : Π →
125"
CO-LEARNING BENEFITS,0.2,"[0, 1], where Π is the policy space and Σ is the corresponding strategy space. A utility function
126"
CO-LEARNING BENEFITS,0.2015267175572519,"U : Π →Rn defines the payoffs/returns (i.e., cumulative reward) for each of n players. The tuple
127"
CO-LEARNING BENEFITS,0.20305343511450383,"Γ ≡(Π, U, n) defines a normal-form game (NFG) based on these elements. We represent empirical
128"
CO-LEARNING BENEFITS,0.20458015267175572,"games in normal form. An empirical normal-form game (ENFG) ˆΓ ≡( ˆΠ, ˆU, n) models a game
129"
CO-LEARNING BENEFITS,0.20610687022900764,"with a restricted strategy set ˆΠ and an estimated payoff function ˆU. An empirical game is typically
130"
CO-LEARNING BENEFITS,0.20763358778625954,"built by alternating between game reasoning and strategy exploration. During the game reasoning
131"
CO-LEARNING BENEFITS,0.20916030534351146,"phase, the empirical game is solved based on a solution concept predefined by the modeler. The
132"
CO-LEARNING BENEFITS,0.21068702290076335,"strategy exploration step uses this solution to generate new policies to add to the empirical game. One
133"
CO-LEARNING BENEFITS,0.21221374045801528,"common heuristic is to generate new policies that best-respond to the current solution [45, 57]. As
134"
CO-LEARNING BENEFITS,0.21374045801526717,"exact best-responses typically cannot be computed, RL or DRL are employed to derive approximate
135"
CO-LEARNING BENEFITS,0.2152671755725191,"best-responses [38].
136"
CO-LEARNING BENEFITS,0.216793893129771,"An agent world model w represents dynamics in terms of information available to the agent. Specifi-
137"
CO-LEARNING BENEFITS,0.2183206106870229,"cally, w maps information states and actions to observations and rewards, w : O×A×Mw →O×R,
138"
CO-LEARNING BENEFITS,0.2198473282442748,"where mw ∈Mw is the world model’s memory, or recurrent state. For simplicity, in this work, we
139"
CO-LEARNING BENEFITS,0.22137404580152673,"assume the agent learns and uses a deterministic world model, irrespective of stochasticity that may be
140"
CO-LEARNING BENEFITS,0.22290076335877862,"present in the true game. Specific implementation details for this work are provided in Appendix C.2.
141"
CO-LEARNING BENEFITS,0.22442748091603054,"Until now, we have implicitly assumed the need for distinct models. However, if a single model could
142"
CO-LEARNING BENEFITS,0.22595419847328244,"serve both functions, co-learning two separate models would not be needed. Empirical games, in
143"
CO-LEARNING BENEFITS,0.22748091603053436,"general, cannot replace a world model as they entirely abstract away any concept of game dynamics.
144"
CO-LEARNING BENEFITS,0.22900763358778625,"Conversely, world models have the potential to substitute for the payoff estimations in empirical
145"
CO-LEARNING BENEFITS,0.23053435114503817,"games by estimating payoffs as rollouts with the world model. We explore this possibility in an
146"
CO-LEARNING BENEFITS,0.23206106870229007,"auxiliary experiment included in Appendix E.4, but our findings indicate that this substitution is
147"
CO-LEARNING BENEFITS,0.233587786259542,"impractical. Due to compounding of model-prediction errors, the payoff estimates and entailed game
148"
CO-LEARNING BENEFITS,0.23511450381679388,"solutions were quite inaccurate.
149"
CO-LEARNING BENEFITS,0.2366412213740458,"Having defined the models and established the need for their separate instantiations, we can proceed
150"
CO-LEARNING BENEFITS,0.2381679389312977,"to evaluate the claims of beneficial co-learning. Our first experiment shows that the strategic diversity
151"
CO-LEARNING BENEFITS,0.23969465648854962,"embodied in an empirical game yields diverse game dynamics, resulting in the training of a more
152"
CO-LEARNING BENEFITS,0.24122137404580152,"performant world model. The second set of experiments demonstrates that a world model can help
153"
CO-LEARNING BENEFITS,0.24274809160305344,"reduce the computational cost of policy construction in an empirical game.
154"
STRATEGIC DIVERSITY,0.24427480916030533,"3.1
Strategic Diversity
155"
STRATEGIC DIVERSITY,0.24580152671755726,"A world model is trained to predict successor observations and rewards, from the current observations
156"
STRATEGIC DIVERSITY,0.24732824427480915,"and actions, using a supervised learning signal. Ideally, the training data would cover all possible
157"
STRATEGIC DIVERSITY,0.24885496183206107,"transitions. This is not feasible, so instead draws are conventionally taken from a dataset generated
158"
STRATEGIC DIVERSITY,0.250381679389313,"from play of a behavioral strategy. Performance of the world model is then measured against a target
159"
STRATEGIC DIVERSITY,0.25190839694656486,"strategy. Differences between the behavioral and target strategies present challenges in learning an
160"
STRATEGIC DIVERSITY,0.2534351145038168,"effective world model.
161"
STRATEGIC DIVERSITY,0.2549618320610687,"We call the probability of drawing a state-action pair under some strategy its reach probability. From
162"
STRATEGIC DIVERSITY,0.2564885496183206,"this, we define a strategy’s strategic diversity as the distribution induced from reach probabilities.
163"
STRATEGIC DIVERSITY,0.2580152671755725,"across the full state-action space. These terms allow us to observe two challenges for learning world
164"
STRATEGIC DIVERSITY,0.2595419847328244,"models. First, the diversity of the behavioral strategy ought to cover the target strategy’s diversity.
165"
STRATEGIC DIVERSITY,0.26106870229007634,"Otherwise, transitions will be absent from the training data. It is possible to supplement coverage of
166"
STRATEGIC DIVERSITY,0.26259541984732826,"the absent transitions if they can be generalized from covered data; however, this cannot be generally
167"
STRATEGIC DIVERSITY,0.2641221374045801,"guaranteed. Second, the closer the diversities are, the more accurate the learning objective will be.
168"
STRATEGIC DIVERSITY,0.26564885496183205,"An extended formal argument of these challenges is provided in Appendix C.3.
169"
STRATEGIC DIVERSITY,0.26717557251908397,"If the target strategy were known, we could readily construct the ideal training data for the world
170"
STRATEGIC DIVERSITY,0.2687022900763359,"model. However the target is generally not known at the outset; indeed determining this target is the
171"
STRATEGIC DIVERSITY,0.27022900763358776,"ultimate purpose of empirical game reasoning. The evolving empirical game essentially reflects a
172"
STRATEGIC DIVERSITY,0.2717557251908397,"search for the target. Serendipitously, construction of this empirical game entails generation of data
173"
STRATEGIC DIVERSITY,0.2732824427480916,"that captures elements of likely targets. This data can be reused for world model training without
174"
STRATEGIC DIVERSITY,0.2748091603053435,"incurring any additional data collection cost.
175"
STRATEGIC DIVERSITY,0.27633587786259545,"Game.
We evaluate the claims of independent co-learning benefits within the context of a commons
176"
STRATEGIC DIVERSITY,0.2778625954198473,"game called “Harvest”. In Harvest, players move around an orchard picking apples. The challenging
177"
STRATEGIC DIVERSITY,0.27938931297709924,"commons element is that apple regrowth rate is proportional to nearby apples, so that socially optimum
178"
STRATEGIC DIVERSITY,0.28091603053435116,"behavior would entail managed harvesting. Self-interested agents capture only part of the benefit of
179"
STRATEGIC DIVERSITY,0.2824427480916031,"optimal growth, thus non-cooperative equilibria tend to exhibit collective over-harvesting. The game
180"
STRATEGIC DIVERSITY,0.28396946564885495,"has established roots in human-behavioral studies [30] and in agent-based modeling of emergent
181"
STRATEGIC DIVERSITY,0.28549618320610687,"behavior [53, 40, 39]. For our initial experiments, we use a symmetric two-player version of the game,
182"
STRATEGIC DIVERSITY,0.2870229007633588,"where in-game entities are represented categorically [28]. Each player has a 10 × 10 viewbox within
183"
STRATEGIC DIVERSITY,0.2885496183206107,"their field of vision. The possible actions include moving in the four cardinal directions, rotating
184"
STRATEGIC DIVERSITY,0.2900763358778626,"either way, tagging, or remaining idle. A successful tag temporarily removes the other player from
185"
STRATEGIC DIVERSITY,0.2916030534351145,"the game, but can only be done to other nearby players. Players receive a reward of 1 for each apple
186"
STRATEGIC DIVERSITY,0.2931297709923664,"picked. More detailed information and visualizations are available in Appendix D.1.
187"
STRATEGIC DIVERSITY,0.29465648854961835,"Experiment.
To test the effects of strategic diversity, we train a suite of world models that differ
188"
STRATEGIC DIVERSITY,0.2961832061068702,"in the diversity of their training data. The datasets are constructed from the play of three policies:
189"
STRATEGIC DIVERSITY,0.29770992366412213,"a random baseline policy, and two PSRO-generated policies. The PSRO policies were arbitrarily
190"
STRATEGIC DIVERSITY,0.29923664122137406,"sampled from an approximate solution produced by a run of PSRO. We sampled an additional
191"
STRATEGIC DIVERSITY,0.300763358778626,"policy from PSRO for evaluating the generalization capacity of the world models. These policies
192"
STRATEGIC DIVERSITY,0.30229007633587784,"are then subsampled and used to train seven world models. The world models are referred to by
193"
STRATEGIC DIVERSITY,0.30381679389312977,"icons
that depict the symmetric strategy profiles used to train them in the normal-form. Strategy
194"
STRATEGIC DIVERSITY,0.3053435114503817,"profiles included in the training data of the world models are shaded black. For instance, the first
195"
STRATEGIC DIVERSITY,0.3068702290076336,"(random) policy
, or the first and third policies
. Each world model’s dataset contains 1 million
196"
STRATEGIC DIVERSITY,0.3083969465648855,"total transitions, collected uniformly from each distinct strategy profile (symmetric profiles are not
197"
STRATEGIC DIVERSITY,0.3099236641221374,"re-sampled). The world models are then evaluated on accuracy and recall for their predictions of both
198"
STRATEGIC DIVERSITY,0.3114503816793893,"observation and reward for both players. The world models are optimized with a weighted-average
199"
STRATEGIC DIVERSITY,0.31297709923664124,"cross-entropy objective. Additional details are in Appendix C.2.
200"
STRATEGIC DIVERSITY,0.3145038167938931,Observation
STRATEGIC DIVERSITY,0.31603053435114503,"0.27±0.04
0.75±0.02
0.80±0.02
0.58±0.05
0.83±0.02
0.62±0.05
0.68±0.04"
STRATEGIC DIVERSITY,0.31755725190839695,Reward
STRATEGIC DIVERSITY,0.3190839694656489,"0.73±0.08
0.52±0.10
0.53±0.10
0.68±0.08
0.50±0.10
0.68±0.08
0.69±0.08 0.0 0.2 0.4 0.6 0.8 1.0"
STRATEGIC DIVERSITY,0.32061068702290074,Proﬁles Sampled To Train World Model
STRATEGIC DIVERSITY,0.32213740458015266,"Figure 2: World model accuracy across strategy profiles. Each heatmap portrays a world model’s
accuracy over 16 strategy profiles. The meta x-axis corresponds to the profiles used to train the world
model (as black cells). Above each heatmap is the model’s average accuracy."
STRATEGIC DIVERSITY,0.3236641221374046,"Results.
Figure 2 presents each world model’s per-profile accuracy, as well as its aver-
201"
STRATEGIC DIVERSITY,0.3251908396946565,"age over all profiles.
Inclusion of the random policy corresponds to decreases in observa-
202"
STRATEGIC DIVERSITY,0.3267175572519084,"tion prediction accuracy:
0.75 ± 0.02 →
0.58 ± 0.05,
0.80 ± 0.02 →
0.62 ± 0.05,
203"
STRATEGIC DIVERSITY,0.3282442748091603,"and
0.83 ± 0.02 →
0.68 ± 0.04.
Figure 13 (Appendix E.1) contains the world
204"
STRATEGIC DIVERSITY,0.3297709923664122,"model’s per-profile recall.
Inclusion of the random policy corresponds to increases in re-
205"
STRATEGIC DIVERSITY,0.33129770992366414,"ward 1 recall:
0.25 ± 0.07 →
0.37 ± 0.11,
0.25 ± 0.07 →
0.36 ± 0.11, and
206"
STRATEGIC DIVERSITY,0.332824427480916,"0.26 ± 0.07 →
0.37 ± 0.11.
207"
STRATEGIC DIVERSITY,0.33435114503816793,"Discussion.
The PSRO policies offer the most strategically salient view of the game’s dynamics.
208"
STRATEGIC DIVERSITY,0.33587786259541985,"Consequently, the world model
trained with these policies yields the highest observation accuracy.
209"
STRATEGIC DIVERSITY,0.3374045801526718,"However, this world model performs poorly on reward accuracy, scoring only 0.50 ± 0.10. In
210"
STRATEGIC DIVERSITY,0.33893129770992364,"comparison, the model trained on the random policy
scores 0.73 ± 0.08. This seemingly
211"
STRATEGIC DIVERSITY,0.34045801526717556,"counterintuitive result can be attributed to a significant class imbalance in rewards.
predicts only
212"
STRATEGIC DIVERSITY,0.3419847328244275,"the most common class, no reward, which gives the illusion of higher performance. In contrast, the
213"
STRATEGIC DIVERSITY,0.3435114503816794,"remaining world models attempt to predict rewarding states, which reduces their overall accuracy.
214"
STRATEGIC DIVERSITY,0.3450381679389313,"Therefore, we should compare the world models based on their ability to recall rewards. When we
215"
STRATEGIC DIVERSITY,0.3465648854961832,"examine
again, we find that it also struggles to recall rewards, scoring only 0.26±0.07. However,
216"
STRATEGIC DIVERSITY,0.3480916030534351,"when the random policy is included in the training data (
), the recall improves to 0.37 ± 0.11. This
217"
STRATEGIC DIVERSITY,0.34961832061068704,"improvement is also due to the same class imbalance. The PSRO policies are highly competitive,
218"
STRATEGIC DIVERSITY,0.3511450381679389,"tending to over-harvest. This limits the proportion of rewarding experiences. Including the random
219"
STRATEGIC DIVERSITY,0.3526717557251908,"policy enhances the diversity of rewards in this instance, as its coplayer can demonstrate successful
220"
STRATEGIC DIVERSITY,0.35419847328244275,"harvesting. Given the importance of accurately predicting both observations and rewards for effective
221"
STRATEGIC DIVERSITY,0.35572519083969467,"planning,
appears to be the most promising option. However, the strong performance of
222"
STRATEGIC DIVERSITY,0.35725190839694654,"suggests future work on algorithms that can benefit solely from observation predictions. Overall,
223"
STRATEGIC DIVERSITY,0.35877862595419846,"these results support the claim that strategic diversity enhances the training of world models.
224"
RESPONSE CALCULATIONS,0.3603053435114504,"3.2
Response Calculations
225"
RESPONSE CALCULATIONS,0.3618320610687023,"Empirical games are built by iteratively calculating and incorporating responses to the current
226"
RESPONSE CALCULATIONS,0.36335877862595417,"solution. However, direct computation of these responses is often infeasible, so RL or DRL is used
227"
RESPONSE CALCULATIONS,0.3648854961832061,"to approximate the response. This process of approximating a single response policy using RL is
228"
RESPONSE CALCULATIONS,0.366412213740458,"computationally intensive, posing a significant constraint in empirical game modeling when executed
229"
RESPONSE CALCULATIONS,0.36793893129770994,"repeatedly. World models present an opportunity to address this issue. A world model can serve as a
230"
RESPONSE CALCULATIONS,0.36946564885496186,"medium for transferring previously learned knowledge about the game’s dynamics. Therefore, the
231"
RESPONSE CALCULATIONS,0.3709923664122137,"dynamics need not be relearned, reducing the computational cost associated with response calculation.
232"
RESPONSE CALCULATIONS,0.37251908396946565,"Exercising a world model for transfer is achieved through a process called planning. Planning is
233"
RESPONSE CALCULATIONS,0.37404580152671757,"any procedure that takes a world model and produces or improves a policy. In the context of games,
234"
RESPONSE CALCULATIONS,0.3755725190839695,"planning can optionally take into account the existence of coplayers. This consideration can reduce
235"
RESPONSE CALCULATIONS,0.37709923664122136,"experiential variance caused by unobserved confounders (i.e., the coplayers). However, coplayer
236"
RESPONSE CALCULATIONS,0.3786259541984733,"modeling errors may introduce further errors in the planning procedure [21].
237"
RESPONSE CALCULATIONS,0.3801526717557252,"Planning alongside empirical-game construction allows us to side-step this issue as we have direct
238"
RESPONSE CALCULATIONS,0.3816793893129771,"access to the policies of all players during training. This allows us to circumvent the challenge
239"
RESPONSE CALCULATIONS,0.383206106870229,"of building accurate agent models. Instead, the policies of coplayers can be directly queried and
240"
RESPONSE CALCULATIONS,0.3847328244274809,"used alongside a world model, leading to more accurate planning. In this section, we empirically
241"
RESPONSE CALCULATIONS,0.38625954198473283,"demonstrate the effectiveness of two methods that decrease the cost of response calculation by
242"
RESPONSE CALCULATIONS,0.38778625954198476,"integrating planning with a world model and other agent policies.
243"
BACKGROUND PLANNING,0.3893129770992366,"3.2.1
Background Planning
244"
BACKGROUND PLANNING,0.39083969465648855,"The first type of planning that is investigated is background planning, popularized by the Dyna
245"
BACKGROUND PLANNING,0.39236641221374047,"architecture [67]. In background planning, agents interact with the world model to produce planned
246"
BACKGROUND PLANNING,0.3938931297709924,"experiences1. The planned experiences are then used by a model-free reinforcement learning
247"
BACKGROUND PLANNING,0.39541984732824426,"algorithm as if they were real experiences (experiences generated from the real game). Background
248"
BACKGROUND PLANNING,0.3969465648854962,"planning enables learners to generate experiences of states they are not currently in.
249"
BACKGROUND PLANNING,0.3984732824427481,"Experiment.
To assess whether planned experiences are effective for training a policy in the actual
250"
BACKGROUND PLANNING,0.4,"game, we compute two response policies. The first response policy, serving as our baseline, learns
251"
BACKGROUND PLANNING,0.4015267175572519,"exclusively from real experiences. The second response policy, referred to as the planner, is trained
252"
BACKGROUND PLANNING,0.4030534351145038,"using a two-step procedure. Initially, the planner is exclusively trained on planned experiences. After
253"
BACKGROUND PLANNING,0.40458015267175573,"10 000 updates, it then transitions to learning solely from real experiences. Policies are trained using
254"
BACKGROUND PLANNING,0.40610687022900765,"IMPALA [14], with further details available in Appendix C.1. The planner employs the
world
255"
BACKGROUND PLANNING,0.4076335877862595,"model from Section 3.1, and the opponent plays the previously held-out policy. In this and subsequent
256"
BACKGROUND PLANNING,0.40916030534351144,"experiments, the cost of methods is measured by the number of experiences they require with the
257"
BACKGROUND PLANNING,0.41068702290076337,"actual game. This is because, experience collection is often the bottleneck when applying RL-based
258"
BACKGROUND PLANNING,0.4122137404580153,"methods [48, 24]. Throughout the remainder of this work, each experience represents a trajectory of
259"
BACKGROUND PLANNING,0.41374045801526715,"20 transitions, facilitating the training of recurrent policies.
260"
BACKGROUND PLANNING,0.4152671755725191,"0.0
0.5
1.0
Real Exp.
×106 0 10 20 30 40"
BACKGROUND PLANNING,0.416793893129771,Return
BACKGROUND PLANNING,0.4183206106870229,"−1.0
−0.5
0.0
0.5
1.0
Plan Exp.
Real Exp.
×106"
BACKGROUND PLANNING,0.4198473282442748,"Baseline
Plan: Real
Plan: Model"
BACKGROUND PLANNING,0.4213740458015267,"Figure 3: Effects of background planning on response learning. Left: Return curves measured by the
number of real experiences used. Right: Return curves measured by usage of both real and planned
experiences. The planner’s return is measured against the real game and the world model. (5 seeds,
with 95 % bootstrapped CI)."
BACKGROUND PLANNING,0.42290076335877863,"Results.
Figure 3 presents the results of the background planning experiment. The methods are
261"
BACKGROUND PLANNING,0.42442748091603055,"compared based on their final return, utilizing an equivalent amount of real experiences. The baseline
262"
BACKGROUND PLANNING,0.4259541984732824,"yields a return of 23.00 ± 4.01, whereas the planner yields a return of 31.17 ± 0.25.
263"
BACKGROUND PLANNING,0.42748091603053434,"Discussion.
In this experiment, the planner converges to a stronger policy, and makes earlier gains
264"
BACKGROUND PLANNING,0.42900763358778626,"in performance than the baseline. Despite this, there is a significant gap in the planner’s learning
265"
BACKGROUND PLANNING,0.4305343511450382,"1Other names include “imaginary”, “simulated”, or “hallucinated” experiences."
BACKGROUND PLANNING,0.43206106870229005,"curves, which are reported with respect to both the world model and real game. This gap arises due
266"
BACKGROUND PLANNING,0.433587786259542,"to accumulated model-prediction errors, causing the trajectories to deviate from the true state space.
267"
BACKGROUND PLANNING,0.4351145038167939,"Nevertheless, the planner effectively learns to interact with the world model during planning, and
268"
BACKGROUND PLANNING,0.4366412213740458,"this behavior shows positive transfer into the real game, as evidenced by the planner’s rapid learning.
269"
BACKGROUND PLANNING,0.4381679389312977,"The exact magnitude of benefit will vary across coplayers’ policies, games, and world models. In
270"
BACKGROUND PLANNING,0.4396946564885496,"Figure 14 (Appendix E.2), we repeat the same experiment with the poorly performing
world
271"
BACKGROUND PLANNING,0.44122137404580153,"model, and observe a marginal benefit (26.05±1.32). The key take-away is that background planning
272"
BACKGROUND PLANNING,0.44274809160305345,"tends to lead towards learning benefits, and not generally hamper learning.
273"
DECISION-TIME PLANNING,0.4442748091603053,"3.2.2
Decision-Time Planning
274"
DECISION-TIME PLANNING,0.44580152671755724,"The second main way that a world model is used is to inform action selection at decision time
275"
DECISION-TIME PLANNING,0.44732824427480916,"[planning] (DT). In this case, the agent evaluates the quality of actions by comparing the value of
276"
DECISION-TIME PLANNING,0.4488549618320611,"the model’s predicted successor state for all candidate actions. Action evaluation can also occur
277"
DECISION-TIME PLANNING,0.45038167938931295,"recursively, allowing the agent to consider successor states further into the future. Overall, this
278"
DECISION-TIME PLANNING,0.45190839694656487,"process should enable the learner to select better actions earlier in training, thereby reducing the
279"
DECISION-TIME PLANNING,0.4534351145038168,"amount of experiences needed to compute a response. A potential flaw with decision-time planning
280"
DECISION-TIME PLANNING,0.4549618320610687,"is that the agent’s learned value function may not be well-defined on model-predicted successor
281"
DECISION-TIME PLANNING,0.45648854961832064,"states [71]. To remedy this issue, the value function should also be trained on model-predicted states.
282"
DECISION-TIME PLANNING,0.4580152671755725,"Experiment.
To evaluate the impact the decision-time planning, we perform an experiment similar
283"
DECISION-TIME PLANNING,0.4595419847328244,"to the background planning experiment (Section 3.2.1). However, in this experiment, we evaluate
284"
DECISION-TIME PLANNING,0.46106870229007635,"the quality of four types of decision-time planners that perform one-step three-action search. The
285"
DECISION-TIME PLANNING,0.46259541984732827,"planners differ in the their ablations of background planning types: (1) warm-start background
286"
DECISION-TIME PLANNING,0.46412213740458014,"planning (BG: W) learning from planned experiences before any real experiences, and (2) concurrent
287"
DECISION-TIME PLANNING,0.46564885496183206,"background planning (BG: C) where after BG: W, learning proceeds simultaneously on both planned
288"
DECISION-TIME PLANNING,0.467175572519084,"and real experiences. The intuition behind BG: C is that the agent can complement its learning
289"
DECISION-TIME PLANNING,0.4687022900763359,"process by incorporating planned experiences that align with its current behavior, offsetting the
290"
DECISION-TIME PLANNING,0.47022900763358777,"reliance on costly real experiences. Extended experimental details are provided in Appendix C.
291"
DECISION-TIME PLANNING,0.4717557251908397,"0.0
0.5
1.0
Real Exp. ×106 0 10 20 30 40 50"
DECISION-TIME PLANNING,0.4732824427480916,Return
DECISION-TIME PLANNING,0.47480916030534354,"0.0
0.5
1.0
1.5
2.0
2.5
Real & Planned Exp.
×106 DT"
DECISION-TIME PLANNING,0.4763358778625954,"✓
✓
✓
✓ BG: W ✓ ✓ BG: C ✓
✓"
DECISION-TIME PLANNING,0.4778625954198473,"Figure 4: Effects of decision-time planning on response learning. Four planners using decision-
time planning (DT) are shown in combinations with warm-start background planning (BG: W) and
concurrent background planning (BG: C). (5 seeds, with 95 % bootstrapped CI)."
DECISION-TIME PLANNING,0.47938931297709925,"Results.
The results for this experiment are shown in Figure 4. The baseline policy receives a final
292"
DECISION-TIME PLANNING,0.48091603053435117,"return of 23.00 ± 4.01. The planners that do not include BG: W, perform worse, with final returns of
293"
DECISION-TIME PLANNING,0.48244274809160304,"9.98 ± 7.60 (DT) and 12.42 ± 3.97 (DT & BG: C). The planners that perform BG: W outperform the
294"
DECISION-TIME PLANNING,0.48396946564885496,"baseline, with final returns of 44.11 ± 2.81 (DT & BG: W) and 44.31 ± 2.56 (DT, BG: W, & BG: C).
295"
DECISION-TIME PLANNING,0.4854961832061069,"Discussion.
Our results suggest that the addition of BG: W provides sizable benefits: 9.98 ± 7.60
296"
DECISION-TIME PLANNING,0.4870229007633588,"(DT) →44.11 ± 2.81 (DT & BG:W) and 12.42 ± 3.97 (DT & BG: C) →44.31 ± 2.56 (DT, BG: W,
297"
DECISION-TIME PLANNING,0.48854961832061067,"& BG: C). We postulate that this is because it informs the policy’s value function on model-predictive
298"
DECISION-TIME PLANNING,0.4900763358778626,"states early into training. This allows that the learner is able to more effectively search earlier into
299"
DECISION-TIME PLANNING,0.4916030534351145,"training. BG: C appears to offer minor stability and variance improvements throughout the training
300"
DECISION-TIME PLANNING,0.49312977099236643,"procedure; however, it does not have a measurable difference in final performance. This result
301"
DECISION-TIME PLANNING,0.4946564885496183,"suggests using planning methods in combination to reap their respective advantages.
302"
DECISION-TIME PLANNING,0.4961832061068702,"However, we caution against focusing on the magnitude of improvement found within this experiment.
303"
DECISION-TIME PLANNING,0.49770992366412214,"As the margin of benefit depends on many factors including the world model accuracy, the opponent
304"
DECISION-TIME PLANNING,0.49923664122137407,"policy, and the game. To exemplify, similar to the background planning section, we repeat the same
305"
DECISION-TIME PLANNING,0.500763358778626,"experiment with the poorly performing
world model. The results of this ancillary experiment are
306"
DECISION-TIME PLANNING,0.5022900763358779,"in Figure 15 (Appendix E.3). The trend of BG: W providing benefits was reinforced: 6.29 ± 5.12
307"
DECISION-TIME PLANNING,0.5038167938931297,"(DT) →20.98 ± 9.76 (DT & BG: W) and 3.64 ± 0.26 (DT & BG: C) →33.07 ± 7.67 (DT, BG: W,
308"
DECISION-TIME PLANNING,0.5053435114503817,"& BG: C). However, the addition of BG: C now measurably improved performance 20.98 ± 9.76
309"
DECISION-TIME PLANNING,0.5068702290076336,"(DT & BG: W) →33.07 ± 7.67 (DT, BG: W, & BG: C). The main outcome of these experiments
310"
DECISION-TIME PLANNING,0.5083969465648855,"is the observation that multi-faceted planning is unlikely to harm a response calculation, and has a
311"
DECISION-TIME PLANNING,0.5099236641221374,"potentially large benefit when applied effectively. These results support the claim that world models
312"
DECISION-TIME PLANNING,0.5114503816793893,"offer the potential to improve response calculation through decision-time planning.
313"
DYNA-PSRO,0.5129770992366413,"4
Dyna-PSRO
314"
DYNA-PSRO,0.5145038167938931,"In this section we introduce Dyna-PSRO, Dyna-Policy-Space Response Oracles, an approximate
315"
DYNA-PSRO,0.516030534351145,"game-solving algorithm that builds on the PSRO [38] framework. Dyna-PSRO employs co-learning
316"
DYNA-PSRO,0.517557251908397,"to combine the benefits of world models and empirical games.
317"
DYNA-PSRO,0.5190839694656488,"Dyna-PSRO is defined by two significant alterations to the original PSRO algorithm. First, it trains
318"
DYNA-PSRO,0.5206106870229008,"a world model in parallel with all the typical PSRO routines (i.e., game reasoning and response
319"
DYNA-PSRO,0.5221374045801527,"calculation). We collect training data for the world model from both the episodes used to estimate the
320"
DYNA-PSRO,0.5236641221374045,"empirical game’s payoffs, and the episodes that are generated during response learning and evaluation.
321"
DYNA-PSRO,0.5251908396946565,"This approach ensures that the world model is informed by a diversity of data from a salient set of
322"
DYNA-PSRO,0.5267175572519084,"strategy profiles. By reusing data from empirical game development, training the world model incurs
323"
DYNA-PSRO,0.5282442748091603,"no additional cost for data collection.
324"
DYNA-PSRO,0.5297709923664122,"The second modification introduced by Dyna-PSRO pertains to the way response policies are learned.
325"
DYNA-PSRO,0.5312977099236641,"Dyna-PSRO adopts a Dyna-based reinforcement learner [67, 68, 70] that integrates simultaneous plan-
326"
DYNA-PSRO,0.5328244274809161,"ning, learning, and acting. Consequently, the learner concurrently processes experiences generated
327"
DYNA-PSRO,0.5343511450381679,"from decision-time planning, background planning, and direct game interaction. These experiences,
328"
DYNA-PSRO,0.5358778625954198,"regardless of their origin, are then learned from using the IMPALA [14] update rule. For all accounts
329"
DYNA-PSRO,0.5374045801526718,"of planning, the learner uses the single world model that is trained within Dyna-PSRO. This allows
330"
DYNA-PSRO,0.5389312977099237,"game knowledge accrued from previous response calculations to be transferred and used to reduce
331"
DYNA-PSRO,0.5404580152671755,"the cost of the current and future response calculations. Pseudocode and additional details for both
332"
DYNA-PSRO,0.5419847328244275,"PSRO and Dyna-PSRO are provided in Appendix C.4.
333"
DYNA-PSRO,0.5435114503816794,"Games.
Dyna-PSRO is evaluated on three games. The first is the harvest commons game used in the
334"
DYNA-PSRO,0.5450381679389313,"experiments described above, denoted “Harvest: Categorical”. The other two games come from the
335"
DYNA-PSRO,0.5465648854961832,"MeltingPot [39] evaluation suite and feature rich image-based observations. “Harvest: RGB” is their
336"
DYNA-PSRO,0.5480916030534351,"version of the same commons harvest game (details in Appendix D.2). “Running With Scissors” is a
337"
DYNA-PSRO,0.549618320610687,"temporally extended version of rock-paper-scissors (details in Appendix D.3). World model training
338"
DYNA-PSRO,0.5511450381679389,"and implementation details for each game are in Appendix C.2, likewise, policies in Appendix C.1.
339"
DYNA-PSRO,0.5526717557251909,"Experiment.
Dyna-PSRO’s performance is measured by the quality of the solution it produces
340"
DYNA-PSRO,0.5541984732824428,"when compared against the world-model-free baseline PSRO. The two methods are evaluated on
341"
DYNA-PSRO,0.5557251908396946,"SumRegret (sometimes called Nash convergence), which measures the regret across all players
342"
DYNA-PSRO,0.5572519083969466,"SumRegret(σ, Π) = P
i∈n maxπi∈Πi ˆUi(πi, σ−i) −ˆUi(σi, σ−i), where σ is the method’s solution
343"
DYNA-PSRO,0.5587786259541985,"and Π ⊆Π denotes the deviation set. We define deviation sets based on policies generated across
344"
DYNA-PSRO,0.5603053435114503,"methods (i.e., regret is with respect to the combined game): Π ≡S"
DYNA-PSRO,0.5618320610687023,"method ˆΠmethod, for all methods for
345"
DYNA-PSRO,0.5633587786259542,"a particular seed (detailed in Appendix C.5) [2]. We measure SumRegret for intermediate solutions,
346"
DYNA-PSRO,0.5648854961832062,"and report it as a function of the cumulative number of real experiences employed in the respective
347"
DYNA-PSRO,0.566412213740458,"methods.
348"
DYNA-PSRO,0.5679389312977099,"Results.
Figure 5 presents the results for this experiment. For Harvest: Categorical, Dyna-PSRO
349"
DYNA-PSRO,0.5694656488549619,"found a no regret solution within the combined-game in 3.2e6 experiences. Whereas, PSRO achieves
350"
DYNA-PSRO,0.5709923664122137,"a solution of at best 5.45 ± 1.62 within 2e7 experiences. In Harvest: RGB, Dyna-PSRO reaches a
351"
DYNA-PSRO,0.5725190839694656,"0
1
2
Real Exp.
×107 0 100 101 102"
DYNA-PSRO,0.5740458015267176,SumRegret
DYNA-PSRO,0.5755725190839694,Harvest: Categorical
DYNA-PSRO,0.5770992366412214,"0
1
2
Real Exp.
×107 0 100 101"
DYNA-PSRO,0.5786259541984733,Harvest: RGB
DYNA-PSRO,0.5801526717557252,"0
1
2
Real Exp.
×107 0 0.02"
DYNA-PSRO,0.5816793893129771,Running With Scissors
DYNA-PSRO,0.583206106870229,"PSRO
Dyna-PSRO"
DYNA-PSRO,0.5847328244274809,"Figure 5: PSRO compared against Dyna-PSRO. (5 seeds, with 95 % bootstrapped CI)."
DYNA-PSRO,0.5862595419847328,"solution with 0.89 ± 0.74 regret at 5.12e6 experiences. At the same time, PSRO had found a solution
352"
DYNA-PSRO,0.5877862595419847,"with 6.42 ± 4.73 regret, and at the end of its run had 2.50 ± 2.24 regret. In the final game, RWS,
353"
DYNA-PSRO,0.5893129770992367,"Dyna-PSRO has 2e−3±5e−4 regret at 1.06e7 experiences, and at a similar point (9.6e6 experiences),
354"
DYNA-PSRO,0.5908396946564886,"PSRO has 6.68e−3 ± 2.51e−3. At the end of the run, PSRO achieves a regret 3.50e−3 ± 7.36e−4.
355"
DYNA-PSRO,0.5923664122137404,"Discussion.
The results indicate that across all games, Dyna-PSRO consistently outperforms PSRO
356"
DYNA-PSRO,0.5938931297709924,"by achieving a superior solution. Furthermore, this improved performance is realized while consuming
357"
DYNA-PSRO,0.5954198473282443,"fewer real-game experiences. For instance, in the case of Harvest: Categorical, the application of
358"
DYNA-PSRO,0.5969465648854961,"the world model for decision-time planning enables the computation of an effective policy after only
359"
DYNA-PSRO,0.5984732824427481,"a few iterations. On the other hand, we observe a trend of accruing marginal gains in other games,
360"
DYNA-PSRO,0.6,"suggesting that the benefits are likely attributed to the transfer of knowledge about the game dynamics.
361"
DYNA-PSRO,0.601526717557252,"In Harvest: Categorical and Running With Scissors, Dyna-PSRO also had lower variance than PSRO.
362"
LIMITATIONS,0.6030534351145038,"5
Limitations
363"
LIMITATIONS,0.6045801526717557,"Although our experiments demonstrate benefits for co-learning world models and empirical games,
364"
LIMITATIONS,0.6061068702290077,"there are several areas for potential improvement. The world models used in this study necessitated
365"
LIMITATIONS,0.6076335877862595,"observational data from all players for training, and assumed a simultaneous-action game. Future
366"
LIMITATIONS,0.6091603053435114,"research could consider relaxing these assumptions to accommodate different interaction protocols,
367"
LIMITATIONS,0.6106870229007634,"a larger number of players, and incomplete data perspectives. Furthermore, our world models
368"
LIMITATIONS,0.6122137404580152,"functioned directly on agent observations, which made them computationally costly to query. If
369"
LIMITATIONS,0.6137404580152672,"the generation of experiences is the major limiting factor, as assumed in this study, this approach is
370"
LIMITATIONS,0.6152671755725191,"acceptable. Nevertheless, reducing computational demands through methods like latent world models
371"
LIMITATIONS,0.616793893129771,"presents a promising avenue for future research. Lastly, the evaluation of solution concepts could
372"
LIMITATIONS,0.6183206106870229,"also be improved. While combined-game regret employs all available estimates in approximating
373"
LIMITATIONS,0.6198473282442748,"regret, its inherent inaccuracies may lead to misinterpretations of relative performance.
374"
CONCLUSION,0.6213740458015267,"6
Conclusion
375"
CONCLUSION,0.6229007633587786,"This study showed the mutual benefit of co-learning a world model and empirical game. First, we
376"
CONCLUSION,0.6244274809160305,"demonstrated that empirical games provide strategically diverse training data that could inform a more
377"
CONCLUSION,0.6259541984732825,"robust world model. We then showed that world models can reduce the computational cost, measured
378"
CONCLUSION,0.6274809160305344,"in experiences, of response calculations through planning. These two benefits were combined and
379"
CONCLUSION,0.6290076335877862,"realized in a new algorithm, Dyna-PSRO. In our experiments, Dyna-PSRO computed lower-regret
380"
CONCLUSION,0.6305343511450382,"solutions than PSRO on several partially observable general-sum games. Dyna-PSRO also required
381"
CONCLUSION,0.6320610687022901,"substantially fewer experiences than PSRO, a key algorithmic advantage for settings where collecting
382"
CONCLUSION,0.6335877862595419,"experiences is a cost-limiting factor.
383"
REFERENCES,0.6351145038167939,"References
384"
REFERENCES,0.6366412213740458,"[1] Stefano V Albrecht and Peter Stone. Autonomous agents modelling other agents: A compre-
385"
REFERENCES,0.6381679389312978,"hensive survey and open problems. Artificial Intelligence, 258:66–95, 2018.
386"
REFERENCES,0.6396946564885496,"[2] David Balduzzi, Karl Tuyls, Julien Pérolat, and Thore Graepel. Re-evaluating evaluation. In
387"
REFERENCES,0.6412213740458015,"32nd Conference on Neural Information Processing Systems, 2018.
388"
REFERENCES,0.6427480916030535,"[3] Philip Ball, Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, and Stephen Roberts.
389"
REFERENCES,0.6442748091603053,"Ready policy one: World building through active learning. In 37th International Conference of
390"
REFERENCES,0.6458015267175573,"Machine Learning, 2020.
391"
REFERENCES,0.6473282442748092,"[4] Nolan Bard, Michael Johanson, Neil Burch, and Michael Bowling. Online implicit agent
392"
REFERENCES,0.648854961832061,"modelling. In 12th International Conference on Autonomous Agents and Multiagent Systems,
393"
REFERENCES,0.650381679389313,"2013.
394"
REFERENCES,0.6519083969465649,"[5] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for
395"
REFERENCES,0.6534351145038167,"sequence prediction with recurrent neural networks. In 28th Conference on Neural Information
396"
REFERENCES,0.6549618320610687,"Processing Systems, pages 1171–1179, 2015.
397"
REFERENCES,0.6564885496183206,"[6] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
398"
REFERENCES,0.6580152671755726,"Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
399"
REFERENCES,0.6595419847328244,"Zhang. JAX: composable transformations of Python+NumPy programs, 2018.
400"
REFERENCES,0.6610687022900763,"[7] George W Brown. Iterative solution of games by fictitious play. In Activity analysis of production
401"
REFERENCES,0.6625954198473283,"and allocation, volume 13, pages 374–376, 1951.
402"
REFERENCES,0.6641221374045801,"[8] Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-
403"
REFERENCES,0.665648854961832,"efficient reinforcement learning with stochastic ensemble value expansion. In 22nd Conference
404"
REFERENCES,0.667175572519084,"on Neural Information Processing Systems, 2018.
405"
REFERENCES,0.6687022900763359,"[9] Albin Cassirer, Gabriel Barth-Maron, Eugene Brevdo, Sabela Ramos, Toby Boyd, Thibault
406"
REFERENCES,0.6702290076335878,"Sottiaux, and Manuel Kroiss. Reverb: A framework for experience replay, 2021.
407"
REFERENCES,0.6717557251908397,"[10] Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environ-
408"
REFERENCES,0.6732824427480916,"ment simulators. In 5th International Conference on Learning Representations, 2017.
409"
REFERENCES,0.6748091603053435,"[11] Valliappa Chockingam, Tegg Taekyong Sung, Feryal Behbanai, Rishab Gargeya, Amlesh
410"
REFERENCES,0.6763358778625954,"Sivanantham, and Aleksandra Malysheva. Extending world models for multi-agent reinforce-
411"
REFERENCES,0.6778625954198473,"ment learning in malmö. In Joint AIIDE 2018 Workshops co-located with the 14th AAAI
412"
REFERENCES,0.6793893129770993,"conference on artificial intelligence and interactive digital entertainment, 2018.
413"
REFERENCES,0.6809160305343511,"[12] Brian Collins. Combining opponent modeling and model-based reinforcement learning in a
414"
REFERENCES,0.6824427480916031,"two-player competitive game. Master’s thesis, University of Edinburgh, 2007.
415"
REFERENCES,0.683969465648855,"[13] B. Curtis Eaves. The linear complementarity problem. Management Science, 17(9):612–634,
416"
REFERENCES,0.6854961832061068,"1971.
417"
REFERENCES,0.6870229007633588,"[14] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward,
418"
REFERENCES,0.6885496183206107,"Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu.
419"
REFERENCES,0.6900763358778625,"IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures.
420"
REFERENCES,0.6916030534351145,"In 35th International Conference on Machine Learning, 2018.
421"
REFERENCES,0.6931297709923664,"[15] Jakob N Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and
422"
REFERENCES,0.6946564885496184,"Igor Mordatch. Learning with opponent-learning awareness. In 17th International Conference
423"
REFERENCES,0.6961832061068702,"on Autonomous Agents and MultiAgent Systems, 2018.
424"
REFERENCES,0.6977099236641221,"[16] Kunihiko Fukushima. Cognitron: A self-organizing multilayered neural network. Biological
425"
REFERENCES,0.6992366412213741,"Cybernetics, 20:121–136, 1975.
426"
REFERENCES,0.7007633587786259,"[17] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Bellemare. Deep-
427"
REFERENCES,0.7022900763358778,"MDP: Learning continuous latent space models for representation learning. In 36th International
428"
REFERENCES,0.7038167938931298,"Conference on Machine Learning, volume 97, pages 2170–2179, 2019.
429"
REFERENCES,0.7053435114503817,"[18] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In 31st
430"
REFERENCES,0.7068702290076336,"Conference on Neural Information Processing Systems, 2018.
431"
REFERENCES,0.7083969465648855,"[19] David Ha and Jürgen Schmidhuber. World models. In arXiv preprint arXiv:1803.10122, 2018.
432"
REFERENCES,0.7099236641221374,"[20] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with
433"
REFERENCES,0.7114503816793893,"discrete world models. In 9th International Conference on Learning Representations, 2021.
434"
REFERENCES,0.7129770992366412,"[21] He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daumé III. Opponent modeling in deep
435"
REFERENCES,0.7145038167938931,"reinforcement learning. In 33rd International Conference on Machine Learning, 2016.
436"
REFERENCES,0.716030534351145,"[22] Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX,
437"
REFERENCES,0.7175572519083969,"2020.
438"
REFERENCES,0.7190839694656489,"[23] Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote.
A
439"
REFERENCES,0.7206106870229008,"survey of learning in multiagent environments: Dealing with non-stationarity. arXiv preprint
440"
REFERENCES,0.7221374045801526,"arXiv:1707.09183, 2017.
441"
REFERENCES,0.7236641221374046,"[24] Todd Hester and Peter Stone. Texplore: Real-time sample-efficient reinforcement learning for
442"
REFERENCES,0.7251908396946565,"robots. In Machine Learning for Robotics (MLR), 2012.
443"
REFERENCES,0.7267175572519083,"[25] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation,
444"
REFERENCES,0.7282442748091603,"9(8):1735–1780, 1997.
445"
REFERENCES,0.7297709923664122,"[26] Matthew W. Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev,
446"
REFERENCES,0.7312977099236642,"Danila Sinopalnikov, Piotr Sta´nczyk, Sabela Ramos, Anton Raichuk, Damien Vincent, Léonard
447"
REFERENCES,0.732824427480916,"Hussenot, Robert Dadashi, Gabriel Dulac-Arnold, Manu Orsini, Alexis Jacq, Johan Ferret, Nino
448"
REFERENCES,0.7343511450381679,"Vieillard, Seyed Kamyar Seyed Ghasemipour, Sertan Girgin, Olivier Pietquin, Feryal Behbahani,
449"
REFERENCES,0.7358778625954199,"Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson,
450"
REFERENCES,0.7374045801526717,"Abe Friesen, Ruba Haroun, Alex Novikov, Sergio Gómez Colmenarejo, Serkan Cabi, Caglar
451"
REFERENCES,0.7389312977099237,"Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Andrew Cowie, Ziyu Wang, Bilal Piot, and
452"
REFERENCES,0.7404580152671756,"Nando de Freitas. Acme: A research framework for distributed reinforcement learning. arXiv
453"
REFERENCES,0.7419847328244275,"preprint arXiv:2006.00979, 2020.
454"
REFERENCES,0.7435114503816794,"[27] G. Zacharias Holland, Erin Talvitie, and Michael Bowling. The effect of planning shape on
455"
REFERENCES,0.7450381679389313,"dyna-style planning in high-dimensional state spaces. In FAIM workshop “Prediction and
456"
REFERENCES,0.7465648854961832,"Generative Modeling in Reinforcement Learning”, 2018.
457"
REFERENCES,0.7480916030534351,"[28] HumanCompatibleAI. https://github.com/HumanCompatibleAI/multi-agent, 2019.
458"
REFERENCES,0.749618320610687,"[29] Pararawendy Indarjo. Deep state-space models in multi-agent systems. Master’s thesis, Leiden
459"
REFERENCES,0.751145038167939,"University, 2019.
460"
REFERENCES,0.7526717557251908,"[30] Marco A. Janssen, Robert Holahan, Allen Lee, and Elinor Ostrom. Lab experiments for the
461"
REFERENCES,0.7541984732824427,"study of social-ecological systems. Science, 328(5978):613–617, 2010.
462"
REFERENCES,0.7557251908396947,"[31] Patrick R. Jordan, L. Julian Schvartzman, and Michael P. Wellman. Strategy exploration in
463"
REFERENCES,0.7572519083969466,"empirical games. In 9th International Conference on Autonomous Agents and Multi-Agent
464"
REFERENCES,0.7587786259541984,"Systems, pages 1131–1138, 2010.
465"
REFERENCES,0.7603053435114504,"[32] Gabriel Kalweit and Joschka Boedecker. Uncertainty-driven imagination for continuous deep
466"
REFERENCES,0.7618320610687023,"reinforcement learning. In 1st Conference on Robot Learning, pages 195–206, 2017.
467"
REFERENCES,0.7633587786259542,"[33] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd
468"
REFERENCES,0.7648854961832061,"International Conference for Learning Representations, 2015.
469"
REFERENCES,0.766412213740458,"[34] Christine Konicki, Mithun Chakraborty, and Michael P. Wellman. Exploiting extensive-form
470"
REFERENCES,0.76793893129771,"structure in empirical game-theoretic analysis. In Web and Internet Economics: 18th Interna-
471"
REFERENCES,0.7694656488549618,"tional Conference, 2022.
472"
REFERENCES,0.7709923664122137,"[35] Orr Krupnik, Igor Mordatch, and Aviv Tamar. Multi-agent reinforcement learning with multi-
473"
REFERENCES,0.7725190839694657,"step generative models. In 4th Conference on Robot Learning, pages 776–790, 2020.
474"
REFERENCES,0.7740458015267175,"[36] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
475"
REFERENCES,0.7755725190839695,"trust-region policy optimization. In 6th International Conference on Learning Representations,
476"
REFERENCES,0.7770992366412214,"2018.
477"
REFERENCES,0.7786259541984732,"[37] Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay,
478"
REFERENCES,0.7801526717557252,"Julien Pérolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, Daniel
479"
REFERENCES,0.7816793893129771,"Hennes, Dustin Morrill, Paul Muller, Timo Ewalds, Ryan Faulkner, János Kramár, Bart De
480"
REFERENCES,0.783206106870229,"Vylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud, Matthew Lai,
481"
REFERENCES,0.7847328244274809,"Julian Schrittwieser, Thomas Anthony, Edward Hughes, Ivo Danihelka, and Jonah Ryan-Davis.
482"
REFERENCES,0.7862595419847328,"OpenSpiel: A framework for reinforcement learning in games. CoRR, abs/1908.09453, 2019.
483"
REFERENCES,0.7877862595419848,"[38] Marc Lanctot, Vinicius Zambaldi, Audr¯unas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien
484"
REFERENCES,0.7893129770992366,"Pérolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent
485"
REFERENCES,0.7908396946564885,"reinforcement learning. In 31st Conference on Neural Information Processing Systems, page
486"
REFERENCES,0.7923664122137405,"4193–4206, 2017.
487"
REFERENCES,0.7938931297709924,"[39] Joel Z. Leibo, Edgar Duéñez-Guzmán, Alexander Sasha Vezhnevets, John P. Agapiou, Peter
488"
REFERENCES,0.7954198473282442,"Sunehag, Raphael Koster, Jayd Matyas, Charles Beattie, Igor Mordatch, and Thore Graepel.
489"
REFERENCES,0.7969465648854962,"Scalable evaluation of multi-agent reinforcement learning with melting pot. PMLR, 2021.
490"
REFERENCES,0.7984732824427481,"[40] Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multi-
491"
REFERENCES,0.8,"agent reinforcement learning in sequential social dilemmas. In 16th International Conference
492"
REFERENCES,0.8015267175572519,"on Autonomous Agents and Multiagent Systems, 2017.
493"
REFERENCES,0.8030534351145038,"[41] Zun Li, Marc Lanctot, Kevin McKee, Luke Marris, Ian Gemp, Daniel Hennes, Paul Muller,
494"
REFERENCES,0.8045801526717558,"Kate Larson, Yoram Bachrach, and Michael P. Wellman. Search-improved game-theoretic
495"
REFERENCES,0.8061068702290076,"multiagent reinforcement learning in general and negotiation games (extended abstract). In
496"
REFERENCES,0.8076335877862595,"32nd International Conference on Autonomous Agents and Multiagent Systems, AAMAS, 2023.
497"
REFERENCES,0.8091603053435115,"[42] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In
498"
REFERENCES,0.8106870229007633,"11th International Conference on Machine Learning, pages 157–163, 1994.
499"
REFERENCES,0.8122137404580153,"[43] Stephen McAleer, John Lanier, Kevin Wang, Pierre Baldi, and Roy Fox. XDO: A double oracle
500"
REFERENCES,0.8137404580152672,"algorithm for extensive-form games. In 35th Conference on Neural Information Processing
501"
REFERENCES,0.815267175572519,"Systems, 2021.
502"
REFERENCES,0.816793893129771,"[44] Richard D. McKelvey, Andrew M. McLennan, and Theodore L. Turocy. Gambit: Software
503"
REFERENCES,0.8183206106870229,"tools for game theory. http://www.gambit-project.org/, 2016.
504"
REFERENCES,0.8198473282442749,"[45] H. Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. Planning in the presence of cost
505"
REFERENCES,0.8213740458015267,"functions controlled by an adversary. In 20th International Conference on Machine Learning,
506"
REFERENCES,0.8229007633587786,"pages 536–543, 2003.
507"
REFERENCES,0.8244274809160306,"[46] Richard Mealing and Jonathan L Shapiro. Opponent modeling by expectation–maximization and
508"
REFERENCES,0.8259541984732824,"sequence prediction in simplified poker. In IEEE Transactions on Computational Intelligence
509"
REFERENCES,0.8274809160305343,"and AI in Games, volume 9, pages 11–24, 2015.
510"
REFERENCES,0.8290076335877863,"[47] Vicent Michalski, Roland Memisevic, and Kishore Konda. Modeling deep temporal depen-
511"
REFERENCES,0.8305343511450382,"dencies with recurrent grammar cells. In 27th Conference on Neural Information Processing
512"
REFERENCES,0.8320610687022901,"Systems, 2014.
513"
REFERENCES,0.833587786259542,"[48] Johan S. Obando-Ceron and Pablo Samuel Castro. Revisiting rainbow: Promoting more
514"
REFERENCES,0.8351145038167939,"insightful and inclusive deep reinforcement learning research. In 38th International Conference
515"
REFERENCES,0.8366412213740458,"on Machine Learning, 2021.
516"
REFERENCES,0.8381679389312977,"[49] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh.
Action-
517"
REFERENCES,0.8396946564885496,"conditional video prediction using deep networks in atari games. In 28th Conference on
518"
REFERENCES,0.8412213740458016,"Neural Information Processing Systems, 2015.
519"
REFERENCES,0.8427480916030534,"[50] Junhyuk Oh, Satinderg Singh, and Honglak Lee. Value prediction network. In 30th Conference
520"
REFERENCES,0.8442748091603054,"on Neural Information Processing Systems, pages 6118–6128, 2017.
521"
REFERENCES,0.8458015267175573,"[51] S. Phelps, M. Marcinkiewicz, and S. Parsons. A novel method for automatic strategy acquisition
522"
REFERENCES,0.8473282442748091,"in N-player non-zero-sum games. In Fifth International Joint Conference on Autonomous
523"
REFERENCES,0.8488549618320611,"Agents and Multiagent Systems, page 705–712, 2006.
524"
REFERENCES,0.850381679389313,"[52] Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.
525"
REFERENCES,0.8519083969465648,"John Wiley & Sons, Inc., 1994.
526"
REFERENCES,0.8534351145038168,"[53] Julien Pérolat, Joel Z. Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, and Thore Graepel.
527"
REFERENCES,0.8549618320610687,"A multi-agent reinforcement learning model of common-pool resource appropriation. In 31st
528"
REFERENCES,0.8564885496183207,"Conference on Neural Information Processing Systems, 2017.
529"
REFERENCES,0.8580152671755725,"[54] Stéphane Ross and J. Andrew Bagnell. Reinforcement and imitation learning via interactive
530"
REFERENCES,0.8595419847328244,"no-regret learning. CoRR, abs/1406.5979, 2014.
531"
REFERENCES,0.8610687022900764,"[55] Stéphane Ross, Goeffrey J. Gordon, and J. Andrew Bagnell. A reduction of imitation learning
532"
REFERENCES,0.8625954198473282,"and structured prediction to no-regret online learning. In 14th International Conference on
533"
REFERENCES,0.8641221374045801,"Artificial Intelligence and Statistics, 2011.
534"
REFERENCES,0.8656488549618321,"[56] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre,
535"
REFERENCES,0.867175572519084,"Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy
536"
REFERENCES,0.8687022900763359,"Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned
537"
REFERENCES,0.8702290076335878,"model. Nature, 588:604–609, 2020.
538"
REFERENCES,0.8717557251908397,"[57] L. Julian Schvartzman and Michael P. Wellman. Exploring large strategy spaces in empirical
539"
REFERENCES,0.8732824427480916,"game modeling. In AAMAS-09 Workshop on Agent-Mediated Electronic Commerce, 2009.
540"
REFERENCES,0.8748091603053435,"[58] L. Julian Schvartzman and Michael P. Wellman. Stronger CDA strategies through empiri-
541"
REFERENCES,0.8763358778625954,"cal game-theoretic analysis and reinforcement learning. In 8th International Conference on
542"
REFERENCES,0.8778625954198473,"Autonomous Agents and Multi-Agent Systems, pages 249–256, 2009.
543"
REFERENCES,0.8793893129770992,"[59] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak
544"
REFERENCES,0.8809160305343512,"Pathak. Planning to explore via self-supervised world models. In 37th International Conference
545"
REFERENCES,0.8824427480916031,"of Machine Learning, pages 8583–8592, 2020.
546"
REFERENCES,0.8839694656488549,"[60] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
547"
REFERENCES,0.8854961832061069,"che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander
548"
REFERENCES,0.8870229007633588,"Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap,
549"
REFERENCES,0.8885496183206106,"Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the
550"
REFERENCES,0.8900763358778626,"game of Go with deep neural networks and tree search. Nature, 529:484–489, 2016.
551"
REFERENCES,0.8916030534351145,"[61] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George van den
552"
REFERENCES,0.8931297709923665,"Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al.
553"
REFERENCES,0.8946564885496183,"Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–
554"
REFERENCES,0.8961832061068702,"489, 2016.
555"
REFERENCES,0.8977099236641222,"[62] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur
556"
REFERENCES,0.899236641221374,"Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of
557"
REFERENCES,0.9007633587786259,"go without human knowledge. Nature, 550(7676):354–359, 2017.
558"
REFERENCES,0.9022900763358779,"[63] David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel
559"
REFERENCES,0.9038167938931297,"Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, and Thomas Degris. The
560"
REFERENCES,0.9053435114503817,"predictron: End-to-end learning and planning. In 34th International Conference on Machine
561"
REFERENCES,0.9068702290076336,"Learning, volume 70, pages 3191–3199, 2017.
562"
REFERENCES,0.9083969465648855,"[64] Max Olan Smith, Thomas Anthony, Yongzhao Wang, and Michael P. Wellman. Learning to
563"
REFERENCES,0.9099236641221374,"play against any mixture of opponents. CoRR, 2020.
564"
REFERENCES,0.9114503816793893,"[65] Max Olan Smith, Thomas Anthony, and Michael P. Wellman. Iterative empirical game solving
565"
REFERENCES,0.9129770992366413,"via single policy best response. In 9th International Conference on Learning Representations,
566"
REFERENCES,0.9145038167938931,"2021.
567"
REFERENCES,0.916030534351145,"[66] Chen Sun, Per Karlsson, Jiajun Wu, Joshua B Tenenbaum, and Kevin Murphy. Stochastic
568"
REFERENCES,0.917557251908397,"prediction of multi-agent interactions from partial observations. In 7th International Conference
569"
REFERENCES,0.9190839694656489,"on Learning Representations, 2019.
570"
REFERENCES,0.9206106870229007,"[67] Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on
571"
REFERENCES,0.9221374045801527,"approximating dynamic programming. In 7th International Workshop on Machine Learning,
572"
REFERENCES,0.9236641221374046,"pages 216–224. Morgan Kaufmann, 1990.
573"
REFERENCES,0.9251908396946565,"[68] Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. In
574"
REFERENCES,0.9267175572519084,"SIGART Bulletin, volume 2, pages 160–163. ACM, 1991.
575"
REFERENCES,0.9282442748091603,"[69] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. The MIT
576"
REFERENCES,0.9297709923664123,"Press, 2018.
577"
REFERENCES,0.9312977099236641,"[70] Richard S Sutton, Csaba Szepesvári, Alborz Geramifard, and Michael P. Bowling. Dyna-style
578"
REFERENCES,0.932824427480916,"planning with linear function approximation and prioritized sweeping. In 28th Conference on
579"
REFERENCES,0.934351145038168,"Uncertainty in Artificial Intelligence, 2012.
580"
REFERENCES,0.9358778625954198,"[71] Erin Talvitie. Model regularization for stable sample rollouts. In 30th Conference on Uncertainty
581"
REFERENCES,0.9374045801526718,"in Artificial Intelligence, 2014.
582"
REFERENCES,0.9389312977099237,"[72] Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM,
583"
REFERENCES,0.9404580152671755,"38(3):58–68, 1995.
584"
REFERENCES,0.9419847328244275,"[73] Zheng Tian, Ying Wen, Zhichen Gong, Faiz Punakkath, Shihao Zou, and Jun Wang.
A
585"
REFERENCES,0.9435114503816794,"regularized opponent model with maximum entropy objective. In International Joint Conference
586"
REFERENCES,0.9450381679389313,"on Artificial Intelligence, 2019.
587"
REFERENCES,0.9465648854961832,"[74] Karl Tuyls, Julien Pérolat, Marc Lanctot, Edward Hughes, Richard Everett, Joel Z. Leibo, Csaba
588"
REFERENCES,0.9480916030534351,"Szepesvári, and Thore Graepel. Bounds and dynamics for empirical game theoretic analysis.
589"
REFERENCES,0.9496183206106871,"Autonomous Agents and Multi-Agent Systems, 34(7), 2020.
590"
REFERENCES,0.9511450381679389,"[75] Yevgeniy Vorobeychik. Probabilistic analysis of simulation-based games. ACM Transactions
591"
REFERENCES,0.9526717557251908,"on Modeling and Computer Simulation, 20(3), 2010.
592"
REFERENCES,0.9541984732824428,"[76] Niklas Wahlström, Thomas B. Schön, and Marc Peter Deisenroth. From pixels to torques:
593"
REFERENCES,0.9557251908396946,"Policy learning with deep dynamical models. arXiv preprint arXiv:1502.02251, 2015.
594"
REFERENCES,0.9572519083969465,"[77] William Walsh, Rajarshi Das, Gerald Tesauro, and Jeffrey Kephart. Analyzing complex strategic
595"
REFERENCES,0.9587786259541985,"interactions in multi-agent systems. In AAAI-02 Workshop on Game Theoretic and Decision
596"
REFERENCES,0.9603053435114504,"Theoretic Agents, 2002.
597"
REFERENCES,0.9618320610687023,"[78] Rose E Wang, Chase Kew, Dennis Lee, Edward Lee, Brian Andrew Ichter, Tingnan Zhang, Jie
598"
REFERENCES,0.9633587786259542,"Tan, and Aleksandra Faust. Model-based reinforcement learning for decentralized multiagent
599"
REFERENCES,0.9648854961832061,"rendezvous. In Conference on Robot Learning, 2020.
600"
REFERENCES,0.966412213740458,"[79] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to
601"
REFERENCES,0.9679389312977099,"control: A locally linear latent dynamics model for control from raw images. In 28th Conference
602"
REFERENCES,0.9694656488549618,"on Neural Information Processing Systems, pages 2746–2754, 2015.
603"
REFERENCES,0.9709923664122138,"[80] Michael P. Wellman. Methods for empirical game-theoretic analysis. In 21st National Confer-
604"
REFERENCES,0.9725190839694656,"ence on Artificial Intelligence, page 1552–1555, 2006.
605"
REFERENCES,0.9740458015267176,"[81] Daniël Willemsen, Mario Coppola, and Guido CHE de Croon. MAMBPO: Sample-efficient
606"
REFERENCES,0.9755725190839695,"multi-robot reinforcement learning using learned world models. In IEEE/RSJ International
607"
REFERENCES,0.9770992366412213,"Conference on Intelligent Robots and Systems, 2021.
608"
REFERENCES,0.9786259541984733,"[82] Ronald J. Williams and David Zipser. A learning algorithm for continually running fully
609"
REFERENCES,0.9801526717557252,"recurrent neural networks. Neural Computation, 1(2), 1989.
610"
REFERENCES,0.981679389312977,"[83] Fan Yang, Gabriel Barth-Maron, Piotr Sta´nczyk, Matthew Hoffman, Siqi Liu, Manuel Kroiss,
611"
REFERENCES,0.983206106870229,"Aedan Pope, and Alban Rrustemi. Launchpad: A programming model for distributed machine
612"
REFERENCES,0.9847328244274809,"learning research. arXiv preprint arXiv:2106.04516, 2021.
613"
REFERENCES,0.9862595419847329,"[84] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea
614"
REFERENCES,0.9877862595419847,"Finn, and Tengyu Ma. MOPO: Model-based offline policy optimization. In 33rd Conference on
615"
REFERENCES,0.9893129770992366,"Neural Information Processing Systems, 2020.
616"
REFERENCES,0.9908396946564886,"[85] Kaiqing Zhang, Sham Kakade, Tamer Basar, and Lin Yang. Model-based multi-agent rl in
617"
REFERENCES,0.9923664122137404,"zero-sum markov games with near-optimal sample complexity. In 33rd Conference on Neural
618"
REFERENCES,0.9938931297709923,"Information Processing Systems, 2020.
619"
REFERENCES,0.9954198473282443,"[86] Qizhen Zhang, Chris Lu, Animesh Garg, and Jakob Foerster. Centralized model and exploration
620"
REFERENCES,0.9969465648854962,"policy for multi-agent RL. In 21st International Conference on Autonomous Agents and
621"
REFERENCES,0.9984732824427481,"Multiagent Systems, 2022.
622"
