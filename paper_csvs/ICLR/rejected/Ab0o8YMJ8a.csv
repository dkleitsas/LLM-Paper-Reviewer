Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001996007984031936,"Neural network pruning allows for signiﬁcant reduction of model size and latency. How-
1"
ABSTRACT,0.003992015968063872,"ever, most of the current network pruning methods do not consider channel interdepen-
2"
ABSTRACT,0.005988023952095809,"dencies and a lot of manual adjustments are required before they can be applied to new
3"
ABSTRACT,0.007984031936127744,"network architectures. Moreover, these algorithms are often based on hand-picked, some-
4"
ABSTRACT,0.00998003992015968,"times complicated heuristics and can require thousands of GPU computation hours. In
5"
ABSTRACT,0.011976047904191617,"this paper, we introduce a simple neural network pruning and ﬁne-tuning framework that
6"
ABSTRACT,0.013972055888223553,"requires no manual heuristics, is highly efﬁcient to train (2-6 times speed up compared to
7"
ABSTRACT,0.015968063872255488,"NAS-based competitors) and produces comparable performance. The framework contains
8"
ABSTRACT,0.017964071856287425,"1) an automatic channel detection algorithm that groups the interdependent blocks of
9"
ABSTRACT,0.01996007984031936,"channels; 2) a non-iterative pruning algorithm that learns channel importance directly from
10"
ABSTRACT,0.021956087824351298,"feature maps while masking the coupled computational blocks using Gumbel-Softmax
11"
ABSTRACT,0.023952095808383235,"sampling and 3) a hierarchical knowledge distillation approach to ﬁne-tune the pruned
12"
ABSTRACT,0.02594810379241517,"neural networks. We validate our pipeline on ImageNet classiﬁcation, human segmentation
13"
ABSTRACT,0.027944111776447105,"and image denoising, creating lightweight and low latency models, easy to deploy on
14"
ABSTRACT,0.029940119760479042,"mobile devices. Using our pruning algorithm and hierarchical knowledge distillation for
15"
ABSTRACT,0.031936127744510975,"ﬁne-tuning we are able to prune EfﬁcientNet B0, EfﬁcientNetV2 B0 and MobileNetV2
16"
ABSTRACT,0.033932135728542916,"to 75% of their original FLOPs with no loss of accuracy on ImageNet. We release a set
17"
ABSTRACT,0.03592814371257485,"pruned backbones as Keras models - all of them proved beneﬁcial when deployed in other
18"
ABSTRACT,0.03792415169660679,"projects.
19"
INTRODUCTION,0.03992015968063872,"1
INTRODUCTION
20"
INTRODUCTION,0.041916167664670656,"Efforts directed towards deployment of neural networks on low-performance devices such as mobile phones
21"
INTRODUCTION,0.043912175648702596,"or TVs, created a demand for smaller and faster models. This has led to advances in neural network
22"
INTRODUCTION,0.04590818363273453,"compression techniques, which allow us to minimize existing large-scale architectures and adjust them to
23"
INTRODUCTION,0.04790419161676647,"ﬁt speciﬁc hardware requirements. Some techniques have been especially successful in this area. Neural
24"
INTRODUCTION,0.0499001996007984,"network quantization approaches (Nagel et al., 2021) not only decreased the size of the models, but also
25"
INTRODUCTION,0.05189620758483034,"enabled us to utilize specialized computing accelerators like DSPs. Unfortunately, other techniques, such as
26"
INTRODUCTION,0.05389221556886228,"network pruning (Liu et al., 2020), are not equally effective in low-resource environments.
27"
INTRODUCTION,0.05588822355289421,"Early attempts of naive weight pruning introduced sparse computations, which render them inefﬁcient in
28"
INTRODUCTION,0.05788423153692615,"practical scenarios (Han et al., 2015; Guo et al., 2016). Channel pruning (Li et al., 2016; Liu et al., 2017;
29"
INTRODUCTION,0.059880239520958084,"2021a; Herrmann et al., 2020; Liu et al., 2019b) delivers signiﬁcant improvements in terms of both memory
30"
INTRODUCTION,0.06187624750499002,"consumption and execution speed, and is the preferred approach if we want to deploy our models on mobile
31"
INTRODUCTION,0.06387225548902195,"devices.
32"
INTRODUCTION,0.0658682634730539,"However, the majority of existing approaches to channel pruning share several drawbacks:
33"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.06786427145708583,"1. Little effort has been made to address channel interdependencies that occur in the majority of the
34"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.06986027944111776,"architectures, with Liu et al. (2021a) being a notable exception. Many popular network architectures
35"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.0718562874251497,"contain residual connections inspired by ResNet (He et al., 2015). Feature maps added in residual
36"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.07385229540918163,"connections must hold the same shapes, which is likely to be violated when channels are removed
37"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.07584830339321358,"independently. We refer to channels involved in this kind of dependency as coupled. Automating
38"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.07784431137724551,"the process of adding pruning logic to the network in consideration of channel interdependencies is
39"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.07984031936127745,"extremely important in practical considerations.
40"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.08183632734530938,"2. Most methods require an expensive and time-consuming ﬁne-tuning process after channels are
41"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.08383233532934131,"removed. Some authors use an iterative approach, where channels are removed in a number of steps,
42"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.08582834331337326,"and ﬁne-tuning is performed between these steps. Either way, the ﬁne-tuning process often requires
43"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.08782435129740519,"a signiﬁcant number of GPU hours to complete.
44"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.08982035928143713,"3. Channels in any given convolution are being considered independently. However, some target
45"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.09181636726546906,"platforms, e.g. SNPE (Qualcomm), are optimized for speciﬁc numbers of input and output channels
46"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.09381237524950099,"and pruning channels independently can give little to no speed-up.
47"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.09580838323353294,"In order to overcome these issues we introduce an end-to-end channel pruning pipeline which can be
48"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.09780439121756487,"deployed on a wide array of neural networks in an automated way. Our main insights are that: (1) Channel
49"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.0998003992015968,Under review as a conference paper at ICLR 2022
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.10179640718562874,"Figure 1: ImageNet accuracy of pruned EfﬁcientNet B0 and B1. Considering FLOPs/accuracy trade-off the
some pruned models are better than FBNetV2, which to our knowledge has SOTA results in its FLOPs range."
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.10379241516966067,"importance can be learned from the feature maps using simple additional networks and no hand-crafted
50"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.10578842315369262,"channel importance metric is needed. (2) Neural network computational graphs should be partitioned in a
51"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.10778443113772455,"way which enables removing channels jointly if they are coupled, e.g., if they belong to convolutions whose
52"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.10978043912175649,"outputs are later added (the case in point being skip connections). (3) Hierarchical knowledge distillation (a
53"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.11177644710578842,"variant of classical knowledge distillation in which multiple teacher networks are used consecutively) is the
54"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.11377245508982035,"preferred way of ﬁne-tuning networks after channels are removed since it signiﬁcantly speeds up training,
55"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.1157684630738523,"results in better accuracy and can be used with little or no data augmentation.
56"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.11776447105788423,"Our contributions can be summarized as follows:
57"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.11976047904191617,"1. New pruning algorithm. We introduce a new pruning algorithm in which channel importance is
58"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.1217564870259481,"learned directly from the feature maps. The process of choosing channels is one-shot and requires
59"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.12375249500998003,"just a couple of GPU hours.
60"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.12574850299401197,"2. Automated method for grouping operations that should be pruned jointly, based on channel
61"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.1277445109780439,"interdependencies. We introduce a relatively simple way of inserting the pruning logic into
62"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.12974051896207583,"networks which allows to discard the error-prone process of manual inspection. This makes our
63"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.1317365269461078,"solution easy to scale and be deployed for segmentation, detection or image denoising.
64"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.13373253493013973,"3. Hierarchical knowledge distillation We employ a novel approach to ﬁne-tuning pruned networks.
65"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.13572854291417166,"The approach is related to the one presented in Mirzadeh et al. (2020). The major insight is to
66"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.1377245508982036,"gradually increase the complexity of the teacher network. It leads to much quicker training, and
67"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.13972055888223553,"yet, achieves much better ﬁnal accuracy, while not requiring advanced and time-consuming data
68"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.14171656686626746,"augmentation procedures.
69"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.1437125748502994,"4. Up to 25% reduction if FLOPs with no loss in accuracy on ImageNet. We validate our channel
70"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.14570858283433133,"pruning pipeline on:
71"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.14770459081836326,"• EfﬁcientNet (Tan & Le, 2019), MobileNetV2 (Sandler et al., 2018) and EfﬁcientNetV2 (Tan &
72"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.1497005988023952,"Le, 2021) models for classiﬁcation on ImageNet;
73"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.15169660678642716,"• a human segmentation model based on EfﬁcientNet B0 with EfﬁcientDet - like Tan et al. (2019)
74"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.1536926147704591,"segmentation head;
75"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.15568862275449102,"• PMRID (Wang et al., 2020) network for RawRGB image denoising.
76"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.15768463073852296,"In Figure 1 we can see that for FLOPs between 200M and 300M our pruned models outperform
77"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.1596806387225549,"FBNetV2 which used, among other things, superkernels to modify existing EfﬁcientNet architecture
78"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.16167664670658682,"and produced models that outperform EfﬁcientNet itself. This shows that our pruning and ﬁne-
79"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.16367265469061876,"tuning pipeline (which is much simpler than the NAS algortithms used in Wan et al. (2020)) can
80"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.1656686626746507,"generate better results. Moreover, EfﬁcientNet B0 pruned to 75% of its original FLOPs has the
81"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.16766467065868262,"same accuracy as the original model. Interestingly EfﬁcientNet B1 pruned to match EfﬁcientNet B0
82"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.16966067864271456,"outperforms B0 by approximately 1% in top-1 accuracy on ImageNet.
83"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.17165668662674652,"The whole framework of pruning and ﬁne-tuning we introduce in this paper requires little computational
84"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.17365269461077845,"resources. The pruning algorithm usually only takes a couple of hours to complete on a single GPU. Using
85"
LITTLE EFFORT HAS BEEN MADE TO ADDRESS CHANNEL INTERDEPENDENCIES THAT OCCUR IN THE MAJORITY OF THE,0.17564870259481039,"hierarchical knowledge distillation further speeds up the ﬁne-tuning process.
86"
RELATED WORK,0.17764471057884232,"2
RELATED WORK
87"
RELATED WORK,0.17964071856287425,"Channel selection. Many channel pruning methods employ a greedy approach where channel removal is
88"
RELATED WORK,0.18163672654690619,"interleaved with expensive ﬁne-tuning of the network (Luo et al., 2018; Liu et al., 2015; He et al., 2017).
89"
RELATED WORK,0.18363273453093812,Under review as a conference paper at ICLR 2022
RELATED WORK,0.18562874251497005,"Similar, but a more affordable approach, is to periodically prune channels throughout a single training
90"
RELATED WORK,0.18762475049900199,"procedure (Liu et al., 2021a; Guo et al., 2020; Chen et al., 2020). Ye et al. (2020) and Hou et al. (2021)
91"
RELATED WORK,0.18962075848303392,"point out ﬂaws in the idea of greedy channel removal and propose to selectively restore channels in the
92"
RELATED WORK,0.19161676646706588,"pruned network. Liu et al. (2019b) trains an auxiliary neural network to quickly evaluate pruned networks
93"
RELATED WORK,0.1936127744510978,"and select the best one using an evolutionary algorithm. Other methods jointly train a neural network and
94"
RELATED WORK,0.19560878243512975,"learn importance scores for its channels using channel gating mechanism. In (Chen et al., 2020), this is
95"
RELATED WORK,0.19760479041916168,"achieved by randomly enable and disable channels during each iteration of the training. Gradient descent
96"
RELATED WORK,0.1996007984031936,"was used to update the importance scores in Herrmann et al. (2020); Lin et al. (2020); Ye et al. (2020) and is
97"
RELATED WORK,0.20159680638722555,"based on the idea for optimizing hyperparameters in neural architecture search in Liu et al. (2019a) and Xie
98"
RELATED WORK,0.20359281437125748,"et al. (2018). These gradient-based methods rely on Gumbel-Softmax reparametrization trick (Jang et al.,
99"
RELATED WORK,0.2055888223552894,"2016) to enable back-propagating through the gates distribution. Herrmann et al. (2020) proposes a variant
100"
RELATED WORK,0.20758483033932135,"of such a method where the logits of the channel gates are trainable parameters, as well as a variant where
101"
RELATED WORK,0.20958083832335328,"the logits are produced by an auxiliary neural network that accepts a feature map. Selecting channels based
102"
RELATED WORK,0.21157684630738524,"network input introduces an overhead that is unacceptable on resource-limited devices. Our solution contains
103"
RELATED WORK,0.21357285429141717,"a similar idea, but we ensured that the auxiliary networks can be safely removed after the training.
104"
RELATED WORK,0.2155688622754491,"Channel coupling. The channel coupling pattern occurs in many modern architectures inspired by ResNet
105"
RELATED WORK,0.21756487025948104,"(He et al., 2015), such as MobileNet (Sandler et al., 2018), EfﬁcientNet (Tan & Le, 2019; 2021) or FBNet
106"
RELATED WORK,0.21956087824351297,"(Wan et al., 2020). Many studies seem to ignore this issue (Herrmann et al., 2020; Lin et al., 2020; Ye et al.,
107"
RELATED WORK,0.2215568862275449,"2020); other resolve this issue by manually grouping interdependent layers or providing model-speciﬁc
108"
RELATED WORK,0.22355289421157684,"heuristics (Shao et al., 2021; Hou et al., 2021; Guo et al., 2020; Liu et al., 2021b). Independently to our
109"
RELATED WORK,0.22554890219560877,"efforts, an automated solution for grouping channels has been proposed in Liu et al. (2021a). We propose a
110"
RELATED WORK,0.2275449101796407,"similar algorithm (see section 4), and additionally offer an extension for handling concatenations.
111"
RELATED WORK,0.22954091816367264,"Measuring speed-up. Many pruning methods are parametrised by a fraction of channels to prune, either
112"
RELATED WORK,0.2315369261477046,"globally or per-layer (Lin et al., 2020; Ye et al., 2020; Herrmann et al., 2020). Overall network FLOPs1
113"
RELATED WORK,0.23353293413173654,"better corresponds to the usual business requirements. In Chen et al. (2020) and Liu et al. (2021a), the
114"
RELATED WORK,0.23552894211576847,"maximal FLOPs parameter is included in their stopping criteria and importance scores of channels are
115"
RELATED WORK,0.2375249500998004,"adjusted according to their computation cost. Similarly to Guo et al. (2020), we construct a loss function that
116"
RELATED WORK,0.23952095808383234,"introduce a penalty for exceeding the provided FLOPs budget and use it as a part differentiable importance
117"
RELATED WORK,0.24151696606786427,"optimization.
118"
RELATED WORK,0.2435129740518962,"Knowledge distillation. It has been noted that Knowledge distillation can perform poorly when there is
119"
RELATED WORK,0.24550898203592814,"a large discrepancy in complexity between student and teacher networks (Cho & Hariharan, 2019). Cho
120"
RELATED WORK,0.24750499001996007,"& Hariharan (2019) evaluate a step-wise approach, in which the intermediate teacher networks are trained
121"
RELATED WORK,0.249500998003992,"by distilling knowledge from the original large teacher and then ﬁnd it ineffective. Mirzadeh et al. (2020)
122"
RELATED WORK,0.25149700598802394,"propose using a teacher assistant to bridge the complexity gap. Hou et al. (2021) apply knowledge distillation
123"
RELATED WORK,0.25349301397205587,"to ﬁne-tune pruned network, but do not address aforementioned issues. We propose an inverted version of
124"
RELATED WORK,0.2554890219560878,"the step-wise approach from Cho & Hariharan (2019), and train train our pruned network with increasingly
125"
RELATED WORK,0.25748502994011974,"larger teachers. Such chains can be naturally formed for model families like EfﬁcientNet (Tan & Le, 2019)
126"
RELATED WORK,0.25948103792415167,"and EfﬁcientNetV2 (Tan & Le, 2021). We also observe that in case of generic knowledge distillation, the
127"
RELATED WORK,0.26147704590818366,"ﬁnal results can be improved by (even slightly) disturbing the student model with channel pruning before
128"
RELATED WORK,0.2634730538922156,"starting the distillation.
129"
PRUNING METHOD,0.2654690618762475,"3
PRUNING METHOD
130"
PRUNING METHOD,0.26746506986027946,"The basic idea behind our channel pruning algorithm is to set up a scheme in which the importance of
131"
PRUNING METHOD,0.2694610778443114,"channels is being learned from the feature maps generated by convolutions in neural networks. We assign
132"
PRUNING METHOD,0.2714570858283433,"each channel a score corresponding to its importance that is updated at each training step and used to
133"
PRUNING METHOD,0.27345309381237526,"approximate behavior of the pruned network by appropriate masking (Liu et al., 2017; Herrmann et al.,
134"
PRUNING METHOD,0.2754491017964072,"2020). Similarly to Herrmann et al. (2020) we apply a probabilistic approach where channels in feature
135"
PRUNING METHOD,0.2774451097804391,"maps are masked with samples from random variables with values in (0, 1). This is a continuous relaxation
136"
PRUNING METHOD,0.27944111776447106,"approach to solving a discrete problem. The distributions of these random variables depend on the values of
137"
PRUNING METHOD,0.281437125748503,"corresponding logits (which can be though of as proxies for channel scores and have values in R). These
138"
PRUNING METHOD,0.2834331337325349,"logits are learned during the pruning stage. More precisely, given a feature map of size (B, H, W, C) (B
139"
PRUNING METHOD,0.28542914171656686,"is batch size, H and W are spatial dimension and C is the number of channels) and a logits variable, for
140"
PRUNING METHOD,0.2874251497005988,"each channel separately we sample — using Gumbel-Softmax (Jang et al., 2016) — the random variable
141"
PRUNING METHOD,0.2894211576846307,"parametrized by the corresponding logit in logits. We mask the feature map by multiplying it by the
142"
PRUNING METHOD,0.29141716566866266,"sampled values.
143"
PRUNING METHOD,0.2934131736526946,"We do not consider each feature map individually — instead, we extend our understanding of channels from
144"
PRUNING METHOD,0.2954091816367265,"a single feature map to a series of operations occurring within a network. The intuition is that element-wise
145"
PRUNING METHOD,0.29740518962075846,"operations, like activation functions, propagate channels forward throughout the network, while convolutional
146"
PRUNING METHOD,0.2994011976047904,"layers consume their input channels and create new ones. Pruning sequential models is trivial but in more
147"
PRUNING METHOD,0.3013972055888224,"complicated cases, like models with residual connections, there exist additional couplings between channels,
148"
PRUNING METHOD,0.3033932135728543,"introduced by operations that accept multiple inputs, e.g. element-wise sum, multiplication (Fig. 2). Because
149"
PRUNING METHOD,0.30538922155688625,1a number of ﬂoating-point operations
PRUNING METHOD,0.3073852295409182,Under review as a conference paper at ICLR 2022
PRUNING METHOD,0.3093812375249501,"coupled channels must be pruned jointly to ensure valid shapes, we use a single random variable to mask
150"
PRUNING METHOD,0.31137724550898205,"each set of coupled channels (see Section 4 for details about automatic detection of coupled channels).
151"
PRUNING METHOD,0.313373253493014,"Although logits can be treated as standalone trainable variables, we choose to learn them from the feature
152"
PRUNING METHOD,0.3153692614770459,"maps in a feedback-loop mechanism. This is because the latter approach is faster to train, results in logits
153"
PRUNING METHOD,0.31736526946107785,"which (once converted to probabilities) have lower entropy and produces better results. Once we decide on
154"
PRUNING METHOD,0.3193612774451098,"the feature maps from which we will learn the optimal logits values, we place simple neural networks called
155"
PRUNING METHOD,0.3213572854291417,"logit predictor modules that take these feature maps as inputs. These modules are build of 3x3 depthwise
156"
PRUNING METHOD,0.32335329341317365,"convolution followed by 1x1 convolution and global mean pooling along spatial dimensions. The output
157"
PRUNING METHOD,0.3253493013972056,"output vector of each such module is later used to update the value of the corresponding logits variable
158"
PRUNING METHOD,0.3273453093812375,"(using exponential moving average) as in Figure 2.
159"
PRUNING METHOD,0.32934131736526945,"The masking operations should always be placed just before the convolution operations that absorb the
160"
PRUNING METHOD,0.3313373253493014,"channels (see Figure 2). The placement of logit predictors is more involved and in cases more complicated
161"
PRUNING METHOD,0.3333333333333333,"than the relatively simple one presented in Figure 2, we choose to follow a simple heuristic to place them
162"
PRUNING METHOD,0.33532934131736525,"after convolutions with largest kernel sizes.
163"
PRUNING METHOD,0.3373253493013972,"During the pruning phase we augment the task-speciﬁc loss with an auxiliary latency-based loss. It is based
164"
PRUNING METHOD,0.3393213572854291,"on the expected number of FLOPs in the pruned network, which is computed by using all the logits we have
165"
PRUNING METHOD,0.3413173652694611,"attached to the network. We train network weights and logit predictor modules jointly so that the network
166"
PRUNING METHOD,0.34331337325349304,"can adjust to channels being phased out.
167"
PRUNING METHOD,0.34530938123752497,Activ.
PRUNING METHOD,0.3473053892215569,Activ.
PRUNING METHOD,0.34930139720558884,"Sum
Conv Conv Conv"
PRUNING METHOD,0.35129740518962077,Gumbel-
PRUNING METHOD,0.3532934131736527,"Softmax
masking"
PRUNING METHOD,0.35528942115768464,"channel
logits
moving
average"
PRUNING METHOD,0.35728542914171657,update
PRUNING METHOD,0.3592814371257485,"Logit
Predictor"
PRUNING METHOD,0.36127744510978044,"Figure 2: An subset of a network with logit predictor and masking. The colors indicate the correspondence
between channels. The logit predictor takes a feature map produced by the sum operation and use it to
predict an update for the channel logits."
PRUNING LARGER BLOCKS OF CHANNELS,0.36327345309381237,"3.1
PRUNING LARGER BLOCKS OF CHANNELS
168"
PRUNING LARGER BLOCKS OF CHANNELS,0.3652694610778443,"We allow for blocks of channels (instead of just individual channels) to be treated jointly, so that blocks
169"
PRUNING LARGER BLOCKS OF CHANNELS,0.36726546906187624,"of a predeﬁned size will be chosen or discarded together. This is especially important for platforms where
170"
PRUNING LARGER BLOCKS OF CHANNELS,0.36926147704590817,"convolutions are optimized with a speciﬁc block size o channels in mind, e.g., for SNPE (Qualcomm) this
171"
PRUNING LARGER BLOCKS OF CHANNELS,0.3712574850299401,"number is 32 and pruning individual channels often makes little sense.
172"
LAYER GROUPING ALGORITHM,0.37325349301397204,"4
LAYER GROUPING ALGORITHM
173"
LAYER GROUPING ALGORITHM,0.37524950099800397,"Although channel coupling has been observed in the literature, relevant groups of operations seem to be
174"
LAYER GROUPING ALGORITHM,0.3772455089820359,"usually established via network-speciﬁc heuristics or manual annotation. A notable exception is Liu et al.
175"
LAYER GROUPING ALGORITHM,0.37924151696606784,"(2021a) where the problem is described at length and an algorithm for ﬁnding the groups is derived. The
176"
LAYER GROUPING ALGORITHM,0.3812375249500998,"algorithm is then tested on architectures based on ResNet. However, unlike our solution, it does not support
177"
LAYER GROUPING ALGORITHM,0.38323353293413176,"concatenation operations. For clarity, we focus on convolutional neural networks, but the proposed strategy
178"
LAYER GROUPING ALGORITHM,0.3852295409181637,"can be extended to other kinds of architectures.
179"
SOLUTION,0.3872255489021956,"4.1
SOLUTION
180"
SOLUTION,0.38922155688622756,"To overcome the issues delineated in Section 3 and make channel pruning available for most off-the-
181"
SOLUTION,0.3912175648702595,"shelf architectures we have developed an algorithm that is capable of automatically detecting channel
182"
SOLUTION,0.3932135728542914,"interdependencies between feature maps generated by operations in the network.
183"
SOLUTION,0.39520958083832336,"To keep track of all the places where channels have to be considered in a synchronised way, we introduce the
184"
SOLUTION,0.3972055888223553,"concept of an orbit. An orbit can be thought as subset of operations that are interdependent from the point of
185"
SOLUTION,0.3992015968063872,"view of channel pruning. Operations in the same orbit need to be considered jointly when removing channels.
186"
SOLUTION,0.40119760479041916,"Naively removing channels without taking into account these interdependencies may result in an invalid
187"
SOLUTION,0.4031936127744511,"network. For example, if we remove an output channel from one of the convolutions on the left in Figure 2,
188"
SOLUTION,0.405189620758483,Under review as a conference paper at ICLR 2022 + C4 C5 C6
SOLUTION,0.40718562874251496,concat C1 C2 C3
SOLUTION,0.4091816367265469,(a) extended orbit + C4 C5 C6
SOLUTION,0.4111776447105788,concat C1 C2 C3
SOLUTION,0.41317365269461076,(b) ﬁnal orbits (red and blue)
SOLUTION,0.4151696606786427,"Figure 3: Breaking up an extended orbit. An extended orbit is broken up into two ﬁnal orbits. Nodes C1 and
C2 must have their channels pruned jointly. Node C3 can be pruned separately."
SOLUTION,0.4171656686626746,"the number of channels will no longer match for the Sum operation. A typical network has multiple orbits.
189"
SOLUTION,0.41916167664670656,"It is easiest to understand this concept by seeing how orbits are build, which we delineate in Algorithm 1
190"
SOLUTION,0.42115768463073855,"below.
191"
SOLUTION,0.4231536926147705,"First, we ﬁx some notation to make matters more intuitive. All the operations in a typical convolutional
192"
SOLUTION,0.4251497005988024,"neural network can be described as being of the following types:
193"
SOLUTION,0.42714570858283435,"1. sources are the operation where new channels are being created, namely regular convolution layers
194"
SOLUTION,0.4291417165668663,"(not depthwise!) and dense layers;
195"
SOLUTION,0.4311377245508982,"2. sinks are the operation where channels are being absorbed, namely regular convolution layers (not
196"
SOLUTION,0.43313373253493015,"depthwise!) and dense layers;
197"
CONTINUATORS ARE ALL THE OPERATIONS WITH A SINGLE INPUT TENSOR THAT SIMPLY PASS ON THE CHANNELS,0.4351297405189621,"3. continuators are all the operations with a single input tensor that simply pass on the channels
198"
CONTINUATORS ARE ALL THE OPERATIONS WITH A SINGLE INPUT TENSOR THAT SIMPLY PASS ON THE CHANNELS,0.437125748502994,"forward, e.g., batch normalization, mean pooling, resize, activations;
199"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.43912175648702595,"4. joiners are operations with multiple input tensors of the same shape which join these tensors
200"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.4411177644710579,"without altering the shape, namely element-wise addition and multiplication;
201"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.4431137724550898,"Typically, continuator operations are not problematic since they do not alter the channels structure and have
202"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.44510978043912175,"a single predecessor and a single output. It is the joiner operations that introduce interdependencies between
203"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.4471057884231537,"channels. For brevity, from now on we will only speak of convolutions as sources and sinks, but everything
204"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.4491017964071856,"applies just as well to dense layers.
205"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.45109780439121755,"Note that some sources can be sinks at the same time and vice versa. We refer to operations that are either
206"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.4530938123752495,"sinks or sources as source-sinks. To identify all the subgraphs in the network where channels have to be
207"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.4550898203592814,"considered jointly we run an exhaustive-search type algorithm which has two distinct phases:
208"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.45708582834331335,"In the ﬁst phase we search for extended orbits, where the coupled operations are brought together. In
209"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.4590818363273453,"Algorithm 1 we describe how extended orbits are created. The input is a neural network directed acyclic
210"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.46107784431137727,"graph (DAG). The algorithm amounts to removing all inbound edges from convolution nodes and ﬁnding all
211"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.4630738522954092,"weakly connected components in the resulting graph. The extended orbits are then these weakly connected
212"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.46506986027944114,"components once we restore the inbound edges in convolution nodes.
213"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.46706586826347307,"The second phase is similar to the ﬁrst one. For all extended orbits found in phase one we do the following:
214"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.469061876247505,"take the extended orbit and then mark concatenation nodes (which play a special role, since they group
215"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.47105788423153694,"channels from separate sources) inside as sinks and repeat the process. Most notably, we discard extended
216"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.47305389221556887,"orbits in which there are concatenation nodes followed by joiner nodes, as it makes the whole process much
217"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.4750499001996008,"more difﬁcult to implement. We do not prune channels within such orbits. In Figure 3 we give an example of
218"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.47704590818363274,"an extended orbit and how is broken up into ﬁnal orbits.
219"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.47904191616766467,Algorithm 1 Searching for extended orbits
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.4810379241516966,"Input: network DAG with layers represented as nodes
1: P := {p : p is a path starting and ending with a convolution with no convolutions inside the path }
2: for each path p in P remove the last node
3: for every distinct node ni on paths in P, create an empty color set for the node Cni = {}
4: X := {x : x is the initial node of a path in P }
5: for x in X do
6:
pick an unused color c
7:
add color c to color sets of all the nodes on all the paths in P starting in x
8: end for
9: while there exist nodes with multiple colors do
10:
pick a node with multiple colors {c1, c2, . . . , ck} at random
11:
if any node in the DAG has a color in {c2, . . . , ck} switch the color to c1
12: end while"
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.48303393213572854,Under review as a conference paper at ICLR 2022
JOINERS ARE OPERATIONS WITH MULTIPLE INPUT TENSORS OF THE SAME SHAPE WHICH JOIN THESE TENSORS,0.48502994011976047,"5
PRUNING, FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION
220"
PRUNING STAGE,0.4870259481037924,"5.1
PRUNING STAGE
221"
PRUNING STAGE,0.48902195608782434,"The pruning workﬂow is the same for all types of tasks. We ﬁrst ﬁnd all ﬁnal orbits in the network and attach
222"
PRUNING STAGE,0.49101796407185627,"logit predictors. Final orbits determine both: which parts of the network are being pruned and which of them
223"
PRUNING STAGE,0.4930139720558882,"are pruned jointly. The FLOPs per pixel can be automatically computed (and are differentiable with respect
224"
PRUNING STAGE,0.49500998003992014,"to the channel logits as in (Fig. 2). We can compute FLOPs for the original network and then set some
225"
PRUNING STAGE,0.49700598802395207,"FLOPs target. In practice we compute kFPP (FLOPs per pixel of the input tensor divided by 1000), to have
226"
PRUNING STAGE,0.499001996007984,"a value that is independent of the input size. The latency loss is then given by ReLU(kFPP/target_kFPP−1).
227"
PRUNING STAGE,0.500998003992016,"We add this loss to the quality loss related to the task, e.g., cross entropy in classiﬁcation. To avoid an
228"
PRUNING STAGE,0.5029940119760479,"overly aggressive reduction of kFPP, we anneal the loss using exponential decay so that at the beginning of
229"
PRUNING STAGE,0.5049900199600799,"training the annealing multiplier is 0. and approaches 1. as the training progresses.
230"
PRUNING STAGE,0.5069860279441117,"Once the pruning phase is over we retain or discard output channels in convolutions based on channel
231"
PRUNING STAGE,0.5089820359281437,"interdependence discovered by applying Algorithm 1 and the values of logits variables learned by logit
232"
PRUNING STAGE,0.5109780439121756,"predictors.
233"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5129740518962076,"5.2
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION
234"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5149700598802395,"We propose to ﬁne-tune pruned models with a method we call hierarchical knowledge distillation. This
235"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5169660678642715,"approach relies on increasing the complexity of the teacher network in discrete steps. Given a ﬁne-tuning
236"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5189620758483033,"budget of K GPU hours, and N teacher networks we train the network for K/N GPU hours with each of
237"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5209580838323353,"these teacher networks, starting with the smallest one. Our loss is Lce + 5Lkd where Lce is the standard
238"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5229540918163673,"cross entropy loss and Lkd is the distillation loss. Using higher weight term for the Lkd is crucial to prevent
239"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5249500998003992,"overﬁtting and produce better results.
240"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5269461077844312,"Hierarchical knowledge distillation consistently performs much better than just using the original model as
241"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5289421157684631,"the teacher. The comparisons can be seen in Section 6.2. Given an array of models with increasing FLOPs
242"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.530938123752495,"requirements, like EfﬁcientNet Tan & Le (2019) and EfﬁcientNetV2 Tan & Le (2021), it is possible to cheaply
243"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5329341317365269,"train new models for missing FLOPs values. This may produce better results in terms of FLOPs/accuracy
244"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5349301397205589,"trade-off and require less computational resources.
245"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5369261477045908,"It is perplexing that trying to use hierarchical knowledge distillation on an unpruned network does not work
246"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5389221556886228,"anywhere near as well. Our intuition is that pruning provides some kind of initial perturbation to network
247"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5409181636726547,"weights and architecture which prove beneﬁcial from the point of view of gradient descent optimization.
248"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5429141716566867,"Are there any other types of model perturbations which boost the effectiveness of this type of knowledge
249"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5449101796407185,"distillation? These are the questions we could try to address as our future research. It would be also
250"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5469061876247505,"interesting to see how this approach performs when applied to recent state-of-the-art methods based on
251"
FINE-TUNING AND HIERARCHICAL KNOWLEDGE DISTILLATION,0.5489021956087824,"neural architecture search Wang et al. (2021).
252"
EXPERIMENTS,0.5508982035928144,"6
EXPERIMENTS
253"
EXPERIMENTS,0.5528942115768463,"All the experiments we perform adhere to the same schedule: (1) We ﬁrst run the pruning algorithm with
254"
EXPERIMENTS,0.5548902195608783,"additional latency losses (usually 1-10 epochs, depending on the task). (2) We then ﬁne-tune the pruned
255"
EXPERIMENTS,0.5568862275449101,"model (without resetting its weights). The experiments for classiﬁcation on ImageNet are presented in
256"
EXPERIMENTS,0.5588822355289421,"Section 6.2. Experiments for image denoising and human segmentation are presented in Sections A.2.1 and
257"
EXPERIMENTS,0.5608782435129741,"A.2.2, respectively.
258"
HYPERPARAMETERS FOR THE PRUNING PHASE,0.562874251497006,"6.1
HYPERPARAMETERS FOR THE PRUNING PHASE
259"
HYPERPARAMETERS FOR THE PRUNING PHASE,0.564870259481038,"For the pruning phase, during which channels to be removed are being chosen, the setup is roughly the same
260"
HYPERPARAMETERS FOR THE PRUNING PHASE,0.5668662674650699,"for each task. The logits predictor is always a two layer network with 3 × 3 depthwise convolution followed
261"
HYPERPARAMETERS FOR THE PRUNING PHASE,0.5688622754491018,"by 1 × 1 convolution and global mean pooling. We set the batch size to 16 and run the training updating the
262"
HYPERPARAMETERS FOR THE PRUNING PHASE,0.5708582834331337,"channel gates distributions as described in section 3. The initial value of channel logits is set to 3.0 so that
263"
HYPERPARAMETERS FOR THE PRUNING PHASE,0.5728542914171657,"initially there little to no masking. There is an additional loss that penalizes the entropy of all the logits so
264"
HYPERPARAMETERS FOR THE PRUNING PHASE,0.5748502994011976,"that at the end of the pruning phase the channel enabling probabilities (which we get by applying softmax to
265"
HYPERPARAMETERS FOR THE PRUNING PHASE,0.5768463073852296,"logits) are far away from 0.5. The temperature for Gumbel-Softmax is constant - 0.5.
266"
CLASSIFICATION ON IMAGENET,0.5788423153692615,"6.2
CLASSIFICATION ON IMAGENET
267"
CLASSIFICATION ON IMAGENET,0.5808383233532934,"We prune EfﬁcientNet B0, EfﬁcientNet B1 (Tan & Le, 2019), MobileNetV2 (Sandler et al., 2018), and
268"
CLASSIFICATION ON IMAGENET,0.5828343313373253,"EfﬁcientNetV2 (Tan & Le, 2021). We choose these since they are already highly optimized for mobile devices
269"
CLASSIFICATION ON IMAGENET,0.5848303393213573,"and relatively small. EfﬁcientNetV2 is a recent state-of-the-art architecture optimized for mobile GPUs and
270"
CLASSIFICATION ON IMAGENET,0.5868263473053892,"DSPs. All the models are taken from their ofﬁcial Keras implementations2 except for EfﬁcientNetV2. Larger
271"
CLASSIFICATION ON IMAGENET,0.5888223552894212,2https://www.tensorflow.org/api_docs/python/tf/keras/applications
CLASSIFICATION ON IMAGENET,0.590818363273453,Under review as a conference paper at ICLR 2022
CLASSIFICATION ON IMAGENET,0.592814371257485,Table 1: Top 1 ImageNet accuracy and FLOPs for for EfﬁcientNet B0 and B1 pruned
CLASSIFICATION ON IMAGENET,0.5948103792415169,(a) B0 pruned
CLASSIFICATION ON IMAGENET,0.5968063872255489,"Model
Standard
training
B0 teacher
B1 teacher
(after using B0 ﬁrst)
FLOPs (G)"
CLASSIFICATION ON IMAGENET,0.5988023952095808,"original
77.30
-
-
0.393
m6
-
73.49
73.72
0.156
m8
-
74.88
75.00
0.197
m10
-
76.01
76.56
0.248
m12
-
76.54
77.30
0.299
NPRR Hou et al. (2021)
-
77.00
-
0.346"
CLASSIFICATION ON IMAGENET,0.6007984031936128,(b) B1 pruned
CLASSIFICATION ON IMAGENET,0.6027944111776448,"Model
Standard
training
B1 teacher
B2 teacher
(after using B1 ﬁrst)
FLOPs (G)"
CLASSIFICATION ON IMAGENET,0.6047904191616766,"original
79.10
-
-
0.700
m12
-
77.02
76.99
0.299
m14
-
77.26
77.74
0.348
m16
-
77.66
78.35
0.400"
CLASSIFICATION ON IMAGENET,0.6067864271457086,"networks like the VGG19 or the ResNet family has been predominant in channel pruning literature, but are
272"
CLASSIFICATION ON IMAGENET,0.6087824351297405,"rarely suitable for resource-limited devices, where the need for optimization is biggest. The phase where
273"
CLASSIFICATION ON IMAGENET,0.6107784431137725,"channels are chosen usually lasts a little more than a single epoch on ImageNet. We split the ImageNet train
274"
CLASSIFICATION ON IMAGENET,0.6127744510978044,"data into two parts, leaving about 5% of the data for early-stopping.
275"
CLASSIFICATION ON IMAGENET,0.6147704590818364,"Following Section 5.2 we use multiple teacher networks. The details are as follows:
276"
CLASSIFICATION ON IMAGENET,0.6167664670658682,"• EfﬁcientNet B0: ﬁne-tune the models for 40 epochs with B0 as teacher and then we further
277"
CLASSIFICATION ON IMAGENET,0.6187624750499002,"ﬁne-tune with a B1 for another 40 epochs;
278"
CLASSIFICATION ON IMAGENET,0.6207584830339321,"• EfﬁcientNet B1: ﬁne-tune the models for 25 epochs with B1 as teacher and then we further
279"
CLASSIFICATION ON IMAGENET,0.6227544910179641,"ﬁne-tune with a B2 for another 25 epochs.
280"
CLASSIFICATION ON IMAGENET,0.624750499001996,"• MobileNetV2: ﬁne-tune the models for 40 epochs with MobileNetV2 as teacher and then we
281"
CLASSIFICATION ON IMAGENET,0.626746506986028,"further ﬁne-tune with a EfﬁcientNet B0 for another 40 epochs.
282"
CLASSIFICATION ON IMAGENET,0.6287425149700598,"• EfﬁcientNetV2 B0: ﬁne-tune the models for 16 epochs with B0V2 as, then ﬁne-tune the models
283"
CLASSIFICATION ON IMAGENET,0.6307385229540918,"for 16 epochs with B1V2 as teacher and ﬁnally ﬁne-tune the models for 16 epochs with B2V2 as
284"
CLASSIFICATION ON IMAGENET,0.6327345309381237,"teacher.
285"
CLASSIFICATION ON IMAGENET,0.6347305389221557,"The interesting thing we noticed is that using knowledge distillation without pruning does not help at all.
286"
CLASSIFICATION ON IMAGENET,0.6367265469061876,"For example we tried ﬁne-tuning MobileNetV2 with EfﬁcientNet B0 teacher right away and top 1 Imagenet
287"
CLASSIFICATION ON IMAGENET,0.6387225548902196,"accuracy fell from 71.52% to 71.12%. We conjecture that some kind of initial perturbation is needed for
288"
CLASSIFICATION ON IMAGENET,0.6407185628742516,"knowledge distillation to work. In our case this perturbation is channel pruning.
289"
CLASSIFICATION ON IMAGENET,0.6427145708582834,"Batch size is set to 192 for B0 and MobileNetV2 ﬁne-tuning. For B1 and EfﬁcientNetV2 B0 batch size is
290"
CLASSIFICATION ON IMAGENET,0.6447105788423154,"128. The input image resolution is (224, 224). We use only random crop and ﬂip as augmentations. For
291"
CLASSIFICATION ON IMAGENET,0.6467065868263473,"training we use one NVidia RTX3090 GPU. For the pruning phase we set the batch size to 16 and, quite
292"
CLASSIFICATION ON IMAGENET,0.6487025948103793,"importantly, we freeze all batch normalization layers. We use Adam optimizer for all the training runs.
293"
CLASSIFICATION ON IMAGENET,0.6506986027944112,"During mask-learning phase the learning rate is set to 0.0001. For ﬁne-tuning we use exponential decay with
294"
CLASSIFICATION ON IMAGENET,0.6526946107784432,"learning rate initially set to 0.0001 and the decay rate set to 0.001.
295"
COMPARISONS AND DISCUSSION,0.654690618762475,"6.2.1
COMPARISONS AND DISCUSSION
296"
COMPARISONS AND DISCUSSION,0.656686626746507,"Few authors have attempted to prune EfﬁcientNet (Tan & Le, 2019). We can compare our results with Hou
297"
COMPARISONS AND DISCUSSION,0.6586826347305389,"et al. (2021), where only one model is presented, which was also ﬁne-tuned with knowledge distillation. We
298"
COMPARISONS AND DISCUSSION,0.6606786427145709,"provide a much wider FLOPs spectrum for B0 and prune B1 as well. It is interesting to see that B1 pruned to
299"
COMPARISONS AND DISCUSSION,0.6626746506986028,"the FLOPs level of B0 outperforms B0 by a wide margin. The results are in Table 1.
300"
COMPARISONS AND DISCUSSION,0.6646706586826348,"Comparisons for MobileNetV2 are quite difﬁcult due the inconsistencies between different versions of the
301"
COMPARISONS AND DISCUSSION,0.6666666666666666,"model taken by different authors as their baseline. For instance in Hou et al. (2021) the authors ﬁrst take an
302"
COMPARISONS AND DISCUSSION,0.6686626746506986,"over-pruned backbone which they proceed to prune. In Liu et al. (2019b) the largest version of MobileNetV2
303"
COMPARISONS AND DISCUSSION,0.6706586826347305,"is taken (585M FLOPs) and then pruned. Some of the authors run the ﬁne-tuning for much longer than we do.
304"
COMPARISONS AND DISCUSSION,0.6726546906187625,"Notably, in Ye et al. (2020) the ﬁne-tuning is run on 4 GPUs with batch size 512 and for 250 epochs which is
305"
COMPARISONS AND DISCUSSION,0.6746506986027944,"considerably more expensive than our approach. Detailed results are in Table 2 and Figure 5a. Again using
306"
COMPARISONS AND DISCUSSION,0.6766467065868264,"hierarchical knowledge distillation we are able to ﬁne-tune the model pruned to 75% of original FLOPs so
307"
COMPARISONS AND DISCUSSION,0.6786427145708582,"that it has 0.7% higher accuracy than the original.
308"
COMPARISONS AND DISCUSSION,0.6806387225548902,Under review as a conference paper at ICLR 2022
COMPARISONS AND DISCUSSION,0.6826347305389222,"(a) EfﬁcientNet B0 pruned
(b) EfﬁcientNet B1 pruned"
COMPARISONS AND DISCUSSION,0.6846307385229541,"Figure 4: ImageNet accuracy of pruned EfﬁcientNet B0 and B1. Considering FLOPs/accuracy trade-off the
some pruned models are better than FBNetV2."
COMPARISONS AND DISCUSSION,0.6866267465069861,Table 2: Top 1 ImageNet accuracy and FLOPs for MobileNetV2 pruned
COMPARISONS AND DISCUSSION,0.688622754491018,"Model
Standard
training
MobileNetV2
teacher"
COMPARISONS AND DISCUSSION,0.6906187624750499,"B0 teacher
(after using
MobileNetV2 ﬁrst)
FLOPs (G)"
COMPARISONS AND DISCUSSION,0.6926147704590818,"original
71.52
-
-
0.301
m5
-
67.58
67.99
0.135
GSPE Ye et al. (2020)
68.8
-
-
0.138
m6
-
67.08
68.76
0.140
META Liu et al. (2019b)
68.2
-
-
0.140
GFP Liu et al. (2021a)
69.16
-
-
0.150
GSPE Ye et al. (2020)
69.7
-
-
0.152
m7
-
69.79
70.05
0.170
GSPE Ye et al. (2020)
70.4
-
-
0.170
m8
-
69.47
71.28
0.199
GSPE Ye et al. (2020)
71.2
-
-
0.201
GSPE Ye et al. (2020)
71.6
-
-
0.220
m9
-
70.92
72.22
0.228"
COMPARISONS AND DISCUSSION,0.6946107784431138,"When it comes to EfﬁcientNetV2, we are able to outperform the original model’s results on ImageNet with
309"
COMPARISONS AND DISCUSSION,0.6966067864271457,"the help of hierarchical EKD, inasmuch as the pruned version of B0 (70% of the FLOPs of the original
310"
COMPARISONS AND DISCUSSION,0.6986027944111777,"model) has higher top 1 accuracy than the original. See Table 3 and Figure 5b.
311"
CONCLUSION,0.7005988023952096,"7
CONCLUSION
312"
CONCLUSION,0.7025948103792415,"Using an automated solution to process coupled channels in neural network architectures and a simple
313"
CONCLUSION,0.7045908183632734,"scheme to learn channel importance, we are able to prune models with varying architectures for different
314"
CONCLUSION,0.7065868263473054,"underlying tasks. For ﬁne-tuning pruned classiﬁcation networks we use hierarchical knowledge distillation
315"
CONCLUSION,0.7085828343313373,"which produces much better results than just using the original model as a teacher. The whole pruning
316"
CONCLUSION,0.7105788423153693,"pipeline requires much less computational resources than some of the state-of-the-art NAS based solutions
317"
CONCLUSION,0.7125748502994012,"for ﬁnding efﬁcient FLOPs / accuracy trade-offs like Wang et al. (2021).
318"
CONCLUSION,0.7145708582834331,"(a) ImageNet accuracy of pruned MobileNetV2. For ﬁnetun-
ing we use knowledge-distillation with both original model
and EfﬁcientNet B0 as teachers.
(b) EfﬁcientNetV2 B0 pruned"
CONCLUSION,0.716566866267465,Figure 5: Pruning results for MobileNetV2 and EfﬁcientNetV2
CONCLUSION,0.718562874251497,Under review as a conference paper at ICLR 2022
CONCLUSION,0.720558882235529,Table 3: Top 1 ImageNet accuracy and FLOPs for EfﬁcientNetV2 B0 pruned
CONCLUSION,0.7225548902195609,"Model
Standard training
B0 teacher
hierarchical teachers
FLOPs (G)
original
78.67
-
-
0.722
m20
-
77.59
78.93
0.506
m17
-
77.25
78.37
0.431
m15
-
76.70
77.64
0.379
m12
-
75.59
76.36
0.299"
REFERENCES,0.7245508982035929,"REFERENCES
319"
REFERENCES,0.7265469061876247,"Zhiqiang Chen, Ting-Bing Xu, Changde Du, Cheng-Lin Liu, and Huiguang He. Dynamical channel pruning
320"
REFERENCES,0.7285429141716567,"by conditional accuracy change for deep neural networks. IEEE transactions on neural networks and
321"
REFERENCES,0.7305389221556886,"learning systems, 32(2):799–813, 2020.
322"
REFERENCES,0.7325349301397206,"J. Cho and B. Hariharan. On the efﬁcacy of knowledge distillation. pp. 4793–4801, nov 2019. doi: 10.
323"
REFERENCES,0.7345309381237525,"1109/ICCV.2019.00489. URL https://doi.ieeecomputersociety.org/10.1109/ICCV.
324"
REFERENCES,0.7365269461077845,"2019.00489.
325"
REFERENCES,0.7385229540918163,"Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen, and Liang Lin. Look into person: Self-supervised
326"
REFERENCES,0.7405189620758483,"structure-sensitive learning and a new benchmark for human parsing. In Proceedings of the IEEE
327"
REFERENCES,0.7425149700598802,"Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
328"
REFERENCES,0.7445109780439122,"Shaopeng Guo, Yujie Wang, Quanquan Li, and Junjie Yan. Dmcp: Differentiable markov channel pruning
329"
REFERENCES,0.7465069860279441,"for neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
330"
REFERENCES,0.7485029940119761,"Recognition, pp. 1539–1547, 2020.
331"
REFERENCES,0.7504990019960079,"Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efﬁcient dnns. arXiv preprint
332"
REFERENCES,0.7524950099800399,"arXiv:1608.04493, 2016.
333"
REFERENCES,0.7544910179640718,"Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efﬁcient
334"
REFERENCES,0.7564870259481038,"neural networks. arXiv preprint arXiv:1506.02626, 2015.
335"
REFERENCES,0.7584830339321357,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition,
336"
REFERENCES,0.7604790419161677,"2015.
337"
REFERENCES,0.7624750499001997,"Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In
338"
REFERENCES,0.7644710578842315,"Proceedings of the IEEE international conference on computer vision, pp. 1389–1397, 2017.
339"
REFERENCES,0.7664670658682635,"Charles Herrmann, Richard Strong Bowen, and Ramin Zabih. Channel selection using gumbel softmax. In
340"
REFERENCES,0.7684630738522954,"European Conference on Computer Vision, pp. 241–257. Springer, 2020.
341"
REFERENCES,0.7704590818363274,"Yuenan Hou, Zheng Ma, Chunxiao Liu, Zhe Wang, and Chen Change Loy. Network pruning via resource
342"
REFERENCES,0.7724550898203593,"reallocation. CoRR, abs/2103.01847, 2021. URL https://arxiv.org/abs/2103.01847.
343"
REFERENCES,0.7744510978043913,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint
344"
REFERENCES,0.7764471057884231,"arXiv:1611.01144, 2016.
345"
REFERENCES,0.7784431137724551,"Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for efﬁcient
346"
REFERENCES,0.780439121756487,"convnets. arXiv preprint arXiv:1608.08710, 2016.
347"
REFERENCES,0.782435129740519,"Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling Shao.
348"
REFERENCES,0.7844311377245509,"Hrank: Filter pruning using high-rank feature map.
CoRR, abs/2002.10179, 2020.
URL https:
349"
REFERENCES,0.7864271457085829,"//arxiv.org/abs/2002.10179.
350"
REFERENCES,0.7884231536926147,"Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional
351"
REFERENCES,0.7904191616766467,"neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
352"
REFERENCES,0.7924151696606786,"806–814, 2015.
353"
REFERENCES,0.7944111776447106,"Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search, 2019a.
354"
REFERENCES,0.7964071856287425,"Jiayi Liu, Samarth Tripathi, Unmesh Kurup, and Mohak Shah. Pruning algorithms to accelerate convolutional
355"
REFERENCES,0.7984031936127745,"neural networks for edge applications: A survey. arXiv preprint arXiv:2005.04275, 2020.
356"
REFERENCES,0.8003992015968064,"Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin Chen,
357"
REFERENCES,0.8023952095808383,"Wenming Yang, Qingmin Liao, and Wayne Zhang. Group ﬁsher pruning for practical network compression.
358"
REFERENCES,0.8043912175648703,"In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine
359"
REFERENCES,0.8063872255489022,"Learning, volume 139 of Proceedings of Machine Learning Research, pp. 7021–7032. PMLR, 18–24 Jul
360"
REFERENCES,0.8083832335329342,"2021a. URL https://proceedings.mlr.press/v139/liu21ab.html.
361"
REFERENCES,0.810379241516966,Under review as a conference paper at ICLR 2022
REFERENCES,0.812375249500998,"Xiangcheng Liu, Jian Cao, Hongyi Yao, Wenyu Sun, and Yuan Zhang. Adapruner: Adaptive channel pruning
362"
REFERENCES,0.8143712574850299,"and effective weights inheritance. arXiv preprint arXiv:2109.06397, 2021b.
363"
REFERENCES,0.8163672654690619,"Zechun Liu, Haoyuan Mu, X. Zhang, Zichao Guo, X. Yang, K. Cheng, and Jian Sun. Metapruning: Meta
364"
REFERENCES,0.8183632734530938,"learning for automatic neural network channel pruning. 2019 IEEE/CVF International Conference on
365"
REFERENCES,0.8203592814371258,"Computer Vision (ICCV), pp. 3295–3304, 2019b.
366"
REFERENCES,0.8223552894211577,"Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning
367"
REFERENCES,0.8243512974051896,"efﬁcient convolutional networks through network slimming. In Proceedings of the IEEE international
368"
REFERENCES,0.8263473053892215,"conference on computer vision, pp. 2736–2744, 2017.
369"
REFERENCES,0.8283433133732535,"Jian-Hao Luo, Hao Zhang, Hong-Yu Zhou, Chen-Wei Xie, Jianxin Wu, and Weiyao Lin. Thinet: pruning
370"
REFERENCES,0.8303393213572854,"cnn ﬁlters for a thinner net. IEEE transactions on pattern analysis and machine intelligence, 41(10):
371"
REFERENCES,0.8323353293413174,"2525–2538, 2018.
372"
REFERENCES,0.8343313373253493,"Seyed Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh.
373"
REFERENCES,0.8363273453093812,"Improved knowledge distillation via teacher assistant. Proceedings of the AAAI Conference on Artiﬁcial
374"
REFERENCES,0.8383233532934131,"Intelligence, 34:5191–5198, 04 2020. doi: 10.1609/aaai.v34i04.5963.
375"
REFERENCES,0.8403193612774451,"Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Tijmen
376"
REFERENCES,0.8423153692614771,"Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021.
377"
REFERENCES,0.844311377245509,"Qualcomm. Snpe: Snapdragon neural processing engine. https://developer.qualcomm.com/
378"
REFERENCES,0.846307385229541,"sites/default/files/docs/snpe/.
379"
REFERENCES,0.8483033932135728,"Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Inverted
380"
REFERENCES,0.8502994011976048,"residuals and linear bottlenecks: Mobile networks for classiﬁcation, detection and segmentation. CoRR,
381"
REFERENCES,0.8522954091816367,"abs/1801.04381, 2018. URL http://arxiv.org/abs/1801.04381.
382"
REFERENCES,0.8542914171656687,"Wenqi Shao, Hang Yu, Zhaoyang Zhang, Hang Xu, Zhenguo Li, and Ping Luo. Bwcp: Probabilistic
383"
REFERENCES,0.8562874251497006,"learning-to-prune channels for convnets via batch whitening. arXiv preprint arXiv:2105.06423, 2021.
384"
REFERENCES,0.8582834331337326,"Mennatullah Siam, Heba Mahgoub, Mohamed Zahran, Senthil Yogamani, Martin Jagersand, and Ahmad El-
385"
REFERENCES,0.8602794411177644,"Sallab. Modnet: Motion and appearance based moving object detection network for autonomous driving.
386"
REFERENCES,0.8622754491017964,"In 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pp. 2859–2864, 2018.
387"
REFERENCES,0.8642714570858283,"doi: 10.1109/ITSC.2018.8569744.
388"
REFERENCES,0.8662674650698603,"Mingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.
389"
REFERENCES,0.8682634730538922,"CoRR, abs/1905.11946, 2019. URL http://arxiv.org/abs/1905.11946.
390"
REFERENCES,0.8702594810379242,"Mingxing Tan and Quoc V. Le. Efﬁcientnetv2: Smaller models and faster training. CoRR, abs/2104.00298,
391"
REFERENCES,0.872255489021956,"2021. URL https://arxiv.org/abs/2104.00298.
392"
REFERENCES,0.874251497005988,"Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efﬁcientdet: Scalable and efﬁcient object detection. CoRR,
393"
REFERENCES,0.8762475049900199,"abs/1911.09070, 2019. URL http://arxiv.org/abs/1911.09070.
394"
REFERENCES,0.8782435129740519,"Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining Xie, Bichen Wu, Matthew
395"
REFERENCES,0.8802395209580839,"Yu, Tao Xu, Kan Chen, Peter Vajda, and Joseph E. Gonzalez. Fbnetv2: Differentiable neural architecture
396"
REFERENCES,0.8822355289421158,"search for spatial and channel dimensions. In Proceedings of the IEEE/CVF Conference on Computer
397"
REFERENCES,0.8842315369261478,"Vision and Pattern Recognition (CVPR), June 2020.
398"
REFERENCES,0.8862275449101796,"Dilin Wang, Chengyue Gong, Meng Li, Qiang Liu, and Vikas Chandra. Alphanet: Improved training of
399"
REFERENCES,0.8882235528942116,"supernet with alpha-divergence. arXiv preprint arXiv:2102.07954, 2021.
400"
REFERENCES,0.8902195608782435,"Yuzhi Wang, Haibin Huang, Qin Xu, Jiaming Liu, Yiqun Liu, and Jue Wang. Practical deep raw image
401"
REFERENCES,0.8922155688622755,"denoising on mobile devices, 2020.
402"
REFERENCES,0.8942115768463074,"Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. Snas: stochastic neural architecture search. arXiv
403"
REFERENCES,0.8962075848303394,"preprint arXiv:1812.09926, 2018.
404"
REFERENCES,0.8982035928143712,"Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnetworks
405"
REFERENCES,0.9001996007984032,"provably exist: Pruning via greedy forward selection. CoRR, abs/2003.01794, 2020. URL https:
406"
REFERENCES,0.9021956087824351,"//arxiv.org/abs/2003.01794.
407"
REFERENCES,0.9041916167664671,Under review as a conference paper at ICLR 2022
REFERENCES,0.906187624750499,"A
APPENDIX
408"
REFERENCES,0.908183632734531,"A.1
LAYER WIDTHS VISUALIZATION
409"
REFERENCES,0.9101796407185628,"It is quite interesting to see how layer width looks like after pruning. The pattern that emerge are quite
410"
REFERENCES,0.9121756487025948,"telling. EfﬁcientNets are build of a series of meta-blocks, .e.g, 2, 3, . . . , 7 in EfﬁcientNet B0, where each
411"
REFERENCES,0.9141716566866267,"meta-block consists of a number of MBCONV blocks at the same spatial resolution. It appears that in each
412"
REFERENCES,0.9161676646706587,"such meta-block the most important block is usually the ﬁrst one, and block importance decays proportionally
413"
REFERENCES,0.9181636726546906,"to the depth of the block inside the meta-block. See Figure 6 in the Appendix.
414"
REFERENCES,0.9201596806387226,"(a) EfﬁcientNet B0 pruned
(b) EfﬁcientNet B1 pruned"
REFERENCES,0.9221556886227545,"Figure 6: Visualisation of the layer width after channels are removed. There is a noticeable patter in which
the ﬁrst block in a series of residual blocks at the same spatial resolution is the most important one and the
algorithm is reluctant to remove the channels. Later blocks seem to be less informative, proportionally to
their depth."
REFERENCES,0.9241516966067864,"A.2
FURTHER RESULTS
415"
REFERENCES,0.9261477045908184,"A.2.1
RAWRGB IMAGE DENOISING
416"
REFERENCES,0.9281437125748503,"We prune a recent state-of-the-art network for RawRGB image denoising on mobile devices introduced
417"
REFERENCES,0.9301397205588823,"in Wang et al. (2020). We train the models on SIDD Medium dataset https://www.eecs.yorku.
418"
REFERENCES,0.9321357285429142,"ca/~kamel/sidd/dataset.php. We ﬁrst extract 256x256 patches for training and validation and
419"
REFERENCES,0.9341317365269461,"then test the networks on SIDD validation dataset https://www.eecs.yorku.ca/~kamel/sidd/
420"
REFERENCES,0.936127744510978,"benchmark.php. The batch size is set to 16, learning rate is 0.0001 and we use Adam optimizer. The
421"
REFERENCES,0.93812375249501,"loss is mean absolute error. We train the original model for 150 epochs, prune it and then train the original
422"
REFERENCES,0.9401197604790419,"model for another 150 epochs. The pruned models are ﬁne-tuned for 150 epochs as well. For comparison we
423"
REFERENCES,0.9421157684630739,"also train from scratch smaller (linearly scaled down) versions of the original model. The results can be seen
424"
REFERENCES,0.9441117764471058,"in Table 4 and Figure 7.
425"
REFERENCES,0.9461077844311377,"A.2.2
HUMAN SEGMENTATION
426"
REFERENCES,0.9481037924151696,"For semantic segmentation we use a private dataset for training human segmentation models for real time
427"
REFERENCES,0.9500998003992016,"prediction in video bokeh task. This is dictated by the need to have superior edge quality which is missing
428"
REFERENCES,0.9520958083832335,"in publicly available data for segmentation. The dataset consists of 120k real image/mask pair and 50k
429"
REFERENCES,0.9540918163672655,"synthetic ones. Apart from IoU we also compute edge IoU, which pays attention only to the edges of the
430"
REFERENCES,0.9560878243512974,"masks and can be thought of as a proxy for edge quality. The baseline architecture consists of an EfﬁcientNet
431"
REFERENCES,0.9580838323353293,Table 4: Pruning results for image denoising and human segmentation.
REFERENCES,0.9600798403193613,"(a) PSNR and kFPP for pruned PM-
RID model"
REFERENCES,0.9620758483033932,"Model
PSNR
kFPP
baseline
51.84
29.9
m25
51.84
25.4
m22
51.79
22.1
m20
51.80
19.6
m15
51.67
15.1
m12
51.41
12.3"
REFERENCES,0.9640718562874252,(b) Pruning results for pruned human segmentation model.
REFERENCES,0.9660678642714571,"Model
IoU
Edge IoU
kFPP (G)
baseline
0.9414
0.4039
40.8
m30
0.9440
0.3977
30.3
m25
0.9423
0.3848
25.4
m22
0.9431
0.3816
22
m19
0.9420
0.3574
19.3
m15
0.9372
0.3354
14.6
m11
0.9295
3213
11.3
m8
0.9253
0.2939
7.5
m5
0.9050
0.2265
4.5"
REFERENCES,0.9680638722554891,Under review as a conference paper at ICLR 2022
REFERENCES,0.9700598802395209,Figure 7: Validation results for pruned RawRGB denoising models.
REFERENCES,0.9720558882235529,"(a) IoU
(b) Edge IoU"
REFERENCES,0.9740518962075848,Figure 8: Validation results for pruned human segmentation.
REFERENCES,0.9760479041916168,"B0 (Tan & Le, 2019) backbone, EfﬁcientDet (Tan et al., 2019) (modiﬁed slightly to allow for easier channel
432"
REFERENCES,0.9780439121756487,"pruning) fusion block and a detail branch (Siam et al., 2018) to preserve edge quality. The backbone network
433"
REFERENCES,0.9800399201596807,"is pretrained on ImageNet. We train the original model for 70 epochs, prune and then ﬁne-tune the pruned
434"
REFERENCES,0.9820359281437125,"models for 50 epochs. The validation results are presented in Table 4. The validation dataset is a split of a
435"
REFERENCES,0.9840319361277445,"modiﬁed version of LIP dataset (Gong et al., 2017), where objects belonging to people (such as handbags,
436"
REFERENCES,0.9860279441117764,"etc.) are also considered part of these people. This is done, so that we can train models for video bokeh
437"
REFERENCES,0.9880239520958084,"effect. The results are in Table 4b and are visualized in Figures 8a and 8b.
438"
REFERENCES,0.9900199600798403,"Notice that the smallest pruned model is compressed to around 10% of the size of the original one. Even in
439"
REFERENCES,0.9920159680638723,"these extreme compression scenario our approach produces a model with IoU higher than 90%. IoU starts
440"
REFERENCES,0.9940119760479041,"dropping only after we have removed more than 60% of the original FLOPs. This is an observation which, in
441"
REFERENCES,0.9960079840319361,"our experience, is true for many more architectures for segmentation, the one being presented here is just
442"
REFERENCES,0.998003992015968,"one example. Edge IoU starts falling much more quickly, perhaps beacause we employ no edge-speciﬁc loss.
443"
