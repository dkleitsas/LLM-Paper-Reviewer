Section,Section Appearance Order,Paragraph
DEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE,0.0,"1Department of Electrical Engineering and Computer Science
2Department of Statistics
University of Michigan
Ann Arbor, MI 48109, USA
{yutongw,clayscot}@umich.edu"
ABSTRACT,0.0023752969121140144,ABSTRACT
ABSTRACT,0.004750593824228029,"Vapnik-Chervonenkis (VC) theory has so far been unable to explain the small
generalization error of overparametrized neural networks. Indeed, existing appli-
cations of VC theory to large networks obtain upper bounds on VC dimension
that are proportional to the number of weights, and for a large class of networks,
these upper bound are known to be tight. In this work, we focus on a subclass of
partially quantized networks that we refer to as hyperplane arrangement neural
networks (HANNs). Using a sample compression analysis, we show that HANNs
can have VC dimension signiﬁcantly smaller than the number of weights, while
being highly expressive. In particular, empirical risk minimization over HANNs
in the overparametrized regime achieves the minimax rate for classiﬁcation with
Lipschitz posterior class probability. We further demonstrate the expressivity of
HANNs empirically. On a panel of 121 UCI datasets, overparametrized HANNs
match the performance of state-of-the-art full-precision models."
INTRODUCTION,0.007125890736342043,"1
INTRODUCTION"
INTRODUCTION,0.009501187648456057,"Neural networks have become an indispensable tool for machine learning practitioners, owing to
their impressive performance especially in vision and natural language processing (Goodfellow
et al., 2016). In practice, neural networks are often applied in the overparametrized regime and
are capable of ﬁtting even random labels (Zhang et al., 2021). Evidently, these overparametrized
models perform well on real world data despite their ability to grossly overﬁt, a phenomenon that
has been dubbed “the generalization puzzle” (Nagarajan & Kolter, 2019)."
INTRODUCTION,0.011876484560570071,"Toward solving this puzzle, several research directions have ﬂourished and offer potential expla-
nations, including implicit regularization (Chizat & Bach, 2020), interpolation (Chatterji & Long,
2021), and benign overﬁtting (Bartlett et al., 2020). So far, VC theory has not been able to explain
the puzzle, because existing bounds on the VC dimensions of neural networks are on the order of
the number of weights (Maass, 1994; Bartlett et al., 2019). It remains unknown whether there exist
neural network architectures capable of modeling rich set of classﬁers with low VC dimension."
INTRODUCTION,0.014251781472684086,"The focus of this work is on a class of neural networks with threshold activation that we refer to
as hyperplane arrangement neural networks (HANNs). Using the theory of sample compression
schemes (Littlestone & Warmuth, 1986), we show that HANNs can have VC dimension that is
signiﬁcantly smaller than the number of parameters. Furthermore, we apply this result to show that
HANNs have high expressivity by proving that HANN classiﬁers achieve minimax-optimality when
the data has Lipschitz posterior class probability in an overparametrized setting."
INTRODUCTION,0.0166270783847981,"We benchmark the empirical performance of HANNs on a panel of 121 UCI datasets, following
several recent neural network and neural tangent kernel works (Klambauer et al., 2017; Wu et al.,
2018; Arora et al., 2019; Shankar et al., 2020). In particular, Klambauer et al. (2017) showed
that, using a properly chosen activation, overparametrized neural networks perform competitively
compared to classical shallow methods on this panel of datasets. Our experiments show that HANNs,
a partially-quantized model, match the classiﬁcation accuracy of the self-normalizing neural network"
INTRODUCTION,0.019002375296912115,Published as a conference paper at ICLR 2022
INTRODUCTION,0.021377672209026127,"(Klambauer et al., 2017) and the dendritic neural network (Wu et al., 2018), both of which are full-
precision models."
RELATED WORK,0.023752969121140142,"1.1
RELATED WORK"
RELATED WORK,0.026128266033254157,"VC dimensions of neural networks. The VC dimension (Vapnik & Chervonenkis, 1971) is a com-
binatorial measure of the complexity of a concept class, i.e., a set of classiﬁers. The Fundamental
Theorem of Statistical Learning (Shalev-Shwartz & Ben-David, 2014, Theorem 6.8) states that a
concept class has ﬁnite VC-dimension if and only if it is probably approximately correct (PAC)
learnable, where the VC-dimension is tightly related to the number of samples required for PAC
learning."
RELATED WORK,0.028503562945368172,"For threshold networks, Cover (1965); Baum & Haussler (1989) showed a VC-dimension upper
bounded of O(w log w), where w is the number of parameters. Maass (1994) obtained a matching
lower bound attained by a network architecture with two hidden layers. More recently, Bartlett et al.
(2019) obtained the upper and lower bounds O(wℓlog w) and Ω(wℓlog(w/ℓ)) respectively for the
case when the activation is piecewise linear, where ℓis the number of layers. These lower bounds are
achieved by somewhat unconventional network architectures. The architectures we consider exclude
these and thus we are able to achieve a smaller upper bound on the VC dimensions."
RELATED WORK,0.030878859857482184,"The generalization puzzle. In practice, neural networks that achieve state-of-the-art performance
use signiﬁcantly more parameters than samples, a phenomenon that cannot be explained by classical
VC theory if the VC dimension ≥number of weights. This has been dubbed the generalization
puzzle (Zhang et al., 2021). To explain the puzzle, researchers have pursued new directions including
margin-based bounds (Neyshabur et al., 2017; Bartlett et al., 2017), PAC-Bayes bounds (Dziugaite
& Roy, 2017), and implicit bias of optimization methods (Gunasekar et al., 2018; Chizat & Bach,
2020). We refer the reader to the recent article by Bartlett et al. (2021) for a comprehensive coverage
of this growing literature."
RELATED WORK,0.0332541567695962,"The generalization puzzle is not speciﬁc to deep learning. For instance, AdaBoost has been ob-
served to continue to decrease the test error while the VC dimension grows linearly with the number
of boosting rounds (Schapire, 2013). Other learning algorithms that exhibits similarly surprising
behavior include random forests (Wyner et al., 2017) and kernel methods (Belkin et al., 2018)."
RELATED WORK,0.035629453681710214,"Minimax-optimality. Whereas VC theory is distribution-independent, minimax theory is concerned
with the question of optimal estimation/classiﬁcation under distributional assumptions1."
RELATED WORK,0.03800475059382423,"A minimax optimality result shows that the expected excess classiﬁcation error goes to zero at the
fastest rate possible, as the sample size tend to inﬁnity. For neural networks, this often involves a
hyperparameter selection scheme in terms of the sample size."
RELATED WORK,0.040380047505938245,"Farag´o & Lugosi (1993) show minimax-optimality of (underparametrized) neural networks for
learning to classify under certain assumptions on the Fourier transform of the data distribution.
Schmidt-Hieber (2020) shows minimax-optimality of s-sparse neural networks for regression over
H¨older classes, where at most s = O(n log n) network weights are nonzero, and n = the number
of training samples. Kim et al. (2021) extends the results of Schmidt-Hieber (2020) to the classi-
ﬁcation setting, remarking that effective optimization under sparsity constraint is lacking. Kohler
& Langer (2020) and Langer (2021) proved minimax-optimality without the sparsity assumption,
however in an underparametrized setting. To the best of our knowledge, our result is the ﬁrst to
establish minimax optimality of overparametrized neural networks without a sparsity assumption."
RELATED WORK,0.04275534441805225,"(Partially) quantized neural networks. Quantizing some of the weights and/or activations of neu-
ral networks has the potential to reduce the high computational burden of neural networks at test time
(Qin et al., 2020). Many works have focused on the efﬁcient training of quantized neural networks
to close the performance gap with full-precision architectures (Hubara et al., 2017; Rastegari et al.,
2016; Lin et al., 2017). Several works have observed that quantization of the activations, rather than
of the weights, leads to a larger accuracy gap (Cai et al., 2017; Mishra et al., 2018; Kim et al., 2019)."
RELATED WORK,0.04513064133016627,"Towards explaining this phenomenon, researchers have focused on understanding the so-called
coarse gradient, a term coined by Yin et al. (2019), often used in training QNNs as a surrogate"
RELATED WORK,0.047505938242280284,"1The No-Free-Lunch Theorem (Devroye, 1982) implies that no classiﬁer can be minimax optimal without
distributional assumption."
RELATED WORK,0.0498812351543943,Published as a conference paper at ICLR 2022
RELATED WORK,0.052256532066508314,"for the usual gradient. One commonly used heuristic is the straight-through-estimator (STE) ﬁrst
introduced in an online course by Hinton et al. (2012). Theory supporting the STE heuristic has
recently been studied in Li et al. (2017) and Yin et al. (2019)."
RELATED WORK,0.05463182897862233,"QNNs have also been analyzed from other theoretical angles, including mean-ﬁeld theory (Blumen-
feld et al., 2019), memory capacity (Vershynin, 2020), Boolean function representation capacity
(Baldi & Vershynin, 2019) and adversarial robustness (Lin et al., 2018). Of particular relevance to
our work, Maass (1994) constructed an example of a QNN architecture with VC dimension on the
order of the number of weights in the network. In contrast, our work shows that there exist QNN
architectures with much smaller VC dimensions."
RELATED WORK,0.057007125890736345,"Sample compression schemes. Many concept classes with geometrically structured decision re-
gions, such as axis-parallel rectangles, can be trained on a properly chosen size σ subset of an
arbitrarily large training dataset without affecting the result. Such a concept class is said to admit
a sample compression schemes of size σ, a notion introduced by Littlestone & Warmuth (1986)
who showed that the VC dimension of the class is upper bounded by O(σ). Furthermore, the au-
thors posed the Sample Compression Conjecture. See Moran & Yehudayoff (2016) for the best
known partial result and an extensive review of research in this area. Besides the conjecture, sample
compression schemes have also been applied to other long-standing problems in learning theory
(Hanneke et al., 2019; Bousquet et al., 2020; Ashtiani et al., 2020). To the best of our knowledge,
our work is the ﬁrst to apply sample compression schemes to neural networks."
NOTATIONS,0.05938242280285035,"2
NOTATIONS"
NOTATIONS,0.06175771971496437,"The set of real numbers is denoted R. The unit interval is denoted [0, 1]. For an integer k ≥1, let
[k] = {1, . . . , k}. We use X to denote the feature space, which in this work will either be Rd or
[0, 1]d where d ≥1 is the ambient dimension/number of features."
NOTATIONS,0.06413301662707839,"Denote by I{input} the indicator function which returns 1 if input is true and 0 otherwise. The
sign function is given by σsgn(t) = I{t ≥0}−I{t < 0}. For vector inputs, σsgn applies entry-wise."
NOTATIONS,0.0665083135391924,"The set of labels for binary classiﬁcation is denoted B := {±1}. Joint distributions on X × B are
denoted by P, where X, Y ∼P denotes a random instance-label pair distributed according to P. Let
f : X →B be a binary classiﬁer. The risk with respect to P is denoted by RP (f) := P(f(X) ̸= Y ).
For an integer n ≥1, the empirical risk is the random variable ˆRP,n(f) := 1"
NOTATIONS,0.0688836104513064,"n
Pn
i=1 I{f(Xi) ̸= Yi},
where (X1, Y1), . . . , (Xn, Yn) ∼P are i.i.d. The Bayes risk inff:X→B RP (f) with respect to P is
denoted by R∗
P ."
NOTATIONS,0.07125890736342043,"Let f, g : {1, 2, . . . } →R≥0 be nonnegative functions on the natural numbers. We write f ≍g if
there exists α, β > 0 such that for all n = 1, 2, . . . we have αg(n) ≤f(n) ≤βg(n)."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.07363420427553444,"3
HYPERPLANE ARRANGEMENT NEURAL NETWORKS"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.07600950118764846,"A hyperplane H in Rd is speciﬁed by its normal vector w ∈Rd and bias b ∈R. The mapping
x 7→σsgn(w⊤x + b) indicates the side of H that x lies on, and hence induces a partition of Rd
into two halfspaces. A set of k ≥1 hyperplanes is referred to as a k-hyperplane arrangement, and
speciﬁed by a matrix of normal vectors and a vector of offsets:"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.07838479809976247,"W = [w1 · · · wk] ∈Rd×k
and
b = [b1, . . . , bk]⊤."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.08076009501187649,"Let qW,b(x) := σsgn(W⊤x + b) for all x ∈Rd. The vector qW,b(x) ∈Bk is called a sign vector
and the set of all realizable sign vectors is denoted SW,b := {qW,b(x) : x ∈Rd}. Each sign vector
s ∈SW,b uniquely deﬁnes a set {x ∈Rd : qW,b(x) = s} known as a cell of the hyperplane
arrangement. The set of all cells forms a partition of Rd. For an example, see Figure 1-left."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.0831353919239905,"A classical result in the theory of hyperplane arrangement due to Buck (1943) gives the following
tight upper bound on the number of distinct sign patterns/cells:"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.0855106888361045,"|SW,b| ≤
 k
≤d"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.08788598574821853,"
:=
2k
: k < d,
 k
0

+
 k
1

+ · · · +
 k
d

: k ≥d.
(1)"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.09026128266033254,Published as a conference paper at ICLR 2022
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.09263657957244656,"See Fukuda (2015) Theorem 10.1 for a simple proof. A hyperplane arrangement classiﬁer assigns
a binary label y ∈B to a point x ∈Rd solely based on the sign vector qW,b(x). H1 H2 H3 +++"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.09501187648456057,"-++
--+ +-+ ++- +-- -+-"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.09738717339667459,"B1B2B3
Y
+ + +
+
+ + −
+
+ −+
−
+ −−
+
−+ +
−
−+ −
+
−−+
− + -
- - + + +"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.0997624703087886,"Figure 1: Left: An arrangement of 3 hyperplanes {H1, H2, H3} in R2. There are 7 sign patterns.
Middle: An example of a lookup table (see Remark 3.2). Right: the resulting classiﬁer."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.1021377672209026,"Deﬁnition 3.1. Let BX be the set of all functions from X to B. A concept class C over X is a subset
of BX . Fix r, k positive integers, r ≤min{d, k}. Let Boolk be the set of all Boolean functions
Bk →B. The hyperplane arrangement classiﬁer class is the concept class, denoted HAC(d, r, k),
over Rd deﬁned by"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.10451306413301663,"HAC(d, r, k) = {h ◦qW,b : h ∈Boolk, qW,b(x) := σsgn(W⊤x + b),"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.10688836104513064,"W ∈Rd×k, rank(W) ≤r, b ∈Rk}."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.10926365795724466,"See Figure 2 for a graphical representation of HAC(d, r, k). When the set of Boolean functions is
realized by a neural network, we refer to the resulting classiﬁer as a hyperplane arrangement neural
network (HANN).
Remark 3.2. Consider a ﬁxed hyperplane arrangement W, b and Boolean function h ∈Boolk.
When performing prediction with the classifer h ◦qW,b, the feature vector x is mapped to a sign
vector to which h is applied. Thus, we do not need to know how h behaves outside of SW,b. The
restriction of h to SW,b is a partially deﬁned Boolean function or a lookup table. X1 X2 X3 X4 B1 B2"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.11163895486935867,"B3
Input Rd"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.11401425178147269,Latent Rr
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.1163895486935867,"Boolean Bk
h : Bk →B"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.1187648456057007,"B1B2B3
Y
−−−
−
−−+
+
−+ −
+
...
...
+ + +
−"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.12114014251781473,Output Y hθ
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.12351543942992874,Activations
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.12589073634204276,Linear
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.12826603325415678,Threshold
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.13064133016627077,"Figure 2: The HAC(d, r, k) concept class as a neural network where d = 4, r = 2 and k = 3. The
Boolean function h is realized as a neural network hθ."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.1330166270783848,"Remark 3.3. The hidden layer of width r in Figure 2 allows the user to impose the restriction that the
hyperplane arrangement classiﬁer depends only on r relevant features, which can be either learned
or deﬁned by data preprocessing. When r = d, no restriction is imposed. In this case, the input
layer is directly connected to the Boolean layer. This is consistent with Deﬁnition 3.1 where the
rank constraint rank(W) ≤r becomes trivial."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.13539192399049882,"Our next goal is to upper bound the VC dimension of HAC(d, r, k)."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.1377672209026128,"Deﬁnition 3.4 (VC-dimension). Let C ⊆BX be a concept class over X. A set S := {x1, . . . , xn} ⊆
X is shattered by C if for all sequences (y1, . . . , yn) ∈Bn, there exists f ∈C such that f(xi) = yi
for all i ∈[n]. The VC-dimension of C is deﬁned as"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.14014251781472684,"VC(C) = sup{|S| : S ⊆X, S is shattered by C}."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.14251781472684086,Published as a conference paper at ICLR 2022
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.14489311163895488,"The VC-dimension has many far-reaching consequences in learning theory and, in particular, clas-
siﬁcation. One of these consequences is a sufﬁcient (in fact also necessary) condition for uniform
convergence in the sense of the following well-known theorem. See Shalev-Shwartz & Ben-David
(2014) Theorem 6.8.
Theorem 3.5. Let C be a concept class over X. There exists a constant C > 0 such that for all joint
distributions P on X ×B and all f ∈C, we have | ˆRP,n(f)−RP (f)| ≤C
p"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.14726840855106887,"(VC(C) + log(1/δ))/n
with probability at least 1 −δ with respect to the draw of (X1, Y1), . . . , (Xn, Yn)."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.1496437054631829,"The above VC bound is useless in the overparametrized setting if VC(C) = Θ(# of weights) because
# of weights > n and therefore the VC bound does not vanish. We now present our main result:
an upper bound on the VC dimension of HAC(d, r, k).
Theorem 3.6. Let d, r, k ≥1 be integers and HAC(d, r, k) be deﬁned as in Deﬁnition 3.1. Then"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.15201900237529692,"VC(HAC(d, r, k)) ≤8 ·

k(d + 1) + k(d + 1)(1 + ⌈log2 k⌉) +
 k
≤r 
."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.1543942992874109,"In the next section, we will prove this result using a sample compression scheme. Before proceeding,
we comment on the signiﬁcance of the result.
Remark 3.7. Since
  k
≤r

= O(kr), we have VC(HAC(d, r, k)) = O(kr + dk log k) which only
involves the input dimension d and the width of the ﬁrst two hidden layers r and k. For constant d
and r ≥2, this reduces to VC(HAC(d, r, k)) = O(kr). In particular, the number of weights used by
an architecture to implement the Boolean function h does not affect the VC dimension at all and can
be even inﬁnitely wide such as in Neural Tangent Kernels (Jacot et al., 2018)."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.15676959619952494,"For instance, Mukherjee & Basu (2017) Lemma 2.1 states that a 1-hidden layer neural network with
ReLU activation can model any k-input Boolean function if the hidden layer has width ≥2k. Note
that this network uses ≥k2k weights, and k2k ≫kr for ﬁxed r and k large."
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.15914489311163896,"Baldi & Vershynin (2019) study implementation of Boolean functions using threshold networks. A
consequence of their Theorem 9.3 is that a 2-hidden layer network with widths ≥c2k/2/
√"
HYPERPLANE ARRANGEMENT NEURAL NETWORKS,0.16152019002375298,"k can
implement all k input Boolean functions, where c is a constant not depending on k. This requires
≥c22k/k weights which again is exponentially larger than kr. Furthermore, this lower bound on
the weights is also necessary as k →∞.
Remark 3.8. Using the memorization approach of Vershynin (2020); Rajput et al. (2021), it is also
possible to implement HAC(d, r, k) with far fewer weights than in Remark 3.7. This can be done by
applying Theorem 1 of Rajput et al. (2021) to memorize the
  k
≤r

distinct sign vectors in Rk."
A SAMPLE COMPRESSION SCHEME,0.16389548693586697,"4
A SAMPLE COMPRESSION SCHEME"
A SAMPLE COMPRESSION SCHEME,0.166270783847981,"In this section, we will construct a sample compression scheme for HAC(d, r, k). As alluded to in the
Related Work section, the size of a sample compression scheme upper bounds the VC-dimension of
a concept class, which will be applied to prove Theorem 3.6. We ﬁrst recall the deﬁnition of sample
compression schemes with side information introduced in Littlestone & Warmuth (1986).
Deﬁnition 4.1. Let C be a concept class. A length n sequence {(xi, yi) ∈X ×B}i∈[n] is C-labelled
if there exists f ∈C such that f(xi) = yi for all i ∈[n]. Denote by LC(n) the set of C-labelled
sequences of length at most n. Denote by LC(∞) the set of all C-labelled sequences of ﬁnite length.
The concept class C over X has an m-sample compression scheme with s-bits of side information if
there exists a pair of maps (ρ, κ) where"
A SAMPLE COMPRESSION SCHEME,0.16864608076009502,"κ : LC(∞) →LC(m) × Bs,
ρ : LC(m) × Bs →BX"
A SAMPLE COMPRESSION SCHEME,0.171021377672209,"such that for all C-labelled sequences S := {(xi, yi)}i∈[n], we have ρ(κ(S))(xi) = yi for all i ∈[n].
The size of the sample compression scheme is size(ρ, κ) := m + s."
A SAMPLE COMPRESSION SCHEME,0.17339667458432304,"Intuitively, κ and ρ can be thought of as the compression and the reconstruction maps, respectively.
The compression map κ keeps m elements from the training set and s bits of additional information,
which ρ uses to reconstruct a classiﬁer that correctly labels the uncompressed training set."
A SAMPLE COMPRESSION SCHEME,0.17577197149643706,The main result of this section is:
A SAMPLE COMPRESSION SCHEME,0.17814726840855108,Published as a conference paper at ICLR 2022
A SAMPLE COMPRESSION SCHEME,0.18052256532066507,"Theorem 4.2. HAC(d, r, k) has a sample compression scheme (ρ, κ) of size"
A SAMPLE COMPRESSION SCHEME,0.1828978622327791,"size(ρ, κ) = k(d + 1) + k(d + 1)(1 + ⌈log2 k⌉) +
 k
≤r 
."
A SAMPLE COMPRESSION SCHEME,0.18527315914489312,"We remark that both the hyperplane arrangement (W, b) and the Boolean function h contribute to the
number of parameters/weights. The rest of this section will work toward the proof of Theorem 4.2.
The following result states that a C-labelled sequence can be labelled by a hyperplane arrangement
classiﬁer of a special form.
Proposition 4.3. Let {(xi, yi)}i∈[n] be HAC(d, r, k)-labelled. Then there exist V = [v1 · · · vk] ∈
Rd×k, c ∈Rk and h ∈Boolk such that for all i ∈[n], we have 1) yi = h(σsgn(V⊤xi + c)), 2)
rank(V) ≤r and 3) |v⊤
j xi + cj| ≥1 for all i ∈[n], j ∈[k]."
A SAMPLE COMPRESSION SCHEME,0.1876484560570071,"The proof, given in Appendix A.1, is similar to showing the existence of a max-margin separating
hyperplane for a linearly separable dataset.
Deﬁnition 4.4. Let I be a ﬁnite set and let ai ∈Rn for each i ∈I. Let A = {ai}i∈I. A
conical combination of A is a linear combination P
i∈I λiai where the weights λi ∈R≥0 are
nonnegative. The conical hull of A, denoted coni(A), is the set of all conical combinations of A,
i.e., coni({ai}i∈I) :=
P"
A SAMPLE COMPRESSION SCHEME,0.19002375296912113,"i∈I λiai : λi ∈R≥0, ∀i ∈I
	
."
A SAMPLE COMPRESSION SCHEME,0.19239904988123516,"The result below follows easily from the Carath´edory’s theorem for the conical hull (Lov´asz &
Plummer, 2009). For the sake of completeness, we included the proof in Appendix A.2.
Proposition 4.5. Let a1, . . . , am ∈Rn and b1, . . . , bm ∈R. For each subset I ⊆[m], deﬁne
PI := {x ∈Rn : a⊤
i x ≤bi ∀i ∈I}. Suppose that P[m] is nonempty. Then 1) minx∈PI
1
2∥x∥2"
A SAMPLE COMPRESSION SCHEME,0.19477434679334918,"has a unique minimizer, denoted by x∗
I below, and 2) there exists a subset J ⊆[m] such that
|J| = min{m, n} and for all I ⊆[m] with J ⊆I, we have x∗
[m] = x∗
I."
A SAMPLE COMPRESSION SCHEME,0.19714964370546317,"Proof of Theorem 4.2. Let (xi, yi) be HAC(d, r, k)-realizable, and V, c and h be as in Proposi-
tion 4.3. For each i ∈[n], deﬁne the Boolean vectors si := σsgn(V⊤xi + c) ∈{±1}k and
sij = σsgn(v⊤
j xi + cj) denote the j-th entry of si. Note that sij(v⊤
j xi + cj) = |v⊤
j xi + cj| ≥1."
A SAMPLE COMPRESSION SCHEME,0.1995249406175772,We ﬁrst outline the steps of the proof:
A SAMPLE COMPRESSION SCHEME,0.20190023752969122,"1. Using a subset of the samples {(xiℓ, yiℓ) : ℓ∈[d(k +1)]} with additional k(d+1)(1+⌈log2 k⌉)
bits of side information {(siℓjℓ, jℓ) : ℓ∈[d(k + 1)]}, we can reconstruct W, b such that"
A SAMPLE COMPRESSION SCHEME,0.2042755344418052,"σsgn(W
⊤xi + b) = si for all i ∈[n]."
A SAMPLE COMPRESSION SCHEME,0.20665083135391923,"2. Using an additional subset of samples {(xιℓ, yιℓ) : ℓ= 1, . . . ,
  k
≤r

} in conjunction with the
W, b reconstructed in the previous step, we can ﬁnd g ∈Boolk such that g(si) = h(si) for all i."
A SAMPLE COMPRESSION SCHEME,0.20902612826603326,"Now, consider the set"
A SAMPLE COMPRESSION SCHEME,0.21140142517814728,"P :=

(W, b) ∈Rd×k × Rk : sij(w⊤
j xi + bj) ≥1, ∀i ∈[n], j ∈[k]
	
."
A SAMPLE COMPRESSION SCHEME,0.21377672209026127,"Note that P is a convex polyhedron in (d + 1)k-dimensional space. Let (W, b) be the minimum
norm element of P. Note that σsgn(W
⊤xi + b) = σsgn(V⊤xi + c) = si by construction."
A SAMPLE COMPRESSION SCHEME,0.2161520190023753,"By Proposition 4.5, there exists a set of tuples"
A SAMPLE COMPRESSION SCHEME,0.21852731591448932,"{(iℓ, jℓ)}ℓ=1,...,(d+1)k , where (iℓ, jℓ) ∈[n] × [k]"
A SAMPLE COMPRESSION SCHEME,0.2209026128266033,"such that W, b is also the minimum norm element of"
A SAMPLE COMPRESSION SCHEME,0.22327790973871733,"P′ :=

(W, b) ∈Rd×k × Rk : siℓjℓ(w⊤
jℓxiℓ+ bjℓ) ≥1, ℓ= 1, . . . , d(k + 1)
	
."
A SAMPLE COMPRESSION SCHEME,0.22565320665083136,"To encode the deﬁning equations of P′, we need to store"
A SAMPLE COMPRESSION SCHEME,0.22802850356294538,"samples {(xiℓ, yiℓ)}d(k+1)
ℓ=1
and side information {(siℓjℓ, jℓ)}d(k+1)
ℓ=1
.
(2)"
A SAMPLE COMPRESSION SCHEME,0.23040380047505937,Published as a conference paper at ICLR 2022
A SAMPLE COMPRESSION SCHEME,0.2327790973871734,"Note that each siℓjℓrequires 1 bit while each jℓ∈[k] requires ⌈log2 k⌉bits. In total, encoding P′
requires storing d(k + 1) samples and d(k + 1)(1 + ⌈log2 k⌉) of bits."
A SAMPLE COMPRESSION SCHEME,0.23515439429928742,"To reconstruct g ∈Boolk that agrees with h on all the samples, it sufﬁces to know h when restricted
to {si}n
i=1. Since {si}n
i=1 is a subset of SW,b, we have by eq. (1) that |{si}n
i | ≤
  k
≤r

. Thus,"
A SAMPLE COMPRESSION SCHEME,0.2375296912114014,"{si}n
i=1 has at most
  k
≤r

unique elements. Let
n
sιℓ: ℓ= 1, . . . ,
  k
≤r
o
be a set containing all such
unique elements. Thus, we store"
A SAMPLE COMPRESSION SCHEME,0.23990498812351543,"samples {(xιℓ, yιℓ) : ℓ= 1, . . . ,
  k
≤r

}.
(3)"
A SAMPLE COMPRESSION SCHEME,0.24228028503562946,"Using W, b as deﬁned above, we have sιℓ= σsgn(W
⊤xιℓ+ b). Now, simply choose g such that
g(sιℓ) = yιℓfor all ℓ= 1, . . . ,
  k
≤r

."
A SAMPLE COMPRESSION SCHEME,0.24465558194774348,"To summarize, we formally deﬁne the compression and reconstruction functions (κ, ρ). Let κ take
the full sample {(xi, yi)}n
i=1 and output the subsample (and side information) in eq. (2) and eq. (3).
The reconstruction function ρ ﬁrst constructs W, b using eq. (2). Next, ρ constructs g using W, b
and the samples of eq. (3)."
A SAMPLE COMPRESSION SCHEME,0.24703087885985747,"Now, the following result together with the sample compression scheme for HAC(d, r, k) we con-
structed imply Theorem 3.6 from the previous section."
A SAMPLE COMPRESSION SCHEME,0.2494061757719715,"Theorem 4.6 (Littlestone & Warmuth (1986)). If C has sample compression scheme (ρ, κ), then
VC(C) ≤8 · size(ρ, κ)."
A SAMPLE COMPRESSION SCHEME,0.2517814726840855,"Remark 4.7. Note that the reconstruction function ρ is not permutation-invariant. Furthermore, the
overall sample compression scheme ρ, κ is not stable in the sense of Hanneke & Kontorovich (2021).
In general, sample compression schemes with permutation-invariant ρ (Floyd & Warmuth, 1995) and
stable sample compression schemes (Hanneke & Kontorovich, 2021) enjoy tighter generalization
bounds compared to ordinary sample compression schemes. We leave as an open question whether
HAC(d, r, k) has such specialized compression schemes."
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.25415676959619954,"5
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS"
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.25653206650831356,"In this section, we show that empirical risk minimization (ERM) with respect to the 0-1 loss over
HAC(d, r, k), for properly chosen r and k, is minimax optimal for classiﬁcation where the posterior
class probability function is L-Lipschitz, for ﬁxed L > 0. Furthermore, the choices for r and k is
such that the associated HANN, the neural network realization of HAC(d, r, k), is overparametrized
for the Boolean function implementations discussed in Remark 3.7."
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.2589073634204275,"Below, let X ∈[0, 1]d and Y ∈B be the random variables corresponding to a sample and label
jointly distributed according to P. Write ηP (x) := P(Y = 1|X = x) for the posterior class
probability function."
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.26128266033254155,"Let Σ(L, [0, 1]d) denote the class of L-Lipschitz functions f : [0, 1]d →R, i.e.,"
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.26365795724465557,"|f(x) −f(x′)| ≤L∥x −x′∥2,
∀x, x′ ∈[0, 1]d."
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.2660332541567696,The following minimax lower bound result2 concerns classiﬁcation when ηP is L-Lipschitz:
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.2684085510688836,Theorem 5.1 (Audibert & Tsybakov (2007)). There exists a constant C > 0 such that
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.27078384798099764,"inf
˜
fn
sup
P : ηP ∈Σ(L,[0,1]d)
E[R( ˜fn)] −R∗
P ≥Cn−
1
d+2 ."
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.27315914489311166,"The inﬁmum above is taken over all possible learning algorithms ˜fn, i.e., mappings from (X ×
B)n to Borel measurable functions X →B. When ˆfn is an empirical risk minimizer (ERM) over
HAC(d, r, k) where d = r for k = n
1
d+2 , this minimax rate is achieved."
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.2755344418052256,"2The result we cite here is a special case of (Audibert & Tsybakov, 2007, Theorem 3.5), which gives
minimax lower bound for when ηP has additional smoothness assumptions."
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.27790973871733965,Published as a conference paper at ICLR 2022
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.28028503562945367,"Theorem 5.2. Let d ≥1 be ﬁxed. Let ˆfn be an ERM over HAC(d, d, k) where k = k(n) ≍n
1
d+1 .
Then there exists a constant C′ such that"
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.2826603325415677,"sup
P : ηP ∈Σ(L,[0,1]d)
E[R( ˆfn)] −R∗
P ≤C′n−
1
d+2 ."
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.2850356294536817,"Proof sketch (see Appendix A.3 for full proof). We ﬁrst show that the histogram classiﬁer over the
standard partition of [0, 1]d into smaller cubes is an element of C := HAC(d, d, k), thus reducing
the problem to proving minimax-optimality of the histogram classiﬁer. Previous work Gy¨orﬁet al.
(2006) Theorem 4.3 established this for the histogram regressor. The analogous result for the his-
togram classiﬁer, to the best of our knowledge, has not appeared in the literature and thus is included
for completeness."
MINIMAX-OPTIMALITY FOR LEARNING LIPSCHITZ CLASS,0.28741092636579574,"The neural network implementation of HAC(d, d, k) where k ≍n1/(d+2) in Theorem 5.2 can be
overparametrized. Using either the 1- or the 2-hidden layer neural network implementations of
Boolean functions as in Remark 3.7, the resulting HANN is overparametrized and has number of
weights either ≥k2k or ≥c22k/k respectively. Both k2k and c22k/k ≫n exponentially while
VC(HAC(d, d, k)) = o(n)."
EMPIRICAL RESULTS,0.28978622327790976,"6
EMPIRICAL RESULTS"
EMPIRICAL RESULTS,0.2921615201900237,"In this section, we discuss experimental results of using HANNs for classifying synthetic and real
datasets. Our implementation uses TensorFlow (Abadi et al., 2016) with the Larq (Geiger & Team,
2020) library for training neural networks with threshold activations. Note that Theorem 5.2 holds
for ERM with respect to the 0-1 loss over HANNs, which is intractable in practice. Furthermore,
our theory is for binary classiﬁcation, while some of the datasets in the experiments are multiclass."
EMPIRICAL RESULTS,0.29453681710213775,"Synthetic datasets. We apply a HANN (model speciﬁcation shown in Figure 3-top left) to the
MOONS synthetic dataset with two classes with the hinge loss."
EMPIRICAL RESULTS,0.29691211401425177,"The heuristic for training networks with threshold activation can signiﬁcantly affect the performance
(Kim et al., 2019). We consider two of the most popular heuristics: the straight-through-estimator
(SteSign) and the SwishSign, introduced by Hubara et al. (2017) and Darabi et al. (2019), respec-
tively. Below, we use SwishSign since it reliably leads to higher validation accuracy (Figure 3-
bottom left), consistent with the ﬁnding of Darabi et al. (2019)."
EMPIRICAL RESULTS,0.2992874109263658,Network
EMPIRICAL RESULTS,0.3016627078384798,Architecture Input
EMPIRICAL RESULTS,0.30403800475059384,dim = 2
EMPIRICAL RESULTS,0.30641330166270786,Hidden layer 1
EMPIRICAL RESULTS,0.3087885985748218,dim=10
EMPIRICAL RESULTS,0.31116389548693585,Threshold
EMPIRICAL RESULTS,0.31353919239904987,Hidden layer 2
EMPIRICAL RESULTS,0.3159144893111639,dim=1024 ReLU
EMPIRICAL RESULTS,0.3182897862232779,Output
EMPIRICAL RESULTS,0.32066508313539194,dim = 1
EMPIRICAL RESULTS,0.32304038004750596,"0
100
200
300
400
500"
EMPIRICAL RESULTS,0.3254156769596199,Epochs 0.800 0.825 0.850 0.875 0.900 0.925 0.950 0.975 1.000
EMPIRICAL RESULTS,0.32779097387173395,Acc.rac1
EMPIRICAL RESULTS,0.33016627078384797,Validation acc.rac1 ov r 100 ind p nd nt r.ns
EMPIRICAL RESULTS,0.332541567695962,"C.rv  = m dian, Ribbon, = 20/80 int r-quantile"
EMPIRICAL RESULTS,0.334916864608076,SwishSign
EMPIRICAL RESULTS,0.33729216152019004,SteSign
EMPIRICAL RESULTS,0.33966745843230406,"Figure 3: Top left. Architecture of HANN used for the MOONS dataset (make moons in sklearn).
Bottom left. Validation accuracies from 100 independent runs with random initialization and data.
Right. Dotted lines denote the hyperplane arrangement. Coloring of the cells denote the decision
region of the trained classiﬁer. A cell ∆is bolded if 1) no training data lies in ∆and 2) ∆does not
touch the decision boundary."
EMPIRICAL RESULTS,0.342042755344418,"The width of the hidden layer is 210 = 1024. Thus, by Mukherjee & Basu (2017) Lemma 2.1, the
model can assign labels to the bolded cells arbitrarily without changing the training loss. Neverthe-
less, the optimization appears to be biased toward a topologically simpler classiﬁer. This behavior
is consistently reproducible. See Figure 7."
EMPIRICAL RESULTS,0.34441805225653205,Published as a conference paper at ICLR 2022
EMPIRICAL RESULTS,0.34679334916864607,"Real-world datasets. Klambauer et al. (2017) introduced self-normalizing neural networks (SNN)
which were shown to outperform other neural networks on a panel of 121 UCI datasets. Subse-
quently, Wu et al. (2018) proposed the dendritic neural network architecture, which further im-
proved classiﬁcation performance on this panel of datasets. Following their works, we evaluate the
performance of HANNs on the 121 UCI datasets."
EMPIRICAL RESULTS,0.3491686460807601,"A crucial hyperparameter for HANN is the number of hyperplanes k. We ran the experiments with
k ∈{15, 100} to test the impact on accuracy. The Boolean function h is implemented as a 1-hidden
layer residual network (He et al., 2016) of width 1000. The logistic loss is used."
EMPIRICAL RESULTS,0.3515439429928741,"We use the same train, validation, and test sets as in Klambauer et al. (2017). The reported accuracies
on the held-out test set are based on the model with the highest validation accuracy. The models will
be referred to as HANN15 and HANN100, respectively. The results are shown in Figure 4. The
accuracies of SNN and DENN are obtained from Table A1 in the supplemental materials of Wu
et al. (2018). Full details for the training and accuracy tables can be found in the appendix. 0.02 0.04 0.06 0.08 0.10"
EMPIRICAL RESULTS,0.35391923990498814,"-1.0 [-7.1, 0.8] 15"
EMPIRICAL RESULTS,0.35629453681710216,"-1.4 [-7.4, 1.1]"
EMPIRICAL RESULTS,0.3586698337292161,Median
EMPIRICAL RESULTS,0.36104513064133015,20/80-/h
EMPIRICAL RESULTS,0.36342042755344417,",uan/ile."
EMPIRICAL RESULTS,0.3657957244655582,"−20
−15
−10
−5
0
5
10
15
20 SNN 0.02 0.04 0.06 0.08 0.10"
EMPIRICAL RESULTS,0.3681710213776722,"0.0 [-2.5, 2.6]"
EMPIRICAL RESULTS,0.37054631828978624,"−20
−15
−10
−5
0
5
10
15
20 DENN 100"
EMPIRICAL RESULTS,0.37292161520190026,"0.0 [-2.8, 1.1]"
EMPIRICAL RESULTS,0.3752969121140142,Den.ity of KDE fit
EMPIRICAL RESULTS,0.37767220902612825,A  u-a y of HANN minu. a  u-a y of ____ (in percentages)
EMPIRICAL RESULTS,0.38004750593824227,# of hyperplanes in HANN
EMPIRICAL RESULTS,0.3824228028503563,Comparing HANN with SNN and DENN over the UCI datasets
EMPIRICAL RESULTS,0.3847980997624703,"Figure 4: Each blue tick above the x-axis represents a single dataset, where the x-coordinate of the
tick is the difference of the accuracy of HANN and either SNN (left) or DENN (right) on the dataset.
The solid black curves are kernel density estimates for the blue ticks. The number of hyperplanes
used by HANN is either 15 (top) or 100 (bottom). The quantities shown in the top-left corner of
each subplot are the median, 20-th and 80-th quantiles of the differences, respectively, rounded to 1
decimal place."
EMPIRICAL RESULTS,0.38717339667458434,"The HANN15 model (top row of Figure 4) already achieves median accuracy within 1.5% of both
SNN and DENN. With the larger HANN100 model (bottom row), the gap is reduced to zero. The
largest training set in this panel of datasets has size 77904. The HANN15 and HANN100 models use
≈104 and 105 weights, respectively. By comparison, the average numbers of weights used by SNN
and DENN are both ≥5 ∗105. Details on these estimates are included in Appendix C. Thus, all
three models considered here are overparametrized."
DISCUSSION,0.38954869358669836,"7
DISCUSSION"
DISCUSSION,0.3919239904988123,"We have introduced an architecture for which the VC theorem can be used to prove minimax-
optimality of ERM over HANNs in an overparametrized setting with Lipschitz posterior. To our
knowledge, this is the ﬁrst time VC theory has been used to analyze the performance of a neural net-
work in the overparametrized regime. Furthermore, the same architecture leads to state-of-the-art
performance over a benchmark collection of unstructured datasets."
DISCUSSION,0.39429928741092635,"To the best of our knowledge, no existing theoretical bound for overparametrized NNs yields mean-
ingful results. Yet there is immense interest in understanding what aspects of deep NNs can explain
their performance, even if the bounds aren’t yet small (Bartlett et al., 2017; Neyshabur et al., 2017;
Jiang et al., 2019). Our work shows that the compressibility of the network, as reﬂected by the sam-
ple compression scheme, is a useful avenue, and one that has not previously been explored – ours is
the ﬁrst work applying sample compression to NNs. This seems likely to open the door to further
analysis of quantized NNs."
DISCUSSION,0.39667458432304037,Published as a conference paper at ICLR 2022
DISCUSSION,0.3990498812351544,ACKNOWLEDGEMENTS
DISCUSSION,0.4014251781472684,"The authors were supported in part by the National Science Foundation under awards 1838179
and 2008074, and by the Department of Defense, Defense Threat Reduction Agency under award
HDTRA1-20-2-0002."
REPRODUCIBILITY STATEMENT,0.40380047505938244,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.40617577197149646,"All code for downloading and parsing the data, training the models, and generating plots in this
manuscript are available at https://github.com/YutongWangUMich/HANN.
Complete
proofs for all novel results are included in the main article or in an appendix."
REFERENCES,0.4085510688836104,REFERENCES
REFERENCES,0.41092636579572445,"Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-
cent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wat-
tenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A system for large-scale
machine learning. In 12th USENIX symposium on operating systems design and implementation
(OSDI 16), pp. 265–283, 2016."
REFERENCES,0.41330166270783847,"Sanjeev Arora, Simon S Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu.
Harnessing the power of inﬁnitely wide deep nets on small-data tasks. In International Conference
on Learning Representations, 2019."
REFERENCES,0.4156769596199525,"Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas Mehrabian, and
Yaniv Plan. Near-optimal sample complexity bounds for robust learning of Gaussian mixtures via
compression schemes. Journal of the ACM (JACM), 67(6):1–42, 2020."
REFERENCES,0.4180522565320665,"Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classiﬁers. The
Annals of Statistics, 35(2):608–633, 2007."
REFERENCES,0.42042755344418054,"Pierre Baldi and Roman Vershynin. The capacity of feedforward neural networks. Neural networks,
116:288–311, 2019."
REFERENCES,0.42280285035629456,"Peter L Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, pp. 6241–6250, 2017."
REFERENCES,0.4251781472684085,"Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension
and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning
Research, 20(63):1–17, 2019."
REFERENCES,0.42755344418052255,"Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overﬁtting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020."
REFERENCES,0.42992874109263657,"Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint.
Acta Numerica, 2021."
REFERENCES,0.4323040380047506,"Eric B Baum and David Haussler. What size net gives valid generalization? Neural computation, 1
(1):151–160, 1989."
REFERENCES,0.4346793349168646,"Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-
stand kernel learning. In International Conference on Machine Learning, pp. 541–549, 2018."
REFERENCES,0.43705463182897863,"Yaniv Blumenfeld, Dar Gilboa, and Daniel Soudry. A mean ﬁeld theory of quantized deep net-
works: The quantization-depth trade-off. In Advances in Neural Information Processing Systems,
volume 32, 2019."
REFERENCES,0.43942992874109266,Published as a conference paper at ICLR 2022
REFERENCES,0.4418052256532066,"Olivier Bousquet, Steve Hanneke, Shay Moran, and Nikita Zhivotovskiy. Proper learning, Helly
number, and an optimal SVM bound. In Conference on Learning Theory, 2020."
REFERENCES,0.44418052256532065,"Robert Creighton Buck. Partition of space. The American Mathematical Monthly, 50(9):541–544,
1943."
REFERENCES,0.44655581947743467,"Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by
half-wave Gaussian quantization. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 5918–5926, 2017."
REFERENCES,0.4489311163895487,"Niladri S Chatterji and Philip M Long. Finite-sample analysis of interpolating linear classiﬁers in
the overparameterized regime. Journal of Machine Learning Research, 22(129):1–30, 2021."
REFERENCES,0.4513064133016627,"Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In Conference on Learning Theory, pp. 1305–1338. PMLR, 2020."
REFERENCES,0.45368171021377673,"Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with ap-
plications in pattern recognition. IEEE transactions on electronic computers, (3):326–334, 1965."
REFERENCES,0.45605700712589076,"Sajad Darabi, Mouloud Belbahri, Matthieu Courbariaux, and Vahid Partovi Nia. Regularized bi-
nary network training. In Fifth Workshop on Energy Efﬁcient Machine Learning and Cognitive
Computing - NeurIPS Edition (EMC2-NIPS), 2019."
REFERENCES,0.4584323040380047,"Luc Devroye. Any discrimination rule can have an arbitrarily bad probability of error for ﬁnite
sample size.
IEEE Transactions on Pattern Analysis and Machine Intelligence, (2):154–157,
1982."
REFERENCES,0.46080760095011875,"Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Proceedings
of the 33rd Conference on Uncertainty in Artiﬁcial Intelligence, 2017."
REFERENCES,0.46318289786223277,"Andr´as Farag´o and G´abor Lugosi. Strong universal consistency of neural network classiﬁers. IEEE
Transactions on Information Theory, 39(4):1146–1151, 1993."
REFERENCES,0.4655581947743468,"Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis
dimension. Machine learning, 21(3):269–304, 1995."
REFERENCES,0.4679334916864608,"Komei
Fukuda.
Lecture:
Polyhedral
computation,
Spring
2013,
2015.
URL
http://www-oldurls.inf.ethz.ch/personal/fukudak/lect/pclect/
notes2015/PolyComp2015.pdf."
REFERENCES,0.47030878859857483,"Lukas Geiger and Plumerai Team. Larq: An open-source library for training binarized neural net-
works. Journal of Open Source Software, 5(45):1746, January 2020."
REFERENCES,0.47268408551068886,"Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016."
REFERENCES,0.4750593824228028,"Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.47743467933491684,"L´aszl´o Gy¨orﬁ, Michael Kohler, Adam Krzyzak, and Harro Walk.
A distribution-free theory of
nonparametric regression. Springer Science & Business Media, 2006."
REFERENCES,0.47980997624703087,"Steve Hanneke and Aryeh Kontorovich. Stable sample compression schemes: New applications and
an optimal SVM margin bound. In Algorithmic Learning Theory, pp. 697–721. PMLR, 2021."
REFERENCES,0.4821852731591449,"Steve Hanneke, Aryeh Kontorovich, and Menachem Sadigurschi. Sample compression for real-
valued learners. In Algorithmic Learning Theory, pp. 466–488. PMLR, 2019."
REFERENCES,0.4845605700712589,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.48693586698337293,"Geoffrey Hinton, Nitsh Srivastava, and Kevin Swersky.
Neural networks for machine learning.
Coursera, video lectures, 264(1):2146–2153, 2012."
REFERENCES,0.48931116389548696,Published as a conference paper at ICLR 2022
REFERENCES,0.4916864608076009,"Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. The Jour-
nal of Machine Learning Research, 18(1):6869–6898, 2017."
REFERENCES,0.49406175771971494,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.49643705463182897,"Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantas-
tic generalization measures and where to ﬁnd them. In International Conference on Learning
Representations, 2019."
REFERENCES,0.498812351543943,"Hyungjun Kim, Kyungsu Kim, Jinseok Kim, and Jae-Joon Kim. Binaryduo: Reducing gradient mis-
match in binary activation network by coupling binary activations. In International Conference
on Learning Representations, 2019."
REFERENCES,0.501187648456057,"Yongdai Kim, Ilsang Ohn, and Dongha Kim. Fast convergence rates of deep neural networks for
classiﬁcation. Neural Networks, 138:179–197, 2021."
REFERENCES,0.503562945368171,"G¨unter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In Advances in Neural Information Processing Systems, pp. 971–980, 2017."
REFERENCES,0.505938242280285,"Michael Kohler and Sophie Langer. On the rate of convergence of fully connected deep neural
network regression estimates. arXiv preprint arXiv:1908.11133, 2020."
REFERENCES,0.5083135391923991,"Sophie Langer. Analysis of the rate of convergence of fully connected deep neural network regres-
sion estimates with smooth activation function. Journal of Multivariate Analysis, 182:104695,
2021."
REFERENCES,0.5106888361045131,"Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein.
Training
quantized nets: A deeper understanding. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pp. 5813–5823, 2017."
REFERENCES,0.5130641330166271,"Ji Lin, Chuang Gan, and Song Han. Defensive quantization: When efﬁciency meets robustness. In
International Conference on Learning Representations, 2018."
REFERENCES,0.5154394299287411,"Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network.
Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.517814726840855,Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. 1986.
REFERENCES,0.5201900237529691,"L´aszl´o Lov´asz and Michael D Plummer. Matching theory, volume 367. American Mathematical
Soc., 2009."
REFERENCES,0.5225653206650831,"Wolfgang Maass. Neural nets with superlinear VC-dimension. Neural Computation, 6(5):877–884,
1994."
REFERENCES,0.5249406175771971,"Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook, and Debbie Marr. WRPN: Wide reduced-precision
networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.5273159144893111,"Shay Moran and Amir Yehudayoff. Sample compression schemes for VC classes. Journal of the
ACM (JACM), 63(3):1–10, 2016."
REFERENCES,0.5296912114014252,"Anirbit Mukherjee and Amitabh Basu. Lower bounds over Boolean inputs for deep neural networks
with ReLU gates. arXiv preprint arXiv:1711.03073, 2017."
REFERENCES,0.5320665083135392,"Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generaliza-
tion in deep learning. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.5344418052256532,"Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring gener-
alization in deep learning. In Proceedings of the 31st International Conference on Neural Infor-
mation Processing Systems, pp. 5949–5958, 2017."
REFERENCES,0.5368171021377672,"Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural
networks: A survey. Pattern Recognition, 105:107281, 2020."
REFERENCES,0.5391923990498813,Published as a conference paper at ICLR 2022
REFERENCES,0.5415676959619953,"Shashank Rajput, Kartik Sreenivasan, Dimitris Papailiopoulos, and Amin Karbasi. An exponential
improvement on the memorization capacity of deep threshold networks. In Advances in Neural
Information Processing Systems, 2021."
REFERENCES,0.5439429928741093,"Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XOR-Net: Imagenet
classiﬁcation using binary convolutional neural networks. In European conference on computer
vision, pp. 525–542. Springer, 2016."
REFERENCES,0.5463182897862233,"Robert E Schapire. Explaining AdaBoost. In Empirical inference, pp. 37–52. Springer, 2013."
REFERENCES,0.5486935866983373,"Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU acti-
vation function. Annals of Statistics, 48(4):1875–1897, 2020."
REFERENCES,0.5510688836104513,"Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014."
REFERENCES,0.5534441805225653,"Vaishaal Shankar, Alex Fang, Wenshuo Guo, Sara Fridovich-Keil, Jonathan Ragan-Kelley, Ludwig
Schmidt, and Benjamin Recht. Neural kernels without tangents. In International Conference on
Machine Learning, pp. 8614–8623. PMLR, 2020."
REFERENCES,0.5558194774346793,"VN Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events
to their probabilities. Measures of Complexity, 16(2):11, 1971."
REFERENCES,0.5581947743467933,"Roman Vershynin. Memory capacity of neural networks with threshold and rectiﬁed linear unit
activations. SIAM Journal on Mathematics of Data Science, 2(4):1004–1033, 2020."
REFERENCES,0.5605700712589073,"Xundong Wu, Xiangwen Liu, Wei Li, and Qing Wu. Improved expressivity through dendritic neural
networks. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pp. 8068–8079, 2018."
REFERENCES,0.5629453681710214,"Abraham J Wyner, Matthew Olson, Justin Bleich, and David Mease. Explaining the success of
AdaBoost and random forests as interpolating classiﬁers. The Journal of Machine Learning Re-
search, 18(1):1558–1590, 2017."
REFERENCES,0.5653206650831354,"Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Under-
standing straight-through estimator in training activation quantized neural nets. In International
Conference on Learning Representations, 2019."
REFERENCES,0.5676959619952494,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
115, 2021."
REFERENCES,0.5700712589073634,"A
PROOFS"
REFERENCES,0.5724465558194775,"A.1
PROOF OF PROPOSITION 4.3"
REFERENCES,0.5748218527315915,"By deﬁnition, there exists h ∈Boolk, W ∈Rd×k of rank at most r, and b ∈Rk such that
yi = h(σsgn(W⊤xi + b))."
REFERENCES,0.5771971496437055,"Now, let j ∈[k] be ﬁxed. Since |w⊤
j xi + bj| ≥0 for all i ∈[n], there exists a small perturbation
˜cj of bj such that |w⊤
j xi + ˜cj| > 0 for all i ∈[n]. Now, let λj := mini∈[n] |w⊤
j xi + ˜cj| which is
positive. Deﬁne vj := wj/λj and cj = ˜cj/λj, we have |v⊤
j xi + cj| ≥1 for all i ∈[n], as desired.
Note that rank(V) = rank(W).
□"
REFERENCES,0.5795724465558195,"A.2
PROOF OF PROPOSITION 4.5"
REFERENCES,0.5819477434679335,"If m ≤n, then the result holds trivially by letting J = [m]. Below, suppose that m > n. Let
gi(x) = a⊤
i x −bi for each i ∈[m] and f(x) = 1"
REFERENCES,0.5843230403800475,"2∥x∥2
2. Then ∇f(x) = x and ∇gi(x) = ai. By
deﬁnition, x∗
I is a minimizer of"
REFERENCES,0.5866983372921615,"min
x∈Rn f(x) s.t. gi(x) ≤0, ∀i ∈I,"
REFERENCES,0.5890736342042755,Published as a conference paper at ICLR 2022
REFERENCES,0.5914489311163895,"which is a convex optimization with strongly convex objective. Thus, the minimizer x∗
I is unique
and furthermore is the unique element x of Rn satisfying the KKT conditions:"
REFERENCES,0.5938242280285035,"x ∈PI and ∃a set of nonnegative weights {λi}i∈I such that −x =
X"
REFERENCES,0.5961995249406176,"i∈I
λiai."
REFERENCES,0.5985748218527316,"Thus, x∗
I can be equivalently characterized as the unique element of x ∈Rn satisfying
x ∈PI and −x ∈coni({ai}i∈I).
(4)
In particular, x∗
[m] ∈P[m] and −x∗
[m] ∈coni({ai}i∈[m]). By the Carath´edory’s theorem for the
conical hull (Lov´asz & Plummer, 2009), there exists I ⊆[m] such that |I| = n and −x∗
[m] ∈
coni({ai}i∈I). Thus, for any J ⊆[m] such that I ⊆J, we have −x∗
[m] ∈coni({ai}i∈J).
Furthermore, J ⊆[m] implies PJ ⊇P[m]. In particular, x∗
[m] ∈PJ. Putting it all together, we have
x∗
[m] ∈PJ and −x∗
[m] ∈coni({ai}i∈J). By the uniqueness, we have x∗
J = x∗
[m].
□"
REFERENCES,0.6009501187648456,"A.3
PROOF OF THEOREM 5.2"
REFERENCES,0.6033254156769596,"In this proof, the constant C does not depending on n, and may change from line to line."
REFERENCES,0.6057007125890737,"We ﬁx a joint distribution P such that ηP ∈Σ(L, [0, 1]d) throughout the proof. Thus, the notation
for risks will omit the P in their subscript, e.g., we write ˆRn(f) instead of ˆRP,n(f) and R∗instead of
R∗
P . Below, let β > α > 0 be constants such that αdn1/(d+2) ≤k ≤βdn1/(d+2). Let ˜k := ⌈k/d⌉."
REFERENCES,0.6080760095011877,"Let R1, R2, . . . , R˜kd denote the hypercubes of side length ℓ= 1/˜k forming a partition of [0, 1]d.
For each i ∈[˜kd], let R−
i := {x ∈Ri : ηP (x) < 1/2} and R+
i := {x ∈Ri : ηP (x) ≥1/2}."
REFERENCES,0.6104513064133017,"Let ˜f : [0, 1]d →B be the classiﬁer such that"
REFERENCES,0.6128266033254157,˜f(x) =
REFERENCES,0.6152019002375297,"(
+1
: x ∈Ri,
R"
REFERENCES,0.6175771971496437,"Ri ηP (x)dP(x) ≥
R"
REFERENCES,0.6199524940617577,"Ri(1 −ηP (x))dP(x)
−1
: x ∈Ri,
R"
REFERENCES,0.6223277909738717,"Ri ηP (x)dP(x) <
R"
REFERENCES,0.6247030878859857,Ri(1 −ηP (x))dP(x).
REFERENCES,0.6270783847980997,"In other words, ˜f classiﬁes all x ∈Ri as +1 if and only if P(Y = 1|X ∈Ri) ≥1/2. This is
commonly referred to as the histogram classiﬁer (Gy¨orﬁet al., 2006). It is easy to see that"
REFERENCES,0.6294536817102138,"P( ˜f(X) ̸= Y, X ∈Ri) = min
Z"
REFERENCES,0.6318289786223278,"Ri
(1 −ηP (x))dP(x),
Z"
REFERENCES,0.6342042755344418,"Ri
ηP (x)dP(x)
"
REFERENCES,0.6365795724465558,"For the remainder of this proof, we write “P"
REFERENCES,0.6389548693586699,i” to mean “P
REFERENCES,0.6413301662707839,"i∈[˜kd]”. Thus,"
REFERENCES,0.6437054631828979,"R( ˜f) =
X"
REFERENCES,0.6460807600950119,"i
P( ˜f(X) ̸= Y, X ∈Ri) =
X"
REFERENCES,0.6484560570071259,"i
min
Z"
REFERENCES,0.6508313539192399,"Ri
(1 −ηP (x))dP(x),
Z"
REFERENCES,0.6532066508313539,"Ri
ηP (x)dP(x)

."
REFERENCES,0.6555819477434679,"Next, we note that ˜f ∈HAC(d, d, k). To see this, let j ∈[d]. Take Hj1, . . . , Hj(˜k−1) ⊆Rd to be"
REFERENCES,0.6579572446555819,"the hyperplanes perpendicular to the j-th coordinate where, for each ℓ∈[˜k], Hjℓintersects the j-th
coordinate axis at ℓ/˜k. Consider the hyperplane arrangement consisting of all {Hjℓ}j∈[d],ℓ∈[˜k−1]
and let {C1, C2, . . . } be its cells. Then {C1 ∩[0, 1]d, C2 ∩[0, 1]d, . . . } = {R1, . . . , R˜kd} is the
partition of [0, 1]d by 1/˜k side length hypercubes. See Figure 5."
REFERENCES,0.6603325415676959,"Let W be the matrix of normal vectors and b be the vector of offsets representing this hyperplane
arrangement, which requires d(˜k −1) = d(⌈k/d⌉−1) ≤d(k/d) = k hyperplanes. Since ˜f is
constant on Ri, there exists a Boolean function h ∈Boolk such that h ◦qW,b|[0,1]d = ˜f. From this,
we conclude that ˜f ∈HAC(d, d, k)."
REFERENCES,0.66270783847981,Thus ˆRn( ˆfn) −ˆRn( ˜f) ≤0 and so
REFERENCES,0.665083135391924,"R( ˆfn) −R∗= R( ˆfn) −ˆRn( ˆfn) + ˆRn( ˆfn) −ˆRn( ˜f)
|
{z
}
≤0"
REFERENCES,0.667458432304038,+ ˆRn( ˜f) −R( ˜f) + R( ˜f) −R∗
REFERENCES,0.669833729216152,"≤R( ˆfn) −ˆRn( ˆfn)
|
{z
}"
REFERENCES,0.672209026128266,Term 1
REFERENCES,0.6745843230403801,"+ ˆRn( ˜f) −R( ˜f)
|
{z
}"
REFERENCES,0.6769596199524941,Term 2
REFERENCES,0.6793349168646081,"+ R( ˜f) −R∗
|
{z
}"
REFERENCES,0.6817102137767221,Term 3 .
REFERENCES,0.684085510688836,Published as a conference paper at ICLR 2022
REFERENCES,0.6864608076009501,"H11
H12 H21 H22 1/3"
REFERENCES,0.6888361045130641,"Figure 5: Partition of [0, 1]d into 1/˜k hypercubes via arrangement of d(˜k −1) hyperplanes, where
d = 2 and ˜k = 3. Shaded region is [0, 1]d. Dotted region is a cell of the hyperplane arrangement."
REFERENCES,0.6912114014251781,"We now bound Terms 1 and 2 using the uniform deviation bound. From Theorem 3.6, we know that
there exists a constant C independent of n such that"
REFERENCES,0.6935866983372921,"VC(HAC(d, d, k)) ≤8 ·

k(d + 1) + k(d + 1)(1 + ⌈log2(k)⌉) +
 k
≤d"
REFERENCES,0.6959619952494062,"
≤Ckd."
REFERENCES,0.6983372921615202,"Thus, by Theorem 3.5 with δ = 1/(2n) and a union bound, with probability at least 1 −1/n"
REFERENCES,0.7007125890736342,"max
n
| ˆRn( ˆfn) −R( ˆfn)|, | ˆRn( ˜f) −R( ˜f)|
o
≤C r"
REFERENCES,0.7030878859857482,"kd + log(n) n
(5)"
REFERENCES,0.7054631828978623,for some C > 0.
REFERENCES,0.7078384798099763,"Next, we focus on Term 3. Recall that R∗=
Z"
REFERENCES,0.7102137767220903,"[0,1]d min{ηP (x), 1 −ηP (x)}dP(x) =
X i Z"
REFERENCES,0.7125890736342043,"Ri
min{ηP (x), 1 −ηP (x)}dP(x)"
REFERENCES,0.7149643705463183,and that
REFERENCES,0.7173396674584323,"R( ˜f) =
X"
REFERENCES,0.7197149643705463,"i
min
Z"
REFERENCES,0.7220902612826603,"Ri
ηP (x)dP(x),
Z"
REFERENCES,0.7244655581947743,"Ri
1 −ηP (x)dP(x)

."
REFERENCES,0.7268408551068883,"Fix some i ∈[kd]. Our goal now is to bound the difference between the i-th summands in the above
expressions for R( ˜f) and R∗:"
REFERENCES,0.7292161520190024,"min
Z"
REFERENCES,0.7315914489311164,"Ri
ηP (x)dP(x),
Z"
REFERENCES,0.7339667458432304,"Ri
1 −ηP (x)dP(x)

−
Z"
REFERENCES,0.7363420427553444,"Ri
min{ηP (x), 1 −ηP (x)}dP(x).
(6)"
REFERENCES,0.7387173396674585,"First, consider the case that"
REFERENCES,0.7410926365795725,"min
Z"
REFERENCES,0.7434679334916865,"Ri
ηP (x)dP(x),
Z"
REFERENCES,0.7458432304038005,"Ri
1 −ηP (x)dP(x)

=
Z"
REFERENCES,0.7482185273159145,"Ri
ηP (x)dP(x).
(7)"
REFERENCES,0.7505938242280285,"We claim that there must exist x0 ∈Ri such that ηP (x0) ≤1/2. Suppose ηP (x) > 1/2 for all
x ∈Ri. Then ηP (x) > 1/2 > 1 −ηP (x). Since ηP (x) is continuous, this would contradict eq. (7)."
REFERENCES,0.7529691211401425,"Continue assuming eq. (7), we further divide into two subcases: (1) ηP (x) ≤1/2 for all x ∈Ri,
and (2) there exists some x1 ∈Ri such that ηP (x1) > 1/2."
REFERENCES,0.7553444180522565,"Under subcase (1), min{ηP (x), 1 −ηP (x)} = ηP (x) for all x ∈Ri in which case eq. (6) = 0."
REFERENCES,0.7577197149643705,Published as a conference paper at ICLR 2022
REFERENCES,0.7600950118764845,"Under subcase (2), since ηP (x0) ≤1/2 < ηP (x1), we know by the intermediate value theorem that
there must exist x′ ∈Ri such that ηP (x′) = 1/2. Now,"
REFERENCES,0.7624703087885986,"eq. (6) =
Z"
REFERENCES,0.7648456057007126,"Ri
(ηP (x) −min{ηP (x), 1 −ηP (x)})dP(x) ≤
Z"
REFERENCES,0.7672209026128266,"R+
i
(ηP (x) −min{ηP (x), 1 −ηP (x)})dP(x) +
Z"
REFERENCES,0.7695961995249406,"R−
i
(ηP (x) −min{ηP (x), 1 −ηP (x)})dP(x) =
Z"
REFERENCES,0.7719714964370546,"R+
i
(ηP (x) −(1 −ηP (x))dP(x)
∵Deﬁnition of R±
i +
Z"
REFERENCES,0.7743467933491687,"R−
i
(ηP (x) −ηP (x))dP(x) =
Z"
REFERENCES,0.7767220902612827,"R+
i
(2ηP (x) −1)dP(x) = 2
Z"
REFERENCES,0.7790973871733967,"R+
i
(ηP (x) −ηP (x′))dP(x)
∵2ηP (x′) = 1 ≤2L
Z"
REFERENCES,0.7814726840855107,"R+
i
∥x −x′∥2dP(x) ≤2L
√"
REFERENCES,0.7838479809976246,"d Pr(Ri)/˜k
∵∥x −x′∥2 ≤
√"
REFERENCES,0.7862232779097387,"d∥x −x′∥1 ≤
√"
REFERENCES,0.7885985748218527,d(1/˜k)
REFERENCES,0.7909738717339667,"≤2Ld3/2 Pr(Ri)/k
∵1/˜k = 1/⌈k/d⌉≤1/(k/d) = d/k."
REFERENCES,0.7933491686460807,"Thus, under assumption eq. (7), we have proven that eq. (6) ≤2Ld3/2/k. For the other assumption,
i.e., the minimum in eq. (7) is attained by
R"
REFERENCES,0.7957244655581948,"Ri 1 −ηP (x)dP(x), a completely analogous argument
again shows that eq. (6) ≤2Ld3/2/k."
REFERENCES,0.7980997624703088,"Putting it all together, we have"
REFERENCES,0.8004750593824228,R( ˜fn) −R∗≤2Ld3/2 X
REFERENCES,0.8028503562945368,"i
P(Ri)/k = 2Ld3/2/k.
(8)"
REFERENCES,0.8052256532066508,"We have shown that, with probability at least 1 −1/n,"
REFERENCES,0.8076009501187649,R( ˆfn) −R∗≤C r
REFERENCES,0.8099762470308789,kd + log(n)
REFERENCES,0.8123515439429929,"n
+ 2Ld3/2 k
."
REFERENCES,0.8147268408551069,"Using αdn1/(d+2) ≤k ≤βdn1/(d+2), we have with probably at least 1 −1/n that"
REFERENCES,0.8171021377672208,R( ˆfn) −R∗≤C r
REFERENCES,0.8194774346793349,kd + log(n)
REFERENCES,0.8218527315914489,"n
+ 2Ld3/2 k ≤C r"
REFERENCES,0.8242280285035629,(βd)dnd/(d+2) + log(n)
REFERENCES,0.8266033254156769,"n
+
2Ld3/2"
REFERENCES,0.828978622327791,αdn1/(d+2) ≤C r
REFERENCES,0.831353919239905,nd/(d+2)
REFERENCES,0.833729216152019,"n
+ n−1/(d+2)
!"
REFERENCES,0.836104513064133,∵log(n) = o(n1/d+2)
REFERENCES,0.838479809976247,"= C
p"
REFERENCES,0.8408551068883611,n−2/(d+2) + n−1/(d+2)
REFERENCES,0.8432304038004751,"≤Cn−
1
d+2 ."
REFERENCES,0.8456057007125891,"Taking expectation, we have E[R( ˆfn)] −R∗≤(1 −1/n)Cn−
1
d+2 + 1/n · 1 ≤Cn−
1
d+2 .
□"
REFERENCES,0.8479809976247031,Published as a conference paper at ICLR 2022
REFERENCES,0.850356294536817,"B
TRAINING DETAILS"
REFERENCES,0.8527315914489311,"Data preprocessing. The pooled training and validation data is centered and standardized using
the StandardScaler function from sklearn. The transformation is also applied to the test data,
using the centers and scaling from the pooled training and validation data:"
REFERENCES,0.8551068883610451,"scaler = StandardScaler().fit(X_train_valid)
X_train_valid = scaler.transform(X_train_valid)
X_test = scaler.transform(X_test)"
REFERENCES,0.8574821852731591,"If the feature dimension and training sample size are both > 50, then the data is dimension reduced
to 50 principal component features:"
REFERENCES,0.8598574821852731,if min(X_train_valid.shape) > 50:
REFERENCES,0.8622327790973872,"pca = PCA(n_components = 50).fit(X_train_valid)
X_train_valid = pca.transform(X_train_valid)
X_test = pca.transform(X_test)"
REFERENCES,0.8646080760095012,Note that this is equivalent to freezing the weights between the Input and the Latent layer in Figure 2.
REFERENCES,0.8669833729216152,"Validation and test accuracy. Every 10 epochs, the validation accuracy during the past 10 epochs
are averaged. A smoothed validation accuracy is calculated as follows:"
REFERENCES,0.8693586698337292,"val_acc_sm = (1-sm_param)*val_acc_sm + sm_param*val_acc_av
## Variable description:
# sm_param = 0.1
# val_acc_av = average of the validation in the past 10 epochs
# val_acc_sm = smoothed validation accuracy"
REFERENCES,0.8717339667458432,"The predicted test labels is based on the snapshot of the model at the highest smoothed validation
accuracy, at the end once max epochs is reached."
REFERENCES,0.8741092636579573,"Heuristic for coarse gradient of the threshold function. We use the SwishSign from the Larq
library (Geiger & Team, 2020)."
REFERENCES,0.8764845605700713,"# import larq as lq
qtz = lq.quantizers.SwishSign()"
REFERENCES,0.8788598574821853,"Dropout. During training, dropout is applied to the Boolean output of the threshold function, i.e,
the variables B1, B2, . . . , Bk in Figure 2. This improves generalization by preventing the training
accuracy from reaching 100%."
REFERENCES,0.8812351543942993,"# from tensorflow.keras.layers import Dense, Dropout
hyperplane_enc = Dense(n_hyperplanes, activation = qtz)(inputs)
hyperplane_enc = Dropout(dropout_rate)(hyperplane_enc)"
REFERENCES,0.8836104513064132,"Implementation of the Boolean function. For the Boolean function h, we use a 1-hidden layer
residual network (He et al., 2016) with 1000 hidden nodes:"
REFERENCES,0.8859857482185273,"# from tensorflow.keras.layers import Dense, Add
# output_dim = num_classes
n_hidden = 1000
hidden = Dense(n_hidden, activation=""relu"")(hyperplane_enc)
out_hidden = Dense(output_dim, activation = ""linear"")(hidden)
out_skip = Dense(output_dim, activation = ""linear"")(hyperplane_enc)
outputs = Add()([out_skip,out_hidden])"
REFERENCES,0.8883610451306413,"Hyperparameters. HANN15 is trained with a hyperparameter grid of size 3 where only the dropout
rate is tuned. The hyperparameters are summarized in Table 2. The model with the highest smoothed
validation accuracy is chosen."
REFERENCES,0.8907363420427553,The model HANN15 is trained with the following hyperparameters:
REFERENCES,0.8931116389548693,Published as a conference paper at ICLR 2022
REFERENCES,0.8954869358669834,Table 1: HANN15 model and training hyperparameter grid
REFERENCES,0.8978622327790974,"OPTIMIZER
SGD
LEARNING RATE
0.01
DROPOUT RATE
{0.1, 0.25, 0.5}
MINIBATCH SIZE
128
BOOLEAN FUNCTION
1-HIDDEN LAYER RESNET
WITH 1000 HIDDEN NODES"
REFERENCES,0.9002375296912114,"EPOCHS
100 MINIBOONE
5000 FOR ALL OTHERS"
REFERENCES,0.9026128266033254,"For HANN100, we only used 1 set of hyperparameters."
REFERENCES,0.9049881235154394,Table 2: HANN100 model and training hyperparameter
REFERENCES,0.9073634204275535,"OPTIMIZER
SGD
LEARNING RATE
0.01
DROPOUT RATE
0.5
MINIBATCH SIZE
128
BOOLEAN FUNCTION
1-HIDDEN LAYER RESNET
WITH 1000 HIDDEN NODES"
REFERENCES,0.9097387173396675,"EPOCHS
100 MINIBOONE
5000 FOR ALL OTHERS"
REFERENCES,0.9121140142517815,"C
PARAMETER COUNTS"
REFERENCES,0.9144893111638955,"The widest part of HANN15 and HANN100 models are the weights mapping from Bk (k = number
of hyperplanes) to R1000 (1000 = number of hidden layer of the boolean function) where k ∈
{15, 100}. Thus, the two HANN models use ≥15 × 1000 ≥104 and ≥100 × 1000 = 105
weights, respectively."
REFERENCES,0.9168646080760094,"The weight count estimates for the Self-normalized Neural Network (SNN) and Dendritic Neural
Network (DENN) use the formula (# layers −1) × (# neurons per layer)2."
REFERENCES,0.9192399049881235,"For the Self-normalized Neural Network (SNN), average number of layers = 10.8, and the number
of neurons per layers ≥256, found on page 7 and Table A4 of Klambauer et al. (2017), respectively.
The number of weights is ≥(10 −1) ∗(2562) = 655, 360 weights."
REFERENCES,0.9216152019002375,"The parameters for the dendritic neural network (DENN) is found in the public GitHub repository
xiangwenliu/DENN of Wu et al. (2018) which lists number of layers = 3 and number of neurons per
layer = 512, found on line 41 and 52 of train uci.py, respectively. The number of weights is
≥(3 −1) ∗(5122) = 524, 288 weights."
REFERENCES,0.9239904988123515,"D
ADDITIONAL PLOTS"
REFERENCES,0.9263657957244655,"Multiclass hinge versus cross-entropy loss. Figure 6 shows the accuracy differences when the
Weston-Watkins hinge loss is used. Compared to the results shown in Figure 4, the performance for
HANN100 is slightly worse and the performance for HANN15 is slightly better."
REFERENCES,0.9287410926365796,Published as a conference paper at ICLR 2022 0.02 0.04 0.06 0.08 0.10
REFERENCES,0.9311163895486936,"-0.7 [-6.0, 1.5] 15"
REFERENCES,0.9334916864608076,"-1.1 [-7.9, 0.6]"
REFERENCES,0.9358669833729216,Median
REFERENCES,0.9382422802850356,20/80-1h
REFERENCES,0.9406175771971497,.2an1iles
REFERENCES,0.9429928741092637,"−20
−15
−10
−5
0
5
10
15
20 SNN 0.02 0.04 0.06 0.08 0.10"
REFERENCES,0.9453681710213777,"0.0 [-3.8, 3.0]"
REFERENCES,0.9477434679334917,"−20
−15
−10
−5
0
5
10
15
20 DENN 100"
REFERENCES,0.9501187648456056,"-0.2 [-3.1, 1.4]"
REFERENCES,0.9524940617577197,"Den0i1y ,f KDE fi1"
REFERENCES,0.9548693586698337,"Acc2racy ,f HANN min20 acc2racy ,f      (in percentages)"
REFERENCES,0.9572446555819477,# of hyperplanes in HANN
REFERENCES,0.9596199524940617,Comparing HANN with SNN and DENN over the UCI datasets
REFERENCES,0.9619952494061758,"Figure 6: Each blue tick above the x-axis represents a single dataset, where the x-coordinate of the
tick is the difference of the accuracy of HANN and either SNN (left) or DENN (right) on the dataset.
The number of hyperplanes used by HANN is either 15 (top) or 100 (bottom). The quantities shown
in the top-left corner of each subplot are the median, 20-th and 80-th quantiles of the differences,
respectively, rounded to 1 decimal place."
REFERENCES,0.9643705463182898,"Implicit bias for low complexity decision boundary. In Figure 7, we show additional results ran
with the same setting for the MOONS synthetic dataset as in the Empirical Results section. From
the perspective of the training loss, the label assignment in the bold-boundary regions is irrelevant.
Nevertheless, the optimization consistently appears to be biased toward the geometrically simpler
classiﬁer, despite the capacity for ﬁtting complex classiﬁers."
REFERENCES,0.9667458432304038,Figure 7: Four independent runs of HANN on the MOONS synthetic dataset. Similar to Figure 3.
REFERENCES,0.9691211401425178,Published as a conference paper at ICLR 2022
REFERENCES,0.9714964370546318,"E
TABLE OF ACCURACIES"
REFERENCES,0.9738717339667459,"Below is the table of accuracies used to make Figure 4. The last column “HANN100trn” records the
training accuracy at the epoch of the highest validation accuracy."
REFERENCES,0.9762470308788599,"DSName
HANN15
HANN100
SNN
DENN
HANN100trn"
REFERENCES,0.9786223277909739,"abalone
63.41
65.13
66.57
66.38
70.60
acute-inﬂammation
100.00
100.00
100.00
100.00
100.00
acute-nephritis
100.00
100.00
100.00
100.00
100.00
adult
84.32
85.04
84.76
84.80
86.35
annealing
47.00
74.00
76.00
75.00
96.81
arrhythmia
62.83
64.60
65.49
67.26
97.19
audiology-std
56.00
68.00
80.00
76.00
99.79
balance-scale
92.95
96.79
92.31
98.08
98.32
balloons
100.00
100.00
100.00
100.00
100.00
bank
88.50
88.05
89.03
89.65
95.62
blood
75.94
75.40
77.01
73.26
82.03
breast-cancer
70.42
63.38
71.83
69.01
86.63
breast-cancer-wisc
97.71
98.29
97.14
97.71
98.65
breast-cancer-wisc-diag
97.89
98.59
97.89
98.59
99.65
breast-cancer-wisc-prog
73.47
71.43
67.35
71.43
98.50
breast-tissue
61.54
80.77
73.08
65.38
95.78
car
98.84
100.00
98.38
98.84
99.32
cardiotocography-10clases
78.91
82.11
83.99
82.30
94.69
cardiotocography-3clases
90.58
93.97
91.53
94.35
97.09
chess-krvk
47.75
72.77
88.05
80.41
67.16
chess-krvkp
98.62
99.37
99.12
99.62
99.93
congressional-voting
61.47
57.80
61.47
57.80
66.54
conn-bench-sonar-mines-rocks
78.85
84.62
78.85
82.69
99.51
conn-bench-vowel-deterding
89.39
98.92
99.57
99.35
98.38
connect-4
78.96
86.39
88.07
86.46
86.83
contrac
52.72
49.73
51.90
54.89
62.95
credit-approval
81.98
79.65
84.30
82.56
98.47
cylinder-bands
69.53
73.44
72.66
78.12
99.54
dermatology
98.90
97.80
92.31
97.80
99.63
echocardiogram
84.85
87.88
81.82
87.88
94.62
ecoli
86.90
84.52
89.29
85.71
92.83
energy-y1
93.23
97.40
95.83
95.83
97.84
energy-y2
89.06
91.15
90.63
90.62
95.72
fertility
92.00
92.00
92.00
88.00
95.44
ﬂags
39.58
50.00
45.83
52.08
94.96
glass
77.36
60.38
73.58
60.38
96.77
haberman-survival
72.37
65.79
73.68
65.79
82.34
hayes-roth
71.43
82.14
67.86
85.71
85.61
heart-cleveland
53.95
59.21
61.84
57.89
96.65
heart-hungarian
72.60
79.45
79.45
78.08
95.67
heart-switzerland
45.16
51.61
35.48
48.39
89.86
heart-va
36.00
30.00
36.00
32.00
94.34
hepatitis
82.05
82.05
76.92
79.49
100.00
hill-valley
66.83
68.81
52.48
54.62
72.69
horse-colic
80.88
83.82
80.88
82.35
97.65
ilpd-indian-liver
70.55
69.18
69.86
71.92
86.75
image-segmentation
87.76
90.57
91.14
90.57
99.31
ionosphere
89.77
87.50
88.64
96.59
98.84
iris
100.00
97.30
97.30
100.00
97.39
led-display
73.60
75.20
76.40
76.00
75.96"
REFERENCES,0.9809976247030879,Continued on next page
REFERENCES,0.9833729216152018,Published as a conference paper at ICLR 2022
REFERENCES,0.9857482185273159,"DSName
HANN15
HANN100
SNN
DENN
HANN100trn"
REFERENCES,0.9881235154394299,"lenses
50.00
66.67
66.67
66.67
100.00
letter
81.82
96.86
97.26
96.20
96.47
libras
64.44
81.11
78.89
77.78
98.20
low-res-spect
86.47
90.23
85.71
90.23
99.81
lung-cancer
37.50
62.50
62.50
62.50
100.00
lymphography
89.19
94.59
91.89
94.59
99.67
magic
86.52
87.49
86.92
86.81
87.84
mammographic
81.25
80.00
82.50
80.83
87.10
miniboone
90.04
90.73
93.07
93.30
89.98
molec-biol-promoter
73.08
80.77
84.62
88.46
99.38
molec-biol-splice
79.05
78.04
90.09
85.45
98.35
monks-1
65.97
69.91
75.23
81.71
99.89
monks-2
66.20
66.44
59.26
65.05
98.42
monks-3
54.63
61.81
60.42
80.09
99.78
mushroom
100.00
100.00
100.00
100.00
99.98
musk-1
77.31
84.87
87.39
89.92
98.86
musk-2
97.21
98.61
98.91
99.27
99.83
nursery
99.75
99.91
99.78
100.00
99.56
oocytes-merluccius-nucleus-4d
86.27
83.14
82.35
83.92
94.29
oocytes-merluccius-states-2f
92.16
92.55
95.29
92.94
97.61
oocytes-trisopterus-nucleus-2f
81.14
82.02
79.82
82.46
96.13
oocytes-trisopterus-states-5b
93.86
96.05
93.42
94.74
99.26
optical
93.10
95.94
97.11
96.38
99.55
ozone
96.53
95.58
97.00
97.48
99.78
page-blocks
96.49
96.13
95.83
96.13
98.33
parkinsons
87.76
89.80
89.80
85.71
99.47
pendigits
94.40
97.11
97.06
97.37
99.79
pima
71.88
73.44
75.52
69.79
84.47
pittsburg-bridges-MATERIAL
88.46
92.31
88.46
92.31
99.33
pittsburg-bridges-REL-L
76.92
73.08
69.23
73.08
97.66
pittsburg-bridges-SPAN
60.87
69.57
69.57
73.91
95.09
pittsburg-bridges-T-OR-D
84.00
84.00
84.00
84.00
99.67
pittsburg-bridges-TYPE
65.38
65.38
65.38
57.69
96.39
planning
66.67
55.56
68.89
60.00
93.45
plant-margin
50.50
79.50
81.25
83.25
98.18
plant-shape
39.00
66.50
72.75
72.50
81.77
plant-texture
51.75
75.25
81.25
81.00
99.14
post-operative
40.91
63.64
72.73
68.18
95.64
primary-tumor
54.88
47.56
52.44
53.66
79.04
ringnorm
90.43
85.35
97.51
97.57
98.42
seeds
92.31
96.15
88.46
92.31
98.52
semeion
74.37
92.71
91.96
96.73
99.12
soybean
77.93
88.83
85.11
88.03
99.48
spambase
93.57
94.17
94.09
94.87
98.14
spect
62.90
63.44
63.98
62.37
90.87
spectf
91.98
91.98
49.73
89.30
99.66
statlog-australian-credit
65.12
63.37
59.88
61.05
78.57
statlog-german-credit
72.40
72.40
75.60
72.00
97.52
statlog-heart
85.07
91.04
92.54
92.54
93.88
statlog-image
95.15
96.88
95.49
97.75
99.00
statlog-landsat
87.55
89.25
91.00
89.90
96.39
statlog-shuttle
99.92
99.92
99.90
99.91
99.96
statlog-vehicle
78.67
77.25
80.09
81.04
95.04
steel-plates
73.61
76.49
78.35
77.53
94.86
synthetic-control
94.00
98.00
98.67
99.33
99.76"
REFERENCES,0.9904988123515439,Continued on next page
REFERENCES,0.9928741092636579,Published as a conference paper at ICLR 2022
REFERENCES,0.995249406175772,"DSName
HANN15
HANN100
SNN
DENN
HANN100trn"
REFERENCES,0.997624703087886,"teaching
57.89
57.89
50.00
57.89
78.35
thyroid
98.37
98.25
98.16
98.22
99.65
tic-tac-toe
96.65
97.07
96.65
98.33
99.84
titanic
78.73
78.73
78.36
78.73
78.61
trains
100.00
50.00
NaN
NaN
100.00
twonorm
97.30
98.27
98.05
98.16
99.51
vertebral-column-2clases
88.31
85.71
83.12
85.71
93.85
vertebral-column-3clases
81.82
80.52
83.12
80.52
95.40
wall-following
92.45
94.79
90.98
91.86
98.16
waveform
85.84
84.00
84.80
83.92
95.14
waveform-noise
84.72
84.96
86.08
84.32
96.39
wine
97.73
100.00
97.73
100.00
99.80
wine-quality-red
62.50
65.00
63.00
63.50
90.25
wine-quality-white
54.82
61.03
63.73
62.25
81.79
yeast
59.03
60.65
63.07
58.22
68.48
zoo
96.00
96.00
92.00
100.00
99.68"
