Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002932551319648094,"Graph-based next-step prediction models have recently been very successful in
modeling complex high-dimensional physical systems on irregular meshes. How-
ever, due to their short temporal attention span, these models suffer from error
accumulation and drift. In this paper, we propose a new method that captures
long-term dependencies through a transformer-style temporal attention model. We
introduce an encoder-decoder structure to summarize features and create a com-
pact mesh representation of the system state, to allow the temporal model to op-
erate on a low-dimensional mesh representations in a memory efﬁcient manner.
Our method outperforms a competitive GNN baseline on several complex ﬂuid
dynamics prediction tasks, from sonic shocks to vascular ﬂow. We demonstrate
stable rollouts without the need for training noise and show perfectly phase-stable
predictions even for very long sequences. More broadly, we believe our approach
paves the way to bringing the beneﬁts of attention-based sequence models to solv-
ing high-dimensional complex physics tasks."
INTRODUCTION,0.005865102639296188,"1
INTRODUCTION"
INTRODUCTION,0.008797653958944282,"There has been an increasing interest in many scientiﬁc disciplines, from computational ﬂuid dy-
namics [3, 40] over graphics [43, 41] to quantum mechanics [21, 1], to accelerate numerical simu-
lation using learned models. In particular, methods based on Graph Neural Networks (GNN) have
shown to be powerful and ﬂexible. These methods can directly work with unstructured simulation
meshes, simulate systems with complex domain boundaries, and adaptively allocate computation to
the spatial regions where it is needed [8, 39, 35, 52]."
INTRODUCTION,0.011730205278592375,"Most models for complex physics prediction tasks, in particular those on unstructured meshes, are
next-step prediction models; that is, they predict the next state u(t+1, x) of a physical system from
the current state u(t, x). As next-step models suffer from error accumulation, mitigation strategies
such as training noise [39] or augmented training data using a solver-in-the-loop [42] have to be used
to keep rollouts stable. These remedies are not without drawbacks– training noise can be hard to
tune, and ultimately place a bound on the achievable model accuracy. And worse, next-step models
also tend to show drift, which is not as easily mitigated. Failure examples include failure to conserve
volume or energy, shift in phase, or loss of shape information (see e.g., the failure case example in
[39])."
INTRODUCTION,0.01466275659824047,"On the other hand, auto-regressive sequence models such as Recurrent neural networks (RNNs), or
more recently transformers, have been hugely successful in predicting sequences in NLP and image
applications [36, 31]. They can capture stochastic dynamics and work with partial observations
[50]. Furthermore, their long attention span allows them to better preserve phase and conserved
quantities [18]. However, as memory cost for full-sequence transformer models scales with both"
INTRODUCTION,0.017595307917888565,∗Equal contribution.
INTRODUCTION,0.020527859237536656,Published as a conference paper at ICLR 2022
INTRODUCTION,0.02346041055718475,"sequence length and spatial extents, it is hard to directly extend such models to predict physical
systems deﬁned on large unstructured meshes."
INTRODUCTION,0.026392961876832845,"This paper combines powerful GNNs and a transformer to model high-dimensional physical systems
on meshes. The key idea is creating a locally coarsened yet more expressive graph, to limit memory
consumption of the sequence model, and allow effective training. We ﬁrst use a GNN to aggregate
local information of solution ﬁelds of a dynamic system into node representations by performing
several rounds of message passing, and then coarsen the output graph to a small set of pivotal nodes.
The pivotal nodes’ representations form a latent that encodes the system state in a low-dimensional
space. We apply a transformer model on this latent, with attention over the whole sequence, and
predict the latent for the next step. We then use a second GNN to recover the full-sized graph by
up-sampling and message passing. This procedure of solving on a coarse scale, upsampling, and
performing updates on a ﬁne-scale is related to a V-cycle in multigrid methods [4]."
INTRODUCTION,0.02932551319648094,"We show that this approach can outperform the state-of-the-art MeshGraphNets[35] baseline on
accuracy over a set of challenging ﬂuid dynamics tasks. We obtain stable rollouts without the need
to inject training noise, and unlike the baseline, do not observe strong error accumulation or drift in
the phase of vortex shedding."
RELATED WORK,0.03225806451612903,"2
RELATED WORK"
RELATED WORK,0.03519061583577713,"Developing and running simulations of complex, high-dimensional systems can be very time-
intensive. Particular for computational ﬂuid dynamics, there is considerable interest in using neural
networks for accelerating simulations of aerodynamics [3] or turbulent ﬂows [22, 46]. Fast predic-
tions and differentiable learned models can be useful for tasks from airfoil design [40] over weather
prediction [37] to visualizations for graphics [43, 41, 7]. While many of these methods use convo-
lutional networks on 2D grids, recently GNN-based learned simulators, in particular, have shown to
be a very ﬂexible approach, which can model a wide range of systems, from articulated dynamics
[38] to dynamics of particle systems [24, 39]. In particular, GNNs naturally enable simulating on
meshes with irregular or adaptive resolution [35, 8]."
RELATED WORK,0.03812316715542522,"These methods are either steady-state or next-step prediction models. There are, however, strong
beneﬁts of training on the whole predicted sequence: next-step models tend to drift and accumu-
late error, while sequence models can use their long temporal range to detect drift and propagate
gradients through time to prevent error accumulation. Models based on RNNs have been success-
fully applied to 1D time series and small n-body systems [6, 54] or small 2D systems [50]. More
recently, transformer models [44], which have been hugely successful for NLP tasks [36], are be-
ing applied to low-dimensional physics prediction tasks [18]. However, applying sequence mod-
els to predict high-dimensional systems remains a challenge due to their high memory overhead.
Dimensionality reduction techniques, such as CNN autoencoders [34, 33, 27, 23, 30, 17, 12, 28],
POD [45, 49, 5, 32, 19, 9, 48, 11], or Koopman operators [25, 10, 15] can be used to construct a low-
dimensional latent space. The auto-regressive sequence model then operates on these linear (POD
modes) or nonlinear (CNNs) latents. However, these methods cannot directly handle unstructured
data with moving or varying-size meshes, and many of them do not consider parameter variations.
For example, POD cannot operate on state vectors with different lengths (e.g., variable mesh sizes
data in Fig.2 bottom). On the other hand, CNN auto-encoders can only be applied to rectangular
domains and uniform grids; and rasterization of complex simulation domains to uniform grids are
known to be inefﬁcient and are linked to many drawbacks as discussed in [13]."
RELATED WORK,0.04105571847507331,"In contrast, our method reduces the input space locally by aggregating information on a coarsened
graph. This plays to the strength of GNNs to learn local universal rules and is very closely related
to multi-level methods [51, 16] and graph coarsening [14, 53, 2]."
METHODOLOGY,0.04398826979472141,"3
METHODOLOGY"
PROBLEM DEFINITION AND OVERVIEW,0.0469208211143695,"3.1
PROBLEM DEFINITION AND OVERVIEW"
PROBLEM DEFINITION AND OVERVIEW,0.04985337243401759,"We are interested in predicting spatiotemporal dynamics of complex physical systems (e.g., ﬂuid dy-
namics), usually governed by a set of nonlinear, coupled, parameterized partial differential equations"
PROBLEM DEFINITION AND OVERVIEW,0.05278592375366569,Published as a conference paper at ICLR 2022
PROBLEM DEFINITION AND OVERVIEW,0.05571847507331378,Multi-Head Attention
PROBLEM DEFINITION AND OVERVIEW,0.05865102639296188,Layer Normalization
PROBLEM DEFINITION AND OVERVIEW,0.06158357771260997,Fully-Connected Layer
PROBLEM DEFINITION AND OVERVIEW,0.06451612903225806,Layer Normalization
PROBLEM DEFINITION AND OVERVIEW,0.06744868035190615,"Representation
Select Nodes
Encoder
Yt"
PROBLEM DEFINITION AND OVERVIEW,0.07038123167155426,"Yt+1
Initialize pivotal nodes
Decoder
Interpolation"
PROBLEM DEFINITION AND OVERVIEW,0.07331378299120235,"…
z−1
z0
zt−1
zt zt+1 μ"
PROBLEM DEFINITION AND OVERVIEW,0.07624633431085044,"Figure 1: The diagram of the proposed model, GMR-Transformer-GMUS. We ﬁrst represent the
domain as a graph and then select pivotal nodes (red/green/yellow) to encode information over the
entire graph. The encoder GMR runs Message passing along graph edges so that the pivotal nodes
collect information from nearby nodes. The latent vector zt summarizes information at the pivotal
nodes, and represents the whole domain at the current step. The transformer will predict zt+1 based
on all previous state latent vectors. Finally, we decode zt+1 through message passing to obtain the
next-step prediction Yt+1."
PROBLEM DEFINITION AND OVERVIEW,0.07917888563049853,"(PDEs) as shown in its general form,"
PROBLEM DEFINITION AND OVERVIEW,0.08211143695014662,"∂u(x, t)"
PROBLEM DEFINITION AND OVERVIEW,0.08504398826979472,"∂t
= F

u(x, t); µ

,
x, t ∈Ω× [0, Te],
(1)"
PROBLEM DEFINITION AND OVERVIEW,0.08797653958944282,"where u(x, t) ∈Rd represents the state variables (e.g., velocity, pressure, or temperature) and
F is a partial differential operator parameterized by µ. Given initial and boundary conditions,
unique spatiotemporal solutions of the system can be obtained. One popular method of solving these
PDE systems is the ﬁnite volume method (FVM) [29], which discretizes the simulation domain Ω
into an unstructured mesh consisting of N cells (Ci : i = 1, . . . , N). At time t, the discrete
state ﬁeld Yt = {ui,t : i = 1, . . . , N} can thus be deﬁned by the state value ui,t at each cell
center. Traditionally, solving this discretized system involves sophisticated numerical integration
over time and space. This process is often computationally expensive, making it infeasible for real-
time predictions or applications requiring multiple model queries, e.g., optimization and control.
Therefore, our goal is to learn a simulator, which, given the initial state Y0 and system parameters
µ, can rapidly produce a rollout trajectory of states Y1...YT ."
PROBLEM DEFINITION AND OVERVIEW,0.09090909090909091,"As mentioned above, solving these systems with high spatial resolution by traditional numerical
solvers can be quite expensive. In particular, propagating local updates over the ﬁne grid, such
as pressure updates in incompressible ﬂow, can require many solver iterations. Commonly used
technique to improve the efﬁciency include multigrid methods [47], which perform local updates
on both the ﬁne grid, as well as one or multiple coarsened grids with fewer nodes, to accelerate the
propagation of information. One building block of multigrid methods is the V-cycle, which consists
of down-sampling the ﬁne to a coarser grid, performing a solver update, up-sampling back on the
ﬁne grid, and performing an update on the ﬁne grid."
PROBLEM DEFINITION AND OVERVIEW,0.093841642228739,"Noting that GNNs excel at local updates while the attention mechanism over temporal sequences
allows long-term dependency (see remark A.2), we devise an algorithm inspired by the multigrid
V-cycle. For each time step, we use a GNN to locally summarize neighborhood information on our
ﬁne simulation mesh into pivotal nodes, which form a coarse mesh (section 3.2). We use temporal
attention over the entire sequence in this lower-dimensional space (section 3.3), upsample back onto
the ﬁne simulation, and perform local updates using a mesh recovery GNN (section 3.2). This com-
bination allows us to efﬁciently make use of long-term temporal attention for stable, high-ﬁdelity
dynamics predictions."
GRAPH REPRESENTATION AND MESH REDUCTION,0.0967741935483871,"3.2
GRAPH REPRESENTATION AND MESH REDUCTION"
GRAPH REPRESENTATION AND MESH REDUCTION,0.09970674486803519,"Graph Representation. We construct a graph G = (V, E) to represent a snapshot of a dynamic
system at time step t. Here each node i ∈V corresponds the mesh cell Ci, so the graph size is
|V| = N. The set of edges E are derived from neighboring relations of cells: if two cells Ci and Cj"
GRAPH REPRESENTATION AND MESH REDUCTION,0.10263929618768329,Published as a conference paper at ICLR 2022
GRAPH REPRESENTATION AND MESH REDUCTION,0.10557184750733138,"are neighbors, then two directional edges (i, j) and (j, i) are both in E. We ﬁx this graph for all time
steps. At each step t, each node i ∈V uses the local state vector ui,t as its attribute. Therefore,
(G, (Y0, . . . , YT )) forms a temporal graph sequence. The goal of the learning model is to predict
(Y1, . . . , YT ) given G and Y0."
GRAPH REPRESENTATION AND MESH REDUCTION,0.10850439882697947,"Mesh Reduction. After representing the system as a graph, we use a GNN to summarize and extract
a low-dimensional representation zt from Yt for each step t. In this part of discussion, we omit the
subscript t for notational simplicity. We refer to the encoder as Graph Mesh Reducer (GMR) since
its role is coarsening the mesh graph. GMR ﬁrst selects a small set S ⊆V of pivotal graph nodes and
locally encodes the information of the entire graph into representations at these nodes. By operating
on rich, summarized node representations, the dynamics of the entire system is well-approximated
even on this coarser graph."
GRAPH REPRESENTATION AND MESH REDUCTION,0.11143695014662756,"There are a few considerations for selection of pivotal nodes, for example, the spatial spread and the
centrality of nodes in the selection. We generally use uniform sampling to select S from V. This
effectively preserves the density of graph nodes over the simulation domain, i.e. pivotal nodes are
more concentrated in important regions of the simulation domain. More details and visualizations
on the node selection process can be found in section A.6."
GRAPH REPRESENTATION AND MESH REDUCTION,0.11436950146627566,"GMR is implemented as a Encode-Process-Decode (EPD) GraphNet [39]. GMR ﬁrst extracts node
features and edge features from the system state using the node and edge Multi-Layer Perceptrons
(MLPs)."
GRAPH REPRESENTATION AND MESH REDUCTION,0.11730205278592376,"v0
i = mlpv(Y [i]),
e0
ij = mlpe(p(i) −p(j)).
(2)"
GRAPH REPRESENTATION AND MESH REDUCTION,0.12023460410557185,"Here Y [i] is the i-th row of Y , i.e. the state vector at each node, and p(i) is the spatial position of
cell Ci."
GRAPH REPRESENTATION AND MESH REDUCTION,0.12316715542521994,"Then GMR uses L GraphNet processer blocks [38] to further reﬁne node representations through
message passing. In this process a node can receive and aggregate information from all neighboring
nodes within graph distance L. Each processor updates the node and edge representations as"
GRAPH REPRESENTATION AND MESH REDUCTION,0.12609970674486803,"eℓ
ij = mlpe
ℓ
 
eℓ−1
ij , vℓ−1
i
, vℓ−1
j

,
vℓ
i = mlpv
ℓ "
GRAPH REPRESENTATION AND MESH REDUCTION,0.12903225806451613,"vℓ−1
i
,
X"
GRAPH REPRESENTATION AND MESH REDUCTION,0.13196480938416422,"j∈Ni
eℓ−1
ij "
GRAPH REPRESENTATION AND MESH REDUCTION,0.1348973607038123,",
ℓ= 1, . . . , L.
(3)"
GRAPH REPRESENTATION AND MESH REDUCTION,0.1378299120234604,"Here v0
i , e0
ij are the outputs of Equation 2, and Ni denotes all neighbors of node i. The two functions
mlpe
ℓ(·) and mlpv
ℓ(·) respectively concatenate their arguments as vectors and then apply MLPs. The
calculation in equation 2 and equation 3 computes a new set of node representations V = (vL
i : i ∈
V) for the graph G."
GRAPH REPRESENTATION AND MESH REDUCTION,0.14076246334310852,"Finally GMR applies an MLP to representations of the pivotal nodes in S only to “summarize” the
entire graph onto a coarse graph:"
GRAPH REPRESENTATION AND MESH REDUCTION,0.1436950146627566,"hi = mlpr(vL
i ),
i ∈S
(4)"
GRAPH REPRESENTATION AND MESH REDUCTION,0.1466275659824047,"We concatenate these vectors into a single latent z = concat(hi : i ∈S) as reduced vector represen-
tation of the entire graph. We collectively denote these three computation steps as z = GMR(G, Y ).
The latents z can be computed independently for each time step t and will be used as the represen-
tation for the attention-based simulator."
GRAPH REPRESENTATION AND MESH REDUCTION,0.1495601173020528,"Mesh Recovery To recover the system state from the coarse vector representation z, we deﬁne a
Graph Mesh Up-Sampling (GMUS) network. The key step of GMUS is to restore information on
the full graph from representations of pivotal nodes. This procedure is the inverse of operation of
GMR. We ﬁrst set the representations at pivotal nodes by splitting z, that is, ri = hi, i ∈S. We
then compute representations of non-pivotal nodes by spatial interpolation [2]: for a non-pivotal
node j, we choose a set N ′
j of k nearest pivotal nodes in terms of spatial distance and compute the
its representations rj by"
GRAPH REPRESENTATION AND MESH REDUCTION,0.15249266862170088,"rj =
X"
GRAPH REPRESENTATION AND MESH REDUCTION,0.15542521994134897,"i∈N ′
j"
GRAPH REPRESENTATION AND MESH REDUCTION,0.15835777126099707,"wijhi
P"
GRAPH REPRESENTATION AND MESH REDUCTION,0.16129032258064516,"i∈N ′
j wij
,
wij =
1
d(j, i)2
(5)"
GRAPH REPRESENTATION AND MESH REDUCTION,0.16422287390029325,"Here d(j, i) is the spatial distance between cells Cj and Ci. Then every node i ∈V has a represen-
tation ri, and all nodes’ representations are collectively denoted as R = (ri : i ∈V)."
GRAPH REPRESENTATION AND MESH REDUCTION,0.16715542521994134,Published as a conference paper at ICLR 2022
GRAPH REPRESENTATION AND MESH REDUCTION,0.17008797653958943,"Similar to GMR, GMUS applies EPD GraphNet to the initial node representation R to restore Y
on the full graph G. We denote the chain of operations so far as ˆY = GMUS(G, z). Details about
information ﬂows in GMR and GMUS can be found in section A.1."
GRAPH REPRESENTATION AND MESH REDUCTION,0.17302052785923755,"We train GMR and GMUS as an auto-encoder over all time steps and sequences. For each time step
t, we compute ˆYt = GMUS(G, GMR(G, Yt)) and minimize the reconstruction loss"
GRAPH REPRESENTATION AND MESH REDUCTION,0.17595307917888564,"Lgraph = T
X"
GRAPH REPRESENTATION AND MESH REDUCTION,0.17888563049853373,"n=1
∥Yt −ˆYt∥2
2
.
(6)"
GRAPH REPRESENTATION AND MESH REDUCTION,0.18181818181818182,"With these two modules we can encode system states (Y1, . . . , YT ) to latents (z1, . . . , zT ) as a low-
dimensional representation of system dynamics. In the next section, we train a transformer model
to predict the sequences of latents."
ATTENTION-BASED SIMULATOR,0.18475073313782991,"3.3
ATTENTION-BASED SIMULATOR"
ATTENTION-BASED SIMULATOR,0.187683284457478,"As a dynamics model, we learn a simulator which can predict the sequence (z1, . . . , zT ) autore-
gressively based on an initial state z0 = GMR(G, Y0) and the system parameters µ. The system
parameters include conditions such as different Reynolds numbers, initial temperatures, or/and pa-
rameters of the spatial domain (see Table 3 for details). The model is conditioned on µ, to be able
to predict ﬂows with arbitrary system parameters at test time.We encode µ into a special parameter
token z−1 of the same length as the state representation vectors zt
z−1 = mlpp(µ).
(7)
Then, the transformer model [44], trans(·) predicts the subsequent latent vectors in an autoregres-
sive manner.
˜z1 = trans(z−1, z0),
˜zt = trans(z−1, z0, ˜z1, . . . , ˜zt−1)
(8)
Speciﬁcally, we use a single layer of multi-head attention in our transformer model, which we found
sufﬁciently powerful for the environments we studied. We ﬁrst describe the computation of a single
attention head: At step t, the prediction ˜zt−1 from the previous step issues a query to compute
attention a(t−1) over latents (z−1, z0, ˜z1, . . . , ˜zt−1)."
ATTENTION-BASED SIMULATOR,0.1906158357771261,"a(t−1) = softmax
 ˜z⊤
t−1W ⊤
1 W2
√"
ATTENTION-BASED SIMULATOR,0.1935483870967742,"d′
·

z−1, z0, ˜z1, . . . , ˜zt−1,

(9)"
ATTENTION-BASED SIMULATOR,0.19648093841642228,"Here W1 and W2 are learnable parameters, and d′ denotes the length of the vector zt−1. This
equation corresponds the popular inner-product attention model [44]."
ATTENTION-BASED SIMULATOR,0.19941348973607037,"Next, the attention head computes the prediction vector
gt = W3

z−1, z0, ˜z1, . . . , ˜z(t−1)

at−1,
(10)
with the learned parameter W3."
ATTENTION-BASED SIMULATOR,0.20234604105571846,"Our multi-head attention model uses K parallel attention heads with separate parameters to compute
K vectors (g1
t , . . . , gK
t ) as described above. These vectors are concatenated and fed into an MLP to
predict the residual of ˜zt over ˜zt−1.
˜zt = ˜zt−1 + mlpm
 
concat(g1
t , . . . , gK
t )

(11)"
ATTENTION-BASED SIMULATOR,0.20527859237536658,We train the autoregressive model on entire sequences by minimizing the prediction loss
ATTENTION-BASED SIMULATOR,0.20821114369501467,"Latt = T
X"
ATTENTION-BASED SIMULATOR,0.21114369501466276,"t=1
∥zt −˜zt∥2
2.
(12)"
ATTENTION-BASED SIMULATOR,0.21407624633431085,"Compared to next-step models such as MeshGraphNet [35], this model can propagate gradients
through the entire sequence, and can use information from all previous steps to produce stable
predictions with minimal drift and error accumulation."
ATTENTION-BASED SIMULATOR,0.21700879765395895,"The proposed model also contains next-step models as special cases: if we select all nodes as pivotal
nodes and set the temporal attention model such that it predicts the next step with only physical
parameters and the current step, then the proposed model becomes a next-step model. The proposed
model shows advantage when it reduces the dimension of representation by using pivotal nodes and
expands the input range when predicting the next step. A.2 shows how this method reduces the
amount of computation and memory usage."
ATTENTION-BASED SIMULATOR,0.21994134897360704,Published as a conference paper at ICLR 2022
MODEL TRAINING AND TESTING,0.22287390029325513,"3.4
MODEL TRAINING AND TESTING"
MODEL TRAINING AND TESTING,0.22580645161290322,"We train the mesh reduction and recovery modules GMR, GMUS by minimizing equation 6 and
the temporal attention model by minimizing equation 12. We obtain best results by training those
components separately, as this provides stable targets for the autoregressive predictive module. A
second reason is memory and computation efﬁciency; we can train the temporal attention method
on full sequences in reduced space, without needing to jointly train the more memory-intensive
GMR/GMUS modules."
MODEL TRAINING AND TESTING,0.2287390029325513,"The transformer model is trained by incrementally including loss terms computed from different
time steps. For every time step t, we train the transformer by minimizing the objective Pt
t′=1 ∥zt′ −
˜zt′∥2
2. Until the objective reaches a threshold, we add the next loss term ∥zt+1 −˜zt+1∥to the
objective and continue to train the model We found that this incremental approach leads to much
more stable training than directly minimizing the overall objective."
MODEL TRAINING AND TESTING,0.2316715542521994,"As the transformer model can directly attend to all previous steps via equation 10, we don’t suffer as
much from vanishing gradients, compared to e.g. LSTM sequence models. By predicting in mesh-
reduced space, the memory footprint is limited, and the model can be trained without approximate
gradient calculations such as teacher forcing or limiting the attention history. Details for training
procedure can be found in section A.3."
MODEL TRAINING AND TESTING,0.23460410557184752,"Once we have trained our models, we can make predictions for a new problem. We ﬁrst represent
system parameters as a token z−1 by equation 7 and encode the initial state into a token z0 =
GMR(G, Y0), then we predict the sequence ˜z1, . . . , ˜zT by equation 8, and ﬁnally we decode the
system state ˜Y by"
MODEL TRAINING AND TESTING,0.2375366568914956,"˜Yt = GMUS(˜zt),
t = 1, . . . , T.
(13)"
RESULTS,0.2404692082111437,"4
RESULTS"
RESULTS,0.2434017595307918,"Datasets. We tested the proposed method to three datasets of ﬂuid dynamic systems: (1) ﬂow
over a cylinder with varying Reynolds number; (2) high-speed ﬂow over a moving wedge with
different initial temperatures; (3) ﬂow in different vascular geometries. These datasets correspond to
applications with ﬁxed, moving, and varying unstructured mesh, respectively. A detailed description
of the datasets can be found in section A.4."
RESULTS,0.24633431085043989,"Methods. The proposed methods is compared to the state-of-the-art MeshGraphNet method [35],
which is a next-step model and has been shown to outperform a list of previous methods. Two
versions of MeshGraphNet are considered here, with and without Noise Injection (NI). We also study
variants of our model with LSTM and GRU (GMR-LSTM and GMR-GRU) instead of temporal
attention. These variants also operate in mesh-reduced space, and use the same encoder (GMR) and
decoder (GMUS) as our main method. Model details can be found in section A.5. And section A.6
shows pivotal nodes for each dataset."
RESULTS,0.24926686217008798,"Stable and visually accurate model predictions. We tested the proposed method on three ﬂuid
systems. Figure 2 compares our model’s predictions for velocity against the ground truth (CFD)
on six test set trajectories. Our model shows stable predictions even without noise injection, and
model rollouts are visually very closely to the CFD reference. On the other hand, without mitiga-
tion strategies, next-step models often struggle to predict such long sequences stably and accurately.
For cylinder ﬂow, our model can successfully learn the varying dynamics under different Reynolds
numbers, and accurately predict their corresponding transition behaviors from laminar to vortex
shedding. In supersonic ﬂow over a moving wedge, our model can accurately capture the shock
bouncing back and forth on a moving mesh. In particular, the small discontinuities at the leeward
side of the edge are discernible in our model predictions. Lastly, in vascular ﬂow with a variable-size
circular thrombus, and thus different mesh sizes, the model is able to predict the ﬂow accurately. In
both cylinder ﬂow and vascular ﬂow, both frequency and phase of vortex shedding is accurately cap-
tured. The excellent agreement between model predictions and reference simulation demonstrates
the capability of our model when predicting long sequences with varying-size meshes. Results for
pressure can be found in section A.9."
RESULTS,0.25219941348973607,Published as a conference paper at ICLR 2022
RESULTS,0.25513196480938416,"Re = 307 (Cylinder Flow)
Re = 993 (Cylinder Flow)
t=0
t=85
t= 215
t=298
t=0
t=85
t= 215
t=298"
RESULTS,0.25806451612903225,"Truth
Ours"
RESULTS,0.26099706744868034,"T(0) = 201 (Sonic Flow)
T(0) = 299 (Sonic Flow)
t=0
t=15
t= 30
t=40
t=0
t=15
t= 30
t=40"
RESULTS,0.26392961876832843,"Truth
Ours"
RESULTS,0.2668621700879765,"R = 0.31 (Vascular ﬂow)
R = 0.49 (Vascular ﬂow)
t=40
t=80
t= 160
t=250
t=40
t=80
t= 160
t=250"
RESULTS,0.2697947214076246,"Truth
Ours"
RESULTS,0.2727272727272727,"Figure 2: Contours of the velocity ﬁeld, as predicted by our model versus the ground truth (CFD).
Our model accurately predicts long rollout sequences under varying system parameters."
RESULTS,0.2756598240469208,"Table 1: The average relative rollout error of three systems, with unit of ×10−3. We compare
MeshGraphNet with or without noise injection (NI), to three variants of the our model (LSTM,
GRU, or Transformer), which all shared the same GMR/GMUS mesh reduction models. Our model
signiﬁcantly outperform MeshGraphNet on datasets with long rollouts."
RESULTS,0.2785923753665689,"Dataset-rollout step
Cylinder ﬂow-400
Sonic ﬂow-40
Vascular ﬂow-250
Variable
u
v
p
u
v
p
T
u
v
p"
RESULTS,0.28152492668621704,"MeshGraphNet
NI
25
778
136
1.71 3.67 0.4
0.027
57
133
55
without NI
98 2036
673
4.12 6.13 0.24 0.020 3117 1771
601"
RESULTS,0.2844574780058651,"Ours
GRU
114 1491
1340
1.34 4.59 0.59
0.37
8.2
11.2
23.6
LSTM
124 1537
1574
1.57 5.8 0.69
0.45
8.4
11.1
23.3
Transformer 4.9
89
38
0.95 2.8 0.43
0.39
7.3
10
22"
RESULTS,0.2873900293255132,"Error behavior under long rollouts To quantitatively assess the performances of our
models and baselines,
we compute the relative mean square error (RMSE), deﬁned as"
RESULTS,0.2903225806451613,"RMSE(ˆuprediction, ˆutruth) ="
RESULTS,0.2932551319648094,"P(ˆuprediction
i
−ˆutruth
i
)2
P(ˆuprediction
i
)2
, over the full rollout trajectory. Table 1 com-"
RESULTS,0.2961876832844575,"pares average prediction errors on all three domains. Our attention-based model outperforms base-
lines and model variants in most scenarios. For sonic ﬂow, which has 40 time steps, our model
performs better than MeshGraphNet when predicting velocity values (u and v). The quantities p and
T are relatively simple to predict, and all models show very low relative error rates (< 10−3), though
our model has slightly worse performance. The examples with long rollout sequences (predicting
cylinder ﬂow and vascular ﬂow) highlight the superior performance of our attention-based method
compared with baseline next-step models, and the RMSE of our method is signiﬁcantly lower than
baselines."
RESULTS,0.2991202346041056,"Figure 3 shows how error accumulates in different models. Our model has higher error for the ﬁrst
few iterations, which can be explained by the information bottleneck of mesh reduction. However,
the error only increases very slowly over time. MeshGraphNet on the other hand suffers from strong
error accumulation, rendering it less accurate for long-span predictions. Noise injection partially
addresses the issue, but error accumulation is still considerably higher compared to our model.
We can attribute this better long-span error behavior of our sequence model, to being able to pass
gradients to all previous steps, which is not possible in next-step models. We also note that the"
RESULTS,0.3020527859237537,Published as a conference paper at ICLR 2022
RESULTS,0.30498533724340177,"0
100
200
300
400 10−3 10−2 10−1 100"
RESULTS,0.30791788856304986,Time Step RMSE
RESULTS,0.31085043988269795,"0
10
20
30
40 10−4 10−3 10−2"
RESULTS,0.31378299120234604,Time Step
RESULTS,0.31671554252199413,"0
100
250 10−3 10−2 10−1 100"
RESULTS,0.3196480938416422,Time Step
RESULTS,0.3225806451612903,"Figure 3: Averaged error over all state variables on cylinder ﬂow (left), sonic ﬂow (middle) and vas-
cular ﬂow (right), for the models MeshGraphNets(MGN)(
), MGN-NI(
), Ours-GRU (
),
Ours-LSTM (
), Ours-Transformer (
). Our model, particularly the transformer, show much
less error accumulation compared to the next-step model."
RESULTS,0.3255131964809384,"Re = 405, t = 250 (Cylinder ﬂow)
R = 0.31, t = 228 (Vascular ﬂow)
Next-Step
Ours
Truth
Next-Step
Ours
Truth"
RESULTS,0.3284457478005865,"Figure 4: Predictions of the next-step MeshGraphNet model and our model, compared to ground
truth. The next-step model fails to keep the shedding frequency and show drifts on cylinder ﬂow.
On vascular ﬂow, we notice the left inﬂow diminishing over the time, while our model remains close
to the reference simulation."
RESULTS,0.3313782991202346,"transformer model performs much better compared to the GRU and LSTM model variants. We
believe its ability to directly attend to long-term history helps to stabilize gradients in training, and
also to identify recurrent features of the ﬂow, as discussed later in this section."
RESULTS,0.3343108504398827,"In Figure 4 we illustrate the difference in the models’ abilities to predict long rollouts. For cylinder
ﬂow, our model is able to retain phase information. MeshGraphNet on the other hand drifts to an
opposite vortex shedding phase at t = 250, though its prediction still looks plausible. For vascular
ﬂow, our model gives an accurate prediction at step t = 228, while the prediction of MeshGraphNet
is signiﬁcantly different from the ground-truth at this step: the left inﬂow diminishes, making the
prediction less physically plausible."
RESULTS,0.33724340175953077,"Diagnosis of state latents. The low-dimensional latent vectors, computed by the encoder-decoder
(GMR and GMUS), can be viewed as lossy compression of high-dimensional physical data on ir-
regular unstructured meshes. In our experiment, we observe the compression ratio is ≤4% while
the loss of the information measured by RMSE is below 1.5%. More details are in section A.7."
RESULTS,0.34017595307917886,"We also visualize state latents z-s from the cylinder ﬂow dataset and show that the model can distin-
guish rollouts from different Reynolds numbers. We ﬁrst use Principal Component Analysis (PCA)
to reduce the dimension of z-s to two and plot them in Figure 5 (left). Rollouts with two different
Reynolds numbers have two clearly distinctive trajectories. They eventually enters their respective
periodic phase. We also apply PCA directly to the system states (Y) and get 2-d latent vectors di-
rectly from the original data. We plot these vectors in Figure 5 (right). Latent vectors from PCA start
in the center and form a circle. The two trajectories from two different Reynolds numbers are hard
to distinguish, which makes it challenging for the learned model to capture parameter variations. Al-
though CNN-based embedding methods [15, 51, 30] are also nonlinear, they cannot handle irregular
geometries with unstructured meshes due to the limitation of classic convolution operations. Using
pivotal nodes combined with GNN learning, the proposed model is both ﬂexible in dealing data with
irregular meshes and effective in capturing state transitions in the system."
RESULTS,0.34310850439882695,"Diagnosis of attention weights. The attention vector at in equation 9 plays an important role in
predicting the state of the dynamic system. For example, if the system enters a perfect oscillating"
RESULTS,0.3460410557184751,Published as a conference paper at ICLR 2022
RESULTS,0.3489736070381232,"−1
−0.95
−0.9
−0.85
−0.8 0 1 C0 C1"
RESULTS,0.3519061583577713,"−1
−0.5
0
0.5
1
−1 −0.5 0 0.5 1 C0"
RESULTS,0.3548387096774194,"Figure 5: 2-D principle subspace of the latent vectors from
GMR (left) and PCA (middle) for ﬂow past cylinder system:
Re = 307 (
), Re = 993 (
) start from
and
respectively."
RESULTS,0.35777126099706746,"0
100
200
300
400
0.6 0.7 0.8 0.9 1"
RESULTS,0.36070381231671556,Time Step
RESULTS,0.36363636363636365,Parameter Token Attention
RESULTS,0.36656891495601174,"Figure 6: Attention values of
the parameter tokens versus
time, Re = 307 (
), and
Re = 993 (
)."
RESULTS,0.36950146627565983,"0.2
0.4
0 0.5 1"
RESULTS,0.3724340175953079,Frequency
RESULTS,0.375366568914956,Magnitude
RESULTS,0.3782991202346041,"0.2
0.4
0 0.5 1"
RESULTS,0.3812316715542522,Frequency
RESULTS,0.3841642228739003,"300
650
1,000 0.12 0.14 0.16"
RESULTS,0.3870967741935484,Reynolds Number
RESULTS,0.39002932551319647,Freq at Max Mag
RESULTS,0.39296187683284456,"Figure 7: Attention (
), CFD data (
). (left) Re versus frequency for attention and CFD data.
Fourier transform for both attention and CFD data at (middle) Re = 307 and (right) Re = 993."
RESULTS,0.39589442815249265,"stage, the model could directly look up and re-use the encoding from the last cycle as the prediction,
to make predictions with little deterioration or drift."
RESULTS,0.39882697947214074,"We can observe the way our model makes predictions via the attention magnitude over time. Figure 6
shows attention weights for the initial parameter token z−1, which encodes the system parameters
µ, over one cylinder ﬂow trajectory. In the initial transition phase, we see high attention to the
parameter token, as the initial velocity ﬁelds of different Reynold numbers are hard to distinguish.
As soon as vortex shedding begins around t = 120, and the ﬂow enters an oscillatory phase, and
attention shift away from the parameter token, and locks onto these cycles. This can be seen in
Figure 7 (left): we perform a Fourier analysis of the attention weights, and notice that the peak
of the attention frequency (blue) coincides with the vortex shedding frequency in the data (purple).
That is, our dynamics model learned to attend to previous cycles in vortex shedding, and can use this
information to make accurate predictions that keep both frequency and phase stable. Figure 7 (right)
shows that this observation holds for the whole range of Reynolds numbers, and their corresponding
vortex shedding frequencies."
RESULTS,0.40175953079178883,"For sonic ﬂow, the system state during the ﬁrst few steps is much easier to discern even without
knowing the system parameters µ, and hence, the attention to z−1 quickly decays. This shows that
the transformer can adjust the attention distribution to the most physically useful signals, whether
that is parameters or system state. Analysis on this, as well as the full attention weights maps can be
found in section A.10."
CONCLUSION,0.4046920821114369,"5
CONCLUSION"
CONCLUSION,0.40762463343108507,"In this paper we introduced a graph-based mesh reduction method, together with a temporal at-
tention module, to efﬁciently perform autoregressive prediction of complex physical dynamics. By
attending to whole simulation trajectories, our method can more easily identify conserved properties
or fundamental frequencies of the system, allowing prediction with higher accuracy and less drift
compared to next-step methods."
CONCLUSION,0.41055718475073316,Published as a conference paper at ICLR 2022
CONCLUSION,0.41348973607038125,ACKNOWLEDGEMENT
CONCLUSION,0.41642228739002934,"Han Gao and Jianxun Wang are supported by the National Science Foundation under award numbers
CMMI-1934300 and OAC-2047127. Li-Ping Liu are supported by NSF 1908617. Xu Han was also
supported by NSF 1934553."
REFERENCES,0.41935483870967744,REFERENCES
REFERENCES,0.4222873900293255,"[1] MS Albergo, G Kanwar, and PE Shanahan. Flow-based generative models for markov chain
monte carlo in lattice ﬁeld theory. Physical Review D, 100(3):034515, 2019."
REFERENCES,0.4252199413489736,"[2] Ferran Alet, Adarsh Keshav Jeewajee, Maria Bauza Villalonga, Alberto Rodriguez, Tomas
Lozano-Perez, and Leslie Kaelbling. Graph element networks: adaptive, structured compu-
tation and memory. In International Conference on Machine Learning, pp. 212–222. PMLR,
2019."
REFERENCES,0.4281524926686217,"[3] Saakaar Bhatnagar, Yaser Afshar, Shaowu Pan, Karthik Duraisamy, and Shailendra Kaushik.
Prediction of aerodynamic ﬂow ﬁelds using convolutional neural networks. Computational
Mechanics, 64(2):525–545, 2019."
REFERENCES,0.4310850439882698,"[4] Dietrich Braess and Wolfgang Hackbusch. A new convergence proof for the multigrid method
including the v-cycle. SIAM journal on numerical analysis, 20(5):967–975, 1983."
REFERENCES,0.4340175953079179,"[5] Ashesh Chattopadhyay, Pedram Hassanzadeh, and Devika Subramanian. Data-driven predic-
tions of a multiscale lorenz 96 chaotic system using machine-learning methods: reservoir com-
puting, artiﬁcial neural network, and long short-term memory network. Nonlinear Processes
in Geophysics, 27(3):373–389, 2020."
REFERENCES,0.436950146627566,"[6] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary
differential equations. arXiv preprint arXiv:1806.07366, 2018."
REFERENCES,0.4398826979472141,"[7] Xiaohui Chen, Xu Han, Jiajing Hu, Francisco JR Ruiz, and Liping Liu. Order matters: Prob-
abilistic modeling of node sequence for graph generation. arXiv preprint arXiv:2106.06189,
2021."
REFERENCES,0.44281524926686217,"[8] Filipe de Avila Belbute-Peres, Kevin Smith, Kelsey Allen, Josh Tenenbaum, and J Zico Kolter.
End-to-end differentiable physics for learning and control. In Advances in Neural Information
Processing Systems, pp. 7178–7189, 2018."
REFERENCES,0.44574780058651026,"[9] Hamidreza Eivazi, Hadi Veisi, Mohammad Hossein Naderi, and Vahid Esfahanian. Deep neu-
ral networks for nonlinear model order reduction of unsteady ﬂows. Physics of Fluids, 32(10):
105104, 2020."
REFERENCES,0.44868035190615835,"[10] Hamidreza Eivazi, Luca Guastoni, Philipp Schlatter, Hossein Azizpour, and Ricardo Vinuesa.
Recurrent neural networks and koopman-based frameworks for temporal predictions in a low-
order model of turbulence. International Journal of Heat and Fluid Flow, 90:108816, 2021."
REFERENCES,0.45161290322580644,"[11] Stefania Fresca and Andrea Manzoni. Pod-dl-rom: enhancing deep learning-based reduced
order models for nonlinear parametrized pdes by proper orthogonal decomposition. Computer
Methods in Applied Mechanics and Engineering, 388:114181, 2022."
REFERENCES,0.45454545454545453,"[12] Kai Fukami, Koji Fukagata, and Kunihiko Taira. Assessment of supervised machine learn-
ing methods for ﬂuid ﬂows. Theoretical and Computational Fluid Dynamics, 34(4):497–519,
2020."
REFERENCES,0.4574780058651026,"[13] Han Gao, Luning Sun, and Jian-Xun Wang. Phygeonet: physics-informed geometry-adaptive
convolutional neural networks for solving parameterized steady-state pdes on irregular domain.
Journal of Computational Physics, 428:110079, 2021."
REFERENCES,0.4604105571847507,"[14] Hongyang Gao and Shuiwang Ji. Graph u-nets. In international conference on machine learn-
ing, pp. 2083–2092. PMLR, 2019."
REFERENCES,0.4633431085043988,Published as a conference paper at ICLR 2022
REFERENCES,0.4662756598240469,"[15] Nicholas Geneva and Nicholas Zabaras. Transformers for modeling physical systems. arXiv
preprint arXiv:2010.03957, 2020."
REFERENCES,0.46920821114369504,"[16] Dirk Hartmann, Christian Lessig, Nils Margenberg, and Thomas Richter. A neural network
multigrid solver for the navier-stokes equations. arXiv preprint arXiv:2008.11520, 2020."
REFERENCES,0.47214076246334313,"[17] Kazuto Hasegawa, Kai Fukami, Takaaki Murata, and Koji Fukagata. Machine-learning-based
reduced-order modeling for unsteady ﬂows around bluff bodies of various shapes. Theoretical
and Computational Fluid Dynamics, 34(4):367–383, 2020."
REFERENCES,0.4750733137829912,"[18] Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh,
and Hyunjik Kim. Lietransformer: equivariant self-attention for lie groups. In International
Conference on Machine Learning, pp. 4533–4543. PMLR, 2021."
REFERENCES,0.4780058651026393,"[19] Pierre Jacquier, Azzedine Abdedou, Vincent Delmas, and Azzeddine Soula¨ımani.
Non-
intrusive reduced-order modeling using uncertainty-aware deep neural networks and proper
orthogonal decomposition: Application to ﬂood modeling. Journal of Computational Physics,
424:109854, 2021."
REFERENCES,0.4809384164222874,"[20] Hrvoje Jasak, Aleksandar Jemcov, Zeljko Tukovic, et al. Openfoam: A c++ library for complex
physics simulations. In International workshop on coupled methods in numerical dynamics,
volume 1000, pp. 1–20. IUC Dubrovnik Croatia, 2007."
REFERENCES,0.4838709677419355,"[21] Gurtej Kanwar, Michael S. Albergo, Denis Boyda, Kyle Cranmer, Daniel C. Hackett, S´ebastien
Racani`ere, Danilo Jimenez Rezende, and Phiala E. Shanahan. Equivariant ﬂow-based sampling
for lattice gauge theory. Phys. Rev. Lett., 125:121601, Sep 2020. doi: 10.1103/PhysRevLett.
125.121601."
REFERENCES,0.4868035190615836,"[22] Dmitrii Kochkov, Jamie A. Smith, Ayya Alieva, Qing Wang, Michael P. Brenner, and Stephan
Hoyer. Machine learning–accelerated computational ﬂuid dynamics. Proceedings of the Na-
tional Academy of Sciences, 118(21), 2021."
REFERENCES,0.4897360703812317,"[23] Angran Li, Ruijia Chen, Amir Barati Farimani, and Yongjie Jessica Zhang. Reaction diffusion
system prediction based on convolutional neural network. Scientiﬁc reports, 10(1):1–9, 2020."
REFERENCES,0.49266862170087977,"[24] Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, and Antonio Torralba. Learning
particle dynamics for manipulating rigid bodies, deformable objects, and ﬂuids. In Interna-
tional Conference on Learning Representations, 2019."
REFERENCES,0.49560117302052786,"[25] Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Deep learning for universal linear
embeddings of nonlinear dynamics. Nature communications, 9(1):1–10, 2018."
REFERENCES,0.49853372434017595,"[26] Luis F Guti´errez Marcantoni, Jos´e P Tamagno, and Sergio A Elaskar. High speed ﬂow simula-
tion using openfoam. Mec´anica Computacional, 31(16):2939–2959, 2012."
REFERENCES,0.501466275659824,"[27] Romit Maulik, Bethany Lusch, and Prasanna Balaprakash.
Reduced-order modeling of
advection-dominated systems with recurrent neural networks and convolutional autoencoders.
Physics of Fluids, 33(3):037106, 2021."
REFERENCES,0.5043988269794721,"[28] Masaki Morimoto, Kai Fukami, Kai Zhang, Aditya G Nair, and Koji Fukagata.
Convo-
lutional neural networks for ﬂuid ﬂow analysis: toward effective metamodeling and low-
dimensionalization. arXiv preprint arXiv:2101.02535, 2021."
REFERENCES,0.5073313782991202,"[29] Fadl Moukalled, L Mangani, Marwan Darwish, et al. The ﬁnite volume method in computa-
tional ﬂuid dynamics, volume 113. Springer, 2016."
REFERENCES,0.5102639296187683,"[30] Takaaki Murata, Kai Fukami, and Koji Fukagata. Nonlinear mode decomposition with convo-
lutional neural networks for ﬂuid dynamics. Journal of Fluid Mechanics, 882, 2020."
REFERENCES,0.5131964809384164,"[31] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,
and Dustin Tran. Image transformer. In International Conference on Machine Learning, pp.
4055–4064. PMLR, 2018."
REFERENCES,0.5161290322580645,Published as a conference paper at ICLR 2022
REFERENCES,0.5190615835777126,"[32] Suraj Pawar, SM Rahman, H Vaddireddy, Omer San, Adil Rasheed, and Prakash Vedula. A
deep learning enabler for nonintrusive reduced order modeling of ﬂuid ﬂows. Physics of Fluids,
31(8):085101, 2019."
REFERENCES,0.5219941348973607,"[33] Jiang-Zhou Peng, Siheng Chen, Nadine Aubry, Zhi-Hua Chen, and Wei-Tao Wu. Time-variant
prediction of ﬂow over an airfoil using deep neural network. Physics of Fluids, 32(12):123602,
2020."
REFERENCES,0.5249266862170088,"[34] Jiang-Zhou Peng, Siheng Chen, Nadine Aubry, Zhihua Chen, and Wei-Tao Wu. Unsteady
reduced-order model of ﬂow over cylinders based on convolutional and deconvolutional neural
network structure. Physics of Fluids, 32(12):123609, 2020."
REFERENCES,0.5278592375366569,"[35] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning
mesh-based simulation with graph networks. In International Conference on Learning Repre-
sentations, 2021."
REFERENCES,0.530791788856305,"[36] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Lan-
guage models are unsupervised multitask learners. 2019."
REFERENCES,0.533724340175953,"[37] Stephan Rasp and Nils Thuerey. Data-driven medium-range weather prediction with a resnet
pretrained on climate simulations: A new model for weatherbench. Journal of Advances in
Modeling Earth Systems, 13(2):e2020MS002405, 2021."
REFERENCES,0.5366568914956011,"[38] Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Ried-
miller, Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for
inference and control. In International Conference on Machine Learning, pp. 4470–4479.
PMLR, 2018."
REFERENCES,0.5395894428152492,"[39] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Pe-
ter Battaglia. Learning to simulate complex physics with graph networks. In International
Conference on Machine Learning, pp. 8459–8468. PMLR, 2020."
REFERENCES,0.5425219941348973,"[40] Nils Thuerey, Konstantin Weißenow, Lukas Prantl, and Xiangyu Hu. Deep learning methods
for reynolds-averaged navier–stokes simulations of airfoil ﬂows. AIAA Journal, 58(1):25–36,
2020."
REFERENCES,0.5454545454545454,"[41] Kiwon Um, Xiangyu Hu, and Nils Thuerey. Liquid splash modeling with neural networks. In
Computer Graphics Forum, volume 37, pp. 171–182. Wiley Online Library, 2018."
REFERENCES,0.5483870967741935,"[42] Kiwon Um, Robert Brand, Philipp Holl, Nils Thuerey, et al. Solver-in-the-loop: Learning from
differentiable physics to interact with iterative pde-solvers. arXiv preprint arXiv:2007.00016,
2020."
REFERENCES,0.5513196480938416,"[43] Benjamin Ummenhofer, Lukas Prantl, Nils Th¨urey, and Vladlen Koltun. Lagrangian ﬂuid
simulation with continuous convolutions. In International Conference on Learning Represen-
tations, 2020."
REFERENCES,0.5542521994134897,"[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural informa-
tion processing systems, pp. 5998–6008, 2017."
REFERENCES,0.5571847507331378,"[45] Pantelis R Vlachas, Wonmin Byeon, Zhong Y Wan, Themistoklis P Sapsis, and Petros
Koumoutsakos. Data-driven forecasting of high-dimensional chaotic systems with long short-
term memory networks. Proceedings of the Royal Society A: Mathematical, Physical and
Engineering Sciences, 474(2213):20170844, 2018."
REFERENCES,0.5601173020527859,"[46] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu.
Towards
physics-informed deep learning for turbulent ﬂow prediction, 2020."
REFERENCES,0.5630498533724341,"[47] Pieter Wesseling. Introduction to multigrid methods. Technical report, INSTITUTE FOR
COMPUTER APPLICATIONS IN SCIENCE AND ENGINEERING HAMPTON VA, 1995."
REFERENCES,0.5659824046920822,Published as a conference paper at ICLR 2022
REFERENCES,0.5689149560117303,"[48] Pin Wu, Junwu Sun, Xuting Chang, Wenjie Zhang, Rossella Arcucci, Yike Guo, and Christo-
pher C Pain. Data-driven reduced order model with temporal convolutional neural network.
Computer Methods in Applied Mechanics and Engineering, 360:112766, 2020."
REFERENCES,0.5718475073313783,"[49] Xuping Xie, Guannan Zhang, and Clayton G Webster. Non-intrusive inference reduced order
model for ﬂuids using deep multistep neural network. Mathematics, 7(8):757, 2019."
REFERENCES,0.5747800586510264,"[50] SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun
Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting.
In Advances in neural information processing systems, pp. 802–810, 2015."
REFERENCES,0.5777126099706745,"[51] Jiayang Xu and Karthik Duraisamy. Multi-level convolutional autoencoder networks for para-
metric prediction of spatio-temporal dynamics. Computer Methods in Applied Mechanics and
Engineering, 372:113379, 2020."
REFERENCES,0.5806451612903226,"[52] Jiayang Xu, Aniruddhe Pradhan, and Karthik Duraisamy.
Conditionally parameterized,
discretization-aware neural networks for mesh-based modeling of physical systems. arXiv
preprint arXiv:2109.09510, 2021."
REFERENCES,0.5835777126099707,"[53] Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure
Leskovec.
Hierarchical graph representation learning with differentiable pooling.
arXiv
preprint arXiv:1806.08804, 2018."
REFERENCES,0.5865102639296188,"[54] Ruiyang Zhang, Yang Liu, and Hao Sun. Physics-informed multi-lstm networks for metamod-
eling of nonlinear structures. Computer Methods in Applied Mechanics and Engineering, 369:
113226, 2020."
REFERENCES,0.5894428152492669,Published as a conference paper at ICLR 2022
REFERENCES,0.592375366568915,"A
APPENDIX"
REFERENCES,0.5953079178885631,"A.1
INFORMATION FLOW IN GMR AND GMUS"
REFERENCES,0.5982404692082112,"Graph Mesh Reducer
Graph Mesh Up-Sampling"
REFERENCES,0.6011730205278593,"Figure 8: Schematic of information ﬂow in the Graph Mesh Reducer (GMR) and Graph Mesh
Up-Sampling (GMUS). Red and blue nodes are pivotal nodes. The node size increase represents
information aggregation from surroundings."
REFERENCES,0.6041055718475073,"Figure 8 shows how the information ﬂows in Graph Mesh Reducer (GMR) and Graph Mesh Up-
Sampling (GMUS). This is analogous to a V-cycle in multigrid methods: Consider a graph with
seven nodes (|V| = 7) to be reduced and recovered by GMR and GMSU with two message-passing
layers. The increased size of pivotal nodes after a message-passing layer represents the information
aggregation from surrounding nodes."
REFERENCES,0.6070381231671554,"A.2
COMPUTATIONAL COMPLEXITY"
REFERENCES,0.6099706744868035,"The computation of the proposed model is from the three components: GMR, GMUS, and the
transformer. The graph G contains N nodes and also has O(N) edges because of its special structure.
Then GMR and GMUS as L-layer graph neural networks have computation time O(NLd∗), with
d∗being the maximum number of hidden units."
REFERENCES,0.6129032258064516,"The transformer takes time O(T · (d′)2) to make a prediction: a prediction needs to consider O(T)
previous steps, and the computation over a time step is O((d′)2). Note that d′ is the length of the
latent vector at each step."
REFERENCES,0.6158357771260997,"In our actual calculation, the transformer takes much less time than GMR or GMUS since the latent
dimension d′ is small."
REFERENCES,0.6187683284457478,"However, it would be very expensive if the model did not use GMR and directly applied a trans-
former to the original system states. In this case, d′ would be Nd, and the total complexity would
be O(TN 2d2), which would not be feasible for typical computation servers."
REFERENCES,0.6217008797653959,Published as a conference paper at ICLR 2022
REFERENCES,0.624633431085044,"(a) Cylinder Flow
(b) Sonic Flow
(c) Vascular Flow"
REFERENCES,0.6275659824046921,"Figure 9: Overview of datasets of (a) cylinder ﬂow, (b) sonic ﬂow, and (c) vascular ﬂow."
REFERENCES,0.6304985337243402,"A.3
MODEL TRAINING"
REFERENCES,0.6334310850439883,"The GMR/GMUS and temporal attention model are trained separately. We ﬁrst train GMR/GMUS
by minimizing the reconstruction loss Lgraph deﬁned by Eqn. 6. Then, the parameters of the trained
encoder-decoder are frozen, and only the parameters of the temporal attention model will be trained
by minimizing the prediction loss Latt (Eqn. 12) with early stopping. The parameter token encoder
mlpp is trained jointly with temporal attention. The training algorithm is detailed in Algorithm 1."
REFERENCES,0.6363636363636364,Algorithm 1 Training Process
REFERENCES,0.6392961876832844,"Input: Domain graph G, Node features over time [Y0, . . . , YT ], GMRθ, GMUSφ,Attentionω,
RMSE threshold R′, MLPψ, condition parameter µ
Output: Learned parameters θ, φ, ω and ψ
repeat"
REFERENCES,0.6422287390029325,"for Yt ∈[Y0, . . . , YT ] do"
REFERENCES,0.6451612903225806,"ˆYt = GMUSφ(GMRθ(Yt, G))
end for
Compute ∇θ,φ ←∇θ,φLgraph(θ, φ, [Y0, . . . , YT ], [ ˆY0, . . . , ˆYT ])
Update φ, θ using the gradients ∇φ, ∇θ
until convergence of the parameters (θ, φ)
[z0, z1, . . . , zT ] = GMRθ([Y0, . . . , YT ]), z−1 = MLPψ(µ)
for t ∈[1, 2, . . . , T] do"
REFERENCES,0.6480938416422287,repeat
REFERENCES,0.6510263929618768,"[ˆz1, . . . , ˆzt] = Attentionω([z−1, z0])
Compute ∇ω,ψ ←∇ω,ψLatt(ω, ψ, [z1, . . . , zt], [ˆz1, . . . , ˆzt])
Update ω, ψ using the gradients ∇ω, ∇ψ
until RMSE([z1, . . . , zt], [ˆz1, . . . , ˆzt]) < R′
end for"
REFERENCES,0.6539589442815249,"A.4
DATASET DETAILS"
REFERENCES,0.656891495601173,"Three ﬂow datasets, cylinder ﬂow, sonic ﬂow and vascular ﬂow, are used in our numerical ex-
periments, and the schematics of ﬂow conﬁgurations are shown in Figure 9. Cylinder and vascu-
lar ﬂow are governed by incompressible Navier-Stokes (NS) equations, whereas compressible NS
equations govern sonic ﬂow. The ground truth datasets are generated by solving the incompress-
ible/compressible NS equations based on the ﬁnite volume method (FVM). We use the open-source
FVM library OpenFOAM [20] to conduct all CFD simulations. All ﬂow domains are discretized by
triangle (tri) mesh cells and the mesh resolutions are adaptive to the richness of ﬂow features. The
case setup details can be found in Table 2. Both cylinder ﬂow and sonic ﬂow datasets consist of 51
training and 50 test trajectories, while vascular ﬂow consists of 11 training and 10 test trajectories."
REFERENCES,0.6598240469208211,Published as a conference paper at ICLR 2022
REFERENCES,0.6627565982404692,Table 2: Simulation details of datasets
REFERENCES,0.6656891495601173,"Dataset
PDEs
Cell type
Meshing
# nodes
# steps
δt (sec)
Cylinder ﬂow
Incompr. NS
tri
Fixed
1699
400
0.5
Sonic ﬂow
Compr. NS
tri
Moving
1900
40
5 × 10−7"
REFERENCES,0.6686217008797654,"Vascular ﬂow
Incompr. NS
tri
Small-varying
7561 (avg.)
250
0.5"
REFERENCES,0.6715542521994134,"Table 3: Input-output information of our networks: u, v, p, T, ρ, m denote X-axis velocity, Y-axis
velocity, pressure, temperature, density and cell volume, respectively. x denote the cell coordinates.
T,0, Re, r denote the initial temperature (sonic ﬂow), Reynolds number (cylinder ﬂow) and radius
of the thrombus (vascular ﬂow)."
REFERENCES,0.6744868035190615,Dataset
REFERENCES,0.6774193548387096,"GMR
node
input"
REFERENCES,0.6803519061583577,"GMR
edge
input"
REFERENCES,0.6832844574780058,"GMUS
node
output"
REFERENCES,0.6862170087976539,"Parameter
token model
input"
REFERENCES,0.6891495601173021,"Nodal
embed
dim"
REFERENCES,0.6920821114369502,"# selected
node"
REFERENCES,0.6950146627565983,"Cylinder ﬂow
ui, vi, pi, mi, Re"
REFERENCES,0.6979472140762464,"xi −xj,
|xi −xj|
ui, vi, pi
Re
4
256"
REFERENCES,0.7008797653958945,Sonic ﬂow
REFERENCES,0.7038123167155426,"ui, vi, pi, Ti, ρi
mi, T,0"
REFERENCES,0.7067448680351907,"xi −xj,
|xi −xj|"
REFERENCES,0.7096774193548387,"ui, vi, pi
Ti, ρi
T,0
4
256"
REFERENCES,0.7126099706744868,"Vascular ﬂow
ui, vi, pi, mi, r"
REFERENCES,0.7155425219941349,"xi −xj,
|xi −xj|
ui, vi, pi
r
2
400"
REFERENCES,0.718475073313783,"Here we summarize all input and outputs of the GMR/GMUS models, as well as the attention simu-
lator for each dataset. GMR encodes the high-dimensional system state onto the node presentations
on pivotal nodes, while GMR decodes latents back to the high-dimensional physical state (Figure
8). The parameter token encoder takes the physical parameter µ as input, and outputs a parameter
token. The attention model takes the parameter token, together with the latent vectors of states at all
previous time steps z, to predict the latents of the next time step. All input and output variables for
each dataset are detailed in Table 3."
REFERENCES,0.7214076246334311,"The governing equations for all ﬂuid dynamic cases can be summarized as follows, ∂ρ"
REFERENCES,0.7243401759530792,"∂t + ∇· (ρv) = 0, ∂(ρv)"
REFERENCES,0.7272727272727273,"∂t
+ ∇· (ρvv) = ∇· (µ∇v) −∇p + ∇· (µ(∇v)T ) −2"
REFERENCES,0.7302052785923754,"3∇(µ∇· v),"
REFERENCES,0.7331378299120235,"∂
∂t(ρcpT) + ∇· (ρcpvT) = ∇· (k∇T) + ρT Dcp"
REFERENCES,0.7360703812316716,Dt + Dp Dt −2
REFERENCES,0.7390029325513197,"3µΨ + µΦ, (14)"
REFERENCES,0.7419354838709677,"where v is the velocity vector, µ is the viscosity, k is the thermal conductivity, cp is the speciﬁc
heat of the ﬂuid and the deﬁnitions of the stream scalar Ψ, the potential scalar Φ are referred to the
chapter 3 in [29]. Only the sonic ﬂow (compressible ﬂow) involves solving the third equation."
REFERENCES,0.7448680351906158,"For cylinder ﬂow, the simulation mesh topology remains the same for all trajectories, and the
Reynolds number varies. In sonic ﬂow, topology also remains ﬁxed; however, the mesh node coor-
dinates change over the course of the simulation to simulate the moving ramp. Here, we vary the
initial temperature. Finally, for vascular ﬂow, we vary the radius of the thrombus, which means that
each trajectory will have a different mesh topology, and the simulation model should be able to work
with variable-sized inputs in this case."
REFERENCES,0.7478005865102639,"A.5
MODEL DETAILS"
REFERENCES,0.750733137829912,"Node and edge representations in our GraphNets are vectors of width 128. The node and edge
functions (mlpv, mlpe, mlpr) are MLPs with two hidden layers of size 128, and ReLU activation.
The input dimension of mlpv is based on the number of input features in each node (see details in
table 3), and the input dimension of mlpe is three due to the one-layer neighboring edge. We use
L = 3 GraphNet blocks for both GMR and GMUS. The node and edge functions mlpe
ℓand mlpv
ℓ
used in the GraphNet blocks are 3-Layer, ReLU-activated MLPs with hidden size of 128. The output"
REFERENCES,0.7536656891495601,Published as a conference paper at ICLR 2022
REFERENCES,0.7565982404692082,"(a)
(b)"
REFERENCES,0.7595307917888563,Figure 10: Flow past cylinder ﬂow system: (a) all cells in the FV mesh; (b) pivotal nodes
REFERENCES,0.7624633431085044,"(a)
(b)
(c)"
REFERENCES,0.7653958944281525,"(d)
(e)
(f)"
REFERENCES,0.7683284457478006,"Figure 11: Sonic ﬂow system: (a-c) all cells in the FV mesh for 3 different time steps; (d-f) pivotal
nodes for 3 different time steps."
REFERENCES,0.7712609970674487,"size is 128 for the all MLPs, except for the ﬁnal layer, which is of size 4 for Cylinder, Sonic and 2
for Vascular."
REFERENCES,0.7741935483870968,"We use a single layer and four attention heads in our transformer model. The embedding sizes of z
for each dataset (cylinder ﬂow, sonic ﬂow, and vascular ﬂow) are 1024, 1024, and 800, respectively.
This is calculated by #pivotal nodes × nout, in which nout is the output size of the last GraphNet
block. The mlpp used to encode the system parameter µ is a MLP with 2 hidden layers of size 100,
and with output dimensions that match z, i.e. 1024, 1024, and 800 for Cylinder Flow, Sonic Flow,
and Vascular Flow, respectively."
REFERENCES,0.7771260997067448,Table 4: Number of parameters for each model
REFERENCES,0.7800586510263929,"Dataset
MeshGraphNet
GRU
LSTM
Transformer
GMR-GMU
Cylinder ﬂow
2.2M
9.4M
11.5M
14.1M
1.2M
Sonic ﬂow
2.2M
9.4M
11.5M
14.1M
1.2M
Vascular ﬂow
2.2M
5.8M
7.0M
8.5M
1.2M"
REFERENCES,0.782991202346041,"A.6
PIVOTAL POINTS SELECTION"
REFERENCES,0.7859237536656891,"In general, pivotal nodes are selected by uniformly sampling from the entire mesh, as it preserves
the mesh density distribution, which is designed based on the ﬂow physics observed in training data.
For our datasets, we select 256 pivotal nodes out of 1699 cells for cylinder ﬂow, 256 pivotal nodes
out of 1900 cells for sonic ﬂow, and 400 pivotal nodes out of 7561 cells for vascular ﬂow. The
pivotal nodes in vascular ﬂow are manually reduced in the aneurysm region, where ﬂow features
are not rich. The pivotal nodes distributions for the three ﬂow systems are shown in Figures 10, 11,
and 12, respectively."
REFERENCES,0.7888563049853372,Published as a conference paper at ICLR 2022
REFERENCES,0.7917888563049853,"(a)
(b)"
REFERENCES,0.7947214076246334,"(c)
(d)"
REFERENCES,0.7976539589442815,"(e)
(f)"
REFERENCES,0.8005865102639296,"Figure 12: Vascular ﬂow system: all cells in the FV mesh (left), pivotal nodes (right), r = 0.3 (a-b),
r = 0.4 (c-d), r = 0.5 (e-f)."
REFERENCES,0.8035190615835777,Published as a conference paper at ICLR 2022
REFERENCES,0.8064516129032258,"Table 5: Report of data compressing: QoI data represent original data of the variables of interest;
Embedding represents the saved embedding vectors from the GMR (encoder); Decoder is the saved
GMUS model which is used to decode the embeddings; Compression Ratio is the ratio of QoI data
size to the sum of the embedding and decoder sizes. The RMSE is the relative mean square error of
the reconstructed data from embedding."
REFERENCES,0.8093841642228738,"Dataset
QoI Data
Embedding
Decoder
Compression Ratio
RMSE (×10−3)
Cylinder ﬂow
3570 MB
82.1MB
12.8MB
37
14.3
Sonic ﬂow
651MB
8.5MB
18 MB
25
1.11
Vascular ﬂow
1951MB
8.5MB
49.3 MB
33
10"
REFERENCES,0.8123167155425219,"A.7
DATA COMPRESSION"
REFERENCES,0.8152492668621701,"The Graph Mesh Reducer (GMR) and Graph Mesh Up-Sampling (GMUS) can be treated as an
encoder and decoder for compressing high-dimensional physical data on irregular unstructured
meshes. For any given mesh data (G, Y ), we can encode it into a latent vector z = GMR(G, Y )
through GMR. Therefore, a high-dimensional spatiotemporal trajectory (G, Y0, . . . , YT ) can be
compressed into a low-dimensional representation (G, z0, . . . , zT ), which signiﬁcantly reduces
memory for storage. Similar to other compressed sensing techniques, the original data can be re-
stored by GMUS from latent vectors. Table 5 shows that the decoder can signiﬁcantly reduce the
size of the original data to about 3% with a slight loss of accuracy after being restored by the de-
coder. Note that only the GMR/GMUS are included in the data compressing process, the transformer
dynamics model is not involved."
REFERENCES,0.8181818181818182,"A.8
INFERENCE TIME"
REFERENCES,0.8211143695014663,"Our ground truth solver uses the ﬁnite volume method to solve the governing PDEs. Speciﬁcally,
the ”pressure implicit with splitting of operators” (PISO) algorithm and ”semi-implicit method for
pressure linked equations” (SIMPLE) [29] are used to simulate the unsteady cylinder and vascular
ﬂows, and a PISO-based compressible solver [26] is used to simulate the high-speed sonic ﬂow. All
these traditional numerical methods involve a large number of iterations and time integration steps.
In contrast, the trained neural network model is able to make predictions without the need of sub-
timestepping. Hence, these predictions can be are signiﬁcantly faster than running the ground-truth
simulator. The online evaluation of our learned model at inference time only involves two steps,"
REFERENCES,0.8240469208211144,"Step 1:
ˆz1 = trans(z−1, z0),
ˆzt = trans(z−1, z0, ˆz1, . . . , ˆzt−1)"
REFERENCES,0.8269794721407625,"Step 2:
˜Yt = GMUS(˜zt),
t = 1, . . . , T
(15)"
REFERENCES,0.8299120234604106,"which are completely decoupled. We compare the evaluation cost of the learned model with the
FV-based numerical models in Table 6, and observe signiﬁcant speedups for all three datasets."
REFERENCES,0.8328445747800587,"Table 6: Wall-clock time of ﬁnite volume (FV) model and attention-GMUS to simulate a trajectory
for three ﬂow cases."
REFERENCES,0.8357771260997068,"Dataset
GMUS
Transformer
GT Model
Speed-up
Cylinder ﬂow (for 400 time steps)
2sec
0.4758sec
1688.59sec
682
Sonic ﬂow (for 40 time steps)
0.2sec
0.047sec
25.026sec
100
Vascular ﬂow (for 250 time steps)
2.63sec
0.31sec
2451sec
800"
REFERENCES,0.8387096774193549,"A.9
OTHER VARIABLE CONTOUR"
REFERENCES,0.841642228739003,Figure 13 shows the pressure contours.
REFERENCES,0.844574780058651,Published as a conference paper at ICLR 2022
REFERENCES,0.8475073313782991,"Re = 307 (Cylinder Flow)
Re = 993 (Cylinder Flow)
t=0
t=85
t= 215
t=298
t=0
t=85
t= 215
t=298"
REFERENCES,0.8504398826979472,"Truth
Ours"
REFERENCES,0.8533724340175953,"T(0) = 201 (Sonic Flow)
T(0) = 299 (Sonic Flow)
t=0
t=15
t= 30
t=40
t=0
t=15
t= 30
t=40"
REFERENCES,0.8563049853372434,"Truth
Ours"
REFERENCES,0.8592375366568915,"R = 0.31 (Vascular ﬂow)
R = 0.49 (Vascular ﬂow)
t=40
t=80
t= 160
t=250
t=40
t=80
t= 160
t=250"
REFERENCES,0.8621700879765396,"Truth
Ours"
REFERENCES,0.8651026392961877,"Figure 13: The pressure contours of rollouts predicted by our model versus the ground truth (CFD).
With different condition settings, the model accurately predict long sequences of rollouts."
REFERENCES,0.8680351906158358,"A.10
FURTHER ANALYSIS OF LATENT ENCODINGS AND ATTENTION"
REFERENCES,0.8709677419354839,"Figure 14 analyzes the latent encoding and attention for the sonic ﬂows in a similar vein to section 4.
Different initial temperatures (as the system parameter) lead to fast diverging of state dynamics, and
thus the parameter tokens overlap for different parameters. However, the attentions to z−1 also
rapidly decay, demonstrating that the transformer can adjust the attention distribution to capture
most physically useful signals."
REFERENCES,0.873900293255132,"−1
−0.5
0
−0.5 0 0.5 1 C0 C1 (a)"
REFERENCES,0.8768328445747801,"−0.5
0
−0.5 0 0.5 C0 (b)"
REFERENCES,0.8797653958944281,"0
20
40 0 0.5 1"
REFERENCES,0.8826979472140762,Time Step
REFERENCES,0.8856304985337243,Parameter Token Attention (c)
REFERENCES,0.8885630498533724,"Figure 14: 2-D principle subspace of the embedded vectors from GMR (a) and PCA (b) for the high-
speed sonic ﬂow system: T(0) = 201 trajectory (
), T(0) = 299 trajectory (
) start from their
parameter tokens
and
, respectively. (c) Attention values of the parameter tokens versus
time, T(0) = 201 (
), and T(0) = 299 (
)."
REFERENCES,0.8914956011730205,"Figure 15 shows the overview of attention value distribution in a time-time diagram for all three ﬂuid
systems. The attentions of the state to that at different time step is plotted as a log-scale contour.
Note that the sum of attention values in the same row equals to 1 due to the softmax layer (Eqn. 9)."
REFERENCES,0.8944281524926686,Published as a conference paper at ICLR 2022
REFERENCES,0.8973607038123167,"0
100
200
300
400
Time 0 100 200 300 400 Time 20 15 10 5 0 (a)"
REFERENCES,0.9002932551319648,"0
10
20
30
40
Time 0 10 20 30 40 Time 30 25 20 15 10 5 0 (b)"
REFERENCES,0.9032258064516129,"0
100
200
Time 0 50 100 150 200 250 Time 60 40 20 0 (c)"
REFERENCES,0.906158357771261,"Figure 15: Examples of attention value (log value contour) for (a) ﬂow past cylinder, (b) sonic ﬂow
and (c) vascular ﬂow."
REFERENCES,0.9090909090909091,"Table 7: The average relative rollout error for the cylinder ﬂow, with unit of ×10−3. We compare
MeshGraphNet with noise injection (NI) to our model with transformer. We train the model on the
training trajectories with 400 steps, and test it on the unseen trajectories with 800 steps."
REFERENCES,0.9120234604105572,"rollout step
800
Variable
u
v
p
MeshGraphNet-NI 43 1274 258"
REFERENCES,0.9149560117302052,"Ours-Transformer
6
158
48"
REFERENCES,0.9178885630498533,"A.11
LONGER ROLLOUTS"
REFERENCES,0.9208211143695014,"We perform an additional experiment to test how the model performs when rolling out of the training
range. We train a model for the cylinder ﬂow setup (n=400 steps), and evaluate it for lengths of 2n."
REFERENCES,0.9237536656891495,"To limit the memory footprint for very long rollouts, we use a sliding window of n for the attention,
i.e., we always attend to the last n time steps. This way allow us to retain the same complexity for
the training. Fig. 16 illustrates this procedure: we always use the parameter embedding token ˜z−1
as the ﬁrst entry of the attention sequence, followed by a moving window of the n last timesteps."
REFERENCES,0.9266862170087976,"Table 7 compares the performance of the proposed model with MeshGraphNet on a longer rollout
length (2n = 2 × 400 steps). We ﬁnd that the proposed model still outperforms the baseline for all
u, v and p trajectories."
REFERENCES,0.9296187683284457,"˜z−1
˜zt−n
˜zt−n+1
˜zt−1
˜zt−2
˜zt−3
…"
REFERENCES,0.9325513196480938,"Figure 16: Sliding window for attention mechanism on latent representations. The parameter em-
bedding token ˜z−1 is always included in the ﬁrst entry."
REFERENCES,0.9354838709677419,"A.12
PREDICTION ON CONVECTION-DIFFUSION FLOW"
REFERENCES,0.9384164222873901,"Many of the systems we study in the main paper have oscillatory behaviors, which are hard to
capture by next-step models stably. To further demonstrate the capability of the model on predicting
non-oscillatory dynamics, we add an additional example: a convection-diffusion ﬂow, governed by"
REFERENCES,0.9413489736070382,"∂c
∂t + ∇· (vc) = ∇2( |v|"
REFERENCES,0.9442815249266863,"Pec),
(16)"
REFERENCES,0.9472140762463344,"where c is the concentration of the transport scalar, v = [1, 1] is the convection velocity, and Pe is the
Peclet number, which controls the ratio of the advective transport rate to its diffusive transport rate."
REFERENCES,0.9501466275659824,Published as a conference paper at ICLR 2022
REFERENCES,0.9530791788856305,"Convection Flow
Diffusion Flow
t=0
t=25
t= 50
t=100
t=0
t=2
t= 10
t=100"
REFERENCES,0.9560117302052786,"Truth
Ours"
REFERENCES,0.9589442815249267,"Figure 17: The concentration contours of rollouts predicted by our model versus the ground truth
(CFD)."
REFERENCES,0.9618768328445748,"As shown in Figure 17, the model can accurately predict the non-oscillatory sequences of rollouts
Quantitatively, our model (RMSE = 1.03 × 10−3) still outperforms MeshGraphNet (RMSE =
2.79 × 10−3)."
REFERENCES,0.9648093841642229,"A.13
CARDIAC FLOW PREDICTION"
REFERENCES,0.967741935483871,"We test the proposed model on cardiac ﬂows (pulsatile ﬂows) with varying viscosity in multiple
cardiac cycles, which is more realistic for cardiovascular systems. An idealized spatial-temporal
inﬂow condition is deﬁned as"
REFERENCES,0.9706744868035191,"v(x, t) = 0.6(x −xmin)(x −xmax)"
REFERENCES,0.9736070381231672,"(xmax −xmin)2
(sin(2πt"
REFERENCES,0.9765395894428153,"1.8 ))2 + 0.15
(17)"
REFERENCES,0.9794721407624634,"to simulate the pulsating inﬂow from the heart. The model can accurately predict 10 cardiac cycles
(Figure 18). The comparison with MeshGraphNet is listed in Table 8, showing signiﬁcantly better
performance for our model."
REFERENCES,0.9824046920821115,"Table 8: The average relative rollout error for cardiac ﬂow, with unit of ×10−3."
REFERENCES,0.9853372434017595,"rollout step
300
Variable
u
v
p
MeshGraphNet-NI 194 71 944"
REFERENCES,0.9882697947214076,Ours-Transformer 4.7 2.3 106
REFERENCES,0.9912023460410557,"Cardiac Flow
t=0
t=90
t= 200
t=270"
REFERENCES,0.9941348973607038,"Truth
Ours"
REFERENCES,0.9970674486803519,"Figure 18: The velocity contours of rollouts predicted by our model versus the ground truth (CFD)
for a cardiac ﬂow."
