Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029850746268656717,"Training deep neural networks with data corruption is a challenging problem. One
example of such corruption is the backdoor data poisoning attack, in which an ad-
versary strategically injects a backdoor trigger to a small fraction of the training
data to subtly compromise the training process. Consequently, the trained deep
neural network would misclassify testing examples that have been corrupted by
the same trigger. While the label of the data could be changed to arbitrary values
by an adversary, the extent of corruption injected to the feature values are strictly
limited in order to keep the backdoor attack in disguise, which leads to a resem-
blance between the backdoor attack and a milder attack that involves only noisy
labels. In this paper, we investigate an intriguing question: Can we leverage al-
gorithms that defend against noisy labels corruptions to defend against general
backdoor attacks? We ﬁrst discuss the limitations of directly using the noisy-
label defense algorithms to defend against backdoor attacks. Next, we propose a
meta-algorithm that transforms an existing noisy label defense algorithm to one
that protects against backdoor attacks. Extensive experiments on different types
of backdoor attacks show that, by introducing a lightweight alteration for mini-
max optimization to the existing noisy-label defense algorithms, the robustness
against backdoor attacks can be substantially improved, while the intial form of
those algorithms would fail in presence of a backdoor attacks."
INTRODUCTION,0.005970149253731343,"1
INTRODUCTION"
INTRODUCTION,0.008955223880597015,"Deep neural networks (DNN) have achieved signiﬁcant success in a variety of applications such as
image classiﬁcation (Krizhevsky et al., 2012), autonomous driving (Major et al., 2019), and natural
language processing (Devlin et al., 2018), due to its powerful generalization ability. In the mean-
time, DNN can be highly susceptible to even small perturbations of training data, which has raised
considerable concerns about its trustworthiness (Liu et al., 2020). One representative perturbation
approach is backdoor attack, which undermines the DNN performance by modifying a small fraction
of the training samples with speciﬁc triggers injected into their input features, whose ground-truth
labels are altered accordingly to be the attacker-speciﬁed ones. It is unlikely to detect backdoor
attacks by monitoring the model training performance, since the trained model can still perform
well on the benign validation samples. Consequently, during testing phase, if the data is augmented
with the trigger, it would be classiﬁed as the attacker-speciﬁed label. Subtle yet effective, backdoor
attacks can pose serious threats to the practical application of DNNs."
INTRODUCTION,0.011940298507462687,"Another typical type of data poisoning attack is noisy label attacks (Han et al., 2018; Patrini et al.,
2017; Yi & Wu, 2019; Jiang et al., 2017), in which the labels of a small fraction of data are al-
tered deliberately to compromise the model learning, while the input features of the training data
remain untouched. Backdoor attacks share a close connection to noisy label attacks, in that during a
backdoor attack, the feature can only be altered insigniﬁcantly to put the trigger in disguise, which
makes the corrupted feature (e.g. images with the trigger) highly similar to the uncorrupted ones.
Prior efforts have been made to effectively address noisy label attacks. For instance, there are algo-
rithms that can tolerate a large fraction of label corruption, with up to 45% noisy labels (Han et al.,
2018; Jiang et al., 2018). However, to the best of our knowledge, most algorithms defending against
backdoor attacks cannot deal with a high corruption ratio even if the features of corrupted data are
only slightly perturbed. Observing the limitation of prior arts, we aim to answer one key question:
Can one train a deep neural network that is robust against a large number of backdoor attacks?"
INTRODUCTION,0.014925373134328358,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01791044776119403,"Moreover, given the resemblance between noisy label attacks and backdoor attacks, we also investi-
gate another intriguing question: Can one leverage algorithms initially designed for handling noisy
label attacks to defend against backdoor attacks more effectively?"
INTRODUCTION,0.020895522388059702,"The contributions of this paper are multi-fold. First, we provide a novel and principled perspective
to decouple the challenges of defending backdoor attacks into two components: one induced by
the corrupted input features, and the other induced by the corrupted labels, based on which we can
draw a theoretical connection between the noisy-label attacks and backdoor data attacks. Second,
we propose a meta-algorithm which addresses both challenges by a novel minimax optimization.
Speciﬁcally, the proposed approach takes any noisy-label defense algorithm as the input and outputs
a reinforced version of the algorithm that is robust against backdoor poisoning attacks, while the
initial form of the algorithm fails to provide such defense. Extensive experiments show that the
proposed meta-algorithm improves the robustness of DNN models against various backdoor attacks
on a variety of benchmark datasets with up to 45% corruption ratio. Furthermore, we propose a
systematic, meta-framework to solve backdoor attacks, which can effectively join existing force in
noisy label attack defenses and provides more insights to future development of defense algorithms."
RELATED WORK,0.023880597014925373,"2
RELATED WORK"
ROBUST DEEP LEARNING AGAINST ADVERSARIAL ATTACK,0.026865671641791045,"2.1
ROBUST DEEP LEARNING AGAINST ADVERSARIAL ATTACK
Although DNNs have shown high generalization performance on various tasks, it has been observed
that a trained DNN model would yield different results even by perturbing the image in an invis-
ible manner (Goodfellow et al., 2014; Yuan et al., 2019). Prior efforts have been made to tackle
this issue, among which one natural defense strategy is to change the empirical loss minimiza-
tion into a minimax objective. By solving the minimax problem, the model is guaranteed a better
worst-case generalization performance (Duchi & Namkoong, 2021). Since exactly solving the inner
maximization problem can be computationally prohibitive, different strategies have been proposed
to approximate the inner maximization optimization, including heuristic alternative optimization,
linear programming Wong & Kolter (2018), semi-deﬁnite programming Raghunathan et al. (2018),
etc. Besides minimax optimization, another approach to improve model robustness is imposing a
Lipschitz constraint on the network. Work along this line includes randomized smoothing Cohen
et al. (2019); Salman et al. (2019), spectral normalization Miyato et al. (2018a), and adversarial Lip-
schitz regularization Terj´ek (2019). Although there are algorithms that are robust against adversarial
samples, they are not designed to confront backdoor attacks, in which clean training data is usually
inaccessible. There are also studies that investigated the connection between adversarial robustness
and robustness against backdoor attack (Weber et al., 2020). However, to our best knowledge, there
is no literature studying the relationship between label ﬂipping attack and backdoor attack."
ROBUST DEEP LEARNING AGAINST LABEL NOISE,0.029850746268656716,"2.2
ROBUST DEEP LEARNING AGAINST LABEL NOISE
Many recent studies have investigated the robustness of classiﬁcation tasks with noisy labels. For
example, Kumar et al. (2010) proposed the Self-Paced Learning (SPL) approach, which assigns
higher weights to examples with a smaller loss. A similar idea was used in Curriculum Learning
(Bengio et al., 2009), in which a model learns on easier examples before moving to the harder
ones. Other methods inspired by SPL include learning the data weights (Jiang et al., 2018) and
collaborative learning (Han et al., 2018; Yu et al., 2019). An alternative approach to defending noisy
label attacks is label correction (Patrini et al., 2017; Li et al., 2017; Yi & Wu, 2019), which attempts
to revise the original labels of the data to recover clean labels from corrupted ones. However,
since we do not have the knowledge of which data points have been corrupted, it is nontrivial to
obtain provable guarantees for label corrections, unless strong assumptions have been made on the
corruption type."
DATA POISONING BACKDOOR ATTACK AND ITS DEFENSE,0.03283582089552239,"2.3
DATA POISONING BACKDOOR ATTACK AND ITS DEFENSE
Robust learning against backdoor attacks has been widely studied recently. Gu et al. (2017) showed
that even a small patch of perturbation can compromise the generalization performance when data
is augmented with a backdoor trigger. Other types of attacks include the blend attacks (Chen et al.,
2017), clean label attacks (Turner et al., 2018; Shafahi et al., 2018), latent backdoor attacks (Yao
et al., 2019), etc. While there are various types of backdoor attacks, some attack requires that the
adversary not only has access to the data but also can has limited control on the training and inference"
DATA POISONING BACKDOOR ATTACK AND ITS DEFENSE,0.03582089552238806,Under review as a conference paper at ICLR 2022
DATA POISONING BACKDOOR ATTACK AND ITS DEFENSE,0.03880597014925373,"process. Those attacks include Trojan attacks and blind backdoor attacks (Pang et al., 2020). We
refer readers to Pang et al. (2020) for a comprehensive survey on different types of backdoor attacks."
DATA POISONING BACKDOOR ATTACK AND ITS DEFENSE,0.041791044776119404,"Various defense methods have been proposed to defend against backdoor attacks. One defense
category is to remove the corrupted data by using anomaly detection (Tran et al., 2018; Chen et al.,
2018). Another category of work is the model inspection (Wang et al., 2019), which aims to inspect
and modify the backdoored model to make it robust against the trigger. There are also other methods
of tackling the backdoor attacks, such as randomized smoothing (Cohen et al., 2019; Weber et al.,
2020), and the median of means (Levine & Feizi, 2020). However, they are either inefﬁcient or
cannot defend against backdoor attacks with a large ratio of corrupted data. Some of the above
methods also hinge on a clean set of validation data, while in practice, it is unlikely to guarantee the
existence of clean validation data, since validation data is usually a subset of training data. To the
best of our knowledge, there is no existing backdoor defense algorithm that is motivated from the
label corruption perspective."
PRELIMINARIES,0.04477611940298507,"3
PRELIMINARIES"
LEARNING WITH NOISY LABELS,0.04776119402985075,"3.1
LEARNING WITH NOISY LABELS"
LEARNING WITH NOISY LABELS,0.050746268656716415,"In this section, we review some representative approaches for defending noisy-labels. Although
the initial forms of these approaches can be vulnerable to backdoor attacks, we show later in the
next section that our proposed meta-algorithm can empower them to effectively confront backdoor
attacks. Speciﬁcally, we look into two types of nosiy-label defending approaches: ﬁltering-based
approaches and consistency-based approaches."
LEARNING WITH NOISY LABELS,0.05373134328358209,"The ﬁltering-based approach is one of the most effective strategies for defending noisy labels,
which works by selecting or weighting the training samples based on indicators such as sample
losses (Jiang et al., 2017; Han et al., 2018; Jiang et al., 2020) or gradient norms of the loss-layer
(Liu et al., 2021). For instance, Jiang et al. (2017) proposed to assign higher probabilities to samples
with lower losses to be selected for model training. Consistency-based approach modiﬁes data labels
during model training. Speciﬁcally, the Bootstrap approach (Reed et al., 2014) encourages model
predictions to be consistent between iterations, by modifying the labels as a linear combination of
the observed labels and previous predictions."
LEARNING WITH NOISY LABELS,0.056716417910447764,"In this paper, we leverage two ﬁltering-based noisy label algorithms, i.e. the Self-Paced Learning
(SPL) (Jiang et al., 2017; Kumar et al., 2010) and Provable Robust Learning (PRL) (Liu et al.,
2021), and one consistency-based algorithm, i.e. the Bootstrap (Reed et al., 2014), to investigate
the efﬁcacy of the proposed meta algorithm. Strong empirical results in Section 5 on those input
algorithms suggest that our meta framework is readily to beneﬁt other robust noisy-label algorithms.
We brieﬂy summarize the main idea of the above algorithms in table 1."
LEARNING WITH NOISY LABELS,0.05970149253731343,"PRL (Filtering-based)
SPL (Filtering-based)
Bootstrap (Modifying Label)"
LEARNING WITH NOISY LABELS,0.0626865671641791,"Mini-batch
Keep
data
with
small
loss-layer gradient norm
Keep data with small loss
ytrue = αytrue + (1 −α)ypred"
LEARNING WITH NOISY LABELS,0.06567164179104477,"Table 1: Overview of noisy-label defending algorithms, which achieve robustness against up to 45%
of pairwise ﬂipping label noises."
PROBLEM SETTING OF BACKDOOR ATTACKS,0.06865671641791045,"3.2
PROBLEM SETTING OF BACKDOOR ATTACKS"
PROBLEM SETTING OF BACKDOOR ATTACKS,0.07164179104477612,"In this paper, we follow a standard setting for backdoor attacks and assume that there is an adversary
that tries to perform the backdoor attack. Firstly, the adversary can choose up to ϵ fraction of
clean labels Y ∈Rn×q and modify them to arbitrary valid numbers to form the corrupted labels
Yb ∈R⌊nϵ⌋×q. Let Yr represent the remaining untouched labels, then the ﬁnal training labels
can be denoted as Yϵ = [Yb, Yr]. Accordingly, the corresponding original feature are denoted as
X = [Xo ∈R⌊nϵ⌋×d, Xr ∈R(n−⌊nϵ⌋)×d]. The adversary can design a trigger t ∈Rd to form the
corrupted feature set Xb ∈R⌊nϵ⌋×d such that for any bi in Xb, oi in Xo, it satisﬁes bi = oi + t."
PROBLEM SETTING OF BACKDOOR ATTACKS,0.07462686567164178,Under review as a conference paper at ICLR 2022
PROBLEM SETTING OF BACKDOOR ATTACKS,0.07761194029850746,"Finally, the training feature will be Xϵ = [Xb ∈R⌊nϵ⌋×d, Xr ∈R(n−⌊nϵ⌋)×d]. Without ambiguity,1"
PROBLEM SETTING OF BACKDOOR ATTACKS,0.08059701492537313,"we also denote T = [t, t, ..., t] ∈R⌊nϵ⌋×d, so that Xo + T = Xb."
PROBLEM SETTING OF BACKDOOR ATTACKS,0.08358208955223881,"Before analyzing the algorithm, we make following assumptions about the adversary attack:
Assumption 1 (Bounded Corruption Ratio). The overall corruption ratio and the corruption ratio
in each class is bounded. Speciﬁcally,"
PROBLEM SETTING OF BACKDOOR ATTACKS,0.08656716417910448,"E(x,y,yb)∈(X,Y,Yb)"
PROBLEM SETTING OF BACKDOOR ATTACKS,0.08955223880597014,I(yb = c|y ̸= c)
PROBLEM SETTING OF BACKDOOR ATTACKS,0.09253731343283582,I(y = c)
PROBLEM SETTING OF BACKDOOR ATTACKS,0.0955223880597015,"
≤ϵ = 0.5 ∀c ∈△Y."
PROBLEM SETTING OF BACKDOOR ATTACKS,0.09850746268656717,"Assumption 2 (Small Trigger). Any backdoor trigger satisﬁes ∥t∥p ≤τ, which subtly alters the
data within a small radius-τ ball without changing its ground-truth label."
PROBLEM SETTING OF BACKDOOR ATTACKS,0.10149253731343283,"We also assume that there exists at least one black-box robust algorithm A which can defend any
noisy label attacks so long as the noisy-label ratio is bounded by ϵ. Note that the above assumption
is mild, since a variety of existing algorithm can handle noisy labels attacks with a large corruption
rate (e.g. 45%) (Jiang et al., 2017; Han et al., 2018; Reed et al., 2014; Liu et al., 2021)."
METHODOLOGY,0.1044776119402985,"4
METHODOLOGY"
METHODOLOGY,0.10746268656716418,"Given an ϵ-backdoor attacked dataset (Xϵ, Yϵ), a clean distribution p∗:= (X, Y), and a loss func-
tion L, we aim to answer the following questions: 1) can we learn a network function f that min-
imizes the generalization error under the corrupted distribution, i.e. E(x,y)∼p∗[L(f(x + t), y)]?
2) can the learned f also minimize the generalization error under the ground-truth, clean distri-
bution, i.e. E(x,y)∼p∗[L(f(x), y)]? Next, we elaborate our meta-approach for defending against
backdoor attacks, which answers the above two questions afﬁrmatively."
A BLACK-BOX ROBUST ALGORITHM AGAINST NOISY LABELS,0.11044776119402985,"4.1
A BLACK-BOX ROBUST ALGORITHM AGAINST NOISY LABELS"
A BLACK-BOX ROBUST ALGORITHM AGAINST NOISY LABELS,0.11343283582089553,"The ultimate goal for defending against backdoor attacks is to learn a network function f to minimize
its risk given corrupted feature inputs:"
A BLACK-BOX ROBUST ALGORITHM AGAINST NOISY LABELS,0.11641791044776119,"min
f
J(f) := E(x,y)∼p∗[L(f(x + t), y)] .
(1)"
A BLACK-BOX ROBUST ALGORITHM AGAINST NOISY LABELS,0.11940298507462686,"However, Equation 1 is not directly optimizable due to two-fold challenges: 1) the corrupted inputs
Xϵ with an unknown trigger t, and 2) the corrupted labels Yϵ. That is, neither the clean inputs or
ground-truth labels are available. As such, we turn to an approachable objective that optimizes the
worst-case of Equation 1:"
A BLACK-BOX ROBUST ALGORITHM AGAINST NOISY LABELS,0.12238805970149254,"min
f
max
∥c∥p≤τ
1
n X"
A BLACK-BOX ROBUST ALGORITHM AGAINST NOISY LABELS,0.1253731343283582,"x∈X,y∈Y
[L(f(x + c), y)] .
(2)"
A BLACK-BOX ROBUST ALGORITHM AGAINST NOISY LABELS,0.12835820895522387,"Since the trigger satisﬁes ∥t∥p ≤τ, it is easy to see that Equation 2 minimizes an upper-bound of
the ground-truth loss, in that:"
N,0.13134328358208955,"1
n X"
N,0.13432835820895522,"x∈X,y∈Y
L(f(x + t), y) ≤max
∥c∥p≤τ
1
n X"
N,0.1373134328358209,"x∈X,y∈Y
[L(f(x + c), y)] ."
N,0.14029850746268657,"To this end, directly optimizing the surrogate objective in Equation 2 is still intractable, since we do
not have access to clean X and Y, which are involved in the inner maximization loop. To tackle this
challenge, we will ﬁrst assume that the clean label Y is available, and then relax this prerequisite
by using any learning algorithms that are robust against noisy labels. Speciﬁcally, assume that
φw = L ◦f has a Lipschitz constant L w.r.t x, we further obtain a new upper bound (see Appendix
for derivation details):"
N,0.14328358208955225,"1
n X"
N,0.14626865671641792,"x∈X,y∈Y
[L(f(x + c), y)] ≤1 n X"
N,0.14925373134328357,"x∈Xϵ,y∈Y
φw(xi + c, y) + ϵτL,
(3)"
N,0.15223880597014924,"1Some backdoor attack algorithms design instance-speciﬁc trigger. In this paper, we only focus on the static
trigger case and leave the instance-speciﬁc trigger case for our future study."
N,0.15522388059701492,Under review as a conference paper at ICLR 2022
N,0.1582089552238806,which draws a principled connection between the risks when using corrupted data and clean data:
N,0.16119402985074627,"min
f
max
∥c∥p≤τ
1
n X"
N,0.16417910447761194,"x∈X,y∈Y
[L(f(x + c), y)] ≈ ("
N,0.16716417910447762,"min
f
max
∥c∥p≤τ
1
n X"
N,0.1701492537313433,"x∈Xϵ,y∈Y
[L(f(x + c), y)] + ϵτL ) ,
(4)"
N,0.17313432835820897,"where the ﬁrst term on the RHS of Equation 4 involves optimization on the corrupted features Xϵ
and clean labels Y, while the second term on the RHS requires minimizing the Lipschitz constant
L w.r.t x. Recall that minimizing the maximum gradient norm is equivalent to minimizing the Lips-
chitz constant (Terj´ek, 2019). Therefore, optimizing the ﬁrst term naturally regulates the maximum
change of the loss function within a small ball, which hence constrains the magnitude of the gradient
and has negligible effects on the Lipschitz regularization. The relationship between Lipschitz regu-
larization and adversarial training has also been well discussed by literatures (Terj´ek, 2019; Miyato
et al., 2018b). We defer more discussion along this line to Appendix."
N,0.1761194029850746,"Equation 4 indicates that if the target labels are not corrupted, and the learned function has a small
Lipschitz constant, learning with corrupted features is feasible to achieve low risks. Up to now,
the remaining challenge of optimizing the surrogate objective in Equation 4 is the inaccessible clean
label set Y. Fortunately, a variety of algorithms are at hand for handling noisy labels during learning
(Jiang et al., 2017; Liu et al., 2021; Kumar et al., 2010), which we can directly apply to our minimax
optimization scheme. Speciﬁcally, for the outer minimization, one can have:"
N,0.1791044776119403,"min
f
1
n X"
N,0.18208955223880596,"x∈Xϵ,y∈Yϵ
[L(f(x + c), y)] ,"
N,0.18507462686567164,"For the outer minimization, we can perform the noisy-label update for the above optimization ob-
jective. For instance, Given the mini-batch Mx, My with batch size m, if we use SPL to perform
the update, we can get the top (1 −ϵ)m data with a small risk L(f(x + c), y) to perform one-step
gradient descent. If we use the PRL to perform the update, assume L is the cross-entropy loss, one
can get the top (1−ϵ)m data with small loss-layer gradient norm ∥f(x+c)−y∥to perform one-step
gradient descent. If we use the bootstrap, we can directly add a bootstrap regularization to update
the above objective."
N,0.1880597014925373,"In the meantime, it is non-trivial to directly solve the inner maximization, since adversarial learning
c in Equation 4 still faces the threat of noisy labels. To tackle this issue, we can leverage the same
robust noisy label algorithm. Speciﬁcally, we ﬁrst approximate the inner optimization using the
ﬁrst-order Tyler expansion:"
N,0.191044776119403,"c∗= arg max
∥c∥p≤τ"
N,0.19402985074626866,"1
n X"
N,0.19701492537313434,"x∈X,y∈Y
L(f(x + c), y) ≈arg max
∥c∥p≤τ"
N,0.2,"1
n X"
N,0.20298507462686566,"x∈X,y∈Y
L(f(x), y) + cT ∇xL(f(x), y)"
N,0.20597014925373133,"= arg max
∥c∥p≤τ
cT ∇x
1
n X"
N,0.208955223880597,"x∈X,y∈Y
L(f(x), y)."
N,0.21194029850746268,"Above optimization is a linear programming problem. With the l∞norm ball constraint on the
perturbation, the above update can be reduced to the fast gradient sign method (FGSM). Given a
minibatch Mx, My with batchsize m, we can have the closed-form solution as the following:"
N,0.21492537313432836,"˜c = Clipc  τ m ·
X"
N,0.21791044776119403,"x∈Mx,y∈My sign (∇xL (f(x), y))

.
(5)"
N,0.2208955223880597,"To relax the prerequisite of a clean label set y in Equation 5, we will use a noisy-label algorithm to
perform the update. For instance, if we use a loss-ﬁltering based algorithm (e.g. SPL), then for each
mini-batch, only the top (1 −ϵ)m data with small L (f(x), y) would be included in the update. If
we adopt a gradient-based ﬁltering algorithm (e.g. PRL), given that L is the cross-entropy loss, then
only top (1−ϵ)m data with small ∥f(x)−y∥will be included. The outside clipping ensures that the
feature value of the corrupted image is in the valid range. Based on the above building blocks, we
now introduce our algorithm in Algorithm 1, a meta defense scheme that are robust against backdoor
attacks, given arbitrary noisy-label robust algorithm A as an input. The algorithm is illustrated in
Figure 1."
N,0.22388059701492538,Under review as a conference paper at ICLR 2022
N,0.22686567164179106,"Algorithm 1: Meta algorithm for Robust Learning Against Backdoor Attacks
input: Corrupted training data Xϵ, Yϵ, perturbation limit: τ, learning with noisy label algorithm A (e.g.
PRL, SPL, Bootstrap).
return trained neural network ;
while epoch ≤max epoch do"
N,0.2298507462686567,"for sampled minibatch Mx, My in Xϵ,Yϵ do"
N,0.23283582089552238,"#Inner maximization step
initialize c as 0 vector.
optimize the objective max∥c∥≤τ L(f(Mx + c), My) w.r.t to c by using robust algorithm A
optimize the objective minf L(f(Mx + c), My) w.r.tf by using robust algorithm A
end
end"
N,0.23582089552238805,"(a) clean data
(b)
two
samples
are
added with small trigger
and ﬂipped label"
N,0.23880597014925373,"(c) Inner maximization
by noisy label algorithm"
N,0.2417910447761194,"(d) Reduce the back-
door attack to label ﬂip-
ping attack"
N,0.24477611940298508,"Figure 1: Illustration of our meta algorithm. By combining the minimax objective and noisy label
algorithm, we could reduce the backdoor attack problem to the label ﬂipping attack. The left most
is the clean original data and the second one is corrupted data. The third ﬁgure shows the inner
maximization step while the last ﬁgure shows the outer minimization step."
THEORETICAL JUSTIFICATION,0.24776119402985075,"4.2
THEORETICAL JUSTIFICATION"
THEORETICAL JUSTIFICATION,0.2507462686567164,"Our ultimate goal is to learn w that achieves a small risk Ex,y∼p∗φw(x + t, y). To study the
generalization performance on the ground-truth distribution p∗, we ﬁrst deﬁne the following risks:"
THEORETICAL JUSTIFICATION,0.2537313432835821,"Remp
t
= 1 n
P"
THEORETICAL JUSTIFICATION,0.25671641791044775,"x∈X,y∈Y φw(x+t, y), Rt = Ex,y∼p∗φw(x+t, y), Remp
c
= 1 n
P"
THEORETICAL JUSTIFICATION,0.25970149253731345,"x∈Xϵ,y∈Y φw(x+"
THEORETICAL JUSTIFICATION,0.2626865671641791,"c, y), and ˜Remp
c
= 1 n
P"
THEORETICAL JUSTIFICATION,0.2656716417910448,"x∈Xϵ,y∈Yϵ φw(x+c, y). Next, we focus on the gap between Rt and Remp
c
."
THEORETICAL JUSTIFICATION,0.26865671641791045,"Theorem 1. Let ˜Remp
c
, Remp
c
, Rt, ϵ, τ deﬁned as above. Assume that the prior distribution of the
network parameter w is N(0, σ), and the posterior distribution of parameter is N(w, σ) which is
learned from the training data. Let k be the number of parameters, and n be the sample size. If the
objective function φw = L ◦f is Lφ-Lipschitz smooth, then with probability at least 1-δ, one can
derive:"
THEORETICAL JUSTIFICATION,0.2716417910447761,"Rt ≤Remp
c
+ Lφ(2τ + ϵτ) +"
THEORETICAL JUSTIFICATION,0.2746268656716418,"v
u
u
t"
THEORETICAL JUSTIFICATION,0.27761194029850744,"1
4k log

1 +
∥w∥2
2
kσ2

+ 1"
THEORETICAL JUSTIFICATION,0.28059701492537314,4 + log n
THEORETICAL JUSTIFICATION,0.2835820895522388,δ + 2 log(6n + 3k)
THEORETICAL JUSTIFICATION,0.2865671641791045,"n −1
.
(6)"
THEORETICAL JUSTIFICATION,0.28955223880597014,"We hereby present the skeleton of the proof and defer more details to Appendix. First, we decompose
the error into two components: 1) the generalization gap on the triggered data, and 2) the difference
of performance loss between the trigger t and worst case perturbation c: Rt −Remp
c
= (Rt −
Remp
t
) + (Remp
t
−Remp
c
)."
THEORETICAL JUSTIFICATION,0.29253731343283584,The ﬁrst components is s
THEORETICAL JUSTIFICATION,0.2955223880597015,"1
4 k log

1+
∥w∥2
2
kσ2 
+ 1"
THEORETICAL JUSTIFICATION,0.29850746268656714,4 +log n
THEORETICAL JUSTIFICATION,0.30149253731343284,δ +2 log(6n+3k)
THEORETICAL JUSTIFICATION,0.3044776119402985,"n−1
, which is derived by following
the PAC-Bayes framework (Foret et al., 2020). For the second term, the gap is introduced by two
sources. The ﬁrst source is the difference between c and t, and the second is from the difference
between X and Xϵ. Since the objective is Lφ Lipschitz, and ∥t−c∥≤2τ according to our constraint
to the adversary, it is easy to upper bound the error as 2τLφ. In the meantime, there is ϵ-fraction of"
THEORETICAL JUSTIFICATION,0.3074626865671642,Under review as a conference paper at ICLR 2022
THEORETICAL JUSTIFICATION,0.31044776119402984,"difference between X and Xϵ, which is bounded by ∥t∥< τ and leads to the other difference term
Lφϵτ."
THEORETICAL JUSTIFICATION,0.31343283582089554,"Theorem 1 presents an upper-bound of the gap Rt −Remp
c
. The ﬁrst term in Equation 6 can
be minimized by using a noisy label algorithm. The second term, which is the error induced by
the adversarial trigger, is jointly constrained by the Lipschitz constant Lφ, perturbation limit τ,
and the corruption ratio ϵ. We can regularize the Lφ whereas the τ and ϵ are controlled by the
unknown adversary. Note that existing literature has also shown that adversarial training plays a
resemblant role as the Lipschitz regularization. The last term, i.e. the normal generalization error on
the clean data, is difﬁcult to minimize directly. The bound in Theorem 1 emphasizes the importance
of involving both the noisy label algorithm and the adversarial training. The noisy label algorithm
can reduce the Remp
c
while the adversarial training minimizes the Lipschitz constant Lφ to optimize
the second term.
5
EXPERIMENT"
THEORETICAL JUSTIFICATION,0.3164179104477612,"In this section, we perform the empirical study. In our experiment, we perform experiment on
CIFAR10 and CIFAR100 benchmark data. We use ResNet-32 (He et al., 2016) as the backbone
network structure for all experiment baseline. The initial learning rate of all methods are set to
be 3e-4, and we use AdamW (Loshchilov & Hutter, 2017) as the optimizer for all methods. The
evaluation metric is the top-1 accuracy for both clean testing data and testing data with backdoor
trigger."
THEORETICAL JUSTIFICATION,0.3194029850746269,"For the backdoor data poisoning attack, we use simple badnet attack (Gu et al., 2017) and gaussian
blending attack (Chen et al., 2017) since these two attacks do not require any information about the
model or training procedure (Pang et al., 2020)."
THEORETICAL JUSTIFICATION,0.32238805970149254,"• badnet patch attack: trigger is a 3 × 3 black-white checkerboard and it is added to the right
bottom corner of the image.
• blending attack: trigger is a ﬁxed Gaussian noise which has the same dimension as the
image. The corrupted image generated by xϵ
i = (1 −α)xi + αt. In our experiment, we set
the α as 0.1."
THEORETICAL JUSTIFICATION,0.3253731343283582,"The poisoned sample can be found at Appendix. We deploy the multi-target backdoor attack in this
paper. Our poisoning approach is as following: we ﬁrst systematically ﬂip the label to perform the
label-ﬂipping attack . Then, for those being attacked samples, we add the triggers on their feature.
Without adding the trigger, the problem would reduce to the noisy label problem."
THEORETICAL JUSTIFICATION,0.3283582089552239,and we use the following two evaluation metric.
THEORETICAL JUSTIFICATION,0.33134328358208953,"• top-1 clean accuracy: the top-1 accuracy calculated on trigger-less testing data
• top-1 poison accuracy: the top-1 accuracy calculated by the model prediction on poisoned
testing data and ground truth clean label."
THEORETICAL JUSTIFICATION,0.33432835820895523,"The ﬁrst metric describes how model performs on benign data while the second metric de-
scribes how model performs on triggered data.
We varies our training data poisoning rate as
[15%, 25%, 35%, 45%] to investigate how our algorithm performs against different corruption ra-
tio. All methods are trained 100 epochs, Also, in this paper, we assume there is no clean validation
data available. Thus, it is difﬁcult to perform early stopping or decide which epoch result should be
used. Thus, we report the averaged accuracy across last 10 epochs for every methods."
THEORETICAL JUSTIFICATION,0.3373134328358209,"We investigate three noisy label algorithms by comparing the performance of the original method
and reinforced method. Speciﬁcally, we choose SPL, PRL, and Bootstrap as our original noisy label
algorithm and the corresponding reinforced algorithm with adversarial training SPL-AT, PRL-AT,
Bootstrap-AT. We also compared our method against adversarial training only (AT), which only uses
the adversarial training without using any noisy label algorithm. To show the success of the attack,
we also includes the standard training results.
5.1
HOW ROBUST LEARNING AGAINST NOISY LABEL PERFORMS ON BACKDOOR DATA"
THEORETICAL JUSTIFICATION,0.3402985074626866,"In this section, we aim to answer the ﬁrst question: how does noisy label defense algorithm performs
against backdoor attack?"
THEORETICAL JUSTIFICATION,0.34328358208955223,Under review as a conference paper at ICLR 2022
THEORETICAL JUSTIFICATION,0.34626865671641793,"To answer above question, We evaluate PRL, SPL, and Bootstrap on CIFAR10 and CIFAR100
dataset. The results can be found in table 2 and table 3. As we can see in the table, the standard
training performs well on the benign testing data (i.e. high clean accuracy) while its performance
on triggered data is very bad (i.e. low poison accuracy), which indicates the effectiveness of the
backdoor attack."
THEORETICAL JUSTIFICATION,0.3492537313432836,"As for the three noisy label algorithm, we found that although Bootstrap, SPL, and PRL all performs
well for the benign testing data, they all fail when defending the backdoor attack data especially with
large corruption rate, which illustrates that noisy label algorithm cannot defend the backdoor attack
especially when corruption ratio is large."
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.3522388059701492,"5.2
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM"
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.35522388059701493,"To investigate whether adversarial training could improve the robustness of existing noisy label al-
gorithm against backdoor attack, we performed experiment on SPL-AT, PRL-AT and Bootstrap-AT
to see how they performs on both clean test data and triggered test data. The results can be found in
table 2 and table 3. As we can see that by adding the adversarial training, the performance on trig-
gered data is largely improved (i.e. the poison accuracy signiﬁcantly improves). Also, we noticed
that this improvement pattern is hold for all three noisy label algorithm, which indicates the effec-
tiveness of the proposed method on improving robustness against backdoor attack. Also, compared
to adversarial training only (AT), adding noisy label does improve the performance. Especially for
the PRL algorithm, we see the PRL-AT achieves better performance than AT. Also, we found that
compared to consistency based noisy label algorithm, ﬁltering based algorithm are more easier to
be boosted by adversarial training. The potential reason behind this could be that ﬁltering based
method is more efﬁcient against noisy label algorithm Han et al. (2018); Jiang et al. (2017); Liu
et al. (2021)."
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.3582089552238806,"Backdoor Attack Defense Accuracy.
Dataset
ϵ
AT
BootStrap
Bootstrap-AT
PRL
PRL-AT
SPL
SPL-AT
Standard"
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.3611940298507463,"CIFAR10
with Patch Attack,
Poison Accuracy"
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.3641791044776119,"0.15 66.64 ± 5.28
2.09 ± 0.13
3.05 ± 0.47
81.71 ± 0.37
80.15 ± 0.42 34.60 ± 1.57 77.60 ± 3.81
2.10 ± 0.10
0.25 63.98 ± 7.16
2.01 ± 0.23
2.75 ± 0.17
45.94 ± 25.19 78.14 ± 0.48 10.87 ± 2.13 22.17 ± 10.51 2.13 ± 0.15
0.35 60.19 ± 1.35
1.98 ± 0.15
2.66 ± 0.16
31.27 ± 17.63 75.04 ± 0.29 11.74 ± 1.24 15.40 ± 7.56
2.01 ± 0.09
0.45 51.25 ± 1.81
1.94 ± 0.12
2.53 ± 0.20
17.50 ± 1.66 58.90 ± 12.52 12.32 ± 1.20 14.00 ± 5.35
1.88 ± 0.04"
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.36716417910447763,"CIFAR10
with Patch Attack,
Clean Accuracy"
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.3701492537313433,"0.15 66.77 ± 5.17 85.22 ± 0.48 82.62 ± 0.26
82.06 ± 0.16
80.25 ± 0.43 77.35 ± 2.76 77.70 ± 3.78 85.40 ± 0.37
0.25 63.98 ± 7.16 85.25 ± 0.19 81.90 ± 0.25
78.57 ± 1.03
78.22 ± 0.56 69.52 ± 2.38 68.49 ± 2.76 85.20 ± 0.26
0.35 60.31 ± 1.37 84.86 ± 0.13 81.75 ± 0.25
73.63 ± 0.75
75.10 ± 0.31 60.23 ± 3.14 58.88 ± 3.46 84.73 ± 0.13
0.45 51.25 ± 1.81
1.94 ± 0.12
2.53 ± 0.20
17.50 ± 1.66 58.90 ± 12.52 50.82 ± 1.48 14.00 ± 5.35
1.88 ± 0.04"
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.373134328358209,"CIFAR10
with Blend Attack,
Poison Accuracy"
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.3761194029850746,"0.15 65.15 ± 0.94
2.17 ± 0.17 24.98 ± 10.01
6.41 ± 3.91
79.71 ± 0.33 11.60 ± 6.56 74.77 ± 3.53
2.29 ± 0.10
0.25 56.98 ± 0.72
2.06 ± 0.10 33.33 ± 20.03
6.77 ± 2.81
76.99 ± 0.37 11.60 ± 8.59 52.36 ± 10.57 2.03 ± 0.18
0.35 47.84 ± 1.49
1.86 ± 0.07
13.13 ± 7.11
9.42 ± 5.28
73.17 ± 0.96 12.71 ± 9.33 50.79 ± 7.92
1.97 ± 0.07
0.45 34.66 ± 1.49
1.83 ± 0.11
6.12 ± 2.86
8.13 ± 4.50
49.88 ± 8.43
8.69 ± 4.41
35.06 ± 4.00
1.88 ± 0.06"
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.37910447761194027,"CIFAR10
with Blend Attack,
Clean Accuracy"
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.382089552238806,"0.15 66.14 ± 0.98 85.54 ± 0.58 81.44 ± 0.58
77.51 ± 1.20
80.06 ± 0.34 76.25 ± 2.78 75.65 ± 3.11 85.28 ± 0.34
0.25 58.91 ± 5.70 84.95 ± 0.30 80.89 ± 0.65
71.45 ± 1.40
77.82 ± 0.26 67.86 ± 2.58 65.08 ± 0.82 85.06 ± 0.39
0.35 50.07 ± 13.26 84.72 ± 0.58 80.63 ± 0.57
66.22 ± 1.15
74.34 ± 1.01 60.52 ± 2.26 60.16 ± 2.39 84.72 ± 0.28
0.45 38.03 ± 15.42 84.36 ± 0.38 80.35 ± 0.39
55.78 ± 2.09
57.17 ± 9.02 49.48 ± 2.19 46.74 ± 0.71 84.07 ± 0.17"
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.3850746268656716,"Table 2: Performance on CIFAR10. ϵ is the corruption rate.
5.3
ABLATION STUDY"
HOW ADVERSARIAL TRAINING IMPROVES THE NOISY LABEL ALGORITHM,0.3880597014925373,"In this section, we aim to explore more about the proposed framework. Since our algorithm use
the noisy-label solver for both inner and outer optimization. A interesting question to ask is that
whether both inner and outer noisy-label solver plays an important role in defense the backdoor
attack. Thus, we have two variants. One is we only use noisy label algorithm to update the model
for outer minimization and another one is we only use the noisy label algorithm to update the model
for inner maximization. The results can be found at table 4 and table 5. As we could see in these two
tables, using noisy label algorithm to perform the inner maximization is more important compared
to using noisy label algorithm to perform out minimization."
CONCLUSION,0.39104477611940297,"6
CONCLUSION"
CONCLUSION,0.3940298507462687,"In this paper, we investigate the connection between label ﬂipping attack and backdoor data poison-
ing attack. We show that although robust algorithm against label ﬂipping attack cannot defend the"
CONCLUSION,0.3970149253731343,Under review as a conference paper at ICLR 2022
CONCLUSION,0.4,"Backdoor Attack Defense Accuracy.
Dataset
ϵ
AT
BootStrap
Bootstrap-AT
PRL
PRL-AT
SPL
SPL-AT
Standard"
CONCLUSION,0.40298507462686567,"CIFAR100
with Patch Attack,
Poison Accuracy"
CONCLUSION,0.4059701492537313,"0.15 23.70 ± 1.39
5.23 ± 0.81
44.74 ± 4.05 15.15 ± 9.17 47.11 ± 0.58 24.87 ± 5.27 42.24 ± 0.76
5.28 ± 0.50
0.25 21.84 ± 1.17
3.07 ± 0.23
44.09 ± 1.10 17.53 ± 18.06 43.81 ± 0.41 8.48 ± 1.13 35.46 ± 1.13
3.10 ± 0.60
0.35 17.16 ± 1.09
2.85 ± 0.12
40.14 ± 0.20 20.83 ± 10.03 39.76 ± 0.72 7.37 ± 0.59 28.41 ± 1.72
3.24 ± 1.04
0.45 13.61 ± 0.74 10.60 ± 10.49 31.21 ± 0.30 23.98 ± 9.32 29.76 ± 1.11 7.26 ± 0.76 20.43 ± 1.69 10.51 ± 11.21"
CONCLUSION,0.408955223880597,"CIFAR100
with Patch Attack,
Clean Accuracy"
CONCLUSION,0.41194029850746267,"0.15 34.08 ± 0.40 52.39 ± 0.38 47.76 ± 0.14 50.50 ± 0.41 47.21 ± 0.56 46.38 ± 0.41 42.38 ± 0.73 52.42 ± 0.59
0.25 31.72 ± 0.75 50.54 ± 0.25 44.82 ± 0.52 47.49 ± 0.91 43.89 ± 0.35 39.98 ± 0.80 35.65 ± 1.14 50.53 ± 0.55
0.35 29.50 ± 1.73 48.41 ± 0.42 40.38 ± 0.18 44.21 ± 0.21 39.80 ± 0.67 34.11 ± 1.10 28.52 ± 1.70 48.75 ± 0.71
0.45 23.93 ± 3.43 41.46 ± 5.00 31.48 ± 0.38 34.34 ± 0.91 29.79 ± 1.13 27.87 ± 2.28 20.55 ± 1.75 41.02 ± 6.06"
CONCLUSION,0.41492537313432837,"CIFAR100
with Blend Attack,
Poison Accuracy"
CONCLUSION,0.417910447761194,"0.15 33.65 ± 0.54
2.19 ± 0.28
46.65 ± 0.33
2.10 ± 0.43
46.01 ± 0.50 6.14 ± 1.12 41.57 ± 0.74
2.09 ± 0.20
0.25 30.95 ± 0.42
1.17 ± 0.08
41.84 ± 0.59
1.45 ± 0.21
41.78 ± 0.76 2.95 ± 0.56 33.54 ± 1.76
1.12 ± 0.20
0.35 27.30 ± 0.45
1.05 ± 0.06
31.88 ± 1.26
1.51 ± 0.17
34.51 ± 1.60 2.00 ± 0.49 25.71 ± 2.31
1.08 ± 0.16
0.45 20.79 ± 4.97
0.99 ± 0.07
23.61 ± 1.07
2.68 ± 1.17
22.00 ± 1.95 2.39 ± 0.17 18.62 ± 1.21
0.92 ± 0.11"
CONCLUSION,0.4208955223880597,"CIFAR100
with Blend Attack,
Clean Accuracy"
CONCLUSION,0.42388059701492536,"0.15 34.22 ± 0.58 52.65 ± 0.19 47.77 ± 0.36 48.61 ± 0.18 46.92 ± 0.47 46.01 ± 0.40 42.40 ± 0.70 52.60 ± 0.59
0.25 33.65 ± 0.55 51.12 ± 0.37 44.75 ± 0.45 45.23 ± 0.34 42.87 ± 0.72 40.47 ± 1.47 35.71 ± 1.10 50.98 ± 0.43
0.35 28.14 ± 0.48 49.80 ± 0.24 40.85 ± 0.37 40.46 ± 0.17 36.30 ± 1.24 35.70 ± 1.68 28.56 ± 2.05 49.65 ± 0.49
0.45 22.03 ± 0.49 48.46 ± 0.53 34.78 ± 1.39 34.98 ± 0.83 24.71 ± 1.37 29.91 ± 1.40 21.82 ± 1.21 48.07 ± 0.52"
CONCLUSION,0.42686567164179107,Table 3: Performance on CIFAR100. ϵ is the corruption rate.
CONCLUSION,0.4298507462686567,"Backdoor Attack Defense Accuracy.
Dataset
ϵ
BootStrap-inner
Bootstrap-outer
PRL-inner
PRL-outer
SPL-inner
SPL-outer"
CONCLUSION,0.43283582089552236,"CIFAR10 with Patch Attack,
Poison Accuracy"
CONCLUSION,0.43582089552238806,"0.15
3.15 ± 0.61
3.20 ± 0.63
80.78 ± 0.31
2.84 ± 0.23
65.50 ± 16.22
3.09 ± 0.35
0.25
2.74 ± 0.10
2.73 ± 0.09
79.07 ± 0.20
2.50 ± 0.10
18.95 ± 9.91
2.58 ± 0.19
0.35
2.70 ± 0.24
2.67 ± 0.15
76.06 ± 0.37
2.39 ± 0.26
13.45 ± 5.40
2.38 ± 0.14
0.45
2.32 ± 0.08
2.51 ± 0.11
67.87 ± 2.63
2.24 ± 0.10
12.10 ± 4.46
2.23 ± 0.26"
CONCLUSION,0.4388059701492537,"CIFAR10 with Patch Attack,
Clean Accuracy"
CONCLUSION,0.4417910447761194,"0.15
82.58 ± 0.33
82.45 ± 0.25
80.86 ± 0.31
83.09 ± 0.12
76.48 ± 3.03
83.02 ± 0.49
0.25
82.14 ± 0.28
81.87 ± 0.23
79.10 ± 0.17
83.13 ± 0.21
69.33 ± 2.57
83.30 ± 0.13
0.35
81.71 ± 0.46
81.55 ± 0.54
76.08 ± 0.34
82.83 ± 0.38
59.76 ± 3.59
83.05 ± 0.38
0.45
81.53 ± 0.16
81.00 ± 0.47
69.96 ± 0.37
82.78 ± 0.18
49.31 ± 0.53
82.84 ± 0.27"
CONCLUSION,0.44477611940298506,"CIFAR10 with Blend Attack,
Poison Accuracy"
CONCLUSION,0.44776119402985076,"0.15
29.85 ± 10.65
40.79 ± 13.27
80.38 ± 0.15
46.29 ± 18.09
72.89 ± 6.14
48.21 ± 14.90
0.25
14.81 ± 10.42
27.57 ± 10.93
78.44 ± 0.19
27.34 ± 18.42
54.46 ± 10.45
21.18 ± 11.85
0.35
6.52 ± 3.80
17.41 ± 10.13
71.93 ± 2.69
11.25 ± 5.92
46.12 ± 13.77
14.58 ± 7.12
0.45
11.58 ± 14.94
9.01 ± 4.66
64.98 ± 2.74
5.90 ± 2.28
42.30 ± 5.85
5.16 ± 1.64"
CONCLUSION,0.4507462686567164,"CIFAR10 with Blend Attack,
Clean Accuracy"
CONCLUSION,0.4537313432835821,"0.15
81.54 ± 0.25
81.18 ± 0.75
80.73 ± 0.18
82.51 ± 0.36
76.11 ± 3.32
82.35 ± 0.22
0.25
80.99 ± 1.12
80.37 ± 0.94
78.23 ± 0.46
82.35 ± 0.65
66.64 ± 2.31
82.15 ± 0.37
0.35
81.04 ± 0.81
79.54 ± 1.32
71.62 ± 2.65
82.55 ± 0.47
57.44 ± 1.78
81.81 ± 1.00
0.45
81.06 ± 0.25
78.93 ± 0.84
62.34 ± 2.51
82.15 ± 0.48
48.82 ± 0.94
81.81 ± 1.06"
CONCLUSION,0.45671641791044776,Table 4: Ablation study on CIFAR10. ϵ is the corruption rate.
CONCLUSION,0.4597014925373134,"Backdoor Attack Defense Accuracy.
Dataset
ϵ
BootStrap-inner
Bootstrap-outer
PRL-inner
PRL-outer
SPL-inner
SPL-outer"
CONCLUSION,0.4626865671641791,"CIFAR100 with Patch Attack,
Poison Accuracy"
CONCLUSION,0.46567164179104475,"0.15
45.76 ± 2.65
41.66 ± 8.37
47.34 ± 0.44
44.32 ± 5.45
43.05 ± 0.39
43.41 ± 6.36
0.25
44.41 ± 1.55
43.65 ± 0.88
44.71 ± 0.40
41.13 ± 5.82
35.69 ± 1.05
41.81 ± 3.89
0.35
40.02 ± 0.19
38.72 ± 0.60
40.19 ± 0.39
38.75 ± 0.60
24.98 ± 6.34
38.97 ± 1.10
0.45
31.12 ± 0.40
29.46 ± 0.33
31.49 ± 1.04
29.74 ± 0.35
20.13 ± 1.80
30.19 ± 0.31"
CONCLUSION,0.46865671641791046,"CIFAR100 with Patch Attack,
Clean Accuracy"
CONCLUSION,0.4716417910447761,"0.15
48.01 ± 0.28
47.91 ± 0.21
47.43 ± 0.43
48.41 ± 0.40
43.29 ± 0.21
48.12 ± 0.36
0.25
45.27 ± 0.82
44.68 ± 0.27
44.83 ± 0.38
45.58 ± 0.39
36.21 ± 0.80
45.14 ± 0.64
0.35
40.49 ± 0.23
38.98 ± 0.47
40.40 ± 0.41
39.46 ± 0.30
29.58 ± 1.49
39.89 ± 0.50
0.45
31.32 ± 0.48
29.81 ± 0.34
31.49 ± 1.02
30.39 ± 0.27
20.70 ± 1.51
30.77 ± 0.55"
CONCLUSION,0.4746268656716418,"CIFAR100 with Blend Attack,
Poison Accuracy"
CONCLUSION,0.47761194029850745,"0.15
46.72 ± 0.23
46.56 ± 0.26
46.59 ± 0.43
46.83 ± 1.00
42.15 ± 0.68
46.80 ± 0.70
0.25
41.64 ± 1.54
40.60 ± 0.98
43.43 ± 0.59
40.02 ± 1.44
34.10 ± 1.60
39.30 ± 3.16
0.35
31.18 ± 2.83
30.91 ± 2.02
35.84 ± 1.71
28.86 ± 2.82
25.36 ± 2.65
28.94 ± 3.49
0.45
22.98 ± 1.18
23.37 ± 0.92
24.60 ± 2.19
22.16 ± 3.73
19.57 ± 1.64
24.17 ± 2.70"
CONCLUSION,0.48059701492537316,"CIFAR100 with Blend Attack,
Clean Accuracy"
CONCLUSION,0.4835820895522388,"0.15
48.05 ± 0.33
47.83 ± 0.29
47.24 ± 0.65
48.43 ± 0.44
42.94 ± 0.55
48.37 ± 0.68
0.25
44.85 ± 0.52
44.59 ± 0.31
44.17 ± 0.36
45.19 ± 0.26
36.18 ± 1.20
45.06 ± 0.18
0.35
40.80 ± 0.56
40.08 ± 0.41
38.23 ± 0.80
41.84 ± 0.51
30.23 ± 1.25
41.18 ± 0.69
0.45
35.32 ± 1.77
34.13 ± 1.13
27.33 ± 1.37
39.06 ± 0.45
24.22 ± 1.66
38.62 ± 1.30"
CONCLUSION,0.48656716417910445,Table 5: Ablation study on CIFAR100. ϵ is the corruption rate.
CONCLUSION,0.48955223880597015,"backdoor data poisoning attack, adding the adversarial training on existing algorithm could largely
improve the robustness against backdoor attack.Both theoretical and empirical analysis show the
effectiveness of our proposed meta algorithm."
CONCLUSION,0.4925373134328358,Under review as a conference paper at ICLR 2022
REFERENCES,0.4955223880597015,REFERENCES
REFERENCES,0.49850746268656715,"Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th annual international conference on machine learning, pp. 41–48, 2009."
REFERENCES,0.5014925373134328,"Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung
Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by
activation clustering. arXiv preprint arXiv:1811.03728, 2018."
REFERENCES,0.5044776119402985,"Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017."
REFERENCES,0.5074626865671642,"Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310–1320. PMLR, 2019."
REFERENCES,0.5104477611940299,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.5134328358208955,"John C Duchi and Hongseok Namkoong. Learning models with uniform performance via distribu-
tionally robust optimization. The Annals of Statistics, 49(3):1378–1406, 2021."
REFERENCES,0.5164179104477612,"Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimiza-
tion for efﬁciently improving generalization. arXiv preprint arXiv:2010.01412, 2020."
REFERENCES,0.5194029850746269,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.5223880597014925,"Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017."
REFERENCES,0.5253731343283582,"Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
Advances in neural information processing systems, pp. 8527–8537, 2018."
REFERENCES,0.5283582089552239,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.5313432835820896,"Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei.
Mentornet: Learning
data-driven curriculum for very deep neural networks on corrupted labels.
arXiv preprint
arXiv:1712.05055, 2017."
REFERENCES,0.5343283582089552,"Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In International Conference
on Machine Learning, pp. 2304–2313, 2018."
REFERENCES,0.5373134328358209,"Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on con-
trolled noisy labels. In International Conference on Machine Learning, pp. 4804–4815. PMLR,
2020."
REFERENCES,0.5402985074626866,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.5432835820895522,"M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In Advances in neural information processing systems, pp. 1189–1197, 2010."
REFERENCES,0.5462686567164179,"Alexander Levine and Soheil Feizi. Deep partition aggregation: Provable defense against general
poisoning attacks. arXiv preprint arXiv:2006.14768, 2020."
REFERENCES,0.5492537313432836,"Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distil-
lation: Erasing backdoor triggers from deep neural networks. arXiv preprint arXiv:2101.05930,
2021."
REFERENCES,0.5522388059701493,Under review as a conference paper at ICLR 2022
REFERENCES,0.5552238805970149,"Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from
noisy labels with distillation. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 1910–1918, 2017."
REFERENCES,0.5582089552238806,"Boyang Liu, Mengying Sun, Ding Wang, Pang-Ning Tan, and Jiayu Zhou. Learning deep neural
networks under agnostic corrupted supervision. arXiv preprint arXiv:2102.06735, 2021."
REFERENCES,0.5611940298507463,"Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
Fine-pruning: Defending against back-
dooring attacks on deep neural networks. In International Symposium on Research in Attacks,
Intrusions, and Defenses, pp. 273–294. Springer, 2018."
REFERENCES,0.564179104477612,"Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reﬂection backdoor: A natural backdoor
attack on deep neural networks. In European Conference on Computer Vision, pp. 182–199.
Springer, 2020."
REFERENCES,0.5671641791044776,"Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101, 2017."
REFERENCES,0.5701492537313433,"Bence Major, Daniel Fontijne, Amin Ansari, Ravi Teja Sukhavasi, Radhika Gowaikar, Michael
Hamilton, Sean Lee, Slawomir Grzechnik, and Sundar Subramanian.
Vehicle detection with
automotive radar using deep learning on range-azimuth-doppler tensors. In Proceedings of the
IEEE/CVF International Conference on Computer Vision Workshops, pp. 0–0, 2019."
REFERENCES,0.573134328358209,"David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual confer-
ence on Computational learning theory, pp. 164–170, 1999."
REFERENCES,0.5761194029850746,"Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018a."
REFERENCES,0.5791044776119403,"Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 41(8):1979–1993, 2018b."
REFERENCES,0.582089552238806,"Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, and Ting Wang.
Trojanzoo: Everything you ever wanted to know about neural backdoors (but were afraid to ask).
arXiv preprint arXiv:2012.09302, 2020."
REFERENCES,0.5850746268656717,"Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944–1952, 2017."
REFERENCES,0.5880597014925373,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semideﬁnite relaxations for certifying ro-
bustness to adversarial examples. arXiv preprint arXiv:1811.01057, 2018."
REFERENCES,0.591044776119403,"Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596, 2014."
REFERENCES,0.5940298507462687,"Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck.
Provably robust deep learning via adversarially trained smoothed classiﬁers.
arXiv
preprint arXiv:1906.04584, 2019."
REFERENCES,0.5970149253731343,"Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras,
and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In
Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp.
6106–6116, 2018."
REFERENCES,0.6,"D´avid Terj´ek. Adversarial lipschitz regularization. In International Conference on Learning Repre-
sentations, 2019."
REFERENCES,0.6029850746268657,"Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. arXiv
preprint arXiv:1811.00636, 2018."
REFERENCES,0.6059701492537314,"Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. 2018."
REFERENCES,0.608955223880597,Under review as a conference paper at ICLR 2022
REFERENCES,0.6119402985074627,"Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019
IEEE Symposium on Security and Privacy (SP), pp. 707–723. IEEE, 2019."
REFERENCES,0.6149253731343284,"Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li. Rab: Provable robustness against
backdoor attacks. arXiv preprint arXiv:2003.08904, 2020."
REFERENCES,0.6179104477611941,"Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286–5295. PMLR,
2018."
REFERENCES,0.6208955223880597,"Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep neural
networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communica-
tions Security, pp. 2041–2055, 2019."
REFERENCES,0.6238805970149254,"Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7017–
7025, 2019."
REFERENCES,0.6268656716417911,"Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does dis-
agreement help generalization against label corruption? In International Conference on Machine
Learning, pp. 7164–7173. PMLR, 2019."
REFERENCES,0.6298507462686567,"Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples: Attacks and defenses for
deep learning. IEEE transactions on neural networks and learning systems, 30(9):2805–2824,
2019."
REFERENCES,0.6328358208955224,Under review as a conference paper at ICLR 2022
APPENDIX,0.6358208955223881,"7
APPENDIX"
APPENDIX,0.6388059701492538,"In this section, we provided proof of theorem and more discussion."
APPENDIX,0.6417910447761194,"7.1
PROOF OF EQUATION 3"
N,0.6447761194029851,"1
n X"
N,0.6477611940298508,"x∈X,y∈Y
[L(f(x + c), y)] ≤1 n X"
N,0.6507462686567164,"x∈Xϵ,y∈Y
φw(xi + c, y) + ϵτL"
N,0.6537313432835821,"let G denote the initially clean sample set (i.e. (X, Y)), and B the corrupted sample set (i.e. the
training set corrupted with a trigger whereas the labels are untouched). Let R denote the clean
sample set which is replaced by the adversary (i.e. R is the subset of G, and is replaced by B, i.e.
G
′ = G \ R ∪B = (Xϵ, Y)), and let φw denote the function L ◦f."
N,0.6567164179104478,"One can decompose the inner part of our mini-max objective in Equation 2 as the following,"
N,0.6597014925373135,"1
n X"
N,0.6626865671641791,"x∈X,y∈Y
[L(f(x + c), y)] = 1 n X"
N,0.6656716417910448,"i∈G′\B
φw(xi + c, y) + 1 n X"
N,0.6686567164179105,"i∈R
φw(xi + c, y) = 1 n X"
N,0.6716417910447762,"i∈G′\B
φw(xi + c, y) + 1 n X"
N,0.6746268656716418,"i∈R
φw(xi + c, y) + 1 n X"
N,0.6776119402985075,"i∈B
φw(xi + t + c, y) −1 n X"
N,0.6805970149253732,"i∈B
φw(xi + t + c, y) = 1 n X"
N,0.6835820895522388,"x∈Xϵ,y∈Y
φw(xi + c, y) +"
N,0.6865671641791045,"1
n X"
N,0.6895522388059702,"i∈R
φw(xi + c, y) −1 n X"
N,0.6925373134328359,"i∈B
φw(xi + c + t, y) ! ≤1 n X"
N,0.6955223880597015,"x∈Xϵ,y∈Y
φw(xi + c, y) + "
N,0.6985074626865672,"1
n X"
N,0.7014925373134329,"i∈R
φw(xi + c, y) −1 n X"
N,0.7044776119402985,"i∈B
φw(xi + c + t, y) ! ≤1 n X"
N,0.7074626865671642,"x∈Xϵ,y∈Y
φw(xi + c, y) + ϵL∥t∥≤1 n X"
N,0.7104477611940299,"x∈Xϵ,y∈Y
φw(xi + c, y) + ϵτL,"
N,0.7134328358208956,"7.2
PROOF OF THEOREM 1"
N,0.7164179104477612,"Theorem. Let ˜Remp
c
, Remp
c
, Rt, ϵ, τ, is deﬁned as above. Assume the prior distribution of the
network parameter w is N(0, σ), and the posterior distribution of parameter is N(w, σ) is the
posterior parameter distribution, where w is learned according to training data. Let k to be the
number of parameters, n to be the sample size, assume the objective function φw = L ◦f is Lφ-
lipschitz smooth, then, with probability at least 1-δ, we have:"
N,0.7194029850746269,"Rt ≤Remp
c
+ Lφ(2τ + ϵτ) +"
N,0.7223880597014926,"v
u
u
t"
N,0.7253731343283583,"1
4k log

1 + ∥w∥2
2
kσ2

+ 1"
N,0.7283582089552239,4 + log n
N,0.7313432835820896,δ + 2 log(6n + 3k) n −1
N,0.7343283582089553,Proof: we ﬁrst decompose the gap as following
N,0.7373134328358208,"Rt −Remp
c
= (Rt −Remp
t
) + (Remp
t
−Remp
c
) ≤|(Rt −Remp
t
)| + |(Remp
t
−Remp
c
)|"
N,0.7402985074626866,Under review as a conference paper at ICLR 2022
N,0.7432835820895523,"We bound the second part ﬁrst.
Remp
t
−Remp
c
≤∥Remp
t
−Remp
c
∥ = 1 n∥
X"
N,0.746268656716418,"x∈Xr,y∈Yr
[φ(x + t, y) −φ(x + c, y)] +  
X"
N,0.7492537313432835,"x∈Xo,y∈Yo
φ(x + t, y) −
X"
N,0.7522388059701492,"x∈Xb,y∈Yo
φ(x + c, y)  ∥ ≤1 n∥
X"
N,0.755223880597015,"x∈Xr,y∈Yr
[φ(x + t, y) −φ(x + c, y)] ∥+ 1 n∥
X"
N,0.7582089552238805,"x∈Xo,y∈Yo
φ(x + t, y) −
X"
N,0.7611940298507462,"x∈Xb,y∈Yo
φ(x + c, y)∥"
N,0.764179104477612,"≤(1 −ϵ)Lφ∥t −c∥+ ϵLφ∥t −c∥+ Lφ max
xo,xb ∥xo −xb∥"
N,0.7671641791044777,"≤(1 −ϵ)Lφ∥t −c∥+ ϵLφ∥t −c∥+ ϵLφ∥t∥
= Lφ∥t −c∥+ ϵLφ∥t∥
≤Lφ2τ + ϵLφ∥t∥
≤Lφ(2τ + ϵτ)
Now, we bound the second term. Note the second term is a typical gap term between empirical
loss and generalization loss, and there are many approaches to bound this term like VC dimension.
Since we aimed to focus the deep neural network, we follow the PAC-Bayes framework McAllester
(1999) to analyze the generalization bound. Speciﬁcally, we use results from Foret et al. (2020),"
N,0.7701492537313432,which gives s
N,0.7731343283582089,"1
4 k log

1+
∥w∥2
2
kσ2 
+ 1"
N,0.7761194029850746,4 +log n
N,0.7791044776119403,δ +2 log(6n+3k)
N,0.7820895522388059,"n−1
under the assumption of gaussian prior and
posterior. The proof for this can be found in the appendix of Foret et al. (2020) (i.e. equation 13 on
the paper)."
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.7850746268656716,"7.3
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.7880597014925373,"As we could see from the above theorem that a small lipschitz constant could bring robustness
against backdoor attack. In this section, we provided the reason why we claim adversarial training
helps lipschitz regularization. The deﬁnition of lipschitz function is ∥f(x) −f(y)∥≤L∥x −
y∥, ∀x, y. Since the lipschitz constant showes in the upper bound of the error, we would like to get
the minimum lipschitz constant to tighten the bound. Follow (Terj´ek, 2019), the minimum lipschitz
constant can be expressed as:"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.7910447761194029,"∥f∥L =
sup
x,y∈X;x̸=y"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.7940298507462686,"dY (f(x), f(y))"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.7970149253731343,"dX(x, y)"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.8,"rewrite y as x + c, we would get"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.8029850746268656,"∥f∥L =
sup
x,x+r∈X;0<dX(x,x+c)"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.8059701492537313,"dY (f(x), f(x + r))"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.808955223880597,"dX(x, x + r)"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.8119402985074626,Minimize above objective respect to function f reduces to the adversarial learning.
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.8149253731343283,"inf
f ∥f∥L = inf
f
sup
x,x+c∈X;0<dX(x,x+c)"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.817910447761194,"dY (f(x), f(x + c))"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.8208955223880597,"dX(x, x + c)"
DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING,0.8238805970149253,"If we treat the denominator as a constant, then this is exactly the same as our minimax objective.
More details can be found in (Terj´ek, 2019)."
SUPPLEMENTARY EXPERIMENT RESULTS,0.826865671641791,"7.4
SUPPLEMENTARY EXPERIMENT RESULTS"
SUPPLEMENTARY EXPERIMENT RESULTS,0.8298507462686567,"We provided the experiment hyperparameters, and supplementary results for the experiment."
EXPERIMENT HYPERPARAMETERS,0.8328358208955224,"7.4.1
EXPERIMENT HYPERPARAMETERS"
EXPERIMENT HYPERPARAMETERS,0.835820895522388,"We list the details of experiment in this section. All the methods use resnet-32 as the backbone
network. AdamW is used as the opimitzer for all methods. The perturbation limit τ is set to be 0.05
for all methods requiring τ. All methods are repeated for three different random seeds to calculate
the standard deviation."
EXPERIMENT HYPERPARAMETERS,0.8388059701492537,The trigger for badnet attack and blending attack can be found in ﬁgure 2.
EXPERIMENT HYPERPARAMETERS,0.8417910447761194,Under review as a conference paper at ICLR 2022
EXPERIMENT HYPERPARAMETERS,0.844776119402985,"(a) clean example
(b) badnet attack
(c) blend attack"
EXPERIMENT HYPERPARAMETERS,0.8477611940298507,Figure 2: Example of clean and various poisoned samples
EXPERIMENT ON MNIST,0.8507462686567164,"7.4.2
EXPERIMENT ON MNIST"
EXPERIMENT ON MNIST,0.8537313432835821,"We found interesting results on MNIST. In MNIST, we found adversarial training itself sometimes
gives robustness to the backdoor attack. We hypothesis that this is because the MNIST is potentially
a easier task than CIFAR. In here, we show the performance of adversarial training and PRL-AT on
MNIST. The results can be found at table 6."
EXPERIMENT ON MNIST,0.8567164179104477,"Backdoor Attack Defense Accuracy.
Dataset
ϵ
AT
BootStrap
Bootstrap-AT
PRL
PRL-AT
SPL
SPL-AT"
EXPERIMENT ON MNIST,0.8597014925373134,"MNIST with Patch Attack,
Poison Accuracy"
EXPERIMENT ON MNIST,0.8626865671641791,"0.15
0.30 ± 0.07
0.04 ± 0.01
3.17 ± 3.23
97.96 ± 0.21
98.44 ± 0.05
59.24 ± 33.30
85.61 ± 12.56
0.25
0.26 ± 0.17
0.04 ± 0.02
0.17 ± 0.10
89.91 ± 7.53
97.04 ± 1.07
25.25 ± 1.38
30.70 ± 6.62
0.35
0.10 ± 0.02
0.08 ± 0.06
0.14 ± 0.01
77.91 ± 10.41
97.71 ± 0.18
13.54 ± 0.76
26.03 ± 5.27
0.45
0.11 ± 0.01
0.04 ± 0.02
0.42 ± 0.34
43.42 ± 11.66
76.42 ± 8.65
12.85 ± 1.90
10.97 ± 2.16"
EXPERIMENT ON MNIST,0.8656716417910447,"MNIST with Patch Attack,
Clean Accuracy"
EXPERIMENT ON MNIST,0.8686567164179104,"0.15
98.17 ± 0.69
99.49 ± 0.05
95.48 ± 1.56
98.08 ± 0.26
98.44 ± 0.05
93.23 ± 4.72
97.74 ± 0.37
0.25
98.59 ± 0.22
99.48 ± 0.07
98.83 ± 0.19
97.46 ± 0.07
97.11 ± 1.06
86.98 ± 0.72
85.89 ± 1.76
0.35
94.48 ± 4.87
99.48 ± 0.04
98.45 ± 0.32
97.40 ± 0.46
97.86 ± 0.11
73.09 ± 4.34
77.49 ± 0.96
0.45
98.27 ± 0.43
99.42 ± 0.02
96.44 ± 1.36
75.69 ± 0.99
92.32 ± 4.25
60.58 ± 1.90
57.20 ± 0.37"
EXPERIMENT ON MNIST,0.8716417910447761,"MNIST with Blend Attack,
Poison Accuracy"
EXPERIMENT ON MNIST,0.8746268656716418,"0.15
63.42 ± 35.24
0.04 ± 0.01
96.66 ± 2.58
96.81 ± 1.30
96.74 ± 1.03
97.43 ± 0.13
96.16 ± 0.19
0.25
70.43 ± 28.61
0.04 ± 0.01
97.83 ± 0.91
77.68 ± 20.34
97.20 ± 0.66
6.43 ± 1.27
83.86 ± 2.74
0.35
58.32 ± 40.59
0.05 ± 0.03
97.94 ± 0.57
78.79 ± 17.74
97.59 ± 0.12
11.05 ± 2.70
69.69 ± 6.59
0.45
97.66 ± 1.04
0.03 ± 0.03
98.16 ± 0.58
27.18 ± 19.53
95.17 ± 1.83
4.49 ± 1.08
64.78 ± 3.14"
EXPERIMENT ON MNIST,0.8776119402985074,"MNIST with Blend Attack,
Clean Accuracy"
EXPERIMENT ON MNIST,0.8805970149253731,"0.15
64.78 ± 33.81
99.44 ± 0.02
98.29 ± 0.81
97.93 ± 0.25
96.18 ± 1.44
97.30 ± 0.22
95.93 ± 0.20
0.25
74.00 ± 25.25
99.46 ± 0.05
97.44 ± 0.99
97.30 ± 0.63
97.10 ± 0.74
77.75 ± 0.74
83.34 ± 2.81
0.35
58.62 ± 40.18
99.44 ± 0.02
97.43 ± 0.94
96.25 ± 1.84
97.39 ± 0.19
72.41 ± 3.80
67.92 ± 8.38
0.45
96.78 ± 1.42
99.42 ± 0.06
97.89 ± 0.61
76.47 ± 8.66
95.07 ± 1.59
63.63 ± 4.47
63.82 ± 4.12"
EXPERIMENT ON MNIST,0.8835820895522388,Table 6: Performance on MNIST. ϵ is the corruption rate.
EXPERIMENT ON MNIST,0.8865671641791045,"(a) clean data for binary feature value and continuous feature value (b) label ﬂipping attack on both binary feature value and continuous
feature value"
EXPERIMENT ON MNIST,0.8895522388059701,Figure 3: Example of label ﬂipping attack on both binary feature value and continuous feature value
EXPERIMENT ON MNIST,0.8925373134328358,"As we can see for the MNIST, especially for the blend attack, the poison accuracy for adversarial
training does show good performance with a large standard deviation. This is because that some
random seeds works while some random seed failed. We hypothesis that this is because MNIST"
EXPERIMENT ON MNIST,0.8955223880597015,Under review as a conference paper at ICLR 2022
EXPERIMENT ON MNIST,0.8985074626865671,"clean/poison accuracy
0.15
0.25
0.35
0.45
patch (PRL-AT)
80.25/80.15
78.22/78.14
75.10/75.04
58.90/58.90
patch(SpectralSign)
80.32/35.90
80.40/29.02
72.01/51.59
24.01/24.10
patch(Fine-Pruning)
80.34/56.67
79.50/60.85
79.10/56.84
78.73/44.21
blend (PRL-AT)
67.97/68.48
71.28/71.87
74.12/74.32
61.78/54.19
blend (SpectralSign)
83.60/70.74
81.23/75.40
76.63/66.87
62.53/41.32
blend(Fine-Pruning)
79.53/34.38
79.32/13.94
78.28/23.71
76.70/16.36"
EXPERIMENT ON MNIST,0.9014925373134328,"Table 7: Comparison of averaged performance across three random seed with other baselines on
CIFAR10. The numbers are clean accuracy/poison accuracy. Note ﬁne-pruning used 5% clean data"
EXPERIMENT ON MNIST,0.9044776119402985,"dataset has almost binary feature value. When adding a small gaussian noise on feature x, the label
ﬂipping attack cannot change the decision boundary much. That is why the noisy label algorithm
seems is not as important as the noisy label algorithm in CIFAR dataset. We plot a two dimensional
toy example in ﬁgure 3 to illustrate label ﬂipping attack on continuous features and binary features.
As we can see in the ﬁgure that for the binary value feature, the label ﬂipping attack is not easy
to change the decision boundary too much while it can easily change the decision boundary in the
continuous feature value scenario. However, this is a very rough conjecture for the reason why in
MNIST, adversarial training sometimes works. We leave the investigation of this phenomenon in
the future work."
COMPARISON AGAINST OTHER BACKDOOR DEFENSE ALGORITHM,0.9074626865671642,"7.5
COMPARISON AGAINST OTHER BACKDOOR DEFENSE ALGORITHM"
COMPARISON AGAINST OTHER BACKDOOR DEFENSE ALGORITHM,0.9104477611940298,"In this section, we show further results against other backdoor defense algorithms. There are many
existing backdoor defense algorithms. However, we found that a large fraction of those algorithms
are either designed for single target attack (Liu et al., 2018) or requires clean data (Liu et al., 2018;
Wang et al., 2019; Li et al., 2021). To investigate how our framework performs compared with the
previous study, We compare against the following two baselines in a similar setting:"
COMPARISON AGAINST OTHER BACKDOOR DEFENSE ALGORITHM,0.9134328358208955,"• spectral signature (Tran et al., 2018): ﬁltering data by examining the score of projecting to
singular vector."
COMPARISON AGAINST OTHER BACKDOOR DEFENSE ALGORITHM,0.9164179104477612,"• ﬁne-pruning (Liu et al., 2018): prune the model by deleting non-activated neurons. Note
this method uses 5% clean training data."
COMPARISON AGAINST OTHER BACKDOOR DEFENSE ALGORITHM,0.9194029850746268,"The results are in the table 7. As we can see, PRL-AT has higher poisoned accuracy against Spectral
Signature and Fine-Pruning on most settings while the clean accuracy is still high, which indicates
the effectiveness of our algorithm. With high corruption ratio, we ﬁnd that the robustness of spectral
signature and ﬁne-pruning signiﬁcantly decreases while PRL-AT still gives reasonable poison accu-
racy. Our theorem provides a principled way to design new defense algorithms by leveraging more
knowledge from noisy label attacks. In this comparison, we only use PRL as the noisy label algo-
rithm, and potentially, by using a more powerful noisy label algorithm (i.e. combination of multiple
methods), it is possible to get higher poisoning accuracy."
COMPARISON AGAINST OTHER BACKDOOR DEFENSE ALGORITHM,0.9223880597014925,"7.6
SENSITIVITY ANALYSIS OF THE ϵ"
COMPARISON AGAINST OTHER BACKDOOR DEFENSE ALGORITHM,0.9253731343283582,"One interesting question to ask is how our algorithm performs without knowing the corruption ratio.
In this section, we provided the worst-case result, in which we ﬁx ϵ = 0.5 to perform the noisy
label algorithm. When the ground truth corruption ratio is higher than a half, it is impossible to
learn any meaningful classier. We test the PRL-AT in the CIFAR10 on both badnet and blending
attacks. The results are in table 8. As we can see, our algorithm provides robustness even use highly-
overestimated estimated ϵ compared with the standard training results in table 2, which suggests our
algorithm is not sensitive to the hyperparameter ϵ."
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9283582089552239,"7.7
DISCUSSION ABOUT NOISY LABEL ALGORITHM"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9313432835820895,"One key question to ask for our framework is how to choose the noisy label algorithm. In terms
of practice, we found PRL gives consistent robustness against both badnet and blending attacks on"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9343283582089552,Under review as a conference paper at ICLR 2022
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9373134328358209,"corruption ratio
PRL-AT (patch)
PRL-AT (blend)
0.15
68.14/68.00
67.97/68.48
0.25
71.78/71.74
71.28/71.87
0.35
74.26/74.15
74.17/74.32
0.45
69.91/27.02
64.78/54.19"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9402985074626866,"Table 8: PRL-AT top 1 averaged accuracy across three random seeds. The ﬁrst number is the clean
accuracy while the second number is the poisoned accuracy. The ϵ is ﬁxed to be 0.5."
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9432835820895522,"different settings. This might be because that PRL is designed for agnostic corrupted supervision,
which is suitable for a variety of types of label noise attacks."
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9462686567164179,"From a theoretical perspective, analyzing how different noisy label algorithm can minimize the ﬁrst
term of RHS in equation 6 depends on which noisy label algorithm to use. Here we give a rough
analyze of PRL. PRL guarantees converging to the ϵ-approximated stationary point, where ϵ is the
corrupted ratio. Formally, we have the following corollary:"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9492537313432836,"Corollary 1 (Convergence of PRL to clean objective (Liu et al., 2021)). Assuming the maximum
clean gradient before loss layer has bounded operator norm:∥W∥op ≤C, applying PRL to any
ϵ-fraction supervision corrupted data, yields mint∈[T ] E (∥∇φ(wt)∥) = O(ϵ√q) for large enough
T, where q is the dimension of the supervision."
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9522388059701492,"More details can be found on the in Liu et al. (2021). According to above corollary, let wP RL is the
solution get by PRL algorithm, we can have ∥∇wP RLRemp
c
∥= O(ϵ) (i.e. assume q is small). With"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9552238805970149,Polyak-Lojasiewicz (PL) condition with some constant µ such that 1
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9582089552238806,2∥∇f(x)∥≥µ(f(x) −f ∗)
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9611940298507463,"holds, we have µ(Remp
c
−Remp∗
c
) ≤1"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9641791044776119,"2∥∇wP RLRemp
c
∥= O(ϵ). For a highly-overparameterized
deep neural network, the global optima Remp∗
c
is usually 0. Thus, we can conclude that with PL
condition, using PRL as the noisy label algorithm in our framework can guarantee Remp
c
can be"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9671641791044776,minimized to the order 1
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9701492537313433,µO(ϵ). This yields the following proposition:
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9731343283582089,"Proposition 1. Let Rt, ϵ, τ, is deﬁned as above. Assume the prior distribution of the network param-
eter w is N(0, σ), and the posterior distribution of parameter is N(w, σ) is the posterior parameter
distribution, where w is learned according to training data. Let k to be the number of parameters, n
to be the sample size, assume the objective function φw = L◦f is Lφ-lipschitz smooth and satisfying"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9761194029850746,"the PL condition, which is 1"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9791044776119403,"2∥∇φw∥≥µ(φw −φw∗). then, with the assumption of bounded oper-
ator norm of gradient before loss layer, we have with probability at least 1-δ, by applying PRL-AT,
we have: Rt ≤1"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.982089552238806,µO(ϵ) + Lφ(2τ + ϵτ) +
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9850746268656716,"v
u
u
t"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9880597014925373,"1
4k log

1 + ∥w∥2
2
kσ2

+ 1"
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.991044776119403,4 + log n
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9940298507462687,δ + 2 log(6n + 3k) n −1
DISCUSSION ABOUT NOISY LABEL ALGORITHM,0.9970149253731343,"In general, considering φ is a deep neural network, we believe the ﬁrst term is difﬁcult to analyze
without further assumption (i.e. PL condition). Nevertheless, empirical study shows that many noisy
label algorithms can effectively minimize the ﬁrst term loss, which motivates our method to treat
those algorithms as a black-box algorithm."
