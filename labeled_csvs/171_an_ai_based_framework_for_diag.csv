Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.011111111111111112,"The quality of radiographs is of major importance for diagnosis and treatment planning.
While most research regarding automated radiograph quality assessment uses technical
features such as noise or contrast, we propose to use anatomical structures as more appro-
priate features. We show that based on such anatomical features, a modular deep-learning
framework can serve as a quality control mechanism for the diagnostic quality of ankle
radiographs. For evaluation, a dataset consisting of 950 ankle radiographs was collected
and their quality was labeled by radiologists. We obtain an average accuracy of 94.1%,
which is better than the expert radiologists are on average.
Keywords: radiographs, quality assessment, anatomical features, deep learning"
INTRODUCTION,0.022222222222222223,1. Introduction
INTRODUCTION,0.03333333333333333,"As one of the most frequently used imaging modalities, radiographs are of signiﬁcant impor-
tance for diagnosis and treatment planning. For these tasks, a diagnostic-adequate image
quality is mandatory.
Currently, radiographers have to decide if the quality of the radiograph suﬃces for the
diagnosis or if the imaging process must be repeated. To not be able to immediately judge
the diagnostic quality correctly can result in various disadvantages including unnecessary
radiation exposure. Reasons for misjudging image quality as suﬃcient may be time pressure,
inexperience or overtiredness, in which case the treating radiologist has to schedule a new
examination resulting in additional eﬀort. In the worst case, the radiographer would take
a second radiograph although the ﬁrst one was suﬃcient and thereby re-expose the patient
to radiation.
To prevent these errors and to establish a quality control mechanism, an
automated quality assessment can help.
While there is extensive research assessing radiograph quality based on technical factors
such as contrast and noise (Esses et al., 2018; Takaki et al., 2020; Wang et al., 2020), these"
INTRODUCTION,0.044444444444444446,Mairh¨ofer Laufer Simon Sieren Bischof K¨aster Barth Barkhausen Martinetz
INTRODUCTION,0.05555555555555555,"parameters are less important with digital radiography. A more important quality criterion,
is the alignment of the body part to be X-rayed relative to the X-ray machine. A radiograph
may be of perfect technical quality but can nevertheless be worthless for diagnostic purposes
if relevant anatomical structures are not visible due to misalignment. To our knowledge,
previous approaches do not assess the quality of a radiograph based on this criterion.
In this paper, we propose a framework based on classiﬁcation and segmentation Neural
Networks, which assesses the diagnostic quality of ankle radiographs based on anatomical
features. Furthermore, we test the framework on a new dataset containing radiographs of
ankles, with 950 radiographs in two diﬀerent radiographic views (anterior posterior and
lateral), all labeled by four radiologists. Using this framework, radiographers will be able
to immediately get a ﬁrst quality assessment of the taken radiographs without relying on a
radiologist. Besides reducing the described judgment errors, the framework can be used as
a quality control mechanism to detect causes for low quality radiographs."
LIT REVIEW,0.06666666666666667,2. Related Work
LIT REVIEW,0.07777777777777778,"In recent years, deep learning has become more common in radiology (Choy et al., 2018;
Saba et al., 2019). Scientists working on radiographs successfully applied Neural Networks
to detect fractures (Lindsey et al., 2018; Thian et al., 2019), classify body parts (Agunwa
et al., 2019), and radiographic views (Fang et al., 2020), to facilitate the work process in
radiology. Although our proposed framework also includes radiographic view recognition,
these steps are only part of the preprocessing for assessing diagnostic quality. Distinct to
Fang et al. (2020), where only a single step is used for recognition, we use multiple steps
containing diﬀerent networks and resign to recognize laterality."
LIT REVIEW,0.08888888888888889,"Esses et al. (2018) and (Wang et al., 2020) focus on automated diagnostic quality evalu-
ation of MRT images using Neural Networks. Due to the diﬀerent modalities of the imaging
systems one can not easily transfer the results to radiographs.
Approaches that automatically asses the perceptual quality of radiographs only take
technical parameters such as noise and contrast into account and rely on conventional
computer vision methods (Samei et al., 2014; Willis et al., 2018). Takaki et al. (2020) present
a deep learning approach to calculate the target exposure index for chest radiographs based
on perceptual quality of small patches.
To our knowledge, there are no studies considering anatomical features for the diagnostic
quality of radiographs in a deep-learning framework."
IMPLEMENTATION/METHODS,0.1,3. Proposed Framework
IMPLEMENTATION/METHODS,0.1111111111111111,"To solve the challenge of diagnostic quality assessment and to standardize the required steps,
we propose a framework of several Neural Networks that is able to process radiographs of
ankles and to output their diagnostic quality. It consists of the following steps: Recognition
of radiographic view, extraction of the region of interest (ROI), and quality assessment.
The ﬁrst step relies on the fact that radiographs can be ordered hierarchically by ra-
diographic view. A prediction of quality strongly depends on the view since corresponding
criteria for radiographic views may diﬀer. The second step prepares the input for quality
assessment by removing unnecessary information."
IMPLEMENTATION/METHODS,0.12222222222222222,An AI-based Framework for Diagnostic Quality Assessment of Ankle Radiographs
IMPLEMENTATION/METHODS,0.13333333333333333,"Figure 1: Schematic ﬂow of a radiograph through the framework. For each radiograph the
framework decides ﬁrst which radiographic view was used. Depending on that
decision the radiograph is passed to the corresponding region of interest (ROI)
segmentation network. After segmentation, the resulting ROI is fed into the ﬁnal
quality prediction network, which outputs the quality assessment."
IMPLEMENTATION/METHODS,0.14444444444444443,"Each individual step can be used independently. But only within the whole framework
they provide the possibility to decide whether an ankle radiograph is of high or low diag-
nostic quality, thereby directly supporting the radiographers in their decision process. A
complete overview of the framework can be seen in Figure 1. In the following, we describe
each step in more detail."
IMPLEMENTATION/METHODS,0.15555555555555556,3.1. Recognition of the Radiographic View
IMPLEMENTATION/METHODS,0.16666666666666666,"The ﬁrst step of the proposed framework consists of recognizing the radiographic view of the
radiograph. This classiﬁcation task is mandatory since radiographs of various radiographic
views diﬀer in quality assessment characteristics, as shown in Figure 2. By dividing the
quality assessment task for an ankle radiograph into a view-speciﬁc task, we facilitate the
learning process of our networks, since the radiographs now belong to the same domain."
IMPLEMENTATION/METHODS,0.17777777777777778,3.2. Extraction of the ROI
IMPLEMENTATION/METHODS,0.18888888888888888,"While the entire radiograph is relevant for diagnosis, only a fraction is needed for assessing
the quality of the standard projection (red marks in Figure 2). Based on this fact, the next
step in the framework is to segment this ROI which contains the most information relevant
for the diagnostic quality. An example ROI is shown in Figure 3. Since there are diﬀerent
quality characteristics in the radiographic views, we trained Neural Networks individually
for each view. Besides removing irrelevant information, the beneﬁt of extracting ROIs is
that the subsequent quality assessment can operate on a standardized size and resolution
of the relevant image part."
IMPLEMENTATION/METHODS,0.2,Mairh¨ofer Laufer Simon Sieren Bischof K¨aster Barth Barkhausen Martinetz
IMPLEMENTATION/METHODS,0.2111111111111111,3.3. Quality Assessment
IMPLEMENTATION/METHODS,0.2222222222222222,"Getting standardized ROIs of a particular radiographic view is the basis for assessing the
diagnostic quality with high accuracy. We use two diﬀerent Neural Networks, one for each of
the two radiographic views. These are trained individually on the anterior posterior (AP)
and lateral (LAT) ROI, respectively and output the quality on a continuous scale from 1
to 3 (see Section 4.2)."
IMPLEMENTATION/METHODS,0.23333333333333334,4. Datasets
IMPLEMENTATION/METHODS,0.24444444444444444,"To test the framework presented in Section 3 two datasets were created. The ﬁrst one is a
collection of ankle radiographs as DICOM images and associated metadata. The second one,
which to our knowledge did not exist previously in this or similar form, contains radiographs
labeled by radiologists according to diagnostic quality based on anatomical features. Both
datasets contain radiographs from ﬁve diﬀerent X-ray machines."
IMPLEMENTATION/METHODS,0.25555555555555554,"(a)
(b)"
IMPLEMENTATION/METHODS,0.26666666666666666,"Figure 2: In (a) the most relevant anatomical struc-
tures in the AP radiographic view are
highlighted. These include the joint gap
between medial malleolus and talus as well
as lateral malleolus and talus. In (b) the
joint space between the distal tibia and
the talus is highlighted as the most rele-
vant structure for the LAT view."
IMPLEMENTATION/METHODS,0.2777777777777778,"(a)
(b)"
IMPLEMENTATION/METHODS,0.28888888888888886,"Figure 3: (a) shows an example ROI
of a radiograph in AP view
with perfect alignment in
the upper row and strong
misalignment in the lower.
(b) shows the same for the
LAT view."
IMPLEMENTATION/METHODS,0.3,4.1. Weakly Labeled Dataset for Recognition of the Radiographic View
IMPLEMENTATION/METHODS,0.3111111111111111,"We used a dataset of 26542 ankle radiographs provided by the University Hospital Schleswig-
Holstein, Campus L¨ubeck. From those radiographs we extracted labels for the radiographic
view (LAT or AP) with a keyword matching on the metadata.
The resulting dataset
contains roughly 12000 radiographs for each view. Since creating the metadata is mostly
done manually and the content is not standardized, we assume that not all labels are
accurate."
IMPLEMENTATION/METHODS,0.32222222222222224,An AI-based Framework for Diagnostic Quality Assessment of Ankle Radiographs
IMPLEMENTATION/METHODS,0.3333333333333333,4.2. Diagnostic Quality Dataset
IMPLEMENTATION/METHODS,0.34444444444444444,"In order to learn the relationship between the radiographs and the quality, an annotated
dataset is needed. To create such a dataset, four radiologists labeled 950 ankle radiographs,
containing 475 for LAT and AP each.
The radiologists determined which objective criteria a radiograph of an ankle has to
fulﬁll, to be of high diagnostic quality. One important criterion, for instance, is the complete
visibility of the joint gap between medial malleolus and talus. A high diagnostic quality is
a prerequisite for the radiologist to make a correct diagnosis. According to that criteria,
each radiograph was labeled by each radiologist as 1 if the radiograph fulﬁlled the criteria
perfectly, 2 if partly and 3 if the criteria were not met, and a new radiograph would have
to be taken. In order to determine whether a radiograph can be used for a diagnosis, the
classes 1 and 2 were grouped under the label diagnostic and the class 3 was labeled as not
diagnostic. If the labels diﬀered greatly, the radiologists had a consensus meeting. Of the
475 · 4 labels assigned for the AP radiographs, 37% are 1s, 53% 2s and 10% 3s. For the
LAT view 17% of the assigned labels are 1s, 55% 2s and 28% 3s. Examples for the three
classes can be seen in the Appendix in Figure 5 (a-c) for the AP view and Figure 6 (a-c)
for the LAT view.
Additionally, each of the 950 radiographs was labeled with a ROI. As described in
Section 3.2 only a fraction of the radiograph is relevant for the diagnostic quality. Therefore,
the ROI was labeled as a square containing only the most relevant information. This can
be seen in Figure 3. In Figure 4, which shows examples of ground truth ROI labels, it can
be seen that the size of each ROI is highly dependent on the image content."
RESULTS/EXPERIMENTS,0.35555555555555557,5. Experiments and Results
RESULTS/EXPERIMENTS,0.36666666666666664,"To evaluate the framework described in Section 3, each step was implemented using PyTorch
and evaluated on the datasets of Section 4.
To improve quality control measurements,
we tested each step individually.
Because of the relatively small datasets, we used the
EﬃcientNet-B0 (Tan and Le, 2019) for classiﬁcation. For segmentation a DeepLabV3 (Chen
et al., 2017) with a ResNet-50 (He et al., 2016) backbone was used. Both networks were
not pretrained.
For all experiments we padded the input radiograph with zeros to get
the desired size while maintaining the aspect ratio. Furthermore, the training radiographs
were augmented with random cropping, histogram normalization, Gaussian noise, blurring,
horizontal ﬂipping and rotation. Training and test datasets were split with an 80/20 ratio."
RESULTS/EXPERIMENTS,0.37777777777777777,5.1. Recognition of the Radiographic View
RESULTS/EXPERIMENTS,0.3888888888888889,"For the recognition of diﬀerent radiographic views, the dataset described in Section 4.1 was
used. Therefore, the last layer of the EﬃcientNet-B0 was modiﬁed to output two classes,
either LAT or AP, which was followed by a softmax layer to obtain class probabilities. The
model was trained using the cross-entropy as loss function and stochastic gradient descent
(SGD) as optimizer using a learning rate of 1 · 10−3, a momentum of 0.9, a weight decay
of 1 · 10−5, and a batch size of 8 over 500,000 iterations. To reduce possible overﬁtting,
the drop connect (Wan et al., 2013) rate was set to 0.4. The resulting input size of the
radiographs, after augmentation, was 224 × 224 pixels."
RESULTS/EXPERIMENTS,0.4,Mairh¨ofer Laufer Simon Sieren Bischof K¨aster Barth Barkhausen Martinetz
RESULTS/EXPERIMENTS,0.4111111111111111,"Training with these parameters resulted in an accuracy of 98.4% for the test set and
98.5% for the training set. The results must be interpreted with a certain caution due to
the potentially incorrectly assigned labels in the weakly labeled dataset. It may be that
(i) the model predicts the correct class but the label is assigned incorrectly or that (ii) the
model predicts the incorrect class and the label is also assigned incorrectly.
Reviewing
the resulting radiographs for case (ii) revealed 54 wrong labels for the test set and 244 for
the trainings set. Taking this into account the accuracy increased to 99.5%, respectively
to 99.7% for the training set. Although the actual accuracy may be slightly lower due to
errors of case (i), these results clearly demonstrate that a recognition of the radiographic
view can be achieved with high precision."
RESULTS/EXPERIMENTS,0.4222222222222222,5.2. Extraction of the ROI
RESULTS/EXPERIMENTS,0.43333333333333335,"To segment the ROI, a DeepLabV3 was trained with the labels described in Section 4.2.
The target feature map is binary, with 0 for not ROI and 1 for ROI. As segmentation output
we used a single feature map, followed by a sigmoid function, to get pixel-wise outputs from
0 to 1. For the training we used the mean over the pixel wise squared error, optimized with
the Adam optimizer, a learning rate of 1 · 10−4, a weight decay of 1 · 10−4, and a batch size
of 4 over 50,000 iterations. For this task the input size after augmentation was 400 × 400
pixels. This training was done separately for LAT and AP views. Given the small dataset
we used a random sub-sampling validation over 12 diﬀerent dataset splits.
To measure the accuracy of the predicted ROIs the Dice score was calculated.
If a
pixel value of the output feature map was above a threshold of 0.7, the pixel was classiﬁed
as part of the ROI. Over all 12 dataset splits the mean Dice score was 94.17% on the
AP views and 85.91% on the LAT views.
A reason for the worse result on the LAT
views might be that the ROIs on the LAT views are signiﬁcantly smaller than on the
AP view and thus harder to predict. Regardless of this diﬀerence in the Dice score the
resulting segmentations are suﬃcient to get bounding boxes of the ROIs, which can be seen
in Figure 4. To extract bounding boxes based on the segmentation, ﬁrst the smallest ﬁtting
rectangle of the segmentation is calculated and then rotated to be horizontal. Examples
with the labeled and the predicted ROIs can be seen in Figure 4."
RESULTS/EXPERIMENTS,0.4444444444444444,5.3. Quality Assessment
RESULTS/EXPERIMENTS,0.45555555555555555,"For the quality assessment task an EﬃcientNet-B0 was used.
To preserve the intrinsic
order of the classes we modeled the task as a regression. One beneﬁt of using regression is
that we obtain intermediate scores. We also trained classiﬁcation networks using the earth
mover’s distance but this led to slightly worse results. The model was trained using the
mean squared error (MSE) as loss and the mean label of the four radiologist as target. The
loss was minimized by SGD using a learning rate of 1 · 10−3, a momentum of 0.9, a weight
decay of 1 · 10−3, and a batch size of 16 over 500,000 iterations. As in Section 5.1 the input
size was 224 × 224 pixels. The same random sub-sampling validation as in Section 5.2 was
used for testing.
To evaluate the accuracy of the model, an output was classiﬁed as correct if the nearest
class to the continuous output was the class of the label. Evaluation on the test set resulted
in a mean accuracy of 93.0% for the AP view and 95.1% for the LAT view, with a mean"
RESULTS/EXPERIMENTS,0.4666666666666667,An AI-based Framework for Diagnostic Quality Assessment of Ankle Radiographs
RESULTS/EXPERIMENTS,0.4777777777777778,"(a)
(b)"
RESULTS/EXPERIMENTS,0.4888888888888889,"Figure 4: In (a) two radiographs in the AP view are shown. Their labeled ROI is marked
with a blue box and the predicted ROI with a red box. The predicted segmenta-
tion mask used to construct the red box is highlighted. The same is shown in (b)
for the LAT view. Both examples also show that the proportion of ROI in the
radiograph can vary greatly."
RESULTS/EXPERIMENTS,0.5,"absolute error of 0.19 for AP and 0.20 for LAT. Over the 12 runs the standard deviation is
0.025 and 0.02 and the median accuracy 93.4% and 95.4% for AP and LAT, respectively.
The classiﬁcation into diagnostic and non-diagnostic (see Section 4.2) resulted in an accu-
racy of 97.8% for the AP view and 93.2% for the LAT view. This accuracy shift is because
there are diﬀerent distributions of 1s and 3s in the AP and LAT parts of the dataset.
To evaluate whether the accuracy of the quality assessment beneﬁts from the steps
described in Sections 3.1 and 3.2, we repeated the training with and without these steps.
The results, which are given in Table 1, show, that each step of the pipeline improves
the accuracy. Overall, the mean accuracy improves from 82.4% to 94.1% when all steps
are included. While the beneﬁt of training separately for the diﬀerent views is small, the
extraction of ROIs seems to be necessary to obtain high accuracy. When trained without
the previous view recognition, a single model is trained on the combined AP and LAT data
to predict the quality of both views. For this each view is sampled equally often.
To get an estimation on how accurate the labels are, we tested each labeling radiologist
against the others, taking one label as prediction and the mean of the remaining three as
ground truth. If the diﬀerence between prediction and ground truth was at least 1, the
prediction was counted as wrong. This resulted in a mean accuracy of 92.6% for AP and
90.1% for LAT. Across the four radiologists the standard deviation is 0.026 and 0.037 for
AP and LAT, respectively. The mean accuracy over both views is 94.1% for the networks
and 91.4% for the radiologists. Although our method, for evaluating the performance of
the radiologists, is based on only four experts it should suﬃce as a ﬁrst estimate.
A visual comparison of the expert labels and framework predictions on the unlabeled
dataset can be seen in the Appendix in Figure 5 for the AP view and Figure 6 for the LAT
view. For further illustration the ROIs with the highest error between expert label and
predicted quality are shown in Figure 7. Note that there is no clear pattern that explains
the deviation."
RESULTS/EXPERIMENTS,0.5111111111111111,Mairh¨ofer Laufer Simon Sieren Bischof K¨aster Barth Barkhausen Martinetz
RESULTS/EXPERIMENTS,0.5222222222222223,"Table 1: Accuracy of quality assessment
depending on the steps View
Recognition (Section 3.1) and
ROI Extraction (Section 3.2).
Not training separately for AP
and LAT and not extracting the
ROI leads to the lowest accu-
racy. Both steps on their own
increased the accuracy, while
using both provided the best re-
sult."
RESULTS/EXPERIMENTS,0.5333333333333333,"View
Recog.
ROI
Ext.
Accuracy"
RESULTS/EXPERIMENTS,0.5444444444444444,"mean
AP
LAT"
RESULTS/EXPERIMENTS,0.5555555555555556,"

82.4%
80.3%
84.5%


85.1%
82.9%
87.2%


92.4%
92.2%
92.5%


94.1%
93.0%
95.1%"
RESULTS/EXPERIMENTS,0.5666666666666667,"Table 2: Overview of all steps in the frame-
work and their results. The results
for the View Recognition and the
Quality Assessment are the achieved
accuracy.
For the ROI Extraction
the result is the achieved Dice score.
The AP and LAT results are not
from the same model, because we
trained individually for each view.
Since this is not the case for the View
Recognition, there is only a single ac-
curacy."
RESULTS/EXPERIMENTS,0.5777777777777777,"Step
Accuracy or Dice"
RESULTS/EXPERIMENTS,0.5888888888888889,"mean
AP
LAT"
RESULTS/EXPERIMENTS,0.6,"View Recognition
99.5%
–
–
ROI Extraction
90.1%
94.2%
85.9%
Quality Assessment
94.1%
93.0%
95.1%"
CONCLUSION/DISCUSSION ,0.6111111111111112,6. Discussion
CONCLUSION/DISCUSSION ,0.6222222222222222,"The aim of this paper was to develop a framework for automatic quality assessment and to
evaluate how well it performs. We were able to show that the accuracies of the predicted
quality (93.0% anterior posterior, 95.1% lateral) are better than those made by radiologists
(92.6% anterior posterior, 90.1% lateral). The results of the individual steps included in
the framework are summarized in Table 2.
With this framework it is now possible for
radiographers to immediately get a ﬁrst feedback on the same level of expertise as they
would get from a radiologist. These results support our view that anatomical features can
be learned and are therefore suitable for the automatic assessment of diagnostic quality.
In order to achieve these results, an initial separation of the radiographs into lateral and
anterior posterior was necessary. This task could be achieved with an accuracy of 99.5%."
CONCLUSION/DISCUSSION ,0.6333333333333333,"If our framework had been already in place when capturing the 950 radiographs of
our dataset, 80.0% of the non-diagnostic radiographs would have been immediately and
correctly recognized as such. Since 12.9% of the dataset are non-diagnostic radiographs,
for every 100 radiographs the number of additional needed appointments for examinations
could have been decreased from 13 to only 3."
CONCLUSION/DISCUSSION ,0.6444444444444445,"Regarding scalability, our experiments show that about 500 labeled radiographs per
radiographic view are suﬃcient to train a network to the accuracy level of an expert. We
assume that the framework can be transferred to radiographs of other body parts.
In
addition to its use in day-to-day operations, the framework can potentially help to comply
with quality standards and optimize the clinical routine."
CONCLUSION/DISCUSSION ,0.6555555555555556,An AI-based Framework for Diagnostic Quality Assessment of Ankle Radiographs
CONCLUSION/DISCUSSION ,0.6666666666666666,Acknowledgments
CONCLUSION/DISCUSSION ,0.6777777777777778,"We thank Hauke Gerdes, Jan Preuß and Fabio Leal dos Reis for their help and support in
collecting and labeling the radiograph datasets. This work was funded by the Bundesmin-
isterium f¨ur Wirtschaft und Energie (BMWi) through the KI-SIGS project."
REFERENCES,0.6888888888888889,References
REFERENCES,0.7,"Chinyere Agunwa, Mehdi Moradi, Ken C. L. Wong, and Tanveer Syeda-Mahmood. Body
part and imaging modality classiﬁcation for a general radiology cognitive assistant. In
Medical Imaging 2019: Image Processing, volume 10949, page 1094910. International
Society for Optics and Photonics, March 2019. doi: 10.1117/12.2513074."
REFERENCES,0.7111111111111111,"Liang-Chieh Chen, George Papandreou, Florian Schroﬀ, and Hartwig Adam. Rethinking
Atrous Convolution for Semantic Image Segmentation. arXiv:1706.05587 [cs], December
2017."
REFERENCES,0.7222222222222222,"Garry Choy, Omid Khalilzadeh, Mark Michalski, Synho Do, Anthony E. Samir, Oleg S.
Pianykh, J. Raymond Geis, Pari V. Pandharipande, James A. Brink, and Keith J. Dreyer.
Current Applications and Future Impact of Machine Learning in Radiology. Radiology,
288(2):318–328, June 2018. ISSN 0033-8419. doi: 10.1148/radiol.2018171820."
REFERENCES,0.7333333333333333,"Steven J. Esses, Xiaoguang Lu, Tiejun Zhao, Krishna Shanbhogue, Bari Dane, Mary Bruno,
and Hersh Chandarana. Automated image quality evaluation of T2 -weighted liver MRI
utilizing deep learning architecture. Journal of magnetic resonance imaging: JMRI, 47
(3):723–728, March 2018. ISSN 1522-2586. doi: 10.1002/jmri.25779."
REFERENCES,0.7444444444444445,"Xiang Fang, Leah Harris, Wei Zhou, and Donglai Huo. Generalized Radiographic View
Identiﬁcation with Deep Learning. Journal of Digital Imaging, December 2020. ISSN
1618-727X. doi: 10.1007/s10278-020-00408-z."
REFERENCES,0.7555555555555555,"K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
770–778, June 2016. doi: 10.1109/CVPR.2016.90."
REFERENCES,0.7666666666666667,"Robert Lindsey, Aaron Daluiski, Sumit Chopra, Alexander Lachapelle, Michael Mozer,
Serge Sicular, Douglas Hanel, Michael Gardner, Anurag Gupta, Robert Hotchkiss, and
Hollis Potter. Deep neural network improves fracture detection by clinicians. Proceedings
of the National Academy of Sciences, 115(45):11591–11596, November 2018. ISSN 0027-
8424, 1091-6490. doi: 10.1073/pnas.1806905115."
REFERENCES,0.7777777777777778,"Luca Saba, Mainak Biswas, Venkatanareshbabu Kuppili, Elisa Cuadrado Godia, Harman S.
Suri, Damodar Reddy Edla, Tomaˇz Omerzu, John R. Laird, Narendra N. Khanna, Sophie
Mavrogeni, Athanasios Protogerou, Petros P. Sﬁkakis, Vijay Viswanathan, George D.
Kitas, Andrew Nicolaides, Ajay Gupta, and Jasjit S. Suri. The present and future of
deep learning in radiology. European Journal of Radiology, 114:14–24, May 2019. ISSN
0720-048X. doi: 10.1016/j.ejrad.2019.02.038."
REFERENCES,0.7888888888888889,Mairh¨ofer Laufer Simon Sieren Bischof K¨aster Barth Barkhausen Martinetz
REFERENCES,0.8,"Ehsan Samei, Yuan Lin, Kingshuk R. Choudhury, and H. Page McAdams.
Automated
characterization of perceptual quality of clinical chest radiographs: Validation and cali-
bration to observer preference. Medical Physics, 41(11):111918, 2014. ISSN 2473-4209.
doi: 10.1118/1.4899183."
REFERENCES,0.8111111111111111,"Takeshi Takaki, Seiichi Murakami, Ryo Watanabe, Takatoshi Aoki, and Toshioh Fujibuchi.
Calculating the target exposure index using a deep convolutional neural network and a
rule base. Physica Medica, 71:108–114, March 2020. ISSN 1120-1797. doi: 10.1016/j.
ejmp.2020.02.012."
REFERENCES,0.8222222222222222,"Mingxing Tan and Quoc Le.
EﬃcientNet: Rethinking Model Scaling for Convolutional
Neural Networks. In International Conference on Machine Learning, pages 6105–6114.
PMLR, May 2019."
REFERENCES,0.8333333333333334,"Yee Liang Thian, Yiting Li, Pooja Jagmohan, David Sia, Vincent Ern Yao Chan, and
Robby T. Tan. Convolutional Neural Networks for Automated Fracture Detection and
Localization on Wrist Radiographs. Radiology: Artiﬁcial Intelligence, 1(1):e180001, Jan-
uary 2019. doi: 10.1148/ryai.2019180001."
REFERENCES,0.8444444444444444,"Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of
Neural Networks using DropConnect. In International Conference on Machine Learning,
pages 1058–1066. PMLR, May 2013."
REFERENCES,0.8555555555555555,"Yida Wang, Yang Song, Fang Wang, Jingjing Sun, Xinyi Gao, Zhe Han, Lei Shi, Guoliang
Shao, Mingxia Fan, and Guang Yang. A two-step automated quality assessment for liver
MR images based on convolutional neural network. European Journal of Radiology, 124,
March 2020. ISSN 0720-048X, 1872-7727. doi: 10.1016/j.ejrad.2020.108822."
REFERENCES,0.8666666666666667,"Charles E. Willis, Thomas K. Nishino, Jered R. Wells, H. Asher Ai, Joshua M. Wilson, and
Ehsan Samei. Automated quality control assessment of clinical chest images. Medical
Physics, 45(10):4377–4391, October 2018. ISSN 2473-4209. doi: 10.1002/mp.13107."
REFERENCES,0.8777777777777778,An AI-based Framework for Diagnostic Quality Assessment of Ankle Radiographs
OTHER,0.8888888888888888,Appendix A. Example ROI Images
OTHER,0.9,"(a)
(b)
(c)
(d)
(e)
(f )"
OTHER,0.9111111111111111,"Figure 5: Each column shows ﬁve example ROIs of the labeled dataset in the anterior
posterior view with the expert label 1(a), 2(b), and 3(c); and ﬁve examples of
unlabeled ROIs for which our framework predicts the quality classes 1(d), 2(e),
and 3(f )."
OTHER,0.9222222222222223,Mairh¨ofer Laufer Simon Sieren Bischof K¨aster Barth Barkhausen Martinetz
OTHER,0.9333333333333333,"(a)
(b)
(c)
(d)
(e)
(f )"
OTHER,0.9444444444444444,"Figure 6: Each column shows ﬁve example ROIs of the labeled dataset in the lateral view
with the expert label 1(a), 2(b), and 3(c); and ﬁve examples of unlabeled ROIs
for which our framework predicts the quality classes 1(d), 2(e), and 3(f )."
OTHER,0.9555555555555556,An AI-based Framework for Diagnostic Quality Assessment of Ankle Radiographs
OTHER,0.9666666666666667,Appendix B. Failure Cases
OTHER,0.9777777777777777,"(a)
(b)
(c)
(d)
(e)
(f )"
OTHER,0.9888888888888889,"Figure 7: Each column shows ﬁve ROIs of the labeled dataset in the anterior posterior
view with the expert label 1(a), 2(b), and 3(c); and ﬁve examples of ROIs in
the lateral view with the expert label 1(d), 2(e), and 3(f ). The quality assessed
by our framework is printed on each ROI. For each class and view the ﬁve ROIs
with the highest error between expert label and predicted quality are shown."
