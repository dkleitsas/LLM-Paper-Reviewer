Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010976948408342481,"In recent years, there has been a significant growth in research focusing on min-
1"
ABSTRACT,0.0021953896816684962,"imum ℓ2 norm (ridgeless) interpolation least squares estimators. However, the
2"
ABSTRACT,0.003293084522502744,"majority of these analyses have been limited to an unrealistic regression error struc-
3"
ABSTRACT,0.0043907793633369925,"ture, assuming independent and identically distributed errors with zero mean and
4"
ABSTRACT,0.005488474204171241,"common variance. In this paper, we explore prediction risk as well as estimation
5"
ABSTRACT,0.006586169045005488,"risk under more general regression error assumptions, highlighting the benefits of
6"
ABSTRACT,0.007683863885839737,"overparameterization in a more realistic setting that allows for clustered or serial
7"
ABSTRACT,0.008781558726673985,"dependence. Notably, we establish that the estimation difficulties associated with
8"
ABSTRACT,0.009879253567508232,"the variance components of both risks can be summarized through the trace of the
9"
ABSTRACT,0.010976948408342482,"variance-covariance matrix of the regression errors. Our findings suggest that the
10"
ABSTRACT,0.012074643249176729,"benefits of overparameterization can extend to time series, panel and grouped data.
11"
INTRODUCTION,0.013172338090010977,"1
Introduction
12"
INTRODUCTION,0.014270032930845226,"Recent years have witnessed a fast growing body of work that analyzes minimum ℓ2 norm (ridgeless)
13"
INTRODUCTION,0.015367727771679473,"interpolation least squares estimators [see, e.g., 2, 17, 27, and references therein]. Researchers in this
14"
INTRODUCTION,0.01646542261251372,"field were inspired by the ability of deep neural networks to accurately predict noisy training data
15"
INTRODUCTION,0.01756311745334797,"with perfect fits, a phenomenon known as “double descent” or “benign overfitting” [e.g., 3–5, 29, 22,
16"
INTRODUCTION,0.018660812294182216,"among many others]. They discovered that to achieve this phenomenon, overparameterization is
17"
INTRODUCTION,0.019758507135016465,"critical.
18"
INTRODUCTION,0.020856201975850714,"In the setting of linear regression, we have the training data {(xi, yi) ∈Rp × R : i = 1, · · · , n}, where
19"
INTRODUCTION,0.021953896816684963,"the outcome variable yi is generated from
20"
INTRODUCTION,0.02305159165751921,"yi = x⊤
i β + εi, i = 1, . . . , n,"
INTRODUCTION,0.024149286498353458,"xi is a vector of features (or regressors), β is a vector of unknown parameters, and εi is a regression
21"
INTRODUCTION,0.025246981339187707,"error. Here, n is the sample size of the training data and p is the dimension of the parameter vector β.
22"
INTRODUCTION,0.026344676180021953,"In the literature, the main object for the theoretical analyses has been mainly on the out-of-sample
23"
INTRODUCTION,0.027442371020856202,"prediction risk. That is, for the ridge or interpolation estimator ˆβ, the literature has focused on
24"
INTRODUCTION,0.02854006586169045,"E
h
(x⊤
0 ˆβ −x⊤
0 β)2 | x1, . . . , xn
i
,"
INTRODUCTION,0.029637760702524697,"where x0 is a test observation that is identically distributed as xi but independent of the training data.
25"
INTRODUCTION,0.030735455543358946,"For example, Dobriban and Wager [13], Wu and Xu [28], Richards et al. [23], Hastie et al. [17]
26"
INTRODUCTION,0.031833150384193196,"analyzed the predictive risk of ridge(less) regression and obtained exact asymptotic expressions under
27"
INTRODUCTION,0.03293084522502744,"the assumption that p/n converges to some constant as both p and n go to infinity. Overall, they
28"
INTRODUCTION,0.03402854006586169,"found the double descent behavior of the ridgeless least squares estimator in terms of the prediction
29"
INTRODUCTION,0.03512623490669594,"risk. Bartlett et al. [2], Kobak et al. [19], Tsigler and Bartlett [27] characterized the phenomenon of
30"
INTRODUCTION,0.036223929747530186,"benign overfitting in a different setting.
31"
INTRODUCTION,0.03732162458836443,"To the best of our knowledge, a vast majority of the theoretical analyses have been confined to a
32"
INTRODUCTION,0.038419319429198684,"simple data generating process, namely, the observations are independent and identically distributed
33"
INTRODUCTION,0.03951701427003293,"(i.i.d.), and the regression errors have mean zero, have the common variance, and are independent of
34"
INTRODUCTION,0.04061470911086718,"the feature vectors. That is,
35"
INTRODUCTION,0.04171240395170143,"(yi, x⊤
i )⊤∼i.i.d. with E[εi] = 0, E[ε2
i ] = σ2 < ∞and εi is independent of xi.
(1)"
INTRODUCTION,0.042810098792535674,"This assumption, although convenient, is likely to be unrealistic in various real-world examples. For
36"
INTRODUCTION,0.043907793633369926,"instance, Liao et al. [21] adopted high-dimensional linear models to examine the double descent
37"
INTRODUCTION,0.04500548847420417,"phenomenon in economic forecasts. In their applications, the outcome variables include S&P firms’
38"
INTRODUCTION,0.04610318331503842,"earnings, U.S. equity premium, U.S. unemployment rate, and countries’ GDP growth rate. As in
39"
INTRODUCTION,0.04720087815587267,"their applications, economic forecasts are associated with time series or panel data. As a result, it
40"
INTRODUCTION,0.048298572996706916,"is improbable that (1) holds in these applications. As another example, Spiess et al. [26] examined
41"
INTRODUCTION,0.04939626783754116,"the performance of high-dimensional synthetic control estimators with many control units. The
42"
INTRODUCTION,0.050493962678375415,"outcome variable in their application is the state-level smoking rates in the Abadie et al. [1] dataset.
43"
INTRODUCTION,0.05159165751920966,"Considering the geographical aspects of the U.S. states, it is unlikely that the regression errors
44"
INTRODUCTION,0.052689352360043906,"underlying the synthetic control estimators adhere to (1). In short, it is desirable to go beyond the
45"
INTRODUCTION,0.05378704720087816,"simple but unrealistic regression error assumption given in (1).
46"
INTRODUCTION,0.054884742041712405,"0
500
1000
1500
2000
p 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 error"
INTRODUCTION,0.05598243688254665,train error (c = 0)
INTRODUCTION,0.0570801317233809,train error (c = 1/4)
INTRODUCTION,0.05817782656421515,train error (c = 2/4)
INTRODUCTION,0.059275521405049394,train error (c = 3/4)
INTRODUCTION,0.06037321624588365,test error (c = 0)
INTRODUCTION,0.06147091108671789,test error (c = 1/4)
INTRODUCTION,0.06256860592755215,test error (c = 2/4)
INTRODUCTION,0.06366630076838639,test error (c = 3/4)
INTRODUCTION,0.06476399560922064,"Figure 1: Comparison of in-sample and out-of-sample mean squared error (MSE) across various
degrees of clustered noise. The vertical line indicates p = n (= 1, 415)."
INTRODUCTION,0.06586169045005488,"To further motivate, we start with our own real-data example from American Community Survey
47"
INTRODUCTION,0.06695938529088913,"(ACS) 2018, extracted from IPUMS USA [24]. The ACS is an ongoing annual survey by the
48"
INTRODUCTION,0.06805708013172337,"US Census Bureau that provides key information about the US population. To have a relatively
49"
INTRODUCTION,0.06915477497255763,"homogeneous population, the sample extract is restricted to white males residing in California with at
50"
INTRODUCTION,0.07025246981339188,"least a bachelor’s degree. We consider a demographic group defined by their age, the type of degree,
51"
INTRODUCTION,0.07135016465422613,"and the field of degree. Then, we compute the average of log hourly wages for each age-degree-
52"
INTRODUCTION,0.07244785949506037,"field group, treat each group average as the outcome variable, and predict group wages by various
53"
INTRODUCTION,0.07354555433589462,"group-level regression models where the regressors are constructed using the indicator variables of
54"
INTRODUCTION,0.07464324917672886,"age, degree, and field as well as their interactions. We consider 7 specifications ranging from 209
55"
INTRODUCTION,0.07574094401756312,"to 2,182 regressors. To understand the role of non-i.i.d. regressor errors, we add the artificial noise
56"
INTRODUCTION,0.07683863885839737,"to the training sample. See Appendix A for details regarding how to generate the artificial noise.
57"
INTRODUCTION,0.07793633369923161,"In the experiment, the constant c varies such that c = 0 corresponds to no clustered dependence
58"
INTRODUCTION,0.07903402854006586,"across observations but as a positive c gets larger, the noise has a larger share of clustered errors but
59"
INTRODUCTION,0.0801317233809001,"the variance of the overall regression errors remains the same regardless of the value of c. Figure 1
60"
INTRODUCTION,0.08122941822173436,"shows the in-sample (train) vs. out-of-sample (test) mean squared error (MSE) for various values
61"
INTRODUCTION,0.08232711306256861,"of c ∈{0, 0.25, 0.5, 0.75}. It can be seen that the experimental results are almost identical across
62"
INTRODUCTION,0.08342480790340286,"different values of c especially when p > n, suggesting that the double descent phenomenon might
63"
INTRODUCTION,0.0845225027442371,"be universal for various degrees of clustered dependence, provided that the overall variance of the
64"
INTRODUCTION,0.08562019758507135,"regression errors remains the same. It is our main goal to provide a firm foundation for this empirical
65"
INTRODUCTION,0.0867178924259056,"phenomenon. To do so, we articulate the following research questions:
66"
INTRODUCTION,0.08781558726673985,"• How to analyze the out-of-sample prediction risk of the ridgeless least squares estimator
67"
INTRODUCTION,0.0889132821075741,"under general assumptions on the regression errors?
68"
INTRODUCTION,0.09001097694840834,"• Why does not the prediction risk seem to be affected by the degrees of dependence across
69"
INTRODUCTION,0.09110867178924259,"observations?
70"
INTRODUCTION,0.09220636663007684,"To delve into the prediction risk, suppose that Σ := E[x0x⊤
0 ] is finite and positive definite. Then,
71"
INTRODUCTION,0.09330406147091108,"E
h
(x⊤
0 ˆβ −x⊤
0 β)2 | x1, . . . , xn
i
= E
h
(ˆβ −β)⊤Σ(ˆβ −β) | x1, . . . , xn
i
."
INTRODUCTION,0.09440175631174534,"If Σ = I (i.e., the case of isotropic features), where I is the identity matrix, the mean squared error
72"
INTRODUCTION,0.09549945115257959,"of the estimator defined by E[∥ˆβ −β∥2], where ∥· ∥is the usual Euclidean norm, is the same as
73"
INTRODUCTION,0.09659714599341383,"the expectation of the prediction risk defined above. However, if Σ , I, the link between the two
74"
INTRODUCTION,0.09769484083424808,"quantities is less intimate. One may regard the prediction risk as the Σ-weighted mean squared error
75"
INTRODUCTION,0.09879253567508232,"of the estimator; whereas E[∥ˆβ −β∥2] can be viewed as an “unweighted” version, even if Σ , I. In
76"
INTRODUCTION,0.09989023051591657,"other words, regardless of the variance-covariance structure of the feature vector, E[∥ˆβ −β∥2] treats
77"
INTRODUCTION,0.10098792535675083,"each component of β “equally.” The mean squared error of the estimator is arguably one of the most
78"
INTRODUCTION,0.10208562019758508,"standard criteria to evaluate the quality of the estimator in statistics. For instance, in the celebrated
79"
INTRODUCTION,0.10318331503841932,"work by James and Stein [18], the mean squared error criterion is used to show that the sample mean
80"
INTRODUCTION,0.10428100987925357,"vector is not necessarily optimal even for standard normal vectors (so-called “Stein’s paradox”).
81"
INTRODUCTION,0.10537870472008781,"Many follow-up papers used the same criterion; e.g., Hansen [16] compared the mean-squared error
82"
INTRODUCTION,0.10647639956092206,"of ordinary least squares, James–Stein, and Lasso estimators in an underparameterized regime. Both
83"
INTRODUCTION,0.10757409440175632,"Σ-weighted and unweighted versions of the mean squared error are interesting objects to study. For
84"
INTRODUCTION,0.10867178924259056,"example, Dobriban and Wager [13] called the former “predictive risk” and the latter “estimation risk”
85"
INTRODUCTION,0.10976948408342481,"in high-dimensional linear models; Berthier et al. [6] called the former “generalization error” and the
86"
INTRODUCTION,0.11086717892425905,"latter “reconstruction error” in the context of stochastic gradient descent for the least squares problem
87"
INTRODUCTION,0.1119648737650933,"using the noiseless linear model. In this paper, we analyze both weighted and unweighted mean
88"
INTRODUCTION,0.11306256860592755,"squared errors of the ridgeless estimator under general assumptions on the data-generating processes,
89"
INTRODUCTION,0.1141602634467618,"not to mention anisotropic features. Furthermore, our focus is on the finite-sample analysis, that is,
90"
INTRODUCTION,0.11525795828759605,"both p and n are fixed but p > n.
91"
INTRODUCTION,0.1163556531284303,"Although most of the existing papers consider the simple setting as in (1), our work is not the first paper
92"
INTRODUCTION,0.11745334796926454,"to consider more general regression errors in the overparameterized regime. Chinot et al. [9], Chinot
93"
INTRODUCTION,0.11855104281009879,"and Lerasle [8] analyzed minimum norm interpolation estimators as well as regularized empirical
94"
INTRODUCTION,0.11964873765093303,"risk minimizers in linear models without any conditions on the regression errors. Specifically,
95"
INTRODUCTION,0.1207464324917673,"Chinot and Lerasle [8] showed that, with high probability, without assumption on the regression
96"
INTRODUCTION,0.12184412733260154,"errors, for the minimum norm interpolation estimator, (ˆβ −β)⊤Σ(ˆβ −β) is bounded from above
97"
INTRODUCTION,0.12294182217343579,"by

∥β∥2 P
i≥c·n λi(Σ) ∨Pn
i=1 ε2
i

/n, where c is an absolute constant and λi(Σ) is the eigenvalues of
98"
INTRODUCTION,0.12403951701427003,"Σ in descending order. Chinot and Lerasle [8] also obtained the bounds on the estimation error
99"
INTRODUCTION,0.1251372118551043,"(ˆβ −β)⊤(ˆβ −β). Our work is distinct and complements these papers in the sense that we allow for
100"
INTRODUCTION,0.12623490669593854,"a general variance-covariance matrix of the regression errors. The main motivation of not making
101"
INTRODUCTION,0.12733260153677278,"any assumptions on εi in Chinot et al. [9] and Chinot and Lerasle [8] is to allow for potentially
102"
INTRODUCTION,0.12843029637760703,"adversarial errors. We aim to allow for a general variance-covariance matrix of the regression errors
103"
INTRODUCTION,0.12952799121844127,"to accommodate time series and clustered data, which are common in applications. See, e.g., Hansen
104"
INTRODUCTION,0.13062568605927552,"[15] for a textbook treatment (see Chapter 14 for time series and Section 4.21 for clustered data).
105"
INTRODUCTION,0.13172338090010977,"The main contribution of this paper is that we provide exact finite-sample characterization of the vari-
106"
INTRODUCTION,0.132821075740944,"ance component of the prediction and estimation risks under the assumption that X = [x1, x2, · · · , xn]⊤
107"
INTRODUCTION,0.13391877058177826,"is left-spherical (e.g., xi’s can be i.i.d. normal with mean zero but more general); εi’s can be corre-
108"
INTRODUCTION,0.1350164654226125,"lated and have non-identical variances; and εi’s are independent of xi’s. Specifically, the variance
109"
INTRODUCTION,0.13611416026344675,"term can be factorized into a product between two terms: one term depends only on the trace of the
110"
INTRODUCTION,0.13721185510428102,"variance-covariance matrix, say Ω, of εi’s; the other term is solely determined by the distribution of
111"
INTRODUCTION,0.13830954994511527,"xi’s. Interestingly, we find that although Ωmay contain non-zero off-diagonal elements, only the trace
112"
INTRODUCTION,0.1394072447859495,"of Ωmatters, as hinted by Figure 1, and further demonstrate our finding via numerical experiments. In
113"
INTRODUCTION,0.14050493962678376,"addition, we obtain exact finite-sample expression for the bias terms when the regression coefficients
114"
INTRODUCTION,0.141602634467618,"follow the random-effects hypothesis [13]. Our finite-sample findings offer a distinct viewpoint on
115"
INTRODUCTION,0.14270032930845225,"the prediction and estimation risks, contrasting with the asymptotic inverse relationship (for optimally
116"
INTRODUCTION,0.1437980241492865,"chosen ridge estimators) between the predictive and estimation risks uncovered by Dobriban and
117"
INTRODUCTION,0.14489571899012074,"Wager [13]. Finally, we connect our findings to the existing results on the prediction risk [e.g., 17] by
118"
INTRODUCTION,0.145993413830955,"considering the asymptotic behavior of estimation risk.
119"
INTRODUCTION,0.14709110867178923,"One of the limitations of our theoretical analysis is that the design matrix X is assumed to be left-
120"
INTRODUCTION,0.14818880351262348,"spherical, although it is more general than i.i.d. normal with mean zero. We not only view this as a
121"
INTRODUCTION,0.14928649835345773,"convenient assumption but also expect that our findings will hold at least approximately even if X
122"
INTRODUCTION,0.150384193194292,"does not follow the left-spherical distribution. It is a topic for future research to formally investigate
123"
INTRODUCTION,0.15148188803512624,"this conjecture.
124"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1525795828759605,"2
The Framework under General Assumptions on Regression Errors
125"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.15367727771679474,"We first describe the minimum ℓ2 norm (ridgeless) interpolation least squares estimator in the
126"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.15477497255762898,"overparameterized case (p > n). Define
127"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.15587266739846323,"y := [y1, y2, · · · , yn]⊤∈Rn,"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.15697036223929747,"ε := [ε1, ε2, · · · , εn]⊤∈Rn,"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.15806805708013172,"X⊤:=
h
x1, x2, · · · , xn
i
∈Rp×n,"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.15916575192096596,"so that y = Xβ + ε. The estimator we consider is
128"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1602634467618002,"ˆβ := arg min
b∈Rp {∥b∥: Xb = y} = (X⊤X)†X⊤y = X†y,"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.16136114160263446,"where A† denotes the Moore–Penrose inverse of a matrix A.
129"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.16245883644346873,"The main object of interest in this paper is the prediction and estimation risks of ˆβ under the
130"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.16355653128430298,"data scenario such that the regression error εi may not be i.i.d. Formally, we make the following
131"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.16465422612513722,"assumptions.
132"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.16575192096597147,"Assumption 2.1. (i) y = Xβ + ε, where ε is independent of X, and E[ε] = 0. (ii) Ω:= E[εε⊤] is finite
133"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1668496158068057,"and positive definite (but not necessarily spherical).
134"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.16794731064763996,"We emphasize that Assumption 2.1 is more general than the standard assumption in the literature
135"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1690450054884742,"on benign overfitting that typically assumes that Ω≡σ2I. Assumption 2.1 allows for non-identical
136"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.17014270032930845,"variances across the elements of ε because the diagonal elements of Ωcan be different among each
137"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1712403951701427,"other. Furthermore, it allows for non-zero off-diagonal elements in Ω. It is difficult to assume that the
138"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.17233809001097694,"regression errors are independent among each other with time series or clustered data; thus, in these
139"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1734357848518112,"settings, it is important to allow for general Ω, σ2I. Below we present a couple of such examples.
140"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.17453347969264543,"Example 2.1 (AR(1) Errors). Suppose that the regressor error follows an autoregressive process:
141"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1756311745334797,"εi = ρεi−1 + ηi,
(2)"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.17672886937431395,"where ρ ∈(−1, 1) is an autoregressive parameter, ηi is independent and identically distributed with
142"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1778265642151482,"mean zero and variance σ2(0 < σ2 < ∞) and is independent of X. Then, the (i, j) element of Ωis
143"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.17892425905598244,"Ωi j =
σ2"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1800219538968167,1 −ρ2 ρ|i−j|.
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.18111964873765093,"Note that Ωij , 0 as long as ρ , 0.
144"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.18221734357848518,"Example 2.2 (Clustered Errors). Suppose that regression errors are mutually independent across
145"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.18331503841931943,"clusters but they can be arbitrarily correlated within the same cluster. For instance, students in
146"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.18441273326015367,"the same school may affect each other and also have the same teachers; thus it would be difficult
147"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.18551042810098792,"to assume independence across student test scores within the same school. However, it might be
148"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.18660812294182216,"reasonable that student test scores are independent across different schools. For example, assume that
149"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1877058177826564,"(i) if the regression error εi belongs to cluster g, where g = 1, . . . ,G and G is the number of clusters,
150"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.18880351262349068,"E[ε2
i ] = σ2
g for some constant σ2
g > 0 that can vary over g; (ii) if the regression errors εi and εj (i , j)
151"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.18990120746432493,"belong to the same cluster g, E[εiεj] = ρg for some constant ρg , 0 that can be different across g;
152"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.19099890230515917,"and (iii) if the regression errors εi and εj (i , j) do not belong to the same cluster, E[εiεj] = 0. Then,
153"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.19209659714599342,"Ωis block diagonal with possibly non-identical blocks.
154"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.19319429198682767,"For vector a and square matrix A, let ∥a∥2
A := a⊤Aa. Conditional on X and given A, we define
155"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1942919868276619,"BiasA(ˆβ | X) := ∥E[ˆβ | X] −β∥A
and
VarA(ˆβ | X) := Tr(Cov(ˆβ | X)A),"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.19538968166849616,"and we write Var = VarI and Bias = BiasI for the sake of brevity in notation.
156"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1964873765093304,"The mean squared prediction error for an unseen test observation x0 with the positive definite
157"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.19758507135016465,"covariance matrix Σ := E[x0x⊤
0 ] (assuming that x0 is independent of the training data X) and the mean
158"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.1986827661909989,"squared estimation error of ˆβ conditional on X can be written as:
159"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.19978046103183314,"RP(ˆβ | X) := E(x⊤
0 ˆβ −x⊤
0 β)2 | X = [BiasΣ(ˆβ | X)]2 + VarΣ(ˆβ | X),"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.20087815587266739,RE(ˆβ | X) := E∥ˆβ −β∥2 | X = [Bias(ˆβ | X)]2 + Var(ˆβ | X).
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.20197585071350166,"In what follows, we obtain exact finite-sample expressions for prediction and estimation risks:
160"
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.2030735455543359,"RP(ˆβ) := EX[RP(ˆβ | X)]
and
RE(ˆβ) := EX[RE(ˆβ | X)]."
THE FRAMEWORK UNDER GENERAL ASSUMPTIONS ON REGRESSION ERRORS,0.20417124039517015,"We first analyze the variance terms for both risks and then study the bias terms.
161"
THE VARIANCE COMPONENTS OF PREDICTION AND ESTIMATION RISKS,0.2052689352360044,"3
The Variance Components of Prediction and Estimation Risks
162"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.20636663007683864,"3.1
The variance component of prediction risk
163"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2074643249176729,"We rewrite the variance component of prediction risk as follows:
164"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.20856201975850713,"VarΣ(ˆβ | X) = Tr(Cov(ˆβ | X)Σ) = Tr(X†ΩX†⊤Σ) = ∥S X†T∥2
F,
(3)"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.20965971459934138,"where positive definite symmetric matrices S := Σ1/2 and T := Ω1/2 are the square root matrices of
165"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.21075740944017562,"the positive definite matrices Σ and Ω, respectively. To compute the above Frobenius norm of the
166"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.21185510428100987,"matrix S X†T, we need to compute the alignment of the right-singular vectors of B := S X† ∈Rp×n
167"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.21295279912184412,"with the left-eigenvectors of T ∈Rn×n. Here, B is a random matrix while T is fixed. Therefore, we
168"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.21405049396267836,"need the distribution of the right-singular vectors of the random matrix B.
169"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.21514818880351264,"Perhaps surprisingly, to compute the expected variance EX[VarΣ(ˆβ | X)], it turns out that we do not
170"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.21624588364434688,"need the distribution of the singular vectors if we make a minimal assumption (the left-spherical
171"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.21734357848518113,"symmetry of X) which is weaker than the assumption that {xi}n
i=1 is i.i.d. normal with E[x1] = 0.
172"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.21844127332601537,"Definition 3.1 (Left-Spherical Symmetry [10–12, 14]). A random matrix Z or its distribution is
173"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.21953896816684962,"called to be left-spherical if OZ and Z have the same distribution (OZ
d= Z) for any fixed orthogonal
174"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.22063666300768386,"matrix O ∈O(n) := {A ∈Rn×n : AA⊤= A⊤A = I}.
175"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2217343578485181,"Assumption 3.2. The design matrix X is left-spherical.
176"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.22283205268935236,"For the isotropic error case (Ω= I), we have EX[VarΣ(ˆβ | X)] = EX[Tr((X⊤X)†Σ)] directly from
177"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2239297475301866,"equation 3 since X†X†⊤= (X⊤X)†. Moreover, for the arbitrary error, the left-spherical symmetry of X
178"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.22502744237102085,"plays a critical role to factor out the same EX[Tr((X⊤X)†Σ)] and the trace of the variance-covariance
179"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2261251372118551,"matrix of the regression errors, Tr(Ω), from the variance after the expectation over X.
180"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.22722283205268934,"Lemma 3.3. For a subset S ⊂Rm×m satisfying C−1 ∈S for all C ∈S, if matrix-valued random
variables Z and AZ have the same distribution measure µZ for any A ∈S, then we have"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2283205268935236,EZ[ f(Z)] = EZ[f(AZ)] = EZ[EA′∼ν[ f(A′Z)]]
THE VARIANCE COMPONENT OF PREDICTION RISK,0.22941822173435786,"for any function f ∈L1(µZ) and any probability density function ν on S.
181"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2305159165751921,"Theorem 3.4. Let Assumptions 2.1, and 3.2 hold. Then, we have
182"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.23161361141602635,EX[VarΣ(ˆβ | X)] = 1
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2327113062568606,n Tr(Ω)EX[Tr((X⊤X)†Σ)].
THE VARIANCE COMPONENT OF PREDICTION RISK,0.23380900109769484,"Sketch of Proof. With B = Σ1/2X† and T = Ω1/2, we can rewrite the variance as follows:
183"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2349066959385291,"VarΣ(ˆβ | X) = ∥BT∥2
F = ∥UDV⊤UT DTV⊤
T ∥2
F= ∥DV⊤UT DT∥2
F"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.23600439077936333,"from the singular value decompositions B = UDV⊤and T = UT DTV⊤
T with orthogonal matrices
184"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.23710208562019758,"U, V, UT, VT, and diagonal matrices D, DT. Then, we need to compute the alignment V⊤UT of the
185"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.23819978046103182,"right-singular vectors of B with the left-eigenvectors of T because
186"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.23929747530186607,"∥DV⊤UT DT∥2
F = λ

(X⊤X)†Σ
⊤Γ(X)λ(Ω) = a(X)⊤Γ(X)b,"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.24039517014270034,"0.00
0.25
0.50
0.75
1.00
2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 2"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2414928649835346,Prediction Risk - AR(1) Errors 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4
THE VARIANCE COMPONENT OF PREDICTION RISK,0.24259055982436883,"2.7
3.0"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.24368825466520308,"0.00
0.25
0.50
0.75
1.00
2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 2"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.24478594950603733,Estimation Risk - AR(1) Errors 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4
THE VARIANCE COMPONENT OF PREDICTION RISK,0.24588364434687157,"2.7
3.0"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.24698133918770582,"Figure 2: Our theory (dashed lines) matches the expected variances (solid lines) of the prediction (left)
and estimation risks (right) in Example 2.1 (AR(1) Errors). Each point (σ2, ρ2) represents a different
noise covariance matrix Ω, but with the same Tr(Ω) along each line {(σ2, ρ2) : σ2/κ2 + ρ2 = 1} for
some κ2 > 0, they have the same expected variance. We set n = 50, p = 100, and evaluate on 100
samples of X and 100 samples of ε (for each realization of X) to approximate the expectations."
THE VARIANCE COMPONENT OF PREDICTION RISK,0.24807903402854006,"where v(i) := V:i, u(j) := (UT): j, γij := ⟨v(i), u(j)⟩2 ≥0, Γ(X) := (γi j)i,j ∈Rn×n and λ(A) ∈Rn is a
187"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2491767288693743,"vector where its elements are the eigenvalues of A.
188"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2502744237102086,"Now, we want to compute the expected variance. To do so, from Lemma 3.3 with S = O(n) and the
189"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2513721185510428,"left-spherical symmetry of X, we can obtain
190"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2524698133918771,"EX[a(X)⊤Γ(X)b] = EX
h
EO∼ν[a(OX)⊤Γ(OX)b]
i
= EX
h
a(X)⊤EO∼ν[Γ(OX)]b
i
,"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2535675082327113,"where ν is the unique uniform distribution (the Haar measure) over the orthogonal matrices O(n).
191"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.25466520307354557,"Here, we can show that EO∼ν[Γ(OX)] =
1
n J, where J is the all-ones matrix with Ji j = 1(i, j =
192"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2557628979143798,"1, 2, · · · , n). Therefore, we have the expected variance as follows:
193"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.25686059275521406,EX[VarΣ(ˆβ | X)] = EX
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2579582875960483,"""
a(X)⊤1"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.25905598243688255,"n Jb
#
= 1 n n
X"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2601536772777168,"i,j=1
EX[ai(X)]b j = 1"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.26125137211855104,"nEX[Tr((X⊤X)†Σ)] Tr(Ω). □
194"
THE VARIANCE COMPONENT OF PREDICTION RISK,0.2623490669593853,"The proofs of Lemma 3.3 and Theorem 3.4 are in the supplementary appendix.
195"
THE VARIANCE COMPONENT OF ESTIMATION RISK,0.26344676180021953,"3.2
The variance component of estimation risk
196"
THE VARIANCE COMPONENT OF ESTIMATION RISK,0.2645444566410538,"For the expected variance EX[Var(ˆβ | X)] of the estimation risk, a similar argument still holds if
197"
THE VARIANCE COMPONENT OF ESTIMATION RISK,0.265642151481888,"plugging-in B = X† instead of B = Σ1/2X†.
198"
THE VARIANCE COMPONENT OF ESTIMATION RISK,0.2667398463227223,"Theorem 3.5. Let Assumptions 2.1, and 3.2 hold. Then, we have
199"
THE VARIANCE COMPONENT OF ESTIMATION RISK,0.2678375411635565,EX[Var(ˆβ | X)] = 1
THE VARIANCE COMPONENT OF ESTIMATION RISK,0.2689352360043908,"np Tr(Ω)EX[Tr(Λ†)],"
THE VARIANCE COMPONENT OF ESTIMATION RISK,0.270032930845225,"where XX⊤/p = UΛU⊤for some orthogonal matrix U ∈O(n).
200"
NUMERICAL EXPERIMENTS,0.2711306256860593,"3.3
Numerical experiments
201"
NUMERICAL EXPERIMENTS,0.2722283205268935,"In this section, we validate our theory with some numerical experiments of Examples 2.1 and
202"
NUMERICAL EXPERIMENTS,0.27332601536772777,"2.2, especially how the expected variance is related to the general covariance Ωof the regressor
203"
NUMERICAL EXPERIMENTS,0.27442371020856204,"error ε. In the both examples, we sample {xi}n
i=1 from N(0, Σ) with a general feature covariance
204"
NUMERICAL EXPERIMENTS,0.27552140504939626,"Σ = UΣDΣU⊤
Σ for an orthogonal matrix UΣ ∈O(p) and a diagonal matrix DΣ ≻0. In this setting, we
205"
NUMERICAL EXPERIMENTS,0.27661909989023054,"have rank(XX⊤) = n and Λ† = Λ−1 almost everywhere.
206"
NUMERICAL EXPERIMENTS,0.27771679473106475,"0.1
0.2
0.3
0.4
0.5 2
1 0.1 0.2 0.3 0.4 0.5 2
2"
NUMERICAL EXPERIMENTS,0.278814489571899,Prediction Risk - Clustered Errors 0 15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65
NUMERICAL EXPERIMENTS,0.27991218441273324,"0.1
0.2
0.3
0.4
0.5 2
1 0.1 0.2 0.3 0.4 0.5 2
2"
NUMERICAL EXPERIMENTS,0.2810098792535675,Estimation Risk - Clustered Errors 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60
NUMERICAL EXPERIMENTS,0.28210757409440174,"Figure 3: Our theory (dashed lines) matches the expected variances (solid lines) of the prediction
(left) and estimation risks (right) in Example 2.2 (Clustered Errors). Each point (σ2, ρ2) represents a
different noise covariance matrix Ω, but with the same Tr(Ω) along each line {(σ2
1, σ2
2) : n1"
NUMERICAL EXPERIMENTS,0.283205268935236,"n σ2
1+ n2"
NUMERICAL EXPERIMENTS,0.2843029637760702,"n σ2
2 =
κ2} for some κ2 > 0, they have the same expected variance. We set G = 2, (n1 = 5, n2 = 15), n =
20, p = 40, ρ1 = ρ2 = 0.05, and evaluate on 100 samples of X and 100 samples of ε (for each
realization of X) to approximate the expectations."
NUMERICAL EXPERIMENTS,0.2854006586169045,"AR(1) Errors
As shown in Example 2.1, when the regressor error follows an autoregressive process
207"
NUMERICAL EXPERIMENTS,0.2864983534577388,"in equation 2, we have Ωij = σ2ρ|i−j|/(1 −ρ2) and Tr(Ω)/n = σ2/(1 −ρ2). Therefore, for pairs of
208"
NUMERICAL EXPERIMENTS,0.287596048298573,"(σ2, ρ2) with the same Tr(Ω)/n, they are expected to yield the same variances of the prediction and
209"
NUMERICAL EXPERIMENTS,0.28869374313940727,"estimation risk from Theorem 3.4 and 3.5 even though they have different off-diagonal elements in Ω.
210"
NUMERICAL EXPERIMENTS,0.2897914379802415,"To be specific, the pairs (σ2, ρ2) on a line {(σ2, ρ2) : σ2/κ2 + ρ2 = 1} have the same Tr(Ω)/n and the
211"
NUMERICAL EXPERIMENTS,0.29088913282107576,"same expected variance which gets larger for the line with respect to a larger κ2.
212"
NUMERICAL EXPERIMENTS,0.29198682766191,Figure 2 (left) shows the contour plots of EX[VarΣ(ˆβ | X)] and 1
NUMERICAL EXPERIMENTS,0.29308452250274425,"n Tr(Ω)EX[Tr((X⊤X)†Σ)] for different
213"
NUMERICAL EXPERIMENTS,0.29418221734357847,"pairs of (σ2, ρ2) in Example 2.1. They have different slopes −κ−2 according to the value of κ2 =
214"
NUMERICAL EXPERIMENTS,0.29527991218441274,"Tr(Ω)/n. The right panel shows equivalent contour plots for estimation risk.
215"
NUMERICAL EXPERIMENTS,0.29637760702524696,"Clustered Errors
Now consider the block diagonal covariance matrix Ω= diag(Ω1, Ω2, · · · , ΩG)
216"
NUMERICAL EXPERIMENTS,0.29747530186608123,"in Example 2.2, where Ωg is an ng × ng matrix with (Ωg)ii = σ2
g and (Ωg)i j = ρg (i , j) for each
217"
NUMERICAL EXPERIMENTS,0.29857299670691545,"i, j = 1, 2, · · · , ng and g = 1, 2, · · · ,G. Let n = PG
g=1 ng. We then have Tr(Ω)/n = PG
g=1 Tr(Ωg)/n =
218
PG
g=1(ng/n)σ2
g. Therefore, given a partition {ng}G
g=1 of the n observations, the covariance matrices Ω
219"
NUMERICAL EXPERIMENTS,0.2996706915477497,"with different {σ2
g}G
g=1 have the same Tr(Ω)/n if (σ2
1, σ2
2, · · · , σ2
G) ∈RG are on the same hyperplane
220 n1"
NUMERICAL EXPERIMENTS,0.300768386388584,"n σ2
1 + n2"
NUMERICAL EXPERIMENTS,0.3018660812294182,"n σ2
2 + · · · + nG"
NUMERICAL EXPERIMENTS,0.3029637760702525,"n σ2
G = κ2 for some κ2 > 0.
221"
NUMERICAL EXPERIMENTS,0.3040614709110867,Figure 3 (left) shows the contour plots of EX[VarΣ(ˆβ | X)] and 1
NUMERICAL EXPERIMENTS,0.305159165751921,"n Tr(Ω)EX[Tr((X⊤X)†Σ)] for different
222"
NUMERICAL EXPERIMENTS,0.3062568605927552,"pairs of (σ2
1, σ2
2) for a simple two-clusters example (G = 2) of Example 2.2 with (n1, n2) = (5, 15).
223"
NUMERICAL EXPERIMENTS,0.30735455543358947,"Here, we use a fixed value of ρ1 = ρ2 = 0.05, but the results are the same regardless of their values,
224"
NUMERICAL EXPERIMENTS,0.3084522502744237,"as shown in the appendix. Unlike Example 2.1, the hyperplanes are orthogonal to v = [n1, n2]⊤
225"
NUMERICAL EXPERIMENTS,0.30954994511525796,"regardless of the value of κ2 = Tr(Ω)/n. Again, the right panel shows equivalent contour plots for
226"
NUMERICAL EXPERIMENTS,0.3106476399560922,"estimation risk.
227"
THE BIAS COMPONENTS OF PREDICTION AND ESTIMATION RISKS,0.31174533479692645,"4
The Bias Components of Prediction and Estimation Risks
228"
THE BIAS COMPONENTS OF PREDICTION AND ESTIMATION RISKS,0.31284302963776073,"Our main contribution is to allow for general assumptions on the regression errors, and thus the bias
229"
THE BIAS COMPONENTS OF PREDICTION AND ESTIMATION RISKS,0.31394072447859495,"parts remain the same as they do not change with respect to the regression errors. For completeness,
230"
THE BIAS COMPONENTS OF PREDICTION AND ESTIMATION RISKS,0.3150384193194292,"in this section, we briefly summarize the results on the bias components. First, we make the following
231"
THE BIAS COMPONENTS OF PREDICTION AND ESTIMATION RISKS,0.31613611416026344,"assumption for a constant rank deficiency of X⊤X which holds, for example, each xi has a positive
232"
THE BIAS COMPONENTS OF PREDICTION AND ESTIMATION RISKS,0.3172338090010977,"definite covariance matrix and is independent of each other.
233"
THE BIAS COMPONENTS OF PREDICTION AND ESTIMATION RISKS,0.31833150384193193,"Assumption 4.1. rank(X) = n almost everywhere.
234"
THE BIAS COMPONENT OF PREDICTION RISK,0.3194291986827662,"4.1
The bias component of prediction risk
235"
THE BIAS COMPONENT OF PREDICTION RISK,0.3205268935236004,"The bias term of prediction risk can be expressed as follows:
236"
THE BIAS COMPONENT OF PREDICTION RISK,0.3216245883644347,"[BiasΣ(ˆβ | X)]2 = (S β)⊤lim
λ↘0 λ2(S −1 ˆΣS + λI)−2S β,
(4)"
THE BIAS COMPONENT OF PREDICTION RISK,0.3227222832052689,"where ˆΣ := X⊤X/n. Now, in order to obtain an exact closed form solution, we make the following
237"
THE BIAS COMPONENT OF PREDICTION RISK,0.3238199780461032,"assumption:
238"
THE BIAS COMPONENT OF PREDICTION RISK,0.32491767288693746,"Assumption 4.2. Eβ[S β(S β)⊤] = r2
ΣI/p, where r2
Σ := Eβ[∥β∥2
Σ] < ∞and β is independent of X.
239"
THE BIAS COMPONENT OF PREDICTION RISK,0.3260153677277717,"A similar assumption (see Assumption 4.4) has been shown to be useful to obtain closed-form
240"
THE BIAS COMPONENT OF PREDICTION RISK,0.32711306256860595,"expressions in the literature [e.g., 13, 23, 20, 7].
241"
THE BIAS COMPONENT OF PREDICTION RISK,0.32821075740944017,"Under this assumption, since [BiasΣ(ˆβ | X)]2 = Tr[S β(S β)⊤limλ↘0 λ2(S −1 ˆΣS + λI)−2] from equa-
242"
THE BIAS COMPONENT OF PREDICTION RISK,0.32930845225027444,"tion 4, we have the expected bias (conditional on X) as follows:
243"
THE BIAS COMPONENT OF PREDICTION RISK,0.33040614709110866,"Eβ[BiasΣ(ˆβ | X)2 | X] = r2
Σ
p lim
λ↘0 p
X i=1 λ2"
THE BIAS COMPONENT OF PREDICTION RISK,0.33150384193194293,"(˜si + λ)2 = r2
Σ
p |{i ∈[p] : ˜si = 0}| = r2
Σ
p −n p
,"
THE BIAS COMPONENT OF PREDICTION RISK,0.33260153677277715,"where ˜si are the eigenvalues of S −1 ˆΣS ∈Rp×p and rank(S −1 ˆΣS ) = rank(X) = n almost everywhere
244"
THE BIAS COMPONENT OF PREDICTION RISK,0.3336992316136114,"under Assumption 4.1. This bias is independent of the distribution of X or the spectral density of
245"
THE BIAS COMPONENT OF PREDICTION RISK,0.33479692645444564,"S −1 ˆΣS , but only depending on the rank deficiency of the realization of X.
246"
THE BIAS COMPONENT OF PREDICTION RISK,0.3358946212952799,"Finally, the prediction risk RP(ˆβ) can be summarized as follows:
247"
THE BIAS COMPONENT OF PREDICTION RISK,0.33699231613611413,"Corollary 4.3. Let Assumptions 2.1, 3.2, 4.1, and 4.2 hold. Then, we have
248"
THE BIAS COMPONENT OF PREDICTION RISK,0.3380900109769484,"RP(ˆβ) = r2
Σ 1 −n p"
THE BIAS COMPONENT OF PREDICTION RISK,0.3391877058177827,"!
+ Tr(Ω)"
THE BIAS COMPONENT OF PREDICTION RISK,0.3402854006586169,"n
EX
h
Tr((X⊤X)†Σ)
i
."
THE BIAS COMPONENT OF ESTIMATION RISK,0.3413830954994512,"4.2
The bias component of estimation risk
249"
THE BIAS COMPONENT OF ESTIMATION RISK,0.3424807903402854,"For the bias component of estimation risk, we can obtain a similar result with 4.1 as follows:
250"
THE BIAS COMPONENT OF ESTIMATION RISK,0.34357848518111966,"[Bias(ˆβ | X)]2 = β⊤(I −ˆΣ† ˆΣ)β = lim
λ↘0 β⊤λ(ˆΣ + λI)−1β."
THE BIAS COMPONENT OF ESTIMATION RISK,0.3446761800219539,"Assumption 4.4. Eβ[ββ⊤] = r2I/p, where r2 := Eβ[∥β∥2] < ∞and β is independent of X.
251"
THE BIAS COMPONENT OF ESTIMATION RISK,0.34577387486278816,"Under Assumption 4.4, we have the expected bias (conditional on X) as follows:
252"
THE BIAS COMPONENT OF ESTIMATION RISK,0.3468715697036224,Eβ[Bias(ˆβ | X)2 | X] = r2
THE BIAS COMPONENT OF ESTIMATION RISK,0.34796926454445665,"p lim
λ↘0 p
X i=1"
THE BIAS COMPONENT OF ESTIMATION RISK,0.34906695938529086,"λ
si + λ = r2"
THE BIAS COMPONENT OF ESTIMATION RISK,0.35016465422612514,p |{i ∈[p] : si = 0}| = r2 p −n
THE BIAS COMPONENT OF ESTIMATION RISK,0.3512623490669594,"p
,
(5)"
THE BIAS COMPONENT OF ESTIMATION RISK,0.35236004390779363,"where si are the eigenvalues of ˆΣ ∈Rp×p and rank(ˆΣ) = rank(X) = n under Assumption 4.1.
253"
THE BIAS COMPONENT OF ESTIMATION RISK,0.3534577387486279,"Thanks to Theorem 3.5 and equation 5, we obtain the following corollary for estimation risk.
254"
THE BIAS COMPONENT OF ESTIMATION RISK,0.3545554335894621,"Corollary 4.5. Let Assumptions 2.1, 3.2, 4.1, and 4.4 hold. Then, we have
255"
THE BIAS COMPONENT OF ESTIMATION RISK,0.3556531284302964,"RE(ˆβ) = r2
 
1 −n p"
THE BIAS COMPONENT OF ESTIMATION RISK,0.3567508232711306,"!
+ Tr(Ω) n
EX ""Z 1"
THE BIAS COMPONENT OF ESTIMATION RISK,0.3578485181119649,"s dFXX⊤/n(s)
#
,"
THE BIAS COMPONENT OF ESTIMATION RISK,0.3589462129527991,"where FA(s) :=
1
n
Pn
i=1 1{λi(A) ≤s} is the empirical spectral distribution of a matrix A and
256"
THE BIAS COMPONENT OF ESTIMATION RISK,0.3600439077936334,"λ1(A), λ2(A), · · · , λn(A) are the eigenvalues of A.
257"
THE BIAS COMPONENT OF ESTIMATION RISK,0.3611416026344676,"The proof of Corollary 4.5 is in the appendix.
258"
ASYMPTOTIC ANALYSIS OF ESTIMATION RISK,0.36223929747530187,"4.2.1
Asymptotic analysis of estimation risk
259"
ASYMPTOTIC ANALYSIS OF ESTIMATION RISK,0.3633369923161361,"To study the asymptotic behavior of estimation risk, we follow the previous approaches [13, 17].
260"
ASYMPTOTIC ANALYSIS OF ESTIMATION RISK,0.36443468715697036,"First, we define the Stieltjes transform as follows:
261"
ASYMPTOTIC ANALYSIS OF ESTIMATION RISK,0.36553238199780463,Definition 4.6. The Stieltjes transform sF(z) of a df F is defined as:
ASYMPTOTIC ANALYSIS OF ESTIMATION RISK,0.36663007683863885,"sF(z) :=
Z
1
x −zdF(x), for z ∈C \ supp(F)."
ASYMPTOTIC ANALYSIS OF ESTIMATION RISK,0.3677277716794731,"100
101
102 = p/n 10
2 10
1 100 101"
PREDICTION RISK,0.36882546652030734,"102
Prediction Risk"
PREDICTION RISK,0.3699231613611416,"variance
variance (theory)
variance (theory, iso.)
bias
bias (theory)"
PREDICTION RISK,0.37102085620197583,"100
101
102 = p/n 10
2 10
1 100 101"
ESTIMATION RISK,0.3721185510428101,"102
Estimation Risk"
ESTIMATION RISK,0.3732162458836443,"variance
variance (theory)
variance (theory, iso.)
bias
bias (theory)"
ESTIMATION RISK,0.3743139407244786,"Figure 4:
The “descent curve” in the overparameterization regime for prediction risk (left) and
estimation risk (right). We test Ω’s with Tr(Ω)/n = 1, 2, 4 in black, blue, red, respectively. For the
anisotropic feature, the expected variance (×) and its theoretical expression () are Θ
 Tr(Ω)/n"
ESTIMATION RISK,0.3754116355653128,"γ−1

and
larger than that in the high-dimensional asymptotics for the isotropic Σ = I. For the isotropic Σ = I,
the variance terms (dotted) and the bias terms (dashed) in the high-dimensional asymptotics are
1
γ−1 limn→∞
Tr(Ω)"
ESTIMATION RISK,0.3765093304061471,"n
and r2 
1 −1"
ESTIMATION RISK,0.37760702524698136,"γ

, respectively."
ESTIMATION RISK,0.3787047200878156,"We are now ready to investigate the asymptotic behavior of the mean squared estimation error with
262"
ESTIMATION RISK,0.37980241492864986,"the following theorem:
263"
ESTIMATION RISK,0.3809001097694841,"Theorem 4.7. [25, Theorem 1.1] Suppose that the rows {xi}n
i=1 in X are i.i.d. centered random vectors
264"
ESTIMATION RISK,0.38199780461031835,"with E[x1x⊤
1 ] = Σ and that the empirical spectral distribution FΣ(s) = 1"
ESTIMATION RISK,0.38309549945115257,"p
Pp
i=1 1{τi ≤s} of Σ converges
265"
ESTIMATION RISK,0.38419319429198684,"almost surely to a probability distribution function H as p →∞. When p/n →γ > 0 as n, p →∞,
266"
ESTIMATION RISK,0.38529088913282106,"then a.s., FXX⊤/n converges vaguely to a df F and the limit s∗:= limz↘0 sF(z) of its Stieltjes transform
267"
ESTIMATION RISK,0.38638858397365533,"sF is the unique solution to the equation:
268 1 −1"
ESTIMATION RISK,0.38748627881448955,"γ =
Z
1
1 + τs∗dH(τ).
(6)"
ESTIMATION RISK,0.3885839736553238,"This theorem is a direct consequence of Theorem 1.1 in Silverstein and Bai [25]. Then, from Corollary
269"
ESTIMATION RISK,0.3896816684961581,"4.5, we can write the limit of estimation risk as follows:
270"
ESTIMATION RISK,0.3907793633369923,"Corollary 4.8. Let Assumptions 2.1, 3.2, 4.1, and 4.4 hold. Then, under the same assumption as
271"
ESTIMATION RISK,0.3918770581778266,"Theorem 4.7, as n, p →∞and p/n →γ, where 1 < γ < ∞is a constant, we have
272"
ESTIMATION RISK,0.3929747530186608,"RE(ˆβ) = E∥ˆβ −β∥2 →r2
 
1 −1 γ"
ESTIMATION RISK,0.3940724478594951,"!
+ s∗lim
n→∞
Tr(Ω) n
."
ESTIMATION RISK,0.3951701427003293,"Here, the limit s∗of the Stieltjes transform sF is highly connected with the shape of the spectral
273"
ESTIMATION RISK,0.39626783754116357,"distribution of Σ. For example, in the case of isotropic features (Σ = I), i.e., dH(τ) = δ(τ −1)dτ, we
274"
ESTIMATION RISK,0.3973655323819978,"have s∗
iso = (γ −1)−1 from 1 −1"
ESTIMATION RISK,0.39846322722283206,"γ =
1
1+s∗
iso . In addition, if Ω= σ2I, then the limit of the mean squared
275"
ESTIMATION RISK,0.3995609220636663,"error is exactly the same as the expression for γ > 1 in equation (10) of Hastie et al. [17, Theorem 1].
276"
ESTIMATION RISK,0.40065861690450055,"This is because prediction risk is the same as estimation risk when Σ = I.
277"
ESTIMATION RISK,0.40175631174533477,"Remark 4.9. Generally, if the support of H is bounded within [cH,CH] ⊂R for some positive constants
278"
ESTIMATION RISK,0.40285400658616904,"0 < cH ≤CH < ∞, then we can observe the double descent phenomenon in the overparameterization
279"
ESTIMATION RISK,0.4039517014270033,"regime with limγ↘1 s∗= ∞and limγ→∞s∗= 0 with s∗= Θ
 1"
ESTIMATION RISK,0.40504939626783754,"γ−1

from the following inequalities:
280"
ESTIMATION RISK,0.4061470911086718,"C−1
H
1
γ −1 ≤s∗≤c−1
H
1
γ −1.
(7)"
ESTIMATION RISK,0.407244785949506,"In fact, a tighter lower bound is available:
281"
ESTIMATION RISK,0.4083424807903403,"s∗≥µ−1
H (γ −1)−1,
(8)
where µH := Eτ∼H[τ], i.e., the mean of distribution H. The proofs of equation 7 and equation 8 are
282"
ESTIMATION RISK,0.4094401756311745,"given in the supplementary appendix.
283"
ESTIMATION RISK,0.4105378704720088,"We conclude this paper by plotting the “descent curve” in the overparameterization regime in Figure
284"
ESTIMATION RISK,0.411635565312843,"4. On one hand, the expected variance (×) perfectly matches its theoretical counterpart () and goes
285"
ESTIMATION RISK,0.4127332601536773,"to zero as γ gets large. On the other hand, the bias term is bounded even if γ →∞. The appendix
286"
ESTIMATION RISK,0.4138309549945115,"contains the experimental details for all the figures.
287"
REFERENCES,0.4149286498353458,"References
288"
REFERENCES,0.41602634467618005,"[1] Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Synthetic control methods for compar-
289"
REFERENCES,0.41712403951701427,"ative case studies: Estimating the effect of california’s tobacco control program. Journal of the
290"
REFERENCES,0.41822173435784854,"American Statistical Association, 105(490):493–505, 2010.
291"
REFERENCES,0.41931942919868276,"[2] Peter L Bartlett, Philip M Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in
292"
REFERENCES,0.42041712403951703,"linear regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070,
293"
REFERENCES,0.42151481888035125,"2020.
294"
REFERENCES,0.4226125137211855,"[3] Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to
295"
REFERENCES,0.42371020856201974,"understand kernel learning. In International Conference on Machine Learning, pages 541–549.
296"
REFERENCES,0.424807903402854,"PMLR, 2018.
297"
REFERENCES,0.42590559824368823,"[4] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
298"
REFERENCES,0.4270032930845225,"learning practice and the classical bias–variance trade-off. Proceedings of the National Academy
299"
REFERENCES,0.4281009879253567,"of Sciences, 116(32):15849–15854, 2019.
300"
REFERENCES,0.429198682766191,"[5] Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
301"
REFERENCES,0.43029637760702527,"Journal on Mathematics of Data Science, 2(4):1167–1180, 2020.
302"
REFERENCES,0.4313940724478595,"[6] Raphaël Berthier, Francis Bach, and Pierre Gaillard. Tight nonparametric convergence rates for
303"
REFERENCES,0.43249176728869376,"stochastic gradient descent under the noiseless linear model. Advances in Neural Information
304"
REFERENCES,0.433589462129528,"Processing Systems, 33:2576–2586, 2020.
305"
REFERENCES,0.43468715697036225,"[7] Xin Chen, Yicheng Zeng, Siyue Yang, and Qiang Sun. Sketched ridgeless linear regression: The
306"
REFERENCES,0.43578485181119647,"role of downsampling. In International Conference on Machine Learning, pages 5296–5326.
307"
REFERENCES,0.43688254665203075,"PMLR, 2023.
308"
REFERENCES,0.43798024149286496,"[8] Geoffrey Chinot and Matthieu Lerasle.
On the robustness of the minimum ℓ2 interpola-
309"
REFERENCES,0.43907793633369924,"tor. Bernoulli, 2023. forthcoming, available at https://www.bernoullisociety.org/
310"
REFERENCES,0.44017563117453345,"publications/bernoulli-journal/bernoulli-journal-papers.
311"
REFERENCES,0.44127332601536773,"[9] Geoffrey Chinot, Matthias Löffler, and Sara van de Geer. On the robustness of minimum norm
312"
REFERENCES,0.442371020856202,"interpolators and regularized empirical risk minimizers. The Annals of Statistics, 50(4):2306 –
313"
REFERENCES,0.4434687156970362,"2333, 2022. doi: 10.1214/22-AOS2190. URL https://doi.org/10.1214/22-AOS2190.
314"
REFERENCES,0.4445664105378705,"[10] AP Dawid. Spherical matrix distributions and a multivariate model. Journal of the Royal
315"
REFERENCES,0.4456641053787047,"Statistical Society: Series B (Methodological), 39(2):254–261, 1977.
316"
REFERENCES,0.446761800219539,"[11] AP Dawid. Extendibility of spherical matrix distributions. Journal of Multivariate Analysis, 8
317"
REFERENCES,0.4478594950603732,"(4):559–566, 1978.
318"
REFERENCES,0.4489571899012075,"[12] AP Dawid. Some matrix-variate distribution theory: Notational considerations and a Bayesian
319"
REFERENCES,0.4500548847420417,"application. Biometrika, pages 265–274, 1981.
320"
REFERENCES,0.45115257958287597,"[13] Edgar Dobriban and Stefan Wager.
High-dimensional asymptotics of prediction: Ridge
321"
REFERENCES,0.4522502744237102,"regression and classification.
The Annals of Statistics, 46(1):247 – 279, 2018.
doi:
322"
REFERENCES,0.45334796926454446,"10.1214/17-AOS1549. URL https://doi.org/10.1214/17-AOS1549.
323"
REFERENCES,0.4544456641053787,"[14] Arjun K Gupta and Daya K Nagar. Matrix variate distributions, volume 104. CRC Press, 1999.
324"
REFERENCES,0.45554335894621295,"[15] Bruce Hansen. Econometrics. Princeton University Press, 2022.
325"
REFERENCES,0.4566410537870472,"[16] Bruce E Hansen. The risk of James–Stein and Lasso shrinkage. Econometric Reviews, 35(8-10):
326"
REFERENCES,0.45773874862788144,"1456–1470, 2016.
327"
REFERENCES,0.4588364434687157,"[17] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
328"
REFERENCES,0.45993413830954993,"dimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949–986,
329"
REFERENCES,0.4610318331503842,"2022.
330"
REFERENCES,0.4621295279912184,"[18] W. James and Charles Stein. Estimation with quadratic loss. In Proc. 4th Berkeley Sympos.
331"
REFERENCES,0.4632272228320527,"Math. Statist. and Prob., Vol. I, pages 361–379. Univ. California Press, Berkeley, Calif., 1961.
332"
REFERENCES,0.4643249176728869,"[19] Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez. The optimal ridge penalty for real-world
333"
REFERENCES,0.4654226125137212,"high-dimensional data can be zero or negative due to the implicit ridge regularization. The
334"
REFERENCES,0.4665203073545554,"Journal of Machine Learning Research, 21(1):6863–6878, 2020.
335"
REFERENCES,0.4676180021953897,"[20] Zeng Li, Chuanlong Xie, and Qinwen Wang. Asymptotic normality and confidence intervals
336"
REFERENCES,0.46871569703622395,"for prediction risk of the min-norm least squares estimator. In International Conference on
337"
REFERENCES,0.4698133918770582,"Machine Learning, pages 6533–6542. PMLR, 2021.
338"
REFERENCES,0.47091108671789245,"[21] Yuan Liao, Xinjie Ma, Andreas Neuhierl, and Zhentao Shi. Economic forecasts using many
339"
REFERENCES,0.47200878155872666,"noises.
arXiv preprint arXiv:2312.05593, 2023.
URL https://arxiv.org/abs/2312.
340"
REFERENCES,0.47310647639956094,"05593.
341"
REFERENCES,0.47420417124039516,"[22] Song Mei and Andrea Montanari. The generalization error of random features regression:
342"
REFERENCES,0.47530186608122943,"Precise asymptotics and the double descent curve. Communications on Pure and Applied
343"
REFERENCES,0.47639956092206365,"Mathematics, 75(4):667–766, 2022.
344"
REFERENCES,0.4774972557628979,"[23] Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge (less)
345"
REFERENCES,0.47859495060373214,"regression under general source condition. In International Conference on Artificial Intelligence
346"
REFERENCES,0.4796926454445664,"and Statistics, pages 3889–3897. PMLR, 2021.
347"
REFERENCES,0.4807903402854007,"[24] Steven Ruggles, Sarah Flood, Matthew Sobek, Daniel Backman, Annie Chen, Grace Cooper,
348"
REFERENCES,0.4818880351262349,"Stephanie Richards, Renae Rodgers, and Megan Schouweiler. IPUMS USA: Version 15.0
349"
REFERENCES,0.4829857299670692,"[dataset]. https://doi.org/10.18128/D010.V15.0, 2024. Minneapolis, MN: IPUMS.
350"
REFERENCES,0.4840834248079034,"[25] Jack W Silverstein and ZD Bai. On the empirical distribution of eigenvalues of a class of large
351"
REFERENCES,0.48518111964873767,"dimensional random matrices. Journal of Multivariate analysis, 54(2):175–192, 1995.
352"
REFERENCES,0.4862788144895719,"[26] Jann Spiess, Guido Imbens, and Amar Venugopal. Double and single descent in causal inference
353"
REFERENCES,0.48737650933040616,"with an application to high-dimensional synthetic control. In Thirty-seventh Conference on
354"
REFERENCES,0.4884742041712404,"Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=
355"
REFERENCES,0.48957189901207465,"dL0GM9Wwtq.
356"
REFERENCES,0.49066959385290887,"[27] Alexander Tsigler and Peter L. Bartlett. Benign overfitting in ridge regression. Journal of
357"
REFERENCES,0.49176728869374314,"Machine Learning Research, 24(123):1–76, 2023. URL http://jmlr.org/papers/v24/
358"
REFERENCES,0.49286498353457736,"22-1398.html.
359"
REFERENCES,0.49396267837541163,"[28] Denny Wu and Ji Xu. On the optimal weighted ℓ2 regularization in overparameterized linear
360"
REFERENCES,0.4950603732162459,"regression. Advances in Neural Information Processing Systems, 33:10112–10123, 2020.
361"
REFERENCES,0.4961580680570801,"[29] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Benign
362"
REFERENCES,0.4972557628979144,"overfitting of constant-stepsize SGD for linear regression. In Conference on Learning Theory,
363"
REFERENCES,0.4983534577387486,"pages 4633–4635. PMLR, 2021.
364"
REFERENCES,0.4994511525795829,"Appendix
365"
REFERENCES,0.5005488474204172,"A
Details for drawing Figure 1
366"
REFERENCES,0.5016465422612514,"To draw Figure 1, we use a sample extract from American Community Survey (ACS) 2018. To
367"
REFERENCES,0.5027442371020856,"have a relatively homogeneous population, the sample extract is restricted to white males residing in
368"
REFERENCES,0.5038419319429198,"California with at least a bachelor’s degree. We consider a demographic group defined by their age in
369"
REFERENCES,0.5049396267837541,"years (between 25 and 70), the type of degree (bachelor’s, master’s, professional, and doctoral), and
370"
REFERENCES,0.5060373216245884,"the field of degree (172 unique values). Then, we compute the average of log hourly wages for each
371"
REFERENCES,0.5071350164654226,"age-degree-field group (all together 7,073 unique groups in the sample). We treat each group average
372"
REFERENCES,0.5082327113062569,"as the outcome variable (say, ya,d, f ) and predict group wages by various group-level regression models
373"
REFERENCES,0.5093304061470911,"where the regressors are constructed using the indicator variables of age, degree, and field as well as
374"
REFERENCES,0.5104281009879253,"their interactions: that is,
375"
REFERENCES,0.5115257958287596,"ya,d,f = x⊤
a,d, f β + εa,d, f ."
REFERENCES,0.5126234906695939,"For the regressors xa,d, f , we consider 7 specifications ranging from 209 to 2,183 regressors:
376"
REFERENCES,0.5137211855104281,"• Spec. 1 (p = 209): dummy variables for age (say, xa) + dummy variables for the type of
377"
REFERENCES,0.5148188803512623,"degree (say, xd) + dummy variables for the field of degree (say, x f ),
378"
REFERENCES,0.5159165751920965,"• Spec. 2 (p = 391): Spec. 1 + all interactions between xd and xa,
379"
REFERENCES,0.5170142700329309,"• Spec. 3 (p = 598): Spec. 1 + all interactions between xd and xf ,
380"
REFERENCES,0.5181119648737651,"• Spec. 4 (p = 778): Spec. 1 + all interactions between xd and xa + all interactions between
381"
REFERENCES,0.5192096597145993,"xd and xf ,
382"
REFERENCES,0.5203073545554336,"• Spec. 5 (p = 1640): Spec. 1 + all interactions between xd and xa + all interactions between
383"
REFERENCES,0.5214050493962679,"xa and xf ,
384"
REFERENCES,0.5225027442371021,"• Spec. 6 (p = 1754): Spec. 1 + all interactions between xd and x f + all interactions between
385"
REFERENCES,0.5236004390779363,"xa and xf ,
386"
REFERENCES,0.5246981339187706,"• Spec. 7 (p = 2182): Spec. 1 + all three-way interactions among xa, xd and xf .
387"
REFERENCES,0.5257958287596048,"Here, the dummy variable are constructed using one-hot encoding. We randomly split the sample
388"
REFERENCES,0.5268935236004391,"into the train and test samples with a ratio of 1 : 4. The resulting sample sizes are 1,415 and 5,658,
389"
REFERENCES,0.5279912184412733,"respectively. To understand the role of non-i.i.d. regressor errors, we add the artificial noise to the
390"
REFERENCES,0.5290889132821076,"training sample: that is, we compute the ridgeless least squares estimator using the training sample of
391"
REFERENCES,0.5301866081229418,"(˜ya,d,f , x⊤
a,d,f )⊤, where ˜ya,d, f = ya,d, f + ua,d,f . Here, the artificial noise ua,d, f has the form
392"
REFERENCES,0.531284302963776,"ua,d, f ≡(1 −c)ea,d,f + c · e f
p"
REFERENCES,0.5323819978046103,"(1 −c)2 + c2
,"
REFERENCES,0.5334796926454446,"where ea,d, f ∼N(0, σ2), independently across age (a), degree (d) and field ( f); ef is the average of
393"
REFERENCES,0.5345773874862788,"another independent N(0, σ2) variable within f (hence, ef is identical for each value of f) and thus
394"
REFERENCES,0.535675082327113,"the source of clustered errors; and c ∈{0, 0.25, 0.5, 0.75} is a constant that will be varied across the
395"
REFERENCES,0.5367727771679474,"experiment. As c gets larger, the noise has a larger share of clustered errors but the variance of the
396"
REFERENCES,0.5378704720087816,"overall regression errors (ua,d, f ) remains the same: in other words, var(ua,d, f ) = σ2 for each value of
397"
REFERENCES,0.5389681668496158,"c. Figure 1 was generated with σ = 0.5 by generating the artificial noise only once.
398"
REFERENCES,0.54006586169045,"B
Details for drawing Figures 2, 3, and 4
399"
REFERENCES,0.5411635565312843,"To draw Figure 2, 3, and 4, we sample {xi}n
i=1 from N(0, Σ) with Σ = UΣDΣU⊤
Σ where UΣ is an
400"
REFERENCES,0.5422612513721186,"orthogonal matrix random variable, drawn from the uniform (Haar) distribution on O(p), and DΣ
401"
REFERENCES,0.5433589462129528,"is a diagonal matrix with its elements di = |zi|/ Pp
i=1 |zi| being sampled with zi ∼N(0, 1) for each
402"
REFERENCES,0.544456641053787,"i = 1, 2, · · · , p. With this general anisotropic Σ, the term EX[Tr(Λ−1)]/p is somewhat larger than
403"
REFERENCES,0.5455543358946213,"µ−1
H s∗
iso = (γ −1)−1 which is 1 in Figure 2 and 3 since µH = 1 and γ = 2. For example, in Figure 2,
404"
REFERENCES,0.5466520307354555,"when σ2 = 1, ρ2 = 0, we have Tr(Ω)/n = 1 but Tr(Ω)EX[Tr(Λ−1)]/(np) > 1.
405"
REFERENCES,0.5477497255762898,"In Figure 4, we fix n = 50 and use p = nγ for γ ∈[1, 100].
406"
REFERENCES,0.5488474204171241,"To compute the expectations of EX[Var(ˆβ|X)] and EX[Tr(Λ−1)] over X, we sample NX sam-
407"
REFERENCES,0.5499451152579583,"ples of X’s, X1, X2, · · · , XNX.
Moreover, to compute the expectation over ε in Var(ˆβ|Xi) ≡
408"
REFERENCES,0.5510428100987925,"Tr

Eε[ˆβˆβ⊤] −Eε[ˆβ]Eε[ˆβ]⊤
, we sample Nε samples of ε’s, ε1, ε2, · · · , εNε for each realization Xi.
409"
REFERENCES,0.5521405049396267,"To be specific,
410"
REFERENCES,0.5532381997804611,"EX[Var(ˆβ|X)] ≈1 NX NX
X"
REFERENCES,0.5543358946212953,"i=1
Var(ˆβ|Xi) ≈1 NX NX
X"
REFERENCES,0.5554335894621295,"i=1
Tr"
REFERENCES,0.5565312843029637,"
1
Nε Nε
X"
REFERENCES,0.557628979143798,"j=1
ˆβi, j ˆβ⊤
i, j −1 Nε Nε
X"
REFERENCES,0.5587266739846323,"j=1
ˆβi, j
1
Nε Nε
X"
REFERENCES,0.5598243688254665,"j=1
ˆβ⊤
i,j"
REFERENCES,0.5609220636663008,
REFERENCES,0.562019758507135,"1
pEX[Tr(Λ−1)] ≈1 NX NX
X"
REFERENCES,0.5631174533479693,"i=1
Tr((XiX⊤
i )−1) = 1 NX NX
X i=1 n
X k=1"
REFERENCES,0.5642151481888035,"1
λk(XiX⊤
i ),"
REFERENCES,0.5653128430296378,"where ˆβi, j = arg minβ{∥b∥: Xib −yi,j = 0}, yi, j = Xiβ + εj, and λk(XiX⊤
i ) is the k-th eigenvalue of
411"
REFERENCES,0.566410537870472,"XiX⊤
i . We can do similarly for the variance part of the prediction risk.
412"
REFERENCES,0.5675082327113062,"Figure 5 shows an additional experimental result.
413"
REFERENCES,0.5686059275521405,"0.1
0.2
0.3
0.4
0.5 2
1 0.1 0.2 0.3 0.4 0.5 2
2"
REFERENCES,0.5697036223929748,Prediction Risk - Clustered Errors 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70
REFERENCES,0.570801317233809,"0.1
0.2
0.3
0.4
0.5 2
1 0.1 0.2 0.3 0.4 0.5 2
2"
REFERENCES,0.5718990120746432,Estimation Risk - Clustered Errors 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.6
REFERENCES,0.5729967069154775,"Figure 5: We use the same setting as Figure 3, except uniformly sample each ρi from [0, 0.05] for
each experiment with the pairs (σ2
1, σ2
1). As expected, the off-diagonal elements ρi of Ωdo not affect
the expected variances."
REFERENCES,0.5740944017563118,"C
Proofs omitted in the main text
414"
REFERENCES,0.575192096597146,"Proof of Lemma 3.3. For a given A ∈S, since A−1 ∈S, we have Z
d= A−1Z := ˜Z and
415"
REFERENCES,0.5762897914379802,EZ[ f(Z)] = EA−1Z[ f(Z)] = E ˜Z[f(A ˜Z)] = EZ[ f(AZ)].
REFERENCES,0.5773874862788145,This naturally leads to
REFERENCES,0.5784851811196488,EZ[EA′∼ν[ f(A′Z)]] = EA′∼ν[EZ[ f(A′Z)]] = EA′∼ν[EZ[ f(Z)]] = EZ[ f(Z)]
REFERENCES,0.579582875960483,"where the first equality comes from Fubini’s theorem and the integrability of f.
□
416"
REFERENCES,0.5806805708013172,"Proof of Theorem 3.4. Since ˆβ = X†y, we have Cov(ˆβ | X) = X†Cov(y | X)X†⊤= X†ΩX†⊤, which
417"
REFERENCES,0.5817782656421515,"leads to the following expression for the variance component of prediction risk:
418"
REFERENCES,0.5828759604829857,"VarΣ(ˆβ | X) = Tr(Cov(ˆβ | X)Σ) = Tr(X†ΩX†⊤Σ) = ∥S X†T∥2
F = ∥BT∥2
F,"
REFERENCES,0.58397365532382,"where S = Σ1/2, T = Ω1/2, and B = S X†. Using the singular value decomposition (SVD) of B and T,
419"
REFERENCES,0.5850713501646543,"respectively, we can rewrite this as follows:
420"
REFERENCES,0.5861690450054885,"∥BT∥2
F = ∥UDV⊤UT DTV⊤
T ∥2
F= ∥DV⊤UT DT∥2
F,"
REFERENCES,0.5872667398463227,"where B = UDV⊤and T = UT DTV⊤
T with orthogonal matrices U, V, UT, VT, and diagonal matrices
421"
REFERENCES,0.5883644346871569,"D, DT. Now we need to compute the alignment V⊤UT of the right-singular vectors of B with the
422"
REFERENCES,0.5894621295279913,"left-eigenvectors of T.
423"
REFERENCES,0.5905598243688255,"∥DV⊤UT DT∥2
F = n
X i,j=1"
REFERENCES,0.5916575192096597,"
Dii
Xn"
REFERENCES,0.5927552140504939,"k=1 V⊤
ik(UT)k j(DT) j j
2 =
Xn"
REFERENCES,0.5938529088913282,"i,j=1 λi(B)2λ j(T)2γi j =
Xn"
REFERENCES,0.5949506037321625,"i,j=1 λi

(X⊤X)†Σ

λj(Ω)γi j"
REFERENCES,0.5960482985729967,"= λ

(X⊤X)†Σ
⊤ 1×n Γ(X)"
REFERENCES,0.5971459934138309,"n×n
λ(Ω) n×1
,"
REFERENCES,0.5982436882546652,"where γij := ⟨V:i, (UT): j⟩2 ≥0, Γ(X) := (γi j)i,j ∈Rn×n and λ(A) ∈Rn is a vector with its element
424"
REFERENCES,0.5993413830954994,"λi(A) as the i-th largest eigenvalue of A.
425"
REFERENCES,0.6004390779363337,"Therefore, we can rewrite the variance as VarΣ(ˆβ | X) = a(X)⊤Γ(X)b with
426"
REFERENCES,0.601536772777168,"a(X) := λ

(X⊤X)†Σ

∈Rn,"
REFERENCES,0.6026344676180022,"b := λ(Ω) ∈Rn,"
REFERENCES,0.6037321624588364,"Γ(X)i j = γi j = ⟨v(i), u(j)⟩2,"
REFERENCES,0.6048298572996706,"where v(i) := V:i and u( j) := (UT): j. Note that the alignment matrix Γ(X) is a doubly stochastic matrix
427"
REFERENCES,0.605927552140505,"since P
j γij = P
i γij = 1 and 0 ≤γij ≤1.
428"
REFERENCES,0.6070252469813392,"Now, we want to compute the expected variance. To do so, from Lemma 3.3 with S = O(n), we can
429"
REFERENCES,0.6081229418221734,"obtain
430"
REFERENCES,0.6092206366630076,"EX[a(X)⊤Γ(X)b] = EX
h
EO∼ν[a(OX)⊤Γ(OX)b]
i
= EX
h
a(X)⊤EO∼ν[Γ(OX)]b
i
,"
REFERENCES,0.610318331503842,"where ν is the unique uniform distribution (the Haar measure) over the orthogonal matrices O(n). For
431"
REFERENCES,0.6114160263446762,"an orthogonal matrix O ∈O(n), we have
432"
REFERENCES,0.6125137211855104,"Γ(OX)ij = ⟨Ov(i), u( j)⟩2 = (v(i)⊤O⊤u(j))2,"
REFERENCES,0.6136114160263447,"since S (OX)† = S X†O⊤= BO⊤= UD(OV)⊤. Here, (OX)† = X†O⊤follows from the orthogonality
433"
REFERENCES,0.6147091108671789,"of O ∈O(n). Since the Haar measure is invariant under the matrix multiplication in O(n), if we take
434"
REFERENCES,0.6158068057080132,"the expectation over the Haar measure, then we have
435"
REFERENCES,0.6169045005488474,¯Γ(X)ij := EO∼ν[Γ(OX)ij] = EO∼ν[(v(i)⊤O⊤u(j))2] = EO∼ν[(v(i)⊤O⊤O(j)⊤u(j))2].
REFERENCES,0.6180021953896817,"Here, for a given j, we can choose a matrix O(j) ∈O(n) such that its first column is u(j) and
436"
REFERENCES,0.6190998902305159,"O(j)⊤u( j) = e1, then ¯Γ(X)ij is independent of j (say ¯Γ(X)i j = αi). Since Γ(X) is doubly stochastic,
437"
REFERENCES,0.6201975850713501,"so is ¯Γ(X) and we have Pn
j=1 ¯Γ(X)ij = nαi = 1 which yields ¯Γ(X)i j = αi = 1/n, regardless of the
438"
REFERENCES,0.6212952799121844,"distribution of V; thus, ¯Γ(X) = 1"
REFERENCES,0.6223929747530187,"n J, where Ji j = 1(i, j = 1, 2, · · · , n).
439"
REFERENCES,0.6234906695938529,"Therefore, we have the expected variance as follows:
440"
REFERENCES,0.6245883644346871,EX[VarΣ(ˆβ | X)] = EX[a(X)⊤1
REFERENCES,0.6256860592755215,"n Jb] = 1 n n
X"
REFERENCES,0.6267837541163557,"i,j=1
EX[ai(X)]bj = 1"
REFERENCES,0.6278814489571899,"nEX[Tr((X⊤X)†Σ)] Tr(Ω). □
441"
REFERENCES,0.6289791437980241,"Proof of Corollary 4.5. Note that
442"
REFERENCES,0.6300768386388584,"EX[Var(ˆβ|X)] = Tr(Ω) p
EX"
REFERENCES,0.6311745334796927,"
1
n X i 1
λi"
REFERENCES,0.6322722283205269,
REFERENCES,0.6333699231613611,"= Tr(Ω) p
EX ""Z 1"
REFERENCES,0.6344676180021954,"s dFXX⊤/p(s)
#"
REFERENCES,0.6355653128430296,"= Tr(Ω) n
EX ""Z 1"
REFERENCES,0.6366630076838639,"s dFXX⊤/n(s)
#
."
REFERENCES,0.6377607025246982,"Then, the desired result follows directly from equation 5.
□
443"
REFERENCES,0.6388583973655324,"Proof of equation 4. The bias term of the prediction risk can be expressed as follows:
444"
REFERENCES,0.6399560922063666,"[BiasΣ(ˆβ | X)]2 = ∥E[ˆβ | X] −β∥2
Σ
= ∥(ˆΣ† ˆΣ −I)β∥2
Σ
= β⊤(I −ˆΣ† ˆΣ)Σ(I −ˆΣ† ˆΣ)β"
REFERENCES,0.6410537870472008,"= β⊤lim
λ↘0 λ(ˆΣ + λI)−1Σ lim
λ↘0 λ(ˆΣ + λI)−1β"
REFERENCES,0.6421514818880352,"= (S β)⊤lim
λ↘0 λ2(S −1 ˆΣS + λI)−2S β,"
REFERENCES,0.6432491767288694,"where ˆΣ = X⊤X/n. Here, the fourth equality comes from the equation
445"
REFERENCES,0.6443468715697036,"I −ˆΣ† ˆΣ = lim
λ↘0 I −(ˆΣ + λI)−1 ˆΣ"
REFERENCES,0.6454445664105378,"= lim
λ↘0 I −(ˆΣ + λI)−1(ˆΣ + λI −λI)"
REFERENCES,0.6465422612513722,"= lim
λ↘0 λ(ˆΣ + λI)−1. □
446"
REFERENCES,0.6476399560922064,"Proof of equation 7. The RHS of equation 6 is bounded above by
R
1
1+cH s∗dH(τ) =
1
1+cH s∗, and thus
447 1 −1"
REFERENCES,0.6487376509330406,"γ ≤
1
1+cH s∗, which yields s∗≤c−1
H
1
γ−1. We can similarly prove the other inequality in equation 7
448"
REFERENCES,0.6498353457738749,"with a lower bound
1
1+CH s∗on the RHS of equation 6.
□
449"
REFERENCES,0.6509330406147091,"Proof of equation 8. To further explore the inequalities equation 7, we rewrite equation 6 from
450"
REFERENCES,0.6520307354555434,"Theorem 4.7 as follows:
451 1 −1"
REFERENCES,0.6531284302963776,"γ = Eτ∼H
g(τ; s∗) ,
where g(t; s) :=
1
1 + ts for t, s > 0."
REFERENCES,0.6542261251372119,"Here, since g(t; s) is convex with respect to t > 0 for a given s > 0, by Jensen’s inequality, we then
have
Eτ∼H[g(τ; µ−1
H s∗
iso)] ≥g

µH; µ−1
H s∗
iso

= g(1; s∗
iso) = 1 −γ−1"
REFERENCES,0.6553238199780461,"where µH = Eτ∼H[τ]. Therefore, the limit Stieltjes transform s∗in the anisotropic case should be larger
452"
REFERENCES,0.6564215148188803,"than µ−1
H s∗
iso of the isotropic case to satisfy Eτ∼H[g(τ; s∗)] = 1−γ−1 since g(t; s) is a decreasing function
453"
REFERENCES,0.6575192096597146,"with respect to s ≥0 when t > 0. This leads to a tighter lower bound s∗≥µ−1
H s∗
iso = µ−1
H (γ −1)−1 than
454"
REFERENCES,0.6586169045005489,"equation 7 because µH ≤CH.
□
455"
REFERENCES,0.6597145993413831,"NeurIPS Paper Checklist
456"
CLAIMS,0.6608122941822173,"1. Claims
457"
CLAIMS,0.6619099890230515,"Question: Do the main claims made in the abstract and introduction accurately reflect the
458"
CLAIMS,0.6630076838638859,"paper’s contributions and scope?
459"
CLAIMS,0.6641053787047201,"Answer: [Yes]
460"
CLAIMS,0.6652030735455543,"Justification: The claims accurately reflect the paper’s contributions and scope.
461"
CLAIMS,0.6663007683863886,"Guidelines:
462"
CLAIMS,0.6673984632272228,"• The answer NA means that the abstract and introduction do not include the claims
463"
CLAIMS,0.6684961580680571,"made in the paper.
464"
CLAIMS,0.6695938529088913,"• The abstract and/or introduction should clearly state the claims made, including the
465"
CLAIMS,0.6706915477497256,"contributions made in the paper and important assumptions and limitations. A No or
466"
CLAIMS,0.6717892425905598,"NA answer to this question will not be perceived well by the reviewers.
467"
CLAIMS,0.672886937431394,"• The claims made should match theoretical and experimental results, and reflect how
468"
CLAIMS,0.6739846322722283,"much the results can be expected to generalize to other settings.
469"
CLAIMS,0.6750823271130626,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
470"
CLAIMS,0.6761800219538968,"are not attained by the paper.
471"
LIMITATIONS,0.677277716794731,"2. Limitations
472"
LIMITATIONS,0.6783754116355654,"Question: Does the paper discuss the limitations of the work performed by the authors?
473"
LIMITATIONS,0.6794731064763996,"Answer: [Yes]
474"
LIMITATIONS,0.6805708013172338,"Justification: See the last paragraph of Introduction.
475"
LIMITATIONS,0.681668496158068,"Guidelines:
476"
LIMITATIONS,0.6827661909989023,"• The answer NA means that the paper has no limitation while the answer No means that
477"
LIMITATIONS,0.6838638858397366,"the paper has limitations, but those are not discussed in the paper.
478"
LIMITATIONS,0.6849615806805708,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
479"
LIMITATIONS,0.686059275521405,"• The paper should point out any strong assumptions and how robust the results are to
480"
LIMITATIONS,0.6871569703622393,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
481"
LIMITATIONS,0.6882546652030735,"model well-specification, asymptotic approximations only holding locally). The authors
482"
LIMITATIONS,0.6893523600439078,"should reflect on how these assumptions might be violated in practice and what the
483"
LIMITATIONS,0.6904500548847421,"implications would be.
484"
LIMITATIONS,0.6915477497255763,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
485"
LIMITATIONS,0.6926454445664105,"only tested on a few datasets or with a few runs. In general, empirical results often
486"
LIMITATIONS,0.6937431394072447,"depend on implicit assumptions, which should be articulated.
487"
LIMITATIONS,0.6948408342480791,"• The authors should reflect on the factors that influence the performance of the approach.
488"
LIMITATIONS,0.6959385290889133,"For example, a facial recognition algorithm may perform poorly when image resolution
489"
LIMITATIONS,0.6970362239297475,"is low or images are taken in low lighting. Or a speech-to-text system might not be
490"
LIMITATIONS,0.6981339187705817,"used reliably to provide closed captions for online lectures because it fails to handle
491"
LIMITATIONS,0.6992316136114161,"technical jargon.
492"
LIMITATIONS,0.7003293084522503,"• The authors should discuss the computational efficiency of the proposed algorithms
493"
LIMITATIONS,0.7014270032930845,"and how they scale with dataset size.
494"
LIMITATIONS,0.7025246981339188,"• If applicable, the authors should discuss possible limitations of their approach to
495"
LIMITATIONS,0.703622392974753,"address problems of privacy and fairness.
496"
LIMITATIONS,0.7047200878155873,"• While the authors might fear that complete honesty about limitations might be used by
497"
LIMITATIONS,0.7058177826564215,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
498"
LIMITATIONS,0.7069154774972558,"limitations that aren’t acknowledged in the paper. The authors should use their best
499"
LIMITATIONS,0.70801317233809,"judgment and recognize that individual actions in favor of transparency play an impor-
500"
LIMITATIONS,0.7091108671789242,"tant role in developing norms that preserve the integrity of the community. Reviewers
501"
LIMITATIONS,0.7102085620197585,"will be specifically instructed to not penalize honesty concerning limitations.
502"
THEORY ASSUMPTIONS AND PROOFS,0.7113062568605928,"3. Theory Assumptions and Proofs
503"
THEORY ASSUMPTIONS AND PROOFS,0.712403951701427,"Question: For each theoretical result, does the paper provide the full set of assumptions and
504"
THEORY ASSUMPTIONS AND PROOFS,0.7135016465422612,"a complete (and correct) proof?
505"
THEORY ASSUMPTIONS AND PROOFS,0.7145993413830956,"Answer: [Yes]
506"
THEORY ASSUMPTIONS AND PROOFS,0.7156970362239298,"Justification: We explicitly mention the assumptions and provide a complete proof (see
507"
THEORY ASSUMPTIONS AND PROOFS,0.716794731064764,"Appendix).
508"
THEORY ASSUMPTIONS AND PROOFS,0.7178924259055982,"Guidelines:
509"
THEORY ASSUMPTIONS AND PROOFS,0.7189901207464325,"• The answer NA means that the paper does not include theoretical results.
510"
THEORY ASSUMPTIONS AND PROOFS,0.7200878155872668,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
511"
THEORY ASSUMPTIONS AND PROOFS,0.721185510428101,"referenced.
512"
THEORY ASSUMPTIONS AND PROOFS,0.7222832052689352,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
513"
THEORY ASSUMPTIONS AND PROOFS,0.7233809001097695,"• The proofs can either appear in the main paper or the supplemental material, but if
514"
THEORY ASSUMPTIONS AND PROOFS,0.7244785949506037,"they appear in the supplemental material, the authors are encouraged to provide a short
515"
THEORY ASSUMPTIONS AND PROOFS,0.725576289791438,"proof sketch to provide intuition.
516"
THEORY ASSUMPTIONS AND PROOFS,0.7266739846322722,"• Inversely, any informal proof provided in the core of the paper should be complemented
517"
THEORY ASSUMPTIONS AND PROOFS,0.7277716794731065,"by formal proofs provided in appendix or supplemental material.
518"
THEORY ASSUMPTIONS AND PROOFS,0.7288693743139407,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
519"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7299670691547749,"4. Experimental Result Reproducibility
520"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7310647639956093,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
521"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7321624588364435,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
522"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7332601536772777,"of the paper (regardless of whether the code and data are provided or not)?
523"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7343578485181119,"Answer: [Yes]
524"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7354555433589463,"Justification: We provide the code (supplementary material) and all the information needed
525"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7365532381997805,"to reproduce the main results (Appendix).
526"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7376509330406147,"Guidelines:
527"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7387486278814489,"• The answer NA means that the paper does not include experiments.
528"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7398463227222832,"• If the paper includes experiments, a No answer to this question will not be perceived
529"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7409440175631175,"well by the reviewers: Making the paper reproducible is important, regardless of
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7420417124039517,"whether the code and data are provided or not.
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.743139407244786,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
532"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7442371020856202,"to make their results reproducible or verifiable.
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7453347969264544,"• Depending on the contribution, reproducibility can be accomplished in various ways.
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7464324917672887,"For example, if the contribution is a novel architecture, describing the architecture fully
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.747530186608123,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7486278814489572,"be necessary to either make it possible for others to replicate the model with the same
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497255762897914,"dataset, or provide access to the model. In general. releasing code and data is often
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7508232711306256,"one good way to accomplish this, but reproducibility can also be provided via detailed
539"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75192096597146,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
540"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7530186608122942,"of a large language model), releasing of a model checkpoint, or other means that are
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7541163556531284,"appropriate to the research performed.
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7552140504939627,"• While NeurIPS does not require releasing code, the conference does require all submis-
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.756311745334797,"sions to provide some reasonable avenue for reproducibility, which may depend on the
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7574094401756312,"nature of the contribution. For example
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7585071350164654,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7596048298572997,"to reproduce that algorithm.
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7607025246981339,"(b) If the contribution is primarily a new model architecture, the paper should describe
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7618002195389681,"the architecture clearly and fully.
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7628979143798024,"(c) If the contribution is a new model (e.g., a large language model), then there should
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7639956092206367,"either be a way to access this model for reproducing the results or a way to reproduce
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7650933040614709,"the model (e.g., with an open-source dataset or instructions for how to construct
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7661909989023051,"the dataset).
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7672886937431395,"(d) We recognize that reproducibility may be tricky in some cases, in which case
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7683863885839737,"authors are welcome to describe the particular way they provide for reproducibility.
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7694840834248079,"In the case of closed-source models, it may be that access to the model is limited in
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7705817782656421,"some way (e.g., to registered users), but it should be possible for other researchers
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7716794731064764,"to have some path to reproducing or verifying the results.
558"
OPEN ACCESS TO DATA AND CODE,0.7727771679473107,"5. Open access to data and code
559"
OPEN ACCESS TO DATA AND CODE,0.7738748627881449,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
560"
OPEN ACCESS TO DATA AND CODE,0.7749725576289791,"tions to faithfully reproduce the main experimental results, as described in supplemental
561"
OPEN ACCESS TO DATA AND CODE,0.7760702524698134,"material?
562"
OPEN ACCESS TO DATA AND CODE,0.7771679473106476,"Answer: [Yes]
563"
OPEN ACCESS TO DATA AND CODE,0.7782656421514819,"Justification: We provide the code (supplementary material) and all the information needed
564"
OPEN ACCESS TO DATA AND CODE,0.7793633369923162,"to reproduce the main results (Appendix).
565"
OPEN ACCESS TO DATA AND CODE,0.7804610318331504,"Guidelines:
566"
OPEN ACCESS TO DATA AND CODE,0.7815587266739846,"• The answer NA means that paper does not include experiments requiring code.
567"
OPEN ACCESS TO DATA AND CODE,0.7826564215148188,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
568"
OPEN ACCESS TO DATA AND CODE,0.7837541163556532,"public/guides/CodeSubmissionPolicy) for more details.
569"
OPEN ACCESS TO DATA AND CODE,0.7848518111964874,"• While we encourage the release of code and data, we understand that this might not be
570"
OPEN ACCESS TO DATA AND CODE,0.7859495060373216,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
571"
OPEN ACCESS TO DATA AND CODE,0.7870472008781558,"including code, unless this is central to the contribution (e.g., for a new open-source
572"
OPEN ACCESS TO DATA AND CODE,0.7881448957189902,"benchmark).
573"
OPEN ACCESS TO DATA AND CODE,0.7892425905598244,"• The instructions should contain the exact command and environment needed to run to
574"
OPEN ACCESS TO DATA AND CODE,0.7903402854006586,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
575"
OPEN ACCESS TO DATA AND CODE,0.7914379802414928,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
576"
OPEN ACCESS TO DATA AND CODE,0.7925356750823271,"• The authors should provide instructions on data access and preparation, including how
577"
OPEN ACCESS TO DATA AND CODE,0.7936333699231614,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
578"
OPEN ACCESS TO DATA AND CODE,0.7947310647639956,"• The authors should provide scripts to reproduce all experimental results for the new
579"
OPEN ACCESS TO DATA AND CODE,0.7958287596048299,"proposed method and baselines. If only a subset of experiments are reproducible, they
580"
OPEN ACCESS TO DATA AND CODE,0.7969264544456641,"should state which ones are omitted from the script and why.
581"
OPEN ACCESS TO DATA AND CODE,0.7980241492864983,"• At submission time, to preserve anonymity, the authors should release anonymized
582"
OPEN ACCESS TO DATA AND CODE,0.7991218441273326,"versions (if applicable).
583"
OPEN ACCESS TO DATA AND CODE,0.8002195389681669,"• Providing as much information as possible in supplemental material (appended to the
584"
OPEN ACCESS TO DATA AND CODE,0.8013172338090011,"paper) is recommended, but including URLs to data and code is permitted.
585"
OPEN ACCESS TO DATA AND CODE,0.8024149286498353,"6. Experimental Setting/Details
586"
OPEN ACCESS TO DATA AND CODE,0.8035126234906695,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
587"
OPEN ACCESS TO DATA AND CODE,0.8046103183315039,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
588"
OPEN ACCESS TO DATA AND CODE,0.8057080131723381,"results?
589"
OPEN ACCESS TO DATA AND CODE,0.8068057080131723,"Answer: [Yes]
590"
OPEN ACCESS TO DATA AND CODE,0.8079034028540066,"Justification: We provide all the details necessary to understand the results (Appendix).
591"
OPEN ACCESS TO DATA AND CODE,0.8090010976948409,"Guidelines:
592"
OPEN ACCESS TO DATA AND CODE,0.8100987925356751,"• The answer NA means that the paper does not include experiments.
593"
OPEN ACCESS TO DATA AND CODE,0.8111964873765093,"• The experimental setting should be presented in the core of the paper to a level of detail
594"
OPEN ACCESS TO DATA AND CODE,0.8122941822173436,"that is necessary to appreciate the results and make sense of them.
595"
OPEN ACCESS TO DATA AND CODE,0.8133918770581778,"• The full details can be provided either with the code, in appendix, or as supplemental
596"
OPEN ACCESS TO DATA AND CODE,0.814489571899012,"material.
597"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8155872667398463,"7. Experiment Statistical Significance
598"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8166849615806806,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
599"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8177826564215148,"information about the statistical significance of the experiments?
600"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.818880351262349,"Answer: [NA]
601"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8199780461031834,"Justification: To support our claim, the experiments in the paper do not need error bars.
602"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8210757409440176,"Guidelines:
603"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8221734357848518,"• The answer NA means that the paper does not include experiments.
604"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.823271130625686,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
605"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8243688254665203,"dence intervals, or statistical significance tests, at least for the experiments that support
606"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8254665203073546,"the main claims of the paper.
607"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8265642151481888,"• The factors of variability that the error bars are capturing should be clearly stated (for
608"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.827661909989023,"example, train/test split, initialization, random drawing of some parameter, or overall
609"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8287596048298573,"run with given experimental conditions).
610"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8298572996706916,"• The method for calculating the error bars should be explained (closed form formula,
611"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8309549945115258,"call to a library function, bootstrap, etc.)
612"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8320526893523601,"• The assumptions made should be given (e.g., Normally distributed errors).
613"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8331503841931943,"• It should be clear whether the error bar is the standard deviation or the standard error
614"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8342480790340285,"of the mean.
615"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8353457738748628,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
616"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8364434687156971,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
617"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8375411635565313,"of Normality of errors is not verified.
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8386388583973655,"• For asymmetric distributions, the authors should be careful not to show in tables or
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8397365532381997,"figures symmetric error bars that would yield results that are out of range (e.g. negative
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8408342480790341,"error rates).
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8419319429198683,"• If error bars are reported in tables or plots, The authors should explain in the text how
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8430296377607025,"they were calculated and reference the corresponding figures or tables in the text.
623"
EXPERIMENTS COMPUTE RESOURCES,0.8441273326015367,"8. Experiments Compute Resources
624"
EXPERIMENTS COMPUTE RESOURCES,0.845225027442371,"Question: For each experiment, does the paper provide sufficient information on the com-
625"
EXPERIMENTS COMPUTE RESOURCES,0.8463227222832053,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
626"
EXPERIMENTS COMPUTE RESOURCES,0.8474204171240395,"the experiments?
627"
EXPERIMENTS COMPUTE RESOURCES,0.8485181119648738,"Answer: [NA]
628"
EXPERIMENTS COMPUTE RESOURCES,0.849615806805708,"Justification: Our models are linear regression models which do not require much resources.
629"
EXPERIMENTS COMPUTE RESOURCES,0.8507135016465422,"Guidelines:
630"
EXPERIMENTS COMPUTE RESOURCES,0.8518111964873765,"• The answer NA means that the paper does not include experiments.
631"
EXPERIMENTS COMPUTE RESOURCES,0.8529088913282108,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
632"
EXPERIMENTS COMPUTE RESOURCES,0.854006586169045,"or cloud provider, including relevant memory and storage.
633"
EXPERIMENTS COMPUTE RESOURCES,0.8551042810098792,"• The paper should provide the amount of compute required for each of the individual
634"
EXPERIMENTS COMPUTE RESOURCES,0.8562019758507134,"experimental runs as well as estimate the total compute.
635"
EXPERIMENTS COMPUTE RESOURCES,0.8572996706915478,"• The paper should disclose whether the full research project required more compute
636"
EXPERIMENTS COMPUTE RESOURCES,0.858397365532382,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
637"
EXPERIMENTS COMPUTE RESOURCES,0.8594950603732162,"didn’t make it into the paper).
638"
CODE OF ETHICS,0.8605927552140505,"9. Code Of Ethics
639"
CODE OF ETHICS,0.8616904500548848,"Question: Does the research conducted in the paper conform, in every respect, with the
640"
CODE OF ETHICS,0.862788144895719,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
641"
CODE OF ETHICS,0.8638858397365532,"Answer: [Yes]
642"
CODE OF ETHICS,0.8649835345773875,"Justification: We conform with the NeurIPS Code of Ethics.
643"
CODE OF ETHICS,0.8660812294182217,"Guidelines:
644"
CODE OF ETHICS,0.867178924259056,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
645"
CODE OF ETHICS,0.8682766190998902,"• If the authors answer No, they should explain the special circumstances that require a
646"
CODE OF ETHICS,0.8693743139407245,"deviation from the Code of Ethics.
647"
CODE OF ETHICS,0.8704720087815587,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
648"
CODE OF ETHICS,0.8715697036223929,"eration due to laws or regulations in their jurisdiction).
649"
BROADER IMPACTS,0.8726673984632273,"10. Broader Impacts
650"
BROADER IMPACTS,0.8737650933040615,"Question: Does the paper discuss both potential positive societal impacts and negative
651"
BROADER IMPACTS,0.8748627881448957,"societal impacts of the work performed?
652"
BROADER IMPACTS,0.8759604829857299,"Answer: [NA]
653"
BROADER IMPACTS,0.8770581778265643,"Justification: The paper is a theoretical work.
654"
BROADER IMPACTS,0.8781558726673985,"Guidelines:
655"
BROADER IMPACTS,0.8792535675082327,"• The answer NA means that there is no societal impact of the work performed.
656"
BROADER IMPACTS,0.8803512623490669,"• If the authors answer NA or No, they should explain why their work has no societal
657"
BROADER IMPACTS,0.8814489571899012,"impact or why the paper does not address societal impact.
658"
BROADER IMPACTS,0.8825466520307355,"• Examples of negative societal impacts include potential malicious or unintended uses
659"
BROADER IMPACTS,0.8836443468715697,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
660"
BROADER IMPACTS,0.884742041712404,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
661"
BROADER IMPACTS,0.8858397365532382,"groups), privacy considerations, and security considerations.
662"
BROADER IMPACTS,0.8869374313940724,"• The conference expects that many papers will be foundational research and not tied
663"
BROADER IMPACTS,0.8880351262349067,"to particular applications, let alone deployments. However, if there is a direct path to
664"
BROADER IMPACTS,0.889132821075741,"any negative applications, the authors should point it out. For example, it is legitimate
665"
BROADER IMPACTS,0.8902305159165752,"to point out that an improvement in the quality of generative models could be used to
666"
BROADER IMPACTS,0.8913282107574094,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
667"
BROADER IMPACTS,0.8924259055982436,"that a generic algorithm for optimizing neural networks could enable people to train
668"
BROADER IMPACTS,0.893523600439078,"models that generate Deepfakes faster.
669"
BROADER IMPACTS,0.8946212952799122,"• The authors should consider possible harms that could arise when the technology is
670"
BROADER IMPACTS,0.8957189901207464,"being used as intended and functioning correctly, harms that could arise when the
671"
BROADER IMPACTS,0.8968166849615807,"technology is being used as intended but gives incorrect results, and harms following
672"
BROADER IMPACTS,0.897914379802415,"from (intentional or unintentional) misuse of the technology.
673"
BROADER IMPACTS,0.8990120746432492,"• If there are negative societal impacts, the authors could also discuss possible mitigation
674"
BROADER IMPACTS,0.9001097694840834,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
675"
BROADER IMPACTS,0.9012074643249177,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
676"
BROADER IMPACTS,0.9023051591657519,"feedback over time, improving the efficiency and accessibility of ML).
677"
SAFEGUARDS,0.9034028540065862,"11. Safeguards
678"
SAFEGUARDS,0.9045005488474204,"Question: Does the paper describe safeguards that have been put in place for responsible
679"
SAFEGUARDS,0.9055982436882547,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
680"
SAFEGUARDS,0.9066959385290889,"image generators, or scraped datasets)?
681"
SAFEGUARDS,0.9077936333699231,"Answer: [NA]
682"
SAFEGUARDS,0.9088913282107574,"Justification: The paper is a theoretical work.
683"
SAFEGUARDS,0.9099890230515917,"Guidelines:
684"
SAFEGUARDS,0.9110867178924259,"• The answer NA means that the paper poses no such risks.
685"
SAFEGUARDS,0.9121844127332601,"• Released models that have a high risk for misuse or dual-use should be released with
686"
SAFEGUARDS,0.9132821075740944,"necessary safeguards to allow for controlled use of the model, for example by requiring
687"
SAFEGUARDS,0.9143798024149287,"that users adhere to usage guidelines or restrictions to access the model or implementing
688"
SAFEGUARDS,0.9154774972557629,"safety filters.
689"
SAFEGUARDS,0.9165751920965971,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
690"
SAFEGUARDS,0.9176728869374314,"should describe how they avoided releasing unsafe images.
691"
SAFEGUARDS,0.9187705817782656,"• We recognize that providing effective safeguards is challenging, and many papers do
692"
SAFEGUARDS,0.9198682766190999,"not require this, but we encourage authors to take this into account and make a best
693"
SAFEGUARDS,0.9209659714599341,"faith effort.
694"
LICENSES FOR EXISTING ASSETS,0.9220636663007684,"12. Licenses for existing assets
695"
LICENSES FOR EXISTING ASSETS,0.9231613611416026,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
696"
LICENSES FOR EXISTING ASSETS,0.9242590559824369,"the paper, properly credited and are the license and terms of use explicitly mentioned and
697"
LICENSES FOR EXISTING ASSETS,0.9253567508232712,"properly respected?
698"
LICENSES FOR EXISTING ASSETS,0.9264544456641054,"Answer: [Yes]
699"
LICENSES FOR EXISTING ASSETS,0.9275521405049396,"Justification: We used some datasets and properly credited the creators of assets (cf. ACS
700"
LICENSES FOR EXISTING ASSETS,0.9286498353457738,"2018 [24]).
701"
LICENSES FOR EXISTING ASSETS,0.9297475301866082,"Guidelines:
702"
LICENSES FOR EXISTING ASSETS,0.9308452250274424,"• The answer NA means that the paper does not use existing assets.
703"
LICENSES FOR EXISTING ASSETS,0.9319429198682766,"• The authors should cite the original paper that produced the code package or dataset.
704"
LICENSES FOR EXISTING ASSETS,0.9330406147091108,"• The authors should state which version of the asset is used and, if possible, include a
705"
LICENSES FOR EXISTING ASSETS,0.9341383095499451,"URL.
706"
LICENSES FOR EXISTING ASSETS,0.9352360043907794,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
707"
LICENSES FOR EXISTING ASSETS,0.9363336992316136,"• For scraped data from a particular source (e.g., website), the copyright and terms of
708"
LICENSES FOR EXISTING ASSETS,0.9374313940724479,"service of that source should be provided.
709"
LICENSES FOR EXISTING ASSETS,0.9385290889132821,"• If assets are released, the license, copyright information, and terms of use in the
710"
LICENSES FOR EXISTING ASSETS,0.9396267837541163,"package should be provided. For popular datasets, paperswithcode.com/datasets
711"
LICENSES FOR EXISTING ASSETS,0.9407244785949506,"has curated licenses for some datasets. Their licensing guide can help determine the
712"
LICENSES FOR EXISTING ASSETS,0.9418221734357849,"license of a dataset.
713"
LICENSES FOR EXISTING ASSETS,0.9429198682766191,"• For existing datasets that are re-packaged, both the original license and the license of
714"
LICENSES FOR EXISTING ASSETS,0.9440175631174533,"the derived asset (if it has changed) should be provided.
715"
LICENSES FOR EXISTING ASSETS,0.9451152579582875,"• If this information is not available online, the authors are encouraged to reach out to
716"
LICENSES FOR EXISTING ASSETS,0.9462129527991219,"the asset’s creators.
717"
NEW ASSETS,0.9473106476399561,"13. New Assets
718"
NEW ASSETS,0.9484083424807903,"Question: Are new assets introduced in the paper well documented and is the documentation
719"
NEW ASSETS,0.9495060373216246,"provided alongside the assets?
720"
NEW ASSETS,0.9506037321624589,"Answer: [Yes]
721"
NEW ASSETS,0.9517014270032931,"Justification: We provide the code with the details as a supplementary material.
722"
NEW ASSETS,0.9527991218441273,"Guidelines:
723"
NEW ASSETS,0.9538968166849616,"• The answer NA means that the paper does not release new assets.
724"
NEW ASSETS,0.9549945115257958,"• Researchers should communicate the details of the dataset/code/model as part of their
725"
NEW ASSETS,0.9560922063666301,"submissions via structured templates. This includes details about training, license,
726"
NEW ASSETS,0.9571899012074643,"limitations, etc.
727"
NEW ASSETS,0.9582875960482986,"• The paper should discuss whether and how consent was obtained from people whose
728"
NEW ASSETS,0.9593852908891328,"asset is used.
729"
NEW ASSETS,0.960482985729967,"• At submission time, remember to anonymize your assets (if applicable). You can either
730"
NEW ASSETS,0.9615806805708014,"create an anonymized URL or include an anonymized zip file.
731"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9626783754116356,"14. Crowdsourcing and Research with Human Subjects
732"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9637760702524698,"Question: For crowdsourcing experiments and research with human subjects, does the paper
733"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.964873765093304,"include the full text of instructions given to participants and screenshots, if applicable, as
734"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9659714599341384,"well as details about compensation (if any)?
735"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9670691547749726,"Answer: [NA]
736"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9681668496158068,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
737"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.969264544456641,"Guidelines:
738"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9703622392974753,"• The answer NA means that the paper does not involve crowdsourcing nor research with
739"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714599341383096,"human subjects.
740"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9725576289791438,"• Including this information in the supplemental material is fine, but if the main contribu-
741"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973655323819978,"tion of the paper involves human subjects, then as much detail as possible should be
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747530186608123,"included in the main paper.
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758507135016465,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769484083424808,"or other labor should be paid at least the minimum wage in the country of the data
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780461031833151,"collector.
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791437980241493,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802414928649835,"Subjects
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813391877058177,"Question: Does the paper describe potential risks incurred by study participants, whether
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824368825466521,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835345773874863,"approvals (or an equivalent approval/review based on the requirements of your country or
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846322722283205,"institution) were obtained?
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857299670691547,"Answer: [NA]
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986827661909989,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9879253567508233,"Guidelines:
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890230515916575,"• The answer NA means that the paper does not involve crowdsourcing nor research with
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901207464324918,"human subjects.
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991218441273326,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923161361141603,"may be required for any human subjects research. If you obtained IRB approval, you
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934138309549945,"should clearly state this in the paper.
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945115257958288,"• We recognize that the procedures for this may vary significantly between institutions
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995609220636663,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967069154774972,"guidelines for their institution.
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978046103183315,"• For initial submissions, do not include any information that would break anonymity (if
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989023051591658,"applicable), such as the institution conducting the review.
765"
