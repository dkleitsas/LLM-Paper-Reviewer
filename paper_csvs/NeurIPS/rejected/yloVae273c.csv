Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001941747572815534,"Ofﬂine Reinforcement Learning (RL) aims to learn a near-optimal policy from
1"
ABSTRACT,0.003883495145631068,"a ﬁxed dataset of transitions collected by another policy. This problem has at-
2"
ABSTRACT,0.005825242718446602,"tracted a lot of attention recently, but most existing methods with strong theoretical
3"
ABSTRACT,0.007766990291262136,"guarantees are restricted to ﬁnite-horizon or tabular settings. In constrast, few
4"
ABSTRACT,0.009708737864077669,"algorithms for inﬁnite-horizon settings with function approximation and minimal
5"
ABSTRACT,0.011650485436893204,"assumptions on the dataset are both sample and computationally efﬁcient. Another
6"
ABSTRACT,0.013592233009708738,"gap in the current literature is the lack of theoretical analysis for the average-reward
7"
ABSTRACT,0.015533980582524271,"setting, which is more challenging than the discounted setting. In this paper, we
8"
ABSTRACT,0.017475728155339806,"address both of these issues by proposing a primal-dual optimization method based
9"
ABSTRACT,0.019417475728155338,"on the linear programming formulation of RL. Our key contribution is a new
10"
ABSTRACT,0.021359223300970873,"reparametrization that allows us to derive low-variance gradient estimators that can
11"
ABSTRACT,0.02330097087378641,"be used in a stochastic optimization scheme using only samples from the behavior
12"
ABSTRACT,0.02524271844660194,"policy. Our method ﬁnds an ε-optimal policy with O(ε−4) samples, improving
13"
ABSTRACT,0.027184466019417475,"on the previous O(ε−5), while being computationally efﬁcient for inﬁnite-horizon
14"
ABSTRACT,0.02912621359223301,"discounted and average-reward MDPs with realizable linear function approxima-
15"
ABSTRACT,0.031067961165048542,"tion and partial coverage. Moreover, to the best of our knowledge, this is the ﬁrst
16"
ABSTRACT,0.03300970873786408,"theoretical result for average-reward ofﬂine RL.
17"
INTRODUCTION,0.03495145631067961,"1
Introduction
18"
INTRODUCTION,0.036893203883495145,"We study the setting of Ofﬂine Reinforcement Learning (RL), where the goal is to learn an ε-optimal
19"
INTRODUCTION,0.038834951456310676,"policy without being able to interact with the environment, but only using a ﬁxed dataset of transitions
20"
INTRODUCTION,0.040776699029126215,"collected by a behavior policy. Learning from ofﬂine data proves to be useful especially when
21"
INTRODUCTION,0.04271844660194175,"interacting with the environment can be costly or dangerous [16].
22"
INTRODUCTION,0.04466019417475728,"In this setting, the quality of the best policy learnable by any algorithm is constrained by the quality
23"
INTRODUCTION,0.04660194174757282,"of the data, implying that ﬁnding an optimal policy without further assumptions on the data is not
24"
INTRODUCTION,0.04854368932038835,"feasible. Therefore, many methods [23, 33] make a uniform coverage assumption, requiring that the
25"
INTRODUCTION,0.05048543689320388,"behavior policy explores sufﬁciently well the whole state-action space. However, recent work [17, 31]
26"
INTRODUCTION,0.05242718446601942,"demonstrated that partial coverage of the state-action space is sufﬁcient. In particular, this means that
27"
INTRODUCTION,0.05436893203883495,"the behavior policy needs only to sufﬁciently explore the state-actions visited by the optimal policy.
28"
INTRODUCTION,0.05631067961165048,"Moreover, like its online counterpart, modern ofﬂine RL faces the problem of learning efﬁciently in
29"
INTRODUCTION,0.05825242718446602,"environments with very large state spaces, where function approximation is necessary to compactly
30"
INTRODUCTION,0.06019417475728155,"represent policies and value functions. Although function approximation, especially with neural
31"
INTRODUCTION,0.062135922330097085,"networks, is widely used in practice, its theoretical understanding in the context of decision-making
32"
INTRODUCTION,0.06407766990291262,"is still rather limited, even when considering linear function approximation.
33"
INTRODUCTION,0.06601941747572816,"In fact, most existing sample complexity results for ofﬂine RL algorithms are limited either to the
34"
INTRODUCTION,0.06796116504854369,"tabular and ﬁnite horizon setting, by the uniform coverage assumption, or by lack of computational
35"
INTRODUCTION,0.06990291262135923,"efﬁciency — see the top section of Table 1 for a summary. Notable exceptions are the recent works of
36"
INTRODUCTION,0.07184466019417475,"Algorithm
Partial
Coverage"
INTRODUCTION,0.07378640776699029,"Polynomial
Sample
Complexity"
INTRODUCTION,0.07572815533980583,"Polynomial
Computational
Complexity"
INTRODUCTION,0.07766990291262135,"Function
Approximation
Inﬁnite Horizon"
INTRODUCTION,0.07961165048543689,"Discounted
Average-Reward"
INTRODUCTION,0.08155339805825243,"FQI [23]





"
INTRODUCTION,0.08349514563106795,"Rashidinejad et al. [31]





"
INTRODUCTION,0.0854368932038835,"Jin et al. [14]
Zanette et al. [38]





"
INTRODUCTION,0.08737864077669903,"Uehara & Sun [32]





"
INTRODUCTION,0.08932038834951456,"Cheng et al. [9]

O(ε−5)
superlinear


"
INTRODUCTION,0.0912621359223301,"Xie et al. [36]

O(ε−5)
O(n7/5)


"
INTRODUCTION,0.09320388349514563,"Ours

O(ε−4)
O(n)


"
INTRODUCTION,0.09514563106796116,"Table 1: Comparison of existing ofﬂine RL algorithms. The table is divided horizontally in two
sections. The upper section qualitatively compares algorithms for easier settings, that is, methods
for the tabular or ﬁnite-horizon settings or methods which require uniform coverage. The lower
section focuses on the setting considered in this paper, that is computationally efﬁcient methods for
the inﬁnite horizon setting with function approximation and partial coverage."
INTRODUCTION,0.0970873786407767,"Xie et al. [36] and Cheng et al. [9] who provide computationally efﬁcient methods for inﬁnite-horizon
37"
INTRODUCTION,0.09902912621359224,"discounted MDPs under realizable linear function approximation and partial coverage. Despite
38"
INTRODUCTION,0.10097087378640776,"being some of the ﬁrst implementable algorithms, their methods work only with discounted rewards,
39"
INTRODUCTION,0.1029126213592233,"have superlinear computational complexity and ﬁnd an ε-optimal policy with O(ε−5) samples – see
40"
INTRODUCTION,0.10485436893203884,"the bottom section of Table 1 for more details. Therefore, this work is motivated by the following
41"
INTRODUCTION,0.10679611650485436,"research question:
42"
INTRODUCTION,0.1087378640776699,"Can we design a linear-time algorithm with polynomial sample complexity for the discounted and
43"
INTRODUCTION,0.11067961165048544,"average-reward inﬁnite-horizon settings, in large state spaces under a partial-coverage assumption?
44 45"
INTRODUCTION,0.11262135922330097,"We answer this question positively by designing a method based on the linear-programming (LP)
46"
INTRODUCTION,0.1145631067961165,"formulation of sequential decision making [20]. Albeit less known than the dynamic-programming
47"
INTRODUCTION,0.11650485436893204,"formulation [3] that is ubiquitous in RL, it allows us to tackle this problem with the powerful tools
48"
INTRODUCTION,0.11844660194174757,"of convex optimization. We turn in particular to a relaxed version of the LP formulation [21, 2]
49"
INTRODUCTION,0.1203883495145631,"that considers action-value functions that are linear in known state-action features. This allows to
50"
INTRODUCTION,0.12233009708737864,"reduce the dimensionality of the problem from the cardinality of the state space to the number of
51"
INTRODUCTION,0.12427184466019417,"features. This relaxation still allows to recover optimal policies in linear MDPs [37, 13], a structural
52"
INTRODUCTION,0.1262135922330097,"assumption that is widely employed in the theoretical study of RL with linear function approximation.
53"
INTRODUCTION,0.12815533980582525,"Our algorithm for learning near-optimal policies from ofﬂine data is based on primal-dual optimization
54"
INTRODUCTION,0.13009708737864079,"of the Lagrangian of the relaxed LP. The use of saddle-point optimization in MDPs was ﬁrst proposed
55"
INTRODUCTION,0.13203883495145632,"by Wang & Chen [34] for planning in small state spaces, and was extended to linear function
56"
INTRODUCTION,0.13398058252427184,"approximation by Chen et al. [8], Bas-Serrano & Neu [1], and Neu & Okolo [26]. We largely take
57"
INTRODUCTION,0.13592233009708737,"inspiration from this latter work, which was the ﬁrst to apply saddle-point optimization to the relaxed
58"
INTRODUCTION,0.1378640776699029,"LP. However, primal-dual planning algorithms assume oracle access to a transition model, whose
59"
INTRODUCTION,0.13980582524271845,"samples are used to estimate gradients. In our ofﬂine setting, we only assume access to i.i.d. samples
60"
INTRODUCTION,0.141747572815534,"generated by a possibly unknown behavior policy. To adapt the primal-dual optimization strategy
61"
INTRODUCTION,0.1436893203883495,"to this setting we employ a change of variable, inspired by Nachum & Dai [24], which allows easy
62"
INTRODUCTION,0.14563106796116504,"computation of unbiased gradient estimates.
63"
INTRODUCTION,0.14757281553398058,"Notation.
We denote vectors with bold letters, such as x .= [x1, . . . , xd]⊤∈Rd, and use ei to
64"
INTRODUCTION,0.14951456310679612,"denote the i-th standard basis vector. We interchangeably denote functions f : X →R over a ﬁnite
65"
INTRODUCTION,0.15145631067961166,"set X, as vectors f ∈R|X| with components f(x), and use ≥to denote element-wise comparison. We
66"
INTRODUCTION,0.1533980582524272,"denote the set of probability distributions over a measurable set S as ∆S, and the probability simplex
67"
INTRODUCTION,0.1553398058252427,"in Rd as ∆d. We use σ : Rd →∆d to denote the softmax function deﬁned as σi(x) .= exi/ Pd
j=1 exj.
68"
INTRODUCTION,0.15728155339805824,"We use upper-case letters for random variables, such as S, and denote the uniform distribution over a
69"
INTRODUCTION,0.15922330097087378,"ﬁnite set of n elements as U(n). In the context of iterative algorithms, we use Ft−1 to denote the
70"
INTRODUCTION,0.16116504854368932,"sigma-algebra generated by all events up to the end of iteration t −1, and use the shorthand notation
71"
INTRODUCTION,0.16310679611650486,"Et [·] = E [·| Ft−1] to denote expectation conditional on the history. For nested-loop algorithms, we
72"
INTRODUCTION,0.1650485436893204,"write Ft,i−1 for the sigma-algebra generated by all events up to the end of iteration i −1 of round t,
73"
INTRODUCTION,0.1669902912621359,"and Et,i [·] = E [·| Ft,i−1] for the corresponding conditional expectation.
74"
PRELIMINARIES,0.16893203883495145,"2
Preliminaries
75"
PRELIMINARIES,0.170873786407767,"We study discounted Markov decision processes [MDP, 29] denoted as (X, A, p, r, γ), with discount
76"
PRELIMINARIES,0.17281553398058253,"factor γ ∈[0, 1] and ﬁnite, but potentially very large, state space X and action space A. For
77"
PRELIMINARIES,0.17475728155339806,"every state-action pair (x, a), we denote as p(· | x, a) ∈∆X the next-state distribution, and as
78"
PRELIMINARIES,0.1766990291262136,"r(x, a) ∈[0, 1] the reward, which is assumed to be deterministic and bounded for simplicity. The
79"
PRELIMINARIES,0.1786407766990291,"transition function p is also denoted as the matrix P ∈R|X×A|×|X| and the reward as the vector
80"
PRELIMINARIES,0.18058252427184465,"r ∈R|X×A|. The objective is to ﬁnd an optimal policy π∗: X →∆A. That is, a stationary
81"
PRELIMINARIES,0.1825242718446602,"policy that maximizes the normalized expected return ρ(π∗) .= (1 −γ)Eπ∗[P∞
t=0 r(Xt, At)], where
82"
PRELIMINARIES,0.18446601941747573,"the initial state X0 is sampled from the initial state distribution ν0, the other states according to
83"
PRELIMINARIES,0.18640776699029127,"Xt+1 ∼p(·|Xt, At) and where the notation Eπ[·] is used to denote that the actions are sampled
84"
PRELIMINARIES,0.1883495145631068,"from policy π as At ∼π(·|Xt). Moreover, we deﬁne the following quantities for each policy π: its
85"
PRELIMINARIES,0.19029126213592232,"state-action value function qπ(x, a) .= Eπ[P∞
t=0 γtr(Xt, At) | X0 = x, A0 = a], its value function
86"
PRELIMINARIES,0.19223300970873786,"vπ(x) .= Eπ[qπ(x, A0)], its state occupancy measure νπ(x) .= (1 −γ)Eπ[P∞
t=0 1{Xt = x}], and
87"
PRELIMINARIES,0.1941747572815534,"its state-action occupancy measure µπ(x, a) .= π(a|x)νπ(x). These quantities are known to satisify
88"
PRELIMINARIES,0.19611650485436893,"the following useful relations, more commonly known respectively as Bellman’s equation and ﬂow
89"
PRELIMINARIES,0.19805825242718447,"constraint for policy π [4]:
90"
PRELIMINARIES,0.2,"qπ = r + γP vπ
νπ = (1 −γ)ν0 + γP
Tµπ
(1)"
PRELIMINARIES,0.20194174757281552,"Given this notation, we can also rewrite the normalized expected return in vector form as ρ(π) =
91"
PRELIMINARIES,0.20388349514563106,"(1 −γ)⟨ν0, vπ⟩or equivalently as ρ(π) = ⟨r, µπ⟩.
92"
PRELIMINARIES,0.2058252427184466,"Our work is based on the linear programming formulation due to Manne [19] (see also 29) which
93"
PRELIMINARIES,0.20776699029126214,"transforms the reinforcement learning problem into the search for an optimal state-action occupancy
94"
PRELIMINARIES,0.20970873786407768,"measure, obtained by solving the following Linear Program (LP):
95"
PRELIMINARIES,0.21165048543689322,"maximize
⟨r, µ⟩
subject to
E
Tµ = (1 −γ)ν0 + γP
Tµ
µ ≥0
(2)"
PRELIMINARIES,0.21359223300970873,"where E ∈R|X×A|×|X| denotes the matrix with components E(x,a),x′ .= 1{x = x′}. The constraints
96"
PRELIMINARIES,0.21553398058252426,"of this LP are known to characterize the set of valid state-action occupancy measures. Therefore,
97"
PRELIMINARIES,0.2174757281553398,"an optimal solution µ∗of the LP corresponds to the state-action occupancy measure associated to a
98"
PRELIMINARIES,0.21941747572815534,"policy π∗maximizing the expected return, and which is therefore optimal in the MDP. This policy
99"
PRELIMINARIES,0.22135922330097088,"can be extracted as π∗(a|x) .= µ∗(x, a)/ P"
PRELIMINARIES,0.22330097087378642,"¯a∈A µ∗(x, ¯a). However, this linear program cannot be
100"
PRELIMINARIES,0.22524271844660193,"directly solved in an efﬁcient way in large MDPs due to the number of constraints and dimensions
101"
PRELIMINARIES,0.22718446601941747,"of the variables scaling with the size of the state space X. Therefore, taking inspiration from the
102"
PRELIMINARIES,0.229126213592233,"previous works of Bas-Serrano et al. [2], Neu & Okolo [26] we assume the knowledge of a feature
103"
PRELIMINARIES,0.23106796116504855,"map ϕ, which we then use to reduce the dimension of the problem. More speciﬁcally we consider the
104"
PRELIMINARIES,0.23300970873786409,"setting of Linear MDPs [13, 37].
105"
PRELIMINARIES,0.23495145631067962,"Deﬁnition 2.1 (Linear MDP). An MDP is called linear if both the transition and reward functions
106"
PRELIMINARIES,0.23689320388349513,"can be expressed as a linear function of a given feature map ϕ : X × A →Rd. That is, there exist
107"
PRELIMINARIES,0.23883495145631067,"ψ : X →Rd and ω ∈Rd such that, for every x, x′ ∈X and a ∈A:
108"
PRELIMINARIES,0.2407766990291262,"r(x, a) = ⟨ϕ(x, a), ω⟩,
p(x′ | x, a) = ⟨ϕ(x, a), ψ(x′)⟩."
PRELIMINARIES,0.24271844660194175,"We assume that for all x, a, the norms of all relevant vectors are bounded by known constants as
109"
PRELIMINARIES,0.2446601941747573,"∥ϕ(x, a)∥2 ≤Dϕ, ∥P"
PRELIMINARIES,0.24660194174757283,"x′ ψ(x′)∥2 ≤Dψ, and ∥ω∥2 ≤Dω. Moreover, we represent the feature map
110"
PRELIMINARIES,0.24854368932038834,"with the matrix Φ ∈R|X×A|×d with rows given by ϕ(x, a)T, and similarly we deﬁne Ψ ∈Rd×|X|
111"
PRELIMINARIES,0.2504854368932039,"as the matrix with columns given by ψ(x).
112"
PRELIMINARIES,0.2524271844660194,"With this notation we can rewrite the transition matrix as P = ΦΨ. Furthermore, it is convenient
113"
PRELIMINARIES,0.2543689320388349,"to assume that the dimension d of the feature map cannot be trivially reduced, and therefore that
114"
PRELIMINARIES,0.2563106796116505,"the matrix Φ is full-rank. An easily veriﬁable consequence of the Linear MDP assumption is that
115"
PRELIMINARIES,0.258252427184466,"state-action value functions can be represented as a linear combinations of ϕ. That is, there exist
116"
PRELIMINARIES,0.26019417475728157,"θπ ∈Rd such that:
117"
PRELIMINARIES,0.2621359223300971,"qπ = r + γP vπ = Φ(ω + Ψvπ) = Φθπ.
(3)"
PRELIMINARIES,0.26407766990291265,"It can be shown that for all policies π, the norm of θπ is at most Dθ = Dω + Dψ"
PRELIMINARIES,0.26601941747572816,"1−γ (cf. Lemma B.1
118"
PRELIMINARIES,0.26796116504854367,"in 13). We then translate the linear program (2) to our setting, with the addition of the new variable
119"
PRELIMINARIES,0.26990291262135924,"λ ∈Rd, resulting in the following new LP and its corresponding dual:
120"
PRELIMINARIES,0.27184466019417475,"maximize
⟨ω, λ⟩
subject to
E
Tµ = (1 −γ)ν0 + γΨ
Tλ
λ = Φ
Tµ
µ ≥0. (4)"
PRELIMINARIES,0.2737864077669903,"minimize
(1 −γ)⟨ν0, v⟩
subject to
θ = ω + γΨv
Ev ≥Φθ
(5)"
PRELIMINARIES,0.2757281553398058,"It can be immediately noticed how the introduction of λ did not change neither the set of admissible
121"
PRELIMINARIES,0.27766990291262134,"µs nor the objective, and therefore did not alter the optimal solution. The Lagrangian associated to
122"
PRELIMINARIES,0.2796116504854369,"this set of linear programs is the function:
123"
PRELIMINARIES,0.2815533980582524,"L(v, θ, λ, µ) = (1 −γ)⟨ν0, v⟩+ ⟨λ, ω + γΨv −θ⟩+ ⟨µ, Φθ −Ev⟩
= ⟨λ, ω⟩+ ⟨v, (1 −γ)ν0 + γΨ
Tλ −E
Tµ⟩+ ⟨θ, Φ
Tµ −λ⟩.
(6)"
PRELIMINARIES,0.283495145631068,"It is known that ﬁnding optimal solutions (λ⋆, µ⋆) and (v⋆, θ⋆) for the primal and dual LPs is
124"
PRELIMINARIES,0.2854368932038835,"equivalent to ﬁnding a saddle point (v⋆, θ⋆, λ⋆, µ⋆) of the Lagrangian function [5]. In the next
125"
PRELIMINARIES,0.287378640776699,"section, we will develop primal-dual methods that aim to ﬁnd approximate solutions to the above
126"
PRELIMINARIES,0.28932038834951457,"saddle-point problem, and convert these solutions to policies with near-optimality guarantees.
127"
ALGORITHM AND MAIN RESULTS,0.2912621359223301,"3
Algorithm and Main Results
128"
ALGORITHM AND MAIN RESULTS,0.29320388349514565,"This section introduces the concrete setting we study in this paper, and presents our main contributions.
129"
ALGORITHM AND MAIN RESULTS,0.29514563106796116,"We consider the ofﬂine-learning scenario where the agent has access to a dataset D = (Wt)n
t=1,
130"
ALGORITHM AND MAIN RESULTS,0.2970873786407767,"collected by a behavior policy πB, and composed of n random observations of the form Wt =
131"
ALGORITHM AND MAIN RESULTS,0.29902912621359223,"(X0
t , Xt, At, Rt, X′
t). The random variables X0
t , (Xt, At) and X′
t are sampled, respectively, from
132"
ALGORITHM AND MAIN RESULTS,0.30097087378640774,"the initial-state distribution ν0, the discounted occupancy measure of the behavior policy, denoted as
133"
ALGORITHM AND MAIN RESULTS,0.3029126213592233,"µB, and from p(·|Xt, At). Finally, Rt denotes the reward r(Xt, At). We assume that all observations
134"
ALGORITHM AND MAIN RESULTS,0.3048543689320388,"Wt are generated independently of each other, and will often use the notation ϕt = ϕ(Xt, At).
135"
ALGORITHM AND MAIN RESULTS,0.3067961165048544,"Our strategy consists in ﬁnding approximately good solutions for the LPs (4) and (5) using stochastic
136"
ALGORITHM AND MAIN RESULTS,0.3087378640776699,"optimization methods, which require access to unbiased gradient estimates of the Lagrangian (Equa-
137"
ALGORITHM AND MAIN RESULTS,0.3106796116504854,"tion 6). The main challenge we need to overcome is constructing suitable estimators based only on
138"
ALGORITHM AND MAIN RESULTS,0.312621359223301,"observations drawn from the behavior policy. We address this challenge by introducing the matrix
139"
ALGORITHM AND MAIN RESULTS,0.3145631067961165,"Λ = EX,A∼µB [ϕ(X, A)ϕ(X, A)T] (supposed to be invertible for the sake of argument for now),
140"
ALGORITHM AND MAIN RESULTS,0.31650485436893205,"and rewriting the gradient with respect to λ as
141"
ALGORITHM AND MAIN RESULTS,0.31844660194174756,"∇λL(λ, µ; v, θ) = ω + γΨv −θ = Λ−1Λ (ω + γΨv −θ)"
ALGORITHM AND MAIN RESULTS,0.32038834951456313,"= Λ−1E [ϕ(Xt, At)ϕ(Xt, At)
T (ω + γΨv −θ)]"
ALGORITHM AND MAIN RESULTS,0.32233009708737864,"= Λ−1E [ϕ(Xt, At) (Rt + γv(X′
t) −⟨θ, ϕ(Xt, At)⟩)] ."
ALGORITHM AND MAIN RESULTS,0.32427184466019415,"This suggests that the vector within the expectation can be used to build an unbiased estimator of the
142"
ALGORITHM AND MAIN RESULTS,0.3262135922330097,"desired gradient. A downside of using this estimator is that it requires knowledge of Λ. However,
143"
ALGORITHM AND MAIN RESULTS,0.32815533980582523,"this can be sidestepped by a reparametrization trick inspired by Nachum & Dai [24]: introducing the
144"
ALGORITHM AND MAIN RESULTS,0.3300970873786408,"parametrization β = Λ−1λ, the objective can be rewritten as
145"
ALGORITHM AND MAIN RESULTS,0.3320388349514563,"L(β, µ; v, θ) = (1 −γ)⟨ν0, v⟩+ ⟨β, Λ
 
ω + γΨv −θ

⟩+ ⟨µ, Φθ −Ev⟩."
ALGORITHM AND MAIN RESULTS,0.3339805825242718,"This can be indeed seen to generalize the tabular reparametrization of Nachum & Dai [24] to the case
146"
ALGORITHM AND MAIN RESULTS,0.3359223300970874,"of linear function approximation. Notably, our linear reparametrization does not change the structure
147"
ALGORITHM AND MAIN RESULTS,0.3378640776699029,"of the saddle-point problem, but allows building an unbiased estimator of ∇βL(β, µ; v, θ) without
148"
ALGORITHM AND MAIN RESULTS,0.33980582524271846,"knowledge of Λ as
149"
ALGORITHM AND MAIN RESULTS,0.341747572815534,"˜gβ = ϕ(Xt, At) (Rt + γv(X′
t) −⟨θ, ϕ(Xt, At)⟩) ."
ALGORITHM AND MAIN RESULTS,0.34368932038834954,"In what follows, we will use the more general parametrization β = Λ−cλ, with c ∈{1/2, 1}, and
150"
ALGORITHM AND MAIN RESULTS,0.34563106796116505,"construct a primal-dual stochastic optimization method that can be implemented efﬁciently in the
151"
ALGORITHM AND MAIN RESULTS,0.34757281553398056,"ofﬂine setting based on the observations above. Using c = 1 allows to run our algorithm without
152"
ALGORITHM AND MAIN RESULTS,0.34951456310679613,"knowledge of Λ, that is, without knowing the behavior policy that generated the dataset, while using
153"
ALGORITHM AND MAIN RESULTS,0.35145631067961164,"c = 1/2 results in a tighter bound, at the price of having to assume knowledge of Λ.
154"
ALGORITHM AND MAIN RESULTS,0.3533980582524272,"Our algorithm (presented as Algorithm 1) is inspired by the method of Neu & Okolo [26], originally
155"
ALGORITHM AND MAIN RESULTS,0.3553398058252427,"designed for planning with a generative model. The algorithm has a double-loop structure, where
156"
ALGORITHM AND MAIN RESULTS,0.3572815533980582,Algorithm 1 Ofﬂine Primal-Dual RL
ALGORITHM AND MAIN RESULTS,0.3592233009708738,"Input: Learning rates α, ζ, η, initial points θ0 ∈B(Dθ), β1 ∈B(Dβ), π1, and data D = (Wt)n
t=1
for t = 1 to T do"
ALGORITHM AND MAIN RESULTS,0.3611650485436893,"Initialize θt,1 = θt−1
for k = 1 to K −1 do"
ALGORITHM AND MAIN RESULTS,0.36310679611650487,"Obtain sample Wt,k = (X0
t,k, Xt,k, At,k, X′
t,k)
µt,k = πt ◦

(1 −γ)eX0
t,k + γ⟨ϕ(Xt,k, At,k), Λc−1βt⟩eX′
t,k
"
ALGORITHM AND MAIN RESULTS,0.3650485436893204,"˜gθ,t,i = ΦTµt,k −Λc−1ϕ(Xt,k, At,k)⟨ϕ(Xt,k, At,k), βt⟩
θt,k+1 = ΠB(Dθ)(θt,k −η˜gθ,t,i)
// Stochastic gradient descent
end for
θt = 1"
ALGORITHM AND MAIN RESULTS,0.36699029126213595,"K
PK
k=1 θt,k"
ALGORITHM AND MAIN RESULTS,0.36893203883495146,"Obtain sample Wt = (X0
t , Xt, At, X′
t)
vt = ET 
πt ◦Φθt
"
ALGORITHM AND MAIN RESULTS,0.37087378640776697,"˜gβ,t = ϕ(Xt, A)
 
Rt + γvt(X′
t) −⟨ϕ(Xt, At), θt⟩
"
ALGORITHM AND MAIN RESULTS,0.37281553398058254,"βt+1 = ΠB(Dβ)(βt + ζ˜gβ,t)
// Stochastic gradient ascent"
ALGORITHM AND MAIN RESULTS,0.37475728155339805,"πt+1 = σ(α Pt
i=1 Φθi)
// Policy update
end for
return πJ with J ∼U(T)."
ALGORITHM AND MAIN RESULTS,0.3766990291262136,"at each iteration t we run one step of stochastic gradient ascent for β, and also an inner loop
157"
ALGORITHM AND MAIN RESULTS,0.3786407766990291,"which runs K iterations of stochastic gradient descent on θ making sure that ⟨ϕ(x, a), θt⟩is a
158"
ALGORITHM AND MAIN RESULTS,0.38058252427184464,"good approximation of the true action-value function of πt. Iterations of the inner loop are indexed
159"
ALGORITHM AND MAIN RESULTS,0.3825242718446602,"by k. The main idea of the algorithm is to compute the unbiased estimators ˜gθ,t,k and ˜gβ,t of
160"
ALGORITHM AND MAIN RESULTS,0.3844660194174757,"the gradients ∇θL(βt, µt; ·, θt,k) and ∇βL(βt, ·; vt, θt), and use them to update the respective
161"
ALGORITHM AND MAIN RESULTS,0.3864077669902913,"variables iteratively. We then deﬁne a softmax policy πt at each iteration t using the θ parameters as
162"
ALGORITHM AND MAIN RESULTS,0.3883495145631068,"πt(a|x) = σ

α Pt−1
i=1⟨ϕ(x, a), θi⟩

. The other higher-dimensional variables (µt, vt) are deﬁned
163"
ALGORITHM AND MAIN RESULTS,0.39029126213592236,"symbolically in terms of βt, θt and πt, and used only as auxiliary variables for computing the
164"
ALGORITHM AND MAIN RESULTS,0.39223300970873787,"estimates ˜gθ,t,k and ˜gβ,t. Speciﬁcally, we set these variables as
165"
ALGORITHM AND MAIN RESULTS,0.3941747572815534,"vt(x) =
X"
ALGORITHM AND MAIN RESULTS,0.39611650485436894,"a
πt(a|x)⟨ϕ(x, a), θt⟩,
(7)"
ALGORITHM AND MAIN RESULTS,0.39805825242718446,"µt,k(x, a) = πt(a|x)
 
(1 −γ)1{X0
t,k = x} + γ⟨ϕt,k, Λc−1βt⟩1{X′
t,k = x}

.
(8)"
ALGORITHM AND MAIN RESULTS,0.4,"Finally, the gradient estimates can be deﬁned as
166"
ALGORITHM AND MAIN RESULTS,0.40194174757281553,"˜gβ,t = Λc−1ϕt (Rt + γvt(X′
t) −⟨ϕt, θt⟩) ,
(9)"
ALGORITHM AND MAIN RESULTS,0.40388349514563104,"˜gθ,t,k = Φ
Tµt,k −Λc−1ϕt,k⟨ϕt,k, βt⟩.
(10)"
ALGORITHM AND MAIN RESULTS,0.4058252427184466,"These gradient estimates are then used in a projected gradient ascent/descent scheme, with the ℓ2
167"
ALGORITHM AND MAIN RESULTS,0.4077669902912621,"projection operator denoted by Π. The feasible sets of the two parameter vectors are chosen as ℓ2
168"
ALGORITHM AND MAIN RESULTS,0.4097087378640777,"balls of radii Dθ and Dβ, denoted respectively as B(Dθ) and B(Dβ). Notably, the algorithm does not
169"
ALGORITHM AND MAIN RESULTS,0.4116504854368932,"need to compute vt(x), µt,k(x, a), or πt(a|x) for all states x, but only for the states that are accessed
170"
ALGORITHM AND MAIN RESULTS,0.41359223300970877,"during the execution of the method. In particular, πt does not need to be computed explicitly, and it
171"
ALGORITHM AND MAIN RESULTS,0.4155339805825243,"can be efﬁciently represented by the single d-dimensional parameter vector Pt
i=1 θi.
172"
ALGORITHM AND MAIN RESULTS,0.4174757281553398,"Due to the double-loop structure, each iteration t uses K samples from the dataset D, adding up to
173"
ALGORITHM AND MAIN RESULTS,0.41941747572815535,"a total of n = KT samples over the course of T iterations. Each gradient update calculated by the
174"
ALGORITHM AND MAIN RESULTS,0.42135922330097086,"method uses a constant number of elementary vector operations, resulting in a total computational
175"
ALGORITHM AND MAIN RESULTS,0.42330097087378643,"complexity of O(|A|dn) elementary operations. At the end, our algorithm outputs a policy selected
176"
ALGORITHM AND MAIN RESULTS,0.42524271844660194,"uniformly at random from the T iterations.
177"
MAIN RESULT,0.42718446601941745,"3.1
Main result
178"
MAIN RESULT,0.429126213592233,"We are now almost ready to state our main result. Before doing so, we ﬁrst need to discuss the
179"
MAIN RESULT,0.43106796116504853,"quantities appearing in the guarantee, and provide an intuitive explanation for them.
180"
MAIN RESULT,0.4330097087378641,"Similarly to previous work, we capture the partial coverage assumption by expressing the rate of
181"
MAIN RESULT,0.4349514563106796,"convergence to the optimal policy in terms of a coverage ratio that measures the mismatch between
182"
MAIN RESULT,0.4368932038834951,"the behavior and the optimal policy. Several deﬁnitions of coverage ratio are surveyed by Uehara &
183"
MAIN RESULT,0.4388349514563107,"Sun [32]. In this work, we employ a notion of feature coverage ratio for linear MDPs that deﬁnes
184"
MAIN RESULT,0.4407766990291262,"coverage in feature space rather than in state-action space, similarly to Jin et al. [14], but with a
185"
MAIN RESULT,0.44271844660194176,"smaller ratio.
186"
MAIN RESULT,0.4446601941747573,"Deﬁnition 3.1. Let c ∈{1/2, 1}. We deﬁne the generalized coverage ratio as
187"
MAIN RESULT,0.44660194174757284,"Cϕ,c(π∗; πB) = E(X∗,A∗)∼µπ∗[ϕ(X∗, A∗)]⊤Λ−2cE[ϕ(X∗, A∗)]."
MAIN RESULT,0.44854368932038835,"We defer a detailed discussion of this ratio to Section 6, where we compare it with similar notions in
188"
MAIN RESULT,0.45048543689320386,"the literature. We are now ready to state our main result.
189"
MAIN RESULT,0.4524271844660194,"Theorem 3.2. Given a linear MDP (Deﬁnition 2.1) such that θπ ∈B(Dθ) for any policy π. Assume
190"
MAIN RESULT,0.45436893203883494,"that the coverage ratio is bounded Cϕ,c(π∗; πB) ≤Dβ. Then, for any comparator policy π∗, the
191"
MAIN RESULT,0.4563106796116505,"policy output by an appropriately tuned instance of Algorithm 1 satisﬁes E

⟨µπ∗−µπout, r⟩

≤ε
192"
MAIN RESULT,0.458252427184466,"with a number of samples nϵ that is O

ε−4D4
θD8c
ϕ D4
βd2−2c log |A|

.
193"
MAIN RESULT,0.4601941747572815,"The concrete parameter choices are detailed in the full version of the theorem in Appendix A. The
194"
MAIN RESULT,0.4621359223300971,"main theorem can be simpliﬁed by making some standard assumptions, formalized by the following
195"
MAIN RESULT,0.4640776699029126,"corollary.
196"
MAIN RESULT,0.46601941747572817,"Corollary 3.3. Assume that the bound of the feature vectors Dϕ is of order O(1), that Dω = Dψ =
197
√"
MAIN RESULT,0.4679611650485437,"d and that Dβ = c · Cϕ,c(π∗; πB) for some positive universal constant c. Then, under the same
198"
MAIN RESULT,0.46990291262135925,"assumptions of Theorem 3.2, nε is of order O

d4Cϕ,c(π∗;πB)2 log |A|"
MAIN RESULT,0.47184466019417476,"d2c(1−γ)4ε4

.
199"
ANALYSIS,0.47378640776699027,"4
Analysis
200"
ANALYSIS,0.47572815533980584,"This section explains the rationale behind some of the technical choices of our algorithm, and sketches
201"
ANALYSIS,0.47766990291262135,"the proof of our main result.
202"
ANALYSIS,0.4796116504854369,"First, we explicitly rewrite the expression of the Lagrangian (6), after performing the change of
203"
ANALYSIS,0.4815533980582524,"variable λ = Λcβ:
204"
ANALYSIS,0.48349514563106794,"L(β, µ; v, θ) = (1 −γ)⟨ν0, v⟩+ ⟨β, Λc 
ω + γΨv −θ

⟩+ ⟨µ, Φθ −Ev⟩
(11)"
ANALYSIS,0.4854368932038835,"= ⟨β, Λcω⟩+ ⟨v, (1 −γ)ν0 + γΨ
TΛcβ −E
Tµ⟩+ ⟨θ, Φ
Tµ −Λcβ⟩.
(12)"
ANALYSIS,0.487378640776699,"We aim to ﬁnd an approximate saddle-point of the above convex-concave objective function. One
205"
ANALYSIS,0.4893203883495146,"challenge that we need to face is that the variables v and µ have dimension proportional to the size of
206"
ANALYSIS,0.4912621359223301,"the state space |X|, so making explicit updates to these parameters would be prohibitively expensive
207"
ANALYSIS,0.49320388349514566,"in MDPs with large state spaces. To address this challenge, we choose to parametrize µ in terms of a
208"
ANALYSIS,0.49514563106796117,"policy π and β through the symbolic assignment µ = µβ,π, where
209"
ANALYSIS,0.4970873786407767,"µβ,π(x, a) .= π(a|x)
h
(1 −γ)ν0(x) + γ⟨ψ(x), Λcβ⟩
i
.
(13)"
ANALYSIS,0.49902912621359224,"This choice can be seen to satisfy the ﬁrst constraint of the primal LP (4), and thus the gradient of the
210"
ANALYSIS,0.5009708737864078,"Lagrangian (12) evaluated at µβ,π with respect to v can be veriﬁed to be 0. This parametrization
211"
ANALYSIS,0.5029126213592233,"makes it possible to express the Lagrangian as a function of only θ, β and π as
212"
ANALYSIS,0.5048543689320388,"f(θ, β, π) .= L(β, µβ,π; v, θ) = ⟨β, Λcω⟩+ ⟨θ, Φ
Tµβ,π −Λcβ⟩.
(14)"
ANALYSIS,0.5067961165048543,"For convenience, we also deﬁne the quantities νβ = ETµβ,π and vθ,π(s) .= P"
ANALYSIS,0.5087378640776699,"a π(a|s) ⟨θ, ϕ(x, a)⟩,
213"
ANALYSIS,0.5106796116504855,"which enables us to rewrite f as
214"
ANALYSIS,0.512621359223301,"f(θ, β, π) = ⟨Λcβ, ω −θ⟩+ ⟨vθ,π, νβ⟩= (1 −γ)⟨ν0, vθ,π⟩+ ⟨Λcβ, ω + γΨvθ,π −θ⟩. (15)"
ANALYSIS,0.5145631067961165,"The above choices allow us to perform stochastic gradient / ascent over the low-dimensional parame-
215"
ANALYSIS,0.516504854368932,"ters θ and β and the policy π. In order to calculate an unbiased estimator of the gradients, we ﬁrst
216"
ANALYSIS,0.5184466019417475,"observe that the choice of µt,k in Algorithm 1 is an unbiased estimator of µβt,πt:
217"
ANALYSIS,0.5203883495145631,"Et,k [µt,k(x, a)] = πt(a|x)

(1 −γ)P(X0
t,k = x) + Et,k

1{X′
t,k = x}⟨ϕt, Λc−1βt⟩
"
ANALYSIS,0.5223300970873787,"= πt(a|x)

(1 −γ)ν0(x) + γ
X"
ANALYSIS,0.5242718446601942,"¯x,¯a
µB(¯x, ¯a)p(x|¯x, ¯a)ϕ(¯x, ¯a)
TΛc−1βt
"
ANALYSIS,0.5262135922330097,"= πt(a|x)

(1 −γ)ν0(x) + γψ(x)
TΛΛc−1βt

= µβt,πt(x, a),"
ANALYSIS,0.5281553398058253,"where we used the fact that p(x|¯x, ¯a) = ⟨ψ(x), ϕ(¯x, ¯a)⟩, and the deﬁnition of Λ. This in turn
218"
ANALYSIS,0.5300970873786408,"facilitates proving that the gradient estimate ˜gθ,t,k, deﬁned in Equation 10, is indeed unbiased:
219"
ANALYSIS,0.5320388349514563,"Et,k [˜gθ,t,k] = Φ
TEt,k [µt,k] −Λc−1Et,k

ϕt,kϕ
T
t,k

βt = Φ
Tµβt,πt −Λcβt = ∇θL(βt, µt; vt, ·)."
ANALYSIS,0.5339805825242718,"A similar proof is used for ˜gβ,t and is detailed in Appendix B.3.
220"
ANALYSIS,0.5359223300970873,"Our analysis is based on arguments by Neu & Okolo [26], carefully adapted to the reparametrized
221"
ANALYSIS,0.537864077669903,"version of the Lagrangian presented above. The proof studies the following central quantity that we
222"
ANALYSIS,0.5398058252427185,"refer to as dynamic duality gap:
223"
ANALYSIS,0.541747572815534,"GT (β∗, π∗; θ∗
1:T ) .= 1 T T
X"
ANALYSIS,0.5436893203883495,"t=1
(f(β∗, π∗; θt) −f(βt, πt; θ∗
t )).
(16)"
ANALYSIS,0.545631067961165,"Here, (θt, βt, πt) are the iterates of the algorithm, θ∗
1:T = (θ∗
t )T
t=1 a sequence of comparators for θ,
224"
ANALYSIS,0.5475728155339806,"and ﬁnally β∗and π∗are ﬁxed comparators for β and π, respectively. Our ﬁrst key lemma relates
225"
ANALYSIS,0.5495145631067961,"the suboptimality of the output policy to GT for a speciﬁc choice of comparators.
226"
ANALYSIS,0.5514563106796116,"Lemma 4.1. Let θ∗
t
.= θπt, π∗be any policy, and β∗= Λ−cΦ⊤µπ∗. Then, E

⟨µπ∗−µπout, r⟩

=
227"
ANALYSIS,0.5533980582524272,"GT
 
β∗, π∗; θ∗
1:T

.
228"
ANALYSIS,0.5553398058252427,"The proof is relegated to Appendix B.1. Our second key lemma rewrites the gap GT for any choice of
229"
ANALYSIS,0.5572815533980583,"comparators as the sum of three regret terms:
230"
ANALYSIS,0.5592233009708738,"Lemma 4.2. With the choice of comparators of Lemma 4.1
231"
ANALYSIS,0.5611650485436893,"GT (β∗, π∗; θ∗
1:T ) = 1 T T
X"
ANALYSIS,0.5631067961165048,"t=1
⟨θt −θ∗
t , gθ,t⟩+ 1 T T
X"
ANALYSIS,0.5650485436893203,"t=1
⟨β∗−βt, gβ,t⟩ + 1 T T
X t=1 X"
ANALYSIS,0.566990291262136,"s
νπ∗(s)
X"
ANALYSIS,0.5689320388349515,"a
(π∗(a|s) −πt(a|s))⟨θt, ϕ(x, a)⟩,"
ANALYSIS,0.570873786407767,"where gθ,t = Φ⊤µβt,πt −Λcβt and gβ,t = Λc(ω + γΨvθt,πt −θt).
232"
ANALYSIS,0.5728155339805825,"The proof is presented in Appendix B.2. To conclude the proof we bound the three terms appearing
233"
ANALYSIS,0.574757281553398,"in Lemma 4.2. The ﬁrst two of those are bounded using standard gradient descent/ascent analysis
234"
ANALYSIS,0.5766990291262136,"(Lemmas B.1 and B.2), while for the latter we use mirror descent analysis (Lemma B.3). The details
235"
ANALYSIS,0.5786407766990291,"of these steps are reported in Appendix B.3.
236"
EXTENSION TO AVERAGE-REWARD MDPS,0.5805825242718446,"5
Extension to Average-Reward MDPs
237"
EXTENSION TO AVERAGE-REWARD MDPS,0.5825242718446602,"In this section, we brieﬂy explain how to extend our approach to ofﬂine learning in average reward
238"
EXTENSION TO AVERAGE-REWARD MDPS,0.5844660194174758,"MDPs, establishing the ﬁrst sample complexity result for this setting. After introducing the setup, we
239"
EXTENSION TO AVERAGE-REWARD MDPS,0.5864077669902913,"outline a remarkably simple adaptation of our algorithm along with its performance guarantees for
240"
EXTENSION TO AVERAGE-REWARD MDPS,0.5883495145631068,"this setting. The reader is referred to Appendix C for the full details, and to Chapter 8 of Puterman
241"
EXTENSION TO AVERAGE-REWARD MDPS,0.5902912621359223,"[29] for a more thorough discussion of average-reward MDPs.
242"
EXTENSION TO AVERAGE-REWARD MDPS,0.5922330097087378,"In
the
average
reward
setting
we
aim
to
optimize
the
objective
ρπ(x)
=
243"
EXTENSION TO AVERAGE-REWARD MDPS,0.5941747572815534,lim infT →∞1
EXTENSION TO AVERAGE-REWARD MDPS,0.596116504854369,"T Eπ
PT
t=1 r(xt, at)
 x1 = x

, representing the long-term average reward of
244"
EXTENSION TO AVERAGE-REWARD MDPS,0.5980582524271845,"policy π when started from state x ∈X. Unlike the discounted setting, the average reward criterion
245"
EXTENSION TO AVERAGE-REWARD MDPS,0.6,"prioritizes long-term frequency over proximity of good rewards due to the absence of discounting
246"
EXTENSION TO AVERAGE-REWARD MDPS,0.6019417475728155,"which expresses a preference for earlier rewards. As is standard in the related literature, we will
247"
EXTENSION TO AVERAGE-REWARD MDPS,0.6038834951456311,"assume that ρπ is well-deﬁned for any policy and is independent of the start state, and thus will
248"
EXTENSION TO AVERAGE-REWARD MDPS,0.6058252427184466,"use the same notation to represent the scalar average reward of policy π. Due to the boundedness
249"
EXTENSION TO AVERAGE-REWARD MDPS,0.6077669902912621,"of the rewards, we clearly have ρπ ∈[0, 1]. Similarly to the discounted setting, it is possible
250"
EXTENSION TO AVERAGE-REWARD MDPS,0.6097087378640776,"to deﬁne quantities analogous to the value and action value functions as the solutions to the
251"
EXTENSION TO AVERAGE-REWARD MDPS,0.6116504854368932,"Bellman equations qπ = r −ρπ1 + P vπ, where vπ is related to the action-value function as
252"
EXTENSION TO AVERAGE-REWARD MDPS,0.6135922330097088,vπ(x) = P
EXTENSION TO AVERAGE-REWARD MDPS,0.6155339805825243,"a π(a|x)qπ(x, a). We will make the following standard assumption about the MDP (see,
253"
EXTENSION TO AVERAGE-REWARD MDPS,0.6174757281553398,"e.g., Section 17.4 of Meyn & Tweedie [22]):
254"
EXTENSION TO AVERAGE-REWARD MDPS,0.6194174757281553,"Assumption 5.1. For all stationary policies π, the Bellman equations have a solution qπ satisfying
255"
EXTENSION TO AVERAGE-REWARD MDPS,0.6213592233009708,"supx,a qπ(x, a) −infx,a qπ(x, a) < Dq.
256"
EXTENSION TO AVERAGE-REWARD MDPS,0.6233009708737864,"Furthermore, we will continue to work with the linear MDP assumption of Deﬁnition 2.1, and will
257"
EXTENSION TO AVERAGE-REWARD MDPS,0.625242718446602,"additionally make the following minor assumption:
258"
EXTENSION TO AVERAGE-REWARD MDPS,0.6271844660194175,"Assumption 5.2. The all ones vector 1 is contained in the column span of the feature matrix Φ.
259"
EXTENSION TO AVERAGE-REWARD MDPS,0.629126213592233,"Furthermore, let ϱ ∈Rd such that for all (x, a) ∈Z, ⟨ϕ(x, a), ϱ⟩= 1.
260"
EXTENSION TO AVERAGE-REWARD MDPS,0.6310679611650486,"Using these insights, it is straightforward to derive a linear program akin to (2) that characterize the
261"
EXTENSION TO AVERAGE-REWARD MDPS,0.6330097087378641,"optimal occupancy measure and thus an optimal policy in average-reward MDPs. Starting from this
262"
EXTENSION TO AVERAGE-REWARD MDPS,0.6349514563106796,"formulation and proceeding as in Sections 2 and 4, we equivalently restate this optimization problem
263"
EXTENSION TO AVERAGE-REWARD MDPS,0.6368932038834951,"as ﬁnding the saddle-point of the reparametrized Lagrangian deﬁned as follows:
264"
EXTENSION TO AVERAGE-REWARD MDPS,0.6388349514563106,"L(β, µ; ρ, v, θ) = ρ + ⟨β , Λc[ω + Ψv −θ −ρϱ]⟩+ ⟨µ , Φθ −Ev⟩."
EXTENSION TO AVERAGE-REWARD MDPS,0.6407766990291263,"As previously, the saddle point can be shown to be equivalent to an optimal occupancy measure under
265"
EXTENSION TO AVERAGE-REWARD MDPS,0.6427184466019418,"the assumption that the MDP is linear in the sense of Deﬁnition 2.1. Notice that the above Lagrangian
266"
EXTENSION TO AVERAGE-REWARD MDPS,0.6446601941747573,"slightly differs from that of the discounted setting in Equation (11) due to the additional optimization
267"
EXTENSION TO AVERAGE-REWARD MDPS,0.6466019417475728,"parameter ρ, but otherwise our main algorithm can be directly generalized to this objective. We
268"
EXTENSION TO AVERAGE-REWARD MDPS,0.6485436893203883,"present details of the derivations and the resulting algorithm in Appendix C. The following theorem
269"
EXTENSION TO AVERAGE-REWARD MDPS,0.6504854368932039,"states the performance guarantees for this method.
270"
EXTENSION TO AVERAGE-REWARD MDPS,0.6524271844660194,"Theorem 5.3. Given a linear MDP (Deﬁnition 2.1) satisfying Assumption 5.2 and such that θπ ∈
271"
EXTENSION TO AVERAGE-REWARD MDPS,0.654368932038835,"B(Dθ) for any policy π. Assume that the coverage ratio is bounded Cϕ,c(π∗; πB) ≤Dβ. Then, for
272"
EXTENSION TO AVERAGE-REWARD MDPS,0.6563106796116505,"any comparator policy π∗, the policy output by an appropriately tuned instance of Algorithm 2 satisﬁes
273"
EXTENSION TO AVERAGE-REWARD MDPS,0.658252427184466,"E

⟨µπ∗−µπout, r⟩

≤ε with a number of samples nϵ that is O

ε−4D4
θD12c−2
ϕ
D4
βd2−2c log |A|

.
274"
EXTENSION TO AVERAGE-REWARD MDPS,0.6601941747572816,"As compared to the discounted case, this additional dependence of the sample complexity on Dϕ is
275"
EXTENSION TO AVERAGE-REWARD MDPS,0.6621359223300971,"due to the extra optimization variable ρ. We provide the full proof of this theorem along with further
276"
EXTENSION TO AVERAGE-REWARD MDPS,0.6640776699029126,"discussion in Appendix C.
277"
DISCUSSION AND FINAL REMARKS,0.6660194174757281,"6
Discussion and Final Remarks
278"
DISCUSSION AND FINAL REMARKS,0.6679611650485436,"In this section, we compare our results with the most relevant ones from the literature. Our Table 1 can
279"
DISCUSSION AND FINAL REMARKS,0.6699029126213593,"be used as a reference. As a complement to this section, we refer the interested reader to the recent
280"
DISCUSSION AND FINAL REMARKS,0.6718446601941748,"work by Uehara & Sun [32], which provides a survey of ofﬂine RL methods with their coverage and
281"
DISCUSSION AND FINAL REMARKS,0.6737864077669903,"structural assumptions. Detailed computations can be found in Appendix E.
282"
DISCUSSION AND FINAL REMARKS,0.6757281553398058,"An important property of our method is that it only requires partial coverage. This sets it apart from
283"
DISCUSSION AND FINAL REMARKS,0.6776699029126214,"classic batch RL methods like FQI [11, 23], which require a stronger uniform-coverage assumption.
284"
DISCUSSION AND FINAL REMARKS,0.6796116504854369,"Algorithms working under partial coverage are mostly based on the principle of pessimism. However,
285"
DISCUSSION AND FINAL REMARKS,0.6815533980582524,"our algorithm does not implement any form of explicit pessimism. We recall that, as shown by Xiao
286"
DISCUSSION AND FINAL REMARKS,0.683495145631068,"et al. [35], pessimism is just one of many ways to achieve minimax-optimal sample efﬁciency.
287"
DISCUSSION AND FINAL REMARKS,0.6854368932038835,"Let us now compare our notion of coverage ratio to the existing notions previsouly used in the
288"
DISCUSSION AND FINAL REMARKS,0.6873786407766991,"literature. Jin et al. [14] (Theorem 4.4) rely on a feature coverage ratio which can be written as
289"
DISCUSSION AND FINAL REMARKS,0.6893203883495146,"C⋄(π∗; πB) = EX,A∼µ∗
ϕ(X, A)
TΛ−1ϕ(X, A)

.
(17)"
DISCUSSION AND FINAL REMARKS,0.6912621359223301,"By Jensen’s inequality, our Cϕ,1/2 (Deﬁnition 3.1) is never larger than C⋄. Indeed, notice how
290"
DISCUSSION AND FINAL REMARKS,0.6932038834951456,"the random features in Equation (17) are coupled, introducing an extra variance term w.r.t. Cϕ,1/2.
291"
DISCUSSION AND FINAL REMARKS,0.6951456310679611,"Speciﬁcally, we can show that Cϕ,1/2(π∗; πB) = C⋄(π∗; πB) −VX,A∼µ∗
Λ−1/2ϕ(X, A)

, where
292"
DISCUSSION AND FINAL REMARKS,0.6970873786407767,"V [Z] = E[∥Z −E [Z]∥2] for a random vector Z. So, besides ﬁne comparisons with existing notions
293"
DISCUSSION AND FINAL REMARKS,0.6990291262135923,"of coverage ratios, we can regard Cϕ,1/2 as a low-variance version of the standard feature coverage
294"
DISCUSSION AND FINAL REMARKS,0.7009708737864078,"ratio. However, our sample complexity bounds do not fully take advantage of this low-variance
295"
DISCUSSION AND FINAL REMARKS,0.7029126213592233,"property, since they scale quadratically with the ratio itself, rather than linearly, as is more common
296"
DISCUSSION AND FINAL REMARKS,0.7048543689320388,"in previous work.
297"
DISCUSSION AND FINAL REMARKS,0.7067961165048544,"To scale with Cϕ,1/2, our algorithm requires knowledge of Λ, hence of the behavior policy. However,
298"
DISCUSSION AND FINAL REMARKS,0.7087378640776699,"so does the algorithm from Jin et al. [14]. Zanette et al. [38] remove this requirement at the price of a
299"
DISCUSSION AND FINAL REMARKS,0.7106796116504854,"computationally heavier algorithm. However, both are limited to the ﬁnite-horizon setting.
300"
DISCUSSION AND FINAL REMARKS,0.7126213592233009,"Uehara & Sun [32] and Zhang et al. [39] use a coverage ratio that is conceptually similar to Equa-
301"
DISCUSSION AND FINAL REMARKS,0.7145631067961165,"tion (17),
302"
DISCUSSION AND FINAL REMARKS,0.7165048543689321,"C†(π∗; πB) = sup
y∈Rd
yTEX,A∼µ∗[ϕ(X, A)ϕ(X, A)T] y
yTEX,A∼µB [ϕ(X, A)ϕ(X, A)T] y .
(18)"
DISCUSSION AND FINAL REMARKS,0.7184466019417476,"Some linear algebra shows that C† ≤C⋄≤dC†. Therefore, chaining the previous inequalities
303"
DISCUSSION AND FINAL REMARKS,0.7203883495145631,"we know that Cϕ,1/2 ≤C⋄≤dC†. It should be noted that the algorithm from Uehara & Sun [32]
304"
DISCUSSION AND FINAL REMARKS,0.7223300970873786,"also works in the representation-learning setting, that is, with unknown features. However, it is far
305"
DISCUSSION AND FINAL REMARKS,0.7242718446601941,"from being efﬁciently implementable. The algorithm from Zhang et al. [39] instead is limited to the
306"
DISCUSSION AND FINAL REMARKS,0.7262135922330097,"ﬁnite-horizon setting.
307"
DISCUSSION AND FINAL REMARKS,0.7281553398058253,"In the special case of tabular MDPs, it is hard to compare our ratio with existing ones, because in
308"
DISCUSSION AND FINAL REMARKS,0.7300970873786408,"this setting, error bounds are commonly stated in terms of supx,a µ∗(x,a)/µB(x,a), often introducing
309"
DISCUSSION AND FINAL REMARKS,0.7320388349514563,"an explicit dependency on the number of states [e.g., 17], which is something we carefully avoided.
310"
DISCUSSION AND FINAL REMARKS,0.7339805825242719,"However, looking at how the coverage ratio specializes to the tabular setting can still provide
311"
DISCUSSION AND FINAL REMARKS,0.7359223300970874,"some insight. With known behavior policy, Cϕ,1/2(π∗; πB) = P"
DISCUSSION AND FINAL REMARKS,0.7378640776699029,"x,aµ∗(x,a)2/µB(x,a) is smaller than
312"
DISCUSSION AND FINAL REMARKS,0.7398058252427184,the more standard C⋄(π∗; πB) = P
DISCUSSION AND FINAL REMARKS,0.7417475728155339,"x,aµ∗(x,a)/µB(x,a). With unknown behavior, Cϕ,1(π∗; πB) =
313 P"
DISCUSSION AND FINAL REMARKS,0.7436893203883496,"x,a(µ∗(x,a)/µB(x,a))2 is non-comparable with C⋄in general, but larger than Cϕ,1/2. Interestingly,
314"
DISCUSSION AND FINAL REMARKS,0.7456310679611651,"Cϕ,1(π∗; πB) is also equal to 1+X 2(µ∗∥µB), where X 2 denotes the chi-square divergence, a crucial
315"
DISCUSSION AND FINAL REMARKS,0.7475728155339806,"quantity in off-distribution learning based on importance sampling [10]. Moreover, a similar quantity
316"
DISCUSSION AND FINAL REMARKS,0.7495145631067961,"to Cϕ,1 was used by Lykouris et al. [18] in the context of (online) RL with adversarial corruptions.
317"
DISCUSSION AND FINAL REMARKS,0.7514563106796116,"We now turn to the works of Xie et al. [36] and Cheng et al. [9], which are the only practical
318"
DISCUSSION AND FINAL REMARKS,0.7533980582524272,"methods to consider function approximation in the inﬁnite horizon setting, with minimal assumption
319"
DISCUSSION AND FINAL REMARKS,0.7553398058252427,"on the dataset, and thus the only directly comparable to our work. They both use the coverage
320"
DISCUSSION AND FINAL REMARKS,0.7572815533980582,"ratio CF(π∗; πB) = maxf∈F ∥f−T f∥2
µ∗/∥f−T f∥2
µB,where F is a function class and T is Bellman’s
321"
DISCUSSION AND FINAL REMARKS,0.7592233009708738,"operator. This can be shown to reduce to Equation (18) for linear MDPs. However, the specialized
322"
DISCUSSION AND FINAL REMARKS,0.7611650485436893,"bound of Xie et al. [36] (Theorem 3.2) scales with the potentially larger ratio from Equation (17).
323"
DISCUSSION AND FINAL REMARKS,0.7631067961165049,"Both their algorithms have superlinear computational complexity and a sample complexity of O(ε−5).
324"
DISCUSSION AND FINAL REMARKS,0.7650485436893204,"Hence, in the linear MDP setting, our algorithm is a strict improvement both for its O(ε−4) sample
325"
DISCUSSION AND FINAL REMARKS,0.7669902912621359,"complexity and its O(n) computational complexity. However, It is very important to notice that no
326"
DISCUSSION AND FINAL REMARKS,0.7689320388349514,"practical algorithm for this setting so far, including ours, can match the minimax optimal sample
327"
DISCUSSION AND FINAL REMARKS,0.7708737864077669,"complexity rate of O(ε2) [35, 31]. This leaves space for future work in this area. In particular, by
328"
DISCUSSION AND FINAL REMARKS,0.7728155339805826,"inspecting our proofs, it should be clear the the extra O(ε−2) factor is due to the nested-loop structure
329"
DISCUSSION AND FINAL REMARKS,0.7747572815533981,"of the algorithm. Therefore, we ﬁnd it likely that our result can be improved using optimistic descent
330"
DISCUSSION AND FINAL REMARKS,0.7766990291262136,"methods [6] or a two-timescale approach [15, 30].
331"
DISCUSSION AND FINAL REMARKS,0.7786407766990291,"As a ﬁnal remark, we remind that when Λ is unknown, our error bounds scales with Cϕ,1, instead of
332"
DISCUSSION AND FINAL REMARKS,0.7805825242718447,"the smaller Cϕ,1/2. However, we ﬁnd it plausible that one can replace the Λ with an estimate that is
333"
DISCUSSION AND FINAL REMARKS,0.7825242718446602,"built using some fraction of the overall sample budget. In particular, in the tabular case, we could
334"
DISCUSSION AND FINAL REMARKS,0.7844660194174757,"simply use all data to estimate the visitation probabilities of each-state action pairs and use them to
335"
DISCUSSION AND FINAL REMARKS,0.7864077669902912,"build an estimator of Λ. Details of a similar approach have been worked out by Gabbianelli et al.
336"
DISCUSSION AND FINAL REMARKS,0.7883495145631068,"[12]. Nonetheless, we designed our algorithm to be ﬂexible and work in both cases.
337"
DISCUSSION AND FINAL REMARKS,0.7902912621359224,"To summarize, our method is one of the few not to assume the state space to be ﬁnite, or the dataset
338"
DISCUSSION AND FINAL REMARKS,0.7922330097087379,"to have global coverage, while also being computationally feasible. Moreover, it offers a signiﬁcant
339"
DISCUSSION AND FINAL REMARKS,0.7941747572815534,"advantage, both in terms of sample and computational complexity, over the two existing polynomial-
340"
DISCUSSION AND FINAL REMARKS,0.7961165048543689,"time algorithms for discounted linear MDPs with partial coverage [36, 9]; it extends to the challenging
341"
DISCUSSION AND FINAL REMARKS,0.7980582524271844,"average-reward setting with minor modiﬁcations; and has error bounds that scale with a low-variance
342"
DISCUSSION AND FINAL REMARKS,0.8,"version of the typical coverage ratio. These results were made possible by employing algorithmic
343"
DISCUSSION AND FINAL REMARKS,0.8019417475728156,"principles, based on the linear programming formulation of sequential decision making, that are new
344"
DISCUSSION AND FINAL REMARKS,0.8038834951456311,"in ofﬂine RL. Finally, the main direction for future work is to develop a single-loop algorithm to
345"
DISCUSSION AND FINAL REMARKS,0.8058252427184466,"achieve the optimal rate of ε−2, which should also improve the dependence on the coverage ratio
346"
DISCUSSION AND FINAL REMARKS,0.8077669902912621,"from Cϕ,c(π∗; πB)2 to Cϕ,c(π∗; πB).
347"
REFERENCES,0.8097087378640777,"References
348"
REFERENCES,0.8116504854368932,"[1] Bas-Serrano, J. and Neu, G. Faster saddle-point optimization for solving large-scale markov
349"
REFERENCES,0.8135922330097087,"decision processes. In L4DC, volume 120 of Proceedings of Machine Learning Research, pp.
350"
REFERENCES,0.8155339805825242,"413–423. PMLR, 2020.
351"
REFERENCES,0.8174757281553398,"[2] Bas-Serrano, J., Curi, S., Krause, A., and Neu, G. Logistic q-learning. In Banerjee, A. and Fuku-
352"
REFERENCES,0.8194174757281554,"mizu, K. (eds.), Proceedings of The 24th International Conference on Artiﬁcial Intelligence and
353"
REFERENCES,0.8213592233009709,"Statistics, volume 130 of Proceedings of Machine Learning Research, pp. 3610–3618. PMLR,
354"
REFERENCES,0.8233009708737864,"13–15 Apr 2021. URL https://proceedings.mlr.press/v130/bas-serrano21a.html.
355"
REFERENCES,0.8252427184466019,"[3] Bellman, R. Dynamic programming. Technical report, RAND CORP SANTA MONICA CA,
356"
REFERENCES,0.8271844660194175,"1956.
357"
REFERENCES,0.829126213592233,"[4] Bellman, R. Dynamic programming. Science, 153(3731):34–37, 1966.
358"
REFERENCES,0.8310679611650486,"[5] Bertsekas, D. P. Constrained Optimization and Lagrange Multiplier Methods. Academic Press,
359"
REFERENCES,0.8330097087378641,"1982. ISBN 978-0-12-093480-5.
360"
REFERENCES,0.8349514563106796,"[6] Borkar, V. S. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):
361"
REFERENCES,0.8368932038834952,"291–294, 1997.
362"
REFERENCES,0.8388349514563107,"[7] Cesa-Bianchi, N. and Lugosi, G. Prediction, Learning, and Games. Cambridge University
363"
REFERENCES,0.8407766990291262,"Press, New York, NY, USA, 2006.
364"
REFERENCES,0.8427184466019417,"[8] Chen, Y., Li, L., and Wang, M. Scalable bilinear learning using state and action features. In
365"
REFERENCES,0.8446601941747572,"ICML, volume 80 of Proceedings of Machine Learning Research, pp. 833–842. PMLR, 2018.
366"
REFERENCES,0.8466019417475729,"[9] Cheng, C.-A., Xie, T., Jiang, N., and Agarwal, A. Adversarially trained actor critic for ofﬂine
367"
REFERENCES,0.8485436893203884,"reinforcement learning. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and
368"
REFERENCES,0.8504854368932039,"Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning,
369"
REFERENCES,0.8524271844660194,"volume 162 of Proceedings of Machine Learning Research, pp. 3852–3878. PMLR, 17–23 Jul
370"
REFERENCES,0.8543689320388349,"2022. URL https://proceedings.mlr.press/v162/cheng22b.html.
371"
REFERENCES,0.8563106796116505,"[10] Cortes, C., Mansour, Y., and Mohri, M. Learning bounds for importance weighting. In NeurIPS,
372"
REFERENCES,0.858252427184466,"pp. 442–450. Curran Associates, Inc., 2010.
373"
REFERENCES,0.8601941747572815,"[11] Ernst, D., Geurts, P., and Wehenkel, L. Tree-based batch mode reinforcement learning. J. Mach.
374"
REFERENCES,0.8621359223300971,"Learn. Res., 6:503–556, 2005.
375"
REFERENCES,0.8640776699029126,"[12] Gabbianelli, G., Neu, G., and Papini, M. Online learning with off-policy feedback. In Agrawal,
376"
REFERENCES,0.8660194174757282,"S. and Orabona, F. (eds.), ALT, volume 201 of Proceedings of Machine Learning Research,
377"
REFERENCES,0.8679611650485437,"pp. 620–641. PMLR, 20 Feb–23 Feb 2023. URL https://proceedings.mlr.press/v201/
378"
REFERENCES,0.8699029126213592,"gabbianelli23a.html.
379"
REFERENCES,0.8718446601941747,"[13] Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. Provably efﬁcient reinforcement learning with
380"
REFERENCES,0.8737864077669902,"linear function approximation. In COLT, volume 125 of Proceedings of Machine Learning
381"
REFERENCES,0.8757281553398059,"Research, pp. 2137–2143. PMLR, 2020.
382"
REFERENCES,0.8776699029126214,"[14] Jin, Y., Yang, Z., and Wang, Z. Is pessimism provably efﬁcient for ofﬂine rl? In International
383"
REFERENCES,0.8796116504854369,"Conference on Machine Learning, pp. 5084–5096. PMLR, 2021.
384"
REFERENCES,0.8815533980582524,"[15] Korpelevich, G. The extragradient method for ﬁnding saddle points and other problems.
385"
REFERENCES,0.883495145631068,"Matecon, 12:747–756, 1976.
386"
REFERENCES,0.8854368932038835,"[16] Levine, S., Kumar, A., Tucker, G., and Fu, J. Ofﬂine reinforcement learning: Tutorial, review,
387"
REFERENCES,0.887378640776699,"and perspectives on open problems. 2020.
388"
REFERENCES,0.8893203883495145,"[17] Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. Provably good batch off-policy
389"
REFERENCES,0.8912621359223301,"reinforcement learning without great exploration. In NeurIPS, 2020.
390"
REFERENCES,0.8932038834951457,"[18] Lykouris, T., Simchowitz, M., Slivkins, A., and Sun, W. Corruption-robust exploration in
391"
REFERENCES,0.8951456310679612,"episodic reinforcement learning. In COLT, volume 134 of Proceedings of Machine Learning
392"
REFERENCES,0.8970873786407767,"Research, pp. 3242–3245. PMLR, 2021.
393"
REFERENCES,0.8990291262135922,"[19] Manne, A. S. Linear programming and sequential decisions. Manage. Sci., 6(3):259–267,
394"
REFERENCES,0.9009708737864077,"apr 1960. ISSN 0025-1909. doi: 10.1287/mnsc.6.3.259. URL https://doi.org/10.1287/
395"
REFERENCES,0.9029126213592233,"mnsc.6.3.259.
396"
REFERENCES,0.9048543689320389,"[20] Manne, A. S. Linear programming and sequential decisions. Management Science, 6(3):
397"
REFERENCES,0.9067961165048544,"259–267, 1960.
398"
REFERENCES,0.9087378640776699,"[21] Mehta, P. G. and Meyn, S. P. Q-learning and pontryagin’s minimum principle. In CDC, pp.
399"
REFERENCES,0.9106796116504854,"3598–3605. IEEE, 2009.
400"
REFERENCES,0.912621359223301,"[22] Meyn, S. and Tweedie, R. Markov Chains and Stochastic Stability. Springer-Verlag, 1996.
401"
REFERENCES,0.9145631067961165,"[23] Munos, R. and Szepesvári, C. Finite-time bounds for ﬁtted value iteration. J. Mach. Learn.
402"
REFERENCES,0.916504854368932,"Res., 9:815–857, 2008.
403"
REFERENCES,0.9184466019417475,"[24] Nachum, O. and Dai, B. Reinforcement learning via fenchel-rockafellar duality. 2020.
404"
REFERENCES,0.920388349514563,"[25] Nemirovski, A. and Yudin, D. Problem Complexity and Method Efﬁciency in Optimization.
405"
REFERENCES,0.9223300970873787,"Wiley Interscience, 1983.
406"
REFERENCES,0.9242718446601942,"[26] Neu, G. and Okolo, N. Efﬁcient global planning in large mdps via stochastic primal-dual
407"
REFERENCES,0.9262135922330097,"optimization. In ALT, volume 201 of Proceedings of Machine Learning Research, pp. 1101–
408"
REFERENCES,0.9281553398058252,"1123. PMLR, 2023.
409"
REFERENCES,0.9300970873786408,"[27] Neu, G., Jonsson, A., and Gómez, V. A uniﬁed view of entropy-regularized Markov decision
410"
REFERENCES,0.9320388349514563,"processes. arXiv preprint arXiv:1705.07798, 2017.
411"
REFERENCES,0.9339805825242719,"[28] Orabona, F. A modern introduction to online learning. arXiv preprint arXiv:1912.13213, 2019.
412"
REFERENCES,0.9359223300970874,"[29] Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
413"
REFERENCES,0.9378640776699029,"Wiley & Sons, Inc., USA, 1994. ISBN 0471619779.
414"
REFERENCES,0.9398058252427185,"[30] Rakhlin, A. and Sridharan, K. Optimization, learning, and games with predictable sequences.
415"
REFERENCES,0.941747572815534,"In Advances in Neural Information Processing Systems, pp. 3066–3074, 2013.
416"
REFERENCES,0.9436893203883495,"[31] Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. Bridging ofﬂine reinforcement learning
417"
REFERENCES,0.945631067961165,"and imitation learning: A tale of pessimism. IEEE Trans. Inf. Theory, 68(12):8156–8196, 2022.
418"
REFERENCES,0.9475728155339805,"[32] Uehara, M. and Sun, W. Pessimistic model-based ofﬂine reinforcement learning under partial
419"
REFERENCES,0.9495145631067962,"coverage. In ICLR. OpenReview.net, 2022.
420"
REFERENCES,0.9514563106796117,"[33] Uehara, M., Huang, J., and Jiang, N. Minimax weight and q-function learning for off-policy
421"
REFERENCES,0.9533980582524272,"evaluation. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference
422"
REFERENCES,0.9553398058252427,"on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9659–
423"
REFERENCES,0.9572815533980582,"9668. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/uehara20a.
424"
REFERENCES,0.9592233009708738,"html.
425"
REFERENCES,0.9611650485436893,"[34] Wang, M. and Chen, Y. An online primal-dual method for discounted markov decision processes.
426"
REFERENCES,0.9631067961165048,"In CDC, pp. 4516–4521. IEEE, 2016.
427"
REFERENCES,0.9650485436893204,"[35] Xiao, C., Wu, Y., Mei, J., Dai, B., Lattimore, T., Li, L., Szepesvári, C., and Schuurmans, D. On
428"
REFERENCES,0.9669902912621359,"the optimality of batch policy optimization algorithms. In ICML, volume 139 of Proceedings of
429"
REFERENCES,0.9689320388349515,"Machine Learning Research, pp. 11362–11371. PMLR, 2021.
430"
REFERENCES,0.970873786407767,"[36] Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. Bellman-consistent pessimism
431"
REFERENCES,0.9728155339805825,"for ofﬂine reinforcement learning. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P.,
432"
REFERENCES,0.974757281553398,"and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34,
433"
REFERENCES,0.9766990291262136,"pp. 6683–6694. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/
434"
REFERENCES,0.9786407766990292,"paper_files/paper/2021/file/34f98c7c5d7063181da890ea8d25265a-Paper.pdf.
435"
REFERENCES,0.9805825242718447,"[37] Yang, L. and Wang, M. Sample-optimal parametric q-learning using linearly additive features.
436"
REFERENCES,0.9825242718446602,"In ICML, volume 97 of Proceedings of Machine Learning Research, pp. 6995–7004. PMLR,
437"
REFERENCES,0.9844660194174757,"2019.
438"
REFERENCES,0.9864077669902913,"[38] Zanette, A., Wainwright, M. J., and Brunskill, E. Provable beneﬁts of actor-critic methods for
439"
REFERENCES,0.9883495145631068,"ofﬂine reinforcement learning. In NeurIPS, pp. 13626–13640, 2021.
440"
REFERENCES,0.9902912621359223,"[39] Zhang, X., Chen, Y., Zhu, X., and Sun, W. Corruption-robust ofﬂine reinforcement learning. In
441"
REFERENCES,0.9922330097087378,"AISTATS, volume 151 of Proceedings of Machine Learning Research, pp. 5757–5773. PMLR,
442"
REFERENCES,0.9941747572815534,"2022.
443"
REFERENCES,0.996116504854369,"[40] Zinkevich, M. Online convex programming and generalized inﬁnitesimal gradient ascent. In
444"
REFERENCES,0.9980582524271845,"Proceedings of the Twentieth International Conference on Machine Learning (ICML), 2003.
445"
