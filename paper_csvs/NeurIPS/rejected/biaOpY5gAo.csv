Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002079002079002079,"Despite the dominance and effectiveness of scaling, resulting in large networks with
1"
ABSTRACT,0.004158004158004158,"hundreds of billions of parameters, the necessity to train overparametrized models
2"
ABSTRACT,0.006237006237006237,"remains poorly understood, and alternative approaches do not necessarily make
3"
ABSTRACT,0.008316008316008316,"it cheaper to train high-performance models. In this paper, we explore low-rank
4"
ABSTRACT,0.010395010395010396,"training techniques as an alternative approach to training large neural networks.
5"
ABSTRACT,0.012474012474012475,"We introduce a novel method called ReLoRA, which utilizes low-rank updates to
6"
ABSTRACT,0.014553014553014554,"train high-rank networks. We apply ReLoRA to pre-training transformer language
7"
ABSTRACT,0.016632016632016633,"models with up to 350M parameters, and demonstrate comparable performance
8"
ABSTRACT,0.018711018711018712,"to regular neural network training. Furthermore, we observe that the efficiency
9"
ABSTRACT,0.02079002079002079,"of ReLoRA increases with model size, making it a promising approach for train-
10"
ABSTRACT,0.02286902286902287,"ing multi-billion-parameter networks efficiently. Our findings shed light on the
11"
ABSTRACT,0.02494802494802495,"potential of low-rank training techniques and their implications for scaling laws.1
12"
INTRODUCTION,0.02702702702702703,"1
Introduction
13"
INTRODUCTION,0.029106029106029108,"Over the past decade, the machine learning field has been dominated by the trend of training
14"
INTRODUCTION,0.031185031185031187,"increasingly overparametrized networks or adopting the ""stack more layers"" approach [32, 21, 27].
15"
INTRODUCTION,0.033264033264033266,"The definition of a large network has evolved from models with 100 million [46, 39] to hundreds
16"
INTRODUCTION,0.035343035343035345,"of billions [8, 12] of parameters, which has made computational costs associated with training of
17"
INTRODUCTION,0.037422037422037424,"such networks prohibitive to most of the research groups. Despite this, the necessity to train models
18"
INTRODUCTION,0.0395010395010395,"which can have orders of magnitude more parameters than the training examples [8, 12, 16], is poorly
19"
INTRODUCTION,0.04158004158004158,"understood theoretically [25, 4, 60].
20"
INTRODUCTION,0.04365904365904366,"Alternative approaches to scaling, such as more compute-efficient scaling optima [22], retrieval-
21"
INTRODUCTION,0.04573804573804574,"augmented models [28, 7], and the simple approach of training smaller models for longer [50], have
22"
INTRODUCTION,0.04781704781704782,"offered new interesting trade-offs. However, they do not bring us closer to understanding why we
23"
INTRODUCTION,0.0498960498960499,"need overparametrized models and rarely democratize the training of these models. For example,
24"
INTRODUCTION,0.05197505197505198,"training RETRO [7] requires a complex training setup and infrastructure capable of quickly searching
25"
INTRODUCTION,0.05405405405405406,"over trillions of tokens, while training LLaMA-6B [50] still requires hundreds of GPUs.
26"
INTRODUCTION,0.056133056133056136,"In contrast, approaches like zero-redundancy optimizers [43], 16-bit training [37], 8-bit inference [14],
27"
INTRODUCTION,0.058212058212058215,"and parameter-efficient fine-tuning (PEFT) [33] have played a crucial role in making large models
28"
INTRODUCTION,0.060291060291060294,"more accessible. Specifically, PEFT methods have enabled fine-tuning of billion-scale language or
29"
INTRODUCTION,0.062370062370062374,"diffusion models on consumer hardware. This raises the question: Can these approaches also benefit
30"
INTRODUCTION,0.06444906444906445,"pre-training?
31"
INTRODUCTION,0.06652806652806653,"On one hand, pre-training is exactly the step that allows for small modifications to the network to
32"
INTRODUCTION,0.06860706860706861,"adapt it to new tasks. Aghajanyan et al. [1] demonstrated that the rank of the changes required
33"
INTRODUCTION,0.07068607068607069,1The code is provided with the supplementary material of the submission.
INTRODUCTION,0.07276507276507277,"0
2500
5000
7500
10000
12500
15000
17500
20000
3.0 3.5 4.0 4.5 5.0 Loss"
M,0.07484407484407485,"250M
250M ReLoRA
(99M trainable)
99M"
M,0.07692307692307693,"0
2500
5000
7500
10000
12500
15000
17500
20000
Step 0
100 250"
M,0.079002079002079,Trainable Params
M,0.08108108108108109,"Figure 1: ReLoRA learns a high-rank network through a sequence of low-rank updates. It outperforms
networks with the same trainable parameter count and achieves similar performance to training a full
network at 100M+ scale. The efficiency of ReLoRA increases with the model size, making it a viable
candidate for multi-billion-parameter training."
M,0.08316008316008316,"to learn a task decreases the more you pre-train the network. On the other hand, multiple studies
34"
M,0.08523908523908524,"have demonstrated the simplicity of features extracted and utilized by language and vision models,
35"
M,0.08731808731808732,"along with their low intrinsic dimensionality [31, 17, 42, 47]. For instance, attention patterns in
36"
M,0.0893970893970894,"transformers [51] often exhibit a small rank, which has been successfully leveraged to develop more
37"
M,0.09147609147609148,"efficient variants of attention [52, 11]. Moreover, overparametrization is also not necessary for
38"
M,0.09355509355509356,"training. The Lottery Ticket Hypothesis [17] empirically demonstrates that during initialization (or
39"
M,0.09563409563409564,"early in training [18]), there exist sub-networks – winning tickets – that when trained in isolation
40"
M,0.09771309771309772,"reach the performance of the full network.
41"
M,0.0997920997920998,"In this study, we focus on low-rank training techniques and introduce ReLoRA that uses low-rank
42"
M,0.10187110187110188,"updates to train a high-rank network. We empirically demonstrate that ReLoRA performs a high-rank
43"
M,0.10395010395010396,"update and achieves performance similar to regular neural network training. The components of
44"
M,0.10602910602910603,"ReLoRA include initial full-rank training of the neural network (similar to Frankle et al. [18]), LoRA
45"
M,0.10810810810810811,"training, restarts, a jagged learning rate schedule, and partial optimizer resets. We evaluate ReLoRA
46"
M,0.1101871101871102,"on transformer language models up to 350M parameters. We chose to focus on autoregressive
47"
M,0.11226611226611227,"language modeling, as this approach has demonstrated its universality in most of the applications of
48"
M,0.11434511434511435,"neural networks [41, 56, 3, 35, 10]. Finally, we observe that the efficiency of ReLoRA increases with
49"
M,0.11642411642411643,"model size, making it a viable option for efficient training of multi-billion-parameter networks.
50"
M,0.11850311850311851,"Each experiment in this study has used no more than 8 GPU days of compute.
51"
RELATED WORK,0.12058212058212059,"2
Related work
52"
RELATED WORK,0.12266112266112267,"Scaling versus Efficiency
The relationship between overparametrization and neural network
53"
RELATED WORK,0.12474012474012475,"trainability and generalization has been extensively studied [59, 5, 17, 38, 47], yet it remains a
54"
RELATED WORK,0.12681912681912683,"mystery [60]. Moreover, scaling laws [27, 19, 22, 30, 2] demonstrate a simple and strong power-law
55"
RELATED WORK,0.1288981288981289,"dependence between network size and its performance across a variety of modalities. This finding
56"
RELATED WORK,0.13097713097713098,"not only supports overparametrization but also encourages the training of extraordinarily resource-
57"
RELATED WORK,0.13305613305613306,"intensive neural networks [8, 12, 16]. Nonetheless, the Lottery Ticket Hypothesis [17, 18] suggests
58"
RELATED WORK,0.13513513513513514,"that overparametrization could, in principle, be minimized. Specifically, it shows that early in training,
59"
RELATED WORK,0.13721413721413722,"subnetworks exist that can be trained to achieve the performance of the full network (winning tickets).
60"
RELATED WORK,0.1392931392931393,"Parameter-efficient fine-tuning
Aghajanyan et al. [1] found that pre-training reduces the amount
61"
RELATED WORK,0.14137214137214138,"of change to the network, or its intrinsic dimensionality, to learn a new task through fine-tuning. I.e.,
62"
RELATED WORK,0.14345114345114346,"larger networks or networks pre-trained on more data require smaller modifications in terms of the
63"
RELATED WORK,0.14553014553014554,"rank of the range to learn a new task. This explains the success of parameter-efficient fine-tuning
64"
RELATED WORK,0.14760914760914762,"methods [33] and has also motivated the development of low-rank fine-tuning methods such as LoRA
65"
RELATED WORK,0.1496881496881497,"[23] and Compacter [36].
66"
RELATED WORK,0.15176715176715178,"0
2000
4000
6000
8000
10000
Step 0.0 0.2 0.4 0.6 0.8 1.0"
RELATED WORK,0.15384615384615385,Learning Rate
RELATED WORK,0.15592515592515593,"Figure 2: Jagged cosine scheduler used in ReLoRA. On every ReLoRA reset, we set the learning rate
to zero and perform a quick (50-100 steps) learning rate warmup back to the cosine schedule."
RELATED WORK,0.158004158004158,"Low-rank neural network training
Training low-rank representations has been explored in the
67"
RELATED WORK,0.1600831600831601,"context of CNN compression, regularization, and efficient training [24, 26, 49, 44, 34, 57]. However,
68"
RELATED WORK,0.16216216216216217,"most of these methods are either specific to CNNs, do not scale well, or have not been evaluated
69"
RELATED WORK,0.16424116424116425,"on large transformers [51] with hundreds of millions of parameters, which can benefit greatly from
70"
RELATED WORK,0.16632016632016633,"efficient training. While transformers have been shown to have a low-rank internal dimensionality
71"
RELATED WORK,0.1683991683991684,"and representations [1, 52], the study by Bhojanapalli et al. [6] demonstrated that the low rank of key
72"
RELATED WORK,0.1704781704781705,"and query projections in multi-head attention bottlenecks the performance of transformers. Our own
73"
RELATED WORK,0.17255717255717257,"experiments (Section 3) also demonstrate that low-rank transformers perform significantly worse
74"
RELATED WORK,0.17463617463617465,"compared to the full-rank baseline and ReLoRA.
75"
METHOD,0.17671517671517672,"3
Method
76"
METHOD,0.1787941787941788,"Let’s start by revisiting linear algebra-101. In particular, we are interested in the rank of the sum of
77"
METHOD,0.18087318087318088,"two matrices:
78"
METHOD,0.18295218295218296,"rank(A + B) ≤rank(A) + rank(B).
(1)"
METHOD,0.18503118503118504,"This bound on the rank of the sum is tight: for a matrix A, rank(A) < dim(A), there exists B,
79"
METHOD,0.18711018711018712,"rank(B) < dim(B) such that sum of the matrices has a higher rank than either A or B. We want to
80"
METHOD,0.1891891891891892,"exploit this property to make a flexible parameter-efficient training method. We start with LoRA [23]
81"
METHOD,0.19126819126819128,"which is a parameter-efficient fine-tuning method based on the idea of low-rank updates. LoRA can
82"
METHOD,0.19334719334719336,"be applied to any linear operation parametrized through W ∈Rm×n. Specifically, LoRA decomposes
83"
METHOD,0.19542619542619544,"the weight update δW into a low-rank product WAWB as shown in Equation 2, where s ∈R is a
84"
METHOD,0.19750519750519752,"fixed scaling factor usually equal to 1 r.
85"
METHOD,0.1995841995841996,"δW = sWAWB
WA ∈Rin×r, WB ∈Rr×out
(2)"
METHOD,0.20166320166320167,"In practice, LoRA is usually implemented by adding new trainable parameters WA and WB, which
86"
METHOD,0.20374220374220375,"could be merged back into the original parameters after training. Thus, even though Equation 1
87"
METHOD,0.20582120582120583,allows the total update over training time P
METHOD,0.2079002079002079,"t δWt to have a higher rank than any of the individual
88"
METHOD,0.20997920997921,"matrices, LoRA implementations are restricted by the rank r = maxWA,WB rank(WAWB).
89"
METHOD,0.21205821205821207,"If we could restart LoRA, meaning we merge WA and WB during training and reset the values of
90"
METHOD,0.21413721413721415,"these matrices, we could increase the total rank of the update. Doing this multiple times brings the
91"
METHOD,0.21621621621621623,"total neural network update to
92 ∆W = T1
X"
METHOD,0.2182952182952183,"t=0
δWt + T2
X"
METHOD,0.2203742203742204,"t=T1
δWt + · · · + TN
X"
METHOD,0.22245322245322247,"t=TN−1
δWt = sW 1
AW 1
B + sW 2
AW 2
B + · · · + sW N
A W N
B (3)"
METHOD,0.22453222453222454,"where the sums are independent enough, meaning that rank(W i
AW i
B) + rank(W j
AW j
B) ≥r.
93"
METHOD,0.22661122661122662,"However, implementing restarts is not trivial in practice and requires certain modifications to the
94"
METHOD,0.2286902286902287,"optimization procedure. Naïve implementation causes the model to diverge right after the restart.
95"
METHOD,0.23076923076923078,"Unlike plain stochastic gradient descent, which solely relies on the value of the gradient at the current
96"
METHOD,0.23284823284823286,"optimization timestep, Adam [29] update is guided mainly by the first and second moments of the
97"
METHOD,0.23492723492723494,"gradient accumulated over the previous steps. In practice, gradient moment smoothing parameters β1
98"
METHOD,0.23700623700623702,"and β2 are usually very high 0.9 −0.999. Let’s assume that at the reinitialization boundary W 1
A and
99"
METHOD,0.2390852390852391,"the corresponding gradient moments mA and vA, are full-rank (r). Then, after the merge-and-reinit,
100"
METHOD,0.24116424116424118,"continuing to use old gradient moments for W 2
A will guide it in the same direction as W 1
A and
101"
METHOD,0.24324324324324326,"optimize the same subspace.
102"
METHOD,0.24532224532224534,"To resolve this issue, we propose ReLoRA. ReLoRA performs a partial reset of the optimizer state
103"
METHOD,0.24740124740124741,"during merge-and-reinit and sets the learning rate to 0 with a subsequent warmup. Specifically, we set
104"
METHOD,0.2494802494802495,"99% of low-magnitude optimizer state values to zero and use a jagged-cosine learning rate schedule
105"
METHOD,0.2515592515592516,"(Figure 2). Our ablation studies (Section 3) show that both of these modifications are required to
106"
METHOD,0.25363825363825365,"improve the performance over vanilla LoRA.
107"
METHOD,0.25571725571725573,"To reiterate, ReLoRA is a low-rank training method inspired by LoRA that uses restarts to increase
108"
METHOD,0.2577962577962578,"the effective rank of the update, uses partial optimizer reset, and a jagged scheduler to stabilize
109"
METHOD,0.2598752598752599,"training and warm starts. All of this allows ReLoRA to achieve performance comparable to full-rank
110"
METHOD,0.26195426195426197,"training, especially in large transformer networks, by only training a small set of parameters at a time.
111"
METHOD,0.26403326403326405,"ReLoRA is described in Algorithm 1.
112"
METHOD,0.2661122661122661,"Enhancing computational efficiency
Unlike other low-rank training techniques [44, 49], ReLoRA
113"
METHOD,0.2681912681912682,"follows the LoRA approach by maintaining the frozen weights of the original network and adding
114"
METHOD,0.2702702702702703,"new trainable parameters. At first glance, this may appear computationally inefficient; however, the
115"
METHOD,0.27234927234927236,"differentiation between frozen and trainable parameters plays a crucial role in parameter-efficient
116"
METHOD,0.27442827442827444,"fine-tuning [33].
117"
METHOD,0.2765072765072765,"These methods achieve significant improvements in training time and memory efficiency by reducing
118"
METHOD,0.2785862785862786,"the size of the gradients and the optimizer states. Notably, Adam states consume twice as much
119"
METHOD,0.2806652806652807,"memory as the model weights. Moreover, it is common practice to maintain gradient accumulation
120"
METHOD,0.28274428274428276,"buffers in 32-bit precision for large networks, thereby adding significant overhead to the memory
121"
METHOD,0.28482328482328484,"consumption of gradients.
122"
METHOD,0.2869022869022869,"By substantially reducing the number of trainable parameters, ReLoRA enables the utilization of larger
123"
METHOD,0.288981288981289,"batch sizes, maximizing hardware efficiency. Additionally, it reduces the bandwidth requirements in
124"
METHOD,0.2910602910602911,"distributed setups, which are often the limiting factor in large-scale training.
125"
METHOD,0.29313929313929316,"Furthermore, since the frozen parameters are not being updated between restarts, they can be kept in
126"
METHOD,0.29521829521829523,"a low-precision quantized format, further reducing their memory and computational impact. This
127"
METHOD,0.2972972972972973,"additional optimization contributes to overall improved efficiency in terms of memory utilization and
128"
METHOD,0.2993762993762994,"computational resources of ReLoRA and increases at scale.
129"
EXPERIMENTS,0.30145530145530147,"4
Experiments
130"
EXPERIMENTS,0.30353430353430355,"To evaluate the effectiveness of ReLoRA, we apply it to train a transformer language model on the C4
131"
EXPERIMENTS,0.30561330561330563,"dataset [41] using various model sizes: 60M, 130M, 250M, and 350M. Language modeling has been
132"
EXPERIMENTS,0.3076923076923077,"shown to be a fundamental task in machine learning [40], it enables text and image classification
133"
EXPERIMENTS,0.3097713097713098,"[56], translation [8], programming [9], in-context learning, step-by-step reasoning [54], and many
134"
EXPERIMENTS,0.31185031185031187,"other emergent abilities [53]. Given its significance, we focus solely on language modeling for the
135"
EXPERIMENTS,0.31392931392931395,"purposes of this paper.
136"
EXPERIMENTS,0.316008316008316,"Architecture and training hyperparameters
Our architecture is based on transformer [51] and
137"
EXPERIMENTS,0.3180873180873181,"closely resembles LLaMA [50]. Namely, we use pre-normalization, RMSNorm [58], SwiGLU
138"
EXPERIMENTS,0.3201663201663202,"activations [45], 8"
EXPERIMENTS,0.32224532224532226,"3h fully-connected hidden state size [50], and rotary embeddings [48]. All hyperpa-
139"
EXPERIMENTS,0.32432432432432434,"rameters are presented in Table 1.
140"
EXPERIMENTS,0.3264033264033264,"We use bfloat16 for all floating point operations and Flash attention [13] for effective attention
141"
EXPERIMENTS,0.3284823284823285,"computation. Compared to attention in LLaMA, which uses float32 for softmax computation, this
142"
EXPERIMENTS,0.3305613305613306,"increased training throughput by 50-100% without any training stability issues.
143"
EXPERIMENTS,0.33264033264033266,"Algorithm 1 ReLoRA. θ is model parameters, ˆθ is model parameters with linear layers replaced with
ReLoRA, M and V are Adam optimizer states, η is learning rate scheduled according to a jagged
scheduler, and finally, q is the reinit frequency."
EXPERIMENTS,0.33471933471933474,"Require: θ, M, V , q, η"
EXPERIMENTS,0.3367983367983368,"1: for t in warm start steps do
2:
Update θ, M, V , η {Regular training for warm start}
3: end for
4: for layer in model layers do
5:
if layer is linear then
6:
layer ←ReLoRA(W i, W i
A, W i
B)
7:
Freeze W i"
EXPERIMENTS,0.3388773388773389,"8:
end if
9: end for
10: for t in training steps do
11:
Update ˆθ, M, V {Training step with ReLoRA}
12:
if MOD(t, q) = 0 then
13:
for l in model layers do
14:
if l is linear then
15:
W i ←(W i + sW i
AW i
B)
16:
W i
A ←kaiming_init(W i
A); W i
B ←0
17:
MW i
A ←prune(MW i
A); VW i
A ←prune(VW i
A)
18:
end if
19:
end for
20:
Start η warmup
21:
end if
22: end for
23: return θ"
EXPERIMENTS,0.340956340956341,"Most of our models were trained on 8 RTX 4090 for one day or less. Due to computational constraints,
144"
EXPERIMENTS,0.34303534303534305,"we train much smaller models than LLaMA, with the largest model having 350M parameters, the
145"
EXPERIMENTS,0.34511434511434513,"same as BERT Large [15]. We select the number of pre-training tokens based on the Chinchilla
146"
EXPERIMENTS,0.3471933471933472,"scaling laws [22] for all models, except for the largest one, which we train for 6.8B tokens while 9.5B
147"
EXPERIMENTS,0.3492723492723493,"tokens are Chinchilla-optimal.
148"
EXPERIMENTS,0.35135135135135137,"ReLoRA and baselines setup
In our low-rank training experiments, ReLoRA replaces all attention
149"
EXPERIMENTS,0.35343035343035345,"and fully-connected network parameters, while keeping the embeddings full-rank. The RMSNorm
150"
EXPERIMENTS,0.35550935550935553,"parametrization remains unchanged. Since ReLoRA-wrapped models have fewer trainable parameters
151"
EXPERIMENTS,0.3575883575883576,"than full-rank training, we include a Control baseline, which is a full-rank transformer with the same
152"
EXPERIMENTS,0.3596673596673597,"number of trainable parameters as ReLoRA.
153"
EXPERIMENTS,0.36174636174636177,"We initialize ReLoRA from a checkpoint of full-rank training at 5,000 update steps and reset it
154"
EXPERIMENTS,0.36382536382536385,"every 5,000 steps thereafter, 3 times in total. After each reset, 99% of the optimizer state is pruned
155"
EXPERIMENTS,0.3659043659043659,"based on magnitude, and the loss is warmed up for the next 100 iterations. ReLoRA parameters are
156"
EXPERIMENTS,0.367983367983368,"reinitialized following LoRA best practices, Kaiming initialization [20] for A-matrix, and zeros for
157"
EXPERIMENTS,0.3700623700623701,"B-matrix. In case of not using the restarts, the B-matrix also uses Kaiming initialization to avoid
158"
EXPERIMENTS,0.37214137214137216,"gradient-symmetry issues.
159"
EXPERIMENTS,0.37422037422037424,"Params
Hidden
Heads
Layers
Learning rate
Batch (tokens)
Seq. len.
Tokens"
M,0.3762993762993763,"60M
512
8
8
1e-3
122K
256
1.2B
130M
768
12
12
1e-3
154K
256
2.6B
250M
768
16
24
5e-4
590K
512
6.8B
350M
1024
16
24
5e-4
590K
512
6.8B"
M,0.3783783783783784,Table 1: Hyperparameters of the language models trained in this study.
M,0.3804573804573805,"60M
130M
250M
350M"
M,0.38253638253638256,"Full training
33.81
23.65
22.39
20.40
Control
36.52
27.30
29.12
23.65
Low-rank pre-training with LoRA
47.44
34.17
36.60
57.11
Low-rank pre-training with ReLoRA
38.28
25.04
23.28
22.48"
M,0.38461538461538464,"No. of training tokens (billions)
1.2
2.6
6.8
6.8"
M,0.3866943866943867,"Table 2: Comparing perplexities between baseline methods and ReLoRA (lower is better). Control
has the same number of trainable parameters as low-rank training. Low-rank training is bold if it
outperforms the Control baseline. Notice that ReLoRA efficacy increases as the network size grows."
M,0.3887733887733888,"0.0
0.5
1.0
1.5
2.0
Singular Value 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
M,0.3908523908523909,Frequency
M,0.39293139293139295,Q Projections
M,0.39501039501039503,"ReLoRA
LoRA
Full-rank
training"
M,0.3970893970893971,"0.0
0.5
1.0
1.5
2.0
Singular Value 0.0 0.5 1.0 1.5 2.0 2.5"
K PROJECTIONS,0.3991683991683992,"3.0
K Projections"
K PROJECTIONS,0.40124740124740127,"ReLoRA
LoRA
Full-rank
training"
K PROJECTIONS,0.40332640332640335,"0.0
0.5
1.0
1.5
2.0
Singular Value 0.0 0.5 1.0 1.5 2.0 2.5"
V PROJECTIONS,0.40540540540540543,"3.0
V Projections"
V PROJECTIONS,0.4074844074844075,"ReLoRA
LoRA
Full-rank
training"
V PROJECTIONS,0.4095634095634096,"0.0
0.5
1.0
1.5
2.0
Singular Value 0.0 0.5 1.0 1.5 2.0 2.5"
DOWN PROJECTIONS,0.41164241164241167,"3.0
Down Projections"
DOWN PROJECTIONS,0.41372141372141374,"ReLoRA
LoRA
Full-rank
training"
DOWN PROJECTIONS,0.4158004158004158,"Figure 3: Singular values spectra of the weight difference between ReLoRA and LoRA at 5,000
iterations (warm start) and 20,000 iterations. ReLoRA exhibits a closer resemblance to full-rank
training singular values than LoRA, indicating its effectiveness in approximating full-rank behavior."
RESULTS,0.4178794178794179,"5
Results
160"
RESULTS,0.41995841995842,"Parameter-efficient pre-training
Our main results are resented in Table 2. ReLoRA significantly
161"
RESULTS,0.42203742203742206,"outperforms low-rank LoRA training demonstrating the effectiveness of our proposed modifications
162"
RESULTS,0.42411642411642414,"(ablated in Section 3). Furthermore, ReLoRA achieves similar performance to full-rank training, and
163"
RESULTS,0.4261954261954262,"the performance gap diminishes as network size increases.
164"
RESULTS,0.4282744282744283,"Interestingly, the only model in which ReLoRA couldn’t surpass the Control baseline was our smallest
165"
RESULTS,0.4303534303534304,"model with 60M parameters. This observation suggests that ReLoRA is particularly effective in
166"
RESULTS,0.43243243243243246,"improving the training of large networks, which aligns with our goal of developing a method that
167"
RESULTS,0.43451143451143454,"improves large-network training.
168"
RESULTS,0.4365904365904366,"High-rank training through low-rank updates
To determine whether ReLoRA performs a higher
169"
RESULTS,0.4386694386694387,"rank update than LoRA we plot the singular value spectrum of the difference between warm-start
170"
RESULTS,0.4407484407484408,"weights and the final weights for ReLoRA, LoRA, and full-rank training. Figure 3 illustrates
171"
RESULTS,0.44282744282744285,"significant qualitative differences between LoRA and ReLoRA for the singular values of WQ, WK,
172"
RESULTS,0.44490644490644493,"WV , and Wdown.
173"
RESULTS,0.446985446985447,"While most of the singular values for LoRA are zero (Figure 4) with a noticeable number of
174"
RESULTS,0.4490644490644491,"exceptionally high values above 1.5, ReLoRA exhibits a higher distribution mass between 0.1 and 1.0,
175"
RESULTS,0.45114345114345117,"reminiscent of full-rank training. This observation emphasizes the significance of high-rank updates
176"
RESULTS,0.45322245322245325,"and demonstrates the qualitative efficacy of ReLoRA, which accomplishes a high-rank update by
177"
RESULTS,0.4553014553014553,"performing multiple low-rank updates.
178"
ABLATION STUDIES,0.4573804573804574,"5.1
Ablation studies
179"
ABLATION STUDIES,0.4594594594594595,"We conduct ablation studies on all four crucial components of ReLoRA: restarts, jagged schedule,
180"
ABLATION STUDIES,0.46153846153846156,"optimizer resets, and warm starts, utilizing the 130M-sized model. The results are presented in
181"
ABLATION STUDIES,0.46361746361746364,"Table 3. In this section, we will focus on and analyze certain combinations of these components.
182"
ABLATION STUDIES,0.4656964656964657,"Restarts
Jagged Schedule
Optimizer Reset
Warm Start
Perplexity (↓)"
ABLATION STUDIES,0.4677754677754678,"×
×
×
×
34.17
✓
×
×
×
34.25
✓
×
✓
×
N/A
✓
✓
×
×
34.29
✓
✓
✓
×
29.77
×
×
×
✓
25.46
✓
✓
✓
✓
25.04"
ABLATION STUDIES,0.4698544698544699,"Table 3: Ablation studies of ReLoRA. Restarts and warm starts are essential for good performance.
Using restarts and optimizer reset without a jagged schedule causes the model to diverge."
ABLATION STUDIES,0.47193347193347196,"WQ
WK
WV
Wup
Wdown
0 200 400 600"
ABLATION STUDIES,0.47401247401247404,"Full-rank
Training
ReLoRA
LoRA"
ABLATION STUDIES,0.4760914760914761,Figure 4: Number of singular values < 0.1 in attention and FCN projection matrices.
ABLATION STUDIES,0.4781704781704782,"LoRA
ReLoRA, without the aforementioned components, is essentially equivalent to training
183"
ABLATION STUDIES,0.4802494802494803,"a low-rank network parameterized by LoRA. This approach yields remarkably high perplexity,
184"
ABLATION STUDIES,0.48232848232848236,"indicating that a simple matrix decomposition has significantly different training dynamics from
185"
ABLATION STUDIES,0.48440748440748443,"full-rank training.
186"
ABLATION STUDIES,0.4864864864864865,"Adding restarts and optimizer resets
ReLoRA, without a jagged schedule and optimizer reset,
187"
ABLATION STUDIES,0.4885654885654886,"performs similarly to LoRA because old optimizer states force the newly initialized parameters
188"
ABLATION STUDIES,0.49064449064449067,"into the same subspace as the prior weights, limiting the model’s capacity. However, doing a naive
189"
ABLATION STUDIES,0.49272349272349275,"optimizer reset with ReLoRA causes the model to diverge. A jagged schedule helps to stabilize
190"
ABLATION STUDIES,0.49480249480249483,"training and has a positive impact on the mixture. In our initial experiments, we also observed that a
191"
ABLATION STUDIES,0.4968814968814969,"combination of partial optimizer reset and jagged scheduler allows for a quicker warm-up, as low as
192"
ABLATION STUDIES,0.498960498960499,"50 steps, instead of hundreds of steps required when the optimizer is initialized from scratch.
193"
ABLATION STUDIES,0.501039501039501,"Warm start
The warm start shows the most significant improvement, dropping perplexity by
194"
ABLATION STUDIES,0.5031185031185031,"almost 10 points. To investigate whether post-warmup training contributes to the loss, we measured
195"
ABLATION STUDIES,0.5051975051975052,"the perplexity of the warmed-up network, which equals 27.03. It outperforms all low-rank methods
196"
ABLATION STUDIES,0.5072765072765073,"except for our final ReLoRA recipe but still demonstrates a significant difference from the final
197"
ABLATION STUDIES,0.5093555093555093,"network. This demonstrates the importance of early training, similar to the concept of the lottery
198"
ABLATION STUDIES,0.5114345114345115,"ticket hypothesis with rewinding [18].
199"
CONCLUSION,0.5135135135135135,"6
Conclusion
200"
CONCLUSION,0.5155925155925156,"In this paper, we investigated low-rank training techniques for large transformer language models.
201"
CONCLUSION,0.5176715176715176,"We first examined the limitations of a simple low-rank matrix factorization (LoRA) approach and
202"
CONCLUSION,0.5197505197505198,"observed that it struggles to effectively train high-performing transformer models. To address this
203"
CONCLUSION,0.5218295218295218,"issue, we proposed a novel method called ReLoRA, which leverages the rank of sum property to train
204"
CONCLUSION,0.5239085239085239,"a high-rank network through multiple low-rank updates. Similar to the lottery ticket hypothesis with
205"
CONCLUSION,0.525987525987526,"rewinding, ReLoRA employs a full-rank training warm start before transitioning to ReLoRA. Addi-
206"
CONCLUSION,0.5280665280665281,"tionally, ReLoRA introduces a merge-and-reinit (restart) strategy, a jagged learning rate scheduler,
207"
CONCLUSION,0.5301455301455301,"and partial optimizer resets, which collectively enhance the efficiency of ReLoRA and bring it closer
208"
CONCLUSION,0.5322245322245323,"to full-rank training, particularly in large networks. ReLoRA efficiency increases with the network
209"
CONCLUSION,0.5343035343035343,"size making it a viable candidate for multi-billion-scale training.
210"
CONCLUSION,0.5363825363825364,"We firmly believe that the development of low-rank training methods holds great promise for improv-
211"
CONCLUSION,0.5384615384615384,"ing the efficiency of training large language models and neural networks in general. Furthermore,
212"
CONCLUSION,0.5405405405405406,"low-rank training has the potential to provide valuable insights for the advancement of deep learning
213"
CONCLUSION,0.5426195426195426,"theories, aiding our understanding of neural network trainability through gradient descent and their
214"
CONCLUSION,0.5446985446985447,"exceptional generalization capabilities in the overparametrized regime.
215"
LIMITATIONS AND FUTURE WORK,0.5467775467775468,"7
Limitations and Future Work
216"
LIMITATIONS AND FUTURE WORK,0.5488565488565489,"Scaling beyond 350M
Due to limited computational resources, our experiments were constrained to
217"
LIMITATIONS AND FUTURE WORK,0.5509355509355509,"training language models with up to 350M parameters. Nonetheless, ReLoRA already demonstrates
218"
LIMITATIONS AND FUTURE WORK,0.553014553014553,"promising results at this scale. However, we anticipate its true potential will be realized in the 1B+
219"
LIMITATIONS AND FUTURE WORK,0.5550935550935551,"parameter region. Additionally, while the 350M model outperforms the Control baseline, it does not
220"
LIMITATIONS AND FUTURE WORK,0.5571725571725572,"continue the trend of narrowing the gap between ReLoRA and full-rank training. We attribute this to
221"
LIMITATIONS AND FUTURE WORK,0.5592515592515592,"suboptimal hyperparameter choice, which requires further investigation.
222"
LIMITATIONS AND FUTURE WORK,0.5613305613305614,"Furthermore, in 60-350M experiments, even though ReLoRA significantly reduces the number of
223"
LIMITATIONS AND FUTURE WORK,0.5634095634095634,"trainable parameters, we did not observe substantial improvements in memory and computation
224"
LIMITATIONS AND FUTURE WORK,0.5654885654885655,"for the networks of this size. To evaluate the efficiency of our current implementation at a larger
225"
LIMITATIONS AND FUTURE WORK,0.5675675675675675,"scale, we trained the 1.3B-parameter model for a small number of iterations to estimate memory and
226"
LIMITATIONS AND FUTURE WORK,0.5696465696465697,"compute improvements of ReLoRA. At this scale, we observe 30% memory consumption reduction
227"
LIMITATIONS AND FUTURE WORK,0.5717255717255717,"and 52% training throughput increase. We expect to observe even bigger improvements over the
228"
LIMITATIONS AND FUTURE WORK,0.5738045738045738,"full-training baseline for larger networks since the number of trainable parameters for ReLoRA,
229"
LIMITATIONS AND FUTURE WORK,0.5758835758835759,"similar to LoRA, increases at a much slower rate compared to the number of frozen parameters.
230"
LIMITATIONS AND FUTURE WORK,0.577962577962578,"ReLoRA implementation could be further improved by effectively utilizing gradient checkpointing
231"
LIMITATIONS AND FUTURE WORK,0.58004158004158,"for ReLoRA layers, custom backward functions, and converting frozen model weights to int8 or int4
232"
LIMITATIONS AND FUTURE WORK,0.5821205821205822,"quantized format [14].
233"
LIMITATIONS AND FUTURE WORK,0.5841995841995842,"Comparison to other low-rank training methods
A number of approaches to low-rank training
234"
LIMITATIONS AND FUTURE WORK,0.5862785862785863,"have been explored with other model architectures in earlier work [44, 49, 55]. Two aspects set our
235"
LIMITATIONS AND FUTURE WORK,0.5883575883575883,"work apart from these earlier efforts. First, the approach we propose performs high-rank updates
236"
LIMITATIONS AND FUTURE WORK,0.5904365904365905,"through low-rank training. Second, our work demonstrates competitiveness of the low-rank training
237"
LIMITATIONS AND FUTURE WORK,0.5925155925155925,"methods in large-scale transformer language models with 100M+ parameters.
238"
REFERENCES,0.5945945945945946,"References
239"
REFERENCES,0.5966735966735967,"[1] A. Aghajanyan, S. Gupta, and L. Zettlemoyer. Intrinsic dimensionality explains the effectiveness of
240"
REFERENCES,0.5987525987525988,"language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computa-
241"
REFERENCES,0.6008316008316008,"tional Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume
242"
REFERENCES,0.6029106029106029,"1: Long Papers), pages 7319–7328, Online, Aug. 2021. Association for Computational Linguistics. doi:
243"
REFERENCES,0.604989604989605,"10.18653/v1/2021.acl-long.568. URL https://aclanthology.org/2021.acl-long.568.
244"
REFERENCES,0.6070686070686071,"[2] A. Aghajanyan, L. Yu, A. Conneau, W.-N. Hsu, K. Hambardzumyan, S. Zhang, S. Roller, N. Goyal,
245"
REFERENCES,0.6091476091476091,"O. Levy, and L. Zettlemoyer. Scaling laws for generative mixed-modal language models, 2023.
246"
REFERENCES,0.6112266112266113,"[3] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,
247"
REFERENCES,0.6133056133056133,"M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural
248"
REFERENCES,0.6153846153846154,"Information Processing Systems, 35:23716–23736, 2022.
249"
REFERENCES,0.6174636174636174,"[4] Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization. In
250"
REFERENCES,0.6195426195426196,"K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine
251"
REFERENCES,0.6216216216216216,"Learning, volume 97 of Proceedings of Machine Learning Research, pages 242–252. PMLR, 09–15 Jun
252"
REFERENCES,0.6237006237006237,"2019. URL https://proceedings.mlr.press/v97/allen-zhu19a.html.
253"
REFERENCES,0.6257796257796258,"[5] M. Belkin, D. J. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice and the
254"
REFERENCES,0.6278586278586279,"classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116:15849 – 15854,
255"
REFERENCES,0.6299376299376299,"2018.
256"
REFERENCES,0.632016632016632,"[6] S. Bhojanapalli, C. Yun, A. S. Rawat, S. Reddi, and S. Kumar. Low-rank bottleneck in multi-head attention
257"
REFERENCES,0.6340956340956341,"models. In International Conference on Machine Learning, pages 864–873. PMLR, 2020.
258"
REFERENCES,0.6361746361746362,"[7] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B.
259"
REFERENCES,0.6382536382536382,"Lespiau, B. Damoc, A. Clark, D. De Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang,
260"
REFERENCES,0.6403326403326404,"L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan,
261"
REFERENCES,0.6424116424116424,"J. Rae, E. Elsen, and L. Sifre. Improving language models by retrieving from trillions of tokens. In
262"
REFERENCES,0.6444906444906445,"K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the
263"
REFERENCES,0.6465696465696466,"39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning
264"
REFERENCES,0.6486486486486487,"Research, pages 2206–2240. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/
265"
REFERENCES,0.6507276507276507,"borgeaud22a.html.
266"
REFERENCES,0.6528066528066528,"[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
267"
REFERENCES,0.6548856548856549,"A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,
268"
REFERENCES,0.656964656964657,"C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,
269"
REFERENCES,0.659043659043659,"A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle,
270"
REFERENCES,0.6611226611226612,"M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing
271"
REFERENCES,0.6632016632016632,"Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.
272"
REFERENCES,0.6652806652806653,"neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
273"
REFERENCES,0.6673596673596673,"[9] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,
274"
REFERENCES,0.6694386694386695,"G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray,
275"
REFERENCES,0.6715176715176715,"N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
276"
REFERENCES,0.6735966735966736,"M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang,
277"
REFERENCES,0.6756756756756757,"I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra,
278"
REFERENCES,0.6777546777546778,"E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew,
279"
REFERENCES,0.6798336798336798,"D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on
280"
REFERENCES,0.681912681912682,"code. 2021.
281"
REFERENCES,0.683991683991684,"[10] J. Cho, J. Lei, H. Tan, and M. Bansal. Unifying vision-and-language tasks via text generation. In
282"
REFERENCES,0.6860706860706861,"International Conference on Machine Learning, pages 1931–1942. PMLR, 2021.
283"
REFERENCES,0.6881496881496881,"[11] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlós, P. Hawkins, J. Q. Davis,
284"
REFERENCES,0.6902286902286903,"A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller. Rethinking attention with performers.
285"
REFERENCES,0.6923076923076923,"In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7,
286"
REFERENCES,0.6943866943866944,"2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH.
287"
REFERENCES,0.6964656964656964,"[12] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,
288"
REFERENCES,0.6985446985446986,"S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. M. Shazeer,
289"
REFERENCES,0.7006237006237006,"V. Prabhakaran, E. Reif, N. Du, B. C. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari,
290"
REFERENCES,0.7027027027027027,"P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. García, V. Misra, K. Robinson,
291"
REFERENCES,0.7047817047817048,"L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal,
292"
REFERENCES,0.7068607068607069,"M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee,
293"
REFERENCES,0.7089397089397089,"Z. Zhou, X. Wang, B. Saeta, M. Díaz, O. Firat, M. Catasta, J. Wei, K. S. Meier-Hellstern, D. Eck, J. Dean,
294"
REFERENCES,0.7110187110187111,"S. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.
295"
REFERENCES,0.7130977130977131,"[13] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Re. Flashattention: Fast and memory-efficient exact attention
296"
REFERENCES,0.7151767151767152,"with IO-awareness. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural
297"
REFERENCES,0.7172557172557172,"Information Processing Systems, 2022. URL https://openreview.net/forum?id=H4DqfPSibmx.
298"
REFERENCES,0.7193347193347194,"[14] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. GPT3.int8(): 8-bit matrix multiplication for
299"
REFERENCES,0.7214137214137214,"transformers at scale. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural
300"
REFERENCES,0.7234927234927235,"Information Processing Systems, 2022. URL https://openreview.net/forum?id=dXiGWqBoxaD.
301"
REFERENCES,0.7255717255717256,"[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers
302"
REFERENCES,0.7276507276507277,"for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the
303"
REFERENCES,0.7297297297297297,"Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short
304"
REFERENCES,0.7318087318087318,"Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
305"
REFERENCES,0.7338877338877339,"doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.
306"
REFERENCES,0.735966735966736,"[16] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple
307"
REFERENCES,0.738045738045738,"and efficient sparsity. J. Mach. Learn. Res., 23(1), jan 2022. ISSN 1532-4435.
308"
REFERENCES,0.7401247401247402,"[17] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In
309"
REFERENCES,0.7422037422037422,"International Conference on Learning Representations, 2019. URL https://openreview.net/forum?
310"
REFERENCES,0.7442827442827443,"id=rJl-b3RcF7.
311"
REFERENCES,0.7463617463617463,"[18] J. Frankle, G. Karolina Dziugaite, D. M. Roy, and M. Carbin. Stabilizing the lottery ticket hypothesis.
312"
REFERENCES,0.7484407484407485,"arXiv e-prints, pages arXiv–1903, 2019.
313"
REFERENCES,0.7505197505197505,"[19] B. Ghorbani, O. Firat, M. Freitag, A. Bapna, M. Krikun, X. García, C. Chelba, and C. Cherry. Scaling
314"
REFERENCES,0.7525987525987526,"laws for neural machine translation. ArXiv, abs/2109.07740, 2021.
315"
REFERENCES,0.7546777546777547,"[20] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on
316"
REFERENCES,0.7567567567567568,"imagenet classification. CoRR, abs/1502.01852, 2015. URL http://arxiv.org/abs/1502.01852.
317"
REFERENCES,0.7588357588357588,"[21] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE Conference
318"
REFERENCES,0.760914760914761,"on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.
319"
REFERENCES,0.762993762993763,"[22] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de las Casas, L. A.
320"
REFERENCES,0.7650727650727651,"Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc,
321"
REFERENCES,0.7671517671517671,"A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. W. Rae, and L. Sifre. An empirical analysis
322"
REFERENCES,0.7692307692307693,"of compute-optimal large language model training. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho,
323"
REFERENCES,0.7713097713097713,"editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/
324"
REFERENCES,0.7733887733887734,"forum?id=iBBcRUlOAPR.
325"
REFERENCES,0.7754677754677755,"[23] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank
326"
REFERENCES,0.7775467775467776,"adaptation of large language models. In International Conference on Learning Representations, 2022.
327"
REFERENCES,0.7796257796257796,"URL https://openreview.net/forum?id=nZeVKeeFYf9.
328"
REFERENCES,0.7817047817047817,"[24] Y. Idelbayev and M. A. Carreira-Perpinan. Low-rank compression of neural nets: Learning the rank of
329"
REFERENCES,0.7837837837837838,"each layer. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
330"
REFERENCES,0.7858627858627859,"8046–8056, 2020. doi: 10.1109/CVPR42600.2020.00807.
331"
REFERENCES,0.7879417879417879,"[25] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
332"
REFERENCES,0.7900207900207901,"networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems,
333"
REFERENCES,0.7920997920997921,"NIPS’18, page 8580–8589, Red Hook, NY, USA, 2018. Curran Associates Inc.
334"
REFERENCES,0.7941787941787942,"[26] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low
335"
REFERENCES,0.7962577962577962,"rank expansions. In Proceedings of the British Machine Vision Conference. BMVA Press, 2014. doi:
336"
REFERENCES,0.7983367983367984,"http://dx.doi.org/10.5244/C.28.88.
337"
REFERENCES,0.8004158004158004,"[27] J. Kaplan, S. McCandlish, T. J. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,
338"
REFERENCES,0.8024948024948025,"and D. Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.
339"
REFERENCES,0.8045738045738046,"[28] U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization:
340"
REFERENCES,0.8066528066528067,"Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL
341"
REFERENCES,0.8087318087318087,"https://openreview.net/forum?id=HklBjCEKvH.
342"
REFERENCES,0.8108108108108109,"[29] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015.
343"
REFERENCES,0.8128898128898129,"[30] T. Klug and R. Heckel. Scaling laws for deep learning based image reconstruction. In The Eleventh
344"
REFERENCES,0.814968814968815,"International Conference on Learning Representations, 2023. URL https://openreview.net/forum?
345"
REFERENCES,0.817047817047817,"id=op-ceGueqc4.
346"
REFERENCES,0.8191268191268192,"[31] O. Kovaleva, A. Romanov, A. Rogers, and A. Rumshisky. Revealing the dark secrets of bert. In Proceedings
347"
REFERENCES,0.8212058212058212,"of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International
348"
REFERENCES,0.8232848232848233,"Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4365–4374, 2019.
349"
REFERENCES,0.8253638253638254,"[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
350"
REFERENCES,0.8274428274428275,"networks. In F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information
351"
REFERENCES,0.8295218295218295,"Processing Systems, volume 25. Curran Associates, Inc., 2012. URL https://proceedings.neurips.
352"
REFERENCES,0.8316008316008316,"cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.
353"
REFERENCES,0.8336798336798337,"[33] V. Lialin, V. Deshpande, and A. Rumshisky. Scaling down to scale up: A guide to parameter-efficient
354"
REFERENCES,0.8357588357588358,"fine-tuning, 2023.
355"
REFERENCES,0.8378378378378378,"[34] R. Lin, C.-Y. Ko, Z. He, C. Chen, Y. Cheng, H. Yu, G. Chesi, and N. Wong. Hotcake: Higher order tucker
356"
REFERENCES,0.83991683991684,"articulated kernels for deeper cnn compression. In 2020 IEEE 15th International Conference on Solid-State
357"
REFERENCES,0.841995841995842,"& Integrated Circuit Technology (ICSICT), pages 1–4, 2020. doi: 10.1109/ICSICT49897.2020.9278257.
358"
REFERENCES,0.8440748440748441,"[35] J. Lu, C. Clark, R. Zellers, R. Mottaghi, and A. Kembhavi. UNIFIED-IO: A unified model for vision,
359"
REFERENCES,0.8461538461538461,"language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations,
360"
REFERENCES,0.8482328482328483,"2023. URL https://openreview.net/forum?id=E01k9048soZ.
361"
REFERENCES,0.8503118503118503,"[36] R. K. mahabadi, J. Henderson, and S. Ruder. Compacter: Efficient low-rank hypercomplex adapter layers.
362"
REFERENCES,0.8523908523908524,"In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information
363"
REFERENCES,0.8544698544698545,"Processing Systems, 2021. URL https://openreview.net/forum?id=bqGK5PyI6-N.
364"
REFERENCES,0.8565488565488566,"[37] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston,
365"
REFERENCES,0.8586278586278586,"O. Kuchaiev, G. Venkatesh, and H. Wu. Mixed precision training. In International Conference on
366"
REFERENCES,0.8607068607068608,"Learning Representations, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.
367"
REFERENCES,0.8627858627858628,"[38] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever. Deep double descent: where bigger
368"
REFERENCES,0.8648648648648649,"models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021, 2019.
369"
REFERENCES,0.8669438669438669,"[39] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by
370"
REFERENCES,0.8690228690228691,"generative pre-training. 2018.
371"
REFERENCES,0.8711018711018711,"[40] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised
372"
REFERENCES,0.8731808731808732,"multitask learners. 2019.
373"
REFERENCES,0.8752598752598753,"[41] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring
374"
REFERENCES,0.8773388773388774,"the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research,
375"
REFERENCES,0.8794178794178794,"21(140):1–67, 2020. URL http://jmlr.org/papers/v21/20-074.html.
376"
REFERENCES,0.8814968814968815,"[42] A. Raganato, Y. Scherrer, and J. Tiedemann. Fixed encoder self-attention patterns in transformer-based
377"
REFERENCES,0.8835758835758836,"machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages
378"
REFERENCES,0.8856548856548857,"556–568, 2020.
379"
REFERENCES,0.8877338877338877,"[43] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion
380"
REFERENCES,0.8898128898128899,"parameter models. In SC20: International Conference for High Performance Computing, Networking,
381"
REFERENCES,0.8918918918918919,"Storage and Analysis, pages 1–16, 2020. doi: 10.1109/SC41405.2020.00024.
382"
REFERENCES,0.893970893970894,"[44] S. Schotthöfer, E. Zangrando, J. Kusch, G. Ceruti, and F. Tudisco. Low-rank lottery tickets: finding efficient
383"
REFERENCES,0.896049896049896,"low-rank neural networks via matrix differential equations. In S. Koyejo, S. Mohamed, A. Agarwal,
384"
REFERENCES,0.8981288981288982,"D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35,
385"
REFERENCES,0.9002079002079002,"pages 20051–20063. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_
386"
REFERENCES,0.9022869022869023,"files/paper/2022/file/7e98b00eeafcdaeb0c5661fb9355be3a-Paper-Conference.pdf.
387"
REFERENCES,0.9043659043659044,"[45] N. Shazeer. Glu variants improve transformer, 2020.
388"
REFERENCES,0.9064449064449065,"[46] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In
389"
REFERENCES,0.9085239085239085,"Y. Bengio and Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015,
390"
REFERENCES,0.9106029106029107,"San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/
391"
REFERENCES,0.9126819126819127,"abs/1409.1556.
392"
REFERENCES,0.9147609147609148,"[47] S. P. Singh, G. Bachmann, and T. Hofmann. Analytic insights into structure and rank of neural network
393"
REFERENCES,0.9168399168399168,"hessian maps. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural
394"
REFERENCES,0.918918918918919,"Information Processing Systems, 2021. URL https://openreview.net/forum?id=otDgw7LM7Nn.
395"
REFERENCES,0.920997920997921,"[48] J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding.
396"
REFERENCES,0.9230769230769231,"ArXiv, abs/2104.09864, 2021.
397"
REFERENCES,0.9251559251559252,"[49] Y. Sui, M. Yin, W. Yang, Y. Gong, J. Xiao, H. Phan, D. Ding, X. Xu, S. Liu, Z. Chen, and B. Yuan. ELRT:
398"
REFERENCES,0.9272349272349273,"Towards efficient low-rank training for compact neural networks, 2023. URL https://openreview.
399"
REFERENCES,0.9293139293139293,"net/forum?id=TC39w69m8bB.
400"
REFERENCES,0.9313929313929314,"[50] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro,
401"
REFERENCES,0.9334719334719335,"F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language
402"
REFERENCES,0.9355509355509356,"models. arXiv preprint arXiv:2302.13971, 2023.
403"
REFERENCES,0.9376299376299376,"[51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
404"
REFERENCES,0.9397089397089398,"Attention is all you need. Advances in neural information processing systems, 30, 2017.
405"
REFERENCES,0.9417879417879418,"[52] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-attention with linear complexity.
406"
REFERENCES,0.9438669438669439,"arXiv preprint arXiv:2006.04768, 2020.
407"
REFERENCES,0.9459459459459459,"[53] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou,
408"
REFERENCES,0.9480249480249481,"D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus. Emergent abilities
409"
REFERENCES,0.9501039501039501,"of large language models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL
410"
REFERENCES,0.9521829521829522,"https://openreview.net/forum?id=yzkSU5zdwD. Survey Certification.
411"
REFERENCES,0.9542619542619543,"[54] J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou.
412"
REFERENCES,0.9563409563409564,"Chain of thought prompting elicits reasoning in large language models. In A. H. Oh, A. Agarwal,
413"
REFERENCES,0.9584199584199584,"D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. URL
414"
REFERENCES,0.9604989604989606,"https://openreview.net/forum?id=_VjQlMeSB_J.
415"
REFERENCES,0.9625779625779626,"[55] H. Yang, M. Tang, W. Wen, F. Yan, D. Hu, A. Li, H. H. Li, and Y. Chen. Learning low-rank deep neural
416"
REFERENCES,0.9646569646569647,"networks via singular vector orthogonality regularization and singular value sparsification. 2020 IEEE/CVF
417"
REFERENCES,0.9667359667359667,"Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 2899–2908, 2020.
418"
REFERENCES,0.9688149688149689,"[56] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners are
419"
REFERENCES,0.9708939708939709,"image-text foundation models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL
420"
REFERENCES,0.972972972972973,"https://openreview.net/forum?id=Ee277P3AYC.
421"
REFERENCES,0.975051975051975,"[57] X. Yuan, P. H. P. Savarese, and M. Maire.
Growing efficient deep networks by structured contin-
422"
REFERENCES,0.9771309771309772,"uous sparsification. In International Conference on Learning Representations, 2021. URL https:
423"
REFERENCES,0.9792099792099792,"//openreview.net/forum?id=wb3wxCObbRT.
424"
REFERENCES,0.9812889812889813,"[58] B. Zhang and R. Sennrich.
Root mean square layer normalization.
In H. Wallach, H. Larochelle,
425"
REFERENCES,0.9833679833679834,"A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
426"
REFERENCES,0.9854469854469855,"Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_
427"
REFERENCES,0.9875259875259875,"files/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf.
428"
REFERENCES,0.9896049896049897,"[59] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals.
Understanding deep learning requires
429"
REFERENCES,0.9916839916839917,"rethinking generalization.
In International Conference on Learning Representations, 2017.
URL
430"
REFERENCES,0.9937629937629938,"https://openreview.net/forum?id=Sy8gdB9xx.
431"
REFERENCES,0.9958419958419958,"[60] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires
432"
REFERENCES,0.997920997920998,"rethinking generalization. Communications of the ACM, 64:107 – 115, 2021.
433"
