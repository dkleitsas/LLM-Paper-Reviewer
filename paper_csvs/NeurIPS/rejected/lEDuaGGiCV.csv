Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007358351729212656,"Large Language Models (LLMs) have made significant progress in assisting
1"
ABSTRACT,0.0014716703458425313,"users to query databases in natural language. While LLM-based techniques
2"
ABSTRACT,0.002207505518763797,"provide state-of-the-art results on many standard benchmarks, their perfor-
3"
ABSTRACT,0.0029433406916850625,"mance significantly drops when applied to large enterprise databases. The
4"
ABSTRACT,0.003679175864606328,"reason is that these databases have a large number of tables with complex
5"
ABSTRACT,0.004415011037527594,"relationships that are challenging for LLMs to reason about. We analyze
6"
ABSTRACT,0.0051508462104488595,"challenges that LLMs face in these settings and propose a new solution that
7"
ABSTRACT,0.005886681383370125,"combines the power of LLMs in understanding questions with automated
8"
ABSTRACT,0.006622516556291391,"reasoning techniques to handle complex database constraints. Based on these
9"
ABSTRACT,0.007358351729212656,"ideas, we have developed a new framework that outperforms state-of-the-art
10"
ABSTRACT,0.008094186902133923,"techniques in zero-shot text-to-SQL on complex benchmarks.
11"
INTRODUCTION,0.008830022075055188,"1
Introduction
12"
INTRODUCTION,0.009565857247976454,"Large Language Models (LLMs) have significantly enhanced AI agents’ capacity to assist
13"
INTRODUCTION,0.010301692420897719,"humans in a variety of important tasks, including co-pilot programming [Chen et al., 2021,
14"
INTRODUCTION,0.011037527593818985,"GitHub, Inc., 2021], program verification [Wu et al., 2024, Chakraborty et al., 2023], and
15"
INTRODUCTION,0.01177336276674025,"math problem solving [Zhou et al., 2024]. One of the fastest-growing areas in this space
16"
INTRODUCTION,0.012509197939661517,"is the development of LLM-based assistants for querying SQL databases. In this task, a
17"
INTRODUCTION,0.013245033112582781,"user poses a question to a database in natural language. The agent’s goal is to generate an
18"
INTRODUCTION,0.013980868285504048,"SQL query that, when executed against the database, answers the user’s question. Such
19"
INTRODUCTION,0.014716703458425313,"assistance enables users with different levels of expertise to effectively analyze their data.
20"
INTRODUCTION,0.01545253863134658,"Recently, LLM-based solutions have made significant progress in addressing the text-to-SQL
21"
INTRODUCTION,0.016188373804267846,"problem [Gao et al., 2024, Li et al., 2024a]. While GPT-based methods have quickly reached
22"
INTRODUCTION,0.01692420897718911,"near-human performance on academic benchmarks, like Spider [Yu et al., 2018], they struggle
23"
INTRODUCTION,0.017660044150110375,"to provide high-quality user assistance on large industrial databases [Sequeda et al., 2023, Li
24"
INTRODUCTION,0.01839587932303164,"et al., 2023]. One of the core challenges is that industrial databases model many objects with
25"
INTRODUCTION,0.01913171449595291,"complex relationships between them. To transform a natural language question into an SQL
26"
INTRODUCTION,0.019867549668874173,"query, the LLM must effectively reason about these intricate relationships, which is highly
27"
INTRODUCTION,0.020603384841795438,"non-trivial for LLM models. Interestingly, we found that gpt4 can even indicate in some
28"
INTRODUCTION,0.021339220014716703,"cases that it needs help with logical reasoning on complex databases. Here is a common
29"
INTRODUCTION,0.02207505518763797,"gpt4 output message on a question that requires multiple joins from ACME insurance
30"
INTRODUCTION,0.022810890360559236,"database [Sequeda et al., 2023]: ‘This join may need adjustment based on the actual logic of
31"
INTRODUCTION,0.0235467255334805,"relating claims to policy coverage details.’. While we do provide the database schema as part
32"
INTRODUCTION,0.024282560706401765,"of the input, it is still challenging for LLMs to formally reason about database logic.
33"
INTRODUCTION,0.025018395879323033,"In this work, we propose a new text-to-SQL framework, Lucy, designed for large databases
34"
INTRODUCTION,0.025754231052244298,"with complex relationships between objects. Our main underlying idea is to combine the
35"
INTRODUCTION,0.026490066225165563,"ability of LLM models to effectively relate user questions to database objects with the power
36"
INTRODUCTION,0.027225901398086828,"of automated reasoning to analyze relationships between these objects. The Lucy workflow
37"
INTRODUCTION,0.027961736571008096,"consists of three high-level steps. First, upon receiving a user’s question, we identify the
38"
INTRODUCTION,0.02869757174392936,"relevant objects and their attributes in the target database. In the second step, we employ
39"
INTRODUCTION,0.029433406916850625,"an automated reasoner to build a view that joins the relevant tables based on relational
40"
INTRODUCTION,0.03016924208977189,"constraints defined by the database schema. This view contains all the necessary information
41"
INTRODUCTION,0.03090507726269316,Star A
INTRODUCTION,0.03164091243561442,"Star B
Snowflake"
INTRODUCTION,0.03237674760853569,"Retention
Client
[id, name, loc_id ]"
INTRODUCTION,0.033112582781456956,"Location
[id ]"
INTRODUCTION,0.03384841795437822,"Datacenter
[id, name, loc_id ]"
INTRODUCTION,0.034584253127299486,"Gift
Bonus"
INTRODUCTION,0.03532008830022075,"Payment
[id ]"
INTRODUCTION,0.036055923473142015,"PayAmount
[id, pay_id, amount ]"
INTRODUCTION,0.03679175864606328,"Tax
[payamt_id ]"
INTRODUCTION,0.037527593818984545,"Supercharge
[payamt_id ]"
INTRODUCTION,0.03826342899190582,"Income
[payamt_id ]"
INTRODUCTION,0.03899926416482708,"Compute,
[id, dc_id ]"
INTRODUCTION,0.039735099337748346,"ResourcePool
[id ]"
INTRODUCTION,0.04047093451066961,"Config
[id, rspool_id ]"
INTRODUCTION,0.041206769683590876,"Runtime
[id, rspool_id ]"
INTRODUCTION,0.04194260485651214,"cMemory
[config_id,
overheadlimit ]"
INTRODUCTION,0.042678440029433405,"cCPU
[config_id,
overheadlimit ]"
INTRODUCTION,0.04341427520235467,"rMemory
[runtime_id,
overallusage ]"
INTRODUCTION,0.04415011037527594,"rCPU
[runtime_id,
overallusage ] m:m"
INTRODUCTION,0.044885945548197206,RsPool2Client
INTRODUCTION,0.04562178072111847,Figure 1: Objects and their relations in the database ddo.
INTRODUCTION,0.046357615894039736,"to answer the user’s questions. In the third step, we construct a query targeting this view to
42"
INTRODUCTION,0.047093451066961,"produce an answer for the user. Our contributions are summarized as follows:
43"
INTRODUCTION,0.047829286239882265,"• We propose a text-to-SQL framework Lucy capable of querying large industrial
44"
INTRODUCTION,0.04856512141280353,"databases. To the best of our knowledge, Lucy is the first framework designed to
45"
INTRODUCTION,0.049300956585724795,"support logical reasoning in the context of the text-to-SQL problem.
46"
INTRODUCTION,0.05003679175864607,"• Lucy offers several advantages:
47"
INTRODUCTION,0.05077262693156733,"– alleviates the need for complex reasoning from a LLM, allowing it to focus on
48"
INTRODUCTION,0.051508462104488596,"tasks where it currently excels,
49"
INTRODUCTION,0.05224429727740986,"– supports modeling and reasoning about complex, commonly used design patterns
50"
INTRODUCTION,0.052980132450331126,"to model relationships, like many-to-many, Star, and Snowflake,
51"
INTRODUCTION,0.05371596762325239,"– its modular workflow allows for effective debugging of failures,
52"
INTRODUCTION,0.054451802796173655,"– performs zero-shot generation and does not require fine-tuning of LLMs.
53"
INTRODUCTION,0.05518763796909492,"• Our experimental results demonstrate significant performance improvements on
54"
INTRODUCTION,0.05592347314201619,"several standard benchmarks as well as introduced large benchmarks. We also
55"
INTRODUCTION,0.05665930831493746,"demonstrate the debugging capabilities of Lucy.
56"
MOTIVATION,0.05739514348785872,"2
Motivation
57"
MOTIVATION,0.058130978660779986,"To provide high-quality user assistance in text-to-SQL tasks, we face two types of challenges.
58"
MOTIVATION,0.05886681383370125,"The first type of challenge comes from the formulation of the user’s question. A question
59"
MOTIVATION,0.059602649006622516,"can be poorly specified, ambiguous, or require additional knowledge that is not present in
60"
MOTIVATION,0.06033848417954378,"the question. For example, the user might ask to list clients eligible for a loan; however, the
61"
MOTIVATION,0.061074319352465045,"eligibility criteria are not present in the question [Li et al., 2023, 2024b]. The second class is
62"
MOTIVATION,0.06181015452538632,"related to the complexity of the queried database that can have a large number of tables
63"
MOTIVATION,0.06254598969830757,"with complex relations between them [Sequeda et al., 2023, Li et al., 2023]. In this work, we
64"
MOTIVATION,0.06328182487122884,"focus on the second class. One approach to deal with complex relationships is to introduce
65"
MOTIVATION,0.0640176600441501,"an intermediate layer, like a knowledge graph or ontology structure, that contains rich
66"
MOTIVATION,0.06475349521707138,"information about the underlying database. Then, LLMs generate queries to this knowledge
67"
MOTIVATION,0.06548933038999265,"graph using specialized languages, e.g., SPARQL, [Sequeda et al., 2023]. In turn, these
68"
MOTIVATION,0.06622516556291391,"queries can be automatically translated to SQL. While this approach does show promise, it
69"
MOTIVATION,0.06696100073583518,"does not alleviate the core issue: an LLM is still expected to reason about complex relations
70"
MOTIVATION,0.06769683590875644,"between objects in this intermediate representation. Moreover, such a rich intermediate layer,
71"
MOTIVATION,0.0684326710816777,"like an ontology, might not be easy to obtain for a database. Other standard techniques,
72"
MOTIVATION,0.06916850625459897,"like additional training, multi-shot or fine-tuning, also rely on LLMs to perform constrained
73"
MOTIVATION,0.06990434142752024,"reasoning steps [Gao et al., 2023, Pourreza and Rafiei, 2024, Gao et al., 2024]. To the best of
74"
MOTIVATION,0.0706401766004415,"our knowledge, dealing with complex relationships in text-to-SQL remains an open problem.
75"
MOTIVATION,0.07137601177336277,"In order to isolate the underlying challenges in this problem, we created an example database
76"
MOTIVATION,0.07211184694628403,"Q1: List customers who use datacenters with names
starting with ‘dev’. Output clients and datacenters
names."
MOTIVATION,0.0728476821192053,"/* GPT4
generated
SQL */:
select Client.name, Datacenter.name
from Client
join Location on Location.id = Client.loc_id
join Datacenter on Location.id = Datacenter.loc_id
where Datacenter.name like 'dev%'"
MOTIVATION,0.07358351729212656,"/* Correct
SQL*/
select Client.name, Datacenter.name
from Datacenter
join Compute on Datacenter.id = Compute.dc_id
join ResourcePool on"
MOTIVATION,0.07431935246504782,"Compute.id = ResourcePool.compute_id
join RsPool2Client on"
MOTIVATION,0.07505518763796909,"ResourcePool.id = RsPool2Client.rspool_id
join Client on Client.id = RsPool2Client.client_id
where Datacenter.name like 'dev%'"
MOTIVATION,0.07579102281089035,"Q2: List resource pools names with CPU overhead
limit greater than runtime overall usage by 100."
MOTIVATION,0.07652685798381163,"/* GPT4
generated
SQL */:
select ResourcePool.name
from ResourcePool
join rCPU on"
MOTIVATION,0.0772626931567329,"ResourcePool.runtime_id = rCPU.runtime_id
join cCPU on"
MOTIVATION,0.07799852832965416,"ResourcePool.config_id = cCPU.config_id
where cCPU.overheadlimit > rCPU.overallusage + 100"
MOTIVATION,0.07873436350257543,"/* Correct
SQL */:
select
distinct ResourcePool.name
from
ResourcePool
left
join Config on
ResourcePool.id = Config.rspool_id
left
join cCPU on Config.id = cCPU.config_id
left
join Runtime on
ResourcePool.id = Runtime.rspool_id
left
join rCPU on Runtime.id = rCPU.runtime_id
where cCPU.overheadlimit > rCPU.overallusage + 100
Table 1: User’s questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red."
MOTIVATION,0.07947019867549669,"that covers standard relationship patterns adopted in industry and academia. We identified a
77"
MOTIVATION,0.08020603384841796,"set of simple and clearly formulated questions and demonstrated that even on this simplified
78"
MOTIVATION,0.08094186902133922,"schema and clear questions, state-of-the-art LLMs struggle to assist the user.
79"
DATABASE DESCRIPTION,0.08167770419426049,"2.1
Database description
80"
DATABASE DESCRIPTION,0.08241353936718175,"We describe a minimal example database schema that contains basic relations, like 1:1
81"
DATABASE DESCRIPTION,0.08314937454010302,"and 1:m, and more advanced relationship patterns, like m:m and Star, and analyze the
82"
DATABASE DESCRIPTION,0.08388520971302428,"performance of LLMs on this schema (See Appendix A for relational database definitions).
83"
DATABASE DESCRIPTION,0.08462104488594555,"Suppose a business sells cloud compute resources to customers and uses a database, ddo,
84"
DATABASE DESCRIPTION,0.08535688005886681,"to manage its Day-to-Day Operations. Figure 1 shows objects’ corresponding tables, their
85"
DATABASE DESCRIPTION,0.08609271523178808,"relationships, and a subset of attributes. In particular, each table has a primary key, e.g.,
86"
DATABASE DESCRIPTION,0.08682855040470934,"Location.id, and might have foreign keys to refer to another table, e.g., Client refers
87"
DATABASE DESCRIPTION,0.0875643855776306,"to Location using Client.loc_id. All attributes relevant to our examples are shown in
88"
DATABASE DESCRIPTION,0.08830022075055188,"Figure 1 with self-explanatory names. ddo manages payments (Payment) and marketing
89"
DATABASE DESCRIPTION,0.08903605592347315,"retention strategies (Retention) for clients (Client) and resources (ResourcePool)
90"
DATABASE DESCRIPTION,0.08977189109639441,"in datacenters (Datacenter). This example is in part inspired by the VMware vSphere
91"
DATABASE DESCRIPTION,0.09050772626931568,"data model (discussed in Section 5). The full data model contains hundreds of types of
92"
DATABASE DESCRIPTION,0.09124356144223694,"resources that form deep tree-like structures [Managed Object, 2024]. Next, we consider
93"
DATABASE DESCRIPTION,0.09197939661515821,"how relationships between objects are modeled in ddo. Figure 1 already defines basic
94"
DATABASE DESCRIPTION,0.09271523178807947,"relationships, including 1:1 (dotted edges) and 1:m (solid edges).
95"
DATABASE DESCRIPTION,0.09345106696100074,"Many-to-many (m:m). Client and ResourcePool are related via a m:m relationship
96"
DATABASE DESCRIPTION,0.094186902133922,"(the dashed edge) meaning that a client might use multiple resource pools and one resource
97"
DATABASE DESCRIPTION,0.09492273730684327,"pool can serve multiple clients. The table RsPool2Client models this relation.
98"
DATABASE DESCRIPTION,0.09565857247976453,"Star. A Star pattern is a type of database schema composed of a single, central fact table
99"
DATABASE DESCRIPTION,0.0963944076526858,"surrounded by dimension tables. There are two groups of objects connected in a Star
100"
DATABASE DESCRIPTION,0.09713024282560706,"patterns in our example. Star A keeps track of retention marketing strategies for each
101"
DATABASE DESCRIPTION,0.09786607799852833,"client that can be either Gift or/and Bonus. Star B records clients’ payments (Payment).
102"
DATABASE DESCRIPTION,0.09860191317144959,"Payments’ amounts are stored in the PayAmount table. Each amount can be exactly one
103"
DATABASE DESCRIPTION,0.09933774834437085,"of three types: Tax, Supercharge, and Income.
104"
DATABASE DESCRIPTION,0.10007358351729213,"Snowflake. A Snowflake schema consists of one fact table connected to many dimension
105"
DATABASE DESCRIPTION,0.1008094186902134,"tables, which can be connected to other dimension tables through a many-to-one relationship.
106"
DATABASE DESCRIPTION,0.10154525386313466,"In ddo, database resource pools are modeled using the snowflake pattern. Each resource
107"
DATABASE DESCRIPTION,0.10228108903605593,"pool has configurations (Config) and snapshots of the current usage (Runtime). Config
108"
DATABASE DESCRIPTION,0.10301692420897719,"and Runtime have two children nodes each to define CPU and memory properties.
109"
DATABASE DESCRIPTION,0.10375275938189846,"Lookup. A lookup table is a table that contains descriptions and code values used by
110"
DATABASE DESCRIPTION,0.10448859455481972,"multiple tables, e.g., zip codes, country names. etc. In ddo, Location is a lookup table
111"
DATABASE DESCRIPTION,0.10522442972774099,"that stores geo-location related data for quick access.
112"
DATABASE DESCRIPTION,0.10596026490066225,"Question
MatchTables
GenerateView
QueryView"
DATABASE DESCRIPTION,0.10669610007358352,dbModel
DATABASE DESCRIPTION,0.10743193524650478,Relevant
DATABASE DESCRIPTION,0.10816777041942605,tables (RT )
DATABASE DESCRIPTION,0.10890360559234731,Summary
DATABASE DESCRIPTION,0.10963944076526858,"view (V) V, Q"
DATABASE DESCRIPTION,0.11037527593818984,"Figure 2: Lucy’s high-level workflow. Red colored boxes indicate phases performed by
LLMs, and a green colored box is a phase performed by an automated reasoner."
USER QUESTIONS,0.1111111111111111,"2.2
User questions
113"
USER QUESTIONS,0.11184694628403238,"We consider three simple questions to ddo that are well formulated: outputs are explicitly
114"
USER QUESTIONS,0.11258278145695365,"specified, so no additional information is needed to answer them. We use gpt4 (‘gpt-4-0125-
115"
USER QUESTIONS,0.11331861662987491,"preview’), and promptB [Sequeda et al., 2023] for these questions. For each question, we
116"
USER QUESTIONS,0.11405445180279618,"present a ground truth answer and a GPT answer. Table 1 presents both questions (Q3 is
117"
USER QUESTIONS,0.11479028697571744,"presented in Appendix C.1).
118"
USER QUESTIONS,0.11552612214863871,"Question Q1 is ‘List customers who use datacenters with names starting with ‘dev’. Output
119"
USER QUESTIONS,0.11626195732155997,"clients and datacenters names’.
The user asks for information that relates clients and
120"
USER QUESTIONS,0.11699779249448124,"datacenters. Consider GPT’s answer. GPT misses the core logic of the database: clients
121"
USER QUESTIONS,0.1177336276674025,"and datacenter resources are related via a m:m relation (modeled with RsPool2Client).
122"
USER QUESTIONS,0.11846946284032377,"GPT outputs clients and datacenters that share the same location, which is incorrect.
123"
USER QUESTIONS,0.11920529801324503,"Question Q2 is ‘List resource pool names with CPU overhead limit greater than runtime
124"
USER QUESTIONS,0.1199411331861663,"overall usage by 100’. Here the user asks about resource pool properties. However, the
125"
USER QUESTIONS,0.12067696835908756,"GPT answer ignores the database’s primary/foreign relations.
It performs an inner
126"
USER QUESTIONS,0.12141280353200883,"join between ResourcePool, cCPU, and rCPU tables, using non-existent attributes
127"
USER QUESTIONS,0.12214863870493009,"ResourcePool.config_id and ResourcePool.runtime_id, which is clearly incorrect.
128"
USER QUESTIONS,0.12288447387785136,"In summary, these examples demonstrated that LLMs struggle to handle complex relation-
129"
USER QUESTIONS,0.12362030905077263,"ships between objects.
130"
FRAMEWORK DESIGN,0.1243561442236939,"3
Framework design
131"
FRAMEWORK DESIGN,0.12509197939661515,"In this section, we present our framework Lucy. Figure 2 illustrates the workflow diagram,
132"
FRAMEWORK DESIGN,0.12582781456953643,"and Algorithm 1 shows the main steps of the workflow.
There are two inputs to the
133"
FRAMEWORK DESIGN,0.12656364974245768,"framework. The first input is a user question Q. The second input is dbModel, which is a
134"
FRAMEWORK DESIGN,0.12729948491537896,"description of the database schema that we discuss in the next section (Section 3.1). The
135"
FRAMEWORK DESIGN,0.1280353200883002,"workflow consists of three sequential subtasks: MatchTables, GenerateView, and QueryView.
136"
FRAMEWORK DESIGN,0.1287711552612215,"MatchTables identifies the relevant tables and their attributes related to the user question
137"
FRAMEWORK DESIGN,0.12950699043414277,"(Section 3.2). GenerateView finds a combined view of relevant tables taking into account
138"
FRAMEWORK DESIGN,0.13024282560706402,"database constraints (Section 3.3). The third phase, QueryView, takes V and the user
139"
FRAMEWORK DESIGN,0.1309786607799853,"question Q and produces an SQL query Q for V (Section 3.4). To simplify notations, we
140"
FRAMEWORK DESIGN,0.13171449595290655,"assume that dbModel is a global variable in Algorithm 1.
141"
FRAMEWORK DESIGN,0.13245033112582782,"3.1
Database model (dbModel)
142"
FRAMEWORK DESIGN,0.13318616629874908,"We start with dbModel, or dbm for short. dbm is a data structure that contains aggregated
143"
FRAMEWORK DESIGN,0.13392200147167035,"information about the database, maintained as a JSON structure. dbm should be constructed
144"
FRAMEWORK DESIGN,0.1346578366445916,"once for a database as the structure of the database is relatively stable. dbm can always be
145"
FRAMEWORK DESIGN,0.13539367181751288,"extended if the database requires modifications. Here are the two main blocks of dbm:
146"
FRAMEWORK DESIGN,0.13612950699043413,"Database schema. The schema is written using the SQL Data Definition Language (CREATE
147"
FRAMEWORK DESIGN,0.1368653421633554,"TABLE statements). It includes table names, names and types of columns in each table,
148"
FRAMEWORK DESIGN,0.13760117733627666,"and database constraints such as primary and foreign keys. It can also contain optional user
149"
FRAMEWORK DESIGN,0.13833701250919794,"comments associated with each table and column. We refer to tables and constraints as
150"
FRAMEWORK DESIGN,0.1390728476821192,"dbm.tables and dbm.constraints, respectively. We extract this information in the form of
151"
FRAMEWORK DESIGN,0.13980868285504047,"JSON. Appendix D.1.1–D.1.2 shows examples of these structures.
152"
FRAMEWORK DESIGN,0.14054451802796172,"Patterns summary. The user can optionally list higher-level design patterns that are not
153"
FRAMEWORK DESIGN,0.141280353200883,"captured by the schema explicitly. This information can help to improve the accuracy of the
154"
FRAMEWORK DESIGN,0.14201618837380428,"algorithm. We support m:m, Star, Snowflake, and lookup patterns, but the model is
155"
FRAMEWORK DESIGN,0.14275202354672553,"extendable to support other patterns. The user identifies these patterns manually, based on
156"
FRAMEWORK DESIGN,0.1434878587196468,"the logic of the target domain. In the future, we envision that the process can be partially
157"
FRAMEWORK DESIGN,0.14422369389256806,"automated. Appendix D.1.3 shows the JSON format used to specify pattern structures.
158"
FRAMEWORK DESIGN,0.14495952906548934,"Formal notations. We introduce formal notations. dbm.tables contains a list of tables ti,
159"
FRAMEWORK DESIGN,0.1456953642384106,"i ∈[1, m] where m is the number of tables. dbm.constraints contains a set of pairs (ti, tj)
160"
FRAMEWORK DESIGN,0.14643119941133187,"such that ti and tj are related via 1:1, 1:m or m:1 relation. We denote dbm.m:m as a
161"
FRAMEWORK DESIGN,0.14716703458425312,"set of triplets (ti, tj, tk), where a join table tk models a m:m relation between tables ti
162"
FRAMEWORK DESIGN,0.1479028697571744,"and tj. Note that (ti, tk) and (tj, tk) must be in dbm.constraints. Additionally, we denote
163"
FRAMEWORK DESIGN,0.14863870493009565,"dbm.lookup as the set of lookup tables. For example, in the ddo database, dbm.m:m =
164"
FRAMEWORK DESIGN,0.14937454010301693,"{(Client, ResourcePool, RsPool2Client)} and dbm.lookup = {Location}. For a
165"
FRAMEWORK DESIGN,0.15011037527593818,"tree-like pattern, like Star or Snowflake, we distinguish between root table and inner
166"
FRAMEWORK DESIGN,0.15084621044885946,"tables using two predicates, e.g., star_root(t) returns True if t is the root table of a Star
167"
FRAMEWORK DESIGN,0.1515820456217807,"and star_inner(t) returns True if t is an inner table (not root) of a Star.
168"
THE MATCHTABLES PHASE,0.152317880794702,"3.2
The MatchTables phase
169"
THE MATCHTABLES PHASE,0.15305371596762327,"The first phase, MatchTables, needs to find relevant tables and their attributes to the user
170"
THE MATCHTABLES PHASE,0.15378955114054452,"question. One approach to achieve that can be to provide the schema and a question to an
171"
THE MATCHTABLES PHASE,0.1545253863134658,"LLM and ask for this information. However, one of the distinguishing features of real-world
172"
THE MATCHTABLES PHASE,0.15526122148638705,"databases is their large number of tables and attributes. Hence, feeding all of them along with
173"
THE MATCHTABLES PHASE,0.15599705665930833,"their descriptions to the prompt might not be feasible for many LLM models. Therefore, we
174"
THE MATCHTABLES PHASE,0.15673289183222958,"build an iterative procedure that takes advantage of database tree-like patterns. In general,
175"
THE MATCHTABLES PHASE,0.15746872700515085,"this procedure can be customized to best support the structure of a database.
176"
THE MATCHTABLES PHASE,0.1582045621780721,Algorithm 1 Lucy
THE MATCHTABLES PHASE,0.15894039735099338,"Require: User question Q, database model dbModel
Ensure: Summary view V, SQL query Q"
THE MATCHTABLES PHASE,0.15967623252391464,"1: Phase 1: MatchTables //LLM-based phase
2: // get core tables (these are tables that are not inner tables in Star or Snowflake)
3: core_tables = {t|t ∈dbm.tables ∧t /∈(snowflake_inner(t) ∨star_inner(t))}
4: // identify relevant core tables to the user query
5: _, T = promptA (Q, core_tables, {})
6: RT = {}
7: for t ∈T do
8:
if t ∈snowflake_root(t) ∨t ∈star_root(t) then
9:
// a breadth-first deepening to identify relevant tables and attributes inside a pattern rooted at t
10:
RT = RT ∪IterativePrompting(Q, t)
11:
else
12:
R′
T , _ = promptA(Q, {}, t.attributes), RT = RT ∪R′
T // identify t’s relevant attributes"
THE MATCHTABLES PHASE,0.1604120676968359,"13: Phase 2: GenerateView // constraint reasoner-based phase
14: // formulate a constraint satisfaction problem
15: S = formulate_csp(RT )
16: // solve S to find a path in G that satisfies constraints (C1)–(C5)
17: P = solve_csp(S)
18: // build a view V base on P by joining tables along the path P.
19: V = build_view(P)
20: Phase 3: QueryView //LLM-based phase
21: Q= promptC (Q, V)
22: return V, Q"
THE MATCHTABLES PHASE,0.16114790286975716,"Algorithm 1 shows MatchTables in lines 2–12. First, the algorithm focuses on tables that are
177"
THE MATCHTABLES PHASE,0.16188373804267844,"not inner tables of any patterns. We refer to such tables as core tables (core_tables in line 3).
178"
THE MATCHTABLES PHASE,0.1626195732155997,"For example, Figure 3 shows core tables for ddo. Next, we ask LLM to find relevant tables
179"
THE MATCHTABLES PHASE,0.16335540838852097,"among these core tables using promptA in line 5. (Appendix D.2.1 shows a promptA with
180"
THE MATCHTABLES PHASE,0.16409124356144222,"a few examples.) As a result, we obtain a set of relevant core tables. We explore them one
181"
THE MATCHTABLES PHASE,0.1648270787343635,"by one in the loop in line 7. If it is a root table of a pattern, we perform a search inside the
182"
THE MATCHTABLES PHASE,0.16556291390728478,"corresponding pattern to find more relevant tables using a breadth-first deepening procedure,
183"
THE MATCHTABLES PHASE,0.16629874908020603,"IterativePrompting, in line 10 (Algorithm 2 shows IterativePrompting’s pseudocode
184"
THE MATCHTABLES PHASE,0.1670345842531273,"in Appendix D.2). Otherwise, we use promptA to obtain relevant attributes in line 12.
185"
THE MATCHTABLES PHASE,0.16777041942604856,"Example 3.1. Consider questions Q1 and Q2 from Table 1. Figure 3 shows ddo’s core
186"
THE MATCHTABLES PHASE,0.16850625459896984,"tables.
For Q1, a LLM identifies relevant core tables: T = {Client, Datacenter}
187"
THE MATCHTABLES PHASE,0.1692420897718911,"Retention
Client
Location
Datacenter"
THE MATCHTABLES PHASE,0.16997792494481237,"Payment
RsPool2Client
Compute
ResourcePool ..."
THE MATCHTABLES PHASE,0.17071376011773362,"...
..."
THE MATCHTABLES PHASE,0.1714495952906549,Figure 3: A part of the abstract schema graph G for ddo that includes core tables.
THE MATCHTABLES PHASE,0.17218543046357615,"(line 5). Since none of these tables is a root of a Snowflake or a Star, we prompt
188"
THE MATCHTABLES PHASE,0.17292126563649743,"for relevant attributes for each table in line 12 to get RT = {Client.name, Client.gender,
189"
THE MATCHTABLES PHASE,0.17365710080941868,"Datacenter.name}. Now consider Q2. LLM identifies ResourcePool as a relevant
190"
THE MATCHTABLES PHASE,0.17439293598233996,"table in line 5. As ResourcePool is the root table of Snowflake (see Figure 1), we begin
191"
THE MATCHTABLES PHASE,0.1751287711552612,"to explore the pattern tree in a breadth-first order using IterativePrompting in line 10.
192"
THE MATCHTABLES PHASE,0.1758646063281825,"ResourcePool has two child nodes, Config and Runtime, and several attributes. We
193"
THE MATCHTABLES PHASE,0.17660044150110377,"query the LLM and find that both Config and Runtime are relevant as well as its attribute
194"
THE MATCHTABLES PHASE,0.17733627667402502,"ResourcePool.name. Following the breadth-first search order, we consider Config with
195"
THE MATCHTABLES PHASE,0.1780721118469463,"two descendants cCPU and cMemory and discover cCPU is relevant (Example D.4 in
196"
THE MATCHTABLES PHASE,0.17880794701986755,"Appendix shows a full version).
197"
THE GENERATEVIEW PHASE,0.17954378219278883,"3.3
The GenerateView phase
198"
THE GENERATEVIEW PHASE,0.18027961736571008,"The MatchTables phase identifies a set of relevant tables and their attributes. Next, we
199"
THE GENERATEVIEW PHASE,0.18101545253863136,"construct a view table that combines relevant tables and attributes into a single table.
200"
THE GENERATEVIEW PHASE,0.1817512877115526,"We build an abstract schema graph G which provides a graph view of dbm, and define a
201"
THE GENERATEVIEW PHASE,0.18248712288447388,"CSP over this graph. For each table ti in dbm.tables, we introduce a node in G. We use
202"
THE GENERATEVIEW PHASE,0.18322295805739514,"the names ti to refer to the corresponding nodes. For each pair of tables ti and tj, s.t.
203"
THE GENERATEVIEW PHASE,0.18395879323031641,"(ti, tj) ∈dbm.constraints, we introduce an edge that connects them. We denote V the set of
204"
THE GENERATEVIEW PHASE,0.18469462840323767,"nodes in G and E its edges. Figure 3 illustrates a part of the graph (core tables) for ddo.
205"
THE GENERATEVIEW PHASE,0.18543046357615894,"Algorithm 1 shows three main steps of this phase: build an abstract graph representation G
206"
THE GENERATEVIEW PHASE,0.1861662987490802,"of the schema (line 15); formulate and solve CSP to obtain a path P (line 17); and perform
207"
THE GENERATEVIEW PHASE,0.18690213392200147,"joins along this path to obtain the designed view V (line 19). Next, we describe these steps.
208"
THE GENERATEVIEW PHASE,0.18763796909492272,"Problem Formulation. Let T = tables(RT ) be a set of relevant tables returned by
209"
THE GENERATEVIEW PHASE,0.188373804267844,"MatchTables. We formulate the problem of finding a path P in G that visits a set of nodes
210"
THE GENERATEVIEW PHASE,0.18910963944076528,"T and satisfies a set of database constraints.
211"
THE GENERATEVIEW PHASE,0.18984547461368653,"(C1) P must be a valid path in G. This ensures that we follow primary/foreign keys
212"
THE GENERATEVIEW PHASE,0.1905813097866078,"relationships, i.e., 1:1, 1:m, and build a valid view.
213"
THE GENERATEVIEW PHASE,0.19131714495952906,"(C2) P visits all relevant tables T. This ensures combining all relevant tables to a view.
214"
THE GENERATEVIEW PHASE,0.19205298013245034,"(C3) Consider (ti, tj, tk) ∈dbm.m:m. If ti ∈P and tj ∈P then tk must occur in P once
215"
THE GENERATEVIEW PHASE,0.1927888153053716,"between ti and tj. These constraints enforce m:m relationships.
216"
THE GENERATEVIEW PHASE,0.19352465047829287,"(C4) If t ∈P and t ∈dbm.lookup then t’s predecessor equals its successor in P. This
217"
THE GENERATEVIEW PHASE,0.19426048565121412,"ensures that a lookup table serves as a look-up function for each table individually.
218"
THE GENERATEVIEW PHASE,0.1949963208241354,"(C5) Cost function: we minimize the number of occurrences of tables outside of T in P. A
219"
THE GENERATEVIEW PHASE,0.19573215599705665,"shorter path that focuses on the tables in T allows us to build more succinct views.
220"
THE GENERATEVIEW PHASE,0.19646799116997793,"(C1)–(C5) are common constraints that we encounter in the benchmark sets. In general, the
221"
THE GENERATEVIEW PHASE,0.19720382634289918,"user can specify more constraints to capture the logical relationships of the modeled data.
222"
THE GENERATEVIEW PHASE,0.19793966151582046,"Constraint satisfaction problem (CSP). We define a CSP formulation S of constraints
223"
THE GENERATEVIEW PHASE,0.1986754966887417,"(C1)–(C5). We start with a basic formulation. Let n be the maximum length of the path P.
224"
THE GENERATEVIEW PHASE,0.199411331861663,"For each node ti in G and step r, where r ∈[1, n], we introduce a Boolean variable br
i . br
i is
225"
THE GENERATEVIEW PHASE,0.20014716703458427,"true iff ti is the rth node in P. We also introduce a sink-node Boolean variable br
d for each
226"
THE GENERATEVIEW PHASE,0.20088300220750552,"layer to model paths that are shorter than n. S contains the following logical constraints:
227"
THE GENERATEVIEW PHASE,0.2016188373804268,"(C5) :
minimize P
i,ti /∈T occi
(1)"
THE GENERATEVIEW PHASE,0.20235467255334805,"∀i.ti ∈V
occi = b1
i + . . . + bn
i
(2)"
THE GENERATEVIEW PHASE,0.20309050772626933,"(C1) :
∀i.ti ∈V, r ∈[1, n −1]
br
i ⇒(∨j.(ti,tj)∈Ebr+1
j
) ∨br+1
d
(3)"
THE GENERATEVIEW PHASE,0.20382634289919058,"(C2) :
∀i.ti ∈T
occi ≥1
(4) 228"
THE GENERATEVIEW PHASE,0.20456217807211186,"(C3) :
∀k.(ti, tj, tk) ∈dbm.m:m
occk = 1
(5)"
THE GENERATEVIEW PHASE,0.2052980132450331,"(C3) :
∀k.(ti, tj, tk) ∈dbm.m:m, r ∈[2, n −1]
br
k ⇒(br−1
i
∧br+1
j
) ∨(br−1
j
∧br+1
i
)
(6)"
THE GENERATEVIEW PHASE,0.20603384841795438,"(C4) :
∀i.ti ∈dbm.lookup, r ∈[2, n −1]
br
i ⇒(br−1
j
⇒br+1
j
)
(7)"
THE GENERATEVIEW PHASE,0.20676968359087564,"∀r ∈[1, n]
br
1 + . . . + br
|V | = 1
(8)"
THE GENERATEVIEW PHASE,0.20750551876379691,"∀r ∈[1, n −1]
br
d ⇒br+1
d
(9)"
THE GENERATEVIEW PHASE,0.20824135393671817,"Consider the encoding S. Equations 2 specify integer variables, occi, for i ∈[1, n], that count
229"
THE GENERATEVIEW PHASE,0.20897718910963944,"the occurrences of each table in the path. Equations 8 encode that only one node belongs to
230"
THE GENERATEVIEW PHASE,0.2097130242825607,"a path at each step. Equations 9 encode that if the path visits the sink node, then it must
231"
THE GENERATEVIEW PHASE,0.21044885945548197,"stay there. Other equations encode constraints (C1)–(C5). By construction, Equations 1–9
232"
THE GENERATEVIEW PHASE,0.21118469462840322,"generate a valid path in G that satisfies the constraints (C1)–(C5).
233"
THE GENERATEVIEW PHASE,0.2119205298013245,"Example 3.2. For Q1, solving S gives the green path between Datacenter and Client
234"
THE GENERATEVIEW PHASE,0.21265636497424578,"in Figure 3. S rules out the red path as we enforce constraint (C4) and optimization (C5).
235"
THE GENERATEVIEW PHASE,0.21339220014716703,"Improvements of CSP. Our basic model S can be improved to take advantage of Star
236"
THE GENERATEVIEW PHASE,0.2141280353200883,"and Snowflake patterns. Namely, we can leverage the decomposition of G and find a path
237"
THE GENERATEVIEW PHASE,0.21486387049300956,"P among core tables only. Then, for each core table in P that is a pattern root, and for each
238"
THE GENERATEVIEW PHASE,0.21559970566593084,"inner relevant table in this pattern, we build a path P′ along the corresponding branch. For
239"
THE GENERATEVIEW PHASE,0.2163355408388521,"example, Figure 1 shows two paths from ResourcePool to cCPU (an orange path) and
240"
THE GENERATEVIEW PHASE,0.21707137601177337,"rCPU (a blue path). We use left join to combine tables along each such branch. Finally,
241"
THE GENERATEVIEW PHASE,0.21780721118469462,"we combine P and P’s into a single view.
242"
THE GENERATEVIEW PHASE,0.2185430463576159,"Summary view. Given a path P in a graph, we join tables along the path using their
243"
THE GENERATEVIEW PHASE,0.21927888153053715,"primary and foreign key relations. We keep the same set of attributes that MatchTables
244"
THE GENERATEVIEW PHASE,0.22001471670345843,"identified. An example of the V for Q1 that corresponds to the green path in Figure 3 is
245"
THE GENERATEVIEW PHASE,0.22075055187637968,"shown in the listing in Table 7 in Appendix D.3.1.
246"
THE GENERATEVIEW PHASE,0.22148638704930096,"3.4
The QueryView phase.
247"
THE GENERATEVIEW PHASE,0.2222222222222222,"QueryView takes the summary view V along with the user question, and prompts an LLM
248"
THE GENERATEVIEW PHASE,0.2229580573951435,"to obtain the final SQL using promptC (line 21 in Algorithm 1). promptC is defined in
249"
THE GENERATEVIEW PHASE,0.22369389256806477,"Appendix D.4.1. The listing in Table 7 shows an SQL Q to answer Q1 (Appendix D.3.1).
250"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.22442972774098602,"4
Discussion on strengths and limitations
251"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.2251655629139073,"Strengths. Lucy is designed based on the principle of separation of responsibilities between
252"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.22590139808682855,"generative tasks and automated reasoning tasks: each step focuses on either an NLP-related
253"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.22663723325974983,"subproblem or a constraint reasoning subproblem. This separation allows us to support a
254"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.22737306843267108,"number of unique capabilities. First, Lucy shifts the burden of complex reasoning from
255"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.22810890360559236,"LLMs to constraint solvers. Second, we support reasoning on complex relationships, like
256"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.2288447387785136,"m:m, lookup, Star or Snowflake. Third, our framework is flexible and extensible as
257"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.22958057395143489,"it is easy to incorporate domain-specific constraints as soon as they can be expressed by
258"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.23031640912435614,"constraint modeling language. This assumes that the user has a data analytics role and
259"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.23105224429727741,"understands the logic of the database. Such formal reasoning capability is important, as it is
260"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.23178807947019867,"hard to control LLMs via prompts when non-trivial reasoning is required. Fourth, we can
261"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.23252391464311994,"evaluate each phase and diagnose Lucy failure modes. For example, if MatchTables misses
262"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.2332597498160412,"relevant tables, this indicates that we need to provide more information about the schema to
263"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.23399558498896247,"an LLM. Fifth, based on our evaluation, Lucy can support complex queries that include
264"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.23473142016188372,"multiple filtering operators and aggregators, e.g. average or sum. This capability follows
265"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.235467255334805,"from the QueryView phase as the final call to an LLM is performed on a single view table.
266"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.23620309050772628,"Limitations. The first limitation is that we cannot guarantee that the SQL query answers
267"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.23693892568064753,"the user’s question. Given the current state of the art, providing such guarantees is beyond the
268"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.2376747608535688,"reach of any copilot method that takes natural language descriptions and outputs structured
269"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.23841059602649006,"text, like code or SQL. However, our solution does guarantee that V satisfies database
270"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.23914643119941134,"constraints, which is a step forward in this direction. Second, we do not support questions
271"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.2398822663723326,"that require union operators in the GenerateView phase. In fact, there are no benchmarks
272"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.24061810154525387,"available that require the union operator to answer questions. Supporting union would
273"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.24135393671817512,"require an extension of MatchTables and GenerateView. Third, we observed experimentally
274"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.2420897718910964,"that Lucy struggles with certain types of queries that involve a particular interleaving
275"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.24282560706401765,"ordering of filtering and aggregate operators or question-specific table dependencies, like a
276"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.24356144223693893,"lookup table that has to be used multiple times to answer the user’s question. We further
277"
DISCUSSION ON STRENGTHS AND LIMITATIONS,0.24429727740986018,"discuss such questions in our experiments.
278"
EXPERIMENTAL EVALUATION,0.24503311258278146,"5
Experimental evaluation
279"
EXPERIMENTAL EVALUATION,0.2457689477557027,"In our experimental evaluation, we aim to answer the main questions:
280"
EXPERIMENTAL EVALUATION,0.246504782928624,"• Is Lucy competitive with existing LLM-based approaches?
281"
EXPERIMENTAL EVALUATION,0.24724061810154527,"• Can we debug Lucy to gain insights about failure modes?
282"
EXPERIMENTAL EVALUATION,0.24797645327446652,"• Can Lucy handle complex questions?
283"
EXPERIMENTAL EVALUATION,0.2487122884473878,"Setup. We compare with the following zero-shot baselines: gpt4, nsql, and chat2query
284"
EXPERIMENTAL EVALUATION,0.24944812362030905,"(c2q for short). gpt4 and c2q methods are the best zero-shot techniques according to the
285"
EXPERIMENTAL EVALUATION,0.2501839587932303,"BIRD leadership board that are accessible for evaluation [Li et al., 2024b]. nsql is the best
286"
EXPERIMENTAL EVALUATION,0.2509197939661516,"open-source large foundation model designed specifically for the SQL generation task [Labs,
287"
EXPERIMENTAL EVALUATION,0.25165562913907286,"2023b]. chat2query is closed-source but the authors kindly extended their API that we
288"
EXPERIMENTAL EVALUATION,0.2523914643119941,"can run experiments with gpt4. We provide all benchmarks and frameworks’ results in the
289"
EXPERIMENTAL EVALUATION,0.25312729948491536,"supplementary materials. For gpt4 and Lucy, we use the ‘gpt-4-0125-preview’ API without
290"
EXPERIMENTAL EVALUATION,0.25386313465783666,"fine-tuning. We use OR-Tools as a constraint solver [Perron and Didier, 2024] (Appendix E.1
291"
EXPERIMENTAL EVALUATION,0.2545989698307579,"provides full details of the experimental setup).
292"
EXPERIMENTAL EVALUATION,0.25533480500367917,"Evaluation metrics. We use the standard Execution Accuracy (ex) [Li et al., 2023]. In
293"
EXPERIMENTAL EVALUATION,0.2560706401766004,"addition, we consider a relaxation of this metric. We noticed that frameworks often add
294"
EXPERIMENTAL EVALUATION,0.2568064753495217,"additional attributes to the output as the exact format of the output is rarely specified. Hence,
295"
EXPERIMENTAL EVALUATION,0.257542310522443,"we extend ex to esx metrics that check if the output of a framework contains the ground
296"
EXPERIMENTAL EVALUATION,0.2582781456953642,"truth outputs. To better understand performance characteristics and possible failure modes,
297"
EXPERIMENTAL EVALUATION,0.25901398086828553,"we consider the coverage metric that captures whether a framework correctly identified a
298"
EXPERIMENTAL EVALUATION,0.2597498160412068,"subset of relevant tables and attributes. Let sqlG be the ground truth answer and sqlF be a
299"
EXPERIMENTAL EVALUATION,0.26048565121412803,"generated query. Then we assess the percentage of the ground truth content slqF captures:
300"
EXPERIMENTAL EVALUATION,0.2612214863870493,covt = |tables(slqF ) ∩tables(slqG)|
EXPERIMENTAL EVALUATION,0.2619573215599706,"|tables(slqG)|
cova = |attributes(slqF ) ∩attributes(slqG)|"
EXPERIMENTAL EVALUATION,0.26269315673289184,"|attributes(slqG)|
,
(10)"
EXPERIMENTAL EVALUATION,0.2634289919058131,"where tables () and attributes () are functions that return a set of tables and attributes.
301"
EXPERIMENTAL EVALUATION,0.26416482707873434,Table 2: The ACME insurance dataset.
EXPERIMENTAL EVALUATION,0.26490066225165565,"gpt4
gpt4ex
c2q
nsql
Lucy
dw
covt
0.44
0.47
0.82
0.31
0.95
-
cova
0.36
0.42
0.81
0.25
0.93
-
ex
9
13
16
2
30
24
esx
9
13
16
3
33
-"
EXPERIMENTAL EVALUATION,0.2656364974245769,Table 3: The Cloud Resources dataset.
EXPERIMENTAL EVALUATION,0.26637233259749815,"gpt4
gpt4ex
c2q
Lucy
covt
0.46
0.44
0.44
0.98
cova
0.50
0.44
0.48
0.98
ex
6
4
2
17
esx
9
5
2
18"
EXPERIMENTAL EVALUATION,0.2671081677704194,"ACME insurance.
We consider the ACME insurance dataset that was recently pub-
302"
EXPERIMENTAL EVALUATION,0.2678440029433407,"lished [Sequeda et al., 2023]. The dataset represents an enterprise relational database schema
303"
EXPERIMENTAL EVALUATION,0.26857983811626196,"in the insurance domain. The authors focused on a subset of 13 tables out of 200 tables and
304"
EXPERIMENTAL EVALUATION,0.2693156732891832,"proposed a set of 45 challenging questions. We identified two Star patterns in this database.
305"
EXPERIMENTAL EVALUATION,0.27005150846210446,"The authors showed that their method (dw) solved 24 out of 45 problems using intermediate
306"
EXPERIMENTAL EVALUATION,0.27078734363502577,"representation of a knowledge graph, while gpt4 solved only 8 problems. However, results
307"
EXPERIMENTAL EVALUATION,0.271523178807947,"are not publicly available, so we cannot perform coverage analysis and compute esx.
308"
EXPERIMENTAL EVALUATION,0.27225901398086827,"We reran the experiment on gpt4 with the same promptB (Appendix C.1.1) and obtained
309"
EXPERIMENTAL EVALUATION,0.2729948491537896,"similar results to those reported in [Sequeda et al., 2023]. In addition, we extended the
310"
EXPERIMENTAL EVALUATION,0.2737306843267108,"schema with descriptions of table attributes from dbModel in the form of comments, which
311"
EXPERIMENTAL EVALUATION,0.2744665194996321,"we called gpt4ex (See Appendix E.2 for examples). Table 2 shows our results. First, we
312"
EXPERIMENTAL EVALUATION,0.27520235467255333,"observe that there is a strong correlation between coverage and accuracy metrics in the
313"
EXPERIMENTAL EVALUATION,0.27593818984547464,"results. c2q and Lucy show good coverage, meaning that they can correctly identify most of
314"
EXPERIMENTAL EVALUATION,0.2766740250183959,"the required tables and attributes. They also demonstrate better performance compared to
315"
EXPERIMENTAL EVALUATION,0.27740986019131714,"other methods. Our framework shows very high coverage and solves about 30 of benchmarks
316"
EXPERIMENTAL EVALUATION,0.2781456953642384,"according to the ex metric, which outperforms dw that solves 24 and other methods.
317"
EXPERIMENTAL EVALUATION,0.2788815305371597,"Lucy still cannot solve 13 benchmarks, which is surprising given high coverage.
We
318"
EXPERIMENTAL EVALUATION,0.27961736571008095,"performed a study to locate where Lucy fails on these benchmarks (See Appendix E.2.1 for
319"
EXPERIMENTAL EVALUATION,0.2803532008830022,"all questions where Lucy was unsuccessful). In summary, the majority of failures come from
320"
EXPERIMENTAL EVALUATION,0.28108903605592345,"under-specified output attributes or nonstandard aggregators, like specialized formulas to
321"
EXPERIMENTAL EVALUATION,0.28182487122884475,"compute an average. In four cases, MatchTables missed a table, and in one case, QueryView
322"
EXPERIMENTAL EVALUATION,0.282560706401766,"missed the attribute to output. The most interesting mode of failure is when we need to
323"
EXPERIMENTAL EVALUATION,0.28329654157468726,"perform multiple lookups on the same table. The reason for that is the MatchTables phase
324"
EXPERIMENTAL EVALUATION,0.28403237674760856,"identifies only relevant tables but ignores possible relationships between them. Extending
325"
EXPERIMENTAL EVALUATION,0.2847682119205298,"MatchTables to retrieve relationships between tables is interesting future work.
326"
EXPERIMENTAL EVALUATION,0.28550404709345106,"BIRD datasets.
Next, we consider the state-of-the-art dataset BIRD [Li et al., 2023].
327"
EXPERIMENTAL EVALUATION,0.2862398822663723,"From the development set, we chose two datasets with complex relationships between objects:
328"
EXPERIMENTAL EVALUATION,0.2869757174392936,"financial (106 instances) and formula1 (174 instances)1. The accuracy of chat2query
329"
EXPERIMENTAL EVALUATION,0.28771155261221487,"on the BIRD development set is ∼58%; however, its accuracy on financial and formula1
330"
EXPERIMENTAL EVALUATION,0.2884473877851361,"are much lower, ∼45%. We compare with results from gpt4’23 and c2q available from [Al-
331"
EXPERIMENTAL EVALUATION,0.2891832229580574,"ibabaResearch, 2020] and
[TiDBCloud, 2020a], respectively.
However, we reran these
332"
EXPERIMENTAL EVALUATION,0.2899190581309787,"benchmarks with gpt4 and gpt4ex as the gpt4’23 results are nearly one year old. Table 5
333"
EXPERIMENTAL EVALUATION,0.29065489330389993,"and Table 4 show results on financial and formula1, respectively. Lucy and c2q have
334"
EXPERIMENTAL EVALUATION,0.2913907284768212,"higher coverage and good accuracy. Lucy shows the best results in most cases. Again,
335"
EXPERIMENTAL EVALUATION,0.29212656364974243,"Lucy has very good coverage on financial but was able to solve only 68 out of 106 queries
336"
EXPERIMENTAL EVALUATION,0.29286239882266374,"based on the esx metric. We manually performed an questions study on the failed questions.
337"
EXPERIMENTAL EVALUATION,0.293598233995585,"There are two major groups there that are interesting. First, Lucy has difficulty if there are
338"
EXPERIMENTAL EVALUATION,0.29433406916850624,"multiple orderings, especially nested or ordering in different directions. Second, sometimes,
339"
EXPERIMENTAL EVALUATION,0.29506990434142755,"MatchTables adds an additional table that is not needed to find the answer. The rest are
340"
EXPERIMENTAL EVALUATION,0.2958057395143488,"either ambiguous questions or small mistakes like outputting a wrong attribute, i.e., id
341"
EXPERIMENTAL EVALUATION,0.29654157468727005,"instead of name. See Appendix E.3.3 for examples of questions where Lucy was unsuccessful.
342"
EXPERIMENTAL EVALUATION,0.2972774098601913,Table 4: The formula1 dataset.
EXPERIMENTAL EVALUATION,0.2980132450331126,"gpt4’23 gpt4 gpt4ex c2q nsql Lucy
covt
0.86
0.78
0.77 0.88
0.52
0.93
cova
0.84
0.75
0.75 0.81
0.50
0.94
ex
54
67
65
80
9
83
esx
66
80
79
93
10
103"
EXPERIMENTAL EVALUATION,0.29874908020603386,Table 5: The financial dataset.
EXPERIMENTAL EVALUATION,0.2994849153789551,"gpt4’23 gpt4 gpt4ex c2q nsql Lucy
covt
0.81
0.84
0.87 0.92
0.50
0.97
cova
0.81
0.81
0.85 0.91
0.59
0.96
ex
36
47
52
59
6
56
esx
38
55
64
62
6
68 343"
EXPERIMENTAL EVALUATION,0.30022075055187636,"Cloud resources.
Next, we propose a new benchmark based on the vSphere API data
344"
EXPERIMENTAL EVALUATION,0.30095658572479767,"model [VMware, Inc., 2024]. We experimented with this publicly available data model of
345"
EXPERIMENTAL EVALUATION,0.3016924208977189,"an industrial product, as it is well-documented and easily accessible via a web interface. It
346"
EXPERIMENTAL EVALUATION,0.30242825607064017,"describes the state of the system as well as its configuration parameters. States are stored in
347"
EXPERIMENTAL EVALUATION,0.3031640912435614,"a database and queried by customers to keep track of performance, maintenance, and data
348"
EXPERIMENTAL EVALUATION,0.3038999264164827,"analysis. We extracted the descriptions of main objects in Managed Object [2024], including
349"
EXPERIMENTAL EVALUATION,0.304635761589404,"data centers, resource pools, hosts, and virtual machines and their properties, and built a
350"
EXPERIMENTAL EVALUATION,0.3053715967623252,"database that captures these relationships using 52 tables. Overall, we have two Stars, five
351"
EXPERIMENTAL EVALUATION,0.30610743193524653,"Snowflakes and two m:ms patterns. For each table and an attribute, we get descriptions
352"
EXPERIMENTAL EVALUATION,0.3068432671081678,"from [Managed Object, 2024]. As these can be a lengthy description, we use GPT to shorten
353"
EXPERIMENTAL EVALUATION,0.30757910228108903,"it to 15 words (see promptD in Appendix E.3.2) . We generated data randomly using
354"
EXPERIMENTAL EVALUATION,0.3083149374540103,"sqlfaker [Kohlegger, 2020]. We create 20 challenging questions for this benchmark.
355"
EXPERIMENTAL EVALUATION,0.3090507726269316,"Table 3 shows our results. nsql cannot process this benchmark due to a limited context
356"
EXPERIMENTAL EVALUATION,0.30978660779985284,"window. We again see that Lucy outperforms other models in both coverage and accuracy.
357"
EXPERIMENTAL EVALUATION,0.3105224429727741,"c2q failed on 6 questions with an error ‘Unable to generate SQL for this database due to its
358"
EXPERIMENTAL EVALUATION,0.31125827814569534,"extensive tables’ and it often does not follow instructions on the output columns. In terms of
359"
EXPERIMENTAL EVALUATION,0.31199411331861665,"failure mode, Lucy failed in the third phase as it hallucinated some attribute names when
360"
EXPERIMENTAL EVALUATION,0.3127299484915379,"names are long, e.g., ‘Resourcepoolruntimemory’ instead of ‘Resourcepoolruntimememory’.
361"
EXPERIMENTAL EVALUATION,0.31346578366445915,"1Recently, Wretblad et al. [2024b] provided a detailed analysis of the BIRD dataset and found a
number of errors of various types. See Appendix E.3 for the discussion."
REFERENCES,0.3142016188373804,"References
362"
REFERENCES,0.3149374540103017,"AlibabaResearch. BIRD-SQL: A BIg Bench for Large-Scale Relational Database Grounded
363"
REFERENCES,0.31567328918322296,"Text-to-SQLs. https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird,
364"
REFERENCES,0.3164091243561442,"2020. Accessed: May 2024.
365"
REFERENCES,0.31714495952906546,"A. Beaulieu. Learning SQL. O’Reilly Media, Inc., 2nd edition, 2009. ISBN 9780596520830.
366"
REFERENCES,0.31788079470198677,"S.
Chakraborty,
S.
Lahiri,
S.
Fakhoury,
M.
Musuvathi,
A.
Lal,
A.
Rastogi,
367"
REFERENCES,0.318616629874908,"N. Swamy,
and R. Sharma.
Ranking llm-generated loop invariants for pro-
368"
REFERENCES,0.31935246504782927,"gram verification.
In 2023 Empirical Methods in Natural Language Processing,
369"
REFERENCES,0.3200883002207506,"December 2023.
URL https://www.microsoft.com/en-us/research/publication/
370"
REFERENCES,0.3208241353936718,"ranking-llm-generated-loop-invariants-for-program-verification/.
EMNLP-
371"
REFERENCES,0.3215599705665931,"Findings 2023.
372"
REFERENCES,0.32229580573951433,"S. Chang and E. Fosler-Lussier. How to prompt llms for text-to-sql: A study in zero-shot,
373"
REFERENCES,0.32303164091243564,"single-domain, and cross-domain settings. arXiv preprint arXiv:2305.11853, 2023.
374"
REFERENCES,0.3237674760853569,"M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards,
375"
REFERENCES,0.32450331125827814,"Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf,
376"
REFERENCES,0.3252391464311994,"G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser,
377"
REFERENCES,0.3259749816041207,"M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis,
378"
REFERENCES,0.32671081677704195,"E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang,
379"
REFERENCES,0.3274466519499632,"I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam,
380"
REFERENCES,0.32818248712288445,"V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer,
381"
REFERENCES,0.32891832229580575,"P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba.
382"
REFERENCES,0.329654157468727,"Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL
383"
REFERENCES,0.33038999264164826,"https://arxiv.org/abs/2107.03374.
384"
REFERENCES,0.33112582781456956,"datadotworld,
Inc.
data
instances.
https://github.com/datadotworld/
385"
REFERENCES,0.3318616629874908,"cwd-benchmark-data/issues/3, 2024. Accessed: May 2024.
386"
REFERENCES,0.33259749816041206,"X. Dong, C. Zhang, Y. Ge, Y. Mao, Y. Gao, lu Chen, J. Lin, and D. Lou. C3: Zero-shot
387"
REFERENCES,0.3333333333333333,"text-to-sql with chatgpt, 2023.
388"
REFERENCES,0.3340691685062546,"D. Gao, H. Wang, Y. Li, X. Sun, Y. Qian, B. Ding, and J. Zhou. Text-to-sql empowered by
389"
REFERENCES,0.33480500367917587,"large language models: A benchmark evaluation, 2023.
390"
REFERENCES,0.3355408388520971,"D. Gao, H. Wang, Y. Li, X. Sun, Y. Qian, B. Ding, and J. Zhou. Text-to-sql empowered by
391"
REFERENCES,0.3362766740250184,"large language models: A benchmark evaluation. Proc. VLDB Endow., 17(5):1132–1145,
392"
REFERENCES,0.3370125091979397,"2024. URL https://www.vldb.org/pvldb/vol17/p1132-gao.pdf.
393"
REFERENCES,0.33774834437086093,"GitHub, Inc. GitHub Copilot. https://copilot.github.com/, 2021. Accessed: May 2024.
394"
REFERENCES,0.3384841795437822,"B. Hui.
Panel of BIRD Annotation Issues.
https://github.com/AlibabaResearch/
395"
REFERENCES,0.33922001471670343,"DAMO-ConvAI/issues/39, 2024. Accessed: May 2024.
396"
REFERENCES,0.33995584988962474,"IBM, Inc.
Dimensional schemas.
https://www.ibm.com/docs/en/ida/9.1.2?topic=
397"
REFERENCES,0.340691685062546,"design-dimensional-schemas, 2021. Accessed: May 2024.
398"
REFERENCES,0.34142752023546724,"M. Kohlegger. sqlfaker. https://pypi.org/project/sqlfaker/#description, 2020. Ac-
399"
REFERENCES,0.34216335540838855,"cessed: May 2024.
400"
REFERENCES,0.3428991905813098,"N. S. Labs. Nstext2sql: An open source text-to-sql dataset for foundation model training, July
401"
REFERENCES,0.34363502575423105,"2023a. URL https://github.com/NumbersStationAI/NSQL?tab=readme-ov-file.
402"
REFERENCES,0.3443708609271523,"N. S. Labs. Nstext2sql: An open source text-to-sql dataset for foundation model training,
403"
REFERENCES,0.3451066961000736,"July 2023b. URL https://github.com/NumbersStationAI/NSQL.
404"
REFERENCES,0.34584253127299486,"H. Li, J. Zhang, H. Liu, J. Fan, X. Zhang, J. Zhu, R. Wei, H. Pan, C. Li, and H. Chen.
405"
REFERENCES,0.3465783664459161,"Codes: Towards building open-source language models for text-to-sql, 2024a.
406"
REFERENCES,0.34731420161883736,"J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao, R. Geng, N. Huo,
407"
REFERENCES,0.34805003679175867,"X. Zhou, C. Ma, G. Li, K. C. Chang, F. Huang, R. Cheng, and Y. Li.
Can LLM
408"
REFERENCES,0.3487858719646799,"already serve as A database interface? A big bench for large-scale database grounded
409"
REFERENCES,0.34952170713760117,"text-to-sqls. CoRR, abs/2305.03111, 2023. doi: 10.48550/ARXIV.2305.03111. URL
410"
REFERENCES,0.3502575423105224,"https://doi.org/10.48550/arXiv.2305.03111.
411"
REFERENCES,0.3509933774834437,"J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao, R. Geng, N. Huo, X. Zhou,
412"
REFERENCES,0.351729212656365,"C. Ma, G. Li, K. C. Chang, F. Huang, R. Cheng, and Y. Li. BIRD_SQL: Leaderboard
413"
REFERENCES,0.3524650478292862,"- Execution Accuracy (EX). https://bird-bench.github.io/, 2024b. Accessed: May
414"
REFERENCES,0.35320088300220753,"2024.
415"
REFERENCES,0.3539367181751288,"A. Liu, X. Hu, L. Wen, and P. S. Yu. A comprehensive evaluation of chatgpt’s zero-shot
416"
REFERENCES,0.35467255334805003,"text-to-sql capability, 2023.
417"
REFERENCES,0.3554083885209713,"Managed
Object.
Managed
Object
Types
Overview,
418"
REFERENCES,0.3561442236938926,"VMware,
Inc.
https://vdc-download.vmware.com/
419"
REFERENCES,0.35688005886681384,"vmwb-repository/dcr-public/c476b64b-c93c-4b21-9d76-be14da0148f9/
420"
REFERENCES,0.3576158940397351,"04ca12ad-59b9-4e1c-8232-fd3d4276e52c/SDK/vsphere-ws/docs/ReferenceGuide/
421"
REFERENCES,0.35835172921265634,"index.html, 2024. Accessed: May 2024.
422"
REFERENCES,0.35908756438557765,"L. Perron and F. Didier.
Cp-sat, 2024.
URL https://developers.google.com/
423"
REFERENCES,0.3598233995584989,"optimization/cp/cp_solver/.
424"
REFERENCES,0.36055923473142015,"M. Pourreza and D. Rafiei. Dts-sql: Decomposed text-to-sql with small large language
425"
REFERENCES,0.3612950699043414,"models, 2024.
426"
REFERENCES,0.3620309050772627,"F. Rossi, P. van Beek, and T. Walsh, editors. Handbook of Constraint Programming, volume 2
427"
REFERENCES,0.36276674025018396,"of Foundations of Artificial Intelligence. Elsevier, 2006. ISBN 978-0-444-52726-4. URL
428"
REFERENCES,0.3635025754231052,"https://www.sciencedirect.com/science/bookseries/15746526/2.
429"
REFERENCES,0.36423841059602646,"J. Sequeda, D. Allemang, and B. Jacob. A benchmark to understand the role of knowledge
430"
REFERENCES,0.36497424576894777,"graphs on large language model’s accuracy for question answering on enterprise sql
431"
REFERENCES,0.365710080941869,"databases, 2023.
432"
REFERENCES,0.36644591611479027,"L. Silverston, W. H. Inmon, and K. Graziano. The data model resource book: a library of
433"
REFERENCES,0.3671817512877116,"logical data models and data warehouse designs. John Wiley & Sons, Inc., USA, 1997.
434"
REFERENCES,0.36791758646063283,"ISBN 0471153648.
435"
REFERENCES,0.3686534216335541,"TiDBCloud.
chat2query bench.
https://github.com/tidbcloud/chat2query_bench,
436"
REFERENCES,0.36938925680647533,"2020a. Accessed: May 2024.
437"
REFERENCES,0.37012509197939664,"TiDBCloud. Get Started with Chat2Query API. https://docs.pingcap.com/tidbcloud/
438"
REFERENCES,0.3708609271523179,"use-chat2query-api, 2020b. Accessed: May 2024.
439"
REFERENCES,0.37159676232523914,"VMware,
Inc.
VMware
vSphere
API
Reference
Documentation.
440"
REFERENCES,0.3723325974981604,"https://vdc-download.vmware.com/vmwb-repository/dcr-public/
441"
REFERENCES,0.3730684326710817,"c476b64b-c93c-4b21-9d76-be14da0148f9/04ca12ad-59b9-4e1c-8232-fd3d4276e52c/
442"
REFERENCES,0.37380426784400295,"SDK/vsphere-ws/docs/ReferenceGuide/index.html, 2024. Accessed: May 2024.
443"
REFERENCES,0.3745401030169242,"N. Wretblad, F. G. Riseby, R. Biswas, A. Ahmadi, and O. Holmström.
Finan-
444"
REFERENCES,0.37527593818984545,"cial
annotations,
March
2024a.
URL
https://github.com/niklaswretblad/
445"
REFERENCES,0.37601177336276675,"the-effects-of-noise-in-text-to-SQL/blob/main/annotations/financial_
446"
REFERENCES,0.376747608535688,"annotations.xlsx.
447"
REFERENCES,0.37748344370860926,"N. Wretblad, F. G. Riseby, R. Biswas, A. Ahmadi, and O. Holmström. Understanding the
448"
REFERENCES,0.37821927888153056,"effects of noise in text-to-sql: An examination of the bird-bench benchmark, 2024b.
449"
REFERENCES,0.3789551140544518,"H. Wu, C. Barrett, and N. Narodytska. Lemur: Integrating large language models in
450"
REFERENCES,0.37969094922737306,"automated program verification. In The Twelfth International Conference on Learning
451"
REFERENCES,0.3804267844002943,"Representations, 2024. URL https://openreview.net/forum?id=Q3YaCghZNt.
452"
REFERENCES,0.3811626195732156,"T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman,
453"
REFERENCES,0.3818984547461369,"Z. Zhang, and D. Radev. Spider: A large-scale human-labeled dataset for complex and cross-
454"
REFERENCES,0.3826342899190581,"domain semantic parsing and text-to-SQL task. In E. Riloff, D. Chiang, J. Hockenmaier,
455"
REFERENCES,0.3833701250919794,"and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural
456"
REFERENCES,0.3841059602649007,"Language Processing, pages 3911–3921, Brussels, Belgium, Oct.-Nov. 2018. Association for
457"
REFERENCES,0.38484179543782193,"Computational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.
458"
REFERENCES,0.3855776306107432,"org/D18-1425.
459"
REFERENCES,0.38631346578366443,"A. Zhou, K. Wang, Z. Lu, W. Shi, S. Luo, Z. Qin, S. Lu, A. Jia, L. Song, M. Zhan, and H. Li.
460"
REFERENCES,0.38704930095658574,"Solving challenging math word problems using GPT-4 code interpreter with code-based
461"
REFERENCES,0.387785136129507,"self-verification. In The Twelfth International Conference on Learning Representations,
462"
REFERENCES,0.38852097130242824,"2024. URL https://openreview.net/forum?id=c8McWs4Av0.
463"
REFERENCES,0.38925680647534955,"A
Background
464"
REFERENCES,0.3899926416482708,"Relational databases.
Let D1, . . . , Dn be a set of domains. A relation or table, t, is
465"
REFERENCES,0.39072847682119205,"defined over subset of domains: t(Xi0, . . . , Xik) ⊆Di0 × . . . × Dik, Xij ⊆Dij, j ∈[0, k]. In
466"
REFERENCES,0.3914643119941133,"addition, t defines a set of attributes (or columns) names Xi0, . . . , Xik. Projection is a unary
467"
REFERENCES,0.3922001471670346,"operation on a set of attribute names Y , Y ⊆X. The result of such projection is the set
468"
REFERENCES,0.39293598233995586,"of tuples that is obtained when all tuples in t are restricted to attributes Y . Inner join, or
469"
REFERENCES,0.3936718175128771,"simply join, is a binary operator between two tables t1 and t2 over their common attributes
470"
REFERENCES,0.39440765268579836,"Y that returns a set of all combinations of tuples in t1 and t2 that are equal on Y . Left
471"
REFERENCES,0.39514348785871967,"join, left join, is similar to the join but returns all rows of t1 filling unmatched rows of
472"
REFERENCES,0.3958793230316409,"t2 with null values. A database can support a large set of constraints over tables. The two
473"
REFERENCES,0.39661515820456217,"main constraint types are related to primary and foreign keys. A primary key is the smallest
474"
REFERENCES,0.3973509933774834,"subset of attributes guaranteed to uniquely differentiate each tuple in a table. A foreign key
475"
REFERENCES,0.3980868285504047,"is a subset of attributes Y in a table t1 that corresponds with (usually) a primary key of
476"
REFERENCES,0.398822663723326,"another table t2, with the property that the projection of t1 on Y is a subset of the projection
477"
REFERENCES,0.3995584988962472,"of t2 on Y [Beaulieu, 2009].
478"
REFERENCES,0.40029433406916853,"Design patterns.
A database typically represents entities and their interactions in real-
479"
REFERENCES,0.4010301692420898,"world processes, e.g., the financial management of a company. To effectively model these
480"
REFERENCES,0.40176600441501104,"complex entities, several design patterns have been developed [IBM, Inc., 2021, Silverston
481"
REFERENCES,0.4025018395879323,"et al., 1997]. A many-to-one pattern (m:1) specifies a relationship when any number of
482"
REFERENCES,0.4032376747608536,"attributes from one table is associated with unique attributes of the same or another table,
483"
REFERENCES,0.40397350993377484,"typically enforced by foreign key and primary key relationships. A many-to-many relationship
484"
REFERENCES,0.4047093451066961,"(m:m) occurs when any number of attributes from one table is associated with any number
485"
REFERENCES,0.40544518027961735,"of attributes from the same or another table. It is typically modeled with an auxiliary join
486"
REFERENCES,0.40618101545253865,"table that refers to the primary keys of the tables in the relationship. The lookup table is
487"
REFERENCES,0.4069168506254599,"a table that contains descriptions and code values used by multiple tables, e.g., zip codes,
488"
REFERENCES,0.40765268579838115,"country names. etc. A Star pattern is a type of relational database schema composed of a
489"
REFERENCES,0.4083885209713024,"single, central fact table surrounded by dimension tables. A Snowflake schema consists
490"
REFERENCES,0.4091243561442237,"of one fact table connected to many dimension tables, which can be connected to other
491"
REFERENCES,0.40986019131714496,"dimension tables through a many-to-one relationship.
492"
REFERENCES,0.4105960264900662,"Constraint satisfaction.
A constraint satisfaction problem (CSP) consists of a set of
493"
REFERENCES,0.41133186166298746,"variables, each with a finite domain of values, and a set of constraints specifying allowed
494"
REFERENCES,0.41206769683590877,"combinations of values for subsets of variables [Rossi et al., 2006]. A solution is an assignment
495"
REFERENCES,0.41280353200883,"of values to the variables satisfying the constraints. In the constraint optimization problem,
496"
REFERENCES,0.41353936718175127,"we are looking for a solution that optimizes a given cost function. Constraint solvers typically
497"
REFERENCES,0.4142752023546726,"explore partial assignments enforcing a local consistency property using either specialized
498"
REFERENCES,0.41501103752759383,"or general-purpose propagation algorithms and employ conflict-driven learning to store
499"
REFERENCES,0.4157468727005151,"information from failures as the search proceeds. We used OR-Tools CP-SAT solver [Perron
500"
REFERENCES,0.41648270787343633,"and Didier, 2024] in our experiments.
501"
REFERENCES,0.41721854304635764,"B
Related work
502"
REFERENCES,0.4179543782192789,"We focus on the zero-shot text-to-SQL problem, which has received significant attention
503"
REFERENCES,0.41869021339220014,"in the last few years. Liu et al. [2023] performed a comprehensive evaluation of ChatGPT
504"
REFERENCES,0.4194260485651214,"on the Spider dataset and demonstrated that it shows good performance. In [Dong et al.,
505"
REFERENCES,0.4201618837380427,"2023], a new framework based on the GPT model was proposed, involving several techniques
506"
REFERENCES,0.42089771891096395,"for promoting and post-processing the output to get more consistent results. Chang and
507"
REFERENCES,0.4216335540838852,"Fosler-Lussier [2023] proposed several techniques to improve the performance of ChatGPT.
508"
REFERENCES,0.42236938925680645,"[TiDBCloud, 2020b] represents the most recent zero-shot method. According to the API
509"
REFERENCES,0.42310522442972776,"documentation [TiDBCloud, 2020b], the authors construct a data summary object that
510"
REFERENCES,0.423841059602649,"contains ‘AI exploration information of the given database.’ This method performs very well
511"
REFERENCES,0.42457689477557026,"on the BIRD dataset. However, it relies on LLMs to reason about database relationships.
512"
REFERENCES,0.42531272994849156,"Sequeda et al. [2023] performed an interesting investigation of the performance of LLMs on
513"
REFERENCES,0.4260485651214128,"large industrial databases. They identified that GPT does not perform well when it needs
514"
REFERENCES,0.42678440029433407,"to reason about complex relationships. The authors proposed a two-step approach to tackle
515"
REFERENCES,0.4275202354672553,"this problem. As a knowledge graph is available for these benchmarks, the authors proposed
516"
REFERENCES,0.4282560706401766,"Q3: What are the total tax payment, which is the sum of Tax and Supercharge?"
REFERENCES,0.4289919058130979,"/* GPT4
generated
SQL*/: select sum(Tax.amount + Supercharge.amount)
from Tax
join PayAmount on PayAmount.id = Tax.payamt_id
join Supercharge on PayAmount.id = Supercharge.payamt_id"
REFERENCES,0.4297277409860191,"/* Correct
SQL*/:
select sum(ifnull(payTax.amount, 0)) + sum(ifnull(paySupercharge.amount,0))
from Payment
join PayAmount as payTax on Payment.id = payTax.pay_id
left join Tax on payTax.id = Tax.payamt_id
join PayAmount as paySupercharge on Payment.id = paySupercharge.pay_id
left join Supercharge on paySupercharge.id = Supercharge.payamt_id
Table 6: A user’s question Q3. Incorrect parts of the GPT answer are highlighted in red."
REFERENCES,0.4304635761589404,"using the knowledge graph as an intermediate representation. Namely, the user’s question is
517"
REFERENCES,0.4311994113318617,"answered using the KG structure with SPARQL, and this answer is automatically translated
518"
REFERENCES,0.43193524650478293,"to SQL using a given mapping from ontology to SQL (R2RML). However, while reasoning
519"
REFERENCES,0.4326710816777042,"on a knowledge graph can be easier for LLMs, it is still challenging to take all complex
520"
REFERENCES,0.43340691685062543,"relationships into account.
521"
REFERENCES,0.43414275202354674,"C
Motivation (additional materials)
522"
REFERENCES,0.434878587196468,"C.1
User’s questions
523"
REFERENCES,0.43561442236938924,"The third question Q3, ‘What are the total tax payments, which is the sum of Tax and
524"
REFERENCES,0.43635025754231055,"Supercharge?’, asks about the total amount of taxes paid from all payments (Table 6). There
525"
REFERENCES,0.4370860927152318,"are a few issues with the GPT answer. First, it outputs all payment amounts that are both
526"
REFERENCES,0.43782192788815305,"tax and supercharge. We reminded that that each payment amount can be of one type, so
527"
REFERENCES,0.4385577630610743,"the result will be empty. Second, it hallucinates as there are no amount columns in the Tax
528"
REFERENCES,0.4392935982339956,"or Supercharge tables.
529"
REFERENCES,0.44002943340691686,"C.1.1
Definition of promptB
530"
REFERENCES,0.4407652685798381,"Inputs:
DB_SCHEMA, Question
promptB
Given the database described by the following DDL: <DB_SCHEMA>.
Write a SQL query that answers the following question. Do not explain the query.
Return just the query, so it can be run verbatim from your response. Here’s the
question: <Question>. [Sequeda et al., 2023]
Returns
: SQL 531"
REFERENCES,0.44150110375275936,"D
Framework design (additional materials)
532"
REFERENCES,0.44223693892568067,"D.1
Database model (dbModel)
533"
REFERENCES,0.4429727740986019,"D.1.1
Example of a table from dbm.tables
534"
REFERENCES,0.44370860927152317,"Here is a JSON structure for the Client table from the financial dataset [Li et al., 2023].
535"
REFERENCES,0.4444444444444444,"It contains the table name, primary keys, attributes, their types, and descriptions. This
536"
REFERENCES,0.4451802796173657,"information is available in the dataset. The description of the table is generated by gpt4
537"
REFERENCES,0.445916114790287,"using the prompt promptD.
538 539"
REFERENCES,0.44665194996320823,"1
""Client"": {
540"
REFERENCES,0.44738778513612953,"2
""type"": ""ManagedObject"",
541"
REFERENCES,0.4481236203090508,"3
""primary"": [
542"
REFERENCES,0.44885945548197204,"4
""client_id""
543"
REFERENCES,0.4495952906548933,"5
],
544"
REFERENCES,0.4503311258278146,"6
""path"": ""<path -to >/ Client.json"",
545"
REFERENCES,0.45106696100073584,"7
""path_to_types"": """"<path -to >/ Client_types.json""
546"
REFERENCES,0.4518027961736571,"8
}
547
548"
REFERENCES,0.45253863134657835,"Here is the JSON structure for Client.json:
549 550"
REFERENCES,0.45327446651949965,"1
{
551"
REFERENCES,0.4540103016924209,"2
""NameField"": ""Client"",
552"
REFERENCES,0.45474613686534215,"3
"" DescriptionField "": ""Focuses on client
information,
553"
REFERENCES,0.4554819720382634,"encompassing
unique
client
identifiers, gender, birth
554"
REFERENCES,0.4562178072111847,"dates, and the
location of the branch
with
which
they
555"
REFERENCES,0.45695364238410596,"are
associated ."",
556"
REFERENCES,0.4576894775570272,"4
""client_id"": ""the unique
number"",
557"
REFERENCES,0.45842531272994846,"5
""gender"": ""
Description: 'F: female; M: male
'"",
558"
REFERENCES,0.45916114790286977,"6
""birth_date"": ""birth
date"",
559"
REFERENCES,0.459896983075791,"7
""district_id"": ""location of branch""
560"
REFERENCES,0.4606328182487123,"8
}
561
562"
REFERENCES,0.4613686534216336,"Here is the JSON structure for Client_types.json:
563 564"
REFERENCES,0.46210448859455483,"1
{
565"
REFERENCES,0.4628403237674761,"2
""NameField"": {
566"
REFERENCES,0.46357615894039733,"3
""type"": ""varchar(100)"",
567"
REFERENCES,0.46431199411331864,"4
""default"": ""DEFAULT
NULL""
568"
REFERENCES,0.4650478292862399,"5
},
569"
REFERENCES,0.46578366445916114,"6
"" DescriptionField "": {
570"
REFERENCES,0.4665194996320824,"7
""type"": ""varchar(5000)"",
571"
REFERENCES,0.4672553348050037,"8
""default"": ""DEFAULT
NULL""
572"
REFERENCES,0.46799116997792495,"9
},
573"
REFERENCES,0.4687270051508462,"10
""client_id"": {
574"
REFERENCES,0.46946284032376745,"11
""type"": ""bigint"",
575"
REFERENCES,0.47019867549668876,"12
""default"": ""NOT NULL""
576"
REFERENCES,0.47093451066961,"13
},
577"
REFERENCES,0.47167034584253126,"14
""gender"": {
578"
REFERENCES,0.47240618101545256,"15
""type"": ""varchar(46)"",
579"
REFERENCES,0.4731420161883738,"16
""default"": ""NOT NULL""
580"
REFERENCES,0.47387785136129507,"17
},
581"
REFERENCES,0.4746136865342163,"18
""birth_date"": {
582"
REFERENCES,0.4753495217071376,"19
""type"": ""date"",
583"
REFERENCES,0.4760853568800589,"20
""default"": ""NOT NULL""
584"
REFERENCES,0.4768211920529801,"21
},
585"
REFERENCES,0.4775570272259014,"22
""district_id"": {
586"
REFERENCES,0.4782928623988227,"23
""type"": ""bigint"",
587"
REFERENCES,0.47902869757174393,"24
""default"": ""NOT NULL""
588"
REFERENCES,0.4797645327446652,"25
}
589"
REFERENCES,0.48050036791758644,"26
}
590
591"
REFERENCES,0.48123620309050774,"D.1.2
Example of a m:1 relation from dbm.constraints
592"
REFERENCES,0.481972038263429,"Here is the JSON structure for the Client and District relation from the financial dataset [Li
593"
REFERENCES,0.48270787343635024,"et al., 2023].
594 595"
REFERENCES,0.48344370860927155,"1
""Client, District"": {
596"
REFERENCES,0.4841795437821928,"2
""type"": ""Relationships"",
597"
REFERENCES,0.48491537895511405,"3
""sqlrelation"": ""M:1"",
598"
REFERENCES,0.4856512141280353,"4
"" foreign_relation "": {
599"
REFERENCES,0.4863870493009566,"5
""FOREIGN"": [
600"
REFERENCES,0.48712288447387786,"6
""district_id""
601"
REFERENCES,0.4878587196467991,"7
],
602"
REFERENCES,0.48859455481972036,"8
"" foreign_relation_ref_table "": ""District"",
603"
REFERENCES,0.48933038999264167,"9
"" foreign_relation_ref_table_keys "": [
604"
REFERENCES,0.4900662251655629,"10
""district_id""
605"
REFERENCES,0.49080206033848417,"11
]
606"
REFERENCES,0.4915378955114054,"12
}
607"
REFERENCES,0.4922737306843267,"13
}
608
609"
REFERENCES,0.493009565857248,"D.1.3
Example of a m:m pattern from dbm.patterns
610"
REFERENCES,0.49374540103016923,"Here is the JSON structure for the Account and District m:m relation from the financial
611"
REFERENCES,0.49448123620309054,"dataset [Li et al., 2023].
612 613"
REFERENCES,0.4952170713760118,"1
{
614"
REFERENCES,0.49595290654893304,"2
""Account, Client"": {
615"
REFERENCES,0.4966887417218543,"3
""type"": ""Relationships"",
616"
REFERENCES,0.4974245768947756,"4
""description"": """",
617"
REFERENCES,0.49816041206769685,"5
""sqlrelation"": ""M:M"",
618"
REFERENCES,0.4988962472406181,"6
""m2m_relation"": {
619"
REFERENCES,0.49963208241353935,"7
""m2 m_middle_table"": ""Disp"",
620"
REFERENCES,0.5003679175864606,"8
""m2m_side_tables"": [
621"
REFERENCES,0.5011037527593819,"9
""Client"",
622"
REFERENCES,0.5018395879323032,"10
""Account""
623"
REFERENCES,0.5025754231052244,"11
],
624"
REFERENCES,0.5033112582781457,"12
""m2 m_relation_one"": [
625"
REFERENCES,0.5040470934510669,"13
""Disp"",
626"
REFERENCES,0.5047829286239882,"14
""Client""
627"
REFERENCES,0.5055187637969095,"15
],
628"
REFERENCES,0.5062545989698307,"16
""m2 m_relation_two"": [
629"
REFERENCES,0.506990434142752,"17
""Disp"",
630"
REFERENCES,0.5077262693156733,"18
""Account""
631"
REFERENCES,0.5084621044885945,"19
]
632"
REFERENCES,0.5091979396615158,"20
}
633"
REFERENCES,0.5099337748344371,"21
}
634"
REFERENCES,0.5106696100073583,"22
}
635
636"
REFERENCES,0.5114054451802796,"Here is the JSON structure for the Snowflake pattern rooted ta ResourcePool (Cloud
637"
REFERENCES,0.5121412803532008,"Resources dataset).
638 639"
REFERENCES,0.5128771155261221,"1
{
640"
REFERENCES,0.5136129506990434,"2
""NameField"": ""ResourcePool"",
641"
REFERENCES,0.5143487858719646,"3
""config"": {
642"
REFERENCES,0.515084621044886,"4
""cpualloc"",
643"
REFERENCES,0.5158204562178073,"5
""memalloc""
644"
REFERENCES,0.5165562913907285,"6
},
645"
REFERENCES,0.5172921265636498,"7
""runtime"": {
646"
REFERENCES,0.5180279617365711,"8
""cpu"",
647"
REFERENCES,0.5187637969094923,"9
""memory""
648"
REFERENCES,0.5194996320824136,"10
}
649"
REFERENCES,0.5202354672553348,"11
}
650
651"
REFERENCES,0.5209713024282561,"D.2
MatchTables
652"
REFERENCES,0.5217071376011774,"D.2.1
promptA
653"
REFERENCES,0.5224429727740986,"promptA requires three inputs: a user question, a set of tables (can be empty), and a set of
654"
REFERENCES,0.5231788079470199,"attributes for a given table t (can be empty).
655"
REFERENCES,0.5239146431199412,"Inputs:
Question, Tables, Attributes
promptA:
Here is a json schema. Please treat json schema objects as a description
of tables in a database <JSON(Tables, Attributes)>. The user has a query to answer
<Question>. What are all relevant json elements to a user query from the list [<list
of json elements>]? Output is a list of elements, [element, element, element,...]. Do
not explain.
Returns:
We post-process the output to extract a set of tables and their attributes
(RT ) and relevant tables T 656"
REFERENCES,0.5246504782928624,"In the prompt, we provide description of tables and attributes from dbm. We show a few
657"
REFERENCES,0.5253863134657837,"examples of <JSON(Tables, Attributes)> and the corresponding <list of json elements>.
658"
REFERENCES,0.5261221486387049,"Example D.1. Here is an example of a JSON(Tables, {}) used in line 5 in Algorithm 1 for
659"
REFERENCES,0.5268579838116262,"financial dataset. The goal is to determine relevant core tables.
660 661"
REFERENCES,0.5275938189845475,"1
{
662"
REFERENCES,0.5283296541574687,"2
""Account"": ""Manages
financial
accounts, tracking
each
663"
REFERENCES,0.52906548933039,"account 's unique
identification, the
location of the
664"
REFERENCES,0.5298013245033113,"associated
bank
branch, the
frequency of account
665"
REFERENCES,0.5305371596762325,"servicing, and the account 's creation
date. It
666"
REFERENCES,0.5312729948491538,"categorizes
the
servicing
frequency
with
options
like
667"
REFERENCES,0.5320088300220751,"monthly, weekly, and post -transaction
issuances
668"
REFERENCES,0.5327446651949963,"Properties of Account: account_id, district_id,
669"
REFERENCES,0.5334805003679176,"frequency, date. "",
670"
REFERENCES,0.5342163355408388,"3
""Card"": ""Manages of credit
cards, incorporating
unique
671"
REFERENCES,0.5349521707137601,"identifiers
for each card and the
related
672"
REFERENCES,0.5356880058866814,"dispositions. It also
categorizes
credit
cards
into
673"
REFERENCES,0.5364238410596026,"various
classes, such as junior, standard, and
674"
REFERENCES,0.5371596762325239,"high -level, reflecting
their
tier and
associated
675"
REFERENCES,0.5378955114054452,"benefits. Properties of Card: card_id, disp_id, type,
676"
REFERENCES,0.5386313465783664,"issued. "",
677"
REFERENCES,0.5393671817512877,"4
""Client"": ""Focuses on client
information, encompassing
678"
REFERENCES,0.5401030169242089,"unique
client
identifiers, gender, birth
dates, and
679"
REFERENCES,0.5408388520971302,"the
location of the branch
with
which
they are
680"
REFERENCES,0.5415746872700515,"associated. Properties of Client: client_id, gender,
681"
REFERENCES,0.5423105224429727,"birth_date, district_id. "",
682"
REFERENCES,0.543046357615894,"5
""Disp"": ""Manage
dispositions in financial
accounts. It
683"
REFERENCES,0.5437821927888153,"contains a unique
identifier
for each
record, links
684"
REFERENCES,0.5445180279617365,"each
disposition to specific
clients
and
accounts,
685"
REFERENCES,0.5452538631346578,"and
categorizes
the nature of each
disposition
into
686"
REFERENCES,0.5459896983075792,"types
like 'OWNER ', 'USER ', or 'DISPONENT '.
687"
REFERENCES,0.5467255334805003,"Properties of Disp: disp_id, client_id, account_id,
688"
REFERENCES,0.5474613686534217,"type. "",
689"
REFERENCES,0.5481972038263428,"6
""District"": ""Provides a detailed
overview of
690"
REFERENCES,0.5489330389992642,"district -level
data, essential
for
regional
analysis
691"
REFERENCES,0.5496688741721855,"and decision -making. It includes a unique
identifier
692"
REFERENCES,0.5504047093451067,"for each
district, along
with the district 's name and
693"
REFERENCES,0.551140544518028,"its
broader
region. The table
delves
into
694"
REFERENCES,0.5518763796909493,"demographic, economic
data and
economic
indicators,
695"
REFERENCES,0.5526122148638705,"records
crime
statistics. Properties of District:
696"
REFERENCES,0.5533480500367918,"district_id, A2, A3, A4, A5, A6, A7, A8, A9, A10,
697"
REFERENCES,0.5540838852097131,"A11, A12, A13, A14, A15, A16. "",
698"
REFERENCES,0.5548197203826343,"7
""Loan"": ""Manages loan -related
data, offering
insights
699"
REFERENCES,0.5555555555555556,"into each loan 's unique
identifier, associated
700"
REFERENCES,0.5562913907284768,"account
details, approval
dates, amounts, durations,
701"
REFERENCES,0.5570272259013981,"and
monthly
payments. Properties of Loan: loan_id,
702"
REFERENCES,0.5577630610743194,"account_id, date, amount, duration, payments, status.
703"
REFERENCES,0.5584988962472406,""",
704"
REFERENCES,0.5592347314201619,"8
""Order_"": ""Manages
payment
orders, detailing
unique
705"
REFERENCES,0.5599705665930832,"identifiers
for each
order, linked
account
numbers,
706"
REFERENCES,0.5607064017660044,"and
recipient
bank
details. It captures
the bank and
707"
REFERENCES,0.5614422369389257,"account
number, the
debited
amount for each
order and
708"
REFERENCES,0.5621780721118469,"categorizes
the
purpose of each
payment. Properties
709"
REFERENCES,0.5629139072847682,"of Order_: order_id, account_id, bank_to, account_to,
710"
REFERENCES,0.5636497424576895,"amount, k_symbol. "",
711"
REFERENCES,0.5643855776306107,"9
""Trans"": ""Includes
transaction
management, encompassing
712"
REFERENCES,0.565121412803532,"details
such as transaction
identifiers, associated
713"
REFERENCES,0.5658572479764533,"account
numbers, and dates of transactions,
714"
REFERENCES,0.5665930831493745,"categorizes
transactions, covering a range of
715"
REFERENCES,0.5673289183222958,"activities
from
insurance
payments
and
statement
fees
716"
REFERENCES,0.5680647534952171,"to interest
credits, sanctions
for
negative
balances,
717"
REFERENCES,0.5688005886681383,"household
payments, pension
disbursements, and loan
718"
REFERENCES,0.5695364238410596,"payments; and
details
about the
transaction
partner 's
719"
REFERENCES,0.5702722590139808,"bank, identified by a unique two -letter
code, and
720"
REFERENCES,0.5710080941869021,"their
account
number. Properties of Trans: trans_id,
721"
REFERENCES,0.5717439293598234,"account_id, date, type, operation, amount, balance,
722"
REFERENCES,0.5724797645327446,"k_symbol, bank, account. ""
723"
REFERENCES,0.5732155997056659,"10
}
724
725"
REFERENCES,0.5739514348785872,"The list of JSON elements is as follows
726 727"
REFERENCES,0.5746872700515084,"1
['Account ', 'Card ', 'Client ', 'Disp ', 'District ', 'Loan ',
728"
REFERENCES,0.5754231052244297,"'Order_ ', 'Trans ']
729
730"
REFERENCES,0.5761589403973509,"Example D.2. Here is an example of a JSON({}, Attributes) used in line 11 in Algorithm 1
731"
REFERENCES,0.5768947755702722,"for the table District to determine relevant attributes (from the financial dataset).
732 733"
REFERENCES,0.5776306107431936,"1
{
734"
REFERENCES,0.5783664459161147,"2
"" DescriptionField "": ""Provides a detailed
overview of
735"
REFERENCES,0.579102281089036,"district -level
data, essential
for
regional
analysis
736"
REFERENCES,0.5798381162619574,"and decision -making. It includes a unique
identifier
737"
REFERENCES,0.5805739514348786,"for each
district, along
with the district 's name and
738"
REFERENCES,0.5813097866077999,"its
broader
region. The table
delves
into
739"
REFERENCES,0.5820456217807212,"demographic, economic
data and
economic
indicators,
740"
REFERENCES,0.5827814569536424,"records
crime
statistics ."",
741"
REFERENCES,0.5835172921265637,"3
""district_id"": ""location of branch"",
742"
REFERENCES,0.5842531272994849,"4
""A2"": ""district_name"",
743"
REFERENCES,0.5849889624724062,"5
""A3"": ""region"",
744"
REFERENCES,0.5857247976453275,"6
""A4"": """",
745"
REFERENCES,0.5864606328182487,"7
""A5"": ""municipality < district < region"",
746"
REFERENCES,0.58719646799117,"8
""A6"": ""municipality < district < region"",
747"
REFERENCES,0.5879323031640913,"9
""A7"": ""municipality < district < region"",
748"
REFERENCES,0.5886681383370125,"10
""A8"": ""municipality < district < region"",
749"
REFERENCES,0.5894039735099338,"11
""A9"": ""
Description: not useful"",
750"
REFERENCES,0.5901398086828551,"12
""A10"": ""ratio of urban
inhabitants"",
751"
REFERENCES,0.5908756438557763,"13
""A11"": ""average
salary"",
752"
REFERENCES,0.5916114790286976,"14
""A12"": ""unemployment
rate 1995"",
753"
REFERENCES,0.5923473142016188,"15
""A13"": ""unemployment
rate 1996"",
754"
REFERENCES,0.5930831493745401,"16
""A14"": ""no. of entrepreneurs
per 1000 inhabitants"",
755"
REFERENCES,0.5938189845474614,"17
""A15"": ""no. of committed
crimes 1995"",
756"
REFERENCES,0.5945548197203826,"18
""A16"": ""no. of committed
crimes 1996""
757"
REFERENCES,0.5952906548933039,"19
}
758
759"
REFERENCES,0.5960264900662252,"The list of json elements is as follows
760 761"
REFERENCES,0.5967623252391464,"1
[district_id, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12,
762"
REFERENCES,0.5974981604120677,"A13, A14, A15, A16]
763
764"
REFERENCES,0.5982339955849889,"Moreover, if a table is a root table of a pattern, we provide inner tables and their attribute
765"
REFERENCES,0.5989698307579102,"names so that an LLM can determine the relevance of Snowflake to the user question.
766"
REFERENCES,0.5997056659308315,"Example D.3. Here is an example of the Snowflake summary rooted at ResourcePool
767"
REFERENCES,0.6004415011037527,"from the Cloud Resources benchmark.
768 769"
REFERENCES,0.601177336276674,"1
""ResourcePool"": ""Resource
pools
manage VM resources
within a
770"
REFERENCES,0.6019131714495953,"hierarchy, ensuring
efficient
allocation
through
771"
REFERENCES,0.6026490066225165,"configurable
settings
and states. Properties of
772"
REFERENCES,0.6033848417954378,"ResourcePool: namespace, name, owner, summary, config,
773"
REFERENCES,0.6041206769683591,"config, config.changeVersion, config.entity,
774"
REFERENCES,0.6048565121412803,"config.lastModified, config. scaleDescendantsShares ,
775"
REFERENCES,0.6055923473142016,"config.cpualloc, config.cpualloc,
776"
REFERENCES,0.6063281824871228,"config.cpualloc. expandableReservation ,
777"
REFERENCES,0.6070640176600441,"config.cpualloc.limit_, config.cpualloc.overheadLimit,
778"
REFERENCES,0.6077998528329654,"config.cpualloc.reservation, config.cpualloc.shares,
779"
REFERENCES,0.6085356880058866,"config.cpualloc, config.memalloc, config.memalloc,
780"
REFERENCES,0.609271523178808,"config.memalloc. expandableReservation ,
781"
REFERENCES,0.6100073583517293,"config.memalloc.limit_, config.memalloc.overheadLimit,
782"
REFERENCES,0.6107431935246505,"config.memalloc.reservation, config.memalloc.shares,
783"
REFERENCES,0.6114790286975718,"config.memalloc, config, runtime, runtime,
784"
REFERENCES,0.6122148638704931,"runtime.overallStatus, runtime.sharesScalable ,
785"
REFERENCES,0.6129506990434143,"runtime.cpu, runtime.cpu, runtime.cpu.maxUsage,
786"
REFERENCES,0.6136865342163356,"runtime.cpu.overallUsage, runtime.cpu. reservationUsed ,
787"
REFERENCES,0.6144223693892568,"runtime.cpu. reservationUsedForVm ,
788"
REFERENCES,0.6151582045621781,"runtime.cpu. unreservedForPool ,
789"
REFERENCES,0.6158940397350994,"runtime.cpu. unreservedForVm , runtime.cpu, runtime.memory,
790"
REFERENCES,0.6166298749080206,"runtime.memory, runtime.memory.maxUsage,
791"
REFERENCES,0.6173657100809419,"runtime.memory.overallUsage,
792"
REFERENCES,0.6181015452538632,"runtime.memory. reservationUsed ,
793"
REFERENCES,0.6188373804267844,"runtime.memory. reservationUsedForVm ,
794"
REFERENCES,0.6195732155997057,"runtime.memory. unreservedForPool ,
795"
REFERENCES,0.6203090507726269,"runtime.memory. unreservedForVm , runtime.memory, runtime,
796"
REFERENCES,0.6210448859455482,"ResourcePool_id . ""
797
798"
REFERENCES,0.6217807211184695,"D.2.2
Description of the IterativePrompting algorithm.
799"
REFERENCES,0.6225165562913907,Algorithm 2 IterativePrompting
REFERENCES,0.623252391464312,"Require: Q, t
Ensure: Relevant tables and attributes in a tree-like pattern rooted at t"
REFERENCES,0.6239882266372333,"1: stack_tables = [t]
2: RT = {}
3: while stack_tables do
4:
r = stack_tables.pop()
5:
// check if r is a leaf in a tree-like pattern
6:
if leaf(r) then
7:
R′
T , _ = promptA(Q, {}, r.attributes)
8:
else
9:
// find children of r in a tree-like pattern
10:
children_tables = {t|t ∈dbm.tables ∩children(r)} // children(r) returns descendants of r in the pattern.
11:
R′
T , T = promptA(Q, children_tables, r)
12:
stack_tables.push(T )"
REFERENCES,0.6247240618101545,"13:
RT = RT ∪R′
T"
REFERENCES,0.6254598969830758,"Example D.4 (Full version of Example 3.1 for the question Q2). Consider Q2 from Table 1.
800"
REFERENCES,0.6261957321559971,"Figure 3 shows ddo’s core tables. LLM identifies ResourcePool as a relevant table
801"
REFERENCES,0.6269315673289183,"in line 5, along with its attribute ResourcePool.name. Since ResourcePool is the
802"
REFERENCES,0.6276674025018396,"root table of a Snowflake pattern, we begin to explore the pattern tree in a breadth-first
803"
REFERENCES,0.6284032376747608,"order using IterativePrompting in line 10. See Figure 1 for the structure of the the
804"
REFERENCES,0.6291390728476821,"Snowflake pattern. ResourcePool has two child nodes, Config and Runtime, and
805"
REFERENCES,0.6298749080206034,"several attributes. We then query an LLM and find that both Config and Runtime are
806"
REFERENCES,0.6306107431935246,"relevant as well its attribute ResourcePool.name. Following the breadth-first search order,
807"
REFERENCES,0.6313465783664459,"/* --Summary
view V --*/
create
view V as select
Client.id as Client_id,
Client.name as Client_name,
Client.gender as Client_gender,
Datacenter.name as Datacenter_name,
Datacenter.id as Datacenter_id"
REFERENCES,0.6320824135393672,"from Datacenter
join Compute on Datacenter.id = Compute.dc_id
join ResourcePool on Compute.id = ResourcePool.compute_id
join RsPool2Client on ResourcePool.id = RsPool2Client.rspool_id
join Client on Client.id = RsPool2Client.client_id"
REFERENCES,0.6328182487122884,"/* --Final
query Q--*/
select Client_name,
Datacenter_name"
REFERENCES,0.6335540838852097,"from V
where Datacenter_id > 1;
Table 7: GenerateView and QueryView results for Q1."
REFERENCES,0.6342899190581309,"we next consider Config which has descendants cCPU and cMemory and a few attributes.
808"
REFERENCES,0.6350257542310522,"We discover that only one of them, cCPU, is relevant. We then move to the next table in
809"
REFERENCES,0.6357615894039735,"order, Runtime. It has two descendants rCPU and rMemory and a few attributes. We
810"
REFERENCES,0.6364974245768947,"discover that only one of them, rCPU, is relevant. Next, we identify relevant attributes of
811"
REFERENCES,0.637233259749816,"cCPU in line 7 (Algorithm 2) and find that cCPU.overheadlimit is relevant to the user
812"
REFERENCES,0.6379690949227373,"query. Finally, we identify relevant attributes of rCPU in line 7 in (Algorithm 2) and find
813"
REFERENCES,0.6387049300956585,"that rCPU.overallusage is relevant to the user query.
814"
REFERENCES,0.6394407652685798,"D.3
The GenerateView phase
815"
REFERENCES,0.6401766004415012,"D.3.1
Summary view.
816"
REFERENCES,0.6409124356144223,"Consider again the question Q1 from Example 3.1. The view V that corresponds to the
817"
REFERENCES,0.6416482707873437,"green path in Figure 3 is shown in the listing in Table 7. We keep the same set of attributes
818"
REFERENCES,0.6423841059602649,"that MatchTables identified. In addition, we also perform renaming of all attributes, as we
819"
REFERENCES,0.6431199411331862,"can control the length of the aliases (in case they are too long). For example, Client.name
820"
REFERENCES,0.6438557763061075,"gets an alias Client_name, Client.gender gets Client_gender, so on.
821"
REFERENCES,0.6445916114790287,"D.4
The QueryView phase
822"
REFERENCES,0.64532744665195,"D.4.1
promptC
823"
REFERENCES,0.6460632818248713,"Here is promptC that we use in the final phase QueryView (Algorithm 1, line 21). The
824"
REFERENCES,0.6467991169977925,"function name() returns name of the view V.
825"
REFERENCES,0.6475349521707138,"Inputs:
Question, V
promptC
I created a view table <name(V)> with all relevant information. Here
is a view <V >. Please write MySQL query to name(V) view to answer the following
question: <Question>.
Use only name(V) columns in the query.
Absolutely NO
columns renaming. Absolutely NO HAVING operators. Absolutely NO COUNT(*).
Output query that I can run via python interface. Output ’“‘sql...’. Do not explain.
Returns:
SQL 826"
REFERENCES,0.6482707873436351,"We used a few assertive statements that we discuss next. ’Absolutely NO column renaming’
827"
REFERENCES,0.6490066225165563,"means that we want to use aliases in the view table to form a valid SQL query. The statement
828"
REFERENCES,0.6497424576894776,"’Absolutely NO HAVING operators.’ reflects our observation that gpt4 cannot generate
829"
REFERENCES,0.6504782928623988,"valid SQL when using HAVING in combination with GROUP BY. It is a subject of future
830"
REFERENCES,0.6512141280353201,"research to deal with MySQL constraints, so we encourage QueryView to avoid this operator.
831"
REFERENCES,0.6519499632082414,"Finally, we discourage the use of COUNT(*), ‘Absolutely NO COUNT(*)’, to ensure that
832"
REFERENCES,0.6526857983811626,"gpt4 focuses on counting the entities specified in the user’s question.
833"
REFERENCES,0.6534216335540839,"We noticed that better results are obtained if we provide a description of tables that are used
834"
REFERENCES,0.6541574687270052,"to generate this view together with their relevant attributes. Here is an extended version of
835"
REFERENCES,0.6548933038999264,"promptC where we provide relevant tables and their attributes that are used to obtain the
836"
REFERENCES,0.6556291390728477,"V. We also provide an evidence if available.
837"
REFERENCES,0.6563649742457689,"Inputs:
Question, V, DB_SCHEMA
promptC’ (with evidence and a part of the schema)
Here is a SQL schema
for in MySQL: <DB_SCHEMA> I created a view table <name(V)> with all relevant
information.
Here is a view <V >.
Please write MySQL query to name(V) view
to answer the following question: <Question>.
Additional knowledge to answer:
<Evidence>
Use only name(V) columns in the query.
Absolutely NO columns
renaming. Absolutely NO HAVING operators. Absolutely NO COUNT(*). Output
query that I can run via python interface. Output ’“‘sql...’. Do not explain.
Returns:
SQL 838"
REFERENCES,0.6571008094186902,"E
Experimental evaluation (additional materials)
839"
REFERENCES,0.6578366445916115,"E.1
Setup
840"
REFERENCES,0.6585724797645327,"We run experiments on a laptop Intel(r) Core 2.40Hz and 32GB of memory. For nsql we use
841"
REFERENCES,0.659308314937454,"the largest model with 7B parameters (NumbersStation/nsql-llama-2-7B [Labs, 2023a]). For
842"
REFERENCES,0.6600441501103753,"gpt4 and Lucy, we use the ‘gpt-4-0125-preview’ model as a LLM and set the temperature
843"
REFERENCES,0.6607799852832965,"to 0.2 . We do not fine-tune a LLM. We require 20 answers from gpt4 for each question. If
844"
REFERENCES,0.6615158204562178,"the number of correct answers is more than 5, then we count that benchmark as solved.
845"
REFERENCES,0.6622516556291391,"In the case of Lucy, we require 5 answers for each GPT call for the MatchTables phase.
846"
REFERENCES,0.6629874908020603,"We sort tables based on the number of occurrences in these answers and take at most 8
847"
REFERENCES,0.6637233259749816,"candidates among relevant tables from each promptA output. Similarly to gpt4, we require
848"
REFERENCES,0.6644591611479028,"20 answers from QueryView and decide on the success as described above. We use ORTools
849"
REFERENCES,0.6651949963208241,"as a constraint solver [Perron and Didier, 2024].
850"
REFERENCES,0.6659308314937454,"We support MySQL as a relational database. However, BIRD uses SQLite. We automatically,
851"
REFERENCES,0.6666666666666666,"converted queries from sqlite to MySQL.
852"
REFERENCES,0.6674025018395879,"We provide all benchmarks and their results in the supplementary materials.
853"
REFERENCES,0.6681383370125092,"E.2
ACME insurance
854"
REFERENCES,0.6688741721854304,"Note on the database.
There are a few issues with broken relational constraints due to
855"
REFERENCES,0.6696100073583517,"missing tables, as reported [datadotworld, Inc., 2024], which we fixed by adding the missing
856"
REFERENCES,0.6703458425312729,"tables from the original database.
857"
REFERENCES,0.6710816777041942,"Extended schema examples.
Example of tables extended with comments that describe
858"
REFERENCES,0.6718175128771156,"each attribute for the ACME insurance benchmark.
859"
"CREATE
TABLE",0.6725533480500367,"860
CREATE
TABLE
Claim_Amount
861 (
862"
"CREATE
TABLE",0.673289183222958,"Claim_Amount_Identifier
bigint
NOT NULL
COMMENT
Claim
Amount
863"
"CREATE
TABLE",0.6740250183958794,"Identifier is the unique
identifier of the
financial
864"
"CREATE
TABLE",0.6747608535688006,"amount
reserved , paid , or collected in connection
with a
865"
"CREATE
TABLE",0.6754966887417219,"claim. The money
being
paid or collected
for
settling a
866"
"CREATE
TABLE",0.6762325239146432,"claim and paying the claimants , reinsurers , other
867"
"CREATE
TABLE",0.6769683590875644,"insurers , and other
interested
parties. Claim
amounts
are
868"
"CREATE
TABLE",0.6777041942604857,"classified by various
attributes.,
869"
"CREATE
TABLE",0.6784400294334069,"Claim_Identifier
int
NOT NULL
COMMENT
Claim
Identifier
870"
"CREATE
TABLE",0.6791758646063282,"is the unique
identifier
for a Claim.,
871"
"CREATE
TABLE",0.6799116997792495,"Claim_Offer_Identifier
int
NULL
COMMENT
Claim
Offer
872"
"CREATE
TABLE",0.6806475349521707,"Identifier is the unique
identifier
for a Claim
Offer.,
873"
"CREATE
TABLE",0.681383370125092,"Amount_Type_Code
varchar (20)
NULL
COMMENT
Amount
Type
874"
"CREATE
TABLE",0.6821192052980133,"Code
defines
the
category to which a monetary
amount
will
875"
"CREATE
TABLE",0.6828550404709345,"be applied. Example:
premium , commission , tax ,
876"
"CREATE
TABLE",0.6835908756438558,"surcharge.,
877"
"CREATE
TABLE",0.6843267108167771,"Event_Date
datetime
NULL
COMMENT
Event
Date is the
878"
"CREATE
TABLE",0.6850625459896983,"date on which a transaction or insurance -related
879"
"CREATE
TABLE",0.6857983811626196,"happening
takes
place.,
880"
"CREATE
TABLE",0.6865342163355408,"Claim_Amount
decimal (15 ,2)
NULL
COMMENT
The money
881"
"CREATE
TABLE",0.6872700515084621,"being
paid or collected
for
settling a claim and paying
882"
"CREATE
TABLE",0.6880058866813834,"the claimants , reinsurers , other
insurers , and other
883"
"CREATE
TABLE",0.6887417218543046,"interested
parties. Claim
amounts
are
classified by
884"
"CREATE
TABLE",0.6894775570272259,"various
attributes.,
885"
"CREATE
TABLE",0.6902133922001472,"Insurance_Type_Code
char (1)
NULL
COMMENT
Insurance
Type
886"
"CREATE
TABLE",0.6909492273730684,"Code
represents
the
category
under
which
risk is assumed.
887"
"CREATE
TABLE",0.6916850625459897,"Examples: Direct for
policies
directly
issued by a
888"
"CREATE
TABLE",0.6924208977189109,"company; Assumed
for risks
assumed
from
another
company;
889"
"CREATE
TABLE",0.6931567328918322,"Ceded for
portions of risk
ceded to another
insurer.,
890"
"CREATE
TABLE",0.6938925680647535,"PRIMARY
KEY ( Claim_Amount_Identifier
ASC),
891"
"CREATE
TABLE",0.6946284032376747,"FOREIGN
KEY ( Claim_Offer_Identifier ) REFERENCES
892"
"CREATE
TABLE",0.695364238410596,"Claim_Offer( Claim_Offer_Identifier ),
893"
"CREATE
TABLE",0.6961000735835173,"FOREIGN
KEY ( Claim_Identifier ) REFERENCES
Claim( Claim_Identifier )
894"
"CREATE
TABLE",0.6968359087564385,")
895
896"
"CREATE
TABLE",0.6975717439293598,"897
898"
"CREATE
TABLE",0.6983075791022811,"CREATE
TABLE
Claim_Reserve
899 (
900"
"CREATE
TABLE",0.6990434142752023,"Claim_Amount_Identifier
bigint
NOT NULL
COMMENT
Claim
Amount
901"
"CREATE
TABLE",0.6997792494481236,"Identifier is the unique
identifier of the
financial
902"
"CREATE
TABLE",0.7005150846210448,"amount
reserved , paid , or collected in connection
with a
903"
"CREATE
TABLE",0.7012509197939661,"claim. The amount of expected
loss over the life of the
904"
"CREATE
TABLE",0.7019867549668874,"Claim.,
905"
"CREATE
TABLE",0.7027225901398086,"PRIMARY
KEY ( Claim_Amount_Identifier
ASC),
906"
"CREATE
TABLE",0.70345842531273,"FOREIGN
KEY ( Claim_Amount_Identifier ) REFERENCES
907"
"CREATE
TABLE",0.7041942604856513,"Claim_Amount( Claim_Amount_Identifier )
908"
"CREATE
TABLE",0.7049300956585725,")
909
910"
"CREATE
TABLE",0.7056659308314938,"E.2.1
Challenging questions
911"
"CREATE
TABLE",0.7064017660044151,"In this section, we present 13 questions that Lucy found challenging to answer and identify
912"
"CREATE
TABLE",0.7071376011773363,"reasons for these failures.
913"
"CREATE
TABLE",0.7078734363502576,"Question1:
What are the loss payment, loss reserve, expense payment, expense
reserve amount by claim number and corresponding policy number, policy holder,
premium amount paid, the catastrophe it had, and the agent who sold it?
Reason:
Multiple lookups. ""policy holder"" and ""agent"" require a look up to the
same table Agreement_Party_Role. 914"
"CREATE
TABLE",0.7086092715231788,"Question2:
What are the total loss, which is the sum of loss payment, loss reserve,
expense payment, expense reserve amount by claim number and corresponding policy
number, policy holder and premium amount paid?
Reason:
Phase 1 issue. Phase 1 misses the relevant table Agreement_Party_Role. 915"
"CREATE
TABLE",0.7093451066961001,"Question3:
What is the total amount of premiums that a policy holder has paid?
Reason:
Phase 3 issue. Phase 3 makes a mistake in the group by clause. 916"
"CREATE
TABLE",0.7100809418690214,"Question4:
What are the total loss, which is the sum of loss payment, loss reserve,
expense payment, expense reserve amount by catastrophe and policy number?
Reason:
Ambiguous question. By ""by catastrophe"", the user means to output
Catastrophe’s attribute Name. However, Phase 1 identifies Catastrophe’s attribute
Identifier as relevant instead of Name. 917"
"CREATE
TABLE",0.7108167770419426,"Question5:
What is the average policy size which is the the total amount of
premium divided by the number of policies?
Reason:
Ambiguous question. The definition of average is not standard, as the
same policy can have multiple amount values. 918"
"CREATE
TABLE",0.7115526122148639,"Question6:
What are the loss payment, loss reserve, expense payment, expense
reserve amount by claim number and corresponding policy number, policy holder,
premium amount paid and the agent who sold it?
Reason:
Multiple lookups. 919"
"CREATE
TABLE",0.7122884473877852,"Question7:
Return agents and the policy they have sold that have had a claim
and the corresponding catastrophe it had.
Reason:
Ambiguous question. The output includes Company_Claim_Number,
although this information is not specified in the question. 920"
"CREATE
TABLE",0.7130242825607064,"Question8:
What is the loss ratio of each policy and agent who sold it by policy
number and agent id?
Reason:
Ambiguous question. ""the loss ratio"" is a complex formula here, making it
hard to guess without its proper specification. 921"
"CREATE
TABLE",0.7137601177336277,"Question9:
What are all the premiums that have been paid by policy holders?
Reason:
Ambiguous question. Policy.Policy_Number and Party_Identifier should
be included in the output. But they are not specified in the question. 922"
"CREATE
TABLE",0.7144959529065489,"Question10:
What are the loss payment, loss reserve, expense payment, expense
reserve amount by claim number and corresponding policy number, policy holder and
premium amount paid?
Reason:
Phase 1 issue. Phase 1 misses the relevant table Agreement_Party_Role. 923"
"CREATE
TABLE",0.7152317880794702,"Question11:
What is the loss ratio, number of claims, total loss by policy number
and premium where total loss is the sum of loss payment, loss reserve, expense
payment, expense reserve amount and loss ratio is total loss divided by premium?
Reason:
Phase 1 issue. Phase 1 misses the relevant table Policy. 924"
"CREATE
TABLE",0.7159676232523915,"Question12:
What are the total loss, which is the sum of loss payment, loss
reserve, expense payment, expense reserve amount by claim number, catastrophe and
corresponding policy number?
Reason:
Phase 1 issue. Phase 1 misses the relevant table Catastrophe. 925"
"CREATE
TABLE",0.7167034584253127,"Question13:
What is the total amount of premiums that a policy holder has paid
by policy number?
Reason:
Ambiguous question. Party_Identifier is included in the output. But it is
not specified in the question. 926"
"CREATE
TABLE",0.717439293598234,"E.3
BIRD datasets
927"
"CREATE
TABLE",0.7181751287711553,"E.3.1
Additional notes on the dataset.
928"
"CREATE
TABLE",0.7189109639440765,"Note on dbModel.
We used attribute descriptions available in BIRD in dbModel. We
929"
"CREATE
TABLE",0.7196467991169978,"also build table descriptions in the following way. We provided the description from BIRD to
930"
"CREATE
TABLE",0.7203826342899191,"an LLM to generate a short summary description using promptD defined in Section E.3.2.
931"
"CREATE
TABLE",0.7211184694628403,"Note on datasets.
It has been shown that there are a number of incorrect ground truth
932"
"CREATE
TABLE",0.7218543046357616,"SQLs in BIRD datasets [Hui, 2024, Wretblad et al., 2024b]. For example, Wretblad et al.
933"
"CREATE
TABLE",0.7225901398086828,"[2024b] found that 72 out of 106 benchmark questions in financial have errors of various
934"
"CREATE
TABLE",0.7233259749816041,"types. Most of the issues have been reported to the authors from multiple sources, and we
935"
"CREATE
TABLE",0.7240618101545254,"also reported additional problems via private communication. The authors acknowledge
936"
"CREATE
TABLE",0.7247976453274466,"these issues and are working on them. To provide an example we reported from formula1:
937"
"CREATE
TABLE",0.7255334805003679,"• Question: ‘Where can the introduction of the races held on Circuit de Barcelona-
938"
"CREATE
TABLE",0.7262693156732892,"Catalunya be found?’
939"
"CREATE
TABLE",0.7270051508462104,"• Ground truth SQL: select distinct circuits.url from circuits inner join races
940"
"CREATE
TABLE",0.7277409860191317,"on races.circuitId = circuits.circuitId where circuits.name = ’Circuit de Barcelona-
941"
"CREATE
TABLE",0.7284768211920529,"Catalunya’.
942"
"CREATE
TABLE",0.7292126563649742,"• The issue is that select should be on race.url rather than circuits.url as the user
943"
"CREATE
TABLE",0.7299484915378955,"requests information about the race, not the circuit.
944"
"CREATE
TABLE",0.7306843267108167,"On top of that, there are logical inconsistencies in ground truth answers for the financial
945"
"CREATE
TABLE",0.731420161883738,"dataset. Often, users ask for information about clients’ accounts. Client and account tables
946"
"CREATE
TABLE",0.7321559970566593,"have a m:m relationship modeled using an additional table disp. At the same time, they are
947"
"CREATE
TABLE",0.7328918322295805,"both related to a lookup table district. Unfortunately, many ground truth SQL statements
948"
"CREATE
TABLE",0.7336276674025018,"perform a join between clients and accounts via the district table, which is incorrect. Let’s
949"
"CREATE
TABLE",0.7343635025754232,"consider an example.
950"
"CREATE
TABLE",0.7350993377483444,"• Question: ‘Please provide the IDs of the 3 female clients with the largest loans.’
951"
"CREATE
TABLE",0.7358351729212657,"• Ground truth SQL: select T1.client_id from client as T1 inner join account as
952"
"CREATE
TABLE",0.7365710080941869,"T2 on T1.district_id = T2.district_id inner join loan as T3 on T2.account_id =
953"
"CREATE
TABLE",0.7373068432671082,"T3.account_id where T1.gender = ’F’ order by T3.amount DESC LIMIT 3
954"
"CREATE
TABLE",0.7380426784400295,"• The issue is that the answer relates clients and accounts that have the same dis-
955"
"CREATE
TABLE",0.7387785136129507,"trict_id. However, this does not mean that the client is related to the account.
956"
"CREATE
TABLE",0.739514348785872,"As the authors are working on corrections, we analyzed the reported issues and manually
957"
"CREATE
TABLE",0.7402501839587933,"corrected the ground truth. We only adjusted the SQL ground truth values to match
958"
"CREATE
TABLE",0.7409860191317145,"the user questions; we did not alter the questions or evidences. We provide the corrected
959"
"CREATE
TABLE",0.7417218543046358,"benchmarks in the supplementary material. Specifically, we corrected the financial and
960"
"CREATE
TABLE",0.7424576894775571,"formula1 benchmarks and used the correct answers to evaluate all methods. Interestingly,
961"
"CREATE
TABLE",0.7431935246504783,"the performance of all frameworks improved by a few percentage points when we fixed these
962"
"CREATE
TABLE",0.7439293598233996,"ground truth SQL statements.
963"
"CREATE
TABLE",0.7446651949963208,"E.3.2
promptD
964"
"CREATE
TABLE",0.7454010301692421,"Here is promptD that we use to generate tables summaries for financial and formula1
965"
"CREATE
TABLE",0.7461368653421634,"datasets.
966"
"CREATE
TABLE",0.7468727005150846,"promptD
Give me a very brief description of the <NAME> table. 967"
"CREATE
TABLE",0.7476085356880059,"Example for the Loan table (the financial database)
Prompt:
Give me a very brief description of the Loan table.
original_column_name,column_name,column_description,data_format,
value_description
loan_id„the id number identifying the loan data,integer,
account_id„the id number identifying the account,integer,
date„the date when the loan is approved,date,
amount„approved amount,integer,unit: US dollar
duration„loan duration,integer,unit: month
payments,monthly payments,monthly payments,real,unit: month
status„repayment status,text,""’A’ stands for contract finished, no problems; ’B’ stands
for contract finished, loan not paid; ’C’ stands for running contract, OK so far; ’D’
stands for running contract, client in debt""
Return:
The Loan table manages loan-related data, offering insights into each
loan’s unique identifier, associated account details, approval dates, amounts, durations,
and monthly payments. 968"
"CREATE
TABLE",0.7483443708609272,"E.3.3
Challenging questions
969"
"CREATE
TABLE",0.7490802060338484,"We discuss three major groups of challenging questions with examples.
970"
"CREATE
TABLE",0.7498160412067697,"The first group contains ambiguous questions. Here are a few examples.
971"
"CREATE
TABLE",0.7505518763796909,"Question:
List out the no. of districts that have female average salary is more
than 6000 but less than 10000?
Reason:
Ambiguous question. ‘no. of districts’ refers to the district number based
on the ground truth. However, Lucy counts the number of districts. 972"
"CREATE
TABLE",0.7512877115526122,"Question:
W that the client whose card was opened in 1996/10/21 made?
Reason:
Ambiguous question. Lucy filters on ‘card issued date’, while ground
truth filters on ‘account opened date’. However, the user is indeed asking about ‘card
open date’ in this question. This issue was also independently observed in [Wretblad
et al., 2024a]. 973"
"CREATE
TABLE",0.7520235467255335,"The second group contains complex filtering, ordering, and/or formulas to compute. Here
974"
"CREATE
TABLE",0.7527593818984547,"are a few examples.
975"
"CREATE
TABLE",0.753495217071376,"Question:
List out the account numbers of clients who are youngest and have
highest average salary?
Reason:
Phase 3 issue. There are two filtering conditions that have to be applied
in order. First, we find the youngest clients, then select the one with the highest
average salary among them. Lucy treats these conditions as a conjunction, resulting
in an empty output. 976"
"CREATE
TABLE",0.7542310522442973,"Question:
List out the account numbers of female clients who are oldest and has
lowest average salary, calculate the gap between this lowest average salary with the
highest average salary?
Reason:
Phase 3 issue. Two filtering conditions are required: first, in descending
order, and then in ascending order. However, Lucy fails to perform them in this
sequence. 977"
"CREATE
TABLE",0.7549668874172185,"Question:
For the client who applied the biggest loan, what was his/her first
amount of transaction after opened the account.
Reason:
Phase 3 issue. Two filtering conditions are required: first, in ascending
order, and then in descending order. However, Lucy fails to perform them in this
sequence. 978"
"CREATE
TABLE",0.7557027225901398,"The third group contains questions where the MatchTables phase either adds an extra table,
979"
"CREATE
TABLE",0.7564385577630611,"or occasionally misses a table or attributes. Here is an example.
980"
"CREATE
TABLE",0.7571743929359823,"Question:
How many accounts have an owner disposition and request for a state-
ment to be generated upon a transaction?
Reason:
Phase 1 issue. Lucy identifies ""Tran"" (transaction) as a relevant table,
but it is not needed to answer the query. 981"
"CREATE
TABLE",0.7579102281089036,"E.4
Cloud resources
982"
"CREATE
TABLE",0.7586460632818248,"Note on the cost of running.
One note here is that GPT and c2q models are costly to
983"
"CREATE
TABLE",0.7593818984547461,"run. For example, in the Cloud Resources experiment, the costs are as follows: c2q costs
984"
"CREATE
TABLE",0.7601177336276674,"$15, gpt4 $2, and gpt4ex $5, while Lucy costs $0.5.
985"
"CREATE
TABLE",0.7608535688005886,"NeurIPS Paper Checklist
986"
CLAIMS,0.7615894039735099,"1. Claims
987"
CLAIMS,0.7623252391464312,"Question: Do the main claims made in the abstract and introduction accurately
988"
CLAIMS,0.7630610743193524,"reflect the paper’s contributions and scope?
989"
CLAIMS,0.7637969094922737,"Answer: [Yes]
990"
CLAIMS,0.7645327446651949,"Justification: Yes, we do.
991"
CLAIMS,0.7652685798381162,"Guidelines:
992"
CLAIMS,0.7660044150110376,"• The answer NA means that the abstract and introduction do not include the
993"
CLAIMS,0.7667402501839587,"claims made in the paper.
994"
CLAIMS,0.7674760853568801,"• The abstract and/or introduction should clearly state the claims made, including
995"
CLAIMS,0.7682119205298014,"the contributions made in the paper and important assumptions and limitations.
996"
CLAIMS,0.7689477557027226,"A No or NA answer to this question will not be perceived well by the reviewers.
997"
CLAIMS,0.7696835908756439,"• The claims made should match theoretical and experimental results, and reflect
998"
CLAIMS,0.7704194260485652,"how much the results can be expected to generalize to other settings.
999"
CLAIMS,0.7711552612214864,"• It is fine to include aspirational goals as motivation as long as it is clear that
1000"
CLAIMS,0.7718910963944077,"these goals are not attained by the paper.
1001"
LIMITATIONS,0.7726269315673289,"2. Limitations
1002"
LIMITATIONS,0.7733627667402502,"Question: Does the paper discuss the limitations of the work performed by the
1003"
LIMITATIONS,0.7740986019131715,"authors?
1004"
LIMITATIONS,0.7748344370860927,"Answer: [Yes]
1005"
LIMITATIONS,0.775570272259014,"Justification: Yes, see Section 4
1006"
LIMITATIONS,0.7763061074319353,"Guidelines:
1007"
LIMITATIONS,0.7770419426048565,"• The answer NA means that the paper has no limitation while the answer No
1008"
LIMITATIONS,0.7777777777777778,"means that the paper has limitations, but those are not discussed in the paper.
1009"
LIMITATIONS,0.7785136129506991,"• The authors are encouraged to create a separate ""Limitations"" section in their
1010"
LIMITATIONS,0.7792494481236203,"paper.
1011"
LIMITATIONS,0.7799852832965416,"• The paper should point out any strong assumptions and how robust the results
1012"
LIMITATIONS,0.7807211184694628,"are to violations of these assumptions (e.g., independence assumptions, noiseless
1013"
LIMITATIONS,0.7814569536423841,"settings, model well-specification, asymptotic approximations only holding
1014"
LIMITATIONS,0.7821927888153054,"locally). The authors should reflect on how these assumptions might be violated
1015"
LIMITATIONS,0.7829286239882266,"in practice and what the implications would be.
1016"
LIMITATIONS,0.7836644591611479,"• The authors should reflect on the scope of the claims made, e.g., if the approach
1017"
LIMITATIONS,0.7844002943340692,"was only tested on a few datasets or with a few runs. In general, empirical
1018"
LIMITATIONS,0.7851361295069904,"results often depend on implicit assumptions, which should be articulated.
1019"
LIMITATIONS,0.7858719646799117,"• The authors should reflect on the factors that influence the performance of the
1020"
LIMITATIONS,0.7866077998528329,"approach. For example, a facial recognition algorithm may perform poorly when
1021"
LIMITATIONS,0.7873436350257542,"image resolution is low or images are taken in low lighting. Or a speech-to-text
1022"
LIMITATIONS,0.7880794701986755,"system might not be used reliably to provide closed captions for online lectures
1023"
LIMITATIONS,0.7888153053715967,"because it fails to handle technical jargon.
1024"
LIMITATIONS,0.789551140544518,"• The authors should discuss the computational efficiency of the proposed algo-
1025"
LIMITATIONS,0.7902869757174393,"rithms and how they scale with dataset size.
1026"
LIMITATIONS,0.7910228108903605,"• If applicable, the authors should discuss possible limitations of their approach
1027"
LIMITATIONS,0.7917586460632818,"to address problems of privacy and fairness.
1028"
LIMITATIONS,0.7924944812362031,"• While the authors might fear that complete honesty about limitations might
1029"
LIMITATIONS,0.7932303164091243,"be used by reviewers as grounds for rejection, a worse outcome might be that
1030"
LIMITATIONS,0.7939661515820456,"reviewers discover limitations that aren’t acknowledged in the paper. The
1031"
LIMITATIONS,0.7947019867549668,"authors should use their best judgment and recognize that individual actions in
1032"
LIMITATIONS,0.7954378219278881,"favor of transparency play an important role in developing norms that preserve
1033"
LIMITATIONS,0.7961736571008095,"the integrity of the community. Reviewers will be specifically instructed to not
1034"
LIMITATIONS,0.7969094922737306,"penalize honesty concerning limitations.
1035"
THEORY ASSUMPTIONS AND PROOFS,0.797645327446652,"3. Theory Assumptions and Proofs
1036"
THEORY ASSUMPTIONS AND PROOFS,0.7983811626195733,"Question: For each theoretical result, does the paper provide the full set of assump-
1037"
THEORY ASSUMPTIONS AND PROOFS,0.7991169977924945,"tions and a complete (and correct) proof?
1038"
THEORY ASSUMPTIONS AND PROOFS,0.7998528329654158,"Answer: [Yes]
1039"
THEORY ASSUMPTIONS AND PROOFS,0.8005886681383371,"Justification: We model a part of the problem as an optimization problem and
1040"
THEORY ASSUMPTIONS AND PROOFS,0.8013245033112583,"provide formal encoding. See Section 3.3.
1041"
THEORY ASSUMPTIONS AND PROOFS,0.8020603384841796,"Guidelines:
1042"
THEORY ASSUMPTIONS AND PROOFS,0.8027961736571008,"• The answer NA means that the paper does not include theoretical results.
1043"
THEORY ASSUMPTIONS AND PROOFS,0.8035320088300221,"• All the theorems, formulas, and proofs in the paper should be numbered and
1044"
THEORY ASSUMPTIONS AND PROOFS,0.8042678440029434,"cross-referenced.
1045"
THEORY ASSUMPTIONS AND PROOFS,0.8050036791758646,"• All assumptions should be clearly stated or referenced in the statement of any
1046"
THEORY ASSUMPTIONS AND PROOFS,0.8057395143487859,"theorems.
1047"
THEORY ASSUMPTIONS AND PROOFS,0.8064753495217072,"• The proofs can either appear in the main paper or the supplemental material,
1048"
THEORY ASSUMPTIONS AND PROOFS,0.8072111846946284,"but if they appear in the supplemental material, the authors are encouraged to
1049"
THEORY ASSUMPTIONS AND PROOFS,0.8079470198675497,"provide a short proof sketch to provide intuition.
1050"
THEORY ASSUMPTIONS AND PROOFS,0.8086828550404709,"• Inversely, any informal proof provided in the core of the paper should be
1051"
THEORY ASSUMPTIONS AND PROOFS,0.8094186902133922,"complemented by formal proofs provided in appendix or supplemental material.
1052"
THEORY ASSUMPTIONS AND PROOFS,0.8101545253863135,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
1053"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8108903605592347,"4. Experimental Result Reproducibility
1054"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.811626195732156,"Question: Does the paper fully disclose all the information needed to reproduce
1055"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8123620309050773,"the main experimental results of the paper to the extent that it affects the main
1056"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8130978660779985,"claims and/or conclusions of the paper (regardless of whether the code and data are
1057"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8138337012509198,"provided or not)?
1058"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8145695364238411,"Answer: [Yes]
1059"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8153053715967623,"Justification: Yes, we describe all algorithms and an optimization model.
1060"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8160412067696836,"Guidelines:
1061"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8167770419426048,"• The answer NA means that the paper does not include experiments.
1062"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8175128771155261,"• If the paper includes experiments, a No answer to this question will not be
1063"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8182487122884474,"perceived well by the reviewers: Making the paper reproducible is important,
1064"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8189845474613686,"regardless of whether the code and data are provided or not.
1065"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8197203826342899,"• If the contribution is a dataset and/or model, the authors should describe the
1066"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8204562178072112,"steps taken to make their results reproducible or verifiable.
1067"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8211920529801324,"• Depending on the contribution, reproducibility can be accomplished in various
1068"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8219278881530537,"ways. For example, if the contribution is a novel architecture, describing the
1069"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8226637233259749,"architecture fully might suffice, or if the contribution is a specific model and
1070"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8233995584988962,"empirical evaluation, it may be necessary to either make it possible for others
1071"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8241353936718175,"to replicate the model with the same dataset, or provide access to the model. In
1072"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8248712288447387,"general. releasing code and data is often one good way to accomplish this, but
1073"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.82560706401766,"reproducibility can also be provided via detailed instructions for how to replicate
1074"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8263428991905813,"the results, access to a hosted model (e.g., in the case of a large language model),
1075"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8270787343635025,"releasing of a model checkpoint, or other means that are appropriate to the
1076"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8278145695364238,"research performed.
1077"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8285504047093452,"• While NeurIPS does not require releasing code, the conference does require all
1078"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8292862398822664,"submissions to provide some reasonable avenue for reproducibility, which may
1079"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8300220750551877,"depend on the nature of the contribution. For example
1080"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8307579102281089,"(a) If the contribution is primarily a new algorithm, the paper should make it
1081"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8314937454010302,"clear how to reproduce that algorithm.
1082"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8322295805739515,"(b) If the contribution is primarily a new model architecture, the paper should
1083"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8329654157468727,"describe the architecture clearly and fully.
1084"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.833701250919794,"(c) If the contribution is a new model (e.g., a large language model), then there
1085"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8344370860927153,"should either be a way to access this model for reproducing the results or a
1086"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8351729212656365,"way to reproduce the model (e.g., with an open-source dataset or instructions
1087"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8359087564385578,"for how to construct the dataset).
1088"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8366445916114791,"(d) We recognize that reproducibility may be tricky in some cases, in which
1089"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8373804267844003,"case authors are welcome to describe the particular way they provide for
1090"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8381162619573216,"reproducibility. In the case of closed-source models, it may be that access to
1091"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8388520971302428,"the model is limited in some way (e.g., to registered users), but it should be
1092"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8395879323031641,"possible for other researchers to have some path to reproducing or verifying
1093"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8403237674760854,"the results.
1094"
OPEN ACCESS TO DATA AND CODE,0.8410596026490066,"5. Open access to data and code
1095"
OPEN ACCESS TO DATA AND CODE,0.8417954378219279,"Question: Does the paper provide open access to the data and code, with sufficient
1096"
OPEN ACCESS TO DATA AND CODE,0.8425312729948492,"instructions to faithfully reproduce the main experimental results, as described in
1097"
OPEN ACCESS TO DATA AND CODE,0.8432671081677704,"supplemental material?
1098"
OPEN ACCESS TO DATA AND CODE,0.8440029433406917,"Answer: [Yes]
1099"
OPEN ACCESS TO DATA AND CODE,0.8447387785136129,"Justification: We provide the data in supplementary materials and describe prompts.
1100"
OPEN ACCESS TO DATA AND CODE,0.8454746136865342,"We will make code publicly available.
1101"
OPEN ACCESS TO DATA AND CODE,0.8462104488594555,"Guidelines:
1102"
OPEN ACCESS TO DATA AND CODE,0.8469462840323767,"• The answer NA means that paper does not include experiments requiring code.
1103"
OPEN ACCESS TO DATA AND CODE,0.847682119205298,"• Please see the NeurIPS code and data submission guidelines (https://nips.
1104"
OPEN ACCESS TO DATA AND CODE,0.8484179543782193,"cc/public/guides/CodeSubmissionPolicy) for more details.
1105"
OPEN ACCESS TO DATA AND CODE,0.8491537895511405,"• While we encourage the release of code and data, we understand that this might
1106"
OPEN ACCESS TO DATA AND CODE,0.8498896247240618,"not be possible, so “No” is an acceptable answer. Papers cannot be rejected
1107"
OPEN ACCESS TO DATA AND CODE,0.8506254598969831,"simply for not including code, unless this is central to the contribution (e.g., for
1108"
OPEN ACCESS TO DATA AND CODE,0.8513612950699043,"a new open-source benchmark).
1109"
OPEN ACCESS TO DATA AND CODE,0.8520971302428256,"• The instructions should contain the exact command and environment needed
1110"
OPEN ACCESS TO DATA AND CODE,0.8528329654157468,"to run to reproduce the results.
See the NeurIPS code and data submis-
1111"
OPEN ACCESS TO DATA AND CODE,0.8535688005886681,"sion guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy)
1112"
OPEN ACCESS TO DATA AND CODE,0.8543046357615894,"for more details.
1113"
OPEN ACCESS TO DATA AND CODE,0.8550404709345106,"• The authors should provide instructions on data access and preparation, in-
1114"
OPEN ACCESS TO DATA AND CODE,0.8557763061074319,"cluding how to access the raw data, preprocessed data, intermediate data, and
1115"
OPEN ACCESS TO DATA AND CODE,0.8565121412803532,"generated data, etc.
1116"
OPEN ACCESS TO DATA AND CODE,0.8572479764532744,"• The authors should provide scripts to reproduce all experimental results for
1117"
OPEN ACCESS TO DATA AND CODE,0.8579838116261957,"the new proposed method and baselines. If only a subset of experiments are
1118"
OPEN ACCESS TO DATA AND CODE,0.8587196467991169,"reproducible, they should state which ones are omitted from the script and why.
1119"
OPEN ACCESS TO DATA AND CODE,0.8594554819720382,"• At submission time, to preserve anonymity, the authors should release
1120"
OPEN ACCESS TO DATA AND CODE,0.8601913171449596,"anonymized versions (if applicable).
1121"
OPEN ACCESS TO DATA AND CODE,0.8609271523178808,"• Providing as much information as possible in supplemental material (appended
1122"
OPEN ACCESS TO DATA AND CODE,0.8616629874908021,"to the paper) is recommended, but including URLs to data and code is permitted.
1123"
OPEN ACCESS TO DATA AND CODE,0.8623988226637234,"6. Experimental Setting/Details
1124"
OPEN ACCESS TO DATA AND CODE,0.8631346578366446,"Question: Does the paper specify all the training and test details (e.g., data splits,
1125"
OPEN ACCESS TO DATA AND CODE,0.8638704930095659,"hyperparameters, how they were chosen, type of optimizer, etc.)
necessary to
1126"
OPEN ACCESS TO DATA AND CODE,0.8646063281824872,"understand the results?
1127"
OPEN ACCESS TO DATA AND CODE,0.8653421633554084,"Answer: [Yes]
1128"
OPEN ACCESS TO DATA AND CODE,0.8660779985283297,"Justification: We specified parameters of prompts. We do not train new models.
1129"
OPEN ACCESS TO DATA AND CODE,0.8668138337012509,"Guidelines:
1130"
OPEN ACCESS TO DATA AND CODE,0.8675496688741722,"• The answer NA means that the paper does not include experiments.
1131"
OPEN ACCESS TO DATA AND CODE,0.8682855040470935,"• The experimental setting should be presented in the core of the paper to a level
1132"
OPEN ACCESS TO DATA AND CODE,0.8690213392200147,"of detail that is necessary to appreciate the results and make sense of them.
1133"
OPEN ACCESS TO DATA AND CODE,0.869757174392936,"• The full details can be provided either with the code, in appendix, or as
1134"
OPEN ACCESS TO DATA AND CODE,0.8704930095658573,"supplemental material.
1135"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8712288447387785,"7. Experiment Statistical Significance
1136"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8719646799116998,"Question: Does the paper report error bars suitably and correctly defined or other
1137"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8727005150846211,"appropriate information about the statistical significance of the experiments?
1138"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8734363502575423,"Answer: [NA]
1139"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8741721854304636,"Justification: We provide details for Lucy and gpt4. Existing methods either
1140"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8749080206033848,"provide their results as a single answer [Li et al., 2024b] or are too costly to run
1141"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8756438557763061,"multiple times.
1142"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8763796909492274,"Guidelines:
1143"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8771155261221486,"• The answer NA means that the paper does not include experiments.
1144"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8778513612950699,"• The authors should answer ""Yes"" if the results are accompanied by error bars,
1145"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8785871964679912,"confidence intervals, or statistical significance tests, at least for the experiments
1146"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8793230316409124,"that support the main claims of the paper.
1147"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8800588668138337,"• The factors of variability that the error bars are capturing should be clearly
1148"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8807947019867549,"stated (for example, train/test split, initialization, random drawing of some
1149"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8815305371596762,"parameter, or overall run with given experimental conditions).
1150"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8822663723325975,"• The method for calculating the error bars should be explained (closed form
1151"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8830022075055187,"formula, call to a library function, bootstrap, etc.)
1152"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.88373804267844,"• The assumptions made should be given (e.g., Normally distributed errors).
1153"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8844738778513613,"• It should be clear whether the error bar is the standard deviation or the standard
1154"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8852097130242825,"error of the mean.
1155"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8859455481972038,"• It is OK to report 1-sigma error bars, but one should state it. The authors
1156"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8866813833701251,"should preferably report a 2-sigma error bar than state that they have a 96%
1157"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8874172185430463,"CI, if the hypothesis of Normality of errors is not verified.
1158"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8881530537159676,"• For asymmetric distributions, the authors should be careful not to show in
1159"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8888888888888888,"tables or figures symmetric error bars that would yield results that are out of
1160"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8896247240618101,"range (e.g. negative error rates).
1161"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8903605592347315,"• If error bars are reported in tables or plots, The authors should explain in the
1162"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8910963944076526,"text how they were calculated and reference the corresponding figures or tables
1163"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.891832229580574,"in the text.
1164"
EXPERIMENTS COMPUTE RESOURCES,0.8925680647534953,"8. Experiments Compute Resources
1165"
EXPERIMENTS COMPUTE RESOURCES,0.8933038999264165,"Question: For each experiment, does the paper provide sufficient information on the
1166"
EXPERIMENTS COMPUTE RESOURCES,0.8940397350993378,"computer resources (type of compute workers, memory, time of execution) needed
1167"
EXPERIMENTS COMPUTE RESOURCES,0.8947755702722591,"to reproduce the experiments?
1168"
EXPERIMENTS COMPUTE RESOURCES,0.8955114054451803,"Answer: [Yes]
1169"
EXPERIMENTS COMPUTE RESOURCES,0.8962472406181016,"Justification: Yes, we describe the experimental setup.
1170"
EXPERIMENTS COMPUTE RESOURCES,0.8969830757910228,"Guidelines:
1171"
EXPERIMENTS COMPUTE RESOURCES,0.8977189109639441,"• The answer NA means that the paper does not include experiments.
1172"
EXPERIMENTS COMPUTE RESOURCES,0.8984547461368654,"• The paper should indicate the type of compute workers CPU or GPU, internal
1173"
EXPERIMENTS COMPUTE RESOURCES,0.8991905813097866,"cluster, or cloud provider, including relevant memory and storage.
1174"
EXPERIMENTS COMPUTE RESOURCES,0.8999264164827079,"• The paper should provide the amount of compute required for each of the
1175"
EXPERIMENTS COMPUTE RESOURCES,0.9006622516556292,"individual experimental runs as well as estimate the total compute.
1176"
EXPERIMENTS COMPUTE RESOURCES,0.9013980868285504,"• The paper should disclose whether the full research project required more
1177"
EXPERIMENTS COMPUTE RESOURCES,0.9021339220014717,"compute than the experiments reported in the paper (e.g., preliminary or failed
1178"
EXPERIMENTS COMPUTE RESOURCES,0.9028697571743929,"experiments that didn’t make it into the paper).
1179"
CODE OF ETHICS,0.9036055923473142,"9. Code Of Ethics
1180"
CODE OF ETHICS,0.9043414275202355,"Question: Does the research conducted in the paper conform, in every respect, with
1181"
CODE OF ETHICS,0.9050772626931567,"the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1182"
CODE OF ETHICS,0.905813097866078,"Answer: [Yes]
1183"
CODE OF ETHICS,0.9065489330389993,"Justification:
1184"
CODE OF ETHICS,0.9072847682119205,"Guidelines:
1185"
CODE OF ETHICS,0.9080206033848418,"• The answer NA means that the authors have not reviewed the NeurIPS Code
1186"
CODE OF ETHICS,0.9087564385577631,"of Ethics.
1187"
CODE OF ETHICS,0.9094922737306843,"• If the authors answer No, they should explain the special circumstances that
1188"
CODE OF ETHICS,0.9102281089036056,"require a deviation from the Code of Ethics.
1189"
CODE OF ETHICS,0.9109639440765268,"• The authors should make sure to preserve anonymity (e.g., if there is a special
1190"
CODE OF ETHICS,0.9116997792494481,"consideration due to laws or regulations in their jurisdiction).
1191"
BROADER IMPACTS,0.9124356144223694,"10. Broader Impacts
1192"
BROADER IMPACTS,0.9131714495952906,"Question: Does the paper discuss both potential positive societal impacts and
1193"
BROADER IMPACTS,0.9139072847682119,"negative societal impacts of the work performed?
1194"
BROADER IMPACTS,0.9146431199411332,"Answer: [Yes]
1195"
BROADER IMPACTS,0.9153789551140544,"Justification: We believe it has a positive impact as we enhance users with new
1196"
BROADER IMPACTS,0.9161147902869757,"capabilities.
1197"
BROADER IMPACTS,0.9168506254598969,"Guidelines:
1198"
BROADER IMPACTS,0.9175864606328182,"• The answer NA means that there is no societal impact of the work performed.
1199"
BROADER IMPACTS,0.9183222958057395,"• If the authors answer NA or No, they should explain why their work has no
1200"
BROADER IMPACTS,0.9190581309786607,"societal impact or why the paper does not address societal impact.
1201"
BROADER IMPACTS,0.919793966151582,"• Examples of negative societal impacts include potential malicious or unintended
1202"
BROADER IMPACTS,0.9205298013245033,"uses (e.g., disinformation, generating fake profiles, surveillance), fairness consid-
1203"
BROADER IMPACTS,0.9212656364974245,"erations (e.g., deployment of technologies that could make decisions that unfairly
1204"
BROADER IMPACTS,0.9220014716703459,"impact specific groups), privacy considerations, and security considerations.
1205"
BROADER IMPACTS,0.9227373068432672,"• The conference expects that many papers will be foundational research and
1206"
BROADER IMPACTS,0.9234731420161884,"not tied to particular applications, let alone deployments. However, if there
1207"
BROADER IMPACTS,0.9242089771891097,"is a direct path to any negative applications, the authors should point it out.
1208"
BROADER IMPACTS,0.9249448123620309,"For example, it is legitimate to point out that an improvement in the quality
1209"
BROADER IMPACTS,0.9256806475349522,"of generative models could be used to generate deepfakes for disinformation.
1210"
BROADER IMPACTS,0.9264164827078735,"On the other hand, it is not needed to point out that a generic algorithm for
1211"
BROADER IMPACTS,0.9271523178807947,"optimizing neural networks could enable people to train models that generate
1212"
BROADER IMPACTS,0.927888153053716,"Deepfakes faster.
1213"
BROADER IMPACTS,0.9286239882266373,"• The authors should consider possible harms that could arise when the technology
1214"
BROADER IMPACTS,0.9293598233995585,"is being used as intended and functioning correctly, harms that could arise when
1215"
BROADER IMPACTS,0.9300956585724798,"the technology is being used as intended but gives incorrect results, and harms
1216"
BROADER IMPACTS,0.9308314937454011,"following from (intentional or unintentional) misuse of the technology.
1217"
BROADER IMPACTS,0.9315673289183223,"• If there are negative societal impacts, the authors could also discuss possible
1218"
BROADER IMPACTS,0.9323031640912436,"mitigation strategies (e.g., gated release of models, providing defenses in addition
1219"
BROADER IMPACTS,0.9330389992641648,"to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a
1220"
BROADER IMPACTS,0.9337748344370861,"system learns from feedback over time, improving the efficiency and accessibility
1221"
BROADER IMPACTS,0.9345106696100074,"of ML).
1222"
SAFEGUARDS,0.9352465047829286,"11. Safeguards
1223"
SAFEGUARDS,0.9359823399558499,"Question: Does the paper describe safeguards that have been put in place for
1224"
SAFEGUARDS,0.9367181751287712,"responsible release of data or models that have a high risk for misuse (e.g., pretrained
1225"
SAFEGUARDS,0.9374540103016924,"language models, image generators, or scraped datasets)?
1226"
SAFEGUARDS,0.9381898454746137,"Answer: [NA] .
1227"
SAFEGUARDS,0.9389256806475349,"Justification:
1228"
SAFEGUARDS,0.9396615158204562,"Guidelines:
1229"
SAFEGUARDS,0.9403973509933775,"• The answer NA means that the paper poses no such risks.
1230"
SAFEGUARDS,0.9411331861662987,"• Released models that have a high risk for misuse or dual-use should be released
1231"
SAFEGUARDS,0.94186902133922,"with necessary safeguards to allow for controlled use of the model, for example
1232"
SAFEGUARDS,0.9426048565121413,"by requiring that users adhere to usage guidelines or restrictions to access the
1233"
SAFEGUARDS,0.9433406916850625,"model or implementing safety filters.
1234"
SAFEGUARDS,0.9440765268579838,"• Datasets that have been scraped from the Internet could pose safety risks. The
1235"
SAFEGUARDS,0.9448123620309051,"authors should describe how they avoided releasing unsafe images.
1236"
SAFEGUARDS,0.9455481972038263,"• We recognize that providing effective safeguards is challenging, and many papers
1237"
SAFEGUARDS,0.9462840323767476,"do not require this, but we encourage authors to take this into account and
1238"
SAFEGUARDS,0.9470198675496688,"make a best faith effort.
1239"
LICENSES FOR EXISTING ASSETS,0.9477557027225901,"12. Licenses for existing assets
1240"
LICENSES FOR EXISTING ASSETS,0.9484915378955114,"Question: Are the creators or original owners of assets (e.g., code, data, models),
1241"
LICENSES FOR EXISTING ASSETS,0.9492273730684326,"used in the paper, properly credited and are the license and terms of use explicitly
1242"
LICENSES FOR EXISTING ASSETS,0.9499632082413539,"mentioned and properly respected?
1243"
LICENSES FOR EXISTING ASSETS,0.9506990434142752,"Answer: [Yes]
1244"
LICENSES FOR EXISTING ASSETS,0.9514348785871964,"Justification:
1245"
LICENSES FOR EXISTING ASSETS,0.9521707137601177,"Guidelines:
1246"
LICENSES FOR EXISTING ASSETS,0.9529065489330389,"• The answer NA means that the paper does not use existing assets.
1247"
LICENSES FOR EXISTING ASSETS,0.9536423841059603,"• The authors should cite the original paper that produced the code package or
1248"
LICENSES FOR EXISTING ASSETS,0.9543782192788816,"dataset.
1249"
LICENSES FOR EXISTING ASSETS,0.9551140544518028,"• The authors should state which version of the asset is used and, if possible,
1250"
LICENSES FOR EXISTING ASSETS,0.9558498896247241,"include a URL.
1251"
LICENSES FOR EXISTING ASSETS,0.9565857247976454,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1252"
LICENSES FOR EXISTING ASSETS,0.9573215599705666,"• For scraped data from a particular source (e.g., website), the copyright and
1253"
LICENSES FOR EXISTING ASSETS,0.9580573951434879,"terms of service of that source should be provided.
1254"
LICENSES FOR EXISTING ASSETS,0.9587932303164092,"• If assets are released, the license, copyright information, and terms of use in
1255"
LICENSES FOR EXISTING ASSETS,0.9595290654893304,"the package should be provided. For popular datasets, paperswithcode.com/
1256"
LICENSES FOR EXISTING ASSETS,0.9602649006622517,"datasets has curated licenses for some datasets. Their licensing guide can help
1257"
LICENSES FOR EXISTING ASSETS,0.9610007358351729,"determine the license of a dataset.
1258"
LICENSES FOR EXISTING ASSETS,0.9617365710080942,"• For existing datasets that are re-packaged, both the original license and the
1259"
LICENSES FOR EXISTING ASSETS,0.9624724061810155,"license of the derived asset (if it has changed) should be provided.
1260"
LICENSES FOR EXISTING ASSETS,0.9632082413539367,"• If this information is not available online, the authors are encouraged to reach
1261"
LICENSES FOR EXISTING ASSETS,0.963944076526858,"out to the asset’s creators.
1262"
NEW ASSETS,0.9646799116997793,"13. New Assets
1263"
NEW ASSETS,0.9654157468727005,"Question: Are new assets introduced in the paper well documented and is the
1264"
NEW ASSETS,0.9661515820456218,"documentation provided alongside the assets?
1265"
NEW ASSETS,0.9668874172185431,"Answer: [Yes]
1266"
NEW ASSETS,0.9676232523914643,"Justification: We provide all benchmarks in supplementary materials.
1267"
NEW ASSETS,0.9683590875643856,"Guidelines:
1268"
NEW ASSETS,0.9690949227373068,"• The answer NA means that the paper does not release new assets.
1269"
NEW ASSETS,0.9698307579102281,"• Researchers should communicate the details of the dataset/code/model as part
1270"
NEW ASSETS,0.9705665930831494,"of their submissions via structured templates.
This includes details about
1271"
NEW ASSETS,0.9713024282560706,"training, license, limitations, etc.
1272"
NEW ASSETS,0.9720382634289919,"• The paper should discuss whether and how consent was obtained from people
1273"
NEW ASSETS,0.9727740986019132,"whose asset is used.
1274"
NEW ASSETS,0.9735099337748344,"• At submission time, remember to anonymize your assets (if applicable). You
1275"
NEW ASSETS,0.9742457689477557,"can either create an anonymized URL or include an anonymized zip file.
1276"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9749816041206769,"14. Crowdsourcing and Research with Human Subjects
1277"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757174392935982,"Question: For crowdsourcing experiments and research with human subjects, does
1278"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9764532744665195,"the paper include the full text of instructions given to participants and screenshots,
1279"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771891096394407,"if applicable, as well as details about compensation (if any)?
1280"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977924944812362,"Answer: [NA]
1281"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9786607799852833,"Justification:
1282"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793966151582045,"Guidelines:
1283"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801324503311258,"• The answer NA means that the paper does not involve crowdsourcing nor
1284"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808682855040471,"research with human subjects.
1285"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816041206769683,"• Including this information in the supplemental material is fine, but if the main
1286"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823399558498896,"contribution of the paper involves human subjects, then as much detail as
1287"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830757910228108,"possible should be included in the main paper.
1288"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838116261957321,"• According to the NeurIPS Code of Ethics, workers involved in data collection,
1289"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845474613686535,"curation, or other labor should be paid at least the minimum wage in the
1290"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9852832965415746,"country of the data collector.
1291"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986019131714496,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research
1292"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9867549668874173,"with Human Subjects
1293"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874908020603385,"Question: Does the paper describe potential risks incurred by study participants,
1294"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882266372332598,"whether such risks were disclosed to the subjects, and whether Institutional Review
1295"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889624724061811,"Board (IRB) approvals (or an equivalent approval/review based on the requirements
1296"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896983075791023,"of your country or institution) were obtained?
1297"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904341427520236,"Answer: [NA]
1298"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911699779249448,"Justification:
1299"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9919058130978661,"Guidelines:
1300"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926416482707874,"• The answer NA means that the paper does not involve crowdsourcing nor
1301"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933774834437086,"research with human subjects.
1302"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941133186166299,"• Depending on the country in which research is conducted, IRB approval (or
1303"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948491537895512,"equivalent) may be required for any human subjects research. If you obtained
1304"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955849889624724,"IRB approval, you should clearly state this in the paper.
1305"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9963208241353937,"• We recognize that the procedures for this may vary significantly between insti-
1306"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970566593083149,"tutions and locations, and we expect authors to adhere to the NeurIPS Code of
1307"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977924944812362,"Ethics and the guidelines for their institution.
1308"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9985283296541575,"• For initial submissions, do not include any information that would break
1309"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9992641648270787,"anonymity (if applicable), such as the institution conducting the review.
1310"
