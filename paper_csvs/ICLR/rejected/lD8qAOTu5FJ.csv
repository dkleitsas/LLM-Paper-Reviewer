Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002777777777777778,"Continual learning agents should incrementally learn a sequence of tasks while
satisfying two main desiderata: accumulating on previous knowledge without for-
getting and transferring previous relevant knowledge to help in future learning.
Existing research largely focuses on alleviating the catastrophic forgetting prob-
lem. There, an agent is altered to prevent forgetting based solely on previous tasks.
This hinders the balance between preventing forgetting and maximizing the for-
ward transfer. In response to this, we investigate the stability-plasticity dilemma
to determine which model components are eligible to be reused, added, ﬁxed, or
updated to achieve this balance. We address the class incremental learning sce-
nario where the agent is prone to ambiguities between old and new classes. With
our proposed Knowledge-Aware contiNual learner (KAN) 1, we demonstrate that
considering the semantic similarity between old and new classes helps in achiev-
ing this balance. We show that being aware of existing knowledge helps in: (1)
increasing the forward transfer from similar knowledge, (2) reducing the required
capacity by leveraging existing knowledge, (3) protecting dissimilar knowledge,
and (4) increasing robustness to the class order in the sequence. We evaluated se-
quences of similar tasks, dissimilar tasks, and a mix of both constructed from the
two commonly used benchmarks for class-incremental learning; CIFAR-10 and
CIFAR-100."
INTRODUCTION,0.005555555555555556,"1
INTRODUCTION"
INTRODUCTION,0.008333333333333333,"Continual learning (CL) aims to build intelligent agents based on deep neural networks that can
learn a sequence of tasks, use previous knowledge in future learning, and accumulate on it without
forgetting. The main challenge in this paradigm is the stability-plasticity dilemma (Mermillod et al.,
2013). Optimizing all model weights on a new task, highest plasticity, causes catastrophic forgetting
of previous tasks (McCloskey & Cohen, 1989). While ﬁxing all weights, highest stability, hinders
learning new tasks. Finding the right balance between stability and plasticity is challenging. This
sharpens the community’s focus on the forgetting problem. The excessive focus on one aspect im-
pedes building agents that balance between mitigating forgetting and exploiting relevant knowledge
in future learning while considering the capacity constraints (D´ıaz-Rodr´ıguez et al., 2018)."
INTRODUCTION,0.011111111111111112,"Task-speciﬁc components methods (Section 2) address the stability-plasticity dilemma by allocating
different connections for each task. The newly added connections for a new task are ﬂexible to learn
while the connections of previously learned tasks are ﬁxed. Despite that these methods are quite
successful in mitigating forgetting, some limitations remain to be tackled to achieve the balance
between the above-mentioned CL desiderata. First, new components (connections/neurons) are
allocated in each layer for every new task. However, the new components might capture knowledge
that already exists in the learned components of previous similar tasks. In this case, adding new
components would be redundant and resource inefﬁcient. Second, the topology of the new task
is allocated using the unimportant components of previous tasks. The design choice of the new
topology is made to protect previous knowledge but does not take into consideration the usefulness
of this topology for the current task. This limits the forward transfer of useful knowledge for the
new task and its speed of learning. Further discussions of these limitations are in Appendix A."
THE CODE WILL BE MADE PUBLICLY AVAILABLE AFTER THE PUBLICATION OF THIS PAPER AND NOW CAN BE FOUND IN THE,0.013888888888888888,"1The code will be made publicly available after the publication of this paper and now can be found in the
supplementary material in the system."
THE CODE WILL BE MADE PUBLICLY AVAILABLE AFTER THE PUBLICATION OF THIS PAPER AND NOW CAN BE FOUND IN THE,0.016666666666666666,Under review as a conference paper at ICLR 2022
THE CODE WILL BE MADE PUBLICLY AVAILABLE AFTER THE PUBLICATION OF THIS PAPER AND NOW CAN BE FOUND IN THE,0.019444444444444445,"In response to these limitations, we study the core of the stability-plasticity dilemma in the CL
paradigm. In particular, we address the following question: Which components are eligible to be
reused, added, updated, or ﬁxed when a CL agent faces a new task to achieve the balance be-
tween the CL desiderata? With our proposed Knowledge-Aware contiuNal learner (KAN), we ﬁnd
that considering the semantic similarity between old and new classes is crucial in addressing this
question. With the awareness of existing knowledge, the CL agent could identify similar previous
knowledge that could be reusable in learning a new task (forward transfer) and add the necessary
components only to capture the speciﬁc knowledge that cannot be explained with the existing knowl-
edge (resource efﬁciency). Moreover, the agent could protect the dissimilar previous knowledge and
limit its transfer to the new task (mitigating forgetting)."
THE CODE WILL BE MADE PUBLICLY AVAILABLE AFTER THE PUBLICATION OF THIS PAPER AND NOW CAN BE FOUND IN THE,0.022222222222222223,"To the best of our knowledge, the recent work, CAT (Ke et al., 2020), is the only one that attempted
to consider the task similarities to balance between forward transfer and mitigating forgetting. CAT
was proposed for the task-Incremental Learning (task-IL) scenario, where each task is a separate
classiﬁcation problem. In this work, we aim to tackle the more challenging class-Incremental Learn-
ing (class-IL) scenario where a uniﬁed classiﬁer is used for all seen classes. The inaccessibility of
the task identity at inference in the latter scenario arises the following challenges: (1) ambiguities
between old and new tasks which affect the balance between maintaining forgetting and forward
transfer. (2) Inability to select the corresponding components to a task at inference; all the existing
components in the model are involved in making predictions. To fully analyze the stability-plasticity
dilemma in class-IL, given the aforementioned challenges, we focus on the rehearsal-free strategy
(Section 2) in which there is no reliance on past data to address forgetting and a ﬁxed-capacity
model is used. We also report the performance of the model in Task-IL to further demonstrate the
challenges in class-IL. Our contributions are:"
THE CODE WILL BE MADE PUBLICLY AVAILABLE AFTER THE PUBLICATION OF THIS PAPER AND NOW CAN BE FOUND IN THE,0.025,"• We demonstrate that considering the semantic relation between old and new classes is cru-
cial in addressing the stability-plasticity dilemma and the balance between CL desiderata."
THE CODE WILL BE MADE PUBLICLY AVAILABLE AFTER THE PUBLICATION OF THIS PAPER AND NOW CAN BE FOUND IN THE,0.027777777777777776,"• We show that considering both previous and current tasks in allocating the initial topology
of a new task allows for: selective transfer for the relevant (similar) knowledge, protecting
the irrelevant (dissimilar) knowledge, and identifying the reusable previous components to
avoid allocating unnecessary resources."
THE CODE WILL BE MADE PUBLICLY AVAILABLE AFTER THE PUBLICATION OF THIS PAPER AND NOW CAN BE FOUND IN THE,0.030555555555555555,"• We demonstrate that the standard softmax-classiﬁcation layer is a source of constituting the
ambiguities between old and new tasks in class-IL. We study different setups for the output
layer which signiﬁcantly improve the performance of rehearsal-free methods. Our analyses
reveal the limitations of the softmax in class-IL and shed light on the necessity of paying
more attention to these limitations to narrow the gap between class-IL and task-IL."
THE CODE WILL BE MADE PUBLICLY AVAILABLE AFTER THE PUBLICATION OF THIS PAPER AND NOW CAN BE FOUND IN THE,0.03333333333333333,"• We propose Knowledge-Aware coNtinual leaner (KAN); a resource-efﬁcient task-speciﬁc
components method to explain these ﬁndings."
RELATED WORK,0.03611111111111111,"2
RELATED WORK"
RELATED WORK,0.03888888888888889,We divide the CL methods into two main categories: Rehearsal-free and Rehearsal-based methods.
RELATED WORK,0.041666666666666664,"Rehearsal-free methods. In this category, previous data is inaccessible during future learning.
There are two strategies to address forgetting. (1) Task-speciﬁc components. Speciﬁc components
are assigned to each task. The components of previous tasks are stable during future learning. While
the components that have not been allocated to any task are ﬂexible for future learning. These meth-
ods either extend the model size when facing a new task (Rusu et al., 2016; Yoon et al., 2018) or uses
a ﬁxed-capacity and each task is trained either using sparse sub-network within the model (Mallya
& Lazebnik, 2018; Mallya et al., 2018; Yoon et al., 2019). These methods are limited to task-IL
as they require the task identity to specify the components of each task at inference. Recently, few
methods have been proposed for class-IL using ﬁxed-capacity model. SupSup (Wortsman et al.,
2020) learns a mask for each task over randomly initialized ﬁxed network. FSLL (Mazumder et al.,
2021) addressed the few-shot lifelong learning setup. It relies on the availability of large data for
the ﬁrst task to train a dense model. Then, few unimportant parameters are used to learn each task.
SpaceNet (Sokar et al., 2021c) learns sparse sub-network for each task from scratch using dynamic
sparse training (Mocanu et al., 2018; Hoeﬂer et al., 2021) where the weights and the sparse topol-
ogy are optimized simultaneously. (2) Regularization-based. A ﬁxed-capacity model is used and"
RELATED WORK,0.044444444444444446,Under review as a conference paper at ICLR 2022
RELATED WORK,0.04722222222222222,"all parameters are involved in optimizing each task. Forgetting is addressed either by constraining
changes in the important weights of previous tasks (Zenke et al., 2017; Kirkpatrick et al., 2017;
Aljundi et al., 2018) or via distillation loss (Li & Hoiem, 2017; Dhar et al., 2019). Previous stud-
ies (Kemker et al., 2018; Hsu et al., 2018; Farquhar & Gal, 2019; van de Ven & Tolias, 2018) showed
the big performance gap of most of these methods between class-IL and task-IL."
RELATED WORK,0.05,"Rehearsal-based methods. In this category, forgetting is addressed by replaying: (1) a subset of
old samples (Rebufﬁet al., 2017; Lopez-Paz & Ranzato, 2017; Riemer et al., 2018; Chaudhry et al.,
2018), (2) pseudo-samples from a generative model (Mocanu et al., 2016; Shin et al., 2017; Sokar
et al., 2021a), or (3) generative high-level features (Liu et al., 2020; van de Ven et al., 2020). They
use a classiﬁcation model and buffer to store old samples or a generative model to generate them."
RELATED WORK,0.05277777777777778,"Other related studies. Recent work by (Ramasesh et al., 2020) evaluates forgetting in hidden
representations in Task-IL and shows how semantic similarities inﬂuence it. They showed that
higher layers are more prone to forgetting and intermediate similarity results in maximal forgetting.
SAM (Sokar et al., 2021b) studies the importance of selective transfer in dense networks by meta-
training (Finn et al., 2017) a self-attention mechanism (Hu et al., 2018). CAT (Ke et al., 2020)
addresses the relation between task similarities and forward/backward transfer in task-IL where
they rely on the task identity to ﬁnd the previous similar knowledge in dense networks. In (Chen
et al., 2020), the lottery ticket hypothesis (Frankle & Carbin, 2018) is studied for the CL paradigm."
PRELIMINARIES,0.05555555555555555,"3
PRELIMINARIES"
PROBLEM FORMULATION,0.058333333333333334,"3.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.06111111111111111,"We consider the problem of learning a sequence of T tasks. Each task brings a new set of C classes.
Each task t has its own dataset Dt = {Dt
c}|C
c=1, where Dt
c is the data of class c in task t. Once we
have trained task t, its data is discarded. The goal is to sequentially learn the model f t with a uniﬁed
classiﬁer that can map any input to its corresponding target for all seen classes up to time step t."
REHEARSAL-FREE STRATEGY,0.06388888888888888,"3.2
REHEARSAL-FREE STRATEGY"
REHEARSAL-FREE STRATEGY,0.06666666666666667,"In this work, we focus to analyze how the model components should be altered for each task in
task-speciﬁc components methods. In addition, we compare the behavior of these methods against
regularization-based methods which use all model components for each task."
SPACENET,0.06944444444444445,"3.2.1
SPACENET"
SPACENET,0.07222222222222222,"We analyze the recent task-speciﬁc components method, SpaceNet (Sokar et al., 2021c). The mo-
tivations for choosing SpaceNet are: (1) unlike most task-speciﬁc component methods, it does not
rely on task identity; making it applicable for class-IL. (2) Instead of relying on a pre-trained model,
it trains sparse subnetworks from scratch. These are favorable for real-world applications in which
we cannot assume the availability of task identity at inference and large datasets for pretraining. To
the best of our knowledge, it is only existing work that fulﬁlls these objectives."
SPACENET,0.075,"The main idea of SpaceNet is to learn sparse representation for each task to reduce the interference
between tasks. SpaceNet consists of three main steps. (1) Sparse connections allocation: sparse
sub-network W t
l is randomly allocated for each task t in each layer l between subset of the unim-
portant neurons hsel
l−1 and hsel
l
; where hsel
l
are randomly selected from the free neurons hfree
l
that
have not been reserved for any previous task. (2) Dynamic sparse training: the sparse weights and
topology are optimized through “drop-and-grow” cycles to produce sparse representation. (3) Neu-
ron reservation: after learning, a fraction of the most important neurons for the current task hspec
l
are ﬁxed and removed from the free list of neurons hfree
l
. The full details are provided in Appendix
I. SpaceNet considers only previous tasks during the allocation of new connections for the new tasks
and randomly allocates a new sub-network between the free neurons for each task. In this work,
we propose novel criteria which consider both past and current tasks in allocation and allow reusing
some of the past components instead of allocating new ones."
SPACENET,0.07777777777777778,Under review as a conference paper at ICLR 2022
SPACENET,0.08055555555555556,"Task 2
(dog,truck) dog truck"
SPACENET,0.08333333333333333,Model after training Task 1 car cat
SPACENET,0.08611111111111111,(1) Detect similar knowledge car cat
SPACENET,0.08888888888888889,(2) Connections allocation car cat
SPACENET,0.09166666666666666,"lreuse
Reserved Free Fixed"
SPACENET,0.09444444444444444,"Task 2
connections"
SPACENET,0.09722222222222222,Candidate
SPACENET,0.1,"for dog
Candidate"
SPACENET,0.10277777777777777,for truck
SPACENET,0.10555555555555556,"Figure 1: An overview of our proposed method KAN. The most left panel shows the network after
training on Task 1. When the model faces Task 2, KAN reuses the existing similar knowledge up
to layer lreuse and adds new components in the subsequent layers. For l ≥lreuse, (1) the candidate
similar neurons for each new class are detected. (2) New sparse connections are allocated between
the candidate neurons and free neurons in layer l −1 and the free neurons in layer l ."
SPACENET,0.10833333333333334,"4
PROPOSED KNOWLEDGE-AWARE CONTINUAL LEARNER (KAN)"
SPACENET,0.1111111111111111,"In this work, we aim to study how the model components should be altered, when a CL agent
faces a new task, to balance between the CL desiderata (i.e. forward transfer, reducing forgetting,
and resource efﬁciency). To address this goal, we propose Knowledge-Aware contiNual learner
(KAN), a new task-speciﬁc components model that exploits the semantic similarity between old and
new classes in altering a CL model. The novel contributions of KAN are: (1) the allocation of a
new topology considers both previous and current tasks; (2) reusing some of the existing learned
components from previous similar tasks instead of allocating new components in each layer."
SPACENET,0.11388888888888889,"Figure 1 shows an overview of KAN. When the agent faces a new task t, new connections are al-
located for this task. The new connections are trained with stochastic gradient descent (SGD). All
previous connections are ﬁxed. Instead of allocating new connections in each layer, KAN starts allo-
cation from layer lreuse ∈[1, L −1] which is a hyper-parameter that controls the trade-off between
allocating new components and reusing old components based on existing knowledge. L is the num-
ber of layers. The topology from layer l1 up to but excluding layer lreuse remains unchanged. In
KAN, we operate on the class level instead of tasks for connections allocation. Starting from lreuse
onwards, KAN allocates new connections for each new class. This requires two steps."
SPACENET,0.11666666666666667,"The ﬁrst step is the detection of similar knowledge. Typically, some of the previously learned
classes are similar to the new classes and some are dissimilar. Classes that have semantic similarity
are most likely to share similar representation (Appendix A; Figures 5b and 6). For each new class
c, KAN ﬁnds the candidate neurons hc cand
l
in each layer l, ∀l ≥lreuse, that could contain relevant
representation learned by a previous similar class. To achieve this, we feed the data of class c, Dt
c,
on the trained model at time step t −1, f t−1, and calculate the average activation ac
l in each layer l.
We estimate the relevance by the activation value. The higher the activation is, the more relevant is
the neuron (i.e. the neurons that ﬁre are the most important to the current class). We select a fraction
selc cand
l
of the most relevant neurons to be the candidate neurons hc cand
l
for class c. Please note
that by exploiting the representation learned in the candidate neurons hc cand
lreuse in layer lreuse, we are
reusing previous learned components connected to these neurons in preceding layers (l < llreuse).
Therefore, these components are stable but reusable."
SPACENET,0.11944444444444445,"The second step is to exploit the relevant candidate neurons in each layer l, ∀l ≥lreuse, in allo-
cating the new connections for each new class c. Adding new connections in layer l between the
candidate neurons hc cand
l−1
and hc cand
l
could maximize the forward transfer from previous knowl-
edge. However, this would lead to forgetting previously similar knowledge as the representation
learned by the important neurons for previous classes would be updated. To solve this dilemma, we
allow reusing the candidate neurons by allocating new connections out of these neurons. However,
we do not allow new connections to be added into the important neurons for previous tasks hspec
l
(fully ﬁlled circles in Figure 1). In addition, we block the gradient ﬂow through all important neu-"
SPACENET,0.12222222222222222,Under review as a conference paper at ICLR 2022
SPACENET,0.125,Algorithm 1 KAN Connections Allocation
SPACENET,0.12777777777777777,"1: Require: selt
l, selc cand
l−1
, lreuse, sparsity level ϵl
2: for each class c in task t do
▷Get candidate neurons in each layer for each new class
3:
ac
l ←forward pass Dt
c on f t−1"
SPACENET,0.13055555555555556,"4:
eac
l ←sortDescending (ac
l )
5:
hc cand
l
←select the ﬁrst selc cand
l
from eac
l
6: end for each
7: (hsel
l−1, hsel
l
) ←randomly select selt
l−l and selt
l neurons from hfree
l−1 and hfree
l
8: for each class c in task t do
9:
for l ←lreuse to L −1 do
10:
halloc
l−1 ←{hsel
l−1 ∪hc cand
l−1
}
11:
halloc
l
←hsel
l
12:
randomly allocate parameters W c
l with sparsity ϵl between halloc
l−1 and halloc
l
13:
Wl ←Wl ∪W c
l
14:
end for each
15: end for each"
SPACENET,0.13333333333333333,"rons for previous tasks hspec
l
, even if it is a candidate neuron for the current class, to protect previous
knowledge. Thus, hspec
l
is used to mask the gradient gl through the neurons in layer l as follows:
gl = gl ⊗(1 −hspec
l
), where hspec
l
is a one-hot encoding vector in which the entities contain value
of one represent an important neuron in layer l. This allows us not to forget the previously learned
knowledge while performing selective transfer for the useful knowledge. Let halloc
l−1 and halloc
l
be the
list of neurons for allocating new sparse connections W c
l for class c in layer l with a sparsity level
ϵl
C ; where ϵl is a hyper-parameter that controls the number of sparse connections for each task and
C is the total number of classes. The neurons used for allocation are selected as follows:"
SPACENET,0.1361111111111111,"halloc
l−1 = {hcand
l−1 ∪hsel
l−1}
and
halloc
l
= hsel
l
,
where
hsel
i
∈hfree
i
∀i = {l, l −1},
(1)"
SPACENET,0.1388888888888889,"where hsel
i
is a subset, selt
l, of the free neurons in layer i that are not reserved for any previous task.
Since the important neurons for the dissimilar classes are not involved in the topology allocation, we
are protecting their knowledge. Thus, the selective transfer in KAN addresses the forward transfer
while maintaining previous similar and dissimilar knowledge. Full details are in Algorithm 1."
SPACENET,0.14166666666666666,"After allocating the topology for task t, we follow steps 2 and 3 of SpaceNet, dynamic sparse
training and neuron reservation, to train the task and ﬁx its important neurons hspec
l
afterwards
(Section 3.2.1)."
EXPERIMENTS AND RESULTS,0.14444444444444443,"5
EXPERIMENTS AND RESULTS"
EXPERIMENTS AND RESULTS,0.14722222222222223,"We now use KAN to investigate how the model should deal with sequences of similar, dissimilar
tasks, or a mix of both to achieve the balance between the competing CL desiderata. We compare
against other rehearsal-free methods. We analyze the accuracy, forward and backward transfer,
model capacity, and class-order sensitivity. Each experiment is repeated 5 times with different seeds."
DATASETS AND BASELINES,0.15,"5.1
DATASETS AND BASELINES"
DATASETS AND BASELINES,0.1527777777777778,"Datasets.
We construct sequences with varying similarities from the standard benchmarks for
class-IL, CIFAR-10 and CIFAR-100 (Zenke et al., 2017), by altering the original class or-
der.
From CIFAR-10, we construct two-tasks and ﬁve-tasks sequences.
For the two-tasks se-
quences, we construct: (1) Similar sequence (sim seq 2Tasks): Task1{car,cat}, Task2{dog,truck},
where the two tasks share semantic similarity.
(2) Dissimilar sequence (dissim seq 2Tasks):
Task1{car,truck}, Task2{cat,dog}, where the semantic similarity is limited.
For the ﬁve-tasks
sequence (sim seq 5Tasks:), we arranged the 10 classes of CIFAR-10 in the following or-
der:{car,cat,horse,truck,dog,deer,airplane,bird,ship,truck}. Each task consists of two consecutive
classes. With this order, we study the case where there is past knowledge that has common se-
mantic similarity with the new task. From CIFAR-100, we construct a longer sequence of 8 tasks
(sim seq 8Tasks). Each task contain two classes coming from two different categories and each
two consecutive tasks shares semantic similarity. The classes has the following order: {apples,girl,
orange,boy,mouse,bicycle,rabbit,motocycle,bee,lion,butterﬂy,tiger,bottle,couch,chair,cans}."
DATASETS AND BASELINES,0.15555555555555556,Under review as a conference paper at ICLR 2022
DATASETS AND BASELINES,0.15833333333333333,Table 1: Forward transfer. Test accuracy [%] of Task 2 in two cases: similar and dissimilar tasks.
DATASETS AND BASELINES,0.16111111111111112,"Strategy
Method
sim seq 2Tasks
dissim seq 2Tasks
# Parameters allocated
Use whole capacity
SI creg = 0.1
98.19 ± 0.240
76.05 ± 3.95
1×(884576)
Use whole capacity
SI creg = 0.01
98.30 ± 0.127
81.91 ± 1.60
1×(884576)
Use one sub-network
Scratch
97.73 ± 0.008
80.21 ± 0.02
(0.065 ×)
Add new sub-network per task
SpaceNet
98.17 ± 0.002
81.27 ± 0.02
(0.131 ×)
Reuse relevant knowledge
KAN (ours) lreuse = L −1
96.29 ± 0.006
66.17 ± 0.02
(0.065 ×)
Reuse random knowledge
Random reuse lreuse = L −1
95.43 ± 0.002
64.60 ± 0.01
(0.065 ×)"
DATASETS AND BASELINES,0.1638888888888889,"Architecture. We followed the architecture used by (Zenke et al., 2017; Sokar et al., 2021c) for a
direct comparison. It also allows us to study the challenging case where the available capacity is
limited. It consists of 4 convolutional layers (32-32-64-64 feature maps) and 2 feed-forward layers
(512-N neurons), where N is the number of classes. More details are in Appendix B."
DATASETS AND BASELINES,0.16666666666666666,"Baselines. We compare with the following baselines: (1) Scratch: learning a single task from scratch
using a sparse neural network with the same sparsity level used for one task in the task-speciﬁc
components baselines, (2) SpaceNet (Sokar et al., 2021a), (3) SI (Zenke et al., 2017), (4) Random
reuse: a modiﬁed version of KAN in which the neurons used for allocating new connections are
subset of the free neurons (i.e. no candidate neurons), and (5) Irrelevant reuse: a modiﬁed version of
KAN in which the candidate neurons are the ones with the lowest activation. The last two baselines
allow us to study the role of the initial sparse topology in the performance. In our experiments, we
report the performance of our modiﬁed improved version of SI where a multi-headed output layer is
used instead of single-headed (Appendix H.2). A comparison with other baselines (PackNet (Mallya
& Lazebnik, 2018), EWC (Serra et al., 2018), MAS (Aljundi et al., 2018), and LWF (Li & Hoiem,
2017)) and analysis of forgetting can be found in Appendix G."
SELECTIVE KNOWLEDGE TRANSFER FROM PREVIOUSLY LEARNED KNOWLEDGE,0.16944444444444445,"5.2
SELECTIVE KNOWLEDGE TRANSFER FROM PREVIOUSLY LEARNED KNOWLEDGE"
ATTENTION TO RELEVANT KNOWLEDGE,0.17222222222222222,"5.2.1
ATTENTION TO RELEVANT KNOWLEDGE"
ATTENTION TO RELEVANT KNOWLEDGE,0.175,"In this paragraph, we address the question: Is all previous knowledge always useful for the current
task? To answer this question, we begin by investigating the extreme case where new connections
for a new task are added in the output layer only (i.e. lreuse = L −1). In this case, the minimum
new capacity is allocated, and the maximum reusability and highest stability of existing knowledge
occur. We performed this analysis on two-tasks sequences (sim seq 2Tasks, dissim seq 2Tasks). To
estimate the forward transfer, we focus on the performance of Task 2 assuming the availability of
the task identity at inference. Table 1 shows the accuracy of Task 2 using different methods."
ATTENTION TO RELEVANT KNOWLEDGE,0.17777777777777778,"In the similar case, SI and SpaceNet achieve a better performance than learning from scratch which
indicates that the learned knowledge by Task 1 is useful for Task 2. Interestingly, despite that Task
2 is trained by learning the output layer only in KAN, it achieves a close performance to SI and
SpaceNet. This performance is achieved using only 6.5 % and 50% of the capacity used by SI and
SpaceNet respectively. KAN outperforms the Random reuse baseline which reveals the importance
of selective transfer. In the dissimilar case, using SI with creg of 0.1 limits the performance of
Task 2 while reducing it to 0.01 gives the connections the ﬂexibility to learn at the expense of
forgetting Task 1 as we will discuss next. This shows that solving the stability-plasticity dilemma
by regularization strategies is challenging when tasks are dissimilar. This is because the changes in
the model weights after learning a dissimilar task are larger than learning a similar one (Appendix
A). In KAN, allocating new connections in the last layer only is not sufﬁcient to learn Task 2 since
the existing knowledge is irrelevant. This experiment reveals that adding new components in earlier
layers is essential when existing knowledge is irrelevant. However, if there is a semantic similarity
between classes, with a selective transfer, we could learn new classes with minimum components."
ATTENTION TO RELEVANT KNOWLEDGE,0.18055555555555555,"To further analyze the importance of the selective transfer, we performed another experiment on
sim seq 2Tasks in which we fool KAN by making the hc cand
L−1
of class dog in Task 2 equals to the
candidate neurons of the dissimilar class in Task 1 (car). We named this baseline as Inattentive
learner. Figure 2a shows the test accuracy of Task 2 along training. As illustrated, attention to irrel-
evant knowledge leads to lower initial performance, slower learning speed, and lower ﬁnal accuracy."
ATTENTION TO RELEVANT KNOWLEDGE,0.18333333333333332,Under review as a conference paper at ICLR 2022
ATTENTION TO RELEVANT KNOWLEDGE,0.18611111111111112,"0
5
10
15
20
25
30
35
Training epoch # 0.940 0.945 0.950 0.955 0.960"
ATTENTION TO RELEVANT KNOWLEDGE,0.18888888888888888,Accuracy on Task 2 [%]
ATTENTION TO RELEVANT KNOWLEDGE,0.19166666666666668,"KAN (ours)
Inattentive learner"
ATTENTION TO RELEVANT KNOWLEDGE,0.19444444444444445,(a) Learning speed 0 20 40 60 80 100
ATTENTION TO RELEVANT KNOWLEDGE,0.19722222222222222,Accuracy on class dog [%]
ATTENTION TO RELEVANT KNOWLEDGE,0.2,"sim_seq KAN (ours)
sim_seq Inattentive learner
dissim_seq"
ATTENTION TO RELEVANT KNOWLEDGE,0.20277777777777778,(b) Class order sensitivity
ATTENTION TO RELEVANT KNOWLEDGE,0.20555555555555555,"Task 1 Task 2 Average
Task 1 Task 2 Average
Class-IL
Task-IL 0 20 40 60 80 100"
ATTENTION TO RELEVANT KNOWLEDGE,0.20833333333333334,Accuracy [%]
ATTENTION TO RELEVANT KNOWLEDGE,0.2111111111111111,"KAN (ours)
Inattentive learner"
ATTENTION TO RELEVANT KNOWLEDGE,0.21388888888888888,(c) Backward transfer
ATTENTION TO RELEVANT KNOWLEDGE,0.21666666666666667,"Figure 2: (a) Using irrelevant knowledge decreases the learning speed. (b) Unawareness of existing
knowledge decreases robustness to the class order. (c) Selective transfer reduces forgetting."
ATTENTION TO RELEVANT KNOWLEDGE,0.21944444444444444,"Previous experiments reveal the sensitivity of the learner to the class order when the relation between
old and new classes is not considered or the irrelevant knowledge is used. Figure 2b shows the test
accuracy of class dog which appears in Task 2 in three different settings: (1) Similar tasks with KAN,
(2) Similar tasks with Inattentive learner (relevant knowledge exists but we reuse the dissimilar one),
and (3) Dissimilar tasks with KAN (existing knowledge has limited relevance). Despite that the same
class appears at the same time step in the three scenarios, its performance varies."
ADDING NEW COMPONENTS FOR A NEW TASK,0.2222222222222222,"5.2.2
ADDING NEW COMPONENTS FOR A NEW TASK"
ADDING NEW COMPONENTS FOR A NEW TASK,0.225,"We showed that new tasks could be learned using previous similar knowledge in Task-IL. Does this
hold in class-IL? Next, we will discuss the ambiguities that arise in class-IL between old and new
classes when they are similar. We performed this experiment on the sim seq 2Tasks benchmark."
ADDING NEW COMPONENTS FOR A NEW TASK,0.22777777777777777,"Class ambiguities. In class-IL, a uniﬁed classiﬁer is used for all seen classes. Reusing all relevant
knowledge from previous similar classes up to the highest representation level (closest layer to the
output; lreuse = L −1) in learning a new class maximizes the forward transfer with minimum
additional capacity. Nevertheless, it misleads the classiﬁer and leads to ambiguities between old
and new classes. The ﬁrst row in Table 2, illustrates this. We report the accuracy of each task after
learning the two. We ﬁnd that in task-IL, we can easily fully utilize previous knowledge up to the
last layer in learning a new task; achieving high performance on old and new tasks. However, in
class-IL, the model tends to bias toward one of the tasks due to the ambiguity between classes. To
address this problem, we propose two modiﬁcations for KAN: (1) usage of the free neurons only in
allocating the output connections (i.e. no candidate neurons are used; halloc
L−1 = hsel
L−1). (2) Enforcing
the re-usage of previous components to be till earlier layers than the highest one (lreuse < L−1) and
adding new components in higher layers to capture the speciﬁc representations of the new classes.
Next, we will study the effect of these proposed constraints. Ablation study is in Appendix D."
ADDING NEW COMPONENTS FOR A NEW TASK,0.23055555555555557,"Adding new components in earlier layer. To reduce the ambiguities, we need to satisfy the condi-
tion lreuse < L −1. Here, we study the effect of lreuse in the performance. As illustrated in Table
2, adding new components in the last three layers, lreuse = L −3, achieves the highest balance
between the two tasks for the two scenarios. Adding new components for Task 2 in earlier layers
(lreuse < L−3) does not lead to a signiﬁcant performance gain for Task 2 in Task-IL. Nevertheless,
it consumes additional resources and hinders the balance between forgetting and forward transfer in
class-IL. The learner becomes biased towards the second task and starts forgetting the previous one."
ADDING NEW COMPONENTS FOR A NEW TASK,0.23333333333333334,"Figure 3 shows the performance on sim seq 2Tasks using different methods. We compare the best
two settings for KAN (lreuse = {L −2, L −3}) against other baselines. SpaceNet achieves higher
performance on Task 2 at the expense of forgetting Task 1. KAN achieves a balance between the
two tasks and a bit higher average accuracy than SpaceNet in class-IL with less computational cost
(Appendix E). Random re-usage of previous knowledge or paying attention to the least relevant one
(irrelevant reuse) hinders learning the second task. The regularization-based method, SI, struggles
in balancing between the two tasks in the class-IL. Using all the network parameters in learning each
task and relying on the regularization coefﬁcient creg to address the stability and plasticity trade-off
is challenging. The learner either maintains the performance of Task 1 using a high value of creg or
forgets it and is biased towards the second task by decreasing the value of creg."
ADDING NEW COMPONENTS FOR A NEW TASK,0.2361111111111111,Under review as a conference paper at ICLR 2022
ADDING NEW COMPONENTS FOR A NEW TASK,0.2388888888888889,"Table 2: The accuracy [%] of each task in sim seq 2Tasks after learning the two tasks in sequence
along with their average accuracy using different values of lreuse in class-IL and task-IL."
ADDING NEW COMPONENTS FOR A NEW TASK,0.24166666666666667,"Class-IL
Task-IL
lreuse
Task 1
Task 2
Average
Task 1
Task 2
Average
# Parameters allocated
L −1
69.66
34.77
52.21
95.34
96.29
95.81
1×(58145)
L −2
64.34
61.11
62.72
95.28
97.16
96.21
(1.85×)
L −3
64.90
63.97
64.43
94.41
97.90
96.15
(1.93×)
L −4
63.36
66.85
65.10
94.43
97.68
96.05
(1.97×)
L −5
58.99
69.82
64.41
93.37
98.19
95.78
(1.99×)
L −6
59.69
70.81
65.25
95.38
98.29
96.84
(2×)"
ADDING NEW COMPONENTS FOR A NEW TASK,0.24444444444444444,"Task 1
Task 2
Average
20 30 40 50 60 70 80"
ADDING NEW COMPONENTS FOR A NEW TASK,0.24722222222222223,Accuracy [%]
ADDING NEW COMPONENTS FOR A NEW TASK,0.25,(a) Class-IL
ADDING NEW COMPONENTS FOR A NEW TASK,0.25277777777777777,"Task 1
Task 2
Average
40 50 60 70 80 90 100"
ADDING NEW COMPONENTS FOR A NEW TASK,0.25555555555555554,Accuracy [%]
ADDING NEW COMPONENTS FOR A NEW TASK,0.25833333333333336,(b) Task-IL
ADDING NEW COMPONENTS FOR A NEW TASK,0.2611111111111111,"KAN (ours) lreuse = L
2"
ADDING NEW COMPONENTS FOR A NEW TASK,0.2638888888888889,"KAN (ours) lreuse = L
3"
ADDING NEW COMPONENTS FOR A NEW TASK,0.26666666666666666,"SpaceNet
Irrelevant reuse
Random reuse
SI creg = 0.1"
ADDING NEW COMPONENTS FOR A NEW TASK,0.26944444444444443,SI creg = 0.01
ADDING NEW COMPONENTS FOR A NEW TASK,0.2722222222222222,Figure 3: Test accuracy of each task in sim seq 2Tasks and their average in class-IL and task-IL.
SELECTIVE BACKWARD TRANSFER FROM NEW TASKS,0.275,"5.3
SELECTIVE BACKWARD TRANSFER FROM NEW TASKS"
SELECTIVE BACKWARD TRANSFER FROM NEW TASKS,0.2777777777777778,"Constraining the capacity of the learner using a ﬁxed-capacity model is one of the desiderata of
CL. However, sooner or later this capacity would reach its limit and we have to use the existing
connections to learn new tasks. In this experiment, we demonstrate that performing selective back-
ward transfer based on the relevant knowledge reduces forgetting even if the previous weights are
updated without regularization. We performed this experiment on sim seq 2Tasks. We study the
extreme case where new connections are added in the last layer only (i.e. lreuse = L −1). We
update the connections in all preceding layers which connected to the candidate neurons hc cand
l
for
l: 2 ≤l ≤lreuse. We compare KAN with the Inattentive learner described in Section 5.2.1. Figure
2c shows the accuracy of each task in the sequence after learning the two and their average. With the
selective transfer in KAN, we managed to maintain higher performance for Task 1 in class-IL and
Task-IL. More interestingly, KAN outperforms the Inattentive learner by 10.4 % in class-IL. This
reveals the importance of selective transfer for both forward transfer and selective backward update."
PERFORMANCE ON LONG SEQUENCE OF TASKS,0.28055555555555556,"5.4
PERFORMANCE ON LONG SEQUENCE OF TASKS"
PERFORMANCE ON LONG SEQUENCE OF TASKS,0.2833333333333333,"Next, we study the performance of KAN on long sequences. For CIFAR-10 sim seq 5Tasks, we use
lreuse = L −3. For CIFAR-100 sim seq 8Tasks, since every two consecutive tasks are dissimilar
to previous ones, we allocate new sparse connections in all layers to capture the new knowledge
for every odd time step (t = 1, 3, ..). In other time steps, we use lreuse = L −3. For SI, we
use creg of 0.01. As illustrated in Figure 4, KAN achieves higher performance than SpaceNet on
the two benchmarks. The topology of the new connections matters; paying attention to relevant
knowledge is better than using random neurons or irrelevant ones. SI is more prone to forgetting in
long sequences of tasks. Further discussion on scalability and resource efﬁciency is in Appendix C."
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.2861111111111111,"5.5
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING"
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.28888888888888886,"In this paper, we have observed the performance gap between class-IL and task-IL which was also
reported by previous works (Maltoni & Lomonaco, 2019; Hsu et al., 2018). In our experiments, the
learned representations by the base network in both scenarios are the same, this suggests investi-
gating the output layer in class-IL. Current CL methods use the standard softmax. A single-head
output layer is usually used and extended when new classes arrive. We named this as Extendable
layer. The softmax has some limitations for this setup. It is based on the closed-world assumption (a"
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.2916666666666667,Under review as a conference paper at ICLR 2022
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.29444444444444445,"1
2
3
4
5
# learned tasks 30 40 50 60 70 80 90 100"
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.2972222222222222,Accuracy [%]
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.3,"KAN (ours)
SpaceNet
Random reuse
Irrelevant reuse
SI"
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.30277777777777776,(a) CIFAR-10 sim seq 5Tasks
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.3055555555555556,"1
2
3
4
5
6
7
8
# learned tasks 20 30 40 50 60 70 80 90"
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.30833333333333335,Accuracy [%]
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.3111111111111111,"KAN (ours)
SpaceNet
Random reuse
Irrelevant reuse
SI"
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.3138888888888889,(b) CIFAR-100 sim seq 8Tasks
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.31666666666666665,"Task1
Task2
Task3
Task4
Task5
Average
0 10 20 30 40 50 60 70 80"
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.3194444444444444,Accuracy [%]
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.32222222222222224,"Pre-defined singlehead
Multi-headed
Cosine Normalization
Extendable singlehead"
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.325,(c) Different output setups
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.3277777777777778,"Figure 4: The performance of different methods in class-IL on long sequences of tasks (CIFAR-10
(a) and CIFAR-100 (b)). (c) The performance of Split CIFAR-10 using different output layers."
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.33055555555555555,"ﬁxed number of classes). It divides the feature space between known classes (Geng et al., 2020a;b);
normalizing the output probabilities to be summed to one, leaving no space for future classes. In
addition, the model is biased towards new classes due to the unavailability of old data (Wu et al.,
2019). Few setups were proposed in the literature to alleviate these limitations: (1) Cosine nor-
malization (Hou et al., 2019) which enforces balanced magnitudes of the output weights across all
classes, (2) Multi-headed layer (Mittal et al., 2021) where each task has its own output layer, (3)
Pre-deﬁned layer (Sokar et al., 2021c) where a large number of output neurons is deﬁned at t = 0.
At each time step, the output weights of the current task are trainable and all other output weights are
set to zero. The ﬁrst two were evaluated for rehearsal-based methods. More details in Appendix H."
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.3333333333333333,"In this work, we evaluate these setups for the rehearsal-free strategy. We analyze the task-speciﬁc
components method SpaceNet (Sokar et al., 2021c) on Split CIFAR-10 benchmark with the typical
class order. As illustrated in Figure 4c, the commonly used extendable layer is the most prone to
forgetting previous tasks. There is no signiﬁcant gain by using cosine normalization with the ex-
tendable layer. Both the multi-headed and predeﬁned single-headed, which address the closed-world
assumption, succeed in mitigating forgetting while the latter achieves the best average performance.
Analysis on the other rehearsal-free strategy, regularization-based, is provided in Appendix H.2."
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.33611111111111114,"5.6
CLASS SIMILARITY DETECTION: LEARNING CLASS RELATIONSHIP"
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.3388888888888889,"In the previous analyses, we assume that we know whether the model previously learned a similar
class to the new one. Automatic detection of class relationships is required. We evaluated the ap-
proach by (You et al., 2020) which averages the predictions of the model f t−1 over all samples
of each new class; p(yt−1
c
|yt
c) ≈
1
|Dtc|
P"
SOFTMAX CLASSIFIER FOR CLASS-INCREMENTAL LEARNING,0.3416666666666667,"(x,ytc) f t−1(x). We observe that it works well in short se-
quences but does not scale for long sequences. Another approach by (Ke et al., 2020) is to compare
the performance of task t when it learns from scratch against learning from the model f t−1. How-
ever, it is computationally expensive. In this work, we focus on analyzing the effect of considering
task similarities. We leave designing sophisticated methods for similarity detection for future work."
CONCLUSION,0.34444444444444444,"6
CONCLUSION"
CONCLUSION,0.3472222222222222,"In this paper, we study the stability-plasticity dilemma in continual learning where the model learns
incrementally a sequence of tasks. We focus on the rehearsal-free methods using a ﬁxed-capacity
model in class-IL. With our proposed Knowledge-Aware contiNual learner (KAN), we analyzed
how should the model be altered under different tasks similarity. Our analysis reveals that taking
into consideration the relation between old and new classes is crucial to balance the CL desiderata.
We show that it helps in: (1) identifying the existing similar knowledge which could be exploited
to increase the forward transfer and reduce the allocated capacity for a new task; (2) protecting
dissimilar knowledge via selective update. We demonstrate that achieving a balance between CL
desiderata is more challenging in class-IL than in task-IL. Moreover, we show that the conventional
softmax is a cause of this challenge. Our ﬁndings suggest that more efforts should be dedicated to
(1) detecting the class similarities, (2) designing attention mechanisms for selective transfer, and (3)
addressing the limitations of the softmax classiﬁer to close the gap between class-IL and task-IL."
CONCLUSION,0.35,Under review as a conference paper at ICLR 2022
REFERENCES,0.3527777777777778,REFERENCES
REFERENCES,0.35555555555555557,"Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pp. 139–154, 2018."
REFERENCES,0.35833333333333334,"Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient
lifelong learning with a-gem. In International Conference on Learning Representations, 2018."
REFERENCES,0.3611111111111111,"Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Long live the lottery:
The existence of winning tickets in lifelong learning. In International Conference on Learning
Representations, 2020."
REFERENCES,0.3638888888888889,"Selima Curci, Decebal Constantin Mocanu, and Mykola Pechenizkiyi. Truly sparse neural networks
at scale. arXiv preprint arXiv:2102.01732, 2021."
REFERENCES,0.36666666666666664,"Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. Learn-
ing without memorizing. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 5138–5146, 2019."
REFERENCES,0.36944444444444446,"Natalia D´ıaz-Rodr´ıguez, Vincenzo Lomonaco, David Filliat, and Davide Maltoni. Don’t forget,
there is more than forgetting: new metrics for continual learning. In Workshop on Continual
Learning, NeurIPS 2018 (Neural Information Processing Systems, 2018."
REFERENCES,0.37222222222222223,"Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:
Making all tickets winners. In International Conference on Machine Learning, pp. 2943–2952.
PMLR, 2020."
REFERENCES,0.375,"Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. In Privacy in
Machine Learning and Artiﬁcial Intelligence workshop, ICML, jun 2019."
REFERENCES,0.37777777777777777,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pp. 1126–1135. PMLR,
2017."
REFERENCES,0.38055555555555554,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.38333333333333336,"Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. Recent advances in open set recognition:
A survey. IEEE transactions on pattern analysis and machine intelligence, 2020a."
REFERENCES,0.3861111111111111,"Chuanxing Geng, Lue Tao, and Songcan Chen. Guided cnn for generalized zero-shot and open-set
recognition using visual and semantic prototypes. Pattern Recognition, 102:107263, 2020b."
REFERENCES,0.3888888888888889,"Torsten Hoeﬂer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.
Sparsity in
deep learning: Pruning and growth for efﬁcient inference and training in neural networks. arXiv
preprint arXiv:2102.00554, 2021."
REFERENCES,0.39166666666666666,"Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a uniﬁed classiﬁer
incrementally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 831–839, 2019."
REFERENCES,0.39444444444444443,"Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learn-
ing scenarios: A categorization and case for strong baselines. In NeurIPS Continual learning
Workshop, 2018."
REFERENCES,0.3972222222222222,"Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 7132–7141, 2018."
REFERENCES,0.4,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015."
REFERENCES,0.4027777777777778,Under review as a conference paper at ICLR 2022
REFERENCES,0.40555555555555556,"Khurram Javed and Martha White. Meta-learning representations for continual learning. In Pro-
ceedings of the 33rd International Conference on Neural Information Processing Systems, pp.
1820–1830, 2019."
REFERENCES,0.4083333333333333,"Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence of similar and
dissimilar tasks. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.4111111111111111,"Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L Hayes, and Christopher Kanan. Mea-
suring catastrophic forgetting in neural networks. In Thirty-second AAAI conference on artiﬁcial
intelligence, 2018."
REFERENCES,0.41388888888888886,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521–3526, 2017."
REFERENCES,0.4166666666666667,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009."
REFERENCES,0.41944444444444445,"Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998."
REFERENCES,0.4222222222222222,"Shuang Li, Yilun Du, Gido Martijn van de Ven, and Igor Mordatch.
Energy-based models for
continual learning. In Energy Based Models Workshop-ICLR 2021, 2021."
REFERENCES,0.425,"Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence, 40(12):2935–2947, 2017."
REFERENCES,0.42777777777777776,"Xialei Liu, Chenshen Wu, Mikel Menta, Luis Herranz, Bogdan Raducanu, Andrew D Bagdanov,
Shangling Jui, and Joost van de Weijer. Generative feature replay for class-incremental learn-
ing. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
(CVPRW), pp. 915–924. IEEE Computer Society, 2020."
REFERENCES,0.4305555555555556,"David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Proceedings of the 31st International Conference on Neural Information Processing Systems, pp.
6470–6479, 2017."
REFERENCES,0.43333333333333335,"Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative
pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 7765–7773, 2018."
REFERENCES,0.4361111111111111,"Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multi-
ple tasks by learning to mask weights. In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 67–82, 2018."
REFERENCES,0.4388888888888889,"Davide Maltoni and Vincenzo Lomonaco. Continuous learning in single-incremental-task scenarios.
Neural Networks, 116:56–73, 2019."
REFERENCES,0.44166666666666665,"Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost
van de Weijer. Class-incremental learning: survey and performance evaluation. arXiv preprint
arXiv:2010.15277, 2020."
REFERENCES,0.4444444444444444,"Pratik Mazumder, Pravendra Singh, and Piyush Rai. Few-shot lifelong learning. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 2337–2345, 2021."
REFERENCES,0.44722222222222224,"Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989."
REFERENCES,0.45,"Martial Mermillod, Aur´elia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Inves-
tigating the continuum from catastrophic forgetting to age-limited learning effects. Frontiers in
psychology, 4:504, 2013."
REFERENCES,0.4527777777777778,"Sudhanshu Mittal, Silvio Galesso, and Thomas Brox. Essentials for class incremental learning.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
3513–3522, 2021."
REFERENCES,0.45555555555555555,Under review as a conference paper at ICLR 2022
REFERENCES,0.4583333333333333,"Decebal Constantin Mocanu, Maria Torres Vega, Eric Eaton, Peter Stone, and Antonio Liotta. On-
line contrastive divergence with generative replay: Experience replay without storing data. arXiv
preprint arXiv:1610.05555, 2016."
REFERENCES,0.46111111111111114,"Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu,
and Antonio Liotta. Scalable training of artiﬁcial neural networks with adaptive sparse connec-
tivity inspired by network science. Nature communications, 9(1):1–12, 2018."
REFERENCES,0.4638888888888889,"Vinay Venkatesh Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting:
Hidden representations and task semantics. In International Conference on Learning Represen-
tations, 2020."
REFERENCES,0.4666666666666667,"Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:
Incremental classiﬁer and representation learning. In Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition, pp. 2001–2010, 2017."
REFERENCES,0.46944444444444444,"Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald
Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interfer-
ence. In International Conference on Learning Representations, 2018."
REFERENCES,0.4722222222222222,"Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016."
REFERENCES,0.475,"Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. In International Conference on Machine Learning, pp.
4548–4557. PMLR, 2018."
REFERENCES,0.4777777777777778,"Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017."
REFERENCES,0.48055555555555557,"Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Learning invariant represen-
tation for continual learning. In Meta-Learning for Computer Vision Workshop at the 35th AAAI
Conference on Artiﬁcial Intelligence (AAAI-21), 2021a."
REFERENCES,0.48333333333333334,"Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Self-attention meta-learner for
continual learning. In Proceedings of the 20th International Conference on Autonomous Agents
and MultiAgent Systems, pp. 1658–1660, 2021b."
REFERENCES,0.4861111111111111,"Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Spacenet: Make free space
for continual learning. Neurocomputing, 439:1–11, 2021c."
REFERENCES,0.4888888888888889,"Gido M van de Ven and Andreas S Tolias. Three scenarios for continual learning. In Continual
Learning Workshop NeurIPS, 2018."
REFERENCES,0.49166666666666664,"Gido M van de Ven, Hava T Siegelmann, and Andreas S Tolias. Brain-inspired replay for continual
learning with artiﬁcial neural networks. Nature communications, 11(1):1–14, 2020."
REFERENCES,0.49444444444444446,"Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari,
Jason Yosinski, and Ali Farhadi. Supermasks in superposition. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.49722222222222223,"Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu.
Large scale incremental learning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 374–382, 2019."
REFERENCES,0.5,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.5027777777777778,"Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically
expandable networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.5055555555555555,"Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Scalable and order-robust con-
tinual learning with additive parameter decomposition. In International Conference on Learning
Representations, 2019."
REFERENCES,0.5083333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.5111111111111111,"Kaichao You, Zhi Kou, Mingsheng Long, and Jianmin Wang. Co-tuning for transfer learning. Ad-
vances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.5138888888888888,"Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui,
and Joost van de Weijer. Semantic drift compensation for class-incremental learning. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6982–6991,
2020."
REFERENCES,0.5166666666666667,"Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong, Zheng
Zhan, Chaoyang He, Qing Jin, et al. Mest: Accurate and fast memory-economic sparse training
framework on the edge. In Thirty-Fifth Conference on Neural Information Processing Systems,
2021."
REFERENCES,0.5194444444444445,"Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
In International Conference on Machine Learning, pp. 3987–3995. PMLR, 2017."
REFERENCES,0.5222222222222223,Under review as a conference paper at ICLR 2022
REFERENCES,0.525,Task 1
REFERENCES,0.5277777777777778,Task 3
REFERENCES,0.5305555555555556,Task 2
REFERENCES,0.5333333333333333,Output layer
REFERENCES,0.5361111111111111,Shared
REFERENCES,0.5388888888888889,(a) New components/task
REFERENCES,0.5416666666666666,"dog
truck"
REFERENCES,0.5444444444444444,Similar Tasks
REFERENCES,0.5472222222222223,"car
cat
Task 1"
REFERENCES,0.55,"0.28
0.95"
REFERENCES,0.5527777777777778,"0.99
0.5"
REFERENCES,0.5555555555555556,Task 2 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.5583333333333333,(b) Representational similarity
REFERENCES,0.5611111111111111,Activation 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
REFERENCES,0.5638888888888889,(c) Sparse representation
REFERENCES,0.5666666666666667,"Figure 5: (a) New components are allocated per task in each layer. (b) Cosine similarity between
the last hidden representation of each class in Task 1 and Task 2 produced by the model trained on
Task 1. Task 2 has representational similarity with Task 1 that could be exploited in its learning. (c)
Activation of the last hidden layer of one class in Task 2 using the dense model trained on Task 1.
The relevant representation from previous knowledge is sparse."
REFERENCES,0.5694444444444444,"A
OBSERVATIONS IN THE CONTINUAL LEARNING PARADIGM"
REFERENCES,0.5722222222222222,"In this appendix, we will discuss some of the observations in the continual learning paradigm and
provide a further discussion on the limitations stated in Section 1."
REFERENCES,0.575,"New components of each task. Figure 5a, show an illustration of connections allocation in the
task-speciﬁc components method SpaceNet (Sokar et al., 2021c). New connections are added in
each layer for each new task. The new connections are randomly allocated between the free neurons
that could be shared between tasks. The full-ﬁlled colored neurons represent the neurons that are
important for a task and reserved (ﬁxed). The criteria for adding new connections consider only
protecting previous knowledge."
REFERENCES,0.5777777777777777,"Representational similarities between tasks. Figure 5b shows the representational similarities be-
tween classes in two similar tasks. This experiment is performed on the sim seq 2Tasks benchmark
using the same experimental setup for KAN described in Section 5.1 and Appendix B. The repre-
sentations are obtained from the last hidden layer (closest to the output). We average the activation,
a, of the model trained on Task 1 f 1 over all samples of each class in Task 1 and Task 2. The cosine
similarity between the two representations a1, a2 is calculated as follows: < ¯a1, ¯a2 >= ¯a1T ¯a2;
where ¯ai =
ai
||ai|| denotes the L2-normalized vector. As illustrated in the ﬁgure, when the classes
share some semantic similarity, as the case in this benchmark, there would be representational sim-
ilarity between the classes. In KAN, we exploit this in learning Task 2 by maximizing the forward
transfer from Task 1 instead of allocating new components for learning these representation for the
new task."
REFERENCES,0.5805555555555556,"Figure 6 shows the activation of a subset of the neurons in the last hidden layer in two cases: similar
tasks (sim seq 2Tasks) and dissimilar tasks (dissim seq 2Tasks). As illustrated in Figure 6a, there
are high representational similarities between each class in Task 1 and the corresponding class that
shares some semantic similarity in Task 2. On the other hand, if the tasks are dissimilar (Figure 6b),
the new task has a different representation than the previous one."
REFERENCES,0.5833333333333334,"Relevant representation. As illustrated in Figure 6, the relevant representation from previous
knowledge is sparse. This representation is obtained from a sparse neural network. It would be
interesting to study whether the same case holds in dense neural networks. We performed this ex-
periment on the sim seq 2Tasks benchmark using a dense model with the architecture described in
Section 5.1 and the experimental setting in Appendix B. Figure 5c shows the 2D visualization of the
average of the activation of the last hidden layer over samples of the class dog in Task 2 using the
model trained on Task 1 f 1. As illustrated in the ﬁgure, even if the network is dense, the relevant
representation for this new class is sparse. This is also discussed in (Sokar et al., 2021b; Javed &
White, 2019)."
REFERENCES,0.5861111111111111,"Backward transfer. The main cause of catastrophic forgetting arises from updating the previously
learned weights when the model is optimized for a new one. Next, we will analyze the weight change"
REFERENCES,0.5888888888888889,Under review as a conference paper at ICLR 2022 200 210 220 230 240 250 260 270 280 290
REFERENCES,0.5916666666666667,Neuron #
REFERENCES,0.5944444444444444,"car
cat
dog
truck
  Task 2                      Task 1 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5972222222222222,(a) Similar tasks 200 210 220 230 240 250 260 270 280 290
REFERENCES,0.6,Neuron #
REFERENCES,0.6027777777777777,"car
truck
cat
dog
  Task 2                      Task 1 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.6055555555555555,(b) Dissimilar tasks
REFERENCES,0.6083333333333333,"Figure 6: Visualization of the representation of a subset of the neurons in the last hidden layer of
each class in the sim seq 2Tasks (a) and dissim seq 2Tasks (b) benchmarks."
REFERENCES,0.6111111111111112,"in dense neural networks in two cases: similar and dissimilar sequences of tasks. We performed this
experiment on sim seq 2Tasks and dissim seq 2Tasks benchmarks. We use a dense model with the
architecture described in Section 5.1 and the experimental settings provided in Appendix B. We
estimated the average absolute change in the parameters in each layer between the models f 1 and
f 2. This change is calculated as follows:"
REFERENCES,0.6138888888888889,∆l = 1 Nl X
REFERENCES,0.6166666666666667,"i
(|W t=2
l i
−W t=1
l i |),
(2)"
REFERENCES,0.6194444444444445,"where W t
l i is the weight i in layer l after learning task t and Nl is the number of weights in layer l.
Figure 7 shows the average absolute change in each layer in the network. As illustrated, the change
is higher when the two tasks are dissimilar. This experiment demonstrates selective update based
on the relation between the classes is essential. Updating the dissimilar knowledge will cause the
weights to move far away from the values learned in the previous time step. This causes forgetting
of previous tasks."
REFERENCES,0.6222222222222222,Under review as a conference paper at ICLR 2022
REFERENCES,0.625,"Layer 1
Layer 2
Layer 3
Layer 4
Layer 5
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08"
REFERENCES,0.6277777777777778,Average Absolute Weight Change
REFERENCES,0.6305555555555555,"sim_seq_2Tasks
dissim_seq_2Tasks"
REFERENCES,0.6333333333333333,"Figure 7: The average absolute difference between the weights of the model at t = 1 (f 1) and the
weights at t = 2 (f 2). The change is higher when the tasks in the sequence are dissimilar."
REFERENCES,0.6361111111111111,"B
EXPERIMENTAL SETUP"
REFERENCES,0.6388888888888888,"In this appendix, we will describe the experimental settings used in the experiments provided in
Section 5."
REFERENCES,0.6416666666666667,"Datasets. CIFAR-10 and CIFAR-100 Krizhevsky et al. (2009) are well-known benchmarks for
classiﬁcation tasks. They contain tiny natural images of size (32 × 32). CIFAR-10 consists of 10
classes. It contains 6000 images per class (6000 training + 1000 test). While CIFAR-100 contains
100 classes, with 600 images per class (500 train + 100 test). We split these classes into tasks with
different levels of semantic similarity. The network is trained using stochastic gradient descent with
a batch size of 64 and a learning rate of 0.1. Each task is trained for 40 epochs. The hyperparameters
are selected using a random search."
REFERENCES,0.6444444444444445,"Architecture. The model consists of 4 convolutional layers (32-32-64-64 feature maps). The kernel
size is 3 × 3. Max pooling layer is added after every 2 convolutional layers. Two feed-forward lay-
ers follow the convolutional layers (512-N neurons), where N is the total number of classes from
all tasks. Similar as in (Sokar et al., 2021c), we replace the dropout layers with batch normaliza-
tion (Ioffe & Szegedy, 2015)."
REFERENCES,0.6472222222222223,"Hyperparameters. Next, we will report the hyperparameters used in each experiment in Section 5.
For the SpaceNet baseline, the sparsity levels ϵl (1-density) used in each layer is: {0.84, 0.86, 0.87,
0.87, 0.93, 0.90} for CIFAR-10 and {0.88, 0.91, 0.90, 0.97, 0.97, 0.90} for CIFAR-100. For both
datasets, the number of reserved neurons for each task in each layer is: {0, 2, 2, 5, 5, 30}. Following
the original paper of SpaceNet, the number of selected neurons for connections allocation selt
l equals
the number of free neurons in that layer. For KAN, on CIFAR-10, the same sparsity levels used for
SpaceNet are used for the ﬁrst task. For subsequent tasks, we use the above-mentioned sparsity
levels starting from lreuse. We will discuss the sparsity levels for KAN on CIFAR-100 in Appendix
C."
REFERENCES,0.65,"For Selective knowledge transfer analysis (Section 5.2), Table 3 shows the number of candidate
neurons selected for each class selc cand
l
and the selected number of free neurons for each task selt
l
in each layer. We report these values for every value evaluated for lreuse."
REFERENCES,0.6527777777777778,"For selective backward transfer analysis (Section 5.3), the number of candidate neurons selc cand
l
used in each layer from 2 to L −1 is as follows: {6, 6, 15, 15, 50}."
REFERENCES,0.6555555555555556,"For longer sequence of tasks (Section 5.4), the number of candidate neurons selc cand
l−1
used in layers
L −3 and L −2 are 15 and 10 respectively. While the number of selected neurons from the free list
selt
l in these two layers are 10 and 20 respectively."
REFERENCES,0.6583333333333333,"C
RESOURCE EFFICIENCY AND SCALABILITY"
REFERENCES,0.6611111111111111,"For CIFAR-100 benchmark, sim seq 8Tasks, we had to increase the sparsity level for each sub-
network for SpaceNet to be able to ﬁt the 8 tasks in the ﬁxed-capacity model. The sparsity levels"
REFERENCES,0.6638888888888889,Under review as a conference paper at ICLR 2022
REFERENCES,0.6666666666666666,"Table 3: The number of the candidate neurons for each class selc cand
l−1
and the number of selected
neurons from the free list in each layer selt
l. x is the number of free neurons in layer L-1 (|hfree
L−1|)."
REFERENCES,0.6694444444444444,"selccand
l
selt
l
lreuse/l#
1
2
3
4
5
6
1
2
3
4
5
6
L −1
0
0
0
0
0
50
0
0
0
0
0
0
L −2
0
0
0
0
15
0
0
0
0
0
10
x
L −3
0
0
0
15
10
0
0
0
0
10
20
x
L −4
0
0
8
10
10
0
0
0
6
20
20
x
L −5
0
8
6
10
10
0
0
6
12
20
20
x
L −6
0
6
6
10
10
0
3
10
12
20
20
x"
REFERENCES,0.6722222222222223,"Table 4: The accuracy [%] of each task in sim seq 2Tasks after learning the two tasks in sequence
along with their average accuracy. The reported performance is in class-IL using different strategies
for allocating the new connections in the output layer."
REFERENCES,0.675,"Strategy for connections allocation in the last layer
lreuse
Task 1
Task 2
Average
Use candidate neurons only
L-1
69.66
34.77
52.21
Use candidate neurons and free neurons
L-2
68.41
52.71
60.5
Use free neurons only
L-2
64.34
61.11
62.72"
REFERENCES,0.6777777777777778,"are mentioned in Appendix B. On the other hand, since KAN utilizes the existing capacity for
similar tasks, we have a free capacity that allows us to use a lower sparsity level for allocating the
connections in all layers for every odd time step (t = 1, 3, ..). We use the same sparsity level used
for CIFAR-10. For even time steps (t = 2, 4, ..), where we utilize the existing knowledge, we follow
the same sparsity level of SpaceNet for CIFAR-100 for l ≥lreuse."
REFERENCES,0.6805555555555556,"KAN allows utilizing the resources efﬁciently. When the new task shares some similarities with
previous existing knowledge, KAN exploits this knowledge instead of allocating unnecessary re-
sources. The saved capacity is utilized to learn new knowledge that the agent has not encountered
before. Thus, being aware of the existing knowledge helps in utilizing the available capacity efﬁ-
ciently and allowing for scaling to a large number of tasks in the CL sequence."
REFERENCES,0.6833333333333333,"D
CLASS AMBIGUITIES"
REFERENCES,0.6861111111111111,"In Section 5.2.2, we demonstrated that reusing the highest level representation (lreuse = L−1) from
the similar previous class in learning a new one causes ambiguities between the two classes in class-
IL. Therefore, we suggested to: (1) use the free neurons only in allocating the output connections
for the new class and (2) add new components for the new class in higher layers (lreuse < L −1).
In Section 5.2.2, we analyzed point (2) by studying different values for lreuse while satisfying point
(1). In this appendix, we analyze the effect of the point (1)."
REFERENCES,0.6888888888888889,"We performed this analysis on the sim seq 2Tasks benchmark. We use lreuse = L −2. We use the
candidate neurons in the output layer along with the free neurons to allocate the output connections.
The number of candidate neurons selc cand
l
in layers L −2 and L −1 are 15 and 40 respectively.
The number of selected free neurons in these two layers are 10 and 55 respectively. We compared
this baseline with the KAN model presented in Section 5.2.2 with lreuse = L −2. The number of
candidate neurons in layer L −2 is 15 and no candidate neurons are used in layer L-1 (Table 3).
We also compare against the KAN model with lreuse = L −1 presented in Section 5.2.1 where the
connections are allocated in the last layer only using the candidate neurons in layer L −1."
REFERENCES,0.6916666666666667,"Table 4 shows the performance of these models. Adding new connections in earlier layers (lreuse <
L −1) improves the performance of Task 2. However, using the free neurons only in allocating the
connections in the last layer reduces the ambiguities between similar classes since each class learns
its own speciﬁc representation. Thus, satisfying the two suggestions discussed above (lreuse < L−1
and halloc
L−1 = hsel
L−1) helps in achieving the balance in performance between the tasks."
REFERENCES,0.6944444444444444,Under review as a conference paper at ICLR 2022
REFERENCES,0.6972222222222222,Table 5: FLOPs required to learn the sim seq 2Tasks benchmark using different methods.
REFERENCES,0.7,"Strategy
Method
FLOPS
Regularization
SI
6.69e13"
REFERENCES,0.7027777777777777,Task-speciﬁc components
REFERENCES,0.7055555555555556,"PackNet
1.004e14
SpaceNet
1.02e13
KAN (ours)
8.14e12"
REFERENCES,0.7083333333333334,"E
COMPUTATIONAL EFFICIENCY"
REFERENCES,0.7111111111111111,"In this appendix, we report the number of ﬂoating-point operations (FLOPs) required for training
the regularization-based and task-speciﬁc components models. Regularization-based models are
based on dense networks while task-speciﬁc components models are based on sparse networks either
trained from scratch (i.e. SpaceNet (Sokar et al., 2021c), KAN (ours)) or pruned after training a
dense model (PackNet) (Mallya & Lazebnik, 2018)."
REFERENCES,0.7138888888888889,"The FLOPs metric is the traditional metric used in the literature to estimate the speed of training
algorithms and compare the computation efﬁciency of a sparse model against its dense counterpart
(Evci et al., 2020; Yuan et al., 2021). This is because current sparse neural network models are
simulated using binary masks over network weights. Truly sparse implementations with arbitrary
sparsity during training is a highly researched approach with no general solution yet (Curci et al.,
2021)."
REFERENCES,0.7166666666666667,"We follow the method described in (Evci et al., 2020) to calculate the FLOPs. The FLOPs are
calculated with the total number of multiplications and additions layer by layer in the network. We
compute this metric for the sim seq 2Task benchmark."
REFERENCES,0.7194444444444444,"Table 5 shows the computed FLOPs for each method. As illustrated in the table, SI requires more
FLOPs than SpaceNet because it trains the dense model for each task. KAN, requires fewer FLOPs
than SpaceNet as it updates the weights starting from lreuse. PacKNet requires the highest number
of FLOPs because it trains the dense model for each task and performs additional ﬁne-tuning epochs
after pruning the least important weights to recover the model performance."
REFERENCES,0.7222222222222222,"F
PERFORMANCE ACROSS SEVERAL BENCHMARKS"
REFERENCES,0.725,"In this appendix, we analyze the performance of the studied task-speciﬁc components models
on a sequence that contains two different datasets from two domains.
We construct this se-
quence using two standard datasets for class-IL: MNIST LeCun (1998); Zenke et al. (2017) and
Fashion MNIST (Xiao et al., 2017; Farquhar & Gal, 2019).
We name the new benchmark as
MNIST FashionMNIST 6tasks. The benchmark consists of 6 tasks. Every odd task contains two
consecutive classes from the MNIST dataset while every even task contains two consecutive classes
from Fashion MNIST."
REFERENCES,0.7277777777777777,"We performed this experiment using the same experimental setting used for the CIFAR-10 bench-
mark described in Appendix B except for the number of epochs. We train each task for 5 epochs.
We allocate new sparse connections in each layer for Task 2 as it contains new knowledge (Fashion
MNIST) different from the one learned from Task 1 (MNIST). Starting from the third task on-wards,
we use lreuse=L-3."
REFERENCES,0.7305555555555555,"Figure 8 shows the average accuracy on the seen tasks at each time step. KAN succeeds in main-
taining higher average performance than SpaceNet especially when the number of tasks increases.
Consistent with previous ﬁndings, selective transfer of KAN is more effective than random reuse of
previous components or irrelevant reuse."
REFERENCES,0.7333333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.7361111111111112,"1
2
3
4
5
6
# learned tasks 50 60 70 80 90 100"
REFERENCES,0.7388888888888889,Accuracy [%]
REFERENCES,0.7416666666666667,"KAN (ours)
SpaceNet
Random reuse
Irrelevant reuse"
REFERENCES,0.7444444444444445,"Figure 8: The performance of different methods in class-IL on MNIST FashionMNIST 6Tasks
benchmark."
REFERENCES,0.7472222222222222,"G
ADDITIONAL EXPERIMENTS AND ANALYSES"
REFERENCES,0.75,"G.1
PERFORMANCE OF REHEARSAL-BASED METHODS"
REFERENCES,0.7527777777777778,"In this paragraph, we compare the performance of KAN to more rehearsal-based methods. We ana-
lyze the performance of the task-speciﬁc component method PackNet (Mallya & Lazebnik, 2018).
PackNet is originally designed for the task-IL as discussed in Section 2. To adapt PackNet to the
class-IL, we use all the learned connections during inference without masks. The dense model is
trained from scratch on the CL tasks. We prune 85% of the network weights after training each task
to get approximately the same sparsity level used for the task-speciﬁc components baselines. We
perform 20 training epochs for ﬁne-tuning the network after pruning to restore its performance. We
use the ofﬁcial code from the authors to obtain the results of PackNet on the studied benchmarks."
REFERENCES,0.7555555555555555,"We also analyze the performance of other well-known regularization-based methods including
EWC (Serra et al., 2018), MAS (Aljundi et al., 2018), and LWF (Li & Hoiem, 2017). We use
the public code from (Masana et al., 2020) to get the results of these algorithms."
REFERENCES,0.7583333333333333,"Task 1
Task 2
Average
10 20 30 40 50 60 70 80 90"
REFERENCES,0.7611111111111111,Accuracy [%]
REFERENCES,0.7638888888888888,(a) Class-IL
REFERENCES,0.7666666666666667,"Task 1
Task 2
Average
40 50 60 70 80 90 100"
REFERENCES,0.7694444444444445,Accuracy [%]
REFERENCES,0.7722222222222223,(b) Task-IL
REFERENCES,0.775,"PackNet
KAN (ours) lreuse = L
3"
REFERENCES,0.7777777777777778,"SpaceNet
EWC
LWF
MAS
SI creg = 0.01"
REFERENCES,0.7805555555555556,Figure 9: Test accuracy of each task in sim seq 2Tasks and their average in class-IL and task-IL.
REFERENCES,0.7833333333333333,"1
2
3
4
5
# learned tasks 30 40 50 60 70 80 90 100"
REFERENCES,0.7861111111111111,Accuracy [%]
REFERENCES,0.7888888888888889,"PackNet
KAN (ours)
SpaceNet
LWF
EWC
MAS
SI"
REFERENCES,0.7916666666666666,Figure 10: The performance of different methods in class-IL on sim seq 5Tasks.
REFERENCES,0.7944444444444444,Under review as a conference paper at ICLR 2022
REFERENCES,0.7972222222222223,"Table 6: The average accuracy (ACC) [%] and the average backward transfer (BWT) [%] on
sim seq 2Tasks and sim seq 5Tasks benchmarks in class-IL."
REFERENCES,0.8,"sim seq 2Tasks
sim seq 5Tasks
Strategy
Method
ACC
BWT
ACC
BWT"
REFERENCES,0.8027777777777778,Regularization
REFERENCES,0.8055555555555556,"SI
54.51
-61.6
30.72
-47.66
EWC
49.95
- 78.60
27.70
-54.90
LWF
57.30
- 54.90
33.50
-57.4
MAS
51.80
- 60.50
25.40
-42.3"
REFERENCES,0.8083333333333333,Task speciﬁc components
REFERENCES,0.8111111111111111,"PackNet
57.42
-68.85
29.64
-58.01
SpaceNet
64.05
-36.67
41.53
-28.56
KAN (ours)
64.43
-32.58
43.48
-22.46
Random reuse
60.51
-33.17
41.10
-23.87
Irrelevant reuse
59.25
-29.39
39.54
-26.13"
REFERENCES,0.8138888888888889,"Figure 9 and Figure 10 show the performance of the studied methods on sim seq 2Tasks and
sim seq 5Tasks. Consistent with our previous ﬁndings, the stability-plasticity dilemma is more
challenging to be addressed using regularization-based methods especially in class-IL. The methods
based on Sparse representation (KAN, SpaceNet) perform better than PackNet. KAN outperforms
all studied baselines on both benchmarks."
REFERENCES,0.8166666666666667,"G.2
NEGATIVE BACKWARD TRANSFER (FORGETTING)"
REFERENCES,0.8194444444444444,"In this paragraph, we estimate the average forgetting that occurred in the studied methods. We use
the backward transfer metric (Lopez-Paz & Ranzato, 2017), BWT, which measures the inﬂuence of
learning new tasks on the performance of previous tasks. Formally BWT is calculated as follows:"
REFERENCES,0.8222222222222222,"BWT =
1
T −1"
REFERENCES,0.825,"T −1
X"
REFERENCES,0.8277777777777777,"i=1
RT,i −Ri,i,
(3)"
REFERENCES,0.8305555555555556,"where Rj,i is the accuracy on task i after learning the j-th task in the sequence, and T is the total
number of tasks."
REFERENCES,0.8333333333333334,"Table 6 shows the average accuracy over all tasks (ACC) after the model learned the whole sequence
along with the average backward transfer (BWT) on the sim seq 2Tasks and sim seq 5Tasks bench-
marks. As illustrated in the table, all regularization-based methods are more prone to forgetting
than most of the task-speciﬁc components methods. PackNet is more prone to forgetting than other
task-speciﬁc components methods which learn sparse representations. KAN achieves the highest
average performance and least forgetting on the two benchmarks."
REFERENCES,0.8361111111111111,"H
SOFTMAX CLASSIFICATION LAYER FOR CONTINUAL LEARNING"
REFERENCES,0.8388888888888889,"H.1
DIFFERENT SETUPS FOR THE OUTPUT LAYER"
REFERENCES,0.8416666666666667,"Next, we will discuss the different setups proposed in the literature for the softmax classiﬁcation
layer in class-IL."
REFERENCES,0.8444444444444444,"Cosine normalization. In (Hou et al., 2019), the authors proposed to use cosine normalization in
the last layer to address the imbalance in the magnitudes of the output weights between old and new
classes. Thus, the predicted probability for sample x is calculated as:"
REFERENCES,0.8472222222222222,"pi(x) =
exp(η < ¯
Wi, ¯fL−1(x) >)
P"
REFERENCES,0.85,"j exp(η < ¯
Wj, ¯fL−1(x) >),
(4)"
REFERENCES,0.8527777777777777,"where ¯fL−1(x) is the l2 normalized vector of the last hidden representation, ¯
Wi is the normalized
weights connected to the output neuron i, < a, b > is the cosine similarity between the two vectors
a and b, and η is a learnable scalar to control the peakiness of softmax distribution. The setup was
evaluated on a rehearsal-base method."
REFERENCES,0.8555555555555555,Under review as a conference paper at ICLR 2022
REFERENCES,0.8583333333333333,"Task 1
Task 2
0 1 2 3 4"
REFERENCES,0.8611111111111112,L2 norm of weights
REFERENCES,0.8638888888888889,"Masked output weights
Unmasked output weights"
REFERENCES,0.8666666666666667,"Figure 11: L2 normalization of the weights of the output layer corresponding to each task in the
sim seq 2Tasks benchmark. The magnitude of the weights of Task 2 is higher than Task 1 when the
Task 1 weights are not masked during learning Task 2."
REFERENCES,0.8694444444444445,"Multi-headed output layer. The multi-headed output setup is the common practice used in Task-IL
where each task has its own output head. In (Mittal et al., 2021), the authors showed that this setup
improves the performance of rehearsal-based methods in class-IL. In the multi-headed setup, the
softmax activation function σ(.) is applied on the classes of the current task only (y ∈y[t ∗C :
t ∗C + C]), where C is the number of classes in each task. The cross-entropy loss is evaluated on
this selected part of the vector only."
REFERENCES,0.8722222222222222,"Pre-deﬁned single-headed. SpaceNet (Sokar et al., 2021c) tries to address the closed-world as-
sumption by deﬁning an output layer with a large number of neurons before start learning the se-
quence of tasks t = 0. To address the weights imbalance between old and new classes, only the
weights of the new classes of the current task are active during their learning. During learning task
t, a mask M t
L−1 is used to indicate the connections connected to the output neurons for the current
task where M t
L−1[t ∗C : t ∗C + C] equals 1 and 0 in the other indices. During learning task t, the
weights of the output layer are multiplied by this mask which blocks the gradient ﬂow from other
output neurons:
WL−1 = WL−1 ⊗M t
L−1,
(5)"
REFERENCES,0.875,"where ⊗is the element-wise multiplication operator. Figure 11 shows the effect of masking the
output weights in learning a new task. We calculate the L2 normalization of the output weights of
each task in the sim seq 2Tasks benchmarks using SpaceNet. As illustrated in the ﬁgure, using all
output weights in learning new tasks leads to higher weights for the new task. This causes bias in
prediction towards the new task. While using a mask over the current task output weight alleviates
this problem."
REFERENCES,0.8777777777777778,"Extendable layer. This is the typical setup used in class-IL where a single-head output layer is
used. At each time step, the layer is extended to include new output neurons for the new classes
in the current task. In our analysis in Section 5.5, we implement the masking trick described in
the previous paragraph for the extendable layer baseline. As you can see from Figure 4c, this does
not solve the other limitation for the softmax being based on the closed world assumption. Figure
12 illustrates this limitation. We show the learned biases after learning two tasks from the Split
CIFAR-10 benchmark in two different setups: pre-deﬁned single-headed and extendable layer. As
illustrated in the ﬁgure, the learned biases for the ﬁrst task in the case of extendable layer divides
the feature space between the two classes in the ﬁrst task. The biases of Task 2 classes have a larger
value of the bias than for Task 1. The pre-deﬁned single-head layer overcomes this limitation."
REFERENCES,0.8805555555555555,"Few previous works (Yu et al., 2020; Li et al., 2021) started to propose alternatives for the standard
softmax for continual learning to overcome its limitations. More efforts are needed in addressing
the limitations of the softmax to improve the performance of the CL agent in class-IL."
REFERENCES,0.8833333333333333,"H.2
RE-EVALUATION OF REGULARIZATION-BASED METHODS IN CLASS-IL"
REFERENCES,0.8861111111111111,"Previous studies show the huge performance drop of the regularization-based method in class-
IL (van de Ven & Tolias, 2018; Hsu et al., 2018; Maltoni & Lomonaco, 2019). In these methods,
the agent tends to be biased toward the last task while catastrophically forgets the previous ones."
REFERENCES,0.8888888888888888,Under review as a conference paper at ICLR 2022
REFERENCES,0.8916666666666667,"Task 1
Task 2
Task 1
Task 2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Bias"
REFERENCES,0.8944444444444445,"Extendable singlehead
Pre-defined singlehead"
REFERENCES,0.8972222222222223,"Figure 12: The learned biases in two different setups: Extendable single-headed and Pre-deﬁned
single-head. Each column represents the value of the bias for each class in the two tasks."
REFERENCES,0.9,"Table 7: Test accuracy on Split CIFAR-10 of the regularization method SI using different setups of
classiﬁcation output layer in the class-incremental learning."
REFERENCES,0.9027777777777778,"Setup
Accuracy"
REFERENCES,0.9055555555555556,"Extendable layer
19.20 ± 0.04
Multi-headed layer
36.84 ± 0.94"
REFERENCES,0.9083333333333333,"We evaluated one of the regularization-based methods SI (Zenke et al., 2017) using one of the se-
tups discussed before; multiheaded setup. We performed this experiment on the Split CIFAR-10
benchmark with 5 tasks."
REFERENCES,0.9111111111111111,"Table 7 shows the average test accuracy overall tasks after learning the whole sequence. The multi-
headed output layer increases the performance of the agent by 17.64%. This experiment suggests
to re-consider the regularization methods in class-IL while addressing the limitations of the softmax
output layer."
REFERENCES,0.9138888888888889,"I
SPACENET"
REFERENCES,0.9166666666666666,"In this appendix, we include the details of the three main steps of the SpaceNet algorithm (Sokar
et al., 2021c): (1) Connection Allocation (Algorithm 3), (2) Dynamic Sparse Training (Algorithm
4), and (3) Neuron Reservation 5. The full algorithm of SpaceNet is illustrated in Algorithm 2."
REFERENCES,0.9194444444444444,Algorithm 2 SpaceNet for Continual Learning
REFERENCES,0.9222222222222223,"1: Require: loss function L , training dataset for each task in the sequence Dt"
REFERENCES,0.925,"2: Require: sparsity level ϵ, rewiring fraction r
3: Require: number of selected neurons selt
l, number of speciﬁc neurons spect
l
4: for each layer l do
5:
hfree
l
←hl
▷Initialize free neurons with all neurons in l
6:
hspec
l
←∅
7:
Wl ←∅
8:
W saved
L
←∅
9: end for each
10: for each available task t do
11:
W ←ConnectionsAllocation(ϵ, selt
l, hfree)
▷Perform Algorithm 3
12:
W t ←TaskTraining(W,Dt,L,r)
▷Perform Algorithm 4
13:
hfree
l
←NeuronsReservation(spect
l)
▷Perform Algorithm 5
14:
W saved
L
←W saved
L
∪W t
L
▷Retain the connections of last layer for task t
15:
WL ←WL \ W t
L
16: end for each"
REFERENCES,0.9277777777777778,Under review as a conference paper at ICLR 2022
REFERENCES,0.9305555555555556,Algorithm 3 Connections allocation
REFERENCES,0.9333333333333333,"1: Require: number of selected neurons selt
l, sparsity level ϵ
2: for each layer do
3:
(hsel
l−1, hsel
l
) ←randomly select selt
l−l and selt
l neurons from hfree
l−1 and hfree
l
4:
randomly allocate parameters W t
l with sparsity ϵ between hsel
l−1 and hsel
l
5:
Wl ←Wl ∪W t
l
6: end for each"
REFERENCES,0.9361111111111111,Algorithm 4 Dynamic sparse training
REFERENCES,0.9388888888888889,"1: Require: loss function L , training dataset Dt, rewiring fraction r
2: for each training epoch do
3:
perform standard forward pass through the network parameters W
4:
update parameters W t using stochastic gradient descent
5:
for each sparse parameter W t
l do
6:
g
W t
l ←sort W t
l based on the connections importance Ωl
7:
(W t
l , kl) ←drop (g
W t
l ,r)
▷Remove the weights with smallest importance
8:
compute neuron importance al−1 and al
▷Neurons importance for task t
9:
Gl ←al−1aT
l
10:
f
Gl ←sortDescending(Gl)
11:
Gpos ←select top-kl positions in f
Gl where Wl equals zero
12:
W t
l ←grow(W t
l ,Gpos)
▷Grow kl zero-initialized weights in Gpos
13:
end for each
14: end for each"
REFERENCES,0.9416666666666667,Algorithm 5 Neurons reservation
REFERENCES,0.9444444444444444,"1: Require: number of speciﬁc neurons spect
l
2: for each layer l do
3:
compute the neuron importance al for task t
4:
eal ←sortDescending(al)
5:
h
tspec
l
←top-spect
l from eal
6:
hspec
l
←hspec
l
∪h
tspec
l
7:
hfree
l
←hfree
l
\ h
tspec
l
8: end for each"
REFERENCES,0.9472222222222222,"J
SYNAPTIC INTELLIGENCE (SI)"
REFERENCES,0.95,"SI (Zenke et al., 2017) is a regularization-based method proposed for the continual learning
paradigm. It uses a ﬁxed-capacity dense model in learning the sequence of tasks. During train-
ing each task, it estimates how important each parameter is in learning the current task. When the
model faces a new task, it regularizes (penalizes) changes to parameters according to their impor-
tance to previously learned tasks. The importance of each parameter to a certain task is estimated
by: (1) its contribution to the change of the loss ω(t)
i
(i.e. how much does a small change to the
parameter change the loss function?) and (2) how far it moved ∆(t)
i . Following the paper notations,
for the current task t, the per-parameter contribution in loss ω(t)
i
is calculated as follows:"
REFERENCES,0.9527777777777777,"ωt
i = −"
REFERENCES,0.9555555555555556,"Niters
X"
REFERENCES,0.9583333333333334,"j=1
(θt
i(j) −θt
i(j −1))δLt
j
δθi
,
(6)"
REFERENCES,0.9611111111111111,"where Niters is the total number of training iterations per task, θt
i(j) is the value of parameter i after"
REFERENCES,0.9638888888888889,"performing iteration j on task t, and
δLt
j
δθi is the gradient of the loss with respect to parameter i at
iteration j during learning task t."
REFERENCES,0.9666666666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.9694444444444444,The distance travelled by a parameter i during learning task t is calculated as follows:
REFERENCES,0.9722222222222222,"∆t
i = θt
i(Niters) −θ(t−1)
i
(Niters).
(7)"
REFERENCES,0.975,"Thus, the estimated importance of each parameter i for previous t −1 tasks is calculated as follows:"
REFERENCES,0.9777777777777777,"Ω(t−1)
i
="
REFERENCES,0.9805555555555555,"(t−1)
X k=1"
REFERENCES,0.9833333333333333,"ωk
i
 
∆k
i
2 + ξ
,
(8)"
REFERENCES,0.9861111111111112,"where ξ is a damping parameter to bound the expression in cases where ∆t
i goes to zero. During
learning task t, a regularization term Lt
reg is added to the classiﬁcation loss Lt
cls to penalize the
change in the weights based on their importance. Lt
reg is given by:"
REFERENCES,0.9888888888888889,"Lt
reg =
X"
REFERENCES,0.9916666666666667,"i
Ω(t−1)
i

θi −θ(t−1)
i
(Niters)
2
(9)"
REFERENCES,0.9944444444444445,"The total loss for task t is:
Lt = Lt
cls + cregLt
reg,
(10)"
REFERENCES,0.9972222222222222,"where creg is a hyper-parameter to control the trade-off between the classiﬁcation and regularization
terms."
