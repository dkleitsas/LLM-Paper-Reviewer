Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001081081081081081,"We propose a novel encoding method called “Structure Token” to unify the pro-
1"
ABSTRACT,0.002162162162162162,"cessing and generation of both graphs and texts with a single transformer-based
2"
ABSTRACT,0.003243243243243243,"model. This method allows graphs with text labels to be generated by a series
3"
ABSTRACT,0.004324324324324324,"of tokens, enabling both graph and text data to be handled interchangeably. By
4"
ABSTRACT,0.005405405405405406,"utilizing structure tokens, our model learns a unified representation, enhancing
5"
ABSTRACT,0.006486486486486486,"the ability to process diverse data without requiring extra modules or models.
6"
ABSTRACT,0.0075675675675675675,"Additionally, the model can be trained like most transformer models with simply
7"
ABSTRACT,0.008648648648648649,"cross-entropy loss. To demonstrate the effectiveness of our method, we introduce a
8"
ABSTRACT,0.00972972972972973,"pre-training scheme inspired by mBART but adapted to leverage structure tokens.
9"
ABSTRACT,0.010810810810810811,"Our model, named TextGraphBART, uses the same architecture as normal Trans-
10"
ABSTRACT,0.011891891891891892,"former Encoder-Decoder models with small modifications on the input and output
11"
ABSTRACT,0.012972972972972972,"to accommodate structure tokens. The evaluations show that this approach achieves
12"
ABSTRACT,0.014054054054054054,"comparable results against baseline models of similar sizes on both text-to-graph
13"
ABSTRACT,0.015135135135135135,"and graph-to-text generation tasks, without needing specialized loss functions or
14"
ABSTRACT,0.016216216216216217,"sampling techniques. These findings suggest that our approach can effectively
15"
ABSTRACT,0.017297297297297298,"bridge the gap between textual and structural data representations, and the design
16"
ABSTRACT,0.018378378378378378,"of encoding method could offer a new direction for future improvement.
17"
INTRODUCTION,0.01945945945945946,"1
Introduction
18"
INTRODUCTION,0.02054054054054054,"Transformer layers have been proven to work well in several domains beyond text, like audio, image,
19"
INTRODUCTION,0.021621621621621623,"and even multi-modal data. Some research has also shown that with careful design, transformer layers
20"
INTRODUCTION,0.022702702702702703,"can extract features from graph data[30, 16]. Graph is a common data structure for representing
21"
INTRODUCTION,0.023783783783783784,"concepts and relationships. In this work, we focus on a specific type, named text graph, where
22"
INTRODUCTION,0.024864864864864864,"the concepts and relationships are expressed as texts, such as knowledge graphs and parsing trees.
23"
INTRODUCTION,0.025945945945945945,"Learning vector representations and generating new text graphs are two essential aspects of text
24"
INTRODUCTION,0.02702702702702703,"graphs in machine learning. Since texts can be viewed as a chain of words or characters, the text
25"
INTRODUCTION,0.02810810810810811,"graph then becomes a nested graph. The complexity of handling such a nested graph leads to two
26"
INTRODUCTION,0.02918918918918919,"major approaches for generating text graphs.
27"
INTRODUCTION,0.03027027027027027,"The first strand is the multi-stage approach which generates concepts, relationships, and texts in
28"
INTRODUCTION,0.03135135135135135,"different steps [22, 12]. The process usually involves multiple models that generate different parts
29"
INTRODUCTION,0.032432432432432434,"of the text graph. For example, Grapher [22] uses the T5 [24] pre-trained model to generate all the
30"
INTRODUCTION,0.03351351351351351,"concepts and then uses a relation extraction model to predict the relationships between every two
31"
INTRODUCTION,0.034594594594594595,"concepts. This approach requires the models to generate a complete graph despite the edge sparsity
32"
INTRODUCTION,0.03567567567567568,"of the target text graph. Therefore, the model includes a special “no-relation"" relationship and turns
33"
INTRODUCTION,0.036756756756756756,"every graph into a complete graph thus requiring extra predictions. Since the relation extraction is
34"
INTRODUCTION,0.03783783783783784,1code available at: https://github.com
INTRODUCTION,0.03891891891891892,"Transformer Encoder
Transformer Decoder ..."
INTRODUCTION,0.04,"...
... id"
INTRODUCTION,0.04108108108108108,Structure Predictor
INTRODUCTION,0.04216216216216216,"x N
x N"
INTRODUCTION,0.043243243243243246,"unmodified Transformer
Encoder-Decoder"
INTRODUCTION,0.04432432432432432,"autoregressive
token decoding ..."
INTRODUCTION,0.04540540540540541,structure encode / decode
INTRODUCTION,0.046486486486486484,structure embed
INTRODUCTION,0.04756756756756757,input projection
INTRODUCTION,0.04864864864864865,structure encode / decode
INTRODUCTION,0.04972972972972973,structure embed
INTRODUCTION,0.05081081081081081,input projection
INTRODUCTION,0.05189189189189189,(a) Model with structure token ...
INTRODUCTION,0.05297297297297297,Transformer Encoder
INTRODUCTION,0.05405405405405406,"Akron, Ohio is located ...."
INTRODUCTION,0.055135135135135134,encode & embed
INTRODUCTION,0.05621621621621622,input projection
INTRODUCTION,0.057297297297297295,Iteration 0
INTRODUCTION,0.05837837837837838,"Decoder & Predictor
[graph]
Iteration 1"
INTRODUCTION,0.05945945945945946,"country
[graph]"
INTRODUCTION,0.06054054054054054,"country {
}"
INTRODUCTION,0.061621621621621624,Iteration 2
INTRODUCTION,0.0627027027027027,"{
}
[graph] country"
INTRODUCTION,0.06378378378378378,"Decoder & Predictor
Akron Akron"
INTRODUCTION,0.06486486486486487,Iteration 3
INTRODUCTION,0.06594594594594595,"{
}
Akron"
INTRODUCTION,0.06702702702702702,Decoder & Predictor
INTRODUCTION,0.0681081081081081,[graph] country Ohio Ohio ...
INTRODUCTION,0.06918918918918919,Iteration 4
INTRODUCTION,0.07027027027027027,"{
}
encode
&
embed"
INTRODUCTION,0.07135135135135136,"encode
&
embed"
INTRODUCTION,0.07243243243243243,"encode
&
embed"
INTRODUCTION,0.07351351351351351,(b) Autoregressive decoding
INTRODUCTION,0.0745945945945946,"Figure 1: Overview of the proposed structure token approach. (1a) The model takes the input text
graph (left) and the partially generated sub-graph (right) and then generates a new structure token.
Each structure token contains a (sub-)word token with the locational information of that word token
in the text graph. (1b) An example of autoregressive decoding with structure token. The procedure is
mostly the same as normal text decoding with Transformer model."
INTRODUCTION,0.07567567567567568,"done on every two concepts, the model does not consider multi-hop relations. Moreover, the model
35"
INTRODUCTION,0.07675675675675675,"cannot handle the case where two concepts have more than one relation. The second method is the
36"
INTRODUCTION,0.07783783783783783,"graph linearization approach that fuses the hierarchy in text graph into chain of tokens [1, 10]. This
37"
INTRODUCTION,0.07891891891891892,"approach treats the text graph as a special text sequence and enables the direct adoption of Language
38"
INTRODUCTION,0.08,"Model (LM) for text graph generation. The idea can also be applied to learning vector representations
39"
INTRODUCTION,0.08108108108108109,"of text graphs. For example, BT5 [1] convert the text graph into sequence of (subject, relation,
40"
INTRODUCTION,0.08216216216216216,"object) triples and train T5 to translate between sentence and sequence of triples. Since sequence
41"
INTRODUCTION,0.08324324324324324,"generation with LM is done in an autoregressive manner, the generation is conditioned on the
42"
INTRODUCTION,0.08432432432432432,"already generated triples. This behavior allows the model to handle multi-hop relations and the
43"
INTRODUCTION,0.08540540540540541,"multi-relational case. Meanwhile, the model does not suffer from generating complete graphs because
44"
INTRODUCTION,0.08648648648648649,"the model can learn to terminate when the generated triples match the target text graph. However,
45"
INTRODUCTION,0.08756756756756756,"using sequence of triples also introduces extra complexity to the LM. Since the format requires
46"
INTRODUCTION,0.08864864864864865,"matching the concepts in triples to reconstruct the text graph, there will be duplications of subject
47"
INTRODUCTION,0.08972972972972973,"or object in the sequence. Thus, the model generates duplicated texts and cannot handle the case
48"
INTRODUCTION,0.09081081081081081,"where two concepts are represented by the same text but refer to different things. Also, it relies on
49"
INTRODUCTION,0.0918918918918919,"the model to implicitly learn the connection between two triples with duplication. Furthermore, LM
50"
INTRODUCTION,0.09297297297297297,"is neither permutation invariance nor equivariance, which means the prediction alters if the generated
51"
INTRODUCTION,0.09405405405405405,"triples are being shuffled.
52"
INTRODUCTION,0.09513513513513513,"The goal of this work is to design a new approach that preserves some of the advantages while
53"
INTRODUCTION,0.09621621621621622,"avoiding the drawbacks of previous approaches. This sets a few desired properties of the new
54"
INTRODUCTION,0.0972972972972973,"approach. First, the method should be suitable for both representation and generation. It should also
55"
INTRODUCTION,0.09837837837837837,"consider the cases that cannot be handled by multi-stage and graph linearization approach. Second,
56"
INTRODUCTION,0.09945945945945946,"the model should be permutation equivariance and perform generation in an autoregressive manner.
57"
INTRODUCTION,0.10054054054054054,"Last, the method should avoid extra computation, such as the duplication of concepts. To achieve
58"
INTRODUCTION,0.10162162162162162,"the desired characteristics, we propose the structure token approach, as illustrated in 1. Our method
59"
INTRODUCTION,0.10270270270270271,"employs a concept we call “Structure Token"" which losslessly encodes the text graph into a set of
60"
INTRODUCTION,0.10378378378378378,"tokens. The token contains a word and a few identifiers for the precise location of that (sub-)word
61"
INTRODUCTION,0.10486486486486486,"in the text graph. Our model incorporates an unmodified Transformer Encoder-Decoder model [26]
62"
INTRODUCTION,0.10594594594594595,"with structure token embeddings and a structure predictor for predicting new structure tokens. The
63"
INTRODUCTION,0.10702702702702703,"text graph is generated autoregressively like regular text generation and the generated structure tokens
64"
INTRODUCTION,0.10810810810810811,"form a subgraph. Once the generation stops, we can decode the set of structure tokens into the
65"
INTRODUCTION,0.10918918918918918,"target text graph. A notable difference between our approach and previous approaches is that our
66"
INTRODUCTION,0.11027027027027027,"Transformer model operates on sets instead of sequences and we view text graphs as nested graphs.
67"
INTRODUCTION,0.11135135135135135,"To our knowledge, our structure token approach is the first method that can autoregressively generate
68"
INTRODUCTION,0.11243243243243244,"sub-graphs with multi-token labels without modifying transformers.
69"
INTRODUCTION,0.11351351351351352,"We validate our structure token approach on text-to-graph (T2G) generation and graph-to-text (G2T)
70"
INTRODUCTION,0.11459459459459459,"generation tasks. By treating sentence as a special text graph without any edges, the generation
71"
INTRODUCTION,0.11567567567567567,"tasks become a text-graph-to-text-graph translation problem. Therefore, we present TextGraphBART,
72"
INTRODUCTION,0.11675675675675676,"a Transformer Encoder-Decoder model pre-trained on text-graph-to-text-graph translation with
73"
INTRODUCTION,0.11783783783783784,"our structure token approach. The model is evaluated on two publicly available parallel datasets,
74"
INTRODUCTION,0.11891891891891893,"EventNarrative [8] and WebNLG (2020) [5], and achieves comparable results using fewer parameters
75"
INTRODUCTION,0.12,"and pre-training data.
76"
RELATED WORK,0.12108108108108108,"2
Related Work
77"
RELATED WORK,0.12216216216216216,"Recent works for text graph generation primarily focus on reusing pre-trained LMs. BT5 [1] applies
78"
RELATED WORK,0.12324324324324325,"the graph linearization approach with T5 [24] pre-trained model. ReGen [10] further proposes a
79"
RELATED WORK,0.12432432432432433,"Reinforcement Learning (RL) objective to improve the performance. On the other hand, Grapher [22]
80"
RELATED WORK,0.1254054054054054,"uses T5 as an entity extraction model and jointly trains another relation extraction model. It provides
81"
RELATED WORK,0.1264864864864865,"two types of implementation: Grapher (Text) and Grapher (Query). The Grapher (Text), which is the
82"
RELATED WORK,0.12756756756756757,"state-of-the-art method on WebNLG (2020) [5] dataset, generates entities as a flat sequence, while
83"
RELATED WORK,0.12864864864864864,"Grapher (Query) feeds a set of query embeddings to T5 decoder and apply another model to fully
84"
RELATED WORK,0.12972972972972974,"decode the outputs to entities. Other works target on the problem of lacking paired datasets for T2G
85"
RELATED WORK,0.1308108108108108,"generation. CycleGT [12] apply the cycle-training framework on a G2T model and a multi-stage
86"
RELATED WORK,0.1318918918918919,"T2G model. INFINITY [29] apply the cycle-training framework on a single T5 model with graph
87"
RELATED WORK,0.13297297297297297,"linearization approach.
88"
RELATED WORK,0.13405405405405404,"Most of the works for learning vector representations only focus on G2T generation. GAP [9]
89"
RELATED WORK,0.13513513513513514,"proposes a graph-aware attention that first uses the pooling operator to get the features of each text
90"
RELATED WORK,0.1362162162162162,"label and then applies a modified attention operator on those features. KGPT [6] provides two types
91"
RELATED WORK,0.1372972972972973,"of encoder: graph encoder and sequence encoder. The graph encoder is based on graph attention
92"
RELATED WORK,0.13837837837837838,"network [27] that also operates on the pooled text features. On the other hand, the sequence encoder
93"
RELATED WORK,0.13945945945945945,"infuses the structure information into the text tokens and feeds them into a transformer model. This
94"
RELATED WORK,0.14054054054054055,"approach resembles our structure token approach upon learning vector representations. However,
95"
RELATED WORK,0.14162162162162162,"they convert the graph into a dictionary-like format which suffers from a similar duplication problem
96"
RELATED WORK,0.14270270270270272,"as graph linearization approach.
97"
RELATED WORK,0.1437837837837838,"Our method is essentially derived from the design of TokenGT [16], which also converts graphs
98"
RELATED WORK,0.14486486486486486,"into sets of tokens containing labels and identifiers. However, their idea does not directly fit in our
99"
RELATED WORK,0.14594594594594595,"scenario of text graph for two reasons. First, a single graph element (node or edge) in TokenGT needs
100"
RELATED WORK,0.14702702702702702,"to be representable by a single token. On the contrary, it would require multiple tokens for an element
101"
RELATED WORK,0.14810810810810812,"of text graph because the label is a multi-token text. Second, TokenGT only focuses on representing
102"
RELATED WORK,0.1491891891891892,"the graph, while we are interested in graph generation as well.
103"
METHOD,0.15027027027027026,"3
Method
104"
STRUCTURE TOKEN,0.15135135135135136,"3.1
Structure Token
105"
STRUCTURE TOKEN,0.15243243243243243,"The proposed structure token is a data representation that can losslessly encode all data in a text
106"
STRUCTURE TOKEN,0.1535135135135135,"graph as a set of tokens. Given a text graph G = (N, A) containing a node set N and an arc set A.
107"
STRUCTURE TOKEN,0.1545945945945946,"Each arc is a triple of a head node, an edge, and a tail node. Each graph element (node and edge)
108"
STRUCTURE TOKEN,0.15567567567567567,"is a unique text label S identifiable with an integer ID. This allows different nodes or edges to have
109"
STRUCTURE TOKEN,0.15675675675675677,"the same text label. The full formal definitions can be found in Appendix A.1. In order to convert
110"
STRUCTURE TOKEN,0.15783783783783784,"text graph to structure tokens, we express the node set and arc set into one unified structure of graph
111"
STRUCTURE TOKEN,0.1589189189189189,"elements. Each graph element will be represented by multiple structure tokens. A structure token
112"
STRUCTURE TOKEN,0.16,"consists of seven parts: 1. Label: The (sub-)word token of a graph element. 2. Type: A binary
113"
STRUCTURE TOKEN,0.16108108108108107,"indicator specifying whether this graph element is a node or an edge. 3. Token ID: An unique ID for
114"
STRUCTURE TOKEN,0.16216216216216217,"this token. 4. Previous ID: The token ID of previous token. 5. Segment ID: An unique ID for the
115"
STRUCTURE TOKEN,0.16324324324324324,"graph element. 6. Head ID: The segment ID of the head node. 7. Tail ID: The segment ID of the
116"
STRUCTURE TOKEN,0.1643243243243243,"tail node. If the token is part of a node, the head ID and tail ID will just be the segment ID of itself.
117"
STRUCTURE TOKEN,0.1654054054054054,"With these information, we are able to differentiate between structure tokens that are from different
118"
STRUCTURE TOKEN,0.16648648648648648,"parts of the graph. The text graph is converted into a set of structure tokens. Since the IDs can point
119"
STRUCTURE TOKEN,0.16756756756756758,"to a graph element directly, there is no need for duplications like graph linearization approach. We
120"
STRUCTURE TOKEN,0.16864864864864865,"provide the formal definition of structure token in the Appendix A.2. A real example of text graph
121"
STRUCTURE TOKEN,0.16972972972972972,"[T]
[T]
[T]
[N]
[Domain]"
STRUCTURE TOKEN,0.17081081081081081,Text Graph Label
STRUCTURE TOKEN,0.17189189189189188,Token ID
STRUCTURE TOKEN,0.17297297297297298,Segment ID
STRUCTURE TOKEN,0.17405405405405405,Head ID
STRUCTURE TOKEN,0.17513513513513512,Tail ID
STRUCTURE TOKEN,0.17621621621621622,Previous ID
STRUCTURE TOKEN,0.1772972972972973,"2
1
10
10"
STRUCTURE TOKEN,0.1783783783783784,"2
1
2
10"
STRUCTURE TOKEN,0.17945945945945946,"2
1
6
10"
STRUCTURE TOKEN,0.18054054054054053,"1
1
2
3
4
1
6
7
10
1
12
8"
STRUCTURE TOKEN,0.18162162162162163,"2
1
3
4
5
6
7
8
11
10
13
9"
STRUCTURE TOKEN,0.1827027027027027,president of
STRUCTURE TOKEN,0.1837837837837838,"Joe
[E] president
of
Biden
[N]"
STRUCTURE TOKEN,0.18486486486486486,Joe Biden
STRUCTURE TOKEN,0.18594594594594593,United States
STRUCTURE TOKEN,0.18702702702702703,United States
STRUCTURE TOKEN,0.1881081081081081,"Type
1
1
0
1 11"
STRUCTURE TOKEN,0.1891891891891892,"12
... ..."
STRUCTURE TOKEN,0.19027027027027027,(a) Example of structure tokens. Type
STRUCTURE TOKEN,0.19135135135135134,"Label
Token ID"
STRUCTURE TOKEN,0.19243243243243244,Segment ID
STRUCTURE TOKEN,0.1935135135135135,Head ID
STRUCTURE TOKEN,0.1945945945945946,Tail ID
STRUCTURE TOKEN,0.19567567567567568,Previous ID
STRUCTURE TOKEN,0.19675675675675675,")
Orthn( 10"
STRUCTURE TOKEN,0.19783783783783784,")
OneHot2( 0"
STRUCTURE TOKEN,0.1989189189189189,")
Orthn( 7"
STRUCTURE TOKEN,0.2,")
Orthn( 2"
STRUCTURE TOKEN,0.20108108108108108,")
Orthn( 6"
STRUCTURE TOKEN,0.20216216216216215,")
Orthn( 8
+"
STRUCTURE TOKEN,0.20324324324324325,concat embed
STRUCTURE TOKEN,0.20432432432432432,"structure
embedding"
STRUCTURE TOKEN,0.20540540540540542,"OneHot|T |(
)
of"
STRUCTURE TOKEN,0.2064864864864865,(b) Converting structure token to embedding.
STRUCTURE TOKEN,0.20756756756756756,"Figure 2: Structure token and embedding. (2a) Each column is a structure token and each token has a
unique token ID. The IDs can be used to locate the (sub-)word in the text graph. (2b) Both OneHot
and Orth convert the ID or word into a vector and those vectors would be concatenated together to
form the embeddings of the structure token"
STRUCTURE TOKEN,0.20864864864864865,"and corresponding structure tokens can be found in Figure 2a. The idea of type, head ID, and tail ID
122"
STRUCTURE TOKEN,0.20972972972972972,"are inherited from TokenGT [16], which uses identifiers to indicate the connections. We modify their
123"
STRUCTURE TOKEN,0.21081081081081082,"definition and introduce extra identifiers for our text components. The token ID and previous ID are
124"
STRUCTURE TOKEN,0.2118918918918919,"text-level identifiers. The text order is determined by the token ID and previous ID for reconstructing
125"
STRUCTURE TOKEN,0.21297297297297296,"the text label. On the other hand, the segment ID, head ID, and tail ID are graph-level identifiers. For
126"
STRUCTURE TOKEN,0.21405405405405406,"tokens of a specific graph element, the graph-level identifiers of each token will be the same.
127"
STRUCTURE TOKEN,0.21513513513513513,"Furthermore, We add an extra “domain token” to the structure tokens of a text graph to indicate the
128"
STRUCTURE TOKEN,0.21621621621621623,"domain of the graph, like the special language token used in multi-lingual translation [15, 21]. With
129"
STRUCTURE TOKEN,0.2172972972972973,"the domain token, we can specify what kind of data the text graph is holding. For example, since text
130"
STRUCTURE TOKEN,0.21837837837837837,"is treated as a text graph without any edges, we use a “[text]” domain token to indicate that this text
131"
STRUCTURE TOKEN,0.21945945945945947,"graph represents a text. Besides, we use the domain token as the first token of every text label, so the
132"
STRUCTURE TOKEN,0.22054054054054054,"previous ID of the first token of all labels are pointing to the domain token.
133"
STRUCTURE EMBEDDING,0.22162162162162163,"3.2
Structure Embedding
134"
STRUCTURE EMBEDDING,0.2227027027027027,"The structure tokens are transformed into fixed-size high-dimensional vector representations. Each
135"
STRUCTURE EMBEDDING,0.22378378378378377,"part of the structure token is converted into a vector and concatenated together. Then the vectorized
136"
STRUCTURE EMBEDDING,0.22486486486486487,"result will be fed to a trainable projection layer for getting the token embedding, as illustrated in
137"
STRUCTURE EMBEDDING,0.22594594594594594,"Figure 2b. Label and type are converted with one-hot encoding, denoted as OneHot. On the other
138"
STRUCTURE EMBEDDING,0.22702702702702704,"hand, the IDs need to be handled differently. In order to preserve the graph structure in the tokens
139"
STRUCTURE EMBEDDING,0.2281081081081081,"with the Transformer model, each ID needs to be converted into orthonormal vectors as proved by
140"
STRUCTURE EMBEDDING,0.22918918918918918,"TokenGT [16]. We loose the requirement of orthonormality and use a set of orthonormal-like vectors.
141"
STRUCTURE EMBEDDING,0.23027027027027028,"The dot product value of two different orthonormal-like vector is close to zero or less than some
142"
STRUCTURE EMBEDDING,0.23135135135135135,"thresholds. These vectors of identifiers enable the attention operation in the Transformer model to be
143"
STRUCTURE EMBEDDING,0.23243243243243245,"able to aggregate corresponding information through dot product. Each ID would be converted into an
144"
STRUCTURE EMBEDDING,0.23351351351351352,"orthonormal-like vector through a transform function Orth. We use this transform function to convert
145"
STRUCTURE EMBEDDING,0.23459459459459459,"the graph-level identifiers directly. On the other hand, we add the vectors of the text-level identifiers
146"
STRUCTURE EMBEDDING,0.23567567567567568,"together, which allows the attention to aggregate information from neighbor tokens like the absolute
147"
STRUCTURE EMBEDDING,0.23675675675675675,"position embedding [26]. Details and definitions can be found in Appendix A.3. Unlike position
148"
STRUCTURE EMBEDDING,0.23783783783783785,"embedding which depends on the location of the tokens in a sequence, the text-level identifiers
149"
STRUCTURE EMBEDDING,0.23891891891891892,"directly point to the neighbor token no matter their location in the sequence. Meanwhile, sequence
150"
STRUCTURE EMBEDDING,0.24,"orders are defined by IDs and the IDs can be randomly assigned. Therefore, applying any permutation
151"
STRUCTURE EMBEDDING,0.2410810810810811,"of the input embeddings is equivalent to applying the same permutation on the output hidden states.
152"
GENERATION,0.24216216216216216,"3.3
Generation
153"
GENERATION,0.24324324324324326,"Text Generation
After converting the structure tokens into embeddings, those embeddings are
154"
GENERATION,0.24432432432432433,"fed into the unmodified Transformer Encoder-Decoder model. Conceptually, our model generates a
155"
GENERATION,0.2454054054054054,"structure token at each step which contains seven objects. However, we do not really need to generate
156"
GENERATION,0.2464864864864865,"seven objects at every step. The token ID is unique for every token and we can randomly pick any ID
157"
GENERATION,0.24756756756756756,"sequence beforehand. Notably, the structure token representation is a set, while the autoregressive
158"
GENERATION,0.24864864864864866,Causal Transformer Layer ...
GENERATION,0.24972972972972973,hidden_state1:k−1
GENERATION,0.2508108108108108,UniqueID1:k
GENERATION,0.2518918918918919,"SegmentID2:k +
..."
GENERATION,0.252972972972973,"Tail Projection
Head Projection ..."
GENERATION,0.25405405405405407,logits
GENERATION,0.25513513513513514,"multiply
softmax"
GENERATION,0.2562162162162162,"Figure 3: The Structure Predictor predicts
graph-level identifier of the k-th token by tak-
ing the hidden state of previously generated
tokens plus the segment ID of the next tokens.
The output is then multiplied with all possible
token IDs."
GENERATION,0.2572972972972973,"masked text
[text]"
GENERATION,0.2583783783783784,Transformer encoder
GENERATION,0.2594594594594595,[graph]
GENERATION,0.26054054054054054,[text]
GENERATION,0.2616216216216216,[graph] text
GENERATION,0.2627027027027027,subgraph T2T G2T T2G G2G
GENERATION,0.2637837837837838,Transformer encoder
GENERATION,0.2648648648648649,"Transformer
Decoder"
GENERATION,0.26594594594594595,"Transformer
Decoder graph"
GENERATION,0.267027027027027,"Figure 4:
Illustration of our pre-training
scheme. The input data is corrupted with
masks or subgraph sampling on the encoder
side, and the decoder would need to generate
the uncorrupted data based on a domain to-
ken."
GENERATION,0.2681081081081081,"generation manner makes the generated tokens resemble a sequence. Although the design of structure
159"
GENERATION,0.2691891891891892,"tokens enables the possibility of non-monotonic order of text generation, we slightly restrict the
160"
GENERATION,0.2702702702702703,"generation order of the structure tokens from the same graph element to be ordered and contiguous.
161"
GENERATION,0.27135135135135136,"With this restriction, we do not need to predict the token ID and previous ID. We can use the same
162"
GENERATION,0.2724324324324324,"generation scheme of other text generation Transformer model that simply generates the next text
163"
GENERATION,0.2735135135135135,"token until we are done with this element. Meanwhile, since the graph-level identifiers are the same
164"
GENERATION,0.2745945945945946,"within a graph element, we only need to predict the graph-level identifiers for the first token of labels.
165"
GENERATION,0.2756756756756757,"The generation can be further simplified for a single sentence since the graph-level identifiers are
166"
GENERATION,0.27675675675675676,"merely the token ID of the first token. Thus, text generation with our structure token approach is
167"
GENERATION,0.27783783783783783,"almost the same as other Transformer-based text generation models.
168"
GENERATION,0.2789189189189189,"Text Graph Generation
For text graph generation, the same methodology applies. We use a
169"
GENERATION,0.28,"structure predictor for predicting the identifiers. The graph-level identifiers are the same within a
170"
GENERATION,0.2810810810810811,"graph element. The prediction of graph-level identifiers can be done only one time per graph element.
171"
GENERATION,0.28216216216216217,"Moreover, the type and segment ID can also be omitted because we can tell the values once we get
172"
GENERATION,0.28324324324324324,"the head ID and tail ID. As a result, our structure predictor only needs to predict the head ID and tail
173"
GENERATION,0.2843243243243243,"ID. For predicting the IDs, we employ a single causal Transformer layer (a layer of the Transformer
174"
GENERATION,0.28540540540540543,"decoder), as illustrated in Figure 3. The causal Transformer layer takes the output of the Transformer
175"
GENERATION,0.2864864864864865,"model plus the transformed segment ID to produce a hidden vector. The hidden vector will be fed
176"
GENERATION,0.2875675675675676,"into two projection layers to get a prediction of the head ID and tail ID. To get the ID, we multiply
177"
GENERATION,0.28864864864864864,"the final hidden vectors with a list of our orthonormal-like vectors, and perform softmax on the
178"
GENERATION,0.2897297297297297,"multiplication result to get the predictions. With this setup, we can apply the same teacher forcing
179"
GENERATION,0.29081081081081084,"technique as other Transformer decoders, so the training process is also parallelized.
180"
PRE-TRAINING,0.2918918918918919,"3.4
Pre-Training
181"
PRE-TRAINING,0.292972972972973,"We introduce a pre-training method for our model based on the mBART pre-trained model for
182"
PRE-TRAINING,0.29405405405405405,"multilingual text generation [21]. The pre-training method contains two types of training objectives:
183"
PRE-TRAINING,0.2951351351351351,"the self-supervised objective and translation-like objective, as illustrated in Figure 4. The translation-
184"
PRE-TRAINING,0.29621621621621624,"like objective forces the model to generate tokens depending on the domain token. The self-supervised
185"
PRE-TRAINING,0.2972972972972973,"objective allows us to utilize more datasets without paired data (e.g. plain text datasets or sample
186"
PRE-TRAINING,0.2983783783783784,"subgraphs from a large graph database). By using both kinds of objectives, the effective training data
187"
PRE-TRAINING,0.29945945945945945,"are doubled. Meanwhile, the model is encouraged to learn a more unified representation. With these
188"
PRE-TRAINING,0.3005405405405405,"objectives, we could utilize many different datasets to improve our model.
189"
PRE-TRAINING,0.3016216216216216,Table 1: Datasets statistics.
PRE-TRAINING,0.3027027027027027,"Size
(# samples / uncompressed disk space /
# tokens in texts / # tokens in graphs)
TEKGEN
6.3 M / 1.5 GB / 218 M / 99 M
GenWiki
750 K / 1.1 GB / 27 M / 11 M
EventNarrative
180 K / 135 MB / 12 M / 4 M
WebNLG (2020)
13 K / 16 MB / 399 K / 301 K"
PRE-TRAINING,0.3037837837837838,"Table 2: Performance of Graph-to-Text on
EventNarrative."
PRE-TRAINING,0.30486486486486486,"# Params
BLEU / METEOR / BERTScore
T5-Base
220 M
12.80 / 22.77 / 89.59
T5-Large
770 M
34.31 / 26.84 / 93.02
BART-Base
140 M
31.38 / 26.68 / 93.12
GAP
153 M
35.08 / 27.50 / 93.38
TextGraphBART
75 M
33.06 / 27.17 / 94.23"
PRE-TRAINING,0.30594594594594593,"Table 3: Performance of Text-to-Graph on
WebNLG (2020). The Grapher-small∗is ob-
tained by running the officially released source
code of Grapher with T5-small weights. The #
Params of CycleGT is not disclosed [5]."
PRE-TRAINING,0.307027027027027,"# Params
F1
(Strict / Exact / Partial)
CycleGT
N/A
0.309 / 0.342 / 0.360
BT5
770 M
0.675 / 0.682 / 0.713
Grapher (Query)
770+ M
0.289 / 0.395 / 0.325
Grapher (Text)
809 M
0.681 / 0.683 / 0.713
Grapher-small∗(Text)
95 M
0.561 / 0.569 / 0.592
TextGraphBART
75 M
0.555 / 0.570 / 0.602"
EXPERIMENTS AND RESULTS,0.3081081081081081,"4
Experiments and Results
190"
EXPERIMENTS AND RESULTS,0.3091891891891892,"Datasets
We use four parallel datasets containing both text and text graph for our experiments,
191"
EXPERIMENTS AND RESULTS,0.31027027027027027,"as presented in Table 1. The model is pre-trained on TEKGEN [2] and GenWiki [14], and then
192"
EXPERIMENTS AND RESULTS,0.31135135135135134,"we fine-tune the pre-trained model on EventNarrative [8] and WebNLG (2020) [5] for evaluating
193"
EXPERIMENTS AND RESULTS,0.3124324324324324,"our model on G2T and T2G generation, respectively. The datasets are automatically generated by
194"
EXPERIMENTS AND RESULTS,0.31351351351351353,"aligning texts with existing databases, except WebNLG (2020). TEKGEN is a large-scale dataset
195"
EXPERIMENTS AND RESULTS,0.3145945945945946,"curated by aligning a subset of the Wikipedia text with Wikidata [28]. GenWiki is another large-scale
196"
EXPERIMENTS AND RESULTS,0.31567567567567567,"dataset built on Wikipedia text. The text graphs are collected from DBpedia [3]. EventNarrative is an
197"
EXPERIMENTS AND RESULTS,0.31675675675675674,"event-centric dataset that contains text graphs from the EventKG [11] and Wikidata. The text is also
198"
EXPERIMENTS AND RESULTS,0.3178378378378378,"a subset of Wikipedia text. WebNLG (2020) is crowd-sourced dataset crafted by human annotators.
199"
EXPERIMENTS AND RESULTS,0.31891891891891894,"The text graphs are collected from DBpedia, while the texts are manually written by annotators. It
200"
EXPERIMENTS AND RESULTS,0.32,"contains 16 categories in the training set and 19 categories (3 extra categories) in the test set. We
201"
EXPERIMENTS AND RESULTS,0.3210810810810811,"use the official data split for all the datasets. As for the metrics, we used BLEU [23], METEOR [4],
202"
EXPERIMENTS AND RESULTS,0.32216216216216215,"and BERTScore [31] to evaluate the G2T performance, and we use the official evaluation script of
203"
EXPERIMENTS AND RESULTS,0.3232432432432432,"WebNLG (2020) to evaluate the T2G performance.
204"
EXPERIMENTS AND RESULTS,0.32432432432432434,"Setups
Our model is trained in two phases: pre-training and fine-tuning. We initialize our model
205"
EXPERIMENTS AND RESULTS,0.3254054054054054,"from scratch and perform the pre-training method. For pre-training, We use the RAdam optimizer
206"
EXPERIMENTS AND RESULTS,0.3264864864864865,"[20] with a learning rate of 0.0001. The model is updated with an effective batch size of 256 and
207"
EXPERIMENTS AND RESULTS,0.32756756756756755,"being trained for 5 epochs on a single A100 40GB GPU. All fine-tuning experiments are done on a
208"
EXPERIMENTS AND RESULTS,0.3286486486486486,"single RTX 3090 24GB GPU. For fine-tuning on EventNarrative, we use the Lion optimizer [7] with
209"
EXPERIMENTS AND RESULTS,0.32972972972972975,"a learning rate of 0.00001. The model is updated with an effective batch size of 128 and trained for
210"
EXPERIMENTS AND RESULTS,0.3308108108108108,"20 epochs. For fine-tuning on WebNLG (2020), we use the Adam optimizer [17] with a learning rate
211"
EXPERIMENTS AND RESULTS,0.3318918918918919,"of 0.0001. The model is updated with an effective batch size of 128 and trained for 100 epochs.
212"
EXPERIMENTS AND RESULTS,0.33297297297297296,"We use an overall hidden size of 512 for our model. The unmodified Transformer encoder and decoder
213"
EXPERIMENTS AND RESULTS,0.33405405405405403,"both have 6 layers. Each attention has 16 heads, and we use a hidden size of 32 for self attentions and
214"
EXPERIMENTS AND RESULTS,0.33513513513513515,"64 for cross attentions. The feed-forward layer in Transformer has an input and output hidden size of
215"
EXPERIMENTS AND RESULTS,0.3362162162162162,"512, and the intermediate hidden size is 2048. We use these numbers for the structure predictor as
216"
EXPERIMENTS AND RESULTS,0.3372972972972973,"well. For the activation functions, we use the GELU activation function [13] for Transformers and
217"
EXPERIMENTS AND RESULTS,0.33837837837837836,"hyperbolic tangent function for the projection layers of structure predictor. During pre-training, we
218"
EXPERIMENTS AND RESULTS,0.33945945945945943,"apply the dropout [25] on the attention weights and the residual connections with a dropout rate of
219"
EXPERIMENTS AND RESULTS,0.34054054054054056,"0.1. The model weights are randomly initialized with a mean of 0 and a standard deviation of 0.02.
220"
EXPERIMENTS AND RESULTS,0.34162162162162163,"For data processing, we use the same subword tokenizer as T5 which uses the Unigram tokenization
221"
EXPERIMENTS AND RESULTS,0.3427027027027027,"method [24, 18]. The tokenizer has a vocabulary of 32100 text tokens, which contain 32000 subword
222"
EXPERIMENTS AND RESULTS,0.34378378378378377,"text tokens and 100 reserved special tokens. We use the reserved special tokens for our domain tokens.
223"
EXPERIMENTS AND RESULTS,0.34486486486486484,"Each dataset is assigned with a corresponding domain token for the graph data, while all text data
224"
EXPERIMENTS AND RESULTS,0.34594594594594597,"from different datasets share the same text domain token. The samples in each dataset are truncated
225"
EXPERIMENTS AND RESULTS,0.34702702702702704,"with a maximum length of 128 or 256 text tokens depending on the training stage. A random unique
226"
EXPERIMENTS AND RESULTS,0.3481081081081081,"ID sequence is determined for each sample at every epoch. During the pre-training, we randomly
227"
EXPERIMENTS AND RESULTS,0.3491891891891892,"Table 4: Performance of our model on each category of WebNLG (2020) test set comparing to
Grapher-small. The ∗denotes the unseen categories."
EXPERIMENTS AND RESULTS,0.35027027027027025,"Category
# Samples
TextGraphBART
Grapher-small (Text)
(train / test)
(Strict / Exact / Partial)
(Strict / Exact / Partial)
Total
13211 / 2155
0.555 / 0.570 / 0.602
0.561 / 0.569 / 0.592
Airport
1085 / 111
0.798 / 0.799 / 0.801
0.831 / 0.831 / 0.833
Artist
1222 / 129
0.636 / 0.650 / 0.666
0.696 / 0.709 / 0.727
Astronaut
529 / 102
0.797 / 0.805 / 0.809
0.847 / 0.847 / 0.848
Athlete
903 / 68
0.632 / 0.635 / 0.641
0.732 / 0.732 / 0.734
Building
972 / 53
0.811 / 0.812 / 0.817
0.889 / 0.889 / 0.890
CelestialBody
634 / 63
0.713 / 0.716 / 0.716
0.664 / 0.664 / 0.669
City
1110 / 104
0.565 / 0.580 / 0.588
0.387 / 0.390 / 0.395
ComicsCharacter
285 / 35
0.781 / 0.781 / 0.787
0.917 / 0.917 / 0.919
Company
351 / 93
0.878 / 0.878 / 0.880
0.919 / 0.919 / 0.919
Film∗
0 / 333
0.338 / 0.391 / 0.439
0.260 / 0.284 / 0.323
Food
1406 / 51
0.756 / 0.761 / 0.761
0.908 / 0.908 / 0.908
MeanOfTransportation
1132 / 65
0.623 / 0.628 / 0.647
0.585 / 0.587 / 0.590
Monument
263 / 53
0.848 / 0.848 / 0.848
0.915 / 0.915 / 0.916
MusicalWork∗
0 / 355
0.244 / 0.257 / 0.354
0.163 / 0.174 / 0.247
Politician
1194 / 34
0.805 / 0.805 / 0.807
0.810 / 0.810 / 0.811
Scientist∗
0 / 302
0.499 / 0.510 / 0.538
0.483 / 0.490 / 0.516
SportsTeam
782 / 51
0.689 / 0.691 / 0.696
0.856 / 0.856 / 0.862
University
406 / 107
0.636 / 0.640 / 0.674
0.627 / 0.629 / 0.659
WrittenWork
937 / 46
0.400 / 0.405 / 0.526
0.297 / 0.308 / 0.384"
EXPERIMENTS AND RESULTS,0.35135135135135137,"assign a unique ID sequence with a maximum value of 512. For the encoder input, we randomly drop
228"
EXPERIMENTS AND RESULTS,0.35243243243243244,"15% of graph elements or tokens depending on the domain.
229"
EXPERIMENTS AND RESULTS,0.3535135135135135,"G2T Results
We compared our model with T5 [24], BART [19], and GAP [9]. Both T5 and
230"
EXPERIMENTS AND RESULTS,0.3545945945945946,"BART are Transformer Encoder-Decoder models pre-trained on text data and fine-tuned with graph
231"
EXPERIMENTS AND RESULTS,0.35567567567567565,"linearization [8], while GAP modifies the encoder of Transformer Encoder-Decoder model with
232"
EXPERIMENTS AND RESULTS,0.3567567567567568,"graph-aware modules for extracting graph features [9]. It is noteworthy that all these models use a
233"
EXPERIMENTS AND RESULTS,0.35783783783783785,"similar Transformer decoder. The main difference among TextGraphBART and these models is the
234"
EXPERIMENTS AND RESULTS,0.3589189189189189,"way we represent and handle the text graph input.
235"
EXPERIMENTS AND RESULTS,0.36,"The result is shown in Table 2. In comparison to T5 and BART, our structure token method achieves
236"
EXPERIMENTS AND RESULTS,0.36108108108108106,"better score with fewer parameters than graph linearization approach. Meanwhile, our model is
237"
EXPERIMENTS AND RESULTS,0.3621621621621622,"comparable with GAP without modifying the Transformer model. As a conclusion, our structure
238"
EXPERIMENTS AND RESULTS,0.36324324324324325,"token representations enabled the Transformer model to capture better features from the text graph
239"
EXPERIMENTS AND RESULTS,0.3643243243243243,"than the graph linearization approach.
240"
EXPERIMENTS AND RESULTS,0.3654054054054054,"T2G Results
We compare our model with CycleGT [12, 5], BT5 [1], and Grapher [22]. BT5
241"
EXPERIMENTS AND RESULTS,0.36648648648648646,"is T5 pre-trained and fine-tuned with graph linearization. On the other hand, both CycleGT and
242"
EXPERIMENTS AND RESULTS,0.3675675675675676,"Grapher adopt the multi-stage approach. The CycleGT is a well-known multi-stage approach for
243"
EXPERIMENTS AND RESULTS,0.36864864864864866,"text-to-graph generation using cycle training [12], while Grapher performs supervised learning with
244"
EXPERIMENTS AND RESULTS,0.36972972972972973,"a special loss function [22]. Meanwhile, we use the officially released source code of Grapher1to
245"
EXPERIMENTS AND RESULTS,0.3708108108108108,"train a Grapher-small (Text) which has a similar model size (95M) with our model (75M). Both
246"
EXPERIMENTS AND RESULTS,0.37189189189189187,"Grapher-small and our TextGraphBART are trained for 100 epochs with the same learning rate and
247"
EXPERIMENTS AND RESULTS,0.372972972972973,"effective batch size.
248"
EXPERIMENTS AND RESULTS,0.37405405405405406,"The result is shown in Table 3. In comparison to CycleGT and Grapher (Query), our simple generation
249"
EXPERIMENTS AND RESULTS,0.37513513513513513,"method with structure tokens outperforms models with special training methods. Although our model
250"
EXPERIMENTS AND RESULTS,0.3762162162162162,"does not directly match the performance of the large models like BT5 or Grapher (Text), our model is
251"
EXPERIMENTS AND RESULTS,0.3772972972972973,"comparable with Grapher-small that has similar model size. Furthermore, we analyze the result by
252"
EXPERIMENTS AND RESULTS,0.3783783783783784,"measuring the performance on each category of the WebNLG (2020) test set comparing to Grapher-
253"
EXPERIMENTS AND RESULTS,0.37945945945945947,"small. The result is shown in Table 4. Even though Grapher-small is based on the T5-small pre-trained
254"
EXPERIMENTS AND RESULTS,0.38054054054054054,"model, which is trained on an extremely large dataset of 750 GB: the Colossal Clean Crawled Corpus
255"
EXPERIMENTS AND RESULTS,0.3816216216216216,1https://github.com/IBM/Grapher
EXPERIMENTS AND RESULTS,0.3827027027027027,Table 5: Ablation results of our structure embedding on WebNLG (2020) test set.
EXPERIMENTS AND RESULTS,0.3837837837837838,"F1
(Strict / Exact / Partial)
TextGraphBART
0.555 / 0.570 / 0.602
w/o segment ID
0.547 / 0.562 / 0.595
w/o type
0.544 / 0.561 / 0.594
w/o head ID & tail ID
0.489 / 0.507 / 0.539
w/o token ID & previous ID
0.365 / 0.378 / 0.404"
EXPERIMENTS AND RESULTS,0.3848648648648649,"(C4) [24], we can see that our model performs slightly better than Grapher-small on unseen categories
256"
EXPERIMENTS AND RESULTS,0.38594594594594595,"(0 samples in training set). In conclusion, our structure token approach can achieve comparable
257"
EXPERIMENTS AND RESULTS,0.387027027027027,"performance on text-to-graph generation under similar model size without using special training
258"
EXPERIMENTS AND RESULTS,0.3881081081081081,"methods or loss functions.
259"
EXPERIMENTS AND RESULTS,0.3891891891891892,"Ablation Study
To investigate the performance contribution of the components of structure tokens,
260"
EXPERIMENTS AND RESULTS,0.3902702702702703,"we conducted the ablation study on our structure embedding by fine-tuning our model with the
261"
EXPERIMENTS AND RESULTS,0.39135135135135135,"removal of some parts of the embeddings. The model is trained on the WebNLG (2020) with the
262"
EXPERIMENTS AND RESULTS,0.3924324324324324,"same setup. The results are shown in Table 5. In all ablations, the model performance was attenuated
263"
EXPERIMENTS AND RESULTS,0.3935135135135135,"as expected. First, the ablation of the token ID and previous ID removes the text order information in
264"
EXPERIMENTS AND RESULTS,0.3945945945945946,"the text labels hence the degeneration of performance. Similarly, the head ID and tail ID provide the
265"
EXPERIMENTS AND RESULTS,0.3956756756756757,"connectivities of the graph. Removal of this embedding decreases the performance, indicating the
266"
EXPERIMENTS AND RESULTS,0.39675675675675676,"importance of the connectivities. On the other hand, the ablation of type and segment ID are not as
267"
EXPERIMENTS AND RESULTS,0.3978378378378378,"detrimental as others because the type and segment ID may be inferred from other IDs. Thus, our
268"
EXPERIMENTS AND RESULTS,0.3989189189189189,"model is still able to perform albeit less performant. In conclusion, the ablation study showed that all
269"
EXPERIMENTS AND RESULTS,0.4,"of our structure embedding is important for good model performance.
270"
DISCUSSION,0.4010810810810811,"5
Discussion
271"
DISCUSSION,0.40216216216216216,"The primary objective of this work is to demonstrate the effectiveness of the proposed structure token
272"
DISCUSSION,0.40324324324324323,"approach. Due to resource constraints, there are numerous aspects remain unexplored.
273"
DISCUSSION,0.4043243243243243,"Scaling Up
We believe that TextGraphBART has the potential to achieve better results when scaled
274"
DISCUSSION,0.40540540540540543,"up. The backbone of our model is simply the transformer model, like most of the other baselines.
275"
DISCUSSION,0.4064864864864865,"Meanwhile, we observed a clear improvement on other baselines when scaling up (e.g. comparing
276"
DISCUSSION,0.40756756756756757,"T5-Base and T5-Large for G2T in 2 and Grapher-small and Grapher for T2G in 3). Therefore, we
277"
DISCUSSION,0.40864864864864864,"anticipate that transformers on T2G and G2T follow the scaling laws.
278"
DISCUSSION,0.4097297297297297,"Model Architecture
While we use The Transformer Encoder-Decoder model in our experiments,
279"
DISCUSSION,0.41081081081081083,"there are no strict restrictions on the model architecture as long as there are dot product attention
280"
DISCUSSION,0.4118918918918919,"operations. The same approach can be applied to encoder or decoder-only model.
281"
DISCUSSION,0.412972972972973,"Data and Objective for Pre-Training
In the experiments, we only use parallel datasets for pre-
282"
DISCUSSION,0.41405405405405404,"training. Since our pre-training scheme contains a T2T path, it’s possible to pre-train the model
283"
DISCUSSION,0.4151351351351351,"merely with plain-text datasets. On the other hand, we can also incorporate programming language
284"
DISCUSSION,0.41621621621621624,"datasets and use syntax parsers to generate the Abstract Syntax Tree (AST) as the text graph for
285"
DISCUSSION,0.4172972972972973,"pre-training. However, it is unclear how much the self-supervised objective and translation-like
286"
DISCUSSION,0.4183783783783784,"objective affect the downstream performances.
287"
CONCLUSIONS,0.41945945945945945,"6
Conclusions
288"
CONCLUSIONS,0.4205405405405405,"We present a novel approach to the problem of text graph generation leveraging the strength of
289"
CONCLUSIONS,0.42162162162162165,"Transformer models. Our exploration has led to an effective method for structured data representation
290"
CONCLUSIONS,0.4227027027027027,"and generation via structure tokens. In the structure token, we use several identifiers to indicate the
291"
CONCLUSIONS,0.4237837837837838,"connectivities of the graphs and the order of the texts. Then an embedding method for structure
292"
CONCLUSIONS,0.42486486486486486,"token is proposed, allowing the Transformer model to utilize the structural information. We show
293"
CONCLUSIONS,0.4259459459459459,"that the structure token approach can be used to represent and generate both texts and text graphs.
294"
CONCLUSIONS,0.42702702702702705,"The experiment results demonstrated the effectiveness of our method with less data and parameters.
295"
CONCLUSIONS,0.4281081081081081,"Meanwhile, the ablation study further confirmed the importance of various elements of the structure
296"
CONCLUSIONS,0.4291891891891892,"tokens.
297"
REFERENCES,0.43027027027027026,"References
298"
REFERENCES,0.43135135135135133,"[1] O. Agarwal, M. Kale, H. Ge, S. Shakeri, and R. Al-Rfou. Machine translation aided bilin-
299"
REFERENCES,0.43243243243243246,"gual data-to-text generation and semantic parsing. In Proceedings of the 3rd International
300"
REFERENCES,0.4335135135135135,"Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 125–
301"
REFERENCES,0.4345945945945946,"130, Dublin, Ireland (Virtual), 12 2020. Association for Computational Linguistics. URL
302"
REFERENCES,0.43567567567567567,"https://aclanthology.org/2020.webnlg-1.13.
303"
REFERENCES,0.43675675675675674,"[2] O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou.
Knowledge graph based synthetic cor-
304"
REFERENCES,0.43783783783783786,"pus generation for knowledge-enhanced language model pre-training.
In Proceedings of
305"
REFERENCES,0.43891891891891893,"the 2021 Conference of the North American Chapter of the Association for Computational
306"
REFERENCES,0.44,"Linguistics: Human Language Technologies, pages 3554–3565, Online, June 2021. As-
307"
REFERENCES,0.4410810810810811,"sociation for Computational Linguistics.
doi: 10.18653/v1/2021.naacl-main.278.
URL
308"
REFERENCES,0.44216216216216214,"https://aclanthology.org/2021.naacl-main.278.
309"
REFERENCES,0.44324324324324327,"[3] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives. Dbpedia: A nucleus for
310"
REFERENCES,0.44432432432432434,"a web of open data. In Proceedings of the 6th International The Semantic Web and 2nd Asian
311"
REFERENCES,0.4454054054054054,"Conference on Asian Semantic Web Conference, ISWC’07/ASWC’07, page 722–735, Berlin,
312"
REFERENCES,0.4464864864864865,"Heidelberg, 2007. Springer-Verlag. ISBN 3540762973.
313"
REFERENCES,0.44756756756756755,"[4] S. Banerjee and A. Lavie. METEOR: An automatic metric for MT evaluation with improved
314"
REFERENCES,0.4486486486486487,"correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and
315"
REFERENCES,0.44972972972972974,"Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72,
316"
REFERENCES,0.4508108108108108,"Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. URL https:
317"
REFERENCES,0.4518918918918919,"//aclanthology.org/W05-0909.
318"
REFERENCES,0.45297297297297295,"[5] T. Castro Ferreira, C. Gardent, N. Ilinykh, C. van der Lee, S. Mille, D. Moussallem, and
319"
REFERENCES,0.4540540540540541,"A. Shimorina. The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and
320"
REFERENCES,0.45513513513513515,"evaluation results (WebNLG+ 2020). In Proceedings of the 3rd International Workshop on
321"
REFERENCES,0.4562162162162162,"Natural Language Generation from the Semantic Web (WebNLG+), pages 55–76, Dublin, Ireland
322"
REFERENCES,0.4572972972972973,"(Virtual), 12 2020. Association for Computational Linguistics. URL https://aclanthology.
323"
REFERENCES,0.45837837837837836,"org/2020.webnlg-1.7.
324"
REFERENCES,0.4594594594594595,"[6] W. Chen, Y. Su, X. Yan, and W. Y. Wang. KGPT: Knowledge-grounded pre-training for
325"
REFERENCES,0.46054054054054056,"data-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in
326"
REFERENCES,0.4616216216216216,"Natural Language Processing (EMNLP), pages 8635–8648, Online, Nov. 2020. Association
327"
REFERENCES,0.4627027027027027,"for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.697. URL https://
328"
REFERENCES,0.46378378378378377,"aclanthology.org/2020.emnlp-main.697.
329"
REFERENCES,0.4648648648648649,"[7] X. Chen, C. Liang, D. Huang, E. Real, K. Wang, Y. Liu, H. Pham, X. Dong, T. Luong,
330"
REFERENCES,0.46594594594594596,"C.-J. Hsieh, Y. Lu, and Q. V. Le. Symbolic discovery of optimization algorithms. ArXiv,
331"
REFERENCES,0.46702702702702703,"abs/2302.06675, 2023.
332"
REFERENCES,0.4681081081081081,"[8] A. Colas, A. Sadeghian, Y. Wang, and D. Z. Wang.
Eventnarrative:
A large-
333"
REFERENCES,0.46918918918918917,"scale
event-centric
dataset
for
knowledge
graph-to-text
generation.
In
J.
Van-
334"
REFERENCES,0.4702702702702703,"schoren and S. Yeung,
editors,
Proceedings of the Neural Information Process-
335"
REFERENCES,0.47135135135135137,"ing Systems Track on Datasets and Benchmarks, volume 1. Curran, 2021.
URL
336"
REFERENCES,0.47243243243243244,"https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/
337"
REFERENCES,0.4735135135135135,"2021/file/a3f390d88e4c41f2747bfa2f1b5f87db-Paper-round1.pdf.
338"
REFERENCES,0.4745945945945946,"[9] A. Colas, M. Alvandipour, and D. Z. Wang. Gap: A graph-aware language model framework for
339"
REFERENCES,0.4756756756756757,"knowledge graph-to-text generation. In International Conference on Computational Linguistics,
340"
REFERENCES,0.47675675675675677,"2022.
341"
REFERENCES,0.47783783783783784,"[10] P. Dognin, I. Padhi, I. Melnyk, and P. Das. ReGen: Reinforcement learning for text and
342"
REFERENCES,0.4789189189189189,"knowledge base generation using pretrained language models. In Proceedings of the 2021 Con-
343"
REFERENCES,0.48,"ference on Empirical Methods in Natural Language Processing, pages 1084–1099, Online and
344"
REFERENCES,0.4810810810810811,"Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi:
345"
REFERENCES,0.4821621621621622,"10.18653/v1/2021.emnlp-main.83. URL https://aclanthology.org/2021.emnlp-main.
346"
REFERENCES,0.48324324324324325,"83.
347"
REFERENCES,0.4843243243243243,"[11] S. Gottschalk and E. Demidova. Eventkg: A multilingual event-centric temporal knowledge
348"
REFERENCES,0.4854054054054054,"graph. In Extended Semantic Web Conference, 2018.
349"
REFERENCES,0.4864864864864865,"[12] Q. Guo, Z. Jin, X. Qiu, W. Zhang, D. Wipf, and Z. Zhang. CycleGT: Unsupervised graph-to-
350"
REFERENCES,0.4875675675675676,"text and text-to-graph generation via cycle training. In Proceedings of the 3rd International
351"
REFERENCES,0.48864864864864865,"Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 77–
352"
REFERENCES,0.4897297297297297,"88, Dublin, Ireland (Virtual), 12 2020. Association for Computational Linguistics. URL
353"
REFERENCES,0.4908108108108108,"https://aclanthology.org/2020.webnlg-1.8.
354"
REFERENCES,0.4918918918918919,"[13] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016.
355"
REFERENCES,0.492972972972973,"[14] Z. Jin, Q. Guo, X. Qiu, and Z. Zhang. GenWiki: A dataset of 1.3 million content-sharing text and
356"
REFERENCES,0.49405405405405406,"graphs for unsupervised graph-to-text generation. In Proceedings of the 28th International Con-
357"
REFERENCES,0.49513513513513513,"ference on Computational Linguistics, pages 2398–2409, Barcelona, Spain (Online), Dec. 2020.
358"
REFERENCES,0.4962162162162162,"International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.217.
359"
REFERENCES,0.4972972972972973,"URL https://aclanthology.org/2020.coling-main.217.
360"
REFERENCES,0.4983783783783784,"[15] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat, F. B. Viégas,
361"
REFERENCES,0.49945945945945946,"M. Wattenberg, G. S. Corrado, M. Hughes, and J. Dean. Google’s multilingual neural ma-
362"
REFERENCES,0.5005405405405405,"chine translation system: Enabling zero-shot translation. Transactions of the Association for
363"
REFERENCES,0.5016216216216216,"Computational Linguistics, 5:339–351, 2016.
364"
REFERENCES,0.5027027027027027,"[16] J. Kim, D. Nguyen, S. Min, S. Cho, M. Lee, H. Lee, and S. Hong. Pure transformers are
365"
REFERENCES,0.5037837837837837,"powerful graph learners. Advances in Neural Information Processing Systems, 35:14582–14595,
366"
REFERENCES,0.5048648648648648,"2022.
367"
REFERENCES,0.505945945945946,"[17] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,
368"
REFERENCES,0.5070270270270271,"2014.
369"
REFERENCES,0.5081081081081081,"[18] T. Kudo. Subword regularization: Improving neural network translation models with multi-
370"
REFERENCES,0.5091891891891892,"ple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for
371"
REFERENCES,0.5102702702702703,"Computational Linguistics (Volume 1: Long Papers), pages 66–75, Melbourne, Australia,
372"
REFERENCES,0.5113513513513513,"July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1007. URL
373"
REFERENCES,0.5124324324324324,"https://aclanthology.org/P18-1007.
374"
REFERENCES,0.5135135135135135,"[19] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and
375"
REFERENCES,0.5145945945945946,"L. Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language
376"
REFERENCES,0.5156756756756756,"generation, translation, and comprehension.
In Proceedings of the 58th Annual Meeting
377"
REFERENCES,0.5167567567567568,"of the Association for Computational Linguistics, pages 7871–7880, Online, July 2020.
378"
REFERENCES,0.5178378378378379,"Association for Computational Linguistics.
doi: 10.18653/v1/2020.acl-main.703.
URL
379"
REFERENCES,0.518918918918919,"https://aclanthology.org/2020.acl-main.703.
380"
REFERENCES,0.52,"[20] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han. On the variance of the adaptive
381"
REFERENCES,0.5210810810810811,"learning rate and beyond.
In 8th International Conference on Learning Representations,
382"
REFERENCES,0.5221621621621622,"ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:
383"
REFERENCES,0.5232432432432432,"//openreview.net/forum?id=rkgz2aEKDr.
384"
REFERENCES,0.5243243243243243,"[21] Y. Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad, M. Lewis, and L. Zettlemoyer.
385"
REFERENCES,0.5254054054054054,"Multilingual denoising pre-training for neural machine translation. Transactions of the Asso-
386"
REFERENCES,0.5264864864864864,"ciation for Computational Linguistics, 8:726–742, 2020. doi: 10.1162/tacl_a_00343. URL
387"
REFERENCES,0.5275675675675676,"https://aclanthology.org/2020.tacl-1.47.
388"
REFERENCES,0.5286486486486487,"[22] I. Melnyk, P. Dognin, and P. Das.
Knowledge graph generation from text.
In Findings
389"
REFERENCES,0.5297297297297298,"of the Association for Computational Linguistics: EMNLP 2022, pages 1610–1622, Abu
390"
REFERENCES,0.5308108108108108,"Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. URL
391"
REFERENCES,0.5318918918918919,"https://aclanthology.org/2022.findings-emnlp.116.
392"
REFERENCES,0.532972972972973,"[23] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evalua-
393"
REFERENCES,0.534054054054054,"tion of machine translation.
In Proceedings of the 40th Annual Meeting of the Associa-
394"
REFERENCES,0.5351351351351351,"tion for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July
395"
REFERENCES,0.5362162162162162,"2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL
396"
REFERENCES,0.5372972972972972,"https://aclanthology.org/P02-1040.
397"
REFERENCES,0.5383783783783784,"[24] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.
398"
REFERENCES,0.5394594594594595,"Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal
399"
REFERENCES,0.5405405405405406,"of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr.org/papers/v21/
400"
REFERENCES,0.5416216216216216,"20-074.html.
401"
REFERENCES,0.5427027027027027,"[25] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple
402"
REFERENCES,0.5437837837837838,"way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15
403"
REFERENCES,0.5448648648648649,"(56):1929–1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html.
404"
REFERENCES,0.5459459459459459,"[26] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
405"
REFERENCES,0.547027027027027,"I. Polosukhin. Attention is all you need. In NIPS, 2017.
406"
REFERENCES,0.5481081081081081,"[27] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio. Graph attention
407"
REFERENCES,0.5491891891891892,"networks. 6th International Conference on Learning Representations, 2017.
408"
REFERENCES,0.5502702702702703,"[28] D. Vrandeˇci´c and M. Krötzsch. Wikidata: A free collaborative knowledgebase. Commun.
409"
REFERENCES,0.5513513513513514,"ACM, 57(10):78–85, sep 2014. ISSN 0001-0782. doi: 10.1145/2629489. URL https:
410"
REFERENCES,0.5524324324324325,"//doi.org/10.1145/2629489.
411"
REFERENCES,0.5535135135135135,"[29] Y. Xu, S. Sheng, J. Qi, L. Fu, Z. Lin, X. Wang, and C. Zhou. Unsupervised graph-text mutual
412"
REFERENCES,0.5545945945945946,"conversion with a unified pretrained language model. In Proceedings of the 61st Annual
413"
REFERENCES,0.5556756756756757,"Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
414"
REFERENCES,0.5567567567567567,"5130–5144, Toronto, Canada, July 2023. Association for Computational Linguistics. doi:
415"
REFERENCES,0.5578378378378378,"10.18653/v1/2023.acl-long.281. URL https://aclanthology.org/2023.acl-long.281.
416"
REFERENCES,0.5589189189189189,"[30] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu. Do transformers really
417"
REFERENCES,0.56,"perform bad for graph representation? In Neural Information Processing Systems, 2021.
418"
REFERENCES,0.5610810810810811,"[31] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. Bertscore: Evaluating text
419"
REFERENCES,0.5621621621621622,"generation with BERT. In 8th International Conference on Learning Representations, ICLR
420"
REFERENCES,0.5632432432432433,"2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://
421"
REFERENCES,0.5643243243243243,"openreview.net/forum?id=SkeHuCVFDr.
422"
REFERENCES,0.5654054054054054,"A
Formal Definitions
423"
REFERENCES,0.5664864864864865,"A.1
Preliminaries
424"
REFERENCES,0.5675675675675675,"Let T be the set of all possible text tokens. A text label is defined as a set of pairs containing the text
425"
REFERENCES,0.5686486486486486,"token and positions. We generalize this definition by replacing the position with a contiguous sequence
426"
REFERENCES,0.5697297297297297,"of unique ID. Given an infinite unique ID sequence: I = (idi)∞
i=0 where idi ∈Z+∧idi ̸= idj if i ̸= j.
427"
REFERENCES,0.5708108108108109,"Each idi is a positive integer. We also define id(i) = idi for simplicity. By picking a corresponding
428"
REFERENCES,0.5718918918918919,"ID sequence, we can use any positive integer sequence as the positions. Then a text label S of
429"
REFERENCES,0.572972972972973,"length l is a set of token-ID pairs defined as: S = {(ti, idj+i) | 1 ≤i ≤l, ti ∈T } ⊆T × Z+.
430"
REFERENCES,0.5740540540540541,"We can conditionally specify the start point j ∈N with Sj. Let S be the set of all possible text
431"
REFERENCES,0.5751351351351351,"labels and Sj for a specific start point. With the ID sequence I, the positions (p)l
p=1 can be
432"
REFERENCES,0.5762162162162162,"replaced with (idj+k)l
k=1. Then we can union text labels without missing information by picking
433"
REFERENCES,0.5772972972972973,"non-overlapped ID sequences, which is a desired property for the attention operation. A text
434"
REFERENCES,0.5783783783783784,"graph G = (N, A) is composed of a node set N with q nodes and an arc set A with r arcs.
435"
REFERENCES,0.5794594594594594,"The node set N is a set of node labels paired with unique IDs of the nodes, defined as: N =
436"
REFERENCES,0.5805405405405405,"{(Ni, ni) | 1 ≤i ≤q, ni ∈Z+, Ni ∈S} ⊆S × Z+ where Ni is the node label and ni is the
437"
REFERENCES,0.5816216216216217,"corresponding node ID. Similarly, an edge set E is a set of edge labels paired with unique IDs,
438"
REFERENCES,0.5827027027027027,"defined as: E = {(Ei, ei) | 1 ≤i ≤r, ei ∈Z+, Ei ∈S} ⊆S × Z+ where Ei is the edge label
439"
REFERENCES,0.5837837837837838,"and ei is the edge ID. Notably, the ID used in N and E are disjoint. Then the arc set is defined as:
440"
REFERENCES,0.5848648648648649,"A = {(Nh
i , Ei, Nt
i) | 1 ≤i ≤r, Nh
i , Nt
i ∈N, Ei ∈E} ⊆N × E × N where Ei is the edge and
441"
REFERENCES,0.585945945945946,"Nh
i , Nt
i is the head node and tail node, respectively.
442"
REFERENCES,0.587027027027027,"A.2
Structure Tokens
443"
REFERENCES,0.5881081081081081,"With the setup in A.1, the formal definition of the structure token representation is defined as a set
444"
REFERENCES,0.5891891891891892,"of septuples (tuples with 7 elements). Given a text graph G = (N, A) and its components: node
445"
REFERENCES,0.5902702702702702,"Ni = (Ni, ni) ∈N, 1 ≤i ≤|N|, arc Aj = ((N h
j , nh
j ), (Ej, ej), (N t
j, nt
j)) ∈A, 1 ≤j ≤|A|,
446"
REFERENCES,0.5913513513513513,"edge Ej ∈E = {(Ej, ej) | 1 ≤j ≤|A|}, and the length of the text labels lN
i
= |Ni|, lE
j = |Ej|,
447"
REFERENCES,0.5924324324324325,"we define two sequences:
448"
REFERENCES,0.5935135135135136,"LN = (LN
k )|N |
k=1 where LN
k ="
REFERENCES,0.5945945945945946,"(
0,
if k = 1"
REFERENCES,0.5956756756756757,"l N
k−1 + LN
k−1, if k ̸= 1"
REFERENCES,0.5967567567567568,"LE = (LE
k )|A|
k=1 where LE
k ="
REFERENCES,0.5978378378378378,"( P|N |
k=1 l N
k ,
if k = 1"
REFERENCES,0.5989189189189189,"l E
k−1 + LE
k−1, if k ̸= 1"
REFERENCES,0.6,"We assign ni = id(LN
i + 1), ej = id(LE
j + 1) and specify the start point of each text label such that
449"
REFERENCES,0.601081081081081,"Ni ∈SLN
i , Ej ∈SLE
j . By doing so, we can get the node (or edge) ID from the token IDs of its text
450"
REFERENCES,0.6021621621621621,"label. For each node Ni and edge Ej, we define the corresponding structure token representation
451"
REFERENCES,0.6032432432432432,"XN
i
and XE
j as:
452"
REFERENCES,0.6043243243243244,"XN
i
= {(tk, 1, uidk, uidk−1, ni, ni, ni) |"
REFERENCES,0.6054054054054054,"1 ≤k ≤l N
i , (tk, uidk) ∈Ni, uid0 = id0}"
REFERENCES,0.6064864864864865,"XE
j = {(tk, 0, uidk, uidk−1, ej, nh
j , nt
j) |"
REFERENCES,0.6075675675675676,"1 ≤k ≤l E
j , (tk, uidk) ∈Ej, uid0 = id0} (1)"
REFERENCES,0.6086486486486486,"Then the corresponding structure token representation G′ of the text graph G is defined as:
453 G′ = |N|
["
REFERENCES,0.6097297297297297,"k=1
XN
k ∪ |A|
["
REFERENCES,0.6108108108108108,"k=1
XE
k ∪XD"
REFERENCES,0.6118918918918919,"⊆T × {0, 1} × Z+ × Z+ × Z+ × Z+ × Z+
(2)"
REFERENCES,0.6129729729729729,"where tD ∈T , id0 is the ID of domain token and XD = {(tD, 1, id0, id0, id0, id0, id0)} is the
454"
REFERENCES,0.614054054054054,"domain token. Each septuple X ∈G′ is a structure token containing the label, type, token ID,
455"
REFERENCES,0.6151351351351352,"previous ID, segment ID, head ID, and tail ID. With this definition, we can represent every possible
456"
REFERENCES,0.6162162162162163,"token ID assignment by specifying the unique ID sequence I. On the other hand, since the graph
457"
REFERENCES,0.6172972972972973,"element can be randomly permuted, every possible ordering is also representable with our set G′ by
458"
REFERENCES,0.6183783783783784,"picking the corresponding unique ID sequence.
459"
REFERENCES,0.6194594594594595,"A.3
Structure Embedding
460"
REFERENCES,0.6205405405405405,"To convert the structure tokens into embeddings, we use 3 kinds of transform functions. For label
461"
REFERENCES,0.6216216216216216,"and type, we use the one-hot encoding, denoted as OneHotn : A →En where A is a set with n
462"
REFERENCES,0.6227027027027027,"elements and En is the standard basis of Rn. On the other hand, each ID would first be converted
463"
REFERENCES,0.6237837837837837,"into a d-dimensional orthonormal-like vector with a function Orthd. To get the orthonormal-like
464"
REFERENCES,0.6248648648648648,"vectors, we modify and normalize the sinusoidal position encoding of Transformer [26] with different
465"
REFERENCES,0.625945945945946,"frequencies. The d-dimensional sinusoidal position encoding PEd at position i is defined as:
466"
REFERENCES,0.6270270270270271,PEd(i) = d/2 ∥
REFERENCES,0.6281081081081081,"k=1
pe(i, k)
∈Rd"
REFERENCES,0.6291891891891892,"pe(i, k) = sin(
i
10000k/d )∥cos(
i
10000k/d )
∈R2
(3)"
REFERENCES,0.6302702702702703,"where ∥denotes the vector concatenation. We generalize the definition of PEd with a frequency
467"
REFERENCES,0.6313513513513513,"function f:
468"
REFERENCES,0.6324324324324324,"PE∗
d{f}(i) = d/2 ∥"
REFERENCES,0.6335135135135135,"k=1
pe∗{f}(i, k)
∈Rd"
REFERENCES,0.6345945945945946,"pe∗{f}(i, k) = sin(i ∗f(k))∥cos(i ∗f(k))
∈R2
(4)"
REFERENCES,0.6356756756756756,"Then by taking f ′(k) = 10000−k/d, the original PEd can be defined with PEd = PE∗
d{f ′}. We
469"
REFERENCES,0.6367567567567568,"use this generalized position encoding to define the function Orthd : Z+ →Rd of the IDs for our
470"
REFERENCES,0.6378378378378379,"Figure 5: Histogram of cos-similarities of 1024 vectors with different frequency functions. The
orange line is our Orth, while the blue line is the normalized sinusoidal position embedding. A
dimensionality of 512 is used in this figure."
REFERENCES,0.6389189189189189,"orthonormal-like vectors as:
471"
REFERENCES,0.64,"Orthd = norm2 ◦PE∗
d{−log(k)}
(5)"
REFERENCES,0.6410810810810811,"where norm2 is the L2 normalization and ◦denote the function composition. We find that by picking
472"
REFERENCES,0.6421621621621622,"the frequency function −log(k), the generated vectors satisfied the desired properties. In Figure 5,
473"
REFERENCES,0.6432432432432432,"we generate 1024 vectors by applying Orth on 1 ≤i ≤1024 and compute the cosine similarity of
474"
REFERENCES,0.6443243243243243,"every possible pair. We can see that the similarity values are mostly close to zero.
475"
REFERENCES,0.6454054054054054,"Once we can convert the IDs into orthonormal-like vectors, we use Orthd directly as the transform
476"
REFERENCES,0.6464864864864864,"functions for the graph-level identifiers. For text-level identifiers, we add the vector of token ID and
477"
REFERENCES,0.6475675675675676,"previous ID together. Given two non-domain token a and b with token ID ta, tb and previous ID
478"
REFERENCES,0.6486486486486487,"pa, pb, we have Orth(ta) · Orth(tb) ≈0 since token ID is unique and Orth is orthonormal-like.
479"
REFERENCES,0.6497297297297298,"Meanwhile, we have pa ̸= pb for most tokens. The dot product value of a and b become:
480"
REFERENCES,0.6508108108108108,"(Orth(ta) + Orth(pa)) · (Orth(tb) + Orth(pb))
= Orth(ta) · Orth(tb) + Orth(ta) · Orth(pb) + Orth(pa) · Orth(tb) + Orth(pa) · Orth(pb)
= Orth(ta) · Orth(pb) + Orth(pa) · Orth(tb) + Orth(pa) · Orth(pb) + ε ="
REFERENCES,0.6518918918918919,"



"
REFERENCES,0.652972972972973,"


"
REFERENCES,0.654054054054054,"Orth(ta) · Orth(pb) + ε = 1 + ε,
if a is the previous token of b
Orth(pa) · Orth(tb) + ε = 1 + ε,
if b is the previous token of a
Orth(id0) · Orth(id0) + ε = 1 + ε,
if both a and b are the first token of text label
ε,
otherwise (6)"
REFERENCES,0.6551351351351351,"where ε is a small value around 0. The value will only be meaningful in the dot product if one token
481"
REFERENCES,0.6562162162162162,"is the previous token of the other. The case of first tokens is set to allow exchanging information with
482"
REFERENCES,0.6572972972972972,"the domain token. This can be suppressed by making Orth(id0) = 0.
483"
REFERENCES,0.6583783783783784,"With these designs, we define our structure token vectorize function V ec as:
484"
REFERENCES,0.6594594594594595,"V ec(X) =
OneHot|T |(π1(X)) ∥OneHot2(π2(X)) ∥(Orthn(π3(X)) + Orthnπ4(X)))"
REFERENCES,0.6605405405405406,"∥Orthn(π5(X)) ∥Orthn(π6(X)) ∥Orthn(π7(X)) ∈R4n+|T |+2
(7)"
REFERENCES,0.6616216216216216,"where πi(X) denote the i-th element of the septuple X. The vectorized result will be fed into a train-
485"
REFERENCES,0.6627027027027027,"able projection layer Emb: R4n+|T |+2 →Rd to get the structure embedding with d dimensions.
486"
REFERENCES,0.6637837837837838,"NeurIPS Paper Checklist
487"
CLAIMS,0.6648648648648648,"1. Claims
488"
CLAIMS,0.6659459459459459,"Question: Do the main claims made in the abstract and introduction accurately reflect the
489"
CLAIMS,0.667027027027027,"paper’s contributions and scope?
490"
CLAIMS,0.6681081081081081,"Answer: [Yes]
491"
CLAIMS,0.6691891891891892,"Justification: The proposed methods and results are described in the Method Section (Section
492"
CLAIMS,0.6702702702702703,"3) and the Experiments and Results Section (Section 4).
493"
CLAIMS,0.6713513513513514,"Guidelines:
494"
CLAIMS,0.6724324324324324,"• The answer NA means that the abstract and introduction do not include the claims
495"
CLAIMS,0.6735135135135135,"made in the paper.
496"
CLAIMS,0.6745945945945946,"• The abstract and/or introduction should clearly state the claims made, including the
497"
CLAIMS,0.6756756756756757,"contributions made in the paper and important assumptions and limitations. A No or
498"
CLAIMS,0.6767567567567567,"NA answer to this question will not be perceived well by the reviewers.
499"
CLAIMS,0.6778378378378378,"• The claims made should match theoretical and experimental results, and reflect how
500"
CLAIMS,0.6789189189189189,"much the results can be expected to generalize to other settings.
501"
CLAIMS,0.68,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
502"
CLAIMS,0.6810810810810811,"are not attained by the paper.
503"
LIMITATIONS,0.6821621621621622,"2. Limitations
504"
LIMITATIONS,0.6832432432432433,"Question: Does the paper discuss the limitations of the work performed by the authors?
505"
LIMITATIONS,0.6843243243243243,"Answer: [Yes]
506"
LIMITATIONS,0.6854054054054054,"Justification: The limitations are discussed in the Discussion Section (Section 5)
507"
LIMITATIONS,0.6864864864864865,"Guidelines:
508"
LIMITATIONS,0.6875675675675675,"• The answer NA means that the paper has no limitation while the answer No means that
509"
LIMITATIONS,0.6886486486486486,"the paper has limitations, but those are not discussed in the paper.
510"
LIMITATIONS,0.6897297297297297,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
511"
LIMITATIONS,0.6908108108108109,"• The paper should point out any strong assumptions and how robust the results are to
512"
LIMITATIONS,0.6918918918918919,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
513"
LIMITATIONS,0.692972972972973,"model well-specification, asymptotic approximations only holding locally). The authors
514"
LIMITATIONS,0.6940540540540541,"should reflect on how these assumptions might be violated in practice and what the
515"
LIMITATIONS,0.6951351351351351,"implications would be.
516"
LIMITATIONS,0.6962162162162162,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
517"
LIMITATIONS,0.6972972972972973,"only tested on a few datasets or with a few runs. In general, empirical results often
518"
LIMITATIONS,0.6983783783783784,"depend on implicit assumptions, which should be articulated.
519"
LIMITATIONS,0.6994594594594594,"• The authors should reflect on the factors that influence the performance of the approach.
520"
LIMITATIONS,0.7005405405405405,"For example, a facial recognition algorithm may perform poorly when image resolution
521"
LIMITATIONS,0.7016216216216217,"is low or images are taken in low lighting. Or a speech-to-text system might not be
522"
LIMITATIONS,0.7027027027027027,"used reliably to provide closed captions for online lectures because it fails to handle
523"
LIMITATIONS,0.7037837837837838,"technical jargon.
524"
LIMITATIONS,0.7048648648648649,"• The authors should discuss the computational efficiency of the proposed algorithms
525"
LIMITATIONS,0.705945945945946,"and how they scale with dataset size.
526"
LIMITATIONS,0.707027027027027,"• If applicable, the authors should discuss possible limitations of their approach to
527"
LIMITATIONS,0.7081081081081081,"address problems of privacy and fairness.
528"
LIMITATIONS,0.7091891891891892,"• While the authors might fear that complete honesty about limitations might be used by
529"
LIMITATIONS,0.7102702702702702,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
530"
LIMITATIONS,0.7113513513513513,"limitations that aren’t acknowledged in the paper. The authors should use their best
531"
LIMITATIONS,0.7124324324324325,"judgment and recognize that individual actions in favor of transparency play an impor-
532"
LIMITATIONS,0.7135135135135136,"tant role in developing norms that preserve the integrity of the community. Reviewers
533"
LIMITATIONS,0.7145945945945946,"will be specifically instructed to not penalize honesty concerning limitations.
534"
THEORY ASSUMPTIONS AND PROOFS,0.7156756756756757,"3. Theory Assumptions and Proofs
535"
THEORY ASSUMPTIONS AND PROOFS,0.7167567567567568,"Question: For each theoretical result, does the paper provide the full set of assumptions and
536"
THEORY ASSUMPTIONS AND PROOFS,0.7178378378378378,"a complete (and correct) proof?
537"
THEORY ASSUMPTIONS AND PROOFS,0.7189189189189189,"Answer: [NA]
538"
THEORY ASSUMPTIONS AND PROOFS,0.72,"Justification: This work does not introduce any new theoretical results
539"
THEORY ASSUMPTIONS AND PROOFS,0.721081081081081,"Guidelines:
540"
THEORY ASSUMPTIONS AND PROOFS,0.7221621621621621,"• The answer NA means that the paper does not include theoretical results.
541"
THEORY ASSUMPTIONS AND PROOFS,0.7232432432432433,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
542"
THEORY ASSUMPTIONS AND PROOFS,0.7243243243243244,"referenced.
543"
THEORY ASSUMPTIONS AND PROOFS,0.7254054054054054,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
544"
THEORY ASSUMPTIONS AND PROOFS,0.7264864864864865,"• The proofs can either appear in the main paper or the supplemental material, but if
545"
THEORY ASSUMPTIONS AND PROOFS,0.7275675675675676,"they appear in the supplemental material, the authors are encouraged to provide a short
546"
THEORY ASSUMPTIONS AND PROOFS,0.7286486486486486,"proof sketch to provide intuition.
547"
THEORY ASSUMPTIONS AND PROOFS,0.7297297297297297,"• Inversely, any informal proof provided in the core of the paper should be complemented
548"
THEORY ASSUMPTIONS AND PROOFS,0.7308108108108108,"by formal proofs provided in appendix or supplemental material.
549"
THEORY ASSUMPTIONS AND PROOFS,0.7318918918918919,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7329729729729729,"4. Experimental Result Reproducibility
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7340540540540541,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7351351351351352,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7362162162162162,"of the paper (regardless of whether the code and data are provided or not)?
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7372972972972973,"Answer: [Yes]
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7383783783783784,"Justification: The experiment setups are described in the Experiments and Results Section
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7394594594594595,"(Section 4) and code is also provided.
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7405405405405405,"Guidelines:
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7416216216216216,"• The answer NA means that the paper does not include experiments.
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7427027027027027,"• If the paper includes experiments, a No answer to this question will not be perceived
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7437837837837837,"well by the reviewers: Making the paper reproducible is important, regardless of
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7448648648648649,"whether the code and data are provided or not.
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.745945945945946,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7470270270270271,"to make their results reproducible or verifiable.
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7481081081081081,"• Depending on the contribution, reproducibility can be accomplished in various ways.
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7491891891891892,"For example, if the contribution is a novel architecture, describing the architecture fully
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7502702702702703,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7513513513513513,"be necessary to either make it possible for others to replicate the model with the same
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7524324324324324,"dataset, or provide access to the model. In general. releasing code and data is often
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7535135135135135,"one good way to accomplish this, but reproducibility can also be provided via detailed
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7545945945945945,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7556756756756757,"of a large language model), releasing of a model checkpoint, or other means that are
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7567567567567568,"appropriate to the research performed.
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7578378378378379,"• While NeurIPS does not require releasing code, the conference does require all submis-
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7589189189189189,"sions to provide some reasonable avenue for reproducibility, which may depend on the
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.76,"nature of the contribution. For example
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7610810810810811,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7621621621621621,"to reproduce that algorithm.
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7632432432432432,"(b) If the contribution is primarily a new model architecture, the paper should describe
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7643243243243243,"the architecture clearly and fully.
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7654054054054054,"(c) If the contribution is a new model (e.g., a large language model), then there should
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7664864864864865,"either be a way to access this model for reproducing the results or a way to reproduce
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7675675675675676,"the model (e.g., with an open-source dataset or instructions for how to construct
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7686486486486487,"the dataset).
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7697297297297298,"(d) We recognize that reproducibility may be tricky in some cases, in which case
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7708108108108108,"authors are welcome to describe the particular way they provide for reproducibility.
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7718918918918919,"In the case of closed-source models, it may be that access to the model is limited in
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.772972972972973,"some way (e.g., to registered users), but it should be possible for other researchers
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.774054054054054,"to have some path to reproducing or verifying the results.
589"
OPEN ACCESS TO DATA AND CODE,0.7751351351351351,"5. Open access to data and code
590"
OPEN ACCESS TO DATA AND CODE,0.7762162162162162,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
591"
OPEN ACCESS TO DATA AND CODE,0.7772972972972974,"tions to faithfully reproduce the main experimental results, as described in supplemental
592"
OPEN ACCESS TO DATA AND CODE,0.7783783783783784,"material?
593"
OPEN ACCESS TO DATA AND CODE,0.7794594594594595,"Answer: [Yes]
594"
OPEN ACCESS TO DATA AND CODE,0.7805405405405406,"Justification: code is provided
595"
OPEN ACCESS TO DATA AND CODE,0.7816216216216216,"Guidelines:
596"
OPEN ACCESS TO DATA AND CODE,0.7827027027027027,"• The answer NA means that paper does not include experiments requiring code.
597"
OPEN ACCESS TO DATA AND CODE,0.7837837837837838,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
598"
OPEN ACCESS TO DATA AND CODE,0.7848648648648648,"public/guides/CodeSubmissionPolicy) for more details.
599"
OPEN ACCESS TO DATA AND CODE,0.7859459459459459,"• While we encourage the release of code and data, we understand that this might not be
600"
OPEN ACCESS TO DATA AND CODE,0.787027027027027,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
601"
OPEN ACCESS TO DATA AND CODE,0.7881081081081082,"including code, unless this is central to the contribution (e.g., for a new open-source
602"
OPEN ACCESS TO DATA AND CODE,0.7891891891891892,"benchmark).
603"
OPEN ACCESS TO DATA AND CODE,0.7902702702702703,"• The instructions should contain the exact command and environment needed to run to
604"
OPEN ACCESS TO DATA AND CODE,0.7913513513513514,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
605"
OPEN ACCESS TO DATA AND CODE,0.7924324324324324,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
606"
OPEN ACCESS TO DATA AND CODE,0.7935135135135135,"• The authors should provide instructions on data access and preparation, including how
607"
OPEN ACCESS TO DATA AND CODE,0.7945945945945946,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
608"
OPEN ACCESS TO DATA AND CODE,0.7956756756756757,"• The authors should provide scripts to reproduce all experimental results for the new
609"
OPEN ACCESS TO DATA AND CODE,0.7967567567567567,"proposed method and baselines. If only a subset of experiments are reproducible, they
610"
OPEN ACCESS TO DATA AND CODE,0.7978378378378378,"should state which ones are omitted from the script and why.
611"
OPEN ACCESS TO DATA AND CODE,0.798918918918919,"• At submission time, to preserve anonymity, the authors should release anonymized
612"
OPEN ACCESS TO DATA AND CODE,0.8,"versions (if applicable).
613"
OPEN ACCESS TO DATA AND CODE,0.8010810810810811,"• Providing as much information as possible in supplemental material (appended to the
614"
OPEN ACCESS TO DATA AND CODE,0.8021621621621622,"paper) is recommended, but including URLs to data and code is permitted.
615"
OPEN ACCESS TO DATA AND CODE,0.8032432432432433,"6. Experimental Setting/Details
616"
OPEN ACCESS TO DATA AND CODE,0.8043243243243243,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
617"
OPEN ACCESS TO DATA AND CODE,0.8054054054054054,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
618"
OPEN ACCESS TO DATA AND CODE,0.8064864864864865,"results?
619"
OPEN ACCESS TO DATA AND CODE,0.8075675675675675,"Answer: [Yes]
620"
OPEN ACCESS TO DATA AND CODE,0.8086486486486486,"Justification: The experiment setups are described in the Experiments and Results Section
621"
OPEN ACCESS TO DATA AND CODE,0.8097297297297297,"(Section 4)
622"
OPEN ACCESS TO DATA AND CODE,0.8108108108108109,"Guidelines:
623"
OPEN ACCESS TO DATA AND CODE,0.8118918918918919,"• The answer NA means that the paper does not include experiments.
624"
OPEN ACCESS TO DATA AND CODE,0.812972972972973,"• The experimental setting should be presented in the core of the paper to a level of detail
625"
OPEN ACCESS TO DATA AND CODE,0.8140540540540541,"that is necessary to appreciate the results and make sense of them.
626"
OPEN ACCESS TO DATA AND CODE,0.8151351351351351,"• The full details can be provided either with the code, in appendix, or as supplemental
627"
OPEN ACCESS TO DATA AND CODE,0.8162162162162162,"material.
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8172972972972973,"7. Experiment Statistical Significance
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8183783783783783,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8194594594594594,"information about the statistical significance of the experiments?
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8205405405405405,"Answer: [No]
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8216216216216217,"Justification: Due to resource constraints, we are unable to perform significance tests.
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8227027027027027,"Guidelines:
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8237837837837838,"• The answer NA means that the paper does not include experiments.
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8248648648648649,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.825945945945946,"dence intervals, or statistical significance tests, at least for the experiments that support
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.827027027027027,"the main claims of the paper.
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8281081081081081,"• The factors of variability that the error bars are capturing should be clearly stated (for
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8291891891891892,"example, train/test split, initialization, random drawing of some parameter, or overall
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8302702702702702,"run with given experimental conditions).
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8313513513513513,"• The method for calculating the error bars should be explained (closed form formula,
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8324324324324325,"call to a library function, bootstrap, etc.)
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8335135135135135,"• The assumptions made should be given (e.g., Normally distributed errors).
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8345945945945946,"• It should be clear whether the error bar is the standard deviation or the standard error
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8356756756756757,"of the mean.
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8367567567567568,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8378378378378378,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8389189189189189,"of Normality of errors is not verified.
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.84,"• For asymmetric distributions, the authors should be careful not to show in tables or
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.841081081081081,"figures symmetric error bars that would yield results that are out of range (e.g. negative
651"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8421621621621621,"error rates).
652"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8432432432432433,"• If error bars are reported in tables or plots, The authors should explain in the text how
653"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8443243243243244,"they were calculated and reference the corresponding figures or tables in the text.
654"
EXPERIMENTS COMPUTE RESOURCES,0.8454054054054054,"8. Experiments Compute Resources
655"
EXPERIMENTS COMPUTE RESOURCES,0.8464864864864865,"Question: For each experiment, does the paper provide sufficient information on the com-
656"
EXPERIMENTS COMPUTE RESOURCES,0.8475675675675676,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
657"
EXPERIMENTS COMPUTE RESOURCES,0.8486486486486486,"the experiments?
658"
EXPERIMENTS COMPUTE RESOURCES,0.8497297297297297,"Answer: [Yes]
659"
EXPERIMENTS COMPUTE RESOURCES,0.8508108108108108,"Justification: The experiment setups are described in the Experiments and Results Section
660"
EXPERIMENTS COMPUTE RESOURCES,0.8518918918918919,"(Section 4)
661"
EXPERIMENTS COMPUTE RESOURCES,0.8529729729729729,"Guidelines:
662"
EXPERIMENTS COMPUTE RESOURCES,0.8540540540540541,"• The answer NA means that the paper does not include experiments.
663"
EXPERIMENTS COMPUTE RESOURCES,0.8551351351351352,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
664"
EXPERIMENTS COMPUTE RESOURCES,0.8562162162162162,"or cloud provider, including relevant memory and storage.
665"
EXPERIMENTS COMPUTE RESOURCES,0.8572972972972973,"• The paper should provide the amount of compute required for each of the individual
666"
EXPERIMENTS COMPUTE RESOURCES,0.8583783783783784,"experimental runs as well as estimate the total compute.
667"
EXPERIMENTS COMPUTE RESOURCES,0.8594594594594595,"• The paper should disclose whether the full research project required more compute
668"
EXPERIMENTS COMPUTE RESOURCES,0.8605405405405405,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
669"
EXPERIMENTS COMPUTE RESOURCES,0.8616216216216216,"didn’t make it into the paper).
670"
CODE OF ETHICS,0.8627027027027027,"9. Code Of Ethics
671"
CODE OF ETHICS,0.8637837837837837,"Question: Does the research conducted in the paper conform, in every respect, with the
672"
CODE OF ETHICS,0.8648648648648649,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
673"
CODE OF ETHICS,0.865945945945946,"Answer: [Yes]
674"
CODE OF ETHICS,0.867027027027027,"Justification: The content does not violate NeurIPS Code of Ethics
675"
CODE OF ETHICS,0.8681081081081081,"Guidelines:
676"
CODE OF ETHICS,0.8691891891891892,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
677"
CODE OF ETHICS,0.8702702702702703,"• If the authors answer No, they should explain the special circumstances that require a
678"
CODE OF ETHICS,0.8713513513513513,"deviation from the Code of Ethics.
679"
CODE OF ETHICS,0.8724324324324324,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
680"
CODE OF ETHICS,0.8735135135135135,"eration due to laws or regulations in their jurisdiction).
681"
BROADER IMPACTS,0.8745945945945945,"10. Broader Impacts
682"
BROADER IMPACTS,0.8756756756756757,"Question: Does the paper discuss both potential positive societal impacts and negative
683"
BROADER IMPACTS,0.8767567567567568,"societal impacts of the work performed?
684"
BROADER IMPACTS,0.8778378378378379,"Answer: [NA]
685"
BROADER IMPACTS,0.8789189189189189,"Justification: The main result of this work is a new encoding method, which has no societal
686"
BROADER IMPACTS,0.88,"impact
687"
BROADER IMPACTS,0.8810810810810811,"Guidelines:
688"
BROADER IMPACTS,0.8821621621621621,"• The answer NA means that there is no societal impact of the work performed.
689"
BROADER IMPACTS,0.8832432432432432,"• If the authors answer NA or No, they should explain why their work has no societal
690"
BROADER IMPACTS,0.8843243243243243,"impact or why the paper does not address societal impact.
691"
BROADER IMPACTS,0.8854054054054054,"• Examples of negative societal impacts include potential malicious or unintended uses
692"
BROADER IMPACTS,0.8864864864864865,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
693"
BROADER IMPACTS,0.8875675675675676,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
694"
BROADER IMPACTS,0.8886486486486487,"groups), privacy considerations, and security considerations.
695"
BROADER IMPACTS,0.8897297297297297,"• The conference expects that many papers will be foundational research and not tied
696"
BROADER IMPACTS,0.8908108108108108,"to particular applications, let alone deployments. However, if there is a direct path to
697"
BROADER IMPACTS,0.8918918918918919,"any negative applications, the authors should point it out. For example, it is legitimate
698"
BROADER IMPACTS,0.892972972972973,"to point out that an improvement in the quality of generative models could be used to
699"
BROADER IMPACTS,0.894054054054054,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
700"
BROADER IMPACTS,0.8951351351351351,"that a generic algorithm for optimizing neural networks could enable people to train
701"
BROADER IMPACTS,0.8962162162162162,"models that generate Deepfakes faster.
702"
BROADER IMPACTS,0.8972972972972973,"• The authors should consider possible harms that could arise when the technology is
703"
BROADER IMPACTS,0.8983783783783784,"being used as intended and functioning correctly, harms that could arise when the
704"
BROADER IMPACTS,0.8994594594594595,"technology is being used as intended but gives incorrect results, and harms following
705"
BROADER IMPACTS,0.9005405405405406,"from (intentional or unintentional) misuse of the technology.
706"
BROADER IMPACTS,0.9016216216216216,"• If there are negative societal impacts, the authors could also discuss possible mitigation
707"
BROADER IMPACTS,0.9027027027027027,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
708"
BROADER IMPACTS,0.9037837837837838,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
709"
BROADER IMPACTS,0.9048648648648648,"feedback over time, improving the efficiency and accessibility of ML).
710"
SAFEGUARDS,0.9059459459459459,"11. Safeguards
711"
SAFEGUARDS,0.907027027027027,"Question: Does the paper describe safeguards that have been put in place for responsible
712"
SAFEGUARDS,0.9081081081081082,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
713"
SAFEGUARDS,0.9091891891891892,"image generators, or scraped datasets)?
714"
SAFEGUARDS,0.9102702702702703,"Answer: [NA]
715"
SAFEGUARDS,0.9113513513513514,"Justification: All datasets used are publicly available open datasets.
716"
SAFEGUARDS,0.9124324324324324,"Guidelines:
717"
SAFEGUARDS,0.9135135135135135,"• The answer NA means that the paper poses no such risks.
718"
SAFEGUARDS,0.9145945945945946,"• Released models that have a high risk for misuse or dual-use should be released with
719"
SAFEGUARDS,0.9156756756756756,"necessary safeguards to allow for controlled use of the model, for example by requiring
720"
SAFEGUARDS,0.9167567567567567,"that users adhere to usage guidelines or restrictions to access the model or implementing
721"
SAFEGUARDS,0.9178378378378378,"safety filters.
722"
SAFEGUARDS,0.918918918918919,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
723"
SAFEGUARDS,0.92,"should describe how they avoided releasing unsafe images.
724"
SAFEGUARDS,0.9210810810810811,"• We recognize that providing effective safeguards is challenging, and many papers do
725"
SAFEGUARDS,0.9221621621621622,"not require this, but we encourage authors to take this into account and make a best
726"
SAFEGUARDS,0.9232432432432433,"faith effort.
727"
LICENSES FOR EXISTING ASSETS,0.9243243243243243,"12. Licenses for existing assets
728"
LICENSES FOR EXISTING ASSETS,0.9254054054054054,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
729"
LICENSES FOR EXISTING ASSETS,0.9264864864864865,"the paper, properly credited and are the license and terms of use explicitly mentioned and
730"
LICENSES FOR EXISTING ASSETS,0.9275675675675675,"properly respected?
731"
LICENSES FOR EXISTING ASSETS,0.9286486486486486,"Answer: [Yes]
732"
LICENSES FOR EXISTING ASSETS,0.9297297297297298,"Justification: All dataset and code used are publicly available and cited.
733"
LICENSES FOR EXISTING ASSETS,0.9308108108108109,"Guidelines:
734"
LICENSES FOR EXISTING ASSETS,0.9318918918918919,"• The answer NA means that the paper does not use existing assets.
735"
LICENSES FOR EXISTING ASSETS,0.932972972972973,"• The authors should cite the original paper that produced the code package or dataset.
736"
LICENSES FOR EXISTING ASSETS,0.9340540540540541,"• The authors should state which version of the asset is used and, if possible, include a
737"
LICENSES FOR EXISTING ASSETS,0.9351351351351351,"URL.
738"
LICENSES FOR EXISTING ASSETS,0.9362162162162162,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
739"
LICENSES FOR EXISTING ASSETS,0.9372972972972973,"• For scraped data from a particular source (e.g., website), the copyright and terms of
740"
LICENSES FOR EXISTING ASSETS,0.9383783783783783,"service of that source should be provided.
741"
LICENSES FOR EXISTING ASSETS,0.9394594594594594,"• If assets are released, the license, copyright information, and terms of use in the
742"
LICENSES FOR EXISTING ASSETS,0.9405405405405406,"package should be provided. For popular datasets, paperswithcode.com/datasets
743"
LICENSES FOR EXISTING ASSETS,0.9416216216216217,"has curated licenses for some datasets. Their licensing guide can help determine the
744"
LICENSES FOR EXISTING ASSETS,0.9427027027027027,"license of a dataset.
745"
LICENSES FOR EXISTING ASSETS,0.9437837837837838,"• For existing datasets that are re-packaged, both the original license and the license of
746"
LICENSES FOR EXISTING ASSETS,0.9448648648648649,"the derived asset (if it has changed) should be provided.
747"
LICENSES FOR EXISTING ASSETS,0.9459459459459459,"• If this information is not available online, the authors are encouraged to reach out to
748"
LICENSES FOR EXISTING ASSETS,0.947027027027027,"the asset’s creators.
749"
NEW ASSETS,0.9481081081081081,"13. New Assets
750"
NEW ASSETS,0.9491891891891892,"Question: Are new assets introduced in the paper well documented and is the documentation
751"
NEW ASSETS,0.9502702702702702,"provided alongside the assets?
752"
NEW ASSETS,0.9513513513513514,"Answer: [NA]
753"
NEW ASSETS,0.9524324324324325,"Justification: This work does not release new assets
754"
NEW ASSETS,0.9535135135135135,"Guidelines:
755"
NEW ASSETS,0.9545945945945946,"• The answer NA means that the paper does not release new assets.
756"
NEW ASSETS,0.9556756756756757,"• Researchers should communicate the details of the dataset/code/model as part of their
757"
NEW ASSETS,0.9567567567567568,"submissions via structured templates. This includes details about training, license,
758"
NEW ASSETS,0.9578378378378378,"limitations, etc.
759"
NEW ASSETS,0.9589189189189189,"• The paper should discuss whether and how consent was obtained from people whose
760"
NEW ASSETS,0.96,"asset is used.
761"
NEW ASSETS,0.961081081081081,"• At submission time, remember to anonymize your assets (if applicable). You can either
762"
NEW ASSETS,0.9621621621621622,"create an anonymized URL or include an anonymized zip file.
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9632432432432433,"14. Crowdsourcing and Research with Human Subjects
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9643243243243244,"Question: For crowdsourcing experiments and research with human subjects, does the paper
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9654054054054054,"include the full text of instructions given to participants and screenshots, if applicable, as
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9664864864864865,"well as details about compensation (if any)?
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9675675675675676,"Answer: [NA]
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9686486486486486,"Justification: This work does not involve crowdsourcing nor research with human subjects.
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9697297297297297,"Guidelines:
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708108108108108,"• The answer NA means that the paper does not involve crowdsourcing nor research with
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9718918918918918,"human subjects.
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972972972972973,"• Including this information in the supplemental material is fine, but if the main contribu-
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9740540540540541,"tion of the paper involves human subjects, then as much detail as possible should be
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751351351351352,"included in the main paper.
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9762162162162162,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772972972972973,"or other labor should be paid at least the minimum wage in the country of the data
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783783783783784,"collector.
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9794594594594594,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805405405405405,"Subjects
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816216216216216,"Question: Does the paper describe potential risks incurred by study participants, whether
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9827027027027027,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837837837837838,"approvals (or an equivalent approval/review based on the requirements of your country or
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848648648648649,"institution) were obtained?
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985945945945946,"Answer: [NA]
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987027027027027,"Justification: This work does not involve crowdsourcing nor research with human subjects.
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881081081081081,"Guidelines:
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891891891891892,"• The answer NA means that the paper does not involve crowdsourcing nor research with
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902702702702703,"human subjects.
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913513513513513,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924324324324324,"may be required for any human subjects research. If you obtained IRB approval, you
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935135135135135,"should clearly state this in the paper.
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945945945945946,"• We recognize that the procedures for this may vary significantly between institutions
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956756756756757,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967567567567568,"guidelines for their institution.
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978378378378379,"• For initial submissions, do not include any information that would break anonymity (if
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989189189189189,"applicable), such as the institution conducting the review.
797"
