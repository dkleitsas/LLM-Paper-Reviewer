Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0035842293906810036,"Dequantisation is a general technique used for transforming data described by a
discrete random variable x into a continuous (latent) random variable z, for the pur-
pose of it being modeled by likelihood-based density models. Dequantisation was
ﬁrst introduced in the context of ordinal data, such as image pixel values. However,
when the data is categorical, the dequantisation scheme is not obvious. We learn
such a dequantisation scheme q(z|x), using variational inference with TRUncated
FLows (TRUFL) — a novel ﬂow-based model that allows the dequantiser to have
a learnable truncated support. Unlike previous work, the TRUFL dequantiser is
(i) capable of embedding the data losslessly in certain cases, since the truncation
allows the conditional distributions q(z|x) to have non-overlapping bounded sup-
ports, while being (ii) trainable with back-propagation. Addtionally, since the
support of the marginal q(z) is bounded and the support of prior p(z) is not, we
propose to renormalise the prior distribution over the support of q(z). We derive
a lower bound for training, and propose a rejection sampling scheme to account
for the invalid samples. Experimentally, we benchmark TRUFL on constrained
generation tasks, and ﬁnd that it outperforms prior approaches. In addition, we ﬁnd
that rejection sampling results in higher validity for the constrained problems."
INTRODUCTION,0.007168458781362007,"1
INTRODUCTION"
INTRODUCTION,0.010752688172043012,"Deep generative models aim to model a distribution of high-dimensional natural data. Many of these
methods assume that the data is continuous, despite it being digitally stored in bits and therefore
intrinsically discrete. This discrepancy has led to recent interest in dequantising discrete data types
to avoid some of the degeneracies of ﬁtting continuous models to discrete data (Theis et al., 2015).
When data is ordinal (such as pixel intensities) a naive dequantisation scheme can be obtained by
adding uniform noise to the discrete values (Theis et al., 2015). More recently, a generalisation of
this approach where dequantisation is seen as inference in a latent variable model has also been
proposed (Ho et al., 2019; Hoogeboom et al., 2020; Nielsen et al., 2020). However, these methods
may not be directly applied in cases where the data is categorical (Hoogeboom et al., 2021), because
the data is not naturally represented in a vector space."
INTRODUCTION,0.014336917562724014,"Attempts at devising dequantisation schemes for categorical data by building upon the variational
dequantisation scheme have been recently proposed in Hoogeboom et al. (2021) and Lippe &
Gavves (2020). These approaches dequantise a categorical input into a latent continuous space.
Ideally, a dequantisation scheme for categorical data should be: (i) easily learnable by standard
optimization techniques and (ii) possibly lossless, in the sense that quantisation should recover the
input category. Argmax Flow (Hoogeboom et al., 2021) offer lossless dequantisation but the support
of the stochastic embedding is chosen arbitrarily and not optimised, and the dimensionality of the
continuous (dequantised) variable is required to be at least logarithmic in the number of categories
of the input data. Moreover, the method makes minimal assumptions about the topology of the
categorical data, disregarding the possible relationships between categories, which can occur for
example between word indices in natural language (Bengio et al., 2003) or the atomic representations"
INTRODUCTION,0.017921146953405017,Published as a conference paper at ICLR 2022
INTRODUCTION,0.021505376344086023,q(zt|xt)
INTRODUCTION,0.025089605734767026,p(xt|zt)
INTRODUCTION,0.02867383512544803,"CatNF
Argmax ﬂow (thresh.)
Argmax ﬂow (binary)
TRUFL"
INTRODUCTION,0.03225806451612903,"Figure 1: Decoder for CNFs (left) is the posterior of the encoder and the posteriors have unbounded
support resulting in probability mass in all categories in p(xt|zt). Argmax ﬂow (center) have bounded
supports that partitions the full latent space into regions where the posterior for each category is
deterministic, but (1) the naive thresholding (thresh.) requires the dimensionality of the space to be
equal to the number of categories, and (2) the binary encoding partitions data into separate octants,
sometimes leaving octants unsupported. TRUFL (right) allows bounded support for each category,
and for latent dimensions to be different from the category size."
INTRODUCTION,0.035842293906810034,"of a molecule’s constituents. On the other hand, Categorical Normalizing Flows (CatNF; Lippe &
Gavves 2020) can learn a more compact representation of the input category but the dequantisation
might be lossy given that the posteriors over the continuous variables have overlapping support."
INTRODUCTION,0.03942652329749104,"Is there a trade-off between these two schemes? In this paper, we propose TRUFL, which builds
upon the aforementioned variational dequantisation techniques. We achieve that by using truncated
posterior distributions over the continuous variables with potentially bounded and disjoint support. In
addition, we present a parametrisation of truncated distributions that can be optimised with standard
stochastic reparametrisation techniques. Overall, our method inherits strengths of both CatNF and
Argmax ﬂow. Our experimental results highlight the effectiveness of our approach."
INTRODUCTION,0.043010752688172046,"2
BACKGROUND: VARIATIONAL DEQUANTISATION"
INTRODUCTION,0.04659498207885305,"Dequantisation refers to the process of embedding discrete-valued data into a continuous space, which
allows us to employ density-based models to capture the distribution of the continuous representation.
Concretely, let z = {z1, . . . , zT } denote this continuous representation, and x = {x1, . . . , xT }
describe the observed data, where each xt represent, e.g. a node in a graph or a token in a sentence.
Each xt is assumed to be categorical, i.e. xt ∈{0, · · · , K −1} for some integer K > 1. z can be
interpreted as a latent variable, which follows a prior distribution p(z). We refer to q(zt|xt) as the
dequantiser and p(xt|zt) as the quantiser. Training can be achieved by maximizing a variational
lower bound on the marginal likelihood of the data, i.e.:"
INTRODUCTION,0.05017921146953405,log p(x) ≥Eq(z|x)
INTRODUCTION,0.053763440860215055,"
log p(x|z) p(z)"
INTRODUCTION,0.05734767025089606,q(z|x)
INTRODUCTION,0.06093189964157706,"
=: L(x)
(1)"
INTRODUCTION,0.06451612903225806,"We are interested in the case where the representation zt can be inferred from xt alone, so we choose
the factorisation p(x|z) = Q"
INTRODUCTION,0.06810035842293907,t p(xt|zt) and q(z|x) = Q
INTRODUCTION,0.07168458781362007,"t q(zt|xt), following Lippe & Gavves
(2020). In this case, the “optimal” quantiser p(xt|zt) can be conveniently computed as:"
INTRODUCTION,0.07526881720430108,"arg max
p(xt|zt)
Eq(x)[L(x)] =
q(zt|xt) ˜p(xt)
PK−1
x′
t=0 q(zt|x′
t) ˜p(x′
t)
= q(xt|zt) =: p(xt|zt)
(2)"
INTRODUCTION,0.07885304659498207,"where q(x) denotes the (empirical) data distribution, and ˜p(xt) denotes the estimate of the marginal
distribution of each category (which can be obtained by counting and, in the case of textual data, this
corresponds to the unigram distribution over words). This equation shows that the optimal quantiser
can be obtained implicitly by applying Bayes’ rule with the parametric dequantiser q(zt|xt). The
factorisation we chose for p(x|z) and q(z|x) is crucial for the arg max above to be represented in this"
INTRODUCTION,0.08243727598566308,Published as a conference paper at ICLR 2022
INTRODUCTION,0.08602150537634409,"simple form. Without this assumption, the solution will involve a combinatorial sum or an integral,
which results in the choice of a parametric quantiser in Ziegler & Rush (2019) for computational
tractability. Plugging the optimal decoder into Eq. 1 yields:"
INTRODUCTION,0.08960573476702509,"L(x) = Eq(z|x) ""X"
INTRODUCTION,0.0931899641577061,"t
log ˜p(xt) + log
p(z)
PK−1
x′
t=0 q(zt|x′
t)˜p(x′
t) # (3)"
INTRODUCTION,0.0967741935483871,"We note that the ﬁrst term is a constant. Therefore, the expression above implies that accurately
modelling the dependencies in x boils down to learning an expressive prior p(z) and regularising the
dequantiser q(zt|xt). q(xt|zt) is deterministic when q(zt|xt) does not overlap with other q(zt|x′
t),
in which case q(zt|xt) is encouraged to be expanded to maximize the entropy. If there is certain
amount of overlapping, the denominator in the second term will push down the density of other
q(zt|x′
t), therefore resulting in a spikier aggregate posterior distribution (see more discussion on this
in Section 5.3). With this general framework that also accounts for lossy quantisation, we brieﬂy
present some of the previously proposed strategies for dequantisation."
INTRODUCTION,0.1003584229390681,"Ordinal dequantisation
In the case where the data is ordinal, such as the case of image pixel
values (e.g. for an 8-bit representation, K = 256), a dequantisation scheme can be obtained by
setting q(zt|xt) = Uniform(xt, xt + 1). The resulting quantisation process is simply ⌊zt⌋, and is
deterministic. More generally, q(zt|xt) can be any distribution on [xt, xt +1]. See Nielsen & Winther
(2020); Hoogeboom et al. (2019) for extensions of the uniform dequantisation scheme."
INTRODUCTION,0.1039426523297491,"Argmax Flow
For categorical data, uniform dequantisation is not applicable, as there is no intrinsic
ordering between the categories. Argmax Flow (Hoogeboom et al., 2021) dequantise categorical
data by letting zt ∈RK be distributed by q(zt|xt) with support {zt : arg maxk(zt)k = xt}. When
the support over the latent space is disjoint, p(xt|zt) = q(xt|zt) = 1[xt = arg maxk(zt)k]1; we
depict this in Figure 1, Argmax ﬂow (thresh.). Argmax Flow makes minimal assumptions on the
topology of the data: the support of the dequantiser partitions the continuous space evenly and the
representations are equally far from each other. As an example, synonyms in text may still have
very distinct dequantised representations despite having similar functions and meaning in a language
modelling setting. In the naive formulation, Argmax Flow requires the dimensionality to the latent
space to be the same as the number of the input categories K. To accomodate for larger categorical
spaces, the authors suggest a binary factorisation, reducing the required latent space dimension to
⌈log2 K⌉; See Figure 1, Argmax ﬂow (binary)."
INTRODUCTION,0.10752688172043011,"Categorical Normalising Flows (CatNF)
In the previous cases, the quantisation is deterministic,
and there is no loss of information. This is because in a cleanly partitioned latent space like in the
ordinal setting or argmax ﬂow, the dequantising distributions q(zt|xt = k) for all 0 ≤k ≤K −1
have non-overlapping support. CatNF learns a dequantiser that can “softly” partition the space. Lippe
& Gavves (2020) propose using a conditional logistic distribution as q(zt|xt). In this case, the optimal
quantiser q(xt|zt) is nearly-deterministic if the locations of the dequantisers are far away from each
other and they have sufﬁciently small scale. For this reason, and unlike the ﬁrst two approaches,
CatNF is not capable of losslessly dequantising the data (we provide a formal discussion on this in
Appendix A.2, which is based on an data-processing inequality argument, using the dequantiser as a
transitional kernel). It can approximate the lossless limit by pushing the bulk of the mass of q(zt|xt)
away from each other, but that could potentially lead to a highly complex and multi-modal empirical
distribution over the representation space for p(z) to approximate."
INTRODUCTION,0.1111111111111111,"Next, we consider the case where q(zt|xt) is a truncated distribution, and as such has the ability to
encode the data losslessly while learning a meaningful latent topology of the different categories."
TRUNCATED DEQUANTISER AND PRIOR,0.11469534050179211,"3
TRUNCATED DEQUANTISER AND PRIOR"
TRUNCATED DEQUANTISER AND PRIOR,0.11827956989247312,"The general approach to optimising the variational lower bound proposed in Kingma & Welling
(2014) involves sampling from the proposal distribution q(zt|xt) to estimate the expectation (Eq. 1).
In our case, we want to parameterise this using a TRUncated FLow, which we will refer to as TRUFL."
TRUNCATED DEQUANTISER AND PRIOR,0.12186379928315412,1We use 1[·] to denote an indicator function.
TRUNCATED DEQUANTISER AND PRIOR,0.12544802867383512,Published as a conference paper at ICLR 2022
TRUNCATED DEQUANTISER AND PRIOR,0.12903225806451613,"For simplicity, we will drop the dependency on t and xt, but all of the variational distribution is
conditioned on the categorical value xt."
TRUNCATED DEQUANTISER AND PRIOR,0.13261648745519714,"If we want to bound a scalar distribution between (a, b), and we have a density function f where its
cumulative distribution function F (CDF) and the inverse of its CDF F −1 are tractable, we can easily
sample from f by sampling u from Uniform(F(a), F(b)), and then evaluating F −1(u). Note that
this method is differentiable, and we use it to sample from our dequantiser."
TRUNCATED DEQUANTISER AND PRIOR,0.13620071684587814,"However, multi-variate distributions may not simply be truncated at the tails, but rather have a
support which is a strict subset of its base distribution. One general approach to sampling from
such a distribution is via rejection sampling (Murphy, 2012). This approach has been used in prior
work for sampling from bounded-support distributions (Polykovskiy & Vetrov, 2020; Xu & Durrett,
2018; Davidson et al., 2018). Computing gradients for this method is possible via implicit gradients
(Figurnov et al., 2018), but we do not need gradients in our case, as we use rejection sampling for
generating samples from the generative model (See Section 3.2)."
TRUNCATED LOGISTIC DISTRIBUTION,0.13978494623655913,"3.1
TRUNCATED LOGISTIC DISTRIBUTION"
TRUNCATED LOGISTIC DISTRIBUTION,0.14336917562724014,"We choose the truncated logistic distribution to parameterise the dequantiser, because the density, the
CDF and the inverse CDF of the logistic distribution can all be easily computed:"
TRUNCATED LOGISTIC DISTRIBUTION,0.14695340501792115,"f(z) = σ(z) · σ(1 −z),
F(z) = σ(z),
F −1(u) = log
u
1 −u,
(4)"
TRUNCATED LOGISTIC DISTRIBUTION,0.15053763440860216,"for z ∈R and u ∈(0, 1), where σ is the logistic sigmoid function. Additionally, we can parameterise
the width of Uniform(F(a), F(b)) directly by s, where 0 < s ≤1, and the mean of this distribution
as m, such that m + s"
TRUNCATED LOGISTIC DISTRIBUTION,0.15412186379928317,2 < 1 and m −s
TRUNCATED LOGISTIC DISTRIBUTION,0.15770609318996415,"2 > 0. Given a set of statistical parameters ˆs ∈R and ˆm ∈R
for each category, we can impose these constraints by the following reparameterisation:"
TRUNCATED LOGISTIC DISTRIBUTION,0.16129032258064516,"m = σ( ˆm),
s = 2 · min(m, 1 −m) · σ(ˆs).
(5)"
TRUNCATED LOGISTIC DISTRIBUTION,0.16487455197132617,"Then, to sample from Uniform(F(a), F(b)),"
TRUNCATED LOGISTIC DISTRIBUTION,0.16845878136200718,"u = m +

u0 −1 2"
TRUNCATED LOGISTIC DISTRIBUTION,0.17204301075268819,"
· s,
u0 ∼Uniform(0, 1).
(6)"
TRUNCATED LOGISTIC DISTRIBUTION,0.17562724014336917,"This allows for a simple implementation for a truncated logistic distribution that is differentiable (see
Appendix A.1 for the derivation of the gradient). We can write the pdf as"
TRUNCATED LOGISTIC DISTRIBUTION,0.17921146953405018,"˜f(z; m, s) ="
TRUNCATED LOGISTIC DISTRIBUTION,0.1827956989247312,"(
1
sσ(z) · σ(1 −z),
if 0 < F (z)−m s
+ 1"
TRUNCATED LOGISTIC DISTRIBUTION,0.1863799283154122,"2 ≤1
0,
otherwise.
(7)"
TRUNCATED LOGISTIC DISTRIBUTION,0.18996415770609318,"Like in Lippe & Gavves (2020), the resulting approximate posterior q(zt|xt) requires category
speciﬁc parameters for the truncation (m(xt) and s(xt)) and any ﬂow applied on top of the truncated
logistic (we denote this ﬂow as g). During training, conditioned on a given category xt, we can
sample zt with the method described above, and compute the probability q(zt|xt). Since decoding
requires computing g−1(zt, ˆxt) for all K categories, it limits the choice of ﬂows applicable. In CatNF
implementations, a linear ﬂow is used (Kingma & Dhariwal, 2018), while empirically the authors ﬁnd
that a category conditional scale and shift sufﬁces. Algorithm 1 details the steps taken for computing
zt, log q(zt|xt), and log p(xt|zt)."
TRUNCATED LOGISTIC DISTRIBUTION,0.1935483870967742,"With this parameterisation of the decoder and the truncated approximate posterior, some q(zt|ˆxt)
where xt ̸= ˆxt may not have support over the sample zt. In the extreme case, when q(zt|xt)
for all possible assignments of xt had mutually exclusive support, then decoding will always be
deterministic, similar to Argmax ﬂow. This is when TRUFL has the ﬂexibility to embed discrete
data losslessly in a continuous space. Empirically we ﬁnd that q(xt|zt) is sparse, but not always
deterministic."
TRUNCATED LOGISTIC DISTRIBUTION,0.1971326164874552,"Recovering Argmax Flow and CatNF
Speciﬁc choices of m and s in Eq. 6 can recover CatNF
and Argmax Flow support for each category. By setting m = 0.5 and s = 1, we perform no truncation
on the posterior distribution, where we recover CatNF. For Argmax Flow, we can achieve orthant
support by setting m to 0.25 or 0.75 for each dimension according to the orthant it was assigned
based on the binary encoding, and s = 0.25. This will ensure that the support covers only half of the
real line in that dimension, thus matching Argmax Flow support for that category."
TRUNCATED LOGISTIC DISTRIBUTION,0.2007168458781362,Published as a conference paper at ICLR 2022
TRUNCATED LOGISTIC DISTRIBUTION,0.20430107526881722,Algorithm 1 Truncated Categorical Encoding for a timestep t
TRUNCATED LOGISTIC DISTRIBUTION,0.2078853046594982,"Input: Categorical data xt, Flow g(·, ·)
Output: zt, log q(zt|xt), log p(xt|zt)
u0 ∼Uniform(0, 1)
▷Begin encoding
u ←m(xt) +
 
u0 −1"
TRUNCATED LOGISTIC DISTRIBUTION,0.2114695340501792,"2

· s(xt)
z′
t ←F −1(u)
zt ←g(z′
t, xt)
▷End Encoding
for ˆxt = 0 to K −1 do
▷Compute probability of zt given all possible ˆxt
ˆz′
t ←g−1(zt, ˆxt)"
TRUNCATED LOGISTIC DISTRIBUTION,0.21505376344086022,"log q(zt|ˆxt) ←log ˜f(ˆz′
t; m(ˆxt), s(ˆxt)) + log

dz′
t
dˆzt"
TRUNCATED LOGISTIC DISTRIBUTION,0.21863799283154123,"end for
log p(xt|zt) ←log q(zt|xt) ˜p(xt) −log P"
TRUNCATED LOGISTIC DISTRIBUTION,0.2222222222222222,"ˆxt q(zt|ˆxt) ˜p(ˆxt)
▷log computation of the q posterior"
TRUNCATED LOGISTIC DISTRIBUTION,0.22580645161290322,Algorithm 2 Rejection sampling
TRUNCATED LOGISTIC DISTRIBUTION,0.22939068100358423,"Output: x
z ∼p(z)
while ∀ˆx : ˜f(z; m(ˆx), s(ˆx)) = 0 do"
TRUNCATED LOGISTIC DISTRIBUTION,0.23297491039426524,"z ∼p(z)
end while
x ∼p(x|z)"
TRUNCATED LOGISTIC DISTRIBUTION,0.23655913978494625,"p(z)
ptrunc(z)"
TRUNCATED LOGISTIC DISTRIBUTION,0.24014336917562723,"Figure 2: Left: Pseudo-code for rejection sampling. Centre: The untruncated prior p(z). Right: The
prior truncated according to the unioned support of q. We denote this as ptrunc(z)."
REJECTION SAMPLING,0.24372759856630824,"3.2
REJECTION SAMPLING"
REJECTION SAMPLING,0.24731182795698925,"When sampling from the model, the standard process is ancestral sampling: z ∼p(z), and then
x ∼p(x|z). However, in our model, the sample zt may not have support under any of the q(zt|xt),
resulting in an undeﬁned optimal p(xt|zt). As mentioned, we reject the samples that have no support
for any of the mixture components. This is equivalent to redeﬁning a new prior ptrunc(z) proportional
to p(z) such that its support matches the unioned support of q(z|x) and renormalising. The existing
ELBO is still a valid lower-bound of such a distribution:"
REJECTION SAMPLING,0.25089605734767023,L = Eq(z|x)
REJECTION SAMPLING,0.25448028673835127,"
log p(x|z) p(z)"
REJECTION SAMPLING,0.25806451612903225,q(z|x)
REJECTION SAMPLING,0.2616487455197133,"
≤Eq(z|x) """
REJECTION SAMPLING,0.26523297491039427,log p(x|z) p(z)
REJECTION SAMPLING,0.26881720430107525,"Z
q(z|x) #"
REJECTION SAMPLING,0.2724014336917563,"≤
Z
p(x|z) ptrunc(z) dz,
(8)"
REJECTION SAMPLING,0.27598566308243727,"where
Z :=
Z
p(z) · 1
h
∃ˆx : ˜f(z; m(ˆx), s(ˆx)) > 0
i
dz ≤1.
(9)"
REJECTION SAMPLING,0.27956989247311825,"Eq. 9 in principle can be estimated by counting the frequency of accepted samples. For one of our
experiments, the overall rejection rate at the end of training is about 0.14, suggesting that rejection
sampling from a trained prior is not inefﬁcient (see Section 5.1). Using the decoder as an accepting
heuristic, we ﬁnd empirically that this results in more valid samples in constrained tasks that require
sampling from the model."
RELATED WORK,0.2831541218637993,"4
RELATED WORK"
RELATED WORK,0.2867383512544803,"Flows on Discrete Data
Discrete ﬂows (Tran et al., 2019; Hoogeboom et al., 2019) deal with ﬂows
in the discrete space, but because of this they resort to the straight-through method (Bengio et al.,
2013) for estimating gradient. Using conditional permutations, Lindt & Hoogeboom (2021) develop
a way to use discrete ﬂows that do not have the same gradient bias that Discrete ﬂows introduce due
to its gradient estimator. Our approach maps the discrete data into a continuous space, allowing the
prior to take on most of the modelling complexity."
RELATED WORK,0.2903225806451613,Published as a conference paper at ICLR 2022
RELATED WORK,0.2939068100358423,"Table 1: Results on the graph coloring problem (Lippe & Gavves, 2020). SMALL are graphs of
size 10 ≤|V | ≤20, and LARGE 25 ≤|V | ≤50. All results are attained using the CatNF
codebase, and averaged across 3 random seeds. Results in the rounded box are using a different set of
hyperparameters than the ones used in CatNF."
RELATED WORK,0.2974910394265233,"SMALL
LARGE"
RELATED WORK,0.3010752688172043,"Method
Validity
bpd
Validity
bpd"
RELATED WORK,0.3046594982078853,"RNN+Largest ﬁrst
93.41% ±0.42%
0.68 ±0.01
71.32% ±0.77%
0.43 ±0.01"
RELATED WORK,0.30824372759856633,"CatNF
94.56% ±0.55%
0.67 ±0.00
66.80% ±1.14%
0.45 ±0.01"
RELATED WORK,0.3118279569892473,"68.06% ±0.04%
0.45 ±0.00"
RELATED WORK,0.3154121863799283,"Argmax (thresh.)
94.81% ±0.37%
0.66±0.00
63.65% ±0.24%
0.46 ±0.00"
RELATED WORK,0.31899641577060933,"No rejection sampling
95.40% ±0.35%
0.65 ±0.01
68.10% ±0.00%
Rejection sampling
95.90% ±0.29%
74.20% ±0.01%
0.45 ±0.00"
RELATED WORK,0.3225806451612903,"Learning the Prior
Since most of the modelling happens in the prior after dequantisation, the prior
has to have the expressibility to model the resulting continuous data. Learnable priors are not new in
the VAE literature (Tomczak & Welling, 2018; Huang et al., 2017). One common way for learning
sequential data is to use an autoregressive prior (Ziegler & Rush, 2019). Liu et al. (2019) introduces
a graph-structured ﬂows which can be used for modelling such data, and there have been ﬂow-based
models developed for molecule generation (Madhawa et al., 2019; Shi et al., 2020). Ziegler & Rush
(2019) and Huang et al. (2018) also introduce normalising ﬂows that can learn multi-modal target
distributions, which Ziegler & Rush (2019) show is important in modelling text data."
RELATED WORK,0.32616487455197135,"Optimal decoder
In the case of categorical data with a sufﬁciently small K, computing the
posterior of the encoder is tractable during training. The optimal decoder in Eq. 3 was used in
practice in Lippe & Gavves (2020). Argmax ﬂow (Hoogeboom et al., 2021) are a special case in
which the encoding of the categorical variable results in a latent variable that has a deterministic
decoding, which Nielsen et al. (2020) generalises and calls surjectivity which can be parameterised
in either inference or generation. The optimal decoder also has the added effect of alleviating the
posterior collapse problem commonly faced when modelling discrete data such as text (Yang et al.,
2017; Bowman et al., 2015; Ziegler & Rush, 2019; Chen et al., 2016)."
RELATED WORK,0.32974910394265233,"Bounded-support distributions in latent variable models
While normalising ﬂows (Papamakar-
ios et al., 2019) allow for learning a more ﬂexible posterior distribution, Jaini et al. (2020) and Verine
et al. (2021) discuss the current limitations of ﬂows due to their bi-lipschitz property which result
in nearly no transformation at the tails of distributions. Truncated distributions are an extreme case
of a light-tailed distribution, achieved by performing a shift and scaling of the base uniform distri-
bution. There have also been efforts on spherical latent variables, modelled by a von Mises-Fisher
(vMF) distribution (Xu & Durrett, 2018; Davidson et al., 2018). Polykovskiy & Vetrov (2020) uses
bounded-support kernels as the basis of their posterior distributions, and modify the decoder so that
the results are deterministic, but introduce gradient estimations in order to train the model. In the
latter two cases, rejection sampling was used in order to attain samples for inference."
EXPERIMENTS,0.3333333333333333,"5
EXPERIMENTS"
EXPERIMENTS,0.33691756272401435,"In this section, we present experiments on TRUFL. See Appendix C for implementation details."
GRAPH COLORING,0.34050179211469533,"5.1
GRAPH COLORING"
GRAPH COLORING,0.34408602150537637,"We ﬁrst consider the graph colouring problem (Bondy et al., 1976) introduced in Lippe & Gavves
(2020). The task is to colour the nodes in a graph with 3 different colours such that any two nodes
connected by an edge do not have the same colour, which provides a meaningful benchmark for
evaluating how well a model learns unknown constraints from the data. We evaluate the Validity of
the generated samples conditioned a given graph."
GRAPH COLORING,0.34767025089605735,Published as a conference paper at ICLR 2022
GRAPH COLORING,0.35125448028673834,"Table 2: Performance on molecule generation trained on Zinc250k (Irwin et al., 2012), calculated on
10k samples and averaged over 4 random seeds."
GRAPH COLORING,0.3548387096774194,"Method
Validity
Uniqueness
Novelty"
GRAPH COLORING,0.35842293906810035,Constrained generation
GRAPH COLORING,0.36200716845878134,"JT-VAE (Jin et al., 2018)
100%
100%
100%"
GRAPH COLORING,0.3655913978494624,"GraphAF (Shi et al., 2020)
68%
99.10%
100%
R-VAE (Ma et al., 2018)
34.90%
100%
—
GraphNVP (Madhawa et al., 2019)
42.60%
94.80%
100%
Argmax ﬂow (thresh.)
78.36% ±4.93%
100%
99.99%
CNF (Lippe & Gavves, 2020)
83.41% ±2.34%
99.99%
100%
,→Sub-graphs
96.35% ±2.58%
99.98%
99.98%"
GRAPH COLORING,0.36917562724014336,"No rejection sampling
84.46%±4.24%
100%
100%
,→Sub-graphs
97.51%±2.37%
99.98%
99.99%
,→Rejection sampling
85.01%±1.05%
100%
100%
,→Sub-graphs
97.87%±0.06%
100%
100%"
GRAPH COLORING,0.3727598566308244,"For baselines, we present results from CatNF (Lippe & Gavves, 2020), and implemented the thresh-
olding version of Argmax ﬂow. We implement the truncated ﬂows using the CatNF codebase, and
compare results with and without rejection sampling for TRUFL. Additionally, we ﬁnd a better
set of hyperparameters work better for the LARGE setting, and report the results base on those
hyperparameters for the baseline as well. All graph-structured ﬂows used in this task are the same as
the ones detailed in CatNF, we simply replace the categorical embedding module. For results without
rejection sampling, we also compute the probability over the categories using Eq. 9, but disregard the
truncation of the densities. We ﬁnd we perform marginally better than the baselines in the small graph
regime. However, with rejection sampling and better hyperparameters, we signiﬁcantly outperform
the ﬂow-based benchmarks, and perform better than the RNN baseline used in Lippe & Gavves
(2020)."
GRAPH COLORING,0.3763440860215054,"Figure 3: Rejection rate vs training iterations. The re-
jection rate when sampling for evaluation for the graph
colouring task reduces during iteration, indicating (1)
the rejection rate is not intractably high, and (2) the
prior tries to match the support of TRUFL over time."
GRAPH COLORING,0.37992831541218636,"Rejection Rate of TRUFL
Since the
prior will be optimised to ﬁt the aggregate
posterior during training, we expect the re-
jection rate of sampling from the truncated
prior to drop as training progresses. We
measure the rejection rate when sampling
from the model and report its average rate
of rejection as training progresses. While
the model is conditioned on graphs of dif-
ferent sizes during sampling, we look at
the average rate of rejection go obtain a
measure of the efﬁciency of our rejection
sampling scheme. Figure 3 suggests that
the prior learns to ﬁt the support of the ag-
gregate posterior better during training, as
the rejection rate reduces from about 0.45
to about 0.14 at the end of training. This
corresponds to roughly losing log Z ≈0.15 nats (or 0.22 bits) per molecule. Note that we did not
account for this gap while computing the bpd in Table 1 since we would have to estimate log Z
conditioned on each graph size, and this would give us slightly lower value in theory."
MOLECULE GENERATION,0.3835125448028674,"5.2
MOLECULE GENERATION"
MOLECULE GENERATION,0.3870967741935484,"Molecule generation is another task commonly used as a generative modelling benchmark (Jin et al.,
2018; Shi et al., 2020; Ma et al., 2018; Madhawa et al., 2019). The atoms constituting the molecules
are naturally represented by tokens that are usually interpreted categorically, but the intrinsic structure
of the atoms, such as the electron conﬁgurations, is often neglected. For this reason, we believe
TRUFL and CatNF will learn a more useful representation than Argmax Flow. Moreover, given the"
MOLECULE GENERATION,0.3906810035842294,Published as a conference paper at ICLR 2022
MOLECULE GENERATION,0.3942652329749104,"Table 3: Results on character-level and word-level language modelling, an average across 3 different
random seeds. Results reported with † indicates results attained with the Argmax ﬂow codebase. ∗
indicates results attained with the CatNF codebase. All other results are from the CatNF paper."
MOLECULE GENERATION,0.3978494623655914,"CHARACTER
WORD"
MOLECULE GENERATION,0.4014336917562724,"Model
PTB (char.)
Text8
Wikitext103"
MOLECULE GENERATION,0.4050179211469534,"LSTM
1.28 ±0.01
1.44 ±0.01
4.81 ±0.05
Latent NF (Ziegler & Rush, 2019)
1.30 ±0.01
1.61 ±0.02
6.39 ±0.19
Categorical NF (Lippe & Gavves, 2020)
1.27 ±0.01
1.45 ±0.01
5.43 ±0.09
Argmax Flow (Hoogeboom et al., 2021)
∗1.26 ±0.01
†1.39 ±0.00
∗5.42 ±0.01"
MOLECULE GENERATION,0.40860215053763443,"Ours
∗1.26 ±0.02
†1.40 ±0.01
∗5.35 ±0.01"
MOLECULE GENERATION,0.4121863799283154,"Table 4: Results on word-level language modelling, each sentence is modelled as a separate data
point. CatNF, Argmax Flow, and TRUFL results are averaged across 4 random seeds. Setting and
baseline LSTM provided by Kim et al. (2019)."
MOLECULE GENERATION,0.4157706093189964,"Model
Dim.
PPL
NLL
Recon.
KL"
MOLECULE GENERATION,0.41935483870967744,"LSTM
—
86.2
4.46
—
—
CatNF
12
139.7
± 3.0
4.93
± 0.02
1.01
4.18
Argmax Flow (binary)
14
242.7
± 2.7
5.49
± 0.01
0.00
5.75"
MOLECULE GENERATION,0.4229390681003584,"Ours
12
143.6
± 4.9
4.97
± 0.03
1.47
3.71"
MOLECULE GENERATION,0.4265232974910394,"results in graph colouring, we believe that rejection sampling can aid in creating more valid samples
than prior unconstrained generation methods. Molecule generation also requires the generation of
edge categories in addition to the node categories."
MOLECULE GENERATION,0.43010752688172044,"Our benchmarks were implemented with the Lippe & Gavves (2020) codebase, and the Argmax ﬂow
benchmark uses the same thresholding scheme. Rejection sampling in this case requires rejecting
invalid edges and vertices both, and so resulted in longer sampling times. The results are in Table
2 Again, we ﬁnd that TRUFL consistently improves upon CatNF in terms of the validity of the
generated samples, which can be further increased by truncating the prior via rejection sampling, and
taking the largest sub-graph in cases where multiple disconnected graphs are generated (as in done in
Lippe & Gavves (2020))."
LANGUAGE MODELLING,0.4336917562724014,"5.3
LANGUAGE MODELLING"
LANGUAGE MODELLING,0.43727598566308246,"One can view these variational dequantisation methods when applied to language modelling as a
method of using stochastic embeddings: each token is represented by a distribution over the embed-
ding space, and language models then operate over samples from these distributions. Probabilistic
embeddings have been suggested in prior work (Li et al., 2018; Dasgupta et al., 2020; Chen et al.,
2021), but here, distribution parameters are trained with a language modelling objective. We per-
form experiments on character-level and word-level language modelling. For word-level language
modelling, many tasks require a large number of categories (∼10k). The naive thresholding method
will result in a large latent space, no different from using a one-hot representation, so we use the
previously mentioned binary encoding of each category in our implementation of the Argmax ﬂow."
LANGUAGE MODELLING,0.44086021505376344,"For the character level experiments on the Penn Treebank (PTB; Marcus et al. 1993), we use the setup
in CatNF, replacing the categorical encoding with TRUFL. For text8 (Mikolov et al., 2014), we use
the Argmax ﬂow codebase, modifying the Argmax ﬂow module and replacing it with TRUFL. For
word level experiments on Wikitext103 (Merity et al., 2016), we use the setting provided by CatNF
as well, which evaluates chunks of 256 words, and initialises the word embeddings using GloVe
(Pennington et al., 2014). While this is not the standard setting for language modelling, we include
these results for comparison with Argmax Flow and CatNF. We report the negative log-likelihood in
Table 3. For word level experiments on PTB, we use the setting speciﬁed by Kim et al. (2019), which
also provides an LSTM baseline. Unlike standard PTB results, this benchmark treats each sentence"
LANGUAGE MODELLING,0.4444444444444444,Published as a conference paper at ICLR 2022
LANGUAGE MODELLING,0.44802867383512546,"Figure 4: Scatter plot of t-SNE embeddings of samples from Armgax Flows (left) and TRUFL (right).
Since t-SNE reveals the relative proximities of the embeddings, we should note that the visualisation
here reveals what clusters of embeddings are close to others relative to the other words in the plot."
LANGUAGE MODELLING,0.45161290322580644,"as i.i.d., instead of treating the entire dataset as a continuous string of text. We provide further details
of the architecture in Appendix C.3."
LANGUAGE MODELLING,0.4551971326164875,"The reconstruction loss for TRUFL is higher than that of CatNF, suggesting that CatNF may have
optimised the variance of each dequantising distribution to be fairly small in order to approach
deterministic decoding. To verify this, we compute the Within-Group Standard Deviation (WGSD)
across all q(zt|xt). Sampling 200 samples from each category, we ﬁrst normalise the mean and
standard deviation across all 10, 000 × 200 categories and samples. We then compute the standard
deviation for each category across the 200 samples, and average across all 12 dimensions. CatNF
has an WGSD of 0.52, while TRUFL has an WGSD of 0.66. A smaller WGSD would indicate
peakier disributions in the latent space, given that each quantising distribution has a lower dispersion
on average. Since the prior is modelling the correlations between the categories in the sequence, a
peakier distribution will require more capacity to model. In this scenario, we ﬁnd that both models
are comparable, with CatNF performing slightly better. However, recall that even with importance
sampling, we are not accounting for the unsupported regions of the prior. This means this is an
estimate of the upper bound in Eq. 8, and that estimating Z will result in a lower perplexity score. In
general, there has been a gap between autoregressive models (e.g. RNNs, Transformers) that model
the discrete distribution directly, and latent variable models, and we believe this gap will close as
dequantising techniques improve."
LANGUAGE MODELLING,0.45878136200716846,"We perform a qualitative analysis on the learned stochastic embeddings, comparing the t-SNE plots
of samples of 10 chosen words. We took 200 samples from q(zt|xt) for each word, and performed
t-SNE over all 200 × 10 vectors. As expected, Argmax ﬂow partitions the latent space into separate
orthants, resulting in word embeddings that are arbitrarily chosen. N and three are an exception,
but this is perhaps due to a chance occurrence of their orthants being dissimilar in only one dimension.
On the other hand, TRUFL learning the topology of these embeddings results in clusters that retain
some properties of the words they represent. Interestingly, while flew:fly and walked:walk
are different tenses of the same word, the learned distribution for each word appears to group them
by tense instead. Since this would inform predictions of the tense of future words, this is perhaps a
functional representation of these words."
CONCLUSION,0.46236559139784944,"6
CONCLUSION"
CONCLUSION,0.4659498207885305,"In this paper, we propose TRUFL, a ﬂow based dequantiser with truncated support for stochastically
embedding categorical data in a continuous space. This allows us to employ density-based models
like normalising ﬂows to ﬁt the continuous representation of the data. To deal with the unsupported
regions in the decoder, we further propose truncating the prior distribution to account for the unioned
support of the dequantiser, which proves to be more effective at generating samples with constraints,
such as for graph coloring and molecule generation. In language modelling, we ﬁnd that learned
stochastic embeddings has better performance than a strict orthant partitioning of the space, and
qualitative analysis reveals that the proximity between similar words under such a partitioning is poor.
We believe that further work on variational dequantisation will close the gap between auto-regressive
models that directly model the categorical space and latent variable models."
CONCLUSION,0.46953405017921146,Published as a conference paper at ICLR 2022
CONCLUSION,0.4731182795698925,ACKNOWLEDGEMENTS
CONCLUSION,0.4767025089605735,"We would like to thank Yikang Shen and Christos Tsirigotis for their input and suggestions during
the course of this work. Phillip Lippe was also gracious in helping us understand the speciﬁcs of the
CatNF codebase and running the experiments."
REFERENCES,0.48028673835125446,REFERENCES
REFERENCES,0.4838709677419355,"Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic
language model. The journal of machine learning research, 3:1137–1155, 2003."
REFERENCES,0.4874551971326165,"Yoshua Bengio, Nicholas L´eonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013."
REFERENCES,0.4910394265232975,"John Adrian Bondy, Uppaluri Siva Ramachandra Murty, et al. Graph theory with applications,
volume 290. Macmillan London, 1976."
REFERENCES,0.4946236559139785,"Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio.
Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015."
REFERENCES,0.4982078853046595,"Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731,
2016."
REFERENCES,0.5017921146953405,"Xuelu Chen, Michael Boratko, Muhao Chen, Shib Sankar Dasgupta, Xiang Lorraine Li, and Andrew
McCallum. Probabilistic box embeddings for uncertain knowledge graph reasoning. arXiv preprint
arXiv:2104.04597, 2021."
REFERENCES,0.5053763440860215,"Shib Dasgupta, Michael Boratko, Dongxu Zhang, Luke Vilnis, Xiang Li, and Andrew McCallum.
Improving local identiﬁability in probabilistic box embeddings. Advances in Neural Information
Processing Systems, 33:182–192, 2020."
REFERENCES,0.5089605734767025,"Tim R Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M Tomczak. Hyperspherical
variational auto-encoders. arXiv preprint arXiv:1804.00891, 2018."
REFERENCES,0.5125448028673835,"Michael Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit reparameterization gradients. arXiv
preprint arXiv:1805.08498, 2018."
REFERENCES,0.5161290322580645,"Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for
distribution estimation. In International Conference on Machine Learning, pp. 881–889. PMLR,
2015."
REFERENCES,0.5197132616487455,"Joshua Goodman. Classes for fast maximum entropy training. In 2001 IEEE International Conference
on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 01CH37221), volume 1, pp.
561–564. IEEE, 2001."
REFERENCES,0.5232974910394266,"Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving ﬂow-
based generative models with variational dequantization and architecture design. In International
Conference on Machine Learning, pp. 2722–2730. PMLR, 2019."
REFERENCES,0.5268817204301075,"Emiel Hoogeboom, Jorn Peters, Rianne van den Berg, and Max Welling. Integer discrete ﬂows and
lossless compression. Advances in Neural Information Processing Systems, 32:12134–12144,
2019."
REFERENCES,0.5304659498207885,"Emiel Hoogeboom, Taco Cohen, and Jakub Mikolaj Tomczak. Learning discrete distributions by
dequantization. In Third Symposium on Advances in Approximate Bayesian Inference, 2020."
REFERENCES,0.5340501792114696,"Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr´e, and Max Welling.
Argmax
ﬂows and multinomial diffusion: Towards non-autoregressive language models. arXiv preprint
arXiv:2102.05379, 2021."
REFERENCES,0.5376344086021505,"Chin-Wei Huang, Ahmed Touati, Laurent Dinh, Michal Drozdzal, Mohammad Havaei, Laurent
Charlin, and Aaron Courville. Learnable explicit density for continuous latent space and variational
inference. arXiv preprint arXiv:1710.02248, 2017."
REFERENCES,0.5412186379928315,Published as a conference paper at ICLR 2022
REFERENCES,0.5448028673835126,"Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
ﬂows. In International Conference on Machine Learning, pp. 2078–2087. PMLR, 2018."
REFERENCES,0.5483870967741935,"John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a
free tool to discover chemistry for biology. Journal of chemical information and modeling, 52(7):
1757–1768, 2012."
REFERENCES,0.5519713261648745,"Priyank Jaini, Ivan Kobyzev, Yaoliang Yu, and Marcus Brubaker. Tails of lipschitz triangular ﬂows.
In International Conference on Machine Learning, pp. 4673–4681. PMLR, 2020."
REFERENCES,0.5555555555555556,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. In International conference on machine learning, pp. 2323–2332.
PMLR, 2018."
REFERENCES,0.5591397849462365,"Yoon Kim, Chris Dyer, and Alexander M Rush. Compound probabilistic context-free grammars for
grammar induction. arXiv preprint arXiv:1906.10225, 2019."
REFERENCES,0.5627240143369175,"Diederik P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions.
arXiv preprint arXiv:1807.03039, 2018."
REFERENCES,0.5663082437275986,"Diederik P Kingma and Max Welling. Stochastic gradient vb and the variational auto-encoder. In
Second International Conference on Learning Representations, ICLR, volume 19, pp. 121, 2014."
REFERENCES,0.5698924731182796,"Xiang Li, Luke Vilnis, Dongxu Zhang, Michael Boratko, and Andrew McCallum. Smoothing the ge-
ometry of probabilistic box embeddings. In International Conference on Learning Representations,
2018."
REFERENCES,0.5734767025089605,"Alexandra Lindt and Emiel Hoogeboom. Discrete denoising ﬂows. In ICML Workshop on Invertible
Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021."
REFERENCES,0.5770609318996416,"Phillip Lippe and Efstratios Gavves. Categorical normalizing ﬂows via continuous transformations.
arXiv preprint arXiv:2006.09790, 2020."
REFERENCES,0.5806451612903226,"Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing ﬂows.
arXiv preprint arXiv:1905.13177, 2019."
REFERENCES,0.5842293906810035,"Tengfei Ma, Jie Chen, and Cao Xiao. Constrained generation of semantically valid graphs via
regularizing variational autoencoders. arXiv preprint arXiv:1809.02630, 2018."
REFERENCES,0.5878136200716846,"Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, and Motoki Abe. Graphnvp: An invertible
ﬂow model for generating molecular graphs. arXiv preprint arXiv:1905.11600, 2019."
REFERENCES,0.5913978494623656,"Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. 1993."
REFERENCES,0.5949820788530465,"Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016."
REFERENCES,0.5985663082437276,"Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc’Aurelio Ranzato.
Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753, 2014."
REFERENCES,0.6021505376344086,"Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances
in neural information processing systems, 21:1081–1088, 2008."
REFERENCES,0.6057347670250897,"Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In
International workshop on artiﬁcial intelligence and statistics, pp. 246–252. PMLR, 2005."
REFERENCES,0.6093189964157706,"Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012."
REFERENCES,0.6129032258064516,"Didrik Nielsen and Ole Winther. Closing the dequantization gap: Pixelcnn as a single-layer ﬂow.
arXiv preprint arXiv:2002.02547, 2020."
REFERENCES,0.6164874551971327,"Didrik Nielsen, Priyank Jaini, Emiel Hoogeboom, Ole Winther, and Max Welling. Survae ﬂows:
Surjections to bridge the gap between vaes and ﬂows. Advances in Neural Information Processing
Systems, 33, 2020."
REFERENCES,0.6200716845878136,Published as a conference paper at ICLR 2022
REFERENCES,0.6236559139784946,"George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji
Lakshminarayanan. Normalizing ﬂows for probabilistic modeling and inference. arXiv preprint
arXiv:1912.02762, 2019."
REFERENCES,0.6272401433691757,"Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532–1543, 2014."
REFERENCES,0.6308243727598566,"Daniil Polykovskiy and Dmitry Vetrov. Deterministic decoding for discrete data in variational
autoencoders. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 3046–3056.
PMLR, 2020."
REFERENCES,0.6344086021505376,"Yikang Shen, Shawn Tan, Chrisopher Pal, and Aaron Courville. Self-organized hierarchical softmax.
arXiv preprint arXiv:1707.08588, 2017."
REFERENCES,0.6379928315412187,"Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a
ﬂow-based autoregressive model for molecular graph generation. arXiv preprint arXiv:2001.09382,
2020."
REFERENCES,0.6415770609318996,"Lucas Theis, A¨aron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015."
REFERENCES,0.6451612903225806,"Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 1214–1223. PMLR, 2018."
REFERENCES,0.6487455197132617,"Dustin Tran, Keyon Vafa, Kumar Agrawal, Laurent Dinh, and Ben Poole. Discrete ﬂows: Invertible
generative models of discrete data. Advances in Neural Information Processing Systems, 32:
14719–14728, 2019."
REFERENCES,0.6523297491039427,"Alexandre Verine, Benjamin Negrevergne, Fabrice Rossi, and Yann Chevaleyre. On the expressivity
of bi-lipschitz normalizing ﬂows. arXiv preprint arXiv:2107.07232, 2021."
REFERENCES,0.6559139784946236,"Jiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders. arXiv
preprint arXiv:1808.10805, 2018."
REFERENCES,0.6594982078853047,"Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational
autoencoders for text modeling using dilated convolutions. In International conference on machine
learning, pp. 3881–3890. PMLR, 2017."
REFERENCES,0.6630824372759857,"Zachary Ziegler and Alexander Rush. Latent normalizing ﬂows for discrete sequences. In Interna-
tional Conference on Machine Learning, pp. 7673–7682. PMLR, 2019."
REFERENCES,0.6666666666666666,Published as a conference paper at ICLR 2022
REFERENCES,0.6702508960573477,"A
APPENDIX"
REFERENCES,0.6738351254480287,"A.1
GRADIENT ESTIMATES OF a AND b"
REFERENCES,0.6774193548387096,"p(z; a, b) ="
REFERENCES,0.6810035842293907,"(
1
b−a,
if a < z < b,
0,
otherwise
(10) Let"
REFERENCES,0.6845878136200717,"z = a + (b −a) · u
(11)
u ∼Uniform(0, 1)
(12)"
REFERENCES,0.6881720430107527,u = z −a
REFERENCES,0.6917562724014337,"b −a
(13)"
REFERENCES,0.6953405017921147,"∇aEz[f(z)] = Eu∼Uniform(0,1) [∇af(a + (b −a) · u)]
(14)"
REFERENCES,0.6989247311827957,"= Eu∼Uniform(0,1) [(1 −u)(∇zf(z))]
(15)"
REFERENCES,0.7025089605734767,"∇bEz[f(z)] = Eu∼Uniform(0,1) [u(∇zf(z))]
(16)"
REFERENCES,0.7060931899641577,"A.2
SOFT DEQUANTIZATION"
REFERENCES,0.7096774193548387,"Let x denote the data (e.g. a sentence) and L denote the dimensionality of the data; i.e. x ∈CL and
L ∈Z+. x can be dequantized by sampling zj ∼q(zj|xj) for j ∈1, ..., L. When {q(zj|xj = c) :
c ∈C} has the same support, we call it soft dequantization. We write p(xj|zj) and p(z) to denote the
quantizer and the prior over the dequantized data. We consider the following ELBO"
REFERENCES,0.7132616487455197,"L(x, L) = EQ q(zj|xj)  
L
X"
REFERENCES,0.7168458781362007,"j=1
log p(xj|zj)"
REFERENCES,0.7204301075268817,q(zj|xj) + log p(z) 
REFERENCES,0.7240143369175627,"
(17)"
REFERENCES,0.7275985663082437,"Theorem 1 (Maximizer of ELBO, dequantization). Assume q(x, L) > 0 for all x ∈CL and L ∈Z+,
and that x1, ..., xL are not conditionally independent given L. Then"
REFERENCES,0.7311827956989247,"Eq(x,L)[log q(x|L)] > Eq(x,L)[L]
(18)"
REFERENCES,0.7347670250896058,"as long as the densities q(zj|xj = c) for different categories c ∈C have the same support. That is,
soft dequantization with shared support is suboptimal."
REFERENCES,0.7383512544802867,Proof. The optimal quantizer is
REFERENCES,0.7419354838709677,"q(xj|zj) ∝q(xj)q(zj|xj)
where
q(xj) ∝ ∞
X L=1 L
X"
REFERENCES,0.7455197132616488,"j′=1
q(x′
j = xj, L)
(19)"
REFERENCES,0.7491039426523297,Plugging it into the ELBO gives
REFERENCES,0.7526881720430108,"L(x, L) = EQ q(zj|xj)  
L
X"
REFERENCES,0.7562724014336918,"j=1
log q(xj|zj)"
REFERENCES,0.7598566308243727,q(zj|xj) + log p(z) 
REFERENCES,0.7634408602150538,"= EQ q(zj|xj)  
L
X"
REFERENCES,0.7670250896057348,"j=1
log q(xj) + log
p(z)
Q"
REFERENCES,0.7706093189964157,j q(zj)   (20)
REFERENCES,0.7741935483870968,where q(zj) = P
REFERENCES,0.7777777777777778,"x′
j q(x′
j)q(zj|x′
j)."
REFERENCES,0.7813620071684588,"On the other hand, we can rewrite the negentropy of the data as"
REFERENCES,0.7849462365591398,"Eq(x,L)[log q(x|L)] = Eq(x,L)  
L
X"
REFERENCES,0.7885304659498208,"j=1
log q(xj) + log
q(x|L)
QL
j=1 q(xj) "
REFERENCES,0.7921146953405018,"
(21)"
REFERENCES,0.7956989247311828,Published as a conference paper at ICLR 2022
REFERENCES,0.7992831541218638,"Now to compare the second term with that of the ELBO we let q(z|L) = P q(x|L) QL
j=1 q(zj|xj)."
REFERENCES,0.8028673835125448,"For simplicity, we denote by T(z|x) = QL
j=1 q(zj|xj) the transition kernel applied to either q(x|L)"
REFERENCES,0.8064516129032258,"or QL
j=1 q(xj). Then"
REFERENCES,0.8100358422939068,"Eq(x,L) """
REFERENCES,0.8136200716845878,"log
q(x|L)
QL
j=1 q(xj) #"
REFERENCES,0.8172043010752689,"= Eq(x,L)T (z|x) "
REFERENCES,0.8207885304659498,"log
q(x|L)T(z|x)
QL
j=1 q(xj)

T(z|x) "
REFERENCES,0.8243727598566308,"
(22)"
REFERENCES,0.8279569892473119,"= Eq(z,L)q(x|z,L)  −log"
REFERENCES,0.8315412186379928,"QL
j=1 q(xj)

T(z|x)"
REFERENCES,0.8351254480286738,q(x|L)T(z|x) 
REFERENCES,0.8387096774193549,"
(23)"
REFERENCES,0.8422939068100358,"∗> Eq(z,L)  −log "
REFERENCES,0.8458781362007168,"Eq(x|z,L)   Q"
REFERENCES,0.8494623655913979,"j q(xj)

T(z|x)"
REFERENCES,0.8530465949820788,q(x|L)T(z|x)     
REFERENCES,0.8566308243727598,"
(24)"
REFERENCES,0.8602150537634409,"= Eq(z,L)"
REFERENCES,0.8637992831541219,"
−log Q"
REFERENCES,0.8673835125448028,j q(zj)
REFERENCES,0.8709677419354839,q(z|L)
REFERENCES,0.8745519713261649,"
= Eq(z,L) """
REFERENCES,0.8781362007168458,"log q(z|L)
Q"
REFERENCES,0.8817204301075269,j q(zj) # (25)
REFERENCES,0.8853046594982079,"which is greater or equal to the second term of the ELBO by Gibb’s inequality. The inequality marked
by ∗is due to Jensen and the convexity of −log, and is strict since the dependency of x1,...,L implies
there exist some x1 and x2 s.t. Q"
REFERENCES,0.8888888888888888,"j q(xi
j) ̸= q(xi|L) for i ∈{1, 2}. Now, since T(z|x) > 0 over the
shared support by assumption, for almost all z and L, q(xi|z, L) > 0 for i ∈{1, 2}. That is, with
non-zero probability the argument to the convex function −log has a different value from its mean.
This implies the inequality is strict and concludes the proof."
REFERENCES,0.8924731182795699,"B
FURTHER RELATED WORK"
REFERENCES,0.8960573476702509,"Consequences of a binary encoding
The binary encoding scheme of Argmax ﬂow bears some
resemblance to prior work on hierachical softmax schemes to reduce the computational footprint
of the large softmax layer (Goodman, 2001; Morin & Bengio, 2005; Mnih & Hinton, 2008; Shen
et al., 2017). Speciﬁcally, Mnih & Hinton (2008) proposed an analogous scheme for scaling up the
softmax layer by representing words as leaves in a tree, and binary encodings as a traversal of such a
tree. However, the partitioning of the ⌈log2 K⌉-dimensional space into K equal categories will only
be exact if K is a power of 2. Otherwise, K −2⌈log2 K⌉orthants of the embedding space will not
be utilised by the dequantiser. TRUFL has unsupported regions in the decoder as well, but we take
the approach of modifying the prior during generation to account for these regions. Furthermore,
because the binary vectors are arbitrary, the similarity between words/categories is not reﬂected in
the embedding space as it is in a learned distributed representation."
REFERENCES,0.899641577060932,"C
EXPERIMENTS & IMPLEMENTATION DETAILS"
REFERENCES,0.9032258064516129,We incorporated the codebases from the following sources:
REFERENCES,0.9068100358422939,1. Lippe & Gavves (2020): https://github.com/phlippe/CategoricalNF
REFERENCES,0.910394265232975,2. Hoogeboom et al. (2021): https://github.com/didriknielsen/argmax_flows
REFERENCES,0.9139784946236559,3. Kim et al. (2019): https://github.com/harvardnlp/compound-pcfg
REFERENCES,0.9175627240143369,"C.1
TOY EXAMPLE: TWO CATEGORY JOINT DISTRIBUTION"
REFERENCES,0.921146953405018,"In this simple synthetic example, we demonstrate a case in which TRUFL provides an advantage over
CatNF and Argmax ﬂow. Our goal is to model a joint probability over 2 categorical random variables,
each with 4 categories. This joint distribution is described in Table 5."
REFERENCES,0.9247311827956989,Published as a conference paper at ICLR 2022
REFERENCES,0.9283154121863799,"(a) CatNF
(b) TRUFL
(c) Argmax Flow"
REFERENCES,0.931899641577061,Figure 5: Embeddings (q(zi|xi)) of toy example for each method.
REFERENCES,0.9354838709677419,"Table 5: Left: The joint distribution for x1 and x2 for the toy example. Right: Log probability over
the data of the model, and the breakdown of the ELBO."
REFERENCES,0.9390681003584229,"x2
Cat.
0
1
2
3 x1"
REFERENCES,0.942652329749104,"0
1
8
0
1
8
0
1
0
1
8
0
1
8
2
1
8
0
1
8
0
3
0
0
0
1
4"
REFERENCES,0.946236559139785,"Method
log p(x)
E [log p(x|z)]
E
h
log q(z|x)"
REFERENCES,0.9498207885304659,"p(z)
i"
REFERENCES,0.953405017921147,"CatNF
-2.000
-1.064
0.967
Argmax
-1.915
-0.687
1.263
TRUFL
-1.983
0.000
2.098"
REFERENCES,0.956989247311828,"Notice that under this distribution, p(x1|x2 = 0) = p(x1|x2 = 2) and p(x2|x1 = 0) = p(x2|x1 = 2).
For the purposes of this subsection we will say categories 0 and 2 have equal functionality, while
categories 1 and 3 are similar in functionality, but not equal."
REFERENCES,0.9605734767025089,"To model this distribution, we perform variational dequantisation over the discrete variables x1, x2
using the above methods, and learn a prior over the latent variables. Conditioned on xi, the dequanti-
sation method produces a distribution over a 2 dimensional space — q(zi|xi). It is this distribution
we plot in Figure 5."
REFERENCES,0.96415770609319,"In both CatNF and TRUFL, the distribution q(zi|xi = 0) and q(zi|xi = 2) are similar. Intuitively,
this aligns with the motivation behind the construction of the joint distribution: that the categories are
functionally equivalent. However, the dequantisation distribution may not always have this property,
as evidenced by the Argmax Flow, due to the way supports are constrained by the orthants. This
bimodality in the distributions of these two categories will have to be modelled by the prior, which
evidently resulted in a higher log probability in this particular case. In our implementation of Argmax
ﬂow for this task, the training eventually resulted in numerical instability. Here we report the best log
probability attained before numerical errors occured."
REFERENCES,0.967741935483871,"As mentioned in Lippe & Gavves (2020), CatNF can approach deterministic decoding if all other
classes are sufﬁciently far from the target class. However, if several tokens have similar ‘function’ in
the dataset, the corresponding components can be forced to be close together in latent space. When
this happens, the closeness in the components again result in decoding becoming non-deterministic.
TRUFL allows such components to have minimal and sparse overlapping support but yet have the
modes of these components be relatively close. If we consider Figure 5b, we can see that the support
and distribution for xi = 0 and xi = 2 overlap the most, while the overlap with xi = 1 and xi = 3 is
not as great. Since TRUFL minimises the support for the classes that do not overlap in functionality
the TRUFL reconstruction loss is lower in comparison to CatNF."
REFERENCES,0.9713261648745519,"C.2
HYPERPARAMETERS FOR LARGE GRAPH COLOURING TASK"
REFERENCES,0.974910394265233,"C.3
ARCHITECTURE FOR THE PRIOR IN LANGUAGE MODELLING TASK"
REFERENCES,0.978494623655914,"We develop our own architecture for this benchmark, replacing only the categorical encoding scheme
for each of the benchmarks. The prior is autoregressive both ‘in time’ and ‘in hidden’, to use"
REFERENCES,0.982078853046595,Published as a conference paper at ICLR 2022
REFERENCES,0.985663082437276,"Param.
Value"
REFERENCES,0.989247311827957,"batch size
128
encoding dim
2
coupling num flows
10
optimizer
4
learning rate
3e-4"
REFERENCES,0.992831541218638,Table 6: New arguments for the LARGE graph colouring task we use in our experiments.
REFERENCES,0.996415770609319,"nomenclature from Ziegler & Rush (2019). Autoregression in time is governed by an LSTM, and
autoregression in hidden is governed by a MADE model (Germain et al., 2015), conditioned on
said LSTM. We use a mixture of 4 logistics per-dimension (Ho et al., 2019) in order to model the
multi-modality of the distribution over z, which Ziegler & Rush (2019) suggests is prevalent in text
data for characters."
