Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0026246719160104987,"Factorizing a large matrix into small matrices is a popular strategy for model com-
pression. Singular value decomposition (SVD) plays a vital role in this compres-
sion strategy, approximating a learned matrix with fewer parameters. However,
SVD minimizes the squared error toward reconstructing the original matrix with-
out gauging the importance of the parameters, potentially giving a larger recon-
struction error for those who affect the task accuracy more. In other words, the
optimization objective of SVD is not aligned with the trained model’s task accu-
racy. We analyze this previously unexplored problem, make observations, and ad-
dress it by introducing Fisher information to weigh the importance of parameters
affecting the model prediction. This idea leads to our method: Fisher-Weighted
SVD (FWSVD). Although the factorized matrices from our approach do not re-
sult in smaller reconstruction errors, we ﬁnd that our resulting task accuracy is
much closer to the original model’s performance. We perform analysis with the
transformer-based language models, showing our weighted SVD largely allevi-
ates the mismatched optimization objectives and can maintain model performance
with a higher compression rate. Our method can directly compress a task-speciﬁc
model while achieving better performance than other compact model strategies
requiring expensive model pre-training. Moreover, the evaluation of compress-
ing an already compact model shows our method can further reduce 9% to 30%
parameters with an insigniﬁcant impact on task accuracy."
INTRODUCTION,0.005249343832020997,"1
INTRODUCTION"
INTRODUCTION,0.007874015748031496,"Language models built with transformers (Devlin et al., 2018) have attained extensive success in
natural language tasks such as language modeling (Radford et al., 2018), text classiﬁcation (Wang
et al., 2018), question answering (Rajpurkar et al., 2016), and summarization (Liu, 2019). The
success is achieved by ﬁne-tuning a big transformer model pre-trained with a large corpus. The target
task for ﬁne-tuning may only focus on a restricted scenario such as sentiment analysis (Socher et al.,
2013) and multiple-choice question inference (Zellers et al., 2018). Having a big transformer model
is often overkill for the target task and prohibits the model deployment to resource-constrained
hardware. Therefore, language model compression raises immense interest."
INTRODUCTION,0.010498687664041995,"The popular strategy creates a compact model from scratch (Jiao et al., 2019) or a subset of the big
model’s layers (Sun et al., 2019; Sanh et al., 2019), then pre-trains with a large corpus and distills
knowledge from the big model. This process is called generic pre-training (Wang et al., 2020b; Sun
et al., 2019; Sanh et al., 2019) and is necessary for a compact model to achieve good performance
on the target tasks. However, the generic pre-training could still cost considerable computational
resources. For example, it takes 384 NVIDIA V100 GPU hours to get the pre-trained TinyBERT
(Jiao et al., 2019) on the Wiki corpus dataset. So it may not be affordable for everyone who wants
to create a compact model. In contrast, another line of strategy, speciﬁcally low-rank factorization
(Golub & Reinsch, 1971; Noach & Goldberg, 2020), can potentially reduce a big model’s parameters"
INTRODUCTION,0.013123359580052493,*These authors contributed equally to this work.
INTRODUCTION,0.015748031496062992,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01837270341207349,"without the generic pre-training. Since the factorization aims to approximate the learned model
parameters, the method has the nature of directly inheriting the knowledge of the big trained model."
INTRODUCTION,0.02099737532808399,"However, approximating the learned weights with standard factorization often loses most of the
task performance. This work investigates this issue with the most popular strategy, which uses
singular value decomposition (SVD) to compress the learned model weights. With SVD, the learned
matrix is factorized into three matrices (U, S, V ). The portion associated with small singular values
will be truncated to produce a smaller version of factorized matrices. The multiplication of these
smaller matrices will approximate the original one with fewer total parameters to achieve the model
compression. In other words, SVD minimizes the reconstruction error with fewer parameters as its
objective. However, this objective does not necessarily correlate to the ultimate goal of keeping task
performance. Speciﬁcally, the SVD algorithm is biased to reconstruct the parameters associated
with large singular values. As a result, the parameters mainly reconstructed by the ranks with small
singular values will become the victim in the compression process. Are these victimized parameters
less critical to achieving a good task performance? We argue that this is not true, and the optimization
objective of SVD is not properly aligned with the target task objective. This paper is the ﬁrst work
to provide an empirical analysis of this issue, proposing a novel weighted SVD to mitigate it."
INTRODUCTION,0.023622047244094488,"Our weighted SVD addresses the above issue by assigning importance scores to the parameters. This
score has to correlate to how much the task performance is affected by the parameter change. The
Fisher information nicely ﬁts into this purpose (Pascanu & Bengio, 2014). Besides, the calculation
of Fisher information is usually simpliﬁed to accumulating a parameter’s squared gradient over the
training dataset based on its task objective (e.g.cross-entropy, regression error, etc.), conveniently
providing the importance of each parameter in a model. Then we modify the optimization objective
of factorization (i.e., reconstruction error) by multiplying it with Fisher information, providing a
new objective that jointly considers matrix reconstruction error and the target task objective."
INTRODUCTION,0.026246719160104987,"In summary, this work makes the following contributions: (1) we analyze the issue of mismatched
objectives between factorization and the target task for model compression; (2) we propose a novel
compression strategy with the SVD weighted by the Fisher information; (3) we perform extensive
analysis on varied language tasks, showing our Fisher-weighted SVD can compress an already com-
pact model, and it can achieve comparable compression rate and performance with methods that
require an expensive generic model pre-training."
BACKGROUND,0.028871391076115485,"2
BACKGROUND"
MODEL COMPRESSION WITH LOW-RANK APPROXIMATION,0.031496062992125984,"2.1
MODEL COMPRESSION WITH LOW-RANK APPROXIMATION"
MODEL COMPRESSION WITH LOW-RANK APPROXIMATION,0.03412073490813648,"Given a matrix W ∈RN×M, the low-rank approximation is achieved via singular value decompo-
sition (SVD):
W ≈USV T ,
(1)
where U ∈RN×r, V ∈RM×r, and k is the rank of matrix W. S is a diagonal matrix of non-
zero singular values diag(σ1, , ..., σr), where σ1 ≥σ2 ≥· · · σr ≥· · · σk > 0. The low-rank
approximation with targeted rank r is obtained by setting zeros to σr+1, ..., σk."
MODEL COMPRESSION WITH LOW-RANK APPROXIMATION,0.03674540682414698,"Given input data X ∈R1×N, a linear layer in neural networks is represented below with the weight
matrix W ∈RN×M and bias b ∈R1×M:"
MODEL COMPRESSION WITH LOW-RANK APPROXIMATION,0.03937007874015748,"Z = XW + b ≈(XUS)V T + b.
(2)"
MODEL COMPRESSION WITH LOW-RANK APPROXIMATION,0.04199475065616798,"Factorizing W with Equation (1) leads to Equation (2), which can be implemented with two smaller
linear layers: 1) The ﬁrst layer has Nr parameters without bias. Its weight matrix is US. 2) The
second layer has Mr parameters plus bias. Its weight matrix and bias are V and b, correspondingly.
The total number of parameters for approximating W is Nr + Mr. In the case of full rank matrix
and M = N, the model size is reduced when r < 0.5N. For example, if we set r to reserve
the largest 30% singular values, the method will reduce about 40% of the parameters from W. In
general, the reduced size will be NM −(Nr + Mr)."
MODEL COMPRESSION WITH LOW-RANK APPROXIMATION,0.04461942257217848,"Low rank approximation in neural networks has been extensively studied (Jaderberg et al., 2014;
Zhang et al., 2015; Denton et al., 2014). In more recent works, SVD is often applied to compress
the word embedding layer (Chen et al., 2018a; Acharya et al., 2019). Noach & Goldberg (2020)"
MODEL COMPRESSION WITH LOW-RANK APPROXIMATION,0.047244094488188976,Published as a conference paper at ICLR 2022
MODEL COMPRESSION WITH LOW-RANK APPROXIMATION,0.049868766404199474,"applies SVD to the transformer layers, but it does not investigate why SVD gives a very poor result
without ﬁne-tuning. Our work explores this issue and provides a weighted version to address it."
FISHER INFORMATION,0.05249343832020997,"2.2
FISHER INFORMATION"
FISHER INFORMATION,0.05511811023622047,"The Fisher information measures the amount of information that an observable dataset D carries
about a model parameter w. The computation of its exact form is generally intractable since it
requires marginalizing over the space of D, which includes data and its labels. Therefore, most of
the previous works estimate its empirical Fisher information:"
FISHER INFORMATION,0.05774278215223097,"Iw = E "" ∂"
FISHER INFORMATION,0.06036745406824147,"∂w log p(D|w)
2#"
FISHER INFORMATION,0.06299212598425197,"≈
1
|D| |D|
X i=1  ∂"
FISHER INFORMATION,0.06561679790026247,"∂wL(di; w)
2
= ˆIw.
(3)"
FISHER INFORMATION,0.06824146981627296,"The estimated information ˆIw accumulates the squared gradients over the training data di ∈D,
where L is the target task objective (e.g., cross-entropy for a classiﬁcation task, or mean squared
error for a regression task). This approximation provides a straight intuition: the parameters that
change the task objective with a large absolute gradient are important to the target task; therefore,
those parameters should be reconstructed better than others in the compression process."
FISHER INFORMATION,0.07086614173228346,"The above estimation of Fisher information computes only the ﬁrst-order derivatives and has been
shown to measure the importance of parameters effectively. Kirkpatrick et al. (2017) and Hua et al.
(2021) use it to avoid the model catastrophic forgetting in a continual learning scenario. Liu et al.
(2021) and Molchanov et al. (2019) use it or a similar variant to help the structured model pruning.
However, no previous work has explored its potential in assisting SVD for model compression."
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.07349081364829396,"3
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.07611548556430446,"The singular values in S implicitly give an importance score for a group of parameters. Since the
small singular values will be truncated ﬁrst, those parameters affected by the truncation are expected
to be not important for the task performance. We verify the above assumption with a brute force
attack: truncate one singular value at a time, then reconstruct the matrix, put it into a model, evaluate
and get its performance. Ideally, we hope to see less performance drop when we truncate the smaller
singular values. This process can be written as having the reconstructed model weights ¯Wi with the
i-th singular value be truncated:"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.07874015748031496,"¯Wi = u1σ1vT
1 + ... + ui−1σi−1vT
i−1 + ui+1σi+1vT
i+1 + ... + ukσkvT
k ,
(4)"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.08136482939632546,"where ui and vi are the i-th column in U and V , correspondingly."
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.08398950131233596,"Applying this brute force attack to test a deep neural network is not straightforward since a model
can have hundreds of linear layers. Therefore, we truncate a group of singular values together instead
of only one. Speciﬁcally, we split the singular values of a layer into 10 groups sorted by their values.
The 1st group has the top 10% singular values, while the 10th group contains the smallest 10%"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.08661417322834646,Rank Group
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.08923884514435695,Performance Drop 0.0 0.1 0.2 0.3
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.09186351706036745,"1
2
3
4
5
6
7
8
9
10"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.09448818897637795,"SVD
Ideal"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.09711286089238845,"Figure 1: The grouped truncation and its performance. The truncation of the 10th group, which has
the smallest singular values resulting from SVD, is expected to have a minor performance impact
(i.e., follow the ideal trend of red dashed line), but this may not be true in actual cases (blue bar)."
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.09973753280839895,"Published as a conference paper at ICLR 2022 W
≈
U S
VT"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.10236220472440945,"×
×
=
W’
Poorly reconstructed"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.10498687664041995,parameters
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.10761154855643044,"Important
parameters"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.11023622047244094,"Truncated 
parameters"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.11286089238845144,"Figure 2: The dilemma of vanilla SVD. Some parameters (the overlap of meshed orange and green)
that signiﬁcantly impact the task performance may not be reconstructed well by SVD because their
associated singular values are small and truncated."
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.11548556430446194,"W
≈
×
×
U*"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.11811023622047244,"S*
V*T"
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.12073490813648294,"×
W’
="
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.12335958005249344,"Figure 3: The schematic effect of our Fisher-Weighted SVD (FWSVD). ˆI is a diagonal matrix
containing estimated Fisher information of parameters. By involving Fisher information to weigh
the importance, our method reduces the overlap between meshed orange and green, making less
performance drop after truncation."
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.12598425196850394,"values. When we truncate a speciﬁc group, e.g., 5th group, the 5th group of all the layers in a model
are truncated together. In other words, we observe the summed impact in a rank group. This results
in a smoothed trend for the observation."
MODEL COMPRESSION WITH SVD MAY LOSE PERFORMANCE QUICKLY,0.12860892388451445,"Figure 1 plots the result of truncating the 10 groups separately in a standard 12-layer BERT model
(Devlin et al., 2018) trained for STS-B task (Cer et al., 2017). The red dashed line shows an ideal
trend which has a smaller performance drop with the tail groups. The blue bars show the actual
performance drop. The 10th group surprisingly caused a performance drop as large as the 2nd
group. This means the parameters associated with the 10th group are as important as the 2nd group.
However, the magnitude of singular value does not reﬂect this importance, causing a model to lose
its performance quickly even when truncating only a small portion."
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.13123359580052493,"4
FISHER-WEIGHTED LOW-RANK APPROXIMATION"
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.13385826771653545,"The issue in Section 3 has an intuitive cause: the optimization objective of SVD does not consider
each parameter’s impact on the task performance. This issue is illustrated in Figure 2, and we address
it by introducing the Fisher information into SVD’s optimization objective, described as below."
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.13648293963254593,"In the generic low-rank approximation, its objective minimizes ||W −AB||2. SVD can solve this
problem efﬁciently by having A = US and B = V T . Since we can obtain the importance of each
element Wij in W, we weigh the individual reconstruction error by multiplying with the estimated
Fisher information ˆIWij:"
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.13910761154855644,"min
A,B X"
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.14173228346456693,"i,j
ˆIWij(Wij −(AB)ij)2.
(5)"
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.14435695538057744,"In general, weighted SVD does not have a closed-form solution (Srebro & Jaakkola, 2003) when
each element has its weight. To make our method easy to deploy and analyze, we propose a simpli-
ﬁcation by making the same row of the W matrix to share the same importance. The importance for
the row i is deﬁned to be the summation of the row, i.e., ˆIWi = P"
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.14698162729658792,"j
ˆIWij."
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.14960629921259844,"Deﬁne the diagonal matrix ˆI = diag(
q"
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.15223097112860892,"ˆIW1, ...,
q"
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.15485564304461943,"ˆIWN ), then the optimization problem of Equation
(5) can be written as:"
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.15748031496062992,"min
A,B ||ˆIW −ˆIAB||2.
(6)"
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.16010498687664043,Published as a conference paper at ICLR 2022
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.16272965879265092,"Equation 6 can be solved by the standard SVD on ˆIW.
We use the notation svd(ˆIW) =
(U ∗, S∗, V ∗), then the solution of Equation (6) will be A = ˆI−1U ∗S∗, and B = V ∗T . In other
words, the solution is the result of removing the information ˆI from the factorized matrices. Figure
3 illustrates this process and its schematic effect of reducing the overlap between important param-
eters and poorly reconstructed parameters. We will measure this overlap with the performance drop
analysis of Section 3. Lastly, to compress W, we will have A = ˆI−1U ∗
r S∗
r, and B = V ∗T
r
, where r
denotes the truncated U ∗, S∗, and V ∗with reserving only r ranks."
FISHER-WEIGHTED LOW-RANK APPROXIMATION,0.16535433070866143,"We call the above method FWSVD in this paper. One thing to highlight is that since we share the
same optimization process with the standard SVD, any advantage we observed will be the result of
a direct contribution from the ˆI in Equation (6)."
EXPERIMENTS,0.1679790026246719,"5
EXPERIMENTS"
THE PATHS TO A COMPRESSED LANGUAGE MODEL,0.17060367454068243,"5.1
THE PATHS TO A COMPRESSED LANGUAGE MODEL"
THE PATHS TO A COMPRESSED LANGUAGE MODEL,0.1732283464566929,"This section describes how we obtain a compressed model under the popular pre-training schemes
of language models. Figure 4 illustrates three paths that we examined for creating compressed
language models. All the paths start from retraining a large transformer-based model pre-trained
with a large language corpus in a self-supervised way, called the generic pre-training (L →Lg)."
THE PATHS TO A COMPRESSED LANGUAGE MODEL,0.17585301837270342,"The path-1 (S →Sg →St) is a popular scheme that creates a small model ﬁrst, then performs
the generic distillation for the small model to learn the knowledge of the large model. The resulting
small generic model Sg will be ﬁne-tuned with the target task dataset to obtain the task-speciﬁc
model St. The representative works of path-1 include DistilBERT (Sanh et al., 2019), TinyBERT
(Jiao et al., 2019), MobileBERT (Sun et al., 2020), and MiniLM v1/v2 (Wang et al., 2020b;a). Some
previous methods may include task-speciﬁc distillation (Lt →St) and data augmentation (Jiao
et al., 2019), but we exclude those from the scheme (and all the experiments in this paper) to make a
fair and clean comparison across methods. The task-speciﬁc distillation and data augmentation are
orthogonal to all the methods and can be jointly applied with low-rank factorization to make further
improvements."
THE PATHS TO A COMPRESSED LANGUAGE MODEL,0.1784776902887139,"The path-2 (Lg →Lt →Ltf) avoids the costly generic pre-training, directly compresses the task-
speciﬁc model with factorization and task-speciﬁc ﬁne-tuning (optional). Our analysis for the mis-
matched objectives phenomenon is based on this path. We also compare the models from path-1 and
path-2, showing that path-2 can generate a model with a comparable performance under the same
compression rate. Although path-2 requires much less training than path-1 (no generic pre-training
for the compressed model)."
THE PATHS TO A COMPRESSED LANGUAGE MODEL,0.18110236220472442,"The path-3 (St →Stf) is a challenging setting that aims to compress an already compact model.
This setting examines whether FWSVD can further improve the compression rate on models ob-
tained by path-1. Our experiments show the answer is yes."
THE PATHS TO A COMPRESSED LANGUAGE MODEL,0.1837270341207349,"With the three compression paths, we make four examinations as follows. Section 5.3: the compar-
ison of path-1 versus path-2; Section 5.4: the compression of an already compact model (path-3);
Section 5.5: the detailed comparison between FWSVD and vanilla SVD; Section 5.5.1: the empiri-
cal evidence for the schematic effects illustrated in Figures 2 and 3."
EXPERIMENT SETUP,0.18635170603674542,"5.2
EXPERIMENT SETUP"
LANGUAGE TASKS AND DATASETS,0.1889763779527559,"5.2.1
LANGUAGE TASKS AND DATASETS"
LANGUAGE TASKS AND DATASETS,0.19160104986876642,"We evaluate the methods of all three paths in Figure 4 on the General Language Understanding Eval-
uation (GLUE) benchmark (Wang et al., 2019) and a token classiﬁcation task. We include 2 single
sentence tasks: CoLA (Warstadt et al., 2018) measured in Matthew’s correlation, SST2 (Socher
et al., 2013) measured in classiﬁcation accuracy; 3 sentence similarity tasks: MRPC (Dolan et al.,
2005) measured in F-1 score, STS-B (Cer et al., 2017) measured in Pearson-Spearman correlation,
QQP (Chen et al., 2018b) measured in F-1 score; and 3 natural language inference tasks: MNLI
(Williams et al., 2018) measured in classiﬁcation accuracy with the average of the matched and mis-
matched subsets, QNLI (Rajpurkar et al., 2016) measured in accuracy. The token classiﬁcation task"
LANGUAGE TASKS AND DATASETS,0.1942257217847769,"Published as a conference paper at ICLR 2022 ❷ ❶ ❷ ❸
❶"
LANGUAGE TASKS AND DATASETS,0.1968503937007874,Distillation
LANGUAGE TASKS AND DATASETS,0.1994750656167979,"L
Lg
Lt
Ltf"
LANGUAGE TASKS AND DATASETS,0.2020997375328084,"Generic 
pre-training"
LANGUAGE TASKS AND DATASETS,0.2047244094488189,"S
Sg
St
Stf"
LANGUAGE TASKS AND DATASETS,0.2073490813648294,"Task 
fine-tuning"
LANGUAGE TASKS AND DATASETS,0.2099737532808399,"Factorization & 
fine-tuning (optional)"
LANGUAGE TASKS AND DATASETS,0.2125984251968504,Distillation
LANGUAGE TASKS AND DATASETS,0.2152230971128609,"L
: Large model"
LANGUAGE TASKS AND DATASETS,0.2178477690288714,"S
: Small model"
LANGUAGE TASKS AND DATASETS,0.2204724409448819,": Compression path 
  (previous work)"
LANGUAGE TASKS AND DATASETS,0.2230971128608924,: Added step of this work
LANGUAGE TASKS AND DATASETS,0.22572178477690288,: Step of previous works
LANGUAGE TASKS AND DATASETS,0.2283464566929134,": Compression path 
  (this work)"
LANGUAGE TASKS AND DATASETS,0.23097112860892388,"Figure 4: The three paths to create compressed language models are examined in this paper. L/S
denote the initial models, Lg/Sg are models after generic pre-training, Lt/St correspond to task-
speciﬁc models, and Ltf/Stf are factorized task-speciﬁc models. Detailed elaborations are in Sec-
tions 5.1 nad 5.2.2"
LANGUAGE TASKS AND DATASETS,0.2335958005249344,"Table 1: Results of CoNLL and GLUE benchmark. G-Avg means the average of GLUE tasks,
A-Avg denotes the average of all tasks, including CoNLL. Our FWSVD+ﬁne-tuning is the best
performer in terms of both average scores, without the expensive generic pre-training required by
path-1 models (e.g., DistillBERT costs 720 V100 GPU hours for training)."
LANGUAGE TASKS AND DATASETS,0.23622047244094488,"Model
#Param CoNLL CoLA MNLI MRPC QNLI QQP SST-2 STS-B G-Avg A-Avg"
LANGUAGE TASKS AND DATASETS,0.2388451443569554,"Original BERTbase
109.5M
94.1
56.2
84.7
87.4
91.3
87.8
93.0
88.5
84.1
85.4"
LANGUAGE TASKS AND DATASETS,0.24146981627296588,"Path-1
DistilBERT
67.0M
93.2
49.8
82.2
88.7
89.3
86.7
90.4
86.1
81.9
83.3
MiniLMv2
67.0M
92.2
43.3
84.0
89.1
90.6
86.7
91.4
88.1
81.9
83.2"
LANGUAGE TASKS AND DATASETS,0.2440944881889764,Path-2
LANGUAGE TASKS AND DATASETS,0.24671916010498687,"BERT-PKD
67.0M
−
45.5
81.3
85.7
88.4
88.4
91.3
86.2
81.0
−
BERT+SVD
66.5M
12.0
2.7
35.6
61.4
37.2
60.0
76.7
26.8
42.9
39.0
+ﬁne-tuning 66.5M
92.4
40.5
82.8
84.1
89.6
87.3
90.9
85.7
80.1
81.6
BERT+FWSVD 66.5M
49.6
13.5
52.8
81.2
52.2
65.7
82.1
68.6
59.4
58.2
+ﬁne-tuning 66.5M
93.2
49.4
83.0
88.0
89.5
87.6
91.2
87.0
82.2
83.6"
LANGUAGE TASKS AND DATASETS,0.24934383202099739,"we used is the named entity recognition (NER) on the CoNLL-2003 dataset (Sang & De Meulder,
2003). In summary, our evaluation includes 8 different natural language tasks."
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.25196850393700787,"5.2.2
IMPLEMENTATION DETAILS AND THE BASELINE MODELS"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.2545931758530184,"First of all, we use the same training conﬁguration for all the experiments in this paper and avoid
any hyperparameter screening to ensure a fair comparison."
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.2572178477690289,"For the SOTA models on path-1 (MiniLMv2 and DistilBERT), we use the pre-trained generic com-
pact models (Sg) provided by the original authors as the starting point, then directly ﬁne-tune them
with 3 epochs on the target task training data. The ﬁne-tuning is optimized by Adam with learning
rate of 2 × 10−5 and batch size of 32 on one GPU."
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.25984251968503935,"For the methods on path-2 (FWSVD and SVD), we start from the pre-trained generic large model
(Lg), which is the standard 12-layer BERT model (Devlin et al., 2018). Then we ﬁne-tune it with
the training setting exactly the same as we used for the path-1 models to get the large task-speciﬁc
models (Lt). The last step is applying the low-rank factorization (SVD or FWSVD) followed by
another ﬁne-tuning with the same training setting described above. The performance with and with-
out ﬁne-tuning will be both reported. We also note that we compress only the linear layers in the
transformer blocks by reserving only 33% of the ranks in this work. The setup intentionally makes
a fair comparison to the path-1 methods. In other words, we do not compress the non-transformer
modules such as the token embedding. Previous works (Chen et al., 2018a) have shown signiﬁcant
success in using low-rank factorization to compress the embedding layer, which occupies 23.4M
(21.3%) parameters in the standard BERT model. Therefore, the results we reported for the path-
2 methods still have room for improvement by applying our method to non-transformer modules.
Lastly, we add BERT-PKD (Sun et al., 2019) based on its reproduced results (Chen et al., 2018a) for
comparison. BERT-PKD uses knowledge distillation instead of factorization in the path-2 process."
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.26246719160104987,Published as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.2650918635170604,"Table 2: Results of compressing an already compact model. The original task-speciﬁc models are
directly downloaded from Huggingface pretrained models. Our FWSVD successfully reduces more
parameters from all the compact models, while achieving the same level of accuracy. (ft: ﬁne-tuning)"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.2677165354330709,"Original Compact Model (St)
Path-3 Compression (St →Stf)"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.27034120734908135,"Model-Task
#Param.
Perf.
#Param.
SVD
SVD+ft.
FWSVD
FWSVD+ft."
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.27296587926509186,"TinyBERT-STSB
14.4M (7.8x)
87.5
11.8M (-18%)
73.8
86.1
84.9
88.0
MiniLM-CoNLL
22.7M (4.8x)
88.5
18.4M (-19%)
12.5
88.0
70.1
88.6
MobileBERT-MNLI
24.6M (4.4x)
83.6
22.5M (-9%)
36.4
81.9
51.1
82.5
DistillBERT-MRPC
66.9M (1.6x)
88.7
46.7M (-30%)
0.0
83.4
67.9
89.0"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.2755905511811024,"Table 3: Results of compressing an already compact model. This table compresses ALBERT (Lan
et al., 2019), which uses the parameter-sharing strategy to create the compact model. FWSVD pre-
serves the performance signiﬁcantly better than SVD in all 8 tasks, indicating its excellent compati-
bility in combining the parameter-sharing strategy. This experiment examines the path-3 process."
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.2782152230971129,"Model
#Param
CoNLL CoLA MNLI MRPC QNLI QQP SST-2 STS-B G-Avg A-Avg"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.28083989501312334,"BERTbase
109.5M
94.1
56.2
84.7
87.4
91.3
87.8
93.0
88.5
84.1
85.4"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.28346456692913385,"ALBERTlarge
17.7M
93.5
50.9
84.3
89.9
91.7
86.7
90.7
90.1
83.5
84.7"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.28608923884514437,"w SVD
15.2M (-14%)
0.3
0.0
41.1
0.0
54.2
5.4
70.2
9.6
25.8
22.6
w SVD+ft.
15.2M (-14%)
92.2
46.4
83.4
81.8
49.5
86.9
89.8
86.8
74.9
77.1
w FWSVD
15.2M (-14%)
22.8
0.0
65.2
50.0
78.2
72.6
81.4
76.4
60.5
55.8
w FWSVD+ft. 15.2M (-14%)
93.0
50.6
83.3
90.4
90.6
87.0
90.6
89.0
83.1
84.3"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.2887139107611549,"ALBERTbase
11.7M
92.1
43.0
82.3
88.6
90.6
86.6
89.7
89.1
81.4
82.7"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.29133858267716534,"w SVD
9.6M (-18%)
3.5
0.0
32.0
0.0
55.1
53.4
52.4
9.6
28.9
25.7
w SVD+ft.
9.6M (-18%)
89.8
28.8
81.3
81.2
88.3
85.5
88.2
75.0
75.5
77.3
w FWSVD
9.6M (-18%)
16.9
6.9
55.6
47.7
69.1
54.7
72.9
54.0
51.6
47.2
w FWSVD+ft. 9.6M (-18%)
91.2
42.2
81.8
86.9
88.9
86.2
88.7
87.0
80.2
81.6"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.29396325459317585,"For the path-3 experiments, we use the pre-trained task-speciﬁc compact models (St) as the start-
ing point. These pre-trained models have a much smaller size (TinyBERT-STSB, MiniLM-CoNLL,
MobileBERT-MNLI) or a better performance (DistilBERT-MRPC) than the models we used in the
path-1 and path-2, indicating they may contain denser knowledge in their compact models. There-
fore, compressing these models introduces a signiﬁcant challenge. In order to have better coverage
for all tasks, we additionally use ALBERTlarge and ALBERTbase (Lan et al., 2019) as the already
compact models to generate all 8 task-speciﬁc models (Sg →St). Then follow path-3 to compress
the compact models. All the training involved here has the same setting as described in path-1."
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.29658792650918636,"Lastly, our implementation and experiments are built on top of the popular HuggingFace Transform-
ers library (Wolf et al., 2020). All other unspeciﬁed training settings use the default conﬁguration
of the library. Since no hyperparameter tuning is involved in our experiments, we directly report the
results on the dev set of all the datasets, making the numbers convenient to compare and verify."
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.2992125984251969,"5.3
PATH-1 VERSUS PATH-2"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.30183727034120733,"Table 1 reports the results of the GLUE benchmark and a NER task. Our FWSVD with ﬁne-tuning
achieves an average score of 83.6, beating all other path-1 and path-2 methods. This is a non-trivial
accomplishment since FWSVD with ﬁne-tuning does not need the expensive generic pre-training.
Furthermore, FWSVD has consistent performance retention for all the tasks; it contrasts the path-1
methods, which may have a more considerable variance. For example, DistilBERT is good at CoLA
but poor at STS-B; oppositely, MiniLMv2 is a strong performer at STS-B but is weak with CoLA.
In contrast, FWSVD+ﬁne-tuning does not show an obvious shortcoming."
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.30446194225721784,Published as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.30708661417322836,Rank ratio
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.30971128608923887,Matthews Correlation -0.20 0.00 0.20 0.40 0.60
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.3123359580052493,"0.2
0.4
0.6
0.8
1.0"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.31496062992125984,"SVD
FWSVD"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.31758530183727035,(a) COLA
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.32020997375328086,Rank ratio
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.3228346456692913,F1 Score 0.00 0.25 0.50 0.75 1.00
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.32545931758530183,"0.2
0.4
0.6
0.8
1.0"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.32808398950131235,"SVD
FWSVD"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.33070866141732286,(b) NER (CoNLL-2003)
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.3333333333333333,Rank ratio
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.3359580052493438,Pearson Correlation 0.00 0.25 0.50 0.75 1.00
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.33858267716535434,"0.2
0.4
0.6
0.8
1.0"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.34120734908136485,"SVD
FWSVD"
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.3438320209973753,(c) STS-B
IMPLEMENTATION DETAILS AND THE BASELINE MODELS,0.3464566929133858,"Figure 5: FWSVD versus SVD by varying the ratio of reserved ranks. The model with a rank ratio
1.0 indicates the full-rank reconstruction with the same accuracy as the original model (i.e., the Lt
in Figure 4). Note that all the models here do not have ﬁne-tuning after factorization."
COMPRESSING AN ALREADY COMPACT MODEL,0.34908136482939633,"5.4
COMPRESSING AN ALREADY COMPACT MODEL"
COMPRESSING AN ALREADY COMPACT MODEL,0.35170603674540685,"The setting of path-3 targets to further compress the lightweight models. This is challenging as
the compact models are already 1.6x ∼7.8x smaller than the original BERT. The results in Table
2 demonstrate the effectiveness of FWSVD on further reducing the number of parameters. The
original SVD is almost useless without ﬁne-tuning, while our FWSVD can still retain a signiﬁcant
part of the performance. For example, SVD ends with a zero accuracy when compressing Dis-
tillBERT, while our FWSVD keeps a score of 67.9 under the same setting. When combined with
ﬁne-tuning, FWSVD can cut off 30% redundancy for DistillBERT. Even for the highly compact
model TinyBERT (only 14.4M parameters), FWSVD+ﬁne-tuning still successfully reduces 18%
of the parameters without any performance loss. More interestingly, the TinyBERT, MiniLM, and
DistillBERT-MRPC compressed by FWSVD+ﬁne-tuning exceed the original performance slightly.
The result suggests FWSVD+ﬁne-tuning might introduce a small regularization effect to improve
the model’s generalizability."
COMPRESSING AN ALREADY COMPACT MODEL,0.3543307086614173,"Lastly, Table 3 examines the compatibility between SVD/FWSVD and the parameter-sharing strat-
egy of the ALBERT model. The average score of ALBERT-large is 84.7%. The performance of
FWSVD (84.3%) is far better than that of SVD (77.1%) when both reducing 14% parameters, sug-
gesting FWSVD is more robust than SVD in combining the parameter-sharing strategy."
FWSVD VERSUS SVD,0.3569553805774278,"5.5
FWSVD VERSUS SVD"
FWSVD VERSUS SVD,0.35958005249343833,"In Table 1, FWSVD consistently produces much better results than SVD on all tasks. On average,
FWSVD without ﬁne-tuning obtains an absolute improvement of 17.5% over SVD. To highlight,
FWSVD without ﬁne-tuning can maintain a signiﬁcant portion of performance in challenging tasks
such as CoNLL and STS-B, where SVD completely fails. With ﬁne-tuning, FWSVD provides better
initialization for ﬁne-tuning and consistently achieves a better or comparable performance."
FWSVD VERSUS SVD,0.36220472440944884,"Figure 5 plots the performance trend with respect to the change of targeted rank ratio, where the
full-rank reconstruction corresponds to the results at rank ratio 1.0. These results demonstrate the
apparent advantage of FWSVD over standard SVD. First, at each rank ratio, FWSVD shows signiﬁ-
cant improvements over SVD. Second, the performance of FWSVD keeps growing with the increase
of rank ratio, while SVD shows ﬂuctuations in its trend. Speciﬁcally, two tasks (COLA and STS-B)
in Figure 5 show that SVD has abrupt performance drops at some points. On the STS-B task, the
performance of SVD at rank ratio 0.3 is signiﬁcantly lower than having a smaller rank ratio of 0.2.
In contrast, FWSVD shows a much stable trend of increasing performance along with the rank ratio."
REVISIT THE BRUTE FORCE ATTACK,0.3648293963254593,"5.5.1
REVISIT THE BRUTE FORCE ATTACK"
REVISIT THE BRUTE FORCE ATTACK,0.3674540682414698,"This section applies the same analysis of Section 3, but adds FWSVD to see if it matches the task’s
objective better. In Figure 6a, the red bars are signiﬁcantly lower than the blue bars, especially
for the tail groups, which will be truncated ﬁrst. We speciﬁcally highlight group-10 in 6a, which
has the smallest 10% singular values. The height of the blue bar is equivalent to the size of the
overlapped (green and meshed orange) region in Figures 2. Similarly, its red bar (close to zero) is
equivalent to the overlapped region in Figure 3. In other words, the illustrations of Figures 2 and
3 are strongly supported by the results here. Although FWSVD shows a smaller performance drop"
REVISIT THE BRUTE FORCE ATTACK,0.3700787401574803,Published as a conference paper at ICLR 2022
REVISIT THE BRUTE FORCE ATTACK,0.37270341207349084,Rank Group
REVISIT THE BRUTE FORCE ATTACK,0.3753280839895013,Performance Drop 0.0 0.1 0.2 0.3
REVISIT THE BRUTE FORCE ATTACK,0.3779527559055118,"1
2
3
4
5
6
7
8
9
10"
REVISIT THE BRUTE FORCE ATTACK,0.3805774278215223,"SVD
FWSVD"
REVISIT THE BRUTE FORCE ATTACK,0.38320209973753283,(a) STS-B performance drop
REVISIT THE BRUTE FORCE ATTACK,0.3858267716535433,Rank Group
REVISIT THE BRUTE FORCE ATTACK,0.3884514435695538,Relative Reconstruction Error 0 2 4 6
REVISIT THE BRUTE FORCE ATTACK,0.3910761154855643,"1
2
3
4
5
6
7
8
9
10"
REVISIT THE BRUTE FORCE ATTACK,0.3937007874015748,"SVD
FWSVD"
REVISIT THE BRUTE FORCE ATTACK,0.3963254593175853,(b) STS-B reconstruction error
REVISIT THE BRUTE FORCE ATTACK,0.3989501312335958,"Figure 6: Results of grouped rank truncation on STS-B task. In (a), FWSVD shows a consis-
tent trend of having less performance drop with the small singular value groups (group 10 has the
smallest singular values), mitigating the issue of Figure 1. In (b), FWSVD results in a larger recon-
struction error with almost all truncated groups, although FWSVD retains the model accuracy better
than SVD."
REVISIT THE BRUTE FORCE ATTACK,0.4015748031496063,"shown by Figure 6a, it has a more signiﬁcant reconstruction error than SVD in many cases (see
Figure 6b), especially with the rank groups that will be truncated ﬁrst (e.g., groups 5 to 10). In
other words, FWSVD’s objective (Equation 6) aligns with the task objective better by sacriﬁcing the
reconstruction error."
LIMITATION AND FUTURE WORK,0.4041994750656168,"6
LIMITATION AND FUTURE WORK"
LIMITATION AND FUTURE WORK,0.4068241469816273,"FWSVD has two limitations. First, FWSVD relies on a given task objective and a target task training
dataset to compute the importance matrix; thus, it is more proper to compress a task-speciﬁc model
(e.g., Lt or St) than the pre-trained generic model (e.g., Lg). In contrast, the vanilla SVD can apply
to any case. In other words, FWSVD trades the method’s applicability for the target task perfor-
mance. Second, FWSVD only uses a simpliﬁed importance matrix that gives the same importance
for the parameters on the same row of matrix W. Although this strategy is simple and effective,
it does not fully utilize the Fisher information. Therefore, a future improvement can be made by
directly seeking an element-wise factorization solution for Equation (5)."
CONCLUSION,0.4094488188976378,"7
CONCLUSION"
CONCLUSION,0.4120734908136483,"In this work, we investigate why using standard low-rank factorization (SVD) to compress the model
may quickly lose most of its performance, pointing out the issue of the mismatched optimization
objectives between the low-rank approximation and the target task. We provide empirical evidence
and observations for the issue, and propose a new strategy, FWSVD, to alleviate it. Our FWSVD
uses the estimated Fisher information to weigh the importance of parameters for the factorization
and achieve signiﬁcant success in compressing an already compact model. Furthermore, FWSVD
reuses the existing SVD solver and can still implement its factorized matrices with linear layers;
therefore, it is simple to implement and deploy. We believe FWSVD could be one of the most
easy-to-use methods with good performance for language model compression."
REFERENCES,0.4146981627296588,REFERENCES
REFERENCES,0.41732283464566927,"Anish Acharya, Rahul Goel, Angeliki Metallinou, and Inderjit Dhillon. Online embedding com-
pression for text classiﬁcation using low rank matrix factorization. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, volume 33, pp. 6196–6203, 2019."
REFERENCES,0.4199475065616798,"Daniel Cer, Mona Diab, Eneko Agirre, I˜nigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task
1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of
the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14, Vancouver,
Canada, August 2017. Association for Computational Linguistics."
REFERENCES,0.4225721784776903,Published as a conference paper at ICLR 2022
REFERENCES,0.4251968503937008,"Patrick H Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-Jui Hsieh. Groupreduce: block-wise
low-rank approximation for neural language model shrinking. In Proceedings of the 32nd Inter-
national Conference on Neural Information Processing Systems, pp. 11011–11021, 2018a."
REFERENCES,0.42782152230971127,"Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs. 2018b."
REFERENCES,0.4304461942257218,"Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear
structure within convolutional networks for efﬁcient evaluation. In Advances in neural informa-
tion processing systems, pp. 1269–1277, 2014."
REFERENCES,0.4330708661417323,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.4356955380577428,"Bill Dolan, Chris Brockett, and Chris Quirk.
Microsoft research paraphrase corpus.
Retrieved
March, 29(2008):63, 2005."
REFERENCES,0.43832020997375326,"Gene H Golub and Christian Reinsch. Singular value decomposition and least squares solutions. In
Linear algebra, pp. 134–151. Springer, 1971."
REFERENCES,0.4409448818897638,"Ting Hua, Yilin Shen, Changsheng Zhao, Yen-Chang Hsu, and Hongxia Jin. Hyperparameter-free
continuous learning for domain classiﬁcation in natural language understanding. In Proceedings
of the 2021 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pp. 2669–2678, 2021."
REFERENCES,0.4435695538057743,"Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks
with low rank expansions. arXiv preprint arXiv:1405.3866, 2014."
REFERENCES,0.4461942257217848,"Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351,
2019."
REFERENCES,0.44881889763779526,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Over-
coming catastrophic forgetting in neural networks. volume 114, pp. 3521–3526. National Acad
Sciences, 2017."
REFERENCES,0.45144356955380577,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942, 2019."
REFERENCES,0.4540682414698163,"Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin
Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang. Group ﬁsher pruning for practical
network compression. In International Conference on Machine Learning, pp. 7021–7032. PMLR,
2021."
REFERENCES,0.4566929133858268,"Yang Liu. Fine-tune bert for extractive summarization. arXiv preprint arXiv:1903.10318, 2019."
REFERENCES,0.45931758530183725,"Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation
for neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 11264–11272, 2019."
REFERENCES,0.46194225721784776,"Matan Ben Noach and Yoav Goldberg. Compressing pre-trained language models by matrix de-
composition. In Proceedings of the 1st Conference of the Asia-Paciﬁc Chapter of the Association
for Computational Linguistics and the 10th International Joint Conference on Natural Language
Processing, pp. 884–889, 2020."
REFERENCES,0.4645669291338583,"Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In In Interna-
tional Conference on Learning Representations (ICLR), 2014."
REFERENCES,0.4671916010498688,"Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing with unsupervised learning. 2018."
REFERENCES,0.46981627296587924,"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing, pp. 2383–2392, 2016."
REFERENCES,0.47244094488188976,Published as a conference paper at ICLR 2022
REFERENCES,0.47506561679790027,"Erik Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings of the Seventh Conference on Natural
Language Learning at HLT-NAACL 2003, pp. 142–147, 2003."
REFERENCES,0.4776902887139108,"Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019."
REFERENCES,0.48031496062992124,"Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing, pp. 1631–1642, 2013."
REFERENCES,0.48293963254593175,"Nathan Srebro and Tommi Jaakkola. Weighted low-rank approximations. In Proceedings of the 20th
International Conference on Machine Learning (ICML-03), pp. 720–727, 2003."
REFERENCES,0.48556430446194226,"Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu.
Patient knowledge distillation for bert model
compression. arXiv preprint arXiv:1908.09355, 2019."
REFERENCES,0.4881889763779528,"Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobile-
bert: a compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984,
2020."
REFERENCES,0.49081364829396323,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461, 2018."
REFERENCES,0.49343832020997375,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. In 7th
International Conference on Learning Representations, ICLR 2019, 2019."
REFERENCES,0.49606299212598426,"Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei.
Minilmv2:
Multi-
head self-attention relation distillation for compressing pretrained transformers. arXiv preprint
arXiv:2012.15828, 2020a."
REFERENCES,0.49868766404199477,"Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-
attention distillation for task-agnostic compression of pre-trained transformers. arXiv preprint
arXiv:2002.10957, 2020b."
REFERENCES,0.5013123359580053,"Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.
2018."
REFERENCES,0.5039370078740157,"Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT
2018, pp. 1112–1122. Association for Computational Linguistics (ACL), 2018."
REFERENCES,0.5065616797900262,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-
ger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art
natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. As-
sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/
2020.emnlp-demos.6."
REFERENCES,0.5091863517060368,"Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial
dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326, 2018."
REFERENCES,0.5118110236220472,"Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional
networks for classiﬁcation and detection. IEEE transactions on pattern analysis and machine
intelligence, 38(10):1943–1955, 2015."
REFERENCES,0.5144356955380578,Published as a conference paper at ICLR 2022
REFERENCES,0.5170603674540682,"A
SUPPLEMENTARY"
REFERENCES,0.5196850393700787,Rank Group
REFERENCES,0.5223097112860893,Performance Drop 0.00%
REFERENCES,0.5249343832020997,10.00%
REFERENCES,0.5275590551181102,20.00%
REFERENCES,0.5301837270341208,30.00%
REFERENCES,0.5328083989501312,40.00%
REFERENCES,0.5354330708661418,50.00%
REFERENCES,0.5380577427821522,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.5406824146981627,"SVD
FWSVD
CoLA (ALBERT base) (a)"
REFERENCES,0.5433070866141733,Rank Group
REFERENCES,0.5459317585301837,Performance Drop 0.00%
REFERENCES,0.5485564304461942,20.00%
REFERENCES,0.5511811023622047,40.00%
REFERENCES,0.5538057742782152,60.00%
REFERENCES,0.5564304461942258,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.5590551181102362,"SVD
FWSVD
MNLI (ALBERT base) (b)"
REFERENCES,0.5616797900262467,Rank Group
REFERENCES,0.5643044619422573,Performance Drop 0.00%
REFERENCES,0.5669291338582677,10.00%
REFERENCES,0.5695538057742782,20.00%
REFERENCES,0.5721784776902887,30.00%
REFERENCES,0.5748031496062992,40.00%
REFERENCES,0.5774278215223098,50.00%
REFERENCES,0.5800524934383202,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.5826771653543307,"SVD
FWSVD
QNLI (ALBERT base) (c)"
REFERENCES,0.5853018372703412,Rank Group
REFERENCES,0.5879265091863517,Performance Drop 0.00%
REFERENCES,0.5905511811023622,25.00%
REFERENCES,0.5931758530183727,50.00%
REFERENCES,0.5958005249343832,75.00%
REFERENCES,0.5984251968503937,100.00%
REFERENCES,0.6010498687664042,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.6036745406824147,"SVD
FWSVD
STS-B (ALBERT base) (d)"
REFERENCES,0.6062992125984252,Rank Group
REFERENCES,0.6089238845144357,Performance Drop 0.00%
REFERENCES,0.6115485564304461,25.00%
REFERENCES,0.6141732283464567,50.00%
REFERENCES,0.6167979002624672,75.00%
REFERENCES,0.6194225721784777,100.00%
REFERENCES,0.6220472440944882,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.6246719160104987,"SVD
FWSVD
MRPC (ALBERT base) (e)"
REFERENCES,0.6272965879265092,Rank Group
REFERENCES,0.6299212598425197,Performance Drop 0.00%
REFERENCES,0.6325459317585301,25.00%
REFERENCES,0.6351706036745407,50.00%
REFERENCES,0.6377952755905512,75.00%
REFERENCES,0.6404199475065617,100.00%
REFERENCES,0.6430446194225722,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.6456692913385826,"SVD
FWSVD
NER-CoNLL2003 (ALBERT base) (f)"
REFERENCES,0.6482939632545932,Rank Group
REFERENCES,0.6509186351706037,Performance Drop 0.00%
REFERENCES,0.6535433070866141,25.00%
REFERENCES,0.6561679790026247,50.00%
REFERENCES,0.6587926509186351,75.00%
REFERENCES,0.6614173228346457,100.00%
REFERENCES,0.6640419947506562,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.6666666666666666,"SVD
FWSVD
QQP (ALBERT base) (g)"
REFERENCES,0.6692913385826772,Rank Group
REFERENCES,0.6719160104986877,Performance Drop 0.00%
REFERENCES,0.6745406824146981,10.00%
REFERENCES,0.6771653543307087,20.00%
REFERENCES,0.6797900262467191,30.00%
REFERENCES,0.6824146981627297,40.00%
REFERENCES,0.6850393700787402,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.6876640419947506,"SVD
FWSVD
SST-2 (ALBERT base) (h)"
REFERENCES,0.6902887139107612,"Figure 7: The grouped rank truncation experiment. The experiments are the same with Figure 6a,
but we use ALBERTbase (11.7M parameters) model for this ﬁgure."
REFERENCES,0.6929133858267716,Published as a conference paper at ICLR 2022
REFERENCES,0.6955380577427821,Rank Group
REFERENCES,0.6981627296587927,Performance Drop 0.00%
REFERENCES,0.7007874015748031,20.00%
REFERENCES,0.7034120734908137,40.00%
REFERENCES,0.7060367454068242,60.00%
REFERENCES,0.7086614173228346,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.7112860892388452,"SVD
FWSVD
CoLA (BERT base) (a)"
REFERENCES,0.7139107611548556,Rank Group
REFERENCES,0.7165354330708661,Performance Drop 0.00%
REFERENCES,0.7191601049868767,20.00%
REFERENCES,0.7217847769028871,40.00%
REFERENCES,0.7244094488188977,60.00%
REFERENCES,0.7270341207349081,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.7296587926509186,"SVD
FWSVD
MNLI (BERT base) (b)"
REFERENCES,0.7322834645669292,Rank Group
REFERENCES,0.7349081364829396,Performance Drop 0.00%
REFERENCES,0.7375328083989501,10.00%
REFERENCES,0.7401574803149606,20.00%
REFERENCES,0.7427821522309711,30.00%
REFERENCES,0.7454068241469817,40.00%
REFERENCES,0.7480314960629921,50.00%
REFERENCES,0.7506561679790026,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.7532808398950132,"SVD
FWSVD
QNLI (BERT base) (c)"
REFERENCES,0.7559055118110236,Rank Group
REFERENCES,0.7585301837270341,Performance Drop 0.00%
REFERENCES,0.7611548556430446,10.00%
REFERENCES,0.7637795275590551,20.00%
REFERENCES,0.7664041994750657,30.00%
REFERENCES,0.7690288713910761,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.7716535433070866,"SVD
WSVD
STS-B (BERT base) (d)"
REFERENCES,0.7742782152230971,Rank Group
REFERENCES,0.7769028871391076,Performance Drop 0.00%
REFERENCES,0.7795275590551181,25.00%
REFERENCES,0.7821522309711286,50.00%
REFERENCES,0.7847769028871391,75.00%
REFERENCES,0.7874015748031497,100.00%
REFERENCES,0.7900262467191601,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.7926509186351706,"SVD
FWSVD
MRPC (BERT base) (e)"
REFERENCES,0.7952755905511811,Rank Group
REFERENCES,0.7979002624671916,Performance Drop 0.00%
REFERENCES,0.800524934383202,25.00%
REFERENCES,0.8031496062992126,50.00%
REFERENCES,0.8057742782152231,75.00%
REFERENCES,0.8083989501312336,100.00%
REFERENCES,0.8110236220472441,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.8136482939632546,"SVD
FWSVD
NER-CoNLL2003 (BERT base) (f)"
REFERENCES,0.8162729658792651,Rank Group
REFERENCES,0.8188976377952756,Performance Drop 0.00%
REFERENCES,0.821522309711286,20.00%
REFERENCES,0.8241469816272966,40.00%
REFERENCES,0.8267716535433071,60.00%
REFERENCES,0.8293963254593176,80.00%
REFERENCES,0.8320209973753281,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.8346456692913385,"SVD
FWSVD
QQP (BERT base) (g)"
REFERENCES,0.8372703412073491,Rank Group
REFERENCES,0.8398950131233596,Performance Drop 0.00%
REFERENCES,0.84251968503937,10.00%
REFERENCES,0.8451443569553806,20.00%
REFERENCES,0.847769028871391,30.00%
REFERENCES,0.8503937007874016,40.00%
REFERENCES,0.8530183727034121,50.00%
REFERENCES,0.8556430446194225,"1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.8582677165354331,"SVD
FWSVD
SST-2 (BERT base) (h)"
REFERENCES,0.8608923884514436,"Figure 8: The grouped rank truncation experiment. The experiments are the same with Figure 6a,
but this ﬁgure includes all 8 language tasks with BERTbase (109.5M parameters) model. FWSVD
has a smaller performance drop with those groups truncated ﬁrst (e.g., group 5 to 10) in all the cases.
SVD usually shows a signiﬁcant drop with group 10, which has the smallest singular values and is
truncated ﬁrst. FWSVD has no such issue in all cases."
REFERENCES,0.863517060367454,Published as a conference paper at ICLR 2022
REFERENCES,0.8661417322834646,Rank Group
REFERENCES,0.868766404199475,Performance Drop 0.00% 5.00%
REFERENCES,0.8713910761154856,10.00%
REFERENCES,0.8740157480314961,15.00%
REFERENCES,0.8766404199475065,20.00%
REFERENCES,0.8792650918635171,"3
4
5
6
7
8
9
10"
REFERENCES,0.8818897637795275,"SVD
FWSVD
CoLA (BERT base) (a)"
REFERENCES,0.884514435695538,Rank Group
REFERENCES,0.8871391076115486,Performance Drop 0.00% 5.00%
REFERENCES,0.889763779527559,10.00%
REFERENCES,0.8923884514435696,15.00%
REFERENCES,0.89501312335958,"3
4
5
6
7
8
9
10"
REFERENCES,0.8976377952755905,"SVD
FWSVD
MNLI (BERT base) (b)"
REFERENCES,0.9002624671916011,Rank Group
REFERENCES,0.9028871391076115,Performance Drop 0.00% 5.00%
REFERENCES,0.905511811023622,10.00%
REFERENCES,0.9081364829396326,15.00%
REFERENCES,0.910761154855643,20.00%
REFERENCES,0.9133858267716536,"3
4
5
6
7
8
9
10"
REFERENCES,0.916010498687664,"SVD
FWSVD
QNLI (BERT base) (c)"
REFERENCES,0.9186351706036745,Rank Group
REFERENCES,0.9212598425196851,Performance Drop 0.00% 1.00% 2.00% 3.00% 4.00% 5.00%
REFERENCES,0.9238845144356955,"3
4
5
6
7
8
9
10"
REFERENCES,0.926509186351706,"SVD
WSVD
STS-B (BERT base) (d)"
REFERENCES,0.9291338582677166,Rank Group
REFERENCES,0.931758530183727,Performance Drop 0.00%
REFERENCES,0.9343832020997376,10.00%
REFERENCES,0.937007874015748,20.00%
REFERENCES,0.9396325459317585,30.00%
REFERENCES,0.9422572178477691,40.00%
REFERENCES,0.9448818897637795,"3
4
5
6
7
8
9
10"
REFERENCES,0.94750656167979,"SVD
FWSVD
MRPC (BERT base) (e)"
REFERENCES,0.9501312335958005,Rank Group
REFERENCES,0.952755905511811,Performance Drop 0.00% 2.00% 4.00% 6.00%
REFERENCES,0.9553805774278216,"3
4
5
6
7
8
9
10"
REFERENCES,0.958005249343832,"SVD
FWSVD
NER-CoNLL2003 (BERT base) (f)"
REFERENCES,0.9606299212598425,Rank Group
REFERENCES,0.963254593175853,Performance Drop 0.00% 2.50% 5.00% 7.50%
REFERENCES,0.9658792650918635,10.00%
REFERENCES,0.968503937007874,"3
4
5
6
7
8
9
10"
REFERENCES,0.9711286089238845,"SVD
FWSVD
QQP (BERT base) (g)"
REFERENCES,0.973753280839895,Rank Group
REFERENCES,0.9763779527559056,Performance Drop 0.00% 1.00% 2.00% 3.00% 4.00%
REFERENCES,0.979002624671916,"3
4
5
6
7
8
9
10"
REFERENCES,0.9816272965879265,"SVD
FWSVD
SST-2 (BERT base) (h)"
REFERENCES,0.984251968503937,"Figure 9: This ﬁgure shows only groups 3 to 10 of Figure 8 to better visualize the groups of a smaller
performance drop."
REFERENCES,0.9868766404199475,Published as a conference paper at ICLR 2022
REFERENCES,0.989501312335958,"Table 4: The raw values for Figure 6a. We additionally include the averaged singular values for
each truncated group. The singular values from FWSVD are multiplied with Fisher information;
thus their scales are different from SVD."
REFERENCES,0.9921259842519685,"Truncated group
1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.994750656167979,"SVD performance drop
25.4%
6.7%
2.8%
2.2%
1.1%
0.9%
0.7%
0.4%
0.1%
4.9%
FWSVD performance drop
24.1%
6.1%
2.7%
1.5%
0.8%
0.6%
0.2%
0.3%
0.2%
0.2%"
REFERENCES,0.9973753280839895,"SVD average singular value
2.674
1.933
1.622
1.381
1.176
0.994
0.828
0.671
0.519
0.353
FWSVD average singular value
1093
631
510
424
355
298
247
201
157
110"
