Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00196078431372549,"Existing deep clustering methods rely on contrastive learning for representation
learning, which requires negative examples to form an embedding space where
all instances are well-separated. However, the negative examples inevitably give
rise to the class collision issue, compromising the representation learning for clus-
tering. In this paper, we explore non-contrastive representation learning for deep
clustering, termed NCC, which is based on BYOL, a representative method with-
out negative examples. First, we propose to align one augmented view of instance
with the neighbors of another view in the embedding space, called positive sam-
pling strategy, which avoids the class collision issue caused by the negative ex-
amples and hence improves the within-cluster compactness. Second, we propose
to encourage alignment between two augmented views of one prototype and uni-
formity among all prototypes, named prototypical contrastive loss or ProtoCL,
which can maximize the inter-cluster distance. Moreover, we formulate NCC in
an Expectation-Maximization (EM) framework, in which E-step utilizes spherical
k-means to estimate the pseudo-labels of instances and distribution of prototypes
from a target network and M-step leverages the proposed losses to optimize an on-
line network. As a result, NCC forms an embedding space where all clusters are
well-separated and within-cluster examples are compact. Experimental results on
several clustering benchmark datasets including ImageNet-1K demonstrate that
NCC outperforms the state-of-the-art methods by a signiﬁcant margin."
INTRODUCTION,0.00392156862745098,"1
INTRODUCTION"
INTRODUCTION,0.0058823529411764705,"Deep clustering is gaining considerable attention as it can learn representation of images and perform
clustering in an end-to-end fashion. Remarkably, contrastive learning-based methods (Wang et al.,
2021; Van Gansbeke et al., 2020; Li et al., 2021a;b; Tao et al., 2021; Tsai et al., 2021; Niu et al.,
2021) have become the main thrust to advance the representation of images on several complex
benchmark datasets, signiﬁcantly contributing to the clustering performance. In addition, some
contrastive learning methods such as MoCo (He et al., 2020) and SimCLR (Chen et al., 2020) usually
require specially designed losses (Wang et al., 2021; Li et al., 2021a;b; Tao et al., 2021; Tsai et al.,
2021) or an extra pre-training stage for more discriminative representations (Van Gansbeke et al.,
2020; Niu et al., 2021)."
INTRODUCTION,0.00784313725490196,"Although achieving promising clustering results, contrastive learning requires a large number of
negative examples to achieve the instance-wise discrimination in an embedding space where all
instances are well-separated. The constructed negative pairs usually require a large batch size (Chen
et al., 2020), memory queue (He et al., 2020), or memory bank (Wu et al., 2018), which not only
bring extra computational cost but also give rise to class collision issue (Saunshi et al., 2019). Here,
the class collision issue refers to that different instances from the same semantic class are regarded
as negative pairs, hurting the representation learning for clustering. A question naturally arises: are
negative examples necessary for deep clustering?"
INTRODUCTION,0.00980392156862745,"Another kind of self-supervised learning is the non-contrastive methods such as BYOL (Grill et al.,
2020) and SimSiam (Chen & He, 2021), which use the representations of one augmented view to
predict another view. Their success demonstrates that negative examples are not the key to avoiding
representation collapse. However, to the best of our knowledge, almost all recent successful liter-
ature of deep clustering is built upon contrastive learning-based methods such as MoCo (He et al.,"
INTRODUCTION,0.011764705882352941,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.013725490196078431,"2020) and SimCLR (Chen et al., 2020). There is a general consensus that the negative examples are
helpful to stabilize the training of representation learning for deep clustering. As discussed in (Wang
& Isola, 2020), the typical contrastive loss can be identiﬁed into two properties: 1) alignment term
to improve the closeness of positive pairs; and 2) uniformity term to encourage instances to be
uniformly distributed on a unit hypersphere. In contrast, non-contrastive methods such as BYOL
only optimize the alignment term, leading to unstable training and suffering from the representation
collapse—which may be worsen when adding extra losses."
INTRODUCTION,0.01568627450980392,"To tackle the class collision issue, we explore the non-contrastive representation learning for deep
clustering, termed non-contrastive clustering or NCC, which is based on BYOL, a representative
method without negative examples. First, instead of negative sampling that is a double-edged sword,
i.e., causing class collision issue but improving the training stability, we propose to align one aug-
mented view of the instance with the neighbors of another view in the embedding space, called
positive sampling strategy, which can avoid the class collision issue and hence improve the within-
cluster compactness. Second, as for the clustering task, the different clusters are truly negative pairs
for contrastive loss. To this end, we propose to encourage the alignment between two augmented
views of prototypes and the uniformity among all prototypes, named prototypical contrastive loss
or ProtoCL, which can maximize the inter-cluster distance. Moreover, we formulate our method
into an EM framework, in which we iteratively perform E-step as estimating the pseudo-labels of
instances and distribution of prototypes via spherical k-means based on the target network and M-
step as optimizing the online network via the proposed losses. As a result, NCC is able to form
an embedding space where all clusters are well-separated and within-cluster examples are compact.
The contributions of this paper are summarized as follows:"
INTRODUCTION,0.01764705882352941,"• We explore the non-contrastive representation learning for deep clustering by proposing non-
contrastive clustering or NCC, which is based on the Bootstrap Your Own Latent (BYOL), a
representative method without negative examples."
INTRODUCTION,0.0196078431372549,"• We propose a positive sampling strategy to augment instance alignment by taking into account
neighboring positive examples in the embedding space, which can avoid the class collision issue
and hence improve the within-cluster compactness."
INTRODUCTION,0.021568627450980392,"• We propose a novel prototypical contrastive loss or ProtoCL, which can align one augmented
view of prototypes with another view and encourage the uniformity among all prototypes on a
unit hypersphere, hence maximizing the inter-cluster distance."
INTRODUCTION,0.023529411764705882,"• We formulate our method into an EM framework, in which we can iteratively estimate the pseudo-
labels and distribution of prototypes via spherical k-means based on the target network and opti-
mize the online network via the proposed losses."
INTRODUCTION,0.025490196078431372,"• Extensively experimental results on several benchmark datasets as well as ImageNet-1K demon-
strate that NCC outperforms the existing state-of-the-art methods by a signiﬁcant margin."
RELATED WORK,0.027450980392156862,"2
RELATED WORK"
RELATED WORK,0.029411764705882353,"Deep clustering can be signiﬁcantly advanced by discriminative representations. Examples of tra-
ditional deep clustering methods include: Xie et al. (2016); Yang et al. (2017) use autoencoders to
simultaneously perform representation learning and clustering; Chang et al. (2017); Haeusser et al.
(2018); Wu et al. (2019); Ji et al. (2019) learn pair-wise relationships between original and aug-
mented instances. However, they often suffer from inferior performance on some complex datasets
such as CIFAR-20. Inspired by the success of contrastive learning, recent studies turn to exploit
the discriminative representations learned from contrastive learning to assist the downstream clus-
tering tasks (Van Gansbeke et al., 2020; Niu et al., 2021) or simultaneously optimize representation
learning and clustering (Tao et al., 2021; Tsai et al., 2021; Li et al., 2021a; Shen et al., 2021).
SCAN (Van Gansbeke et al., 2020) uses the model pre-trained by SimCLR to yield the conﬁdent
pseudo-labels. IDFD (Tao et al., 2021) proposes to perform both instance discrimination and fea-
ture decorrelation. GCC (Zhong et al., 2021) and WCL (Zheng et al., 2021) build a graph to label
the neighbor samples as pseudo-positive examples, however, they still suffer from the class colli-
sion issue due to the contrastive loss involved and these pseudo-positive examples that may not be
truly positive. All of them are built upon the contrastive learning framework, which means that
they require a large number of negative examples for training stability, inevitably giving rise to class"
RELATED WORK,0.03137254901960784,Under review as a conference paper at ICLR 2022
RELATED WORK,0.03333333333333333,"collision issue. Different from prior work, this paper explores the non-contrastive self-supervised
methods, i.e., BYOL, to achieve both representation learning and clustering. We note that Regatti
et al. (2021); Lee et al. (2020) have tried to build the clustering framework based on BYOL, however,
their methods do not consider improving within-cluster compactness and maximizing inter-cluster
distance like ours. Therefore, to the best of our knowledge, this is the ﬁrst successful attempt that
introduces the non-contrastive representation learning into deep clustering that yields a substantial
performance improvement over previous state-of-the-art methods. In Appendix A, we present re-
lated work on self-supervised learning and difference from existing methods including CC (Li et al.,
2021b), GCC (Zhong et al., 2021), WCL (Zheng et al., 2021), and PCL (Li et al., 2021a)."
PRELIMINARY,0.03529411764705882,"3
PRELIMINARY"
PRELIMINARY,0.03725490196078431,"The most successful self-supervised learning methods in recent years can be roughly divided into
contrastive (Chen et al., 2020; He et al., 2020) and non-contrastive (Grill et al., 2020; Chen & He,
2021). Here, we brieﬂy summarize their formulas and discuss their difference."
PRELIMINARY,0.0392156862745098,"Contrastive learning.
Contrastive learning methods perform instance-wise discrimination (Wu
et al., 2018) using the InfoNCE loss (Oord et al., 2018). Formally, assume that we have one instance
x, its augmented version x+ by using random data augmentation, and a set of M negative examples
drawn from the dataset , {x−
1 , x−
2 , . . . , x−
M}. The contrastive learning aims to learn an embedding
function f that maps x onto a unit hypersphere, in which the InfoNCE loss can be deﬁned as:"
PRELIMINARY,0.041176470588235294,"Lcontr = −log
exp
 
f(x)Tf(x+)/τ
"
PRELIMINARY,0.043137254901960784,"exp (f(x)Tf(x+)/τ) + PM
i=1 exp
 
f(x)Tf(x−
i )/τ

(1)"
PRELIMINARY,0.045098039215686274,"≈−f(x)Tf(x+)/τ + log
XM"
PRELIMINARY,0.047058823529411764,"i=1 exp
 
f(x)Tf(x−
i )/τ

,
(2)"
PRELIMINARY,0.049019607843137254,"where the ﬁrst and second terms in Eq. (2) refer to as instance alignment and instance uniformity,
respectively. Here, we assume that the output of f(·) is ℓ2 normalized. That is, the representation is
on a unit hypersphere. The temperature τ controls the concentration level of representations; please
refer to (Wang & Liu, 2021) for detailed behaviors of τ in the contrastive loss. Intuitively, the In-
foNCE loss aims to pull together the positive pair (x, x+) from two different data augmentations of
the same instance, and push x away from M negative examples of other instances. As discussed
in (Wang & Isola, 2020), when M →∞, the InfoNCE loss in Eq. (1) can be approximately de-
coupled into two terms: alignment and uniformity, as shown in Eq. (2). Despite the alignment term
closes the positive pair, the key to avoiding representation collapse is the uniformity term, which
makes the negative examples uniformly distributed on the hypersphere. Although beneﬁcial, the
negative examples inevitably lead to the class collision issue, hurting the representation learning for
clustering."
PRELIMINARY,0.050980392156862744,"Non-contrastive learning.
Non-contrastive learning-based methods have shown more promising
results than contrastive learning for representation learning and downstream tasks (Ericsson et al.,
2021). Non-contrastive methods only optimize the alignment term in Eq. (2) to match the repre-
sentations between two augmented views. Without negative examples, they leverage an online and
a target network for two views, and use a predictor network to bridge the gap between these two
views. They also stop the gradient of the target network to avoid the representation collapse. In
particular, if τ = 0.5, the loss used in (Grill et al., 2020; Chen & He, 2021) can be written as:"
PRELIMINARY,0.052941176470588235,"Lnon−contr = −2g (f(x))T f ′(x+) =
g (f(x)) −f ′(x+)
2
2 + const,
(3)"
PRELIMINARY,0.054901960784313725,"where g the predictor; f and f ′ are the online and target networks, respectively; the outputs of
g(f(·)) and f ′(·) are ℓ2-normalized. However, as mentioned in (Fetterman & Albrecht, 2020),
the non-contrastive learning methods often suffer from unstable training and highly rely on the
batch-statistics and hyper-parameter tuning to avoid representation collapse. Even though Grill
et al. (2020); Richemond et al. (2020) have proposed to use some tricks such as SyncBN (Ioffe
& Szegedy, 2015) and weight normalization (Qiao et al., 2019) to alleviate this issue, the additional
computation cost is signiﬁcant. Without negative examples, the collapse issue could be worsen
when adding additional clustering losses for clustering task; see Fig. A1 for the analysis of applying
PCL (Li et al., 2021a) to the BYOL."
PRELIMINARY,0.056862745098039215,Under review as a conference paper at ICLR 2022
PRELIMINARY,0.058823529411764705,"In a nutshell, most of existing successful deep clustering methods are based on contrastive learning
for representation learning—giving rise to class collision issue—while the non-contrastive learning,
due to unstable training with additional losses, is not yet ready for deep clustering. To that end, we
explore the non-contrastive learning, i.e. BYOL, for deep clustering with positive sampling strat-
egy and prototypical contrastive loss to avoid the class collision issue, improve the within-cluster
compactness, and maximize the inter-class distance."
NON-CONTRASTIVE CLUSTERING,0.060784313725490195,"4
NON-CONTRASTIVE CLUSTERING ••• Input"
NON-CONTRASTIVE CLUSTERING,0.06274509803921569,Online Network
NON-CONTRASTIVE CLUSTERING,0.06470588235294118,Target Network
NON-CONTRASTIVE CLUSTERING,0.06666666666666667,Target Network
NON-CONTRASTIVE CLUSTERING,0.06862745098039216,Moving Average
NON-CONTRASTIVE CLUSTERING,0.07058823529411765,"For every epoch
Clustering"
NON-CONTRASTIVE CLUSTERING,0.07254901960784314,For every iteration
NON-CONTRASTIVE CLUSTERING,0.07450980392156863,Predictor
NON-CONTRASTIVE CLUSTERING,0.07647058823529412,E-step
NON-CONTRASTIVE CLUSTERING,0.0784313725490196,M-step
NON-CONTRASTIVE CLUSTERING,0.0803921568627451,Feature Extraction
NON-CONTRASTIVE CLUSTERING,0.08235294117647059,Representations
NON-CONTRASTIVE CLUSTERING,0.08431372549019608,Estimate
NON-CONTRASTIVE CLUSTERING,0.08627450980392157,"Optimization
Positive 
Sampling"
NON-CONTRASTIVE CLUSTERING,0.08823529411764706,Data augmentations
NON-CONTRASTIVE CLUSTERING,0.09019607843137255,Figure 1: The overall framework of the proposed NCC in an EM framework.
NON-CONTRASTIVE CLUSTERING,0.09215686274509804,"Fig. 1 presents the overall framework of the proposed NCC. Based on BYOL, NCC is comprised
of three networks: an online, a target, and a predictor. In Sec. 4.1, we propose a positive sampling
strategy to augment instance alignment to improve the within-cluster compactness. In Sec. 4.2, a
prototypical contrastive loss is introduced to maximize the inter-cluster distance using the pseudo-
labels from k-means clustering, which can encourage uniform representations. Finally, we formulate
NCC into an EM framework to facilitate the understanding of training procedure in Sec. 4.3."
POSITIVE SAMPLING STRATEGY,0.09411764705882353,"4.1
POSITIVE SAMPLING STRATEGY"
POSITIVE SAMPLING STRATEGY,0.09607843137254903,"(a) Negative Sampling
(b) Positive Sampling
(c) Prototypical Contrastive Loss"
POSITIVE SAMPLING STRATEGY,0.09803921568627451,"push away
pull together"
POSITIVE SAMPLING STRATEGY,0.1,"Figure 2: Illustration of the proposed techniques compared to negative sampling. (a) Negative sam-
pling in contrastive learning giving rise to class collision issue. (b) The proposed positive sampling
encouraging the alignment between neighbors of one view with another one. (c) The proposed pro-
totypical contrastive loss encouraging prototypical alignment and prototypical uniformity."
POSITIVE SAMPLING STRATEGY,0.10196078431372549,"The negative examples are essential for contrastive learning-based deep clustering to stabilize the
training of representation learning, at the cost of inevitable class collision issue (Saunshi et al.,
2019), as shown in Fig. 2(a). This issue can hurt the representation learning for clustering as the
instances from the same class/cluster—should be close to each other—could be treated as negative
pairs and are pushed away during training, discouraging the within-cluster compactness."
POSITIVE SAMPLING STRATEGY,0.10392156862745099,"To address the class collision issue, we resort to non-contrastive learning-based methods for rep-
resentation learning, which no longer need negative examples. Although we cannot optimize the"
POSITIVE SAMPLING STRATEGY,0.10588235294117647,Under review as a conference paper at ICLR 2022
POSITIVE SAMPLING STRATEGY,0.10784313725490197,"uniformity term like contrastive loss, our idea is to optimize the opposite of the uniformity instead.
That is, we aim to encourage the neighboring examples around one augmented view, sampled in the
embedding space, to be aligned with another view, as shown in Fig. 2(b). Our motivation is that
although we cannot guarantee the negative examples in contrastive loss are from different classes,
we can certainly assume that the neighboring samples around one view in the embedding space
are positive with respect to another view and belong to the same class. Therefore, we propose a
positive sampling strategy to augment the instance alignment in Eq. (3) by taking into account the
neighboring samples towards within-cluster compactness."
POSITIVE SAMPLING STRATEGY,0.10980392156862745,"Speciﬁcally, we model the representation of one augmented view of an instance as a Gaussian dis-
tribution in the embedding space, which can be formulated as follows:
v ∼N
 
f(x), σ2I

,
(4)
where I represents the identity matrix and σ is a positive hyperparameter controlling how many
samples around one view can be treated as positive pairs with another view. However, the sampled
examples from Eq. (4) cannot allow the error to be backpropagated through the network to update the
network parameters. We employ the reparametrization trick (Kingma & Welling, 2014) to achieve
the backpropagation. As a result, the positive sampling strategy can be implemented as follows:
v = f(x) + σϵ,
ϵ ∼N (0, I) .
(5)
Therefore, we can augment the instance alignment in Eq. (3) by taking into account the neighboring
samples to encourage the within-cluster compactness. With only sampling one example from the
Gaussian distribution, the augmented instance alignment term can be formally deﬁned as:"
POSITIVE SAMPLING STRATEGY,0.11176470588235295,"Laug−ins =
g(v) −f ′(x+)
2
2 =
g (f(x) + σϵ) −f ′(x+)
2
2 .
(6)"
POSITIVE SAMPLING STRATEGY,0.11372549019607843,The beneﬁts of the proposed positive sampling are summarized as follows.
POSITIVE SAMPLING STRATEGY,0.11568627450980393,"Improved within-cluster compactness.
The conventional instance alignment in Eq. (3) only en-
courages the representation of one augmented view to be close to another view. In the context of
clustering, such compactness is instance-wise and neutral for the clustering. Put differently, all in-
stances are treated as cluster centers and the semantic class information cannot be captured at only
instance level. In contrast, our augmented instance alignment in Eq. (6) encourages neighboring
examples around one augmented view—either different augmented examples of the same instance
or same/different augmented examples of different instances within the same cluster—to be positive
pairs with another view. This is helpful to improve within-cluster compactness."
POSITIVE SAMPLING STRATEGY,0.11764705882352941,"Avoidable class collision issue.
As we mentioned before, class collision issue induced by the
negative examples indicates that we cannot guarantee that the negative examples are from different
clusters. However, our positive sampling strategy can guarantee that the positive examples around
one instance, sampled in the embedding space, are from the same cluster as the instance, getting rid
of class collision issue. The difference from recent work (Zhong et al., 2021; Zheng et al., 2021) is
discussed in Appendix A. We note that our positive sampling strategy does not consider uniformity,
which is solved by the proposed prototypical contrastive loss in Sec. 4.2."
PROTOTYPICAL CONTRASTIVE LOSS,0.11960784313725491,"4.2
PROTOTYPICAL CONTRASTIVE LOSS"
PROTOTYPICAL CONTRASTIVE LOSS,0.12156862745098039,"A good clustering is supposed to have distinct semantic prototypes/clusters. Assume that the dataset
has K clusters, where K is a predeﬁned hyperparameter, it naturally constructs a contrastive loss
for these K prototypes as for one prototype, the remaining K −1 prototypes are deﬁnitely negative
examples. Therefore, we propose a prototypical contrastive loss or ProtoCL, which encourages
the prototypical alignment between two augmented views and the prototypical uniformity, hence
maximizing the inter-cluster distance."
PROTOTYPICAL CONTRASTIVE LOSS,0.12352941176470589,"Speciﬁcally, assume we have K prototypes from the online network, {µ1, µ2, . . . , µK}, and an-
other K prototypes from the target network, {µ′
1, µ′
2, . . . , µ′
K}, our proposed ProtoCL, illustrated
in Fig. 2(c), is given as follows:"
PROTOTYPICAL CONTRASTIVE LOSS,0.12549019607843137,Lpcl = 1 K XK
PROTOTYPICAL CONTRASTIVE LOSS,0.12745098039215685,"k=1 −log
exp(µT
kµ′
k/τ)"
PROTOTYPICAL CONTRASTIVE LOSS,0.12941176470588237,"exp(µT
kµ′
k/τ) + PK
j=1,j̸=k exp(µT
kµj/τ)
,
(7) ≈1 K XK"
PROTOTYPICAL CONTRASTIVE LOSS,0.13137254901960785,"k=1 −µT
kµ′
k/τ + 1 K XK"
PROTOTYPICAL CONTRASTIVE LOSS,0.13333333333333333,"k=1 log
XK"
PROTOTYPICAL CONTRASTIVE LOSS,0.13529411764705881,"j=1,j̸=k exp(µT
kµj/τ),
(8)"
PROTOTYPICAL CONTRASTIVE LOSS,0.13725490196078433,Under review as a conference paper at ICLR 2022
PROTOTYPICAL CONTRASTIVE LOSS,0.1392156862745098,"where the ﬁrst and second terms in Eq. (8) refer to as prototypical alignment and prototypical uni-
formity, respectively. Here, the cluster centers µk and µ′
k are computed within a mini-batch B as"
PROTOTYPICAL CONTRASTIVE LOSS,0.1411764705882353,follows:µk = P
PROTOTYPICAL CONTRASTIVE LOSS,0.14313725490196078,"x∈B p(k|x)f(x)
∥P"
PROTOTYPICAL CONTRASTIVE LOSS,0.1450980392156863,"x∈B p(k|x)f(x)∥2 and
µ′
k = P"
PROTOTYPICAL CONTRASTIVE LOSS,0.14705882352941177,"x∈B p(k|x)f ′(x)
∥P"
PROTOTYPICAL CONTRASTIVE LOSS,0.14901960784313725,"x∈B p(k|x)f ′(x)∥2 , and p(k|x) is the cluster as-
signment posterior probability. When K > |B|, it is obvious that the mini-batch cannot cover all
clusters. To this end, we zero out the losses and logits of empty clusters for each iteration; see the
pseudocode in Appendix D for more details."
PROTOTYPICAL CONTRASTIVE LOSS,0.15098039215686274,"Clearly, our ProtoCL is quite similar to conventional contrastive loss in Eq. (1) but for prototypes
with non-contrastive representation learning framework. The prototypical alignment is to align the
prototypes derived from the online network with the ones from the target network, which can sta-
bilize the update of the prototypes. The prototypical uniformity is to encourage the prototypes to
be uniformly distributed on a unit hypersphere, which can maximize the inter-cluster distance. The
difference from ProtoNCE in (Li et al., 2021a) is discussed in Appendix A."
EM FRAMEWORK,0.15294117647058825,"4.3
EM FRAMEWORK"
EM FRAMEWORK,0.15490196078431373,"We formulate our NCC into an EM framework to facilitate the understanding of the training proce-
dure, detailed in Fig. 1 and derived in Appendix B."
EM FRAMEWORK,0.1568627450980392,"E-step.
This step aims to estimate p(k|x). We perform spherical k-means algorithm on the fea-
tures extracted from the target network since the target network performs more stable and yields
more consistent clusters, similar to BYOL and MoCo. Although we need an additional k-means
clustering to obtain the cluster pseudo-labels p(k|x) for every r epochs, we found that even with a
larger r, rather than every epoch r = 1, our method can still produce consistent performance im-
provement over the baseline methods. Therefore, our method will not introduce much computation
cost and is robust to the cluster pseudo-labels; see detailed results in Fig. A2. The analysis of com-
putational cost is discussed in Appendix C. Finally, with p(k|x), we build the cluster centers within
a mini-batch without additional memory like queue (He et al., 2020) or bank (Wu et al., 2018)."
EM FRAMEWORK,0.1588235294117647,"M-step.
Combining the augmented instance alignment loss in Eq. (6) and the proposed ProtoCL
in Eq. (7) yields our objective function for M-step as follows:"
EM FRAMEWORK,0.1607843137254902,"L = Laug−ins + λpclLpcl,
(9)"
EM FRAMEWORK,0.1627450980392157,"where λpcl controls the balance between two loss components. Therefore, there are only two addi-
tional hyper-parameters compared to original BYOL, including: σ in Laug−ins and the loss weight
λpcl; see detailed results of these two hyper-parameters in Figs. A3 and A4."
EXPERIMENTS,0.16470588235294117,"5
EXPERIMENTS"
EXPERIMENTS,0.16666666666666666,Table 1: Summary of the datasets.
EXPERIMENTS,0.16862745098039217,"Dataset
Split
# Samples
# Classes
Image Size"
EXPERIMENTS,0.17058823529411765,"CIFAR-10
Train+Test
60,000
10
32×32
CIFAR-20
Train+Test
60,000
20
32×32
STL-10
Train+Test
13,000
10
96×96
ImageNet-10
Train
13,000
10
96×96
ImageNet-Dogs
Train
19,500
15
96×96
ImageNet-1K
Train
1,281,167
1,000
224×224"
EXPERIMENTS,0.17254901960784313,"We
conducted
experiments
on
six
benchmark datasets, including CIFAR-
10 (Krizhevsky et al., 2009), CIFAR-
20
(Krizhevsky
et
al.,
2009),
STL-
10 (Coates et al., 2011),
ImageNet-
10 (Chang et al., 2017),
ImageNet-
Dogs (Chang et al., 2017), and ImageNet-
1K (Deng et al., 2009), which are summarized in Table 1. We note that CIFAR-20 contains 20
superclasses of CIFAR-100. This paper follows the experimental settings widely used in deep
clustering work (Chang et al., 2017; Wu et al., 2019; Ji et al., 2019; Tsai et al., 2021; Tao et al.,
2021), including the image size, backbone and train-test split. We employ three common metrics
to evaluate the clustering performance, including Normalized Mutual Information (NMI), Cluster
Accuracy (ACC), and Adjusted Rand Index (ARI) for the ﬁrst ﬁve datasets. Following (Li et al.,
2021a), we report Adjusted Mutual Information (AMI) to evaluate the clustering performance for
ImageNet-1K. The results are presented in percentage (%) and the higher the better clustering
performance. For fair comparisons, we use ResNet-34 (He et al., 2016) as the backbone to report
the results in Table 2. Unless noted otherwise, we use ResNet-18 for the rest of experiments. We
run each experiment three times and report the mean and standard deviation as the ﬁnal results. We
provided detailed training settings in Appendix C. We also provide the pseudocode of NCC for
better understanding in Appendix D. The source code will be publicly available upon acceptance."
EXPERIMENTS,0.17450980392156862,Under review as a conference paper at ICLR 2022
MAIN RESULTS,0.17647058823529413,"5.1
MAIN RESULTS"
MAIN RESULTS,0.1784313725490196,"Table 2: Clustering results (%) of various methods on ﬁve benchmark datasets. The best and second
best results are shown in bold and underline, respectively."
MAIN RESULTS,0.1803921568627451,"Dataset
CIFAR-10
CIFAR-20
STL-10
ImageNet-10
ImageNet-Dogs
Method1
NMI
ACC
ARI
NMI
ACC
ARI
NMI
ACC
ARI
NMI
ACC
ARI
NMI
ACC
ARI"
MAIN RESULTS,0.18235294117647058,"k-means
8.7
22.9
4.9
8.4
13.0
2.8
12.5
19.2
6.1
11.9
24.1
5.7
5.5
10.5
2.0
SC
10.3
24.7
8.5
9.0
13.6
2.2
9.8
15.9
4.8
15.1
27.4
7.6
3.8
11.1
1.3
AE
23.9
31.4
16.9
10.0
16.5
4.8
25.0
30.3
16.1
21.0
31.7
15.2
10.4
18.5
7.3
VAE
24.5
29.1
16.7
10.8
15.2
4.0
20.0
28.2
14.6
19.3
33.4
16.8
10.7
17.9
7.9
JULE
19.2
27.2
13.8
10.3
13.7
3.3
18.2
27.7
16.4
17.5
30.0
13.8
5.4
13.8
2.8
DEC
25.7
30.1
16.1
13.6
18.5
5.0
27.6
35.9
18.6
28.2
38.1
20.3
12.2
19.5
7.9
DAC
39.6
52.2
30.6
18.5
23.8
8.8
36.6
47.0
25.7
39.4
52.7
30.2
21.9
27.5
11.1
IIC
51.3
61.7
41.1
-
25.7
-
43.1
49.9
29.5
-
-
-
-
-
-
DCCM
49.6
62.3
40.8
28.5
32.7
17.3
37.6
48.2
26.2
60.8
71.0
55.5
32.1
38.3
18.2
PICA
56.1
64.5
46.7
29.6
32.2
15.9
-
-
-
78.2
85.0
73.3
33.6
32.4
17.9"
MAIN RESULTS,0.1843137254901961,"CC2
70.5
79.0
63.7
43.1
42.9
26.6
76.4
85.0
72.6
85.9
89.3
82.2
44.5
42.9
27.4
SCAN3
79.7
88.3
77.2
48.6
50.7
33.3
80.9
69.8
64.6
-
-
-
-
-
-
GCC
76.4
85.6
72.8
47.2
47.2
30.5
68.4
78.8
63.1
84.2
90.1
82.2
49.0
52.6
36.2
MiCE
73.7
83.5
69.8
43.6
44.0
28.0
63.5
75.2
57.5
-
-
-
42.3
43.9
28.6
IDFD
71.1
81.5
66.3
42.6
42.5
26.4
64.3
75.6
57.5
89.8
95.4
90.1
54.6
59.1
41.3
PCL
80.2
87.4
76.6
52.8
52.6
36.3
71.8
41.0
67.0
84.1
90.7
82.2
44.0
41.2
29.9"
MAIN RESULTS,0.18627450980392157,"BYOL
81.7
89.4
79.0
55.9
56.9
39.3
71.3
82.5
65.7
86.6
93.9
87.2
63.5
69.4
54.8
±0.1
±0.6
±0.1
±0.3
±1.8
±0.2
±0.9
±0.5
±1.3
±0.2
±0.1
±0.2
±2.2
±3.0
±2.9
NCC
88.6
94.3
88.4
60.6
61.4
45.1
75.8
86.7
73.7
89.6
95.6
90.6
69.2
74.5
62.7
(ours)
±0.1
±0.6
±1.1
±0.3
±1.1
±0.1
±1.8
±1.3
±2.4
±0.2
±0.0
±0.1
±0.3
±0.1
±0.1"
MAIN RESULTS,0.18823529411764706,"1 k-means (Lloyd, 1982), SC (Zelnik-manor & Perona, 2005), AE (Bengio et al., 2007), VAE (Kingma & Welling, 2014),
JULE (Yang et al., 2016), DEC (Xie et al., 2016), DAC (Chang et al., 2017), IIC (Ji et al., 2019), DCCM (Wu et al., 2019),
PICA (Huang et al., 2020), CC (Li et al., 2021b), SCAN (Van Gansbeke et al., 2020), GCC (Zhong et al., 2021),
MiCE (Tsai et al., 2021), IDFD (Tao et al., 2021), PCL (Li et al., 2021a), BYOL (Grill et al., 2020),
2 CC uses a large image size (224) for all datasets.
3 SCAN needs an additional pre-training stage while NCC is trained in an end to end manner. It only uses training set for all datasets."
MAIN RESULTS,0.19019607843137254,"Quantitative results
We compared NCC with previous state-of-the-art clustering methods in Ta-
ble 2. NCC achieves signiﬁcant performance improvement on all ﬁve benchmark datasets, demon-
strating the superiority of NCC for deep clustering to capture the semantic class information. In-
terestingly, directly using the representations learned by BYOL for k-means clustering outperforms
previous work including the contrastive-based ones (Li et al., 2021b; Tsai et al., 2021; Tao et al.,
2021), which suggests a great potential for non-contrastive representation learning for deep cluster-
ing without suffering class collision issue."
MAIN RESULTS,0.19215686274509805,"On the ImageNet-10, our NCC achieves competitive performance as compared to IDFD (Tao et al.,
2021) since this dataset is relatively small with only 13K images, which cannot arise discriminative
differences for current state-of-the-art methods. On the ImageNet-Dogs, a ﬁne-grained dataset con-
taining different species of dogs from the ImageNet dataset, there are almost 20% improvements
over previous SOTA work. The contrastive-based methods cannot handle this kind of dataset due to
severe class collision issue that pushes away the instances from the same class. Meanwhile, IDFD
can deal with this problem to some degree thanks to the feature decorrelation along with the instance
discrimination. Without the need of negative examples, BYOL can achieve signiﬁcant improvement,
although its performance is unstable. Our NCC, built upon BYOL with a positive sampling strategy
and prototypical contrastive loss, has shown its signiﬁcant and stable performance against vanilla
BYOL and contrastive-based methods.
Table 3:
Clustering results
(%) on ImageNet-1K."
MAIN RESULTS,0.19411764705882353,"Method
AMI"
MAIN RESULTS,0.19607843137254902,"DeepCluster
28.1
MoCo
28.5
PCL
41.0
NCC (Ours)
52.5"
MAIN RESULTS,0.1980392156862745,"Table 3 further presents the results between our NCC and baseline
methods including DeepCluster (Caron et al., 2018), MoCo (He
et al., 2020), and PCL (Li et al., 2021a) on ImageNet-1K dataset,
showing that NCC achieves signiﬁcantly higher AMI score."
MAIN RESULTS,0.2,"Although we employed the fair conditions, some work has trained
the network with different split (Van Gansbeke et al., 2020) for
CIFAR-10 and CIFAR-20, or large image size (Li et al., 2021b) for
ImageNet-10 and ImageNet-Dogs. For the sake of fair comparisons
with different image sizes and splits, we report these comparison results in Table A1. In addition,
we also reported the clustering results on ImageNet subsets like SCAN (Van Gansbeke et al., 2020)"
MAIN RESULTS,0.2019607843137255,Under review as a conference paper at ICLR 2022
MAIN RESULTS,0.20392156862745098,"in Table A2 and Tiny-ImageNet (Le & Yang, 2015) in Table A3. We also conducted additional
experiments in Table A4 to demonstrate the ability of NCC handling the long-tailed datasets."
MAIN RESULTS,0.20588235294117646,"Qualitative results.
Fig. 3 visualizes the learned representations by t-SNE (Van der Maaten &
Hinton, 2008) for four different training epochs throughout the training process. At the beginning,
the random-initialized model cannot distinguish the instances from different semantic classes, where
all instances are mixed together. As the training process goes, NCC gradually attracts the instances
from the same cluster while pushing the clusters away from each other. Obviously, at the end
of the training, NCC produces clear boundary between clusters and within-cluster compactness.
Visualization for the outlier points produced by the model at 1000-th epoch is shown in Fig. A5."
MAIN RESULTS,0.20784313725490197,"a) Epoch 0 (NMI=6.98%)
b) Epoch 300 (NMI=77.8%)
c) Epoch 700 (NMI=83.5%)
d) Epoch 1000 (NMI=85.5%)
Figure 3: Visualization of feature representations learned by NCC on CIFAR-10 with t-SNE. Dif-
ferent colors denote the different semantic classes. Zoom in for better view."
ABLATION STUDY,0.20980392156862746,"5.2
ABLATION STUDY"
ABLATION STUDY,0.21176470588235294,"Here, we perform detailed ablation studies with both quantitative and qualitative comparisons to
provide more insights into why NCC performs so well for deep clustering."
ABLATION STUDY,0.21372549019607842,"Table 4: Ablation studies for different self-supervised learning frameworks, and positive sampling
(PS) strategy, and prototypical contrastive loss for NCC. The best and second best results are shown
in bold and underline, respectively."
ABLATION STUDY,0.21568627450980393,"CIFAR-10
CIFAR-20"
ABLATION STUDY,0.21764705882352942,"Method
PS
Prototypical
NMI
ACC
ARI
NMI
ACC
ARI
Alignment
Uniformity"
ABLATION STUDY,0.2196078431372549,"MoCo v2 (He et al., 2020)
76.9±0.2
84.9±0.3
72.4±0.5
49.2±0.1
48.0±0.2
32.1±0.0
SimSiam (Chen & He, 2021)
78.8±0.9
86.5±0.8
74.9±1.3
46.6±0.8
47.3±1.1
28.8±1.2
CC (Li et al., 2021b)
66.1±0.3
74.6±0.3
58.3±0.4
46.4±0.3
45.0±0.1
29.5±0.2
+ ProtoCL (Ours)
✓
✓
74.3±0.4
83.4±0.5
69.6±1.0
48.3±0.2
49.1±0.2
32.2±0.4
PCL (Li et al., 2021a)
77.6±0.1
85.5±0.1
73.4±0.0
50.0±0.3
48.6±0.7
32.7±0.4"
ABLATION STUDY,0.22156862745098038,"BYOL (Grill et al., 2020)
79.4±1.7
87.8±1.7
76.6±2.8
55.5±0.6
53.9±1.6
37.6±0.9
+ CC (Li et al., 2021b)
76.6±3.1
86.3±2.7
73.8±4.7
51.0±2.0
48.9±3.0
33.3±2.9
+ PCL (Li et al., 2021a)
74.4±2.3
85.3±0.9
71.4±1.4
49.7±0.7
46.9±0.7
27.8±1.5"
ABLATION STUDY,0.2235294117647059,NCC (Ours)
ABLATION STUDY,0.22549019607843138,"✓
79.4±0.9
87.9±0.5
76.4±1.1
57.0±0.0
55.0±0.6
39.8±1.1
✓
✓
83.4±1.2
90.3±0.9
81.1±1.7
56.6±0.4
55.1±0.5
40.7±1.0
✓
✓
79.6±0.7
87.8±1.5
76.5±2.1
56.7±0.3
56.6±1.4
39.7±1.1
✓
✓
85.3±0.2
92.1±0.1
84.4±0.3
57.2±0.3
57.3±0.6
41.7±0.5
✓
✓
✓
85.1±0.5
91.6±0.4
83.5±0.7
58.2±0.3
57.8±0.2
42.3±0.3"
ABLATION STUDY,0.22745098039215686,"Quantitative ablation study.
We report the quantitative results of ablation studies in Table 4.
BYOL outperforms MoCo v2 and SimSiam by a large margin on both two datasets. The differ-
ence between BYOL and MoCo v2 is that MoCo v2 uses a memory queue to store the consistent
negative examples while BYOL directly aligns two augmented views with a predictor network. Dif-
ferent from the BYOL that employs a momentum-updated network as the target network to yield
the positive representations, SimSiam shares the weights of the target and online networks. There-
fore, BYOL outperforms MoCo v2 by dealing with the class collision issue and SimSiam by the
momentum-updated target network."
ABLATION STUDY,0.22941176470588234,"Compared to vanilla BYOL, simply using the positive sampling strategy can stable and further im-
prove the performance, especially when the number of semantic classes increases for CIFAR-20.
Although ProtoCL improves the baseline results by a large margin, positive sampling can further
boost the clustering performance. This is because ProtoCL only considers inter-cluster distance,
and cannot beneﬁt within-cluster compactness. Therefore, the combination of the positive sampling"
ABLATION STUDY,0.23137254901960785,Under review as a conference paper at ICLR 2022
ABLATION STUDY,0.23333333333333334,"and ProtoCL achieves the best clustering results, where positive sampling strategy can improve the
within-cluster compactness and ProtoCL encourages the prototypical alignment between two aug-
mented views and maximizes the inter-cluster distance."
ABLATION STUDY,0.23529411764705882,"To further explore the effect of the proposed ProtoCL, we split this loss function into prototypical
alignment and uniformity as shown in Eq. (8). It is clear that the performance gain from the align-
ment term is marginal while the gain from the uniformity term is signiﬁcant. Note that for only
alignment term, we compute the loss after predictor network instead of feature extractor, otherwise
representation collapse will turn out. This indicates that prototypical uniformity is more important
than prototypical alignment since BYOL has already performed instance alignment at an augmented
instance level. However, we note that the prototypical alignment term is essential to stabilize the
training process, as demonstrated in the results for CIFAR-20 with more clusters."
ABLATION STUDY,0.2372549019607843,"To demonstrate that BYOL is not robust to additional losses for deep clustering tasks, we integrate
CC (Li et al., 2021b) and PCL (Li et al., 2021a) into BYOL; the results in Table 4 show that both
of them compromise clustering performance and become unstable. This is because CC contrasts
the cluster probability not helpful for representation learning, and the representations of PCL would
collapse without negative examples; see Fig. A1 for detailed analysis. For fair comparisons of the
self-supervised learning framework, we integrate our ProtoCL into CC by replacing the cluster head
with our ProtoCL on the representations while keeping other ofﬁcial hyper-parameters unchanged.
Although class collision issue remains, the signiﬁcant improvements over CC on both datasets sug-
gest that class-level contrastive loss over representations of cluster centers is better than the one over
cluster probabilities and ProtoCL can be generalized to other self-supervised learning frameworks."
ABLATION STUDY,0.23921568627450981,"Qualitative ablation study.
Fig. 4 visualizes the distribution of representations learned from
MoCo v2, BYOL, NCC w/o ProtoCL, and NCC. The representations from MoCo v2 are mixed
up at the center due to the class collision issue. The rest can reduce this phenomenon where NCC
w/o ProtoCL produces more compact clusters than BYOL, and NCC further maintains distinct bor-
ders between different clusters. In addition, we also visualize the training and clustering results of
BYOL and NCC in Fig. A7, demonstrating that NCC can produce more uniform representations,
more balanced clusters, and better clustering performance than BYOL."
ABLATION STUDY,0.2411764705882353,"a) MoCo (NMI=76.6%)
b) BYOL (NMI=78.4%)
c) NCC w/o ProtoCL (NMI=78.8%)
d) NCC (NMI=85.5%)
Figure 4: Visualization of feature representations learned by different representation learning frame-
works and our proposed NCC on CIFAR-10 with t-SNE. Zoom in for better view."
ABLATION STUDY,0.24313725490196078,"Additional ablation studies.
To explore the inﬂuences of different hyper-parameters in NCC,
we perform the following ablation studies: 1) performing k-means clustering for every r epochs
in Fig. A2; 2) σ in positive sampling in Fig. A3; 3) λpcl for ProtoCL in Fig. A4; 4) predeﬁned
number of clusters K in Fig. A6; 5) projection dimension for self-supervised learning in Fig. A8; 6)
data augmentation in Fig. A9 for self-supervised learning; and 7) different ResNet architectures in
Table A5. All ablation study results verify that the performance gain of NCC does not come from
backbone, projection dimension, or any other hyper-parameters. The results also suggest that NCC
is robust to the choice of hyper-parameters."
CONCLUSION,0.24509803921568626,"6
CONCLUSION"
CONCLUSION,0.24705882352941178,"We have explored the non-contrastive representation learning for deep clustering. The proposed
positive sampling strategy and prototypical contrastive loss can lead to within-cluster compactness
and well-separated clusters towards the goodness of clustering. The results suggest that the proposed
NCC outperforms the state-of-the-art methods by a signiﬁcant margin. We hope our study will
attract the community’s attention to the non-contrastive representation learning methods for deep
clustering, which do not suffer from class collision issue."
CONCLUSION,0.24901960784313726,Under review as a conference paper at ICLR 2022
REFERENCES,0.25098039215686274,REFERENCES
REFERENCES,0.2529411764705882,"David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. Technical
report, Stanford, 2006."
REFERENCES,0.2549019607843137,"Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training
of deep networks. In Advances in Neural Information Processing Systems, pp. 153–160, 2007."
REFERENCES,0.2568627450980392,"Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing
Systems, 2019."
REFERENCES,0.25882352941176473,"Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-
pervised learning of visual features. In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 132–149, 2018."
REFERENCES,0.2607843137254902,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments.
arXiv preprint
arXiv:2006.09882, 2020."
REFERENCES,0.2627450980392157,"Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Deep adap-
tive image clustering. In Proceedings of the IEEE International Conference on Computer Vision,
pp. 5879–5887, 2017."
REFERENCES,0.2647058823529412,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning,
pp. 1597–1607. PMLR, 2020."
REFERENCES,0.26666666666666666,"Xinlei Chen and Kaiming He. Exploring simple Siamese representation learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021."
REFERENCES,0.26862745098039215,"Ching-Yao Chuang, Joshua Robinson, Lin Yen-Chen, Antonio Torralba, and Stefanie Jegelka. De-
biased contrastive learning. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.27058823529411763,"Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsuper-
vised feature learning. In Proceedings of the Fourteenth International Conference on Artiﬁcial
Intelligence and Statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011."
REFERENCES,0.2725490196078431,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchical image database. In Proceedings of the IEEE/CVF International Conference on Com-
puter Vision, pp. 248–255. Ieee, 2009."
REFERENCES,0.27450980392156865,"Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by
context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp.
1422–1430, 2015."
REFERENCES,0.27647058823529413,"Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. Advances in
Neural Information Processing Systems, 2019."
REFERENCES,0.2784313725490196,"Jeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Adversarial feature learning. International
Conference on Learning Representations, 2017."
REFERENCES,0.2803921568627451,"Linus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models
transfer? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 5414–5423, 2021."
REFERENCES,0.2823529411764706,"Abe Fetterman and Josh Albrecht. Understanding self-supervised and contrastive learning with
bootstrap your own latent (BYOL). https://untitled-ai.github.io/understanding-self-supervised-
contrastive-learning.html, 2020."
REFERENCES,0.28431372549019607,"Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. Advances in
Neural Information Processing Systems, 2020."
REFERENCES,0.28627450980392155,Under review as a conference paper at ICLR 2022
REFERENCES,0.28823529411764703,"Philip Haeusser, Johannes Plapp, Vladimir Golkov, Elie Aljalbout, and Daniel Cremers. Associative
deep clustering: Training a classiﬁcation network with no labels. In German Conference on
Pattern Recognition, pp. 18–32. Springer, 2018."
REFERENCES,0.2901960784313726,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016."
REFERENCES,0.29215686274509806,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729–9738, 2020."
REFERENCES,0.29411764705882354,"Qianjiang Hu, Xiao Wang, Wei Hu, and Guo-Jun Qi. AdCo: Adversarial contrast for efﬁcient
learning of unsupervised representations from self-trained negative adversaries. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1074–1083, 2021."
REFERENCES,0.296078431372549,"Jiabo Huang, Shaogang Gong, and Xiatian Zhu. Deep semantic clustering by partition conﬁdence
maximisation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 8849–8858, 2020."
REFERENCES,0.2980392156862745,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448–456.
PMLR, 2015."
REFERENCES,0.3,"Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised
image classiﬁcation and segmentation. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp. 9865–9874, 2019."
REFERENCES,0.30196078431372547,"Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural
Information Processing Systems, 2020."
REFERENCES,0.30392156862745096,"Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations, 2014."
REFERENCES,0.3058823529411765,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.307843137254902,"Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015."
REFERENCES,0.30980392156862746,"Junsoo Lee, Hojoon Lee, Inkyu Shin, Jaekyoung Bae, In So Kweon, and Jaegul Choo. Learning
representations by contrasting clusters while bootstrapping instances. 2020."
REFERENCES,0.31176470588235294,"Junnan Li, Pan Zhou, Caiming Xiong, and Steven Hoi. Prototypical contrastive learning of unsuper-
vised representations. In International Conference on Learning Representations, 2021a."
REFERENCES,0.3137254901960784,"Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive cluster-
ing. In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2021b."
REFERENCES,0.3156862745098039,"Stuart Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):
129–137, 1982."
REFERENCES,0.3176470588235294,"Chuang Niu, Hongming Shan, and Ge Wang. SPICE: Semantic pseudo-labeling for image cluster-
ing. arXiv preprint arXiv:2103.09382, 2021."
REFERENCES,0.3196078431372549,"Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In European Conference on Computer Vision, pp. 69–84. Springer, 2016."
REFERENCES,0.3215686274509804,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.3235294117647059,"Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. 2019."
REFERENCES,0.3254901960784314,Under review as a conference paper at ICLR 2022
REFERENCES,0.32745098039215687,"Jayanth Reddy Regatti, Aniket Anand Deshmukh, Eren Manavoglu, and Urun Dogan. Consensus
clustering with unsupervised representation learning. In International Joint Conference on Neural
Networks (IJCNN), pp. 1–9. IEEE, 2021."
REFERENCES,0.32941176470588235,"Pierre H Richemond, Jean-Bastien Grill, Florent Altch´e, Corentin Tallec, Florian Strub, Andrew
Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, et al. BYOL works even without
batch statistics. arXiv preprint arXiv:2010.10241, 2020."
REFERENCES,0.33137254901960783,"Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar.
A theoretical analysis of contrastive unsupervised representation learning. In International Con-
ference on Machine Learning, pp. 5628–5637. PMLR, 2019."
REFERENCES,0.3333333333333333,"Yuming Shen, Ziyi Shen, Menghan Wang, Jie Qin, Philip HS Torr, and Ling Shao. You never cluster
alone. arXiv preprint arXiv:2106.01908, 2021."
REFERENCES,0.3352941176470588,"Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classiﬁcation by keeping the good
and removing the bad momentum causal effect. In Advances in Neural Information Processing
Systems, 2020."
REFERENCES,0.33725490196078434,"Yaling Tao, Kentaro Takagi, and Kouta Nakata.
Clustering-friendly representation learning via
instance discrimination and feature decorrelation. International Conference on Learning Repre-
sentations, 2021."
REFERENCES,0.3392156862745098,"Tsung Wei Tsai, Chongxuan Li, and Jun Zhu. MiCE: Mixture of contrastive experts for unsupervised
image clustering. In International Conference on Learning Representations, 2021."
REFERENCES,0.3411764705882353,"Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine
Learning Research, 9(11), 2008."
REFERENCES,0.3431372549019608,"Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc
Van Gool. SCAN: Learning to classify images without labels. In European Conference on Com-
puter Vision, pp. 268–285. Springer, 2020."
REFERENCES,0.34509803921568627,"Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2495–2504, 2021."
REFERENCES,0.34705882352941175,"Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In International Conference on Machine Learning, pp.
9929–9939. PMLR, 2020."
REFERENCES,0.34901960784313724,"Xudong Wang, Ziwei Liu, and Stella X Yu. Unsupervised feature learning by cross-level instance-
group discrimination.
In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 12586–12595, 2021."
REFERENCES,0.3509803921568627,"Jianlong Wu, Keyu Long, Fei Wang, Chen Qian, Cheng Li, Zhouchen Lin, and Hongbin Zha. Deep
comprehensive correlation mining for image clustering. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pp. 8150–8159, 2019."
REFERENCES,0.35294117647058826,"Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3733–3742, 2018."
REFERENCES,0.35490196078431374,"Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.
In International Conference on Machine Learning, pp. 478–487. PMLR, 2016."
REFERENCES,0.3568627450980392,"Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. Towards k-means-friendly spaces:
Simultaneous deep learning and clustering. In International Conference on Machine Learning,
pp. 3861–3870. PMLR, 2017."
REFERENCES,0.3588235294117647,"Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations
and image clusters. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5147–5156, 2016."
REFERENCES,0.3607843137254902,Under review as a conference paper at ICLR 2022
REFERENCES,0.3627450980392157,"Lihi Zelnik-manor and Pietro Perona. Self-tuning spectral clustering. In Advances in Neural Infor-
mation Processing Systems, 2005."
REFERENCES,0.36470588235294116,"Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European Con-
ference on Computer Vision, pp. 649–666. Springer, 2016."
REFERENCES,0.36666666666666664,"Mingkai Zheng, Fei Wang, Shan You, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang
Xu. Weakly supervised contrastive learning. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2021."
REFERENCES,0.3686274509803922,"Huasong Zhong, Jianlong Wu, Chong Chen, Jianqiang Huang, Minghua Deng, Liqiang Nie,
Zhouchen Lin, and Xian-Sheng Hua.
Graph contrastive clustering.
In Proceedings of the
IEEE/CVF International Conference on Computer Vision, 2021."
REFERENCES,0.37058823529411766,"Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. BBN: Bilateral-branch network with
cumulative learning for long-tailed visual recognition. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pp. 9719–9728, 2020."
REFERENCES,0.37254901960784315,Under review as a conference paper at ICLR 2022
REFERENCES,0.37450980392156863,APPENDICES
REFERENCES,0.3764705882352941,"A
ADDITIONAL RELATED WORK"
REFERENCES,0.3784313725490196,"Self-supervised learning.
Previous self-supervised learning (SSL) methods for representation
learning attempt to capture the data distribution using generative models (Donahue et al., 2017;
Donahue & Simonyan, 2019) or learn the representations through some special designed pretext
tasks (Doersch et al., 2015; Noroozi & Favaro, 2016; Zhang et al., 2016; Caron et al., 2018). In
recent years, contrastive learning methods (Wu et al., 2018; He et al., 2020; Chen et al., 2020)
have shown promising results for both representation learning and downstream tasks. For example,
MoCo (He et al., 2020) uses a memory queue to store the consistent representations output by a
moving-averaged encoder. However, the class collision issue remains unavoidable; i.e., the seman-
tic similar instances are pushed away since they could be regarded as negative pairs (Saunshi et al.,
2019). Some attempts have been made to address this issue (Khosla et al., 2020; Hu et al., 2021;
Chuang et al., 2020). On the contrary, the recent studies of SSL demonstrate that the negative exam-
ples are not necessary, termed non-contrastive methods (Caron et al., 2020; Grill et al., 2020; Chen
et al., 2020). In summary, SSL methods mainly focus on inducing transferable representations for
the downstream tasks instead of grouping the data into different semantic classes in deep clustering."
REFERENCES,0.3803921568627451,"Relation to CC.
Although both Lpcl and CC (Li et al., 2021b) are class-level contrastive loss,
which perform contrastive learning at the cluster level, they have the following difference."
REFERENCES,0.38235294117647056,"• The class-level contrastive loss in CC implements the contrastive loss on the cluster probabilities
while ours on the representation of cluster centers. Implementing contrastive loss on the cluster
probability in (Li et al., 2021b) would lose the semantic information of the learned representations,
which is not helpful for representation learning. Speciﬁcally, given the x ∈B, CC obtains the
cluster assignments Pk = [p(k|x(1)), . . . , p(k|x(N))] from one view and Pk′ from another view,
and then contrasts Pk and Pk′ at the cluster level using the InfoNCE loss. In contrast, Lpcl
implements the contrastive loss on the representation of the cluster centers within a mini-batch
using the pseudo-labels from k-means clustering. As a result, Lpcl is able to sense the semantic
information of the latent space and make the representations of clusters more discriminative and
suitable for the clustering task."
REFERENCES,0.3843137254901961,"• The class-level contrastive loss in CC does not encourage cluster uniformity while our ProtoCL
does. CC still needs the instance-wise contrastive loss to encourage the instance uniformity, which
inevitably introduces the class collision issue. We have also conducted experiments that integrate
CC into BYOL and reported the results in Table 4. The performance of BYOL drops and the
training becomes unstable. Under the same conditions, NCC achieves signiﬁcant improvements
over CC."
REFERENCES,0.3862745098039216,"Relation to GCC and WCL.
GCC (Zhong et al., 2021) and WCL (Zheng et al., 2021) built
a graph to label the neighbor samples as pseudo-positive examples. Then, they enforce the two
data augmentations of one example to be close to its multiple pseudo-positive examples using a
supervised contrastive loss. GCC adopted a moving-averaged memory bank for the graph-based
pseudo-labeling while WCL built the graph within a mini-batch. GCC and WCL mainly focus
on how to effectively select positive examples from mini-batch/memory bank to alleviate the class
collision issue. Here, we divide the class collision issue into the following two cases:"
REFERENCES,0.38823529411764707,"• Negative class collision issue: negative examples may not be truly negative, which is the case we
discussed in the paper."
REFERENCES,0.39019607843137255,"• Positive class collision issue: positive examples may not be truly positive, which is a new case
raised in GCC and WCL."
REFERENCES,0.39215686274509803,"Consequently, they still suffer from the positive class collision issue as the selected pseudo-positive
examples may not be truly positive. In addition to this, they also suffer from the negative class
collision issue since they still need negative examples for instance-wise contrastive learning."
REFERENCES,0.3941176470588235,"We summarize the difference between our positive sampling strategy and theirs in the following four
aspects."
REFERENCES,0.396078431372549,Under review as a conference paper at ICLR 2022
REFERENCES,0.3980392156862745,"• GCC and WCL select the examples that exist in the dataset (mini-batch/memory bank) while ours
samples augmented examples from the latent space that may not exist in the dataset.
• GCC and WCL select neighbor examples in a graph as pseudo-positive examples that may not be
truly positive while ours samples examples around the instance that can be assumed to be positive
in the semantic space.
• GCC and WCL still rely on instance-wise contrastive loss that could lead to class collision issue
while ours can avoid class collision issue by using BYOL.
• GCC and WCL require additional computational cost for graph construction while ours is rather
cheap in sampling one augmented example."
REFERENCES,0.4,"Relation to PCL.
Here, we summarize the difference between our NCC and PCL (Li et al., 2021a)
in terms of the losses and EM frameworks. First, we summarize the difference of our ProtoCL loss
and ProtoNCE loss in PCL as follow."
REFERENCES,0.4019607843137255,"• Our NCC can avoid class collision issue while PCL cannot. NCC is based on BYOL that does
not require negative examples for representation learning while PCL is based on instance-wise
contrastive loss that requires a number of negative examples for representation learning, inevitably
leading to class collision issue.
• The proposed ProtoCL in NCC is conceptually different from the ProtoNCE in PCL. ProtoCL
is to maximize the inter-cluster distance to form a uniformly distributed space while ProtoNCE
is to minimize the instance-to-cluster distance to improve the within-cluster compactness. The
within-cluster compactness of NCC is improved by the proposed positive sampling strategy.
• Pure ProtoCL can work well for deep clustering while ProtoNCE requires another InfoNCE to
form uniformly distributed space. This is a direct result of the different designs of the losses.
ProtoCL can maximize the inter-cluster distance to form a uniformly distributed space while Pro-
toNCE suffers from collapse without the help of another InfoNCE to form such a space."
REFERENCES,0.403921568627451,"Second, we summarize difference between our NCC and PCL in the EM framework. Formulating
NCC into an EM framework can offer more insights about NCC and make it easy to understand.
Although both in an EM framework, the M-step in PCL is signiﬁcantly different from the one in
our NCC. More speciﬁcally, the M-step in PCL is to optimize the ProtoNCE, which is an instance-
to-prototypes contrastive loss to improve the within-cluster compactness while the M-step in our
NCC is to optimize the proposed ProtoCL, which is a prototypes-to-prototypes contrastive loss
to maximize the inter-cluster distance for better clustering performance. In addition, NCC also
proposes a positive sampling strategy by sampling positive examples around each sample to improve
within-cluster compactness. Finally, Table 3 demonstrates that NCC outperforms PCL by almost
10% AMI on the ImageNet-1K dataset."
REFERENCES,0.40588235294117647,"B
EM FRAMEWORK"
REFERENCES,0.40784313725490196,"In this section, we ﬁrst describe the von Mises-Fisher (vMF) distribution on the hypersphere, and
then derive the ELBO for our EM framework, followed by detailed E-step and M-step. Finally, we
describe our proposed prototypical contrastive loss and provide proof for convergence analysis."
REFERENCES,0.40980392156862744,"Von Mises-Fisher distribution.
Since the features in current SSL methods are usually ℓ2-
normalized, it is more proper to employ the spherical distribution to describe the features. The
von Mises-Fisher (vMF) distribution, often seen as the Gaussian distribution on a hypersphere, is
parameterized by µ ∈Rd the mean direction and κ ∈R+ the concentration around µ. For the
special case of κ = 0, the vMF distribution represents a uniform distribution on the hypersphere.
The PDF of vMF distribution for the random unit vector v ∈Rd is deﬁned as:"
REFERENCES,0.4117647058823529,"p(v | µ, κ) = Cd(κ) exp(κµTv);
Cd(κ) =
κd/2−1"
REFERENCES,0.4137254901960784,"(2π)d/2Id/2−1(κ),
(A1)"
REFERENCES,0.41568627450980394,"where d is the feature dimension, ∥µ∥2 = 1, Cd(κ) is the normalizing constant, and Iv denotes the
modiﬁed Bessel function of the ﬁrst kind at order v. The standard Gaussian distribution z ∼N(0, I)
can be approximately seen as the uniform vMF distribution if the z is ℓ2-normalized and d is large
for the high dimension data."
REFERENCES,0.4176470588235294,Under review as a conference paper at ICLR 2022
REFERENCES,0.4196078431372549,"Derivation of ELBO.
Given the dataset D = {x(n)}N
n=1 with N observed data points that are
related to a set of K cluster latent variables, k ∈K = {1, 2, . . . , K}, the marginal likelihood can be
written as:"
REFERENCES,0.4215686274509804,"L(D; θ) = 1 N N
X"
REFERENCES,0.4235294117647059,"n=1
log p(x(n); θ) = 1 N N
X"
REFERENCES,0.42549019607843136,"n=1
log
X"
REFERENCES,0.42745098039215684,"k∈K
p(x(n), k; θ),
(A2)"
REFERENCES,0.4294117647058823,"where θ denotes the model parameters. Eq. (A2) is usually maximized to train the neural network.
However, it is hard to directly optimize the log-likelihood function. Using an inference model q(k)
like VAE (Kingma & Welling, 2014) to approximate the distribution of K, especially P"
REFERENCES,0.43137254901960786,"k∈K q(k) =
1, we can re-write the log-likelihood function for one example as:"
REFERENCES,0.43333333333333335,"log p(x; θ) =
X"
REFERENCES,0.43529411764705883,"k∈K
q(k) log p(x; θ)
(A3) =
X"
REFERENCES,0.4372549019607843,"k∈K
q(k)(log p(x, k; θ) −log p(k|x; θ))
(A4) =
X"
REFERENCES,0.4392156862745098,"k∈K
q(k) log p(x, k; θ)"
REFERENCES,0.4411764705882353,"q(k)
−
X"
REFERENCES,0.44313725490196076,"k∈K
q(k) log p(k|x; θ)"
REFERENCES,0.44509803921568625,"q(k)
(A5)"
REFERENCES,0.4470588235294118,"= ELBO(q, x; θ) + KL(q(k)∥p(k|x; θ)),
(A6)"
REFERENCES,0.44901960784313727,"where p(x, k; θ) = p(k|x; θ)p(x; θ) so we have log p(x; θ) = log p(x, k; θ) −log p(k|x; θ)
and the evidence lower bound (ELBO) is the lower bound of log-likelihood function since
KL(q(k)∥p(k|x; θ)) ≥0. When KL(q(k)∥p(k|x; θ)) = 0, the ELBO reaches its maximum value
log p(x; θ), making q(k) = p(k|x; θ). By replacing q(k) with p(k|x; θ) and ignoring the constant
value P"
REFERENCES,0.45098039215686275,"k∈K −q(k) log q(k), we are ready to maximize:
X"
REFERENCES,0.45294117647058824,"k∈K
p(k|x; θ) log p(x, k; θ).
(A7)"
REFERENCES,0.4549019607843137,"E-step.
With the ﬁxed θt at the iteration t, this step aims to estimate qt+1(k) that makes
qt+1(k) = p(k|x; θt) so that ELBO(qt+1, x; θt) = log p(x; θt). Here, we perform the spheri-
cal k-means algorithm to estimate p(k|x; θt). We extract features from the target network since
the target network performs more stable and yields more consistent clusters, similar to BYOL and
MoCo."
REFERENCES,0.4568627450980392,"M-step.
With the ﬁxed suboptimal qt+1(k) = p(k|x; θt) after E-step, we turn to optimize the θ to
maximize the ELBO:"
REFERENCES,0.4588235294117647,"θt+1 = arg max
θ N
X"
REFERENCES,0.46078431372549017,"n=1
ELBO(qt+1, x(n); θ).
(A8)"
REFERENCES,0.4627450980392157,"Using a uniform prior for k as p(k) = 1/K, we can obtain p(x, k; θ) = p(k)p(x|k; θ) =
p(x|k; θ)/K. By replacing p(x, k; θ) in Eq. (A7) and ignoring constant value, in this step, we
should maximize:
X"
REFERENCES,0.4647058823529412,"k∈K
1(x ∈k) log p(x|k; θ),
(A9)"
REFERENCES,0.4666666666666667,"where p(k|x; θ) = 1(x ∈k). 1(·) is an indicator function using the hard labels estimated from E-
step so that 1(x ∈k) = 1 if x belongs to k-th cluster; otherwise 1(x ∈k) = 0. Following (Li et al.,
2021a), if we assume that the distribution for each cluster is the vMF distribution with a constant κ
as the temperature of softmax function, we can further obtain the follow:"
REFERENCES,0.46862745098039216,"p(x|k; θ) =
exp(µT
kv/τ)
PK
k=1 exp(µT
kv/τ)
,
(A10)"
REFERENCES,0.47058823529411764,"where v = f(x; θ), τ = 1/κ, and µk is the cluster center of k-th cluster. Combining Eqs. (A9)
and (A10), we can achieve the maximum log-likelihood estimation to ﬁnd the optimal θ∗by mini-
mizing the following negative log-likelihood:"
REFERENCES,0.4725490196078431,"θ∗= arg min
θ N
X"
REFERENCES,0.4745098039215686,"n=1
−log
exp(µT
y(n)v(n)/τ)
PK
k=1 exp(µT
kv(n)/τ)
,
(A11)"
REFERENCES,0.4764705882352941,Under review as a conference paper at ICLR 2022
REFERENCES,0.47843137254901963,where y(n) is the pseudo-label for x(n) estimated by the k-means algorithm in E-step.
REFERENCES,0.4803921568627451,"Directly optimizing Eq. (A11) usually leads to improve the cluster compactness, which, however,
will compromise the stability of BYOL since it does not consider the uniformity term. To this end,
we propose a prototypical contrastive loss (ProtoCL) to maximize the log-likelihood at the cluster
level by employing the clusters centers as the special instances, or prototypes from a set of instances.
The ProtoCL is deﬁned as:"
REFERENCES,0.4823529411764706,Lpcl = 1 K XK
REFERENCES,0.4843137254901961,"k=1 −log
exp(µT
kµ′
k/τ)"
REFERENCES,0.48627450980392156,"exp(µT
kµ′
k/τ) + PK
j=1,j̸=k exp(µT
kµj/τ)
,
(A12)"
REFERENCES,0.48823529411764705,"where {µ1, µ2, . . . , µK} and {µ′
1, µ′
2, . . . , µ′
K} are K prototypes from target and online networks,
respectively. Here, instead of using the centroids computed from k-means, our cluster center µk and
µ′
k is empirically estimated within mini-batch B as follows:"
REFERENCES,0.49019607843137253,"µk =
P"
REFERENCES,0.492156862745098,"x∈B p(k|x)f(x)
∥P
x∈B p(k|x)f(x)∥2
and
µ′
k =
P"
REFERENCES,0.49411764705882355,"x∈B p(k|x)f ′(x)
∥P
x∈B p(k|x)f ′(x)∥2
,
(A13)"
REFERENCES,0.49607843137254903,"where p(k|x) is estimated from E-step. When K > |B|, it is obvious that the mini-batch cannot
cover all clusters. To this end, we zero out the losses and logits of empty clusters for each iteration;
see the pseudocode in Appendix D for more details."
REFERENCES,0.4980392156862745,"Intuitively, ProtoCL can encourage the prototypical alignment between two augmented views and
the prototypical uniformity, hence maximizing the inter-cluster distance."
REFERENCES,0.5,"Convergence
analysis.
At
E-step
of
the
iteration
t,
we
estimate
qt+1(k)
to
make
ELBO(qt+1, x; θt) = log p(x; θt). At M-step after E-step, we obtain the optimized θt+1 with
the ﬁxed qt+1(k) so that ELBO(qt+1, x; θt+1) ≥ELBO(qt+1, x; θt). Consequently, we obtain the
following sequence:"
REFERENCES,0.5019607843137255,"log p(x; θt+1) ≥ELBO(qt+1, x; θt+1) ≥ELBO(qt+1, x; θt) = log p(x; θt).
(A14)"
REFERENCES,0.503921568627451,"Given log p(x; θt+1) ≥log p(x; θt), we can guarantee the convergence of our NCC."
REFERENCES,0.5058823529411764,"C
EXPERIMENTAL SETUP"
REFERENCES,0.5078431372549019,"Datasets.
We
conducted
experiments
on
six
benchmark
datasets,
including
CIFAR-
10 (Krizhevsky et al., 2009), CIFAR-20 (Krizhevsky et al., 2009), STL-10 (Coates et al.,
2011), ImageNet-10 (Chang et al., 2017), ImageNet-Dogs (Chang et al., 2017), ImageNet-
1K (Deng et al., 2009), which are summarized in Table 1.
We note that CIFAR-20 contains
20 superclasses of CIFAR-100.
STL-10 includes extra unlabeled images.
ImageNet-10 and
ImageNet-Dogs are the subset of ImageNet-1K, containing 10 and 15 classes, respectively. This
paper follows the experimental settings widely used in deep clustering work (Chang et al., 2017; Wu
et al., 2019; Ji et al., 2019; Tsai et al., 2021; Tao et al., 2021), including the image size, backbone
and train-test split. For image size, we have used 32 × 32 for CIFAR-10 and CIFAR-20, 96 × 96 for
STL-10, ImageNet-10 and ImageNet-Dogs, and 224 × 224 for ImageNet-1K. For train-test split,
we use the whole datasets including training and testing set for CIFAR-10 and CIFAR-20."
REFERENCES,0.5098039215686274,"Backbone.
We use ResNet-34 (He et al., 2016) as the backbone for fair comparisons to report the
results in Table 2. Unless noted otherwise, we use ResNet-18 for the rest of the experiments. Since
the image sizes of CIFAR-10 and CIFAR-100 are relatively small, following (Chen et al., 2020), we
replace the ﬁrst convolution layer of kernel size 7 × 7 and stride 2 with a 3 × 3 Conv of stride 1 and
remove the ﬁrst max-pooling layer for all experiments on CIFAR-10 and CIFAR-100."
REFERENCES,0.5117647058823529,"Metrics.
We employ three common metrics to evaluate the clustering performance for the for-
mer ﬁve datasets, including Normalized Mutual Information (NMI), Cluster Accuracy (ACC), and
Adjusted Rand Index (ARI). Following (Li et al., 2021a), we report Adjusted Mutual Informa-
tion (AMI) to evaluate the clustering performance for ImageNet-1K. All the metrics are presented
in percentage (%), and the higher the better clustering performance. We run each experiment three
times and report the mean and standard deviation as the ﬁnal results."
REFERENCES,0.5137254901960784,"Optimization.
We train all methods with 1000 epochs, strictly following the literature (Tsai et al.,
2021; Tao et al., 2021), and adopt the SGD optimizer and the cosine decay learning rate schedule"
REFERENCES,0.515686274509804,Under review as a conference paper at ICLR 2022
REFERENCES,0.5176470588235295,"with 50 epochs for learning rate warmup. The base learning rate for MoCo v2, BYOL, and NCC
were 0.05, scaled linearly with the batch size (LearningRate = 0.05×BatchSize/256). Note that the
learning rates for predictor networks of BYOL and NCC are 10× as the learning rate of feature
extractor. It is relatively important to achieve satisfactory performance, as discussed in (Grill et al.,
2020; Chen & He, 2021). For other hyperparameters of NCC, the temperature τ, λpcl for prototyp-
ical contrastive loss, and σ for positive sampling were set as 0.5, 0.1, and 0.001, respectively. The
mini-batch size was 512 for MoCo and 256 for the rest methods, trained on 4 NVIDIA V100 GPUs.
In terms of the results of CC (Li et al., 2021b) and PCL (Li et al., 2021a) in Table 4, we tried our
best to reproduce their results for fair comparisons. For CC, we used their ofﬁcial code. For PCL,
under the fair conditions of MoCo, we set the loss weight of ProtoNCE to 0.01 and the number of
clusters to {250, 500, 1000} following the suggestions of authors, which we found can achieve the
best results. We integrated CC and PCL into BYOL by adding their losses without changing other
settings."
REFERENCES,0.5196078431372549,"Conﬁgurations of self-supervised learning frameworks.
We adopt the same data augmentations
as SimCLR (Chen et al., 2020), including ResizedCrop, ColorJitter, Grayscale, and HorizontalFlip.
We have removed GaussianBlur since we only used a small image size for all datasets. We also
strictly follow the settings of BYOL (Grill et al., 2020). Speciﬁcally, despite the standard ResNet
backbones, the projection and predictor networks have the architectures of FC-BN-ReLU-FC, where
the projection dimension and hidden size were 256 and 4096 for both two networks, respectively. In
the context of the whole paper, we have included the backbone and projection network as the encoder
network, or feature extractor, since the architectures except predictor are symmetric across views.
For fair comparisons, we have also set the projection dimension of MoCo v2 as 256. We have used
symmetric loss for all methods, i.e., swapping two data augmentations to compute twice loss. Given
the target network f ′
θ′ and online network fθ, the momentum rule describes that the θ′ is updated
as θ′ = mθ′ + (1 −m)θ with a momentum hyperparameter m ∈[0, 1). We set the m to 0.996 for
both BYOL and NCC same as (Grill et al., 2020) and 0.99 for MoCo v2. For MoCo v2, the queue
size, temperature for InfoNCE loss, weight decay were 4096, 1.0, 1.0×10−4, respectively. We have
not employed SyncBN in NCC like BYOL. We note that SyncBN will introduce 1/3 additional
computation costs. Instead, we adopt the shufﬂingBN in MoCo to avoid the trivial solution of non-
contrastive learning."
REFERENCES,0.5215686274509804,"Conﬁgurations of ImageNet-1K.
For ImageNet-1K, the weight decay and base learning rate were
10−4 and 0.4, respectively. We employ ResNet-50 as the backbone, train our method for 200 epochs
without symetric loss to reduce training time, under the same conditions of PCL (Li et al., 2021a).
We include the GaussianBlur to the data augmentations since we use the original image size for
training. During evaluation, images are resized to 256 × 256 and center cropped. Due to the large
computation cost, we run NCC on ImageNet-1K only once."
REFERENCES,0.5235294117647059,"Computational cost.
Both BYOL and MoCo use a momentum-updated network f ′
θ′. The com-
putation cost between them lies on the fact that MoCo needs a large number of negative examples
while BYOL uses a large hidden size for the predictor network. Our NCC is built upon BYOL, the
main additional computational cost is the k-means clustering procedure. We have implemented the
k-means algorithm with k-means++ (Arthur & Vassilvitskii, 2006) initialization using PyTorch to
utilize the GPU and accelerate the clustering process. Taking CIFAR-10 and CIFAR-20 datasets for
example, it takes only 5 seconds for k-means clustering, leading to only 5 hours of training time.
Besides, as suggested in the results of Fig. A2, NCC is robust to the different r, so that there is no
need to perform k-means for every epoch. The computational cost of the ProtoCL is also small since
we build the cluster centers within mini-batch, saying that NCC does not need additional memory to
store the cluster centers. Consequently, considering the promising performance improvements, the
additional computational cost is relatively affordable."
REFERENCES,0.5254901960784314,Under review as a conference paper at ICLR 2022
REFERENCES,0.5274509803921569,"D
PSEUDOCODE OF NCC"
REFERENCES,0.5294117647058824,Algorithm 1 Pseudocode of NCC in a PyTorch-like style
REFERENCES,0.5313725490196078,"1
# fθ, f ′
θ′, gϕ: online network, target network, predictor respectively;"
REFERENCES,0.5333333333333333,"2
# K: number of clusters, τ: temperature for ProtoCL loss;"
REFERENCES,0.5352941176470588,"3
# m: momentum, max_epochs: the training epochs;"
REFERENCES,0.5372549019607843,"4
# σ: sampling standard deviation of Gaussian distribution;"
REFERENCES,0.5392156862745098,"5
# λpcl: loss weight of protocl loss, λpcl = 0 during learning-rate warmup;"
REFERENCES,0.5411764705882353,"6
# r: performs K-means clustering for every r epoch; 7"
REFERENCES,0.5431372549019607,"8
# compute the centers from their representations and pseudo-labels;"
REFERENCES,0.5450980392156862,"9
def compute_centers(z, y):"
REFERENCES,0.5470588235294118,"10
weight = onehot(y).T # KxN;"
REFERENCES,0.5490196078431373,"11
weight = normalize(weight, p=1, dim=1) # ℓ1-normalization;"
REFERENCES,0.5509803921568628,"12
centers = normalize(weight.mm(z), p=2, dim=1) # ℓ2-normalization;"
RETURN CENTERS,0.5529411764705883,"13
return centers 14"
RETURN CENTERS,0.5549019607843138,"15
# compute ProtoCL from the cluster centers of two views"
RETURN CENTERS,0.5568627450980392,"16
def compute_protocl_loss(µq, µk, y):"
RETURN CENTERS,0.5588235294117647,"17
zero_classes = counts(y) # find the empty clusters with indices;"
RETURN CENTERS,0.5607843137254902,"18
# align two views, Kx1;"
RETURN CENTERS,0.5627450980392157,"19
proto_alignment = (µq * µk).sum(dim=1, keepdim=True) / τ"
RETURN CENTERS,0.5647058823529412,"20
proto_uniformity = µq.mm(µq.T) / τ # inter-cluster distance;"
RETURN CENTERS,0.5666666666666667,"21
# fill the logits of empty clusters with -10;"
RETURN CENTERS,0.5686274509803921,"22
proto_uniformity = proto_uniformity.fill_(-10, zero_classes, dim=1)"
RETURN CENTERS,0.5705882352941176,"23
# retrive inter-cluster distance for proto uniformity;"
RETURN CENTERS,0.5725490196078431,"24
proto_uniformity = proto_uniformity[off_diagonal_mask] # Kx(K-1);"
RETURN CENTERS,0.5745098039215686,"25
protocl_loss = - proto_alignment + logsumexp(cat([proto_alignment,
proto_uniformity], dim=1), dim=1)
,→"
RETURN CENTERS,0.5764705882352941,"26
protocl_loss[zero_classes] = 0 # zero the loss of empty clusters;"
RETURN CENTERS,0.5784313725490197,"27
# neglect the empty clusters;"
RETURN CENTERS,0.5803921568627451,"28
protocl_loss = protocl_loss.sum() / (K - len(zero_classes))"
RETURN CENTERS,0.5823529411764706,"29
return protocl_loss 30"
RETURN CENTERS,0.5843137254901961,"31
# compute both augmented instance alignment and ProtoCL;"
RETURN CENTERS,0.5862745098039216,"32
def forward_loss(im_q, im_k, y):"
RETURN CENTERS,0.5882352941176471,"33
q, k = fθ(im_q), f ′
θ′(im_k) # forward;"
RETURN CENTERS,0.5901960784313726,"34
ins_loss = mse(gϕ(q + σ * randn_like(q)), k) # instance alignment;"
RETURN CENTERS,0.592156862745098,"35
µq = compute_centers(q, y) # compute the centers of two views;"
RETURN CENTERS,0.5941176470588235,"36
µk = compute_centers(k, y)"
RETURN CENTERS,0.596078431372549,"37
protocl_loss = compute_protocl_loss(µq, µk, y)"
RETURN CENTERS,0.5980392156862745,"38
return ins_loss + protocl_loss * λpcl
39"
RETURN CENTERS,0.6,"40
θ′.copy_(θ)
# initialize the target network with online network;"
RETURN CENTERS,0.6019607843137255,"41
for epoch in range(max_epochs): # start training;"
RETURN CENTERS,0.6039215686274509,"42
# E-step"
RETURN CENTERS,0.6058823529411764,"43
if epoch % r == 0:"
RETURN CENTERS,0.6078431372549019,"44
# K-means based on target network for every r epochs: N labels"
RETURN CENTERS,0.6098039215686275,"45
pseudo_labels = K-means(loader, f ′
θ′)"
RETURN CENTERS,0.611764705882353,"46
# M-step"
RETURN CENTERS,0.6137254901960785,"47
for indices, im in loader: # load a mini-batch x with N samples;"
RETURN CENTERS,0.615686274509804,"48
im_q, im_k = aug(im) # two randomly data augmentations;"
RETURN CENTERS,0.6176470588235294,"49
y = pseudo_labels[indices] # pseudo-labels in each batch;"
RETURN CENTERS,0.6196078431372549,"50
# symetric loss same as vanilla BYOL: swap inputs of two views;"
RETURN CENTERS,0.6215686274509804,"51
loss = forward_loss(im_q, im_k, y) + forward_loss(im_k, im_q, y)"
RETURN CENTERS,0.6235294117647059,"52
loss /= 2"
RETURN CENTERS,0.6254901960784314,"53
# SGD updating online network and predictor;"
RETURN CENTERS,0.6274509803921569,"54
loss.backward()"
RETURN CENTERS,0.6294117647058823,"55
update(θ, ϕ)"
RETURN CENTERS,0.6313725490196078,"56
# momentum update target network;"
RETURN CENTERS,0.6333333333333333,"57
θ′.copy_(m*θ′+(1-m)*θ)"
RETURN CENTERS,0.6352941176470588,Under review as a conference paper at ICLR 2022
RETURN CENTERS,0.6372549019607843,"E
ADDITIONAL EXPERIMENTAL RESULTS"
RETURN CENTERS,0.6392156862745098,"0
200
400
600
800
1000
Epoch 0.0 0.2 0.4 0.6 0.8 NMI"
RETURN CENTERS,0.6411764705882353,a) Normalized Mutual Information
RETURN CENTERS,0.6431372549019608,"PCL+MoCo
PCL+BYOL"
RETURN CENTERS,0.6450980392156863,"0
200
400
600
800
1000
Epoch 0 1 d STD"
RETURN CENTERS,0.6470588235294118,b) Standard Deviation
RETURN CENTERS,0.6490196078431373,"PCL+MoCo
PCL+BYOL"
RETURN CENTERS,0.6509803921568628,"Figure A1:
Visualizations of NMIs and STDs by applying PCL (Li et al., 2021a) to MoCo and
BYOL on CIFAR-10. Compared to PCL+BYOL, PCL+MoCo performs more stable during cluster-
ing with a more uniform distribution of representations. The decreasing STDs (standard deviation
of ℓ2-normalized features) also indicate that PCL+BYOL suffers from the representation collapse.
This is because PCL can only improve the within-cluster compactness. During training, the ﬁxed
prototypes of PCL will also be gradually collapsed without negative examples, making the repre-
sentations for BYOL collapse at the same time. These results validate our assumptions that BYOL
is not robust to additional clustering losses for clustering tasks, since there is no negative example
for BYOL to maintain uniform representations to avoid collapse. Our NCC can avoid the class col-
lision issue and representation collapse by the proposed positive sampling strategy to improve the
within-cluster compactness and prototypical contrastive loss to maximize the inter-class distance."
RETURN CENTERS,0.6529411764705882,"BYOL
r = 1
r = 2
r = 4
r = 8
r = 16
40 50 60 70 80 90 100"
RETURN CENTERS,0.6549019607843137,Performance (%)
RETURN CENTERS,0.6568627450980392,a) CIFAR-10
RETURN CENTERS,0.6588235294117647,"NMI
ACC
ARI"
RETURN CENTERS,0.6607843137254902,"BYOL
r = 1
r = 2
r = 4
r = 8
r = 16
10 20 30 40 50 60 70"
RETURN CENTERS,0.6627450980392157,Performance (%)
RETURN CENTERS,0.6647058823529411,b) CIFAR-20
RETURN CENTERS,0.6666666666666666,"NMI
ACC
ARI"
RETURN CENTERS,0.6686274509803921,"Figure A2: The effect of the hyperparameter r. NCC performs K-means clustering for every r epoch.
Here we study how different r inﬂuences the clustering performance. The results demonstrate NCC
is robust to large r and the cluster pseudo-labels, which means it is not necessary to perform clus-
tering for every epoch so that the computation cost can be signiﬁcantly reduced. In summary, we
suggest that r can be set to [1, 8] by considering the datasets and computation resources."
RETURN CENTERS,0.6705882352941176,Under review as a conference paper at ICLR 2022
RETURN CENTERS,0.6725490196078432,"0
10
4
10
3
5 × 10
3
10
2
5 × 10
2
40 50 60 70 80 90 100"
RETURN CENTERS,0.6745098039215687,Performance (%)
RETURN CENTERS,0.6764705882352942,a) CIFAR-10
RETURN CENTERS,0.6784313725490196,Metric
RETURN CENTERS,0.6803921568627451,"NMI
ACC
ARI"
RETURN CENTERS,0.6823529411764706,"0
10
4
10
3
5 × 10
3
10
2
5 × 10
2 10 20 30 40 50 60 70"
RETURN CENTERS,0.6843137254901961,Performance (%)
RETURN CENTERS,0.6862745098039216,b) CIFAR-20
RETURN CENTERS,0.6882352941176471,Metric
RETURN CENTERS,0.6901960784313725,"NMI
ACC
ARI"
RETURN CENTERS,0.692156862745098,"Figure A3: The effect of the hyperparameter σ for positive sampling. Taking a look at σ ∼[0, 10−3],
although introducing the positive sampling into BYOL causes a slight drop on CIFAR-10, the clus-
tering performance becomes more stable as evidenced by the standard deviation. This is because the
neighbors of one sample are regarded as positive examples. Besides, the performance for CIFAR-20
has increased over baseline BYOL with the standard deviation reduced. These results indicate that
positive sampling can improve the stability of BYOL. However, when σ is too large, the performance
becomes unstable and drops a lot. It is not surprised since during positive sampling with large σ, the
instances from other clusters could be sampled and regarded as positive examples. Therefore, we
suggest setting σ to a small value, saying (0, 10−3]."
RETURN CENTERS,0.6941176470588235,"0
0.01
0.05
0.1
0.5
1
40 50 60 70 80 90 100"
RETURN CENTERS,0.696078431372549,Performance (%)
RETURN CENTERS,0.6980392156862745,a) CIFAR-10
RETURN CENTERS,0.7,"NMI
ACC
ARI"
RETURN CENTERS,0.7019607843137254,"0
0.01
0.05
0.1
0.5
1
10 20 30 40 50 60 70"
RETURN CENTERS,0.703921568627451,Performance (%)
RETURN CENTERS,0.7058823529411765,b) CIFAR-20
RETURN CENTERS,0.707843137254902,"NMI
ACC
ARI"
RETURN CENTERS,0.7098039215686275,"Figure A4: The effect of the hyperparameter λpcl for the ProtoCL. The results suggest that NCC
is robust to different loss weights of ProtoCL loss on CIFAR-20. However, the higher loss weight
leads to instability on CIFAR-10. The possible reason is that CIFAR-20 is more diverse and has
more semantic classes than CIFAR-10 (100 versus 10). Anyway, we suggest that the loss weight
for ProtoCL loss can be set to [0.01, 0.1] for different situations, which has demonstrated superior
performance on both two datasets."
RETURN CENTERS,0.711764705882353,Under review as a conference paper at ICLR 2022
RETURN CENTERS,0.7137254901960784,"Figure A5: Visualizations for the outlier points produced by the model at 1000-th epoch on CIFAR-
10 with t-SNE."
RETURN CENTERS,0.7156862745098039,"10
20
30
40
50
Number of clusters 30 40 50 60 70 80 90"
RETURN CENTERS,0.7176470588235294,Performance (%)
RETURN CENTERS,0.7196078431372549,a) CIFAR-10
RETURN CENTERS,0.7215686274509804,Method
RETURN CENTERS,0.7235294117647059,"NCC
BYOL"
RETURN CENTERS,0.7254901960784313,"10
20
30
40
50
Number of clusters 10 20 30 40 50 60 70"
RETURN CENTERS,0.7274509803921568,Performance (%)
RETURN CENTERS,0.7294117647058823,b) CIFAR-20
RETURN CENTERS,0.7313725490196078,Method
RETURN CENTERS,0.7333333333333333,"NCC
BYOL"
RETURN CENTERS,0.7352941176470589,"Figure A6: The effect of the predeﬁned number of clusters K. We reported NMIs following (Caron
et al., 2018). We note that the predeﬁned K of NCC during the training of NCC is the same as
the K in k-means clustering process for evaluation. To further demonstrate the inﬂuences of over-
clustering, we also reported the results of vanilla BYOL during k-means clustering. The results
demonstrate that both BYOL and NCC have the same over-clustering behavior on these two datasets;
that is, opposite trends on these two datasets. Speciﬁcally, over-clustering leads to the performance
drop for CIFAR-10, but it leads to performance increase for CIFAR-20. However, our NCC can
still produce large improvements over BYOL with the same predeﬁned number of clusters. We note
that the opposite results are due to the signiﬁcant difference between these two datasets. Although
having the same number of samples, CIFAR-10 has 10 distinct classes while CIFAR-20 has, in fact,
100 classes but uses 20 super-classes instead. If the representations are well aligned within the same
semantic clusters, the over-clustering would try to destroy the structures of the clusters and push the
semantically similar examples away, which certainly compromises the clustering performance."
RETURN CENTERS,0.7372549019607844,Under review as a conference paper at ICLR 2022
RETURN CENTERS,0.7392156862745098,"0
200
400
600
800
1000
Epoch 0.2 0.4 0.6 0.8 NMI"
RETURN CENTERS,0.7411764705882353,a) Normalized Mutual Information
RETURN CENTERS,0.7431372549019608,Method
RETURN CENTERS,0.7450980392156863,"BYOL
NCC"
RETURN CENTERS,0.7470588235294118,"0
200
400
600
800
1000
Epoch 0 1 d STD"
RETURN CENTERS,0.7490196078431373,b) Standard Deviation
RETURN CENTERS,0.7509803921568627,Method
RETURN CENTERS,0.7529411764705882,"BYOL
NCC"
RETURN CENTERS,0.7549019607843137,"0
200
400
600
800
1000
Epoch 0.2 0.4 0.6 0.8"
RETURN CENTERS,0.7568627450980392,Imbalance Ratio
RETURN CENTERS,0.7588235294117647,c) Cluster Imbalance Ratio
RETURN CENTERS,0.7607843137254902,Method
RETURN CENTERS,0.7627450980392156,"BYOL
NCC"
RETURN CENTERS,0.7647058823529411,"1
2
3
4
5
6
7
8
9
10
Cluster Index 0 2500 5000 7500 10000 12500"
RETURN CENTERS,0.7666666666666667,Cluster Count
RETURN CENTERS,0.7686274509803922,d) Cluster Statistics
RETURN CENTERS,0.7705882352941177,Method
RETURN CENTERS,0.7725490196078432,"BYOL
NCC"
RETURN CENTERS,0.7745098039215687,"Figure A7: Visualization of training and cluster statistics for BYOL and NCC: a) normalized mutual in-
formation (NMI)1 between the clustering results and ground-truth labels; b) standard deviation (STD)2
of ℓ2-normalized features to evaluate the uniformity, where the higher indicates the more uniform rep-
resentations; c) cluster imbalance ratio3 computed by Nmin/Nmax, where N is the number of samples
in each class; and d) cluster statistics, or the sorted number of samples in each cluster for the model
at 1000-th epoch on CIFAR-10. Taking a look at NMIs and STDs during the training stage in (a) and
(b), NCC produces higher NMIs with stable and higher STDs, while BYOL performs unstable with the
STDs gradually decreased. The results indicate that NCC yields a more uniform representation space
with the samples well-clustered. On the other hand, the uniform representations of NCC can also avoid
the collapse of k-means clustering at the same time. As shown in (c), the k-means clustering process of
NCC produces more balanced clusters with a higher cluster imbalance ratio. On the contrary, the clus-
ters of BYOL are highly imbalanced, which is consistent with the unstable NMIs and decreasing STDs.
Moreover, we visualize the fourth cluster statistic in (d), with the sorted number of samples. Unsur-
prisingly, the samples for NCC are approximately and equally assigned to different clusters, compared
to almost long-tailed assignments of BYOL."
RETURN CENTERS,0.7764705882352941,"1 NMI is a normalized mutual information between the clustering and ground-truth labels.
2 Here we visualize the distribution of representations following (Chen & He, 2021). Given the latent vectors z ∼N (0, I), the standard"
RETURN CENTERS,0.7784313725490196,"deviation for ℓ2-normalized z′ = z/∥z∥2 is std

z′
≈1/d
1
2 , where d is the dimension of z. As we discussed in Appendix B, the
ℓ2-normalized standard Gaussian distribution can be approximately seen as the uniform vMF distribution on the hypersphere. Therefore, a"
RETURN CENTERS,0.7803921568627451,"higher standard deviation close to 1/d
1
2 indicates more uniform representations.
3 Same as the coefﬁcient to control the data imbalance for the long-tailed datasets in Table. A4, a higher value indicates more balanced clusters."
RETURN CENTERS,0.7823529411764706,Under review as a conference paper at ICLR 2022
RETURN CENTERS,0.7843137254901961,"128
256
512
1024
2048
40 50 60 70 80 90 100"
RETURN CENTERS,0.7862745098039216,Performance (%)
RETURN CENTERS,0.788235294117647,a) NMI for CIFAR-10
RETURN CENTERS,0.7901960784313725,"BYOL
NCC"
RETURN CENTERS,0.792156862745098,"128
256
512
1024
2048
10 20 30 40 50 60 70"
RETURN CENTERS,0.7941176470588235,Performance (%)
RETURN CENTERS,0.796078431372549,d) NMI for CIFAR-20
RETURN CENTERS,0.7980392156862746,"BYOL
NCC"
RETURN CENTERS,0.8,"128
256
512
1024
2048
40 50 60 70 80 90 100"
RETURN CENTERS,0.8019607843137255,Performance (%)
RETURN CENTERS,0.803921568627451,b) ACC for CIFAR-10
RETURN CENTERS,0.8058823529411765,"BYOL
NCC"
RETURN CENTERS,0.807843137254902,"128
256
512
1024
2048
10 20 30 40 50 60 70"
RETURN CENTERS,0.8098039215686275,Performance (%)
RETURN CENTERS,0.8117647058823529,e) ACC for CIFAR-20
RETURN CENTERS,0.8137254901960784,"BYOL
NCC"
RETURN CENTERS,0.8156862745098039,"128
256
512
1024
2048
40 50 60 70 80 90 100"
RETURN CENTERS,0.8176470588235294,Performance (%)
RETURN CENTERS,0.8196078431372549,c) ARI for CIFAR-10
RETURN CENTERS,0.8215686274509804,"BYOL
NCC"
RETURN CENTERS,0.8235294117647058,"128
256
512
1024
2048
10 20 30 40 50 60 70"
RETURN CENTERS,0.8254901960784313,Performance (%)
RETURN CENTERS,0.8274509803921568,f) ARI for CIFAR-20
RETURN CENTERS,0.8294117647058824,"BYOL
NCC"
RETURN CENTERS,0.8313725490196079,"Figure A8: The effect of the different projection dimension. NCC achieves consistent and signiﬁcant
performance improvement over BYOL regardless of different projection dimension."
RETURN CENTERS,0.8333333333333334,Baseline
RETURN CENTERS,0.8352941176470589,No Grayscale
RETURN CENTERS,0.8372549019607843,No Color Jitter
RETURN CENTERS,0.8392156862745098,Crop only 40 50 60 70 80 90 100
RETURN CENTERS,0.8411764705882353,Performance (%)
RETURN CENTERS,0.8431372549019608,a) NMI for CIFAR-10
RETURN CENTERS,0.8450980392156863,Method
RETURN CENTERS,0.8470588235294118,"BYOL
NCC"
RETURN CENTERS,0.8490196078431372,Baseline
RETURN CENTERS,0.8509803921568627,No Grayscale
RETURN CENTERS,0.8529411764705882,No Color Jitter
RETURN CENTERS,0.8549019607843137,Crop only 10 20 30 40 50 60 70
RETURN CENTERS,0.8568627450980392,Performance (%)
RETURN CENTERS,0.8588235294117647,d) NMI for CIFAR-20
RETURN CENTERS,0.8607843137254902,Method
RETURN CENTERS,0.8627450980392157,"BYOL
NCC"
RETURN CENTERS,0.8647058823529412,Baseline
RETURN CENTERS,0.8666666666666667,No Grayscale
RETURN CENTERS,0.8686274509803922,No Color Jitter
RETURN CENTERS,0.8705882352941177,Crop only 40 50 60 70 80 90 100
RETURN CENTERS,0.8725490196078431,Performance (%)
RETURN CENTERS,0.8745098039215686,b) ACC for CIFAR-10
RETURN CENTERS,0.8764705882352941,Method
RETURN CENTERS,0.8784313725490196,"BYOL
NCC"
RETURN CENTERS,0.8803921568627451,Baseline
RETURN CENTERS,0.8823529411764706,No Grayscale
RETURN CENTERS,0.884313725490196,No Color Jitter
RETURN CENTERS,0.8862745098039215,Crop only 10 20 30 40 50 60 70
RETURN CENTERS,0.888235294117647,Performance (%)
RETURN CENTERS,0.8901960784313725,e) ACC for CIFAR-20
RETURN CENTERS,0.8921568627450981,Method
RETURN CENTERS,0.8941176470588236,"BYOL
NCC"
RETURN CENTERS,0.8960784313725491,Baseline
RETURN CENTERS,0.8980392156862745,No Grayscale
RETURN CENTERS,0.9,No Color Jitter
RETURN CENTERS,0.9019607843137255,Crop only 40 50 60 70 80 90 100
RETURN CENTERS,0.903921568627451,Performance (%)
RETURN CENTERS,0.9058823529411765,c) ARI for CIFAR-10
RETURN CENTERS,0.907843137254902,Method
RETURN CENTERS,0.9098039215686274,"BYOL
NCC"
RETURN CENTERS,0.9117647058823529,Baseline
RETURN CENTERS,0.9137254901960784,No Grayscale
RETURN CENTERS,0.9156862745098039,No Color Jitter
RETURN CENTERS,0.9176470588235294,Crop only 10 20 30 40 50 60 70
RETURN CENTERS,0.9196078431372549,Performance (%)
RETURN CENTERS,0.9215686274509803,f) ARI for CIFAR-20
RETURN CENTERS,0.9235294117647059,Method
RETURN CENTERS,0.9254901960784314,"BYOL
NCC"
RETURN CENTERS,0.9274509803921569,"Figure A9: The effect of the different data augmentation. It is not surprised to see the performance
drops for both BYOL and NCC when removing some data augmentations. On the contrary, the
clustering results suggest that NCC still performs more stable and is robust to data augmentations."
RETURN CENTERS,0.9294117647058824,Under review as a conference paper at ICLR 2022
RETURN CENTERS,0.9313725490196079,"Table A1: Clustering results (%) for fair comparisons. We train NCC to demonstrate its effective-
ness for fair comparisons with the following settings: 1) we exclude test set from the whole dataset;
and 2) we use an original image size (224) for ImageNet-10 and ImageNet-Dogs. All results were
trained with ResNet-34. There is no clear margin for CIFAR-10 and CIFAR-20 datasets with dif-
ferent splits, while signiﬁcant improvements can be observed for ImageNet-10 and ImageNet-Dogs
datasets. Considering that NCC has already achieved state-of-the-art performance against previous
work in Table 2, these results further demonstrate the superiority of NCC."
RETURN CENTERS,0.9333333333333333,"NMI
ACC
ARI
NMI
ACC
ARI"
RETURN CENTERS,0.9352941176470588,"CIFAR-10
CIFAR-20"
RETURN CENTERS,0.9372549019607843,"Baseline (train+test)
88.6±1.0
94.3±0.6
88.4±1.1
60.6±0.3
61.4±1.1
45.1±0.1
Exclude test set
88.3±0.2
94.2±0.2
88.1±0.3
61.2±0.9
61.5±0.9
45.9±0.5"
RETURN CENTERS,0.9392156862745098,"ImageNet-10
ImageNet-Dogs"
RETURN CENTERS,0.9411764705882353,"Baseline (96)
89.6±0.2
95.6±0.0
90.6±0.1
69.2±0.3
74.5±0.1
62.7±0.1
Large image size (224)
90.8±0.4
96.2±0.1
91.8±0.3
73.7±0.2
77.5±0.1
67.5±0.1"
RETURN CENTERS,0.9431372549019608,"Table A2:
Clustering results (%) on the subsets of ImageNet. We strictly follow the settings
in (Van Gansbeke et al., 2020): we have adopted the same 50, 100, and 200 classes from ImageNet,
clustered on the training set and tested on the validation set. We have used the same experimental
settings as the other benchmarked datasets and trained NCC with ResNet-50 for 300 epochs. We
note that SCAN has used the pre-trained model of MoCo trained on the full ImageNet for 800
epochs. The results are directly referred from their published paper including k-means with pre-
trained MoCo1, SCAN after the clustering step2, and SCAN after the self-labeling step3. With
much fewer training epochs and training data, NCC still produces better performance with a clear
margin, demonstrating the superiority of NCC."
RETURN CENTERS,0.9450980392156862,"ImageNet
50 Classes
100 Classes
200 Classes"
RETURN CENTERS,0.9470588235294117,"Method
NMI
ARI
NMI
ARI
NMI
ARI"
RETURN CENTERS,0.9490196078431372,"k-means1
77.5
57.9
76.1
50.8
75.5
43.2
SCAN2
80.5
63.5
78.7
54.4
75.7
44.1
SCAN3
82.2
66.1
80.8
57.6
77.2
47.0
NCC (Ours)
82.8
69.1
83.5
63.5
80.6
53.8"
RETURN CENTERS,0.9509803921568627,"Table A3:
Clustering results (%) on Tiny-ImageNet (Le & Yang, 2015). Tiny-ImageNet is also
a subset of ImageNet, with a total of 200 classes and 500 images for each class. We trained NCC
with same settings in Table 2 except for ResNet-18 and the image size 96 × 96. Even though with a
smaller backbone and image size (CC used ResNet-34 and the image size 224 × 224), our NCC still
produces promising improvements over previous state-of-the-art methods."
RETURN CENTERS,0.9529411764705882,"Dataset
Tiny-ImageNet"
RETURN CENTERS,0.9549019607843138,"Method
NMI
ACC
ARI"
RETURN CENTERS,0.9568627450980393,"k-means (Lloyd, 1982)
6.5
2.5
0.5
SC (Zelnik-manor & Perona, 2005)
6.3
2.2
0.4
AE (Bengio et al., 2007)
13.1
4.1
0.7
VAE (Kingma & Welling, 2014)
11.3
3.6
0.6
JULE (Yang et al., 2016)
10.2
3.3
0.6
DEC (Xie et al., 2016)
11.5
3.7
0.7
DAC (Chang et al., 2017)
19.0
6.6
1.7
DCCM (Wu et al., 2019)
22.4
10.8
3.8
PICA (Huang et al., 2020)
27.7
9.8
4.0
CC (Li et al., 2021b)
34.0
14.0
7.1
GCC (Zhong et al., 2021)
34.7
13.8
7.5
NCC (Ours)
40.5
25.6
14.3"
RETURN CENTERS,0.9588235294117647,Under review as a conference paper at ICLR 2022
RETURN CENTERS,0.9607843137254902,"Table A4: Clustering results (%) on long-tailed datasets of different self-supervised learning frame-
works and our proposed NCC. We built the long-tailed version of CIFAR-10 and CIFAR-20, termed
CIFAR-10-LT and CIFAR-20-LT using the codes of (Tang et al., 2020), which follows (Zhou et al.,
2020; Cao et al., 2019). Speciﬁcally, they were built upon the training datasets under the control of
data imbalance ratio Nmin/Nmax = 0.1 for the data distribution, where N is the number of samples in
each class. The samples in the long-tailed datasets are almost all in the head of distributions. MoCo
cannot handle this problem well due to the class collision issue, as a results, the samples in the head
will be pushed away and the ones in the tail will be mixed together. The rest three methods do not
need the negative examples so that they outperform MoCo v2 by a large margin. By introducing
the positive sampling and ProtoCL, we can further boost and stabilize the performance of vanilla
BYOL."
RETURN CENTERS,0.9627450980392157,"CIFAR-10-LT
CIFAR-20-LT"
RETURN CENTERS,0.9647058823529412,"Methods
NMI
ACC
ARI
NMI
ACC
ARI"
RETURN CENTERS,0.9666666666666667,"MoCo v2
46.7±0.1
33.4±0.3
27.7±0.0
31.2±0.3
28.2±0.2
16.1±0.3
BYOL
51.6±1.0
41.3±0.4
30.8±0.4
41.9±0.4
34.6±0.5
22.3±1.0
NCC
55.3±0.4
43.9±0.1
36.3±0.3
44.6±0.2
39.0±0.7
27.3±0.2
w/o ProtoCL
53.1±0.7
42.7±0.4
31.6±0.8
43.4±0.8
35.1±0.6
24.0±0.1"
RETURN CENTERS,0.9686274509803922,"Table A5: Clustering results (%) on different ResNet architectures. With the deeper ResNet net-
works, there are little performance gain and even drops for MoCo v2 and BYOL. On the contrary,
NCC achieves signiﬁcant improvement with small standard deviation for clustering, demonstrating
its superior stability and performance against the baseline MoCo v2 and BYOL. The best and second
best results are shown in bold and underline, respectively."
RETURN CENTERS,0.9705882352941176,"Backbone
Method
Dataset
NMI
ACC
ARI
Dataset
NMI
ACC
ARI"
RETURN CENTERS,0.9725490196078431,"ResNet-18
MoCo"
RETURN CENTERS,0.9745098039215686,CIFAR-10
RETURN CENTERS,0.9764705882352941,"76.9±0.2
84.9±0.3
72.4±0.5"
RETURN CENTERS,0.9784313725490196,CIFAR-20
RETURN CENTERS,0.9803921568627451,"49.2±0.1
48.0±0.2
32.1±0.0
BYOL
79.4±1.7
87.8±1.7
76.6±2.8
55.5±0.6
53.9±1.6
37.6±0.9
Ours
85.1±0.5
91.6±0.4
83.5±0.7
58.2±0.3
57.8±0.2
42.3±0.3"
RETURN CENTERS,0.9823529411764705,"ResNet-34
MoCo
79.4±0.0
86.9±0.0
75.6±0.0
51.3±0.0
49.2±0.6
34.3±0.3
BYOL
81.7±1.0
89.4±0.6
79.0±1.0
55.9±0.3
56.9±1.8
39.3±0.2
Ours
88.6±1.0
94.3±0.6
88.4±1.1
60.6±0.3
61.4±1.1
45.1±0.1"
RETURN CENTERS,0.984313725490196,"ResNet-50
MoCo
79.9±0.1
87.1±0.1
76.0±0.1
53.3±0.0
52.9±0.1
36.7±0.1
BYOL
80.0±0.4
88.1±0.9
76.3±1.8
54.1±1.4
53.1±2.4
36.9±2.9
Ours
89.4±0.5
94.7±0.4
89.1±0.7
63.2±1.0
61.3±1.2
47.1±1.4"
RETURN CENTERS,0.9862745098039216,"ResNet-18
MoCo"
RETURN CENTERS,0.9882352941176471,ImageNet-10
RETURN CENTERS,0.9901960784313726,"83.3±0.1
90.4±0.1
81.6±0.1"
RETURN CENTERS,0.9921568627450981,ImageNet-Dogs
RETURN CENTERS,0.9941176470588236,"38.7±0.5
36.7±0.5
23.9±0.6
BYOL
87.2±0.6
94.2±0.5
87.6±1.1
59.6±0.8
66.1±1.1
49.8±1.1
Ours
89.3±0.4
95.4±0.1
89.7±0.8
64.4±1.3
69.6±2.2
56.1±2.3"
RETURN CENTERS,0.996078431372549,"ResNet-34
MoCo
84.2±0.1
90.7±0.1
82.3±0.1
41.6±1.2
39.0±1.5
26.8±1.7
BYOL
86.6±0.2
93.9±0.1
87.2±0.2
63.5±2.2
69.4±3.0
54.8±2.9
Ours
89.6±0.2
95.6±0.0
90.6±0.1
69.2±0.3
74.5±0.1
62.7±0.1"
RETURN CENTERS,0.9980392156862745,"ResNet-50
MoCo
82.6±0.1
90.0±0.0
80.9±0.0
40.8±1.0
38.6±1.1
25.4±0.7
BYOL
88.7±1.8
95.2±0.9
89.7±1.9
66.2±0.2
71.9±1.2
58.1±1.2
Ours
91.5±0.2
96.5±0.1
92.5±0.2
72.0±0.2
76.3±0.1
65.7±0.1"
