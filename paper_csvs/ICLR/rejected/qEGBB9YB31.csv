Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002770083102493075,"Conventional saliency maps highlight input features to which neural network
predictions are highly sensitive. We take a different approach to saliency, in
which we identify and analyze the network parameters, rather than inputs, which
are responsible for erroneous decisions. We ﬁrst verify that identiﬁed salient
parameters are indeed responsible for misclassiﬁcation by showing that turning
these parameters off improves predictions on the associated samples, more than
pruning the same number of random or least salient parameters. We further
validate the link between salient parameters and network misclassiﬁcation errors
by observing that ﬁne-tuning a small number of the most salient parameters on a
single sample results in error correction on other samples which were misclassiﬁed
for similar reasons – nearest neighbors in the saliency space. After validating our
parameter-space saliency maps, we demonstrate that samples which cause similar
parameters to malfunction are semantically similar. Further, we introduce an
input-space saliency counterpart which reveals how image features cause speciﬁc
network components to malfunction."
INTRODUCTION,0.00554016620498615,"1
INTRODUCTION"
INTRODUCTION,0.008310249307479225,"With the widespread deployment of deep neural networks in high-stakes applications such as medical
imaging (Kang et al., 2017), credit score assessment (West, 2000), and facial recognition (Deng
et al., 2019), practitioners need to understand why their models make the decisions they do. In fact,
“right to explanation” legislation in the European Union and the United States dictates that relevant
public and private organizations must be able to justify the decisions their algorithms make (United
States Congress Senate Committee on Banking and Housing and Urban Affairs, 1976; European
Commission, 2018). Diagnosing the causes of system failures is particularly crucial for understanding
the ﬂaws and limitations of models we intend to employ."
INTRODUCTION,0.0110803324099723,"Conventional saliency methods focus on highlighting sensitive pixels (Simonyan et al., 2014) or
image regions that maximize speciﬁc activations (Erhan et al., 2009). However, such maps may not
be useful in diagnosing undesirable model behaviors as they do not necessarily identify areas that
speciﬁcally cause bad performance since the most sensitive pixels may not be the ones responsible
for triggering misclassiﬁcation."
INTRODUCTION,0.013850415512465374,"We develop an alternative approach to saliency which highlights network parameters that inﬂuence
decisions rather than input features. These parameter saliency maps yield a number of useful analyses:"
INTRODUCTION,0.01662049861495845,"• Nearest neighbors in parameter saliency space share common semantic information. That
is, samples which are misclassiﬁed for similar reasons and cause similar parameters to
malfunction are semantically similar.
• By ﬁrst identifying the network parameters responsible for an erroneous classiﬁcation, we
can then visualize the image regions that interact with those parameters and trigger the
identiﬁed misbehavior.
• We verify that identiﬁed salient parameters are indeed responsible for misclassiﬁcation by
showing that turning these parameters off improves predictions on the associated samples,
more than pruning the same number of random or least salient parameters.
• We further validate the link between salient parameters and network misclassiﬁcation errors
by observing that ﬁne-tuning a small number of the most salient parameters on a single"
INTRODUCTION,0.019390581717451522,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0221606648199446,"sample results in error correction on other samples which were misclassiﬁed for similar
reasons."
INTRODUCTION,0.024930747922437674,"After carefully delineating our methodology and experimentally validating the meaningfulness of
our parameter saliency maps, we showcase the practical utility of this paradigm as an explainability
tool with a case study in which we are able to uncover a neural network’s reliance on a spurious
correlation which causes interpretable failures."
RELATED WORK,0.027700831024930747,"1.1
RELATED WORK"
RELATED WORK,0.030470914127423823,"Neural network interpretability and parameter importance. A major line of work in neural
network interpretability focuses on convolutional neural networks. Works visualizing, interpreting,
and analysing feature maps (Zeiler & Fergus, 2014; Yosinski et al., 2015; Olah et al., 2017; Mahendran
& Vedaldi, 2015) provide insight into the role of individual convolutional ﬁlters. These methods,
together with other approaches for ﬁlter explainability (Bau et al., 2017; Zhou et al., 2018; 2019) ﬁnd
that individual convolutional ﬁlters often are responsible for speciﬁc tasks such as edge, shape, and
texture detection."
RELATED WORK,0.0332409972299169,"The idea of measuring neural network parameter importance has been studied in multiple contexts.
Notions of neuron and parameter importance have been used for AI explainability (Srinivas &
Fleuret, 2019; Selvaraju et al., 2017; Morcos et al., 2018; Shrikumar et al., 2017; Shrikumar et al.),
manipulating model behavior (Bau et al., 2018), and parameter pruning (Abbasi-Asl & Yu, 2017; Liu
& Wu, 2019)."
RELATED WORK,0.036011080332409975,"Input space saliency maps. A considerable amount of literature focuses on identifying input
features that are important for neural network decisions. These methods include using deconvolution
approaches (Zeiler & Fergus, 2014) and data gradient information (Simonyan et al., 2014). Several
works build on these ideas and propose improvements such as Integrated Gradients (Sundararajan
et al., 2017), SmoothGrad (Smilkov et al., 2017), and Guided Backpropagation (Springenberg et al.,
2015) which result in sharper and more localized saliency maps. Other approaches focus on the use
of class activation maps (Zhou et al., 2016) with improvements incorporating gradient information
(Selvaraju et al., 2017) and more novel approaches to weighting the activation maps (Wang et al.,
2020). In addition, various saliency methods are based on manipulating the input image (Fong &
Vedaldi, 2017; Zeiler & Fergus, 2014). Another line of work is aimed at evaluating the effectiveness
of saliency maps (Adebayo et al., 2018; Alqaraawi et al., 2020)."
RELATED WORK,0.038781163434903045,"Although extensive work studies how different regions of images affect a network’s predictions,
limited work (Srinivas & Fleuret, 2019) aims to distinguish important network parameters. Our work
combines the ideas of saliency maps and parameter importance and evaluates saliency directly on
model parameters by aggregating their absolute gradients on a ﬁlter level. We leverage the resulting
parameter saliency proﬁles as an explainability tool and develop an input-space saliency counterpart
which highlights image features that cause speciﬁc ﬁlters to malfunction to study the interaction
between the image features and the erroneous ﬁlters."
METHOD,0.04155124653739612,"2
METHOD"
METHOD,0.0443213296398892,"It is known that different network ﬁlters are responsible for identifying different image properties
and objects (Zeiler & Fergus, 2014; Yosinski et al., 2015; Olah et al., 2017; Mahendran & Vedaldi,
2015). This motivates the idea that mistakes made on wrongly classiﬁed images can be understood
by investigating the network parameters, rather than only the pixels, that played a role in making a
decision. We develop parameter-space saliency methods geared towards identifying and analyzing
neural network parameters that are responsible for making erroneous decisions. Central to our method
is the use of gradient information of the loss function as a measure of parameter sensitivity and
optimality of the network at a given point in image space."
PARAMETER SALIENCY PROFILE,0.04709141274238227,"2.1
PARAMETER SALIENCY PROFILE"
PARAMETER SALIENCY PROFILE,0.04986149584487535,"Let x be a sample in the validation set D with label y, and suppose a trained classiﬁer has parameters
θ that minimize a loss function L. We deﬁne the parameter-wise saliency proﬁle of x as a vector"
PARAMETER SALIENCY PROFILE,0.05263157894736842,Under review as a conference paper at ICLR 2022
PARAMETER SALIENCY PROFILE,0.055401662049861494,"0
10
20
30
40
50
Layer ID 0.0 0.1 0.2 0.3 0.4"
PARAMETER SALIENCY PROFILE,0.05817174515235457,Saliency
PARAMETER SALIENCY PROFILE,0.060941828254847646,Average gradient magnitudes
PARAMETER SALIENCY PROFILE,0.06371191135734072,"Figure 1: Filter-wise parameter saliency proﬁle. ResNet-50 ﬁlter-wise saliency proﬁle (without
standardization) averaged over samples in ImageNet validation set. The ﬁlter saliency values in each
layer are sorted in descending order, and each layer’s saliency values are concatenated. The layers
are displayed left-to-right from shallow to deep and have equal width on x-axis."
PARAMETER SALIENCY PROFILE,0.0664819944598338,"s(x, y) with entries s(x, y)i := |∇θiLθ(x, y)|, the magnitudes of the gradient of the loss with respect
to each model parameter. Because the gradients on training data for a model trained to convergence
are near zero, it is important to specify that D be a validation, or holdout, set. Intuitively, a larger
gradient norm at the point (x, y) indicates a greater inefﬁciency in the network’s classiﬁcation of
sample x, and thus each entry of s(x, y) measures the suboptimality of individual parameters."
PARAMETER SALIENCY PROFILE,0.06925207756232687,"Aggregation of parameter saliency. Convolutional ﬁlters are known to specialize in tasks such as
edge, shape, and texture detection (Yosinski et al., 2015; Bau et al., 2017; Olah et al., 2017). We
therefore choose to aggregate saliency on the ﬁlter-wise basis by averaging the gradient magnitudes
of parameters corresponding to each convolutional ﬁlter. This allows us to isolate ﬁlters to which the
loss is most sensitive (i.e. those which, when corrected, lead to the greatest reduction in loss)."
PARAMETER SALIENCY PROFILE,0.07202216066481995,"Formally, for each convolutional ﬁlter Fk in the network, consider its respective index set αk, which
gives the indices of parameters corresponding to the ﬁlter Fk. The ﬁlter-wise saliency proﬁle of x is
deﬁned to be a vector s(x, y) with entries"
PARAMETER SALIENCY PROFILE,0.07479224376731301,"s(x, y)k :=
1
|αk| X"
PARAMETER SALIENCY PROFILE,0.07756232686980609,"i∈αk
s(x, y)i,
(1)"
PARAMETER SALIENCY PROFILE,0.08033240997229917,the parameter-wise saliency proﬁle aggregated by averaging on the ﬁlter level.
PARAMETER SALIENCY PROFILE,0.08310249307479224,"Standardizing parameter saliency. Figure 1 exhibits the ResNet-50 (He et al., 2016) ﬁlter-wise
saliency proﬁle averaged over the ImageNet (Deng et al., 2009) validation set, where ﬁlters within
each layer are sorted from highest to lowest saliency. One clear observation is the difference in the
scale of gradient magnitudes – shallower ﬁlters are more salient than deeper ﬁlters. This phenomenon
might occur for a number of reasons. First, early ﬁlters encode low-level features, such as edges and
textures, which are active across a wide spectrum of images. Second, typical networks have fewer
ﬁlters in shallow layers than in deep layers, making each individual ﬁlter more inﬂuential at shallower
layers. Third, the effects of early ﬁlters cascade and accumulate as they pass through a network."
PARAMETER SALIENCY PROFILE,0.08587257617728532,"To isolate ﬁlters that uniquely cause erroneous behavior on particular samples, we ﬁnd ﬁlters that are
abnormally salient for a sample, x, but not for others. That is, we further standardize the saliency
proﬁle of x with respect to all ﬁlter-wise saliency proﬁles of D."
PARAMETER SALIENCY PROFILE,0.0886426592797784,"Formally, let µ be the average ﬁlter-wise saliency proﬁle across all x ∈D, and let σ be an equal-
length vector with the corresponding standard deviation for each entry. We use these statistics to
produce the standardized ﬁlter-wise saliency proﬁle as follows:"
PARAMETER SALIENCY PROFILE,0.09141274238227147,"ˆs(x, y) := |s(x, y) −µ|"
PARAMETER SALIENCY PROFILE,0.09418282548476455,"σ
.
(2)"
PARAMETER SALIENCY PROFILE,0.09695290858725762,"The resulting tensor ˆs(x, y) is of length equal to the number of convolutional ﬁlters in the network,
and we henceforth call it the saliency proﬁle for sample x. By standardizing saliency proﬁles, we
create a saliency map that activates when the importance of a ﬁlter is unusually strong relative to other
samples in the dataset. This prevents the saliency map from highlighting ﬁlters that are uniformly
important for all images, and instead focuses saliency on ﬁlters that are uniquely important and serve"
PARAMETER SALIENCY PROFILE,0.0997229916897507,Under review as a conference paper at ICLR 2022
PARAMETER SALIENCY PROFILE,0.10249307479224377,"0
10
20
30
40
50
Layer ID 0.4 0.2 0.0"
PARAMETER SALIENCY PROFILE,0.10526315789473684,Saliency
PARAMETER SALIENCY PROFILE,0.10803324099722991,Average saliency profile of correctly classified samples
PARAMETER SALIENCY PROFILE,0.11080332409972299,"0
10
20
30
40
50
Layer ID 0 2 4 6"
PARAMETER SALIENCY PROFILE,0.11357340720221606,Saliency
PARAMETER SALIENCY PROFILE,0.11634349030470914,Average saliency profile of incorrectly classified samples
PARAMETER SALIENCY PROFILE,0.11911357340720222,"Figure 2: Standardized ﬁlter-wise saliency proﬁles, correctly vs incorrectly classiﬁed samples.
Top: Standardized saliency proﬁles averaged over correctly classiﬁed samples in the ImageNet
validation set. Bottom: Standardized saliency proﬁles averaged over incorrectly classiﬁed samples
in the ImageNet validation set. On both panels, the ﬁlter saliency values in each layer are sorted
in descending order, and each layer’s saliency values are concatenated. The layers are displayed
left-to-right from shallow to deep and have equal width on x-axis. Both proﬁles are generated on
ResNet-50."
PARAMETER SALIENCY PROFILE,0.12188365650969529,"an image-dependent role. In the rest of the paper, unless explicitly noted otherwise, we use ˆs(x, y)
and refer to it as parameter saliency."
PARAMETER SALIENCY PROFILE,0.12465373961218837,"Incorrectly classiﬁed samples are more salient. Empirically, we observe the saliency proﬁles of
incorrectly classiﬁed samples exhibit, on average, greater values than those of correctly classiﬁed
examples. This bolsters the intuition that salient ﬁlters are precisely those malfunctioning — if
the classiﬁcation is correct, there should be few malfunctioning ﬁlters or none at all. Moreover,
we see deeper parts of the network appear to be most salient for the incorrectly classiﬁed samples
while earlier layers are often the most salient for correctly classiﬁed samples. An example of
these behaviors for ResNet-50 is shown in Figure 2 which presents standardized ﬁlter-wise saliency
proﬁles averaged over the correctly and incorrectly classiﬁed examples from the ImageNet validation
set. Additionally, we note the improved relative scale of the standardized saliency proﬁle across
different layers compared to the absolute gradient magnitudes in Figure 1. Saliency proﬁles for
other architectures could be found in Appendix A. Henceforth, we will focus speciﬁcally on saliency
proﬁles of misclassiﬁed samples in order to explore how neural networks make mistakes."
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.12742382271468145,"2.2
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION"
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.13019390581717452,"The parameter saliency proﬁle allows us to identify ﬁlters that are most responsible for mistakes and
erroneous network behavior. In this section, we develop an input-space counterpart to our parameter
saliency method to understand which features of the image affect the saliency of particular ﬁlters.
Geiping et al. (2020) show that the gradient information of a network is invertible, providing a
link between input space and parameter saliency space. This work, along with existing input-space
saliency map tools (Simonyan et al., 2014; Springenberg et al., 2015; Smilkov et al., 2017; Zhou
et al., 2016; Selvaraju et al., 2017), inspires our method."
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.1329639889196676,"Given a parameter saliency proﬁle ˆs = ˆs(x, y) for an image x with label y, our goal is to highlight
the input features that drive large ﬁlter saliency values. That is, we would like to identify image pixels
altering which can make ﬁlters more salient. To this end, we ﬁrst select some set F of the most salient
ﬁlters that we would like to explore. Then, we create a boosted saliency proﬁle s′
F by increasing the
entries of ˆs corresponding to the chosen ﬁlters F (e.g., multiplying by a large constant). Now, we can
ﬁnd pixels that are important for making the chosen ﬁlters F more salient and, equivalently, making
the ﬁlter saliency proﬁle ˆs(x, y) close to the boosted saliency proﬁle s′
F by taking the following
gradients:"
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.13573407202216067,"MF = |∇xDC(ˆs(x, y), s′
F )|,
(3)"
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.13850415512465375,"where DC(·, ·) is cosine distance."
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.14127423822714683,Under review as a conference paper at ICLR 2022
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.1440443213296399,"0
20
40
60
80
100
Number of pruned filters 35 30 25 20 15 10 5 0"
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.14681440443213298,Incorrect class confidence
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.14958448753462603,change (%)
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.1523545706371191,"Most salient
Random
Least salient (a)"
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.15512465373961218,"0
20
40
60
80
100
Number of pruned filters 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5"
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.15789473684210525,True class confidence
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.16066481994459833,change (%)
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.1634349030470914,"Most salient
Random
Least salient (b)"
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.16620498614958448,"0
20
40
60
80
100
Number of pruned filters 0 2 4 6 8 10 12"
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.16897506925207756,Percentage of
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.17174515235457063,corrected samples (%)
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.1745152354570637,"Most salient
Random
Least salient (c)"
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.1772853185595568,"Figure 3: Effect of turning salient ﬁlters off. (a) Change in incorrect class conﬁdence score. (b)
Change in true class conﬁdence score. (c) Percentage of samples that were corrected as the result
of pruning ﬁlters. These trends are averaged across all images misclassiﬁed by ResNet-50 in the
ImageNet validation set. The error bars represent 95% bootstrap conﬁdence intervals."
INPUT-SPACE SALIENCY FOR VISUALIZING HOW FILTERS MALFUNCTION,0.18005540166204986,"The resulting input saliency map MF contains input features (pixels) that affect the saliency of the
chosen ﬁlters F the most."
EXPERIMENTS,0.18282548476454294,"3
EXPERIMENTS"
EXPERIMENTS,0.18559556786703602,"In this section, we aim to validate the meaningfulness of our parameter saliency method. First, we
show on the dataset level that turning salient parameters off improves predictions on the associated
samples thus verifying that the salient parameters are indeed responsible for misclassiﬁcation. We
then ﬁnd that samples which cause similar ﬁlters to malfunction are semantically similar. We also
show on the dataset level that ﬁne-tuning a small number of the most salient parameters on a single
sample results in error correction on other samples which were misclassiﬁed for similar reasons. We
then use our input-space saliency technique in conjunction with its parameter-space counterpart as an
explainability tool to explore how neural networks make mistakes and how salient ﬁlters interact with
visual input features."
EXPERIMENTS,0.1883656509695291,"We evaluate our saliency method in the context of image classiﬁcation on CIFAR-10 (Krizhevsky,
2009) and ImageNet (Deng et al., 2009). Images we use for visualization, unless otherwise speciﬁed,
are sampled from ImageNet validation set. Throughout the experiments, we use a pre-trained ResNet-
18 classiﬁer (He et al., 2016) on CIFAR-10 and a pre-trained ResNet-50 on ImageNet. Both models
are trained in a standard fashion on the corresponding dataset12."
PRUNING SALIENT FILTERS,0.19113573407202217,"3.1
PRUNING SALIENT FILTERS"
PRUNING SALIENT FILTERS,0.19390581717451524,"We begin validating the meaningfulness of our parameter-space saliency maps by turning off the
malfunctioning ﬁlters which cause misclassiﬁcation of the associated image. We prune away the
most salient convolutional ﬁlters (i.e., ﬁlters identiﬁed by our method as malfunctioning). In order to
remove the inﬂuence of a particular salient ﬁlter, we zero out the ﬁlter weights and also the biases of
the associated batch normalization layers. This procedure guarantees that the corresponding input
feature map to the next convolutional layer is always zero."
PRUNING SALIENT FILTERS,0.19667590027700832,"Remarkably, we ﬁnd that this simple procedure improves the network’s behavior on the associated
samples. In particular, we gradually increase the number of pruned most salient ﬁlters and track three
metrics: the change in the incorrect class conﬁdence, the change in the true class conﬁdence, and the
percentage of the samples that ﬂip their label to the correct class. In every case, we compare pruning
the most salient ﬁlters against pruning the same number of random ﬁlters and the least salient ﬁlters.
These experiments are performed on the dataset level: we average the trends across all misclassiﬁed
images in the ImageNet validation set."
PRUNING SALIENT FILTERS,0.1994459833795014,"As shown in Figure 3, pruning the most salient ﬁlters is signiﬁcantly more effective for decreasing
the incorrect class conﬁdence than random or least salient ﬁlters. Speciﬁcally, gradually pruning the"
PRUNING SALIENT FILTERS,0.20221606648199447,"1https://github.com/kuangliu/pytorch-cifar (under MIT license)
2https://github.com/pytorch/vision (under BSD 3-Clause License)"
PRUNING SALIENT FILTERS,0.20498614958448755,Under review as a conference paper at ICLR 2022
PRUNING SALIENT FILTERS,0.2077562326869806,"top 100 salient ﬁlters achieves up to 30% drop in the incorrect class conﬁdence score while pruning
random ﬁlters yields only about 7% decrease. We also note that pruning the least salient ﬁlters does
not produce any effect on the incorrect class conﬁdence."
PRUNING SALIENT FILTERS,0.21052631578947367,"We repeat the same experiment with the true class conﬁdence and observe that the highest true
conﬁdence gain occurs when we prune around 20 most salient ﬁlters. Pruning enough salient ﬁlters
eventually leads to a gradual decrease in the true class conﬁdence. We note that this behavior is
expected since we are destroying, not correcting, the inference power of all of the most sensitive
ﬁlters for an image, some of which may be essential for inference. Finally, pruning random ﬁlters
provides a much slower increase in the true conﬁdence class while the least salient ﬁlters again do
not produce a signiﬁcant effect."
PRUNING SALIENT FILTERS,0.21329639889196675,"In addition, we count the number of images that were corrected as a result of pruning and ﬁnd that
pruning around 30 most salient ﬁlters results in the best correct classiﬁcation rate of 12%. Similar to
the true class conﬁdence, the trend decreases beyond this point. Pruning the random ﬁlters increases
the percentage of corrected samples at a much slower rate and does not perform better than the most
salient ﬁlters when pruning up to 100 ﬁlters. Notably, pruning the least salient ﬁlters manages to
correct a nontrivial number of samples but still much smaller than pruning random ﬁlters."
PRUNING SALIENT FILTERS,0.21606648199445982,"Given the trends in panels (b) and (c) of Figure 3, and given that pruning is a coarse tool for ﬁxing
misbehavior, we explore the natural idea of correcting the most salient ﬁlters instead of removing
them altogether in our ﬁne-tuning experiments in Section 3.3."
NEAREST NEIGHBORS IN PARAMETER SALIENCY SPACE,0.2188365650969529,"3.2
NEAREST NEIGHBORS IN PARAMETER SALIENCY SPACE"
NEAREST NEIGHBORS IN PARAMETER SALIENCY SPACE,0.22160664819944598,"We validate the semantic meaning of our saliency proﬁles by clustering images based on the cosine
similarity of their proﬁles. In this section, we present visual depictions of a nearest neighbor search
among all images in the ImageNet validation set. We also conduct this analysis on CIFAR-10 images,
and this can be found in Appendix A."
NEAREST NEIGHBORS IN PARAMETER SALIENCY SPACE,0.22437673130193905,"(a) Great pyrenees ↔Kuvasz
(b) Basset hound ↔Beagle"
NEAREST NEIGHBORS IN PARAMETER SALIENCY SPACE,0.22714681440443213,Figure 4: Examples of nearest neighbors in parameter saliency space (from ImageNet).
NEAREST NEIGHBORS IN PARAMETER SALIENCY SPACE,0.2299168975069252,"We ﬁnd that the nearest neighbors of misclassiﬁed images in saliency space are mostly other mis-
classiﬁed images from the same pair of predicted and true classes but possibly in reverse order. For
example, in Figure 4, the reference image in (a) is a great Pyrenees misclassiﬁed as kuvasz, and the
4 images with the most similar proﬁles exhibit either the same misclassiﬁcation or the reverse (i.e.,
kuvasz misclassiﬁed as great Pyrenees). Intuitively, the common salient parameters across these
neighbors are those which are important for discriminating between the two classes in question but
are not well-tuned for this purpose."
NEAREST NEIGHBORS IN PARAMETER SALIENCY SPACE,0.23268698060941828,"Note that we ﬁnd the concept of “being similar” in parameter saliency space to be different from the
one in image space. The nearest neighbors we ﬁnd are often not similar in a pixel-wise sense, but
rather they are similar in their reason for causing misclassiﬁcation. For example, images in Figure 4
(b) are beagles mistaken by a network for basset hounds and vice versa. We ﬁnd that these pictures
are either taken from a high angle or do not include the dog’s legs, making the leg length, a major
distinction between the two breeds, indistinguishable from the picture. We include more example
images along with their nearest neighbors in Appendix A."
NEAREST NEIGHBORS IN PARAMETER SALIENCY SPACE,0.23545706371191136,"In addition, we compute nearest neighbors when only considering ﬁlters in a speciﬁc range of layers
in order to visualize the types of misbehavior triggered by network components (ﬁlters) at various
network depths. We search for similar images using parameter saliency in the shallow and deep"
NEAREST NEIGHBORS IN PARAMETER SALIENCY SPACE,0.23822714681440443,Under review as a conference paper at ICLR 2022
NEAREST NEIGHBORS IN PARAMETER SALIENCY SPACE,0.2409972299168975,"Figure 5: Neighbors in parameter saliency space found using only early or only deep layers.
The reference image is in the ﬁrst column. Images in the top row resemble the reference image in the
saliency on early layers of VGG-19, and images in the bottom row are found using deeper layers."
NEAREST NEIGHBORS IN PARAMETER SALIENCY SPACE,0.24376731301939059,"layers of a VGG-19 network (Simonyan & Zisserman, 2015), which we divide into the shallow and
deep parts that respectively occur up to and after layer relu4 1. The top row of ﬁgure Figure 5
shows neighbors found using shallow parameters, which share basic image attributes such as color
histogram, while images in the bottom row share more abstract similarities."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.24653739612188366,"3.3
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.24930747922437674,"To validate whether salient ﬁlters are more responsible for the erroneous behavior of neural networks,
we show that updating salient ﬁlters alone is sufﬁcient for correcting the mistakes made by a neural
network. In this experiment, for a pre-trained image classiﬁcation network, we ﬁne-tune it for one
step on a single image for which the network makes the wrong prediction. We restrict the number
of tunable ﬁlters to be no more than 1.0% of the total number of ﬁlters in a network, and we update
the chosen ﬁlters by taking one step of gradient descent with a ﬁxed step size. To validate the
effectiveness of optimizing salient ﬁlters, we compare it with two other choices of tunable ﬁlters:
the least salient ﬁlters and random ﬁlters. For a more general evaluation, we use images from the
ImageNet validation set that are misclassiﬁed by a ResNet-50, making up to over 10,000 samples.
We evaluate the effect of ﬁne-tuning on each of these images independently."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.2520775623268698,"1
5
25
75
150
300
number of tunable filters (a) 0 10 20 30 40 50 60 70 80 90 100"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.2548476454293629,Percentage of corrected samples (%)
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.25761772853185594,"All filters
Most Salient"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.26038781163434904,"Random
Least Salient"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.2631578947368421,"1
5
25
75
150
300
number of tunable filters (b) 0 10 20 30 40 50 60"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.2659279778393352,Average percentage of
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.26869806094182824,corrected neighbors (%)
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.27146814404432135,"All filters
Most Salient"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.2742382271468144,"Random
Least Salient"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.2770083102493075,"1
5
25
75
150
300
number of tunable filters (c) -1 9 19 29 39"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.27977839335180055,True class confidence change
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.28254847645429365,in neighbors (%)
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.2853185595567867,"All filters
Most Salient"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.2880886426592798,"Random
Least Salient"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.29085872576177285,"Figure 6: Effect of updating a small number of ﬁlters. (a) Percentage of samples that are corrected
after ﬁne-tuning. (b) Average percentage of nearest neighbors that are also corrected after ﬁne-tuning.
(c) Average change in the conﬁdence score of the true class among nearest neighbors. The horizontal
line in each plot is the effect of updating the entire network."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.29362880886426596,"In Figure 6, we compare the average performance of our three choices of tunable ﬁlters under three
evaluation metrics. For a given sample image and a set of tunable ﬁlters, an update step is considered
to be effective if the updated network corrects its mistake on the sample image. In addition, it is
more useful if the updated network also corrects its mistake on other images that resemble the sample
image but are not seen during ﬁne-tuning."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.296398891966759,"First, by inspecting the percentage of samples that are corrected after ﬁne-tuning (Figure 6 (a)), we
ﬁnd that updating 150 salient ﬁlters (∼0.6% of total ﬁlters) can achieve the same result as updating
the entire network. The second and third metrics evaluate the effect on the nearest neighbors of the"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.29916897506925205,Under review as a conference paper at ICLR 2022
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.30193905817174516,"(a) Reference image
4.07%
great white shark
78.36 %
killer whale"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3047091412742382,(b) Input-space saliency
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3074792243767313,"0
1
2
3
Masking step 80 60 40 20 0"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.31024930747922436,Filter saliency change (%)
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.31301939058171746,Most salient filters
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3157894736842105,"(c) Saliency values change
of the most erroneous ﬁlters
across masking experiments"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3185595567867036,"(d) Mask seal
11.83%
great white shark
53.47%
killer whale"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.32132963988919666,"(e) Mask boat and seal
43.93%
great white shark
24.24%
killer whale"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.32409972299168976,"(f) Extended mask
58.73%
great white shark
12.88%
killer whale"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3268698060941828,"Figure 7: Interaction between input features and salient ﬁlters. (a) Reference image of “great
white shark” misclassiﬁed by ResNet-50 as “killer whale” with conﬁdence scores. (b) Input-space
saliency visualization. Pixels that cause the top 10 salient ﬁlters to have high saliency. (c) Change in
saliency values of the erroneous ﬁlters across masking experiments. The vertical bars represent the
standard deviation of the change across 10 most salient ﬁlters. (d)-(f) Masking experiments."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3296398891966759,"training sample (Figure 6 (b),(c)). We ﬁnd nearest neighbors for each training sample through the
process introduced in Section 3.2, where we limit the search scope exclusively to other misclassiﬁed
images. Note that for a given training sample, its nearest neighbors are not involved in our one-step
single sample ﬁne-tuning process. By tracking model predictions and true class conﬁdence scores
among the 10 nearest neighbors of each sample, we ﬁnd that ﬁne-tuning salient ﬁlters is signiﬁcantly
more effective than other choices. Results in Figure 6, (b) and (c), also imply that the nearest
neighbors found using our method are the images that are wrong for similar reasons and that they can
be corrected altogether by only updating the salient ﬁlters on a single image. We note that we do not
propose a new pruning or ﬁne-tuning method. Rather, we use these experiments to verify that the
salient ﬁlters are indeed responsible for misclassiﬁcation."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.33240997229916897,"3.4
INPUT FEATURES THAT CAUSE FILTERS TO MALFUNCTION: A CASE STUDY"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.33518005540166207,"We consider a case study of an image misclassiﬁed by ResNet-50 as “killer whale” (Figure 7(a)). The
correct label of the image is “great white shark”. Our goal is to study the interaction between the
most salient ﬁlters and input features. We ﬁrst identify ﬁlters most responsible for misclassiﬁcation
by computing the ﬁlter saliency proﬁle and visualize parts of the image that drive the high saliency
values for those ﬁlters using the input-space saliency counterpart (Section 2.2)."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3379501385041551,"Panel (b) of Figure 7 presents our image-space visualization, which depicts the causes of misbehavior
for the ten most salient ﬁlters – the pixels that trigger misbehavior in these ﬁlters are highlighted. For
example, we see that the seal and boat are both triggers. One natural hypothesis is that the seal looks
like a killer whale to the network and is the source of the classiﬁcation error. We test this hypothesis
by masking out the seal (Figure 7 (d)) . However, although the probability of “killer whale” goes
down and the probability of the correct class increases, the network still misclassiﬁes the image as
“killer whale”."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3407202216066482,"Now, if we mask out exactly the most salient areas of the image according to our visualization
(see Figure 7 (b), (e)), the network manages to ﬂip the label of the image and classify it correctly.
If we extend our mask to the less pronounced, but still salient, areas of the image as in Figure 7"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.34349030470914127,Under review as a conference paper at ICLR 2022
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3462603878116344,"(a)
dalmation
soccer ball
(b)
pizza
bell pepper
(c)
leopard
jaguar
(d)
junco
house ﬁnch
(e)
passenger car
locomotive"
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3490304709141274,"Figure 8: Different types of network mistakes. All of the presented images are misclassiﬁed by
ResNet-50. The correct class label is speciﬁed in the top row and the incorrect class label – in the
bottom row of the subcaption on each panel. (a)-(b) The target object is confused with another object
in the image. (c) A regular mistake. The salient pixels are focused on the target object features which
confuse the network. (d) Background features confuse the network. (e) An example of a noisy label
where the network is “more correct” than the target label. These are examples where masking top 5%
of the salient pixels corrects the misclassiﬁcation."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3518005540166205,"(f), we observe that the correct class conﬁdence increases even more while the probability of the
incorrect “killer whale” label further decreases. Additionally, we ﬁnd that masking out the non-salient
parts of the image results in even worse misclassiﬁcation conﬁdence than that of the original image
(see Appendix A). In order to further investigate the effect of the salient region, we pasted it from
this image onto other great white shark images (see Appendix A) and observed that this drives the
probability of “killer whale” up for 39 out of 40 examples of great white sharks from the ImageNet
validation set with an average increase of 3.75%."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3545706371191136,"Our experiments suggest that secondary objects in the image are associated with the misclassiﬁcation.
However, we see that the erroneous behavior of the model does not just stem from classifying a
non-target object in the image. It is possible that the model correlates the combination of sea creatures
(e.g. a seal) and man-made structures (e.g. a boat) with the “killer whale” label. We note that images
of killer whales in ImageNet often have man-made structures which look similar to the boat (see
Appendix A for examples of other “killer whale” images)."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3573407202216066,"Finally, at each step of our masking experiments, we recompute the saliency values of the originally
chosen 10 ﬁlters (i.e. the ﬁlters that caused erroneous behavior on the reference image). From Figure
7 (c), we observe that as we mask out the input features according to our input-saliency, the saliency
values of those ﬁlters decrease gradually and reach an 80% drop, conﬁrming that highlighted regions
indeed drive the high saliency of the chosen ﬁlters."
CORRECTING MISTAKES BY FINE-TUNING SALIENT FILTERS,0.3601108033240997,"More visualizations of input space saliency showcasing different illustrative examples of neural
network mistakes can be found in Figure 8. For a thorough discussion of mistake categories we
identify using our saliency method, we refer to Appendix A."
DISCUSSION,0.3628808864265928,"4
DISCUSSION"
DISCUSSION,0.3656509695290859,"Numerous applications demand that practitioners be able to understand the decisions their models
make, especially when those decisions are incorrect. Existing methods for explainability focus
on locating the input regions to which the network’s output is sensitive or on associating network
components with speciﬁc roles. In contrast, we develop a framework for ﬁnding the exact ﬁlters
which are responsible for faulty predictions and studying the interactions between these ﬁlters and
images. This direction yields both an interpretable and intuitive understanding of model behaviors."
DISCUSSION,0.3684210526315789,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.37119113573407203,"5
ETHICS STATEMENT"
ETHICS STATEMENT,0.3739612188365651,"Although our formulation of parameter saliency is not restricted to image datasets and CNNs, we only
conduct experiments in these settings. In contrast, real-world data and models come in many forms.
Explainability methods which shed light in some settings may fail to do so in others. Moreover, we
emphasize that some erroneous model behaviors are simply difﬁcult to understand through existing
methods, and the capabilities of parameter saliency are limited. In many applications, it is imperative
that practitioners understand why their models behave as they do and that they are able to diagnose
problems when they arise. We hope that our work helps to enable solutions to real-world problems.
However, we caution against a false sense of security. Our visual interpretations of model behavior
should be viewed as approximations since neural networks are incredibly complex."
REPRODUCIBILITY STATEMENT,0.3767313019390582,"6
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.37950138504155123,"We include our implementation of our parameter saliency method as well as the input-space saliency
counterpart as a supplementary material. All the datasets used in the experiments are publicly
available. The implementation details are available in Appendix B."
REFERENCES,0.38227146814404434,REFERENCES
REFERENCES,0.3850415512465374,"Reza Abbasi-Asl and Bin Yu. Interpreting convolutional neural networks through compression. arXiv
preprint arXiv:1711.02329, 2017."
REFERENCES,0.3878116343490305,"Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity
checks for saliency maps. arXiv preprint arXiv:1810.03292, 2018."
REFERENCES,0.39058171745152354,"Ahmed Alqaraawi, Martin Schuessler, Philipp Weiß, Enrico Costanza, and Nadia Berthouze. Evaluat-
ing saliency map explanations for convolutional neural networks: a user study. In Proceedings of
the 25th International Conference on Intelligent User Interfaces, pp. 275–285, 2020."
REFERENCES,0.39335180055401664,"Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass.
Identifying and controlling important neurons in neural machine translation. arXiv preprint
arXiv:1811.01157, 2018."
REFERENCES,0.3961218836565097,"David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection:
Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 6541–6549, 2017."
REFERENCES,0.3988919667590028,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009."
REFERENCES,0.40166204986149584,"Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin
loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 4690–4699, 2019."
REFERENCES,0.40443213296398894,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.407202216066482,"Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and
Vincent YF Tan. Efﬁcient sharpness-aware minimization for improved training of neural networks.
arXiv preprint arXiv:2110.03141, 2021."
REFERENCES,0.4099722991689751,"Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer
features of a deep network. University of Montreal, 1341(3):1, 2009."
REFERENCES,0.41274238227146814,"European Commission. Recital 71 EU General Data Protection Regulation. 2018. URL https:
//www.privacy-regulation.eu/en/r71.htm."
REFERENCES,0.4155124653739612,"A. H. Miller P. Lewis A. Bakhtin Y. Wu F. Petroni, T. Rockt¨aschel and S. Riedel. Language models
as knowledge bases? In In: Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2019, 2019."
REFERENCES,0.4182825484764543,Under review as a conference paper at ICLR 2022
REFERENCES,0.42105263157894735,"Ruth C. Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturba-
tion. In ICCV, 2017."
REFERENCES,0.42382271468144045,"Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efﬁciently improving generalization. arXiv preprint arXiv:2010.01412, 2020."
REFERENCES,0.4265927977839335,"Jonas Geiping, Hartmut Bauermeister, Hannah Dr¨oge, and Michael Moeller. Inverting gradients–how
easy is it to break privacy in federated learning? arXiv preprint arXiv:2003.14053, 2020."
REFERENCES,0.4293628808864266,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016."
REFERENCES,0.43213296398891965,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700–4708, 2017."
REFERENCES,0.43490304709141275,"Eunhee Kang, Junhong Min, and Jong Chul Ye. A deep convolutional neural network using directional
wavelets for low-dose x-ray ct reconstruction. Medical physics, 44(10):e360–e375, 2017."
REFERENCES,0.4376731301939058,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.4404432132963989,"Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-
aware minimization for scale-invariant learning of deep neural networks.
arXiv preprint
arXiv:2102.11600, 2021."
REFERENCES,0.44321329639889195,"Congcong Liu and Huaming Wu. Channel pruning based on mean gradient for accelerating convolu-
tional neural networks. Signal Processing, 156:84–91, 2019."
REFERENCES,0.44598337950138506,"Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
them. In CVPR, 2015."
REFERENCES,0.4487534626038781,"Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick. On the importance of
single directions for generalization. arXiv preprint arXiv:1803.06959, 2018."
REFERENCES,0.4515235457063712,"Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2017. doi:
10.23915/distill.00007. https://distill.pub/2017/feature-visualization."
REFERENCES,0.45429362880886426,"Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt¨aschel, Yuxiang Wu, Alexander H.
Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In
Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?
id=025X0zPfn."
REFERENCES,0.45706371191135736,"Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classiﬁers
generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, pp. 5389–5400. PMLR, 2019."
REFERENCES,0.4598337950138504,"Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localiza-
tion. In ICCV, 2017."
REFERENCES,0.4626038781163435,"A Shrikumar, P Greenside, A Shcherbina, and A Kundaje. Not just a black box: Learning important
features through propagating activation differences. 2016. arXiv preprint arXiv:1605.01713."
REFERENCES,0.46537396121883656,"Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In ICML, 2017."
REFERENCES,0.46814404432132967,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015."
REFERENCES,0.4709141274238227,"Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classiﬁcation models and saliency maps. In ICLR, 2014."
REFERENCES,0.47368421052631576,"Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi´egas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017."
REFERENCES,0.47645429362880887,Under review as a conference paper at ICLR 2022
REFERENCES,0.4792243767313019,"Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. In ICLR, 2015."
REFERENCES,0.481994459833795,"Suraj Srinivas and Franc¸ois Fleuret. Full-gradient representation for neural network visualization.
arXiv preprint arXiv:1905.00780, 2019."
REFERENCES,0.48476454293628807,"Xu Sun, Zhiyuan Zhang, Xuancheng Ren, Ruixuan Luo, and Liangyou Li. Exploring the vulnerability
of deep neural networks: A study of parameter corruption. arXiv preprint arXiv:2006.05620, 2020."
REFERENCES,0.48753462603878117,"Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In ICML,
2017."
REFERENCES,0.4903047091412742,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818–2826, 2016."
REFERENCES,0.4930747922437673,"United States Congress Senate Committee on Banking and Housing and Urban Affairs. Equal Credit
Opportunity Act. [electronic resource]. S. Rpt. 94-685. Washington, 1976."
REFERENCES,0.49584487534626037,"Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and
Xia Hu. Score-cam: Score-weighted visual explanations for convolutional neural networks. In
CVPR, 2020."
REFERENCES,0.4986149584487535,"David West. Neural network credit scoring models. Computers & Operations Research, 27(11-12):
1131–1152, 2000."
REFERENCES,0.5013850415512465,"Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural
networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015."
REFERENCES,0.5041551246537396,"Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818–833. Springer, 2014."
REFERENCES,0.5069252077562327,"Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 2921–2929, 2016."
REFERENCES,0.5096952908587258,"Bolei Zhou, David Bau, Aude Oliva, and Antonio Torralba. Interpreting deep visual representations
via network dissection. IEEE transactions on pattern analysis and machine intelligence, 41(9):
2131–2145, 2018."
REFERENCES,0.5124653739612188,"Bolei Zhou, David Bau, Aude Oliva, and Antonio Torralba. Comparing the interpretability of deep
networks via network dissection. In Explainable AI: Interpreting, Explaining and Visualizing Deep
Learning, pp. 243–252. Springer, 2019."
REFERENCES,0.5152354570637119,Under review as a conference paper at ICLR 2022
REFERENCES,0.518005540166205,"0
2
4
6
8
10
12
14
16
Layer ID 0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.5207756232686981,Saliency
REFERENCES,0.5235457063711911,Average gradient magnitudes
REFERENCES,0.5263157894736842,(a) VGG-19
REFERENCES,0.5290858725761773,"0
20
40
60
80
Layer ID 0.0 0.2 0.4 0.6"
REFERENCES,0.5318559556786704,Saliency
REFERENCES,0.5346260387811634,Average gradient magnitudes
REFERENCES,0.5373961218836565,(b) Inception v3
REFERENCES,0.5401662049861495,"0
20
40
60
80
100
120
Layer ID 0.0 0.1 0.2 0.3"
REFERENCES,0.5429362880886427,Saliency
REFERENCES,0.5457063711911357,Average gradient magnitudes
REFERENCES,0.5484764542936288,(c) DenseNet
REFERENCES,0.5512465373961218,"Figure 9: Filter-wise saliency proﬁles for other architectures. (a) VGG-19 saliency proﬁle (with-
out standardization). (b) Inception v3 saliency proﬁle (without standardization). (c) DenseNet
saliency proﬁles (without standardization). In each panel the ﬁlter-wise saliency proﬁle is averaged
over the ImageNet validation set. In every panel, the ﬁlter saliency values in each layer are sorted
in descending order, and each layer’s saliency values are concatenated. The layers are displayed
left-to-right from shallow to deep and have equal width on x-axis."
REFERENCES,0.554016620498615,"A
ADDITIONAL EXPERIMENTS"
REFERENCES,0.556786703601108,"A.1
PARAMETER SALIENCY PROFILES FOR OTHER NETWORK ARCHITECTURES"
REFERENCES,0.5595567867036011,"In this section, we present average saliency proﬁles for several popular network architectures other
than ResNet-50 (He et al., 2016). Analogously to Figure 1, Figure 9 presents average gradient
magnitudes for VGG-19 (Simonyan & Zisserman, 2015), Inception v3 (Szegedy et al., 2016), and
DenseNet (Huang et al., 2017). Similarly to Figure 2, we also present in Figure 10 standardized
ﬁlter-wise saliency proﬁles for those architectures averaged across correctly and incorrectly classiﬁed
ImageNet (Deng et al., 2009) samples."
REFERENCES,0.5623268698060941,"A.2
MORE EXAMPLES OF NEAREST NEIGHBORS"
REFERENCES,0.5650969529085873,"We present more examples of nearest neighbors in our parameter saliency space. Figure 12 are nearest
neighbors in CIFAR-10 (Krizhevsky, 2009) dataset, where reference images are chosen from samples
misclassiﬁed by our classiﬁer. Figure 13 are examples from ImageNet, where images are captioned
with the true label of the reference images."
REFERENCES,0.5678670360110804,"In addition, in comparison with the nearest neighbor in our parameter saliency space, we also conduct
the nearest neighbor search in the feature representation space. We take the feature representation
from the conv5 3 layer of a ResNet-50, and run the nearest neighbor search using the same reference
images and candidate pool as in Figure 4. Results are shown in Figure 11. We ﬁnd that nearest
neighbors in the feature space bear more resemblance in image structures, but it fails to identify
samples that share the same mistakes. In fact, most of the nearest neighbors in Figure 11 are correctly
classiﬁed samples."
REFERENCES,0.5706371191135734,Under review as a conference paper at ICLR 2022
REFERENCES,0.5734072022160664,"0
2
4
6
8
10
12
14
16
Layer ID 0.4 0.3 0.2 0.1"
REFERENCES,0.5761772853185596,Saliency
REFERENCES,0.5789473684210527,Average saliency profile of correctly classified samples
REFERENCES,0.5817174515235457,"0
2
4
6
8
10
12
14
16
Layer ID 0 1 2 3"
REFERENCES,0.5844875346260388,Saliency
REFERENCES,0.5872576177285319,Average saliency profile of incorrectly classified samples
REFERENCES,0.590027700831025,(a) VGG-19
REFERENCES,0.592797783933518,"0
20
40
60
80
Layer ID 0.4 0.2 0.0"
REFERENCES,0.5955678670360111,Saliency
REFERENCES,0.5983379501385041,Average saliency profile of correctly classified samples
REFERENCES,0.6011080332409973,"0
20
40
60
80
Layer ID 0 2 4"
REFERENCES,0.6038781163434903,Saliency
REFERENCES,0.6066481994459834,Average saliency profile of incorrectly classified samples
REFERENCES,0.6094182825484764,(b) Inception v3
REFERENCES,0.6121883656509696,"0
20
40
60
80
100
120
Layer ID 0.4 0.2 0.0"
REFERENCES,0.6149584487534626,Saliency
REFERENCES,0.6177285318559557,Average saliency profile of correctly classified samples
REFERENCES,0.6204986149584487,"0
20
40
60
80
100
120
Layer ID 0.0 0.5 1.0 1.5"
REFERENCES,0.6232686980609419,Saliency
REFERENCES,0.6260387811634349,Average saliency profile of incorrectly classified samples
REFERENCES,0.628808864265928,(c) DenseNet
REFERENCES,0.631578947368421,"Figure 10: Standardized saliency proﬁles averaged over correctly vs incorrectly classiﬁed sam-
ples. (a) VGG-19 saliency proﬁles. (b) Inception v3 saliency proﬁles. (c) DenseNet saliency proﬁles.
In each panel, the top row presents the standardized saliency proﬁles averaged over correctly classiﬁed
samples and the bottom row shows standardized saliency proﬁles averaged over incorrectly classiﬁed
samples. On every panel, the ﬁlter saliency values in each layer are sorted in descending order, and
each layer’s saliency values are concatenated. The layers are displayed left-to-right from shallow to
deep and have equal width on x-axis."
REFERENCES,0.6343490304709142,"(a)
(b)"
REFERENCES,0.6371191135734072,Figure 11: Examples of nearest neighbors in the feature representation space (from ImageNet).
REFERENCES,0.6398891966759003,Under review as a conference paper at ICLR 2022
REFERENCES,0.6426592797783933,"Furthermore, we present preliminary results on applying our method to language models. We use
a BERT (Devlin et al., 2018) model, pre-trained on the task of predicting the word at a masked
position. We consider an independent dataset for evaluation in our experiments, which consists of
short sentences with masked words, provided by LAMA (F. Petroni & Riedel, 2019; Petroni et al.,
2020), an open-source language model analysis framework. Similar to the ﬁlter-wise aggregation
in convolutional networks, we adopt column-wise aggregation for obtaining our saliency proﬁles in
the transformer-based architecture. We conduct the nearest neighbor search by comparing sentences
from the dataset in saliency space and analyzing the top-5 nearest neighbors."
REFERENCES,0.6454293628808865,"We present four examples below, where the numbering indicates n-th nearest neighbor sentence, and
the italic word is the ground truth. Each example is accompanied with a description of similarities
between the neighbors:"
REFERENCES,0.6481994459833795,Reference: “Cany Ash and Robert Sakula are both Architects.” incorrectly predicted as: actors
REFERENCES,0.6509695290858726,"1. “David Castlles-Quintana and Vicente Royuela are economists.” incorrectly predicted as:
actors
2. “Raghuram Rajan is an economist.” incorrectly predicted as: actor
3. “Richard G. Wilkinson and Kate Pickett are British.” incorrectly predicted as: actors
4. “Nathan Alterman was a poet.” correctly predicted: poet
5. “Zbigniew Badowski is an architect.” incorrectly predicted as: author"
REFERENCES,0.6537396121883656,"Note that all 5 neighboring sentences here share the common structure of being declarations of
profession for speciﬁcally named people. Interestingly, the ﬁrst three closest sentences to the
reference incorrectly predict the profession to be an actor as well. Moreover, the ground truth of the
reference and its closest neighbor is economist/s."
REFERENCES,0.6565096952908587,Reference: “D’Olier Street is in Dublin.” incorrectly predicted as: Paris
REFERENCES,0.6592797783933518,"1. “A group who call themselves Huguenots lives in Australia.” incorrectly predicted as: France
2. “Huguenots and Walloons settled in Canterbury.” incorrectly predicted as: France
3. “In the Treaty of Lisbon 2007 Ireland refused to consent to changes.” incorrectly predicted
as: it
4. “Samuel Marsden Collegiate School is located in Wellington.” incorrectly predicted as:
Melbourne
5. “Konstantin Mereschkowski has Russian nationality.” correctly predicted: Russian"
REFERENCES,0.6620498614958449,"Again, we see all ﬁve nearest neighbor sentences are semantically similar to the reference, this time
relating to national afﬁliations and geography. In the ﬁrst two closest sentences, the model incorrectly
ﬁlls in a location with France, which is similar to the reference which incorrectly predicts Paris."
REFERENCES,0.6648199445983379,Reference: “The Super Bowl sponsor was the Gap clothing company.” incorrectly predicted as Nike
REFERENCES,0.667590027700831,"1. “During Super Bowl 50 the Nintendo gaming company debuted their ad for the ﬁrst time.”
incorrectly predicted as: video
2. “Experimental measurements on a model steam engine was made by Watt.” incorrectly
predicted as: Siemens
3. “ABC’s programming strategy was criticized in May 1961 by Life magazine.” incorrectly
predicted as: Time
4. “In 2009, Doctor Who started to be shown on Canadian cable station Space.” incorrectly
predicted as: CBC
5. “To emphasize the 50th anniversary of the Super Bowl the gold color was used.” incorrectly
predicted as: blue"
REFERENCES,0.6703601108033241,"All but one of the nearest neighbors in this example relate to some kind of TV programming, and
two also mention the Super Bowl. Much like the reference sentence, which incorrectly predicts the
name of a corporation, the ﬁrst four neighbors have a ground truth or incorrect prediction that is also
a corporation."
REFERENCES,0.6731301939058172,Under review as a conference paper at ICLR 2022
REFERENCES,0.6759002770083102,"Reference sample: “Tetzel’s collections of money to free souls from purgatory was objected by
Luther.” incorrectly predicted as: some"
REFERENCES,0.6786703601108033,"1. “Newcastle was granted a new charter in 1589 by Elizabeth.” incorrectly predicted as:
Parliament"
REFERENCES,0.6814404432132964,"2. “The suggestion that imperialism was the “highest” form of capitalism is from Lenin.”
incorrectly predicted as: Aristotle"
REFERENCES,0.6842105263157895,3. “Fritschel said the man’s sleep was disturbed by dreams.” incorrectly predicted as: lightning
REFERENCES,0.6869806094182825,"4. “The concept that falling objects fell at the same speed regardless of weight was introduced
by Galileo.” incorrectly predicted as: NASA"
REFERENCES,0.6897506925207756,"5. “One of the earliest examples of Civil Disobedience was brought forward by the Egyptians.”
incorrectly predicted as: government"
REFERENCES,0.6925207756232687,"This is an example where there is a weak relation between the incorrect classiﬁcations (three of
ﬁve involve some kind of government or government organization) of the nearest neighbors, but the
sentences are still highly semantically similar. All of the neighbors except the third are declarations
of historical actions performed by a speciﬁc person or group of people."
REFERENCES,0.6952908587257618,"(a) bird ↔cat
(b) frog ↔cat
(c) airplane ↔ship
(d) horse ↔deer"
REFERENCES,0.6980609418282548,"Figure 12: CIFAR-10 examples of nearest neighbors in parameter saliency space. On CIFAR-10
images that cause similar ﬁlters to malfunction are often misclassiﬁed in a similar way."
REFERENCES,0.7008310249307479,"(a) coyote
(b) goldﬁsh
(c) terrapin
(d) great white shark"
REFERENCES,0.703601108033241,"Figure 13: ImageNet examples of nearest neighbors in parameter saliency space. In every panel,
the reference image is in the left column and its nearest neighbors are in the right column. Panels are
captioned by the true label of their reference image."
REFERENCES,0.7063711911357341,"A.3
CORRECTING MISTAKES ON OTHER DATASETS"
REFERENCES,0.7091412742382271,"We have shown in Section 3.3 that updating a few salient parameters of an ImageNet pre-trained
network is enough for effectively correcting mistakes on the ImageNet validation set. Moreover,
in this section, we show that this effect can be extended to other independent datasets. We use the
ImageNet-v2 test set, consisting of 10,000 images collected by Recht et al. (2019), independent from
the original ImageNet data."
REFERENCES,0.7119113573407202,"In Figure 14, we observe the similar pattern as in Figure 6. With test samples independent of the
training data, salient parameters still demonstrate the most strength in correcting mistakes and their
nearest neighbors. The advantage is more prominent when inspecting model’s predictions of unseen
nearest neighbors."
REFERENCES,0.7146814404432132,"A.4
FINE-TUNING SALIENT FILTERS OF A VGG-19"
REFERENCES,0.7174515235457064,"In this section, we conduct the ﬁne-tuning experiment introduced in Section 3.3 on a VGG-19 network
trained on ImageNet, which has a total of 5504 ﬁlters. The learning rate for training our VGG network"
REFERENCES,0.7202216066481995,Under review as a conference paper at ICLR 2022
REFERENCES,0.7229916897506925,"1
5
25
75
150
300
number of tunable filters (a) 0 10 20 30 40 50 60 70 80 90 100"
REFERENCES,0.7257617728531855,Percentage of corrected samples (%)
REFERENCES,0.7285318559556787,"All filters
Most Salient"
REFERENCES,0.7313019390581718,"Random
Least Salient"
REFERENCES,0.7340720221606648,"1
5
25
75
150
300
number of tunable filters (b) 0 10 20 30"
REFERENCES,0.7368421052631579,Average percentage of corrected neighbors (%)
REFERENCES,0.739612188365651,"All filters
Most Salient"
REFERENCES,0.7423822714681441,"Random
Least Salient"
REFERENCES,0.7451523545706371,"1
5
25
75
150
300
number of tunable filters (c) -4 6 16"
REFERENCES,0.7479224376731302,True class confidence change in neighbors (%)
REFERENCES,0.7506925207756233,"All filters
Most Salient"
REFERENCES,0.7534626038781164,"Random
Least Salient"
REFERENCES,0.7562326869806094,"Figure 14: Effect of updating a small number of ﬁlters on the ImageNet-v2 test data. (a)
Percentage of samples that are corrected after ﬁne-tuning. (b) Average percentage of nearest neighbors
that are also corrected after ﬁne-tuning. (c) Average change in the conﬁdence score of the true class
among nearest neighbors. The horizontal line in each plot is the effect of updating the entire network."
REFERENCES,0.7590027700831025,"is 1/10 of that for the ResNet, so we decrease the ﬁne-tuning step size by 10 in this experiment.
Figure 15 shows the effect of updating salient ﬁlters of a VGG-19. Note that we use the same range
for the number of tunable ﬁlters in this experiment; 300 ﬁlters correspond to 5.5% of total ﬁlters in a
VGG-19, while it is 1.2% for a ResNet-50."
REFERENCES,0.7617728531855956,"1
5
25
75
150
300
number of tunable filters (a) 0 10 20 30 40 50 60 70 80 90"
REFERENCES,0.7645429362880887,Percentage of corrected samples (%)
REFERENCES,0.7673130193905817,"All filters
Most Salient"
REFERENCES,0.7700831024930748,"Random
Least Salient"
REFERENCES,0.7728531855955678,"1
5
25
75
150
300
number of tunable filters (b) 0 10 20 30"
REFERENCES,0.775623268698061,Average percentage of corrected neighbors (%)
REFERENCES,0.778393351800554,"All filters
Most Salient"
REFERENCES,0.7811634349030471,"Random
Least Salient"
REFERENCES,0.7839335180055401,"1
5
25
75
150
300
number of tunable filters (c) 0 10"
REFERENCES,0.7867036011080333,True class confidence change in neighbors (%)
REFERENCES,0.7894736842105263,"All filters
Most Salient"
REFERENCES,0.7922437673130194,"Random
Least Salient"
REFERENCES,0.7950138504155124,"Figure 15: Effect of updating a small number of ﬁlters on VGG-19. (a) Percentage of samples
that are corrected after ﬁne-tuning. (b) Average percentage of nearest neighbors that are also corrected
after ﬁne-tuning. (c) Average change in the conﬁdence score of the true class among nearest neighbors.
The horizontal line in each plot is the effect of updating the entire network."
REFERENCES,0.7977839335180056,"A.5
RANDOM PERTURBATION OF SALIENT FILTERS"
REFERENCES,0.8005540166204986,"As an alternative to the pruning approach described in Section 3.1, random perturbations could be
used to show that the most salient ﬁlters are indeed responsible for misclassiﬁcation. We perturbed the
ﬁlters using small Gaussian noise N(0, 0.001). Figure 16 presents the effect of randomly perturbing
salient ﬁlters, we observe similar trends to our pruning experiments in Section 3.1."
REFERENCES,0.8033240997229917,"A.6
CONNECTION TO ADVERSARIAL ATTACKS IN PARAMETER SPACE"
REFERENCES,0.8060941828254847,"Adversarial attacks in parameter space have been used for optimizers which ﬁnd ﬂat loss minima
(Foret et al., 2020; Kwon et al., 2021; Du et al., 2021) and for improving model robustness through
parameter-corruption-resistant training (Sun et al., 2020)."
REFERENCES,0.8088642659279779,"One could instead apply adversarial attacks to construct parameter saliency proﬁles – choose a
constraint space and perturb parameters in order to minimize loss, subject to the constraint, using
the perturbation to parameters as a saliency proﬁle (perhaps standardizing afterwards). We notice
several advantages and disadvantages of this alternative. On the one hand, the “adversarial” approach
requires a choice of constraint space and may require more compute (our method results in the exact"
REFERENCES,0.8116343490304709,Under review as a conference paper at ICLR 2022
REFERENCES,0.814404432132964,"0
20
40
60
80
100
Number of perturbed filters 0.25 0.20 0.15 0.10 0.05 0.00"
REFERENCES,0.817174515235457,Incorrect class confidence change (%)
REFERENCES,0.8199445983379502,"Most salient
Random
Least salient (a)"
REFERENCES,0.8227146814404432,"0
20
40
60
80
100
Number of perturbed filters 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.8254847645429363,True class confidence change (%)
REFERENCES,0.8282548476454293,"Most salient
Random
Least salient (b)"
REFERENCES,0.8310249307479224,"0
20
40
60
80
100
Number of perturbed filters 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0"
REFERENCES,0.8337950138504155,Percentage of corrected samples (%)
REFERENCES,0.8365650969529086,"Most salient
Random
Least salient (c)"
REFERENCES,0.8393351800554016,"Figure 16: Effect of randomly perturbing ﬁlters. (a) Change in incorrect class conﬁdence score.
(b) Change in true class conﬁdence score. (c) Percentage of samples that were corrected as the result
of pruning ﬁlters. These trends are averaged across all images misclassiﬁed by ResNet-50 in the
ImageNet validation set. The error bars represent 95% bootstrap conﬁdence intervals."
REFERENCES,0.8421052631578947,"same saliency proﬁle as a single-step adversary). On the other hand, the choice of constraint space
and optimizer in the adversarial approach yields more ﬂexibility."
REFERENCES,0.8448753462603878,"In this section, we compare our method with the adversarial-attack-based method. More speciﬁcally,
for a given sample (x, y), we perturb parameters either to maximize or minimize the loss subject to
an L2-norm constraint on the parameter change:"
REFERENCES,0.8476454293628809,"min
θ
/ max
θ
Lθ(x, y)
(4)"
REFERENCES,0.850415512465374,s.t. ∥θ −θ0∥< ϵ
REFERENCES,0.853185595567867,"Then, given the adversarially perturbed parameters θ∗, we deﬁne the adversarial-attack-based param-
eter saliency proﬁle of the sample (x, y) as the absolute difference from the initial parameters θ0:
s(x, y) = |θ∗−θ0|."
REFERENCES,0.8559556786703602,"We compare the resulting adversarial saliency proﬁles with our original method on a random sample
of 100 images from the ImageNet validation set. We observe that for a reasonably small constraint
(ϵ = 10−4), the resulting adversarial saliency proﬁles are similar to our original parameter saliency
proﬁles (as one would expect for smooth loss) with the average cosine similarity between the saliency
proﬁles generated by each method for the same images reaching 0.99 (the average is taken over the
random sample of 100 images). We also see that both methods agree on the top-k (we tried k=100)
most salient ﬁlters: on average, 95% of ﬁlters identiﬁed by our original method as top-k salient ﬁlters
were also identiﬁed as top-k salient ﬁlters by the adversarial parameter saliency. The differences
were gradually more distinct with larger constraints, however, we note that the smaller epsilons are of
greater interest since they reﬂect the intuition of perturbing only the most important parameters."
REFERENCES,0.8587257617728532,"We also tried L1-regularized adversarial attacks. We can similarly enforce sparsity in our original
method by simply adding the L1 regularizer to our loss before computing the gradient:"
REFERENCES,0.8614958448753463,"s(x, y)i := |(1 −α)∇θi(Lθ(x, y) + α∥θ −θ0∥1)|
(5)"
REFERENCES,0.8642659279778393,"This modiﬁcation can be seen as one step of the L1-regularized adversarial attack, and we experimen-
tally checked that it produces very similar results with the cosine similarity between the resulting
saliency proﬁles of 0.97 on average (for sufﬁciently large α = 0.99)."
REFERENCES,0.8670360110803325,"A.7
ADDITIONAL CASE STUDY FIGURES"
REFERENCES,0.8698060941828255,"Masking non-salient parts of the image. As noted in section 3.4 and presented in Figure 17,
masking the non-salient parts of the image results in even worse misclassiﬁcation conﬁdence with the
incorrect class conﬁdence increasing compared to the reference image."
REFERENCES,0.8725761772853186,"Pasting the salient region from the reference image onto other “great white shark” images. As
mentioned in section 3.4, in order to further investigate the effect of the salient region, we pasted it
(i.e., the seal and the boat) from the original image onto other images with “great white shark” ground
truth label that were correctly classiﬁed by ResNet-50 (see Figure 18 for examples). We observed
that this increased the probability of “killer whale” for 39 out of 40 examples of great white sharks
from the ImageNet validation set with an average increase of 3.75%."
REFERENCES,0.8753462603878116,Under review as a conference paper at ICLR 2022
REFERENCES,0.8781163434903048,"(a) Reference image
4.07%
great white shark
78.36 %
killer whale"
REFERENCES,0.8808864265927978,"(b) Input-space saliency for top 10
salient ﬁlters (pixels most responsi-
ble for misclassiﬁcation)"
REFERENCES,0.8836565096952909,"(c) Masked human
3.84%
great white shark
86.04%
killer whale"
REFERENCES,0.8864265927977839,"(d) Masked non-salient water
5.15%
great white shark
79.38%
killer whale"
REFERENCES,0.889196675900277,"Figure 17: Masking non-salient parts of the image. (a) Reference image of “great white shark”
misclassiﬁed by the model as “killer whale” and the corresponding conﬁdence scores. (b) Pixels
that cause the top 10 most salient ﬁlters to have high saliency. (c) Masked (non-salient) human. (d)
Masked non-salient water region."
REFERENCES,0.8919667590027701,"Examples of images with “killer whale” label. As we discussed in section 3.4, the model might
have learned to correlate a combination of sea creatures (e.g. a seal) and man-made structures (e.g.
a boat) with the “killer whale” label. Images of killer whales in ImageNet often have man-made
structures which look similar to the boat, we provide examples of that in Figure 19."
REFERENCES,0.8947368421052632,"Figure 18: Sample “great white shark” images with boat and seal. The salient region from the
case study image pasted onto other “great white shark” images."
REFERENCES,0.8975069252077562,Figure 19: ImageNet examples of “killer whale”.
REFERENCES,0.9002770083102493,Under review as a conference paper at ICLR 2022
REFERENCES,0.9030470914127424,"(a)
malamute
husky
(b)
stingray
electric ray
(c)
appenzeller
sennenhund
(d)
redbone
beagle"
REFERENCES,0.9058171745152355,"Figure 20: Pixels responsible for mistakes focused on the target object. (a)-(b) Masking the
salient pixels corrects the misclassiﬁcation where masking confusing features (e.g. dog ears or
spot patterns) helps distinguish animals. (c)-(d) Masking the salient pixels results in correct class
conﬁdence decrease, when the salient pixels are densely focused on the target object. The correct
class label is speciﬁed in the top row and the predicted incorrect class label – in the bottom row of the
subcaption on each panel."
REFERENCES,0.9085872576177285,"A.8
EXPLORING NEURAL NETWORK MISTAKES"
REFERENCES,0.9113573407202216,"In Section 3.4, we apply our parameter-space saliency method as an explainability tool using it
alongside our input-space technique to study how image features cause speciﬁc ﬁlters to malfunction.
In our case study, the salient pixels that confuse the network are focused less on the target object than
on other image features, and masking the salient regions which are not on the target object improves
the network behavior. Such examples expose the network’s reliance on spurious correlations and
constitute an interesting type of model mistake."
REFERENCES,0.9141274238227147,"The masking approach can be adopted to explore network mistakes and ﬁnd other interesting cases
(e.g., cases where the salient pixels are not concentrated on the target object). Investigating examples
where masking the most salient pixels improves performance may provide insights into the model’s
misbehavior as well as expose dataset noise and biases. We select misclassiﬁed samples where
masking the top 5% of salient pixels leads to an increase of at least 25% in conﬁdence corresponding
to the correct class. We showcase representative examples of different types of mistakes that we
observe in those samples in Figure 8. Many of the neural network misclassiﬁcations stem from
classifying a non-target object in images with multiple objects (see Figure 8 (a), (b)). However,
other mistakes are triggered by background features (Figure 8 (d)), dataset biases (as our case study
experiments in Figure 7 suggest), and label noise (Figure 8 (e))."
REFERENCES,0.9168975069252078,"Interestingly, in some of the selected cases the salient regions still focus on the target object features
(see Figure 8 (c)), and masking them improves the model’s behavior. Masking salient target object
features that confuse the network seems to be particularly beneﬁcial for images of animals; for
example, masking dog ears helps the network identify the correct breed (see Figure 20(a)) or masking
spot patterns helps distinguish different types of rays (see Figure 20(b))."
REFERENCES,0.9196675900277008,"While masking the top 5 % of salient pixels results in correct classiﬁcation for all samples in Figure 8
and in Figure 20(a), (b), this is, of course, not always the case. Sometimes, pixels which cause large
ﬁlter saliency values are focused densely on the target object, and masking them results in decreased
conﬁdence corresponding to the correct class. Selecting such samples can be used to ﬁnd situations
where the network is genuinely confused by the target object rather than background features (Figure
20 (c)) or samples with multiple objects present and with salient pixels more focused on the target
object (Figure 20 (d))."
REFERENCES,0.9224376731301939,Under review as a conference paper at ICLR 2022
REFERENCES,0.925207756232687,"A.9
COMPARISON TO GRADCAM"
REFERENCES,0.9279778393351801,"Existing input saliency maps used with the predicted label can highlight features which are related to
misclassiﬁcation. However, they are not speciﬁcally geared towards that goal. Our input saliency
technique highlights image features that cause speciﬁc ﬁlters to malfunction and those features,
while they might in some cases coincide with the features that explain a high class conﬁdence score
corresponding to the predicted label, may not be the same."
REFERENCES,0.9307479224376731,"In this section, we will compare our input-space saliency technique which highlights pixels that drive
high parameter saliency values of speciﬁc ﬁlters (i.e., pixels that confuse the network) to the visual
explanations produced by GradCAM with the predicted label 3 (Selvaraju et al., 2017) – one of the
most popular and high quality input-saliency methods."
REFERENCES,0.9335180055401662,"Figures 21 and 22 present panels of images comparing our method to GradCAM explanations
computed with the predicted label. From the perspective of highlighting pixels responsible for neural
network mistakes and for driving high ﬁlter saliency values, we note the following:"
REFERENCES,0.9362880886426593,"• GradCAM highlights the object that corresponds to the incorrect label, and the entire target
object is highlighted in the images where only the target object is present (see Figure 21
(c)-(e), Figure 22 (a)-(c)). However, when our method focuses on the target object, it
highlights speciﬁc features of that object. Those are the features that confuse the network,
and masking them can correct the misclassiﬁcation."
REFERENCES,0.9390581717451524,"• In cases where the network classiﬁes the non-target object in the image (see Figure 21
(a)-(b), Figure 22 (d)), both methods highlight the non-target objects. However, GradCAM
is more localized to the non-target object. This is expected since GradCAM produces visual
explanations for the predicted label (and has been shown to produce highly localized saliency
maps (Selvaraju et al., 2017)) while our method highlights all pixels that drive the ﬁlter
saliency, and these pixels may be located on the target object as well."
REFERENCES,0.9418282548476454,"• In cases where the misclassiﬁcation does not stem from confusion by the target object or
classifying the non-target object (see Figure 21 (d)-(e), Figure 22 (e)), our method highlights
background features and/or a combination of non-target object features, while GradCAM
still highlights the target object. For example, in Figure 22 (e), our method highlights the
boat and the sky much more than GradCAM, and our case study masking experiments in
section 3.4 show that those regions indeed confuse the network."
REFERENCES,0.9445983379501385,"• In addition, we emphasize that our input saliency technique is speciﬁc to the chosen ﬁlter set
F and is introduced to study the interaction between the image features and the malfunction-
ing ﬁlters. In contrast, GradCAM is not able to relate image-space mistakes to an arbitrary
set of model parameters or ﬁlters chosen by the user."
REFERENCES,0.9473684210526315,"To summarize, GradCAM (as well as many other input-space saliency methods) was designed to
be highly localized to the object correponding to the label of interest, while our method highlights
sparse ﬁne-grained features of images which we believe is a desirable property for our speciﬁc
application. Therefore, we opt for using input-gradient information similar to the original Vanilla
Gradient (Simonyan et al., 2014) method. However, instead of class conﬁdence scores, we use a
different loss – cosine distance to the boosted parameter-saliency proﬁle (as described in Section 2.2)
which allows us to explore how image features cause speciﬁc ﬁlters to malfunction."
REFERENCES,0.9501385041551247,"A.10
INPUT-SALIENCY SANITY CHECK"
REFERENCES,0.9529085872576177,"To assure that our input-space saliency technique is model dependent, we performed the model
randomization test from (Adebayo et al., 2018). We can see that the input saliency map is model
dependent. We note that the data randomization test is not applicable in our case because our input-
space saliency map is based on the parameter-saliency proﬁle and parameter-saliency is designed to
investigate a pretrained model with particular weights."
REFERENCES,0.9556786703601108,3Implementation from https://github.com/kazuto1011/grad-cam-pytorch under MIT license
REFERENCES,0.9584487534626038,Under review as a conference paper at ICLR 2022
REFERENCES,0.961218836565097,"(a)
dalmation
soccer ball
(b)
pizza
bell pepper
(c)
leopard
jaguar
(d)
junco
house ﬁnch
(e)
passenger car
locomotive"
REFERENCES,0.96398891966759,"Figure 21: Comparison to GradCAM. Top row: original image. Middle row: GradCAM input-
space saliency map for the predicted label. Bottom row: our input-space saliency technique which
highlights pixels that drive high parameter saliency values of speciﬁc ﬁlters (i.e., pixels that confuse
the network). The correct class label is speciﬁed in the top row and the predicted incorrect class label
– in the bottom row of the subcaption on each panel."
REFERENCES,0.9667590027700831,"B
IMPLEMENTATION DETAILS"
REFERENCES,0.9695290858725761,"B.1
HYPER-PARAMETER SETTING FOR FINE-TUNING SALIENT FILTERS"
REFERENCES,0.9722991689750693,"When tuning a small number of random or least salient ﬁlters, we re-normalize the gradient magnitude
of these parameters to be the same as the salient ﬁlters for a fair comparison; otherwise the gradients
for these parameters are always smaller than the salient ones by the deﬁnition of our saliency proﬁle,
and updating them would make less change to a model than updating the salient ones. In addition
to re-normalizing the gradients, we also multiply them with a step size, similar to the concept of
learning rate in stochastic gradient descent. For ResNet-50, we set the step size to be 0.001, which
equals to the learning rate of the last epoch when training the ResNet-50 on ImageNet from scratch.
For VGG-19, we also set the ﬁne-tuning step size to be the learning rate from the last training epoch,
which is 0.0001. We also note that the batch normalization layers were set to the test mode for our
ﬁne-tuning experiments."
REFERENCES,0.9750692520775623,"B.2
INPUT SALIENCY VISUALIZATION"
REFERENCES,0.9778393351800554,"The number of top salient ﬁlters to boost was chosen to be 10 in all input-space saliency experiments.
The ﬁlters were boosted by multiplying by 100. For visualization, absolute input gradients were
thresholded at 90-th percentile and Gaussian Blur with (3, 3) kernel was applied."
REFERENCES,0.9806094182825484,"B.3
HARDWARE"
REFERENCES,0.9833795013850416,"The experiments were run on Nvidia GeForce RTX 2080Ti GPUs with 11Gb GPU memory on a
machine with 4 cpu cores and 64Gb RAM. The input-space and parameter-saliency proﬁles take
seconds to compute for a single sample."
REFERENCES,0.9861495844875346,Under review as a conference paper at ICLR 2022
REFERENCES,0.9889196675900277,"(a)
malamute
husky
(b)
stingray
electric ray
(c)
appenzeller
sennenhund
(d)
redbone
beagle
(e)
great white shark
killer whale"
REFERENCES,0.9916897506925207,"Figure 22: Comparison to GradCAM. Top row: original image. Middle row: GradCAM input-
space saliency map for the predicted label. Bottom row: our input-space saliency technique which
highlights pixels that drive high parameter saliency values of speciﬁc ﬁlters (i.e., pixels that confuse
the network). The correct class label is speciﬁed in the top row and the predicted incorrect class label
– in the bottom row of the subcaption on each panel."
REFERENCES,0.9944598337950139,"(a) Original
(b) Stage 4
(c) Stages 3-4
(d) Stages 2-4
(e) Stages 1-4"
REFERENCES,0.997229916897507,"Figure 23: Sanity checks. (a) No randomization of ResNet-50. (b) Only stage 4 of ResNet-50
is randomized. (c) Stages 3-4 of ResNet-50 are randomized. (d) Stages 2-4 of ResNet-50 are
randomized. (e) The entire ResNet-50 is randomized."
