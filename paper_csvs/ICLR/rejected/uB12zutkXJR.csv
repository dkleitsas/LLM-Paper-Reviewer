Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024330900243309003,"We present GRAPHIX, a pre-trained graph edit model for automatically detecting
and ﬁxing bugs and code quality issues in Java programs. Unlike sequence-to-
sequence models, GRAPHIX leverages the abstract syntax structure of code and
represents the code using a multi-head graph encoder. Along with an autoregressive
tree decoder, the model learns to perform graph edit actions for automated program
repair. We devise a novel pre-training strategy for GRAPHIX, namely deleted
sub-tree reconstruction, to enrich the model with implicit knowledge of program
structures from unlabeled source code. The pre-training objective is made consis-
tent with the bug ﬁxing task to facilitate the downstream learning. We evaluate
GRAPHIX on the Patches in The Wild Java benchmark, using both abstract and
concrete code. Experimental results show that GRAPHIX signiﬁcantly outperforms
a wide range of baselines including CodeBERT and BART and is as competitive
as state-of-the-art pre-trained Transformer models despite using fewer parameters.
Further analysis demonstrates strong inductive biases of GRAPHIX in learning
meaningful structural and semantic code patterns, both in abstract and concrete
source code."
INTRODUCTION,0.004866180048661801,"1
INTRODUCTION"
INTRODUCTION,0.0072992700729927005,"Detecting bugs and code quality issues in programs and ﬁxing them is an important task in software
development. Oftentimes, this two-step process is manual and labor-intensive. In order to ease the
burden on developers and reduce the development cost, several automated tools are developed and
integrated with development environments (IDEs) and/or code review systems. There exist many
program analysis techniques (Sadowski et al., 2015) that use hand-written rules to provide linter-style
recommendations while others (Paletov et al., 2018) are able to detect speciﬁc bug types (e.g., crypto
API uses) but leave them to the developer to ﬁx. In the recent years, advances in deep learning have
appealed researchers to move to data-driven approaches (Vasic et al., 2018) that learn to detect and
ﬁx bugs from data. Henceforth, we refer to the problem of automatically detecting and ﬁxing bugs as
automated program repair (Goues et al., 2019)."
INTRODUCTION,0.009732360097323601,"One main challenge with using machine learning for automated program repair is the lack of large
human-labeled datasets. Several works (Tufano et al., 2019b;a) propose heuristics to extract bug-ﬁx
data from code changes. Code changes are represented as (code before, code after)
pairs mostly at the method level, and typically pertain to feature enhancements, bug ﬁxes and code
refactoring. Code changes relating to bug ﬁxes are identiﬁed through the presence of keywords such
as “bug” in commit messages, through the size of changes or both. An advantage of using code
change data is that there are plenty of code changes available in source code repositories."
INTRODUCTION,0.012165450121654502,"Based on code change data, the learning of bug ﬁxes has been taken in three main directions. The ﬁrst
line of work regards the bug-ﬁx learning problem as sequence-based code generation and harnesses
the power of sequence-to-sequence models with RNN (Hata et al., 2018; Chen et al., 2019; Tufano
et al., 2019b;a) and more recently with pre-trained Transformer models (Feng et al., 2020; Guo et al.,
2021; Ahmad et al., 2021; Wang et al., 2021; Berabi et al., 2021). Secondly, instead of generating the
ﬁxed code from scratch, Zhao et al. (2019); Chakraborty et al. (2020); Li et al. (2020); Panthaplackel
et al. (2021) extend sequence models and propose to generate edits on the buggy code. Lastly,
Yin et al. (2018); Tarlow et al. (2019); Dinella et al. (2020); Yao et al. (2021) deviate from the
token-based representation of code and leverage the hierarchical structure in the abstract syntax tree"
INTRODUCTION,0.014598540145985401,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0170316301703163,"(AST). The high level idea is to learn a graph edit model to iteratively transform the AST of the
buggy code into the AST of the ﬁxed code. The learning is supervised by tree differencing edits via
a cross-entropy loss. Despite the beneﬁts of more precise bug localization, shorter tree edits and
natural graph representations of programs, existing graph-based models have been less competitive
than Transformer-based models thus far."
INTRODUCTION,0.019464720194647202,"In this paper, we pursue the direction of representing explicit program structures with graphs and make
contributions toward source code understanding using machine learning. Speciﬁcally, we present
GRAPHIX, a pre-trained graph edit model for automated program repair. Inspired by HOPPITY
(Dinella et al., 2020), we design GRAPHIX as a sequential decision process with a multi-head graph
encoder and an autoregressive tree decoder. Conditioned on the graph state and the edit history at each
step, the decoder iteratively makes primitive actions (e.g., adding a node) on the graph corresponding
to the input AST. Unlike HOPPITY (Dinella et al., 2020), we enhance the encoder with multiple graph
heads to capture diverse aspects of hierarchical code structures. In addition, we guide the decoder
with an underlying Abstract Syntax Description Language (ASDL). Owing to the syntax language,
the graph structure and semantic edits, GRAPHIX exhibits strong inductive biases in learning generic
ﬁxing and refactoring patterns from code changes. Moreover, we devise a novel pre-training strategy,
namely deleted sub-tree reconstruction, that enables GRAPHIX to learn implicit program structures
from unlabeled data. Here, the model is pre-trained to reconstruct a randomly deleted sub-tree given
other code context, using the same cross-entropy loss as the program repair task. Our pre-training task
generalizes the masked language model and denoising objectives (Devlin et al., 2019; Lewis et al.,
2020) in sequence models to the AST representation of source code. Our idea shares the conceptual
principle with generative models of code (Li et al., 2018; Brockschmidt et al., 2018) for expression
generation and the structural language model (Alon et al., 2020) for any-code completion."
INTRODUCTION,0.021897810218978103,"We evaluate GRAPHIX on both concrete and abstract versions of the Patches in The Wild Java
benchmark (Tufano et al., 2019b). We show that GRAPHIX signiﬁcantly outperforms edit-based
and Transformer models, and the performance is as competitive as recent state-of-the-art pre-trained
baselines despite using fewer parameters. Our in-depth analysis of the generated patterns demonstrates
the ability of GRAPHIX to learn a wide range of meaningful and generic bug ﬁxing and code
refactoring patterns. Compared to HOPPITY, which merely learns short edits such as modifying
access keywords (private to public), adding break, or removing redundant type arguments,
etc., GRAPHIX is able to handle longer, more complicated structural and semantic edits (up to 20
graph edits). We showcase a variety of bug-ﬁx examples such as ﬁxing off-by-one errors, possible
null pointer exceptions, etc. in our analysis."
INTRODUCTION,0.024330900243309004,"In summary, we make the following main contributions in this work:"
GRAPHIX IS A MEDIUM-SCALE GRAPH EDIT MODEL THAT CAN BE PRE-TRAINED WITH THE DELETED,0.0267639902676399,"1. GRAPHIX is a medium-scale graph edit model that can be pre-trained with the deleted
sub-tree reconstruction objective for automated program repair."
GRAPHIX IS A MEDIUM-SCALE GRAPH EDIT MODEL THAT CAN BE PRE-TRAINED WITH THE DELETED,0.029197080291970802,"2. GRAPHIX signiﬁcantly outperforms the edit-based models and is able to achieve high top-1
exact match accuracy on the Patches in the Wild benchmark even without pre-training. Using
an explicit graph representation on the abstract syntax tree, our model is as competitive as
large-scale Transformer models."
GRAPHIX IS A MEDIUM-SCALE GRAPH EDIT MODEL THAT CAN BE PRE-TRAINED WITH THE DELETED,0.031630170316301706,"3. Unlike previous work, which only focused on the abstract version of the benchmark, we
evaluate GRAPHIX on both abstract and concrete code and show the effectiveness of our
model in both settings. This also suggests that the code abstraction may not be necessary."
GRAPHIX IS A MEDIUM-SCALE GRAPH EDIT MODEL THAT CAN BE PRE-TRAINED WITH THE DELETED,0.0340632603406326,"4. Our in-depth analysis of the generated ﬁxes demonstrates that GRAPHIX exhibits strong
inductive biases in learning to both structural and semantic code patterns, leading many
meaningful bug ﬁxes."
GRAPHIX IS A MEDIUM-SCALE GRAPH EDIT MODEL THAT CAN BE PRE-TRAINED WITH THE DELETED,0.0364963503649635,"Despite our focus on Java, our model and the pre-training strategy can be extended further to work
with multiple languages for universal source code understanding using a language-agnostic abstract
language and parser such as tree-sitter1. In addition, the syntax tree structure is highly amenable to
program dependencies. We hope our work will inspire the community to advance the area further by
incorporating more program dependencies, designing efﬁcient graph learning algorithms with large
model architectures and datasets."
GRAPHIX IS A MEDIUM-SCALE GRAPH EDIT MODEL THAT CAN BE PRE-TRAINED WITH THE DELETED,0.038929440389294405,1https://tree-sitter.github.io/tree-sitter/
GRAPHIX IS A MEDIUM-SCALE GRAPH EDIT MODEL THAT CAN BE PRE-TRAINED WITH THE DELETED,0.0413625304136253,Under review as a conference paper at ICLR 2022
RELATED WORK,0.043795620437956206,"2
RELATED WORK"
RELATED WORK,0.046228710462287104,"Deep learning for program repair Advances in machine learning in the past decade have drastically
changed the landscape of research for automated program repair. Inspired by Neural Machine Transla-
tion, Hata et al. (2018); Chen et al. (2019); Tufano et al. (2019b;a) regard code as a sequence of tokens
and use sequence-to-sequence models to “translate” the buggy code into the ﬁxed code. More recently,
pre-trained Transformer-based models such as CodeBERT (Feng et al., 2020), GraphCodeBERT
(Guo et al., 2021), PL-BART (Ahmad et al., 2021) and most recently CodeT5 (Wang et al., 2021)
achieve state-of-the-art results on “Patches in the Wild” Java benchmark (Tufano et al., 2019b) for
program repair. A series of edit-based approaches model the code edits made on the buggy code as
opposed to the “translation”-based counterparts. Zhao et al. (2019); Chakraborty et al. (2020); Li
et al. (2020); Panthaplackel et al. (2021) extend sequence-to-sequence RNN models to predict the
sequence of edits given the buggy code. Unlike these works, we build GRAPHIX on the inherent
abstract syntax structure of code instead of its token-based representation."
RELATED WORK,0.04866180048661801,"Graph neural networks GNNs such as graph convolutional networks (Kipf & Welling, 2016),
graph attention networks (Veliˇckovi´c et al., 2018) provide a set of powerful tools for learning
graph-structured data. Source code with rich graph structures in abstract syntax trees and program
dependency graphs naturally ﬁts to the GNN toolbox. Yin et al. (2018); Allamanis et al. (2018);
Hellendoorn et al. (2019); Wang et al. (2020) augment the AST with additional data ﬂow edges
between variables and use GNNs for tasks such as variable name prediction. Yin et al. (2018); Tarlow
et al. (2019); Dinella et al. (2020); Yasunaga & Liang (2020); Yao et al. (2021) use graph models
based on ASTs for program repair and editing tasks in C, C# and Javascript, separately. Particularly,
Brockschmidt et al. (2018); Dinella et al. (2020); Yao et al. (2021) use graph representations with tree-
based decoders for AST generation by editing the partial AST one node at a time. Brockschmidt et al.
(2018); Yao et al. (2021) further equip the decoder with an Abstract Syntax Description Language
(ASDL) to ensure the syntactic correctness of the generated code. Our work advances HOPPITY
(Dinella et al., 2020) and (Yao et al., 2021) with multi-head graph encoding and pre-training and
demonstrates the beneﬁts of program structures on learning bug-ﬁx patterns in Java code."
RELATED WORK,0.051094890510948905,"Pre-training Transformer (Vaswani et al., 2017) and large-scale pre-training (Radford et al., 2018;
Devlin et al., 2019; Lewis et al., 2020) have gained much traction in the broader ﬁeld of natural
language processing (NLP). Sequence-based pre-training with Transformer has also proven to be
effective in learning source code representations. In contrast, techniques for pre-training with graphs
are less well-developed, especially for learning graph representations of source code. Existing
work on graph pre-training are mainly based on attribute and edge prediction (see (Hu et al., 2020)
and references therein). Distinct from these works, our pre-training task is conceptually related to
structural generative modeling of source code (Brockschmidt et al., 2018; Alon et al., 2020) for tree
generation, just as pre-training techniques in BERT and BART derived from language modeling and
denoising for sequence generation in NLP. However, we focus on pre-training GRAPHIX to learn
hierarchical tree structures of code for program repair rather than on generating code."
MOTIVATING EXAMPLES,0.0535279805352798,"3
MOTIVATING EXAMPLES"
MOTIVATING EXAMPLES,0.05596107055961071,"We begin by motivating the graph edit approach with two examples of code changes. Listing 1
displays two test samples from the Patches in the Wild benchmark, which are real-world Java code
extracted from GitHub repositories."
MOTIVATING EXAMPLES,0.058394160583941604,"In both examples, we can see that the code changes are much smaller compared to the method bodies.
The ﬁx of the ﬁrst bug is straightforward with one edit that replaces || inside the condition with && to
prevent the program from crashing. For the second bug, the change is more structural and hierarchical
than merely replacing the token Object with void and removing return keyword. Rather, the
corresponding ﬁx can be summarized by a sequence of four tree edits: (i) replace the return type
Object with void type, (ii) remove the ReturnStmt, (iii) add an ExpressionStmt at the
same position, (iv) copy the AssignExpr in the original code over the child of the newly added
expression statement. More generally, each edit is associated with a speciﬁc node and consists of
several primitive actions such as node deletion, addition or copy and type or value prediction."
MOTIVATING EXAMPLES,0.06082725060827251,"These examples illustrate the beneﬁts of the edit-based approach over the pure translation-based
approach in three aspects. First, by modeling the edits, the model maintains the global code context"
MOTIVATING EXAMPLES,0.06326034063260341,Under review as a conference paper at ICLR 2022
MOTIVATING EXAMPLES,0.06569343065693431,public static List<String> getDomains(String consumerKey) throws APIManagementException {
MOTIVATING EXAMPLES,0.0681265206812652,String list = ApiMgtDAO.getAuthorizedDomainsByConsumerKey(consumerKey);
MOTIVATING EXAMPLES,0.0705596107055961,"if ((list != null)
||
&&
(!(list.isEmpty()))) {
return Arrays.asList(list.split("",""));
}
return null;
}"
MOTIVATING EXAMPLES,0.072992700729927,"Object
void
setData(Object newData) {"
MOTIVATING EXAMPLES,0.07542579075425791,"return
data = newData;
}"
MOTIVATING EXAMPLES,0.07785888077858881,"Listing 1: Two bug-ﬁx examples in the test Patches in the Wild benchmark Tufano et al. (2019b)
GRAPHIX can detect and ﬁx. In each method, we highlight added text in green and deleted text in
red. We also remove package names from APIs and change the long name of the ﬁrst method for
readability. (top) A bug that may cause NullPointerException when list = null in the
sample medium/8390. (bottom) A code quality issue detected in the sample small/8616 where the
setter has redundant return statement and type. The ﬁgure is best seen with color."
MOTIVATING EXAMPLES,0.08029197080291971,"instead of decoding from scratch. Second, the edit sequence is often much shorter than the code
sequence per se. Finally, with the structural edits, the decoding can be guided by an underlying syntax
language. For example, the child of ExpressionStmt must be an expression (e.g., AssignExpr
in the second bug ﬁx) and must not be other production rules such as ReturnStmt. Another
advantage is in terms of an end-to-end framework for bug detection and ﬁxing in the sense that such
an edit model can predict whether or not a program is buggy and localize the bug more precisely."
APPROACH,0.0827250608272506,"4
APPROACH"
APPROACH,0.0851581508515815,"At a high level, we want to model p(gf|θ, gb) where gb is the buggy code and gf is the ﬁxed code.
Instead of encoding gb and generating gf from scratch, we learn a sequence of T tree-edit actions
{at}T
t=1 that transforms gb to gf as motivated in Section 3. Indirectly, we learn the model p(a1:T |gb)
from a labeled dataset of samples, each with buggy code and the corresponding ground-truth edit, by
minimizing a cross-entropy loss. Figure 1 illustrates the overall architecture."
APPROACH,0.08759124087591241,Concat
APPROACH,0.09002433090024331,GIN head
APPROACH,0.09245742092457421,Input gb
APPROACH,0.0948905109489051,Method
APPROACH,0.09732360097323602,ReturnStmt body
APPROACH,0.09975669099756691,ClassType
APPROACH,0.10218978102189781,AssignExpr
APPROACH,0.10462287104622871,expression type value data
APPROACH,0.1070559610705596,target
APPROACH,0.10948905109489052,newData
APPROACH,0.11192214111922141,Object name
APPROACH,0.11435523114355231,"LSTM
Node, Op, Type,
Value Predictors"
APPROACH,0.11678832116788321,"{n}, g"
APPROACH,0.1192214111922141,"Multi-head GNN
Multi-head GNN
Multi-head GNN"
APPROACH,0.12165450121654502,"LSTM
Node, Op, Type,
Value Predictors
LSTM
Node, Op, Type,
Value Predictors
LSTM
Node, Op, Type,
Value Predictors"
APPROACH,0.12408759124087591,Step 3: Remove ReturnStmt
APPROACH,0.12652068126520682,Method Dummy body
APPROACH,0.12895377128953772,VoidType type
APPROACH,0.13138686131386862,Step 4&5: Add ExpressionStmt and copy AssignExpr
APPROACH,0.13381995133819952,Method
APPROACH,0.1362530413625304,ExpressionStmt body
APPROACH,0.1386861313868613,VoidType
APPROACH,0.1411192214111922,AssignExpr
APPROACH,0.1435523114355231,expression type
APPROACH,0.145985401459854,"value
data"
APPROACH,0.14841849148418493,target
APPROACH,0.15085158150851583,newData
APPROACH,0.15328467153284672,Step 1&2: Remove ClassType  and add VoidType
APPROACH,0.15571776155717762,Method
APPROACH,0.15815085158150852,ReturnStmt body
APPROACH,0.16058394160583941,VoidType
APPROACH,0.1630170316301703,AssignExpr
APPROACH,0.1654501216545012,expression type
APPROACH,0.1678832116788321,"value
data"
APPROACH,0.170316301703163,target
APPROACH,0.17274939172749393,newData s
APPROACH,0.17518248175182483,"Figure 1: The overall architecture of GRAPHIX with an illustration of the edit sequence on the second
buggy code in Listing 1. For readability, we show the partial AST with only two children of the root
node. The graph encoder and decoder components are repeated for the illustration purpose."
APPROACH,0.17761557177615572,"One may recover the original token-based generation model when each action corresponds to adding
a new token and T is the length of gb. The key departure from such a model is a more hierarchical
program representation that allows shorter and more structural edits between the input gb and the
output gf. Inspired by (Dinella et al., 2020; Yao et al., 2021) which build on syntax structures and"
APPROACH,0.18004866180048662,Under review as a conference paper at ICLR 2022
APPROACH,0.18248175182481752,"tree differencing-based edits, we represent the buggy code as graph gb that is constructed from the
program’s syntax tree and extra data ﬂow edges between certain leaf nodes."
APPROACH,0.18491484184914841,"Our model is sequential in nature with a multi-head graph encoder and an autoregressive tree decoder.
At each time step t, the encoder encodes the current graph state gt, and the decoder computes the
graph edit history st. Conditioned on the current state, the decoder then predicts and performs tree
edit actions. We discuss the baseline encoder, edit operations and ASDL-guided decoder in greater
details in Appendix A. Next, we elaborate more on the multi-head graph encoder and our proposed
pre-training strategy."
MULTI-HEAD GRAPH ENCODER,0.1873479318734793,"4.1
MULTI-HEAD GRAPH ENCODER"
MULTI-HEAD GRAPH ENCODER,0.1897810218978102,"Earlier work (Yin et al., 2018; Allamanis et al., 2018; Brockschmidt et al., 2018; Hellendoorn et al.,
2019) has explored graph neural networks on rich graph structures of programs including syntax
trees and dependency graphs for program representations. We follow (Dinella et al., 2020) and
represent each program using its abstract syntax tree: the core graph is built upon the syntax tree
with bidirectional edges between parent and child nodes, edges between adjacent leaf nodes and
additional edges between value nodes to partially capture data ﬂow information (Dinella et al., 2020;
Allamanis et al., 2018). Then, we use a graph isomorphism network (GIN, Xu et al. (2019)) to
compute a vector representation for each node and an aggregate graph representation. Motivated by
the success of multi-head attentions in (Vaswani et al., 2017; Veliˇckovi´c et al., 2018), we propose a
multi-head graph encoder for the graph encoding in which each head is a GIN and the head outputs
are concatenated into corresponding node and graph embeddings. Using multiple graph heads allows
the model to encode diverse aspects of the input program."
MULTI-HEAD GRAPH ENCODER,0.1922141119221411,"Our architecture is different from the per-layer multi-head attention in Transformer (Vaswani et al.,
2017) and graph attention networks (Veliˇckovi´c et al., 2018) as well as multiple towers in message
passing neural networks (Gilmer et al., 2017). We do not have cross-attention, hence having multiple
heads per graph network layer is equivalent to adding more linear projections, which merely increase
the GIN’s depth. We choose to concatenate the resulting GIN representations at the end as an
ensemble model and ﬁnd such a strategy effective in our ablation study (c.f. Appendix E)."
MULTI-HEAD GRAPH ENCODER,0.19464720194647203,"The graph encoder is the component with the most parameters. The encoder’s size is the function
of the number of layers, the latent dimension and the number of heads. We mainly experiment with
a multi-head model that has 4 layers, 8 heads and the latent dimension d = 256, which amounts to
32M parameters."
PRE-TRAINING OBJECTIVE,0.19708029197080293,"4.2
PRE-TRAINING OBJECTIVE"
PRE-TRAINING OBJECTIVE,0.19951338199513383,"We have described the architecture of GRAPHIX as a graph-based, autoregressive model. GRAPHIX is
a supervised model and trained on labeled bug-ﬁx data using the teacher-forcing technique. Since the
labeled dataset is relatively small, in order for the model to learn more inherent program structures,
we devise a novel strategy to pre-train GRAPHIX on unlabeled source code and allow the knowledge
to be transferred to the downstream program repair task."
PRE-TRAINING OBJECTIVE,0.20194647201946472,"Pre-training task. Generalizing the masked language model pre-training idea in NLP (Devlin et al.,
2019; Lewis et al., 2020), we devise a pre-training technique for the tree-structured data, namely
deleted sub-tree reconstruction. To that end, we take the ASTs from a collection of unlabeled source
code. For each AST, we randomly select a sub-tree of a certain size rooted at node n. We replace
n with a dummy node, and remove all the subsequent nodes and edges in the sub-tree. Given the
dummy node’s location, GRAPHIX is then trained to reconstruct the sub-tree using the other code
context in the sequential manner. The decoder expands the partial AST one node at a time in the
depth-ﬁrst traversal order (Maddison & Tarlow, 2014; Bielik et al., 2016), using the production rules
in the underlying abstract language. Similar to one tree edit in our bug-ﬁxing task, each expansion
step corresponds to a classiﬁcation problem, and therefore we use the same cross-entropy objective
for training without changing the network architecture."
PRE-TRAINING OBJECTIVE,0.20437956204379562,"Our pre-training task can be viewed as a generalization of the masked language model objective in
BERT (Devlin et al., 2019) and a specialization of the denoising objective in BART (Lewis et al.,
2020) to the tree structure of code. The graph reconstruction task is conceptually similar to the graph
generative models (Li et al., 2018; Brockschmidt et al., 2018) for expression generation and the"
PRE-TRAINING OBJECTIVE,0.20681265206812652,Under review as a conference paper at ICLR 2022
PRE-TRAINING OBJECTIVE,0.20924574209245742,"structural language model of code (Alon et al., 2020) for any-code completion. Unlike these works,
we are interested in pre-training rather than code generation/completion. Note that our pre-training
technique is speciﬁc to ASTs with an underlying syntax language and not applicable to arbitrary
graph structures. The pre-training strategy, however, can be extended to work with other types of
graph corruptions such as multiple sub-tree additions and replacements as well as objectives such as
link prediction and so on. We defer these directions to future work."
EXPERIMENTS,0.2116788321167883,"5
EXPERIMENTS"
EXPERIMENTS,0.2141119221411192,"We evaluate GRAPHIX on the Patches in the Wild benchmark and compare its performance to existing
translation-based and edit-based baselines. We conduct a series of experiments and studies on the
network architectures (i.e., model size and single vs. multiple heads) and the data types (short vs.
long function, and abstract vs. concrete code) to strengthen our evaluation."
DATASET,0.21654501216545013,"5.1
DATASET"
DATASET,0.21897810218978103,"We use the Patches in the Wild Java bug-ﬁx benchmark, which was curated and popularized by Tufano
et al. (2019b). The dataset consists of 123,804 bug-ﬁx pairs extracted from GitHub repositories. It has
two subsets: the small subset contains 58,350 methods/functions with less than 50 tokens whereas
each of the 65,454 medium functions is between 50 and 100 tokens. To build the ﬁnal dataset, the
authors extracted about 10M GitHub commits whose messages textually match one of these (“ﬁx”,
“bug”, “error”, and “exception”) patterns. Finally, the pair of code before and after each commit is
considered one bug-ﬁx sample with duplicates being removed. Each subset was originally divided
into training, validation and test buckets, and we use the same splits in our evaluation."
DATASET,0.22141119221411193,"Code abstraction. Tufano et al. (2019b) promoted the code abstraction idea where they abstracted
away types, names and literal values in each method with generic names such as METHOD_1,
VAR_2, INT_3. Most existing work employs this abstracted code benchmark with an only exception
of (Drain et al., 2021). For convenience, we call it abstract benchmark. We also conduct extensive
experiments on the concrete version that comes from the same dataset (Tufano et al., 2019b) without
performing the code abstraction procedure."
DATA PROCESSING FOR GRAPHIX,0.22384428223844283,"5.2
DATA PROCESSING FOR GRAPHIX"
DATA PROCESSING FOR GRAPHIX,0.22627737226277372,"Our proposed model GRAPHIX and its baseline HOPPITY (Dinella et al., 2020) operate on the
AST level, so we need two additional processing steps: code parsing and ground-truth graph edit
construction. Given each pair of buggy and ﬁxed methods, we run JavaParser2 to parse the source
code into a pair of ASTs and serialize the output ASTs into a JSON format. We discarded 120
samples (out of 123,804 samples) that JavaParser was unable to parse. Those few samples should not
affect the evaluation."
DATA PROCESSING FOR GRAPHIX,0.22871046228710462,"Ground-truth edit construction. Ground-truth edit sequences are the sole supervision signals that
enable the learning of our graph edit model. Each edit includes a pre-deﬁned operation type (see
Appendix A.2 for more details), the edit location, the node type and value. HOPPITY (Dinella et al.,
2020) uses JSON differencing algorithm3 to create the ground-truth tree edit sequence for each pair of
buggy and ﬁxed code. Since GRAPHIX is a grammar-driven tree transformation (Yao et al., 2021), we
implement a grammar-aware and shortest distance tree differencing algorithm, inspired by a dynamic
program in Yao et al. (2021) (Algorithm 3). One could also use the GumTree differencing tool4. We
show an example of two types of ground-truth edit sequences from the two differencing algorithms
and statistics highlighting the advantages of the grammar-aware tree differencing in Appendix B."
DATA PROCESSING FOR GRAPHIX,0.23114355231143552,"Pre-training data. We use the CodeSearchNet dataset released by Husain et al. (2019) for our
pre-training purpose. The entire dataset consists of 6.4M functions in six programming languages
extracted from public non-fork GitHub repositories. Since our focus is on the Patches in the Wild Java
benchmark, we use the Java portion of CodeSearchNet with a total of 1.5M functions. We ﬁlter out"
DATA PROCESSING FOR GRAPHIX,0.23357664233576642,"2https://javaparser.org/
32https://www.npmjs.com/package/fast-json-patch
4https://github.com/GumTreeDiff/gumtree"
DATA PROCESSING FOR GRAPHIX,0.2360097323600973,Under review as a conference paper at ICLR 2022
DATA PROCESSING FOR GRAPHIX,0.2384428223844282,"those functions that are not parsable or have more than 600 AST nodes. The remaining functions can
be considered ﬁxed code and are used the pre-training task. During pre-training, we select sub-trees
between 2 and 6 descendants for the deletion and construct a sequence of addition operations as the
ground-truth edits. More details on implementation are provided in Appendix C."
BASELINES AND METRICS,0.24087591240875914,"5.3
BASELINES AND METRICS"
BASELINES AND METRICS,0.24330900243309003,"We compare GRAPHIX to translation-based models including an LTSTM model (Tufano et al., 2019b)
as well as state-of-the-art pre-trained Transformer models including notably PL-BART (Ahmad et al.,
2021) and CodeT5 (Wang et al., 2021). We also compare our model against edit-based models
including Copy That! (Panthaplackel et al., 2021) and HOPPITY (Dinella et al., 2020) of which
the latter is directly related to our work. HOPPITY applies to Javascript and only works on short
single-node edits (up to 3). That is, it does not handle a non-trivial sub-tree addition. For comparison,
we modify its implementation to handle Java source code, and in order to add a sub-tree, we ﬂatten
the sub-tree from its root in the top-down and left-right order and create a series of single-node
addition edits. To train the model, we use the teacher forcing technique so that at each intermediate
step of the addition, the model is given the location of the parent node as well as type and value in the
previous step. We name this enhanced baseline e-HOPPITY for convenience."
BASELINES AND METRICS,0.24574209245742093,"Metrics. We use the standard top-1 exact match accuracy (EM) as the metric and compare the
performance of all the above models on the small and medium test sets. Note that EMs for e-
HOPPITY and GRAPHIX are computed based on AST matching, which is equivalent to sequence
matching in the other models. For GRAPHIX and e-HOPPITY, we use beam search of size 5 during
inference and select the candidate with the highest score normalized by the edit sequence length."
RESULTS,0.24817518248175183,"5.4
RESULTS"
RESULTS,0.25060827250608275,"In this section, GRAPHIX refers to the 8-head model with 32M parameters that is not pre-trained.
GRAPHIX-P refers to GRAPHIX that is pre-trained and then ﬁne-tuned on the respective datasets.
We present the results on the abstract and concrete benchmark in Table 1 and Table 2, respectively.
Each table consists of three groups of results: the ﬁrst group represents the translation-based methods
without pre-training, the second group is for edit-based methods and the last group includes pre-
trained models including GRAPHIX-P. Note that DeepDebug (Drain et al., 2021) was not evaluated
on the abstract code while most were not on the concrete code."
RESULTS,0.25304136253041365,Table 1: Top-1 exact match (EM) accuracy on the abstract benchmark with and without pre-training.
RESULTS,0.25547445255474455,"Model
Pre-trained Data
Size
Small
Medium
LSTM (Tufano et al., 2019b)
None
10M
9.22%
3.22%
GRU + Token Copy (Panthaplackel et al., 2021)
250K
14.80%
7.00%
Transformer (Drain et al., 2021)
60M
11.10%
2.70%"
RESULTS,0.25790754257907544,"GRU + Span Copy (Panthaplackel et al., 2021)
None
250K
17.70%
8.00%
e-HOPPITY (Dinella et al., 2020)
1M
7.30%
1.21%
GRAPHIX
32M
18.20%
9.19%"
RESULTS,0.26034063260340634,"CodeBERT (Feng et al., 2020),
CodeSearchNet
180M
16.40%
5.20%
GraphCodeBERT (Guo et al., 2021)
CodeSearchNet
110M
17.30%
9.10%
BART (Drain et al., 2021)
Large (54GB)
400M
16.70%
6.70%
PL-BART (Ahmad et al., 2021)
Large (655GB)
140M
19.21%
8.98%
CodeT5-small (Wang et al., 2021)
CodeSearchNet+
60M
19.06%
10.92%
CodeT5-base (Wang et al., 2021)
CodeSearchNet+
220M
21.61%
13.96%
GRAPHIX-P
CodeSearchNet Java
32M
19.81%
8.81%"
RESULTS,0.26277372262773724,"From Table 1, we can see that our models signiﬁcantly outperform the other edit-based baselines and
are on a par with larger pre-trained models. Speciﬁcally, without pre-training GRAPHIX improves
CodeBERT by 1.8% on the small subset and 4% on the medium subset. We observe a similar
improvement of GRAPHIX over BART. When pre-trained on CodeSearchNet Java, GRAPHIX-P
slightly outperforms PL-BART, CodeT5-small on the small but underperforms them on the medium.
CodeT5-base with 220M parameters is currently the state-of-the-art performance on both datasets."
RESULTS,0.26520681265206814,Under review as a conference paper at ICLR 2022
RESULTS,0.26763990267639903,Table 2: Top-1 exact match (EM) accuracy on the concrete benchmark with and without pre-training.
RESULTS,0.27007299270072993,"Model
Pre-trained Data
Size
Small
Medium
Transformer (Drain et al., 2021)
None
60M
14.60%
3.70%
GRU + Token Copy (Panthaplackel et al., 2021)
10M
6.80%
N/A
DeepDebug (T5) (Drain et al., 2021)
400M
13.90%
3.60%"
RESULTS,0.2725060827250608,"GRU + Span Copy (Panthaplackel et al., 2021)
None
250K
9.20%
N/A
e-HOPPITY (Dinella et al., 2020)
1M
7.28%
1.51%
GRAPHIX
32M
17.87%
9.01%"
RESULTS,0.2749391727493917,"DeepDebug (T5) (Drain et al., 2021)
Java (54GB)
400M
16.80%
6.30%
DeepDebug (T5) (Drain et al., 2021)
English and Java
400M
18.70%
11.40%
PL-BART (See below)
Large (655GB)
140M
19.81%
6.37%
GRAPHIX-P
CodeSearchNet Java
32M
19.31%
8.69%"
RESULTS,0.2773722627737226,"Table 2 shows the results of the models on the concrete benchmark. The PL-BART result, which was
not reported in the original paper, is obtained by ﬁne-tuning the released pre-trained PL-BART model
on the concrete code. We observe a similar improvement on the concrete benchmark for GRAPHIX-P
over GRAPHIX, which is signiﬁcantly better the edit baselines, LSTM and the Transformer model.
GRAPHIX-P is slightly better than DeepDebug (Drain et al., 2021) on the small set but worse on
the medium. Compared to PL-BART, GRAPHIX-P performs comparably on the small dataset and
outperforms PL-BART by a good margin on the medium set. In short, our model works comparably
to DeepDebug and PL-BART on the concrete benchmark despite much fewer parameters."
RESULTS,0.2798053527980535,"Comparing the results in Table 1 and Table 2, we see a negligible drop about 0.2 – 0.5% in accuracy
of GRAPHIX from the abstract to concrete code. On the contrary, the accuracy of GRU with span copy
(Panthaplackel et al., 2021) decreases by almost half. This demonstrates that GRAPHIX is insensitive
to the naming of variables, types and APIs. We also observe a similar trend for PL-BART. These
observations suggest that the code abstraction may not be necessary. Additionally, we notice that
while the pre-training provides an additional 10% relative gain on the small subsets, GRAPHIX-P does
not achieve similar gains on the medium set, which is inherently longer and harder. We hypothesize
that the synthetic edits used for pre-training might be more aligned with the small dataset than the
medium, and more sampling strategies may be needed to bridge the gap."
ANECDOTAL EXAMPLES,0.2822384428223844,"6
ANECDOTAL EXAMPLES"
ANECDOTAL EXAMPLES,0.2846715328467153,"In addition to several state-of-the-art results in EM accuracy, GRAPHIX demonstrates strong inductive
biases in learning complex bug-ﬁx patterns. In this section, we showcase several bug examples that
GRAPHIX is able to detect and ﬁx. We show more such examples in Appendix F."
ANECDOTAL EXAMPLES,0.2871046228710462,public void stopLocationUpdates() throws SecurityException {
ANECDOTAL EXAMPLES,0.2895377128953771,"if ((locationManager) != null)
{
locationManager.removeUpdates(this);
}
}"
ANECDOTAL EXAMPLES,0.291970802919708,public static Throwable getRootCause(Throwable t) {
ANECDOTAL EXAMPLES,0.2944038929440389,"if (t == null)
return null;
Throwable rootCause = t;
Throwable cause = rootCause.getCause();
while ((cause != null) && (cause != rootCause)) {
rootCause = cause;
cause = cause.getCause();
}
return rootCause;
}"
ANECDOTAL EXAMPLES,0.29683698296836986,Listing 2: Missing null check bugs in the samples small/11219 (top) and medium/10509 (bottom).
ANECDOTAL EXAMPLES,0.29927007299270075,Under review as a conference paper at ICLR 2022
ANECDOTAL EXAMPLES,0.30170316301703165,"@Override
public void resolveAnaphora() {
List<Proposition> props = VariableStorage.getPopostionList();
int i = 0;
for (CQuantifer quant : this.getQuantifers()) {"
ANECDOTAL EXAMPLES,0.30413625304136255,"if (i < =
(props.size()))
props.get(i).setLinkedId(((String) (quant.getVar().getSourceId())));
i++;
}
}"
ANECDOTAL EXAMPLES,0.30656934306569344,"public static int mul(int n1, int n2) {"
ANECDOTAL EXAMPLES,0.30900243309002434,"return n1
+
*
n2;
}"
ANECDOTAL EXAMPLES,0.31143552311435524,"Listing 3: (top) Off-by-one error in sample medium/11874 (including = may cause an index-out-of-
bound exception) and (bottom) logical bug in sample small/8295."
ANECDOTAL EXAMPLES,0.31386861313868614,"We manually analyze the generated ﬁxes of GRAPHIX on the test concrete benchmark. Since longer
edits tend to have lower probability, we normalize the prediction score of each generated ﬁx by
the length of the corresponding generated edit sequence. More speciﬁcally, we divide the negative
log-likelihood by the edit length and exponentiate the result to obtain the normalized score. Then, we
manually check 500 correct and incorrect ﬁxes with highest scores in each of the small and medium
sets. By inspecting the generated edits, we can easily spot out the code changes in each case. We
ﬁnd that similar ﬁxes have similar edits and scores. This analysis enables us to recognize interesting
bug-ﬁx patterns as well as noises in the data. The patterns range from syntactic ﬁxes such as using
equals() instead of == or simplifying logic statements to semantic ones such as like adding null
check conditions, ﬁxing variable misuses and so on. Here, we show some exemplary samples of ﬁx
patches that match the ground-truth in the benchmark. Listing 2 displays two examples of potential
bugs where the developers fail to validate the input/a class ﬁeld. GRAPHIX can not only detect
missing input validations (i.e., checking null), it can also return an intended behavior in the second
example when the input is null). Listing 3 shows two other examples where GRAPHIX ﬁxes an
off-by-one error and a logical bug by apparently understanding the method name mul."
ANECDOTAL EXAMPLES,0.31630170316301703,"We show several instances of incorrect ﬁxes generated by GRAPHIX in Appendix G. We note that
extra enclosed parentheses () in the examples are artifacts of our parser. Finally, GRAPHIX and
GRAPHIX-P share similar inductive biases and hence share similar sets of bug-ﬁx patterns."
DISCUSSION AND FUTURE WORK,0.31873479318734793,"7
DISCUSSION AND FUTURE WORK"
DISCUSSION AND FUTURE WORK,0.32116788321167883,"In this paper, we consider the problem of generating bug ﬁxes using a neural model. We present
GRAPHIX, a multi-head graph edit model that is pre-trained based on deleted sub-tree reconstruction.
GRAPHIX establishes the high performance on a Java benchmark. Anecdotal ﬁxes demonstrate that
the model is able to detect and suggest ﬁxes for a diverse set of bugs and program issues. In particular,
GRAPHIX is about 2× smaller than state-of-the-art Transformer models. Finally, GRAPHIX performs
equally well on generating abstract as well as concrete ﬁxes."
DISCUSSION AND FUTURE WORK,0.3236009732360097,"Practical use. The problem of bug detection is nuanced, and several times ML models can be
replaced by simple rules. We have shown that GRAPHIX can capture semantic bug ﬁxes through
dozens of anecdotal examples (see Appendix F for more). In addition, we demonstrate in Appendix
D that one can calibrate the model score to achieve an increased precision up to 57% by trading-off
the coverage of bug detection."
DISCUSSION AND FUTURE WORK,0.3260340632603406,"We anticipate several directions in the future work: (i) pre-train based on multiple sub-tree additions
and replacements (ii) incorporate more program dependencies into the graph representation and (iii)
leverage Transformer and GNNs in a uniﬁed architecture for the global code context in the sequence."
DISCUSSION AND FUTURE WORK,0.3284671532846715,Under review as a conference paper at ICLR 2022
REFERENCES,0.3309002433090024,REFERENCES
REFERENCES,0.3333333333333333,"Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Uniﬁed pre-training for
program understanding and generation. In NAACL, 2021."
REFERENCES,0.3357664233576642,"Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs
with graphs. International Conference on Learning Representations, 2018."
REFERENCES,0.3381995133819951,"Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In
International Conference on Machine Learning, 2020."
REFERENCES,0.340632603406326,"Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. Tﬁx: Learning to ﬁx coding
errors with a text-to-text transformer. In International Conference on Machine Learning, 2021."
REFERENCES,0.34306569343065696,"Pavol Bielik, Veselin Raychev, and Martin Vechev. Phog: probabilistic model for code. In Interna-
tional Conference on Machine Learning, pp. 2933–2942. PMLR, 2016."
REFERENCES,0.34549878345498786,"Marc Brockschmidt, Miltiadis Allamanis, Alexander L Gaunt, and Oleksandr Polozov. Generative
code modeling with graphs. In International Conference on Learning Representations, 2018."
REFERENCES,0.34793187347931875,"Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis, and Baishakhi Ray. Codit: Code editing
with tree-based neural models. IEEE Transactions on Software Engineering (TSE), 2020."
REFERENCES,0.35036496350364965,"Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk,
and Martin Monperrus. Sequencer: Sequence-to-sequence learning for end-to-end program repair.
IEEE Transactions on Software Engineering (TSE), 2019."
REFERENCES,0.35279805352798055,"J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL-HLT, 2019."
REFERENCES,0.35523114355231145,"Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. Hoppity: Learning
graph transformations to detect and ﬁx bugs in programs. In International Conference on Learning
Representations (ICLR), 2020."
REFERENCES,0.35766423357664234,"Dawn Drain, Chen Wu, Alexey Svyatkovskiy, and Neel Sundaresan. Generating bug-ﬁxes using
pretrained transformers. arXiv preprint arXiv:2104.07896, 2021."
REFERENCES,0.36009732360097324,"Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural
languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: Findings, pp. 1536–1547, 2020."
REFERENCES,0.36253041362530414,"Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, pp.
1263–1272. PMLR, 2017."
REFERENCES,0.36496350364963503,"Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. Automated program repair. Commun.
ACM, 2019."
REFERENCES,0.36739659367396593,"Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with
data ﬂow. In International Conference on Learning Representations, 2021."
REFERENCES,0.36982968369829683,"Hideaki Hata, Emad Shihab, and Graham Neubig. Learning to generate corrective patches using
neural machine translation. arXiv preprint arXiv:1812.07170, 2018."
REFERENCES,0.3722627737226277,"Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global
relational models of source code. In International Conference on Learning Representations, 2019."
REFERENCES,0.3746958637469586,"Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.
Strategies for pre-training graph neural networks. International Conference on Learning Represen-
tations, 2020."
REFERENCES,0.3771289537712895,Under review as a conference paper at ICLR 2022
REFERENCES,0.3795620437956204,"Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Code-
searchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436,
2019."
REFERENCES,0.3819951338199513,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.
In International Conference on Learning Representations, 2016."
REFERENCES,0.3844282238442822,"M. Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension. ArXiv, abs/1910.13461, 2020."
REFERENCES,0.38686131386861317,"Yi Li, Shaohua Wang, and Tien N Nguyen. Dlﬁx: Context-based code transformation learning for
automated program repair. In Proceedings of the ACM/IEEE 42nd International Conference on
Software Engineering, pp. 602–614, 2020."
REFERENCES,0.38929440389294406,"Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative
models of graphs. International Conference on Machine Learning, 2018."
REFERENCES,0.39172749391727496,"Chris Maddison and Daniel Tarlow. Structured generative models of natural source code. In
International Conference on Machine Learning, pp. 649–657. PMLR, 2014."
REFERENCES,0.39416058394160586,"Rumen Paletov, Petar Tsankov, Veselin Raychev, and Martin Vechev. Inferring crypto api rules from
code changes. ACM SIGPLAN Notices, 53(4):450–464, 2018."
REFERENCES,0.39659367396593675,"Sheena Panthaplackel, Miltiadis Allamanis, and Marc Brockschmidt. Copy that! editing sequences
by copying spans. AAAI, 2021."
REFERENCES,0.39902676399026765,"Maxim Rabinovich, Mitchell Stern, and Dan Klein. Abstract syntax networks for code generation and
semantic parsing. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1139–1149, 2017."
REFERENCES,0.40145985401459855,"Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training, 2018."
REFERENCES,0.40389294403892945,"Caitlin Sadowski, Jeffrey Van Gogh, Ciera Jaspan, Emma Soderberg, and Collin Winter. Tricorder:
Building a program analysis ecosystem. In 2015 IEEE/ACM 37th IEEE International Conference
on Software Engineering, pp. 598–608, 2015."
REFERENCES,0.40632603406326034,"Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine Manzagol, Charles
Sutton, and Edward Aftandilian. Learning to ﬁx build errors with graph2diff neural networks.
arXiv preprint arXiv:1911.01205, 2019."
REFERENCES,0.40875912408759124,"Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and Denys Poshyvanyk.
On learning meaningful code changes via neural machine translation. In 2019 IEEE/ACM 41st
International Conference on Software Engineering (ICSE), pp. 25–36. IEEE, 2019a."
REFERENCES,0.41119221411192214,"Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys
Poshyvanyk. An empirical study on learning bug-ﬁxing patches in the wild via neural machine
translation. ACM Transactions on Software Engineering and Methodology (TOSEM), 28(4):1–29,
2019b."
REFERENCES,0.41362530413625304,"Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh. Neural pro-
gram repair by jointly learning to localize and repair. In International Conference on Learning
Representations, 2018."
REFERENCES,0.41605839416058393,"Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Neural Information Processing
Systems, 2017."
REFERENCES,0.41849148418491483,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018."
REFERENCES,0.4209245742092457,Under review as a conference paper at ICLR 2022
REFERENCES,0.4233576642335766,"Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Neural Information
Processing Systems, 2015."
REFERENCES,0.4257907542579075,"Yu Wang, Ke Wang, Fengjuan Gao, and Linzhang Wang. Learning semantic program embeddings
with graph interval neural network. 2020."
REFERENCES,0.4282238442822384,"Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven CH Hoi. Codet5: Identiﬁer-aware uniﬁed pre-trained
encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859,
2021."
REFERENCES,0.4306569343065693,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? International Conference on Learning Representations, 2019."
REFERENCES,0.43309002433090027,"Ziyu Yao, Frank F. Xu, Pengcheng Yin, Huan Sun, and Graham Neubig. Learning structural edits
via incremental tree transformations. In International Conference on Learning Representations
(ICLR), 2021. URL https://openreview.net/forum?id=v9hAX77--cZ."
REFERENCES,0.43552311435523117,"Michihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from diagnostic
feedback. In International Conference on Machine Learning, pp. 10799–10808. PMLR, 2020."
REFERENCES,0.43795620437956206,"Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation.
In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 440–450, 2017."
REFERENCES,0.44038929440389296,"Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L Gaunt.
Learning to represent edits. In International Conference on Learning Representations, 2018."
REFERENCES,0.44282238442822386,"Rui Zhao, David Bieber, Kevin Swersky, and Daniel Tarlow. Neural networks for modeling source
code edits. 2019."
REFERENCES,0.44525547445255476,Under review as a conference paper at ICLR 2022
REFERENCES,0.44768856447688565,Appendix
REFERENCES,0.45012165450121655,"A
ENCODER AND DECODER"
REFERENCES,0.45255474452554745,"This section describes the technical formulation of the single-head encoder and the auto-regressive
decoder. For more details into these two components, we refer the reader to (Dinella et al., 2020; Yao
et al., 2021)."
REFERENCES,0.45498783454987834,"A.1
SINGLE-HEAD ENCODER"
REFERENCES,0.45742092457420924,"Formally, given a graph g = (V, E) with a set of nodes V and edges E, a single-head graph encoder
gives a d-dimensional representation of the graph g and representations of individual nodes v ∈V .
Similar to (Dinella et al., 2020), we compute the graph presentations for each edge type (i.e., AST
edges, last-variable-use edges and successive edges) as follows:"
REFERENCES,0.45985401459854014,"h(l+1,k)
v
= σ  
X"
REFERENCES,0.46228710462287104,"u∈Nk(v)
W l,k
1 h(l)
u "
REFERENCES,0.46472019464720193,", ∀k ∈{1, 2, . . . , K}
(1)"
REFERENCES,0.46715328467153283,"h(l+1)
v
= σ

W l
2

h(l+1,1)
v
, h(l+1,2)
v
, . . . , h(l+1,K)
v

+ h(l)
v

,
(2)"
REFERENCES,0.46958637469586373,"where K denotes the total number of edge types, L is the number of propagations. W l,k
1
∈Rd×d and
W l,k
2
∈RdK×d are the network weights. N k(v) denotes a set of neighbors of v that are connected
to v with the edge type k. These formulae give the node embeddings n = {h(L)
v
}v∈V for all nodes
v ∈V while the graph representation is an aggregation of h(l)
v for all l. More precisely, we use max
pooling to combine h(l)
v for each l and average the L + 1 resulting vectors to compute the ﬁnal graph
representation g."
REFERENCES,0.4720194647201946,"The initial embedding h(0)
v
vector of node v is set as the sum of the embedding of its node type and
node value if the value exists. The node type and value embeddings are stored in separate tables."
REFERENCES,0.4744525547445255,"A.2
TREE EDIT OPERATIONS"
REFERENCES,0.4768856447688564,"GRAPHIX is a sequential decision process that iteratively performs tree edits on the partial tree at
each step. Following (Brockschmidt et al., 2018; Yao et al., 2021), we employ a grammar-based
tree decoder to ensure the grammatical correctness of each edit action. To that end, we construct
an abstract language based on JavaParser5. The grammar includes a set of primitive and composite
types (c.f. (Rabinovich et al., 2017; Yin & Neubig, 2017)). Each composite type (e.g., stmt) has
a collection of production rules such as ExpressionStmt(expr expression) where each
rule is represented as a constructor with a list of ﬁeld arguments. Each argument has the ﬁeld type
(e.g., expr), the ﬁeld name (e.g., expression) and the cardinality of the ﬁeld as single, optional,
or sequential. The cardinality indicates the number of children a node accepts as grammatically valid
children. Based on the grammar, we deﬁne the following primitive operations:"
REFERENCES,0.4793187347931874,"DELETE operator takes a tree node n and removes it from the tree. As a result, the corresponding
sub-tree and the node n are removed. However, removing a node that corresponds to a single ﬁeld
results in a grammatically invalid tree. For instance, removing node ClassType in step 1 of Figure
1 causes the AST to have no type, which is invalid. To ensure the syntax validity at each edit step,
after the removal, we add a Dummy node as a placeholder with no node type or value. Another beneﬁt
of Dummy node is to reduce the uncertainty of the node selection in the subsequent step."
REFERENCES,0.48175182481751827,"ADD operator adds a node to the graph. Under the grammar and the dummy node mechanism, the
addition of a new node occurs in two cases. In the ﬁrst case, we add a new child to a parent node of
a sequential ﬁeld. For example, the node of type (stmt* statements) may have an arbitrary
number of children. In this case, adding a node requires the location of the parent node and the
position to be added in its children list. In the second case, we add a new node into the position of a
Dummy node (adding a non-terminal node ExpressionStmt in Figure 1 in step 4). Similarly to"
REFERENCES,0.48418491484184917,5https://javaparser.org/
REFERENCES,0.48661800486618007,Under review as a conference paper at ICLR 2022
REFERENCES,0.48905109489051096,"the DELETE operation, once adding a non-terminal node, we follow the underlying grammar speciﬁed
by the added production rule and instantiate the set of dummy, child nodes at the newly added node.
The ADD operator is also used to populate empty terminal nodes with actual node types and possible
values. Note that while constructing the original tree, we also add Dummy at the empty ﬁelds."
REFERENCES,0.49148418491484186,"COPY operator is introduced to simplify and shorten the edit sequence. While making code changes,
the developer often moves code one place to another. Such changes can be translated directly on the
AST by copying the corresponding sub-tree from the input AST. In essence, this operator locates the
target position similarly to the ADD operation and then copies the sub-tree from the original tree to
the target position of the current tree in one single step. For example, the subtree for the assignment
expression data = newData in Figure 1 at t = 2 is copied to the new position in step 6."
REFERENCES,0.49391727493917276,"Finally, the NO_OP action is used to automatically terminate the iterative tree editing procedure. After
this step, the remaining dummy nodes will be pruned."
REFERENCES,0.49635036496350365,"A.3
AUTOREGRESSIVE TREE DECODER"
REFERENCES,0.49878345498783455,"Given the graph representation gt of the program at step t and the edit history st−1 from the previous
step, we use an LSTM-based tree decoder to update the state st = LSTM(gt|st−1). Based on the
updated state st, the model predict edit actions where each node edit includes a node selector, an
operator predictor and a type/value predictor. The node selector selects node loct, then the operator
predictor decides which operator opt to apply the edit on loct among the four possible operators
{DELETE, ADD, COPY, NO_OP}. For the ADD operation, the type predictor further determines the node
type/label typet for the newly added node. For terminal nodes, the value predictor computes the node
value valuet. The probability of an edit is then decomposed as"
REFERENCES,0.5012165450121655,"p(at|st) = p(loct|st)p(opt|st, loct)p(typet|st, opt, loct)p(valuet|st, opt, loct, typet).
(3)"
REFERENCES,0.5036496350364964,We provide more details into one decoding step below:
REFERENCES,0.5060827250608273,"Node selection. For node selection, we employ a pointer network (Vinyals et al., 2015) since different
graphs have different number of nodes. For ADD, the selector either selects a sequential-ﬁeld node
or a Dummy node. In the ﬁrst case, it determines the parent of the node to be added and its left
sibling. Therefore, we further decomposte p(loct|st) = p(siblingt|st, parentt)p(loct|st, parentt).
If parentt does not have any children, the left sibling node is considered as parentt itself. We
compute the logits by taking inner product between the edit history st and the node embeddings nt.
We select the node that maximizes the inner product. After selecting the node, we update the state
with an additional LSTM call st = LSTM(emb(loct)|st) where emb(loct) is the node embedding
of the selected node loct."
REFERENCES,0.5085158150851582,"Operator prediction: The operator prediction is a regular classiﬁcation problem with four labels
{DELETE, ADD, COPY, NO_OP}. We calculate the probability of operator opt using a softmax layer
such that p(opt|st, loct) = softmax(Wopst + bop). After that, we again update the state via
st = LSTM(emb(opt)|st) where emb(opt) is the embeding of the selected operator opt."
REFERENCES,0.5109489051094891,"Type prediction. This module assigns a type typet to the newly added node in the AST. Since the
number of types is speciﬁed a priori by the underlying syntax language, we simply treat the type
predictor as a classiﬁcation problem over all possible types while masking out the impossible choices.
We calculate the probability of operator opt using a softmax layer such that p(typet|st, loct) =
softmax(Wtypest + btype). We update the state via st = LSTM(emb(typet)|st) where emb(typet)
is the embedding of the newly predicted type typet."
REFERENCES,0.51338199513382,"Value Prediction. If the newly added node is a terminal node, the value predictor assigns to it a value
valuet. The possible values are chosen from the union of the local values present in the input (i.e.,
local value table) and the global values that frequently appear in other programs. The local value
embeddings are computed by the graph encoder while the global value embeddings are stored globally
in the value embedding table. We compute the scores by simply taking inner product between the
state st and the value embeddings and select the one maximizing the score. Finally, we update the
state via st = LSTM(emb(valuet)|st) where emb(valuet) is the embedding of the value valuet."
REFERENCES,0.5158150851581509,The decoding process stops once it encounters NO_OP or reaches the maximum number edits.
REFERENCES,0.5182481751824818,Under review as a conference paper at ICLR 2022
REFERENCES,0.5206812652068127,"B
COMPARISON OF TREE DIFFERENCING ALGORITHMS"
REFERENCES,0.5231143552311436,"Table 3 shows the advantages of structural, grammar-aware tree differencing with copy operations in
GRAPHIX over JSON differencing used in HOPPITY."
REFERENCES,0.5255474452554745,"Table 3: Comparison of the edit sequence length between JSON differencing and grammar-aware
differencing algorithms."
REFERENCES,0.5279805352798054,"Differencing
Type
No. of tokens
Mean
Median
75th quantile"
REFERENCES,0.5304136253041363,"JSON differencing
Small
< 50
14.1
6
20
Medium
50-100
35.2
14
41
Combined
≤100
25.2
9
31"
REFERENCES,0.5328467153284672,"Grammar-aware differencing
Small
< 50
5.4
2
7
Medium
50-100
8.5
4
10
Combined
≤100
7.1
3
9"
REFERENCES,0.5352798053527981,medium/6715
REFERENCES,0.537712895377129,"1
@Override"
REFERENCES,0.5401459854014599,"2
public boolean create(POJO.User user) throws SQLException {"
REFERENCES,0.5425790754257908,"3
stmt = connect.prepareStatement(""INSERT INTO User ..."");"
REFERENCES,0.5450121654501217,"4
stmt.setString(1, user.getPseudo());"
REFERENCES,0.5474452554744526,"5
stmt.setString(2, user.getPassword());"
REFERENCES,0.5498783454987834,"6
stmt.setString(3, user.getEmail());"
REFERENCES,0.5523114355231143,"7
stmt.executeUpdate();"
REFERENCES,0.5547445255474452,"8
stmt.close();"
REFERENCES,0.5571776155717761,"9
System.out.println(...);"
REFERENCES,0.559610705596107,"10
return true; 11
}"
REFERENCES,0.5620437956204379,JSON differencing edits
REFERENCES,0.5644768856447688,"1. Remove the statement in line 10: return true;
2. Remove the scope: System.out
3. Remove the arguments in println
3. Remove the name println
4. Replace the remaining MethodCallExpr with BooleanLiteralExpr
5. Add true
6. Replace ExpressionStmt with ReturnStmt"
REFERENCES,0.5669099756690997,Grammar-aware differencing edits
REFERENCES,0.5693430656934306,1. Remove the statement in line 9: System.out.println(...);
REFERENCES,0.5717761557177615,"Listing 4: An example of code change where the println statement in line 9 is removed. The
structural edit is straightforward with one remove step. The JSON differencing edits are less natural:
the algorithm ﬁrst removes line 10 and gradually changes line 9 to re-add the return true
statement."
REFERENCES,0.5742092457420924,"We can see that the structural edit differencing results in much shorter edit sequences on average.
Examination of edits shows that structural edits also more meaningful. We show an example of code
change and edits in Listing 4."
REFERENCES,0.5766423357664233,"C
IMPLEMENTATION AND TRAINING DETAILS"
REFERENCES,0.5790754257907542,"We implement GRAPHIX with PyTorch based on the open source implementation of HOPPITY
(Dinella et al., 2020). We train GRAPHIX on a machine with 8 V100 GPUs, each with 32GB memory.
We use batch size 5 on each GPU both in pre-training and ﬁne-tuning. We use the Adam optimizer"
REFERENCES,0.5815085158150851,Under review as a conference paper at ICLR 2022
REFERENCES,0.583941605839416,"and a linear learning rate scheduler, used in Transformer (Vaswani et al., 2017). We set the initial
learning rate 10−4 and the warmup steps corresponding to 1 epoch. We pre-train GRAPHIX for 10
epochs for 10 days. We ﬁne-tune GRAPHIX for 20 epochs with the maximum number of edit steps
being 20. The ﬁne-tuning takes about 2 hours per epoch on the small subset and about 3.5 hours per
epoch on the medium set. We choose the best model checkpoint using the validation set and report its
top-1 accuracy on the test set."
REFERENCES,0.5863746958637469,"D
HIGH PRECISION REGIME FOR GRAPHIX"
REFERENCES,0.5888077858880778,"As shown previously and in Appendix F, GRAPHIX is able to learn meaningful and interpretable code
patterns, leading to a wide range of interesting bug ﬁxes. Indeed, the actual kinds of ﬁxes have not
been studied thoroughly for the pre-trained Transformer models. Moreover, we show that our model
can be of potential use in practice by trading-off the recall for an increased precision. To demonstrate
that, we show the impact of calibrating the prediction scores to achieve such a trade-off."
REFERENCES,0.5912408759124088,Table 4: A trade-off between the accuracy and bug detection coverage.
REFERENCES,0.5936739659367397,"Theshold
0.5
0.6
0.7
0.8
0.9"
REFERENCES,0.5961070559610706,"Accuracy
29.27%
38.12%
47.37%
56.59%
52.38%
No. predictions
1882
926
418
129
21"
REFERENCES,0.5985401459854015,"Table 4 suggests that we can achieve higher accuracy at the cost of lower prediction frequency.
For example, the model can suggest 418 most probable bug ﬁxes at 47.37% correct patches, at the
threshold 0.7 on the prediction score."
REFERENCES,0.6009732360097324,"E
ABLATION STUDIES"
REFERENCES,0.6034063260340633,"E.1
EFFECTS OF THE GRAPH MODEL SIZE AND PRE-TRAINING"
REFERENCES,0.6058394160583942,"In Table 5, we compare GRAPHIX with HOPPITY using about 1M parameters. Here, GRAPHIX-
B refers to the proposed GRAPHIX model with 1M parameters. The results show that GRAPHIX
is signiﬁcantly better than HOPPITY because of the ASDL-guided decoding and the grammar-
aware shortest edit differencing. To be speciﬁc, GRAPHIX edits are signiﬁcantly shorter and more
meaningful than JSON differencing edits (see Table 3 and Listing 4 in Appendix B for the comparison).
Moreover, the grammar-constrained decoder reduces the uncertainty of predicting edit properties at
each decoding step."
REFERENCES,0.6082725060827251,Table 5: Effects of the graph model size and pre-training on GRAPHIX.
REFERENCES,0.610705596107056,"Setting
Model
Size
Abstract
Concrete
Small
Medium
Small
Medium
No pre-training
No pre-training
e-HOPPITY
1M
7.30%
1.21%
7.28%
1.51%
GRAPHIX-B
1M
12.40%
5.34%
11.21%
4.31%"
REFERENCES,0.6131386861313869,"No pre-training
GRAPHIX
32M
18.20%
9.19%
17.87%
9.01%"
REFERENCES,0.6155717761557178,"With pre-training
GRAPHIX-P
32M
19.81%
8.81%
19.31%
8.69%"
REFERENCES,0.6180048661800487,"We also study the effects of the graph model size and pre-training on GRAPHIX. Table 5 suggests that
the multi-head encoding leads to about 50% relative boost in accuracy on the small, abstract subset
and almost 2× boost on the abstract, medium. The improvements are consistent on the concrete
code. It can be seen that the pre-training gives an additional 10% relative gain on the small subsets
but does not help on the medium sets. We explore more pre-training options and model sizes in our
future work."
REFERENCES,0.6204379562043796,"Next, we study the role of our proposed multi-head architecture."
REFERENCES,0.6228710462287105,Under review as a conference paper at ICLR 2022
REFERENCES,0.6253041362530414,"E.2
ROLE OF THE MULTI-HEAD ARCHITECTURE"
REFERENCES,0.6277372262773723,Table 6: Role of the multi-head architecture when the dimension d and the number of heads H vary.
REFERENCES,0.6301703163017032,"Model
H
d
Size
Abstract
Concrete
Small
Medium
Small
Medium
GRAPHIX-B
1
128
1M
12.40%
5.34%
11.21%
4.31%
1
256
4M
14.94%
7.07%
16.58%
7.79%
4
128
4M
15.37%
7.88%
16.79%
8.34%
1
566
32M
14.63%
6.52%
15.95%
6.63%
GRAPHIX
8
256
32M
18.20%
9.19%
17.87%
9.01%"
REFERENCES,0.6326034063260341,"Following (Dinella et al., 2020), we use the GIN depth of 4 for all the experiments. To understand the
the impact of the number of heads compared to the latent dimension, we experiment with the latent
dimension d and the number of heads H such that the corresponding model sizes are comparable.
It can be seen from Table 6 that the multi-head models outperform the corresponding single-head
counterparts with comparable sizes, for both 4M and 32M models. Moreover, increasing the model
capacity with d improves the performance as d increases from 128 to 256 while the model with
d = 566 tends to overﬁt the training data and performs poorly on the test set. In summary, our
experiments show the beneﬁts of the proposed multi-head architecture."
REFERENCES,0.635036496350365,"E.3
PER-LAYER VERSUS LAST-LAYER MULTI-HEAD"
REFERENCES,0.6374695863746959,"We implement a per-layer multi-head architecture with head averaging and GIN, following the idea
in GAT (Veliˇckovi´c et al., 2018). In this architecture, each head computes a linear projection of the
input before it is fed to the respective GIN layer, so without attention having multiple heads per
layer is the same as adding two more consecutive linear layers before that GIN layer: the ﬁrst layer
stacks the linear projection matrices into one weight matrix whereas the second has a ﬁxed weight
and computes average. We take the average of the head outputs instead of the concatenation for a
computational reason."
REFERENCES,0.6399026763990268,"We compare this new model with a single-head model of the same latent dimension (d = 256), and
our ensemble-style multi-head models (with H = 4 and H = 8 heads). The results and conﬁguration
of each model are shown in Table 7."
REFERENCES,0.6423357664233577,Table 7: Effects of the per-layer versus last-layer multi-head architectures.
REFERENCES,0.6447688564476886,"Head type
H
Agg.
d
Size
Abstract
Concrete
Small
Medium
Small
Medium
Single-head
1
N/A
256
4M
14.94%
7.07%
16.58%
7.79%
Last-layer MH
4
Concat
128
4M
15.37%
7.88%
16.79%
8.34%
Per-layer MH
8
Average
256
6M
15.78%
7.40%
16.18%
7.93%
Last-layer MH
(GRAPHIX)
8
Concat
256
32M
18.20%
9.19%
17.87%
9.01%"
REFERENCES,0.6472019464720195,"We can see this model performs slightly better than the single-head model thanks to the additional
multi-head layers across the datasets, but it is worse than GRAPHIX with 4 heads at the end and 2M
fewer parameters, except for the abstract, small dataset. Along with the previous empirical study on
the width, this experiment demonstrates the beneﬁts of the proposed multi-head architecture over
purely increasing the graph neural network’s depth or width."
REFERENCES,0.6496350364963503,"F
MORE ANECDOTAL EXAMPLES"
REFERENCES,0.6520681265206812,"We show some more bug ﬁxing and code refactoring patterns learned by GRAPHIX. In some cases,
we show the pairs of buggy code and ﬁxed code. In other cases, we only highlight the changes with"
REFERENCES,0.6545012165450121,added text in green and deleted text in red. A few long statements are replaced with “. . . ”.
REFERENCES,0.656934306569343,Under review as a conference paper at ICLR 2022
REFERENCES,0.6593673965936739,"F.1
SIMPLIFYING LOGIC EXPRESSIONS"
REFERENCES,0.6618004866180048,"Before
After"
REFERENCES,0.6642335766423357,small/6886
REFERENCES,0.6666666666666666,public boolean isEmpty() {
REFERENCES,0.6690997566909975,if ((first) == null) {
REFERENCES,0.6715328467153284,"return true;
}
return false;
}"
REFERENCES,0.6739659367396593,public boolean isEmpty() {
REFERENCES,0.6763990267639902,"return (first) == null;
}"
REFERENCES,0.6788321167883211,medium/7382
REFERENCES,0.681265206812652,"@Override
public boolean onTouchEvent(android.view.MotionEvent ev) {"
REFERENCES,0.683698296836983,"super.onTouchEvent(ev);
dragHelper.processTouchEvent(ev);
ViewGroup parent = ((ViewGroup) (findBottomView(this, x, y).getParent()));
return
false ||
(parent == (this));
}"
REFERENCES,0.6861313868613139,"F.2
FIXING OFF-BY-ONE ERRORS"
REFERENCES,0.6885644768856448,"Before
After"
REFERENCES,0.6909975669099757,small/6229
REFERENCES,0.6934306569343066,"@Override
public boolean hasNext() {"
REFERENCES,0.6958637469586375,"return ((cursor) + 1) < (batches);
}"
REFERENCES,0.6982968369829684,"@Override
public boolean hasNext() {"
REFERENCES,0.7007299270072993,"return (cursor) < (batches);
}"
REFERENCES,0.7031630170316302,small/9321
REFERENCES,0.7055961070559611,public boolean hasNext() {
REFERENCES,0.708029197080292,"return (frameIndex) < ((count) - 1);
}"
REFERENCES,0.7104622871046229,public boolean hasNext() {
REFERENCES,0.7128953771289538,return (frameIndex) < (count);
REFERENCES,0.7153284671532847,medium/11106
REFERENCES,0.7177615571776156,public static boolean isPowerOfTwo(long number) {
REFERENCES,0.7201946472019465,"if (number < =
0) {
throw new IllegalArgumentException(...);
}
if ((number & (-number)) == number) {"
REFERENCES,0.7226277372262774,"return true;
}
return false;
}"
REFERENCES,0.7250608272506083,Under review as a conference paper at ICLR 2022
REFERENCES,0.7274939172749392,medium/10998
REFERENCES,0.7299270072992701,public int run() {
REFERENCES,0.732360097323601,"int exponent = 1000;
BigInteger base = BigInteger.valueOf(2);
BigInteger value = BigInteger.ZERO;
int sum = 0;
value = base.pow(exponent);
String str = value.toString();
for (int i = 0; i <
((str.length()) - 1)
(str.length() ; i++)
sum += ((int) ((str.charAt(i)) - '0'));"
REFERENCES,0.7347931873479319,"return sum;
}"
REFERENCES,0.7372262773722628,"F.3
USING EQUALS INSTEAD OF == FOR ST R I N G COMPARISON"
REFERENCES,0.7396593673965937,small/9253
REFERENCES,0.7420924574209246,public TimelineConfig findChannelById(String id) {
REFERENCES,0.7445255474452555,for (TimelineConfig channel : channels) {
REFERENCES,0.7469586374695864,if (channel.getId() .equals(id) )
REFERENCES,0.7493917274939172,return channel;
REFERENCES,0.7518248175182481,"}
return null;
}"
REFERENCES,0.754257907542579,small/6752
REFERENCES,0.7566909975669099,private Customer findCustomer(String customerCode) {
REFERENCES,0.7591240875912408,for (Customer c : customers) {
REFERENCES,0.7615571776155717,if (c.getCode() .equals(customerCode) ) {
REFERENCES,0.7639902676399026,"return c;
}
}
return null;
}"
REFERENCES,0.7664233576642335,small/10790
REFERENCES,0.7688564476885644,public boolean checkUsername(String username) {
REFERENCES,0.7712895377128953,for (Model.User user : userRepository.findAll()) {
REFERENCES,0.7737226277372263,if (user.getName() .equals(username) ) {
REFERENCES,0.7761557177615572,"return false;
}
}
return true;
}"
REFERENCES,0.7785888077858881,Under review as a conference paper at ICLR 2022
REFERENCES,0.781021897810219,"F.4
CORRECTING RETURNED OBJECTS"
REFERENCES,0.7834549878345499,medium/7800
REFERENCES,0.7858880778588808,"private static List<GroupData> generateGroups(int count) {
List<GroupData> groups = new ArrayList<GroupData>();
for (int i = 0; i < count; i++) {
groups.add(new GroupData().withName(...));
}
return
null
groups;
}"
REFERENCES,0.7883211678832117,medium/11925
REFERENCES,0.7907542579075426,"@Override
public String toString() {
String str = ""Symbol Table list:"";
for (int i = nestinglevel; (-1) < (nestinglevel); i++) {
str += (""Nesting level "" + i) + "":\n"";
str += tables[i].toString();
}"
REFERENCES,0.7931873479318735,"return str;
}"
REFERENCES,0.7956204379562044,"F.5
FIXING CONDITIONS"
REFERENCES,0.7980535279805353,medium/9887
REFERENCES,0.8004866180048662,public View getViewWithConf(String viewName) {
REFERENCES,0.8029197080291971,if ((viewName != null) && ( ! (viewName.isEmpty()))) {
REFERENCES,0.805352798053528,for (fi.nls.oskari.domain.map.view.View item : list) {
REFERENCES,0.8077858880778589,if (viewName.equals(item.getName())) {
REFERENCES,0.8102189781021898,"return item;
}
}
}
return null;
}"
REFERENCES,0.8126520681265207,medium/9181
REFERENCES,0.8150851581508516,private double getStepMovement(genetics.MusicPhenotype p) {
REFERENCES,0.8175182481751825,"double steps = 0;
double intervalCount = 0;
for (java.util.ArrayList<Integer> measure : p.melodyIntervals) {"
REFERENCES,0.8199513381995134,"for (int interval : measure) {
interval = Math.abs(interval);
intervalCount++;
if ((interval >= 1)
||
&&
(interval <= 2)) {
steps += 1;
}
}
}
if (intervalCount == 0) {"
REFERENCES,0.8223844282238443,"return 0;
}
return steps / intervalCount;
}"
REFERENCES,0.8248175182481752,Under review as a conference paper at ICLR 2022
REFERENCES,0.8272506082725061,medium/11246
REFERENCES,0.829683698296837,"public boolean insertBudget(final String name, final int max) {
android.database.sqlite.SQLiteDatabase db = getWritableDatabase();
ContentValues contentValues = new ContentValues();
contentValues.put(DBHelper.BudgetDbIds.NAME, name);
contentValues.put(DBHelper.BudgetDbIds.MAX, max);
final long newId = db.insert(DBHelper.BudgetDbIds.TABLE, null,
contentValues);
return newId !=
1
(-1) ;
}"
REFERENCES,0.8321167883211679,"G
NEGATIVE EXAMPLES"
REFERENCES,0.8345498783454988,"While learning meaningful frequent change patterns, GRAPHIX also conﬁdently suggests incomplete
and/or incorrect bug ﬁxes. We provide three such examples below. For each example, we show the
buggy input, the generated ﬁx by GRAPHIX and the ground-truth ﬁxed code."
REFERENCES,0.8369829683698297,"In Listing 5, GRAPHIX suggests a quick ﬁx that replaces || with && but does not go beyond adding
logical ! to the second condition. Since GRAPHIX can add ! in medium/9887, Section F.5 when
trained on the medium dataset, we suspect that the model might not have seen many !.isEmpty()
patterns on the small set. Note that, even if the model could do that, the generated ﬁx and the
ground-truth label with two nested if conditions would not match, despite the logical equivalence."
REFERENCES,0.8394160583941606,Buggy code
REFERENCES,0.8418491484184915,"@Override
protected void onSaveInstanceState(android.os.Bundle savedInstance) {"
REFERENCES,0.8442822384428224,"super.onSaveInstanceState(savedInstance);
if (((listData) != null) || (listData.isEmpty())) {
savedInstance.putSerializable(""HEADER"", listData);
}
}"
REFERENCES,0.8467153284671532,Generated ﬁxed code
REFERENCES,0.8491484184914841,"@Override
protected void onSaveInstanceState(android.os.Bundle savedInstance) {"
REFERENCES,0.851581508515815,"super.onSaveInstanceState(savedInstance);
if (((listData) != null)
&&
(listData.isEmpty())) {
savedInstance.putSerializable(""HEADER"", listData);
}
}"
REFERENCES,0.8540145985401459,Ground-truth ﬁxed code
REFERENCES,0.8564476885644768,"@Override
protected void onSaveInstanceState(android.os.Bundle savedInstance) {"
REFERENCES,0.8588807785888077,"super.onSaveInstanceState(savedInstance);
if ((listData) != null) {"
REFERENCES,0.8613138686131386,"if (!(listData.isEmpty())) {
savedInstance.putSerializable(""HEADER"", listData);
}
}
}"
REFERENCES,0.8637469586374696,Listing 5: Test sample small/6783 in the benchmark.
REFERENCES,0.8661800486618005,"Listing 6 shows another incorrect ﬁx by GRAPHIX. This example is nuanced because it could be
difﬁcult to know in the ﬁrst place whether the original code is buggy or not, without more information."
REFERENCES,0.8686131386861314,"The last example, shown in Listing 7, is similar to the pattern GRAPHIX correctly learns in small/6886,
Section F.1 and many others we observe in our manual analysis. In this case, the model simpliﬁes the"
REFERENCES,0.8710462287104623,Under review as a conference paper at ICLR 2022
REFERENCES,0.8734793187347932,"logical return expression in the input code, but that does not match with the correct ﬁx. Again, it
is difﬁcult to ﬁx this kind of issues unless we have a way to represent the global context in the class
and more broadly the whole project."
REFERENCES,0.8759124087591241,“Buggy” code
REFERENCES,0.878345498783455,private boolean isValidInLength(android.text.Editable s) {
REFERENCES,0.8807785888077859,"return (!(respectPatternLength)) || ((s.length()) <= (maxLength));
}"
REFERENCES,0.8832116788321168,Generated ﬁxed code
REFERENCES,0.8856447688564477,private boolean isValidInLength(android.text.Editable s) {
REFERENCES,0.8880778588807786,"return (!(respectPatternLength))
&&
((s.length()) <= (maxLength));
}"
REFERENCES,0.8905109489051095,Ground-truth ﬁxed code
REFERENCES,0.8929440389294404,private boolean isValidInLength(android.text.Editable s) {
REFERENCES,0.8953771289537713,if ((respectPatternLength) && ((s.length()) > (maxLength)))
REFERENCES,0.8978102189781022,return false;
REFERENCES,0.9002433090024331,"return true;
}"
REFERENCES,0.902676399026764,Listing 6: Test sample small/9672 in the benchmark.
REFERENCES,0.9051094890510949,Buggy code
REFERENCES,0.9075425790754258,"@Override
public boolean onKeyUp(int keyCode, KeyEvent event) {"
REFERENCES,0.9099756690997567,if ((keyCode == (KEYCODE_VOLUME_DOWN))
REFERENCES,0.9124087591240876,"|| (keyCode == (KEYCODE_VOLUME_UP))) {
return true;
}
return false;"
REFERENCES,0.9148418491484185,Generated ﬁxed code
REFERENCES,0.9172749391727494,"@Override
public boolean onKeyUp(int keyCode, KeyEvent event) {"
REFERENCES,0.9197080291970803,return (keyCode == (KEYCODE_VOLUME_DOWN))
REFERENCES,0.9221411192214112,|| (keyCode == (KEYCODE_VOLUME_UP))
REFERENCES,0.9245742092457421,Ground-truth ﬁxed code
REFERENCES,0.927007299270073,"@Override
public boolean onKeyUp(int keyCode, KeyEvent event) {"
REFERENCES,0.9294403892944039,if ((keyCode == (KEYCODE_VOLUME_DOWN))
REFERENCES,0.9318734793187348,"|| (keyCode == (KEYCODE_VOLUME_UP))) {
return true;
}
return super.onKeyUp(keyCode, event);
}"
REFERENCES,0.9343065693430657,Listing 7: Test sample small/7677 in the benchmark.
REFERENCES,0.9367396593673966,"H
EXAMPLES OF HOPPITY"
REFERENCES,0.9391727493917275,"Finally, we show representative examples learned by e-HOPPITY in the decreasing order of the scores."
REFERENCES,0.9416058394160584,Under review as a conference paper at ICLR 2022
REFERENCES,0.9440389294403893,small/11245
REFERENCES,0.9464720194647201,"protected
public
final int getRetryMaxAttempts() {
return retryMaxAttempts;
}"
REFERENCES,0.948905109489051,small/6034
REFERENCES,0.9513381995133819,public java.lang.String[] getText() {
REFERENCES,0.9537712895377128,"return
this. text;
}"
REFERENCES,0.9562043795620438,small/10646
REFERENCES,0.9586374695863747,"@Override
public projectx.persistence.entities.Category findByid(long id) {"
REFERENCES,0.9610705596107056,"return
db.findCategoryById(id);
}"
REFERENCES,0.9635036496350365,small/5862
REFERENCES,0.9659367396593674,public models.T1Entity save(models.T1Entity entity)
REFERENCES,0.9683698296836983,"throws ClientException, ParseException {
if (entity == null)"
REFERENCES,0.9708029197080292,"return null;
if (!(isAuthenticated()))"
REFERENCES,0.9732360097323601,"return null;
models.T1Entity response = postService.save(entity);
return response;"
REFERENCES,0.975669099756691,"return postService.save(entity);
}"
REFERENCES,0.9781021897810219,small/11352
REFERENCES,0.9805352798053528,"@Override
public float computeBonus() {"
REFERENCES,0.9829683698296837,float bonus = ((this.salary) * (this.pctBonus)) + 1000;
REFERENCES,0.9854014598540146,"return bonus;
}"
REFERENCES,0.9878345498783455,medium/10324
REFERENCES,0.9902676399026764,"@java.lang.Override
public boolean onOptionsItemSelected(android.view.MenuItem item) {"
REFERENCES,0.9927007299270073,"boolean retval = true;
switch (item.getItemId()) {"
REFERENCES,0.9951338199513382,"case android.R.id.home :
onBackPressed();
break;
case R.id.change_password :
changeAccountPassword(account);
break;
case R.id.delete_account :
openAccountRemovalConfirmationDialog(account);
break;
default :
retval = super.onOptionsItemSelected(item);"
REFERENCES,0.9975669099756691,"break;
}
return retval;
}"
