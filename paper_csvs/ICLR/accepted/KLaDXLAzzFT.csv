Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.000846740050804403,"Ofﬂine reinforcement learning, which seeks to utilize ofﬂine/historical data to
optimize sequential decision-making strategies, has gained surging prominence
in recent studies. Due to the advantage that appropriate function approximators
can help mitigate the sample complexity burden in modern reinforcement learning
problems, existing endeavors usually enforce powerful function representation
models (e.g. neural networks) to learn the optimal policies. However, a precise
understanding of the statistical limits with function representations, remains elusive,
even when such a representation is linear.
Towards this goal, we study the statistical limits of ofﬂine reinforcement learning
with linear model representations. To derive the tight ofﬂine learning bound, we
design the variance-aware pessimistic value iteration (VAPVI), which adopts the
conditional variance information of the value function for time-inhomogeneous
episodic linear Markov decision processes (MDPs). VAPVI leverages estimated
variances of the value functions to reweight the Bellman residuals in the least-
square pessimistic value iteration and provides improved ofﬂine learning bounds
over the best-known existing results (whereas the Bellman residuals are equally
weighted by design). More importantly, our learning bounds are expressed in terms
of system quantities, which provide natural instance-dependent characterizations
that previous results are short of. We hope our results draw a clearer picture of
what ofﬂine learning should look like when linear representations are provided."
INTRODUCTION,0.001693480101608806,"1
INTRODUCTION"
INTRODUCTION,0.002540220152413209,"Ofﬂine reinforcement learning (ofﬂine RL or batch RL Lange et al. (2012); Levine et al. (2020))
is the framework for learning a reward-maximizing policy in an unknown environment (Markov
Decision Process or MDP)1 using the logged data coming from some behavior policy µ. Function
approximations, on the other hand, are well-known for generalization in the standard supervised
learning. Ofﬂine RL with function representation/approximation, as a result, provides generalization
across large state-action spaces for the challenging sequential decision-making problems when no
iteration is allowed (as opposed to online learning). This paradigm is crucial to the success of modern
RL problems as many deep RL algorithms ﬁnd their prototypes in the literature of ofﬂine RL. For
example, Xie and Jiang (2020) provides a view that Fitted Q-Iteration (Gordon, 1999; Ernst et al.,
2005) can be considered as the theoretical prototype of the deep Q-networks algorithm (DQN) Mnih
et al. (2015) with neural networks being the function representors. On the empirical side, there are
a huge body of deep RL-based algorithms (Mnih et al., 2015; Silver et al., 2017; Fujimoto et al.,
2019; Kumar et al., 2019; Wu et al., 2019; Kidambi et al., 2020; Yu et al., 2020; Kumar et al., 2020;
Janner et al., 2021; Chen et al., 2021a; Kostrikov et al., 2022) that utilize function approximations to
achieve respective successes in the ofﬂine regime. However, it is also realized that practical function
approximation schemes can be quite sample inefﬁcient (e.g. millions of samples are needed for deep
Q-network to solve certain Atari games Mnih et al. (2015))."
INTRODUCTION,0.003386960203217612,"1The environment could have other forms as well, e.g. partially-observed MDP (POMDP) or non-markovian
decision process (NMDP)."
INTRODUCTION,0.004233700254022015,Published as a conference paper at ICLR 2022
INTRODUCTION,0.005080440304826418,"To understand this phenomenon, there are numerous studies consider how to achieve sample efﬁciency
with function approximation from the theoretical side, as researchers ﬁnd sample efﬁcient algorithms
are possible with particular model representations, in either online RL (e.g. Yang and Wang (2019;
2020); Modi et al. (2020); Jin et al. (2020); Ayoub et al. (2020); Jiang et al. (2017); Du et al. (2019);
Sun et al. (2019); Zanette et al. (2020); Zhou et al. (2021a); Jin et al. (2021a); Du et al. (2021)) or
ofﬂine RL (e.g. Munos (2003); Chen and Jiang (2019); Xie and Jiang (2020); Jin et al. (2021b); Xie
et al. (2021a); Min et al. (2021); Duan et al. (2021); Nguyen-Tang et al. (2021); Zanette et al. (2021))."
INTRODUCTION,0.0059271803556308214,"Among them, the linear MDP model (Yang and Wang, 2020; Jin et al., 2020), where the transition is
represented as a linear combinations of the given d-dimensional feature, is (arguably) the most studied
setting in function approximation and there are plenty of extensions based upon it (e.g. generalized
linear model (Wang et al., 2021b), reward-free RL (Wang et al., 2020), gap-dependent analysis (He
et al., 2021) or generative adversarial learning (Liu et al., 2021)). Given its prosperity, however, there
are still unknowns for understanding function representations in RL, especially in the ofﬂine case."
INTRODUCTION,0.006773920406435224,"• While there are surging researches in showing provable sample efﬁciency (polynomial
sample complexity is possible) under a variety of function approximation schemes, how
to improve the sample efﬁciency for a given class of function representations remains
understudied. For instance, given a neural network approximation class, an algorithm that
learns the optimal policy with complexity O(H10) is far worse than the one that can learn
in O(H3) sample complexity, despite that both algorithms are considered sample efﬁcient.
Therefore, how to achieve the optimal/tight sample complexity when function approximation
is provided is a valuable question to consider. On the other hand, it is known that tight
sample complexity, due to the limit of the existing statistical analysis tools, can be very
tough to establish when function representation has a very complicated form. However, does
this mean tight analysis is not hopeful even when the representation is linear?
• Second, in the existing analysis of ofﬂine RL (with function approximation or simply the
tabular MDPs), the learning bounds depend either explicitly on the data-coverage quantities
(e.g. uniform concentrability coefﬁcients Chen and Jiang (2019); Xie and Jiang (2020),
uniform visitation measure Yin et al. (2021); Yin and Wang (2021a) and single concen-
trability Rashidinejad et al. (2021); Xie et al. (2021b)) or the horizon length H (Jin et al.,
2021b; Uehara and Sun, 2021). While those results are valuable as they do not depend on
the structure of the particular problem (therefore, remain valid even for pathological MDPs),
in practice, the empirical performances of ofﬂine reinforcement learning are often far better
than those non-adaptive bounds would indicate. Can the learning bounds reﬂect the nature
of individual MDP instances when the MDP model has a certain function representation?"
INTRODUCTION,0.007620660457239628,"In this work, we think about ofﬂine RL from the above two aspects. In particular, we consider the
fundamental linear model representations and ask the following question of interest:"
INTRODUCTION,0.00846740050804403,Can we achieve the statistical limits for ofﬂine RL when models have linear representations?
RELATED WORKS,0.009314140558848433,"1.1
RELATED WORKS"
RELATED WORKS,0.010160880609652836,"Ofﬂine RL with general function representations. The ﬁnite sample analysis of ofﬂine RL with
function approximation is initially conducted by Fitted Q-Iteration (FQI) type algorithms and can
be dated back to (Munos, 2003; Szepesv´ari and Munos, 2005; Antos et al., 2008a;b). Later, Chen
and Jiang (2019); Le et al. (2019); Xie and Jiang (2020) follow this line of research and derive the
improved learning results. However, owing to the aim for tackling general function approximation,
those learning bounds are expressed in terms of the stringent concentrability coefﬁcients (therefore,
are less adaptive to individual instances) and are usually only information-theoretical, due to the
computational intractability of the optimization procedure over the general function classes. Other
works impose weaker assumptions (e.g. partial coverage (Liu et al., 2020; Kidambi et al., 2020;
Uehara and Sun, 2021)), and their ﬁnite sample analysis are generally suboptimal in terms of H or
the effective horizon (1 −γ)−1."
RELATED WORKS,0.01100762066045724,"Ofﬂine RL with tabular models. For tabular MDPs, tight learning bounds can be achieved under
several data-coverage assumptions. For the class of problems with uniform data-visitation measure
dm, the near-optimal sample complexity bound has the rate O(H3/dmϵ2) for time-inhomogeneous
MDPs (Yin et al., 2021) and O(H2/dmϵ) for time-homogeneous MDPs (Yin and Wang, 2021a; Ren"
RELATED WORKS,0.011854360711261643,Published as a conference paper at ICLR 2022
RELATED WORKS,0.012701100762066046,"et al., 2021). Under the single concentrability assumption, the tight rate O(H3SC⋆/ϵ2) is obtained
by Xie et al. (2021b). In particular, the recent study Yin and Wang (2021b) introduces the intrinsic
ofﬂine learning bound that is not only instance-dependent but also subsumes previous optimal results."
RELATED WORKS,0.013547840812870448,"Ofﬂine RL with linear model representations. Recently, there is more focus on studying the
provable efﬁcient ofﬂine RL under the linear model representations. Jin et al. (2021b) ﬁrst shows
ofﬂine RL with linear MDP is provably efﬁcient by the pessimistic value iteration. Their analysis
deviates from their lower bound by a factor of d · H (check their Theorem 4.4 and 4.6). Later, Xie
et al. (2021a) considers function approximation under the Bellman-consistent assumptions, and,
when realized to linear MDP setting, improves the sample complexity guarantee of Jin et al. (2021b)
by an order O(d) (Theorem 3.2).2 However, their improvement only holds for ﬁnite action space
(due to the dependence log |A|) and by the direct reduction (from Theorem 3.1) their result does not
imply a computationally tractable algorithm with the same guarantee. Concurrently, Zanette et al.
(2021) considers the Linear Bellman Complete model and designs the actor-critic style algorithm
that achieves tight result under the assumption that the value function is bounded by 1. While their
algorithm is efﬁcient (which is based on solving a sequence of second-order cone programs), the
resulting learning bound requires the action space to be ﬁnite due to the mirror descent updates in
the Actor procedure (Agarwal et al., 2021). Besides, assuming the value function to be less than 1
simpliﬁes the challenges in dealing with horizon H since when rescaling their result to [0, H], there
is a H factor blow-up, which makes no horizon improvement comparing to Jin et al. (2021b). As a
result, none of the existing algorithms can achieve the statistical limit for the well-structured linear
MDP model with the general (inﬁnite or continuous) state-action spaces. On the other hand, Wang
et al. (2021a); Zanette (2021) study the statistical hardness of ofﬂine RL with linear representations
by proving the exponential lower bounds. Recently, Foster et al. (2021) shows realizability and
concentrability are not sufﬁcient for ofﬂine learning when state space is arbitrary large."
RELATED WORKS,0.014394580863674851,"Variance-aware studies. Talebi and Maillard (2018) ﬁrst incorporates the variance structure in online
tabular MDPs and Zanette and Brunskill (2019) tightens the result. For linear mixture MDPs, Zhou
et al. (2021a) ﬁrst uses variance structure to achieve near-optimal result and the Weighted OFUL
incorporates the variance structure explicitly in the regret bound. Recently, Variance-awareness is
also considered in Zhang et al. (2021) for horizon-free setting and for OPE problem (Min et al., 2021).
In particular, We point out that Min et al. (2021) is the ﬁrst work that uses variance reweighting
for policy evaluation in ofﬂine RL, which inspires our study for policy optimization problem. The
guarantee of Min et al. (2021) strictly improves over Duan et al. (2020) for OPE problem."
OUR CONTRIBUTION,0.015241320914479255,"1.2
OUR CONTRIBUTION"
OUR CONTRIBUTION,0.016088060965283656,"In this work, we study ofﬂine RL for time-inhomogeneous episodic linear Markov decision processes.
Linear MDPs serve as one critical step towards understanding function approximation in RL since: 1.
unlike general function representation, linear MDP representation has the well-structured form by
the given feature representors, which makes delicate statistical analysis hopeful; 2. unlike tabular
representation, which only works for ﬁnite models, linear MDP provides generalization as it adapts to
inﬁnite or continuous state-action spaces. Especially, we design the variance-aware pessimistic value
iteration (VAPVI, Algorithm 1) which incorporates the conditional variance information of the value
function and, by the variance structure, Theorem 3.2 is able to improve over the aforementioned state-
of-the-art guarantees. In addition, we further improve the state-action guarantee by designing an even
tighter bonus (4). VAPVI-Improved (Theorem 3.3) is near-minimax optimal as indicated by our lower
bound (Theorem 3.5). Importantly, the resulting learning bounds from VAPVI/VAPVI-Improved
are able to characterize the adaptive nature of individual instances and yield different convergence
rates for different problems. Algorithmically, our algorithm builds upon the nice Min et al. (2021)
with pessimism as we use the estimated variances to reweight the Bellman residual learning objective
so that the (training) samples with high uncertainty get less attention (Section 3). This is the key to
obtaining instance-adaptive guarantees."
PRELIMINARIES,0.01693480101608806,"2
PRELIMINARIES"
PRELIMINARIES,0.017781541066892465,2This comparison is based on translating their inﬁnite horizon discounted setting to the ﬁnite-horizon case.
PRELIMINARIES,0.018628281117696866,Published as a conference paper at ICLR 2022
PROBLEM SETTINGS,0.01947502116850127,"2.1
PROBLEM SETTINGS"
PROBLEM SETTINGS,0.02032176121930567,"Episodic time-inhomogeneous linear Markov decision process. A ﬁnite-horizon Markov Decision
Process (MDP) is denoted as M = (S, A, P, r, H, d1) (Sutton and Barto, 2018), where S is the
arbitrary state space and A is the arbitrary action space which can be inﬁnite or even continuous.
A time-inhomogeneous transition kernel Ph : S × A 7→∆S (∆S represents a probability simplex)
maps each state action(sh, ah) to a probability distribution Ph(·|sh, ah) and Ph can be different
across time. In addition, r : S × A 7→R is the mean reward function satisfying 0 ≤r ≤1. d1
is the initial state distribution. H is the horizon. A policy π = (π1, . . . , πH) assigns each state
sh ∈S a probability distribution over actions according to the map sh 7→πh(·|sh) ∀h ∈[H] and
induces a random trajectory s1, a1, r1, . . . , sH, aH, rH, sH+1 with s1 ∼d1, ah ∼π(·|sh), sh+1 ∼
Ph(·|sh, ah), ∀h ∈[H]. In particular, we adopts the linear MDP protocol from Jin et al. (2020;
2021b), meaning that the transition kernel and the mean reward function admit linear structures in the
feature map.
Deﬁnition 2.1 (Linear MDPs). 3 An episodic MDP (S, A, H, P, r) is called a linear MDP with
a known (unsigned) feature map φ : S × A →Rd if there exist d unknown (unsigned) measures
νh = (ν(1)
h , . . . , ν(d)
h ) over S and an unknown vector θh ∈Rd such that"
PROBLEM SETTINGS,0.021168501270110076,"Ph (s′ | s, a) = ⟨φ(s, a), νh (s′)⟩,
rh (s, a) = ⟨φ(x, a), θh⟩,
∀s′, s ∈S, a ∈A, h ∈[H]."
PROBLEM SETTINGS,0.02201524132091448,"where ∥νh(S)∥2 ≤
√"
PROBLEM SETTINGS,0.02286198137171888,"d and max(∥φ(s, a)∥2 , ∥θh∥2) ≤1 for all h ∈[H] and ∀s, a ∈S × A.
∥µh(S)∥=
R"
PROBLEM SETTINGS,0.023708721422523286,S ∥µh(s)∥ds.
PROBLEM SETTINGS,0.024555461473327687,"V -values and Q-values. For any policy π, the V -value functions V π
h (·) ∈RS and Q-value functions
Qπ
h(·, ·) ∈RS×A are deﬁned as: V π
h (s) = Eπ[PH
t=h rt|sh = s],
Qπ
h(s, a) = Eπ[PH
t=h rt|sh, ah ="
PROBLEM SETTINGS,0.02540220152413209,"s, a], ∀s, a, h ∈S, A, [H]. The performance measure is deﬁned as vπ := Ed1 [V π
1 ] = Eπ,d1
hPH
t=1 rt
i
.
The Bellman (optimality) equations follow ∀h ∈[H]: Qπ
h = rh + PhV π
h+1, V π
h = Ea∼πh[Qπ
h],
Q⋆
h =
rh + PhV ⋆
h+1, V ⋆
h = maxa Q⋆
h(·, a) (where Qh, Vh, Ph are vectors). By Deﬁnition 2.1, the Q-values
also admit linear structures, i.e. Qπ
h = ⟨φ, wπ
h⟩for some wπ
h ∈Rd (Lemma H.9). Lastly, for a policy
π, we denote the induced occupancy measure over the state-action space at any time h ∈[H] to be:
for any E ⊆S × A, dπ
h(E) := E[(sh, ah) ∈E|s1 ∼d1, ai ∼π(·|si), si ∼Pi−1(·|si−1, ai−1), 1 ≤
i ≤h] and Eπ,h[f(s, a)] :=
R"
PROBLEM SETTINGS,0.026248941574936496,"S×A f(s, a)dπ
h(s, a)dsda. Here for notation simplicity we abuse dπ
h(·)
to denote either probability measure or density function."
PROBLEM SETTINGS,0.027095681625740897,"Ofﬂine learning setting. Ofﬂine RL requires the agent to learn the policy π that maximizes vπ,
provided with the historical data D = {(sτ
h, aτ
h, rτ
h, sτ
h+1)}h∈[H]
τ∈[K] rolled out from some behavior policy µ.
The ofﬂine nature requires we cannot change µ and in particular we do not know the data generating
distribution of µ. To sum up, the agent seeks to ﬁnd a policy πalg such that v⋆−vπalg ≤ϵ for the
given batch data D and a given targeted accuracy ϵ > 0."
ASSUMPTIONS,0.0279424216765453,"2.2
ASSUMPTIONS"
ASSUMPTIONS,0.028789161727349702,"It is known that learning a near-optimal policy from the ofﬂine data D cannot be sample efﬁcient
without certain data-coverage assumptions (Wang et al., 2021a; Yin and Wang, 2021b). To begin
with, we deﬁne the population covariance matrix under the behavior policy µ for all h ∈[H]:"
ASSUMPTIONS,0.029635901778154106,"Σp
h := Eµ,h

φ(s, a)φ(s, a)⊤
,
(1)"
ASSUMPTIONS,0.03048264182895851,"since Σp
h measure the coverage of state-action space for data D, we make the following assumption.
Assumption 2.2 (Feature Coverage). The data distributions µ satisfy the minimum eigenvalue
condition: ∀h ∈[H], κh := λmin(Σp
h) > 0 and denote κ = minh κh. Note κ is a system-dependent
(non-universal) quantity as it is upper bounded by 1/d (Assumption 2 in Wang et al. (2021a))."
ASSUMPTIONS,0.03132938187976291,"We make this assumption for the following reasons. First of all, our ofﬂine learning guarantee
(Theorem 3.2) provides simultaneously comparison to all the policies, which is stronger than only
competing with the optimal policy (whereas relaxed assumption sufﬁces, e.g. supx∈Rd xΣπ⋆x⊤"
ASSUMPTIONS,0.03217612193056731,xΣµx⊤< ∞
ASSUMPTIONS,0.03302286198137172,"3This deﬁnition is a standard extension over the tabular MDPs by referencing the similar notions from the
bandit literature, i.e. from Multi-armed Bandit to Linear Bandit (Lattimore and Szepesv´ari, 2020)."
ASSUMPTIONS,0.03386960203217612,Published as a conference paper at ICLR 2022
ASSUMPTIONS,0.03471634208298052,"(Uehara and Sun, 2021)). As a consequence, the behavior distribution µ must be able to explore each
feature dimension for the result to be valid. Second, even if Assumption 2.2 does not hold, we can
always restrict our algorithmic design to the effective subspan of Σp
h, which causes the alternative
notion of κ := minh∈[H]{κh : s.t. κh = smallest positive eigenvalue at time h} (see Appendix G.1
for detailed discussions). In this scenario, learning the optimal policy cannot be guaranteed as a
constant suboptimality gap needs to be suffered due to the lack of coverage and this is formed as
assumption-free RL in Yin and Wang (2021b). Lastly, previous works analyzing the linear MDPs
impose very similar assumptions, e.g. Xie et al. (2021a) Theorem 3.2 where Σ−1
D exists and Min et al.
(2021) for the OPE problem."
ASSUMPTIONS,0.03556308213378493,"Next, for any function Vh+1(·) ∈[0, H −h], we deﬁne the conditional variance σVh+1 : S ×A →R+
as σVh+1(s, a)2 := max{1, VarPh(Vh+1)(s, a)}.4
Based on this deﬁnition, we can deﬁne the
variance-involved population covariance matrices as:Λp
h := Eµ,h

σVh+1(s, a)−2φ(s, a)φ(s, a)⊤
. In
particular, when Vh = V ⋆
h , we use the notation Λ⋆p
h instead."
ALGORITHM,0.03640982218458933,"3
ALGORITHM"
ALGORITHM,0.03725656223539373,"Least square regression is usually considered as one of the “default” tools for handling problems
with linear structures (e.g. LinUCB algorithm for linear Bandits) and ﬁnds its popularity in RL as
well since Least-Square Value Iteration (LSVI, Jin et al. (2020)) is shown to be provably efﬁcient for
linear MDPs, due to that Vh+1(s′) is an unbiased estimator of [PhVh+1](s, a). Concretely, it solves
the ridge regression problems at each time steps (with λ > 0 being the regularization parameter):"
ALGORITHM,0.03810330228619814,"bwh := argmin
w∈Rd
λ∥w∥2
2 + K
X k=1"
ALGORITHM,0.03895004233700254,"h
⟨φ(sk
h, ak
h), w⟩−rk
h −Vh+1(s′k
h+1)
i2
(2)"
ALGORITHM,0.03979678238780694,"and has the closed-form solution bwh = Σ−1
h
PK
k=1 φ(sk
h, ak
h)[rk,h + Vh+1(s′k
h )] with Σ−1
h
=
PK
k=1 φ(sk
h, ak
h)φ(sk
h, ak
h)⊤+ λI. In ofﬂine RL, this has also been leveraged in pessimistic value
iteration (Jin et al., 2021b) and ﬁtted Q-evaluation (Duan et al., 2020). Nevertheless, LSVI could
only yield suboptimal guarantees, as illustrated by the following example."
ALGORITHM,0.04064352243861134,"Example 3.1. Instantiate PEVI (Theorem 4.4 in Jin et al. (2021b)) with φ(s, a) = 1s,a (i.e. tabular
MDPs)5, by direct calculation the learning bound has the form O(dH·P"
ALGORITHM,0.04149026248941575,"h,s,a dπ⋆
h (s, a)
q"
ALGORITHM,0.04233700254022015,"1
K·dµ
h(s,a)) and"
ALGORITHM,0.04318374259102455,the optimal result (Yin and Wang (2021b) Theorem 4.1) gives O(P
ALGORITHM,0.04403048264182896,"h,s,a dπ⋆
h (s, a)
r"
ALGORITHM,0.04487722269263336,"VarPs,a (r+V ⋆
h+1)"
ALGORITHM,0.04572396274343776,"K·dµ
h(s,a)
)."
ALGORITHM,0.04657070279424217,The former has the horizon dependence H2 and the latter is H3/2 by law of total variance.
ALGORITHM,0.04741744284504657,"Motivation. By comparing the above two expressions, it can be seen that PEVI cannot get rid of
the explicit H factor due to missing the variance information (w.r.t V ⋆). If we go deeper, one could
ﬁnd that it might not be all that ideal to put equal weights on all the training samples in the least
square objective (2), since, unlike linear regression where the randomness coming from one source
distribution, we are regressing over a sequence of distributions in RL (i.e. each sh, ah corresponds
to a different distribution P(·|sh, ah) and there are possibly inﬁnite many of them). Therefore,
conceptually, the sample piece (sh, ah, sh+1) that has higher variance distribution P(·|sh, ah) tends
to be less “reliable” than the one (s′
h, a′
h, s′
h+1) with lower variance (hence should not have equal
weight in (2)). This suggests reweighting scheme might help improve the learning guarantee and
reweighting over the variance of the value function stands as a natural choice."
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.04826418289585097,"3.1
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.04911092294665537,"Now we explain our framework that incorporates the variance information. Our design is motivated
by previous Zhou et al. (2021a) (for online learning) and Min et al. (2021) (for policy evaluation).
By the ofﬂine nature, we can use the independent episodic data D′ = {(¯sτ
h, ¯aτ
h, ¯rτ
h, ¯sτ′
h )}h∈[H]
τ∈[K] (from
µ) to estimate the conditional variance of any V -values Vh+1 via the deﬁnition [VarhVh+1](s, a) ="
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.04995766299745978,"4The max(1, ·) applied here is for technical reason only. In general, it sufﬁces to think σ2
Vh+1 ≈VarhVh+1.
5This provides a valid illustration since tabular MDP is a special case of linear MDPs."
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05080440304826418,Published as a conference paper at ICLR 2022
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05165114309906858,"[Ph(Vh+1)2](s, a) −([PhVh+1](s, a))2. For the second order moment, by Deﬁnition 2.1, it holds

PhV 2
h+1

(s, a) =
Z"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05249788314987299,"S
V 2
h+1
 
s′
dPh
 
s′ | s, a

= φ(s, a)⊤
Z"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05334462320067739,"S
V 2
h+1
 
s′
dνh
 
s′
."
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05419136325148179,"Denote βh :=
R"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.0550381033022862,"S V 2
h+1 (s′) dνh (s′), then PhV 2
h+1 = ⟨φ, βh⟩and we can estimator it via:"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.0558848433530906,"¯βh = argmin
β∈Rd K
X k=1"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.056731583403895,"hD
φ(¯sk
h, ¯ak
h), β
E
−V 2
h+1

¯sk
h+1
i2
+ λ∥β∥2
2 = ¯Σ−1
h K
X"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.057578323454699404,"k=1
φ(¯sk
h, ¯ak
h)V 2
h+1

¯sk
h+1
"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05842506350550381,"and, similarly, the ﬁrst order moment PhVh+1 := ⟨φ, θh⟩can be estimated via:"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05927180355630821,"¯θh = argmin
θ∈Rd K
X k=1"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.060118543607112614,"hD
φ(¯sk
h, ¯ak
h), θ
E
−Vh+1

¯sk
h+1
i2
+ λ∥θ∥2
2 = ¯Σ−1
h K
X"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06096528365791702,"k=1
φ(¯sk
h, ¯ak
h)Vh+1

¯sk
h+1
"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06181202370872142,"The ﬁnal estimator is deﬁned as bσ2
Vh(·, ·) := max{1, d
VarhVh+1(·, ·)} with d
VarhVh+1(·, ·) ="
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06265876375952582,"⟨φ(·, ·), ¯βh⟩[0,(H−h+1)2] −

⟨φ(·, ·), ¯θh⟩[0,H−h+1]
2.6 In particular, when setting Vh+1 = bVh+1,
it recovers bσh in Algorithm 1 line 8. Here ¯Σh = PK
τ=1 φ(¯sτ
h, ¯aτ
h)φ(¯sτ
h, ¯aτ
h)⊤+ λId."
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06350550381033022,"Variance-weighted LSVI. The idea of LSVI (2) is based on approximate the Bellman updates:
Th(V )(s, a) = rh(s, a) + (PhV )(s, a). With variance estimator bσh at hand, we can modify (2) to
solve the variance-weighted LSVI instead (Line 10 of Algorithm 1)"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06435224386113463,"b
wh := argmin
w∈Rd
λ∥w∥2
2 + K
X k=1"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06519898391193904,"h
⟨φ(sk
h, ak
h), w⟩−rk
h −bVh+1(s′k
h+1)
i2"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06604572396274344,"bσ2
h(sk
h, ak
h)
= bΛ−1
h K
X k=1"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06689246401354784,"φ

sk
h, ak
h

·
h
rk
h + bVh+1

sk
h+1
i"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06773920406435224,"bσ2(sk
h, ak
h)"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06858594411515664,"where bΛh = PK
k=1 φ(sk
h, ak
h)φ(sk
h, ak
h)⊤/bσ2
h(sk
h, ak
h) + λId. The estimated Bellman update bTh (acts
on bVh+1) is deﬁned as: (bTh bVh+1)(·, ·) = φ(·, ·)⊤bwh and the pessimism Γh is assigned to update
bQh ≈bTh bVh+1 −Γh, i.e. Bellman update + Pessimism (Line 10-12 in Algorithm 1)."
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06943268416596104,"Tighter Pessimistic Design. To improve the learning guarantee, we create a tighter penalty design
that includes bΛ−1
h
rather than ¯Σ−1
h
and an extra higher order O( 1"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.07027942421676546,K ) term:
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.07112616426756986,"Γh ←O
√"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.07197290431837426,"d · (φ(·, ·)⊤bΛ−1
h φ(·, ·))1/2
+ 2H3√"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.07281964436917866,"d
K
Note such a design admits no explicit factor in H in the main term (as opposed to Jin et al. (2021b))
therefore is the key for achieving adaptive/problem-dependent results (as we shall discuss later).
The full algorithm VAPVI is stated in Algorithm 1. In particular, we halve the ofﬂine data into two
independent parts with D = {(sτ
h, aτ
h, rτ
h, sτ′
h )}h∈[H]
τ∈[K] and D′ = {(¯sτ
h, ¯aτ
h, ¯rτ
h, ¯sτ′
h )}h∈[H]
τ∈[K] for different
purposes (estimating variance and updating Q-values)."
MAIN RESULT,0.07366638441998306,"3.2
MAIN RESULT"
MAIN RESULT,0.07451312447078746,"We denote quantities M1, M2, M3, M4 as in the notation list A. Then VAPVI provides the following
result. The complete proof is provided in Appendix C.
Theorem 3.2. Let K be the number of episodes. If K > max{M1, M2, M3, M4} and
√"
MAIN RESULT,0.07535986452159187,"d > ξ,"
MAIN RESULT,0.07620660457239628,"where ξ := supV ∈[0,H], s′∼Ph(s,a), h∈[H]"
MAIN RESULT,0.07705334462320068,"rh+V (s′)−(ThV )(s,a)"
MAIN RESULT,0.07790008467400508,"σV (s,a)"
MAIN RESULT,0.07874682472480948,". Then for any 0 < λ < κ, with"
MAIN RESULT,0.07959356477561388,"probability 1 −δ, for all policy π simultaneously, the output bπ of Algorithm 1 satisﬁes"
MAIN RESULT,0.08044030482641829,"vπ −vbπ ≤eO
 √ d · H
X"
MAIN RESULT,0.08128704487722269,"h=1
Eπ q"
MAIN RESULT,0.0821337849280271,"φ(·, ·)⊤Λ−1
h φ(·, ·)

+ 2H4√ d
K"
MAIN RESULT,0.0829805249788315,"where Λh = PK
k=1
φ(sk
h,ak
h)·φ(sk
h,ak
h)⊤"
MAIN RESULT,0.0838272650296359,"σ2
b
Vh+1(sk
h,ak
h)
+ λId. In particular, we have with probability 1 −δ,"
MAIN RESULT,0.0846740050804403,"v⋆−vbπ ≤eO
 √ d · H
X"
MAIN RESULT,0.0855207451312447,"h=1
Eπ⋆
q"
MAIN RESULT,0.0863674851820491,"φ(·, ·)⊤Λ⋆−1
h
φ(·, ·)

+ 2H4√"
MAIN RESULT,0.08721422523285352,"d
K
(3)"
MAIN RESULT,0.08806096528365792,"where Λ⋆
h = PK
k=1
φ(sk
h,ak
h)·φ(sk
h,ak
h)⊤"
MAIN RESULT,0.08890770533446232,"σ2
V ⋆
h+1(sk
h,ak
h)
+ λId and eO hides universal constants and the Polylog terms."
MAIN RESULT,0.08975444538526672,6The truncation used here is a standard treatment for making the estimator to be within the valid range.
MAIN RESULT,0.09060118543607112,Published as a conference paper at ICLR 2022
MAIN RESULT,0.09144792548687553,Algorithm 1 Variance-Aware Pessimistic Value Iteration (VAPVI)
MAIN RESULT,0.09229466553767993,"1: Input: Dataset D = {(sτ
h, aτ
h, rτ
h)}K,H
τ,h=1 D′ = {(¯sτ
h, ¯aτ
h, ¯rτ
h)}K,H
τ,h=1. Universal constant C."
MAIN RESULT,0.09314140558848434,"2: Initialization: Set bVH+1(·) ←0.
3: for h = H, H −1, . . . , 1 do
4:
⋄Phase1: Regular Least-square Value Iteration for conditional variances
5:
Set ¯Σh ←PK
τ=1 φ(¯sτ
h, ¯aτ
h)φ(¯sτ
h, ¯aτ
h)⊤+ λI
6:
Set ¯βh ←¯Σ−1
h
PK
τ=1 φ(¯sτ
h, ¯aτ
h) · bVh+1(¯sτ
h+1)2"
MAIN RESULT,0.09398814563928874,"7:
Set ¯θh ←¯Σ−1
h
PK
τ=1 φ(¯sτ
h, ¯aτ
h) · bVh+1(¯sτ
h+1)
8:
Set
d
Varh bVh+1

(·, ·) =

φ(·, ·), ¯βh

[0,(H−h+1)2] −

φ(·, ·), ¯θh"
MAIN RESULT,0.09483488569009314,"[0,H−h+1]
2"
MAIN RESULT,0.09568162574089754,"9:
Set bσh(·, ·)2 ←max{1, d
VarPh bVh+1(·, ·)}
10:
⋄Phase2: Weighted Least-square Value Iteration for pessimistic updates
11:
Set bΛh ←PK
τ=1 φ (sτ
h, aτ
h) φ (sτ
h, aτ
h)⊤/bσ2(sτ
h, aτ
h) + λ · I,"
MAIN RESULT,0.09652836579170194,"12:
Set bwh ←bΛ−1
h
PK
τ=1 φ (sτ
h, aτ
h) ·

rτ
h + bVh+1 (sτ
h+1)

/bσ2(sτ
h, aτ
h)
"
MAIN RESULT,0.09737510584250635,"13:
Set Γh(·, ·) ←C
√"
MAIN RESULT,0.09822184589331075,"d ·

φ(·, ·)⊤bΛ−1
h φ(·, ·)
1/2
+ 2H3√"
MAIN RESULT,0.09906858594411516,"d
K
(Use ΓI
h for the improved version)"
MAIN RESULT,0.09991532599491956,"14:
Set ¯Qh(·, ·) ←φ(·, ·)⊤bwh −Γh(·, ·)
15:
Set bQh(·, ·) ←min
 ¯Qh(·, ·), H −h + 1
	+"
MAIN RESULT,0.10076206604572396,"16:
Set bπh(· | ·) ←arg maxπh

 bQh(·, ·), πh(· | ·)"
MAIN RESULT,0.10160880609652836,"A, bVh(·) ←maxπh

 bQh(·, ·), πh(· | ·)"
MAIN RESULT,0.10245554614733277,"A
17: end for
18: Output: {bπh}H
h=1."
MAIN RESULT,0.10330228619813717,"Theorem 3.2 provides improvements over the existing best-known results and we now explain it.
However, before that, we ﬁrst discuss about our theorem condition."
MAIN RESULT,0.10414902624894158,"Comparing to Zhou et al. (2021a). In the online regime, Zhou et al. (2021a) is the ﬁrst result
that achieves optimal regret rate with O(dH
√"
MAIN RESULT,0.10499576629974598,"T) in the linear (mixture) MDPs. However, this
result requires the condition d ≥H (their Theorem 6 and Remark 7). In ofﬂine RL, VAPVI only
requires a milder condition
√"
MAIN RESULT,0.10584250635055038,"d > ξ comparing to d ≥H (since for any ﬁxed V ∈[0, H], the
standardized quantity r+V (s′)−(ThV )(s,a)"
MAIN RESULT,0.10668924640135478,"σV (s,a)
is bounded by constant with high probability, e.g. by
chebyshev inequality), which makes our result apply to a wider range of linear MDPs."
MAIN RESULT,0.10753598645215919,"Comparing to Jin et al. (2021b). Jin et al. (2021b) ﬁrst shows pessimistic value iteration (PEVI)
is provably efﬁcient for Linear MDPs in ofﬂine RL. VAPVI improves PEVI over O(
√"
MAIN RESULT,0.10838272650296359,"d) on
the feature dimension, and improves the horizon dependence as Λh ≽
1
H2 Σh implies Λ−1
h
≼
H2Σ−1
h . In addition, when instantiate to the tabular case, i.e. φ(s, a) = 1s,a, VAPVI gives O(
√ d P"
MAIN RESULT,0.10922946655376799,"h,s,a dπ⋆
h (s, a)
r"
MAIN RESULT,0.1100762066045724,"VarPs,a (r+V ⋆
h+1)"
MAIN RESULT,0.1109229466553768,"K·dµ
h(s,a)
), which enjoys O(
√"
MAIN RESULT,0.1117696867061812,H) improvement over PEVI (recall
MAIN RESULT,0.1126164267569856,Example 3.1) and the order O(H3/2) is tight (check Section G for the detailed derivation).
MAIN RESULT,0.11346316680779,"Comparing to Xie et al. (2021a). Their linear MDP guarantee in Theorem 3.2. enjoys the same
rate as VAPVI in feature dimension but the horizon dependence is essentially the same as Jin et al.
(2021b) (by translating H ≈O(
1
1−γ )) therefore is not optimal. The general function approximation
scheme in Xie et al. (2021a) provides elegant characterizations for on-support error and off-support
error, but the algorithmic framework is information-theoretical only (and the practical version PSPI
will not yield the same learning guarantee). Also, due to the use ﬁnite function class and policy class,
the reduction to linear MDP only works with ﬁnite action space. As a comparison, VAPVI has no
constraints on any of these."
MAIN RESULT,0.1143099068585944,"Comparing to Zanette et al. (2021). Concurrently, Zanette et al. (2021) considers ofﬂine RL with
the linear Bellman complete model, which is more general than linear MDPs and, with the assumption
Qπ ≤1, their PACLE algorithm provides near-minimax optimal guarantee in this setting. However,
when recovering to the standard setting Qπ ∈[0, H], their bound will rescale by an H factor,7 which
could be suboptimal due to the variance-unawareness. The reason behind this is: when Qπ ≤1,
lack of variance information encoding will not matter, since in this case VarP (V π) ≤1 has constant"
MAIN RESULT,0.11515664690939881,7Check their Footnote 2 in Page 9.
MAIN RESULT,0.11600338696020322,Published as a conference paper at ICLR 2022
MAIN RESULT,0.11685012701100762,"order (therefore will not affect the optimal rate); when Qπ ∈[0, H], VarP (V π) can be as large as
H2, effectively leveraging the variance information can help improve the sample efﬁciency, e.g. via
law of total variances, just like VAPVI does. On the other hand, their guarantee also requires ﬁnite
action space, due to the mirror descent style analysis. Nevertheless, we do point out Zanette et al.
(2021) has improved state-action measure than VAPVI, as ∥Eπ[φ(·, ·)]∥M −1 ≤Eπ[∥φ(·, ·)∥M −1] by
Jensen’s inequality and that norm ∥·∥M −1 is convex for some positive-deﬁnite matrix M."
MAIN RESULT,0.11769686706181202,"Adaptive characterization and faster convergence. Comparing to existing works, one major"
MAIN RESULT,0.11854360711261643,"improvement is that the main term for VAPVI
√"
MAIN RESULT,0.11939034716342083,"d PH
h=1 Eπ⋆q"
MAIN RESULT,0.12023708721422523,"φ(·, ·)⊤Λ⋆−1
h
φ(·, ·)

admits no
explicit dependence on H, which provides a more adaptive/instance-dependent characterization. For
instance, if we ignore the technical treatment by taking λ = 0 and σ⋆
h ≈VarP (V ⋆
h+1), then for the
partially deterministic systems (where there are t stochastic Ph’s and H −t deterministic Ph’s), the"
MAIN RESULT,0.12108382726502964,"main term diminishes to
√"
MAIN RESULT,0.12193056731583404,"d Pt
i=1 Eπ⋆q"
MAIN RESULT,0.12277730736663844,"φ(·, ·)⊤Λ⋆−1
hi φ(·, ·)

with hi ∈{h : s.t. Ph is stochastic}
and can be a much smaller quantity when t ≪H. Furthermore, for the fully deterministic system,
VAPVI automatically provides faster convergence rate O( 1"
MAIN RESULT,0.12362404741744284,"K ) from the higher order term, given that
the main term degenerates to 0. Those adaptive/instance-dependent features are not enjoyed by (Xie
et al., 2021a; Zanette et al., 2021), as they always provide the standard statistical rate O(
1
√"
MAIN RESULT,0.12447078746824725,"K ) (also
check Remark C.9 for a related discussion)."
MAIN RESULT,0.12531752751905165,"3.3
VAPVI-IMPROVED: FURTHER IMPROVEMENT IN STATE-ACTION DIMENSION"
MAIN RESULT,0.12616426756985605,"Can we further improve the VAPVI? Indeed, by deploying a carefully tuned tighter penalty, we are
able to further improve the state-action dependence if the feature is non-negative (φ ≥0). Concretely,
we replace the following ΓI
h in Algorithm 1 instead, and call the algorithm VAPVI-Improved (or
VAPVI-I for short). The proof can be found in Appendix D."
MAIN RESULT,0.12701100762066045,"ΓI
h(s, a) ←φ(s, a)⊤
bΛ−1
h K
X τ=1"
MAIN RESULT,0.12785774767146485,"φ (sτ
h, aτ
h) ·

rτ
h + bVh+1 (sτ
h+1) −

bTh bVh+1

(sτ
h, aτ
h)
"
MAIN RESULT,0.12870448772226925,"bσ2
h(sτ
h, aτ
h)"
MAIN RESULT,0.12955122777307368,+ eO(H3d/κ
MAIN RESULT,0.13039796782387808,"K
) (4)"
MAIN RESULT,0.13124470787468248,"Theorem 3.3. Suppose the feature is non-negative (φ ≥0). Let K be the number of episodes. If
K > max{M1, M2, M3, M4} and
√"
MAIN RESULT,0.13209144792548688,"d > ξ. Deploying ΓI
h (4) in Algorithm 1. Then for any
0 < λ < κ, with probability 1 −δ, for all policy π simultaneously, the output bπ of Algorithm 1
(VAPVI-I) satisﬁes"
MAIN RESULT,0.13293818797629128,"vπ −vbπ ≤eO
 √ d · H
X h=1 q"
MAIN RESULT,0.13378492802709568,"Eπ[φ(·, ·)]⊤Λ−1
h Eπ[φ(·, ·)]

+ eO(H4d/κ K
)"
MAIN RESULT,0.13463166807790009,"In particular, when choosing π = π⋆, the above guarantee holds true with Λ−1
h
replaced by Λ⋆−1
h
.
Here Λ−1
h , Λ⋆−1
h
, ξ are deﬁned the same as Theorem 3.2."
MAIN RESULT,0.1354784081287045,"Theorem 3.3 maintains nearly all the features of Theorem 3.2 (except higher order term is slightly
worse) and the dominate term evolves from Eπ ∥φ∥Λ−1
h
to ∥Eπ[φ]∥Λ−1
h . Clearly, the two bounds
differ by the magnitude of Jensen’s inequality. To provide a concrete view of how much improvement
is made, we check the parameter dependence in the context of tabular MDPs (where we ignore the
higher order term for conciseness). In particular, we compare the results under the single-policy
concentrability."
MAIN RESULT,0.1363251481795089,"Assumption 3.4 (Rashidinejad et al. (2021); Xie et al. (2021b)). There exists a optimal policy π⋆, s.t.
suph,s,a dπ⋆
h (s, a)/dµ
h(s, a) := C⋆< ∞, where dπ is the marginal state-action probability under π."
MAIN RESULT,0.1371718882303133,"In tabular RL, φ(s, a) = 1s,a and d = S · A (S, A be the ﬁnite state, action cardinality), then"
MAIN RESULT,0.1380186282811177,"Theorem 3.2 →
√ SA H
X h X"
MAIN RESULT,0.1388653683319221,"s,a
dπ⋆
h (s, a) s"
MAIN RESULT,0.1397121083827265,"VarPs,a(r + V ⋆
h+1)
K · dµ
h(s, a)
≤ r"
MAIN RESULT,0.14055884843353092,"H3C⋆S2A K
;"
MAIN RESULT,0.14140558848433532,"Theorem 3.3 →
√ SA H
X h"
MAIN RESULT,0.14225232853513972,"v
u
u
tX"
MAIN RESULT,0.14309906858594412,"s,a
dπ⋆
h (s, a)2 VarPs,a(r + V ⋆
h+1)
K · dµ
h(s, a)
≤ r"
MAIN RESULT,0.14394580863674852,"H3C⋆SA K
. (5)"
MAIN RESULT,0.14479254868755292,Published as a conference paper at ICLR 2022
MAIN RESULT,0.14563928873835733,"Theorem 3.3 enjoys a S state improvement over Theorem 3.2 and nearly recovers the minimax rate
q H3C⋆S"
MAIN RESULT,0.14648602878916173,"K
(Xie et al., 2021b). The detailed derivation can be found in Appendix G. Also, to show our
result is near-optimal, we provide the corresponding lower bound. The proof is in Appendix E.
Theorem 3.5 (Minimax lower bound). There exist a pair of universal constants c, c′ > 0 such that
given dimension d, horizon H and sample size K > c′d3, one can always ﬁnd a family of linear
MDP instances M such that (where Λ⋆
h = PK
k=1
φ(sk
h,ak
h)·φ(sk
h,ak
h)⊤"
MAIN RESULT,0.14733276883996613,"Varh(V ⋆
h+1)(sk
h,ak
h) satisﬁes (Λ⋆
h)−1 exists and"
MAIN RESULT,0.14817950889077053,"Varh(V ⋆
h+1)(sk
h, ak
h) > 0 ∀M ∈M)"
MAIN RESULT,0.14902624894157493,"inf
bπ
sup
M∈M
EM

v⋆−vbπ√ d · H
X h=1 q"
MAIN RESULT,0.14987298899237933,"Eπ⋆[φ]⊤(Λ⋆
h)−1Eπ⋆[φ]

≥c.
(6)"
MAIN RESULT,0.15071972904318373,Theorem 3.5 nearly matches the main term in VAPVI-I (Theorem 3.3) and certiﬁes it is near-optimal.
PROOF OVERVIEW,0.15156646909398813,"4
PROOF OVERVIEW"
PROOF OVERVIEW,0.15241320914479256,"Due to the space constraint, we could only provide a brief overview of the key proving ideas of the
theorems. We begin with Theorem 3.2. First, by the extended value difference lemma (Lemma H.7),
we can convert bounding the suboptimality gap of v⋆−vbπ to bounding PH
h=1 2 · Eπ [Γh(sh, ah)],
given that |(Th bVh+1 −bTh bVh+1)(s, a)| ≤Γh(s, a) for all s, a, h. To bound Th bVh+1 −bTh bVh+1, by
decomposing it reduces to bounding the key quantity"
PROOF OVERVIEW,0.15325994919559696,"φ(s, a)⊤bΛ−1
h  K
X"
PROOF OVERVIEW,0.15410668924640136,"τ=1
φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h)

/bσ2
h(sτ
h, aτ
h)

(7)"
PROOF OVERVIEW,0.15495342929720576,"The term is treated in two steps. First, we bound the gap of
σ2
bVh+1 −bσ2
h
 so we can convert bσ2
h to"
PROOF OVERVIEW,0.15580016934801016,"σ2
bVh+1. Next, since Var
h
rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h) | sτ
h, aτ
h
i
≈σ2
bVh+1, therefore by
the variance-weighted scheme in (equation 7), we can leverage the recent technical development
Bernstein inequality for self-normalized martingale (Lemma H.3) for acquiring the tight result, in
contrast to the previous treatment of Hoeffding inequality for self-normalized martingale + Covering.8
For the second part, one needs to further convert σ2
bVh+1 to σ⋆2
h (Λ−1
h
to Λ⋆−1
h
) with appropriate
concentrations. The proof of Theorem 3.3 is similar but with more complicated computations and
relies on using the linear representation of φ in ΓI
h (4), so that the expectation over π is inside the
square root by taking expectation over the linear representation at the beginning. The lower bound
proof uses a simple modiﬁcation of Zanette et al. (2021) which consists of the reduction from learning
to testing with Assouad’s method, and the use of standard information inequalities (e.g. from total
variation to KL divergence). For completeness, we provide the full proof in Appendix E."
DISCUSSION AND CONCLUSION,0.15664690939881457,"5
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.15749364944961897,"This work studies ofﬂine RL with linear MDP representation and contributes Variance Aware
Pessimistic Value Iteration (VAPVI) which adopts the conditional variance information of the value
function. VAPVI uses the estimated variances to reweight the Bellman residuals in the least-square
pessimistic value iteration and provides improved ofﬂine learning bounds over the existing best-known
results. VAPVI-I further improves over VAPVI in the state-action dimension and is near-minimax
optimal. One highlight of the theorems is that our learning bounds are expressed in terms of system
quantities, which automatically provide natural instance-dependent characterizations that previous
results are short of."
DISCUSSION AND CONCLUSION,0.15834038950042337,"On the other hand, while VAPVI/VAPVI-I close the existing gap from previous literature (Jin et al.,
2021b; Xie et al., 2021a), the optimal guarantee is in the minimax sense. Although our upper bounds
possess instance-dependent characterizations, the lower bound only holds true for a class of hard
instances. In this sense, whether “instance-dependent optimality” can be achieved remains elusive in
the current linear MDP setting (such a discussion is recently initiated in MAB problems (Xiao et al.,
2021)). We leave this as future work."
DISCUSSION AND CONCLUSION,0.15918712955122777,"8Variance-reweighting in (7) is important, since applying Bernstein inequality for self-normalized martingale
(Lemma H.3) without variance-reweighting cannot provide any improvement."
DISCUSSION AND CONCLUSION,0.16003386960203217,Published as a conference paper at ICLR 2022
DISCUSSION AND CONCLUSION,0.16088060965283657,ACKNOWLEDGMENTS
DISCUSSION AND CONCLUSION,0.16172734970364097,"The authors would like to thank Quanquan Gu for explaining Min et al. (2021) and introducing a
couple of related literatures. Ming Yin would like to thank Zhuoran Yang for the helpful suggestions
and Dan Qiao for a careful proofreading. Mengdi Wang gratefully acknowledges funding from Ofﬁce
of Naval Research (ONR) N00014-21-1-2288, Air Force Ofﬁce of Scientiﬁc Research (AFOSR)
FA9550-19-1-0203, and NSF 19-589, CMMI-1653435. Yu-Xiang Wang gratefully acknowledges
funding from National Science Foundation (NSF) #2007117 and #2003257."
REFERENCES,0.16257408975444537,REFERENCES
REFERENCES,0.1634208298052498,"Yasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pages 2312–2320, 2011."
REFERENCES,0.1642675698560542,"Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1–76, 2021."
REFERENCES,0.1651143099068586,"Sanae Amani, Christos Thrampoulidis, and Lin F Yang. Safe reinforcement learning with linear
function approximation. arXiv preprint arXiv:2106.06239, 2021."
REFERENCES,0.165961049957663,"Andras Antos, Remi Munos, and Csaba Szepesvari. Fitted q-iteration in continuous action-space
mdps. In Advances in Neural Information Processing Systems, pages 9–16, 2008a."
REFERENCES,0.1668077900084674,"Andr´as Antos, Csaba Szepesv´ari, and R´emi Munos. Learning near-optimal policies with bellman-
residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning, 71
(1):89–129, 2008b."
REFERENCES,0.1676545300592718,"Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pages
463–474. PMLR, 2020."
REFERENCES,0.1685012701100762,"Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efﬁcient exploration in policy optimiza-
tion. In International Conference on Machine Learning, pages 1283–1294. PMLR, 2020."
REFERENCES,0.1693480101608806,"Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In
International Conference on Machine Learning, pages 1042–1051, 2019."
REFERENCES,0.170194750211685,"Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel,
Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence
modeling. arXiv preprint arXiv:2106.01345, 2021a."
REFERENCES,0.1710414902624894,"Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player markov
games with linear function approximation. arXiv preprint arXiv:2102.07404, 2021b."
REFERENCES,0.1718882303132938,"Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: a survey. Internet
Mathematics, 3(1):79–127, 2006."
REFERENCES,0.1727349703640982,"Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efﬁcient rl with rich observations via latent state decoding. In International Conference
on Machine Learning, pages 1665–1674. PMLR, 2019."
REFERENCES,0.1735817104149026,"Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong
Wang. Bilinear classes: A structural framework for provable generalization in rl. International
Conference on Machine Learning, 2021."
REFERENCES,0.17442845046570704,"Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function
approximation. In International Conference on Machine Learning, pages 8334–8342, 2020."
REFERENCES,0.17527519051651144,"Yaqi Duan, Chi Jin, and Zhiyuan Li. Risk bounds and rademacher complexity in batch reinforcement
learning. International Conference on Machine Learning, 2021."
REFERENCES,0.17612193056731584,"Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503–556, 2005."
REFERENCES,0.17696867061812024,Published as a conference paper at ICLR 2022
REFERENCES,0.17781541066892464,"Dylan J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu. Ofﬂine reinforcement
learning: Fundamental barriers for value function approximation. arXiv preprint arXiv:2111.10919,
2021."
REFERENCES,0.17866215071972905,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pages 2052–2062. PMLR, 2019."
REFERENCES,0.17950889077053345,"Minbo Gao, Tianle Xie, Simon S Du, and Lin F Yang. A provably efﬁcient algorithm for linear
markov decision process with low switching cost. arXiv preprint arXiv:2101.00494, 2021."
REFERENCES,0.18035563082133785,"Geoffrey J Gordon. Approximate solutions to Markov decision processes. Carnegie Mellon University,
1999."
REFERENCES,0.18120237087214225,"Jiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with
linear function approximation. In International Conference on Machine Learning, pages 4171–
4180. PMLR, 2021."
REFERENCES,0.18204911092294665,"Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling
problem. arXiv preprint arXiv:2106.02039, 2021."
REFERENCES,0.18289585097375105,"Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex-
tual decision processes with low bellman rank are pac-learnable. In International Conference on
Machine Learning-Volume 70, pages 1704–1713, 2017."
REFERENCES,0.18374259102455545,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efﬁcient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pages 2137–2143.
PMLR, 2020."
REFERENCES,0.18458933107535985,"Chi Jin, Qinghua Liu, and Sobhan Miryooseﬁ. Bellman eluder dimension: New rich classes of rl
problems, and sample-efﬁcient algorithms. arXiv preprint arXiv:2102.00815, 2021a."
REFERENCES,0.18543607112616428,"Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efﬁcient for ofﬂine rl? In
International Conference on Machine Learning, pages 5084–5096. PMLR, 2021b."
REFERENCES,0.18628281117696868,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofﬂine reinforcement learning. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.18712955122777308,"Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Ofﬂine reinforcement learning with in-sample
q-learning. In International Conference on Learning Representations, 2022."
REFERENCES,0.18797629127857748,"Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Pac reinforcement learning with rich
observations. Advances in neural information processing systems, 2016."
REFERENCES,0.18882303132938189,"Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.1896697713801863,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.1905165114309907,"Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement
learning, pages 45–73. Springer, 2012."
REFERENCES,0.1913632514817951,"Tor Lattimore and Csaba Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.1922099915325995,"Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Interna-
tional Conference on Machine Learning, pages 3703–3712, 2019."
REFERENCES,0.1930567315834039,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tutorial,
review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.1939034716342083,"Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch reinforce-
ment learning without great exploration. arXiv preprint arXiv:2007.08202, 2020."
REFERENCES,0.1947502116850127,Published as a conference paper at ICLR 2022
REFERENCES,0.1955969517358171,"Zhihan Liu, Yufeng Zhang, Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Provably efﬁcient genera-
tive adversarial imitation learning for online and ofﬂine setting with linear function approximation.
arXiv preprint arXiv:2108.08765, 2021."
REFERENCES,0.1964436917866215,"Yifei Min, Tianhao Wang, Dongruo Zhou, and Quanquan Gu. Variance-aware off-policy evaluation
with linear function approximation. Advances in neural information processing systems, 2021."
REFERENCES,0.19729043183742592,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.19813717188823032,"Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artiﬁcial
Intelligence and Statistics, pages 2010–2020. PMLR, 2020."
REFERENCES,0.19898391193903472,"R´emi Munos. Error bounds for approximate policy iteration. In ICML, volume 3, pages 560–567,
2003."
REFERENCES,0.19983065198983913,"Thanh Nguyen-Tang, Sunil Gupta, Hung Tran-The, and Svetha Venkatesh. On ﬁnite-sample analysis
of ofﬂine reinforcement learning with deep relu networks. arXiv preprint arXiv:2103.06671, 2021."
REFERENCES,0.20067739204064353,"Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging ofﬂine rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021."
REFERENCES,0.20152413209144793,"Tongzheng Ren, Jialian Li, Bo Dai, Simon S Du, and Sujay Sanghavi. Nearly horizon-free ofﬂine
reinforcement learning. Advances in neural information processing systems, 2021."
REFERENCES,0.20237087214225233,"Paul D Sampson and Peter Guttorp. Nonparametric estimation of nonstationary spatial covariance
structure. Journal of the American Statistical Association, 87(417):108–119, 1992."
REFERENCES,0.20321761219305673,"Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic q-learning for ofﬂine
reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890,
2022."
REFERENCES,0.20406435224386113,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354–359, 2017."
REFERENCES,0.20491109229466553,"Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on learning theory, pages 2898–2933. PMLR, 2019."
REFERENCES,0.20575783234546993,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.20660457239627433,"Csaba Szepesv´ari and R´emi Munos. Finite time bounds for sampling based ﬁtted value iteration. In
Proceedings of the 22nd international conference on Machine learning, pages 880–887, 2005."
REFERENCES,0.20745131244707873,"Mohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-aware regret bounds for undis-
counted reinforcement learning in mdps. In Algorithmic Learning Theory, pages 770–805. PMLR,
2018."
REFERENCES,0.20829805249788316,"Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389–434, 2012."
REFERENCES,0.20914479254868756,"Masatoshi Uehara and Wen Sun. Pessimistic model-based ofﬂine rl: Pac bounds and posterior
sampling under partial coverage. arXiv preprint arXiv:2107.06226, 2021."
REFERENCES,0.20999153259949196,"Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free reinforcement
learning with linear function approximation. Advances in neural information processing systems,
2020."
REFERENCES,0.21083827265029637,"Ruosong Wang, Dean P Foster, and Sham M Kakade. What are the statistical limits of ofﬂine rl with
linear function approximation? International Conference on Learning Representations, 2021a."
REFERENCES,0.21168501270110077,Published as a conference paper at ICLR 2022
REFERENCES,0.21253175275190517,"Yining Wang, Ruosong Wang, Simon Shaolei Du, and Akshay Krishnamurthy. Optimism in rein-
forcement learning with generalized linear function approximation. In International Conference
on Learning Representations, 2021b."
REFERENCES,0.21337849280270957,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.21422523285351397,"Chenjun Xiao, Yifan Wu, Jincheng Mei, Bo Dai, Tor Lattimore, Lihong Li, Csaba Szepesvari, and
Dale Schuurmans. On the optimality of batch policy optimization algorithms. In International
Conference on Machine Learning, pages 11362–11371. PMLR, 2021."
REFERENCES,0.21507197290431837,"Tengyang Xie and Nan Jiang. Q* approximation schemes for batch reinforcement learning: A
theoretical comparison. In Uncertainty in Artiﬁcial Intelligence, pages 550–559, 2020."
REFERENCES,0.21591871295512277,"Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for ofﬂine reinforcement learning. Advances in neural information processing systems,
2021a."
REFERENCES,0.21676545300592717,"Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy ﬁnetuning: Bridg-
ing sample-efﬁcient ofﬂine and online reinforcement learning. Advances in neural information
processing systems, 2021b."
REFERENCES,0.21761219305673157,"Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.
In International Conference on Machine Learning, pages 6995–7004. PMLR, 2019."
REFERENCES,0.21845893310753597,"Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pages 10746–10756. PMLR,
2020."
REFERENCES,0.2193056731583404,"Ming Yin and Yu-Xiang Wang. Asymptotically efﬁcient off-policy evaluation for tabular rein-
forcement learning. In International Conference on Artiﬁcial Intelligence and Statistics, pages
3948–3958. PMLR, 2020."
REFERENCES,0.2201524132091448,"Ming Yin and Yu-Xiang Wang. Optimal uniform ope and model-based ofﬂine reinforcement learning
in time-homogeneous, reward-free and task-agnostic settings. Advances in neural information
processing systems, 2021a."
REFERENCES,0.2209991532599492,"Ming Yin and Yu-Xiang Wang. Towards instance-optimal ofﬂine reinforcement learning with
pessimism. Advances in neural information processing systems, 2021b."
REFERENCES,0.2218458933107536,"Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in ofﬂine policy
evaluation for reinforcement learning. In International Conference on Artiﬁcial Intelligence and
Statistics, pages 1567–1575. PMLR, 2021."
REFERENCES,0.222692633361558,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and
Tengyu Ma. Mopo: Model-based ofﬂine policy optimization. arXiv preprint arXiv:2005.13239,
2020."
REFERENCES,0.2235393734123624,"Andrea Zanette. Exponential lower bounds for batch reinforcement learning: Batch rl can be
exponentially harder than online rl. International Conference on Machine Learning, 2021."
REFERENCES,0.2243861134631668,"Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on
Machine Learning, pages 7304–7312. PMLR, 2019."
REFERENCES,0.2252328535139712,"Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near
optimal policies with low inherent bellman error. In International Conference on Machine Learning,
pages 10978–10989. PMLR, 2020."
REFERENCES,0.2260795935647756,"Andrea Zanette, Martin J. Wainwright, and Emma Brunskill. Provable beneﬁts of actor-critic methods
for ofﬂine reinforcement learning, 2021."
REFERENCES,0.22692633361558,"Zihan Zhang, Jiaqi Yang, Xiangyang Ji, and Simon S Du. Variance-aware conﬁdence set: Variance-
dependent bound for linear bandits and horizon-free bound for linear mixture mdp. arXiv preprint
arXiv:2101.12745, 2021."
REFERENCES,0.2277730736663844,Published as a conference paper at ICLR 2022
REFERENCES,0.2286198137171888,"Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement
learning for linear mixture markov decision processes. In Conference on Learning Theory, pages
4532–4576. PMLR, 2021a."
REFERENCES,0.22946655376799321,"Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efﬁcient reinforcement learning for discounted
mdps with feature mapping. In International Conference on Machine Learning, pages 12793–
12802. PMLR, 2021b."
REFERENCES,0.23031329381879762,Published as a conference paper at ICLR 2022
REFERENCES,0.23116003386960204,Appendix
REFERENCES,0.23200677392040644,"A
NOTATION LIST"
REFERENCES,0.23285351397121085,"Σp
h
Eµ,h

φ(s, a)φ(s, a)⊤"
REFERENCES,0.23370025402201525,"Λp
h
Eµ,h

σVh+1(s, a)−2φ(s, a)φ(s, a)⊤"
REFERENCES,0.23454699407281965,"κ
minh λmin(Σp
h)"
REFERENCES,0.23539373412362405,"ι
minh λmin(Λp
h) ≥κ/H2 for any Vh"
REFERENCES,0.23624047417442845,"σ2
V (s, a)
max{1, VarPh(V )(s, a)} for any V"
REFERENCES,0.23708721422523285,"σ2
Vh+1(s, a)
max{1, VarPh(Vh+1)(s, a)}"
REFERENCES,0.23793395427603725,"bσ2
Vh(s, a)
max{1, d
VarhVh+1(s, a)}
¯Σh
PK
τ=1 φ(¯sτ
h, ¯aτ
h)φ(¯sτ
h, ¯aτ
h)⊤+ λId
bΛh
PK
k=1 φ(sk
h, ak
h)φ(sk
h, ak
h)⊤/bσ2
h(sk
h, ak
h) + λId"
REFERENCES,0.23878069432684165,"M1
max{2λ, 128 log(2d/δ), 128H4 log(2d/δ)/κ2}"
REFERENCES,0.23962743437764605,"M2
max{
λ2
κ log((λ+K)H/λδ), 962H12d log((λ + K)H/λδ)/κ5}"
REFERENCES,0.24047417442845045,"M3
max

512H4/κ2 log
  2d"
REFERENCES,0.24132091447925486,"δ

, 4λH2/κ"
REFERENCES,0.24216765453005928,"M4
12
p"
REFERENCES,0.24301439458086369,H4d log((λ + K)H/λδ)/κ
REFERENCES,0.2438611346316681,"δ
Failure probability"
REFERENCES,0.2447078746824725,"ξ
supV ∈[0,H], s′∼Ph(s,a), h∈[H]"
REFERENCES,0.2455546147332769,"rh+V (s′)−(ThV )(s,a)"
REFERENCES,0.2464013547840813,"σV (s,a) "
REFERENCES,0.2472480948348857,"CH,d,κ,K
36
r H4d3"
REFERENCES,0.2480948348856901,"κ
log

(λ+K)2KdH2"
REFERENCES,0.2489415749364945,"λδ

+ 12λ H2√ d
κ"
REFERENCES,0.2497883149872989,"B
EXTENDED LITERATURE REVIEW"
REFERENCES,0.2506350550381033,"B.1
LINEAR MODEL REPRESENTATION AND ITS EXTENSION IN ONLINE RL"
REFERENCES,0.2514817950889077,"There are numerous works in online RL that study linear model representations. Yang and Wang
(2019; 2020); Jin et al. (2020) propose Linear MDP, which assumes the transition kernel and the
reward are linear in given features. Cai et al. (2020); Ayoub et al. (2020); Modi et al. (2020); Zhou
et al. (2021b) propose Linear mixture MDP, which assumes the transition probability is a linear
combination of some base kernels. Linear Bellman Complete model (Zanette et al., 2020) generalizes
linear MDP model by allowing linear functions to approximate the Q-function and the function class
is closed under the Bellman update. The notion of low Bellman rank (Jiang et al., 2017) subsumes
not only linear MDPs but also models including linear quadratic regulator (LQR), Reactive POMDP
(Krishnamurthy et al., 2016) and Block MDP (Du et al., 2019). There are also other models, e.g.
factored MDP (Sun et al., 2019), Bellman Eluder dimension (Jin et al., 2021a) and Bilinear class (Du
et al., 2021). With the linear MDP model itself, there are also fruitful extensions, e.g. gap-dependent
analysis with logarithmic regret (He et al., 2021), low-switching cost RL (Gao et al., 2021), safe RL
(Amani et al., 2021), reward-free RL (Wang et al., 2020), generalized linear model (GLM) (Wang
et al., 2021b), two-player markov game (Chen et al., 2021b) and generative adversarial learning (Liu
et al., 2021). In particular, Zhou et al. (2021a) shows UCRL-VTR+ is near-minimax optimal when
feature dimension d ≥H."
REFERENCES,0.2523285351397121,"B.2
EXISTING RESULTS IN OFFLINE RL WITH MODEL REPRESENTATIONS"
REFERENCES,0.2531752751905165,"Ofﬂine RL with general function representations. The ﬁnite sample analysis of ofﬂine RL with
function approximation is initially conducted by Fitted Q-Iteration (FQI) type algorithms and can
be dated back to (Munos, 2003; Szepesv´ari and Munos, 2005; Antos et al., 2008a;b). Later, Chen
and Jiang (2019); Le et al. (2019); Xie and Jiang (2020) follow this line of research and derive the"
REFERENCES,0.2540220152413209,Published as a conference paper at ICLR 2022
REFERENCES,0.2548687552921253,"improved learning results. However, owing to the aim for tackling general function approximation,
those learning bounds are expressed in terms of the stringent concentrability coefﬁcients (therefore,
are less adaptive to individual instances) and are usually only information-theoretical, due to the
computational intractability of the optimization procedure over the general function classes. Other
works impose weaker assumptions (e.g. partial coverage (Liu et al., 2020; Kidambi et al., 2020;
Uehara and Sun, 2021)), and their ﬁnite sample analysis are generally suboptimal in terms of H or
the effective horizon (1 −γ)−1."
REFERENCES,0.2557154953429297,"Ofﬂine RL with tabular models. For tabular MDPs, tight learning bounds can be achieved under
several data-coverage assumptions. For the class of problems with uniform data-visitation measure
dm, the near-optimal sample complexity bound has the rate O(H3/dmϵ2) for time-inhomogeneous
MDPs (Yin et al., 2021) and O(H2/dmϵ) for time-homogeneous MDPs (Yin and Wang, 2021a; Ren
et al., 2021). Under the single concentrability assumption, the tight rate O(H3SC⋆/ϵ2) is obtained
by Xie et al. (2021b). In particular, the recent study Yin and Wang (2021b) introduces the intrinsic
ofﬂine learning bound that is not only instance-dependent but also subsumes previous optimal results.
More recently, Shi et al. (2022) uses model-free approach to achieve minimax rate with a larger
ϵ-range."
REFERENCES,0.25656223539373413,"Ofﬂine RL with linear model representations. Recently, there are more focus on studying the
provable efﬁcient ofﬂine RL under the linear model representations. Jin et al. (2021b) ﬁrst shows
ofﬂine RL with linear MDP is provably efﬁcient by the pessimistic value iteration (PEVI), which
is an ofﬂine counterpart of LSVI-UCB in Jin et al. (2020). Their analysis deviates from their lower
bound by a factor of d · H (check their Theorem 4.4 and 4.6). Later, Xie et al. (2021a) considers
function approximation under the Bellman-consistent assumptions, and, when realized to linear MDP
setting, improve the sample complexity guarantee of Jin et al. (2021b) by a order O(d) (Theorem 3.2).
However, their improvement only holds for ﬁnite action space (due to the dependence log |A|) and
by the direct reduction (from Theorem 3.1) their result does not imply a computationally tractable
algorithm. In addition, there is no improvement on the horizon dependence. Concurrently, Zanette
et al. (2021) considers the Linear Bellman Complete model (which originates from its online version
Zanette et al. (2020)) and designs the actor-critic style algorithm that achieves tight result under
the assumption that the value function is bounded by 1. While their algorithm is efﬁcient (which is
based on solving a sequence of second-order cone programs), the resulting learning bound requires
the action space to be ﬁnite due to the mirror descent/natural policy gradient updates in the Actor
procedure (Agarwal et al., 2021). Besides, assuming the value function to be less than 1 simpliﬁes
the challenges in dealing with horizon H since when rescale their result to [0, H], there is a H factor
blow-up, which makes no improvement in the horizon dependence comparing to Jin et al. (2021b).
On the other hand, Wang et al. (2021a); Zanette (2021) study the statistical hardness of ofﬂine RL
with linear representations by prooﬁng the exponential lower bounds. As a result, none of the existing
algorithms can achieve the statistical limit for the well-structured linear MDP model with the general
(inﬁnite or continuous) state-action spaces in the ofﬂine regime."
REFERENCES,0.2574089754445385,"C
PROOFS IN SECTION 3.2"
REFERENCES,0.25825571549534293,"Instead of prooﬁng the result for v⋆−vbπ, in most parts of the proof we deal with V ⋆
1 −V bπ
1 , which is
more general."
REFERENCES,0.25910245554614736,"C.1
SOME PREPARATIONS"
REFERENCES,0.25994919559695173,"Deﬁne the Bellman update error ζh(s, a) := (Th bVh+1)(s, a) −bQh(s, a) and recall bπh(s) =
arg maxπh⟨bQh(s, ·), πh(· | s)⟩A, then by the direct application of Lemma H.8"
REFERENCES,0.26079593564775616,"V π
1 (s) −V bπ
1 (s) ≤ H
X"
REFERENCES,0.26164267569856053,"h=1
Eπ [ζh(sh, ah) | s1 = s] − H
X"
REFERENCES,0.26248941574936496,"h=1
Ebπ [ζh(sh, ah) | s1 = s] .
(8)"
REFERENCES,0.26333615580016934,"The next lemma shows it is sufﬁcient to bound the pessimistic penalty, which is the key in the proof."
REFERENCES,0.26418289585097376,"Lemma C.1. Suppose with probability 1 −δ, it holds for all h, s, a ∈[H] × S × A that |(Th bVh+1 −
bTh bVh+1)(s, a)| ≤Γh(s, a), then it implies ∀s, a, h ∈S × A × [H], 0 ≤ζh(s, a) ≤2Γh(s, a)."
REFERENCES,0.26502963590177814,Published as a conference paper at ICLR 2022
REFERENCES,0.26587637595258257,"Furthermore, it holds for any policy π simultaneously, with probability 1 −δ,"
REFERENCES,0.26672311600338694,"V π
1 (s) −V bπ
1 (s) ≤ H
X"
REFERENCES,0.26756985605419137,"h=1
2 · Eπ [Γh(sh, ah) | s1 = s] ."
REFERENCES,0.26841659610499574,"Proof of Lemma C.1. We ﬁrst show given |(Th bVh+1 −bTh bVh+1)(s, a)| ≤Γh(s, a), then 0 ≤
ζh(s, a) ≤2Γh(s, a), ∀s, a, h ∈S × A × [H]."
REFERENCES,0.26926333615580017,"Step1: we ﬁrst show 0 ≤ζh(s, a), ∀s, a, h ∈S × A × [H]."
REFERENCES,0.2701100762066046,"Indeed, if ¯Qh(s, a) ≤0, then by deﬁnition bQh(s, a) = 0 and in this case ζh(s, a) :=
(Th bVh+1)(s, a) −bQh(s, a) = (Th bVh+1)(s, a) ≥0; if ¯Qh(s, a) > 0, then bQh(s, a) ≤¯Qh(s, a)
and"
REFERENCES,0.270956816257409,"ζh(s, a) :=(Th bVh+1)(s, a) −bQh(s, a) ≥(Th bVh+1)(s, a) −¯Qh(s, a)"
REFERENCES,0.2718035563082134,"=(Th bVh+1)(s, a) −(bTh bVh+1)(s, a) + Γh(s, a) ≥0."
REFERENCES,0.2726502963590178,"Step2: next we show ζh(s, a) ≤2Γh(s, a), ∀s, a, h ∈S × A × [H]."
REFERENCES,0.2734970364098222,"Indeed, we have bQh(s, a) = max( ¯Qh(s, a), 0) and this is because: ¯Qh(x, a) = (bTh bVh+1)(x, a) −
Γh(x, a) ≤(Th bVh+1)(x, a) ≤H −h + 1. Therefore, in this case we have:"
REFERENCES,0.2743437764606266,"ζh(s, a) :=(Th bVh+1)(s, a) −bQh(s, a) ≤(Th bVh+1)(s, a) −¯Qh(s, a)"
REFERENCES,0.275190516511431,"=(Th bVh+1)(s, a) −(bTh bVh+1)(s, a) + Γh(s, a) ≤2 · Γh(s, a)."
REFERENCES,0.2760372565622354,"For the last statement, denote F := {0 ≤ζh(s, a) ≤2Γh(s, a), ∀s, a, h ∈S × A × [H]}. Note
conditional on F, then by equation 8, V π
1 (s) −V bπ
1 (s) ≤PH
h=1 2 · Eπ[Γh(sh, ah) | s1 = s] holds for
any policy π almost surely. Therefore, P """
REFERENCES,0.2768839966130398,"∀π, V π
1 (s) −V bπ
1 (s) ≤ H
X"
REFERENCES,0.2777307366638442,"h=1
2 · Eπ[Γh(sh, ah) | s1 = s]. # =P """
REFERENCES,0.2785774767146486,"∀π, V π
1 (s) −V bπ
1 (s) ≤ H
X"
REFERENCES,0.279424216765453,"h=1
2 · Eπ[Γh(sh, ah) | s1 = s] F #"
REFERENCES,0.2802709568162574,"· P[F] +P """
REFERENCES,0.28111769686706184,"∀π, V π
1 (s) −V bπ
1 (s) ≤ H
X"
REFERENCES,0.2819644369178662,"h=1
2 · Eπ[Γh(sh, ah) | s1 = s] Fc
#"
REFERENCES,0.28281117696867064,"· P[Fc] ≥P """
REFERENCES,0.283657917019475,"∀π, V π
1 (s) −V bπ
1 (s) ≤ H
X"
REFERENCES,0.28450465707027944,"h=1
2 · Eπ[Γh(sh, ah) | s1 = s] F #"
REFERENCES,0.2853513971210838,"· P[F] ≥1 · P[F] ≥1 −δ,"
REFERENCES,0.28619813717188824,which ﬁnishes the proof.
REFERENCES,0.2870448772226926,"C.2
BOUNDING
(Th bVh+1)(s, a) −(bTh bVh+1)(s, a)
."
REFERENCES,0.28789161727349705,"By Lemma C.1, it remains to bound |(Th bVh+1)(s, a)−(bTh bVh+1)(s, a)|. Suppose wh is the coefﬁcient
corresponding to the Th bVh+1 (such wh exists by Lemma H.9), i.e. Th bVh+1 = φ⊤wh, and recall"
REFERENCES,0.2887383573243014,Published as a conference paper at ICLR 2022
REFERENCES,0.28958509737510585,"(bTh bVh+1)(s, a) = φ(s, a)⊤bwh, then:"
REFERENCES,0.2904318374259102,"
Th bVh+1

(s, a) −

bTh bVh+1

(s, a) = φ(s, a)⊤(wh −bwh)"
REFERENCES,0.29127857747671465,"=φ(s, a)⊤wh −φ(s, a)⊤bΛ−1
h K
X"
REFERENCES,0.292125317527519,"τ=1
φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

/bσ2
h(sτ
h, aτ
h) !"
REFERENCES,0.29297205757832345,"= φ(s, a)⊤wh −φ(s, a)⊤bΛ−1
h K
X"
REFERENCES,0.2938187976291279,"τ=1
φ (sτ
h, aτ
h) ·

Th bVh+1

(sτ
h, aτ
h) /bσ2
h(sτ
h, aτ
h) !"
REFERENCES,0.29466553767993225,"|
{z
}
(i)"
REFERENCES,0.2955122777307367,"+ φ(s, a)⊤bΛ−1
h K
X"
REFERENCES,0.29635901778154106,"τ=1
φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h)

/bσ2
h(sτ
h, aτ
h) !"
REFERENCES,0.2972057578323455,"|
{z
}
(ii) . (9)"
REFERENCES,0.29805249788314986,The term (i) is dealt by the following lemma.
REFERENCES,0.2988992379339543,"Lemma C.2. Recall κ in Assumption 2.2. Suppose K ≥max

512H4/κ2 log
  2d"
REFERENCES,0.29974597798475866,"δ

, 4λH2/κ
	
,
then with probability 1 −δ, for all s, a, h ∈S × A × [H]"
REFERENCES,0.3005927180355631,"φ(s, a)⊤wh −φ(s, a)⊤bΛ−1
h K
X"
REFERENCES,0.30143945808636746,"τ=1
φ (sτ
h, aτ
h) ·

Th bVh+1

(sτ
h, aτ
h) /bσ2(sτ
h, aτ
h)"
REFERENCES,0.3022861981371719,! ≤2λH3√
REFERENCES,0.30313293818797626,"d/κ
K
."
REFERENCES,0.3039796782387807,"Proof. Recall Th bVh+1 = φ⊤wh and apply Lemma H.6, we obtain with probability 1 −δ, for all
s, a, h ∈S × A × [H],"
REFERENCES,0.3048264182895851,"φ(s, a)⊤wh −φ(s, a)⊤bΛ−1
h K
X"
REFERENCES,0.3056731583403895,"τ=1
φ (sτ
h, aτ
h) ·

Th bVh+1

(sτ
h, aτ
h) /bσ2(sτ
h, aτ
h) !"
REFERENCES,0.3065198983911939,"=φ(s, a)⊤wh −φ(s, a)⊤bΛ−1
h K
X"
REFERENCES,0.3073666384419983,"τ=1
φ (sτ
h, aτ
h) · φ(sτ
h, aτ
h)⊤wh/bσ2(sτ
h, aτ
h) !"
REFERENCES,0.3082133784928027,"=φ(s, a)⊤wh −φ(s, a)⊤bΛ−1
h

bΛh −λI

wh = λ · φ(s, a)⊤bΛ−1
h wh"
REFERENCES,0.3090601185436071,"≤λ ∥φ(s, a)∥bΛ−1
h · ∥wh∥bΛ−1
h
≤λ"
REFERENCES,0.3099068585944115,"K ∥φ(s, a)∥(˜Λp
h)−1 · ∥wh∥(˜Λp
h)−1 ≤λ K 1 ·"
REFERENCES,0.3107535986452159,"r(˜Λp
h)−1
 · 2H
√ d ·"
REFERENCES,0.31160033869602033,"r(˜Λp
h)−1"
REFERENCES,0.3124470787468247,"where ˜Λp
h := Eµ,h

bσh(s, a)−2φ(s, a)φ(s, a)⊤
and the second inequality is by Lemma H.6 (with
φ′ = φ/bσh and ∥φ/bσh∥≤∥φ∥≤1 := C) and the third inequality uses
√"
REFERENCES,0.31329381879762913,"a⊤· A · a ≤
p"
REFERENCES,0.3141405588484335,"∥a∥2 ∥A∥2 ∥a∥2 = ∥a∥2
p"
REFERENCES,0.31498729889923793,"∥A∥2 with a to be either φ or wh.
Moreover, λmin(˜Λp
h) ≥"
REFERENCES,0.31583403895004236,"κ/ maxh,s,a bσh(s, a)2 ≥κ/H2 implies
(˜Λp
h)−1 ≤H2/κ, therefore for all s, a, h ∈S × A × [H],
with probability 1 −δ"
REFERENCES,0.31668077900084673,"φ(s, a)⊤wh −φ(s, a)⊤bΛ−1
h K
X"
REFERENCES,0.31752751905165116,"τ=1
φ (sτ
h, aτ
h) ·

Th bVh+1

(sτ
h, aτ
h) /bσ2(sτ
h, aτ
h)"
REFERENCES,0.31837425910245554,! ≤2λH3√
REFERENCES,0.31922099915325997,"d/κ
K
."
REFERENCES,0.32006773920406434,Published as a conference paper at ICLR 2022
REFERENCES,0.32091447925486877,"For term (ii), denote: xτ = φ(sτ
h,aτ
h)
bσ(sτ
h,aτ
h),
ητ =

rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h)

/bσ(sτ
h, aτ
h),
then by Cauchy inequality it follows"
REFERENCES,0.32176121930567314,"φ(s, a)⊤bΛ−1
h K
X"
REFERENCES,0.32260795935647757,"τ=1
φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h)

/bσ2
h(sτ
h, aτ
h) ! ≤
q"
REFERENCES,0.32345469940728194,"φ(s, a)⊤bΛ−1
h φ(s, a) · || K
X"
REFERENCES,0.32430143945808637,"τ=1
xτητ||bΛ−1
h
(10)"
REFERENCES,0.32514817950889074,"C.2.1
ANALYZING THE TERM
q"
REFERENCES,0.3259949195596952,"φ(s, a)bΛ−1
h φ(s, a)"
REFERENCES,0.3268416596104996,"Recall (in Theorem 3.2) the estimated bΛh = PK
τ=1 φ (sτ
h, aτ
h) φ (sτ
h, aτ
h)⊤/bσ2(sτ
h, aτ
h) + λ · I and
Λh = PK
τ=1 φ(sτ
h, aτ
h)⊤φ(sτ
h, aτ
h)/σ2
bVh+1(sτ
h, aτ
h) + λI. Then we have the following lemma to"
REFERENCES,0.327688399661304,"control the term
q"
REFERENCES,0.3285351397121084,"φ(s, a)bΛ−1
h φ(s, a)."
REFERENCES,0.3293818797629128,"Lemma C.3. Denote the quantities C1 = max{2λ, 128 log(2d/δ), 128H4 log(2d/δ)/κ2} and
C2 = max{
λ2
κ log((λ+K)H/λδ), 962H12d log((λ + K)H/λδ)/κ5}. Suppose the number of episode
K satisﬁes K > max{C1, C2}, then with probability 1 −δ, q"
REFERENCES,0.3302286198137172,"φ(s, a)bΛ−1
h φ(s, a) ≤2
q"
REFERENCES,0.3310753598645216,"φ(s, a)Λ−1
h φ(s, a),
∀s, a ∈S × A."
REFERENCES,0.331922099915326,"Proof of Lemma C.3. By deﬁnition
q"
REFERENCES,0.3327688399661304,"φ(s, a)bΛ−1
h φ(s, a) = ∥φ(s, a)∥bΛ−1
h . Then denote"
REFERENCES,0.3336155800169348,"bΛ′
h = 1"
REFERENCES,0.3344623200677392,"K
bΛh,
Λ′
h = 1 K Λh,"
REFERENCES,0.3353090601185436,"where Λh = PK
τ=1 φ(sτ
h, aτ
h)⊤φ(sτ
h, aτ
h)/σ2
bVh+1(sτ
h, aτ
h) + λI.
Under the condition of K, by
Lemma C.7, with probability 1 −δ"
REFERENCES,0.336155800169348,"bΛ′
h −Λ′
h
 ≤sup
s,a"
REFERENCES,0.3370025402201524,"φ(s, a)φ(s, a)⊤"
REFERENCES,0.33784928027095684,"bσ2
h(s, a)
−φ(s, a)φ(s, a)⊤"
REFERENCES,0.3386960203217612,"σ2
bVh+1(s, a) "
REFERENCES,0.33954276037256564,"≤sup
s,a "
REFERENCES,0.34038950042337,"bσ2
h(s, a) −σ2
bVh+1(s, a)"
REFERENCES,0.34123624047417445,"bσ2
h(s, a)σ2
bVh+1(s, a)"
REFERENCES,0.3420829805249788,"· ∥φ(s, a)∥2 ≤sup
s,a "
REFERENCES,0.34292972057578325,"bσ2
h(s, a) −σ2
bVh+1(s, a) 1 · 1 ≤12 s H4d"
REFERENCES,0.3437764606265876,"κK log
(λ + K)H λδ"
REFERENCES,0.34462320067739205,"
+ 12λH2√"
REFERENCES,0.3454699407281964,"d
κK
. (11)"
REFERENCES,0.34631668077900085,"Next by Lemma H.5 (with φ to be φ/σbVh+1 and C = 1), it holds with probability 1 −δ,"
REFERENCES,0.3471634208298052,"Λ′
h −

Eµ,h[φ(s, a)φ(s, a)⊤/σ2
bVh+1(s, a)] + λ K Id"
REFERENCES,0.34801016088060965," ≤4
√ 2
√ K"
REFERENCES,0.3488569009314141,"
log 2d δ"
REFERENCES,0.34970364098221846,"1/2
."
REFERENCES,0.3505503810330229,Published as a conference paper at ICLR 2022
REFERENCES,0.35139712108382726,"Therefore
by
Weyl’s
spectrum
theorem
and
the
condition
K
>
max{2λ, 128 log(2d/δ), 128H4 log(2d/δ)/κ2}, the above implies"
REFERENCES,0.3522438611346317,"∥Λ′
h∥=λmax(Λ′
h) ≤λmax

Eµ,h[φ(s, a)φ(s, a)⊤/σ2
bVh+1(s, a)]

+ λ"
REFERENCES,0.35309060118543606,"K + 4
√ 2
√ K"
REFERENCES,0.3539373412362405,"
log 2d δ 1/2"
REFERENCES,0.35478408128704486,"=
Eµ,h[φ(s, a)φ(s, a)⊤/σ2
bVh+1(s, a)]

2 + λ"
REFERENCES,0.3556308213378493,"K + 4
√ 2
√ K"
REFERENCES,0.35647756138865366,"
log 2d δ 1/2"
REFERENCES,0.3573243014394581,"≤∥φ(s, a)∥2 + λ"
REFERENCES,0.35817104149026247,"K + 4
√ 2
√ K"
REFERENCES,0.3590177815410669,"
log 2d δ"
REFERENCES,0.3598645215918713,"1/2
≤1 + λ"
REFERENCES,0.3607112616426757,"K + 4
√ 2
√ K"
REFERENCES,0.3615580016934801,"
log 2d δ"
REFERENCES,0.3624047417442845,"1/2
≤2,"
REFERENCES,0.3632514817950889,"λmin(Λ′
h) ≥λmin

Eµ,h[φ(s, a)φ(s, a)⊤/σ2
bVh+1(s, a)]

+ λ"
REFERENCES,0.3640982218458933,"K −4
√ 2
√ K"
REFERENCES,0.36494496189669773,"
log 2d δ 1/2"
REFERENCES,0.3657917019475021,"≥λmin

Eµ,h[φ(s, a)φ(s, a)⊤/σ2
bVh+1(s, a)]

−4
√ 2
√ K"
REFERENCES,0.36663844199830653,"
log 2d δ 1/2 ≥κ"
REFERENCES,0.3674851820491109,"H2 −4
√ 2
√ K"
REFERENCES,0.36833192209991533,"
log 2d δ"
REFERENCES,0.3691786621507197,"1/2
≥
κ
2H2 ."
REFERENCES,0.37002540220152413,"Hence with probability 1 −δ, ∥Λ′
h∥≤2 and
Λ′−1
h
 = 1/λmin(Λ′
h) ≤2H2/κ. Similarly, one can"
REFERENCES,0.37087214225232856,"show
bΛ′−1
h
 ≤2H2/κ with high probability."
REFERENCES,0.37171888230313294,"Now apply Lemma H.4 to bΛ′
h and Λ′
h and a union bound, we obtain with probability 1−δ, for all s, a"
REFERENCES,0.37256562235393736,"∥φ(s, a)∥bΛ′−1
h
≤ "" 1 +"
REFERENCES,0.37341236240474174,"rΛ′−1
h
 ∥Λ′
h∥·
bΛ′−1
h
 ·
bΛ′
h −Λ′
h #"
REFERENCES,0.37425910245554617,"· ∥φ(s, a)∥Λ′−1
h ≤ "" 1 + r 2H2"
REFERENCES,0.37510584250635054,"κ
· 1 · 2H2"
REFERENCES,0.37595258255715497,"κ
·
bΛ′
h −Λ′
h #"
REFERENCES,0.37679932260795934,"· ∥φ(s, a)∥Λ′−1
h ≤  1 +"
REFERENCES,0.37764606265876377,"v
u
u
t48H4 κ2 s H4d"
REFERENCES,0.37849280270956814,"κK log
(λ + K)H λδ"
REFERENCES,0.3793395427603726,"
+ λH2√ d
κK !"
REFERENCES,0.38018628281117695,"· ∥φ(s, a)∥Λ′−1
h ≤  1 +"
REFERENCES,0.3810330228619814,"v
u
u
t96H4 κ2 s H4d"
REFERENCES,0.38187976291278575,"κK log
(λ + K)H λδ 
"
REFERENCES,0.3827265029635902,"· ∥φ(s, a)∥Λ′−1
h
≤2 ∥φ(s, a)∥Λ′−1
h"
REFERENCES,0.3835732430143946,"where the third inequality uses equation 11 and the last and the second last inequality use
K > max{
λ2
κ log((λ+K)H/λδ), 962H12d log((λ + K)H/λδ)/κ5}. Note the above is equivalent to
q"
REFERENCES,0.384419983065199,"φ(s, a)bΛ−1
h φ(s, a) ≤2
q"
REFERENCES,0.3852667231160034,"φ(s, a)Λ−1
h φ(s, a) by multiplying 1/
√"
REFERENCES,0.3861134631668078,K on both sides.
REFERENCES,0.3869602032176122,"C.2.2
ANALYZING THE TERM ||PK
τ=1 xτητ||bΛ−1"
REFERENCES,0.3878069432684166,"Lemma C.4. Recall xτ = φ(sτ
h,aτ
h)
bσ(sτ
h,aτ
h) and ητ =

rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h)

/bσ(sτ
h, aτ
h)."
REFERENCES,0.388653683319221,"Let CH,d,κ,K := 36
r H4d3"
REFERENCES,0.3895004233700254,"κ
log

(λ+K)2KdH2"
REFERENCES,0.3903471634208298,"λδ

+ 12λ H2√"
REFERENCES,0.3911939034716342,"d
κ
and denote"
REFERENCES,0.3920406435224386,"ξ :=
sup
V ∈[0,H], s′∼Ph(s,a), h∈[H]"
REFERENCES,0.392887383573243,"rh + V (s′) −(ThV ) (s, a)"
REFERENCES,0.3937341236240474,"σV (s, a) ."
REFERENCES,0.39458086367485184,Published as a conference paper at ICLR 2022
REFERENCES,0.3954276037256562,"If K ≥4C2
H,d,κ,K and K ≥eO(H6d/κ), then with probability 1 −δ,  K
X"
REFERENCES,0.39627434377646065,"τ=1
xτητ"
REFERENCES,0.397121083827265,"bΛ−1
≤16 s"
REFERENCES,0.39796782387806945,"d log

1 + K λd"
REFERENCES,0.3988145639288738,"
· log
4K2 δ"
REFERENCES,0.39966130397967825,"
+ 4ξ log
4K2 δ"
REFERENCES,0.4005080440304826,"
≤eO max
√"
REFERENCES,0.40135478408128705,"d, ξ
	
,"
REFERENCES,0.4022015241320914,where eO absorbs the constants and Polylog terms.
REFERENCES,0.40304826418289585,"Proof of Lemma C.4. By construction, we have ∥xτ∥≤∥φ/bσ∥≤1 and by Lemma C.7, with
probability 1 −δ/3,"
REFERENCES,0.4038950042337002,"σbVh+1 −bσh

∞= sup
s,a"
REFERENCES,0.40474174428450466,"σ2
bVh+1(s, a) −bσ2
h(s, a)

σbVh+1(s, a) + bσh(s, a)

≤1 2"
REFERENCES,0.4055884843353091,"σ2
bVh+1 −bσ2
h

∞≤CH,d,κ,K r"
K,0.40643522438611346,"1
K"
K,0.4072819644369179,"Therefore, when K ≥4C2
H,d,κ,K, CH,d,κ,K
q"
K,0.40812870448772226,"1
K ≤1/2 ≤σbVh+1(sτ
h, aτ
h)/2 and hence"
K,0.4089754445385267,|ητ| ≤ 
K,0.40982218458933106,"rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h)"
K,0.4106689246401355,"σbVh+1(sτ
h, aτ
h) −CH,d,κ,K K1/2 ≤2 "
K,0.41151566469093986,"rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h)"
K,0.4123624047417443,"σbVh+1(sτ
h, aτ
h) "
K,0.41320914479254867,"≤2
sup
V ∈[0,H], s′∼Ph(s,a)"
K,0.4140558848433531,"r + V (s′) −(ThV ) (s, a)"
K,0.41490262489415747,"σV (s, a) := ξ."
K,0.4157493649449619,"Next, for a ﬁxed function V , we deﬁne the Bellman error as Bh(V )(s, a) = rh+V (s′)−(ThV )(s, a),
then"
K,0.4165961049957663,"Var [ητ|Fτ−1] =
Var
h
rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h)
Fτ−1
i"
K,0.4174428450465707,"bσ2(sτ
h, aτ
h)"
K,0.4182895850973751,"=
Var
h
Bh bVh+1(sτ
h, aτ
h) −BhV ⋆
h+1(sτ
h, aτ
h) + BhV ⋆
h+1(sτ
h, aτ
h)
Fτ−1
i"
K,0.4191363251481795,"bσ2(sτ
h, aτ
h)"
K,0.41998306519898393,"≤
Var

BhV ⋆
h+1(sτ
h, aτ
h)
Fτ−1

+ 8H
Bh bVh+1 −BhV ⋆
h+1

∞
bσ2(sτ
h, aτ
h)"
K,0.4208298052497883,"≤
Var

BhV ⋆
h+1(sτ
h, aτ
h)
Fτ−1

+ 16H
bVh+1 −V ⋆
h+1

∞
bσ2(sτ
h, aτ
h)"
K,0.42167654530059273,"≤
Var

BhV ⋆
h+1(sτ
h, aτ
h)
Fτ−1

+ eO( H3√ d
√ κK )"
K,0.4225232853513971,"bσ2(sτ
h, aτ
h)"
K,0.42337002540220153,"=
Var

BhV ⋆
h+1(sτ
h, aτ
h)
sτ
h, aτ
h

+ eO( H3√ d
√ κK )"
K,0.4242167654530059,"bσ2(sτ
h, aτ
h)"
K,0.42506350550381033,"=
VarV ⋆
h+1(sτ
h, aτ
h) + eO( H3√ d
√ κK )"
K,0.4259102455546147,"bσ2(sτ
h, aτ
h)
≤
2VarV ⋆
h+1(sτ
h, aτ
h) + eO( H3√ d
√ κK )"
K,0.42675698560541914,"σ⋆2(sτ
h, aτ
h)
≤2 +
eO( H3√ d
√ κK )"
K,0.42760372565622357,"σ⋆2(sτ
h, aτ
h)"
K,0.42845046570702794,≤eO(1)
K,0.42929720575783237,"where the ﬁrst inequality is by Lemma H.11, the second inequality is by Th is non-expansive,
the third inequality is by Lemma C.8, the next equality is by Markovian property, and the fourth
inequality is by Lemma C.7 and Lemma C.10. The ﬁfth inequality uses deﬁnition σh,V (s, a)2 :=
max{1, VarPh(V )(s, a)} and the last one is by condition K ≥eO(H6d/κ) and σh,V ⋆(s, a)2 :=
max{1, VarPh(V ⋆)(s, a)} ≥1. Thus, by Bernstein inequality for self-normalized martingale"
K,0.43014394580863674,Published as a conference paper at ICLR 2022
K,0.43099068585944117,"(Lemma H.3),9 with probability 1 −δ, K
X"
K,0.43183742591024554,"τ=1
xτητ"
K,0.43268416596104997,"bΛ−1
≤eO s"
K,0.43353090601185434,"d log

1 + K λd"
K,0.4343776460626588,"
· log
4K2 δ !"
K,0.43522438611346315,"+ 4ξ log
4K2 δ"
K,0.4360711261642676,"
≤eO max
√ d, ξ"
K,0.43691786621507195,where eO absorbs the constants and Polylog terms.
K,0.4377646062658764,"Recall M1, M2, M3, M4 in List A. Based on the above results, we have the following key lemma:"
K,0.4386113463166808,"Lemma C.5. Assume K > max{M1, M2, M3, M4}, for any 0 < λ < κ, suppose
√"
K,0.4394580863674852,"d > ξ,"
K,0.4403048264182896,"where ξ := supV ∈[0,H], s′∼Ph(s,a), h∈[H]"
K,0.441151566469094,"rh+V (s′)−(ThV )(s,a)"
K,0.4419983065198984,"σV (s,a)"
K,0.4428450465707028,". Then with probability 1 −δ, for all"
K,0.4436917866215072,"h, s, a ∈[H] × S × A,"
K,0.4445385266723116,"(Th bVh+1 −bTh bVh+1)(s, a)
 ≤eO
√ d
q"
K,0.445385266723116,"φ(s, a)Λ−1
h φ(s, a)

+ 2H3√ d
K
,"
K,0.4462320067739204,"where Λh = PK
τ=1 φ(sτ
h, aτ
h)⊤φ(sτ
h, aτ
h)/σ2
bVh+1(sτ
h, aτ
h)+λI and eO absorbs the universal constants
and Polylog terms."
K,0.4470787468247248,"Proof of Lemma C.5. Combing equation 9, Lemma C.2, equation 10, Lemma C.3 and C.4 and a
union bound to ﬁnish the proof."
K,0.4479254868755292,"C.3
PROOF OF THE FIRST PART OF THEOREM 3.2"
K,0.4487722269263336,"Theorem C.6 (First part of Theorem 3.2). Let K be the number of episodes. Suppose
√"
K,0.44961896697713805,"d > ξ, where"
K,0.4504657070279424,"ξ := supV ∈[0,H], s′∼Ph(s,a), h∈[H]"
K,0.45131244707874685,"rh+V (s′)−(ThV )(s,a)"
K,0.4521591871295512,"σV (s,a)"
K,0.45300592718035565,"and K > max{M1, M2, M3, M4}10."
K,0.45385266723116,"Then for any 0 < λ < κ, with probability 1 −δ, for all policy π simultaneously, the output bπ of
Algorithm 1 satisﬁes"
K,0.45469940728196445,"vπ −vbπ ≤eO √ d · H
X"
K,0.4555461473327688,"h=1
Eπ
h 
φ(·, ·)⊤Λ−1
h φ(·, ·)
1/2i!"
K,0.45639288738357325,"+ 2H4√ d
K"
K,0.4572396274343776,"where Λh = PK
τ=1
φ(sτ
h,aτ
h)·φ(sτ
h,aτ
h)⊤"
K,0.45808636748518206,"σ2
b
Vh+1(sτ
h,aτ
h)
+ λId and eO absorbs the universal constants and the Polylog"
K,0.45893310753598643,terms.
K,0.45977984758679086,"Proof of Theorem C.6. Combing Lemma C.1 and Lemma C.5, we directly have with probability
1 −δ, for all policy π simultaneously,"
K,0.46062658763759523,"V π
1 (s) −V bπ
1 (s) ≤eO √ d · H
X"
K,0.46147332768839966,"h=1
Eπ
h 
φ(·, ·)⊤Λ−1
h φ(·, ·)
1/2s1 = s
i!"
K,0.4623200677392041,+ 2H4√
K,0.46316680779000846,"d
K
,
(12)"
K,0.4640135478408129,now take the initial distribution d1 on both sides to get the stated result.
K,0.46486028789161726,"C.4
TWO INTERMEDIATE RESULTS"
K,0.4657070279424217,The next two lemmas provide intermediate results in ﬁnishing the whole proofs.
K,0.46655376799322607,"9To be rigorous, Lemma H.3 needs to be modiﬁed since the absolute value bound and the variance bound
here are in the high probability sense. However, this will not affect the validity of the result as the weaker version
can also be obtained (see Chung and Lu (2006) and a related discussion in Yin et al. (2021) Remark E.7.) To
make the proof more readable, we do not include them here to avoid over-technicality.
10The deﬁnition of Mi is in List A."
K,0.4674005080440305,Published as a conference paper at ICLR 2022
K,0.46824724809483487,"C.4.1
BOUNDING THE VARIANCE"
K,0.4690939881456393,"Lemma C.7. Recall the deﬁnition bσh(·, ·)2 = max{1, d
VarPh bVh+1(·, ·)} + 1 and σbVh+1(·, ·)2 :="
K,0.46994072819644367,"max{1, VarPh bVh+1(·, ·)} + 1.
Moreover,
d
Varh bVh+1

(·, ·)
=

φ(·, ·), ¯βh

[0,(H−h+1)2] −

φ(·, ·), ¯θh"
K,0.4707874682472481,"[0,H−h+1]
2
(where
¯βh
and
¯θh
are deﬁned in Algorithm 1).
Let K
≥
max

512(1/κ)2 log
  4Hd"
K,0.47163420829805247,"δ

, 4λ/κ
	
, then with probability 1 −δ,"
K,0.4724809483488569,"sup
h
||bσ2
h −σ2
bVh+1||∞≤36 s H4d3"
K,0.47332768839966133,"κK log
(λ + K)2KdH2 λδ"
K,0.4741744284504657,"
+ 12λH2√"
K,0.47502116850127013,"d
κK
."
K,0.4758679085520745,"Proof. Step1: we ﬁrst show for all h, s, a ∈[H] × S × A, with probability 1 −δ"
K,0.47671464860287893,"⟨φ(s, a), ¯βh⟩[0,(H−h+1)2] −Ph(bVh+1)2(s, a)
 ≤12 s H4d3"
K,0.4775613886536833,"κK log
(λ + K)2KdH2 λδ"
K,0.47840812870448773,"
+4λH2√"
K,0.4792548687552921,"d
κK
."
K,0.48010160880609654,"Proof of Step1. Note
⟨φ(s, a), ¯βh⟩[0,(H−h+1)2] −Ph(bVh+1)2(s, a)
 ≤
⟨φ(s, a), ¯βh⟩−Ph(bVh+1)2(s, a) ="
K,0.4809483488569009,"φ(s, a)⊤¯Σ−1
h K
X"
K,0.48179508890770534,"τ=1
φ(¯sτ
h, ¯aτ
h) · bVh+1(¯sτ
h+1)2 −Ph(bVh+1)2(s, a)  ="
K,0.4826418289585097,"φ(s, a)⊤¯Σ−1
h K
X"
K,0.48348856900931414,"τ=1
φ(¯sτ
h, ¯aτ
h) · bVh+1(¯sτ
h+1)2 −φ(s, a)⊤
Z"
K,0.48433530906011857,"S
(bVh+1)2(s′)dνh(s′)  ="
K,0.48518204911092294,"φ(s, a)⊤¯Σ−1
h K
X"
K,0.48602878916172737,"τ=1
φ(¯sτ
h, ¯aτ
h) · bVh+1(¯sτ
h+1)2 −φ(s, a)⊤¯Σ−1
h ( K
X"
K,0.48687552921253174,"τ=1
φ(¯sτ
h, ¯aτ
h)φ(¯sτ
h, ¯aτ
h)⊤+ λI)
Z"
K,0.4877222692633362,"S
(bVh+1)2(s′)dνh(s′)  ≤"
K,0.48856900931414055,"φ(s, a)⊤¯Σ−1
h K
X"
K,0.489415749364945,"τ=1
φ(¯sτ
h, ¯aτ
h) ·

bVh+1(¯sτ
h+1)2 −Ph(bVh+1)2(¯sτ
h, ¯aτ
h)

|
{z
}
1"
K,0.49026248941574935,"+ λ
φ(s, a)⊤¯Σ−1
h Z"
K,0.4911092294665538,"S
(bVh+1)2(s′)dνh(s′)

|
{z
}
2"
K,0.49195596951735815,"For 2 , since K ≥max

512(1/κ)2 log
  4Hd"
K,0.4928027095681626,"δ

, 4λ/κ
	
, by Lemma H.6 and a union bound over
h ∈[H], with probability 1 −δ for all h, s, a ∈[H] × S × A,"
K,0.49364944961896695,"2 ≤λ ∥φ(s, a)∥¯Σ−1
h  Z"
K,0.4944961896697714,"S
(bVh+1)2(s′)dνh(s′)
¯Σ−1
h"
K,0.4953429297205758,"≤λ 2
√"
K,0.4961896697713802,"K
∥φ(s, a)∥(Σp
h)−1
2
√ K  Z"
K,0.4970364098221846,"S
(bVh+1)2(s′)dνh(s′)

(Σp
h)−1 ≤4λ
(Σp
h)−1 H2√"
K,0.497883149872989,"d
K
≤4λH2√"
K,0.4987298899237934,"d
κK
. (13)"
K,0.4995766299745978,"For 1 , we have"
K,0.5004233700254022,"1 ≤∥φ(s, a)∥¯Σ−1
h  K
X"
K,0.5012701100762066,"τ=1
φ(¯sτ
h, ¯aτ
h) ·

bVh+1(¯sτ
h+1)2 −Ph(bVh+1)2(¯sτ
h, ¯aτ
h)
¯Σ−1
h (14)"
K,0.502116850127011,"Bounding using covering. Note for any ﬁx Vh+1, we can deﬁne xτ = φ(¯sτ
h, ¯aτ
h) (∥φ∥2 ≤1) and
ητ = Vh+1(¯sτ
h+1)2 −Ph(Vh+1)2(¯sτ
h, ¯aτ
h) is H2-subgaussian, by Lemma H.2 (where t = K and
L = 1) with probability 1 −δ, K
X"
K,0.5029635901778154,"τ=1
φ(¯sτ
h, ¯aτ
h) ·
 
Vh+1(¯sτ
h+1)2 −Ph(Vh+1)2(¯sτ
h, ¯aτ
h)

¯Σ−1
h ≤ s"
K,0.5038103302286198,8H4 · d
LOG,0.5046570702794242,"2 log
λ + K λδ "
LOG,0.5055038103302286,let Nh(ϵ) be the minimal ϵ-cover (with respect the supremum norm) of Vh := {Vh : Vh(·) =
LOG,0.506350550381033,maxa∈A
LOG,0.5071972904318375,"
min{φ(s, a)⊤θ −C1
q"
LOG,0.5080440304826418,"d · φ(·, ·)⊤bΛ−1
h φ(·, ·) −C2, H −h + 1}+}

. That is, for any"
LOG,0.5088907705334462,Published as a conference paper at ICLR 2022
LOG,0.5097375105842507,"V ∈Vh, there exists a value function V ′ ∈Nh(ϵ) such that sups∈S |V (s) −V ′(s)| < ϵ. Now by a
union bound, we obtain with probability 1 −δ"
LOG,0.5105842506350551,"sup
Vh+1∈Nh+1(ϵ)  K
X"
LOG,0.5114309906858594,"τ=1
φ(¯sτ
h, ¯aτ
h) ·
 
Vh+1(¯sτ
h+1)2 −Ph(Vh+1)2(¯sτ
h, ¯aτ
h)

¯Σ−1
h ≤ s"
LOG,0.5122777307366638,8H4 · d
LOG,0.5131244707874683,"2 log
λ + K"
LOG,0.5139712108382727,"λδ
|Nh+1(ϵ)|
"
LOG,0.514817950889077,"which implies K
X"
LOG,0.5156646909398814,"τ=1
φ(¯sτ
h, ¯aτ
h) ·

bVh+1(¯sτ
h+1)2 −Ph(bVh+1)2(¯sτ
h, ¯aτ
h)
¯Σ−1
h ≤ s"
LOG,0.5165114309906859,8H4 · d
LOG,0.5173581710414903,"2 log
λ + K"
LOG,0.5182049110922947,"λδ
|Nh+1(ϵ)|

+ 4H2p"
LOG,0.519051651143099,ϵ2K2/λ
LOG,0.5198983911939035,"choosing ϵ = d
√"
LOG,0.5207451312447079,"λ/K, applying Lemma B.3 of Jin et al. (2021b)11 to the covering number Nh+1(ϵ)
w.r.t. Vh+1, we can further bound above by ≤ s"
LOG,0.5215918712955123,8H4 · d3
LOG,0.5224386113463166,"2 log
λ + K"
LOG,0.5232853513971211,"λδ
2dHK

+ 4H2√ d2 ≤6 s"
LOG,0.5241320914479255,"H4 · d3 log
λ + K"
LOG,0.5249788314987299,"λδ
2dHK
"
LOG,0.5258255715495342,"Apply a union bound for h ∈[H], we have with probability 1 −δ, for all h ∈[H], K
X"
LOG,0.5266723116003387,"τ=1
φ(¯sτ
h, ¯aτ
h) ·

bVh+1(¯sτ
h+1)2 −Ph(bVh+1)2(¯sτ
h, ¯aτ
h)
¯Σ−1
h ≤6 s"
LOG,0.5275190516511431,"H4d3 log
(λ + K)2KdH2 λδ "
LOG,0.5283657917019475,"(15)
and similar to 2 , with probability 1 −δ for all h, s, a ∈[H] × S × A,"
LOG,0.529212531752752,"∥φ(s, a)∥¯Σ−1
h
≤2
(Σp
h)−11/2
√"
LOG,0.5300592718035563,"K
≤
2
√"
LOG,0.5309060118543607,"κK
.
(16)"
LOG,0.5317527519051651,"Combing equation 13, equation 14, equation 15 and equation 16 we obtain with probability 1 −δ for
all h, s, a ∈[H] × S × A,"
LOG,0.5325994919559696,"⟨φ(s, a), ¯βh⟩[0,(H−h+1)2] −Ph(bVh+1)2(s, a)
 ≤12 s H4d3"
LOG,0.5334462320067739,"κK log
(λ + K)2KdH2 λδ"
LOG,0.5342929720575783,"
+4λH2√"
LOG,0.5351397121083827,"d
κK
."
LOG,0.5359864521591872,"Step2: we show for all h, s, a ∈[H] × S × A, with probability 1 −δ"
LOG,0.5368331922099915,"⟨φ(s, a), ¯θh⟩[0,H−h+1] −Ph(bVh+1)(s, a)
 ≤12 s H2d3"
LOG,0.5376799322607959,"κK log
(λ + K)2KdH2 λδ"
LOG,0.5385266723116003,"
+ 4λH
√"
LOG,0.5393734123624048,"d
κK . (17)"
LOG,0.5402201524132092,"The proof of Step2 follows nearly the identical way as Step1 except bV 2
h is replaced by bVh."
LOG,0.5410668924640135,"Step3: We prove suph||bσ2
h −σ2
bVh||∞≤36
r H4d3"
LOG,0.541913632514818,"κK log

(λ+K)2KdH2"
LOG,0.5427603725656224,"λδ

+ 12λ H2√"
LOG,0.5436071126164268,"d
κK ."
LOG,0.5444538526672311,"Proof of Step3. By equation 17,


φ(·, ·), ¯θh"
LOG,0.5453005927180355,"[0,H−h+1]
2 −

Ph(bVh+1)(s, a)
2"
LOG,0.54614733276884,"=
⟨φ(s, a), ¯θh⟩[0,H−h+1] + Ph(bVh+1)(s, a)
 ·
⟨φ(s, a), ¯θh⟩[0,H−h+1] −Ph(bVh+1)(s, a)"
LOG,0.5469940728196444,"≤2H ·
⟨φ(s, a), ¯θh⟩[0,H−h+1] −Ph(bVh+1)(s, a)
 ≤24 s H4d3"
LOG,0.5478408128704487,"κK log
(λ + K)2KdH2 λδ"
LOG,0.5486875529212532,"
+ 8λH2√"
LOG,0.5495342929720576,"d
κK
."
LOG,0.550381033022862,11Note the same result in Jin et al. (2021b) applies even though we have an extra constant C2.
LOG,0.5512277730736664,Published as a conference paper at ICLR 2022
LOG,0.5520745131244708,"Combining this with Step1 we receive ∀h, s, a ∈[H] × S × A, with probability 1 −δ"
LOG,0.5529212531752752,"d
Varh bVh+1(s, a) −VarPh bVh+1(s, a)
 ≤36 s H4d3"
LOG,0.5537679932260796,"κK log
(λ + K)2KdH2 λδ"
LOG,0.554614733276884,"
+ 12λH2√"
LOG,0.5554614733276884,"d
κK
."
LOG,0.5563082133784928,"Finally, by the non-expansiveness of operator max{1, ·}, we have the stated result."
LOG,0.5571549534292972,"C.4.2
A CRUDE BOUND ON suph||V ⋆
h −bVh||∞."
LOG,0.5580016934801016,"Lemma
C.8.
Deﬁne
bσh(s, a)
=
r"
LOG,0.558848433530906,"max
n
1, d
VarPh bVh+1(s, a)
o
+ 1,
if
K
≥"
LOG,0.5596951735817104,"max{M1, M2, M3, M4} and K > C · H4κ2, then with probability at least 1 −δ, sup
h"
LOG,0.5605419136325148,"V ⋆
h −bVh

∞≤eO H2√ d
√ κK ! ."
LOG,0.5613886536833192,"Proof. Step1: We show with probability at least 1 −δ, suph
V ⋆
h −V bπ
h

∞≤eO

H2√ d
√ κK 
."
LOG,0.5622353937341237,"Indeed, combing Lemma C.1 and Lemma C.5, similar to the proof of Theorem C.6, we directly have
with probability 1 −δ, for all policy π simultaneously, and for all s ∈S, h ∈[H]"
LOG,0.563082133784928,"V π
h (s) −V bπ
h (s) ≤eO √ d · H
X"
LOG,0.5639288738357324,"t=h
Eπ
h 
φ(·, ·)⊤Λ−1
t φ(·, ·)
1/2sh = s
i!"
LOG,0.5647756138865369,+ 2H4√
LOG,0.5656223539373413,"d
K
,
(18)"
LOG,0.5664690939881456,"Next, since K ≥max

512(1/κ)2 log
  4Hd"
LOG,0.56731583403895,"δ

, 4λ/κ
	
, by Lemma H.6 and a union bound over
h ∈[H], with probability 1 −δ"
LOG,0.5681625740897545,"sup
s,a ∥φ(s, a)∥bΛ−1
h
≤
2
√"
LOG,0.5690093141405589,"K
sup
s,a ∥φ(s, a)∥Λp−1
h
≤
2H
√"
LOG,0.5698560541913632,"κK
, ∀h ∈[H]."
LOG,0.5707027942421676,"Lastly, taking π = π⋆in equation 18 to obtain"
LOG,0.5715495342929721,"0 ≤V π⋆
h (s) −V bπ
h (s) ≤eO √ d · H
X"
LOG,0.5723962743437765,"t=h
Eπ⋆
h 
φ(·, ·)⊤Λ−1
t φ(·, ·)
1/2sh = s
i!"
LOG,0.5732430143945809,"+ 2H4√ d
K ≤eO H2√ d
√ κK !"
LOG,0.5740897544453852,"+ 2H4√ d
K
. (19)"
LOG,0.5749364944961897,"This implies by using the condition K > C · H4κ2, we ﬁnish the proof of Step1."
LOG,0.5757832345469941,"Step2: We show with probability 1 −δ, suph
bVh −V bπ
h

∞≤eO

H2√ d
√ κK 
."
LOG,0.5766299745977985,"Indeed, applying Extended Value Difference Lemma H.7 for π = π′ = bπ, then with probability 1 −δ,
for all s, h"
LOG,0.5774767146486028,"bVh(s) −V bπ
h (s)
 =  H
X"
LOG,0.5783234546994073,"t=h
Ebπ
h
bQh(sh, ah) −

Th bVh+1

(sh, ah)
sh = s
i ≤ H
X t=h"
LOG,0.5791701947502117,"(bTh bVh+1 −Th bVh+1)(s, a)
 + ∥Γh(s, a)∥"
LOG,0.5800169348010161,"≤eO

H
√ d q"
LOG,0.5808636748518204,"φ(s, a)Λ−1
h φ(s, a)"
LOG,0.5817104149026249,"
+ 4H4√"
LOG,0.5825571549534293,"d
K
≤eO H2√ d
√ κK !"
LOG,0.5834038950042337,Published as a conference paper at ICLR 2022
LOG,0.584250635055038,"where the second inequality uses Lemma C.512 and the last inequality follows the same procedure as
Step1."
LOG,0.5850973751058425,"Step3: Combine Step1 and Step2, by triangular inequality and a union bound we ﬁnish the proof of
the lemma."
LOG,0.5859441151566469,"Remark C.9. Note as an intermediate calculation, equation 19 ensures a learning bound with order
eO( H2√ d
√"
LOG,0.5867908552074513,"κK ). Here, the convergence rate is the standard statistical rate
1
√"
LOG,0.5876375952582558,K and the H2 dependence
LOG,0.5884843353090601,"is loose. However, the feature dependence
p"
LOG,0.5893310753598645,"d/κ is roughly tight, since, in the well-explored case
(Assumption 2 of Wang et al. (2021a)), κ = 1/d and the
p"
LOG,0.5901778154106689,"d/κ =
√"
LOG,0.5910245554614734,"d2 recovers the optimal feature
dependence dH
√"
LOG,0.5918712955122777,"T in the online setting (Zhou et al., 2021a). If κ ≪1/d, then doing ofﬂine learning
requires sample size proportional to d/κ, which reveals ofﬂine RL is harder when the exploration
of behavior policy is insufﬁcient. When κ = 0, learning the optimal policy accurately cannot be
guaranteed even if the sample/episode size K →∞."
LOG,0.5927180355630821,"C.5
PROOF OF THE SECOND PART OF THEOREM 3.2"
LOG,0.5935647756138865,"Lemma C.10. Recall bσh =
r"
LOG,0.594411515664691,"max
n
1, d
VarPh bVh+1
o
+ 1 and σ⋆
h =
q"
LOG,0.5952582557154953,"max

1, VarPhV ⋆
h+1
	
+ 1."
LOG,0.5961049957662997,"Let K ≥max

512(1/κ)2 log
  4Hd"
LOG,0.5969517358171041,"δ

, 4λ/κ
	
and K ≥max{M1, M2, M3, M4}, then with prob-
ability 1 −δ,"
LOG,0.5977984758679086,"sup
h
||bσ2
h −σ⋆2
h ||∞≤eO H3√ d
√ κK ! ."
LOG,0.598645215918713,"Proof. By deﬁnition and the non-expansiveness of max{1, ·} + 1, we have
σ2
bVh+1 −σ⋆2
h

∞≤
VarbVh+1 −VarV ⋆
h+1

∞"
LOG,0.5994919559695173,"≤
Ph

bV 2
h+1 −V ⋆2
h+1

∞+
(Ph bVh+1)2 −(PhV ⋆
h+1)2
∞"
LOG,0.6003386960203217,"≤
bV 2
h+1 −V ⋆2
h+1

∞+
(Ph bVh+1 + PhV ⋆
h+1)(Ph bVh+1 −PhV ⋆
h+1)

∞"
LOG,0.6011854360711262,"≤2H
bVh+1 −V ⋆
h+1

∞+ 2H
Ph bVh+1 −PhV ⋆
h+1

∞≤eO H3√ d
√ κK ! ."
LOG,0.6020321761219306,"with probability 1 −δ for all h ∈[H], where the last inequality comes from Lemma C.8. Combining
this with Lemma C.7, we have the stated result."
LOG,0.6028789161727349,"Lemma C.11. Denote the quantities C1 = max{2λ, 128 log(2d/δ), 128H4 log(2d/δ)/κ2} and
C2 = max{
λ2
κ log((λ+K)H/λδ), 962H12d log((λ + K)H/λδ)/κ5}. Suppose the number of episode
K satisﬁes K > max{C1, C2}, then with probability 1 −δ,
q"
LOG,0.6037256562235394,"φ(s, a)Λ−1
h φ(s, a) ≤2
q"
LOG,0.6045723962743438,"φ(s, a)Λ⋆−1
h
φ(s, a),
∀s, a ∈S × A,"
LOG,0.6054191363251482,"Proof of Lemma C.11. By deﬁnition
q"
LOG,0.6062658763759525,"φ(s, a)Λ−1
h φ(s, a) = ∥φ(s, a)∥Λ−1
h . Then denote"
LOG,0.607112616426757,"Λ′
h = 1"
LOG,0.6079593564775614,"K Λh,
Λ⋆′
h = 1"
LOG,0.6088060965283658,"K Λ⋆
h,"
LOG,0.6096528365791702,"12To be absolutely rigorous, we cannot directly apply Lemma C.5 here since the crude bound has already
been used in Lemma C.4. However, this can be resolved completely by ﬁrst deriving an even cruder bound for
suph||V ⋆
h −bVh||∞that has 1/
√"
LOG,0.6104995766299746,"K rate without using Lemma C.5 (which we call it Lemma C.8∗), and we can
use Lemma C.8∗to show a similar result Lemma C.5∗. Finally, we can use Lemma C.5∗here to ﬁnish the proof
of this Lemma C.8. However, we avoid explicitly doing this to prevent over-technicality."
LOG,0.611346316680779,Published as a conference paper at ICLR 2022
LOG,0.6121930567315834,"where Λh = PK
τ=1 φ(sτ
h, aτ
h)⊤φ(sτ
h, aτ
h)/σ2
V ⋆
h+1(sτ
h, aτ
h) + λI.
Under the condition of K, by
Lemma C.10, with probability 1 −δ
Λ⋆′
h −Λ′
h
 ≤sup
s,a"
LOG,0.6130397967823878,"φ(s, a)φ(s, a)⊤"
LOG,0.6138865368331922,"σ⋆2
h (s, a)
−φ(s, a)φ(s, a)⊤"
LOG,0.6147332768839966,"σ2
bVh+1(s, a) "
LOG,0.615580016934801,"≤sup
s,a "
LOG,0.6164267569856055,"σ⋆2
h (s, a) −σ2
bVh+1(s, a)"
LOG,0.6172734970364098,"σ⋆2
h (s, a)σ2
bVh+1(s, a)"
LOG,0.6181202370872142,"· ∥φ(s, a)∥2 ≤sup
s,a "
LOG,0.6189669771380186,"σ⋆2
h (s, a) −σ2
bVh+1(s, a) 1 · 1 ≤eO H3√ d
√ κK ! . (20)"
LOG,0.619813717188823,"Next by Lemma H.5 (with φ to be φ/σV ⋆
h+1 and C = 1), it holds with probability 1 −δ,
Λ⋆′
h −

Eµ,h[φ(s, a)φ(s, a)⊤/σ2
V ⋆
h+1(s, a)] + λ K Id"
LOG,0.6206604572396275," ≤4
√ 2
√ K"
LOG,0.6215071972904318,"
log 2d δ"
LOG,0.6223539373412362,"1/2
."
LOG,0.6232006773920407,"Therefore
by
Weyl’s
spectrum
theorem
and
the
condition
K
>
max{2λ, 128 log(2d/δ), 128H4 log(2d/δ)/κ2}, the above implies
Λ⋆′
h
 =λmax(Λ⋆′
h ) ≤λmax

Eµ,h[φ(s, a)φ(s, a)⊤/σ2
V ⋆
h+1(s, a)]

+ λ"
LOG,0.6240474174428451,"K + 4
√ 2
√ K"
LOG,0.6248941574936494,"
log 2d δ 1/2"
LOG,0.6257408975444538,"≤
Eµ,h[φ(s, a)φ(s, a)⊤/σ2
V ⋆
h+1(s, a)]
 + λ"
LOG,0.6265876375952583,"K + 4
√ 2
√ K"
LOG,0.6274343776460627,"
log 2d δ 1/2"
LOG,0.628281117696867,"≤∥φ(s, a)∥2 + λ"
LOG,0.6291278577476714,"K + 4
√ 2
√ K"
LOG,0.6299745977984759,"
log 2d δ"
LOG,0.6308213378492803,"1/2
≤1 + λ"
LOG,0.6316680779000847,"K + 4
√ 2
√ K"
LOG,0.632514817950889,"
log 2d δ"
LOG,0.6333615580016935,"1/2
≤2,"
LOG,0.6342082980524979,"λmin(Λ⋆′
h ) ≥λmin

Eµ,h[φ(s, a)φ(s, a)⊤/σ2
V ⋆
h+1(s, a)]

+ λ"
LOG,0.6350550381033023,"K −4
√ 2
√ K"
LOG,0.6359017781541066,"
log 2d δ 1/2"
LOG,0.6367485182049111,"≥λmin

Eµ,h[φ(s, a)φ(s, a)⊤/σ2
V ⋆
h+1(s, a)]

−4
√ 2
√ K"
LOG,0.6375952582557155,"
log 2d δ 1/2 ≥κ"
LOG,0.6384419983065199,"H2 −4
√ 2
√ K"
LOG,0.6392887383573242,"
log 2d δ"
LOG,0.6401354784081287,"1/2
≥
κ
2H2 ."
LOG,0.6409822184589331,"Hence with probability 1 −δ,
Λ⋆′
h
 ≤2 and
Λ⋆′−1
h
 = 1/λmin(Λ⋆′
h ) ≤2H2/κ. Similarly,
Λ
′−1
h
 ≤2H2/κ with high probability."
LOG,0.6418289585097375,"Now apply Lemma H.4 to Λ⋆′
h and Λ′
h and a union bound, we obtain with probability 1 −δ, for all
s, a"
LOG,0.642675698560542,"∥φ(s, a)∥Λ′−1
h
≤

1 +
qΛ⋆′−1
h
 Λ⋆′
h
 ·
Λ′−1
h
 ·
Λ⋆′
h −Λ′
h


· ∥φ(s, a)∥Λ⋆′−1
h ≤ "" 1 + r 2H2"
LOG,0.6435224386113463,"κ
· 1 · 2H2"
LOG,0.6443691786621507,"κ
·
Λ⋆′
h −Λ′
h

#"
LOG,0.6452159187129551,"· ∥φ(s, a)∥Λ⋆′−1
h ≤  1 +"
LOG,0.6460626587637596,"v
u
u
tH4 κ2 ""
eO H3√ d
√ κK !#"
LOG,0.6469093988145639,"· ∥φ(s, a)∥Λ⋆′−1
h
≤2 ∥φ(s, a)∥Λ⋆′−1
h"
LOG,0.6477561388653683,"where
the
third
inequality
uses
equation
20
and
the
last
inequality
uses
K
>
max{
λ2
κ log((λ+K)H/λδ), 962H12d log((λ + K)H/λδ)/κ5}. The claimed result follows straightfor-"
LOG,0.6486028789161727,"wardly by multiplying 1/
√"
LOG,0.6494496189669772,K on both sides of the above.
LOG,0.6502963590177815,Published as a conference paper at ICLR 2022
LOG,0.6511430990685859,"Proof of Theorem 3.2. The ﬁrst part of the theorem has been shown in Theorem C.6. For the second
part, apply Theorem C.6 with π = π⋆, then with probability 1 −δ,"
LOG,0.6519898391193903,"vπ⋆−vbπ ≤eO √ d · H
X"
LOG,0.6528365791701948,"h=1
Eπ⋆
h 
φ(·, ·)⊤Λ−1
h φ(·, ·)
1/2i!"
LOG,0.6536833192209992,"+ 2H4√ d
K
,"
LOG,0.6545300592718035,"Now apply Lemma C.11 and a union bound, with probability 1 −δ,"
LOG,0.655376799322608,"0 ≤v⋆−vbπ ≤eO √ d · H
X"
LOG,0.6562235393734124,"h=1
Eπ⋆
h 
φ(·, ·)⊤Λ⋆−1
h
φ(·, ·)
1/2i!"
LOG,0.6570702794242168,"+ 2H4√ d
K
."
LOG,0.6579170194750211,"D
PROOF OF THEOREM 3.3"
LOG,0.6587637595258256,"First of all, we show the following lemma.
Lemma D.1. Suppose K > max{M1, M2, M3, M4}. Plug"
LOG,0.65961049957663,"ΓI
h(s, a) ←φ(s, a)⊤"
LOG,0.6604572396274344,"bΛ−1
h K
X τ=1"
LOG,0.6613039796782387,"φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

−

bTh bVh+1

(sτ
h, aτ
h)
"
LOG,0.6621507197290432,"bσ2
h(sτ
h, aτ
h)"
LOG,0.6629974597798476,"+ eO(H3d/κ K
)"
LOG,0.663844199830652,"in Algorithm 1 and let Th be the Bellman operator and bTh be the approximated Bellman operator.
Then we have with probability 1 −δ:"
LOG,0.6646909398814564,"|(Th bVh+1 −bTh bVh+1)(s, a)| ≤ΓI
h(s, a),
∀s, a ∈S × A."
LOG,0.6655376799322608,"Proof of Lemma D.1. Suppose wh is the coefﬁcient corresponding to the Th bVh+1 (such wh exists by
Lemma H.9), i.e. Th bVh+1 = φ⊤wh, and recall (bTh bVh+1)(s, a) = φ(s, a)⊤bwh, then:

Th bVh+1

(s, a) −

bTh bVh+1

(s, a) = φ(s, a)⊤(wh −bwh)"
LOG,0.6663844199830652,"=φ(s, a)⊤wh −φ(s, a)⊤bΛ−1
h K
X"
LOG,0.6672311600338696,"τ=1
φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

/bσ2
h(sτ
h, aτ
h) !"
LOG,0.668077900084674,"= φ(s, a)⊤wh −φ(s, a)⊤bΛ−1
h K
X"
LOG,0.6689246401354784,"τ=1
φ (sτ
h, aτ
h) ·

Th bVh+1

(sτ
h, aτ
h) /bσ2
h(sτ
h, aτ
h) !"
LOG,0.6697713801862828,"|
{z
}
(i)"
LOG,0.6706181202370872,"+ φ(s, a)⊤bΛ−1
h K
X"
LOG,0.6714648602878917,"τ=1
φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

−

bTh bVh+1

(sτ
h, aτ
h)

/bσ2
h(sτ
h, aτ
h) !"
LOG,0.672311600338696,"|
{z
}
(ii)"
LOG,0.6731583403895004,"+ φ(s, a)⊤bΛ−1
h K
X"
LOG,0.6740050804403048,"τ=1
φ (sτ
h, aτ
h) ·

bTh bVh+1

(sτ
h, aτ
h) −

Th bVh+1

(sτ
h, aτ
h)

/bσ2
h(sτ
h, aτ
h) !"
LOG,0.6748518204911093,"|
{z
}
(iii)
(21)"
LOG,0.6756985605419137,"For term (i), by Lemma C.2 it is bounded by 2λH3√"
LOG,0.676545300592718,"d/κ
K
with probability 1 −δ/2.13"
LOG,0.6773920406435224,"For term (ii), it is bounded by"
LOG,0.6782387806943269,"φ(s, a)⊤"
LOG,0.6790855207451313,"bΛ−1
h K
X τ=1"
LOG,0.6799322607959356,"φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

−

bTh bVh+1

(sτ
h, aτ
h)
"
LOG,0.68077900084674,"bσ2
h(sτ
h, aτ
h) ."
LOG,0.6816257408975445,"13Note Here Lemma C.2 still applies even if the Γh changes since it works for all bVh ∈[0, H] so that
∥wh∥2 ≤2H
√"
LOG,0.6824724809483489,d and the truncation (Line 13 in Algorithm 1) guarantees this.
LOG,0.6833192209991532,Published as a conference paper at ICLR 2022
LOG,0.6841659610499576,"For term (iii), by Cauchy inequality"
LOG,0.6850127011007621,"φ(s, a)⊤bΛ−1
h K
X"
LOG,0.6858594411515665,"τ=1
φ (sτ
h, aτ
h) ·

bTh bVh+1

(sτ
h, aτ
h) −

Th bVh+1

(sτ
h, aτ
h)

/bσ2
h(sτ
h, aτ
h) !"
LOG,0.6867061812023709,"≤∥φ(s, a)∥bΛ−1
h ·  K
X"
LOG,0.6875529212531752,"τ=1
φ (sτ
h, aτ
h) ·

bTh bVh+1

(sτ
h, aτ
h) −

Th bVh+1

(sτ
h, aτ
h)

/bσ2
h(sτ
h, aτ
h)"
LOG,0.6883996613039797,"bΛ−1
h ≤2H
√ κK
·  K
X"
LOG,0.6892464013547841,"τ=1
φ (sτ
h, aτ
h) ·

bTh bVh+1

(sτ
h, aτ
h) −

Th bVh+1

(sτ
h, aτ
h)

/bσ2
h(sτ
h, aτ
h)"
LOG,0.6900931414055885,"bΛ−1
h ≤2H
√"
LOG,0.6909398814563928,"κK
· eO(H2p d/κ
√"
LOG,0.6917866215071973,"K
) ·
√"
LOG,0.6926333615580017,"d = eO(H3d/κ K
)"
LOG,0.6934801016088061,"where the ﬁrst inequality is by Lemma H.6 (with φ′ = φ/bσh and ∥φ/bσh∥≤∥φ∥≤1 := C) and the
third inequality uses
√"
LOG,0.6943268416596104,"a⊤· A · a ≤
p"
LOG,0.6951735817104149,"∥a∥2 ∥A∥2 ∥a∥2 = ∥a∥2
p"
LOG,0.6960203217612193,∥A∥2 with a to be either φ or wh.
LOG,0.6968670618120237,"Moreover, λmin(˜Λp
h) ≥κ/ maxh,s,a bσh(s, a)2 ≥κ/H2 implies
(˜Λp
h)−1 ≤H2/κ."
LOG,0.6977138018628282,"The second inequality is true by denoting xτ = φ(sτ
h, aτ
h)/bσ(sτ
h, aτ
h) and"
LOG,0.6985605419136325,"ητ =

bTh bVh+1

(sτ
h, aτ
h) −

Th bVh+1

(sτ
h, aτ
h)

/bσh(sτ
h, aτ
h)"
LOG,0.6994072819644369,"and use Lemma H.10 as the condition for applying Lemma H.2. By collecting those three terms
together we have the result."
LOG,0.7002540220152413,"D.1
PROOF OF THEOREM 3.3"
LOG,0.7011007620660458,"Proof. Use Lemma D.1 as the condition for Lemma C.1 and average over initial distribution d1, we
obtain with probability 1 −δ,"
LOG,0.7019475021168501,"vπ −vbπ ≤ H
X"
LOG,0.7027942421676545,"h=1
Eπh [φ(s, a)]⊤"
LOG,0.7036409822184589,"bΛ−1
h K
X τ=1"
LOG,0.7044877222692634,"φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

−

bTh bVh+1

(sτ
h, aτ
h)
"
LOG,0.7053344623200677,"bσ2
h(sτ
h, aτ
h)"
LOG,0.7061812023708721,"+ eO(H4d/κ K
) (22)"
LOG,0.7070279424216765,"Denote Ah := PK
τ=1
φ(sτ
h,aτ
h)·(rτ
h+bVh+1(sτ
h+1)−(Th bVh+1)(sτ
h,aτ
h))
bσ2
h(sτ
h,aτ
h)
, then"
LOG,0.707874682472481,"Eπh [φ(s, a)]⊤"
LOG,0.7087214225232854,"bΛ−1
h K
X τ=1"
LOG,0.7095681625740897,"φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

−

bTh bVh+1

(sτ
h, aτ
h)
"
LOG,0.7104149026248942,"bσ2
h(sτ
h, aτ
h) "
LOG,0.7112616426756986,"≤Eπh [φ]⊤·
bΛ−1
h Ah
 + Eπh [φ]⊤"
LOG,0.712108382726503,"bΛ−1
h K
X τ=1"
LOG,0.7129551227773073,"φ (sτ
h, aτ
h) ·

Th bVh+1 (sτ
h, aτ
h) −bTh bVh+1 (sτ
h, aτ
h)
"
LOG,0.7138018628281118,"bσ2
h(sτ
h, aτ
h) "
LOG,0.7146486028789162,"For the second term, it can be bounded similar to term (iii) in Lemma D.1 and for the ﬁrst term we
have the following:"
LOG,0.7154953429297206,"Eπh [φ]⊤·
bΛ−1
h Ah
 = Eπh [φ]⊤· bΛ−1
h
· bΛh
bΛ−1
h Ah
 ≤∥Eπh[φ]∥bΛ−1
h ·
bΛh|bΛ−1
h Ah|
bΛ−1
h
≤∥Eπh[φ]∥bΛ−1
h · ∥|Ah|∥bΛ−1
h
≤eO(
√"
LOG,0.7163420829805249,"d) ∥Eπh[φ]∥bΛ−1
h
≤eO(
√"
LOG,0.7171888230313294,"d ∥Eπh[φ]∥Λ−1
h ),"
LOG,0.7180355630821338,"where the ﬁrst inequality uses Cauchy’s inequality, the second inequality uses bΛh is coordinate-wise
positive (since we assume here φ ≥0), the third inequality is identical to the analysis in Section C.2.2
and the fourth inequality is identical to the analysis in Section C.2.1 with φ replaced by E[φ]. Plug
this back to equation 22 we ﬁnish the proof for the ﬁrst part. For the second part, converting Λ−1
h
to
Λ⋆−1
h
is identical to Section C.5. This ﬁnishes the proof."
LOG,0.7188823031329382,Published as a conference paper at ICLR 2022
LOG,0.7197290431837426,"E
PROOF OF MINIMAX LOWER BOUND THEOREM 3.5"
LOG,0.720575783234547,"The proof follows the lower bound proof of Zanette et al. (2021). For completeness, we provide all
the details in below."
LOG,0.7214225232853514,"E.1
CONSTRUCTION"
LOG,0.7222692633361558,"Similar to the proof of [Zanette et al. (2021), Theorem 2], we construct a family of MDPs, each
parameterized by a Boolean vector u = (u1, . . . , uH) with each uh ∈{−1, +1}d−2 for h ∈[H].
The MDPs share the same transition kernel and are only different in the reward observations."
LOG,0.7231160033869602,"State space: At each time step h, there are two states S = {+1, −1}.
Action space: The action space A = {−1, 0, +1}d−2.
Feature map: The feature map φ : S × A 7→Rd is given by"
LOG,0.7239627434377646,"φ(+1, a) =   a
√"
D,0.724809483488569,"2d
1
√ 2
0 "
D,0.7256562235393734,"∈Rd,
φ(−1, a) =   a
√"
D,0.7265029635901779,"2d
0
1
√ 2  ∈Rd."
D,0.7273497036409822,"The construction ensures the condition ∥φ(s, a)∥2 ≤1 for any (s, a) ∈S × A.
Transition kernel: The transition probability Ph(s′ | s, a) is independent of action a. In other
words, the Markov decision process reduces to a homogeneous Markov chain with transition
matrix"
D,0.7281964436917866,"P =
 1"
D,0.729043183742591,"2
1
2
1
2
1
2"
D,0.7298899237933955,"
∈R2."
D,0.7307366638441999,By letting
D,0.7315834038950042,νh(+1) = νh(−1) = 
D,0.7324301439458086,"
0d−2
1
√ 2
1
√ 2  ∈Rd,"
D,0.7332768839966131,"we have Ph(s′ | s, a) = ⟨φ(s, a), νh(s′)⟩to be a valid probability transition.
Reward observations: For any MDP Mu, at each times step h, the reward follows a Gaussian
distribution with"
D,0.7341236240474175,"Ru,h(s, a) ∼N
 s
√"
D,0.7349703640982218,"6 +
δ
√"
D,0.7358171041490262,"2d
⟨a, uh⟩, 1

,"
D,0.7366638441998307,"where δ ∈

0,
1
√"
D,0.7375105842506351,"3d

determines to what extent the MDP models are different from each
other. The mean reward function satisﬁes ru,h(s, a) = ⟨φ(s, a), θu,h⟩with"
D,0.7383573243014394,"θu,h = "
D,0.7392040643522438,"
δuh
1
√"
D,0.7400508044030483,"3
−1
√ 3  ∈Rd."
D,0.7408975444538527,"Ofﬂine data collection Scheme: The dataset D = {(sτ
h, aτ
h, rτ
h, sτ
h+1)}h∈[H]
τ∈[K] consist of K i.i.d.
trajectories. All the trajectories initiate from uniform distribution. We take a behavior policy
µ(· | s) that is independent of state s. Let {e1, e2, . . . , ed−2} be the canonical bases of
Rd−2 and 0d−2 ∈Rd−2 be the zero vector. The behavior policy µ is set as"
D,0.7417442845046571,µ(ej | s) = 1
D,0.7425910245554614,"d
for any j ∈[d −2]
and
µ(0d−2 | s) = 2 d."
D,0.7434377646062659,"E.2
OVERVIEW OF PROOF"
D,0.7442845046570703,"The proof of the theorem is based on Assouad’s method, where we ﬁrst reduce the problem to binary
hypothesis tests (Lemmas E.1 and E.2) and then connect the testing error to the uncertainty quantity
in the upper bound (Lemma E.3)."
D,0.7451312447078747,Published as a conference paper at ICLR 2022
D,0.745977984758679,Lemma E.1 (Reduction to testing). There exists a universal constant c1 > 0 such thata
D,0.7468247248094835,"inf
bπ max
u∈U Eu

V ⋆
u −V bπ
u

≥c1 δ
√"
D,0.7476714648602879,"d H
min
u,u′∈U:DH(u′;u)=1 inf
ψ

Pu(ψ ̸= u) + Pu′(ψ ̸= u′)

,
(23)"
D,0.7485182049110923,"where bπ denotes the output of any algorithm that maps from observations to an estimated policy. ψ is
any test function for parameter u and DH is the hamming distance."
D,0.7493649449618967,"Lemma E.2. There exists a universal constant c2 > 0 such that when taking δ := c2 d
√"
D,0.7502116850127011,"K , we have"
D,0.7510584250635055,"min
u,u′∈U:DH(u′;u)=1 inf
ψ

Pu(ψ ̸= u) + Pu′(ψ ̸= u′)

≥1"
D,0.7519051651143099,"2.
(24)"
D,0.7527519051651143,"When K ≳d3, δ := c2 d
√"
D,0.7535986452159187,"K ensures that δ ≤1/
√"
D,0.7544453852667231,"3d. Combining Lemmas E.1 and E.2 yields a lower
bound"
D,0.7552921253175275,"inf
bπ max
u∈U Eu

V ⋆
u −V bπ
u

≥c d
√ dH
√"
D,0.756138865368332,"K
,
(25)"
D,0.7569856054191363,"where c > 0 is a universal constant. We then use the following Lemma E.3 to connect the above
lower bound to the uncertainty term
√"
D,0.7578323454699407,"d·PH
h=1
p"
D,0.7586790855207451,"Eπ⋆[φ]⊤(Λ⋆
h)−1Eπ⋆[φ] for the chosen linear MDP
instances class M."
D,0.7595258255715496,"Lemma E.3. There exists a universal constant c3 > 0 such that for all M ∈M, H
X h=1 q"
D,0.7603725656223539,"Eπ⋆[φ]⊤(Λ⋆
h)−1Eπ⋆[φ] ≤c3
d H
√"
D,0.7612193056731583,"K
.
(26)"
D,0.7620660457239627,"Plugging inequality equation E.3 into the bound equation 25, we obtain the minimax lower bound equa-
tion 6 in the statement of theorem."
D,0.7629127857747672,"E.3
REDUCTION TO TESTING VIA ASSOUAD’S METHOD"
D,0.7637595258255715,"Proof of Lemma E.1. For any index vector u = (u1, . . . , uH) ∈U = {−1, +1}(d−2)×H, the optimal
policy for MDP instance Mu is simply"
D,0.7646062658763759,"π⋆
h(·) = uh
for h ∈[H]."
D,0.7654530059271804,"Similar to the proof of Lemma 9 in Zanette et al. (2021), we can show that the value suboptimality of
policy π on MDP Mu is given by"
D,0.7662997459779848,"V ⋆
u −V π
u =
δ
√"
D,0.7671464860287892,"2d H
X h=1"
D,0.7679932260795935,"uh −Eπ[ah]

1."
D,0.768839966130398,"Deﬁne uπ = (uπ
1, . . . , uπ
H) with uπ
h := sign
 
Eπ[ah]

, then the ℓ1-norm is lower bounded as
uh −Eπ[ah]

1 ≥DH(uπ
h; uh),"
D,0.7696867061812024,where DH(·; ·) denotes the Hamming distance. It follows that
D,0.7705334462320068,"V ⋆
u −V π
u ≥
δ
√"
D,0.7713801862828111,"2d
DH(uπ; u).
(27)"
D,0.7722269263336156,We then apply Assouad’s method (Lemma 2.12 in Sampson and Guttorp (1992)) and obtain that
D,0.77307366638442,"inf
ˆu∈U max
u∈U Eu

DH(ˆu; u)

≥(d −2)H"
MIN,0.7739204064352244,"2
min
u,u′∈U:DH(u′;u)=1 inf
ψ

Pu(ψ ̸= u) + Pu′(ψ ̸= u′)

,
(28)"
MIN,0.7747671464860287,"where ψ is any test functions mapping from observations to {u, u′}. Combining inequalities equa-
tion 27 and equation 28, we ﬁnish the proof."
MIN,0.7756138865368332,Published as a conference paper at ICLR 2022
MIN,0.7764606265876376,"E.4
LOWER BOUND ON THE TESTING ERROR"
MIN,0.777307366638442,"Proof of Lemma E.2. The proof of Lemma E.2 is similar to that of Lemma 10 in Zanette et al. (2021).
We ﬁrst apply Theorem 2.12 in Sampson and Guttorp (1992) to lower bound the testing error using
Kullback–Leibler divergence and obtain"
MIN,0.7781541066892464,"min
u,u′∈U:DH(u′;u)=1 inf
ψ

Pu(ψ ̸= u) + Pu′(ψ ̸= u′)

≥1 −

1
2
max
u,u′∈U:DH(u′;u)=1 DKL(Qu∥Qu′)
1/2
. (29)"
MIN,0.7790008467400508,It only remains to estimate DKL(Qu∥Qu′).
MIN,0.7798475867908552,The probability density Qu takes the form
MIN,0.7806943268416596,"Qu(D) = K
Y"
MIN,0.781541066892464,"k=1
ξ1(sk
1) H
Y"
MIN,0.7823878069432684,"h=1
µ
 
ak
h | sk
h)

Ru,h(sk
h, ak
h)

(rk
h) Ph(sk
h+1 | sk
h, ak
h)"
MIN,0.7832345469940728,"where ξ1 =
 1 2, 1"
MIN,0.7840812870448772,"2

is the initial distribution. It follows that"
MIN,0.7849280270956817,"DKL(Qu∥Qu′) = Eu

log(Qu/Qu′)
 = K · H
X"
MIN,0.785774767146486,"h=1
Eu
h
log
 
Ru,h(s1
h, a1
h)

(r1
h)

Ru′,h(s1
h, a1
h)

(r1
h)
i = K d d−2
X"
MIN,0.7866215071972904,"j=1
DKL

N
 
δ
√"
MIN,0.7874682472480948,"2d⟨ej, uh⟩, 1
  N
 
δ
√"
MIN,0.7883149872988993,"2d⟨ej, u′
h⟩, 1

."
MIN,0.7891617273497037,"If we take δ = c2 d
√"
MIN,0.790008467400508,"K , then inequality equation 29 ensures inequality equation 24, as claimed in the
statement of the lemma."
MIN,0.7908552074513124,"E.5
CONNECTION TO THE UNCERTAINTY TERM"
MIN,0.7917019475021169,"Proof of Lemma E.3. We ﬁrst calculate the explicit form of the inverse of variance-rescaled covari-
ance matrix Λ⋆,p
h . For each time step h ∈[H], the value function V ⋆
u,h+1 takes the form"
MIN,0.7925486875529213,"V ⋆
u,h+1 = Eπ⋆ru,h+1 +
 
Pπ⋆
h+1V ⋆
u,h+2

."
MIN,0.7933954276037256,"Since
 
Ph+1V ⋆
u,h+2

(+1) =
 
Ph+1V ⋆
u,h+2

(−1) and ru,h+1(+1, a) −ru,h+1(−1, a) = 2/
√"
MIN,0.79424216765453,"6, we
have"
MIN,0.7950889077053345,"VarPh(V ⋆
u,h+1)(+1, a) = VarPh(Eπ⋆ru,h+1)(+1, a) = 1 6."
MIN,0.7959356477561389,"Similarly,"
MIN,0.7967823878069432,"VarPh(V ⋆
u,h+1)(−1, a) = VarPh(V ⋆
u,h+1)(+1, a) = 1 6."
MIN,0.7976291278577476,"By routine calculation, we ﬁnd that the population-level rescaled covariance matrix takes the form"
MIN,0.7984758679085521,"Λ⋆,p
h
= 3K 2"
MIN,0.7993226079593565,"2
d2 Id−2
1
d
√"
MIN,0.8001693480101609,"d1(d−2)×2
1
d
√"
MIN,0.8010160880609652,"d12×(d−2)
I2 ! ∈Rd×d"
MIN,0.8018628281117697,"for any h ∈[H]. Applying Gaussian elimination on Λ⋆,p
h , we have"
MIN,0.8027095681625741,"(Λ⋆,p
h )−1 =
2
3K   d2"
MIN,0.8035563082133785,"2

Id−2 +
1
d−21(d−2)×(d−2)
	
−
d
√"
MIN,0.8044030482641829,"d
2(d−2)1(d−2)×2 −
d
√"
MIN,0.8052497883149873,"d
2(d−2)12×(d−2)
1
d−2"
MIN,0.8060965283657917,"
d −1
1
1
d −1 
 ."
MIN,0.8069432684165961,Published as a conference paper at ICLR 2022
MIN,0.8077900084674005,"For each time step h ∈[H], we have (by Jensen’s inequality)
q"
MIN,0.8086367485182049,"Eπ⋆[φ]⊤(Λ⋆
h)−1Eπ⋆[φ] ≤1 2"
MIN,0.8094834885690093,"φ(+1, uh)

(Λ⋆,p
h
)−1 + 1 2"
MIN,0.8103302286198137,"φ(−1, uh)

(Λ⋆,p
h
)−1."
MIN,0.8111769686706182,"Recall that by our construction,"
MIN,0.8120237087214225,"φ(+1, uh) =   uh
√"
D,0.8128704487722269,"2d
1
√ 2
0 "
D,0.8137171888230313,"∈Rd,
φ(−1, uh) =   uh
√"
D,0.8145639288738358,"2d
0
1
√ 2  ∈Rd."
D,0.8154106689246401,"It follows that
φ(+1, uh)
2
(Λ⋆,p
h
)−1 =
φ(−1, uh)
2
(Λ⋆,p
h
)−1"
D,0.8162574089754445,"=
2
3K d"
D,0.817104149026249,"4u⊤
h

Id−2 +
1
d−21(d−2)×(d−2)
	
uh −
d
2(d −2)1⊤
d−2uh +
d −1
2(d −2) "
D,0.8179508890770534,"=
2
3K d2"
D,0.8187976291278577,"4 +
d
4(d −2)
 
1 −1⊤
d−2uh
2 + 1 4 "
D,0.8196443691786621,"≤
2
3K d2"
D,0.8204911092294666,4 + d(d −1)2
D,0.821337849280271,4(d −2) + 1 4
D,0.8221845893310754,"
=
2
3K d2"
D,0.8230313293818797,"2 +
d −1
2(d −2)"
D,0.8238780694326842,"
≲d2/K."
D,0.8247248094834886,"Therefore,
q"
D,0.825571549534293,"Eπ⋆[φ]⊤(Λ⋆
h)−1Eπ⋆[φ] ≲d/
√ K."
D,0.8264182895850973,"Taking the summation over h ∈[H], we obtain the bound equation 26 as claimed in the lemma
statement."
D,0.8272650296359018,"F
A NUMERICAL SIMULATION"
D,0.8281117696867062,"F.1
A LINEAR MDP CONSTRUCTION"
D,0.8289585097375106,"We consider a synthetic linear MDP example that is similar to Min et al. (2021) but with some
modiﬁcations for the ofﬂine learning task. The MDP instance we use consists of |S| = 2 states and
|A| = 100 actions, and feature dimension d = 100. We set S = {0, 1} and A = {0, 1, . . . , 99}
respectively.
For each action a ∈{0, 1, . . . , 99}, we use binary encoding to obtain a vec-
tor a ∈R8 using its binary representation (i.e.
each coordinate is either 0 or 1).
we inter-
changebly use a and and its vector representation a for the ease of explanation. We ﬁrst deﬁne"
D,0.8298052497883149,"δ(s, a) =
 1
if 1{s = 0} = 1{a = 0}
0
otherwise
, then the non-stationary linear MDP is speciﬁed by the"
D,0.8306519898391194,following conﬁguration
D,0.8314987298899238,"• Feature mapping:
φ(s, a) =
 
a⊤, δ(s, a), 1 −δ(s, a)
⊤∈R10"
D,0.8323454699407282,• The true measure νh
D,0.8331922099915327,"νh(s) = (0, . . . , 0, (1 −s) ⊕αh, s ⊕αh) ,"
D,0.834038950042337,"where {αh}h∈[H] is a sequence of integers taking values 0 or 1 and ⊕is the standard XOR
operator. We deﬁne
θh ≡(0, . . . , 0, r, 1 −r) ∈R10"
D,0.8348856900931414,"with the choice of r = 0.9. The transition follows Ph(s′|s, a) = ⟨φ(s, a), νh(s′)⟩and the
mean reward function rh(s, a) = ⟨φ(s, a), θh⟩.
• Behavior policy: always choose action a = 0 with probability p, and other actions uniformly
with probability (1 −p)/99. The initial distribution chooses s = 0 and s = 1 with equal
probability 1/2. We use p = 0.6."
D,0.8357324301439458,Published as a conference paper at ICLR 2022
D,0.8365791701947503,"0
200
400
600
800
1000
Episode K 10−3 10−2 10−1 100 101"
D,0.8374259102455546,Suboptimality gap
D,0.838272650296359,Fixed horizon H = 20
D,0.8391193903471634,"PEVI
VAPVI
LSVI
VAVI"
D,0.8399661303979679,"(a) Suboptimality vs. Episode K
(Horizon H = 20)"
D,0.8408128704487722,"0
200
400
600
800
1000
Episode K 10−3 10−2 10−1 100 101"
D,0.8416596104995766,Suboptimality gap
D,0.842506350550381,Fixed horizon H = 30
D,0.8433530906011855,"PEVI
VAPVI
LSVI
VAVI"
D,0.8441998306519899,"(b) Suboptimality vs. Episode K
(Horizon H = 30)"
D,0.8450465707027942,"0
200
400
600
800
1000
Episode K 10−2 10−1 100 101"
D,0.8458933107535986,Suboptimality gap
D,0.8467400508044031,Fixed horizon H = 50
D,0.8475867908552075,"PEVI
VAPVI
LSVI
VAVI"
D,0.8484335309060118,"(c) Suboptimality vs. Episode K
(Horizon H = 50)"
D,0.8492802709568162,"Figure 1: Comparison between PEVI and VAPVI in the non-stationary linear MDP instance described
above. In each ﬁgure, y-axis denotes suboptimality gap v⋆−vbπ, x-axis denotes number of episodes
K. The problem horizons are ﬁxed to be H = 20, 30, 50. The solid line denotes the average
suboptimality gap over 50 trials and the error bar area is the corresponding standard deviation. The
range of K is from 5 to 1000."
D,0.8501270110076207,"F.2
EMPIRICAL COMPARISON BETWEEN PEVI AND VAPVI ON THE CONSTRUCTED LINEAR
MDP"
D,0.8509737510584251,"We compare Pessimistic Value Iteration (PEVI) in Jin et al. (2021b) and our VAPVI Algorithm 1 in
Figure 1, with horizon to be H = 20, 30, 50. In addition, we add the non-pessimistic version for both
algorithms, i.e. least-square value iteration (LSVI) and variance-aware value iteration (VAVI). The
true optimal value v⋆is computed via value iteration using the underlying transition kernels. For
the empirical validation of VAPVI, we do not split the data and, in particular, in all the methods we
choose λ = 0.01 (instead of λ = 1 used in theory (Jin et al., 2021b) which causes over-regularization
in the simulation)."
D,0.8518204911092294,"We can observe VAPVI outperforms PEVI and the gap becomes larger when horizon H increases.
One main reason for this to happen is due to the bonus used in PEVI (Jin et al., 2021b)"
D,0.8526672311600338,"O

dH ·

φ(·, ·)⊤Σ−1
h φ(·, ·)
1/2"
D,0.8535139712108383,"is overly pessimistic comparing to our O
√"
D,0.8543607112616427,"d ·

φ(·, ·)⊤Λ−1
h φ(·, ·)
1/2"
D,0.8552074513124471,"when H becomes larger and this could potentially make the learning less accurate. In addition, both
non-pessimistic algorithms exhibit similar accuracy, and this is partially owing to our truncation
scheme bσh(·, ·)2 ←max{1, d
VarPh bVh+1(·, ·)} so bσh(·, ·)2 will just be 1 when the estimated variance
is small. Lastly, variance-aware pessimism eventually outperforms non-pessimism algorithms when
sample size is large and this might come from that the pessimistic bonus is estimated more accurately
when more samples are collected."
D,0.8560541913632514,"G
SOME MISSING DERIVATIONS AND DISCUSSIONS"
D,0.8569009314140559,"G.1
REGARDING COVERAGE ASSUMPTION"
D,0.8577476714648603,"Now we discuss the feature coverage assumption. Indeed, even if Assumption 2.2 is not satisﬁed,
we can still learn in the effective subspan of Σp
h := Eµ,h

φ(s, a)φ(s, a)⊤
. Concretely, since Σp
h is
symmetric, by orthogonal decomposition we have Σp
h = ZhΛZ⊤
h , where Zh (can be estimated using
the samples for practical purpose) consists of orthogonal basis and Λ consists of eigenvalues of Σp
h in
the diagonal. Suppose we do not have a full coverage, i.e.
Λ = diag[λ1, λ2, ..., λd′, 0, ..., 0]
with
d′ < d,
then we can create transformed features φ′
h(s, a) = Zh · φh(s, a), and then"
D,0.8585944115156647,"Eµ,h

φ′
h(s, a)φ′
h(s, a)⊤
= Λ = diag[λ1, λ2, ..., λd′, 0, ..., 0]."
D,0.859441151566469,"Then we can do learning w.r.t. the truncated features φ′
h|1:d′’s instead of the original φ. It reduces to
the weaker notion of κ := minh∈[H]{κh : s.t. κh = smallest positive eigenvalue at time h}."
D,0.8602878916172735,Published as a conference paper at ICLR 2022
D,0.8611346316680779,"G.2
DERIVATION OF EQUATION 5"
D,0.8619813717188823,"When reducing Theorem 3.2,3.3 to the tabular case, set φ(s, a) = 1s,a, d = SA, λ = 0, and recall
by Assumption 3.4 (let’s assume π⋆is a deterministic policy as it always exists in tabular MDP)
C⋆:= suph,s,a dπ⋆
h (s, a)/dµ
h(s, a), then for Theorem 3.2 √ d · H
X"
D,0.8628281117696867,"h=1
Eπ⋆
q"
D,0.8636748518204911,"φ(·, ·)⊤Λ⋆−1
h
φ(·, ·)

=
√ d · H
X h=1 X"
D,0.8645215918712955,"s,a
dπ⋆
h (s, a)
q"
D,0.8653683319220999,"1⊤
s,aΛ⋆−1
h
1s,a =
√ SA · H
X h=1 X"
D,0.8662150719729044,"s,a
dπ⋆
h (s, a) s"
D,0.8670618120237087,"1⊤
s,adiag
VarP·,·(V ⋆
h+1)
nh,·,·"
D,0.8679085520745131,"
1s,a =
√ SA · H
X h=1 X"
D,0.8687552921253175,"s,a
dπ⋆
h (s, a) s"
D,0.869602032176122,"VarPs,a(V ⋆
h+1)
nh,s,a
nh,s,a := K
X"
D,0.8704487722269263,"τ=1
1[sτ
h, aτ
h = s, a] ≲
√ SA · H
X h=1 X"
D,0.8712955122777307,"s,a
dπ⋆
h (s, a) s"
D,0.8721422523285352,"VarPs,a(V ⋆
h+1)
K · dµ
h(s, a)
≤
p"
D,0.8729889923793396,"SAC⋆/K · H
X h=1 X s,a q"
D,0.8738357324301439,"dπ⋆
h (s, a)VarPs,a(V ⋆
h+1) =
p"
D,0.8746824724809483,"SAC⋆/K · H
X h=1 X s q"
D,0.8755292125317528,"dπ⋆
h (s, π⋆(s))VarPs,π⋆(s)(V ⋆
h+1) ≤
p"
D,0.8763759525825572,"SAC⋆/K · H
X h=1 s S ·
X"
D,0.8772226926333616,"s
dπ⋆
h (s, π⋆(s))VarPs,π⋆(s)(V ⋆
h+1) ≤
p"
D,0.8780694326841659,S2AC⋆/K ·
D,0.8789161727349704,"v
u
u
tH H
X h=1 X"
D,0.8797629127857748,"s
dπ⋆
h (s, π⋆(s))VarPs,π⋆(s)(V ⋆
h+1) =
p"
D,0.8806096528365792,S2AC⋆/K ·
D,0.8814563928873835,"v
u
u
tH · H
X"
D,0.882303132938188,"h=1
Eπ⋆
h[VarP(·,·)(V ⋆
h+1)] ≤
p"
D,0.8831498729889924,H3S2AC⋆/K
D,0.8839966130397968,"where the ﬁrst inequality is by Chernoff bound and the last one is by Lemma 3.4. of Yin and Wang
(2020) (Law of total variances). The rest of them are from Cauchy’s inequality. Similarly, for
Theorem 3.3, we also have √ d · H
X h=1 q"
D,0.8848433530906011,"Eπ⋆[φ]⊤Λ⋆−1
h
Eπ⋆[φ] =
√ d · H
X h=1 q"
D,0.8856900931414056,"Vec{dπ⋆}Λ⋆−1
h
Vec{dπ⋆} =
√ d · H
X h=1 s"
D,0.88653683319221,"Vec{dπ⋆}diag
VarP·,·(V ⋆
h+1)
nh,·,·"
D,0.8873835732430144,"
Vec{dπ⋆} =
√ SA · H
X h=1"
D,0.8882303132938189,"v
u
u
tX"
D,0.8890770533446232,"s,a
dπ⋆
h (s, a)2 VarPs,a(V ⋆
h+1)
nh,s,a ≲
√ SA · H
X h=1"
D,0.8899237933954276,"v
u
u
tX"
D,0.890770533446232,"s,a
dπ⋆
h (s, a)2 VarPs,a(V ⋆
h+1)
K · dµ
h(s, a) ≤
p"
D,0.8916172734970365,"SAC⋆/K · H
X h=1 sX"
D,0.8924640135478408,"s,a
dπ⋆
h (s, a)VarPs,a(V ⋆
h+1) =
p"
D,0.8933107535986452,"SAC⋆/K · H
X h=1 sX"
D,0.8941574936494496,"s
dπ⋆
h (s, π⋆(s))VarPs,π⋆(s)(V ⋆
h+1) ≤
p"
D,0.8950042337002541,SAC⋆/K ·
D,0.8958509737510584,"v
u
u
tH · H
X"
D,0.8966977138018628,"h=1
Eπ⋆
h[VarP(·,·)(V ⋆
h+1)] ≤
p"
D,0.8975444538526672,H3SAC⋆/K.
D,0.8983911939034717,Published as a conference paper at ICLR 2022
D,0.8992379339542761,"H
AUXILIARY LEMMAS"
D,0.9000846740050804,"Lemma H.1 (Matrix McDiarmid inequality / Matrix Chernoff bound (Tropp, 2012)). Let zk, k =
1, . . . , K be independent random vectors in Rd, and let H be a mapping that maps K vectors to a
d × d symmetric matrix. Assume there exists a sequence of ﬁxed symmetric matrices {Ak}k∈[K] such
that for zk, z′
k ranges over all possible values for each k ∈[K], it holds
(H(z1, . . . , zk, . . . , zK) −H(z1, . . . , z′
k, . . . , zK))2 ⪯A2
k.
Deﬁne σ2 :=
P"
D,0.9009314140558848,"k A2
k
. Then for any t > 0,"
D,0.9017781541066893,"P {∥H(z1, . . . , zK) −EH(z1, . . . , zK)∥≥t} ≤d · exp
−t2 8σ2 "
D,0.9026248941574937,"Lemma H.2 (Hoeffding inequality for self-normalized martingales (Abbasi-Yadkori et al., 2011)).
Let {ηt}∞
t=1 be a real-valued stochastic process. Let {Ft}∞
t=0 be a ﬁltration, such that ηt is Ft-
measurable. Assume ηt also satisﬁes ηt given Ft−1 is zero-mean and R-subgaussian, i.e."
D,0.903471634208298,"∀λ ∈R,
E

eληt | Ft−1

≤eλ2R2/2"
D,0.9043183742591024,"Let {xt}∞
t=1 be an Rd-valued stochastic process where xt is Ft−1 measurable and ∥xt∥≤L. Let
Λt = λId + Pt
s=1 xsx⊤
s . Then for any δ > 0, with probability 1 −δ, for all t > 0, t
X"
D,0.9051651143099069,"s=1
xsηs  2 Λ−1
t"
D,0.9060118543607113,≤8R2 · d
LOG,0.9068585944115156,"2 log
λ + tL λδ 
."
LOG,0.90770533446232,"Lemma H.3 (Bernstein inequality for self-normalized martingales (Zhou et al., 2021a)). Let {ηt}∞
t=1
be a real-valued stochastic process. Let {Ft}∞
t=0 be a ﬁltration, such that ηt is Ft-measurable.
Assume ηt also satisﬁes
|ηt| ≤R, E [ηt | Ft−1] = 0, E

η2
t | Ft−1

≤σ2."
LOG,0.9085520745131245,"Let {xt}∞
t=1 be an Rd-valued stochastic process where xt is Ft−1 measurable and ∥xt∥≤L. Let
Λt = λId + Pt
s=1 xsx⊤
s . Then for any δ > 0, with probability 1 −δ, for all t > 0, t
X"
LOG,0.9093988145639289,"s=1
xsηs Λ−1
t ≤8σ s"
LOG,0.9102455546147333,"d log

1 + tL2 λd"
LOG,0.9110922946655376,"
· log
4t2 δ"
LOG,0.9119390347163421,"
+ 4R log
4t2 δ "
LOG,0.9127857747671465,"Lemma H.4 (Converting the variance under the matrix norm). Let Λ1 and Λ2 ∈Rd×d are two
positive semi-deﬁnite matrices. Then:
Λ−1
1
 ≤
Λ−1
2
 +
Λ−1
1
 ·
Λ−1
2
 · ∥Λ1 −Λ2∥
and"
LOG,0.9136325148179509,"∥φ∥Λ−1
1
≤

1 +
qΛ−1
2
 ∥Λ2∥·
Λ−1
1
 · ∥Λ1 −Λ2∥

· ∥φ∥Λ−1
2
."
LOG,0.9144792548687553,for all φ ∈Rd.
LOG,0.9153259949195597,"Proof. For the ﬁrst part, note
Λ−1
1
 ≤
Λ−1
2
 +
Λ−1
1
−Λ−1
2
 ≤
Λ−1
2
 +
Λ−1
2
 ∥Λ1 −Λ2∥
Λ−1
1"
LOG,0.9161727349703641,"For the second one,"
LOG,0.9170194750211685,"∥φ∥Λ−1
1
=
q"
LOG,0.9178662150719729,"φ⊤Λ−1
1 φ =
q"
LOG,0.9187129551227773,"φ⊤ 
Λ−1
1
−Λ−1
2

φ + φ⊤Λ−1
2 φ = r"
LOG,0.9195596951735817,"φ⊤Λ−1/2
2

Λ1/2
2
Λ−1
1 Λ1/2
2
−I + I

Λ−1/2
2
φ ≤ r"
LOG,0.9204064352243861,"∥φ∥Λ−1
2
·

1 +
Λ1/2
2
Λ−1
1 Λ1/2
2
−I


∥φ∥Λ−1
2"
LOG,0.9212531752751905,"≤

1 +
Λ1/2
2
Λ−1
1 Λ1/2
2
−I

1/2
· ∥φ∥Λ−1
2
=

1 +
Λ1/2
2
Λ−1
1
(Λ2 −Λ1) Λ−1
2 Λ1/2
2

1/2
· ∥φ∥Λ−1
2"
LOG,0.9220999153259949,"≤

1 +
q"
LOG,0.9229466553767993,"∥Λ2∥
Λ−1
1
 Λ−1
2
 ∥Λ1 −Λ2∥

· ∥φ∥Λ−1
2"
LOG,0.9237933954276037,Published as a conference paper at ICLR 2022
LOG,0.9246401354784082,"Lemma H.5 (Lemma H.4 of Min et al. (2021)). let φ : S × A →Rd satisﬁes ∥φ(s, a)∥≤C for
all s, a ∈S × A. For any K > 0, λ > 0, deﬁne ¯GK = PK
k=1 φ(sk, ak)φ(sk, ak)⊤+ λId where
(sk, ak)’s are i.i.d samples from some distribution ν. Then with probability 1 −δ,

¯GK K −Eν  ¯GK K"
LOG,0.9254868755292125," ≤4
√ 2C2
√ K"
LOG,0.9263336155800169,"
log 2d δ"
LOG,0.9271803556308214,"1/2
."
LOG,0.9280270956816258,"Proof of Lemma H.5. For completeness, we provide the proof of Lemma H.5. Let xk = φ(sk, ak).
Denote eΣh as the matrix obtained by replacing the k-th vector xk in bΣh by exk and leaving the rest
K −1 vectors unchanged. Then
 bΣh"
LOG,0.9288738357324301,"K −
eΣh K !2"
LOG,0.9297205757832345,"=
xkx⊤
k −˜xk˜x⊤
k
K"
LOG,0.930567315834039,"
⪯
1
K2
 
2xkx⊤
k xkx⊤
k + 2˜xk˜x⊤
k ˜xk˜x⊤
k

⪯4C4"
LOG,0.9314140558848434,"K2 Id := A2
k."
LOG,0.9322607959356477,"Notice that
PK
k A2
k
 = 4C4"
LOG,0.9331075359864521,"K , by Lemma H.1 we have the result."
LOG,0.9339542760372566,"Lemma H.6 (Lemma H.5. of Min et al. (2021)). Let φ : S × A →Rd be a bounded function s.t.
∥φ∥2 ≤C. Deﬁne ¯GK = PK
k=1 φ(sk, ak)φ(sk, ak)⊤+ λId where (sk, ak)’s are i.i.d samples from
some distribution ν. Let G = Eν[φ(s, a)φ(s, a)⊤]. Then for any δ ∈(0, 1), if K satisﬁes"
LOG,0.934801016088061,"K ≥max

512C4 G−12 log
2d δ"
LOG,0.9356477561388654,"
, 4λ
G−1

."
LOG,0.9364944961896697,"Then with probability at least 1 −δ, it holds simultaneously for all u ∈Rd that"
LOG,0.9373412362404742,"∥u∥¯
G−1
K ≤
2
√"
LOG,0.9381879762912786,"K
∥u∥G−1 ."
LOG,0.939034716342083,"Lemma H.7 (Extended Value Difference (Section B.1 in Cai et al. (2020))). Let π = {πh}H
h=1 and
π′ = {π′
h}H
h=1 be two arbitrary policies and let { bQh}H
h=1 be any given Q-functions. Then deﬁne
bVh(s) := ⟨bQh(s, ·), πh(· | s)⟩for all s ∈S. Then for all s ∈S,"
LOG,0.9398814563928873,"bV1(s) −V π′
1 (s) = H
X"
LOG,0.9407281964436918,"h=1
Eπ′
h
⟨bQh (sh, ·) , πh (· | sh) −π′
h (· | sh)⟩| s1 = s
i + H
X"
LOG,0.9415749364944962,"h=1
Eπ′
h
bQh (sh, ah) −

Th bVh+1

(sh, ah) | s1 = s
i
(30)"
LOG,0.9424216765453006,"where (ThV )(·, ·) := rh(·, ·) + (PhV )(·, ·) for any V ∈RS."
LOG,0.9432684165961049,"Proof. Denote ξh = bQh −Th bVh+1. For any h ∈[H], we have"
LOG,0.9441151566469094,"bVh −V π′
h
= ⟨bQh, πh⟩−⟨Qπ′
h , π′
h⟩"
LOG,0.9449618966977138,"= ⟨bQh, πh −π′
h⟩+ ⟨bQh −Qπ′
h , π′
h⟩"
LOG,0.9458086367485182,"= ⟨bQh, πh −π′
h⟩+ ⟨Ph(bVh+1 −V π′
h+1) + ξh, π′
h⟩"
LOG,0.9466553767993227,"= ⟨bQh, πh −π′
h⟩+ ⟨Ph(bVh+1 −V π′
h+1), π′
h⟩+ ⟨ξh, π′
h⟩"
LOG,0.947502116850127,"recursively apply the above for bVh+1 −V π′
h+1 and use the Eπ′ notation (instead of the inner product
of Ph, π′
h) we can ﬁnish the prove of this lemma."
LOG,0.9483488569009314,"Lemma H.8. Let bπ = {bπh}H
h=1 and bQh(·, ·) be the arbitrary policy and Q-function and also
bVh(s) = ⟨bQh(s, ·), bπh(·|s)⟩∀s ∈S. and ζh(s, a) := (Th bVh+1)(s, a) −bQh(s, a) (element-wisely)"
LOG,0.9491955969517358,Published as a conference paper at ICLR 2022
LOG,0.9500423370025403,"to be the Bellman update error. Then for any arbitrary π, we have"
LOG,0.9508890770533446,"V π
1 (s) −V bπ
1 (s) = H
X"
LOG,0.951735817104149,"h=1
Eπ [ζh(sh, ah) | s1 = s] − H
X"
LOG,0.9525825571549534,"h=1
Ebπ [ζh(sh, ah) | s1 = s] + H
X"
LOG,0.9534292972057579,"h=1
Eπ
h
⟨bQh (sh, ·) , πh (·|sh) −bπh (·|sh)⟩| s1 = x
i"
LOG,0.9542760372565622,"where the expectation are taken over sh, ah."
LOG,0.9551227773073666,Proof. Note the gap can be rewritten as
LOG,0.955969517358171,"V π
1 (s) −V bπ
1 (s) = V π
1 (s) −bV1(s) + bV1(s) −V bπ
1 (s)."
LOG,0.9568162574089755,"By Lemma H.7 with π = bπ, π′ = π, we directly have"
LOG,0.9576629974597799,"V π
1 (s)−bV1(s) = H
X"
LOG,0.9585097375105842,"h=1
Eπ [ζh(sh, ah) | s1 = s]+ H
X"
LOG,0.9593564775613886,"h=1
Eπ
h
⟨bQh (sh, ·) , πh (·|sh) −bπh (·|sh)⟩| s1 = s
i"
LOG,0.9602032176121931,"(31)
Next apply Lemma H.7 again with π = π′ = bπ, we directly have"
LOG,0.9610499576629975,"bV1(s) −V bπ
1 (s) = − H
X"
LOG,0.9618966977138018,"h=1
Ebπ [ζh(sh, ah) | s1 = s] .
(32)"
LOG,0.9627434377646062,Combine the above two results we prove the stated result.
LOG,0.9635901778154107,"Lemma H.9. For a linear MDP, for any 0 ≤V (·) ≤H, then there exists a wh ∈Rd s.t. ThV =
⟨φ, wh⟩and ∥wh∥2 ≤2H
√"
LOG,0.9644369178662151,"d for all h ∈[H]. Here Th(V )(s, a) = rh(x, a) + (PhV )(s, a).
Similarly, for any π, there exists wπ
h ∈Rd, such that Qπ
h = ⟨φ, wπ
h⟩with ∥wπ
h∥2 ≤2(H −h + 1)
√ d."
LOG,0.9652836579170194,"Proof. By deﬁnition,"
LOG,0.9661303979678239,"ThV = rh + (PhV ) = ⟨φ, θh⟩+ ⟨φ,
Z"
LOG,0.9669771380186283,"S
V (s)dνh(s)⟩"
LOG,0.9678238780694327,"⇒wh = θh +
Z"
LOG,0.9686706181202371,"S
V (s)dνh(s),"
LOG,0.9695173581710415,"therefore ∥wh∥2 ≤∥θh∥2 + H · ∥νh(S)∥≤1 + H
√"
LOG,0.9703640982218459,"d ≤2H
√"
LOG,0.9712108382726503,"d. The proof of the second part is
similar by backward induction and the fact V π
h ≤H −h + 1 for any π."
LOG,0.9720575783234547,"Lemma H.10. For any pessimistic bonus design Γh, suppose K > max{M1, M2, M3, M4}, then
with probability 1 −δ, Algorithm 1 yields"
LOG,0.9729043183742591,"Th bVh+1 −bTh bVh+1

∞≤eO(H2p d/κ
√ K
)"
LOG,0.9737510584250635,Published as a conference paper at ICLR 2022
LOG,0.9745977984758679,"Proof of Lemma H.10. Suppose wh is the coefﬁcient corresponding to the Th bVh+1 (such wh exists
by Lemma H.9), i.e. Th bVh+1 = φ⊤wh, and recall (bTh bVh+1)(s, a) = φ(s, a)⊤bwh, then:

Th bVh+1

(s, a) −

bTh bVh+1

(s, a) = φ(s, a)⊤(wh −bwh)"
LOG,0.9754445385266723,"=φ(s, a)⊤wh −φ(s, a)⊤bΛ−1
h K
X"
LOG,0.9762912785774767,"τ=1
φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

/bσ2
h(sτ
h, aτ
h) !"
LOG,0.9771380186282811,"= φ(s, a)⊤wh −φ(s, a)⊤bΛ−1
h K
X"
LOG,0.9779847586790855,"τ=1
φ (sτ
h, aτ
h) ·

Th bVh+1

(sτ
h, aτ
h) /bσ2
h(sτ
h, aτ
h) !"
LOG,0.97883149872989,"|
{z
}
(i)"
LOG,0.9796782387806944,"+ φ(s, a)⊤bΛ−1
h K
X"
LOG,0.9805249788314987,"τ=1
φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h)

/bσ2
h(sτ
h, aτ
h) !"
LOG,0.9813717188823031,"|
{z
}
(ii) . (33)"
LOG,0.9822184589331076,"For term (i), it is bounded by 2λH3√"
LOG,0.983065198983912,"d/κ
K
with probability 1 −δ by Lemma C.2."
LOG,0.9839119390347163,"For term (ii), by Cauchy inequality it is bounded by"
LOG,0.9847586790855207,"∥φ(s, a)∥bΛ−1
h ·  K
X"
LOG,0.9856054191363252,"τ=1
φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h)

/bσ2
h(sτ
h, aτ
h)"
LOG,0.9864521591871296,"bΛ−1
h ≤2H
√ κK  K
X"
LOG,0.9872988992379339,"τ=1
φ (sτ
h, aτ
h) ·

rτ
h + bVh+1
 
sτ
h+1

−

Th bVh+1

(sτ
h, aτ
h)

/bσ2
h(sτ
h, aτ
h)"
LOG,0.9881456392887383,"bΛ−1
h ≤2H
√"
LOG,0.9889923793395428,"κK
· ˜O(H
√"
LOG,0.9898391193903472,"d) = eO(H2p d/κ
√ K
),"
LOG,0.9906858594411516,"where the ﬁrst inequality is by Lemma H.6 (with φ′ = φ/bσh and ∥φ/bσh∥≤∥φ∥≤1 := C)
and the third inequality uses
√"
LOG,0.9915325994919559,"a⊤· A · a ≤
p"
LOG,0.9923793395427604,"∥a∥2 ∥A∥2 ∥a∥2 = ∥a∥2
p"
LOG,0.9932260795935648,∥A∥2 with a to be either
LOG,0.9940728196443692,"φ or wh. Moreover, λmin(˜Λp
h) ≥κ/ maxh,s,a bσh(s, a)2 ≥κ/H2 implies
(˜Λp
h)−1 ≤H2/κ."
LOG,0.9949195596951735,"The second inequality comes from Lemma H.2 with R = H since |ητ| = |(rτ
h + bVh+1
 
sτ
h+1

−
(Th bVh+1)(sτ
h, aτ
h))/bσh(sτ
h, aτ
h)| ≤H and |xτ| = |φ(sτ
h, aτ
h)/bσh(sτ
h, aτ
h)| ≤1."
LOG,0.995766299745978,"The
ﬁnal
result
is
obtained
by
absorbing
the
term
(i)
via
the
condition
K
>
max{M1, M2, M3, M4}."
LOG,0.9966130397967824,"Lemma H.11. Suppose random variables ∥X∥∞≤2H, ∥Y ∥∞≤2H, then"
LOG,0.9974597798475868,|Var(X) −Var(Y )| ≤8H · ∥X −Y ∥∞.
LOG,0.9983065198983911,Proof of Lemma H.11.
LOG,0.9991532599491956,"|Var(X) −Var(Y )| =|E[X2] −E[Y 2] −(E[X]2 −E[Y ]2)| = |E[(X + Y )(X −Y )] −(E[X + Y ])(E[X −Y ])|
≤E[|X + Y | · |X −Y |] + 4H · ∥X −Y ∥∞
≤4HE[|X −Y |] + 4H · ∥X −Y ∥∞= 8H · ∥X −Y ∥∞."
