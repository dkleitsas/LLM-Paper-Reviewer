Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0018248175182481751,"Federated learning (FL) is a trending training paradigm to utilize decentralized
training data. FL allows clients to update model parameters locally for several
epochs, then share them to a global model for aggregation. This training paradigm
with multi-local step updating before aggregation exposes unique vulnerabilities
to adversarial attacks. Adversarial training is a trending method to improve the
robustness of neural networks against adversarial perturbations. First, we formu-
late a general form of federated adversarial learning (FAL) that is adapted from
adversarial learning in the centralized setting. On the client side of FL training,
FAL has an inner loop to optimize an adversarial to generate adversarial samples
for adversarial training and an outer loop to update local model parameters. On
the server side, FAL aggregates local model updates and broadcast the aggregated
model. We design a global training loss to formulate FAL training as a min-max
optimization problem. Unlike the convergence analysis in centralized training that
relies on the gradient direction, it is signiﬁcantly harder to analyze the convergence
in FAL for three reasons: 1) the complexity of min-max optimization, 2) model
not updating in the gradient direction due to the multi-local updates on the client-
side before aggregation and 3) inter-client heterogeneity. Further, we address the
challenges using appropriate gradient approximation and coupling techniques and
present the convergence analysis in the over-parameterized regime. Our main
result theoretically shows that the minimal value of loss function under this al-
gorithm can converge to ϵ small with chosen learning rate and communication
rounds. It is noteworthy that our analysis is feasible for non-IID clients."
INTRODUCTION,0.0036496350364963502,"1
INTRODUCTION"
INTRODUCTION,0.005474452554744526,"Federated learning (FL) is playing an important role nowadays, as it allows different parties or clients
to collaboratively train deep learning models without sharing private data. One popular FL paradigm
called FedAvg (McMahan et al., 2017) introduces an easy-to-implement distributed learning method
without data sharing. Speciﬁcally, it requires a central server to aggregate model updates computed
by the local participants (also known as nodes or clients) using local imparticipable private data.
Then the central server aggregates these updates to train a globally learned model."
INTRODUCTION,0.0072992700729927005,"Nowadays deep learning model are exposed to severe threats of adversarial samples. Namely, small
adversarial perturbations on the inputs will dramatically change the outputs or output wrong an-
swers (Szegedy et al., 2013). In this regard, much effort has been made to improve neural networks’
resistance to such perturbations using adversarial learning (Tramèr et al., 2017; Samangouei et al.,
2018; Madry et al., 2018). Among these studies, the adversarial training scheme in Madry et al.
(2018) has achieved the good robustness in practice. Madry et al. (2018) proposes an adversarial
training scheme that uses projected gradient descent (PGD) to generate alternative adversarial sam-
ples as the augmented training set. Generating adversarial examples during neural network training
is considered as one of the most effective approaches for adversarial training up to now according to
the literature (Carlini & Wagner, 2017; Athalye et al., 2018; Croce & Hein, 2020)."
INTRODUCTION,0.009124087591240875,"Although adversarial learning has attracted much attention in the centralized domain, its practice
in FL is under-explored (Zizzo et al., 2020). Like training classical deep neural networks that use
gradient-based methods, FL paradigms are vulnerable to adversarial samples. Adversarial learning
in FL brings multiple open challenges due to FL properties on low convergence rate, application"
INTRODUCTION,0.010948905109489052,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.012773722627737226,"in non-IID environments, and secure aggregation solutions. Hence applying adversarial learning
in an FL paradigm may lead to unstable training loss and a lack of robustness. However, a recent
practical work Zizzo et al. (2020) observed that although there exist difﬁculties of convergence,
the federation of adversarial training with suitable hyperparameter settings can achieve adversarial
robustness and acceptable performance. Motivated by the empirical results, we want to address the
provable property of combining adversarial learning into FL from the theoretical perspective."
INTRODUCTION,0.014598540145985401,"This work aims to theoretically study the unexplored convergence challenges that lie in the interac-
tion between adversarial training and FL. To achieve a general understanding, we consider a general
form of federated adversarial learning (FAL), which deploys adversarial training scheme on local
clients in the most common FL paradigm, FedAvg (McMahan et al., 2017) system. Speciﬁcally,
FAL has an inner loop of local updating that generates adversarial samples (i.e., using Madry et al.
(2018)) for adversarial training and an outer loop to update local model weights on the client side.
Then global model is aggregated using FedAvg (McMahan et al., 2017). The algorithm is detailed
in Algorithm 1."
INTRODUCTION,0.016423357664233577,"We are interested in theoretically understanding the proposed FAL scheme from the aspects of model
robustness and convergence:"
INTRODUCTION,0.01824817518248175,"Can federated adversarial learning ﬁt training data robustly and converge with a feasibly sized
neural network?"
INTRODUCTION,0.020072992700729927,"The theoretical convergence analysis of adversarial training itself is challenging in the centralized
training setting. Tu et al. (2018) recently proposed a general theoretical method to analyze the risk
bound with adversaries but did not address the convergence problem. The investigation of conver-
gence on over-parameterized neural network has achieved tremendous progress (Du et al., 2019a;
Allen-Zhu et al., 2019b;c; Du et al., 2019b; Arora et al., 2019b). The basic statement is that training
can converge to sufﬁciently small training loss in polynomial iterations using gradient descent or
stochastic gradient descent when the width of the network is polynomial in the number of training
examples when initialized randomly. Recent theoretical analysis (Gao et al., 2019; Zhang et al.,
2020b) extends these standard training convergence results to adversarial training settings. To an-
swer the above interesting but challenging question, we formulate FAL as an min-max optimization
problem. We extend the convergence analysis on the general formulation of over-parameterized
neural networks in the FL setting that allows each client to perform min-max training and generate
adversarial examples (see Algorithm 1). Involved challenges are arising in FL convergence analysis
due to its unique optimization method: 1) unlike classical centralized setting, the global model of FL
does not update in the gradient direction; 2) inter-client heterogeneity issue needs to be considered."
INTRODUCTION,0.021897810218978103,"Despite the challenges, we give an afﬁrmative answer to the above question. To the best of our
knowledge, this work is the ﬁrst theoretical study that examines these unexplored challenges on the
convergence of adversarial training with FL. The contributions of this paper are:"
INTRODUCTION,0.023722627737226276,"• We propose a framework to analyze a general form of FAL in over-parameterized neural
networks. We follow a natural and valid assumption of data separability that the training
dataset are well separated apropos of the adversarial perturbations’ magnitude. After sufﬁ-
cient rounds of global communication and certain steps of local gradient descent for each
t, we obtain the minimal loss close to zero. Notably, our assumptions do not rely on data
distribution. Thus the proposed analysis framework is feasible for non-IID clients."
INTRODUCTION,0.025547445255474453,"• We are the ﬁrst to theoretically formulate the convergence of the FAL problem into a min-
max optimization framework with the proposed loss function. In FL, the update in the
global model is no longer directly determined by the gradient directions due to multiple
local steps. To tackle the challenges, we deﬁne a new ‘gradient’, FL gradient. With valid
ReLU Lipschitz and over-parameterized assumptions, we use gradient coupling for gra-
dient updates in FL to show the model updates of each global updating is bounded in
federated adversarial learning."
INTRODUCTION,0.02737226277372263,"Roadmap
The rest of the paper is organized as follows. We overview the related work on federated
learning and adversarial training in Section 2. We describe the problem formulation for the federated
adversarial learning algorithm and introduce the notations, assumptions to be used, and conditions
to be considered to ensure convergence in Section 3. Our main convergence result is presented in"
INTRODUCTION,0.029197080291970802,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03102189781021898,"Section 4. In Section 5, we present the core techniques and an overview of the proof. We summarize
and conclude our results in Section 6."
RELATED WORK,0.032846715328467155,"2
RELATED WORK"
RELATED WORK,0.03467153284671533,"Federated Learning
A efﬁcient and privacy-preserving way to learn from the distributed data
collected on the edge devices (a.k.a clients) would be FL. FedAvg is a easy-to-implement distributed
learning strategy by aggregating local model updates on the server side and transmitting the averaged
model back to local clients. Later, many FL methods are developed baed on FedAvg. Theses FL
schemes can be divided into aggregation schemes (McMahan et al., 2017; Wang et al., 2020; Li et al.,
2021) and optimization schemes (Reddi et al., 2020; Zhang et al., 2020a). Nearly all the them have
the common characteristics that client model are updating using gradient descent-based methods,
which is venerable to adversarial attacks. In addition, data heterogeneity brings in huge challeng
in FL. For IID data, FL has been proven effective. However, in practice, data mostly distribute as
non-IID. Non-IID data could substantially degrade the performance of FL models (Zhao et al., 2018;
Li et al., 2019; 2021; 2020a). Despite the potential risk in security and unstable performance in non-
IID setting, as FL mitigates the concern of data sharing, it is still a popular and practical solution for
distributed data learning in many real applications, such as healthcare (Li et al., 2020b; Rieke et al.,
2020), autonomous driving (Liang et al., 2019), IoTs (Wang et al., 2019; Lim et al., 2020)."
RELATED WORK,0.0364963503649635,"Learning with Adversaries
Since the discovery of adversarial examples (Szegedy et al., 2013), to
make neural networks robust to perturbations, efforts have been made to propose more effective de-
fense methods. As adversarial examples are an issue of robustness, the popular scheme is to include
learning with adversarial examples, which can be traced back to (Goodfellow et al., 2014). It pro-
duces adversarial examples and injecting them into training data. Later, Madry et al. (Madry et al.,
2018) proposed training on multi-step PGD adversaries and empirically observed that adversarial
training consistently achieves small and robust training loss in wide neural networks."
RELATED WORK,0.03832116788321168,"Federated Adversarial Learning
Adversarial examples, which may not be visually distinguish-
able from benign samples, are often classiﬁed. This poses potential security threats for practical
machine learning applications. Adversarial training (Goodfellow et al., 2014; Kurakin et al., 2016)
is a popular protocol to train more adversarial robust models by inserting adversarial examples in
training. The use of adversarial training in FL presents a number of open challenges, including
poor convergence due to multiple local update steps, instability and heterogeneity of clients, cost
and security request of communication, and so on. To defend the adversarial attacks in federated
learning, limited recent studies have proposed to include adversarial training on clients in the lo-
cal training steps (Bhagoji et al., 2019; Zizzo et al., 2020). These two works empirically showed
the performance of adversarial training. The theoretical analysis of convergence is under explored.
In addition, (Zhang et al., 2021) proposed an adversarial training strategy in classical distributed
setting, not meeting the specialty in FL."
RELATED WORK,0.040145985401459854,"Convergence via Over-parameterization
Convergence analysis on over-parameterized neural
networks falls in two lines. In the ﬁrst line of work (Li & Liang, 2018; Allen-Zhu et al., 2019b;c;a),
data separability plays a crucial role in deep learning theory, especially in showing the convergence
result of over-parameterized neural network training. Denote δ as the minimum distance between all
pairs of data points. Data separability theory requires the width (m) of the neural network is at least
polynomial factor of all the parameters (i.e. m ≥poly(n, d, 1/δ)), where n is the number of data
points and d is the dimension of data. Another line of work (Du et al., 2019b; Arora et al., 2019a;b;
Song & Yang, 2019; Lee et al., 2020) builds on neural tangent kernel (Jacot et al., 2018). It requires
the minimum eigenvalue (λ) of the neural tangent kernel to be lower bounded. Our analysis focuses
on the former approach based on data separability."
RELATED WORK,0.041970802919708027,"Robustness of Federated Learning
Previously there were several works that theoretically ana-
lyzed the robustness of federated learning under noise. Yin et al. (2018) developed distributed opti-
mization algorithms that were provably robust against arbitrary and potentially adversarial behavior
in distributed computing systems, and mainly focused on achieving optimal statistical performance.
Reisizadeh et al. (2020) developed a robust federated learning algorithm by considering a structured"
RELATED WORK,0.043795620437956206,Under review as a conference paper at ICLR 2022
RELATED WORK,0.04562043795620438,"afﬁne distribution shift in users’ data. Their analysis was built on several assumptions on the loss
functions without a direct connection to neural network."
PROBLEM FORMULATION,0.04744525547445255,"3
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.04927007299270073,"To explore the properties of FAL in deep learning, we formulate the problem in over-parameterized
neural network regime. We start by presenting the notations and setup required for federated adver-
sarial learning, then we will describe the loss function we use and our FAL algorithm."
PROBLEM FORMULATION,0.051094890510948905,"The rest of this section is organized as follows. In Section 3.1 we introduce the notations to be used.
In Section 3.2 we state the assumptions on the dataset and initialization values. In Section 3.3 we
formally describe our FAL algorithm (Algorithm 1) to be investigated."
NOTATIONS,0.05291970802919708,"3.1
NOTATIONS"
NOTATIONS,0.05474452554744526,"For a vector x, we use ∥x∥p to denote its ℓp norm, in this paper we mainly consider the situation
when p = 1, 2, or ∞. For a matrix W ∈Rd×m, we use W ⊤to denote its transpose and use tr[W] to
denote its trace. We use ∥W∥1, ∥W∥2 and ∥W∥F to denote the entry-wise ℓ1 norm, spectral norm
and Frobenius norm of W respectively. For each j ∈[m], we let Wj ∈Rd be the j-th column of W.
We let ∥W∥2,1 denotes Pm
j=1 ∥Wj∥2 and ∥W∥2,∞denotes maxj∈[m] ∥Wj∥2. We denote Gaussian
distribution with mean µ and covariance Σ as N(µ, Σ). We use σ(·) to denote the ReLU function
σ(z) = max{z, 0}, and use 1{E} to denote the indicator function of event E."
PROBLEM SETUP,0.05656934306569343,"3.2
PROBLEM SETUP"
PROBLEM SETUP,0.058394160583941604,"Two-layer ReLU network in FAL
Following recent theoretical work in understanding neural
networks training in deep learning (Du et al., 2019b; Arora et al., 2019a;b; Song & Yang, 2019; Lee
et al., 2020), in this paper, we focus on a two-layer neural network that has m neurons in the hidden
layer, where each neuron is a ReLU activation function. We deﬁne the global network as"
PROBLEM SETUP,0.060218978102189784,"fU(x) := m
X"
PROBLEM SETUP,0.06204379562043796,"r=1
ar · σ(⟨Ur, x⟩+ br)
(1)"
PROBLEM SETUP,0.06386861313868614,"and for c ∈[N], we deﬁne the local network of client c as"
PROBLEM SETUP,0.06569343065693431,"fWc(x) := m
X"
PROBLEM SETUP,0.06751824817518248,"r=1
ar · σ(⟨Wc,r, x⟩+ br).
(2)"
PROBLEM SETUP,0.06934306569343066,"Here U = (U1, . . . , Um) ∈Rd×m is the global hidden weight matrix, Wc = (Wc,1, . . . , Wc,m) ∈
Rd×m is the local hidden weight matrix of client c, and a = (a1, . . . , am) ∈Rm denotes the output
weight vector, b = (b1, . . . , bm) ∈Rm denotes the bias vector. During the process of federated
adversarial learning, we only update U and W, keeping a and b equal to their initialization values,
so we can write the global network as fU(x) and the local network as fWc(x). For the situation we
don’t care about the weight matrix, we write f(x) or fc(x) for short."
PROBLEM SETUP,0.07116788321167883,"Next, we make some standard assumptions regarding our training set.
Deﬁnition 3.1 (Dataset). There are N clients and n = NJ data in total.1 Let S = ∪c∈[N]Sc where
Sc = {(xc,1, yc,1), ..., (xc,J, yc,J)} ⊆Rd × R denotes the J training data of client c. Without loss
of generality, we assume that for all c ∈[N], j ∈[J], ∥xc,j∥2 = 1 and the last coordinate of each
point equals to 1/2 , so we consider X := {x ∈Rd : ∥x∥2 = 1, xd = 1/2}. For simplicity, we also
assume that for all c ∈[N], j ∈[J], |yc,j| ≤1.2"
PROBLEM SETUP,0.072992700729927,We now deﬁne the initialization for the neural networks.
PROBLEM SETUP,0.07481751824817519,"1Without loss of generality, we assume that all clients have same number of training data. Our result can be
generalized to the setting where each client has a different number of data as the future work.
2Our assumptions on data points are reasonable since we can do scale-up. In addition, l2 norm normalization
is a typical technique in experiments. Same assumptions also appears in many previous theoretical works like
Arora et al. (2019b); Allen-Zhu et al. (2019a;b)."
PROBLEM SETUP,0.07664233576642336,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.07846715328467153,"Deﬁnition 3.2 (Initialization). The initialization of a ∈Rm, U ∈Rd×m, b ∈Rm is a(0) ∈
Rm, U(0) ∈Rd×m, b(0) ∈Rm.
The initialization of client c’s local weight matrix Wc is
Wc(0, 0) = U(0). Here the second term in Wc denotes iteration of local steps."
PROBLEM SETUP,0.08029197080291971,"• For each r ∈[m], ar(0) are i.i.d. sampled from [−1/m1/3, +1/m1/3] uniformly."
PROBLEM SETUP,0.08211678832116788,"• For each i ∈[d], r ∈[m], Ui,r(0) and br(0) are i.i.d. random Gaussians sampled from
N(0, 1/m). Here Ui,r means the (i, r)-entry of U."
PROBLEM SETUP,0.08394160583941605,"For each global iteration t ∈[T],"
PROBLEM SETUP,0.08576642335766424,"• For each c ∈[N], the initial value of client c’s local weight matrix Wc is Wc(t, 0) = U(t)."
PROBLEM SETUP,0.08759124087591241,"Next we formulate the adversary model that will be used.
Deﬁnition 3.3 (ρ-Bounded adversary). Let F denote the function class. An adversary is a mapping
A : F × X × R →X which denotes the adversarial perturbation. For ρ > 0, we deﬁne the ℓ2
ball as B2(x, ρ) := {ex ∈Rd : ∥ex −x∥2 ≤ρ} ∩X, we say an adversary A is ρ-bounded if it
satisﬁes A(f, x, y) ∈B2(x, ρ). Moreover, given ρ > 0, we denote the worst-case adversary as
A∗:= argmaxex∈B2(x,ρ) ℓ(f(ex), y), where ℓis loss function deﬁned in Deﬁnition 3.5."
PROBLEM SETUP,0.08941605839416059,"Well-separated training sets
In the over-parameterized regime, it is a standard assumption that
the training set is well-separated. Since we deal with adversarial perturbations, we require the fol-
lowing γ-separability, which is a bit stronger.
Deﬁnition 3.4 (γ-separability). Let γ ∈(0, 1/2), δ ∈(0, 1/2), ρ ∈(0, 1/2) denote three parame-
ters such that γ ≤δ·(δ−2ρ). We say our training set S = ∪c∈[N]Sc = ∪c∈[N],j∈[J]{(xc,j, yc,j)} ⊂
Rd × R is globally γ-separable w.r.t a ρ-bounded adversary, if ∥xc1,j1 −xc2,j2∥2 ≥δ holds for any
c1 ̸= c2 and j1 ̸= j2."
PROBLEM SETUP,0.09124087591240876,"It is worth noting that, our problem setup does not require the assumption on independent and
identically distribution (IID) on data, thus such a formation can be applied to unique challenge of
the non-IID setting in FL."
FEDERATED ADVERSARIAL LEARNING,0.09306569343065693,"3.3
FEDERATED ADVERSARIAL LEARNING"
FEDERATED ADVERSARIAL LEARNING,0.0948905109489051,"Algorithm
We focus on a general FAL framework that is adapted from the most common adver-
sarial training in the classical setting on the client. Speciﬁcally, we describe the adversarial learning
of a local neural network fWc against an adversary A that generate adversarial examples during
training as shown in Algorithm 1. As for the analysis of a general theoretical analysis framework,
we do not specify the explicit format of A."
FEDERATED ADVERSARIAL LEARNING,0.09671532846715329,"The FAL algorithm contains two procedures: one is CLIENTUPDATE running on client side and the
other is SERVEREXECUTION running on server side. These two procedures are iteratively processed
through communication iterations. Adversarial training is addressed in procedure CLIENTUPDATE.
Hence, there are two loops in CLIENTUPDATE procedure: the outer loop is iteration for local model
updating; and the inner loop is iteratively generating adversarial samples by the adversary A. In
the outer loop in SERVEREXECUTION procedure, the neural network’s parameters are updated to
reduce its prediction loss on the new adversarial samples."
FEDERATED ADVERSARIAL LEARNING,0.09854014598540146,"Adversary and robust loss
We set the following loss for the sake of technical presentation sim-
plicity, as is customary in prior studies Gao et al. (2019); Allen-Zhu et al. (2019a):
Deﬁnition 3.5 (Lipschitz convex loss). A loss function ℓ: R × R →R is said to be a Lipschitz
convex loss, if it satisﬁes the following four properties:"
FEDERATED ADVERSARIAL LEARNING,0.10036496350364964,• non-negative;
FEDERATED ADVERSARIAL LEARNING,0.10218978102189781,• convex with respect to the ﬁrst input of ℓ;
FEDERATED ADVERSARIAL LEARNING,0.10401459854014598,"• 1−Lipshcitz, which means ∥ℓ(x1, y1) −ℓ(x2, y2)∥2 ≤∥(x1, y1) −(x2, y2)∥2;"
FEDERATED ADVERSARIAL LEARNING,0.10583941605839416,"• ℓ(y, y) = 0 for all y ∈R."
FEDERATED ADVERSARIAL LEARNING,0.10766423357664233,Under review as a conference paper at ICLR 2022
FEDERATED ADVERSARIAL LEARNING,0.10948905109489052,"Algorithm 1 Federated Adversarial Learning (FAL)
Notations: Training sets of clients with each client is indexed by c, Sc = {(xc,j, yc,j)}J
j=1; adver-
sary A; local learning rate ηlocal; global learning rate ηglobal; local updating iterations K; global
communication round T."
FEDERATED ADVERSARIAL LEARNING,0.11131386861313869,"1: Initialization a(0) ∈Rm, U(0) ∈Rd×m, b(0) ∈Rm"
FEDERATED ADVERSARIAL LEARNING,0.11313868613138686,"2: For t = 0 →T, we iteratively run Procedure A then Procedure B
3: procedure A. CLIENTUPDATE(t, c)
4:
Sc(t) ←∅
5:
Wc(t, 0) ←U(t)
▷Receive global model weights update.
6:
for k = 0 →K −1 do
7:
for j = 1 →J do
8:
ex(t)
c,j ←A(fWc(t,k), xc,j, yc,j)
▷Adversarial samples. fWc is deﬁned as (2)."
FEDERATED ADVERSARIAL LEARNING,0.11496350364963503,"9:
Sc(t) ←Sc(t) ∪(ex(t)
c,j, yc,j)
10:
end for
11:
Wc(t, k + 1) ←Wc(t, k) −ηlocal · ∇WcL(fWc(t,k), Sc(t))
12:
end for
13:
∆Uc(t) ←Wc(t, K) −U(t)
14:
Send ∆Uc(t) to SERVEREXECUTION
15: end procedure
16: procedure B. SERVEREXECUTION(t):
17:
for each client c in parallel do do
18:
∆Uc(t) ←CLIENTUPDATE(c, t)
▷Receive local model weights update.
19:
∆U(t) ←1 N
P"
FEDERATED ADVERSARIAL LEARNING,0.11678832116788321,"c∈[N] ∆Uc(t)
20:
U(t + 1) ←U(t) + ηglobal · ∆U(t)
▷Aggregation on the server side.
21:
Send U(t + 1) to client c for CLIENTUPDATE(c, t)
22:
end for
23: end procedure"
FEDERATED ADVERSARIAL LEARNING,0.11861313868613138,"In this paper we assume ℓis a Lipschitz convex loss. Next, we deﬁne our robust loss function of a
neural network, which is based on the adversarial examples generated by a ρ-bounded adversary A.
Deﬁnition 3.6 (Training loss). Given a client’s training set Sc = {(xc,j, yc,j)}J
j=1 ⊂Rd × R of
J examples, the standard training loss of a neural net fc : Rd →R is deﬁned as L(fc, Sc) :=
1
J
PJ
j=1 ℓ(fc(xc,j), yc,j).
Given S = ∪c∈[N]Sc, we deﬁne the global loss as L(fU, S) :="
"NJ
PN",0.12043795620437957,"1
NJ
PN
c=1
PJ
j=1 ℓ(fU(xc,j), yc,j). Given a ρ-bounded adversary A, we deﬁne the global loss with
respect to A as"
"NJ
PN",0.12226277372262774,"LA(fU) :=
1
NJ N
X c=1 J
X"
"NJ
PN",0.12408759124087591,"j=1
ℓ(fU(A(fc, xc,j, yc,j)), yc,j) =
1
NJ N
X c=1 J
X"
"NJ
PN",0.1259124087591241,"j=1
ℓ(fU(exc,j), yc,j)"
"NJ
PN",0.12773722627737227,and also deﬁne the worst-case global robust loss as
"NJ
PN",0.12956204379562045,"LA∗(fU) :=
1
NJ N
X c=1 J
X"
"NJ
PN",0.13138686131386862,"j=1
ℓ(fU(A∗(fc, xc,j, yc,j)), yc,j) =
1
NJ N
X c=1 J
X"
"NJ
PN",0.1332116788321168,"j=1
max
x∗
c,j∈B2(xc,j,ρ) ℓ
 
fU(x∗
c,j), yc,j

."
"NJ
PN",0.13503649635036497,"Moreover, since we deal with pseudo-net which is deﬁned in Deﬁnition 5.1, we also de-
ﬁne the loss of a pseudo-net as L(gc, Sc)
:=
1
J
PJ
j=1 ℓ(gc(xc,j), yc,j) and L(gU, S)
:="
"NJ
PN",0.13686131386861314,"1
NJ
PN
c=1
PJ
j=1 ℓ(gU(xc,j), yc,j)."
OUR RESULT,0.1386861313868613,"4
OUR RESULT"
OUR RESULT,0.14051094890510948,"The main result of this work is showing the convergence of FAL algorithm (Algorithm 1) in overpa-
rameterized neural networks. Speciﬁcally, our deﬁned global training loss (Deﬁnition 3.6) converges
to a small ϵ with the chosen communication round T, local and global learning rate ηlocal, ηglobal.
It is plausible to see that we can control ηlocal according to the local update steps K to achieve
convergence. We now formally present our main result Theorem 4.1."
OUR RESULT,0.14233576642335766,Under review as a conference paper at ICLR 2022
OUR RESULT,0.14416058394160583,"Theorem 4.1 (Federated Adversarial Learning). Let c0 ∈(0, 1) be a ﬁxed constant. Let N denotes
the total number of clients and J denotes the number of data points per client. Suppose that our
training set S = ∪c∈[N]Sc is globally γ-separable for some γ > 0. Then, for all ϵ ∈(0, 1),
there exists R = poly((NJ/ϵ)1/γ) that satisﬁes: for all m ≥poly(d, (NJ/ϵ)1/γ), for every
K ≥1 and T ≥poly(R/ϵ), with probability at least 1 −exp(−Ω(m1/3)) over the randomness
of a(0) ∈Rm, U(0) ∈Rd×m, b(0) ∈Rm, running federated adversarial learning (Algorithm 1)
with step size choices ηglobal = 1/ poly(NJ, R, 1/ϵ) and ηlocal = 1/K will output a list of weights
{U(1), U(2), · · · , U(T)} ∈Rd×m that satisfy"
OUR RESULT,0.145985401459854,"min
t∈[T ] LA(fU(t)) ≤ϵ."
PROOF SKETCH,0.1478102189781022,"5
PROOF SKETCH"
PROOF SKETCH,0.14963503649635038,"To handle the min-max objective in FAL, we formulate the optimization of FAL in the framework
of online gradient descent3 : at each local step k on the client side, ﬁrstly the adversary generates
adversarial samples and computes the loss function L
 
fWc(t,k), Sc(t)

, then the local client learner
takes the fresh loss function and update Wc(t, k + 1) = Wc(t, k) −ηlocal · ∇WcL
 
fWc(t,k), Sc(t)

."
PROOF SKETCH,0.15145985401459855,"Compared with the centralized setting, the key difﬁculties in the convergence analysis of FL are
induced by multiple local updates on the client side and the updates on both local and global sides.
Speciﬁcally, local updates are not the standard gradient as the centralized adversarial training when
K ≥2. We used −∆U(t) in substitution of the real gradient of U to update the value of U(t). This
brings in challenges to bound the gradient of the neural networks. Nevertheless, gradient bounding
is challenging in adversarial training solely. To this end, we use gradient coupling method twice
to solve this core problem: ﬁrstly we bound the difference between real gradient and FL gradient
(deﬁned below), then we bound the difference between pseudo gradient and real gradient."
EXISTENCE OF SMALL ROBUST LOSS,0.15328467153284672,"5.1
EXISTENCE OF SMALL ROBUST LOSS"
EXISTENCE OF SMALL ROBUST LOSS,0.1551094890510949,"In this section, we denote eU = U(0) as the initialization of global weights U and denote U(t) as
the global weights of communication round t. U ∗is the value of U after small perturbations from
eU which satisﬁes ∥U ∗−eU∥2,∞≤R/mc1, here c1 ∈(0, 1) is a constant (e.g. c1 = 2/3), m is
the width of the neural network and R is a parameter. We will specify the concrete value of these
parameters later in appendix."
EXISTENCE OF SMALL ROBUST LOSS,0.15693430656934307,"We study the over-parameterized neural nets’ well-approximated pseudo-network to learn gradient
descent for over-parameterized neural nets whose weights are close to initialization. Pseudo-network
can be seen as a linear approximation of our two layer ReLU neural network near initialization, and
the introducing of pseudo-network makes the proof more intuitive."
EXISTENCE OF SMALL ROBUST LOSS,0.15875912408759124,"Deﬁnition 5.1 (Pseudo-network). Given weights U ∈Rd×m, a ∈Rm and b ∈Rm, for a neural
network fU(x) = Pm
r=1 ar · σ(⟨Ur, x⟩+ br), we deﬁne the corresponding pseudo-network gU :
Rd →R as gU(x) := Pm
r=1 ar · ⟨Ur(t) −Ur(0), x⟩· 1{⟨Ur(0), x⟩+ br ≥0}."
EXISTENCE OF SMALL ROBUST LOSS,0.16058394160583941,"Existence of small robust loss
In order to obtain our main result, we ﬁrst show that we can ﬁnd
a U ∗which is close to U(0) and also makes LA∗(fU ∗) sufﬁciently small. Later in Theorem 5.6 we
show that the average of LA(fU(t)) is dominated by LA∗(fU ∗), thus we can prove the minimum of
LA(fU(t)) is ϵ small."
EXISTENCE OF SMALL ROBUST LOSS,0.1624087591240876,"Theorem 5.2 (Existence, informal version of Theorem F.3). For all ϵ ∈(0, 1), there exists M0 =
poly(d, (NJ/ϵ)1/γ) and R = poly((NJ/ϵ)1/γ) such that for every m ≥M0, with high probability
there exists U ∗∈Rd×m that satisﬁes ∥U ∗−U(0)∥2,∞≤R/mc1 and LA∗(fU ∗) ≤ϵ."
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.16423357664233576,"5.2
CONVERGENCE RESULT FOR FEDERATED LEARNING"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.16605839416058393,"For ease of presentation, we ﬁrst describe the notions of local and global gradients in our federated
adversarial learning setting."
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.1678832116788321,3We refer our readers to Hazan (2016) for more details regarding online gradient descent.
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.16970802919708028,Under review as a conference paper at ICLR 2022
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.17153284671532848,"Deﬁnition 5.3 (Gradient). For a local real network fWc(t,k), we denote its gradient by ∇(fc, t, k) :=
∇WcL(fWc(t,k), Sc(t)). If the corresponding pseudo-network is gWc(t,k), then denote the pseudo-
network gradient by ∇(gc, t, k) := ∇WcL(gWc(t,k), Sc(t))."
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.17335766423357665,"Now we consider the global network. We deﬁne pseudo gradient as ∇(g, t) := ∇UL(gU(t), S(t))
and deﬁne FL gradient as e∇(f, t) := −1"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.17518248175182483,"N ∆U(t), which is used in the proof of Theorem 5.6. We
present our gradient coupling methods in the following two lemmas.
Lemma 5.4 (Bounding the difference between real gradient and FL gradient, informal version of
Lemma E.4). With probability at least 1 −exp(−Ω(mc0)) over the initialization, for iterations t
such that ∥U(t) −U(0)∥2,∞≤1/o(m), the gradients satisfy ∥∇(f, t) −e∇(f, t)∥2,1 ≤o(m).
Lemma 5.5 (Bounding the difference between pseudo gradient and real gradient, informal version
of Lemma E.5). With probability at least 1 −exp(−Ω(mc0)) over the initialization, for iterations t
such that ∥U(t) −U(0)∥2,∞≤1/o(m), the gradients satisfy ∥∇(g, t) −∇(f, t)∥2,1 ≲NJ · o(m)."
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.177007299270073,"The above two lemmas are essential in proving Theorem 5.6, which is our convergence result.
Theorem 5.6 (Convergence result, informal version of Theorem E.3). For all ϵ ∈(0, 1), R ≥1,
there exists an M = poly(n, R, 1/ϵ), such that for every m ≥M, for every K ≥1, for every
T ≥poly(R/ϵ), with probability at least 1 −exp(−Ω(mc0)) over the randomness of a(0) ∈Rm,
U(0) ∈Rd×m, b(0) ∈Rm, for all U ∗such that ∥U ∗−U(0)∥2,∞≤R/mc1, running Algorithm 1
with setting ηglobal = 1/ poly(NJ, R, 1/ϵ) and ηlocal = 1/K will output weights (U(t))T
t=1 that
satisfy 1"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.17883211678832117,"T
PT
t=1 LA
 
fU(t)

≤LA∗(fU ∗) + ϵ."
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.18065693430656934,"In the proof of Theorem 5.6 we ﬁrst bound the local gradient ∇r(fc, t, k).
We consider the
pseudo-network and bound L(gU(t), S(t)) −L(gU ∗, S(t)) ≤α(t) + β(t) + γ(t), where α(t) :=
⟨e∇(f, t), U(t) −U ∗⟩, β(t) = ∥∇(f, t) −e∇(f, t)∥2,1 · ∥U(t) −U ∗∥2,∞and γ(t) := ∥∇(g, t) −
∇(f, t)∥2,1 · ∥U(t) −U ∗∥2,∞. In bounding α(t), we unfold ∥U(t + 1) −U ∗∥2
F and by rearranging
we have"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.18248175182481752,α(t) = ηglobal
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.1843065693430657,"2
∥∆U(t)∥2
F +
1
2ηglobal
· (∥U(t) −U ∗∥2
F −∥U(t + 1) −U ∗∥2
F )."
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.18613138686131386,"We bound ∥∆U(t)∥2
F ≤ηlocalK · o(m). By doing summation over t, we have T
X"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.18795620437956204,"t=1
α(t) = ηglobal 2 T
X"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.1897810218978102,"t=1
∥∆U(t)∥2
F +
1
2ηglobal
· T
X"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.19160583941605838,"t=1
(∥U(t) −U ∗∥2
F −∥U(t + 1) −U ∗∥2
F )"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.19343065693430658,"≤ηglobal 2 T
X"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.19525547445255476,"t=1
∥∆U(t)∥2
F +
1
2ηglobal
· ∥U(1) −U ∗∥2
F"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.19708029197080293,"≲ηglobalηlocalTK · o(m) +
1
ηglobal
mD2
U ∗"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.1989051094890511,"In bounding β(t), we apply Lemma 5.4 and have"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.20072992700729927,"β(t) = ∥∇(f, t) −e∇(f, t)∥2,1 · ∥U(t) −U ∗∥2,∞
≲o(m) · ∥U(t) −U ∗∥2,∞"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.20255474452554745,"≲o(m) · (∥U(t) −eU∥2,∞+ DU ∗)."
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.20437956204379562,"where DU ∗:= ∥U ∗−eU∥2,∞≤R/mc1. As for the ﬁrst term, we bound"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.2062043795620438,"∥U(t) −eU∥2,∞≤ηglobal t
X"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.20802919708029197,"τ=1
∥∆U(τ)∥2,∞"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.20985401459854014,"= ηglobal t
X"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.2116788321167883,"τ=1
∥ηlocal N N
X c=1 K−1
X"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.21350364963503649,"k=0
∇(fc, t, k)∥2,∞"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.21532846715328466,"≤ηglobalηlocal N t
X τ=1 N
X c=1 K−1
X"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.21715328467153286,"k=0
∥∇(fc, t, k)∥2,∞"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.21897810218978103,≤ηglobalηlocaltKm−1/3
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.2208029197080292,Under review as a conference paper at ICLR 2022
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.22262773722627738,"and have β(t) ≲ηglobalηlocaltK · o(m) + o(m) · DU ∗, then we do summation and obtain T
X"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.22445255474452555,"t=1
β(t) ≲ηglobalηlocalT 2K · o(m) + o(m) · TDU ∗."
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.22627737226277372,"In bounding γ(t), we apply Lemma 5.5 and have"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.2281021897810219,"γ(t) = ∥∇(g, t) −∇(f, t)∥2,1 · ∥U(t) −U ∗∥2,∞≲NJ · o(m) · (∥U(t) −eU∥2,∞+ DU ∗)."
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.22992700729927007,"Then we do summation over t and have T
X"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.23175182481751824,"t=1
γ(t) ≲ηglobalηlocalT 2KNJ · o(m) + TNJ · o(m)DU ∗"
CONVERGENCE RESULT FOR FEDERATED LEARNING,0.23357664233576642,"Putting it together with our choice of our all parameters (i.e. ηlocal, ηglobal, R, K, T, m), we obtain"
T,0.2354014598540146,"1
T T
X"
T,0.23722627737226276,"t=1
L(gU(t), S(t)) −1 T T
X"
T,0.23905109489051096,"t=1
L(gU ∗, S(t)) ≤1 T ( T
X"
T,0.24087591240875914,"t=1
α(t) + T
X"
T,0.2427007299270073,"t=1
β(t) + T
X"
T,0.24452554744525548,"t=1
γ(t)) ≤O(ϵ)."
T,0.24635036496350365,"From Theorem D.2 in appendix, we have: supx∈X |fU(x) −gU(x)| ≤O(ϵ), and thus,"
T,0.24817518248175183,"1
T T
X"
T,0.25,"t=1
L(fU(t), S(t)) −1 T T
X"
T,0.2518248175182482,"t=1
L(fU ∗, S(t)) ≤O(ϵ).
(3)"
T,0.25364963503649635,"From the deﬁnition of A∗we have L(fU ∗, S(t)) ≤LA∗(fU ∗). From the deﬁnition of loss we have
L(fU(t), S(t)) = LA(fU(t)). Moreover, since Eq. (3) holds for all ϵ > 0, we can replace O(ϵ) with
ϵ. Thus we prove that for ∀ϵ > 0,"
T,0.25547445255474455,"1
T T
X"
T,0.2572992700729927,"t=1
LA(fU(t)) ≤LA∗(fU ∗) + ϵ."
T,0.2591240875912409,"Combining the results
From Theorem 5.2 we obtain U ∗that is close to U(0) and makes
LA∗(fU ∗) close to zero, from Theorem 5.6 we have that the average of LA(fU(t)) is dominated
by LA∗(fU ∗). By aggregating these two results, we prove that the minimal of LA(fU(t)) is ϵ small
and ﬁnish the proof of our main Theorem 4.1."
CONCLUSION,0.26094890510948904,"6
CONCLUSION"
CONCLUSION,0.26277372262773724,"We have studied the convergence of a general format of adopting adversarial training in FL setting
to improve FL training robustness. We propose the general framework, FAL, which deploys adver-
sarial samples generation-based adversarial training method on the client-side and then aggregate
local model using FedAvg (McMahan et al., 2017). In FAL, each client is trained via min-max opti-
mization with inner loop adversarial generation and outer loop loss minimization. To the best of our
knowledge, we are the ﬁrst to present the comprehensive proof of theoretical convergence guarantee
for over-parameterized ReLU network on the presented FAL strategy, using gradient descent. Unlike
the convergence of adversarial training in classical settings, we consider the updates on both local
client and global server sides. Our result indicates that we can control learning rates ηlocal and ηglobal
according to the local update steps K and global communication round T to make the minimal loss
close to zero. The technical challenges lie in the multiple local update steps and heterogeneous data,
leading to the difﬁculties of convergence. Under ReLU Lipschitz and over-prameterization assump-
tions, we use gradient coupling methods twice. Together, we show the model updates of each global
updating bounded in our federated adversarial learning. Note that we do not require IID assump-
tions for data distribution. In sum, the proposed FAL formulation and analysis framework can well
handle the multi-local updates and non-IID data in FL. Moreover, our framework can be generalized
to other FL aggregation methods, such as sketching and selective aggregation."
CONCLUSION,0.2645985401459854,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.2664233576642336,ETHICS STATEMENT
ETHICS STATEMENT,0.26824817518248173,"Ensuring the security and robustness of the deployed algorithms is of paramount importance in AI
algorithms nowadays. Recently, machine learning training has revealed its vulnerability to adver-
sarial attacks and tendency to generate wrong predictions. The tread cannot be underestimated in
FL, where the heterogeneity among clients and requirement for efﬁcient communication brings in
challenges in stable gradient updates. In this regard, our work theoretically shows the feasibility of
FAL. As a theoretical work, we do not involve any human subjects or datasets. Our work has the
potential positive impact on the machine learning community."
REPRODUCIBILITY STATEMENT,0.27007299270072993,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.2718978102189781,We provide the proof details in the appendix to ensure reproducibility.
REFERENCES,0.2737226277372263,REFERENCES
REFERENCES,0.2755474452554745,"Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In NeurIPS. https://arxiv.org/pdf/
1811.04918.pdf, 2019a."
REFERENCES,0.2773722627737226,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In NeurIPS. https://arxiv.org/pdf/1810.12065.pdf, 2019b."
REFERENCES,0.2791970802919708,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In ICML. https://arxiv.org/pdf/1811.03962.pdf, 2019c."
REFERENCES,0.28102189781021897,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an inﬁnitely wide neural net. In NeurIPS. https://arxiv.org/
pdf/1904.11955.pdf, 2019a."
REFERENCES,0.28284671532846717,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. In ICML.
https://arxiv.org/pdf/1901.08584.pdf, 2019b."
REFERENCES,0.2846715328467153,"Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420,
2018."
REFERENCES,0.2864963503649635,"Sergei Bernstein. On a modiﬁcation of chebyshev’s inequality and of the error formula of laplace.
Ann. Sci. Inst. Sav. Ukraine, Sect. Math, 1(4):38–49, 1924."
REFERENCES,0.28832116788321166,"Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated
learning through an adversarial lens. In International Conference on Machine Learning, pp. 634–
643. PMLR, 2019."
REFERENCES,0.29014598540145986,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39–57. IEEE, 2017."
REFERENCES,0.291970802919708,"Herman Chernoff. A measure of asymptotic efﬁciency for tests of a hypothesis based on the sum of
observations. The Annals of Mathematical Statistics, pp. 493–507, 1952."
REFERENCES,0.2937956204379562,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In International Conference on Machine Learning, pp. 2206–
2216. PMLR, 2020."
REFERENCES,0.2956204379562044,"Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global
minima of deep neural networks.
In ICML. https://arxiv.org/pdf/1811.03804,
2019a."
REFERENCES,0.29744525547445255,"Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In ICLR. https://arxiv.org/pdf/1810.02054.
pdf, 2019b."
REFERENCES,0.29927007299270075,Under review as a conference paper at ICLR 2022
REFERENCES,0.3010948905109489,"Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason D Lee. Convergence
of adversarial training in overparametrized neural networks. Advances in Neural Information
Processing Systems, 32:13029–13040, 2019."
REFERENCES,0.3029197080291971,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.30474452554744524,"Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization,
2(3-4):157–325, 2016."
REFERENCES,0.30656934306569344,"Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13–30, 1963."
REFERENCES,0.3083941605839416,"Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems (NeurIPS),
pp. 8571–8580, 2018."
REFERENCES,0.3102189781021898,"Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016."
REFERENCES,0.31204379562043794,"Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu. Generalized leverage score
sampling for neural networks. In NeurIPS, 2020."
REFERENCES,0.31386861313868614,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. In Conference on Machine Learning and
Systems, 2020a, 2020a."
REFERENCES,0.3156934306569343,"Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019."
REFERENCES,0.3175182481751825,"Xiaoxiao Li, Yufeng Gu, Nicha Dvornek, Lawrence Staib, Pamela Ventola, and James S Dun-
can. Multi-site fmri analysis using privacy-preserving federated learning and domain adaptation:
Abide results. Medical Image Analysis, 2020b."
REFERENCES,0.3193430656934307,"Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learn-
ing on non-iid features via local batch normalization. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=6YEQUn0QICG."
REFERENCES,0.32116788321167883,"Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In NeurIPS. https://arxiv.org/pdf/1808.01204.pdf,
2018."
REFERENCES,0.32299270072992703,"Xinle Liang, Yang Liu, Tianjian Chen, Ming Liu, and Qiang Yang. Federated transfer reinforcement
learning for autonomous driving. arXiv preprint arXiv:1910.06001, 2019."
REFERENCES,0.3248175182481752,"Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang Liang,
Qiang Yang, Dusit Niyato, and Chunyan Miao. Federated learning in mobile edge networks: A
comprehensive survey. IEEE Communications Surveys & Tutorials, 22(3):2031–2063, 2020."
REFERENCES,0.3266423357664234,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR. https://arxiv.
org/pdf/1706.06083.pdf, 2018."
REFERENCES,0.3284671532846715,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelli-
gence and Statistics, pp. 1273–1282. PMLR, 2017."
REFERENCES,0.3302919708029197,"Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn`y,
Sanjiv Kumar, and H Brendan McMahan.
Adaptive federated optimization.
arXiv preprint
arXiv:2003.00295, 2020."
REFERENCES,0.33211678832116787,"Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated
learning: The case of afﬁne distribution shifts. arXiv preprint arXiv:2006.08907, 2020."
REFERENCES,0.33394160583941607,Under review as a conference paper at ICLR 2022
REFERENCES,0.3357664233576642,"Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyri-
don Bakas, Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al. The future of digital
health with federated learning. NPJ digital medicine, 3(1):1–7, 2020."
REFERENCES,0.3375912408759124,"Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classiﬁers against
adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018."
REFERENCES,0.33941605839416056,"Zhao Song and Xin Yang. Quadratic sufﬁces for over-parametrization via matrix chernoff bound. In
arXiv preprint. https://arxiv.org/pdf/1906.03593.pdf, 2019."
REFERENCES,0.34124087591240876,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In arXiv preprint. https://arxiv.
org/pdf/1312.6199.pdf, 2013."
REFERENCES,0.34306569343065696,"Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,
2017."
REFERENCES,0.3448905109489051,"Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and Trends R⃝in
Machine Learning, 8(1-2):1–230, 2015."
REFERENCES,0.3467153284671533,"Zhuozhuo Tu, Jingwei Zhang, and Dacheng Tao. Theoretical analysis of adversarial learning: A
minimax approach. arXiv preprint arXiv:1811.05232, 2018."
REFERENCES,0.34854014598540145,"Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective in-
consistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481,
2020."
REFERENCES,0.35036496350364965,"Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K. Leung, Christian Makaya, Ting He, and
Kevin Chan. Adaptive federated learning in resource constrained edge computing systems. IEEE
Journal on Selected Areas in Communications, 37(6):1205–1221, 2019."
REFERENCES,0.3521897810218978,"Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning,
pp. 5650–5659. PMLR, 2018."
REFERENCES,0.354014598540146,"Gaoyuan Zhang, Songtao Lu, Sijia Liu, Xiangyi Chen, Pin-Yu Chen, Lee Martie, and Mingyi
Horesh, Lior abd Hong. Distributed adversarial training to robustify deep neural networks at
scale. 2021. URL https://openreview.net/pdf?id=kmBFHJ5pr0o."
REFERENCES,0.35583941605839414,"Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. Fedpd: A federated learning
framework with optimal rates and adaptivity to non-iid data. arXiv preprint arXiv:2005.11418,
2020a."
REFERENCES,0.35766423357664234,"Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song, and Sanjeev Arora.
Over-
parameterized adversarial training: An analysis overcoming the curse of dimensionality.
In
NeurIPS. arXiv preprint arXiv:2002.06668, 2020b."
REFERENCES,0.3594890510948905,"Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018."
REFERENCES,0.3613138686131387,"Giulio Zizzo, Ambrish Rawat, Mathieu Sinn, and Beat Buesser. Fat: Federated adversarial training.
arXiv preprint arXiv:2012.01791, 2020."
REFERENCES,0.36313868613138683,Under review as a conference paper at ICLR 2022
REFERENCES,0.36496350364963503,"Roadmap
The appendix is organized as follows. We introduce the probability tools to be used
in our proof in Section A. In addition, we introduce the preliminaries in Section B. We present the
proof overview in Section C and additional remarks used in the proof sketch in Section D. We show
the detailed proof for the convergence in Section E and the detailed proof of existence in Section F
correspondingly."
REFERENCES,0.36678832116788324,"A
PROBABILITY TOOLS"
REFERENCES,0.3686131386861314,"We introduce the probability tools that will be used in our proof. First we show three lemmas about
the tail bounds for random scalar variables in Lemma A.1, A.2 and A.3:
Lemma A.1 (Chernoff bound Chernoff (1952)). Let X = Pn
i=1 Xi, where Xi = 1 with probability
pi and Xi = 0 with probability 1 −pi, and all Xi are independent. Let µ = E[X] = Pn
i=1 pi. Then
1. Pr[X ≥(1 + δ)µ] ≤exp(−δ2µ/3), ∀δ > 0 ;
2. Pr[X ≤(1 −δ)µ] ≤exp(−δ2µ/2), ∀0 < δ < 1.
Lemma A.2 (Hoeffding bound Hoeffding (1963)). Let X1, · · · , Xn denote n independent bounded
variables in [ai, bi]. Let X = Pn
i=1 Xi, then we have"
REFERENCES,0.3704379562043796,"Pr[|X −E[X]| ≥t] ≤2 exp

−
2t2
Pn
i=1(bi −ai)2 
."
REFERENCES,0.3722627737226277,"Lemma A.3 (Bernstein inequality Bernstein (1924)). Let X1, · · · , Xn be independent zero-mean
random variables. Suppose that |Xi| ≤M almost surely, for all i. Then, for all positive t, Pr "" n
X"
REFERENCES,0.3740875912408759,"i=1
Xi > t # ≤exp "
REFERENCES,0.3759124087591241,"−
t2/2
Pn
j=1 E[X2
j ] + Mt/3 ! ."
REFERENCES,0.3777372262773723,"Next, we introduce Lemma A.4 about CDF of Gaussian distributions:
Lemma A.4 (Anti-concentration of Gaussian distribution). Let X ∼N(0, σ2), that is, the proba-"
REFERENCES,0.3795620437956204,"bility density function of X is given by φ(x) =
1
√"
REFERENCES,0.3813868613138686,2πσ2 e−x2
REFERENCES,0.38321167883211676,2σ2 . Then
REFERENCES,0.38503649635036497,"Pr[|X| ≤t] ∈
2"
T,0.38686131386861317,"3
t
σ , 4"
T,0.3886861313868613,"5
t
σ 
."
T,0.3905109489051095,"Finally, we introduce Lemma A.5 as a concentration result on random matrices and Claim A.6 about
elementary anti-concentration property of Gaussian distribution.
Lemma A.5 (Matrix Bernstein, Theorem 6.1.1 in Tropp (2015)). Consider a ﬁnite sequence
{X1, · · · , Xm} ⊂Rn1×n2 of independent, random matrices with common dimension n1 × n2.
Assume that"
T,0.39233576642335766,"E[Xi] = 0, ∀i ∈[m] and ∥Xi∥≤M, ∀i ∈[m]."
T,0.39416058394160586,"Let Z = Pm
i=1 Xi. Let Var[Z] be the matrix variance statistic of sum:"
T,0.395985401459854,"Var[Z] = max ( m
X"
T,0.3978102189781022,"i=1
E[XiX⊤
i ]
, m
X"
T,0.39963503649635035,"i=1
E[X⊤
i Xi] ) . Then"
T,0.40145985401459855,E[∥Z∥] ≤(2Var[Z] · log(n1 + n2))1/2 + M · log(n1 + n2)/3.
T,0.4032846715328467,"Furthermore, for all t ≥0,"
T,0.4051094890510949,"Pr[∥Z∥≥t] ≤(n1 + n2) · exp

−
t2/2
Var[Z] + Mt/3 
."
T,0.40693430656934304,"We state a standard probabilistic result for Gaussian,
Claim A.6. Let u ∼N(0, Id) and β ∼N(0, 1), which are independent. For all x ∈X and t ≥0,"
T,0.40875912408759124,"Pr[|⟨u, x⟩+ β| ≤t] = O(t)."
T,0.41058394160583944,Under review as a conference paper at ICLR 2022
T,0.4124087591240876,"B
PRELIMINARIES"
T,0.4142335766423358,"B.1
NOTATIONS"
T,0.41605839416058393,"For a vector x, we use ∥x∥p to denote its ℓp norm, in this paper we mainly consider the situation
when p = 1, 2, or ∞."
T,0.41788321167883213,"For a matrix W ∈Rd×m, we use W ⊤to denote its transpose and use tr[W] to denote its trace. We
use ∥W∥1, ∥W∥2 and ∥W∥F to denote the entry-wise ℓ1 norm, spectral norm and Frobenius norm
of W respectively. For each j ∈[m], we let Wj ∈Rd be the j-th column of W. We let ∥W∥2,1
denotes Pm
j=1 ∥Wj∥2 and ∥W∥2,∞denotes maxj∈[m] ∥Wj∥2. For two matrices A, B with the same
dimensions, we denote their Euclidean inner product as ⟨A, B⟩:= tr[A⊤B]."
T,0.4197080291970803,"We denote Gaussian distribution with mean µ and covariance Σ as N(µ, Σ). We use σ(·) to denote
the ReLU function σ(z) = max{z, 0}, and use 1{E} to denote the indicator function of event E."
T,0.4215328467153285,"B.2
TWO LAYER RELU NEURAL NETWORK AND INITIALIZATION"
T,0.4233576642335766,"In this paper, we focus on a two-layer neural network that has m neurons in the hidden layer, where
each neuron is a ReLU activation function. We deﬁne the global network as"
T,0.4251824817518248,"fU(x) := m
X"
T,0.42700729927007297,"r=1
ar · σ(⟨Ur, x⟩+ br)
(4)"
T,0.42883211678832117,"and for c ∈[N], we deﬁne the local network of client c as"
T,0.4306569343065693,"fWc(x) := m
X"
T,0.4324817518248175,"r=1
ar · σ(⟨Wc,r, x⟩+ br).
(5)"
T,0.4343065693430657,"Here U = (U1, . . . , Um) ∈Rd×m is the global hidden weight matrix, Wc = (Wc,1, . . . , Wc,m) ∈
Rd×m is the local hidden weight matrix of client c, and a = (a1, . . . , am) ∈Rm denotes the output
weight vector, b = (b1, . . . , bm) ∈Rm denotes the bias vector. During the process of federated
adversarial learning, for convenience we keep a and b equal to their initialized values and only
update U and W, so we can write the global network as fU(x) and the local network as fWc(x).
For the situation we don’t care about the weight matrix, we write f(x) or fc(x) for short. Next, we
make some standard assumptions regarding our training set."
T,0.43613138686131386,"Deﬁnition B.1 (Dataset). There are N clients and n = NJ data in total.4 Let S = ∪c∈[N]Sc where
Sc = {(xc,1, yc,1), ..., (xc,J, yc,J)} ⊆Rd × R denotes the J training data of client c. Without loss
of generality, we assume that for all c ∈[N], j ∈[J], ∥xc,j∥2 = 1 and the last coordinate of each
point equals to 1/2 , so we consider X := {x ∈Rd : ∥x∥2 = 1, xd = 1/2}. For simplicity, we also
assume that for all c ∈[N], j ∈[J], |yc,j| ≤1.5"
T,0.43795620437956206,We now deﬁne the initialization for the neural networks.
T,0.4397810218978102,"Deﬁnition B.2 (Initialization). The initialization of a ∈Rm, U ∈Rd×m, b ∈Rm is a(0) ∈
Rm, U(0) ∈Rd×m, b(0) ∈Rm.
The initialization of client c’s local weight matrix Wc is
Wc(0, 0) = U(0). Here the second term in Wc denotes iteration of local steps."
T,0.4416058394160584,"• For each r ∈[m], ar(0) are i.i.d. sampled from [−1/m1/3, +1/m1/3]6 uniformly."
T,0.44343065693430656,"• For each i ∈[d], r ∈[m], Ui,r(0) and br(0) are i.i.d. random Gaussians sampled from
N(0, 1/m). Here Ui,r means the (i, r)-entry of U."
T,0.44525547445255476,"4Without loss of generality, we assume that all clients have same number of training data. Our result can be
generalized to the setting where each client has a different number of data as the future work.
5Our assumptions on data points are reasonable since we can do scale-up. In addition, l2 norm normalization
is a typical technique in experiments. Same assumptions also appears in many previous theoretical works like
Arora et al. (2019b); Allen-Zhu et al. (2019a;b).
6Here the choice of m1/3 is not a must. Actually what we need is [−1/mc, +1/mc] for some c that satisﬁes
Ω(1) ≤c ≤1/3."
T,0.4470802919708029,Under review as a conference paper at ICLR 2022
T,0.4489051094890511,"For each global iteration t ∈[T],"
T,0.45072992700729925,"• For each c ∈[N], the initial value of client c’s local weight matrix Wc is Wc(t, 0) = U(t)."
T,0.45255474452554745,"B.3
ADVERSARY AND WELL-SEPARATED TRAINING SETS"
T,0.4543795620437956,"We ﬁrst formulate the adversary as a mapping.
Deﬁnition B.3 (ρ-Bounded adversary). Let F denote the function class. An adversary is a mapping
A : F × X × R →X which denotes the adversarial perturbation. For ρ > 0, we deﬁne the ℓ2 ball
as B2(x, ρ) := {ex ∈Rd : ∥ex −x∥2 ≤ρ} ∩X, we say an adversary A is ρ-bounded if it satisﬁes"
T,0.4562043795620438,"A(f, x, y) ∈B2(x, ρ)."
T,0.458029197080292,"Moreover, given ρ > 0, we denote the worst-case adversary as A∗:= argmaxex∈B2(x,ρ) ℓ(f(ex), y),
where ℓis loss function deﬁned in Deﬁnition B.5."
T,0.45985401459854014,"In the over-parameterized regime, it is a standard assumption that the training set is well-separated.
Since we deal with adversarial perturbations, we require the following γ-separability, which is a bit
stronger.
Deﬁnition B.4 (γ-separability). Let γ ∈(0, 1/2), δ ∈(0, 1/2), ρ ∈(0, 1/2) denote three parame-
ters such that γ ≤δ·(δ−2ρ). We say our training set S = ∪c∈[N]Sc = ∪c∈[N],j∈[J]{(xc,j, yc,j)} ⊂
Rd × R is globally γ-separable w.r.t a ρ-bounded adversary, if"
T,0.46167883211678834,"min
c1̸=c2,j1̸=j2 ∥xc1,j1 −xc2,j2∥2 ≥δ."
T,0.4635036496350365,"It is worth noting that, our problem setup does not require the assumption on independent and
identically distribution (IID) on data, thus such a formation can be applied to unique challenge of
the non-IID setting in FL."
T,0.4653284671532847,"B.4
ROBUST LOSS FUNCTION"
T,0.46715328467153283,"We deﬁne the following Lipschitz convex loss function that will be used.
Deﬁnition B.5 (Lipschitz convex loss). A loss function ℓ: R × R →R is said to be a Lipschitz
convex loss, if it satisﬁes the following four properties:"
T,0.46897810218978103,• non-negative;
T,0.4708029197080292,• convex in the ﬁrst input of ℓ;
T,0.4726277372262774,"• 1−Lipshcitz, which means ∥ℓ(x1, y1) −ℓ(x2, y2)∥2 ≤∥(x1, y1) −(x2, y2)∥2;"
T,0.4744525547445255,"• ℓ(y, y) = 0 for all y ∈R."
T,0.4762773722627737,"The choice of this type of loss is for the sake of technical presentation simplicity, as is customary in
prior studies Gao et al. (2019); Allen-Zhu et al. (2019a)."
T,0.4781021897810219,"We assume ℓis a Lipschitz convex loss in this paper. Next, we deﬁne our robust loss function of a
neural network, which is based on the adversarial examples generated by a ρ-bounded adversary A.
Deﬁnition B.6 (Training loss). Given a client’s training set Sc = {(xc,j, yc,j)}J
j=1 ⊂Rd × R of
J examples, the standard training loss of a neural net fc : Rd →R is deﬁned as L(fc, Sc) :=
1
J
PJ
j=1 ℓ(fc(xc,j), yc,j).
Given S = ∪c∈[N]Sc, the global loss is deﬁned as L(fU, S) :="
"NJ
PN",0.47992700729927007,"1
NJ
PN
c=1
PJ
j=1 ℓ(fU(xc,j), yc,j). Given a ρ-bounded adversary A, we deﬁne the global loss with
respect to A as"
"NJ
PN",0.48175182481751827,"LA(fU) :=
1
NJ N
X c=1 J
X"
"NJ
PN",0.4835766423357664,"j=1
ℓ(fU(A(fc, xc,j, yc,j)), yc,j)"
"NJ
PN",0.4854014598540146,"=
1
NJ N
X c=1 J
X"
"NJ
PN",0.48722627737226276,"j=1
ℓ(fU(exc,j), yc,j)"
"NJ
PN",0.48905109489051096,Under review as a conference paper at ICLR 2022
"NJ
PN",0.4908759124087591,and also deﬁne the worst-case global robust loss as
"NJ
PN",0.4927007299270073,"LA∗(fU) :=
1
NJ N
X c=1 J
X"
"NJ
PN",0.49452554744525545,"j=1
ℓ(fU(A∗(fc, xc,j, yc,j)), yc,j)"
"NJ
PN",0.49635036496350365,"=
1
NJ N
X c=1 J
X"
"NJ
PN",0.4981751824817518,"j=1
max
x∗
c,j∈B2(xc,j,ρ) ℓ
 
fU(x∗
c,j), yc,j

."
"NJ
PN",0.5,"Moreover, since we deal with pseudo-net which is deﬁned in Deﬁnition D.1, we also de-
ﬁne the loss of a pseudo-net as L(gc, Sc)
:=
1
J
PJ
j=1 ℓ(gc(xc,j), yc,j) and L(gU, S)
:="
"NJ
PN",0.5018248175182481,"1
NJ
PN
c=1
PJ
j=1 ℓ(gU(xc,j), yc,j)."
"NJ
PN",0.5036496350364964,"B.5
FEDERATED ADVERSARIAL LEARNING ALGORITHM"
"NJ
PN",0.5054744525547445,"Classical adversarial training algorithm can be found in Zhang et al. (2020b). Different from the
classical setting, our federated adversarial learning of a local neural network fWc against an adver-
sary A is shown in Algorithm 2, where there are two procedures: one is CLIENTUPDATE running on
client side and the other is SERVEREXECUTION running on server side. These two procedures are
iteratively processed through communication iterations. Adversarial training is addressed in proce-
dure CLIENTUPDATE. Hence, there are two loops in CLIENTUPDATE procedure: the outer loop is
iteration for local model updating; and the inner loop is iteratively generating adversarial samples by
the adversary A. In the outer loop in SERVEREXECUTION procedure, the neural network’s parame-
ters are updated to reduce its prediction loss on the new adversarial samples. These loops constitute
an intertwining dynamics."
"NJ
PN",0.5072992700729927,Under review as a conference paper at ICLR 2022
"NJ
PN",0.5091240875912408,Algorithm 2 Federated Adversarial Learning (FAL). This is a complete version of Algorithm 1.
"NJ
PN",0.5109489051094891,"1: /*Deﬁning notations and parameters*/
2:
We use c to denote the client’s index
3:
The training set of client c is denoted as Sc = {(xc,j, yc,j)}J
j=1
4:
Let A be the adversary
5:
We denote local learning rate as ηlocal
6:
We denote global learning rate as ηglobal
7:
We denote local updating iterations as K
8:
We denote global communication round as T
9:
10: /*Initialization*/
11:
Initialization a(0) ∈Rm, U(0) ∈Rd×m, b(0) ∈Rm"
"NJ
PN",0.5127737226277372,"12:
For t = 0 →T, we iteratively run Procedure A then Procedure B
13:
14: /* Procedure running on client side */
15: procedure A. CLIENTUPDATE(t, c)
16:
Sc(t) ←∅
17:
Wc(t, 0) ←U(t)
▷Receive global model weights update
18:
for k = 0 →K −1 do
19:
for j = 1 →J do
20:
ex(t)
c,j ←A(fWc(t,k), xc,j, yc,j)
▷Adversarial examples, fWc is deﬁned as (5)"
"NJ
PN",0.5145985401459854,"21:
Sc(t) ←Sc(t) ∪(ex(t)
c,j, yc,j)
22:
end for
23:
Wc(t, k + 1) ←Wc(t, k) −ηlocal · ∇WcL(fWc(t,k), Sc(t))
24:
end for
25:
∆Uc(t) ←Wc(t, K) −U(t)
26:
Send ∆Uc(t) to SERVEREXECUTION
27: end procedure
28:
29: /*Procedure running on server side*/
30: procedure B. SERVEREXECUTION(t):
31:
for each client c in parallel do
32:
∆Uc(t) ←CLIENTUPDATE(c, t)
▷Receive local model weights update
33:
∆U(t) ←1 N
P"
"NJ
PN",0.5164233576642335,"c∈[N] ∆Uc(t)
34:
U(t + 1) ←U(t) + ηglobal · ∆U(t)
▷Aggregation on the server side
35:
Send U(t + 1) to client c for CLIENTUPDATE(c, t)
36:
end for
37: end procedure"
"NJ
PN",0.5182481751824818,Under review as a conference paper at ICLR 2022
"NJ
PN",0.5200729927007299,"C
PROOF OVERVIEW"
"NJ
PN",0.5218978102189781,"In this section we give an overview of our main result’s proof. Two theorems to be used are Theo-
rem E.3 and Theorem F.3."
"NJ
PN",0.5237226277372263,"C.1
PSEUDO-NETWORK"
"NJ
PN",0.5255474452554745,"We study the over-parameterized neural nets’ well-approximated pseudo-network to learn gradient
descent for over-parameterized neural nets whose weights are close to initialization. The introducing
of pseudo-network makes the proof more intuitive."
"NJ
PN",0.5273722627737226,"To be speciﬁc, we give the deﬁnition of pseudo-network in Section D, and also state Theorem D.2
which shows the fact that the pseudo-network approximates the real network uniformly well. It can
be seen that the notion of pseudo-network is used for several times in our proof."
"NJ
PN",0.5291970802919708,"C.2
ONLINE GRADIENT DESCENT IN FEDERATED ADVERSARIAL LEARNING"
"NJ
PN",0.531021897810219,"Our federated adversarial learning algorithm is formulated in the framework of online gradient de-
scent: at each local step k on the client side, ﬁrstly the adversary generates adversarial samples and
computes the loss function L
 
fWc(t,k), Sc(t)

, then the local client learner takes the fresh loss func-
tion and update Wc(t, k + 1) = Wc(t, k) −ηlocal · ∇WcL
 
fWc(t,k), Sc(t)

. We refer our readers
to Gao et al. (2019); Hazan (2016) for more details regarding online learning and online gradient
descent."
"NJ
PN",0.5328467153284672,"Compared with the centralized setting, the key difﬁculties in the convergence analysis of FL are
induced by multiple local updates on the client side and the updates on both local and global sides.
Speciﬁcally, local updates are not the standard gradient as the centralized adversarial training when
K ≥2. We used −∆U(t) in substitution of the real gradient of U to update the value of U(t). This
brings in challenges to bound the gradient of the neural networks. Nevertheless, gradient bounding
is challenging in adversarial training solely. We use gradient coupling method twice to solve this
core problem: ﬁrstly we bound the difference between real gradient and FL gradient in Lemma E.4,
then we bound the difference between pseudo gradient and real gradient in Lemma E.5. We show the
connection of online gradient descent and federated adversarial learning in the proof of Theorem E.3."
"NJ
PN",0.5346715328467153,"C.3
EXISTENCE OF ROBUST NETWORK NEAR INITIALIZATION"
"NJ
PN",0.5364963503649635,"In Section F we show that there exists a global network fU ∗whose weight is close to the initial
value U(0) and the worst-case global robust loss LA∗(fU ∗) is sufﬁciently small. We show that the
required width m is poly(d, (NJ/ϵ)1/γ)."
"NJ
PN",0.5383211678832117,"Suppose we are given a ρ-bounded adversary. For a globally γ-separable training set, in order to
prove Theorem F.3, we ﬁrst state Lemma F.1 which shows the existence of function f ∗that has
""low complexity"" and satisﬁes f ∗(exc,j) ≈yc,j for all data point (xc,j, yc,j) and perturbation inputs
exc,j ∈B2(xc,j, ρ). Then, we state Lemma F.2 which shows the existence of a pseudo-network gU ∗
that approximates f ∗well. Finally, by using Theorem D.2 we show that fU ∗approximates gU ∗well.
By combining these results, we we ﬁnish the proof of Theorem F.3."
"NJ
PN",0.5401459854014599,Under review as a conference paper at ICLR 2022
"NJ
PN",0.541970802919708,"D
REAL APPROXIMATES PSEUDO"
"NJ
PN",0.5437956204379562,"To make additional remark to proof sketch in Section 5, in this section, we state a tool that will be
used in our proof that is related to our deﬁnition of pseudo-network. First, we recall the deﬁnition
of pseudo-network."
"NJ
PN",0.5456204379562044,"Deﬁnition D.1 (Pseudo-network). Given weights U ∈Rd×m, a ∈Rm and b ∈Rm, the global
neural network function fU : Rd →R is deﬁned as"
"NJ
PN",0.5474452554744526,"fU(x) := m
X"
"NJ
PN",0.5492700729927007,"r=1
ar · σ(⟨Ur, x⟩+ br)."
"NJ
PN",0.551094890510949,"Given this fU(x), we deﬁne the corresponding pseudo-network function gU : Rd →R as"
"NJ
PN",0.5529197080291971,"gU(x) := m
X"
"NJ
PN",0.5547445255474452,"r=1
ar · ⟨Ur(t) −Ur(0), x⟩· 1{⟨Ur(0), x⟩+ br ≥0}."
"NJ
PN",0.5565693430656934,"From the deﬁnition we can know that pseudo-network can be seen as a linear approximation of the
two layer ReLU network we study near initialization. Next, we cite a Theorem from Zhang et al.
(2020b), which gives a uniform bound of the difference between a network and its pseudo-network."
"NJ
PN",0.5583941605839416,"Theorem D.2 (Uniform approximation, Theorem 5.1 in Zhang et al. (2020b)). Let R ≥1. For all
m ≥poly(d), with probability at least 1 −exp(−Ω(m1/3)) over the choice of a(0), U(0), b(0), for
all U ∈Rd×m such that ∥U −U(0)∥2,∞≤R/m2/3,"
"NJ
PN",0.5602189781021898,"sup
x∈X
|fU(x) −gU(x)| ≤O(R2/m1/6)."
"NJ
PN",0.5620437956204379,Under review as a conference paper at ICLR 2022
"NJ
PN",0.5638686131386861,"E
CONVERGENCE"
"NJ
PN",0.5656934306569343,"Section
Statement
Comment
Statements Used
E.1
Deﬁnition E.1 and E.2
Deﬁnition
-
E.2
Theorem E.3
Convergence result
Lem. E.4, E.5, Thm. D.2
E.3
Lemma E.4
Approximates real gradient
-
E.4
Lemma E.5
Approximates pseudo gradient
Claim E.6
E.5
Claim E.6
Auxiliary bounding
Claim A.6"
"NJ
PN",0.5675182481751825,"Table 1: List of theorems and lemmas in Section E. The main result of this section is Theorem E.3.
By saying ""Statements Used"" we mean these statements are used in the proof in the corresponding
section. For example, Lemma E.4, E.5 and Theorem D.2 are used in the proof of Theorem E.3."
"NJ
PN",0.5693430656934306,"E.1
DEFINITIONS AND NOTATIONS"
"NJ
PN",0.5711678832116789,"In Section E, we follow the notations used in Deﬁnition D.1. Since we are dealing with pseudo-
network, we ﬁrst introduce some additional deﬁnitions and notations regarding gradient.
Deﬁnition E.1 (Gradient). For a local real network fWc(t,k), we denote its gradient by"
"NJ
PN",0.572992700729927,"∇(fc, t, k) := ∇WcL(fWc(t,k), Sc(t))."
"NJ
PN",0.5748175182481752,"If the corresponding pseudo-network is gWc(t,k), then we deﬁne the pseudo-network gradient as"
"NJ
PN",0.5766423357664233,"∇(gc, t, k) := ∇WcL(gWc(t,k), Sc(t))."
"NJ
PN",0.5784671532846716,"Now we consider the global matrix. For convenience we write ∇(f, t) := ∇UL(fU(t), S(t)) and
∇(g, t) := ∇UL(gU(t), S(t)). We deﬁne the FL gradient as e∇(f, t) := −1"
"NJ
PN",0.5802919708029197,N ∆U(t).
"NJ
PN",0.5821167883211679,"Deﬁnition E.2 (Distance). For U ∗∈Rd×m such that ∥U ∗−eU∥2,∞≤R/m3/4, we deﬁne the
following distance for simplicity:"
"NJ
PN",0.583941605839416,"Dmax := max
t∈[T ] ∥U(t) −eU∥2,∞"
"NJ
PN",0.5857664233576643,"DU ∗:= ∥U ∗−eU∥2,∞"
"NJ
PN",0.5875912408759124,"We have DU ∗= O(R/m3/4) and ∥U(t) −U ∗∥2,∞≤Dmax + DU ∗by using triangle inequality."
"NJ
PN",0.5894160583941606,"Notation
Meaning
Satisfy
U(0) or eU
Initialization of U
Wc(0, 0) = U(0)
U(t)
The value of U after t iterations
Dmax = max ∥U(t) −eU∥2,∞
U ∗
The value of U after small perturbations from eU
∥U ∗−eU∥2,∞≤R/m3/4"
"NJ
PN",0.5912408759124088,Table 2: Notations of global model weights in federated learning to be used in this section.
"NJ
PN",0.593065693430657,"E.2
CONVERGENCE RESULT"
"NJ
PN",0.5948905109489051,"The goal of this section is to prove Theorem E.3.
Theorem E.3 (Convergence result, formal version of Theorem 5.6). For all ϵ ∈(0, 1), for all
R ≥1, there exists an M = poly(n, R, 1/ϵ), such that for every m ≥M, for every K ≥1,
for every T ≥poly(R/ϵ), with probability at least 1 −exp(−Ω(m1/3)) over the randomness of
a(0) ∈Rm, U(0) ∈Rd×m, b(0) ∈Rm, if we run Algorithm 2 with setting"
"NJ
PN",0.5967153284671532,"ηglobal = 1/ poly(NJ, R, 1/ϵ) and ηlocal = 1/K,"
"NJ
PN",0.5985401459854015,"then for every U ∗such that ∥U ∗−U(0)∥2,∞≤R/m3/4, the output weights (U(t))T
t=1 satisfy"
T,0.6003649635036497,"1
T T
X"
T,0.6021897810218978,"t=1
LA
 
fU(t)

≤LA∗(fU ∗) + ϵ."
T,0.6040145985401459,Under review as a conference paper at ICLR 2022
T,0.6058394160583942,Proof. We set our parameters as follows:
T,0.6076642335766423,"M = Ω

max

(NJ)8, (R"
T,0.6094890510948905,ϵ )12	
T,0.6113138686131386,"ηglobal = O(
ϵ
Nm1/3 · poly(R/ϵ))"
T,0.6131386861313869,ηlocal = 1/K
T,0.614963503649635,"Since the loss function is 1-Lipschitz, we can ﬁrst bound the norm of real net gradient:"
T,0.6167883211678832,"∥∇r(fc, t, k)∥2 ≤|ar| ·
 1 J J
X"
T,0.6186131386861314,"j=1
σ′(⟨Wc,r(t, k), xc,j⟩+ br) · ∥exc,j∥2

≤|ar| ≤
1
m1/3 .
(6)"
T,0.6204379562043796,"Now we consider the pseudo-net gradient. The loss L(gU, S(t)) is convex in U due to the fact that
g is linear with U. Then we have
L(gU(t), S(t)) −L(gU ∗, S(t))"
T,0.6222627737226277,"≤⟨∇UL(gU(t), S(t)), U(t) −U ∗⟩"
T,0.6240875912408759,"= ⟨e∇(f, t), U(t) −U ∗⟩+ ⟨∇(f, t) −e∇(f, t), U(t) −U ∗⟩+ ⟨∇(g, t) −∇(f, t), U(t) −U ∗⟩
≤α(t) + β(t) + γ(t)
where the last step follows from"
T,0.6259124087591241,"α(t) := ⟨e∇(f, t), U(t) −U ∗⟩,"
T,0.6277372262773723,"β(t) := ∥∇(f, t) −e∇(f, t)∥2,1 · ∥U(t) −U ∗∥2,∞,
γ(t) := ∥∇(g, t) −∇(f, t)∥2,1 · ∥U(t) −U ∗∥2,∞."
T,0.6295620437956204,"Note that the FL gradient e∇(f, t) = −1"
T,0.6313868613138686,"N ∆U(t) is the direction moved by center, in contrast, ∇(f, t)
is the true gradient of function f. We deal with these three terms separately. As for α(t), we have"
T,0.6332116788321168,"∥U(t + 1) −U ∗∥2
F = ∥U(t) + ηglobal∆U(t) −U ∗∥2
F
= ∥U(t) −U ∗∥2
F −2Nηglobalα(t) + η2
global∥∆U(t)∥2
F
and by rearranging we get"
T,0.635036496350365,α(t) = ηglobal
N,0.6368613138686131,"2N
∥∆U(t)∥2
F +
1
2Nηglobal
· (∥U(t) −U ∗∥2
F −∥U(t + 1) −U ∗∥2
F )."
N,0.6386861313868614,"Next, we need to upper bound ∥∆U(t)∥2
F ,"
N,0.6405109489051095,"∥∆U(t)∥2
F = ∥ηlocal N N
X c=1 K−1
X"
N,0.6423357664233577,"k=0
∇(fc, t, k)∥2
F"
N,0.6441605839416058,"≤ηlocal N N
X c=1 K−1
X k=0 m
X"
N,0.6459854014598541,"r=1
∥∇r(fc, t, k)∥2
2"
N,0.6478102189781022,= ηlocalKm1/3
N,0.6496350364963503,"= m1/3.
(7)
where the last step follows from Kηlocal = 1. Then we do summation over t and have T
X"
N,0.6514598540145985,"t=1
α(t) = ηglobal"
N,0.6532846715328468,"2N T
X"
N,0.6551094890510949,"t=1
∥∆U(t)∥2
F +
1
2Nηglobal
· T
X"
N,0.656934306569343,"t=1
(∥U(t) −U ∗∥2
F −∥U(t + 1) −U ∗∥2
F )"
N,0.6587591240875912,= ηglobal
N,0.6605839416058394,"2N T
X"
N,0.6624087591240876,"t=1
∥∆U(t)∥2
F +
1
2Nηglobal
· (∥U(1) −U ∗∥2
F −∥U(T + 1) −U ∗∥2
F )"
N,0.6642335766423357,≤ηglobal
N,0.666058394160584,"2N T
X"
N,0.6678832116788321,"t=1
∥∆U(t)∥2
F +
1
2Nηglobal
· ∥U(1) −U ∗∥2
F"
N,0.6697080291970803,≲ηglobal
N,0.6715328467153284,"N
Tm1/3 +
1
Nηglobal
mD2
U ∗"
N,0.6733576642335767,Under review as a conference paper at ICLR 2022
N,0.6751824817518248,"where the last step follows from Eq. (7) and ∥eU −U ∗∥2
F ≤m · ∥eU −U ∗∥2
2,∞= mD2
U ∗."
N,0.677007299270073,"As for β(t), we apply Lemma E.4 and also triangle inequality and have"
N,0.6788321167883211,"β(t) = ∥∇(f, t) −e∇(f, t)∥2,1 · ∥U(t) −U ∗∥2,∞"
N,0.6806569343065694,"≲m2/3 · ∥U(t) −U ∗∥2,∞"
N,0.6824817518248175,"≲m2/3 · (∥U(t) −eU∥2,∞+ DU ∗)."
N,0.6843065693430657,"By using Eq. (6) we bound the size of ∥U(t) −eU∥2,∞:"
N,0.6861313868613139,"∥U(t) −eU∥2,∞≤ηglobal t
X"
N,0.6879562043795621,"τ=1
∥∆U(τ)∥2,∞"
N,0.6897810218978102,"= ηglobal t
X"
N,0.6916058394160584,"τ=1
∥ηlocal N N
X c=1 K−1
X"
N,0.6934306569343066,"k=0
∇(fc, t, k)∥2,∞"
N,0.6952554744525548,"≤ηglobalηlocal N t
X τ=1 N
X c=1 K−1
X"
N,0.6970802919708029,"k=0
∥∇(fc, t, k)∥2,∞"
N,0.698905109489051,≤ηglobalηlocaltKm−1/3
N,0.7007299270072993,and have
N,0.7025547445255474,β(t) ≲ηglobalηlocaltKm1/3 + m2/3DU ∗.
N,0.7043795620437956,"Then we do summation over t and have T
X"
N,0.7062043795620438,"t=1
β(t) ≲ T
X"
N,0.708029197080292,"t=1
(ηglobalηlocaltKm1/3 + m2/3DU ∗)"
N,0.7098540145985401,≲ηglobalηlocalT 2Km1/3 + m2/3TDU ∗
N,0.7116788321167883,≲ηglobalT 2m1/3 + m2/3TDU ∗.
N,0.7135036496350365,"As for γ(t), we apply Lemma E.5 and have"
N,0.7153284671532847,"γ(t) = ∥∇(g, t) −∇(f, t)∥2,1 · ∥U(t) −U ∗∥2,∞"
N,0.7171532846715328,"≲NJm13/24 · (∥U(t) −eU∥2,∞+ DU ∗)."
N,0.718978102189781,"Since ∥U(t) −eU∥2,∞≤ηglobalηlocaltKm−1/3, we have"
N,0.7208029197080292,γ(t) ≲ηglobalηlocaltKNJm5/24 + NJm13/24DU ∗.
N,0.7226277372262774,"Then we do summation over t and have T
X"
N,0.7244525547445255,"t=1
γ(t) ≲ T
X t=1"
N,0.7262773722627737," 
ηglobalηlocaltKNJm5/24 + NJm13/24DU ∗"
N,0.7281021897810219,≲ηglobalηlocalT 2KNJm5/24 + NJm13/24TDU ∗
N,0.7299270072992701,≲ηglobalT 2NJm5/24 + NJm13/24TDU ∗.
N,0.7317518248175182,Under review as a conference paper at ICLR 2022
N,0.7335766423357665,"Next we put it altogether. Note that DU ∗= O(
R
m3/4 ), thus we obtain T
X"
N,0.7354014598540146,"t=1
L(gU(t), S(t)) − T
X"
N,0.7372262773722628,"t=1
L(gU ∗, S(t)) ≤ T
X"
N,0.7390510948905109,"t=1
α(t) + T
X"
N,0.7408759124087592,"t=1
β(t) + T
X"
N,0.7427007299270073,"t=1
γ(t)"
N,0.7445255474452555,≲ηglobal
N,0.7463503649635036,"N
Tm1/3 +
1
Nηglobal
mD2
U ∗+ ηglobalT 2m1/3"
N,0.7481751824817519,+ m2/3TDU ∗+ ηglobalT 2NJm5/24 + NJm13/24TDU ∗
N,0.75,≲ηglobal
N,0.7518248175182481,"N
Tm1/3 +
1
Nηglobal
R2m−1/2 + ηglobalT 2m1/3"
N,0.7536496350364964,"+ RTm−1/12 + ηglobalT 2NJm5/24 + NJm−5/24RT.
We then have"
T,0.7554744525547445,"1
T T
X"
T,0.7572992700729927,"t=1
L(gU(t), S(t)) −1 T T
X"
T,0.7591240875912408,"t=1
L(gU ∗, S(t))"
T,0.7609489051094891,≲ηglobal
T,0.7627737226277372,"N
m1/3 +
1
NηglobalT R2m−1/2 + ηglobalTm1/3 + Rm−1/12"
T,0.7645985401459854,+ ηglobalTNJm5/24 + NJm−5/24R.
T,0.7664233576642335,"≲
1
NηglobalT R2m−1/2 + ηglobalTm1/3 + Rm−1/12 + ηglobalTNJm5/24 + NJm−5/24R
(8)"
T,0.7682481751824818,≤O(ϵ).
T,0.7700729927007299,"From Theorem D.2 we know
sup
x∈X
|fU(x) −gU(x)| ≤O(R2/m1/6) = O(ϵ)"
T,0.7718978102189781,"and thus, we get"
T,0.7737226277372263,"1
T T
X"
T,0.7755474452554745,"t=1
L(fU(t), S(t)) −1 T T
X"
T,0.7773722627737226,"t=1
L(fU ∗, S(t)) ≤c · ϵ
(9)"
T,0.7791970802919708,"where c > 0 is a constant. From the deﬁnition of A∗we have L(fU ∗, S(t)) ≤LA∗(fU ∗). From the
deﬁnition of loss we have L(fU(t), S(t)) = LA(fU(t)). Moreover, since Eq. (9) holds for all ϵ > 0,
we can replace ϵ"
T,0.781021897810219,"c with ϵ. Thus we prove that for ∀ϵ > 0,"
T,0.7828467153284672,"1
T T
X"
T,0.7846715328467153,"t=1
LA(fU(t)) ≤LA∗(fU ∗) + ϵ."
T,0.7864963503649635,"E.3
APPROXIMATES REAL GLOBAL GRADIENT"
T,0.7883211678832117,"The goal of this section is to prove Lemma E.4.
Lemma E.4 (Bounding the difference between real gradient and FL gradient). With probability at
least 1 −exp(−Ω(m1/3)) over the randomness of a(0) ∈Rm, U(0) ∈Rd×m, b(0) ∈Rm, for all
iterations t such that ∥U(t) −U(0)∥2,∞≤O(m−15/24), the following holds:"
T,0.7901459854014599,"∥∇(f, t) −e∇(f, t)∥2,1 ≤O(m2/3)."
T,0.791970802919708,"Proof. Notice that ∇(f, t) = ∇UL(fU(t), S(t)) and"
T,0.7937956204379562,"e∇(f, t) = −1"
T,0.7956204379562044,"N ∆U(t) = −1 N N
X"
T,0.7974452554744526,"c=1
∆Uc(t) = ηlocal N N
X c=1 K−1
X"
T,0.7992700729927007,"k=0
∇(fc, t, k)."
T,0.801094890510949,Under review as a conference paper at ICLR 2022
T,0.8029197080291971,So we have
T,0.8047445255474452,"∥∇(f, t) −e∇(f, t)∥2,1 = m
X"
T,0.8065693430656934,"r=1
∥∇r(f, t) −e∇r(f, t)∥2 = 1 N m
X"
T,0.8083941605839416,"r=1
∥N · ∇r(f, t) −ηlocal N
X c=1 K−1
X"
T,0.8102189781021898,"k=0
∇r(fc, t, k)∥2"
T,0.8120437956204379,"≤ηlocal N m
X r=1 K−1
X"
T,0.8138686131386861,"k=0
∥N · ∇r(f, t)"
T,0.8156934306569343,"Kηlocal
− N
X"
T,0.8175182481751825,"c=1
∇r(fc, t, k)∥2"
T,0.8193430656934306,"=
1
NK m
X r=1 K−1
X"
T,0.8211678832116789,"k=0
∥N · ∇r(f, t) − N
X"
T,0.822992700729927,"c=1
∇r(fc, t, k)∥2"
T,0.8248175182481752,where the last step follows from the assumption that ηlocal = 1 K .
T,0.8266423357664233,"As for ∥N · ∇r(f, t) −PN
c=1 ∇r(fc, t, k)∥2, we have"
T,0.8284671532846716,"∥N · ∇r(f, t) − N
X"
T,0.8302919708029197,"c=1
∇r(fc, t, k)∥2"
T,0.8321167883211679,"≤|ar| ·

  N NJ N
X c=1 J
X"
T,0.833941605839416,"j=1
1{⟨Ur(t), xc,j⟩+ br ≥0} −1 J N
X c=1 J
X"
T,0.8357664233576643,"j=1
1{⟨Wc,r(t, k), xc,j⟩+ br ≥0}

· ∥xc,j∥2"
T,0.8375912408759124,"≤
1
m1/3 · 1 J N
X c=1 J
X"
T,0.8394160583941606,"j=1
| 1{⟨Ur(t), xc,j⟩+ br ≥0} −1{⟨Wc,r(t, k), xc,j⟩+ br ≥0}|"
T,0.8412408759124088,"≤
N
m1/3 ."
T,0.843065693430657,Then we do summation and have
T,0.8448905109489051,"∥∇(f, t) −e∇(f, t)∥2,1 ≤
1
NK m
X r=1 K−1
X"
T,0.8467153284671532,"k=0
∥N · ∇r(f, t) − N
X"
T,0.8485401459854015,"c=1
∇r(fc, t, k)∥2"
T,0.8503649635036497,"≤
1
NK m
X r=1 K−1
X k=0"
T,0.8521897810218978,"N
m1/3"
T,0.8540145985401459,= m2/3.
T,0.8558394160583942,Thus we ﬁnish the proof.
T,0.8576642335766423,"E.4
APPROXIMATES PSEUDO GLOBAL GRADIENT"
T,0.8594890510948905,"The goal of this section is to prove Lemma E.5.
Lemma E.5 (Bounding the difference between pseudo gradient and real gradient). With probability
at least 1 −exp(−Ω(m1/3)) over the randomness of a(0) ∈Rm, U(0) ∈Rd×m, b(0) ∈Rm, for
all iterations t such that ∥U(t) −U(0)∥2,∞≤O(m−15/24), the following holds:"
T,0.8613138686131386,"∥∇(g, t) −∇(f, t)∥2,1 ≤O(NJm13/24)."
T,0.8631386861313869,"Proof. Notice that ∇(g, t) = ∇UL(gU(t), S(t)) and ∇(f, t) = ∇UL(fU(t), S(t)). By Claim E.6,
with the given probability we have m
X"
T,0.864963503649635,"r=1
1{∇r(g, t) ̸= ∇r(f, t)} ≤O(NJm7/8)."
T,0.8667883211678832,Under review as a conference paper at ICLR 2022
T,0.8686131386861314,"For indices r ∈[m] such that ∇r(g, t) ̸= ∇r(f, t), the following holds:"
T,0.8704379562043796,"∥∇r(g, t) −∇r(f, t)∥2 = ∥∇U,rL(gU(t), S(t)) −∇U,rL(fU(t), S(t))∥2"
T,0.8722627737226277,"≤|ar| ·
1
NJ · N
X c=1 J
X"
T,0.8740875912408759,"j=1
∥xc,j∥2 ·
 1{⟨eUr, xc,j⟩+ br ≥0}"
T,0.8759124087591241,"−1{⟨Ur, xc,j⟩+ br ≥0}"
T,0.8777372262773723,"≤
1
m1/3 ·
1
NJ · N
X c=1 J
X j=1"
T,0.8795620437956204,"1{⟨eUr, xc,j⟩+ br ≥0} −1{⟨Ur, xc,j⟩+ br ≥0}"
T,0.8813868613138686,"≤
1
m1/3 ."
T,0.8832116788321168,"where the ﬁrst step is deﬁnition, the second step follows that the loss function is 1-Lipschitz, the
third step follows from |ar| ≤
1
m1/3 and ∥xc,j∥2 = 1, the last step follows from the bound of the
indicator function. Thus, we do the conclusion:"
T,0.885036496350365,"∥∇(g, t) −∇(f, t)∥2,1 = m
X"
T,0.8868613138686131,"r=1
∥∇r(g, t) −∇r(f, t)∥2 · 1{∇r(g, t) ̸= ∇r(f, t)}"
T,0.8886861313868614,"≤
1
m1/3 m
X"
T,0.8905109489051095,"r=1
1{∇r(g, t) ̸= ∇r(f, t)}"
T,0.8923357664233577,"≤
1
m1/3 · O(NJm7/8)"
T,0.8941605839416058,= O(NJm13/24)
T,0.8959854014598541,and ﬁnish the proof.
T,0.8978102189781022,"E.5
BOUNDING AUXILIARY"
T,0.8996350364963503,"Claim E.6 (Bounding auxiliary). With probability at least 1 −exp(−Ω(m1/3)) over the initializa-
tion, we have m
X"
T,0.9014598540145985,"r=1
1{∇r(g, t) ̸= ∇r(f, t)} ≤O(NJm7/8)."
T,0.9032846715328468,"Proof. For r ∈[m], let Ir := 1{∇r(g, t) ̸= ∇r(f, t)}. By Claim A.6 we know that for each xc,j
we have"
T,0.9051094890510949,"Pr[|⟨f
Wc,r, xc,j⟩+ br| ≤m−15/24] ≤O(m−1/8)."
T,0.906934306569343,"By putting a union bound over c and j, we get"
T,0.9087591240875912,"Pr

∃c ∈[N], j ∈[J], |⟨f
Wc,r, xc,j⟩+ br| ≤m−15/24
≤O(NJm−1/8). Since"
T,0.9105839416058394,"Pr[Ir = 1] ≤Pr[∃c ∈[N], j ∈[J], |⟨f
Wc,r, xc,j⟩+ br| ≤m−15/24],"
T,0.9124087591240876,we have
T,0.9142335766423357,Pr[Ir = 1] ≤O(NJm−1/8).
T,0.916058394160584,"By applying concentration inequality on Ir (independent Bernoulli) for r ∈[m], we have that with
probability at least 1 −exp(−Ω(NJm7/8)) > 1 −exp(−Ω(m1/3)), the following holds: m
X"
T,0.9178832116788321,"r=1
Ir ≤O(NJm7/8)."
T,0.9197080291970803,Thus we ﬁnish the proof.
T,0.9215328467153284,Under review as a conference paper at ICLR 2022
T,0.9233576642335767,"E.6
FURTHER DISCUSSION"
T,0.9251824817518248,"Note that in the proof of Theorem E.3 we set the hidden layer’s width m to be greater than O(ϵ−12),
which seems impractical in reality: if we choose our convergence accuracy to be 10−2, the width
will become 1024 which is impossible to achieve."
T,0.927007299270073,"However, we want to claim that the ""−12"" term is not intrinsic in our theorem and proof, and
we can actually further improve the lower bound of m to O((R/ϵ)c2) where c2 is some constant
between −3 and −4. To be speciﬁc, we observe from Eq. (8) that the ""−12"" term comes from
2
3 −3"
T,0.9288321167883211,4 = −1
T,0.9306569343065694,"12, where 2"
T,0.9324817518248175,3 appears in Lemma E.4 and 3
T,0.9343065693430657,4 appears in the assumption that DU ∗≤R/m3/4
T,0.9361313868613139,"in Deﬁnition E.2. As for our observations, the 2"
T,0.9379562043795621,"3 term is hard to improve. On the other hand, we can
actually adjust the value of DU ∗as long as we ensure"
T,0.9397810218978102,DU ∗≤R/mc3
T,0.9416058394160584,"for some constant c3 ∈(0, 1). When we let c3 →1, the ﬁnal result will achieve O((R ϵ )3)"
T,0.9434306569343066,which is much more feasible in reality.
T,0.9452554744525548,"As the ﬁrst work and the ﬁrst step towards understanding the convergence of federated adversarial
learning, the priority of our work is not achieving the tightest bounds. Instead, our main goal is to
show the convergence of a general federated adversarial learning framework. Nevertheless, we will
improve the bound in the ﬁnal version."
T,0.9470802919708029,Under review as a conference paper at ICLR 2022
T,0.948905109489051,"F
EXISTENCE"
T,0.9507299270072993,In this section we prove the existence of U ∗that is close to U(0) and makes LA∗(fU ∗) close to zero.
T,0.9525547445255474,"F.1
TOOLS FROM PREVIOUS WORK"
T,0.9543795620437956,"In order to prove our existence result, we ﬁrst state two lemmas that will be used."
T,0.9562043795620438,"Lemma F.1 (Lemma 6.2 from Zhang et al. (2020b)). Suppose that ∥xc1,j1 −xc2,j2∥2 ≥δ holds for
each pair of two different data points xc1,j1, xc2,j2. Let D = 24γ−1 ln(48NJ/ϵ), then there exists
a polynomial q : R →R with degree at most D, size of coefﬁcients at most O(γ−126D), such that
for all c0 ∈[N], j0 ∈[J] and exc0,j0 ∈B2(xc0,j0, ρ), N
X c=1 J
X"
T,0.958029197080292,"j=1
yc,j · q(⟨xc,j, exc0,j0⟩) −yc0,j0 ≤ϵ 3."
T,0.9598540145985401,"We let f ∗(x) := PN
c=1
PJ
j=1 yc,j · q(⟨xc,j, x⟩) and have |f ∗(exc0,j0) −yc0,j0| ≤ϵ/3."
T,0.9616788321167883,"Lemma F.2 (Lemma 6.5 from Zhang et al. (2020b)). For all ϵ ∈(0, 1), there exist M
=
poly(d, (NJ/ϵ)1/γ) and R = poly((NJ/ϵ)1/γ) such that for m ≥M, with probability at least
1 −exp(−Ω(
p"
T,0.9635036496350365,"m/NJ)) over the choice of a(0) ∈Rm, U(0) ∈Rd×m, b(0) ∈Rm, there exists a
U ∗∈Rd×m that satisﬁes ∥U ∗−U(0)∥2,∞≤R/m2/3 and"
T,0.9653284671532847,"sup
x∈X
|gU ∗(x) −f ∗(x)| ≤ϵ/3."
T,0.9671532846715328,"F.2
EXISTENCE RESULT"
T,0.968978102189781,The goal of this section is to prove Theorem F.3 which is the existence result.
T,0.9708029197080292,"Theorem F.3 (Existence, formal version of Theorem 5.2). For all ϵ ∈(0, 1), there exist"
T,0.9726277372262774,"M0 = poly(d, (NJ/ϵ)1/γ) and R = poly((NJ/ϵ)1/γ)"
T,0.9744525547445255,"such that for every m ≥M0, with probability at least 1 −exp(−Ω(m1/3)) over the randomness of
a(0) ∈Rm, U(0) ∈Rd×m, b(0) ∈Rm, there exists U ∗∈Rd×m that satisﬁes ∥U ∗−U(0)∥2,∞≤
R/m2/3 and"
T,0.9762773722627737,LA∗(fU ∗) ≤ϵ.
T,0.9781021897810219,"Proof. From Lemma F.1 we obtain the function f ∗. From Lemma F.2 we know the existence of
M0 = poly(d, (NJ/ϵ)1/γ) and also R = poly((NJ/ϵ)1/γ). By combining these two results
with Theorem D.2, we have that for all m ≥poly(d, (NJ/ϵ)1/γ), with probability at least 1 −
exp(−Ω(
p"
T,0.9799270072992701,"m/NJ))−exp(−Ω(m1/3)), there exists a U ∗∈Rd×m that satisﬁes ∥U ∗−U(0)∥2,∞≤
R/m2/3 and also the following properties:"
T,0.9817518248175182,"• ∀x ∈X, |gU ∗(x) −f ∗(x)| ≤ϵ/3"
T,0.9835766423357665,"• ∀x ∈X, |fU ∗(x) −gU ∗(x)| ≤O(R2/m1/6)"
T,0.9854014598540146,"We consider the loss function. For all c ∈[N], j ∈[J] and exc,j ∈B(xc,j, ρ), we have"
T,0.9872262773722628,"ℓ(fU ∗(exc,j), yc,j) ≤|fU ∗(exc,j) −yc,j|
≤|fU ∗(exc,j) −gU ∗(exc,j)| + |gU ∗(exc,j) −f ∗(exc,j)| + |f ∗(exc,j) −yc,j|"
T,0.9890510948905109,"≤O(R2/m1/6) + ϵ 3 + ϵ 3
≤ϵ,"
T,0.9908759124087592,Under review as a conference paper at ICLR 2022
T,0.9927007299270073,"Thus, we have that LA∗(fU ∗) =
1
NJ
PN
c=1
PJ
j=1 max ℓ
 
fU ∗(x∗
c,j), yc,j

≤ϵ. Furthermore, since
the m we consider satisﬁes m ≥Ω((NJ)1/γ), the holding probability is at least"
T,0.9945255474452555,"1 −exp(−Ω(
p"
T,0.9963503649635036,m/NJ)) −exp(−Ω(m1/3)) = 1 −exp(−Ω(m1/3)).
T,0.9981751824817519,"Thus, we ﬁnish the proof of this theorem."
