Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002277904328018223,"Logical reasoning over natural text is an important capability towards human level
intelligence. Existing datasets are either limited and inadequate to train and evaluate
logical reasoning capability (e.g., LogiQA and ReClor), or not oriented for logical
reasoning (e.g., SQuAD and HotpotQA). In this paper, we focus on a speciï¬c
category of logical reasoning, named naÂ¨Ä±ve logical reasoning, and propose a new
large-scale benchmark, named NAIL, aiming to help models learn and evaluate
naÂ¨Ä±ve logical reasoning capability. NAIL is sourced from standardized exams such
as Chinese National Civil Servants Examination and Law School Admission Test.
Furthermore, to collect more data, we propose to imitate examples of standardized
exams rather than designing them from scratch. NAIL is available in both Chinese
and English containing a total of 10, 296 âˆ—2 instances. Empirical results show
that state-of-the-art neural models struggle on NAIL with very poor accuracy (the
best result is 30.1% for NAIL and 36.2% for Chinese NAIL), while human experts
can perform 100% accuracy. Further results indicate that human imitations can
signiï¬cantly help models learn naÂ¨Ä±ve logical reasoning ability."
INTRODUCTION,0.004555808656036446,"1
INTRODUCTION"
INTRODUCTION,0.00683371298405467,"Current deep models have achieved near human-level performance on many tasks in NLP (Devlin
et al., 2019; Liu et al., 2019), and more often than not, superï¬cial knowledge sufï¬ces to solve the
problems. To move towards human intelligence, we need to equip the models with logical reasoning
capabilities (e.g., ability to draw logical conclusions from given statements), which is also a long
sought-after goal of AI (Newell & Simon, 1956; McCarthy et al., 1960). One related task is natural
language inference whose goal is to assign the logical relationships (contradicted, neutral
and entailment) to sentence pairs. To push the development of models in logical reasoning, the
researchers have focused on more challenging reading comprehension tasks, which often require
more complex reasoning as well as longer input. However, most existing reading comprehension
datasets are not oriented for the logical reasoning (e.g., SQuAD and HotpotQA), with the exception
of LogiQA (Liu et al., 2020), ReClor (Yu et al., 2020) and LR-LSAT (Wang et al., 2021)."
INTRODUCTION,0.009111617312072893,"Above datasets (LogiQA, ReClor and LR-LSAT) are limited and inadequate to train and evaluate
logical reasoning capability. The reason is that all of the three datasets involve diverse types of logical
reasoning, such as drawing an alternate conclusion to the argument, ï¬nding necessary/sufï¬cient
assumptions, whether statements strengthen/weaken the argument or explain/resolve the situation.
Mixing multiple types of logical reasoning may pose the following challenges. a). From the perspec-
tive of human cognition, different types of logical reasoning correspond to different problem-solving
ideas. But in practice we usually train a model on the whole dataset with the same idea, which makes
it more limited for models to learn different logical reasoning capability. b). From the perspective
of machine learning, if there are many reasoning types mixed in a dataset, then there will be less
data for each reasoning type, which is inadequate to train and evaluate logical reasoning capability
(demonstrated in our experiments). Furthermore, when the model does not work, it is difï¬cult to
determine which reasoning type is the bottleneck (no reasoning type annotation in the dataset), which
may hinder the design of better models."
INTRODUCTION,0.011389521640091117,"To tackle the challenges, we focus on a more ï¬ne-grained type of logical reasoning, named naÂ¨Ä±ve
logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe
triples (subject, predicate, object) and the relationships among them. A typical example of naÂ¨Ä±ve"
INTRODUCTION,0.01366742596810934,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.015945330296127564,"Paragraph: Aï¼ŒBï¼ŒCï¼ŒDä¸ºå››ä½æ¼‚äº®å¥³ç”Ÿã€‚å¥¹ä»¬å–œæ¬¢ç©¿æ¼‚äº®è¡£æœã€‚æŸå¤©ï¼Œå¥¹ä»¬ç©¿çš„è¡£æœé¢œè‰²å„ä¸ç›¸åŒï¼Œæœ‰é»„è‰²ï¼Œç»¿è‰²ï¼Œè“è‰²
å’Œçº¢è‰²å››ç§ï¼Œåœ¨é—®åˆ°å¥¹ä»¬å„è‡ªè¡£æœçš„é¢œè‰²æ—¶ï¼ŒAè¯´ï¼šâ€œBçš„è¡£æœä¸æ˜¯é»„è‰²çš„ã€‚â€Bè¯´ï¼šâ€œCçš„è¡£æœä¸æ˜¯ç»¿è‰²çš„ã€‚â€Cè¯´ï¼šâ€œDçš„è¡£æœä¸
æ˜¯è“è‰²çš„ã€‚â€Dè¯´ï¼šâ€œAï¼ŒBï¼ŒCä¸‰äººä¸­æœ‰ä¸€ä¸ªäººçš„è¡£æœæ˜¯ç»¿è‰²çš„ï¼Œè€Œä¸”åªæœ‰è¿™ä¸ªäººè¯´çš„æ˜¯å®è¯ã€‚â€ (There are four pretty girls, 
namely A, B, C, and D, all of whom love to dress beautifully. One day, four girls get dressed in different colors: yellow, green, blue, and
red. When asked about the color, A said that B did not dress in yellow; B said that C did not dress in green; C said that D did not dress 
in blue; D said that among A, B, and C, only one girl dressed in green and she was the only one who told the truth.)"
INTRODUCTION,0.018223234624145785,Color of clothes
INTRODUCTION,0.02050113895216401,"Y
G
B
R"
INTRODUCTION,0.022779043280182234,"A
-
-
-
-"
INTRODUCTION,0.025056947608200455,"B
-
-
-
-
C
-
-
-
-"
INTRODUCTION,0.02733485193621868,"D
-
0
-
- S P O"
INTRODUCTION,0.029612756264236904,Color of clothes
INTRODUCTION,0.03189066059225513,"Y
G
B
R
A
0"
INTRODUCTION,0.03416856492027335,"B
0/1
1
0
0"
INTRODUCTION,0.03644646924829157,"C
0
D
0 S P O"
INTRODUCTION,0.0387243735763098,A told the truth
INTRODUCTION,0.04100227790432802,B told the truth
INTRODUCTION,0.04328018223234624,C told the truth
INTRODUCTION,0.04555808656036447,Color of clothes
INTRODUCTION,0.04783599088838269,"Y
G
B
R
A
0
1
0
0 B
0
0"
INTRODUCTION,0.05011389521640091,"C
0/1
D
0 S P O"
INTRODUCTION,0.05239179954441914,Color of clothes
INTRODUCTION,0.05466970387243736,"Y
G
B
R"
INTRODUCTION,0.05694760820045558,"A
0
0
1
0
B
1
0
0
0"
INTRODUCTION,0.05922551252847381,"C
0
1
0
0"
INTRODUCTION,0.06150341685649203,"D
0
0
0
1"
INTRODUCTION,0.06378132118451026,Cghv+HGhSJu/Rl3/o2TNgtPTBwOde7pkTJwp7TjfVmVldW19o7pb23v7O7V9g86Kk4loW0S81j2AqwoZ4K2NdOc9hJcRw2g0mt4XfaJSsVg86mlC/QiPBAsZwdpInu1FWI8J5tlDPqjVnYzA1ombknqUKI1qH15w5ikERWacKxU3US7WdYakY4zW0vVTBZIJHtG+owBFVfjbLnKMTowxRGEvzhEYz9fdGhiOlplFgJouIatErxP+8fqrDaz9jIk1FWR+KEw50jEqCkBDJinRfGoIJpKZrIiMscREm5psU4K7+OVl0jlruJeN8/uLevOmrKMKR3AMp+DCFThDlrQBgIJPMrvFmp9WK9Wx/z0YpV7hzCH1ifP8LSkYU=</latexit>S P
INTRODUCTION,0.06605922551252848,O2iy09cDA4Zx7uWdOmHKmtOt+W5WV1bX1jeqmvbW9s7vn7B+0VZJQlsk4YnshlhRzgRtaY57aS4jktBObwu/80SlYol41JOUBjEeChYxgrWRfNuPsR4RzP7ad+puXV3BrRMvJLUoESz73z5g4RkMRWacKxUz3NTHeRYakY4ndp+pmiKyRgPac9QgWOqgnyWeYpOjDJAUSLNExrN1N8bOY6VmsShmSwiqkWvEP/zepmOroOciTVJD5oSjSCeoKAANmKRE84khmEhmsiIywhITbWqyTQne4peXSfus7l3Wzx8uao2bso4qHMExnIHV9CAO2hCwik8Ayv8GZl1ov1bn3MRytWuXMIf2B9/gC8vpGB</latexit>O
INTRODUCTION,0.0683371298405467,"Q: å¦‚æœDè¯´çš„æ˜¯å®è¯é‚£ä¹ˆä»¥ä¸‹è¯´æ³•ä¸­æ­£ç¡®çš„æ˜¯ï¼Ÿ(If D told the truth, then which of the 
following statements is correct?)
A. Cçš„è¡£æœæ˜¯ç»¿è‰²çš„ï¼ŒDçš„è¡£æœæ˜¯çº¢è‰²çš„(C is dressed in green, and D is dressed in red.)
B. Açš„è¡£æœæ˜¯ç»¿è‰²çš„ï¼ŒBçš„è¡£æœæ˜¯çº¢è‰²çš„(A is dressed in green, and B is dressed in red.)
C. Bçš„è¡£æœæ˜¯è“è‰²çš„ï¼ŒCçš„è¡£æœæ˜¯çº¢è‰²çš„(B is dressed in blue, and C is dressed in red.)
D. Dçš„è¡£æœæ˜¯ç»¿è‰²çš„ï¼ŒAçš„è¡£æœæ˜¯çº¢è‰²çš„(D is dressed in green, and A is dress in red.)"
INTRODUCTION,0.07061503416856492,"Figure 1: An example of NAIL requiring naÂ¨Ä±ve logical reasoning. Highlighted option A is the correct answer.
Tables depict the reasoning process from the human perspective. - indicates uncertain state. 0 and 1 indicate
false and true respectively. Red grid with â€œ0/1â€ means there is a conï¬‚ict."
INTRODUCTION,0.07289293849658314,"logical reasoning is shown in Figure 1. To answer this query, we need to iteratively derive conclusions
according to the conditions, and stop the searching branch if a conï¬‚ict occurs. Speciï¬cally, in the
initial table, we only know that â€œD is not greenâ€ from â€œD told the truthâ€. Assume that â€œA told
the truthâ€, and infer that â€œA is green, C is non-greenâ€. Next derive â€œC is greenâ€ from â€œC told the
lieâ€. Then C is derived as both green and non-green, thus causing a conï¬‚ict. Similar processes for
assuming â€œB told the truthâ€ and â€œC told the truthâ€. It takes extensive training and practice for human
brains to cope with such complex logical reasoning."
INTRODUCTION,0.07517084282460136,"Inspired by the datasets extracted from standardized examinations (Lai et al., 2017; Clark et al., 2018;
Liu et al., 2020), we build a new large-scale benchmark, NAIL, by selecting naÂ¨Ä±ve logical reasoning
examples from standardized exams such as the National Civil Servants Examination of China and
Law School Admission Test. However, such examples are limited in their number, as it takes efforts
for human experts to design these questions. To collect more data, we propose to imitate examples of
standardized exams rather than designing them from scratch. Unlike simple data augmentation (e.g.,
substitution, paraphrasing), human imitation aims at getting more diverse examples while maintaining
the underlying logic (Figure 1). NAIL is available in both Chinese and English, containing a total of
10, 296 âˆ—2 instances."
INTRODUCTION,0.0774487471526196,"Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy
(the best result is 30.1% for NAIL and 36.2% for Chinese NAIL), while human experts can perform
100% accuracy. Further results indicate that human imitations can signiï¬cantly help models learn
naÂ¨Ä±ve logical ability from natural text."
INTRODUCTION,0.07972665148063782,"2
NAÂ¨IVE LOGICAL REASONING"
INTRODUCTION,0.08200455580865604,"The naÂ¨Ä±ve logical reasoning is a more ï¬ne-grained type of logical reasoning. Formally, we give the
deï¬nition as follows.
Deï¬nition 1. The subject is a described resource, usually an entity, such as person or location. The
predicate indicates an attribute of the subject or indicates some kind of relationship between the
subject and the object. When denoting an attribute, the object is the attribute value, usually a literal
value, e.g., (Jacket, Color, Red), otherwise the object is an entity, e.g., (Beijing, Capital, China).
Deï¬nition 2. Assuming the set of subjects S, the set of predicates P, the set of objects O and several
statements, each statement describes a triple (subject, predicate, object) or some logical relationship"
INTRODUCTION,0.08428246013667426,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.08656036446469248,"between triples. The naive logical reasoning is the process of reasoning from these statements to
reach a logical conclusion. 1"
INTRODUCTION,0.0888382687927107,"In this work, we explore naÂ¨Ä±ve logical reasoning in the form of reading comprehension. A typical
example and detailed reasoning process is shown in Figure 1. Similar to the format of multiple-choice
reading comprehension, it contains a context, a query and four options with only one correct answer.
To solve the problem, the model needs to understand the logical connections between the subjects,
predicates and objects, and then derive a valid option."
INTRODUCTION,0.09111617312072894,"3
NAIL: DATA COLLECTION AND ANALYSIS"
INTRODUCTION,0.09339407744874716,"NAIL is a carefully designed benchmark for naÂ¨Ä±ve logical reasoning similar to the format of multiple-
choice reading comprehension. Inspired by the datasets extracted from standardized examinations (Lai
et al., 2017; Clark et al., 2018; Liu et al., 2020), we ï¬rst collect a small amount of examples from
examinations, denoted as NAIL-E (Section 3.1). Then we propose to imitate examples of NAIL-E
to collect more data, denoted as NAIL-I (Section 3.2). Finally we provide a detailed analysis of the
proposed NAIL (Section 3.3)."
COLLECTION FROM EXAMINATION,0.09567198177676538,"3.1
COLLECTION FROM EXAMINATION"
COLLECTION FROM EXAMINATION,0.0979498861047836,"We searched for such examples widely from two different types of public examinations: Chinese
National Civil Servants Examination (CNCSE) and Law School Admission Test (LSAT). 2 CNCSE
is a once-a-year competitive examination in China, and there are overall 120-140 examples per exam
per year. But only 1-4 examples belong to the scope of naÂ¨Ä±ve logical reasoning, which is also the
most difï¬cult type of problems for candidates within the given 120 minutes. And the LSAT is a
standardized test for prospective law school candidates in the United States, Canada, and a growing
number of other countries. Logical Reasoning is a multiple-choice section of LSAT, containing
24-26 questions under the limitation of 35 minutes, where about 2-4 problems fall into naÂ¨Ä±ve logical
reasoning category."
COLLECTION FROM EXAMINATION,0.10022779043280182,"We artiï¬cially selected examples that belong to the naÂ¨Ä±ve logical reasoning category from the above
two examinations. Finally we obtained 488 examples from the last 25 years of CNCSE and LSAT
in the last 30 years, denoted as NAIL-E. These two exams are from countries with different native
languages. The former is expressed in Chinese, and the latter is expressed in English. And note that
there are slight difference in language style between them. And for diversity and fairness, in later
step we get all examples in both Chinese and English through translation."
COLLECTION FROM IMITATION,0.10250569476082004,"3.2
COLLECTION FROM IMITATION"
COLLECTION FROM IMITATION,0.10478359908883828,"After collecting a small amount of NAIL-E, we expect to expand the number of the dataset. Designing
examples from scratch requires a huge effort from human experts. One simple solution is data
augmentation, which artiï¬cially scales up data by creating modiï¬ed data from existing data, such as
word/sentence shufï¬‚ing, word replacement and syntactic variation. However, the data augmented
in this way is highly correlated with the original data, and the model easily captures these semantic
surface correlations, which makes it limited to train and evaluate logical reasoning capability. To
alleviate this limitation, we propose to imitate examples of NAIL-E, to create more examples with a
diverse semantic surface while keeping the underlying logic of the original example. Furthermore, in
the process of human imitation, we design strict strategies to control the quality of imitation.
Imitation Example
See Figure 2, the context of an example from NAIL-E consists of 6 sentences
and a query (split by blank lines), each of which can be represented as a logical template (see
â€œbackbone templateâ€). The example focus on the description of a scenario: picking Prince
Charming (m1), which involves ï¬ve entities: Li Na (s1), Wang Wei (p1), Wu Gang (p2),
Li Qiang (p3), Liu Dawei (p4), and three noun/adjective properties: tall (a1), handsome
(a2), a PhD (a3). An accepted imitation needs to keep original underlying logic but have seman-
tically different groundings, i.e., to describe a completely different scenario. Imitation 1 and 2"
COLLECTION FROM IMITATION,0.1070615034168565,"1The term naive refers to the fact that this logical reasoning process of human in this task is spontaneous,
intuitive and unsystematic.
2https://www.lsac.org/lsat/taking-lsat/test-format/logical-reasoning"
COLLECTION FROM IMITATION,0.10933940774487472,Under review as a conference paper at ICLR 2022
COLLECTION FROM IMITATION,0.11161731207289294,"ğ’”ğŸå¿ƒä¸­çš„ğ’ğŸæœ‰å¦‚ä¸‹ç‰¹å¾ï¼š
ğ’‚ğŸ, ğ’‚ğŸ, and ğ’‚ğŸ‘.
(ğ’”ğŸâ€™s ğ’ğŸis supposed to 
be: ğ’‚ğŸ, ğ’‚ğŸ, and ğ’‚ğŸ‘.)"
COLLECTION FROM IMITATION,0.11389521640091116,"æå¨œå¿ƒä¸­çš„ç™½é©¬ç‹å­æœ‰å¦‚ä¸‹
ç‰¹å¾ï¼šé«˜ä¸ªå­ã€ç›¸è²Œè‹±ä¿Šã€
åšå£«ã€‚
(Li Na's Prince Charming 
is supposed to be: tall, 
handsome, and a PhD.)"
COLLECTION FROM IMITATION,0.11617312072892938,"å­™æ€¡å¿ƒä¸­çš„ç™½é©¬ç‹å­æœ‰å¦‚ä¸‹
ç‰¹å¾ï¼šé«˜ä¸ªå­ã€ç›¸è²Œè‹±ä¿Šã€
åšå£«ã€‚(Sun Yi's Prince 
Charming is supposed to 
be: tall, handsome, and a 
PhD.)"
COLLECTION FROM IMITATION,0.11845102505694761,"å­™æ€¡å¿ƒä¸­çš„ç™½é©¬ç‹å­æœ‰å¦‚ä¸‹
ç‰¹å¾ï¼šæœ‰é’±ã€æœ‰æˆ¿ã€æœ‰è½¦ã€‚
(Sun Yi's Prince 
Charming is supposed to 
be: rich, has a house, and 
has a car.)"
COLLECTION FROM IMITATION,0.12072892938496584,"å°ä¸½å¿ƒä¸­çš„ç†æƒ³ç¤¼ç‰©æœ‰å¦‚ä¸‹
ç‰¹å¾ï¼šæ˜‚è´µï¼Œæœ‰æ”¶è—ä»·å€¼ï¼Œ
ç¾è§‚ã€‚(Xiao Liâ€™s ideal gift 
is supposed to be: 
expensive, valuable to 
collect, and nice-looking.)"
COLLECTION FROM IMITATION,0.12300683371298406,"å¥¹è®¤è¯†ğ’‘ğŸ, ğ’‘ğŸ, ğ’‘ğŸ‘, ğ’‘ğŸ’4ï¼ˆé‡
è¯ï¼‰ï¼ˆğ’‘çš„ç±»åˆ«ï¼‰,å…¶ä¸­æœ‰ä¸€
ç¬¦åˆğ’”ğŸæ‰€è¦æ±‚çš„å…¨éƒ¨æ¡ä»¶ã€‚
(She knows 4 type_of(ğ’‘ğ’Š), 
ğ’‘ğŸ, ğ’‘ğŸ, ğ’‘ğŸ‘, ğ’‘ğŸ’, and only one 
meets all ğ’”ğŸâ€™s
requirements.)"
COLLECTION FROM IMITATION,0.1252847380410023,"å¥¹è®¤è¯†ç‹å¨ã€å´åˆšã€æå¼ºã€
åˆ˜å¤§ä¼Ÿ4ä½ç”·å£«,å…¶ä¸­æœ‰ä¸€ä½
ç¬¦åˆå¥¹æ‰€è¦æ±‚çš„å…¨éƒ¨æ¡ä»¶ã€‚
(She knows 4 men, Wang 
Wei, Wu Gang, Li Qiang
and Liu Dawei, and only 
one meets all her 
requirements.)"
COLLECTION FROM IMITATION,0.1275626423690205,"å¥¹è®¤è¯†ç¿è¡¡ã€æ¢…éœ‡ã€å­£å‡¡ã€
å¼ ç£Š4ä½ç”·å£«,å…¶ä¸­æœ‰ä¸€ä½ç¬¦
åˆå¥¹æ‰€è¦æ±‚çš„å…¨éƒ¨æ¡ä»¶ã€‚
(She knows 4 men, Qu 
Heng, Mei Zhen, Ji Fan, 
and Zhang Lei, and only 
one meets all her 
requirements.)"
COLLECTION FROM IMITATION,0.12984054669703873,"å¥¹è®¤è¯†ç¿è¡¡ã€æ¢…éœ‡ã€å­£å‡¡ã€
å¼ ç£Š4ä½ç”·å£«,å…¶ä¸­æœ‰ä¸€ä½ç¬¦
åˆå¥¹æ‰€è¦æ±‚çš„å…¨éƒ¨æ¡ä»¶ã€‚
(She knows 4 men, Qu 
Heng, Mei Zhen, Ji Fan, 
and Zhang Lei, and only 
one meets all her 
requirements.)"
COLLECTION FROM IMITATION,0.13211845102505695,"å¥¹çŸ¥é“æ‰‹é“¾ã€æ‰‹è¡¨ã€æ‰‹é•¯ã€
æˆ’æŒ‡è¿™4ä¸ªç¤¼ç‰©,å…¶ä¸­æœ‰ä¸€ä¸ª
ç¬¦åˆå¥¹æ‰€è¦æ±‚çš„å…¨éƒ¨æ¡ä»¶ã€‚
(She knows 4 gifts, 
bracelets, watches, 
bangles, and rings, and 
only one meets all her 
requirements.)"
COLLECTION FROM IMITATION,0.13439635535307518,"4ï¼ˆé‡è¯ï¼‰ï¼ˆğ’‘çš„ç±»åˆ«ï¼‰ä¸­,æœ‰
3ï¼ˆé‡è¯ï¼‰æ˜¯ğ’‚ğŸ,æœ‰2ï¼ˆé‡è¯ï¼‰
æ˜¯ğ’‚ğŸ‘, æœ‰1ï¼ˆé‡è¯ï¼‰æ˜¯ğ’‚ğŸã€‚
(Among the 4 type_of(ğ’‘ğ’Š), 
ğŸ‘are ğ’‚ğŸ, ğŸare ğ’‚ğŸ‘, and ğŸis 
ğ’‚ğŸ.)"
COLLECTION FROM IMITATION,0.1366742596810934,"(1)4ä½ç”·å£«ä¸­,æœ‰3ä¸ªé«˜ä¸ª
å­,2ååšå£«,1äººé•¿ç›¸è‹±ä¿Šã€‚
(Among the 4 men, 3 are 
tall, 2 are PhDs and 1 is 
handsome.)"
COLLECTION FROM IMITATION,0.13895216400911162,"(1) 4ä½ç”·å£«ä¸­,æœ‰3ä¸ªé«˜ä¸ª
å­,2ååšå£«,1äººé•¿ç›¸è‹±ä¿Šã€‚
(Among the 4 men, 3 are 
tall, 2 are PhDs and 1 is 
handsome.)"
COLLECTION FROM IMITATION,0.14123006833712984,"(1) 4ä½ç”·å£«ä¸­,æœ‰3ä¸ªæœ‰é’±,2
äººæœ‰è½¦,1äººæœ‰æˆ¿ã€‚(Among 
the 4 men, 3 are rich, 2 
have a car and 1  has a 
house.)"
COLLECTION FROM IMITATION,0.14350797266514806,"(1) 4ä¸ªç¤¼ç‰©ä¸­,æœ‰3ä¸ªå¾ˆæ˜‚
è´µ,2ä¸ªå¾ˆç¾è§‚,1ä¸ªå…·æœ‰æ”¶è—
ä»·å€¼ï¼›(Among the 4 gifts, 
3 are expensive, 2 are 
nice-looking and 1 is 
valuable to collect;)"
COLLECTION FROM IMITATION,0.14578587699316628,"ğ’‘ğŸå’Œğ’‘ğŸéƒ½æ˜¯ğ’‚ğŸ‘ã€‚
( ğ’‘ğŸand ğ’‘ğŸare both ğ’‚ğŸ‘.)"
COLLECTION FROM IMITATION,0.1480637813211845,"(2)ç‹å¨å’Œå´åˆšéƒ½æ˜¯åšå£«ã€‚
(Wang Wei and Wu Gang 
are both PhDs.)"
COLLECTION FROM IMITATION,0.15034168564920272,"(2) ç¿è¡¡å’Œæ¢…éœ‡éƒ½æ˜¯åšå£«ã€‚
(Qu Heng and Mei Zhen 
are both PhDs.)"
COLLECTION FROM IMITATION,0.15261958997722094,"(2) ç¿è¡¡å’Œæ¢…éœ‡éƒ½æœ‰è½¦ã€‚(Qu 
Heng and Mei Zhen both 
have a car.)"
COLLECTION FROM IMITATION,0.1548974943052392,"(2) æ‰‹é“¾å’Œæ‰‹è¡¨éƒ½å¾ˆç¾ã€‚
(Bracelets and watches 
are both nice-looking.)"
COLLECTION FROM IMITATION,0.1571753986332574,"ğ’‘ğŸ’å’Œğ’‘ğŸ‘å…³äºğ’‚ğŸçš„å±æ€§ç›¸åŒã€‚
(ğ’‘ğŸ’and ğ’‘ğŸ‘are of the same 
attribute_of ğ’‚ğŸ.)"
COLLECTION FROM IMITATION,0.15945330296127563,"(3)åˆ˜å¤§ä¼Ÿå’Œæå¼ºèº«é«˜ç›¸åŒã€‚
(Liu Dawei and Li Qiang
are of the same height.)"
COLLECTION FROM IMITATION,0.16173120728929385,"(3) å¼ ç£Šå’Œå­£å‡¡èº«é«˜ç›¸åŒã€‚
(Zhang Lei and Ji Fan are 
of the same height.)"
COLLECTION FROM IMITATION,0.16400911161731208,"(3) å¼ ç£Šå’Œå­£å‡¡æœ‰åŒç­‰ç¨‹åº¦
çš„é’±ã€‚(Zhang Lei and Ji 
Fan are of the same 
wealth.)"
COLLECTION FROM IMITATION,0.1662870159453303,"(3) æˆ’æŒ‡å’Œæ‰‹é•¯ä»·æ ¼ç›¸åŒã€‚
(Rings and bangles are of 
the same price.)"
COLLECTION FROM IMITATION,0.16856492027334852,"ğ’‘ğŸ‘å’Œğ’‘ğŸå¹¶ééƒ½æ˜¯ğ’‚ğŸã€‚
(Either ğ’‘ğŸ‘or ğ’‘ğŸğ¢ğ¬ğ’‚ğŸ.)"
COLLECTION FROM IMITATION,0.17084282460136674,"(4)æå¼ºå’Œç‹å¨å¹¶ééƒ½æ˜¯é«˜ä¸ª
å­ã€‚(Either Li Qiang or 
Wang Wei is tall.)"
COLLECTION FROM IMITATION,0.17312072892938496,"(4) å­£å‡¡å’Œç¿è¡¡å¹¶ééƒ½æ˜¯é«˜
ä¸ªå­ã€‚(Either Ji Fan or Qu 
Heng is tall.)"
COLLECTION FROM IMITATION,0.17539863325740318,"(4) å­£å‡¡å’Œç¿è¡¡å¹¶ééƒ½å¾ˆå¯Œ
æœ‰ã€‚(Either Ji Fan or Qu 
Heng is rich.)"
COLLECTION FROM IMITATION,0.1776765375854214,"(4) æ‰‹é•¯å’Œæ‰‹é“¾å¹¶ééƒ½æ˜¯æ˜‚
è´µçš„ã€‚(Either Bangles or 
bracelets is expensive.)"
COLLECTION FROM IMITATION,0.17995444191343962,"è¯·é—®è°ç¬¦åˆğ’”ğŸè¦æ±‚çš„å…¨éƒ¨æ¡
ä»¶ï¼Ÿ
(Who meets all the 
requirements of ğ’”ğŸ?)"
COLLECTION FROM IMITATION,0.18223234624145787,"è¯·é—®è°ç¬¦åˆæå¨œè¦æ±‚çš„å…¨éƒ¨
æ¡ä»¶?
(Who meets all the 
requirements of Li Na?)"
COLLECTION FROM IMITATION,0.1845102505694761,"è¯·é—®è°ç¬¦åˆå­™æ€¡è¦æ±‚çš„å…¨éƒ¨
æ¡ä»¶?
(Who meets all the 
requirements of Sun Yi?)"
COLLECTION FROM IMITATION,0.1867881548974943,"è¯·é—®è°ç¬¦åˆå­™æ€¡è¦æ±‚çš„å…¨éƒ¨
æ¡ä»¶?
(Who meets all the 
requirements of Sun Yi?)"
COLLECTION FROM IMITATION,0.18906605922551253,"è¯·é—®å“ªä¸ªç¬¦åˆå°ä¸½è¦æ±‚çš„å…¨
éƒ¨æ¡ä»¶?
(Which meets all the 
requirements of Xiao Li?)"
COLLECTION FROM IMITATION,0.19134396355353075,"A. ğ’‘ğŸ’
B. ğ’‘ğŸ‘
C. ğ’‘ğŸ
D. ğ’‘ğŸ"
COLLECTION FROM IMITATION,0.19362186788154898,"A.åˆ˜å¤§ä¼Ÿ(Liu Dawei)
B.æå¼º(Li Qiang)
C.å´åˆš(Wu Gang)
D.ç‹å¨(Wang Wei)"
COLLECTION FROM IMITATION,0.1958997722095672,"A.å¼ ç£Š(Zhang Lei)
B.å­£å‡¡(Ji Fan)
C.æ¢…éœ‡(Mei Zhen)
D.ç¿è¡¡(Qu Heng)"
COLLECTION FROM IMITATION,0.19817767653758542,"A.å¼ ç£Š(Zhang Lei)
B.å­£å‡¡(Ji Fan)
C.æ¢…éœ‡(Mei Zhen)
D.ç¿è¡¡(Qu Heng)"
COLLECTION FROM IMITATION,0.20045558086560364,"A.æˆ’æŒ‡(Rings)
B.æ‰‹é•¯(Bangles)
C.æ‰‹è¡¨(Watches)
D.æ‰‹é“¾(Bracelets)"
COLLECTION FROM IMITATION,0.20273348519362186,"Original Example
Imitation 1
Imitation 2
Imitation 3
Backbone Template"
COLLECTION FROM IMITATION,0.20501138952164008,"Figure 2: An original example from CNCSE and its three imitations, which share the same backbone template.
Imitation 1 is subject-level. Imitation 2 is subject-and-object-level. And Imitation 3 is subject-and-predicate-and-
object-level."
COLLECTION FROM IMITATION,0.2072892938496583,"are unqualiï¬ed imitations. Since Imitation 1 only conducts subject-level imitations (S imitations),
i.e., only superï¬cially substitute the ï¬ve entities (s1:Li Naâ†’Sun Yi, p1:Wang Weiâ†’Qu Heng,
p2:Wu Gangâ†’Mei Zhen, p3:Li Qiangâ†’Ji Fan, p4:Liu Daweiâ†’Zhang Lei). Further, Im-
itation 2 conducts subject-and-object-level imitations (SO imitations), i.e., not only substituting
the entities, but also altering the corresponding objects (property values).
(a1:tallâ†’rich,
a2:handsomeâ†’having a house, a3:a PhDâ†’having a car). The difference between Imi-
tation 2 and the original example is much greater than that between Imitation 1 and and the raw
problem, however, Imitation 2 is still not an ideal imitation. Furthermore, Imitation 3 changes the
scenario m1 into picking ideal gift. Imitation 3 is an expected imitative writing, which
conducts subject-and-predicate-and-object-level imitations (SPO imitations). Note that all of the
three imitations share the same logic templates. And p2 is the answer for all of the original example
and above three imitations."
COLLECTION FROM IMITATION,0.20956719817767655,"Imitation Process
We ï¬rst select a group of people from a variety of occupations: professional
editors, legal practitioners, in-service civil servants and college students (from different majors).
Empirically, people in these occupations have strong logical and verbal skills. These people are
asked to conduct SPO imitations based on original examples from NAIL-E, and reasonable imitations
should meet two requirements: logic invariance and semantic diversity. We trained these candidate
people how to conduct SPO imitations such as Figure 2. We then conducted a trial phase before the
ofï¬cial imitation phase, in which process we eliminated some people of poor quality. Finally we
employed 82 qualiï¬ed people3 to imitatively construct problems based on NAIL-E, and they are paid
RMBÂ¥2.8 per imitation4. Averagely, it costs a trained person about 3-4 minutes to ï¬nish an imitation:
starting from coming up a scenario, then replacing the subjects, predicates, and objects of the raw"
COLLECTION FROM IMITATION,0.21184510250569477,"3Consisting of 42 native Chinese speakers and 40 native English speakers. Native speakers of a language
will be assigned imitation tasks expressed by that language.
4A part-time employee can produce 15-20 imitations per hour, where he/she can get RMBÂ¥42-RMBÂ¥56,
while the local minimum wage is RMBÂ¥23 per hour."
COLLECTION FROM IMITATION,0.214123006833713,Under review as a conference paper at ICLR 2022
COLLECTION FROM IMITATION,0.2164009111617312,"problem with those scenario-related while keeping invariant logic, and ï¬nally smoothing the new
sentence with some transition words if necessary. For each original example in NAIL-E, we expect at
least 20 imitations (except for extremely difï¬cult cases). Overall we use 813 paid work hours in total
to build the NAIL-I."
COLLECTION FROM IMITATION,0.21867881548974943,"Imitation Quality Control
We adopt following strategies to ensure the quality of imitations:
1. As mentioned above, we conducted a trial phase before the ofï¬cial imitation phase. In this phase,
we asked them to imitate a small number of problems. Although we do not necessarily need them
to write the backbone template, we will check the logic and give feedback to help them understand
the task. This process was iterated for three rounds. Only those who passed the trial phase could
participate in the ofï¬cial imitation.
2. During the ofï¬cial imitation, we set up an online chat room to communicate with employees and
answer their questions timely.
3. To embrace semantic diversity, each original example is shown to at least 5 people, that is, one
can only conduct 4 imitations based on one original example. People who are assigned to the same
example imitate independently without interference from each other, to ensure varied inspiration for
imitation.
4. To ensure logic invariance, we adopt a double-checking strategy:"
COLLECTION FROM IMITATION,0.22095671981776766,"â€¢ Cross Checking: Everyday, for each employee, we sample 5 imitations from all of his/her daily
imitations. And the sampled imitation is assigned to other 2 employees for cross-checking.
The imitation will be qualiï¬ed only if they both approved, and the criterion for approval is that
the imitation share the same logics with the original example. If any one of the 5 imitations
produced by one employee fail, then all imitations of that employee for that day will be returned
to re-check and repair.
â€¢ Post Checking: To further ensure that the underlying logic do not deviate during imitation,
we introduce another team of experts to solve the imitative examples. The team consists of 20
experienced experts. 10 of them speaks Chinese as native language and have passed CNCSE,
while the other 10 speaks English and have passed LSAT. 5 Each imitative example was presented
to 3 experts randomly, who are allowed to select one and only choice from â€œAâ€, â€œBâ€, â€œCâ€, â€œDâ€,
otherwise, â€œUNABLE TO ANSWERâ€ if bugs exist in the example, causing no correct choice or
multiple correct choices. As long as one of the 3 experts pointed out â€œUNABLE TO ANSWERâ€,
then the imitator of this problem should recheck the logic, until each of these 3 experts could
give a choice from â€œAâ€, â€œBâ€, â€œCâ€, â€œDâ€. Note that in the post checking process, we broke up the
imitations together and shufï¬‚e randomly, otherwise if a person is faced with imitations from
same original example, he/she is prone to give a shortcut option with speculation."
COLLECTION FROM IMITATION,0.22323462414578588,"Translation Quality Control
After collecting high-quality mono-collections, we ï¬rst adopted
Google Translation to translate Chinese/English collections into another language, and then employed
10 professional bilingual experts in Chinese and English for manual correction. Bilingual experts
were asked to pay attention to logic-invariance and faithfulness during translation. Next, to ensure
translation quality, we also adopted the post checking strategy. That is, we asked the 20 human
experts mentioned above to solve the translated examples. Each translated example was presented
to 3 experts randomly. Since human experts excel in solving naÂ¨Ä±ve logical reasoning problems, (i.e.
achieve 100% accuracy on mono-collections), if any expert made a mistake on a translated sample
or pointed out â€œUNABLE TO ANSWERâ€, the translated instance is sent back to the bilingual experts
for revision. Finally, we asked 50 native speakers to read through all paragraphs of the translation
parts in NAIL and mark â€œ0â€/â€œ1â€ for each, where â€œ1â€ stands for a translated sample is idiomatic, and
â€œ0â€ otherwise. Then for all samples marked with â€œ0â€ (about 20%), the bilingual experts and native
speakers will work together to polish them and conform to the target language norms."
COLLECTION FROM IMITATION,0.2255125284738041,"Human Evaluation
As mentioned above, an imitation is ï¬nally regarded as qualiï¬ed only if the
sampled 3 experts could all solve the example. For any original example in NAIL-E, we also ask
3 experts in the team to solve it. Since the gold answers to examples in NAIL-E are provided in
public by the examination committee, and corresponding imitations in NAIL-I share the same answer
with the original example. Therefore, we calculated the mean accuracy of these three submissions
on the overall NAIL, denoted as the performance of human experts. To better demonstrate the"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.22779043280182232,"5Experts who are native English speakers will be assigned English problems and experts who are native
Chinese speakers will be assigned Chinese problems."
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.23006833712984054,Under review as a conference paper at ICLR 2022
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.23234624145785876,"Paragraph: æŸçœä¸¾è¡Œâ€œæ–‡æ˜åŸå¸‚â€è¯„æ¯”ã€‚4ä½è¯„å§”å¯¹å¤§å®¶æ™®éçœ‹å¥½çš„Aã€Bã€Cç­‰3åŸå¸‚è·å¾—â€œæ–‡æ˜åŸå¸‚â€ç§°å·çš„å¯èƒ½æ€§è¿›è¡Œäº†åˆ†æ
é¢„æµ‹ã€‚è¯„å§”ç”²è¯´ï¼šâ€œè¦ä¹ˆAå¸‚èƒ½è·å¾—ï¼Œè¦ä¹ˆCå¸‚èƒ½è·å¾—ã€‚â€è¯„å§”ä¹™è¯´ï¼šâ€œå¦‚æœAå¸‚ä¸Cå¸‚èƒ½è·å¾—ï¼Œåˆ™Bå¸‚ä¹Ÿèƒ½è·å¾—ã€‚â€è¯„å§”ä¸™è¯´ï¼š
â€œåªæœ‰å½“Bå¸‚ä¸èƒ½è·å¾—æˆ–è€…Cå¸‚èƒ½è·å¾—æ—¶ï¼ŒAå¸‚æ‰ä¸èƒ½è·å¾—ã€‚â€è¯„å§”ä¸è¯´ï¼šâ€œæˆ‘çœ‹Bå¸‚èƒ½è·å¾—çš„å¯èƒ½æ€§ä¸ºé›¶ï¼Œè€ŒAå¸‚ä¸Cå¸‚ä¸€å®šèƒ½
è·å¾—ã€‚â€è¯„æ¯”ç»“æŸåå‘ç°ï¼Œ4ä½è¯„å§”ä¸­åªæœ‰ä¸€äººé¢„æµ‹æˆç«‹ã€‚(Three cities, namely City A, B, and C, took part in a competition to
win the title of ""a civilized city"", and four judges were making their predictions. Judge 1 said that either A or C would win.
Judge 2 said that if A and C had won, B would win too. Judge 3 said that only if B could not win or C won, A would not win.
Judge 4 said that it was impossible for B to win, and A & C was sure to win. After the results came out, only one judge
predicted correctly.)
Q: æ®æ­¤å¯ä»¥æ¨å‡ºèƒ½è·å¾—â€œæ–‡æ˜åŸå¸‚â€ç§°å·çš„åŸå¸‚æ˜¯? (Which city won the title?)
A. Aå¸‚(City A)
B. Bå¸‚(City B)
C. Cå¸‚(City C)
D. Aå¸‚ï¼ŒBå¸‚å’ŒCå¸‚(City A,B and C)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.23462414578587698,"Real/Fake
Mixture
(36.03%)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.23690205011389523,"Paragraph + Query + Answers
Reasoning Type"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.23917995444191345,"Paragraph: ç”²ã€ä¹™å’Œä¸™ï¼Œä¸€ä½æ˜¯å±±ä¸œäººï¼Œä¸€ä½æ˜¯æ²³å—äººï¼Œä¸€ä½æ˜¯æ¹–åŒ—äººã€‚ç°åœ¨åªçŸ¥é“ï¼šä¸™æ¯”æ¹–åŒ—äººå¹´é¾„å¤§ï¼Œç”²å’Œæ²³å—äººä¸åŒå²ï¼Œ
æ²³å—äººæ¯”ä¹™å¹´é¾„å°ã€‚(A, B, and C come from different places: Shandong province, Henan province, and Hubei province.
Now the following information is given: C is older than the one who comes from Hubei; A and the one who comes from Henan
are different in their age; the one who comes from Henan is younger than B.)
Q: ç”±æ­¤å¯ä»¥æ¨çŸ¥: (Which of the following statements can be inferred from the passage?)
A. ç”²ä¸æ˜¯æ¹–åŒ—äºº(A is not from Hubei.)
B. æ²³å—äººæ¯”ç”²å¹´é¾„å°(The one from Henan is younger than A.)
C. æ²³å—äººæ¯”å±±ä¸œäººå¹´é¾„å¤§(The one from Henan is older than that from Shandong.)
D. æ¹–åŒ—äººå¹´é¾„æœ€å°(The one from Hubei is the youngest.)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.24145785876993167,"Ordering
(20.40%)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.2437357630979499,"Paragraph: ç”²ã€ä¹™ã€ä¸™åœ¨åŒ—äº¬ã€å—äº¬å’Œæˆéƒ½å·¥ä½œï¼Œä»–ä»¬çš„èŒä¸šæ˜¯åŒ»ç”Ÿã€æ¼”å‘˜å’Œæ•™å¸ˆã€‚å·²çŸ¥ï¼šç”²ä¸åœ¨åŒ—äº¬å·¥ä½œï¼Œä¹™ä¸åœ¨å—äº¬å·¥ä½œï¼›
åœ¨åŒ—äº¬å·¥ä½œçš„ä¸æ˜¯æ•™å¸ˆï¼›åœ¨å—äº¬å·¥ä½œçš„æ˜¯åŒ»ç”Ÿï¼›ä¹™ä¸æ˜¯æ¼”å‘˜ã€‚(A, B, and C have different jobs (Beijing, Nanjing, and Chengdu)
in different cities (a doctor, an actor, and a teacher), and now the following information is given. A does not work in Beijing, B
does not work in Nanjing; The one who works in Beijing is not a teacher; The one who works in Nanjing is a doctor; B is not an
actor.)
Q: é‚£ä¹ˆ,ç”²,ä¹™,ä¸™åˆ†åˆ«åœ¨å“ªé‡Œå·¥ä½œ? (So, where do A, B and C work respectively?)
A. å—äº¬,æˆéƒ½å’ŒåŒ—äº¬(Nanjing, Chengdu and Beijing)
B. æˆéƒ½,åŒ—äº¬å’Œå—äº¬(Chengdu, Beijing and Nanjing)
C. å—äº¬,åŒ—äº¬å’Œæˆéƒ½(Nanjing, Beijing and Chengdu)
D. æˆéƒ½,å—äº¬å’ŒåŒ—äº¬(Chengdu, Nanjing and Beijing)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.2460136674259681,"Matching
(29.26)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.24829157175398633,"Paragraph: Så¸‚ä¸€æ‰€å°å­¦çš„å­¦ç”Ÿæˆ·ç±æƒ…å†µæ¯”è¾ƒå¤æ‚ï¼Œæ‰€æœ‰ä¸‰å¹´çº§å­¦ç”Ÿçš„æˆ·ç±éƒ½åœ¨æœ¬å¸‚ï¼Œæœ‰äº›äºŒå¹´çº§å­¦ç”Ÿçš„æˆ·ç±ä¹Ÿåœ¨æœ¬å¸‚ï¼Œæœ‰äº›
ä¸€å¹´çº§å­¦ç”Ÿæ˜¯å†œæ°‘å·¥å­å¼Ÿï¼Œè€Œå†œæ°‘å·¥å­å¼Ÿçš„æˆ·ç±éƒ½ä¸åœ¨æœ¬å¸‚ã€‚(An elementary school in City S has complicated Hukou (a
system of household registration in China) issues. All students in grade 3 are registered in the City S. Some of the students in
grade 2 are registered in the City S. The parents of some students in grade 1 are migrant workers, who are not registered in
the city S. )
Q: æ®æ­¤ï¼Œå¯ä»¥æ¨å‡ºï¼š(Which of the following statements can be inferred from the material?)
A. æ‰€æœ‰äºŒå¹´çº§å­¦ç”Ÿéƒ½ä¸æ˜¯å†œæ°‘å·¥å­å¼Ÿ(all students in grade 2 are not children of migrant workers)
B. æœ‰äº›å†œæ°‘å·¥å­å¼Ÿæ˜¯ä¸‰å¹´çº§å­¦ç”Ÿ(some migrant workers' children are in grade 3)
C. æœ‰äº›æˆ·ç±åœ¨æœ¬å¸‚çš„å­¦ç”Ÿæ˜¯ä¸‰å¹´çº§å­¦ç”Ÿ(some students registered in this city are in grade 3)
D. æœ‰äº›ä¸€å¹´çº§å­¦ç”Ÿä¸æ˜¯å†œæ°‘å·¥å­å¼Ÿ(some students in grade 1 are not children of migrant workers)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.2505694760820046,"Set
Operation
(14.31%)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.2528473804100228,Figure 3: The percentage and representative examples of each naÂ¨Ä±ve logical reasoning type.
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.255125284738041,"difï¬culty of NAIL, we also selected another team consisting of 20 ï¬rst-year college students. Same as
above, an example is shown to 3 students randomly and they have to give the answer independently.
We calculated the overall mean accuracy as the performance of human baseline.The evaluation of
human baseline is paid separately. Each person can receive RMBÂ¥1.5 for answering each example,
which generally cost the person 2-3 minutes."
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.25740318906605925,"Since we hire part-time people to write imitatively rather than using existing crowd-sourcing platforms,
we build our own website, where the quality and overall progress can be viewed at any time."
DATA ANALYSIS,0.25968109339407747,"3.3
DATA ANALYSIS"
DATA ANALYSIS,0.2619589977220957,"As mentioned above, NAIL-E and NAIL-I together compose NAIL. NAIL-E, NAIL-I and NAIL are
divided into training set, validation set and test set respectively. The overall statistics of NAIL are
summarized in Table 1. It is worth noting that an original example in NAIL-E and its corresponding
imitations will only be in the same data split. We analyze and manually annotate the ï¬ne-grained
types of examples in the NAIL-E and group them into 4 categories including real/fake mixture,
ordering, matching and set operation, whose percentages and representative examples are shown in
Figure 3. Each of these types of examples requires naÂ¨Ä±ve logical reasoning."
DATA ANALYSIS,0.2642369020501139,"Train(NAIL)
Dev(NAIL)
Test(NAIL)
NAIL-E
NAIL-I
NAIL-E
NAIL-I
NAIL-E
NAIL-I"
DATA ANALYSIS,0.26651480637813213,"# Example
292
5906
97
1904
99
1998
# Ave./Max. of paragraph
68.7 / 155
70.2 / 188
67.3 / 151
70.3 / 212
67.0 / 149
73.3 / 185
# Ave./Max. of query
8.1 / 74
6.9 / 81
9.3 / 71
9.3 / 134
7.9 / 92
7.6 / 71
# Ave./Max. of option
9.2 / 74
9.7 / 130
9.1 / 65
8.5 / 134
8.6 / 76
7.7 / 97"
DATA ANALYSIS,0.26879271070615035,Table 1: Data split and corresponding statistics.
DATA ANALYSIS,0.27107061503416857,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2733485193621868,"4
EXPERIMENTS"
EXPERIMENTS,0.275626423690205,"We adopt the accuracy as the evaluation metric. Simple rule-based methods and strong neural-
based methods are included as our baseline. Rule-based methods involve text matching and sliding
window. Neural-based methods include BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019).
Detailed descriptions (e.g., hyper-parameters) of the baseline models and the results on the validation
set are listed in the Appendix A. 6"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.27790432801822323,"4.1
RESULTS AND ANALYSIS
Model
Input
NAIL
Chinese NAIL"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.28018223234624146,"Dev
Test
Dev
Test"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.2824601366742597,"Random
(C, Q, A)
25.0
25.0
25.0
25.0"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.2847380410022779,"Word Matching
(Q, A)
23.1
24.6
23.3
24.9
(C, Q, A)
21.6
25.8
22.2
24.2"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.2870159453302961,"Sliding Window
(Q, A)
23.8
24.2
24.1
24.7
(C, Q, A)
21.9
22.1
21.6
22.5"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.28929384965831434,"BERT LARGE
(A)
26.8
26.7
27.2
26.7
(Q, A)
27.3
27.1
27.4
26.8
(C, Q, A)
29.0
29.4
29.3
27.7"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.29157175398633256,"RoBERTa LARGE
(A)
27.3
26.5
29.4
27.7
(Q, A)
27.8
27.9
32.6
30.5
(C, Q, A)
34.6
30.1
37.3
36.2"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.2938496583143508,"Human baseline
(C, Q, A)
70.1
71.3
75.5
76.4
Human expert
(C, Q, A)
100.0
100.0
100.0
100.0"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.296127562642369,"Table 2: Main results on NAIL (accuracy%). The column Input
means whether to input context (C), query (Q) and answer options
(A)."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.2984054669703872,"The performance of all baselines on
the NAIL is presented in Table 2. In
particular, the human baseline is 71.3
and 76.4 on the test set of NAIL and
Chinese NAIL separately, while the
human expert is 100.0 since ambigu-
ous examples are not included in the
dataset. In comparison, all of the
baselines perform much worse than
humans, demonstrating that these
methods are very limited in naÂ¨Ä±ve log-
ical reasoning. In addition, the results
are relatively similar on the English
and Chinese datasets. The rule-based
approaches achieve an accuracy of
25.8 and 24.2, which is similar to
random guess, indicating that word
correlation can not be used to help im-
prove performance. Pre-trained mod-
els have relatively good performance with about 4-10 points improvement compared to the rule-based
approaches, showing that pre-trained models have a certain degree of commonsense and logical
reasoning capabilities (Huang et al., 2019b). However, the best result by RoBERTa LARGE is 36.2 on
the testing data of Chinese NAIL, which is still far below the human performance. This indicates that
the knowledge in the pre-trained model is quite weak in logical reasoning."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.30068337129840544,"Different Input Settings
We conduct experiments with different input settings. The setting of
questions and answer options (Q, A) does not lead to signiï¬cant improvements compared to the
input setting of only answer options (A). One likely reason is that the queries usually do not provide
much information, e.g., According to this, it can be deduced that? Further adding context yields a
noticeable boost, showing that the context is highly informative."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.30296127562642367,"Transfer Learning
We conduct a set of transfer learning experiments to understand the degree of
overlap in terms of necessary knowledge for solving problems in our dataset and existing datasets."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3052391799544419,"What are the results if pre-trained model is ï¬rst trained on existing reading comprehension datasets,
and then ï¬ne-tuned on NAIL? Table 3 shows the results where LogiQA, ReClor and RACE are
adopted. 7 Overall, we observe that when LogiQA, ReClor or RACE is regarded as extra training
resource for RoBERTa, the performance on NAIL will increase (30.1â†’34.5/33.4/31.0), since models
can learn some degree of general reasoning ability when learning other comprehension tasks. While
interestingly, if models are trained only on LogiQA, ReClor, or RACE, and then zero-shot to directly
test on NAIL, the performance is poor and close to random guess. And as for RACE, the zero-shot
performance on NAIL (22.0) is even far below than that of RoBERTa (23.4). We attribute this to the"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.30751708428246016,"6We also try seq2seq-based models (i.e. T5 (Raffel et al., 2019)) and well-designed models for other related
tasks (i.e., DAGN (Huang et al., 2021) for LogiQA and ReClor, HGN (Fang et al., 2020) for HotpotQA (Yang
et al., 2018), and QDGAT (Chen et al., 2020) for DROP (Dua et al., 2019)) on our English version of NAIL.
Details and results are shown in Appendix B.
7For fair comparison, in all cross-benchmarks experiments in this paper, we removed samples in the training
set that are duplicates of those in the test set."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3097949886104784,Under review as a conference paper at ICLR 2022
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3120728929384966,(zero-shot)
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3143507972665148,"NAIL
(zero-shot)"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.31662870159453305,"*
(finetune)"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.31890660592255127,"*
NAIL
(finetune)
Training data 20 30 40 50 60 70 80"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3211845102505695,Accuracy
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3234624145785877,Zero-shot/Finetuned Results
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.32574031890660593,"LogiQA
ReClor
RACE"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.32801822323462415,"Figure 4: Transfer learning results when evaluating
on the test split of LogiQA, ReClor and RACE. âˆ…
is the zero-shot performance of RoBERTa. * is a
placeholder for these three datasets."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.33029612756264237,"Evaluate on â†’
NAIL
NAIL-E
NAIL-I
Train on â†“"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3325740318906606,"âˆ…
23.4
20.8
23.5
LogiQA
25.4
39.6
24.7
ReClor
24.7
19.8
25.0
RACE
22.0
29.2
21.6"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3348519362186788,"NAIL
30.1
37.4
29.7
LogiQAâ†’NAIL
34.5
38.5
34.3
ReClorâ†’NAIL
33.6
33.3
33.4
RACEâ†’NAIL
31.0
32.3
31.0"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.33712984054669703,"Table 3: Transfer learning results when evaluating
on the test split of NAIL, NAIL-E, NAIL-I. RACE
â†’NAIL denotes ï¬netuned on RACE ï¬rst and then
ï¬netuned on NAIL."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.33940774487471526,"fact that, naÂ¨Ä±ve logical reasoning is a typical category of reasoning which should be signiï¬cantly
distinguished from RACE. Fine-tuning on RACE makes the parameters ï¬t the RACE data, which can
lead to side effects on NAIL. We believe that completely different categories of reasoning deserve
to be explored separately since they may have different forms or strategies of reasoning, especially
when training data is insufï¬cient."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3416856492027335,"What are the results if NAIL is used as extra training resource for existing reading comprehension
tasks? Figure 4 shows answers to this question. Generally, we can draw the conclusion that using
NAIL as a pre-training step can signiï¬cantly improve the supervised-learning performance for other
tasks, such as LogiQA (35.3â†’36.9 for test), ReClor (62.6â†’64.4),8 and RACE (83.2â†’85.2). This
indicates that NAIL can bring naÂ¨Ä±ve logical reasoning ability to the model, which is a basic reasoning
ability and can be reï¬‚ected into other comprehension tasks. An interesting thing is that for zero-shot
evaluating on RACE, NAIL seems to have a side effect in the pretraining process (22.6<27.8). This is
because of a similar reason as mentioned above, that NAIL and RACE consist of completely different
categories of examples. Results of more datasets are shown in Appendix D.2."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3439635535307517,"Fine-grained Types
We further analyze the model performance with respect to different types of
naÂ¨Ä±ve logical reasoning (Figure 5). We ï¬nd that language models perform well on set operation
problems, while struggle on matching and ordering. We think that language models can provide good
representation of set object, even if models do not really reason derived from the context."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3462414578587699,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.34851936218678814,Accuracy
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.35079726651480636,"29.2
28.5
29.4"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3530751708428246,"35.4
34.7 24.1 27.9 37.8"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3553530751708428,"Trained on NAIL, Evaluated on NAIL"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.357630979498861,"BERT
RoBERTa"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.35990888382687924,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3621867881548975,Accuracy 38.2 27.6 42.1 28.6 38.2 34.5 36.8 50
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.36446469248291574,"Trained on NAIL, Evaluated on NAIL-E"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.36674259681093396,"BERT
RoBERTa"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3690205011389522,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3712984054669704,Accuracy
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3735763097949886,"28.8
28.5
28.8"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.37585421412300685,"35.7
34.6 23.6 27.5 37.1"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.37813211845102507,"Trained on NAIL, Evaluated on NAIL-I"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3804100227790433,"BERT
RoBERTa"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3826879271070615,Figure 5: Accuracy against reasoning types evaluated on different testing sets.
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.38496583143507973,"Error Analysis
We further perform detailed analysis of human-baseline errors and model errors,
while from Table 2, we can observe that human baseline on NAIL (around 70%) is much lower than
that of human expert (100%), and RoBERTa performs much worse (above 30%). We measure the
accuracy against several factors on the test set of NAIL: number of sentences in the backbone template
of the context , number of possible worlds, 9 and context length."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.38724373576309795,"See Figure 6(a), as the number of sentences in the backbone template decreases, the accuracy rate
of human baseline increases signiï¬cantly, and when reduced to 2 and below, human baseline does"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3895216400911162,"8We report the result on the validation set of ReClor, since gold answers for test are not acquired.
9The number of possible worlds is denoted as, the number of subjects Ã— the number of predicates Ã— the
number of objects."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3917995444191344,Under review as a conference paper at ICLR 2022
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3940774487471526,"1
2
3
4
5
6
7"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.39635535307517084,Number of sentences in the backbone template 0 100 200 300 400 500 600
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.39863325740318906,Number of samples in test set
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4009111617312073,"100.0
100.0
98.8 92.3 78.3 59.8 55.4"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4031890660592255,"23.8
60.3
36.9 30.1 27.4 25.2 31.2"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4054669703872437,"human baseline:wrong
human baseline:correct
RoBERTa-large:wrong
RoBERTa-large:correct"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.40774487471526194,"[0,10) #917"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.41002277904328016,"[10,20) #564"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4123006833712984,"[20,30) #357"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4145785876993166,"[30,40) #21"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4168564920273349,"[40,50) #42"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4191343963553531,"[50,60) #42"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4214123006833713,"[60,70) #103"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.42369020501138954,"[70,100)"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.42596810933940776,"#8
Number of possible worlds in the context 0 20 40 60 80 100"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.428246013667426,Accuracy
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4305239179954442,"human baseline
RoBERTa-large model"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4328018223234624,"0
50
100
150
200
250
300
Context length 0 5 10 15 20"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.43507972665148065,Number of mistakes
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.43735763097949887,"human baseline
RoBERTa-large model"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4396355353075171,"Figure 6: (a). X-axis: the number of sentences in the backbone template. The height of bars: Number of samples
in the test set grouped by X-axis. The number in bars: The accuracy against X-axis. (b). Accuracy of human
baseline and RoBERTa against the number of possible worlds in the context.(â€œ[a, b)#kâ€ in the x-axis denotes
there are k samples in the test set where a â‰¤number of possible worlds<b. (c). Number of mistakes made
by human baseline and RoBERTa model against various context length."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4419134396355353,"not make any mistakes, while performance of RoBERTa does not show any trends, even when the
number equals to 1, the model still makes mistakes in a large percentage of cases. See Figure 6(b),
as the number of possible worlds in the context increases, the accuracy of human baseline tends to
decrease signiï¬cantly. This is because the more possible worlds that need to be considered, the more
judgments humans need to perform, and humans tend to overlook certain conï¬‚icts, which leads to
wrong decisions for problems. However there is no signiï¬cant correlation between the performance
of the model and number of possible worlds. This indicates that the model does not really ï¬lter and
judge the possible worlds. See Figure 6(c), the model makes mistakes at a variety of context lengths,
while humans perform perfectly for problems with context length â‰¤73."
RELATED WORK,0.44419134396355353,"5
RELATED WORK"
RELATED WORK,0.44646924829157175,"There are abundant datasets in reading comprehension, which could facilitate the development of
the ï¬eld. MCTest (Richardson et al., 2013) is a multiple-choice reading comprehension dataset that
contains 500 ï¬ctional stories and 2k questions. Rajpurkar et al. (2016) proposes the ï¬rst large-scale
reading comprehension dataset SQuAD (100k+ questions), where the answer to each question is a
span of text from the passage. Recently more datasets requiring more complicated reasoning types are
introduced, such as multi-document (Joshi et al., 2017; Dunn et al., 2017), multi-hop reasoning (Yang
et al., 2018; Welbl et al., 2018; Talmor & Berant, 2018), numerical discrete reasoning (Dua et al.,
2019) and commonsense reasoning (Mihaylov et al., 2018; Zhang et al., 2018; Huang et al., 2019a).
However, these datasets cannot test the logical reasoning ability of the models. To ï¬ll this gap,
LogiQA (Liu et al., 2020), ReClor (Yu et al., 2020) and LR-LSAT (Wang et al., 2021) was proposed."
RELATED WORK,0.44874715261959,"LogiQA is collected from National Civil Servants Examination. ReClor and LR-LSAT are extracted
from Law School Admission Test. These datasets require logical reasoning to answer the questions.
However, from human experience, there are different forms of reasoning strategies in answering
different logical questions. We believe that completely different categories of logical reasoning
deserve to be explored separately (Rudinger et al., 2020). Different from these datasets, we inductively
deï¬ne a typical class of logical reasoning, named naÂ¨Ä±ve logical reasoning, and then create a new
benchmark targeting the task, named NAIL. Compared with LogiQA, Reclor and LR-LSAT, NAIL
focus on the more ï¬ne-grained logical reasoning type (naÂ¨Ä±ve logical reasoning)."
RELATED WORK,0.4510250569476082,"There have been many datasets extracted from human examinations, such as LogiQA and ReClor
mentioned above. Besides, RACE dataset (Lai et al., 2017) is collected from English exams for middle
and high school Chinese students. ARC dataset (Clark et al., 2018) consists of 7,787 science exam
questions drawn from a variety of sources.DREAM (Sun et al., 2019) is dialogue-based multiple-
choice reading comprehension dataset collected from English as a Foreign Language examinations
which contains 10,197 questions for 6,444 dialogues."
CONCLUSION,0.4533029612756264,"6
CONCLUSION"
CONCLUSION,0.45558086560364464,"We introduce a more ï¬ne-grained logical reasoning, naÂ¨Ä±ve logical reasoning, and We propose a new
large-scale benchmark, NAIL, aiming to help models learn and evaluate naÂ¨Ä±ve logical reasoning
capability. NAIL is sourced from standardized exams and human imitation. Preliminary results show
that there is still a long way to go to equip deep models with true logical reasoning capability."
CONCLUSION,0.45785876993166286,Under review as a conference paper at ICLR 2022
REFERENCES,0.4601366742596811,REFERENCES
REFERENCES,0.4624145785876993,"Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh
Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based
formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), pp. 2357â€“2367, 2019."
REFERENCES,0.4646924829157175,"Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan
Qi, and Wei Chu. Question directed graph attention network for numerical reasoning over text.
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 6759â€“6768, 2020."
REFERENCES,0.46697038724373574,"Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.
arXiv preprint arXiv:1803.05457, 2018."
REFERENCES,0.46924829157175396,"Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu.
Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101, 2019."
REFERENCES,0.4715261958997722,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171â€“4186, 2019."
REFERENCES,0.47380410022779046,"Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.
Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In
Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.
2368â€“2378, 2019."
REFERENCES,0.4760820045558087,"Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho.
Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint
arXiv:1704.05179, 2017."
REFERENCES,0.4783599088838269,"Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, and Jingjing Liu. Hierarchical graph
network for multi-hop question answering. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 8823â€“8838, 2020."
REFERENCES,0.4806378132118451,"Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos QA: Machine reading
comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), pp. 2391â€“2401, Hong Kong, China, November
2019a. Association for Computational Linguistics. doi: 10.18653/v1/D19-1243. URL https:
//www.aclweb.org/anthology/D19-1243."
REFERENCES,0.48291571753986334,"Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos qa: Machine reading
comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), pp. 2391â€“2401, 2019b."
REFERENCES,0.48519362186788156,"Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, and Xiaodan Liang. Dagn: Discourse-aware graph
network for logical reasoning. arXiv preprint arXiv:2103.14349, 2021."
REFERENCES,0.4874715261958998,"Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601â€“
1611, 2017."
REFERENCES,0.489749430523918,"Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding
comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pp. 785â€“794, Copenhagen, Denmark, September
2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https:
//www.aclweb.org/anthology/D17-1082."
REFERENCES,0.4920273348519362,Under review as a conference paper at ICLR 2022
REFERENCES,0.49430523917995445,"Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A challenge
dataset for machine reading comprehension with logical reasoning, 2020."
REFERENCES,0.49658314350797267,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.4988610478359909,"John McCarthy et al. Programs with common sense. RLE and MIT computation center, 1960."
REFERENCES,0.5011389521640092,"Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,
2018."
REFERENCES,0.5034168564920274,"Allen Newell and Herbert Simon. The logic theory machineâ€“a complex information processing
system. IRE Transactions on information theory, 2(3):61â€“79, 1956."
REFERENCES,0.5056947608200456,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniï¬ed text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019."
REFERENCES,0.5079726651480638,"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pp. 2383â€“2392, Austin, Texas, November 2016. Association for
Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/
anthology/D16-1264."
REFERENCES,0.510250569476082,"Matthew Richardson, Christopher JC Burges, and Erin Renshaw. Mctest: A challenge dataset for the
open-domain machine comprehension of text. In Proceedings of the 2013 conference on empirical
methods in natural language processing, pp. 193â€“203, 2013."
REFERENCES,0.5125284738041003,"Rachel Rudinger, Vered Shwartz, Jena D Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan
Le Bras, Noah A Smith, and Yejin Choi. Thinking like a skeptic: Defeasible inference in natural
language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: Findings, pp. 4661â€“4675, 2020."
REFERENCES,0.5148063781321185,"Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. Dream: A challenge data
set and models for dialogue-based reading comprehension. Transactions of the Association for
Computational Linguistics, 7:217â€“231, 2019."
REFERENCES,0.5170842824601367,"Alon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions.
In Proceedings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 641â€“651,
2018."
REFERENCES,0.5193621867881549,"Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, and
Nan Duan. From lsat: The progress and challenges of complex reasoning. arXiv preprint
arXiv:2108.00648, 2021."
REFERENCES,0.5216400911161732,"Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading
comprehension across documents. Transactions of the Association for Computational Linguistics,
6:287â€“302, 2018."
REFERENCES,0.5239179954441914,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, RÂ´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von
Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language
processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-
cessing: System Demonstrations, pp. 38â€“45, Online, October 2020. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6."
REFERENCES,0.5261958997722096,"Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,
and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question
answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pp. 2369â€“2380, 2018."
REFERENCES,0.5284738041002278,Under review as a conference paper at ICLR 2022
REFERENCES,0.530751708428246,"Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: A reading comprehension dataset
requiring logical reasoning. In International Conference on Learning Representations (ICLR),
April 2020."
REFERENCES,0.5330296127562643,"Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.
Record: Bridging the gap between human and machine commonsense reading comprehension.
arXiv preprint arXiv:1810.12885, 2018."
REFERENCES,0.5353075170842825,Under review as a conference paper at ICLR 2022
REFERENCES,0.5375854214123007,"A
IMPLEMENTATION DETAILS OF BASELINES"
REFERENCES,0.5398633257403189,"Rule-based Methods
Following LogiQA (Liu et al., 2020), we adopt two simple rule-based
methods based on text matching. Speciï¬cally, word matching is to measure the degree of unigram
overlap between the candidate answer and the given paragraph-query pair; sliding window takes into
account the n-gram when calculating the matching score."
REFERENCES,0.5421412300683371,"Neural-based Methods
Pre-trained neural models, such as BERT (Devlin et al., 2019), RoBERTa
(Liu et al., 2019), have achieved impressive performance on reading comprehension. The input to
the pre-trained model is the concatenation of the paragraph, the query and the candidate answer,
separated by [SEP] tokens, denoted as: [CLS], paragraph, [SEP], query, [SEP], answer. After
the encoding of the pre-trained model, the representation of [CLS] token followed by an multiple
layer perceptron (MLP) is used for scoring."
REFERENCES,0.5444191343963554,"We re-implement the rule-based methods following LogiQA (Liu et al., 2020). For pre-trained
models, we modify the code of Transformers of HuggingFace (Wolf et al., 2020) to implement them
on NAIL. We take the off-the-shelf model BERT and RoBERTa for NAIL, and Chinese BERT and
Chinese RoBERTa (Cui et al., 2019) for Chinese NAIL. All hyper-parameters are selected by the
model performance on the development sets."
REFERENCES,0.5466970387243736,"B
MORE STRONG BASELINES"
REFERENCES,0.5489749430523918,"We also try seq2seq-based pre-trained models (i.e. T5 (Raffel et al., 2019)) and well-designed models
for other related tasks (i.e. DAGN (Huang et al., 2021) for LogiQA and Reclor, HGN (Fang et al.,
2020) for HotpotQA (Yang et al., 2018), and QDGAT (Chen et al., 2020) for DROP (Dua et al.,
2019)) on our English version of NAIL."
REFERENCES,0.55125284738041,"A problem is regarded as four samples to T5 during training. The input of a sample is: con-
text+query+one of the four choices. And we let the model generate the label for the sample: if the
choice is the correct answer, then generate â€œTrueâ€, otherwise generate â€œFalseâ€. And during inference,
for a problem, we choose the option with the highest probability of generating â€œTrueâ€ among the
four options as the prediction. We set input max len = 512 and ï¬netuned the off-the-shelf T5-base
model on the training set of NAIL, results are shown in Table 4."
REFERENCES,0.5535307517084282,"LogiQA and ReClor are two logical benchmarks introduced in the text of the paper to test modelsâ€™
various reasoning abilities. HotpotQA is a multiple-choice QA dataset featuring natural, multi-hop
questions, with strong supervision for supporting facts to test modelsâ€™ multi-hop factual reasoning
ability. DROP is a 96k-question benchmark to test modelsâ€™ numerical discrete reasoning ability over
paragraphs (such as addition, counting, or sorting)."
REFERENCES,0.5558086560364465,"We choose three state-of-the-art and open-sourced models designed for the above benchmarks: DAGN
for LogiQA and Reclor, HGN for HotpotQA, and QDGAT for DROP respectively.We reproduced
these three models with the code released by authors10 and NAIL as training data. Results on NAIL
are shown as Table 4."
REFERENCES,0.5580865603644647,"Model
Encoder
NAIL"
REFERENCES,0.5603644646924829,"Dev
Test"
REFERENCES,0.5626423690205011,"RoBERTa LARGE
RoBERTa LARGE
34.6
30.1
RoBERTa BASE
RoBERTa BASE
27.6
26.4"
REFERENCES,0.5649202733485194,"T5 BASE
T5 BASE
27.1
26.3
DAGN
RoBERTa LARGE
36.0
32.3
HGN
RoBERTa LARGE
30.1
28.7
QDGAT
RoBERTa LARGE
28.3
28.7"
REFERENCES,0.5671981776765376,Table 4: Results on NAIL (accuracy%).
REFERENCES,0.5694760820045558,"10DAGN:
https://github.com/Eleanor-H/DAGN,
QDGAT:
https://github.com/
emnlp2020qdgat/QDGAT, HGN: https://github.com/yuwfan/HGN"
REFERENCES,0.571753986332574,Under review as a conference paper at ICLR 2022
REFERENCES,0.5740318906605922,"We observe that DAGN achieves the best results on NAIL, since ReClor and LogiQA are much
closer to NAIL, the well-designed discourse-aware graph network DAGN is more applicable to
NAIL. However there is still very much space for modelsâ€™ improvement compared to humans. HGN
is a hierarchical graph network for multi-hop question answering, which is designed to aggregate
heterogeneous information, and QDGAT is a question directed graph attention network, which is
dedicated to identifying numbers and the computation between them, which are both of minimal
help to NAIL. T5 is a very powerful generative model, but it seems that T5 lacks the naÂ¨Ä±ve reasoning
capability as well. Due to computational resource limitation, we did not try a T5 model with larger
number of parameters."
REFERENCES,0.5763097949886105,"C
THE EFFECT OF NAIL, NAIL-E AND NAIL-I"
REFERENCES,0.5785876993166287,"To investigate the effect of NAIL, NAIL-E and NAIL-I, we train a RoBERTa LARGE model on NAIL,
NAIL-E and NAIL-I separately and evaluate on the corresponding set of all tests (Table 5). When
trained on NAIL-I and evaluated on NAIL-E, the model achieves the best accuracy 38.5%, showing
that human imitations can signiï¬cantly help models learn logic from natural text. When trained on
NAIL-E and evaluated on NAIL-E, the model achieves the worst accuracy 22.9%. The possible reason
is that the amount of data of NAIL-E is too small to train a deep model. Corresponding results on the
validation set are shown in Table 6."
REFERENCES,0.5808656036446469,"Trained on â†’
NAIL
NAIL-E
NAIL-I
Evaluated on â†“"
REFERENCES,0.5831435079726651,"NAIL
30.1
26.4
33.9
NAIL-E
37.4
22.9
38.5
NAIL-I
29.7
26.6
33.7"
REFERENCES,0.5854214123006833,Table 5: Performance of RoBERTa on different training sets and test sets.
REFERENCES,0.5876993166287016,"Trained on â†’
NAIL
NAIL-E
NAIL-I
Evaluated on â†“"
REFERENCES,0.5899772209567198,"NAIL
34.6
24.9
32.0
NAIL-E
43.3
37.4
33.0
NAIL-I
34.2
24.3
31.9"
REFERENCES,0.592255125284738,Table 6: Performance of RoBERTa on different training sets and validation sets.
REFERENCES,0.5945330296127562,"D
TRANSFER LEARNING"
REFERENCES,0.5968109339407744,"D.1
OTHER DATASETS AS EXTRA TRAINING RESOURCE"
REFERENCES,0.5990888382687927,"What are the results if pre-trained model is ï¬rst trained on existing reading comprehension datasets,
and then ï¬ne-tuned on NAIL? Table 3 shows the results on the test set of NAIL,NAIL-E, NAIL-I
where LogiQA, ReClor11 and RACE are adopted. And here in Table 7 we give the results on the
validation splits of NAIL,NAIL-E, NAIL-I."
REFERENCES,0.6013667425968109,"D.2
NAIL AS EXTRA TRAINING RESOURCE"
REFERENCES,0.6036446469248291,"What are the results if NAIL is used as extra training resource for existing reading comprehension
tasks? Using NAIL as extra training resource, we conduct plenty of experiments on existing
benchmarks. Besides LogiQA, ReClor and RACE (see in Figure 4), more results are shown in"
REFERENCES,0.6059225512528473,"11For fair comparison, in all cross-benchmarks experiments in this paper, we removed samples in the training
set that are duplicates of those in the test set. For example, when using ReClor or LogiQA as extra training
resource when testing on NAIL, we remove problems in the training set of ReClor or LogiQA if they appear in
the test set of NAIL."
REFERENCES,0.6082004555808656,Under review as a conference paper at ICLR 2022
REFERENCES,0.6104783599088838,"Evaluate on â†’
NAIL
NAIL-E
NAIL-I
Train on â†“"
REFERENCES,0.6127562642369021,"âˆ…
23.6
24.2
23.6
LogiQA
33.0
45.1
32.4
ReClor
28.4
25.3
28.5
RACE
26.4
27.5
26.4"
REFERENCES,0.6150341685649203,"NAIL
34.6
43.3
34.2
LogiQAâ†’NAIL
37.9
47.3
37.5
ReClorâ†’NAIL
35.2
39.6
35.9
RACEâ†’NAIL
35.4
36.3
35.4"
REFERENCES,0.6173120728929385,"Table 7: Transfer learning results when evaluating on the validation split of NAIL, NAIL-E, NAIL-I
(accuracy%)."
REFERENCES,0.6195899772209568,"Table 8, 9. We adopted MathQA (Amini et al., 2019), and HotpotQA (Yang et al., 2018). For
MathQA, we include a prediction head for multiple choice based on the RoBERTa-large model, and
for HotpotQA,12 we include a prediction head for question answering. For MathQA, we concat the
problem and annotated formula as input."
REFERENCES,0.621867881548975,"Evaluate on â†’
MathQA
Train on â†“
Test"
REFERENCES,0.6241457858769932,"MathQA
39.8"
REFERENCES,0.6264236902050114,"NAIL â†’MathQA
41.2"
REFERENCES,0.6287015945330297,"Table
8:
Evaluating
on
MathQA
after
RoBERTa-large pretrained on MathQA/NAIL
training set."
REFERENCES,0.6309794988610479,"Evaluate on â†’
HotpotQA
Train on â†“
Ans F1"
REFERENCES,0.6332574031890661,"HotpotQA
69.8"
REFERENCES,0.6355353075170843,"NAIL â†’HotpotQA
72.6"
REFERENCES,0.6378132118451025,"Table 9:
Evaluating on HotpotQA after
RoBERTa-large pretrained on HotpotQA/NAIL
training set."
REFERENCES,0.6400911161731208,"Experimental results show that, if we use NAIL as extra training resource, the supervised-learning
results on all of these six datasets: LogiQA, ReClor, RACE, MathQA, HotpotQA will improve.
This indicates that equipping models with naÂ¨Ä±ve logical reasoning ability can help solve math
word problems (MathQA), improve multi-hop factual reasoning skills (HotpotQA), solve various
logical reasoning problems (LogiQA and ReClor), and enhance general understanding and reading
comprehension capability (RACE). In the future, we will test on more known datasets, to verify that
naÂ¨Ä±ve logical reasoning is a basic capability and is helpful for other tasks."
REFERENCES,0.642369020501139,"E
ANALYSIS OF FINE-GRAINED TYPES"
REFERENCES,0.6446469248291572,"In Section 4, we analyze the model performance with respect to different types of naÂ¨Ä±ve logical
reasoning (Figure 5). Figure 8 shows the accuracy against ï¬ne-grained reasoning types on the test set
of NAIL, NAIL-E, NAIL-I when training on NAIL, NAIL-E, NAIL-I respectively. And Figure 7 shows
the corresponding results on the validation set."
REFERENCES,0.6469248291571754,"From above ï¬gures we can ï¬nd that language models perform well on set operation problems, while
struggle on matching and ordering. We think that language models can provide good representation
of set object, even if models do not really reason derived from the context."
REFERENCES,0.6492027334851936,"F
DISCUSSIONS ABOUT FUTURE DIRECTIONS"
REFERENCES,0.6514806378132119,We have some ideas for future directions for models to solve NAIL.
REFERENCES,0.6537585421412301,"The ï¬rst point is to identify deterministic information or information with low uncertainty. Generally,
we need to detect the atomic information that can be used directly without reasoning, which is
the key to solving the problem. Different conditions should not be considered with equal priority,"
REFERENCES,0.6560364464692483,"12Due to computational resource limitations, we only trained 3 epochs for experiments involved HotpotQA."
REFERENCES,0.6583143507972665,Under review as a conference paper at ICLR 2022
REFERENCES,0.6605922551252847,Real/Fake
REFERENCES,0.662870159453303,Ordering
REFERENCES,0.6651480637813212,Matching
REFERENCES,0.6674259681093394,Set operation 0 10 20 30 40 50 60
REFERENCES,0.6697038724373576,Accuracy
REFERENCES,0.6719817767653758,"26.4
25.9 22.4"
REFERENCES,0.6742596810933941,"36.3
38.5 34"
REFERENCES,0.6765375854214123,"37.8
37.3"
REFERENCES,0.6788154897494305,"Trained on NAIL, Evaluated on NAIL"
REFERENCES,0.6810933940774487,"BERT
RoBERTa"
REFERENCES,0.683371298405467,Real/Fake
REFERENCES,0.6856492027334852,Ordering
REFERENCES,0.6879271070615034,Matching
REFERENCES,0.6902050113895216,Set operation 0 10 20 30 40 50 60
REFERENCES,0.6924829157175398,Accuracy 30.4 41.2 20 36.8 43.5 52.9 50 36.8
REFERENCES,0.6947608200455581,"Trained on NAIL, Evaluated on NAIL-E"
REFERENCES,0.6970387243735763,"BERT
RoBERTa"
REFERENCES,0.6993166287015945,Real/Fake
REFERENCES,0.7015945330296127,Ordering
REFERENCES,0.7038724373576309,Matching
REFERENCES,0.7061503416856492,Set operation 0 10 20 30 40 50 60
REFERENCES,0.7084282460136674,Accuracy
REFERENCES,0.7107061503416856,"26.1
25.1 22.5"
REFERENCES,0.7129840546697038,"36.3
38.2 33"
REFERENCES,0.715261958997722,"37.3
37.4"
REFERENCES,0.7175398633257403,"Trained on NAIL, Evaluated on NAIL-I"
REFERENCES,0.7198177676537585,"BERT
RoBERTa"
REFERENCES,0.7220956719817767,Real/Fake
REFERENCES,0.724373576309795,Ordering
REFERENCES,0.7266514806378133,Matching
REFERENCES,0.7289293849658315,Set operation 0 10 20 30 40 50 60
REFERENCES,0.7312072892938497,Accuracy
REFERENCES,0.7334851936218679,"24.8
24.4
23.6 28.6"
REFERENCES,0.7357630979498861,"23.7
24.4
23.3 27.1"
REFERENCES,0.7380410022779044,"Trained on NAIL-E, Evaluated on NAIL"
REFERENCES,0.7403189066059226,"BERT
RoBERTa"
REFERENCES,0.7425968109339408,Real/Fake
REFERENCES,0.744874715261959,Ordering
REFERENCES,0.7471526195899773,Matching
REFERENCES,0.7494305239179955,Set operation 0 10 20 30 40 50 60
REFERENCES,0.7517084282460137,Accuracy 17.4 23.5
REFERENCES,0.7539863325740319,"30
31.6 52.2 35.3 20 26.3"
REFERENCES,0.7562642369020501,"Trained on NAIL-E, Evaluated on NAIL-E"
REFERENCES,0.7585421412300684,"BERT
RoBERTa"
REFERENCES,0.7608200455580866,Real/Fake
REFERENCES,0.7630979498861048,Ordering
REFERENCES,0.765375854214123,Matching
REFERENCES,0.7676537585421412,Set operation 0 10 20 30 40 50 60
REFERENCES,0.7699316628701595,Accuracy
REFERENCES,0.7722095671981777,"25.2
24.4
23.3"
REFERENCES,0.7744874715261959,"28.4
30.6 24.9 18.6 22.4"
REFERENCES,0.7767653758542141,"Trained on NAIL-E, Evaluated on NAIL-I"
REFERENCES,0.7790432801822323,"BERT
RoBERTa"
REFERENCES,0.7813211845102506,Real/Fake
REFERENCES,0.7835990888382688,Ordering
REFERENCES,0.785876993166287,Matching
REFERENCES,0.7881548974943052,Set operation 0 10 20 30 40 50 60
REFERENCES,0.7904328018223234,Accuracy 36
REFERENCES,0.7927107061503417,"26.7
28.1 42.4 35.8 32 36.2 42.4"
REFERENCES,0.7949886104783599,"Trained on NAIL-I, Evaluated on NAIL"
REFERENCES,0.7972665148063781,"BERT
RoBERTa"
REFERENCES,0.7995444191343963,Real/Fake
REFERENCES,0.8018223234624146,Ordering
REFERENCES,0.8041002277904328,Matching
REFERENCES,0.806378132118451,Set operation 0 10 20 30 40 50 60
REFERENCES,0.8086560364464692,Accuracy 34.8 38.2 30 47.4 39.1 29.4 40 36.8
REFERENCES,0.8109339407744874,"Trained on NAIL-I, Evaluated on NAIL-E"
REFERENCES,0.8132118451025057,"BERT
RoBERTa"
REFERENCES,0.8154897494305239,Real/Fake
REFERENCES,0.8177676537585421,Ordering
REFERENCES,0.8200455580865603,Matching
REFERENCES,0.8223234624145785,Set operation 0 10 20 30 40 50 60
REFERENCES,0.8246013667425968,Accuracy 36.1
REFERENCES,0.826879271070615,"26.1
28 42.1 35.6 32.2 36.02 42.63"
REFERENCES,0.8291571753986332,"Trained on NAIL-I, Evaluated on NAIL-I"
REFERENCES,0.8314350797266514,"BERT
RoBERTa"
REFERENCES,0.8337129840546698,Figure 7: Accuracy against the reasoning types based on different training set and development set.
REFERENCES,0.835990888382688,"i.e., the more certain a condition we start from, the fewer possible worlds that the condition yields.
Speciï¬cally, for real/fake mixture problems, we also need to promptly identify two atomic statements
that yield a contradictory, that is, the two statements cannot be both true (there must be one false)
or both false (there must be one true), which is often the breakthrough in solving real/fake mixture
problems."
REFERENCES,0.8382687927107062,"The second is to detect informative subjects/objects. Generally, the more frequently a subject/object
is mentioned in context, the more relevant information it carries. Tables and bi(/tri)partite diagrams
can help to clarify the correspondence between given subjects and objects. For example, in the case
of Figure 1, tables play an effective role in solving the problems. And as for the third case in Figure
3, which is categorized as a matching problem, a tri-partite diagram could help, that is, three disjoint
and independent sets U, V and W represent {A,B,C}, {Beijing, Nanjing, Chengdu}, {a doctor, an
actor, and a teacher} respectively, an edge connecting a vertex in one set to one in another set denotes
the â€œis aâ€ predicate."
REFERENCES,0.8405466970387244,"The third direction is allowing models to better utilize elimination. On the one hand, models can
perform forward elimination, i.e., according to the context, each time we draw a deï¬nite conclusion,
we can retain options that are logically consistent and exclude those that do not ï¬t the derived
conclusion. On the other hand, models can also perform backward elimination. For example, when
sometimes it is not easy to draw exact inferences directly from the context, then we can substitute the
options into the context. If substituting an option creates a contradiction within the context, then the
option should be excluded. Forward and backward elimination facilitate the model in arriving at the
correct answer."
REFERENCES,0.8428246013667426,Under review as a conference paper at ICLR 2022
REFERENCES,0.8451025056947609,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
REFERENCES,0.8473804100227791,Accuracy
REFERENCES,0.8496583143507973,"29.2
28.5
29.4"
REFERENCES,0.8519362186788155,"35.4
34.7 24.1 27.9 37.8"
REFERENCES,0.8542141230068337,"Trained on NAIL, Evaluated on NAIL"
REFERENCES,0.856492027334852,"BERT
RoBERTa"
REFERENCES,0.8587699316628702,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
REFERENCES,0.8610478359908884,Accuracy 38.2 27.6 42.1 28.6 38.2 34.5 36.8 50
REFERENCES,0.8633257403189066,"Trained on NAIL, Evaluated on NAIL-E"
REFERENCES,0.8656036446469249,"BERT
RoBERTa"
REFERENCES,0.8678815489749431,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
REFERENCES,0.8701594533029613,Accuracy
REFERENCES,0.8724373576309795,"28.8
28.5
28.8"
REFERENCES,0.8747152619589977,"35.7
34.6 23.6 27.5 37.1"
REFERENCES,0.876993166287016,"Trained on NAIL, Evaluated on NAIL-I"
REFERENCES,0.8792710706150342,"BERT
RoBERTa"
REFERENCES,0.8815489749430524,Real/Fake
REFERENCES,0.8838268792710706,Ordering
REFERENCES,0.8861047835990888,Matching
REFERENCES,0.8883826879271071,Set operation 0 10 20 30 40 50 60
REFERENCES,0.8906605922551253,Accuracy
REFERENCES,0.8929384965831435,"26.9
26 22.4 29.4 23.9"
REFERENCES,0.8952164009111617,"26
26.5
28.2"
REFERENCES,0.89749430523918,"Trained on NAIL-E, Evaluated on NAIL"
REFERENCES,0.8997722095671982,"BERT
RoBERTa"
REFERENCES,0.9020501138952164,Real/Fake
REFERENCES,0.9043280182232346,Ordering
REFERENCES,0.9066059225512528,Matching
REFERENCES,0.908883826879271,Set operation 0 10 20 30 40 50 60
REFERENCES,0.9111617312072893,Accuracy 14.7 17.2 21.1
REFERENCES,0.9134396355353075,"35.7
32.4 20.7"
REFERENCES,0.9157175398633257,"15.8
14.3"
REFERENCES,0.9179954441913439,"Trained on NAIL-E, Evaluated on NAIL-E"
REFERENCES,0.9202733485193622,"BERT
RoBERTa"
REFERENCES,0.9225512528473804,Real/Fake
REFERENCES,0.9248291571753986,Ordering
REFERENCES,0.9271070615034168,Matching
REFERENCES,0.929384965831435,Set operation 0 10 20 30 40 50 60
REFERENCES,0.9316628701594533,Accuracy
REFERENCES,0.9339407744874715,"27.5
26.4 22.5"
REFERENCES,0.9362186788154897,"28.9
29.3 24.7 28 21.4"
REFERENCES,0.9384965831435079,"Trained on NAIL-E, Evaluated on NAIL-I"
REFERENCES,0.9407744874715261,"BERT
RoBERTa"
REFERENCES,0.9430523917995444,Real/Fake
REFERENCES,0.9453302961275627,Ordering
REFERENCES,0.9476082004555809,Matching
REFERENCES,0.9498861047835991,Set operation 0 10 20 30 40 50 60
REFERENCES,0.9521640091116174,Accuracy 34.5 29.1 20.5
REFERENCES,0.9544419134396356,"35.7
37.8 31.6 24.8 41.8"
REFERENCES,0.9567198177676538,"Trained on NAIL-I, Evaluated on NAIL"
REFERENCES,0.958997722095672,"BERT
RoBERTa"
REFERENCES,0.9612756264236902,Real/Fake
REFERENCES,0.9635535307517085,Ordering
REFERENCES,0.9658314350797267,Matching
REFERENCES,0.9681093394077449,Set operation 0 10 20 30 40 50 60
REFERENCES,0.9703872437357631,Accuracy 38.2 13.8 42.1
REFERENCES,0.9726651480637813,"35.7
32.4 34.5 21.1"
REFERENCES,0.9749430523917996,"57.1
Trained on NAIL-I, Evaluated on NAIL-E"
REFERENCES,0.9772209567198178,"BERT
RoBERTa"
REFERENCES,0.979498861047836,Real/Fake
REFERENCES,0.9817767653758542,Ordering
REFERENCES,0.9840546697038725,Matching
REFERENCES,0.9863325740318907,Set operation 0 10 20 30 40 50 60
REFERENCES,0.9886104783599089,Accuracy 34.3 29.9 19.5
REFERENCES,0.9908883826879271,"35.7
38.1 31.5 25 41.1"
REFERENCES,0.9931662870159453,"Trained on NAIL-I, Evaluated on NAIL-I"
REFERENCES,0.9954441913439636,"BERT
RoBERTa"
REFERENCES,0.9977220956719818,Figure 8: Accuracy against the reasoning types based on different training set and testing set.
