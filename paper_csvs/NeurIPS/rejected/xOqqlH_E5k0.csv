Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019801980198019802,"Snapshot compressive hyperspectral imaging requires reconstructing a hyperspec-
1"
ABSTRACT,0.0039603960396039604,"tral image from its snapshot measurement. This paper proposes an augmented deep
2"
ABSTRACT,0.005940594059405941,"unrolling neural network for solving such a challenging reconstruction problem.
3"
ABSTRACT,0.007920792079207921,"The proposed network is based on the unrolling of a proximal gradient descent
4"
ABSTRACT,0.009900990099009901,"algorithm with two innovative modules for gradient update and proximal mapping.
5"
ABSTRACT,0.011881188118811881,"The gradient update is modeled by a memory-assistant descent module motivated
6"
ABSTRACT,0.013861386138613862,"by the momentum-based acceleration heuristics. The proximal mapping is mod-
7"
ABSTRACT,0.015841584158415842,"eled by a sub-network with a cross-stage self-attention which effectively exploits
8"
ABSTRACT,0.01782178217821782,"inherent self-similarities of a hyperspectral image along the spectral axis, as well
9"
ABSTRACT,0.019801980198019802,"as enhancing the feature Ô¨Çow through the network. Moreover, a spectral geometry
10"
ABSTRACT,0.02178217821782178,"consistency loss is proposed to encourage the model to concentrate more on the
11"
ABSTRACT,0.023762376237623763,"geometric layer of spectral curves for better reconstruction. Extensive experiments
12"
ABSTRACT,0.02574257425742574,"on several datasets showed the performance advantage of our approach over the
13"
ABSTRACT,0.027722772277227723,"latest methods.
14"
INTRODUCTION,0.0297029702970297,"1
Introduction
15"
INTRODUCTION,0.031683168316831684,"Hyperspectral imaging captures a hyperspectral image (HSI) which is a 3D cube of intensities
16"
INTRODUCTION,0.033663366336633666,"that represents the integrals the radiance of a real scene across a wide range of spectral bands.
17"
INTRODUCTION,0.03564356435643564,"As an HSI provides rich spectral characteristics of objects of a scene, hyperspectral imaging has
18"
INTRODUCTION,0.03762376237623762,"found wide applications in many areas, e.g., agriculture, industry, and science. Snapshot compres-
19"
INTRODUCTION,0.039603960396039604,"sive spectral imaging [1], often known as coded aperture snapshot spectral imaging (CASSI), is
20"
INTRODUCTION,0.041584158415841586,"a compressed-sensing-based technique for rapid and efÔ¨Åcient acquisition of HSIs. In contrast to
21"
INTRODUCTION,0.04356435643564356,"traditional hyperspectral imaging techniques which use a sensor array for measuring the object at
22"
INTRODUCTION,0.04554455445544554,"many spectral bands, the CASSI only collects a single coded 2D snapshot, which measures the
23"
INTRODUCTION,0.047524752475247525,"object modulated by a physical mask and a disperser at the mixture of different wavelengths. A
24"
INTRODUCTION,0.04950495049504951,"reconstruction algorithm is then called to reconstruct the 3D HSI from its 2D compressive snapshot.
25"
INTRODUCTION,0.05148514851485148,"Let X ‚ààRM√óN√óŒõ denote an HSI with spatial indices m, n and spectral index Œª. In general, the
26"
INTRODUCTION,0.053465346534653464,"snapshot from a CASSI device can be expressed as the following [1]:
27"
INTRODUCTION,0.055445544554455446,"Y (m, n) = Œõ
X"
INTRODUCTION,0.05742574257425743,"Œª=1
œÅ(Œª)œï(m ‚àíJ(Œª), n)X(m ‚àíJ(Œª), n, Œª) + N(m, n),
(1)"
INTRODUCTION,0.0594059405940594,"where œÅ(¬∑) is the spectral response of the camera, œï(¬∑, ¬∑) the coded aperture pattern, J(¬∑) the dispersive
28"
INTRODUCTION,0.061386138613861385,"function, and N the measurement noise. For convenience, we re-express it in a matrix-vector form:
29"
INTRODUCTION,0.06336633663366337,"y = Œ¶x + n,
(2)
where Œ¶ denotes the measurement matrix determined by œÅ, œà, and x, y, n are the vectorized form of
30"
INTRODUCTION,0.06534653465346535,"X, Y , N, respectively. As Eq. (2) is an under-determined linear system with measurement noise,
31"
INTRODUCTION,0.06732673267326733,"HSI reconstruction needs to solve an ill-posed inverse problem,
32"
INTRODUCTION,0.06930693069306931,"In recent years, deep learning has become a prominent approach for developing powerful solutions to
33"
INTRODUCTION,0.07128712871287128,"HSI reconstruction; see e.g. [2, 36, 3‚Äì6, 3, 7‚Äì10].Most of them models the inverse mapping from
34"
INTRODUCTION,0.07326732673267326,"the 2D snapshot to its corresponding HSI by a neural network (NN) trained over a dataset. Among
35"
INTRODUCTION,0.07524752475247524,"existing designs of NN architecture, deep unrolling is the most popular one for HSI reconstruction, as
36"
INTRODUCTION,0.07722772277227723,"it allows the inclusion of the physics of imaging. A typical deep unrolling network (DUN) unfolds
37"
INTRODUCTION,0.07920792079207921,"an iterative scheme for solving some regularized variational model of (2), where the regularization-
38"
INTRODUCTION,0.08118811881188119,"related parts are replaced by learnable NN modules. It can also be interpreted as a concatenation of
39"
INTRODUCTION,0.08316831683168317,"the steps that alternates between an updating step and a reÔ¨Ånement step: x(0)
Update
‚àí‚àí‚àí‚àí‚Üíz(0)
Refine
‚àí‚àí‚àí‚àí‚àí‚Üí
40"
INTRODUCTION,0.08514851485148515,"x(1)
Update
‚àí‚àí‚àí‚àí‚Üíz(1)
Refine
‚àí‚àí‚àí‚àí‚àí‚Üíx(2) ‚àí‚Üí¬∑ ¬∑ ¬∑ . Despite extensive studies on HSI reconstruction, the
41"
INTRODUCTION,0.08712871287128712,"practical need remains for the methods with better reconstruction accuracy.
42"
INTRODUCTION,0.0891089108910891,"The paper aims at developing a DUN for HSI reconstruction that brings noticeable performance
43"
INTRODUCTION,0.09108910891089109,"improvement over existing deep NNs. The proposed DUN is based on the proximal gradient descent
44"
INTRODUCTION,0.09306930693069307,"(PGD) algorithm [11, 12], one often seen iterative scheme for solving inverse problems in imaging.
45"
INTRODUCTION,0.09504950495049505,"The PGD algorithm alternatively iterates between the following two steps:
46"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.09702970297029703,"1. A gradient descent step for updating the estimate of the image
47"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.09900990099009901,"2. A proximal mapping for reÔ¨Åning the estimate via Ô¨Åtting some regularization term.
48"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.100990099009901,"In comparison to existing DUNs for HSI reconstructions, there are three innovations in the design
49"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.10297029702970296,"and training of the proposed one:
50"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.10495049504950495,"1. Updating step: Modeling the gradient descent step using an NN block with a momentum-
51"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.10693069306930693,"motivated memory-assistant module which is implemented by long short-term memory.
52"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.10891089108910891,"2. ReÔ¨Ånement step: Modeling the proximal mapping by a sub-NN with a across-stage self- attention
53"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.11089108910891089,"module, for exploiting speciÔ¨Åc characteristics of HSIs and efÔ¨Åcient feature Ô¨Çow.
54"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.11287128712871287,"3. Training loss: A spectral geometry consistency loss is proposed for regularizing the reconstruc-
55"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.11485148514851486,"tion with better accuracy.
56"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.11683168316831684,"Learnable memory-assistant module In most existing DUNs for HSI reconstruction, the updating
57"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.1188118811881188,"step usually is some pre-deÔ¨Åned non-learnable process, e.g. gradient-based update. Gradient-based
58"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.12079207920792079,"updates are in a zig-zag direction which slows down the movement to a minima. Also, the updates
59"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.12277227722772277,"crawl near the minima or saddle points slowly as the gradient magnitude vanishes rapidly over there.
60"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.12475247524752475,"A popular technique used for acceleration is the so-called momentum (e.g. RMSProp and Adam).
61"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.12673267326732673,"Instead of using only the current gradient, momentum accumulates the gradients of the past steps to
62"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.12871287128712872,"determine the direction to go, which helps move more quickly towards the minima as it dampens the
63"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.1306930693069307,"zig-zag oscillations and builds the speed to quicken the convergence.
64"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.13267326732673268,"Motivated by the beneÔ¨Åt brought by momentum in gradient-based update, we propose to learn an
65"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.13465346534653466,"NN-based model for gradient-based update with the concept of momentum. As the effectiveness of
66"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.13663366336633664,"momentum comes from its memory of the gradients of past steps, we propose an NN block with a
67"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.13861386138613863,"memory-assistant mechanism such that it will leverage the gradient descents from previous stages,
68"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.1405940594059406,"which is implemented using convolutional long short-term memory (ConvLSTM) units.
69"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.14257425742574256,"Cross-stage self-attention module An HSI has its speciÔ¨Åc physical characteristics. One is the
70"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.14455445544554454,"self-similarity and strong correlation along the spectral axis, as the entries along the spectral axis
71"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.14653465346534653,"measure the same object region but at different wavelengths. To exploit such speciÔ¨Åc physical
72"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.1485148514851485,"property of HSIs, we propose a self-attention module along the spectral axis. While self-attention is
73"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.1504950495049505,"not completely new in image reconstruction, our implementation is different from existing ones by
74"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.15247524752475247,"deÔ¨Åning in a cross-stage manner.
75"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.15445544554455445,"One additional function for such a cross-stage self-attention module is to exploit the similarity of the
76"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.15643564356435644,"features learned over different stages by forming a path between two different stages. Such similarities
77"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.15841584158415842,"among the featured learned at different stages come from the fact that the role of reÔ¨Ånement step is
78"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.1603960396039604,"supposed to the same across different stages. The beneÔ¨Åt of utilizing such similarity is two-fold. One
79"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.16237623762376238,"is for more efÔ¨Åcient feature delivery across the full stages, and the other is for enabling interactions
80"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.16435643564356436,"among the features at different stages during the training.
81"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.16633663366336635,"Loss on spectral geometry consistency In addition to the standard ‚Ñì1 loss, a spectral geometry
82"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.16831683168316833,"consistency loss is proposed for training the DUN for HSI reconstruction. Such a loss encourages the
83"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.1702970297029703,"model to concentrate more on the proÔ¨Åle of spectral changes during reconstruction, which helps to
84"
A GRADIENT DESCENT STEP FOR UPDATING THE ESTIMATE OF THE IMAGE,0.17227722772277226,"improve the reconstruction accuracy as empirically observed.
85"
RELATED WORK,0.17425742574257425,"2
Related Work
86"
RELATED WORK,0.17623762376237623,"By imposing certain priors on HSIs, regularization is a widely-used approach to solving the problem
87"
RELATED WORK,0.1782178217821782,"of HSI reconstruction. The priors for natural images have been extended to HSIs, e.g., sparsity prior
88"
RELATED WORK,0.1801980198019802,"in image gradients used in total variation [13, 14], sparsity prior under a learned dictionary [2, 15],
89"
RELATED WORK,0.18217821782178217,"and non-local self-similarity prior in the form of low-rankness for spatial-spectral patches [16‚Äì19].
90"
RELATED WORK,0.18415841584158416,"These pre-deÔ¨Åned priors are often insufÔ¨Åcient for the HSIs with complex and diverse structures.
91"
RELATED WORK,0.18613861386138614,"There is an increasing trend to use the implicit image prior encoded in a pre-trained or untrained
92"
RELATED WORK,0.18811881188118812,"NN for regularization. Plug-and-play methods [14, 20, 21] employ the NNs pre-trained on the
93"
RELATED WORK,0.1900990099009901,"denoising tasks of HSIs or natural images to regularize the reconstruction process. However, pre-
94"
RELATED WORK,0.19207920792079208,"trained denoising NNs are usually not very effective to handle the noise and artifacts generated in the
95"
RELATED WORK,0.19405940594059407,"iterative reconstruction process. Self-supervised learning methods [22, 23] use an untrained NN to
96"
RELATED WORK,0.19603960396039605,"re-parameterize the latent HSI and train it to match the observed snapshot. Such an online learning
97"
RELATED WORK,0.19801980198019803,"scheme is computationally expensive and cannot leverage the knowledge from external data.
98"
RELATED WORK,0.2,"It has been a prominent approach that to end-to-end train a DNN that maps a snapshot to the latent
99"
RELATED WORK,0.201980198019802,"HSI; see e.g. [24, 5, 25, 26, 9, 8]. Many existing studies employ the DUN architecturee.g. [3, 6, 7, 4].
100"
RELATED WORK,0.20396039603960395,"Recall that a DUN often consists of pairs of steps: one step for updating the estimate of the latent HSI
101"
RELATED WORK,0.20594059405940593,"and the other step for reÔ¨Åning the estimate with a learnable prior. Most existing works focus on the
102"
RELATED WORK,0.2079207920792079,"latter, which can be viewed as a denoising NN that exploits different image priors, e.g., spatial-spectral
103"
RELATED WORK,0.2099009900990099,"prior [3],non-local self-similarity prior [6], and patch-level Gaussian scale mixture prior [7].
104"
RELATED WORK,0.21188118811881188,"Learning updating steps in DUNs Zhang et al. [4] replaced the operators Œ¶, Œ¶‚ä§appearing in the
105"
RELATED WORK,0.21386138613861386,"gradient descent step of PGD by convolutions and residual blocks, with a channel attention block to
106"
RELATED WORK,0.21584158415841584,"estimate the step size in PGD from the estimate output by the previous stage. Different from that, we
107"
RELATED WORK,0.21782178217821782,"do not learn those operators but utilize them to have a better update step. Working on natural image
108"
RELATED WORK,0.2198019801980198,"recovery rather than on HSI reconstruction, Mou et al. [27] used a residual block to estimate the
109"
RELATED WORK,0.22178217821782178,"gradient descent step. In comparison, we use an LSTM to leverage the dependency between different
110"
RELATED WORK,0.22376237623762377,"stages for estimating the updating step.
111"
RELATED WORK,0.22574257425742575,"Self-attention for HSI reconstruction Self-attention (SA) has been exploited in existing works for
112"
RELATED WORK,0.22772277227722773,"HSI reconstruction. Miao et al. [5] used a generative adversarial network with SA for the initial stage
113"
RELATED WORK,0.2297029702970297,"in the NN. Meng et al. [28] used three spatial-spectral SA modules to exploit the spatial-spectral
114"
RELATED WORK,0.2316831683168317,"correlation of an HSI. Hu et al. [9] develops a spatial-spectral attention module with efÔ¨Åcient feature
115"
RELATED WORK,0.23366336633663368,"fusion. In comparison to these methods, ours treats spectral maps as tokens for SA and calculates the
116"
RELATED WORK,0.23564356435643563,"SA along the spectral dimension. This shares a similar idea with a parallel work [8] which also treats
117"
RELATED WORK,0.2376237623762376,"spectral maps as tokens in a transformer-based model. Different from it, we use SA in a cross-stage
118"
RELATED WORK,0.2396039603960396,"manner which enhances the feature Ô¨Çow at the same time.
119"
RELATED WORK,0.24158415841584158,"Training loss for HSI reconstruction Most existing NNs for HSI reconstruction are trained by
120"
RELATED WORK,0.24356435643564356,"the standard mean-squared-error loss or ‚Ñì1 loss. Hu et al. [9] introduced a frequency-domain loss
121"
RELATED WORK,0.24554455445544554,"to narrow the frequency-domain discrepancy between network predictions and ground truths. In
122"
RELATED WORK,0.24752475247524752,"comparison, the loss we proposed narrows the discrepancy in terms of spectral geometric changes.
123"
PROPOSED APPROACH,0.2495049504950495,"3
Proposed Approach
124"
PROPOSED APPROACH,0.2514851485148515,"The proposed DUN for HSI reconstruction is based on the PGD algorithm [11, 12] for the following
125"
PROPOSED APPROACH,0.25346534653465347,"optimization model regularized by the functional R:
126"
PROPOSED APPROACH,0.25544554455445545,"min
x
‚à•y ‚àíŒ¶x‚à•2
2 + ŒªR(x),
Œª ‚ààR+,
(3)"
PROPOSED APPROACH,0.25742574257425743,"The PGD algorithm for solving Eq. (3) alternately iterates between two steps: gradient-descent (GD)
127"
PROPOSED APPROACH,0.2594059405940594,"step for updating the estimate, and proximal mapping (PM) step for reÔ¨Åning the estimate by Ô¨Åtting
128"
PROPOSED APPROACH,0.2613861386138614,"the functional R with encoded image prior: For k = 1, ¬∑ ¬∑ ¬∑ , K,
129"
PROPOSED APPROACH,0.2633663366336634,"[GD]:
u(k) = x(k‚àí1) + Œ≥(k)Œ¶‚ä§(y ‚àíŒ¶x(k‚àí1)),
(4)"
PROPOSED APPROACH,0.26534653465346536,"[PM]:
x(k) = ProxR(u(k)) ‚âúargmin
x‚Ä≤
‚à•x ‚àíu(k)‚à•2
2 + 2Œ≥(k)R(u(k)).
(5)"
PROPOSED APPROACH,0.26732673267326734,"where Œ≥(k) denotes step size. Most existing DUNs focus on modeling the PM step (5) by an NN for a
130"
PROPOSED APPROACH,0.2693069306930693,"data-driven prior. The GD step (4) usually is kept unchanged with the learnable parameter Œ≥(k).
131"
PROPOSED APPROACH,0.2712871287128713,"We propose a Memory-Assistant Descent (MAD) block to model the GD step (4) and a Cross-stage
132"
PROPOSED APPROACH,0.2732673267326733,"Attentive Proximal (CAP) sub-network to model the PM step (5). The former functions as gradient
133"
PROPOSED APPROACH,0.27524752475247527,"descent across different stages for momentum-motivated acceleration, which leads to a more efÔ¨Åcient
134"
PROPOSED APPROACH,0.27722772277227725,"update than that only using the gradient at current stage. The latter is to utilize the self-similarities
135"
PROPOSED APPROACH,0.27920792079207923,"existing in an HSI with a cross-stage manner, which enable us to exploit special characteristics of
136"
PROPOSED APPROACH,0.2811881188118812,"HSIs and fasten feature Ô¨Çow through the DNN. In short, the proposed NN, called MadcapNet, is the
137"
PROPOSED APPROACH,0.28316831683168314,"concatenation of K stages, each of which contains a pair of a MAD block and a CAP sub-network;
138"
PROPOSED APPROACH,0.2851485148514851,"see Figure 1 for the diagram of MadcapNet.
139"
PROPOSED APPROACH,0.2871287128712871,"ùíö
ùíô(ùüé)"
PROPOSED APPROACH,0.2891089108910891,"Stage 1
ùíÑ(ùüé) MAD CAP"
PROPOSED APPROACH,0.29108910891089107,Stage K
PROPOSED APPROACH,0.29306930693069305,ùíô($%&) $ùíô
PROPOSED APPROACH,0.29504950495049503,"Skip connection
ReLU
Triplet Attention
3√ó3 Conv"
PROPOSED APPROACH,0.297029702970297,Stage k MAD CAP CAP
PROPOSED APPROACH,0.299009900990099,1√ó1 Conv
PROPOSED APPROACH,0.300990099009901,Stage k-1 CAP MAD
PROPOSED APPROACH,0.30297029702970296,"ùíÑ("")
ùíÑ(""$%)
ùíÑ(%) ùíô(ùüè) MAD ùíô($)"
PROPOSED APPROACH,0.30495049504950494,"‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶"
PROPOSED APPROACH,0.3069306930693069,Stage k
PROPOSED APPROACH,0.3089108910891089,ùíÑ((%&)
PROPOSED APPROACH,0.3108910891089109,ùíô((%&) ùíÑ(() ùíâ(()
PROPOSED APPROACH,0.31287128712871287,"ùíõ(()ùíõ((%&) ùíô("")"
PROPOSED APPROACH,0.31485148514851485,Cross-Stage
PROPOSED APPROACH,0.31683168316831684,Self-Attention
PROPOSED APPROACH,0.3188118811881188,ConvLSTM ùíõ(() CAP ùíñ(()
PROPOSED APPROACH,0.3207920792079208,"ùíõ(ùíå)
ùíõ(ùíå%ùüè)"
PROPOSED APPROACH,0.3227722772277228,"1√ó1
1√ó1
1√ó1
3√ó3
3√ó3
3√ó3"
PROPOSED APPROACH,0.32475247524752476,"ùëΩ(&)
ùë≤(&)
ùë∏(&)"
PROPOSED APPROACH,0.32673267326732675,"reshape
reshape"
PROPOSED APPROACH,0.3287128712871287,Cross-stage Self-Attention
PROPOSED APPROACH,0.3306930693069307,reshape ùöΩ‚ä§ùíö
PROPOSED APPROACH,0.3326732673267327,Figure 1: Diagram of the proposed augmented deep unrolling neural network for HSI reconstruction.
MEMORY-ASSISTANT DESCENT BLOCKS,0.3346534653465347,"3.1
Memory-Assistant Descent Blocks
140"
MEMORY-ASSISTANT DESCENT BLOCKS,0.33663366336633666,"The MAD blocks are a set of ConvLSTM units [29] placed at each stage of the NN, which utilizes
141"
MEMORY-ASSISTANT DESCENT BLOCKS,0.33861386138613864,"the long-range dependencies among all cascading stages for momentum-assistant gradient update. In
142"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3405940594059406,"each MAD block, the gradient map is deÔ¨Åned by
143"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3425742574257426,"u(k) = Œ¶‚ä§(y ‚àíŒ¶x(k‚àí1))
(6)"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3445544554455445,"is taken as the input for the k-th ConvLSTM unit, which introduces information on gradient descent.
144"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3465346534653465,"Let h(k), c(k) denote the hidden state and cell state in the ConvLSTM at the k-th stage respectively,
145"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3485148514851485,"where h(k) is of the same size as x(k). The MAD block is deÔ¨Åned as
146"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3504950495049505,"[h(k), c(k)] = ConvLSTM(u(k), x(k‚àí1), c(k‚àí1)),
(7)"
MEMORY-ASSISTANT DESCENT BLOCKS,0.35247524752475246,"for k = 1, ¬∑ ¬∑ ¬∑ , K. Different from original ConvLSTM units which use the previous hidden state
147"
MEMORY-ASSISTANT DESCENT BLOCKS,0.35445544554455444,"h(k‚àí1) as input, we replace h(k‚àí1) by x(k‚àí1), the output from the CAP sub-network of the previous
148"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3564356435643564,"stage. The motivation behind is to utilize the current gradient decent deÔ¨Åned over x(k‚àí1). Then, h(k)
149"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3584158415841584,"is used as the input of the CAP sub-network and c(k) is fed to the MAD block at the next stage as an
150"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3603960396039604,"accumulator of state information.
151"
MEMORY-ASSISTANT DESCENT BLOCKS,0.36237623762376237,"In the k-th stage, the ConvLSTM unit calculates hk, ck by the following rules
152"
MEMORY-ASSISTANT DESCENT BLOCKS,0.36435643564356435,"c(k)
=
fk ‚äôc(k‚àí1) + i(k) ‚äôtanh(g(k)),
(8)"
MEMORY-ASSISTANT DESCENT BLOCKS,0.36633663366336633,"h(k)
=
o(k) ‚äôtanh(c(k)),
(9)"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3683168316831683,"where ‚äôdenotes Hadamard product, and ik, fk, ok, gk denote the input gate, forget gate, output
153"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3702970297029703,"gate, and the intermediate result, respectively, which are calculated as follows:
154"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3722772277227723,"i(k)
=
sigmoid(Wmiu(k) + Wxix(k‚àí1) + bi),
(10)"
MEMORY-ASSISTANT DESCENT BLOCKS,0.37425742574257426,"f (k)
=
sigmoid(Wmfu(k) + Wxfx(k‚àí1) + bf),
(11)"
MEMORY-ASSISTANT DESCENT BLOCKS,0.37623762376237624,"g(k)
=
Wmgu(k) + Wxgx(k‚àí1) + bg,
(12)"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3782178217821782,"o(k)
=
sigmoid(Wmou(k) + Wxox(k‚àí1) + bo),
(13)"
MEMORY-ASSISTANT DESCENT BLOCKS,0.3801980198019802,"where W‚àó‚àóare implemented by 3 √ó 3 convolutional layers with bias terms b*.
155"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.3821782178217822,"3.2
Cross-stage Attentive Proximal Sub-networks
156"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.38415841584158417,"The CAP blocks function as a learnable PM step (5) which reÔ¨Ånes the estimate from the MAD
157"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.38613861386138615,"block. It can be understood as a denoising NN by interpreting the estimation residual as noise. Given
158"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.38811881188118813,"h(k) (of the same size as x) from the MAD block as input, we map it to a feature tensor z(k) via a
159"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.3900990099009901,"convolutional layer, which is then processed by a cross-stage SA module. Afterward, the results are
160"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.3920792079207921,"fed to a sequence of convolutional layers with rectiÔ¨Åed linear units (ReLUs) and a triplet attention [30].
161"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.3940594059405941,"The output with the same size as x is combined with the input h(k) via a skip connection, yielding
162"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.39603960396039606,"the reconstructed HSI x(k) at the current stage. See Figure 1 for the details.
163"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.39801980198019804,"Recall that SA [31] relates input feature tokens to compute a reÔ¨Åned feature representation. It Ô¨Årst
164"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4,"generates a key/query/value vector of length d from each token, and all the key/query/value vectors
165"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.401980198019802,"are stored as K, Q, V respectively. Then, SA is calculated as follows:
166"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.403960396039604,"SA(Q, K, V ) = softmax
  1
‚àö"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.40594059405940597,"d
QK‚ä§
V .
(14)"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4079207920792079,"We treat each feature channel as a token so as to exploit the self-similarities among feature channels.
167"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4099009900990099,"Such tokens are aligned due to natural alignment of spectral slices of an HSI. In the kth stage, rather
168"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.41188118811881186,"than use the feature z(k) at current stage to calculate K(k), Q(k), V (k), we only use z(k) for Q(k)
169"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.41386138613861384,"while using the feature z(k‚àí1) of previous stage for K(k), V (k). Concretely, we calculate
170"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4158415841584158,"Q(k) = W (k)
Qd W (k)
Qp z(k), K(k) = W (k)
Kd W (k)
Kp z(k‚àí1), V (k) = W (k)
Vd W (k)
Vp z(k‚àí1),
(15)"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4178217821782178,"where W (k)
(‚àóp), W (k)
(‚àód) are 1 √ó 1 convolutions and 3 √ó 3 depth-wise convolutions respectively for better
171"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4198019801980198,"encoding spatial-channel context.
172"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.42178217821782177,"The motivation of the cross-stage strategy is as follows. The DUN architecture alternates between
173"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.42376237623762375,"the update and the reÔ¨Ånement. Since the CAP sub-networks at different stages play the same role
174"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.42574257425742573,"of reÔ¨Ånement, their extracted features should be highly correlated and the features extracted from
175"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4277227722772277,"the previous stage provide good initials for the corresponding ones at the next stage. However, the
176"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4297029702970297,"aforementioned pipeline does not utilize such correlations for more efÔ¨Åcient training, which may
177"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4316831683168317,"result in a bottleneck for features Ô¨Çowing through the whole DUN. The proposed cross-stage SA
178"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.43366336633663366,"scheme forms a path between two stages, which allows efÔ¨Åcient feature transmission during inference
179"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.43564356435643564,"and enhances feature interactions during training.
180"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4376237623762376,"The mutli-head strategy [31] is adopted for the cross-stage SA. First, we split the key/query/value ma-
181"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4396039603960396,"trices into H heads along channel dimension: Q(k) = [Q(k)
1 , ¬∑ ¬∑ ¬∑ , Q(k)
H ], K(k) = [K(k)
1 , ¬∑ ¬∑ ¬∑ , K(k)
H ],
182"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.4415841584158416,"and V (k) = [V (k)
1
, ¬∑ ¬∑ ¬∑ , V (k)
H ]. Then, the output is calculated as
183"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.44356435643564357,"O(k) = ‚à™H
j=1SA(Q(k)
j , K(k)
j
V (k)
j
),
(16)"
CROSS-STAGE ATTENTIVE PROXIMAL SUB-NETWORKS,0.44554455445544555,"which is reshaped for subsequent processing.
184"
LOSS FUNCTION FOR TRAINING,0.44752475247524753,"3.3
Loss function for Training
185"
LOSS FUNCTION FOR TRAINING,0.4495049504950495,"To better train a NN for HSI reconstruction, we propose an additional loss called spectral geometry
186"
LOSS FUNCTION FOR TRAINING,0.4514851485148515,"consistency (SGC) loss. For an HSI X ‚ààRM√óN√óŒõ, we deÔ¨Åne the geometry map D(x) as follows.
187"
LOSS FUNCTION FOR TRAINING,0.4534653465346535,"D(X) = ‚àác(sign(‚àácX)) ‚àà{‚àí1, 0, 1}M√óN√óŒõ,
(17)"
LOSS FUNCTION FOR TRAINING,0.45544554455445546,"where ‚àác calculates the gradient along the spectral axis, and sign(¬∑) denotes element-wise sign
188"
LOSS FUNCTION FOR TRAINING,0.45742574257425744,"function. For a spatial location (m0, n0), D(X)[m0, n0, ¬∑] indicates the wavelengths where the
189"
LOSS FUNCTION FOR TRAINING,0.4594059405940594,"monotony of spectral values changes, which is one geometrical property of the spectral curve. Based
190"
LOSS FUNCTION FOR TRAINING,0.4613861386138614,"on D, the SGC loss emphasizes the geometrical layout consistency between the reconstructed HSI
191"
LOSS FUNCTION FOR TRAINING,0.4633663366336634,"and ground truth.
192"
LOSS FUNCTION FOR TRAINING,0.46534653465346537,"Considering HSIs exhibit high spatial sparsity, the irrelevant dark regions are omitted for robustness.
193"
LOSS FUNCTION FOR TRAINING,0.46732673267326735,"This is achieved by constructing a mask MX that thresholds the max density along spectral dimension:
194"
LOSS FUNCTION FOR TRAINING,0.4693069306930693,"MX(m, n, Œª) = 1 if maxŒª X(m, n, Œª) ‚â•Œ±; and 0otherwise. Let X, c
X denote the reconstructed
195"
LOSS FUNCTION FOR TRAINING,0.47128712871287126,"HSI and its ground truth respectively. The SGC loss is deÔ¨Åned as
196"
LOSS FUNCTION FOR TRAINING,0.47326732673267324,"Lsgc ‚âú‚à•MX ‚äôD(X) ‚àíM b
X ‚äôD(c
X)‚à•1.
(18)"
LOSS FUNCTION FOR TRAINING,0.4752475247524752,"By minimizing Lsgc, the HSI predicted by the NN is biased to the one with the same wavelength-
197"
LOSS FUNCTION FOR TRAINING,0.4772277227722772,"density trends of ground truths, which helps to alleviate possible over-Ô¨Åtting. Then, the overall loss is
198 199"
LOSS FUNCTION FOR TRAINING,0.4792079207920792,"L ‚âúL1 + Œ≥Lsgc = ‚à•X ‚àíc
X‚à•1 + Œ≥‚à•MX ‚äôD(X) ‚àíM b
X ‚äôD(c
X)‚à•1, Œ≥ ‚ààR+.
(19)"
EXPERIMENTS,0.48118811881188117,"4
Experiments
200"
EXPERIMENTS,0.48316831683168315,"We implement MadcapNet with PyTorch. The stage number K is set to 6. On all convolutional
201"
EXPERIMENTS,0.48514851485148514,"layers, the kernel sizes are all set to 3 √ó 3, and both the stride and padding number are set to 1. The
202"
EXPERIMENTS,0.4871287128712871,"head number H for the self-attention in CAP blocks is set to 8. Regarding the training loss, we
203"
EXPERIMENTS,0.4891089108910891,"set Œ± =
5
255 for MX and Œ≥ = 0.5 for Eq. (19) The training is done via the Adam optimizer with
204"
EXPERIMENTS,0.4910891089108911,"a Ô¨Åxed learning rate of 10‚àí4 and a maximal epoch number of 200. The same data augmentation
205"
EXPERIMENTS,0.49306930693069306,"scheme as [7] is adopted, including rotation and Ô¨Çipping. All the models are trained and tested
206"
EXPERIMENTS,0.49504950495049505,"on an NVIDIA GeForce RTX 1080Ti GPU. Our code will be released on GitHub. upon paper‚Äôs
207"
EXPERIMENTS,0.497029702970297,"acceptance. Following [7], Peak-Signal-to-Noise-Ratio (PSNR) and Structured SIMilarity (SSIM)
208"
EXPERIMENTS,0.499009900990099,"index are adopted as the metrics to evaluate the reconstruction results quantitatively.
209"
EVALUATION ON SYNTHETIC DATA,0.500990099009901,"4.1
Evaluation on Synthetic Data
210"
EVALUATION ON SYNTHETIC DATA,0.502970297029703,"CAVE and KAIST datasets Following [28, 7], we use the CAVE dataset [32] containing 32 HSIs
211"
EVALUATION ON SYNTHETIC DATA,0.504950495049505,"with 31 spectral bands for training, and 10 scenes with 31 spectral bands from the KAIST dataset [14]
212"
EVALUATION ON SYNTHETIC DATA,0.5069306930693069,"for test. All these HSIs are cropped into patches with a spatial size of 256 √ó 256 and reduced to 28
213"
EVALUATION ON SYNTHETIC DATA,0.5089108910891089,"wavelengths ranging from 450nm to 650nm via spectral interpolation. The snapshot measurements
214"
EVALUATION ON SYNTHETIC DATA,0.5108910891089109,"are generated by the 256 √ó 256 mask of CASSI used in [28].
215"
EVALUATION ON SYNTHETIC DATA,0.5128712871287129,"Ten existing methods are chosen for comparison, including (a) two conventional methods: GAP-
216"
EVALUATION ON SYNTHETIC DATA,0.5148514851485149,"TV [13] and DeSCI [17]; (b) one self-supervised learning-based method: PnP-DIP [22]; and (c)
217"
EVALUATION ON SYNTHETIC DATA,0.5168316831683168,"seven supervised learning-based methods: Œª-Net [5] HSSP [3], DNU [6], TSA-Net [28], DGSMP [7],
218"
EVALUATION ON SYNTHETIC DATA,0.5188118811881188,"HDNet [9], and MST-L [8]. The HSSP, DNU and DGSMP are based on DUNs. The HDNet and
219"
EVALUATION ON SYNTHETIC DATA,0.5207920792079208,"MST-L are from two latest works accepted in an upcoming conference.
220"
EVALUATION ON SYNTHETIC DATA,0.5227722772277228,"The quantitative results are listed in Table 1, which are quoted from [8, 9] whenever possible and
221"
EVALUATION ON SYNTHETIC DATA,0.5247524752475248,"otherwise obtained with released codes. It can be seen that our approach signiÔ¨Åcantly outperforms the
222"
EVALUATION ON SYNTHETIC DATA,0.5267326732673268,"compared ones. SpeciÔ¨Åcally, MadcapNet shows remarkable superior performance over other DUNs.
223"
EVALUATION ON SYNTHETIC DATA,0.5287128712871287,"It also surpasses MST-L and HDNet (i.e. two latest methods) with an average PSNR gain of more
224"
EVALUATION ON SYNTHETIC DATA,0.5306930693069307,"than 1dB and 2dB respectively. Table 1 also compares the model complexity of different methods in
225"
EVALUATION ON SYNTHETIC DATA,0.5326732673267327,"terms of number of parameters and number of Giga Floating-point Operations Per Second (GFLOPS).
226"
EVALUATION ON SYNTHETIC DATA,0.5346534653465347,"Although our model contains ConvLSTM and self-attention blocks, it is still kept compact to maintain
227"
EVALUATION ON SYNTHETIC DATA,0.5366336633663367,"a relatively-low model complexity. Among all compared methods, our MadcapNet has the smallest
228"
EVALUATION ON SYNTHETIC DATA,0.5386138613861386,"number of GLOPS, and it is smaller than all other models except DNU. These results show the
229"
EVALUATION ON SYNTHETIC DATA,0.5405940594059406,"practicability of MadcapNet for real applications. To conclude, our approach can achieve the best
230"
EVALUATION ON SYNTHETIC DATA,0.5425742574257426,"trade-off between performance and model complexity.
231"
EVALUATION ON SYNTHETIC DATA,0.5445544554455446,"ICVL and Harvard datasets We also conduct experiments on the ICVL dataset [33] and the
232"
EVALUATION ON SYNTHETIC DATA,0.5465346534653466,"Harvard dataset [34], respectively. The ICVL dataset consists of 201 HSIs of real-world objects, each
233"
EVALUATION ON SYNTHETIC DATA,0.5485148514851486,"with 31 spectral bands collected from 400nm to 700 nm at a 10nm step. The Harvard dataset consists
234"
EVALUATION ON SYNTHETIC DATA,0.5504950495049505,"of 50 outdoor scenes, each with 31 spectral bands collected from 420nm to 720nm at a 10nm step.
235"
EVALUATION ON SYNTHETIC DATA,0.5524752475247525,Table 1: Quantitative results in PSNR(dB) (even rows) and SSIM (odd rows) on KAIST dataset.
EVALUATION ON SYNTHETIC DATA,0.5544554455445545,"Method
#Param. #GFLOPS Scene#1
#2
#3
#4
#5
#6
#7
#8
#9
#10
Mean"
EVALUATION ON SYNTHETIC DATA,0.5564356435643565,"GAP-TV
-
-
26.82
22.89 26.31 30.65 23.64 21.85 23.76 21.98 22.63 23.10 24.36
0.754
0.61 0.802 0.852 0.703 0.663 0.688 0.655 0.682 0.584 0.669"
EVALUATION ON SYNTHETIC DATA,0.5584158415841585,"DeSCI
-
-
27.13
23.04 26.62 34.96 23.94 22.38 24.45 22.03 24.56 23.59 25.27
0.748
0.62 0.818 0.897 0.706 0.683 0.743 0.673 0.732 0.587 0.721"
EVALUATION ON SYNTHETIC DATA,0.5603960396039604,"Œª-net
62.64M
117.98
30.10
28.49 27.73 37.01 26.19 28.64 26.47 26.09 27.50 27.13 28.53
0.849
0.805 0.870 0.934 0.817 0.853 0.806 0.831 0.826 0.816 0.841"
EVALUATION ON SYNTHETIC DATA,0.5623762376237624,"HSSP
-
-
31.48
31.09 28.96 34.56 28.53 30.83 28.71 30.09 30.43 28.78 30.35
0.858
0.842 0.823 0.902 0.808 0.877 0.824 0.881 0.868 0.842 0.852"
EVALUATION ON SYNTHETIC DATA,0.5643564356435643,"DNU
1.19M
163.48
31.72
31.13 29.99 35.34 29.03 30.87 28.99 30.13 31.03 29.14 30.74
0.863
0.846 0.845 0.908 0.833 0.887 0.839 0.885 0.876 0.849 0.863"
EVALUATION ON SYNTHETIC DATA,0.5663366336633663,"PnP-DIP
33.85M
64.42
32.68
27.26 31.30 40.54 29.79 30.39 28.18 29.44 34.51 28.51 31.26
0.890
0.833 0.914 0.962 0.900 0.877 0.913 0.874 0.927 0.851 0.894"
EVALUATION ON SYNTHETIC DATA,0.5683168316831683,"TSA-Net
44.25M
110.06
32.03
31.00 32.25 39.19 29.39 31.44 30.32 29.35 30.01 29.59 31.46
0.892
0.858 0.915 0.953 0.884 0.908 0.878 0.888 0.890 0.874 0.894"
EVALUATION ON SYNTHETIC DATA,0.5702970297029702,"DGSMP
3.76M
646.65
33.26
32.09 33.06 40.54 28.86 33.08 30.74 31.55 31.66 31.44 32.63
0.915
0.898 0.925 0.964 0.882 0.937 0.886 0.923 0.911 0.925 0.917"
EVALUATION ON SYNTHETIC DATA,0.5722772277227722,"HDNet
2.35M
154.00
34.95
32.52 34.52 43.00 32.49 35.96 29.18 34.00 34.56 32.22 34.34
0.948
0.953 0.957 0.981 0.957 0.965 0.937 0.961 0.958 0.950 0.957"
EVALUATION ON SYNTHETIC DATA,0.5742574257425742,"MST-L
2.03M
28.15
35.40
35.87 36.51 42.27 32.77 34.80 33.66 32.67 35.39 32.50 35.18
0.941
0.944 0.953 0.973 0.947 0.955 0.925 0.948 0.949 0.941 0.948"
EVALUATION ON SYNTHETIC DATA,0.5762376237623762,"MadcapNet 1.51M
24.24
36.24
37.49 37.07 42.85 34.09 35.61 35.37 33.96 36.67 33.12 36.32
0.951
0.961 0.963 0.981 0.962 0.966 0.949 0.962 0.960 0.948 0.961"
EVALUATION ON SYNTHETIC DATA,0.5782178217821782,"Following the protocol of [3, 35], 50 HSIs in the ICVL dataset and 9 HSIs in the Harvard dataset are
236"
EVALUATION ON SYNTHETIC DATA,0.5801980198019802,"used for test respectively, and the rest samples for training. All HSIs for training and test are cropped
237"
EVALUATION ON SYNTHETIC DATA,0.5821782178217821,"into patches with a spatial size of 48 √ó 48, while keeping the band number unchanged. The snapshot
238"
EVALUATION ON SYNTHETIC DATA,0.5841584158415841,"measurements are generated by the 48 √ó 48 mask of CASSI used in [3].
239"
EVALUATION ON SYNTHETIC DATA,0.5861386138613861,"Six existing methods are selected for comparison, including (a) a conventional method: SSNR [16];
240"
EVALUATION ON SYNTHETIC DATA,0.5881188118811881,"and (b) six supervised learning-based methods: HSCNN [36],Œª-Net [5], DNU [6], DTLP [37], and
241"
EVALUATION ON SYNTHETIC DATA,0.5900990099009901,"HDNet [9]. The DNU and DTLP use DUNs, and the HDNet is a latest method.
242"
EVALUATION ON SYNTHETIC DATA,0.592079207920792,"See Table 2 for the quantitative comparison. The results of the compared methods are cited from [37].
243"
EVALUATION ON SYNTHETIC DATA,0.594059405940594,"The proposed one outperformed all other methods, with more than 0.85db PSNR improvement on
244"
EVALUATION ON SYNTHETIC DATA,0.596039603960396,"both datasets. Such noticeable performance gains of MadcapNet over other DUNs again demonstrated
245"
EVALUATION ON SYNTHETIC DATA,0.598019801980198,"the effectiveness of our network architecture.
246"
EVALUATION ON SYNTHETIC DATA,0.6,Table 2: Quantitative results in PSNR(dB) and SSIM on ICVL and Harvard datasets.
EVALUATION ON SYNTHETIC DATA,0.601980198019802,"Dataset
Metric
SSNR
HSCNN
Œª-Net
DNU
DTLP
HDNet
MadcapNet"
EVALUATION ON SYNTHETIC DATA,0.6039603960396039,"ICVL
PSNR
30.40
28.45
29.01
32.61
34.53
36.38
37.60
SSIM
0.943
0.934
0.946
0.966
0.977
0.981
0.985"
EVALUATION ON SYNTHETIC DATA,0.6059405940594059,"Harvard
PSNR
31.14
27.60
29.37
31.11
32.43
34.02
34.88
SSIM
0.942
0.895
0.909
0.929
0.941
0.950
0.956"
EVALUATION ON SYNTHETIC DATA,0.6079207920792079,"Visual inspection See Figure 2 for the visualization of HSI reconstruction results on two samples
247"
EVALUATION ON SYNTHETIC DATA,0.6099009900990099,"from the KAIST and Harvard datasets respectively. The spectral curves (density versus wavelength)
248"
EVALUATION ON SYNTHETIC DATA,0.6118811881188119,"correspond to the points marked by green boxes in the RGB references. In the legends of both
249"
EVALUATION ON SYNTHETIC DATA,0.6138613861386139,"Ô¨Ågures, we provide the curve correlation value between the result of a compared method and the
250"
EVALUATION ON SYNTHETIC DATA,0.6158415841584158,"ground truth. Those values show that the HSIs reconstructed by the proposed MadcapNet have
251"
EVALUATION ON SYNTHETIC DATA,0.6178217821782178,"the highest correlation to the ground truths. We also visualize three spectral channels of an entire
252"
EVALUATION ON SYNTHETIC DATA,0.6198019801980198,"reconstructed HSI and zoom in the selected regions marked by yellow boxes. It can be seen that the
253"
EVALUATION ON SYNTHETIC DATA,0.6217821782178218,"results of MadcapNet are more visually pleasing than that of other compared methods, with a better
254"
EVALUATION ON SYNTHETIC DATA,0.6237623762376238,"reconstruction of structures.
255"
EVALUATION ON SYNTHETIC DATA,0.6257425742574257,"RGB Ref.
Measurement"
EVALUATION ON SYNTHETIC DATA,0.6277227722772277,"450
500
550
600
650
Wavelength (nm) 40 60 80 100 120"
EVALUATION ON SYNTHETIC DATA,0.6297029702970297,Density
EVALUATION ON SYNTHETIC DATA,0.6316831683168317,"TSA-Net (Corr: 0.9927)
PnP-DIP (Corr: 0.9847)
DGMP (Corr: 0.9930)
Ours (Corr: 0.9959)
GT"
EVALUATION ON SYNTHETIC DATA,0.6336633663366337,"TSA-Net
PnP-DIP
DGMP
Ours
Ground Truth"
EVALUATION ON SYNTHETIC DATA,0.6356435643564357,"RGB Ref.
Measurement"
EVALUATION ON SYNTHETIC DATA,0.6376237623762376,"450
500
550
600
650
700
Wavelength (nm) 5 10 15 20 25 30"
EVALUATION ON SYNTHETIC DATA,0.6396039603960396,Density
EVALUATION ON SYNTHETIC DATA,0.6415841584158416,"HyperRecon (Corr: 0.9952)
DNU (Corr: 0.9953)
DTLP (Corr: 0.9960)
Ours (Corr: 0.9973)
GT"
EVALUATION ON SYNTHETIC DATA,0.6435643564356436,"Œª-Net
DNU
DTLP
Ours
Ground Truth"
EVALUATION ON SYNTHETIC DATA,0.6455445544554456,"Figure 2: Visual comparison of HSI reconstruction on two samples from KAIST and Harvard
datasets respectively. Left: spectra curves of the selected regions marked by green boxes. Right:
reconstruction on the spectral channels."
EVALUATION ON REAL DATA,0.6475247524752475,"4.2
Evaluation on Real Data
256"
EVALUATION ON REAL DATA,0.6495049504950495,"We also conduct a test on the real snapshots of spatial size 660√ó714 from [7, 28], which are captured
257"
EVALUATION ON REAL DATA,0.6514851485148515,"by a real system with 28 wavelengths ranging from 450nm to 650nm and with 54-pixel dispersion
258"
EVALUATION ON REAL DATA,0.6534653465346535,"in the column dimension. Following [7, 28], we use the mask associated with that real system to
259"
EVALUATION ON REAL DATA,0.6554455445544555,"generate snapshots on both the CAVE and KAIST datasets, and then we inject 11-bit shot noise to the
260"
EVALUATION ON REAL DATA,0.6574257425742575,"snapshots for simulating real situations. The resulting data is used to retrain our model. Due to the
261"
EVALUATION ON REAL DATA,0.6594059405940594,"lack of ground truths in test data, we only compare the qualitative results of different methods. See
262"
EVALUATION ON REAL DATA,0.6613861386138614,"Figure 3 for the reconstruction results on a real scene, and see more in the supplementary materials.
263"
EVALUATION ON REAL DATA,0.6633663366336634,"The performance of MadcapNet is also good on the real data. This indeed has demonstrated the good
264"
EVALUATION ON REAL DATA,0.6653465346534654,"generalization performance of our model.
265"
ABLATION STUDIES,0.6673267326732674,"4.3
Ablation Studies
266"
ABLATION STUDIES,0.6693069306930693,"Ablation studies are conducted on the KAIST dataset. We form some baselines by removing one or
267"
ABLATION STUDIES,0.6712871287128713,"more main components of our approach. Concretely, we consider (a) replace the MAD blocks by
268"
ABLATION STUDIES,0.6732673267326733,"the GD steps (4); (b) replace the cross-stage SA in the CAP network with the inner-stage SA which
269"
ABLATION STUDIES,0.6752475247524753,"uses the features at current stage to calculate K(k), Q(k), V (k) in (15); (c) replace the cross-stage SA
270"
ABLATION STUDIES,0.6772277227722773,"Ref. & Snapshot
DeSCI
TSA-Net
PnP-DIP
DGMP
Ours"
ABLATION STUDIES,0.6792079207920793,"Figure 3: Visual comparison of HSI reconstruction on real data, in terms of two spectral channels."
ABLATION STUDIES,0.6811881188118812,"with a same number of convolutional layers; (d) remove the SGC loss Lsgc. For a fair comparison,
271"
ABLATION STUDIES,0.6831683168316832,"each baseline is conÔ¨Ågured to have (nearly) the same number of parameters as the original model, by
272"
ABLATION STUDIES,0.6851485148514852,"uniformly increasing the channel numbers of convolutional layers. The results are listed in Table 3.
273"
ABLATION STUDIES,0.6871287128712872,"It can be seen that each main component in our approach plays an important role. Using the MAD
274"
ABLATION STUDIES,0.689108910891089,"blocks as an alternate to GD steps can improve PSNR by almost 1db. It also brings improvement across
275"
ABLATION STUDIES,0.691089108910891,"all baseline settings. BeneÔ¨Åting from the power of SA, the cross-stage SA brings noticeable PSNR
276"
ABLATION STUDIES,0.693069306930693,"gain. In addition, the SA utilized in the cross-stage manner leads to around 0.36dB improvement in
277"
ABLATION STUDIES,0.695049504950495,"PSNR over that utilized in the inner-stage manner. The SGC loss also has a solid contribution to the
278"
ABLATION STUDIES,0.697029702970297,"performance. See Figure 4 for an illustration of the effect of the SGC loss, where training with Lsgc
279"
ABLATION STUDIES,0.699009900990099,"makes the tendency of the predicted spectral curves closer to ground truths. See also supplementary
280"
ABLATION STUDIES,0.700990099009901,"materials for more results.
281"
ABLATION STUDIES,0.7029702970297029,Table 3: Results in ablation studies on KAIST dataset.
ABLATION STUDIES,0.7049504950495049,"Metric
w/o MAD
w/o CAP
Cross‚ÜíInner
w/o Lsgc
Original"
ABLATION STUDIES,0.7069306930693069,"PSNR(dB)
35.35
35.80
35.96
35.53
36.32
SSIM
0.947
0.956
0.958
0.951
0.961"
ABLATION STUDIES,0.7089108910891089,450 475 500 525 550 575 600 625 650
ABLATION STUDIES,0.7108910891089109,Wavelength (nm) 7 8 9 10 11 12 13
ABLATION STUDIES,0.7128712871287128,Density
ABLATION STUDIES,0.7148514851485148,"w/o 
sgc
w 
sgc
GT"
ABLATION STUDIES,0.7168316831683168,450 475 500 525 550 575 600 625 650
ABLATION STUDIES,0.7188118811881188,Wavelength (nm) 11 12 13 14 15 16
ABLATION STUDIES,0.7207920792079208,Density
ABLATION STUDIES,0.7227722772277227,"w/o 
sgc
w 
sgc
GT"
ABLATION STUDIES,0.7247524752475247,450 475 500 525 550 575 600 625 650
ABLATION STUDIES,0.7267326732673267,Wavelength (nm) 5 6 7 8 9 10 11
ABLATION STUDIES,0.7287128712871287,Density
ABLATION STUDIES,0.7306930693069307,"w/o 
sgc
w 
sgc
GT"
ABLATION STUDIES,0.7326732673267327,450 475 500 525 550 575 600 625 650
ABLATION STUDIES,0.7346534653465346,Wavelength (nm) 6 8 10 12 14 16 18 20
ABLATION STUDIES,0.7366336633663366,Density
ABLATION STUDIES,0.7386138613861386,"w/o 
sgc
w 
sgc
GT"
ABLATION STUDIES,0.7405940594059406,Figure 4: Spectra of selected regions on Scene#1 (Ô¨Årst two) and Scene#5 (last two) of KAIST dataset.
CONCLUSION,0.7425742574257426,"5
Conclusion
282"
CONCLUSION,0.7445544554455445,"In this paper, we proposed an augmented DUN for CASSI-based hyperspectral imaging. The proposed
283"
CONCLUSION,0.7465346534653465,"DUN is based on the unfolding of PGD, with three-fold augmentations: momentum-motivated
284"
CONCLUSION,0.7485148514851485,"ConvLSTM-assistant module for improving the gradient descent steps, a sub-network with cross-stage
285"
CONCLUSION,0.7504950495049505,"self-attention for exploiting self-similarities of an HSI and enhancing feature Ô¨Çow simultaneously,
286"
CONCLUSION,0.7524752475247525,"and a loss to induce predictions biased to spectral geometry consistency. The combination of these
287"
CONCLUSION,0.7544554455445545,"augmentations leads to noticeable performance improvement in HSI reconstruction, which were
288"
CONCLUSION,0.7564356435643564,"demonstrated by extensive experiments. The proposed DUN also sees its potential application to
289"
CONCLUSION,0.7584158415841584,"other compressive imaging problems. We will study it in the future.
290"
REFERENCES,0.7603960396039604,"References
291"
REFERENCES,0.7623762376237624,"[1] Michael E Gehm, Renu John, David J Brady, Rebecca M Willett, and Timothy J Schulz. Single-shot
292"
REFERENCES,0.7643564356435644,"compressive spectral imaging with a dual-disperser architecture. Opt. Lett., 15(21):14013‚Äì14027, 2007.
293"
REFERENCES,0.7663366336633664,"[2] Xing Lin, Yebin Liu, Jiamin Wu, and Qionghai Dai. Spatial-spectral encoded compressive hyperspectral
294"
REFERENCES,0.7683168316831683,"imaging. ACM Trans. Graph., 33(6):1‚Äì11, 2014.
295"
REFERENCES,0.7702970297029703,"[3] Lizhi Wang, Chen Sun, Ying Fu, Min H Kim, and Hua Huang. Hyperspectral image reconstruction using a
296"
REFERENCES,0.7722772277227723,"deep spatial-spectral prior. In Proc. CVPR, pages 8032‚Äì8041, 2019.
297"
REFERENCES,0.7742574257425743,"[4] Xuanyu Zhang, Yongbing Zhang, Ruiqin Xiong, Qilin Sun, and Jian Zhang. Herosnet: Hyperspectral
298"
REFERENCES,0.7762376237623763,"explicable reconstruction and optimal sampling deep network for snapshot compressive imaging. Proc.
299"
REFERENCES,0.7782178217821782,"CVPR, 2022.
300"
REFERENCES,0.7801980198019802,"[5] Xin Miao, Xin Yuan, Yunchen Pu, and Vassilis Athitsos. Œª-net: Reconstruct hyperspectral images from a
301"
REFERENCES,0.7821782178217822,"snapshot measurement. In Proc. CVPR, pages 4059‚Äì4069, 2019.
302"
REFERENCES,0.7841584158415842,"[6] Lizhi Wang, Chen Sun, Maoqing Zhang, Ying Fu, and Hua Huang. Dnu: deep non-local unrolling for
303"
REFERENCES,0.7861386138613862,"computational spectral imaging. In Proc. CVPR, pages 1661‚Äì1671, 2020.
304"
REFERENCES,0.7881188118811882,"[7] Tao Huang, Weisheng Dong, Xin Yuan, Jinjian Wu, and Guangming Shi. Deep gaussian scale mixture
305"
REFERENCES,0.7900990099009901,"prior for spectral compressive imaging. In Proc. CVPR, pages 16216‚Äì16225, 2021.
306"
REFERENCES,0.7920792079207921,"[8] Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc
307"
REFERENCES,0.7940594059405941,"Van Gool. Mask-guided spectral-wise transformer for efÔ¨Åcient hyperspectral image reconstruction. Proc.
308"
REFERENCES,0.7960396039603961,"CVPR, 2022.
309"
REFERENCES,0.7980198019801981,"[9] Xiaowan Hu, Yuanhao Cai, Jing Lin, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc
310"
REFERENCES,0.8,"Van Gool. Hdnet: High-resolution dual-domain learning for spectral compressive imaging. Proc. CVPR,
311"
REFERENCES,0.801980198019802,"2022.
312"
REFERENCES,0.803960396039604,"[10] Ying Fu, Tao Zhang, Lizhi Wang, and Hua Huang. Coded hyperspectral image reconstruction using deep
313"
REFERENCES,0.805940594059406,"external and internal learning. IEEE Trans. Pattern Anal. Mach. Intell., 2021.
314"
REFERENCES,0.807920792079208,"[11] Patrick L Combettes and Jean-Christophe Pesquet. Proximal splitting methods in signal processing. In
315"
REFERENCES,0.80990099009901,"Fixed-point algorithms for inverse problems in science and engineering, pages 185‚Äì212. Springer, 2011.
316"
REFERENCES,0.8118811881188119,"[12] Atsushi Nitanda. Stochastic proximal gradient descent with acceleration techniques. Proc. NeurIPS, 27,
317"
REFERENCES,0.8138613861386138,"2014.
318"
REFERENCES,0.8158415841584158,"[13] Xin Yuan. Generalized alternating projection based total variation minimization for compressive sensing.
319"
REFERENCES,0.8178217821782178,"In Proc. ICIP, pages 2539‚Äì2543. IEEE, 2016.
320"
REFERENCES,0.8198019801980198,"[14] Inchang Choi, Daniel S. Jeon, Giljoo Nam, Diego Gutierrez, and Min H. Kim. High-quality hyperspectral
321"
REFERENCES,0.8217821782178217,"reconstruction using a spectral prior. ACM Trans. Graph., 36(6):218:1‚Äì13, 2017.
322"
REFERENCES,0.8237623762376237,"[15] Xin Yuan, Tsung-Han Tsai, Ruoyu Zhu, Patrick Llull, David Brady, and Lawrence Carin. Compressive
323"
REFERENCES,0.8257425742574257,"hyperspectral imaging with side information. IEEE J. Sel. Top Signal Process, 9(6):964‚Äì976, 2015.
324"
REFERENCES,0.8277227722772277,"[16] Ying Fu, Yinqiang Zheng, Imari Sato, and Yoichi Sato. Exploiting spectral-spatial correlation for coded
325"
REFERENCES,0.8297029702970297,"hyperspectral image restoration. In Proc. CVPR, pages 3727‚Äì3736, 2016.
326"
REFERENCES,0.8316831683168316,"[17] Yang Liu, Xin Yuan, Jinli Suo, David J Brady, and Qionghai Dai. Rank minimization for snapshot
327"
REFERENCES,0.8336633663366336,"compressive imaging. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):2990‚Äì3006, 2018.
328"
REFERENCES,0.8356435643564356,"[18] Shipeng Zhang, Lizhi Wang, Ying Fu, Xiaoming Zhong, and Hua Huang. Computational hyperspectral
329"
REFERENCES,0.8376237623762376,"imaging based on dimension-discriminative low-rank tensor recovery. In Proc. CVPR, pages 10183‚Äì10192,
330"
REFERENCES,0.8396039603960396,"2019.
331"
REFERENCES,0.8415841584158416,"[19] Yong Chen, Ting-Zhu Huang, Wei He, Naoto Yokoya, and Xi-Le Zhao. Hyperspectral image compressive
332"
REFERENCES,0.8435643564356435,"sensing reconstruction using subspace-based nonlocal tensor ring decomposition. IEEE Transactions on
333"
REFERENCES,0.8455445544554455,"Image Processing, 29:6813‚Äì6828, 2020.
334"
REFERENCES,0.8475247524752475,"[20] Xin Yuan, Yang Liu, Jinli Suo, and Qionghai Dai. Plug-and-play algorithms for large-scale snapshot
335"
REFERENCES,0.8495049504950495,"compressive imaging. In Proc. CVPR, pages 1447‚Äì1457, 2020.
336"
REFERENCES,0.8514851485148515,"[21] Haiquan Qiu, Yao Wang, and Deyu Meng. Effective snapshot compressive-spectral imaging via deep
337"
REFERENCES,0.8534653465346534,"denoising and total variation priors. In Proc. CVPR, pages 9127‚Äì9136, 2021.
338"
REFERENCES,0.8554455445544554,"[22] Ziyi Meng, Zhenming Yu, Kun Xu, and Xin Yuan. Self-supervised neural networks for spectral snapshot
339"
REFERENCES,0.8574257425742574,"compressive imaging. Proc. CVPR, 2021.
340"
REFERENCES,0.8594059405940594,"[23] Yuhui Quan, Xinran Qin, Mingqin Chen, and Yan Huang. High-quality self-supervised snapshot hyper-
341"
REFERENCES,0.8613861386138614,"spectral imaging. In proc. ICASSP, pages 1526‚Äì1530. IEEE, 2022.
342"
REFERENCES,0.8633663366336634,"[24] Lizhi Wang, Tao Zhang, Ying Fu, and Hua Huang. Hyperreconnet: Joint coded aperture optimization and
343"
REFERENCES,0.8653465346534653,"image reconstruction for compressive hyperspectral imaging. IEEE Trans. Image Process., 28(5):2257‚Äì
344"
REFERENCES,0.8673267326732673,"2270, 2018.
345"
REFERENCES,0.8693069306930693,"[25] Tao Zhang, Ying Fu, Lizhi Wang, and Hua Huang. Hyperspectral image reconstruction using deep external
346"
REFERENCES,0.8712871287128713,"and internal learning. In Proc. CVPR, pages 8559‚Äì8568, 2019.
347"
REFERENCES,0.8732673267326733,"[26] Kouhei Yorimoto and Xian-Hua Han. Hypermixnet: Hyperspectral image reconstruction with deep mixed
348"
REFERENCES,0.8752475247524752,"network fryom a snapshot measurement. In Proc. ICCV Workshop, pages 1184‚Äì1193, 2021.
349"
REFERENCES,0.8772277227722772,"[27] Chong Mou, Qian Wang, and Jian Zhang. Deep generalized unfolding networks for image restoration.
350"
REFERENCES,0.8792079207920792,"Proc. CVPR, 2022.
351"
REFERENCES,0.8811881188118812,"[28] Ziyi Meng, Jiawei Ma, and Xin Yuan. End-to-end low cost compressive spectral imaging with spatial-
352"
REFERENCES,0.8831683168316832,"spectral self-attention. In Proc. ECCV, pages 187‚Äì204. Springer, 2020.
353"
REFERENCES,0.8851485148514852,"[29] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Con-
354"
REFERENCES,0.8871287128712871,"volutional lstm network: A machine learning approach for precipitation nowcasting. Proc. NeurIPS, 28,
355"
REFERENCES,0.8891089108910891,"2015.
356"
REFERENCES,0.8910891089108911,"[30] Diganta Misra, Trikay Nalamada, Ajay Uppili Arasanipalai, and Qibin Hou. Rotate to attend: Convolutional
357"
REFERENCES,0.8930693069306931,"triplet attention module. In Proc. WACV, pages 3139‚Äì3148, 2021.
358"
REFERENCES,0.8950495049504951,"[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
359"
REFERENCES,0.897029702970297,"Kaiser, and Illia Polosukhin. Attention is all you need. Proc. NeurIPS, 30, 2017.
360"
REFERENCES,0.899009900990099,"[32] Fumihito Yasuma, Tomoo Mitsunaga, Daisuke Iso, and Shree K Nayar. Generalized assorted pixel
361"
REFERENCES,0.900990099009901,"camera: postcapture control of resolution, dynamic range, and spectrum. IEEE Trans. Image Process.,
362"
REFERENCES,0.902970297029703,"19(9):2241‚Äì2253, 2010.
363"
REFERENCES,0.904950495049505,"[33] Boaz Arad and Ohad Ben-Shahar. Sparse recovery of hyperspectral signal from natural rgb images. In
364"
REFERENCES,0.906930693069307,"Proc. ECCV, pages 19‚Äì34. Springer, 2016.
365"
REFERENCES,0.9089108910891089,"[34] Ayan Chakrabarti and Todd Zickler. Statistics of real-world hyperspectral images. In Proc. CVPR, pages
366"
REFERENCES,0.9108910891089109,"193‚Äì200. IEEE, 2011.
367"
REFERENCES,0.9128712871287129,"[35] Shipeng Zhang, Lizhi Wang, Lei Zhang, and Hua Huang. Learning tensor low-rank prior for hyperspectral
368"
REFERENCES,0.9148514851485149,"image reconstruction. In Proc. CVPR, pages 12006‚Äì12015, 2021.
369"
REFERENCES,0.9168316831683169,"[36] Zhiwei Xiong, Zhan Shi, Huiqun Li, Lizhi Wang, Dong Liu, and Feng Wu. Hscnn: Cnn-based hyperspectral
370"
REFERENCES,0.9188118811881189,"image recovery from spectrally undersampled projections. In Proc. ICCV Workshop, pages 518‚Äì525, 2017.
371"
REFERENCES,0.9207920792079208,"[37] Siming Zheng, Yang Liu, Ziyi Meng, Mu Qiao, Zhishen Tong, Xiaoyu Yang, Shensheng Han, and Xin Yuan.
372"
REFERENCES,0.9227722772277228,"Deep plug-and-play priors for spectral snapshot compressive imaging. Photonics Res., 9(2):B18‚ÄìB29,
373"
REFERENCES,0.9247524752475248,"2021.
374"
REFERENCES,0.9267326732673268,"Checklist
375"
REFERENCES,0.9287128712871288,"1. For all authors...
376"
REFERENCES,0.9306930693069307,"(a) Do the main claims made in the abstract and introduction accurately reÔ¨Çect the paper‚Äôs
377"
REFERENCES,0.9326732673267327,"contributions and scope? [Yes]
378"
REFERENCES,0.9346534653465347,"(b) Did you describe the limitations of your work? [Yes] See supplemental material.
379"
REFERENCES,0.9366336633663367,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] See
380"
REFERENCES,0.9386138613861386,"supplemental material.
381"
REFERENCES,0.9405940594059405,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
382"
REFERENCES,0.9425742574257425,"them?[Yes]
383"
REFERENCES,0.9445544554455445,"2. If you are including theoretical results...
384"
REFERENCES,0.9465346534653465,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
385"
REFERENCES,0.9485148514851485,"(b) Did you include complete proofs of all theoretical results? [N/A]
386"
REFERENCES,0.9504950495049505,"3. If you ran experiments...
387"
REFERENCES,0.9524752475247524,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
388"
REFERENCES,0.9544554455445544,"mental results (either in the supplemental material or as a URL)? [No] But we promise
389"
REFERENCES,0.9564356435643564,"to release all our codes upon paper‚Äôs acceptance, as stated in Section 4.
390"
REFERENCES,0.9584158415841584,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
391"
REFERENCES,0.9603960396039604,"were chosen)? [Yes]
392"
REFERENCES,0.9623762376237623,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
393"
REFERENCES,0.9643564356435643,"ments multiple times)? [N/A]
394"
REFERENCES,0.9663366336633663,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
395"
REFERENCES,0.9683168316831683,"of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.
396"
REFERENCES,0.9702970297029703,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
397"
REFERENCES,0.9722772277227723,"(a) If your work uses existing assets, did you cite the creators? [Yes]
398"
REFERENCES,0.9742574257425742,"(b) Did you mention the license of the assets? [N/A]
399"
REFERENCES,0.9762376237623762,"(c) Did you include any new asset either in the supplemental material or as a URL? [N/A]
400"
REFERENCES,0.9782178217821782,"(d) Did you discuss whether and how consent was obtained from people whose data you‚Äôre
401"
REFERENCES,0.9801980198019802,"using/curating? [N/A]
402"
REFERENCES,0.9821782178217822,"(e) Did you discuss whether the data you are using/curating contains personally identiÔ¨Åable
403"
REFERENCES,0.9841584158415841,"information or offensive content? [N/A]
404"
REFERENCES,0.9861386138613861,"5. If you used crowdsourcing or conducted research with human subjects...
405"
REFERENCES,0.9881188118811881,"(a) Did you include the full text of instructions given to participants and screenshots, if
406"
REFERENCES,0.9900990099009901,"applicable? [N/A]
407"
REFERENCES,0.9920792079207921,"(b) Did you describe any potential participant risks, with links to Institutional Review
408"
REFERENCES,0.994059405940594,"Board (IRB) approvals, if applicable? [N/A]
409"
REFERENCES,0.996039603960396,"(c) Did you include the estimated hourly wage paid to participants and the total amount
410"
REFERENCES,0.998019801980198,"spent on participant compensation? [N/A]
411"
