Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001996007984031936,"In recent years, methods based on convolutional kernels have achieved state-of-the-
1"
ABSTRACT,0.003992015968063872,"art performance in video frame interpolation task. However, due to the inherent
2"
ABSTRACT,0.005988023952095809,"limitations of their convolutional kernel size, it seems that their performances
3"
ABSTRACT,0.007984031936127744,"have reached a plateau. On the other hand, Transformers are gradually replac-
4"
ABSTRACT,0.00998003992015968,"ing convolutional neural networks as a new backbone structure in image tasks,
5"
ABSTRACT,0.011976047904191617,"thanks to their ability to establish global correlations. However, in video tasks, the
6"
ABSTRACT,0.013972055888223553,"computational complexity and memory requirements of Transformer will become
7"
ABSTRACT,0.015968063872255488,"more challenging. To address this issue, we employ two different Transformers,
8"
ABSTRACT,0.017964071856287425,"SGuTA and SCubA, in VFI task. SGuTA utilizes the spatial information of each
9"
ABSTRACT,0.01996007984031936,"video frame to guide the generation of temporal vector at each pixel position.
10"
ABSTRACT,0.021956087824351298,"Meanwhile, SCubA introduces local attention into the VFI task, which can be
11"
ABSTRACT,0.023952095808383235,"viewed as a counterpart of 3D convolution in local attention Transformers. Ad-
12"
ABSTRACT,0.02594810379241517,"ditionally, we analyze and compare different embedding strategies and propose
13"
ABSTRACT,0.027944111776447105,"a more balanced embedding strategy in terms of parameter count, computational
14"
ABSTRACT,0.029940119760479042,"complexity, and memory requirements. Extensive quantitative and qualitative
15"
ABSTRACT,0.031936127744510975,"experiments demonstrate that our models exhibit high proficiency in handling
16"
ABSTRACT,0.033932135728542916,"large motions and providing precise motion estimation, resulting in new state-of-
17"
ABSTRACT,0.03592814371257485,"the-art results in various benchmark tests. The source code can be obtained at
18"
ABSTRACT,0.03792415169660679,"https://github.com/esthen-bit/SGuTA-SCubA.
19"
INTRODUCTION,0.03992015968063872,"1
Introduction
20"
INTRODUCTION,0.041916167664670656,"Video frame interpolation (VFI) is the process of reconstructing uncaptured intermediate frames
21"
INTRODUCTION,0.043912175648702596,"during the exposure time by synthesizing adjacent frames, which can enhance its visual quality and
22"
INTRODUCTION,0.04590818363273453,"smoothness of motion. As a fundamental problem in computer vision, it requires an understanding of
23"
INTRODUCTION,0.04790419161676647,"both spatially and temporally consistence within the video frames, breaking the limitations of video
24"
INTRODUCTION,0.0499001996007984,"sampling rate and lighting conditions. Its applications span across diverse domains, including virtual
25"
INTRODUCTION,0.05189620758483034,"reality [1], video compression [2, 3, 4], and slow-motion generation [5, 6].
26"
INTRODUCTION,0.05389221556886228,"The majority of state-of-the-art techniques for VFI rely on convolutional neural networks (CNNs),
27"
INTRODUCTION,0.05588822355289421,"particularly those based on kernels [7, 8, 9, 10, 11, 12, 13, 14], which have gained increasing
28"
INTRODUCTION,0.05788423153692615,"popularity in recent years. Nevertheless, due to the inherent constraint imposed by the kernel
29"
INTRODUCTION,0.059880239520958084,"size, convolutional kernels seem to have reached their performance ceiling, even after undergoing
30"
INTRODUCTION,0.06187624750499002,"transformations such as 2D kernels, separable kernels, deformable kernels, and 3D kernels. It appears
31"
INTRODUCTION,0.06387225548902195,"that there is a limited potential for further improvement of VFI methods based on CNNs and their
32"
INTRODUCTION,0.0658682634730539,"associated kernels. Meanwhile, Transformers [15] have recently demonstrated its great potential
33"
INTRODUCTION,0.06786427145708583,"in various image tasks such as image classification [16, 17, 18], object detection [19, 20], spectral
34"
INTRODUCTION,0.06986027944111776,"reconstruction [21], and image restoration [22, 23, 24], due to their ability to capture long-range
35"
INTRODUCTION,0.0718562874251497,A Embedded Token
INTRODUCTION,0.07385229540918163,A Partiton Cube
INTRODUCTION,0.07584830339321358,Coordinate System
INTRODUCTION,0.07784431137724551,f1faee
INTRODUCTION,0.07984031936127745,Uncaptured
INTRODUCTION,0.08183632734530938,frames
INTRODUCTION,0.08383233532934131,"(a)
(b) H W
T S T I0
I1 SGuTA"
INTRODUCTION,0.08582834331337326,"Figure 1: a) A simple illustration of the correlation between space and time within a video. The
colored pixels move from left to right, these uncaptured frames can be restored because T = S. b) A
simple illustration of shifted cubes approach, where the boundaries of the dimensions are connected,
and the cubes with the same color are merged and masked after being shifted."
INTRODUCTION,0.08782435129740519,"dependencies and contextual relationships in sequences. However, extending the Transformer to
36"
INTRODUCTION,0.08982035928143713,"video tasks is not as straightforward as extending 2D convolutions to 3D convolutions, as it poses
37"
INTRODUCTION,0.09181636726546906,"challenges such as computational complexity and memory requirements.
38"
INTRODUCTION,0.09381237524950099,"This article introduces two distinct Transformer-based approaches, SCubA (Shifted-Cube Attention)
39"
INTRODUCTION,0.09580838323353294,"and SGuTA (Spatially-Guided Temporal Attention), which are integrated into a multi-stage multi-scale
40"
INTRODUCTION,0.09780439121756487,"framework for VFI task. Both methodologies exhibit linear computational complexity with respect to
41"
INTRODUCTION,0.0998003992015968,"the patch number, making them concise, efficient, and demonstrating exceptional performance.
42"
INTRODUCTION,0.10179640718562874,"It is revealed by [25] that there exists an inherent correlation between the spatial information and
43"
INTRODUCTION,0.10379241516966067,"temporal sequence of a video. Fig. 1(a) illustrates a simple example for this phenomenon. If we
44"
INTRODUCTION,0.10578842315369262,"exchange any spatial dimension (height or width) with the temporal dimension, a new video sequence
45"
INTRODUCTION,0.10778443113772455,"can be obtained in which the low-resolution version of the original spatial information is recurred.
46"
INTRODUCTION,0.10978043912175649,"Therefore, the higher-resolution original spatial information can provide powerful guidance for
47"
INTRODUCTION,0.11177644710578842,"improving the temporal resolution. Inspired by this, we propose SGuTA, a self-attention mechanism
48"
INTRODUCTION,0.11377245508982035,"that establishes intrinsic correlations between spatial information and temporal sequence.
49"
INTRODUCTION,0.1157684630738523,"Inspired by [16, 18, 26], as shown in Fig. 1(b), SCubA treats 3D-patches as tokens and partitions
50"
INTRODUCTION,0.11776447105788423,"them into cubes with a fixed size along the height, width, and time axis. Local self-attention is
51"
INTRODUCTION,0.11976047904191617,"computed within each cube, followed by shifted-cube mechanism to establish connections between
52"
INTRODUCTION,0.1217564870259481,"adjacent cubes. This approach enables the model to exploit spatiotemporal locality inductive bias and
53"
INTRODUCTION,0.12375249500998003,"achieve better performance than existing methods.
54"
INTRODUCTION,0.12574850299401197,"The main contributions of this work are listed as follow:
55"
INTRODUCTION,0.1277445109780439,"1) We present a novel Transformer called SGuTA, which is designed to establish the inherent
56"
INTRODUCTION,0.12974051896207583,"correlations between the spatial characteristics and the temporal sequence within a video. SGuTA
57"
INTRODUCTION,0.1317365269461078,"outperforms VFIT-B [14] in terms of PSNR by 0.58dB on vimeo-90k test set.
58"
INTRODUCTION,0.13373253493013973,"2) We propose a method called SCubA, which applies Video Swin Transformer [26] to VFI task.
59"
INTRODUCTION,0.13572854291417166,"Compared to VFIT-B, SCubA achieves a PSNR improvement of 1.08dB while reducing both the
60"
INTRODUCTION,0.1377245508982036,"number of parameters (Params) and computational complexity (FLOPs) by approximately 40%.
61"
INTRODUCTION,0.13972055888223553,"3) We conduct a analysis of existing embedding strategies, and put forth a novel half-overlapping
62"
INTRODUCTION,0.14171656686626746,"embedding strategy. This method exhibits a more balanced performance in relation to Params,
63"
INTRODUCTION,0.1437125748502994,"computational complexity, and memory usage.
64"
RELATED WORKS,0.14570858283433133,"2
Related Works
65"
VIDEO FRAME INTERPOLATION,0.14770459081836326,"2.1
Video Frame Interpolation
66"
VIDEO FRAME INTERPOLATION,0.1497005988023952,"The objective of VFI is to generate intermediate frames by combining adjacent frames that were not
67"
VIDEO FRAME INTERPOLATION,0.15169660678642716,"captured during the exposure period. This longstanding and classical problem in video processing
68"
VIDEO FRAME INTERPOLATION,0.1536926147704591,"is currently tackled through three prominent approaches: phase-based methods, optical flow-based
69"
VIDEO FRAME INTERPOLATION,0.15568862275449102,"methods, and kernel-based methods.
70"
VIDEO FRAME INTERPOLATION,0.15768463073852296,"Phase-based methods [27, 28] utilize Fourier theory to estimate motion by analyzing the phase
71"
VIDEO FRAME INTERPOLATION,0.1596806387225549,"discrepancy between corresponding pixels in consecutive frames. These techniques generate interme-
72"
VIDEO FRAME INTERPOLATION,0.16167664670658682,"diate frames by applying phase-shifted sinusoids. However, the 2π-ambiguity problem can pose a
73"
VIDEO FRAME INTERPOLATION,0.16367265469061876,"significant challenge in determining the correct motion.
74"
VIDEO FRAME INTERPOLATION,0.1656686626746507,"Flow-based methods [5, 10, 29, 30, 31, 32, 33] utilize optical flow estimation to perceive motion
75"
VIDEO FRAME INTERPOLATION,0.16766467065868262,"information and capture dense pixel correspondence between frames. These methods use a flow
76"
VIDEO FRAME INTERPOLATION,0.16966067864271456,"prediction network to compute bidirectional optical flow that guides frame synthesis, along with
77"
VIDEO FRAME INTERPOLATION,0.17165668662674652,"predicting occlusion masks or depth maps to reason about occlusions. However, these methods
78"
VIDEO FRAME INTERPOLATION,0.17365269461077845,"are limited by the accuracy of the underlying flow estimator remaining challenging problems in
79"
VIDEO FRAME INTERPOLATION,0.17564870259481039,"real-world videos, especially when there is large motion and heavy occlusion.
80"
VIDEO FRAME INTERPOLATION,0.17764471057884232,"Kernel-based methods have gained momentum in VFI since the emergence of AdaConv [7], a
81"
VIDEO FRAME INTERPOLATION,0.17964071856287425,"method that uses a fully convolutional network to estimate spatially adaptive convolution kernels.
82"
VIDEO FRAME INTERPOLATION,0.18163672654690619,"This is because it no longer requires motion estimation or pixel synthesis like flow-based methods.
83"
VIDEO FRAME INTERPOLATION,0.18363273453093812,"DSepConv [9] and AdaCoF [10] employ Deformable convolution to overcome the limitation of
84"
VIDEO FRAME INTERPOLATION,0.18562874251497005,"a fixed grid of locations in original convolution. CAIN [11] expands the receptive field size of
85"
VIDEO FRAME INTERPOLATION,0.18762475049900199,"convolution by utilizing Pixel Shuffle. SepConv [8] performs separable convolution, thereby reducing
86"
VIDEO FRAME INTERPOLATION,0.18962075848303392,"the Params and memory usage. Then, FLAVR [13] substitutes the 2D convolutions utilized in
87"
VIDEO FRAME INTERPOLATION,0.19161676646706588,"Unet with their 3D counterparts, while applying feature gating to each of the resultant 3D feature
88"
VIDEO FRAME INTERPOLATION,0.1936127744510978,"maps. This achieves the best performance among CNN-based methods at the cost of a large Params.
89"
VIDEO FRAME INTERPOLATION,0.19560878243512975,"However, these CNN-based architectures still cannot overcome their inherent limitation of using
90"
VIDEO FRAME INTERPOLATION,0.19760479041916168,"fixed-size kernels, which prevent them from capturing global dependencies to handle large motion
91"
VIDEO FRAME INTERPOLATION,0.1996007984031936,"and limit their further development for VFI task. Inspired by Depth-wise separable convolution
92"
VIDEO FRAME INTERPOLATION,0.20159680638722555,"[34], Zhihao Shi et al. introduce VFIT [14], a separated spatio-temporal multi-head self-attention
93"
VIDEO FRAME INTERPOLATION,0.20359281437125748,"mechanism, which outperforms all existing CNN-based approaches while significantly reducing the
94"
VIDEO FRAME INTERPOLATION,0.2055888223552894,"Params. Within the field of kernel-based methods, CNN backbones have undergone a developmental
95"
VIDEO FRAME INTERPOLATION,0.20758483033932135,"trajectory from 2D to separable and then to 3D kernels. Zhihao Shi et al. has proposed a space-time
96"
VIDEO FRAME INTERPOLATION,0.20958083832335328,"separation strategy [14] in Transformer methods. In this work, we introduce a 3D version of the local
97"
VIDEO FRAME INTERPOLATION,0.21157684630738524,"self-attention mechanism and a spatially-guided temporal self-attention mechanism to the VFI task.
98"
VISION TRANSFORMER,0.21357285429141717,"2.2
Vision Transformer
99"
VISION TRANSFORMER,0.2155688622754491,"The key innovation of the ViT [16] is its application of the Transformer architecture, originally
100"
VISION TRANSFORMER,0.21756487025948104,"developed for natural language processing, to computer vision tasks. This represents a notable
101"
VISION TRANSFORMER,0.21956087824351297,"departure from the standard backbone architecture of CNNs in computer vision. By dividing the image
102"
VISION TRANSFORMER,0.2215568862275449,"into a sequence of patches and leveraging the Transformer encoder to capture global dependencies
103"
VISION TRANSFORMER,0.22355289421157684,"between them, ViT achieves impressive performance on image classification benchmarks. This
104"
VISION TRANSFORMER,0.22554890219560877,"pioneering work has paved the way for subsequent research aimed at improving the utility of the ViT
105"
VISION TRANSFORMER,0.2275449101796407,"model, and underscores the potential of the Transformer architecture in computer vision applications.
106"
VISION TRANSFORMER,0.22954091816367264,"To mitigate the computational and memory challenges associated with ViT, Swin-Transformer
107"
VISION TRANSFORMER,0.2315369261477046,"[18] partitions the embedded patches into non-overlapping windows. Within each window, local
108"
VISION TRANSFORMER,0.23353293413173654,"self-attention is calculated by ViT. Subsequently, shifted-window self-attention is computed to
109"
VISION TRANSFORMER,0.23552894211576847,"establish the correlation among windows. This strategy has demonstrated remarkable performance
110"
VISION TRANSFORMER,0.2375249500998004,"in various image tasks, such as image classification [18, 35], object detection [20, 19], and image
111"
VISION TRANSFORMER,0.23952095808383234,"restoration [36, 24], achieving state-of-the-art results. Despite Swin-Transformer’s success in image
112"
VISION TRANSFORMER,0.24151696606786427,"tasks, extending it to video tasks by simply expanding along the time dimension resurfaces thorny
113"
VISION TRANSFORMER,0.2435129740518962,"computational and memory issues [14]. To address these issues, Ze Liu et al. further proposed a
114"
VISION TRANSFORMER,0.24550898203592814,"new 3D shifted windows mechanism [26] that efficiently captures temporal information, reduces the
115"
VISION TRANSFORMER,0.24750499001996007,"computational and memory demands, and achieves state-of-the-art results on video action recognition
116"
VISION TRANSFORMER,0.249500998003992,"tasks. This method makes Swin-Transformer a promising approach for video analysis tasks.
117"
PROPOSED METHOD,0.25149700598802394,"3
Proposed method
118"
PROPOSED METHOD,0.25349301397205587,"SCubA and SGuTA share this same network architecture. Fig. 2(a) depicts a multi-stage architecture
119"
PROPOSED METHOD,0.2554890219560878,"that utilizes Ns cascaded Multi-Scale Transformer. Instead of selecting two or four adjacent frames
120"
PROPOSED METHOD,0.25748502994011974,"next to the reference frame I0.5 as input, as in previous methods [11, 13, 14], our approach chooses
121"
PROPOSED METHOD,0.25948103792415167,"six adjacent frames to accurately estimate the motion of the interpolated frame. Moreover, a long
122"
PROPOSED METHOD,0.26147704590818366,"identity mapping is employed to mitigate the vanishing gradient problem. The desired interpolated
123 + +"
PROPOSED METHOD,0.2634730538922156,3D Conv 1x1x1
D DOWNSAMPLE,0.2654690618762475,3D DownSample
D DOWNSAMPLE,0.26746506986027946,"Embedding
De-Embedding
MAB MAB"
D DOWNSAMPLE,0.2694610778443114,"MAB
MAB MAB"
D UPSAMPLE,0.2714570858283433,3D UpSample
D UPSAMPLE,0.27345309381237526,(b)Multi-Scale Transformer
D UPSAMPLE,0.2754491017964072,"+
Concatenate ×"
D UPSAMPLE,0.2774451097804391,MatMul ×
D UPSAMPLE,0.27944111776447106,(f) SC-MAB
D UPSAMPLE,0.281437125748503,Embedded Tokens × Q K V
D UPSAMPLE,0.2834331337325349,Linear Projection Scale
D UPSAMPLE,0.28542914171656686,SoftMax ×
D UPSAMPLE,0.2874251497005988,Linear Projection + + PE
D UPSAMPLE,0.2894211576846307,(c) MSA ×
D OUT-CONV,0.29141716566866266,3D Out-Conv
D OUT-CONV,0.2934131736526946,(a)Multi-Stage Architecture
D OUT-CONV,0.2954091816367265,"+
Trans-U Net xNs"
D CONV,0.29740518962075846,3D Conv GELU
D CONV,0.2994011976047904,3D Conv GELU
D CONV,0.3013972055888224,3D Conv
D CONV,0.3033932135728543,(e) FFN
D CONV,0.30538922155688625,(d) G-MAB G-MSA +
D CONV,0.3073852295409182,LayerNorm FFN +
D CONV,0.3093812375249501,LayerNorm
D CONV,0.31137724550898205,SC-MSA +
D CONV,0.313373253493014,LayerNorm FFN +
D CONV,0.3153692614770459,LayerNorm
D CONV,0.31736526946107785,"Element-Wise add
+ +"
D CONV,0.3193612774451098,"Figure 2: The overall pipeline of SGuTA and SCubA. a) Multi-stage Architecture. b) Multi-scale
Transformer. c) Brief explanation of Multi-head Self-Attention. d) Global Multi-head Self-Attention
Block (G-MAB). e) Feed Forward Network. f) Shifted-Cube Multi-head Self-Attention Block
(SC-MAB)."
D CONV,0.3213572854291417,"frame ˆI0.5 is finally obtained via a 3D convolution operation. Fig.
2(b) illustrates the network
124"
D CONV,0.32335329341317365,"structure of the Multi-Scale Transformer when Ns = 1. Specifically, in the embedding layer, patches
125"
D CONV,0.3253493013972056,"of frames are transformed into dense representations. The de-embedding layer performs the inverse
126"
D CONV,0.3273453093812375,"operation of the embedding layer, whereby the representations are restored to patches. To enable
127"
D CONV,0.32934131736526945,"multi-scale self-attention, it is essential to downsample the output of the Multi-Head Attention Block
128"
D CONV,0.3313373253493014,"(MAB) from the previous scale before each MAB layer in the encoder. Similarly, in the decoder,
129"
D CONV,0.3333333333333333,"the upsampling of the output of each MAB layer is first performed to restore the original spatial
130"
D CONV,0.33532934131736525,"resolution, before sending into the next scale. Moreover, skip connections are employed at same
131"
D CONV,0.3373253493013972,"scale, while a 1 × 1 × 1 convolutional operation is applied to halve the depth of the concatenated
132"
D CONV,0.3393213572854291,"feature maps.
133"
D CONV,0.3413173652694611,"SGuTA and SCubA are two transformer-based models that differ in their self-attention mechanisms.
134"
D CONV,0.34331337325349304,"SGuTA is derived from the global self-attention mechanism, and its MAB module comprises solely
135"
D CONV,0.34530938123752497,"the G-MAB module illustrated in Fig. 2(d). In contrast, SCubA is based on the local self-attention
136"
D CONV,0.3473053892215569,"mechanism and its MAB module is constituted by the G-MAB module shown in Fig. 2(d), as well as
137"
D CONV,0.34930139720558884,"the SC-MAB module depicted in Fig. 2(f), with the dotted line being solely applicable when utilizing
138"
D CONV,0.35129740518962077,"SCubA. The composition of the feed-forward network (FFN) is presented in Fig. 2(e), whereas a
139"
D CONV,0.3532934131736527,"concise procedure of multi-head self-attention (MSA) is portrayed in Fig. 2(c) (with some details
140"
D CONV,0.35528942115768464,"omitted for concision). The disparity between SCubA and SGuTA is situated in the MSA module,
141"
D CONV,0.35728542914171657,"which will be expounded upon in Section 3.1
142"
PROPOSED MSA,0.3592814371257485,"3.1
Proposed MSA
143"
SGUTA,0.36127744510978044,"3.1.1
SGuTA
144"
SGUTA,0.36327345309381237,"Assuming an input tensor of shape Xin ∈RT ×H×W ×D, where D denotes the length of embedding
145"
SGUTA,0.3652694610778443,"vector , the MSA module of SGuTA first transposes and reshapes it into a 2D tensor X ∈RHW ×T D.
146"
SGUTA,0.36726546906187624,"This reshaped tensor X is then projected through linear transformations WQ, WK, and WV ∈
147"
SGUTA,0.36926147704590817,"RT D×T D to obtain the query Q, key K, and value V ∈RHW ×T D, respectively:
148"
SGUTA,0.3712574850299401,"Q = XWQ, K = XWK, V = XWV
(1)"
SGUTA,0.37325349301397204,"Such a transformation enables MSA to leverage the interactions among different spatial features within
149"
SGUTA,0.37524950099800397,"the input tensor, facilitating the capturing of complex dependencies in the subsequent processing.
150"
SGUTA,0.3772455089820359,"Then, Q, K, V ∈RHW ×T D are divided into n heads: Q = [Q1, ..., Qn], K = [K1, ..., Kn],
151"
SGUTA,0.37924151696606784,"V = [V1, ..., Vn], so that each head has a dimension of dh = T D"
SGUTA,0.3812375249500998,"n . The remaining process of SGuTA
152"
SGUTA,0.38323353293413176,"can be expressed as follows:
153"
SGUTA,0.3852295409181637,"SGuTA(Qi, Ki, Vi, d) = [
n
Concat
j=1
(headj)]W + P(V ), headj = Vjsoftmax(QT
j Kj"
SGUTA,0.3872255489021956,"d
)
(2)"
SGUTA,0.38922155688622756,"where d ∈R1 and W ∈RT D×T D are learnable parameters. P(V ) = 3DConv(Gelu(3DCon(v)))
154"
SGUTA,0.3912175648702595,"to generate positional embedding. The output Xout ∈RT ×H×W ×D are obtained by reshaping the
155"
SGUTA,0.3932135728542914,"result of Eq. (2). Observing that SGuTA establishes correlations from space to time. Compared to
156"
SGUTA,0.39520958083832336,"the Global MSA method of establishing spatio-temporal correlations between all patches, SGuTA is
157"
SGUTA,0.3972055888223553,"capable of effectively alleviating memory requirements and computational complexity issues. The
158"
SGUTA,0.3992015968063872,"computational complexity of SGuTA can be easily obtained as follows:
159"
SGUTA,0.40119760479041916,Ω(SGuTA) = 4TD2(THW) + 2TD2
SGUTA,0.4031936127744511,"n
(THW)
(3)"
SCUBA,0.405189620758483,"3.1.2
SCubA
160"
SCUBA,0.40718562874251496,"In accordance with [26], the input tensor Xin ∈RT ×H×W ×D is subjected to a process of partitioning
161"
SCUBA,0.4091816367265469,into T HW
SCUBA,0.4111776447105788,"thw non-overlapping cubes of size t×h×w utilizing an even partitioning strategy, as presented
162"
SCUBA,0.41317365269461076,"in Fig. 1b. The resulting cubes are reshaped into x ∈Rthw×D by the MSA module of SCubA. Linear
163"
SCUBA,0.4151696606786427,"transformations, specifically wq, wk, and wv ∈RD×D, are employed to produce the query q, key k,
164"
SCUBA,0.4171656686626746,"and value v ∈Rthw×D representations, respectively.
165"
SCUBA,0.41916167664670656,"q = xwq, k = xwk, v = xwv
(4)"
SCUBA,0.42115768463073855,"Similarly, q, k, v ∈Rthw×D are divided into n heads: q = [q1, ..., qn], k = [k1, ..., kn], v =
166"
SCUBA,0.4231536926147705,"[v1, ..., vn], so that each head has a dimension of dh = D"
SCUBA,0.4251497005988024,"n . The multi-head self-attention operation is
167"
SCUBA,0.42714570858283435,"then conducted within each cube according to the following equation:
168"
SCUBA,0.4291417165668663,"SCubA(qi, ki, vi, d) = [
n
Concat
j=1
(headj)]W + P(v), headj = softmax(qjkT
j
d
)vj
(5)"
SCUBA,0.4311377245508982,"To establish connections among the cubes, each cube is shifted along the time, height, and width
169"
SCUBA,0.43313373253493015,"dimensions by t/2, h/2, and w/2 steps, respectively, as depicted in Fig. 1b. The SC-MSA (corre-
170"
SCUBA,0.4351297405189621,"sponding to Fig. 2f) is calculated within each new cube.
171"
SCUBA,0.437125748502994,The process in Eq. (4) and Eq. (5) is calculated for T HW
SCUBA,0.43912175648702595,"thw times, and its computational complexity
172"
SCUBA,0.4411177644710579,"can be specifically expressed as:
173"
SCUBA,0.4431137724550898,"Ω(SCuBA) = 4D2(THW) + 2thwD(THW)
(6)"
OTHER MSAS,0.44510978043912175,"3.2
Other MSAs
174"
OTHER MSAS,0.4471057884231537,"In general, Global MSA [16] and Feature MSA [21] follow a standard procedure: the input Xin ∈
175"
OTHER MSAS,0.4491017964071856,"RT ×H×W ×D is reshaped and linearly transformed using W ′
Q, W ′
K, and W ′
V ∈RD×D to obtain
176"
OTHER MSAS,0.45109780439121755,"Q′, K′, and V ′ ∈RT HW ×D, which are then divided into n heads. Specifically, Q′ = [Q′
1, ..., Q′
n],
177"
OTHER MSAS,0.4530938123752495,"K′ = [K′
1, ..., K′
n], and V ′ = [V ′
1, ..., V ′
n], with each head having a dimension of dh = D"
OTHER MSAS,0.4550898203592814,"n .
178"
OTHER MSAS,0.45708582834331335,"For Global MSA and Feature MSA, The multi-head self-attention is obtained by:
179"
OTHER MSAS,0.4590818363273453,"Global(Q′
j, K′
j, V ′
j , d) = [
n
Concat
j=1
(headj)]W ′ + P(V ′), headj = softmax(Q′
jK′T
j
d
)V ′
j
(7) 180"
OTHER MSAS,0.46107784431137727,"Feature(Q′
j, K′
j, V ′
j , d) = [
n
Concat
j=1
(headj)]W ′ + P(V ′), headj = V ′
j softmax(Q′T
j K′
j
d
)
(8)"
OTHER MSAS,0.4630738522954092,"The computational complexity for Global MSA and Feature MSA is respectively given by:
181"
OTHER MSAS,0.46506986027944114,"Ω(Global) = 4D2(THW) + 2D(THW)2
(9)
182"
OTHER MSAS,0.46706586826347307,Ω(Feature) = 4D2(THW) + 2D2
OTHER MSAS,0.469061876247505,"n (THW)
(10)"
OTHER MSAS,0.47105788423153694,"We validate the performance of the MSAs listed above, in addition to STS and Sep-STS [14], in the
183"
OTHER MSAS,0.47305389221556887,"VFI task. Specific results and analysis can be found in Section 4.3.2.
184"
HALF OVERLAPPING EMBEDDING STRATEGY,0.4750499001996008,"3.3
Half Overlapping Embedding Strategy
185"
HALF OVERLAPPING EMBEDDING STRATEGY,0.47704590818363274,"We observe that the embedding strategies of Transformers can be mainly classified into two categories:
186"
HALF OVERLAPPING EMBEDDING STRATEGY,0.47904191616766467,"the Non-overlapping [16] and Wide-overlapping [14] embedding strategy, which have no overlap
187"
HALF OVERLAPPING EMBEDDING STRATEGY,0.4810379241516966,"and significant overlap between adjacent patches respectively. Specifically, if the patch size is set to
188"
HALF OVERLAPPING EMBEDDING STRATEGY,0.48303393213572854,"t × h × w, the Non-overlapping and Wide-overlapping embedding strategies extract patches with
189"
HALF OVERLAPPING EMBEDDING STRATEGY,0.48502994011976047,"strides of t × h × w, and 1 × 1 × 1 respectively. Clearly, on the one hand, different stride will
190"
HALF OVERLAPPING EMBEDDING STRATEGY,0.4870259481037924,"significantly impact the number of tokens and further affect the memory requirements during training.
191"
HALF OVERLAPPING EMBEDDING STRATEGY,0.48902195608782434,"On the other hand, the length of the representation will change Params. Both factors influence the
192"
HALF OVERLAPPING EMBEDDING STRATEGY,0.49101796407185627,"final performance of the model. We found that different embedding strategies maintain comparable
193"
HALF OVERLAPPING EMBEDDING STRATEGY,0.4930139720558882,"performance when the following equation is satisfied:
194"
HALF OVERLAPPING EMBEDDING STRATEGY,0.49500998003992014,"D2
D1
= r"
HALF OVERLAPPING EMBEDDING STRATEGY,0.49700598802395207,"t1h1w1
t2h2w2 (11)"
HALF OVERLAPPING EMBEDDING STRATEGY,0.499001996007984,"Here, D1 and D2 respectively represent the length of the embedding representation when patches are
195"
HALF OVERLAPPING EMBEDDING STRATEGY,0.500998003992016,"extracted with strides of t1 × h1 × w1 and t2 × h2 × w2.
196"
HALF OVERLAPPING EMBEDDING STRATEGY,0.5029940119760479,"Hence, we propose a compromise solution - the Half-overlapping embedding strategy - where adjacent
197"
HALF OVERLAPPING EMBEDDING STRATEGY,0.5049900199600799,"patches overlap by half of their area or the stride is set to t × h/2 × w/2. The length of its embedding
198"
HALF OVERLAPPING EMBEDDING STRATEGY,0.5069860279441117,"representation is set by Eq. (11). A detailed performance comparison and analysis of the three
199"
HALF OVERLAPPING EMBEDDING STRATEGY,0.5089820359281437,"different embedding strategies can be found in Section 4.3.1.
200"
EXPERIMENT,0.5109780439121756,"4
Experiment
201"
IMPLEMENTATION DETAILS,0.5129740518962076,"4.1
Implementation Details
202"
IMPLEMENTATION DETAILS,0.5149700598802395,"Training: Consistent with [13], a basic l1 loss is employed to train the networks: ||I0.5 −ˆI0.5||. The
203"
IMPLEMENTATION DETAILS,0.5169660678642715,"training batch size is set to 4, and the cube size of SCubA is set to 2 × 4 × 4. The Adam optimizer
204"
IMPLEMENTATION DETAILS,0.5189620758483033,"[37] is utilized with β1 = 0.9 and β2 = 0.99. The learning rate is initialized to 2e−4, and a Cosine
205"
IMPLEMENTATION DETAILS,0.5209580838323353,"Annealing scheme is adopted over 100 epochs. Both SGuTA and SCubA employ Half-overlapping
206"
IMPLEMENTATION DETAILS,0.5229540918163673,"strategy with patch size setting to 1 × 4 × 4 pixels.
207"
IMPLEMENTATION DETAILS,0.5249500998003992,"Dataset: In this study, we use the Vimeo-90K septuplet training set [38] for training, it includes
208"
IMPLEMENTATION DETAILS,0.5269461077844312,"64,612 seven-frame sequences with a resolution of 448 × 256. We selected the middle frame from
209"
IMPLEMENTATION DETAILS,0.5289421157684631,"each sequence as the ground truth, and pad one blank frame to the beginning and end of the remaining
210"
IMPLEMENTATION DETAILS,0.530938123752495,"six frames. After random cropping, we obtain a video sequence of size 8×3×128×128 as input. We
211"
IMPLEMENTATION DETAILS,0.5329341317365269,"use the data augmentation method of FLAVR [13], which randomly applied horizontal and vertical
212"
IMPLEMENTATION DETAILS,0.5349301397205589,"flips and temporal flips to the input video sequence.
213"
IMPLEMENTATION DETAILS,0.5369261477045908,"The performance of our models is accessed on widely-used datasets, including the Vimeo-90K
214"
IMPLEMENTATION DETAILS,0.5389221556886228,"septuplet test set [38], which comprises 7824 septuplets with a resolution of 448 × 256; the DAVIS
215"
IMPLEMENTATION DETAILS,0.5409181636726547,"dataset [39], containing 2849 triplets with a resolution of 832 × 448; and the SNU-FILM dataset
216"
IMPLEMENTATION DETAILS,0.5429141716566867,"[11], which is classified four categories based on the degree of motion: Easy, Medium, Hard, and
217"
IMPLEMENTATION DETAILS,0.5449101796407185,"Extreme. Each category comprises 310 triplets, primarily with a resolution of 1280 × 720. we
218"
IMPLEMENTATION DETAILS,0.5469061876247505,"transform the DAVIS dataset and SNU-FILM dataset into septuplets while preserving the ground
219"
IMPLEMENTATION DETAILS,0.5489021956087824,"truth to accommodate our network requirements and ensure fairness in comparing various models.
220"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5508982035928144,"4.2
Evaluation against the State of the Arts
221"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5528942115768463,"We conducted a comparative analysis of SGuTA and SCubA with competitive state-of-the-art methods,
222"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5548902195608783,"including SuperSlomo [31], SepConv [8], QVI [30], BMBC [32], CAIN [11], AdaCoF [10], FLAVR
223"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5568862275449101,"[13], VFIT-S [14], VFIT-B [14]. Tab. 1 reports the performance of each model in terms of Params,
224"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5588822355289421,"FLOPs, peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM) on the Vimeo-90K
225"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5608782435129741,"and Davis datasets. Compared with the current SOTA method VFIT on the Vimeo-90K dataset,
226"
EVALUATION AGAINST THE STATE OF THE ARTS,0.562874251497006,"SGuTA achieves a significant performance improvement of 0.58dB with similar Params and FLOPs.
227"
EVALUATION AGAINST THE STATE OF THE ARTS,0.564870259481038,"Moreover, SCubA reduces the Params and FLOPs by 40% and 39%, respectively while achieving a
228"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5668662674650699,"notable performance improvement of 1.08dB. Fig. 3 illustrates the PSNR-FLOPs-Params comparison
229"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5688622754491018,"of these methods, which demonstrated that both SCubA and SGuTA are located in the upper-left
230"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5708582834331337,"region of the figure.
231"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5728542914171657,"Table 1: Quantitative comparisons on the Vimeo-90K
and DAVIS datasets."
EVALUATION AGAINST THE STATE OF THE ARTS,0.5748502994011976,"Methods
Params FLOPs Vimeo-90K
DAVIS
(M)
(G)"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5768463073852296,"SuperSloMo [31] 39.61
49.81
32.90/0.957 25.65/0.857
SepConv [8]
21.68
25.00
33.60/0.944 26.21/0.857
QVI [30]
29.21
72.93
35.15/0.971 27.17/0.874
BMBC [32]
11.01
175.27 34.76/0.965 26.42/0.868
CAIN [11]
42.78
43.50
34.83/0.970 27.21/0.873
AdaCoF [10]
21.84
24.83
35.40/0.971 26.49/0.866
FLAVR [13]
42.06
133.14 36.30/0.975 27.44/0.874
VFIT-S [14]
7.54
40.09
36.48/0.976 27.92/0.885
VFIT-B [14]
29.08
85.03
36.96/0.978 28.09/0.888
SGuTA
27.60
73.55
37.54/0.980 28.39/0.892
SCubA
17.30
51.71
38.04/0.981 28.86/0.899"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5788423153692615,"Figure 3: PSNR-FLOPS-Params compar-
isons on Vimeo-90K dataset"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5808383233532934,"Tab. 2 reports the performance of each model on the SNU-FILM dataset. Compared with the third-
232"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5828343313373253,"best model, SGuTA and SCubA achieve an average improvement of 0.67dB and 0.96dB, respectively,
233"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5848303393213573,"and a remarkable improvement of 1.14dB and 1.59dB in Hard scenario. This indicates that SGuTA
234"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5868263473053892,"and SCubA fully utilize the Transformer’s ability to establish long-range correlations and prove their
235"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5888223552894212,"capability to handle challenging large-motion scenarios.
236"
EVALUATION AGAINST THE STATE OF THE ARTS,0.590818363273453,"We provide qualitative results comparing our SGuTA and SCubA models to FLAVR [13] and VFIT
237"
EVALUATION AGAINST THE STATE OF THE ARTS,0.592814371257485,"[14]. As shown in Fig. 4. The first two rows fully demonstrate the ability of SGuTA and SCubA to
238"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5948103792415169,"provide accurate motion estimation (please carefully compare the rotation of the wheels and balls
239"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5968063872255489,"with the ground truth; other methods fail to restore the accurate rotation angles). The third row shows
240"
EVALUATION AGAINST THE STATE OF THE ARTS,0.5988023952095808,"the performance of various models in non-rigid motion scenarios, where only SCubA clearly restores
241"
EVALUATION AGAINST THE STATE OF THE ARTS,0.6007984031936128,"all the letters. In the fourth row, SGuTA and SCubA reconstruct clearer texture details. The last two
242"
EVALUATION AGAINST THE STATE OF THE ARTS,0.6027944111776448,rows again demonstrate the strong ability of our models to handle large motion scenarios.
EVALUATION AGAINST THE STATE OF THE ARTS,0.6047904191616766,Table 2: Quantitative comparisons on the SNU-FILM datasets.
EVALUATION AGAINST THE STATE OF THE ARTS,0.6067864271457086,"Methods
SNU-FILM"
EVALUATION AGAINST THE STATE OF THE ARTS,0.6087824351297405,"Easy
Medium
Hard
Extreme"
EVALUATION AGAINST THE STATE OF THE ARTS,0.6107784431137725,"SuperSloMo [31]
37.28/0.986
33.80/0.973
28.98/0.925
24.15/0.845
SepConv [8]
39.41/0.990
34.97/0.976
29.36/0.925
24.31/0.845
BMBC [32]
39.88/0.990
35.30/0.977
29.31/0.927
23.92/0.843
CAIN [11]
39.92/0.990
35.61/0.978
29.92/0.929
24.81/0.851
AdaCoF [10]
40.08/0.990
35.92/0.980
30.36/0.935
25.16/0.860
FLAVR [13]
40.43/0.991
36.36/0.981
30.86/0.942
25.41/0.867
VFIT-S [14]
40.43/0.991
36.52/0.983
31.07/0.946
25.69/0.870
VFIT-B [14]
40.53/0.991
36.53/0.982
31.03/0.945
25.73/0.871
SGuTA
40.79/0.991
37.41/0.985
32.17/0.957
26.15/0.880
SCubA
40.90/0.992
37.78/0.986
32.62/0.960
26.37/0.884 243"
ABLATION STUDY,0.6127744510978044,"4.3
Ablation Study
244"
EMBEDDING STRATEGY,0.6147704590818364,"4.3.1
Embedding Strategy
245"
EMBEDDING STRATEGY,0.6167664670658682,"In this section, we explore the relationship between various embedding strategies and the length of
246"
EMBEDDING STRATEGY,0.6187624750499002,"embedding vectors D. Taking SCubA as an example, given the input video size of T × H × W =
247"
EMBEDDING STRATEGY,0.6207584830339321,"8 × 128 × 128, we set Ns = 2 and the patch size to 1 × 4 × 4. The Wide, Half and Non-Overlapping
248"
EMBEDDING STRATEGY,0.6227544910179641,"Strategies extract patches with stride of 1 × 1 × 1, 1 × 2 × 2, 1 × 4 × 4, respectively. As shown in
249"
EMBEDDING STRATEGY,0.624750499001996,"Tab. 3, changing the Wide-Overlapping Strategy to the Non-Overlapping Strategy while keeping
250"
EMBEDDING STRATEGY,0.626746506986028,"the size of D = 32 the same can reduce FLOPs and memory usage, but lower the performance.
251"
EMBEDDING STRATEGY,0.6287425149700598,"Overlayed
FLAVR
VFIT-S
VFIT-B
SGuTA
SCubA
GT"
EMBEDDING STRATEGY,0.6307385229540918,"Figure 4: Qualitative comparisons against state-of-the-art VFI methods. Both SGuTA and SCubA
outperform others in providing precise motion estimation, clear texture details, handling non-rigid
motion and large motion scenarios. Note the rotational position of the wheel and the ball when
comparing these methods in the first two rows."
EMBEDDING STRATEGY,0.6327345309381237,"Strategies that satisfy Eq. (11) perform similarly, thus we can use this equation to balance Params,
252"
EMBEDDING STRATEGY,0.6347305389221557,"FLOPs, and memory usage. The rationale behind this phenomenon is that the correlation between
253"
EMBEDDING STRATEGY,0.6367265469061876,"adjacent patches exhibits redundancy under the Wide-Overlapping Strategy, whereas it manifests
254"
EMBEDDING STRATEGY,0.6387225548902196,"sparsity in the Non-Overlapping Strategy. Consequently, the latter requires a lengthier representation
255"
EMBEDDING STRATEGY,0.6407185628742516,"to restore the comparable performance. The correlation of Half-Overlapping Strategy lies between
256"
EMBEDDING STRATEGY,0.6427145708582834,"the previous two strategies, and the appropriate overlapping region can provide some inductive bias,
257"
EMBEDDING STRATEGY,0.6447105788423154,"such as the relative positional information, to MSA. It is worth noting that due to the non-linear nature
258"
EMBEDDING STRATEGY,0.6467065868263473,"of Eq. (11), the Half-Overlapping Strategy exhibits a distinct feature of high returns on investment,
259"
EMBEDDING STRATEGY,0.6487025948103793,"with lower Params, FLOPs, and memory requirements compared to the average values of the Wide
260"
EMBEDDING STRATEGY,0.6506986027944112,Overlapping Strategy and Non-Overlapping Strategy at the same performance level.
EMBEDDING STRATEGY,0.6526946107784432,Table 3: Quantitative comparisons on different embedding strategy.
EMBEDDING STRATEGY,0.654690618762475,"Embedding Strategy
(Patch Number) ×D Params FLOPs Memory Usage Vimeo-90K
(M)
(G)
(Gi)"
EMBEDDING STRATEGY,0.656686626746507,"Wide-Overlapping
(8 × 128 × 128) × 32
2.99
39.60
23.78
36.03/0.973
Non-Overlapping
(8 × 32 × 32) × 32
2.99
2.99
3.15
33.64/0.956
Non-Overlapping
(8 × 32 × 32) × 128
45.24
2.99
7.75
36.04/0.973
Half-Overlapping
(8 × 64 × 64) × 64
11.53
11.53
12.38
36.08/0.973 261"
SELF-ATTENTION MECHANISM,0.6586826347305389,"4.3.2
Self-Attention Mechanism
262"
SELF-ATTENTION MECHANISM,0.6606786427145709,"In this section, We first replace all MSA blocks with two layers of 3D ResBlocks [40] to enable a com-
263"
SELF-ATTENTION MECHANISM,0.6626746506986028,"parative assessment of CNN-based methods with other Transformer-based methods. Subsequently, a
264"
SELF-ATTENTION MECHANISM,0.6646706586826348,"thorough evaluation of the performance of different MSAs is conducted. The scrutinized MSA-based
265"
SELF-ATTENTION MECHANISM,0.6666666666666666,"methods are enumerated as follows: 1) Baseline where the MSA modules are all removed from the
266"
SELF-ATTENTION MECHANISM,0.6686626746506986,"multi-scale Transformer. 2) STS MSA [14] and 3) Sep-STS MSA [14] replaces our MSA modules
267"
SELF-ATTENTION MECHANISM,0.6706586826347305,"with STS blocks and Sep-STS blocks [14] respectively. 4) Feature MSA [21] obtain self-attention
268"
SELF-ATTENTION MECHANISM,0.6726546906187625,"from Eq. (8). 5) SGuTA and 6) SCubA are the methods proposed in this paper. Besides, Global
269"
SELF-ATTENTION MECHANISM,0.6746506986027944,"MSA [16] employs Eq. (7) for self-attention, but its performance is unreported due to the excessively
270"
SELF-ATTENTION MECHANISM,0.6766467065868264,"high computational complexity (587.93G) and memory requirements. To ensure fairness, all methods
271"
SELF-ATTENTION MECHANISM,0.6786427145708582,"are configured with Ns = 2 and adopt the half overlapping embedding strategy with D = 64.
272"
SELF-ATTENTION MECHANISM,0.6806387225548902,"Because the differences between models can be distinguished at the early stages of training, we report
273"
SELF-ATTENTION MECHANISM,0.6826347305389222,"performance for all models trained for 20 epochs.
274"
SELF-ATTENTION MECHANISM,0.6846307385229541,"As shown in Tab. 4, on the one hand, compared to the 3D ResBlock method based on CNN,
275"
SELF-ATTENTION MECHANISM,0.6866267465069861,"Transformers benefit from their ability to establish long-range dependencies, achieving improved
276"
SELF-ATTENTION MECHANISM,0.688622754491018,"performance with lower Params and FLOPs. On the other hand, compared to the Baseline, Feature
277"
SELF-ATTENTION MECHANISM,0.6906187624750499,"MSA only provides a modest improvement in PSNR by 0.06dB, indicating that the self-attention for
278"
SELF-ATTENTION MECHANISM,0.6926147704590818,"features has limited benefit for VFI task. STS MSA and Sep-STS MSA show PSNR improvements
279"
SELF-ATTENTION MECHANISM,0.6946107784431138,"of 0.37dB and 0.60dB, respectively, with Sep-STS MSA acting similarly to depth-wise separable
280"
SELF-ATTENTION MECHANISM,0.6966067864271457,"convolution [34], resulting in a lighter and more efficient STS-MSA. Compared to Feature MSA,
281"
SELF-ATTENTION MECHANISM,0.6986027944111777,"SGuTA significantly improves PSNR by 0.54dB, demonstrating the effectiveness of SGuTA in
282"
SELF-ATTENTION MECHANISM,0.7005988023952096,"establishing correlations between space and time. SCubA leverages the shifted-cube mechanism to
283"
SELF-ATTENTION MECHANISM,0.7025948103792415,"fully exploit the power of local attention, achieving the best performance among all MSAs."
SELF-ATTENTION MECHANISM,0.7045908183632734,"Table 4: Ablation study of different MSA
Methods
Params (M) FLOPs (G) Vimeo-90K"
D RESBLOCK,0.7065868263473054,"3D ResBlock
17.50
57.94
34.82/0.966
Baseline
7.84
5.12
35.33/0.969
Feature MSA
8.76
22.11
35.39/0.970
STS MSA
11.53
37.97
35.70/0.972
Sep-STS MSA
10.61
29.76
35.91/0.973
SGuTA
18.40
49.04
35.93/0.973
SCubA
11.53
34.48
36.08/0.973"
D RESBLOCK,0.7085828343313373,Table 5: Ablation study of stage number
D RESBLOCK,0.7105788423153693,"Methods Ns Params FLOPs Vimeo-90K
(M)
(G)"
D RESBLOCK,0.7125748502994012,"SGuTA
1
9.20
24.52
35.65/0.971
2
18.40
49.04
37.28/0.979
3
27.60
73.55
37.54/0.980"
D RESBLOCK,0.7145708582834331,"SCubA
1
5.77
17.24
36.72/0.976
2
11.53
34.48
37.48/0.980
3
17.30
51.71
38.04/0.981 284"
STAGE,0.716566866267465,"4.3.3
Stage
285"
STAGE,0.718562874251497,"In this section, we explore the impact of the number of cascaded Multi-scale Transformers Ns. Due
286"
STAGE,0.720558882235529,"to concerns regarding Params and FLOPs, we only consider the case when Ns ≤3. The results are
287"
STAGE,0.7225548902195609,"presented in Tab. 5, where it can be observed that when Ns = 3, both SGuTA and SCubA perform
288"
STAGE,0.7245508982035929,"the best. Additionally, it is worth noting that when Ns = 1, compared to VFIT-S, SCubA achieves a
289"
STAGE,0.7265469061876247,"PSNR improvement of 0.24dB while reducing the Params and FLOPs by 23% and 58%, respectively.
290"
STAGE,0.7285429141716567,"When Ns = 2, compared to VFIT-B, SCubA achieves a PSNR improvement of 0.52dB with 43% of
291"
STAGE,0.7305389221556886,"its Params and 40% of the FLOPs.
292"
CONCLUSIONS,0.7325349301397206,"5
Conclusions
293"
CONCLUSIONS,0.7345309381237525,"In this paper, we employ two different Transformers, SGuTA and SCubA, to the VFI task. SGuTA is
294"
CONCLUSIONS,0.7365269461077845,"designed to establish intrinsic connections between video spatial and temporal information, while
295"
CONCLUSIONS,0.7385229540918163,"SCubA employs a 3D local self-attention mechanism. Both methods are integrated into a multi-stage
296"
CONCLUSIONS,0.7405189620758483,"multi-scale framework. Compared to previous state-of-the-arts, extensive experiments show that our
297"
CONCLUSIONS,0.7425149700598802,"methods achieve the best and second-best performance on multiple benchmarks, and particularly excel
298"
CONCLUSIONS,0.7445109780439122,"in handling large motion and providing accurate motion estimation. Additionally, we summarized the
299"
CONCLUSIONS,0.7465069860279441,"regularity between the patch extraction stride and the length representation when different embedding
300"
CONCLUSIONS,0.7485029940119761,"strategies maintain comparable performance. We will further verify the universality of this regularity,
301"
CONCLUSIONS,0.7504990019960079,"as well as extend our model to multi-frame interpolation in future work.
302"
REFERENCES,0.7524950099800399,"References
303"
REFERENCES,0.7544910179640718,"[1] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah Snavely, Car-
304"
REFERENCES,0.7564870259481038,"los Hernández, Sameer Agarwal, and Steven M Seitz. Jump: virtual reality video. ACM
305"
REFERENCES,0.7584830339321357,"Transactions on Graphics (TOG), 35(6):1–13, 2016.
306"
REFERENCES,0.7604790419161677,"[2] Chao-Yuan Wu, Nayan Singhal, and Philipp Krahenbuhl. Video compression through image
307"
REFERENCES,0.7624750499001997,"interpolation. In Proceedings of the European conference on computer vision (ECCV), pages
308"
REFERENCES,0.7644710578842315,"416–431, 2018.
309"
REFERENCES,0.7664670658682635,"[3] Kai-Chieh Yang, Ai-Mei Huang, Truong Q Nguyen, Clark C Guest, and Pankaj K Das. A new
310"
REFERENCES,0.7684630738522954,"objective quality metric for frame interpolation used in video compression. IEEE transactions
311"
REFERENCES,0.7704590818363274,"on broadcasting, 54(3):680–11, 2008.
312"
REFERENCES,0.7724550898203593,"[4] Jean Bégaint, Franck Galpin, Philippe Guillotel, and Christine Guillemot. Deep frame interpo-
313"
REFERENCES,0.7744510978043913,"lation for video compression. In DCC 2019-Data Compression Conference, pages 1–10. IEEE,
314"
REFERENCES,0.7764471057884231,"2019.
315"
REFERENCES,0.7784431137724551,"[5] Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang.
316"
REFERENCES,0.780439121756487,"Depth-aware video frame interpolation. In Proceedings of the IEEE/CVF Conference on
317"
REFERENCES,0.782435129740519,"Computer Vision and Pattern Recognition, pages 3703–3712, 2019.
318"
REFERENCES,0.7844311377245509,"[6] Youjian Zhang, Chaoyue Wang, and Dacheng Tao. Video frame interpolation without temporal
319"
REFERENCES,0.7864271457085829,"priors. Advances in Neural Information Processing Systems, 33:13308–13318, 2020.
320"
REFERENCES,0.7884231536926147,"[7] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive convolution.
321"
REFERENCES,0.7904191616766467,"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
322"
REFERENCES,0.7924151696606786,"670–679, 2017.
323"
REFERENCES,0.7944111776447106,"[8] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable
324"
REFERENCES,0.7964071856287425,"convolution. In Proceedings of the IEEE international conference on computer vision, pages
325"
REFERENCES,0.7984031936127745,"261–270, 2017.
326"
REFERENCES,0.8003992015968064,"[9] Xianhang Cheng and Zhenzhong Chen. Video frame interpolation via deformable separable
327"
REFERENCES,0.8023952095808383,"convolution. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages
328"
REFERENCES,0.8043912175648703,"10607–10614, 2020.
329"
REFERENCES,0.8063872255489022,"[10] Hyeongmin Lee, Taeoh Kim, Tae-young Chung, Daehyun Pak, Yuseok Ban, and Sangyoun Lee.
330"
REFERENCES,0.8083832335329342,"Adacof: Adaptive collaboration of flows for video frame interpolation. In Proceedings of the
331"
REFERENCES,0.810379241516966,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5316–5325, 2020.
332"
REFERENCES,0.812375249500998,"[11] Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, and Kyoung Mu Lee. Channel
333"
REFERENCES,0.8143712574850299,"attention is all you need for video frame interpolation. In Proceedings of the AAAI Conference
334"
REFERENCES,0.8163672654690619,"on Artificial Intelligence, volume 34, pages 10663–10671, 2020.
335"
REFERENCES,0.8183632734530938,"[12] Whan Choi, Yeong Jun Koh, and Chang-Su Kim. Multi-scale warping for video frame interpo-
336"
REFERENCES,0.8203592814371258,"lation. IEEE Access, 9:150470–150479, 2021.
337"
REFERENCES,0.8223552894211577,"[13] Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, and Du Tran. Flavr: Flow-agnostic
338"
REFERENCES,0.8243512974051896,"video representations for fast frame interpolation. In Proceedings of the IEEE/CVF Winter
339"
REFERENCES,0.8263473053892215,"Conference on Applications of Computer Vision, pages 2071–2082, 2023.
340"
REFERENCES,0.8283433133732535,"[14] Zhihao Shi, Xiangyu Xu, Xiaohong Liu, Jun Chen, and Ming-Hsuan Yang. Video frame
341"
REFERENCES,0.8303393213572854,"interpolation transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision
342"
REFERENCES,0.8323353293413174,"and Pattern Recognition, pages 17482–17491, 2022.
343"
REFERENCES,0.8343313373253493,"[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
344"
REFERENCES,0.8363273453093812,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
345"
REFERENCES,0.8383233532934131,"processing systems, 30, 2017.
346"
REFERENCES,0.8403193612774451,"[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
347"
REFERENCES,0.8423153692614771,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
348"
REFERENCES,0.844311377245509,"An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
349"
REFERENCES,0.846307385229541,"arXiv:2010.11929, 2020.
350"
REFERENCES,0.8483033932135728,"[17] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spa-
351"
REFERENCES,0.8502994011976048,"tiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Proceedings
352"
REFERENCES,0.8522954091816367,"of the European conference on computer vision (ECCV), pages 305–321, 2018.
353"
REFERENCES,0.8542914171656687,"[18] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
354"
REFERENCES,0.8562874251497006,"Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
355"
REFERENCES,0.8582834331337326,"of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.
356"
REFERENCES,0.8602794411177644,"[19] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-end transformer model for 3d object
357"
REFERENCES,0.8622754491017964,"detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
358"
REFERENCES,0.8642714570858283,"pages 2906–2917, 2021.
359"
REFERENCES,0.8662674650698603,"[20] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
360"
REFERENCES,0.8682634730538922,"Sergey Zagoruyko. End-to-end object detection with transformers. In Computer Vision–ECCV
361"
REFERENCES,0.8702594810379242,"2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16,
362"
REFERENCES,0.872255489021956,"pages 213–229. Springer, 2020.
363"
REFERENCES,0.874251497005988,"[21] Yuanhao Cai, Jing Lin, Zudi Lin, Haoqian Wang, Yulun Zhang, Hanspeter Pfister, Radu
364"
REFERENCES,0.8762475049900199,"Timofte, and Luc Van Gool. Mst++: Multi-stage spectral-wise transformer for efficient spectral
365"
REFERENCES,0.8782435129740519,"reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
366"
REFERENCES,0.8802395209580839,"Recognition, pages 745–755, 2022.
367"
REFERENCES,0.8822355289421158,"[22] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang
368"
REFERENCES,0.8842315369261478,"Li. Uformer: A general u-shaped transformer for image restoration. In Proceedings of the
369"
REFERENCES,0.8862275449101796,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17683–17693, 2022.
370"
REFERENCES,0.8882235528942116,"[23] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir:
371"
REFERENCES,0.8902195608782435,"Image restoration using swin transformer. In Proceedings of the IEEE/CVF international
372"
REFERENCES,0.8922155688622755,"conference on computer vision, pages 1833–1844, 2021.
373"
REFERENCES,0.8942115768463074,"[24] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and
374"
REFERENCES,0.8962075848303394,"Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In
375"
REFERENCES,0.8982035928143712,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
376"
REFERENCES,0.9001996007984032,"5728–5739, 2022.
377"
REFERENCES,0.9021956087824351,"[25] Liad Pollak Zuckerman, Eyal Naor, George Pisha, Shai Bagon, and Michal Irani. Across
378"
REFERENCES,0.9041916167664671,"scales and across dimensions: Temporal super-resolution using deep internal learning. In
379"
REFERENCES,0.906187624750499,"Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
380"
REFERENCES,0.908183632734531,"Proceedings, Part VII 16, pages 52–68. Springer, 2020.
381"
REFERENCES,0.9101796407185628,"[26] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin
382"
REFERENCES,0.9121756487025948,"transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern
383"
REFERENCES,0.9141716566866267,"recognition, pages 3202–3211, 2022.
384"
REFERENCES,0.9161676646706587,"[27] Simone Meyer, Oliver Wang, Henning Zimmer, Max Grosse, and Alexander Sorkine-Hornung.
385"
REFERENCES,0.9181636726546906,"Phase-based frame interpolation for video. In Proceedings of the IEEE conference on computer
386"
REFERENCES,0.9201596806387226,"vision and pattern recognition, pages 1410–1418, 2015.
387"
REFERENCES,0.9221556886227545,"[28] Simone Meyer, Abdelaziz Djelouah, Brian McWilliams, Alexander Sorkine-Hornung, Markus
388"
REFERENCES,0.9241516966067864,"Gross, and Christopher Schroers. Phasenet for video frame interpolation. In Proceedings of the
389"
REFERENCES,0.9261477045908184,"IEEE Conference on Computer Vision and Pattern Recognition, pages 498–507, 2018.
390"
REFERENCES,0.9281437125748503,"[29] Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, and Aseem Agarwala. Video frame
391"
REFERENCES,0.9301397205588823,"synthesis using deep voxel flow. In Proceedings of the IEEE international conference on
392"
REFERENCES,0.9321357285429142,"computer vision, pages 4463–4471, 2017.
393"
REFERENCES,0.9341317365269461,"[30] Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and Ming-Hsuan Yang. Quadratic video
394"
REFERENCES,0.936127744510978,"interpolation. Advances in Neural Information Processing Systems, 32, 2019.
395"
REFERENCES,0.93812375249501,"[31] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik Learned-Miller, and
396"
REFERENCES,0.9401197604790419,"Jan Kautz. Super slomo: High quality estimation of multiple intermediate frames for video
397"
REFERENCES,0.9421157684630739,"interpolation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
398"
REFERENCES,0.9441117764471058,"pages 9000–9008, 2018.
399"
REFERENCES,0.9461077844311377,"[32] Junheum Park, Keunsoo Ko, Chul Lee, and Chang-Su Kim. Bmbc: Bilateral motion estimation
400"
REFERENCES,0.9481037924151696,"with bilateral cost volume for video interpolation. In Computer Vision–ECCV 2020: 16th
401"
REFERENCES,0.9500998003992016,"European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16, pages
402"
REFERENCES,0.9520958083832335,"109–125. Springer, 2020.
403"
REFERENCES,0.9540918163672655,"[33] Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya Jia. Video frame interpolation
404"
REFERENCES,0.9560878243512974,"with transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
405"
REFERENCES,0.9580838323353293,"Recognition, pages 3532–3542, 2022.
406"
REFERENCES,0.9600798403193613,"[34] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias
407"
REFERENCES,0.9620758483033932,"Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural
408"
REFERENCES,0.9640718562874252,"networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
409"
REFERENCES,0.9660678642714571,"[35] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for
410"
REFERENCES,0.9680638722554891,"video understanding? In ICML, volume 2, page 4, 2021.
411"
REFERENCES,0.9700598802395209,"[36] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,
412"
REFERENCES,0.9720558882235529,"Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In Proceedings
413"
REFERENCES,0.9740518962075848,"of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12299–12310,
414"
REFERENCES,0.9760479041916168,"2021.
415"
REFERENCES,0.9780439121756487,"[37] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
416"
REFERENCES,0.9800399201596807,"arXiv:1412.6980, 2014.
417"
REFERENCES,0.9820359281437125,"[38] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. Video enhancement
418"
REFERENCES,0.9840319361277445,"with task-oriented flow. International Journal of Computer Vision, 127:1106–1125, 2019.
419"
REFERENCES,0.9860279441117764,"[39] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and
420"
REFERENCES,0.9880239520958084,"Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video
421"
REFERENCES,0.9900199600798403,"object segmentation. In Proceedings of the IEEE conference on computer vision and pattern
422"
REFERENCES,0.9920159680638723,"recognition, pages 724–732, 2016.
423"
REFERENCES,0.9940119760479041,"[40] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
424"
REFERENCES,0.9960079840319361,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
425"
REFERENCES,0.998003992015968,"pages 770–778, 2016.
426"
