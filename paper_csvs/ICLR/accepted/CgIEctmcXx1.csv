Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001937984496124031,"Frequently, population studies feature pyramidally-organized data represented us-
ing Hierarchical Bayesian Models (HBM) enriched with plates. These models
can become prohibitively large in settings such as neuroimaging, where a sample
is composed of a functional MRI signal measured on 300 brain locations, across 4
measurement sessions, and 30 subjects, resulting in around 1 million latent param-
eters. Such high dimensionality hampers the usage of modern, expressive flow-
based techniques. To infer parameter posterior distributions in this challenging
class of problems, we designed a novel methodology that automatically produces
a variational family dual to a target HBM. This variational family, represented as a
neural network, consists in the combination of an attention-based hierarchical en-
coder feeding summary statistics to a set of normalizing flows. Our automatically-
derived neural network exploits exchangeability in the plate-enriched HBM and
factorizes its parameter space. The resulting architecture reduces by orders of
magnitude its parameterization with respect to that of a typical flow-based rep-
resentation, while maintaining expressivity. Our method performs inference on
the specified HBM in an amortized setup: once trained, it can readily be applied
to a new data sample to compute the parameters’ full posterior. We demonstrate
the capability and scalability of our method on simulated data, as well as a chal-
lenging high-dimensional brain parcellation experiment. We also open up several
questions that lie at the intersection between normalizing flows, SBI, structured
Variational Inference, and inference amortization."
INTRODUCTION,0.003875968992248062,"1
INTRODUCTION"
INTRODUCTION,0.005813953488372093,"Inference aims at obtaining the posterior distribution p(θ|X) of latent model parameters θ given the
observed data X. In the context of Hierarchical Bayesian Models (HBM), p(θ|X) usually has no
known analytical form, and can be of a complex shape -different from the prior’s (Gelman et al.,
2004). Modern normalizing-flows based techniques -universal density estimators- can overcome
this difficulty (Papamakarios et al., 2019a; Ambrogioni et al., 2021b). Yet, in setups such as neu-
roimaging, featuring HBMs representing large population studies (Kong et al., 2019; Bonkhoff et al.,
2021), the dimensionality of θ can go over the million. This high dimensionality hinders the usage
of normalizing flows, since their parameterization usually scales quadratically with the size of the
parameter space (e.g. Dinh et al., 2017; Papamakarios et al., 2018; Grathwohl et al., 2018). Popu-
lation studies with large dimensional features are therefore inaccessible to off-the-shelf flow-based
techniques and their superior expressivity. This can in turn lead to complex, problem-specific deriva-
tions: for instance Kong et al. (2019) rely on a manually-derived Expectation Maximization (EM)
technique. Such an analytical complexity constitutes a strong barrier to entry, and limits the wide
and fruitful usage of Bayesian modelling in fields such as neuroimaging. Our main aim is to meet
that experimental need: how can we derive a technique both automatic and efficient in the context
of very large, hierarchically-organised data?"
INTRODUCTION,0.007751937984496124,"Approximate inference features a large corpus of methods including Monte Carlo methods (Koller &
Friedman, 2009) and Variational Auto Encoders (Zhang et al., 2019). We take particular inspiration E1 E2"
INTRODUCTION,0.009689922480620155,"Dual
Amortized
Variational"
INTRODUCTION,0.011627906976744186,Family
INTRODUCTION,0.013565891472868217,Simulated Data
INTRODUCTION,0.015503875968992248,Automatic Derivation
INTRODUCTION,0.01744186046511628,Template
INTRODUCTION,0.01937984496124031,Ground
INTRODUCTION,0.02131782945736434,Generative
INTRODUCTION,0.023255813953488372,Pyramidal HBM
INTRODUCTION,0.025193798449612403,ADAVI working principle
INTRODUCTION,0.027131782945736434,"Figure 1: Automatic Dual Amortized Variational Inference (ADAVI) working principle. On the left
is a generative HBM, with 2 alternative representations: a graph template featuring 2 plates P0, P1
of cardinality 2, and the equivalent ground graph depicting a typical pyramidal shape. We note B1 =
Card(P1) the batch shape due to the cardinality of P1. The model features 3 latent RV λ, κ and
Γ = [γ1, γ2], and one observed RV X = [[x1,1, x1,2], [x2,1, x2,2]]. We analyse automatically the
structure of the HBM to produce its dual amortized variational family (on the right). The hierarchical
encoder HE processes the observed data X through 2 successive set transformers ST to produce
encodings E aggregating summary statistics at different hierarchies. Those encodings are then used
to condition density estimators -the combination of a normalizing flow F and a link function l-
producing the variational distributions for each latent RV."
INTRODUCTION,0.029069767441860465,"from the field of Variational Inference (VI) (Blei et al., 2017), deemed to be most adapted to large
parameter spaces. In VI, the experimenter posits a variational family Q so as to approximate q(θ) ≈
p(θ|X). In practice, deriving an expressive, yet computationally attractive variational family can
be challenging (Blei et al., 2017). This triggered a trend towards the derivation of automatic VI
techniques (Kucukelbir et al., 2016; Ranganath et al., 2013; Ambrogioni et al., 2021b). We follow
that logic and present a methodology that automatically derives a variational family Q. In Fig. 1,
from the HBM on the left we derive automatically a neural network architecture on the right. We aim
at deriving our variational family Q in the context of amortized inference (Rezende & Mohamed,
2016; Cranmer et al., 2020). Amortization is usually obtained at the cost of an amortization gap from
the true posterior, that accumulates on top of a approximation gap dependent on the expressivity of
the variational family Q (Cremer et al., 2018). However, once an initial training overhead has been
“paid for”, amortization means that our technique can be applied to a any number of data points to
perform inference in a few seconds."
INTRODUCTION,0.031007751937984496,"Due to the very large parameter spaces presented above, our target applications aren’t amenable to
the generic flow-based techniques described in Cranmer et al. (2020) or Ambrogioni et al. (2021b).
We therefore differentiate ourselves in exploiting the invariance of the problem not only through the
design of an adapted encoder, but down to the very architecture of our density estimator. Specifi-
cally, we focus on the inference problem for Hierarchical Bayesian Models (HBMs) (Gelman et al.,
2004; Rodrigues et al., 2021). The idea to condition the architecture of a density estimator by an
analysis of the dependency structure of an HBM has been studied in (Wehenkel & Louppe, 2020;
Weilbach et al., 2020), in the form of the masking of a single normalizing flow. With Ambrogioni
et al. (2021b), we instead share the idea to combine multiple separate flows. More generally, our
static analysis of a generative model can be associated with structured VI (Hoffman & Blei, 2014;
Ambrogioni et al., 2021a;b). Yet our working principles are rather orthogonal: structured VI usually
aims at exploiting model structure to augment the expressivity of a variational family, whereas we
aim at reducing its parameterization."
INTRODUCTION,0.03294573643410853,"Our objective is therefore to derive an automatic methodology that takes as input a generative HBM
and generates a dual variational family able to perform amortized parameter inference. This vari-
ational family exploits the exchangeability in the HBM to reduce its parameterization by orders
of magnitude compared to generic methods (Papamakarios et al., 2019b; Greenberg et al., 2019;"
INTRODUCTION,0.03488372093023256,"Ambrogioni et al., 2021b).
Consequently, our method can be applied in the context of large,
pyramidally-structured data, a challenging setup inaccessible to existing flow-based methods and
their superior expressivity. We apply our method to such a large pyramidal setup in the context
of neuroimaging (section 3.5), but demonstrate the benefit of our method beyond that scope. Our
general scheme is visible in Fig. 1, a figure that we will explain throughout the course of the next
section."
METHODS,0.03682170542635659,"2
METHODS"
PYRAMIDAL BAYESIAN MODELS,0.03875968992248062,"2.1
PYRAMIDAL BAYESIAN MODELS"
PYRAMIDAL BAYESIAN MODELS,0.040697674418604654,"We are interested in experimental setups modelled using plate-enriched Hierarchical Bayesian Mod-
els (HBMs) (Kong et al., 2019; Bonkhoff et al., 2021). These models feature independent sampling
from a common conditional distribution at multiple levels, translating the graphical notion of plates
(Gilks et al., 1994). This nested structure, combined with large measurements -such as the ones in
fMRI- can result in massive latent parameter spaces. For instance the population study in Kong et al.
(2019) features multiple subjects, with multiple measures per subject, and multiple brain vertices per
measure, for a latent space of around 0.4 million parameters. Our method aims at performing infer-
ence in the context of those large plate-enriched HBMs."
PYRAMIDAL BAYESIAN MODELS,0.04263565891472868,"Such HBMs can be represented with Directed Acyclic Graphs (DAG) templates (Koller & Fried-
man, 2009) with vertices -corresponding to RVs- {θi}i=0...L and plates {Pp}p=0...P . We denote
as Card(P) the -fixed- cardinality of the plate P, i.e. the number of independent draws from a
common conditional distribution it corresponds to. In a template DAG, a given RV θ can belong to
multiple plates Ph, . . . PP . When grounding the template DAG into a ground graph -instantiating
the repeated structure symbolized by the plates P- θ would correspond to multiple RVs of similar
parametric form {θih,...,iP }, with ih = 1 . . . Card(Ph), . . . , iP = 1 . . . Card(PP ). This equiva-
lence visible on the left on Fig. 1, where the template RV Γ corresponds to the ground RVs [γ1, γ2].
We wish to exploit this plate-induced exchangeability."
PYRAMIDAL BAYESIAN MODELS,0.044573643410852716,"We define the sub-class of models we specialize upon as pyramidal models, which are plate-enriched
DAG templates with the 2 following differentiating properties. First, we consider a single stack of
the plates P0, . . . , PP . This means that any RV θ belonging to plate Pp also belongs to plates
{Pq}q>p. We thus don’t treat in this work the case of colliding plates (Koller & Friedman, 2009).
Second, we consider a single observed RV θ0, with observed value X, belonging to the plate P0
(with no other -latent- RV belonging to P0). The obtained graph follows a typical pyramidal struc-
ture, with the observed RV at the basis of the pyramid, as seen in Fig. 1. This figure features 2 plates
P0 and P1, the observed RV is X, at the basis of the pyramid, and latent RVs are Γ, λ and κ at upper
levels of the pyramid. Pyramidal HBMs delineate models that typically arise as part of population
studies -for instance in neuroimaging- featuring a nested group structure and data observed at the
subject level only (Kong et al., 2019; Bonkhoff et al., 2021)."
PYRAMIDAL BAYESIAN MODELS,0.046511627906976744,"The fact that we consider a single pyramid of plates allows us to define the hierarchy of an RV θi
denoted Hier(θi). An RV’s hierarchy is the level of the pyramid it is placed at. Due to our pyramidal
structure, the observed RV will systematically be at hierarchy 0 and latent RVs at hierarchies > 0.
For instance, in the example in Fig. 1 the observed RV X is at hierarchy 0, Γ is at hierarchy 1 and
both λ and κ are at hierarchy 2."
PYRAMIDAL BAYESIAN MODELS,0.04844961240310078,"Our methodology is designed to process generative models whose dependency structure follows a
pyramidal graph, and to scale favorably when the plate cardinality in such models augments. Given
the observed data X, we wish to obtain the posterior density for latent parameters θ1, . . . , θL,
exploiting the exchangeability induced by the plates P0, . . . , PP ."
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.050387596899224806,"2.2
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.05232558139534884,"In this section, we derive our main methodological contribution. We aim at obtaining posterior
distributions for a generative model of pyramidal structure. For this purpose, we construct a family
of variational distributions Q dual to the model. This architecture consists in the combination of 2
items. First, a Hierarchical Encoder (HE) that aggregates summary statistics from the data. Second,
a set of conditional density estimators."
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.05426356589147287,"Tensor functions We first introduce the notations for tensor functions which we define in the spirit
of Magnus & Neudecker (1999). We leverage tensor functions throughout our entire architecture to
reduce its parameterization. Consider a function f : F →G, and a tensor TF ∈F B of shape B.
We denote the tensor TG ∈GB resulting from the element-wise application of f over TF as TG =
−→f (B)(TF ) (in reference to the programming notion of vectorization in Harris et al. (2020)). In Fig. 1,
−→
ST(B1)
0
and −−−−→
lγ ◦Fγ(B1) are examples of tensor functions. At multiple points in our architecture,
we will translate the repeated structure in the HBM induced by plates into the repeated usage of
functions across plates."
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.0562015503875969,"Hierarchical Encoder For our encoder, our goal is to learn a function HE that takes as input the
observed data X and successively exploits the permutation invariance across plates P0, . . . , PP . In
doing so, HE produces encodings E at different hierarchy levels. Through those encodings, our goal
is to learn summary statistics from the observed data, that will condition our amortized inference.
For instance in Fig. 1, the application of HE over X produces the encodings E1 and E2."
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.05813953488372093,"To build HE, we need at multiple hierarchies to collect summary statistics across i.i.d samples from a
common distribution. To this end we leverage SetTransformers (Lee et al., 2019): an attention-based,
permutation-invariant architecture. We use SetTransformers to derive encodings across a given plate,
repeating their usage for all larger-rank plates. We cast the observed data X as the encoding E0.
Then, recursively for every hierarchy h = 1 . . . P + 1, we define the encoding Eh as the application
to the encoding Eh−1 of the tensor function corresponding to the set transformer STh−1. HE(X)
then corresponds to the set of encodings {E1, . . . , EP +1} obtained from the successive application
of {STh}h=0,...,P . If we denote the batch shape Bh = Card(Ph) × . . . × Card(PP ):"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.060077519379844964,"Eh = −→
ST(Bh)
h−1 (Eh−1)
HE(X) = {E1, . . . , EP +1}
(1)"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.06201550387596899,"In collecting summary statistics across the i.i.d. samples in plate Ph−1, we decrease the order of the
encoding tensor Eh−1. We repeat this operation in parallel on every plate of larger rank than the rank
of the contracted plate. We consequently produce an encoding tensor Eh with the batch shape Bh,
which is the batch shape of every RV of hierarchy h. In that line, successively summarizing plates
P0, , . . . , PP , of increasing rank results in encoding tensors E1, . . . , EP +1 of decreasing order. In
Fig. 1, there are 2 plates P0 and P1, hence 2 encodings E1 = −→
ST(B1)
0
(X) and E2 = ST1(E1). E1
is an order 2 tensor: it has a batch shape of B1 = Card(P1) -similar to Γ- whereas E2 is an order 1
tensor. We can decompose E1 = [e1
1, e2
1] = [ST0([X1,1, X1,2]), ST0([X2,1, X2,2])]."
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.06395348837209303,"Conditional density estimators We now will use the encodings E, gathering hierarchical sum-
mary statistics on the data X, to condition the inference on the parameters θ.
The encodings
{Eh}h=1...P +1 will respectively condition the density estimators for the posterior distribution of
parameters sharing their hierarchy {{θi : Hier(θi) = h}}h=1...P +1."
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.06589147286821706,"Consider a latent RV θi of hierarchy hi = Hier(θi). Due to the plate structure of the graph, θi
can be decomposed in a batch of shape Bhi = Card(Phi) × . . . × Card(PP ) of multiple similar,
conditionally independent RVs of individual size Sθi. This decomposition is akin to the grounding
of the considered graph template (Koller & Friedman, 2009). A conditional density estimator is
a 2-step diffeomorphism from a latent space onto the event space in which the RV θi lives. We
initially parameterize every variational density as a standard normal distribution in the latent space
RSθi. First, this latent distribution is reparameterized by a conditional normalizing flow Fi (Rezende
& Mohamed, 2016; Papamakarios et al., 2019a) into a distribution of more complex density in the
space RSθi. The flow Fi is a diffeomorphism in the space RSθi conditioned by the encoding Ehi.
Second, the obtained latent distribution is projected onto the event space in which θi lives by the
application of a link function diffeomorphism li. For instance, if θi is a variance parameter, the link
function would map R onto R+∗(li = Exp as an example). The usage of Fi and the link function
li is repeated on plates of larger rank than the hierarchy hi of θi. The resulting conditional density
estimator qi for the posterior distribution p(θi|X) is given by:"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.06782945736434108,"ui ∼N
−→0 Bhi×Sθi, IBhi×Sθi"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.06976744186046512,"
˜θi = −−−→
li ◦Fi
(Bhi)(ui; Ehi) ∼qi(θi; Ehi)
(2)"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.07170542635658915,"In Fig. 1 Γ = [γ1, γ2] is associated to the diffeomorphism −−−−→
lγ ◦Fγ(B1). This diffeomorphism is con-
ditioned by the encoding E1. Both Γ and E1 share the batch shape B1 = Card(P1). Decomposing
the encoding E1 = [e1
1, e2
1], e1
1 is used to condition the inference on γ1, and e2
1 for γ2. λ is associated
to the diffeomorphism lλ ◦Fλ, and κ to lκ ◦Fκ, both conditioned by E2."
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.07364341085271318,"101
103
105
# examples 102 104 106"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.0755813953488372,Total wall time (s)
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.07751937984496124,"Non-amortized (MF-VI)
Amortized (ADAVI)"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.07945736434108527,"(a) Cumulative GPU training and inference
time for a non-amortized (MF-VI) and an
amortized (ADAVI) method."
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.08139534883720931,"X
X
B
X"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.08333333333333333,"NC
GRE
GM
MSHBM"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.08527131782945736,"(b) Experiment’s HBMs from left to right: Non-conjugate (NC)
(section 3.2), Gaussian random effects (GRE) (3.1, 3.3), Gaus-
sian mixture (GM) (3.4), Multi-scale (MSHBM) (3.5)."
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.0872093023255814,"Figure 2: panel (a): inference amortization on the Gaussian random effects example defined in
eq. (6): as the number of examples rises, the amortized method becomes more attractive; panel (b):
graph templates corresponding to the HBMs presented as part of our experiments."
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.08914728682170543,"Parsimonious parameterization Our approach produces a parameterization effectively indepen-
dent from plate cardinalities. Consider the latent RVs θ1, . . . , θL. Normalizing flow-based density
estimators have a parameterization quadratic with respect to the size of the space they are applied
to (e.g. Papamakarios et al., 2018). Applying a single normalizing flow to the total event space of
θ1, . . . , θL would thus result in O([PL
i=1 Sθi
QP
p=hi Card(Pp)]2) weights. But since we instead
apply multiple flows on the spaces of size Sθi and repeat their usage across all plates Phi, . . . , PP ,
we effectively reduce this parameterization to:"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.09108527131782945,"# weightsADAVI = O L
X"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.09302325581395349,"i=1
S2
θi ! (3)"
AUTOMATIC DERIVATION OF A DUAL AMORTIZED VARIATIONAL FAMILY,0.09496124031007752,"As a consequence, our method can be applied to HBMs featuring large plate cardinalities without
scaling up its parameterization to impractical ranges, preventing a computer memory blow-up."
VARIATIONAL DISTRIBUTION AND TRAINING,0.09689922480620156,"2.3
VARIATIONAL DISTRIBUTION AND TRAINING"
VARIATIONAL DISTRIBUTION AND TRAINING,0.09883720930232558,"Given the encodings Ep provided by HE, and the conditioned density estimators qi, we define our
parametric amortized variational distribution as a mean field approximation (Blei et al., 2017):"
VARIATIONAL DISTRIBUTION AND TRAINING,0.10077519379844961,"qχ,Φ(θ|X) = qΦ(θ; HEχ(X)) =
Y"
VARIATIONAL DISTRIBUTION AND TRAINING,0.10271317829457365,"i=1...L
qi(θi; Ehi, Φ)
(4)"
VARIATIONAL DISTRIBUTION AND TRAINING,0.10465116279069768,"In Fig. 1, we factorize q(Γ, κ, λ|X) = qγ(Γ; E1) × qλ(λ; E2) × qκ(κ; E2). Grouping parameters as
Ψ = (χ, Φ), our objective is to have qΨ(θ|X) ≈p(θ|X). Our loss is an amortized version of the
classical ELBO expression (Blei et al., 2017; Rezende & Mohamed, 2016):"
VARIATIONAL DISTRIBUTION AND TRAINING,0.1065891472868217,"Ψ⋆= arg min
Ψ
1
M M
X"
VARIATIONAL DISTRIBUTION AND TRAINING,0.10852713178294573,"m=1
log qΨ(θm|Xm) −log p(Xm, θm),
Xm ∼p(X), θm ∼qΨ(θ|X)
(5)"
VARIATIONAL DISTRIBUTION AND TRAINING,0.11046511627906977,"Where we denote z ∼p(z) the sampling of z according to the distribution p(z). We jointly train
HE and qi, i = 1 . . . L to minimize the amortized ELBO. The resulting architecture performs amor-
tized inference on latent parameters. Furthermore, since our parameterization is invariant to plate
cardinalities, our architecture is suited for population studies with large-dimensional feature space."
EXPERIMENTS,0.1124031007751938,"3
EXPERIMENTS"
EXPERIMENTS,0.11434108527131782,"In the following experiments, we consider a variety of inference problems on pyramidal HBMs. We
first illustrate the notion of amortization (section 3.1). We then test the expressivity (section 3.2, 3.4),
scalability (section 3.3) of our architecture, as well as its practicality on a challenging neuroimaging
experiment (section 3.5)."
EXPERIMENTS,0.11627906976744186,"Baseline choice In our experiments we use as baselines: Mean Field VI (MF-VI) (Blei et al., 2017)
is a common-practice method; (Sequential) Neural Posterior Estimation (NPE-C, SNPE-C) (Green-
berg et al., 2019) is a structure-unaware, likelihood-free method: SNPE-C results from the sequential
-and no longer amortized- usage of NPE-C; Total Latent Space Flow (TLSF) (Rezende & Mohamed,
2016) is a reverse-KL counterpoint to SNPE-C: both fit a single normalizing flow to the entirety of
the latent parameter space but SNPE-C uses a forward KL loss while TLSF uses a reverse KL loss;
Cascading Flows (CF) (Ambrogioni et al., 2021b) is a structure-aware, prior-aware method: CF-A
is our main point of comparison in this section. For relevant methods, the suffix -(N)A designates
the (non) amortized implementation. More details related to the choice and implementation of those
baselines can be found in our supplemental material."
INFERENCE AMORTIZATION,0.1182170542635659,"3.1
INFERENCE AMORTIZATION"
INFERENCE AMORTIZATION,0.12015503875968993,"In this experiment we illustrate the trade-off between amortized versus non-amortized techniques
(Cranmer et al., 2020). For this, we define the following Gaussian random effects HBM (Gelman
et al., 2004) (see Fig. 2b-GRE):"
INFERENCE AMORTIZATION,0.12209302325581395,"D, N = 2, 50
µ ∼N(⃗0D, σ2
µ)"
INFERENCE AMORTIZATION,0.12403100775193798,"G = 3
µg|µ ∼N(µ, σ2
g)
M G = [µg]g=1...G"
INFERENCE AMORTIZATION,0.12596899224806202,"σµ, σg, σx = 1.0, 0.2, 0.05
xg,n|µg ∼N(µg, σ2
x)
X = [xg,n]
g=1...G
n=1...N (6)"
INFERENCE AMORTIZATION,0.12790697674418605,"In Fig. 2a we compare the cumulative time to perform inference upon a batch of examples drawn
from this generative HBM. For a single example, a non-amortized technique can be faster -and
deliver a posterior closer to the ground truth- than an amortized technique. This is because the non-
amortized technique fits a solution for this specific example, and can tune it extensively. In terms
of ELBO, on top of an approximation gap an amortized technique will add an amortization gap
(Cremer et al., 2018). On the other hand, when presented with a new example, the amortized tech-
nique can infer directly whereas the optimization of the non-amortized technique has to be repeated.
As the number of examples rises, an amortized technique becomes more and more attractive. This
result puts in perspective the quantitative comparison later on performed between amortized and
non-amortized techniques, that are qualitatively distinct."
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.1298449612403101,"3.2
EXPRESSIVITY IN A NON-CONJUGATE CASE"
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.13178294573643412,"In this experiment, we underline the superior expressivity gained from using normalizing flows -
used by ADAVI or CF- instead of distributions of fixed parametric form -used by MF-VI. For this
we consider the following HBM (see Fig. 2b-NC):
N, D = 10, 2
ra, σb = 0.5, 0.3"
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.13372093023255813,"a ∼Gamma(⃗1D, ra)
bn|a ∼Laplace(a, σb)
B = [bn]n=1...N
(7)"
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.13565891472868216,"This example is voluntarily non-canonical: we place ourselves in a setup where the posterior distri-
bution of a given an observed value from B has no known parametric form, and in particular is not
of the same parametric form as the prior. Such an example is called non-conjugate in Gelman et al.
(2004). Results are visible in table 1-NC: MF-VI is limited in its ability to approximate the correct
distribution as it attempts to fit to the posterior a distribution of the same parametric form as the
prior. As a consequence, contrary to the experiments in section 3.3 and section 3.4 -where MF-VI
stands as a strong baseline- here both ADAVI and CF-A are able to surpass its performance."
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.1375968992248062,"Proxy to the ground truth posterior MF-VI plays the role of an ELBO upper bound in our experi-
ments GRE (section 3.3), GM (section 3.4) and MS-HBM (section 3.5). We crafted those examples
to be conjugate: MF-VI thus doesn’t feature any approximation gap, meaning KL(q(θ)||p(θ|X)) ≃
0. As such, its ELBO(q) = log p(X)−KL(q(θ)||p(θ|X)) is approximately equal to the evidence of
the observed data. As a consequence, any inference method with the same ELBO value -calculated
over the same examples- as MF-VI would yield an approximate posterior with low KL divergence to
the true posterior. Our main focus in this work are flow-based methods, whose performance would
be maintained in non-conjugate cases, contrary to MF-VI (Papamakarios et al., 2019a). We further
focus on amortized methods, providing faster inference for a multiplicity of problem instances, see
e.g. section 3.1. MF-VI is therefore not to be taken as part of a benchmark but as a proxy to the
unknown ground truth posterior."
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.13953488372093023,"Table 1: Expressivity comparison on the non-conjugate (NC) and Gaussian mixture (GM) examples.
NC: both CF-A and ADAVI show higher ELBO than MF-VI. GM: TLSF-A and ADAVI show
higher ELBO than CF-A, but do not reach the ELBO levels of MF-VI, TLSF-NA and CF-NA.
Are compared from left to right: ELBO median (larger is better) and standard deviation; for non-
amortized techniques: CPU inference time for one example (seconds); for amortized techniques:
CPU amortization time (seconds). Methods are ran over 20 random seeds, except for SNPE-C and
TLSF-NA who were ran on 5 seeds per sample, for a number of effective runs of 100. For CF, the
ELBO designates the numerically comparable augmented ELBO (Ranganath et al., 2016)."
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.14147286821705427,"HBM
Type
Method
ELBO
Inf. (s)
Amo. (s)"
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.1434108527131783,"NC
Fixed param. form
MF-VI
-21.0 (± 0.2)
17
-"
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.14534883720930233,"(section 3.2)
Flow-based
CF-A
-17.5 (± 0.1)
-
220
ADAVI
-17.6 (± 0.3)
-
1,000"
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.14728682170542637,"GM
Ground truth proxy
MF-VI
171 (± 970)
23
-"
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.14922480620155038,"(section 3.4)
Non amortized
SNPE-C
-14,800 (± 15,000)
70,000
-
TLSF-NA
181 (± 680)
330
-
CF-NA
191 (± 390)
240
-"
EXPRESSIVITY IN A NON-CONJUGATE CASE,0.1511627906976744,"Amortized
NPE-C
-27,000 (± 19,000)
-
1300
TLSF-A
-530 (± 980)
-
360,000
CF-A
-7,000 (± 640)
-
23,000
ADAVI
-494 (± 430)
-
150,000"
PERFORMANCE SCALING WITH RESPECT TO PLATE CARDINALITY,0.15310077519379844,"3.3
PERFORMANCE SCALING WITH RESPECT TO PLATE CARDINALITY"
PERFORMANCE SCALING WITH RESPECT TO PLATE CARDINALITY,0.15503875968992248,"In this experiment, we illustrate our plate cardinality independent parameterization defined in sec-
tion 2.2. We consider 3 instances of the Gaussian random effects model presented in eq. (6), in-
creasing the number of groups from G = 3 to G = 30 and G = 300. In doing so, we augment
the total size of the latent parametric space from 8 to 62 to 602 parameters, and the observed data
size from 300 to 3, 000 to 30, 000 values. Results for this experiment are visible in Fig. 3 (see also
table 2). On this example we note that amortized techniques only feature a small amortization gap
(Cremer et al., 2018), reaching the performance of non-amortized techniques -as measured by the
ELBO, using MF-VI as an upper bound- at the cost of large amortization times. We note that the
performance of (S)NPE-C quickly degrades as the plate dimensionality augments, while TLSF’s
performance is maintained, hinting towards the advantages of using the likelihood function when
available. As the HBM’s plate cardinality augments, we match the performance and amortization
time of state-of-the-art methods, but we do so maintaining a constant parameterization."
EXPRESSIVITY IN A CHALLENGING SETUP,0.1569767441860465,"3.4
EXPRESSIVITY IN A CHALLENGING SETUP"
EXPRESSIVITY IN A CHALLENGING SETUP,0.15891472868217055,"In this experiment, we test our architecture on a challenging setup in inference: a mixture model.
Mixture models notably suffer from the label switching issue and from a loss landscape with multiple
strong local minima (Jasra et al., 2005). We consider the following mixture HBM (see Fig. 2b-GM):"
EXPRESSIVITY IN A CHALLENGING SETUP,0.16085271317829458,"κ, σµ, σg, σx = 1, 1.0, 0.2, 0.05
G, L, D, N = 3, 3, 2, 50"
EXPRESSIVITY IN A CHALLENGING SETUP,0.16279069767441862,"µl ∼N(⃗0D, σ2
µ)
M L = [µl]l=1...L"
EXPRESSIVITY IN A CHALLENGING SETUP,0.16472868217054262,"µg
l |µl ∼N(µl, σ2
g)
M L,G = [µg
l ]
l=1...L
g=1...G (8a)"
EXPRESSIVITY IN A CHALLENGING SETUP,0.16666666666666666,"πg ∈[0, 1]L ∼Dir([κ] × L)
ΠG = [πg]g=1...G"
EXPRESSIVITY IN A CHALLENGING SETUP,0.1686046511627907,"xg,n|πg, [µg
1, . . . , µg
L] ∼Mix(πg, [N(µg
1, σ2
x) . . . N(µg
L, σ2
x)])
X = [xg,n]
g=1...G
n=1...N
(8b)"
EXPRESSIVITY IN A CHALLENGING SETUP,0.17054263565891473,"Where Mix(π, [p1, . . . , pN]) denotes the finite mixture of the densities [p1, . . . , pN] with π the mix-
ture weights. The results are visible in table 1-GM. In this complex example, similar to TLSF-A we
obtain significantly higher ELBO than CF-A, but we do feature an amortization gap, not reaching the
ELBO level of non-amortized techniques. We also note that despite our efforts (S)NPE-C failed to"
EXPRESSIVITY IN A CHALLENGING SETUP,0.17248062015503876,"3
30
300
G 101 102 103 104 105 106"
EXPRESSIVITY IN A CHALLENGING SETUP,0.1744186046511628,# weights
EXPRESSIVITY IN A CHALLENGING SETUP,0.17635658914728683,Parameterization (lower is better)
EXPRESSIVITY IN A CHALLENGING SETUP,0.17829457364341086,"3
30
300
G 50 60 70 80 90 100"
EXPRESSIVITY IN A CHALLENGING SETUP,0.18023255813953487,"median ELBO
G"
EXPRESSIVITY IN A CHALLENGING SETUP,0.1821705426356589,Inference quality (larger is better)
EXPRESSIVITY IN A CHALLENGING SETUP,0.18410852713178294,"3
30
300
G 101 102 103 104 105 106"
EXPRESSIVITY IN A CHALLENGING SETUP,0.18604651162790697,Fit time (s)
EXPRESSIVITY IN A CHALLENGING SETUP,0.187984496124031,Time (lower is better)
EXPRESSIVITY IN A CHALLENGING SETUP,0.18992248062015504,"Amortized
methods:
NPE-C
TLSF-A
CF-A
ADAVI
Non amortized
methods:
SNPE-C
TLSF-NA
CF-NA
Ground truth
proxy:
MF-VI"
EXPRESSIVITY IN A CHALLENGING SETUP,0.19186046511627908,"Figure 3: Scaling comparison on the Gaussian random effects example. ADAVI -in red- maintains
constant parameterization as the plates cardinality goes up (first panel); it does so while maintaining
its inference quality (second panel) and a comparable amortization time (third panel). Are compared
from left to right: number of weights in the model; closeness of the approximate posterior to the
ground truth via the ELBO"
EXPRESSIVITY IN A CHALLENGING SETUP,0.1937984496124031,"G
median -that allows for a comparable numerical range as G augments;
CPU amortization + inference time (s) for a single example -this metric advantages non-amortized
methods. Non-amortized techniques are represented using dashed lines, and amortized techniques
using plain lines. MF-VI, in dotted lines, plays the role of the upper bound for the ELBO. Results
for SNPE-C and NPE-C have to be put in perspective, as from G = 30 and G = 300 respectively
both methods reach data regimes in which the inference quality is very degraded (see table 2).
Implementation details are shared with table 1."
EXPRESSIVITY IN A CHALLENGING SETUP,0.19573643410852712,"reach the ELBO level of other techniques. We interpret this result as the consequence of a forward-
KL-based training taking the full blunt of the label switching problem, as seen in appendix D.2.
Fig. D.3 shows how our higher ELBO translates into results of greater experimental value."
EXPRESSIVITY IN A CHALLENGING SETUP,0.19767441860465115,"3.5
NEUROIMAGING: MODELLING MULTI-SCALE VARIABILITY IN BROCA’S AREA
FUNCTIONAL PARCELLATION"
EXPRESSIVITY IN A CHALLENGING SETUP,0.1996124031007752,"To show the practicality of our method in a high-dimensional context, we consider the model pro-
posed by Kong et al. (2019). We apply this HBM to parcel the human brain’s Inferior Frontal Gyrus
in 2 functional MRI (fMRI)-based connectivity networks. Data is extracted from the Human Con-
nectome Project dataset (Van Essen et al., 2012). The HBM models a population study with 30
subjects and 4 large fMRI measures per subject, as seen in Fig. 2b-MSHBM: this nested structure
creates a large latent space of ≃0.4 million parameters and an even larger observed data size of ≃50
million values. Due to our parsimonious parameterization, described in eq. (4), we can nonetheless
tackle this parameter range without a memory blow-up, contrary to all other presented flow-based
methods -CF, TLSF, NPE-C. Resulting population connectivity profiles can be seen in Fig. 4. We
are in addition interested in the stability of the recovered population connectivity considering sub-
sets of the population. For this we are to sample without replacement hundreds of sub-populations
of 5 subjects from our population. On GPU, the inference wall time for MF-VI is 160 seconds per
sub-population, for a mean log(−ELBO) of 28.6(±0.2) (across 20 examples, 5 seeds per example).
MF-VI can again be considered as an ELBO upper bound. Indeed the MSHBM can be considered
as a 3-level (subject, session, vertex) Gaussian mixture with random effects, and therefore features
conjugacy. For multiple sub-populations, the total inference time for MF-VI reaches several hours.
On the contrary, ADAVI is an amortized technique, and as such features an amortization time of 550
seconds, after which it can infer on any number of sub-populations in a few seconds. The posterior
quality is similar: a mean log(−ELBO) of 29.0(±0.01). As shown in our supplemental material
-as a more meaningful comparison- the resulting difference in the downstream parcellation task is
marginal (Fig. E.7)."
EXPRESSIVITY IN A CHALLENGING SETUP,0.20155038759689922,"We therefore bring the expressivity of flow-based methods and the speed of amortized techniques
to parameter ranges previously unreachable. This is due to our plate cardinality-independent pa-
rameterization. What’s more, our automatic method only necessitates a practitioner to declare the
generative HBM, therefore reducing the analytical barrier to entry there exists in fields such as neu- s"
EXPRESSIVITY IN A CHALLENGING SETUP,0.20348837209302326,"Figure 4: Results for our neuroimaging experiment. On the left, networks show the top 1% con-
nected components. Network 0 (in blue) agrees with current knowledge in semantic/phonologic pro-
cessing while network 1 (in red) agrees with current networks known in language production (Heim
et al., 2009; Zhang et al., 2020). Our soft parcellation, where coloring lightens as the cortical
point is less probably associated with one of the networks, also agrees with current knowledge
where more posterior parts are involved in language production while more anterior ones in seman-
tic/phonological processing (Heim et al., 2009; Zhang et al., 2020)."
EXPRESSIVITY IN A CHALLENGING SETUP,0.2054263565891473,"roimaging for large-scale Bayesian analysis. Details about this experiment, along with subject-level
results, can be found in our supplemental material."
DISCUSSION,0.20736434108527133,"4
DISCUSSION"
DISCUSSION,0.20930232558139536,"Exploiting structure in inference In the SBI and VAE setups, data structure can be exploited
through learnable data embedders (Zhang et al., 2019; Radev et al., 2020). We go one step be-
yond and also use the problem structure to shape our density estimator: we factorize the parameter
space of a problem into smaller components, and share network parameterization across tasks we
know to be equivalent (see section 2.2 and 3.3). In essence, we construct our architecture not based
on a ground HBM graph, but onto its template, a principle that could be generalized to other types
of templates, such as temporal models (Koller & Friedman, 2009). Contrary to the notion of black
box, we argue that experimenters oftentimes can identify properties such as exchangeability in their
experiments (Gelman et al., 2004). As our experiments illustrate (section 3.4, section 3.3), there
is much value in exploiting this structure. Beyond the sole notion of plates, a static analysis of a
forward model could automatically identify other desirable properties that could be then leveraged
for efficient inference. This concept points towards fruitful connections to be made with the field of
lifted inference (Broeck et al., 2021; Chen et al., 2020)."
DISCUSSION,0.21124031007751937,"Mean-Field approximation A limitation in our work is that our posterior distribution is akin to a
mean field approximation (Blei et al., 2017): with the current design, no statistical dependencies
can be modelled between the RV blocks over which we fit normalizing flows (see section 2.3). Re-
grouping RV templates, we could model more dependencies at a given hierarchy. On the contrary,
our method prevents the direct modelling of dependencies between ground RVs corresponding to
repeated instances of the same template. Those dependencies can arise as part of inference (Webb
et al., 2018). We made the choice of the Mean Field approximation to streamline our contribution,
and allow for a clear delineation of the advantages of our methods, not tying them up to a method
augmenting a variational family with statistical dependencies, an open research subject (Ambrogioni
et al., 2021b; Weilbach et al., 2020). Though computationally attractive, the mean field approxima-
tion nonetheless limits the expressivity of our variational family (Ranganath et al., 2016; Hoffman
& Blei, 2014). We ponder the possibility to leverage VI architectures such as the one derived by
Ranganath et al. (2016); Ambrogioni et al. (2021b) and their augmented variational objectives for
structured populations of normalizing flows such as ours."
DISCUSSION,0.2131782945736434,"Conclusion For the delineated yet expressive class of pyramidal Bayesian models, we have intro-
duced a potent, automatically derived architecture able to perform amortized parameter inference.
Through a Hierarchical Encoder, our method conditions a network of normalizing flows that stands
as a variational family dual to the forward HBM. To demonstrate the expressivity and scalability of
our method, we successfully applied it to a challenging neuroimaging setup. Our work stands as an
original attempt to leverage exchangeability in a generative model."
DISCUSSION,0.21511627906976744,ACKNOWLEDGMENTS
DISCUSSION,0.21705426356589147,This work was supported by the ERC-StG NeuroLang ID:757672.
DISCUSSION,0.2189922480620155,"We would like to warmly thank Dr. Thomas Yeo and Dr. Ru Kong (CBIG) who made pre-processed
HCP functional connectivity data available to us."
DISCUSSION,0.22093023255813954,"We also would like to thank Dr. Majd Abdallah (Inria) for his insights and perspectives regarding
our functional connectivity results."
REPRODUCIBILITY STATEMENT,0.22286821705426357,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.2248062015503876,"All experiments were performed on a computational cluster with 16 Intel(R) Xeon(R) CPU E5-2660
v2 @ 2.20GHz (256Mb RAM), 16 AMD EPYC 7742 64-Core Processor (512Mb RAM) CPUs and
1 NVIDIA Quadro RTX 6000 (22Gb), 1 Tesla V100 (32Gb) GPUs."
REPRODUCIBILITY STATEMENT,0.22674418604651161,"All methods were implemented in Python. We implemented most methods using Tensorflow Prob-
ability (Dillon et al., 2017), and SBI methods using the SBI Python library (Tejero-Cantero et al.,
2020)."
REPRODUCIBILITY STATEMENT,0.22868217054263565,"As part of our submission we release the code associated to our experiments. Our supplemental ma-
terial furthermore contains an entire section dedicated to the implementation details of the baseline
methods presented as part of our experiments. For our neuromimaging experiment, we also provide
a section dedicated to our pre-processing and post-processing steps"
REFERENCES,0.23062015503875968,REFERENCES
REFERENCES,0.23255813953488372,"Luca Ambrogioni, Kate Lin, Emily Fertig, Sharad Vikram, Max Hinne, Dave Moore, and Marcel van
Gerven. Automatic structured variational inference. In Arindam Banerjee and Kenji Fukumizu
(eds.), Proceedings of The 24th International Conference on Artificial Intelligence and Statistics,
volume 130 of Proceedings of Machine Learning Research, pp. 676–684. PMLR, 13–15 Apr
2021a. URL https://proceedings.mlr.press/v130/ambrogioni21a.html."
REFERENCES,0.23449612403100775,"Luca Ambrogioni, Gianluigi Silvestri, and Marcel van Gerven. Automatic variational inference with
cascading flows. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
254–263. PMLR, 18–24 Jul 2021b. URL https://proceedings.mlr.press/v139/
ambrogioni21a.html."
REFERENCES,0.2364341085271318,"Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis
Karaletsos, Rohit Singh, Paul A. Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep
universal probabilistic programming. J. Mach. Learn. Res., 20:28:1–28:6, 2019. URL http:
//jmlr.org/papers/v20/18-403.html."
REFERENCES,0.23837209302325582,"David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statis-
ticians. Journal of the American Statistical Association, 112(518):859–877, April 2017. ISSN
0162-1459, 1537-274X. doi: 10.1080/01621459.2017.1285773. URL http://arxiv.org/
abs/1601.00670. arXiv: 1601.00670."
REFERENCES,0.24031007751937986,"Anna K Bonkhoff, Jae-Sung Lim, Hee-Joon Bae, Nick A Weaver, Hugo J Kuijf, J Matthijs
Biesbroek, Natalia S Rost, and Danilo Bzdok.
Generative lesion pattern decomposition of
cognitive impairment after stroke.
Brain Communications, 05 2021.
ISSN 2632-1297.
doi: 10.1093/braincomms/fcab110.
URL https://doi.org/10.1093/braincomms/
fcab110. fcab110."
REFERENCES,0.24224806201550386,"Guy van den Broeck, Kristian Kersting, Sriraam Natarajan, and David Poole (eds.). An introduction
to lifted probabilistic inference. Neural information processing series. The MIT Press, Cambridge,
Massachusetts, 2021. ISBN 978-0-262-54259-3."
REFERENCES,0.2441860465116279,"Yuqiao Chen, Yibo Yang, Sriraam Natarajan, and Nicholas Ruozzi. Lifted hybrid variational in-
ference. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Con-
ference on Artificial Intelligence, IJCAI-20, pp. 4237–4244. International Joint Conferences on"
REFERENCES,0.24612403100775193,"Artificial Intelligence Organization, 7 2020.
doi: 10.24963/ijcai.2020/585.
URL https:
//doi.org/10.24963/ijcai.2020/585. Main track."
REFERENCES,0.24806201550387597,"Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference.
Proceedings of the National Academy of Sciences, pp. 201912789, May 2020. ISSN 0027-8424,
1091-6490. doi: 10.1073/pnas.1912789117. URL http://www.pnas.org/lookup/doi/
10.1073/pnas.1912789117."
REFERENCES,0.25,"Chris Cremer, Xuechen Li, and David Duvenaud. Inference Suboptimality in Variational Autoen-
coders. arXiv:1801.03558 [cs, stat], May 2018. URL http://arxiv.org/abs/1801.
03558. arXiv: 1801.03558."
REFERENCES,0.25193798449612403,"Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore,
Brian Patton, Alex Alemi, Matt Hoffman, and Rif A. Saurous.
TensorFlow Distributions.
arXiv:1711.10604 [cs, stat], November 2017.
URL http://arxiv.org/abs/1711.
10604. arXiv: 1711.10604."
REFERENCES,0.25387596899224807,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
Density estimation using Real NVP.
arXiv:1605.08803 [cs, stat], February 2017.
URL http://arxiv.org/abs/1605.
08803. arXiv: 1605.08803."
REFERENCES,0.2558139534883721,"David L. Donoho. High-dimensional data analysis: The curses and blessings of dimensionality. In
AMS CONFERENCE ON MATH CHALLENGES OF THE 21ST CENTURY, 2000."
REFERENCES,0.25775193798449614,"Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. Bayesian Data Analysis.
Chapman and Hall/CRC, 2nd ed. edition, 2004."
REFERENCES,0.2596899224806202,"W. R. Gilks, A. Thomas, and D. J. Spiegelhalter. A Language and Program for Complex Bayesian
Modelling. The Statistician, 43(1):169, 1994. ISSN 00390526. doi: 10.2307/2348941. URL
https://www.jstor.org/stable/10.2307/2348941?origin=crossref."
REFERENCES,0.2616279069767442,"Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duve-
naud. FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models.
arXiv:1810.01367 [cs, stat], October 2018. URL http://arxiv.org/abs/1810.01367.
arXiv: 1810.01367."
REFERENCES,0.26356589147286824,"David S. Greenberg, Marcel Nonnenmacher, and Jakob H. Macke. Automatic Posterior Transfor-
mation for Likelihood-Free Inference. arXiv:1905.07488 [cs, stat], May 2019. URL http:
//arxiv.org/abs/1905.07488. arXiv: 1905.07488."
REFERENCES,0.2655038759689923,"Charles R. Harris, K. Jarrod Millman, St´efan J. van der Walt, Ralf Gommers, Pauli Virtanen,
David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert
Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane,
Jaime Fern´andez del R´ıo, Mark Wiebe, Pearu Peterson, Pierre G´erard-Marchant, Kevin Sheppard,
Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Ar-
ray programming with NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/
s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2."
REFERENCES,0.26744186046511625,"Stefan Heim, Simon B. Eickhoff, Anja K. Ischebeck, Angela D. Friederici, Klaas E. Stephan, and
Katrin Amunts. Effective connectivity of the left BA 44, BA 45, and inferior temporal gyrus
during lexical and phonological decisions identified with DCM. Human Brain Mapping, 30(2):
392–402, February 2009. ISSN 10659471. doi: 10.1002/hbm.20512."
REFERENCES,0.2693798449612403,"Matthew D. Hoffman and David M. Blei.
Structured Stochastic Variational Inference.
arXiv:1404.4114 [cs], November 2014.
URL http://arxiv.org/abs/1404.4114.
arXiv: 1404.4114."
REFERENCES,0.2713178294573643,"Ekaterina Iakovleva, Jakob Verbeek, and Karteek Alahari. Meta-learning with shared amortized vari-
ational inference. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International
Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.
4572–4582. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/
iakovleva20a.html."
REFERENCES,0.27325581395348836,"A. Jasra, C. C. Holmes, and D. A. Stephens. Markov Chain Monte Carlo Methods and the Label
Switching Problem in Bayesian Mixture Modeling. Statistical Science, 20(1), February 2005.
ISSN 0883-4237. doi: 10.1214/088342305000000016. URL https://projecteuclid.
org/journals/statistical-science/volume-20/issue-1/
Markov-Chain-Monte-Carlo-Methods-and-the-Label-Switching-Problem/
10.1214/088342305000000016.full."
REFERENCES,0.2751937984496124,"Diederik P. Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980."
REFERENCES,0.2771317829457364,"Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv:1312.6114 [cs,
stat], May 2014. URL http://arxiv.org/abs/1312.6114. arXiv: 1312.6114."
REFERENCES,0.27906976744186046,"Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. Adap-
tive computation and machine learning. MIT Press, Cambridge, MA, 2009. ISBN 978-0-262-
01319-2."
REFERENCES,0.2810077519379845,"Ru Kong, Jingwei Li, Csaba Orban, Mert Rory Sabuncu, Hesheng Liu, Alexander Schaefer, Nanbo
Sun, Xi-Nian Zuo, Avram J. Holmes, Simon B. Eickhoff, and B. T. Thomas Yeo. Spatial topogra-
phy of individual-specific cortical networks predicts human cognition, personality, and emotion.
Cerebral cortex, 29 6:2533–2551, 2019."
REFERENCES,0.28294573643410853,"Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Automatic
Differentiation Variational Inference. arXiv:1603.00788 [cs, stat], March 2016. URL http:
//arxiv.org/abs/1603.00788. arXiv: 1603.00788."
REFERENCES,0.28488372093023256,"Olivier Ledoit and Michael Wolf. A well-conditioned estimator for large-dimensional covariance
matrices. Journal of Multivariate Analysis, 88(2):365–411, 2004. ISSN 0047-259X. doi: https:
//doi.org/10.1016/S0047-259X(03)00096-4. URL https://www.sciencedirect.com/
science/article/pii/S0047259X03000964."
REFERENCES,0.2868217054263566,"Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set
transformer: A framework for attention-based permutation-invariant neural networks. In Kama-
lika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Confer-
ence on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 3744–
3753. PMLR, 09–15 Jun 2019. URL http://proceedings.mlr.press/v97/lee19d.
html."
REFERENCES,0.28875968992248063,"Jan R. Magnus and Heinz Neudecker. Matrix Differential Calculus with Applications in Statis-
tics and Econometrics. John Wiley, second edition, 1999. ISBN 0471986321 9780471986324
047198633X 9780471986331."
REFERENCES,0.29069767441860467,"George Papamakarios and Iain Murray. Fast $\epsilon$-free Inference of Simulation Models with
Bayesian Conditional Density Estimation. arXiv:1605.06376 [cs, stat], April 2018. URL http:
//arxiv.org/abs/1605.06376. arXiv: 1605.06376."
REFERENCES,0.2926356589147287,"George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density
Estimation. arXiv:1705.07057 [cs, stat], June 2018. URL http://arxiv.org/abs/1705.
07057. arXiv: 1705.07057."
REFERENCES,0.29457364341085274,"George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lak-
shminarayanan. Normalizing Flows for Probabilistic Modeling and Inference. arXiv:1912.02762
[cs, stat], December 2019a.
URL http://arxiv.org/abs/1912.02762.
arXiv:
1912.02762."
REFERENCES,0.29651162790697677,"George Papamakarios, David C. Sterratt, and Iain Murray.
Sequential Neural Likelihood: Fast
Likelihood-free Inference with Autoregressive Flows.
arXiv:1805.07226 [cs, stat], January
2019b. URL http://arxiv.org/abs/1805.07226. arXiv: 1805.07226."
REFERENCES,0.29844961240310075,"Stefan T. Radev, Ulf K. Mertens, Andreass Voss, Lynton Ardizzone, and Ullrich K¨othe. BayesFlow:
Learning complex stochastic models with invertible neural networks. arXiv:2003.06281 [cs, stat],
April 2020. URL http://arxiv.org/abs/2003.06281. arXiv: 2003.06281."
REFERENCES,0.3003875968992248,"Rajesh Ranganath, Sean Gerrish, and David M. Blei.
Black Box Variational Inference.
arXiv:1401.0118 [cs, stat], December 2013. URL http://arxiv.org/abs/1401.0118.
arXiv: 1401.0118."
REFERENCES,0.3023255813953488,"Rajesh Ranganath,
Dustin Tran,
and David M. Blei.
Hierarchical Variational Models.
arXiv:1511.02386 [cs, stat], May 2016.
URL http://arxiv.org/abs/1511.02386.
arXiv: 1511.02386."
REFERENCES,0.30426356589147285,"Danilo Jimenez Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows.
arXiv:1505.05770 [cs, stat], June 2016.
URL http://arxiv.org/abs/1505.05770.
arXiv: 1505.05770."
REFERENCES,0.3062015503875969,"Pedro L. C. Rodrigues, Thomas Moreau, Gilles Louppe, and Alexandre Gramfort.
Leveraging
Global Parameters for Flow-based Neural Posterior Estimation. arXiv:2102.06477 [cs, q-bio,
stat], April 2021. URL http://arxiv.org/abs/2102.06477. arXiv: 2102.06477."
REFERENCES,0.3081395348837209,"Rui Shu, Hung H Bui, Shengjia Zhao, Mykel J Kochenderfer, and Stefano Ermon. Amortized infer-
ence regularization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
1819932ff5cf474f4f19e7c7024640c2-Paper.pdf."
REFERENCES,0.31007751937984496,"Alvaro Tejero-Cantero, Jan Boelts, Michael Deistler, Jan-Matthis Lueckmann, Conor Durkan, Pe-
dro J. Gonc¸alves, David S. Greenberg, and Jakob H. Macke. SBI – A toolkit for simulation-based
inference. arXiv:2007.09114 [cs, q-bio, stat], July 2020. URL http://arxiv.org/abs/
2007.09114. arXiv: 2007.09114."
REFERENCES,0.312015503875969,"Owen Thomas, Ritabrata Dutta, Jukka Corander, Samuel Kaski, and Michael U. Gutmann.
Likelihood-free inference by ratio estimation. arXiv:1611.10242 [stat], September 2020. URL
http://arxiv.org/abs/1611.10242. arXiv: 1611.10242."
REFERENCES,0.313953488372093,"D. C. Van Essen, K. Ugurbil, E. Auerbach, D. Barch, T. E. Behrens, R. Bucholz, A. Chang, L. Chen,
M. Corbetta, S. W. Curtiss, S. Della Penna, D. Feinberg, M. F. Glasser, N. Harel, A. C. Heath,
L. Larson-Prior, D. Marcus, G. Michalareas, S. Moeller, R. Oostenveld, S. E. Petersen, F. Prior,
B. L. Schlaggar, S. M. Smith, A. Z. Snyder, J. Xu, and E. Yacoub. The Human Connectome
Project: a data acquisition perspective. Neuroimage, 62(4):2222–2231, Oct 2012."
REFERENCES,0.31589147286821706,"Stefan Webb, Adam Golinski, Robert Zinkov, N. Siddharth, Tom Rainforth, Yee Whye Teh,
and Frank Wood.
Faithful Inversion of Generative Models for Effective Amortized Infer-
ence. arXiv:1712.00287 [cs, stat], November 2018. URL http://arxiv.org/abs/1712.
00287. arXiv: 1712.00287."
REFERENCES,0.3178294573643411,"Antoine Wehenkel and Gilles Louppe. Graphical Normalizing Flows. arXiv:2006.02548 [cs, stat],
October 2020. URL http://arxiv.org/abs/2006.02548. arXiv: 2006.02548."
REFERENCES,0.31976744186046513,"Christian Weilbach, Boyan Beronov, Frank Wood, and William Harvey. Structured conditional con-
tinuous normalizing flows for efficient amortized inference in graphical models. In Silvia Chiappa
and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Arti-
ficial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp.
4441–4451. PMLR, 26–28 Aug 2020. URL https://proceedings.mlr.press/v108/
weilbach20a.html."
REFERENCES,0.32170542635658916,"Mike Wu, Kristy Choi, Noah Goodman, and Stefano Ermon. Meta-Amortized Variational Inference
and Learning. arXiv:1902.01950 [cs, stat], September 2019. URL http://arxiv.org/
abs/1902.01950. arXiv: 1902.01950."
REFERENCES,0.3236434108527132,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and
Alexander Smola. Deep Sets. arXiv:1703.06114 [cs, stat], April 2018. URL http://arxiv.
org/abs/1703.06114. arXiv: 1703.06114."
REFERENCES,0.32558139534883723,"Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt. Advances in Variational
Inference. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(8):2008–2026,
August 2019. ISSN 0162-8828, 2160-9292, 1939-3539. doi: 10.1109/TPAMI.2018.2889774.
URL https://ieeexplore.ieee.org/document/8588399/."
REFERENCES,0.32751937984496127,"Yizhen Zhang, Kuan Han, Robert Worth, and Zhongming Liu. Connecting concepts in the brain
by mapping cortical representations of semantic relations. Nature Communications, 11(1):1877,
April 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-15804-w."
REFERENCES,0.32945736434108525,SUPPLEMENTAL MATERIAL
REFERENCES,0.3313953488372093,"This supplemental material complements our main work both with theoretical points and experi-
ments:"
REFERENCES,0.3333333333333333,"A complements to our methods section 2. We present the HBM descriptors needed for the
automatic derivation of our dual architecture;
B complements to our discussion section 3. We elaborate on various points including amorti-
zation;
C complements to the Gaussian random effects experiment described in eq. (6). We present
results mostly related to hyperparameter analysis;
D complements to the Gaussian mixture with random effects experiment (section 3.4). We
explore the complexity of the example at hand;
E complements to the MS-HBM experiments (section 3.5). We present some context for the
experiment, a toy dimensions experiment and implementation details.
F justification and implementation details for the baseline architectures used in our experi-
ments;"
REFERENCES,0.33527131782945735,"A
COMPLEMENTS TO THE METHODS: MODEL DESCRIPTORS FOR
AUTOMATIC VARIATIONAL FAMILY DERIVATION"
REFERENCES,0.3372093023255814,"This section is a complement to section 2. We formalize explicitly the descriptors of the generative
HBM needed for our method to derive its dual architecture. This information is of experimental
value, since those descriptors need to be available in any API designed to implement our method."
REFERENCES,0.3391472868217054,"If we denote plates(θ) the plates the RV θ belongs to, then the following HBM descriptors are the
needed input to derive automatically our ADAVI dual architecture:
V = {θi}i=0...L
P = {Pp}p=0...P
Card = {Pp →#Pp}p=0...P
Hier = {θi 7→hi = min
p {p : Pp ∈plates(θi)}}i=0...L"
REFERENCES,0.34108527131782945,"Shape = {θi 7→Sevent
θi
}i=0...L
Link = {θi 7→(li : Sθi →Sevent
θi
)}i=0...L (A.1)"
REFERENCES,0.3430232558139535,Where:
REFERENCES,0.3449612403100775,"• V lists the RVs in the HBM (vertices in the HBM’s corresponding graph template);
• P lists the plates in the HBM’s graph template;
• Card maps a plate P to its cardinality, that is to say the number of independent draws from
a common conditional density it corresponds to;
• Hier maps a RV θ to its hierarchy, that is to say the level of the pyramid it is placed at, or
equivalently the smallest rank for the plates it belongs to;
• Shape maps a RV to its event shape Sevent
θi
. Consider the plate-enriched graph template
representing the HBM. A single graph template RV belonging to plates corresponds to
multiple similar RVs when grounding this graph template. Sevent
θi
is the potentially high-
order shape for any of those multiple ground RVs.
• Link maps a RV θ to its link function l. The Link function projects the latent space for the
RV θ onto the event space in which θ lives. For instance, if θ is a variance parameter, the
link function would map R onto R+∗(l = Exp as an example). Note that the latent space
of shape Sθ is necessary an order 1 unbounded real space. l therefore potentially implies a
reshaping to the high-order shape Sevent
θi
."
REFERENCES,0.34689922480620156,"Those descriptors can be readily obtained from a static analysis of a generative model, especially
when the latter is expressed in a modern probabilistic programming framework (Dillon et al., 2017;
Bingham et al., 2019)."
REFERENCES,0.3488372093023256,"B
COMPLEMENTS TO OUR DISCUSSION"
REFERENCES,0.3507751937984496,"B.1
AMORTIZATION"
REFERENCES,0.35271317829457366,"Contrary to traditional VI, we aim at deriving a variational family Q in the context of amortized in-
ference (Rezende & Mohamed, 2016; Cranmer et al., 2020). This means that, once an initial training
overhead has been “paid for”, our technique can readily be applied to a new data point. Amortized
inference is an active area of research in the context of Variational Auto Encoders (VAE) (Kingma
& Welling, 2014; Wu et al., 2019; Shu et al., 2018; Iakovleva et al., 2020). It is also a the original
setup of normalizing flows (NF) (Rezende & Mohamed, 2016; Radev et al., 2020), our technology
of choice. From this amortized starting point, Cranmer et al. (2020); Papamakarios et al. (2019b);
Thomas et al. (2020); Greenberg et al. (2019) have notably developed sequential techniques, refining
a posterior -and losing amortization- across several rounds of simulation. To streamline our contri-
bution, we chose not build upon that research, and rather focus on the amortized implementation of
normalizing flows. But we argue that our contribution is actually rather orthogonal to those: similar
to Ambrogioni et al. (2021b) we propose a principled and automated way to combine several density
estimators in a hierarchical structure. As such, our methods could be applied to a different class of
estimators such as VAEs (Kingma & Welling, 2014). We could leverage the SBI techniques and ex-
tend our work into a sequential version through the reparameterization of our conditional estimators
qi (see section 2.2). Ultimately, our method is not meant as an alternative to SBI, but a complement
to it for the pyramidal class of problems described in section 2.1."
REFERENCES,0.3546511627906977,"We choose to posit ourselves as an amortized technique. Yet, in our target experiment from Kong
et al. (2019) (see section 3.5), the inference is performed on a specific data point. An amortized
method could therefore appear as a more natural option. What’s more, it is generally admitted that
amortized inference implies an amortization gap from the true posterior, which accumulates on top
of the approximation gap that depends on the expressivity of the considered variational family. This
amortization gap further reduces the quality of the approximate posterior for a given data point."
REFERENCES,0.35658914728682173,"Our experimental experience on the example in section 3.4 however makes us put forth the value that
can be obtained from sharing learning across multiple examples, as amortization entitles (Cranmer
et al., 2020). Specifically, we encountered less issues related to local minima of the loss, a canonical
issue for MF-VI (Blei et al., 2017) that is for instance illustrated in our supplemental material. We
would therefore argue against the intuition that a (locally) amortized technique is necessarily waste-
ful in the context of a single data point. However, as the results in table 2 and table 1 underline, there
is much work to be done for amortized technique to reach the performance consistency and train-
ing time of amortized techniques, especially in high dimension, where exponentially more training
examples can be necessary to estimate densities properly (Donoho, 2000)."
REFERENCES,0.35852713178294576,"Specializing for a local parameter regime -as sequential method entitles (Cranmer et al., 2020)-
could therefore make us benefit from amortization without too steep an upfront training cost."
REFERENCES,0.36046511627906974,"B.2
EXTENSION TO A BROADER CLASS OF SIMULATORS"
REFERENCES,0.3624031007751938,"The presence of exchangeability in a problem’s data structure is not tied to the explicit modelling of
a problem as a HBM: Zaheer et al. (2018) rather describe this property as an permutation invariance
present in the studied data. As a consequence, though our derivation is based on HBMs, we believe
that the working principle of our method could be applied to a broader class of simulators featuring
exchangeability. Our reliance on HBMs is in fact only tied to our usage of the reverse KL loss (see
section 2.3), a readily modifiable implementation detail."
REFERENCES,0.3643410852713178,"In this work, we restrict ourselves to the pyramidal class of Bayesian networks (see section 2.1).
Going further, this class of models could be extended to cover more and more use-cases. This
bottom-up approach stands at opposite ends from the generic approach of SBI techniques (Cranmer
et al., 2020). But, as our target experiment in 3.5 demonstrates, we argue that in the long run this
bottom-up approach could result in more scalable and efficient architectures, applicable to challeng-
ing setups such as neuroimaging."
REFERENCES,0.36627906976744184,"B.3
RELEVANCE OF LIKELIHOOD-FREE METHODS IN THE PRESENCE OF A LIKELIHOOD"
REFERENCES,0.3682170542635659,"As part of our benchmark, we made the choice to include likelihood-free methods (NPE-C and
SNPE-C), based on a forward KL loss. In our supplemental material (appendix C.4) we also study
the implementation of our method using a forward KL loss."
REFERENCES,0.3701550387596899,"There is a general belief in the research community that likelihood-free methods are not intended
to be as competitive as likelihood-based methods in the presence of a likelihood (Cranmer et al.,
2020). In this manuscript, we tried to provide quantitative results to nourish this debate. We would
argue that likelihood-free methods generally scaled poorly to high dimensions (section 3.3). The
result of the Gaussian Mixture experiment also shows poorer performance in a multi-modal case,
but we would argue that the performance drop of likelihood-free methods is actually largely due
to the label switching problem (see appendix D.2). On the other hand, likelihood-free methods are
dramatically faster to train and can perform on par with likelihood-based methods in examples such
as the Gaussian Random Effects for G = 3 (see table 2). Depending on the problem at hand, it is
therefore not straightforward to systematically disregard likelihood-free methods."
REFERENCES,0.37209302325581395,"As an opening, there maybe is more at the intersection between likelihood-free and likelihood-based
methods than meets the eye. The symmetric loss introduced by Weilbach et al. (2020) stands as a
fruitful example of that connection."
REFERENCES,0.374031007751938,"B.4
INFERENCE OVER A SUBSET OF THE LATENT PARAMETERS"
REFERENCES,0.375968992248062,"Depending on the downstream tasks, out of all the parameters θ, an experimenter could only be
interested in the inference of a subset Θ1. Decomposing θ = Θ1 ∪Θ2, the goal would be to derive
a variational distribution q1(Θ1) instead of the distribution q(Θ1, Θ2)."
REFERENCES,0.37790697674418605,"Reverse KL setup
We first consider the reverse KL setup. The original ELBO maximized as part
of inference is equal to:
ELBO(q) = log p(X) −KL[q(Θ1, Θ2)||p(Θ1, Θ2|X)]
= Eq[log p(X, Θ1, Θ2) −log q(Θ1, Θ2)]
(B.2)"
REFERENCES,0.3798449612403101,"To keep working with normalized distributions, we get a similar expression for the inference of Θ1
only via:
ELBO(q1) = Eq1[log p(X, Θ1) −log q1(Θ1)]
(B.3)
In this expression, p(X, Θ1) is unknown: it results from the marginalization of Θ2 in p, which is
non-trivial to obtain, even via a Monte Carlo scheme. As a consequence, working with the reverse
KL does not allow for the inference over a subset of latent parameters."
REFERENCES,0.3817829457364341,"Forward KL setup
Contrary to reverse KL, in the forward KL setup the evaluation of p is not
required. Instead, the variational family is trained using samples (θ, X) from the joint distribution
p. In this setup, inference can be directly restricted over the parameter subset Θ1. Effectively,
one wouldn’t have to construct density estimators for the parameters Θ2, and the latter would be
marginalized in the obtained variational distribution q(Θ1). However, as our experiments point
out (section 3.3, section 3.4), likelihood-free training can be less competitive in large data regimes
or complex inference problems. As a consequence, even if this permits inference over only the
parameters of interest, switching to a forward KL loss can be inconvenient."
REFERENCES,0.38372093023255816,"B.5
EMBEDDING SIZE FOR THE HIERARCHICAL ENCODER"
REFERENCES,0.3856589147286822,"An important hyper-parameter in our architecture is the embedding size for the Set Transformer
(ST) architecture (Lee et al., 2019). The impact of the embedding size for a single ST has already
been studied in Lee et al. (2019), as a consequence we didn’t devote any experiments to the study of
the impact of this hyper-parameter."
REFERENCES,0.3875968992248062,"However, our architecture stacks multiple ST networks, and the evolution of the embedding size
with the hierarchy could be an interesting subject:"
REFERENCES,0.38953488372093026,"• it is our understanding that the embedding size for the encoding Eh should be increasing
with:"
REFERENCES,0.39147286821705424,"– the number of latent RVs θ whose inference depends on Eh, i.e. the latent RVs of
hierarchy h
– the dimensionality of the latent RVs θ of hierarchy h
– the complexity of the inference problem at hand, for instance how many statistical
moments need to be computed from i.i.d data points"
REFERENCES,0.39341085271317827,"• experimentally, we kept the embedding size constant across hierarchies, and fixed this con-
stant value based on the aforementioned criteria (see appendix F.2). This approach is prob-
ably conservative and drives up the number of weights in HE"
REFERENCES,0.3953488372093023,"• higher-hierarchy encodings are constructed from sets of lower-hierarchy encodings. Should
the embedding size vary, it would be important not to ”bottleneck” the information col-
lected at low hierarchies, even if the aforementioned criteria would argue for a low embed-
ding size."
REFERENCES,0.39728682170542634,"There would be probably experimental interest in deriving algorithms estimating the optimal em-
bedding size at different hierarchies. We leave this to future work."
REFERENCES,0.3992248062015504,"B.6
BOUNDS FOR ADAVI’S INFERENCE PERFORMANCE"
REFERENCES,0.4011627906976744,"When considering an amortized variational family, the non-amortized family with the same para-
metric form can be considered as an upper bound for the inference performance -as measured by the
ELBO. Indeed, considering the fixed parametric family q(θ; Ψ), for a given data point X1 the best
performance can be obtained by freely setting up the Ψ1 parameters. Instead setting Ψ1 = f(X1)
-amortizing the inference- can only result in worst performance. On the other hand the parameters
for another data point X2 can then readily be obtained via Ψ2 = f(X2) (Cremer et al., 2018)."
REFERENCES,0.40310077519379844,"In a similar fashion, it can be useful to look for upper bounds for ADAVI’s performance. This is
notably useful to compare ADAVI to traditional MF-VI (Blei et al., 2017):"
REFERENCES,0.4050387596899225,"1. Base scenario: traditional MF-VI In traditional MF-VI, the variational distribution is
qMF-VI = Q"
REFERENCES,0.4069767441860465,"i qPrior’s parametric form
i
(θi):"
REFERENCES,0.40891472868217055,"• qPrior’s parametric form
i
can for instance be a Gaussian with parametric mean and variance;
• in non-conjugate cases, using the prior’s parametric form can result in poor perfor-
mance due to an approximation gap, as seen in section 3.2;
• due to the difference in expressivity introduced by normalizing flows, except in con-
jugate cases, qMF-VI is not an upper bound for ADAVI’s performance."
REFERENCES,0.4108527131782946,"2. Superior upper limit scenario: normalizing flows using the Mean Field approximation
A family more expressive then qMF-VI can be obtained via a collection of normalizing flows
combined using the mean field approximation: qMF-NF = Q"
REFERENCES,0.4127906976744186,"i qNormalizing flow
i
(θi):"
REFERENCES,0.41472868217054265,"• every
individual
qNormalizing flow
i
is
more
expressive
than
the
corresponding
qPrior’s parametric form
i
: in a non-conjugate case it would provide better performance (Pa-
pamakarios et al., 2019a);
• since the mean field approximation treats the inference over each θi as a separate
problem, the resulting distribution qMF-NF is more expressive than qMF-VI;"
REFERENCES,0.4166666666666667,"• consider a plate-enriched DAG (Koller & Friedman, 2009), a template RV θi, and θj
i
with j = 1 . . . Card(P) the corresponding ground RVs. In qMF-NF, every θj
i would be
associated to a separate normalizing flow;
• consequently, the parameterization of qMF-NF is linear with respect to Card(P). This is
less than the quadratic scaling of TLSF or NPE-C -as explained in section 2.2 and ap-
pendix F.2. But this scaling still makes qMF-NF not adapted to large plate cardinalities,
all the more since the added number of weights -corresponding to a full normalizing
flow- per θj
i is high;"
REFERENCES,0.4186046511627907,"• this scaling is similar to the one of Cascading Flows (Ambrogioni et al., 2021b): CF
can be considered as the improvement of qMF-NF with statistical dependencies between
the qi;
• as far as we know, the literature doesn’t feature instances of the qMF-NF architecture.
Though straightforward, the introduction of normalizing flows in a variational family
is non-trivial, and for instance marks the main difference between CF and its prede-
cessor ASVI (Ambrogioni et al., 2021a)."
REFERENCES,0.42054263565891475,"3. Inferior upper limit scenario: non-amortized ADAVI At this point, it is useful to con-
sider the non-existent architecture qADAVI-NA:"
REFERENCES,0.42248062015503873,"• compared to qMF-NF, considering the ground RVs θj
i corresponding to the template
RV θi, each θj
i would no longer correspond to a different normalizing flow, but to the
same conditional normalizing flow;"
REFERENCES,0.42441860465116277,"• each θj
i would then be associated to a separate independent encoding vector. There
wouldn’t be a need for our Hierarchical Encoder anymore -as referenced to in sec-
tion 2.2;
• as for qMF-NF, the parameterization of qADAVI-NA would scale linearly with Card(P).
Each new θj
i would only necessitate an additional embedding vector, which would
make qADAVI-NA more adapted to high plate cardinalities than qMF-NF or CF;"
REFERENCES,0.4263565891472868,"• using separate flows for the θj
i instead of a shared conditional flow, qMF-NF can be
considered as an upper bound for qADAVI-NA’s performance;
• due to the amortization gap, qADAVI-NA can be considered as an upper bound for
ADAVI’s performance.
By transitivity, qMF-NF is then an even higher bound for
ADAVI’s performance."
REFERENCES,0.42829457364341084,"It is to be noted that amongst the architectures presented above, ADAVI is the only architecture with
a parameterization invariant to the plate cardinalities. This brings the advantage to theoretically
being able to use ADAVI on plates of any cardinality, as seen in eq. (4). In that sense, our main
claim is tied to the amortization of our variational family, though the linear scaling of qADAVI-NA
could probably be acceptable for reasonable plate cardinalities."
REFERENCES,0.43023255813953487,"C
COMPLEMENTS TO THE GAUSSIAN RANDOM EFFECTS EXPERIMENT:
HYPERPARAMETER ANALYSIS"
REFERENCES,0.4321705426356589,"This section features additional results on the experiment described in eq. (6) with G = 3 groups.
We present results of practical value, mostly related to hyperparameters."
REFERENCES,0.43410852713178294,"C.1
DESCRIPTORS, INPUTS TO ADAVI"
REFERENCES,0.436046511627907,"We can analyse the model described in eq. (6) using the descriptors defined in eq. (A.1). Those
descriptors constitute the inputs our methodology needs to automatically derive the dual architecture
from the generative HBM:"
REFERENCES,0.437984496124031,"V = {µ, M G, X}
P = {P0, P1}
Card = {P0 7→N, P1 7→G}"
REFERENCES,0.43992248062015504,"Hier = {µ 7→2, M G 7→1, X 7→0}"
REFERENCES,0.4418604651162791,"Shape = {µ 7→(D, ), M G 7→(D, ), X 7→(D, )}"
REFERENCES,0.4437984496124031,"Link = {µ 7→Identity, M G 7→Identity, X 7→Identity} (C.4)"
REFERENCES,0.44573643410852715,"C.2
TABULAR RESULTS FOR THE SCALING EXPERIMENT"
REFERENCES,0.4476744186046512,A tabular representation of the results presented in Fig. 3 can be seen in table 2.
REFERENCES,0.4496124031007752,"G
Type
Method
ELBO (102)
# weights
Inf. (s)
Amo. (s)"
"GRD TRUTH PROXY
MF-VI",0.45155038759689925,"3
Grd truth proxy
MF-VI
2.45 (± 0.15)
10
5
-"
"GRD TRUTH PROXY
MF-VI",0.45348837209302323,"Non amortized
SNPE-C
2.17 (± 33)
45,000
53,000
-
TLSF-NA
2.33 (± 0.20)
18,000
80
-
CF-NA
2.12 (± 0.15)
15,000
190
-"
"GRD TRUTH PROXY
MF-VI",0.45542635658914726,"Amortized
NPE-C
2.33 (± 0.15)
12,000
-
920
TLSF-A
2.37 (± 0.072)
12,000
-
9,400
CF-A
2.36 (± 0.029)
16,000
-
7,400
ADAVI
2.25 (± 0.14)
12,000
-
11,000
30
Grd truth proxy
MF-VI
24.4 (± 0.41)
64
18
-"
"GRD TRUTH PROXY
MF-VI",0.4573643410852713,"Non amortized
SNPE-C
-187 (± 110)
140,000
320,000
-
TLSF-NA
24.0 (± 0.49)
63,000
400
-
CF-NA
21,2 (± 0.40)
150,000
1,800
-"
"GRD TRUTH PROXY
MF-VI",0.45930232558139533,"Amortized
NPE-C
23.6 (± 50)
68,000
-
6,000
TLSF-A
22.7 (± 13)
68,000
-
130,000
CF-A
23.8 (± 0.06)
490,000
-
68,000
ADAVI
23.2 (± 0.89)
12,000
-
140,000
300
Grd truth proxy
MF-VI
244 (± 1.3)
600
240
-"
"GRD TRUTH PROXY
MF-VI",0.46124031007751937,"Non amortized
SNPE-C
-9,630 (± 3,500)
1,100,000
3,100,000
-
TLSF-NA
243 (± 1.8)
960,000
5,300
-
CF-NA
212 (± 1.5)
1,500,000
30,000
-"
"GRD TRUTH PROXY
MF-VI",0.4631782945736434,"Amortized
NPE-C
195 (±3 × 106)1
3,200,000
-
72,000
TLSF-A
202 (± 120)
3,200,000
-
2,800,000
CF-A
238 (± 0.1)
4,900,000
-
580,000
ADAVI
224 (± 9.4)
12,000
-
1,300,000"
"GRD TRUTH PROXY
MF-VI",0.46511627906976744,"Table 2: Scaling comparison on the Gaussian random effects example (see Fig. 2b-GRE). Methods
are ran over 20 random seeds (Except for SNPE-C and TLSF: to limit computational resources us-
age, those non-amortized computationally intensive methods were only ran on 5 seeds per sample,
for a number of effective runs of 100). Are compared: from left to right ELBO median (higher
is better) and standard deviation (ELBO for all techniques except for Cascading Flows, for which
ELBO is the numerically comparable augmented ELBO (Ranganath et al., 2016)); number of train-
able parameters (weights) in the model; for non-amortized techniques: CPU inference time for one
example (seconds); for amortized techniques: CPU amortization time (seconds).
1- Results for
NPE-C are extremely unstable, with multiple NaN results: the median value is rather random and
not necessarily indicative of a good performance"
"GRD TRUTH PROXY
MF-VI",0.46705426356589147,"C.3
DERIVATION OF AN ANALYTIC POSTERIOR"
"GRD TRUTH PROXY
MF-VI",0.4689922480620155,"To have a ground truth to which we can compare our methods results, we derive the following
analytic posterior distributions. Assuming we know σµ, σg, σx:"
"GRD TRUTH PROXY
MF-VI",0.47093023255813954,"ˆµg = 1 N N
X"
"GRD TRUTH PROXY
MF-VI",0.4728682170542636,"n=1
xg
n
(C.5a)"
"GRD TRUTH PROXY
MF-VI",0.4748062015503876,"˜µg|ˆµg ∼N

ˆµg, σ2
x
N IdD"
"GRD TRUTH PROXY
MF-VI",0.47674418604651164,"
(C.5b)"
"GRD TRUTH PROXY
MF-VI",0.4786821705426357,"ˆµ = 1 G G
X"
"GRD TRUTH PROXY
MF-VI",0.4806201550387597,"g=1
ˆµg
(C.5c)"
"GRD TRUTH PROXY
MF-VI",0.48255813953488375,˜µ|ˆµ ∼N
"GRD TRUTH PROXY
MF-VI",0.4844961240310077,"G
σ2g ˆµ"
"GRD TRUTH PROXY
MF-VI",0.48643410852713176,"1
σ2µ + G"
"GRD TRUTH PROXY
MF-VI",0.4883720930232558,"σ2g
,
1
1
σ2µ + G"
"GRD TRUTH PROXY
MF-VI",0.4903100775193798,"σ2g
IdD !"
"GRD TRUTH PROXY
MF-VI",0.49224806201550386,(C.5d)
"GRD TRUTH PROXY
MF-VI",0.4941860465116279,"Where in equation C.5b we neglect the influence of the prior (against the evidence) on the posterior
in light of the large number of points drawn from the distribution. We note that this analytical
posterior is conjugate, as argued in section 3.2."
"GRD TRUTH PROXY
MF-VI",0.49612403100775193,"C.4
TRAINING LOSSES FULL DERIVATION AND COMPARISON"
"GRD TRUTH PROXY
MF-VI",0.49806201550387597,"Full formal derivation
Following the nomenclature introduced in Papamakarios et al. (2019a),
there are 2 different ways in which we could train our variational distribution:"
"GRD TRUTH PROXY
MF-VI",0.5,"• using a forward KL divergence, benefiting from the fact that we can sample from our
generative model to produce a dataset {(θm, Xm)}m=1...M, θm ∼p(θ), Xm ∼p(X|θ).
This is the loss used in most of the SBI literature (Cranmer et al., 2020), as those are based
around the possibility to be likelihood-free, and have a target density p only implicitly
defined by a simulator:"
"GRD TRUTH PROXY
MF-VI",0.501937984496124,"Ψ⋆= arg min
Ψ EX∼p(X)[KL(p(θ|X)||qΨ(θ|X)]"
"GRD TRUTH PROXY
MF-VI",0.5038759689922481,"= arg min
Ψ EX∼p(X)[Eθ∼p(θ|X)[log p(θ|X) −log qΨ(θ|X)]]"
"GRD TRUTH PROXY
MF-VI",0.5058139534883721,"= arg min
Ψ EX∼p(X)[Eθ∼p(θ|X)[−log qΨ(θ|X)]]"
"GRD TRUTH PROXY
MF-VI",0.5077519379844961,"= arg min
Ψ"
"GRD TRUTH PROXY
MF-VI",0.5096899224806202,"Z
p(X)
 Z
−p(θ|X) log qΨ(θ|X)dθ

dX"
"GRD TRUTH PROXY
MF-VI",0.5116279069767442,"= arg min
Ψ"
"GRD TRUTH PROXY
MF-VI",0.5135658914728682,"Z Z
−p(X, θ) log qΨ(θ|X)dθdX"
"GRD TRUTH PROXY
MF-VI",0.5155038759689923,"≈arg min
Ψ
1
M × M
X"
"GRD TRUTH PROXY
MF-VI",0.5174418604651163,"m=1
−log qΨ(θm|Xm)"
"GRD TRUTH PROXY
MF-VI",0.5193798449612403,"where θm ∼p(θ), Xm ∼p(X|θ) (C.6)"
"GRD TRUTH PROXY
MF-VI",0.5213178294573644,"• using a reverse KL divergence, benefiting from the access to a target joint density p(X, θ).
The reverse KL loss is an amortized version of the classical ELBO expression (Blei et al.,
2017). For training, one only needs to have access to a dataset {Xm}m=1...M, Xm ∼p(X)
of points drawn from the generative HBM of interest. Indeed, the θm points are sampled
from the variational distribution:
Ψ⋆= arg min
Ψ EX∼p(X)[KL(qΨ(θ|X)||p(θ|X)]"
"GRD TRUTH PROXY
MF-VI",0.5232558139534884,"= arg min
Ψ EX∼p(X)[Eθ∼qΨ(θ|X)[log qΨ(θ|X) −log p(θ|X)]]"
"GRD TRUTH PROXY
MF-VI",0.5251937984496124,"= arg min
Ψ EX∼p(X)[Eθ∼qΨ(θ|X)[log qΨ(θ|X) −log p(X, θ) + log p(X)]]"
"GRD TRUTH PROXY
MF-VI",0.5271317829457365,"= arg min
Ψ EX∼p(X)[Eθ∼qΨ(θ|X)[log qΨ(θ|X) −log p(X, θ)]]"
"GRD TRUTH PROXY
MF-VI",0.5290697674418605,"= arg min
Ψ"
"GRD TRUTH PROXY
MF-VI",0.5310077519379846,"Z
p(X)
 Z
qΨ(θ|X)[log qΨ(θ|X) −log p(X, θ)]dθ

dX"
"GRD TRUTH PROXY
MF-VI",0.5329457364341085,"= arg min
Ψ"
"GRD TRUTH PROXY
MF-VI",0.5348837209302325,"Z Z
p(X)qΨ(θ|X)[log qΨ(θ|X) −log p(X, θ)]dθdX"
"GRD TRUTH PROXY
MF-VI",0.5368217054263565,"≈arg min
Ψ
1
M × M
X"
"GRD TRUTH PROXY
MF-VI",0.5387596899224806,"m=1
log qΨ(θm|Xm) −log p(Xm, θm)"
"GRD TRUTH PROXY
MF-VI",0.5406976744186046,"where Xm ∼p(X), θm ∼qΨ(θ|X) (C.7)"
"GRD TRUTH PROXY
MF-VI",0.5426356589147286,"As it more uniquely fits our setup and provided better results experimentally, we chose to focus on
the usage of the reverse KL divergence. During our experiments, we also tested the usage of the
unregularized ELBO loss:"
"GRD TRUTH PROXY
MF-VI",0.5445736434108527,"Ψ⋆= arg min
Ψ
1
M × M
X"
"GRD TRUTH PROXY
MF-VI",0.5465116279069767,"m=1
−log p(Xm, θm)"
"GRD TRUTH PROXY
MF-VI",0.5484496124031008,"where Xm ∼p(X), θm ∼qΨ(θ|X) (C.8)"
"GRD TRUTH PROXY
MF-VI",0.5503875968992248,"Mean of analytical KL divergences
from the theoretical posterior (low is good)"
"GRD TRUTH PROXY
MF-VI",0.5523255813953488,"Early stopping
After convergence"
"GRD TRUTH PROXY
MF-VI",0.5542635658914729,"Loss
NaN runs
Mean
Std
Mean
Std"
"GRD TRUTH PROXY
MF-VI",0.5562015503875969,"forward KL
0
3847.7
5210.4
2855.3
4248.1
unregularized ELBO
0
6.6
0.7
6.2
0.9
reverse KL
2
12.3
19.8
3.0
4.1"
"GRD TRUTH PROXY
MF-VI",0.5581395348837209,"Table 3: Convergence of the variational posterior to the analytical posterior over an early stopped
training (200 batches) and after convergence (1000 batches) for the Gaussian random effects exam-
ple"
"GRD TRUTH PROXY
MF-VI",0.560077519379845,"This formula differs from the one of the reverse KL loss by the absence of the term qΨ(θm|Xm),
and is a converse formula to the one of the forward KL (in the sense that it permutes the roles of q
and p)."
"GRD TRUTH PROXY
MF-VI",0.562015503875969,"Intuitively, it posits our architecture as a pure sampling distribution that aims at producing points
θm in regions of high joint density p. In that sense, it acts as a first moment approximation for the
target posterior distribution (akin to MAP parameter regression). Experimentally, the usage of the
unregularized ELBO loss provided fast convergence to a mode of the posterior distribution, with
very low variance for the variational approximation."
"GRD TRUTH PROXY
MF-VI",0.563953488372093,"We argue the possibility to use the unregularized ELBO loss as a warm-up before switching to the
reverse KL loss, with the latter considered here as a regularization of the former. We introduce this
training strategy as an example of the modularity of our approach, where one could transfer the rapid
learning from one task (amortized mode finding) to another task (amortized posterior estimation)."
"GRD TRUTH PROXY
MF-VI",0.5658914728682171,"Graphical comparison
In Figure C.1 we analyse the influence of these 3 different losses on the
training of our posterior distribution, compared to the analytical ground truth. This example is
typical of the relative behaviors induced on the variational distributions by each loss:"
"GRD TRUTH PROXY
MF-VI",0.5678294573643411,"• The forward KL provides very erratic training, and results after several dozen epochs (sev-
eral minutes) with a careful early stopping in posteriors with too large variance."
"GRD TRUTH PROXY
MF-VI",0.5697674418604651,"• The unregularized ELBO loss converges in less then 3 epochs (a couple dozen seconds),
and provides posteriors with very low variance, concentrated on the MAP estimates of their
respective parameters."
"GRD TRUTH PROXY
MF-VI",0.5717054263565892,"• The reverse KL converges in less 10 epochs (less than 3 minutes) and provides relevant
variance."
"GRD TRUTH PROXY
MF-VI",0.5736434108527132,"Losses convergence speed
We analyse the relative convergence speed of our variational posterior
to the analytical one (derived in eq. (C.5a)) when using the 3 aforementioned losses for training.
To measure the convergence, we compute analytically the KL divergence between the variational
posterior and the analytical one (every distribution being a Gaussian), summed for every distribution,
and averaged over a validation dataset of size 2000."
"GRD TRUTH PROXY
MF-VI",0.5755813953488372,"We use a training dataset of size 2000, and for each loss repeated the training 20 times (batch size 10,
10 θm samples per Xm) for 10 epochs, resulting in 200 optimizer calls. This voluntary low number
allows us to asses how close is the variational posterior to the analytical posterior after only a brief
training. Results are visible in appendix C.4, showing a faster convergence for the unregularized
ELBO. After 800 more optimizer calls, the tendency gets inverted and the reverse KL loss appears
as the superior loss (though we still notice a larger variance)."
"GRD TRUTH PROXY
MF-VI",0.5775193798449613,"The large variance in the results may point towards the need for adapted training strategies involving
Learning rate decay and/or scheduling (Kucukelbir et al., 2016), an extension that we leave for future
work."
"GRD TRUTH PROXY
MF-VI",0.5794573643410853,"0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00 1.75 1.50 1.25 1.00 0.75 0.50 0.25 0.00"
"GRD TRUTH PROXY
MF-VI",0.5813953488372093,Example 0 Data
"GRD TRUTH PROXY
MF-VI",0.5833333333333334,"0.5
0.0
0.5
1.0 1.5 1.0 0.5 0.0"
"GRD TRUTH PROXY
MF-VI",0.5852713178294574,forward KL
"GRD TRUTH PROXY
MF-VI",0.5872093023255814,"0.5
0.0
0.5
1.0 1.5 1.0 0.5 0.0"
"GRD TRUTH PROXY
MF-VI",0.5891472868217055,unregularized ELBO
"GRD TRUTH PROXY
MF-VI",0.5910852713178295,"0.5
0.0
0.5
1.0 1.5 1.0 0.5 0.0"
"GRD TRUTH PROXY
MF-VI",0.5930232558139535,reverse KL
"GRD TRUTH PROXY
MF-VI",0.5949612403100775,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
1.50 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.50"
"GRD TRUTH PROXY
MF-VI",0.5968992248062015,Example 1
"GRD TRUTH PROXY
MF-VI",0.5988372093023255,"0.0
0.5
1.0
1.5
1.5 1.0 0.5 0.0 0.5"
"GRD TRUTH PROXY
MF-VI",0.6007751937984496,"0.0
0.5
1.0
1.5
1.5 1.0 0.5 0.0 0.5"
"GRD TRUTH PROXY
MF-VI",0.6027131782945736,"0.0
0.5
1.0
1.5
1.5 1.0 0.5 0.0 0.5"
"GRD TRUTH PROXY
MF-VI",0.6046511627906976,"1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75 2.00 1.75 1.50 1.25 1.00 0.75 0.50 0.25"
"GRD TRUTH PROXY
MF-VI",0.6065891472868217,Example 2
"GRD TRUTH PROXY
MF-VI",0.6085271317829457,"1.0
0.5
0.0
0.5
2.0 1.5 1.0 0.5"
"GRD TRUTH PROXY
MF-VI",0.6104651162790697,"1.0
0.5
0.0
0.5
2.0 1.5 1.0 0.5"
"GRD TRUTH PROXY
MF-VI",0.6124031007751938,"1.0
0.5
0.0
0.5
2.0 1.5 1.0 0.5"
"GRD TRUTH PROXY
MF-VI",0.6143410852713178,"Figure C.1: Graphical results on the Gaussian random effects example for our architecture trained
using 3 different losses. Rows represent 3 different data points. Left column represents data, with
colors representing 3 different groups. Other columns represent posterior samples for µ (black) and
µ1, µ2, µ3. Visually, posterior samples µ1, µ2, µ3 should be concentrated around the mean of the
data points with the same color, and the black points µ should be repartitioned around the mean of
the 3 group means (with a shift towards 0 due to the prior). Associated with the posterior samples
are analytical solutions (thin black circles), centered on the analytical MAP point, and whose radius
correspond to 2 times the standard deviation of the analytical posterior: 95 % of the draws from a
posterior should fall within the corresponding circle."
"GRD TRUTH PROXY
MF-VI",0.6162790697674418,"C.5
MONTE CARLO APPROXIMATION FOR THE GRADIENTS AND COMPUTATIONAL BUDGET
COMPARISON"
"GRD TRUTH PROXY
MF-VI",0.6182170542635659,"In section 2.3, for the reverse KL loss, we approximate expectations using Monte Carlo integration.
We further train our architecture using minibatch gradient descent, as opposed to stochastic gradient
descent as proposed by Kucukelbir et al. (2016). An interesting hyper-parametrization of our system
resides in the effective batch size of our training, that depends upon:"
"GRD TRUTH PROXY
MF-VI",0.6201550387596899,"• the size of the mini batches, determining the number of Xm points considered in parallel"
"GRD TRUTH PROXY
MF-VI",0.622093023255814,"• the number of θm draws per Xm point, that we use to approximate the gradient in the
ELBO"
"GRD TRUTH PROXY
MF-VI",0.624031007751938,"More formally, we define a computational budget as the relative allocation of a constant effective
batch size batch size × θ draws per X between batch size and θ draws per X."
"GRD TRUTH PROXY
MF-VI",0.625968992248062,"To analyse the effect of the computational budget on training, we use a dataset of size 1000, and run
experiment 20 times over the same number of optimizer calls with the same effective batch size per
call. Results can be seen in Fig. C.2. From this experiment we can draw the following conclusions:"
"GRD TRUTH PROXY
MF-VI",0.627906976744186,"• we didn’t witness massive difference in the global convergence speed across computational
budgets"
"GRD TRUTH PROXY
MF-VI",0.6298449612403101,"• the bigger the budget we allocate to the sampling of multiple θm per point Xm (effectively
going towards a stochastic training in terms of the points Xm), the more erratic is the loss
evolution"
"GRD TRUTH PROXY
MF-VI",0.6317829457364341,"• the bigger the budget we allocate to the Xm batch size, the more stable is the loss evolution,
but our interpretation is that the resulting reduced number of θm draws per Xm augments
the risk of an instability resulting in a NaN run"
"GRD TRUTH PROXY
MF-VI",0.6337209302325582,"Experimentally, we obtained the best results by evenly allocating our budget to the Xm batch size
and the number of θm draws per Xm point (typically, 32 and 32 respectively for an effective batch
size of 1024). Overall, in the amortized setup, our experiment stand as a counterpoint to those of
Kucukelbir et al. (2016) who pointed towards the case of a single θm draw per point Xm as their
preferred hyper-parametrization."
"GRD TRUTH PROXY
MF-VI",0.6356589147286822,"D
COMPLEMENTS TO THE GAUSSIAN MIXTURE WITH RANDOM EFFECTS
EXPERIMENT: FURTHER POSTERIOR ANALYSIS"
"GRD TRUTH PROXY
MF-VI",0.6375968992248062,"This section is a complement to the experiment described in section 3.4, we thus consider the model
described in eq. (8a). We explore the complexity of the theoretical posterior for this experiment."
"GRD TRUTH PROXY
MF-VI",0.6395348837209303,"D.1
DESCRIPTORS, INPUTS TO ADAVI"
"GRD TRUTH PROXY
MF-VI",0.6414728682170543,"We can analyse the model described in eq. (8a) using the descriptors defined in eq. (A.1). Those
descriptors constitute the inputs our methodology needs to automatically derive the dual architecture"
"GRD TRUTH PROXY
MF-VI",0.6434108527131783,"20
30
40
50
60
70
batch 0 5000 10000 15000 20000 25000 30000"
"GRD TRUTH PROXY
MF-VI",0.6453488372093024,reverse KL loss
"GRD TRUTH PROXY
MF-VI",0.6472868217054264,Training loss
"GRD TRUTH PROXY
MF-VI",0.6492248062015504,"batch: 1, theta draws: 1000, NaN runs: 0
batch: 10, theta draws: 100, NaN runs: 0
batch: 100, theta draws: 10, NaN runs: 5
batch: 1000, theta draws: 1, NaN runs: 19"
"GRD TRUTH PROXY
MF-VI",0.6511627906976745,"0
200
400
600
800
1000
batch 0 10000 20000 30000 40000 50000 60000 70000"
"GRD TRUTH PROXY
MF-VI",0.6531007751937985,reverse KL loss
"GRD TRUTH PROXY
MF-VI",0.6550387596899225,Training loss
"GRD TRUTH PROXY
MF-VI",0.6569767441860465,"batch: 1, theta draws: 1000, NaN runs: 0
batch: 10, theta draws: 100, NaN runs: 0
batch: 100, theta draws: 10, NaN runs: 5
batch: 1000, theta draws: 1, NaN runs: 19"
"GRD TRUTH PROXY
MF-VI",0.6589147286821705,"Figure C.2: Loss evolution across batches for different computational budgets. All experiments are
designed so that to have the same number of optimizer calls (meaning that batch size × epochs =
1000) and the same effective batch size (meaning that batch size × θ draws per X = 1000). Every
experiment is run 20 times, error bands showing the standard deviation of the loss at the given time
point. Note that the blue line (batch size 1, 1000 θ draws per X) is more erratic than the other ones
(even after a large number of batches). On the other hand, the red line (batch size 1000, 1 θ draws
per X) is more stable, but 19 out of 20 runs ultimately resulted in an instability"
"GRD TRUTH PROXY
MF-VI",0.6608527131782945,"from the generative HBM:
V = {M L, M L,G, ΠG, X}
P = {P0, P1}
Card = {P0 7→N, P1 7→G}"
"GRD TRUTH PROXY
MF-VI",0.6627906976744186,"Hier = {M L 7→2, M L,G 7→1, ΠG 7→1, X 7→0}"
"GRD TRUTH PROXY
MF-VI",0.6647286821705426,"Shape = {M L 7→(L, D), M L,G 7→(L, D), ΠG 7→(L, ), X 7→(D, )}
Link = {"
"GRD TRUTH PROXY
MF-VI",0.6666666666666666,"M L 7→Reshape((LD, ) →(L, D)),"
"GRD TRUTH PROXY
MF-VI",0.6686046511627907,"M L,G 7→Reshape((LD, ) →(L, D)),"
"GRD TRUTH PROXY
MF-VI",0.6705426356589147,"ΠG 7→SoftmaxCentered((L −1, ) →(L, )),
X 7→Identity
} (D.9)"
"GRD TRUTH PROXY
MF-VI",0.6724806201550387,"For the definition of the SoftmaxCentered link function, see Dillon et al. (2017)."
"GRD TRUTH PROXY
MF-VI",0.6744186046511628,"D.2
THEORETICAL POSTERIOR RECOVERY IN THE GAUSSIAN MIXTURE RANDOM EFFECTS
MODEL"
"GRD TRUTH PROXY
MF-VI",0.6763565891472868,We further analyse the complexity of model described in section 3.4.
"GRD TRUTH PROXY
MF-VI",0.6782945736434108,"Due to the label switching problem (Jasra et al., 2005), the relative position of the L mixture com-
ponents in the D space is arbitrary. Consider a non-degenerate example like the one in Fig. D.3,
where the data points are well separated in 3 blobs (likely corresponding to the L = 3 mixture com-
ponents). Since there is no deterministic way to assign component l = 1 unequivocally to a blob
of points, the marginalized posterior distribution for the position of the component l = 1 should
be multi-modal, with -roughly- a mode placed at each one of the 3 blobs of points. This posterior
would be the same for the components l = 2 and l = 3. In truth, the posterior is even more complex
than this 3-mode simplification, especially when the mixture components are closer to each other in
2D (and the grouping of points into draws from a common component is less evident)."
"GRD TRUTH PROXY
MF-VI",0.6802325581395349,"In Fig. D.3, we note that our technique doesn’t recover this multi-modality in its posterior, and
instead assigns different posterior components to different blobs of points. Indeed, when plotting
only the posterior samples for the first recovered component l = 1, all points are concentrated
around the bottom-most blob, and not around each blob like the theoretical posterior would entail
(see Fig. D.3 second row)."
"GRD TRUTH PROXY
MF-VI",0.6821705426356589,"This behavior most likely represents a local minimum in the reverse KL loss that is common to many
inference techniques (for instance consider multiple non-mixing chains for MCMC in Jasra et al.,
2005). We note that training in forward KL wouldn’t provide such a flexibility in that setup, as it
would enforce the multi-modality of the posterior, even at the cost of an overall worst result (as it is
the case for NPE-C and SNPE-C in table 1. Indeed, let’s imagine that our training dataset features
M ′ draws similar to the one in Fig. D.3. Out of randomness, the labelling l of the 3 blobs of points
would be permuted across those M ′ examples. A forward-KL-trained density estimator would then
most likely attempt to model a multi-modal posterior."
"GRD TRUTH PROXY
MF-VI",0.6841085271317829,"Though it is not similar to the theoretical result, we argue that our result is of experimental value,
and close to the intuition one forms of the problem: using our results one can readily estimate
the original components for the mixture. Indeed, for argument’s sake, say we would recover the
theoretical, roughly trimodal posterior. To recover the original mixture components, one would
need to split the 3 modes of the posterior and arbitrarily assign a label l to each one of the modes.
In that sense, our posterior naturally features this splitting, and can be used directly to estimate the
L = 3 mixture components."
"GRD TRUTH PROXY
MF-VI",0.686046511627907,"E
COMPLEMENTS TO THE NEUROIMAGING EXPERIMENT"
"GRD TRUTH PROXY
MF-VI",0.687984496124031,"This section is a complement to the experiment described in section 3.5, we thus consider the model
described in eq. (E.10a) and eq. (E.10b). We present a toy dimension version of our experiment, use-"
"GRD TRUTH PROXY
MF-VI",0.689922480620155,"1
0
1
2
3 2 1 0 1 Data"
"GRD TRUTH PROXY
MF-VI",0.6918604651162791,"1
0
1
2
3 2 1 0 1 MF-VI"
"GRD TRUTH PROXY
MF-VI",0.6937984496124031,"1
0
1
2
3 2 1 0 1 CF-A"
"GRD TRUTH PROXY
MF-VI",0.6957364341085271,"1
0
1
2
3 2 1 0 1"
"GRD TRUTH PROXY
MF-VI",0.6976744186046512,"Full
posterior
sample ADAVI"
"GRD TRUTH PROXY
MF-VI",0.6996124031007752,"1
0
1
2
3 2 1 0 1"
"GRD TRUTH PROXY
MF-VI",0.7015503875968992,"1
0
1
2
3 2 1 0 1"
"GRD TRUTH PROXY
MF-VI",0.7034883720930233,"1
0
1
2
3 2 1 0 1"
"GRD TRUTH PROXY
MF-VI",0.7054263565891473,"Posterior
sample
component l="
"GRD TRUTH PROXY
MF-VI",0.7073643410852714,"0.5
1.0
1.5
2.0 0.5 0.0 0.5 1.0"
"GRD TRUTH PROXY
MF-VI",0.7093023255813954,"0.5
0.0
0.5
1.0 0.0 0.5 1.0 1.5"
"GRD TRUTH PROXY
MF-VI",0.7112403100775194,"1.2
1.0
0.8
0.6
0.4 3.0 2.8 2.6 2.4 2.2"
"GRD TRUTH PROXY
MF-VI",0.7131782945736435,"Posterior
sample
component l=
(zoom)"
"GRD TRUTH PROXY
MF-VI",0.7151162790697675,"Figure D.3: Graphical comparison for various methods on the Gaussian mixture with random effects
example. First column represents a non-degenerate data point, with colored points corresponding
to [x1,1, ..., x1,N], [x2,1, ..., x2,N], [x3,1, ..., x3,N]. Note the distribution of the points around in 3
multi-colored groups (population components), and 3 colored sub-groups per group (group com-
ponents). All other columns represent the posterior samples for population mixture components
µ1, . . . , µ3 (black) and group mixture components µ1
1, . . . , µ1
3, µ2
1, . . . , µ2
3, µ3
1, . . . , µ3
3. Sec-
ond column represents the results of a non-amortized MF-VI (best ELBO score across all random
seeds): results are typical of a local minimum for the loss. Third column represents the result of an
amortized CF-A (best amortized ELBO). Last column represents our amortized ADAVI technique
(best amortized ELBO). First row represents the full posterior samples. Second and third row only
represents the first mixture component samples (third row zooms in on the data). Notice how neither
technique recovers the actual multi-modality of the theoretical posterior. We obtain results of good
experimental value, usable to estimate likely population mixture components."
"GRD TRUTH PROXY
MF-VI",0.7170542635658915,"ful to build an intuition of the problem. We also present implementation details for our experiment,
and additional neuroimaging results."
"GRD TRUTH PROXY
MF-VI",0.7189922480620154,"E.1
NEUROIMAGING CONTEXT"
"GRD TRUTH PROXY
MF-VI",0.7209302325581395,"The main goal of Kong et al. (2019) is to address the classical problem in neuroscience of estimating
population commonalities along with individual characteristics. In our experiment, we are interested
in parcelling the region of left inferior frontal gyrus (IFG). Anatomically, the IFG is decomposed in
2 parts: pars opercularis and triangularis. Our aim is to reproduce this binary split from a functional
connectivity point of view, an open problem in neuroscience (see e.g. Heim et al., 2009)."
"GRD TRUTH PROXY
MF-VI",0.7228682170542635,"As Kong et al. (2019), we consider a population of S=30 subjects, each with T = 4 acquisition ses-
sions, from the Human Connectome Project dataset (Van Essen et al., 2012). The fMRI connectivity
between a cortical point and the rest of the brain, split in D = 1, 483 regions, is represented as a
vector of length D with each component quantifying the temporal correlation of blood-oxygenation
between the point and a region. A main hypothesis of Kong et al. (2019), and the fMRI field, is that
the fMRI connectivity of points belonging to the same parcel share a similar connectivity pattern
or correlation vector. Following Kong et al. (2019), we represent D-dimensional correlation vectors
as RVs on the positive quadrant of the D-dimensional unit-sphere. We do this efficiently assum-
ing they have a L-normal distribution, or Gaussian under the transformation of the link function
L(x) =
p"
"GRD TRUTH PROXY
MF-VI",0.7248062015503876,"SoftmaxCentered(x) (Dillon et al., 2017):"
"GRD TRUTH PROXY
MF-VI",0.7267441860465116,"π, s−, s+ = 2, −10, 8"
"GRD TRUTH PROXY
MF-VI",0.7286821705426356,"L−1(µg
l ) ∼N(⃗0D−1, Σg)
M L = [µg
l ]l=1...L"
"GRD TRUTH PROXY
MF-VI",0.7306201550387597,"log(ϵl) ∼U(s−, s+)
EL = [ϵl]l=1...L"
"GRD TRUTH PROXY
MF-VI",0.7325581395348837,"L−1(µs
l ) | µg
l , ϵl ∼N(L−1(µg
l ), ϵ2
l )
M L,S = [µs
l ]
l=1...L
s=1...S"
"GRD TRUTH PROXY
MF-VI",0.7344961240310077,"log(σl) ∼U(s−, s+)
ΣL = [σl]l=1...L"
"GRD TRUTH PROXY
MF-VI",0.7364341085271318,"L−1(µs,t
l ) | µs
l , σl ∼N(L−1(µs
l ), σ2
l )
M L,S,T = [µs,t
l ]
l=1...L
s=1...S
t=1...T
log(κ) ∼U(s−, s+)
Π ∼Dir([π] × L)"
"GRD TRUTH PROXY
MF-VI",0.7383720930232558,(E.10a)
"GRD TRUTH PROXY
MF-VI",0.7403100775193798,"L−1(Xs,t
n ) | [µs,t
1 , . . . , µs,t
L ], κ, Π ∼Mix(Π, [N(L−1(µs,t
1 ), κ2), . . . , N(L−1(µs,t
L ), κ2)])"
"GRD TRUTH PROXY
MF-VI",0.7422480620155039,"X = [Xs,t
n ]
s=1...S
t=1...T
n=1...N
(E.10b)
Our aim is therefore to identify L = 2 functional networks that would produce a functional par-
cellation of the studied IFG section. In this setting, the parameters θ of interest are the networks
µ. Instead of the complex EM computation derived in Kong et al. (2019), we perform full-posterior
inference for those parameters using our automatically derived architecture."
"GRD TRUTH PROXY
MF-VI",0.7441860465116279,"E.2
DESCRIPTORS, INPUTS TO ADAVI"
"GRD TRUTH PROXY
MF-VI",0.7461240310077519,"We can analyse the model described in eq. (E.10a) and eq. (E.10b) using the descriptors defined in
eq. (A.1). Those descriptors constitute the inputs our methodology needs to automatically derive the"
"GRD TRUTH PROXY
MF-VI",0.748062015503876,dual architecture from the generative HBM:
"GRD TRUTH PROXY
MF-VI",0.75,"V = {M L, EL, M L,S, ΣL, M L,S,T , κ, Π, X}
P = {P0, P1, P2}
Card = {P0 7→N, P1 7→T, P2 7→S}"
"GRD TRUTH PROXY
MF-VI",0.751937984496124,"Hier = {M L 7→3, EL 7→3, M L,S 7→2, ΣL 7→3, M L,S,T 7→1, κ 7→3, Π 7→3, X 7→0}
Shape = {"
"GRD TRUTH PROXY
MF-VI",0.7538759689922481,"M L 7→(L, D),"
"GRD TRUTH PROXY
MF-VI",0.7558139534883721,"EL 7→(L, ),"
"GRD TRUTH PROXY
MF-VI",0.7577519379844961,"M L,S 7→(L, D),"
"GRD TRUTH PROXY
MF-VI",0.7596899224806202,"ΣL 7→(L, ),"
"GRD TRUTH PROXY
MF-VI",0.7616279069767442,"M L,S,T 7→(L, D),
κ 7→(1, ),
Π 7→(L, ),
X 7→(D, )
}
Link = {"
"GRD TRUTH PROXY
MF-VI",0.7635658914728682,"M L 7→L ◦Reshape((LD, ) →(L, D)),"
"GRD TRUTH PROXY
MF-VI",0.7655038759689923,"EL 7→Exp,"
"GRD TRUTH PROXY
MF-VI",0.7674418604651163,"M L,S 7→L ◦Reshape((LD, ) →(L, D)),"
"GRD TRUTH PROXY
MF-VI",0.7693798449612403,"ΣL 7→Exp,"
"GRD TRUTH PROXY
MF-VI",0.7713178294573644,"M L,S,T 7→L ◦Reshape((LD, ) →(L, D)),
κ 7→Exp,
Π 7→SoftmaxCentered((L −1, ) →(L, )),
X 7→L
}
(E.11)"
"GRD TRUTH PROXY
MF-VI",0.7732558139534884,"E.3
EXPERIMENT ON MS-HBM MODEL ON TOY DIMENSIONS"
"GRD TRUTH PROXY
MF-VI",0.7751937984496124,"To get an intuition of the behavior of our architecture on the MS-HBM model, we consider the
following toy dimensions reproduction of the model:"
"GRD TRUTH PROXY
MF-VI",0.7771317829457365,"N, T, S, D, L = 50, 2, 2, 2, 2"
"GRD TRUTH PROXY
MF-VI",0.7790697674418605,"g−, g+ = −4, 4"
"GRD TRUTH PROXY
MF-VI",0.7810077519379846,"κ−, κ+, σ−, σ+, ϵ−, ϵ+ = −4, −4, −3, −3, −2, −2, −1
π = 2"
"GRD TRUTH PROXY
MF-VI",0.7829457364341085,"L−1(µg
l ) ∼U(−g−, g+)"
"GRD TRUTH PROXY
MF-VI",0.7848837209302325,"log(ϵl) ∼U(ϵ−, ϵ+)"
"GRD TRUTH PROXY
MF-VI",0.7868217054263565,"L−1(µs
l )|µg
l , ϵl ∼N(L−1(µg
l ), ϵ2
l )"
"GRD TRUTH PROXY
MF-VI",0.7887596899224806,"log(σl) ∼U(σ−, σ+)"
"GRD TRUTH PROXY
MF-VI",0.7906976744186046,"L−1(µs,t
l )|µs
l , σl ∼N(L−1(µs
l ), σ2
l )"
"GRD TRUTH PROXY
MF-VI",0.7926356589147286,"log(κ) ∼U(κ−, κ+)
Π ∼Dir([π] × L)"
"GRD TRUTH PROXY
MF-VI",0.7945736434108527,"L−1(Xs,t
n )|[µs,t
1 , . . . , µs,t
L ], κ, Π ∼Mix(Π, [N(L−1(µs,t
1 ), κ2), . . . , L(L−1(µs,t
L ), κ2)])"
"GRD TRUTH PROXY
MF-VI",0.7965116279069767,(E.12)
"GRD TRUTH PROXY
MF-VI",0.7984496124031008,"0.00
0.25
0.50
0.75
1.00 0.0 0.2 0.4 0.6 0.8 1.0"
"GRD TRUTH PROXY
MF-VI",0.8003875968992248,Example 0
"GRD TRUTH PROXY
MF-VI",0.8023255813953488,Data and posterior sample
"GRD TRUTH PROXY
MF-VI",0.8042635658914729,"0.00
0.25
0.50
0.75
1.00 0.0 0.2 0.4 0.6 0.8 1.0"
"GRD TRUTH PROXY
MF-VI",0.8062015503875969,Example 1
"GRD TRUTH PROXY
MF-VI",0.8081395348837209,"0.00
0.25
0.50
0.75
1.00 0.0 0.2 0.4 0.6 0.8 1.0"
"GRD TRUTH PROXY
MF-VI",0.810077519379845,Example 2
"GRD TRUTH PROXY
MF-VI",0.812015503875969,"Figure E.4: Visual representation of our results on a synthetic MS-HBM example. Data is rep-
resented as colored points on the unit positive quadrant, each color corresponding to a subject ×
session. Samples from posterior distributions are represented as concentric colored markings. Just
below the data points are µs,t samples. Then samples of µs. Then samples of µg (black lines). No-
tice how the µ posteriors are distributed around the angle bisector of the arc covered by the points at
the subsequent plate."
"GRD TRUTH PROXY
MF-VI",0.813953488372093,"The results can be visualized on Fig. E.4. This experiment shows the expressivity we gain from the
usage of link functions."
"GRD TRUTH PROXY
MF-VI",0.8158914728682171,"E.4
IMPLEMENTATION DETAILS FOR THE NEUROIMAGING MS-HBM EXAMPLE"
"GRD TRUTH PROXY
MF-VI",0.8178294573643411,"Main implementation differences with the original MS-HBM model
Our implementation of
the MS-HBM (eq. (E.10a) and eq. (E.10b)) contains several notable differences with the original
one from Kong et al. (2019):"
"GRD TRUTH PROXY
MF-VI",0.8197674418604651,"• we model µ distributions as Gaussians linked to the positive quadrant of the unit sphere via
the function L. In the orignal model, RVs are modelled using Von Mises Fisher distribu-
tions. Our choice allows us to express the entirety of the connectivity vectors (that only lie
on a portion of the unit sphere). However, we also acknowledge that the densities incurred
by the 2 distributions on the positive quadrant of the unit sphere are different."
"GRD TRUTH PROXY
MF-VI",0.8217054263565892,"• we forgo any spatial regularization, and also the assumption that the parcellation of a given
subject s should be constant across sessions t. This is to streamline our implementation.
Adding components to the loss optimized at training could inject those constraints back
into the model, but this was not the subject of our experiment, so we left those for future
work."
"GRD TRUTH PROXY
MF-VI",0.8236434108527132,"Data pre-processing and dimensionality reduction
Our model was able to run on the full di-
mensionality of the connectivity, D0 = 1483. However, we obtained better results experimentally
when further pre-processing the used data down to the dimension D1 = 141. The displayed results
in Fig. 4 are the ones resulting from this dimensionality reduction:"
"GRD TRUTH PROXY
MF-VI",0.8255813953488372,"1. we projected the (S, T, N, D0) X connectome (lying on the D0 unit sphere) to the un-
bounded RD0−1 space using the function L"
"GRD TRUTH PROXY
MF-VI",0.8275193798449613,"2. in this RD0−1 space, we performed a Principal Component Analysis (PCA) to bring us
down to D1 −1 = 140 dimensions responsible for 80% of the explained data variance"
"GRD TRUTH PROXY
MF-VI",0.8294573643410853,"3. in the resulting RD1−1 space, we calculated the mean of all the connectivity points, and
their standard deviation, and used both to whiten the data"
"GRD TRUTH PROXY
MF-VI",0.8313953488372093,"4. from the whitened data, we calculated the Ledoit-Wolf regularised covariance (Ledoit &
Wolf, 2004), that we used to construct the Σg matrix used in eq. (E.10a)"
"GRD TRUTH PROXY
MF-VI",0.8333333333333334,"5. finally, we projected the whitened data onto the unit sphere in D1 = 141 dimensions via
the function L"
"GRD TRUTH PROXY
MF-VI",0.8352713178294574,"To project our results back to the original D0 space, we simply ran back all the aforementioned
steps. Our prior for µg has been carefully designed so has to sample connectivity points in the
vicinity of the data point of interest. Our implementation is therefore in spirit close to SBI (Cranmer
et al., 2020; Papamakarios et al., 2019b; Greenberg et al., 2019; Thomas et al., 2020) that aims at
obtaining an amortized posterior only in the relevant data regime."
"GRD TRUTH PROXY
MF-VI",0.8372093023255814,"Mutli-step training strategy
In appendix F.2 we describe our conditional density estimators as the
stacking of a MAF (Papamakarios et al., 2018) on top of a diagonal-scale affine block. To accelerate
the training of our architecture and minimize numerical instability (resulting in NaN evaluations of
the loss) we used the following 3-step training strategy:"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8391472868217055,"1. we only trained the shift part of our affine block into a Maximum A Posteriori regression
setup. This can be viewed as the amortized fitting of the first moment of our posterior
distribution"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8410852713178295,"2. we trained both the shift and scale of our affine block using an unregularized ELBO loss.
This is to rapidly bring the variance of our posterior to relevant values"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8430232558139535,"3. we then trained our full posterior (shift and scale of our affine block, in addition to our
MAF block) using the reverse KL loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8449612403100775,"This training strategy shows the modularity of our approach and the transfer learning capabilities
already introduced in appendix C.4. Loss evolution can be seen in Fig. E.5"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8468992248062015,"0
500
1000
1500
2000
2500
3000
calls 15 20 25 30 35"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8488372093023255,losses (log scale)
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8507751937984496,Training losses train
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8527131782945736,"Figure E.5: 3-step loss evolution across epochs for the MS-HBM ADAVI training. Losses switch
are visible at epochs 1000 and 2000. Training was run for a longer period after epoch 3000, with no
significant results difference."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8546511627906976,"Soft labelling
In eq. (E.10a) and eq. (E.10b) we define µ variables as following Gaussian distribu-
tions in the latent space RD1−1. This means that, considering a vertex Xs,t
n
and a session network
µs,t
k , the squared Euclidean distance between L−1(Xs,t
n ) and L−1(µs,t
k ) in the latent space RD1−1"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8565891472868217,"is proportional to the log-likelihood of the point L−1(Xs,t
n ) for the mixture component k:"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8585271317829457,"∥L−1(Xs,t
n ) −L−1(µs,t
k )∥2 = log p(Xs,t
n |l = k) + C(κ)
(E.13)
Note that κ is the same for both networks. Additionally, considering Bayes theorem:
log p(l = k|Xs,t
n ) = log p(Xs,t
n |l = k) + log p(l = k) −log p(Xs,t
n )"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8604651162790697,"log p(l = 0|Xs,t
n )
p(l = 1|Xs,t
n )
= log p(Xs,t
n |l = 0) −log p(Xs,t
n |l = 1) + log p(l = 0)"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8624031007751938,"p(l = 1)
(E.14)"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8643410852713178,"Where log p(Xs,t
n |l = k) can be obtained through eq. (E.13) and log p(l = k) via draws from the
posterior of Π (see eq. (E.10a)). To integrate those equations, we used a Monte Carlo procedure."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8662790697674418,"E.5
ADDITIONAL NEUROIMAGING RESULTS"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8682170542635659,"E.5.1
SUBJECT-LEVEL PARCELLATION"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8701550387596899,"As pointed out in section 3.5, the MS-HBM model aims at representing the functional connectivity
of the brain at different levels, allowing for estimates of population characteristics and individual
variability (Kong et al., 2019). It is of experimental value to compare the parcellation for a given
subject, that is to say the soft label we give to a vertex Xs,t
n , and how this subject parcellation can
deviate from the population parcellation. Those differences underline how an individual brain can
have a unique local organization. Similarly, we can obtain the subject networks µs and observe how
those can deviate from the population networks µg. Those results underline how a given subject can
have his own connectivity, or, very roughly, his own ”wiring” between different areas of the brain.
Results can be seen in Fig. E.6."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.872093023255814,"E.5.2
COMPARISON OF LABELLING WITH THE MF-VI RESULTS"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.874031007751938,"We can compare the subject-level parcellation resulting from the latent networks recovered using
the ADAVI vs the MF-VI method. The result, for the same subjects as the previous section, can be
seen in Fig. E.7, where the difference in ELBO presented in our main text translates into marginal
differences for our downstream task of interest."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.875968992248062,"F
BASELINES FOR EXPERIMENTS"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.877906976744186,"F.1
BASELINE CHOICE"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8798449612403101,"In this section we justify further the choice of architectures presented as baselines in the our experi-
ments:"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8817829457364341,"Figure E.6: Subject-level parcellations and networks. For 3 different HCP subjects, we display
the individual parcellation (on the bottom) and the individual µs networks (on the top). Note how
the individual parcellations, though showing the same general split between the pars opercularis
and pars triangularis, slightly differ from each other and from the population parcellation (Fig. 4).
Similarly, networks µs differ from each other and from the population networks µg (Fig. 4) but keep
their general association to semantic/phonologic processing (0, in blue) and language production (1,
in red) (Heim et al., 2009; Zhang et al., 2020). To be able to model and display such variability is
one of the interests of models like the MS-HBM (Kong et al., 2019)."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8837209302325582,"Figure E.7: Gap in labelling between ADAVI and MF-VI results. Following eq. (E.14), we compute
the difference in latent space between the odds for the ADAVI and MF-VI techniques, before ap-
plying a sigmoid function. Differences are marginal, and interestingly located at the edges between
networks, where the labelling is less certain."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8856589147286822,"• Mean Field VI (MF-VI) (Blei et al., 2017). This methods stands as a common-practice non-
amortized baseline, is fast to compute, and due to our choice of conjugate examples (see
section 3.2) can be considered as a proxy to the ground truth posterior. We implemented
MF-VI in its usual setup, fitting to the posterior a distribution of the prior’s parametric
form;"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8875968992248062,"• (Sequential) Neural Posterior Estimation (SNPE-C) architecture (Greenberg et al., 2019).
NPE-C is an typical example from the SBI literature (Cranmer et al., 2020), and functions as
a likelihood-free, black box method. Indeed, NPE-C is trained using forward KL (samples
from the latent parameters), and is not made ”aware” of any structure in the problem. NPE-
C fits a single normalizing flow over the entirety of the latent parameter space, and its
number of weights scales quadratically with the parameter space size. When ran over
several simulation rounds, the method becomes sequential (SNPE-C), specializing for a
certain parameter regime to improve performance, but loosing amortization in the process;"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8895348837209303,"• Total Latent Space Flow (TLSF) architecture (Rezende & Mohamed, 2016). Following
the original normalizing flow implementation from Rezende & Mohamed (2016), we posit
TLSF as a counterpoint to SNPE-C. Like SNPE-C, TLSF fits a single normalizing flow
over the entirety of the latent parameter space, and is not made ”aware” of the structure of
the model. But contrary to SNPE-C, TLSF is trained using reverse KL and benefits from
the presence of a likelihood function. We can use TLSF in a non-amortized setup (TLSF-
NA), or in an amortized setup (TLSF-A) trough an observed data encoder conditioning the
single normalizing flow;"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8914728682170543,"• Cascading Flows (CF) (Ambrogioni et al., 2021b). CF is an example of a structure-aware,
prior-aware VI method, trained using reverse KL. By design, its number of weights scales
linearly with the plate’s cardinalities. CF can be ran both in a non-amortized (CF-NA)
and amortized (CF-A) setup, with the introduction of amortization through observed data
encoders in the auxiliary graph. As a structure-aware amortized architecture, CF-A is our
main point of comparison in this section;"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8934108527131783,"F.2
IMPLEMENTATION DETAILS"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8953488372093024,"In this section, we describe with precision and per experiment the implementation details for the ar-
chitectures described in appendix F.1. We implemented algorithms in Python, using the Tensorflow
probability (TFP, Dillon et al., 2017) and Simulation Based Inference (SBI, Tejero-Cantero et al.,
2020) libraries. For all experiments in TFP, we used the Adam optimizer (Kingma & Ba, 2015).
For normalizing flows, we leveraged Masked Autoregressive Flow (MAF, Papamakarios & Murray,
2018)."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8972868217054264,For all experiments:
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.8992248062015504,"• Mean Field VI (MF-VI) (Blei et al., 2017). We implemented MF-VI in TFP. The precise
form of the variational family is described below for each experiment."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9011627906976745,"• Sequential Neural Posterior Estimation (SNPE-C) architecture (Greenberg et al., 2019).
We implemented SNPE-C with the SBI library, using the default parameters proposed by
the API. Simulations were ran over 5 rounds, to ensure maximal performance. We acknowl-
edge that this choice probably results in an overestimate of the runtime for the algorithm.
To condition the density estimation based on the observed data, we designed an encoder
that is a variation of our Hierarchical Encoder (see section 2.2). Its architecture is the same
as HE -the hierarchical stacking of 2 Set Transformers (Lee et al., 2019)- but the encoder’s
output is the concatenation of the G per-group encodings with the population encodings.
This encoder is therefore parsimoniously parametrized, and adapted to the structure of the
problem."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9031007751937985,"• Neural Posterior Estimation (NPE-C). Though we acknowledge that NPE-C can be imple-
mented easily using the SBI library, we preferred to use our own implementation, notably
to have more control over the runtime of the algorithm. We implemented the algorithm
using TFP. We used the same encoder architecture as for SNPE-C."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9050387596899225,"• Total Latent Space Flow (TLSF). We implemented TLSF using TFP. Our API is actually
the same as for NPE-C, since TLSF-A and NPE-C only differ by their training loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9069767441860465,"• Cascading Flows (CF) (Ambrogioni et al., 2021b). We implemented our own version of
Cascading Flows, using TFP, and having consulted with the authors. An important im-
plementation detail that is not specified explicitly in Ambrogioni et al. (2021b) (whose
notations we follow here) is the implementation of the target distribution over the auxiliary
variables r, notably in the amortized setup. Following the authors specifications during our
discussion, we implemented r as the Mean Field distribution r = Q"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9089147286821705,j pj(ϵj).
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9108527131782945,• ADAVI (ours). We implemented ADAVI using TFP.
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9127906976744186,Regarding the training data:
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9147286821705426,"• All amortized methods were trained over a dataset of 20, 000 samples"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9166666666666666,"• All non-amortized methods except SNPE-C were trained on a single data point (separately
for 20 different data points)"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9186046511627907,"• SNPE-C was trained over 5 rounds of simulations, with 1000 samples per round, for an
effective dataset size of 5000"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9205426356589147,For the non conjugate experiment (see section 3.2):
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9224806201550387,• MF-VI: variational distribution is
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9244186046511628,"q = Gamma(a; concentration=V(D,), rate= Softplus(V(1,)))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9263565891472868,"We used the Adam optimizer with a learning rate of 10−2. The optimization was ran for
20, 000 steps, with a sample size of 32."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9282945736434108,"• CF: auxiliary size 8, observed data encoders with 8 hidden units. Minibatch size 32, 32
theta draws per X point (see appendix C.5), Adam (10−2), 40 epochs using a reverse KL
loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9302325581395349,"• ADAVI: NF with 1 Affine block with triangular scale, followed by 1 MAF with [32, 32, 32]
units. HE with embedding size 8, 2 modules with 2 ISABs (2 heads, 8 inducing points), 1
PMA (seed size 1), 1 SAB and 1 linear unit each. Minibatch size 32, 32 theta draws per X
point (see appendix C.5), Adam (10−3), 40 epochs using a reverse KL loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9321705426356589,For the Gaussian random effects experiment (see section 3.3):
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9341085271317829,• MF-VI: variational distribution is
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.936046511627907,"q = N(µ; mean=V(D,), std= Softplus(V(1,)))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.937984496124031,"× N([µ1, ..., µG]; mean=V(G,D), std= Softplus(V(1,)))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.939922480620155,"We used the Adam optimizer with a learning rate of 10−2. The optimization was ran for
10, 000 steps, with a sample size of 32."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9418604651162791,"• SNPE-C: 5 MAF blocks with 50 units each. Encoder with embedding size 8, 2 modules
with 2 SABs (4 heads) and 1 PMA (seed size 1) each, and 1 linear unit. See SBI for
optimization details."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9437984496124031,"• NPE-C: 1 MAF with [32, 32, 32] units. Encoder with embedding size 8, 2 modules with 2
ISABs (2 heads, 8 inducing points), 1 PMA (seed size 1), 1 SAB and 1 linear unit each.
Minibatch size 32, 15 epochs with Adam (10−3) using a forward KL loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9457364341085271,"• TLSF: same architecture as NPE-C. TLSF-A: minibatch size 32, 32 theta draws per X
point (see appendix C.5), 15 epochs with Adam (10−3) using a reverse KL loss. TLSF-
NA: minibatch size 1, 32 theta draws per X point (see appendix C.5), 1500 epochs with
Adam (10−3) using a reverse KL loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9476744186046512,"• CF: auxiliary size 16, observed data encoders with 16 hidden units. CF-A: minibatch size
32, 32 theta draws per X point (see appendix C.5), Adam (10−3), 40 epochs using a reverse
KL loss. CF-NA: minibatch size 1, 32 theta draws per X point (see appendix C.5), Adam
(10−2), 1500 epochs using a reverse KL loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9496124031007752,"• ADAVI: NF with 1 Affine block with triangular scale, followed by 1 MAF with [32, 32, 32]
units. HE with embedding size 8, 2 modules with 2 ISABs (2 heads, 8 inducing points), 1
PMA (seed size 1), 1 SAB and 1 linear unit each. Minibatch size 32, 32 theta draws per
X point (see appendix C.5), Adam (10−3), 10 epochs using an unregularized ELBO loss,
followed by 10 epochs using a reverse KL loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9515503875968992,For the Gaussian mixture with random effects experiment (see section 3.4):
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9534883720930233,• MF-VI: variational distribution is
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9554263565891473,"q = N([µ1, . . . , µL]; mean=V(L,D,), std= Softplus(V(1,)))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9573643410852714,"× N([[µ1
1, . . . , µ1
L], ..., [µG
1 , ..., µG
L]]; mean=V(G,L,D), std= Softplus(V(1,)))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9593023255813954,"× Dir(concentration= Softplus(V(G,L)))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9612403100775194,"We used the Adam optimizer with a learning rate of 10−2. The optimization was ran for
10, 000 steps, with a sample size of 32."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9631782945736435,"• SNPE-C: 5 MAF blocks with 50 units each. Encoder with embedding size 8, 2 modules
with 2 SABs (4 heads) and 1 PMA (seed size 1) each, and 1 linear unit. See SBI for
optimization details."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9651162790697675,"• NPE-C: 1 MAF with [32, 32, 32] units. Encoder with embedding size 8, 2 modules with 2
ISABs (2 heads, 8 inducing points), 1 PMA (seed size 1), 1 SAB and 1 linear unit each.
Minibatch size 32, 20 epochs with Adam (10−3) using a forward KL loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9670542635658915,"• TLSF-A: same architecture as NPE-C, minibatch size 32, 32 theta draws per X point (see
appendix C.5), 250 epochs with Adam (10−3) using a reverse KL loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9689922480620154,"• TLSF-NA: same NF architecture as NPE-C. Encoder with embedding size 16, 2 modules
with 2 ISABs (2 heads, 8 inducing points), 1 PMA (seed size 1), 1 SAB and 1 linear unit
each. Minibatch size 1, 32 theta draws per X point (see appendix C.5), 1000 epochs with
Adam (10−3) using a reverse KL loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9709302325581395,"• CF: auxiliary size 8, observed data encoders with 8 hidden units. CF-A: minibatch size 32,
32 theta draws per X point (see appendix C.5), Adam (10−3), 200 epochs using a reverse
KL loss. CF-NA: minibatch size 1, 32 theta draws per X point (see appendix C.5), Adam
(10−2), 1500 epochs using a reverse KL loss."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9728682170542635,"• ADAVI: NF with 1 Affine block with diagonal scale, followed by 1 MAF with [32] units.
HE with embedding size 16, 2 modules with 2 ISABs (4 heads, 8 inducing points), 1 PMA
(seed size 1), 1 SAB and 1 linear unit each. Minibatch size 32, 32 theta draws per X
point (see appendix C.5), Adam (10−3), 50 epochs using a MAP loss on the affine blocks,
followed by 2 epochs using an unregularized ELBO loss on the affine blocks, followed by
50 epochs of reverse KL loss (see appendix E.4 for the training strategy, total 102 epochs)."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9748062015503876,For the MSHBM example in toy dimensions (see appendix E.3):
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9767441860465116,"• ADAVI: NF with 1 Affine block with diagonal scale, followed by 1 MAF with [32] units.
HE with embedding size 32, 2 modules with 2 SABs (4 heads), 1 PMA (seed size 1), 1 SAB
and 1 linear unit each. Minibatch size 32, 32 theta draws per X point (see appendix C.5),
Adam (10−3), 5 epochs using a MAP loss on the affine blocks, followed by 1 epoch using
an unregularized ELBO loss on the affine blocks, followed by 5 epochs of reverse KL loss
(see appendix E.4 for the training strategy, total 11 epochs)."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9786821705426356,For the MSHBM example in (see section 3.5):
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9806201550387597,• MF-VI: variational distribution is
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9825581395348837,"q = L ◦N([µg
1, . . . , µg
L]; mean=V(L,D−1,), std= Exp(V(L,))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9844961240310077,"× Exp ◦N([ϵ1, . . . , ϵL]; mean=V(L,), std= Exp(V(L,))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9864341085271318,"× L ◦N([µs
1, . . . , µs
L]; mean=V(S,L,D−1,), std= Exp(V(L,))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9883720930232558,"× Exp ◦N([σ1, . . . , σL]; mean=V(L,), std= Exp(V(L,))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9903100775193798,"× L ◦N([µst
1 , . . . , µst
L ]; mean=V(S,T,L,D−1,), std= Exp(V(L,))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9922480620155039,"× Exp ◦N(κ; mean=V(1,), std= Exp(V(1,))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9941860465116279,"× Dirichlet(Π; concentration=V(L,))"
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.9961240310077519,"We used the Adam optimizer with a learning rate of 10−2. The optimization was ran for
10, 000 steps, with a sample size of 32."
WE ONLY TRAINED THE SHIFT PART OF OUR AFFINE BLOCK INTO A MAXIMUM A POSTERIORI REGRESSION,0.998062015503876,"• ADAVI: NF with 1 Affine block with diagonal scale, followed by 1 MAF with [1024] units.
HE with embedding size 1024, 3 modules with 1 SABs (4 heads), 1 PMA (seed size 1), 1
SAB and 1 linear unit each. Minibatch size 1, 4 theta draws per X point (see appendix C.5),
Adam (10−3), 1000 epochs using a MAP loss on the affine blocks, followed by 1000 epochs
using an unregularized ELBO loss on the affine blocks, followed by 1000 epochs of reverse
KL loss (see appendix E.4 for the training strategy, total 3000 epochs)."
