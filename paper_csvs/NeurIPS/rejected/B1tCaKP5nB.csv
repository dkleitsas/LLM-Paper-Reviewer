Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007645259938837921,"Testing conditional independence (CI) has many important applications, such as
1"
ABSTRACT,0.0015290519877675841,"Bayesian network learning and causal discovery. Although several approaches have
2"
ABSTRACT,0.0022935779816513763,"been developed for learning CI structures for observed variables, those existing
3"
ABSTRACT,0.0030581039755351682,"methods generally fail to work when the variables of interest can not be directly
4"
ABSTRACT,0.00382262996941896,"observed and only discretized values of those variables are available. For example,
5"
ABSTRACT,0.0045871559633027525,"if X1, ˜X2 and X3 are the observed variables, where ˜X2 is a discretization of the
6"
ABSTRACT,0.005351681957186544,"latent variable X2, applying the existing methods to the observations of X1, ˜X2
7"
ABSTRACT,0.0061162079510703364,"and X3 would lead to a false conclusion about the underlying CI of variables
8"
ABSTRACT,0.006880733944954129,"X1, X2 and X3. Motivated by this, we propose a CI test specifically designed to
9"
ABSTRACT,0.00764525993883792,"accommodate the presence of discretization. To achieve this, a bridge equation
10"
ABSTRACT,0.008409785932721712,"and nodewise regression are used to recover the precision coefficients reflecting
11"
ABSTRACT,0.009174311926605505,"the conditional dependence of the latent continuous variables under the nonpara-
12"
ABSTRACT,0.009938837920489297,"normal model. An appropriate test statistic has been proposed, and its asymptotic
13"
ABSTRACT,0.010703363914373088,"distribution under the null hypothesis of CI has been derived. Theoretical analysis,
14"
ABSTRACT,0.011467889908256881,"along with empirical validation on various datasets, rigorously demonstrates the
15"
ABSTRACT,0.012232415902140673,"effectiveness of our testing methods.
16"
INTRODUCTION,0.012996941896024464,"1
Introduction
17"
INTRODUCTION,0.013761467889908258,"Independence and conditional independence (CI) are fundamental concepts in statistics. They are
18"
INTRODUCTION,0.01452599388379205,"leveraged for exploring queries in statistical inference, such as sufficiency, parameter identification,
19"
INTRODUCTION,0.01529051987767584,"adequacy, and ancillarity [9]. They also play a central role in emerging areas such as causal discovery
20"
INTRODUCTION,0.016055045871559634,"[18], graphical model learning, and feature selection [36]. Tests for CI have attracted increasing
21"
INTRODUCTION,0.016819571865443424,"attention from both theoretical and application sides.
22"
INTRODUCTION,0.017584097859327217,"Formally, the problem is to test the CI of two variables Xj1 and Xj2 given a random vector (a set
23"
INTRODUCTION,0.01834862385321101,"of other variables) Z. In statistical notation, the null hypothesis is written as H0 : Xj1 ⊥Xj2 | Z,
24"
INTRODUCTION,0.0191131498470948,"where ⊥denotes “independent from.” The alternative hypothesis is written as H1 : Xj1 ̸⊥Xj2 | Z,
25"
INTRODUCTION,0.019877675840978593,"where ̸⊥denotes “dependent with.” The null hypothesis implies that once Z is known, the values of
26"
INTRODUCTION,0.020642201834862386,"Xj1 provide no additional information about Xj2, and vice versa. Different tests have been designed
27"
INTRODUCTION,0.021406727828746176,"to handle different scenarios, including Gaussian variables with linear dependence [37, 25, 22, 26]
28"
INTRODUCTION,0.02217125382262997,"and non-linear dependence [16, 38, 31, 27, 1] (For detailed related work, please refer to App. D).
29"
INTRODUCTION,0.022935779816513763,"Given observations of Xj1, Xj2, and Z, the CI can be effectively tested with existing methods.
30"
INTRODUCTION,0.023700305810397553,"However, in many scenarios, accurately measuring continuous variables of interest is challenging
31"
INTRODUCTION,0.024464831804281346,"due to limitations in data collection. Sometimes the data obtained are approximations represented as
32"
INTRODUCTION,0.02522935779816514,"discretized values. For example, in finance, variables such as asset values cannot be measured and are
33"
INTRODUCTION,0.02599388379204893,"binned into ranges for assessing investment risks (e.g., sell, hold, and strong buy) [7, 8]. Similarly,
34"
INTRODUCTION,0.026758409785932722,"in mental health, anxiety levels are often assessed using scales like the GAD-7, which categorizes
35"
INTRODUCTION,0.027522935779816515,"responses into levels such as mild, moderate, or severe [23, 17]. In the entertainment industry, the
36"
INTRODUCTION,0.028287461773700305,"quality of movies is typically summarized through viewer ratings [29, 10].
37"
INTRODUCTION,0.0290519877675841,"(a)
(b)
(c)"
INTRODUCTION,0.02981651376146789,"Figure 1: We illustrate different data generative
processes with causal graphical models. The dis-
cretization process introduces new discrete vari-
ables which are denoted with a tilde (∼)."
INTRODUCTION,0.03058103975535168,"When discretization is present, existing CI tests
38"
INTRODUCTION,0.03134556574923547,"can fail to determine the CI of underlying con-
39"
INTRODUCTION,0.03211009174311927,"tinuous variables. This issue arises because ex-
40"
INTRODUCTION,0.03287461773700306,"isting CI tests treat discretized observations as
41"
INTRODUCTION,0.03363914373088685,"observations of continuous variables, leading
42"
INTRODUCTION,0.034403669724770644,"to incorrect conclusions about their CI relation-
43"
INTRODUCTION,0.035168195718654434,"ships. More precisely, the problem lies in the
44"
INTRODUCTION,0.035932721712538224,"discretization process, which introduces new dis-
45"
INTRODUCTION,0.03669724770642202,"crete variables. Consequently, although the in-
46"
INTRODUCTION,0.03746177370030581,"tent is to test the CI of the underlying continuous
47"
INTRODUCTION,0.0382262996941896,"variables, what is actually being tested is the CI
48"
INTRODUCTION,0.0389908256880734,"involving a mix of both continuous and newly introduced discrete variables. In general, this CI
49"
INTRODUCTION,0.039755351681957186,"relationship is inconsistent with the one among the underlying continuous variables.
50"
INTRODUCTION,0.040519877675840976,"As illustrated in Fig. 1, we show different data-generative processes using causal graphical models
51"
INTRODUCTION,0.04128440366972477,"[24] in the presence of discretization. A gray node indicates an observable variable, while a white
52"
INTRODUCTION,0.04204892966360856,"node indicates a latent variable. Variables denoted by Xj (without a tilde ∼) represent continuous
53"
INTRODUCTION,0.04281345565749235,"variables, which may not be observed; while variables denoted by ˜Xj represent observed discretized
54"
INTRODUCTION,0.04357798165137615,"variables derived from Xj due to discretization. In Fig. 1(a), X2 is latent, and only its discrete
55"
INTRODUCTION,0.04434250764525994,"counterpart ˜X2 is observed. In this case, rather than observing X1, X2, and X3, we only observe
56"
INTRODUCTION,0.04510703363914373,"X1, ˜X2, and X3. Existing CI methods use these observations to test whether X1 ⊥X3 | {X2}, but
57"
INTRODUCTION,0.045871559633027525,"what is actually being tested is whether X1 ⊥X3 | { ˜X2}. In fact, according to the causal Markov
58"
INTRODUCTION,0.046636085626911315,"condition [30], , it can be inferred from Fig. 1(a) that X1 ⊥X3 | {X2} and X1 ̸⊥X3 | { ˜X2}.
59"
INTRODUCTION,0.047400611620795105,"This mismatch leads to existing CI methods, that employ observations to check the CI relationships
60"
INTRODUCTION,0.0481651376146789,"between X1 and X3 given X2, to reach incorrect conclusions. Due to the same reason, checking the
61"
INTRODUCTION,0.04892966360856269,"CI also fails in Fig 1(b) and Fig 1(c).
62"
INTRODUCTION,0.04969418960244648,"In this paper, we design a CI test specifically for handling the presence of discretization. An appropri-
63"
INTRODUCTION,0.05045871559633028,"ate test statistic for the CI of latent continuous variables, based solely on discretized observations, is
64"
INTRODUCTION,0.05122324159021407,"derived. The key is to build connections between the discretized observations and the parameters
65"
INTRODUCTION,0.05198776758409786,"needed for testing the CI of the latent continuous variables. To achieve this, we first develop bridge
66"
INTRODUCTION,0.052752293577981654,"equations that allow us to estimate the covariance of the underlying continuous variables with dis-
67"
INTRODUCTION,0.053516819571865444,"cretized observations. Then, we leverage a node-wise regression [5] to derive appropriate test statistics
68"
INTRODUCTION,0.054281345565749234,"for CI relationships from the estimated covariance. By assuming that the continuous variables follow
69"
INTRODUCTION,0.05504587155963303,"a Gaussian distribution, we can derive the asymptotic distributions of the test statistics under the null
70"
INTRODUCTION,0.05581039755351682,"hypothesis of CI. The major contributions of our paper include that
71"
INTRODUCTION,0.05657492354740061,"• We develop a CI test for ensuring accurate analysis in scenarios where data has been discretized,
72"
INTRODUCTION,0.05733944954128441,"which are common due to limitations in data collection or measurement techniques, such as in
73"
INTRODUCTION,0.0581039755351682,"financial analysis and healthcare.
74"
INTRODUCTION,0.058868501529051986,"• Our CI test can handle various scenarios including 1). Both variables Xj1 and Xj2 are discretized
75"
INTRODUCTION,0.05963302752293578,"2). Both variables Xj1 and Xj2 are continuous. 3). One of the variables Xj1 or Xj2 is discretized.
76"
INTRODUCTION,0.06039755351681957,"• We compare our test with the existing methods on both synthetic and real-world datasets, confirm-
77"
INTRODUCTION,0.06116207951070336,"ing that our method can effectively estimate the CI of the underlying continuous variables and
78"
INTRODUCTION,0.06192660550458716,"outperform the existing tests applied on the discretized observations.
79"
INTRODUCTION,0.06269113149847094,"2
DCT: A CI Test in the Presence of Discretization
80"
INTRODUCTION,0.06345565749235474,"Problem Setting
Consider a set of independent and identically distributed (i.i.d.) p-dimensional
81"
INTRODUCTION,0.06422018348623854,"random vectors, denoted as ˜
X = (X1, X2, . . . , ˜Xj, . . . , ˜Xp)T . In this set, some variables, indicated
82"
INTRODUCTION,0.06498470948012232,"by a tilde (∼), such as ˜Xj, follow a discrete distribution. For each such variable, there exists a
83"
INTRODUCTION,0.06574923547400612,"corresponding latent Gaussian random variable Xj. The transformation from Xj to ˜Xj is governed
84"
INTRODUCTION,0.06651376146788991,"by an unknown monotone nonlinear function gj. This function, gj : X →˜
X, maps the continuous
85"
INTRODUCTION,0.0672782874617737,"domain of Xj onto the discrete domain of ˜
Xj, such that ˜Xj = gj(Xj) for each observation. Given n
86"
INTRODUCTION,0.06804281345565749,"observations {˜x1, ˜x2, . . . , ˜xn} randomly sampled from ˜
X, specifically, for each variable Xj, there
87"
INTRODUCTION,0.06880733944954129,"exists a constant vector d = (d1, . . . , dM) characterized by strictly increasing elements such that
88"
INTRODUCTION,0.06957186544342507,"˜xi
j = 
 "
INTRODUCTION,0.07033639143730887,"1
0 < gj(xi
j) < d1
m
dm−1 < gj(xi
j) < dm
M
gj(xi
j) > dm
(1)"
INTRODUCTION,0.07110091743119266,"This model is also known as the nonparanormal model [20]. The cardinality of the domain after
89"
INTRODUCTION,0.07186544342507645,"discretization is at least 2 and smaller than infinity. Our goal is to assess both conditional and
90"
INTRODUCTION,0.07262996941896024,"unconditional independence among the variables of the vector X = (X1, X2, . . . , Xj, . . . , Xp)T .
91"
INTRODUCTION,0.07339449541284404,"In our model, we assume X ∼N(0, Σ), Σ only contain 1 among its diagonal, i.e., σjj = 1 for all
92"
INTRODUCTION,0.07415902140672782,"j ∈[1, . . . , p]. One should note this assumption is without loss of generality. We provide a detailed
93"
INTRODUCTION,0.07492354740061162,"discussion of our assumption in App. A.8.
94"
INTRODUCTION,0.07568807339449542,"Preliminary Framework of DCT
To develop an independence test, one needs to design a test
95"
INTRODUCTION,0.0764525993883792,"statistic that can reflect the dependence relation and be calculated from observations. Next, it is
96"
INTRODUCTION,0.077217125382263,"essential to derive the underlying distribution of this statistic under the null hypothesis that the tested
97"
INTRODUCTION,0.0779816513761468,"variables are conditionally (or unconditionally) independent. By calculating the value of the test
98"
INTRODUCTION,0.07874617737003058,"statistic from observations and determining if this statistic is likely to be sampled from the derived
99"
INTRODUCTION,0.07951070336391437,"distribution (i.e., calculating the p-value and comparing it with the significance level α), we can
100"
INTRODUCTION,0.08027522935779817,"decide if the null hypothesis should be rejected.
101"
INTRODUCTION,0.08103975535168195,"Our objective is to deduce the independence and CI relationships within the original multivariate
102"
INTRODUCTION,0.08180428134556575,"Gaussian model, based on its discretized observations. In the context of a multivariate Gaussian
103"
INTRODUCTION,0.08256880733944955,"model, this challenge is directly equivalent to constructing statistical inferences for its covariance
104"
INTRODUCTION,0.08333333333333333,"matrix Σ = (σj1,j2) and its precision matrix Ω= (ωj,k) = Σ−1 [3]. The covariance matrix Σ
105"
INTRODUCTION,0.08409785932721713,"captures the pairwise covariances between variables, while the precision matrix Ω(also known as the
106"
INTRODUCTION,0.08486238532110092,"concentration matrix) provides information about the CI between variables. Specifically, the entry
107"
INTRODUCTION,0.0856269113149847,"ωj,k in the precision matrix is related to the partial correlation coefficient between variables Xj and
108"
INTRODUCTION,0.0863914373088685,"Xk, which can be used to test whether these variables are conditionally independent given some other
109"
INTRODUCTION,0.0871559633027523,"variables. Technically, we are interested in two things: (1) the calculation of the covariance ˆσj1,j2
110"
INTRODUCTION,0.08792048929663608,"and the precision coefficient (or the partial correlation coefficient) ˆωj,k, serving as the estimation
111"
INTRODUCTION,0.08868501529051988,"of σj1,j2 and ωj,k respectively (in this paper, a variable with a hat indicates its estimation); and
112"
INTRODUCTION,0.08944954128440367,"(2) the derivation of the distribution of ˆσj1,j2 −σj1,j2 and ˆωj,k −ωj,k under the null hypothesis of
113"
INTRODUCTION,0.09021406727828746,"independence and CI.
114"
INTRODUCTION,0.09097859327217125,"In the subsequent section, 1). we first introduce bridge equations to address the estimation challenge
115"
INTRODUCTION,0.09174311926605505,"of the covariance σj1,j2; 2). we proceed to derive the distribution of ˆσj1,j2 −σj1,j2, demonstrating it
116"
INTRODUCTION,0.09250764525993883,"is asymptotically normal; 3). utilizing nodewise regression, we establish the relationship between
117"
INTRODUCTION,0.09327217125382263,"the covariance matrix Σ and the precision matrix Ω, where the regression parameter βj,k acts as an
118"
INTRODUCTION,0.09403669724770643,"effective surrogate for ωj,k. Leveraging the distribution of ˆσj1,j2 −σj1,j2, we further illustrate that
119"
INTRODUCTION,0.09480122324159021,"ˆβj,k −βj,k is also asymptotically normal.
120"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.095565749235474,"2.1
Design Bridge Equation for Test Statistics
121"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.0963302752293578,"Estimating Covariance with Bridge Equations
The bridge equation establishes a connection
122"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.09709480122324159,"between the underlying covariance σj1,j2 of two continuous variables Xj1 and Xj2 with the ob-
123"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.09785932721712538,"servations. When in the presence of discretization, the discrete transformations make the sample
124"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.09862385321100918,"covariance matrix based on ˜
X inconsistent with the covariance matrix of X. To obtain the estimation
125"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.09938837920489296,"ˆσj1,j2 of σj1,j2, the bridge equation is leveraged. In general, its form is as follows.
126"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10015290519877676,"ˆτj1,j2 = T(σj1,j2; ˆΛ),
(2)"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10091743119266056,"where σj1,j2 is the covariance needed to be estimated, ˆτj1,j2 is a statistic that can also be estimated
127"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10168195718654434,"from observations, and ˆΛ is a set of additional parameters required by the function T(·). The specific
128"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10244648318042814,"form of the function T(·) will be derived later. Both ˆτj1,j2 and ˆΛ should be able to be calculated
129"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10321100917431193,"purely relying on observations. Then, given the calculated ˆτj1,j2 and ˆΛ, ˆσj1,j2 can be obtained by
130"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10397553516819572,"solving the bridge equation ˆτj1,j2 = T(σj1,j2; ˆΛ). As a result, the covariance matrix Σ of X can be
131"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10474006116207951,"estimated, which contains information about both unconditional independence and CI (which can be
132"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10550458715596331,"derived from its inverse).
133"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10626911314984709,"To estimate the covariance of a latent multivariate Gaussian distribution, we need to design appropriate
134"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10703363914373089,"ˆτj1,j2, ˆΛ, and T(·). Notably, bridge equations have to be designed to handle all three possible cases:
135"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10779816513761468,"C1. both observed variables are discretized; C2. one variable is continuous while the other is
136"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10856269113149847,"discretized; and C3. both variables remain continuous. We will show that cases C1 and C2 can be
137"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.10932721712538226,"merged into a single form of bridge equation with different parameters and a binarization operation
138"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.11009174311926606,"applied to the observations. Our bridge equations are presented in Def. 2.2, Def. 2.3, and Def. 2.4.
139"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.11085626911314984,"Bridge Equations for Discretized and Mixed Pairs
Let us first address the challenging cases
140"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.11162079510703364,"where both observed variables are discretized or where one variable is continuous while the other
141"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.11238532110091744,"is discretized. In general, different bridge equations would need to be designed to handle each case
142"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.11314984709480122,"individually. However, in our analysis, we provide a unified bridge equation that is applicable to both
143"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.11391437308868502,"cases. This is achieved by binarizing the observed variables, thereby unifying both cases into a binary
144"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.11467889908256881,"case. As some information may be lost in the binarization process, this unification may require more
145"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1154434250764526,"examples compared to using tailored bridge functions for each specific case. Developing specific
146"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1162079510703364,"bridge equations for each case to improve sample efficiency is left in future work.
147"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.11697247706422019,"Intuitionally, for the original continuous variable Xj, binarization separates it into two parts based on
148"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.11773700305810397,"a boundary hj: the part for Xj larger than hj and the part for Xj smaller than hj. In this case, we can
149"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.11850152905198777,"estimate the boundary by calculating the proportion of Xj that exceeds the boundary. In the scenario
150"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.11926605504587157,"of two variables where the threshold hj1 and hj2 divide the space into four regions, the proportions of
151"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12003058103975535,"these areas are influenced by the covariance σj1,j2, which connects the relation between the binarized
152"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12079510703363915,"variables with the latent covariance. This approach allows us to initially estimate the threshold hj1,
153"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12155963302752294,"hj2 of a pair of variables, followed by estimating the covariance σj1,j2.
154"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12232415902140673,"Let PnZ denote the average of a random variable Z given n i.i.d. observation of Z and E[Z] as the
155"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12308868501529052,"true mean of Z, P as the probability and ˆP as the empirical probability. We then define the boundary
156"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12385321100917432,"hj as follows: for any single discretized variable ˜Xj, there exists a constant cj such that:
157"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1246177370030581,"1{˜xi
j > E[ ˜Xj]} = 1{gj(xi
j) > cj} = 1{xi
j > hj},"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12538226299694188,"where hj = g−1
j (cj). Specifically, hj is the boundary in the original continuous domain to determine
158"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12614678899082568,"if the discretized observation ˜Xk is larger than its mean. When the continuous variable Xj follows
159"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12691131498470948,"a normal distribution, there is a relation P( ˜Xj > E[ ˜Xj]) = 1 −Φ(hj), where Φ is the cumulative
160"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12767584097859327,"distribution function (cdf) of a standard normal distribution. We then provide the following definition:
161"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12844036697247707,"Definition 2.1. The estimated boundary can be expressed as ˆhj = Φ−1(1 −ˆτj), where ˆτj =
162
Pn
i=1 1{˜xi
j>Pn ˜
Xj}/n, serving as the estimation of P( ˜Xj > E[ ˜Xj]).
163"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12920489296636087,"Let ¯Φ(z1, z2; ρ) = P(Z1 > z1, Z2 > z2), where (Z1, Z2)T follows a bivariate normal distribution
164"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.12996941896024464,"with mean zero, variance one and covariance ρ. We define
165"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.13073394495412843,"τj1,j2 = P(˜xi
j1 > E[ ˜Xj1], ˜xi
j2 > E[ ˜Xj2]) = ¯Φ(hj1, hj2; σj1,j2).
(3)"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.13149847094801223,"That is, the proportion of discretized variables larger than their mean can be expressed as a function
166"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.13226299694189603,"of underlying covariance. This equation serves as the key of estimating latent covariance based on the
167"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.13302752293577982,"discretized observations. Specifically, we can substitute those true parameters with their estimation
168"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.13379204892966362,"and construct the bridge equation to get the estimated covariance:
169"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1345565749235474,"Definition 2.2 (Bridge Equation for A Discretized-Variable Pair). For discretized variables ˜Xj1 and
170"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1353211009174312,"˜Xj2, the bridge equation is defined as:
171"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.13608562691131498,"ˆτj1,j2 = ˆP( ˜Xj1 > Pn ˜Xj1, ˜Xj2 > Pn ˜Xj2) = 1 n n
X"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.13685015290519878,"i=1
1{˜xi
j1>Pn ˜
Xj1,˜xi
j2>Pn ˜
Xj2} = T(σj1,j2; {ˆhj1, ˆhj2}),"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.13761467889908258,"and the function T(σj1,j2; {ˆhj1, ˆhj2}) := ¯Φ(ˆhj1, ˆhj2; σ) =
Z"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.13837920489296637,x1>ˆhj1 Z
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.13914373088685014,"x2>ˆhj2
ϕ(xj1, xj2; σ)dxj1dxj2,"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.13990825688073394,"where ϕ is the probability density function of a bivariate normal distribution, ˆhj1, ˆhj2 can be simply
172"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.14067278287461774,"calculated using Def. 2.1.
173"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.14143730886850153,"Following the same intuition, we can directly apply the same bridge equation to estimate the co-
174"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.14220183486238533,"variance of mixed pairs. The only difference is there is no need to estimate the boundary ˆhj for the
175"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.14296636085626913,"continuous variable. Instead, we can incorporate its true mean of zero into the equation.
176"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1437308868501529,"Definition 2.3 (Bridge Equation for A Continuous-Discretized-Variable Pair). For one continuous
177"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1444954128440367,"variable Xj1 and one discretized variable ˜Xj2, the bridge function is defined as follows:
178"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1452599388379205,"ˆτj1,j2 = ˆP(Xj1 > 0, ˜Xj2 > Pn ˜Xj2) = 1 n n
X"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.14602446483180428,"i=1
1{xi
j1>0,˜xi
j2>Pn ˜
Xj2} = T(σj1,j2; {0, ˆhj2}),"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.14678899082568808,"and the function T(·) has the same form of Def. 2.2.
179"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.14755351681957188,"A Bridge Equation for A Continuous-Variable Pair
When there is no discretized transformation,
180"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.14831804281345565,"the sample covariance of Xj1 and Xj2 provides a consistent estimation. In this context, the function
181"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.14908256880733944,"T acts merely as an identity mapping.
182"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.14984709480122324,"Definition 2.4 (A Bridge Equation for A Continuous-Variable Pair). For two continuous variables
183"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.15061162079510704,"Xj1 and Xj2 , the bridge equation is defined as:
184"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.15137614678899083,"ˆτj1,j2 := ˆσj1,j2 = 1 n n
X"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.15214067278287463,"i=1
xi
j1xi
j2 −1 n n
X"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1529051987767584,"i=1
xi
j1
1
n n
X"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1536697247706422,"i=1
xi
j2 = T(σj1,j2; ∅)."
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.154434250764526,"For two continuous variables Xj1 and Xj2, the analytic solution of the estimated covariance can be
185"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1551987767584098,"simply obtained using Def. 2.4.
186"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.1559633027522936,"Calculation of Estimated Covariance
For the continuous case, the analytic solution of ˆσj1,j2
187"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.15672782874617738,"can be simply obtained using Def. 2.4. For the cases involving the discretized variable as proposed
188"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.15749235474006115,"in Def. 2.2 and Def. 2.3, we can rely on the property that variance Σ only contains 1 among the
189"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.15825688073394495,"diagonal, which implies the covariance σj1,j2 should vary from −1 to 1. Thus, we can calculate the
190"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.15902140672782875,"estimated covariance by solving the objective
191"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.15978593272171254,"min
σj1,j2
||ˆτj1,j2 −T(σj1,j2; {ˆhj1, ˆhj2})||2
s.t. −1 < σj1,j2 < 1.
(4)"
DESIGN BRIDGE EQUATION FOR TEST STATISTICS,0.16055045871559634,"The ˆτj1,j2 is a one-to-one mapping with calculated ˆσj1,j2, ˆhj1 and ˆhj2, which is proved in App. A.2
192"
UNCONDITIONAL INDEPENDENCE TEST,0.16131498470948014,"2.2
Unconditional Independence Test
193"
UNCONDITIONAL INDEPENDENCE TEST,0.1620795107033639,"The estimation of covariance ˆσj1,j2 can be effectively solved using the designed bridge equation.
194"
UNCONDITIONAL INDEPENDENCE TEST,0.1628440366972477,"Now, we focus on deriving the distribution of ˆσj1,j2 −σj1,j2. These results is used as an unconditional
195"
UNCONDITIONAL INDEPENDENCE TEST,0.1636085626911315,"independence test in the presence of the discretization. Moreover, Thm. 2.5, Lem. 2.6, Lem. 2.7
196"
UNCONDITIONAL INDEPENDENCE TEST,0.1643730886850153,"and Lem. 2.8 will be leveraged in the derivation process of the CI test in Section 2.3. The detailed
197"
UNCONDITIONAL INDEPENDENCE TEST,0.1651376146788991,"derivation steps for both unconditional test and CI test are relatively intricate, therefore, we will
198"
UNCONDITIONAL INDEPENDENCE TEST,0.1659021406727829,"provide a general intuition. For a complete derivation, please refer to the App. A.3.
199"
UNCONDITIONAL INDEPENDENCE TEST,0.16666666666666666,"Assume we are interested in the true parameter θ0. We denote ˆθ as its estimation which is close to θ0,
200"
UNCONDITIONAL INDEPENDENCE TEST,0.16743119266055045,"and f(θ) is a continuous function. By leveraging Taylor expansion, we have
201"
UNCONDITIONAL INDEPENDENCE TEST,0.16819571865443425,"f(ˆθ) = f(θ0) + f ′(θ0)(ˆθ −θ0),
(5)"
UNCONDITIONAL INDEPENDENCE TEST,0.16896024464831805,"which directly constructs the relationship between the estimated parameter with the true one. Re-
202"
UNCONDITIONAL INDEPENDENCE TEST,0.16972477064220184,"arrange the term, we get ˆθ −θ0 = (f(ˆθ) −f(θ0))/f ′(θ0). If the denominator is a constant and the
203"
UNCONDITIONAL INDEPENDENCE TEST,0.1704892966360856,"numerator can be expressed as a sum of i.i.d samples, we can see ˆθ −θ0 will be asymptotically
204"
UNCONDITIONAL INDEPENDENCE TEST,0.1712538226299694,"normal according to the central limit theorem [35].
205"
UNCONDITIONAL INDEPENDENCE TEST,0.1720183486238532,"Let ψˆθ = [f 1
ˆθ (·), f 2
ˆθ (·), f 3
ˆθ (·)]T contains a group of functions parameterized by ˆθ (For discretized
206"
UNCONDITIONAL INDEPENDENCE TEST,0.172782874617737,"pairs, ˆθ = (ˆσj1,j2, ˆhj1, ˆhj2)). Define Pnψˆθ as sample mean of these functions evaluated at n sample
207"
UNCONDITIONAL INDEPENDENCE TEST,0.1735474006116208,"points. Similarly, Pnψˆθψˆθ
T is defined as sample mean of the outer product ψˆθψˆθ
T . The notation
208"
UNCONDITIONAL INDEPENDENCE TEST,0.1743119266055046,"Pψˆθ := EPnψˆθ denotes the expectations of the functions in ψˆθ. Furthermore, let ψˆθ
′ denote the
209"
UNCONDITIONAL INDEPENDENCE TEST,0.17507645259938837,"derivative of the functions contained in ψˆθ. We now provide the main result of derived distribution
210"
UNCONDITIONAL INDEPENDENCE TEST,0.17584097859327216,"ˆσj1,j2 −σj1,j2 under the hull hypothesis that test pairs are independent.
211"
UNCONDITIONAL INDEPENDENCE TEST,0.17660550458715596,"Theorem 2.5 (Independence Test). In our settings, under the null hypothesis that two observed
212"
UNCONDITIONAL INDEPENDENCE TEST,0.17737003058103976,"variables indexed with j1 and j2 are statistically independent under our framework, i.e., σj1,j2 = 0,
213"
UNCONDITIONAL INDEPENDENCE TEST,0.17813455657492355,"the independence can be tested using the statistic
214"
UNCONDITIONAL INDEPENDENCE TEST,0.17889908256880735,"ˆσj1,j2 = T −1(ˆτj1,j2; ˆθ)."
UNCONDITIONAL INDEPENDENCE TEST,0.17966360856269112,"This statistic is approximated to follow a normal distribution, as detailed below:
215"
UNCONDITIONAL INDEPENDENCE TEST,0.18042813455657492,"ˆσj1,j2
approx
∼
N

0, 1"
UNCONDITIONAL INDEPENDENCE TEST,0.1811926605504587,"n((Pnψ′
ˆθ)−1PnψˆθψT
ˆθ (Pnψ′T
ˆθ )−1)1,1"
UNCONDITIONAL INDEPENDENCE TEST,0.1819571865443425,"
,
(6)"
UNCONDITIONAL INDEPENDENCE TEST,0.1827217125382263,"where the specific form of ψˆθ are presented in Lem. 2.6,Lem. 2.7 and Lem. 2.8.
216"
UNCONDITIONAL INDEPENDENCE TEST,0.1834862385321101,"We now provide the specific forms of ψˆθ. Since the variables being tested for independence can be
217"
UNCONDITIONAL INDEPENDENCE TEST,0.18425076452599387,"both discretized, only one being discretized, or neither being discretized. This results in different
218"
UNCONDITIONAL INDEPENDENCE TEST,0.18501529051987767,"forms of ψˆθ consequently differs across these scenarios.
Let Zj1 and Zj2 be any two random
219"
UNCONDITIONAL INDEPENDENCE TEST,0.18577981651376146,"variables indexed by j1 and j2. Let ˆσi
j1,j2 = zi
j1 · zi
j2 −PnZj1 · PnZj2 denote the sample covariance
220"
UNCONDITIONAL INDEPENDENCE TEST,0.18654434250764526,"based on a i-th pairwise observation of the variables Zj1 and Zj2. Let ˆτ i
j1 = 1{zi
j1>PnZj1} and
221"
UNCONDITIONAL INDEPENDENCE TEST,0.18730886850152906,"ˆτ i
j2 = 1{Zi
j2>PnZj2}, each calculated based on i-th observations of the variables Zj1 and Zj2,
222"
UNCONDITIONAL INDEPENDENCE TEST,0.18807339449541285,"respectively. Let ˆτ i
j1,j2 be ˆτ i
j1 · ˆτ i
j2. We further denote ¯Φ(·) = 1 −Φ(·). The different forms of ψˆθ
223"
UNCONDITIONAL INDEPENDENCE TEST,0.18883792048929662,"that arise in different cases are defined as follows:
224"
UNCONDITIONAL INDEPENDENCE TEST,0.18960244648318042,"Lemma 2.6. (ψˆθ for A Continuous-Variable Pair). For two continuous variables Xj1 and Xj2,
225"
UNCONDITIONAL INDEPENDENCE TEST,0.19036697247706422,"ψˆθ := ˆσi
j1,j2 −ˆσj1,j2.
(7)"
UNCONDITIONAL INDEPENDENCE TEST,0.191131498470948,"Lemma 2.7 (ψˆθ for A Discretized-Variable Pair). For discretized variables ˜Xj1 and ˜Xj2,
226"
UNCONDITIONAL INDEPENDENCE TEST,0.1918960244648318,ψˆθ := 
UNCONDITIONAL INDEPENDENCE TEST,0.1926605504587156,"

ˆτ i
j1,j2 −T(ˆσj1,j2; {ˆhj1, ˆhj2})
ˆτ i
j1 −¯Φ(ˆhj1)
ˆτ i
j2 −¯Φ(ˆhj2) "
UNCONDITIONAL INDEPENDENCE TEST,0.19342507645259938,"
.
(8)"
UNCONDITIONAL INDEPENDENCE TEST,0.19418960244648317,"Lemma 2.8 (ψˆθ for A Continuous-Discretized-Variable Pair). For one discretized variable ˜Xj2 and
227"
UNCONDITIONAL INDEPENDENCE TEST,0.19495412844036697,"one continuous variable Xj1,
228"
UNCONDITIONAL INDEPENDENCE TEST,0.19571865443425077,ψˆθ :=
UNCONDITIONAL INDEPENDENCE TEST,0.19648318042813456,"ˆτ i
j1,j2 −T(ˆσj1,j2; {0, ˆhj2)}
ˆτ i
j1 −¯Φ(ˆhj2) ! .
(9)"
UNCONDITIONAL INDEPENDENCE TEST,0.19724770642201836,"Derivation of forms of ψˆθ for different cases and their corresponding distribution defined in Eq (6)
229"
UNCONDITIONAL INDEPENDENCE TEST,0.19801223241590213,"can be found in App. A.4, App. A.5, App. A.6. Up to this point, our discussion has been confined to
230"
UNCONDITIONAL INDEPENDENCE TEST,0.19877675840978593,"the case of covariance σj1,j2, the indicator of unconditional independence. In the next section, we
231"
UNCONDITIONAL INDEPENDENCE TEST,0.19954128440366972,"will present the results of our CI test.
232"
UNCONDITIONAL INDEPENDENCE TEST,0.20030581039755352,"2.3
Conditional Independence (CI) Test
233"
UNCONDITIONAL INDEPENDENCE TEST,0.20107033639143732,"To construct a CI test of our model, we are interested at two things: calculation of the estimated
234"
UNCONDITIONAL INDEPENDENCE TEST,0.2018348623853211,"precision coefficient ˆωj,k and the derivation of the corresponding distribution ˆωj,k −ωj,k. In the
235"
UNCONDITIONAL INDEPENDENCE TEST,0.20259938837920488,"following, we first build βj,k, which is obtained using nodewise regression and show it serves as a
236"
UNCONDITIONAL INDEPENDENCE TEST,0.20336391437308868,"surrogate of testing for ωj,k = 0, we then construct the formulation of ˆβj,k −βj,k as the combination
237"
UNCONDITIONAL INDEPENDENCE TEST,0.20412844036697247,"of formulation of ˆσj1,j2 −σj1,j2 and show it will also be asymptotically normal.
238"
UNCONDITIONAL INDEPENDENCE TEST,0.20489296636085627,"Nodewise Regression for CI
To utilize covariance for testing CI, it is necessary to establish a
239"
UNCONDITIONAL INDEPENDENCE TEST,0.20565749235474007,"relationship between the estimated covariance and a metric capable of reflecting CI. To achieve this,
240"
UNCONDITIONAL INDEPENDENCE TEST,0.20642201834862386,"we employ the nodewise regression which effectively builds the connection between covariance
241"
UNCONDITIONAL INDEPENDENCE TEST,0.20718654434250763,"and precision matrix. Suppose we can access observations {x1, x2, . . . , xn} from latent continuous
242"
UNCONDITIONAL INDEPENDENCE TEST,0.20795107033639143,"variables X = (X1, . . . , Xp) ∼N(0, Σ), nodewise regression will do regression on every dimension
243"
UNCONDITIONAL INDEPENDENCE TEST,0.20871559633027523,"with all other dimensions as predictors.
244"
UNCONDITIONAL INDEPENDENCE TEST,0.20948012232415902,"xi
j1 =
X"
UNCONDITIONAL INDEPENDENCE TEST,0.21024464831804282,"j1̸=j2
xi
j2βj + ϵi
j1.
(10)"
UNCONDITIONAL INDEPENDENCE TEST,0.21100917431192662,"It can be shown that there are deterministic relationships between the regression coefficients and the
245"
UNCONDITIONAL INDEPENDENCE TEST,0.2117737003058104,"covariance and precision matrices of X, as illustrated below and proved in App. A.7.1.
246"
UNCONDITIONAL INDEPENDENCE TEST,0.21253822629969418,"βj = Σ−1
−j−jΣ−jj ∈Rp−1,
βj,k = −ωj,k"
UNCONDITIONAL INDEPENDENCE TEST,0.21330275229357798,"ωj,j
,
j ̸= k,
(11)"
UNCONDITIONAL INDEPENDENCE TEST,0.21406727828746178,"where Σ−j−j is the submatrix of Σ without jth column and jth row, and the Σ−jj is the vector of jth
247"
UNCONDITIONAL INDEPENDENCE TEST,0.21483180428134557,"column without jth row. βj,k ∈R is the surrogate of ωj,k to capture the independence relationship of
248"
UNCONDITIONAL INDEPENDENCE TEST,0.21559633027522937,"Xj with Xk conditioning on other variables. We can use Def. 2.2, Def. 2.3 and Def. 2.4 to get the
249"
UNCONDITIONAL INDEPENDENCE TEST,0.21636085626911314,"estimation ˆΣ−j−j and ˆΣ−jj and thus get the estimation ˆβj.
250"
UNCONDITIONAL INDEPENDENCE TEST,0.21712538226299694,"Statistical Inference for βj,k
Nodewise regression offers a robust solution for the estimation
251"
UNCONDITIONAL INDEPENDENCE TEST,0.21788990825688073,"problem. A pertinent inquiry pertains to the construction of the distribution of ˆβj −βj. It is crucial
252"
UNCONDITIONAL INDEPENDENCE TEST,0.21865443425076453,"to recognize that the distribution of ˆσj1,j2 −σj1,j2 is already established. Therefore, if we can
253"
UNCONDITIONAL INDEPENDENCE TEST,0.21941896024464833,"conceptualize ˆβj −βj as a linear combination of ˆσj1,j2 −σj1,j2, the problem is directly solved, i.e.,
254"
UNCONDITIONAL INDEPENDENCE TEST,0.22018348623853212,"the ˆβj −βj is linear combination of dependent Gaussian variables. The underlying relationship
255"
UNCONDITIONAL INDEPENDENCE TEST,0.2209480122324159,"between these variables is as follows:
256"
UNCONDITIONAL INDEPENDENCE TEST,0.2217125382262997,"ˆβj −βj = −ˆΣ−1
−j−j

( ˆΣ−j−j −Σ−j−j)βj −( ˆΣ−jj −Σ−jj)

."
UNCONDITIONAL INDEPENDENCE TEST,0.22247706422018348,"The derivation is provided in App. A.7.2. For ease of notation, we further express the distribution of
257"
UNCONDITIONAL INDEPENDENCE TEST,0.22324159021406728,"the difference between the estimated covariance and the true covariance as
258"
UNCONDITIONAL INDEPENDENCE TEST,0.22400611620795108,"ˆσj1,j2 −σj1,j2 = 1 n n
X"
UNCONDITIONAL INDEPENDENCE TEST,0.22477064220183487,"i=1
ξi
j1,j2.
(12)"
UNCONDITIONAL INDEPENDENCE TEST,0.22553516819571864,"The specific form of ξi
j1,j2 is given in App. A.4, A.5, A.6 respectively for different cases. For
259"
UNCONDITIONAL INDEPENDENCE TEST,0.22629969418960244,"notational convenience, we express ˆΣ−j−j −Σ−j−j =
1
n
Pn
i=1 Ξi
−j,−j and ˆΣ−jj −Σ−jj =
260"
"N
PN",0.22706422018348624,"1
n
Pn
i=1 Ξi
−j,j, where ξj1,j2 is the element of the matrix Ξ at the position indexed by (j1, j2). We
261"
"N
PN",0.22782874617737003,"now propose the statistic and its asymptotic distribution for the CI test in the following theorem.
262"
"N
PN",0.22859327217125383,"Theorem 2.9 (Conditional Independence test). In our settings, under the null hypothesis that Xj and
263"
"N
PN",0.22935779816513763,"Xk are conditional statistically independent given a set of variables Z, i.e., βj,k = 0, the statistic
264"
"N
PN",0.2301223241590214,"ˆβj,k = ( ˆΣ−1
−j−j ˆΣ−jj)[k],
(13)"
"N
PN",0.2308868501529052,"where [k] denotes the element corresponding to the variable Xk in ˆΣ−1
−j−j ˆΣ−jj. The statistic ˆβj,k
265"
"N
PN",0.231651376146789,"has the asymptotic distribution:
266"
"N
PN",0.2324159021406728,"ˆβj,k ∼N(0, a[k]T 1 n2 n
X"
"N
PN",0.23318042813455658,"i=1
vec(Bi
−j)vec(Bi
−j)T )a[k]), 267"
"N
PN",0.23394495412844038,"where Bi =
Ξi
−j,−j
Ξi
−j,j"
"N
PN",0.23470948012232415,"
,
a[k]
l
= 

 
"
"N
PN",0.23547400611620795,"
ˆΣ−1
−j−j
"
"N
PN",0.23623853211009174,"[k],l ,
for l ∈{1, . . . , p −1}
Pn
q=1

ˆΣ−1
−j−j
 [k],l"
"N
PN",0.23700305810397554,"
˜βj
"
"N
PN",0.23776758409785934,"q ,
for l ∈{p, . . . , p2 −p}"
"N
PN",0.23853211009174313,"and ˜βj is βj whose βj,k = 0.
268"
"N
PN",0.2392966360856269,"In practice, we can plug in the estimation of regression parameter ˆβj and set ˆβj,k = 0 as the
269"
"N
PN",0.2400611620795107,"substitution of ˜βj to calculate the variance and do the CI test. Specifically, we can obtain the ˆβj,k
270"
"N
PN",0.2408256880733945,"using Eq. (13) where the estimated covariance terms can be calculated by solving the bridge equation
271"
"N
PN",0.2415902140672783,"Eq. 2. Under the null hypothesis that βj,k = 0 (conditional independence), we can take the calculated
272"
"N
PN",0.2423547400611621,"ˆβj,k into the distribution defined in Thm. 2.9 and obtain the p-value. If the p-value is smaller than the
273"
"N
PN",0.24311926605504589,"predefined significance level α (normally set at 0.05), we will infer the tested pairs are conditionally
274"
"N
PN",0.24388379204892965,"dependent; otherwise, we do not. The detailed derivation of the Thm. 2.9 can be found in App. A.7.2.
275"
EXPERIMENTS,0.24464831804281345,"3
Experiments
276"
EXPERIMENTS,0.24541284403669725,"We applied the proposed method DCT to synthetic data to evaluate its practical performance and
277"
EXPERIMENTS,0.24617737003058104,"compare it with Fisher-Z test [14] (for all three data types) and Chi-Square test [15] (for discrete data
278"
EXPERIMENTS,0.24694189602446484,"only) as baselines. Specifically, we investigated its Type I and Type II error and its application in
279"
EXPERIMENTS,0.24770642201834864,"causal discovery. The experiments investigating its robustness, performance in denser graphs and
280"
EXPERIMENTS,0.2484709480122324,"effectiveness in a real-world dataset can be found in App. C.
281"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2492354740061162,"3.1
On the Effect of the Cardinality of Conditioning Set and the Sample Size
282"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.25,"Our experiment investigates the variations in Type I and Type II error (1 minus power) probabilities
283"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.25076452599388377,"under two conditions. In the first scenario, we focus on the effects of modifying the sample size,
284"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2515290519877676,"denoted as n = (100, 500, 1000, 2000), while conditioning on a single variable. In the second, the
285"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.25229357798165136,"sample size is held constant at 2000, and we vary the cardinality of the conditioning set, represented
286"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2530581039755352,Continuous Mixed
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.25382262996941896,Discrete
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2545871559633027,"(a) Type I and Type II error for D=1, n=(100,500,1000,2000)
(b) Type I and Type II error for D=(1,2,3,4,5)  n=2000"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.25535168195718655,𝛼= 0.05
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2561162079510703,"Figure 2: Comparison of results of Type I and Type II error (1 −power) for all three types of tested
data (continuous, mixed, discrete) and different number of samples and cardinality of conditioning set.
The suffix attached to a test’s name denotes the cardinality of discretization; for example, ""Fsherz_4""
signifies the application of the Fsher-z test to data discretized into four levels. Chi-square test is only
applicable for the discrete case."
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.25688073394495414,"as D = (1, 2, . . . , 5). It is assumed that every variable within this conditioning set is effective, i.e.,
287"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2576452599388379,"they influence the CI of the tested pairs. We repeat each test 1500 times.
288"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.25840978593272174,"We use Y, W to denote the variables being tested and use Z to denote the variables being conditioned
289"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2591743119266055,"on. The discretized versions of the variables are denoted with a tilde symbol (e.g., ˜Z). For both con-
290"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2599388379204893,"ditions, we evaluate three distinct types of observations of tested variables: continuous observations
291"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2607033639143731,"for both variables (Y, W), discrete observations for both variables ( ˜Y , ˜W) and a mixed type ( ˜Y , W).
292"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.26146788990825687,"The variables in the conditioning set will always be discretized observations ( ˜Z).
293"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2622324159021407,"To see how well the derived asymptotic null distribution approximates the true one, we verify if
294"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.26299694189602446,"the probability of Type I error aligns with the significance level α preset in advance. We generate
295"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.26376146788990823,"true continuous multivariate Gaussian data Y, W from Zi (single i = 1 for the first scenario, and
296"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.26452599388379205,"summed over n for the second), structured as aiZi + E and Pn
i=1 aiZi + E, where ai is sampled
297"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2652905198776758,"from U(0.5, 1.5) and E follows a standard normal distribution, independent of all other variables.
298"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.26605504587155965,"This ensures Y ⊥⊥W|Z. The data are then discretized into K = (2, 4, 8, 12) levels, with boundaries
299"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2668195718654434,"randomly set based on the variable range. The first column in Fig. 2 (a) (b) shows the resulting
300"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.26758409785932724,"probability of Type I errors at the significance level α = 0.05 compared with other methods.
301"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.268348623853211,"A good test should have as small a probability of Type II error as possible, i.e., a larger power. To
302"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2691131498470948,"test the power of our DCT, we generate the continuous multivariate Gaussian data Zi from Y, W;
303"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2698776758409786,"constructed as Zi = aiY + biW + E, where ai, bi are sampled from U(0.5, 1.5) and E follows a
304"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2706422018348624,"standard normal distribution independent with all others, i.e., Y ̸⊥⊥W|Z. The same discretization
305"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2714067278287462,"approach is applied here. The second column in Fig. 2 (a) (b) shows the Type II error with the
306"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.27217125382262997,"changing number of samples and cardinality of the conditioning set compared with other methods.
307"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.27293577981651373,"From Fig. 2 (a), we note that the Type I error rates with our derived null distribution are well-
308"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.27370030581039756,"approximated at 0.05 across all three data types in both scenarios. In contrast, other testing methods
309"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.27446483180428133,"show significantly higher Type I error rates, increasing with the number of samples and the size of
310"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.27522935779816515,"the conditioning set. This indicates that such methods are more prone to erroneously concluding
311"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2759938837920489,"that tested variables are conditionally dependent. Additionally, while alternative tests demonstrate
312"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.27675840978593275,"considerable power with smaller sample sizes, our approach requires a sample size of 2000 to achieve
313"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2775229357798165,"satisfactory power, particularly in mixed and continuous cases. A possible explanation for this
314"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2782874617737003,"phenomenon is that our method binarizes discretized data, which may not effectively utilize all
315"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2790519877675841,"observations. This aspect warrants further investigation in future research. Moreover, our test shows
316"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2798165137614679,"remarkable stability in response to changes in the number of conditioning sets.
317"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.2805810397553517,"(a) fixed nodes p = 8, changing sample size n = (500, 1000, 5000, 1000)"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.28134556574923547,"(b) fixed sample size n = 5000, changing node p = (4, 6, 8, 10)"
ON THE EFFECT OF THE CARDINALITY OF CONDITIONING SET AND THE SAMPLE SIZE,0.28211009174311924,"Figure 3: Experiment result of skeleton discovery on synthetic data for changing sample size (a) and
changing number of nodes (b). Fisherz_nodis is the Fisher-z test applied to original continuous data.
We evaluate F1 (↑), Precision (↑), Recall (↑) and SHD (↓)."
APPLICATION IN CAUSAL DISCOVERY,0.28287461773700306,"3.2
Application in Causal Discovery
318"
APPLICATION IN CAUSAL DISCOVERY,0.28363914373088683,"Causal discovery aims to recover the true causal structure from the data. Constraint-based causal
319"
APPLICATION IN CAUSAL DISCOVERY,0.28440366972477066,"discovery methods like the PC algorithm [30] rely on testing CI from observations to discover causal
320"
APPLICATION IN CAUSAL DISCOVERY,0.2851681957186544,"graphs. However, in the presence of discretization, failures in testing CI leads to false conclusions
321"
APPLICATION IN CAUSAL DISCOVERY,0.28593272171253825,"about causal graphs. To evaluate the efficacy of the DCT, we construct causal graphs utilizing the
322"
APPLICATION IN CAUSAL DISCOVERY,0.286697247706422,"Bipartite Pairing (BP) model as detailed in [2], with the number of edges being one fewer than
323"
APPLICATION IN CAUSAL DISCOVERY,0.2874617737003058,"the number of nodes. The detailed generation process is provided in App. B due to limited space.
324"
APPLICATION IN CAUSAL DISCOVERY,0.2882262996941896,"Our experiment is divided into two scenarios: (a) fixed data samples n = 5000, with changing
325"
APPLICATION IN CAUSAL DISCOVERY,0.2889908256880734,"number of nodes p = (4, 6, 8, 10); (b) fixed number of nodes p = 8 and changing sample sizes
326"
APPLICATION IN CAUSAL DISCOVERY,0.2897553516819572,"n = (500, 1000, 5000, 10000).
327"
APPLICATION IN CAUSAL DISCOVERY,0.290519877675841,"Comparative analysis is conducted using the PC algorithm integrated with different testing methods.
328"
APPLICATION IN CAUSAL DISCOVERY,0.29128440366972475,"Specifically, we compare DCT against the Fisher-Z test applied to discretized data, the chi-square
329"
APPLICATION IN CAUSAL DISCOVERY,0.29204892966360857,"test, and the Fisher-Z test on original continuous data, the latter serving as a theoretical upper bound
330"
APPLICATION IN CAUSAL DISCOVERY,0.29281345565749234,"for comparison. Since the PC algorithm can only return a completed partially directed acyclic graph
331"
APPLICATION IN CAUSAL DISCOVERY,0.29357798165137616,"(CPDAG), we use the same orientation rules [11] implemented by Causal-DAG [6] to convert a
332"
APPLICATION IN CAUSAL DISCOVERY,0.29434250764525993,"CPDAG into a DAG. We evaluate both the undirected skeleton and the directed graph using criteria
333"
APPLICATION IN CAUSAL DISCOVERY,0.29510703363914376,"such as structural Hamming distance (SHD), F1 score, precision, and recall. For each setting, we
334"
APPLICATION IN CAUSAL DISCOVERY,0.2958715596330275,"run 10 graph instances with different seeds and report the mean and standard deviation of skeleton
335"
APPLICATION IN CAUSAL DISCOVERY,0.2966360856269113,"discovery in Fig. 3, and DAG in Fig. 4 in App B.
336"
APPLICATION IN CAUSAL DISCOVERY,0.2974006116207951,"According to the result, DCT exhibits performance nearly on par with the theoretical upper bound
337"
APPLICATION IN CAUSAL DISCOVERY,0.2981651376146789,"across metrics such as F1 score, precision, and Structural Hamming Distance (SHD) when the number
338"
APPLICATION IN CAUSAL DISCOVERY,0.2989296636085627,"of variables (p) is small and the sample size (n) is large. Despite a decline in performance as the
339"
APPLICATION IN CAUSAL DISCOVERY,0.2996941896024465,"number of variables increases with a smaller sample size, DCT significantly outperforms both the
340"
APPLICATION IN CAUSAL DISCOVERY,0.30045871559633025,"Fisher-Z test and the Chi-square test. Notably, in almost all settings, the recall of DCT is lower than
341"
APPLICATION IN CAUSAL DISCOVERY,0.3012232415902141,"that of the baseline tests, which is a reasonable outcome since these tests tend to infer conditional
342"
APPLICATION IN CAUSAL DISCOVERY,0.30198776758409784,"dependencies, thereby retaining all edges given the discretized observations. For instance, a fully
343"
APPLICATION IN CAUSAL DISCOVERY,0.30275229357798167,"connected graph, would achieve a recall of 1.
344"
CONCLUSION,0.30351681957186544,"4
Conclusion
345"
CONCLUSION,0.30428134556574926,"In this paper, we present a new testing method tailored for scenarios commonly encountered in
346"
CONCLUSION,0.30504587155963303,"real-world applications, where variables, though inherently continuous, are only observable in their
347"
CONCLUSION,0.3058103975535168,"discretized forms. Our method distinguishes itself from existing CI tests by effectively mitigating the
348"
CONCLUSION,0.3065749235474006,"misjudgment introduced by discretization and accurately recovering the CI relationships of latent
349"
CONCLUSION,0.3073394495412844,"continuous variables. We substantiate our approach with theoretical results and empirical validation,
350"
CONCLUSION,0.3081039755351682,"underscoring the effectiveness of our testing methods.
351"
REFERENCES,0.308868501529052,"References
352"
REFERENCES,0.30963302752293576,"[1] Constantin F Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, and Xenofon D
353"
REFERENCES,0.3103975535168196,"Koutsoukos. Local causal and markov blanket induction for causal discovery and feature selection for
354"
REFERENCES,0.31116207951070335,"classification part i: algorithms and empirical evaluation. Journal of Machine Learning Research, 11(1),
355"
REFERENCES,0.3119266055045872,"2010.
356"
REFERENCES,0.31269113149847094,"[2] Armen S Asratian, Tristan MJ Denley, and Roland Häggkvist. Bipartite graphs and their applications,
357"
REFERENCES,0.31345565749235477,"volume 131. Cambridge university press, 1998.
358"
REFERENCES,0.31422018348623854,"[3] Kunihiro Baba, Ritei Shibata, and Masaaki Sibuya. Partial correlation and conditional correlation as
359"
REFERENCES,0.3149847094801223,"measures of conditional independence. Australian & New Zealand Journal of Statistics, 46(4):657–664,
360"
REFERENCES,0.31574923547400613,"2004.
361"
REFERENCES,0.3165137614678899,"[4] Kunihiro Baba, Ritei Shibata, and Masaaki Sibuya. Partial correlation and conditional correlation as
362"
REFERENCES,0.3172782874617737,"measures of conditional independence. Australian & New Zealand Journal of Statistics, 46(4):657–664,
363"
REFERENCES,0.3180428134556575,"2004.
364"
REFERENCES,0.31880733944954126,"[5] Laurent Callot, Mehmet Caner, Esra Ulasan, and A. Özlem Önder. A nodewise regression approach to
365"
REFERENCES,0.3195718654434251,"estimating large portfolios, 2019.
366"
REFERENCES,0.32033639143730885,"[6] Chandler Squires. causaldag: creation, manipulation, and learning of causal models, 2018.
367"
REFERENCES,0.3211009174311927,"[7] Hu Changsheng and Wang Yongfeng. Investor sentiment and assets valuation. Systems Engineering
368"
REFERENCES,0.32186544342507645,"Procedia, 3:166–171, 2012.
369"
REFERENCES,0.32262996941896027,"[8] Aswath Damodaran. Investment valuation: Tools and techniques for determining the value of any asset,
370"
REFERENCES,0.32339449541284404,"volume 666. John Wiley & Sons, 2012.
371"
REFERENCES,0.3241590214067278,"[9] A Philip Dawid. Conditional independence in statistical theory. Journal of the Royal Statistical Society
372"
REFERENCES,0.32492354740061163,"Series B: Statistical Methodology, 41(1):1–15, 1979.
373"
REFERENCES,0.3256880733944954,"[10] Simon Dooms, Toon De Pessemier, and Luc Martens. Movietweetings: a movie rating dataset collected
374"
REFERENCES,0.3264525993883792,"from twitter. In Workshop on Crowdsourcing and human computation for recommender systems, CrowdRec
375"
REFERENCES,0.327217125382263,"at RecSys, volume 2013, page 43, 2013.
376"
REFERENCES,0.32798165137614677,"[11] Dorit Dor and Michael Tarsi. A simple algorithm to construct a consistent extension of a partially oriented
377"
REFERENCES,0.3287461773700306,"graph. 1992.
378"
REFERENCES,0.32951070336391436,"[12] Gary Doran, Krikamol Muandet, Kun Zhang, and Bernhard Schölkopf. A permutation-based kernel
379"
REFERENCES,0.3302752293577982,"conditional independence test. In UAI, pages 132–141, 2014.
380"
REFERENCES,0.33103975535168195,"[13] Jianqing Fan, Han Liu, Yang Ning, and Hui Zou. High dimensional semiparametric latent graphical model
381"
REFERENCES,0.3318042813455658,"for mixed data. Journal of the Royal Statistical Society Series B: Statistical Methodology, 79(2):405–421,
382"
REFERENCES,0.33256880733944955,"2017.
383"
REFERENCES,0.3333333333333333,"[14] Ronald Aylmer Fisher. On the ""Probable Error"" of a Coefficient of Correlation Deduced from a Small
384"
REFERENCES,0.33409785932721714,"Sample. Metron, 1:3–32, 1921.
385"
REFERENCES,0.3348623853211009,"[15] Karl Pearson F.R.S. X. on the criterion that a given system of deviations from the probable in the case of
386"
REFERENCES,0.33562691131498473,"a correlated system of variables is such that it can be reasonably supposed to have arisen from random
387"
REFERENCES,0.3363914373088685,"sampling. Philosophical Magazine Series 1, 50:157–175, 2009.
388"
REFERENCES,0.33715596330275227,"[16] Kenji Fukumizu, Francis R Bach, and Michael I Jordan. Dimensionality reduction for supervised learning
389"
REFERENCES,0.3379204892966361,"with reproducing kernel hilbert spaces. Journal of Machine Learning Research, 5(Jan):73–99, 2004.
390"
REFERENCES,0.33868501529051986,"[17] Sverre Urnes Johnson, Pål Gunnar Ulvenes, Tuva Øktedalen, and Asle Hoffart. Psychometric properties
391"
REFERENCES,0.3394495412844037,"of the general anxiety disorder 7-item (gad-7) scale in a heterogeneous psychiatric sample. Frontiers in
392"
REFERENCES,0.34021406727828746,"psychology, 10:449461, 2019.
393"
REFERENCES,0.3409785932721712,"[18] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. Adaptive
394"
REFERENCES,0.34174311926605505,"computation and machine learning. MIT Press, 2009.
395"
REFERENCES,0.3425076452599388,"[19] Loka Li, Ignavier Ng, Gongxu Luo, Biwei Huang, Guangyi Chen, Tongliang Liu, Bin Gu, and Kun Zhang.
396"
REFERENCES,0.34327217125382264,"Federated causal discovery from heterogeneous data, 2024.
397"
REFERENCES,0.3440366972477064,"[20] Han Liu, John Lafferty, and Larry Wasserman. The nonparanormal: Semiparametric estimation of high
398"
REFERENCES,0.34480122324159024,"dimensional undirected graphs. Journal of Machine Learning Research, 10(10), 2009.
399"
REFERENCES,0.345565749235474,"[21] Dimitris Margaritis. Distribution-free learning of bayesian network structure in continuous domains. In
400"
REFERENCES,0.3463302752293578,"AAAI, volume 5, pages 825–830, 2005.
401"
REFERENCES,0.3470948012232416,"[22] Karthik Mohan, Mike Chung, Seungyeop Han, Daniela Witten, Su-In Lee, and Maryam Fazel. Structured
402"
REFERENCES,0.34785932721712537,"learning of gaussian graphical models. Advances in neural information processing systems, 25, 2012.
403"
REFERENCES,0.3486238532110092,"[23] Sarah A Mossman, Marissa J Luft, Heidi K Schroeder, Sara T Varney, David E Fleck, Drew H Barzman,
404"
REFERENCES,0.34938837920489296,"Richard Gilman, Melissa P DelBello, and Jeffrey R Strawn. The generalized anxiety disorder 7-item
405"
REFERENCES,0.35015290519877673,"(gad-7) scale in adolescents with generalized anxiety disorder: signal detection and validation. Annals of
406"
REFERENCES,0.35091743119266056,"clinical psychiatry: official journal of the American Academy of Clinical Psychiatrists, 29(4):227, 2017.
407"
REFERENCES,0.3516819571865443,"[24] Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000.
408"
REFERENCES,0.35244648318042815,"[25] Christine Peterson, Francesco C Stingo, and Marina Vannucci. Bayesian inference of multiple gaussian
409"
REFERENCES,0.3532110091743119,"graphical models. Journal of the American Statistical Association, 110(509):159–174, 2015.
410"
REFERENCES,0.35397553516819574,"[26] Zhao Ren, Tingni Sun, Cun-Hui Zhang, and Harrison H Zhou. Asymptotic normality and optimalities in
411"
REFERENCES,0.3547400611620795,"estimation of large gaussian graphical models. 2015.
412"
REFERENCES,0.3555045871559633,"[27] Rajat Sen, Ananda Theertha Suresh, Karthikeyan Shanmugam, Alexandros G Dimakis, and Sanjay
413"
REFERENCES,0.3562691131498471,"Shakkottai. Model-powered conditional independence test. Advances in neural information processing
414"
REFERENCES,0.3570336391437309,"systems, 30, 2017.
415"
REFERENCES,0.3577981651376147,"[28] Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen, Yoshinobu Kawahara, Takashi
416"
REFERENCES,0.35856269113149847,"Washio, Patrik O. Hoyer, and Kenneth Bollen. Directlingam: A direct method for learning a linear
417"
REFERENCES,0.35932721712538224,"non-gaussian structural equation model, 2011.
418"
REFERENCES,0.36009174311926606,"[29] E Isaac Sparling and Shilad Sen. Rating: how difficult is it? In Proceedings of the fifth ACM conference on
419"
REFERENCES,0.36085626911314983,"Recommender systems, pages 149–156, 2011.
420"
REFERENCES,0.36162079510703365,"[30] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press, 2nd edition, 2000.
421"
REFERENCES,0.3623853211009174,"[31] Eric V Strobl, Kun Zhang, and Shyam Visweswaran. Approximate kernel-based conditional independence
422"
REFERENCES,0.36314984709480125,"tests for fast non-parametric causal discovery. Journal of Causal Inference, 7(1):20180017, 2019.
423"
REFERENCES,0.363914373088685,"[32] Liangjun Su and Halbert White. A nonparametric hellinger metric test for conditional independence.
424"
REFERENCES,0.3646788990825688,"Econometric Theory, 24(4):829–864, 2008.
425"
REFERENCES,0.3654434250764526,"[33] A. W. van der Vaart. M–and Z-Estimators, page 41–84. Cambridge Series in Statistical and Probabilistic
426"
REFERENCES,0.3662079510703364,"Mathematics. Cambridge University Press, 1998.
427"
REFERENCES,0.3669724770642202,"[34] A. W. van der Vaart. Stochastic Convergence, page 5–24. Cambridge Series in Statistical and Probabilistic
428"
REFERENCES,0.367737003058104,"Mathematics. Cambridge University Press, 1998.
429"
REFERENCES,0.36850152905198774,"[35] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
430"
REFERENCES,0.36926605504587157,"[36] Eric P Xing, Michael I Jordan, Richard M Karp, et al. Feature selection for high-dimensional genomic
431"
REFERENCES,0.37003058103975534,"microarray data. In Icml, volume 1, pages 601–608. Citeseer, 2001.
432"
REFERENCES,0.37079510703363916,"[37] Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model. Biometrika,
433"
REFERENCES,0.37155963302752293,"94(1):19–35, 2007.
434"
REFERENCES,0.37232415902140675,"[38] Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Kernel-based conditional indepen-
435"
REFERENCES,0.3730886850152905,"dence test and application in causal discovery. arXiv preprint arXiv:1202.3775, 2012.
436"
REFERENCES,0.3738532110091743,"[39] Yishi Zhang, Zigang Zhang, Kaijun Liu, and Gangyi Qian. An improved iamb algorithm for markov
437"
REFERENCES,0.3746177370030581,"blanket discovery. J. Comput., 5(11):1755–1761, 2010.
438"
REFERENCES,0.3753822629969419,"[40] Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming Gong, Ruichu Cai, Shohei Shimizu,
439"
REFERENCES,0.3761467889908257,"Peter Spirtes, and Kun Zhang. Causal-learn: Causal discovery in python, 2023.
440"
REFERENCES,0.3769113149847095,"Appendix for
441 442"
REFERENCES,0.37767584097859325,"“A Conditional Independence Test in the Presence of Discretization”
443"
REFERENCES,0.37844036697247707,"Appendix organization:
444 445"
REFERENCES,0.37920489296636084,"A Proof of Things
12
446"
REFERENCES,0.37996941896024466,"A.1
Proof of ˆθ
p→θ0
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
447"
REFERENCES,0.38073394495412843,"A.2
Proof of one-to-one mapping between ˆτj1,j2 with ˆσj1,j2 . . . . . . . . . . . . . . .
13
448"
REFERENCES,0.38149847094801226,"A.3
Proof of Thm. 2.5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
449"
REFERENCES,0.382262996941896,"A.4
Derivation of Lem. 2.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
450"
REFERENCES,0.3830275229357798,"A.5
Derivation of Lem. 2.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
451"
REFERENCES,0.3837920489296636,"A.6
Derivation of Lem. 2.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
452"
REFERENCES,0.3845565749235474,"A.7
Proof of Thm. 2.9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
453"
REFERENCES,0.3853211009174312,"A.7.1
Proof of Relation between Σ, Ωwith β
. . . . . . . . . . . . . . . . . . .
16
454"
REFERENCES,0.386085626911315,"A.7.2
Detailed derivation of inference for βj
. . . . . . . . . . . . . . . . . . .
17
455"
REFERENCES,0.38685015290519875,"A.8
Discussion of assumption of zero mean and identity variance . . . . . . . . . . . .
19
456"
REFERENCES,0.3876146788990826,"B
Data Generation and Figure of main experiments: causal discovery
20
457"
REFERENCES,0.38837920489296635,"C Additional experiments
21
458"
REFERENCES,0.38914373088685017,"C.1
Linear non-Gaussian and nonlinear
. . . . . . . . . . . . . . . . . . . . . . . . .
21
459"
REFERENCES,0.38990825688073394,"C.2
Denser graph
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
460"
REFERENCES,0.39067278287461776,"C.3
multivariate Gaussian with nonzero mean and non-unit variance
. . . . . . . . . .
21
461"
REFERENCES,0.39143730886850153,"C.4
Real-world dataset
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
462"
REFERENCES,0.3922018348623853,"D Related Work
24
463"
REFERENCES,0.3929663608562691,"E
Resource Usage
25
464"
REFERENCES,0.3937308868501529,"F
Limiation and Broader Impacts
25
465 466 467"
REFERENCES,0.3944954128440367,"A
Proof of Things
468"
REFERENCES,0.3952599388379205,"A.1
Proof of ˆθ
p→θ0
469"
REFERENCES,0.39602446483180426,"Lemma A.1. For the estimation ˆθ which is calculated using bridge equation 2.4 2.2 2.3,
470"
REFERENCES,0.3967889908256881,"as a zero of Ψn defined in Eq. (26),(33), (36) , will converge in probability to θ0
=
471"
REFERENCES,0.39755351681957185,"(σj1,j2, hj1, hj2), (σj1,j2, hj2), (σj1,j2) respectively.
472"
REFERENCES,0.3983180428134557,"Proof We first focus on the most challenging one where both variables are discrete. According to
473"
REFERENCES,0.39908256880733944,"the law of large numbers, for the estimated boundary ˆhj1 and ˆhj2 whose calculations are defined as
474"
REFERENCES,0.39984709480122327,"ˆhj = Φ−1(1 −ˆτj), we should have
475"
REFERENCES,0.40061162079510704,"n →∞,
ˆτj = 1 n n
X"
REFERENCES,0.4013761467889908,"i=1
1{˜xi
j>Pn ˜
Xj}
p→P( ˜Xj > E[ ˜Xj]).
(14)"
REFERENCES,0.40214067278287463,"Recall the definition P( ˜Xj > E[ ˜Xj]) = 1 −Φ(hj), according to continuous mapping theorem [34],
476"
REFERENCES,0.4029051987767584,"as long as the function Φ−1(1 −·) is continuous, we should have ˆhj
p→hj. And thus ˆhj1
p→hj1,
477"
REFERENCES,0.4036697247706422,"ˆhj2
p→hj2.
478"
REFERENCES,0.404434250764526,"We have ˆτj1,j2 = ¯Φ(ˆhj1, ˆhj2, ˆσj1,j2) and the estimation ˆσj1,j2 can be obtained through solving the
479"
REFERENCES,0.40519877675840976,"function. Similarly, we also have
480"
REFERENCES,0.4059633027522936,"n →∞,
ˆτj1,j2 = 1 n n
X"
REFERENCES,0.40672782874617736,"i=1
1{˜xi
j1>Pn ˜
Xj1}1{˜xi
j2>Pn ˜
Xj2}
p→P(˜xi
j1 > E[ ˜Xj1], ˜xi
j2 > E[ ˜Xj2]) = τj1,j2."
REFERENCES,0.4074923547400612,"(15)
Similarly, according to the continuous mapping theorem, we have ˆσj1,j2
p→σj1,j2. Thus, the
481"
REFERENCES,0.40825688073394495,"parameter (ˆσj1,j2, ˆhj1, ˆhj2)
p→(σj1,j2, hj1, hj2).
482"
REFERENCES,0.4090214067278288,"Apparently, the result above could easily extend to the mixed case where we fix ˆh1 = h1 = 0. Using
483"
REFERENCES,0.40978593272171254,"the same procedure, we should have (ˆσj1,j2, ˆhj2)
p→(σj1,j2, hj2).
484"
REFERENCES,0.4105504587155963,"For the continuous case whose estimated variance is calculated as ˆσj1,j2 =
1
n
Pn
i=1 xi
j1xi
j2 −
485"
"N
PN",0.41131498470948014,"1
n
Pn
i=1 xi
j1
1
n
Pn
i=1 xi
j2., according to law of large numbers, we should have
486"
"N
PN",0.4120795107033639,"n →∞,
ˆσj1,j2 = 1 n n
X"
"N
PN",0.41284403669724773,"i=1
xi
j1xi
j2−1 n n
X"
"N
PN",0.4136085626911315,"i=1
xi
j1
1
n n
X"
"N
PN",0.41437308868501527,"i=1
xi
j2
p→E(Xj1Xj2)−E(Xj1)E(Xj2) = σj1,j2. (16)"
"N
PN",0.4151376146788991,"A.2
Proof of one-to-one mapping between ˆτj1,j2 with ˆσj1,j2
487"
"N
PN",0.41590214067278286,"Lemma
A.2.
For
any
fixed
ˆhj1
and
ˆhj2,
T(σj1,j2; {ˆhj1, ˆhj2})
=
488
R"
"N
PN",0.4166666666666667,"x1>ˆhj1
R"
"N
PN",0.41743119266055045,"x2>ˆhj2 ϕ(xj1, xj2; σ)dxj1dxj2,
is a strictly monotonically increasing function on
489"
"N
PN",0.4181957186544342,"σ ∈(−1, 1).
490"
"N
PN",0.41896024464831805,"Proof To prove the lemma, we just need to show the gradient ∂T (σj1,j2;{ˆhj1,ˆhj2}"
"N
PN",0.4197247706422018,"∂σ
> 0 for σ ∈(−1, 1).
491"
"N
PN",0.42048929663608564,"∂T(σj1,j2; {ˆhj1, ˆhj2}"
"N
PN",0.4212538226299694,"∂σ
==
1 2π
p"
"N
PN",0.42201834862385323,"(1 −σ2)
exp "
"N
PN",0.422782874617737,"−(ˆh2
j1 −2σˆhj1ˆhj2 + ˆh2
j2)
2(1 −σ2) !"
"N
PN",0.4235474006116208,",
(17)"
"N
PN",0.4243119266055046,"which is obviously positive for σ ∈(−1, 1). Thus, we have one-to-one mapping between ˆτj1j2 with
492"
"N
PN",0.42507645259938837,"the calculated ˆσj1,j2 for fixed ˆhj1 and ˆhj2.
493"
"N
PN",0.4258409785932722,"A.3
Proof of Thm. 2.5
494"
"N
PN",0.42660550458715596,"In this section, we provide the proof of Thm. 2.5, which utilizes a regular statistical tool: Z-estimator
495"
"N
PN",0.42737003058103973,"[33]. Specifically, we are interested in the parameter θ and we have it estimation ˆθ. Let x1, . . . , xn
496"
"N
PN",0.42813455657492355,"are sampled from some true distribution P, we can construct the function characterized by the
497"
"N
PN",0.4288990825688073,"parameter θ related the x as ψθ(x). As long as we have n observations, we can construct the function
498"
"N
PN",0.42966360856269115,"as follows
499"
"N
PN",0.4304281345565749,"Ψn(θ) = 1 n n
X"
"N
PN",0.43119266055045874,"i=1
ψθ(xi) = Pnψθ.
(18)"
"N
PN",0.4319571865443425,"We further specify the form
500"
"N
PN",0.4327217125382263,"Ψ(θ) =
Z
ψθ(x)dx = Pψθ.
(19)"
"N
PN",0.4334862385321101,"Assume the estimator ˆθ is a zero of Ψn, i.e., Ψn(ˆθ) = 0 and will converge in probability to θ0, which
501"
"N
PN",0.43425076452599387,"is a zero of Ψ, i.e., Ψ(θ0) = 0. Expand Ψn(ˆθ) in a Taylor series around θ0, we should have
502"
"N
PN",0.4350152905198777,"0 = Ψn(ˆθ) = Ψn(θ0) + (ˆθ −θ0)Ψ′
n(θ0) + 1"
"N
PN",0.43577981651376146,"2(ˆθ −θ0)Ψ′′
n(θ0).
(20)"
"N
PN",0.43654434250764523,"Rearrange the equation above, we have
503"
"N
PN",0.43730886850152906,"ˆθ −θ0 = −
Ψn(θ0)"
"N
PN",0.4380733944954128,Ψ′n(θ0) + 1
"N
PN",0.43883792048929665,2(ˆθ −θ0)Ψ′′n(θ0) = −
"N
PN",0.4396024464831804,"1
n
Pn
i=1 ψθ(xi)"
"N
PN",0.44036697247706424,Ψ′n(θ0) + 1
"N
PN",0.441131498470948,"2(ˆθ −θ0)Ψ′′n(θ0)
.
(21)"
"N
PN",0.4418960244648318,"According to the central limit theorem, the numerator will be asymptotic normal with variance
504"
"N
PN",0.4426605504587156,"Pψ2
θ0/n as the mean Ψ(θ0) = 0 is zero. The first term of denominator Ψ′
n(θ0) will converge in
505"
"N
PN",0.4434250764525994,"probability to Ψ′(θ0) according to the law of large numbers. The second term ˆθ −θ0 = oP (1). 1
506"
"N
PN",0.4441896024464832,"As long as the denominator converges in probability and the numerator converges in distribution,
507"
"N
PN",0.44495412844036697,"according to Slusky’s lemma, we have
508"
"N
PN",0.44571865443425074,√n(ˆθ −θ0) ⇝N 
"N
PN",0.44648318042813456,"0,
Pψ2
θ0
(Pψ′
θ0)2 !"
"N
PN",0.44724770642201833,".
(22)"
"N
PN",0.44801223241590216,"Extend into the high-dimensional case we should have
509"
"N
PN",0.4487767584097859,"ˆθ −θ0 = −(Ψ′
n(θ0))−1Ψn(θ0),
(23)
where the second order term is omitted, further assume the matrix Pψ′
θ0 is invertible, we have
510"
"N
PN",0.44954128440366975,"√n(ˆθ −θ0) ⇝N
 
0, (Pψ′
θ0)−1Pψθ0ψT
θ0(Pψ′T
θ0 )−1
,
(24)
Specifically, in our case θ0 = (σj1,j2, Λ), where Λ is another parameter set influencing the estimation
511"
"N
PN",0.4503058103975535,"of σj1,j2 (will discuss case in case in later proof). In the practical scenario, we only have access to
512"
"N
PN",0.4510703363914373,"the estimated parameter ˆθ and the empirical distribution Pn, thus we have
513"
"N
PN",0.4518348623853211,"ˆσj1,j2 −σj1,j2
approx
∼
N

0, ((Pnψ′
ˆθ)−1PnψˆθψT
ˆθ (Pnψ′T
ˆθ )−1)1,1

.
(25)"
"N
PN",0.4525993883792049,"Under the null hypothesis of independent, σj1,j2=0. We provide the proof that ˆθ
p→θ0 of our case
514"
"N
PN",0.4533639143730887,"in App. A.1. Thus, Pnψˆθ, the function parameterized by ˆθ, should also converge in Pnψˆθ0 when
515"
"N
PN",0.4541284403669725,"n →∞. Besides, by the law of large numbers, Pnψˆθ0 will converge to Pψˆθ0. Thus, the equation
516"
"N
PN",0.45489296636085624,"above will converge to Eq. (24) when n →∞.
517"
"N
PN",0.45565749235474007,"A.4
Derivation of Lem. 2.7
518"
"N
PN",0.45642201834862384,"Let’s first focus on the most challenging case where both variables are discretized observations
519"
"N
PN",0.45718654434250766,"and our interested parameter will include ˆθ = (ˆσj1,j2, ˆhj1, ˆhj2) (Although we only care about the
520"
"N
PN",0.45795107033639143,"distribution of ˆσj1,j2 −σj1,j2, the estimation of boundary ˆhj1and ˆhj2 will influence the estimation of
521"
"N
PN",0.45871559633027525,"ˆσj1,j2, thus we need to consider all of them).
522"
"N
PN",0.459480122324159,"The next step will be to construct an appropriate criterion function ψ such that Ψn(ˆθ) = 0. Given n
523"
"N
PN",0.4602446483180428,"observations {˜x1, ˜x2, . . . , ˜xn}, which are discretized version of {x1, x2, . . . , xn} we should have
524"
"N
PN",0.4610091743119266,Ψn(ˆθ) = 
"N
PN",0.4617737003058104,"
Ψn(ˆσj1,j2)
Ψn(ˆhj1)
Ψn(ˆhj2)  = 1 n n
X"
"N
PN",0.4625382262996942,"i=1
ψˆθ(˜xi) = 1 n n
X i=1 "
"N
PN",0.463302752293578,"

ˆτ i
j1,j2 −T(ˆσj1,j2; {ˆhj1, ˆhj2})
ˆτ i
j1 −¯Φ(ˆhj1)
ˆτ i
j2 −¯Φ(ˆhj2) "
"N
PN",0.46406727828746175,"
= 0. (26) 525"
"N
PN",0.4648318042813456,Ψn(θ0) =
"N
PN",0.46559633027522934,"Ψn(σj1,j2)
Ψn(hj1)
Ψn(hj2) ! = 1 n n
X"
"N
PN",0.46636085626911317,"i=1
ψθ0(˜xi) = 1 n n
X i=1 "
"N
PN",0.46712538226299694,"
ˆτ i
j1,j2 −T(σj1,j2; {hj1, hj2})
ˆτ i
j1 −¯Φ(hj1)
ˆτ i
j2 −¯Φ(hj2) "
"N
PN",0.46788990825688076,".
(27)"
"N
PN",0.46865443425076453,"The difference between the estimated parameter with the true parameter can be expressed as
526"
"N
PN",0.4694189602446483,ˆθ −θ0 = 
"N
PN",0.4701834862385321,"
ˆσj1,j2 −σj1,j2
ˆhj1 −hj1
ˆhj2 −hj2  = −1 n n
X i=1  

"
"N
PN",0.4709480122324159,"∂Ψn(σj1,j2)"
"N
PN",0.4717125382262997,"∂σj1,j2"
"N
PN",0.4724770642201835,"∂Ψn(σj1,j2) ∂hj1"
"N
PN",0.47324159021406725,"∂Ψn(σj1,j2)"
"N
PN",0.4740061162079511,"∂hj2
∂Ψn(hj1)"
"N
PN",0.47477064220183485,"∂σj1,j2"
"N
PN",0.47553516819571867,∂Ψn(hj1) ∂hj1
"N
PN",0.47629969418960244,∂Ψn(hj1)
"N
PN",0.47706422018348627,"∂hj2
∂Ψn(hj2)"
"N
PN",0.47782874617737003,"∂σj1,j2"
"N
PN",0.4785932721712538,∂Ψn(hj2) ∂hj1
"N
PN",0.4793577981651376,"∂Ψn(hj2) ∂hj2  

 −1 · "
"N
PN",0.4801223241590214,"
ˆτ i
j1,j2 −T(σj1,j2; {hj1, hj2})
ˆτ i
j1 −¯Φ(hj1)
ˆτ i
j2 −¯Φ(hj2) "
"N
PN",0.4808868501529052,",
(28)"
"N
PN",0.481651376146789,"1We will not provide proof of this in this paper; however, interested readers may refer to [33]"
"N
PN",0.48241590214067276,"where the specific form of each entry of the gradient matrix is expressed as
527"
"N
PN",0.4831804281345566,"∂Ψn(σj1,j2)"
"N
PN",0.48394495412844035,"∂σj1,j2
= −
1 2π
q"
"N
PN",0.4847094801223242,"(1 −σ2
j1,j2)
exp "
"N
PN",0.48547400611620795,"−(h2
j1 −2σj1,j2hj1hj2 + h2
j2)
2(1 −σ2
j1,j2) ! ;"
"N
PN",0.48623853211009177,"∂Ψn(σj1,j2)"
"N
PN",0.48700305810397554,"∂hj1
=
Z ∞ hj2 1 2π
q"
"N
PN",0.4877675840978593,"1 −σ2
j1,j2
exp "
"N
PN",0.48853211009174313,"−h2
j1 −2σj1,j2hj1x2 + x2
2
2(1 −σ2
j1,j2) ! dx2;"
"N
PN",0.4892966360856269,"∂Ψn(σj1,j2)"
"N
PN",0.4900611620795107,"∂hj2
=
Z ∞ hj1 1 2π
q"
"N
PN",0.4908256880733945,"1 −σ2
j1,j2
exp "
"N
PN",0.49159021406727826,"−h2
2 −2σj1,j2hj2x1 + x2
1
2(1 −σ2
j1,j2) ! dx1;"
"N
PN",0.4923547400611621,∂Ψn(hj1)
"N
PN",0.49311926605504586,"∂σj1,j2
= 0;"
"N
PN",0.4938837920489297,∂Ψn(hj1)
"N
PN",0.49464831804281345,"∂hj1
=
1
√"
"N
PN",0.4954128440366973,2π exp 
"N
PN",0.49617737003058104,"−h2
j1
2 ! ;"
"N
PN",0.4969418960244648,∂Ψn(hj1)
"N
PN",0.49770642201834864,"∂hj2
= 0;"
"N
PN",0.4984709480122324,∂Ψn(hj2)
"N
PN",0.49923547400611623,"∂σj1,j2
= 0;"
"N
PN",0.5,∂Ψn(hj2)
"N
PN",0.5007645259938838,"∂hj1
= 0;"
"N
PN",0.5015290519877675,∂Ψn(hj2)
"N
PN",0.5022935779816514,"∂hj2
=
1
√"
"N
PN",0.5030581039755352,2π exp 
"N
PN",0.503822629969419,"−h2
j2
2 ! . (29)"
"N
PN",0.5045871559633027,"For simplicity of notation, we define
528"
"N
PN",0.5053516819571865,"ˆσj1,j2 −σj1,j2 = 1 n n
X"
"N
PN",0.5061162079510704,"i=1
ξi
j1,j2,
(30)"
"N
PN",0.5068807339449541,"where the specific form is of {ξi
j1,j2} is defined in Eq. (28). We should note that {ξi
j1,j2} are i.i.d
529"
"N
PN",0.5076452599388379,"random variables with mean zero (this property will be the key to the derivation of inference of CI).
530"
"N
PN",0.5084097859327217,"As long as our estimation ˆθ converge in probability to θ0 as proved in A.1, we have
531"
"N
PN",0.5091743119266054,"√n(ˆθ −θ0) ⇝N
 
0, ((Pψ′
θ0)−1Pψθ0ψT
θ0(Pψ′T
θ0 )−1)1,1

,
(31)"
"N
PN",0.5099388379204893,"where ψθ0 is defined in Eq. (27). However, in practice, we don’t have access to either P or θ0. In this
532"
"N
PN",0.5107033639143731,"scenario, we can plug in the empirical distribution of Pnψˆθ to get the estimated variance, i.e., the
533"
"N
PN",0.5114678899082569,"actual variance used in the calculation of ˆσj1,j2 −σj1,j2 is
534"
N,0.5122324159021406,"1
n"
N,0.5129969418960245,"
(Pnψ′
ˆθ)−1PnψˆθψT
ˆθ (Pnψ′T
ˆθ )−1"
N,0.5137614678899083,"1,1 .
(32)"
N,0.514525993883792,"A.5
Derivation of Lem. 2.8
535"
N,0.5152905198776758,"Use the same line of procedure as in the derivation of Lem. 2.7, for mixed pair of observations where
536"
N,0.5160550458715596,"Xj1 is continuous and ˜Xj2 is discrete, we can construct the criterion function
537"
N,0.5168195718654435,"Ψn(ˆθ) =
Ψn(ˆσj1,j2)
Ψn(ˆhj2) 
= 1 n n
X"
N,0.5175840978593272,"i=1
ψˆθ(˜xi) = 1 n n
X i=1"
N,0.518348623853211,"ˆτ i
j1,j2 −T(ˆσj1,j2; {0, ˆhj2})
ˆτ i
j2 −¯Φ(ˆhj2) !"
N,0.5191131498470948,"= 0.
(33) 538"
N,0.5198776758409785,"Ψn(θ0) =

Ψn(σj1,j2)
Ψn(hj2) 
= 1 n n
X"
N,0.5206422018348624,"i=1
ψθ0(˜xi) = 1 n n
X i=1"
N,0.5214067278287462,"ˆτ i
j1,j2 −T(σj1,j2; {0, hj2})
ˆτ i
j2 −¯Φ(hj2)"
N,0.52217125382263,"
.
(34)"
N,0.5229357798165137,"The difference between the estimated parameter with the true parameter can be expressed as
539"
N,0.5237003058103975,"ˆθ−θ0 =
ˆσj1,j2 −σj1,j2
ˆhj2 −hj2"
N,0.5244648318042814,"
= −1 n n
X i=1  "
N,0.5252293577981652,"∂Ψn(σj1,j2)"
N,0.5259938837920489,"∂σj1,j2"
N,0.5267584097859327,"∂Ψn(σj1,j2)"
N,0.5275229357798165,"∂hj2
∂Ψn(hj2)"
N,0.5282874617737003,"∂σj1,j2"
N,0.5290519877675841,∂Ψn(hj2) ∂hj2  
N,0.5298165137614679,"−1 ˆτ i
j1,j2 −T(σj1,j2; {0, hj2})
ˆτ i
j2 −¯Φ(hj2). 
,"
N,0.5305810397553516,"(35)
where the specific form of each entry of the gradient matrix can be found in Eq. (29). Using exactly
540"
N,0.5313455657492355,"the same procedure, we should have the same formation of the variance calculated as Eq. (32) with a
541"
N,0.5321100917431193,"different definition of ψθ0 and ψˆθ defined in Eq. (34) (33).
542"
N,0.5328746177370031,"A.6
Derivation of Lem. 2.6
543"
N,0.5336391437308868,"Use the same line of procedure as in derivation of Lem. 2.7, for a continuous pair of variables, we
544"
N,0.5344036697247706,"can construct the criterion function
545"
N,0.5351681957186545,"Ψn(ˆθ) = Ψn(ˆσj1,j2) = 1 n n
X"
N,0.5359327217125383,"i=1
xi
j1xi
j2 −1 n n
X"
N,0.536697247706422,"i=1
xi
j1
1
n n
X"
N,0.5374617737003058,"i=1
xi
j2 −ˆσj1,j2 = 0.
(36) 546"
N,0.5382262996941896,"Ψn(θ0) = Ψn(σj1,j2) = 1 n n
X"
N,0.5389908256880734,"i=1
xi
j1xi
j2 −1 n n
X"
N,0.5397553516819572,"i=1
xi
j1
1
n n
X"
N,0.540519877675841,"i=1
xi
j2 −σj1,j2.
(37)"
N,0.5412844036697247,Denote 1
N,0.5420489296636085,"n
Pn
i=1 xi
j1 as ¯xj1 and 1"
N,0.5428134556574924,"n
Pn
i=1 xi
j2 as ¯xj2. We should have
547"
N,0.5435779816513762,"ˆσj1,j2 −σj1,j2 = 1 n n
X"
N,0.5443425076452599,"i=1
xi
j1xi
j2 −¯xj1 ¯xj2 −σj1,j2.
(38)"
N,0.5451070336391437,"According to Eq. (22), we have
548"
N,0.5458715596330275,"√n(ˆσj1,j2 −σj1,j2) ⇝N "
N,0.5466360856269113,"0,
Pψ2
θ0
(Pψ′
θ0)2 !"
N,0.5474006116207951,".
(39)"
N,0.5481651376146789,"where (Pψ′
θ0)2 = 1. In practical calculation, we have the variance
549"
N,0.5489296636085627,"1
nPnψ2
ˆθ/(Pnψ′
ˆθ)2 = 1 n2 n
X"
N,0.5496941896024465,"i=1
(xi
j1xi
j2 −¯xj1 ¯xj2 −ˆσj1,j2)2.
(40)"
N,0.5504587155963303,"A.7
Proof of Thm. 2.9
550"
N,0.5512232415902141,"A.7.1
Proof of Relation between Σ, Ωwith β
551"
N,0.5519877675840978,"Consider our latent continuous variables X = (X1, . . . , Xp) ∼N(0, Σ) and do nodewise regression
552"
N,0.5527522935779816,"Xj = X−jβj + ϵj.
(41)"
N,0.5535168195718655,"We can divide its covariance Σ and its precision matrix Ω= Σ−1 into X and Y part in our regression:
553"
N,0.5542813455657493,"Σ =

Σjj
Σj−j
Σ−jj
Σ−j−j"
N,0.555045871559633,"
Ω=

Ωjj
Ωj−j
Ω−jj
Ω−j−j "
N,0.5558103975535168,".
(42)"
N,0.5565749235474006,"Just like regular linear regression, we can get
554"
N,0.5573394495412844,"n →∞,
βj = Σ−1
−j−jΣ−jj.
(43)"
N,0.5581039755351682,"From the invertibility of a block matrix
555"
N,0.558868501529052,"
A
B
C
D"
N,0.5596330275229358,"−1
=

(A −BD−1C)−1
−(A −BD−1C)−1BD−1"
N,0.5603975535168195,"−D−1C(A −BD−1C)−1
D−1 + D−1C(A −BD−1C)−1BD−1"
N,0.5611620795107034,"
.
(44)"
N,0.5619266055045872,"If A and D is invertible, we will have
556"
N,0.5626911314984709,"
A
B
C
D"
N,0.5634556574923547,"−1
=

A −BD−1C
0
0
(D −CA−1B)−1"
N,0.5642201834862385," 
I
−BD−1"
N,0.5649847094801224,"−CA−1
I"
N,0.5657492354740061,"
.
(45)"
N,0.5665137614678899,"Thus, we can get:
557"
N,0.5672782874617737,"Ωjj = Σjj −(Σj−jΣ−1
−j−jΣ−jj)−1;"
N,0.5680428134556575,"Ωj−j = −
 
Σjj −(Σj−jΣ−1
−j−jΣ−jj)−1
Σj−j(Σ−j−j)−1.
(46)"
N,0.5688073394495413,"Move one step forward:
558"
N,0.5695718654434251,"−Ω−1
jj Ωj−j = Σj−j(Σ−j−j)−1.
(47)"
N,0.5703363914373089,"Take transpose for both sides, as long as Ωis a symmetric matrix and Ω−jj = ΩT
j−j, we will have
559"
N,0.5711009174311926,"−Ω−1
jj Ω−jj = Σ−1
−j−jΣ−jj = βj.
(48)"
N,0.5718654434250765,"We should note testing Ω−jj = 0 is equivalent to testing βj = 0 as the Ωjj will always be nonzero.
560"
N,0.5726299694189603,"The variable Ω−jj captures the CI of Xj with other variables. As long as the variable Ωjj is just one
561"
N,0.573394495412844,"scalar, we can get
562"
N,0.5741590214067278,"βj,k = −ωj,k"
N,0.5749235474006116,"ωj,j
(49)"
N,0.5756880733944955,"capturing the independence relationship between variable Xj with Xk conditioning on all other
563"
N,0.5764525993883792,"variables.
564"
N,0.577217125382263,"A.7.2
Detailed derivation of inference for βj
565"
N,0.5779816513761468,"Nodewise regression allows us to use the regression parameter βj as the surrogate of Ω−jj. The
566"
N,0.5787461773700305,"problem now transfers to constructing the inference for βj, specifically, the derivation of distribution
567"
N,0.5795107033639144,"of ˆβj −βj. The overarching concept is that we are already aware of the distribution of ˆσj1,j2 −σj1,j2
568"
N,0.5802752293577982,"and we know that there exists a deterministic relationship between βj with Σ. Consequently, we can
569"
N,0.581039755351682,"express ˆβj −βj as a composite of ˆσj1,j2 −σj1,j2 to establish such an inference. Specifically, we have
570"
N,0.5818042813455657,"ˆβj −βj = ˆΣ−1
−j−j ˆΣ−jj −Σ−1
−j−jΣ−jj"
N,0.5825688073394495,"= ˆΣ−1
−j−j

ˆΣ−jj −ˆΣ−j−jΣ−1
−j−jΣ−jj
"
N,0.5833333333333334,"= −ˆΣ−1
−j−j

ˆΣ−j−jβj −Σ−j−jβj + Σ−j−jβj −ˆΣ−jj
"
N,0.5840978593272171,"= −ˆΣ−1
−j−j

( ˆΣ−j−j −Σ−j−j)βj −( ˆΣ−jj −Σ−jj)

, (50)"
N,0.5848623853211009,"where each entry in matrix ( ˆΣ−j−j −Σ−j−j) and ( ˆΣ−jj −Σ−jj) denotes the difference between
571"
N,0.5856269113149847,"estimated covariance with true covariance. Suppose that we want to test the CI of the variable X1
572"
N,0.5863914373088684,"with other variables, j = 1, then
573"
N,0.5871559633027523,ˆΣ−j−j −Σ−j−j =  
N,0.5879204892966361,"ˆσ1,1 . . . ˆσ1,j−1, ˆσ1,j+1 . . . ˆσ1,p
. . .
ˆσj−1,1 . . . ˆσj−1,j−1, ˆσj−1,j+1 . . . ˆσj−1,p
. . .
ˆσp,1 . . . ˆσp,j−1, ˆσp,j+1 . . . ˆσp,p "
N,0.5886850152905199,"
(51) −  "
N,0.5894495412844036,"σ1,1 . . . σ1,j−1, σ1,j+1 . . . σ1,p
. . .
σj−1,1 . . . σj−1,j−1, σj−1,j+1 . . . σj−1,p
. . .
σp,1 . . . σp,j−1, σp,j+1 . . . σp,p. "
N,0.5902140672782875,".
(52)"
N,0.5909785932721713,"Suppose that we want to test the CI of the variable X1 with other variables, j = 1. then
574"
N,0.591743119266055,ˆΣ−1−1 −Σ−1−1 =
N,0.5925076452599388,"""ˆσ2,2 . . . ˆσ2,p
. . .
ˆσp,2 . . . ˆσp,p # −"
N,0.5932721712538226,"""σ2,2 . . . σ2,p
. . .
σp,2 . . . σp,p # (53) := 1 n n
X i=1 "
N,0.5940366972477065,"
ξi
2,2 . . . ξi
2,p
. . .
ξi
p,2 . . . ξi
p,p "
N,0.5948012232415902,",
(54)"
N,0.595565749235474,"where {ξi
j1,j2} are i.i.d random variables with specific form defined in Eq. (28) for discrete case,
575"
N,0.5963302752293578,"Eq. (35) for mixed case and Eq. (38) in continuous case. Put them together:
576  "
N,0.5970948012232415,"ˆβ1,2 −β1,2
ˆβ1,3 −β1,3
. . .
ˆβ1,p −β1,p "
N,0.5978593272171254,"= −ˆΣ−1
−1−1
1
n n
X i=1  

  "
N,0.5986238532110092,"ξi
2,2
ξi
2,3
. . .
ξi
2,p
ξi
3,2
ξi
3,3
. . .
ξi
3,p
. . .
. . .
. . .
. . .
ξi
p,2
ξi
p,3
. . .
ξi
p,p    "
N,0.599388379204893,"β1,2
β1,3
. . .
β1,p  −  "
N,0.6001529051987767,"ξi
2,1
ξi
3,1
. . .
ξi
p,1   "
N,0.6009174311926605,"

.
(55) As 1"
N,0.6016819571865444,"n
Pn
i=1 ξi
j1,j2 is asymptotically normal, the who vector of ˆβ1 −β1 is a linear combination of
577"
N,0.6024464831804281,"Gaussian distribution. However, We cannot merely engage in a linear combination of its variance as
578"
N,0.6032110091743119,"they are dependent with each other. For example, if Y1, Y2 are dependent and we are trying to find
579"
N,0.6039755351681957,"out V ar(aY1 + bY2), we should have
580"
N,0.6047400611620795,"V ar(aY1 + bY2) = [a
b]

V ar(Y1)
Cov(Y1, Y2)
Cov(Y1, Y2)
V ar(Y2)"
N,0.6055045871559633," 
a
b"
N,0.6062691131498471,"
.
(56)"
N,0.6070336391437309,"Now, suppose we are interested in the distribution of ˆβ1,2 −β1,2, we should have
581"
N,0.6077981651376146,"ˆβ1,2 −β1,2 = 1 n n
X"
N,0.6085626911314985,"i=1
( ˆΣ−1
−1−1)[2],:  

  "
N,0.6093272171253823,"ξi
2,2
ξi
2,3
. . .
ξi
2,p
ξi
3,2
ξi
3,3
. . .
ξi
3,p
. . .
. . .
. . .
. . .
ξi
p,2
ξi
p,3
. . .
ξi
p,p    "
N,0.6100917431192661,"β1,2
β1,3
. . .
β1,p  −  "
N,0.6108562691131498,"ξi
2,1
ξi
3,1
. . .
ξi
p,1   "
N,0.6116207951070336,"

,
(57)"
N,0.6123853211009175,"where ( ˆΣ−1
−1−1)[2],: is the row of index of X2 of ˆΣ−1
−1−1 ([2] denotes the index of the variable). For
582"
N,0.6131498470948012,"ease of notation, let
583"
N,0.613914373088685,"Ξi
−1,−1 =  "
N,0.6146788990825688,"ξi
2,2
ξi
2,3
. . .
ξi
2,p
ξi
3,2
ξi
3,3
. . .
ξi
3,p
. . .
. . .
. . .
. . .
ξi
p,2
ξi
p,3
. . .
ξi
p,p "
N,0.6154434250764526,",
Ξi
−1,1 =  "
N,0.6162079510703364,"ξi
2,1
ξi
3,1
. . .
ξi
p,1 "
N,0.6169724770642202,",
(58)"
N,0.617737003058104,"and let
584"
N,0.6185015290519877,"Bi
−1 = "
N,0.6192660550458715,"



"
N,0.6200305810397554,"ξi
2,1
ξi
3,1
. . .
ξi
p,1
ξi
2,2
ξi
2,3
. . .
ξi
2,p
ξi
3,2
ξi
3,3
. . .
ξi
3,p
. . .
. . .
. . .
. . .
ξi
p,2
ξi
p,3
. . .
ξi
p,p "
N,0.6207951070336392,"




(59)"
N,0.6215596330275229,"as the concatenation of those two matrices. The variance is calculated as
585"
N,0.6223241590214067,"V ar
√n(ˆβ1,2 −β1,2)

= a[2]T 1 n n
X"
N,0.6230886850152905,"i=1
vec(Bi
−1)vec(Bi
−1)T a[2],
(60)"
N,0.6238532110091743,"where
586"
N,0.6246177370030581,"a[2]
l
= 

 
"
N,0.6253822629969419,"
ˆΣ−1
−1−1
"
N,0.6261467889908257,"[2],l ,
for l ∈{1, . . . , p −1}
Pn
q=1

ˆΣ−1
−1−1
"
N,0.6269113149847095,"[2],l (β1)q ,
for l ∈{p, . . . , p2 −p}
(61)"
N,0.6276758409785933,"vec(Bi
−1) is the squeezed vector form of matrix vec(Bi
−1) ∈Rp×p−1, i.e.,
587"
N,0.6284403669724771,"vec(Bi
−1) = "
N,0.6292048929663608,"


"
N,0.6299694189602446,"ξi
2,1
ξi
3,1
...
ξi
p,p "
N,0.6307339449541285,"


.
(62)"
N,0.6314984709480123,"Thus, the distribution of ˆβj,k −βj,k is
588"
N,0.632262996941896,"ˆβj,k −βj,k ∼N(0, a[k]T 1 n2 n
X"
N,0.6330275229357798,"i=1
vec(Bi
−j)vec(Bi
−j)T )a[k]).
(63)"
N,0.6337920489296636,"In practice, we can plug in the estimates of βj to estimate the interested distribution and do the CI
589"
N,0.6345565749235474,"test by hypothesizing βj,k = 0.
590"
N,0.6353211009174312,"A.8
Discussion of assumption of zero mean and identity variance
591"
N,0.636085626911315,"In this section, we engage in a more thorough discussion regarding our assumptions about X.
592"
N,0.6368501529051988,"Specifically, we demonstrate that this assumption of mean and variance does not compromise the
593"
N,0.6376146788990825,"generality. In other words, the true model may possess different mean and variance values, but we
594"
N,0.6383792048929664,"proceed by treating it as having a mean of zero and identity variance.
595"
N,0.6391437308868502,"The key ingredient allowing us to assume such a model is, the discretization function gj is an unknown
nonlinear monotonic function. Suppose the g′
j maps the continuous domain to a binary variable, and
we have the ""groundtruth"" variable, denoted X′
j, with mean a and variance b. Assume the cardinality
of the discretized domain is only 2, i.e., our observation ˜Xj can only be 0 or 1. We further have the
constant d′
j as the discretization boundary such that we have the observation"
N,0.6399082568807339,"˜Xj = 1(g′
j(X′
j) > d′
j) = 1(X′
j > g′−1
j
(dj))"
N,0.6406727828746177,"We can always produce our assumed variable Xj with mean 0 and variance 1, such that Xj =
596 1
√"
N,0.6414373088685015,"bX′
j −
a
√"
N,0.6422018348623854,"b and the same observation with a different nonlinear transformation gj and decision
597"
N,0.6429663608562691,"boundary dj, such that
598"
N,0.6437308868501529,"˜Xj = 1(gj(Xj) > dj) = 1(Xj > g−1
j (dj)) = 1(X′
j >
√"
N,0.6444954128440367,"bg−1
j (dj) + a)"
N,0.6452599388379205,"As long as the observation ˜Xj is the same, we should have
√"
N,0.6460244648318043,"bg−1
j (dj)+a = g′−1
j
(dj). Our assumed
599"
N,0.6467889908256881,"model Xj clearly mimics the ""groundtruth"" X′
j. Besides, according to Lem. A.2, we have one-to-
600"
N,0.6475535168195719,"one mapping between ˆτj1j2 with the estimated covariance for fixed ˆhj1, ˆhj2. Thus, as long as the
601"
N,0.6483180428134556,"observation is the same, the estimation of covariance ˆσj1,j2 remains unaffected by our assumptions
602"
N,0.6490825688073395,"regarding the mean and variance of X, so do the following inference.
603"
N,0.6498470948012233,"We further conduct casual discovery experiments to empirically validate our statement, which is
604"
N,0.650611620795107,"shown in App. C.3.
605"
N,0.6513761467889908,"B
Data Generation and Figure of main experiments: causal discovery
606"
N,0.6521406727828746,"Data Generation and Code
We construct the true DAG G using the Bipartite Pairing (BP) model
607"
N,0.6529051987767585,"[2], with the number of edges being one fewer than the number of nodes. The subsequent generation of
608"
N,0.6536697247706422,"true multivariate Gaussian data involves assigning causal weights drawn from a uniform distribution
609"
N,0.654434250764526,"U ∼(0.5, 2) and incorporating noise via samples from a standard normal distribution for each
610"
N,0.6551987767584098,"variable. Following this, we binarize the data, setting the threshold randomly based on each variable’s
611"
N,0.6559633027522935,"range. The code implementation is based on [40] .
612"
N,0.6567278287461774,"(a) fixed nodes p = 8, changing sample size n = (500, 1000, 5000, 1000)"
N,0.6574923547400612,"4
6
8
10
Numb of variables 0.5 1.0"
N,0.658256880733945,F1 score (Direction)
N,0.6590214067278287,"4
6
8
10
Numb of variables 0.5 1.0"
N,0.6597859327217125,Precision (Direction)
N,0.6605504587155964,"4
6
8
10
Numb of variables 0.5 1.0"
N,0.6613149847094801,Recall (Direction)
N,0.6620795107033639,"4
6
8
10
Numb of variables 0 10 20"
N,0.6628440366972477,SHD (Direction)
N,0.6636085626911316,"DCT
Fisherz
Fisherz_nodis
Chis-q"
N,0.6643730886850153,"(b) fixed sample size n = 5000, changing node p = (4, 6, 8, 10)"
N,0.6651376146788991,"Figure 4: Experiment result of DAG discovery on synthetic data for changing sample size (a) and
changing number of nodes (b). Fisherz_nodis is the Fisher-z test applied to original continuous data.
We evaluate F1 (↑), Precision (↑), Recall (↑) and SHD (↓)."
N,0.6659021406727829,"C
Additional experiments
613"
N,0.6666666666666666,"C.1
Linear non-Gaussian and nonlinear
614"
N,0.6674311926605505,"Our model requires that the original data must adhere to the hypothesis of following a multivariate
615"
N,0.6681957186544343,"normal distribution, which appears to potentially limit the generalizability. Therefore, it is worthwhile
616"
N,0.668960244648318,"to explore its robustness when such assumptions are violated. In this regard, we conducted several
617"
N,0.6697247706422018,"experiments, including scenarios involving linear non-Gaussian and nonlinear Gaussian.
618"
N,0.6704892966360856,"For both cases, we follow the setting of our experiment where there are p = 8 nodes and p −1
619"
N,0.6712538226299695,"edges. We explore the effect of changing sample size n = (100, 500, 2000, 5000). Specifically for
620"
N,0.6720183486238532,"linear non-Gaussian case, we adhere to some of the settings outlined by [28], conducting experiments
621"
N,0.672782874617737,"where the original continuous data followed: (1) a Student’s t-distribution with 3 degrees of freedom,
622"
N,0.6735474006116208,"(2) a uniform distribution, and (3) an exponential distribution. Each variable is generated as Xi =
623"
N,0.6743119266055045,"f(PAi) + noise, where noise follows the distribution in (1), (2), (3) correspondingly and f is a
624"
N,0.6750764525993884,"linear function. The first three rows of Fig. 5 and Fig. 6 show the result of the linear non-Gaussian
625"
N,0.6758409785932722,"case.
626"
N,0.676605504587156,"For the nonlinear cases, we follow setting in [19], where every variable Xi is generated as Xi =
627"
N,0.6773700305810397,"f(WPAi + noise), noise ∼N(0, 1) and f is a function randomly chosen from (a) f(x) = sin(x),
628"
N,0.6781345565749235,"(b) f(x) = x3, (c) f(x) = tanh(x), and (d) f(x) = ReLU(x). W is a linear function. Similarly,
629"
N,0.6788990825688074,"we set the number of nodes at p = 8 and change the number of samples n = (500, 2000, 5000).
630"
N,0.6796636085626911,"For both cases, we run 10 graph instances with different seeds and report the result of skeleton
631"
N,0.6804281345565749,"discovery in Fig. 5 and DAG in Fig. 6 (The same orientation rules [11] used in the main experiment
632"
N,0.6811926605504587,"are employed to convert a CPDAG [6] into a DAG). The last row of Fig. 5 and Fig. 6 shows the result
633"
N,0.6819571865443425,"of the nonlinear case.
634"
N,0.6827217125382263,"Based on the experimental outcomes, DCT demonstrates marginally superior or comparable efficacy
635"
N,0.6834862385321101,"in terms of the F1-score, precision, and SHD relative to both the Fisher-Z test and the Chi-square test
636"
N,0.6842507645259939,"when dealing with small sample sizes. Nevertheless, as the sample size increases, DCT’s performance
637"
N,0.6850152905198776,"clearly surpasses that of the aforementioned tests across all three evaluated metrics, especially in the
638"
N,0.6857798165137615,"linear case. Consistent with observations from the main experiment, DCT exhibits a lower recall in
639"
N,0.6865443425076453,"comparison to the baseline tests. This discrepancy can be attributed to the baseline tests being prone
640"
N,0.6873088685015291,"to incorrectly infer conditional dependence and connect a large proportion of nodes. According to
641"
N,0.6880733944954128,"the results, our test shows notable robustness under the case assumptions are violated, confirming its
642"
N,0.6888379204892966,"practical effectiveness.
643"
N,0.6896024464831805,"C.2
Denser graph
644"
N,0.6903669724770642,"DCT primarily works on cases where CI is mistakenly judged as conditional dependence due
645"
N,0.691131498470948,"to discretization. Consequently, its efficacy is more pronounced in scenarios characterized by a
646"
N,0.6918960244648318,"relatively sparse graph, as numerous instances are truly conditionally independent. Nevertheless, the
647"
N,0.6926605504587156,"investigation of causal discovery with a dense latent graph is essential for evaluating the power of a
648"
N,0.6934250764525994,"test, i.e., its ability to successfully reject the null hypothesis when the tested pairs are conditionally
649"
N,0.6941896024464832,"dependent. Thus, we conduct the experiment where p = 8, n = 10000 and changing edges (p +
650"
N,0.694954128440367,"2, p + 4, p + 6). Similarly, the latent continuous data follows a multivariate Gaussian model and
651"
N,0.6957186544342507,"the true DAG G is constructed using BP model. We run 10 graph instances with different seeds and
652"
N,0.6964831804281345,"report the result of the skeleton discovery and DAG in Fig. 7.
653"
N,0.6972477064220184,"According to the experiment results, DCT exhibits better performance in terms of the F1-score,
654"
N,0.6980122324159022,"precision, and SHD relative to both the Fisher-Z test and the Chi-square test. As the graph becomes
655"
N,0.6987767584097859,"progressively denser, the superiority of the Discrete Causality Test (DCT) correspondingly diminishes
656"
N,0.6995412844036697,"as there are few conditional independent cases in the true DAG. Due to the same reason, The recall
657"
N,0.7003058103975535,"remains lower than that of other baseline methods.
658"
N,0.7010703363914373,"C.3
multivariate Gaussian with nonzero mean and non-unit variance
659"
N,0.7018348623853211,"We employed a setting nearly identical to the main experiment, with the only difference being the
660"
N,0.7025993883792049,"alteration in data generation: instead of using a standard normal distribution, we used a Gaussian
661"
N,0.7033639143730887,"distribution with mean sampled from U(−2, 2) and variance sampled from U(0, 3). We fix the
662"
N,0.7041284403669725,"number of variables as p = 8 and change the number of samples n = (100, 500, 2000, 5000). The
663"
N,0.7048929663608563,"Fig. 8 shows the result and demonstrates the effectiveness of our method.
664"
N,0.7056574923547401,(a) Linear Exponential.
N,0.7064220183486238,(b) Linear Student.
N,0.7071865443425076,(c) Linear Uniform.
N,0.7079510703363915,(d) Nonlinear Gaussian.
N,0.7087155963302753,"Figure 5:
Experiment result of causal discovery on synthetic data with p
=
8, n
=
(100, 500, 2000, 5000) where the data generation process violates our assumptions. The data are
generated with either nongaussian distributed (a), (b), (c) or the relations are not linear (d). The figure
reports F1 (↑), Precision (↑), Recall (↑) and SHD (↓) on skeleton."
N,0.709480122324159,"C.4
Real-world dataset
665"
N,0.7102446483180428,"To further validate DCT, we employ it on a real-world dataset:
Big Five Personality
666"
N,0.7110091743119266,"https://openpsychometrics.org/, which includes 50 personality indicators and over 19000 data sam-
667"
N,0.7117737003058104,"ples. Each variable contains 5 possible discrete values to represent the scale of the corresponding
668"
N,0.7125382262996942,"questions, where 1=Disagree, 2=Weakly disagree, 3=Neutral, 4=Weakly agree and 5=Agree, e.g.,
669"
N,0.713302752293578,"""N3=1"" means ""I agree that I worry about things"". This scenario clearly suits DCT, where the degree
670"
N,0.7140672782874617,"of agreement with a certain question must be a continuous variable while we can only observe the
671"
N,0.7148318042813455,"result after categorization. We choose three variables respectively: [N3: I worry about things], [N10:
672"
N,0.7155963302752294,"I often feel blue ], [N4: I seldom feel blue]. We then do the casual discovery using PC algorithm with
673"
N,0.7163608562691132,"DCT and compare it with the Chi-square test and Fisher-Z test. The result can be found in Fig. 9.
674"
N,0.7171253822629969,"Based on the experimental outcomes, despite the absence of a groundtruth for reference, we observe
675"
N,0.7178899082568807,"that the results obtained via DCT appear more plausible than those derived from Fisher-Z and Chi-
676"
N,0.7186544342507645,"square tests. Specifically, DCT suggests the relationship N3 ⊥⊥N4|N10, which is reasonable as
677"
N,0.7194189602446484,"intuitively, the answer of ’I often feel blue’ already captures the information of ’I seldom feel blue’.
678"
N,0.7201834862385321,(a) Linear Exponential.
N,0.7209480122324159,(b) Linear Student.
N,0.7217125382262997,(c) Linear Uniform.
N,0.7224770642201835,(d) Nonlinear Gaussian.
N,0.7232415902140673,"Figure 6:
Experiment result of causal discovery on synthetic data with p
=
8, n
=
(100, 500, 2000, 5000) where the data generation process violates our assumptions. The data are
generated with either nongaussian distributed (a), (b), (c) or the relations are not linear (d). The figure
reports F1 (↑), Precision (↑), Recall (↑) and SHD (↓) on DAG."
N,0.7240061162079511,"As a comparison, both Fisher-Z and Chi-square return a fully connected graph. The results directly
679"
N,0.7247706422018348,"correspond to our illustrative example shown in Fig. 1, substantiating the necessity of our proposed
680"
N,0.7255351681957186,"test.
681"
N,0.7262996941896025,"Figure 7: Experimental comparison of causal discovery on synthetic datasets for denser graphs with
p = 8, n = 10000 and edges varying p + 2, p + 4, p + 6. We evaluate F1 (↑), Precision (↑), Recall
(↑) and SHD (↓) on both skeleton and DAG."
N,0.7270642201834863,"Figure 8: Experimental comparison of causal discovery on synthetic datasets for multivariate Gaussian
model with p = 8, n = (100, 500, 2000, 5000) and where mean is not zero. We evaluate F1 (↑),
Precision (↑), Recall (↑) and SHD (↓) on both skeleton and DAG."
N,0.72782874617737,"D
Related Work
682"
N,0.7285932721712538,"Testing for CI is pivotal in the field of causal discovery [30], and a variety of methods exist for
683"
N,0.7293577981651376,"performing CI tests (CI tests). An important group of CI test methods involves the assumption of
684"
N,0.7301223241590215,"Gaussian variables with linear dependencies. For example, under this assumption, Gaussian graphical
685"
N,0.7308868501529052,"models are extensively studied [37, 25, 22, 26]. To address CI test under Gaussian assumption, partial
686"
N,0.731651376146789,"correlation serves as a viable method for CI testing [4]. To evaluate the independence of variables
687"
N,0.7324159021406728,"X1 and X2 conditional on Z, The technique proposed by [32] determines CI by comparing the
688"
N,0.7331804281345565,"estimations of p(X1|X2, Z) and p(X1|X2).
689"
N,0.7339449541284404,"[N3]
I worry about things [N10]"
N,0.7347094801223242,I often feel blue [N4]
N,0.735474006116208,I seldom feel blue
N,0.7362385321100917,(a) Fisher-Z test
N,0.7370030581039755,"[N3]
I worry about things [N10]"
N,0.7377675840978594,I often feel blue [N4]
N,0.7385321100917431,I seldom feel blue
N,0.7392966360856269,(b) Chi-square test [N3]
N,0.7400611620795107,I seldom feel blue [N10]
N,0.7408256880733946,"I often feel blue
I worry about things [N4]"
N,0.7415902140672783,(c) DCT
N,0.7423547400611621,Figure 9: Experimental comparison of causal discovery on the real-world dataset.
N,0.7431192660550459,"Another approach involves discretizing Z and performing independent tests within each resulting bin
690"
N,0.7438837920489296,"[21]. Our work, however, diverges from these existing methods in two significant ways. Firstly, we
691"
N,0.7446483180428135,"are equipped to handle data, where partial variables are discretized. Additionally, we postulate that
692"
N,0.7454128440366973,"discrete variables are derived from the transformation of continuous variables in a latent Gaussian
693"
N,0.746177370030581,"model. With the same assumption, the most closely related study is by [13], where the authors
694"
N,0.7469418960244648,"developed a novel rank-based estimator for the precision matrix of mixed data. However, their work
695"
N,0.7477064220183486,"stops short of providing a CI test for this method. Our research fills this gap, offering the ability to
696"
N,0.7484709480122325,"estimate the precision matrix for both discrete and mixed data and providing a rigorous CI test for
697"
N,0.7492354740061162,"our methodology.
698"
N,0.75,"Recent advancements in CI testing have utilized kernel methods for continuous variables influenced
699"
N,0.7507645259938838,"by nonlinear relationships. [16] describes non-parametric CI relationships using covariance operators
700"
N,0.7515290519877675,"in reproducing kernel Hilbert spaces (RKHS). KCI test [38] assesses the partial associations of
701"
N,0.7522935779816514,"regression functions linking x, y, and z, while RCI test [31] aims to enhance the KCI test’s efficiency.
702"
N,0.7530581039755352,"In KCIP test [12] employs permutations of samples to emulate CI scenarios. CCI test [27] further
703"
N,0.753822629969419,"reformulates testing into a process that leverages the capabilities of supervised learning models. For
704"
N,0.7545871559633027,"discrete variable analysis, the G2 test [1] and conditional mutual information [39] are commonly
705"
N,0.7553516819571865,"employed. However, their method cannot deal with our setting where only discretized version of
706"
N,0.7561162079510704,"latent variables can be observed.
707"
N,0.7568807339449541,"E
Resource Usage
708"
N,0.7576452599388379,"All the experiments are run using Intel(R) Xeon(R) CPU E5-2680 v4 with 55 processors. It costs 4
709"
N,0.7584097859327217,"hours to run experiments in Section 3.1.
710"
N,0.7591743119266054,"F
Limiation and Broader Impacts
711"
N,0.7599388379204893,"Limitation
So far, the largest limitation of our method is to treat discretized variables as binary,
712"
N,0.7607033639143731,"which wastes the available information. Besides that, the parametric assumption limits its generaliz-
713"
N,0.7614678899082569,"ability. However, we need to point out this is pretty normal in CI test fields.
714"
N,0.7622324159021406,"Broader Impacts
The goal of our proposed method is to test the conditional independence relation-
715"
N,0.7629969418960245,"ship given discretized observation. This task is essential and has broad applications. We are confident
716"
N,0.7637614678899083,"that our method will be beneficial and will not result in negative societal impacts.
717"
N,0.764525993883792,"NeurIPS Paper Checklist
718"
CLAIMS,0.7652905198776758,"1. Claims
719"
CLAIMS,0.7660550458715596,"Question: Do the main claims made in the abstract and introduction accurately reflect the
720"
CLAIMS,0.7668195718654435,"paper’s contributions and scope?
721"
CLAIMS,0.7675840978593272,"Answer: [Yes]
722"
CLAIMS,0.768348623853211,"Justification: Section1 Introduction and Abstract
723"
CLAIMS,0.7691131498470948,"Guidelines:
724"
CLAIMS,0.7698776758409785,"• The answer NA means that the abstract and introduction do not include the claims
725"
CLAIMS,0.7706422018348624,"made in the paper.
726"
CLAIMS,0.7714067278287462,"• The abstract and/or introduction should clearly state the claims made, including the
727"
CLAIMS,0.77217125382263,"contributions made in the paper and important assumptions and limitations. A No or
728"
CLAIMS,0.7729357798165137,"NA answer to this question will not be perceived well by the reviewers.
729"
CLAIMS,0.7737003058103975,"• The claims made should match theoretical and experimental results, and reflect how
730"
CLAIMS,0.7744648318042814,"much the results can be expected to generalize to other settings.
731"
CLAIMS,0.7752293577981652,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
732"
CLAIMS,0.7759938837920489,"are not attained by the paper.
733"
LIMITATIONS,0.7767584097859327,"2. Limitations
734"
LIMITATIONS,0.7775229357798165,"Question: Does the paper discuss the limitations of the work performed by the authors?
735"
LIMITATIONS,0.7782874617737003,"Answer: [Yes]
736"
LIMITATIONS,0.7790519877675841,"Justification: Section2.1 line145-line147, Appendix F
737"
LIMITATIONS,0.7798165137614679,"Guidelines:
738"
LIMITATIONS,0.7805810397553516,"• The answer NA means that the paper has no limitation while the answer No means that
739"
LIMITATIONS,0.7813455657492355,"the paper has limitations, but those are not discussed in the paper.
740"
LIMITATIONS,0.7821100917431193,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
741"
LIMITATIONS,0.7828746177370031,"• The paper should point out any strong assumptions and how robust the results are to
742"
LIMITATIONS,0.7836391437308868,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
743"
LIMITATIONS,0.7844036697247706,"model well-specification, asymptotic approximations only holding locally). The authors
744"
LIMITATIONS,0.7851681957186545,"should reflect on how these assumptions might be violated in practice and what the
745"
LIMITATIONS,0.7859327217125383,"implications would be.
746"
LIMITATIONS,0.786697247706422,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
747"
LIMITATIONS,0.7874617737003058,"only tested on a few datasets or with a few runs. In general, empirical results often
748"
LIMITATIONS,0.7882262996941896,"depend on implicit assumptions, which should be articulated.
749"
LIMITATIONS,0.7889908256880734,"• The authors should reflect on the factors that influence the performance of the approach.
750"
LIMITATIONS,0.7897553516819572,"For example, a facial recognition algorithm may perform poorly when image resolution
751"
LIMITATIONS,0.790519877675841,"is low or images are taken in low lighting. Or a speech-to-text system might not be
752"
LIMITATIONS,0.7912844036697247,"used reliably to provide closed captions for online lectures because it fails to handle
753"
LIMITATIONS,0.7920489296636085,"technical jargon.
754"
LIMITATIONS,0.7928134556574924,"• The authors should discuss the computational efficiency of the proposed algorithms
755"
LIMITATIONS,0.7935779816513762,"and how they scale with dataset size.
756"
LIMITATIONS,0.7943425076452599,"• If applicable, the authors should discuss possible limitations of their approach to
757"
LIMITATIONS,0.7951070336391437,"address problems of privacy and fairness.
758"
LIMITATIONS,0.7958715596330275,"• While the authors might fear that complete honesty about limitations might be used by
759"
LIMITATIONS,0.7966360856269113,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
760"
LIMITATIONS,0.7974006116207951,"limitations that aren’t acknowledged in the paper. The authors should use their best
761"
LIMITATIONS,0.7981651376146789,"judgment and recognize that individual actions in favor of transparency play an impor-
762"
LIMITATIONS,0.7989296636085627,"tant role in developing norms that preserve the integrity of the community. Reviewers
763"
LIMITATIONS,0.7996941896024465,"will be specifically instructed to not penalize honesty concerning limitations.
764"
THEORY ASSUMPTIONS AND PROOFS,0.8004587155963303,"3. Theory Assumptions and Proofs
765"
THEORY ASSUMPTIONS AND PROOFS,0.8012232415902141,"Question: For each theoretical result, does the paper provide the full set of assumptions and
766"
THEORY ASSUMPTIONS AND PROOFS,0.8019877675840978,"a complete (and correct) proof?
767"
THEORY ASSUMPTIONS AND PROOFS,0.8027522935779816,"Answer: [Yes]
768"
THEORY ASSUMPTIONS AND PROOFS,0.8035168195718655,"Justification: Assumption: Section2 line81 to line 94, Proof: Appendix A.
769"
THEORY ASSUMPTIONS AND PROOFS,0.8042813455657493,"Guidelines:
770"
THEORY ASSUMPTIONS AND PROOFS,0.805045871559633,"• The answer NA means that the paper does not include theoretical results.
771"
THEORY ASSUMPTIONS AND PROOFS,0.8058103975535168,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
772"
THEORY ASSUMPTIONS AND PROOFS,0.8065749235474006,"referenced.
773"
THEORY ASSUMPTIONS AND PROOFS,0.8073394495412844,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
774"
THEORY ASSUMPTIONS AND PROOFS,0.8081039755351682,"• The proofs can either appear in the main paper or the supplemental material, but if
775"
THEORY ASSUMPTIONS AND PROOFS,0.808868501529052,"they appear in the supplemental material, the authors are encouraged to provide a short
776"
THEORY ASSUMPTIONS AND PROOFS,0.8096330275229358,"proof sketch to provide intuition.
777"
THEORY ASSUMPTIONS AND PROOFS,0.8103975535168195,"• Inversely, any informal proof provided in the core of the paper should be complemented
778"
THEORY ASSUMPTIONS AND PROOFS,0.8111620795107034,"by formal proofs provided in appendix or supplemental material.
779"
THEORY ASSUMPTIONS AND PROOFS,0.8119266055045872,"• Theorems and lemmas that the proof relies upon should be properly referenced.
780"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8126911314984709,"4. Experimental Result Reproducibility
781"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8134556574923547,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
782"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8142201834862385,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
783"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8149847094801224,"of the paper (regardless of whether the code and data are provided or not)?
784"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8157492354740061,"Answer: [Yes]
785"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8165137614678899,"Justification: Secition3 and Appendix B,C.
786"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8172782874617737,"Guidelines:
787"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8180428134556575,"• The answer NA means that the paper does not include experiments.
788"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8188073394495413,"• If the paper includes experiments, a No answer to this question will not be perceived
789"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8195718654434251,"well by the reviewers: Making the paper reproducible is important, regardless of
790"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8203363914373089,"whether the code and data are provided or not.
791"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8211009174311926,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
792"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8218654434250765,"to make their results reproducible or verifiable.
793"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8226299694189603,"• Depending on the contribution, reproducibility can be accomplished in various ways.
794"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.823394495412844,"For example, if the contribution is a novel architecture, describing the architecture fully
795"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8241590214067278,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
796"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8249235474006116,"be necessary to either make it possible for others to replicate the model with the same
797"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8256880733944955,"dataset, or provide access to the model. In general. releasing code and data is often
798"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8264525993883792,"one good way to accomplish this, but reproducibility can also be provided via detailed
799"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.827217125382263,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
800"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8279816513761468,"of a large language model), releasing of a model checkpoint, or other means that are
801"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8287461773700305,"appropriate to the research performed.
802"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8295107033639144,"• While NeurIPS does not require releasing code, the conference does require all submis-
803"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8302752293577982,"sions to provide some reasonable avenue for reproducibility, which may depend on the
804"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.831039755351682,"nature of the contribution. For example
805"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8318042813455657,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
806"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8325688073394495,"to reproduce that algorithm.
807"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8333333333333334,"(b) If the contribution is primarily a new model architecture, the paper should describe
808"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8340978593272171,"the architecture clearly and fully.
809"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8348623853211009,"(c) If the contribution is a new model (e.g., a large language model), then there should
810"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8356269113149847,"either be a way to access this model for reproducing the results or a way to reproduce
811"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8363914373088684,"the model (e.g., with an open-source dataset or instructions for how to construct
812"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8371559633027523,"the dataset).
813"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8379204892966361,"(d) We recognize that reproducibility may be tricky in some cases, in which case
814"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8386850152905199,"authors are welcome to describe the particular way they provide for reproducibility.
815"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8394495412844036,"In the case of closed-source models, it may be that access to the model is limited in
816"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8402140672782875,"some way (e.g., to registered users), but it should be possible for other researchers
817"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8409785932721713,"to have some path to reproducing or verifying the results.
818"
OPEN ACCESS TO DATA AND CODE,0.841743119266055,"5. Open access to data and code
819"
OPEN ACCESS TO DATA AND CODE,0.8425076452599388,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
820"
OPEN ACCESS TO DATA AND CODE,0.8432721712538226,"tions to faithfully reproduce the main experimental results, as described in supplemental
821"
OPEN ACCESS TO DATA AND CODE,0.8440366972477065,"material?
822"
OPEN ACCESS TO DATA AND CODE,0.8448012232415902,"Answer: [Yes]
823"
OPEN ACCESS TO DATA AND CODE,0.845565749235474,"Justification: We provide the full code in our supplementary.
824"
OPEN ACCESS TO DATA AND CODE,0.8463302752293578,"Guidelines:
825"
OPEN ACCESS TO DATA AND CODE,0.8470948012232415,"• The answer NA means that paper does not include experiments requiring code.
826"
OPEN ACCESS TO DATA AND CODE,0.8478593272171254,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
827"
OPEN ACCESS TO DATA AND CODE,0.8486238532110092,"public/guides/CodeSubmissionPolicy) for more details.
828"
OPEN ACCESS TO DATA AND CODE,0.849388379204893,"• While we encourage the release of code and data, we understand that this might not be
829"
OPEN ACCESS TO DATA AND CODE,0.8501529051987767,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
830"
OPEN ACCESS TO DATA AND CODE,0.8509174311926605,"including code, unless this is central to the contribution (e.g., for a new open-source
831"
OPEN ACCESS TO DATA AND CODE,0.8516819571865444,"benchmark).
832"
OPEN ACCESS TO DATA AND CODE,0.8524464831804281,"• The instructions should contain the exact command and environment needed to run to
833"
OPEN ACCESS TO DATA AND CODE,0.8532110091743119,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
834"
OPEN ACCESS TO DATA AND CODE,0.8539755351681957,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
835"
OPEN ACCESS TO DATA AND CODE,0.8547400611620795,"• The authors should provide instructions on data access and preparation, including how
836"
OPEN ACCESS TO DATA AND CODE,0.8555045871559633,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
837"
OPEN ACCESS TO DATA AND CODE,0.8562691131498471,"• The authors should provide scripts to reproduce all experimental results for the new
838"
OPEN ACCESS TO DATA AND CODE,0.8570336391437309,"proposed method and baselines. If only a subset of experiments are reproducible, they
839"
OPEN ACCESS TO DATA AND CODE,0.8577981651376146,"should state which ones are omitted from the script and why.
840"
OPEN ACCESS TO DATA AND CODE,0.8585626911314985,"• At submission time, to preserve anonymity, the authors should release anonymized
841"
OPEN ACCESS TO DATA AND CODE,0.8593272171253823,"versions (if applicable).
842"
OPEN ACCESS TO DATA AND CODE,0.8600917431192661,"• Providing as much information as possible in supplemental material (appended to the
843"
OPEN ACCESS TO DATA AND CODE,0.8608562691131498,"paper) is recommended, but including URLs to data and code is permitted.
844"
OPEN ACCESS TO DATA AND CODE,0.8616207951070336,"6. Experimental Setting/Details
845"
OPEN ACCESS TO DATA AND CODE,0.8623853211009175,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
846"
OPEN ACCESS TO DATA AND CODE,0.8631498470948012,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
847"
OPEN ACCESS TO DATA AND CODE,0.863914373088685,"results?
848"
OPEN ACCESS TO DATA AND CODE,0.8646788990825688,"Answer: [Yes]
849"
OPEN ACCESS TO DATA AND CODE,0.8654434250764526,"Justification: Section3 and Appendix B, C.
850"
OPEN ACCESS TO DATA AND CODE,0.8662079510703364,"Guidelines:
851"
OPEN ACCESS TO DATA AND CODE,0.8669724770642202,"• The answer NA means that the paper does not include experiments.
852"
OPEN ACCESS TO DATA AND CODE,0.867737003058104,"• The experimental setting should be presented in the core of the paper to a level of detail
853"
OPEN ACCESS TO DATA AND CODE,0.8685015290519877,"that is necessary to appreciate the results and make sense of them.
854"
OPEN ACCESS TO DATA AND CODE,0.8692660550458715,"• The full details can be provided either with the code, in appendix, or as supplemental
855"
OPEN ACCESS TO DATA AND CODE,0.8700305810397554,"material.
856"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8707951070336392,"7. Experiment Statistical Significance
857"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8715596330275229,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
858"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8723241590214067,"information about the statistical significance of the experiments?
859"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8730886850152905,"Answer: [Yes]
860"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8738532110091743,"Justification: Section 3 and Appendix B, C.
861"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8746177370030581,"Guidelines:
862"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8753822629969419,"• The answer NA means that the paper does not include experiments.
863"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8761467889908257,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
864"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8769113149847095,"dence intervals, or statistical significance tests, at least for the experiments that support
865"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8776758409785933,"the main claims of the paper.
866"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8784403669724771,"• The factors of variability that the error bars are capturing should be clearly stated (for
867"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8792048929663608,"example, train/test split, initialization, random drawing of some parameter, or overall
868"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8799694189602446,"run with given experimental conditions).
869"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8807339449541285,"• The method for calculating the error bars should be explained (closed form formula,
870"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8814984709480123,"call to a library function, bootstrap, etc.)
871"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.882262996941896,"• The assumptions made should be given (e.g., Normally distributed errors).
872"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8830275229357798,"• It should be clear whether the error bar is the standard deviation or the standard error
873"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8837920489296636,"of the mean.
874"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8845565749235474,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
875"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8853211009174312,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
876"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.886085626911315,"of Normality of errors is not verified.
877"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8868501529051988,"• For asymmetric distributions, the authors should be careful not to show in tables or
878"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8876146788990825,"figures symmetric error bars that would yield results that are out of range (e.g. negative
879"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8883792048929664,"error rates).
880"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8891437308868502,"• If error bars are reported in tables or plots, The authors should explain in the text how
881"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8899082568807339,"they were calculated and reference the corresponding figures or tables in the text.
882"
EXPERIMENTS COMPUTE RESOURCES,0.8906727828746177,"8. Experiments Compute Resources
883"
EXPERIMENTS COMPUTE RESOURCES,0.8914373088685015,"Question: For each experiment, does the paper provide sufficient information on the com-
884"
EXPERIMENTS COMPUTE RESOURCES,0.8922018348623854,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
885"
EXPERIMENTS COMPUTE RESOURCES,0.8929663608562691,"the experiments?
886"
EXPERIMENTS COMPUTE RESOURCES,0.8937308868501529,"Answer: [Yes]
887"
EXPERIMENTS COMPUTE RESOURCES,0.8944954128440367,"Justification: Appendix E.
888"
EXPERIMENTS COMPUTE RESOURCES,0.8952599388379205,"Guidelines:
889"
EXPERIMENTS COMPUTE RESOURCES,0.8960244648318043,"• The answer NA means that the paper does not include experiments.
890"
EXPERIMENTS COMPUTE RESOURCES,0.8967889908256881,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
891"
EXPERIMENTS COMPUTE RESOURCES,0.8975535168195719,"or cloud provider, including relevant memory and storage.
892"
EXPERIMENTS COMPUTE RESOURCES,0.8983180428134556,"• The paper should provide the amount of compute required for each of the individual
893"
EXPERIMENTS COMPUTE RESOURCES,0.8990825688073395,"experimental runs as well as estimate the total compute.
894"
EXPERIMENTS COMPUTE RESOURCES,0.8998470948012233,"• The paper should disclose whether the full research project required more compute
895"
EXPERIMENTS COMPUTE RESOURCES,0.900611620795107,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
896"
EXPERIMENTS COMPUTE RESOURCES,0.9013761467889908,"didn’t make it into the paper).
897"
CODE OF ETHICS,0.9021406727828746,"9. Code Of Ethics
898"
CODE OF ETHICS,0.9029051987767585,"Question: Does the research conducted in the paper conform, in every respect, with the
899"
CODE OF ETHICS,0.9036697247706422,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
900"
CODE OF ETHICS,0.904434250764526,"Answer: [Yes]
901"
CODE OF ETHICS,0.9051987767584098,"Justification: We completely follow NeurIPS Code of Ethics.
902"
CODE OF ETHICS,0.9059633027522935,"Guidelines:
903"
CODE OF ETHICS,0.9067278287461774,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
904"
CODE OF ETHICS,0.9074923547400612,"• If the authors answer No, they should explain the special circumstances that require a
905"
CODE OF ETHICS,0.908256880733945,"deviation from the Code of Ethics.
906"
CODE OF ETHICS,0.9090214067278287,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
907"
CODE OF ETHICS,0.9097859327217125,"eration due to laws or regulations in their jurisdiction).
908"
BROADER IMPACTS,0.9105504587155964,"10. Broader Impacts
909"
BROADER IMPACTS,0.9113149847094801,"Question: Does the paper discuss both potential positive societal impacts and negative
910"
BROADER IMPACTS,0.9120795107033639,"societal impacts of the work performed?
911"
BROADER IMPACTS,0.9128440366972477,"Answer: [Yes]
912"
BROADER IMPACTS,0.9136085626911316,"Justification: We propose a new conditional independence test with applications range in
913"
BROADER IMPACTS,0.9143730886850153,"multiple fields. Please refer to Appendix F.
914"
BROADER IMPACTS,0.9151376146788991,"Guidelines:
915"
BROADER IMPACTS,0.9159021406727829,"• The answer NA means that there is no societal impact of the work performed.
916"
BROADER IMPACTS,0.9166666666666666,"• If the authors answer NA or No, they should explain why their work has no societal
917"
BROADER IMPACTS,0.9174311926605505,"impact or why the paper does not address societal impact.
918"
BROADER IMPACTS,0.9181957186544343,"• Examples of negative societal impacts include potential malicious or unintended uses
919"
BROADER IMPACTS,0.918960244648318,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
920"
BROADER IMPACTS,0.9197247706422018,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
921"
BROADER IMPACTS,0.9204892966360856,"groups), privacy considerations, and security considerations.
922"
BROADER IMPACTS,0.9212538226299695,"• The conference expects that many papers will be foundational research and not tied
923"
BROADER IMPACTS,0.9220183486238532,"to particular applications, let alone deployments. However, if there is a direct path to
924"
BROADER IMPACTS,0.922782874617737,"any negative applications, the authors should point it out. For example, it is legitimate
925"
BROADER IMPACTS,0.9235474006116208,"to point out that an improvement in the quality of generative models could be used to
926"
BROADER IMPACTS,0.9243119266055045,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
927"
BROADER IMPACTS,0.9250764525993884,"that a generic algorithm for optimizing neural networks could enable people to train
928"
BROADER IMPACTS,0.9258409785932722,"models that generate Deepfakes faster.
929"
BROADER IMPACTS,0.926605504587156,"• The authors should consider possible harms that could arise when the technology is
930"
BROADER IMPACTS,0.9273700305810397,"being used as intended and functioning correctly, harms that could arise when the
931"
BROADER IMPACTS,0.9281345565749235,"technology is being used as intended but gives incorrect results, and harms following
932"
BROADER IMPACTS,0.9288990825688074,"from (intentional or unintentional) misuse of the technology.
933"
BROADER IMPACTS,0.9296636085626911,"• If there are negative societal impacts, the authors could also discuss possible mitigation
934"
BROADER IMPACTS,0.9304281345565749,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
935"
BROADER IMPACTS,0.9311926605504587,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
936"
BROADER IMPACTS,0.9319571865443425,"feedback over time, improving the efficiency and accessibility of ML).
937"
SAFEGUARDS,0.9327217125382263,"11. Safeguards
938"
SAFEGUARDS,0.9334862385321101,"Question: Does the paper describe safeguards that have been put in place for responsible
939"
SAFEGUARDS,0.9342507645259939,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
940"
SAFEGUARDS,0.9350152905198776,"image generators, or scraped datasets)?
941"
SAFEGUARDS,0.9357798165137615,"Answer: [NA]
942"
SAFEGUARDS,0.9365443425076453,"Justification: Method proposed in this paper don’t pose such risks.
943"
SAFEGUARDS,0.9373088685015291,"Guidelines:
944"
SAFEGUARDS,0.9380733944954128,"• The answer NA means that the paper poses no such risks.
945"
SAFEGUARDS,0.9388379204892966,"• Released models that have a high risk for misuse or dual-use should be released with
946"
SAFEGUARDS,0.9396024464831805,"necessary safeguards to allow for controlled use of the model, for example by requiring
947"
SAFEGUARDS,0.9403669724770642,"that users adhere to usage guidelines or restrictions to access the model or implementing
948"
SAFEGUARDS,0.941131498470948,"safety filters.
949"
SAFEGUARDS,0.9418960244648318,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
950"
SAFEGUARDS,0.9426605504587156,"should describe how they avoided releasing unsafe images.
951"
SAFEGUARDS,0.9434250764525994,"• We recognize that providing effective safeguards is challenging, and many papers do
952"
SAFEGUARDS,0.9441896024464832,"not require this, but we encourage authors to take this into account and make a best
953"
SAFEGUARDS,0.944954128440367,"faith effort.
954"
LICENSES FOR EXISTING ASSETS,0.9457186544342507,"12. Licenses for existing assets
955"
LICENSES FOR EXISTING ASSETS,0.9464831804281345,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
956"
LICENSES FOR EXISTING ASSETS,0.9472477064220184,"the paper, properly credited and are the license and terms of use explicitly mentioned and
957"
LICENSES FOR EXISTING ASSETS,0.9480122324159022,"properly respected?
958"
LICENSES FOR EXISTING ASSETS,0.9487767584097859,"Answer: [Yes]
959"
LICENSES FOR EXISTING ASSETS,0.9495412844036697,"Justification: We have cited the dataset we use and we provide the code we based in
960"
LICENSES FOR EXISTING ASSETS,0.9503058103975535,"Appendix B.
961"
LICENSES FOR EXISTING ASSETS,0.9510703363914373,"Guidelines:
962"
LICENSES FOR EXISTING ASSETS,0.9518348623853211,"• The answer NA means that the paper does not use existing assets.
963"
LICENSES FOR EXISTING ASSETS,0.9525993883792049,"• The authors should cite the original paper that produced the code package or dataset.
964"
LICENSES FOR EXISTING ASSETS,0.9533639143730887,"• The authors should state which version of the asset is used and, if possible, include a
965"
LICENSES FOR EXISTING ASSETS,0.9541284403669725,"URL.
966"
LICENSES FOR EXISTING ASSETS,0.9548929663608563,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
967"
LICENSES FOR EXISTING ASSETS,0.9556574923547401,"• For scraped data from a particular source (e.g., website), the copyright and terms of
968"
LICENSES FOR EXISTING ASSETS,0.9564220183486238,"service of that source should be provided.
969"
LICENSES FOR EXISTING ASSETS,0.9571865443425076,"• If assets are released, the license, copyright information, and terms of use in the
970"
LICENSES FOR EXISTING ASSETS,0.9579510703363915,"package should be provided. For popular datasets, paperswithcode.com/datasets
971"
LICENSES FOR EXISTING ASSETS,0.9587155963302753,"has curated licenses for some datasets. Their licensing guide can help determine the
972"
LICENSES FOR EXISTING ASSETS,0.959480122324159,"license of a dataset.
973"
LICENSES FOR EXISTING ASSETS,0.9602446483180428,"• For existing datasets that are re-packaged, both the original license and the license of
974"
LICENSES FOR EXISTING ASSETS,0.9610091743119266,"the derived asset (if it has changed) should be provided.
975"
LICENSES FOR EXISTING ASSETS,0.9617737003058104,"• If this information is not available online, the authors are encouraged to reach out to
976"
LICENSES FOR EXISTING ASSETS,0.9625382262996942,"the asset’s creators.
977"
NEW ASSETS,0.963302752293578,"13. New Assets
978"
NEW ASSETS,0.9640672782874617,"Question: Are new assets introduced in the paper well documented and is the documentation
979"
NEW ASSETS,0.9648318042813455,"provided alongside the assets?
980"
NEW ASSETS,0.9655963302752294,"Answer: [Yes]
981"
NEW ASSETS,0.9663608562691132,"Justification: We have submitted the code.
982"
NEW ASSETS,0.9671253822629969,"Guidelines:
983"
NEW ASSETS,0.9678899082568807,"• The answer NA means that the paper does not release new assets.
984"
NEW ASSETS,0.9686544342507645,"• Researchers should communicate the details of the dataset/code/model as part of their
985"
NEW ASSETS,0.9694189602446484,"submissions via structured templates. This includes details about training, license,
986"
NEW ASSETS,0.9701834862385321,"limitations, etc.
987"
NEW ASSETS,0.9709480122324159,"• The paper should discuss whether and how consent was obtained from people whose
988"
NEW ASSETS,0.9717125382262997,"asset is used.
989"
NEW ASSETS,0.9724770642201835,"• At submission time, remember to anonymize your assets (if applicable). You can either
990"
NEW ASSETS,0.9732415902140673,"create an anonymized URL or include an anonymized zip file.
991"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9740061162079511,"14. Crowdsourcing and Research with Human Subjects
992"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747706422018348,"Question: For crowdsourcing experiments and research with human subjects, does the paper
993"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9755351681957186,"include the full text of instructions given to participants and screenshots, if applicable, as
994"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9762996941896025,"well as details about compensation (if any)?
995"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9770642201834863,"Answer: [NA]
996"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97782874617737,"Justification: We don’t use any crowdsourcing resoruce.
997"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785932721712538,"Guidelines:
998"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793577981651376,"• The answer NA means that the paper does not involve crowdsourcing nor research with
999"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801223241590215,"human subjects.
1000"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808868501529052,"• Including this information in the supplemental material is fine, but if the main contribu-
1001"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981651376146789,"tion of the paper involves human subjects, then as much detail as possible should be
1002"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824159021406728,"included in the main paper.
1003"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831804281345565,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1004"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9839449541284404,"or other labor should be paid at least the minimum wage in the country of the data
1005"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847094801223242,"collector.
1006"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985474006116208,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1007"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862385321100917,"Subjects
1008"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9870030581039755,"Question: Does the paper describe potential risks incurred by study participants, whether
1009"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9877675840978594,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1010"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9885321100917431,"approvals (or an equivalent approval/review based on the requirements of your country or
1011"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892966360856269,"institution) were obtained?
1012"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900611620795107,"Answer: [NA]
1013"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908256880733946,"Justification: NA.
1014"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915902140672783,"Guidelines:
1015"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923547400611621,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1016"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931192660550459,"human subjects.
1017"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938837920489296,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1018"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946483180428135,"may be required for any human subjects research. If you obtained IRB approval, you
1019"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9954128440366973,"should clearly state this in the paper.
1020"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996177370030581,"• We recognize that the procedures for this may vary significantly between institutions
1021"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969418960244648,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1022"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977064220183486,"guidelines for their institution.
1023"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9984709480122325,"• For initial submissions, do not include any information that would break anonymity (if
1024"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9992354740061162,"applicable), such as the institution conducting the review.
1025"
