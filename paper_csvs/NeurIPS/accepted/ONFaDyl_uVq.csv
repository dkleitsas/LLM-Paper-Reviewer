Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004784688995215311,"Algorithmic pricing on online e-commerce platforms raises the concern of tacit
collusion, where reinforcement learning algorithms learn to set collusive prices
in a decentralized manner and through nothing more than proﬁt feedback. This
raises the question as to whether collusive pricing can be prevented through the
design of suitable ""buy boxes,"" i.e., through the design of the rules that govern
the elements of e-commerce sites that promote particular products and prices to
consumers. In this paper, we demonstrate that reinforcement learning (RL) can also
be used by platforms to learn buy box rules that are effective in preventing collusion
by RL sellers. For this, we adopt the methodology of Stackelberg POMDPs, and
demonstrate success in learning robust rules that continue to provide high consumer
welfare together with sellers employing different behavior models or having out-of-
distribution costs for goods."
INTRODUCTION,0.009569377990430622,"1
Introduction"
INTRODUCTION,0.014354066985645933,"The last decade has witnessed a dramatic shift of trading from retailers to online e-commerce platforms
such as Amazon and Alibaba. In these platforms, sellers are increasingly using algorithms to set prices.
Algorithmic pricing can be beneﬁcial for market efﬁciency, enabling sellers to quickly react to market
changes and also in enabling price competition. At the same time, the U.S. Federal Trade Commission
(FTC) U.S. Federal Trade Commission (2018) and European Commission (The Organisation for
Economic Co-operation and Development, 2017) have raised concerns that algorithmic pricing may
facilitate collusive behaviors. Calvano et al. (2020a) support these concerns through a study of pricing
agents in a simulated platform economy, and show that commonly used reinforcement-learning
(RL) algorithms learn to initiate and sustain collusive behaviors.1 Assad et al. (2020) also provide
empirical support for algorithmic collusion in a study of Germany’s retail gas stations, showing an
association between algorithmic pricing and an increase in price markups. As highlighted by Calvano
et al. (2020b), these kinds of collusive behaviors are unlikely to be a violation of antitrust law, as they
are learned responses to proﬁt signals and not the result of explicit agreements."
INTRODUCTION,0.019138755980861243,"One can try to prevent algorithmic collusion by introducing suitable rules by which platforms can
choose which sellers to promote to buyers, thus promoting competition. Could Amazon’s ""buy box"
INTRODUCTION,0.023923444976076555,"*These authors contributed equally to this work.
1More precisely, Calvano et al. (2020a) showed that RL algorithms learn to quote supra-competitive prices
and punish deviations from collusive agreements via lower prices."
INTRODUCTION,0.028708133971291867,"algorithm,"" for example, play this role in the future, in determining for a given consumer search
which products and prices to highlight to a consumer? Responding to this, Johnson et al. (2021)
design hand-crafted rules that succeed in hindering collusion between RL algorithms. At the same
time, their rules introduce the undesirable effect of limiting consumers to a single seller, and there
remains potential for more effective interventions."
INTRODUCTION,0.03349282296650718,"In this paper, we demonstrate for the ﬁrst time how RL can also be used defensively by a platform
to automatically design rules that promote consumer welfare and prevent collusive pricing. This is
a problem of multi-agent learning, with the interaction between the platform and sellers modeled
as a Stackelberg game (Fudenberg and Tirole, 1991). The leader is the platform designer and sets
the platform rules and the sellers respond, using RL to set prices given these rules. We introduce the
class of threshold platform rules, and formally show that this class contains rules that approximately
maximize consumer surplus in a unique subgame perfect equilibrium (Mas-Colell et al., 1995, chapter
9). At the same time, this class of threshold rules is fragile to unexpected deviations by sellers, for
example caused by cost perturbations. The role of RL on the part of the platform is to learn rules
with similar performance that are also more robust."
INTRODUCTION,0.03827751196172249,"To solve the Stackelberg problem, we make use of the Stackelberg partially observable Markov
decision process (POMDP) framework (Brero et al., 2022), which deﬁnes an episode structure of a
POMDP such that the RL algorithm representing the leader will learn to optimize its reward (here,
consumer surplus) given that its rules cause re-equilibration on the part of the followers (here, the
sellers who use Q-learning algorithms to set prices). The Stackelberg POMDP framework is well
formed as long as the re-equilibration behavior of the sellers can be modeled through Markovian
dynamics, as is the case with Q-learning."
INTRODUCTION,0.0430622009569378,"We show successful results in learning effective platform policies that outperform handcrafted
rules (Johnson et al., 2021). This demonstrates how the Stackelberg POMDP framework can be
successfully applied in settings where followers play repeated games, and their strategies are also
policies trained via reinforcement learning algorithms. We then show how our threshold platform
rules allow us to obtain a similar learning performance when training the platform policy ""in the
wild,"" i.e., without accessing the sellers’ private information. With this, we demonstrate how the
Stackelberg POMDP framework can be applied in more general learning scenarios than the ofﬂine
learning ones for which it was originally designed. Finally, we show how the platform rules learned
via our Stackelberg POMDP framework continue to be effective when market conditions change, for
example as the result of a change to the cost structure of sellers."
INTRODUCTION,0.04784688995215311,"Further related work.
Zheng et al. (2022); Tang (2017); Shen et al. (2020); Brero et al. (2021)
make use of RL to optimize different economic systems (including matching markets, internet
advertising, tax policies, and auctions) under strategic agents’ responses. Unlike our work, these
methods do not leverage the designer’s commitment power or the Stackelberg structure of the induced
game. Brero et al. (2022) introduce and study the Stackelberg POMDP framework for matrix games
and simple auctions using regret minimization for followers.2 Abada and Lambin (2022) study
collusion by RL pricing in markets for electric power, and use machine learning by a regulator agent
for the mitigation of collusion, albeit without a Stackelberg framing. The broader research program
on differentiable economics uses representation learning for optimal economic design (Duetting
et al., 2019; Shen et al., 2019; Kuo et al., 2020; Tacchetti et al., 2019; Rahme et al., 2021a; Curry
et al., 2022; Rahme et al., 2021b; Curry et al., 2020; Peri et al., 2021); this work avoids the need for
Stackelberg design by emphasizing the use of direct, incentive-compatible mechanisms. Also related
is empirical mechanism design (Areyan Viqueira et al., 2019; Vorobeychik et al., 2006; Brinkman
and Wellman, 2017), which applies empirical game theory to search for the equilibria of mechanisms
with a set of candidate strategies (Wellman, 2006; Kiekintveld and Wellman, 2008; Jordan et al.,
2010); see also Bünz et al. (2018) for the design of iterative auctions."
THE ONLY OTHER METHOD WE KNOW FOR STACKELBERG LEARNING IN STOCHASTIC GAMES WITH PROVABLY GUARANTEES,0.05263157894736842,"2The only other method we know for Stackelberg learning in stochastic games with provably guarantees
solves for a single follower (Mishra et al., 2020); see also Mguni et al. (2019); Cheng et al. (2017); Shi et al.
(2020) and Shu and Tian (2019), and Tharakunnel and Bhattacharyya (2007) for a partial convergence result for
a static game with two followers. For other convergence results for single-follower, static, and often zero-sum
games see Li et al. (2019); Sengupta and Kambhampati (2020); Xu et al. (2021); Fiez et al. (2020); Jin et al.
(2020). For multi-follower static games, Wang et al. (2022) make use of a differentiable relaxation of follower
best-response behavior together with a subroutine to solve an optimization problem for follower behavior."
PRELIMINARIES,0.05741626794258373,"2
Preliminaries"
PRELIMINARIES,0.06220095693779904,"Seller Competition Model.
There is a set of sellers N = {1, . . . , n}, each of whom sells a
differentiated product on an economic platform. Each seller has the same marginal cost c > 0 for
producing one unit of its product. Sellers interact with each other repeatedly over time in setting
prices and selling goods. At each time period, t = 0, 1, . . . , each seller i observes all past prices, and
sets a price pi,t ≥0. We let pt = (p1,t, . . . , pn,t) denote a generic price proﬁle quoted at time t. The
platform sets the rules of a buy box that governs, in each period t, which set Nt ✓N of sellers are
available. Consumers can only buy from these sellers and others forfeit sales. There is also an outside
option, indexed by 0, which provides each consumer with a fallback choice with zero utility."
PRELIMINARIES,0.06698564593301436,"Following Johnson et al. (2021), competition between sellers for consumer demand is modeled
through the standard logit model of consumer choice. For this, seller i has quality index ↵i > 0,
this providing horizontal differentiation across products, and the outside good has quality index
↵0 > 0. In the logit model, each consumer samples ⇣0, ⇣1, ...⇣n, independently from a type I extreme
value distribution with scale parameter µ > 0, for each product and the outside option, with utility
↵i+⇣i−pi,t for product i, and ↵0+⇣0 for the outside option. Considering a unit mass of consumers in
period t, seller i 2 Nt receives fractional demand Di(pt; Nt) = exp((↵i−pi,t)/µ)/λ(pt; Nt), where
λ(pt; Nt) = P"
PRELIMINARIES,0.07177033492822966,"j2Nt exp((↵j −pj,t)/µ)+exp(↵0/µ), and any seller i /2 Nt has zero demand. Scale
parameter µ > 0 serves to control the extent of horizontal differentiation, with no differentiation and
perfect substitutes obtained as µ ! 0. The total consumer surplus is U(pt; Nt) = µ · log[λ(pt; Nt)],
and is maximized with minimum prices and all sellers displayed (so consumers have a full choice of
products). Seller i’s proﬁt ⇢i in period t is ⇢i(pt; Nt) = (pi,t −c) · Di(pt; Nt), i.e., its per-unit proﬁt
multiplied by demand."
PRELIMINARIES,0.07655502392344497,"Reinforcement learning by sellers.
In a single-agent Markov decision process (MDP), an agent
faces a sequential decision problem under uncertainty. At each step t, the agent observes a state
variable st 2 S and chooses an action at 2 A. Upon action at in state st, the agent obtains
reward r(st, at), and the environment moves to state st+1 according to q(st+1|st, at). We let
⌧= (s0, a0, ..., sT , aT ) denote a state-action trajectory determined by executing policy policy
⇡: S ! A, and q⇡(⌧) denote the probability of trajectory ⌧. The optimal policy ⇡⇤solves
⇡⇤2 argmax⇡E⌧⇠q⇡(⌧)[PT"
PRELIMINARIES,0.08133971291866028,"t=0 δtr(st, at)], where δ 2 [0, 1] is the discount factor and time-horizon
T can be ﬁnite or inﬁnite. In a partially-observable MDP (POMDP), the policy ⇡cannot access
state st but only observation ot sampled from q(ot|st). A multi-agent MDP (Boutilier, 1996) for
n agents has states S common to all agents and a set of actions Ai for each agent i. When each
agent i picks action ai,t in state st, the environment moves to state st+1 according to a distribution
q(st+1|st, a1,t, .., an,t) and agent i obtains a reward ri(st, at) that depends on the joint action. We
follow Calvano et al. (2020a) and Johnson et al. (2021) and adopt decentralized Q-learning by sellers
as a positive theory for sellers in regard to their behavior in setting prices on an e-commerce platform
(see Appendix A). Although Q-learners may not converge, we also conﬁrm these earlier studies in
showing convergence in our simulations (deﬁned over a particular time horizon as detailed by Johnson
et al. (2021))."
THE PLATFORM STACKELBERG PROBLEM,0.0861244019138756,"3
The Platform Stackelberg Problem"
THE PLATFORM STACKELBERG PROBLEM,0.09090909090909091,"To formalize the problem facing the platform designer in mitigating collusive behavior by sellers, we
model the interaction between the platform, which sets the rules of the buy box, and the sellers as a
Stackelberg game. The platform designer is the leader, and ﬁxes the platform rules. The sellers are
the followers, and play an inﬁnitely repeated game according to these rules. As discussed above, and
following Calvano et al. (2020a) and Johnson et al. (2021), we model the sellers’ behavior through
decentralized Q-learning. As a result, the problem facing the platform is a behavioral Stackelberg
problem, in that the followers are modeled as Q-learners (and need not, necessarily, be playing an
equilibrium of the induced game)."
THE PLATFORM STACKELBERG PROBLEM,0.09569377990430622,"The sellers.
In this model, we ﬁx the states that comprise the MDP of a seller to include the prices
set by all sellers in the last period, i.e., st = pt−1. We initialize s0 to be a randomly selected price
proﬁle. The action of a seller is modeled as one of m equally-spaced points in the interval ranging
from just below the sellers’ cost c to just above the monopoly price pm. At each step t ≥0, each"
THE PLATFORM STACKELBERG PROBLEM,0.10047846889952153,"seller i selects a price pi,t and is rewarded by its per-period proﬁt ⇢i(pt; Nt), which depends on
pt = (p1,t, . . . , pn,t) and the choice of which sellers Nt are displayed by the platform."
THE PLATFORM STACKELBERG PROBLEM,0.10526315789473684,"The platform.
To formalize the platform’s problem, let σ⇤= (σ⇤"
THE PLATFORM STACKELBERG PROBLEM,0.11004784688995216,"1, .., σ⇤"
THE PLATFORM STACKELBERG PROBLEM,0.11483253588516747,"n) denote a strategy proﬁle
selected by Q-learning on the part of sellers, in response to the platform rule, and in the long run, after
a suitably large number of steps. We leave implicit here the dependence of seller strategy proﬁle on
the platform’s policy. The platform must decide in each period which sellers to display to consumers.
For this, we denote the platform rule as policy ⇡, and we adopt for the state of the platform policy the
prices quoted by sellers in step t, pt, so that the platform’s policy uses these prices to select a set of
agents to display, with Nt selected according to ⇡(pt). Let p⇤"
THE PLATFORM STACKELBERG PROBLEM,0.11961722488038277,"t = σ⇤(st) denote a price proﬁle chosen
under seller strategies σ⇤, i.e., in response to the platform rules, and at some large enough time step
t⇤, and let ⌧⇤= (p⇤"
THE PLATFORM STACKELBERG PROBLEM,0.12440191387559808,"t⇤, p⇤"
THE PLATFORM STACKELBERG PROBLEM,0.1291866028708134,"t⇤+1, ..) denote a trajectory of prices forward from t⇤(the dependence on the
platform’s policy is left implicit in this notation). We denote the distribution of these trajectories as
q⇡(⌧⇤). The Stackelberg problem facing the platform is to ﬁnd a platform policy ⇡that maximizes
consumer surplus given the effect of this policy on the induced strategy proﬁle of sellers."
THE PLATFORM STACKELBERG PROBLEM,0.1339712918660287,"Deﬁnition 1 (Behavioral Stackelberg Problem) The optimal platform policy solves ⇡⇤
2
argmax⇡CS(⇡), where CS(⇡) is the expected sum consumer surplus when sellers follow strategy σ⇤
forward from period t⇤, i.e.,"
THE PLATFORM STACKELBERG PROBLEM,0.13875598086124402,CS(⇡) = E⌧⇤⇠q⇡(⌧⇤)
THE PLATFORM STACKELBERG PROBLEM,0.14354066985645933,""" T ⇤
X t=t⇤ U(p⇤"
THE PLATFORM STACKELBERG PROBLEM,0.14832535885167464,"t ; ⇡(p⇤ t )) # ,
(1)"
THE PLATFORM STACKELBERG PROBLEM,0.15311004784688995,"where T ⇤is a suitably chosen horizon3 and q⇡(⌧⇤) denotes the distribution over Q-learning induced
seller pricing trajectories in response to platform policy ⇡."
LEARNING OPTIMAL PLATFORM RULES,0.15789473684210525,"4
Learning Optimal Platform Rules"
LEARNING OPTIMAL PLATFORM RULES,0.16267942583732056,"In this section, we solve the platform’s problem, in responding to Q-learning sellers, through the
Stackelberg POMDP framework (Brero et al., 2022). This creates a suitably deﬁned POMDP in
which the optimal policy solves the behavioral Stackelberg problem (Deﬁnition 1)."
LEARNING OPTIMAL PLATFORM RULES,0.1674641148325359,"Deﬁnition 2 (Stackelberg POMDP for platform rules) The Stackelberg POMDP for platform
rules is a ﬁnite-horizon POMDP, where each episode has the following two phases:"
LEARNING OPTIMAL PLATFORM RULES,0.1722488038277512,"• An equilibrium phase, consisting of ne ≥1 steps. In this phase, each state st includes the step
counter t, the sellers’ current Q-matrices, and the prices pt quoted by the agents. Observations
consists of the prices quoted by the sellers (ot = pt) and policy actions determine the set of agents
displayed (in their more general version, at = Nt). State transitions are determined by Q-learning,
where each seller i updates its Q-matrix after being rewarded by ⇢i(pt; Nt). The policy has zero
reward in every time step (r(st, at) = 0, for t ne)."
LEARNING OPTIMAL PLATFORM RULES,0.17703349282296652,"• A reward phase, consisting of nr ≥1 steps, each with the same actions, states, and observations as
the equilibrium steps. The reward phase differs in two ways. First, the Q-matrices of the sellers are
not updated, and second, the platform policy now receives a non-zero reward, and this is set in each
step to be equal to the consumer surplus in that step (r(st, at) = U(pt; Nt), for t > ne)."
LEARNING OPTIMAL PLATFORM RULES,0.18181818181818182,"This Stackelberg POMDP formulation is an adaptation of that provided by Brero et al. (2022), who
used it to learn leader strategies in matrix games and allocation mechanisms. Following Brero et al.
(2022), we show the Stackelberg POMDP formulation is well-founded by showing that an optimal
policy will also solve the Behavioral Stackelberg design problem of Deﬁnition 1. Speciﬁcally, when
the number of reward steps nr is large enough and when ne ≥t⇤, the optimal policy, denoted ⇡⇤"
LEARNING OPTIMAL PLATFORM RULES,0.18660287081339713,"ne,nr,
for the Stackelberg POMDP with ne equilibrium and nr reward steps maximizes the objective in
Equation (1)."
LEARNING OPTIMAL PLATFORM RULES,0.19138755980861244,Proposition 1 The optimal Stackelberg POMDP policy ⇡⇤
LEARNING OPTIMAL PLATFORM RULES,0.19617224880382775,"ne,nr, for an equilibrium phase with ne ≥1
steps and a reward phase with nr ≥1 steps, maximizes CS(⇡), for seller behavior induced after ne
steps when nr = T ⇤."
LEARNING OPTIMAL PLATFORM RULES,0.20095693779904306,"3Note that we can use a ﬁnite time horizon as trajectories ⌧⇤consists of price cycles due to our sellers’
behavior model."
LEARNING OPTIMAL PLATFORM RULES,0.20574162679425836,"The proposition follows from the construction of the Stackelberg POMDP, especially the fact the our
policy is only rewarded under the response behavior reached after ne steps, in line with the deﬁnition
of CS(⇡) (see Appendix B for the full proof argument)."
LEARNING OPTIMAL PLATFORM RULES,0.21052631578947367,"Brero et al. (2022) use the Stackelberg POMDP framework in an ""ofﬂine"" environment, i.e., in
a simulation that assumes access, at design time, to followers’ internal information. This allows
them to solve their POMDP using the paradigm of centralized training and decentralized execution
(Lowe et al., 2017). The leader policy is trained via an actor-critic deep RL algorithm, and the critic
network (which estimates the sum of rewards until the end of the episode) accesses the full state
during training. Only the actor network, which represents the policy, is restricted to the partial-state
information."
LEARNING OPTIMAL PLATFORM RULES,0.215311004784689,"Here, we also study the use of the Stackelberg POMDP framework to train useful leader policies
""in the wild,"" where the learning algorithm of the platform can only access the kind of information
that an economic platform would have based on observations of sellers. As we will empirically
demonstrate, we can successfully operate without access to sellers’ private information in regard to
Q-matrices and exploration rate without affecting learning performance.4"
LEARNING OPTIMAL PLATFORM RULES,0.22009569377990432,"Threshold platform rules.
In our experiments, we consider the class of threshold platform rules.
These rules use the current prices set by sellers to set a price threshold above which a seller will not
be displayed, with the same threshold set for all sellers."
LEARNING OPTIMAL PLATFORM RULES,0.22488038277511962,"Deﬁnition 3 (Threshold Platform Rule) A threshold platform rule sets a threshold ⌧(pt) ≥0, for
each price proﬁle pt, such that Nt = {i 2 {1, .., n} : pi,t ⌧(pt)}, i.e., any seller whose price is no
greater than the threshold is displayed to consumers."
LEARNING OPTIMAL PLATFORM RULES,0.22966507177033493,"This class of threshold rules has a corresponding optimality result: there is a threshold rule that
makes the market competitive, with all sellers displayed and consumer surplus maximized in a
subgame perfect Nash equilibrium (SPE) of the induced continuous pricing game. Even though the
pricing behaviors that arise from Q-learning need not converge to SPEs, we use these equilibria as a
theoretical support for our platform intervention choice, as previously done by Johnson et al. (2021).
We have the following result:"
LEARNING OPTIMAL PLATFORM RULES,0.23444976076555024,"Proposition 2 For any ✏> 0, there exists a threshold platform rule ⇡such that CS(⇡) ≥CS(⇡⇤) −
P"
LEARNING OPTIMAL PLATFORM RULES,0.23923444976076555,"t δt✏under a subgame perfect Nash equilibrium (SPE) of the inﬁnitely-repeated continuous pricing
game induced by platform rule ⇡."
LEARNING OPTIMAL PLATFORM RULES,0.24401913875598086,"This proposition follows from a platform rule with a limiting threshold that is arbitrarily close to the
sellers’ cost c (see Appendix C for the proof). Under this rule, sellers are displayed only if their price
is minimal. At the same time, this particular threshold rule is fragile, and would lead to market failure
if seller costs vary in unexpected ways. By letting the threshold ⌧also vary with the price proﬁle pt,
as is allowed by the family of threshold platform rules, we seek to learn milder interventions that still
mitigate collusion but remain robust to variations in the costs faced by sellers in the marketplace."
EXPERIMENTAL RESULTS,0.24880382775119617,"5
Experimental Results"
EXPERIMENTAL RESULTS,0.2535885167464115,"In this section, we evaluate our learning approach via three main experiments. We ﬁrst consider
performance in terms of consumer surplus, benchmarking our RL interventions against the ones
introduced by Johnson et al. (2021). We demonstrate the ability to learn optimal leader strategies in
the Stackelberg game with the followers across all the seeds we tested, signiﬁcantly outperforming
existing interventions. We then train platform rules without access to the sellers’ private information
(""in the wild""), and show that this is not crucial for our learning performances. We conclude by
testing the robustness of our interventions, adding price perturbations during training and evaluating
the effect on the robustness of our learned platform rules in environments where sellers have different
costs from those assumed during training."
EXPERIMENTAL RESULTS,0.2583732057416268,"4We notice this is also in line with the recent ﬁndings in Fujimoto et al. (2022) highlighting how the Bellman
error minimization (for which we require environments to be Markovian) may not be a good proxy of the
accuracy of the value function."
EXPERIMENTAL RESULTS,0.2631578947368421,"Experimental set-up.
As in Calvano et al. (2020a) and Johnson et al. (2021), we consider settings
with two pricing agents with cost c = 1, quality indexes ↵1 = ↵2 = 2, and ↵0 = 0, and we set
parameter µ = 0.25 to control horizontal differentiation. The seller Q-learning algorithms are also
trained using discount factor δ = 0.95, exploration rate ""t = e−βt with β = 1e −5, and learning rate
↵= 0.15. We also include results for variations of this default setting in Appendix D. A particular
concern is that if the exploration rate is still high during the reward phase of the Stackelberg POMDP
episode, our policy may be rewarded based on random prices. To address this problem, one could
extend the Stackelberg POMDP equilibrium phase to reach a minimal ""t or isolate stages when
price proﬁles are more stable to audit rewards. To satisfy our computational constraints, we pause
exploration during the reward phase of the Stackelberg POMDP. As in Johnson et al. (2021), these
prices range from 0.95 (just below the sellers’ cost) to 2.1 (which is above the monopoly price under
no intervention). Earlier work provided sellers with a choice of ﬁfteen different prices (over a similar
range). We need a smaller grid in order to satisfy our computational constraints; earlier work studied
the effect of different, hand-designed platform rules, and did not also use RL for the automated design
of suitable platform rules. We also follow the choices of earlier work, and study an economy with
two sellers (again, for reasons of computational resources). This coarsened price grid allows us to
train a platform policy through Stackelberg POMDP for 50 million steps in 18 hours using a single
core on a Intel(R) Xeon(R) Platinum 8268 CPU @ 2.90GHz machine."
EXPERIMENTAL RESULTS,0.2679425837320574,"Learning algorithm.
To train the platform policy, we start from the A2C algorithm provided by
Stable Baselines3 (Rafﬁn et al., 2021, MIT License). Given that our policy is only rewarded at
the end of a Stackelberg POMDP episode, we conﬁgure A2C so that neural network parameters
are only updated after this reward phase. In this way, we guarantee that policies inducing desired
followers’ equilibria are properly rewarded. Furthermore, to reduce variance in sellers’ responses
due to non-deterministic policy behavior, we maintain an observation-action map throughout each
episode. When a new observation is encountered during the episode, the policy chooses an action
following the default training behavior and stores this new observation-action pair in the map. We
will show the importance of this variation via an ablation study that is presented in Appendix E. In
this section, we assume that sellers restart the Q-learning process by re-initializing exploration rates
every time the platform rules change (i.e., at the beginning of every Stackelberg POMDP episode). In
Appendix F, we also show how the training approach is robust to different sellers’ behaviors, where
the sellers restart the learning rate asynchronously, and not necessarily at the beginning of episodes."
PLATFORM LEARNING PERFORMANCE,0.2727272727272727,"5.1
Platform Learning Performance"
PLATFORM LEARNING PERFORMANCE,0.27751196172248804,"In this section, we evaluate the performance of our learned platform policies. For this, we train our
policies for 50 million steps in total. We set up the Stackelberg POMDP environment using 50k
equilibrium steps and 30 reward steps.5 In these initial experiments, we train our policies using the
centralized training-decentralized execution paradigm as used for this Stackelberg learning problem
by Brero et al. (2022), giving the critic network access to the sellers’ learning information (i.e.,
Q-tables and exploration rates). We relax this below in studying the robustness of the computational
framework to online training (""in the wild""). We consider the following interventions on behalf of the
platform designer:"
PLATFORM LEARNING PERFORMANCE,0.2822966507177033,"• No intervention: Sellers are always displayed, no matter the price they quote. To derive this baseline,
we run our Q learning dynamics until convergence (as described in Johnson et al. (2021)) for each
seed and then average the surplus at ﬁnal strategies."
PLATFORM LEARNING PERFORMANCE,0.28708133971291866,"• PDP: We test price-directed prominence, a platform intervention introduced by Johnson et al.
(2021). Here, the platform only displays the seller who quotes the lower price (breaking ties at
random), thus enhancing competition. As for no intervention, we compute the performance of PDP
by averaging consumer surplus after Q-learning dynamics converge."
PLATFORM LEARNING PERFORMANCE,0.291866028708134,"• DPDP: Dynamic price-directed prominence is another intervention introduced by Johnson et al.
(2021), which also conditions the choice of the (unique) displayed seller on past prices. Under this
intervention, quoting prices equal to cost is a subgame perfect equilibrium of the induced game (under
suitable discount factors). As for the previous baselines, we compute the performance of DPDP by
averaging consumer surplus after Q-learning dynamics converge."
PLATFORM LEARNING PERFORMANCE,0.2966507177033493,5See Appendix H for a discussion around parameter selection.
PLATFORM LEARNING PERFORMANCE,0.3014354066985646,"Figure 1: Learning performance of No State RL and State-based RL compared with different
baselines. The results are averaged over 50 runs and shaded regions show 95% conf. intervals. The
No-Stackelberg interventions are displayed on the left, the Stackelberg ones are on the right."
PLATFORM LEARNING PERFORMANCE,0.3062200956937799,"• No State RL: Here we use the Stackelberg POMDP methodology to train a platform policy that
does not use prices pt to determine the threshold at which to admit each seller to the buy box (thus,
""no state"").6 Here, Q-learning is restarted whenever a Stackelberg POMDP episode begins."
PLATFORM LEARNING PERFORMANCE,0.31100478468899523,"•No Stackelberg No State RL: A variation on ""No State RL"" that does not use the Stackelberg POMDP
methodology. Rather, the platform and sellers each follow decentralized learning, and the platform
receives a consumer surplus reward at every step. Q-learning is restarted after the same number of
steps that are used in a Stackelberg POMDP episode."
PLATFORM LEARNING PERFORMANCE,0.3157894736842105,"• State-based RL: Here we use the Stackelberg POMDP methodology to train a platform policy that
sets a threshold at which to admit each seller as a function of the price proﬁle quoted by the sellers
(thus, ""state-based""). This is the full class of threshold platform rules. Here, Q-learning is restarted
whenever a Stackelberg POMDP episodes begins."
PLATFORM LEARNING PERFORMANCE,0.32057416267942584,"• No Stackelberg State-based RL: A variation on ""State-based RL"" that does not use the Stackelberg
POMDP methodology. Rather, the platform and sellers each follow decentralized learning, and the
platform receives a consumer surplus reward at every step. Q-learning is restarted after the same
number of steps that are used in a Stackelberg POMDP episode."
PLATFORM LEARNING PERFORMANCE,0.3253588516746411,"Figure 1 shows the consumer surplus that is realized under these different interventions. First, we
conﬁrm the results of Johnson et al. (2021), and see consumer surplus improvements from both
PDP and DPDP compared to No intervention, with DPDP outperforming PDP. At the same time,
the no Stackelberg baselines are not able to outperform DPDP, conﬁrming the beneﬁts of using
learning methodologies that exploit the leader-follower structure of our game. Indeed, our RL
interventions based on the Stackelberg framework dramatically improve consumer surplus, driving it
to (approximately, in the state-based scenario) its maximal level. In our setting, this optimal level for
surplus is approximately 0.94. This is conﬁrmed by the fact that, for both No State and State-based
RL, all sellers are displayed and they invariably quote minimum prices at the end of training. This is
the optimal (i.e., surplus maximizing) seller behavior, conﬁrming the effectiveness of the Stackelberg-
based learning methodology in ﬁnding an optimal leader strategy given the Q-learning behavior of
sellers. It is easier for No State RL to reach the optimal performance since its class of policies is
much smaller than the class considered by State-based RL. However, as we will see in Section 5.3,
the state-based policy is more ﬂexible and is robust to the case that the cost basis changes for sellers
while No State RL is not."
PLATFORM LEARNING PERFORMANCE,0.33014354066985646,6This class of policies already includes the optimal policy described in the proof of Proposition 2.
PLATFORM LEARNING PERFORMANCE,0.3349282296650718,"Figure 2: Ofﬂine learning (left) vs. online, ""learning in the wild"" (right) performance. The results are
averaged over 50 runs, and the shaded regions show 95% conf. intervals."
LEARNING IN THE WILD,0.3397129186602871,"5.2
Learning in the Wild"
LEARNING IN THE WILD,0.3444976076555024,"We now test the performance of the Stackelberg POMDP learning methodology when it has no access
to sellers’ private information during training. This can potentially create learning instabilities given
that actor-critic training such as A2C generally require that the environment accessed by their critic
networks is Markovian (Grondman et al., 2012). Despite this, we ﬁnd success in this test of ""in
the wild"" learning. The results are displayed in Figure 2 and show, despite relaxing this Markov
assumption, that the A2C algorithm is able to learn optimal policies for both policy classes (No State
and State-based). We conjecture that the reason behind this good performance is related to the class
of threshold platform policies. Given a threshold policy, it is possible to predict the overall episode
reward based only on the action taken by the policy (the threshold) and ignoring the part of the state
that is internal to the sellers (i.e., the Q-matrix and exploration rate)."
LEARNING IN THE WILD,0.3492822966507177,"In Appendix G, we also demonstrate successful experimental results when we replace the use of
consumer surplus (1) for reward with a reward that corresponds to the number of agents displayed
and the sum of the negated prices offered by sellers. This shows robustness to a possible knowledge
gap in knowing the speciﬁc functional form of consumer surplus."
ROBUSTNESS OF LEARNED PLATFORM RULES,0.35406698564593303,"5.3
Robustness of Learned Platform Rules"
ROBUSTNESS OF LEARNED PLATFORM RULES,0.3588516746411483,"As observed in our previous experiments, the Stackelberg-based RL algorithm is effective in learning
interventions that maximize consumer surplus for a given economic setting. However, as they are
tailored to the economic setting at hand, these interventions can perform poorly when facing settings
that are different from those during training. To learn more robust platform rules, we also train
with a modiﬁed version of the Stackelberg POMDP: at each reward step, with some random-price
probability, sellers quote prices sampled uniformly at random from the price grid. In this way,
the platform is rewarded during training for performance that remains robust to prices that are not
produced by the Q-learning equilibrium dynamics (given seller costs at training)."
ROBUSTNESS OF LEARNED PLATFORM RULES,0.36363636363636365,"We evaluate the effect of adding this perturbation-based robustness to the training procedure in
settings with different seller costs: in addition to the default c = 1.0, we also test with cost c = 1.38
(between the second and the third price in the grid of prices between 0.95 and 2.1) and cost c = 1.67
(between the third and the fourth price in the price grid)."
ROBUSTNESS OF LEARNED PLATFORM RULES,0.3684210526315789,"As we see in Figure 3 (right), this training approach (and in particular with probability 0.4 of random-
price perturbation) succeeds in making the state-based policy much more robust in the face of sellers
who experience a different cost environment at test time. The robust, state-based policy displays
sellers with higher prices (due to their higher costs), while continuing to signiﬁcantly mitigate
collusion when seller costs are as they were during training. This is also conﬁrmed by the policy
visualizations in Figure 3 (left), which show how the buy box learned for State-based RL tends to be"
ROBUSTNESS OF LEARNED PLATFORM RULES,0.37320574162679426,"Figure 3: Left: Policy visualization, with number of displayed agents given price selection, averaged
over 50 seeds (white–avg. num. sellers displayed 2, black–avg. num. sellers displayed 0). A: No State
RL with no price perturbation during training. B: No State RL with 40% random price perturbation
(rpp) during training. C: State-based RL with no price perturbation during training. D: State-based
RL with 40% random price perturbation during training. Right: Robustness test, with buy box policy
trained without price perturbation and with price perturbation with prob. 0.4, averaged over 50 runs."
ROBUSTNESS OF LEARNED PLATFORM RULES,0.37799043062200954,"much more open under this modiﬁed training regime. In contrast, the policy learned by No State RL
performs very poorly when tested at costs that differ from those assumed during training, and even
under this modiﬁed training regime. There is no single threshold that provides a good compromise
between performance at cost 1 and handling price perturbations."
CONCLUSION,0.3827751196172249,"6
Conclusion"
CONCLUSION,0.3875598086124402,"This work has demonstrated that rules that are effective in preventing collusion by sellers can be
learned through a framework that correctly solves the two-level, Stackelberg problem (making use of
the platform’s commitment power). Speciﬁcally, we have introduced the class of threshold policies
that contain policies that optimize consumer surplus and a learning methodology that is effective in
learning optimal leader policies in this class. The interventions we learned are shown to substantially
outperform the hand-designed interventions introduced in prior work when the cost environment at
test time is as anticipated during training. We also showed how our learned platform interventions
can be made more robust when settings are dynamic, with varying seller cost structures, by adopting
a suitably-modiﬁed training methodology. This also highlights the importance of the state-based
platform rule relative to a no-state rule."
CONCLUSION,0.3923444976076555,"Interesting future directions include testing our approach in more complex settings, e.g., when sellers’
costs vary between training episodes. In this case, optimal policy actions are based on the prices
quoted during the sellers’ equilibration phase, as these prices may provide useful information about
the current underlying costs (intuitively, the quoted prices will be higher under higher costs). In
this scenario, it may be necessary to represent our platform policies via recurrent neural networks,
keeping a memory of past prices. Finally, we believe that this approach can also be effective in other
applications, e.g., to design and understand effective interventions for the electricity markets studied
by Abada and Lambin (2022), a setting where the successful use of RL as a defensive response by a
platform is not yet established."
CONCLUSION,0.39712918660287083,Acknowledgements
CONCLUSION,0.4019138755980861,"This research is funded in part by Defense Advanced Research Projects Agency under Cooperative
Agreement HR00111920029. The content of the information does not necessarily reﬂect the position
or the policy of the Government, and no ofﬁcial endorsement should be inferred. This is approved
for public release; distribution is unlimited. The work of G. Brero was also supported by the SNSF"
CONCLUSION,0.40669856459330145,"(Swiss National Science Foundation) under Fellowship P2ZHP1_191253. We thank Emilio Calvano
and Justin Johnson for their availability to answer questions about their work and for guidance in
replicating some of their results. We also thank Alon Eden, Matthias Gerstgrasser, and Alexander
MacKay for for helpful discussions and feedback. Many computations in this paper were run on the
FASRC Cannon cluster supported by the FAS Division of Science Research Computing Group at
Harvard University."
REFERENCES,0.41148325358851673,References
REFERENCES,0.41626794258373206,Ibrahim Abada and Xavier Lambin. 2022. Artiﬁcial Intelligence: Can Seemingly Collusive Outcomes
REFERENCES,0.42105263157894735,Be Avoided? Available at SSRN 3559308 (2022).
REFERENCES,0.4258373205741627,Simon P Anderson and Andre De Palma. 1992. The logit as a model of product differentiation.
REFERENCES,0.430622009569378,"Oxford Economic Papers 44, 1 (1992), 51–67."
REFERENCES,0.4354066985645933,"Enrique Areyan Viqueira, Cyrus Cousins, Yasser Mohammad, and Amy Greenwald. 2019. Empir-"
REFERENCES,0.44019138755980863,"ical Mechanism Design: Designing Mechanisms from Data. In Proceedings of the Thirty-Fifth
Conference on Uncertainty in Artiﬁcial Intelligence (Proceedings of Machine Learning Research,
Vol. 115). 1094–1104."
REFERENCES,0.4449760765550239,"Stephanie Assad, Robert Clark, Daniel Ershov, and Lei Xu. 2020. Algorithmic Pricing and Competi-"
REFERENCES,0.44976076555023925,"tion: Empirical Evidence from the German Retail Gasoline Market. CESifo Working Paper Series
8521 (2020)."
REFERENCES,0.45454545454545453,"Craig Boutilier. 1996. Planning, Learning and Coordination in Multiagent Decision Processes. In"
REFERENCES,0.45933014354066987,Proceedings of the 6th Conference on Theoretical Aspects of Rationality and Knowledge. 195–210.
REFERENCES,0.46411483253588515,"Gianluca Brero, Alon Eden, Darshan Chakrabarti, Matthias Gerstgrasser, Vincent Li, and David C."
REFERENCES,0.4688995215311005,"Parkes. 2022. Learning Stackelberg Equilibria and Applications to Economic Design Games. arXiv
preprint arXiv:2210.03852 (2022)."
REFERENCES,0.47368421052631576,"Gianluca Brero, Alon Eden, Matthias Gerstgrasser, David C. Parkes, and Duncan Rheingans-Yoo."
REFERENCES,0.4784688995215311,"2021. Reinforcement Learning of Sequential Price Mechanisms. In Thirty-Fifth AAAI Conference
on Artiﬁcial Intelligence, AAAI. 5219–5227."
REFERENCES,0.48325358851674644,Erik Brinkman and Michael P. Wellman. 2017. Empirical Mechanism Design for Optimizing Clearing
REFERENCES,0.4880382775119617,"Interval in Frequent Call Markets. In Proceedings of the 2017 ACM Conference on Economics and
Computation. 205–221."
REFERENCES,0.49282296650717705,"Benedikt Bünz, Benjamin Lubin, and Sven Seuken. 2018. Designing Core-selecting Payment Rules:"
REFERENCES,0.49760765550239233,"A Computational Search Approach. In Proceedings of the 2018 ACM Conference on Economics
and Computation. 109."
REFERENCES,0.5023923444976076,"Emilio Calvano, Giacomo Calzolari, Vincenzo Denicolò, Joseph E Harrington, and Sergio Pastorello."
REFERENCES,0.507177033492823,"2020b. Protecting consumers from collusive prices due to AI. Science 370, 6520 (2020), 1040–
1042."
REFERENCES,0.5119617224880383,"Emilio Calvano, Giacomo Calzolari, Vincenzo Denicolo, and Sergio Pastorello. 2020a. Artiﬁcial"
REFERENCES,0.5167464114832536,"intelligence, algorithmic pricing, and collusion. American Economic Review 110, 10 (2020),
3267–97."
REFERENCES,0.5215311004784688,"C. Cheng, Z. Zhu, B. Xin, and C Chen. 2017. A multi-agent reinforcement learning algorithm based"
REFERENCES,0.5263157894736842,on Stackelberg game. In 6th Data Driven Control and Learning Systems (DDCLS). 727–732.
REFERENCES,0.5311004784688995,"Michael J. Curry, Ping-Yeh Chiang, Tom Goldstein, and John Dickerson. 2020. Certifying Strate-"
REFERENCES,0.5358851674641149,"gyproof Auction Networks. In Proc. 33rd Annual Conference on Neural Information Processing
Systems."
REFERENCES,0.5406698564593302,"Michael J. Curry, Uro Lyi, Tom Goldstein, and John Dickerson. 2022. Learning Revenue-Maximizing"
REFERENCES,0.5454545454545454,"Auctions With Differentiable Matching. In Proc. 25th International Conference on Artiﬁcial
Intelligence and Statistics. Forthcoming."
REFERENCES,0.5502392344497608,"Paul Duetting, Zhe Feng, Harikrishna Narasimhan, David C. Parkes, and Sai Srivatsa Ravindranath."
REFERENCES,0.5550239234449761,"2019. Optimal Auctions through Deep Learning. In Proceedings of the 36th International Confer-
ence on Machine Learning. 1706–1715."
REFERENCES,0.5598086124401914,"Tanner Fiez, Benjamin Chasnov, and Lillian J. Ratliff. 2020. Implicit Learning Dynamics in Stackel-"
REFERENCES,0.5645933014354066,"berg Games: Equilibria Characterization, Convergence Analysis, and Empirical Study. In Proceed-
ings of the 37th International Conference on Machine Learning. 3133–3144."
REFERENCES,0.569377990430622,Drew Fudenberg and Jean Tirole. 1991. Game theory. MIT press.
REFERENCES,0.5741626794258373,"Scott Fujimoto, David Meger, Doina Precup, Oﬁr Nachum, and Shixiang Shane Gu. 2022. Why"
REFERENCES,0.5789473684210527,"Should I Trust You, Bellman? The Bellman Error is a Poor Replacement for Value Error. arXiv
preprint arXiv:2201.12417 (2022)."
REFERENCES,0.583732057416268,"Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert Babuska. 2012. A survey of actor-"
REFERENCES,0.5885167464114832,"critic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems,
Man, and Cybernetics, Part C (Applications and Reviews) 42, 6 (2012), 1291–1307."
REFERENCES,0.5933014354066986,"Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. 2020. What is Local Optimality in Nonconvex-"
REFERENCES,0.5980861244019139,"Nonconcave Minimax Optimization?. In Proceedings of the 37th International Conference on
Machine Learning, Vol. 119. 4880–4889."
REFERENCES,0.6028708133971292,"Justin Johnson, Andrew Rhodes, and Matthijs R Wildenbeest. 2021. Platform design when sellers"
REFERENCES,0.6076555023923444,use pricing algorithms. Available at SSRN 3753903 (2021).
REFERENCES,0.6124401913875598,"Patrick R. Jordan, L. Julian Schvartzman, and Michael P. Wellman. 2010. Strategy exploration in"
REFERENCES,0.6172248803827751,"empirical games. In 9th International Conference on Autonomous Agents and Multiagent Systems.
1131–1138."
REFERENCES,0.6220095693779905,Christopher Kiekintveld and Michael P. Wellman. 2008. Selecting strategies using empirical game
REFERENCES,0.6267942583732058,"models: an experimental analysis of meta-strategies. In 7th International Joint Conference on
Autonomous Agents and Multiagent Systems. 1095–1101."
REFERENCES,0.631578947368421,"Kevin Kuo, Anthony Ostuni, Elizabeth Horishny, Michael J. Curry, Samuel Dooley, Ping-Yeh Chiang,"
REFERENCES,0.6363636363636364,"Tom Goldstein, and John P. Dickerson. 2020. ProportionNet: Balancing Fairness and Revenue for
Auction Design with Deep Learning. CoRR abs/2010.06398 (2020)."
REFERENCES,0.6411483253588517,"Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. 2019. Robust Multi-"
REFERENCES,0.645933014354067,"Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient. Proceedings of
the AAAI Conference on Artiﬁcial Intelligence 33 (July 2019), 4213–4220."
REFERENCES,0.6507177033492823,"Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. 2017."
REFERENCES,0.6555023923444976,"Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in Neural
Information Processing Systems (NeurIPS) 30 (2017)."
REFERENCES,0.6602870813397129,"Andreu Mas-Colell, Michael Dennis Whinston, Jerry R Green, et al. 1995. Microeconomic theory."
REFERENCES,0.6650717703349283,Vol. 1. Oxford university press New York.
REFERENCES,0.6698564593301436,"David Mguni, Joel Jennings, Emilio Sison, Sergio Valcarcel Macua, Soﬁa Ceppi, and Enrique Munoz"
REFERENCES,0.6746411483253588,"de Cote. 2019. Coordinating the Crowd: Inducing Desirable Equilibria in Non-Cooperative Sys-
tems. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent
Systems. 386–394."
REFERENCES,0.6794258373205742,"Rajesh K. Mishra, Deepanshu Vasal, and Sriram Vishwanath. 2020. Model-free Reinforcement"
REFERENCES,0.6842105263157895,"Learning for Stochastic Stackelberg Security Games. In 59th IEEE Conference on Decision and
Control. 348–353."
REFERENCES,0.6889952153110048,"Neehar Peri, Michael J. Curry, Samuel Dooley, and John P. Dickerson. 2021. PreferenceNet: Encoding"
REFERENCES,0.69377990430622,"Human Preferences in Auction Design with Deep Learning. In Proceedings of the 35th Conference
on Neural Information Processing Systems."
REFERENCES,0.6985645933014354,"Antonin Rafﬁn, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah"
REFERENCES,0.7033492822966507,"Dormann. 2021. Stable-Baselines3: Reliable Reinforcement Learning Implementations. Journal
of Machine Learning Research 22, 268 (2021), 1–8."
REFERENCES,0.7081339712918661,"Jad Rahme, Samy Jelassi, Joan Bruna, and S. Matthew Weinberg. 2021b. A Permutation-Equivariant"
REFERENCES,0.7129186602870813,"Neural Network Architecture For Auction Design. In Proc. Thirty-Fifth AAAI Conference on
Artiﬁcial Intelligence. 5664–5672."
REFERENCES,0.7177033492822966,"Jad Rahme, Samy Jelassi, and S. Matthew Weinberg. 2021a. Auction Learning as a Two-Player"
REFERENCES,0.722488038277512,"Game. In Proc. 9th International Conference on Learning Representations, ICLR."
REFERENCES,0.7272727272727273,Sailik Sengupta and Subbarao Kambhampati. 2020. Multi-agent Reinforcement Learning in Bayesian
REFERENCES,0.7320574162679426,"Stackelberg Markov Games for Adaptive Moving Target Defense. arXiv:2007.10457 [cs] (July
2020). arXiv: 2007.10457."
REFERENCES,0.7368421052631579,"Weiran Shen, Binghui Peng, Hanpeng Liu, Michael Zhang, Ruohan Qian, Yan Hong, Zhi Guo,"
REFERENCES,0.7416267942583732,"Zongyao Ding, Pengjun Lu, and Pingzhong Tang. 2020. Reinforcement mechanism design,
with applications to dynamic pricing in sponsored search auctions. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, Vol. 34. 2236–2243."
REFERENCES,0.7464114832535885,"W. Shen, P. Tang, and S. Zuo. 2019. Automated Mechanism Design via Neural Networks. In"
REFERENCES,0.7511961722488039,Proceedings of the 18th International Conference on Autonomous Agents and Multiagent Systems.
REFERENCES,0.7559808612440191,"Zhenyu Shi, Runsheng Yu, Xinrun Wang, Rundong Wang, Youzhi Zhang, Hanjiang Lai, and Bo An."
REFERENCES,0.7607655502392344,"2020. Learning Expensive Coordination: An Event-Based Deep RL Approach. In 8th International
Conference on Learning Representations."
REFERENCES,0.7655502392344498,Tianmin Shu and Yuandong Tian. 2019. Mˆ3RL: Mind-aware Multi-agent Management Reinforce-
REFERENCES,0.7703349282296651,ment Learning. In 7th International Conference on Learning Representations.
REFERENCES,0.7751196172248804,"Andrea Tacchetti, DJ Strouse, Marta Garnelo, Thore Graepel, and Yoram Bachrach. 2019. A Neural"
REFERENCES,0.7799043062200957,Architecture for Designing Truthful and Efﬁcient Auctions. CoRR 1907.05181 (2019).
REFERENCES,0.784688995215311,Pingzhong Tang. 2017. Reinforcement mechanism design.. In Proc. 26th Int Joint Conf. on Art. Intell.
REFERENCES,0.7894736842105263,(IJCAI). 5146–5150.
REFERENCES,0.7942583732057417,K. Tharakunnel and S. Bhattacharyya. 2007. Leader-follower semi-markov decision problems:
REFERENCES,0.7990430622009569,"theoretical framework and approximate solution. In IEEE International Symposium on Approximate
Dynamic Programming and Reinforcement Learning. 111–118."
REFERENCES,0.8038277511961722,The Organisation for Economic Co-operation and Development. 2017. Algorithms and Collusion–
REFERENCES,0.8086124401913876,"Note from the European Union. www.oecd.org/competition/algorithms-and-collusion.
htm"
REFERENCES,0.8133971291866029,U.S. Federal Trade Commission. 2018. The Competition and Consumer Protection Issues of Algo-
REFERENCES,0.8181818181818182,"rithms, Artiﬁcial Intelligence, and Predictive Analytics, Hearing on Competition and Consumer
Protection in the 21st Century, U.S. Federal Trade Commission.
https://www.ftc.gov/
policy/hearings-competition-consumer-protection"
REFERENCES,0.8229665071770335,"Yevgeniy Vorobeychik, Christopher Kiekintveld, and Michael P. Wellman. 2006. Empirical mech-"
REFERENCES,0.8277511961722488,"anism design: methods, with application to a supply-chain scenario. In Proceedings 7th ACM
Conference on Electronic Commerce (EC-2006). 306–315."
REFERENCES,0.8325358851674641,"Kai Wang, Lily Xu, Andrew Perrault, Michael K. Reiter, and Milind Tambe. 2022. Coordinating"
REFERENCES,0.8373205741626795,"Followers to Reach Better Equilibria: End-to-End Gradient Descent for Stackelberg Games. In
AAAI Conference on Artiﬁcial Intelligence."
REFERENCES,0.8421052631578947,"Michael P. Wellman. 2006. Methods for Empirical Game-Theoretic Analysis. In Proceedings, The"
REFERENCES,0.84688995215311,Twenty-First National Conference on Artiﬁcial Intelligence. 1552–1556.
REFERENCES,0.8516746411483254,"Lily Xu, Andrew Perrault, Fei Fang, Haipeng Chen, and Milind Tambe. 2021. Robust Reinforcement"
REFERENCES,0.8564593301435407,"Learning Under Minimax Regret for Green Security. In Proc. 37th Conference on Uncertainty in
Artiﬁcal Intelligence (UAI-21)."
REFERENCES,0.861244019138756,"Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C Parkes, and Richard Socher. 2022. The"
REFERENCES,0.8660287081339713,"AI Economist: Optimal Economic Policy Design via Two-level Deep Reinforcement Learning.
Science Advances 8 (2022)."
REFERENCES,0.8708133971291866,Checklist
REFERENCES,0.8755980861244019,"The checklist follows the references. Please read the checklist guidelines carefully for information on
how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or
[N/A] . You are strongly encouraged to include a justiﬁcation to your answer, either by referencing
the appropriate section of your paper or providing a brief inline description. For example:"
REFERENCES,0.8803827751196173,"• Did you include the license to the code and datasets? [Yes] See Section 5.
• Did you include the license to the code and datasets? [No] The code and the data are"
REFERENCES,0.8851674641148325,"proprietary.
• Did you include the license to the code and datasets? [N/A]"
REFERENCES,0.8899521531100478,"Please do not modify the questions and only use the provided macros for your answers. Note that the
Checklist section does not count towards the page limit. In your paper, please delete this instructions
block and only keep the Checklist section heading above along with the questions/answers below."
REFERENCES,0.8947368421052632,1. For all authors...
REFERENCES,0.8995215311004785,(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
REFERENCES,0.9043062200956937,"contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See conclusion."
REFERENCES,0.9090909090909091,(c) Did you discuss any potential negative societal impacts of your work? [N/A] The
REFERENCES,0.9138755980861244,"paper itself is focused on tackling a potential negative societal impact.
(d) Have you read the ethics review guidelines and ensured that your paper conforms to"
REFERENCES,0.9186602870813397,"them? [Yes]
2. If you are including theoretical results..."
REFERENCES,0.9234449760765551,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
(b) Did you include complete proofs of all theoretical results? [Yes] Proofs are provided"
REFERENCES,0.9282296650717703,"in the appendix.
3. If you ran experiments..."
REFERENCES,0.9330143540669856,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-"
REFERENCES,0.937799043062201,"mental results (either in the supplemental material or as a URL)? [Yes] We will include
it in the supplemental material.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they"
REFERENCES,0.9425837320574163,"were chosen)? [Yes]
(c) Did you report error bars (e.g., with respect to the random seed after running experi-"
REFERENCES,0.9473684210526315,"ments multiple times)? [Yes] But not in Figure 3 to reduce clutter.
(d) Did you include the total amount of compute and the type of resources used (e.g., type"
REFERENCES,0.9521531100478469,"of GPUs, internal cluster, or cloud provider)? [Yes] See the beginning of Section 5.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9569377990430622,"(a) If your work uses existing assets, did you cite the creators? [Yes] We used Stable"
REFERENCES,0.9617224880382775,"Baselines3, which we cite at the beginning of Section 5.
(b) Did you mention the license of the assets? [Yes] We mentioned the Stable Baselines3"
REFERENCES,0.9665071770334929,"at the beginning of Section 5.
(c) Did you include any new assets either in the supplemental material or as a URL? [No]
(d) Did you discuss whether and how consent was obtained from people whose data you’re"
REFERENCES,0.9712918660287081,"using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identiﬁable"
REFERENCES,0.9760765550239234,"information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9808612440191388,"(a) Did you include the full text of instructions given to participants and screenshots, if"
REFERENCES,0.9856459330143541,"applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review"
REFERENCES,0.9904306220095693,"Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount"
REFERENCES,0.9952153110047847,spent on participant compensation? [N/A]
