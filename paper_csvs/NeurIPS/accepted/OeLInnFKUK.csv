Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0024271844660194173,"Tuning the hyperparameters of differentially private (DP) machine learning (ML)
algorithms often requires use of sensitive data and this may leak private information
via hyperparameter values. Recently, Papernot and Steinke (2022) proposed a
certain class of DP hyperparameter tuning algorithms, where the number of random
search samples is randomized. Commonly, these algorithms still considerably
increase the DP privacy parameter ε over non-tuned DP ML model training and can
be computationally heavy as evaluating each hyperparameter candidate requires
a new training run. We focus on lowering both the DP bounds and the compute
cost of these methods by using only a random subset of the sensitive data for the
hyperparameter tuning and by extrapolating the optimal values to a larger dataset.
We provide a Rényi differential privacy analysis for the proposed method and
experimentally show that it consistently leads to better privacy-utility trade-off than
the baseline method by Papernot and Steinke."
INTRODUCTION,0.0048543689320388345,"1
Introduction"
INTRODUCTION,0.007281553398058253,"Our aim is two-fold: to decrease the computational cost as well as the privacy cost of hyperparameter
tuning of DP ML models. The reasons for this are clear. As the dataset sizes grow and models get
more complex, blackbox optimization of hyperparameters becomes more expensive since evaluation
of a single set of hyperparameters often requires retraining a new model. On the other hand, tuning
the hyperparameters often depends on the use of sensitive data, so it requires privacy protection
as well, as illustrated by the example by Papernot and Steinke (2022). Intuitively, the leakage
from hyperparameters is much smaller than from the model parameters, however, providing tuning
algorithms with low additional DP cost has turned out challenging. Current best algorithms (Papernot
and Steinke, 2022) still come with a considerable DP cost overhead."
INTRODUCTION,0.009708737864077669,"Although our methods and results are applicable to general DP mechanisms, we focus in particular
on tuning of the DP stochastic gradient descent (DP-SGD) (Song et al., 2013; Bassily et al., 2014;
Abadi et al., 2016) which has become the most widely used method to train ML models with DP
guarantees. Compared to plain SGD, DP brings additional hyperparameters to tune: the noise level σ
and the clipping constant C. Additionally, also the subsampling ratio γ affects the DP guarantees, as
well as length of the training. Tuning all the hyperparameters of DP-SGD commonly requires use of
sensitive data."
INTRODUCTION,0.012135922330097087,"We use the results by Papernot and Steinke (2022) as building blocks of our methods. Their work
was based on the analysis of Liu and Talwar (2019) who provided the first results for DP black-
box optimization of hyperparameters, where, if the base training algorithm is (ε, 0)-DP, then the
tuned model is approximately (3ε, 0)-DP. Papernot and Steinke (2022) provided a Rényi differential
privacy (RDP) analysis for a class of black-box tuning algorithms, where the number of runs in the
hyperparameter tuning is randomized. As the privacy bounds are in terms of RDP and assume only
RDP bounds about the candidate model training algorithms, they are particularly suitable to tuning"
INTRODUCTION,0.014563106796116505,"DP-SGD. However, still, running these algorithms increase the ε-values two or three-fold or more,
and they can be computationally heavy as evaluating each candidate model requires training a new
model. Our novelty is to consider using only a random subset of the sensitive data for the tuning part
and use the output hyperparameter values (and potentially the model) for training subsequent models.
Using a random subset for the privacy and computation costly part automatically leads to both lower
DP privacy leakage as well as computational cost. We also consider ways to appropriately extrapolate
the optimal value from the small subset of data to a larger dataset."
INTRODUCTION,0.01699029126213592,"The RDP bounds for the DP tuning methods by Papernot and Steinke (2022) assume that the RDP-
values of the candidate model training algorithms are fixed. We also consider ways to use these
bounds for tuning hyperparameters that affect the RDP-values of the base algorithm, being the noise
level σ, the subsampling ratio γ and the length of training in case of DP-SGD."
RELATED WORK ON HYPERPARAMETER TUNING,0.019417475728155338,"1.1
Related Work on Hyperparameter Tuning"
RELATED WORK ON HYPERPARAMETER TUNING,0.021844660194174758,"Chaudhuri and Vinterbo (2013) were the first ones to focus on DP bounds for hyperparameter
tuning. An improvement was made by Liu and Talwar (2019) who considered black-box tuning of
(ε, δ)-DP mechanisms. Mohapatra et al. (2022) showed that for reasonable numbers of adaptively
chosen private candidates a naive RDP accounting (i.e., RDP parameters grow linearly w.r.t. the
number of model evaluations) often leads to lower DP bounds than the methods by Liu and Talwar
(2019). Papernot and Steinke (2022) gave RDP bounds for black-box tuning algorithms that grow
only logarithmically w.r.t. the number of model evaluations. In a non-DP setting, hyperparameter
tuning with random subsamples has been considered for SVMs (Horváth et al., 2017) and for large
datasets in healthcare (Waring et al., 2020). Small random subsets of data have been used in Bayesian
optimization of hyperparameters (Swersky et al., 2013; Klein et al., 2017). Recent works (Killamsetty
et al., 2021, 2022) consider using subsets of data for hyperparameter tuning of deep learning models."
OUR CONTRIBUTIONS,0.024271844660194174,"1.2
Our Contributions"
OUR CONTRIBUTIONS,0.02669902912621359,"• We propose a subsampling strategy to lower the privacy cost and computational cost of DP
hyperparameter tuning. We provide a tailored RDP analysis for the proposed strategy. Our
analysis is in terms of RDP and we use existing results for tuning Papernot and Steinke
(2022) and DP-SGD (Zhu and Wang, 2019) as building blocks."
OUR CONTRIBUTIONS,0.02912621359223301,"• We propose algorithms to tune hyperparameters that affect the RDP guarantees of the base
model training algorithms. We provide a rigorous RDP analysis for these algorithms."
OUR CONTRIBUTIONS,0.03155339805825243,"• We carry out experiments on several standard datasets, where we are able to improve upon the
baseline tuning method by a clear margin. While our experiments focus mainly on training
of deep learning models with DP-SGD and DP-Adam, our framework is currently applicable
to any computation that involves selecting the best among several alternatives (consider e.g.,
DP model selection, Thakurta and Smith, 2013)."
OUR CONTRIBUTIONS,0.03398058252427184,"2
Background: DP, DP-SGD and DP Hyperparameter Tuning"
OUR CONTRIBUTIONS,0.03640776699029126,"We first give the basic definitions. An input dataset containing n data points is denoted as X =
{x1, . . . , xn}. Denote the set of all possible datasets by X. We say X and Y are neighbors if we
get one by adding or removing one data element to or from the other (denoted X ∼Y ). Consider a
randomized mechanism M : X →O, where O denotes the output space. The (ε, δ)-definition of
DP can be given as follows (Dwork, 2006)."
OUR CONTRIBUTIONS,0.038834951456310676,"Definition 1. Let ε > 0 and δ ∈[0, 1]. We say that a mechanism M is (ε, δ)-DP, if for all neighboring
datasets X and Y and for every measurable set E ⊂O we have:"
OUR CONTRIBUTIONS,0.0412621359223301,Pr(M(X) ∈E) ≤eεPr(M(Y ) ∈E) + δ.
OUR CONTRIBUTIONS,0.043689320388349516,"We will also use the Rényi differential privacy (RDP) (Mironov, 2017) which is defined as follows.
Rényi divergence of order α > 1 between two distributions P and Q is defined as"
OUR CONTRIBUTIONS,0.04611650485436893,"Dα(P||Q) =
1
α −1 log
Z P(t) Q(t)"
OUR CONTRIBUTIONS,0.04854368932038835,"α
Q(t) dt.
(2.1)"
OUR CONTRIBUTIONS,0.050970873786407765,"Definition 2. We say that a mechanism M is (α, ε)-RDP, if for all neighboring datasets X and Y ,
the output distributions of M(X) and M(Y ) have Rényi divergence of order α at most ε, i.e.,"
OUR CONTRIBUTIONS,0.05339805825242718,"max
X∼Y Dα
 
M(X)||M(Y )

≤ε."
OUR CONTRIBUTIONS,0.055825242718446605,"We can convert from Rényi DP to approximate DP using, for example, the following formula:"
OUR CONTRIBUTIONS,0.05825242718446602,"Lemma 3 (Canonne et al. 2020). Suppose the mechanism M is
 
α, ε′
-RDP. Then M is also
(ε, δ(ε))-DP for arbitrary ε ≥0 with"
OUR CONTRIBUTIONS,0.06067961165048544,"δ(ε) = exp
 
(α −1)(ε′ −ε)
 α"
OUR CONTRIBUTIONS,0.06310679611650485,"
1 −1 α"
OUR CONTRIBUTIONS,0.06553398058252427,"α−1
.
(2.2)"
OUR CONTRIBUTIONS,0.06796116504854369,"As is common in practice, we carry out the RDP accounting such that we do bookkeeping of total
ε(α)-values for a list of RDP-orders (e.g. integer α’s) and in the end convert to (ε, δ)-guarantees by
minimizing over the values given by Equation(2.2) w.r.t. α. RDP accounting for compositions of DP
mechanisms is carried using standard RDP composition results (Mironov, 2017)."
OUR CONTRIBUTIONS,0.0703883495145631,"DP-SGD differs from SGD such that sample-wise gradients of a random mini-batch are clipped to
have an L2-norm at most C and normally distributed noise with variance σ2 is added to the sum of
the gradients of the mini-batch (Abadi et al., 2016). One iteration is given by"
OUR CONTRIBUTIONS,0.07281553398058252,"θj+1 = θj −ηj
 1 |B| X"
OUR CONTRIBUTIONS,0.07524271844660194,"x∈Bj
clip(∇f(x, θj), C) + Zj

,
(2.3)"
OUR CONTRIBUTIONS,0.07766990291262135,"where the noise Zj ∼N(0, C2σ2"
OUR CONTRIBUTIONS,0.08009708737864078,"|B|2 Id), f denotes the loss function, θ the model parameters, ηj the
learning rate hyperparameter at iteration j and |B| is the expected batch size (if we carry out Poisson
subsampling of mini-batches, |Bj| varies). There are several results that enable the RDP analysis
of DP-SGD iterations (Abadi et al., 2016; Balle et al., 2018; Zhu and Wang, 2019). The following
result by Zhu and Wang (2019) is directly applicable to analyzing DP-SGD, however, we also use
it for analyzing a variant of our hyperparameter tuning method."
OUR CONTRIBUTIONS,0.0825242718446602,"Theorem 4 (Zhu and Wang 2019). Suppose M is a
 
α, ε(α)

-RDP mechanism, w.r.t.
to the add/remove neighbourhood relation.
Consider the subsampled mechanism (M ◦
subsamplePoisson(γ))(X), where subsamplePoisson(γ) denotes Poisson subsampling with sampling
ratio γ. If M is
 
α, ε(α)

-RDP then M ◦subsamplePoisson(γ) is
 
α, ε′(α)

-RDP (α ≥2 is an
integer), where"
OUR CONTRIBUTIONS,0.08495145631067962,"ε′(α) =
1
α −1 log

(1 −γ)α−1(αγ −γ + 1) +
α
2"
OUR CONTRIBUTIONS,0.08737864077669903,"
γ2(1 −γ)α−2 exp(ε(2))"
OUR CONTRIBUTIONS,0.08980582524271845,"+ 3
Xα j=3 α
j"
OUR CONTRIBUTIONS,0.09223300970873786,"
γj(1 −γ)α−j exp((j −1)ε(j))

."
OUR CONTRIBUTIONS,0.09466019417475728,"We remark that the recent works (Koskela et al., 2020; Gopi et al., 2021; Zhu et al., 2022) give methods
to carry out (ε, δ)-analysis of DP-SGD tightly. As the state-of-the-art bounds for hyperparameter
tuning methods are RDP bounds (Papernot and Steinke, 2022), for simplicity, we will also analyze
DP-SGD using RDP."
OUR CONTRIBUTIONS,0.0970873786407767,"Intuitively, the leakage from hyperparameters is much smaller than from the model parameters,
however, considering it in the final accounting is needed to ensure rigorous DP guarantees. Currently
the most practical (ε, δ)-guarantees for DP hyperparameter tuning algorithms are those of (Papernot
and Steinke, 2022). In the results of Papernot and Steinke (2022) it is important that the number
of runs K with the hyperparameter tuning is randomized. They analyze various distributions for
drawing K, however, we focus on using the Poisson distribution as it is the most concentrated around
the mean among all the alternatives. The corresponding hyperparameter tuning algorithm and its
privacy guarantees are given by Thm.5."
OUR CONTRIBUTIONS,0.09951456310679611,"First recall: K is distributed according to a Poisson distribution with mean µ > 0, if for all non-
negative integer values k: P(K = k) = e−µ · µk k! ."
OUR CONTRIBUTIONS,0.10194174757281553,"Theorem 5 (Papernot and Steinke 2022). Let Q : X N →Y be a randomized algorithm satisfying
 
α, ε(α)

-RDP and (bε, bδ)-DP for some α ∈(1, ∞) and ε, bε, bδ ≥0. Assume Y is totally ordered. Let
the Poisson distribution parameter µ > 0. Define the hyperparameter tuning algorithm A : X N →
Y as follows. Draw K from a Poisson distribution with mean µ. Run Q(X) for K times. Then A(X)
returns the best value of those K runs (both the hyperparameters and the model parameters). If
K = 0, A(X) returns some arbitrary output. If e bε ≤1 +
1
α−1, then A satisfies
 
α, ε′(α)

-RDP,"
OUR CONTRIBUTIONS,0.10436893203883495,where ε′(α) = ε(α) + µ · bδ + log µ α−1 .
DP HYPERPARAMETER TUNING WITH A RANDOM SUBSET,0.10679611650485436,"3
DP Hyperparameter Tuning with a Random Subset"
DP HYPERPARAMETER TUNING WITH A RANDOM SUBSET,0.10922330097087378,"We next consider our main tool: we carry out the private hyperparameter tuning on a random subset,
and if needed, extrapolate the found hyperparameter values to larger datasets that we use for training
subsequent models. In our approach the subset of data used for tuning is generally smaller than the
data used for training the final model and thus we extrapolate the hyperparameter values."
DP HYPERPARAMETER TUNING WITH A RANDOM SUBSET,0.11165048543689321,"3.1
Our Method: Small Random Subset for Tuning"
DP HYPERPARAMETER TUNING WITH A RANDOM SUBSET,0.11407766990291263,Our method works as below:
DP HYPERPARAMETER TUNING WITH A RANDOM SUBSET,0.11650485436893204,"1. Use Poisson subsampling to draw X1 ⊂X: draw a random subset X1 such that each x ∈X
is included in X1 with probability q.
2. Compute (θ1, t1) = Mtune(X1), where Mtune is a hyperparameter tuning algorithm (e.g.,
the method by Papernot and Steinke, 2022) that outputs the vector of optimal hyperparame-
ters t1 and the corresponding model parameters θ1.
3. If needed, extrapolate the hyperparameters t1 to the dataset X \ X1: t1 →t2.
4. Compute θ2 = Mbase(θ1, t2, X \X1), where Mbase is the base mechanism (e.g., DP-SGD)."
DP HYPERPARAMETER TUNING WITH A RANDOM SUBSET,0.11893203883495146,"Denote the whole mechanism by M. Then, we may write"
DP HYPERPARAMETER TUNING WITH A RANDOM SUBSET,0.12135922330097088,"M(X) =
 
Mtune(X1), Mbase
 
Mtune(X1), X \ X1

,
(3.1)"
DP HYPERPARAMETER TUNING WITH A RANDOM SUBSET,0.12378640776699029,"where X1 ∼subsamplePoisson(q)(X). Additionally, we consider a variation of our method in which
we use the full dataset X instead of X \ X1 from step 3 onwards, i.e., the mechanism"
DP HYPERPARAMETER TUNING WITH A RANDOM SUBSET,0.1262135922330097,"M(X) =
 
Mtune(X1), Mbase
 
Mtune(X1), X

,
(3.2)"
DP HYPERPARAMETER TUNING WITH A RANDOM SUBSET,0.12864077669902912,"where X1 ∼subsamplePoisson(q)(X). We call these methods variant 1 and variant 2, respectively.
The RDP bounds for the variant 2 can be obtained with a standard subsampling and composition
result (e.g., Thm 4). We provide a tailored privacy analysis of the variant 1 in Section 3.3."
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.13106796116504854,"3.2
Extrapolating the DP-SGD Hyperparameters"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.13349514563106796,"We use simple heuristics to transfer the optimal hyperparameter values found for the small subset of
data to a larger dataset. The clipping constant C, the noise level σ, the subsampling ratio γ and the
total number of iterations T are kept constant in this transfer. As a consequence the (ε, δ)-privacy
guarantees are also the same for the models trained with the smaller and the larger dataset. For scaling
the learning rate, we use the heuristics used by van der Veen et al. (2018): we scale the learning rate
η with the dataset size. I.e., if we carry out the hyperparameter tuning using a subset of size m and
find an optimal value η∗, we multiply η∗by n/m when transferring to the dataset of size n."
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.13592233009708737,"This can be also heuristically motivated as follows. Consider T iterations of the DP-SGD (2.3). With
the above rules, the distribution of the noise that gets injected into the model trained with dataset
of size n is T
X"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.1383495145631068,"j=1
Zj ∼N "
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.1407766990291262,"0, T ·
  n"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.14320388349514562,mη∗2 σ2C2
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.14563106796116504,"(γ · n)2
Id !"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.14805825242718446,"∼N

0, T · η∗2σ2C2"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.15048543689320387,"(γ · m)2
Id "
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.1529126213592233,"which is exactly the distribution of the noise that was added to the model trained with the subsample
of size m. This principle of keeping the noise constant when scaling the hyperparameters was also
used by Sander et al. (2022)."
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.1553398058252427,"We arrive at our scaling rule also by taking a variational Bayesian view of DP-SGD. Mandt et al.
(2017) model the stochasticity of the SGD mini-batch gradients in a region approximated by a
constant quadratic convex loss by invoking the central limit theorem, and arrive at a continuous-time
multivariate Ornstein-Uhlenbeck (OU) process for which the discrete approximation is given by"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.15776699029126215,"∆θ = −ηg(θ) +
η
p"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.16019417475728157,"|B|
L∆W,
∆W ∼N(0, Id),
(3.3)"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.16262135922330098,"where |B| denotes the batch size of the SGD approximation, g(θ) the full gradient and LLT the covari-
ance matrix of the SGD noise. By minimizing the Kullback–Leibler divergence between the stationary
distribution of this OU-process and the Gaussian posterior distribution f(θ) ∝exp
 
−n·L(θ)

, where
L(θ) denotes the quadratic loss function and n is the size of the dataset, they arrive at the expression"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.1650485436893204,η∗= 2|B|
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.16747572815533981,"n
d
Tr(LLT )
for the optimal learning rate value (see Thm. 1, Mandt et al., 2017). We consider the case where
the additive DP noise dominates the SGD noise, and instead of the update (3.3) consider the update"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.16990291262135923,∆θ = −ηg(θ) + η · σ · C
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.17233009708737865,"|B|
∆W,
∆W ∼N(0, Id)
(3.4)"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.17475728155339806,"which equals the DP-SGD update (2.3) with the mini-batch gradient replaced by the full gradient.
Essentially the difference between (3.3) and (3.4) is
p"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.17718446601941748,"|B| replaced by |B|, and by the reasoning
used in (Thm. 1, Mandt et al., 2017), we see that the learning rate value that minimizes the KL
divergence between the approximate posterior and the Gaussian posterior f(θ) is then given by"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.1796116504854369,η∗= 2|B|2
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.1820388349514563,"n
d
Tr(σ2C2I) = 2
|B|2"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.18446601941747573,n · σ2C2 = 2 γ2n
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.18689320388349515,"σ2C2 .
(3.5)"
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.18932038834951456,"The scaling rule (3.5) also indicates that the optimal value of the learning rate should be scaled
linearly with the size of the dataset in case γ, σ and C are kept constant."
EXTRAPOLATING THE DP-SGD HYPERPARAMETERS,0.19174757281553398,"Training of certain models benefits from use of adaptive optimizers such as Adam (Kingma and Ba,
2014) or RMSProp, e.g., due to sparse gradients. Then the above extrapolation rules for DP-SGD are
not necessarily meaningful anymore. In our experiments, when training a neural network classifier
using Adam with DP-SGD gradients, we found that keeping the value of the learning rate fixed in
the transfer to the larger dataset lead to better results than increasing it as in case of DP-SGD. We
mention that there are principled ways of extrapolating the hyperparameters in non-DP setting such
as those of Klein et al. (2017)."
PRIVACY ANALYSIS,0.1941747572815534,"3.3
Privacy Analysis"
PRIVACY ANALYSIS,0.1966019417475728,"The RDP analysis of the variant 2 given in Equation (3.2) is straightforward. Since the tuning set X1
is sampled with Poisson subsampling with subsampling ratio q, we may write the mechanism as an
adaptive composition"
PRIVACY ANALYSIS,0.19902912621359223,"M(X) =

f
Mtune(X), Mbase
  f
Mtune(X), X

,"
PRIVACY ANALYSIS,0.20145631067961164,"where f
Mtune(X) = (Mtune ◦subsamplePoisson(q))(X). Using the RDP values given by Thm. 5 for"
PRIVACY ANALYSIS,0.20388349514563106,"Mtune and the subsampling amplification result of Thm. 4, we obtain RDP bounds for f
Mtune(X).
Using RDP bounds for Mbase (e.g., DP-SGD) and RDP composition results, we further get RDP
bounds for the mechanism M given in (3.2)."
PRIVACY ANALYSIS,0.20631067961165048,"Tailored RDP-Analysis. When we use the variant (3.1), i.e., we only use the rest of the data
X\X1 for Mbase, we can get even tighter RDP bounds. The following theorem gives tailored
RDP bounds for the mechanism (3.1). Similarly to the analysis by Zhu and Wang (2019) for the
Poisson subsampled Gaussian mechanism, we obtain RDP bounds using the RDP bounds of the
non-subsampled mechanisms and by using binomial expansions (the proof is given in Appendix C).
Theorem 6. Let M be the mechanism (3.1), such that the subset X1 is Poisson sampled with
subsampling ratio q, 0 ≤q ≤1 and let α > 1. Denote by εtune(α) and εbase(α) the RDP-values of
mechanisms Mtune and Mbase, respectively. Then, M is
 
α, ε(α)

-RDP for"
PRIVACY ANALYSIS,0.2087378640776699,"ε(α) = max{ε1(α), ε2(α)}, where"
PRIVACY ANALYSIS,0.2111650485436893,"ε1(α) =
1
α −1 log

qα · exp
 
(α −1)εtune(α)

+ (1 −q)α · exp
 
(α −1)εbase(α)
 + α−1
X j=1 α
j"
PRIVACY ANALYSIS,0.21359223300970873,"
· qα−j · (1 −q)j · exp
 
(α −j −1)εtune(α −j)

exp
 
(j −1)εbase(j)

(3.6) and"
PRIVACY ANALYSIS,0.21601941747572814,"ε2(α) =
1
α −1 log

(1 −q)α−1 · exp
 
(α −1)εbase(α)
 + α−1
X j=1"
PRIVACY ANALYSIS,0.21844660194174756,"α −1
j"
PRIVACY ANALYSIS,0.220873786407767,"
· qj · (1 −q)α−1−j · exp
 
j · εtune(j + 1)

· exp
 
(α −j −1)εbase(α −j)

."
PRIVACY ANALYSIS,0.22330097087378642,"(3.7)
Remark 7. The RDP bound given by Thm. 6 is optimal in a sense that it approaches εtune(α) and
εbase(α) as q →1 and q →0, respectively."
PRIVACY ANALYSIS,0.22572815533980584,"Remark 8. We can initialize the subsequent model training Mbase using the model θ1. This adaptivity
is included in all the RDP analyses of both mechanisms (3.1) and (3.2)."
PRIVACY ANALYSIS,0.22815533980582525,"Notice that in the bounds (3.6) and (3.7) the RDP parameter of the tuning algorithm, εtune(α), is
weighted with the parameter q and the RDP parameter of the base algorithm, εbase(α), is weighted
with 1 −q. This lowers the overall privacy cost in case the tuning set is chosen small enough."
PRIVACY ANALYSIS,0.23058252427184467,"Figure 1 illustrates how the (ε, δ)-bounds of the two variants (3.1) and (3.2) behave as functions of
the sampling parameter q used for sampling the tuning set X1, when the base mechanism DP-SGD is
run for 50 epochs with the subsampling ratio γ = 0.01 and noise level σ = 2.0. The bounds for the
variant 1 given in Equation (3.1) are computed using the RDP results of Thm. 6 and the bounds for
the variant 2 are computed using the subsampling amplification result of Thm. 4. The RDP bounds
are converted to (ε, δ)-bounds using the conversion rule of Lemma 3 with δ = 10−5. The fact that
the bounds for the variants 1 and 2 cross when µ = 45 at small values of q suggests that the bounds
of Thm. 6 could still be tightened."
PRIVACY ANALYSIS,0.23300970873786409,"Figure 1: Comparison of (ε, δ)-bounds for the variant 1 given in Equation (3.1) and the variant 2
given in Equation (3.2) as a function of the subsampling ratio q used for sampling the tuning set X1.
Also shown is the (ε, δ)-bound for the baseline algorithm described in Thm. 5. Here µ refers to the
expected number of model evaluations in the tuning algorithm."
COMPUTATIONAL SAVINGS,0.2354368932038835,"3.4
Computational Savings"
COMPUTATIONAL SAVINGS,0.23786407766990292,"Our scaling approach for DP-SGD described in Section 3.2 implies that the DP-SGD subsampling
ratio γ, the noise level σ and the number of iterations T are the same when evaluating the private
candidate models using the tuning set X1 and when evaluating the final model using the larger dataset."
COMPUTATIONAL SAVINGS,0.24029126213592233,"Thus, if we run the base algorithm for E epochs, we easily see that the expected number of required
gradient evaluations for the variant 1 given in (3.1) is given by E·(µ·q·n+(1−q)·n) and for the variant
2 given in (3.2) it is given by E · (µ · q · n + n), whereas the baseline requires in expectation µ · n · E
evaluations in case it is carrying out tuning with the same hyperparameter candidates as our method.
Since the number of iterations T is kept fixed, there are some constant overheads in the compute cost
such as those coming from the model updates. Therefore the actual speed ups are slightly smaller."
COMPUTATIONAL SAVINGS,0.24271844660194175,"For example, in our experiments with µ = 15 and q = 0.1, the baseline requires
µ
µ·q+1 ≈6 times
more gradient evaluations than our method and when µ = 45 and q = 0.1 the baseline requires ≈8
times more gradient evaluations. The actual speed ups are shown in the figures of Section 5."
DEALING WITH DP-SGD HYPERPARAMETERS THAT AFFECT THE DP GUARANTEES,0.24514563106796117,"4
Dealing with DP-SGD Hyperparameters that Affect the DP Guarantees"
DEALING WITH DP-SGD HYPERPARAMETERS THAT AFFECT THE DP GUARANTEES,0.24757281553398058,"Thm. 5 gives RDP-parameters of order α for the tuning algorithm, assuming the underlying candidate
picking algorithm is
 
α, ε(α)

-RDP. In case of DP-SGD, if we are tuning the learning rate η or
clipping constant C, and fix rest of the hyperparameters, these
 
α, ε(α)

-RDP bounds are fixed
for all the hyperparameter candidates. However, if we are tuning hyperparameters that affect the
DP guarantees, i.e., the subsampling ratio γ, the noise level σ or the length of the training T, it is
less straightforward to determine suitable uniform ε(α)-upper bounds. As is common practice, we
consider a grid Λ of α-orders for RDP bookkeeping (e.g. integer values of α’s)."
GRID SEARCH WITH RANDOMIZATION,0.25,"4.1
Grid Search with Randomization"
GRID SEARCH WITH RANDOMIZATION,0.2524271844660194,"To deal with this problem, we first set an approximative DP target value (ε, δ) that we use to adjust
some of the hyperparameters. For example, if we are tuning the subsampling ratio γ and training
length T, we can, for each choice of (γ, T), adjust the noise scale σ so that the resulting training
iteration is at most (ε, δ)-DP. Vice versa, we may tune γ and σ, and take maximal value of T such
that the resulting training iteration is at most (ε, δ)-DP."
GRID SEARCH WITH RANDOMIZATION,0.25485436893203883,"More specifically, we first fix ε, δ > 0 which represent the target approximative DP bound for each
candidate model. Denote by ε(T, δ, γ, σ) the ε-value of the subsampled Gaussian mechanism with
parameter values γ, σ and T and for fixed δ. To each pair of (γ, T), we attach a noise scale σγ,T such
that it is the smallest number with which the resulting composition is (ε, δ)-DP:"
GRID SEARCH WITH RANDOMIZATION,0.25728155339805825,"σγ,T = min{σ ∈R+ : ε(T, δ, γ, σ) ≤ε}."
GRID SEARCH WITH RANDOMIZATION,0.25970873786407767,"As the RDP values increase monotonously w.r.t. the number of compositions, it is straightforward to
find σγ,T , e.g., using the bisection method. Alternatively, we could fix a few values of σ, and to each
combination of (γ, σ), attach the largest T (denoted Tγ,σ) such that the target (ε, δ)-guarantee holds."
GRID SEARCH WITH RANDOMIZATION,0.2621359223300971,"We consider a finite grid Γ of possible hyperparameter values t (e.g., t = (γ, σ, T), where T is
adjusted to γ and σ). Then, for all t ∈Γ, we compute the corresponding RDP value εt(α) for each
α ∈Λ. Finally, for each α ∈Λ, we set"
GRID SEARCH WITH RANDOMIZATION,0.2645631067961165,"ε(α) = max
t∈Γ εt(α)."
GRID SEARCH WITH RANDOMIZATION,0.2669902912621359,"Then, since for each random draw of t, the DP-SGD trained candidate model is ε(α)-RDP, by
Lemma 9 given below, the candidate picking algorithm Q is also ε(α)-RDP. This approach is used in
the experiments of Figure 4, where we jointly tune T, γ and η."
RDP ANALYSIS,0.26941747572815533,"4.2
RDP Analysis"
RDP ANALYSIS,0.27184466019417475,"For completeness, in Appendix D we prove the following result which gives RDP bounds for the case
we randomly draw hyperparemeters that affect the privacy guarantees of the candidate models."
RDP ANALYSIS,0.27427184466019416,"Lemma 9. Denote by β the random variable of which outcomes are the hyperparameter candidates
(drawing either randomly from a grid or from given distributions). Consider an algorithm Q, that
first randomly picks hyperparameters t ∼β, then runs a randomized mechanism M(t, X). Suppose
M(t, X) is
 
α, ε(α)

-RDP for all t. Then, Q is
 
α, ε(α)

-RDP."
EXPERIMENTAL RESULTS,0.2766990291262136,"5
Experimental Results"
EXPERIMENTAL RESULTS,0.279126213592233,"We perform our evaluations on standard benchmark datasets for classification: CIFAR-10 (Krizhevsky
and Hinton, 2009), MNIST (LeCun et al., 1998), FashionMNIST (Xiao et al., 2017) and IMDB (Maas
et al., 2011). Full details of the experiments are given in Appendix A. When reporting the results, we
set δ = 10−5 in all experiments."
EXPERIMENTAL RESULTS,0.2815533980582524,"Learning rate tuning. Figures 2 and 3 summarize the results for learning rate tuning using our
methods and the baseline method by Papernot and Steinke (2022). The learning rate grid size is either
9 or 10 and the grid values are specified in Table 2 (Appendix). We fix the subsampling ratio γ and
the number of epochs to the values given in Table 1 (Appendix) for all models. The batch sizes in the
submodels are defined by scaling γ on the corresponding dataset sizes. We use q = 0.1 which means
that, for example, if the Poisson subsampling of DP-SGD gives in expectation a batch size of 128 in
the tuning phase of our methods, the expected batch sizes for the final models of variant 1 and 2 are
1152 and 1280, respectively. We use µ = 15 (the expected number of runs for the tuning algorithm)."
EXPERIMENTAL RESULTS,0.28398058252427183,"(a) trained with DP-SGD
(b) trained with DP-SGD"
EXPERIMENTAL RESULTS,0.28640776699029125,"(c) trained with DP-SGD
(d) trained with DP-SGD"
EXPERIMENTAL RESULTS,0.28883495145631066,"Figure 2: Tuning learning rate with DP-SGD. Test accuracies are averaged across 10 independent
runs and the error bars denote the standard error of the mean. The numbers in the legends refer to the
mean training timings of the baseline scaled with respect to minimum of variant 1 and 2. For example,
for CIFAR-10, the average training time for the baseline method is 6.06 times bigger than for the
fastest of our methods. For perspective, we also add curves showing the privacy cost of training a
single model with optimal hyperparameters obtained from the baseline."
EXPERIMENTAL RESULTS,0.2912621359223301,"Tuning all hyperparameters. Next, we jointly optimize the number of epochs, the DP-SGD
subsampling ratio γ, and the learning rate η using the hyperparameter candidates given in Table 2
(Appendix). The remaining setup is the same as in the previous experiment. Figure 4 shows the same"
EXPERIMENTAL RESULTS,0.2936893203883495,"(a) trained with DP-Adam
(b) trained with DP-Adam"
EXPERIMENTAL RESULTS,0.2961165048543689,"(c) trained with DP-Adam
(d) trained with DP-Adam"
EXPERIMENTAL RESULTS,0.29854368932038833,"Figure 3: Tuning learning rate with DP-Adam. Test accuracies are averaged across 10 independent
runs and the error bars denote the standard error of the mean. The numbers in the legends refer to the
mean training timings of the baseline scaled with respect to minimum of variant 1 and 2. For example,
for FashionMNIST, the average training time for the baseline method is 9.03 times bigger than the
fastest of our methods. For perspective, we also add curves showing the privacy cost of training a
single model with optimal hyperparameters obtained from the baseline. Figure 6 (Appendix) shows a
more detailed version of this plot."
EXPERIMENTAL RESULTS,0.30097087378640774,"quantities as Figures 2 and 3, however, higher values of µ are used to accommodate to increased
hyperparameter spaces."
EXPERIMENTAL RESULTS,0.30339805825242716,"Takeaways. Overall, for both DP-SGD and DP-Adam, we observe that both variants of our method
provide better privacy-utility trade-off and have a lower computational cost than the baseline method.
Additionally, we also note the benefits of tailored analysis (Thm 6) in Figure 2 in terms of slightly
higher accuracy for variant 1 compared to variant 2 for DP-SGD. In Figure 4 where we are also
tuning the batch size and epochs, the noise levels are higher compared to Figure 2. One reason for
slightly better privacy-utility trade-off for variant 2 with DP-SGD is possibly the fact that our tailored
bound is less tight for higher values of µ and small values of q (see Figure 1)."
DISCUSSION,0.3058252427184466,"6
Discussion"
DISCUSSION,0.308252427184466,"We have considered a simple strategy for lowering the privacy cost and computational cost of DP
hyperparameter tuning: we carry out tuning using a random subset of data and extrapolate the optimal
values to the larger training set used to train the final model. We have also provided methods to
tune the hyperparameters that affect the DP guarantees of the model training themselves, those"
DISCUSSION,0.3106796116504854,"(a) trained with DP-SGD
(b) trained with DP-SGD"
DISCUSSION,0.3131067961165049,"(c) trained with DP-SGD
(d) trained with DP-SGD"
DISCUSSION,0.3155339805825243,"Figure 4: Tuning of subsampling ratio, training epochs, and learning rate with DP-SGD. Test
accuracies are averaged across 10 independent runs and the error bars denote the standard error of
the mean. The numbers in the legends refer to the mean timings of the baseline method scaled with
respect to the minimum of variant 1 and 2. For perspective, we also add curves showing the privacy
cost of training a single model with optimal hyperparameters obtained from the baseline. Figure 7
(Appendix) shows a more detailed version of this plot."
DISCUSSION,0.3179611650485437,"being the noise level and subsampling ratio in case of DP-SGD. Our experiments show a clear
improvement over the baseline method by Papernot and Steinke (2022) when tuning DP-SGD for
neural networks and using simple but well-justified heuristics for the extrapolation. One obvious
limitation of our method is that it is limited to DP-SGD although we show also positive results for the
Adam optimizer combined with DP-SGD gradients. Finding out whether effective scaling rules could
be derived for DP-SGD with momentum, DP-FTRL (Kairouz et al., 2021) or more refined adaptive
DP optimizers such as DP2-RMSprop (Li et al., 2023), is left for future work. An interesting avenue
of future work is also to find more black-box type of extrapolation methods, something that has been
considered in the non-DP case (see, e.g. Klein et al., 2017). Another interesting question is how to
carry out more accurate privacy analysis using the so-called privacy loss distributions (PLDs) and
numerical accounting. The main reason for using RDP instead of PLD accounting in this work was
that the tightest privacy bounds for DP hyperparameter tuning are given in terms of RDP. We have
used Poisson subsampling to obtain the tuning set as it is relatively easy to analyze, however, other
sampling methods could be analyzed as well."
BROADER IMPACT STATEMENT,0.32038834951456313,"7
Broader Impact Statement"
BROADER IMPACT STATEMENT,0.32281553398058255,"Our present work makes DP machine learning more appealing and potentially more widespread in
practice. This can improve privacy protection in general but can also carry negative side effects. The
proposed method can make DP learning more appealing by improving DP hyperparameter tuning
leading to higher utility machine learning models at equivalent provable total privacy cost. Our
method has lower compute cost than some alternatives, potentially leading to considerable resource
savings. While DP gives strong privacy guarantees, it may have negative impacts as well, as some
DP learning methods have for example been shown to have a relatively lower utility for minorities."
BROADER IMPACT STATEMENT,0.32524271844660196,Acknowledgments
BROADER IMPACT STATEMENT,0.3276699029126214,"We would like to thank our colleague Laith Zumot for discussions about hyperparameter tuning
methods at the early stages of the project."
REFERENCES,0.3300970873786408,References
REFERENCES,0.3325242718446602,"Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. (2016).
Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on
Computer and Communications Security, pages 308–318."
REFERENCES,0.33495145631067963,"Balle, B., Barthe, G., and Gaboardi, M. (2018). Privacy amplification by subsampling: Tight analyses
via couplings and divergences. In Advances in Neural Information Processing Systems, volume 31."
REFERENCES,0.33737864077669905,"Bassily, R., Smith, A., and Thakurta, A. (2014). Private empirical risk minimization: Efficient
algorithms and tight error bounds. In 2014 IEEE 55th annual symposium on foundations of
computer science, pages 464–473. IEEE."
REFERENCES,0.33980582524271846,"Bun, M. and Steinke, T. (2016). Concentrated differential privacy: Simplifications, extensions, and
lower bounds. In Theory of Cryptography Conference, pages 635–658. Springer."
REFERENCES,0.3422330097087379,"Canonne, C., Kamath, G., and Steinke, T. (2020). The discrete Gaussian for differential privacy. In
Advances in Neural Information Processing Systems, volume 33."
REFERENCES,0.3446601941747573,"Chaudhuri, K. and Vinterbo, S. A. (2013). A stability-based validation procedure for differentially
private machine learning. Advances in Neural Information Processing Systems, 26."
REFERENCES,0.3470873786407767,"Dong, J., Roth, A., Su, W. J., et al. (2022). Gaussian differential privacy. Journal of the Royal
Statistical Society Series B, 84(1):3–37."
REFERENCES,0.34951456310679613,"Dwork, C. (2006). Differential privacy. In Proc. 33rd Int. Colloq. on Automata, Languages and Prog.
(ICALP 2006), Part II, pages 1–12."
REFERENCES,0.35194174757281554,"Gopi, S., Lee, Y. T., and Wutschitz, L. (2021). Numerical composition of differential privacy.
Advances in Neural Information Processing Systems, 34:11631–11642."
REFERENCES,0.35436893203883496,"Horváth, T., Mantovani, R. G., and de Carvalho, A. C. (2017). Effects of random sampling on
svm hyper-parameter tuning. In International Conference on Intelligent Systems Design and
Applications, pages 268–278. Springer."
REFERENCES,0.3567961165048544,"Kairouz, P., McMahan, B., Song, S., Thakkar, O., Thakurta, A., and Xu, Z. (2021). Practical and
private (deep) learning without sampling or shuffling. In International Conference on Machine
Learning, pages 5213–5225. PMLR."
REFERENCES,0.3592233009708738,"Killamsetty, K., Abhishek, G. S., Lnu, A., Ramakrishnan, G., Evfimievski, A., Popa, L., and Iyer, R.
(2022). Automata: Gradient based data subset selection for compute-efficient hyper-parameter
tuning. Advances in Neural Information Processing Systems, 35:28721–28733."
REFERENCES,0.3616504854368932,"Killamsetty, K., Durga, S., Ramakrishnan, G., De, A., and Iyer, R. (2021). Grad-match: Gradient
matching based data subset selection for efficient deep model training. In International Conference
on Machine Learning, pages 5464–5474. PMLR."
REFERENCES,0.3640776699029126,"Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. In International
Conference on Learning Representations."
REFERENCES,0.36650485436893204,"Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. (2017). Fast Bayesian optimization of
machine learning hyperparameters on large datasets. In International Conference on Artificial
Intelligence and Statistics, pages 528–536. PMLR."
REFERENCES,0.36893203883495146,"Koskela, A., Jälkö, J., and Honkela, A. (2020). Computing tight differential privacy guarantees
using FFT. In International Conference on Artificial Intelligence and Statistics, pages 2560–2569.
PMLR."
REFERENCES,0.3713592233009709,"Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images.
Technical report, University of Toronto."
REFERENCES,0.3737864077669903,"LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324."
REFERENCES,0.3762135922330097,"Li, T., Zaheer, M., Liu, K. Z., Reddi, S. J., McMahan, H. B., and Smith, V. (2023). Differentially
private adaptive optimization with delayed preconditioners. In International Conference on
Learning Representations."
REFERENCES,0.3786407766990291,"Liaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J. E., and Stoica, I. (2018). Tune: A research
platform for distributed model selection and training. arXiv preprint arXiv:1807.05118."
REFERENCES,0.38106796116504854,"Liese, F. and Vajda, I. (2006). On divergences and informations in statistics and information theory.
IEEE Transactions on Information Theory, 52(10):4394–4412."
REFERENCES,0.38349514563106796,"Liu, J. and Talwar, K. (2019). Private selection from private candidates. In Proceedings of the 51st
Annual ACM SIGACT Symposium on Theory of Computing, pages 298–309."
REFERENCES,0.3859223300970874,"Maas, A., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. (2011). Learning word
vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for
computational linguistics: Human language technologies, pages 142–150."
REFERENCES,0.3883495145631068,"Mandt, S., Hoffman, M. D., and Blei, D. M. (2017). Stochastic gradient descent as approximate
Bayesian inference. Journal of Machine Learning Research, 18:1–35."
REFERENCES,0.3907766990291262,"McSherry, F. D. (2009). Privacy integrated queries: an extensible platform for privacy-preserving data
analysis. In Proceedings of the 2009 ACM SIGMOD International Conference on Management of
data, pages 19–30."
REFERENCES,0.3932038834951456,"Mironov, I. (2017). Rényi differential privacy. In 2017 IEEE 30th computer security foundations
symposium (CSF), pages 263–275. IEEE."
REFERENCES,0.39563106796116504,"Mironov, I., Talwar, K., and Zhang, L. (2019). Rényi differential privacy of the sampled Gaussian
mechanism. arXiv preprint arXiv:1908.10530."
REFERENCES,0.39805825242718446,"Mohapatra, S., Sasy, S., He, X., Kamath, G., and Thakkar, O. (2022). The role of adaptive optimizers
for honest private hyperparameter selection. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 36, pages 7806–7813."
REFERENCES,0.40048543689320387,"Papernot, N. and Steinke, T. (2022). Hyperparameter tuning with Rényi differential privacy. In
International Conference on Learning Representations."
REFERENCES,0.4029126213592233,"Papernot, N., Thakurta, A., Song, S., Chien, S., and Erlingsson, Ú. (2020). Tempered sigmoid
activations for deep learning with differential privacy. In AAAI Conference on Artificial Intelligence."
REFERENCES,0.4053398058252427,"Sander, T., Stock, P., and Sablayrolles, A. (2022). Tan without a burn: Scaling laws of dp-sgd. arXiv
preprint arXiv:2210.03403."
REFERENCES,0.4077669902912621,"Smith, J., Asghar, H. J., Gioiosa, G., Mrabet, S., Gaspers, S., and Tyler, P. (2022). Making the most
of parallel composition in differential privacy. Proceedings on Privacy Enhancing Technologies,
1:253–273."
REFERENCES,0.41019417475728154,"Song, S., Chaudhuri, K., and Sarwate, A. D. (2013). Stochastic gradient descent with differentially
private updates. In 2013 IEEE global conference on signal and information processing, pages
245–248. IEEE."
REFERENCES,0.41262135922330095,"Steinke, T. (2022). Composition of differential privacy & privacy amplification by subsampling.
arXiv preprint arXiv:2210.00597."
REFERENCES,0.41504854368932037,"Swersky, K., Snoek, J., and Adams, R. P. (2013). Multi-task Bayesian optimization. Advances in
neural information processing systems, 26."
REFERENCES,0.4174757281553398,"Thakurta, A. G. and Smith, A. (2013). Differentially private feature selection via stability arguments,
and the robustness of the lasso. In Conference on Learning Theory, pages 819–850. PMLR."
REFERENCES,0.4199029126213592,"van der Veen, K. L., Seggers, R., Bloem, P., and Patrini, G. (2018). Three tools for practical differential
privacy. NeurIPS 2018 Privacy Preserving Machine Learning workshop, arXiv:1812.02890."
REFERENCES,0.4223300970873786,"Van Erven, T. and Harremos, P. (2014). Rényi divergence and kullback-leibler divergence. IEEE
Transactions on Information Theory, 60(7):3797–3820."
REFERENCES,0.42475728155339804,"Waring, J., Lindvall, C., and Umeton, R. (2020). Automated machine learning: Review of the
state-of-the-art and opportunities for healthcare. Artificial intelligence in medicine, 104:101822."
REFERENCES,0.42718446601941745,"Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-MNIST: a novel image dataset for benchmarking
machine learning algorithms. Technical report."
REFERENCES,0.42961165048543687,"Yousefpour, A., Shilov, I., Sablayrolles, A., Testuggine, D., Prasad, K., Malek, M., Nguyen, J., Ghosh,
S., Bharadwaj, A., Zhao, J., et al. (2021). Opacus: User-friendly differential privacy library in
PyTorch. In NeurIPS 2021 Workshop Privacy in Machine Learning, arXiv:2109.12298."
REFERENCES,0.4320388349514563,"Zhu, Y., Dong, J., and Wang, Y.-X. (2022). Optimal accounting of differential privacy via characteris-
tic function. In International Conference on Artificial Intelligence and Statistics, pages 4782–4817.
PMLR."
REFERENCES,0.4344660194174757,"Zhu, Y. and Wang, Y.-X. (2019). Poisson subsampled Rényi differential privacy. In International
Conference on Machine Learning, pages 7634–7642."
REFERENCES,0.4368932038834951,Appendix
REFERENCES,0.4393203883495146,"A
Full Description of Experiments"
REFERENCES,0.441747572815534,"Quality Metric and Evaluation. In all of our experiments, we have a partitioning of the available
data into train and test sets and we choose the best model based on the test accuracy. The quality
score applied on the test set is a relatively low sensitivity function, and therefore, even for a private
test set, parallel composition (for an RDP bound of parallel compositions, we refer to Appendix E)
can accommodate DP evaluation of a quality metric in the training budget itself. However, we assume
that only the training dataset is private and the test data is public for simplicity. This assumption of
test dataset being public and the approach of making only two (train and test) partitions of available
data instead of three (train, validation, and test) has been considered in many prior works (Mohapatra
et al., 2022; Papernot and Steinke, 2022) to study the proposed method in isolation."
REFERENCES,0.4441747572815534,"Our utility plots (Figures 2 to Figure 8) show the test accuracy of the final model against the final
approximate DP ε which includes the privacy cost of the tuning process and of the final model for
all methods. We fix δ = 10−5 always when reporting the ε-values. We fix q = 0.1 for all of our
methods in all experiments. We mention that in several cases smaller value of q would have lead to
better privacy-accuracy trade-off of the final model, however, we use the same value q = 0.1 in all
experiments for consistency."
REFERENCES,0.44660194174757284,"Methods. We consider the both variants of our proposed approach in our experiments. The RDP
parameters for the variant 1 are obtained by using Thm. 6 (new RDP result) and for variant 2 by
combining Thm. 4 (subsampling amplification) with Thm. 5 (RDP cost of the hyperparameter tuning).
We scale the hyperparameters in our methods for the training data of the final model as discussed in
Sec. 3.2. The method by Papernot and Steinke (2022) described in Thm. 5 is the baseline."
REFERENCES,0.44902912621359226,"Datasets and Models. We carry out our experiments on the following standard benchmark datasets
for classification: CIFAR-10 (Krizhevsky and Hinton, 2009), MNIST (LeCun et al., 1998), Fash-
ionMNIST (Xiao et al., 2017) and IMDB (Maas et al., 2011). For MNIST and IMDB, we use the
convolutional neural networks from the examples provided in the Opacus library Yousefpour et al.
(2021). For FashionMNIST, we consider a simple feedforward 3-layer network with hidden layers of
width 120. For CIFAR-10, we use a Resnet20 pre-trained on CIFAR-100 (Krizhevsky and Hinton,
2009) dataset so that only the last fully connected layer is trained. We minimize the cross-entropy loss
in all models. We optimize with DP-SGD and DP-Adam for all datasets. As suggested by Papernot
et al. (2020), we replace all ReLU activations with tempered sigmoid functions in CIFAR-10, MNIST,
and FashionMNIST networks, which limits the magnitudes of non-private gradients and improves
model accuracies."
REFERENCES,0.45145631067961167,"Hyperparameters. For these datasets, in one of the experiments we tune only the learning rate (η),
and in the other one η, γ (subsampling ratio), and the number of epochs, while fixing the clipping
constant C. For DP-Adam, we do not scale the learning rate. The number of trainable parameters and
the hyperparameter grids are provided in Table 2 (Appendix B). The numbers of epochs are chosen
to suit our computational constraints. Following the procedure described in Section 4.1, we always
adjust σ such that we compute the smallest σ that satisfies a target final (ε, δ) bound for each (γ,
epoch) pair. Furthermore following the RDP accounting procedure desribed in Section 4.1 we obtain
uniform RDP guarantees for the candidate models and furthermore RDP guarantees for our methods."
REFERENCES,0.4538834951456311,"Implementation.
For the implementation of DP-SGD and DP-Adam, we use the Opacus li-
brary (Yousefpour et al., 2021). For scalability, we explore the hyperparameter spaces with Ray
Tune (Liaw et al., 2018) on a dedicated multi-GPU cluster. We use two sets of gpus and one set is
much weaker than the other. All three methods have independent randomness (e.g. K hyperparameter
candidates sampled, DP noise). Ray tune maintains a job queue and the number of models trained
parallelly depends on the gpu count. Each model is trained on 0.5 gpu, and each gpu has enough
memory to accommodate 2 models. Models finish their entire training on the same gpu that was
allocated to them at the start of their training."
REFERENCES,0.4563106796116505,"Measuring training time. We want to reduce the dependence of our measurements on the available
hardware resources (e.g. number of gpus). For each method, we store the per epoch time in seconds
for each model being trained in parallel, and finally sum the timings for all epochs of all K models
(as if they were trained serially). The clock for each model starts only when it is under execution.
Among variant 1 and 2, the final training time depends mainly on the value of K sampled, though"
REFERENCES,0.4587378640776699,"variant 1 trains the final model with slightly smaller data. The baseline method takes much more time
to run compared to our methods on weaker GPUs, which explains the differences in speed gains."
REFERENCES,0.46116504854368934,(a) trained with DP-SGD
REFERENCES,0.46359223300970875,(b) trained with DP-SGD
REFERENCES,0.46601941747572817,(c) trained with DP-SGD
REFERENCES,0.4684466019417476,(d) trained with DP-SGD
REFERENCES,0.470873786407767,"Figure 5: Tuning only the learning rate with DP-SGD. Test accuracies are averaged across 10
independent runs and the error bars denote the standard error of the mean. The number in the legends
in the first column refer to the scaled mean training timings for the baseline method with respect
to the fastest of variant 1 and 2. The second column plots final ε vs. mean σ. Our methods inject
significantly smaller noise compared to the baseline for all ε regimes. We also observe that due to
tight analysis in Thm 6, σ for variant 1 is consistently lower than for variant 2. As a result, we see
slightly higher accuracy for variant 1 in many cases. The third column plots final ε vs. mean optimal
η. Note that due to randomess in the candidate selection process, optimal η’s for all three methods
need not be the same. For perspective, we also add curves showing the privacy cost of training a
single model with optimal hyperparameters obtained from the baseline."
REFERENCES,0.4733009708737864,(a) trained with DP-Adam
REFERENCES,0.47572815533980584,(b) trained with DP-Adam
REFERENCES,0.47815533980582525,(c) trained with DP-Adam
REFERENCES,0.48058252427184467,(d) trained with DP-Adam
REFERENCES,0.4830097087378641,"Figure 6: Tuning only the learning rate with DP-Adam. Test accuracies are averaged across 10
independent runs and the error bars denote the standard error of the mean. No learning rate scaling
was applied. The number in the legends in the first column refer to the scaled mean training timings
for the baseline method with respect to the fastest of variant 1 and 2. The second column plots final ε
vs. mean σ. Our methods inject significantly smaller noise compared to the baseline for all ε regimes.
We also observe that due to tight analysis in Thm 6, σ for variant 1 is consistently lower than for
variant 2. As a result, we see slightly higher accuracy for variant 1 in many cases. The third column
plots final ε vs. mean optimal η. Note that due to randomess in the candidate selection process,
optimal η’s for all three methods need not be the same. For perspective, we also add curves showing
the privacy cost of training a single model with optimal hyperparameters obtained from the baseline."
REFERENCES,0.4854368932038835,(a) trained with DP-SGD
REFERENCES,0.4878640776699029,(b) trained with DP-SGD
REFERENCES,0.49029126213592233,(c) trained with DP-SGD
REFERENCES,0.49271844660194175,(d) trained with DP-SGD
REFERENCES,0.49514563106796117,"Figure 7: Tuning all hyperparameters (η, epochs, and γ) with DP-SGD. Test accuracies are averaged
across 10 independent runs and the error bars denote the standard error of the mean. The numbers in
the legends in the first column refer to the scaled mean training timings for the baseline method with
respect to the fastest of variant 1 and 2. The second column plots final ε vs. mean σ. Our methods
inject significantly smaller noise compared to the baseline for all ε regimes even at high values of
µ. Since there are several hyperparameters at play together, it is difficult to attribute slightly higher
accuracy for variant 2 compared to 1 to any one factor. However, we suspect slight inferiority of our
bound for higher values of µ (Figure 1, right plot) and higher training dataset for the final model in
variant 2 could be the main reasons. The third (and fourth) column plots final ε vs. mean optimal η
(and γ). Noise level σ is a proxy for number of epochs, since we obtain a σ for each combination of γ
and number of epochs. Therefore, we omit plots showing final ϵ vs. epochs plot for readability. Note
that due to randomness in the candidate selection process, optimal η’s and γ’s for all three methods
need not be the same. For perspective, we also add curves showing the privacy cost of training a
single model with optimal hyperparameters obtained from the baseline."
REFERENCES,0.4975728155339806,(a) trained with DP-Adam
REFERENCES,0.5,(b) trained with DP-Adam
REFERENCES,0.5024271844660194,(c) trained with DP-Adam
REFERENCES,0.5048543689320388,"Figure 8: Tuning all hyperparameters (η, epochs, and γ) with DP-Adam. Test accuracies are averaged
across 10 independent runs and the error bars denote the standard errors of the means. The number in
the legends in the first column refer to the scaled mean training timings for the baseline method with
respect to the fastest of variant 1 and 2. The second column plots final ε vs. mean σ. Our methods
inject significantly smaller noise compared to the baseline for all ε regimes even at high values of
µ. Since there are several hyperparameters at play together, it is difficult to attribute slightly higher
accuracy for variant 2 compared to 1 to any one factor. However, we suspect slight inferiority of our
bound for higher values of µ (Figure 1, right plot) and higher training dataset for the final model in
variant 2 could be the main reasons. The third (and fourth) column plots final ε vs. mean optimal η
(and γ). Noise level σ is a proxy for number of epochs, since we obtain a σ for each combination of γ
and number of epochs. Therefore, we omit plots showing final ϵ vs. epochs plot for readability. Note
that due to randomness in the candidate selection process, optimal η’s and γ’s for all three methods
need not be the same."
REFERENCES,0.5072815533980582,"B
Hyperparameter Tables"
REFERENCES,0.5097087378640777,"Table 1: Tuning η: rest of the hyperparameters are fixed to these values. The hyperaparameter grids
for the learning rates are the same as in the second experiment and are given in Table 2."
REFERENCES,0.5121359223300971,"MNIST
FashionMNIST
CIFAR-10
IMDB
γ = B"
REFERENCES,0.5145631067961165,"N
0.0213
0.0213
0.0256
0.0256
epochs
40
40
40
110"
REFERENCES,0.5169902912621359,"Table 2: Tuning σ, η and T: datasets and the corresponding hyperparameter grids."
REFERENCES,0.5194174757281553,"train/test set
parameters
C
B
learning rate (η)
epochs
MNIST
60k/10k
∼26k
1
{128, 256}
{10−i}i∈{4,3.5,3,2.5,2,1.5,1,0.5,0}
{10,20,30,40}
FashionMNIST
60k/10k
∼109k
3
{128, 256}
{10−i}i∈{3,2.5,2,1.5,1,0.5,0,−0.5,−1,−1.5}
{10,20,30,40}
CIFAR-10
50k/10k
0.65k
3
{64, 128}
{10−i}i∈{3,2.5,2,1.5,1,0.5,0,−0.5,−1,−1.5}
{20,30,40}
IMDB
25k/25k
∼464k
1
{64, 128}
{10−i}i∈{4,3.5,3,2.5,2,1.5,1,0.5,0}
{70,90,110}"
REFERENCES,0.5218446601941747,"C
Proof of Thm 6"
REFERENCES,0.5242718446601942,"Proof. For the proof, let us denote for short Mtune =: M1 and Mbase =: M2. Let X ∈X n and
x′ ∈X We first consider bounding the Rényi divergence Dα
 
M(X ∪{x′})||M(X)

. Looking at
our approach which uses Poisson subsampling with subsampling ratio q to obtain the dataset X1, and
conditioning the output on the randomness in choosing X1, we can write the mechanism as a mixture
over all possible choices of X1 as"
REFERENCES,0.5266990291262136,"M(X) =
X"
REFERENCES,0.529126213592233,"X1
pX1 ·
 
M1(X1), M2(M1(X1), X\X1)

,
(C.1)"
REFERENCES,0.5315533980582524,"where pX1 is the probability of sampling X1. Since each data element is in X1 with probability q, we
can furthermore write M(X ∪{x′}) as a mixture"
REFERENCES,0.5339805825242718,"M(X ∪{x′}) =
X"
REFERENCES,0.5364077669902912,"X1
pX1 ·

q ·
 
M1(X1 ∪{x′}), M2(M1(X1 ∪{x′}), X\X1)
"
REFERENCES,0.5388349514563107,"+ (1 −q) ·
 
M1(X1), M2(M1(X1), X\X1 ∪{x′})

. (C.2)"
REFERENCES,0.5412621359223301,"From the quasi-convexity of the Rényi divergence (Van Erven and Harremos, 2014) and the expres-
sions (C.1) and (C.2), it follows that"
REFERENCES,0.5436893203883495,"Dα
 
M(X ∪{x′})||M(X)

≤sup
X1
Dα
 
q ·
 
M1(X1 ∪{x′}), M2(M1(X1 ∪{x′}), X\X1)
"
REFERENCES,0.5461165048543689,"+ (1 −q) ·
 
M1(X1), M2(M1(X1), X\X1 ∪{x′})

||
 
M1(X1), M2(M1(X1), X\X1)

.
(C.3)"
REFERENCES,0.5485436893203883,"Our aim is to express the right-hand side of the inequality (C.3) in terms of RDP parameters of M1
and M2. To this end, take an arbitrary X1 ⊂X, and denote by"
REFERENCES,0.5509708737864077,"• eP(t) the density function of M1(X1 ∪{x′}),"
REFERENCES,0.5533980582524272,"• P(t) the density function of M1(X1),"
REFERENCES,0.5558252427184466,"• eQ(t, s) the density function of M2(t, X\X1 ∪{x′}) for auxiliary variable t (the output of
M1),"
REFERENCES,0.558252427184466,"• Q(t, s) the density function of M2(t, X\X1) for auxiliary variable t."
REFERENCES,0.5606796116504854,"Then, we see that"
REFERENCES,0.5631067961165048,"P
  
M1(X1), M2(M1(X1), X\X1)

= (t, s)

= P(t) · Q(t, s)"
REFERENCES,0.5655339805825242,and similarly that
REFERENCES,0.5679611650485437,"P
 
q ·
 
M1(X1 ∪{x′}), M2(M1(X1 ∪{x′}), X\X1)
"
REFERENCES,0.5703883495145631,"+ (1 −q) ·
 
M1(X1), M2(M1(X1), X\X1 ∪{x′})

= (t, s)
"
REFERENCES,0.5728155339805825,"= q · P
  
M1(X1 ∪{x′}), M2(M1(X1 ∪{x′}), X\X1)

= (t, s)
"
REFERENCES,0.5752427184466019,"+ (1 −q) · P
  
M1(X1), M2(M1(X1), X\X1 ∪{x′})

= (t, s)
"
REFERENCES,0.5776699029126213,"= q · eP(t) · Q(t, s) + (1 −q) · P(t) · eQ(t, s)."
REFERENCES,0.5800970873786407,"By the definition of the Rényi divergence, we have that"
REFERENCES,0.5825242718446602,"exp

(α −1)Dα
 
q ·
 
M1(X1 ∪{x′}), M2(M1(X1 ∪{x′}), X\X1)
"
REFERENCES,0.5849514563106796,"+ (1 −q) ·
 
M1(X1), M2(M1(X1), X\X1 ∪{x′})

||
 
M1(X1), M2(M1(X1), X\X1)
"
REFERENCES,0.587378640776699,"=
Z Z  
q · eP(t) · Q(t, s) + (1 −q) · P(t) · eQ(t, s)"
REFERENCES,0.5898058252427184,"P(t) · Q(t, s) !α"
REFERENCES,0.5922330097087378,"· P(t) · Q(t, s) dt ds."
REFERENCES,0.5946601941747572,"(C.4)
which can be expanded as"
REFERENCES,0.5970873786407767,"Z Z  
q · eP(t) · Q(t, s) + (1 −q) · P(t) · eQ"
REFERENCES,0.5995145631067961,"P(t) · Q(t, s) !α"
REFERENCES,0.6019417475728155,"P(t) · Q(t, s) dt ds =
Z Z"
REFERENCES,0.6043689320388349,"q ·
eP(t)
P(t) + (1 −q) ·
eQ(t, s)
Q(t, s) !α"
REFERENCES,0.6067961165048543,"P(t) · Q(t, s) dt ds"
REFERENCES,0.6092233009708737,"=
Z Z
qα
 eP(t) P(t) !α"
REFERENCES,0.6116504854368932,"P(t) · Q(t, s) dt ds"
REFERENCES,0.6140776699029126,"+
Z Z
(1 −q)α
 eQ(t, s)"
REFERENCES,0.616504854368932,"Q(t, s) !α"
REFERENCES,0.6189320388349514,"P(t) · Q(t, s) dt ds"
REFERENCES,0.6213592233009708,"+
Z Z
α · qα−1 · (1 −q) · eP(t) P(t) !α−1"
REFERENCES,0.6237864077669902,"P(t) · eQ(t, s) dt ds"
REFERENCES,0.6262135922330098,"+
Z Z
α · q · (1 −q)α−1 ·"
REFERENCES,0.6286407766990292,"eQ(t, s)"
REFERENCES,0.6310679611650486,"Q(t, s) !α−1"
REFERENCES,0.633495145631068,"Q(t, s) · eP(t) dt ds"
REFERENCES,0.6359223300970874,"+
Z Z
α−2
X j=2 α
j"
REFERENCES,0.6383495145631068,"
· qα−j · (1 −q)j ·   eP(t) P(t) !α−j P(t)    "
REFERENCES,0.6407766990291263,"eQ(t, s)"
REFERENCES,0.6432038834951457,"Q(t, s) !j"
REFERENCES,0.6456310679611651,"Q(t, s) "
REFERENCES,0.6480582524271845,dt ds. (C.5)
REFERENCES,0.6504854368932039,"We next bound five integrals on the right hand side of Equation (C.5). For the first two integrals, we
use the RDP-bounds for M1 and M2 to obtain
Z Z  eP(t) P(t) !α"
REFERENCES,0.6529126213592233,"P(t)Q(t, s) dt ds =
Z  eP(t) P(t) !α"
REFERENCES,0.6553398058252428,"P(t) dt ≤exp
 
(α −1)ε1(α)

.
(C.6)"
REFERENCES,0.6577669902912622,"and
Z Z  eQ(t, s)"
REFERENCES,0.6601941747572816,"Q(t, s) !α"
REFERENCES,0.662621359223301,"Q(t, s)P(t) ds dt ≤
Z
exp
 
(α −1)ε2(α)

P(t) dt = exp
 
(α −1)ε2(α)

,"
REFERENCES,0.6650485436893204,"(C.7)
where ε1 and ε2 give the RDP-parameters of order α for M1 and M2, respectively. The third and
fourth integral can be bounded analogously. In the second inequality we have also used the fact"
REFERENCES,0.6674757281553398,"that the RDP-parameters of M2 are independent of the auxiliary variable t. Similarly, for the third
integral, we have Z Z   eP(t) P(t) !α−j P(t)    "
REFERENCES,0.6699029126213593,"eQ(t, s)"
REFERENCES,0.6723300970873787,"Q(t, s) !j"
REFERENCES,0.6747572815533981,"Q(t, s) "
REFERENCES,0.6771844660194175,"ds dt ≤
Z   eP(t) P(t) !α−j P(t) "
REFERENCES,0.6796116504854369,"exp
 
(j −1)ε2(j)

dt"
REFERENCES,0.6820388349514563,"≤exp
 
(α −j −1)ε1(α −j)

· exp
 
(j −1)ε2(j)

. (C.8)"
REFERENCES,0.6844660194174758,"Substituting (C.6), (C.7) (and similar expressions for the third and fourth integral) and (C.8) to
Equation (C.5), we get a bound for the right-hand side of Equation (C.4). Since X1 ⊂X was
arbitrary, we arrive at the claim via the inequality (C.3)."
REFERENCES,0.6868932038834952,"Next, we consider bounding Dα
 
M(X)||M(Y )

.
The proof goes similarly as the one for
Dα
 
M(Y )||M(X)

. Denote"
REFERENCES,0.6893203883495146,"ε2(α) = Dα
 
M(X)||M(X ∪{x′})

."
REFERENCES,0.691747572815534,"With the notation of proof of Thm. 6, we see that, instead of the right-hand side of(C.4), we need to
bound"
REFERENCES,0.6941747572815534,"exp
 
(α −1)ε2(α)
"
REFERENCES,0.6966019417475728,"=
Z Z  
P(t) · Q(t, s)"
REFERENCES,0.6990291262135923,"q · eP(t) · Q(t, s) + (1 −q) · P(t) · eQ(t, s) !α"
REFERENCES,0.7014563106796117,"(q · eP(t) · Q(t, s) + (1 −q) · P(t) · eQ(t, s)) dt ds."
REFERENCES,0.7038834951456311,"In order to use here the series approach, we need to use Lemma 10: P · Q"
REFERENCES,0.7063106796116505,q · eP · Q + (1 −q) · P · eQ !α
REFERENCES,0.7087378640776699,(q · eP · Q + (1 −q) · P · eQ) = P · Q
REFERENCES,0.7111650485436893,q · eP · Q + (1 −q) · P · eQ !α−1
REFERENCES,0.7135922330097088,· P · Q = 
REFERENCES,0.7160194174757282,"q ·
eP
P + (1 −q) ·
eQ
Q !1−α"
REFERENCES,0.7184466019417476,· P · Q = 
REFERENCES,0.720873786407767,"q ·
eP
P
Q"
REFERENCES,0.7233009708737864,"eQ
+ (1 −q) !1−α · eQ Q !1−α"
REFERENCES,0.7257281553398058,· P · Q = 
REFERENCES,0.7281553398058253,"q ·
eP
P
Q"
REFERENCES,0.7305825242718447,"eQ
+ (1 −q) !1−α ·
Q eQ"
REFERENCES,0.7330097087378641,"α−1
· P · Q ≤  q · P eP"
REFERENCES,0.7354368932038835,"eQ
Q + (1 −q) !α−1 ·
Q eQ"
REFERENCES,0.7378640776699029,"α−1
· P · Q =  q · P eP"
REFERENCES,0.7402912621359223,"eQ
Q + (1 −q) !α−1 ·
Q eQ"
REFERENCES,0.7427184466019418,"α
· P · eQ, (C.9)"
REFERENCES,0.7451456310679612,"where in the inequality we have used Lemma 10. Now we can expand

q · P e
P"
REFERENCES,0.7475728155339806,"e
Q
Q + 1 −q
α−1
: "
REFERENCES,0.75,"1 −q + q · P eP eQ
Q !α−1 ·
Q eQ"
REFERENCES,0.7524271844660194,"α
· P · eQ = "
REFERENCES,0.7548543689320388,"
α−1
X j=0"
REFERENCES,0.7572815533980582,"α −1
j"
REFERENCES,0.7597087378640777,"
qj · (1 −q)α−1−j ·
P eP"
REFERENCES,0.7621359223300971,"j  eQ Q !j ·
Q eQ"
REFERENCES,0.7645631067961165,"α
· P · eQ = "
REFERENCES,0.7669902912621359,"
α−1
X j=0"
REFERENCES,0.7694174757281553,"α −1
j"
REFERENCES,0.7718446601941747,"
qj · (1 −q)α−1−j ·
P eP j Q eQ"
REFERENCES,0.7742718446601942,"α−j
"
REFERENCES,0.7766990291262136,"· P · eQ = α−1
X j=0"
REFERENCES,0.779126213592233,"α −1
j"
REFERENCES,0.7815533980582524,"
qj · (1 −q)α−1−j ·
P eP"
REFERENCES,0.7839805825242718,"j+1
eP ·
Q eQ"
REFERENCES,0.7864077669902912,"α−j
eQ."
REFERENCES,0.7888349514563107,"Then, we use the known ε1(α) and ε2(α)-values as in the inequalities (C.8) to arrive at the claim."
REFERENCES,0.7912621359223301,"C.1
Auxiliary Lemma"
REFERENCES,0.7936893203883495,We need the following inequality for the proof of Thm 6.
REFERENCES,0.7961165048543689,"Lemma 10 (Lemma 35, Steinke 2022). For all p ∈[0, 1] and x ∈(0, ∞),"
REFERENCES,0.7985436893203883,"1
1 −p + p"
REFERENCES,0.8009708737864077,"x
≤1 −p + p · x."
REFERENCES,0.8033980582524272,"D
Additional Details to Section 4"
REFERENCES,0.8058252427184466,"D.1
Random Search"
REFERENCES,0.808252427184466,"Here, we assume we are given some distributions of the hyperparameter candidates and the algorithm
Q draws hyperparameters using them. In order to adjust the noise level for each candidate, we
take a α-line as an RDP upper bound. More specifically, we require that the candidate models are
 
α, c · α

-RDP for some c > 0 and for all α ∈Λ. Then the noise scale σγ,T for each draw of (γ, T)
is the minimum scale for which the
 
α, c · α

-RDP bound holds, i.e.,"
REFERENCES,0.8106796116504854,"σγ,T = min{σ ∈R+ : T · εγ,σ(α) ≤c · α for all α ∈Λ}."
REFERENCES,0.8131067961165048,"Similarly, we can find the maximum T based on σ and γ such that the mechanism is
 
α, c · α

-RDP
for all α ∈Λ. Again, by Lemma 9 below, the candidate picking algorithm Q is then c · α-RDP and
we may use Thm. 5 to obtain RDP bounds for the tuning algorithm."
REFERENCES,0.8155339805825242,"D.1.1
Adjusting the Parameters T and σ for DP-SGD"
REFERENCES,0.8179611650485437,"We next discuss the reasons for the success of strategies described in Section 4. It is often a good
approximation to say that the RDP-guarantees of the Poisson subsampled Gaussian mechanism are
lines as functions of the RDP order α, i.e., that the guarantees are those a Gaussian mechanism
with some sensitivity and noise level values. For example, (Thm. 11, Mironov et al., 2019) show
that the Poisson subsampled Gaussian mechanism is
 
α, 2γ2α/σ2
-RDP when α is sufficiently
small. Also, (Thm. 38, Steinke, 2022) show that if the underlying mechanism is ρ-zCDP, then the
Poisson subsampled version with subsampling ratio γ is
 
α, 10γ2ρα

-RDP when α is sufficiently
small. Notice that the Gaussian mechanism with L2-sensitivity ∆and noise level σ is (∆2/2σ2)-
zCDP (Bun and Steinke, 2016)."
REFERENCES,0.8203883495145631,"We numerically observe, that the larger the noise level σ and the smaller the subsampling ratio γ, the
better the line approximation of the RDP-guarantees (see Figure 9)."
REFERENCES,0.8228155339805825,"In case the privacy guarantees (either (ε, δ)-DP or RDP) are approximately those of a Gaussian
mechanisms with some sensitivity and noise level values, both of the approaches for tuning the
hyperparameters γ, σ and T described in Section 4 would lead to very little slack. This is because for
the Gaussian mechanism, both the RDP guarantees (Mironov, 2017) and (ε, δ)-DP guarantees (Dong
et al., 2022) depend monotonously on the scaled parameter"
REFERENCES,0.8252427184466019,"eσ =
σ
∆·
√ T
."
REFERENCES,0.8276699029126213,"This means that if we adjust the training length T based on values of σ by having some target
(δ, ε)-bound for the candidate model with grid search of Section 4.1, the resulting RDP upper bounds
of different candidates will not be far from each other (and similarly for adjusting σ based on value
of T). Similarly, for random search of Section D.1, when adjusting T based on values of σ, the RDP
guarantees of all the candidate models would be close to the upper bound (c · α, c > 0), i.e., they
would not be far from each other."
REFERENCES,0.8300970873786407,"5
10
15
20
25 0.05 0.10 0.15 0.20 0.25 0.30 0.35 ( )"
REFERENCES,0.8325242718446602,"=2, T = 1000
Gauss RDP-fit ( =2)"
REFERENCES,0.8349514563106796,"=4, T = 4000
Gauss RDP-fit ( =4)"
REFERENCES,0.837378640776699,"=6, T = 8000
Gauss RDP-fit ( =6)"
REFERENCES,0.8398058252427184,"5
10
15
20
25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 ( )"
REFERENCES,0.8422330097087378,"=2, T = 1000
Gauss RDP-fit ( =2)"
REFERENCES,0.8446601941747572,"=4, T = 4000
Gauss RDP-fit ( =4)"
REFERENCES,0.8470873786407767,"=6, T = 8000
Gauss RDP-fit ( =6)"
REFERENCES,0.8495145631067961,"Figure 9: DP-SGD RDP curves for different values of noise level σ and number of compostions T.
Left: γ = 1/100, right: γ = 1/50 and the corresponding lines with the smallest slope that give upper
bounds for the RDP orders up to α = 24."
REFERENCES,0.8519417475728155,"D.2
Proof of Lemma 9"
REFERENCES,0.8543689320388349,"Lemma 11. Denote by β the random variable of which outcomes are the hyperparameter candidates
(drawing either randomly from a grid or from given distributions). Consider an algorithm Q, that
first randomly picks hyperparameters t ∼β, then runs a randomised mechanism M(t, X). Suppose
M(t, X) is
 
α, ε(α)

-RDP for all t. Then, Q is
 
α, ε(α)

-RDP."
REFERENCES,0.8567961165048543,"Proof. Suppose the hyperparameters t are outcomes of a random variable β. Let X and Y be
neighbouring datasets. Then, if p(t, s) and q(t, s) (as functions of s) give the density functions of
M(t, X) and M(t, Y ), respectively, we have that"
REFERENCES,0.8592233009708737,"Q(X) ∼Et∼β p(t, s)
and
Q(Y ) ∼Et∼β q(t, s)."
REFERENCES,0.8616504854368932,"Since for any distributions p and q, and for any α > 1, exp
 
(α −1)Dα(p||q)

=
R 
p(t)
q(t)
α
q(t) dt"
REFERENCES,0.8640776699029126,"gives an f-divergence (for f(x) = xα), it is also jointly convex w.r.t. p and q (Liese and Vajda, 2006).
Using Jensen’s inequality, we have"
REFERENCES,0.866504854368932,"exp
 
(α −1)Dα
 
Q(X)||Q(Y )

=
Z Et∼β p(t, s)"
REFERENCES,0.8689320388349514,"Et∼β q(t, s)"
REFERENCES,0.8713592233009708,"α
· Et∼β q(t, s) ds ≤Et∼β"
REFERENCES,0.8737864077669902,"Z p(t, s)"
REFERENCES,0.8762135922330098,"q(t, s)"
REFERENCES,0.8786407766990292,"α
· q(t, s) ds"
REFERENCES,0.8810679611650486,"≤Et∼β exp
 
(α −1)Dα
 
M(t, X)||M(t, Y )
"
REFERENCES,0.883495145631068,= exp ((α −1)ε(α))
REFERENCES,0.8859223300970874,from which the claim follows.
REFERENCES,0.8883495145631068,"E
f-Divergence of Parallel Compositions"
REFERENCES,0.8907766990291263,"We first formulate the parallel composition result for general f-divergences (Lemma 12). We then
obtain the RDP bound for parallel compositions as a corollary (Cor. 13)."
REFERENCES,0.8932038834951457,"Our Lemma 12 below can be seen as an f-divergence version of the (ε, 0)-DP result given in (Thm. 4
McSherry, 2009). Corollary 2 by (Smith et al., 2022) gives the corresponding result in terms of
µ-Gaussian differential privacy (GDP), and it is a special case of our Lemma 12 as µ-GDP equals
the (ε, δ)-DP (i.e., the hockey-stick divergence) of the Gaussian mechanism with a certain noise
scale (Cor. 1, Dong et al., 2022)."
REFERENCES,0.8956310679611651,"We define f-divergence for distributions on Rd as follows. Consider two probability densities P and
Q defined on Rd, such that if Q(x) = 0 then also P(x) = 0, and a convex function f : [0, ∞) →R.
Then, an f-divergence (Liese and Vajda, 2006) is defined as"
REFERENCES,0.8980582524271845,"Df(P||Q) =
Z
f
P(t) Q(t)"
REFERENCES,0.9004854368932039,"
Q(t) dt."
REFERENCES,0.9029126213592233,"In case the data is divided into disjoint shards and separate mechanisms are applied to each shard,
the f-divergence upper bound for two neighbouring datasets can be obtained from the individual
f-divergence upper bounds:
Lemma 12. Suppose a dataset X ∈X N is divided into k disjoint shards Xi, i ∈[k], and mechanisms
Mi, i ∈[k], are applied to the shards, respectively. Consider the adaptive composition"
REFERENCES,0.9053398058252428,"M(X) =
 
M1(X1), M2(X2, M1(X1)), . . . , Mk(Xk, M1(X1), . . . , Mk−1(Xk−1))

."
REFERENCES,0.9077669902912622,"Then, we have that"
REFERENCES,0.9101941747572816,"max
X∼Y Df
 
M(X)||M(Y )

≤max
i∈[k] max
X∼Y Df
 
Mi(X)||Mi(Y )

."
REFERENCES,0.912621359223301,"Proof. Let X and Y be divided into k equal-sized disjoint shards and suppose Y is a neighbour-
ing dataset such that X and Y differ in jth shard, i.e., Xj ∼Yj and Y =
 
Y1, Y2, . . . , Yk

=
 
X1, . . . , Xj−1, Yj, Xj+1, . . . , Xk

."
REFERENCES,0.9150485436893204,"Then, we see that"
REFERENCES,0.9174757281553398,"P
 
M(X) = (a1, . . . , ak)
"
REFERENCES,0.9199029126213593,"P
 
M(Y ) = (a1, . . . , ak)
"
REFERENCES,0.9223300970873787,"= P
 
M1(X1) = a1

· P
 
M1(X2, a1) = a2

· · · P
 
Mk(Xk, a1, . . . , ak−1) = ak
"
REFERENCES,0.9247572815533981,"P
 
M1(Y1) = a1

· P
 
M1(Y2, a1) = a2

· · · P
 
Mk(Yk, a1, . . . , ak−1) = ak
"
REFERENCES,0.9271844660194175,"= P
 
Mj(Xj, a1, . . . , aj−1) = aj
"
REFERENCES,0.9296116504854369,"P
 
Mj(Yj, a1, . . . , aj−1) = aj
 ."
REFERENCES,0.9320388349514563,"and furthermore, denoting a = (a1, . . . , ak),"
REFERENCES,0.9344660194174758,"Df
 
M(X)||M(Y ))
 =
Z
f"
REFERENCES,0.9368932038834952,"P
 
M(X) = a
"
REFERENCES,0.9393203883495146,"P
 
M(Y ) = a
 !"
REFERENCES,0.941747572815534,"P
 
M(Y ) = a

da =
Z
f"
REFERENCES,0.9441747572815534,"P
 
Mj(Xj, a1, . . . , aj−1) = aj
"
REFERENCES,0.9466019417475728,"P
 
Mj(Yj, a1, . . . , aj−1) = aj
 !"
REFERENCES,0.9490291262135923,"P
 
M(Y ) = a

da =
Z
f"
REFERENCES,0.9514563106796117,"P
 
Mj(Xj, a1, . . . , aj−1) = aj
"
REFERENCES,0.9538834951456311,"P
 
Mj(Yj, a1, . . . , aj−1) = aj
 !"
REFERENCES,0.9563106796116505,"P
 
M1(Y1) = a1

· P
 
M1(Y2, a1) = a2

· · ·"
REFERENCES,0.9587378640776699,"P
 
Mj(Yj, a1, . . . , aj−1) = aj

da1 . . . daj"
REFERENCES,0.9611650485436893,"= Df(Mj(Xj)||Mj(Yj))

·
Z
P
 
M1(Y1) = a1

· P
 
M1(Y2, a1) = a2

· · ·"
REFERENCES,0.9635922330097088,"P
 
Mj−1(Yj−1, a1, . . . , aj−2) = aj−1

da1 . . . daj−1
= Df(Mj(Xj)||Mj(Yj))

. Thus,"
REFERENCES,0.9660194174757282,"Df(M(X)||M(Y )) = Df(Mj(Xj)||Mj(Yj)) ≤max
X∼Y Df(Mj(X)||Mj(Y ))"
REFERENCES,0.9684466019417476,"and also, we have that"
REFERENCES,0.970873786407767,"max
X∼Y Df(M(X)||M(Y )) = max
i∈[k] max
X∼Y Df(Mi(X)||Mi(Y ))."
REFERENCES,0.9733009708737864,"Corollary 13. Suppose a dataset X ∈X N is divided into k disjoint shards Xi, i ∈[k], and
mechanisms Mi, i ∈[k], are applied to the shards, respectively. Consider the mechanism"
REFERENCES,0.9757281553398058,"M(X) =
 
M1(X1), . . . , Mk(Xk)

."
REFERENCES,0.9781553398058253,"Suppose each Mi is
 
α, εi(α)

-RDP, respectively. Then, M is
 
α, maxi∈[k] εi(α)

-RDP."
REFERENCES,0.9805825242718447,Proof. This follows from Lemma 12 since
REFERENCES,0.9830097087378641,"exp
 
(α −1)Dα(M(X)||M(Y ))

=
Z  
P
 
M(X) = a
"
REFERENCES,0.9854368932038835,"P
 
M(Y ) = a
 !α"
REFERENCES,0.9878640776699029,"P
 
M(Y ) = a

da"
REFERENCES,0.9902912621359223,"is an f-divergence for f(x) = xα. Thus, by Lemma 12 we have that"
REFERENCES,0.9927184466019418,"max
X∼Y exp
 
(α −1)Dα(M(X)||M(Y ))

≤max
i∈[k] max
X∼Y exp
 
(α −1)Dα(Mi(X)||Mi(Y ))
"
REFERENCES,0.9951456310679612,from which it follows that
REFERENCES,0.9975728155339806,"max
X∼Y Dα(M(X)||M(Y )) ≤max
i∈[k] max
X∼Y Dα(Mi(X)||Mi(Y )) = max
i∈[k] εi(α)."
