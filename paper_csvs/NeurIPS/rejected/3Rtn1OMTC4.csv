Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008635578583765112,"Robotic motor control necessitates the ability to predict the dynamics of envi-
1"
ABSTRACT,0.0017271157167530224,"ronments and interaction objects. However, advanced self-supervised pre-trained
2"
ABSTRACT,0.0025906735751295338,"visual representations (PVRs) in robotic motor control, leveraging large-scale
3"
ABSTRACT,0.0034542314335060447,"egocentric videos, often focus solely on learning the static content features of
4"
ABSTRACT,0.004317789291882556,"sampled image frames. This neglects the crucial temporal motion clues in human
5"
ABSTRACT,0.0051813471502590676,"video data, which implicitly contain key knowledge about sequential interacting
6"
ABSTRACT,0.006044905008635579,"and manipulating with the environments and objects. In this paper, we present
7"
ABSTRACT,0.0069084628670120895,"a simple yet effective robotic motor control visual pre-training framework that
8"
ABSTRACT,0.007772020725388601,"jointly performs spatiotemporal prediction with dual decoders, utilizing large-scale
9"
ABSTRACT,0.008635578583765112,"video data, termed as STP. STP adheres to two key designs in a multi-task learning
10"
ABSTRACT,0.009499136442141624,"manner. First, we perform spatial prediction on the masked current frame for
11"
ABSTRACT,0.010362694300518135,"learning content features. Second, we utilize the future frame with an extremely
12"
ABSTRACT,0.011226252158894647,"high masking ratio as a condition, based on the masked current frame, to conduct
13"
ABSTRACT,0.012089810017271158,"temporal prediction of future frame for capturing motion features. This asymmet-
14"
ABSTRACT,0.012953367875647668,"ric masking and decoder architecture design is very efficient, ensuring that our
15"
ABSTRACT,0.013816925734024179,"representation focusing on motion information while capturing spatial details. We
16"
ABSTRACT,0.01468048359240069,"carry out the largest-scale BC evaluation of PVRs for robotic motor control to date,
17"
ABSTRACT,0.015544041450777202,"which encompasses 21 tasks within a real-world Franka robot arm and 5 simulated
18"
ABSTRACT,0.016407599309153715,"environments. Extensive experiments demonstrate the effectiveness of STP as well
19"
ABSTRACT,0.017271157167530225,"as unleash its generality and data efficiency by further post-pre-training and hybrid
20"
ABSTRACT,0.018134715025906734,"pre-training. Our code and weights will be released for further applications.
21"
INTRODUCTION,0.018998272884283247,"1
Introduction
22"
INTRODUCTION,0.019861830742659757,"In NLP and CV, adapting pre-trained foundation models from large-scale data to various downstream
23"
INTRODUCTION,0.02072538860103627,"tasks has seen great success. For example, pre-trained visual representations using self-supervised [38,
24"
INTRODUCTION,0.02158894645941278,"15, 67, 2, 93] or weakly-supervised [71, 25, 55] methods exhibit strong generalization ability for
25"
INTRODUCTION,0.022452504317789293,"visual understanding. However, in robot learning, due to data scarcity and homogeneity, some
26"
INTRODUCTION,0.023316062176165803,"groundbreaking methods [53, 1] resort to training from scratch only using domain-specific data.
27"
INTRODUCTION,0.024179620034542316,"Recently, inspired by the success of transfer learning in CV, many works [69, 73, 65, 58, 59, 19] have
28"
INTRODUCTION,0.025043177892918825,"explored developing a pre-trained visual representation (PVR) using large-scale out-of-domain data
29"
INTRODUCTION,0.025906735751295335,"for various robotic motor control tasks. Currently, one successful paradigm [73, 99, 59, 19] is to use
30"
INTRODUCTION,0.02677029360967185,"large-scale egocentric video datasets [29] and train vanilla vision transformers (ViT) [22] based on
31"
INTRODUCTION,0.027633851468048358,"MAE [38], which exhibits excellent learning efficiency and generalization ability for learning policy
32"
INTRODUCTION,0.02849740932642487,"from raw pixel. Among them, the Ego4D [29] dataset offers numerous first-person human-object
33"
INTRODUCTION,0.02936096718480138,"interaction scenes and good motion clues. We argue that although learning static spatial structure
34"
INTRODUCTION,0.030224525043177894,"priors from task-relevant pre-training data sources is crucial, designing a more relevant self-supervised
35"
INTRODUCTION,0.031088082901554404,"proxy task for motor control should not be overlooked. Therefore, in this paper, we aim to develop a
36"
INTRODUCTION,0.03195164075993091,"more relevant self-supervised proxy task for robotic motor control representation learning.
37"
INTRODUCTION,0.03281519861830743,"Robotic motor control typically requires fine-grained spatial localization and relatively dense se-
38"
INTRODUCTION,0.03367875647668394,"mantics. With its ability to effectively capture low-level geometry and space structure, MAE [38]
39"
INTRODUCTION,0.03454231433506045,"pre-training excels at this task. However, is dense spatial content sufficient for robotic motor control?
40"
INTRODUCTION,0.03540587219343696,"Some neuroscientific studies [50, 21, 88] suggest the brain’s different areas or cells show special-
41"
INTRODUCTION,0.03626943005181347,"ization. Some are dedicated to processing the information of temporal object motion, while others
42"
INTRODUCTION,0.037132987910189985,"focus on static spatial details. Their combination results in subjective pattern perception. Inspired by
43"
INTRODUCTION,0.037996545768566495,"this finding, we hypothesize that an effective robotic motor control pre-training proxy task should
44"
INTRODUCTION,0.038860103626943004,"require joint learning of spatial content features and temporal motion features. However, current
45"
INTRODUCTION,0.039723661485319514,"methods [73, 59, 19] use MAE pre-training with image frames from human videos, capturing only
46"
INTRODUCTION,0.04058721934369603,"static content features. They overlook the temporal motion clues in human videos, which implicitly
47"
INTRODUCTION,0.04145077720207254,"contain key knowledge about sequential interaction with environment and manipulation of objects.
48"
INTRODUCTION,0.04231433506044905,"Therefore, we aim to bridge this gap by incorporating these motion clues into our proxy task.
49"
INTRODUCTION,0.04317789291882556,"Based on the analysis above, the most critical challenge is the absence of action annotations in human
50"
INTRODUCTION,0.04404145077720207,"video data for modeling object motion. To model interaction and manipulation actions from actionless
51"
INTRODUCTION,0.044905008635578586,"video data, we propose to implicitly capture them by predicting future frame pixels based on current
52"
INTRODUCTION,0.045768566493955096,"frame. However, predicting the future frame without any conditions could contain high uncertainty
53"
INTRODUCTION,0.046632124352331605,"and be extremely difficult. Therefore, we propose to use the future frame with an extremely high
54"
INTRODUCTION,0.047495682210708115,"masking ratio as a prompt condition, specifically 95%, which serves to reveal some behavior and
55"
INTRODUCTION,0.04835924006908463,"dynamic priors, i.e. what to do and how to do it. In the experiments section, we will further explore
56"
INTRODUCTION,0.04922279792746114,"different condition alternatives, including language narration and their combination. Additionally,
57"
INTRODUCTION,0.05008635578583765,"directly and simply executing temporal prediction could lead the model to overlook static spatial
58"
INTRODUCTION,0.05094991364421416,"details, and it is also not efficient enough. Therefore, another technical contribution of STP is to jointly
59"
INTRODUCTION,0.05181347150259067,"perform spatial prediction by masking the current frame with 75% masking ratio. In summary, we
60"
INTRODUCTION,0.05267702936096719,"present STP, a multi-task self-supervised pre-training framework through spatiotemporal predictive
61"
INTRODUCTION,0.0535405872193437,"learning. Our STP asymmetrically mask the current frame and future frame from a video clip, using
62"
INTRODUCTION,0.054404145077720206,"a spatial decoder to conduct spatial prediction for content learning and a temporal decoder to conduct
63"
INTRODUCTION,0.055267702936096716,"temporal prediction for motion learning. This asymmetric masking and decoder architecture design
64"
INTRODUCTION,0.05613126079447323,"ensures that our pre-trained encoder focusing on motion information while capturing spatial details.
65"
INTRODUCTION,0.05699481865284974,"Subsequently, we establish our evaluation scheme. Currently, how to adapt pre-trained visual
66"
INTRODUCTION,0.05785837651122625,"representations for robotic motor control still remain an open question. Considering the expensive
67"
INTRODUCTION,0.05872193436960276,"cost of robot data collection or exploration, we employ a data-efficient paradigm of few-shot behavior
68"
INTRODUCTION,0.05958549222797927,"cloning by learning from demonstrations (Lfd). To demonstrate the generalization ability of visual
69"
INTRODUCTION,0.06044905008635579,"representation, our primary evaluation scheme involves freezing the visual encoder during policy
70"
INTRODUCTION,0.0613126079447323,"training. Additionally, considering that fine-tuning ViT with few demonstrations might lead to
71"
INTRODUCTION,0.06217616580310881,"overfitting and masked modeling exhibits excellent data efficiency [86, 102, 52] in domain-in data,
72"
INTRODUCTION,0.06303972366148532,"we further follow the post-pre-training [7, 93, 59] paradigm to perform STP pre-training with task-
73"
INTRODUCTION,0.06390328151986183,"specific data to achieve better results. It is noteworthy that different tasks do not share representation
74"
INTRODUCTION,0.06476683937823834,"in this setting. Finally, we conduct the largest-scale BC evaluation of PVRs for robotic motor control
75"
INTRODUCTION,0.06563039723661486,"to date to demonstrate the effectiveness of STP, which encompasses 21 tasks ( 2 real-world tasks and
76"
INTRODUCTION,0.06649395509499137,"19 simulation tasks across 5 environments). These simulation tasks are derived from the union of
77"
INTRODUCTION,0.06735751295336788,"manipulation and locomotion tasks from prior works [65, 59].
78"
INTRODUCTION,0.06822107081174439,"We make the following four contributions: (1) We present STP, a self-supervised visual pre-
79"
INTRODUCTION,0.0690846286701209,"training framework for robotic motor control, which jointly conducts spatiotemporal prediction with
80"
INTRODUCTION,0.06994818652849741,"asymmetric masking and decoder architecture design for content and motion features learning. (2)
81"
INTRODUCTION,0.07081174438687392,"We further expand STP by performing hybrid pre-training with ImageNet-MAE and post-pre-training
82"
INTRODUCTION,0.07167530224525043,"with task-specific data, unleashing its generality and data efficiency. (3) To our best knowledge, we
83"
INTRODUCTION,0.07253886010362694,"conduct the largest-scale BC evaluation of PVRs for robotic motor control to date to demonstrate the
84"
INTRODUCTION,0.07340241796200346,"effectiveness of STP. (4) Our experiments yield some insightful observations. In temporal prediction,
85"
INTRODUCTION,0.07426597582037997,"language does not significantly enhance performance. Instead, single-modality self-supervised
86"
INTRODUCTION,0.07512953367875648,"paradigm achieves the best results. This finding is highly encouraging for self-supervised robotic
87"
INTRODUCTION,0.07599309153713299,"motor control representation learning. Moreover, in the few-shot BC setting, naively scaling up model
88"
INTRODUCTION,0.0768566493955095,"size does not necessarily lead to improved outcomes. Finally, incorporating more diverse data and
89"
INTRODUCTION,0.07772020725388601,"domain-in data into the pre-training can further enhance performance.
90"
RELATED WORK,0.07858376511226252,"2
Related Work
91"
RELATED WORK,0.07944732297063903,"Pre-trained Visual Representation Learning. Large-scale visual representation pre-training are
92"
RELATED WORK,0.08031088082901554,"continually empowering computer vision. The primary supervised learning methods include learning
93"
RELATED WORK,0.08117443868739206,"image recognition [40, 87] from ImageNet [20] and learning multi-modal alignment [71] from image-
94"
RELATED WORK,0.08203799654576857,"text pairs. Currently, self-supervised learning methods are enjoying significant popularity, primarily
95"
RELATED WORK,0.08290155440414508,"falling into two main categories. The first category utilizes contrastive learning [39, 15, 14] technique
96"
RELATED WORK,0.08376511226252159,"or joint-embedding architecture [13] to learn view-invariance. The second category performs masked
97"
RELATED WORK,0.0846286701208981,"modeling [7, 38, 100, 95, 4, 2] and predict the pixel or representation of invisible parts in space. In
98"
RELATED WORK,0.08549222797927461,"addition, some methods [106, 67, 8] have also proposed to combine different optimization objectives
99"
RELATED WORK,0.08635578583765112,"in a multi-task learning manner. Recently pre-trained visual representation learning for robotic motor
100"
RELATED WORK,0.08721934369602763,"control have bee rapidly developing [69, 65, 73, 99, 58, 57, 46, 59, 19]. These methods cover different
101"
RELATED WORK,0.08808290155440414,"backbones (ResNet [40], ViT [22]), different policy learning methods (reinforcement learning [99],
102"
RELATED WORK,0.08894645941278066,"behavior cloning [69, 65, 59], reward function [58] and task specification [42]), different adaptation
103"
RELATED WORK,0.08981001727115717,"schemes (linear probing [69, 65, 46, 59], fine-tuning [19] and designing adapters [78, 56]), and
104"
RELATED WORK,0.09067357512953368,"different evaluation environments (diverse simulation benchmarks). At present, it is still unclear how
105"
RELATED WORK,0.09153713298791019,"these factors collectively influence the performance. In this paper, we choose scalable vanilla vision
106"
RELATED WORK,0.0924006908462867,"transformer [22] as our backbone and data-efficient few-shot behavior cloning paradigm to conduct
107"
RELATED WORK,0.09326424870466321,"policy learning, while ensuring the backbone is frozen during policy training.
108"
RELATED WORK,0.09412780656303972,"Temporal Predictive Learning. Early works once explored representation learning through future
109"
RELATED WORK,0.09499136442141623,"prediction, encompassing image [61], video [35, 80] and audio [66]. VideoMAE [86, 93] extend
110"
RELATED WORK,0.09585492227979274,"MAE [38] to 3D video architecture. Recently TrackMAE [17] and SiamMAE [33] predict the
111"
RELATED WORK,0.09671848013816926,"masked future frame based on unmasked current frame, leading to a better capture of temporal
112"
RELATED WORK,0.09758203799654577,"correspondence and achieving outstanding performance in object tracking and segmentation tasks. In
113"
RELATED WORK,0.09844559585492228,"robot learning, predicting future visual states primarily serves as a transition dynamic model such as
114"
RELATED WORK,0.09930915371329879,"World Models [62, 77] and Dreamer [76]. [85, 9] predict the future visual states using goal image in
115"
RELATED WORK,0.1001727115716753,"robot data. GR-1 [97] conducts language-conditioned video prediction for policy model pre-training
116"
RELATED WORK,0.10103626943005181,"in a frozen visual representation space.
[96] proposed dynamics-aware representation learning,
117"
RELATED WORK,0.10189982728842832,"and [82, 72] employed forward dynamics for self-supervised pre-training. Some works explored
118"
RELATED WORK,0.10276338514680483,"to train video prediction models and utilize visual foresight [32], inverse dynamics models [18],
119"
RELATED WORK,0.10362694300518134,"goal-conditioned policy learning [23], and geometry estimation [51] methods for motor control,
120"
RELATED WORK,0.10449050086355786,"respectively.
[92] fine-tuned pre-trained representations into dynamic and functional distance
121"
RELATED WORK,0.10535405872193437,"modules for manipulation tasks. Unlike these works, we utilize the public large-scale egocentric
122"
RELATED WORK,0.10621761658031088,"video data and employ masked spatiotemporal predictive learning as a self-supervised proxy task
123"
RELATED WORK,0.1070811744386874,"(without any language or action annotations) for robotic motor control representation learning,
124"
RELATED WORK,0.1079447322970639,"instead of designing elaborate architectures or methods for specific predictive tasks [28, 37].
125"
RELATED WORK,0.10880829015544041,"Vision-based Robot Learning. Vision-based robot learning plays a crucial role in robotics com-
126"
RELATED WORK,0.10967184801381692,"munity. Recently some related works focus on studying model architectures [44, 12, 47], observa-
127"
RELATED WORK,0.11053540587219343,"tion spaces [107], downstream policy learning methods [41], sim-to-real transfer [79], designing
128"
RELATED WORK,0.11139896373056994,"adapters [78, 56], learning-from-scratch baseline [36], and affordance model [6, 105, 45, 60], in
129"
RELATED WORK,0.11226252158894647,"visuo-motor representation learning. Other related works [70, 5, 91, 101, 48] attempt to learn ma-
130"
RELATED WORK,0.11312607944732297,"nipulation skills from small-scale and in-domain human videos. In addition, language-conditioned
131"
RELATED WORK,0.11398963730569948,"vision robot learning has received significant attention. Some works scale multimodal robotic
132"
RELATED WORK,0.114853195164076,"data [42, 11, 34, 90, 24, 68, 84] or introduce Internet data and knowledge [81, 103, 10, 54, 43, 94, 64]
133"
RELATED WORK,0.1157167530224525,"for end-to-end robot learning. In our study, we pre-train a off-the-shelf visual representation from
134"
RELATED WORK,0.11658031088082901,"large-scale egocentric video datasets for robotic motor control tasks. Our method is more simple and
135"
RELATED WORK,0.11744386873920552,"general for different downstream tasks of motor control.
136"
METHOD,0.11830742659758203,"3
Method
137"
METHOD,0.11917098445595854,"In this section, we describe our method in details. First, we give an overview of our spatiotemporal
138"
METHOD,0.12003454231433507,"predictive pretraining (STP) framework. Then, we give a technical description on our core components
139"
METHOD,0.12089810017271158,"during pre-training: the masked image encoder and dual decoders scheme. Finally, we describe how
140"
METHOD,0.12176165803108809,"to adapt our pre-trained encoder to downstream robotic motor control tasks.
141"
OVERIEW OF STP,0.1226252158894646,"3.1
Overiew of STP
142"
OVERIEW OF STP,0.1234887737478411,"As illustrated in Figure 1, our STP aims to pre-train an image encoder for robotic motor control from
143"
OVERIEW OF STP,0.12435233160621761,"video datasets. This pre-trained image encoder is subsequently frozen and directly transferred to solve
144"
OVERIEW OF STP,0.12521588946459414,"motor control tasks. Specifically, given a video dataset D, our goal is to learn an image encoder Φenc,
145"
OVERIEW OF STP,0.12607944732297063,"that maps images to the visual representations. During pre-training and post-pre-training, D represents
146"
OVERIEW OF STP,0.12694300518134716,"large-scale out-of-domain videos and task-specific demonstration videos, respectively. After pre-
147"
OVERIEW OF STP,0.12780656303972365,"training, we reuse Φenc for downstream motor control policy learning. Specifically, the downstream
148"
OVERIEW OF STP,0.12867012089810018,Encoder
OVERIEW OF STP,0.12953367875647667,Meta World
OVERIEW OF STP,0.1303972366148532,"Adroit
Trifinger"
OVERIEW OF STP,0.13126079447322972,DMControl Real-world Franka
OVERIEW OF STP,0.13212435233160622,"Visual Representation Pre-training
Evaluation on Motor Control"
OVERIEW OF STP,0.13298791018998274,Sampling 75% 95%
OVERIEW OF STP,0.13385146804835923,Masking
OVERIEW OF STP,0.13471502590673576,Predictive future frame pixel
OVERIEW OF STP,0.13557858376511225,Policy Model
OVERIEW OF STP,0.13644214162348878,"Encoder
Spatial
Decoder"
OVERIEW OF STP,0.13730569948186527,Temporal
OVERIEW OF STP,0.1381692573402418,Decoder
OVERIEW OF STP,0.13903281519861832,"Predictive current frame pixel
Future frame feature
Current frame feature
Masking token"
OVERIEW OF STP,0.13989637305699482,"Concatenate 
views, frames
 & proprioceptive"
OVERIEW OF STP,0.14075993091537134,Encoder
OVERIEW OF STP,0.14162348877374784,"Optional
Future frame"
OVERIEW OF STP,0.14248704663212436,Current frame
OVERIEW OF STP,0.14335060449050085,Franka-Kitchen
TASKS,0.14421416234887738,21 Tasks
TASKS,0.14507772020725387,"Figure 1: STP framework. Left: During pre-training, we sample the current frame and the future frame
from the video clip, and carry out spatiotemporal predictive pre-training. Right: During motor control tasks
evaluation, we freeze the pre-trained encoder to extract visual state representations and discard the decoders."
TASKS,0.1459412780656304,"task will require an agent to make sequential action decisions based on visual observations O. Instead
149"
TASKS,0.14680483592400692,"of using the raw observation images as direct input like end-to-end policy learning from pixel, the
150"
TASKS,0.14766839378238342,"agent will employ the pre-trained Φenc to extract its visual state representation Φenc(O) for the
151"
TASKS,0.14853195164075994,"subsequent policy learning module.
152"
MASKED IMAGE ENCODER,0.14939550949913644,"3.2
Masked Image Encoder
153"
MASKED IMAGE ENCODER,0.15025906735751296,"We first introduce the pipeline of our image encoder. Our image encoder processes image frames
154"
MASKED IMAGE ENCODER,0.15112262521588946,"using a vanilla vision transformer [22]. Given a image I ∈RC×H×W , we initially process it by the
155"
MASKED IMAGE ENCODER,0.15198618307426598,"patch embedding layer to obtain its token sequences T, where T = {Pi}N
i=1 and N is the the total
156"
MASKED IMAGE ENCODER,0.15284974093264247,"token number, (e.g., N = 196 for a 224 × 224 image with a patch size of 16 × 16). Then we add the
157"
MASKED IMAGE ENCODER,0.153713298791019,"fixed 2D sine-cosine positional embeddings for all tokens. Following this, we mask and remove a
158"
MASKED IMAGE ENCODER,0.15457685664939552,"part of tokens, according to a randomly generated masking map M(ρ), where ρ is the masking ratio.
159"
MASKED IMAGE ENCODER,0.15544041450777202,"The encoder applies several transformer blocks (consisting of a global self-attention layer and a FFN
160"
MASKED IMAGE ENCODER,0.15630397236614854,"layer) on all unmasked tokens: Z = Φenc(Tu), where Tu = {Ti}i∈(1−M(ρ)). During this process, a
161"
MASKED IMAGE ENCODER,0.15716753022452504,"[CLS] token is added at the beginning.
162"
MASKED IMAGE ENCODER,0.15803108808290156,"Then we describe our encoding process during pre-training. We randomly sample two frames from a
163"
MASKED IMAGE ENCODER,0.15889464594127806,"video clip based on an interval: the current frame Ic and the future frame If. Following the above
164"
MASKED IMAGE ENCODER,0.15975820379965458,"pipeline, we randomly generate two asymmetric masking maps for the current frame and the future
165"
MASKED IMAGE ENCODER,0.16062176165803108,"frame, denoted as Mc = Mc(ρc) and Mf = Mf(ρf), respectively. Each of these maps has a
166"
MASKED IMAGE ENCODER,0.1614853195164076,"different masking ratio. We then use these maps to separately process the two frames and obtain their
167"
MASKED IMAGE ENCODER,0.16234887737478412,"features, Zc and Zf. As analyzed above, our STP aims to jointly learn content and motion features
168"
MASKED IMAGE ENCODER,0.16321243523316062,"by spatiotemporal predictive learning. For content feature learning, we follow MAE [38], masking a
169"
MASKED IMAGE ENCODER,0.16407599309153714,"portion of the current frame based on Mc, with ρc = 75%, and predict the masked parts during the
170"
MASKED IMAGE ENCODER,0.16493955094991364,"decoding process. This encourages the model to learn spatial and geometric structure priors from the
171"
MASKED IMAGE ENCODER,0.16580310880829016,"current frame data through spatial reasoning. For motion feature learning, we establish an objective
172"
MASKED IMAGE ENCODER,0.16666666666666666,"to predict the future frame based on the masked current frame. However, predicting the future frame
173"
MASKED IMAGE ENCODER,0.16753022452504318,"without any conditions could be meaningless and extremely challenging. Therefore, we use the future
174"
MASKED IMAGE ENCODER,0.16839378238341968,"frame with an extremely high masking ratio as a condition, specifically ρf = 95%, which reveals
175"
MASKED IMAGE ENCODER,0.1692573402417962,"some behavior and dynamic priors. In the experiments section, we will further discuss different
176"
MASKED IMAGE ENCODER,0.17012089810017272,"condition schemes, including language narration and the combination between them. In summary,
177"
MASKED IMAGE ENCODER,0.17098445595854922,"our encoding process during pre-training can be formally described as follows:
178"
MASKED IMAGE ENCODER,0.17184801381692574,"Zc = Φenc(Ic, Mc),
Zf = Φenc(If, Mf).
(1)"
MASKED IMAGE ENCODER,0.17271157167530224,"Joint 
Self-attention
FFN
Joint 
Self-attention
FFN
Cross-attention"
MASKED IMAGE ENCODER,0.17357512953367876,"(a)
(b)"
MASKED IMAGE ENCODER,0.17443868739205526,Current frame feature
MASKED IMAGE ENCODER,0.17530224525043178,Masking token
MASKED IMAGE ENCODER,0.17616580310880828,Future frame feature
MASKED IMAGE ENCODER,0.1770293609671848,"Key & Value ×N
×N"
MASKED IMAGE ENCODER,0.17789291882556132,Figure 2: Temporal decoder design. (a) Standard joint-self architecture. (b) Our self-cross architecture.
DUAL DECODERS,0.17875647668393782,"3.3
Dual Decoders
179"
DUAL DECODERS,0.17962003454231434,"To jointly capture static content and object motion features for better spatiotemporal understanding,
180"
DUAL DECODERS,0.18048359240069084,"our STP present a dual decoders scheme to predict both the pixel of current and future frame
181"
DUAL DECODERS,0.18134715025906736,"simultaneously in a multi-task learning manner. As shown in Figure 1, our dual decoder scheme
182"
DUAL DECODERS,0.18221070811744386,"includes a spatial decoder Φdec_s for spatial prediction and a temporal decoder Φdec_t for temporal
183"
DUAL DECODERS,0.18307426597582038,"prediction. We firstly give a technical description on them, respectively. Then we describe how we
184"
DUAL DECODERS,0.18393782383419688,"combine them into our final method.
185"
DUAL DECODERS,0.1848013816925734,"Spatial Decoder. To capture static content features, our spatial decoder is solely utilized for pro-
186"
DUAL DECODERS,0.18566493955094993,"cessing the current frame visual feature. Specifically, after obtaining the masked current frame
187"
DUAL DECODERS,0.18652849740932642,"visual feature Zc, we concatenate it with some learnable masking tokens, leading to the formation
188"
DUAL DECODERS,0.18739205526770294,"of Zd
c = Zc ∪{Mi}i∈Mc, where Mc is the current frame masking map. Then, each of these tokens
189"
DUAL DECODERS,0.18825561312607944,"further adds a corresponding positional embedding. Subsequently, Zd
c undergoes decoding in the
190"
DUAL DECODERS,0.18911917098445596,"decoder and is continuously updated. The architecture of the spatial decoder block aligns with the
191"
DUAL DECODERS,0.18998272884283246,"standard transformer encoder block, comprised of a global self-attention layer and a FFN layer.
192"
DUAL DECODERS,0.19084628670120898,"Finally, with the deocoded token sequence Zd
c, our spatial decoder predicts the invisible tokens of the
193"
DUAL DECODERS,0.19170984455958548,"current frame ˆIdc, operating under the current frame masking map Mc.
194"
DUAL DECODERS,0.192573402417962,"Temporal Decoder. To capture motion features, our temporal decoder jointly processes the current
195"
DUAL DECODERS,0.19343696027633853,"frame and the future frame which serves as the temporal prediction condition. To elaborate, we
196"
DUAL DECODERS,0.19430051813471502,"firstly obtain the masked current frame feature Zc and the masked future frame feature Zf. We then
197"
DUAL DECODERS,0.19516407599309155,"concatenate Zf with the masking tokens that have the positional embedding added, resulting in Zd
f.
198"
DUAL DECODERS,0.19602763385146804,"Following this, Zd
f and Zc interact within the temporal decoder for decoding. The architecture of our
199"
DUAL DECODERS,0.19689119170984457,"temporal decoder block is in alignment with the standard transformer decoder block [89], consisting
200"
DUAL DECODERS,0.19775474956822106,"of a self-attention layer, a cross-attention layer, and a FFN layer, as shown in Figure 2 (b). During
201"
DUAL DECODERS,0.19861830742659758,"decoding, the self-attention layer and FFN are solely used to process Zd
f. For the cross-attention
202"
DUAL DECODERS,0.19948186528497408,"layer, Zd
f is continuously updated as the query, while Zc, acting as the key and value, is kept constant.
203"
DUAL DECODERS,0.2003454231433506,"Compared to standard architecture, it ensures that the past frame representation space will not be
204"
DUAL DECODERS,0.20120898100172713,"updated in the temporal decoder and are specifically used for temporal correlation and prediction.
205"
DUAL DECODERS,0.20207253886010362,"This asymmetric interact architecture not only achieves more efficient training but also produces better
206"
DUAL DECODERS,0.20293609671848015,"results. Finally, with the decoded token sequence Zd
f, our temporal decoder predicts the invisible
207"
DUAL DECODERS,0.20379965457685664,"tokens of the future frame ˆId
f, operating under the future frame masking map Mf.
208"
DUAL DECODERS,0.20466321243523317,"Multi-task Predictive Learning. As mentioned above, our STP jointly conducts spatiotemporal
209"
DUAL DECODERS,0.20552677029360966,"prediction by asymmetric masking ratio and dual decoders scheme, the whole decoding pipeline can
210"
DUAL DECODERS,0.20639032815198619,"be formally described as follows:
211"
DUAL DECODERS,0.20725388601036268,"(ˆId
c = Φdec_s(Zd
c),
ˆId
f = Φdec_t(Zc, Zd
f).
(2)"
DUAL DECODERS,0.2081174438687392,"Our loss function is the mean squared error (MSE) loss between the normalized masked pixels and
212"
DUAL DECODERS,0.20898100172711573,"the predicted pixels. So our loss function ℓis as follows:
213"
DUAL DECODERS,0.20984455958549222,"ℓ= MSE(ˆIc, Ic) + MSE(ˆIf, If).
(3)"
DOWNSTREAM POLICY LEARNING,0.21070811744386875,"3.4
Downstream Policy Learning
214"
DOWNSTREAM POLICY LEARNING,0.21157167530224524,"To enable data and computation efficiency during the policy learning process, we adopt the paradigm
215"
DOWNSTREAM POLICY LEARNING,0.21243523316062177,"of few-shot behavior cloning by learning from demonstrations (Lfd), and we keep the image encoder
216 Pick Pour"
DOWNSTREAM POLICY LEARNING,0.21329879101899826,"Figure 3: The evaluation demonstrations of our real-world tasks. For picking, the robot arm needs to pick up
the bowl on the desktop. For pouring, the robot arm needs to pour the ingredients from the bowl into the pot."
DOWNSTREAM POLICY LEARNING,0.2141623488773748,"frozen. Concretely, for each task, we are given offline expert demonstrations S = {τ1, ..., τn}, where
217"
DOWNSTREAM POLICY LEARNING,0.21502590673575128,"each τi is a trajectory of robot observations and actions, denoted as τi = [(o0, a0), . . . , (oT , aT )].
218"
DOWNSTREAM POLICY LEARNING,0.2158894645941278,"Based on the S, we train a policy mdoel, πθ(a|C(Φenc(o))), parameterized by θ, which maps from
219"
DOWNSTREAM POLICY LEARNING,0.21675302245250433,"robot’s state representations to actions. Here, C represents an optional concatenation operation that
220"
DOWNSTREAM POLICY LEARNING,0.21761658031088082,"effectively fuses multi-view and multi-frame visual features, along with the robot’s proprioceptive
221"
DOWNSTREAM POLICY LEARNING,0.21848013816925735,"state in the channel dimension. We optimize the πθ through a standard behavior cloning MSE loss:
222"
DOWNSTREAM POLICY LEARNING,0.21934369602763384,"min
θ
P"
DOWNSTREAM POLICY LEARNING,0.22020725388601037,"(o,a)∼SMSE(a, πθ(C(Φenc(o)))).
(4)"
EXPERIMENTS,0.22107081174438686,"4
Experiments
223"
IMPLEMENTATION ON PRE-TRAINING,0.2219343696027634,"4.1
Implementation on Pre-training
224"
IMPLEMENTATION ON PRE-TRAINING,0.22279792746113988,"We execute pre-training with data from EgoVLP [55] for comprehensive ablation and fair comparison.
225"
IMPLEMENTATION ON PRE-TRAINING,0.2236614853195164,"It processes untrimmed videos of Ego4D and filters out that miss language narrations and belong
226"
IMPLEMENTATION ON PRE-TRAINING,0.22452504317789293,"to validation or test sets, resulting in a total of 3.8 million clips, called as Egoclip. In pre-training,
227"
IMPLEMENTATION ON PRE-TRAINING,0.22538860103626943,"we sample a frame pair from each clip for training. As for all experiments, we employ ViT [22] as
228"
IMPLEMENTATION ON PRE-TRAINING,0.22625215889464595,"backbone. Additionally, we maintain consistency with prior works [73, 59], directly using the [CLS]
229"
IMPLEMENTATION ON PRE-TRAINING,0.22711571675302245,"token as the global representation. The pre-training hyperparameters can be found in section A.3.
230"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.22797927461139897,"4.2
Implementation on Downstream Policy
231"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.22884283246977546,"Evaluation Scheme. Following popular settings on PVRs for robotic motor control [65, 46, 59], for
232"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.229706390328152,"each task, we learn a single policy π which is structured as a MLPs network. The policy models
233"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.23056994818652848,"utilize both the history of visual observation embeddings and optional robot proprioceptive as inputs,
234"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.231433506044905,"subsequently generating executable actions as outputs.
235"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.23229706390328153,"Simulation Tasks.
We select the union of manipulation and locomotion tasks from prior
236"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.23316062176165803,"works [65, 59] for evaluation, encompassing 19 tasks across 5 simulated environments. These inclue
237"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.23402417962003455,"Meta-World [104] (Assembly, Bin-Picking, Button-Press, Drawer-Open, and Hammer), Franka-
238"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.23488773747841105,"Kitchen [31] (Sliding Door, Turning Light On, Opening Door, Turning Knob, and Opening Mi-
239"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.23575129533678757,"crowave), Adroit [74] (Relocate and Reorient-Pen), DMControl [83] (Finger-Spin, Reacher-Hard,
240"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.23661485319516407,"Cheetah-Run, Walker-Stand, and Walker-Walk), and Trifinger [98] (Reach-Cube and Push-Cube).
241"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.2374784110535406,"More detailed simulation evaluation details can be found in section A.4.
242"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.23834196891191708,"Real-World Tasks. In our real-world experiments, we evaluate contact-rich picking and pouring
243"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.2392055267702936,"tasks using a Franka Emika Research 3 robot arm in a tabletop environment, ensuring no duplication
244"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.24006908462867013,"with simulation Franka-Kitchen [31]. For each task, we collect 100 noise demonstrations for training,
245"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.24093264248704663,"and we conduct 20 trials per task during evaluation phase. The robotic arm and objects have different
246"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.24179620034542315,"initial pose between training and testing. The evaluation demonstrations of our real-world tasks is
247"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.24265975820379965,"shown in Figure 3. Please see section A.5 for more real-world setup details.
248 MAE"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.24352331606217617,"Assembly
Sliding Door
Reorient-Pen
Reacher-Hard
Walker-Stand STP"
IMPLEMENTATION ON DOWNSTREAM POLICY,0.24438687392055267,"Figure 4: Attention Visualization. We use the [CLS] token as query, average the attention of all heads at the
last layer of the frozen ViT encoder, and perform min-max normalization. We then upsample the attention map
and overlay it on the original image, where the size of the attention value is directly proportional to the intensity
of the yellow light. Top: MAE pre-training. Bottom: STP pre-training."
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2452504317789292,"4.3
Performance on Downstream Simulation Tasks
249"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.24611398963730569,"In this section, we mainly analyze the performance of some pre-trained image representations on
250"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2469775474956822,"reproducible simulation tasks. Specifically, we first evaluate the following models: (1) public
251"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.24784110535405873,"DINOv2 [67] that combines masked image modeling with self-distillation on large-scale image
252"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.24870466321243523,"datasets; (2) public CLIP [71] that conducts contrastive learning on large-scale image-text pairs;
253"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.24956822107081175,"(3) R3M trained based on Egoclip [55]; (4) public VC-1 [59]; (5) MAE trained based on Egoclip;
254"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2504317789291883,"(6) STP trained based on Egoclip. (7) STP that conducts hybrid pre-training with initialization
255"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.25129533678756477,"using ImageNet-MAE [59]. Among them, (1) and (2) achieve excellent performance on core visual
256"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.25215889464594127,"understanding tasks using zero-shot or linear probing evaluation settings. (3) and (4) utilize egocentric
257"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.25302245250431776,"videos for robotic motor control. (5), (6) and (7) are used for fair comparison and exploring the
258"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2538860103626943,"potential benefits of STP from more diverse image data, respectively. The experimental results are
259"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2547495682210708,"presented in Table 1. Consistent with prior findings [41, 59], there is not a universal foundation model
260"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2556131260794473,"that performs optimally across all benchmarks. However, on the whole, the MAE method is superior
261"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.25647668393782386,"due to its effective modeling of low-level geometry and spatial structure, especially for the MetaWorld
262"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.25734024179620035,"tasks that demand fine-grained control. Another intriguing observation is that MAE underperforms
263"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.25820379965457685,"in the Franka-Kitchen and Adroit tasks. We believe that this could be due to its relatively weaker
264"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.25906735751295334,"semantic representation. Under a fair comparison, our STP outperforms MAE by 4.1 (59.6 →63.7),
265"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2599309153713299,"and additionally benefits from a more diverse image data, improving by 0.5 (63.7 →64.2). This is
266"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2607944732297064,"attributed to that our STP not only captures static content features but also effectively models motion
267"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2616580310880829,"information by extracting temporal clues from videos of interactions and manipulations with the
268"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.26252158894645944,"environment and objects. Additionally, we provide the visualization of the attention maps (model (5)
269"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.26338514680483593,"and (6)) of several specific tasks in Figure 4. The results indicate that, on top of effectively capturing
270"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.26424870466321243,"spatial information, our method further encourages the model to focus on motion areas or objects,
271"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2651122625215889,"thereby providing a more sparse and compact representation for downstream low-data BC paradigm.
272"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2659758203799655,"Next, we also evaluate and compare the adaptation results of our representations to downstream motor
273"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.266839378238342,"control tasks. Specifically, we evaluate following settings: (a) The MAE pre-trained representation
274"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.26770293609671847,"undergoes further MAE post-pre-training with task-specific data, and is frozen during policy training;
275"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.26856649395509496,"(b) The STP pre-trained representation undergoes further STP post-pre-training with task-specific
276"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2694300518134715,"data, and is frozen during policy training; (c) The STP pre-trained representation undergoes end-to-
277"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.270293609671848,"end fine-tuning with task-specific data; (d) STP pre-training is performed directly using task-specific
278"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2711571675302245,"data and the resulting representation is frozen during policy training. The results show that end-to-end
279"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.27202072538860106,"fine-tuning fails to yield the best results, suggesting that naively fine-tuning VIT-base could still lead
280"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.27288428324697755,"to overfitting under few-shot behavior cloning scheme. Conversely, (a) and (b) achieve competitive
281"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.27374784110535405,"results, with our STP achieving a 3.9 (72.5 →76.4) improvement on the weight average success rate
282"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.27461139896373055,"than MAE, further demonstrating the effectiveness and data efficiency of our STP for in-domain data.
283"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2754749568221071,"In addition, the comparison between (a) and (d) also proves the effectiveness of pre-training with
284"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2763385146804836,"out-of-domain data. Finally, we also scale up both MAE and our STP to ViT-L/16, and the results
285"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2772020725388601,"still demonstrate the superiority of STP. Among them, compared to ViT-B/16, ViT-L/16 brings a
286"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.27806563039723664,"smaller performance improvement, which may be due to the task’s performance saturation. However,
287"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.27892918825561314,"the ViT-L/16 of STP does not show improvement in Meta-World and Trifinger, indicating that simply
288"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.27979274611398963,"Table 1: Performance comparations of visual representations on simulation benchmarks. We report the average
score across all tasks for each simulation environment. DINOv2 uses ViT-B/14, CLIP uses ViT-B/32, and unless
otherwise specified, others use ViT-B/16. Mt-Wd, Fr-Ki, DMC, Adro, Tr-fi, and WA respectively represent
MetaWorld, Franka-Kitchen, DMControl, Adroit, Trifinger, and weight average. * denotes that public VC-1
samples image frmaes form full Ego4D dataset."
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.28065630397236613,"Pre-training Data
Mt-Wd
Fr-Ki
DMC
Adro
Tr-fi
WA
DINOv2 [67]
LVD-142M
77.9
41.2
59.4
50.7
69.0
59.6
CLIP [71]
Image-text pairs
75.5
39.8
52.2
51.3
57.7
55.6
R3M [65]
Ego
81.3
30.6
52.2
46.7
64.7
54.9
VC-1 [59]
Ego*+MNI
88.8
38.4
60.9
46.0
70.5
61.8
MAE [38]
Ego
85.1
36.7
59.2
43.4
70.6
59.6
STP
Ego
92.0
40.9
62.1
48.0
69.3
63.7
STP
Ego+I
94.1
42.5
61.6
47.3
66.7
64.2
MAE (Post PT)
Ego+Demo
93.6
46.9
81.1
58.0
76.8
72.5
STP (Post PT)
Ego+Demo
97.3
53.6
82.8
63.3
78.0
76.4
STP (E2E FT)
Ego
87.2
52.4
55.2
40.0
70.4
62.9
STP
Demo
70.3
30.4
52.5
38.0
70.8
51.8
MAE-L/16 (Post PT)
Ego+Demo
95.7
54.7
83.5
66.0
77.6
76.7
STP-L/16 (Post PT)
Ego+Demo
97.3
57.4
85.0
70.0
75.4
78.4"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2815198618307427,"Table 2: The ablation experiment results. Me, Fra, DMC, Adr, Tri, and WA respectively represent MetaWorld,
Franka-Kitchen, DMControl, Adroit, Trifinger, and weight average. All models use ViT-B/16."
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2823834196891192,(a) Current Frame Masking and Spatial Prediction.
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.28324697754749567,"ρc
Predict
Me
Fra
DMC
Adr
Tri
WA"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.28411053540587217,"75%
✓
92.0
40.9
62.1
48.0
69.3
63.7
75%
84.5
34.7
55.4
43.3
65.3
57.4
50%
✓
82.1
36.0
60.3
48.0
66.8
59.0
0%
79.2
39.7
54.8
44.0
63.1
57.0"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2849740932642487,(b) Temporal Prediction Condition Design.
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2858376511226252,"Condition
Me
Fra
DMC
Adr
Tri
WA"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.2867012089810017,"L-E
82.1
30.7
55.5
42.0
63.8
55.4
95%
92.0
40.9
62.1
48.0
69.3
63.7
90%
91.2
42.5
62.8
44.7
65.9
63.4
L-E + 95%
91.0
37.7
64.1
46.7
70.8
63.1
L-D + 95%
88.0
34.3
62.6
46.7
69.3
60.9"
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.28756476683937826,(c) Temporal Decoder Architecture Design.
PERFORMANCE ON DOWNSTREAM SIMULATION TASKS,0.28842832469775476,"Decoder
Me
Fra
DMC
Adr
Tri
WA"
JOINT-SELF,0.28929188255613125,"8 joint-self
87.7
36.9
55.7
46.0
71.3
59.8
12 joint-self
88.5
35.0
55.7
46.0
67.0
59.1
8 self-cross
92.0
40.9
62.1
48.0
69.3
63.7"
JOINT-SELF,0.29015544041450775,(d) Frame Sampling Strategy.
JOINT-SELF,0.2910189982728843,"Frame interval
Me
Fra
DMC
Adr
Tri
WA"
JOINT-SELF,0.2918825561312608,"8
89.6
39.9
58.4
46.0
67.0
61.3
16
92.0
40.9
62.1
48.0
69.3
63.7
24
89.1
41.1
61.5
46.0
68.1
62.5
8, 24
92.3
37.1
57.3
42.0
68.4
60.8"
JOINT-SELF,0.2927461139896373,"scaling up model capacity does not necessarily lead to performance gains. In the few-shot BC setting,
289"
JOINT-SELF,0.29360967184801384,"there is a risk of overfitting in both policy and backbone training.
290"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.29447322970639034,"4.4
Ablation on Downstream Simulation Tasks
291"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.29533678756476683,"In this section, we perform extensive ablation studies to further demonstrate the effectiveness of our
292"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.29620034542314333,"joint spatial and temporal prediction, as well as temporal prediction condition design. In addition, we
293"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.2970639032815199,"also study the influence of temporal decoder architecture design and future frame sampling strategy.
294"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.2979274611398964,"Current frame masking. The design of the current frame masking is crucial. On one hand, similar
295"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.2987910189982729,"to MAE [38], masking some patches and predicting the missing parts can effectively promote the
296"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.29965457685664937,"learning of image content features. On the other hand, the visible patches of the current frame need
297"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3005181347150259,"to interact with the condition to predict the future frame. Specifically, we mask the current frame at
298"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3013816925734024,"masking rates of 75%, 50%, and 0%, respectively, and optionally predict the missing parts through
299"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3022452504317789,"the spatial decoder. The results are shown in Table 2 (a). From results, we see that the masking ratio
300"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.30310880829015546,"of 75% and performing spatial prediction still lead to the best performance. This demonstrates the
301"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.30397236614853196,"importance of retaining MAE [38] for content features learning, especially for low-level manipulation
302"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.30483592400690845,"in Meta-World, while a current frame with a high masking ratio (75%) is sufficient to interact with
303"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.30569948186528495,"other conditions to predict the future frame.
304"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3065630397236615,"Temporal prediction condition design. Subsequently, we discuss the influence of temporal predic-
305"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.307426597582038,"tion condition design. We implicitly model motion in actionless video data by predicting the pixels of
306"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3082901554404145,"the future frame. A direct and simple idea is to use language narration as a condition. The text tokens
307"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.30915371329879104,"can be flexibly utilized as inputs to ViT [22], forming a multimodal encoder. Language narration
308"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.31001727115716754,"provides a high-level behavior description, but lacks low-level visual dynamic priors for pixel-level
309"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.31088082901554404,"prediction. However, leaking part of the future frame can effectively provide these priors. In order
310"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.31174438687392053,"to explore how to construct a more meaningful temporal prediction proxy task, we compare the
311"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3126079447322971,"following schemes: (1) only language narration, (2) masking 95% of the future frame, (3) masking
312"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3134715025906736,"90% of the future frame, (4) masking 95% of the future frame and language narration, and (5) masking
313"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3143350604490501,"95% of the future frame and language narration, but the language is added in the temporal decoder,
314"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.31519861830742657,"instead of being fused with the visible image patches in the multimodal encoder. We tokenize all
315"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3160621761658031,"language narration by pre-trained DistilBERT [75]. The results are shown in Table 2 (b). From
316"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3169257340241796,"results, we see that using only language as a prediction condition leads to a significant decline in
317"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3177892918825561,"performance, while leaking a small amount of future frame (masking 95%) in the temporal decoder
318"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.31865284974093266,"can achieve competitive results. As for joint conditions of language and future frame with 95%
319"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.31951640759930916,"masking ratio, adding language in the encoder is better than in the decoder. Additionally, adding
320"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.32037996545768566,"language performs better on DMControl (64.1 vs. 62.1) and Trifinger (70.8 vs. 69.3), while not
321"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.32124352331606215,"adding language performs better on Meta-World (92.0 vs. 91.0), Franka-Kitchen (40.9 vs. 37.7) and
322"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3221070811744387,"Adroit (48.0 vs. 46.7). We speculate the reasons for language hurts performance are as follows: (i)
323"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3229706390328152,"The input gap (multi-modal and single-modal) between upstream and downstream; (ii) Extra language
324"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3238341968911917,"in ViT may result in the loss of some fine-grained information capture. Furthermore, the latter does
325"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.32469775474956825,"not require language supervision, and can provide a more scalable self-supervised solution.
326"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.32556131260794474,"Temporal decoder design. We also investigate the impact of the temporal decoder design. Specifi-
327"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.32642487046632124,"cally, we consider two types of decoder blocks. One is the joint-self architecture, as shown in Figure 2
328"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.32728842832469773,"(a), and similar joint architecture are adopted in [26, 102]. The other is the self-cross architecture, as
329"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3281519861830743,"shown in Figure 2 (b), and similar cross architecture are adopted in [3, 33]. We consider the following
330"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3290155440414508,"settings: (1) 8 joint-self decoder blocks, (2) 12 joint-self decoder blocks, (3) 8 self-cross decoder
331"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3298791018998273,"blocks. Among them, setting (2) and (3) have similar amounts of parameters for a fairer comparison.
332"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.33074265975820377,"The results are shown in Table 2 (c). The results demonstrate the importance of maintaining a fixed
333"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3316062176165803,"representation space of the past frame during temporal prediction.
334"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3324697754749568,"Frame sampling strategy. Finally, we investigate the impact of the sampling strategy between the
335"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.3333333333333333,"current frame and future frame. The difficulty of temporal prediction is directly proportional to the
336"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.33419689119170987,"frame interval values. We establish four settings where we fix the sampling intervals at 8, 16, and 24
337"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.33506044905008636,"respectively, and for the fourth setting, we randomly select an interval within the range of [8, 24].
338"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.33592400690846286,"The results are shown in Table 2 (d). The results show that an interval of 16 achieves the best balance
339"
ABLATION ON DOWNSTREAM SIMULATION TASKS,0.33678756476683935,"for building temporal prediction proxy task.
340"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.3376511226252159,"4.5
Performance on Downstream Real-world Tasks
341"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.3385146804835924,"Table 3: Performance comparations on real-world
tasks."
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.3393782383419689,"Method
Picking
Pouring
Average"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.34024179620034545,"MAE
65.0
45.0
55.0
STP
65.0
65.0
65.0"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.34110535405872194,"In this section, we report our experiment results on
342"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.34196891191709844,"real-world picking and pouring tasks. We report the
343"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.34283246977547494,"average success rate for each task. Specifically, we
344"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.3436960276338515,"compare STP with the baseline MAE, both of which
345"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.344559585492228,"are trained on out-of-domain videos and kept frozen
346"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.3454231433506045,"during policy training. The results are shown in Ta-
347"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.34628670120898103,"ble 3. From the results, it can be seen that STP has
348"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.3471502590673575,"achieved significant advantages in the pouring task.
349"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.348013816925734,"It can more accurately align with the moving bowl
350"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.3488773747841105,"and the pot. In addition, although MAE and STP have a same success rate in picking tasks, STP tends
351"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.34974093264248707,"to execute grasping in a better position. This indicates that the trend and conclusion of our STP are
352"
PERFORMANCE ON DOWNSTREAM REAL-WORLD TASKS,0.35060449050086356,"consistent in both simulation and the real-world, which also aligns with the findings of [79].
353"
CONCLUSION,0.35146804835924006,"5
Conclusion
354"
CONCLUSION,0.35233160621761656,"In this work, we have proposed the STP, a simple, efficient and effective self-supervised visual repre-
355"
CONCLUSION,0.3531951640759931,"sentation pre-training framework for robotic motor control. Our STP jointly performs spatiotemporal
356"
CONCLUSION,0.3540587219343696,"predictive learning on large-scale videos within a multi-task learning manner. Our STP captures
357"
CONCLUSION,0.3549222797927461,"content features by predicting the invisible areas within the masked current frame, and simultaneously
358"
CONCLUSION,0.35578583765112265,"captures motion features by using a future frame with an extremely high masking ratio as a condition
359"
CONCLUSION,0.35664939550949915,"to predict the invisible areas within that future frame. We carry out the largest-scale BC evaluation of
360"
CONCLUSION,0.35751295336787564,"PVRs for robotic motor control to date to demonstrate the effectiveness of STP. Furthermore, as for
361"
CONCLUSION,0.35837651122625214,"pre-training data, we also prove that extending STP to hybrid pre-training and post-pre-training could
362"
CONCLUSION,0.3592400690846287,"further unleash its generality and data efficiency.
363"
REFERENCES,0.3601036269430052,"References
364"
REFERENCES,0.3609671848013817,"[1] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob
365"
REFERENCES,0.36183074265975823,"McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al.
366"
REFERENCES,0.3626943005181347,"Learning dexterous in-hand manipulation. IJRR, 39(1):3–20, 2020.
367"
REFERENCES,0.3635578583765112,"[2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael
368"
REFERENCES,0.3644214162348877,"Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a
369"
REFERENCES,0.36528497409326427,"joint-embedding predictive architecture. In CVPR, pages 15619–15629, 2023.
370"
REFERENCES,0.36614853195164077,"[3] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal
371"
REFERENCES,0.36701208981001726,"multi-task masked autoencoders. In ECCV, pages 348–367. Springer, 2022.
372"
REFERENCES,0.36787564766839376,"[4] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli.
373"
REFERENCES,0.3687392055267703,"Data2vec: A general framework for self-supervised learning in speech, vision and language.
374"
REFERENCES,0.3696027633851468,"In ICML, pages 1298–1312. PMLR, 2022.
375"
REFERENCES,0.3704663212435233,"[5] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak. Human-to-robot imitation in the wild. In
376"
REFERENCES,0.37132987910189985,"Kris Hauser, Dylan A. Shell, and Shoudong Huang, editors, RSS, 2022.
377"
REFERENCES,0.37219343696027635,"[6] Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances
378"
REFERENCES,0.37305699481865284,"from human videos as a versatile representation for robotics. In CVPR, pages 13778–13790,
379"
REFERENCES,0.37392055267702934,"2023.
380"
REFERENCES,0.3747841105354059,"[7] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image
381"
REFERENCES,0.3756476683937824,"transformers. In ICLR, 2021.
382"
REFERENCES,0.3765112262521589,"[8] Adrien Bardes, Jean Ponce, and Yann LeCun.
Mc-jepa: A joint-embedding predictive
383"
REFERENCES,0.37737478411053543,"architecture for self-supervised learning of motion and content features. arXiv preprint
384"
REFERENCES,0.37823834196891193,"arXiv:2307.12698, 2023.
385"
REFERENCES,0.3791018998272884,"[9] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X Lee, Maria
386"
REFERENCES,0.3799654576856649,"Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, et al. Robocat: A self-
387"
REFERENCES,0.38082901554404147,"improving foundation agent for robotic manipulation. arXiv preprint arXiv:2306.11706,
388"
REFERENCES,0.38169257340241797,"2023.
389"
REFERENCES,0.38255613126079446,"[10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof
390"
REFERENCES,0.38341968911917096,"Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-
391"
REFERENCES,0.3842832469775475,"language-action models transfer web knowledge to robotic control. In CoRL, 2023.
392"
REFERENCES,0.385146804835924,"[11] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea
393"
REFERENCES,0.3860103626943005,"Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian
394"
REFERENCES,0.38687392055267705,"Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian,
395"
REFERENCES,0.38773747841105355,"Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu,
396"
REFERENCES,0.38860103626943004,"Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn
397"
REFERENCES,0.38946459412780654,"Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia
398"
REFERENCES,0.3903281519861831,"Salazar, Pannag R. Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone,
399"
REFERENCES,0.3911917098445596,"Clayton Tan, Huong T. Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted
400"
REFERENCES,0.3920552677029361,"Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: robotics transformer for
401"
REFERENCES,0.39291882556131263,"real-world control at scale. In Kostas E. Bekris, Kris Hauser, Sylvia L. Herbert, and Jingjin
402"
REFERENCES,0.39378238341968913,"Yu, editors, RSS, 2023.
403"
REFERENCES,0.3946459412780656,"[12] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task
404"
REFERENCES,0.3955094991364421,"control through goal-aware representation learning and adaptive horizon prediction. In CVPR,
405"
REFERENCES,0.3963730569948187,"pages 13734–13744, 2023.
406"
REFERENCES,0.39723661485319517,"[13] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,
407"
REFERENCES,0.39810017271157166,"and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV,
408"
REFERENCES,0.39896373056994816,"pages 9650–9660, 2021.
409"
REFERENCES,0.3998272884283247,"[14] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
410"
REFERENCES,0.4006908462867012,"for contrastive learning of visual representations. In ICML, pages 1597–1607. PMLR, 2020.
411"
REFERENCES,0.4015544041450777,"[15] X Chen, S Xie, and K He. An empirical study of training self-supervised vision transformers.
412"
REFERENCES,0.40241796200345425,"in 2021 ieee. In ICCV, pages 9620–9629.
413"
REFERENCES,0.40328151986183075,"[16] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and
414"
REFERENCES,0.40414507772020725,"Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Kostas E.
415"
REFERENCES,0.40500863557858374,"Bekris, Kris Hauser, Sylvia L. Herbert, and Jingjin Yu, editors, RSS, 2023.
416"
REFERENCES,0.4058721934369603,"[17] Yutao Cui, Cheng Jiang, Gangshan Wu, and Limin Wang. Mixformer: End-to-end tracking
417"
REFERENCES,0.4067357512953368,"with iterative mixed attention. arXiv preprint arXiv:2302.02814, 2023.
418"
REFERENCES,0.4075993091537133,"[18] Yilun Dai, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuur-
419"
REFERENCES,0.40846286701208984,"mans, and Pieter Abbeel. Learning universal policies via text-guided video generation. arXiv
420"
REFERENCES,0.40932642487046633,"preprint arXiv:2302.00111, 2023.
421"
REFERENCES,0.41018998272884283,"[19] Sudeep Dasari, Mohan Kumar Srirama, Unnat Jain, and Abhinav Gupta. An unbiased look at
422"
REFERENCES,0.4110535405872193,"datasets for visuo-motor pre-training. In CoRL, 2023.
423"
REFERENCES,0.4119170984455959,"[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
424"
REFERENCES,0.41278065630397237,"hierarchical image database. In CVPR, pages 248–255. Ieee, 2009.
425"
REFERENCES,0.41364421416234887,"[21] AM Derrington and P Lennie. Spatial and temporal contrast sensitivities of neurones in lateral
426"
REFERENCES,0.41450777202072536,"geniculate nucleus of macaque. The Journal of physiology, 357(1):219–240, 1984.
427"
REFERENCES,0.4153713298791019,"[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
428"
REFERENCES,0.4162348877374784,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
429"
REFERENCES,0.4170984455958549,"Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
430"
REFERENCES,0.41796200345423146,"recognition at scale. In ICLR.
431"
REFERENCES,0.41882556131260795,"[23] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet,
432"
REFERENCES,0.41968911917098445,"Tianhe Yu, Pieter Abbeel, Joshua B Tenenbaum, et al. Video language planning. arXiv preprint
433"
REFERENCES,0.42055267702936094,"arXiv:2310.10625, 2023.
434"
REFERENCES,0.4214162348877375,"[24] Hao-Shu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu, Chenxi Wang, Junbo Wang, Haoyi
435"
REFERENCES,0.422279792746114,"Zhu, and Cewu Lu. Rh20t: A comprehensive robotic dataset for learning diverse skills in
436"
REFERENCES,0.4231433506044905,"one-shot. In Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@
437"
REFERENCES,0.42400690846286704,"CoRL2023, 2023.
438"
REFERENCES,0.42487046632124353,"[25] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang,
439"
REFERENCES,0.42573402417962003,"Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation
440"
REFERENCES,0.4265975820379965,"learning at scale. In CVPR, pages 19358–19369, 2023.
441"
REFERENCES,0.4274611398963731,"[26] Xinyang Geng, Hao Liu, Lisa Lee, Dale Schuurams, Sergey Levine, and Pieter Abbeel.
442"
REFERENCES,0.4283246977547496,"Multimodal masked autoencoders learn transferable representations.
arXiv preprint
443"
REFERENCES,0.42918825561312607,"arXiv:2205.14204, 2022.
444"
REFERENCES,0.43005181347150256,"[27] Abraham George, Alison Bartsch, and Amir Barati Farimani. Openvr: Teleoperation for
445"
REFERENCES,0.4309153713298791,"manipulation. arXiv preprint arXiv:2305.09765, 2023.
446"
REFERENCES,0.4317789291882556,"[28] Rohit Girdhar and Kristen Grauman. Anticipative video transformer. In ICCV, pages 13505–
447"
REFERENCES,0.4326424870466321,"13515, 2021.
448"
REFERENCES,0.43350604490500866,"[29] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit
449"
REFERENCES,0.43436960276338515,"Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the
450"
REFERENCES,0.43523316062176165,"world in 3,000 hours of egocentric video. In CVPR, pages 18995–19012, 2022.
451"
REFERENCES,0.43609671848013815,"[30] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Tri-
452"
REFERENCES,0.4369602763385147,"antafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al.
453"
REFERENCES,0.4378238341968912,"Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives.
454"
REFERENCES,0.4386873920552677,"arXiv preprint arXiv:2311.18259, 2023.
455"
REFERENCES,0.43955094991364424,"[31] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay
456"
REFERENCES,0.44041450777202074,"policy learning: Solving long-horizon tasks via imitation and reinforcement learning. In CoRL,
457"
REFERENCES,0.44127806563039723,"pages 1025–1037. PMLR, 2020.
458"
REFERENCES,0.4421416234887737,"[32] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Martín-Martín, and Li Fei-Fei.
459"
REFERENCES,0.4430051813471503,"Maskvit: Masked visual pre-training for video prediction. In ICLR, 2022.
460"
REFERENCES,0.4438687392055268,"[33] Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese masked autoencoders. 2023.
461"
REFERENCES,0.44473229706390327,"[34] Huy Ha, Pete Florence, and Shuran Song. Scaling up and distilling down: Language-guided
462"
REFERENCES,0.44559585492227977,"robot skill acquisition. In Conference on Robot Learning, pages 3766–3777. PMLR, 2023.
463"
REFERENCES,0.4464594127806563,"[35] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense
464"
REFERENCES,0.4473229706390328,"predictive coding. In ICCV Workshops, pages 0–0, 2019.
465"
REFERENCES,0.4481865284974093,"[36] Nicklas Hansen, Zhecheng Yuan, Yanjie Ze, Tongzhou Mu, Aravind Rajeswaran, Hao Su,
466"
REFERENCES,0.44905008635578586,"Huazhe Xu, and Xiaolong Wang. On pre-training for visuo-motor control: Revisiting a
467"
REFERENCES,0.44991364421416236,"learning-from-scratch baseline. arXiv preprint arXiv:2212.05749, 2022.
468"
REFERENCES,0.45077720207253885,"[37] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood.
469"
REFERENCES,0.45164075993091535,"Flexible diffusion modeling of long videos. NeurIPS, 35:27953–27965, 2022.
470"
REFERENCES,0.4525043177892919,"[38] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
471"
REFERENCES,0.4533678756476684,"autoencoders are scalable vision learners. In CVPR, pages 16000–16009, 2022.
472"
REFERENCES,0.4542314335060449,"[39] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
473"
REFERENCES,0.45509499136442144,"unsupervised visual representation learning. In CVPR, pages 9729–9738, 2020.
474"
REFERENCES,0.45595854922279794,"[40] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
475"
REFERENCES,0.45682210708117443,"recognition. In CVPR, pages 770–778, 2016.
476"
REFERENCES,0.45768566493955093,"[41] Yingdong Hu, Renhao Wang, Li Erran Li, and Yang Gao. For pre-trained vision models in mo-
477"
REFERENCES,0.4585492227979275,"tor control, not all policy learning methods are created equal. arXiv preprint arXiv:2304.04591,
478"
REFERENCES,0.459412780656304,"2023.
479"
REFERENCES,0.46027633851468047,"[42] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey
480"
REFERENCES,0.46113989637305697,"Levine, and Chelsea Finn. BC-Z: zero-shot task generalization with robotic imitation learning.
481"
REFERENCES,0.4620034542314335,"In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors, CoRL, volume 164 of
482"
REFERENCES,0.46286701208981,"Proceedings of Machine Learning Research, pages 991–1002. PMLR, 2021.
483"
REFERENCES,0.4637305699481865,"[43] Chuhao Jin, Wenhui Tan, Jiange Yang, Bei Liu, Ruihua Song, Limin Wang, and Jianlong Fu.
484"
REFERENCES,0.46459412780656306,"Alphablock: Embodied finetuning for vision-language reasoning in robot manipulation. arXiv
485"
REFERENCES,0.46545768566493956,"preprint arXiv:2305.18898, 2023.
486"
REFERENCES,0.46632124352331605,"[44] Ya Jing, Xuelin Zhu, Xingbin Liu, Qie Sima, Taozheng Yang, Yunhai Feng, and Tao Kong.
487"
REFERENCES,0.46718480138169255,"Exploring visual pre-training for robot manipulation: Datasets, models and methods. arXiv
488"
REFERENCES,0.4680483592400691,"preprint arXiv:2308.03620, 2023.
489"
REFERENCES,0.4689119170984456,"[45] Yuanchen Ju, Kaizhe Hu, Guowei Zhang, Gu Zhang, Mingrun Jiang, and Huazhe Xu. Robo-
490"
REFERENCES,0.4697754749568221,"abc: Affordance generalization beyond categories via semantic correspondence for robot
491"
REFERENCES,0.47063903281519864,"manipulation. arXiv preprint arXiv:2401.07487, 2024.
492"
REFERENCES,0.47150259067357514,"[46] Siddharth Karamcheti, Suraj Nair, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh,
493"
REFERENCES,0.47236614853195164,"and Percy Liang. Language-driven representation learning for robotics. In Kostas E. Bekris,
494"
REFERENCES,0.47322970639032813,"Kris Hauser, Sylvia L. Herbert, and Jingjin Yu, editors, RSS, 2023.
495"
REFERENCES,0.4740932642487047,"[47] Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Multi-task robot data for dual-arm
496"
REFERENCES,0.4749568221070812,"fine manipulation. arXiv preprint arXiv:2401.07603, 2024.
497"
REFERENCES,0.4758203799654577,"[48] Moo Jin Kim, Jiajun Wu, and Chelsea Finn. Giving robots a hand: Learning generalizable
498"
REFERENCES,0.47668393782383417,"manipulation with eye-in-hand human video demonstrations. arXiv preprint arXiv:2307.05959,
499"
REFERENCES,0.4775474956822107,"2023.
500"
REFERENCES,0.4784110535405872,"[49] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
501"
REFERENCES,0.4792746113989637,"preprint arXiv:1412.6980, 2014.
502"
REFERENCES,0.48013816925734026,"[50] Andreas Kleinschmidt, Kai V Thilo, Christian Büchel, Michael A Gresty, Adolfo M Bronstein,
503"
REFERENCES,0.48100172711571676,"and Richard SJ Frackowiak. Neural correlates of visual-motion perception as object-or self-
504"
REFERENCES,0.48186528497409326,"motion. Neuroimage, 16(4):873–882, 2002.
505"
REFERENCES,0.48272884283246975,"[51] Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and Joshua B Tenenbaum. Learning to
506"
REFERENCES,0.4835924006908463,"act from actionless videos through dense correspondences. arXiv preprint arXiv:2310.08576,
507"
REFERENCES,0.4844559585492228,"2023.
508"
REFERENCES,0.4853195164075993,"[52] Xiangwen Kong and Xiangyu Zhang. Understanding masked image modeling via learning
509"
REFERENCES,0.48618307426597585,"occlusion invariant feature. In CVPR, pages 6241–6251, 2023.
510"
REFERENCES,0.48704663212435234,"[53] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep
511"
REFERENCES,0.48791018998272884,"visuomotor policies. JMLR, 17(1):1334–1373, 2016.
512"
REFERENCES,0.48877374784110533,"[54] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang,
513"
REFERENCES,0.4896373056994819,"Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective
514"
REFERENCES,0.4905008635578584,"robot imitators. arXiv preprint arXiv:2311.01378, 2023.
515"
REFERENCES,0.4913644214162349,"[55] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Z XU,
516"
REFERENCES,0.49222797927461137,"Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric video-language
517"
REFERENCES,0.4930915371329879,"pretraining. NeurIPS, 35:7575–7586, 2022.
518"
REFERENCES,0.4939550949913644,"[56] Xingyu Lin, John So, Sashwat Mahalingam, Fangchen Liu, and Pieter Abbeel. Spawn-
519"
REFERENCES,0.4948186528497409,"net: Learning generalizable visuomotor skills from pre-trained networks. arXiv preprint
520"
REFERENCES,0.49568221070811747,"arXiv:2307.03567, 2023.
521"
REFERENCES,0.49654576856649396,"[57] Yecheng Jason Ma, William Liang, Vaidehi Som, Vikash Kumar, Amy Zhang, Osbert Bastani,
522"
REFERENCES,0.49740932642487046,"and Dinesh Jayaraman. Liv: Language-image representations and rewards for robotic control.
523"
REFERENCES,0.49827288428324695,"arXiv preprint arXiv:2306.00958, 2023.
524"
REFERENCES,0.4991364421416235,"[58] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and
525"
REFERENCES,0.5,"Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit
526"
REFERENCES,0.5008635578583766,"pre-training. In ICLR, 2023.
527"
REFERENCES,0.501727115716753,"[59] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Yecheng Jason Ma, Claire Chen, Sneha
528"
REFERENCES,0.5025906735751295,"Silwal, Aryan Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra Malik, et al. Where are we
529"
REFERENCES,0.5034542314335061,"in the search for an artificial visual cortex for embodied intelligence? 2023.
530"
REFERENCES,0.5043177892918825,"[60] Pierre Marza, Laetitia Matignon, Olivier Simonin, and Christian Wolf. Task-conditioned
531"
REFERENCES,0.5051813471502591,"adaptation of visual features in multi-task policy learning. arXiv preprint arXiv:2402.07739,
532"
REFERENCES,0.5060449050086355,"2024.
533"
REFERENCES,0.5069084628670121,"[61] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction
534"
REFERENCES,0.5077720207253886,"beyond mean square error. arXiv preprint arXiv:1511.05440, 2015.
535"
REFERENCES,0.5086355785837651,"[62] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human
536"
REFERENCES,0.5094991364421416,"videos. In Kostas E. Bekris, Kris Hauser, Sylvia L. Herbert, and Jingjin Yu, editors, RSS, 2023.
537"
REFERENCES,0.5103626943005182,"[63] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and
538"
REFERENCES,0.5112262521588946,"Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million
539"
REFERENCES,0.5120898100172712,"narrated video clips. In CVPR, pages 2630–2640, 2019.
540"
REFERENCES,0.5129533678756477,"[64] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang,
541"
REFERENCES,0.5138169257340242,"Jifeng Dai, Yu Qiao, and Ping Luo. EmbodiedGPT: Vision-language pre-training via embodied
542"
REFERENCES,0.5146804835924007,"chain of thought. In NeurIPS, 2023.
543"
REFERENCES,0.5155440414507773,"[65] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A
544"
REFERENCES,0.5164075993091537,"universal visual representation for robot manipulation. In CoRL, 2022.
545"
REFERENCES,0.5172711571675302,"[66] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive
546"
REFERENCES,0.5181347150259067,"predictive coding. arXiv preprint arXiv:1807.03748, 2018.
547"
REFERENCES,0.5189982728842832,"[67] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
548"
REFERENCES,0.5198618307426598,"Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2:
549"
REFERENCES,0.5207253886010362,"Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.
550"
REFERENCES,0.5215889464594128,"[68] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan,
551"
REFERENCES,0.5224525043177893,"Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment:
552"
REFERENCES,0.5233160621761658,"Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023.
553"
REFERENCES,0.5241796200345423,"[69] Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsur-
554"
REFERENCES,0.5250431778929189,"prising effectiveness of pre-trained vision models for control. In ICML, pages 17359–17371.
555"
REFERENCES,0.5259067357512953,"PMLR, 2022.
556"
REFERENCES,0.5267702936096719,"[70] Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Hanwen Jiang, Ruihan Yang, Yang Fu, and Xiaolong
557"
REFERENCES,0.5276338514680483,"Wang. Dexmv: Imitation learning for dexterous manipulation from human videos. In ECCV,
558"
REFERENCES,0.5284974093264249,"pages 570–587. Springer, 2022.
559"
REFERENCES,0.5293609671848014,"[71] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
560"
REFERENCES,0.5302245250431779,"Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
561"
REFERENCES,0.5310880829015544,"models from natural language supervision. In ICLR, pages 8748–8763. PMLR, 2021.
562"
REFERENCES,0.531951640759931,"[72] Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, and Jitendra Malik.
563"
REFERENCES,0.5328151986183074,"Robot learning with sensorimotor pre-training. 2023.
564"
REFERENCES,0.533678756476684,"[73] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell.
565"
REFERENCES,0.5345423143350605,"Real-world robot learning with masked visual pre-training. In CoRL, pages 416–426. PMLR,
566"
REFERENCES,0.5354058721934369,"2023.
567"
REFERENCES,0.5362694300518135,"[74] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman,
568"
REFERENCES,0.5371329879101899,"Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep
569"
REFERENCES,0.5379965457685665,"reinforcement learning and demonstrations. RSS, 2018.
570"
REFERENCES,0.538860103626943,"[75] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled
571"
REFERENCES,0.5397236614853195,"version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
572"
REFERENCES,0.540587219343696,"[76] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter
573"
REFERENCES,0.5414507772020726,"Abbeel. Masked world models for visual control. In CoRL, pages 1332–1344. PMLR, 2023.
574"
REFERENCES,0.542314335060449,"[77] Younggyo Seo, Kimin Lee, Stephen L James, and Pieter Abbeel. Reinforcement learning with
575"
REFERENCES,0.5431778929188256,"action-free pre-training from videos. In ICML, pages 19561–19579. PMLR, 2022.
576"
REFERENCES,0.5440414507772021,"[78] Mohit Sharma, Claudio Fantacci, Yuxiang Zhou, Skanda Koppula, Nicolas Heess, Jon Scholz,
577"
REFERENCES,0.5449050086355786,"and Yusuf Aytar. Lossless adaptation of pretrained vision models for robotic manipulation. In
578"
REFERENCES,0.5457685664939551,"ICLR, 2023.
579"
REFERENCES,0.5466321243523317,"[79] Sneha Silwal, Karmesh Yadav, Tingfan Wu, Jay Vakil, Arjun Majumdar, Sergio Arnaud, Claire
580"
REFERENCES,0.5474956822107081,"Chen, Vincent-Pierre Berges, Dhruv Batra, Aravind Rajeswaran, et al. What do we learn from
581"
REFERENCES,0.5483592400690847,"a large-scale study of pre-trained visual representations in sim and real environments? arXiv
582"
REFERENCES,0.5492227979274611,"preprint arXiv:2310.02219, 2023.
583"
REFERENCES,0.5500863557858376,"[80] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of
584"
REFERENCES,0.5509499136442142,"video representations using lstms. In ICML, pages 843–852. PMLR, 2015.
585"
REFERENCES,0.5518134715025906,"[81] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong,
586"
REFERENCES,0.5526770293609672,"Paul Wohlhart, Brianna Zitkovich, Fei Xia, Chelsea Finn, et al. Open-world object manipula-
587"
REFERENCES,0.5535405872193437,"tion using pre-trained vision-language models. In CoRL, 2023.
588"
REFERENCES,0.5544041450777202,"[82] Yanchao Sun, Shuang Ma, Ratnesh Madaan, Rogerio Bonatti, Furong Huang, and Ashish
589"
REFERENCES,0.5552677029360967,"Kapoor. Smart: Self-supervised multi-task pretraining with control transformers. In ICLR,
590"
REFERENCES,0.5561312607944733,"2023.
591"
REFERENCES,0.5569948186528497,"[83] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David
592"
REFERENCES,0.5578583765112263,"Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite.
593"
REFERENCES,0.5587219343696027,"arXiv preprint arXiv:1801.00690, 2018.
594"
REFERENCES,0.5595854922279793,"[84] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep
595"
REFERENCES,0.5604490500863558,"Dasari, Joey Hejna, Charles Xu, Jianlan Luo, et al. Octo: An open-source generalist robot
596"
REFERENCES,0.5613126079447323,"policy, 2023.
597"
REFERENCES,0.5621761658031088,"[85] Garrett Thomas, Ching-An Cheng, Ricky Loynd, Felipe Vieira Frujeri, Vibhav Vineet, Mihai
598"
REFERENCES,0.5630397236614854,"Jalobeanu, and Andrey Kolobov. Plex: Making the most of the available data for robotic
599"
REFERENCES,0.5639032815198618,"manipulation pretraining. In CoRL, pages 2624–2641. PMLR, 2023.
600"
REFERENCES,0.5647668393782384,"[86] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are
601"
REFERENCES,0.5656303972366149,"data-efficient learners for self-supervised video pre-training. NeurIPS, 35:10078–10093, 2022.
602"
REFERENCES,0.5664939550949913,"[87] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
603"
REFERENCES,0.5673575129533679,"Hervé Jégou. Training data-efficient image transformers & distillation through attention. In
604"
REFERENCES,0.5682210708117443,"ICLR, pages 10347–10357. PMLR, 2021.
605"
REFERENCES,0.5690846286701209,"[88] David C Van Essen and Jack L Gallant. Neural mechanisms of form and motion processing in
606"
REFERENCES,0.5699481865284974,"the primate visual system. Neuron, 13(1):1–10, 1994.
607"
REFERENCES,0.5708117443868739,"[89] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
608"
REFERENCES,0.5716753022452504,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017.
609"
REFERENCES,0.572538860103627,"[90] Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe
610"
REFERENCES,0.5734024179620034,"Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2:
611"
REFERENCES,0.57426597582038,"A dataset for robot learning at scale. In Conference on Robot Learning, pages 1723–1736.
612"
REFERENCES,0.5751295336787565,"PMLR, 2023.
613"
REFERENCES,0.575993091537133,"[91] Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and
614"
REFERENCES,0.5768566493955095,"Anima Anandkumar. Mimicplay: Long-horizon imitation learning by watching human play.
615"
REFERENCES,0.5777202072538861,"arXiv preprint arXiv:2302.12422, 2023.
616"
REFERENCES,0.5785837651122625,"[92] Jianren Wang, Sudeep Dasari, Mohan Kumar Srirama, Shubham Tulsiani, and Abhinav Gupta.
617"
REFERENCES,0.5794473229706391,"Manipulate by seeing: Creating manipulation controllers from pre-trained representations. In
618"
REFERENCES,0.5803108808290155,"ICCV, pages 3859–3868, 2023.
619"
REFERENCES,0.581174438687392,"[93] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and
620"
REFERENCES,0.5820379965457686,"Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In CVPR,
621"
REFERENCES,0.582901554404145,"pages 14549–14560, 2023.
622"
REFERENCES,0.5837651122625216,"[94] Lirui Wang, Jialiang Zhao, Yilun Du, Edward H Adelson, and Russ Tedrake. Poco: Policy
623"
REFERENCES,0.5846286701208981,"composition from and for heterogeneous robot learning. arXiv preprint arXiv:2402.02511,
624"
REFERENCES,0.5854922279792746,"2024.
625"
REFERENCES,0.5863557858376511,"[95] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer.
626"
REFERENCES,0.5872193436960277,"Masked feature prediction for self-supervised visual pre-training. In CVPR, pages 14668–
627"
REFERENCES,0.5880829015544041,"14678, 2022.
628"
REFERENCES,0.5889464594127807,"[96] William F Whitney, Rajat Agarwal, Kyunghyun Cho, and Abhinav Gupta. Dynamics-aware
629"
REFERENCES,0.5898100172711571,"embeddings. In ICLR, 2020.
630"
REFERENCES,0.5906735751295337,"[97] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan
631"
REFERENCES,0.5915371329879102,"Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual
632"
REFERENCES,0.5924006908462867,"robot manipulation. arXiv preprint arXiv:2312.13139, 2023.
633"
REFERENCES,0.5932642487046632,"[98] Manuel Wuthrich, Felix Widmaier, Felix Grimminger, Shruti Joshi, Vaibhav Agrawal, Bi-
634"
REFERENCES,0.5941278065630398,"lal Hammoud, Majid Khadiv, Miroslav Bogdanovic, Vincent Berenz, Julian Viereck, et al.
635"
REFERENCES,0.5949913644214162,"Trifinger: An open-source robot for learning dexterity. In CoRL, pages 1871–1882. PMLR,
636"
REFERENCES,0.5958549222797928,"2021.
637"
REFERENCES,0.5967184801381693,"[99] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training
638"
REFERENCES,0.5975820379965457,"for motor control. arXiv preprint arXiv:2203.06173, 2022.
639"
REFERENCES,0.5984455958549223,"[100] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han
640"
REFERENCES,0.5993091537132987,"Hu. Simmim: A simple framework for masked image modeling. In CVPR, pages 9653–9663,
641"
REFERENCES,0.6001727115716753,"2022.
642"
REFERENCES,0.6010362694300518,"[101] Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, and Shuran Song. Xskill: Cross
643"
REFERENCES,0.6018998272884283,"embodiment skill discovery. In CoRL, 2023.
644"
REFERENCES,0.6027633851468048,"[102] Jiange Yang, Sheng Guo, Gangshan Wu, and Limin Wang. Comae: Single model hybrid
645"
REFERENCES,0.6036269430051814,"pre-training on small-scale rgb-d datasets. arXiv preprint arXiv:2302.06148, 2023.
646"
REFERENCES,0.6044905008635578,"[103] Jiange Yang, Wenhui Tan, Chuhao Jin, Bei Liu, Jianlong Fu, Ruihua Song, and Limin Wang.
647"
REFERENCES,0.6053540587219344,"Pave the way to grasp anything: Transferring foundation models for universal pick-place
648"
REFERENCES,0.6062176165803109,"robots. arXiv preprint arXiv:2306.05716, 2023.
649"
REFERENCES,0.6070811744386874,"[104] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn,
650"
REFERENCES,0.6079447322970639,"and Sergey Levine.
Meta-world: A benchmark and evaluation for multi-task and meta
651"
REFERENCES,0.6088082901554405,"reinforcement learning. In CoRL, pages 1094–1100. PMLR, 2020.
652"
REFERENCES,0.6096718480138169,"[105] Chengbo Yuan, Chuan Wen, Tong Zhang, and Yang Gao. General flow as foundation affordance
653"
REFERENCES,0.6105354058721935,"for scalable robot learning. arXiv preprint arXiv:2401.11439, 2024.
654"
REFERENCES,0.6113989637305699,"[106] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong.
655"
REFERENCES,0.6122625215889465,"ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.
656"
REFERENCES,0.613126079447323,"[107] Haoyi Zhu, Yating Wang, Di Huang, Weicai Ye, Wanli Ouyang, and Tong He. Point cloud
657"
REFERENCES,0.6139896373056994,"matters: Rethinking the impact of different observation spaces on robot learning. arXiv
658"
REFERENCES,0.614853195164076,"preprint arXiv:2402.02500, 2024.
659"
REFERENCES,0.6157167530224525,"A
Appendix
660"
REFERENCES,0.616580310880829,"A.1
Limitations and Discussion
661"
REFERENCES,0.6174438687392055,"Although STP has demonstrated superior performance in extensive experiments, there remain some
662"
REFERENCES,0.6183074265975821,"challenges and future works. From the perspective of pre-training data, Ego4D provides numerous
663"
REFERENCES,0.6191709844559585,"human-object interaction scenes and good motion clues. Building larger-scale and more diverse
664"
REFERENCES,0.6200345423143351,"potential datasets such as [63, 30] to scale up STP is worth exploring. Regarding pre-training methods,
665"
REFERENCES,0.6208981001727115,"exploring predictive targets outside of pixel space and more effective sampling and masking strategies
666"
REFERENCES,0.6217616580310881,"present intriguing research directions. From an evaluation standpoint, we utilize a frozen ViT to
667"
REFERENCES,0.6226252158894646,"extract agent state representations and adopt the paradigm of few-shot behavior cloning, other policy
668"
REFERENCES,0.6234887737478411,"learning methods (reinforcement learning, visual reward function, visual task specification), have not
669"
REFERENCES,0.6243523316062176,"been explored. In conclusion, as the first method of performing temporal prediction on large-scale
670"
REFERENCES,0.6252158894645942,"videos for self-supervised visual representation learning intended for robotic motor control tasks, we
671"
REFERENCES,0.6260794473229706,"hope STP can be taken as a strong baseline and facilitate further research along this direction.
672"
REFERENCES,0.6269430051813472,"A.2
The influence of the loss weight ratio between temporal prediction and spatial prediction
673"
REFERENCES,0.6278065630397237,"In this section, we further explore the influence of the loss weight ratio between temporal prediction
674"
REFERENCES,0.6286701208981001,"and spatial prediction. Specifically, taking five tasks from Franka-Kitchen as examples, we load the
675"
REFERENCES,0.6295336787564767,"pre-trained STP and perform post-pre-training with three different loss weight ratios (temporal to
676"
REFERENCES,0.6303972366148531,"spatial). The results, as shown in Figure 5, are 54.7, 55.2, and 57.4 for the average results of the ratios
677"
REFERENCES,0.6312607944732297,"3:1, 1:3, and 1:1, respectively. The results indicate that due to the different attributes of the tasks, the
678"
REFERENCES,0.6321243523316062,"trends are not consistent. However, overall, the 1:1 ratio achieves the best balance and results. We
679"
REFERENCES,0.6329879101899827,"chose it as a universal setting.
680"
REFERENCES,0.6338514680483592,"A.3
Pre-training Details
681"
REFERENCES,0.6347150259067358,"In this section, we describe the details of our STP pre-training. Specifically, we list some key training
682"
REFERENCES,0.6355785837651122,"and architectural hyperparameters of STP in Table 4. In addition, as for our MAE [38] baseline, we
683"
REFERENCES,0.6364421416234888,"mainly follow the publicly available code of MAE1. Additionally, we train MAE and STP using the
684"
REFERENCES,0.6373056994818653,"same data and number of epochs to ensure that the comparison between them is completely fair.
685"
REFERENCES,0.6381692573402418,"Finally, we also provide some STP prediction results in Figure 6.
686"
REFERENCES,0.6390328151986183,"A.4
Simulation Environments Details
687"
REFERENCES,0.6398963730569949,"In this section, we first present further details of the STP post-pre-training on downstream simula-
688"
REFERENCES,0.6407599309153713,"tion environments. Subsequently, we delineate the specific hyperparameters used in the behavior
689"
REFERENCES,0.6416234887737479,"cloning policy training within these simulation environments. Finally, we provide the comprehensive
690"
REFERENCES,0.6424870466321243,"evaluation scheme for each simulation environment.
691"
REFERENCES,0.6433506044905009,1https://github.com/facebookresearch/mae
REFERENCES,0.6442141623488774,Figure 5: The results of different loss weight ratios between temporal prediction and spatial prediction.
REFERENCES,0.6450777202072538,Table 4: Training and architectural hyperparameters for STP pre-training.
REFERENCES,0.6459412780656304,"Hyperparameter
Value
STP Pre-training
optimizer
AdamW [49]
base learning rate
0.00015
weight decay
0.05
optimizer momentum
β1, β2 = 0.9, 0.95
effective batch size
4096
learning rate schedule
cosine decay
total epochs
50
warmup epochs
5
augmentation
RandomResizedCrop (0.8, 1)
Encoder ViT-base Architecture
patch size
16
#layers
12
#MHSA heads
12
hidden dim
768
positional embedding
sin-cos initialization and fix
Dual Decoder ViT-base Architecture
#layers
8
#MHSA heads
16
hidden dim
512
positional embedding
sin-cos initialization and fix"
REFERENCES,0.646804835924007,"In regards to the STP post-pre-training, we utilize data that aligns with the policy training, and the
692"
REFERENCES,0.6476683937823834,"specific architecture hyperparameters correspond to those listed in Table 4. Depending on the specific
693"
REFERENCES,0.6485319516407599,"demonstration data, we adjust the values of total epochs, warmup epochs, effective batch size, and
694"
REFERENCES,0.6493955094991365,"the frame interval, as shown in Table 5.
695"
REFERENCES,0.6502590673575129,"As for policy training and evaluation schemes, we primarily refer to the publicly available code2
696"
REFERENCES,0.6511226252158895,"and training data of VC-1 [59] for Metaworld [104], DMControl [83], Adroit [74] and Trifinger [98].
697"
REFERENCES,0.6519861830742659,"Similarly, for Franka-Kitchen [31], we follow the public code3 and training data of R3M [65].
698"
REFERENCES,0.6528497409326425,"Specifically, the policy training hyperparameters and evaluation schemes are shown in Table 6 and
699"
REFERENCES,0.653713298791019,"Table 7, respectively. About policy training, we completely follow the setting of prior works [65, 59]
700"
REFERENCES,0.6545768566493955,"when freezing the encoder; when performing end-to-end fine-tuning, we make appropriate adjustments
701"
REFERENCES,0.655440414507772,"to the batch size and learning rate. About evaluation details, similar to prior works[65, 59], we
702"
REFERENCES,0.6563039723661486,"establish all evaluation details such as the number of expert demonstrations and test trajectories,
703"
REFERENCES,0.657167530224525,"environmental viewpoints, optimization hyperparameters, base seeds, history windows size, and
704"
REFERENCES,0.6580310880829016,"the use of robot proprioceptive. In Table 7, the term ‘prop.’ stands for whether proprioceptive
705"
REFERENCES,0.6588946459412781,"information is used or not, and ‘history window size’ signifies the number of frames received by
706"
REFERENCES,0.6597582037996546,"the policy model at each step, with features between frames being fused through concatenation.
707"
REFERENCES,0.6606217616580311,"‘Number of trajectories’ represents the quantity of trajectories evaluated. For tasks in Meta-World,
708"
REFERENCES,0.6614853195164075,"Franka-Kitchen, Adroit, and Trifinger, we report the maximum success rate, whereas for tasks in
709"
REFERENCES,0.6623488773747841,"DMControl, we report the maximum reward score, rescaling to be in the range of [0, 100] by dividing
710"
REFERENCES,0.6632124352331606,"by 10. We report the average metric across tasks for each environment. In addition, it is worth noting
711"
REFERENCES,0.6640759930915371,"that the metrics we report are the average value across all base seeds and camera viewpoints.
712"
REFERENCES,0.6649395509499136,"Finally, we also report the results of our post-pre-training STP (ViT-B/16) on each task in Table 8.
713"
REFERENCES,0.6658031088082902,"In addition, we emphasize that different random seeds primarily affect the rendering of the initial
714"
REFERENCES,0.6666666666666666,"frame in the sampled trajectories, as shown in Figure 7. During evaluation, the seed value we provide
715"
REFERENCES,0.6675302245250432,"serves as the base seed, and the trajectory sampling process is depicted in Algorithm 1. Therefore,
716"
REFERENCES,0.6683937823834197,"the actual number of trajectories we evaluate is the number of trajectories multiplied by the
717"
REFERENCES,0.6692573402417962,"number of base seeds. For instance, for MetaWorld, we evaluate 25 × 3 = 75 trajectories, with
718"
REFERENCES,0.6701208981001727,"random seeds for rendering being 100-124, 200-224, and 300-324.
719"
REFERENCES,0.6709844559585493,"Finally, for Franka-Kitchen, we utilize MuJoCo210, while all other simulation environments are
720"
REFERENCES,0.6718480138169257,"based on MuJoCo200. Our policy training and evaluation environments are conducted on Cuda 11.3,
721"
REFERENCES,0.6727115716753023,"NVIDIA TITAN Xp GPUs, and OpenGL 3.1.
722"
REFERENCES,0.6735751295336787,"Figure 6: Some examples of our STP prediction result on Ego4D videos. For each six tuple, we show the
ground-truth (left), masked frames (middle), STP prediciton results (right), current frames (top), and future
frames (bottom). We simply overlay the output with the visible patches to improve visual quality."
REFERENCES,0.6744386873920553,"2https://github.com/facebookresearch/eai-vc/tree/main/cortexbench
3https://github.com/facebookresearch/r3m/tree/eval/evaluation"
REFERENCES,0.6753022452504318,"Figure 7: The visualization of initial frame rendering under different random seeds.
Algorithm 1 Trajectories Sampling Pseudocode"
REFERENCES,0.6761658031088082,"# num_traj: the number of evaluation
trajectories
# base_seed: base seed for
rollouts"
REFERENCES,0.6770293609671848,"# rollout to sample
trajectories
for ep in range(num_traj ):
seed = base_seed + ep
env.set_seed(seed)
o = env.reset ()"
REFERENCES,0.6778929188255614,"A.5
Real-World Environments Details
723"
REFERENCES,0.6787564766839378,"In this section, we outline the details of our real-world setup and evaluation scheme. As depicted
724"
REFERENCES,0.6796200345423143,"in Figure 8, our real-world scenario includes four camera viewpoints: top, left, right, and wrist. It
725"
REFERENCES,0.6804835924006909,"includes two Kinect DK and two RealSense cameras. An example of four views is shown in Figure 9.
726"
REFERENCES,0.6813471502590673,"Specifically, we utilize four different camera views and resize their resolution uniformly to 224 × 224.
727"
REFERENCES,0.6822107081174439,"To effectively model the complex and multimodal action distribution in our real-world tasks, we
728"
REFERENCES,0.6830742659758203,"select diffusion policy [16] as our policy model. In accordance with this approach, we concatenate
729"
REFERENCES,0.6839378238341969,"the visual embeddings of all views from two sequential frames. Following the approach in [27], we
730"
REFERENCES,0.6848013816925734,"collect robot data using a VR tele-operation setup. In this way, we collect 100 continuous trajectories
731"
REFERENCES,0.6856649395509499,"for each task. It is worth noting that the quality of these demonstrations leaves room for improvement
732"
REFERENCES,0.6865284974093264,"and contains a lot of noise. During the evaluation phase, we primarily evaluate two contact-rich
733"
REFERENCES,0.687392055267703,"tasks that have not appeared in Franka-Kitchen [31] benchmark: (1) Picking. It requires the robot
734"
REFERENCES,0.6882556131260794,"arm to pick up the transparent bowel off the table; (2) Pouring. It requires the robot arm to pour
735"
REFERENCES,0.689119170984456,"at least three-quarter of the ingredients from the transparent bowl into the black pot. For each task,
736"
REFERENCES,0.6899827288428325,"we change the initial pose of the robot arm and objects within a certain range as well as conduct 20
737"
REFERENCES,0.690846286701209,"trials. In addition, there are different distractors on the desktop during training and testing, which
738"
REFERENCES,0.6917098445595855,"also evaluates the robustness of the model to distractors. Throughout the process, we use ROS and
739"
REFERENCES,0.6925734024179621,"MoveIt for hardware communication and motion planning.
740"
REFERENCES,0.6934369602763385,Kinect DK
REFERENCES,0.694300518134715,Kinect DK
REFERENCES,0.6951640759930915,RealSense
REFERENCES,0.696027633851468,RealSense
REFERENCES,0.6968911917098446,"Bowl
Pot"
REFERENCES,0.697754749568221,Figure 8: Our real-world scene with four cameras and a Franka Emika robot arm.
REFERENCES,0.6986183074265976,"Wrist
Left
Top
Right"
REFERENCES,0.6994818652849741,Figure 9: An example of four views.
REFERENCES,0.7003454231433506,Table 5: STP post-pre-training hyperparameters on simulation environments.
REFERENCES,0.7012089810017271,"MetaWorld
Franka-Kitchen
DMControl
Adroit
Trifinger
total epochs
50
100
50
50
50
warmup epochs
5
5
5
5
5
effective batch size
1024
128
2048
1024
1024
number of demonstrations
25
25
100
100
100
frame interval
4
4
4
4
16"
REFERENCES,0.7020725388601037,Table 6: Policy training hyperparameters on simulation environments.
REFERENCES,0.7029360967184801,"MetaWorld
Franka-Kitchen
DMControl
Adroit
Trifinger
epochs
100
480
100
100
100 / 1000"
REFERENCES,0.7037996545768567,"batch size
frozen
256
32
256
256
32
fine-tuning
64
32
64
64
16"
REFERENCES,0.7046632124352331,"learning rate
frozen
0.001
0.001
0.001
0.001
0.0001
fine-tuning
0.00005
0.0001
0.00005
0.00005
0.0001"
REFERENCES,0.7055267702936097,Table 7: Evaluation schemes on simulation environments.
REFERENCES,0.7063903281519862,"Benchmark
Observation
Space
History
Window Size
Camera
ViewPoints
Base Seeds
Number of
Trajectories"
REFERENCES,0.7072538860103627,"Metaworld
RGB + prop.
3
top_cap2
100, 200, 300
25
Franka-Kitchen
RGB + prop.
1
left, right
123, 124, 125
50
DMControl
RGB
3
0
100, 200, 300
25
Adroit
RGB + prop.
1
vil_camera
100, 200, 300
25
Trifinger
RGB + prop.
1
default
10
25"
REFERENCES,0.7081174438687392,"Table 8: The success rate for each task on simulation bechmarks.
Assembly
Bin-Picking
Button-Press
Drawer-Open
Hammer
94.7
97.3
94.7
100.0
100.0
Sliding Door
Turning Light on
Opening Door
Turning Knob
Opening Microwave
96.0
72.7
39.0
31.3
29.0
Relocate
Reorient-Pen
Finger-Spin
Cheetah-Run
Reacher-Hard
49.3
77.3
69.6
71.9
87.7
Walker-Stand
Walker-Walk
Reach-Cube
Push-Cube
95.9
89.0
85.3
70.6"
REFERENCES,0.7089810017271158,"NeurIPS Paper Checklist
741"
REFERENCES,0.7098445595854922,"The checklist is designed to encourage best practices for responsible machine learning research,
742"
REFERENCES,0.7107081174438687,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
743"
REFERENCES,0.7115716753022453,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
744"
REFERENCES,0.7124352331606217,"follow the references and follow the (optional) supplemental material. The checklist does NOT count
745"
REFERENCES,0.7132987910189983,"towards the page limit.
746"
REFERENCES,0.7141623488773747,"Please read the checklist guidelines carefully for information on how to answer these questions. For
747"
REFERENCES,0.7150259067357513,"each question in the checklist:
748"
REFERENCES,0.7158894645941278,"• You should answer [Yes] , [No] , or [NA] .
749"
REFERENCES,0.7167530224525043,"• [NA] means either that the question is Not Applicable for that particular paper or the
750"
REFERENCES,0.7176165803108808,"relevant information is Not Available.
751"
REFERENCES,0.7184801381692574,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
752"
REFERENCES,0.7193436960276338,"The checklist answers are an integral part of your paper submission. They are visible to the
753"
REFERENCES,0.7202072538860104,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
754"
REFERENCES,0.7210708117443869,"(after eventual revisions) with the final version of your paper, and its final version will be published
755"
REFERENCES,0.7219343696027634,"with the paper.
756"
REFERENCES,0.7227979274611399,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
757"
REFERENCES,0.7236614853195165,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
758"
REFERENCES,0.7245250431778929,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
759"
REFERENCES,0.7253886010362695,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
760"
REFERENCES,0.7262521588946459,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
761"
REFERENCES,0.7271157167530224,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
762"
REFERENCES,0.727979274611399,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
763"
REFERENCES,0.7288428324697754,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
764"
REFERENCES,0.729706390328152,"please point to the section(s) where related material for the question can be found.
765"
REFERENCES,0.7305699481865285,"IMPORTANT, please:
766"
REFERENCES,0.731433506044905,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
767"
REFERENCES,0.7322970639032815,"• Keep the checklist subsection headings, questions/answers and guidelines below.
768"
REFERENCES,0.7331606217616581,"• Do not modify the questions and only use the provided macros for your answers.
769"
CLAIMS,0.7340241796200345,"1. Claims
770"
CLAIMS,0.7348877374784111,"Question: Do the main claims made in the abstract and introduction accurately reflect the
771"
CLAIMS,0.7357512953367875,"paper’s contributions and scope?
772"
CLAIMS,0.7366148531951641,"Answer: [Yes]
773"
CLAIMS,0.7374784110535406,"Justification: Yes, the main claims made in the abstract and introduction accurately reflect
774"
CLAIMS,0.7383419689119171,"the paper’s contributions and scope.
775"
CLAIMS,0.7392055267702936,"Guidelines:
776"
CLAIMS,0.7400690846286702,"• The answer NA means that the abstract and introduction do not include the claims
777"
CLAIMS,0.7409326424870466,"made in the paper.
778"
CLAIMS,0.7417962003454232,"• The abstract and/or introduction should clearly state the claims made, including the
779"
CLAIMS,0.7426597582037997,"contributions made in the paper and important assumptions and limitations. A No or
780"
CLAIMS,0.7435233160621761,"NA answer to this question will not be perceived well by the reviewers.
781"
CLAIMS,0.7443868739205527,"• The claims made should match theoretical and experimental results, and reflect how
782"
CLAIMS,0.7452504317789291,"much the results can be expected to generalize to other settings.
783"
CLAIMS,0.7461139896373057,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
784"
CLAIMS,0.7469775474956822,"are not attained by the paper.
785"
LIMITATIONS,0.7478411053540587,"2. Limitations
786"
LIMITATIONS,0.7487046632124352,"Question: Does the paper discuss the limitations of the work performed by the authors?
787"
LIMITATIONS,0.7495682210708118,"Answer: [Yes]
788"
LIMITATIONS,0.7504317789291882,"Justification: Yes, the paper discusses the limitations of the work performed by the authors.
789"
LIMITATIONS,0.7512953367875648,"Guidelines:
790"
LIMITATIONS,0.7521588946459413,"• The answer NA means that the paper has no limitation while the answer No means that
791"
LIMITATIONS,0.7530224525043178,"the paper has limitations, but those are not discussed in the paper.
792"
LIMITATIONS,0.7538860103626943,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
793"
LIMITATIONS,0.7547495682210709,"• The paper should point out any strong assumptions and how robust the results are to
794"
LIMITATIONS,0.7556131260794473,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
795"
LIMITATIONS,0.7564766839378239,"model well-specification, asymptotic approximations only holding locally). The authors
796"
LIMITATIONS,0.7573402417962003,"should reflect on how these assumptions might be violated in practice and what the
797"
LIMITATIONS,0.7582037996545768,"implications would be.
798"
LIMITATIONS,0.7590673575129534,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
799"
LIMITATIONS,0.7599309153713298,"only tested on a few datasets or with a few runs. In general, empirical results often
800"
LIMITATIONS,0.7607944732297064,"depend on implicit assumptions, which should be articulated.
801"
LIMITATIONS,0.7616580310880829,"• The authors should reflect on the factors that influence the performance of the approach.
802"
LIMITATIONS,0.7625215889464594,"For example, a facial recognition algorithm may perform poorly when image resolution
803"
LIMITATIONS,0.7633851468048359,"is low or images are taken in low lighting. Or a speech-to-text system might not be
804"
LIMITATIONS,0.7642487046632125,"used reliably to provide closed captions for online lectures because it fails to handle
805"
LIMITATIONS,0.7651122625215889,"technical jargon.
806"
LIMITATIONS,0.7659758203799655,"• The authors should discuss the computational efficiency of the proposed algorithms
807"
LIMITATIONS,0.7668393782383419,"and how they scale with dataset size.
808"
LIMITATIONS,0.7677029360967185,"• If applicable, the authors should discuss possible limitations of their approach to
809"
LIMITATIONS,0.768566493955095,"address problems of privacy and fairness.
810"
LIMITATIONS,0.7694300518134715,"• While the authors might fear that complete honesty about limitations might be used by
811"
LIMITATIONS,0.770293609671848,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
812"
LIMITATIONS,0.7711571675302246,"limitations that aren’t acknowledged in the paper. The authors should use their best
813"
LIMITATIONS,0.772020725388601,"judgment and recognize that individual actions in favor of transparency play an impor-
814"
LIMITATIONS,0.7728842832469776,"tant role in developing norms that preserve the integrity of the community. Reviewers
815"
LIMITATIONS,0.7737478411053541,"will be specifically instructed to not penalize honesty concerning limitations.
816"
THEORY ASSUMPTIONS AND PROOFS,0.7746113989637305,"3. Theory Assumptions and Proofs
817"
THEORY ASSUMPTIONS AND PROOFS,0.7754749568221071,"Question: For each theoretical result, does the paper provide the full set of assumptions and
818"
THEORY ASSUMPTIONS AND PROOFS,0.7763385146804835,"a complete (and correct) proof?
819"
THEORY ASSUMPTIONS AND PROOFS,0.7772020725388601,"Answer: [Yes]
820"
THEORY ASSUMPTIONS AND PROOFS,0.7780656303972366,"Justification: The paper does not include theoretical results.
821"
THEORY ASSUMPTIONS AND PROOFS,0.7789291882556131,"Guidelines:
822"
THEORY ASSUMPTIONS AND PROOFS,0.7797927461139896,"• The answer NA means that the paper does not include theoretical results.
823"
THEORY ASSUMPTIONS AND PROOFS,0.7806563039723662,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
824"
THEORY ASSUMPTIONS AND PROOFS,0.7815198618307426,"referenced.
825"
THEORY ASSUMPTIONS AND PROOFS,0.7823834196891192,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
826"
THEORY ASSUMPTIONS AND PROOFS,0.7832469775474957,"• The proofs can either appear in the main paper or the supplemental material, but if
827"
THEORY ASSUMPTIONS AND PROOFS,0.7841105354058722,"they appear in the supplemental material, the authors are encouraged to provide a short
828"
THEORY ASSUMPTIONS AND PROOFS,0.7849740932642487,"proof sketch to provide intuition.
829"
THEORY ASSUMPTIONS AND PROOFS,0.7858376511226253,"• Inversely, any informal proof provided in the core of the paper should be complemented
830"
THEORY ASSUMPTIONS AND PROOFS,0.7867012089810017,"by formal proofs provided in appendix or supplemental material.
831"
THEORY ASSUMPTIONS AND PROOFS,0.7875647668393783,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
832"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7884283246977547,"4. Experimental Result Reproducibility
833"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7892918825561313,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
834"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7901554404145078,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
835"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7910189982728842,"of the paper (regardless of whether the code and data are provided or not)?
836"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7918825561312608,"Answer: [Yes]
837"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7927461139896373,"Justification: The paper fully disclose all the information needed to reproduce results.
838"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7936096718480138,"Guidelines:
839"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7944732297063903,"• The answer NA means that the paper does not include experiments.
840"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7953367875647669,"• If the paper includes experiments, a No answer to this question will not be perceived
841"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7962003454231433,"well by the reviewers: Making the paper reproducible is important, regardless of
842"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7970639032815199,"whether the code and data are provided or not.
843"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7979274611398963,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
844"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7987910189982729,"to make their results reproducible or verifiable.
845"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7996545768566494,"• Depending on the contribution, reproducibility can be accomplished in various ways.
846"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8005181347150259,"For example, if the contribution is a novel architecture, describing the architecture fully
847"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8013816925734024,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
848"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.802245250431779,"be necessary to either make it possible for others to replicate the model with the same
849"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8031088082901554,"dataset, or provide access to the model. In general. releasing code and data is often
850"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.803972366148532,"one good way to accomplish this, but reproducibility can also be provided via detailed
851"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8048359240069085,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
852"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.805699481865285,"of a large language model), releasing of a model checkpoint, or other means that are
853"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8065630397236615,"appropriate to the research performed.
854"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8074265975820379,"• While NeurIPS does not require releasing code, the conference does require all submis-
855"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8082901554404145,"sions to provide some reasonable avenue for reproducibility, which may depend on the
856"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.809153713298791,"nature of the contribution. For example
857"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8100172711571675,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
858"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.810880829015544,"to reproduce that algorithm.
859"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8117443868739206,"(b) If the contribution is primarily a new model architecture, the paper should describe
860"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.812607944732297,"the architecture clearly and fully.
861"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8134715025906736,"(c) If the contribution is a new model (e.g., a large language model), then there should
862"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8143350604490501,"either be a way to access this model for reproducing the results or a way to reproduce
863"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8151986183074266,"the model (e.g., with an open-source dataset or instructions for how to construct
864"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8160621761658031,"the dataset).
865"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8169257340241797,"(d) We recognize that reproducibility may be tricky in some cases, in which case
866"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8177892918825561,"authors are welcome to describe the particular way they provide for reproducibility.
867"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8186528497409327,"In the case of closed-source models, it may be that access to the model is limited in
868"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8195164075993091,"some way (e.g., to registered users), but it should be possible for other researchers
869"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8203799654576857,"to have some path to reproducing or verifying the results.
870"
OPEN ACCESS TO DATA AND CODE,0.8212435233160622,"5. Open access to data and code
871"
OPEN ACCESS TO DATA AND CODE,0.8221070811744386,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
872"
OPEN ACCESS TO DATA AND CODE,0.8229706390328152,"tions to faithfully reproduce the main experimental results, as described in supplemental
873"
OPEN ACCESS TO DATA AND CODE,0.8238341968911918,"material?
874"
OPEN ACCESS TO DATA AND CODE,0.8246977547495682,"Answer: [Yes]
875"
OPEN ACCESS TO DATA AND CODE,0.8255613126079447,"Justification: We will release all codes and model weights on github.
876"
OPEN ACCESS TO DATA AND CODE,0.8264248704663213,"Guidelines:
877"
OPEN ACCESS TO DATA AND CODE,0.8272884283246977,"• The answer NA means that paper does not include experiments requiring code.
878"
OPEN ACCESS TO DATA AND CODE,0.8281519861830743,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
879"
OPEN ACCESS TO DATA AND CODE,0.8290155440414507,"public/guides/CodeSubmissionPolicy) for more details.
880"
OPEN ACCESS TO DATA AND CODE,0.8298791018998273,"• While we encourage the release of code and data, we understand that this might not be
881"
OPEN ACCESS TO DATA AND CODE,0.8307426597582038,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
882"
OPEN ACCESS TO DATA AND CODE,0.8316062176165803,"including code, unless this is central to the contribution (e.g., for a new open-source
883"
OPEN ACCESS TO DATA AND CODE,0.8324697754749568,"benchmark).
884"
OPEN ACCESS TO DATA AND CODE,0.8333333333333334,"• The instructions should contain the exact command and environment needed to run to
885"
OPEN ACCESS TO DATA AND CODE,0.8341968911917098,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
886"
OPEN ACCESS TO DATA AND CODE,0.8350604490500864,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
887"
OPEN ACCESS TO DATA AND CODE,0.8359240069084629,"• The authors should provide instructions on data access and preparation, including how
888"
OPEN ACCESS TO DATA AND CODE,0.8367875647668394,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
889"
OPEN ACCESS TO DATA AND CODE,0.8376511226252159,"• The authors should provide scripts to reproduce all experimental results for the new
890"
OPEN ACCESS TO DATA AND CODE,0.8385146804835925,"proposed method and baselines. If only a subset of experiments are reproducible, they
891"
OPEN ACCESS TO DATA AND CODE,0.8393782383419689,"should state which ones are omitted from the script and why.
892"
OPEN ACCESS TO DATA AND CODE,0.8402417962003454,"• At submission time, to preserve anonymity, the authors should release anonymized
893"
OPEN ACCESS TO DATA AND CODE,0.8411053540587219,"versions (if applicable).
894"
OPEN ACCESS TO DATA AND CODE,0.8419689119170984,"• Providing as much information as possible in supplemental material (appended to the
895"
OPEN ACCESS TO DATA AND CODE,0.842832469775475,"paper) is recommended, but including URLs to data and code is permitted.
896"
OPEN ACCESS TO DATA AND CODE,0.8436960276338514,"6. Experimental Setting/Details
897"
OPEN ACCESS TO DATA AND CODE,0.844559585492228,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
898"
OPEN ACCESS TO DATA AND CODE,0.8454231433506045,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
899"
OPEN ACCESS TO DATA AND CODE,0.846286701208981,"results?
900"
OPEN ACCESS TO DATA AND CODE,0.8471502590673575,"Answer: [Yes]
901"
OPEN ACCESS TO DATA AND CODE,0.8480138169257341,"Justification: The paper specify all the training and test details.
902"
OPEN ACCESS TO DATA AND CODE,0.8488773747841105,"Guidelines:
903"
OPEN ACCESS TO DATA AND CODE,0.8497409326424871,"• The answer NA means that the paper does not include experiments.
904"
OPEN ACCESS TO DATA AND CODE,0.8506044905008635,"• The experimental setting should be presented in the core of the paper to a level of detail
905"
OPEN ACCESS TO DATA AND CODE,0.8514680483592401,"that is necessary to appreciate the results and make sense of them.
906"
OPEN ACCESS TO DATA AND CODE,0.8523316062176166,"• The full details can be provided either with the code, in appendix, or as supplemental
907"
OPEN ACCESS TO DATA AND CODE,0.853195164075993,"material.
908"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8540587219343696,"7. Experiment Statistical Significance
909"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8549222797927462,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
910"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8557858376511226,"information about the statistical significance of the experiments?
911"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8566493955094991,"Answer: [No]
912"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8575129533678757,"Justification: In our experiments, different seeds are primarily used for rendering different
913"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8583765112262521,"initial frames. Therefore, our evaluatation is comprehensive and sufficient, while our
914"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8592400690846287,"comparisons are absolutely fair.
915"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8601036269430051,"Guidelines:
916"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8609671848013817,"• The answer NA means that the paper does not include experiments.
917"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8618307426597582,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
918"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8626943005181347,"dence intervals, or statistical significance tests, at least for the experiments that support
919"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8635578583765112,"the main claims of the paper.
920"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8644214162348878,"• The factors of variability that the error bars are capturing should be clearly stated (for
921"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8652849740932642,"example, train/test split, initialization, random drawing of some parameter, or overall
922"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8661485319516408,"run with given experimental conditions).
923"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8670120898100173,"• The method for calculating the error bars should be explained (closed form formula,
924"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8678756476683938,"call to a library function, bootstrap, etc.)
925"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8687392055267703,"• The assumptions made should be given (e.g., Normally distributed errors).
926"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8696027633851469,"• It should be clear whether the error bar is the standard deviation or the standard error
927"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8704663212435233,"of the mean.
928"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8713298791018999,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
929"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8721934369602763,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
930"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8730569948186528,"of Normality of errors is not verified.
931"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8739205526770294,"• For asymmetric distributions, the authors should be careful not to show in tables or
932"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8747841105354058,"figures symmetric error bars that would yield results that are out of range (e.g. negative
933"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8756476683937824,"error rates).
934"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8765112262521589,"• If error bars are reported in tables or plots, The authors should explain in the text how
935"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8773747841105354,"they were calculated and reference the corresponding figures or tables in the text.
936"
EXPERIMENTS COMPUTE RESOURCES,0.8782383419689119,"8. Experiments Compute Resources
937"
EXPERIMENTS COMPUTE RESOURCES,0.8791018998272885,"Question: For each experiment, does the paper provide sufficient information on the com-
938"
EXPERIMENTS COMPUTE RESOURCES,0.8799654576856649,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
939"
EXPERIMENTS COMPUTE RESOURCES,0.8808290155440415,"the experiments?
940"
EXPERIMENTS COMPUTE RESOURCES,0.8816925734024179,"Answer: [Yes]
941"
EXPERIMENTS COMPUTE RESOURCES,0.8825561312607945,"Justification: We provide sufficient information on the computer resources.
942"
EXPERIMENTS COMPUTE RESOURCES,0.883419689119171,"Guidelines:
943"
EXPERIMENTS COMPUTE RESOURCES,0.8842832469775475,"• The answer NA means that the paper does not include experiments.
944"
EXPERIMENTS COMPUTE RESOURCES,0.885146804835924,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
945"
EXPERIMENTS COMPUTE RESOURCES,0.8860103626943006,"or cloud provider, including relevant memory and storage.
946"
EXPERIMENTS COMPUTE RESOURCES,0.886873920552677,"• The paper should provide the amount of compute required for each of the individual
947"
EXPERIMENTS COMPUTE RESOURCES,0.8877374784110535,"experimental runs as well as estimate the total compute.
948"
EXPERIMENTS COMPUTE RESOURCES,0.8886010362694301,"• The paper should disclose whether the full research project required more compute
949"
EXPERIMENTS COMPUTE RESOURCES,0.8894645941278065,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
950"
EXPERIMENTS COMPUTE RESOURCES,0.8903281519861831,"didn’t make it into the paper).
951"
CODE OF ETHICS,0.8911917098445595,"9. Code Of Ethics
952"
CODE OF ETHICS,0.8920552677029361,"Question: Does the research conducted in the paper conform, in every respect, with the
953"
CODE OF ETHICS,0.8929188255613126,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
954"
CODE OF ETHICS,0.8937823834196891,"Answer:[Yes]
955"
CODE OF ETHICS,0.8946459412780656,"Justification: Our paper aligns with these.
956"
CODE OF ETHICS,0.8955094991364422,"Guidelines:
957"
CODE OF ETHICS,0.8963730569948186,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
958"
CODE OF ETHICS,0.8972366148531952,"• If the authors answer No, they should explain the special circumstances that require a
959"
CODE OF ETHICS,0.8981001727115717,"deviation from the Code of Ethics.
960"
CODE OF ETHICS,0.8989637305699482,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
961"
CODE OF ETHICS,0.8998272884283247,"eration due to laws or regulations in their jurisdiction).
962"
BROADER IMPACTS,0.9006908462867013,"10. Broader Impacts
963"
BROADER IMPACTS,0.9015544041450777,"Question: Does the paper discuss both potential positive societal impacts and negative
964"
BROADER IMPACTS,0.9024179620034543,"societal impacts of the work performed?
965"
BROADER IMPACTS,0.9032815198618307,"Answer: [NA]
966"
BROADER IMPACTS,0.9041450777202072,"Justification: There is no societal impact of the work performed.
967"
BROADER IMPACTS,0.9050086355785838,"Guidelines:
968"
BROADER IMPACTS,0.9058721934369602,"• The answer NA means that there is no societal impact of the work performed.
969"
BROADER IMPACTS,0.9067357512953368,"• If the authors answer NA or No, they should explain why their work has no societal
970"
BROADER IMPACTS,0.9075993091537133,"impact or why the paper does not address societal impact.
971"
BROADER IMPACTS,0.9084628670120898,"• Examples of negative societal impacts include potential malicious or unintended uses
972"
BROADER IMPACTS,0.9093264248704663,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
973"
BROADER IMPACTS,0.9101899827288429,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
974"
BROADER IMPACTS,0.9110535405872193,"groups), privacy considerations, and security considerations.
975"
BROADER IMPACTS,0.9119170984455959,"• The conference expects that many papers will be foundational research and not tied
976"
BROADER IMPACTS,0.9127806563039723,"to particular applications, let alone deployments. However, if there is a direct path to
977"
BROADER IMPACTS,0.9136442141623489,"any negative applications, the authors should point it out. For example, it is legitimate
978"
BROADER IMPACTS,0.9145077720207254,"to point out that an improvement in the quality of generative models could be used to
979"
BROADER IMPACTS,0.9153713298791019,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
980"
BROADER IMPACTS,0.9162348877374784,"that a generic algorithm for optimizing neural networks could enable people to train
981"
BROADER IMPACTS,0.917098445595855,"models that generate Deepfakes faster.
982"
BROADER IMPACTS,0.9179620034542314,"• The authors should consider possible harms that could arise when the technology is
983"
BROADER IMPACTS,0.918825561312608,"being used as intended and functioning correctly, harms that could arise when the
984"
BROADER IMPACTS,0.9196891191709845,"technology is being used as intended but gives incorrect results, and harms following
985"
BROADER IMPACTS,0.9205526770293609,"from (intentional or unintentional) misuse of the technology.
986"
BROADER IMPACTS,0.9214162348877375,"• If there are negative societal impacts, the authors could also discuss possible mitigation
987"
BROADER IMPACTS,0.9222797927461139,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
988"
BROADER IMPACTS,0.9231433506044905,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
989"
BROADER IMPACTS,0.924006908462867,"feedback over time, improving the efficiency and accessibility of ML).
990"
SAFEGUARDS,0.9248704663212435,"11. Safeguards
991"
SAFEGUARDS,0.92573402417962,"Question: Does the paper describe safeguards that have been put in place for responsible
992"
SAFEGUARDS,0.9265975820379966,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
993"
SAFEGUARDS,0.927461139896373,"image generators, or scraped datasets)?
994"
SAFEGUARDS,0.9283246977547496,"Answer: [NA]
995"
SAFEGUARDS,0.9291882556131261,"Justification: The paper poses no such risks.
996"
SAFEGUARDS,0.9300518134715026,"Guidelines:
997"
SAFEGUARDS,0.9309153713298791,"• The answer NA means that the paper poses no such risks.
998"
SAFEGUARDS,0.9317789291882557,"• Released models that have a high risk for misuse or dual-use should be released with
999"
SAFEGUARDS,0.9326424870466321,"necessary safeguards to allow for controlled use of the model, for example by requiring
1000"
SAFEGUARDS,0.9335060449050087,"that users adhere to usage guidelines or restrictions to access the model or implementing
1001"
SAFEGUARDS,0.9343696027633851,"safety filters.
1002"
SAFEGUARDS,0.9352331606217616,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1003"
SAFEGUARDS,0.9360967184801382,"should describe how they avoided releasing unsafe images.
1004"
SAFEGUARDS,0.9369602763385146,"• We recognize that providing effective safeguards is challenging, and many papers do
1005"
SAFEGUARDS,0.9378238341968912,"not require this, but we encourage authors to take this into account and make a best
1006"
SAFEGUARDS,0.9386873920552677,"faith effort.
1007"
LICENSES FOR EXISTING ASSETS,0.9395509499136442,"12. Licenses for existing assets
1008"
LICENSES FOR EXISTING ASSETS,0.9404145077720207,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1009"
LICENSES FOR EXISTING ASSETS,0.9412780656303973,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1010"
LICENSES FOR EXISTING ASSETS,0.9421416234887737,"properly respected?
1011"
LICENSES FOR EXISTING ASSETS,0.9430051813471503,"Answer: [Yes]
1012"
LICENSES FOR EXISTING ASSETS,0.9438687392055267,"Justification: Yes.
1013"
LICENSES FOR EXISTING ASSETS,0.9447322970639033,"Guidelines:
1014"
LICENSES FOR EXISTING ASSETS,0.9455958549222798,"• The answer NA means that the paper does not use existing assets.
1015"
LICENSES FOR EXISTING ASSETS,0.9464594127806563,"• The authors should cite the original paper that produced the code package or dataset.
1016"
LICENSES FOR EXISTING ASSETS,0.9473229706390328,"• The authors should state which version of the asset is used and, if possible, include a
1017"
LICENSES FOR EXISTING ASSETS,0.9481865284974094,"URL.
1018"
LICENSES FOR EXISTING ASSETS,0.9490500863557858,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1019"
LICENSES FOR EXISTING ASSETS,0.9499136442141624,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1020"
LICENSES FOR EXISTING ASSETS,0.9507772020725389,"service of that source should be provided.
1021"
LICENSES FOR EXISTING ASSETS,0.9516407599309153,"• If assets are released, the license, copyright information, and terms of use in the
1022"
LICENSES FOR EXISTING ASSETS,0.9525043177892919,"package should be provided. For popular datasets, paperswithcode.com/datasets
1023"
LICENSES FOR EXISTING ASSETS,0.9533678756476683,"has curated licenses for some datasets. Their licensing guide can help determine the
1024"
LICENSES FOR EXISTING ASSETS,0.9542314335060449,"license of a dataset.
1025"
LICENSES FOR EXISTING ASSETS,0.9550949913644214,"• For existing datasets that are re-packaged, both the original license and the license of
1026"
LICENSES FOR EXISTING ASSETS,0.9559585492227979,"the derived asset (if it has changed) should be provided.
1027"
LICENSES FOR EXISTING ASSETS,0.9568221070811744,"• If this information is not available online, the authors are encouraged to reach out to
1028"
LICENSES FOR EXISTING ASSETS,0.957685664939551,"the asset’s creators.
1029"
NEW ASSETS,0.9585492227979274,"13. New Assets
1030"
NEW ASSETS,0.959412780656304,"Question: Are new assets introduced in the paper well documented and is the documentation
1031"
NEW ASSETS,0.9602763385146805,"provided alongside the assets?
1032"
NEW ASSETS,0.961139896373057,"Answer: [NA]
1033"
NEW ASSETS,0.9620034542314335,"Justification: The paper does not release new assets.
1034"
NEW ASSETS,0.9628670120898101,"Guidelines:
1035"
NEW ASSETS,0.9637305699481865,"• The answer NA means that the paper does not release new assets.
1036"
NEW ASSETS,0.9645941278065631,"• Researchers should communicate the details of the dataset/code/model as part of their
1037"
NEW ASSETS,0.9654576856649395,"submissions via structured templates. This includes details about training, license,
1038"
NEW ASSETS,0.966321243523316,"limitations, etc.
1039"
NEW ASSETS,0.9671848013816926,"• The paper should discuss whether and how consent was obtained from people whose
1040"
NEW ASSETS,0.968048359240069,"asset is used.
1041"
NEW ASSETS,0.9689119170984456,"• At submission time, remember to anonymize your assets (if applicable). You can either
1042"
NEW ASSETS,0.9697754749568221,"create an anonymized URL or include an anonymized zip file.
1043"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9706390328151986,"14. Crowdsourcing and Research with Human Subjects
1044"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9715025906735751,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1045"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9723661485319517,"include the full text of instructions given to participants and screenshots, if applicable, as
1046"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9732297063903281,"well as details about compensation (if any)?
1047"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9740932642487047,"Answer: [NA]
1048"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9749568221070811,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1049"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758203799654577,"Guidelines:
1050"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766839378238342,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1051"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9775474956822107,"human subjects.
1052"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9784110535405872,"• Including this information in the supplemental material is fine, but if the main contribu-
1053"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792746113989638,"tion of the paper involves human subjects, then as much detail as possible should be
1054"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801381692573402,"included in the main paper.
1055"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9810017271157168,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1056"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9818652849740933,"or other labor should be paid at least the minimum wage in the country of the data
1057"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9827288428324698,"collector.
1058"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835924006908463,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1059"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844559585492227,"Subjects
1060"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853195164075993,"Question: Does the paper describe potential risks incurred by study participants, whether
1061"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861830742659758,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1062"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9870466321243523,"approvals (or an equivalent approval/review based on the requirements of your country or
1063"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9879101899827288,"institution) were obtained?
1064"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887737478411054,"Answer: [NA]
1065"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896373056994818,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1066"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905008635578584,"Guidelines:
1067"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913644214162349,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1068"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922279792746114,"human subjects.
1069"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930915371329879,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1070"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9939550949913645,"may be required for any human subjects research. If you obtained IRB approval, you
1071"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948186528497409,"should clearly state this in the paper.
1072"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956822107081175,"• We recognize that the procedures for this may vary significantly between institutions
1073"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965457685664939,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1074"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974093264248705,"guidelines for their institution.
1075"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998272884283247,"• For initial submissions, do not include any information that would break anonymity (if
1076"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991364421416234,"applicable), such as the institution conducting the review.
1077 1078"
