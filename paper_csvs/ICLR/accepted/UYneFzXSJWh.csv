Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0011587485515643105,"When transferring a pretrained model to a downstream task, two popular methods
are full ﬁne-tuning (updating all the model parameters) and linear probing (updat-
ing only the last linear layer—the “head”). It is well known that ﬁne-tuning leads
to better accuracy in-distribution (ID). However, in this paper, we ﬁnd that ﬁne-
tuning can achieve worse accuracy than linear probing out-of-distribution (OOD)
when the pretrained features are good and the distribution shift is large. On 10
distribution shift datasets (BREEDS-Living17, BREEDS-Entity30, DomainNet,
CIFAR →STL, CIFAR-10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A,
ImageNet-Sketch), ﬁne-tuning obtains on average 2% higher accuracy ID but
7% lower accuracy OOD than linear probing. We show theoretically that this
tradeoff between ID and OOD accuracy arises even in a simple setting: ﬁne-tuning
overparameterized two-layer linear networks. Our analysis suggests that the easy
two-step strategy of linear probing then full ﬁne-tuning (LP-FT), sometimes used
as a ﬁne-tuning heuristic, combines the beneﬁts of both ﬁne-tuning and linear
probing. Empirically, LP-FT outperforms both ﬁne-tuning and linear probing on
the above datasets (1% better ID, 10% better OOD than full ﬁne-tuning)."
INTRODUCTION,0.002317497103128621,"1
INTRODUCTION"
INTRODUCTION,0.0034762456546929316,"Pretraining a model on a large dataset before transferring to a downstream task’s training data
substantially improves accuracy over training from scratch—for example, pretraining a ResNet-50
on unlabeled ImageNet boosts accuracy on CIFAR-10 from 94% to 98% (Chen et al., 2020a;b).
High-stakes applications such as poverty mapping in under-resourced countries (Jean et al., 2016),
self-driving cars (Yu et al., 2020), and medical diagnosis (AlBadawy et al., 2018), require models
that also generalize to circumstances not seen in the training distribution. In addition to testing on
data drawn from the downstream task’s training distribution (in-distribution; ID), it is increasingly
important to test on data distributions unseen during training (out-of-distribution; OOD)."
INTRODUCTION,0.004634994206257242,"After initializing with a pretrained model, two popular transfer methods are ﬁne-tuning (running
gradient descent on all the model parameters), and linear probing (tuning the head but freezing
lower layers). In the ID setting it is well known that ﬁne-tuning leads to better accuracy than linear
probing (Kornblith et al., 2019; Zhai et al., 2020; He et al., 2020), and even when testing OOD,
prior work usually ﬁne-tunes all parameters of their model (Hendrycks et al., 2019a; Miller et al.,
2021; Andreassen et al., 2021). Intuitively, ﬁne-tuning all layers of a network can improve pretrained
features by adapting them to the speciﬁc task, while linear probing freezes these features."
INTRODUCTION,0.005793742757821553,"In this work, we investigate the OOD accuracy of ﬁne-tuning and linear probing and ﬁnd that
surprisingly, ﬁne-tuning can do worse than linear probing in the presence of a large distribution shift.
We experiment on ten distribution shift benchmarks (BREEDS Living17, BREEDS Entity30, Do-
mainNet, CIFAR →STL, CIFAR10.1, FMoW Geo-shift, ImageNetV2, ImageNet-R, ImageNet-A,
ImageNet-Sketch), initializing with good pretrained features from MoCo-v2 (Chen et al., 2020b) and
CLIP (Radford et al., 2021). While both methods offer gains over training from scratch, ﬁne-tuning
improves the average ID accuracy relative to linear probing from 83% to 85% but brings down the
OOD accuracy from 66% to 59% (Figure 1)."
INTRODUCTION,0.006952491309385863,"When and why does ﬁne-tuning underperform linear probing? We theoretically consider ﬁne-tuning
a two-layer linear network in an overparameterized regression setting where the feature extractor
layer has been pretrained to map high-dimensional inputs to useful, lower-dimensional, features. We
prove that ﬁne-tuning is worse than linear probing on directions outside the span of the training data
when using “good” pretrained features. Even with an inﬁnitesimally small learning rate, ﬁne-tuning
distorts pretrained features—the features of ID training data are updated while those of OOD data"
INTRODUCTION,0.008111239860950173,Published as a conference paper at ICLR 2022
INTRODUCTION,0.009269988412514484,"Figure 1:
Given a good feature extractor (top-left), a randomly initialized head is added to map
features to outputs and we can (a) ﬁne-tune all the model parameters or (b) linear probe, which
freezes the feature extractor and trains only the head. We run experiments on ten distribution shifts.
Fine-tuning does well when the test example is sampled from the ﬁne-tuning distribution (ID), but
can underperform on test examples sampled from OOD distributions (when the distribution shift is
large). (c) Our theory indicates that ﬁne-tuning can distort the pretrained feature extractor and lead to
poor OOD accuracy, but initializing with a linear probed head can ﬁx this—empirically LP-FT gets
better accuracies both ID and OOD."
INTRODUCTION,0.010428736964078795,"change less. Since the head and feature extractor are simultaneously optimized during ﬁne-tuning
to a conﬁguration that works well on ID training data, the head only accomodates the distorted
features of ID points and performs poorly (relative to linear probing) on the less changed features
of OOD points. Interestingly, we show that this feature distortion issue cannot be simply ﬁxed by
early stopping—throughout the entire process of ﬁne-tuning, we never pass through parameters that
do well OOD (relative to linear probing). On the other hand, given “good” features, linear probing
extrapolates better OOD because it preserves pretrained features, but does worse than ﬁne-tuning ID
because linear probing cannot adapt the features to the downstream task."
INTRODUCTION,0.011587485515643106,"Technical challenges. Existing theoretical work on transfer learning focuses on linear probing (Wu
et al., 2020; Tripuraneni et al., 2020; Du et al., 2020). In contrast, analyses of ﬁne-tuning is scarce and
challenging because it requires understanding the training dynamics, instead of only the loss function
and its global minimizers. In fact, ﬁne-tuning and training from scratch optimize the same training
loss and only differ in their initializations (pretrained vs random).
A mathematical analysis that
distinguishes them needs to capture properties of the different minima that these algorithms converge
to, a phenomenon that is sometimes theoretically referred to as the implicit regularization effect of
initialization (Neyshabur et al., 2014). Accordingly, our analysis reasons about the parameters that
gradient methods pass through starting from the pretrained initialization, which is challenging be-
cause this is a non-convex optimization problem and there is no known closed form for this trajectory.
Two-layer linear networks are widely studied in the literature on implicit regularization (Saxe et al.,
2014; Gunasekar et al., 2017; Gidel et al., 2019; Arora et al., 2018). However, they analyze random
and often small initializations, which don’t capture pretraining."
INTRODUCTION,0.012746234067207415,"Algorithmic implications. Our theory shows that ﬁne-tuning underpeforms because when trying to
ﬁt ID training data with a randomly initialized head, the feature extractor changes signiﬁcantly for
ID examples, making features for ID and OOD examples largely inconsistent. This can be ﬁxed by
initializing with a good head that does not need to be updated much during ﬁne-tuning, reducing how
much the feature extractor changes. This suggests a simple two-step strategy of ﬁrst linear probing to
ﬁnd a good head and then full ﬁne-tuning (LP-FT). Empirically, LP-FT outperforms ﬁne-tuning and
linear probing, both ID and OOD. Even on CIFAR-10.1 (small distribution shift), where ﬁne-tuning
is better for both ID and OOD, we ﬁnd LP-FT outperforms ﬁne-tuning on both metrics. LP-FT
and vanilla ﬁne-tuning use similar amounts of compute because the ﬁrst step of linear probing is
relatively very cheap. Prior work has used LP-FT (Levine et al., 2016; Kanavati & Tsuneki, 2021)
(or variants such as layerwise ﬁne-tuning (Howard & Ruder, 2018) or larger learning rates for the
head layer (Prabhu et al., 2021))—however it has not been used for robustness / OOD accuracy,
and we show that it addresses the ID-OOD tradeoff theoretically and empirically. Note that LP-FT
is not meant to be a SOTA method but rather a simple, principled way to get good ID and OOD
accuracy—we hope our analysis inspires even better methods for robust ﬁne-tuning."
INTRODUCTION,0.013904982618771726,Published as a conference paper at ICLR 2022
INTRODUCTION,0.015063731170336037,"Empirical validation. Finally, we ﬁnd that ﬁne-tuning fails and LP-FT works, for the reasons
predicted by our feature distortion theory: (1) ﬁne-tuning changes the features for ID examples
more than for OOD examples, leading to distortions; (2) LP-FT indeed changes both ID and OOD
features 10−100× less than ﬁne-tuning does; (3) LP-FT gets the best of both worlds, achieving better
accuracies than ﬁne-tuning and linear probing both ID and OOD (Figure 1)."
SETUP,0.016222479721900347,"2
SETUP"
SETUP,0.01738122827346466,"Task and evaluation. Given training examples sampled from some distribution Pid, our goal is to
learn a predictor f : Rd →Y to map inputs x ∈Rd to outputs y ∈Y. We evaluate predictors on
their standard “in-distribution” (ID) performance Lid on new test samples drawn from Pid that the
training data is also sampled from. We also evaluate classiﬁers on their “out-of-distribution” (OOD)
performance Lood on test samples drawn from a new distribution Pood that is different from Pid.
Formally, for some loss function ℓ, we evaluate classiﬁers on:"
SETUP,0.01853997682502897,"Lid(f)=
E
(x,y)∼Pid
[ℓ(f(x),y)] and Lood(f)=
E
(x,y)∼Pood
[ℓ(f(x),y)].
(2.1)"
SETUP,0.019698725376593278,"Models. In this work, we focus on predictors that leverage pretrained representations. We parameter-
ize the ﬁnal predictor f as follows: given features gB(x) ∈Rk for some feature extractor parameters
B ∈B, and a linear “head” v ∈V, we have fv,B(x) = v⊤gB(x). In our experiments (Section 4), gB
is a deep network and in our theory (Section 3), gB is a linear projection."
SETUP,0.02085747392815759,"We assume access to some initial pretrained feature extractor B0 that is obtained by training on
potentially large amounts of data from a distribution that contains unlabeled or weakly supervised x
inputs from Pid and Pood. We focus on two popular methods to learn a predictor fv,B given training
data from Pid: (i) linear probing where B = B0 and the linear head is obtained by minimizing some
loss (e.g., logistic loss for classiﬁcation, squared loss for regression) on the training data, and (ii)
ﬁne-tuning where both v and B are updated by performing gradient descent on some loss on the
training data with B initialized at B0."
SETUP,0.0220162224797219,"3
THEORY: FINE-TUNING DISTORTS PRETRAINED FEATURES"
SETUP,0.023174971031286212,"Our goal is to understand under what conditions ﬁne-tuning does worse than linear probing
out-of-distribution (OOD). We consider a linear setting (feature extractor gB is linear) where the
pretrained features are “good” and the OOD shift is large (Section 3.1). We prove our main result:
that ﬁne-tuning, in which all model parameters are updated, distorts features and gets suboptimal
OOD error (Section 3.2, Theorem 3.2). We use this result to show that linear probing gets better OOD
error but worse ID error than ﬁne-tuning (Section 3.3). Finally, we explain why linear probing then
ﬁne-tuning can mitigate this ID-OOD tradeoff (Section 3.4)."
SETUP,0.02433371958285052,"Our analysis handles two key challenges which distinguishes it from prior work on transfer learning
in linear models (Wu et al., 2020; Tripuraneni et al., 2020; Du et al., 2020; Xie et al., 2021a). Prior
work focuses on linear probing, while we study ﬁne-tuning where the resulting optimization problem
is non-convex. We also study overparameterized models where the training loss alone does not
determine test performance—this captures the fact that both training neural networks from scratch
and ﬁne-tuning them have the same training loss but very different test performance. However, it also
makes the analysis challenging because we need to reason about the trajectory of gradient methods
starting from a pretrained initialization, which has no known closed form."
LINEAR OVERPARAMETERIZED SETTING,0.02549246813441483,"3.1
LINEAR OVERPARAMETERIZED SETTING
For our analysis, we focus on regression, where Y =R and ℓ(by,y)=(by−y)2 is the squared loss."
LINEAR OVERPARAMETERIZED SETTING,0.026651216685979143,"Models. Recall from Section 2 that we parameterize predictors in terms of the feature extractor
and head parameters. In this section, we study models where the feature extractor is linear, i.e.
fv,B(x)=v⊤Bx where B ∈B=Rk×d, and v∈V =Rk."
LINEAR OVERPARAMETERIZED SETTING,0.027809965237543453,"Good pretrained features. For simplicity, we assume the models are well-speciﬁed i.e. y =v⊤
⋆B⋆x
where v⋆∈Rk and B⋆∈Rk×d. 1 Note that B⋆and v⋆are only unique up to rotations, i.e., for any
rotation matrix U, (Uv⋆)T (UB⋆)x=vT
⋆B⋆x. As in prior work (Tripuraneni et al., 2020) suppose B⋆"
LINEAR OVERPARAMETERIZED SETTING,0.028968713789107765,"1Our main contribution, analysis of ﬁne-tuning (Theorem 3.2), does not require well-speciﬁcation. We
compare FT with LP by adapting earlier work on linear probing which requires well-speciﬁcation."
LINEAR OVERPARAMETERIZED SETTING,0.030127462340672075,Published as a conference paper at ICLR 2022
LINEAR OVERPARAMETERIZED SETTING,0.031286210892236384,"and B0 have been orthogonalized to have orthonormal rows. Suppose we have a pretrained feature
extractor B0 close to B⋆, so d(B0,B⋆) ≤ϵ where the distance d is deﬁned as (where the min is over
rotation matrices U ∈Rk×k):"
LINEAR OVERPARAMETERIZED SETTING,0.03244495944380069,"d(B,B′)=min
U ∥B−UB′∥2.
(3.1)"
LINEAR OVERPARAMETERIZED SETTING,0.03360370799536501,"Training data. Let X ∈Rn×d,X ̸=0 be a matrix encoding n training examples from Pid where each
of the n rows is a training input. Let Y ∈Rn be the corresponding outputs. Let S = rowspace(X)
be the m-dimensional subspace spanning the training examples. We consider an overparameterized
setting where 1≤m<d−k. Intuitively, the input dimension d is high (e.g., 10K), feature dimension
k is lower (e.g., 100) and m is in the middle (e.g., 5K)."
LINEAR OVERPARAMETERIZED SETTING,0.03476245654692932,"Large OOD shift. We assume that the OOD data contains examples outside the span of the training
data. Formally, let Pood have second moment Σ=E[xx⊤] where x∼Pood, for invertible Σ."
LINEAR OVERPARAMETERIZED SETTING,0.03592120509849363,"Training methods. Given training data and a pretrained feature extractor B0, we study the two
popular methods of linear probing (LP) and ﬁne-tuning (FT) to learn the ﬁnal predictor. Both methods
involve optimizing the training loss via gradient descent (or variants). In order to effectively analyze
these gradient based algorithms, we study vanishing step sizes leading to gradient ﬂows. Gradient
ﬂows can be thought of as a continuous time analogue of gradient based methods and have been
extensively studied in recent years as a way to understand gradient based methods (Gunasekar et al.,
2017; Arora et al., 2018; Du et al., 2018). Formally, for training loss bL(v,B) = ∥XB⊤v −Y ∥2
2, the
gradient ﬂow differential equations for LP and FT are as follows:"
LINEAR OVERPARAMETERIZED SETTING,0.03707995365005794,"∂tvft(t)=−∇v bL(vft(t),Bft(t)), ∂tBft(t)=−∇B bL(vft(t),Bft(t)),
(3.2)"
LINEAR OVERPARAMETERIZED SETTING,0.038238702201622246,"∂tvlp(t)=−∇v bL(vlp(t),B0), ∂tBlp(t)=0,
(3.3)"
LINEAR OVERPARAMETERIZED SETTING,0.039397450753186555,"initialized with Bft(0) = Blp(0) = B0 and vft(0) = vlp(0) = v0. In practice, the head parameter v0
is initialized randomly—our results hold for any standard random initialization (Glorot & Bengio,
2010), for example v0 ∼N(0,σ2I) for any σ2, or zero initialization where v0 = 0. Recall that the
initial value of the feature extractor B0 is obtained via pretraining."
LINEAR OVERPARAMETERIZED SETTING,0.04055619930475087,The ﬁnal LP and FT solutions are the limit points of the corresponding gradient ﬂows:
LINEAR OVERPARAMETERIZED SETTING,0.04171494785631518,"v∞
ft = lim
t→∞vft(t) and B∞
ft = lim
t→∞Bft(t),
(3.4)"
LINEAR OVERPARAMETERIZED SETTING,0.04287369640787949,"v∞
lp = lim
t→∞vlp(t) and B∞
lp = lim
t→∞Blp(t)=B0.
(3.5)"
FINE-TUNING DISTORTS PRETRAINED FEATURES,0.0440324449594438,"3.2
FINE-TUNING DISTORTS PRETRAINED FEATURES
The more common method of using a pretrained feature extractor is ﬁne-tuning (FT) which typically
improves ID performance relative to linear probing (LP). In this section, we show that FT can distort
features leading to poor OOD performance. We ﬁrst explain the key intuitions and then present our
formal theorem lower bounding the OOD error of FT (Section 3.2.2)."
KEY INTUITIONS,0.04519119351100811,"3.2.1
KEY INTUITIONS
We use two main observations to characterize when and why FT has higher OOD error than LP."
KEY INTUITIONS,0.046349942062572425,"1. Features get distorted: representations change only in the ID subspace (i.e., subspace spanned by
the training data) and are unchanged in the orthogonal subspace. To see this, we take the derivative
of the training loss bL(v,B)=∥XB⊤v−Y ∥2
2 with respect to the feature extractor parameter B:"
KEY INTUITIONS,0.047508690614136734,"∇B bL(v,B)=2v(Y −XB⊤v)⊤X.
(3.6)"
KEY INTUITIONS,0.04866743916570104,"By deﬁnition, if u is a direction orthogonal to the training subspace S = rowspace(X), then
∇B bL(v,B)u = 0, that is the gradient updates to B do not modify Bu for u ∈S⊥. However, the
gradient is non-zero for directions u in the ID subspace and the corresponding features Bu change
across the ﬁne-tuning process. We call this feature distortion: the features in some directions are
changed but not others. Next, we explain why this can lead to high OOD error."
KEY INTUITIONS,0.04982618771726535,"2. Distorted features can lead to higher OOD error. Consider a toy example (Figure 2) where
d = 2 and the dimensionality of the representations k = 1. The linear head v is a scalar quantity
that denotes how much the feature extractor B has to be scaled by. Suppose the ID-subspace is the
x-axis. There are different ways of ﬁtting the ID subspace depending on the feature extractors B as"
KEY INTUITIONS,0.05098493626882966,Published as a conference paper at ICLR 2022
KEY INTUITIONS,0.05214368482039398,"Figure 2: A toy version of our theory illustrating why ﬁne-tuning distorts features, with inputs in 2D.
Given input x, the ground truth output is y =w⊤
⋆x. The ID data is along the x-axis and the pretrained
feature extractor is B0. (a) Linear probing learns wlp, a scaling of the pretrained feature extractor that
gets the ID data correct (wlp and w⋆have the same x coordinate as indicated by the vertical dotted
line). (b) Fine-tuning updates the pretrained feature extractor along the ID data (so horizontally) to
get Bft, and then learns a scaling of these features that gets the ID data correct. While both methods
get ID data correct, ﬁne-tuning makes large errors perpendicular to the ID data, because ﬁne-tuning
updates B0 along the ID direction but not the perpendicular direction."
KEY INTUITIONS,0.05330243337195829,"shown in the Figure—both ﬁne-tuned and linear probed estimators match the true parameter in the
ID subspace (since wlp,wft,w⋆have the same projection on the x-axis). If the feature extractor were
optimal or scaled versions of the optimal, good performance on the ID subspace would translate to
good performance everywhere, even in directions orthogonal to the ID subspace. However, in FT,
the features change only for inputs in the ID subspace (see (1)) and thus the updated features are not
simply scaled but distorted. In Figure 2, this corresponds to the feature extractor B0 changing along
the x-axis. In this case even if the ID error is low, error in directions orthogonal to the ID subspace
can be high, leading to high OOD error."
KEY INTUITIONS,0.054461181923522596,"The only way the pretrained features are not distorted and only scaled during FT is if the initial
feature extractor B0 is exactly aligned with the ID subspace. In Figure 2, if B0 is along the x-axis (the
ID subspace), then updating the features exclusively along the x-axis would simply scale the initial
features. In this case linear probing and ﬁne-tuning will have identical behavior. However, if the angle
between B0 and the x-axis is non-zero, the updates would lead to distortions. In high dimensions, we
measure the alignment between B0 and the ID subspace with the largest principal angle:"
KEY INTUITIONS,0.055619930475086905,"Deﬁnition 3.1 (largest principal angle). Let A and B be arbitrary subspaces, and E and F be
matrices with orthonormal columns that span A and B respectively, with r=min(dim(A),dim(B)).
Then cosθmax(A,B)=σr(E⊤F), which is the r-th largest singular value of E⊤F."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.056778679026651215,"3.2.2
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING
Our main theorem lower bounds the OOD error of ﬁne-tuning outside the span of the training data."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.05793742757821553,"Theorem
3.2.
In
the
overparameterized
linear
setting,
let
S⊥
=
rowspace(X)⊥,
R0 =rowspace(B0), and v⋆,B⋆be the optimal parameters with w⋆=B⋆v⋆. If cosθmax(R0,S⊥)>0,
then for all time steps t, the OOD error of the ﬁne-tuning iterates (Bft(t),vft(t)) is lower bounded: p"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.05909617612977984,"Lood(vft(t),Bft(t))≥
p"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.06025492468134415,"σmin(Σ)
cosθmax(R0,S⊥)
√"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.06141367323290846,"k
min(φ,φ2/∥w⋆∥2)"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.06257242178447277,"(1+∥w⋆∥2)2
−ϵ

,
(3.7)"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.06373117033603708,"where φ2 =|(v⊤
0 v⋆)2−(v⊤
⋆v⋆)2| is deﬁned to be inital head alignment error and ϵ≥d(B0,B⋆) is the
error in the pretrained feature extractor."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.06488991888760139,"Proof sketch. Since the features do not change for examples in S⊥(perpendicular to the training
data), we show that in order to achieve low error on S⊥the linear head vft(t) would have to become
very similar to the optimal v⋆at some time t. The head initialization v0 is random (or zero) and likely
to be far from v⋆(measured by the alignment error φ), so the head would have to change a lot to get
close to v⋆. As we see from the ﬁne-tuning gradient ﬂow (3.2), vft(t) and Bft(t) change in a “coupled”
manner, and a “balancedness” invariant in Du et al. (2018) holds across the ﬁne-tuning trajectory.
Correspondingly, if vft(t) changes a lot and gets close to v⋆, the features Bft(t) also change a lot for"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.0660486674391657,Published as a conference paper at ICLR 2022
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.06720741599073002,"examples in S—we show that this would lead to high error on examples in S. Either way, ﬁne-tuning
would get some subspace (S or S⊥) of examples wrong, leading to high OOD error. The full proof
appears in Appendix A."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.06836616454229433,"Interpretations of various quantities. Quality of pretrained features (ϵ). To unpack the bound
consider a special case where the pretrained features are perfect (ϵ = 0). With perfect features,
Proposition A.21 shows that linear probing gets zero OOD error.
Theorem 3.2 shows that
Lood(vft(t),Bft(t))>0 at all times t—so ﬁne-tuning underperforms when the features are perfect."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.06952491309385864,"Alignment error of random head initialization (φ2). The lower bound (Equation A.14) increases as φ2
increases, because the gradient updates to the head and feature extractor are coupled. If the head were
somehow initialized perfectly at v⋆, ﬁne-tuning updates may not increase the OOD error. However,
when the head is randomly initialized as is standard in ﬁne-tuning, the alignment error is high, leading
to high OOD error. We use this insight in Section 3.4 to show that better head initialization (via linear
probing) improves OOD performance of ﬁne-tuning."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.07068366164542295,"3.3
LINEAR PROBING VS. FINE-TUNING
In this section, we use our main theorem on ﬁne-tuning (Theorem 3.2) and adapt prior work on
linear probing to show that linear probing is better than ﬁne-tuning OOD, but worse ID, when the ID
distribution has density on a lower m<d dimensional subspace S, and B0 is close to B⋆."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.07184241019698726,"Assumption 3.3 (ID subspace assumption). We assume that the ID data lies on an m-dimensional
subspace S where k < m < d −k, and we have n ≥m training examples. Formally, let Pz be a
distribution on Rm which has density, and let the columns of F ∈Rd×m form an orthonormal basis
for S. Then Pid has the distribution of Fz where z ∼Pz."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.07300115874855156,Recall that the ID error is the expected mean-squared error over the ID distribution Pid:
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.07415990730011587,"Lid(v,B)= E
x∼Pid
[(v⊤
⋆B⋆x−v⊤Bx)2]
(3.8)"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.07531865585168018,"OOD comparison: Under mild non-degeneracy conditions, we show that as the feature extractor
error ϵ goes to 0, linear probing does much better than ﬁne-tuning OOD: the ratio of the losses goes to
0. The non-degeneracy conditions are similar to Section 3.2—we require that the training data cannot
be exactly in the same direction or orthogonal to the pretrained features, formally that cosθmax(R∗,S)
and cosθmax(R∗,S⊥) are not 0 where R∗=rowspace(B⋆)."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.07647740440324449,"Theorem 3.4 (Informal version of Theorem A.9). In the linear overparameterized setting, under the
ID subspace assumption (Assumption 3.3), if cosθmax(R∗,S) ̸= 0 and cosθmax(R∗,S⊥) ̸= 0 where
R∗=rowspace(B⋆), then,"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.0776361529548088,"Lood(v∞
lp ,B0)
Lood(vft(t),Bft(t))"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.07879490150637311,"p→0,as B0 →B⋆.
(3.9)"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.07995365005793743,"This holds for all times t for FT (and therefore also for the limit v∞
ft ,B∞
ft ) and the LP iterates converge
to v∞
lp ,B0 as a result of the gradient ﬂow on a convex problem."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.08111239860950174,"Intuitively, if the pretrained features are good, LP learns a near optimal linear head which has small
OOD error (Lemma A.15) but ﬁne-tuning has high OOD error (Theorem 3.2). We give a more formal
version of Theorem 3.4 and a proof in Appendix A.3."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.08227114716106605,"ID comparison: When the pretrained features have some error, we show that ﬁne-tuning does
better than linear probing ID because ﬁne-tuning can update the features to ﬁt the ID data. The
non-degeneracy condition on Raug below is similar to our previous results, and holds with probability
1 if the ID subspace is chosen randomly, from Lemma A.17."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.08342989571263036,"Proposition 3.5. In the linear overparameterized setting, under the ID subspace assumption
(Assumption 3.3), let R0 = rowspace(B0), and Raug = Span({w⋆} ∪R0). Suppose w⋆̸∈R0,
cosθmax(S,Raug) ̸= 0, and that ﬁne-tuning converges to a local minimum of its loss, then ﬁne-tuning
does better ID almost surely: Lid(v∞
ft ,B∞
ft ) < Lid(v∞
lp ,B0) with probability 1 (over the randomness
of the training examples)."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.08458864426419467,"To summarize, we proved that there are tradeoffs between ID and OOD error: FT has lower ID error
but higher OOD error than LP. In the next section, we extend our theoretical insights to illustrate why
a simple variant of FT may mitigate such tradeoffs."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.08574739281575898,Published as a conference paper at ICLR 2022
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.08690614136732329,"3.4
LINEAR PROBING THEN FINE-TUNING: A SIMPLE VARIANT TO MITIGATE TRADEOFFS
The advantage of ﬁne-tuning is it can adapt the feature extractor to ﬁt the downstream task. Can we
keep this beneﬁt while ensuring that our OOD error is low when we have good pretrained features?"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.0880648899188876,"Going back to Theorem 3.2, we see that the alignment error in the head initialization
φ2 = |(v⊤
0 v⋆)2 −(v⊤
⋆v⋆)2| plays an important role. The issue with FT was that under random
or zero initialization, φ2 is usually large and since the gradient updates to the feature extractor param-
eter are coupled with that of the head parameter, the features get distorted in a manner that increases
the OOD error. This suggests that we should use a better head initialization—one obtained from linear
probing. If the pretrained features are decent, a linear probed head would be much better aligned with
v⋆allowing the features to be updated in a manner that does not increase the OOD error much."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.08922363847045191,"We formally prove this intuition in a simple setting where we have perfect pretrained features. Note
that in this case, linear probing alone gets zero OOD error—so Proposition 3.6 is just a ﬁrst cut result
to illustrate that if initialized well, full ﬁne-tuning does not distort features."
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.09038238702201622,"Proposition 3.6. Given perfect pretrained features B0 = UB⋆for some rotation U. Let R0 =
rowspace(B0). Under the non-degeneracy conditions cosθmax(R0,S)̸=0,cosθmax(R0,S⊥)̸=0:"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.09154113557358054,"∀t,Lood(Bft(t)⊤vft(t))>0, if v0 ∼N(0,σ2I) is randomly initialized (FT),
(3.10)"
GENERAL RESULT ON THE OOD ERROR OF FINE-TUNING,0.09269988412514485,"∀t,Lood(Bft(t)⊤vft(t))=0, if v0 is initialized to v∞
lp (LP-FT).
(3.11)"
EXPERIMENTS,0.09385863267670916,"4
EXPERIMENTS"
EXPERIMENTS,0.09501738122827347,"We run experiments on ten benchmark datasets with deep neural networks and see that given good
pretrained features, ﬁne-tuning (FT) does better ID but worse OOD than linear probing (LP). As
predicted by the theory, we ﬁnd that LP-FT does better than both methods. Finally, we see that a
number of predictions from the feature distortion theory hold up in practice. For more details on
datasets, pretraining models, and experiment protocols, see Appendix B."
EXPERIMENTS,0.09617612977983778,"We use standard distribution shift datasets: DomainNet (Peng et al., 2019; Tan et al., 2020),
BREEDS-Living-17 (Santurkar et al., 2020), BREEDS-Entity-30 (Santurkar et al., 2020),
CIFAR-10 →STL (Krizhevsky, 2009; Coates et al., 2011; French et al., 2018), CIFAR-10 →
CIFAR-10.1 (Recht et al., 2018), ImageNet-1K (Russakovsky et al., 2015)—where the OOD
test sets are ImageNetV2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2020), ImageNet-
A (Hendrycks et al., 2019b), and ImageNet-Sketch (Wang et al., 2019)—, and FMoW Geo-shift
which is adapted from the satellite remote sensing dataset Functional Map of the World (Christie
et al., 2018; Koh et al., 2021). See Appendix B for more details on the datasets."
EXPERIMENTS,0.09733487833140209,"Pretraining and models. We use a CLIP pretrained ViT-B/16 for ImageNet. For the other datasets
we use a ResNet-50 architecture and consider a diverse range of pretraining methods and datasets:
MoCo-v2 (Chen et al., 2020b), CLIP (Radford et al., 2021), and MoCo-TP (Ayush et al., 2020). In
Appendix B, we also show results for a CLIP-ViT-B/16 and more ﬁne-tuning baselines on Living-17."
LINEAR PROBING VS FINE-TUNING,0.0984936268829664,"4.1
LINEAR PROBING VS FINE-TUNING
Experiment protocols.
We initialize with the pretrained model, and ﬁne-tune or linear probe
on ID training examples. For ﬁne-tuning on each dataset we swept over 6 learning rates, using
a cosine learning rate schedule and batch size of 64. We early stop and choose the best learning
rate using ID validation accuracy. For linear probing we train an ℓ2-regularized logistic regression
classiﬁer on frozen features from the penultimate layer of the pretrained model, selecting the best
ℓ2-regularization hyperparameter based on ID validation accuracy. For all methods, we run each
hyperparameter conﬁguration 3 times (with different random seeds), and take the average accuracy.
We used a slightly different protocol for ImageNet because the dataset is much larger and running
these experiments involves more computational resources: we used a batch size of 128, swept over 3
learning rates for both ﬁne-tuning and linear probing (we did not sweep over ℓ2-regularization), and
ran each hyperparameter conﬁguration once. In all cases, OOD data was only used for evaluation."
LINEAR PROBING VS FINE-TUNING,0.0996523754345307,"Results. Fine-tuning (FT) does better than linear probing (LP) on 5 out of 6 ID datasets (average
accuracy of 85.1% for FT vs. 82.9% for LP, see Table 1). This is consistent with prior work and
intuitions. However, linear probing does better on 8 out of 10 OOD datasets (average accuracy of
66.2% for LP vs. 59.3% for FT, see Table 2)—LP does better on all datasets except CIFAR-10.1
and ImageNetV2, where the OOD is designed to closely replicate the ID dataset. This matches"
LINEAR PROBING VS FINE-TUNING,0.10081112398609501,Published as a conference paper at ICLR 2022
LINEAR PROBING VS FINE-TUNING,0.10196987253765932,"Table 1:
ID accuracies with 90% conﬁdence intervals over 3 runs—ﬁne-tuning does better
than linear probing on all datasets except DomainNet (which could be because the version of the
DomainNet training dataset from Tan et al. (2020) is fairly small, with around 20K examples). LP-FT
does the best on all except FMoW where it is in between linear probing and ﬁne-tuning."
LINEAR PROBING VS FINE-TUNING,0.10312862108922363,"CIFAR-10
Ent-30
Liv-17
DomainNet
FMoW
ImageNet
Average"
LINEAR PROBING VS FINE-TUNING,0.10428736964078796,"FT
97.3 (0.2)
93.6 (0.2)
97.1 (0.2)
84.5 (0.6)
56.5 (0.3)
81.7 (-)
85.1
LP
91.8 (0.0)
90.6 (0.2)
96.5 (0.2)
89.4 (0.1)
49.1 (0.0)
79.7 (-)
82.9
LP-FT
97.5 (0.1)
93.7 (0.1)
97.8 (0.2)
91.6 (0.0)
51.8 (0.2)
81.7 (-)
85.7"
LINEAR PROBING VS FINE-TUNING,0.10544611819235226,"Table 2:
OOD accuracies with 90% conﬁdence intervals over 3 runs. Linear probing does better
than ﬁne-tuning on all datasets except CIFAR-10.1 and ImageNetV2, where the ID and OOD are
similar (consistent with our theory). LP-FT does the best on all 10 datasets."
LINEAR PROBING VS FINE-TUNING,0.10660486674391657,"STL
CIFAR-10.1
Ent-30
Liv-17
DomainNet
FMoW"
LINEAR PROBING VS FINE-TUNING,0.10776361529548088,"FT
82.4 (0.4)
92.3 (0.4)
60.7 (0.2)
77.8 (0.7)
55.5 (2.2)
32.0 (3.5)
LP
85.1 (0.2)
82.7 (0.2)
63.2 (1.3)
82.2 (0.2)
79.7 (0.6)
36.6 (0.0)
LP-FT
90.7 (0.3)
93.5 (0.1)
62.3 (0.9)
82.6 (0.3)
80.7 (0.9)
36.8 (1.3)"
LINEAR PROBING VS FINE-TUNING,0.10892236384704519,"ImNetV2
ImNet-R
ImNet-Sk
ImNet-A
Average"
LINEAR PROBING VS FINE-TUNING,0.1100811123986095,"FT
71.5 (-)
52.4 (-)
40.5 (-)
27.8 (-)
59.3
LP
69.7 (-)
70.6 (-)
46.4 (-)
45.7 (-)
66.2
LP-FT
71.6 (-)
72.9 (-)
48.4 (-)
49.1 (-)
68.9"
LINEAR PROBING VS FINE-TUNING,0.11123986095017381,"our theoretical predictions. Our training datasets vary in size from 20K examples to over a million
examples, so LP does not appear to perform better than FT simply because of a small training set."
LINEAR PROBING VS FINE-TUNING,0.11239860950173812,"4.2
LINEAR PROBING THEN FINE-TUNING (LP-FT)
Experiment protocols. For LP-FT, we initialize the neural network head using the linear probed
solution, and then ﬁne-tune the model. LP-FT and ﬁne-tuning use similar compute because the linear
probing step is much faster than ﬁne-tuning. As with ﬁne-tuning, we swept over 6 learning rates,
early stopping using ID validation accuracy. For the ImageNet experiments we swept over 3 learning
rates, and explicitly ensured that LP-FT and ﬁne-tuning use exactly the same compute (we ran each
stage of LP-FT for half as many epochs as we ran vanilla ﬁne-tuning)."
LINEAR PROBING VS FINE-TUNING,0.11355735805330243,"Results. We ﬁnd that LP-FT gets the best accuracy ID (average: 85.7%) and OOD (average: 68.9%).
This is true for 5/6 ID and 10/10 OOD datasets—every dataset except FMoW ID, where LP-FT
is better than linear probing but worse than ﬁne-tuning. Since the ID accuracy on FMoW is low
(56.5%), this could be because the pretrained features are not good."
EXAMINING THE FEATURE DISTORTION THEORY,0.11471610660486674,"4.3
EXAMINING THE FEATURE DISTORTION THEORY
Early stopping does not mitigate feature distortion. Our theory predicts that ﬁne-tuning can do
worse OOD (than linear probing) throughout the process of ﬁne-tuning, and not just at the end. To
test this, we early stop each ﬁne-tuning method and choose the best learning rate based on OOD test
accuracy. As expected, ﬁne-tuning does improve a little, but linear probing (average accuracy: 67.1%)
is still better than ﬁne-tuning (average accuracy: 61.3%). See Appendix B for per-dataset results."
EXAMINING THE FEATURE DISTORTION THEORY,0.11587485515643106,"ID-OOD features get distorted from ﬁne-tuning.
The feature distortion theory predicts that
ﬁne-tuning changes features for ID examples more than for OOD examples, which is why ﬁtting a
head on ID examples performs poorly OOD. To test this, for each example x in Living-17 (results
for other datasets are in Appendix B), we took the Euclidean distance of the ResNet-50 features
before and after ﬁne-tuning: ∥gB(x)−gB0(x)∥2. As expected, the average distance for ID examples
(0.0188±0.0001) is more than for OOD examples (0.0167±0.0001). The theory also predicts that
LP-FT changes features less than ﬁne-tuning does. As expected, the average distance changed by
LP-FT both ID (0.0011±0.0001) and OOD (0.0009±0.0001) is 20× smaller than for ﬁne-tuning."
EXAMINING THE FEATURE DISTORTION THEORY,0.11703360370799537,"Pretrained features must be good, ID-OOD far apart.
Our theory says that linear probing does
better than ﬁne-tuning OOD, but only if the OOD and ID data are quite different, and the pretrained
features are good—otherwise ﬁne-tuning can do better OOD by adjusting the feature extractor ID."
EXAMINING THE FEATURE DISTORTION THEORY,0.11819235225955968,Published as a conference paper at ICLR 2022
EXAMINING THE FEATURE DISTORTION THEORY,0.11935110081112399,"Feature quality: We use a checkpoint of MoCo-v1 that got 10% worse accuracy (on ImageNet) and
compare linear probing and ﬁne-tuning on Living-17. With worse features, both methods do worse,
but ﬁne-tuning (96% ID, 71% OOD) does better than linear probing (92% ID, 66% OOD)."
EXAMINING THE FEATURE DISTORTION THEORY,0.1205098493626883,"ID ≈OOD: We ﬁne-tune / linear probe on CIFAR-10, and test on CIFAR-10.1, a dataset collected
using a similar protocol to CIFAR-10. As expected, ﬁne-tuning (92.3%) outperforms linear probing
OOD (82.7%). Even in this case, where we have no tradeoffs, LP-FT does the best (93.5%)."
RELATED WORK AND DISCUSSION,0.12166859791425261,"5
RELATED WORK AND DISCUSSION"
RELATED WORK AND DISCUSSION,0.12282734646581692,"Fine-tuning vs. linear probing. Fine-tuning (FT) and linear probing (LP) are popular transfer
learning algorithms.
There is substantial evidence of FT outperforming LP in-distribution (ID)
including recent large-scale investigations (Kornblith et al., 2019; Chen et al., 2021; Zhai et al., 2020;
Chen et al., 2020b) (the only notable exception is in Peters et al. (2019) where LP performs better than
FT when using ELMo representations, but worse using BERT). FT is therefore the method of choice
for improving accuracy, while LP is used to analyze properties of representations (Peters et al., 2018;
Belinkov et al., 2017; Hewitt & Manning, 2019). In our work, we ﬁnd that FT can underperform LP
especially when using high quality pretrained features in the presence of a large distribution shift.
There are a variety of other ﬁne-tuning heuristics (Ge & Yu, 2017; Guo et al., 2019; Zhang et al.,
2020; Zhu et al., 2020; Jiang et al., 2021; Aghajanyan et al., 2021)—combining our insights with
these ideas might lead to better methods."
RELATED WORK AND DISCUSSION,0.12398609501738123,"The beneﬁt of preserving pretrained features. Our work adds to growing evidence that lightweight
ﬁne-tuning, where only a small part of a pretrained model are updated, can perform better under
distribution shifts—and we give a theoretical grounding to why this might be the case. Zero-shot
language prompting in vision (Radford et al., 2021) and other lightweight ﬁne-tuning approaches in
NLP (Houlsby et al., 2019; Li & Liang, 2021; Xie et al., 2021b; Lester et al., 2021; Utama et al., 2021;
Zhou et al., 2021) have been shown to improve OOD performance. Andreassen et al. (2021) observe
that through the course of ﬁne-tuning, ID accuracy increases but OOD accuracy plateaus."
RELATED WORK AND DISCUSSION,0.12514484356894554,"Mitigating ID-OOD tradeoffs. While LP-FT has sometimes been used as a ﬁne-tuning heuris-
tic (Levine et al., 2016; Kanavati & Tsuneki, 2021; fastai), it has not been used for robustness / OOD
accuracy, and we show that it addresses the ID-OOD tradeoff theoretically and empirically. Tradeoffs
between ID and OOD accuracy are widely studied and prior work self-trains on large amounts of
unlabeled data to mitigate such tradeoffs (Raghunathan et al., 2020; Xie et al., 2021a; Khani & Liang,
2021). In contrast, LP-FT uses no extra unlabeled data and is a simple variant of ﬁne-tuning. In
concurrent and independent work, Wortsman et al. (2021) show that ensembling the weights of a
zero-shot and ﬁne-tuned model mitigates the ID-OOD tradeoff between these approaches, and this
method could be promising for our datasets as well."
RELATED WORK AND DISCUSSION,0.12630359212050984,"Theoretical analysis of transfer learning. Prior works on transfer learning mainly analyze linear
probing (Wu et al., 2020; Tripuraneni et al., 2020; Du et al., 2020). Recent works (Chua et al., 2021;
Shachaf et al., 2021) study ﬁne-tuning, but in the underparameterized regime (where there is a unique
global optimum) or assuming a balanced initialization. Prior works also focus on ID error, while we
analyzeOODerror. SeeSectionCforadditionalrelatedworkontheoryofoverparameterizedmodels."
RELATED WORK AND DISCUSSION,0.12746234067207415,"6
CONCLUSION."
RELATED WORK AND DISCUSSION,0.12862108922363846,"There is a strong trend towards leveraging pretrained models to improve downstream performance,
and whenever feasible, it is common to ﬁne-tune all model parameters. In this work, we show theo-
retically and empirically that preserving features might be important for robustness, and simpler ap-
proaches like linear probing can improve out-of-distribution (OOD) performance. This OOD gap be-
tween ﬁne-tuning and linear probing grows as the quality of pretrained features improve, so we believe
our results are likely to gain signiﬁcance over time with growing innovations and scale of pretraining."
RELATED WORK AND DISCUSSION,0.12977983777520277,"Finally, we showed LP-FT can mitigate tradeoffs between ID and OOD accuracy in our context. LP-
FT could be useful in other situations, for example in CLIP we could initialize the ﬁnal layer with the
zero-shot classiﬁer and then ﬁne-tune the entire model, as done in concurrent work (Wortsman et al.,
2021). In NLP, linear probing is not as good—here we could ﬁrst prompt-tune (Lester et al., 2021) and
then ﬁne-tune the entire model. LP-FT is just a ﬁrst step in leveraging the intuition from our theoretical
analysis and we hope that this work inspires new methods of leveraging powerful pretrained models."
RELATED WORK AND DISCUSSION,0.13093858632676708,Published as a conference paper at ICLR 2022
RELATED WORK AND DISCUSSION,0.1320973348783314,"Proofs and Reproducibility:
We include proofs for our theoretical results in Appendix A
and additional experiment details in Appendix B. Updated code is available at https:
//github.com/AnanyaKumar/transfer_learning and this CodaLab worksheet."
RELATED WORK AND DISCUSSION,0.1332560834298957,"Acknowledgements: We would like to thank Kumar Ayush and Burak Uzkent for MoCo checkpoints
pretrained on unlabeled FMoW images, Nilesh Tripuraneni for clariﬁcations on his work and refer-
ences on principal angles, Daniel Levy for useful suggestions on experiments to run, Niladri Chatterji,
Jeff Z. HaoChen, and Colin Wei for useful papers and comments on ﬁgures, Niladri Chatterji and
Kaidi Cao for reviewing the paper at ML paper swap, Kevin Yang for his help with analyzing differ-
ential equations, Tri Dao and Pang Wei Koh for help with writing, Suriya Gunasekar, Adam Kalai,
Simon Kornblith, Ting Chen, Sang Michael Xie, Albert Gu, and Kendrick Shen for useful discussions,
and Pang Wei Koh, Niladri Chatterji, and Tri Dao for suggestions on framing our results better."
RELATED WORK AND DISCUSSION,0.13441483198146004,"Ananya Kumar was supported by the Rambus Corporation Stanford Graduate Fellowship. Percy
Liang was supported by the Open Philanthropy Project and NSF Award Grant No.
1805310.
Aditi Raghunathan was supported by a Google PhD Fellowship and Open Philanthropy Project AI
Fellowship. Tengyu Ma acknowledges support of a Google Faculty Award, NSF IIS 2045685, the
Sloan Fellowship, JD.com, SAIL, and SDSI."
RELATED WORK AND DISCUSSION,0.13557358053302435,Published as a conference paper at ICLR 2022
REFERENCES,0.13673232908458866,REFERENCES
REFERENCES,0.13789107763615296,"Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal
Gupta. Better ﬁne-tuning by reducing representational collapse. In International Conference on
Learning Representations (ICLR), 2021."
REFERENCES,0.13904982618771727,"EA AlBadawy, A Saha, and MA Mazurowski. Deep learning for segmentation of brain tumors:
Impact of cross-institutional training and testing. Med Phys., 45, 2018."
REFERENCES,0.14020857473928158,"Anders Andreassen, Yasaman Bahri, Behnam Neyshabur, and Rebecca Roelofs. The evolution of
out-of-distribution robustness throughout ﬁne-tuning. arXiv, 2021."
REFERENCES,0.1413673232908459,"Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In International Conference on Machine Learning (ICML),
pp. 244–253, 2018."
REFERENCES,0.1425260718424102,"Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, M. Burke, D. Lobell, and Stefano
Ermon. Geography-aware self-supervised learning. arXiv, 2020."
REFERENCES,0.1436848203939745,"Peter L. Bartlett, Philip M. Long, Gt’abor Lugosi, and Alexander Tsigler. Benign overﬁtting in linear
regression. arXiv, 2019."
REFERENCES,0.14484356894553882,"Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do neural ma-
chine translation models learn about morphology? In Association for Computational Linguistics
(ACL), pp. 861–872, 2017."
REFERENCES,0.14600231749710313,"Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv, 2019."
REFERENCES,0.14716106604866744,"Koby Bibas, Yaniv Fogel, and Meir Feder. A new look at an old problem: A universal learning
approach to linear regression. In 2019 IEEE International Symposium on Information Theory
(ISIT), pp. 2304–2308, 2019."
REFERENCES,0.14831981460023175,"Tianle Cai, Ruiqi Gao, J. Lee, and Qi Lei. A theory of label propagation for subpopulation shift. In
International Conference on Machine Learning (ICML), 2021."
REFERENCES,0.14947856315179606,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning
(ICML), pp. 1597–1607, 2020a."
REFERENCES,0.15063731170336037,"Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv, 2020b."
REFERENCES,0.15179606025492468,"Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision
transformers. arXiv preprint arXiv:2104.02057, 2021."
REFERENCES,0.15295480880648898,"Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In
Computer Vision and Pattern Recognition (CVPR), 2018."
REFERENCES,0.1541135573580533,"Kurtland Chua, Qi Lei, and Jason D Lee. How ﬁne-tuning allows for effective meta-learning. arXiv
preprint arXiv:2105.02221, 2021."
REFERENCES,0.1552723059096176,"Adam Coates, Andrew Ng, and Honlak Lee.
An analysis of single-layer networks in unsuper-
vised feature learning. In Proceedings of the Fourteenth International Conference on Artiﬁcial
Intelligence and Statistics, volume 15, pp. 215–223, 2011."
REFERENCES,0.1564310544611819,"Simon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning
the representation, provably. arXiv, 2020."
REFERENCES,0.15758980301274622,"Simon Shaolei Du, Wei Hu, and Jason Lee. Algorithmic regularization in learning deep homoge-
neous models: Layers are automatically balanced. In Advances in Neural Information Processing
Systems (NeurIPS), 2018."
REFERENCES,0.15874855156431056,"fastai.
fastai tutorial on transfer learning.
https://github.com/fastai/course-v3/
blob/master/nbs/dl1/lesson1-pets.ipynb."
REFERENCES,0.15990730011587487,Published as a conference paper at ICLR 2022
REFERENCES,0.16106604866743918,"Geoff French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation.
In International Conference on Learning Representations, 2018."
REFERENCES,0.16222479721900349,"Weifeng Ge and Yizhou Yu. Borrowing treasures from the wealthy: Deep transfer learning through
selective joint ﬁne-tuning. In Computer Vision and Pattern Recognition (CVPR), 2017."
REFERENCES,0.1633835457705678,"Gauthier Gidel, Francis R. Bach, and Simon Lacoste-Julien.
Implicit regularization of discrete
gradient dynamics in deep linear neural networks. In Advances in Neural Information Processing
Systems (NeurIPS), 2019."
REFERENCES,0.1645422943221321,"Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In International Conference on Artiﬁcial Intelligence and Statistics, 2010."
REFERENCES,0.1657010428736964,"Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University Press,
2013."
REFERENCES,0.16685979142526072,"Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems (NeurIPS), pp. 6151–6159, 2017."
REFERENCES,0.16801853997682503,"Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and Rogerio Feris.
Spottune: Transfer learning through adaptive ﬁne-tuning.
In Computer Vision and Pattern
Recognition (CVPR), 2019."
REFERENCES,0.16917728852838934,"Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.
Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019."
REFERENCES,0.17033603707995365,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsuper-
vised visual representation learning. In Computer Vision and Pattern Recognition (CVPR), 2020."
REFERENCES,0.17149478563151796,"Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness
and uncertainty. In International Conference on Machine Learning (ICML), 2019a."
REFERENCES,0.17265353418308227,"Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial
examples. arXiv preprint arXiv:1907.07174, 2019b."
REFERENCES,0.17381228273464658,"Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul
Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.
The many faces of robustness: A critical analysis of out-of-distribution generalization.
arXiv
preprint arXiv:2006.16241, 2020."
REFERENCES,0.1749710312862109,"John Hewitt and Christopher D. Manning. A structural probe for ﬁnding syntax in word representa-
tions. In Association for Computational Linguistics (ACL), 2019."
REFERENCES,0.1761297798377752,"Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for
NLP. arXiv, 2019."
REFERENCES,0.1772885283893395,"Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation.
In Association for Computational Linguistics (ACL), 2018."
REFERENCES,0.17844727694090382,"Neal Jean, Marshall Burke, Michael Xie, W. Matthew Davis, David B. Lobell, and Stefano Ermon.
Combining satellite imagery and machine learning to predict poverty. Science, 353, 2016."
REFERENCES,0.17960602549246812,"Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. Smart:
Robust and efﬁcient ﬁne-tuning for pre-trained natural language models through principled
regularized optimization. In International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.18076477404403243,"Fahdi Kanavati and Masayuki Tsuneki. Partial transfusion: on the expressive inﬂuence of trainable
batch norm parameters for transfer learning. In Medical Imaging with Deep Learning, 2021."
REFERENCES,0.18192352259559674,"Fereshte Khani and Percy Liang. Removing spurious features can hurt accuracy and affect groups dis-
proportionately. InACMConferenceonFairness, Accountability, andTransparency(FAccT),2021."
REFERENCES,0.18308227114716108,Published as a conference paper at ICLR 2022
REFERENCES,0.1842410196987254,"Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay
Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee,
Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure
Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.
WILDS: A benchmark of in-the-wild distribution shifts. In International Conference on Machine
Learning (ICML), 2021."
REFERENCES,0.1853997682502897,"Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better imagenet models transfer better? In
Computer Vision and Pattern Recognition (CVPR), 2019."
REFERENCES,0.186558516801854,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009."
REFERENCES,0.18771726535341832,"Thomas Laurent and James H. von Brecht. Deep linear neural networks with arbitrary loss: All local
minima are global. In International Conference on Machine Learning (ICML), 2018."
REFERENCES,0.18887601390498263,"Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt
tuning. arXiv preprint arXiv:2104.08691, 2021."
REFERENCES,0.19003476245654694,"S. Levine, Chelsea Finn, Trevor Darrell, and P. Abbeel. End-to-end training of deep visuomotor
policies. Journal of Machine Learning Research (JMLR), 17, 2016."
REFERENCES,0.19119351100811124,"Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. In
Association for Computational Linguistics (ACL), 2021."
REFERENCES,0.19235225955967555,"Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with
convolutional networks. In International Conference on Machine Learning (ICML), 2018."
REFERENCES,0.19351100811123986,"Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019."
REFERENCES,0.19466975666280417,"John Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar,
Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation
between out-of-distribution and in-distribution generalization.
In International Conference on
Machine Learning (ICML), 2021."
REFERENCES,0.19582850521436848,"Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpo-
lation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 1(1):
67–83, 2020."
REFERENCES,0.1969872537659328,"Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv, 2014."
REFERENCES,0.1981460023174971,"Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In International Conference on Computer Vision (ICCV),
2019."
REFERENCES,0.1993047508690614,"Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In North American Association for
Computational Linguistics (NAACL), 2018."
REFERENCES,0.20046349942062572,"Matthew E Peters, Sebastian Ruder, and Noah A Smith. To tune or not to tune? adapting pretrained
representations to diverse tasks. In Proceedings of the 4th Workshop on Representation Learning
for NLP (RepL4NLP-2019), pp. 7–14, 2019."
REFERENCES,0.20162224797219003,"Viraj Prabhu, Shivam Khare, Deeksha Karthik, and Judy Hoffman. Selective entropy optimization
via committee consistency for unsupervised domain adaptation. In International Conference on
Computer Vision (ICCV), 2021."
REFERENCES,0.20278099652375434,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
Learning transferable visual models from natural language supervision.
In International
Conference on Machine Learning (ICML), volume 139, pp. 8748–8763, 2021."
REFERENCES,0.20393974507531865,Published as a conference paper at ICLR 2022
REFERENCES,0.20509849362688296,"Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C. Duchi, and Percy Liang. Understanding
and mitigating the tradeoff between robustness and accuracy.
In International Conference on
Machine Learning (ICML), 2020."
REFERENCES,0.20625724217844726,"Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do CIFAR-10 classiﬁers
generalize to CIFAR-10? arXiv, 2018."
REFERENCES,0.2074159907300116,"Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers
generalize to imagenet? In International Conference on Machine Learning (ICML), 2019."
REFERENCES,0.2085747392815759,"Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix.
Communications on Pure and Applied Mathematics, 62:1707–1739, 2009."
REFERENCES,0.20973348783314022,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition
challenge. International Journal of Computer Vision, 115(3):211–252, 2015."
REFERENCES,0.21089223638470453,"Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation
shift. arXiv, 2020."
REFERENCES,0.21205098493626884,"Andrew M. Saxe, James L. McClelland, and Surya Ganguli.
Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks. arXiv, 2014."
REFERENCES,0.21320973348783315,"Gal Shachaf, Alon Brutzkus, and Amir Globerson. A theoretical analysis of ﬁne-tuning with linear
teachers. In Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.21436848203939746,"Shuhan Tan, Xingchao Peng, and Kate Saenko. Class-imbalanced domain adaptation: An empirical
odyssey. arXiv preprint arXiv:1910.10320, 2020."
REFERENCES,0.21552723059096177,"Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig
Schmidt. Measuring robustness to natural distribution shifts in image classiﬁcation. arXiv preprint
arXiv:2007.00644, 2020."
REFERENCES,0.21668597914252608,"Nilesh Tripuraneni, Michael I. Jordan, and Chi Jin. On the theory of transfer learning: The importance
of task diversity. arXiv, 2020."
REFERENCES,0.21784472769409038,"Joel A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in
Machine Learning, 8:1–230, 2015."
REFERENCES,0.2190034762456547,"Prasetya Ajie Utama, Naﬁse Sadat Moosavi, Victor Sanh, and Iryna Gurevych. Avoiding inference
heuristics in few-shot prompt-based ﬁnetuning. arXiv preprint arXiv:2109.04144, 2021."
REFERENCES,0.220162224797219,"Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations
by penalizing local predictive power.
In Advances in Neural Information Processing Systems
(NeurIPS), 2019."
REFERENCES,0.2213209733487833,"Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi,
Hongseok Namkoong, and Ludwig Schmidt.
Robust ﬁne-tuning of zero-shot models.
arXiv
preprint arXiv:2109.01903, 2021."
REFERENCES,0.22247972190034762,"Sen Wu, Hongyang R. Zhang, and Christopher Ré. Understanding and improving information transfer
in multi-task learning. In International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.22363847045191193,"Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang.
In-N-out:
Pre-training and self-training using auxiliary information for out-of-distribution
robustness. In International Conference on Learning Representations (ICLR), 2021a."
REFERENCES,0.22479721900347624,"Sang Michael Xie, Tengyu Ma, and Percy Liang.
Composed ﬁne-tuning: Freezing pre-trained
denoising autoencoders for improved generalization. In International Conference on Machine
Learning (ICML), 2021b."
REFERENCES,0.22595596755504055,"Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madha-
van, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning.
In Computer Vision and Pattern Recognition (CVPR), 2020."
REFERENCES,0.22711471610660486,Published as a conference paper at ICLR 2022
REFERENCES,0.22827346465816917,"Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario
Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer,
Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil
Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark.
arXiv, 2020."
REFERENCES,0.22943221320973348,"Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning: A
baseline for network adaptation via additive side networks. In European Conference on Computer
Vision (ECCV), 2020."
REFERENCES,0.23059096176129779,"Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu.
Learning to prompt for
vision-language models. arXiv preprint arXiv:2109.01134, 2021."
REFERENCES,0.23174971031286212,"Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. FreeLB: Enhanced
adversarial training for natural language understanding. In International Conference on Learning
Representations (ICLR), 2020."
REFERENCES,0.23290845886442643,Published as a conference paper at ICLR 2022
REFERENCES,0.23406720741599074,"A
PROOFS FOR SECTION 3"
REFERENCES,0.23522595596755505,"A.1
PRELIMINARIES ON IMPORTANT NOTATIONS AND PRINCIPAL ANGLES
Big-Oh Notation: For convenience, we use big-oh notation in a way that differs from standard
theoretical computer science texts. When we say O(<expr1>) we mean that this can be replaced
by c <expr1> for some universal constant such that the statement holds. As an example, we can
say 5x2 ≤O(x2) because there exists some universal constant (c = 5) such that 5x2 ≤5x2. More
examples: we can also say 5x2 ≥O(x2) or if x≥1 then 7x2 ≤O(x3) and 0.1x2 ≥O(x)."
REFERENCES,0.23638470451911936,"Singular Values: Given a rectangular matrix A ∈Rm×n, let r = min(m,n). The minimum singular
value is deﬁned as the r-th largest singular value of A, so σmin(A)=σr(A)."
REFERENCES,0.23754345307068367,"Working with minimum singular values requires more care than maximum singular vectors. In
particular, when we have rectangular matrices some bounds depend on whether the matrix is ‘fat’
(has more columns than rows) or ‘tall’ (has more rows than columns)."
REFERENCES,0.23870220162224798,"Given a matrix A, the operator norm ∥A∥2 is the maximum singular value: ∥A∥2=σmax(A)."
REFERENCES,0.2398609501738123,"Projectors: Given a subspace R of Rd, let ΠR denote the orthogonal projection onto R, satisfying
that for all x∈Rd:"
REFERENCES,0.2410196987253766,"ΠR(x)∈R and ∀r∈R,∥x−ΠR(x)∥2≤∥x−r∥2.
(A.1)"
REFERENCES,0.2421784472769409,"If E ∈Rd×dim(R) has orthonormal columns that form a basis for R, then we have:"
REFERENCES,0.24333719582850522,"ΠR =EE⊤
(A.2)"
REFERENCES,0.24449594438006952,"From this we can easily check that Π2
R = ΠR and Π⊤
R = ΠR. See e.g., Chapter 2.5.1 Golub & Loan
(2013) for more information."
REFERENCES,0.24565469293163383,"PrincipalAngles: Giventwonon-zerovectors x and y, the cosine of theangle between them, cosθ, is:"
REFERENCES,0.24681344148319814,"cosθ=
x⊤y
∥x∥2∥y∥2
(A.3)"
REFERENCES,0.24797219003476245,"If we consider the 1-dimensional subspaces (so basically lines) Sx and Sy spanned by x and y respec-
tively, then the angle between them, cosθ′ is given by the absolute value (since lines are undirected):"
REFERENCES,0.24913093858632676,"cosθ′ =
|x⊤y|
∥x∥2∥y∥2
(A.4)"
REFERENCES,0.25028968713789107,"Principal angles generalize this notion to higher dimensions. See e.g., Chapter 6.4.3 in Golub & Loan
(2013) for more information on principal angles."
REFERENCES,0.2514484356894554,"Deﬁnition A.1. Given two non-empty subspaces R and S of Rd, where r = min(dim(R),dim(S)),
we have r principal angles:"
REFERENCES,0.2526071842410197,"0≤θ1 ≤...≤θr ≤π/2.
(A.5)
The directions of the inequalities swap when we take the cosine of the principal angles:"
REFERENCES,0.253765932792584,"1≥cosθ1 ≥...≥cosθr ≥0.
(A.6)"
REFERENCES,0.2549246813441483,"The cosines of the principal angles are given by the SVD—let E ∈Rd×dim(R) and F ∈Rd×dim(S)
have orthonormal columns which span R and S respectively. Then we have:"
REFERENCES,0.25608342989571264,"cosθi =σi(E⊤F),
(A.7)"
REFERENCES,0.2572421784472769,"where σi denotes the i-th largest singular value. In this paper, we are interested in the cosine of the
largest angle between them, given by:"
REFERENCES,0.25840092699884126,"cosθmax(R,S)=cosθr
(A.8)"
REFERENCES,0.25955967555040554,"We can massage this into a variational characterization of the maximum principal angle, which is
important for lower bounding the error of ﬁne-tuning outside the span of the training data."
REFERENCES,0.2607184241019699,"Lemma A.2. Suppose dim(R) ≤dim(S), and let F ∈Rd×dim(S) have orthonormal columns that
form a basis for S. We have:"
REFERENCES,0.26187717265353416,"cosθmax(R,S)=
min
r∈R,∥r∥2=1∥F ⊤(r)∥2
(A.9)"
REFERENCES,0.2630359212050985,Published as a conference paper at ICLR 2022
REFERENCES,0.2641946697566628,"Proof. Let E ∈Rd×dim(R) and F ∈Rd×dim(S) have orthonormal columns that span R and S
respectively. Since dim(R)≤dim(S) (a crucial condition!), F ⊤E is a ‘tall’ matrix (it has more rows
than columns) so we have:"
REFERENCES,0.2653534183082271,"σmin(F ⊤E)= min
∥v∥2=1∥F ⊤Ev∥2.
(A.10)"
REFERENCES,0.2665121668597914,The result now follows from some algebra:
REFERENCES,0.26767091541135574,"cosθmax(R,S)=σmin(F ⊤E)
(A.11)"
REFERENCES,0.2688296639629201,"= min
∥v∥2=1∥F ⊤Ev∥2
(A.12)"
REFERENCES,0.26998841251448435,"=
min
r∈R,∥r∥2=1∥F ⊤(r)∥2.
(A.13)"
REFERENCES,0.2711471610660487,"A.2
FEATURE DISTORTION THEOREM"
REFERENCES,0.272305909617613,"We ﬁrst prove our core theorem, that ﬁne-tuning distorts pretrained features."
REFERENCES,0.2734646581691773,"Restatement of Theorem 3.2. In the overparameterized linear setting, let S⊥= rowspace(X)⊥,
R0 =rowspace(B0), and v⋆,B⋆be the optimal parameters with w⋆=B⋆v⋆. If cosθmax(R0,S⊥)>0,
then for all time steps t, the OOD error of the ﬁne-tuning iterates (Bft(t),vft(t)) is lower bounded: p"
REFERENCES,0.2746234067207416,"Lood(vft(t),Bft(t))≥
p"
REFERENCES,0.27578215527230593,"σmin(Σ)
cosθmax(R0,S⊥)
√"
REFERENCES,0.2769409038238702,"k
min(φ,φ2/∥w⋆∥2)"
REFERENCES,0.27809965237543455,"(1+∥w⋆∥2)2
−ϵ

,
(A.14)"
REFERENCES,0.27925840092699883,"where φ2 =|(v⊤
0 v⋆)2−(v⊤
⋆v⋆)2| is deﬁned to be inital head alignment error and ϵ≥d(B0,B⋆) is the
error in the pretrained feature extractor."
REFERENCES,0.28041714947856317,"We follow the sketch in the main paper. We begin with a few lemmas, showing that certain quantities
are preserved throughout the ﬁne-tuning process."
REFERENCES,0.28157589803012745,"Our ﬁrst lemma says that the representations Bt
ftx do not change for examples perpendicular the span"
REFERENCES,0.2827346465816918,"of the training examples. Note that the ﬁnal output vt
ft
⊤Bt
ftx still changes, because vt
ft changes."
REFERENCES,0.28389339513325607,"Lemma A.3. For all times t and all x∈S⊥, we have:"
REFERENCES,0.2850521436848204,"B0x=Bt
ftx
(A.15)"
REFERENCES,0.2862108922363847,"Proof. We initialized ﬁne-tuning with the feature extractor Bft(0) = B0. It sufﬁces to show that
∂tBt
ftx=0 for all x∈S⊥. Recall that ∂tBt
ft is given by the gradient ﬂow update equation:"
REFERENCES,0.287369640787949,"∂tBt
ft =−∂B bL(vt
ft,Bt
ft)=−∂B∥XB⊤v−Y ∥2
2
(A.16)"
REFERENCES,0.2885283893395133,"Computing the RHS explicitly using multivariable chain rule, we get:"
REFERENCES,0.28968713789107764,"∂tBt
ft =−2v(XB⊤v−Y )⊤X
(A.17)"
REFERENCES,0.2908458864426419,"Since x is a constant, we get:"
REFERENCES,0.29200463499420626,"∂tBt
ftx=−2v(XB⊤v−Y )⊤Xx
(A.18)"
REFERENCES,0.2931633835457706,"But Xx = 0 for x ∈S⊥, since x ∈S⊥is deﬁned as x is perpendicular to the rowspace of X (i.e.,
perpendicular to the rows of X). So the RHS is 0—that is, ∂tBt
ftx=0, as desired."
REFERENCES,0.2943221320973349,"Next, we show that the change in the head and feature extractor are ‘coupled’. So if the head changes
in a certain way, then the feature extractor cannot just stay the same. In the literature, this is sometimes
called the “balancedness"" lemma, and has been proved in prior work on two layer linear networks."
REFERENCES,0.2954808806488992,Lemma A.4. For all t we have:
REFERENCES,0.2966396292004635,"v0v⊤
0 −B0B⊤
0 =vt
ftvt
ft
⊤−Bt
ftBt
ft
⊤
(A.19)"
REFERENCES,0.29779837775202783,Published as a conference paper at ICLR 2022
REFERENCES,0.2989571263035921,Proof. This follows by showing that the derivative is 0:
REFERENCES,0.30011587485515645,"∂t[vt
ftvt
ft
⊤−Bt
ftBt
ft
⊤]=0
(A.20)"
REFERENCES,0.30127462340672073,"Which can be veriﬁed by direct calculation. See Theorem 2.2 in Du et al. (2018) and the proof of
Theorem 1 in Arora et al. (2018)."
REFERENCES,0.30243337195828507,"For our proof we will require that every feature r ∈R can be generated from some OOD direction,
that is r = B0u for some u ∈S⊥. We will show that this is implied by the condition on the principal
angle: cosθmax(R,S⊥) > 0 where R = rowspace(B0), which we assumed in Theorem 3.2. The
following lemma shows this (and also quantiﬁes that the norm of u does not shrink too much when
projected onto R)."
REFERENCES,0.30359212050984935,"Lemma A.5. Let R, S be subspaces of Rd with dim(R) ≤dim(S).
For all r ∈R with
∥r∥2= cosθmax(R,S), there exists s ∈S with ΠR(s) = r and ∥s∥2≤1. Here ΠR ∈Rd×d projects a
vector onto R."
REFERENCES,0.3047508690614137,"Proof. Let c = cosθmax(R,S). Firt, we get rid of an easy case—if c = 0, then we need to show
the claim for all r ∈R with ∥r∥2= c = 0, which means r = 0. Then we can just pick s = 0, and
ΠR(s)=0=r and ∥s∥2=0≤1. So for the rest of the proof we assume c>0."
REFERENCES,0.30590961761297797,"Consider arbitrary vector r ∈R with ∥r∥2=c. Let E ∈Rd×dim(S),F ∈Rd×dim(R) have orthonormal
columns, which form a basis for R and S respectively."
REFERENCES,0.3070683661645423,"Step1: Findings: SincethecolumnsofE spanR, r=Ez forsomez ∈Rdim(R). c=σmin(E⊤F)>0,
whichmeansthatE⊤F ∈Rdim(R)×dim(S) hasrankdim(R)sincedim(R)≤dim(S)—inotherwords,
E⊤F has full column rank since the column dimension is smaller than the row dimension. So z =
E⊤Fw for some w ∈rowspace(E⊤F). Then we set s=Fw—this means s∈S because the columns
of F form a basis for S. In addition, following the steps above we have r =Ez =EE⊤Fw =EE⊤s.
We note that ΠR =EE⊤is the projection onto R (see e.g., Chapter 2.5.1 of Golub & Loan (2013))."
REFERENCES,0.3082271147161066,"Step 2: Bounding norm of s: It sufﬁces to show that ∥s∥2≤1. Since F has orthonormal columns,
∥s∥2= ∥Fw∥2= ∥w∥2, so it sufﬁces to show that ∥w∥2≤1. Since E has orthonormal columns,
∥r∥2=∥z∥2. Recall that z =E⊤Fw—since w∈rowspace(E⊤F), from Lemma A.6 we have:"
REFERENCES,0.3093858632676709,"∥z∥2≥σmin(E⊤F)∥w∥2=c∥w∥2.
(A.21)"
REFERENCES,0.3105446118192352,"Rearranging, we get ∥w∥2≤∥z∥2/c=1, as desired."
REFERENCES,0.31170336037079954,"In the lemma above, we used a standard linear algebraic result that we include for completeness. This
says that A cannot shrink vectors in its rowspace too much, where the shrinkage factor is given by the
minimum singular value of A."
REFERENCES,0.3128621089223638,"Lemma A.6. Let A ∈Rm×n.
Let r = min(m, n).
Then if x ∈rowspace(A), we have
∥Ax∥2≥σr(A)∥x∥2."
REFERENCES,0.31402085747392816,"Proof. We bound the norm of x using the SVD. Consider the singular value decomposition (SVD) of
A:"
REFERENCES,0.31517960602549244,"A=UDV ⊤
(A.22)"
REFERENCES,0.3163383545770568,"Where U ∈Rm×r, D ∈Rr×r, V ⊤∈Rr×n, where U and V have orthonormal columns, and
D=diag(σ1,...,σr) is a diagonal matrix with σ1 ≥...≥σr ≥0."
REFERENCES,0.3174971031286211,"∥Ax∥2 =∥UDV ⊤x∥2
[Deﬁnition of r]
(A.23)"
REFERENCES,0.3186558516801854,"=∥DV ⊤x∥2
[U ∈Rm×r has orthonormal columns]
(A.24)"
REFERENCES,0.31981460023174973,"≥σr∥V ⊤x∥2
[D is diagonal]
(A.25)"
REFERENCES,0.320973348783314,"=σr∥x∥2
[rows of V ⊤are orthonormal, x is in rowspace]
(A.26)
=σr(A)∥x∥2
(A.27)"
REFERENCES,0.32213209733487835,Published as a conference paper at ICLR 2022
REFERENCES,0.32329084588644263,"Where for the fourth step, we used the fact that if x ∈rowspace(V ⊤) and the rows of V ⊤are
orthonormal, then ∥V ⊤x∥2=∥x∥2. One way to see this is by writing x=P"
REFERENCES,0.32444959443800697,"iαivi, where vi are rows
of V ⊤, and then noting that V ⊤x=(α1,...,αr) and so x and V ⊤x have the same norm."
REFERENCES,0.32560834298957125,"We recall that Pood has second moment Σ: E[xx⊤]=Σ when x∼Pood, where Σ is invertible. So with
some simple algebra we can write the OOD error Lood in terms of Σ (the proof is standard and basic,
but we include it just for completeness):"
REFERENCES,0.3267670915411356,Lemma A.7.
REFERENCES,0.32792584009269987,"Lood(v,B)=(B⊤
⋆v⋆−B⊤v)⊤Σ(B⊤
⋆v⋆−B⊤v)≤σmin(Σ)∥B⊤
⋆v⋆−B⊤v∥2
2.
(A.28)"
REFERENCES,0.3290845886442642,"Proof. Let x∼Pood. We have,"
REFERENCES,0.3302433371958285,"Lood(v,B)=E[(v⊤
⋆B⋆x−v⊤Bx)2]
(A.29)"
REFERENCES,0.3314020857473928,"=E[(B⊤
⋆v⋆−B⊤v)⊤xx⊤(B⊤
⋆v⋆−B⊤v)]
(A.30)"
REFERENCES,0.3325608342989571,"=(B⊤
⋆v⋆−B⊤v)⊤E[xx⊤](B⊤
⋆v⋆−B⊤v)
(A.31)"
REFERENCES,0.33371958285052145,"=(B⊤
⋆v⋆−B⊤v)⊤Σ(B⊤
⋆v⋆−B⊤v).
(A.32)"
REFERENCES,0.3348783314020857,"The inequality follows immediately because σmin(A) (for a square matrix A) is simply the min over
x with unit ℓ2 norm of x⊤Ax."
REFERENCES,0.33603707995365006,"We now prove Theorem 3.2, following the 3 steps outlined in the main text."
REFERENCES,0.33719582850521435,"Proof of Theorem 3.2. Let c = cos θmax(R, S⊥). From Lemma A.7, we have Lood(vt
ft, Bt
ft) ≤"
REFERENCES,0.3383545770567787,"σmin(Σ)∥B⊤
⋆v⋆−Bt
ft
⊤vt
ft∥2
2 so it sufﬁces to bound ∥B⊤
⋆v⋆−Bt
ft
⊤vt
ft∥2."
REFERENCES,0.33951332560834296,"Because it makes the proof much easier, we will prove the contrapositive, and then convert back to
the original theorem statement. We assume ∥B⊤
⋆v⋆−Bt
ft
⊤vt
ft∥2≤∆, and will show that:"
REFERENCES,0.3406720741599073,"|(v⊤
0 v⋆)2−(v⊤
⋆v⋆)2|≤∆+ϵ"
REFERENCES,0.34183082271147164,"c
g1(∥w∥2)
√"
REFERENCES,0.3429895712630359,k+ (∆+ϵ)2
REFERENCES,0.34414831981460026,"c2
g2(∥w∥2)k
(A.33)"
REFERENCES,0.34530706836616454,Where g1 and g2 are non-negative polynomials we will bound in the proof.
REFERENCES,0.3464658169177289,"We gave a basic outline of the proof in the main paper, and here we are just trying to be careful about
capturing all the dependencies. We also give intuition for each step before diving into algebra (which
we include for completeness)."
REFERENCES,0.34762456546929316,"Recall that in the overparameterized linear setting we assumed we have orthonormal B0 with
∥B0 −UB⋆∥2≤ϵ for some U. We note that the setup is rotationally symmetric so without loss of
generality we can suppose ∥B0−B⋆∥2≤ϵ. This is because we can let B′
⋆= UB⋆and v′
⋆= Uv⋆, and
we have w⋆= B⊤
⋆v⋆= (UB⋆)⊤(Uv⋆), where w⋆is the optimal classiﬁer—so we can now write the
entire proof in terms of B′
⋆and v′
⋆."
REFERENCES,0.3487833140208575,"Step 1: Show that ∥vt
ft −v⋆∥2≤∆/c: We ﬁrst give intuition and then dive into the math. The key
insight is to use the fact that in ‘many’ directions Bt
ft and B0 are the same (formally, for all x ∈S⊥,
Bt
ftx = B0x). But B0 and B⋆are close by assumption, which means that Bt
ft and B⋆are close in"
REFERENCES,0.3499420625724218,"‘many’ directions. Then since we assumed in the contrapositive that vt
ft
⊤Bt
ft and v⊤
⋆B⋆are close, we
get that vt
ft and v⋆are close in ‘many’ directions. Because S⊥covers the rowspace of B0, we get that
‘many’ is k, which is precisely the dimensionality of v⋆, so the two vectors vt
ft and v⋆must be close."
REFERENCES,0.3511008111239861,"We now dive into the math. Since B0 has orthogonal rows, B0 has full column rank."
REFERENCES,0.3522595596755504,Let z be given by:
REFERENCES,0.35341830822711473,"z =
c
∥vt
ft−v⋆∥2
(vt
ft−v⋆)
(A.34)"
REFERENCES,0.354577056778679,"We note that ∥z∥2=c. Then, we can ﬁnd y ∈R=rowspace(B0) such that B0y =z (since B0 has full
column-rank) and then ∥y∥2=∥z∥2=c (since B0 has orthonormal rows)."
REFERENCES,0.35573580533024335,Published as a conference paper at ICLR 2022
REFERENCES,0.35689455388180763,"Since c=cosθmax(R,S⊥)>0, and y ∈R with ∥y∥=c, from Lemma A.5 we can choose x∈S⊥with
∥x∥2≤1 and ΠR(x)=y. Then, we have B0x=z."
REFERENCES,0.35805330243337197,"From Proposition A.3, since x ∈S⊥, B0 does not change in directions of x when ﬁne-tuning so we
have: B0x=Bt
ftx."
REFERENCES,0.35921205098493625,"The claim now follows from simple algebraic manipulation, following the intuition we described.
The algebra just captures what ‘close’ means and adds up the error terms."
REFERENCES,0.3603707995365006,"∥vt
ft−v⋆∥2 = 1"
REFERENCES,0.36152954808806487,"c (vt
ft−v⋆)⊤(
c(vt
ft−v⋆)
∥vt
ft−v⋆∥2
)
[Algebra]
(A.35) = 1"
REFERENCES,0.3626882966396292,"c (vt
ft−v⋆)⊤z
[Deﬁnition of z]
(A.36) = 1"
REFERENCES,0.3638470451911935,"c (vt
ft−v⋆)⊤B0x
[Since B0x=z]
(A.37) = 1"
REFERENCES,0.3650057937427578,"c (vt
ft
⊤B0x−v⊤
⋆B0x)
[Algebra]
(A.38) = 1"
REFERENCES,0.36616454229432216,"c (vt
ft
⊤Bt
ftx−v⊤
⋆B0x)
[Bt
ftx=B0x since x∈S⊥]"
REFERENCES,0.36732329084588644,(A.39) = 1
REFERENCES,0.3684820393974508,"c (vt
ft
⊤Bt
ft−v⊤
⋆B0)x
[Algebra]
(A.40) ≤1"
REFERENCES,0.36964078794901506,"c ∥vt
ft
⊤Bt
ft−v⊤
⋆B0∥2∥x∥2
[Cauchy-Schwarz]
(A.41) ≤1"
REFERENCES,0.3707995365005794,"c ∥vt
ft
⊤Bt
ft−v⊤
⋆B0∥2
[since ∥x∥2≤1]
(A.42) ≤1"
REFERENCES,0.3719582850521437,"c ∥vt
ft
⊤Bt
ft−v⊤
⋆B⋆∥2+1"
REFERENCES,0.373117033603708,"c ∥v⊤
⋆B⋆−v⊤
⋆B0∥2
[Triangle inequality]
(A.43) ≤1"
REFERENCES,0.3742757821552723,"c ∥Bt
ft
⊤vt
ft−B⊤
⋆v⋆∥2+1"
REFERENCES,0.37543453070683663,"c ∥v⊤
⋆B⋆−v⊤
⋆B0∥2
[Taking transpose]
(A.44) = 1"
REFERENCES,0.3765932792584009,"c ∥Bt
ft
⊤vt
ft−B⊤
⋆v⋆∥2+1"
REFERENCES,0.37775202780996525,"c σmax(B0−B⋆)∥v⋆∥2
[deﬁnition of max singular value]"
REFERENCES,0.37891077636152953,(A.45) = 1
REFERENCES,0.38006952491309387,"c ∥Bt
ft
⊤vt
ft−B⊤
⋆v⋆∥2+1"
REFERENCES,0.38122827346465815,"c ϵ∥v⋆∥2
[since σmax(B0−B⋆)≤ϵ]"
REFERENCES,0.3823870220162225,(A.46)
REFERENCES,0.38354577056778677,≤∆+ϵ∥v⋆∥2
REFERENCES,0.3847045191193511,"c
[since ∥B⊤
⋆v⋆−Bt
ft
⊤vt
ft∥2≤∆]"
REFERENCES,0.3858632676709154,(A.47)
REFERENCES,0.3870220162224797,(A.48)
REFERENCES,0.388180764774044,"Which shows that ∥vt
ft−v⋆∥2≤(∆+ϵ∥v⋆∥2)/c."
REFERENCES,0.38933951332560834,"Step 2A: Show that ∥Bt
ft∥2
F is small: The key insight is to take the trace on both sides of
Proposition A.4, which bounds the Frobenius norm of Bt
ft and therefore the operator norm."
REFERENCES,0.3904982618771727,"Rearranging Proposition A.4, we have:"
REFERENCES,0.39165701042873696,"Bt
ftBt
ft
⊤=B0B⊤
0 +v⋆v⊤
⋆−v0v⊤
0
(A.49)"
REFERENCES,0.3928157589803013,"Taking the trace everywhere, we get:"
REFERENCES,0.3939745075318656,"Tr(Bt
ftBt
ft
⊤)=Tr(B0B⊤
0 )+Tr(v⋆v⊤
⋆)−Tr(v0v⊤
0 )
(A.50)"
REFERENCES,0.3951332560834299,"For any matrix A, Tr(AA⊤) = ∥A∥2
F , and for a vector v the Frobenius norm is just the ℓ2-norm, so
Tr(vv⊤)=∥v∥2
2. So we have:"
REFERENCES,0.3962920046349942,"∥Bt
ft∥2
F =∥B0∥2
F +∥v⋆∥2
2−∥v0∥2
2
(A.51)"
REFERENCES,0.39745075318655854,"Squares are non-negative, so we get the inequality:"
REFERENCES,0.3986095017381228,"∥Bt
ft∥2
F ≤∥B0∥2
F +∥v⋆∥2
2
(A.52)"
REFERENCES,0.39976825028968715,Published as a conference paper at ICLR 2022
REFERENCES,0.40092699884125144,"Step 2B: Show that ∥B⊤
0 v⋆∥2
2−∥Bt
ft
⊤v⋆∥2
2 is small: This step doesn’t involve much insight, and is
standard peturbation analysis—we simply factor the difference of squares and bound each term."
REFERENCES,0.4020857473928158,"First, we bound ∥Bt
ft
⊤vt
ft−Bt
ft
⊤v⋆∥2:"
REFERENCES,0.40324449594438005,"∥Bt
ft
⊤vt
ft−Bt
ft
⊤v⋆∥2 ≤σmax(Bt
ft)∥vt
ft−v⋆∥2
(A.53)"
REFERENCES,0.4044032444959444,"≤∥Bt
ft∥F ∥vt
ft−v⋆∥2
(A.54) ≤
q"
REFERENCES,0.4055619930475087,"∥B0∥2
F +∥v⋆∥2
2∥vt
ft−v⋆∥2
(A.55) ≤
q"
REFERENCES,0.406720741599073,"∥B0∥2
F +∥v⋆∥2
2
∆+ϵ∥v⋆∥2 c "
REFERENCES,0.4078794901506373,(A.56)
REFERENCES,0.40903823870220163,"Next, we bound ∥B⊤
0 v⋆−Bt
ft
⊤v⋆∥2:"
REFERENCES,0.4101969872537659,"∥B⊤
0 v⋆−Bt
ft
⊤v⋆∥2 ≤∥B⊤
0 v⋆−B⊤
⋆v⋆∥2+∥B⊤
⋆v⋆−Bt
ft
⊤v⋆∥2
(A.57)"
REFERENCES,0.41135573580533025,"≤σmax(B0−B⋆)∥v⋆∥2+∥B⊤
⋆v⋆−Bt
ft
⊤v⋆∥2
(A.58)"
REFERENCES,0.41251448435689453,"≤ϵ∥v⋆∥2+∥B⊤
⋆v⋆−Bt
ft
⊤v⋆∥2
(A.59)"
REFERENCES,0.41367323290845887,"≤ϵ∥v⋆∥2+∥B⊤
⋆v⋆−Bt
ft
⊤vt
ft∥2+∥Bt
ft
⊤vt
ft−Bt
ft
⊤v⋆∥2
(A.60)"
REFERENCES,0.4148319814600232,"≤ϵ∥v⋆∥2+∆+
q"
REFERENCES,0.4159907300115875,"∥B0∥2
F +∥v⋆∥2
2
∆+ϵ∥v⋆∥2 c "
REFERENCES,0.4171494785631518,(A.61)
REFERENCES,0.4183082271147161,"=:∆2
(A.62)"
REFERENCES,0.41946697566628044,"Finally, we bound |∥B⊤
0 v⋆∥2
2−∥Bt
ft
⊤v⋆∥2
2|, using the identity:"
REFERENCES,0.4206257242178447,"|∥u∥2
2−∥v∥2
2|=|(u−v)⊤(u+v)|
(A.63)
≤∥u−v∥2∥u+v∥2
(A.64)
≤∥u−v∥2(2∥u∥2+∥u−v∥2)
(A.65)"
REFERENCES,0.42178447276940906,Applying this:
REFERENCES,0.42294322132097334,"|∥B⊤
0 v⋆∥2
2−∥Bt
ft
⊤v⋆∥2
2|≤∥B⊤
0 v⋆−Bt
ft
⊤v⋆∥2(2∥B⊤
0 v⋆∥2+∥B⊤
0 v⋆−Bt
ft
⊤v⋆∥2)
(A.66)"
REFERENCES,0.4241019698725377,"≤∆2(2∥B⊤
0 v⋆∥2+∆2)
(A.67)"
REFERENCES,0.42526071842410196,"≤∆2(2∥B⊤
⋆v⋆∥2+2∥B⊤
0 v⋆−B⊤
⋆v⋆∥2+∆2)
(A.68)
≤∆2(2∥w⋆∥2+2ϵ∥v⋆∥2+∆2)
(A.69)
=:∆3
(A.70)"
REFERENCES,0.4264194669756663,"Step 3: Use Proposition A.4 to show v0 and v⋆must be close: The key insight is that we start from
Proposition A.4, and left and right multiply by v⋆, after that we use the previous steps and do some
some standard perturbation analysis."
REFERENCES,0.4275782155272306,We start from Proposition A.4:
REFERENCES,0.4287369640787949,"v0v⊤
0 −B0B⊤
0 =vt
ftvt
ft
⊤−Bt
ftBt
ft
⊤
(A.71)"
REFERENCES,0.4298957126303592,"The key step is to left multiply both sides by v⊤
⋆and right multiply both sides by v⋆to get:"
REFERENCES,0.43105446118192353,"(v⊤
0 v⋆)2−∥B⊤
0 v⋆∥2
2=(vt
ft
⊤v⋆)2−∥Bt
ft
⊤v⋆∥2
2
(A.72)"
REFERENCES,0.4322132097334878,"Rearranging, and then using Equation A.66, we get:"
REFERENCES,0.43337195828505215,"|(vt
ft
⊤v⋆)2−(v⊤
0 v⋆)2|=|∥Bt
ft
⊤v⋆∥2
2−∥B⊤
0 v⋆∥2
2|≤∆3
(A.73)"
REFERENCES,0.43453070683661643,"This is close to what we want, except we have (vt
ft
⊤v⋆)2 on the LHS instead of (v⊤
⋆v⋆)2. We
previously showed that vt
ft and v⋆are close, in Step 1, so with some algebra we can bound the"
REFERENCES,0.43568945538818077,Published as a conference paper at ICLR 2022
REFERENCES,0.43684820393974505,"difference between (vt
ft
⊤v⋆)2 and (v⊤
⋆v⋆)2:"
REFERENCES,0.4380069524913094,"|(vt
ft
⊤v⋆)2−(v⊤
⋆v⋆)2|=|(vt
ft
⊤v⋆−v⊤
⋆v⋆)⊤(vt
ft
⊤v⋆+v⊤
⋆v⋆)|
(A.74)"
REFERENCES,0.4391657010428737,"=|(vt
ft
⊤v⋆−v⊤
⋆v⋆)⊤[2v⊤
⋆v⋆+(vt
ft
⊤v⋆−v⊤
⋆v⋆)]|
(A.75)"
REFERENCES,0.440324449594438,"=|(v⊤
⋆(vt
ft−v⋆))⊤[2v⊤
⋆v⋆+(v⊤
⋆(vt
ft−v⋆))]|
(A.76)"
REFERENCES,0.44148319814600234,"≤∥vt
ft−v⋆∥2∥v⋆∥2
2[2∥v⋆∥2+∥vt
ft−v⋆∥2]
(A.77)"
REFERENCES,0.4426419466975666,"=(∆/c)∥v⋆∥2
2(2∥v⋆∥2+(∆/c)):=∆4
(A.78)"
REFERENCES,0.44380069524913096,"Above, from the third line to the fourth line, we used triangle inequality and Cauchy-Schwarz."
REFERENCES,0.44495944380069524,"So ﬁnally, by triangle-inequality we can now bound |(v⊤
⋆v⋆)2−(v⊤
0 v⋆)2|:"
REFERENCES,0.4461181923522596,"|(v⊤
⋆v⋆)2−(v⊤
0 v⋆)2|≤|(v⊤
⋆v⋆)2−(vt
ft
⊤v⋆)2|+|(vt
ft
⊤v⋆)2−(v⊤
0 v⋆)2|
(A.79)"
REFERENCES,0.44727694090382386,"≤∆4+∆3
(A.80)"
REFERENCES,0.4484356894553882,"Wrap up i.e., writing out ∆4 +∆3 explicitly: This is basically the bound we want, but we would
like to express ∆3,∆4 in terms of ∆and ϵ. Note that this step has no insight, and is just algebra—we
include the details for reference and veriﬁability. We recall:"
REFERENCES,0.4495944380069525,"∆4 =(∆/c)∥v⋆∥2
2(2∥v⋆∥2+(∆/c))
(A.81)
∆3 =∆2(2∥w⋆∥2+2ϵ∥v⋆∥2+∆2)
(A.82)"
REFERENCES,0.4507531865585168,"∆2 =ϵ∥v⋆∥2+∆+
q"
REFERENCES,0.4519119351100811,"∥B0∥2
F +∥v⋆∥2
2
∆+ϵ∥v⋆∥2 c "
REFERENCES,0.45307068366164543,(A.83)
REFERENCES,0.4542294322132097,"Since B0 has orthogonal rows (by assumption), B⊤
0
has orthogonal columns, so ∥w⋆∥2=
∥B⊤
0 v⋆∥2= ∥v⋆∥2. In addition, since B0 has k orthogonal rows, ∥B0∥F =
√"
REFERENCES,0.45538818076477405,"k. We also note that
p"
REFERENCES,0.45654692931633833,"∥B0∥2
F +∥v⋆∥2
2 ≤∥B0∥F +∥v⋆∥2=
√"
REFERENCES,0.45770567786790267,"k+∥w⋆∥2. Since c≤1, we have:"
REFERENCES,0.45886442641946695,"ϵ∥v⋆∥2+∆≤
∆+ϵ∥v⋆∥2 c"
REFERENCES,0.4600231749710313,"
(A.84)"
REFERENCES,0.46118192352259557,"So for ∆2, up to constant factors we can ignore the ϵ∥v⋆∥2+∆term—this means we get:"
REFERENCES,0.4623406720741599,"∆2 ≤O

(
√"
REFERENCES,0.46349942062572425,"k+∥w⋆∥2)
∆+ϵ∥w⋆∥2 c"
REFERENCES,0.4646581691772885,"
(A.85)"
REFERENCES,0.46581691772885286,"Using the fact that
√"
REFERENCES,0.46697566628041715,"k+∥w⋆∥2≤
√"
REFERENCES,0.4681344148319815,k(1+∥w⋆∥) we get:
REFERENCES,0.46929316338354576,"∆2 ≤O
√"
REFERENCES,0.4704519119351101,"k(1+∥w⋆∥)
∆+ϵ∥w⋆∥2 c"
REFERENCES,0.4716106604866744,"
(A.86)"
REFERENCES,0.4727694090382387,"Then since ∆+ϵ∥w⋆∥2≤(1+∥w⋆∥2)(∆+ϵ), we get:"
REFERENCES,0.473928157589803,"∆2 ≤O
√"
REFERENCES,0.47508690614136734,k(1+∥w⋆∥)2∆+ϵ c
REFERENCES,0.4762456546929316,"
(A.87)"
REFERENCES,0.47740440324449596,"Now for ∆3, ﬁrst note that ϵ≤2, since B⋆and B0 have orthogonormal rows so ∥B⋆−B0∥2≤2. This
means that ϵ∥w⋆∥2≤∥w⋆∥2, so ∆3 simpliﬁes to:"
REFERENCES,0.47856315179606024,"∆3 ≤O(∆2(∥w⋆∥2+∆2))=O(∆2∥w⋆∥2+∆2)
(A.88)"
REFERENCES,0.4797219003476246,"Substituting the bound for ∆2 into ∆3, we get:"
REFERENCES,0.48088064889918886,"∆3 ≤O
√"
REFERENCES,0.4820393974507532,k∥w⋆∥2(1+∥w⋆∥)2∆+ϵ∥w⋆∥2 c
REFERENCES,0.4831981460023175,"
+k(1+∥w⋆∥)4∆+ϵ∥w⋆∥2 c"
REFERENCES,0.4843568945538818,"2
(A.89)"
REFERENCES,0.4855156431054461,"For ∆4, we get:"
REFERENCES,0.48667439165701043,"∆4 ≤O

∥w⋆∥3
2
∆"
REFERENCES,0.48783314020857477,c +∥w⋆∥2(∆
REFERENCES,0.48899188876013905,"c )

(A.90)"
REFERENCES,0.4901506373117034,"Since ∆/c≤(∆+ϵ)/c and ∥w⋆∥2
2≤(1+∥w⋆∥2)2 we have for the ﬁnal error ∆3+∆4:"
REFERENCES,0.49130938586326767,"∆3+∆4 ≤
√"
REFERENCES,0.492468134414832,"kw(1+∥w⋆∥2
2)2∆+ϵ c"
REFERENCES,0.4936268829663963,"
+k(1+∥w⋆∥2
2)4∆+ϵ c"
REFERENCES,0.4947856315179606,"2
(A.91)"
REFERENCES,0.4959443800695249,Published as a conference paper at ICLR 2022
REFERENCES,0.49710312862108924,"Wrap up i.e., taking the contrapositive: So we’ve shown that if ∥B⊤
⋆v⋆−Bt
ft
⊤vt
ft∥2
2≤∆, then:"
REFERENCES,0.4982618771726535,"|(v⊤
⋆v⋆)2−(v⊤
0 v⋆)2|≤∆+ϵ"
REFERENCES,0.49942062572421786,"c
w(1+∥w⋆∥2
2)2√"
REFERENCES,0.5005793742757821,k+ (∆+ϵ)2
REFERENCES,0.5017381228273464,"c2
(1+∥w⋆∥2
2)4k
(A.92)"
REFERENCES,0.5028968713789108,"We’d like to ﬂip this around: suppose |(v⊤
⋆v⋆)2 −(v⊤
0 v⋆)2| ≥φ2 for some φ. To lower bound
∥B⊤
⋆v⋆−Bt
ft
⊤vt
ft∥2
2, we simply take the contrapositive of what we have proved. Let ∆be given by:"
REFERENCES,0.5040556199304751,"∆=min

c
w(1+∥w⋆∥2
2)2√"
REFERENCES,0.5052143684820394,"k
φ2,
c
p"
REFERENCES,0.5063731170336037,"(1+∥w⋆∥2
2)4k
φ

−ϵ
(A.93)"
REFERENCES,0.507531865585168,"In this case with some algebra, we can show that:"
REFERENCES,0.5086906141367323,"|(v⊤
⋆v⋆)2−(v⊤
0 v⋆)2|≥φ2 ≥∆+ϵ"
REFERENCES,0.5098493626882966,"c
w(1+∥w⋆∥2
2)2√"
REFERENCES,0.5110081112398609,k+ (∆+ϵ)2
REFERENCES,0.5121668597914253,"c2
(1+∥w⋆∥2
2)4k
(A.94)"
REFERENCES,0.5133256083429896,"To see this, we bound each of the terms in the RHS separately using our deﬁnition of ∆. Then, from
the contrapositive of what we proved (compare with Equation A.92, we get:"
REFERENCES,0.5144843568945539,"∥B⊤
⋆v⋆−Bt
ft
⊤vt
ft∥2
2≥∆
(A.95)"
REFERENCES,0.5156431054461182,"Finally, we can massage ∆to combine terms and make it look slightly nicer: ∆≥c
√"
REFERENCES,0.5168018539976825,"k
min(φ,φ2/∥w⋆∥2)"
REFERENCES,0.5179606025492468,"(1+∥w⋆∥2)2
−ϵ
(A.96)"
REFERENCES,0.5191193511008111,"Then applying Lemma A.7 we get the desired result. For even more interpretability, if ∥w∥2= 1 and
φ is bounded above by some constant, then you can think of ∆as approximately
c
√"
REFERENCES,0.5202780996523755,"kφ2 −ϵ. This
completes the proof."
REFERENCES,0.5214368482039398,"A.3
LP VS. FT (OOD)
We now prove Theorem 3.4, which compares linear probing and ﬁne-tuning in the linear overparam-
eterized setting, when the ID data lies in a lower dimensional subspace. We ﬁrst deﬁne a distance
(formally, a pseudometric) between pretrained feature exactors, which measures the operator norm
difference between them up to rotations (e.g., if B = UB′ for a rotation matrix U then B and B′ are
equivalent since this means we can obtain one feature extractor’s representations from another just by
rotating):"
REFERENCES,0.522595596755504,"Deﬁnition A.8 (Feature Extractor Distance). The distance between feature extractors B,B′ ∈Rk×d
(with orthonormal rows) is given by (where the min is over rotation matrices U ∈Rk×k):"
REFERENCES,0.5237543453070683,"d(B,B′)=min
U ∥B−UB′∥2.
(A.97)"
REFERENCES,0.5249130938586327,"We state a more precise version of Theorem 3.4—basically we ﬁx all problem parameters except B0
(which limits to B⋆). To deﬁne the limit, we consider a sequence of pretrained feature extractors:
{Bi
0}∞
i=1. We deﬁne the corresponding limit points of ﬁne-tuning and linear probing when we start
from the i-th pretrained feature extractor. That is, let vfti(t),Bft
i(t) denote the parameters at time t
of ﬁne-tuning if we initialize with v0,Bi
0 (see Equation 3.2 for the ﬁne-tuning updates). Let v∞
lp
i,Bi
0
be the linear probing solution when initialized with v0,Bi
0 (see Equation 3.5 for the linear probing
updates). We note that the LP iterates converge to v∞
lp
i,Bi
0 as a result of gradient ﬂow on a convex
problem."
REFERENCES,0.526071842410197,"Finally, Theorem 3.4 says that as the pretrained representations get better, linear probing does much
better than ﬁne-tuning OOD:"
REFERENCES,0.5272305909617613,"Theorem A.9 (Formal statement of Theorem 3.4). In the linear overparameterized setting, under
the ID subspace assumption, ﬁx the dimensions of the setting d,k,m, number of examples n, the ID
subspace S, ID distribution Pid, the distribution over the head v0, and the ground truth parameters
v⋆,B⋆. Assume the non-degeneracy conditions cosθmax(R∗,S) > 0 and cosθmax(R∗,S⊥) > 0 where
R∗= rowspace(B⋆). Given a sequence of pretrained feature extractors {Bi
0}∞
i=1 with Bi
0 →B⋆,
where the limit is in the pseudometric given by Deﬁnition A.8, the ratio of OOD errors of linear
probing and ﬁne-tuning converges in probability to 0:"
REFERENCES,0.5283893395133256,"Lood(v∞
lp
i,Bi
0)"
REFERENCES,0.52954808806489,"inft≥0Lood(vfti(t),Bft
i(t))"
REFERENCES,0.5307068366164542,"p→0,as i→∞.
(A.98)"
REFERENCES,0.5318655851680185,Published as a conference paper at ICLR 2022
REFERENCES,0.5330243337195828,"The purpose of the inﬁmum is to capture the fact that the bound holds for all times t for ﬁne-tuning
(and therefore also for the limit v∞
ft ,B∞
ft when it exists). Note that the ratio is a random variable
because the training data is sampled from Pid and the head is sampled (v0 ∼N(0,σ2I) for some σ2)."
REFERENCES,0.5341830822711472,"Proof. Recall that we say a sequence of real-valued random variables converges in probability to 0
(written as Xi
p→0) if for every ϵ′,δ >0, for all large enough i (that is, for all i≥Ni for some Ni), we
have:"
REFERENCES,0.5353418308227115,"P(|Xi|>ϵ′)≤δ.
(A.99)"
REFERENCES,0.5365005793742758,"Accordingly, ﬁx arbitrary ϵ′,δ >0, and we will show that the ratio of errors is eventually smaller than
ϵ′ with probability at least 1−δ."
REFERENCES,0.5376593279258401,"Lower bounding ﬁne-tuning error:
Since Bi
0
→B⋆, from Lemma A.11 we have that
cosθmax(Ri,S⊥) →cosθmax(R∗,S⊥) where Ri = rowspace(Bi
0). Since cosθmax(R∗,S⊥) > 0, this
means that for all large enough i we have:"
REFERENCES,0.5388180764774044,"cosθmax(Ri,S⊥)>cosθmax(R∗,S⊥)/2.
(A.100)"
REFERENCES,0.5399768250289687,"Next, from Lemma A.13, we have that with probability at least 1 −δ/2, Head-Error(v0, v⋆) =
|(v⊤
0 v⋆)2 −(v⊤
⋆v⋆)2| ≥cδ for some cδ > 0. Plugging this into the ﬁne-tuning bound in Theorem 3.2,
this means that for all large enough i with probability at least 1−δ/2:"
REFERENCES,0.541135573580533,"inf
t≥0 q"
REFERENCES,0.5422943221320974,"Lood(vfti(t),Bft
i(t))≥c′
δ−d(Bi
0,B⋆),
(A.101)"
REFERENCES,0.5434530706836617,"for some c′
δ >0. But since Bi
0 →B⋆we have d(Bi
0,B⋆)→0 as i→∞. So this means that for all large
enough i with probability at least 1−δ/2:"
REFERENCES,0.544611819235226,"inf
t≥0Lood(vft
i(t),Bft
i(t))≥c′′
δ,
(A.102)"
REFERENCES,0.5457705677867902,"for some c′′
δ >0."
REFERENCES,0.5469293163383546,"Upper bounding the linear probing error: Since Bi
0 →B⋆, from Lemma A.11 we have that
cosθmax(Ri,S)→cosθmax(R∗,S) and so since cosθmax(R∗,S)>0, for all large enough i we have:"
REFERENCES,0.5480880648899189,"cosθmax(Ri,S)>cosθmax(R∗,S)/2.
(A.103)"
REFERENCES,0.5492468134414832,"Plugging this into the RHS of Lemma A.15, Equation A.133, which upper bounds the OOD error of
linear probing, we get that for all large enough i, with probability at least 1−δ/2:"
REFERENCES,0.5504055619930475,"Lood(v∞
lp
i,Bi
0)≤uδ(d(Bi
0,B⋆))2,
(A.104)"
REFERENCES,0.5515643105446119,"for some uδ > 0. Again since d(Bi
0,B⋆) →0 as i →∞, this means for all large enough i, with
probability at least 1−δ/2, d(Bi
0,B⋆) will be small enough so that:"
REFERENCES,0.5527230590961761,"Lood(v∞
lp
i,Bi
0)≤c′′
δϵ.
(A.105)"
REFERENCES,0.5538818076477404,"Taking the ratio: So taking the ratio of the lower bound for ﬁne-tuning, and upper bound for linear
probing, we get with with probability at least 1−δ:"
REFERENCES,0.5550405561993047,"Lood(v∞
lp
i,Bi
0)"
REFERENCES,0.5561993047508691,"inft≥0Lood(vfti(t),Bft
i(t)) ≤ϵ,
(A.106)"
REFERENCES,0.5573580533024334,as desired.
REFERENCES,0.5585168018539977,We now prove the Lemmas that we used in the above proof.
REFERENCES,0.5596755504055619,Published as a conference paper at ICLR 2022
REFERENCES,0.5608342989571263,"A.3.1
CONVERGENCE OF PRINCIPAL ANGLE
Theorem 3.4 assumes conditions on the angle between the perfect feature extractor B⋆and the ID
subspace S. However, ﬁne-tuning and linear probing start from features B0 with some error, and do
not get access to B⋆. We show that if B0 and B⋆are close, then the angles between their rowspaces
to a third subspace T (which could be the the ID subspace S) is similar."
REFERENCES,0.5619930475086906,"Lemma A.10. Given two feature extractors B0, B⋆∈Rk×d with orthonormal rows, where
R0 =rowspace(B0),R∗=rowspace(B⋆), and a subspace T with dimension at least 1, we have:"
REFERENCES,0.5631517960602549,"|cosθmax(R0,T)−cosθmax(R∗,T)|≤d(B0,B⋆)
(A.107)"
REFERENCES,0.5643105446118193,"Proof. Recall that k = dim(R0) = dim(R∗). Let r = min(k,dim(T)) and let F be a d-by-dim(T)
matrix with orthonormal columns that form a basis for T. We have, for arbitrary rotation matrix
U ∈Rk×k:"
REFERENCES,0.5654692931633836,"cosθmax(R0,T)=σr(B0F)
(A.108)
=σr(UB0F)
(A.109)
=σr(B⋆F +(UB0−B⋆)F)
(A.110)
≥σr(B⋆F)−σ1((UB0−B⋆)F)
(A.111)
≥σr(B⋆F)−σ1(UB0−B⋆)
(A.112)
=σr(B⋆F)−∥UB0−B⋆∥2
(A.113)
=cosθmax(R∗,T)−∥UB0−B⋆∥2
(A.114)"
REFERENCES,0.5666280417149478,"Here in the ﬁrst step we used the deﬁnition of cosθmax (Deﬁnition 3.1), and the fact that B⊤
0 has
orthonormal columns which form a basis for R0 (the rowspace of B0), so in Deﬁnition 3.1 we can
subtitute E =B⊤
0 . To get Equation A.111 we used Weyl’s theorem, which bounds the singular value
under perturbations: σr(A + B) ≥σr(A) −σ1(B). To get Equation A.112 we used the fact that
∥Fv∥2=∥v∥since F has orthonormal columns."
REFERENCES,0.5677867902665121,"Since this holds for all rotation matrices U, we can take the minimum over U to get:"
REFERENCES,0.5689455388180765,"cosθmax(R0,T)≥cosθmax(R∗,T)−min
U ∥UB0−B⋆∥2=cosθmax(R∗,T)−d(Bi
0,B⋆)
(A.115)"
REFERENCES,0.5701042873696408,"Since the relationship between B0 and B⋆are symmetric (and the distance d is symmetric), this gives
us the desired result:"
REFERENCES,0.5712630359212051,"|cosθmax(R0,T)−cosθmax(R∗,T)|≤d(B0,B⋆)
(A.116)"
REFERENCES,0.5724217844727694,"Lemma A.11. Given a sequence of pretrained feature extractors {Bi
0}∞
i=1 with Bi
0 →B⋆, where
Bi
0,B⋆∈Rk×d have orthonormal rows, let Ri = rowspace(Bi
0),R∗= rowspace(B⋆). Then for any
subspace T, we have:"
REFERENCES,0.5735805330243338,"cosθmax(Ri,T)→cosθmax(R∗,T),as i→∞.
(A.117)"
REFERENCES,0.574739281575898,"Proof. This follows directly from Lemma A.10. Bi
0 →B⋆means d(Bi
0, B⋆) →0. Then from
Lemma A.10:"
REFERENCES,0.5758980301274623,"|cosθmax(Ri,T)−cosθmax(R∗,T)|→0,as i→∞
(A.118)
This means cosθmax(Ri,T)→cosθmax(R∗,T) as i→∞"
REFERENCES,0.5770567786790266,"A.3.2
BOUNDING THE HEAD ERROR
We prove a lower bound on Head-Error(v0, v⋆) = |(v⊤
0 v⋆)2 −(v⊤
⋆v⋆)2|, which was a key term
in the ﬁne-tuning lower bound (Theorem 3.2). Note that if the head is initialized as v0 = 0, then
Head-Error(v0, v⋆) = ∥v⋆∥2
2= ∥w⋆∥2
2. In practice, the head is usually initialized randomly, for
example normally distributed. Intuitively, the head error is still high because we do not know which
direction the head is pointing in, so most of the time the initial (randomly sampled) head will be
pointing in the wrong direction. If v0 ∼N(0,σ2I) can show that for any σ2, the head error will still
typically be at least Ω(∥v⋆∥2) This is an illustrative result, one can show similar results for other
random initializations as well."
REFERENCES,0.578215527230591,"We ﬁrst prove an anti-concentration lemma, which says that if u is univariate Gaussian, then it cannot
be too close to any particular constant a, no matter how the variance of the Gaussian is chosen."
REFERENCES,0.5793742757821553,Published as a conference paper at ICLR 2022
REFERENCES,0.5805330243337196,"Lemma A.12. For some universal constant c, given a > 0, for all ν2 if u ∼N(0,ν2) then for all
0≤δ≤1:"
REFERENCES,0.5816917728852838,"P(|u−a|≤cδa)≤δ
(A.119)"
REFERENCES,0.5828505214368482,"Proof. Consider δ such that δ ≤1/10. Then for all u with |u−a| ≤δa, we have u ≥9a/10. For all
u≥9a/10, the density f(u) is upper bounded (from the formula for the density of a Gaussian random
variable) by:"
REFERENCES,0.5840092699884125,f(u)≤O(1
REFERENCES,0.5851680185399768,v exp −92a2
REFERENCES,0.5863267670915412,"2·102v2 )
(A.120)"
REFERENCES,0.5874855156431055,"We can maximize this explicitly (e.g., use Mathematica or by taking the logarithm and then setting the
derivative to 0) and we get for some universal constant c′ ≥10 (it is OK to choose a larger universal
constant than needed):"
REFERENCES,0.5886442641946698,f(u)≤c′
REFERENCES,0.589803012746234,"a
(A.121)"
REFERENCES,0.5909617612977984,"Since the density is less than c′/a and if |u −a| ≤δa the size of the interval is 2δa, we get for all
δ≤1/10:"
REFERENCES,0.5921205098493627,P(|u−a|≤δa)≤2c′δa
REFERENCES,0.593279258400927,"a
=2c′δ
(A.122)"
REFERENCES,0.5944380069524913,"Now, we substitute δ′ =2c′δ. We get for all δ′ ≤2c′/10:"
REFERENCES,0.5955967555040557,P(|u−a|≤1
REFERENCES,0.59675550405562,"2c′ δ′a)≤δ′
(A.123)"
REFERENCES,0.5979142526071842,"Since c′ ≥10, 2c′/10≥1, so the statement is true for all 0≤δ′ ≤1."
REFERENCES,0.5990730011587485,"We now bound the error in the head if the initialization is Gaussian.
This bound holds for all
initialization variances σ2. Similar bounds can be shown for other (non-Gaussian) head initializations
using similar anti-concentration arguments."
REFERENCES,0.6002317497103129,"Lemma A.13. For some universal constant c, for all v⋆∈Rk with v⋆̸= 0, σ ∈R+, δ ∈[0,1], if
v0 ∼N(0,σ2Ik), we have with probability at least 1−δ:"
REFERENCES,0.6013904982618772,"(Head-Error(v0,v⋆))2 :=|(v⊤
0 v⋆)2−(v⊤
⋆v⋆)2|≥cδ(v⊤
⋆v⋆)2
(A.124)"
REFERENCES,0.6025492468134415,"Proof. First note that Head-Error(v0,v⋆) = Head-Error(−v0,v⋆) and v0 is symmetric around 0 (v0
and −v0 have the same probability), and is almost surely not exactly 0. So without loss of generality,
we can suppose that v⊤
0 v⋆≥0."
REFERENCES,0.6037079953650057,"Sufﬁces to bound |v⊤
0 v⋆−v⊤
⋆v⋆|: We decompose the error:"
REFERENCES,0.6048667439165701,"|(v⊤
0 v⋆)2−(v⊤
⋆v⋆)2|=|v⊤
0 v⋆−v⊤
⋆v⋆|(|v⊤
0 v⋆+v⊤
⋆v⋆|)
(A.125)"
REFERENCES,0.6060254924681344,"≥|v⊤
0 v⋆−v⊤
⋆v⋆|(v⊤
⋆v⋆)|
(A.126)"
REFERENCES,0.6071842410196987,"So we bound |v⊤
0 v⋆−v⊤
⋆v⋆|."
REFERENCES,0.608342989571263,"v⊤
0 v⋆is normally distributed: We note that v⊤
0 v⋆is distributed as:"
REFERENCES,0.6095017381228274,"v⊤
0 v⋆∼N(0,σ2v⊤
⋆v⋆)
(A.127)"
REFERENCES,0.6106604866743917,"In other words, a normal with mean 0, and variance σ2
1 = σ2v⊤
⋆v⋆, and therefore standard deviation
σ1 =σ
p"
REFERENCES,0.6118192352259559,"v⊤
⋆v⋆."
REFERENCES,0.6129779837775203,"Apply Gaussian anti-concentration lemma: Then, from Lemma A.12, we have for some universal
constant c that with probability at least 1−δ:"
REFERENCES,0.6141367323290846,"|v⊤
0 v⋆−v⊤
⋆v⋆|≥cδv⊤
⋆v⋆
(A.128)"
REFERENCES,0.6152954808806489,"So substituting this back into Equation A.125, we get the desired result:"
REFERENCES,0.6164542294322132,"|(v⊤
0 v⋆)2−(v⊤
⋆v⋆)2|≥cδ(v⊤
⋆v⋆)2
(A.129)"
REFERENCES,0.6176129779837776,Published as a conference paper at ICLR 2022
REFERENCES,0.6187717265353418,"A.3.3
UPPER BOUNDING LINEAR PROBING ERROR
We showed a lower bound for the OOD error of ﬁne-tuning in Theorem 3.2. To compare this with
linear probing, we prove an upper bound on the OOD error of linear probing."
REFERENCES,0.6199304750869061,"For completeness we include an elementary lemma (note that the condition that the matrices are tall
is important for composing σmin, unlike for σmax, and we included this lemma to be careful about
these conditions):"
REFERENCES,0.6210892236384704,"Lemma A.14. Suppose we have two matrices A, B of shape (r,s) and (s,t) respectively, and they
are tall matrices so r≥s≥t. Then we have:"
REFERENCES,0.6222479721900348,"σmin(AB)≥σmin(A)σmin(B)
(A.130)"
REFERENCES,0.6234067207415991,"Proof. For a tall matrix A, we have:"
REFERENCES,0.6245654692931634,"σmin(A)= min
∥x∥2≤1∥Ax∥2
(A.131)"
REFERENCES,0.6257242178447276,So we have:
REFERENCES,0.626882966396292,"σmin(AB)= min
∥x∥2≤1∥ABx∥2≥σmin(A)σmin(B) min
∥x∥2≤1∥x∥2
(A.132)"
REFERENCES,0.6280417149478563,And min∥x∥2≤1∥x∥2=1 which completes the proof.
REFERENCES,0.6292004634994206,"Lemma A.15. In the linear overparameterized setting, under the ID subspace assumption, ﬁx
arbitrary Pz. Then there exists cδ such that with probability at least 1−δ, for all d,n,m,k,w⋆, feature
extractors B⋆,B0, and ID subspaces S with corresponding F (whose columns are orthonormal and
form a basis for S), if cosθmax(S,R)>0, we have: q"
REFERENCES,0.6303592120509849,"Lood(v∞
lp ,B0)≤

cδ
cosθmax(S,R)"
REFERENCES,0.6315179606025493,"2
d(B0,B⋆)||w∗||2
(A.133)"
REFERENCES,0.6326767091541136,"If Pz is isotropic Gaussian so N(0,Im), then we derive a bound for cδ analytically: if n ≥5m and
n≥10log 1"
REFERENCES,0.6338354577056778,"δ then with probability at least 1−δ, the linear probing OOD error is upper bounded by: q"
REFERENCES,0.6349942062572422,"Lood(v∞
lp ,B0)≤O

log(n/δ)
(cosθmax(R,S))2 d(B0,B⋆)∥w⋆∥2

(A.134)"
REFERENCES,0.6361529548088065,"Proof. From the ID subspace assumption, the data matrix X of shape (n, d) can be written as
X = ZF ⊤where Z be a matrix of shape (n,m) with each row Zi sampled iid from Pz, and F is a
matrix of shape (d,m) whose columns are orthonormal and form a basis for the ID subspace S."
REFERENCES,0.6373117033603708,"Let ϵ = ∥B⋆−B0∥2≤.
We ﬁrst prove the bounds for ϵ, in terms of d(B0, B⋆) and we later
handle the fact that the feature extractor distance involves the min over rotation matrices U:
d(B0,B⋆)=minU∥UB0−B⋆∥2."
REFERENCES,0.6384704519119351,"Bounding key singular values: Before proceeding with the proof, we examine a key quantity
XB⊤
0 = ZF ⊤B⊤
0 which comes up in the Hessian of the loss function. We will show that this is
invertible almost surely, and get a lower bound on its min singular value."
REFERENCES,0.6396292004634995,"First, we examine the shapes of the matrices. ZF ⊤B⊤
0 has shape (n,d) where Z has shape (n,m)
and F ⊤B⊤
0 has shape (m,k). Since n ≥m > k we have that Z and F ⊤B⊤
0 are tall matrices, and so
from Lemma A.14 we can write the min singular value of ZF ⊤B⊤
0 as:"
REFERENCES,0.6407879490150638,"σmin(ZF ⊤B⊤
0 )≥σmin(Z)σmin(F ⊤B⊤
0 )
(A.135)"
REFERENCES,0.641946697566628,"Now from the deﬁnion of the principal angle (Deﬁnition 3.1), we have:"
REFERENCES,0.6431054461181923,"σmin(F ⊤B⊤
0 )=cosθmax(R,S)>0.
(A.136)"
REFERENCES,0.6442641946697567,"Since we assumed Pz has density in the ID subspace assumption, from Lemma 3 in Xie et al. (2021a)
we get that for some c′
δ >0 that depends on δ and Pz, with probability at least 1−δ:"
REFERENCES,0.645422943221321,"σmin(Z)≥c′
δ
(A.137)"
REFERENCES,0.6465816917728853,"Note that this also means that σmin(ZF ⊤B⊤
0 ) > 0 and so XB⊤
0 = ZF ⊤B⊤
0 has full rank k almost
surely. This also implies that B0X⊤XB⊤
0 is a matrix of shape (k,k) that is invertible almost surely."
REFERENCES,0.6477404403244496,Published as a conference paper at ICLR 2022
REFERENCES,0.6488991888760139,"Main proof Since B0X⊤XB⊤
0 is invertible almost surely, there is a unique global minimum
(minimizing over v) to the loss optimized by linear probing:"
REFERENCES,0.6500579374275782,"argmin
v ∥XB⊤
0 v−XB⊤
⋆v⋆∥2
2=(B0X⊤XB⊤
0 )−1B0X⊤XB⊤
⋆v⋆
(A.138)"
REFERENCES,0.6512166859791425,"We can see this by noting that the loss function on the LHS is strongly convex in v since the Hessian
B0X⊤XB⊤
0 is invertible. Then, gradient ﬂow converges to the unique minimizer on the RHS, so:"
REFERENCES,0.6523754345307068,"v∞
lp =(B0X⊤XB⊤
0 )−1B0X⊤XB⊤
⋆v⋆
(A.139)"
REFERENCES,0.6535341830822712,"We now bound the square-root OOD error (taking the square root makes it easier to apply triangle
inequalities), starting with the deﬁnition:
q"
REFERENCES,0.6546929316338355,"Lood(v∞
lp ,B0)=∥B⊤
⋆v⋆−B⊤
0 v∞
lp ∥2
(A.140)"
REFERENCES,0.6558516801853997,"≤∥(B⊤
⋆v⋆−B⊤
0 v⋆)+(B⊤
0 v⋆−B⊤
0 v∞
lp )∥2
(A.141)"
REFERENCES,0.657010428736964,"≤∥B⊤
⋆v⋆−B⊤
0 v⋆∥2
|
{z
}
(1)"
REFERENCES,0.6581691772885284,"+∥B⊤
0 v⋆−B⊤
0 v∞
lp )∥2
|
{z
}
(2)"
REFERENCES,0.6593279258400927,(A.142)
REFERENCES,0.660486674391657,We bound each term on the RHS of the last line. For term (1):
REFERENCES,0.6616454229432214,"∥B⊤
⋆v⋆−B⊤
0 v⋆∥2 ≤σmax(B⋆−B0)∥v⋆∥2
(A.143)
≤ϵ∥v⋆∥2
(A.144)
=ϵ∥w⋆∥2.
(A.145)"
REFERENCES,0.6628041714947857,"Where we note that ∥v⋆∥2= ∥w⋆∥2 because w⋆= B⊤
⋆v⋆where the rows of B⋆(columns of B⊤
⋆) are
orthonormal."
REFERENCES,0.6639629200463499,"Let Σ = X⊤X.
For term (2), we ﬁrst subtitute v∞
lp and do some algebra (again noting that
∥v⋆∥2=∥w⋆∥2) to get:"
REFERENCES,0.6651216685979142,"∥B⊤
0 v⋆−B⊤
0 v∞
lp ∥2 =∥B⊤
0 (B0ΣB⊤
0 )−1B0ΣB⊤
0 v⋆−B⊤
0 v∞
lp ∥2
(A.146)"
REFERENCES,0.6662804171494786,"=∥B⊤
0 (B0ΣB⊤
0 )−1B0Σ(B0−B⋆)⊤v⋆∥2
(A.147)"
REFERENCES,0.6674391657010429,"≤σmax(B⊤
0 (B0ΣB⊤
0 )−1B0Σ)σmax(B0−B⋆)∥w⋆∥2
(A.148)"
REFERENCES,0.6685979142526072,"≤σmax(B⊤
0 (B0ΣB⊤
0 )−1B0Σ)ϵ∥w⋆∥2
(A.149)"
REFERENCES,0.6697566628041715,"≤σmax(B0)2σmax(Σ)
1
σmin(B0ΣB⊤
0 )ϵ∥w⋆∥2
(A.150)"
REFERENCES,0.6709154113557358,≤σmax(B0)2σmax(X)2
REFERENCES,0.6720741599073001,"σmin(XB⊤
0 )2
ϵ∥w⋆∥2
(A.151)"
REFERENCES,0.6732329084588644,= σmax(B0)2σmax(ZF ⊤)2
REFERENCES,0.6743916570104287,"σmin(ZF ⊤B⊤
0 )2
ϵ∥w⋆∥2
(A.152)"
REFERENCES,0.6755504055619931,"≤
σmax(B0)2σmax(Z)2"
REFERENCES,0.6767091541135574,"σmin(Z)2(cosθmax(R,S))2 ϵ∥w⋆∥2
(A.153)"
REFERENCES,0.6778679026651216,(A.154)
REFERENCES,0.6790266512166859,"Where in the ﬁrst line we subtituted in the closed form for v∞
lp from Equation A.138, and in the
last line we used the fact that σmax(ZF ⊤) ≤σmax(Z) since F ⊤has orthonormal rows, and
σmin(ZF ⊤B⊤)=σmin(Z)cosθmax(R,S) as explained in Equation A.135 and Equation A.136."
REFERENCES,0.6801853997682503,"So it sufﬁces to bound the quantities in the RHS. Since B0 has orthonormal rows, σmax(B0)=1."
REFERENCES,0.6813441483198146,"No Gaussian assumption: For the ﬁrst part of the Theorem (Equation A.133 where we make no
Gaussian assumptions, but give a less quantitative bound), we just use the fact that σmax(Z) is upper
bounded almost surely, and σmin(Z) ≥c′
δ with probability at least 1−δ. This implies that for some
cδ >0 with probability at least 1−δ: q"
REFERENCES,0.6825028968713789,"Lood(v∞
lp ,B0)≤

cδ
cosθmax(S,R)"
REFERENCES,0.6836616454229433,"2
ϵ||w∗||2,
(A.155)"
REFERENCES,0.6848203939745076,Published as a conference paper at ICLR 2022
REFERENCES,0.6859791425260718,where ϵ=∥B0−B⋆∥2.
REFERENCES,0.6871378910776361,"Gaussian assumption: For the second part of the Theorem (Equation A.134 where we assume Pz is
Gaussian), we use results in random matrix theory to lower bound and upper bound σmin(Z). For the
lower bound we use a result from Rudelson & Vershynin (2009) (see page 4, in the equation below
Equation 1.11), since Z ∈Rn×m is a matrix with each entry sampled from N(0,1), we get for all t>0:"
REFERENCES,0.6882966396292005,"P(σmin(Z)≤√n−√m−t)≤e−t2/2
(A.156)
With a bit of algebra, this gives us that with probability at least 1−δ:"
REFERENCES,0.6894553881807648,σmin(Z)≥√n−√m− r 2log1
REFERENCES,0.6906141367323291,"δ
(A.157)"
REFERENCES,0.6917728852838934,We assumed n≥5m and n≥10log 1
REFERENCES,0.6929316338354577,"δ , so we get:"
REFERENCES,0.694090382387022,"σmin(Z)≥O(√n)
(A.158)
The upper bound is a standard matrix concentration bound—we use the high probability bound
in Theorem 4.1.1 from Tropp (2015) (see Section 4.2.2 which calculates the variance statistic for
rectangular Gaussian matrices, also notice the square on the LHS below):"
REFERENCES,0.6952491309385863,σmax(Z)2 ≤O(nlogn
REFERENCES,0.6964078794901506,"δ )
(A.159)"
REFERENCES,0.697566628041715,Substituting the lower and upper bounds on σmin(Z) into Equation A.146 we get:
REFERENCES,0.6987253765932793,"∥B⊤
0 v⋆−B⊤
0 v∞
lp ∥2≤O

log(n/δ)
(cosθmax(R,S))2 ϵ∥w⋆∥2

(A.160)"
REFERENCES,0.6998841251448435,"Substituting into equation A.140, we have: q"
REFERENCES,0.7010428736964078,"Lood(v∞
lp ,B0)≤O

log(n/δ)
(cosθmax(R,S))2 ϵ∥w⋆∥2

,
(A.161)"
REFERENCES,0.7022016222479722,where ϵ=∥B0−B⋆∥2. Which completes the proof of the second part (Equation A.134).
REFERENCES,0.7033603707995365,"Handling the rotation matrix U: We now handle the fact that the feature extractor distance involves
the min over rotation matrices U: d(B0,B⋆) = minU∥UB0 −B⋆∥2. Let v∞
lp (B0) denote the linear
probing head solution if we use a pretrained feature extractor B0. We ﬁrst note that for any k-by-k
rotation matrix U, we have:"
REFERENCES,0.7045191193511008,"Lood(v∞
lp (B0),B0)=Lood(v∞
lp (UB0),UB0).
(A.162)"
REFERENCES,0.7056778679026651,"This follows from using the closed form we derived above for v∞
lp (B0) and some simple algebraic
manipulation (e.g., recall that U −1 =Y ⊤):"
REFERENCES,0.7068366164542295,"(UB0)⊤v∞
lp (UB0)=(UB0)⊤(UB0X⊤XB⊤
0 U ⊤)−1UB0X⊤XB⊤
⋆v⋆
(A.163)"
REFERENCES,0.7079953650057937,"=B⊤
0 U ⊤U(B0X⊤XB⊤
0 )−1U ⊤UB0X⊤XB⊤
⋆v⋆
(A.164)"
REFERENCES,0.709154113557358,"=B⊤
0 (U ⊤U)(B0X⊤XB⊤
0 )−1(U ⊤U)B0X⊤XB⊤
⋆v⋆
(A.165)"
REFERENCES,0.7103128621089224,"=B⊤
0 (B0X⊤XB⊤
0 )−1B0X⊤XB⊤
⋆v⋆
(A.166)"
REFERENCES,0.7114716106604867,"=B⊤
0 v∞
lp (B0)
(A.167)"
REFERENCES,0.712630359212051,"So the ﬁnal predictors in both cases, (UB0)⊤v∞
lp (UB0) and B⊤
0 v∞
lp (B0) are identical. This means
that the OOD error Lood(v,B)=∥B⊤v−B⊤
⋆v⋆∥2 is the same in both cases."
REFERENCES,0.7137891077636153,"This means that we can just take the min over all rotation matrices U (where the ﬁrst step follows
since the identity matrix is a rotation matrix, and the second step is from Equation A.155):
Lood(v∞
lp (B0),B0)≤min
U Lood(v∞
lp (UB0),UB0)
(A.168)"
REFERENCES,0.7149478563151797,"≤min
U"
REFERENCES,0.7161066048667439,"
c(δ)
cosθmax(S,R)"
REFERENCES,0.7172653534183082,"2
∥UB0−B⋆∥2||w∗||2
2
(A.169)"
REFERENCES,0.7184241019698725,"=

c(δ)
cosθmax(S,R)"
REFERENCES,0.7195828505214369,"2
d(B0,B⋆)||w∗||2
2,
(A.170)"
REFERENCES,0.7207415990730012,"which is as desired. We repeat the same thing for Equation A.161 to get Equation A.134 in the
Theorem statement."
REFERENCES,0.7219003476245655,Published as a conference paper at ICLR 2022
REFERENCES,0.7230590961761297,"A.4
LP VS. FT (OOD), NON-ASYMPTOTIC RESULT FOR GAUSSIAN COVARIATES
Theorem 3.4 showed an asymptotic result: if the error d(B0, B⋆) →0, then linear probing (LP)
achieves better out-of-distribution (OOD) error than ﬁne-tuning (FT). Here we give a more quanti-
tative version of Theorem 3.4 for Gaussian covariates. The result can be extended to the case there
each entry of Pz is independent and identically distributed, mean-zero, constant non-zero variance,
but instead of Gaussian is sub-Gaussian with constant sub-Gaussian variance / moment—this can be
shown using Theorem 1.1 in Rudelson & Vershynin (2009), which is a different matrix concentration
inequality."
REFERENCES,0.7242178447276941,"We show that LP does better than FT out-of-distribution if the error is less than a speciﬁc quantity (in
terms of the representation dimension k, and the angles between the ID subspace S and the important
pretrained directions R∗=rowspace(B⋆))."
REFERENCES,0.7253765932792584,"Theorem A.16. In the linear overparameterized setting, under the ID subspace assumption, as-
sume the non-degeneracy conditions cosθmax(R∗,S) > 0 and cosθmax(R∗,S⊥) > 0 where R∗=
rowspace(B⋆). Suppose the covariates are generated from a Gaussian distribution on the ID sub-
space S, so Pz =N(0,Im). Let ∥w⋆∥2 be a ﬁxed constant. Given failure probability 1≤δ >0, for all
w⋆,B0,n,d,k,ϵ, ifn≥5m, andn≥10log 1"
REFERENCES,0.7265353418308227,"δ , iftheerrorofthepretrainedrepresentationisnottoohigh:"
REFERENCES,0.727694090382387,"d(B0,B⋆)<O
cosθmax(R∗,S⊥)(cosθmax(R∗,S))2δ2 √"
REFERENCES,0.7288528389339514,klog(n/δ)
REFERENCES,0.7300115874855156,"
,
(A.171)"
REFERENCES,0.7311703360370799,"then with probability at least 1 −δ, the OOD error of linear probing is lower (better) than for
ﬁne-tuning at all time steps t≥0 in the ﬁne-tuning trajectory:"
REFERENCES,0.7323290845886443,"Lood(v∞
lp
i,Bi
0)< inf
t≥0Lood(v∞
lp
i,Bi
0).
(A.172)"
REFERENCES,0.7334878331402086,"Proof. Let ϵ = d(B0, B⋆).
We ﬁrst note that the condition in Equation A.171 implies that
d(B0,B⋆) < O(cosθmax(R∗,S⊥)) and d(B0,B⋆) < O(cosθmax(R∗,S)). This is because the cosine
angles are between 0 and 1, δ is between 0 and 1, and k and n are at least 1. We now simplify and
combine the linear probing and ﬁne-tuning bounds."
REFERENCES,0.7346465816917729,"Let R0 = rowspace(B0).
Warning: note that the Equation A.171 in the Theorem statement
assumes conditions on the angles between R∗(corresponding to the optimal representation) and
the ID subspace S. However, our results that bounded the ﬁne-tuning (Theorem 3.2) and linear
probing (Lemma A.134) errors require conditions on the angles between R0 (corresponding to the
representation that linear probing and ﬁne-tuning use) and S. So we have to be careful about this
distinction, and use Lemma A.10 to relate the two, which we do below."
REFERENCES,0.7358053302433372,"Fine-tuning: From Theorem 3.2, we get: p"
REFERENCES,0.7369640787949016,"Lood(vft(t),Bft(t))≥O
cosθmax(R0,S⊥)
√"
REFERENCES,0.7381228273464658,"k
min(φ,φ2/∥w⋆∥2)"
REFERENCES,0.7392815758980301,(1+∥w⋆∥2)2
REFERENCES,0.7404403244495944,"
−ϵ.
(A.173)"
REFERENCES,0.7415990730011588,"Where φ is the head-error, which we lower bounded in Lemma A.13—subtituting this bound and
noting that min(φ,φ2)=O(φ2), ∥v⋆∥2=∥w⋆∥2 (which we assumed is a constant), this gives us: p"
REFERENCES,0.7427578215527231,"Lood(vft(t),Bft(t))≥O
cosθmax(R0,S⊥)
√"
REFERENCES,0.7439165701042874,"k
δ2
−ϵ
(A.174)"
REFERENCES,0.7450753186558516,"Now, since d(B0,B⋆)=ϵ, we use Lemma A.10 to get that:"
REFERENCES,0.746234067207416,"cosθmax(R0,S⊥)≥cosθmax(R∗,S⊥)−ϵ
(A.175)"
REFERENCES,0.7473928157589803,"Subtituting this into Equation A.174, we get (notice the R∗instead of R0 below): p"
REFERENCES,0.7485515643105446,"Lood(vft(t),Bft(t))≥O
cosθmax(R∗,S⊥)−ϵ
√"
REFERENCES,0.7497103128621089,"k
δ2
−ϵ
(A.176)"
REFERENCES,0.7508690614136733,"Since ϵ≤O(cosθmax(R∗,S⊥)), this can be simpliﬁed to: p"
REFERENCES,0.7520278099652375,"Lood(vft(t),Bft(t))≥O
cosθmax(R∗,S⊥)
√"
REFERENCES,0.7531865585168018,"k
δ2
−ϵ
(A.177)"
REFERENCES,0.7543453070683661,Published as a conference paper at ICLR 2022
REFERENCES,0.7555040556199305,"Linear probing: From Lemma A.134, we get: q"
REFERENCES,0.7566628041714948,"Lood(v∞
lp ,B0)≤O

log(n/δ)
(cosθmax(R0,S))2 ϵ∥w⋆∥2

(A.178)"
REFERENCES,0.7578215527230591,"Again, we use Lemma A.10 to get:"
REFERENCES,0.7589803012746235,"cosθmax(R0,S)≥cosθmax(R∗,S)−ϵ
(A.179)"
REFERENCES,0.7601390498261877,"Substituting into Equation A.178, and using the fact that ϵ ≤O(cos θmax(R∗, S)), and since we
assumed ∥w⋆∥2 is a constant, we get: q"
REFERENCES,0.761297798377752,"Lood(v∞
lp ,B0)≤O

log(n/δ)
(cosθmax(R∗,S))2 ϵ

(A.180)"
REFERENCES,0.7624565469293163,Combining the two: We want to show that the OOD error of LP is less than for ﬁne-tuning:
REFERENCES,0.7636152954808807,"O

log(n/δ)
(cosθmax(R∗,S))2 ϵ

≤O
cosθmax(R∗,S⊥)
√"
REFERENCES,0.764774044032445,"k
δ2
−ϵ
(A.181)"
REFERENCES,0.7659327925840093,"We can bring the ϵ to the LHS, so this is equivalent to showing:"
REFERENCES,0.7670915411355735,"O

log(n/δ)
(cosθmax(R∗,S))2 ϵ

+ϵ≤O
cosθmax(R∗,S⊥)
√"
REFERENCES,0.7682502896871379,"k
δ2
(A.182)"
REFERENCES,0.7694090382387022,"Since log(n/δ)≥1 and cosθmax(R∗,S))2 is between 0 and 1, this is equivalent to folding the ϵ inside
the big-oh on the LHS:"
REFERENCES,0.7705677867902665,"O

log(n/δ)
(cosθmax(R∗,S))2 ϵ∥w⋆∥2

≤O
cosθmax(R∗,S⊥)
√"
REFERENCES,0.7717265353418308,"k
δ2
(A.183)"
REFERENCES,0.7728852838933952,"But assuming the condition on ϵ in Equation A.171 of the Theorem statement, this is easy to show
with a bit of algebra."
REFERENCES,0.7740440324449595,"A.5
PRINCIPAL ANGLES ARE LIKELY NON-ZERO
In Theorems 3.2, 3.4, and 3.5, we assumed the cosine of the largest principal angle between the
representations and ID subspace (or complement of the ID subspace) was non-zero. For example,
Theorem 3.4 assumed the largest principal angle between R∗= rowspace(B⋆) and the ID subspace
S is non-zero, and similarly for the angle between R∗and S⊥. Having an angle of 0 is a degenerate
condition. As an example, look at Figure 2—here the input dimension d = 2, the representation
dimension k=1, and the ID subspace S has dimension 1. The only way these angles can be 0 is if B⊤
⋆
is exactly in the same direction as S or S⊥, which seems like too much of a coincidence. intuitively,
if nature introduces even a small amount of randomness in either the optimal representation or ID
subspace, the angle will be non-zero."
REFERENCES,0.7752027809965237,"This example was in two dimensions—to make this intuition a bit more formal in higher dimensions,
we prove a simple claim. Lemma A.17 shows that if the S is a randomly selected m dimensional
subspace, then the angles cosθmax(R∗,S) and cosθmax(R∗,S⊥) are non-zero (and we get quantitative
lower bounds on them)."
REFERENCES,0.776361529548088,"Lemma A.17. Let R be a ﬁxed k dimensional subspace, and let S be a uniformly random m
dimensional subspace (formally, a uniform measure on the Grassmannian manifold) in Rd with
m>k. Then with probability at least 1−δ,"
REFERENCES,0.7775202780996524,"cosθmax(R,S)≥ √m−
√ k−
q"
REFERENCES,0.7786790266512167,"2log 1 δ
q"
REFERENCES,0.779837775202781,dlog 2d
REFERENCES,0.7809965237543454,"δ
(A.184)"
REFERENCES,0.7821552723059096,"In addition, we get that cosθmax(R,S)>0 almost surely (with probability 1)."
REFERENCES,0.7833140208574739,If m≥5k and m≥10log 1
REFERENCES,0.7844727694090382,"δ , then we get with probability at least 1−δ:"
REFERENCES,0.7856315179606026,"cosθmax(R,S)≥O
s"
REFERENCES,0.7867902665121669,"m
dlog 2d δ"
REFERENCES,0.7879490150637312,"
(A.185)"
REFERENCES,0.7891077636152954,"Recall that big-oh notation here means that the RHS is true for some universal constant (independent
of any other problem parameters)."
REFERENCES,0.7902665121668598,Published as a conference paper at ICLR 2022
REFERENCES,0.7914252607184241,"Proof. Note that principal angles are invariant if we rotate R and S by the same rotation matrix U.
That is, if we let U ∈Rd×d be a rotation matrix, and E ∈Rd×k, F ∈Rd×m have orthonormal columns
which form a basis for R and S respectively, then we have:"
REFERENCES,0.7925840092699884,"cosθmax(R,S)=σk(E⊤F)=σk((UE)⊤(UF))
(A.186)"
REFERENCES,0.7937427578215527,"This symmetry means that we can ﬁx S and instead consider R to be a uniform random k dimensional
subspace on the Grassmannian manifold. Without loss of generality, we can also ﬁx S to be the span
of the ﬁrst m standard basis vectors: (e1,...,em), where ei ∈Rd has a 1 in the i-th entry and a 0 in
every other entry."
REFERENCES,0.7949015063731171,"Equivalently, let MR be a d-by-k matrix, where each column is sampled independently from
N(0,Id)—since the columns of MR span a uniformly random k-dimensional subspace, we can let R
be range of MR. This is equivalent to sampling each entry of MR from N(0,1)."
REFERENCES,0.7960602549246814,"Let c=cosθmax(R,S). From Lemma A.2, c can be written as:"
REFERENCES,0.7972190034762456,"c=
min
r∈R,∥r∥2=1∥F ⊤r∥2=
min
r∈R,∥r∥2≥1∥F ⊤r∥2
(A.187)"
REFERENCES,0.7983777520278099,"Since R is the range of MR, any r∈R can be written as r=MRλ for some λ∈Rk. We ﬁrst show that
∥λ∥2 cannot be much smaller than ∥r∥2. This is because:"
REFERENCES,0.7995365005793743,"∥r∥2=∥MRλ∥2≤σmax(MR)∥λ∥2
(A.188)"
REFERENCES,0.8006952491309386,So this gives us:
REFERENCES,0.8018539976825029,"∥λ∥2≥
∥r∥2
σmax(MR)
(A.189)"
REFERENCES,0.8030127462340672,So every r∈R can be written as MRλ where ∥λ∥2 is lower bounded as above.
REFERENCES,0.8041714947856315,"We now simplify the deﬁnition of c, starting from Equation A.187."
REFERENCES,0.8053302433371958,"c=
min
r∈R,∥r∥2≥1∥F ⊤r∥2
(A.190)"
REFERENCES,0.8064889918887601,"≥
min
∥λ∥≥1/σmax(MR)∥F ⊤MRλ∥2
(A.191)"
REFERENCES,0.8076477404403245,"≥
min
∥λ∥≥1/σmax(MR)σmin(F ⊤MR)∥λ∥2
(A.192)"
REFERENCES,0.8088064889918888,= σmin(F ⊤MR)
REFERENCES,0.8099652375434531,"σmax(MR)
(A.193)"
REFERENCES,0.8111239860950173,"So now we want to lower bound the ratio of two random matrices. We note that F ⊤MR is a matrix of
size (m,k) with each entry sampled independently from N(0,1) (this is because F ⊤simple selects the
ﬁrstmrowsofMR). MR isamatrixofsize(d,k)witheachentrysampledindependentlyfromN(0,1)."
REFERENCES,0.8122827346465817,"Now, as in the Gaussian assumption step of the proof of Lemma A.15, we can apply standard matrix
concentration bounds (page 4, below Equation 1.11, in Rudelson & Vershynin (2009) for the bound
on σmin, and Theorem 4.1.1 in Tropp (2015) for the bound on σmax). We get that with probability at
least 1−δ:"
REFERENCES,0.813441483198146,"σmin(F ⊤MR)≥√m−
√ k− r 2log1"
REFERENCES,0.8146002317497103,"δ
(A.194)"
REFERENCES,0.8157589803012746,σmax(MR)≤ r
REFERENCES,0.816917728852839,dlog2d
REFERENCES,0.8180764774044033,"δ
(A.195)"
REFERENCES,0.8192352259559675,"Note that we can use alternate bounds for σmin in Rudelson & Vershynin (2009) that are sometimes
tighter."
REFERENCES,0.8203939745075318,"For the ratio of the two, we get that with probability at least 1−δ, we have:"
REFERENCES,0.8215527230590962,c≥σmin(F ⊤MR)
REFERENCES,0.8227114716106605,"σmax(MR)
≥ √m−
√ k−
q"
REFERENCES,0.8238702201622248,"2log 2 δ
q"
REFERENCES,0.8250289687137891,dlog 2d
REFERENCES,0.8261877172653534,"δ
(A.196)"
REFERENCES,0.8273464658169177,Published as a conference paper at ICLR 2022
REFERENCES,0.828505214368482,"For interpretability, ignoring log factors this is approximately:"
REFERENCES,0.8296639629200464,"c⪆
√m−
√ k
√"
REFERENCES,0.8308227114716107,"d
(A.197)"
REFERENCES,0.831981460023175,The result when m≥5k and n≥10log 2
REFERENCES,0.8331402085747392,δ follows with simple algebra.
REFERENCES,0.8342989571263036,"For the result where we show cosθmax(R,S) > 0 almost surely, we recall that F ⊤MR is a matrix of
size (m,k) with each entry sampled independently from N(0,1). Then applying Lemma 3 in Xie
et al. (2021a), we get that σmin(F ⊤MR) > 0 almost surely. Since σmax(MR) is ﬁnite, this gives us
cosθmax(R,S)>0 almost surely."
REFERENCES,0.8354577056778679,"In our case, the dimension of the ID subspace S is m, and the dimension of R∗= rowspace(B⋆) is
k, with k <m and k <d−m. If S is a uniformly random m-dimensional subspace, then S⊥is a uni-
formlyrandomd−mdimensionalsubspace. Inthiscase, LemmaA.17tellsusthatcosθmax(R∗,S)>0
and cosθmax(R∗,S⊥)>0 almost surely, and gives us quantitative lower bounds for these angles."
REFERENCES,0.8366164542294322,"A.6
LP VS. FT (ID)
We prove Proposition 3.5, where we show that if the representation is imperfect, then ﬁne-tuning
does better than linear probing, in-distribution."
REFERENCES,0.8377752027809965,"Restatement of Proposition 3.5. In the linear overparameterized setting, under the ID subspace
assumption (Assumption 3.3), let R0 = rowspace(B0), and Raug = Span({w⋆} ∪R0). Suppose
w⋆̸∈R0, cosθmax(S,Raug) ̸= 0, and that ﬁne-tuning converges to a local minimum of its loss, then
ﬁne-tuning does better ID almost surely: Lid(v∞
ft ,B∞
ft ) < Lid(v∞
lp ,B0) with probability 1 (over the
randomness of the training examples)."
REFERENCES,0.8389339513325609,"Proof. Fine-tuning gets 0 ID loss: It is well known from prior work (Laurent & von Brecht, 2018)
that all local minima are global for optimizing two layer linear networks under convex losses (which
is our setting), so if ﬁne-tuning converges to a local minimum, it actually converges to a global
minimum of the train loss. Since there exists parameters that achieve 0 loss on the training data
(namely, B⋆,v⋆), this means ﬁne-tuning gets 0 loss on the training data as well. So for all training
examples x (that is, rows of X):"
REFERENCES,0.8400926998841252,"v∞
ft
⊤B∞
ft x=w⊤
⋆x.
(A.198)
Since the models are linear, this implies that ﬁne-tuning gets all examples in the span of the training
examples correct as well. Since Pz has density, and the number of training examples n is at least as
large as the ID subspace dimension m, the training examples span the ID subspace almost surely, so
ﬁne-tuning gets every example in x∈S correct almost surely, giving us:"
REFERENCES,0.8412514484356894,"Lid(v∞
ft ,B∞
ft )=0
(A.199)"
REFERENCES,0.8424101969872537,"Linear probing gets positive ID loss: Lemma A.20 shows that the ID error of linear probing is
greater than zero under the same assumptions as this Proposition, so"
REFERENCES,0.8435689455388181,"Lid(v∞
lp ,B0)>0,
(A.200)"
REFERENCES,0.8447276940903824,which ﬁnishes the proof.
REFERENCES,0.8458864426419467,We now state and prove the Lemmas that we used to lower bound the ID error of linear probing.
REFERENCES,0.847045191193511,"Lemma A.18 gives conditions for when the projection F ⊤w of a vector w is not contained in the
projection Range(F ⊤E0) of the column space of a matrix E0."
REFERENCES,0.8482039397450754,"Lemma A.18. Let w∈Rd be a vector and F ∈Rd×m,E0 ∈Rd×k,Eaug ∈Rd×(k+1) have orthonormal
columns, with Range(Eaug)=Span({w}∪Range(E0)). If m>k, we have:"
REFERENCES,0.8493626882966396,"F ⊤Eaug is full rank
(A.201)"
REFERENCES,0.8505214368482039,"(a)
=⇒F ⊤Eaug has higher rank than F ⊤E0
(A.202)"
REFERENCES,0.8516801853997682,"(b)
⇐⇒F ⊤w̸∈Range(F ⊤E0)
(A.203)"
REFERENCES,0.8528389339513326,Published as a conference paper at ICLR 2022
REFERENCES,0.8539976825028969,"Proof. The proof of (a) is clear—F ⊤Eaug ∈Rm×(k+1) has rank k + 1 (since it is full rank and
m≥k+1), but F ⊤Eaug ∈Rm×k has rank at most k and is therefore lower rank. The assumption that
m>k is crucial here."
REFERENCES,0.8551564310544612,"For (b),
let a1, ... , ak be the columns of E0,
which form a basis for Range(E0).
Then F ⊤a1, ... , F ⊤ak, F ⊤w spans Range(F ⊤Eaug),
while F ⊤a1, ... , F ⊤ak
spans
Range(F ⊤E0).
So (notice the ﬁrst list of vectors has an additional F ⊤w) this means that
dim(Range(F ⊤Eaug))̸=dim(Range(F ⊤E0)) iff F ⊤w is linearly independent from the rest, that is,
F ⊤w ̸∈Range(F ⊤E0). Note that the rank of a matrix is the dimension of its range (column space),
that is, dim(Range(A))=rank(A) so this is what we wanted to show."
REFERENCES,0.8563151796060255,"The next Lemma says that if the projection F ⊤w⋆of the optimal linear model w⋆onto the ID
subspace S, is not contained in the projection Range(F ⊤E0) of the features, then linear probing
incurs non-zero ID error."
REFERENCES,0.8574739281575898,"Lemma A.19. In the linear overparameterized setting, under the ID subspace assumption, if F ⊤w⋆̸∈
Range(F ⊤E0), then Lid(v∞
lp ,B0) > 0, where E0 ∈Rd×k and F ∈Rd×m have orthonormal columns
that form a basis for the feature rowspace R0 =rowspace(B0) and ID subspace S respectively."
REFERENCES,0.8586326767091541,"Proof. We prove the contrapositive. Suppose Lid(v∞
lp ,B0)=0. This means that:"
REFERENCES,0.8597914252607184,"Lid(v∞
lp ,B0)= E
x∼Pid
[(v⊤
⋆B⋆x−v∞
lp
⊤B0x)2]=0
(A.204)"
REFERENCES,0.8609501738122828,"Since the squared error is always non-negative, this means that v∞
lp
⊤B0x=w⊤
⋆x almost surely when
x ∼Pid (recall that we deﬁned w⋆= B⊤
⋆v⋆). Recall Pid is deﬁned as: ﬁrst pick z ∈Pz (which has
density) and then output x=Fz. Since Pz has density, this implies that we get all examples in the ID
subspace S correct:"
REFERENCES,0.8621089223638471,"v∞
lp
⊤B0x=w⊤
⋆x for all x∈S.
(A.205)"
REFERENCES,0.8632676709154113,"Sincethe columns of F forman orthonormal basis for S, this givesus (since each column of F isin S):"
REFERENCES,0.8644264194669756,"v∞
lp
⊤B0F =w⊤
⋆F.
(A.206)"
REFERENCES,0.86558516801854,"Note that the rows of B0 also form an orthonormal basis for R0 just like the columns of E0. So we
can choose v with v⊤E⊤
0 =v∞
lp
⊤B0. Then we have:"
REFERENCES,0.8667439165701043,"v⊤E⊤
0 F =w⊤
⋆F ⇔F ⊤E0v=F ⊤w⋆
(A.207)"
REFERENCES,0.8679026651216686,"⇔F ⊤w⋆∈Range(F ⊤E0),
(A.208)"
REFERENCES,0.8690614136732329,"where we took the transpose of both sides in the ﬁrst step. This ﬁnishes the proof of the contraposi-
tive."
REFERENCES,0.8702201622247973,"Finally, Lemma A.20 combines Lemma A.18 and Lemma A.19 to give a more interpretable condition
for the ID error of linear probing: when the ID subspace S has some components along the optimal
linear model w⋆and the feature rowspace R0, then linear probing has non-zero error. This is measured
in terms of the principal angle cosθmax(Raug,S) between the ID subspace S and Raug which is the
span of R0 combined with w⋆. This angle will typically be non-zero—as an illustrative example,
from Lemma A.17 we have that this angle will be non-zero almost surely if the ID subspace S is a
uniformly random subspace."
REFERENCES,0.8713789107763615,"Lemma A.20. In the linear overparameterized setting, under the ID subspace assumption, let
R0 = rowspace(B0), and Raug = Span({w⋆} ∪R0). If w⋆̸∈R0 and cosθmax(Raug,S) > 0, then
Lid(v∞
lp ,B0)>0."
REFERENCES,0.8725376593279258,"Proof. After a bit of setup, the proof simply combines Lemma A.18 and Lemma A.19. If w⋆̸∈R0,
then Raug has dimension k +1. Let Eaug ∈Rd×(k+1),F ∈Rd×m have orthonormal columns which
form a basis for Raug and S respectively. We assumed cos θmax(Raug, S) = σmin(F ⊤Eaug) > 0
which means that F ⊤Eaug is full rank. The ID subspace assumption assumes that m > k. So from
Lemma A.18, F ⊤w⋆̸∈Range(F ⊤E0) where E0 ∈Rd×k has orthonormal columns that form a basis
for R0. Then from Lemma A.19, Lid(v∞
lp ,B0)>0."
REFERENCES,0.8736964078794901,Published as a conference paper at ICLR 2022
REFERENCES,0.8748551564310545,"A.7
LP-FT
We start by showing a simple proposition, that if the initial feature extractor is perfect, then linear
probing recovers the optimal weights."
REFERENCES,0.8760139049826188,"Proposition A.21. In the overparameterized linear setting, let R = rowspace(B0). If B0 = B⋆, and
cosθmax(S,R)>0, then Lood(v∞
lp ,B0)=0 for all t."
REFERENCES,0.8771726535341831,"Proof. We ﬁrst show that because cosθmax(R,S) > 0, the training loss for linear probing is strongly
convex. Recall that the training loss is:"
REFERENCES,0.8783314020857474,"bL(v,B)=∥XB⊤v−Y ∥2
2
(A.209)"
REFERENCES,0.8794901506373117,"Linear probing keeps B ﬁxed as B0 =B⋆and only tunes v, so we are interested in the Hessian of the
loss with respect to v evaluated at v,B⋆:"
REFERENCES,0.880648899188876,"Hessv bL(v,B⋆)=2(B⋆X⊤)(B⋆X⊤)⊤
(A.210)"
REFERENCES,0.8818076477404403,"For strong convexity, it sufﬁces to show that the min singular value of the Hessian is bounded away
from 0 by a constant. Recall the deﬁnition of cosθmax(R,S). For some F whose columns form an
orthonormal basis for S, we have (since the rows of B⋆form an orthonormal basis for R):"
REFERENCES,0.8829663962920047,"σk(B⋆F)=cosθmax(R,S)>0
(A.211)"
REFERENCES,0.884125144843569,"Note that B⋆F is a k-by-n matrix, so if the k-th singular value is positive it must be full rank. Since the
columns of X⊤span F (since we deﬁned F to be such that the columns of F are an orthonormal basis
for S, i.e. the rows of X), this means B⋆X⊤is rank k. But that means the Hessian (B⋆X⊤)(B⋆X⊤)⊤
is rank k as well. So the linear probing loss is strongly convex."
REFERENCES,0.8852838933951332,"Since the loss is strongly convex, there is a unique minimizer, and gradient ﬂow converges to that.
However, since we are in the well-speciﬁed setting, we know the training loss is:"
REFERENCES,0.8864426419466975,"bL(v,B⋆)=∥XB⊤
⋆v−XB⊤
⋆v⋆∥2
2
(A.212)"
REFERENCES,0.8876013904982619,"So v = v⋆achieves 0 loss and must be the (unique) minimizer. Therefore we have shown that linear
probing converges to the unique minimizer v∞
lp =v⋆, which attains 0 loss, as desired."
REFERENCES,0.8887601390498262,"Note that the entire proof works out if B0 =UB⋆for some rotation matrix U. In that case, the Hessian
becomes 2U(B⋆X⊤)(B⋆X⊤)⊤U ⊤which is still rank k, since multiplying by square rotation matri-
ces does not change the rank. In this case, the minimizer of the loss is v=Uv⋆, since (UB⋆)⊤(Uv⋆)=
B⊤
⋆v⋆. So linear probing converges to v∞
lp =Uv⋆, which achieves 0 loss, as desired."
REFERENCES,0.8899188876013905,"Restatement of Proposition 3.6.
Given perfect pretrained features B0
=
UB⋆
for
some rotation U.
Let R0
=
rowspace(B0).
Under the non-degeneracy conditions
cosθmax(R0,S)̸=0,cosθmax(R0,S⊥)̸=0:"
REFERENCES,0.8910776361529548,"∀t,Lood(Bft(t)⊤vft(t))>0, if v0 ∼N(0,σ2I) is randomly initialized (FT),
(A.213)"
REFERENCES,0.8922363847045192,"∀t,Lood(Bft(t)⊤vft(t))=0, if v0 is initialized to v∞
lp (LP-FT).
(A.214)"
REFERENCES,0.8933951332560834,"Proof. We ﬁrst use Proposition A.21, which in the proof we showed still works if B0 = UB⋆for
some rotation matrix U (which doesn’t have to be identity). We get that v∞
lp = Uv⋆. Then we have
B⊤
0 v∞
lp =B⊤
⋆v⋆=w⋆."
REFERENCES,0.8945538818076477,"We now just show that the gradients with respect to the training loss bL at (v∞
lp ,B0) is 0, so gradient
ﬂow does not update the parameters at all."
REFERENCES,0.895712630359212,The training loss is:
REFERENCES,0.8968713789107764,"bL(v,B)=∥XB⊤v−XB⊤
⋆v⋆∥2
2
(A.215)
The derivative with respect to v is:"
REFERENCES,0.8980301274623407,"∂v bL(v,B)=2BX⊤(XB⊤v−XB⊤
⋆v⋆)
(A.216)"
REFERENCES,0.899188876013905,"Then since B⊤
0 v∞
lp =B⊤
⋆v⋆, we have:"
REFERENCES,0.9003476245654692,"∂v bL(v∞
lp ,B0)=0
(A.217)"
REFERENCES,0.9015063731170336,Published as a conference paper at ICLR 2022
REFERENCES,0.9026651216685979,"Next, the derivative with respect to B is:"
REFERENCES,0.9038238702201622,"∂B bL(v,B)=2v(XB⊤v−XB⊤
⋆v⋆)⊤X
(A.218)
Then since B⊤
0 v∞
lp =B⊤
⋆v⋆, we have:"
REFERENCES,0.9049826187717266,"∂B bL(v∞
lp ,B0)=0
(A.219)"
REFERENCES,0.9061413673232909,"So since both the derivatives are 0, we have ∂tvft(t) = 0 and ∂BBft(t) = 0, which means the
parameters don’t change at all—at all times t we have vft(t)=Uv⋆and Bft(t)=UB⋆which gives us
zero OOD loss: Lood(Bft(t)⊤vft(t))=0 as desired."
REFERENCES,0.9073001158748552,"B
MORE INFORMATION ON EXPERIMENTS"
REFERENCES,0.9084588644264194,"In this Appendix, we include more details on the datasets, pretraining methods, and adaptation
methods. We also include the OOD accuracies for ﬁne-tuning and linear probing if we early stop and
choose the learning rate based on OOD data, where we see that linear probing is still typically better
than ﬁne-tuning OOD. Finally, we include results for additional baselines, pretraining models, and
conclude with a discussion about the effective robustness of LP-FT."
REFERENCES,0.9096176129779838,"B.1
OVERVIEW OF DATASETS
We ﬁrst give an overview of the datasets used in our paper, before diving into more details of the exact
training procedures (e.g., number of epochs, pretraining method, etc). The datasets we use are:"
REFERENCES,0.9107763615295481,"• DomainNet (Peng et al., 2019) is a standard domain adaptation dataset. Here, our ID dataset
contains “sketch” images (e.g., drawings of apples, elephants, etc), and the OOD dataset
contains “real”, “clipart”, and “painting” images of the same categories. We use the version
of the dataset from Tan et al. (2020)."
REFERENCES,0.9119351100811124,"• Living-17 and Entity-30 are sub-population shift datasets from the BREEDS bench-
mark (Santurkar et al., 2020). In Living-17 the goal is to classify an image as one of 17
animal categories such as “bear”—for example, the ID dataset contains images of black
bears and sloth bears and the OOD dataset has images of brown bears and polar bears. In
Entity-30 the goal is to classify an image as one of 30 entities such as “fruit” or “insect”."
REFERENCES,0.9130938586326767,"• FMoW Geo-shift is adapted from the satellite remote sensing dataset Functional Map of the
World (Christie et al., 2018; Koh et al., 2021). The goal is to classify a satellite image into one
of 62 categories such as “impoverished settlement” or “hospital”. Our ID dataset contains
images from North America, and the OOD dataset contains images from Africa and Europe."
REFERENCES,0.9142526071842411,"• CIFAR-10 →STL is a standard domain adaptation dataset (French et al., 2018), where the
ID is CIFAR-10 (Krizhevsky, 2009), and the OOD is STL (Coates et al., 2011). The task is
to classify an image into one of 10 categories such as “dog”, “cat”, or “airplane”—as usual,
we remove the “monkey” class in STL since CIFAR-10 has no “monkey” images."
REFERENCES,0.9154113557358053,"• CIFAR-10 →CIFAR-10.1 (Recht et al., 2018) is a dataset collected using a very similar
protocol to CIFAR-10, and the authors describe it as “a minute distributional shift”. The
hope is that a classiﬁer trained on CIFAR-10 gets high accuracy on CIFAR-10.1."
REFERENCES,0.9165701042873696,"• ImageNet-1K (Russakovsky et al., 2015) is a large scale dataset containing over a
million images, where the goal is to classify an image into one of 1000 categories such
as “Yorkshire terrier”, “Labrador retriever”, “acoustic guitar”, “library”, “school bus”,
etc.
We ﬁne-tune on ImageNet as the ID dataset, and evaluate on four standard OOD
datasets:
ImageNetV2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2020),
ImageNet-A (Hendrycks et al., 2019b), and ImageNet-Sketch (Wang et al., 2019)."
REFERENCES,0.9177288528389339,"B.2
DATASET AND METHOD DETAILS
We use a diverse range of datasets and pretraining strategies."
REFERENCES,0.9188876013904983,"• CIFAR-10 →STL: We ﬁne-tune or linear probe on CIFAR-10 (Krizhevsky, 2009) and test
on STL (Coates et al., 2011). This is a benchmark used in domain adaptation papers (French
et al., 2018). CIFAR-10 and STL share 9 classes, so we follow the common practice of
omitting the unshared class in STL (which is the ‘monkey’ class) when reporting accuracies.
We use a publicly available MoCo-v2 ResNet-50 checkpoint pretrained on unlabeled
examples from ImageNet-1k (Russakovsky et al., 2015), and ﬁne-tune for 20 epochs."
REFERENCES,0.9200463499420626,Published as a conference paper at ICLR 2022
REFERENCES,0.9212050984936269,"Table 3:
OOD accuracies with 90% conﬁdence intervals over 3 runs, for each of the three OOD
domains in the split of DomainNet used by Tan et al. (2020); Prabhu et al. (2021). LP does better than
FT across the board, and LP-FT does the best."
REFERENCES,0.9223638470451911,"Real
Painting
Clipart"
REFERENCES,0.9235225955967555,"Fine-tuning
55.29 (0.52)
50.26 (0.98)
60.93 (2.15)
Linear probing
87.16 (0.18)
74.50 (0.58)
77.29 (0.12)
LP-FT
86.82 (0.51)
75.91 (0.73)
79.48 (0.90)"
REFERENCES,0.9246813441483198,"• DomainNet: We use the dataset splits in Tan et al. (2020) which is also used by follow-up
work, e.g., in Prabhu et al. (2021).
This is different from the original version of the
DomainNet dataset (Peng et al., 2019), speciﬁcally Tan et al. (2020) note that some domains
and classes contain many mislabeled outliers, so they select the 40 most common classes
from the ‘sketch’, ‘real’, ‘clipart’ and ‘painting’ domains. We use the ‘sketch’ domain as ID,
and all other domains (‘real’, ‘clipart’, ‘painting’) as OOD, and in the main paper we report
the average accuracies across the OOD domains. In Table 3 we see that the same trends
hold for each of the three OOD domains. We use a CLIP (Radford et al., 2021) pretrained
ResNet-50 model, and ﬁne-tune for 50 epochs (since this is a smaller dataset)."
REFERENCES,0.9258400926998841,"• Living-17 and Entity-30: We use a publicly available MoCo-v2 ResNet-50 checkpoint
pretrained on unlabeled examples from ImageNet-1k (Russakovsky et al., 2015), and
ﬁne-tune for 20 epochs. Note that Living-17 and Entity-30 are subpopulation shifts derived
from ImageNet, but the pretraining is done on unlabeled data and does not see any OOD
labels, following the pretraining and ﬁne-tuning strategy in Cai et al. (2021). Entity-30 is a
relatively large dataset that contains around 140K training examples."
REFERENCES,0.9269988412514485,"• FMoW Geo-shift: We adapt the version of the dataset from (Koh et al., 2021). We use
training data from ‘North America’ to ﬁne-tune or linear probe, and then evaluate on
validation data from Africa and Europe. We use a MoCo-TP (Ayush et al., 2020) checkpoint,
pretrained on unlabeled FMoW satellite images. We ﬁne-tune for 50 epochs here since the
ID training dataset is smaller (around 20K examples)."
REFERENCES,0.9281575898030128,"• CIFAR-10 →CIFAR-10.1 (Recht et al., 2018): We follow the same protocols as CIFAR-10
→STL, except we test on CIFAR-10.1."
REFERENCES,0.929316338354577,"• ImageNet: we linear probe or ﬁne-tune on ImageNet (Russakovsky et al., 2015), and
evaluate on ImageNetV2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2020),
ImageNet-A (Hendrycks et al., 2019b), and ImageNet-Sketch (Wang et al., 2019). We
use a CLIP pretrained ViT-B/16 (vision transformer), the largest publicly available CLIP
model (Radford et al., 2021). We ran ﬁne-tuning for 10 epochs, linear probing for 10 epochs.
To equalize the runtime for LP-FT, we ran the linear probing stage for 5 epochs, and then the
ﬁne-tuning stage for 5 epochs. We used a batch size of 128 for all methods."
REFERENCES,0.9304750869061413,"Tuning for ImageNet experiments. We swept over three learning rates for ﬁne-tuning (0.0001,
0.0003, 0.001) and linear probing (0.01, 0.03, 0.1)—as is standard we use larger learning rates for
linear probing. For LP-FT, we swept over 3 learning rates (0.01, 0.03, 0.1) for the 5-epoch linear
probing step. We took the run that had the best ImageNet (ID) validation accuracy, and then swept
over 3 learning rates (0.00001, 0.00003, 0.0001) for the 5-epoch ﬁne-tuning step—we use a lower
learning rate for LP-FT since the experiments on the other datasets suggested that the optimal learning
rate that maximizes ID validation accuracy for LP-FT is smaller. We did not ﬁnd the comparisons to
be particularly sensitive to learning rate choice."
REFERENCES,0.9316338354577057,"Augmentations for ImageNet experiments.
We used augmentations for ﬁne-tuning, and no
augmentations for linear probing, following Kornblith et al. (2019). This might raise a question
of whether linear probing and LP-FT do better OOD because of the lack of augmentations. So as
an ablation we also tried ﬁne-tuning without augmentations, however that led to worse accuracy
(than ﬁne-tuning with augmentations) both ID and OOD. We now give details on the preprocess-
ing and augmentations that we used.
On ImageNet, for linear probing and LP-FT, we used no
augmentations—we just resized each image so that the smaller side has size 224 with bicubic
interpolation, and then center-crop to a 224-by-224 image. For ﬁne-tuning, we used augmentations:"
REFERENCES,0.93279258400927,Published as a conference paper at ICLR 2022
REFERENCES,0.9339513325608343,"Table 4: OOD accuracies with 90% conﬁdence intervals over 3 runs, when ﬁne-tuning gets to choose
learning rate and early stop, and linear probing gets to choose ℓ2 regularization weights, on OOD data.
We see that linear probing still typically does better OOD (the only ﬂip from before is on FMoW)."
REFERENCES,0.9351100811123986,"CIFAR-10.1
STL
Ent-30
Liv-17
DomNet
FMoW"
REFERENCES,0.936268829663963,"FT
92.27 (0.36)
85.97 (0.38)
64.09 (0.19)
78.63 (0.53)
59.43 (2.49)
40.23 (3.12)
LP
82.67 (0.22)
86.53 (0.01)
69.15 (0.13)
82.39 (0.14)
79.91 (0.24)
37.12 (0.01)"
REFERENCES,0.9374275782155272,"ImNetV2
ImNet-R
ImNet-Sk
ImNet-A
Average"
REFERENCES,0.9385863267670915,"FT
71.5 (-)
52.4 (-)
40.5 (-)
27.8 (-)
61.3
LP
69.7 (-)
70.9 (-)
46.4 (-)
46.1 (-)
67.1"
REFERENCES,0.9397450753186558,"Table 5: In-distribution (ID): Average distance that features move before and after ﬁne-tuning or
LP-FT, multiplied by 100 to make things easier to read. For linear probing the numbers are all 0, since
the features are not tuned. As predicted by our theory, we see that features for ID examples (this table)
move more than features for OOD examples (Table 6). Both sets of features change substantially less
for LP-FT. As usual we show 90% conﬁdence intervals over three runs."
REFERENCES,0.9409038238702202,"CIFAR-10
Entity-30
Living-17
DomainNet
FMoW"
REFERENCES,0.9420625724217845,"FT
2.23 (0.03)
3.05 (0.02)
1.88 (0.01)
207.6 (12.31)
4.87 (0.15)
LP-FT
0.07 (0.00)
0.03 (0.01)
0.11 (0.01)
0.19 (0.03)
0.57 (0.19)"
REFERENCES,0.9432213209733488,"speciﬁcally we use RandomResizedCrop in TorchVision, with the default arguments and setting the
size of the crop to 224, and then apply a random horizontal ﬂip."
REFERENCES,0.944380069524913,"Notes on pretrained model choice. We note that our results say that the pretraining has to be good
(e.g., at least get reasonable accuracy ID) for linear probing to outperform ﬁne-tuning OOD. So, for
example, we use a model pretrained on unlabeled satellite images for the satellite image dataset—if
we pretrain the model on ImageNet, we expect that ﬁne-tuning might do better.
Similarly, for
DomainNet we use a CLIP pretrained model, which is pretrained on the very large WebImageText
dataset, and sees a variety of photo and sketch like images. Pretraining on ImageNet alone does not
lead to high accuracies on DomainNet (features are not very good), so we do not necessarily expect
linear probing to outperform ﬁne-tuning with these lower quality features (for example, see the MoCo
ablation in our main paper where we used a worse pretrained model, and ﬁne-tuning did better OOD)."
REFERENCES,0.9455388180764774,"Sanity check of ﬁne-tuning implementation. As a sanity check of our implementation, ﬁne-tuning
did substantially better than training from scratch on all datasets (both ID and OOD) and matched
existing ﬁne-tuning numbers where available (e.g. ResNet50 on CIFAR-10 (Chen et al., 2020b) and
Entity-30 (Cai et al., 2021)). Fine-tuning and linear probing also both do substantially better than
training from scratch, ID and OOD, across the datasets. For example, on Living-17, training from
scratch gets 89.3% ID and 58.2% OOD (Santurkar et al., 2020) which is over 5% worse ID and nearly
20% worse OOD, than all the adaptation methods. For reference linear probing gets 96.5% ID and
82.2% OOD, and ﬁne-tuning gets 97.1% ID and 77.8% OOD. This is even though training from
scratch was run for 300 epochs, which is 15 times longer than ﬁne-tuning and LP-FT."
REFERENCES,0.9466975666280417,"B.3
TARGET EARLY STOPPING"
REFERENCES,0.947856315179606,"In the main paper, one ablation we mention is early stopping each ﬁne-tuning method and choose
the best learning rate based on target validation accuracy. As expected, ﬁne-tuning does improve a
little, but linear probing (average accuracy: 67.1%) is still better than ﬁne-tuning (average accuracy:
61.3%). Table 4 shows the full results for all datasets."
REFERENCES,0.9490150637311703,"B.4
FEATURE CHANGE"
REFERENCES,0.9501738122827347,"We examine how much the features changed for ID and OOD examples in each dataset. Speciﬁcally,
for each dataset, for each input example in the held out validation set, we computed the Euclidean
distance of the ResNet-50 features before and after ﬁne-tuning. We averaged these numbers across the
dataset, showing the results for ID validation examples in Table 5, and for OOD examples in Table 6."
REFERENCES,0.951332560834299,Published as a conference paper at ICLR 2022
REFERENCES,0.9524913093858632,"Table 6:
Out-of-distribution (OOD): Average distance that features move before and after
ﬁne-tuning or LP-FT, multiplied by 100 to make things easier to read. For linear probing the numbers
are all 0, since the features are not tuned. As predicted by our theory, we see that features for ID
examples (Table 5) move more than features for OOD examples (this table). Both sets of features
change substantially less for LP-FT. As usual we show 90% conﬁdence intervals over three runs."
REFERENCES,0.9536500579374276,"STL
Entity-30
Living-17
DomainNet
FMoW"
REFERENCES,0.9548088064889919,"FT
1.70 (0.04)
2.60 (0.02)
1.67 (0.01)
159.97 (16.23)
5.62 (0.30)
LP-FT
0.04 (0.00)
0.02 (0.00)
0.09 (0.01)
0.18 (0.02)
0.54 (0.17)"
REFERENCES,0.9559675550405562,"Table 7:
ID and OOD accuracies on Living-17 using a CLIP ResNet-50 model pretrained on the
WebImageText dataset, instead of unlabeled ImageNet examples.
Similar ﬁndings hold—here
ﬁne-tuning does similarly to linear probing ID, but does worse than linear probing OOD. LP-FT does
better than both ID, and closes 86% of the gap OOD. As usual we show 90% conﬁdence intervals
over three runs."
REFERENCES,0.9571263035921205,"ID
OOD"
REFERENCES,0.9582850521436849,"LP
94.7 (0.2)
78.6 (0.5)
FT
94.7 (0.1)
67.3 (0.8)
LP-FT
95.6 (0.2)
77.0 (0.6)"
REFERENCES,0.9594438006952491,"The feature distortion theory predicts that the features for ID examples change more than for OOD
examples. This bears out in 9 out of 10 cases, that is all cases except for FT on FMoW. To see this,
compare each cell in Table 5 with the corresponding cell in Table 6—the former is higher in 9 out of
10 cases."
REFERENCES,0.9606025492468134,"The feature distortion theory says that this large feature change is caused because the head is randomly
initialized—since the head needs to be updated by a large amount, the feature extractor is also updated
a lot because the updates are coupled. Our theory predicts that if the head is initialized via linear
probing then the feature extractor should change a lot less for both ID and OOD examples. As
predicted by the theory, across all the datasets in Table 5 and Table 6, the features change a lot less for
LP-FT than for FT. For example, on CIFAR-10, the features change 30× less for LP-FT than for FT."
REFERENCES,0.9617612977983777,"These results suggest that ﬁne-tuning underperforms OOD, and LP-FT does well ID and OOD, for
the reasons predicted by the feature distortion theory."
REFERENCES,0.9629200463499421,"B.5
ADDITIONAL ARCHITECTURES, FINE-TUNING METHODS"
REFERENCES,0.9640787949015064,"The main contributions of our paper are conceptual understanding and theory. However, to strengthen
the empirical investigation we ran two additional models (a CLIP vision transformer and CLIP
ResNet-50), as well as three additional ﬁne-tuning heuristics. We focus on the Living-17 dataset be-
cause some of these ablations require lots of compute and can take a long time to run on all the datasets."
REFERENCES,0.9652375434530707,"Architectures and pretraining source: In the main paper, we showed results when initializing with
a MoCo-v2 ResNet-50 model pretrained on unlabeled ImageNet examples. Here we examine how
the results change when we 1. Use a ResNet-50 model pretrained on CLIP’s WebImageText dataset
(Table 7), and, 2. Use a much larger vision transformer model (ViT-B/16) pretrained on CLIP’s
WebImageText dataset (Table 8)—this is the largest publicly available CLIP model at the time of
writing. We see that similar ﬁndings to our main paper hold—ﬁne-tuning does better than linear
probing ID, but does worse than linear probing (‘underperforms’) OOD. Finally, LP-FT does better
than both methods ID, and closes most (75%-90%) of the gap OOD."
REFERENCES,0.966396292004635,"These results are from early stopping on ID validation data. If we early stop on OOD validation data,
LP-FT achieves 87.9±0.4% OOD accuracy, and LP gets 88.3±0.2% OOD accuracy and here there
is no statistically signiﬁcant difference between the two. On the other hand, even if we early stop on
OOD validation data, ﬁne-tuning gets 84.4±0.5% OOD accuracy which is lower."
REFERENCES,0.9675550405561993,"Fine-tuning heuristics: Transfer learning (initializing with a pretrained model, and then adapting it
to a downstream task) is the standard way to build modern ML models, because it improves accuracy"
REFERENCES,0.9687137891077636,Published as a conference paper at ICLR 2022
REFERENCES,0.9698725376593279,"Table 8: ID and OOD accuracies on Living-17 using a CLIP ViT-B/16 (Vision Transformer) model
pretrained on the WebImageText dataset, instead of unlabeled ImageNet examples. This is the largest
publicly available CLIP model that we could ﬁnd. The same ﬁndings hold—ﬁne-tuning does better
than linear probing ID, but does worse than linear probing OOD. LP-FT does better than both ID, and
closes 75% of the gap OOD. As usual we show 90% conﬁdence intervals over three runs."
REFERENCES,0.9710312862108922,"ID
OOD"
REFERENCES,0.9721900347624566,"LP
97.5 (0.1)
87.6 (0.5)
FT
97.8 (0.0)
81.5 (2.1)
LP-FT
98.0 (0.0)
86.1 (0.1)"
REFERENCES,0.9733487833140209,"and speeds up training. Since this paradigm is so widely used, there are many heuristics people use
when training their models (as mentioned in the main paper, LP-FT has sometimes been used as a
heuristic as well, although not in the context of OOD). We showed that LP-FT is one way to do well
ID and OOD, but we hope that our theory leads to even better ﬁne-tuning algorithms."
REFERENCES,0.9745075318655851,"In this section, we compare LP-FT with additional ﬁne-tuning heuristics: using a larger learning rate
for the head layer, regularizing the features towards their original values, and side-tuning (Zhang
et al., 2020) where we freeze the features but add a side-network."
REFERENCES,0.9756662804171495,"The intuitions from our theory suggest two other potential ways to improve OOD accuracy: 1. We
could use a higher learning rate on the linear layer, so that the linear layer learns quicker and the
features do not get as distorted, and 2. We could regularize the weights of the feature extractor
towards the pretrained initialization, to prevent feature distortion. These heuristics have been used in
prior work on ﬁne-tuning as well, for example method 2 corresponds to L2-SP in (Li et al., 2018)."
REFERENCES,0.9768250289687138,"We run these two approaches on Living-17. For approach (1), we use a 10× higher learning rate
for the linear layer, and for approach (2) we regularize the Euclidean distance between the current
feature extractor weights (so ignoring the linear head) from the pretrained weights, multiplying by a
hyperparameter λ. We grid search over the same learning rates as ﬁne-tuning for both methods, and
in addition for (2) we grid search over λ∈{1.0,0.1,0.01,0.001,0.0001}, so this amounts to sweeping
over 30 hyperparameters as opposed to just 6 for ﬁne-tuning and LP-FT. For each hyperparameter
conﬁguration we run 3 replication runs with different seeds to reduce the estimation variance, and
early stop and model select using ID data just like for ﬁne-tuning and LP-FT. Just like for ﬁne-tuning
and LP-FT, we use a cosine learning rate decay and train for the same number of epochs. Indeed, we
ﬁnd that both (1) and (2) are able to close part of the OOD gap between ﬁne-tuning and linear probing.
However, LP-FT does better than both methods ID and OOD. The full results are in Table 9."
REFERENCES,0.9779837775202781,"We also compare with another method, (3) side-tuning (Zhang et al., 2020). Side-tuning freezes the
pretrained features g(x) but trains another ‘side’ model s(x), and then outputs v⊤(g(x) + h(x)),
where the head v and the parameters of the side model s are tuned. The intuition for trying this is
that side-tuning also preserves the pretrained features which likely reduces feature distortion. In
the supplementary of Zhang et al. (2020) they use a ResNet-50 for both the original model and the
side model in their vision experiments, so we do the same. We sweep over twelve learning rates
(3·10−5,1·10−4,3·10−4,...,1.0,3.0,10.0), with three replication runs with different seeds for each
learning rate. Just like for ﬁne-tuning and LP-FT, we use a cosine learning rate decay and train for
the same number of epochs, and we early stop and model select using ID validation data. We checked
that the best learning rate was not at the boundary of the grid search. On OOD, side-tuning (81.0%)
improves over ﬁne-tuning (77.7%). However, side-tuning doesn’t do as well ID. LP-FT did better
ID and OOD. This could be because side-tuning does not get to reﬁne the pretrained features for the
ID task—while the side-network is powerful enough to learn good features, it is initialized randomly
and effectively trained from scratch, so it might not be able to learn these good features on the limited
sized training dataset (around 40K examples). The results are also in Table 9."
REFERENCES,0.9791425260718424,"We also include results for training from scratch in Table 9—these results are from Santurkar et al.
(2020). Note that training from scratch was done for 450 epochs, whereas ﬁne-tuning was done for
20 epochs. As a sanity check, all the ﬁne-tuning methods and linear probing do substantially better
than training from scratch, both ID and OOD."
REFERENCES,0.9803012746234068,Published as a conference paper at ICLR 2022
REFERENCES,0.981460023174971,"Table 9:
ID and OOD accuracies on Living-17 including three additional ﬁne-tuning heuristics,
where we (1) Use a 10× larger learning rate for the head, or (2) Regularize the Euclidean distance of
the feature extractor weights to the pretrained initialization, and (3) side-tuning where we freeze the
pretrained model but add a side network that is ﬁne-tuned. As a sanity check, all methods do better
than training from scratch ID and OOD, and we show 90% conﬁdence intervals over three runs. As
per the intuitions from the feature distortion theory, these methods do mitigate feature distortion to
some extent and improve OOD accuracy over ﬁne-tuning. LP-FT does better than all methods ID and
OOD—nonetheless, we believe that LP-FT is just the ﬁrst step and hope that our theory can be used
to inspire or derive better algorithms."
REFERENCES,0.9826187717265353,"ID
OOD"
REFERENCES,0.9837775202780996,"Scratch
92.4 (1.3)
58.2 (2.4)
LP
96.5 (0.1)
82.2 (0.2)
FT
97.1 (0.1)
77.7 (0.7)
FT (10x Linear)
97.2 (0.2)
80.4 (0.3)
FT (regularized)
97.1 (0.2)
80.0 (0.4)
Side-tuning
95.5 (0.4)
81.0 (0.7)
LP-FT
97.8 (0.1)
82.6 (0.3)"
REFERENCES,0.984936268829664,"B.6
DISCUSSION OF EFFECTIVE ROBUSTNESS"
REFERENCES,0.9860950173812283,"LP-FT gets higher OOD accuracy than ﬁne-tuning, but it sometimes gets higher ID accuracy as
well. Taori et al. (2020) and Miller et al. (2021) show that OOD accuracy can often be correlated
with ID accuracy, and suggest examining the effective robustness: intuitively the extra gain in
OOD accuracy than can be predicted from improved ID accuracy alone. Is LP-FT simply better
in-distribution, or does it have higher effective robustness as well?"
REFERENCES,0.9872537659327926,"We start out by noting that linear probing clearly has higher effective robustness in most of our
datasets.
Linear probing does worse than ﬁne-tuning ID so based on the effective robustness
framework we would expect it to do worse than ﬁne-tuning OOD as well. However, linear probing
does better than ﬁne-tuning OOD and therefore has higher effective robustness."
REFERENCES,0.9884125144843569,"Figure 3: We plot the OOD accuracy against ID accuracy on Living-17 for the three methods we con-
sider, when we start from three different pretrained models (CLIP ResNet-50, CLIP ViT-B/16, MoCo-
V2 ResNet-50). The line for linear probing and LP-FT lie above ﬁne-tuning which suggests that they
have higher effective robustness. Each point is produced by averaging over three random seeds."
REFERENCES,0.9895712630359212,"The solutions found by LP-FT also appear to have higher effective robustness than ﬁne-tuning, be-
causewhentheyhavesimilarIDaccuracy, LP-FTdoesmuchbetterOOD.Forafewpiecesofevidence:"
REFERENCES,0.9907300115874855,"1. On CIFAR-10 →STL, there is no statistically signiﬁcant difference between FT and LP-FT
on ID, but LP-FT gets 8% higher accuracy OOD in Table 2."
REFERENCES,0.9918887601390498,"2. If we look at checkpoints earlier in training for CIFAR-10 →STL we can exactly equalize
ID accuracy and compare OOD accuracies. In-distribution, LP-FT and FT both get 97.2%
accuracy, but OOD, LP-FT (90.2%) is much better than FT (81.8%)."
REFERENCES,0.9930475086906141,Published as a conference paper at ICLR 2022
REFERENCES,0.9942062572421785,"3. Finally, in Figure 3 we plot the OOD accuracy against the ID accuracy for ﬁne-tuning and
LP-FT on Living-17. We plot these for three different pretrained models (CLIP ResNet-50,
CLIP ViT-B/16, MoCo-V2 ResNet-50). We see that the ID-OOD line for LP-FT is above
the line for FT indicating effective robustness."
REFERENCES,0.9953650057937428,"Note that higher effective robustness does not mean a method is better. For example, a method A can
have higher effective robustness B by doing a lot worse in-distribution even when they have the same
OOD accuracy. In this case, A is clearly inferior since it does worse ID and same OOD, but has higher
effective robustness because of its worse ID accuracy."
REFERENCES,0.996523754345307,"We believe the ﬁnding that linear probing and LP-FT has higher effective robustness than ﬁne-tuning
when the distributon shift is large is particularly interesting because Taori et al. (2020) and Miller
et al. (2021) show that it is uncommon for methods to have higher effective robustness. In our case
linear probing and LP-FT appear to consistently have higher effective robustness which suggests
that with good transfer learning methods we can get both high in-distribution accuracy and higher
effective robustness."
REFERENCES,0.9976825028968713,"C
ADDITIONAL RELATED WORK"
REFERENCES,0.9988412514484357,"Theoretical analysis of overparameterized models.
Modern deep learning presents an interesting
paradigm for theoretical analysis where the number of parameters is much larger than the number
of training points. The model class is highly expressive and several solutions obtain zero training
loss even in the presence of noise. Such overparameterized models have received a lot of interest
recently especially with a focus on understanding “benign overﬁtting” or the phenomenon where
ﬁtting noisy training data to zero loss leads to classiﬁers that generalize well. By analyzing different
linear overparameterized settings Belkin et al. (2019); Hastie et al. (2019); Bartlett et al. (2019);
Muthukumar et al. (2020); Mei & Montanari (2019); Bibas et al. (2019) study various statistical
properties such as the “double descent curve” in addition to benign overﬁtting. One important aspect
of overparameterized models is that there is no unique minimizer of the training loss. We need some
inductive bias which is typically implicit via the optimization procedure. Prior works study the
statistical properties of the explicit inductive bias of minimum norm interpolation. In contrast, we
study the effect of gradient based optimization from a particular pretrained initialization where we
effectively capture the exact implicit inductive bias of gradient based ﬁne tuning."
