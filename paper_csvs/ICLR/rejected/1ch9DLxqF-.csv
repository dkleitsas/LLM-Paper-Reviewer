Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.000980392156862745,"Recent work (Nguyen et al., 2021) has uncovered a striking phenomenon in large-
capacity neural networks: they contain blocks of contiguous hidden layers with
highly similar representations. This block structure has two seemingly contra-
dictory properties: on the one hand, its constituent layers have highly similar
dominant ﬁrst principal components (PCs), but on the other hand, their represen-
tations, and their common ﬁrst PC, are highly dissimilar across different random
seeds. Our work seeks to reconcile these discrepant properties by investigating
the origin of the block structure in relation to the data and training methods. By
analyzing properties of the dominant PCs, we ﬁnd that the block structure arises
from dominant datapoints — a small group of examples that share similar im-
age statistics (e.g. background color). However, the set of dominant datapoints,
and the precise shared image statistic, can vary across random seeds. Thus, the
block structure reﬂects meaningful dataset statistics, but is simultaneously unique
to each model. Through studying hidden layer activations and creating synthetic
datapoints, we demonstrate that these simple image statistics dominate the repre-
sentational geometry of the layers inside the block structure. We also explore how
the phenomenon evolves through training, ﬁnding that the block structure takes
shape early in training, but the underlying representations and the corresponding
dominant datapoints continue to change substantially. Finally, we study the inter-
play between the block structure and different training mechanisms, introducing
a targeted intervention to eliminate the block structure, as well as examining the
effects of pretraining and Shake-Shake regularization."
INTRODUCTION,0.00196078431372549,"1
INTRODUCTION"
INTRODUCTION,0.0029411764705882353,"Many modern successes of deep neural networks have used simple rules to systematically increase
model capacity, often through scaling architecture depth and width (Tan & Le, 2019). These large
capacity models typically also maintain strong performance even in tasks with small amounts of
training data. This has led to their widespread use across different many applications, including
data-scarce, high-stakes settings such as medical imaging (Wang et al., 2016; Liu et al., 2017)."
INTRODUCTION,0.00392156862745098,"However, recent work has shown that the representational structures of these large capacity models
exhibit distinctive properties that are not present in shallower/narrower networks. Speciﬁcally, when
using linear centered kernel alignment (CKA) (Kornblith et al., 2019) to measure similarity between
hidden representations of neural network layers, Nguyen et al. (2021) ﬁnd that a large set of con-
tiguous layers that share highly similar representations. This is visible as a clear block structure in
the heatmap of pairwise linear CKA similarities between layers (see Figure 1a). This block structure
phenomenon is robust, appearing both in models trained on natural image datasets and in models
trained on medical imaging datasets (Figure 1b)."
INTRODUCTION,0.004901960784313725,"The emergence of the block structure as model capacity increases does not merely indicate a change
in relationships among layer representations; it is also associated with changes in the properties of
individual representations. As shown in Figure 2a, layer representation inside the block structure
have dominant ﬁrst principal components (PCs) that explain the majority of variance in activations,
whereas the ﬁrst PCs of representations of layers outside the block structure, or in networks without
block structure, explain far less variance. But counterintuitively, although layer representations and"
INTRODUCTION,0.0058823529411764705,Under review as a conference paper at ICLR 2022 (a)
INTRODUCTION,0.006862745098039216,25 50 75 25 50 75 Layer
INTRODUCTION,0.00784313725490196,ResNet-26 1×
INTRODUCTION,0.008823529411764706,50 100 150
INTRODUCTION,0.00980392156862745,ResNet-44 1×
INTRODUCTION,0.010784313725490196,"150
300"
INTRODUCTION,0.011764705882352941,ResNet-110 1×
INTRODUCTION,0.012745098039215686,"250
500"
INTRODUCTION,0.013725490196078431,ResNet-164 1×
INTRODUCTION,0.014705882352941176,"50
100
Layer 50 100 Layer"
INTRODUCTION,0.01568627450980392,ResNet-38 1×
INTRODUCTION,0.016666666666666666,"50
100
Layer"
INTRODUCTION,0.01764705882352941,ResNet-38 2×
INTRODUCTION,0.018627450980392157,"50
100
Layer"
INTRODUCTION,0.0196078431372549,ResNet-38 8×
INTRODUCTION,0.020588235294117647,"50
100
Layer"
INTRODUCTION,0.021568627450980392,Deeper Wider
INTRODUCTION,0.022549019607843137,ResNet-38 10×
INTRODUCTION,0.023529411764705882,"0
1
Similarity (b)"
INTRODUCTION,0.024509803921568627,25 50 75 25 50 75 Layer
INTRODUCTION,0.025490196078431372,"CIFAR-100
ResNet-26 1×"
INTRODUCTION,0.026470588235294117,"300
600"
INTRODUCTION,0.027450980392156862,ResNet-218 1×
INTRODUCTION,0.028431372549019607,25 50 75
INTRODUCTION,0.029411764705882353,ResNet-26 10×
INTRODUCTION,0.030392156862745098,"25
50
Layer 25 50 Layer"
INTRODUCTION,0.03137254901960784,"Patch Camelyon
ResNet-20 1×"
INTRODUCTION,0.03235294117647059,"125
250
Layer"
INTRODUCTION,0.03333333333333333,ResNet-80 1×
INTRODUCTION,0.03431372549019608,"25
50
Layer"
INTRODUCTION,0.03529411764705882,ResNet-20 10×
INTRODUCTION,0.03627450980392157,"Figure 1: (a): Block structure arises in wide and deep networks. Heatmaps show similarity between layer
representations, measured with linear CKA, for models of varying widths and depths trained on CIFAR-10.
As models become wider or deeper, “blocks” of consecutive layers share similar representations. (b): Block
structure arises on many datasets. Models trained on CIFAR-100 and the medical histopathology dataset
Patch Camelyon show similar behavior to those in panel (a). (a)"
INTRODUCTION,0.03725490196078431,"10
20
30
Layer 0 1"
INTRODUCTION,0.03823529411764706,Frac. Var. Exp.
INTRODUCTION,0.0392156862745098,"10
20
30 10 20 30 Layer"
INTRODUCTION,0.04019607843137255,ResNet-38 (2×)
INTRODUCTION,0.041176470588235294,"10
20
30
Layer 0"
INTRODUCTION,0.04215686274509804,"1
10
20
30 10 20 30"
INTRODUCTION,0.043137254901960784,"ResNet-38 (10×)
(b)"
INTRODUCTION,0.04411764705882353,25 50 75 25 50 75 Layer
INTRODUCTION,0.045098039215686274,"ResNet-32 1×
Seed 0"
INTRODUCTION,0.04607843137254902,25 50 75
INTRODUCTION,0.047058823529411764,Seed 1
INTRODUCTION,0.04803921568627451,25 50 75
INTRODUCTION,0.049019607843137254,Seed 0 vs. 1
INTRODUCTION,0.05,"250
500
Layer 250 500 Layer"
INTRODUCTION,0.050980392156862744,"ResNet-164 1×
Seed 0"
INTRODUCTION,0.05196078431372549,"250
500
Layer"
INTRODUCTION,0.052941176470588235,Seed 1
INTRODUCTION,0.05392156862745098,"250
500
Layer"
INTRODUCTION,0.054901960784313725,Seed 0 vs. 1
INTRODUCTION,0.05588235294117647,"Figure 2: (a): The ﬁrst principal component of representations inside the block structure explains the
majority of the variance in representations. Top: Linear CKA similarity heatmaps for networks that do
and do not exhibit block structure. Bottom: Fraction of variance in representations explained by ﬁrst principal
component. See Appendix B for more networks. (b): Representations inside the block structure are highly
unique to each seed. 2 leftmost panels show similarity of representations within networks trained with 2
different random seeds. Rightmost panel shows similarity between the 2 networks."
INTRODUCTION,0.056862745098039215,"their dominant ﬁrst PCs are similar across the layers that make up the block structure, they are highly
dissimilar across random seeds, as shown in Figure 2b. Such representational inconsistencies have
been linked to overﬁtting and poor generalization (Morcos et al., 2018)."
INTRODUCTION,0.05784313725490196,"The prevalence of the block structure, and this discrepancy in its properties, motivate a pressing
fundamental question — is the block structure a sign of overﬁtting to idiosyncrasies of the data and
training process, or does it pick up meaningful signals? In this paper, we uncover answers to this
question, investigating the origin of the block structure in relation to the data and training methods,
and reconciling its contradictory behaviors. Speciﬁcally, our ﬁndings are as follows:"
INTRODUCTION,0.058823529411764705,"• The dominant ﬁrst principal components of the layers that make up the block structure arise from
a small number of dominant datapoints that share similar characteristics (e.g., background color).
• But the set of dominant datapoints and their common characteristics can vary across training runs,
leading to the observed block structure dissimilarities across random seeds.
• Like the block structure phenomenon, dominant datapoints can only be found in large capacity
neural networks. We show that if these dominant datapoints are excluded from the dataset used
for representational analysis, the block structure effect disappears.
• We ﬁnd that dominant datapoints strongly activate the hidden layers corresponding to the block
structure. By constructing synthetic examples based off of their shared image characteristics, we
show that these characteristics are indeed responsible for the high activation norms.
• The block structure emerges early in training, but early block structures have different representa-
tions and yield different dominant datapoints than the ﬁnal block structure at the end of training.
• We study whether it is possible to eliminate the block structure without interfering with general-
ization using a novel principal component regularization method. We also show that alternative"
INTRODUCTION,0.059803921568627454,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.060784313725490195,"training mechanisms such as transfer learning and Shake-Shake regularization can reduce the
block structure and yield more consistent representations across different training runs."
RELATED WORK,0.061764705882352944,"2
RELATED WORK"
RELATED WORK,0.06274509803921569,"Previous work has studied certain propensities of deep neural networks in the standard training set-
ting, such as their simplicity bias (Huh et al., 2021; Valle-Perez et al., 2018; Nakkiran et al., 2019),
texture bias (Baker et al., 2018; Geirhos et al., 2018; Hermann et al., 2019) and reliance on spu-
rious correlations (McCoy et al., 2019; Geirhos et al., 2020; Ribeiro et al., 2016; Jo & Bengio,
2017; Hosseini & Poovendran, 2018). Inspired by the ﬁndings of Nguyen et al. (2021) that deep
and wide networks learn many layers with very similar representations, we seek to characterize
the kind of signals these layers may overﬁt to. Additionally, our work also explores how this sig-
nal varies over the course of training instead of focusing on a single, ﬁnal model. To do so, we
analyze the behavior of the internal representations of models of varied depths and widths, using
methods for measuring similarity of neural network hidden representations (Kornblith et al., 2019;
Raghu et al., 2017; Morcos et al., 2018). Besides shedding light on model training procedures (Got-
mare et al., 2018; Neyshabur et al., 2020), features (Resnick et al., 2019; Thompson et al., 2019;
Hermann & Lampinen, 2020), and dynamics (Maheswaranathan et al., 2019), representational anal-
ysis has also furthered understanding of network internals in applications of deep learning, such as
medicine (Raghu et al., 2019) and machine translation (Bau et al., 2019)."
RELATED WORK,0.06372549019607843,"Our work is also related to previous attempts to understand the properties of overparameterized
models (Zhang et al., 2016). Most theoretical work in this area has focused on linear models, mod-
els with random features, or kernel settings (Belkin et al., 2018; Hastie et al., 2019; Liang et al.,
2020; Bartlett et al., 2020), all of which lack intermediate features, or involves networks without
nonlinearities (Advani et al., 2020). Our results suggest that the behavior of intermediate features of
practical neural networks changes dramatically with increasing overparameterization, in ways that
are not obvious from previous analysis."
EXPERIMENTAL SETUP AND BACKGROUND,0.06470588235294118,"3
EXPERIMENTAL SETUP AND BACKGROUND"
EXPERIMENTAL SETUP AND BACKGROUND,0.06568627450980392,"Measuring Representation Similarity with CKA: Centered kernel alignment (CKA) (Kornblith
et al., 2019; Cortes et al., 2012) addresses several challenges in measuring similarity between neural
network hidden representations including (i) their large size; (ii) neuronal alignment between dif-
ferent layers; and (iii) features being distributed across multiple neurons in a layer. Like Nguyen
et al. (2021), we use the minibatch implementation of linear CKA with a batch size of 256 sampled
without replacement from the test dataset, and accumulate statistics over 10 epochs to allow the
minibatch estimator to converge."
EXPERIMENTAL SETUP AND BACKGROUND,0.06666666666666667,"More concretely, given k minibatches of n examples, and two layers having p1 neurons
and p2 neurons each, minibatch CKA takes as inputs k pairs of centered activation matrices
(X1, Y1), . . . , (Xk, Yk) where Xi ∈Rn×p1 and Yi ∈Rn×p2 reﬂect the activations of these layers
to the same minibatches. It produces a scalar similarity score between 0 and 1 by averaging HSIC
scores over minibatches:"
EXPERIMENTAL SETUP AND BACKGROUND,0.06764705882352941,CKAminibatch =
"K
PK",0.06862745098039216,"1
k
Pk
i=1 HSIC1(XiXT
i , YiY T
i )
q"
"K
PK",0.0696078431372549,"1
k
Pk
i=1 HSIC1(XiXT
i , XiXT
i )
q"
"K
PK",0.07058823529411765,"1
k
Pk
i=1 HSIC1(YiY T
i , YiY T
i )
,
(1)"
"K
PK",0.07156862745098039,"where HSIC1 is the unbiased estimator from Song et al. (2012). The result is an estimator of CKA
that converges to the same value regardless of the batch size."
"K
PK",0.07254901960784314,"We compute CKA between all layer representations, including before and after batch normaliza-
tion, activations, and residual connections. In experiments that involve tracking how a model’s in-
ternal properties (principal components of activations, representation similarity, etc.) change across
epochs, we set batch normalization layers to be in training mode, to reduce the difﬁculty of adapting
to batch statistics of the test set when the model has not converged."
"K
PK",0.07352941176470588,"The Block Structure Phenomenon: Nguyen et al. (2021) use linear CKA to compute the represen-
tation similarity for all pairs of layers within the same model and visualizes the result as a heatmap
(with x and y axes indexing the layers from input to output). They ﬁnd a contiguous range of hidden
layers with very high representation similarity (yellow squares on heatmaps in Figure 1a) in very"
"K
PK",0.07450980392156863,Under review as a conference paper at ICLR 2022
"K
PK",0.07549019607843137,"deep or wide models, and call this phenomenon the block structure. The block structure arises in net-
works that are large relative to the size of the training set — although small networks do not exhibit
block structure when trained on CIFAR-10, they do exhibit block structure on smaller datasets."
"K
PK",0.07647058823529412,"Representations of the layers that comprise the block structure exhibit different representational
geometry than other layers. For layers inside the block structure, the ﬁrst principal component
explains a large fraction of the variance in representations; this is not the case for the rest of the layers
or for networks without the block structure (Nguyen et al., 2021). We replicate this observation in
Figure 2a. The substantial similarity between layers inside the block structure reﬂects a high degree
of alignment of their ﬁrst principal components, as can be seen from the following decomposition
of linear CKA for centered activation matrices X ∈Rn×p1, Y ∈Rn×p2:"
"K
PK",0.07745098039215687,"CKA(XXT, Y Y T) ="
"K
PK",0.0784313725490196,"Pp1
i=1
Pp2
j=1 λi
Xλj
Y ⟨ui
X, uj
Y ⟩2
pPp1
i=1(λi
X)2
qPp2
j=1(λj
Y )2
(2)"
"K
PK",0.07941176470588235,"where ui
X ∈Rn and ui
Y ∈Rn are the ith normalized principal components of X and Y , and λi
X and
λi
Y are the amounts of variance that these principal components explain (Kornblith et al., 2019). As
the fraction of variance explained by the ﬁrst principal component of each representation approaches
1, CKA becomes a measure of the squared cosine similarity between ﬁrst principal components
⟨u1
X, u1
Y ⟩2. Nguyen et al. (2021) conclude that the block structure preserves and propagates a
dominant ﬁrst principal component across many hidden layers."
"K
PK",0.0803921568627451,"Datasets & Models: Our setup closely follows that of Nguyen et al. (2021) and analyzes ResNets of
varying depths and widths, trained on common image classiﬁcation datasets CIFAR-10 and CIFAR-
100 (Krizhevsky et al., 2009), as well as the medical imaging dataset Patch Camelyon (Veeling et al.,
2018). These datasets are chosen to reﬂect the image statistics found in different domains, and we
observe that they all easily induce a block structure in reasonably sized ResNets."
"K
PK",0.08137254901960785,"The ResNet architecture design follows Zagoruyko & Komodakis (2016), with the layers distributed
evenly between three stages — each marked by a different feature map size — and the number of
channels is doubled after each stage. To scale the model depth and width, we increase the number
of layers and channels respectively. In experiments involving Shake-Shake regularization (Gastaldi,
2017), the network is modiﬁed to have 3 branches that are combined in a stochastic fashion. More
information on hyperparameters can be found in Appendix A."
DOMINANT DATAPOINTS AND HOW THEY SHAPE THE BLOCK STRUCTURE,0.08235294117647059,"4
DOMINANT DATAPOINTS AND HOW THEY SHAPE THE BLOCK STRUCTURE"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.08333333333333333,"4.1
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.08431372549019608,"Motivated by previous evidence that the block structure propagates a dominant principal component
across its constituent layers, we attempt to interpret the signal carried by this PC by examining the
distribution of values after projecting all test examples onto the ﬁrst PC of each layer’s activations.
We ﬁnd that the distribution is bimodal: while most values are concentrated within a certain range
(in terms of magnitude), there exist some that are orders of magnitude bigger (Figure 3)."
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.08529411764705883,"We call these datapoints with large projections dominant datapoints, and ﬁnd that they are consistent
across the range of layers making up the block structure, as seen from each column of images in
Figure 3 showing the dominant datapoints for two different layers in the same block structure. This
explains why the ﬁrst principal components of different layer activations inside the block structure
are highly similar (Figure 2a), an observation made earlier in (Nguyen et al., 2021). Moreover, this
dominant datapoint phenomenon is present only in networks that also exhibit a block structure. As
shown in Appendix Figure C.1, in networks without block structure, projections on the ﬁrst principal
component are unimodally distributed and the corresponding datapoints differ between layers."
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.08627450980392157,"Dominant datapoints are visually similar. In the left column of Figure 3, we observe that all data-
points have a blue background, although the precise shade of blue varies. However, the background
colors that the ﬁrst principal component picks up on depend on the random seed used to train the
model. The right column of Figure 3 shows the corresponding properties of an architecturally iden-
tical model trained from a different seed, where the dominant datapoints share white backgrounds
instead. Refer to Appendix C for visualizations of dominant images found in other tasks (CIFAR-
100, Patch Camelyon) and model architectures (wide ResNet). We ﬁnd that besides background"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.08725490196078431,Under review as a conference paper at ICLR 2022
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.08823529411764706,"2500
5000
7500
Seed 1 Layer 250"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.0892156862745098,"5
4
3
2
1
Projected Value (log)"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.09019607843137255,"0
100
200"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.09117647058823529,# of Examples
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.09215686274509804,"2500
5000
7500
Seed 1 Layer 500"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.09313725490196079,"5
4
3
2
1
Projected Value (log)"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.09411764705882353,"0
100
200"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.09509803921568627,# of Examples
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.09607843137254903,"2500
5000
7500
Seed 2 Layer 250"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.09705882352941177,"5
4
3
2
1
Projected Value (log)"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.09803921568627451,"0
100
200"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.09901960784313725,"2500
5000
7500
Seed 2 Layer 500"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.1,"5
4
3
2
1
Projected Value (log)"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.10098039215686275,"0
100
200"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.10196078431372549,"Figure 3: Visualization of the distribution of projected values onto the ﬁrst principal component by test
inputs. There exist a small number of datapoints that yield signiﬁcantly larger projected values than the rest
and dominate the ﬁrst principal component of the network. Here we show examples of those dominant images
for two different seeds of ResNet-164 (1×) (columns). Each seed’s dominant images share similar background
colors, but these background colors differ between seeds. Further visualization of dominant datapoints across
different layers — for instance, layers 250 and 500 in this case — show that they are consistent across layers
making up the block structure. See Appendix C for analysis of other models and tasks. 200 400 0 1 Frac."
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.10294117647058823,"Var Exp.
Seed 1"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.10392156862745099,0% Removed 200 400 0 1
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.10490196078431373,1% Removed 200 400 0 1
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.10588235294117647,10% Removed 200 400 0 1 Frac.
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.10686274509803921,"Var Exp.
Seed 2 200 400 0 1 200 400 0 1 200 400"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.10784313725490197,"250
500
Layer 0 1 Frac."
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.10882352941176471,"Var Exp.
Seed 3 200 400"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.10980392156862745,"250
500
Layer 0 1 200 400"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.11078431372549019,"250
500
Layer 0 1"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.11176470588235295,"Figure 4: Removing a small number of dominant datapoints eliminates the block structure. Plots show
the effect of removing examples with the largest projections on the ﬁrst PC of layer 300 of ResNet-164 (1 ×)
models. Columns reﬂect different numbers of examples removed; rows reﬂect models trained from different
seeds. Within each group, the top left plot shows linear CKA heatmaps, the bottom left shows the fraction of
variance explained by the ﬁrst PC, and the images reﬂect the new examples with the largest projection on the
ﬁrst PC after data removal."
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.11274509803921569,"color, the dominant datapoints can also reﬂect other simple image patterns that are prevalent in the
dataset, such as the appearance of large dark spots in histopathologic scans (Appendix Figure C.3)."
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.11372549019607843,"Finally, the block structure observed in linear CKA heatmaps arises solely from dominant datapoints.
As seen in Figure 4, when the 10% most dominant datapoints are excluded from evaluation, the block
structure is completely eliminated in all 3 training runs of ResNet-164 (1×) that we examined, and
the fraction of variance explained by the ﬁrst PCs is substantially reduced. For one seed, removing
only the 1% most dominant datapoints is already enough to achieve this effect. Thus, the block
structure is completely determined by the dominant images, and is sensitive to the frequency of the
dataset statistics that they capture. We investigate the effect of removing dominant datapoints on
CKA heatmaps computed with nonlinear kernels in Appendix Figure J.3."
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.11470588235294117,Under review as a conference paper at ICLR 2022
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.11568627450980393,"250
500
Layer 200 400 Layer 0.00 0.25 0.50 0.75 1.00 CKA"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.11666666666666667,"0
100
200
300
400
500
Layer 0 2000 4000"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.11764705882352941,Activation Norm
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.11862745098039215,"0.0
0.5
Projected Value 500 1000"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.11960784313725491,Activation Norm
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.12058823529411765,"100
200
Layer 100 200 Layer 0.00 0.25 0.50 0.75 1.00 CKA"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.12156862745098039,"0
50
100
150
200
250
Layer 0 250 500 750"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.12254901960784313,Activation Norm
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.12352941176470589,"Dominant Datapoint
Batch Median"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.12450980392156863,"0.0
0.2
Projected Value"
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.12549019607843137,on First PC 0 200 400
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.1264705882352941,Activation Norm
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.12745098039215685,CIFAR-10
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.1284313725490196,Patch Camelyon
DATAPOINTS THAT ACTIVATE THE FIRST PRINCIPAL COMPONENTS,0.12941176470588237,"Figure 5: Datapoints that dominate the ﬁrst principal components of the block structure also strongly
activate the corresponding layers. We explore the relationship between dominant datapoints and activation
norms for ResNet-164 (1×) trained on CIFAR-10 (top row) and ResNet-80 (1×) trained on Patch Camelyon
data (bottom row). For layers inside the block structure (left column), dominant datapoints (inset) produce much
larger activation norms than the median of a randomly selected minibatch (middle column). Moreover, within
these layers, the norms of the activations of different datapoints are highly correlated with the magnitudes of
their projections on the ﬁrst principal component (right column)."
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.1303921568627451,"4.2
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.13137254901960785,"Having discovered dominant datapoints by projecting onto the ﬁrst principal component of the un-
derlying representations, we next explore their implications on other properties of the internal rep-
resentations, such as through layer activations. We investigate what happens to the activations of a
dominant example as it propagates through the network, and observe that it strongly activates the
parts of the network with a visible block structure. Figure 5 shows two dominant datapoints, for a
ResNet trained on CIFAR-10 that has a preference for white background images (top row), and for
another ResNet trained on Patch Camelyon that responds to clear pink backgrounds (bottom row).
Both models contain the block structure in their internal representations, and we ﬁnd that in the
corresponding layers, the activations of the dominant datapoints are substantially bigger than the
median activations of the minibatches they are a part of."
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.1323529411764706,"4.3
CASE STUDY: IMAGE BACKGROUNDS AS A DOMINANT DATASET PROPERTY"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.13333333333333333,"As background colors appear to be a common dataset characteristic that is picked up by many large-
capacity models, we provide further evidence, through data and training manipulations, to conﬁrm
that this is a real property of the hidden representations that emerges only with overparameterization."
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.13431372549019607,"First, we attempt to illustrate the connection between speciﬁc background colors, which vary across
random seeds, and the layer activations. To approximate this data statistic, we repeat only the top
left pixel in each image across the entire dimensions of the image, obtaining solid color images.
These synthetic images indeed yield even larger activations compared to the dominant images they
are taken from, and different initializations of architecturally identical networks respond to different
synthetic images. For instance, given the ResNet-164 (1×) model that has dominant datapoints
containing a blue background (Figure 3), its hidden layers are further activated when all image
pixels are replaced with the same shade of blue, but a solid white image produces considerably
smaller activations (Figure 6). In the same ﬁgure, we observe the opposite trend for another ResNet-
164 (1×) seed, which has been shown to pick up on white backgrounds (see Figure 3). Refer to
Appendix E for similar analysis on the Patch Camelyon dataset."
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.13529411764705881,"Intervention: Color Augmentation: Given earlier insights, a natural intervention to prevent the
network from potentially picking up background color signal is adding color augmentation. This
includes randomly dropping color channels and jittering brightness, contrast, saturation, and hue of
training images (Howard, 2013; Szegedy et al., 2015). As shown in Figure 7, training with this data
augmentation reduces the block structure effect in large capacity models as expected."
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.13627450980392156,Under review as a conference paper at ICLR 2022
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.13725490196078433,"100
200
300
400
500
0 1000 2000 3000"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.13823529411764707,Activation Norm
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.1392156862745098,Seed 1
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.14019607843137255,"Solid Blue
Solid Green
Solid White
Dominant Datapoint"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.1411764705882353,"100
200
300
400
500
Layer 0 2000 4000"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.14215686274509803,Activation Norm
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.14313725490196078,Seed 2
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.14411764705882352,"Figure 6: Solid color images strongly activate intermediate layers. Rows show models with the same ar-
chitecture (ResNet-164 (1×)) that are trained from different random initializations. The ﬁrst model’s dominant
datapoints consist of images with blue backgrounds (see inset images), whereas the second network prefers
white background images. We then observe that layers of the ﬁrst model are strongly activated by solid blue
images, but not solid white images, whereas the second model shows the opposite pattern. Layers of both mod-
els are strongly activated by their respective dominant datapoints (red lines), but other solid colors (e.g., green)
do not yield strong activations in either. To improve readability of the plot, we plot only the representations at
the end of each ResNet block. See Appendix E for similar ﬁndings on Patch Camelyon models."
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.1450980392156863,"100
300"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.14607843137254903,"100
200
300 Layer"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.14705882352941177,ResNet-110 1x
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.1480392156862745,250 500 250 500
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.14901960784313725,ResNet-164 1x
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.15,50 100 50 100
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.15098039215686274,ResNet-38 8x
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.15196078431372548,50 100 50 100
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.15294117647058825,ResNet-38 10x
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.153921568627451,"100
300
Layer"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.15490196078431373,"100
200
300 Layer"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.15588235294117647,ResNet-110 1x
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.1568627450980392,"250 500
Layer 250 500"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.15784313725490196,ResNet-164 1x
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.1588235294117647,"50 100
Layer 50 100"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.15980392156862744,ResNet-38 8x
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.1607843137254902,"50 100
Layer 50 100"
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.16176470588235295,ResNet-38 10x 0.0 0.5 1.0 CKA 0.0 0.5 1.0 CKA
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.1627450980392157,Without Color Augmentation
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.16372549019607843,With Color Augmentation
DOMINANT DATAPOINTS HAVE LARGE ACTIVATION NORMS,0.16470588235294117,"Figure 7: Simply adding color augmenta-
tion helps reduce the block structure ef-
fect. Having established that the activations
and representational components of some
large-capacity models pick up on common
background colors from the inputs, we ex-
periment with color dropping and color jit-
tering during training to counter this effect.
Indeed, color augmentation minimizes the
block structure appearance in the internal
representations."
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.16568627450980392,"5
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.16666666666666666,"In the previous section, we characterize the signals the block structure propagates across its layers,
and explore their implications on other aspects of the network internals. Informed by these ﬁndings,
we next explore what happens to the block structure and the dominant images over time, from
initialization until the model converges, and how this process varies across different training runs."
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.1676470588235294,"Figure 8 shows the evolution of the internal representations of a ResNet-110 (1×) model as it is
trained for 300 epochs on CIFAR-10, and tracks how similar each checkpoint is to the ﬁnal model.
We observe that some structure in the CKA heatmap is already present by the ﬁrst epoch, and the
heatmap undergoes little qualitative change past epoch 20 (top set of plots). However, when we
inspect the corresponding dominant datapoints and compare the hidden representations between
intermediate checkpoints and the ﬁnal model (bottom rows of plots), we ﬁnd that the block structure
does not always carry the same information. Instead, the representations, and corresponding groups
of dominant datapoints, only stabilize much later in training. We observe similar behavior for other
models in Appendix F, and note that the differences in block structure representations across random
seeds already take shape near the start of training as well. Overall these ﬁndings suggest that the
uniqueness of the block structure representations in large-capacity models can be attributed to both
initialization parameters and the image minibatches the models receive throughout training."
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.16862745098039217,"To further explore the link between dominant datapoints and ﬂuctuations in network representations,
in Appendix Figure F.4, we track the magnitude of the projected value on the ﬁrst PC of a single
dominant datapoint found at the end of training, and ﬁnd that the value plummets at epochs when the
internal representation structure diverges from that of the fully trained model (i.e., epochs 0, 120 and
240). At these epochs, the dominant datapoint does not produce large activations either (bottom set"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.1696078431372549,Under review as a conference paper at ICLR 2022
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.17058823529411765,"100
300"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.1715686274509804,"100
200
300 Layer Ep. 1"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.17254901960784313,"100
300 Ep. 3"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.17352941176470588,"100
300 Ep. 5"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.17450980392156862,"100
300 Ep. 7"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.17549019607843136,"100
300 Ep. 9"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.17647058823529413,"100
300"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.17745098039215687,Ep. 20
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.1784313725490196,"100
300"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.17941176470588235,Ep. 50
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.1803921568627451,"100
300"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.18137254901960784,Ep. 100
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.18235294117647058,"100
300"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.18333333333333332,Ep. 200
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.1843137254901961,"100
300"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.18529411764705883,Ep. 300
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.18627450980392157,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0 CKA"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.18725490196078431,"100
300
Layer"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.18823529411764706,"100
200
300 Layer Ep. 1"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.1892156862745098,"100
300
Layer Ep. 3"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.19019607843137254,"100
300
Layer Ep. 5"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.19117647058823528,"100
300
Layer Ep. 7"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.19215686274509805,"100
300
Layer Ep. 9"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.1931372549019608,"100
300
Layer"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.19411764705882353,Ep. 20
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.19509803921568628,"100
300
Layer"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.19607843137254902,Ep. 50
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.19705882352941176,"100
300
Layer"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.1980392156862745,Ep. 100
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.19901960784313724,"100
300
Layer"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.2,Ep. 200
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.20098039215686275,"100
300
Layer"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.2019607843137255,Ep. 300
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.20294117647058824,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0 CKA"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.20392156862745098,"Epoch 1
Epoch 9
Epoch 50
Epoch 300"
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.20490196078431372,Within-Checkpoint Similarity
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.20588235294117646,Similarity with Final Checkpoint
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.2068627450980392,Dominant Datapoints
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.20784313725490197,"Figure 8: Block structure phenomenon arises early during training, but the corresponding dominant
datapoints continue to change substantially. We compute the CKA between all pairs of layers within a
ResNet-110 (1×) model at different stages of training, and ﬁnd that the shape of the block structure is deﬁned
early in training (top row). However, comparing these different model checkpoints to the fully-trained model
reveals that the block structure representations at different epochs are considerably dissimilar to the ﬁnal rep-
resentations, especially during the ﬁrst half of the training process (middle row). The dominant datapoints also
vary signiﬁcantly over the course of training, even after the block structure is clearly visible in the heatmaps
(bottom row). See Appendix F for similar plots with greater granularity, different seeds and architectures."
EVOLUTION OF THE BLOCK STRUCTURE DURING TRAINING,0.2088235294117647,"of plots). This illustrates the signiﬁcance of the observed variations in dominant datapoints through
the training of large-capacity models."
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.20980392156862746,"6
BLOCK STRUCTURE AND TRAINING MECHANISMS"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.2107843137254902,"Having observed how the internal representation structures — more speciﬁcally the dominant PCs
of the layer representations — could vary signiﬁcantly over the course of training, we turn to ex-
amining the interplay between the block structure and the training mechanism. Even though the
block structure arises naturally with standard training, previous work has suggested that the block
structure may be an indication of redundant modules in the corresponding networks (Nguyen et al.,
2021). Thus, it is natural to ask whether it is possible to train large-capacity models that do not have
a block structure, and how such models perform compared to those with block structures."
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.21176470588235294,"Since the block structure reﬂects the similarity of a dominant ﬁrst principal component, propagated
across a wide range of hidden layers (see Section 3), we study whether regularizing the ﬁrst principal
components of layer activations would eliminate the block structure. More speciﬁcally, we estimate
the fraction of variance explained by the ﬁrst principal component of each layer using power iter-
ation and penalize it in the loss when it exceeds 20%. We provide full implementation details in
Appendix G. The resulting heatmap, in Figure 9 top right, shows that not only does this eliminate
the block structure from the internal representations, but surprisingly there is also no detrimental
effect on performance (Appendix Table G.1). We even observe small accuracy improvements on
CIFAR-100 and in the low-data regime, as shown in Appendix Table G.1."
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.21274509803921568,"Other standard training practices that are commonly used to boost performance are also effective
at reducing or eliminating the block structure effect. Shake-Shake regularization (Gastaldi, 2017)
eliminates the block structure for all of the network sizes that we examine (Figure 9, bottom left),
whereas transfer learning (Figure 9, bottom right) and training with smaller batch sizes (Appendix H)
appear to reduce the appearance of the block structure, although blocks are still discernible in the
largest models that we trained. In addition to regularizing the block structure, these training methods
also produce more similar representations across different training runs of the same architecture
conﬁguration (Appendix Figure I.1)."
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.21372549019607842,"Overall, our ﬁndings suggest that it is possible to obtain good generalization accuracy in networks
with and without block structure. We observe that learning processes that reduce the dominance of
the ﬁrst PC of the representations provide slightly higher accuracies than standard training. However,"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.21470588235294116,Under review as a conference paper at ICLR 2022
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.21568627450980393,"100
300
Layer 100 200 300 Layer"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.21666666666666667,ResNet-110 1x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.21764705882352942,"100
300
Layer 100 200 300 Layer"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.21862745098039216,ResNet-110 1x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.2196078431372549,"400
800
Layer 250 500 750 Layer"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.22058823529411764,ResNet-110 1x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.22156862745098038,"100
300
Layer 100 200 300 Layer"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.22254901960784312,ResNet-110 1x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.2235294117647059,"250
500
Layer 200 400"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.22450980392156863,ResNet-164 1x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.22549019607843138,"250
500
Layer 200 400"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.22647058823529412,ResNet-164 1x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.22745098039215686,"500
1000
Layer 500 1000"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.2284313725490196,ResNet-164 1x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.22941176470588234,"250
500
Layer 200 400"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.23039215686274508,ResNet-164 1x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.23137254901960785,"50
100
Layer 50 100"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.2323529411764706,ResNet-38 10x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.23333333333333334,"50
100
Layer 50 100"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.23431372549019608,ResNet-38 10x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.23529411764705882,100 200 300 Layer 100 200 300
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.23627450980392156,ResNet-38 10x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.2372549019607843,"50
100
Layer 50 100"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.23823529411764705,Standard Training
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.23921568627450981,"Transfer Learning
Shake-Shake Regularization"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.24019607843137256,Principal Component Regularization
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.2411764705882353,ResNet-38 10x
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.24215686274509804,"0
1
CKA"
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.24313725490196078,"Figure 9: Training with principal component regularization, transfer learning and Shake-Shake regu-
larization helps to eliminate the block structure. We directly regularize the ﬁrst PC of each layer activations
given that this component explains a large fraction of variance in block structure representations, and ﬁnd that
this eliminates the block structure. Full algorithm details can be found in Appendix G. Shake-Shake regular-
ization (Gastaldi, 2017) has a similar effect. We also ﬁnd that transfer learning reduces the appearance of the
block structure, although it is still present in the largest network we trained. These results demonstrate that the
block structure phenomenon is dependent on the training mechanism (see Appendix I for implications of these
training methods on representations across random seeds)."
BLOCK STRUCTURE AND TRAINING MECHANISMS,0.24411764705882352,"some caution is warranted in interpreting these performance beneﬁts: it may be difﬁcult to causally
determine the connection between the block structure and performance, as any training intervention
targeting the block structure may have other distinct ramiﬁcations that also affect performance."
DISCUSSION,0.24509803921568626,"7
DISCUSSION"
DISCUSSION,0.246078431372549,"Scope and Limitations: Our work primarily focuses on the behavior of large-capacity networks
trained on relatively small datasets. This is motivated by domains such as medical imaging where
data is expensive relative to the cost of training a large model, and the high-stakes nature makes it
important to understand the model’s behavior. Additional future exploration is necessary to study
state-of-the-art settings in e.g. NLP, which use much bigger and heterogeneous datasets."
DISCUSSION,0.24705882352941178,"Conclusion: The block structure phenomenon uncovered in previous work (Nguyen et al., 2021)
reveals signiﬁcant differences in the representational structures of overparameterized neural net-
works and shallower/narrower ones. However, it also exhibits some contradicting behaviors —
being unique to each network while propagating a dominant PC across a wide range of layers —
that suggest the underlying representations could either overﬁt to noise artifacts or capture rele-
vant signals in the data. Our work seeks to provide an explanation for this discrepancy. We ﬁnd
that despite the inconsistency of the block structure across different training runs, it arises not from
noise, but real and simple dataset statistics such as background color. We further discover a small
set of dominant datapoints (with large activation norms) that are responsible for the block structure.
These datapoints emerge early in training and vary across epochs, as well as across random seeds.
We show how different training procedures, including color augmentation, transfer learning, Shake-
Shake regularization, and a novel principal component regularizer, can reduce the inﬂuence of these
dominant datapoints, eliminating the block structure and leading to more consistent representations
across training runs. This work motivates interesting open questions such as exploring how dom-
inant datapoints are manifested in other domains and applications of deep learning, and applying
principal component regularization to distribution shift and self-supervision problems."
REPRODUCIBILITY STATEMENT,0.24803921568627452,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.24901960784313726,"Our experiments are performed on standard datasets and architectures that are publicly available.
We provide information regarding training hyperparameters in Appendix A and implementation
details for the principal component regularizer in Appendix G. We plan to release source code for
reproducing our results upon acceptance of the paper."
REPRODUCIBILITY STATEMENT,0.25,Under review as a conference paper at ICLR 2022
REFERENCES,0.25098039215686274,REFERENCES
REFERENCES,0.2519607843137255,"Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of gener-
alization error in neural networks. Neural Networks, 132:428–446, 2020."
REFERENCES,0.2529411764705882,"Nicholas Baker, Hongjing Lu, Gennady Erlikhman, and Philip J Kellman.
Deep convolutional
networks do not classify based on global object shape. PLoS computational biology, 14(12):
e1006613, 2018."
REFERENCES,0.25392156862745097,"Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overﬁtting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020."
REFERENCES,0.2549019607843137,"Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass.
Identifying and controlling important neurons in neural machine translation. In International
Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=H1z-PsR5KX."
REFERENCES,0.25588235294117645,"Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-
stand kernel learning. In International Conference on Machine Learning, pp. 541–549. PMLR,
2018."
REFERENCES,0.2568627450980392,"Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based
on centered alignment. The Journal of Machine Learning Research, 13(1):795–828, 2012."
REFERENCES,0.257843137254902,"Xavier Gastaldi. Shake-shake regularization. arXiv preprint arXiv:1705.07485, 2017."
REFERENCES,0.25882352941176473,"Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias im-
proves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018."
REFERENCES,0.25980392156862747,"Robert Geirhos, J¨orn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,
Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature
Machine Intelligence, 2(11):665–673, 2020."
REFERENCES,0.2607843137254902,"Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher.
A closer look
at deep learning heuristics: Learning rate restarts, warmup and distillation.
arXiv preprint
arXiv:1810.13243, 2018."
REFERENCES,0.26176470588235295,"Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.
Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019."
REFERENCES,0.2627450980392157,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.26372549019607844,"Katherine L Hermann and Andrew K Lampinen. What shapes feature representations? exploring
datasets, architectures, and training. arXiv preprint arXiv:2006.12433, 2020."
REFERENCES,0.2647058823529412,"Katherine L Hermann, Ting Chen, and Simon Kornblith. The origins and prevalence of texture bias
in convolutional neural networks. arXiv preprint arXiv:1911.09071, 2019."
REFERENCES,0.2656862745098039,"Hossein Hosseini and Radha Poovendran. Semantic adversarial examples. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1614–1619, 2018."
REFERENCES,0.26666666666666666,"Andrew G Howard. Some improvements on deep convolutional neural network based image classi-
ﬁcation. arXiv preprint arXiv:1312.5402, 2013."
REFERENCES,0.2676470588235294,"Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola.
The low-rank simplicity bias in deep networks. arXiv preprint arXiv:2103.10427, 2021."
REFERENCES,0.26862745098039215,"Jason Jo and Yoshua Bengio. Measuring the tendency of CNNs to learn surface statistical regulari-
ties. arXiv preprint arXiv:1711.11561, 2017."
REFERENCES,0.2696078431372549,"Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In ICML, 2019."
REFERENCES,0.27058823529411763,Under review as a conference paper at ICLR 2022
REFERENCES,0.27156862745098037,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.2725490196078431,"Tengyuan Liang, Alexander Rakhlin, et al. Just interpolate: Kernel “ridgeless” regression can gen-
eralize. Annals of Statistics, 48(3):1329–1347, 2020."
REFERENCES,0.2735294117647059,"Yun Liu, Krishna Gadepalli, Mohammad Norouzi, George E Dahl, Timo Kohlberger, Aleksey
Boyko, Subhashini Venugopalan, Aleksei Timofeev, Philip Q Nelson, Greg S Corrado, et al.
Detecting cancer metastases on gigapixel pathology images. arXiv preprint arXiv:1703.02442,
2017."
REFERENCES,0.27450980392156865,"Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, and David Sussillo. Uni-
versality and individuality in neural dynamics across large populations of recurrent networks. In
Advances in neural information processing systems, pp. 15629–15641, 2019."
REFERENCES,0.2754901960784314,"R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019."
REFERENCES,0.27647058823529413,"Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=B1QRgziT-."
REFERENCES,0.2774509803921569,"Ari S Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural
networks with canonical correlation. arXiv preprint arXiv:1806.05759, 2018."
REFERENCES,0.2784313725490196,"Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L Edelman, Fred
Zhang, and Boaz Barak. Sgd on neural networks learns functions of increasing complexity. arXiv
preprint arXiv:1905.11604, 2019."
REFERENCES,0.27941176470588236,"Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learn-
ing? arXiv preprint arXiv:2008.11687, 2020."
REFERENCES,0.2803921568627451,"Thao Nguyen, Maithra Raghu, and Simon Kornblith. Do wide and deep networks learn the same
things? uncovering how neural network representations vary with width and depth. In Interna-
tional Conference on Learning Representations, 2021."
REFERENCES,0.28137254901960784,"Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector
canonical correlation analysis for deep learning dynamics and interpretability. In Advances in
Neural Information Processing Systems, pp. 6076–6085, 2017."
REFERENCES,0.2823529411764706,"Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understanding
transfer learning for medical imaging. In Advances in neural information processing systems, pp.
3347–3357, 2019."
REFERENCES,0.2833333333333333,"Cinjon Resnick, Zeping Zhan, and Joan Bruna. Probing the state of the art: A critical look at visual
representation evaluation. arXiv preprint arXiv:1912.00215, 2019."
REFERENCES,0.28431372549019607,"Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”Why should i trust you?” explaining the
predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135–1144, 2016."
REFERENCES,0.2852941176470588,"Le Song, Alex Smola, Arthur Gretton, Justin Bedo, and Karsten Borgwardt. Feature selection via
dependence maximization. The Journal of Machine Learning Research, 13(1):1393–1434, 2012."
REFERENCES,0.28627450980392155,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015."
REFERENCES,0.2872549019607843,"Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural net-
works. In International Conference on Machine Learning, pp. 6105–6114, 2019."
REFERENCES,0.28823529411764703,"Jessica AF Thompson, Yoshua Bengio, and Marc Schoenwiesner. The effect of task and training on
intermediate representations in convolutional neural networks revealed with modiﬁed rv similarity
analysis. arXiv preprint arXiv:1912.02260, 2019."
REFERENCES,0.28921568627450983,Under review as a conference paper at ICLR 2022
REFERENCES,0.2901960784313726,"Guillermo Valle-Perez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the
parameter-function map is biased towards simple functions. arXiv preprint arXiv:1805.08522,
2018."
REFERENCES,0.2911764705882353,"Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv-
ariant cnns for digital pathology. In Alejandro F. Frangi, Julia A. Schnabel, Christos Davatzikos,
Carlos Alberola-L´opez, and Gabor Fichtinger (eds.), Medical Image Computing and Computer
Assisted Intervention – MICCAI 2018, pp. 210–218, Cham, 2018. Springer International Publish-
ing. ISBN 978-3-030-00934-2."
REFERENCES,0.29215686274509806,"Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, and Andrew H Beck.
Deep
learning for identifying metastatic breast cancer. arXiv preprint arXiv:1606.05718, 2016."
REFERENCES,0.2931372549019608,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
In British Machine Vision
Conference (BMVC), pp. 87.1–87.12, September 2016."
REFERENCES,0.29411764705882354,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016."
REFERENCES,0.2950980392156863,Under review as a conference paper at ICLR 2022
REFERENCES,0.296078431372549,Appendix
REFERENCES,0.29705882352941176,"A
TRAINING DETAILS"
REFERENCES,0.2980392156862745,"For wide ResNets, we look at models with depths of 14, 20, 26, and 38, and width multipliers of 1, 2,
4, 8 and 10. For deep ResNets, we experiment with depths 32, 44, 56, 110 and 164. In CIFAR-100
experiments, the block structure only appears at a greater depth so we also include depths 218 and
224 in our investigation. For Patch Camelyon datasets, we ﬁnd that depth 80 is enough to induce
a block structure in the internal representations. All ResNets follow the architecture design in (He
et al., 2016; Zagoruyko & Komodakis, 2016)."
REFERENCES,0.29901960784313725,"Unless otherwise speciﬁed, we train all the models using SGD with momentum of 0.9 for 300
epochs, together with a cosine decay learning rate schedule and batch size of 128. Learning rate is
tuned with values [0.005, 0.01, 0.001] and L2 regularization strength with values [0.001, 0.005]. For
CIFAR-10 and CIFAR-100 experiments, we apply standard CIFAR-10 data augmentation consisting
of random ﬂips and translations of up to 4 pixels. With Patch Camelyon, we use random crops of
size 32x32, together with random ﬂips, to obtain the training data. At test time, the networks are
evaluated on central crops of the original images. For CKA analysis, each architecture is trained
with 10 different seeds and evaluated on the full test set of the corresponding domain."
REFERENCES,0.3,Under review as a conference paper at ICLR 2022
REFERENCES,0.30098039215686273,"B
BLOCK STRUCTURE AND THE FIRST PRINCIPAL COMPONENT"
REFERENCES,0.30196078431372547,"10
20
30 10 20 30 Layer"
REFERENCES,0.3029411764705882,Cos. similarity of first PCs
REFERENCES,0.30392156862745096,"10
20
30
0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.30490196078431375,Fraction of variance
REFERENCES,0.3058823529411765,Variance explained
REFERENCES,0.30686274509803924,by first PC
REFERENCES,0.307843137254902,"10
20
30 10 20 30 Layer CKA"
REFERENCES,0.3088235294117647,"10
20
30
Layer 10 20 30 Layer"
REFERENCES,0.30980392156862746,ResNet-38 (10×)
REFERENCES,0.3107843137254902,CKA without first PC
REFERENCES,0.31176470588235294,"50
100 50 100"
REFERENCES,0.3127450980392157,Cos. similarity of first PCs
REFERENCES,0.3137254901960784,"50
100
0.00 0.25 0.50 0.75"
VARIANCE EXPLAINED,0.31470588235294117,"1.00
Variance explained"
VARIANCE EXPLAINED,0.3156862745098039,by first PC
VARIANCE EXPLAINED,0.31666666666666665,"50
100 50 100 CKA"
VARIANCE EXPLAINED,0.3176470588235294,"50
100
Layer 50 100"
VARIANCE EXPLAINED,0.31862745098039214,ResNet-110 (1×)
VARIANCE EXPLAINED,0.3196078431372549,"CKA without first PC 20
40 20 40"
VARIANCE EXPLAINED,0.3205882352941177,Cos. similarity of first PCs
VARIANCE EXPLAINED,0.3215686274509804,"20
40
0.00 0.25 0.50 0.75"
VARIANCE EXPLAINED,0.32254901960784316,"1.00
Variance explained"
VARIANCE EXPLAINED,0.3235294117647059,"by first PC 20
40 20 40 CKA"
VARIANCE EXPLAINED,0.32450980392156864,"20
40
Layer 20 40"
VARIANCE EXPLAINED,0.3254901960784314,ResNet-44 (1×)
VARIANCE EXPLAINED,0.3264705882352941,CKA without first PC
VARIANCE EXPLAINED,0.32745098039215687,"10
20
30 10 20 30"
VARIANCE EXPLAINED,0.3284313725490196,Cos. similarity of first PCs
VARIANCE EXPLAINED,0.32941176470588235,"10
20
30
0.00 0.25 0.50 0.75"
VARIANCE EXPLAINED,0.3303921568627451,"1.00
Variance explained"
VARIANCE EXPLAINED,0.33137254901960783,by first PC
VARIANCE EXPLAINED,0.3323529411764706,"10
20
30 10 20 30 CKA"
VARIANCE EXPLAINED,0.3333333333333333,"10
20
30
Layer 10 20 30"
VARIANCE EXPLAINED,0.33431372549019606,ResNet-38 (2×)
VARIANCE EXPLAINED,0.3352941176470588,CKA without first PC
VARIANCE EXPLAINED,0.3362745098039216,"Figure B.1: The relationship between block structure and the ﬁrst principal component. Each column
represents a different architecture. In ResNet-110 (1×) and ResNet-38 (10×), we observe a block structure in
the CKA plot (top row), and ﬁnd that the ﬁrst principal component explains a large fraction of the variance in the
layers that comprise the block structure (second row). We also observe that the cosine similarity of the ﬁrst PCs
(third row) resembles the CKA plot, and removing the ﬁrst PC before computing CKA substantially attenuates
the block structure (bottom row). By contrast, in ResNet-44 (1×) and ResNet-38 (2×), which have no block
structure, the ﬁrst PC explains only a small fraction of the variance, and the CKA plot does not resemble the
cosine similarity between the ﬁrst PCs, but instead resembles CKA computed without the ﬁrst PCs."
VARIANCE EXPLAINED,0.33725490196078434,Under review as a conference paper at ICLR 2022
VARIANCE EXPLAINED,0.3382352941176471,"C
ADDITIONAL VISUALIZATIONS OF DOMINANT DATAPOINTS"
VARIANCE EXPLAINED,0.3392156862745098,"50
100
Layer 20 40 60 80 100 Layer"
VARIANCE EXPLAINED,0.34019607843137256,ResNet-32 1× 0.0 0.2 0.4 0.6 0.8 1.0
VARIANCE EXPLAINED,0.3411764705882353,Similarity
VARIANCE EXPLAINED,0.34215686274509804,"2500
5000
7500
Layer 25"
VARIANCE EXPLAINED,0.3431372549019608,"5
4
3
2
1
Projected Value (log)"
VARIANCE EXPLAINED,0.34411764705882353,"0
100
200"
VARIANCE EXPLAINED,0.34509803921568627,# of Examples
VARIANCE EXPLAINED,0.346078431372549,"2500
5000
7500
Layer 50"
VARIANCE EXPLAINED,0.34705882352941175,"5
4
3
2
1
Projected Value (log)"
VARIANCE EXPLAINED,0.3480392156862745,"0
100
200"
VARIANCE EXPLAINED,0.34901960784313724,# of Examples
VARIANCE EXPLAINED,0.35,"2500
5000
7500
Layer 75"
VARIANCE EXPLAINED,0.3509803921568627,"5
4
3
2
1
Projected Value (log)"
VARIANCE EXPLAINED,0.3519607843137255,"0
100
200"
VARIANCE EXPLAINED,0.35294117647058826,# of Examples
VARIANCE EXPLAINED,0.353921568627451,"2500
5000
7500
Layer 100"
VARIANCE EXPLAINED,0.35490196078431374,"5
4
3
2
1
Projected Value (log)"
VARIANCE EXPLAINED,0.3558823529411765,"0
100
200"
VARIANCE EXPLAINED,0.3568627450980392,# of Examples
VARIANCE EXPLAINED,0.35784313725490197,"50
100
Layer 25 50 75 100 125 Layer"
VARIANCE EXPLAINED,0.3588235294117647,ResNet-38 1× 0.0 0.2 0.4 0.6 0.8 1.0
VARIANCE EXPLAINED,0.35980392156862745,Similarity
VARIANCE EXPLAINED,0.3607843137254902,"2500
5000
7500
Layer 25"
VARIANCE EXPLAINED,0.36176470588235293,"5
4
3
2
1
Projected Value (log)"
VARIANCE EXPLAINED,0.3627450980392157,"0
100
200"
VARIANCE EXPLAINED,0.3637254901960784,# of Examples
VARIANCE EXPLAINED,0.36470588235294116,"2500
5000
7500
Layer 50"
VARIANCE EXPLAINED,0.3656862745098039,"5
4
3
2
1
Projected Value (log)"
VARIANCE EXPLAINED,0.36666666666666664,"0
100
200"
VARIANCE EXPLAINED,0.36764705882352944,# of Examples
VARIANCE EXPLAINED,0.3686274509803922,"2500
5000
7500
Layer 75"
VARIANCE EXPLAINED,0.3696078431372549,"5
4
3
2
1
Projected Value (log)"
VARIANCE EXPLAINED,0.37058823529411766,"0
100
200"
VARIANCE EXPLAINED,0.3715686274509804,# of Examples
VARIANCE EXPLAINED,0.37254901960784315,"2500
5000
7500
Layer 100"
VARIANCE EXPLAINED,0.3735294117647059,"5
4
3
2
1
Projected Value (log)"
VARIANCE EXPLAINED,0.37450980392156863,"0
100
200"
VARIANCE EXPLAINED,0.37549019607843137,# of Examples
VARIANCE EXPLAINED,0.3764705882352941,"Figure C.1: Dominant datapoints are not present in networks without block structure. The topmost row
shows representational similarity heatmaps from two networks without block structure. The rows below show
histograms of the projected values on the ﬁrst PC, as well as images with the largest projections. Note that
the distributions of projected values are unimodal, unlike the bimodal distributions observed in networks with
a block structure (Figure 3). In addition, the datapoints with the highest projected values are highly dissimilar
between layers."
VARIANCE EXPLAINED,0.37745098039215685,Under review as a conference paper at ICLR 2022
VARIANCE EXPLAINED,0.3784313725490196,"2500
5000
7500
Layer 400"
VARIANCE EXPLAINED,0.37941176470588234,"5
4
3
2
1
0
Projected Value (log)"
VARIANCE EXPLAINED,0.3803921568627451,"0
200
400"
VARIANCE EXPLAINED,0.3813725490196078,# of Examples
VARIANCE EXPLAINED,0.38235294117647056,"2500
5000
7500
Layer 450"
VARIANCE EXPLAINED,0.38333333333333336,"5
4
3
2
1
0
Projected Value (log)"
VARIANCE EXPLAINED,0.3843137254901961,"0
200
400"
VARIANCE EXPLAINED,0.38529411764705884,"Figure C.2: Visualization of the distribution of projected values onto the ﬁrst principal component by
test inputs, for ResNet-224 (1×) trained on CIFAR-100. Top row shows histograms of the projected values
on the ﬁrst PC. Bottom rows show images with the largest projections on the ﬁrst PC. See Figure 3 for a similar
plot for ResNet-164 (1×) trained on CIFAR-10."
VARIANCE EXPLAINED,0.3862745098039216,"2500
5000
7500
Layer 200"
VARIANCE EXPLAINED,0.3872549019607843,"5
4
3
2
1
0
Projected Value (log) 0 200"
VARIANCE EXPLAINED,0.38823529411764707,# of Examples
VARIANCE EXPLAINED,0.3892156862745098,"2500
5000
7500
Layer 250"
VARIANCE EXPLAINED,0.39019607843137255,"5
4
3
2
1
0
Projected Value (log) 0 200"
VARIANCE EXPLAINED,0.3911764705882353,"Figure C.3: Visualization of the distribution of projected values onto the ﬁrst principal component by
test inputs, for ResNet-80 (1×) trained on Patch Camelyon. Top row shows histograms of the projected
values on the ﬁrst PC. Bottom rows show images with the largest projections on the ﬁrst PC. See Figure 3 for a
similar plot for ResNet-164 (1×) trained on CIFAR-10."
VARIANCE EXPLAINED,0.39215686274509803,"2500
5000
7500
Layer 45"
VARIANCE EXPLAINED,0.3931372549019608,"5
4
3
2
1
0
Projected Value (log)"
VARIANCE EXPLAINED,0.3941176470588235,"0
200
400"
VARIANCE EXPLAINED,0.39509803921568626,# of Examples
VARIANCE EXPLAINED,0.396078431372549,"2500
5000
7500
Layer 60"
VARIANCE EXPLAINED,0.39705882352941174,"5
4
3
2
1
0
Projected Value (log)"
VARIANCE EXPLAINED,0.3980392156862745,"0
200
400"
VARIANCE EXPLAINED,0.3990196078431373,"Figure C.4: Visualization of the distribution of projected values onto the ﬁrst principal component by
test inputs, for ResNet-26 (8×) trained on Patch Camelyon. Top row shows histograms of the projected
values on the ﬁrst PC. Bottom rows show images with the largest projections on the ﬁrst PC. See Figure 3 for a
similar plot for ResNet-164 (1×) trained on CIFAR-10."
VARIANCE EXPLAINED,0.4,Under review as a conference paper at ICLR 2022
VARIANCE EXPLAINED,0.40098039215686276,"D
BLOCK STRUCTURE ON OUT-OF-DISTRIBUTION DATA"
VARIANCE EXPLAINED,0.4019607843137255,255075
VARIANCE EXPLAINED,0.40294117647058825,"25
50
75 Layer"
VARIANCE EXPLAINED,0.403921568627451,RN-26 1×
VARIANCE EXPLAINED,0.40490196078431373,"50
150"
VARIANCE EXPLAINED,0.40588235294117647,RN-44 1×
VARIANCE EXPLAINED,0.4068627450980392,"50
150"
VARIANCE EXPLAINED,0.40784313725490196,RN-56 1×
VARIANCE EXPLAINED,0.4088235294117647,100 300
VARIANCE EXPLAINED,0.40980392156862744,RN-110 1×
VARIANCE EXPLAINED,0.4107843137254902,250 500
VARIANCE EXPLAINED,0.4117647058823529,RN-164 1×
VARIANCE EXPLAINED,0.41274509803921566,50 100 50 100 Layer
VARIANCE EXPLAINED,0.4137254901960784,RN-38 1×
VARIANCE EXPLAINED,0.4147058823529412,50 100
VARIANCE EXPLAINED,0.41568627450980394,RN-38 2×
VARIANCE EXPLAINED,0.4166666666666667,"50 100
Layer"
VARIANCE EXPLAINED,0.4176470588235294,RN-38 4×
VARIANCE EXPLAINED,0.41862745098039217,50 100
VARIANCE EXPLAINED,0.4196078431372549,RN-38 8×
VARIANCE EXPLAINED,0.42058823529411765,50 100
VARIANCE EXPLAINED,0.4215686274509804,CIFAR-10
VARIANCE EXPLAINED,0.42254901960784313,RN-38 10×
VARIANCE EXPLAINED,0.4235294117647059,255075
VARIANCE EXPLAINED,0.4245098039215686,"25
50
75 Layer"
VARIANCE EXPLAINED,0.42549019607843136,RN-26 1×
VARIANCE EXPLAINED,0.4264705882352941,"50
150"
VARIANCE EXPLAINED,0.42745098039215684,RN-44 1×
VARIANCE EXPLAINED,0.4284313725490196,"50
150"
VARIANCE EXPLAINED,0.4294117647058823,RN-56 1×
VARIANCE EXPLAINED,0.4303921568627451,100 300
VARIANCE EXPLAINED,0.43137254901960786,RN-110 1×
VARIANCE EXPLAINED,0.4323529411764706,250 500
VARIANCE EXPLAINED,0.43333333333333335,RN-164 1×
VARIANCE EXPLAINED,0.4343137254901961,50 100 50 100 Layer
VARIANCE EXPLAINED,0.43529411764705883,RN-38 1×
VARIANCE EXPLAINED,0.4362745098039216,50 100
VARIANCE EXPLAINED,0.4372549019607843,RN-38 2×
VARIANCE EXPLAINED,0.43823529411764706,"50 100
Layer"
VARIANCE EXPLAINED,0.4392156862745098,RN-38 4×
VARIANCE EXPLAINED,0.44019607843137254,50 100
VARIANCE EXPLAINED,0.4411764705882353,RN-38 8×
VARIANCE EXPLAINED,0.442156862745098,50 100
VARIANCE EXPLAINED,0.44313725490196076,CIFAR-10 Corrupted
VARIANCE EXPLAINED,0.4441176470588235,RN-38 10×
VARIANCE EXPLAINED,0.44509803921568625,255075
VARIANCE EXPLAINED,0.44607843137254904,"25
50
75 Layer"
VARIANCE EXPLAINED,0.4470588235294118,RN-26 1×
VARIANCE EXPLAINED,0.44803921568627453,"50
150"
VARIANCE EXPLAINED,0.44901960784313727,RN-44 1×
VARIANCE EXPLAINED,0.45,"50
150"
VARIANCE EXPLAINED,0.45098039215686275,RN-56 1×
VARIANCE EXPLAINED,0.4519607843137255,100 300
VARIANCE EXPLAINED,0.45294117647058824,RN-110 1×
VARIANCE EXPLAINED,0.453921568627451,250 500
VARIANCE EXPLAINED,0.4549019607843137,RN-164 1×
VARIANCE EXPLAINED,0.45588235294117646,50 100 50 100 Layer
VARIANCE EXPLAINED,0.4568627450980392,RN-38 1×
VARIANCE EXPLAINED,0.45784313725490194,50 100
VARIANCE EXPLAINED,0.4588235294117647,RN-38 2×
VARIANCE EXPLAINED,0.4598039215686274,"50 100
Layer"
VARIANCE EXPLAINED,0.46078431372549017,RN-38 4×
VARIANCE EXPLAINED,0.46176470588235297,50 100
VARIANCE EXPLAINED,0.4627450980392157,RN-38 8×
VARIANCE EXPLAINED,0.46372549019607845,50 100
VARIANCE EXPLAINED,0.4647058823529412,CIFAR-100
VARIANCE EXPLAINED,0.46568627450980393,RN-38 10×
VARIANCE EXPLAINED,0.4666666666666667,255075
VARIANCE EXPLAINED,0.4676470588235294,"25
50
75 Layer"
VARIANCE EXPLAINED,0.46862745098039216,RN-26 1×
VARIANCE EXPLAINED,0.4696078431372549,"50
150"
VARIANCE EXPLAINED,0.47058823529411764,RN-44 1×
VARIANCE EXPLAINED,0.4715686274509804,"50
150"
VARIANCE EXPLAINED,0.4725490196078431,RN-56 1×
VARIANCE EXPLAINED,0.47352941176470587,100 300
VARIANCE EXPLAINED,0.4745098039215686,RN-110 1×
VARIANCE EXPLAINED,0.47549019607843135,250 500
VARIANCE EXPLAINED,0.4764705882352941,RN-164 1×
VARIANCE EXPLAINED,0.4774509803921569,50 100 50 100 Layer
VARIANCE EXPLAINED,0.47843137254901963,RN-38 1×
VARIANCE EXPLAINED,0.47941176470588237,50 100
VARIANCE EXPLAINED,0.4803921568627451,RN-38 2×
VARIANCE EXPLAINED,0.48137254901960785,"50 100
Layer"
VARIANCE EXPLAINED,0.4823529411764706,RN-38 4×
VARIANCE EXPLAINED,0.48333333333333334,50 100
VARIANCE EXPLAINED,0.4843137254901961,RN-38 8×
VARIANCE EXPLAINED,0.4852941176470588,50 100
VARIANCE EXPLAINED,0.48627450980392156,Patch Camelyon
VARIANCE EXPLAINED,0.4872549019607843,RN-38 10×
VARIANCE EXPLAINED,0.48823529411764705,"0
1
CKA"
VARIANCE EXPLAINED,0.4892156862745098,"Figure D.1: Appearance of block structure depends on the data on which representations are computed.
We plot CKA heatmaps for models of varying depths (top rows) and widths (bottom rows) trained on CIFAR-
10, evaluated on different datasets ordered by the degree of out-of-distribution. We observe that the block
structure representation are robust to small distribution shifts in the data, as evident from CKAs computed on
CIFAR-10 corrupted dataset (which adds perturbations to the original CIFAR-10 data) and CIFAR-100 dataset
(which contains mutually exclusive classes but undergoes the same data collection procedure as CIFAR-10).
However, larger shifts, such as from CIFAR-10 to Patch Camelyon, produce signiﬁcantly different representa-
tional structures."
VARIANCE EXPLAINED,0.49019607843137253,"E
DOMINANT EXAMPLES AND LAYER ACTIVATIONS"
VARIANCE EXPLAINED,0.49117647058823527,"100
200
Layer 100 200 Layer 0.0 0.2 0.4 0.6 0.8 1.0 CKA"
VARIANCE EXPLAINED,0.492156862745098,"50
100
150
200
250
Layer 0 200 400 600 800"
VARIANCE EXPLAINED,0.4931372549019608,Activation Norm
VARIANCE EXPLAINED,0.49411764705882355,"Solid Pink
Solid White
Dominant Datapoint"
VARIANCE EXPLAINED,0.4950980392156863,"Figure E.1: Solid color images strongly activate layers making up the block structure when trained on
Patch Camelyon. Top row shows dominant examples for a ResNet-80 (1×) model trained on Patch Camelyon
dataset. The CKA heatmap in the bottom left shows the location of the block structure in the internal represen-
tations of the model. We observe that the dominant images share a pink background. When we feed a synthetic
image ﬁlled with this background color into the network, we observe that it yields even larger activations com-
pared to the original image, for layers making up the block structure (i.e., after layer 200). See also Figure 6
for a similar plot for models trained on CIFAR-10 dataset."
VARIANCE EXPLAINED,0.49607843137254903,Under review as a conference paper at ICLR 2022
VARIANCE EXPLAINED,0.4970588235294118,"F
EVOLUTION OF THE BLOCK STRUCTURE"
VARIANCE EXPLAINED,0.4980392156862745,"100
200
300 Layer"
VARIANCE EXPLAINED,0.49901960784313726,"Ep. 1
Ep. 2
Ep. 3
Ep. 4
Ep. 5
Ep. 6
Ep. 7
Ep. 8
Ep. 9
Ep. 10"
VARIANCE EXPLAINED,0.5,"100
200
300 Layer"
VARIANCE EXPLAINED,0.5009803921568627,"Ep. 20
Ep. 30
Ep. 40
Ep. 50
Ep. 60
Ep. 70
Ep. 80
Ep. 90
Ep. 100
Ep. 110"
VARIANCE EXPLAINED,0.5019607843137255,100 300
VARIANCE EXPLAINED,0.5029411764705882,"100
200
300 Layer"
VARIANCE EXPLAINED,0.503921568627451,Ep. 120
VARIANCE EXPLAINED,0.5049019607843137,100 300
VARIANCE EXPLAINED,0.5058823529411764,Ep. 140
VARIANCE EXPLAINED,0.5068627450980392,100 300
VARIANCE EXPLAINED,0.5078431372549019,Ep. 160
VARIANCE EXPLAINED,0.5088235294117647,100 300
VARIANCE EXPLAINED,0.5098039215686274,Ep. 180
VARIANCE EXPLAINED,0.5107843137254902,100 300
VARIANCE EXPLAINED,0.5117647058823529,Ep. 200
VARIANCE EXPLAINED,0.5127450980392156,100 300
VARIANCE EXPLAINED,0.5137254901960784,Ep. 220
VARIANCE EXPLAINED,0.5147058823529411,100 300
VARIANCE EXPLAINED,0.515686274509804,Ep. 240
VARIANCE EXPLAINED,0.5166666666666667,100 300
VARIANCE EXPLAINED,0.5176470588235295,Ep. 260
VARIANCE EXPLAINED,0.5186274509803922,100 300
VARIANCE EXPLAINED,0.5196078431372549,Ep. 280
VARIANCE EXPLAINED,0.5205882352941177,100 300
VARIANCE EXPLAINED,0.5215686274509804,Ep. 300 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 CKA
VARIANCE EXPLAINED,0.5225490196078432,"100
200
300 Layer"
VARIANCE EXPLAINED,0.5235294117647059,"Ep. 1
Ep. 2
Ep. 3
Ep. 4
Ep. 5
Ep. 6
Ep. 7
Ep. 8
Ep. 9
Ep. 10"
VARIANCE EXPLAINED,0.5245098039215687,"100
200
300 Layer"
VARIANCE EXPLAINED,0.5254901960784314,"Ep. 20
Ep. 30
Ep. 40
Ep. 50
Ep. 60
Ep. 70
Ep. 80
Ep. 90
Ep. 100
Ep. 110"
VARIANCE EXPLAINED,0.5264705882352941,100 300 Layer
VARIANCE EXPLAINED,0.5274509803921569,"100
200
300 Layer"
VARIANCE EXPLAINED,0.5284313725490196,Ep. 120
VARIANCE EXPLAINED,0.5294117647058824,100 300 Layer
VARIANCE EXPLAINED,0.5303921568627451,Ep. 140
VARIANCE EXPLAINED,0.5313725490196078,100 300 Layer
VARIANCE EXPLAINED,0.5323529411764706,Ep. 160
VARIANCE EXPLAINED,0.5333333333333333,100 300 Layer
VARIANCE EXPLAINED,0.5343137254901961,Ep. 180
VARIANCE EXPLAINED,0.5352941176470588,100 300 Layer
VARIANCE EXPLAINED,0.5362745098039216,Ep. 200
VARIANCE EXPLAINED,0.5372549019607843,100 300 Layer
VARIANCE EXPLAINED,0.538235294117647,Ep. 220
VARIANCE EXPLAINED,0.5392156862745098,100 300 Layer
VARIANCE EXPLAINED,0.5401960784313725,Ep. 240
VARIANCE EXPLAINED,0.5411764705882353,100 300 Layer
VARIANCE EXPLAINED,0.542156862745098,Ep. 260
VARIANCE EXPLAINED,0.5431372549019607,100 300 Layer
VARIANCE EXPLAINED,0.5441176470588235,Ep. 280
VARIANCE EXPLAINED,0.5450980392156862,100 300 Layer
VARIANCE EXPLAINED,0.546078431372549,Ep. 300 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 CKA
VARIANCE EXPLAINED,0.5470588235294118,"Epoch 1
Epoch 5
Epoch 10
Epoch 20"
VARIANCE EXPLAINED,0.5480392156862746,"Epoch 30
Epoch 40
Epoch 50
Epoch 100"
VARIANCE EXPLAINED,0.5490196078431373,"Epoch 150
Epoch 200
Epoch 250
Epoch 300"
VARIANCE EXPLAINED,0.55,Within-Checkpoint Similarity
VARIANCE EXPLAINED,0.5509803921568628,Similarity with Final Checkpoint
VARIANCE EXPLAINED,0.5519607843137255,Dominant Datapoints
VARIANCE EXPLAINED,0.5529411764705883,"Figure F.1: Fine-grained analysis of the evolution of the block structure in a ResNet-110 (1×) model.
This plot shows the evolution of block structure for the same network as in Figure 8, but with greater temporal
granularity. As in Figure 8, we ﬁnd that the shape of the block structure is deﬁned early in training (top row).
However, comparing these different model checkpoints to the ﬁnal, fully-trained model reveals that the block
structure representations at different epochs are considerably dissimilar, especially during the ﬁrst half of the
training process (middle row). The corresponding dominant datapoints also vary over training, even after the
block structure is clearly visible in the within-checkpoint similarity heatmaps (bottom row)."
VARIANCE EXPLAINED,0.553921568627451,Under review as a conference paper at ICLR 2022
VARIANCE EXPLAINED,0.5549019607843138,"100
200
300 Layer"
VARIANCE EXPLAINED,0.5558823529411765,"Ep. 1
Ep. 2
Ep. 3
Ep. 4
Ep. 5
Ep. 6
Ep. 7
Ep. 8
Ep. 9
Ep. 10"
VARIANCE EXPLAINED,0.5568627450980392,"100
200
300 Layer"
VARIANCE EXPLAINED,0.557843137254902,"Ep. 20
Ep. 30
Ep. 40
Ep. 50
Ep. 60
Ep. 70
Ep. 80
Ep. 90
Ep. 100
Ep. 110"
VARIANCE EXPLAINED,0.5588235294117647,100 300
VARIANCE EXPLAINED,0.5598039215686275,"100
200
300 Layer"
VARIANCE EXPLAINED,0.5607843137254902,Ep. 120
VARIANCE EXPLAINED,0.5617647058823529,100 300
VARIANCE EXPLAINED,0.5627450980392157,Ep. 140
VARIANCE EXPLAINED,0.5637254901960784,100 300
VARIANCE EXPLAINED,0.5647058823529412,Ep. 160
VARIANCE EXPLAINED,0.5656862745098039,100 300
VARIANCE EXPLAINED,0.5666666666666667,Ep. 180
VARIANCE EXPLAINED,0.5676470588235294,100 300
VARIANCE EXPLAINED,0.5686274509803921,Ep. 200
VARIANCE EXPLAINED,0.5696078431372549,100 300
VARIANCE EXPLAINED,0.5705882352941176,Ep. 220
VARIANCE EXPLAINED,0.5715686274509804,100 300
VARIANCE EXPLAINED,0.5725490196078431,Ep. 240
VARIANCE EXPLAINED,0.5735294117647058,100 300
VARIANCE EXPLAINED,0.5745098039215686,Ep. 260
VARIANCE EXPLAINED,0.5754901960784313,100 300
VARIANCE EXPLAINED,0.5764705882352941,Ep. 280
VARIANCE EXPLAINED,0.5774509803921568,100 300
VARIANCE EXPLAINED,0.5784313725490197,Ep. 300 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 CKA
VARIANCE EXPLAINED,0.5794117647058824,"100
200
300 Layer"
VARIANCE EXPLAINED,0.5803921568627451,"Ep. 1
Ep. 2
Ep. 3
Ep. 4
Ep. 5
Ep. 6
Ep. 7
Ep. 8
Ep. 9
Ep. 10"
VARIANCE EXPLAINED,0.5813725490196079,"100
200
300 Layer"
VARIANCE EXPLAINED,0.5823529411764706,"Ep. 20
Ep. 30
Ep. 40
Ep. 50
Ep. 60
Ep. 70
Ep. 80
Ep. 90
Ep. 100
Ep. 110"
VARIANCE EXPLAINED,0.5833333333333334,100 300 Layer
VARIANCE EXPLAINED,0.5843137254901961,"100
200
300 Layer"
VARIANCE EXPLAINED,0.5852941176470589,Ep. 120
VARIANCE EXPLAINED,0.5862745098039216,100 300 Layer
VARIANCE EXPLAINED,0.5872549019607843,Ep. 140
VARIANCE EXPLAINED,0.5882352941176471,100 300 Layer
VARIANCE EXPLAINED,0.5892156862745098,Ep. 160
VARIANCE EXPLAINED,0.5901960784313726,100 300 Layer
VARIANCE EXPLAINED,0.5911764705882353,Ep. 180
VARIANCE EXPLAINED,0.592156862745098,100 300 Layer
VARIANCE EXPLAINED,0.5931372549019608,Ep. 200
VARIANCE EXPLAINED,0.5941176470588235,100 300 Layer
VARIANCE EXPLAINED,0.5950980392156863,Ep. 220
VARIANCE EXPLAINED,0.596078431372549,100 300 Layer
VARIANCE EXPLAINED,0.5970588235294118,Ep. 240
VARIANCE EXPLAINED,0.5980392156862745,100 300 Layer
VARIANCE EXPLAINED,0.5990196078431372,Ep. 260
VARIANCE EXPLAINED,0.6,100 300 Layer
VARIANCE EXPLAINED,0.6009803921568627,Ep. 280
VARIANCE EXPLAINED,0.6019607843137255,100 300 Layer
VARIANCE EXPLAINED,0.6029411764705882,Ep. 300 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 CKA
VARIANCE EXPLAINED,0.6039215686274509,"Epoch 1
Epoch 5
Epoch 10
Epoch 20"
VARIANCE EXPLAINED,0.6049019607843137,"Epoch 30
Epoch 40
Epoch 50
Epoch 100"
VARIANCE EXPLAINED,0.6058823529411764,"Epoch 160
Epoch 200
Epoch 260
Epoch 300"
VARIANCE EXPLAINED,0.6068627450980392,Within-Checkpoint Similarity
VARIANCE EXPLAINED,0.6078431372549019,Similarity with Final Checkpoint
VARIANCE EXPLAINED,0.6088235294117647,Dominant Datapoints
VARIANCE EXPLAINED,0.6098039215686275,"Figure F.2: Fine-grained analysis of the evolution of the block structure in a different ResNet-110 (1×)
model. This plot shows the evolution of block structure for a network that is architecturally identical to the
one in Figure 8, but trained with a different seed. For this training run, the ﬁnal shape of the block structure is
established slightly later in training (top row), and similarity between early checkpoints and the last checkpoint
is very low (middle row). Analysis of the dominant data points shows that they change substantially over the
course of training (bottom row), and continue to vary long after the shape of the block structure ceases to change
in the within-checkpoint similarity heatmaps."
VARIANCE EXPLAINED,0.6107843137254902,Under review as a conference paper at ICLR 2022 250 500 Layer
VARIANCE EXPLAINED,0.611764705882353,"Ep. 10
Ep. 20
Ep. 50
Ep. 70
Ep. 90
Ep. 100
Ep. 110
Ep. 120
Ep. 130
Ep. 140"
VARIANCE EXPLAINED,0.6127450980392157,250500 250 500 Layer
VARIANCE EXPLAINED,0.6137254901960785,Ep. 150
VARIANCE EXPLAINED,0.6147058823529412,250500
VARIANCE EXPLAINED,0.615686274509804,Ep. 160
VARIANCE EXPLAINED,0.6166666666666667,250500
VARIANCE EXPLAINED,0.6176470588235294,Ep. 170
VARIANCE EXPLAINED,0.6186274509803922,250500
VARIANCE EXPLAINED,0.6196078431372549,Ep. 180
VARIANCE EXPLAINED,0.6205882352941177,250500
VARIANCE EXPLAINED,0.6215686274509804,Ep. 210
VARIANCE EXPLAINED,0.6225490196078431,250500
VARIANCE EXPLAINED,0.6235294117647059,Ep. 230
VARIANCE EXPLAINED,0.6245098039215686,250500
VARIANCE EXPLAINED,0.6254901960784314,Ep. 250
VARIANCE EXPLAINED,0.6264705882352941,250500
VARIANCE EXPLAINED,0.6274509803921569,Ep. 270
VARIANCE EXPLAINED,0.6284313725490196,250500
VARIANCE EXPLAINED,0.6294117647058823,Ep. 290
VARIANCE EXPLAINED,0.6303921568627451,250500
VARIANCE EXPLAINED,0.6313725490196078,Ep. 300
VARIANCE EXPLAINED,0.6323529411764706,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0 CKA 250 500 Layer"
VARIANCE EXPLAINED,0.6333333333333333,"Ep. 0
Ep. 10
Ep. 20
Ep. 30
Ep. 40
Ep. 50
Ep. 60
Ep. 70
Ep. 80
Ep. 90"
LAYER,0.634313725490196,"250500
Layer 250 500 Layer"
LAYER,0.6352941176470588,Ep. 100
LAYER,0.6362745098039215,"250500
Layer"
LAYER,0.6372549019607843,Ep. 120
LAYER,0.638235294117647,"250500
Layer"
LAYER,0.6392156862745098,Ep. 140
LAYER,0.6401960784313725,"250500
Layer"
LAYER,0.6411764705882353,Ep. 160
LAYER,0.6421568627450981,"250500
Layer"
LAYER,0.6431372549019608,Ep. 180
LAYER,0.6441176470588236,"250500
Layer"
LAYER,0.6450980392156863,Ep. 200
LAYER,0.6460784313725491,"250500
Layer"
LAYER,0.6470588235294118,Ep. 220
LAYER,0.6480392156862745,"250500
Layer"
LAYER,0.6490196078431373,Ep. 240
LAYER,0.65,"250500
Layer"
LAYER,0.6509803921568628,Ep. 260
LAYER,0.6519607843137255,"250500
Layer"
LAYER,0.6529411764705882,Similarity Between Layers of Same Checkpoint
LAYER,0.653921568627451,Similarity with Final Checkpoint
LAYER,0.6549019607843137,Ep. 280
LAYER,0.6558823529411765,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0 CKA"
LAYER,0.6568627450980392,"Figure F.3: Evolution of the block structure over the course of training for a ResNet-164 (1×) model. We
compute the CKA between all pairs of layers within a ResNet-38 (10×) model at different stages of training,
and ﬁnd that the internal representations already contain a block structure at epoch 10. Comparing these
different model checkpoints to the ﬁnal, fully-trained model reveals that the block structure representations
at different epochs are considerably dissimilar, especially during the ﬁrst half of the training process."
LAYER,0.657843137254902,"0
50
100
150
200
250
300
Epoch 0.0 0.5"
LAYER,0.6588235294117647,Projected Value 0 2
LAYER,0.6598039215686274,Epoch 0 0 2
LAYER,0.6607843137254902,Epoch 30 0 2
LAYER,0.6617647058823529,Epoch 60 0 2
LAYER,0.6627450980392157,Epoch 90 0 2
LAYER,0.6637254901960784,Activation Norm (×103)
LAYER,0.6647058823529411,Epoch 120 0 2
LAYER,0.6656862745098039,Epoch 150 0 2
LAYER,0.6666666666666666,Epoch 180 0 2
LAYER,0.6676470588235294,Epoch 210
LAYER,0.6686274509803921,"0
250
500
Layer 0 2"
LAYER,0.6696078431372549,Epoch 240
LAYER,0.6705882352941176,"0
250
500
Layer 0 2"
LAYER,0.6715686274509803,Epoch 270
LAYER,0.6725490196078432,"0
250
500
Layer 0 2"
LAYER,0.6735294117647059,Epoch 300
LAYER,0.6745098039215687,"Dominant Datapoint
Batch Average"
LAYER,0.6754901960784314,"Figure F.4: Example of how the magnitudes of layer activations and the projected values onto the ﬁrst
principal component for a dominant image vary across different epochs, for a ResNet-164 (1×). Given a
dominant datapoint for a ResNet-164 (1×) model, we track the magnitude of its projected value onto the ﬁrst
principal component of the block structure representations (top left), as well as its activation norms at each layer
in the network (bottom set of plots), over time. Notice the correspondence between these 2 metrics, especially
when their values drop at epochs 0 (initialization), 120 and 240. This is also aligned with the measurement of
CKAs across different epochs (see Figure F.3, where we ﬁnd that the model checkpoints at these 3 epochs are
highly dissimilar from the ﬁnal model in terms of the hidden representations). See Appendix Figure F.6 for the
corresponding visualization of a dominant example of a wide ResNet (ResNet-38 (10×))."
LAYER,0.6764705882352942,Under review as a conference paper at ICLR 2022 50 100 Layer
LAYER,0.6774509803921569,"Ep. 10
Ep. 20
Ep. 50
Ep. 70
Ep. 90
Ep. 100
Ep. 110
Ep. 120
Ep. 130
Ep. 140 50100 50 100 Layer"
LAYER,0.6784313725490196,Ep. 150 50100
LAYER,0.6794117647058824,Ep. 160 50100
LAYER,0.6803921568627451,Ep. 170 50100
LAYER,0.6813725490196079,Ep. 180 50100
LAYER,0.6823529411764706,Ep. 210 50100
LAYER,0.6833333333333333,Ep. 230 50100
LAYER,0.6843137254901961,Ep. 250 50100
LAYER,0.6852941176470588,Ep. 270 50100
LAYER,0.6862745098039216,Ep. 290 50100
LAYER,0.6872549019607843,Ep. 300
LAYER,0.6882352941176471,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0 CKA 50 100 Layer"
LAYER,0.6892156862745098,"Ep. 0
Ep. 10
Ep. 20
Ep. 30
Ep. 40
Ep. 50
Ep. 60
Ep. 70
Ep. 80
Ep. 90"
LAYER,0.6901960784313725,"50100
Layer 50 100 Layer"
LAYER,0.6911764705882353,Ep. 100
LAYER,0.692156862745098,"50100
Layer"
LAYER,0.6931372549019608,Ep. 120
LAYER,0.6941176470588235,"50100
Layer"
LAYER,0.6950980392156862,Ep. 140
LAYER,0.696078431372549,"50100
Layer"
LAYER,0.6970588235294117,Ep. 160
LAYER,0.6980392156862745,"50100
Layer"
LAYER,0.6990196078431372,Ep. 180
LAYER,0.7,"50100
Layer"
LAYER,0.7009803921568627,Ep. 200
LAYER,0.7019607843137254,"50100
Layer"
LAYER,0.7029411764705882,Ep. 220
LAYER,0.703921568627451,"50100
Layer"
LAYER,0.7049019607843138,Ep. 240
LAYER,0.7058823529411765,"50100
Layer"
LAYER,0.7068627450980393,Ep. 260
LAYER,0.707843137254902,"50100
Layer"
LAYER,0.7088235294117647,Similarity Between Layers of Same Checkpoint
LAYER,0.7098039215686275,Similarity with Final Checkpoint
LAYER,0.7107843137254902,Ep. 280
LAYER,0.711764705882353,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0 CKA"
LAYER,0.7127450980392157,"Figure F.5: Evolution of the block structure over the course of training for a ResNet-38 (10×) model. We
compute the CKA between all pairs of layers within a ResNet-38 (10×) model at different stages of training,
and ﬁnd that the internal representations already contain a block structure at epoch 10. Comparing these
different model checkpoints to the ﬁnal, fully-trained model reveals that the block structure representations
at different epochs are considerably dissimilar, especially during the ﬁrst half of the training process. See also
Figure F.3 for a similar plot for a deep ResNet (ResNet-164 (1×))."
LAYER,0.7137254901960784,"0
50
100
150
200
250
300
Epoch 0.0 0.1 0.2"
LAYER,0.7147058823529412,Projected Value 0 2
LAYER,0.7156862745098039,Epoch 0 0 2
LAYER,0.7166666666666667,Epoch 30 0 2
LAYER,0.7176470588235294,Epoch 60 0 2
LAYER,0.7186274509803922,Epoch 90 0 2
LAYER,0.7196078431372549,Activation Norm (×103)
LAYER,0.7205882352941176,Epoch 120 0 2
LAYER,0.7215686274509804,Epoch 150 0 2
LAYER,0.7225490196078431,Epoch 180 0 2
LAYER,0.7235294117647059,Epoch 210
LAYER,0.7245098039215686,"0
50
100
Layer 0 2"
LAYER,0.7254901960784313,Epoch 240
LAYER,0.7264705882352941,"0
50
100
Layer 0 2"
LAYER,0.7274509803921568,Epoch 270
LAYER,0.7284313725490196,"0
50
100
Layer 0 2"
LAYER,0.7294117647058823,Epoch 300
LAYER,0.7303921568627451,"Dominant Datapoint
Batch Median"
LAYER,0.7313725490196078,"Figure F.6: Example of how the activation magnitude and the projected values onto the ﬁrst principal
component for a dominant image vary across different epochs, for ResNet-38 (10×). Given a dominant
datapoint for a ResNet-38 (10×) model, we track the magnitude of its projected value onto the ﬁrst principal
component of the block structure representations (top left), as well as its activation norms at each layer in the
network (bottom set of plots), over time. We observe that before the block structure representations stabilize and
show more similarity with those in the fully trained model (i.e., epoch 90, see Figure F.5 above), the dominant
image yields a small value when projected onto the ﬁrst principal component, and also doesn’t strongly activate
the layers inside the block structure. This is aligned with the observation made in Figure F.4 for ResNet-164
(1×)."
LAYER,0.7323529411764705,Under review as a conference paper at ICLR 2022
LAYER,0.7333333333333333,"50
100
150
200
250
300
350
Layer 0 20 40 60 80 100 Step 0.0 0.2 0.4 0.6 0.8 1.0"
LAYER,0.734313725490196,Fraction Variance Explained by First PC
LAYER,0.7352941176470589,"Figure F.7: Evolution of the ﬁrst principal components of layer representations over the ﬁrst 100 steps
of training. At every step of training, we measure the proportion of variance explained by the ﬁrst principal
component in each layer of a ResNet-110 (1×) network. At around step 30, the ﬁrst principal component
begins to explain the majority of the variance in the layer representations."
LAYER,0.7362745098039216,Under review as a conference paper at ICLR 2022
LAYER,0.7372549019607844,"G
TRAINING WITH PRINCIPAL COMPONENT REGULARIZATION"
LAYER,0.7382352941176471,"To regularize the ﬁrst principal component, we ﬁrst compute the amount of variance that it explains
using power iteration (Miyato et al., 2018). At training step, t, we compute the batch of n convo-
lutional feature maps with height h and width w containing c channels Mt ∈Rn×h×w×c, ﬂatten
the spatial dimensions to the channels dimension to create a matrix of size Xt ∈Rn×p where
p = h × w × c, and subtract its column means to obtain a centered matrix ˜
Xt. We randomly initial-
ize the stored eigenvector u0 ∈Rp at the beginning of training. At each training step, we perform a
single step of power iteration initialized from the previous eigenvector:
vt = ˜
XT
t ˜
Xtut−1
(3)
λt = ∥vt∥2
(4)
ut = vt/λt.
(5)"
LAYER,0.7392156862745098,"λt approximates the top eigenvalue of ˜
XT
t ˜
Xt and thus the amount of variance explained by the
ﬁrst principal component of the representation. The proportion of variance explained is given by
λt/∥˜Xt∥2
F. We incorporate the regularizer as an additive term in the loss:
Lpc reg(λt, ˜
X; α, δ) = α max(λt/∥˜
Xt∥2
F −δ, 0),
(6)
where α is the strength of the regularizer and δ is the threshold proportion of variance explained at
which it is imposed. In our experiments, we tune α as a hyperparameter with values in [0.1, 1, 10],
and set δ = 0.2 based on our analysis of the ﬁrst principal components of models without the block
structure. To speed up the training process, we only apply the regularizer to ReLU layers starting
from the second stage, where block structure is often found."
LAYER,0.7401960784313726,"Depth
Width
Accuracy (%)
(standard training)
Accuracy (%)
(PC regularization)"
LAYER,0.7411764705882353,"CIFAR-10 subsampled (6% of the full dataset):
56
1
77.8 ± 0.429
79.2 ± 0.188
26
8
80.1 ± 0.354
81.1 ± 0.185
26
10
80.3 ± 0.306
81.2 ± 0.194
38
8
80.2 ± 0.362
80.9 ± 0.264
38
10
80.3 ± 0.412
81.4 ± 0.350"
LAYER,0.7421568627450981,"CIFAR-10:
110
1
94.3 ± 0.078
94.4 ± 0.063
164
1
94.4 ± 0.075
94.5 ± 0.063
26
10
95.8 ± 0.087
96.0 ± 0.051
38
8
95.7 ± 0.091
95.8 ± 0.080
38
10
95.7 ± 0.157
95.9 ± 0.067"
LAYER,0.7431372549019608,"CIFAR-100:
218
1
74.1 ± 0.310
75.1 ± 0.132
224
1
74.0 ± 0.350
75.2 ± 0.131
38
8
79.8 ± 0.149
80.6 ± 0.306
38
10
80.5 ± 0.174
81.1 ± 0.241"
LAYER,0.7441176470588236,"Table G.1: Comparison of performance of large capacity models on CIFAR-10 and CIFAR-100, with and
without principal component regularization. We observe that our proposed principal component regularizer
consistently yields accuracy improvements for large capacity models that contain the block structure. The
performance gains are particularly signiﬁcant in the case of CIFAR-100 and subsampled CIFAR-10 datasets."
LAYER,0.7450980392156863,Under review as a conference paper at ICLR 2022
LAYER,0.746078431372549,"H
EFFECT OF BATCH SIZE ON THE BLOCK STRUCTURE"
LAYER,0.7470588235294118,"100
300"
LAYER,0.7480392156862745,"100
200
300 Layer"
LAYER,0.7490196078431373,ResNet-110 1x
LAYER,0.75,250 500 250 500
LAYER,0.7509803921568627,ResNet-164 1x
LAYER,0.7519607843137255,50 100 50 100
LAYER,0.7529411764705882,ResNet-38 8x
LAYER,0.753921568627451,50 100 50 100
LAYER,0.7549019607843137,ResNet-38 10x
LAYER,0.7558823529411764,"100
300
Layer"
LAYER,0.7568627450980392,"100
200
300 Layer"
LAYER,0.7578431372549019,ResNet-110 1x
LAYER,0.7588235294117647,"250 500
Layer 250 500"
LAYER,0.7598039215686274,ResNet-164 1x
LAYER,0.7607843137254902,"50 100
Layer 50 100"
LAYER,0.7617647058823529,ResNet-38 8x
LAYER,0.7627450980392156,"50 100
Layer 50 100"
LAYER,0.7637254901960784,ResNet-38 10x 0.0 0.5 1.0 CKA 0.0 0.5 1.0 CKA
LAYER,0.7647058823529411,Batch Size 128
LAYER,0.765686274509804,Batch Size 16
LAYER,0.7666666666666667,"Figure H.1: Using very small batch sizes during training reduces the appearance of the block structure.
The top row shows the block structure effect in a range of very deep and wide networks, trained with
standard batch size = 128. We experiment with a drastically smaller batch size of 16 (bottom row)
and ﬁnd that the block structure is now highly reduced, especially in deep models."
LAYER,0.7676470588235295,Under review as a conference paper at ICLR 2022
LAYER,0.7686274509803922,"I
IMPACT OF TRANSFER LEARNING AND SHAKE-SHAKE REGULARIZATION
ON SIMILARITY OF LAYERS INSIDE THE BLOCK STRUCTURE 250 500"
LAYER,0.7696078431372549,Seed 1 Layer
LAYER,0.7705882352941177,"0
500
Seed 3 Layer 250 500"
LAYER,0.7715686274509804,Seed 2 Layer
LAYER,0.7725490196078432,"0
500
Seed 2 Layer"
LAYER,0.7735294117647059,"0
500
Seed 1 Layer 250 500"
LAYER,0.7745098039215687,Seed 3 Layer 250 500
LAYER,0.7754901960784314,"0
500
Seed 3 Layer 250 500"
LAYER,0.7764705882352941,"0
500
Seed 1 Layer 250 500"
LAYER,0.7774509803921569,"0
500
Seed 2 Layer 500 1000"
LAYER,0.7784313725490196,"0
1000
Seed 3 Layer 500 1000"
LAYER,0.7794117647058824,"0
1000
Seed 1 Layer 500 1000"
LAYER,0.7803921568627451,"0
1000
Seed 2 Layer"
LAYER,0.7813725490196078,"0
1
CKA"
LAYER,0.7823529411764706,"Standard Training
Transfer Learning
Shake-Shake Regularization"
LAYER,0.7833333333333333,"Figure I.1: Training with transfer learning and Shake-Shake regularization yields models that are more
similar across different training runs. Each group of plots shows CKA between layers of models with the
same architecture but different initializations (off the diagonal) or within a single model (on the diagonal). In
the standard training case, representations across models are highly dissimilar, especially in the block structure
region. In contrast, when we use transfer learning and Shake-Shake regularization, comparisons across seeds
show more similarity in corresponding layers. The same observation can be made for models trained with
principal component regularization (see Appendix Figure I.2). 250 500"
LAYER,0.7843137254901961,Seed 1 Layer
LAYER,0.7852941176470588,"0
500
Seed 3 Layer 250 500"
LAYER,0.7862745098039216,Seed 2 Layer
LAYER,0.7872549019607843,"0
500
Seed 2 Layer"
LAYER,0.788235294117647,"0
500
Seed 1 Layer 250 500"
LAYER,0.7892156862745098,Seed 3 Layer 250 500
LAYER,0.7901960784313725,"0
500
Seed 3 Layer 250 500"
LAYER,0.7911764705882353,"0
500
Seed 1 Layer 250 500"
LAYER,0.792156862745098,"0
500
Seed 2 Layer"
LAYER,0.7931372549019607,"0
1
CKA"
LAYER,0.7941176470588235,"Standard Training
Principal Component Regularization"
LAYER,0.7950980392156862,"Figure I.2: Training with principal component regularization yields models that are more similar across
different training runs. Each group of plots shows CKA between layers of models with the same architecture
but different initializations (off the diagonal) or within a single model (on the diagonal). Similar to the obser-
vation made in Figure I.1, while representations across models are highly dissimilar in the standard training
case, models trained with principal component regularization from different random initializations show more
representational similarity in corresponding layers."
LAYER,0.796078431372549,Under review as a conference paper at ICLR 2022
LAYER,0.7970588235294118,"J
BLOCK STRUCTURE UNDER DIFFERENT KERNELS"
LAYER,0.7980392156862746,"As previously identiﬁed by (Nguyen et al., 2021), the block structure is a phenomenon the linear
CKA heatmaps of large (wide or deep) networks. In this section, we investigate whether the block
structure phenomenon also arises in CKA heatmaps computed with other kernels, and also examine
the effect of removing the dominant datapoints (identiﬁed by the magnitudes of their projections on
the ﬁrst principal component, as in Section 4.1) upon these CKA heatmaps."
LAYER,0.7990196078431373,"To compute CKA heatmaps under alternative kernels, we again use minibatch CKA. The approach
in Eq. 1 can be easily adapted to nonlinear kernels by replacing XiXT
i and YiY T
i
the linear Gram
matrices formed by minibatch i, with minibatch kernel matrices Ki ∈Rn×n and K′
i ∈Rn×n.
The elements of these minibatch kernel matrices are the kernels between pairs of examples in the
minibatches, i.e., Kilm = k(Xil,:, Xim,:) and K′
lm = k′(Yil,:, Yim,:). Like linear minibatch CKA,
nonlinear minibatch CKA is computed by averaging HSIC1 across minibatches:"
LAYER,0.8,CKAminibatch =
"K
PK",0.8009803921568628,"1
k
Pk
i=1 HSIC1(Ki, K′
i)
q"
"K
PK",0.8019607843137255,"1
k
Pk
i=1 HSIC1(Ki, Ki)
q"
"K
PK",0.8029411764705883,"1
k
Pk
i=1 HSIC1(K′
i, K′
i)
.
(7)"
"K
PK",0.803921568627451,"We investigate the behavior of CKA under the linear kernel klinear(x, y) = xTy, the cosine kernel
kcos(x, y) = xTy/(∥x∥∥y∥), and the RBF kernel krbf(x, y; σ) = exp(−∥x −y∥2/(2σ2)). For
each layer, we measure the median Euclidean distance ˜d between examples in each layer and set
σ = c ˜d with c ∈{0.2, 0.5, 1, 2, 5, 10} of that median Euclidean distance. To reduce variance when
computing RBF CKA with small c, we use a minibatch size of 1000 for these experiments."
"K
PK",0.8049019607843138,"Figure J.1 shows the appearance of CKA heatmaps of a narrow, shallow network (ResNet-38 1×,
top), a wide network (ResNet-38 10×, middle), and a deep network (ResNet-164 1×, bottom). Al-
though heatmaps computed for a small network (ResNet-38 1×) look qualitatively similar regardless
of kernels, both wide (ResNet-38 10×) and deep (ResNet-164 1×) networks exhibit signiﬁcant dif-
ferences."
"K
PK",0.8058823529411765,"50
100
Layer 50 100 Layer"
"K
PK",0.8068627450980392,Linear
"K
PK",0.807843137254902,"50
100
Layer"
"K
PK",0.8088235294117647,Cosine
"K
PK",0.8098039215686275,"50
100
Layer"
"K
PK",0.8107843137254902,RBF 0.2
"K
PK",0.8117647058823529,"50
100
Layer"
"K
PK",0.8127450980392157,RBF 0.5
"K
PK",0.8137254901960784,"50
100
Layer RBF 1"
"K
PK",0.8147058823529412,"50
100
Layer RBF 2"
"K
PK",0.8156862745098039,"50
100
Layer RBF 5"
"K
PK",0.8166666666666667,"50
100
Layer"
"K
PK",0.8176470588235294,ResNet-38 1×
"K
PK",0.8186274509803921,RBF 10
"K
PK",0.8196078431372549,"50
100
Layer 50 100 Layer"
"K
PK",0.8205882352941176,Linear
"K
PK",0.8215686274509804,"50
100
Layer"
"K
PK",0.8225490196078431,Cosine
"K
PK",0.8235294117647058,"50
100
Layer"
"K
PK",0.8245098039215686,RBF 0.2
"K
PK",0.8254901960784313,"50
100
Layer"
"K
PK",0.8264705882352941,RBF 0.5
"K
PK",0.8274509803921568,"50
100
Layer RBF 1"
"K
PK",0.8284313725490197,"50
100
Layer RBF 2"
"K
PK",0.8294117647058824,"50
100
Layer RBF 5"
"K
PK",0.8303921568627451,"50
100
Layer"
"K
PK",0.8313725490196079,ResNet-38 10×
"K
PK",0.8323529411764706,RBF 10
"K
PK",0.8333333333333334,"250
500
Layer 200 400 Layer"
"K
PK",0.8343137254901961,Linear
"K
PK",0.8352941176470589,"250
500
Layer"
"K
PK",0.8362745098039216,Cosine
"K
PK",0.8372549019607843,"250
500
Layer"
"K
PK",0.8382352941176471,RBF 0.2
"K
PK",0.8392156862745098,"250
500
Layer"
"K
PK",0.8401960784313726,RBF 0.5
"K
PK",0.8411764705882353,"250
500
Layer RBF 1"
"K
PK",0.842156862745098,"250
500
Layer RBF 2"
"K
PK",0.8431372549019608,"250
500
Layer RBF 5"
"K
PK",0.8441176470588235,"250
500
Layer"
"K
PK",0.8450980392156863,ResNet-164 1×
"K
PK",0.846078431372549,RBF 10
"K
PK",0.8470588235294118,"Figure J.1: Appearance of representation heatmaps in wide and deep networks, but not narrow/shallow
networks, depends on the choice of kernel. Rows reﬂect different models and columns reﬂect different ker-
nels. For RBF kernels, the parameter indicates the fraction of the median distance between examples (computed
separately for each layer) that is used as the standard deviation of the kernel."
"K
PK",0.8480392156862745,"Because differences in representational similarity heatmaps ultimately reﬂect differences in the un-
derlying kernel matrices, in Figure J.2, we show kernel matrices of individual layers taken from
inside the block structure of each network on random minibatches where the examples have been
sorted in descending values of the ﬁrst principal component. All kernels are sensitive to dominant
datapoints, but in different ways and to different degrees. Linear kernel matrices are dominated by"
"K
PK",0.8490196078431372,Under review as a conference paper at ICLR 2022
"K
PK",0.85,"the similarity between dominant datapoints. The cosine kernel ignores activation norms, and ﬁnds
high similarity within groups of dominant and non-dominant datapoints but low similarity between
groups. The RBF kernel effectively considers all far away points to be equally dissimilar, and thus
indicates that dominant datapoints are dissimilar to all other datapoints, including other dominant
datapoints, which are typically far in Euclidean distance (because, while aligned in direction, they
have different norms)."
"K
PK",0.8509803921568627,"Note that the prevalence of dominant datapoints can differ across models and initializations, as
previously demonstrated in Figure 4. The dominant datapoints are clearly visible as a block in the
top-left corner of the cosine kernel matrix. For ResNet-38 10×, there are 14 in the minibatch of 128
examples that is shown, but for ResNet-164 (1×), there are only 2."
"K
PK",0.8519607843137255,"0
100
Example 0 100"
"K
PK",0.8529411764705882,Example
"K
PK",0.8539215686274509,Linear 1 2 3 1e3
"K
PK",0.8549019607843137,"0
100
Example"
"K
PK",0.8558823529411764,Cosine 4 6
"K
PK",0.8568627450980392,"8
1e 1"
"K
PK",0.8578431372549019,"0
100
Example"
"K
PK",0.8588235294117647,RBF 0.2 2 4 1e 3
"K
PK",0.8598039215686275,"0
100
Example"
"K
PK",0.8607843137254902,RBF 0.5 2 4 1e 1
"K
PK",0.861764705882353,"0
100
Example RBF 1 6"
"K
PK",0.8627450980392157,"8
1e 1"
"K
PK",0.8637254901960785,"0
100
Example RBF 2 8.5 9.0 1e 1"
"K
PK",0.8647058823529412,"0
100
Example RBF 5 9.8 9.9 1e 1"
"K
PK",0.865686274509804,"0
100
Example"
"K
PK",0.8666666666666667,ResNet-38 1× Layer 76
"K
PK",0.8676470588235294,RBF 10 9.94 9.96 1e 1
"K
PK",0.8686274509803922,"0
100
Example 0 100"
"K
PK",0.8696078431372549,Example
"K
PK",0.8705882352941177,Linear 1 2 1e4
"K
PK",0.8715686274509804,"0
100
Example"
"K
PK",0.8725490196078431,Cosine 5.0 7.5 1e 1
"K
PK",0.8735294117647059,"0
100
Example"
"K
PK",0.8745098039215686,RBF 0.2 0 5 1e 3
"K
PK",0.8754901960784314,"0
100
Example"
"K
PK",0.8764705882352941,RBF 0.5 0 2 4 1e 1
"K
PK",0.8774509803921569,"0
100
Example RBF 1 0.0 2.5 5.0 7.5 1e 1"
"K
PK",0.8784313725490196,"0
100
Example RBF 2 2.5 5.0 7.5 1e 1"
"K
PK",0.8794117647058823,"0
100
Example RBF 5 2.5 5.0 7.5 1e 1"
"K
PK",0.8803921568627451,"0
100
Example"
"K
PK",0.8813725490196078,ResNet-38 10× Layer 76
"K
PK",0.8823529411764706,RBF 10 6 8 1e 1
"K
PK",0.8833333333333333,"0
100
Example 0 100"
"K
PK",0.884313725490196,Example
"K
PK",0.8852941176470588,Linear 0 1 2 1e5
"K
PK",0.8862745098039215,"0
100
Example"
"K
PK",0.8872549019607843,Cosine 0 5 1e 1
"K
PK",0.888235294117647,"0
100
Example"
"K
PK",0.8892156862745098,RBF 0.2 0.0 0.5 1.0 1e 2
"K
PK",0.8901960784313725,"0
100
Example"
"K
PK",0.8911764705882353,RBF 0.5 0 2 4 1e 1
"K
PK",0.8921568627450981,"0
100
Example RBF 1 0.0 2.5 5.0 7.5 1e 1"
"K
PK",0.8931372549019608,"0
100
Example RBF 2 2.5 5.0 7.5 1e 1"
"K
PK",0.8941176470588236,"0
100
Example RBF 5 2.5 5.0 7.5 1e 1"
"K
PK",0.8950980392156863,"0
100
Example"
"K
PK",0.8960784313725491,ResNet-164 1× Layer 276
"K
PK",0.8970588235294118,RBF 10 6 8 1e 1
"K
PK",0.8980392156862745,"Figure J.2: Kernels based on dot products, cosine similarity, or Euclidean distance are sensitive to dom-
inant datapoints. Plots show kernel matrices for a randomly sampled minibatch of 128 examples, computed
from a layer inside the block structure, for models that exhibit one. Examples are sorted in descending value of
the ﬁrst principal component of the raw activations; top rows and left columns reﬂect dominant datapoints."
"K
PK",0.8990196078431373,"What is the effect of removing dominant datapoints upon CKA similarity heatmaps computed with
these other kernels? In Figure J.3, we show that, once the dominant datapoints are removed from
large networks, we again see only differences among CKA heatmaps computed with different ker-
nels, in line with the results observed for shallow networks in Figure J.1. There are no longer large
blocks of many consecutive similar layers in any of the heatmaps. Across all choices of kernel that
we have investigated, when blocks appear in CKA heatmaps, they can be eliminated by eliminating
the dominant datapoints."
"K
PK",0.9,Under review as a conference paper at ICLR 2022
"K
PK",0.9009803921568628,"50
100 50 100 Layer"
"K
PK",0.9019607843137255,Linear
"K
PK",0.9029411764705882,"50
100"
"K
PK",0.903921568627451,Cosine
"K
PK",0.9049019607843137,"50
100"
"K
PK",0.9058823529411765,RBF 0.2
"K
PK",0.9068627450980392,"50
100"
"K
PK",0.907843137254902,RBF 0.5
"K
PK",0.9088235294117647,"50
100 RBF 1"
"K
PK",0.9098039215686274,"50
100 RBF 2"
"K
PK",0.9107843137254902,"50
100 RBF 5"
"K
PK",0.9117647058823529,"50
100"
"K
PK",0.9127450980392157,No Datapoints Dropped
"K
PK",0.9137254901960784,RBF 10
"K
PK",0.9147058823529411,"50
100 50 100 Layer"
"K
PK",0.9156862745098039,Linear
"K
PK",0.9166666666666666,"50
100"
"K
PK",0.9176470588235294,Cosine
"K
PK",0.9186274509803921,"50
100"
"K
PK",0.9196078431372549,RBF 0.2
"K
PK",0.9205882352941176,"50
100"
"K
PK",0.9215686274509803,RBF 0.5
"K
PK",0.9225490196078432,"50
100 RBF 1"
"K
PK",0.9235294117647059,"50
100 RBF 2"
"K
PK",0.9245098039215687,"50
100 RBF 5"
"K
PK",0.9254901960784314,"50
100"
"K
PK",0.9264705882352942,1% Most Dominant Datapoints Dropped
"K
PK",0.9274509803921569,RBF 10
"K
PK",0.9284313725490196,"50
100 50 100 Layer"
"K
PK",0.9294117647058824,Linear
"K
PK",0.9303921568627451,"50
100"
"K
PK",0.9313725490196079,Cosine
"K
PK",0.9323529411764706,"50
100"
"K
PK",0.9333333333333333,RBF 0.2
"K
PK",0.9343137254901961,"50
100"
"K
PK",0.9352941176470588,RBF 0.5
"K
PK",0.9362745098039216,"50
100 RBF 1"
"K
PK",0.9372549019607843,"50
100 RBF 2"
"K
PK",0.9382352941176471,"50
100 RBF 5"
"K
PK",0.9392156862745098,"50
100"
"K
PK",0.9401960784313725,10% Most Dominant Datapoints Dropped
"K
PK",0.9411764705882353,RBF 10
"K
PK",0.942156862745098,"50
100
Layer 50 100 Layer"
"K
PK",0.9431372549019608,Linear
"K
PK",0.9441176470588235,"50
100
Layer"
"K
PK",0.9450980392156862,Cosine
"K
PK",0.946078431372549,"50
100
Layer"
"K
PK",0.9470588235294117,RBF 0.2
"K
PK",0.9480392156862745,"50
100
Layer"
"K
PK",0.9490196078431372,RBF 0.5
"K
PK",0.95,"50
100
Layer RBF 1"
"K
PK",0.9509803921568627,"50
100
Layer RBF 2"
"K
PK",0.9519607843137254,"50
100
Layer RBF 5"
"K
PK",0.9529411764705882,"50
100
Layer"
"K
PK",0.953921568627451,20% Most Dominant Datapoints Dropped
"K
PK",0.9549019607843138,RBF 10
"K
PK",0.9558823529411765,"250
500 200 400 Layer"
"K
PK",0.9568627450980393,Linear
"K
PK",0.957843137254902,"250
500"
"K
PK",0.9588235294117647,Cosine
"K
PK",0.9598039215686275,"250
500"
"K
PK",0.9607843137254902,RBF 0.2
"K
PK",0.961764705882353,"250
500"
"K
PK",0.9627450980392157,RBF 0.5
"K
PK",0.9637254901960784,"250
500 RBF 1"
"K
PK",0.9647058823529412,"250
500 RBF 2"
"K
PK",0.9656862745098039,"250
500 RBF 5"
"K
PK",0.9666666666666667,"250
500"
"K
PK",0.9676470588235294,No Datapoints Dropped
"K
PK",0.9686274509803922,RBF 10
"K
PK",0.9696078431372549,"250
500 200 400 Layer"
"K
PK",0.9705882352941176,Linear
"K
PK",0.9715686274509804,"250
500"
"K
PK",0.9725490196078431,Cosine
"K
PK",0.9735294117647059,"250
500"
"K
PK",0.9745098039215686,RBF 0.2
"K
PK",0.9754901960784313,"250
500"
"K
PK",0.9764705882352941,RBF 0.5
"K
PK",0.9774509803921568,"250
500 RBF 1"
"K
PK",0.9784313725490196,"250
500 RBF 2"
"K
PK",0.9794117647058823,"250
500 RBF 5"
"K
PK",0.9803921568627451,"250
500"
"K
PK",0.9813725490196078,1% Most Dominant Datapoints Dropped
"K
PK",0.9823529411764705,RBF 10
"K
PK",0.9833333333333333,"250
500
Layer 200 400 Layer"
"K
PK",0.984313725490196,Linear
"K
PK",0.9852941176470589,"250
500
Layer"
"K
PK",0.9862745098039216,Cosine
"K
PK",0.9872549019607844,"250
500
Layer"
"K
PK",0.9882352941176471,RBF 0.2
"K
PK",0.9892156862745098,"250
500
Layer"
"K
PK",0.9901960784313726,RBF 0.5
"K
PK",0.9911764705882353,"250
500
Layer RBF 1"
"K
PK",0.9921568627450981,"250
500
Layer RBF 2"
"K
PK",0.9931372549019608,"250
500
Layer RBF 5"
"K
PK",0.9941176470588236,"250
500
Layer"
"K
PK",0.9950980392156863,10% Most Dominant Datapoints Dropped
"K
PK",0.996078431372549,ResNet-38 10×
"K
PK",0.9970588235294118,ResNet-164 1×
"K
PK",0.9980392156862745,RBF 10
"K
PK",0.9990196078431373,"Figure J.3: Removing dominant datapoints eliminates both blocks and differences among kernels in
CKA representational similarity heatmaps. We remove the top k% of examples according to the magnitudes
of their projections on the ﬁrst PC in layer 76 for ResNet-38 10× and layer 276 for ResNet-164 1×. Because
14/128 datapoints are dominant in the minibatch kernel matrix shown in Figure J.2, we provide results for
removing 20% of datapoints for that network, although they look only modestly different from the results
obtained by removing 10% of datapoints."
