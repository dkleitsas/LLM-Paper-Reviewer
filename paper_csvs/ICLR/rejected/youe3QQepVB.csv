Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002932551319648094,"Generative modeling has recently shown great promise in computer vision, but it
has mostly focused on synthesizing visually realistic images. In this paper, moti-
vated by multi-task learning of shareable feature representations, we consider a
novel problem of learning a shared generative model that is useful across various
visual perception tasks. Correspondingly, we propose a general multi-task oriented
generative modeling (MGM) framework, by coupling a discriminative multi-task
network with a generative network. While it is challenging to synthesize both RGB
images and pixel-level annotations in multi-task scenarios, our framework enables
us to use synthesized images paired with only weak annotations (i.e., image-level
scene labels) to facilitate multiple visual tasks. Experimental evaluation on chal-
lenging multi-task benchmarks, including NYUv2 and Taskonomy, demonstrates
that our MGM framework improves the performance of all the tasks by large mar-
gins, especially in the low-data regimes, and our model consistently outperforms
state-of-the-art multi-task approaches."
INTRODUCTION,0.005865102639296188,"1
INTRODUCTION
Seeing with the mind’s eye — creating internal images of objects and scenes not actually present
to the senses, is perhaps one of the hallmarks in human cognition (Pelaprat & Cole, 2011). For
humans, this visual imagination integrates learning experience and facilitates learning by solving
different problems (Egan, 2014; Pearson, 2019; Pelaprat & Cole, 2011; Egan, 1989). Inspired by
such ability, there has been increasing interest in building generative models that can synthesize
images (Goodfellow et al., 2014). Yet, most of the effort has focused on generating visually realistic
images (Brock et al., 2018; Zhang et al., 2019), which are still far from useful for machine perception
tasks (Shmelkov et al., 2018; Borji, 2019; Wu et al., 2016). Even though recent work has started
improving the “usefulness” of synthesized images, this line of investigation is often limited to a single
speciﬁc task (Nguyen-Phuoc et al., 2018; Sitzmann et al., 2019; Zhu et al., 2018; Souly et al., 2017).
Could we guide generative models to beneﬁt multiple visual tasks?"
INTRODUCTION,0.008797653958944282,"While similar spirits of shared feature representations have been widely studied as multi-task learning
or meta-learning (Finn et al., 2017; Zamir et al., 2018), here we take a different perspective — learning
a shared generative model across various tasks (as illustrated in Figure 1). Leveraging multiple tasks
allows us to capture the underlying image generation mechanism for more comprehensive object
and scene understanding than being done within individual tasks. Taking simultaneous semantic
segmentation, depth estimation, and surface normal prediction as an example (Figure 1), successful
generative modeling requires understanding not only the semantics but also the 3D geometric structure
and physical property of the input image. Meanwhile, a learned generative model facilitates the ﬂow
of knowledge across tasks, so that they beneﬁt one another. For instance, the synthesized images
provide meaningful variations in existing images and could work as additional training data to build
better task-speciﬁc models. These variations are especially critical when the data is limited."
INTRODUCTION,0.011730205278592375,"This paper thus explores multi-task oriented generative modeling (MGM), by coupling a discrim-
inative multi-task network with a generative network. To make them cooperate with each other, a
straightforward solution would be to synthesize both RGB images and corresponding pixel-level
annotations (e.g., pixel-wise class labels for semantic segmentation and depth map for normal esti-
mation). In the single task scenario, existing work trains a separate generative model to synthesize
paired pixel-level labeled data (Sandfort et al., 2019; Choi et al., 2019) and produce an augmented set.
However, these models are still highly task-dependant, and extending them to multi-task scenarios
becomes difﬁcult. A natural question then is: Do we actually need to synthesize paired image and
multi-annotation data to be useful for multi-task visual learning?"
INTRODUCTION,0.01466275659824047,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.017595307917888565,Multi-task Model
INTRODUCTION,0.020527859237536656,"Semantic
segmentation"
INTRODUCTION,0.02346041055718475,"Surface
Normal"
INTRODUCTION,0.026392961876832845,"Depth 
Estimation"
INTRODUCTION,0.02932551319648094,(Other Visual Tasks)
INTRODUCTION,0.03225806451612903,Multi-task Model
INTRODUCTION,0.03519061583577713,Generative Model
INTRODUCTION,0.03812316715542522,"(a) Traditional multi-task framework
(b) Multi-task oriented generative modeling …"
INTRODUCTION,0.04105571847507331,"Semantic
segmentation"
INTRODUCTION,0.04398826979472141,"Surface
Normal"
INTRODUCTION,0.0469208211143695,"Depth 
Estimation"
INTRODUCTION,0.04985337243401759,"(Other Visual Tasks)
…"
INTRODUCTION,0.05278592375366569,"Figure 1: Left: Traditional multi-task learning framework (that learns shared feature representations)
vs. Right: our proposed multi-task oriented generative modeling (that learns a shared generative
model across various visual perception tasks)"
INTRODUCTION,0.05571847507331378,"Our MGM addresses this question by proposing a general framework that uses synthesized images
paired with only weak annotations (i.e., image-level scene labels) to facilitate multiple visual tasks.
Our key insight is to introduce auxiliary discriminative tasks that (i) only require image-level
annotation or no annotation, and (ii) correlate with the original multiple tasks of interest. To this
end, as additional components of the discriminative multi-task network, we introduce a reﬁnement
network and a self-supervision network that satisﬁes these properties. Through joint training, the
discriminative network explicitly guides the image synthesis process. The generative network also
contributes to further reﬁning the shared feature representation. Meanwhile, the synthesized images
of the generative network are used as additional training data for the discriminative network."
INTRODUCTION,0.05865102639296188,"In more detail, the reﬁnement network performs scene classiﬁcation on the basis of the multi-
task network predictions, which requires only scene labels. The self-supervision network can be
operationalized on both real and synthesized images without reliance on annotations. With these
two modules, our MGM is able to learn from both (pixel-wise) fully-annotated real images and
synthesized (image-level) weakly labeled images. We instantiate MGM with the state-of-the-art
encoder-decoder based multi-task network (Zamir et al., 2018), self-attention GAN (Zhang et al.,
2019), and contrastive learning-based self-supervision network (Chen et al., 2020). Note that our
framework is agnostic to the choice of these model components."
INTRODUCTION,0.06158357771260997,"We evaluate our approach on standard multi-task benchmarks, including the NYUv2 (Nathan Sil-
berman & Fergus, 2012) and Taskonomy (Zamir et al., 2018) datasets. Consistent with the previous
works (Sun et al., 2019; Standley et al., 2020), we focus on three tasks of great practical importance:
semantic segmentation, depth estimation, and normal prediction. The evaluation shows that: (1) our
MGM consistently outperforms state-of-the-art multi-task approaches by large margins, especially
in the low-data regime. (2) With the increasing of generated samples, the performance of MGM
consistently improves and it also almost reaches the performance upper-bound that trains with weakly
annotated real images. (3) Our model can be extended to more visual tasks."
PILOT STUDY,0.06451612903225806,"2
PILOT STUDY"
PILOT STUDY,0.06744868035190615,"This pilot study provides an initial experimentation, which shows the importance of our proposed
problem (multi-task oriented generative modeling) and motivates the development of our method.
Speciﬁcally, we show that directly using images synthesized by an off-the-shelf generative model that
is trained with the realistic objective is not helpful for downstream pixel-level perception tasks."
PILOT STUDY,0.07038123167155426,"Experimental Design: For ease of analysis, here we focus on a single task – semantic segmentation,
and use the Tiny-Taskonomy dataset (Zamir et al., 2018). The dataset split and evaluation metric are
the same as our main experiments (See Section 4 for details). We train a self-attention GAN (Zhang
et al., 2019) on Tiny-Taskonomy, and use it to generate the same number of synthesized images as
the real images to augment the training set."
PILOT STUDY,0.07331378299120235,"How to generate pixel-level annotations? One remaining question is how to generate pixel-level
annotations for these synthesized images produced by the self-attention GAN. While prior work has
explored synthesizing both images and their pixel-level annotations for some speciﬁc tasks (Sandfort
et al., 2019; Choi et al., 2019), these annotations are still not reliable. For ease of analysis, in this
study, we factor out the effect of annotations and assume that we have an oracle annotator. We use
the annotator from Taskonomy (Zamir et al., 2018), which is a state-of-the-art large fully supervised
semantic segmentation network. In fact, the ground-truth of semantic segmentation on Taskonomy is
produced as the output of this network rather than labeled by humans. By doing so, we ensure that
the annotations of the synthesized images are “accurate”."
PILOT STUDY,0.07624633431085044,"Comparisons: Single-Task (ST) model is our baseline which follows the architecture of Taskonomy
single task network. ST is trained on real images only. STG is the ST model trained on the augmented
set. Table 1 reports the results of these two models, and STG is worse than ST."
PILOT STUDY,0.07917888563049853,"Under review as a conference paper at ICLR 2022 …
…
…"
PILOT STUDY,0.08211143695014662,“Dining Room” (Scene Label)
PILOT STUDY,0.08504398826979472,Self-supervised Label ~
PILOT STUDY,0.08797653958944282,Real Image
PILOT STUDY,0.09090909090909091,Synthesized Image
PILOT STUDY,0.093841642228739,Latent Input
PILOT STUDY,0.0967741935483871,"Image 
Generator"
PILOT STUDY,0.09970674486803519,Multi-task
PILOT STUDY,0.10263929618768329,Encoder
PILOT STUDY,0.10557184750733138,"Features
Multi-task Outputs
Decoders"
PILOT STUDY,0.10850439882697947,"Latent 
Encoder"
PILOT STUDY,0.11143695014662756,Multi-task Network:
PILOT STUDY,0.11436950146627566,Self-supervision Network
PILOT STUDY,0.11730205278592376,"Image Generation Network:
Refinement Network:"
PILOT STUDY,0.12023460410557185,Augmented
PILOT STUDY,0.12316715542521994,Images
PILOT STUDY,0.12609970674486803,"Shared
Features"
PILOT STUDY,0.12903225806451613,Combined
PILOT STUDY,0.13196480938416422,"Features 𝐌 𝐄 𝐄!""#$"
PILOT STUDY,0.1348973607038123,"𝐑
𝐆
Figure 2: Architecture of our proposed multi-task oriented generative modeling (MGM) framework.
There are four main components in the framework: Multi-task network to address the target multiple
pixel-level prediction tasks; self-supervision network to facilitate representation learning using images
without any annotation; reﬁnement network to perform scene classiﬁcation using weak annotation;
image generation network to synthesize useful images that beneﬁt multiple tasks."
PILOT STUDY,0.1378299120234604,"Model
ST
STG
mLoss (↓)
0.111
0.148"
PILOT STUDY,0.14076246334310852,"Table 1: Pilot experiment for semantic segmentation on the Tiny-Taskonomy dataset. Directly
using images synthesized by an off-the-shelf generative model (self-attention GAN) may hurt the
performance on the downstream task. ST: single-task semantic segmentation model trained on real
images only; STG: the same model trained on both real and synthesized images."
PILOT STUDY,0.1436950146627566,"Does synthesized images help downstream tasks? From Table 1, the answer is NO. Even though
the images are synthesized by one of the state-of-the-art generative models and are labeled by the
oracle annotator, they still cannot beneﬁt the downstream task. This is probably because these images
are synthesized off the shelf without “knowing” the downstream task. Our key insight then is that we
need to explicitly use the downstream task objective to guide the image synthesis process. Moreover,
here we focused on a single task and assumed that we had the oracle annotations. However, an oracle
annotator is difﬁcult to obtain in practice, especially for multiple tasks. Also, existing work cannot
synthesize paired images and pixel-level annotations for multiple tasks (Sandfort et al., 2019; Choi
et al., 2019). To overcome these challenges, in what follows we demonstrate how to facilitate multiple
visual tasks with synthesize images that only need image-level scene labels."
METHOD,0.1466275659824047,"3
METHOD
We propose multi-task oriented generative modeling (MGM) to leverage generative networks for
multi-task visual learning, as summarized in Figure 2. In this section, we ﬁrst formalize the novel
problem setting of MGM. Then, we explain the general framework and an instantiation of the MGM
model with state-of-the-art multi-task learning and image generation approaches. Finally, we discuss
the detailed training strategy for the framework."
PROBLEM SETTING,0.1495601173020528,"3.1
PROBLEM SETTING"
PROBLEM SETTING,0.15249266862170088,"Multi-task discriminative learning: Given a set of n visual tasks T = {T1, T2, · · · , Tn}, we aim
to learn a discriminative multi-task model M that is able to address all of these tasks simultaneously:
M(x) →by = (by1, by2, · · · , byn), where x is an input image and byi is the prediction for task Ti.
Here we focus on the type of per-pixel level prediction tasks (e.g., semantic segmentation or depth
estimation). We treat image classiﬁcation as a special task, which provides global semantic description
(i.e., scene labels) of images and only requires image-level category annotation c. Therefore, the set
of fully annotated real data is denoted as Sreal = {(xj, y1
j , y2
j , · · · , yn
j , cj)}."
PROBLEM SETTING,0.15542521994134897,"Generative learning: Meanwhile, we aim to learn a generative model G that produces a set of
synthesized data but with only corresponding image-level scene labels (weak annotation): G(c, z) →
ex, where z is a random input, and ex is a synthesized image. The scene label of ex is denoted as ec = c.
We denote the set of synthesized images and their corresponding scene labels as eSsyn = {(exk, eck)}."
PROBLEM SETTING,0.15835777126099707,"Cooperation between discriminative and generative learning: Our objective is that the discrimi-
native model M and the generative model G cooperate with each other to improve the performance"
PROBLEM SETTING,0.16129032258064516,Under review as a conference paper at ICLR 2022
PROBLEM SETTING,0.16422287390029325,"on the multiple visual tasks T . During the whole process, the full model only gets access to the real
fully-labeled data Sreal, then the generative network is trained to produce the synthesized set eSsyn.
Finally, M effectively learns from both Sreal and eSsyn. Note that, unlike most of the existing work
on image generation (Brock et al., 2018; Zhang et al., 2019), we do not focus on the visual realism of
the synthesized images ex. Instead, we hope G to capture the underlying mechanism that beneﬁts M."
FRAMEWORK AND ARCHITECTURE,0.16715542521994134,"3.2
FRAMEWORK AND ARCHITECTURE"
FRAMEWORK AND ARCHITECTURE,0.17008797653958943,"Figure 2 shows the architecture of our proposed MGM framework. It contains four components: the
main multi-task discriminative network M, the image generation network G, the reﬁnement network
R, and the self-supervision network. By introducing the reﬁnement network and the self-supervision
network, the full model can leverage both fully-labeled real images and weakly-labeled synthesized
images to facilitate the learning of latent feature representation. These two networks thus allow M
and R to better cooperate with each other. Notice that our MGM is a model-agnostic framework, and
here we instantiate its components with state-of-the-art models. In the ablation study (Sec. 4.3) and
the supplementary material, we show that our MGM works well with different choices of the model
components."
FRAMEWORK AND ARCHITECTURE,0.17302052785923755,"Multi-task Network (M): The multi-task network aims to make predictions for multiple target tasks
based on an input image. Consistent with the most recent work on multi-task learning, we instantiate
an encoder-decoder based architecture (Zamir et al., 2018; Zhang et al., 2019; Sun et al., 2019).
Considering the trade-off between model complexity and performance, we use a shared encoder
E to extract features from input images, and individual decoders for each target task. We adopt a
ResNet-18 (He et al., 2016) for the encoder and symmetric transposed decoders following Zamir et al.
(2018). For each task, we have its own loss function to update the corresponding decoder and the
shared encoder."
FRAMEWORK AND ARCHITECTURE,0.17595307917888564,"Image Generation Network (G): The generative model G is a variant of generative adversarial
networks (GANs). We include the generator in our framework, but this module also has a discriminator
during its own training. G takes as input a latent vector z and a category label c, and synthesizes
an image belonging to category c. Considering the trade-off between performance and training
cost, we instantiate G with self-attention generative adversarial network (SAGAN) (Zhang et al.,
2019). We achieve conditional image generation by applying conditional batch normalization (CBN)
layers (De Vries et al., 2017):"
FRAMEWORK AND ARCHITECTURE,0.17888563049853373,"CBN (fi,c,h,w | γc, βc) = γc
fi,c,w,h −E [f·,c,·,·]
p"
FRAMEWORK AND ARCHITECTURE,0.18181818181818182,"Var [f·,c,·,·] + ϵ
+ βc,
(1)"
FRAMEWORK AND ARCHITECTURE,0.18475073313782991,"where fi,c,h,w is an extracted c-channel 2D feature for the i-th sample, and ϵ is a small value to avoid
collapse. γc and βc are two parameters to control the mean and variance of the normalization, which
are learned by the model for each class. We use hinge loss for the adversarial training. Notice that the
proposed framework is ﬂexible with different generative models, and we also show the effectiveness
of using DCGAN (Radford et al., 2015) in the supplementary."
FRAMEWORK AND ARCHITECTURE,0.187683284457478,"Reﬁnement Network (R): As one of our key contributions, we introduce the reﬁnement network
R to further reﬁne the shared representation using the global scene category labels. R takes the
predictions of the multi-task network as input and predicts the category label of the input image.
Importantly, because R only requires category labels, it can be effortlessly operationalized on the
“weakly-annotated” synthesized images. Meanwhile, R also enforces the semantic consistency of the
synthesized images with G."
FRAMEWORK AND ARCHITECTURE,0.1906158357771261,"We apply an algorithm inspired by Expectation-Maximum (EM) (Dempster et al., 1977) to train the
reﬁnement network R. For the fully-annotated real images (x, y, c), we use the scene classiﬁcation
loss to update R and reﬁne the encoder E in the multi-task network M. Then for the synthesized
images (ex, ec), since their multi-task predictions produced by M might not be reliable, we only reﬁne
E with R frozen using the scene classiﬁcation loss. Through reﬁning the share feature representation
with the synthesized images, this process also provides implicit guidance to the image generation
network."
FRAMEWORK AND ARCHITECTURE,0.1935483870967742,"More speciﬁcally, we model the whole multi-task network and reﬁnement network as a joint proba-
bility graph:"
FRAMEWORK AND ARCHITECTURE,0.19648093841642228,"P(x, y, c; θ, θ′) = P(x) n
Y"
FRAMEWORK AND ARCHITECTURE,0.19941348973607037,"i=1
P
 
yi | x; θ

!"
FRAMEWORK AND ARCHITECTURE,0.20234604105571846,"P(c | y; θ′),
(2)"
FRAMEWORK AND ARCHITECTURE,0.20527859237536658,Under review as a conference paper at ICLR 2022
FRAMEWORK AND ARCHITECTURE,0.20821114369501467,Multi-task
FRAMEWORK AND ARCHITECTURE,0.21114369501466276,Encoder
FRAMEWORK AND ARCHITECTURE,0.21407624633431085,Embedding
FRAMEWORK AND ARCHITECTURE,0.21700879765395895,"Layer
Generator"
FRAMEWORK AND ARCHITECTURE,0.21994134897360704,"feature
noise"
FRAMEWORK AND ARCHITECTURE,0.22287390029325513,Guidance
FRAMEWORK AND ARCHITECTURE,0.22580645161290322,Refinement
FRAMEWORK AND ARCHITECTURE,0.2287390029325513,"(𝐄)
(𝐆)"
FRAMEWORK AND ARCHITECTURE,0.2316715542521994,"Figure 3: Joint training of the multi-task network and the image generation network. The multi-task
network provides useful feature representation to guide the image generation process, while the
generation network reﬁnes the shared representation through back-propagation.
Algorithm 1: The training procedure of MGM
Initialization:
max epoch: Maximum epoch for the training;
M: Multi-task Network, G: Image Generation Network;
E: Multi-task Encoder, R: the Reﬁnement Network;
Eself: Self-supervision Network Encoder;
N: minibatch size;
for epoch ←1 to max epoch do"
FRAMEWORK AND ARCHITECTURE,0.23460410557184752,"Split Sreal into minibatches with size N: Smini;
for (x, y, c) ∈Smini do"
FRAMEWORK AND ARCHITECTURE,0.2375366568914956,"ˆy = M(x)
Lmulti(y, ˆy) →update M;
ˆc = R(ˆy)
LCE(c, ˆc) →update R, E;
Sample 2N augmented images xaug
LNT −Xent(xaug) →update E, Eself;
Use LGAN to train G;
(ex, ec) = G(x, c)
LCE(ec, R(M(ex))) →update E
Sample 2N augmented images for the synthesized data exaug
LNT −Xent(exaug) →update E.
end
end"
FRAMEWORK AND ARCHITECTURE,0.2404692082111437,"where x is an input image, y is the vector of multi-task predictions, c is the scene label, θ is the vector
of parameters of the multi-task network, and θ′ is the vector of parameters of the reﬁnement network.
The parameters θ and θ′ are learned to maximize the joint probability. For data samples in Sreal, we
maximize the joint probability and update θ′ to train the reﬁnement network."
FRAMEWORK AND ARCHITECTURE,0.2434017595307918,"θ′⋆= argmax
θ′
P(eck | y; θ′).
(3)"
FRAMEWORK AND ARCHITECTURE,0.24633431085043989,"For data samples in eSsyn, we update the parameters of M (θ) in an EM-like manner. During the E
step, we estimate the latent multi-task ground-truth by:"
FRAMEWORK AND ARCHITECTURE,0.24926686217008798,"y† = argmax
y
P (y | exk; θ) P(eck | y; θ′).
(4)"
FRAMEWORK AND ARCHITECTURE,0.25219941348973607,"Then for the M step, we back-propagate the error between y† and by (the multi-task predictions) to
the multi-task encoder.
θ⋆= argmax
θ
P
 
y† | exk; θ

.
(5)"
FRAMEWORK AND ARCHITECTURE,0.25513196480938416,We use cross-entropy as the classiﬁcation loss function.
FRAMEWORK AND ARCHITECTURE,0.25806451612903225,"Self-supervision Network: The self-supervision network facilitates the representation learning of
the encoder E by performing self-supervised learning tasks on images without any annotation so that
can be operationalized on both real and synthesized images. We modify SimCLR (Chen et al., 2020),
one of the state-of-the-art approaches, as our self-supervision network."
FRAMEWORK AND ARCHITECTURE,0.26099706744868034,"This network contains an additional embedding network Eself, working on the output of the multi-task
encoder E, to obtain a 1D latent feature of the input image: µ = Eself(E(x)). Then, it performs
contrastive learning with these latent vectors. Speciﬁcally, given a minibatch of N images, this
network ﬁrst randomly samples two transformed views of each source image as augmented images
(See supplementary material for the detailed transformations), resulting in 2N augmented images.
For each augmented image, there is only one pair of positive augmented examples from the same
source image, and other 2(N −1) negative pairs. Then the network jointly minimizes the distance of
positive pairs and maximizes the distance of negative pairs in the latent space, through the normalized"
FRAMEWORK AND ARCHITECTURE,0.26392961876832843,Under review as a conference paper at ICLR 2022
FRAMEWORK AND ARCHITECTURE,0.2668621700879765,"Data Setting
100% Data Setting
50% Data Setting
25% Data Setting
Models
ST
MT
MGM
ST
MT
MGM
MGMr
ST
MT
MGM
MGMr"
FRAMEWORK AND ARCHITECTURE,0.2697947214076246,"NYU
v2"
FRAMEWORK AND ARCHITECTURE,0.2727272727272727,"SS-mIOU(↑)
0.249
± 0.008
0.256
± 0.005
0.264
± 0.005"
FRAMEWORK AND ARCHITECTURE,0.2756598240469208,"0.230
± 0.009
0.237
± 0.006
0.251
± 0.005"
FRAMEWORK AND ARCHITECTURE,0.2785923753665689,"0.258
± 0.004"
FRAMEWORK AND ARCHITECTURE,0.28152492668621704,"0.199
± 0.004
0.207
± 0.007
0.229
± 0.004"
FRAMEWORK AND ARCHITECTURE,0.2844574780058651,"0.231
± 0.005"
FRAMEWORK AND ARCHITECTURE,0.2873900293255132,"DE-mABSE(↓)
0.748
± 0.019
0.708
± 0.021
0.698
± 0.014"
FRAMEWORK AND ARCHITECTURE,0.2903225806451613,"0.837
± 0.017
0.819
± 0.018
0.734
± 0.011"
FRAMEWORK AND ARCHITECTURE,0.2932551319648094,"0.723
± 0.010"
FRAMEWORK AND ARCHITECTURE,0.2961876832844575,"0.908
± 0.017
0.874
± 0.015
0.844
± 0.011"
FRAMEWORK AND ARCHITECTURE,0.2991202346041056,"0.821
± 0.009"
FRAMEWORK AND ARCHITECTURE,0.3020527859237537,"SN-mAD(↓)
0.273
± 0.06
0.283
± 0.008
0.255
± 0.010"
FRAMEWORK AND ARCHITECTURE,0.30498533724340177,"0.309
± 0.008
0.291
± 0.010
0.273
± 0.009"
FRAMEWORK AND ARCHITECTURE,0.30791788856304986,"0.270
± 0.006"
FRAMEWORK AND ARCHITECTURE,0.31085043988269795,"0.312
± 0.007
0.296
± 0.007
0.277
± 0.006"
FRAMEWORK AND ARCHITECTURE,0.31378299120234604,"0.274
± 0.005"
FRAMEWORK AND ARCHITECTURE,0.31671554252199413,"Tiny
Task-
onomy"
FRAMEWORK AND ARCHITECTURE,0.3196480938416422,"SS-mLoss(↓)
0.111
± 0.002
0.137
± 0.003
0.106
± 0.003"
FRAMEWORK AND ARCHITECTURE,0.3225806451612903,"0.120
± 0.003
0.138
± 0.002
0.114
± 0.003"
FRAMEWORK AND ARCHITECTURE,0.3255131964809384,"0.112
± 0.002"
FRAMEWORK AND ARCHITECTURE,0.3284457478005865,"0.119
± 0.003
0.141
± 0.002
0.117
± 0.002"
FRAMEWORK AND ARCHITECTURE,0.3313782991202346,"0.115
± 0.002"
FRAMEWORK AND ARCHITECTURE,0.3343108504398827,"DE-mLoss(↓)
1.716
± 0.006
1.584
± 0.008
1.472
± 0.006"
FRAMEWORK AND ARCHITECTURE,0.33724340175953077,"1.768
± 0.007
1.595
± 0.009
1.499
± 0.008"
FRAMEWORK AND ARCHITECTURE,0.34017595307917886,"1.378
± 0.007"
FRAMEWORK AND ARCHITECTURE,0.34310850439882695,"1.795
± 0.010
1.692
± 0.008
1.585
± 0.009"
FRAMEWORK AND ARCHITECTURE,0.3460410557184751,"1.580
± 0.008"
FRAMEWORK AND ARCHITECTURE,0.3489736070381232,"SN-mLoss(↓)
0.155
± 0.003
0.153
± 0.003
0.145
± 0.002"
FRAMEWORK AND ARCHITECTURE,0.3519061583577713,"0.157
± 0.002
0.156
± 0.002
0.147
± 0.002"
FRAMEWORK AND ARCHITECTURE,0.3548387096774194,"0.140
± 0.001"
FRAMEWORK AND ARCHITECTURE,0.35777126099706746,"0.154
± 0.002
0.152
± 0.002
0.148
± 0.003"
FRAMEWORK AND ARCHITECTURE,0.36070381231671556,"0.142
± 0.002
Table 2: Main results (mean ± std) on the NYUv2 and Tiny-Taskonomy datasets. SS: semantic
segmentation; DE: depth estimation; SN: surface normal prediction. ↑means higher is better; ↓
means lower is better. We use different metrics on the two datasets, following existing protocol. Our
MGM consistently and signiﬁcantly outperforms both single-task (ST) and multi-task (MT) baselines,
even reaching the performance upper-bound of training with weakly annotated real images (MGMr)."
FRAMEWORK AND ARCHITECTURE,0.36363636363636365,"Data Setting
100% Data Setting
50% Data Setting
25% Data Setting
Models
MGM/G
MGM/j
MGM
MGM/G
MGM/j
MGM
MGM/G
MGM/j
MGM"
FRAMEWORK AND ARCHITECTURE,0.36656891495601174,"NYU
v2"
FRAMEWORK AND ARCHITECTURE,0.36950146627565983,"SS-mIOU(↑)
0.261
0.262
0.264
0.243
0.243
0.251
0.215
0.220
0.229
DE-mABSE(↓)
0.707
0.701
0.698
0.799
0.763
0.734
0.868
0.860
0.844
SN-mAD(↓)
0.262
0.259
0.255
0.287
0.281
0.273
0.292
0.286
0.277
Tiny
Task-
onomy"
FRAMEWORK AND ARCHITECTURE,0.3724340175953079,"SS-mLoss(↓)
0.108
0.108
0.106
0.116
0.115
0.114
0.119
0.121
0.117
DE-mLoss(↓)
1.491
1.488
1.472
1.527
1.523
1.499
1.636
1.616
1.585
SN-mLoss(↓)
0.151
0.151
0.145
0.153
0.152
0.147
0.154
0.152
0.148
Table 3: Comparison of our MGM model with its variants. MGM/G: without generating synthesized
images; MGM/j: without joint learning. Our MGM outperforms single-task and multi-task baselines
even without synthesized data, showing its effectiveness as a general multi-task learning framework.
The model performance further improves with joint learning."
FRAMEWORK AND ARCHITECTURE,0.375366568914956,"temperature-scaled cross-entropy (NT-Xent)) loss (Chen et al., 2020):"
FRAMEWORK AND ARCHITECTURE,0.3782991202346041,"ℓi,j = −log
exp (dis (µi, µj) /τ)
P2N
k=1 1[k̸=i] exp (dis (µi, µk) /τ)
,
(6)"
FRAMEWORK AND ARCHITECTURE,0.3812316715542522,"where ℓi,j is the NT-Xent loss for a positive pair of examples in the latent space (µi, µj). 1[k̸=i] ∈0, 1
is an indicator function evaluating to 1 if k ̸= i, and τ is a temperature parameter. dis (µi, µj) is
a distance function, and we use cosine distance following Chen et al. (2020). This loss is further
back-propagated to reﬁne the multi-task encoder E. Notice that other types of self-supervised tasks
are applicable as well. To demonstrate this, in Sec. 4.3 we also report the result with another task —
image reconstruction."
INTERACTION AMONG NETWORKS,0.3841642228739003,"3.3
INTERACTION AMONG NETWORKS"
INTERACTION AMONG NETWORKS,0.3870967741935484,"Cooperation Through Joint Training: We propose a simple but effective joint training algorithm
shown in Figure 3. The image generation network G takes the transferred feature representation of
the multi-task encoder E, added with some Gaussian noise, as the latent input z to conduct conditional
image generation. Hence, the generation network obtains additional, explicit guidance (i.e., extra
effective features) from the multi-task network to facilitate the generation of “better images”— images
that may not look more realistic but are more useful for the multiple target tasks. Then, the generation
error of G will be back-propagated to E to further reﬁne the shared representation. This process
can be also viewed as introducing image generation as an additional task in the multi-task learning
framework."
INTERACTION AMONG NETWORKS,0.39002932551319647,"Training Procedure: We describe the procedure in Algorithm 1 and further explain it in the supple-
mentary material."
EXPERIMENTS,0.39296187683284456,"4
EXPERIMENTS
To evaluate our proposed MGM model and investigate the impact of each component, we conduct
a variety of experiments on two standard multi-task learning datasets. We also perform detailed
analysis and ablation studies."
DATASETS AND COMPARED METHODS,0.39589442815249265,"4.1
DATASETS AND COMPARED METHODS"
DATASETS AND COMPARED METHODS,0.39882697947214074,"Datasets: Following the work of Sun et al. (2019) and Standley et al. (2020), we mainly focus
on three representative visual tasks in the main experiments: semantic segmentation (SS), surface
normal prediction (SN), and depth estimation (DE). At the end of this section, we will show that our
approach is scalable to an additional number of tasks. We evaluate all the models on two widely-"
DATASETS AND COMPARED METHODS,0.40175953079178883,Under review as a conference paper at ICLR 2022 ST MT
DATASETS AND COMPARED METHODS,0.4046920821114369,"MGM
(Ours)"
DATASETS AND COMPARED METHODS,0.40762463343108507,Ground Truth
DATASETS AND COMPARED METHODS,0.41055718475073316,"50%
SS and Error"
DATASETS AND COMPARED METHODS,0.41348973607038125,"50%
DE and Error"
DATASETS AND COMPARED METHODS,0.41642228739002934,"50%
SN and Error ST MT"
DATASETS AND COMPARED METHODS,0.41935483870967744,"MGM
(Ours)"
DATASETS AND COMPARED METHODS,0.4222873900293255,Ground Truth
DATASETS AND COMPARED METHODS,0.4252199413489736,"50%
SS and Error"
DATASETS AND COMPARED METHODS,0.4281524926686217,"50%
DE and Error"
DATASETS AND COMPARED METHODS,0.4310850439882698,"50%
SN and Error
Test Images"
DATASETS AND COMPARED METHODS,0.4340175953079179,"Figure 4: Visualization and error comparison of the multi-task prediction outputs in the 50% data
setting. The prediction results of MGM is quite close to the ground-truth, signiﬁcantly outperforming
the state-of-the-art results."
DATASETS AND COMPARED METHODS,0.436950146627566,"benchmarked datasets: NYUv2 (Nathan Silberman & Fergus, 2012; Eigen & Fergus, 2015) and
Tiny-Taskonomy (Zamir et al., 2018). See Sec. B in the appendix for more details."
DATASETS AND COMPARED METHODS,0.4398826979472141,"Compared Methods: We mainly focus on our comparison with two state-of-the-art discriminative
baselines: Single-Task (ST) model follows the architecture of Taskonomy single task network (Zamir
et al., 2018), and address each task individually; Multi-Task (MT) model refers to the sub-network
for the three tasks of interest in Standley et al. (2020). These two baselines can be viewed as using
our multi-task network without the proposed reﬁnement, self-supervision, and generation networks.
Note that our work is the ﬁrst that introduces generative modeling for multi-task learning, and there
is no existing baseline in this direction."
DATASETS AND COMPARED METHODS,0.44281524926686217,"Our MGM is the full model trained with both fully-labeled real data and weakly-labeled synthesized
data, which are produced by the generation network through joint training. In addition, to further
validate the effectiveness of our MGM model, we consider its variant model MGMr that is trained
with both fully and weakly labeled real data. MGMr is used to show the performance upper bound in
the semi-supervised learning scenario, where the synthesized images are replaced by the real images
in the dataset. The resolution is set to 128 for all the experiments. For all the compared methods, we
use a ResNet-18 like architecture to build the encoder and use the standard decoder architecture of
Taskonomy (Zamir et al., 2018)."
DATASETS AND COMPARED METHODS,0.44574780058651026,"Data Settings: We conduct experiments with three different data settings: (1) 100% data setting;
(2) 50% data setting; and (3) 25% data setting. For each setting, we use 100%, 50%, or 25% of the
entire labeled training set to train the model. For MGMr, we add another 50% or 25% of weakly-
labeled real data in the last two settings. For MGM, we include the same number of weakly-labeled
synthesized data in all three settings."
DATASETS AND COMPARED METHODS,0.44868035190615835,"Evaluation Metrics: For NYUv2, following the metrics in Eigen & Fergus (2015); Sun et al. (2019),
we measure the mean Intersection-Over-Union (mIOU) for the semantic segmentation task, the mean
Absolute Error (mABSE) for the depth estimation task, and the mean Angular Distance (mAD) for the
surface normal estimation task. For Tiny-Taskonomy, we follow the evaluation metrics of previous
work (Zamir et al., 2018; Standley et al., 2020; Sun et al., 2019) and report the averaged loss values
on the test set."
DATASETS AND COMPARED METHODS,0.45161290322580644,"Implementation Details: See appendix Sec. A for the training details and the sensitivity of the
hyper-parameters."
MAIN RESULTS,0.45454545454545453,"4.2
MAIN RESULTS"
MAIN RESULTS,0.4574780058651026,"Quantitative Results: We run all the models for 5 times and report the averaged results and the
standard deviation on the two datasets in Table 2. From this table, we have the following key
observations that support the effectiveness of our approach. (1) Existing discriminative multi-task
learning approaches may not consistently beneﬁt all the three individual tasks. However, our MGM
consistently and signiﬁcantly outperforms both the single-task and multi-task baselines across all
the scenarios. (2) By using weakly-labeled synthesized data, the results of our model in the 50%
data setting are even better than those of baselines in the 100% data setting. (3) More interesting,
the performance of our MGM is close to MGMr, which indicates that our synthesized images are
comparably useful as real images for improving multiple visual perception tasks. (4) The performance
gap between the two models is especially minimal in the 25% labeled data setting, suggesting that
our proposed MGM model is, in particular, helpful with limited data.
Qualitative Results: We also visualize the prediction results on the three tasks for ST, MT, and
MGM in the 50% data setting in Figure 4. While obvious defects can be found for all the baselines,
the results of our proposed method are quite close to the ground-truth.
How Does Generative Modeling Beneﬁt Multi-tasks? To have a better understanding of how
the generative modeling and joint learning mechanism beneﬁt multi-task visual learning, we also"
MAIN RESULTS,0.4604105571847507,Under review as a conference paper at ICLR 2022
MAIN RESULTS,0.4633431085043988,"Model
SS-mIOU (↑)
DE-mABSE (↓)
SN-mAD (↓)
MGM/self
0.239
0.776
0.279
MGM/reﬁne
0.254
0.808
0.290
MGMrecon
0.241
0.768
0.285
MGM
0.251
0.734
0.273
Table 4: Ablation study. (1) MGM/self: without self-supervision task; (2) MGM/reﬁne: without
classiﬁcation reﬁnement network; and (3) MGMrecon: with a simple reconstruction task as self-
supervision. The two proposed components are complementary and both beneﬁt the multiple tasks.
The reﬁnement network works better for surface normal; the self-supervision network works better
for semantic segmentation. Their combination achieves the best.
consider two variants of our MGM model and evaluate their performance. MGM/G is the MGM
model trained with Sreal only (without generative modeling), which shows the performance of our
proposed multi-task learning framework in general (with the help from the auxiliary reﬁnement
and self-supervision networks), and helps to understand the gain of leveraging generative modeling.
MGM/j is trained with synthesized images produced by a pre-trained SAGAN without the joint
training mechanism. Table 3 shows the results on the two datasets."
MAIN RESULTS,0.4662756598240469,"Combining the results of Tables 3 and 2, we ﬁnd: (1) MGM outperforms both ST and MT baseline
even without generative modeling, indicating the beneﬁt of the self-supervised task and the reﬁne-
ment network. (2) By introducing synthesized images that are trained separately, the multi-task
performance slightly improves, which shows the effectiveness of involving generative modeling into
multi-tasks, under the assistance of our reﬁnement and self-supervision networks. (3) The joint learn-
ing mechanism further improves the cooperation between generative modeling and discriminative
learning, thus enabling the generative model to better facilitate multi-task visual learning."
ABLATION STUDY,0.46920821114369504,"4.3
ABLATION STUDY"
ABLATION STUDY,0.47214076246334313,"For all the experiments in this section, models are trained in the 50% data setting, unless speciﬁcally
mentioned."
ABLATION STUDY,0.4750733137829912,"Impact of Self-supervision Task and Reﬁnement Network: Two important components of the
proposed framework are the self-supervision task and the reﬁnement network. We evaluate their
impact individually in Table 4. MGM/self is the model trained without the self-supervision task;
MGM/reﬁne is the model without the reﬁnement network; for MGMrecon, we replace the SimCLR
based self-supervision method with a weaker reconstruction task, and use Mean Square Error as the
loss function."
ABLATION STUDY,0.4780058651026393,"We could see that the reﬁnement network works better for the surface normal task, and the self-
supervision task works better for the semantic segmentation task; they are complementary to each
other, and combining them generally achieves the best performance. In addition, the model could still
gain some beneﬁt even when we use some weak self-supervision tasks like reconstruction, which
indicates the generalizability and robustness of our MGM model.
Number of Synthesized Images vs. Real images: From the previous results, we have found that
the synthesized images could beneﬁt the target multi-tasks in a way similar to weakly labeled real
images. To further investigate the impact of the number of synthesized images, we vary if from 25%
to 125% during multi-task training on NYUv2 in the 25% real data setting. Figure 5 summarizes
the result. First, we can see that the performance gap between MGM/j (without joint training) and
MGM becomes larger for a higher ratio of weakly labeled data, which indicates the importance of
our joint learning mechanism. More importantly, while the real images are constrained in number
due to the human collection effort, our generation network is able to synthesize unlimited amounts
of images. This is demonstrated in the comparison between MGMr (with real images) and MGM:
the performance of our MGM keeps improving with respect to the number of synthesized images,
achieving results almost comparable to that of MGMr when MGMr uses all the available weakly
labeled real images."
ABLATION STUDY,0.4809384164222874,"We also provide ablation studies on the generalizability, impact of parameters, and the resolution of
images in Sec. D in the appendix."
EXTENSION,0.4838709677419355,"4.4
EXTENSION"
EXTENSION,0.4868035190615836,"Experiments with More Tasks: MGM is also ﬂexible and scalable with different tasks. In addition
to the three tasks addressed in the main experiments, here we add three extra tasks: Edge Texture
(ET), Reshading (Re), and Principal Curvature (PC), leading to six tasks in total. We evaluate the
performance of all the compared models on Tiny-taskonomy in the 50% data setting, and report the
mean test loss for all the tasks. The result is reported in Table 5. Again, our proposed method still
outperforms state-of-the-art baselines."
EXTENSION,0.4897360703812317,Under review as a conference paper at ICLR 2022
EXTENSION,0.49266862170087977,"Figure 5: Performance change with different ratios of weakly labeled data. Joint learning signiﬁcantly
improves the performance. The performance of MGM keeps increasing with the number of the
weakly labeled synthesized images, achieving results almost comparable to that of MGMr trained
with all the available weakly labeled real images."
EXTENSION,0.49560117302052786,"Model
SS (↓)
DE (↓)
SN (↓)
ET (↓)
Re (↓)
PC (↓)
ST
0.120
1.768
0.157
0.228
0.703
0.462
MT
0.112
1.747
0.169
0.241
0.704
0.436
MGM
0.108
1.715
0.152
0.201
0.699
0.417
Table 5: Mean test losses for six tasks on Tiny-Taskonomy. Again, our MGM outperforms the
baselines, indicating its ﬂexibility, generability, and scalability.
5
RELATED WORK
Multi-task Learning and Task Relationship: Multi-task learning (MTL) aims to leverage informa-
tion coming from signals of related tasks so that each individual task can gain beneﬁt (Doersch &
Zisserman, 2017). Ruder (2017) identiﬁes that most recent works use two clusters of strategies for
MTL: hard parameter sharing techniques (Kokkinos, 2017; Doersch & Zisserman, 2017; Pentina &
Lampert, 2017) and soft parameter sharing techniques (Misra et al., 2016; Sener & Koltun, 2018;
Chen et al., 2018b). These strategies have achieved good performance for MTL with similar tasks.
Researchers have also carefully studied the task relationships among different tasks to make the
best cooperations among them. Taskonomy exploits the relationships among various visual tasks
to beneﬁt the transfer or multi-task learning (Zamir et al., 2018). Standley et al. (2020) considers
task cooperation and competition, and proposes a method to assign tasks to a few neural networks to
balance all of them. Some other following works also explores task relationships among different
types of tasks (Sun et al., 2019; Zamir et al., 2020; Armeni et al., 2019; Pal & Balasubramanian,
2019; Yeo et al., 2021; Wallace et al., 2021). These works only consider MTL with discriminative
tasks. In comparison, we ﬁrst introduce generative modeling to multi-task visual learning."
EXTENSION,0.49853372434017595,"Generative Modeling for Visual Learning: Besides the initial goal of synthesizing realistic images,
some recent work has explored the potential to leverage generative models to synthesize “usefull”
images for other visual tasks (Shorten & Khoshgoftaar, 2019). The most straightforward way is
to generate images and the corresponding annotations as data augmentation for the target visual
task (Bao et al., 2021; Sandfort et al., 2019; Choi et al., 2019; Wang et al., 2018). Another strategy to
leverage generative models is through well-designed error feedback or adversarial training (Luc et al.,
2016; CS Kumar et al., 2018; Mustikovela et al., 2020). There have been works that apply generative
models for different visual tasks including classiﬁcation (Zhan et al., 2018; Frid-Adar et al., 2018;
Zhu et al., 2018), semantic segmentation (Luc et al., 2016) and depth estimation (Pilzer et al., 2018;
Aleotti et al., 2018). Among them, Souly et al. (2017) also propose a semi-supervised framework to
leverage generative models for semantic segmentation through adversarial training. Different from
these methods, MGM is applicable to various multiple visual tasks and different generative networks."
EXTENSION,0.501466275659824,"Reduced-Supervision Methods: Recent works take advantage of weakly labeled data by assigning
some self-created labels (e.g.colorization, rotation, reconstruction) (Noroozi & Favaro, 2016; Noroozi
et al., 2017; Chen et al., 2020; Dosovitskiy et al., 2014; Pathak et al., 2016). Similar self-supervised
techniques have been proved useful for multi-task learning (Liu et al., 2008; Ren & Jae Lee, 2018;
Doersch & Zisserman, 2017; Lee et al., 2019). Among these techniques, a famous one is the
Expectation-Maximization (EM) algorithm (Dempster et al., 1977; Papandreou et al., 2015), which
leverages the information of weakly or unlabelled data by iteratively estimating and reﬁning their
labels. We adopt a similar spirit and introduce the reﬁnement network for MGM framework."
CONCLUSION,0.5043988269794721,"6
CONCLUSION
This paper proposes to introduce generative modeling for multi-task visual learning. The main chal-
lenge is that current generative models almost cannot synthesize both RGB images and pixel-level
annotations in multi-task scenarios. We address this problem by proposing multi-task oriented gener-
ative modeling (MGM) framework equipped with the self-supervision network and the reﬁnement
network, which enable us to take advantage of synthesized images paired with image-level scene
labels to facilitate multiple visual tasks. Experimental results indicate our MGM model consistently
outperforms state-of-the-art multi-task approaches."
CONCLUSION,0.5073313782991202,Under review as a conference paper at ICLR 2022
REFERENCES,0.5102639296187683,REFERENCES
REFERENCES,0.5131964809384164,"Filippo Aleotti, Fabio Tosi, Matteo Poggi, and Stefano Mattoccia. Generative adversarial networks
for unsupervised monocular depth prediction. In ECCV Workshops, 2018. 9"
REFERENCES,0.5161290322580645,"Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R Zamir, Martin Fischer, Jitendra Malik, and Silvio
Savarese. 3d scene graph: A structure for uniﬁed semantics, 3d space, and camera. In ICCV, 2019.
9"
REFERENCES,0.5190615835777126,"Zhipeng Bao, Yu-Xiong Wang, and Martial Hebert. Bowtie networks: Generative modeling for joint
few-shot recognition and novel-view synthesis. ICLR, 2021. 9"
REFERENCES,0.5219941348973607,"Ali Borji. Pros and cons of gan evaluation measures. Computer Vision and Image Understanding,
2019. 1"
REFERENCES,0.5249266862170088,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural
image synthesis. arXiv preprint arXiv:1809.11096, 2018. 1, 4, 17"
REFERENCES,0.5278592375366569,"Jun-Cheng Chen, Wei-An Lin, Jingxiao Zheng, and Rama Chellappa. A real-time multi-task single
shot face detector. In ICIP, 2018a. 18"
REFERENCES,0.530791788856305,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020. 2, 5, 6, 9,
14"
REFERENCES,0.533724340175953,"Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient
normalization for adaptive loss balancing in deep multitask networks. In ICML, 2018b. 9"
REFERENCES,0.5366568914956011,"Sumanth Chennupati, Ganesh Sistu, Senthil Yogamani, and Samir A Rawashdeh. Multinet++:
Multi-stream feature aggregation and geometric loss strategy for multi-task learning. In CVPR
Workshops, 2019. 18"
REFERENCES,0.5395894428152492,"Jaehoon Choi, Taekyung Kim, and Changick Kim. Self-ensembling with gan-based data augmentation
for domain adaptation in semantic segmentation. In ICCV, 2019. 1, 2, 3, 9"
REFERENCES,0.5425219941348973,"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban
scene understanding. In CVPR, 2016. 18"
REFERENCES,0.5454545454545454,"Arun CS Kumar, Suchendra M Bhandarkar, and Mukta Prasad. Monocular depth prediction using
generative adversarial networks. In CVPRW, 2018. 9"
REFERENCES,0.5483870967741935,"Harm De Vries, Florian Strub, J´er´emie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C
Courville. Modulating early visual processing by language. In NeurIPS, 2017. 4, 14"
REFERENCES,0.5513196480938416,"Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 1977. 4,
9"
REFERENCES,0.5542521994134897,"Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. In ICCV, 2017. 9"
REFERENCES,0.5571847507331378,"Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative
unsupervised feature learning with convolutional neural networks. In NeurIPS, 2014. 9"
REFERENCES,0.5601173020527859,"Kieran Egan. Memory, imagination, and learning: Connected by the story. Phi Delta Kappan, 1989. 1"
REFERENCES,0.5630498533724341,"Kieran Egan. Imagination in teaching and learning: The middle school years. University of Chicago
Press, 2014. 1"
REFERENCES,0.5659824046920822,"David Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with a common
multi-scale convolutional architecture. In ICCV, 2015. 7, 15"
REFERENCES,0.5689149560117303,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In ICML, 2017. 1"
REFERENCES,0.5718475073313783,Under review as a conference paper at ICLR 2022
REFERENCES,0.5747800586510264,"Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit Greenspan.
Gan-based synthetic medical image augmentation for increased cnn performance in liver lesion
classiﬁcation. Neurocomputing, 2018. 9"
REFERENCES,0.5777126099706745,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 1"
REFERENCES,0.5806451612903226,"Saurabh Gupta, Pablo Arbelaez, and Jitendra Malik. Perceptual organization and recognition of
indoor scenes from rgb-d images. In CVPR, 2013. 15"
REFERENCES,0.5835777126099707,"Hu Han, Anil K Jain, Fang Wang, Shiguang Shan, and Xilin Chen. Heterogeneous face attribute
estimation: A deep multi-task learning approach. PAMI, 2017. 18"
REFERENCES,0.5865102639296188,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016. 4, 14"
REFERENCES,0.5894428152492669,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014. 14"
REFERENCES,0.592375366568915,"Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-, and
high-level vision using diverse datasets and limited memory. In CVPR, 2017. 9"
REFERENCES,0.5953079178885631,"Wonhee Lee, Joonil Na, and Gunhee Kim. Multi-task self-supervised object detection via recycling
of bounding box annotations. In CVPR, 2019. 9"
REFERENCES,0.5982404692082112,"Jianshu Li, Jian Zhao, Fang Zhao, Hao Liu, Jing Li, Shengmei Shen, Jiashi Feng, and Terence Sim.
Robust face recognition with deep multi-view representation learning. In International Conference
on Multimedia, 2016. 18"
REFERENCES,0.6011730205278593,"Jianshu Li, Pan Zhou, Yunpeng Chen, Jian Zhao, Sujoy Roy, Yan Shuicheng, Jiashi Feng, and Terence
Sim. Task relation networks. In WACV, 2019. 18"
REFERENCES,0.6041055718475073,"Hao Liu, Dong Li, JinZhang Peng, Qingjie Zhao, Lu Tian, and Yi Shan. Mtnas: Search multi-task
networks for autonomous driving. In ACCV, 2020. 18"
REFERENCES,0.6070381231671554,"Qiuhua Liu, Xuejun Liao, and Lawrence Carin. Semi-supervised multitask learning. In NeurIPS,
2008. 9"
REFERENCES,0.6099706744868035,"Pauline Luc, Camille Couprie, Soumith Chintala, and Jakob Verbeek. Semantic segmentation using
adversarial networks. arXiv preprint arXiv:1611.08408, 2016. 9"
REFERENCES,0.6129032258064516,"Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for
multi-task learning. In CVPR, 2016. 9"
REFERENCES,0.6158357771260997,"Siva Karthik Mustikovela, Varun Jampani, Shalini De Mello, Sifei Liu, Umar Iqbal, Carsten Rother,
and Jan Kautz. Self-supervised viewpoint learning from image collections. In CVPR, 2020. 9"
REFERENCES,0.6187683284457478,"Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support
inference from rgbd images. In ECCV, 2012. 2, 7, 14, 15"
REFERENCES,0.6217008797653959,"Thu H Nguyen-Phuoc, Chuan Li, Stephen Balaban, and Yongliang Yang. Rendernet: A deep
convolutional network for differentiable rendering from 3d shapes. In NeurIPS, 2018. 1"
REFERENCES,0.624633431085044,"Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In ECCV, 2016. 9"
REFERENCES,0.6275659824046921,"Mehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Representation learning by learning to count.
In ICCV, 2017. 9"
REFERENCES,0.6304985337243402,"Arghya Pal and Vineeth N Balasubramanian. Zero-shot task transfer. In CVPR, 2019. 9"
REFERENCES,0.6334310850439883,"George Panagopoulos. Multi-task learning for commercial brain computer interfaces. In BIBE, 2017. 18"
REFERENCES,0.6363636363636364,"George Papandreou, Liang-Chieh Chen, Kevin P Murphy, and Alan L Yuille. Weakly-and semi-
supervised learning of a deep convolutional network for semantic image segmentation. In ICCV,
2015. 9"
REFERENCES,0.6392961876832844,Under review as a conference paper at ICLR 2022
REFERENCES,0.6422287390029325,"Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context
encoders: Feature learning by inpainting. In CVPR, 2016. 9"
REFERENCES,0.6451612903225806,"Joel Pearson. The human imagination: the cognitive neuroscience of visual mental imagery. Nature
Reviews Neuroscience, 2019. 1"
REFERENCES,0.6480938416422287,"Etienne Pelaprat and Michael Cole. “minding the gap”: Imagination, creativity and human cognition.
Integrative Psychological and Behavioral Science, 2011. 1"
REFERENCES,0.6510263929618768,"Anastasia Pentina and Christoph H Lampert. Multi-task learning with labeled and unlabeled tasks. In
ICML, 2017. 9"
REFERENCES,0.6539589442815249,"Andrea Pilzer, Dan Xu, Mihai Puscas, Elisa Ricci, and Nicu Sebe. Unsupervised adversarial depth
estimation using cycled generative networks. In 3DV, 2018. 9"
REFERENCES,0.656891495601173,"Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. 4, 14, 17"
REFERENCES,0.6598240469208211,"Zhongzheng Ren and Yong Jae Lee. Cross-domain self-supervised multi-task feature learning using
synthetic imagery. In CVPR, 2018. 9, 15, 16"
REFERENCES,0.6627565982404692,"Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint
arXiv:1706.05098, 2017. 9"
REFERENCES,0.6656891495601173,"Veit Sandfort, Ke Yan, Perry J Pickhardt, and Ronald M Summers. Data augmentation using
generative adversarial networks (cyclegan) to improve generalizability in ct segmentation tasks.
Scientiﬁc reports, 2019. 1, 2, 3, 9"
REFERENCES,0.6686217008797654,"Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In NeurIPS,
2018. 9"
REFERENCES,0.6715542521994134,"Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari. How good is my gan? In ECCV, 2018. 1"
REFERENCES,0.6744868035190615,"Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of Big Data, 2019. 9"
REFERENCES,0.6774193548387096,"Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael
Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In CVPR, 2019. 1"
REFERENCES,0.6803519061583577,"Nasim Souly, Concetto Spampinato, and Mubarak Shah. Semi supervised semantic segmentation
using generative adversarial network. In ICCV, 2017. 1, 9, 19"
REFERENCES,0.6832844574780058,"Trevor Standley, Amir R Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese.
Which tasks should be learned together in multi-task learning? In ICML, 2020. 2, 6, 7, 9, 15, 16"
REFERENCES,0.6862170087976539,"Ximeng Sun, Rameswar Panda, and Rogerio Feris. Adashare: Learning what to share for efﬁcient
deep multi-task learning. arXiv preprint arXiv:1911.12423, 2019. 2, 4, 6, 7, 9, 14, 15, 16"
REFERENCES,0.6891495601173021,"Bram Wallace, Ziyang Wu, and Bharath Hariharan. Can we characterize tasks without labels or
features? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 1245–1254, June 2021. 9"
REFERENCES,0.6920821114369502,"Dan Wang, Junjie Wen, Yuyong Wang, Xiangdong Huang, and Feng Pei. End-to-end self-driving
using deep neural networks with multi-auxiliary tasks. Automotive Innovation, 2019. 18"
REFERENCES,0.6950146627565983,"Yu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from
imaginary data. In CVPR, 2018. 9"
REFERENCES,0.6979472140762464,"Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of
decoder-based generative models. arXiv preprint arXiv:1611.04273, 2016. 1"
REFERENCES,0.7008797653958945,"Teresa Yeo, O˘guzhan Fatih Kar, and Amir Zamir. Robustness via cross-domain ensembles. arXiv
preprint arXiv:2103.10919, 2021. 9"
REFERENCES,0.7038123167155426,"Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese.
Taskonomy: Disentangling task transfer learning. In CVPR, 2018. 1, 2, 4, 7, 9, 14, 15, 16"
REFERENCES,0.7067448680351907,Under review as a conference paper at ICLR 2022
REFERENCES,0.7096774193548387,"Amir R Zamir, Alexander Sax, Nikhil Cheerla, Rohan Suri, Zhangjie Cao, Jitendra Malik, and
Leonidas J Guibas. Robust learning through cross-task consistency. In CVPR, 2020. 9"
REFERENCES,0.7126099706744868,"Fangneng Zhan, Shijian Lu, and Chuhui Xue. Verisimilar image synthesis for accurate detection and
recognition of texts in scenes. In ECCV, 2018. 9"
REFERENCES,0.7155425219941349,"Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. In ICML, 2019. 1, 2, 4, 14, 15, 17, 19"
REFERENCES,0.718475073313783,"Chenhong Zhou, Changxing Ding, Xinchao Wang, Zhentai Lu, and Dacheng Tao. One-pass multi-task
networks with cross-task guided attention for brain tumor segmentation. TIP, 2020. 18"
REFERENCES,0.7214076246334311,"Xinyue Zhu, Yifan Liu, Jiahong Li, Tao Wan, and Zengchang Qin. Emotion classiﬁcation with data
augmentation using generative adversarial networks. In PAKDD, 2018. 1, 9"
REFERENCES,0.7243401759530792,Under review as a conference paper at ICLR 2022
REFERENCES,0.7272727272727273,"We summarize the content of the appendix as follows. Section A includes additional details of model
architecture of our proposed multi-task oriented generative modeling (MGM) framework. Section B
describes implementation details of MGM and also the dataset settings. Section C compares MGM
with other state-of-the-art multi-task models and further investigates in the few-shot regime. Section D
provides additional ablation study on the training strategies for the reﬁnement network, training with
higher resolution images, generalizibility of the shared feature representation, and different types of
image generation networks such as DCGAN. Section E shows more prediction visualizations. Finally,
Section F discusses the potential of applying MGM to multi-task learning in other speciﬁc domains."
REFERENCES,0.7302052785923754,"A
ADDITIONAL DETAILS OF MODEL ARCHITECTURE"
REFERENCES,0.7331378299120235,"Multi-task Network:
The multi-task network contains a shared encoder network and separate
decoder networks for target tasks. We use a ResNet-18 (He et al., 2016) as the encoder network,
and its architecture follows the standard Pytorch implementation1. We only change the size of the
features of each layer group from [64, 128, 256, 512] to [48, 96, 192, 360], so as to better address our
GPU memory constraints. The size of the ﬁnal feature representation is (360, 8, 8). For the decoder
network, we use the same architecture as in Taskonomy (Zamir et al., 2018)."
REFERENCES,0.7360703812316716,"Self-supervision Network:
We adopt SimCLR (Chen et al., 2020) as our self-supervision network.
SimCLR is one of the state-of-the-art self-supervised learning approaches based on instance-level
discrimination tasks. Following (Chen et al., 2020), we randomly apply 5 types of transformations on
a source image to obtain an augmented image. These transformations are as follows: (1) random
resizing and cropping followed by resizing back to the original size; (2) random horizontal ﬂipping
with probability of 0.5; (3) random color jittering with probability of 0.5; (4) random transformation of
RGB images to gray-scale images with probability of 0.2; (5) random Gaussian blur with probability
of 0.5. The shape of the transformed latent feature, which is used to perform contrastive learning, is
(128, )."
REFERENCES,0.7390029325513197,"Reﬁnement Network:
The reﬁnement network takes as input the prediction results of the multi-
task network. For each individual prediction, we apply a ResNet-10 (He et al., 2016) as the reﬁnement
encoder to extract the features. The feature dimension of each layer group of the reﬁnement encoder
is the same as the multi-task encoder network. Then we concatenate all the features together and
apply a fully-connected layer with the hidden size of 128 to obtain the ﬁnal scene class prediction."
REFERENCES,0.7419354838709677,"Image Generation Network:
We have instantiated the image generation network with two widely
used generative networks: self-attention GAN (SAGAN)2 (Zhang et al., 2019) in the main paper
and deep convolutional GAN (DCGAN)3 (Radford et al., 2015) in Section D.5 of this document.
For DCGAN, we change the original batch-normalization layers to conditional batch normalization
layers (De Vries et al., 2017) to allow conditional image generation. For the additional embedding
layer used for joint learning, we use a simple global averaged pooling layer followed by a dense layer.
e.g."
REFERENCES,0.7448680351906158,"B
IMPLEMENTATION DETAILS"
REFERENCES,0.7478005865102639,"Data Processing:
For the NYUv2 (Nathan Silberman & Fergus, 2012) dataset, following Sun et al.
(2019) we resize and normalize the RGB images to (−1, 1), standardize the normal ground-truth to
(0, 1), and do not normalize the depth. For the Taskonomy dataset (Zamir et al., 2018), we follow the
standard data normalization in Zamir et al. (2018)."
REFERENCES,0.750733137829912,"Additional Implementation Details:
We use Adam (Kingma & Ba, 2014) optimizer for all the
models. The learning rates are set to 0.001 for the multi-task, self-supervision, and reﬁnement
networks, 0.0001 for the SAGAN generator, and 0.0004 for the SAGAN discriminator. The batch
size is set to 32. We use a cross-entropy loss for semantic segmentation and the scene classiﬁcation
task of the reﬁnement network, and an l1 loss for surface normal and depth estimation."
REFERENCES,0.7536656891495601,"1https://github.com/pytorch/vision/blob/master/torchvision/models/
resnet.py"
REFERENCES,0.7565982404692082,"2https://github.com/voletiv/self-attention-GAN-pytorch
3https://github.com/Natsu6767/DCGAN-PyTorch"
REFERENCES,0.7595307917888563,Under review as a conference paper at ICLR 2022
REFERENCES,0.7624633431085044,"Due to the different converge time for the different modules, we use a three-stage strategy to perform
joint training: (1) We ﬁrst train the multi-task network separately with fully labeled real data; (2)
We then freeze the multi-task network, and train the image generation network and the embedding
network separately; (3) We do joint training with the whole network using both fully labeled real data
and weakly labeled sythesized data. During the pre-training process of SAGAN, we set the batch size
to 128 to train a better model following Zhang et al. (2019). Then, for the joint training, we use a
batch size of 32 for all the sub-networks. Additionally, for the same minibatch of data, we update the
image generation network 2 times iteratively during stage (3)."
REFERENCES,0.7653958944281525,"Training Procedure:
We summarized the training procedure in Algorithm 1 in the main paper.
Here we further explain the training procedure in more details. Given a minibatch of data in Sreal, we
conduct the following training procedure."
REFERENCES,0.7683284457478006,"1. For the input images x, we predict ˆy = M(x), and then use the task-speciﬁc losses between
y and ˆy to update the multi-task network M."
REFERENCES,0.7712609970674487,"2. We predict the scene labels by ˆc = R(ˆy), and update the reﬁnement network R and the
multi-task encoding network E using the cross-entropy loss between c and ˆc."
REFERENCES,0.7741935483870968,"3. We randomly sample pairs of augmented images, process them with the self-supervision
network, and then update the self-supervision network and the multi-task encoder E with
the NT-Xent loss in Eqn. (6)."
REFERENCES,0.7771260997067448,"4. We train the image generation network G through adversarial training with (x, c), and
back-propagate the adversarial error and update E at the same time."
REFERENCES,0.7800586510263929,"5. We sample another minibatch of synthesized data (ex, ec), and use these data to update E by
performing both the EM-like algorithm described in Section 3.2 (main paper) with R and
the self-supervised learning as in step 3."
REFERENCES,0.782991202346041,"B.1
DATASET SETTING"
REFERENCES,0.7859237536656891,"We evaluate all the models on two widely-benchmarked datasets: NYUv2 (Nathan Silberman &
Fergus, 2012; Eigen & Fergus, 2015) containing 1,449 images with 40 types of objects (Gupta et al.,
2013); Tiny-Taskonomy which is the standard tiny split of the Taskonomy dataset (Zamir et al.,
2018)."
REFERENCES,0.7888563049853372,"For Tiny-Taskonomy dataset, since a certain amount of images for each category is required to train a
generative network, we keep the images of the top 35 scene categories on Tiny-Taskonomy, with each
one consisting of more than 1,000 images. This resulting dataset contains 358,426 images in total.
For NYUv2, we randomly select 1,049 images as the full training set and 200 images each as the
validation/test set. For Tiny-Taskonomy, we randomly pick 80% of the whole set as the full training
set and 10% each as the validation/test set."
REFERENCES,0.7917888563049853,"C
ADDITIONAL COMPARISONS"
REFERENCES,0.7947214076246334,"C.1
COMPARISON WITH OTHER MULTI-TASK MODELS"
REFERENCES,0.7976539589442815,"In the main paper, for a fair comparison we focused on comparing our MGM model with internal
models (e.g., the multi-task model upon which MGM builds). To have a more comprehensive
understanding of the performance of MGM, we also compare our method with some state-of-the-art
multi-task models. We focus on the 25% data setting for the NYUv2 dataset, where collaboration
between different tasks and the utilization of data is vitally important."
REFERENCES,0.8005865102639296,"We include six models in this experiments. ST is the single-task model, where the encoder and the
decoder are adopted from Zamir et al. (2018). MT is the multi-task model that uses a shared encoder as
ST and separate decoders. ST and MT are the baselines compared in the main paper. TaskGrouping
uses the optimal network for the three tasks concluded from Standley et al. (2020). Another two well-
performing multi-task models are Cross-stitch (Ren & Jae Lee, 2018)4 and AdaShare (Sun et al.,
2019). MGM is our proposed model. Table 6 summarizes the results. Notably, with a simple shared"
REFERENCES,0.8035190615835777,4We modify the network architecture following Sun et al. (2019) to make it work for the three tasks.
REFERENCES,0.8064516129032258,Under review as a conference paper at ICLR 2022
REFERENCES,0.8093841642228738,"Method
SS-mIOU(↑)
DE-mABSE(↓)
SN-mAD(↓)
ST (Zamir et al., 2018)
0.199
0.908
0.312
MT (Zamir et al., 2018)
0.207
0.874
0.296
TaskGrouping (Standley et al., 2020)
0.215
0.853
0.292
Cross Stitch (Ren & Jae Lee, 2018)
0.205
0.917
0.296
AdaShare (Sun et al., 2019)
0.211
0.875
0.289
MGM
0.229
0.844
0.277"
REFERENCES,0.8123167155425219,"Table 6: Comparison with state-of-the-art multi-task models in the 25% data setting on the NYUv2
dataset. Notably, with a simple shared encoder architecture, our MGM model outperforms other
state-of-the-art multi-task networks with more sophisticated architectures, which indicates the beneﬁt
of introducing generative modeling for multi-task learning. In addition, our MGM is a model-agnostic
framework and could be incorporated with these different multi-task models for further improvement."
REFERENCES,0.8152492668621701,"Method
SS-mIOU(↑)
DE-mABSE(↓)
SN-mAD(↓)
ST
0.162
1.004
0.337
MT
0.185
0.930
0.311
MGM
0.197
0.911
0.291"
REFERENCES,0.8181818181818182,"Table 7: Comparison in the few-shot regime – in the 10% data setting on the NYUv2 dataset where
around 3 images for each scene is used as the training set. Again, MGM signiﬁcantly outperforms
the compared models, showing the beneﬁt of generative models in the extremely low-data regime."
REFERENCES,0.8211143695014663,"encoder architecture, our MGM model outperforms other state-of-the-art multi-task networks with
more sophisticated architectures, which indicates the beneﬁt of introducing generative modeling for
multi-task learning. In addition, our MGM is a model-agnostic framework and could be incorporated
with these different multi-task models for further improvement."
REFERENCES,0.8240469208211144,"C.2
EXPERIMENTS WITH FEW-SHOT SETTING"
REFERENCES,0.8269794721407625,"Since the learned generative model facilitates ﬂow of knowledge across tasks and provides meaningful
variations in existing images, it is especially beneﬁcial in the low-data regime. So we designed a 10%
data setting for NYUv2 dataset, where around 3 images for each scene is used as the training set. We
also compare our MGM model with ST and MT. From Table 7, we can see that MGM outperforms
the other compared models signiﬁcantly, indicating the gain of generative models in the extremely
low-data regime."
REFERENCES,0.8299120234604106,"D
ADDITIONAL ABLATION STUDY"
REFERENCES,0.8328445747800587,"D.1
IMPACT OF PARAMETERS"
REFERENCES,0.8357771260997068,"Introducing the reﬁnement, self-supervision, and image generation networks also leads to more
parameters. To validate that the performance improvements come from the novel design of our
architecture rather than merely increasing the number of parameters, we provide two model variants
as additional baselines: STl and MTl use ResNet-34 as the encoder network and the corresponding
decoder networks. These two networks have a similar amount of parameters as MGM. The result in
Table 8 show that simply increasing the number of parameters cannot signiﬁcantly boost performance."
REFERENCES,0.8387096774193549,"D.2
TRAINING STRATEGIES FOR REFINEMENT NETWORK"
REFERENCES,0.841642228739003,"In the main paper, we proposed an Expectation-Maximum (EM) like algorithm to coordinate the
training between the reﬁnement network and the main network. Here we compare our EM-Like
Training (EML) with two alternative ways of the training procedure: Plain End-to-End Training
(PEoE) backwards the reﬁnement loss to update the entire network directly; Loosely Separate
Training (LSeT) trains the reﬁnement network and the main network separately, and only backwards
the error to the encoder network when dealing with weakly labeled images. Table 9 shows the
comparison results on the NYUv2 dataset in the 50% data setting. From this table, we could ﬁnd: (1)
A na¨ıve end-to-end training strategy is not able to facilitate the cooperation between different networks
and thus hurts the overall performance; (2) Loosely separate training enables the communication"
REFERENCES,0.844574780058651,Under review as a conference paper at ICLR 2022
REFERENCES,0.8475073313782991,"Model
SS-mIOU (↑)
DE-mABSE (↓)
SN-mAD (↓)
ST
0.230
0.837
0.309
MT
0.237
0.819
0.291
STl
0.232
0.841
0.304
MTl
0.236
0.804
0.288"
REFERENCES,0.8504398826979472,"Table 8: Impact of parameters. STl and MTl: baselines with a larger number of parameters (with
deeper backbones). Simply increasing the number of parameters cannot signiﬁcantly boost perfor-
mance."
REFERENCES,0.8533724340175953,"Method
SS-mIOU(↑)
DE-mABSE(↓)
SN-mAD(↓)
MT
0.237
0.815
0.291
PEoE
0.211
0.896
0.301
LSeT
0.247
0.768
0.277
EML
0.251
0.734
0.273"
REFERENCES,0.8563049853372434,"Table 9: Results on the NYUv2 dataset in the 50% data setting with different training strategies
for the reﬁnement network. ‘MT’: multi-task learning baseline; ‘PEoE’: plain end-to-end training;
‘LSeT’: loosely separate training; ‘EML’: EM-like training (proposed in the main paper). Our EML
signiﬁcantly outperforms alternative strategies to train the reﬁnement network."
REFERENCES,0.8592375366568915,"between the reﬁnement network and the main network when needed and thus outperforms the baseline;
(3) Our proposed EM-like training strategy further improves over the loosely separate training and
achieves the best performance."
REFERENCES,0.8621700879765396,"D.3
EXPERIMENTS ON HIGHER IMAGE RESOLUTION"
REFERENCES,0.8651026392961877,"In principle, the proposed framework is agnostic to the speciﬁc types of multi-task networks and
image generation networks, thus ﬂexible with image resolutions. The practical constraint lies in
that it is still challenging and resource-consuming for modern generative models to synthesize very
high-resolution images (Zhang et al., 2019; Brock et al., 2018), although the deep multi-task models
normally work better with high-resolution images. In the main experiments, consistent with exiting
image synthesis work (Zhang et al., 2019), we focused on the resolution of 128 × 128. Here we
further made an attempt to run our experiments with a higher resolution, 256 × 256 on NYUv2. The
results of all the compared models in the 50% data setting are shown in Table 10. We could ﬁnd that
MGM still consistently outperforms the baselines, indicating the great robustness and ﬂexibility of
our proposed framework."
REFERENCES,0.8680351906158358,"D.4
GENERALIZATION OF THE SHARED FEATURE REPRESENTATION"
REFERENCES,0.8709677419354839,"Intuitively, our MGM achieves state-of-the-art performance by effectively learning a shared feature
representation. We further show the generalization capability of this representation by designing the
following experiment: for the multi-task model and our MGM model, we ﬁrst learn the shared feature
space with the SS and DE tasks, and we then use that learned feature space to train a new decoder for
the SN task. We report the results on NYUv2 in Table 11. Our MGM outperforms the multi-task
model in all the three data settings, which means that MGM indeed learns a better and robust shared
feature space."
REFERENCES,0.873900293255132,"D.5
EXPERIMENTAL RESULTS WITH DCGAN"
REFERENCES,0.8768328445747801,"The proposed MGM model is a general framework and is ﬂexible with different choices of model
components. Here we show its ﬂexibility for the image generation network: We replace SAGAN
with DCGAN (Radford et al., 2015), and Table 12 shows the result on the NYUv2 dataset in the
50% data setting. Similar to the results in the main paper, MGM equipped with DCGAN (‘MGM-
DCGAN’) consistently improves the performance on all the tasks, which indicates the robustness
of the proposed MGM framework. In addition, we ﬁnd that ‘MGM-SAGAN’ outperforms ‘MGM-
DCGAN’, suggesting that a more powerful image generation network leads to better performance."
REFERENCES,0.8797653958944281,Under review as a conference paper at ICLR 2022
REFERENCES,0.8826979472140762,"Model
SS-mIOU (↑)
DE-mABSE (↓)
SN-mAD (↓)
ST
0.239
0.849
0.282
MT
0.244
0.834
0.313
MGM
0.257
0.819
0.275"
REFERENCES,0.8856304985337243,"Table 10: Experiments with 256 image resolution on the NYUv2 dataset. Our MGM still consistently
outperforms the compared baselines, showing the great robustness and ﬂexibility of the proposed
framework."
REFERENCES,0.8885630498533724,"Model
mAD-100%(↓)
mAD-50%(↓)
mAD-25%(↓)
MT
0.291
0.310
0.323
MGM
0.280
0.298
0.305"
REFERENCES,0.8914956011730205,"Table 11: Results for the SN task with pre-trained feature representations by the SS and DE tasks.
MGM consistently outperforms multi-task (MT), indicating that MGM learns a more effective and
generalizable feature representation."
REFERENCES,0.8944281524926686,"E
MORE VISUALIZATIONS"
REFERENCES,0.8973607038123167,"In Figure 4 of the main paper, we visualized the prediction results. Here we provide more visual-
izations of the multi-task predictions for MGM and the compared baselines in Figure 6. Our MGM
model signiﬁcantly outperforms both ST and MT baselines."
REFERENCES,0.9002932551319648,"F
DISCUSSION AND FUTURE WORK"
REFERENCES,0.9032258064516129,"In the main paper, we focused on applying generative networks to general multi-task learning.
On the other hand, generative models can also be exploited to address multiple tasks in speciﬁc
domains, such as human face modeling (Li et al., 2019; 2016; Han et al., 2017; Chen et al., 2018a),
autonomous driving (Liu et al., 2020; Wang et al., 2019; Chennupati et al., 2019), and bio-medical
processing (Zhou et al., 2020; Panagopoulos, 2017). We leave such exploration of leveraging our
MGM model to these domains as future work."
REFERENCES,0.906158357771261,"G
ADDITIONAL EXPERIMENTAL EVALUATIONS"
REFERENCES,0.9090909090909091,"G.1
EXPERIMENTAL EVALUATION ON CITYSCAPE SUBSET"
REFERENCES,0.9120234604105572,"In this section, we demonstrate that MGM can work with datasets when no image-level labels are
available. In an alternative may, the proposed MGM frame model can work with object labels as
well since the generative network and reﬁnement network can naturally work with multi-hot labels —
the reﬁnement network can work with a multi-label classiﬁer, and the generative network can be a
multi-label-conditional GAN."
REFERENCES,0.9149560117302052,"We conducted semantic segmentation on a subset of CityScape (Cordts et al., 2016) dataset, the
Zurich street scene. We focused on the semantic segmentation task, which is a representative task
on CityScape. We use the 30 standard multi-hot CityScape semantic object labels for the generative
model and also the reﬁnement networks. We use 80% of the data for training and 20 % for testing
and compare the performance of ST and MGM for this experiment. We generate the same amount
of data with random multi-hot labels using MGM. The results are shown in Table 13. MGM still
outperforms the baseline ST model, indicating the robustness and generalizability of the model with
multi-hot object labels."
REFERENCES,0.9178885630498533,"G.2
EXPERIMENTS WITH EXTREME LOW DATA AT TASKONOMY"
REFERENCES,0.9208211143695014,"We noticed that for Tiny-Taskonomy dataset, 25% data setting is still far from low-data regime. To
further explore the effectiveness with our model with low-data, we further conduct an experiment
with a subset of Tiny-Taskonomy dataset. We randomly select 3 nodes (allensville, benevolence,
and coffeen) from Tiny-Taskonomy dataset to build a dataset with 17,404 images—around 5% data
setting compared with the full Tiny-Taskonomy dataset. We then conduct experiments with ST,"
REFERENCES,0.9237536656891495,Under review as a conference paper at ICLR 2022
REFERENCES,0.9266862170087976,"Method
SS-mIOU(↑)
DE-mABSE(↓)
SN-mAD(↓)
ST
0.233
0.835
0.309
MT
0.237
0.815
0.291
MGM-SAGAN
0.251
0.734
0.273
MGM-DCGAN
0.245
0.750
0.285"
REFERENCES,0.9296187683284457,"Table 12: Results on the NYUv2 dataset in the 50% data setting with different image generation
networks. ‘MGM-SAGAN’: MGM equipped with SAGAN (presented in the main paper); ‘MGM-
DCGAN’: MGM equipped with DCGAN. Both ‘MGM-SAGAN’ and ‘MGM-DCGAN’ consistently
improve the performance on all the tasks and outperform single-task (ST) and multi-task (MT)
baselines. This shows the robustness and ﬂexibility of the proposed MGM framework. In addition,
‘MGM-SAGAN’ outperforms ‘MGM-DCGAN’, suggesting that a more powerful image generation
network leads to better performance."
REFERENCES,0.9325513196480938,"Method
SS-mIOU(↑)
ST
0.57
MGM
0.64"
REFERENCES,0.9354838709677419,"Table 13: Results on the CityScape Subset. MGM still outperforms the baseline ST model, indicating
the robustness and generalizability of the model with multi-hot object labels."
REFERENCES,0.9384164222873901,"MT and MGM for this subset. All the other experimental settings keep the same as the main paper.
Table 14 shows the comparable results. Combining the results in Table 2, we can ﬁnd that MGM
consistently outperforms ST and MT and is robust and especially helpful in low-data regime."
REFERENCES,0.9413489736070382,"G.3
ABLATION WITH SINGLE TASKS"
REFERENCES,0.9442815249266863,"MGM is a general framework that can be applied to both single tasks and multiple tasks. In the main
submission, we mainly focused on the more challenging multi-task scenario. In this subsection, we
conduct experiments with single tasks. Here we add three baselines applying MGM to the single
tasks (SS, DE, SN) named MGM-SS, MGM-DE and MGM-SN in the NYUv2 50% data setting.
We further provide an additional baseline by using the equivalently sampled data from the above three
model as the augmented data, but not jointly train the generative network, named MGM-Combine.
The results of these models are shown in Table 15. We have the following observations: (1) The
proposed MGM framework can consistently beneﬁt each single tasks though without leveraging
shared features from multiple tasks. (2) Compared with the full MGM model, the performance
drops when only using single tasks to jointly train with the generative model. (3) When using the
individually optimized generative models, the performance is slight better than MGM/j but still could
not reach MGM, indicating the importance of our joint training mechanism."
REFERENCES,0.9472140762463344,"H
VISUALIZATIONS OF GENERATED IMAGES"
REFERENCES,0.9501466275659824,"We visualize the generated images for the Tiny-Taskonomy dataset of SAGAN (Zhang et al., 2019)
and MGM in Figure 7. From Figure 7, we observe that (1) the conventional SAGAN, trained with
the realistic objective and without the guidance of the downstream tasks, produces photo-realistic
images; (2) the visual quality of the generated images by MGM becomes degraded, where SAGAN is
jointly trained with the multi-task learning objective and under the guidance of the downstream tasks.
Interestingly, a similar phenomenon has been observed in (Souly et al., 2017), where a generative
model is used to facilitate the semantic segmentation task. We hypothesize this is because those
synthesized images that are useful for improving downstream tasks might not be necessarily photo-
realistic. While the images synthesized by MGM are not visually realistic, they may contain some
crucial discriminative information that can be leveraged for addressing the downstream tasks — for
example, the synthesized images may contain some unseen patterns from the real images, which
increases the diversity of the training data. In addition, the difference of the synthesized images
between SAGAN and MGM can also partially explain the result in the pilot study — the images
synthesized off the shelf are quite different from the desired images for multi-task learning, and thus
they are not effective in facilitating downstream tasks."
REFERENCES,0.9530791788856305,Under review as a conference paper at ICLR 2022 ST-SS MT-SS
REFERENCES,0.9560117302052786,MGM-SS GT-SS ST-DE MT-DE
REFERENCES,0.9589442815249267,MGM-DE GT-DE ST-SN MT-SN
REFERENCES,0.9618768328445748,MGM-SN GT-SN
REFERENCES,0.9648093841642229,"Figure 6: More visualizations of the multi-task predictions for MGM and the compared baselines.
SS: semantic segmentation task; DE: depth estimation task; SN: surface normal prediction task;
ST: single-task model; MT: multi-task model; MGM: multi-task oriented generative modeling (our
proposed model); GT: ground-truth. The prediction results of our MGM model are much closer to
the ground-truth and signiﬁcantly outperform the state-of-the-art results."
REFERENCES,0.967741935483871,"I
VISUALIZATIONS OF THE SEMANTIC ANNOTATIONS"
REFERENCES,0.9706744868035191,"We also visualize two samples and the corresponding semantic segmentation annotations we used
for the pilot study in Figure 8. The visualization shows that the annotation is accurate and using
Taskonomy is sufﬁcient for training."
REFERENCES,0.9736070381231672,Under review as a conference paper at ICLR 2022
REFERENCES,0.9765395894428153,"Model
SS (↓)
DE (↓)
SN (↓)
ST
0.137
1.836
0.161
MT
0.156
1.807
0.162
MGM
0.125
1.670
0.153"
REFERENCES,0.9794721407624634,"Table 14: Comparison with extreme low data in Taskonomy. In this data setting, MGM signiﬁcantly
outperforms both ST and MT, indicating that MGM is robust and especially helpful in low-data
regime."
REFERENCES,0.9824046920821115,"Model
SS-mIOU (↑)
DE-mABSE (↓)
SN-mAD (↓)
ST
0.230
0.837
0.309
MGM-SS
0.244
-
-
MGM-DE
-
0.752
-
MGM-SN
-
-
0.277
MGM-Combine
0.249
0.747
0.277
MGM
0.251
0.734
0.273"
REFERENCES,0.9853372434017595,"Table 15: Ablation with MGM for single tasks and a stronger baseline with the learned information
from the three individual tasks but without jointly training. The experiments are conducted on
NYUv2 50% data setting. MGM-SS, MGM-DE, MGM-SN: variantal MGM model for single tasks.
MGM-Combine: MGM variant trained with augmented images generated by the above three models.
The proposed MGM framework can consistently beneﬁt each single tasks and MGM-combine cannot
reach the performance of MGM, indicating the importance of joint training mechanism."
REFERENCES,0.9882697947214076,"SAGAN
MGM"
REFERENCES,0.9912023460410557,"Figure 7: Generated images of SAGAN and MGM for the Tiny-Taskonomy dataset. After jointly
training, the images are not visually realistic, but they are helpful to improve the downstream task
performance."
REFERENCES,0.9941348973607038,"RGB
Segmentation
RGB
Segmentation"
REFERENCES,0.9970674486803519,"Figure 8: Paired RGB and semantic segmentation labels. This paired data indicates that the oracle
annotator we used in the pilot study is effective."
