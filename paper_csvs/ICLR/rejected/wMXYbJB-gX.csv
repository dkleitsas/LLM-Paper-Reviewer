Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002127659574468085,"Label smoothing regularization (LSR) is a prevalent component for training deep
neural networks and can improve the generalization of models effectively. Al-
though it achieves empirical success, the theoretical understanding about the
power of label smoothing, especially about its inﬂuence on optimization, is still
limited. In this work, we, for the ﬁrst time, theoretically analyze the convergence
behaviors of stochastic gradient descent with label smoothing in deep learning.
Our analysis indicates that an appropriate LSR can speed up the convergence by
reducing the variance in gradient, which provides a theoretical interpretation on
the effectiveness of LSR. Besides, the analysis implies that LSR may slow down
the convergence at the end of optimization. Therefore, a novel algorithm, namely
Two-Stage LAbel smoothing (TSLA), is proposed to further improve the conver-
gence. With the extensive analysis and experiments on benchmark data sets, the
effectiveness of TSLA is veriﬁed both theoretically and empirically."
INTRODUCTION,0.00425531914893617,"1
INTRODUCTION"
INTRODUCTION,0.006382978723404255,"Due to the massive number of parameters, over-ﬁtting becomes one of the major challenges in
training effective deep neural networks (M¨uller et al., 2019). Various strategies such as weight
decay, dropout (Hinton et al., 2012b), batch normalization (Ioffe & Szegedy, 2015), data augmenta-
tion (Simard et al., 1998), etc. have been proposed to alleviate the problem. Unlike regularization on
input data or model weights, label smoothing regularization (LSR) (Szegedy et al., 2016) perturbs
labels of data for better generalization. Concretely, for a K-class classiﬁcation problem, the one-hot
label in the prevalent cross entropy loss is replaced by a soft label as yLS = (1 −θ)y + θby, where y
is the one-hot label and by = 1"
INTRODUCTION,0.00851063829787234,"K is a uniform distribution for all labels. By optimizing the soft label
instead, LSR helps to improve the generalization of deep neural networks in diverse applications
including image classiﬁcation (Zoph et al., 2018; He et al., 2019), speech recognition (Chorowski &
Jaitly, 2017; Zeyer et al., 2018), and language translation (Vaswani et al., 2017; Nguyen & Salazar,
2019). Besides, it is complementary to other regularization methods and can work with others to
make learned neural networks applicable (He et al., 2019)."
INTRODUCTION,0.010638297872340425,"Despite the success of LSR, efforts toward theoretical understanding are still limited. M¨uller et al.
(2019) have empirically shown that LSR can help improve model calibration. However, they also
found that LSR could impair knowledge distillation when the learned model is applied as the teacher
network. Yuan et al. (2019a) have proved that LSR provides a virtual teacher model for knowledge
distillation. As a widely used trick, Lukasik et al. (2020) have shown that LSR works since it can
successfully mitigate label noise. Most of existing works try to study LSR as regularizing label
space while little work investigates the direct impact of LSR on training deep neural networks."
INTRODUCTION,0.01276595744680851,"In this work, we analyze the beneﬁts of LSR for the learning process from the view of optimiza-
tion. Concretely, our theory shows that an appropriate LSR can essentially reduce the variance of
stochastic gradient in the assigned class labels and thus it can speed up the convergence of stochas-
tic gradient descent, which is a standard optimizer for deep learning. Besides, the analysis implies
that LSR may incur slow convergence at the end of optimization. Inspired by the result, a novel
algorithm is developed to accelerate the convergence by a two-stage training paradigm. The effec-
tiveness of the proposed method is veriﬁed both theoretically and empirically. We summarize the
main contributions of this paper as follows."
INTRODUCTION,0.014893617021276596,"• To the best of our knowledge, it is the ﬁrst work that establishes improved iteration com-
plexities of stochastic gradient descent (SGD) (Robbins & Monro, 1951) with LSR for"
INTRODUCTION,0.01702127659574468,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.019148936170212766,"ﬁnding an ϵ-approximate stationary point (Deﬁnition 1) in solving a smooth non-convex
problem in the presence of an appropriate label smoothing. The results theoretically ex-
plain why an appropriate LSR can help speed up the convergence. (Subsection 4.1)
• According to the analysis, we propose a simple yet effective method to accelerate the con-
vergence of LSR (Subsection 4.2). It adopts a two-stage training paradigm that applies
LSR in the ﬁrst stage and then runs on one-hot labels in the second stage.
• The theoretical analysis demonstrates that TSLA has an improved iteration complexity
(Subsection 4.3) and better generalization (Subsection 4.4) compared to the conventional
LSR. Extensive experiments on benchmark data sets conﬁrm the effectiveness of our pro-
posed method."
RELATED WORK,0.02127659574468085,"2
RELATED WORK"
RELATED WORK,0.023404255319148935,"In this section, we introduce some related work. A closely related idea to LSR is conﬁdence penalty
proposed by Pereyra et al. (2017), an output regularizer that penalizes conﬁdent output distribu-
tions by adding its negative entropy to the negative log-likelihood during the training process. The
authors (Pereyra et al., 2017) presented extensive experimental results in training deep neural net-
works to demonstrate better generalization comparing to baselines with only focusing on the existing
hyper-parameters. They have shown that LSR is equivalent to conﬁdence penalty with a reversing
direction of KL divergence between uniform distributions and the output distributions."
RELATED WORK,0.02553191489361702,"DisturbLabel introduced by Xie et al. (2016) imposes the regularization within the loss layer, where
it randomly replaces some of the ground truth labels as incorrect values at each training iteration.
Its effect is quite similar to LSR that can help to prevent the neural network training from overﬁt-
ting. The authors have veriﬁed the effectiveness of DisturbLabel via several experiments on training
image classiﬁcation tasks."
RELATED WORK,0.027659574468085105,"Recently, many works (Zhang et al., 2018; Bagherinezhad et al., 2018; Goibert & Dohmatob, 2019;
Shen et al., 2019; Li et al., 2020b) explored the idea of LSR technique. Ding et al. (2019) extended
an adaptive label regularization method, which enables the neural network to use both correctness
and incorrectness during training. Pang et al. (2018) used the reverse cross-entropy loss to smooth
the classiﬁer’s gradients. Wang et al. (2020) proposed a graduated label smoothing method that uses
the higher smoothing penalty for high-conﬁdence predictions than that for low-conﬁdence predic-
tions. They found that the proposed method can improve both inference calibration and translation
performance for neural machine translation models. By contrast, we will try to understand the power
of LSR from an optimization perspective and try to study how and when to use LSR."
PRELIMINARIES AND NOTATIONS,0.029787234042553193,"3
PRELIMINARIES AND NOTATIONS"
PRELIMINARIES AND NOTATIONS,0.031914893617021274,"We ﬁrst present some notations. Let ∇wFS(w) denote the gradient of a function FS(w). When the
variable to be taken a gradient is obvious, we use ∇FS(w) for simplicity. We use ∥· ∥to denote the
Euclidean norm. Let ⟨·, ·⟩be the inner product."
PRELIMINARIES AND NOTATIONS,0.03404255319148936,"In classiﬁcation problem, we aim to seek a classiﬁer to map an example x ∈X onto one of K
labels y ∈Y ⊂RK, where y = (y1, y2, . . . , yK) is a one-hot label, meaning that yi is “1” for the
correct class and “0” for the rest. Suppose the example-label pairs are drawn from a distribution P,
i.e., (x, y) ∼P = (Px, Py). Let S = {(x1, y1), . . . , (xn, yn)} denotes a set of n examples drawn
from P. We denote by E(x,y)[·] the expectation that takes over a random variable (x, y). When
the randomness is obvious, we write E[·] for simplicity. Our goal is to learn a prediction function
f(w; x) : W × X →RK that is as close as possible to y, where w ∈W is the parameter and W is
a closed convex set. To this end, we want to minimize the following expected loss under P:"
PRELIMINARIES AND NOTATIONS,0.036170212765957444,"min
w∈W FS(w) := 1 n n
X"
PRELIMINARIES AND NOTATIONS,0.03829787234042553,"i=1
ℓ(yi, f(w; xi))
(1)"
PRELIMINARIES AND NOTATIONS,0.04042553191489362,where ℓ: Y × RK →R+ is a cross-entropy loss function given by
PRELIMINARIES AND NOTATIONS,0.0425531914893617,"ℓ(y, f(w; x)) = K
X"
PRELIMINARIES AND NOTATIONS,0.04468085106382979,"i=1
−yi log"
PRELIMINARIES AND NOTATIONS,0.04680851063829787,"exp(fi(w; x))
PK
j=1 exp(fj(w; x)) ! .
(2)"
PRELIMINARIES AND NOTATIONS,0.04893617021276596,Under review as a conference paper at ICLR 2022
PRELIMINARIES AND NOTATIONS,0.05106382978723404,"The objective function FS(w) is not convex since f(w; x) is non-convex in terms of w. To solve
the problem (1), one can simply use some iterative methods such as stochastic gradient descent
(SGD). Speciﬁcally, at each training iteration t, SGD updates solutions iteratively by wt+1 = wt −
η∇wℓ(yt, f(wt; xt)), where η > 0 is a learning rate."
PRELIMINARIES AND NOTATIONS,0.05319148936170213,"Next, we present some notations and assumptions that will be used in the convergence analysis.
Throughout this paper, we also make the following assumptions for solving the problem (1).
Assumption 1. Assume the following conditions hold:"
PRELIMINARIES AND NOTATIONS,0.05531914893617021,"(i) The stochastic gradient of FS(w) is unbiased, i.e., E(x,y)[∇ℓ(y, f(w; x))] = ∇FS(w),
and the variance of stochastic gradient is bounded, i.e., there exists a constant σ2 > 0,
such that E(x,y)
h
∥∇ℓ(y, f(w; x)) −∇FS(w)∥2i
= σ2."
PRELIMINARIES AND NOTATIONS,0.0574468085106383,"(ii) FS(w) is smooth with an L-Lipchitz continuous gradient, i.e., it is differentiable and there
exists a constant L > 0 such that ∥∇FS(w) −∇FS(u)∥≤L∥w −u∥, ∀w, u ∈W."
PRELIMINARIES AND NOTATIONS,0.059574468085106386,"Remark. Assumption 1 (i) and (ii) are commonly used assumptions in the literature of non-convex
optimization (Ghadimi & Lan, 2013; Yan et al., 2018; Yuan et al., 2019b; Wang et al., 2019; Li
et al., 2020a). Assumption 1 (ii) says the objective function is L-smooth, and it has an equivalent
expression (Nesterov, 2004) which is FS(w)−FS(u) ≤⟨∇FS(u), w−u⟩+ L"
PRELIMINARIES AND NOTATIONS,0.06170212765957447,"2 ∥w−u∥2,
∀w, u ∈
W. For a classiﬁcation problem, the smoothed label yLS is given by"
PRELIMINARIES AND NOTATIONS,0.06382978723404255,"yLS = (1 −θ)y + θby,
(3)
where θ ∈(0, 1) is the smoothing strength, y is the one-hot label, by is an introduced label. For
example, one can simply use by =
1
K (Szegedy et al., 2016) for K-class problems. Similar to label
y, we suppose the label by is drawn from a distribution Pby. We introduce the variance of stochastic
gradient using label by as follows."
PRELIMINARIES AND NOTATIONS,0.06595744680851064,"E(x,by)
h
∥∇ℓ(by, f(w; x)) −∇FS(w)∥2i
= bσ2 := δσ2.
(4)"
PRELIMINARIES AND NOTATIONS,0.06808510638297872,"where δ > 0 is a constant and σ2 is deﬁned in Assumption 1 (i). We make several remarks for (4).
Remark. (a) We do not require that the stochastic gradient ∇ℓ(by, f(w; x)) is unbiased, i.e., it could
be E[∇ℓ(by, f(w; x))] ̸= ∇FS(w). (b) The variance bσ2 is deﬁned based on the label by rather than
the smoothed label yLS. (c) We do not assume the variance bσ2 is bounded since δ could be an
arbitrary value, however, we will discuss the different cases of δ in our analysis. If δ ≥1, then
bσ2 ≥σ2; while if 0 < δ < 1, then bσ2 < σ2. It is worth mentioning that δ could be small when
an appropriate label is used in the label smoothing. For example, one can smooth labels by using
a teacher model (Hinton et al., 2014) or the model’s own distribution (Reed et al., 2014). In the
ﬁrst paper of label smoothing (Szegedy et al., 2016) and the following related studies (M¨uller et al.,
2019; Yuan et al., 2019a), researchers consider a uniform distribution over all K classes of labels
as the label by, i.e., set by =
1
K . Due to the space limitation, we include more discussions and the
evaluations of δ in real-world applications in Appendix A. For example, we show that δ < 1 for
CIFAR-100 in practice."
PRELIMINARIES AND NOTATIONS,0.07021276595744681,"We now introduce an important property regarding FS(w), i.e. Polyak-Łojasiewicz (PL) condi-
tion (Polyak, 1963). More speciﬁcally, the following assumption holds.
Assumption 2. There exists a constant µ
>
0 such that 2µ(FS(w) −FS(w∗
S))
≤
∥∇FS(w)∥2, ∀w ∈W, where w∗
S ∈minw∈W FS(w) is a optimal solution."
PRELIMINARIES AND NOTATIONS,0.07234042553191489,"Remark. This property has been theoretically and empirically observed in training deep neural
networks (Allen-Zhu et al., 2019; Yuan et al., 2019b). This condition is widely used to establish
convergence in the literature of non-convex optimization, please see (Yuan et al., 2019b; Wang
et al., 2019; Karimi et al., 2016; Li & Li, 2018; Charles & Papailiopoulos, 2018; Li et al., 2020a)
and references therein."
PRELIMINARIES AND NOTATIONS,0.07446808510638298,"To measure the convergence of non-convex and smooth optimization problems as in (Nesterov,
1998; Ghadimi & Lan, 2013; Yan et al., 2018), we need the following deﬁnition of the ﬁrst-order
stationary point.
Deﬁnition 1 (First-order stationary point). For the problem of minw∈W FS(w), a point w ∈W is
called a ﬁrst-order stationary point if ∥∇FS(w)∥= 0. Moreover, if ∥∇FS(w)∥≤ϵ, then the point
w is said to be an ϵ-stationary point, where ϵ ∈(0, 1) is a small positive value."
PRELIMINARIES AND NOTATIONS,0.07659574468085106,Under review as a conference paper at ICLR 2022
PRELIMINARIES AND NOTATIONS,0.07872340425531915,Algorithm 1 SGD with Label Smoothing Regularization
PRELIMINARIES AND NOTATIONS,0.08085106382978724,"1: Initialize: w0 ∈W, θ ∈(0, 1), set η as the value in Theorem 3.
2: for t = 0, 1, . . . , T −1 do
3:
sample (xt, yt), set yLS
t
= (1 −θ)yt + θbyt
4:
update wt+1 = wt −η∇wℓ(yLS
t , f(wt; xt))
5: end for
6: Output: wR, where R is uniformly sampled from {0, 1, . . . , T −1}."
PRELIMINARIES AND NOTATIONS,0.08297872340425531,"4
TSLA: A TWO-STAGE LABEL SMOOTHING ALGORITHM"
PRELIMINARIES AND NOTATIONS,0.0851063829787234,"In this section, we present our main method with its convergence analysis. As a warm-up, we ﬁrst
show the convergence analysis of SGD with LSR to understand LS from theoretical side. Then we
will introduce the proposed TSLA and study its convergence results both in training error (optimiza-
tion) and testing error (generalization)."
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.08723404255319149,"4.1
CONVERGENCE ANALYSIS OF SGD WITH LSR"
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.08936170212765958,"To understand LSR from the optimization perspective, we consider SGD with LSR in Algorithm 1
for the sake of simplicity. The only difference between Algorithm 1 and standard SGD is the use
of the output label for constructing a stochastic gradient. The following theorem shows that Al-
gorithm 1 converges to an approximate stationary point in expectation under some conditions. We
include its proof in Appendix C."
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.09148936170212765,"Theorem 3. Under Assumption 1, run Algorithm 1 with η
=
1
L and θ
=
1
1+δ, then"
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.09361702127659574,"ER[∥∇FS(wR)∥2] ≤
2FS(w0)"
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.09574468085106383,"ηT
+ 2δσ2, where R is uniformly sampled from {0, 1, . . . , T −1}.
Furthermore, given a target accuracy level ϵ, we have the following two results.
(1) when δ ≤
ϵ2
4σ2 , if we set T = 4FS(w0)"
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.09787234042553192,"ηϵ2
, then Algorithm 1 converges to an ϵ-stationary point in
expectation, i.e., ER[∥∇FS(wR)∥2] ≤ϵ2. The total sample complexity is T = O
  1"
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.1,"ϵ2

.
(2) when δ >
ϵ2
4σ2 , if we set T = FS(w0)"
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.10212765957446808,"ηδσ2 , then Algorithm 1 does not converge to an ϵ-stationary
point, but we have ER[∥∇FS(wR)∥2] ≤4δσ2 ≤O(δ)."
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.10425531914893617,"Remark. We observe that the variance term is 2δσ2, instead of ηLσ2 for standard analysis of
SGD without LSR (i.e., θ = 0, please see the detailed analysis of Theorem 6 in Appendix D).
For the convergence analysis, the difference between SGD with LSR and SGD without LSR is
that ∇ℓ(by, f(w; x)) is not an unbiased estimator of ∇FS(w) when using LSR. The convergence
behavior of Algorithm 1 heavily depends on the parameter δ. When δ is small enough, say δ ≤
O(ϵ2) with a small positive value ϵ ∈(0, 1), then Algorithm 1 converges to an ϵ-stationary point
with the total sample complexity of O
  1"
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.10638297872340426,"ϵ2

. Recall that the total sample complexity of standard
SGD without LSR for ﬁnding an ϵ-stationary point is O
  1"
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.10851063829787234,"ϵ4

((Ghadimi & Lan, 2016; Ghadimi
et al., 2016), please also see the detailed analysis of Theorem 6 in Appendix D). The convergence
result shows that if we could ﬁnd a label by that has a reasonably small amount of δ, we will be able to
reduce sample complexity for training a machine learning model from O
  1"
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.11063829787234042,"ϵ4

to O
  1"
CONVERGENCE ANALYSIS OF SGD WITH LSR,0.1127659574468085,"ϵ2

. Thus, the
reduction in variance will happen when an appropriate label smoothing with δ ∈(0, 1) is introduced.
We will ﬁnd in the empirical evaluations that different label by lead to different performances and an
appropriate selection of label by has a better performance (see the performances of LSR and LSR-pre
in Table 2). On the other hand, when the parameter δ is large such that δ > Ω(ϵ2), that is to say,
if an inappropriate label smoothing is used, then Algorithm 1 does not converge to an ϵ-stationary
point, but it converges to a worse level of O(δ), indicating that it may slow down the convergence at
the end of optimization."
THE TSLA ALGORITHM,0.1148936170212766,"4.2
THE TSLA ALGORITHM"
THE TSLA ALGORITHM,0.11702127659574468,"Previous subsection have shown that LSR could not ﬁnd an ϵ−stationary point under some situa-
tions. These motivate us to investigate a strategy that combines the algorithm with and without LSR
during the training progress. Let think in this way, one possible scenario is that training one-hot"
THE TSLA ALGORITHM,0.11914893617021277,Under review as a conference paper at ICLR 2022
THE TSLA ALGORITHM,0.12127659574468085,Algorithm 2 The TSLA algorithm
THE TSLA ALGORITHM,0.12340425531914893,"1: Initialize: w0 ∈W, T1, θ ∈(0, 1), η1, η2 > 0"
THE TSLA ALGORITHM,0.125531914893617,"// First stage: SGD with LSR
2: for t = 0, 1, . . . , T1 −1 do
3:
set yLS
t
= (1 −θ)yt + θbyt
4:
update wt+1 = wt −η1∇ℓ(yLS
t , f(wt; xt))
5: end for"
THE TSLA ALGORITHM,0.1276595744680851,"// Second stage: SGD without LSR
6: for t = T1, 1, . . . , T1 + T2 −1 do
7:
update wt+1 = wt −η2∇ℓ(yt, f(wt; xt)).
8: end for
9: Output: wR, where R is uniformly sampled from
{T1, . . . , T1 + T2 −1}.
Figure 1: Loss on ResNet-18 over CUB-2011."
THE TSLA ALGORITHM,0.12978723404255318,"label could be “easier” than training smoothed label 1. Taking the cross entropy loss in (2) for an
example, one need to optimize a single loss function −log

exp(fk(w; x))/ PK
j=1 exp(fj(w; x))
"
THE TSLA ALGORITHM,0.13191489361702127,"when one-hot label (e.g, yk = 1 and yi = 0 for all i ̸= k) is used, but need to optimize all K
loss functions −PK
i=1 yLS
i log

exp(fi(w; x))/ PK
j=1 exp(fj(w; x))

when smoothed label (e.g.,"
THE TSLA ALGORITHM,0.13404255319148936,yLS = (1 −θ)y + θ 1
THE TSLA ALGORITHM,0.13617021276595745,"K so that yLS
k = 1 −(K −1)θ/K and yLS
i
= θ/K for all i ̸= k) is used. Nev-
ertheless, training deep neural networks is gradually focusing on hard examples with the increase
of training epochs. It seems that training with smoothed label in the late epochs makes the learning
progress more difﬁcult. In addition, after LSR, we focus on optimizing the overall distribution that
contains the minor classes, which are probably not important at the end of training progress. Please
see the blue dashed line (trained by SGD with LSR) in Figure 12, and it shows that there is almost
no change in loss after 30 epochs. One question is whether LSR helps at the early training epochs
but it has less (even negative) effect during the later training epochs? This question encourages us to
propose and analyze a simple strategy with LSR dropping that switches a stochastic algorithm with
LSR to the algorithm without LSR."
THE TSLA ALGORITHM,0.13829787234042554,"A natural choice could be just dropping off LSR after certain epochs. For example, one can run
SGD with LSR in the ﬁrst 50 epochs and then run SGD without LSR after that (see red solid line in
Figure 1). From Figure 1, we can see that the drop of LSR after 50 epochs can further decrease the
loss function."
THE TSLA ALGORITHM,0.14042553191489363,"With this in mind in this subsection, we propose a generic framework that consists of two stages,
wherein the ﬁrst stage it runs SGD with LSR for T1 iterations and the second stage it runs SGD with-
out LSR up to T2 iterations. This framework is referred to as Two-Stage LAbel smoothing (TSLA)
algorithm, whose updating details are presented in Algorithm 2. Although SGD is considered as the
pipeline (Line 4 and Line 8 in Algorithm 2) in the convergence analysis, in practice, SGD can be re-
placed by any stochastic algorithms such as momentum SGD (Polyak, 1964), Stochastic Nesterov’s
Accelerated Gradient (Nesterov, 1983), and adaptive algorithms including ADAGRAD (Duchi et al.,
2011), RMSProp (Hinton et al., 2012a), AdaDelta (Zeiler, 2012), Adam (Kingma & Ba, 2015),
Nadam (Dozat, 2016) and AMSGRAD (Reddi et al., 2018). In this paper, we will not study the the-
oretical guarantees and empirical evaluations of other optimizers, which can be considered as future
work. Please note that the algorithm can use different learning rates η1 and η2 during the two stages.
The last solution of the ﬁrst stage will be used as the initial solution of the second stage. If T1 = 0,
then TSLA reduces to the baseline, i.e., SGD without LSR; while if T2 = 0, TSLA becomes to LSR
method, i.e., SGD with LSR."
THE TSLA ALGORITHM,0.1425531914893617,"1While this is not a rigorous mathematical conclusion, we want to roughly show that optimizing loss of soft
label has to consider minimizing the combination of K functions while one-hot loss only needs to focus on
one. For soft label, each fj appears both in numerator (need to maximize) and denominator (need to minimize)
of the loss function, while for one-hot label, fj, j ̸= k only appears in denominator (need to minimize) of the
loss function, indicating that one-hot label has less “constraints” than soft label.
2The blue dashed line is trained by SGD with LSR. The red solid line is trained by SGD with LSR for the
ﬁrst 50 epochs and trained by SGD without LSR (i.e., using one-hot label) for the last 40 epochs."
THE TSLA ALGORITHM,0.14468085106382977,Under review as a conference paper at ICLR 2022
OPTIMIZATION RESULT OF TSLA,0.14680851063829786,"4.3
OPTIMIZATION RESULT OF TSLA"
OPTIMIZATION RESULT OF TSLA,0.14893617021276595,"In this subsection, we will give the convergence result of the proposed TSLA algorithm. For sim-
plicity, we use SGD as the subroutine algorithm A in the analysis. The convergence result in the
following theorem shows the power of LSR from the optimization perspective. Its proof is presented
in Appendix E. It is easy to see from the proof that by using the last output of the ﬁrst stage as the
initial point of the second stage, TSLA can enjoy the advantage of LSR in the second stage with an
improved convergence.
Theorem 4. Under Assumptions 1, 2, suppose σ2δ/µ ≤F(w0), run Algorithm 2, θ =
1
1+δ, η1 ="
OPTIMIZATION RESULT OF TSLA,0.15106382978723404,"1
L, T1 = log

2µF (w0)(1+δ)"
OPTIMIZATION RESULT OF TSLA,0.15319148936170213,"2δσ2

/(η1µ), η2 =
ϵ2
2Lσ2 and T2 =
8δσ2
µη2ϵ2 , then ER[∥∇FS(wR)∥2] ≤ϵ2,"
OPTIMIZATION RESULT OF TSLA,0.15531914893617021,"where R is uniformly sampled from {T1, . . . , T1 + T2 −1}."
OPTIMIZATION RESULT OF TSLA,0.1574468085106383,"Remark. It is obvious that the learning rate η2 in the second stage is roughly smaller than the
learning rate η1 in the ﬁrst stage, which matches the widely used stage-wise learning rate decay
scheme in training neural networks. To explore the total sample complexity of TSLA, we consider
different conditions on δ. For a ﬁxed the target convergence level ϵ ∈(0, 1), let us discuss the total
sample complexities of ﬁnding ϵ-stationary points for SGD with TSLA (TSLA), SGD with LSR
(LSR), and SGD without LSR (baseline), where we only consider the orders of the complexities
but ignore all constants. When Ω(ϵ2) < δ < 1, LSR does not converge to an ϵ-stationary point,
while TSLA reduces sample complexity from O
  1"
OPTIMIZATION RESULT OF TSLA,0.1595744680851064,"ϵ4

to O
  δ"
OPTIMIZATION RESULT OF TSLA,0.16170212765957448,"ϵ4

, compared to the baseline. When
δ < O(ϵ2), the total complexity of TSLA is between log(1/ϵ) and 1/ϵ2, which is always better than
LSR and the baseline. In summary, TSLA achieves the best total sample complexity by enjoying
the good property of an appropriate label smoothing (i.e., when 0 < δ < 1). However, when δ ≥1,
baseline has better convergence than TSLA, meaning that the selection of label by is not appropriate.
Since T1 contains unknown parameters, it is difﬁculty to know its ground-truth value. However, we
can tune different values in practice."
GENERALIZATION RESULT OF TSLA,0.16382978723404254,"4.4
GENERALIZATION RESULT OF TSLA
In this subsection, we study the generalization result of TSLA. First, we give some notations, where
most of them are followed by (Hardt et al., 2015; Yuan et al., 2019b). Let wS = A(S) be a solution
that generated by a random algorithm A based on dataset S. Recall that problem (1) is called
empirical risk minimization in the literature, and the true risk minimization is given by"
GENERALIZATION RESULT OF TSLA,0.16595744680851063,"min
w∈W F(w) := E(x,y) [ℓ(y, f(w; x))] .
(5)"
GENERALIZATION RESULT OF TSLA,0.16808510638297872,Then the testing error is dedined as
GENERALIZATION RESULT OF TSLA,0.1702127659574468,"EA,S[F(wS)] −ES[FS(w∗
S)].
(6)"
GENERALIZATION RESULT OF TSLA,0.1723404255319149,"We notice that there are several works (Hardt et al., 2015; Yuan et al., 2019b) study the testing error
result of SGD for non-convex setting under different conditions such as bounded stochastic gradient
∥∇wℓ(y, f(w; x))∥≤G and decaying learning rate ηt ≤c"
GENERALIZATION RESULT OF TSLA,0.17446808510638298,"t with a constant c > 0, where t is the
optimization iteration. In this paper, we are not interested in establishing fast rate under different
conditions, but we want to explore the generalization ability of TSLA with the fewest possible
modiﬁcations when building a bridge between theory and practice. For example, weight decay is a
widely used trick when training deep neural networks. With the use of weight decay, the empirical
risk minimization in practice becomes minw∈W{ bFS(w) := FS(w) + λ"
GENERALIZATION RESULT OF TSLA,0.17659574468085107,"2 ∥w∥2}. We present the
ERB of TSLA in the following Theorem 5, whose proofs can be found in the Appendix F.
Theorem 5. Under Assumption 1, assume that ℓ(y, f(w, x)) is L-smooth and B-Lipschitz, suppose
bFS(w) satisﬁes Assumption 2 and minw∈W bFS(w) ≤FS(w∗
S) + λ"
GENERALIZATION RESULT OF TSLA,0.17872340425531916,"2 ∥wt∥2 with λ = 2L, where wt
is the intermediate solution in the second stage of Algorithm 2 by running with θ =
1
1+δ, η1 <"
GENERALIZATION RESULT OF TSLA,0.18085106382978725,"1
3L, T1 = log

2µFS(w0)(1+δ)"
GENERALIZATION RESULT OF TSLA,0.1829787234042553,"2δσ2

/(η1µ), η2 = O(1/√n) and T2 = O(δn) then the testing error"
GENERALIZATION RESULT OF TSLA,0.1851063829787234,"ER,A,S[F(wR)] −ES[FS(w∗
S)] ≤O(1/√n), where A is TSLA.
Remark. Theorem 5 shows that in order to have an bound of testing error in the order of O(1/√n),
TSLA needs to run T = O(δn) iterations. For the standard analysis of SGD with/without LSR
(please see the remarks in Appendix F) under the same conditions, SGD without LSR needs to run
T = O(n) iterations to obtain an O(1/√n) bound of testing error. Thus, if δ is small enough, the"
GENERALIZATION RESULT OF TSLA,0.18723404255319148,Under review as a conference paper at ICLR 2022
GENERALIZATION RESULT OF TSLA,0.18936170212765957,"Table 1: Comparison of Testing Accuracy for Different Methods (mean ± standard deviation, in %)."
GENERALIZATION RESULT OF TSLA,0.19148936170212766,"Stanford Dogs
CUB-2011
Algorithm∗
Top-1 accuracy
Top-5 accuracy
Top-1 accuracy
Top-5 accuracy
baseline
82.31 ± 0.18
97.76 ± 0.06
75.31 ± 0.25
93.14 ± 0.31
LSR
82.80 ± 0.07
97.41 ± 0.09
76.97 ± 0.19
92.73 ± 0.12
TSLA(20)
83.15 ± 0.02
97.91 ± 0.08
76.62 ± 0.15
93.60 ± 0.18
TSLA(30)
83.89 ± 0.16
98.05 ± 0.08
77.44 ± 0.19
93.92 ± 0.16
TSLA(40)
83.93 ± 0.13
98.03 ± 0.05
77.50 ± 0.20
93.99 ± 0.11
TSLA(50)
83.91 ± 0.15
98.07 ± 0.06
77.57 ± 0.21
93.86 ± 0.14
TSLA(60)
83.51 ± 0.11
97.99 ± 0.06
77.25 ± 0.29
94.43 ± 0.18
TSLA(70)
83.38 ± 0.09
97.90 ± 0.09
77.21 ± 0.15
93.31 ± 0.12
TSLA(80)
83.14 ± 0.09
97.73 ± 0.07
77.05 ± 0.14
93.05 ± 0.08"
GENERALIZATION RESULT OF TSLA,0.19361702127659575,∗TSLA(s): TSLA drops off LSR after epoch s.
GENERALIZATION RESULT OF TSLA,0.19574468085106383,"testing error of TSLA is better than that of SGD without LSR, otherwise, SGD without LSR has
better testing error than TSLA. For SGD with LSR, when δ > Ω(1/√n), the bound of testing error
for LSR can not be bounded by O(1/√n); when δ ≤Ω(1/√n), the bound for LSR can be bounded
by O(1/√n) in T = O(√n) iterations. This shows that TSLA bas better testing error than LSR."
EXPERIMENTS,0.19787234042553192,"5
EXPERIMENTS"
EXPERIMENTS,0.2,"To further evaluate the performance of the proposed TSLA method, we trained deep neural networks
on three benchmark data sets, CIFAR-100 (Krizhevsky & Hinton, 2009), Stanford Dogs (Khosla
et al., 2011) and CUB-2011 (Wah et al., 2011), for image classiﬁcation tasks. CIFAR-100 3 has
50,000 training images and 10,000 testing images of 32×32 resolution with 100 classes. Stanford
Dogs data set 4 contains 20,580 images of 120 breeds of dogs, where 100 images from each breed
is used for training. CUB-2011 5 is a birds image data set with 11,788 images of 200 birds species.
The ResNet-18 model (He et al., 2016) is applied as the backbone in the experiments."
EXPERIMENTS,0.20212765957446807,"We compare the proposed TSLA incorporating with SGD (TSLA) with two baselines, SGD with
LSR (LSR) and SGD without LSR (baseline). The mini-batch size of training instances for all
methods is 256 as suggested by (He et al., 2019; 2016). The momentum parameter is ﬁxed as 0.9.
We will include more details of experimental settings in Appendix A."
EXPERIMENTS,0.20425531914893616,"5.1
STANFORD DOGS AND CUB-2011"
EXPERIMENTS,0.20638297872340425,"We separately train ResNet-18 (He et al., 2016) up to 90 epochs over two data sets Stanford Dogs
and CUB-2011. We use weight decay with the parameter value of 10−4. For all algorithms, the
initial learning rates for FC are set to be 0.1, while that for the pre-trained backbones are 0.001 and
0.01 for Standford Dogs and CUB-2011, respectively. The learning rates are divided by 10 every 30
epochs. For LSR, we ﬁx the value of smoothing strength θ = 0.4 for the best performance, and the
label by used for label smoothing is set to be a uniform distribution over all K classes, i.e., by =
1
K .
The same values of the smoothing strength θ and the same by are used during the ﬁrst stage of TSLA.
For TSLA, we drop off the LSR (i.e., let θ = 0) after s epochs during the training process, where
s ∈{20, 30, 40, 50, 60, 70, 80}."
EXPERIMENTS,0.20851063829787234,"We ﬁrst report the highest top-1 and top-5 accuracy on the testing data sets for different methods.
All top-1 and top-5 accuracy are averaged over 5 independent random trails with their standard
deviations. The results of the comparison are summarized in Table 1, where the notation “TSLA(s)”
means that the TSLA algorithm drops off LSR after epoch s. It can be seen from Table 1 that
under an appropriate hyperparameter setting the models trained using TSLA outperform that trained
using LSR and baseline, which supports the convergence result in Section 4.2. We notice that
the best top-1 accuracy of TSLA are TSLA(40) and TSLA(50) for Stanford Dogs and CUB-2011,"
EXPERIMENTS,0.21063829787234042,"3https://www.cs.toronto.edu/˜kriz/cifar.html
4http://vision.stanford.edu/aditya86/ImageNetDogs/
5http://www.vision.caltech.edu/visipedia/CUB-200.html"
EXPERIMENTS,0.2127659574468085,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2148936170212766,"20
30
40
50
60
70
80
90
# epoch 79 80 81 82 83 84"
EXPERIMENTS,0.2170212765957447,Top-1 Accuracy
EXPERIMENTS,0.21914893617021278,"TSLA (30)
TSLA (40)
TSLA (50)
TSLA (60)"
EXPERIMENTS,0.22127659574468084,"TSLA (70)
TSLA (80)
LSR
baseline"
EXPERIMENTS,0.22340425531914893,Stanford Dogs
EXPERIMENTS,0.225531914893617,"20
30
40
50
60
70
80
90
# epoch 97 98"
EXPERIMENTS,0.2276595744680851,Top-5 Accuracy
EXPERIMENTS,0.2297872340425532,"TSLA (30)
TSLA (40)
TSLA (50)
TSLA (60)"
EXPERIMENTS,0.23191489361702128,"TSLA (70)
TSLA (80)
LSR
baseline"
EXPERIMENTS,0.23404255319148937,Stanford Dogs
EXPERIMENTS,0.23617021276595745,"0
20
40
60
80
# epoch 0.6 0.8 1.0 1.2 1.4 Loss"
EXPERIMENTS,0.23829787234042554,"TSLA (30)
TSLA (40)
TSLA (50)
TSLA (60)"
EXPERIMENTS,0.2404255319148936,"TSLA (70)
TSLA (80)
LSR
baseline"
EXPERIMENTS,0.2425531914893617,Stanford Dogs
EXPERIMENTS,0.24468085106382978,"20
30
40
50
60
70
80
90
# epoch 74 75 76 77"
EXPERIMENTS,0.24680851063829787,Top-1 Accuracy
EXPERIMENTS,0.24893617021276596,"TSLA (30)
TSLA (40)
TSLA (50)
TSLA (60)"
EXPERIMENTS,0.251063829787234,"TSLA (70)
TSLA (80)
LSR
baseline"
EXPERIMENTS,0.2531914893617021,CUB-2011
EXPERIMENTS,0.2553191489361702,"20
30
40
50
60
70
80
90
# epoch 92 93 94"
EXPERIMENTS,0.2574468085106383,Top-5 Accuracy
EXPERIMENTS,0.25957446808510637,"TSLA (30)
TSLA (40)
TSLA (50)
TSLA (60)"
EXPERIMENTS,0.26170212765957446,"TSLA (70)
TSLA (80)
LSR
baseline"
EXPERIMENTS,0.26382978723404255,CUB-2011
EXPERIMENTS,0.26595744680851063,"0
20
40
60
80
# epoch 1.0 1.5 2.0 2.5 3.0 Loss"
EXPERIMENTS,0.2680851063829787,"TSLA (30)
TSLA (40)
TSLA (50)
TSLA (60)"
EXPERIMENTS,0.2702127659574468,"TSLA (70)
TSLA (80)
LSR
baseline"
EXPERIMENTS,0.2723404255319149,CUB-2011
EXPERIMENTS,0.274468085106383,"Figure 2: Testing Top-1, Top-5 Accuracy and Loss on ResNet-18 over Stanford Dogs and CUB-
2011. TSLA(s) means TSLA drops off LSR after epoch s."
EXPERIMENTS,0.2765957446808511,"respectively, meaning that the performance of TSLA(s) is not monotonic over the dropping epoch s.
For CUB-2011, the top-1 accuracy of TSLA(20) is smaller than that of LSR. This result matches the
convergence analysis of TSLA showing that it can not drop off LSR too early. For top-5 accuracy,
we found that TSLA(80) is slightly worse than baseline. This is because of dropping LSR too late
so that the update iterations (i.e., T2) in the second stage of TSLA is too small to converge to a good
solution. We also observe that LSR is better than baseline regarding top-1 accuracy but the result is
opposite as to top-5 accuracy."
EXPERIMENTS,0.27872340425531916,"We then plot the averaged top-1 accuracy, averaged top-5 accuracy, and averaged loss among 5 trails
of different methods in Figure 2. We remove the results for TSLA(20) since it dropped off LSR too
early as mentioned before. The ﬁgure shows TSLA improves the top-1 and top-5 testing accuracy
immediately once it drops off LSR. Although TSLA may not converge if it drops off LSR too late,
see TSLA(60), TSLA(70), and TSLA(80) from the third column of Figure 2, it still has the best
performance compared to LSR and baseline. TSLA(30), TSLA(40), and TSLA(50) can converge to
lower objective levels, comparing to LSR and baseline."
EXPERIMENTS,0.28085106382978725,"5.2
CIFAR-100"
EXPERIMENTS,0.28297872340425534,"60
80
100
120
140
160
180
200
# epoch 74 75 76 77 78"
EXPERIMENTS,0.2851063829787234,Top-1 Accuracy
EXPERIMENTS,0.2872340425531915,"TSLA (180)
TSLA (160)
TSLA (140)
TSLA (120)
LSR
TSLA-pre (180)"
EXPERIMENTS,0.28936170212765955,"TSLA-pre (160)
TSLA-pre (140)
TSLA-pre (120)
LSR-pre
baseline"
EXPERIMENTS,0.29148936170212764,cifar100
EXPERIMENTS,0.2936170212765957,"60
80
100
120
140
160
180
200
# epoch 92 93 94 95"
EXPERIMENTS,0.2957446808510638,Top-5 Accuracy
EXPERIMENTS,0.2978723404255319,"TSLA (180)
TSLA (160)
TSLA (140)
TSLA (120)
LSR
TSLA-pre (180)"
EXPERIMENTS,0.3,"TSLA-pre (160)
TSLA-pre (140)
TSLA-pre (120)
LSR-pre
baseline"
EXPERIMENTS,0.3021276595744681,cifar100
EXPERIMENTS,0.30425531914893617,"0
25
50
75
100
125
150
175
200
# epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Loss"
EXPERIMENTS,0.30638297872340425,"TSLA (180)
TSLA (160)
TSLA (140)
TSLA (120)
LSR
TSLA-pre (180)"
EXPERIMENTS,0.30851063829787234,"TSLA-pre (160)
TSLA-pre (140)
TSLA-pre (120)
LSR-pre
baseline"
EXPERIMENTS,0.31063829787234043,cifar100
EXPERIMENTS,0.3127659574468085,"Figure 3: Testing Top-1, Top-5 Accuracy and Loss on ResNet-18 over CIFAR-100. TSLA(s)/TSLA-
pre(s) meansTSLA/TSLA-pre drops off LSR/LSR-pre after epoch s."
EXPERIMENTS,0.3148936170212766,"The total epochs of training ResNet-18 (He et al., 2016) on CIFAR-100 is set to be 200.The weight
decay with the parameter value of 5 × 10−4 is used. We use 0.1 as the initial learning rates for all
algorithms and divide them by 10 every 60 epochs as suggested in (He et al., 2016; Zagoruyko &
Komodakis, 2016). For LSR and the ﬁrst stage of TSLA, the value of smoothing strength θ is ﬁxed
as θ = 0.1, which shows the best performance for LSR. We use two different labels by to smooth the"
EXPERIMENTS,0.3170212765957447,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.3191489361702128,"one-hot label, the uniform distribution over all labels and the distribution predicted by an ImageNet
pre-trained model which is downloaded directly from PyTorch 6 (Paszke et al., 2019). For TSLA, we
try to drop off the LSR after s epochs during the training process, where s ∈{120, 140, 160, 180}."
EXPERIMENTS,0.32127659574468087,"Table 2: Comparison of Testing Accuracy for Different
Methods (mean ± standard deviation, in %)."
EXPERIMENTS,0.32340425531914896,"CIFAR-100
Algorithm∗
Top-1 accuracy
Top-5 accuracy
baseline
76.87 ± 0.04
93.47 ± 0.15
LSR
77.77 ± 0.18
93.55 ± 0.11
TSLA(120)
77.92 ± 0.21
94.13 ± 0.23
TSLA(140)
77.93 ± 0.19
94.11 ± 0.22
TSLA(160)
77.96 ± 0.20
94.19 ± 0.21
TSLA(180)
78.04 ± 0.27
94.23 ± 0.15
LSR-pre
78.07 ± 0.31
94.70 ± 0.14
TSLA-pre(120)
78.34 ± 0.31
94.68 ± 0.14
TSLA-pre(140)
78.39 ± 0.25
94.73 ± 0.11
TSLA-pre(160)
78.55 ± 0.28
94.83 ± 0.08
TSLA-pre(180)
78.53 ± 0.23
94.96 ± 0.23"
EXPERIMENTS,0.32553191489361705,"∗TSLA(s)/TSLA-pre(s):
TSLA/TSLA-pre
drops
off
LSR/LSR-pre after epoch s."
EXPERIMENTS,0.3276595744680851,"All top-1 and top-5 accuracy on the test-
ing data set are averaged over 5 indepen-
dent random trails with their standard
deviations.
We summarize the results
in Table 2, where LSR-pre and TSLA-
pre indicate LSR and TSLA use the la-
bel by based on the ImageNet pre-trained
model.
The results show that LSR-
pre/TSLA-pre has a better performance
than LSR/TSLA. The reason might be
that the pre-trained model-based pre-
diction is closer to the ground truth
than the uniform prediction and it has
lower variance (smaller δ). Then, TSLA
(LSR) with such pre-trained model-
based prediction converges faster than
TSLA (LSR) with uniform prediction,
which veriﬁes our theoretical ﬁndings in
Section 4.2 (Section 4.1). This observa-
tion also empirically tells us the selec-
tion of the prediction function by used for smoothing label is the key to the success of TSLA as
well as LSR. Among all methods, the performance of TSLA-pre is the best. For top-1 accuracy,
TSLA-pre(160) outperforms all other algorithms, while for top-5 accuracy, TSLA-pre(180) has the
best performance."
EXPERIMENTS,0.32978723404255317,"Finally, we observe from Figure 3 that both TSLA and TSLA-pre converge, while TSLA-pre con-
verges to the lowest objective value. Similarly, the top-1 and top-5 accuracies show the improve-
ments of TSLA and TSLA-pre at the point of dropping off LSR."
EXPERIMENTS,0.33191489361702126,"Table 3: Comparison of Testing Top-1 Ac-
curacy for Different θ of LSR and TSLA
(in %)."
EXPERIMENTS,0.33404255319148934,"θ
0.2
0.4
0.9
LSR
77.75
77.72
76.40
TSLA(120)
77.92
77.68
76.05
TSLA(140)
78.06
77.61
76.27
TSLA(160)
78.09
77.54
76.37
TSLA(180)
78.10
77.89
76.60"
EXPERIMENTS,0.33617021276595743,∗TSLA(s): TSLA drops off LSR after epoch s.
EXPERIMENTS,0.3382978723404255,"We conduct an ablation study for the smoothing pa-
rameter θ and the dropping epoch s in TSLA(s). We
follow the same settings in Subsection 5.2 but use
different values of θ in LSR and TSLA. Speciﬁcally,
θ ∈{0.2, 0.4, 0.9}. We use the uniform distribution
over all labels to smooth the one-hot label. The results
are summarized in Table 3, showing that the different
values of θ and s can affect the performances of LSR
and TSLA. With the increase of θ, the performances
become worse. However, with different values of θ,
TSLA can always outperform LSR when an appropri-
ate dropping epoch s is selected. Besides, TSLA(180)
has the best performance for each value of θ."
CONCLUSIONS,0.3404255319148936,"6
CONCLUSIONS"
CONCLUSIONS,0.3425531914893617,"In this paper, we have studied the power of LSR in training deep neural networks by analyzing
SGD with LSR in different non-convex optimization settings. The convergence results show that
an appropriate LSR with reduced label variance can help speed up the convergence. We have pro-
posed a simple yet efﬁcient strategy so-called TSLA whose basic idea is to switch the training from
smoothed label to one-hot label. Integrating TSLA with SGD, we observe from its improved con-
vergence result that TSLA beneﬁts from LSR in the ﬁrst stage and essentially converges faster in the
second stage. Our theoretical result also shows that TSLA has better testing error than that of LSR.
Throughout extensive experiments, we have shown that TSLA improves the generalization accuracy
of deep models on several benchmark data sets."
CONCLUSIONS,0.3446808510638298,6https://pytorch.org/docs/stable/torchvision/models.html
CONCLUSIONS,0.3468085106382979,Under review as a conference paper at ICLR 2022
REFERENCES,0.34893617021276596,REFERENCES
REFERENCES,0.35106382978723405,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242–252, 2019."
REFERENCES,0.35319148936170214,"Hessam Bagherinezhad, Maxwell Horton, Mohammad Rastegari, and Ali Farhadi. Label reﬁnery:
Improving imagenet classiﬁcation through label progression. arXiv preprint arXiv:1805.02641,
2018."
REFERENCES,0.3553191489361702,"Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, pp. 745–754,
2018."
REFERENCES,0.3574468085106383,"Jan Chorowski and Navdeep Jaitly. Towards better decoding and language model integration in
sequence to sequence models. Proc. Interspeech 2017, pp. 523–527, 2017."
REFERENCES,0.3595744680851064,"Qianggang Ding, Sifan Wu, Hao Sun, Jiadong Guo, and Shu-Tao Xia. Adaptive regularization of
labels. arXiv preprint arXiv:1908.05474, 2019."
REFERENCES,0.3617021276595745,Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
REFERENCES,0.3638297872340426,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011."
REFERENCES,0.3659574468085106,"Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013."
REFERENCES,0.3680851063829787,"Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Math. Program., 156(1-2):59–99, 2016."
REFERENCES,0.3702127659574468,"Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267–305, 2016."
REFERENCES,0.3723404255319149,"Morgane Goibert and Elvis Dohmatob.
Adversarial robustness via adversarial label-smoothing.
arXiv preprint arXiv:1906.11567, 2019."
REFERENCES,0.37446808510638296,"Moritz Hardt, Benjamin Recht, and Yoram Singer.
Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015."
REFERENCES,0.37659574468085105,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016."
REFERENCES,0.37872340425531914,"Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for
image classiﬁcation with convolutional neural networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 558–567, 2019."
REFERENCES,0.38085106382978723,"Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. 2012a."
REFERENCES,0.3829787234042553,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In
NeurIPS Deep Learning Workshop, 2014."
REFERENCES,0.3851063829787234,"Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012b."
REFERENCES,0.3872340425531915,"Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017."
REFERENCES,0.3893617021276596,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015."
REFERENCES,0.39148936170212767,Under review as a conference paper at ICLR 2022
REFERENCES,0.39361702127659576,"Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-łojasiewicz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pp. 795–811. Springer, 2016."
REFERENCES,0.39574468085106385,"Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for ﬁne-
grained image categorization: Stanford dogs. In Proc. CVPR Workshop on Fine-Grained Visual
Categorization (FGVC), volume 2, 2011."
REFERENCES,0.39787234042553193,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015."
REFERENCES,0.4,"Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Mas-
ter’s thesis, Technical report, University of Tronto, 2009."
REFERENCES,0.4021276595744681,"Xiaoyu Li, Zhenxun Zhuang, and Francesco Orabona. Exponential step sizes for non-convex opti-
mization. arXiv preprint arXiv:2002.05273, 2020a."
REFERENCES,0.40425531914893614,"Xingjian Li, Haoyi Xiong, Haozhe An, Dejing Dou, and Chengzhong Xu.
Colam:
Co-
learning of deep neural networks and soft labels via alternating minimization. arXiv preprint
arXiv:2004.12443, 2020b."
REFERENCES,0.40638297872340423,"Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex
optimization. In Advances in Neural Information Processing Systems, pp. 5564–5574, 2018."
REFERENCES,0.4085106382978723,"Michal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon, and Sanjiv Kumar.
Does label
smoothing mitigate label noise? arXiv preprint arXiv:2003.02819, 2020."
REFERENCES,0.4106382978723404,"Rafael M¨uller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help?
In
Advances in Neural Information Processing Systems, pp. 4696–4705, 2019."
REFERENCES,0.4127659574468085,"Yurii Nesterov.
A method of solving a convex programming problem with convergence rate
O(1/k2). Soviet Mathematics Doklady, 27:372–376, 1983."
REFERENCES,0.4148936170212766,Yurii Nesterov. Introductory lectures on convex programming volume i: Basic course. 1998.
REFERENCES,0.41702127659574467,"Yurii Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization.
Kluwer Academic Publ., 2004. ISBN 1-4020-7553-7."
REFERENCES,0.41914893617021276,"Toan Q Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of
self-attention. arXiv preprint arXiv:1910.05895, 2019."
REFERENCES,0.42127659574468085,"Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial exam-
ples. In Advances in Neural Information Processing Systems, pp. 4579–4589, 2018."
REFERENCES,0.42340425531914894,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems,
pp. 8024–8035, 2019.
URL https://pytorch.org/docs/stable/torchvision/
models.html."
REFERENCES,0.425531914893617,"Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548,
2017."
REFERENCES,0.4276595744680851,"Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1–17, 1964."
REFERENCES,0.4297872340425532,"Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi
Matematiki i Matematicheskoi Fiziki, 3(4):643–653, 1963."
REFERENCES,0.4319148936170213,"Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018."
REFERENCES,0.4340425531914894,Under review as a conference paper at ICLR 2022
REFERENCES,0.43617021276595747,"Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596, 2014."
REFERENCES,0.43829787234042555,"Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400–407, 1951."
REFERENCES,0.44042553191489364,"Chaomin Shen, Yaxin Peng, Guixu Zhang, and Jinsong Fan.
Defending against adversarial
attacks by suppressing the largest eigenvalue of ﬁsher information matrix.
arXiv preprint
arXiv:1909.06137, 2019."
REFERENCES,0.4425531914893617,"Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance
in pattern recognition—tangent distance and tangent propagation. In Neural networks: tricks of
the trade, pp. 239–274. Springer, 1998."
REFERENCES,0.44468085106382976,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.44680851063829785,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015."
REFERENCES,0.44893617021276594,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2818–2826, 2016."
REFERENCES,0.451063829787234,"Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural net-
works. In International Conference on Machine Learning, pp. 6105–6114. PMLR, 2019."
REFERENCES,0.4531914893617021,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 5998–6008, 2017."
REFERENCES,0.4553191489361702,"C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011
Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011."
REFERENCES,0.4574468085106383,"Shuo Wang, Zhaopeng Tu, Shuming Shi, and Yang Liu. On the inference calibration of neural
machine translation. arXiv preprint arXiv:2005.00963, 2020."
REFERENCES,0.4595744680851064,"Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum:
Faster variance reduction algorithms. In Advances in Neural Information Processing Systems, pp.
2403–2413, 2019."
REFERENCES,0.46170212765957447,"Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, and Qi Tian.
Disturblabel: Regularizing
cnn on the loss layer. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4753–4762, 2016."
REFERENCES,0.46382978723404256,"Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A uniﬁed analysis of stochastic momen-
tum methods for deep learning. In International Joint Conference on Artiﬁcial Intelligence, pp.
2955–2961, 2018."
REFERENCES,0.46595744680851064,"Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisit knowledge distillation: a
teacher-free framework. arXiv preprint arXiv:1909.11723, 2019a."
REFERENCES,0.46808510638297873,"Zhuoning Yuan, Yan Yan, Rong Jin, and Tianbao Yang. Stagewise training accelerates convergence
of testing error over sgd. In Advances in Neural Information Processing Systems, pp. 2604–2614,
2019b."
REFERENCES,0.4702127659574468,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. British Machine Vision Confer-
ence, 2016."
REFERENCES,0.4723404255319149,"Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012."
REFERENCES,0.474468085106383,Under review as a conference paper at ICLR 2022
REFERENCES,0.4765957446808511,"Albert Zeyer, Kazuki Irie, Ralf Schl¨uter, and Hermann Ney. Improved training of end-to-end atten-
tion models for speech recognition. Proc. Interspeech 2018, pp. 7–11, 2018."
REFERENCES,0.4787234042553192,"Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018."
REFERENCES,0.4808510638297872,"Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 8697–8710, 2018."
REFERENCES,0.4829787234042553,Under review as a conference paper at ICLR 2022
REFERENCES,0.4851063829787234,"A
ADDITIONAL EXPERIMENTS"
REFERENCES,0.48723404255319147,"All experiments are implemented using PyTorch7 with version 1.7.1 on a machine with 8 NVIDIA
P100 GPUs. The code is available at"
REFERENCES,0.48936170212765956,"https://drive.google.com/drive/folders/1QLvSE6iB2YN7UBpRMKB_
kKQGZzK7M4Qr?usp=sharing"
REFERENCES,0.49148936170212765,"A.1
THE ESTIMATION OF δ"
REFERENCES,0.49361702127659574,"As mentioned in previsous sections, the value of δ plays a very important role on the theoretical
results. In this section, we provide the estimation of δ for three data sets Stanford Dogs, CUB-2011
and CIFAR-100 that used in the experiments. Our results empirically show that δ < 1 for all three
data sets. Speciﬁcally, we computed the sample variances (denoted by σ2
k and bσ2
k, k = 10, 20, . . .
) of two variance σ2 and bσ2 every 10 epochs, then based on the deﬁnitions in (4), the etimation
(denoted by eδ) of δ was taken as"
REFERENCES,0.4957446808510638,"eδ = max
k
bσ2
k
σ2
k
."
REFERENCES,0.4978723404255319,"All results of estimated δ are listed in Table 4, empirically showing that eδ < 1. We also plot the
estimated values of δ across iterations over three different data sets in Figure 4, showing that eδ < 1
for all data sets across iterations."
REFERENCES,0.5,"Table 4: The estimated value of δ for different data sets
dataset
Stanford Dogs
CUB-2011
CIFAR-100
CIFAR-100 (pretrained)
eδ
0.16
0.13
0.55
0.38"
REFERENCES,0.502127659574468,"10
20
30
40
50
60
70
80
90
# epoch 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.5042553191489362,"CUB-2011
Stanford Dogs"
REFERENCES,0.5063829787234042,value of δ
REFERENCES,0.5085106382978724,"25
50
75
100
125
150
175
200
# epoch 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
REFERENCES,0.5106382978723404,"uniform
pretrained"
REFERENCES,0.5127659574468085,"value of δ, CIFAR-100"
REFERENCES,0.5148936170212766,Figure 4: The estimated values of δ for different data sets.
REFERENCES,0.5170212765957447,"A.2
MORE RESULTS ON DIFFERENT MODELS AND DATASETS"
REFERENCES,0.5191489361702127,"More experiments on different models and datasets are included in this subsection. First, we trained
models including ResNet-50, VGG (Simonyan & Zisserman, 2014), MobileNet (Howard et al.,"
REFERENCES,0.5212765957446809,"7A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L.
Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural
Information Processing Systems, 32: 8026–8037, 2019. https://pytorch.org"
REFERENCES,0.5234042553191489,Under review as a conference paper at ICLR 2022
REFERENCES,0.5255319148936171,"2017), Inception (Szegedy et al., 2015) and EfﬁcientNet (Tan & Le, 2019) on three datasets CI-
FAR100, CUB-2011 and Stanford Dogs. We followed the same hyper-parameter setting as in the
main paper, except for the smoothing strength θ in LSR. We tuned the parameter θ for LSR and used
the one with best performance. For TSLA, the same values of smooth strength θ are used during the
ﬁrst stage of TSLA. The speciﬁc values of θ for different models and datasets are listed in Table 5.
For the simplicity of presentation, we only report the best performance of TSLA among different
drop epochs s, where the values of used hyper-parameter s are listed in Table 5."
REFERENCES,0.5276595744680851,Table 5: The hyper-parameters θ and s for different models and datasets∗
REFERENCES,0.5297872340425532,"θ
s
Model
CUB
Dogs
CIFAR
CUB
Dogs
CIFAR
ResNet-50
0.4
0.5
0.4
70
50
180
VGG
0.4
0.4
0.4
50
30
180
MobileNet v2
0.4
0.5
0.5
50
30
180
Inception v3
0.1
0.1
0.1
80
40
180
EfﬁcientNet
0.1
0.1
0.3
20
30
180"
REFERENCES,0.5319148936170213,∗s: TSLA drops off LSR after s epochs.
REFERENCES,0.5340425531914894,"All top-1 and top-5 accuracy on the testing datasets are summarized Tables 6, 7 , 8 for CUB-2011,
Stanford Dogs, and CIFAR-100, respectively. Overall, the results show that TSLA has the best
performance, comparing with baseline and LSR."
REFERENCES,0.5361702127659574,Table 6: Comparison of Testing Accuracy (%) for Different Models on CUB-2011
REFERENCES,0.5382978723404256,"Top-1 accuracy
Top-5 accuracy
Model
baseline
LSR
TSLA
baseline
LSR
TSLA
ResNet-50
80.15
82.76
82.83
94.87
95.08
95.55
VGG
80.58
81.14
81.71
95.46
95.31
95.62
MobileNet v2
78.44
79.91
80.14
94.58
94.39
94.94
Inception v3
79.08
80.12
80.19
94.63
95.24
95.19
EfﬁcientNet
81.12
80.95
81.55
95.53
95.25
95.56"
REFERENCES,0.5404255319148936,Table 7: Comparison of Testing Accuracy (%) for Different Models on Stanford Dogs
REFERENCES,0.5425531914893617,"Top-1 accuracy
Top-5 accuracy
Model
baseline
LSR
TSLA
baseline
LSR
TSLA
ResNet-50
88.14
89.50
89.90
99.04
99.10
99.42
VGG
85.96
86.42
86.64
98.61
98.72
98.85
MobileNet v2
83.05
83.62
84.34
97.98
97.68
98.40
Inception v3
87.10
87.30
87.48
98.93
98.82
98.90
EfﬁcientNet
85.72
85.71
86.26
98.80
98.60
98.78"
REFERENCES,0.5446808510638298,Table 8: Comparison of Testing Accuracy (%) for Different Models on CIFAR-100
REFERENCES,0.5468085106382978,"Top-1 accuracy
Top-5 accuracy
Model
baseline
LSR
TSLA
baseline
LSR
TSLA
ResNet-50
76.63
77.52
77.71
93.99
93.08
94.17
VGG
72.99
74.09
74.22
90.59
90.37
91.70
MobileNet v2
74.42
76.64
76.72
93.12
93.13
94.01
Inception v3
79.95
79.71
79.87
95.23
94.76
95.19
EfﬁcientNet
69.73
71.38
71.68
90.50
90.48
91.93"
REFERENCES,0.548936170212766,Under review as a conference paper at ICLR 2022
REFERENCES,0.551063829787234,"Besides, we train ResNet-50 over ImageNet with the total epochs of 180. The initial learning rate is
set to be 0.2 for all algorithm and divide them by 10 every 60 epochs as suggested in (He et al., 2016;
Zagoruyko & Komodakis, 2016). The mini-batch size of 512 is used. The momentum parameter
and weight decay are set to be 0.9 and 10−4 respectively. We ﬁx the smoothing strength θ = 0.1
and the drop epoch s = 120 for TSLA. The top-1 accuracy are presented in Table 9, showing that
TSLA and LSR are comparable."
REFERENCES,0.5531914893617021,Table 9: Comparison of Testing Accuracy (%) for ImageNet
REFERENCES,0.5553191489361702,"Top-1 accuracy
Top-5 accuracy
Model
baseline
LSR
TSLA
baseline
LSR
TSLA
ResNet-50
76.35
76.96
77.09
93.14
93.55
93.69"
REFERENCES,0.5574468085106383,"B
TECHNICAL LEMMA"
REFERENCES,0.5595744680851064,Recall that the optimization problem is
REFERENCES,0.5617021276595745,"min
w∈W FS(w) := 1 n n
X"
REFERENCES,0.5638297872340425,"i=1
ℓ(yi, f(w; xi))
(7)"
REFERENCES,0.5659574468085107,where the cross-entropy loss function ℓis given by
REFERENCES,0.5680851063829787,"ℓ(y, f(w; x)) = K
X"
REFERENCES,0.5702127659574469,"i=1
−yi log"
REFERENCES,0.5723404255319149,"exp(fi(w; x))
PK
j=1 exp(fj(w; x)) ! .
(8)"
REFERENCES,0.574468085106383,If we set
REFERENCES,0.5765957446808511,"p(w; x) = (p1(w; x), . . . , pK(w; x)) ∈RK,
pi(w; x) = −log"
REFERENCES,0.5787234042553191,"exp(fi(w; x))
PK
j=1 exp(fj(w; x)) ! , (9)"
REFERENCES,0.5808510638297872,the problem (7) becomes
REFERENCES,0.5829787234042553,"min
w∈W FS(w) := 1 n n
X"
REFERENCES,0.5851063829787234,"i=1
⟨yi, p(w; xi)⟩.
(10)"
REFERENCES,0.5872340425531914,Then the stochastic gradient with respective to w is
REFERENCES,0.5893617021276596,"∇ℓ(y, f(w; x)) = ⟨y, ∇p(w; x)⟩.
(11)"
REFERENCES,0.5914893617021276,"Lemma 1. Under Assumption 1 (i), we have"
REFERENCES,0.5936170212765958,"E
h∇ℓ(yLS
t , f(wt; xt)) −∇FS(wt)
2i
≤(1 −θ)σ2 + θδσ2."
REFERENCES,0.5957446808510638,"Proof. By the facts of yLS
t
= (1 −θ)yt + θbyt and the equation in (11), we have"
REFERENCES,0.597872340425532,"∇ℓ(yLS
t , f(wt; xt)) = (1 −θ)∇ℓ(yt, f(wt; xt)) + θ∇ℓ(byt, f(wt; xt))."
REFERENCES,0.6,"Therefore,"
REFERENCES,0.6021276595744681,"E
h∇ℓ(yLS
t , f(wt; xt)) −∇FS(wt)
2i"
REFERENCES,0.6042553191489362,"=E
h
∥(1 −θ)[∇ℓ(yt, f(wt; xt)) −∇FS(wt)] + θ[∇ℓ(byt, f(wt; xt)) −∇FS(wt)]∥2i"
REFERENCES,0.6063829787234043,"(a)
≤(1 −θ)E
h
∥∇ℓ(yt, f(wt; xt)) −∇FS(wt)∥2i
+ θE
h
∥∇ℓ(byt, f(wt; xt)) −∇FS(wt)∥2i"
REFERENCES,0.6085106382978723,"(b)
≤(1 −θ)σ2 + θδσ2,"
REFERENCES,0.6106382978723405,"where (a) uses the convexity of norm, i.e., ∥(1 −θ)a + θb∥2 ≤(1 −θ)∥a∥2 + θ∥b∥2; (b) uses
assumption 1 (i) and the deﬁnitions in (4), and Assumption 1 (i)."
REFERENCES,0.6127659574468085,Under review as a conference paper at ICLR 2022
REFERENCES,0.6148936170212767,"C
PROOF OF THEOREM 3"
REFERENCES,0.6170212765957447,"Proof. By the smoothness of objective function F(w) in Assumption 1 (ii) and its remark, we have"
REFERENCES,0.6191489361702127,FS(wt+1) −FS(wt)
REFERENCES,0.6212765957446809,"≤⟨∇FS(wt), wt+1 −wt⟩+ L"
REFERENCES,0.6234042553191489,2 ∥wt+1 −wt∥2
REFERENCES,0.625531914893617,"(a)
= −η

∇FS(wt), ∇ℓ(yLS
t , f(wt; xt))

+ η2L 2"
REFERENCES,0.6276595744680851,"∇ℓ(yLS
t , f(wt; xt))
2"
REFERENCES,0.6297872340425532,"(b)
= −η"
REFERENCES,0.6319148936170212,2 ∥∇FS(wt)∥2 + η 2
REFERENCES,0.6340425531914894,"∇FS(wt) −∇ℓ(yLS
t , f(wt; xt))
2 + η(ηL −1) 2"
REFERENCES,0.6361702127659574,"∇ℓ(yLS
t , f(wt; xt))
2"
REFERENCES,0.6382978723404256,"(c)
≤−η"
REFERENCES,0.6404255319148936,2 ∥∇FS(wt)∥2 + η 2
REFERENCES,0.6425531914893617,"∇FS(wt) −∇ℓ(yLS
t , f(wt; xt))
2 ,
(12)"
REFERENCES,0.6446808510638298,"where (a) is due to the update of wt+1; (b) is due to ⟨a, −b⟩= 1"
REFERENCES,0.6468085106382979,"2
 
∥a −b∥2 −∥a∥2 −∥b∥2
; (c)
is due to η ≤1"
REFERENCES,0.648936170212766,"L. Taking the expectation over (xt, yLS
t ) on the both sides of (12), we have"
REFERENCES,0.6510638297872341,E [FS(wt+1) −FS(wt)] ≤−η
"E
H",0.6531914893617021,"2E
h
∥∇FS(wt)∥2i
+ η"
E,0.6553191489361702,"2E
h∇FS(wt) −∇ℓ(yLS
t , f(wt; xt))
2i ≤−η"
E,0.6574468085106383,"2E

∥∇FS(wt)∥2
+ η"
E,0.6595744680851063,"2
 
(1 −θ)σ2 + θδσ2
.
(13)"
E,0.6617021276595745,where the last inequality is due to Lemma 1. Then inequality (13) implies
T,0.6638297872340425,"1
T"
T,0.6659574468085107,"T −1
X"
T,0.6680851063829787,"t=0
E

∥∇FS(wt)∥2
≤2FS(w0)"
T,0.6702127659574468,"ηT
+ (1 −θ)σ2 + θδσ2"
T,0.6723404255319149,"(a)
= 2FS(w0)"
T,0.674468085106383,"ηT
+
2δ
1 + δ σ2"
T,0.676595744680851,"(b)
≤2FS(w0)"
T,0.6787234042553192,"ηT
+ 2δσ2,"
T,0.6808510638297872,"where (a) is due to θ =
1
1+δ; (b) is due to
1
1+δ ≤1."
T,0.6829787234042554,"D
CONVERGENCE ANALYSIS OF SGD WITHOUT LSR (θ = 0)"
T,0.6851063829787234,"Theorem 6. Under Assumption 1, the solutions wt from Algorithm 1 with θ = 0 satisfy"
T,0.6872340425531915,"1
T"
T,0.6893617021276596,"T −1
X"
T,0.6914893617021277,"t=0
E

∥∇FS(wt)∥2
≤2FS(w0)"
T,0.6936170212765957,"ηT
+ ηLσ2."
T,0.6957446808510638,"In order to have ER[∥∇FS(wR)∥2] ≤ϵ2, it sufﬁces to set η = min

1
L,
ϵ2
2Lσ2

and T = 4FS(w0) ηϵ2
,"
T,0.6978723404255319,"the total complexity is O
  1"
T,0.7,"ϵ4

."
T,0.7021276595744681,"Proof. By the smoothness of objective function FS(w) in Assumption 1 (ii) and its remark, we have"
T,0.7042553191489361,FS(wt+1) −FS(wt)
T,0.7063829787234043,"≤⟨∇FS(wt), wt+1 −wt⟩+ L"
T,0.7085106382978723,2 ∥wt+1 −wt∥2
T,0.7106382978723405,"(a)
= −η ⟨∇FS(wt), ∇ℓ(yt, f(wt; xt))⟩+ η2L"
T,0.7127659574468085,"2
∥∇ℓ(yt, f(wt; xt))∥2 ,
(14)"
T,0.7148936170212766,Under review as a conference paper at ICLR 2022
T,0.7170212765957447,"where (a) is due to the update of wt+1. Taking the expectation over (xt; yt) on the both sides of
(14), we have"
T,0.7191489361702128,E [FS(wt+1) −FS(wt)]
T,0.7212765957446808,"(a)
≤−ηE

∥∇FS(wt)∥2
+ η2L"
"E
H",0.723404255319149,"2 E
h
∥∇ℓ(yt, f(wt; xt)) −∇FS(wt) + ∇FS(wt)∥2i"
"E
H",0.725531914893617,"(b)
= −ηE

∥∇FS(wt)∥2
+ η2L"
"E
H",0.7276595744680852,"2 E
h
∥∇ℓ(yt, f(wt; xt)) −∇FS(wt)∥2i
+ η2L"
"E
H",0.7297872340425532,"2 E
h
∥∇FS(wt)∥2i"
"E
H",0.7319148936170212,"(c)
≤−η"
E,0.7340425531914894,"2E

∥∇FS(wt)∥2
+ η2L"
E,0.7361702127659574,"2 σ2.
(15)"
E,0.7382978723404255,"where (a) and (b) use Assumption 1 (i); (c) uses the facts that η ≤
1
L and Assumption 1 (i). The
inequality (15) implies"
T,0.7404255319148936,"1
T"
T,0.7425531914893617,"T −1
X"
T,0.7446808510638298,"t=0
E

∥∇FS(wt)∥2
≤2FS(w0)"
T,0.7468085106382979,"ηT
+ ηLσ2."
T,0.7489361702127659,"By setting η ≤
ϵ2
2Lσ2 and T = 4FS(w0)"
T,0.7510638297872341,"ηϵ2
, we have 1"
T,0.7531914893617021,"T
PT −1
t=0 E

∥∇FS(wt)∥2
≤ϵ2. Thus the total"
T,0.7553191489361702,"complexity is in the order of O

1
ηϵ2

= O
  1"
T,0.7574468085106383,"ϵ4

."
T,0.7595744680851064,"E
PROOF OF THEOREM 4"
T,0.7617021276595745,"Proof. Following the similar analysis of inequality (13) from the proof of Theorem 3, we have"
T,0.7638297872340426,E [FS(wt+1) −FS(wt)] ≤−η1
E,0.7659574468085106,"2 E

∥∇FS(wt)∥2
+ η1"
E,0.7680851063829788,"2
 
(1 −θ)σ2 + θδσ2
.
(16)"
E,0.7702127659574468,Using the condition in Assumption 2 we can simplify the inequality from (16) as
E,0.7723404255319148,"E [FS(wt+1) −FS(w∗
S)]"
E,0.774468085106383,"≤(1 −η1µ)E [FS(wt) −FS(w∗
S)] + η1"
E,0.776595744680851,"2
 
(1 −θ)σ2 + θδσ2"
E,0.7787234042553192,"≤(1 −η1µ)t+1 E [FS(w0) −FS(w∗
S)] + η1"
E,0.7808510638297872,"2
 
(1 −θ)σ2 + θδσ2
t
X"
E,0.7829787234042553,"i=0
(1 −η1µ)i"
E,0.7851063829787234,≤(1 −η1µ)t+1 E [FS(w0)] + η1
E,0.7872340425531915,"2
 
(1 −θ)σ2 + θδσ2
t
X"
E,0.7893617021276595,"i=0
(1 −η1µ)i ,"
E,0.7914893617021277,"where the last inequality is due to the deﬁnition of loss function that FS(w∗
S) ≥0. Since η1 ≤1"
E,0.7936170212765957,"L <
1
µ, then (1 −η1µ)t+1 < exp(−η1µ(t + 1)) and Pt
i=0 (1 −η1µ)i ≤
1
η1µ. As a result, for any T1,
we have"
E,0.7957446808510639,"E [FS(wT1) −FS(w∗
S)] ≤exp(−η1µT1)FS(w0) + 1"
E,0.7978723404255319,"2µ
 
(1 −θ)σ2 + θδσ2
.
(17)"
E,0.8,"Let θ =
1
1+δ and bσ2 := (1 −θ)σ2 + θδσ2 =
2δ
1+δσ2 then
1
2µ
 
(1 −θ)σ2 + θδσ2
=
δσ2
µ(1+δ) ≤
FS(w0) since δ is small enough. By setting"
E,0.8021276595744681,"T1 = log
2µFS(w0) bσ2"
E,0.8042553191489362,"
/(η1µ)"
E,0.8063829787234043,we have
E,0.8085106382978723,"E [F(wT1) −FS(w∗
S)] ≤bσ2"
E,0.8106382978723404,µ ≤2δσ2
E,0.8127659574468085,"µ
.
(18)"
E,0.8148936170212766,Under review as a conference paper at ICLR 2022
E,0.8170212765957446,"After T1 iterations, we drop off the label smoothing, i.e. θ = 0, then we know for any t ≥T1,
following the inequality (15) from the proof of Theorem 6, we have"
E,0.8191489361702128,E [FS(wt+1) −FS(wt)] ≤−η2
E,0.8212765957446808,"2 E

∥∇FS(wt)∥2
+ η2
2Lσ2 2
."
E,0.823404255319149,"Therefore, we get 1
T2"
E,0.825531914893617,"T1+T2−1
X"
E,0.8276595744680851,"t=T1
E

∥∇FS(wt)∥2
≤
2
η2T2
E [FS(wT1) −FS(wT1+T2−1)] + η2Lσ2"
E,0.8297872340425532,"(a)
≤
2
η2T2
E [FS(wT1) −FS(w∗
S)] + η2Lσ2"
E,0.8319148936170213,"(18)
≤4δσ2"
E,0.8340425531914893,"µη2T2
+ η2Lσ2,
(19)"
E,0.8361702127659575,"where (a) is due to FS(wT1+T2−1) ≥FS(w∗
S). By setting η2 =
ϵ2
2Lσ2 and T2 =
8δσ2
µη2ϵ2 , we have"
E,0.8382978723404255,"1
T2
PT1+T2−1
t=T1
E

∥∇FS(wt)∥2
≤ϵ2."
E,0.8404255319148937,"F
GENERALIZATION ANALYSIS"
E,0.8425531914893617,"In this section, let"
E,0.8446808510638298,bFS(w) =FS(w) + λ
E,0.8468085106382979,"2 ∥w∥2 = 1 n n
X"
E,0.8489361702127659,"i=1
ℓ(yi, f(w; xi)) + λ"
E,0.851063829787234,2 ∥w∥2
E,0.8531914893617021,"|
{z
}
bℓ(yi,f(w;xi)) (20)"
E,0.8553191489361702,bF(w) =F(w) + λ
E,0.8574468085106383,"2 ∥w∥2
(21)"
E,0.8595744680851064,"Following by Yuan et al. (2019b), we use the following decomposition of testing error."
E,0.8617021276595744,"EA,S[F(wS)] −ES[FS(w∗
S)] ≤ES[EA[FS(wS) −FS(w∗
S)]] + EA,S[F(wS) −FS(wS)].
(22)"
E,0.8638297872340426,"Let deﬁne the gradient update rule Gbℓ,η as follows"
E,0.8659574468085106,"Gbℓ,η(w) = w −η∇wbℓ(y, f(w, x)).
(23)"
E,0.8680851063829788,"Then we have the following lemma, which is similar to Lemma 2.5 and Lemma 4.2 in Hardt et al.
(2015).
Lemma 2. Assume that ℓ(y, f(w, x)) is L-smooth and B-Lipschitz. Let wt+1 = G(wt) and
w′
t+1 = G′(w′
t), then"
E,0.8702127659574468,"∥wt+1 −w′
t+1∥=

(1 + ηL −ηλ)∥wt −w′
t∥,
G = G′,
(1 −ηλ)∥wt −w′
t∥+ 2ηB,
G ̸= G′."
E,0.8723404255319149,"Proof. Recall that bℓ(yi, f(w; xi)) = ℓ(yi, f(w; xi)) + λ"
E,0.874468085106383,"2 ∥w∥2. Then
(1) When G = G′ and ℓ(·, f(w; ·)) is L-smooth, then"
E,0.8765957446808511,"∥wt+1 −w′
t+1∥≤(1 −ηλ)∥wt −w′
t∥+ η∥∇wℓ(yt, f(wt; xt)) −∇wℓ(yt, f(w′
t; xt))∥"
E,0.8787234042553191,"≤(1 + ηL −ηλ)∥wt −w′
t∥."
E,0.8808510638297873,"(2) When G ̸= G′ and ℓ(·, f(w; ·)) is B-Lipschitz, then"
E,0.8829787234042553,"∥wt+1 −w′
t+1∥≤(1 −ηλ)∥wt −w′
t∥+ η∥∇wℓ(yt, f(wt; xt)) −∇wℓ(y′
t, f(w′
t; x′
t))∥"
E,0.8851063829787233,"≤(1 −ηλ)∥wt −w′
t∥+ 2ηB."
E,0.8872340425531915,Under review as a conference paper at ICLR 2022
E,0.8893617021276595,"F.1
PROOF OF THEOREM 5"
E,0.8914893617021277,"Proof. By Theorem 3.2 of Hardt et al. (2015), we have
∆t+1 :=E[∥wt+1 −w′
t+1∥]"
E,0.8936170212765957,"(a)
≤

1 −1 n"
E,0.8957446808510638,"
(1 + ηL −ηλ)∆t + 1"
E,0.8978723404255319,n((1 −ηλ)∆t + 2ηtB)
E,0.9,"=

1 +

1 −1 n"
E,0.902127659574468,"
ηL −ηλ

∆t + 2ηB n"
E,0.9042553191489362,"(b)
≤(1 −ηL)∆t + 2ηB n"
E,0.9063829787234042,"(c)
≤2ηB n t0
X"
E,0.9085106382978724,"i=0
(1 −ηL)i (d)
≤2B"
E,0.9106382978723404,"nL,
(24)"
E,0.9127659574468086,"where (a) uses Lemma 2; (b) uses λ = 2L; (c) uses ∆t0 = 0; (d) uses ηL < 1. Then by Lemma
3.11 of Hardt et al. (2015) and bℓis L + λ-smooth, we have"
E,0.9148936170212766,"E[bℓ(wt; z) −bℓ(w′
t; z)] ≤t0"
E,0.9170212765957447,n + 2B(L + λ)
E,0.9191489361702128,"nL
≤6B + 1"
E,0.9212765957446809,"n
,
(25)"
E,0.9234042553191489,"where the last inequality holds by selecting t0 = 1 and λ = 2L. Then by Theorem 2.2 of Hardt
et al. (2015)"
E,0.925531914893617,"EA,S[ bF(wt) −bFS(wt)] ≤6B + 1"
E,0.9276595744680851,"n
,
(26)"
E,0.9297872340425531,"which is the generalization error. Next, we will bound the optimization error. Since bFS is (L + λ)-
smooth and satisﬁes µ-PL condition, then follow the similar analysis of Theorem 4, we have 1
T2"
E,0.9319148936170213,"T1+T2−1
X"
E,0.9340425531914893,"t=T1
ES[EA[ bFS(wt) −bFS(bw∗
S)]] ≤
1
2µT2"
E,0.9361702127659575,"T1+T2−1
X"
E,0.9382978723404255,"t=T1
E
h
∥∇bFS(wt)∥2i ≤2δσ2"
E,0.9404255319148936,"µ2η2T2
+ η2(L + λ)σ2"
E,0.9425531914893617,"2µ
,
(27)"
E,0.9446808510638298,"Then by (22), (26) and (27), we get 1
T2"
E,0.9468085106382979,"T1+T2−1
X"
E,0.948936170212766,"t=T1
EA,S[ bF(wt)] −ES[ bFS(bw∗
S)] ≤
2δσ2"
E,0.951063829787234,"µ2η2T2
+ η2(L + λ)σ2"
E,0.9531914893617022,"2µ
+ 6B + 1"
E,0.9553191489361702,"n
.
(28)"
E,0.9574468085106383,"By using the conditions that minw∈W bFS(w) ≤FS(w∗
S) + λ"
E,0.9595744680851064,"2 ∥wt∥2, deﬁnitions (20) and (21), we
get
F(wt) −FS(w∗
S) ≤bF(wt) −bFS(bw∗
S).
(29)
Therefore, we have the following inequality by (28) and (29): 1
T2"
E,0.9617021276595744,"T1+T2−1
X"
E,0.9638297872340426,"t=T1
EA,S[F(wt)] −ES[FS(w∗
S)] ≤
2δσ2"
E,0.9659574468085106,"µ2η2T2
+ η2(L + λ)σ2"
E,0.9680851063829787,"2µ
+ 6B + 1"
E,0.9702127659574468,"n
.
(30)"
E,0.9723404255319149,"By setting η2 = O(1/√n) and T2 = O(δn) then the testing error ER,A,S[F(wR)]−ES[FS(w∗
S)] ≤
O(1/√n), where A is TSLA."
E,0.9744680851063829,"Remark 1. (SGD without LSR) Following the similar analysis of Theorem 5 and similar analysis
of Theorem 6, we have the testing error for SGD without LSR is"
E,0.9765957446808511,"ER,A,S[F(wR)] −ES[FS(w∗
S)] ≤
bFS(w0)"
E,0.9787234042553191,"µηT
+ η(L + λ)σ2"
E,0.9808510638297873,"2µ
+ 6B + 1"
E,0.9829787234042553,"n
.
(31)"
E,0.9851063829787234,"By setting η = O(1/√n) and T = O(n) then ER,A,S[F(wR)] −ES[FS(w∗
S)] ≤O(1/√n), where
A is SGD without LSR."
E,0.9872340425531915,"Remark 2. (SGD with LSR) Following the similar analysis of Theorem 5 and similar analysis of
Theorem 3, we have the testing error for SGD with LSR is"
E,0.9893617021276596,"ER,A,S[F(wR)] −ES[FS(w∗
S)] ≤
bFS(w0)"
E,0.9914893617021276,"µηT
+ δσ2"
E,0.9936170212765958,"µ
+ 6B + 1"
E,0.9957446808510638,"n
,
(32)"
E,0.997872340425532,"where A is SGD with LSR. If δ > Ω(1/√n), the testing error can not be bounded by O(1/√n); if
δ ≤Ω(1/√n), the testing error can be bounded by O(1/√n) in T = O(√n) iterations."
