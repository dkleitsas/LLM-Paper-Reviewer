Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018726591760299626,"This paper proposes an algorithm, RMDA, for training neural networks
(NNs) with a regularization term for promoting desired structures. RMDA
does not incur computation additional to proximal SGD with momentum,
and achieves variance reduction without requiring the objective function
to be of the ﬁnite-sum form. Through the tool of manifold identiﬁcation
from nonlinear optimization, we prove that after a ﬁnite number of iter-
ations, all iterates of RMDA possess a desired structure identical to that
induced by the regularizer at the stationary point of asymptotic conver-
gence, even in the presence of engineering tricks like data augmentation
that complicate the training process. Experiments on training NNs with
structured sparsity conﬁrm that variance reduction is necessary for such an
identiﬁcation, and show that RMDA thus signiﬁcantly outperforms existing
methods for this task. For unstructured sparsity, RMDA also outperforms
a state-of-the-art pruning method, validating the beneﬁts of training struc-
tured NNs through regularization. Implementation of RMDA is available at
https://www.github.com/zihsyuan1214/rmda."
INTRODUCTION,0.003745318352059925,"1
Introduction"
INTRODUCTION,0.0056179775280898875,"Training neural networks (NNs) with regularization to obtain a certain desired structure
such as structured sparsity or discrete-valued parameters is a problem of increasing interest.
Existing approaches either use stochastic subgradients of the regularized objective (Wen
et al., 2016; 2018) or combine popular stochastic gradient algorithms for NNs, like SGD
with momentum (MSGD) or Adam (Kingma & Ba, 2015), with the proximal operator
associated with the regularizer to conduct proximal stochastic gradient updates to obtain a
model with preferred structures (Bai et al., 2019; Yang et al., 2019; Yun et al., 2021; Deleu
& Bengio, 2021).
Such methods come with proven convergence for certain measures of
ﬁrst-order optimality and have shown some empirical success in applications. However, we
notice that an essential theoretical support lacking in existing methods is the guarantee for
the output iterate to possess the same structure as that at the point of convergence. More
speciﬁcally, often the imposed regularization is only known to induce a desired structure
exactly at optimal or stationary points of the underlying optimization problem (see for
example, Zhao & Yu, 2006), but training algorithms are only able to generate iterates
asymptotically converging to a stationary point. Without further theoretical guarantees, it
is unknown whether the output iterate, which is just an approximation of the stationary
point, still has the same structure. For example, let us assume that sparsity is desired,
the point of convergence is x∗= (1, 0, 0), and two algorithms respectively produce iterates
{yt = (1, t−1, t−1)} and {zt = (1 + t−1, 0, 0)}. Clearly, both iterate sequences converge to
x∗, but only zt has the same desired structure as its limit point x∗, while yt is not useful
for sparsity despite that the point of convergence is. This work aims at ﬁlling this gap to
propose an algorithm for training structured NNs that can provably make all its iterates after"
INTRODUCTION,0.00749063670411985,"a ﬁnite number of iterations possess the desired structure of the stationary point to which
the iterates converge. We term the structure at a stationary point a stationary structure,
and it should be understood that for multiple stationary points, each might correspond
to a diﬀerent stationary structure, and we aim at identifying the one at the limit point
of the iterates of an algorithm, instead of selecting the optimal one among all stationary
structures. Although ﬁnding the structure at an inferior stationary point might seem not
very meaningful, another reason for studying this identiﬁcation property is that for the same
point of convergence, the structure at the limit point is the most preferable one. Consider
the same example above, we note that for any sequence {xt} converging to x∗, xt
1 ̸= 0 for
all t large enough, for otherwise xt does not converge to x∗. Therefore, xt cannot be sparser
than x∗if xt →x∗.1 Identifying the structure of the point of convergence thus also amounts
to ﬁnding the locally most ideal structure under the same convergence premise."
INTRODUCTION,0.009363295880149813,"It is well-known in the literature of nonlinear optimization that generating iterates consis-
tently possessing the structure at the stationary point of convergence is possible if all points
with the same structure near the stationary point can be presented locally as a manifold
along which the regularizer is smooth. This manifold is often termed as the active manifold
(relative to the given stationary point), and the task of generating iterates staying in the
active manifold relative to the point of convergence after ﬁnite iterations is called manifold
identiﬁcation (Lewis, 2002; Hare & Lewis, 2004; Lewis & Zhang, 2013). To identify the ac-
tive manifold of a stationary point, we need the regularizer to be partly smooth (Lewis, 2002;
Hare & Lewis, 2004) at that point, roughly meaning that the regularizer is smooth along the
active manifold around the point, while the change in its value is drastic along directions
leaving the manifold. A more technical deﬁnition will be given in Section 3. Fortunately,
most regularizers used in machine learning are partly smooth, so stationary structure iden-
tiﬁcation is possible, and various deterministic algorithms are known to achieve so (Hare
& Lewis, 2007; Hare, 2011; Wright, 2012; Liang et al., 2017a;b; Li et al., 2020; Lee, 2020;
Bareilles et al., 2020)."
INTRODUCTION,0.011235955056179775,"On the other hand, for stochastic gradient-related methods to identify a stationary struc-
ture, existing theory suggests that the variance of the gradient estimation needs to vanish
as the iterates approach a stationary point (Poon et al., 2018), and indeed, it is observed
empirically that proximal stochastic gradient descent (SGD) is incapable of manifold iden-
tiﬁcation due to the presence of the variance in the gradient estimation (Lee & Wright,
2012; Sun et al., 2019).2 Poon et al. (2018) showed that variance-reduction methods such
as SVRG (Johnson & Zhang, 2013; Xiao & Zhang, 2014) and SAGA (Defazio et al., 2014)
that utilize the ﬁnite-sum structure of empirical risk minimization to drive the variance of
their gradient estimators to zero are suitable for this task. Unfortunately, with the standard
practice of data augmentation in deep learning, training of deep learning models with a reg-
ularizer should be treated as the following stochastic optimization problem that minimizes
the expected loss over a distribution, instead of the commonly seen ﬁnite-sum form:"
INTRODUCTION,0.013108614232209739,"min
W ∈E
F (W) := Eξ∼D [fξ (W)] + ψ (W) ,
(1)"
INTRODUCTION,0.0149812734082397,"where E is a Euclidean space with inner product ⟨·, ·⟩and the associated norm ∥·∥, D is a
distribution over a space Ω, fξ is diﬀerentiable almost everywhere for all ξ ∈Ω, and ψ(W)
is a regularizer that might be nondiﬀerentiable. We will also use the notation f(W) :=
Eξ∼D[fξ(W)]. Without a ﬁnite-sum structure in (1), Defazio & Bottou (2019) pointed out
that classical variance-reduction methods are ineﬀective for deep learning, and one major
reason is the necessity of periodically evaluating ∇f(W) (or at least using a large batch from
D to get a precise approximation of it) in variance-reduction methods is intractable, hence
manifold identiﬁcation and therefore ﬁnding the stationary structure becomes an extremely
tough task for deep learning. Although recently there are eﬀorts in developing variance-
reduction methods for (1) inspired by online problems (Wang et al., 2019; Nguyen et al.,
2021; Pham et al., 2020; Cutkosky & Orabona, 2019; Tran-Dinh et al., 2019), these methods
all have multiple hyperparameters to tune and incur computational cost at least twice or"
INTRODUCTION,0.016853932584269662,"1See a more detailed discussion in Appendix B.1.
2An exception is the interpolation case, in which the variance of plain SGD vanishes asymptot-
ically. But data augmentation often fails this interpolation condition."
INTRODUCTION,0.018726591760299626,"thrice to that of (proximal) SGD. As the training of deep learning models is time- and
resource-consuming, these drawbacks make such methods less ideal for deep learning."
INTRODUCTION,0.020599250936329586,"To tackle these diﬃculties, we extend the recently proposed modernized dual averaging
framework (Jelassi & Defazio, 2020) to the regularized setting by incorporating proximal
operations, and obtain a new algorithm RMDA (Regularized Modernized Dual Averaging)
for (1). The proposed algorithm provably achieves variance reduction beyond ﬁnite-sum
problems without any cost or hard-to-tune hyperparameters additional to those of proximal
momentum SGD (proxMSGD), and we provide theoretical guarantees for its convergence
and ability for manifold identiﬁcation. The key diﬀerence between RMDA and the original
regularized dual averaging (RDA) of Xiao (2010) is that RMDA incorporates momentum
and can achieve better performance for deep learning in terms of the generalization ability,
and the new algorithm requires nontrivial proofs for its guarantees. We further conduct
experiments on training deep learning models with a regularizer for structured-sparsity to
demonstrate the ability of RMDA to identify the stationary structure without sacriﬁcing the
prediction accuracy."
INTRODUCTION,0.02247191011235955,"When the desired structure is (unstructured) sparsity, a popular approach is pruning that
trims a given dense model to a speciﬁed level, and works like (Gale et al., 2019; Blalock et al.,
2020; Evci et al., 2020; Verma & Pesquet, 2021) have shown promising results. However, as a
post-processing approach, pruning is essentially diﬀerent from structured training considered
in this work, because pruning is mainly used when a model is available, while structured
training combines training and structure inducing in one procedure to potentially reduce the
computational cost and memory footprint when resources are scarce. We will also show in
our experiment that RMDA can achieve better performance than a state-of-the-art pruning
method, suggesting that structured training indeed has its merits for obtaining sparse NNs."
INTRODUCTION,0.024344569288389514,"The main contributions of this work are summarized as follows.
• Principled analysis: We use the theory of manifold identiﬁcation from nonlinear opti-
mization to provide a uniﬁed way towards better understanding of algorithms for training
structured neural networks.
• Variance reduction beyond ﬁnite-sum with low cost: RMDA achieves variance re-
duction for problems that consist of an inﬁnite-sum term plus a regularizer (see Lemma 2)
while incorporating momentum to improve the generalization performance. Its spatial
and computational cost is almost the same as proxMSGD, and there is no additional
hyperparameters to tune, making RMDA suitable for large-scale deep learning.
• Structure identiﬁcation: With the help of variance reduction, our theory shows that
under suitable conditions, after a ﬁnite number of iterations, iterates of RMDA stay in
the active manifold of its limit point.
• Superior empirical performance: Experiments on neural networks with structured
sparsity exemplify that RMDA can identify a stationary structure without reducing the
validation accuracy, thus outperforming existing methods by achieving higher group spar-
sity.
Another experiment on unstructured sparsity also shows RMDA outperforms a
state-of-the-art pruning method.
After this work is ﬁnished, we found a very recent paper Kungurtsev & Shikhman (2021) that
proposed the same algorithm (with slightly diﬀerences in the parameters setting in Line 5
of Algorithm 1) and analyzed the expected convergence of (1) under a speciﬁc scheduling
of ct = st+1α−1
t+1 when both terms are convex. In contrast, our work focuses on nonconvex
deep learning problems, and especially on the manifold identiﬁcation aspect."
ALGORITHM,0.026217228464419477,"2
Algorithm"
ALGORITHM,0.028089887640449437,"Details of the proposed RMDA are in Algorithm 1.
At the t-th iteration with the iter-
ate W t−1, we draw an independent sample ξt ∼D to compute the stochastic gradient
∇fξt(W t−1), decide a learning rate ηt, and update the weighted sum Vt of previous stochas-
tic gradients using ηt and the scaling factor βt :=
√ t:"
ALGORITHM,0.0299625468164794,"V0 := 0,
Vt :=
Xt"
ALGORITHM,0.031835205992509365,"k=1 ηkβk∇fξk(W k−1) = Vt−1 + ηtβt∇fξt(W t−1),
∀t > 0."
ALGORITHM,0.033707865168539325,"Algorithm 1: RMDA (W 0, T, η(·), c(·))"
ALGORITHM,0.035580524344569285,"input : Initial point W 0, learning rate schedule η(·), momentum schedule c(·),
number of epochs T"
ALGORITHM,0.03745318352059925,"1 V0 ←0,
α0 ←0"
ALGORITHM,0.03932584269662921,"2 for t = 1, . . . , T do"
ALGORITHM,0.04119850187265917,"3
βt ←
√"
ALGORITHM,0.04307116104868914,"t,
st ←η(t)βt,
αt ←αt−1 + st"
ALGORITHM,0.0449438202247191,"4
Sample ξt ∼D and compute V t ←V t−1 + st∇fξt(W t−1)"
ALGORITHM,0.04681647940074907,"5
˜W t ←arg minW ⟨V t, W⟩+ βt"
ALGORITHM,0.04868913857677903,"2
W −W 02 + αtψ(W)
// (2)"
ALGORITHM,0.05056179775280899,"6
W t ←(1 −c(t))W t−1 + c(t) ˜W t"
ALGORITHM,0.052434456928838954,output: The ﬁnal model W T
ALGORITHM,0.054307116104868915,The tentative iterate ˜W t is then obtained by the proximal operation associated with ψ:
ALGORITHM,0.056179775280898875,"˜W t = proxαtβ−1
t
ψ
 
W 0 −β−1
t
V t
,
αt :=
Xt"
ALGORITHM,0.05805243445692884,"k=1 βkηk,
(2)"
ALGORITHM,0.0599250936329588,"where for any function g, proxg(x) := arg miny ∥y −x∥2/2 + g(y) is its proximal operator.
The iterate is then updated along the direction ˜W t −W t−1 with a factor of ct ∈[0, 1]:"
ALGORITHM,0.06179775280898876,"W t = (1 −ct) W t−1 + ct ˜W t = W t−1 + ct
  ˜W t −W t−1
.
(3)"
ALGORITHM,0.06367041198501873,"When ψ ≡0, RMDA reduces to the modernized dual averaging algorithm of Jelassi &
Defazio (2020), in which case it has been shown that mixing W t−1 and ˜W t in (3) equals
to introducing momentum (Jelassi & Defazio, 2020; Tao et al., 2018). We found that this
introduction of momentum greatly improves the performance of RMDA and is therefore
essential for applying it on deep learning problems."
ANALYSIS,0.06554307116104868,"3
Analysis"
ANALYSIS,0.06741573033707865,"We provide theoretical analysis of the proposed RMDA in this section. Our analysis shows
variance reduction in RMDA and stationarity of the limit point of its iterates, but all of
them revolves around our main purpose of identiﬁcation of a stationary structure within a
ﬁnite number of iterations. The key tools for this end are partial smoothness and mani-
fold identiﬁcation (Hare & Lewis, 2004; Lewis, 2002). Our result is the currently missing
cornerstone for those proximal algorithms applied to deep learning problems for identifying
desired structures. In fact, it is actually well-known in convex optimization that those algo-
rithms based on plain proximal stochastic gradient without variance reduction are unable to
identify the active manifold, and the structure of the iterates oscillates due to the variance
in the gradient estimation; see, for example, experiments and discussions in Lee & Wright
(2012); Sun et al. (2019). Our work is therefore the ﬁrst one to provide justiﬁcation for
solving the regularized optimization problem in deep learning to really identify a desired
structure induced by the regularizer. Throughout, ∇fξ denotes the gradient of fξ, ∂ψ is the
(regular) subdiﬀerential of ψ, and relint(C) means the relative interior of the set C."
ANALYSIS,0.06928838951310862,"We start from introducing the notion of partial smoothness.
Deﬁnition 1. A function ψ is partly smooth at a point W ∗relative to a set MW ∗∋W ∗if
1. Around W ∗, MW ∗is a C2-manifold and ψ|MW ∗is C2.
2. ψ is regular (ﬁnite with the Fr´echet subdiﬀerential coincides with the limiting Fr´echet
subdiﬀerential) at all points W ∈MW ∗around W ∗with ∂ψ(W) ̸= ∅.
3. The aﬃne span of ∂ψ(W ∗) is a translate of the normal space to MW ∗at W ∗.
4. ∂ψ is continuous at W ∗relative to MW ∗."
ANALYSIS,0.07116104868913857,"We often call MW ∗the active manifold at W ∗. Another concept required for manifold
identiﬁcation is prox-regularity (Poliquin & Rockafellar, 1996).
Deﬁnition 2. A function ψ is prox-regular at W ∗for V ∗∈∂ψ(W ∗) if ψ is ﬁnite at
W ∗, locally lower semi-continuous around W ∗, and there is ρ > 0 such that ψ(W1) ≥
ψ(W2) + ⟨V, W1 −W2⟩−ρ"
ANALYSIS,0.07303370786516854,"2∥W1 −W2∥2 whenever W1, W2 are close to W ∗with ψ(W2) near
ψ(W ∗) and V ∈∂ψ(W2) near V ∗. ψ is prox-regular at W ∗if it is so for all V ∈∂ψ(W ∗)."
ANALYSIS,0.0749063670411985,"To broaden the applicable range, a function ψ prox-regular at some W ∗is often also assumed
to be subdiﬀerentially continuous (Poliquin & Rockafellar, 1996) there, meaning that if
W t →W ∗, ψ(W t) →ψ(W ∗) holds when there are V ∗∈∂ψ(W ∗) and a sequence {V t}
such that V t ∈∂ψ(W t) and V t →V ∗. Notably, all convex and weakly-convex (Nurminskii,
1973) functions are regular, prox-regular, and subdiﬀerentially continuous in their domain."
THEORETICAL RESULTS,0.07677902621722846,"3.1
Theoretical Results"
THEORETICAL RESULTS,0.07865168539325842,"When the problem is convex, convergence guarantees for Algorithm 1 under two speciﬁc
speciﬁc schemes are known.
First, when ct ≡1, RMDA reduces to the classical RDA,
and convergence to a global optimum (of W t = ˜W t in this case) on convex problems has
been proven by Lee & Wright (2012); Duchi & Ruan (2021), with convergence rates of the
expected objective or the regret given by Xiao (2010); Lee & Wright (2012). Second, when
ct = st+1α−1
t+1 and (βt, αt) in Line 5 of Algorithm 1 are replaced by (βt+1, αt+1), convergence
is recently analyzed by Kungurtsev & Shikhman (2021). In our analysis below, we do not
assume convexity of either term."
THEORETICAL RESULTS,0.08052434456928839,"We show that if { ˜W t} converges to a point W ∗(which could be a non-stationary one), {W t}
also converges to W ∗.
Lemma 1. Consider Algorithm 1 with {ct} satisfying P ct = ∞. If { ˜W t} converges to a
point W ∗, {W t} also converges to W ∗."
THEORETICAL RESULTS,0.08239700374531835,"We then show that if { ˜W t} converges to a point, almost surely this point of convergence
is stationary. This requires the following lemma for variance reduction of RMDA, meaning
that the variance of using Vt to estimate ∇f(W t−1) reduces to zero, as α−1
t Vt converges to
∇f(W t−1) almost surely, and this result could be of its own interest. The ﬁrst claim below
uses a classical result in stochastic optimization that can be found at, for example, (Gupal,
1979, Theorem 4.1, Chapter 2.4), but the second one is, to our knowledge, new.
Lemma 2. Consider Algorithm 1. Assume for any ξ ∼D, fξ is L-Lipschitz-continuously-
diﬀerentiable almost surely for some L, so f is also L-Lipschitz-continuously-diﬀerentiable,
and there is C ≥0 such that Eξt∼D
∇fξt
 
W t−12 ≤C for all t. If {ηt} satisﬁes
X
βtηtα−1
t
= ∞,
X  
βtηtα−1
t
2 < ∞,
W t+1 −W t  
βtηtα−1
t
−1
a.s.
−−→0,
(4)"
THEORETICAL RESULTS,0.08426966292134831,"then α−1
t V t −→∇f(W t−1) with probability one. Moreover, if {W t} lies in a bounded set,
we get E
α−1
t V t −∇f
 
W t−12 →0 even if the second condition in (4) is replaced by a
weaker condition of βtηtα−1
t
→0."
THEORETICAL RESULTS,0.08614232209737828,"In general, the last condition in (4) requires some regularity conditions in F to control the
change speed of W t. One possibility is when ψ is the indicator function of a convex set,
βtηt ∝tp for t ∈(1/2, 1) will satisfy this condition. However, in other settings for ηt, even
when F and ψ are both convex, existing analyses for the classical RDA such that ct ≡1
in Algorithm 1 still need an additional local error bound assumption to control the change
of W t+1 −W t. Hence, to stay focused on our main message, we take this assumption for
granted, and leave ﬁnding suitable suﬃcient conditions for it as future work."
THEORETICAL RESULTS,0.08801498127340825,"With the help of Lemmas 1 and 2, we can now show the stationarity result for the limit
point of the iterates. The assumption of βtα−1
t
approaching 0 below is classical in analyses
of dual averaging in order to gradually remove the inﬂuence of the term
W −W 02.
Theorem 1. Consider Algorithm 1 with the conditions in Lemmas 1 and 2 hold, and
assume the set of stationary points Z := {W | 0 ∈∂F(W)} is nonempty and βtα−1
t
→0.
For any given W 0, consider the event that { ˜W t} converges to a point W ∗(each event
corresponds to a diﬀerent W ∗), then if ∂ψ is outer semicontinuous at W ∗, and this event
has a nonzero probability, W ∗∈Z, or equivalently, W ∗is a stationary point, with probability
one conditional on this event."
THEORETICAL RESULTS,0.0898876404494382,"Finally, with Lemmas 1 and 2 and Theorem 1, we prove the main result that the active
manifold of the limit point is identiﬁed in ﬁnite iterations of RMDA under nondegeneracy."
THEORETICAL RESULTS,0.09176029962546817,"Theorem 2. Consider Algorithm 1 with the conditions in Theorem 1 satisﬁed. Consider
the event of { ˜W t} converging to a certain point W ∗as in Theorem 1, if the probability of
this event is nonzero; ψ is prox-regular and subdiﬀerentially continuous at W ∗and partly
smooth at W ∗relative to the active C2 manifold M; ∂ψ is outer semicontinuous at W ∗; and
the nondegeneracy condition"
THEORETICAL RESULTS,0.09363295880149813,"−∇f (W ∗) ∈relint ∂ψ (W ∗)
(5)"
THEORETICAL RESULTS,0.09550561797752809,"holds at W ∗, then conditional on this event, almost surely there is T0 ≥0 such that
˜W t ∈M,
∀t ≥T0.
(6)"
THEORETICAL RESULTS,0.09737827715355805,"In other words, the active manifold at W ∗is identiﬁed by the iterates of Algorithm 1 after
a ﬁnite number of iterations almost surely."
THEORETICAL RESULTS,0.09925093632958802,"As mentioned in Section 1, an important reason for studying manifold identiﬁcation is to get
the lowest-dimensional manifold representing the structure of the limit point, which often
corresponds to a preferred property for the application, like the highest sparsity, lowest rank,
or lowest VC dimension locally. See an illustrated example in Appendix B.1."
APPLICATIONS IN DEEP LEARNING,0.10112359550561797,"4
Applications in Deep Learning"
APPLICATIONS IN DEEP LEARNING,0.10299625468164794,"We discuss two popular schemes of training structured deep learning models achieved
through regularization to demonstrate the applications of RMDA. More technical details
for applying our theory to the regularizers in these applications are in Appendix B."
STRUCTURED SPARSITY,0.10486891385767791,"4.1
Structured Sparsity"
STRUCTURED SPARSITY,0.10674157303370786,"As modern deep NN models are often gigantic, it is sometimes desirable to trim the model to
a smaller one when only limited resources are available. In this case, zeroing out redundant
parameters during training at the group level is shown to be useful (Zhou et al., 2016),
and one can utilize regularizers promoting structured sparsity for this purpose. The most
famous regularizer of this kind is the group-LASSO norm (Yuan & Lin, 2006; Friedman
et al., 2010). Given λ ≥0 and a collection G of index sets {Ig} of the variable W, this
convex regularizer is deﬁned as"
STRUCTURED SPARSITY,0.10861423220973783,"ψ(W) := λ
X|G|"
STRUCTURED SPARSITY,0.1104868913857678,"g=1 wg
WIg
,
(7)"
STRUCTURED SPARSITY,0.11235955056179775,"with wg > 0 being the pre-speciﬁed weight for Ig. For any W ∗, let GW ∗⊆G be the index
set such that W ∗
Ij = 0 for all j ∈GW ∗, the group-LASSO norm is partly smooth around W ∗"
STRUCTURED SPARSITY,0.11423220973782772,"relative to the manifold MW ∗:= {W | WIi = 0, ∀i ∈GW ∗}, so our theory applies."
STRUCTURED SPARSITY,0.11610486891385768,"In order to promote structured sparsity, we need to carefully design the grouping. For-
tunately, in NNs, the parameters can be grouped naturally (Wen et al., 2016). For any
fully-connected layer, let W ∈Rout×in be the matrix representation of the associated pa-
rameters, where out is the number of output neurons and in is that of input neurons, we
can consider the column-wise groups, deﬁned as W:,j for all j, and the row-wise groups
of the form Wi,:. For a convolutional layer with W ∈Rﬁlter×channel×height×width being the
tensor form of the corresponding parameters, we can consider channel-wise, ﬁlter-wise, and
kernel-wise groups, deﬁned respectively as W:,j,:,:, Wi,:,:,: and Wi,j,:,:."
STRUCTURED SPARSITY,0.11797752808988764,"4.2
Binary/Discrete Neural Networks"
STRUCTURED SPARSITY,0.1198501872659176,"Making the parameters of an NN binary integers is another way to obtain a more compact
model during training and deployment (Hubara et al., 2016), but discrete optimization is
hard to scale-up. Using a vector representation w ∈Rm of the variables, Hou et al. (2017)
thus proposed to use the indicator function of

w | wIi = αibIi, αi > 0, bIi ∈{±1}|Ii|	
to
induce the entries of w to be binary without resorting to discrete optimization tools, where
each Ii enumerates all parameters in the i-th layer. Yang et al. (2019) later proposed to use
minα∈[0,1]m
Pm
i=1
 
αi(wi + 1)2 + (1 −αi)(wi −1)2
as the regularizer and to include α as"
STRUCTURED SPARSITY,0.12172284644194757,"a variable to train. At any α∗with I0 := {i | α∗
i = 0} and I1 := {i | α∗
i = 1}, the objective
is partly smooth relative to the manifold {(W, α) | αI0 = 0, αI1 = 1}. Extension to discrete
NNs beyond the binary ones is possible, and Bai et al. (2019) have proposed regularizers
with closed-form proximal operators for it."
EXPERIMENTS,0.12359550561797752,"5
Experiments"
EXPERIMENTS,0.1254681647940075,"We use the structured sparsity application in Section 4.1 to empirically exemplify the ability
of RMDA to ﬁnd desired structures in the trained NNs. RMDA and the following methods
for structured sparsity in deep learning are compared using PyTorch (Paszke et al., 2019).
• ProxSGD (Yang et al., 2019): A simple proxMSGD algorithm. To obtain group sparsity,
we skip the interpolating step in Yang et al. (2019).
• ProxSSI (Deleu & Bengio, 2021): This is a special case of the adaptive proximal SGD
framework of Yun et al. (2021) that uses the Newton-Raphson algorithm to approximately
solve the subproblem. We directly use the package released by the authors.
We exclude the algorithm of Wen et al. (2016) because their method is shown to be worse
than ProxSSI by Deleu & Bengio (2021)."
EXPERIMENTS,0.12734082397003746,"To compare these algorithms, we examine both the validation accuracy and the group spar-
sity level of their trained models.
We compute the group sparsity as the percentage of
groups whose elements are all zero, so the reported group sparsity is zero when there is no
group with a zero norm, and is one when the whole model is zero. For all methods above,
we use (7) with column-wise and channel-wise groupings in the regularization for training,
but adopt the kernel-wise grouping in their group sparsity evaluation. Throughout the ex-
periments, we always use multi-step learning rate scheduling that decays the learning rate
by a constant factor every time the epoch count reaches a pre-speciﬁed threshold. For all
methods, we conduct grid searches to ﬁnd the best hyperparameters. All results shown in
tables in Sections 5.1 and 5.2 are the mean and standard deviation of three independent
runs with the same hyperparameters, while ﬁgures use one representative run for better
visualization."
EXPERIMENTS,0.12921348314606743,"In convex optimization, a popular way to improve the practical convergence behavior for
momentum-based methods is restarting that periodically reset the momentum to zero
(O’donoghue & Candes, 2015).
Following this idea, we introduce a restart heuristic to
RMDA. At each round, we use the output of Algorithm 1 from the previous round as the
new input to the same algorithm, and continue using the scheduling η and c without reset-
ting them. For ψ ≡0, Jelassi & Defazio (2020) suggested to increase ct proportional to the
decrease of ηt until reaching ct = 1. We adopt the same setting for ct and ηt and restart
RMDA whenever ηt changes. As shown in Section 3 that ˜W t ﬁnds the active manifold,
increasing ct to 1 also accords with our interest in identifying the stationary structure."
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.13108614232209737,"5.1
Correctness of Identified Structure Using Synthetic Data"
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.13295880149812733,"0
200
400
Epochs 0% 25% 50% 75% 100%"
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.1348314606741573,Sparsity Correctness 0% 25% 50% 75% 100%
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.13670411985018727,Training Error
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.13857677902621723,(a) Logistic regression
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.1404494382022472,"RMDA (sparsity correctness)
ProxSSI (sparsity correctness)
ProxSGD (sparsity correctness)
RMDA (training error)
ProxSSI (training error)
ProxSGD (training error)"
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.14232209737827714,(b) Legend
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.1441947565543071,"0
100
200
Epochs 0% 25% 50% 75% 100%"
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.14606741573033707,Sparsity Correctness 0% 25% 50% 75% 100%
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.14794007490636704,Training Error
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.149812734082397,(c) Convolutional network
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.15168539325842698,Figure 1: Group sparsity pattern correctness and training error rates on synthetic data.
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.15355805243445692,"Our ﬁrst step is to numerically verify that RMDA can indeed identify the stationary structure
desired. To exactly ﬁnd a stationary point and its structure a priori, we consider synthetic
problems.
We ﬁrst decide a ground truth model W that is structured sparse, generate
random data points that can be well separated by W, and then decide their labels using
W. The generated data are then taken as our training data. We consider a linear logistic"
CORRECTNESS OF IDENTIFIED STRUCTURE USING SYNTHETIC DATA,0.15543071161048688,"regression model and a small NN that has one fully-connected layer and one convolutional
layer. To ensure convergence to the ground truth, for logistic regression we generate more
data points than the problem dimension to ensure the problem is strongly convex so that
there is only one stationary/optimal point, and for the small NN, we initialize all algorithms
close enough to the ground truth. We report in Fig. 1 training error rates (as an indicator
for the proximity to the ground truth) and percentages of the optimal group sparsity pattern
of the ground truth identiﬁed. Clearly, although all methods converge to the ground truth,
only RMDA identiﬁes the correct structure of it, and other methods without guarantees for
manifold identiﬁcation fail."
NEURAL NETWORKS WITH REAL DATA,0.15730337078651685,"5.2
Neural Networks with Real Data"
NEURAL NETWORKS WITH REAL DATA,0.15917602996254682,"Table 1: Group sparsity and validation accuracy of diﬀerent methods. We report mean and
standard deviation of three independent runs (except that for the linear convex model, only
one run is conducted as we are guaranteed to ﬁnd the global optima). MSGD is the baseline
with no sparsity-inducing regularizer."
NEURAL NETWORKS WITH REAL DATA,0.16104868913857678,"Algorithm
Validation accuracy
Group sparsity
Validation accuracy
Group sparsity
Logistic regression/MNIST
Fully-connected NN/FashionMNIST
ProxSGD
91.31 %
38.78 %
88.72 ± 0.05%
31.42 ± 0.36%
ProxSSI
91.31 %
39.54 %
88.44 ± 0.41%
35.25 ± 1.56%
RMDA
91.34 %
56.51 %
88.09 ± 0.04%
42.89 ± 0.66%
LeNet5/MNIST
LeNet5/FashionMNIST
MSGD
99.36 ± 0.06%
-
91.96 ± 0.01%
-
ProxSGD
99.13 ± 0.02%
76.57 ± 2.33%
90.99 ± 0.17%
50.50 ± 2.66%
ProxSSI
99.07 ± 0.03%
77.82 ± 1.56%
90.93 ± 0.02%
60.49 ± 1.05%
RMDA
99.10 ± 0.06%
79.81 ± 1.56%
91.41 ± 0.10%
66.15 ± 1.68%
VGG19/CIFAR10
VGG19/CIFAR100
MSGD
94.03 ± 0.11%
-
74.62 ± 0.22%
-
ProxSGD
92.38 ± 0.31%
72.57 ± 6.04%
71.91 ± 0.08%
08.63 ± 4.88%
ProxSSI
92.51 ± 0.03%
81.05 ± 0.16%
66.20 ± 0.38%
46.41 ± 1.42%
RMDA
93.62 ± 0.15%
86.37 ± 0.25%
72.23 ± 0.20%
58.86 ± 0.41%
ResNet50/CIFAR10
ResNet50/CIFAR100
MSGD
95.65 ± 0.03%
-
79.13 ± 0.19%
-
ProxSGD
92.36 ± 0.14%
76.82 ± 4.09%
75.53 ± 0.49%
51.83 ± 0.34%
ProxSSI
94.09 ± 0.08%
74.81 ± 1.28%
74.52 ± 0.29%
32.79 ± 2.53%
RMDA
94.25 ± 0.02%
83.01 ± 0.50%
76.12 ± 0.46%
57.67 ± 3.76%"
NEURAL NETWORKS WITH REAL DATA,0.16292134831460675,"We turn to real-world data used in modern computer vision problems. We consider two
rather simple models and six more complicated modern CNN cases. The two simpler models
are linear logistic regression with the MNIST dataset (LeCun et al., 1998), and training a
small NN with seven fully-connected layers on the FashionMNIST dataset (Xiao et al.,
2017). The six more complicated cases are:"
NEURAL NETWORKS WITH REAL DATA,0.1647940074906367,"1. A version of LeNet5 with the MNIST dataset,
2. The same version of LeNet5 with the FashionMNIST dataset,
3. A modiﬁed VGG19 (Simonyan & Zisserman, 2015) with the CIFAR10 dataset
(Krizhevsky, 2009),
4. The same modiﬁed VGG19 with the CIFAR100 dataset (Krizhevsky, 2009),
5. ResNet50 (He et al., 2016) with the CIFAR10 dataset, and
6. ResNet50 with the CIFAR100 dataset."
NEURAL NETWORKS WITH REAL DATA,0.16666666666666666,"For these six more complicated tasks, we include a dense baseline of MSGD with no sparsity-
inducing regularizer in our comparison. For all training algorithms on VGG19 and ResNet50,
we follow the standard practice in modern vision tasks to apply data augmentation through
random cropping and horizontal ﬂipping so that the training problem is no longer a ﬁnite-
sum one. From Fig. 2, we see that similar to the previous experiment, the group sparsity
level of RMDA is stable in the last epochs, while that of ProxSGD and ProxSSI oscillates
below. This suggests that RMDA is the only method that, as proven in Section 3, identiﬁes
the structured sparsity at its limit point, and other methods with no variance reduction
fail. Moreover, Table 1 shows that manifold identiﬁcation of RMDA is achieved with no"
NEURAL NETWORKS WITH REAL DATA,0.16853932584269662,"400
420
440
460
480
500
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
NEURAL NETWORKS WITH REAL DATA,0.1704119850187266,Structured Sparsity
NEURAL NETWORKS WITH REAL DATA,0.17228464419475656,Logistic Regression on MNIST
NEURAL NETWORKS WITH REAL DATA,0.17415730337078653,"ProxSGD
ProxSSI
RMDA"
NEURAL NETWORKS WITH REAL DATA,0.1760299625468165,"400
420
440
460
480
500
Epochs 0.78 0.80 0.82"
NEURAL NETWORKS WITH REAL DATA,0.17790262172284643,Structured Sparsity
NEURAL NETWORKS WITH REAL DATA,0.1797752808988764,LeNet5 on MNIST
NEURAL NETWORKS WITH REAL DATA,0.18164794007490637,"ProxSGD
ProxSSI
RMDA"
NEURAL NETWORKS WITH REAL DATA,0.18352059925093633,"900
920
940
960
980
1000
Epochs 0.6 0.7 0.8 0.9"
NEURAL NETWORKS WITH REAL DATA,0.1853932584269663,Structured Sparsity
NEURAL NETWORKS WITH REAL DATA,0.18726591760299627,VGG19 on CIFAR10
NEURAL NETWORKS WITH REAL DATA,0.1891385767790262,"ProxSGD
ProxSSI
RMDA"
NEURAL NETWORKS WITH REAL DATA,0.19101123595505617,"1400
1420
1440
1460
1480
1500
Epochs 0.70 0.75 0.80 0.85"
NEURAL NETWORKS WITH REAL DATA,0.19288389513108614,Structured Sparsity
NEURAL NETWORKS WITH REAL DATA,0.1947565543071161,ResNet50 on CIFAR10
NEURAL NETWORKS WITH REAL DATA,0.19662921348314608,"ProxSGD
ProxSSI
RMDA"
NEURAL NETWORKS WITH REAL DATA,0.19850187265917604,"400
420
440
460
480
500
Epochs 0.30 0.35 0.40 0.45"
NEURAL NETWORKS WITH REAL DATA,0.20037453183520598,Structured Sparsity
NEURAL NETWORKS WITH REAL DATA,0.20224719101123595,Fully-connected NN
NEURAL NETWORKS WITH REAL DATA,0.20411985018726592,"ProxSGD
ProxSSI
RMDA"
NEURAL NETWORKS WITH REAL DATA,0.20599250936329588,"400
420
440
460
480
500
Epochs 0.45 0.50 0.55 0.60 0.65"
NEURAL NETWORKS WITH REAL DATA,0.20786516853932585,Structured Sparsity
NEURAL NETWORKS WITH REAL DATA,0.20973782771535582,LeNet5 on FashionMNIST
NEURAL NETWORKS WITH REAL DATA,0.21161048689138576,"ProxSGD
ProxSSI
RMDA"
NEURAL NETWORKS WITH REAL DATA,0.21348314606741572,"900
920
940
960
980
1000
Epochs 0.0 0.2 0.4 0.6"
NEURAL NETWORKS WITH REAL DATA,0.2153558052434457,Structured Sparsity
NEURAL NETWORKS WITH REAL DATA,0.21722846441947566,VGG19 on CIFAR100
NEURAL NETWORKS WITH REAL DATA,0.21910112359550563,"ProxSGD
ProxSSI
RMDA"
NEURAL NETWORKS WITH REAL DATA,0.2209737827715356,"1400
1420
1440
1460
1480
1500
Epochs 0.3 0.4 0.5 0.6"
NEURAL NETWORKS WITH REAL DATA,0.22284644194756553,Structured Sparsity
NEURAL NETWORKS WITH REAL DATA,0.2247191011235955,ResNet50 on CIFAR100
NEURAL NETWORKS WITH REAL DATA,0.22659176029962547,"ProxSGD
ProxSSI
RMDA"
NEURAL NETWORKS WITH REAL DATA,0.22846441947565543,Figure 2: Group Sparsity v.s epochs of diﬀerent algorithms on NNs of a single run.
NEURAL NETWORKS WITH REAL DATA,0.2303370786516854,"sacriﬁce of the validation accuracy, so RMDA beats ProxSGD and ProxSSI in both criteria,
and its accuracy is close to that of the dense baseline of MSGD. Moreover, for VGG19
and ResNet50, RMDA succeeds in ﬁnding the optimal structured sparsity pattern despite
the presence of data augmentation, showing that RMDA can indeed overcome the diﬃculty
from the inﬁnite-sum setting of modern deep learning tasks."
NEURAL NETWORKS WITH REAL DATA,0.23220973782771537,"We also report that in the ResNet50/CIFAR100 task, on our NVIDIA RTX 8000 GPU,
MSGD, ProxSGD, and RMDA have similar per-epoch cost of 68, 77, and 91 seconds respec-
tively, while ProxSSI needs 674 seconds per epoch. RMDA is thus also more suitable for
large-scale structured deep learning in terms of practical eﬃciency."
COMPARISON WITH PRUNING,0.2340823970037453,"5.3
Comparison with Pruning"
COMPARISON WITH PRUNING,0.23595505617977527,"We compare RMDA with a state-of-the-art pruning method RigL (Evci et al., 2020). As
pruning focuses on unstructured sparsity, we use RMDA with ψ(W) = λ∥W∥1 to have a fair
comparison, and tune λ to achieve a pre-speciﬁed sparsity level. We run RigL with 1000
epochs, as its performance at the default 500 epochs was unstable, and let RMDA use the
same number of epochs. Results of 98% sparsity in Table 2 show that RMDA consistently
outdoes RigL, indicating regularized training could be a promising alternative to pruning."
COMPARISON WITH PRUNING,0.23782771535580524,"Table 2: Comparison between RMDA and RigL with 1000 epochs for unstructured sparsity
in a single run."
COMPARISON WITH PRUNING,0.2397003745318352,"ResNet50 with CIFAR10
ResNet50 with CIFAR100
Algorithm
Sparsity
Accuracy
Sparsity
Accuracy
Dense baseline
94.81%
74.61%
RMDA
98.36%
93.78%
98.32%
74.32%
RigL
98.00%
93.41%
98.00%
70.88%"
CONCLUSIONS,0.24157303370786518,"6
Conclusions"
CONCLUSIONS,0.24344569288389514,"In this work, we proposed and analyzed a new algorithm, RMDA, for eﬃciently training
structured neural networks with state-of-the-art performance. Even in the presence of data
augmentation, RMDA can still achieve variance reduction and provably identify the desired
structure at a stationary point using the tools of manifold identiﬁcation. Experiments show
that existing algorithms for the same purpose fail to ﬁnd a stable stationary structure, while
RMDA achieves so with no accuracy drop nor additional time cost."
CONCLUSIONS,0.24531835205992508,Acknowledgements
CONCLUSIONS,0.24719101123595505,"This work was supported in part by MOST of R.O.C. grant 109-2222-E-001-003-MY3, and
the AWS Cloud Credits for Research program of Amazon Inc."
REFERENCES,0.24906367041198502,References
REFERENCES,0.250936329588015,"Yu Bai, Yu-Xiang Wang, and Edo Liberty.
ProxQuant: Quantized neural networks via
proximal operators. In International Conference on Learning Representations, 2019. 1, 7"
REFERENCES,0.25280898876404495,"Gilles Bareilles, Franck Iutzeler, and J´erˆome Malick.
Newton acceleration on manifolds
identiﬁed by proximal-gradient methods. Technical report, 2020. arXiv:2012.12936. 2"
REFERENCES,0.2546816479400749,"Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is
the state of neural network pruning? In Machine Learning and Systems, 2020. 3"
REFERENCES,0.2565543071161049,"Patrick Breheny and Jian Huang. Penalized methods for bi-level variable selection. Statistics
and its interface, 2(3):369, 2009. 22"
REFERENCES,0.25842696629213485,"James V. Burke and Michael C. Ferris. Weak sharp minima in mathematical programming.
SIAM Journal on Control and Optimization, 31(5):1340–1359, 1993. 19"
REFERENCES,0.2602996254681648,"Ashok Cutkosky and Francesco Orabona.
Momentum-based variance reduction in non-
convex SGD. In Advances in Neural Information Processing Systems, pp. 15236–15245,
2019. 2"
REFERENCES,0.26217228464419473,"Aaron Defazio and Leon Bottou. On the ineﬀectiveness of variance reduced optimization
for deep learning. In Advances in Neural Information Processing Systems, pp. 1755–1765,
2019. 2"
REFERENCES,0.2640449438202247,"Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in neural
information processing systems, pp. 1646–1654, 2014. 2"
REFERENCES,0.26591760299625467,"Tristan Deleu and Yoshua Bengio. Structured sparsity inducing adaptive optimizers for deep
learning. Technical report, 2021. arXiv:2102.03869. 1, 7, 20, 22"
REFERENCES,0.26779026217228463,"John C. Duchi and Feng Ruan. Asymptotic optimality in stochastic optimization. The
Annals of Statistics, 49(1):21–48, 2021. 5"
REFERENCES,0.2696629213483146,"Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the
lottery: Making all tickets winners. In International Conference on Machine Learning,
pp. 2943–2952, 2020. 3, 9"
REFERENCES,0.27153558052434457,"Jerome Friedman, Trevor Hastie, and Robert Tibshirani. A note on the group lasso and a
sparse group lasso. Technical report, 2010. 6"
REFERENCES,0.27340823970037453,"Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks.
Technical report, 2019. arXiv:1902.09574. 3"
REFERENCES,0.2752808988764045,"A.M. Gupal. Stochastic methods for solving nonsmooth extremal problems. Naukova Dumka,
1979. 5, 14"
REFERENCES,0.27715355805243447,"Warren L. Hare. Identifying active manifolds in regularization problems. In Fixed-Point
Algorithms for Inverse Problems in Science and Engineering, pp. 261–271. Springer, 2011.
2"
REFERENCES,0.27902621722846443,"Warren L. Hare and Adrian S. Lewis. Identifying active constraints via partial smoothness
and prox-regularity. Journal of Convex Analysis, 11(2):251–266, 2004. 2, 4"
REFERENCES,0.2808988764044944,"Warren L. Hare and Adrian S. Lewis. Identifying active manifolds. Algorithmic Operations
Research, 2(2):75–75, 2007. 2"
REFERENCES,0.28277153558052437,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE conference on computer vision and pattern recognition, pp. 770–778,
2016. 8, 20"
REFERENCES,0.2846441947565543,"Lu Hou, Quanming Yao, and James T Kwok. Loss-aware binarization of deep networks. In
International Conference on Learning Representations, 2017. 6"
REFERENCES,0.28651685393258425,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. Densely
connected convolutional networks. In IEEE conference on computer vision and pattern
recognition, pp. 4700–4708, 2017. 20"
REFERENCES,0.2883895131086142,"Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.
Binarized neural networks. Advances in neural information processing systems, 2016. 6"
REFERENCES,0.2902621722846442,"Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International conference on machine learning, pp.
448–456, 2015. 20"
REFERENCES,0.29213483146067415,"Samy Jelassi and Aaron Defazio. Dual averaging is surprisingly eﬀective for deep learning
optimization. Technical report, 2020. arXiv:2010.10502. 3, 4, 7"
REFERENCES,0.2940074906367041,"Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive
variance reduction.
Advances in neural information processing systems, pp. 315–323,
2013. 2"
REFERENCES,0.2958801498127341,"Diederik P. Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
In
International Conference on Learning Representations, 2015. 1"
REFERENCES,0.29775280898876405,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,
2009. 8"
REFERENCES,0.299625468164794,"Vyacheslav Kungurtsev and Vladimir Shikhman. Regularized quasi-monotone method for
stochastic optimization. Technical report, 2021. arXiv:2107.03779. 3, 5"
REFERENCES,0.301498127340824,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 8"
REFERENCES,0.30337078651685395,"Ching-pei Lee.
Accelerating inexact successive quadratic approximation for regularized
optimization through manifold identiﬁcation. Technical report, 2020. arXiv:2012.02522.
2, 17"
REFERENCES,0.3052434456928839,"Sangkyun Lee and Stephen J. Wright. Manifold identiﬁcation in dual averaging for regu-
larized stochastic online learning. Journal of Machine Learning Research, 13:1705–1744,
2012. 2, 4, 5"
REFERENCES,0.30711610486891383,"Adrian S. Lewis. Active sets, nonsmoothness, and sensitivity. SIAM Journal on Optimiza-
tion, 13(3):702–725, 2002. 2, 4"
REFERENCES,0.3089887640449438,"Adrian S. Lewis and Shanshan Zhang. Partial smoothness, tilt stability, and generalized
hessians. 23(1):74–94, 2013. 2"
REFERENCES,0.31086142322097376,"Yu-Sheng Li, Wei-Lin Chiang, and Ching-pei Lee. Manifold identiﬁcation for ultimately
communication-eﬃcient distributed optimization.
In International Conference on Ma-
chine Learning, 2020. 2"
REFERENCES,0.31273408239700373,"Jingwei Liang, Jalal Fadili, and Gabriel Peyr´e. Activity identiﬁcation and local linear con-
vergence of forward–backward-type methods. 27(1):408–437, 2017a. 2"
REFERENCES,0.3146067415730337,"Jingwei Liang, Jalal Fadili, and Gabriel Peyr´e. Local convergence properties of douglas–
rachford and alternating direction method of multipliers. Journal of Optimization Theory
and Applications, 172(3):874–913, 2017b. 2"
REFERENCES,0.31647940074906367,"Lam M. Nguyen, Katya Scheinberg, and Martin Tak´aˇc.
Inexact SARAH algorithm for
stochastic optimization. Optimization Methods and Software, 36(1):237–258, 2021. 2"
REFERENCES,0.31835205992509363,"Evgeni Alekseevich Nurminskii. The quasigradient method for the solving of the nonlinear
programming problems. Cybernetics, 9(1):145–150, 1973. 5"
REFERENCES,0.3202247191011236,"Brendan O’donoghue and Emmanuel Candes.
Adaptive restart for accelerated gradient
schemes. Foundations of computational mathematics, 15(3):715–732, 2015. 7"
REFERENCES,0.32209737827715357,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Advances in neural information
processing systems, pp. 8026–8037, 2019. 7"
REFERENCES,0.32397003745318353,"Nhan H. Pham, Lam M. Nguyen, Dzung T. Phan, and Quoc Tran-Dinh. ProxSARAH: An
eﬃcient algorithmic framework for stochastic composite nonconvex optimization. Journal
of Machine Learning Research, 21:1–48, 2020. 2"
REFERENCES,0.3258426966292135,"Ren´e Poliquin and R Rockafellar. Prox-regular functions in variational analysis. Transac-
tions of the American Mathematical Society, 348(5):1805–1838, 1996. 4, 5"
REFERENCES,0.32771535580524347,"Clarice Poon, Jingwei Liang, and Carola-Bibiane Sch¨onlieb. Local convergence properties of
SAGA/prox-SVRG and acceleration. In International Conference on Machine Learning,
2018. 2"
REFERENCES,0.3295880149812734,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. 2015. 8"
REFERENCES,0.33146067415730335,"Yifan Sun, Halyun Jeong, Julie Nutini, and Mark Schmidt. Are we there yet? manifold iden-
tiﬁcation of gradient-related proximal methods. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 1110–1119, 2019. 2, 4"
REFERENCES,0.3333333333333333,"Varun Sundar and Rajat Vadiraj Dwaraknath. [reproducibility report] rigging the lottery:
Making all tickets winners. In ML Reproducibility Challenge 2020, 2021. 20, 26"
REFERENCES,0.3352059925093633,"Wei Tao, Zhisong Pan, Gaowei Wu, and Qing Tao. Primal averaging: A new gradient evalu-
ation step to attain the optimal individual convergence. IEEE transactions on cybernetics,
50(2):835–845, 2018. 4"
REFERENCES,0.33707865168539325,"Quoc Tran-Dinh, Nhan H. Pham, Dzung T. Phan, and Lam M. Nguyen. Hybrid stochastic
gradient descent algorithms for stochastic nonconvex optimization. Technical report, 2019.
2"
REFERENCES,0.3389513108614232,"Sagar Verma and Jean-Christophe Pesquet. Sparsifying networks via subdiﬀerential inclu-
sion. In International Conference on Machine Learning, pp. 10542–10552, 2021. 3"
REFERENCES,0.3408239700374532,"Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momen-
tum: Faster variance reduction algorithms. In Advances in Neural Information Processing
Systems, pp. 2406–2416, 2019. 2"
REFERENCES,0.34269662921348315,"Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured
sparsity in deep neural networks. Advances in neural information processing systems, pp.
2074–2082, 2016. 1, 6, 7"
REFERENCES,0.3445692883895131,"Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin
Hu, Yiran Chen, and Hai Li. Learning intrinsic sparse structures within long short-term
memory. In International Conference on Learning Representations, 2018. 1"
REFERENCES,0.3464419475655431,"Stephen J. Wright. Accelerated block-coordinate relaxation for regularized optimization.
SIAM Journal on Optimization, 22(1):159–186, 2012. 2"
REFERENCES,0.34831460674157305,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. Technical report, 2017. arXiv:1708.07747. 8"
REFERENCES,0.350187265917603,"Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimiza-
tion. Journal of Machine Learning Research, 11(88):2543–2596, 2010. 3, 5"
REFERENCES,0.352059925093633,"Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance
reduction. SIAM Journal on Optimization, 24(4):2057–2075, 2014. 2"
REFERENCES,0.3539325842696629,"Yang Yang, Yaxiong Yuan, Avraam Chatzimichailidis, Ruud JG van Sloun, Lei Lei, and
Symeon Chatzinotas. ProxSGD: Training structured neural networks under regularization
and constraints. In International Conference on Learning Representations, 2019. 1, 6, 7"
REFERENCES,0.35580524344569286,"Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49–67,
2006. 6"
REFERENCES,0.35767790262172283,"Jihun Yun, Aur´elie C Lozano, and Eunho Yang. Adaptive proximal gradient methods for
structured neural networks. Advances in Neural Information Processing Systems, 2021.
1, 7"
REFERENCES,0.3595505617977528,"Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The
Annals of statistics, 38(2):894–942, 2010. 21"
REFERENCES,0.36142322097378277,"Peng Zhao and Bin Yu. On model selection consistency of Lasso. The Journal of Machine
Learning Research, 7:2541–2563, 2006. 1"
REFERENCES,0.36329588014981273,"Hao Zhou, Jose M. Alvarez, and Fatih Porikli. Less is more: Towards compact CNNs. In
European Conference on Computer Vision, pp. 662–677. Springer, 2016. 6"
REFERENCES,0.3651685393258427,Appendices
REFERENCES,0.36704119850187267,Table of Contents
REFERENCES,0.36891385767790263,"A
Proofs
13
A.1 Proof of Lemma 1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
A.2 Proof of Lemma 2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
A.3 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.4 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17"
REFERENCES,0.3707865168539326,"B
Additional Discussions on Applications
18
B.1
Structured Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
B.2
Binary Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . .
20"
REFERENCES,0.37265917602996257,"C
Experiment Setting Details
20"
REFERENCES,0.37453183520599254,"D
More Results from the Experiments
20"
REFERENCES,0.37640449438202245,"E
Other Regularizers for Possibly Better Group Sparsity and General-
ization
21"
REFERENCES,0.3782771535580524,"A
Proofs"
REFERENCES,0.3801498127340824,"A.1
Proof of Lemma 1"
REFERENCES,0.38202247191011235,"Proof. Using (3), the distance between W t and W ∗can be upper bounded through the
triangle inequality:W t −W ∗ =
(1 −ct)
 
W t−1 −W ∗
+ ct
  ˜W t −W ∗"
REFERENCES,0.3838951310861423,"≤ct
 ˜W t −W ∗ + (1 −ct)
W t−1 −W ∗.
(8)"
REFERENCES,0.3857677902621723,"For any event such that ˜W t →W ∗, for any ϵ > 0, we can ﬁnd Tϵ ≥0 such that
 ˜W t −W ∗ ≤ϵ,
∀t ≥Tϵ."
REFERENCES,0.38764044943820225,"Let δt := ∥W t −W ∗∥, we see from the above inequality and (8) that"
REFERENCES,0.3895131086142322,"δt ≤(1 −ct) δt−1 + ctϵ,
∀t ≥Tϵ."
REFERENCES,0.3913857677902622,"By deducting ϵ from both sides, we get that"
REFERENCES,0.39325842696629215,"(δt −ϵ) ≤(1 −ct) (δt−1 −ϵ) ,
∀t ≥Tϵ."
REFERENCES,0.3951310861423221,"Since P ct = ∞, we further deduce that"
REFERENCES,0.3970037453183521,"lim
t→∞(δt −ϵ) ≤ ∞
Y"
REFERENCES,0.398876404494382,"t=Tϵ
(1 −ct) (δTϵ−1 −ϵ) ≤ ∞
Y"
REFERENCES,0.40074906367041196,"t=Tϵ
exp (−ct) (δTϵ−1 −ϵ) = exp  − ∞
X"
REFERENCES,0.40262172284644193,"t=Tϵ
ct !"
REFERENCES,0.4044943820224719,"(δTϵ−1 −ϵ) = 0,"
REFERENCES,0.40636704119850187,"where in the ﬁrst inequality we used the fact that exp(x) ≥1+x for all real number x. The
result above then implies that
lim
t→∞δt ≤ϵ."
REFERENCES,0.40823970037453183,"As ϵ is arbitrary and δt ≥0 from the deﬁnition, we conclude that limt→∞δt = 0, which is
equivalent to that W t →W ∗."
REFERENCES,0.4101123595505618,"A.2
Proof of Lemma 2"
REFERENCES,0.41198501872659177,Proof. We observe that
REFERENCES,0.41385767790262173,"α−1
t V t = t
X k=1 ηkβk"
REFERENCES,0.4157303370786517,"αt
∇fξk
 
W k−1"
REFERENCES,0.41760299625468167,= αt−1
REFERENCES,0.41947565543071164,"αt
α−1
t−1V t−1 + αt −αt−1"
REFERENCES,0.42134831460674155,"αt
∇fξt
 
W t−1"
REFERENCES,0.4232209737827715,"=

1 −βtηt αt"
REFERENCES,0.4250936329588015,"
α−1
t−1V t−1 + βtηt"
REFERENCES,0.42696629213483145,"αt
∇fξt
 
W t−1
."
REFERENCES,0.4288389513108614,"From that f is L-Lipschitz-continuously diﬀerentiable, we have that
Eξt+1∼D

∇fξt+1
 
W t
−Eξt∼D

∇fξt
 
W t−1 =
∇f
 
W t
−f
 
W t−1"
REFERENCES,0.4307116104868914,"≤L
W t −W t−1.
(9)"
REFERENCES,0.43258426966292135,"Therefore, (4) and (9) imply that 0 ≤"
REFERENCES,0.4344569288389513,"Eξt+1∼D

∇fξt+1 (W t)

−Eξt∼D

∇fξt
 
W t−1"
REFERENCES,0.4363295880149813,"βtηtα−1
t
≤L"
REFERENCES,0.43820224719101125,W t −W t−1
REFERENCES,0.4400749063670412,"βtηtα−1
t"
REFERENCES,0.4419475655430712,"a.s.
−−→
0,"
REFERENCES,0.4438202247191011,"which together with the sandwich lemma shows that
Eξt+1∼D

∇fξt+1 (W t)

−Eξt∼D

∇fξt
 
W t−1"
REFERENCES,0.44569288389513106,"βtηtα−1
t"
REFERENCES,0.44756554307116103,"a.s.
−−→
0.
(10)"
REFERENCES,0.449438202247191,"Therefore, the ﬁrst two conditions of (4) together with (10) and the bounded variance
assumption satisfy the requirements of (Gupal, 1979, Chapter 2.4, Theorem 4.1), so the
conclusion of almost sure convergence hold."
REFERENCES,0.45131086142322097,"For the convergence in L2 part, we ﬁrst deﬁne mt := α−1
t Vt and τt := βtηtα−1
t
for notational
ease. Consider
mt+1 −∇F(W t)
2, we have from the update rule in Algorithm 1 that
mt+1 −∇F(W t)
2"
REFERENCES,0.45318352059925093,"=
(1 −τt)mt + τt∇fξt+1(W t) −∇F(W t)
2"
REFERENCES,0.4550561797752809,"=
(1 −τt)
 
mt −∇F(W t)

+ τt
 
∇fξt+1(W t) −∇F(W t)
2"
REFERENCES,0.45692883895131087,"= (1 −τt)2mt −∇F(W t)
2 + τ 2
t
∇fξt+1(W t) −∇F(W t)
2"
REFERENCES,0.45880149812734083,"+ 2τt(1 −τt)⟨mt −∇F(W t), ∇fξt+1(W t) −∇F(W t)⟩"
REFERENCES,0.4606741573033708,"= (1 −τt)2 
mt −∇F
 
W t−1
+
 
∇F
 
W t−1
−∇F
 
W t2
(11)"
REFERENCES,0.46254681647940077,"+ τ 2
t
∇fξt+1(W t) −∇F(W t)
2 + 2τt(1 −τt)⟨mt −∇F(W t), ∇fξt+1(W t) −∇F(W t)⟩."
REFERENCES,0.46441947565543074,"Let {Ft}t≥0 denote the natural ﬁltration of {(mt, W t)}t≥0. Namely, Ft records the informa-
tion of W 0, {ci}t−1
i=0, {ηi}t−1
i=0, and {ξi}t
i=1. By deﬁning Ut :=
mt −∇F(W t−1)
2 and taking
expectation over (11) conditional on Ft, we obtain from E

∇fξt+1(W t) | Ft

= ∇F(W t)
that"
REFERENCES,0.46629213483146065,"E [Ut+1 | Ft] = (1 −τt)2 
mt −∇F
 
W t−1
+
 
∇F
 
W t−1
−∇F
 
W t2"
REFERENCES,0.4681647940074906,"+ τ 2
t E
h∇fξt(W t) −∇F(W t)
2 | Ft
i
.
(12)"
REFERENCES,0.4700374531835206,"From the last condition in (4) and the Lipschitz continuity of ∇F, there are random variables
{ϵt} and {ut} such that ∥ut∥= 1, ϵt ≥0, and ∇F(W t−1) −∇F(W t) = τtϵtut for all t > 0,
with ϵt ↓0 almost surely. We thus obtain that
mt −∇F(W t−1) + ∇F(W t−1) −∇F(W t)
2"
REFERENCES,0.47191011235955055,"=
mt −∇F(W t−1) + τtϵtut
2"
REFERENCES,0.4737827715355805,"= (1 + τt)2

1
1 + τt"
REFERENCES,0.4756554307116105," 
mt −∇F
 
W t−1
+
τt
1 + τt
ϵtut  2"
REFERENCES,0.47752808988764045,"≤(1 + τt)2

1
1 + τt
Ut +
τt
1 + τt
ϵt
2

,
(13)"
REFERENCES,0.4794007490636704,"where we used Jensen’s inequality and the convexity of ∥·∥2 in the last inequality.
By
substituting (13) back into (12), we obtain"
REFERENCES,0.4812734082397004,E [Ut+1|Ft]
REFERENCES,0.48314606741573035,"≤(1 −τt)2(1 + τt)Ut + (1 −τt)2(1 + τt)τtϵt
2 + τt
2E
h∇fξt(W t) −∇F(W t)
2 | Ft
i"
REFERENCES,0.4850187265917603,"≤(1 −τt)(Ut + τtϵt
2) + τt
2E
h∇fξt(W t) −∇F(W t)
2 | Ft
i"
REFERENCES,0.4868913857677903,"≤(1 −τt)Ut + τtϵt
2 + τt
2E
h∇fξt(W t) −∇F(W t)
2 | Ft
i
.
(14)"
REFERENCES,0.4887640449438202,"For the last term in (14), we notice that"
REFERENCES,0.49063670411985016,"E
h∇fξt(W t) −∇F(W t)
2 | Ft
i
≤2

E
h∇fξt(W t)
2i
+
∇F(W t)
2"
REFERENCES,0.49250936329588013,"≤2

C +
∇F(W t)
2
,
(15)"
REFERENCES,0.4943820224719101,"where the last inequality is from the bounded variance assumption. Since by assumption
the {W t} lies in a bounded set K, we have that for any point W ∗∈K, W t −W ∗is upper
bounded, and thus ∥∇F(W t) −∇F(W ∗)∥is also bounded, implying that ∥∇F(W t)∥2 ≤C2
for some C2 ≥0. Therefore, (15) further leads to"
REFERENCES,0.49625468164794007,"E
h∇fξt(W t) −∇F(W t)
2 | Ft
i
≤C3
(16)"
REFERENCES,0.49812734082397003,for some C3 ≥0.
REFERENCES,0.5,Now we further take expectation on (14) and apply (16) to obtain
REFERENCES,0.50187265917603,"EUt+1 ≤(1 −τt)EUt + τtϵt
2 + τt
2C3 = (1 −τt) EUt + τt
 
ϵ2
t + τtC3

.
(17)"
REFERENCES,0.5037453183520599,"Note that the third implies ϵt ↓0, so this together with the second condition that τt ↓0
means ϵ2
t +τtC3 ↓0 as well, and thus for any δ > 0, we can ﬁnd Tδ ≥0 such that ϵ2
t +τtC3 ≤δ
for all t ≥Tδ. Thus, (17) further leads to"
REFERENCES,0.5056179775280899,"EUt+1 −δ ≤(1 −τt)EUt + τtδ −δ = (1 −τt) (EUt −δ) , ∀t ≥Tδ.
(18)"
REFERENCES,0.5074906367041199,"This implies that (EUt −δ) becomes a decreasing sequence starting from t ≥Tδ, and since
Ut ≥0, this sequence is lower bounded by −δ, and hence it converges to a certain value. By
recursion of (18), we have that"
REFERENCES,0.5093632958801498,EUt −δ ≤ tY
REFERENCES,0.5112359550561798,"i=Tδ
(1 −τi) (EUTδ −δ) ,"
REFERENCES,0.5131086142322098,"and from the well-known inequality (1 + x) ≤expx for all x ∈R, the above result leads to"
REFERENCES,0.5149812734082397,"EUt −δ ≤exp

−
X
i = Tδ
tτi

(EUTδ −δ) ."
REFERENCES,0.5168539325842697,"By letting t approach inﬁnity and noting that the ﬁrst condition of (4) indicates ∞
X"
REFERENCES,0.5187265917602997,"t=k
τt = ∞"
REFERENCES,0.5205992509363296,"for any k ≥0, we see that"
REFERENCES,0.5224719101123596,"−δ ≤lim
t→∞EUt −δ ≤exp  − ∞
X"
REFERENCES,0.5243445692883895,"i=Tδ
τi !"
REFERENCES,0.5262172284644194,"(EUTδ −δ) = 0.
(19)"
REFERENCES,0.5280898876404494,"As δ is arbitrary, by taking δ ↓0 in (19) and noting the nonnegativity of Ut, we conclude
that lim EUt = 0, as desired. This proves the last result in Lemma 2."
REFERENCES,0.5299625468164794,"A.3
Proof of Theorem 1"
REFERENCES,0.5318352059925093,"Proof. Using Lemma 2, we can view α−1
t V t as ∇f(W t) plus some noise that asymptotically
decreases to zero with probability one:"
REFERENCES,0.5337078651685393,"α−1
t Vt = ∇f(W t) + ϵt,
∥ϵt∥
a.s.
−−→0.
(20)"
REFERENCES,0.5355805243445693,We use (20) to rewrite the optimality condition of (2) as (also see Line 5 of Algorithm 1)
REFERENCES,0.5374531835205992,"−
 
∇f
 
W t
+ ϵt + βtα−1
t
  ˜W t −W 0
∈∂ψ
  ˜W t
.
(21)"
REFERENCES,0.5393258426966292,"Now we consider ∂F( ˜W t). Clearly from (21), we have that"
REFERENCES,0.5411985018726592,"∇f
  ˜W t
−∇f
 
W t
−ϵt −βtα−1
t
  ˜W t −W 0
∈∂∇f
  ˜W t
+ ψ
  ˜W t
= ∂F
  ˜W t
.
(22)"
REFERENCES,0.5430711610486891,"Now we consider the said event that ˜W t →W ∗for a certain W ∗, and let us deﬁne this
event as A ⊆Ω. From Lemma 1, we know that W t →W ∗as well under A. Let us deﬁne
B ⊆Ωas the event of ϵt →0, then we know that since P(A) > 0 and P(B) = 1, where P is
the probability function for events in Ω, P(A ∩B) = P(A). Therefore, conditional on the
event of A, we have that ϵt
a.s.
−−→0 still holds. Now we consider any realization of A ∩B. For
the right-hand side of (22), as ˜W t is convergent and βtα−1
t
decreases to zero, by letting t
approach inﬁnity, we have that"
REFERENCES,0.5449438202247191,"lim
t→∞ϵt + βtα−1
t
  ˜W t −W 0
= 0 + 0
 
W ∗−W 0
= 0."
REFERENCES,0.5468164794007491,"By the Lipschitz continuity of ∇f, we have from (3) and (4) that"
REFERENCES,0.548689138576779,"0 ≤
∇f
  ˜W t
−∇f
 
W t ≤L
W t −˜W t."
REFERENCES,0.550561797752809,"As {W t} and { ˜W t} converge to the same point, we see that
W t −˜W t →0, so ∇f
  ˜W t
−
∇f(W t) also approaches zero. Hence, the limit of the right-hand side of (22) is"
REFERENCES,0.552434456928839,"lim
t→∞∇f
  ˜W t
−
 
∇f
 
W t
+ ϵt + βtα−1
t
  ˜W t −W 0
= 0.
(23)"
REFERENCES,0.5543071161048689,"On the other hand, for the left-hand side of (22), the outer semicontinuity of ∂ψ at W ∗and
the continuity of ∇f show that
lim
t→∞∇f( ˜W t) + ∂ψ( ˜W t) ⊆∂∇f(W ∗) + ψ (W ∗) = ∂F(W ∗).
(24)"
REFERENCES,0.5561797752808989,"Substituting (23) and (24) back into (22) then proves that 0 ∈∂F(W ∗) and thus W ∗∈
Z."
REFERENCES,0.5580524344569289,"A.4
Proof of Theorem 2"
REFERENCES,0.5599250936329588,"Proof. Our discussion in this proof are all under the event that ˜W t →W ∗. From the argu-
ment in Appendix A.3, we can view α−1
t V t as ∇f(W t) plus some noise that asymptotically
decreases to zero with probability one as shown in (20). From Lemma 1, we know that
W t →W ∗. From (21), there is U t ∈∂ψ
  ˜W t
such that"
REFERENCES,0.5617977528089888,"U t = −α−1
t V t + α−1
t βt
  ˜W t −W 0
.
(25)
Moreover, we deﬁne
γt := W t −˜W t.
(26)
By combining (25)–(26) with (20), we obtain
min
Y ∈∂F ( ˜
W t) ∥Y ∥"
REFERENCES,0.5636704119850188,"≤
∇f
  ˜W t
+ U t"
REFERENCES,0.5655430711610487,"=
∇f
  ˜W t
−∇f
 
W t
−ϵt −α−1
t βt
  ˜W t −W 0"
REFERENCES,0.5674157303370787,"≤
∇f
  ˜W t
−∇f
 
W t + ∥ϵt∥+ α−1
t βt
 ˜W t −W 0"
REFERENCES,0.5692883895131086,"≤L∥γt∥+ ∥ϵt∥+ α−1
t βt
 W ∗−˜W t +
W 0 −W ∗
,
(27)
where we used the Lipschitz continuity of ∇f and the triangle inequality in the last inequal-
ity."
REFERENCES,0.5711610486891385,"We now separately bound the terms in (27). From that W t →W ∗and ˜W t →W ∗, it is
straightforward that ∥γt∥→0. The second term decreases to zero almost surely according
to (20) and the argument in Appendix A.3.
For the last term, since α−1
t βt →0, and
 ˜W t −W ∗ →0, we know that"
REFERENCES,0.5730337078651685,"α−1
t βt
W 0 −W ∗ →0,
α−1
t βt
 ˜W t −W ∗ →0."
REFERENCES,0.5749063670411985,"Therefore, we conclude from the above argument and (27) that"
REFERENCES,0.5767790262172284,"min
Y ∈∂F ( ˜
W t) ∥Y ∥
a.s.
−−→0."
REFERENCES,0.5786516853932584,"As f is smooth with probability one, we know that if ψ is partly smooth at W ∗relative to M,
then so is F = f + ψ with probability one. Moreover, Lipschitz-continuously diﬀerentiable
functions are always prox-regular, and the sum of two prox-regular functions is still prox-
regular, so F is also prox-regular at W ∗with probability one.
Following the argument
identical to that in Appendix A.3, we know that these probability one events are still
probability one conditional on the event of ˜W t →W ∗as this event has a nonzero probability.
As ˜W t →W ∗and ∇f( ˜W t) + U t
a.s.
−−→0 ∈∂F(W ∗) (the inclusion is from (5)), we have from
the subdiﬀerential continuity of ψ and the smoothness of f that F( ˜W t)
a.s.
−−→F(W ∗). Since
we also have ˜W t →W ∗and minY ∈∂F ( ˜
W t) ∥Y ∥
a.s.
−−→0, clearly
 
˜W t, F
 
W t
,
min
Y ∈∂F( ˜
W t)
∥Y ∥ !"
REFERENCES,0.5805243445692884,"a.s.
−−→(W ∗, F(W ∗), 0) .
(28)"
REFERENCES,0.5823970037453183,"Therefore, (28) and (5) together with the assumptions on ψ at W ∗imply that with proba-
bility one, all conditions of Lemma 1 of Lee (2020) are satisﬁed, so from it, (6) holds almost
surely, conditional on the event of ˜W t →W ∗."
REFERENCES,0.5842696629213483,"B
Additional Discussions on Applications"
REFERENCES,0.5861423220973783,"We now discuss in more technical details the applications in Section 4.1, especially regarding
how the regularizers satisfy the properties required by our theory."
REFERENCES,0.5880149812734082,"B.1
Structured Sparsity"
REFERENCES,0.5898876404494382,"We start our discussion with the simple ℓ1 norm as the warm-up for the group-LASSO norm.
It is clear that ∥W∥1 is a convex function that is ﬁnite everywhere, so it is prox-regular,
subdiﬀerentially continuous, and regular everywhere, hence we just need to discuss about
the remaining parts in Deﬁnition 1. Consider a problem with dimension n > 0. Note that"
REFERENCES,0.5917602996254682,"∥x∥1 = n
X"
REFERENCES,0.5936329588014981,"i=1
|xi|,"
REFERENCES,0.5955056179775281,"and the absolute value is smooth everywhere except the point of origin. Therefore, it is
clear that ∥x∥1 is locally smooth if xi ̸= 0 for all i. For any point x∗, when there is an index
set I such that x∗
i = 0 for all i ∈I and x∗
i ̸= 0 for i /∈I, we see that the part of the norm
corresponds to IC (the complement of I):
X"
REFERENCES,0.5973782771535581,"i∈IC
|x∗
i |"
REFERENCES,0.599250936329588,"is locally smooth around x∗. Without loss of generality, we assume that I = {1, 2, . . . , k}
for some k ≥0, then the subdiﬀerential of ∥x∥1 at x∗is the set"
REFERENCES,0.601123595505618,"{sgn(x1)} × · · · × {sgn(xk)} × [−1, 1]n−k,
(29)"
REFERENCES,0.602996254681648,"and clearly if we move from x∗along any direction y := (y1, . . . , yk, 0, . . . , 0) with a small
step, the function value changes smoothly as it is a linear function, satisfying the ﬁrst
condition of Deﬁnition 1. Along the same direction y with a small enough step, the set
of subdiﬀerential remains the same, so the continuity of subdiﬀerential requirement holds.
We can also observe from the above argument that the manifold should be Mx∗= {x |
xi = 0, ∀i ∈I}, and clearly it is a subspace of Rn with its normal space at x∗being
N := {y | ⟨x∗, y⟩= 0} = {y | yi = 0, ∀i ∈IC}, which is clearly the aﬃne span of (29) with
the translation being (sgn(x1) × · · · × sgn(xk), 0, . . . , 0). Moreover, indeed the manifolds are
low dimensional ones, and for iterates approaching x∗, staying in this active manifold means
that the (unstructured) sparsity of the iterates is the same as the limit point x∗. We also
provide a graphical illustration of ∥x∥1 with n = 2 in Fig. 3. We can observe that for any
x with x1 ̸= 0 and x2 ̸= 0, the function is smooth locally around any point, meaning that
∥x∥1 is partly smooth relative to the whole space at x (so actually smooth locally around
x). For x with x1 = 0, the function value corresponds to the sharp valley in the graph, and
we can see that the function is smooth along the valley, and this valley corresponds to the
one-dimensional manifold {x | x1 = 0} for partial smoothness."
REFERENCES,0.6048689138576779,"Next, we use the same graph to illustrate the importance of manifold identiﬁcation. Consider
that the red point x∗= (0, 1.5) is the limit point of the iterates of a certain algorithm, and
the yellow points and black points are two sequences that both converge to x∗. If the iterates
of the algorithm are the black points, then clearly except for the limit point itself, all iterates
are nonsparse, and thus the ﬁnal output of the algorithm is also nonsparse unless we can get
to exactly the limit point within ﬁnite iterations (which is usually impossible for iterative
methods). On the other hand, if the iterates are the yellow points, this is the case that
the manifold is identiﬁed, because all points sit in the valley and enjoy the same sparsity
pattern as the limit point x∗. This is why we concern about manifold identiﬁcation when
we solve regularized optimization problems."
REFERENCES,0.6067415730337079,"From this example, we can also see an explanation for why our algorithm with the property
of manifold identiﬁcation performs better than other methods without such a property.
Consider a Euclidean space any point x∗with an index set I such that x∗
I = 0 and |I| > 0.
This means that x∗has at least one coordinate being zero, namely x∗contains sparsity.
Now let
ϵ0 := min
i∈IC
|x∗
i |,"
REFERENCES,0.6086142322097379,Figure 3: An illustration of partial smoothness of the ℓ1 norm.
REFERENCES,0.6104868913857678,"then from the deﬁnition of I, ϵ0 > 0. Fro any sequence {xt} converging to x∗, for any
ϵ ∈(0, ϵ0), we can ﬁnd Tϵ ≥0 such that
xt −x∗
2 ≤ϵ,
∀t ≥Tϵ."
REFERENCES,0.6123595505617978,"Therefore, for any i /∈I, we must have that xt
i ̸= 0 for all t ≥Tϵ. Otherwise, ∥xt −x∗∥2 ≥ϵ0,
but ϵ0 > ϵ ≥∥xt −x∗∥2, leading to a contradiction. On the other hand, for any i ∈I, we can
have xt
i ̸= 0 for all t without violating the convergence. That being said, for any sequence
converging to x∗, eventually the iterates cannot be sparser than x∗, so the sparsity level
of x∗, or of its active manifold, is the local upper bound for the sparsity level of points
converging to x∗. Therefore, if iterates of two algorithms converge to the same limit point,
the one with a proven manifold identiﬁcation ability clearly will produce a higher sparsity
level."
REFERENCES,0.6142322097378277,"Similar to our example here, in applications other than sparsity, iterates converging to a
limit point dwell on super-manifolds of the active manifold, and the active manifold is the
minimum one that locally describes points with the same structure as the limit point, and
thus identifying this manifold is equivalent to ﬁnding the locally most ideal structure of the
application."
REFERENCES,0.6161048689138576,"Now back to the sparsity case. One possible concern is the case that the limit point is (0, 0)
in the two-dimension example. In this case, the manifold is the 0-dimensional subspace {0}.
If this is the case and manifold identiﬁcation can be ensured, it means that limit point itself
can be found within ﬁnite iterations. This case is known as the weak sharp minima (Burke
& Ferris, 1993) in nonlinear optimization, and its associated ﬁnite termination property is
also well-studied."
REFERENCES,0.6179775280898876,"For this example, We also see that ∥x∥1 is partly smooth at any point x∗, but the manifold
diﬀers with x∗. This is a speciﬁc benign example, and in other cases, partial smoothness
might happen only locally at some points of interest instead of everywhere."
REFERENCES,0.6198501872659176,"Next, we further extend our argument above to the case of (7). This can be viewed as
the ℓ1 norm for each group and we can easily obtain similar results.
Again, since the
group-LASSO norm is also convex and ﬁnite everywhere, prox-regularity, regularity, and
subdiﬀerential continuity are not issues at all. For the other properties, we consider one
group ﬁrst, then the group-LASSO norm reduces to the ℓ2 norm. Clearly, ∥x∥2 is smooth
locally if x ̸= 0, with the gradient being x/∥x∥2, but it is nonsmooth at the point x = 0,
where the subdiﬀerential is the unit ball. This is very similar to the absolute value, whose
subdiﬀerential at 0 is the interval [−1, 1]. Thus, we can directly apply similar arguments
above, and conclude that for any W ∗, (7) is partly smooth at W ∗with respect to the
manifold MW ∗= {W | WIg = 0, ∀g : W ∗
Ig = 0}, which is again a lower-dimensional
subspace. Therefore, the manifold of deﬁning the partial smoothness for the group-LASSO
norm exactly corresponds to its structured sparsity pattern."
REFERENCES,0.6217228464419475,"B.2
Binary Neural Networks"
REFERENCES,0.6235955056179775,"We continue to consider the binary neural network problem.
For easier description, for
the Euclidean space E we consider, we will use a vectorized representation for W, A ∈E
such that the elements are enumerated as W1, . . . , Wn and α1, . . . , αn. The corresponding
optimization problem can therefore be written as"
REFERENCES,0.6254681647940075,"min
W,A∈E
Eξ∼D [fξ (W)] + λ n
X i=1"
REFERENCES,0.6273408239700374,"
αi (wi + 1)2 + (1 −αi) (wi −1)2 + δ[0,1] (αi)

,
(30)"
REFERENCES,0.6292134831460674,"where given any set C, δC is the indicator function of C, deﬁned as"
REFERENCES,0.6310861423220974,"δC(x) =
0
if x ∈C,
∞
else."
REFERENCES,0.6329588014981273,"We see that except for the indicator function part, the objective is smooth, so the real partly
smooth term that we treat as the regularizer is"
REFERENCES,0.6348314606741573,"Φ(α) := n
X"
REFERENCES,0.6367041198501873,"i=1
δ[0,1](αi)."
REFERENCES,0.6385767790262172,"We note that for αi ∈(0, 1), the value of δ[0,1](αi) remains a constant zero in a neighbor-
hood of αi, and for αi /∈[0, 1], the indicator function is also constantly inﬁnite within a
neighborhood. Thus, the point of nonsmoothness, happens only at αi ∈{0, 1}, and similar
to our discussion in the previous subsection, Φ is partly smooth along directions that we ﬁx
those αi at the boundary (namely, being either 0 or 1) unchanged. The identiﬁed manifold
therefore corresponds to the entries of αi that are ﬁxed at 0 or 1, and this can serve as the
indicator for the desired binary pattern in this task."
REFERENCES,0.6404494382022472,"C
Experiment Setting Details"
REFERENCES,0.6423220973782772,"For the weights wg of each group in (7), for all experiments in Section 5, we follow Deleu &
Bengio (2021) to set wg =
p"
REFERENCES,0.6441947565543071,"|Ig|. All ProxSSI parameter settings, excluding the regulariza-
tion weight and the learning rate schedule, follow the default values in their package."
REFERENCES,0.6460674157303371,"Tables 3 to 13 provide detailed settings of Section 5.2. For the modiﬁed VGG19 model,
we follow Deleu & Bengio (2021) to eliminate all fully-connected layers except the output
layer, and add one batch-norm layer (Ioﬀe & Szegedy, 2015) after each convolutional layer
to simulate modern CNNs like those proposed in He et al. (2016); Huang et al. (2017). For
ResNet50 in the structured sparsity experiment in Section 5.2, our version of ResNet50 is
the one constructed by the publicly available script at https://github.com/weiaicunzai/
pytorch-cifar100."
REFERENCES,0.6479400749063671,"In the unstructured sparsity experiment presented in Section 5.3, for better comparison
with existing works in the literature of pruning, we adopt the version of ResNet50 used by
Sundar & Dwaraknath (2021).3 Table 14 provides detailed settings of Section 5.3. For RigL,
we use the PyTorch implementation of Sundar & Dwaraknath (2021)."
REFERENCES,0.649812734082397,"D
More Results from the Experiments"
REFERENCES,0.651685393258427,"In this section, we provide more details of the results of the experiments we conducted in the
main text. In particular, in Fig. 4, we present the change of validation accuracies and group
sparsity levels with epochs for the group sparsity tasks in Section 5.2. We then present in
Fig. 5 validation accuracies and unstructured sparsity level versus epochs for the task in
Section 5.3. We note that although it takes more epochs for RMDA to fully stabilize in
terms of manifold identiﬁcation, the sparsity level usually only changes in a very limited
range once (sometimes even before) the validation accuracy becomes steady, meaning that
we do not need to run the algorithm for an unreasonably long time to obtain satisfactory
results."
REFERENCES,0.653558052434457,3https://github.com/varun19299/rigl-reproducibility.
REFERENCES,0.6554307116104869,Table 3: Details of the experimental settings of logistic regression on MNIST in Section 5.2.
REFERENCES,0.6573033707865169,"Parameter
Value
Data set
MNIST
Model
Logistic regression
Loss function
Cross entropy
Regularization function
Group LASSO
Regularization weight
10−3
Total epochs
500"
REFERENCES,0.6591760299625468,"ProxSGD
Learning rate schedule
η(epoch) = 10−1−⌊epoch/50⌋"
REFERENCES,0.6610486891385767,"Momentum
10−1"
REFERENCES,0.6629213483146067,"ProxSSI
Learning rate schedule
η(epoch) = 10−3−⌊epoch/50⌋"
REFERENCES,0.6647940074906367,"RMDA
Restart epochs
50, 100, 150, 200
Learning rate schedule
η(epoch) = max(10−5, 10−1−⌊epoch/50⌋)
Momentum schedule
c(epoch) = min(1, 10−2+⌊epoch/50⌋)"
REFERENCES,0.6666666666666666,"Table 4: Details of the experimental settings of the multi-layer fully-connected NN on
FashionMNIST in Section 5.2."
REFERENCES,0.6685393258426966,"Parameter
Value
Data set
FashionMNIST
Model
Fully-connected NN (Table 11)
Loss function
Cross entropy
Regularization function
Group LASSO
Total epochs
500"
REFERENCES,0.6704119850187266,"ProxSGD
Regularization weight
10−4"
REFERENCES,0.6722846441947565,"Learning rate schedule
η(epoch) = 10−1−⌊epoch/50⌋"
REFERENCES,0.6741573033707865,"Momentum
10−1"
REFERENCES,0.6760299625468165,"ProxSSI
Regularization weight
4 × 10−6"
REFERENCES,0.6779026217228464,"Learning rate schedule
η(epoch) = 10−3−⌊epoch/50⌋"
REFERENCES,0.6797752808988764,"RMDA
Regularization weight
7 × 10−5
Restart epochs
50, 100, 150, 200
Learning rate schedule
η(epoch) = max(10−5, 10−1−⌊epoch/50⌋)
Momentum schedule
c(epoch) = min(1, 10−2+⌊epoch/50⌋)"
REFERENCES,0.6816479400749064,"E
Other Regularizers for Possibly Better Group Sparsity
and Generalization"
REFERENCES,0.6835205992509363,"A downside of (7) is that it pushes all groups toward zeros and thus introduces bias in the
ﬁnal model. For its remedy, minimax concave penalty (MCP, Zhang, 2010) is then proposed
to penalize only the groups whose norm is smaller than a user-speciﬁed threshold. More
precisely, given hyperparameters λ ≥0, ω ≥1, the one-dimensional MCP is deﬁned by"
REFERENCES,0.6853932584269663,"MCP(w; λ, ω) :="
REFERENCES,0.6872659176029963,"(
λ|w| −w2"
REFERENCES,0.6891385767790262,"2ω
if|w| < ωλ,
ωλ2"
REFERENCES,0.6910112359550562,"2
if|w| ≥ωλ."
REFERENCES,0.6928838951310862,"One can then apply the above formulation to the norm of a vector to achieve the eﬀect of
inducing group-sparsity. In our case, given an index set Ig that represents a group, the"
REFERENCES,0.6947565543071161,Table 5: Details of the experimental settings of LeNet5 on MNIST in Section 5.2
REFERENCES,0.6966292134831461,"Parameter
Value
Data set
MNIST
Model
LeNet5 (Table 12)
Loss function
Cross entropy
Regularization function
Group LASSO
Total epochs
500"
REFERENCES,0.6985018726591761,"ProxSGD
Regularization weight
1.2 × 10−4"
REFERENCES,0.700374531835206,"Learning rate schedule
η(epoch) = 10−1−⌊epoch/50⌋"
REFERENCES,0.702247191011236,"Momentum
10−1"
REFERENCES,0.704119850187266,"ProxSSI
Regularization weight
9 × 10−5"
REFERENCES,0.7059925093632958,"Learning rate schedule
η(epoch) = 10−3−⌊epoch/50⌋"
REFERENCES,0.7078651685393258,"RMDA
Regularization weight
10−4
Restart epochs
50, 100, 150, 200
Learning rate schedule
η(epoch) = max(10−4, 10−⌊epoch/50⌋)
Momentum schedule
c(epoch) = min(1, 10−2+⌊epoch/50⌋)"
REFERENCES,0.7097378277153558,Table 6: Details of the experimental settings of LeNet5 on FashionMNIST in Section 5.2
REFERENCES,0.7116104868913857,"Parameter
Value
Data set
FashionMNIST
Model
LeNet5 (Table 12)
Loss function
Cross entropy
Regularization function
Group LASSO
Total epochs
500"
REFERENCES,0.7134831460674157,"ProxSGD
Regularization weight
1.2 × 10−4"
REFERENCES,0.7153558052434457,"Learning rate schedule
η(epoch) = 10−1−⌊epoch/50⌋"
REFERENCES,0.7172284644194756,"Momentum
10−1"
REFERENCES,0.7191011235955056,"ProxSSI
Regularization weight
6 × 10−5"
REFERENCES,0.7209737827715356,"Learning rate schedule
η(epoch) = 10−3−⌊epoch/50⌋"
REFERENCES,0.7228464419475655,"RMDA
Regularization weight
10−4
Restart epochs
50, 100, 150, 200
Learning rate schedule
η(epoch) = max(10−4, 10−⌊epoch/50⌋)
Momentum schedule
c(epoch) = min(1, 10−2+⌊epoch/50⌋)"
REFERENCES,0.7247191011235955,"MCP for this group is then computed as (Breheny & Huang, 2009)"
REFERENCES,0.7265917602996255,"MCP
 
WIg; λg, ωg
 := 
"
REFERENCES,0.7284644194756554,"
λg
WIg
2 −∥WIg∥
2"
REFERENCES,0.7303370786516854,"2ωg
if
WIg
 < ωgλg,"
REFERENCES,0.7322097378277154,"ωgλg
2"
"IF
WIG",0.7340823970037453,"2
if
WIg
 ≥ωgλg."
"IF
WIG",0.7359550561797753,We then consider
"IF
WIG",0.7378277153558053,"ψ(W) = |G|
X"
"IF
WIG",0.7397003745318352,"g=1
MCP
 
WIg; λg, ωg

.
(31)"
"IF
WIG",0.7415730337078652,"It is shown in Deleu & Bengio (2021) that group MCP regularization may simultaneously
provide higher group sparsity and better validation accuracy than the group LASSO norm
in vision and language tasks. Another possibility to enhance sparsity is to add another ℓ1-
norm or entry-wise MCP regularization to the group-level regularizer. The major drawback"
"IF
WIG",0.7434456928838952,"Table 7: Details of the experimental settings of the modiﬁed VGG19 on CIFAR10 in Sec-
tion 5.2."
"IF
WIG",0.7453183520599251,"Parameter
Value
Data set
CIFAR10
Model
VGG19 (Table 13)
Loss function
Cross entropy
Regularization function
Group LASSO
Total epochs
1000"
"IF
WIG",0.7471910112359551,"ProxSGD
Regularization weight
5 × 10−5"
"IF
WIG",0.7490636704119851,"Learning rate schedule
η(epoch) = 10−1−⌊epoch/100⌋"
"IF
WIG",0.7509363295880149,"Momentum
10−1"
"IF
WIG",0.7528089887640449,"ProxSSI
Regularization weight
3 × 10−7"
"IF
WIG",0.7546816479400749,"Learning rate schedule
η(epoch) = 10−3−⌊epoch/100⌋"
"IF
WIG",0.7565543071161048,"RMDA
Regularization weight
10−4
Restart epochs
100, 200, 300, 400, 500
Learning rate schedule
η(epoch) = max(10−6, 10−1−⌊epoch/100⌋)
Momentum schedule
c(epoch) = min(1, 10−2+⌊epoch/100⌋)"
"IF
WIG",0.7584269662921348,"Table 8: Details of the experimental settings of the modiﬁed VGG19 on CIFAR100 in
Section 5.2."
"IF
WIG",0.7602996254681648,"Parameter
Value
Data set
CIFAR100
Model
VGG19 (Table 13)
Loss function
Cross entropy
Regularization function
Group LASSO
Total epochs
1000"
"IF
WIG",0.7621722846441947,"ProxSGD
Regularization weight
3 × 10−5"
"IF
WIG",0.7640449438202247,"Learning rate schedule
η(epoch) = 10−1−⌊epoch/100⌋"
"IF
WIG",0.7659176029962547,"Momentum
10−1"
"IF
WIG",0.7677902621722846,"ProxSSI
Regularization weight
10−7"
"IF
WIG",0.7696629213483146,"Learning rate schedule
η(epoch) = 10−3−⌊epoch/100⌋"
"IF
WIG",0.7715355805243446,"RMDA
Regularization weight
10−4
Restart epochs
100, 200, 300, 400, 500
Learning rate schedule
η(epoch) = max(10−6, 10−1−⌊epoch/100⌋)
Momentum schedule
c(epoch) = min(1, 10−2+⌊epoch/100⌋)"
"IF
WIG",0.7734082397003745,"of these approaches is the requirement of additional hyperparameters, and we prefer simpler
approaches over those with more hyperparameters, as hyperparameter tuning in the latter
can be troublesome for users with limited computational resources, and using a simpler
setting can also help us to focus on the comparison of the algorithms themselves.
The
experiment in this subsection is therefore only for illustrating that these more complicated
regularizers can be combined with RMDA if the user wishes, and such regularizers might
lead to better results. Therefore, we train a version of LeNet5, which is slightly simpler
than the one we used in previous experiments, on the MNIST dataset with such regularizers
using RMDA and display the respective performance of various regularization schemes in
Fig. 6. For the weights wg of each group in (7), in this experiment we consider the following
setting. Let Li be the collection of all index sets that belong to the i-th layer in the network,"
"IF
WIG",0.7752808988764045,"Table 9: Details of the experimental settings of ResNet50 on CIFAR10 in Section 5.2.
We use the ResNet50 model from the public script https://github.com/weiaicunzai/
pytorch-cifar100."
"IF
WIG",0.7771535580524345,"Parameter
Value
Data set
CIFAR10
Model
ResNet50
Loss function
Cross entropy
Regularization function
Group LASSO
Total epochs
1500"
"IF
WIG",0.7790262172284644,"ProxSGD
Regularization weight
5 × 10−5"
"IF
WIG",0.7808988764044944,"Learning rate schedule
η(epoch) = 10−1−⌊epoch/150⌋"
"IF
WIG",0.7827715355805244,"Momentum
10−1"
"IF
WIG",0.7846441947565543,"ProxSSI
Regularization weight
3 × 10−7"
"IF
WIG",0.7865168539325843,"Learning rate schedule
η(epoch) = 10−3−⌊epoch/150⌋"
"IF
WIG",0.7883895131086143,"RMDA
Regularization weight
10−5
Restart epochs
150, 300, 450, 600
Learning rate schedule
η(epoch) = max(10−4, 10−⌊epoch/150⌋)
Momentum schedule
c(epoch) = min(1, 10−2+⌊epoch/150⌋)"
"IF
WIG",0.7902621722846442,"Table 10: Details of the experimental settings of ResNet50 on CIFAR100 in Section 5.2.
We use the ResNet50 model from the public script https://github.com/weiaicunzai/
pytorch-cifar100."
"IF
WIG",0.7921348314606742,"Parameter
Value
Data set
CIFAR100
Model
ResNet50
Loss function
Cross entropy
Regularization function
Group LASSO
Total epochs
1500"
"IF
WIG",0.7940074906367042,"ProxSGD
Regularization weight
4 × 10−5"
"IF
WIG",0.795880149812734,"Learning rate schedule
η(epoch) = 10−1−⌊epoch/150⌋"
"IF
WIG",0.797752808988764,"Momentum
10−1"
"IF
WIG",0.799625468164794,"ProxSSI
Regularization weight
3 × 10−7"
"IF
WIG",0.8014981273408239,"Learning rate schedule
η(epoch) = 10−3−⌊epoch/150⌋"
"IF
WIG",0.8033707865168539,"RMDA
Regularization weight
10−5
Restart epochs
150, 300, 450, 600
Learning rate schedule
η(epoch) = max(10−4, 10−⌊epoch/150⌋)
Momentum schedule
c(epoch) = min(1, 10−2+⌊epoch/150⌋)"
"IF
WIG",0.8052434456928839,"and denote
NLi :=
X"
"IF
WIG",0.8071161048689138,"Ij∈Li
|Ij|"
"IF
WIG",0.8089887640449438,"the number of parameters in this layer, for all i, we set wg =
p"
"IF
WIG",0.8108614232209738,"NLi for all g such that
Ig ∈Li. Given two constants λ > 0 and ,ω > 1, The values of λg and ωg in (31) are then
assigned as λg = λwg and ωg = ωwg."
"IF
WIG",0.8127340823970037,"In this ﬁgure, group LASSO is abbreviated as GLASSO; ℓ1-norm plus a group LASSO
norm, L1GLASSO; group MCP, GMCP; element-wise MCP plus group MCP, L1GMCP.
Our results exemplify that diﬀerent regularization schemes might have diﬀerent beneﬁts on"
"IF
WIG",0.8146067415730337,"Table
11:
Details
of
the
multi-layer
fully-connected
NN.
https://github.com/
zihsyuan1214/rmda/blob/master/Experiments/Models/mlp.py."
"IF
WIG",0.8164794007490637,"Parameter
Value
Type of layers
fully-connected layer
Number of layers
7
Number of output neurons each layer: 1, 2, 3, 4, 5, 6, 7
512, 256, 128, 64, 32, 16, 10
Activation function for convolution/output layer
relu/softmax"
"IF
WIG",0.8183520599250936,"Table 12: Details of the modiﬁed LeNet5 for experiments in Section 5.2. https://github.
com/zihsyuan1214/rmda/blob/master/Experiments/Models/lenet5_large.py."
"IF
WIG",0.8202247191011236,"Parameter
Value
Number of layers
4
Number of convolutional layers
2
Number of fully-connected layers
2
Size of convolutional kernels
3 × 3
Number of output ﬁlters 1, 2
20, 50
Number of output neurons 3, 4
500, 10
Kernel size, stride, padding of maxing pooling
2 × 2, none, invalid
Operations after convolutional layers
max pooling
Activation function for convolution/output layer
relu/softmax"
"IF
WIG",0.8220973782771536,"one of the criteria with proper hyperparameter tuning. The detailed numbers are reported
in Table 15 and the experiment settings can be found in Tables 16 and 17."
"IF
WIG",0.8239700374531835,"Table 13: Details of the modiﬁed VGG19.
https://github.com/zihsyuan1214/rmda/
blob/master/Experiments/Models/vgg19.py."
"IF
WIG",0.8258426966292135,"Parameter
Value
Number of layers
17
Number of convolutional layers
16
Number of fully-connected layers
1
Size of convolutional kernels
3 × 3
Number of output ﬁlters 1-2, 3-4, 5-8, 9-16
64, 128, 256, 512
Kernel size, stride, padding of maxing pooling
2 × 2, 2, invalid
Operations after convolutional layers
max pooling, batchnorm
Activation function for convolution/output layer
relu/softmax"
"IF
WIG",0.8277153558052435,"Table 14: Details of experimental settings of ResNet50 on CIFAR10 and CIFAR100 for
unstructured sparsity. In this experiment, we adopt the version of ResNet50 in Sundar &
Dwaraknath (2021)."
"IF
WIG",0.8295880149812734,"Parameter
Value
Model
ResNet50
Loss function
Cross entropy"
"IF
WIG",0.8314606741573034,"RMDA
Data
CIFAR10
Total epochs
1000
L1 weight
10−5
Restart epochs
150, 300, 450
Learning rate schedule
η(epoch) = max(10−3, 10−⌊epoch/150⌋)
Momentum schedule
c(epoch) = min(1, 10−2+⌊epoch/150⌋)
Data
CIFAR100
Total epochs
1000
L1 weight
3 × 10−5
Restart epochs
150, 300, 450
Learning rate schedule
η(epoch) = max(10−3, 10−⌊epoch/150⌋)
Momentum schedule
c(epoch) = min(1, 10−2+⌊epoch/150⌋)"
"IF
WIG",0.8333333333333334,"RigL
Data
CIFAR10
Total epochs
1000
Sparse initialization
erdos-renyi-kernel
Density
0.02
Prune rate
0.3
Decay schedule
cosine
Apply when
step end
Interval
100
End when
65918
Learning rate
0.1
Momentum
0.9
Weight decay
10−4
Label smoothing
0.1
Decay frequency
20000
Warmup steps
1760
Decay factor
0.2
Data
CIFAR100
Total epochs
1000
Sparse initialization
erdos-renyi-kernel
Density
0.02
Prune rate
0.3
Decay schedule
cosine
Apply when
step end
Interval
100
End when
65918
Learning rate
0.1
Momentum
0.9
Weight decay
10−4
Label smoothing
0.1
Decay frequency
20000
Warmup steps
1760
Decay factor
0.2"
"IF
WIG",0.8352059925093633,"0
100
200
300
400
500
Epochs 0.905 0.910 0.915"
"IF
WIG",0.8370786516853933,Validation Accuracy
"IF
WIG",0.8389513108614233,Logistic Regression on MNIST
"IF
WIG",0.8408239700374532,"ProxSGD
ProxSSI
RMDA"
"IF
WIG",0.8426966292134831,"0
100
200
300
400
500
Epochs 0.00 0.25 0.50"
"IF
WIG",0.8445692883895131,Structured Sparsity
"IF
WIG",0.846441947565543,(a) Logistic regression on MNIST
"IF
WIG",0.848314606741573,"0
100
200
300
400
500
Epochs 0.25 0.50 0.75"
"IF
WIG",0.850187265917603,Validation Accuracy
"IF
WIG",0.8520599250936329,Fully-connected NN on FashionMNIST
"IF
WIG",0.8539325842696629,"ProxSGD
ProxSSI
RMDA"
"IF
WIG",0.8558052434456929,"0
100
200
300
400
500
Epochs 0.0 0.2 0.4"
"IF
WIG",0.8576779026217228,Structured Sparsity
"IF
WIG",0.8595505617977528,(b) Fully-connected NN on FashionMNIST
"IF
WIG",0.8614232209737828,"0
100
200
300
400
500
Epochs 0.96 0.98"
"IF
WIG",0.8632958801498127,Validation Accuracy
"IF
WIG",0.8651685393258427,LeNet5 on MNIST
"IF
WIG",0.8670411985018727,"MSGD
ProxSGD
ProxSSI
RMDA"
"IF
WIG",0.8689138576779026,"0
100
200
300
400
500
Epochs 0.0 0.5"
"IF
WIG",0.8707865168539326,Structured Sparsity
"IF
WIG",0.8726591760299626,(c) LeNet5 on MNIST
"IF
WIG",0.8745318352059925,"0
100
200
300
400
500
Epochs 0.85 0.90"
"IF
WIG",0.8764044943820225,Validation Accuracy
"IF
WIG",0.8782771535580525,LeNet5 on FashionMNIST
"IF
WIG",0.8801498127340824,"MSGD
ProxSGD
ProxSSI
RMDA"
"IF
WIG",0.8820224719101124,"0
100
200
300
400
500
Epochs 0.0 0.5"
"IF
WIG",0.8838951310861424,Structured Sparsity
"IF
WIG",0.8857677902621723,(d) LeNet5 on FashionMNIST
"IF
WIG",0.8876404494382022,"0
200
400
600
800
1000
Epochs 0.6 0.8"
"IF
WIG",0.8895131086142322,Validation Accuracy
"IF
WIG",0.8913857677902621,VGG19 on CIFAR10
"IF
WIG",0.8932584269662921,"MSGD
ProxSGD
ProxSSI
RMDA"
"IF
WIG",0.8951310861423221,"0
200
400
600
800
1000
Epochs 0.0 0.5"
"IF
WIG",0.897003745318352,Structured Sparsity
"IF
WIG",0.898876404494382,(e) VGG19 on CIFAR10
"IF
WIG",0.900749063670412,"0
200
400
600
800
1000
Epochs 0.0 0.5"
"IF
WIG",0.9026217228464419,Validation Accuracy
"IF
WIG",0.9044943820224719,VGG19 on CIFAR100
"IF
WIG",0.9063670411985019,"MSGD
ProxSGD
ProxSSI
RMDA"
"IF
WIG",0.9082397003745318,"0
200
400
600
800
1000
Epochs 0.00 0.25 0.50"
"IF
WIG",0.9101123595505618,Structured Sparsity
"IF
WIG",0.9119850187265918,(f) VGG19 on CIFAR100
"IF
WIG",0.9138576779026217,"0
200
400
600
800
1000
1200
1400
Epochs 0.50 0.75"
"IF
WIG",0.9157303370786517,Validation Accuracy
"IF
WIG",0.9176029962546817,ResNet50 on CIFAR10
"IF
WIG",0.9194756554307116,"MSGD
ProxSGD
ProxSSI
RMDA"
"IF
WIG",0.9213483146067416,"0
200
400
600
800
1000
1200
1400
Epochs 0.0 0.5"
"IF
WIG",0.9232209737827716,Structured Sparsity
"IF
WIG",0.9250936329588015,(g) ResNet50 on CIFAR10
"IF
WIG",0.9269662921348315,"0
200
400
600
800
1000
1200
1400
Epochs 0.25 0.50 0.75"
"IF
WIG",0.9288389513108615,Validation Accuracy
"IF
WIG",0.9307116104868914,ResNet50 on CIFAR100
"IF
WIG",0.9325842696629213,"MSGD
ProxSGD
ProxSSI
RMDA"
"IF
WIG",0.9344569288389513,"0
200
400
600
800
1000
1200
1400
Epochs 0.00 0.25 0.50"
"IF
WIG",0.9363295880149812,Structured Sparsity
"IF
WIG",0.9382022471910112,(h) ResNet50 on CIFAR100
"IF
WIG",0.9400749063670412,"Figure 4: Group Sparsity and validation accuracy v.s epochs of diﬀerent algorithms on
various models with the group-LASSO regularization of a single run."
"IF
WIG",0.9419475655430711,"0
200
400
600
800
1000
0.0 0.5 1.0"
"IF
WIG",0.9438202247191011,Sparsity
"IF
WIG",0.9456928838951311,"0
200
400
600
800
1000
Epochs 0.85 0.90"
"IF
WIG",0.947565543071161,Validation Accuracy RMDA
"IF
WIG",0.949438202247191,(a) ResNet50 on CIFAR10
"IF
WIG",0.951310861423221,"0
200
400
600
800
1000
0.0 0.5 1.0"
"IF
WIG",0.9531835205992509,Sparsity
"IF
WIG",0.9550561797752809,"0
200
400
600
800
1000
Epochs 0.5 0.6 0.7"
"IF
WIG",0.9569288389513109,Validation Accuracy RMDA
"IF
WIG",0.9588014981273408,(b) ResNet50 on CIFAR100
"IF
WIG",0.9606741573033708,"Figure 5: Unstructured Sparsity and validation accuracy v.s epochs of RMDA on ResNet50
of a single run."
"IF
WIG",0.9625468164794008,"0
50
100
150
200
250
300
Epochs 0.97 0.98 0.99 1.00"
"IF
WIG",0.9644194756554307,Validation Accuracy
"IF
WIG",0.9662921348314607,"GLASSO
L1GLASSO
GMCP
L1GMCP"
"IF
WIG",0.9681647940074907,"200
220
240
260
280
300
Epochs 0.3 0.4 0.5 0.6 0.7 0.8"
"IF
WIG",0.9700374531835206,Group Sparsity
"IF
WIG",0.9719101123595506,"GLASSO
L1GLASSO
GMCP
L1GMCP"
"IF
WIG",0.9737827715355806,"Figure 6: Comparison between group LASSO, L1+group LASSO, Group MCP, L1+Group
MCP"
"IF
WIG",0.9756554307116105,"Table 15: Results of training LeNet5 on MNIST using RMDA with diﬀerent regularizers.
We report mean and standard deviation of three independent runs."
"IF
WIG",0.9775280898876404,"Regularizers
Validation accuracy
Group sparsity"
"IF
WIG",0.9794007490636704,"GLASSO
99.11 ± 0.06%
45.33 ± 0.99%
L1GLASSO
99.02 ± 0.01%
58.92 ± 1.30%
GMCP
99.25 ± 0.08%
32.81 ± 0.96%
L1GMCP
99.21 ± 0.03%
32.91 ± 0.35%"
"IF
WIG",0.9812734082397003,"Table 16:
Details of the modiﬁed simpler LeNet5 for the experiment in Appendix E.
https://github.com/zihsyuan1214/rmda/blob/master/Experiments/Models/lenet5_
small.py."
"IF
WIG",0.9831460674157303,"Parameter
Value
Number of layers
5
Number of convolutional layers
3
Number of fully-connected layers
2
Size of convolutional kernels
5 × 5
Number of output ﬁlters 1, 2
6, 16
Number of output neurons 3, 4, 5
120, 84, 10
Kernel size, stride, padding of maxing pooling
2 × 2, none, invalid
Operations after convolutional layers
max pooling
Activation function for convolution/output layer
relu/softmax"
"IF
WIG",0.9850187265917603,"Table 17: Details of the experimental settings for comparing diﬀerent regularizers in Ap-
pendix E"
"IF
WIG",0.9868913857677902,"Parameter
Value
Data set
MNIST
Model
LeNet5 (Table 16)
Loss function
Cross entropy
Algorithms
RMDA
Total epochs
300
Restart epochs
30, 60, 90, 120
Learning rate schedule
η(epoch) = max(10−5, 10−1−⌊epoch/30⌋)
Momentum schedule
c(epoch) = min(1, 10−2+⌊epoch/30⌋)"
"IF
WIG",0.9887640449438202,"GLASSO
Group LASSO weight
10−5"
"IF
WIG",0.9906367041198502,"L1GLASSO
L1 weight
10−4"
"IF
WIG",0.9925093632958801,"Group LASSO weight
10−5"
"IF
WIG",0.9943820224719101,"GMCP
Group MCP weight
10−5
γ
64"
"IF
WIG",0.9962546816479401,"L1GMCP
L1 weight
10−4"
"IF
WIG",0.99812734082397,"Group MCP weight
10−5
γ
64"
