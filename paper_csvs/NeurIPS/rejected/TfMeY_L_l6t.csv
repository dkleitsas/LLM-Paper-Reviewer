Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019455252918287938,"To limit the time, cost, and environmental impact associated with the acquisition
1"
ABSTRACT,0.0038910505836575876,"of seismic data, in recent decades considerable effort has been put into so-called
2"
ABSTRACT,0.005836575875486381,"simultaneous shooting acquisitions, where seismic sources are ﬁred at short time
3"
ABSTRACT,0.007782101167315175,"intervals between each other. As a consequence, waves originating from consecu-
4"
ABSTRACT,0.009727626459143969,"tive shots are entangled within the seismic recordings, yielding so-called blended
5"
ABSTRACT,0.011673151750972763,"data. For processing and imaging purposes, the data generated by each individual
6"
ABSTRACT,0.013618677042801557,"shot must be retrieved. This process, called deblending, is achieved by solving
7"
ABSTRACT,0.01556420233463035,"an inverse problem which is heavily underdetermined. Conventional approaches
8"
ABSTRACT,0.017509727626459144,"rely on transformations that render the blending noise into burst-like noise, whilst
9"
ABSTRACT,0.019455252918287938,"preserving the signal of interest. Compressed sensing type regularization is then
10"
ABSTRACT,0.021400778210116732,"applied, where sparsity in some domain is assumed for the signal of interest. The
11"
ABSTRACT,0.023346303501945526,"domain of choice depends on the geometry of the acquisition and the properties
12"
ABSTRACT,0.02529182879377432,"of seismic data within the chosen domain. In this work, we introduce a new
13"
ABSTRACT,0.027237354085603113,"concept that consists of embedding a self-supervised denoising network into the
14"
ABSTRACT,0.029182879377431907,"Plug-and-Play (PnP) framework. A novel network is introduced whose design
15"
ABSTRACT,0.0311284046692607,"extends the blind-spot network architecture of [27] for partially coherent noise
16"
ABSTRACT,0.033073929961089495,"(i.e., correlated in time). The network is trained directly on the noisy input data at
17"
ABSTRACT,0.03501945525291829,"each step of the PnP algorithm. By leveraging both the underlying physics of the
18"
ABSTRACT,0.03696498054474708,"blending operator and the great denoising capabilities of our blind-spot network,
19"
ABSTRACT,0.038910505836575876,"the proposed algorithm is shown to outperform an industry-standard method whilst
20"
ABSTRACT,0.04085603112840467,"being comparable in terms of computational cost. Moreover, being independent on
21"
ABSTRACT,0.042801556420233464,"the acquisition geometry, our method can be easily applied to both marine and land
22"
ABSTRACT,0.04474708171206226,"data without any signiﬁcant modiﬁcation.
23"
INTRODUCTION,0.04669260700389105,"1
Introduction
24"
INTRODUCTION,0.048638132295719845,"Reﬂection seismology [41] is a geophysical technique that uses reﬂected seismic waves to characterize
25"
INTRODUCTION,0.05058365758754864,"the Earth’s subsurface. It comprises of a controlled source of seismic energy and an array of receivers
26"
INTRODUCTION,0.05252918287937743,"that record the pressure (or displacement) induced by the reﬂected waves. After the introduction of
27"
INTRODUCTION,0.054474708171206226,"3D seismic [11], today’s conventional seismic acquisition campaigns may last several weeks up to
28"
INTRODUCTION,0.05642023346303502,"a few months [7, 25, 10]. In an attempt to improve acquisition efﬁciency, and therefore limit the
29"
INTRODUCTION,0.058365758754863814,"time, cost, and associated environmental impact, [6, 9, 3, 36] introduced a new paradigm in seismic
30"
INTRODUCTION,0.06031128404669261,"acquisition referred to as simultaneous shooting. Simply put, consecutive sources are ﬁred at short
31"
INTRODUCTION,0.0622568093385214,"time intervals, thereby minimizing the overall acquisition time. This comes at the cost of recording
32"
INTRODUCTION,0.06420233463035019,"entangled seismic data, also called blended data, where the waves originating from one source tend to
33"
INTRODUCTION,0.06614785992217899,"overlap with those originating from previous and subsequent sources. To render such data suitable for
34"
INTRODUCTION,0.06809338521400778,"subsequent steps of seismic processing and imaging, the interference between consecutive shots must
35"
INTRODUCTION,0.07003891050583658,"be removed such that the contribution of each individual source (also referred to as a shot gather)
36"
INTRODUCTION,0.07198443579766536,"is retrieved. This process is called deblending. In theory, deblending can be achieved by solving
37"
INTRODUCTION,0.07392996108949416,"an inverse problem; however, as this problem is heavily underdetermined, choosing an appropriate
38"
INTRODUCTION,0.07587548638132295,"regularization is fundamental to achieve a successful inversion. Historically, the design of suitable
39"
INTRODUCTION,0.07782101167315175,"regularizers is motivated by the effect of the adjoint of the blending operator on the blended data. In
40"
INTRODUCTION,0.07976653696498054,"fact, the resulting data can be seen as a superposition of coherent signal (i.e, reﬂections from the shot
41"
INTRODUCTION,0.08171206225680934,"whose ﬁring time has been properly accounted for) and trace-wise, burst-like noise (i.e, reﬂections
42"
INTRODUCTION,0.08365758754863813,"from all other interfering shots whose ﬁring times have not been properly accounted for).
43"
INTRODUCTION,0.08560311284046693,"The recent success of deep learning in various scientiﬁc disciplines has attracted the interest of the
44"
INTRODUCTION,0.08754863813229571,"geophysical community, resulting in many opportunities and new challenges [51]. For example,
45"
INTRODUCTION,0.08949416342412451,"whilst training data should consist of clean, representative ground truth examples that resemble
46"
INTRODUCTION,0.0914396887159533,"the solution to the inverse problem at hand, such data is generally not available. Two approaches
47"
INTRODUCTION,0.0933852140077821,"commonly adopted to circumvent this problem are to either generate synthetic data or to use state-
48"
INTRODUCTION,0.09533073929961089,"of-the-art algorithms to produce input-output pairs to train a network on; in both cases, transfer
49"
INTRODUCTION,0.09727626459143969,"learning [42, 34] or domain adaptation [2, 12] techniques are then required to generalize the network
50"
INTRODUCTION,0.09922178988326848,"capabilities to unseen ﬁeld data. A major drawback of the ﬁrst approach is that synthetic data may not
51"
INTRODUCTION,0.10116731517509728,"resemble ﬁeld data accurately enough to be considered a representative dataset: this is well-known
52"
INTRODUCTION,0.10311284046692606,"in the geophysical community and has been a major criticism for decades when new methods are
53"
INTRODUCTION,0.10505836575875487,"tested only on synthetic data. It also represents a serious roadblock to the application of deep learning
54"
INTRODUCTION,0.10700389105058365,"methods in geophysics. Additionally, in most geophysical applications the underlying physics is
55"
INTRODUCTION,0.10894941634241245,"(at least partly) well understood. Pure, end-to-end machine learning methods tend to ignore these
56"
INTRODUCTION,0.11089494163424124,"well-studied physical principles, thereby discarding important a priori knowledge of the problem they
57"
INTRODUCTION,0.11284046692607004,"are tasked to solve.
58"
INTRODUCTION,0.11478599221789883,"Our contribution
We introduce a novel algorithm for seismic deblending, which combines the
59"
INTRODUCTION,0.11673151750972763,"physics of the underlying physical process with a state-of-the-art self-supervised denoiser into a
60"
INTRODUCTION,0.11867704280155641,"single, well-crafted inverse process. This is speciﬁcally achieved within the framework of Plug-
61"
INTRODUCTION,0.12062256809338522,"and-Play (PnP) priors. Our network architecture is inspired by the blind-spot network of [27] and
62"
INTRODUCTION,0.122568093385214,"modiﬁed to handle trace-wise coherent noise. The network is trained on-the-ﬂy at each PnP iteration
63"
INTRODUCTION,0.1245136186770428,"in a self-supervised manner, completely bypassing the need for ground truth data. Our numerical
64"
INTRODUCTION,0.1264591439688716,"experiments illustrate that the proposed algorithm can outperform a state-of-the-art conventional
65"
INTRODUCTION,0.12840466926070038,"method. Finally, we show that our algorithm is independent on the underlying structure of the seismic
66"
INTRODUCTION,0.1303501945525292,"data and can be used easily for different acquisition set-ups - a clear advantage over conventional
67"
INTRODUCTION,0.13229571984435798,"methods.
68"
BACKGROUND,0.13424124513618677,"2
Background
69"
BACKGROUND,0.13618677042801555,"The seismic data layout
Seismic data are commonly acquired by ﬁring a source at a given time and
70"
BACKGROUND,0.13813229571984437,"recording the reﬂections arising from the interaction between the emitted seismic wave and changes
71"
BACKGROUND,0.14007782101167315,"in subsurface properties. Conceptually, seismic data can be arranged as a three dimensional tensor (or
72"
BACKGROUND,0.14202334630350194,"a cube), having the dimensions of the number of sources ns, number of receivers nr, and number of
73"
BACKGROUND,0.14396887159533073,"time samples nt: dc(xs, xr, t). Slicing this cube in different directions gives raise to so-called seismic
74"
BACKGROUND,0.14591439688715954,"gathers: more speciﬁcally, when slicing across the source axis, we obtain the data recorded by all
75"
BACKGROUND,0.14785992217898833,"receivers for a single shot, usually called Common Shot Gather (CSG); conversely, by slicing across
76"
BACKGROUND,0.14980544747081712,"the receiver axis we obtain the data generated by all shots for a single receiver. When the receivers
77"
BACKGROUND,0.1517509727626459,"move alongside the source (i.e., marine case) the resulting gather is called Common Channel Gather
78"
BACKGROUND,0.15369649805447472,"(CCG). For static receivers (i.e., ocean-bottom or land acquisition), the seismic gather is know as the
79"
BACKGROUND,0.1556420233463035,"Common Receiver Gather (CRG). Both scenarios will later be considered.
80"
BACKGROUND,0.1575875486381323,"Blended acquisition
In practice, to be able to collect data where no overlap exists between
81"
BACKGROUND,0.15953307392996108,"consecutive shots, each shot has to be ﬁred with an appropriate time delay, such that all reﬂections
82"
BACKGROUND,0.1614785992217899,"from one shot have been recorded by the receivers before the next shot is ﬁred. This dictates
83"
BACKGROUND,0.16342412451361868,"the overall acquisition time and greatly limits any possible acquisition speed-up. Alternatively,
84"
BACKGROUND,0.16536964980544747,"in blended acquisition, shots are ﬁred at shorter intervals. This means that each individual CSG
85"
BACKGROUND,0.16731517509727625,"contains recordings from both the nominal as well as the previous and subsequent shots. In this work,
86"
BACKGROUND,0.16926070038910507,"we consider the so-called continuous blending setting. This approach is state-of-the-art in marine
87"
BACKGROUND,0.17120622568093385,"seismic acquisition due to the fact it is easy to implement in the ﬁeld. It is achieved by ﬁring the
88"
BACKGROUND,0.17315175097276264,"airgun towed by the acquisition vessel at short time intervals, and continuously recording the waves
89"
BACKGROUND,0.17509727626459143,"returning to the receiver array as depicted in ﬁgure 1. The recorded data db can be simply described
90"
BACKGROUND,0.17704280155642024,"as the superposition of all of the unblended, or clean, data shifted in time by the given time delay
91"
BACKGROUND,0.17898832684824903,"ti = i · T + ∆ti. Here, T is the nominal ﬁring interval and ∆ti is a random dither applied to the
92"
BACKGROUND,0.18093385214007782,"nominal ﬁring time of shot i. The blended data can thus be described as a function of the clean data
93"
BACKGROUND,0.1828793774319066,"db = Bdc := [B1, . . . , Bns] dc = B1dc,1 + . . . + Bnsdc,ns
(1)"
BACKGROUND,0.18482490272373542,"where the blending operator is a horizontal stack of time-shift operators Bi, and the clean data is a
94"
BACKGROUND,0.1867704280155642,"vector where all vectorized shot gathers, dc,i = vec(dc(xs,i, xr, t)), are stacked together. Moreover,
95"
BACKGROUND,0.188715953307393,each Bi time-shift operator has the property that BT
BACKGROUND,0.19066147859922178,"i Bi = I, and a composition of time-shift operators
96"
BACKGROUND,0.1926070038910506,is again a time-shift operator [33].
BACKGROUND,0.19455252918287938,"Figure 1: Schematic illustration of a seismic simultaneous shooting acquisition. a) Cartoon of a
seismic acquisition campaign in continuous blending mode. A single vessel towing a source (red star)
and an array of receivers (blue triangles) moves from right to left and ﬁres energy into the ground at
dithered periodic time samples. For each shot, reﬂections originated from shallow subsurface layers
are immediately recorded by the receivers, whilst those produced by deeper reﬂectors are recorded
later in time alongside the shallow reﬂections from the next ﬁring shot. This phenomenon leads to the
blending of independent shot gathers. b) A short time window of the continuously blended seismic
data. Dashed vertical color lines represent the nominal ﬁring times (i.e., i · T), whilst the solid color
lines represent the actual ﬁring times with dithering. Color rectangles refer to every individual shot
gather that we wish to separate from the other overlapping gathers. c) Pseudo-deblended data for a
single receiver (white dashed line in panel b). 97"
BACKGROUND,0.19649805447470817,"Pseudo-deblending
To better understand how to design effective regularization strategies for the
98"
BACKGROUND,0.19844357976653695,"deblending problem, we ﬁrst have to consider the action of BH on the blended data. For the ith shot
99"
BACKGROUND,0.20038910505836577,"gather, the result of BH"
BACKGROUND,0.20233463035019456,"i Bdc can be written as
100 BH"
BACKGROUND,0.20428015564202334,"i (B1dc,1 + . . . + Bnsdc,ns) = dc,i + ! BH"
BACKGROUND,0.20622568093385213,"i B1dc,1 + . . . + BH"
BACKGROUND,0.20817120622568094,"i Bnsdc,ns "" (2)"
BACKGROUND,0.21011673151750973,Therefore the action of BH
BACKGROUND,0.21206225680933852,"i
on the blended data produces the original ith shot gather alongside
101"
BACKGROUND,0.2140077821011673,"randomly shifted versions of all the other shot gathers. Whilst these randomly shifted shot gathers
102"
BACKGROUND,0.21595330739299612,"are coherent and look like standard seismic signal in the CSG domain, they appear as trace-wise
103"
BACKGROUND,0.2178988326848249,"coherent noise in the CRG (or CCG) domain as shown in ﬁgure 1(c). Because the application of the
104"
BACKGROUND,0.2198443579766537,"adjoint of the blending operator retrieves the true signal, albeit with some additional noise, its action
105"
BACKGROUND,0.22178988326848248,"is usually called pseudo-deblending. As a consequence of this, it is now clear that to retrieve the
106"
BACKGROUND,0.2237354085603113,"various dc,i, an effective regularization must ﬁlter the trace-wise noise in CRGs (or CCGs) whilst
107"
BACKGROUND,0.22568093385214008,"preserving the coherent signal. Conventional approaches identify a domain in which the signal can be
108"
BACKGROUND,0.22762645914396887,"easily discriminated from the noise, and more speciﬁcally the signal in such domain is sparse whilst
109"
BACKGROUND,0.22957198443579765,"the noise is not. Examples of such a kind include the hyperbolic Radon transform for CRGs [24], the
110"
BACKGROUND,0.23151750972762647,"patched Fourier transform for CCGs [1], or the Curvelet transform [30].
111"
BACKGROUND,0.23346303501945526,"Deblending by inversion
Deblending by denoising is achieved by minimizing
112 min"
BACKGROUND,0.23540856031128404,"dc kdc −BHdbk1 + R(dc),
(3)"
BACKGROUND,0.23735408560311283,"whereas deblending by inversion amounts to retrieving the clean data by solving the (heavily)
113"
BACKGROUND,0.23929961089494164,"underdetermined inverse problem,
114 min dc"
BACKGROUND,0.24124513618677043,"1
2kBdc −dbk2"
BACKGROUND,0.24319066147859922,"2 + R(dc).
(4)"
BACKGROUND,0.245136186770428,"where R(·) is any chosen regularization. The literature has shown that deblending by inversion is
115"
BACKGROUND,0.24708171206225682,"superior to deblending by denoising in terms of the overall quality of reconstruction and will be the
116"
BACKGROUND,0.2490272373540856,"focus of this work. More details on both approaches are provided in the supplementary material.
117"
BACKGROUND,0.2509727626459144,"Self-supervised denoising: Incoherent noise
Self-supervised denoisers are designed in such a way
118"
BACKGROUND,0.2529182879377432,"that noisy images can be used as both the input and label to train a neural network to act as a denoiser,
119"
BACKGROUND,0.25486381322957197,"thereby bypassing the need for clean data as labels. Noise2Noise represents the ﬁrst such method
120"
BACKGROUND,0.25680933852140075,"not relying on ground truth labels [28]. The network is forced to infer the signal from pairs of noisy
121"
BACKGROUND,0.2587548638132296,"data. For applications where such pairs are unavailable, an alternative was proposed in the concurrent
122"
BACKGROUND,0.2607003891050584,"works of [26] and [5], who introduced Noise2Void and Noise2Self, respectively. In both cases, the
123"
BACKGROUND,0.26264591439688717,"same image is used as input and label: under the assumption that the noise is incoherent whilst the
124"
BACKGROUND,0.26459143968871596,"signal is coherent, the network can naturally learn to infer only the signal from its neighbouring
125"
BACKGROUND,0.26653696498054474,"pixels. More speciﬁcally, to denoise a particular pixel, [26] replace the pixel of the input image with
126"
BACKGROUND,0.26848249027237353,"a randomly selected neighbouring pixel. As this pre-processing step introduces randomness in the
127"
BACKGROUND,0.2704280155642023,"central pixel of the receptive ﬁeld of the network, the network should not learn anything from it and
128"
BACKGROUND,0.2723735408560311,"naturally learns to infer the signal from its neighbours (since the noise is assumed to be incoherent).
129"
BACKGROUND,0.27431906614785995,"Rather than directly replacing the pixel of interest, [5] pre-process the input image with a blind-spot
130"
BACKGROUND,0.27626459143968873,"convolutional ﬁlter, so that the network cannot rely on the central pixel to predict itself. A key
131"
BACKGROUND,0.2782101167315175,"limitation of both approaches lies in the fact that the self-supervised loss can be evaluated only at the
132"
BACKGROUND,0.2801556420233463,"pixels that have been corrupted, making the training of these denoisers relatively slow. An alternative
133"
BACKGROUND,0.2821011673151751,"approach to blind-spot networks was introduced by [27]. Instead of corrupting the middle pixel, their
134"
BACKGROUND,0.2840466926070039,"network is explicitly designed to have a receptive ﬁeld with a hole in the middle. This is achieved by
135"
BACKGROUND,0.28599221789883267,"combining padding and cropping with a standard convolution layer (i.e., to create a causal ﬁlter) and
136"
BACKGROUND,0.28793774319066145,"by rotating the input image four times prior to feeding it through the network. After the rotated inputs
137"
BACKGROUND,0.2898832684824903,"have been fed through the network, they are rotated back, concatenated, and combined by a series
138"
BACKGROUND,0.2918287937743191,"of 1 ⇥1 convolutions prior to evaluating the loss at every pixel of the output image. A schematic
139"
BACKGROUND,0.29377431906614787,"description of this network is depicted in ﬁgure 2a.
140"
BACKGROUND,0.29571984435797666,"Self-supervised denoising: Coherent noise
Both Noise2Void and Noise2Self operate under the
141"
BACKGROUND,0.29766536964980544,"assumption that the noise is independent and identically distributed. [15] shows that the denoising
142"
BACKGROUND,0.29961089494163423,"quality of Noise2Void is degraded when the noise is structured. This shortcoming of Noise2Void is
143"
BACKGROUND,0.301556420233463,"solved by masking pixels along the direction of the noise: the authors dub their method Structured
144"
BACKGROUND,0.3035019455252918,"Noise2Void. For the seismic deblending problem, the noise that we are interested to suppress is also
145"
BACKGROUND,0.30544747081712065,"structured: more speciﬁcally, the blending noise shows correlation along the time axis. We, therefore,
146"
BACKGROUND,0.30739299610894943,"extend here the efﬁcient implementation of [27] to suppress structured noise in seismic data, by using
147"
BACKGROUND,0.3093385214007782,"the original and ﬂipped (over the source axis) version of the image as input. This produces a network
148"
BACKGROUND,0.311284046692607,"whose receptive ﬁeld is masked over an entire time trace, see ﬁgure 2b. In the following, we will call
149"
BACKGROUND,0.3132295719844358,"this network Structured Blind Spot, or StructBS for short.
150"
RELATED WORK,0.3151750972762646,"3
Related work
151"
RELATED WORK,0.31712062256809337,"Simultaneous shooting
Simultaneous shooting was ﬁrst pioneered by [6] and has gained popularity
152"
RELATED WORK,0.31906614785992216,"in recent years [9]. Although the ﬁrst attempts at deblending were mostly by means of denoising [33],
153"
RELATED WORK,0.321011673151751,"recent research has reveled the superiority of deblending by inversion [1]. Since then, research has
154"
RELATED WORK,0.3229571984435798,"been devoted to ﬁnding appropriate regularization terms. Some approaches involve median-ﬁltering
155"
RELATED WORK,0.32490272373540857,"[22, 21, 23], rank-reduction methods [17, 55], sparse regularization [29, 30, 56, 57, 59], and deep
156"
RELATED WORK,0.32684824902723736,"learning [43, 58, 49]. All the deep learning approaches to date use CNNs and require pre-training. [4]
157"
RELATED WORK,0.32879377431906615,"uses the RED framework introduced in [38], which is similar to the PnP framework. The difference
158"
RELATED WORK,0.33073929961089493,"is that RED explicitly incorporates the denoiser into the objective function. The authors propose the
159"
RELATED WORK,0.3326848249027237,"use of two conventional regularization techniques as a denoiser, the patched Fourier transform [1]
160"
RELATED WORK,0.3346303501945525,"and the singular-spectral-analysis ﬁlter [17], instead of applying them as a sparse penalty.
161"
RELATED WORK,0.33657587548638135,"Figure 2: (a) The blind-spot network of [27], whose receptive ﬁeld excludes the center pixel. (b)
Our newly proposed blind-spot network, whose receptive ﬁeld excludes an entire direction instead of
just the middle pixel. Impulse responses are created by feeding the respective networks with unitary
weights and zero biases with an image containing a unitary spike in the middle."
RELATED WORK,0.33852140077821014,"Self-supervised seismic denoising
Seismic data are a prime example of a noisy data type where
162"
RELATED WORK,0.3404669260700389,"no clean, ground truth labels are available. As such, the application of self-supervised denoisers has
163"
RELATED WORK,0.3424124513618677,"recently been proposed for the suppression of different types of noise present in seismic data. Follow-
164"
RELATED WORK,0.3443579766536965,"ing the Noise2Void methodology, [13] use blind-spot networks for the suppression of random noise
165"
RELATED WORK,0.3463035019455253,"in post-stack seismic data. Expanding on this, [32] adapted the methodology of StucturedNoise2Void
166"
RELATED WORK,0.34824902723735407,"[15] for the suppression of trace-wise noise in seismic shot gathers, originating from poorly coupled
167"
RELATED WORK,0.35019455252918286,"receivers and/or dead sensors. The method that is most closely related to ours is the one presented in
168"
RELATED WORK,0.3521400778210117,"[49] - both with respect to application and methodology. The authors propose to use a self-supervised
169"
RELATED WORK,0.3540856031128405,"denoising network to deblend the data by denoising. To produce satisfactory results they require a
170"
RELATED WORK,0.3560311284046693,"number of additional pre- and post-processing steps. In our work, we incorporate a deep learning
171"
RELATED WORK,0.35797665369649806,"based denoiser in deblending by inversion, thereby leveraging both the underlying physics and the
172"
RELATED WORK,0.35992217898832685,"power of neural networks. Moreover, no pre- and post-processing is required.
173"
RELATED WORK,0.36186770428015563,"The Plug-and-Play framework
The Plug-and-Play framework was pioneered by [48]. The authors
174"
RELATED WORK,0.3638132295719844,"considered a number of popular denoisers, including BM3D [18], K-SVD [19], PLOW [16] and q-
175"
RELATED WORK,0.3657587548638132,"GGMRF [45]. In subsequent works, the denoisers have been replaced by pre-trained neural networks,
176"
RELATED WORK,0.36770428015564205,"most notably CNN and DnCNN. Lately, [31] proposed regularization by artifact-removal (RARE),
177"
RELATED WORK,0.36964980544747084,"a method leveraging a Noise2Noise type approach that requires pre-training. An extensive list of
178"
REFERENCES,0.3715953307392996,"references is provided in [52], and include [38, 54, 35, 47, 20, 46, 29, 44, 53]. This research focuses
179"
REFERENCES,0.3735408560311284,"on progressively training a neural network such that it can adapt to changing noise levels. The novelty
180"
REFERENCES,0.3754863813229572,"of our method is that pre-training is not required.
181"
METHOD,0.377431906614786,"4
Method
182"
METHOD,0.37937743190661477,"Equipped with a self-supervised denoiser, a straightforward approach to deblending is to directly
183"
METHOD,0.38132295719844356,"denoise the pseudo-deblended data. However, deblending by denoising is known to be sub-optimal in
184"
METHOD,0.3832684824902724,"comparison to deblending by inversion. On the other hand, because a denoiser cannot be naturally
185"
METHOD,0.3852140077821012,"added as a constraint to the objective function in equation 4, it is not immediately clear how to
186"
METHOD,0.38715953307393,"incorporate the denoiser into the inversion process. [48] proposed the PnP framework, which is
187"
METHOD,0.38910505836575876,"directly derived from the Alternating Direction Method of Multipliers (ADMM). Whilst resembling
188"
METHOD,0.39105058365758755,"the alternating minimization process of the classical ADMM algorithm, PnP is more ﬂexible in
189"
METHOD,0.39299610894941633,"the sense that it can use any denoiser of choice, without the need for it to be linked to an explicit
190"
METHOD,0.3949416342412451,"regularization term for the so-called y-update. To understand our method clearly, we give a short
191"
METHOD,0.3968871595330739,"derivation of the ADMM following [14], we then link it to the PnP algorithm and ﬁnally to our
192"
METHOD,0.39883268482490275,"proposed algorithm. The ADMM algorithm is generally used to solve inverse problems of the form
193 min"
METHOD,0.40077821011673154,"x D(M(x), d) + R(x),"
METHOD,0.4027237354085603,"where M is the forward model, d is the measured data, D is a data ﬁdelity term that is generally
194"
METHOD,0.4046692607003891,"smooth, and R is a convex, possibly non-smooth regularization term. Due to the non-smoothness
195"
METHOD,0.4066147859922179,"of the objective, this problem cannot be solved with standard gradient-based methods. To account
196"
METHOD,0.4085603112840467,"for the non-smoothness of R, an auxiliary variable y = x is introduced, yielding the equivalent
197"
METHOD,0.41050583657587547,"optimization problem
198 min"
METHOD,0.41245136186770426,"x,y D(M(x), d) + R(y) subject to x = y."
METHOD,0.4143968871595331,"ADMM solves this problem by forming the so-called augmented Lagrangian,
199 max u
min"
METHOD,0.4163424124513619,"x,y D(M(x), d) + R(y) + ⇢"
METHOD,0.4182879377431907,2kx −yk2
METHOD,0.42023346303501946,"2 + uT (x −y),"
METHOD,0.42217898832684825,"where u is the Lagrange multiplier and ⇢is a scalar. This problem is solved by alternatively
200"
METHOD,0.42412451361867703,"minimizing over x and y, and maximizing over u. This yields the following scheme:
201"
METHOD,0.4260700389105058,"xk+1
=
arg min x n"
METHOD,0.4280155642023346,"D(M(x), d) + ⇢"
METHOD,0.42996108949416345,2kx −yk + ukk2 2 o
METHOD,0.43190661478599224,"yk+1
=
arg min y n"
METHOD,0.433852140077821,R(y) + ⇢
METHOD,0.4357976653696498,2kxk+1 −y + ukk2 2 o
METHOD,0.4377431906614786,"uk+1
=
uk + xk+1 −yk+1.
The introduction of y = x and the addition of the quadratic penalty ⇢"
METHOD,0.4396887159533074,"2kx −yk yields the y-update,
202"
METHOD,0.44163424124513617,"which for most popular regularization terms has a simple closed-form solution that can be cheaply
203"
METHOD,0.44357976653696496,"evaluated [37]. The key observation of [48] is that the y-update can be interpreted as a denoising
204"
METHOD,0.4455252918287938,"inverse problem. As such, the authors propose to drop the user-deﬁned regularization R(·) and
205"
METHOD,0.4474708171206226,"instead plug in a denoiser of choice in the y-update of the ADMM iterations. Although this may not
206"
METHOD,0.4494163424124514,"seem a straightforward choice, PnP has been shown to be competitive (or sometimes even better) than
207"
METHOD,0.45136186770428016,"standard regularization methods in a variety of settings. Given the trace-wise structure of the noise
208"
METHOD,0.45330739299610895,"and equipped with the self-supervised denoiser, the PnP framework becomes a natural and attractive
209"
METHOD,0.45525291828793774,"choice for the deblending task at hand. Our proposed algorithm reads as follows:
210"
METHOD,0.4571984435797665,"xk+1
=
arg min x ⇢1"
METHOD,0.4591439688715953,2kBx −dbk2 2 + ⇢
METHOD,0.46108949416342415,2kx −yk + ukk2 2 &
METHOD,0.46303501945525294,"yk+1
=
StructBS✓(xk+1 + uk)
uk+1
=
uk + xk+1 −yk+1.
where x is used here for simplicity in place of dc, and the x-update is performed using an iterative
211"
METHOD,0.4649805447470817,"solver of choice, e.g. LSQR. The y-update is now the denoiser StructBS✓, where ✓denote the network
212"
METHOD,0.4669260700389105,"parameters. The variable u couples both x and y and forces them to be close together. The x-update
213"
METHOD,0.4688715953307393,"requires the solution to satisfy the physics dictated by the equation Bx = db, and the y-update
214"
METHOD,0.4708171206225681,"denoises the noisy receiver gathers.
215"
EXPERIMENTS,0.4727626459143969,"5
Experiments
216"
EXPERIMENTS,0.47470817120622566,"In the following, our algorithm is tested on the openly available Mobil AVO viking graben line
217"
EXPERIMENTS,0.4766536964980545,"12 marine dataset 1. As the data has been originally acquired in a conventional fashion, we create
218"
EXPERIMENTS,0.4785992217898833,"the blending operator and blend the data ourselves. In addition to containing all the challenging
219"
EXPERIMENTS,0.4805447470817121,"features of a ﬁeld dataset, this also provides us with a ground truth, dc, onto which to assess the
220"
EXPERIMENTS,0.48249027237354086,"quality of our reconstruction. In this example, the original dataset is composed of ns = 64 sources,
221"
EXPERIMENTS,0.48443579766536965,"nr = 120 receivers, and nt = 1024 samples (i.e., the total recording time per shot equals 4 seconds).
222"
EXPERIMENTS,0.48638132295719844,"For the continuous blending operator, we choose a ﬁxed ﬁring interval of T = 2 seconds, with
223"
EXPERIMENTS,0.4883268482490272,"added random delays selected uniformly in the interval ∆ti ⇠[−1, 1] seconds. This overlap is quite
224"
EXPERIMENTS,0.490272373540856,"challenging as generally half of the signal overlaps with either that of the previous or that of the next
225"
EXPERIMENTS,0.49221789883268485,"shot. Moreover, some pseudo-deblended shot gathers exhibit contributions from three consecutive
226"
EXPERIMENTS,0.49416342412451364,"shots. Finally, the relative mean-square error, RMSE = kdc −dc,truek2/kdc,truek2, is chosen
227"
EXPERIMENTS,0.4961089494163424,"as a metric of comparison in all of our numerical examples. All experiments are performed on a
228"
EXPERIMENTS,0.4980544747081712,"Intel(R) Xeon(R) CPU @ 2.10GHz equipped with a single NVIDIA GEForce RTX 3090 GPU.
229"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5,"5.1
Comparison with state-of-the-art deblending
230"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5019455252918288,"To begin with, our newly proposed methodology is compared with the state-of-the-art deblending
231"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5038910505836576,"algorithm of [1] that solves the deblending problem as a sparsity promoting inversion
232"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5058365758754864,z? = arg min
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5077821011673151,"z
kBFz −dbk2"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5097276264591439,"2 + λkzk1,
db = Fz?,
(5)"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5116731517509727,1https://wiki.seg.org/wiki/Mobil_AVO_viking_graben_line_12
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5136186770428015,"where F is a linear operator that performs a patched two-dimensional Fourier transform, and z? is the
233"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5155642023346303,"solution in the Fourier domain that is ultimately transformed back to the original time-space domain
234"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5175097276264592,"of the seismic data. In our experiment, the size and number of patches as well as the regularization
235"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.519455252918288,"parameter λ are selected by hand to yield optimal results. Moreover, the FISTA [8] solver is used with
236"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5214007782101168,"an adaptive decreasing sequence, λk (as this has been shown in the literature to outperform a ﬁxed λ
237"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5233463035019456,for this speciﬁc problem). We choose the sequence λk = ! 6
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5252918287937743,"5e−0.05k + 6 """
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5272373540856031,"λ0, which was once again
238"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5291828793774319,"ﬁne-tuned to give the best performance. The ﬁnal error is roughly 9.8% (see supplementary material
239"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5311284046692607,"for details); hereon in this represents the benchmark against which we will assess the effectiveness
240"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5330739299610895,"of our self-supervised PnP algorithm. Next, our PnP algorithm is applied to the same dataset. We
241"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5350194552529183,"choose 30 outer iterations, 3 inner iterations, ⇢= 1 and 30 denoiser epochs. The choice of these
242"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5369649805447471,"hyperparemeters will be justiﬁed in the ablation study. We also use the U-Net architecture in [27],
243"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5389105058365758,"the L1 norm for the self-supervised training loss because it is more appropriate for burst-like noise,
244"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5408560311284046,"and the Adam optimizer with default parameters. Since the denoiser is trained on all the CCGs,
245"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5428015564202334,"the size of our training data is 120 and we use a batch size of 8. This leads to a solution that has
246"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5447470817120622,"an overall error of roughly 6.7%, which is approximately 3% lower than the conventional method.
247"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.546692607003891,"As a visual comparison, ﬁgure 3 displays the results for a given CCG (top) and CSG (bottom) for
248"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5486381322957199,"both the conventional and proposed approaches. It is noteworthy that our algorithm shows a clear
249"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5505836575875487,"improvement in terms of denoising capabilities, as visible in the displayed CCG. Especially after
250"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5525291828793775,"t = 2s, where the signal is weak and blending noise dominates, the conventional approach tends to be
251"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5544747081712063,"more prone to signal leakage compared to our PnP algorithm. Finally, the computational cost of the
252"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.556420233463035,"conventional algorithm can be quantiﬁed in terms of the number of forward and adjoint operations
253"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5583657587548638,"for both the blending (B) and patched Fourier (F) operators: in our example, this amounts to 200
254"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5603112840466926,"forward and adjoint passes. On the other hand, our method requires 90 forward and adjoint passes
255"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5622568093385214,"for the blending operator and a total of 900 training epochs for the network. Considering that all
256"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5642023346303502,"computations (apart from the network related ones) are performed on the CPU, the two algorithms
257"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.566147859922179,"are comparable in terms of overall computational time (2h and 34mins for the conventional algorithm
258"
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5680933852140078,and 1h and 51mins for the PnP algorithm).
COMPARISON WITH STATE-OF-THE-ART DEBLENDING,0.5700389105058365,"Figure 3: Deblending results for one CCG (top) and CSG (bottom). Although both algorithms can
successfully remove most of the blending noise, our algorithm is less prone to signal leakage and
provides better amplitude ﬁdelity - a key factor in seismic data processing. 259"
ABLATION STUDY,0.5719844357976653,"5.2
Ablation study
260"
ABLATION STUDY,0.5739299610894941,"This section provides an extensive analysis of some of the key components of the proposed PnP
261"
ABLATION STUDY,0.5758754863813229,"methodology and their impact on the overall solution of the deblending inverse problem.
262"
ABLATION STUDY,0.5778210116731517,"PnP iterations
To begin with, we assess the importance of the PnP iterations compared to simply
263"
ABLATION STUDY,0.5797665369649806,"training the self-supervised denoiser on pseudo-deblended data and applying it directly to the entire
264"
ABLATION STUDY,0.5817120622568094,"dataset. Although not shown here, the result of this one-shot denoising produces a solution with an
265"
ABLATION STUDY,0.5836575875486382,"overall error of roughly 19%. This is much worse than both the conventional and PnP method and
266"
ABLATION STUDY,0.585603112840467,"therefore considered not suitable.
267"
ABLATION STUDY,0.5875486381322957,"The x-update
The ablation study with regard to the x-update is provided in the supplementary
268"
ABLATION STUDY,0.5894941634241245,"material. This includes a study on the effect of the number of inner iterations and the ⇢parameter. For
269"
ABLATION STUDY,0.5914396887159533,"our continuous deblending problem, we have shown that they could be safely ﬁxed to 3 and 1. Future
270"
ABLATION STUDY,0.5933852140077821,"experiments with different datasets and blending strategies are required to verify this assumption.
271"
ABLATION STUDY,0.5953307392996109,"The y-update
In our implementation we propose to start with a randomly initialized network and
272"
ABLATION STUDY,0.5972762645914397,"train it for a ﬁxed number of epochs at every outer iteration. A warm start strategy is employed such
273"
ABLATION STUDY,0.5992217898832685,"that the weights of the network at a given outer iteration are initialized to those obtained at the end
274"
ABLATION STUDY,0.6011673151750972,"of the training of the previous outer iteration. The efﬁcacy of this approach is shown in ﬁgure 4a,
275"
ABLATION STUDY,0.603112840466926,"where we compare on-the-ﬂy training with and without warm starts, where the latter re-initializes
276"
ABLATION STUDY,0.6050583657587548,"the network at every y-update. From the error curves, we can safely conclude that warm starting the
277"
ABLATION STUDY,0.6070038910505836,"network is clearly beneﬁcial. Since there is no theoretical justiﬁcation for this particular strategy, we
278"
ABLATION STUDY,0.6089494163424124,"consider a few other alternative strategies. The ﬁrst strategy is to use a pre-trained network. Here
279"
ABLATION STUDY,0.6108949416342413,"pre-training is achieved by denoising the pseudo-deblended data in a self-supervised manner; this
280"
ABLATION STUDY,0.6128404669260701,"approach could greatly reduce the computational cost of the overall algorithm since we do not need
281"
ABLATION STUDY,0.6147859922178989,"to train the network at every iteration. A comparison of the relative error with that of the proposed,
282"
ABLATION STUDY,0.6167315175097277,"on-the-ﬂy training shown in ﬁgure 4b reveals that after a few outer iterations, the network is unable
283"
ABLATION STUDY,0.6186770428015564,to further remove the remaining noise in the data.
ABLATION STUDY,0.6206225680933852,"Figure 4: a) Error for network training with and without warm starts. b) Error when running the PnP
algorithm with a network pre-trained on the pseudo-deblended data. 284"
ABLATION STUDY,0.622568093385214,"Another option is to stop training the network after a few outer iterations. Ideally, the network will
285"
ABLATION STUDY,0.6245136186770428,"have learnt how to remove the noise encountered during the ﬁrst iterations, and extra training will
286"
ABLATION STUDY,0.6264591439688716,"not improve the denoiser capabilities. We run experiments where we stop the training after a ﬁxed
287"
ABLATION STUDY,0.6284046692607004,"number of outer iterations to see whether there is an added beneﬁt to continuing training the network.
288"
ABLATION STUDY,0.6303501945525292,Results are shown in ﬁgure 5a. In all of the scenarios we clearly see that stopping the training after
ABLATION STUDY,0.632295719844358,"Figure 5: a) Error for on-the-ﬂy training where training is stopped after a certain number of outer
iterations. b) Error for different number of training epochs. c) Error when using the network at the
end of the PnP algorithm for the entire process versus our proposed on-the-ﬂy training strategy. 289"
ABLATION STUDY,0.6342412451361867,"a certain number of outer iterations leads to a stagnation in the error, or even worse to an increase
290"
ABLATION STUDY,0.6361867704280155,"in the error at later iterations. This behaviour is known as semiconvergence in the inverse problems
291"
ABLATION STUDY,0.6381322957198443,"community. Both results are perhaps not surprising, as the input to the network at every iteration
292"
ABLATION STUDY,0.6400778210116731,"contains a different noise level compared to that of earlier iterations: the noise in xk constantly
293"
ABLATION STUDY,0.642023346303502,"reduces during the overall inversion. Therefore, the network is required to learn a slightly different
294"
ABLATION STUDY,0.6439688715953308,"task at each time. In ﬁgure 5b, we assess the impact of the number training epochs for the denoiser.
295"
ABLATION STUDY,0.6459143968871596,"We clearly observe that at some point the curves for 30 and 20 epochs start to coincide, meaning
296"
ABLATION STUDY,0.6478599221789884,"that there is no additional gain in the extra 10 epochs of training. The curve for training with 10
297"
ABLATION STUDY,0.6498054474708171,"epochs seems stagnant after the ﬁrst three iterations, but eventually it picks up momentum and goes
298"
ABLATION STUDY,0.6517509727626459,"down again. Note that, in terms of overall epochs, the cost of performing 30 outer iterations with
299"
ABLATION STUDY,0.6536964980544747,"10 epochs each is the same as using 10 outer iterations with 30 epochs each. However, every outer
300"
ABLATION STUDY,0.6556420233463035,"iteration carries an additional cost of three inner iterations for the x-update, which is not negligible as
301"
ABLATION STUDY,0.6575875486381323,"it requires evaluating the forward and adjoint of the blending operator. In general, it seems beneﬁcial
302"
ABLATION STUDY,0.6595330739299611,"to perform more epochs in the early outer iterations, although there is a limit after which the error
303"
ABLATION STUDY,0.6614785992217899,"starts to stagnate. Moreover, training with 60 epochs leads to overﬁtting.
304"
ABLATION STUDY,0.6634241245136187,"Finally, to further investigate the generalization capabilities of the network for blending problems,
305"
ABLATION STUDY,0.6653696498054474,"the network weights are saved after the last outer iteration of the PnP algorithm. The PnP algorithm
306"
ABLATION STUDY,0.6673151750972762,"is then re-run using the saved network without performing any on-the-ﬂy training. Figure 5c shows
307"
ABLATION STUDY,0.669260700389105,"that this strategy fails, illustrating that the network may have forgotten how to deal with the higher
308"
ABLATION STUDY,0.6712062256809338,"noise levels encountered in the early iterations. This results highlights the importance of using a
309"
ABLATION STUDY,0.6731517509727627,"self-supervised denoiser that can be easily and cheaply trained on-the-ﬂy. The use of pre-trained
310"
ABLATION STUDY,0.6750972762645915,"denoising networks such as DnCNN may instead require training multiple networks with different
311"
ABLATION STUDY,0.6770428015564203,"noise levels, unless a bias-free, non-blind network is used [52].
312"
LIMITATIONS AND CONCLUSIONS,0.6789883268482491,"6
Limitations and Conclusions
313"
LIMITATIONS AND CONCLUSIONS,0.6809338521400778,"Limitations
Our algorithm requires the setting of a number of hyperparameters, namely the number
314"
LIMITATIONS AND CONCLUSIONS,0.6828793774319066,"of inner and outer iterations, the parameter ⇢, and the number of epochs for the self-supervised
315"
LIMITATIONS AND CONCLUSIONS,0.6848249027237354,"denoiser. The number of epochs seems to have a major impact on the quality of the deblending
316"
LIMITATIONS AND CONCLUSIONS,0.6867704280155642,"process and the overall convergence properties of our algorithm. Additional hyperparameters that
317"
LIMITATIONS AND CONCLUSIONS,0.688715953307393,"have not been explored in this work are associated with the network itself, e.g. the number of layers,
318"
LIMITATIONS AND CONCLUSIONS,0.6906614785992218,"the activation function, batch size, etc. This is also a direction for further research. Another drawback
319"
LIMITATIONS AND CONCLUSIONS,0.6926070038910506,"is that there is no convergence guarantee, since our operator B is underdetermined and therefore not
320"
LIMITATIONS AND CONCLUSIONS,0.6945525291828794,"strongly convex [39]. Empirically, we observe that xk and yk tend to converge to similar values for
321"
LIMITATIONS AND CONCLUSIONS,0.6964980544747081,"some carefully selected hyperparameters, indicating that at least in our experiments the algorithm
322"
LIMITATIONS AND CONCLUSIONS,0.6984435797665369,"converges successfully. Similarly, to obtain convergence guarantees for the PnP method, the denoiser
323"
LIMITATIONS AND CONCLUSIONS,0.7003891050583657,"has to be Lipschitz continuous; when a neural network is used, this means that spectral normalization
324"
LIMITATIONS AND CONCLUSIONS,0.7023346303501945,"is required during training. In [50], it was shown that PnP algorithms can be convergent when
325"
LIMITATIONS AND CONCLUSIONS,0.7042801556420234,"combined with carefully pre-trained denoisers that satisfy such condition.
326"
LIMITATIONS AND CONCLUSIONS,0.7062256809338522,"Societal impact
Blended acquisition greatly reduces the time required to acquire seismic data,
327"
LIMITATIONS AND CONCLUSIONS,0.708171206225681,"thereby limiting the impact of seismic acquisitions on the environment. Apart from shooting at shorter
328"
LIMITATIONS AND CONCLUSIONS,0.7101167315175098,"intervals, there is no difference compared to conventional acquisition. Moreover, recent research has
329"
LIMITATIONS AND CONCLUSIONS,0.7120622568093385,"suggested that the energy emitted by each source could be lowered. This may provide acquisition
330"
LIMITATIONS AND CONCLUSIONS,0.7140077821011673,"solutions that are more environmentally friendly for marine life.
331"
LIMITATIONS AND CONCLUSIONS,0.7159533073929961,"Conclusions
We have introduced a novel hybrid algorithm for seismic deblending, combining the
332"
LIMITATIONS AND CONCLUSIONS,0.7178988326848249,"physics of the blending operator with a self-supervised denoiser that is naturally embedded into the
333"
LIMITATIONS AND CONCLUSIONS,0.7198443579766537,"Plug-and-Play framework. We have adapted the network architecture in [27] to enforce an extended
334"
LIMITATIONS AND CONCLUSIONS,0.7217898832684825,"blind spot along an entire axis (time, in our case) instead of single pixels. Because the denoiser is
335"
LIMITATIONS AND CONCLUSIONS,0.7237354085603113,"self-supervised, our approaches bypasses the need for ground truth labels that are usually unavailable
336"
LIMITATIONS AND CONCLUSIONS,0.72568093385214,"for seismic applications. Experiments on a ﬁeld dataset have shown that the proposed method can
337"
LIMITATIONS AND CONCLUSIONS,0.7276264591439688,"outperform a state-of-the-art, sparsity-based algorithm. Moreover, as show in the supplementary
338"
LIMITATIONS AND CONCLUSIONS,0.7295719844357976,"material, our algorithm is independent on the type of acquisition, which is usually an issue for
339"
LIMITATIONS AND CONCLUSIONS,0.7315175097276264,"conventional algorithms. Although our algorithm requires the setting of a number of hyperparameters,
340"
LIMITATIONS AND CONCLUSIONS,0.7334630350194552,"we have argued that the number of inner iterations and ⇢can most likely be set to a ﬁxed number
341"
LIMITATIONS AND CONCLUSIONS,0.7354085603112841,"and this easily generalizes to different seismic acquisitions. However, the network architecture and
342"
LIMITATIONS AND CONCLUSIONS,0.7373540856031129,"the number of epochs may require tuning for different acquisition setups. We hope to address these
343"
LIMITATIONS AND CONCLUSIONS,0.7392996108949417,"issues by having an adaptive strategy for setting the number epochs in future work.
344"
REFERENCES,0.7412451361867705,"References
345"
REFERENCES,0.7431906614785992,"[1] R. Abma, D. Howe, M. Foster, I. Ahmed, M. Tanis, Q. Zhang, A. Arogunmati, and G. Alexander.
346"
REFERENCES,0.745136186770428,"Independent simultaneous source acquisition and processing. Geophysics, 80(6):WD37–WD44,
347"
REFERENCES,0.7470817120622568,"2015.
348"
REFERENCES,0.7490272373540856,"[2] T. Alkhalifah, H. Wang, and O. Ovcharenko. Mlreal: Bridging the gap between training on
349"
REFERENCES,0.7509727626459144,"synthetic data and real data applications in machine learning. arXiv, 2021.
350"
REFERENCES,0.7529182879377432,"[3] C. Bagaini. Acquisition and processing of simultaneous vibroseis data. Geophysical Prospecting,
351"
REFERENCES,0.754863813229572,"58:81–99, 2010.
352"
REFERENCES,0.7568093385214008,"[4] B. Bahia, R. Rongzhi Lin, and M. Sacchi. Regularization by denoising for simultaneous source
353"
REFERENCES,0.7587548638132295,"separation. Geophysics, 86(6):1942–2156, 2021.
354"
REFERENCES,0.7607003891050583,"[5] J. Batson and L. Royer.
Noise2self: Blind denoising by self-supervision.
International
355"
REFERENCES,0.7626459143968871,"Conference on Machine Learning, pages 524–533, 2019.
356"
REFERENCES,0.7645914396887159,"[6] C.J. Beasley, R.E. Chambers, and Z. Jiang. A new look at simultaneous sources. In 68th
357"
REFERENCES,0.7665369649805448,"Meeting, SEG Expanded Abstracts,, volume 2022, pages 133–135. European Association of
358"
REFERENCES,0.7684824902723736,"Geoscientists & Engineers, 1998.
359"
REFERENCES,0.7704280155642024,"[7] G. Beaudoin and A.A. Ross. Field design and operation of a novel deepwater, wide-azimuth
360"
REFERENCES,0.7723735408560312,"node seismic survey. The Leading Edge, 26:385–544, 2007.
361"
REFERENCES,0.77431906614786,"[8] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
362"
REFERENCES,0.7762645914396887,"problems. SIAM Journal on Imaging Sciences, 2:183–202, 2009.
363"
REFERENCES,0.7782101167315175,"[9] A. J. G. Berkhout. Changing the mindset in seismic data acquisition. The Leading Edge,
364"
REFERENCES,0.7801556420233463,"27:924–938, 2008.
365"
REFERENCES,0.7821011673151751,"[10] M. Berraki, S. Buizard, J. Ramirez, R.M. Elde, D. Eckert S.S. Roy, and J.-F. Synnevag. Grane
366"
REFERENCES,0.7840466926070039,"prm - from acquisition to interpretation in record time. In 79th EAGE Conference and Exhibition
367"
REFERENCES,0.7859922178988327,"2022, volume 2017, pages 1–5. European Association of Geoscientists & Engineers, 2017.
368"
REFERENCES,0.7879377431906615,"[11] B. Biondi. 3D Seismic Imaging Seismology (2nd ed.). SEG Books, 2006.
369"
REFERENCES,0.7898832684824902,"[12] C. Birnie and T. Alkhalifah. Leveraging domain adaptation for efﬁcient seismic denoising. In
370"
REFERENCES,0.791828793774319,"Energy in Data Conference, Austin, Texas, 20–23 February 2022, pages 11–15. Energy in Data,
371"
REFERENCES,0.7937743190661478,"2022.
372"
REFERENCES,0.7957198443579766,"[13] C. Birnie, M. Ravasi, L. Sixiu, and T. Alkhalifah. The potential of self-supervised networks for
373"
REFERENCES,0.7976653696498055,"random noise suppression in seismic data. Artiﬁcial Intelligence in Geosciences, 2:47–59, 2021.
374"
REFERENCES,0.7996108949416343,"[14] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical
375"
REFERENCES,0.8015564202334631,"learning via the alternating direction method of multipliers. Now Publishers Inc 3, 2011.
376"
REFERENCES,0.8035019455252919,"[15] C. Broaddus, A. Krull, M. Weigert, U. Schmidt, and G. Myers. Removing structured noise with
377"
REFERENCES,0.8054474708171206,"self-supervised blind-spot networks. 2020 IEEE 17th International Symposium on Biomedical
378"
REFERENCES,0.8073929961089494,"Imaging (ISBI), pages 159–163, 2020.
379"
REFERENCES,0.8093385214007782,"[16] P. Chatterjee and P. Milanfar. Patch-based near-optimal image denoising. Image Processing,
380"
REFERENCES,0.811284046692607,"IEEE Transactions on, 21(4):161–175, 2012.
381"
REFERENCES,0.8132295719844358,"[17] J. Cheng and M.D. Sacchi. Separation and reconstruction of simultaneous source data via
382"
REFERENCES,0.8151750972762646,"iterative rank reduction. Geophysics, 80(4):V57–V66, 2015.
383"
REFERENCES,0.8171206225680934,"[18] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-d transform-
384"
REFERENCES,0.8190661478599222,"domain collaborative ﬁltering. IEEE Transactions on image processing, 16(8):2080–2095,
385"
REFERENCES,0.8210116731517509,"2007.
386"
REFERENCES,0.8229571984435797,"[19] N. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned
387"
REFERENCES,0.8249027237354085,"dictionaries. Image Processing, IEEE Transactions on, 15(12):3726–3745, 2006.
388"
REFERENCES,0.8268482490272373,"[20] S. Gu, R. Timofte, and L. van Gool. Integrating local and non-local denoiser priors for image
389"
REFERENCES,0.8287937743190662,"restoration. In International Conference on Pattern Recognition, pages 2923–2928, 2018.
390"
REFERENCES,0.830739299610895,"[21] W. Huang, R. Wang, X. Gong, and Y. Chen. Iterative deblending of simultaneous-source
391"
REFERENCES,0.8326848249027238,"seismic data with structuring median constraint. IEEE Geoscience and Remote Sensing Letters,
392"
REFERENCES,0.8346303501945526,"15(1):58–62, 2017.
393"
REFERENCES,0.8365758754863813,"[22] S. Huo, Y. Luo, and P.G. Kelamis. Simultaneous sources separation via multidirectional
394"
REFERENCES,0.8385214007782101,"vector-median ﬁltering. Geophysics, 77(4):123–131, 2012.
395"
REFERENCES,0.8404669260700389,"[23] S. Huo, Y. Luo, and P.G. Kelamis. Deblending using a space-varying median ﬁlter. Exploration
396"
REFERENCES,0.8424124513618677,"Geophysics, 46(4):332–341, 2015.
397"
REFERENCES,0.8443579766536965,"[24] A. Ibrahim and M.D. Sacchi. Simultaneous source separation using a robust radon transform.
398"
REFERENCES,0.8463035019455253,"Geophysics, 79:V1–V11, 2014.
399"
REFERENCES,0.8482490272373541,"[25] P.R.S. Johann, E.A. Thedy, F.A. Gomes, and M.C. Schinelli. 4d seismic in brazil: Experiences
400"
REFERENCES,0.8501945525291829,"in reservoir monitoring. Offshore Technology Conference, 2006.
401"
REFERENCES,0.8521400778210116,"[26] A. Krull, T.-O. Buchholz, and F. Jug. Noise2void - learning denoising from single noisy images.
402"
REFERENCES,0.8540856031128404,"Proc. IEEE Conf. Comput. Vision Pattern Recognit., 2019.
403"
REFERENCES,0.8560311284046692,"[27] S. Laine, T. Karras, J. Lehtinen, and T. Aila. High-quality self-supervised deep image denoising.
404"
REFERENCES,0.857976653696498,"Advances in Neural Information Processing Systems, pages 6970–6980, 2019.
405"
REFERENCES,0.8599221789883269,"[28] J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Karras, M. Aittala, and T. Aila. Noise2noise:
406"
REFERENCES,0.8618677042801557,"Learning image restoration without clean data. International Conference on Machine Learning
407"
REFERENCES,0.8638132295719845,"(ICML), 2019.
408"
REFERENCES,0.8657587548638133,"[29] C. Li, C.C. Mosher, and Y. Ji. An amplitude-preserving deblending approach for simultaneous
409"
REFERENCES,0.867704280155642,"sources. Geophysics, 84(3):V185–V196, 2019.
410"
REFERENCES,0.8696498054474708,"[30] C. Li, C.C. Mosher, and Y. Ji. Randomized marine acquisition with compressive sampling
411"
REFERENCES,0.8715953307392996,"matrices. Geophysical Prospecting, 60(3):648–662, 2019.
412"
REFERENCES,0.8735408560311284,"[31] Jiaming Liu, Yu Sun, Cihat Eldeniz, Weijie Gan, Hongyu An, and Ulugbek S. Kamilov. Rare:
413"
REFERENCES,0.8754863813229572,"Image reconstruction using deep priors learned without groundtruth. JSTSP, 14:1088–1099,
414"
REFERENCES,0.877431906614786,"2020.
415"
REFERENCES,0.8793774319066148,"[32] S. Liu, C. Birnie, and T. Alkhalifah. Coherent noise suppression via a self-supervised deep
416"
REFERENCES,0.8813229571984436,"learning scheme. In 83rd EAGE Conference and Exhibition 2022, volume 2022, pages 1–5.
417"
REFERENCES,0.8832684824902723,"European Association of Geoscientists & Engineers, 2022.
418"
REFERENCES,0.8852140077821011,"[33] A. Mahdad, P. Doulgeris, and G. Blacquiere. Separation of blended data by iterative estimation
419"
REFERENCES,0.8871595330739299,"and subtraction of blending interference noise. Geophysics, 76:Q9–Q17.
420"
REFERENCES,0.8891050583657587,"[34] S. Mandelli, V. Lipari, P. Bestagini, and S. Tubaro. Interpolation and denoising of seismic data
421"
REFERENCES,0.8910505836575876,"using convolutional neural networks. arXiv, 2019.
422"
REFERENCES,0.8929961089494164,"[35] T. Meinhardt, M. Moller, C. Hazirbas, and D. Cremers. Learning proximal operators: Using
423"
REFERENCES,0.8949416342412452,"denoising networks for regularizing inverse imaging problems. In Proceedings of the IEEE
424"
REFERENCES,0.896887159533074,"International Conference on Computer Vision, pages 1781–1790, 2017.
425"
REFERENCES,0.8988326848249028,"[36] I. Moore, B. Dragoset, T. Ommundsen, D. Wilson, C. Ward, and D. Eke. Simultaneous source
426"
REFERENCES,0.9007782101167315,"separation using dithered sources. SEG Technical Program Expanded Abstracts, 2008.
427"
REFERENCES,0.9027237354085603,"[37] N. Parikh. Proximal Algorithms. Foundations and Trends in Optimization, 2014.
428"
REFERENCES,0.9046692607003891,"[38] Y. Romano, M. Elad, and P. Milanfar. The little engine that could: Regularization by denoising
429"
REFERENCES,0.9066147859922179,"(RED). SIAM Journal on Imaging Sciences, 10(4):1804–1844, 2017.
430"
REFERENCES,0.9085603112840467,"[39] Ernest Ryu, Jialin Liu, Sicheng Wang, Xiaohan Chen, Zhangyang Wang, and Wotao Yin.
431"
REFERENCES,0.9105058365758755,"Plug-and-play methods provably converge with properly trained denoisers. In International
432"
REFERENCES,0.9124513618677043,"Conference on Machine Learning, pages 5546–5557. PMLR, 2019.
433"
REFERENCES,0.914396887159533,"[40] Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003.
434"
REFERENCES,0.9163424124513618,"[41] R. E. Sheriff and L.P. Geldart. Exploration Seismology (2nd ed.). Cambridge University Press,
435"
REFERENCES,0.9182879377431906,"1995.
436"
REFERENCES,0.9202334630350194,"[42] A. Siahkoohi, M. Louboutin, and F. J. Herrmann. The importance of transfer learning in seismic
437"
REFERENCES,0.9221789883268483,"modeling and imaging. Geophysics, 84(6):A47–A52, 2019.
438"
REFERENCES,0.9241245136186771,"[43] J. Sun, S. Slang, T. Elboth, T.L. Greiner, S. McDonald, and L.-J. Gelius. A convolutional neural
439"
REFERENCES,0.9260700389105059,"network approach to deblending seismic data. Geophysics, 85(4):WA13–WA26, 2020.
440"
REFERENCES,0.9280155642023347,"[44] Y. Sun, J. Liu, and U. Kamilov. Block coordinate regularization by denoising. In Advances in
441"
REFERENCES,0.9299610894941635,"Neural Information Processing Systems, pages 380–390, 2019.
442"
REFERENCES,0.9319066147859922,"[45] J.-B. Thibault, K.D. Sauer, C.A. Bouman, and J. Hsieh. A three-dimensional statistical approach
443"
REFERENCES,0.933852140077821,"to improved image quality for multislice helical ct.
444"
REFERENCES,0.9357976653696498,"[46] T. Tirer and R. Giryes. Super-resolution via image-adapted de- noising CNNs: Incorporating
445"
REFERENCES,0.9377431906614786,"external and internal learning. IEEE Signal Processing Letters, 26(7).
446"
REFERENCES,0.9396887159533074,"[47] T. Tirer and R. Giryes. Image restoration by iterative denoising and backward projections. IEEE
447"
REFERENCES,0.9416342412451362,"Transactions on Image Processing, 28(3):1220–1234, 2018.
448"
REFERENCES,0.943579766536965,"[48] S.V Venkatakrishnan, C.A. Bouman, and B. Wohlberg. Plug-and-play priors for model based
449"
REFERENCES,0.9455252918287937,"reconstruction. 2013 IEEE Global Conference on Signal and Information Processing, 2013.
450"
REFERENCES,0.9474708171206225,"[49] S. Wang, W. Hu, P. Yuan, X. Wu, Q. Zhang, P. Nadukandi, G.O. Botero, and J. Chen. Seismic
451"
REFERENCES,0.9494163424124513,"deblending by self-supervised deep learning with a blind-trace network. SEG/AAPG/SEPM
452"
REFERENCES,0.9513618677042801,"First International Meeting for Applied Geoscience Energy, 2021.
453"
REFERENCES,0.953307392996109,"[50] Xiajian Xu, Yu Sun, Jiaming Liu, Brendt Wohlberg, and Ulugbek S. Kamilov. Provable
454"
REFERENCES,0.9552529182879378,"convergence of plug-and-play priors with mme denoisers. IEEE Signal Processing Letters,
455"
REFERENCES,0.9571984435797666,"pages 1280–1284, 2020.
456"
REFERENCES,0.9591439688715954,"[51] Siwei Yu and Jianwei Ma. Deep learning for geophysics: Current and future trends. Reviews of
457"
REFERENCES,0.9610894941634242,"Geophysics, 59(3):e2021RG000742, 2021.
458"
REFERENCES,0.9630350194552529,"[52] K. Zhang, Y. Li, W. Zuo, L. Zhang, L. van Gool, and R. Timofte. Plug-and-play image
459"
REFERENCES,0.9649805447470817,"restoration with deep denoiser prior. In IEEE TPAMI, 2021.
460"
REFERENCES,0.9669260700389105,"[53] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a Gaussian denoiser: Residual
461"
REFERENCES,0.9688715953307393,"learning of deep CNN for image denoising. IEEE Transactions on Image Processing, pages
462"
REFERENCES,0.9708171206225681,"3142–3155, 2017.
463"
REFERENCES,0.9727626459143969,"[54] K. Zhang, W. Zuo, S. Gu, and L. Zhang.
464"
REFERENCES,0.9747081712062257,"[55] H. Zhou, W. Mao, D. Zhang, Q. Ge, and H. Wang. Deblending of simultaneous source with
465"
REFERENCES,0.9766536964980544,"rank-reduction and thresholding constraints. 79th EAGE Conference and Exhibition 2017,
466"
REFERENCES,0.9785992217898832,"2017(1):1–5, 2017.
467"
REFERENCES,0.980544747081712,"[56] Y. Zhou, W. Chen, and J. Gao. Separation of seismic blended data by sparse inversion over
468"
REFERENCES,0.9824902723735408,"dictionary learning. Journal of Applied Geophysics, 106:146–153, 2014.
469"
REFERENCES,0.9844357976653697,"[57] Y. Zhou, J. Gao, W. Chen, and P. Frossard. Seismic simultaneous source separation via patchwise
470"
REFERENCES,0.9863813229571985,"sparse representation. IEEE Transactions on Geoscience and Remote Sensing, 54(9):5271–5284,
471"
REFERENCES,0.9883268482490273,"2016.
472"
REFERENCES,0.9902723735408561,"[58] S. Zu, J. Cao, S. Qu, and Y. Chen. Iterative deblending for simultaneous source data using the
473"
REFERENCES,0.9922178988326849,"deep neural network. Geophysics, 85(2):V131–V141, 2020.
474"
REFERENCES,0.9941634241245136,"[59] S. Zu, H. Zhou, R. Wu, W. Mao, and Y. Chen. Hybrid-sparsity constrained dictionary learning
475"
REFERENCES,0.9961089494163424,"for iterative deblending of extremely noisy simultaneous-source data. IEEE Transactions on
476"
REFERENCES,0.9980544747081712,"Geoscience and Remote Sensing, 57(4):2249–2262, 2018.
477"
