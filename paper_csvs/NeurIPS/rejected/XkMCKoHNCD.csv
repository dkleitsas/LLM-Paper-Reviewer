Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0006779661016949153,"There are two broad, opposing views of the recent developments in large language
1"
ABSTRACT,0.0013559322033898306,"models (LLMs). The first of these uses the term ""stochastic parrots"" from Emily
2"
ABSTRACT,0.002033898305084746,"Bender et al [3] to emphasise that because LLMs are simply a method for creating
3"
ABSTRACT,0.002711864406779661,"a probability distribution over sequences of words, they can be viewed as simply
4"
ABSTRACT,0.003389830508474576,"parroting information in the training data. The second view, ""Sparks of AGI"" from
5"
ABSTRACT,0.004067796610169492,"Sebastien Bubeck et al [6], posits that the unprecedented scale of computation in
6"
ABSTRACT,0.0047457627118644066,"the newest generation of LLMs is leading to what its proponents call ""an early (yet
7"
ABSTRACT,0.005423728813559322,"still incomplete) version of an artificial general intelligence (AGI) system"". In this
8"
ABSTRACT,0.006101694915254237,"article, we propose a method for making predictions purely from the representation
9"
ABSTRACT,0.006779661016949152,"of data inside the LLM. Specifically, we create a logistic regression model, using
10"
ABSTRACT,0.007457627118644068,"the principal components of a LLM model embedding as features, in order to
11"
ABSTRACT,0.008135593220338983,"predict an output variable. The task we use to illustrate our method is predicting the
12"
ABSTRACT,0.008813559322033898,"characters in TV series, based on their lines in the show. We show that our method
13"
ABSTRACT,0.009491525423728813,"can, for example, distinguish Penny and Sheldon in the Big Bang Theory with
14"
ABSTRACT,0.010169491525423728,"an AUC performance of 0.79. Logistic regression models for other characters in
15"
ABSTRACT,0.010847457627118645,"Big Bang Theory have lower values of AUC (ranging from 0.59 to 0.79), with the
16"
ABSTRACT,0.01152542372881356,"most significant distinguishing factors between characters relating to the number
17"
ABSTRACT,0.012203389830508475,"and nature of comments they make about women. The characters in the TV-series
18"
ABSTRACT,0.01288135593220339,"Friends are more difficult to distinguish using this method (AUCs range from 0.61
19"
ABSTRACT,0.013559322033898305,"to 0.66). We find that the accuracy of our logistic regression on a linear feature
20"
ABSTRACT,0.01423728813559322,"space is slightly lower than GPT-4, which is in turn at a level comparable to two
21"
ABSTRACT,0.014915254237288136,"human experts. We discuss how the method we propose could be used to help
22"
ABSTRACT,0.015593220338983051,"researchers be more specific in the claims they make about large language models.
23"
INTRODUCTION,0.016271186440677966,"1
Introduction
24"
INTRODUCTION,0.01694915254237288,"Large language models (LLMs) are neural networks trained on a large text corpus to predict the next
25"
INTRODUCTION,0.017627118644067796,"word, phrase or paragraph in that dataset [25]. As the number of network parameters and the size of
26"
INTRODUCTION,0.01830508474576271,"the corpus increases, the ability of this network to write convincing-sounding texts improves [15]. As
27"
INTRODUCTION,0.018983050847457626,"a result, an increasing number of compelling LLM applications, from CHAT-GPT to Copilot, have
28"
INTRODUCTION,0.01966101694915254,"been developed. Recently, Bubek et al. argued that ""beyond its mastery of language, GPT-4 can solve
29"
INTRODUCTION,0.020338983050847456,"novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more,
30"
INTRODUCTION,0.021016949152542375,"without needing any special prompting"" [6]. For these authors, this ability to generalise revealed
31"
INTRODUCTION,0.02169491525423729,"""Sparks of AGI"", going on to state that they believed ""that [GPT-4] could reasonably be viewed as an
32"
INTRODUCTION,0.022372881355932205,"early (yet still incomplete) version of an artificial general intelligence (AGI) system.""
33"
INTRODUCTION,0.02305084745762712,"The stochastic parrots paradigm critiques such claims by pointing out that large language models
34"
INTRODUCTION,0.023728813559322035,"simply predict the next word, sentence or paragraph, and it is humans who attribute understanding to
35"
INTRODUCTION,0.02440677966101695,"its output [3]. LLMs simply replicate examples (i.e. parrot text) from a massive corpus of data [7].
36"
INTRODUCTION,0.025084745762711864,"The stochastic parrots view provides an epistemic critique of claims, such as ""Sparks of AGI"", about
37"
INTRODUCTION,0.02576271186440678,"artificial general intelligence. For example, in the context of the benchmark tests (such as those later
38"
INTRODUCTION,0.026440677966101694,"carried out by [6]), Raj et al. (2021) write, ""the reality of [benchmark] development, use and adoption
39"
INTRODUCTION,0.02711864406779661,"indicates a construct validity issue, where the involved benchmarks — due to their instantiation
40"
INTRODUCTION,0.027796610169491524,"in particular data, metrics and practice — cannot possibly capture anything representative of the
41"
INTRODUCTION,0.02847457627118644,"claims to general applicability being made about them."" In other words, the very notion of generality,
42"
INTRODUCTION,0.029152542372881354,"sought to be proven in ""Sparks of AGI"", cannot be captured by benchmark problems [6]. This
43"
INTRODUCTION,0.029830508474576273,"critique is fundamental: it doesn’t matter how many specific tasks a model completes, there is no
44"
INTRODUCTION,0.030508474576271188,"convergence towards generality. Even setting these epistemic problems aside, the stochastic parrots
45"
INTRODUCTION,0.031186440677966103,"view also has practical implications for how we evaluate LLM performance. For example, Lewis
46"
INTRODUCTION,0.031864406779661014,"and Mitchell (2024) manipulate benchmark tasks to construct ’counterfactual’ tasks, by for example
47"
INTRODUCTION,0.03254237288135593,"adding information that solves the task but LLM’s neglect this information, because they are parroting
48"
INTRODUCTION,0.033220338983050844,"answers to similar, previously trained-on examples [18].
49"
INTRODUCTION,0.03389830508474576,"In spite of the limitation of benchmarks, the fact remains that LLMs do perform well over a wide
50"
INTRODUCTION,0.03457627118644068,"range of tasks, with little or no additional training data. It is the question of understanding how such
51"
INTRODUCTION,0.03525423728813559,"performance might arise which we address in this paper. Instead of proposing new benchmarks, we
52"
INTRODUCTION,0.03593220338983051,"focus on comparing how LLMs perform to simpler, well-understood statistical methods on a novel
53"
INTRODUCTION,0.03661016949152542,"task. An approach like ours has previously been persued medical imaging — where a systematic
54"
INTRODUCTION,0.03728813559322034,"review showed that logistic regression on selected features performed (on average) just as well as
55"
INTRODUCTION,0.03796610169491525,"complicated machine learning approaches [8] — and with respect to conflict prediction — logistic
56"
INTRODUCTION,0.03864406779661017,"regression perform just as well (as is easier to interpret) than more complex machine learning models
57"
INTRODUCTION,0.03932203389830508,"[16].
58"
INTRODUCTION,0.04,"For many general tasks, a relatively straightforward method of making predictions is to use linear or
59"
INTRODUCTION,0.04067796610169491,"logistic regression on the leading principal components of a data set. One example is using principle
60"
INTRODUCTION,0.04135593220338983,"components of ’likes’ of Facebook users to predict the answers people gave to big-five personality
61"
INTRODUCTION,0.04203389830508475,"tests [32, 19, 17]. Konsinski et al. (2016) first performed PCA or Latent Dirichlet Allocation (LDA)
62"
INTRODUCTION,0.04271186440677966,"on the matrix of likes and Facebook users, and then used the leading components of the PCA (or
63"
INTRODUCTION,0.04338983050847458,"clusters of LDA) in a regression model to predict the user’s answers in personality tests [17]. This
64"
INTRODUCTION,0.04406779661016949,"allowed the authors to study how the accuracy of predictions increased with the number of dimensions
65"
INTRODUCTION,0.04474576271186441,"of the Facebook likes. The method is linear in the PCA space and has the advantage that the results
66"
INTRODUCTION,0.04542372881355932,"can be interpreted qualitatively. For example, young and female users could be predicted as liking
67"
INTRODUCTION,0.04610169491525424,"""humorous and juvenile"" (author’s choice of words) statements such as, ""I finally stop laughing . . .
68"
INTRODUCTION,0.04677966101694915,"look back over at you and start all over again"" [17].
69"
INTRODUCTION,0.04745762711864407,"The above method is potentially interesting in the context of stochastic parrots, because it allows us
70"
INTRODUCTION,0.04813559322033898,"to, so to speak, look inside the parrot’s brain. Large language models encode information using vector
71"
INTRODUCTION,0.0488135593220339,"semantics: words and sentences are represented as vectors [14, 24, 20], referred to as embeddings.
72"
INTRODUCTION,0.04949152542372881,"Words that occur in similar contexts tend to have similar meanings, therefore, they will have a similar
73"
INTRODUCTION,0.05016949152542373,"vector [25]. The vectors are generally based on a co-occurrence matrix, a way of representing how
74"
INTRODUCTION,0.05084745762711865,"often words co-occur. An alternative to using the term-document matrix to represent words as vectors
75"
INTRODUCTION,0.05152542372881356,"of document counts, is to use the term-term matrix . If we then take every occurrence of each word
76"
INTRODUCTION,0.05220338983050848,"and count the context words around it, we get a word-word co-occurrence matrix [14]. Embeddings
77"
INTRODUCTION,0.05288135593220339,"can be obtained with transformers models [27, 9, 11, 13, 31, 30], which were initially developed for
78"
INTRODUCTION,0.05355932203389831,"machine translation in 2017 [27, 28].
79"
INTRODUCTION,0.05423728813559322,"We can use principal components of the embeddings of a language model, with respect to a specific
80"
INTRODUCTION,0.05491525423728814,"problem, in order to both understand what information is used in solving the task and to test the
81"
INTRODUCTION,0.05559322033898305,"degree to which performance on that task is achieved from the representation of the data or from
82"
INTRODUCTION,0.05627118644067797,"some other unknown mechanism. To make these statements concrete, we now outline what we do in
83"
INTRODUCTION,0.05694915254237288,"this article. We address the task of predicting which character said which specific lines of dialogue
84"
INTRODUCTION,0.0576271186440678,"in two US TV series: Big Bang Theory and Friends. This task is reminiscent of the personality
85"
INTRODUCTION,0.05830508474576271,"research discussed above in that the characters in the show have very stereotypical personalities:
86"
INTRODUCTION,0.05898305084745763,"can we predict character personalities from their line in the show? Such problems are of specific
87"
INTRODUCTION,0.059661016949152545,"interest for this article, for three reasons (1) an increasing number of applications of AI involve
88"
INTRODUCTION,0.06033898305084746,"supposed personality tests and analyses [10]; (2) such tests raise ethical issues about both reliability
89"
INTRODUCTION,0.061016949152542375,"and applications [29, 1]; (3) they are sometimes used to imply that machines can understand us better
90"
INTRODUCTION,0.06169491525423729,"than we understand ourselves [32]. The character personality test is an example of generalisation in
91"
INTRODUCTION,0.062372881355932205,"the sense that, while large language models might have been fed data from these series, they haven’t
92"
INTRODUCTION,0.06305084745762712,"been trained to solve this specific task.
93"
INTRODUCTION,0.06372881355932203,"We proceed as follows. We first detail the method of and logistic regression on the principal
94"
INTRODUCTION,0.06440677966101695,"components of the embeddings. We then analyse which PCA components are most predictive of
95"
INTRODUCTION,0.06508474576271187,"statements by the characters, how the number components affects accuracy and differences between
96"
INTRODUCTION,0.06576271186440678,"the TV shows. Finally, we compare performance of our simpler model to GPT-4 [22] and one human
97"
INTRODUCTION,0.06644067796610169,"expert, with extensive experience of the two TV shows.
98"
METHODS,0.0671186440677966,"2
Methods
99"
EMBEDDINGS AND PCA,0.06779661016949153,"2.1
Embeddings and PCA
100"
EMBEDDINGS AND PCA,0.06847457627118644,"The dataset is the transcript of the first 10 seasons of the TV-series The Big Bang Theory 1 and 10
101"
EMBEDDINGS AND PCA,0.06915254237288136,"seasons of the TV-serie Friends 2 in English. We cleaned the dataset, by only keeping the main
102"
EMBEDDINGS AND PCA,0.06983050847457627,"characters and their respective dialogue lines. This gives 44, 966 dialogue lines for the TV series The
103"
EMBEDDINGS AND PCA,0.07050847457627119,"Big Bang Theory and 51, 615 dialogue lines for the TV series Friends. We then transformed these
104"
EMBEDDINGS AND PCA,0.0711864406779661,"dialogue lines into a vector, i.e. we create embeddings using the python library SentenceTransformer
105"
EMBEDDINGS AND PCA,0.07186440677966102,"and the model ’all-MiniLM-L6-v2’ [26]. Each dialogue line then has a specific embedding, a vector
106"
EMBEDDINGS AND PCA,0.07254237288135593,"of dimension 384. For comparison, the small text embedding of OpenAi, ’text-embedding-3-small’,
107"
EMBEDDINGS AND PCA,0.07322033898305084,"gives 1,536 output dimension [21, 5].
108"
EMBEDDINGS AND PCA,0.07389830508474576,"We then performed a principal component analysis (PCA) on the embeddings (for more details of
109"
EMBEDDINGS AND PCA,0.07457627118644068,"the method we follow see [12]). Principal Component Analysis(PCA) determines the directions
110"
EMBEDDINGS AND PCA,0.07525423728813559,"that maximize the variation in the data. The PCA is a procedure that takes dataset with several
111"
EMBEDDINGS AND PCA,0.0759322033898305,"variables, to a smaller dataset with new variables (the principal components) that will be a linear
112"
EMBEDDINGS AND PCA,0.07661016949152542,"combination of the former variables. Each dimension in this space corresponds to a feature that will
113"
EMBEDDINGS AND PCA,0.07728813559322034,"be explicitly defined later. To ensure a representative view of the dataset, we need to standardize
114"
EMBEDDINGS AND PCA,0.07796610169491526,"it so that no single variable disproportionately influences the analysis, by removing the mean then
115"
EMBEDDINGS AND PCA,0.07864406779661016,"divide by the standard deviation. Then, we calculate the covariance matrix. A covariance matrix is a
116"
EMBEDDINGS AND PCA,0.07932203389830508,"square matrix that shows the covariance between pairs of variables in the dataset. The diagonal of the
117"
EMBEDDINGS AND PCA,0.08,"matrix gives the variance of the variables and the other terms give the covariance between the pair of
118"
EMBEDDINGS AND PCA,0.08067796610169492,"variables. The covariance measures of how much two random variables vary together, by estimating
119"
EMBEDDINGS AND PCA,0.08135593220338982,"the linearity between them. From the covariance matrix we deduce the eigenvectors and eigenvalues,
120"
EMBEDDINGS AND PCA,0.08203389830508474,"by doing an eigenvalue decomposition of the covariance matrix C. We find the eigenvector by solving
121"
EMBEDDINGS AND PCA,0.08271186440677966,"(C −λId)x = 0, where x is the eigenvector associated with the eigenvalue λ The eigenvalue gives
122"
EMBEDDINGS AND PCA,0.08338983050847458,"the magnitude (or accounted variance) of the data along the new feature dimension. The eigenvector
123"
EMBEDDINGS AND PCA,0.0840677966101695,"gives the direction of the data along the new feature dimension, and forms the linear combination
124"
EMBEDDINGS AND PCA,0.0847457627118644,"for a principal component. The eigenvalues are in descending order and as explained in [12], they
125"
EMBEDDINGS AND PCA,0.08542372881355932,"’maximize the explained variances on each dimension’. We refer to the the coefficients of the leading
126"
EMBEDDINGS AND PCA,0.08610169491525424,"eigenvector as the first principal component (PCA1), the second eigenvector as PCA2 and so on. We
127"
EMBEDDINGS AND PCA,0.08677966101694916,"reduce the 384 dimension of each embeddings to a dimension space of 300. All calculations were
128"
EMBEDDINGS AND PCA,0.08745762711864406,"performed in Sklearn [23] and full code is available here 3.
129"
EMBEDDINGS AND PCA,0.08813559322033898,"An important aspect of our approach is gaining a qualitative understanding of how the principal
130"
EMBEDDINGS AND PCA,0.0888135593220339,"components reflect the meaning of the dialogue lines. Each PCA corresponds to one eigenvector and
131"
EMBEDDINGS AND PCA,0.08949152542372882,"consequently to one dimension from which we are able investigate which kind of phrases tied to that
132"
EMBEDDINGS AND PCA,0.09016949152542372,"dimension. To help us make this analysis we used two-dimensional visualisation of the data. First we
133"
EMBEDDINGS AND PCA,0.09084745762711864,"implemented a 10-means cluster on two principal components at a time, starting with the leading
134"
EMBEDDINGS AND PCA,0.09152542372881356,"components (i.e PCA1 and PCA2). We colour each cluster and then assign the phrase nearest of the
135"
EMBEDDINGS AND PCA,0.09220338983050848,"center as the cluster name (see figure 1). We also looked at the most extreme dialogue line in each
136"
EMBEDDINGS AND PCA,0.0928813559322034,"PCA, by printing out the sentences with the highest values and the smallest values. From these we
137"
EMBEDDINGS AND PCA,0.0935593220338983,"assigned a qualitative interpretation of the ""meaning"" of the leading PCAs. In the annex, we report
138"
EMBEDDINGS AND PCA,0.09423728813559322,"the tenth highest values and the tenth smallest values.
139"
CHARACTER PREDICTION,0.09491525423728814,"2.2
Character prediction
140"
CHARACTER PREDICTION,0.09559322033898306,"In order to predict which dialogue line comes from which character, we use a logistic regression on
141"
CHARACTER PREDICTION,0.09627118644067796,"the PCAs of the dialogue lines of the characters. We follow the notation from [12] and let uj,i be the
142"
CHARACTER PREDICTION,0.09694915254237288,"1http://www.kaggle.com/datasets/mitramir5/the-big-bang-theory-series-transcript
2https://github.com/yaylinda/friends-dialog/blob/master/data.csv
3https://github.com/amandinecaut/Friends_analysis.git"
CHARACTER PREDICTION,0.0976271186440678,"j-th the coefficient of the principal component of the i-th dialogue line. First, we normalise all the
143"
CHARACTER PREDICTION,0.09830508474576272,"coefficients uj,i of the principal components by taking away the mean and dividing by the standard
144"
CHARACTER PREDICTION,0.09898305084745762,"deviation, so each component has mean zero and standard deviation of one. We then performed a
145"
CHARACTER PREDICTION,0.09966101694915254,"binomial logistic regression — e.g. does the dialogue line belong to Penny or Sheldon ? — based on
146"
CHARACTER PREDICTION,0.10033898305084746,"a linear prediction of the dialogue line i:
147"
CHARACTER PREDICTION,0.10101694915254238,"β0 + β1u1,i + ... + βnun,i,"
CHARACTER PREDICTION,0.1016949152542373,"allowing to measure (using regression coefficients {β0, ..., βn}) how the the explanatory variables
148"
CHARACTER PREDICTION,0.1023728813559322,"u1,i, ..., un,i, impact the prediction. The fitted logistic regression is model is given by
149"
CHARACTER PREDICTION,0.10305084745762712,"P (Sheldon|the i-th line is said by Sheldon or Penny) =
1
1 + e−(β0+β1u1,i+...+βnun,i)"
CHARACTER PREDICTION,0.10372881355932204,"where β0 determines the intercept (i.e. it is the outcome when all the other predictors variables are
150"
CHARACTER PREDICTION,0.10440677966101695,"equal to zero). Each coefficient βi estimates the additional effect of adding the corresponding variable
151"
CHARACTER PREDICTION,0.10508474576271186,"to the model prediction.
152"
CHARACTER PREDICTION,0.10576271186440678,"The sign of the coefficient indicates the influence of the specific principal component on the probability
153"
CHARACTER PREDICTION,0.1064406779661017,"it is a particular character. If the sign is positive then it is more likely to be that character (Penny
154"
CHARACTER PREDICTION,0.10711864406779661,"in the example above) if the dialogue line has larger and more positive values of that component.
155"
CHARACTER PREDICTION,0.10779661016949152,"Conversely, if the sign is negative that means it is less likely to be that character if the dialogue line
156"
CHARACTER PREDICTION,0.10847457627118644,"has larger and more positive values of that component. The larger the magnitude of the coefficient,
157"
CHARACTER PREDICTION,0.10915254237288136,"the more important the predictor variable is in making the prediction.
158"
CHARACTER PREDICTION,0.10983050847457627,"For each TV series, we proceed to a logistic regression with 300 first PCAs, for each possible pair
159"
CHARACTER PREDICTION,0.11050847457627119,"of characters. We obtain a predictor function and evaluate the absolute value of each regression
160"
CHARACTER PREDICTION,0.1111864406779661,"coefficient. We obtain the magnitude of each coefficient and therefore assess which coefficients have
161"
CHARACTER PREDICTION,0.11186440677966102,"the most importance in the logistic regression. Afterwards we take the ten regression coefficients with
162"
CHARACTER PREDICTION,0.11254237288135593,"the largest aboslute value and plot them (see figure 3 and 8). From this analyse, we deduce which
163"
CHARACTER PREDICTION,0.11322033898305085,"dimensions that have an impact on the character’s prediction. To evaluate performance we calculate
164"
CHARACTER PREDICTION,0.11389830508474576,"the AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve to evaluate as a
165"
CHARACTER PREDICTION,0.11457627118644068,"function of the dimensions.
166"
CHARACTER PREDICTION,0.1152542372881356,"2.3
Comparing to GPT4 and human expert
167"
CHARACTER PREDICTION,0.11593220338983051,"In order to test our method against a large language model we queried GPT4 with the following
168"
CHARACTER PREDICTION,0.11661016949152542,"system prompt: ""You are expert on the TV series The Big Bang Theory. You are now being challenged
169"
CHARACTER PREDICTION,0.11728813559322034,"to identify characters from the series. Try your best to do well. If you can beat another human expert
170"
CHARACTER PREDICTION,0.11796610169491525,"there is a prize."" and a query that asked ""Tell me who was most likely out of Leonard and Sheldon
171"
CHARACTER PREDICTION,0.11864406779661017,"(from the series Big Bang Theory) to have said the following line of dialogue: [DIALOGUE LINE].
172"
CHARACTER PREDICTION,0.11932203389830509,"Now state the most likely character as a single word, either Leonard and Sheldon. Do not write
173"
CHARACTER PREDICTION,0.12,"anything else."" Character and TV series names were adjusted appropriately for each test. We tested
174"
CHARACTER PREDICTION,0.12067796610169491,"four pairs (Penny/Sheldon, Leonard/Sheldon, Phoebe/Ross, Phoebe/Chandler). We first repeated the
175"
CHARACTER PREDICTION,0.12135593220338983,"above procedure 100 times, 50 times for each character, to test the accuracy of the classification (i.e.
176"
CHARACTER PREDICTION,0.12203389830508475,"proportion of correct answers).
177"
CHARACTER PREDICTION,0.12271186440677966,"We also provided the same dialogue lines to two motivated human experts (who had watched both
178"
CHARACTER PREDICTION,0.12338983050847457,"series in their entirety two times, most recently within the last year) and expressed a determination to
179"
CHARACTER PREDICTION,0.12406779661016949,"beat GPT4. Both participants were relatives of the co-authors of this article. The same dialogue lines
180"
CHARACTER PREDICTION,0.12474576271186441,"on which GPT4 was tested, were presented in a random order in the spreadsheet file. The subjects
181"
CHARACTER PREDICTION,0.12542372881355932,"were asked to guess the name of the character for each dialogue line, and write it into the spreadsheet.
182"
RESULTS,0.12610169491525425,"3
Results
183"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.12677966101694915,"3.1
Qualitative analysis of the principal components
184"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.12745762711864406,"We started by plotting the embedded dialogue lines ’Big Bang Theory’ in terms of the six most
185"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.128135593220339,"important principal components, in order to visualise the most distinguishing features of the dialogue.
186"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.1288135593220339,"The first two of these (PCA1 and PCA2) are shown in figure 1aa. The nearest neighbour clustering
187"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.12949152542372883,"then allows us to see where different dialogue lines are found in these dimensions. We can see that
188"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.13016949152542373,"larger negative values of PCA1 corresponds to very short phrases (for example ’Uh’ in the pink
189"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.13084745762711864,(a) Projection of PCA1 and PCA2
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.13152542372881357,(b) Projection of PCA3 and PCA4
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.13220338983050847,(c) Projection of PCA5 and PCA6
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.13288135593220338,"Figure 1: Projection of the first 6 PCAs. Each PCA has an interpretation from the qualitative analysis.
Each plot has their respective cluster along with the average phrase of each cluster for The Big Bang
Theory dialogue lines"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.1335593220338983,"cluster in the top left of the figure) and larger positive values of PCA1 correspond to phrases about
190"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.1342372881355932,"Sheldon (for example ’Sheldon, what do you expect us to do?’ in the green cluster in the top right
191"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.13491525423728815,"of the figure). The qualitative analysis of PCA1 confirmed this pattern, with ’Yeah’ being the most
192"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.13559322033898305,"extreme negative value and ’You know, I was thinking. Without Sheldon, most of us would have
193"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.13627118644067795,"never met, but Penny would still live across from him.’ being the extreme positive value (see Annex
194"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.1369491525423729,"6 for a list of the ten most extreme positive and negative values of PCA1 and the other principal
195"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.1376271186440678,"components).
196"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.13830508474576272,"Following the same approach for PCA2, we found that the negative values are associated with long
197"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.13898305084745763,"phrases about a female characters an positive values with phrases about Sheldon. The most extreme
198"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.13966101694915253,"negative value is ’Well, there was the time I had my tonsils out, and I shared a room with a little
199"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14033898305084747,"Vietnamese girl. She didn’t make it through the night, but up till then, it was kind of fun.’ and the
200"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14101694915254237,"most extreme positive value is ’Leonard, Sheldon.’(see annex 6). The cluster values in figure 1a also
201"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14169491525423727,"show the same pattern: with ’Indian princess who befriends a monkey who was mocked by all other
202"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.1423728813559322,"monkeys because he was different. For some reason I related to it quite strongly’ in the orange cluster
203"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.1430508474576271,"(a) The Big Bang Theory
(b) Friends"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14372881355932204,"Figure 2: AUC curves to assess the performance of the logistic regression, by increasing the number
of dimensions, in the dialogue lines’s prediction for two different couples for the two Tv serie"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14440677966101695,"at the bottom of figure 1a and ’Sheldon, why are you doing this?’ in the light green cluster at the top
204"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14508474576271185,"of the same figure.
205"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14576271186440679,"A similar approach can be used to interpret figure 1b and c. PCA 3 ranges from phrase that questions
206"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.1464406779661017,"a premise (’Really? I didn’t know that.’) to phrases with a first person future action (’Aw, sweetie,
207"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14711864406779662,"I’m comfortable around you, too.’). PCA 4 ranges from a phrase about relationship (’Really? That
208"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14779661016949153,"seems rather short sighted, coming from someone who is generally considered altogether unlikable.
209"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14847457627118643,"Why don’t you take some time to reconsider?’) to a phrase related to eating out (’Excellent! What
210"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14915254237288136,"are you planning to wear?’). The fifth dimension is phrase with often a negation or counterargument
211"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.14983050847457627,"(like ’Oh no, no, no, crystals don’t work’, which is green in figure 1) to a short question about a
212"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.15050847457627117,"woman (like ’She knows you. She’s tense. We all are. Buy a basket!’, which is red in the same figure).
213"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.1511864406779661,"Finally, PCA 6 ranges from an apology (e.g. ’I wish you weren’t wearing flip-flops. It’s dangerous to
214"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.151864406779661,"drive in flip-flops’) to a phrase with affirmative statement( e.g. ’Still going to introduce him?’). This
215"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.15254237288135594,"final interpretation is even clearer when we look at the extreme negative value ( ’Relax, it wasn’t your
216"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.15322033898305085,"fault.’) and extreme positive value (’Sure. I’d like to meet her.’). Overall, in The Big Bang Theory
217"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.15389830508474575,"the distinguishing characteristics of the principal components often relate to the characters views of
218"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.15457627118644068,"women. For Friends, there are also clear semantic differences in the sentences, although these appear
219"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.1552542372881356,"to be less gender stereotyped. We give a full analysis of the leading six components in annex 5.3.
220"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.15593220338983052,"When we plot the average position of the characters in the space of the first two components, the
221"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.15661016949152542,"differences are very small in comparison to the variation (figure 5 in annex 6). For example, while
222"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.15728813559322033,"there is a distance of 0.33 between Leonard and Amy on the PCA 1 axis, the standard deviation of
223"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.15796610169491526,"the values for the Leonard and Amy on that axis are 3.62 and 3.58, respectively. This observations
224"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.15864406779661017,"indicates that it is impossible to distinguish the characters in terms of just a single dimension. We do
225"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.15932203389830507,"note, though, that Friends characters are even closer together than The Big Bang Theory characters
226"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.16,"(the PCA1 distance between Chandler and Rachel is 0.15 and between Chandler and Joey is 0.11,
227"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.1606779661016949,"while the standard deviations of Chandler, Rachel and Joey are respectively 3.62, 4.03 and 3.78). The
228"
QUALITATIVE ANALYSIS OF THE PRINCIPAL COMPONENTS,0.16135593220338984,"biggest difference we observed is between Penny and Sheldon.
229"
CHARACTER PREDICTION,0.16203389830508474,"3.2
Character prediction
230"
CHARACTER PREDICTION,0.16271186440677965,"While a small number of principal component dimensions is not sufficient to tell the characters apart,
231"
CHARACTER PREDICTION,0.16338983050847458,"can we use more of the dimensions to make the distinction? To test this we performed binomial logistic
232"
CHARACTER PREDICTION,0.1640677966101695,"regression on pairs of characters as a function of the number of principal components we included in
233"
CHARACTER PREDICTION,0.16474576271186442,"the model. The AUC values in figure 2a show a steady improvement in the predictions up to around
234"
CHARACTER PREDICTION,0.16542372881355932,"50 principal components for Big Bang Theory, after which only slight increases in performance are
235"
CHARACTER PREDICTION,0.16610169491525423,"obtained. Sheldon and Penny were easier to distinguish using this method than Sheldon and Leonard.
236"
CHARACTER PREDICTION,0.16677966101694916,"Figure 2b, shows that Friends characters were much more difficult to distinguish using this method.
237"
CHARACTER PREDICTION,0.16745762711864406,"If we view the principal component analysis as an attempt to capture the character’s personality by
238"
CHARACTER PREDICTION,0.168135593220339,"their dialogue lines (as in the analysis by [32]) then we can say that the TV characters personality
239"
CHARACTER PREDICTION,0.1688135593220339,"have a dimension of somewhere between 50 and 100. Each new dimension gives a small extra insight
240"
CHARACTER PREDICTION,0.1694915254237288,"Figure 3: Regression coefficients for each possible character pairs for the TV series The Big Bang
Theory. For each pair, we conduct a logistic regression to predict if the dialogue line is more likely to
be said by a character1 such that P (character1 = 1|the line is said by character1 or character2). We
use the first 300 principal components in the logistic regression. Then, we assess the absolute value
of each coefficient to determine their magnitude. Following this, we select the top ten coefficients
for each linear predictor function. We report in this figure those coefficients, along with their
corresponding dimensions. The coefficients are in decreasing order from left to right: the left side
have the coefficient with the highest magnitude, the right side have the coefficients with the lowest
magnitude."
CHARACTER PREDICTION,0.17016949152542374,"into the character differences. Since Friends characters are more difficult to predict from what they
241"
CHARACTER PREDICTION,0.17084745762711864,"say, we can conclude that Friends characters are less stereotyped than characters in The Big Bang
242"
CHARACTER PREDICTION,0.17152542372881355,"Theory.
243"
CHARACTER PREDICTION,0.17220338983050848,"We can investigate which PCA dimensions best distinguish characters by looking at the coefficients
244"
CHARACTER PREDICTION,0.17288135593220338,"of the binary regression. Figure 3 shows the ten most important components (determined by the
245"
CHARACTER PREDICTION,0.17355932203389832,"magnitude of the absolute value of the coefficients in the regression) for distinguishing the characters
246"
CHARACTER PREDICTION,0.17423728813559322,"dialogue lines in The Big Bang Theory. Each row represents a character pair, with the PCAs ordered
247"
CHARACTER PREDICTION,0.17491525423728813,"from left to right according to the magnitude of the coefficients. The first column corresponds to the
248"
CHARACTER PREDICTION,0.17559322033898306,"coefficient with the largest magnitude in the linear predictor function, the second column corresponds
249"
CHARACTER PREDICTION,0.17627118644067796,"to the second coefficient with the second largest magnitude, and so on.
250"
CHARACTER PREDICTION,0.1769491525423729,"As an example, the first row is the character prediction for the couple ’Penny and Shel-
251"
CHARACTER PREDICTION,0.1776271186440678,"don’ should be read as considering the probability the dialogue line is by Penny, i.e.
252"
CHARACTER PREDICTION,0.1783050847457627,"P (Penny = 1|the line is said by Penny or Sheldon). The first cell entry, PCA19, is the coefficient in
253"
CHARACTER PREDICTION,0.17898305084745764,"the linear predictor function with the largest absolute value. Performing a qualitative analysis on
254"
CHARACTER PREDICTION,0.17966101694915254,"PCA19 (see annex 6) we find that negative coefficients correspond to lines about food and positive
255"
CHARACTER PREDICTION,0.18033898305084745,"coefficients correspond to lines about comics. In this case, the coefficient of the PCA19 is negative,
256"
CHARACTER PREDICTION,0.18101694915254238,"implying that if a dialogue line is about meal or food, it is more likely to be spoken by Penny than
257"
CHARACTER PREDICTION,0.18169491525423728,"Sheldon.
258"
CHARACTER PREDICTION,0.18237288135593221,"The most common occurring component in figure 3 is exactly this PCA 19 (food vs. comics) which
259"
CHARACTER PREDICTION,0.18305084745762712,"has 12 occurrences. PCA 2, which is long phrases about a female character versus phrases with one
260"
CHARACTER PREDICTION,0.18372881355932202,"name has 11 occurrences. PCA 7 has 11 occurrences and ranges from phrases with yes/no to question
261"
CHARACTER PREDICTION,0.18440677966101696,"about the current situation. The next most common occurring components are PCA4 (10 occurrences)
262"
CHARACTER PREDICTION,0.18508474576271186,"which ranges from an apology to phrase with affirmative statement; PCA15 (10 occurrances) ranging
263"
CHARACTER PREDICTION,0.1857627118644068,"from long phrases about a woman to short phrases about houses; PCA17 (9 occurrences) range from
264"
CHARACTER PREDICTION,0.1864406779661017,"short phrases about travel to long food related phrases: PCA5 (9 occurrences) which range from long
265"
CHARACTER PREDICTION,0.1871186440677966,"dialogue lines that express an opinion to short questions about a female character.
266"
CHARACTER PREDICTION,0.18779661016949153,"Figure 4 shows the relationship between the characters in terms of PCA19 (which distinguishes
267"
CHARACTER PREDICTION,0.18847457627118644,"dialogue lines about meal/food related from those about comics). The graph shows the magnitude of
268"
CHARACTER PREDICTION,0.18915254237288134,"Figure 4: Relationship between characters in The Big Bang Theory in terms of PCA19 (that distin-
guishes lines about meal/food related form lines about comics). The value on the arrows show the
magnitude of the coefficient. Only pairings where the absolute value of the regression coefficient is
greater than 0.1 are included. The person at the start of the arrow talks about comics more than they
talk about food compared to the person at the end of the arrow."
CHARACTER PREDICTION,0.18983050847457628,"the coefficient and the direction of the arrow indicates that the coefficient is positive. For example, for
269"
CHARACTER PREDICTION,0.19050847457627118,"P(Penny|the line is said by Penny or Sheldon) the regression coefficient for the PCA19 is negative,
270"
CHARACTER PREDICTION,0.1911864406779661,"reflecting the fact that Penny talks more about food and Sheldon that talks more about comics, so
271"
CHARACTER PREDICTION,0.19186440677966102,"the arrow points from Sheldon to Penny. Similarly, we see that Bernadette talks more about food
272"
CHARACTER PREDICTION,0.19254237288135592,"than Raj, Howard Sheldon and Amy and thus the arrows point toward her. And Raj talks more about
273"
CHARACTER PREDICTION,0.19322033898305085,"comics than Penny, Bernadette and even Sheldon, so the arrows point out from him in the figure. In
274"
CHARACTER PREDICTION,0.19389830508474576,"the case of the TV series Friends, the magnitude of the regression coefficients are smaller than those
275"
CHARACTER PREDICTION,0.1945762711864407,"for The Big Bang Theory and a more varied number of components are represented (see annex 5.3).
276"
CHARACTER PREDICTION,0.1952542372881356,"While the method for constructing figure 4 can give an indication of how the components distinguish
277"
CHARACTER PREDICTION,0.1959322033898305,"the characters, we should bear in mind that in a regression of hundreds of variables (on which this
278"
CHARACTER PREDICTION,0.19661016949152543,"graph is based) the relationships established are not always straightforward. For example, in the
279"
CHARACTER PREDICTION,0.19728813559322034,"figure, we see that the respective models predict that Howard talks more about comics than Sheldon,
280"
CHARACTER PREDICTION,0.19796610169491524,"who talks more about comics than Penny, and Penny talks more about comics than Howard. This
281"
CHARACTER PREDICTION,0.19864406779661017,"inconsistency is likely due to other principal components distinguishing Penny and Howard better
282"
CHARACTER PREDICTION,0.19932203389830508,"than PCA19, and PCA19 acting as a counterbalance, to these additional components. A full analysis
283"
CHARACTER PREDICTION,0.2,"of these relationships is beyond the scope of the current article.
284"
CHARACTER PREDICTION,0.20067796610169492,"3.3
Comparing to GPT4 and human expert
285"
CHARACTER PREDICTION,0.20135593220338982,"Initial prompting of GPT4 revealed that it has knowledge of the two TV series in its training data.
286"
CHARACTER PREDICTION,0.20203389830508475,"GPT4 replied that it ""can provide information about the show, its characters, plot points, cultural
287"
CHARACTER PREDICTION,0.20271186440677966,"impact, and more"". It was also able to provide motivation for its answers. For example, when we
288"
CHARACTER PREDICTION,0.2033898305084746,"asked if this dialogue line ’Okay, sweetie, I don’t know if we’re gonna have cookies, or he’s just
289"
CHARACTER PREDICTION,0.2040677966101695,"gonna say hi, or really what’s gonna happen, so just let me talk, and we’ll. ..’, it correctly answered
290"
CHARACTER PREDICTION,0.2047457627118644,"’Penny’. Then, when asked, it to explain why it draws conclusions about the characters, it cited
291"
CHARACTER PREDICTION,0.20542372881355933,"criteria ""Context of Character Behavior"", ""Speech Patterns"" and ""Interaction Dynamics"".
292"
CHARACTER PREDICTION,0.20610169491525424,"For the set of 100 dialogue lines, a direct prompt to GPT4 (see methods for details) was correct for
293"
CHARACTER PREDICTION,0.20677966101694914,"Penny versus Sheldon on 81 occasions, for Sheldon versus Leonard on 71 occasions, for Phoebe
294"
CHARACTER PREDICTION,0.20745762711864407,"versus Ross on 66 occasions, and for Phoebe versus Chandler on 65 occasions. For these same test
295"
CHARACTER PREDICTION,0.20813559322033898,"examples, the first human expert was correct on 71, 76, 67, and 59 occasions, respectively. The
296"
CHARACTER PREDICTION,0.2088135593220339,"second human expert was correct on 74, 72, 70, and 73 occasions. For comparison, the accuracy
297"
CHARACTER PREDICTION,0.2094915254237288,"(percentage correct over all sentences) for the 300 dimensional PCA model was 72.8%, 68.1%,
298"
CHARACTER PREDICTION,0.21016949152542372,"59.7% and 60.6% respectively. The standard error for a proportion of 70% is 0.7 · 0.3 · 100 ≈4.5%,
299"
CHARACTER PREDICTION,0.21084745762711865,"suggesting a comparable level of performance between the human experts and GPT4, and a slightly
300"
CHARACTER PREDICTION,0.21152542372881356,"lower level of performance for the 300 dimensional PCA model.
301"
CONCLUSION,0.2122033898305085,"4
Conclusion
302"
CONCLUSION,0.2128813559322034,"Our qualitative analysis highlights how, when interpreted by a human, the principal components of the
303"
CONCLUSION,0.2135593220338983,"embeddings reflect the meaning of the dialogue lines of TV series. Many of dimensions contributing
304"
CONCLUSION,0.21423728813559323,"to the prediction are related to female characters. This can be attributed to the fact that the TV series
305"
CONCLUSION,0.21491525423728813,"portrays very stereotypical characters, with the main protagonists portrayed as geeks, embodying
306"
CONCLUSION,0.21559322033898304,"various clichés associated with them. A number of previous studies have identified gender and racial
307"
CONCLUSION,0.21627118644067797,"stereotyping within the way models represent data [4, 2, 29], we have shown that these dimensions
308"
CONCLUSION,0.21694915254237288,"are also important in the predictions these models make. Friends, in which the characters might be
309"
CONCLUSION,0.2176271186440678,"considered to have smaller stereotyped (within-group) differences, was more difficult to predict using
310"
CONCLUSION,0.2183050847457627,"this method.
311"
CONCLUSION,0.21898305084745762,"We have shown that given the principal components of the dialogue in a TV series, we are able to
312"
CONCLUSION,0.21966101694915255,"predict the characters personality using logistic regression, to a level of performance slightly below
313"
CONCLUSION,0.22033898305084745,"that of GPT4. We needed 50-100 dimensions in the logistic regression to predict a dialogue line in the
314"
CONCLUSION,0.22101694915254239,"TV series. This might be said to support the idea of a language model more like a stochastic parrot
315"
CONCLUSION,0.2216949152542373,"than a spark of AI, in the sense that a large part of the predictive skill of the model can be obtained by
316"
CONCLUSION,0.2223728813559322,"adding up the components of the word embeddings and providing an appropriate prediction. Indeed,
317"
CONCLUSION,0.22305084745762713,"we have used a much smaller embedding vector (384 dimensions) that GPT4 (several thousand
318"
CONCLUSION,0.22372881355932203,"dimensions) to achieve somewhat comparable results.
319"
CONCLUSION,0.22440677966101694,"That said, there remain two things which GPT4 does which our model does not. Firstly, our analysis
320"
CONCLUSION,0.22508474576271187,"starts from the sentence embeddings. Taking these embeddings as given ignores the complex process
321"
CONCLUSION,0.22576271186440677,"by which these are generated through training in the first place [9, 30]. Secondly, we had to specify
322"
CONCLUSION,0.2264406779661017,"the problem we wanted to solve as a logistic regression problem and train on previous data. GPT4,
323"
CONCLUSION,0.2271186440677966,"on the other hand, requires no additional training step and, from the given prompt, can identify the
324"
CONCLUSION,0.22779661016949151,"requested character. In light of these limitations, we see our work as highlighting the need to be
325"
CONCLUSION,0.22847457627118645,"more specific about claims related to sparks of AI [22]. We have shown that prediction part of the
326"
CONCLUSION,0.22915254237288135,"question of identifying TV character personality is (to some degree) obtainable from linear models,
327"
CONCLUSION,0.22983050847457628,"the question then is where the supposed spark lies? Is it in the creation of embeddings or is it in
328"
CONCLUSION,0.2305084745762712,"GPT4’s ability to identify the prediction problem from the input provided by the user? We would
329"
CONCLUSION,0.2311864406779661,"suggest that further dissections of how these methods work, like we have done here for the prediction
330"
CONCLUSION,0.23186440677966103,"stage, can shed more light on these questions.
331"
CONCLUSION,0.23254237288135593,"Our study is limited to a qualitative study of two very specific datasets. The contribution is primarily
332"
CONCLUSION,0.23322033898305083,"methodological. We propose an alternative to benchmark testing for understanding why a machine
333"
CONCLUSION,0.23389830508474577,"learning method works in the way it does, by comparing it to a method based on linear predictions.
334"
CONCLUSION,0.23457627118644067,"As such, it is a qualitative contribution to a larger debate around how to evaluate LLMs, rather than a
335"
CONCLUSION,0.2352542372881356,"quantitative demonstration of model performance.
336"
REFERENCES,0.2359322033898305,"References
337"
REFERENCES,0.2366101694915254,"[1]
Rediet Abebe et al. “Roles for computing in social change”. In: Proceedings of the 2020
338"
REFERENCES,0.23728813559322035,"conference on fairness, accountability, and transparency. 2020, pp. 252–260.
339"
REFERENCES,0.23796610169491525,"[2]
Salter Anastasia and Blodgett Bridget. Toxic Geek Masculinity in Media. Springer, 2017.
340"
REFERENCES,0.23864406779661018,"[3]
Emily M Bender et al. “On the dangers of stochastic parrots: Can language models be too big?”
341"
REFERENCES,0.2393220338983051,"In: Proceedings of the 2021 ACM conference on fairness, accountability, and transparency.
342"
REFERENCES,0.24,"2021, pp. 610–623.
343"
REFERENCES,0.24067796610169492,"[4]
Tolga Bolukbasi et al. “Man is to computer programmer as woman is to homemaker? debiasing
344"
REFERENCES,0.24135593220338983,"word embeddings”. In: Advances in neural information processing systems 29 (2016).
345"
REFERENCES,0.24203389830508473,"[5]
Tom B. Brown et al. Language Models are Few-Shot Learners. 2020. arXiv: 2005.14165
346"
REFERENCES,0.24271186440677966,"[cs.CL].
347"
REFERENCES,0.24338983050847457,"[6]
Sébastien Bubeck et al. “Sparks of artificial general intelligence: Early experiments with gpt-4”.
348"
REFERENCES,0.2440677966101695,"In: arXiv preprint arXiv:2303.12712 (2023).
349"
REFERENCES,0.2447457627118644,"[7]
Nicholas Carlini et al. Extracting Training Data from Large Language Models. 2021. arXiv:
350"
REFERENCES,0.2454237288135593,"2012.07805 [cs.CR].
351"
REFERENCES,0.24610169491525424,"[8]
Evangelia Christodoulou et al. “A systematic review shows no performance benefit of ma-
352"
REFERENCES,0.24677966101694915,"chine learning over logistic regression for clinical prediction models”. In: Journal of clinical
353"
REFERENCES,0.24745762711864408,"epidemiology 110 (2019), pp. 12–22.
354"
REFERENCES,0.24813559322033898,"[9]
Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language
355"
REFERENCES,0.2488135593220339,"Understanding. 2019. arXiv: 1810.04805 [cs.CL].
356"
REFERENCES,0.24949152542372882,"[10]
Jinyan Fan et al. “How well can an AI chatbot infer personality? Examining psychometric
357"
REFERENCES,0.2501694915254237,"properties of machine-inferred personality scores.” In: Journal of Applied Psychology 108.8
358"
REFERENCES,0.25084745762711863,"(2023), p. 1277.
359"
REFERENCES,0.25152542372881354,"[11]
William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling to Trillion
360"
REFERENCES,0.2522033898305085,"Parameter Models with Simple and Efficient Sparsity. 2022. arXiv: 2101.03961 [cs.LG].
361"
REFERENCES,0.2528813559322034,"[12]
Michael Greenacre et al. “Principal component analysis”. In: Nature Reviews Methods Primers
362"
REFERENCES,0.2535593220338983,"2.1 (2022), p. 100.
363"
REFERENCES,0.2542372881355932,"[13]
Hongzhao Huang and Fuchun Peng. An Empirical Study of Efficient ASR Rescoring with
364"
REFERENCES,0.2549152542372881,"Transformers. 2019. arXiv: 1910.11450 [cs.CL].
365"
REFERENCES,0.2555932203389831,"[14]
Dan Jurafsky and James H. Martin. Speech and Language Processing. URL: https://web.
366"
REFERENCES,0.256271186440678,"stanford.edu/~jurafsky/slp3/.
367"
REFERENCES,0.2569491525423729,"[15]
Jared Kaplan et al. “Scaling laws for neural language models”. In: arXiv preprint
368"
REFERENCES,0.2576271186440678,"arXiv:2001.08361 (2020).
369"
REFERENCES,0.2583050847457627,"[16]
Sayash Kapoor and Arvind Narayanan. “Leakage and the reproducibility crisis in machine-
370"
REFERENCES,0.25898305084745765,"learning-based science”. In: Patterns 4.9 (2023).
371"
REFERENCES,0.25966101694915256,"[17]
Michal Kosinski et al. “Mining big data to extract patterns and predict real-life outcomes.” In:
372"
REFERENCES,0.26033898305084746,"Psychological methods 21.4 (2016), p. 493.
373"
REFERENCES,0.26101694915254237,"[18]
Martha Lewis and Melanie Mitchell. “Using counterfactual tasks to evaluate the generality of
374"
REFERENCES,0.26169491525423727,"analogical reasoning in large language models”. In: arXiv preprint arXiv:2402.08955 (2024).
375"
REFERENCES,0.26237288135593223,"[19]
Dejan Markovikj et al. “Mining facebook data for predictive personality modeling”. In: Pro-
376"
REFERENCES,0.26305084745762713,"ceedings of the international AAAI conference on Web and social media. Vol. 7. 2. 2013,
377"
REFERENCES,0.26372881355932204,"pp. 23–26.
378"
REFERENCES,0.26440677966101694,"[20]
Tomas Mikolov et al. Distributed Representations of Words and Phrases and their Composi-
379"
REFERENCES,0.26508474576271185,"tionality. 2013. arXiv: 1310.4546 [cs.CL].
380"
REFERENCES,0.26576271186440675,"[21]
OpenAI. Embeddings. URL: https : / / platform . openai . com / docs / guides /
381"
REFERENCES,0.2664406779661017,"embeddings/embedding-models.
382"
REFERENCES,0.2671186440677966,"[22]
OpenAI et al. GPT-4 Technical Report. 2024. arXiv: 2303.08774 [cs.CL].
383"
REFERENCES,0.2677966101694915,"[23]
F. Pedregosa et al. “Scikit-learn: Machine Learning in Python”. In: Journal of Machine
384"
REFERENCES,0.2684745762711864,"Learning Research 12 (2011), pp. 2825–2830.
385"
REFERENCES,0.26915254237288133,"[24]
Jeffrey Pennington, Richard Socher, and Christopher Manning. “GloVe: Global Vectors for
386"
REFERENCES,0.2698305084745763,"Word Representation”. In: Proceedings of the 2014 Conference on Empirical Methods in
387"
REFERENCES,0.2705084745762712,"Natural Language Processing (EMNLP). Ed. by Alessandro Moschitti, Bo Pang, and Walter
388"
REFERENCES,0.2711864406779661,"Daelemans. Doha, Qatar: Association for Computational Linguistics, Oct. 2014, pp. 1532–
389"
REFERENCES,0.271864406779661,"1543. DOI: 10.3115/v1/D14-1162. URL: https://aclanthology.org/D14-1162.
390"
REFERENCES,0.2725423728813559,"[25]
Alec Radford et al. “Language models are unsupervised multitask learners”. In: OpenAI blog
391"
REFERENCES,0.27322033898305087,"1.8 (2019), p. 9.
392"
REFERENCES,0.2738983050847458,"[26]
Nils Reimers and Iryna Gurevych. “Sentence-BERT: Sentence Embeddings using Siamese
393"
REFERENCES,0.2745762711864407,"BERT-Networks”. In: Proceedings of the 2019 Conference on Empirical Methods in Natural
394"
REFERENCES,0.2752542372881356,"Language Processing. Association for Computational Linguistics, Nov. 2019. URL: https:
395"
REFERENCES,0.2759322033898305,"//arxiv.org/abs/1908.10084.
396"
REFERENCES,0.27661016949152545,"[27]
Ashish Vaswani et al. “Attention is All you Need”. In: Advances in Neural Informa-
397"
REFERENCES,0.27728813559322035,"tion Processing Systems. Ed. by I. Guyon et al. Vol. 30. Curran Associates, Inc., 2017.
398"
REFERENCES,0.27796610169491526,"URL: https : / / proceedings . neurips . cc / paper _ files / paper / 2017 / file /
399"
REFERENCES,0.27864406779661016,"3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
400"
REFERENCES,0.27932203389830507,"[28]
Ashish Vaswani et al. “Tensor2Tensor for Neural Machine Translation”. In: Proceedings
401"
REFERENCES,0.28,"of the 13th Conference of the Association for Machine Translation in the Americas (Vol-
402"
REFERENCES,0.28067796610169493,"ume 1: Research Track). Ed. by Colin Cherry and Graham Neubig. Boston, MA: Asso-
403"
REFERENCES,0.28135593220338984,"ciation for Machine Translation in the Americas, Mar. 2018, pp. 193–199. URL: https:
404"
REFERENCES,0.28203389830508474,"//aclanthology.org/W18-1819.
405"
REFERENCES,0.28271186440677964,"[29]
Laura Weidinger et al. “Taxonomy of risks posed by language models”. In: Proceedings of the
406"
REFERENCES,0.28338983050847455,"2022 ACM Conference on Fairness, Accountability, and Transparency. 2022, pp. 214–229.
407"
REFERENCES,0.2840677966101695,"[30]
Thomas Wolf et al. HuggingFace’s Transformers: State-of-the-art Natural Language Process-
408"
REFERENCES,0.2847457627118644,"ing. 2020. arXiv: 1910.03771 [cs.CL].
409"
REFERENCES,0.2854237288135593,"[31]
Linting Xue et al. mT5: A massively multilingual pre-trained text-to-text transformer. 2021.
410"
REFERENCES,0.2861016949152542,"arXiv: 2010.11934 [cs.CL].
411"
REFERENCES,0.28677966101694913,"[32]
Wu Youyou, Michal Kosinski, and David Stillwell. “Computer-based personality judgments
412"
REFERENCES,0.2874576271186441,"are more accurate than those made by humans”. In: Proceedings of the National Academy of
413"
REFERENCES,0.288135593220339,"Sciences 112.4 (2015), pp. 1036–1040.
414"
REFERENCES,0.2888135593220339,"5
Annex1 : Supplementary material
415"
AVERAGE POSITION FOR EACH MAIN CHARACTER OF THE TWO TV SERIES,0.2894915254237288,"5.1
Average Position for each main character of the two Tv series
416"
AVERAGE POSITION FOR EACH MAIN CHARACTER OF THE TWO TV SERIES,0.2901694915254237,"(a) in The Big Bang Theory
(b) in Friends"
AVERAGE POSITION FOR EACH MAIN CHARACTER OF THE TWO TV SERIES,0.29084745762711867,"Figure 5: Projection of the two first PCAs, and their respective interpretation, with the average
position for each main character of the two Tv series"
ACCURACY CURVES THE TWO TV SERIE,0.29152542372881357,"5.2
Accuracy curves the two Tv serie
417"
ACCURACY CURVES THE TWO TV SERIE,0.2922033898305085,"(a) The Big Bang Theory
(b) Friends"
ACCURACY CURVES THE TWO TV SERIE,0.2928813559322034,"Figure 6: Accuracy curves to assess the performance of the logistic regression, by increasing the
number of dimensions, in the dialogue lines’s prediction for two different couples for the two Tv serie"
FRIENDS ANALYSIS,0.2935593220338983,"5.3
Friends Analysis
418"
FRIENDS ANALYSIS,0.29423728813559324,"For Friends, we analyse similarly the 6 first dimensions as seen in Figure 7. The PCA1 is interpreted
419"
FRIENDS ANALYSIS,0.29491525423728815,"as phrase that include a name to ’Hey’. This is illustrate with the figure 7a by the dark blue cluster in
420"
FRIENDS ANALYSIS,0.29559322033898305,"the left with the average phrase ’Ms. Monroe... Oh there you go’, for the negative larger values of the
421"
FRIENDS ANALYSIS,0.29627118644067796,"PCA1, and by the red cluster on the top right with average phrase ’Hey’ for the positive larger values
422"
FRIENDS ANALYSIS,0.29694915254237286,"of the PCA1. The qualitative analysis, in annex 7, gives as the most extreme negative value of the
423"
FRIENDS ANALYSIS,0.2976271186440678,"PCA1 the phrase ’Yeah. It’s just gonna be too hard. Y’know? I mean, it’s Ross. How can I watch
424"
FRIENDS ANALYSIS,0.2983050847457627,"him get married? Y’know it’s just, it’s for the best, y’know it is, it’s... Y’know, plus, somebody’s got
425"
FRIENDS ANALYSIS,0.29898305084745763,"to stay here with Phoebe! Y’know she’s gonna be pretty big by then, and she needs someone to help
426"
FRIENDS ANALYSIS,0.29966101694915254,"her tie her shoes; drive her to the hospital in case she goes into labour.’. The most extreme positive
427"
FRIENDS ANALYSIS,0.30033898305084744,"value of the PCA1 is ’Hey’. The qualitative confirm our earlier statement about the interpretation of
428"
FRIENDS ANALYSIS,0.30101694915254235,"the PCA1.
429"
FRIENDS ANALYSIS,0.3016949152542373,"The PCA2 is phrase that include ’yeah’ to ’Hi’. The negative values of the PCA2 can be found on the
430"
FRIENDS ANALYSIS,0.3023728813559322,"figure 7a, for example from the light green cluster of at the bottom left, with average phrase ’Um,
431"
FRIENDS ANALYSIS,0.3030508474576271,(a) Projection of PCA1 and PCA2
FRIENDS ANALYSIS,0.303728813559322,(b) Projection of PCA3 and PCA4
FRIENDS ANALYSIS,0.3044067796610169,(c) Projection of PCA5 and PCA6
FRIENDS ANALYSIS,0.3050847457627119,"Figure 7: Projection of the first 6 PCAs. Each PCA has an interpretation from the qualitative analysis.
Each plot has their respective cluster along with the average phrase of each cluster for Friends
dialogue lines"
FRIENDS ANALYSIS,0.3057627118644068,"yeah.’, and the positive value are on the top red cluster with average phrase ’Hey’. It is confirm from
432"
FRIENDS ANALYSIS,0.3064406779661017,"the qualitative analysis in annex 7, give the most extreme negative value ’Yeah, fair enough.’ and the
433"
FRIENDS ANALYSIS,0.3071186440677966,"most extreme positive value ’Hey! Hi!’.
434"
FRIENDS ANALYSIS,0.3077966101694915,"The PCA3 is phrase that express the willingness to help and support someone to phrase that include a
435"
FRIENDS ANALYSIS,0.30847457627118646,"name.The projected values of the PCA3 are on the figure 7b, the negative values are on the left of the
436"
FRIENDS ANALYSIS,0.30915254237288137,"graph, for example, the orange cluster with average phrase ’Did you like learn about her family?’. In
437"
FRIENDS ANALYSIS,0.30983050847457627,"regards of the positive values, they are on the right of the graph, for example the light green cluster
438"
FRIENDS ANALYSIS,0.3105084745762712,"with average phrase ’Okay! Okay, you’re yelling again! See that?’. The qualitative analysis, see
439"
FRIENDS ANALYSIS,0.3111864406779661,"annex 7, shows the most extreme negative value of the PCA3 is ’Phoebe?! Wait a-but-but she just,
440"
FRIENDS ANALYSIS,0.31186440677966104,"she said that Joey was her backup.’ and the most extreme positive value is ’ Hi! I’m back. Yeah, that
441"
FRIENDS ANALYSIS,0.31254237288135595,"sounds great. Okay. Well, we’ll do it then. Okay, bye-bye.’
442"
FRIENDS ANALYSIS,0.31322033898305085,"The PCA4 interpretation is about phrase that include ’what’ or ’oh my God’, for example in the
443"
FRIENDS ANALYSIS,0.31389830508474575,"figure 7b in the dark purple cluster in the bottom left with average phrase ’What was that?’, to phrase
444"
FRIENDS ANALYSIS,0.31457627118644066,"about relationship and with the name, for example in the figure 7b with the dark green cluster with
445"
FRIENDS ANALYSIS,0.3152542372881356,"average phrase ’Oh hey, I’d shake your hand but uh: I’m really into the game. Plus, I think it’d be
446"
FRIENDS ANALYSIS,0.3159322033898305,"better for my ego if we didn’t stand rigt to each other.’. The qualitative analysis, in annex 7, confirm
447"
FRIENDS ANALYSIS,0.31661016949152543,"our statement with the following most extreme negative value ’What?! What is it?!. and the most
448"
FRIENDS ANALYSIS,0.31728813559322033,"extreme positive value ’Well it’s okay. Chandler is talking to her.’
449"
FRIENDS ANALYSIS,0.31796610169491524,"PCA5 is phrase about character relationship to phrase that include agreement. As seen in the figure
450"
FRIENDS ANALYSIS,0.31864406779661014,"7c, the negative value of PCA5 are represented on the graph on the left, for example with the light
451"
FRIENDS ANALYSIS,0.3193220338983051,"blue cluster with average phrase ’So, um, have you told your parents?’. The positive value of PCA5
452"
FRIENDS ANALYSIS,0.32,"are on the right of the figure 7c, as we can pick out from the dark purple cluster with average phrase
453"
FRIENDS ANALYSIS,0.3206779661016949,"’No, but Ross. We are never gonna happen, OK. Accept that.’. The qualitative analysis verify our
454"
FRIENDS ANALYSIS,0.3213559322033898,"interpretation, in annex 7, we see that the most extreme negative value is the phrase ’But, also, what
455"
FRIENDS ANALYSIS,0.3220338983050847,"happened between you and your Mom?. and the most extreme positive value is ’Yeah! That would be
456"
FRIENDS ANALYSIS,0.3227118644067797,"great!’.
457"
FRIENDS ANALYSIS,0.3233898305084746,"We interpret the PCA6 as phrase that question a name to phrase about marriage and proposal. The
458"
FRIENDS ANALYSIS,0.3240677966101695,"PCA6 projection is illustrate in the figure 7c, with negative values as the bottom, with for example
459"
FRIENDS ANALYSIS,0.3247457627118644,"the cluster dark orange with average phrase ’Yeah, Chandler why don’t you take a walk? This doesn’t
460"
FRIENDS ANALYSIS,0.3254237288135593,"concern you.’. The positive value of the PCA6 are in the top of the graph, for example dark green
461"
FRIENDS ANALYSIS,0.32610169491525426,"cluster with average phrase ’Okay, come on, I can’t get married until I get something old, something
462"
FRIENDS ANALYSIS,0.32677966101694916,"new, something borrowed, and something blue’. Our statement confirmed by the qualitative analysis,
463"
FRIENDS ANALYSIS,0.32745762711864407,"in annex 7, with the most extreme negative value is the phrase ’Wait a minute. What’s his name?’
464"
FRIENDS ANALYSIS,0.328135593220339,"and the most extreme positive value is the phrase ’Yes! We’re getting married?!’.
465"
FRIENDS ANALYSIS,0.3288135593220339,"Figure 8: Regression coefficients for each possible character pairs for the TV series Friends. For
each pair, we conduct a logistic regression to predict if the dialogue line is more likely to be said
by a character1 such that P (character1 = 1|the line is said by character1 or character2). We use the
first 300 principal components in the logistic regression. Then, we assess the absolute value of each
coefficient to determine their magnitude. Following this, we select the top ten coefficients for each
linear predictor function. We report in this figure those coefficients, along with their corresponding
dimensions. The coefficients are in decreasing order from left to right: the left side have the coefficient
with the highest magnitude, the right side have the coefficients with the lowest magnitude. The rows
are arrange such that the first row (the most significant coefficients) is in increasing order"
FRIENDS ANALYSIS,0.32949152542372884,"In the case of the TV series Friends, in the figure 8, the first column are the most significant regression
466"
FRIENDS ANALYSIS,0.33016949152542374,"coefficient for each pair. We can notice that the most extreme negative value is in the first row
467"
FRIENDS ANALYSIS,0.33084745762711865,"and belongs to the regression coefficient of the character’s dialogue lines prediction between Joey
468"
FRIENDS ANALYSIS,0.33152542372881355,"and Monica. The probability is as follow, P (Joey = 1|the line is said by Joey or Monica) = p and
469"
FRIENDS ANALYSIS,0.33220338983050846,"P (Monica = 0|the line is said by Joey or Monica)) = 1 −p. The corresponding dimension of the
470"
FRIENDS ANALYSIS,0.3328813559322034,"first coefficient is the PCA 18, it depicts phrase from ’Oh no’ to phrase that include ’yeah’ or
471"
FRIENDS ANALYSIS,0.3335593220338983,"’look’ (see qualitative analysis in annex ??. In other words, a phrase that include ’Oh no’ is more
472"
FRIENDS ANALYSIS,0.3342372881355932,"likely from Joey. The most extreme positive value in this first column appears in the last row,
473"
FRIENDS ANALYSIS,0.33491525423728813,"corresponding to the regression coefficients for predicting dialogue lines between the pair ’Rachel
474"
FRIENDS ANALYSIS,0.33559322033898303,"and Monica’. The probability is such that P (Rachel = 1|the line is said by Rachel or Monica) = p
475"
FRIENDS ANALYSIS,0.336271186440678,"and P (Monica = 0|the line is said by Rachel or Monica) = 1−p. The coefficient correspond to the
476"
FRIENDS ANALYSIS,0.3369491525423729,"dimension PCA17: from phrase that include ’Joey’ to phrase that include ’Ross’. We can deduce that,
477"
FRIENDS ANALYSIS,0.3376271186440678,"if a phrase include ’Ross’ it is more likely from Rachel.
478"
FRIENDS ANALYSIS,0.3383050847457627,"PCA 9
8 occurrences
From phrase that include ’Oh’,
to question about what the people has been doing"
FRIENDS ANALYSIS,0.3389830508474576,"PCA 18
8 occurrences
From ’Oh no’
to phrase that include ’yeah’ or ’look’"
FRIENDS ANALYSIS,0.3396610169491525,"PCA 7
7 occurrences
From phrase which is an answer a statement
to ’What?’"
FRIENDS ANALYSIS,0.3403389830508475,"PCA 17
6 occurrences
From phrase that include ’Joey’
to phrase that include ’Ross’"
FRIENDS ANALYSIS,0.3410169491525424,"PCA 16
6 occurrences
From phrase about a statement on a character
to question with ’What’
Table 1: Interpretation of the most important dimension in the dialogue lines prediction in Friends,
with the number of time they occurs in the figure 8"
FRIENDS ANALYSIS,0.3416949152542373,"For Friends, we also count the occurrences of each PCA from the figure 8, and then interpret them.
479"
FRIENDS ANALYSIS,0.3423728813559322,"We recapitulate the information in the table 1. Contrary to The Big Bang Theory the phrases in
480"
FRIENDS ANALYSIS,0.3430508474576271,"Friends are much shorter, more exclamatory, and there are less obvious topic like food or comics.
481"
FRIENDS ANALYSIS,0.34372881355932206,"In the TV series Friends, we note fewer instances of the main principal component analysis. For
482"
FRIENDS ANALYSIS,0.34440677966101696,"instance, in The Big Bang Theory, PCA19 occurs most frequently, appearing 12 times. However, in
483"
FRIENDS ANALYSIS,0.34508474576271186,"Friends, PCA9 and PCA18 are the most common dimensions, each occurring 8 times. If we count the
484"
FRIENDS ANALYSIS,0.34576271186440677,"number of different PCA in figure 3 for The Big Bang Theory we obtain 59, and 56 different PCA for
485"
FRIENDS ANALYSIS,0.3464406779661017,"Friends in the figure 8. The number of dimension is similar in both case, but we can pick out that the
486"
FRIENDS ANALYSIS,0.34711864406779663,"magnitude of the coefficient is slightly higher in The Big Bang Theory than in Friends. Since the TV
487"
FRIENDS ANALYSIS,0.34779661016949154,"serie Friends has less occurrences of the main PCAs, smaller magnitude in the regression coefficients
488"
FRIENDS ANALYSIS,0.34847457627118644,"and less AUC accuracy, therefore more dimension are needed into the dialogue line predictions. This
489"
FRIENDS ANALYSIS,0.34915254237288135,"is visible on the figure 5, where we can see that average position of the character in Friends are more
490"
FRIENDS ANALYSIS,0.34983050847457625,"closer than the average position of the character in The Big Bang Theory.
491"
FRIENDS ANALYSIS,0.3505084745762712,"Figure 9: Relationship between characters in Friends for the dimension that occurs the most in Figure
9 (PCA9) with phrase that include ’Oh’, to question about what the people has been doing. The
person at the start of the arrow ask more about what the people has been doing more than they have
phrase that include ’Oh’ to the person at the end of the arrow."
FRIENDS ANALYSIS,0.3511864406779661,"In the figure 9, we show the relationship between the character of Friends for the PCA9, the
492"
FRIENDS ANALYSIS,0.351864406779661,"dimension that have the most occurrences, it is interpret as with phrase that include ’Oh’ to question
493"
FRIENDS ANALYSIS,0.3525423728813559,"about what the people has been doing. For example, in the dialogue lines prediction P(Rachel =
494"
FRIENDS ANALYSIS,0.35322033898305083,"1|the line is said by Rachel or Ross), the regression coefficient is positive, then if it is a question
495"
FRIENDS ANALYSIS,0.3538983050847458,"about what the people has been doing, it is more likely from Rachel, and if it is a phrase that include
496"
FRIENDS ANALYSIS,0.3545762711864407,"’Oh’, then it is more likely to be from Ross. Then the arrow goes from Rachel to Ross. If the
497"
FRIENDS ANALYSIS,0.3552542372881356,"regression coefficient is negative, for example when we want to predict a dialogue line such that
498"
FRIENDS ANALYSIS,0.3559322033898305,"P(Rachel = 1|the line is said by Rachel or Joey), then if a phrase include ’Oh’ it is more likely to
499"
FRIENDS ANALYSIS,0.3566101694915254,"be said by Rachel, and if it is a question about what the people has been doing, it is more likely from
500"
FRIENDS ANALYSIS,0.3572881355932203,"Joey. The arrows goes from Joey to Rachel.
501"
FRIENDS ANALYSIS,0.3579661016949153,"6
Annex 2: Dialogue example of The Big Bang Theory
502"
FRIENDS ANALYSIS,0.3586440677966102,"6.1
PCA1
503"
LOWEST COEFFICIENT,0.3593220338983051,"6.1.1
Lowest coefficient
504"
LOWEST COEFFICIENT,0.36,"SHELDON : Yeah.
505"
LOWEST COEFFICIENT,0.3606779661016949,"LEONARD : Yeah.
506"
LOWEST COEFFICIENT,0.36135593220338985,"LEONARD : Yeah.
507"
LOWEST COEFFICIENT,0.36203389830508476,"LEONARD : Yeah.
508"
LOWEST COEFFICIENT,0.36271186440677966,"SHELDON : Yeah.
509"
LOWEST COEFFICIENT,0.36338983050847457,"LEONARD : Yeah.
510"
LOWEST COEFFICIENT,0.36406779661016947,"PENNY : Yeah.
511"
LOWEST COEFFICIENT,0.36474576271186443,"PENNY : Yeah.
512"
LOWEST COEFFICIENT,0.36542372881355933,"PENNY : Yeah.
513"
LOWEST COEFFICIENT,0.36610169491525424,"SHELDON : Yeah.
514"
HIGHEST COEFFICIENT,0.36677966101694914,"6.1.2
Highest coefficient
515"
HIGHEST COEFFICIENT,0.36745762711864405,"BERNADETTE : You know, I was thinking. Without Sheldon, most of us would have
516"
HIGHEST COEFFICIENT,0.368135593220339,"never met, but Penny would still live across from him.
517"
HIGHEST COEFFICIENT,0.3688135593220339,"AMY : Which couldn’t have happened if you didn’t live across the hall from her, which
518"
HIGHEST COEFFICIENT,0.3694915254237288,"couldn’t have happened without Sheldon. Same goes with you guys. If Leonard
519"
HIGHEST COEFFICIENT,0.3701694915254237,"wasn’t with Penny, she never would have set you up.
520"
HIGHEST COEFFICIENT,0.3708474576271186,"PENNY : Oh, my God, Sheldon the genius is jealous of Leonard.
521"
HIGHEST COEFFICIENT,0.3715254237288136,"HOWARD : Now, I never thought I’d say this, but I’m kind of excited to see Sheldon.
522"
HIGHEST COEFFICIENT,0.3722033898305085,"AMY : This isn’t about me and Sheldon. This is about Rajesh moving in with Leonard
523"
HIGHEST COEFFICIENT,0.3728813559322034,"and Penny.
524"
HIGHEST COEFFICIENT,0.3735593220338983,"RAJ : It’s a human emotion, Sheldon. Everyone gets jealous. I’m jealous of Leonard
525"
HIGHEST COEFFICIENT,0.3742372881355932,"and Penny and Howard and Bernadette for being in such happy relationships.
526"
HIGHEST COEFFICIENT,0.3749152542372881,"LEONARD : Oh, come on. Sheldon, have you ever once heard me say that I don’t trust
527"
HIGHEST COEFFICIENT,0.37559322033898307,"Penny? Sheldon? Where did he go?
528"
HIGHEST COEFFICIENT,0.376271186440678,"PENNY : Well, yeah, he’d been living with Sheldon.
529"
HIGHEST COEFFICIENT,0.3769491525423729,"LEONARD : Really. Who do you think did that, Sheldon?
530"
HIGHEST COEFFICIENT,0.3776271186440678,"AMY : Well, I was hoping the next person I dated would be a little less like Sheldon.
531"
HIGHEST COEFFICIENT,0.3783050847457627,"6.2
PCA2
532"
LOWEST COEFFICIENT,0.37898305084745765,"6.2.1
Lowest coefficient
533"
LOWEST COEFFICIENT,0.37966101694915255,"AMY : Well, there was the time I had my tonsils out, and I shared a room with a little
534"
LOWEST COEFFICIENT,0.38033898305084746,"Vietnamese girl. She didn’t make it through the night, but up till then, it was kind
535"
LOWEST COEFFICIENT,0.38101694915254236,"of fun.
536"
LOWEST COEFFICIENT,0.38169491525423727,"BERNADETTE : Because it would make you seem like something she already thinks you
537"
LOWEST COEFFICIENT,0.3823728813559322,"are.
538"
LOWEST COEFFICIENT,0.38305084745762713,"BERNADETTE : You don’t think she’d actually send you something gross or dangerous,
539"
LOWEST COEFFICIENT,0.38372881355932204,"do you?
540"
LOWEST COEFFICIENT,0.38440677966101694,"LEONARD : Too expensive. You’d think I’d be used to women withholding their love. I
541"
LOWEST COEFFICIENT,0.38508474576271184,"mean, my mother did. I mean, no matter how hard I tried, she just didn’t have any
542"
LOWEST COEFFICIENT,0.3857627118644068,"interest in me.
543"
LOWEST COEFFICIENT,0.3864406779661017,"LEONARD : I mean, I know she’s not my girlfriend or anything, but wouldn’t you think
544"
LOWEST COEFFICIENT,0.3871186440677966,"she’d feel a little bad that I’m going to be gone for the whole summer?
545"
LOWEST COEFFICIENT,0.3877966101694915,"SHELDON : Or you might think she thinks you think it’s a date even though she doesn’t.
546"
LOWEST COEFFICIENT,0.3884745762711864,"LEONARD : Yeah, yeah, that’s the fun part. We’re also getting new curtains for my
547"
LOWEST COEFFICIENT,0.3891525423728814,"bedroom, and a dust ruffle, and a duvet, and I don’t even know what a duvet is
548"
LOWEST COEFFICIENT,0.3898305084745763,"but I’m pretty sure if I did I wouldn’t want one, but every time I talk to her about
549"
LOWEST COEFFICIENT,0.3905084745762712,"moving out she cries and we have sex.
550"
LOWEST COEFFICIENT,0.3911864406779661,"AMY : Parental pressure can be daunting. I remember the battle with my mother about
551"
LOWEST COEFFICIENT,0.391864406779661,"shaving my legs. Last year, I finally gave in and let her do it.
552"
LOWEST COEFFICIENT,0.3925423728813559,"LEONARD : Don’t you think if a woman was living with me I’d be the first one to know
553"
LOWEST COEFFICIENT,0.39322033898305087,"about it?
554"
LOWEST COEFFICIENT,0.39389830508474577,"SHELDON : I was hoping she might listen to you about the dangers of owning unhygienic
555"
LOWEST COEFFICIENT,0.3945762711864407,"furniture.
556"
HIGHEST COEFFICIENT,0.3952542372881356,"6.2.2
Highest coefficient
557"
HIGHEST COEFFICIENT,0.3959322033898305,"LEONARD : Leonard, Sheldon.
558"
HIGHEST COEFFICIENT,0.39661016949152544,"LEONARD : Hi, I’m Leonard, this is Sheldon.
559"
HIGHEST COEFFICIENT,0.39728813559322035,"HOWARD : What about Sheldon?
560"
HIGHEST COEFFICIENT,0.39796610169491525,"LEONARD : Sheldon...
561"
HIGHEST COEFFICIENT,0.39864406779661016,"LEONARD : Sheldon...
562"
HIGHEST COEFFICIENT,0.39932203389830506,"LEONARD : Sheldon...
563"
HIGHEST COEFFICIENT,0.4,"LEONARD : Sheldon...
564"
HIGHEST COEFFICIENT,0.4006779661016949,"HOWARD : Sheldon.
565"
HIGHEST COEFFICIENT,0.40135593220338983,"LEONARD : Sheldon.
566"
HIGHEST COEFFICIENT,0.40203389830508474,"HOWARD : Sheldon.
567"
HIGHEST COEFFICIENT,0.40271186440677964,"6.3
PCA3
568"
LOWEST COEFFICIENT,0.4033898305084746,"6.3.1
Lowest coefficient
569"
LOWEST COEFFICIENT,0.4040677966101695,"SHELDON : Really? I didn’t know that.
570"
LOWEST COEFFICIENT,0.4047457627118644,"PENNY : Did they make a movie about it?
571"
LOWEST COEFFICIENT,0.4054237288135593,"RAJ : How did that even happen? Did they know that’s what they were doing when they
572"
LOWEST COEFFICIENT,0.4061016949152542,"were doing it?
573"
LOWEST COEFFICIENT,0.4067796610169492,"HOWARD : Yeah, I saw it on Mythbusters.
574"
LOWEST COEFFICIENT,0.4074576271186441,"BERNADETTE : Do they have that?
575"
LOWEST COEFFICIENT,0.408135593220339,"SHELDON : A more plausible explanation is that his work in robotics has made an
576"
LOWEST COEFFICIENT,0.4088135593220339,"amazing leap forward.
577"
LOWEST COEFFICIENT,0.4094915254237288,"SHELDON : Oh. Is it true they used scuba gear to create the sound of Darth Vader
578"
LOWEST COEFFICIENT,0.4101694915254237,"breathing?
579"
LOWEST COEFFICIENT,0.41084745762711866,"HOWARD : Not exactly. They spent a ton of money developing this dandruff medication
580"
LOWEST COEFFICIENT,0.41152542372881357,"that had the side effect of horrible anal leakage.
581"
LOWEST COEFFICIENT,0.41220338983050847,"SHELDON : It’s been around for 25 years, and has been extensively corroborated by
582"
LOWEST COEFFICIENT,0.4128813559322034,"other researchers.
583"
LOWEST COEFFICIENT,0.4135593220338983,"RAJ : Did he get superpowers?
584"
HIGHEST COEFFICIENT,0.41423728813559324,"6.3.2
Highest coefficient
585"
HIGHEST COEFFICIENT,0.41491525423728814,"PENNY : Aw, sweetie, I’m comfortable around you, too.
586"
HIGHEST COEFFICIENT,0.41559322033898305,"LEONARD : Great. Just relax and enjoy. Tonight is all about you.
587"
HIGHEST COEFFICIENT,0.41627118644067795,"SHELDON : Thank you, but I’ll be fine.
588"
HIGHEST COEFFICIENT,0.41694915254237286,"PENNY : Okay, well, we’ll talk to you guys later. Bye. She said not to come. It’s gonna
589"
HIGHEST COEFFICIENT,0.4176271186440678,"be a while.
590"
HIGHEST COEFFICIENT,0.4183050847457627,"SHELDON : Fine, let’s go. Thank you for letting me sleep on your couch.
591"
HIGHEST COEFFICIENT,0.4189830508474576,"SHELDON : Oh, well, you two sit down and get to know each other. I’ll get your room
592"
HIGHEST COEFFICIENT,0.41966101694915253,"ready.
593"
HIGHEST COEFFICIENT,0.42033898305084744,"AMY : I will. I wish you were here.
594"
HIGHEST COEFFICIENT,0.4210169491525424,"LEONARD : Let’s go. Okay, you two, just, have a nice... whatever this is.
595"
HIGHEST COEFFICIENT,0.4216949152542373,"PENNY : All right. Well, you guys have fun. I guess I’ll see you Sunday night.
596"
HIGHEST COEFFICIENT,0.4223728813559322,"LEONARD : Yeah, no, I’m fine. It’s good, it’s a good party, thanks for having us, it’s just
597"
HIGHEST COEFFICIENT,0.4230508474576271,"getting a little late so....
598"
HIGHEST COEFFICIENT,0.423728813559322,"6.4
PCA4
599"
LOWEST COEFFICIENT,0.424406779661017,"6.4.1
Lowest coefficient
600"
LOWEST COEFFICIENT,0.4250847457627119,"SHELDON : Really? That seems rather short sighted, coming from someone who is
601"
LOWEST COEFFICIENT,0.4257627118644068,"generally considered altogether unlikable. Why don’t you take some time to
602"
LOWEST COEFFICIENT,0.4264406779661017,"reconsider?
603"
LOWEST COEFFICIENT,0.4271186440677966,"SHELDON : Yes, and she’s not taking my feelings into account at all. Maybe it’s time I
604"
LOWEST COEFFICIENT,0.4277966101694915,"teach her a lesson.
605"
LOWEST COEFFICIENT,0.42847457627118646,"AMY : No, we’re sorry. We never should have been comparing relationships in the first
606"
LOWEST COEFFICIENT,0.42915254237288136,"place.
607"
LOWEST COEFFICIENT,0.42983050847457627,"HOWARD : Yeah, she was dating this guy, and I was kind of a jerk to her about it.
608"
LOWEST COEFFICIENT,0.43050847457627117,"SHELDON : Yeah, but to be fair, he only said the part about him getting sick of you.
609"
LOWEST COEFFICIENT,0.4311864406779661,"SHELDON : Oh, you’re right. I could never be with a woman whose self-esteem was so
610"
LOWEST COEFFICIENT,0.43186440677966104,"low she’d be with Leonard.
611"
LOWEST COEFFICIENT,0.43254237288135594,"SHELDON : Not true. No, look at me. I had an engagement ring to give a girl, and
612"
LOWEST COEFFICIENT,0.43322033898305085,"instead, she rejected me. And am I emotional about that? No. No, I am sitting
613"
LOWEST COEFFICIENT,0.43389830508474575,"here on a couch, talking about my favourite TV character like nothing happened.
614"
LOWEST COEFFICIENT,0.43457627118644065,"‘Cause I am just like him, all logical, all the time.
615"
LOWEST COEFFICIENT,0.4352542372881356,"SHELDON : It hurts that you would lie to me, Amy. I thought our relationship was based
616"
LOWEST COEFFICIENT,0.4359322033898305,"on trust and a mutual admiration that skews in my favour.
617"
LOWEST COEFFICIENT,0.4366101694915254,"PENNY : Okay, I have not tried to change Leonard. That’s just what happens in relation-
618"
LOWEST COEFFICIENT,0.43728813559322033,"ships. Look how much Amy’s changed you.
619"
LOWEST COEFFICIENT,0.43796610169491523,"PENNY : I get that, okay? It’s just, Leonard and I have been married for two years, and
620"
LOWEST COEFFICIENT,0.4386440677966102,"we’re no further along than when we were dating.
621"
HIGHEST COEFFICIENT,0.4393220338983051,"6.4.2
Highest coefficient
622"
HIGHEST COEFFICIENT,0.44,"SHELDON : Excellent! What are you planning to wear?
623"
HIGHEST COEFFICIENT,0.4406779661016949,"HOWARD : In our new minivan. Hey, what’s for lunch?
624"
HIGHEST COEFFICIENT,0.4413559322033898,"BERNADETTE : Where are you guys going to eat?
625"
HIGHEST COEFFICIENT,0.44203389830508477,"PENNY : What beverage do you make for that?
626"
HIGHEST COEFFICIENT,0.4427118644067797,"SHELDON : Oh, I have quite the evening planned. Our foetus-friendly festival of fun
627"
HIGHEST COEFFICIENT,0.4433898305084746,"begins with an in-depth look at the world of model trains, and then we’ll kick
628"
HIGHEST COEFFICIENT,0.4440677966101695,"things up a notch and explore all the different ways that you can make toast.
629"
HIGHEST COEFFICIENT,0.4447457627118644,"LEONARD : What are you drinking there? A little eggnog?
630"
HIGHEST COEFFICIENT,0.44542372881355935,"RAJ : Sounds great!
631"
HIGHEST COEFFICIENT,0.44610169491525425,"SHELDON : In here, you’ll find emergency provisions. An eight-day supply of food and
632"
HIGHEST COEFFICIENT,0.44677966101694916,"water, a crossbow, season two of Star Trek: The Original Series on a high-density
633"
HIGHEST COEFFICIENT,0.44745762711864406,"flash drive.
634"
HIGHEST COEFFICIENT,0.44813559322033897,"AMY : I’m going to the vending machine. Do you want anything?
635"
HIGHEST COEFFICIENT,0.4488135593220339,"SHELDON : Greetings, gentlemen. How goes your little project?
636"
HIGHEST COEFFICIENT,0.44949152542372883,"6.5
PCA5
637"
LOWEST COEFFICIENT,0.45016949152542374,"6.5.1
Lowest coefficient
638"
LOWEST COEFFICIENT,0.45084745762711864,"BERNADETTE : Absolutely. All we need to do is spend a little time and find something
639"
LOWEST COEFFICIENT,0.45152542372881355,"you’re passionate about.
640"
LOWEST COEFFICIENT,0.45220338983050845,"PENNY : Okay, a simple yes will do.
641"
LOWEST COEFFICIENT,0.4528813559322034,"BERNADETTE : Of course you can. But maybe a good rule would be to wait for people
642"
LOWEST COEFFICIENT,0.4535593220338983,"to bring it up.
643"
LOWEST COEFFICIENT,0.4542372881355932,"RAJ : No, no, it’s a very promising area. In a perfect world I’d spend several more
644"
LOWEST COEFFICIENT,0.4549152542372881,"years on it. But I just couldn’t pass up the opportunity to work with you on your
645"
LOWEST COEFFICIENT,0.45559322033898303,"tremendously exciting and not yet conclusively disproved hypothesis.
646"
LOWEST COEFFICIENT,0.456271186440678,"LEONARD : Sheldon, I think this will work. Let’s just try it my way.
647"
LOWEST COEFFICIENT,0.4569491525423729,"LEONARD : If that’s what you want to do, yes.
648"
LOWEST COEFFICIENT,0.4576271186440678,"HOWARD : Yeah, this is a bad idea. We should go.
649"
LOWEST COEFFICIENT,0.4583050847457627,"AMY : Of course. I get to be part of the first team to use radon markers to map the
650"
LOWEST COEFFICIENT,0.4589830508474576,"structures that...
651"
LOWEST COEFFICIENT,0.45966101694915257,"PENNY : Yeah. And there are a few things we need to stay on top of. So we thought it
652"
LOWEST COEFFICIENT,0.4603389830508475,"would useful, and I can’t believe I’m about to say this, um.
653"
LOWEST COEFFICIENT,0.4610169491525424,"LEONARD : No, I don’t want to do it. You can do it.
654"
HIGHEST COEFFICIENT,0.4616949152542373,"6.5.2
Highest coefficient
655"
HIGHEST COEFFICIENT,0.4623728813559322,"HOWARD : How was she?
656"
HIGHEST COEFFICIENT,0.46305084745762715,"LEONARD : When was the last time you saw her?
657"
HIGHEST COEFFICIENT,0.46372881355932205,"LEONARD : How’s your mom holding up?
658"
HIGHEST COEFFICIENT,0.46440677966101696,"AMY : Oh. What was her name?
659"
HIGHEST COEFFICIENT,0.46508474576271186,"LEONARD : How’s she doing?
660"
HIGHEST COEFFICIENT,0.46576271186440676,"BERNADETTE : It was your mom.
661"
HIGHEST COEFFICIENT,0.46644067796610167,"LEONARD : Aw. What’s wrong with her?
662"
HIGHEST COEFFICIENT,0.46711864406779663,"HOWARD : My mom died.
663"
HIGHEST COEFFICIENT,0.46779661016949153,"SHELDON : What’s her name?
664"
HIGHEST COEFFICIENT,0.46847457627118644,"HOWARD : So, what is she doing today?
665"
HIGHEST COEFFICIENT,0.46915254237288134,"6.6
PCA6
666"
LOWEST COEFFICIENT,0.46983050847457625,"6.6.1
Lowest coefficient
667"
LOWEST COEFFICIENT,0.4705084745762712,"LEONARD : Relax, it wasn’t your fault.
668"
LOWEST COEFFICIENT,0.4711864406779661,"HOWARD : I’m sorry, too. It’s all my fault.
669"
LOWEST COEFFICIENT,0.471864406779661,"AMY : Well, I didn’t, and it’s your fault.
670"
LOWEST COEFFICIENT,0.4725423728813559,"PENNY : I’m sorry I yelled at you. It’s not your fault.
671"
LOWEST COEFFICIENT,0.4732203389830508,"LEONARD : It’s not your fault.
672"
LOWEST COEFFICIENT,0.4738983050847458,"AMY : It’s not your fault.
673"
LOWEST COEFFICIENT,0.4745762711864407,"SHELDON : It’s simple biology. There’s nothing I can do about it.
674"
LOWEST COEFFICIENT,0.4752542372881356,"HOWARD : Look, I have felt terrible about this for years, and I’m glad I have the
675"
LOWEST COEFFICIENT,0.4759322033898305,"opportunity to tell you just how sorry I am.
676"
LOWEST COEFFICIENT,0.4766101694915254,"LEONARD : This time, it’s your fault.
677"
LOWEST COEFFICIENT,0.47728813559322036,"LEONARD : Well, that’s not your fault.
678"
HIGHEST COEFFICIENT,0.47796610169491527,"6.6.2
Highest coefficient
679"
HIGHEST COEFFICIENT,0.4786440677966102,"LEONARD : Sure. I’d like to meet her.
680"
HIGHEST COEFFICIENT,0.4793220338983051,"LEONARD : Will Amy be joining us for dinner?
681"
HIGHEST COEFFICIENT,0.48,"BERNADETTE : Maybe, if she asks.
682"
HIGHEST COEFFICIENT,0.48067796610169494,"HOWARD : Sure she would. Ma, do you mind if Bernadette stays here this weekend?
683"
HIGHEST COEFFICIENT,0.48135593220338985,"LEONARD : No, no, of course not. Just have your relationship someplace else.
684"
HIGHEST COEFFICIENT,0.48203389830508475,"SHELDON : I’m going to find her and ask her to marry me. And if she says yes, we can
685"
HIGHEST COEFFICIENT,0.48271186440677966,"put this behind us and resume our relationship. And if she says no, well, then she
686"
HIGHEST COEFFICIENT,0.48338983050847456,"can just ponfo miran.
687"
HIGHEST COEFFICIENT,0.48406779661016947,"HOWARD : Yes!
688"
HIGHEST COEFFICIENT,0.4847457627118644,"HOWARD : Yes!
689"
HIGHEST COEFFICIENT,0.48542372881355933,"HOWARD : Yes!
690"
HIGHEST COEFFICIENT,0.48610169491525423,"HOWARD : Yes!
691"
HIGHEST COEFFICIENT,0.48677966101694914,"6.7
PCA19
692"
HIGHEST COEFFICIENT,0.48745762711864404,"6.7.1
Highest coefficient
693"
HIGHEST COEFFICIENT,0.488135593220339,"SHELDON : Yes. Oh, I’m so excited. And I just can’t hide it.
694"
HIGHEST COEFFICIENT,0.4888135593220339,"PENNY : I do, it’s just he wants to go to that party at the comic book store. A lot of the
695"
HIGHEST COEFFICIENT,0.4894915254237288,"guys that hang out there are kind of creepy.
696"
HIGHEST COEFFICIENT,0.4901694915254237,"LEONARD : Oh, I’m just trying to find the stupid next of kin to this stupid video store
697"
HIGHEST COEFFICIENT,0.4908474576271186,"owner so I can return the DVD and see the look on Sheldon’s stupid face when he
698"
HIGHEST COEFFICIENT,0.4915254237288136,"sees that I didn’t let this get to me.
699"
HIGHEST COEFFICIENT,0.4922033898305085,"HOWARD : Ooh, I want to go to the comic book store. (He leaves.)
700"
HIGHEST COEFFICIENT,0.4928813559322034,"PENNY : Yeah, but those tickets only get him into Comic-Con. That dress gets me into
701"
HIGHEST COEFFICIENT,0.4935593220338983,"anywhere I want.
702"
HIGHEST COEFFICIENT,0.4942372881355932,"PENNY : No, come on, it’s going to be fun, and you all look great, I mean, look at you,
703"
HIGHEST COEFFICIENT,0.49491525423728816,"Thor, and, oh, Peter Pan, that’s so cute.
704"
HIGHEST COEFFICIENT,0.49559322033898306,"BERNADETTE : Is it me, or is there something fun about watching him just float there?
705"
HIGHEST COEFFICIENT,0.49627118644067797,"HOWARD : Come on, Sheldon, there’s so few places I can wear my jester costume.
706"
HIGHEST COEFFICIENT,0.4969491525423729,"RAJ : So, listen to what he wrote. Uh, I saw you play at the comic book store. You guys
707"
HIGHEST COEFFICIENT,0.4976271186440678,"rock. And then there’s an animated smiley face raising the roof like this.
708"
HIGHEST COEFFICIENT,0.49830508474576274,"SHELDON : Oh no! (He is also wearing a Flash costume.)
709"
LOWEST COEFFICIENT,0.49898305084745764,"6.7.2
Lowest coefficient
710"
LOWEST COEFFICIENT,0.49966101694915255,"SHELDON : We can’t have Thai food, we had Indian for lunch.
711"
LOWEST COEFFICIENT,0.5003389830508475,"SHELDON : It was a Monday afternoon. You joined us for Indian food.
712"
LOWEST COEFFICIENT,0.5010169491525424,"SHELDON : Good morning, Friend Howard. Friend Raj. I see you gentlemen are
713"
LOWEST COEFFICIENT,0.5016949152542373,"enjoying beverages. Perhaps they would taste better out of these.
714"
LOWEST COEFFICIENT,0.5023728813559322,"RAJ : My stomach. Indian food doesn’t agree with me. Ironic, isn’t it?
715"
LOWEST COEFFICIENT,0.5030508474576271,"LEONARD : Well the only way we can play teams at this point is if we cut Raj in half.
716"
LOWEST COEFFICIENT,0.5037288135593221,"LEONARD : I’ve always been a little confused about this. Why don’t Hindus eat beef?
717"
LOWEST COEFFICIENT,0.504406779661017,"RAJ : Of course, but it’s all Indian food. You can’t find a bagel in Mumbai to save your
718"
LOWEST COEFFICIENT,0.5050847457627119,"life. Schmear me.
719"
LOWEST COEFFICIENT,0.5057627118644068,"SHELDON : Yeah, I actually have information about Raj that would be helpful with this
720"
LOWEST COEFFICIENT,0.5064406779661017,"discussion.
721"
LOWEST COEFFICIENT,0.5071186440677966,"RAJ : We Indians invented them. You’re welcome.
722"
LOWEST COEFFICIENT,0.5077966101694915,"LEONARD : Here’s an idea, why don’t we just go out for Indian food.
723"
LOWEST COEFFICIENT,0.5084745762711864,"6.8
PCA7
724"
HIGHEST COEFFICIENT,0.5091525423728813,"6.8.1
Highest coefficient
725"
HIGHEST COEFFICIENT,0.5098305084745762,"RAJ : He’s gonna be here any second, what should we do?
726"
HIGHEST COEFFICIENT,0.5105084745762711,"PENNY : What are you guys gonna do?
727"
HIGHEST COEFFICIENT,0.5111864406779661,"LEONARD : What are we gonna do?
728"
HIGHEST COEFFICIENT,0.511864406779661,"HOWARD : What are we gonna do?!
729"
HIGHEST COEFFICIENT,0.512542372881356,"AMY : What’s going on with him?
730"
HIGHEST COEFFICIENT,0.5132203389830509,"LEONARD : What are we going to do?
731"
HIGHEST COEFFICIENT,0.5138983050847458,"RAJ : So what are we going to do tonight?
732"
HIGHEST COEFFICIENT,0.5145762711864407,"LEONARD : What’s with him?
733"
HIGHEST COEFFICIENT,0.5152542372881356,"HOWARD : What’s with him?
734"
HIGHEST COEFFICIENT,0.5159322033898305,"PENNY : What’s with him?
735"
LOWEST COEFFICIENT,0.5166101694915254,"6.8.2
Lowest coefficient
736"
LOWEST COEFFICIENT,0.5172881355932203,"PENNY : Oh, Sheldon, are these letters from your grandmother?
737"
LOWEST COEFFICIENT,0.5179661016949153,"PENNY : I do, and you know, I don’t think I’ve ever thanked you properly for helping
738"
LOWEST COEFFICIENT,0.5186440677966102,"me get it.
739"
LOWEST COEFFICIENT,0.5193220338983051,"SHELDON : Oh, yes. In fact, I improved upon it.
740"
LOWEST COEFFICIENT,0.52,"SHELDON : No, of course not. No, I used trickery and deceit.
741"
LOWEST COEFFICIENT,0.5206779661016949,"LEONARD : Yeah, no, I do, I use those... uh... just to polish up my... spear-fishing
742"
LOWEST COEFFICIENT,0.5213559322033898,"equipment. I spear fish. When I’m not crossbow hunting, I spear fish. Uh, Penny,
743"
LOWEST COEFFICIENT,0.5220338983050847,"this is Sheldon’s twin sister, Missy. Missy, this is our neighbour Penny.
744"
LOWEST COEFFICIENT,0.5227118644067796,"LEONARD : Yes, I’ve always admired that about you.
745"
LOWEST COEFFICIENT,0.5233898305084745,"PENNY : She was right, you know. The locus of my identity is totally exterior to me.
746"
LOWEST COEFFICIENT,0.5240677966101694,"LEONARD : Oh, yes. Indeed, I did.
747"
LOWEST COEFFICIENT,0.5247457627118645,"LEONARD : No, no, I’m good. If my P.E. teachers had told me this is what I was training
748"
LOWEST COEFFICIENT,0.5254237288135594,"for, I would have tried a lot harder.
749"
LOWEST COEFFICIENT,0.5261016949152543,"RAJ : Do you kind of look like a shiny Sheldon?
750"
LOWEST COEFFICIENT,0.5267796610169492,"6.9
PCA15
751"
HIGHEST COEFFICIENT,0.5274576271186441,"6.9.1
Highest coefficient
752"
HIGHEST COEFFICIENT,0.528135593220339,"BERNADETTE : Yeah. You’re inviting him into your home. It’s intimate. It’s where your
753"
HIGHEST COEFFICIENT,0.5288135593220339,"underpants live.
754"
HIGHEST COEFFICIENT,0.5294915254237288,"RAJ : It’s a lease.
755"
HIGHEST COEFFICIENT,0.5301694915254237,"LEONARD : What was I supposed to do? He needed a place to sleep it off.
756"
HIGHEST COEFFICIENT,0.5308474576271186,"LEONARD : Ask him for a napkin, I dare you. (There is a knock on the door.) I’ll get it.
757"
HIGHEST COEFFICIENT,0.5315254237288135,"RAJ : He probably just goes to the bathroom.
758"
HIGHEST COEFFICIENT,0.5322033898305085,"HOWARD : Maybe the problem is he thinks you’re available. Does he know you’re
759"
HIGHEST COEFFICIENT,0.5328813559322034,"dating Sheldon?
760"
HIGHEST COEFFICIENT,0.5335593220338983,"LEONARD : What if he lives in your garage?
761"
HIGHEST COEFFICIENT,0.5342372881355932,"HOWARD : How’d you get him to come to your house?
762"
HIGHEST COEFFICIENT,0.5349152542372881,"BERNADETTE : What are you going to do? Doesn’t he know you have a boyfriend?
763"
HIGHEST COEFFICIENT,0.535593220338983,"LEONARD : He’s in his bedroom.
764"
LOWEST COEFFICIENT,0.536271186440678,"6.9.2
Lowest coefficient
765"
LOWEST COEFFICIENT,0.5369491525423729,"LEONARD : Look, do I think that you are talented and that you are beautiful? Of course I
766"
LOWEST COEFFICIENT,0.5376271186440678,"do. But isn’t Los Angeles full of actresses who are just as talented, just as beautiful?
767"
LOWEST COEFFICIENT,0.5383050847457627,"All right, look, we’ll come back to that.
768"
LOWEST COEFFICIENT,0.5389830508474577,"AMY : I do. Penny, Bernadette and I are sorry.
769"
LOWEST COEFFICIENT,0.5396610169491526,"RAJ : Oh, yes, we’ve got the moon and the trees and Elizabeth McNulty, who apparently
770"
LOWEST COEFFICIENT,0.5403389830508475,"died when she was the same age I am.
771"
LOWEST COEFFICIENT,0.5410169491525424,"SHELDON : And on a different, but not unrelated topic, based on your current efforts to
772"
LOWEST COEFFICIENT,0.5416949152542373,"buoy my spirits, do you truly believe that you were ever fit to be a cheer leader?
773"
LOWEST COEFFICIENT,0.5423728813559322,"SHELDON : Hello, female children. Allow me to inspire you with a story about a great
774"
LOWEST COEFFICIENT,0.5430508474576271,"female scientist. Polish-born, French-educated Madame Curie. Co-discoverer of
775"
LOWEST COEFFICIENT,0.543728813559322,"radioactivity, she was a hero of science, until her hair fell out, her vomit and stool
776"
LOWEST COEFFICIENT,0.5444067796610169,"became filled with blood, and she was poisoned to death by her own discovery.
777"
LOWEST COEFFICIENT,0.5450847457627118,"With a little hard work, I see no reason why that can’t happen to any of you. Are
778"
LOWEST COEFFICIENT,0.5457627118644067,"we done? Can we go?
779"
LOWEST COEFFICIENT,0.5464406779661017,"SHELDON : No, I don’t think so. Those dolls represent three things I do not care for,
780"
LOWEST COEFFICIENT,0.5471186440677966,"clowns, children and raggediness. I think it’s a lost cause.
781"
LOWEST COEFFICIENT,0.5477966101694915,"SHELDON : Yes. I think prolonged exposure to Penny has turned her into a bit of a
782"
LOWEST COEFFICIENT,0.5484745762711865,"Gabby Gertie.
783"
LOWEST COEFFICIENT,0.5491525423728814,"RAJ : Yes, isn’t she an amazing actress.
784"
LOWEST COEFFICIENT,0.5498305084745763,"SHELDON : Actually, I thought the first two renditions were far more compelling. Previ-
785"
LOWEST COEFFICIENT,0.5505084745762712,"ously I felt sympathy for the Leonard character, now I just find him to be whiny
786"
LOWEST COEFFICIENT,0.5511864406779661,"and annoying.
787"
LOWEST COEFFICIENT,0.551864406779661,"HOWARD : She was just so sad all the time. I was the only person who could cheer her
788"
LOWEST COEFFICIENT,0.5525423728813559,"up. Well, me and Ben and Jerry.
789"
LOWEST COEFFICIENT,0.5532203389830509,"6.10
PCA17
790"
HIGHEST COEFFICIENT,0.5538983050847458,"6.10.1
Highest coefficient
791"
HIGHEST COEFFICIENT,0.5545762711864407,"SHELDON : Penny, a moment. We just had Thai food. In that culture, the last morsel
792"
HIGHEST COEFFICIENT,0.5552542372881356,"is called the krengjai piece, and it is reserved for the most important and valued
793"
HIGHEST COEFFICIENT,0.5559322033898305,"member of the group.
794"
HIGHEST COEFFICIENT,0.5566101694915254,"LEONARD : Yeah, it’s delicious, the sarcasm’s a little stale, though. Hey, how about
795"
HIGHEST COEFFICIENT,0.5572881355932203,"this? Until we figure out what to do with the ring, Penny holds on to it.
796"
HIGHEST COEFFICIENT,0.5579661016949152,"PENNY : Okay, sweetie, I don’t know if we’re gonna have cookies, or he’s just gonna
797"
HIGHEST COEFFICIENT,0.5586440677966101,"say hi, or really what’s gonna happen, so just let me talk, and we’ll...
798"
HIGHEST COEFFICIENT,0.559322033898305,"PENNY : Fine. What do you want?
799"
HIGHEST COEFFICIENT,0.56,"HOWARD : Okay, this one is for a Cadbury Creme Egg.
800"
HIGHEST COEFFICIENT,0.560677966101695,"LEONARD : Ah, well, what’s this? A pot of oatmeal? Or, thanks to you, what I will now
801"
HIGHEST COEFFICIENT,0.5613559322033899,"call gloatmeal.
802"
HIGHEST COEFFICIENT,0.5620338983050848,"SHELDON : I’m sorry, but these are just ordinary foods with the names bent into tortured
803"
HIGHEST COEFFICIENT,0.5627118644067797,"puns. The dishes themselves are in no way Halloweenie.
804"
HIGHEST COEFFICIENT,0.5633898305084746,"LEONARD : Ah, that’s a good question. Apparently someone was being awfully flirty
805"
HIGHEST COEFFICIENT,0.5640677966101695,"while not wearing their engagement ring, causing another someone to show up
806"
HIGHEST COEFFICIENT,0.5647457627118644,"here thinking the first someone might be available.
807"
HIGHEST COEFFICIENT,0.5654237288135593,"PENNY : Okay, well, I’d offer you Halloween candy, but that’s gone. So, what’s up?
808"
HIGHEST COEFFICIENT,0.5661016949152542,"RAJ : Okay. Shall we? Oh, my God. It’s light, it’s flaky, it’s buttery. You don’t need to
809"
HIGHEST COEFFICIENT,0.5667796610169491,"have sex with him, just eat one of these.
810"
LOWEST COEFFICIENT,0.5674576271186441,"6.10.2
Lowest coefficient
811"
LOWEST COEFFICIENT,0.568135593220339,"RAJ : Then she’s going to have to convince your mother to let you go into space.
812"
LOWEST COEFFICIENT,0.5688135593220339,"HOWARD : Then get out of my house.
813"
LOWEST COEFFICIENT,0.5694915254237288,"BERNADETTE : Yeah, if you want to go off the grid, you have to move out of your
814"
LOWEST COEFFICIENT,0.5701694915254237,"mother’s house.
815"
LOWEST COEFFICIENT,0.5708474576271186,"SHELDON : I can’t believe my own mother is abandoning me.
816"
LOWEST COEFFICIENT,0.5715254237288135,"HOWARD : I will. I’m obviously not going to live in my mother’s house for the rest of
817"
LOWEST COEFFICIENT,0.5722033898305084,"my life. I’m not a child.
818"
LOWEST COEFFICIENT,0.5728813559322034,"LEONARD : With your career?
819"
LOWEST COEFFICIENT,0.5735593220338983,"BERNADETTE : You’re a real hero, Howard.
820"
LOWEST COEFFICIENT,0.5742372881355933,"BERNADETTE : I’m proud of her. This is a great opportunity. It’s nice to see her take it
821"
LOWEST COEFFICIENT,0.5749152542372882,"seriously.
822"
LOWEST COEFFICIENT,0.5755932203389831,"LEONARD : Also instead of just living in your mother’s house, you could actually live
823"
LOWEST COEFFICIENT,0.576271186440678,"inside her body.
824"
LOWEST COEFFICIENT,0.5769491525423729,"LEONARD : And now you’re also an astronaut.
825"
LOWEST COEFFICIENT,0.5776271186440678,"7
Annex 3: Dialogue example of Friends
826"
LOWEST COEFFICIENT,0.5783050847457627,"7.1
PCA1
827"
HIGHEST COEFFICIENT,0.5789830508474576,"7.1.1
Highest coefficient
828"
HIGHEST COEFFICIENT,0.5796610169491525,"CHANDLER : Hey.
829"
HIGHEST COEFFICIENT,0.5803389830508474,"CHANDLER : Hey.
830"
HIGHEST COEFFICIENT,0.5810169491525424,"PHOEBE : Hey.
831"
HIGHEST COEFFICIENT,0.5816949152542373,"RACHEL : Hey.
832"
HIGHEST COEFFICIENT,0.5823728813559322,"ROSS : Hey.
833"
HIGHEST COEFFICIENT,0.5830508474576271,"MONICA : Hey.
834"
HIGHEST COEFFICIENT,0.583728813559322,"RACHEL : Hey.
835"
HIGHEST COEFFICIENT,0.584406779661017,"RACHEL : Hey.
836"
HIGHEST COEFFICIENT,0.5850847457627119,"CHANDLER : Hey.
837"
HIGHEST COEFFICIENT,0.5857627118644068,"ROSS : Hey.
838"
LOWEST COEFFICIENT,0.5864406779661017,"7.1.2
Lowest coefficient
839"
LOWEST COEFFICIENT,0.5871186440677966,"RACHEL : Yeah. It’s just gonna be too hard. Y’know? I mean, it’s Ross. How can
840"
LOWEST COEFFICIENT,0.5877966101694915,"I watch him get married? Y’know it’s just, it’s for the best, y’know it is, it’s...
841"
LOWEST COEFFICIENT,0.5884745762711865,"Y’know, plus, somebody’s got to stay here with Phoebe! Y’know she’s gonna be
842"
LOWEST COEFFICIENT,0.5891525423728814,"pretty big by then, and she needs someone to help her tie her shoes; drive her to the
843"
LOWEST COEFFICIENT,0.5898305084745763,"hospital in case she goes into labour.
844"
LOWEST COEFFICIENT,0.5905084745762712,"RACHEL : Ross, you know what? She may need one..We’re just going to have to make
845"
LOWEST COEFFICIENT,0.5911864406779661,"our peace with that! Monica and Chandler’s apartment.
846"
LOWEST COEFFICIENT,0.591864406779661,"JOEY : Look we’ve got to find her. Phoebe just called!! Rachel’s coming to tell Ross
847"
LOWEST COEFFICIENT,0.5925423728813559,"she loves him!!
848"
LOWEST COEFFICIENT,0.5932203389830508,"CHANDLER : Well, she’s just so much fun with Joey, I just assumed, she’d still be living
849"
LOWEST COEFFICIENT,0.5938983050847457,"with him.
850"
LOWEST COEFFICIENT,0.5945762711864406,"JOEY : Well, remember when they got in that big fight and broke up and we were all
851"
LOWEST COEFFICIENT,0.5952542372881356,"stuck in her with no food or anything? Well, when Ross said Rachel at the wedding,
852"
LOWEST COEFFICIENT,0.5959322033898306,"I figured it was gonna happen again, so I hid this in here.
853"
LOWEST COEFFICIENT,0.5966101694915255,"MONICA : I can’t believe this. Rachel and Joey?
854"
LOWEST COEFFICIENT,0.5972881355932204,"RACHEL : Look Monica, getting cold feet is very common. Y’know, it’s-it’s just because
855"
LOWEST COEFFICIENT,0.5979661016949153,"of all the anticipation and you just have to remember that you love Chandler. And
856"
LOWEST COEFFICIENT,0.5986440677966102,"also, I ran out on a wedding. You don’t get to keep the gifts.
857"
LOWEST COEFFICIENT,0.5993220338983051,"MONICA : No, look, she’s obviously unstable, okay? I mean she’s thinking about
858"
LOWEST COEFFICIENT,0.6,"running out on her wedding day. Okay, fine! But I mean, look at the position she’s
859"
LOWEST COEFFICIENT,0.6006779661016949,"putting him in! What’s he gonna do? Ross is gonna run over there on the wedding
860"
LOWEST COEFFICIENT,0.6013559322033898,"day and break up the marriage?! I mean, who would do that?! Okay, fine, all right,
861"
LOWEST COEFFICIENT,0.6020338983050847,"but that’s y’know, it’s different! Although it did involve a lot of the same people.
862"
LOWEST COEFFICIENT,0.6027118644067797,"PHOEBE : Why do you think, she’s having so much fun living with Joey?
863"
LOWEST COEFFICIENT,0.6033898305084746,"PHOEBE : It’s so weird seeing Ross and Rachel with a baby. It’s just so grown up.
864"
LOWEST COEFFICIENT,0.6040677966101695,"7.2
PCA2
865"
HIGHEST COEFFICIENT,0.6047457627118644,"7.2.1
Highest coefficient
866"
HIGHEST COEFFICIENT,0.6054237288135593,"RACHEL : Hey! Hi!
867"
HIGHEST COEFFICIENT,0.6061016949152542,"RACHEL : Hey! Hi!
868"
HIGHEST COEFFICIENT,0.6067796610169491,"ROSS : Hey! Hi!
869"
HIGHEST COEFFICIENT,0.607457627118644,"PHOEBE : Hey! Hi!
870"
HIGHEST COEFFICIENT,0.6081355932203389,"RACHEL : Hey! We’re here!
871"
HIGHEST COEFFICIENT,0.6088135593220338,"RACHEL : Hi!!
872"
HIGHEST COEFFICIENT,0.6094915254237289,"RACHEL : Hi!!
873"
HIGHEST COEFFICIENT,0.6101694915254238,"MONICA : Hi!
874"
HIGHEST COEFFICIENT,0.6108474576271187,"MONICA : Hi!
875"
HIGHEST COEFFICIENT,0.6115254237288136,"MONICA : Hi!
876"
LOWEST COEFFICIENT,0.6122033898305085,"7.2.2
Lowest coefficient
877"
LOWEST COEFFICIENT,0.6128813559322034,"RACHEL : Yeah, fair enough.
878"
LOWEST COEFFICIENT,0.6135593220338983,"RACHEL : Really? You think so?
879"
LOWEST COEFFICIENT,0.6142372881355932,"PHOEBE : Really? You think?
880"
LOWEST COEFFICIENT,0.6149152542372881,"PHOEBE : Yeah, what’s your point?
881"
LOWEST COEFFICIENT,0.615593220338983,"PHOEBE : Yeah, but not just that.
882"
LOWEST COEFFICIENT,0.616271186440678,"RACHEL : No, you’re right, you are absolutely right. I mean that makes, that makes
883"
LOWEST COEFFICIENT,0.6169491525423729,"everything different.
884"
LOWEST COEFFICIENT,0.6176271186440678,"JOEY : No. Really?
885"
LOWEST COEFFICIENT,0.6183050847457627,"ROSS : Really? Its not just frowned upon?
886"
LOWEST COEFFICIENT,0.6189830508474576,"JOEY : Yeah, I wouldn’t know about that.
887"
LOWEST COEFFICIENT,0.6196610169491525,"CHANDLER : Yeah, you’re right about that.
888"
LOWEST COEFFICIENT,0.6203389830508474,"7.3
PCA3
889"
HIGHEST COEFFICIENT,0.6210169491525424,"7.3.1
Highest coefficient
890"
HIGHEST COEFFICIENT,0.6216949152542373,"CHANDLER : Hi! I’m back. Yeah, that sounds great. Okay. Well, we’ll do it then. Okay,
891"
HIGHEST COEFFICIENT,0.6223728813559322,"bye-bye.
892"
HIGHEST COEFFICIENT,0.6230508474576271,"ROSS : I’ll do it. Hey, whatever you need me to do, I’m your man. Whoa-oh-whoa! Are
893"
HIGHEST COEFFICIENT,0.6237288135593221,"you, are you okay?
894"
HIGHEST COEFFICIENT,0.624406779661017,"RACHEL : No, come on, I’m totally ok. I don’t need you to come! I can totally handle
895"
HIGHEST COEFFICIENT,0.6250847457627119,"this on my own.
896"
HIGHEST COEFFICIENT,0.6257627118644068,"ROSS : I’ll help you. Yeah, I’ll make up a schedule and make sure you stick to it. And
897"
HIGHEST COEFFICIENT,0.6264406779661017,"plus, it’ll give me something to do.
898"
HIGHEST COEFFICIENT,0.6271186440677966,"JOEY : Alright, alright. I’m around. Go ahead.
899"
HIGHEST COEFFICIENT,0.6277966101694915,"PHOEBE : Anyway, I should go. Okay, bye.
900"
HIGHEST COEFFICIENT,0.6284745762711864,"MONICA : Ok first of all...It would be great. But that’s not what I’m here to talk to you
901"
HIGHEST COEFFICIENT,0.6291525423728813,"about. I need to borrow some money.
902"
HIGHEST COEFFICIENT,0.6298305084745762,"MONICA : No, I’ll do it. You just stick to your job.
903"
HIGHEST COEFFICIENT,0.6305084745762712,"ROSS : Oh, that’d be great! Okay, but if you do, make sure it seems like you’re there to
904"
HIGHEST COEFFICIENT,0.6311864406779661,"see him, okay, and you’re not like doing it as a favour to me.
905"
HIGHEST COEFFICIENT,0.631864406779661,"JOEY : Sure, yeah. I don’t have time to say thank you because I really gotta go.
906"
LOWEST COEFFICIENT,0.632542372881356,"7.3.2
Lowest coefficient
907"
LOWEST COEFFICIENT,0.6332203389830509,"RACHEL : Phoebe?! Wait a-but-but she just, she said that Joey was her backup.
908"
LOWEST COEFFICIENT,0.6338983050847458,"MONICA : They thought Joey was a child?
909"
LOWEST COEFFICIENT,0.6345762711864407,"CHANDLER : And then Joey remembered something.
910"
LOWEST COEFFICIENT,0.6352542372881356,"RACHEL : I thought it was Chandler!
911"
LOWEST COEFFICIENT,0.6359322033898305,"MONICA : Does it have to do with Joey?
912"
LOWEST COEFFICIENT,0.6366101694915254,"RACHEL : Joey! Why did you tell Chandler that Monica was getting a boob job?
913"
LOWEST COEFFICIENT,0.6372881355932203,"MONICA : And Rachel. And that’s Chandler.
914"
LOWEST COEFFICIENT,0.6379661016949153,"RACHEL : And that’s Phoebe , and that’s Joey.
915"
LOWEST COEFFICIENT,0.6386440677966102,"RACHEL : And that’s Phoebe , and that’s Joey.
916"
LOWEST COEFFICIENT,0.6393220338983051,"ROSS : Phoebe that’s not true.
917"
LOWEST COEFFICIENT,0.64,"7.4
PCA4
918"
HIGHEST COEFFICIENT,0.6406779661016949,"7.4.1
Highest coefficient
919"
HIGHEST COEFFICIENT,0.6413559322033898,"ROSS : Well it’s okay. Chandler is talking to her.
920"
HIGHEST COEFFICIENT,0.6420338983050847,"JOEY : I said a little bit Ross. Now, how about you Chandler?
921"
HIGHEST COEFFICIENT,0.6427118644067796,"JOEY : Okay. I’m Chandler
922"
HIGHEST COEFFICIENT,0.6433898305084745,"JOEY : Hey look Ross, you need to understand something okay? I uh...I am never gonna
923"
HIGHEST COEFFICIENT,0.6440677966101694,"act on this Rachel thing, okay? I-I would never do anything to jeopardize my
924"
HIGHEST COEFFICIENT,0.6447457627118645,"friendship with you.
925"
HIGHEST COEFFICIENT,0.6454237288135594,"JOEY : It’s okay, Ross, alright? I totally understand. Of course you’re not fine. You’re..
926"
HIGHEST COEFFICIENT,0.6461016949152543,"You’re Ross and Rachel.
927"
HIGHEST COEFFICIENT,0.6467796610169492,"JOEY : I’m fine, I’m fine, it’s just, it’s just weird what’s happening with her and Ross.
928"
HIGHEST COEFFICIENT,0.6474576271186441,"You know, yesterday he asked me to fix him up with somebody.
929"
HIGHEST COEFFICIENT,0.648135593220339,"RACHEL : All right. So you’re telling me that there is nothing going on between you
930"
HIGHEST COEFFICIENT,0.6488135593220339,"and Chandler.
931"
HIGHEST COEFFICIENT,0.6494915254237288,"ROSS : Fine, fine, Rachel your with Monica, Joey you’re with me.
932"
HIGHEST COEFFICIENT,0.6501694915254237,"PHOEBE : Okay. Oh umm, Chandler, Monica is looking for you.
933"
HIGHEST COEFFICIENT,0.6508474576271186,"ROSS : Umm, okay, yeah, sure. But wh-what’s wrong with Monica and Chandler?
934"
LOWEST COEFFICIENT,0.6515254237288136,"7.4.2
Lowest coefficient
935"
LOWEST COEFFICIENT,0.6522033898305085,"MONICA : What?! What is it?!
936"
LOWEST COEFFICIENT,0.6528813559322034,"MONICA : Oh my God! I love that!
937"
LOWEST COEFFICIENT,0.6535593220338983,"JOEY : What the hell is that?!!
938"
LOWEST COEFFICIENT,0.6542372881355932,"JOEY : What the hell!
939"
LOWEST COEFFICIENT,0.6549152542372881,"ROSS : What?! It is?!
940"
LOWEST COEFFICIENT,0.655593220338983,"RACHEL : Oh my God! That’s the creepiest thing I’ve ever heard!
941"
LOWEST COEFFICIENT,0.656271186440678,"RACHEL : Oh my God! Look at this!
942"
LOWEST COEFFICIENT,0.6569491525423728,"MONICA : What?! What is it?
943"
LOWEST COEFFICIENT,0.6576271186440678,"ROSS : I can’t believe this!!
944"
LOWEST COEFFICIENT,0.6583050847457627,"JOEY : What?! What?!! What is it?!
945"
LOWEST COEFFICIENT,0.6589830508474577,"7.5
PCA5
946"
HIGHEST COEFFICIENT,0.6596610169491526,"7.5.1
Highest coefficient
947"
HIGHEST COEFFICIENT,0.6603389830508475,"RACHEL : Yeah! That would be great!
948"
HIGHEST COEFFICIENT,0.6610169491525424,"MONICA : Yeah, that’d be great! Thank you!
949"
HIGHEST COEFFICIENT,0.6616949152542373,"JOEY : Yeah! Yeah! That would be very helpful! Yeah.
950"
HIGHEST COEFFICIENT,0.6623728813559322,"CHANDLER : All right, ready?
951"
HIGHEST COEFFICIENT,0.6630508474576271,"ROSS : All right, ready?
952"
HIGHEST COEFFICIENT,0.663728813559322,"CHANDLER : All right, ready?
953"
HIGHEST COEFFICIENT,0.6644067796610169,"PHOEBE : All right, ready?
954"
HIGHEST COEFFICIENT,0.6650847457627118,"MONICA : All right, you ready?
955"
HIGHEST COEFFICIENT,0.6657627118644068,"PHOEBE : Sure, yeah!
956"
HIGHEST COEFFICIENT,0.6664406779661017,"JOEY : Sure. Yep.
957"
LOWEST COEFFICIENT,0.6671186440677966,"7.5.2
Lowest coefficient
958"
LOWEST COEFFICIENT,0.6677966101694915,"PHOEBE : But, also, what happened between you and your Mom?
959"
LOWEST COEFFICIENT,0.6684745762711864,"JOEY : She was nothing compared to you.
960"
LOWEST COEFFICIENT,0.6691525423728814,"JOEY : She was nothing compared to you.
961"
LOWEST COEFFICIENT,0.6698305084745763,"CHANDLER : Hey that’s what I tell girls about me.
962"
LOWEST COEFFICIENT,0.6705084745762712,"JOEY : Me too. I mean I...haven’t thought at all about how I put myself out there and
963"
LOWEST COEFFICIENT,0.6711864406779661,"said all that stuff and how you didn’t feel the same way about me and-and how it
964"
LOWEST COEFFICIENT,0.671864406779661,"was really awkward.
965"
LOWEST COEFFICIENT,0.672542372881356,"ROSS : Well, well I am married. Even though I haven’t spoken to my wife since the
966"
LOWEST COEFFICIENT,0.6732203389830509,"wedding.
967"
LOWEST COEFFICIENT,0.6738983050847458,"PHOEBE : Oh, because, you know... they don’t like you.
968"
LOWEST COEFFICIENT,0.6745762711864407,"MONICA : Well, um, because mainly, um, they don’t like you. I’m sorry.
969"
LOWEST COEFFICIENT,0.6752542372881356,"CHANDLER : Well it couldn’t have been worse. A woman literally passed through me.
970"
LOWEST COEFFICIENT,0.6759322033898305,"OK, so what is it, am I hideously unattractive?
971"
LOWEST COEFFICIENT,0.6766101694915254,"ROSS : Hey, whatever it is, I am sure it has happened to me. Y’know, actually once-once
972"
LOWEST COEFFICIENT,0.6772881355932203,"I got dumped during sex.
973"
LOWEST COEFFICIENT,0.6779661016949152,"7.6
PCA6
974"
HIGHEST COEFFICIENT,0.6786440677966101,"7.6.1
Highest coefficient
975"
HIGHEST COEFFICIENT,0.679322033898305,"ROSS : Yes! We’re getting married?!
976"
HIGHEST COEFFICIENT,0.68,"JOEY : No! No, and I did not ask her to marry me!
977"
HIGHEST COEFFICIENT,0.680677966101695,"ROSS : N-no! Okay? We’ve been through this! We’re not gonna get married just because
978"
HIGHEST COEFFICIENT,0.6813559322033899,"she’s pregnant, okay?
979"
HIGHEST COEFFICIENT,0.6820338983050848,"JOEY : Well all right then, I guess I shouldn’t get to excited about the fact that I just
980"
HIGHEST COEFFICIENT,0.6827118644067797,"kissed her!
981"
HIGHEST COEFFICIENT,0.6833898305084746,"CHANDLER : OH...MY...GAWD! I am so sorry sweetie, are you okay? You didn’t tell
982"
HIGHEST COEFFICIENT,0.6840677966101695,"her we were getting married, did you?
983"
HIGHEST COEFFICIENT,0.6847457627118644,"ROSS : Hey! I offered to marry her!
984"
HIGHEST COEFFICIENT,0.6854237288135593,"CHANDLER : How can I not be upset? Okay? I finally fall in love with this fantastic
985"
HIGHEST COEFFICIENT,0.6861016949152542,"woman and it turns out that she wanted you first!
986"
HIGHEST COEFFICIENT,0.6867796610169492,"PHOEBE : You’re still gonna go out with her?!
987"
HIGHEST COEFFICIENT,0.6874576271186441,"ROSS : Yeah? Oh-oh, she’d be so excited!
988"
HIGHEST COEFFICIENT,0.688135593220339,"ROSS : Okay. I did divert her and we ended up having a great time! Okay?
989"
LOWEST COEFFICIENT,0.6888135593220339,"7.6.2
Lowest coefficient
990"
LOWEST COEFFICIENT,0.6894915254237288,"PHOEBE : Wait a minute. What’s his name?
991"
LOWEST COEFFICIENT,0.6901694915254237,"MONICA : Hey. It’s him. Who is it?
992"
LOWEST COEFFICIENT,0.6908474576271186,"MONICA : Nothing, I don’t know.
993"
LOWEST COEFFICIENT,0.6915254237288135,"JOEY : Seriously, who is this guy?
994"
LOWEST COEFFICIENT,0.6922033898305084,"JOEY : Who the hell is this guy?
995"
LOWEST COEFFICIENT,0.6928813559322033,"RACHEL : Who are these men?
996"
LOWEST COEFFICIENT,0.6935593220338983,"PHOEBE : Come on, give me something. What’s his name?
997"
LOWEST COEFFICIENT,0.6942372881355933,"CHANDLER : There’s the man.
998"
LOWEST COEFFICIENT,0.6949152542372882,"MONICA : Who, who are they?
999"
LOWEST COEFFICIENT,0.6955932203389831,"ROSS : C’mon, what’s his name?
1000"
LOWEST COEFFICIENT,0.696271186440678,"7.7
PCA9
1001"
HIGHEST COEFFICIENT,0.6969491525423729,"7.7.1
Highest coefficient
1002"
HIGHEST COEFFICIENT,0.6976271186440678,"CHANDLER : What are you guys doing together?
1003"
HIGHEST COEFFICIENT,0.6983050847457627,"RACHEL : So what are you guys going to do?
1004"
HIGHEST COEFFICIENT,0.6989830508474576,"ROSS : What are you guys doing later?
1005"
HIGHEST COEFFICIENT,0.6996610169491525,"MONICA : So, what have you guys been doing?
1006"
HIGHEST COEFFICIENT,0.7003389830508474,"ROSS : Well, I’m gonna go see her. I want to bring her something, what do you think
1007"
HIGHEST COEFFICIENT,0.7010169491525424,"she’ll like?
1008"
HIGHEST COEFFICIENT,0.7016949152542373,"MONICA : What are you guys gonna do?
1009"
HIGHEST COEFFICIENT,0.7023728813559322,"ROSS : So uh, any ideas for the bachelor party yet?
1010"
HIGHEST COEFFICIENT,0.7030508474576271,"RACHEL : What’re you guys doing out here?
1011"
HIGHEST COEFFICIENT,0.703728813559322,"ROSS : Hey, what have you guys been up to?
1012"
HIGHEST COEFFICIENT,0.704406779661017,"RACHEL : Hey, what have you guys been up to?
1013"
LOWEST COEFFICIENT,0.7050847457627119,"7.7.2
Lowest coefficient
1014"
LOWEST COEFFICIENT,0.7057627118644068,"PHOEBE : Oh, okay, oh.
1015"
LOWEST COEFFICIENT,0.7064406779661017,"ROSS : Oh. Oh! Oh my God! Okay, I know this, give me-give me a second!
1016"
LOWEST COEFFICIENT,0.7071186440677966,"PHOEBE : All right-Ooh! Oh dead God, save me!
1017"
LOWEST COEFFICIENT,0.7077966101694916,"RACHEL : Oh-oh, sorry, it’s this way, it’s this way.
1018"
LOWEST COEFFICIENT,0.7084745762711865,"RACHEL : Oh, okay!
1019"
LOWEST COEFFICIENT,0.7091525423728814,"CHANDLER : Oh, okay!
1020"
LOWEST COEFFICIENT,0.7098305084745763,"RACHEL : Oh, okay!
1021"
LOWEST COEFFICIENT,0.7105084745762712,"MONICA : Oh, okay!
1022"
LOWEST COEFFICIENT,0.7111864406779661,"ROSS : Oh, you’re right, I’m sorry.
1023"
LOWEST COEFFICIENT,0.711864406779661,"JOEY : Oh, oh, oh, sorry.
1024"
LOWEST COEFFICIENT,0.7125423728813559,"7.8
PCA18
1025"
HIGHEST COEFFICIENT,0.7132203389830508,"7.8.1
Highest coefficient
1026"
HIGHEST COEFFICIENT,0.7138983050847457,"JOEY : Yeah, he did, look... look, it’s right there on the counter! Ha-ho-ho!
1027"
HIGHEST COEFFICIENT,0.7145762711864406,"CHANDLER : Okay, did you see that?! With the inappropriate and the pinching!!
1028"
HIGHEST COEFFICIENT,0.7152542372881356,"CHANDLER : Okay, did you see that?! With the inappropriate and the pinching!!
1029"
HIGHEST COEFFICIENT,0.7159322033898305,"JOEY : Hey! Handcuffs! And fur line, nice! I didn’t know you guys had it in ya!
1030"
HIGHEST COEFFICIENT,0.7166101694915255,"JOEY : Look, it was a job all right?
1031"
HIGHEST COEFFICIENT,0.7172881355932204,"CHANDLER : Look! Look! Look what the... Look what... Look what the floating heads
1032"
HIGHEST COEFFICIENT,0.7179661016949153,"did!
1033"
HIGHEST COEFFICIENT,0.7186440677966102,"ROSS : Okay, there was some staring and pointing.
1034"
HIGHEST COEFFICIENT,0.7193220338983051,"MONICA : Yeah, yeah, it’s interesting.. but y’know what? Just for fun, let’s see what it
1035"
HIGHEST COEFFICIENT,0.72,"looked like in the old spot. Alright, just to compare. Let’s see. Well, it looks good
1036"
HIGHEST COEFFICIENT,0.7206779661016949,"there too. Let’s just leave it there for a while.
1037"
HIGHEST COEFFICIENT,0.7213559322033898,"JOEY : Uh, take a look at the guy’s pants! I mean, I know you told us to show excitement,
1038"
HIGHEST COEFFICIENT,0.7220338983050848,"but don’t you think he went a little overboard?
1039"
HIGHEST COEFFICIENT,0.7227118644067797,"RACHEL : Yeah, he did! Oh, see, this is what I’m talking about!
1040"
LOWEST COEFFICIENT,0.7233898305084746,"7.8.2
Lowest coefficient
1041"
LOWEST COEFFICIENT,0.7240677966101695,"RACHEL : Oh no.
1042"
LOWEST COEFFICIENT,0.7247457627118644,"PHOEBE : Oh no.
1043"
LOWEST COEFFICIENT,0.7254237288135593,"PHOEBE : Oh no.
1044"
LOWEST COEFFICIENT,0.7261016949152542,"RACHEL : Oh no.
1045"
LOWEST COEFFICIENT,0.7267796610169491,"CHANDLER : Oh no.
1046"
LOWEST COEFFICIENT,0.727457627118644,"PHOEBE : Oh no.
1047"
LOWEST COEFFICIENT,0.7281355932203389,"ROSS : Oh no.
1048"
LOWEST COEFFICIENT,0.7288135593220338,"PHOEBE : Oh no.
1049"
LOWEST COEFFICIENT,0.7294915254237289,"PHOEBE : Oh no.
1050"
LOWEST COEFFICIENT,0.7301694915254238,"ROSS : Oh no.
1051"
LOWEST COEFFICIENT,0.7308474576271187,"7.9
PCA7
1052"
HIGHEST COEFFICIENT,0.7315254237288136,"7.9.1
Highest coefficient
1053"
HIGHEST COEFFICIENT,0.7322033898305085,"CHANDLER : What? What?
1054"
HIGHEST COEFFICIENT,0.7328813559322034,"CHANDLER : What? What?
1055"
HIGHEST COEFFICIENT,0.7335593220338983,"ROSS : What? What?
1056"
HIGHEST COEFFICIENT,0.7342372881355932,"ROSS : What? What?
1057"
HIGHEST COEFFICIENT,0.7349152542372881,"PHOEBE : What? What?
1058"
HIGHEST COEFFICIENT,0.735593220338983,"ROSS : What? What?
1059"
HIGHEST COEFFICIENT,0.736271186440678,"JOEY : What? What?
1060"
HIGHEST COEFFICIENT,0.7369491525423729,"ROSS : What? What?
1061"
HIGHEST COEFFICIENT,0.7376271186440678,"ROSS : What? What?
1062"
HIGHEST COEFFICIENT,0.7383050847457627,"MONICA : What?
1063"
LOWEST COEFFICIENT,0.7389830508474576,"7.9.2
Lowest coefficient
1064"
LOWEST COEFFICIENT,0.7396610169491525,"JOEY : Yeah, yeah, I met this woman.
1065"
LOWEST COEFFICIENT,0.7403389830508474,"MONICA : Yes but my mom got me this job.
1066"
LOWEST COEFFICIENT,0.7410169491525423,"PHOEBE : Yes, yes I do. God, oh it’s just perfect! Wow! I bet it has a great story behind
1067"
LOWEST COEFFICIENT,0.7416949152542373,"it too. Did they tell you anything? Like y’know where it was from or...
1068"
LOWEST COEFFICIENT,0.7423728813559322,"PHOEBE : No, not usually. But yeah, I could use one right now.
1069"
LOWEST COEFFICIENT,0.7430508474576272,"PHOEBE : Yeah, kinda.
1070"
LOWEST COEFFICIENT,0.7437288135593221,"MONICA : Yeah, just like the one in the poem.
1071"
LOWEST COEFFICIENT,0.744406779661017,"CHANDLER : Yes, money well spent!
1072"
LOWEST COEFFICIENT,0.7450847457627119,"PHOEBE : No! But it’s the nicest kitchen, the refrigerator told me to have a great day.
1073"
LOWEST COEFFICIENT,0.7457627118644068,"CHANDLER : Yeah, I remember.
1074"
LOWEST COEFFICIENT,0.7464406779661017,"MONICA : No. But I remember people telling me about it.
1075"
LOWEST COEFFICIENT,0.7471186440677966,"7.10
PCA17
1076"
HIGHEST COEFFICIENT,0.7477966101694915,"7.10.1
Highest coefficient
1077"
HIGHEST COEFFICIENT,0.7484745762711864,"RACHEL : And um, what-what is that Ross?
1078"
HIGHEST COEFFICIENT,0.7491525423728813,"RACHEL : Ross’s what?
1079"
HIGHEST COEFFICIENT,0.7498305084745762,"RACHEL : Ok, Ross, Ross, ok listen, what we have is amazing.
1080"
HIGHEST COEFFICIENT,0.7505084745762712,"CHANDLER : Oh, that’s Ross’s.
1081"
HIGHEST COEFFICIENT,0.7511864406779661,"CHANDLER : Oh, that’s Ross’s.
1082"
HIGHEST COEFFICIENT,0.751864406779661,"RACHEL : Ross, I...
1083"
HIGHEST COEFFICIENT,0.752542372881356,"RACHEL : For Ross, Ross, Ross.
1084"
HIGHEST COEFFICIENT,0.7532203389830509,"RACHEL : Well-well, I don’t know Ross-really?
1085"
HIGHEST COEFFICIENT,0.7538983050847458,"RACHEL : Well-well, I don’t know Ross-really?
1086"
HIGHEST COEFFICIENT,0.7545762711864407,"RACHEL : Um... Ross?
1087"
LOWEST COEFFICIENT,0.7552542372881356,"7.10.2
Lowest coefficient
1088"
LOWEST COEFFICIENT,0.7559322033898305,"MONICA : Hey, Joey, I don’t think that you should leave Chandler alone. I mean it’s
1089"
LOWEST COEFFICIENT,0.7566101694915254,"only been two days since he broke up with Kathy. Maybe you can go fishing next
1090"
LOWEST COEFFICIENT,0.7572881355932204,"week?
1091"
LOWEST COEFFICIENT,0.7579661016949153,"JOEY : Chandler, you have to start getting over her. All right, if you play, you get some
1092"
LOWEST COEFFICIENT,0.7586440677966102,"fresh air, maybe it’ll take your mind off Janice, and if you don’t play, everyone will
1093"
LOWEST COEFFICIENT,0.7593220338983051,"be mad at you ’cause the teams won’t be even. Come on.
1094"
LOWEST COEFFICIENT,0.76,"PHOEBE : Joey? How could you just let them leave?
1095"
LOWEST COEFFICIENT,0.7606779661016949,"CHANDLER : Look, Joey, Kathy is clearly not fulfilling your emotional needs. But
1096"
LOWEST COEFFICIENT,0.7613559322033898,"Casey, I mean granted I only saw the back of her head, but I got this sense that
1097"
LOWEST COEFFICIENT,0.7620338983050847,"she’s-she’s smart, and funny, and gets you.
1098"
LOWEST COEFFICIENT,0.7627118644067796,"MONICA : Wait a minute...Joey. Joey you can’t ask her out, she’s your roommate. It-it’ll
1099"
LOWEST COEFFICIENT,0.7633898305084745,"be way too complicated.
1100"
LOWEST COEFFICIENT,0.7640677966101694,"PHOEBE : Okay, but try and get Joey too.
1101"
LOWEST COEFFICIENT,0.7647457627118645,"ROSS : No Joey! Look why don’t, why don’t we just let her decide? Okay? Hey-hey,
1102"
LOWEST COEFFICIENT,0.7654237288135594,"we’ll each go out with her one more time. And-and we’ll see who she likes best.
1103"
LOWEST COEFFICIENT,0.7661016949152543,"RACHEL : Yeah, Joey kinda disabled it when I moved in.
1104"
LOWEST COEFFICIENT,0.7667796610169492,"MONICA : Joey that is horriable.
1105"
LOWEST COEFFICIENT,0.7674576271186441,"CHANDLER : No, see the thing is I want to get out of here before Joey gets all worked
1106"
LOWEST COEFFICIENT,0.768135593220339,"up and starts calling everybody bitch.
1107"
LOWEST COEFFICIENT,0.7688135593220339,"7.11
PCA16
1108"
HIGHEST COEFFICIENT,0.7694915254237288,"7.11.1
Highest coefficient
1109"
HIGHEST COEFFICIENT,0.7701694915254237,"RACHEL : Oh, oh. . What is this?
1110"
HIGHEST COEFFICIENT,0.7708474576271186,"PHOEBE : Oh, yeah. What’s this?
1111"
HIGHEST COEFFICIENT,0.7715254237288136,"JOEY : I don’t know. It’s-it’s just...lately, I’ve been feeling... Okay, here’s what it is...
1112"
HIGHEST COEFFICIENT,0.7722033898305085,"You know what? I feel a lot better, thanks!
1113"
HIGHEST COEFFICIENT,0.7728813559322034,"PHOEBE : Ohh. What is this?
1114"
HIGHEST COEFFICIENT,0.7735593220338983,"CHANDLER : Oh-oh, what are you doing?
1115"
HIGHEST COEFFICIENT,0.7742372881355932,"PHOEBE : Oh that’s so great! Ohh, so what’s going on now?
1116"
HIGHEST COEFFICIENT,0.7749152542372881,"PHOEBE : Oh my God, what’s it doing here?
1117"
HIGHEST COEFFICIENT,0.775593220338983,"JOEY : Yeah! Yeah, why? What’s up?
1118"
HIGHEST COEFFICIENT,0.7762711864406779,"PHOEBE : Oh, why? What’s up?
1119"
HIGHEST COEFFICIENT,0.7769491525423728,"PHOEBE : What-what’s up?
1120"
LOWEST COEFFICIENT,0.7776271186440677,"7.11.2
Lowest coefficient
1121"
LOWEST COEFFICIENT,0.7783050847457628,"CHANDLER : And then he did.
1122"
LOWEST COEFFICIENT,0.7789830508474577,"PHOEBE : And we did.
1123"
LOWEST COEFFICIENT,0.7796610169491526,"ROSS : No you didn’t. You said you would, but you never did!
1124"
LOWEST COEFFICIENT,0.7803389830508475,"CHANDLER : I sure did.
1125"
LOWEST COEFFICIENT,0.7810169491525424,"RACHEL : No, you could’ve lost your job.
1126"
LOWEST COEFFICIENT,0.7816949152542373,"ROSS : Sure, Monica would have to give her up.
1127"
LOWEST COEFFICIENT,0.7823728813559322,"CHANDLER : Yes he did.
1128"
LOWEST COEFFICIENT,0.7830508474576271,"RACHEL : That is not true. She did! She forced me!
1129"
LOWEST COEFFICIENT,0.783728813559322,"RACHEL : That is not true. She did! She forced me!
1130"
LOWEST COEFFICIENT,0.7844067796610169,"ROSS : Monica! Would it?
1131"
LOWEST COEFFICIENT,0.7850847457627118,"NeurIPS Paper Checklist
1132"
CLAIMS,0.7857627118644068,"1. Claims
1133"
CLAIMS,0.7864406779661017,"Question: Do the main claims made in the abstract and introduction accurately reflect the
1134"
CLAIMS,0.7871186440677966,"paper’s contributions and scope?
1135"
CLAIMS,0.7877966101694915,"Answer: [Yes] .
1136"
CLAIMS,0.7884745762711864,"Justification: The article follow indeed the abstract claims.
1137"
CLAIMS,0.7891525423728813,"Guidelines:
1138"
CLAIMS,0.7898305084745763,"• The answer NA means that the abstract and introduction do not include the claims
1139"
CLAIMS,0.7905084745762712,"made in the paper.
1140"
CLAIMS,0.7911864406779661,"• The abstract and/or introduction should clearly state the claims made, including the
1141"
CLAIMS,0.791864406779661,"contributions made in the paper and important assumptions and limitations. A No or
1142"
CLAIMS,0.792542372881356,"NA answer to this question will not be perceived well by the reviewers.
1143"
CLAIMS,0.7932203389830509,"• The claims made should match theoretical and experimental results, and reflect how
1144"
CLAIMS,0.7938983050847458,"much the results can be expected to generalize to other settings.
1145"
CLAIMS,0.7945762711864407,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
1146"
CLAIMS,0.7952542372881356,"are not attained by the paper.
1147"
LIMITATIONS,0.7959322033898305,"2. Limitations
1148"
LIMITATIONS,0.7966101694915254,"Question: Does the paper discuss the limitations of the work performed by the authors?
1149"
LIMITATIONS,0.7972881355932203,"Answer: [Yes] .
1150"
LIMITATIONS,0.7979661016949152,"Justification: We outline the limitations clearly in the last paragraph of the conclusions.
1151"
LIMITATIONS,0.7986440677966101,"Guidelines:
1152"
LIMITATIONS,0.7993220338983051,"• The answer NA means that the paper has no limitation while the answer No means that
1153"
LIMITATIONS,0.8,"the paper has limitations, but those are not discussed in the paper.
1154"
LIMITATIONS,0.800677966101695,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
1155"
LIMITATIONS,0.8013559322033899,"• The paper should point out any strong assumptions and how robust the results are to
1156"
LIMITATIONS,0.8020338983050848,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
1157"
LIMITATIONS,0.8027118644067797,"model well-specification, asymptotic approximations only holding locally). The authors
1158"
LIMITATIONS,0.8033898305084746,"should reflect on how these assumptions might be violated in practice and what the
1159"
LIMITATIONS,0.8040677966101695,"implications would be.
1160"
LIMITATIONS,0.8047457627118644,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
1161"
LIMITATIONS,0.8054237288135593,"only tested on a few datasets or with a few runs. In general, empirical results often
1162"
LIMITATIONS,0.8061016949152542,"depend on implicit assumptions, which should be articulated.
1163"
LIMITATIONS,0.8067796610169492,"• The authors should reflect on the factors that influence the performance of the approach.
1164"
LIMITATIONS,0.8074576271186441,"For example, a facial recognition algorithm may perform poorly when image resolution
1165"
LIMITATIONS,0.808135593220339,"is low or images are taken in low lighting. Or a speech-to-text system might not be
1166"
LIMITATIONS,0.8088135593220339,"used reliably to provide closed captions for online lectures because it fails to handle
1167"
LIMITATIONS,0.8094915254237288,"technical jargon.
1168"
LIMITATIONS,0.8101694915254237,"• The authors should discuss the computational efficiency of the proposed algorithms
1169"
LIMITATIONS,0.8108474576271186,"and how they scale with dataset size.
1170"
LIMITATIONS,0.8115254237288135,"• If applicable, the authors should discuss possible limitations of their approach to
1171"
LIMITATIONS,0.8122033898305084,"address problems of privacy and fairness.
1172"
LIMITATIONS,0.8128813559322033,"• While the authors might fear that complete honesty about limitations might be used by
1173"
LIMITATIONS,0.8135593220338984,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
1174"
LIMITATIONS,0.8142372881355933,"limitations that aren’t acknowledged in the paper. The authors should use their best
1175"
LIMITATIONS,0.8149152542372882,"judgment and recognize that individual actions in favor of transparency play an impor-
1176"
LIMITATIONS,0.8155932203389831,"tant role in developing norms that preserve the integrity of the community. Reviewers
1177"
LIMITATIONS,0.816271186440678,"will be specifically instructed to not penalize honesty concerning limitations.
1178"
THEORY ASSUMPTIONS AND PROOFS,0.8169491525423729,"3. Theory Assumptions and Proofs
1179"
THEORY ASSUMPTIONS AND PROOFS,0.8176271186440678,"Question: For each theoretical result, does the paper provide the full set of assumptions and
1180"
THEORY ASSUMPTIONS AND PROOFS,0.8183050847457627,"a complete (and correct) proof?
1181"
THEORY ASSUMPTIONS AND PROOFS,0.8189830508474576,"Answer: [NA] .
1182"
THEORY ASSUMPTIONS AND PROOFS,0.8196610169491525,"Justification: There are no theoretical results or proof in this paper.
1183"
THEORY ASSUMPTIONS AND PROOFS,0.8203389830508474,"Guidelines:
1184"
THEORY ASSUMPTIONS AND PROOFS,0.8210169491525424,"• The answer NA means that the paper does not include theoretical results.
1185"
THEORY ASSUMPTIONS AND PROOFS,0.8216949152542373,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
1186"
THEORY ASSUMPTIONS AND PROOFS,0.8223728813559322,"referenced.
1187"
THEORY ASSUMPTIONS AND PROOFS,0.8230508474576271,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
1188"
THEORY ASSUMPTIONS AND PROOFS,0.823728813559322,"• The proofs can either appear in the main paper or the supplemental material, but if
1189"
THEORY ASSUMPTIONS AND PROOFS,0.8244067796610169,"they appear in the supplemental material, the authors are encouraged to provide a short
1190"
THEORY ASSUMPTIONS AND PROOFS,0.8250847457627118,"proof sketch to provide intuition.
1191"
THEORY ASSUMPTIONS AND PROOFS,0.8257627118644068,"• Inversely, any informal proof provided in the core of the paper should be complemented
1192"
THEORY ASSUMPTIONS AND PROOFS,0.8264406779661017,"by formal proofs provided in appendix or supplemental material.
1193"
THEORY ASSUMPTIONS AND PROOFS,0.8271186440677966,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
1194"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8277966101694916,"4. Experimental Result Reproducibility
1195"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8284745762711865,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
1196"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8291525423728814,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
1197"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8298305084745763,"of the paper (regardless of whether the code and data are provided or not)?
1198"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8305084745762712,"Answer: [Yes] .
1199"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8311864406779661,"Justification: The code and data are provide. The datasets are public, and all the library used
1200"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.831864406779661,"are also public.
1201"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8325423728813559,"Guidelines:
1202"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8332203389830508,"• The answer NA means that the paper does not include experiments.
1203"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8338983050847457,"• If the paper includes experiments, a No answer to this question will not be perceived
1204"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8345762711864407,"well by the reviewers: Making the paper reproducible is important, regardless of
1205"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8352542372881356,"whether the code and data are provided or not.
1206"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8359322033898305,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
1207"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8366101694915254,"to make their results reproducible or verifiable.
1208"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8372881355932204,"• Depending on the contribution, reproducibility can be accomplished in various ways.
1209"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8379661016949153,"For example, if the contribution is a novel architecture, describing the architecture fully
1210"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8386440677966102,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
1211"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8393220338983051,"be necessary to either make it possible for others to replicate the model with the same
1212"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.84,"dataset, or provide access to the model. In general. releasing code and data is often
1213"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8406779661016949,"one good way to accomplish this, but reproducibility can also be provided via detailed
1214"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8413559322033898,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
1215"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8420338983050848,"of a large language model), releasing of a model checkpoint, or other means that are
1216"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8427118644067797,"appropriate to the research performed.
1217"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8433898305084746,"• While NeurIPS does not require releasing code, the conference does require all submis-
1218"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8440677966101695,"sions to provide some reasonable avenue for reproducibility, which may depend on the
1219"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8447457627118644,"nature of the contribution. For example
1220"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8454237288135593,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
1221"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8461016949152542,"to reproduce that algorithm.
1222"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8467796610169491,"(b) If the contribution is primarily a new model architecture, the paper should describe
1223"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.847457627118644,"the architecture clearly and fully.
1224"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8481355932203389,"(c) If the contribution is a new model (e.g., a large language model), then there should
1225"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.848813559322034,"either be a way to access this model for reproducing the results or a way to reproduce
1226"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8494915254237289,"the model (e.g., with an open-source dataset or instructions for how to construct
1227"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8501694915254238,"the dataset).
1228"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8508474576271187,"(d) We recognize that reproducibility may be tricky in some cases, in which case
1229"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8515254237288136,"authors are welcome to describe the particular way they provide for reproducibility.
1230"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8522033898305085,"In the case of closed-source models, it may be that access to the model is limited in
1231"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8528813559322034,"some way (e.g., to registered users), but it should be possible for other researchers
1232"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8535593220338983,"to have some path to reproducing or verifying the results.
1233"
OPEN ACCESS TO DATA AND CODE,0.8542372881355932,"5. Open access to data and code
1234"
OPEN ACCESS TO DATA AND CODE,0.8549152542372881,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
1235"
OPEN ACCESS TO DATA AND CODE,0.855593220338983,"tions to faithfully reproduce the main experimental results, as described in supplemental
1236"
OPEN ACCESS TO DATA AND CODE,0.856271186440678,"material?
1237"
OPEN ACCESS TO DATA AND CODE,0.8569491525423729,"Answer: [Yes] .
1238"
OPEN ACCESS TO DATA AND CODE,0.8576271186440678,"Justification: Data and full code are on Github
1239"
OPEN ACCESS TO DATA AND CODE,0.8583050847457627,"Guidelines:
1240"
OPEN ACCESS TO DATA AND CODE,0.8589830508474576,"• The answer NA means that paper does not include experiments requiring code.
1241"
OPEN ACCESS TO DATA AND CODE,0.8596610169491525,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
1242"
OPEN ACCESS TO DATA AND CODE,0.8603389830508474,"public/guides/CodeSubmissionPolicy) for more details.
1243"
OPEN ACCESS TO DATA AND CODE,0.8610169491525423,"• While we encourage the release of code and data, we understand that this might not be
1244"
OPEN ACCESS TO DATA AND CODE,0.8616949152542372,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
1245"
OPEN ACCESS TO DATA AND CODE,0.8623728813559322,"including code, unless this is central to the contribution (e.g., for a new open-source
1246"
OPEN ACCESS TO DATA AND CODE,0.8630508474576272,"benchmark).
1247"
OPEN ACCESS TO DATA AND CODE,0.8637288135593221,"• The instructions should contain the exact command and environment needed to run to
1248"
OPEN ACCESS TO DATA AND CODE,0.864406779661017,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
1249"
OPEN ACCESS TO DATA AND CODE,0.8650847457627119,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
1250"
OPEN ACCESS TO DATA AND CODE,0.8657627118644068,"• The authors should provide instructions on data access and preparation, including how
1251"
OPEN ACCESS TO DATA AND CODE,0.8664406779661017,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
1252"
OPEN ACCESS TO DATA AND CODE,0.8671186440677966,"• The authors should provide scripts to reproduce all experimental results for the new
1253"
OPEN ACCESS TO DATA AND CODE,0.8677966101694915,"proposed method and baselines. If only a subset of experiments are reproducible, they
1254"
OPEN ACCESS TO DATA AND CODE,0.8684745762711864,"should state which ones are omitted from the script and why.
1255"
OPEN ACCESS TO DATA AND CODE,0.8691525423728813,"• At submission time, to preserve anonymity, the authors should release anonymized
1256"
OPEN ACCESS TO DATA AND CODE,0.8698305084745763,"versions (if applicable).
1257"
OPEN ACCESS TO DATA AND CODE,0.8705084745762712,"• Providing as much information as possible in supplemental material (appended to the
1258"
OPEN ACCESS TO DATA AND CODE,0.8711864406779661,"paper) is recommended, but including URLs to data and code is permitted.
1259"
OPEN ACCESS TO DATA AND CODE,0.871864406779661,"6. Experimental Setting/Details
1260"
OPEN ACCESS TO DATA AND CODE,0.8725423728813559,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
1261"
OPEN ACCESS TO DATA AND CODE,0.8732203389830508,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
1262"
OPEN ACCESS TO DATA AND CODE,0.8738983050847458,"results?
1263"
OPEN ACCESS TO DATA AND CODE,0.8745762711864407,"Answer: [Yes]
1264"
OPEN ACCESS TO DATA AND CODE,0.8752542372881356,"Justification: The full code used are on gitub, that include all the information that we did
1265"
OPEN ACCESS TO DATA AND CODE,0.8759322033898305,"(test split,...).
1266"
OPEN ACCESS TO DATA AND CODE,0.8766101694915254,"Guidelines:
1267"
OPEN ACCESS TO DATA AND CODE,0.8772881355932204,"• The answer NA means that the paper does not include experiments.
1268"
OPEN ACCESS TO DATA AND CODE,0.8779661016949153,"• The experimental setting should be presented in the core of the paper to a level of detail
1269"
OPEN ACCESS TO DATA AND CODE,0.8786440677966102,"that is necessary to appreciate the results and make sense of them.
1270"
OPEN ACCESS TO DATA AND CODE,0.8793220338983051,"• The full details can be provided either with the code, in appendix, or as supplemental
1271"
OPEN ACCESS TO DATA AND CODE,0.88,"material.
1272"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8806779661016949,"7. Experiment Statistical Significance
1273"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8813559322033898,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1274"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8820338983050847,"information about the statistical significance of the experiments?
1275"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8827118644067796,"Answer:[Yes] .
1276"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8833898305084745,"Justification: We report standard errors for our experimental results on GPT4 and human
1277"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8840677966101695,"subjects.
1278"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8847457627118644,"Guidelines:
1279"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8854237288135594,"• The answer NA means that the paper does not include experiments.
1280"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8861016949152543,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
1281"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8867796610169492,"dence intervals, or statistical significance tests, at least for the experiments that support
1282"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8874576271186441,"the main claims of the paper.
1283"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.888135593220339,"• The factors of variability that the error bars are capturing should be clearly stated (for
1284"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8888135593220339,"example, train/test split, initialization, random drawing of some parameter, or overall
1285"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8894915254237288,"run with given experimental conditions).
1286"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8901694915254237,"• The method for calculating the error bars should be explained (closed form formula,
1287"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8908474576271187,"call to a library function, bootstrap, etc.)
1288"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8915254237288136,"• The assumptions made should be given (e.g., Normally distributed errors).
1289"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8922033898305085,"• It should be clear whether the error bar is the standard deviation or the standard error
1290"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8928813559322034,"of the mean.
1291"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8935593220338983,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1292"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8942372881355932,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1293"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8949152542372881,"of Normality of errors is not verified.
1294"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.895593220338983,"• For asymmetric distributions, the authors should be careful not to show in tables or
1295"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8962711864406779,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1296"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8969491525423728,"error rates).
1297"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8976271186440677,"• If error bars are reported in tables or plots, The authors should explain in the text how
1298"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8983050847457628,"they were calculated and reference the corresponding figures or tables in the text.
1299"
EXPERIMENTS COMPUTE RESOURCES,0.8989830508474577,"8. Experiments Compute Resources
1300"
EXPERIMENTS COMPUTE RESOURCES,0.8996610169491526,"Question: For each experiment, does the paper provide sufficient information on the com-
1301"
EXPERIMENTS COMPUTE RESOURCES,0.9003389830508475,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1302"
EXPERIMENTS COMPUTE RESOURCES,0.9010169491525424,"the experiments?
1303"
EXPERIMENTS COMPUTE RESOURCES,0.9016949152542373,"Answer: [Yes] .
1304"
EXPERIMENTS COMPUTE RESOURCES,0.9023728813559322,"Justification: There are no information about time or resources, the calculus used takes 4
1305"
EXPERIMENTS COMPUTE RESOURCES,0.9030508474576271,"seconds (to do the PCA) to a couple of minutes (to do the embeddings), and do not required
1306"
EXPERIMENTS COMPUTE RESOURCES,0.903728813559322,"lots of memory
1307"
EXPERIMENTS COMPUTE RESOURCES,0.9044067796610169,"Guidelines:
1308"
EXPERIMENTS COMPUTE RESOURCES,0.9050847457627119,"• The answer NA means that the paper does not include experiments.
1309"
EXPERIMENTS COMPUTE RESOURCES,0.9057627118644068,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1310"
EXPERIMENTS COMPUTE RESOURCES,0.9064406779661017,"or cloud provider, including relevant memory and storage.
1311"
EXPERIMENTS COMPUTE RESOURCES,0.9071186440677966,"• The paper should provide the amount of compute required for each of the individual
1312"
EXPERIMENTS COMPUTE RESOURCES,0.9077966101694915,"experimental runs as well as estimate the total compute.
1313"
EXPERIMENTS COMPUTE RESOURCES,0.9084745762711864,"• The paper should disclose whether the full research project required more compute
1314"
EXPERIMENTS COMPUTE RESOURCES,0.9091525423728813,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1315"
EXPERIMENTS COMPUTE RESOURCES,0.9098305084745762,"didn’t make it into the paper).
1316"
CODE OF ETHICS,0.9105084745762712,"9. Code Of Ethics
1317"
CODE OF ETHICS,0.9111864406779661,"Question: Does the research conducted in the paper conform, in every respect, with the
1318"
CODE OF ETHICS,0.911864406779661,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1319"
CODE OF ETHICS,0.912542372881356,"Answer: [Yes]
1320"
CODE OF ETHICS,0.9132203389830509,"Justification: The paper conforms to the NeurIPS code of Ethics
1321"
CODE OF ETHICS,0.9138983050847458,"Guidelines:
1322"
CODE OF ETHICS,0.9145762711864407,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1323"
CODE OF ETHICS,0.9152542372881356,"• If the authors answer No, they should explain the special circumstances that require a
1324"
CODE OF ETHICS,0.9159322033898305,"deviation from the Code of Ethics.
1325"
CODE OF ETHICS,0.9166101694915254,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1326"
CODE OF ETHICS,0.9172881355932203,"eration due to laws or regulations in their jurisdiction).
1327"
BROADER IMPACTS,0.9179661016949152,"10. Broader Impacts
1328"
BROADER IMPACTS,0.9186440677966101,"Question: Does the paper discuss both potential positive societal impacts and negative
1329"
BROADER IMPACTS,0.9193220338983051,"societal impacts of the work performed?
1330"
BROADER IMPACTS,0.92,"Answer: [Yes] .
1331"
BROADER IMPACTS,0.920677966101695,"Justification: The qualitative analysis shows how gender stereotyping is a large part of how
1332"
BROADER IMPACTS,0.9213559322033898,"machine learning models make predictions. The article also critiques ideas around artificial
1333"
BROADER IMPACTS,0.9220338983050848,"general intelligence (AGI) in a way we think illuminates debate on these issues.
1334"
BROADER IMPACTS,0.9227118644067797,"Guidelines:
1335"
BROADER IMPACTS,0.9233898305084746,"• The answer NA means that there is no societal impact of the work performed.
1336"
BROADER IMPACTS,0.9240677966101695,"• If the authors answer NA or No, they should explain why their work has no societal
1337"
BROADER IMPACTS,0.9247457627118644,"impact or why the paper does not address societal impact.
1338"
BROADER IMPACTS,0.9254237288135593,"• Examples of negative societal impacts include potential malicious or unintended uses
1339"
BROADER IMPACTS,0.9261016949152543,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1340"
BROADER IMPACTS,0.9267796610169492,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1341"
BROADER IMPACTS,0.9274576271186441,"groups), privacy considerations, and security considerations.
1342"
BROADER IMPACTS,0.928135593220339,"• The conference expects that many papers will be foundational research and not tied
1343"
BROADER IMPACTS,0.9288135593220339,"to particular applications, let alone deployments. However, if there is a direct path to
1344"
BROADER IMPACTS,0.9294915254237288,"any negative applications, the authors should point it out. For example, it is legitimate
1345"
BROADER IMPACTS,0.9301694915254237,"to point out that an improvement in the quality of generative models could be used to
1346"
BROADER IMPACTS,0.9308474576271186,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1347"
BROADER IMPACTS,0.9315254237288135,"that a generic algorithm for optimizing neural networks could enable people to train
1348"
BROADER IMPACTS,0.9322033898305084,"models that generate Deepfakes faster.
1349"
BROADER IMPACTS,0.9328813559322033,"• The authors should consider possible harms that could arise when the technology is
1350"
BROADER IMPACTS,0.9335593220338984,"being used as intended and functioning correctly, harms that could arise when the
1351"
BROADER IMPACTS,0.9342372881355933,"technology is being used as intended but gives incorrect results, and harms following
1352"
BROADER IMPACTS,0.9349152542372882,"from (intentional or unintentional) misuse of the technology.
1353"
BROADER IMPACTS,0.9355932203389831,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1354"
BROADER IMPACTS,0.936271186440678,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1355"
BROADER IMPACTS,0.9369491525423729,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1356"
BROADER IMPACTS,0.9376271186440678,"feedback over time, improving the efficiency and accessibility of ML).
1357"
SAFEGUARDS,0.9383050847457627,"11. Safeguards
1358"
SAFEGUARDS,0.9389830508474576,"Question: Does the paper describe safeguards that have been put in place for responsible
1359"
SAFEGUARDS,0.9396610169491525,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1360"
SAFEGUARDS,0.9403389830508475,"image generators, or scraped datasets)?
1361"
SAFEGUARDS,0.9410169491525424,"Answer: [NA]
1362"
SAFEGUARDS,0.9416949152542373,"Justification: No such risks
1363"
SAFEGUARDS,0.9423728813559322,"Guidelines:
1364"
SAFEGUARDS,0.9430508474576271,"• The answer NA means that the paper poses no such risks.
1365"
SAFEGUARDS,0.943728813559322,"• Released models that have a high risk for misuse or dual-use should be released with
1366"
SAFEGUARDS,0.9444067796610169,"necessary safeguards to allow for controlled use of the model, for example by requiring
1367"
SAFEGUARDS,0.9450847457627118,"that users adhere to usage guidelines or restrictions to access the model or implementing
1368"
SAFEGUARDS,0.9457627118644067,"safety filters.
1369"
SAFEGUARDS,0.9464406779661017,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1370"
SAFEGUARDS,0.9471186440677966,"should describe how they avoided releasing unsafe images.
1371"
SAFEGUARDS,0.9477966101694916,"• We recognize that providing effective safeguards is challenging, and many papers do
1372"
SAFEGUARDS,0.9484745762711865,"not require this, but we encourage authors to take this into account and make a best
1373"
SAFEGUARDS,0.9491525423728814,"faith effort.
1374"
LICENSES FOR EXISTING ASSETS,0.9498305084745763,"12. Licenses for existing assets
1375"
LICENSES FOR EXISTING ASSETS,0.9505084745762712,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1376"
LICENSES FOR EXISTING ASSETS,0.9511864406779661,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1377"
LICENSES FOR EXISTING ASSETS,0.951864406779661,"properly respected?
1378"
LICENSES FOR EXISTING ASSETS,0.9525423728813559,"Answer:[Yes]
1379"
LICENSES FOR EXISTING ASSETS,0.9532203389830508,"Justification: We credits the owners of the dataset and python libraries that we used
1380"
LICENSES FOR EXISTING ASSETS,0.9538983050847457,"Guidelines:
1381"
LICENSES FOR EXISTING ASSETS,0.9545762711864407,"• The answer NA means that the paper does not use existing assets.
1382"
LICENSES FOR EXISTING ASSETS,0.9552542372881356,"• The authors should cite the original paper that produced the code package or dataset.
1383"
LICENSES FOR EXISTING ASSETS,0.9559322033898305,"• The authors should state which version of the asset is used and, if possible, include a
1384"
LICENSES FOR EXISTING ASSETS,0.9566101694915254,"URL.
1385"
LICENSES FOR EXISTING ASSETS,0.9572881355932203,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1386"
LICENSES FOR EXISTING ASSETS,0.9579661016949153,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1387"
LICENSES FOR EXISTING ASSETS,0.9586440677966102,"service of that source should be provided.
1388"
LICENSES FOR EXISTING ASSETS,0.9593220338983051,"• If assets are released, the license, copyright information, and terms of use in the
1389"
LICENSES FOR EXISTING ASSETS,0.96,"package should be provided. For popular datasets, paperswithcode.com/datasets
1390"
LICENSES FOR EXISTING ASSETS,0.9606779661016949,"has curated licenses for some datasets. Their licensing guide can help determine the
1391"
LICENSES FOR EXISTING ASSETS,0.9613559322033899,"license of a dataset.
1392"
LICENSES FOR EXISTING ASSETS,0.9620338983050848,"• For existing datasets that are re-packaged, both the original license and the license of
1393"
LICENSES FOR EXISTING ASSETS,0.9627118644067797,"the derived asset (if it has changed) should be provided.
1394"
LICENSES FOR EXISTING ASSETS,0.9633898305084746,"• If this information is not available online, the authors are encouraged to reach out to
1395"
LICENSES FOR EXISTING ASSETS,0.9640677966101695,"the asset’s creators.
1396"
NEW ASSETS,0.9647457627118644,"13. New Assets
1397"
NEW ASSETS,0.9654237288135593,"Question: Are new assets introduced in the paper well documented and is the documentation
1398"
NEW ASSETS,0.9661016949152542,"provided alongside the assets?
1399"
NEW ASSETS,0.9667796610169491,"Answer: [Yes] .
1400"
NEW ASSETS,0.967457627118644,"Justification: The details about training, limitations are in the article, and the code is on
1401"
NEW ASSETS,0.9681355932203389,"Github.
1402"
NEW ASSETS,0.968813559322034,"Guidelines:
1403"
NEW ASSETS,0.9694915254237289,"• The answer NA means that the paper does not release new assets.
1404"
NEW ASSETS,0.9701694915254238,"• Researchers should communicate the details of the dataset/code/model as part of their
1405"
NEW ASSETS,0.9708474576271187,"submissions via structured templates. This includes details about training, license,
1406"
NEW ASSETS,0.9715254237288136,"limitations, etc.
1407"
NEW ASSETS,0.9722033898305085,"• The paper should discuss whether and how consent was obtained from people whose
1408"
NEW ASSETS,0.9728813559322034,"asset is used.
1409"
NEW ASSETS,0.9735593220338983,"• At submission time, remember to anonymize your assets (if applicable). You can either
1410"
NEW ASSETS,0.9742372881355932,"create an anonymized URL or include an anonymized zip file.
1411"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9749152542372881,"14. Crowdsourcing and Research with Human Subjects
1412"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9755932203389831,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1413"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.976271186440678,"include the full text of instructions given to participants and screenshots, if applicable, as
1414"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769491525423729,"well as details about compensation (if any)?
1415"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776271186440678,"Answer: [Yes] .
1416"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783050847457627,"Justification: We recruit two relatives to do the test. We give them instructions and spread-
1417"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9789830508474576,"sheets with dialogue where they have to complete with the name of the characters
1418"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9796610169491525,"Guidelines:
1419"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9803389830508474,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1420"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9810169491525423,"human subjects.
1421"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816949152542372,"• Including this information in the supplemental material is fine, but if the main contribu-
1422"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823728813559321,"tion of the paper involves human subjects, then as much detail as possible should be
1423"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830508474576272,"included in the main paper.
1424"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837288135593221,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1425"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984406779661017,"or other labor should be paid at least the minimum wage in the country of the data
1426"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850847457627119,"collector.
1427"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857627118644068,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1428"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864406779661017,"Subjects
1429"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871186440677966,"Question: Does the paper describe potential risks incurred by study participants, whether
1430"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9877966101694915,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1431"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884745762711864,"approvals (or an equivalent approval/review based on the requirements of your country or
1432"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891525423728813,"institution) were obtained?
1433"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9898305084745763,"Answer: [No] .
1434"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905084745762712,"Justification: The two subjects that we asked questions were relatives that like those TV
1435"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911864406779661,"series immensely. There were no risks for them in answering the questions, therefore we do
1436"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991864406779661,"not proceed for an Ethical approval for such a small study.
1437"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925423728813559,"Guidelines:
1438"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932203389830508,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1439"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938983050847457,"human subjects.
1440"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945762711864407,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1441"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952542372881356,"may be required for any human subjects research. If you obtained IRB approval, you
1442"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959322033898305,"should clearly state this in the paper.
1443"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966101694915255,"• We recognize that the procedures for this may vary significantly between institutions
1444"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972881355932204,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1445"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979661016949153,"guidelines for their institution.
1446"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9986440677966102,"• For initial submissions, do not include any information that would break anonymity (if
1447"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9993220338983051,"applicable), such as the institution conducting the review.
1448"
