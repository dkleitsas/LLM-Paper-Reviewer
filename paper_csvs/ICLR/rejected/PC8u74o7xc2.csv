Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0033444816053511705,"Embedding-based approaches ﬁnd the semantic meaning of tokens in structured
data such as natural language, graphs, and even images. To a great degree, these
approaches have developed independently in different domains. However, we ﬁnd
a common principle underlying these formulations, and it is rooted in solutions to
the stable coloring problem in graphs (Weisfeiler-Lehman isomorphism test). For
instance, we ﬁnd links between stable coloring, distribution hypothesis in natural
language processing, and non-local-means denoising algorithm in image signal
processing. We even ﬁnd that stable coloring has strong connections to a broad
class of unsupervised embedding models which is surprising at ﬁrst since stable
coloring is generally applied for combinatorial problems. To establish this con-
nection concretely we deﬁne a mathematical framework that deﬁnes continuous
stable coloring on graphs and develops optimization problems to search for them.
Grounded on this framework, we show that many algorithms ranging across dif-
ferent domains are, in fact, searching for continuous stable coloring solutions of
an underlying graph corresponding to the domain. We show that popular and
widely used embedding models such as Word2Vec, AWE, BERT, Node2Vec, and
Vis-Transformer can be understood as instantiations of our general algorithm that
solves the problem of continuous stable coloring. These instantiations offer useful
insights into the workings of state-of-the-art models like BERT stimulating new
research directions."
INTRODUCTION,0.006688963210702341,"1
INTRODUCTION"
INTRODUCTION,0.010033444816053512,"Embedding models are ubiquitous in wide range of real-world applications such as information
retrieval (Zuccon et al., 2015), natural language processing (NLP) (Mikolov et al., 2013a;b), graph
classiﬁcation (Grover & Leskovec, 2016; Hamilton et al., 2017) and many more. These models map
categorical entities to continuous dense representations (typically in Rd) which provide a continuous
measure of semantic similarity across categorical entities. Nowadays, there is a heavy dependence
on unsupervised pre-trained embedding models across domains like Transformers in NLP (Devlin
et al., 2019), Visual Transformers (ViT) in Computer Vision (Dosovitskiy et al., 2020), Graph Neural
Networks (Hamilton et al., 2017; Xu et al., 2019b) since they learn rich semantic representations of
entities from massive amounts of unlabelled data. With little ﬁnetuning, these models achieve state-
of-the-art results on most of the supervised downstream tasks like sentiment analysis (Xu et al.,
2019a), object detection (Beal et al., 2020), and graph classiﬁcation (Xu et al., 2019b)."
INTRODUCTION,0.013377926421404682,"Historically, embedding models were developed almost independently across structured domains
such as NLP, graphs, images, and so on. These algorithms use the neighborhood structure around
an entity to obtain the embedding for the entity. Interestingly, a popular hypothesis in NLP - Distri-
butional Hypothesis states that the “meaning” of the word is determined by its context (neighbors)
(Harris, 1954; Sahlgren, 2008). This hypothesis forms the basis of most unsupervised embedding
learning models in NLP (Mikolov et al., 2013a;b; Pennington et al., 2014b; Bojanowski et al., 2017;
Sonkar et al., 2020). Similarly, non-local-means, a denoising algorithm in signal processing, tries
to ﬁnd pixels that should be the same based on the similarity of its neighborhood structure (patch
of the image around the pixel in this case) (Awate & Whitaker, 2006; Buades et al., 2005). Even
Graph neural network (GNN) architectures ensure that information of the surrounding neighbors is
systematically incorporated in the embedding of a node, even in supervised settings (Hamilton et al.,
2017; Maron et al., 2019; Xu et al., 2019b). Thus somehow various communities working across"
INTRODUCTION,0.016722408026755852,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.020066889632107024,"diverse domains have narrowed down on an entity’s neighborhood structure to deﬁne the entity’s
meaning. In this paper, we try to establish this common principle on mathematically robust grounds."
INTRODUCTION,0.023411371237458192,"Structured domains can be easily represented as graphs with relations between entities as edges in
the graphs. For example in NLP words can be treated as nodes of some graph, and co-occurrence
relation between words can be represented as an edge. As mentioned before that graph embedding
architectures capture the topological structure around the node in the node embeddings, and if struc-
tured domains can be represented as graphs, this raises the question do embedding models from
structured domains like NLP and Vision also operate on some domain induced graph and capture
neighborhood structural properties in their entity embeddings since we have seen that embedding
models across these domains tend to capture “neighborhood” information?"
INTRODUCTION,0.026755852842809364,"To answer this question, we dive into combinatorial graph theory to understand how to deﬁne the
notion of structural equivalence a.k.a. isomorphic structures. Weisfeiler-Lehman (1-WL) algorithm
(or color reﬁnement algorithm) is the most popular heuristic used to identify graph isomorphism
(Weisfeiler & Leman, 1968) and can distinguish a broad class of graphs (Babai & Kucera, 1979).
The ﬁxed point solution of 1-WL is called a stable-coloring and has the property that any two nodes
with the same color have the same multi-set of colors in their neighborhood. In essence it means if
two nodes have the same color, the graph looks structurally identical from these nodes."
INTRODUCTION,0.030100334448160536,"Finally to answer the question if embedding models from structured domains capture the domain
induced graph’s topological properties in their entity embeddings, one can ﬁnd links between the
mechanics of these models and stable coloring / 1-WL algorithm. In this paper, we establish this
connection by providing a general framework linking existing algorithms to stable coloring. We
propose a more ﬂexible version of stable coloring (SC) called continuous stable coloring (CSC )
— a strict generalization of SC. CSC states that the similarity of their neighborhoods determines
the similarity of two nodes. Based on this notion, we deﬁne a series of optimization problems to
solve the problem of CSC . We show that various algorithms in NLP like word2vec, AWE, BERT
(Devlin et al., 2019), images processing like Visual Transformer (Dosovitskiy et al., 2020), graphs
like Node2Vec (Grover & Leskovec, 2016), etc. are essentially solving different instantiations of
this common optimization problem."
INTRODUCTION,0.033444816053511704,"Current research already establishes the link between the 1-WL algorithm (Grohe, 2020; Morris
et al., 2021; Shervashidze et al., 2011; Morris et al., 2017) and GNN architectures, which has
sparked a new line of research in improving GNN architectures (Hamilton et al., 2017; Xu et al.,
2019b; Maron et al., 2019; Morris et al., 2020a;b). We hope that the new link we establish between
stable coloring and unsupervised embedding algorithms will also stimulate new exciting research in
embeddings for other structured domains of NLP and Vision."
BACKGROUND,0.03678929765886288,"2
BACKGROUND"
BACKGROUND,0.04013377926421405,"In this section, we deﬁne a stable colored graph, provide an outline for 1-WL (Weisfeiler-Lehman)
graph isomorphism test and General Aggregate and Update (GAU ) for Graph Neural Networks
(GNNs). We also discuss how domains of NLP and images can be seen as graphs."
STABLE COLORING,0.043478260869565216,"2.1
STABLE COLORING"
STABLE COLORING,0.046822742474916385,"Let a coloring function C be an overloaded function deﬁned on the vertices as well on set of vertices
of G = (V, E) , i.e., C : V →N and C : 2V →{{N}} where N is a set of natural numbers
representing colors, and {{.}} is a multiset with overloading deﬁned as C(A) = {{C(v)|v ∈A ⊂
V}}. We denote neighborhood of a node u ∈V as N(v) = {u|(v, u) ∈E}."
STABLE COLORING,0.05016722408026756,"Deﬁnition 2.1 (Stable Coloring). An undirected graph G = (V, E) is stable colored w.r.t coloring
function C if it holds that C(u) = C(v) if and only if C(N(u)) = C(N(v))."
STABLE COLORING,0.05351170568561873,"We can extend the above deﬁnition to directed graphs and graphs with labels. A directed graph
G = (V, E) is stable colored w.r.t a coloring function C : V →N if it holds that C(u) = C(v) if and
only if C(Nin(u)) = C(Nin(v)) and C(Nout(u)) = C(Nout(v)) where Nin(u) = {w|(w, u) ∈E}
and Nout(u) = {w|(u, w) ∈E}. Likewise, an edge-labelled undirected graph G = (V, E) is
stable colored w.r.t a coloring function C : V →N if it holds that C(u) = C(v) if and only if
∀l, C(Nl(u)) = C(Nl(v)) where Nl(u) = {w|(u, w) ∈El} where El ⊂E for an edge label l."
STABLE COLORING,0.056856187290969896,Under review as a conference paper at ICLR 2022
STABLE COLORING,0.06020066889632107,"We also deﬁne a weak-stable coloring for a graph G = (V, E) . An undirected graph G = (V, E) is
weak-stable colored w.r.t coloring function C if it holds that C(u) = C(v) if C(N(u)) = C(N(v))
where N(u) = {w|(u, w) ∈E}."
STABLE COLORING,0.06354515050167224,"2.2
1-WL ALGORITHM AND GENERAL AGGREGATE AND UPDATE FRAMEWORK (GAU )"
STABLE COLORING,0.06688963210702341,"1-WL algorithm: 1-WL is an iterative algorithm to achieve a stable coloring C for G = (V, E) . Let
Ci denote the coloring at iteration i. It starts with a coloring scheme C0 such that C0(v) is same for
all v ∈V. In each iteration, it assigns a different color to u and v if Ci(N(u)) ̸= Ci(N(u)) until a
stable coloring C is reached."
STABLE COLORING,0.07023411371237458,"General Aggregate and Update Framework (GAU ) For a general multi-layer GNN, the General
Aggregate and Update framework to compute node/vertex embeddings (corresponding to colors) of
G = (V, E) is given iteratively by:"
STABLE COLORING,0.07357859531772576,"Ek(u) = f (k)
update"
STABLE COLORING,0.07692307692307693,"
Ek−1(u), f (k)
agg

Ek−1(v) : v ∈{{N(u)}}

,
(1)"
STABLE COLORING,0.0802675585284281,"where f (k)
agg(.) and f (k)
update(.) map vertex multiset embeddings to a metric space such as Rd."
STABLE COLORING,0.08361204013377926,"In GraphSage (Hamilton et al., 2017), authors showed that the iterative procedure in 1-WL algorithm
is analogous to General Aggregate and Update procedure in GNNs. This connection has lead to the
research direction where 1-WL is being used as a standard to which GNN architectures are being
compared to (Xu et al., 2019b; Maron et al., 2019; Morris et al., 2020b). Xu et al. (2019b) prove
that GAU is as powerful as 1-WL if the functions fupdate and fagg are injective."
STABLE COLORING,0.08695652173913043,"Various domains such as NLP, Images and Graphs can be viewed as a graph on their elementary
tokens. How we construct graphs is explained in section 6.1 and examples are given in appendix B."
RELATED WORK,0.0903010033444816,"3
RELATED WORK"
RELATED WORK,0.09364548494983277,"Word embeddings have been popular in NLP since decades (Deerwester et al., 1989; Morin & Ben-
gio, 2005; Mikolov et al., 2013b; Bojanowski et al., 2017). A lot of work has been done to under-
stand the mathematical underpinning of these models, for instance, relation of embedding models to
co-occurrence statistics (Levy & Goldberg, 2014; Hashimoto et al., 2016; Allen et al., 2019). Study
of empirical properties of these embedding models (e.g. analogies) has also attracted theoretical
research (Allen & Hospedales, 2019; Ethayarajh et al., 2019)."
RELATED WORK,0.09698996655518395,"Recently, graph community has also seen a surge in learning node and graph embeddings. The
notion of capturing the structural neighborhood around a node inside the node embedding has been
the driving principle of these semi-supervised node embedding algorithms like node2vec (Grover &
Leskovec, 2016), and GraphSAGE (Hamilton et al., 2017). Hamilton et al. (2017) pointed out that
their GraphSAGE node embedding algorithm mimics the aggregate and update procedure of 1-WL
algorithm. Subsequently, these neighborhood informative node embeddings found their applications
in constructing graph embeddings, and thereby used for classiﬁcation of structurally equivalent or
isomorphic graphs (Chen et al., 2019). Xu et al. (2019b) in their GIN (Graph Isomorphism Network)
model modiﬁed the aggregate procedure of GraphSAGE to construct graph embedders which were
provably as powerful as 1-WL algorithm in distinguishing non-isomorphic graphs. This redirected
the research into designing more powerful variants of graph embedders like PPGN (Maron et al.,
2019) and k-GNN (Morris et al., 2019) which were provably as powerful as 3-WL and k-WL test
respectively. Grohe (2020) discusses about these phenomenal works in increasing the expressivity
of graph embeddings for supervised graph classiﬁcation and regression tasks. While this line of
research focuses on improving expressivity and generalizability of Graph networks based on its
connection to 1-WL, we explore and formalize the unsupervised algorithms under the light of stable
coloring / 1-WL algorithm and show that all the current models stem from the common principle
that a tokens meaning is derived from its neighbours."
RELATED WORK,0.10033444816053512,"The rest of the paper is organised as follows. We begin with a discussion of connections between
discrete stable coloring and non-local means algorithm in Image processing and distributional hy-
pothesis in NLP. We then deﬁne a continuous version of SC and develop optimization problems to
solve for CSC in section 5. In section 6 we show how current state-of-the art embedding models are
solving the CSC problem in disguise."
RELATED WORK,0.10367892976588629,Under review as a conference paper at ICLR 2022
ALGORITHMS WITH ROOTS IN DISCRETE STABLE COLORING,0.10702341137123746,"4
ALGORITHMS WITH ROOTS IN DISCRETE STABLE COLORING"
ALGORITHMS WITH ROOTS IN DISCRETE STABLE COLORING,0.11036789297658862,"Distributional hypothesis in NLP states that words that occur in similar contexts are semantically
similar. Analogously, the non-local means image denoising algorithm in CV literature assigns simi-
lar intensity values to pixels that have similar patches surrounding them. The essence of both ideas
is that the value of an entity is determined by its neighborhood. In this section, we connect these two
ideas to the idea of stable coloring which states that if two nodes in a stable colored graph have same
colored neighborhood, they have the same color. The proofs of the following theorems are provided
in appendix."
ALGORITHMS WITH ROOTS IN DISCRETE STABLE COLORING,0.11371237458193979,"Theorem 4.1. (Distributional Hypothesis(DH) in NLP encodes SC) Let {w1, w2, .., wn} be words
in vocabulary, and M ∈Rn×n is a co-occurrence matrix with entries eij ∈N containing the number
of times wi co-occurs with wj within a ﬁxed context window. Let there be a function f : w →N
which takes a word and maps it to a color c ∈N (assigns meaning in accordance with Distributional
Hypothesis), such that f(wi) = f(wj) only if row i is same as row j in matrix M. Construct a graph
GDH with words as nodes and edge labels given by M. Then, function f deﬁnes a stable coloring
on graph GDH."
ALGORITHMS WITH ROOTS IN DISCRETE STABLE COLORING,0.11705685618729098,"DH deﬁnes words to be semantically same if they are substitutions of each other. In a true graph (
not the one created from samples) their co-occurrence frequencies with other words is exactly the
same. Equating color assigned by f to represent meaning of a word as per DH, it is easy to see that
words that end with same color are semantically same."
ALGORITHMS WITH ROOTS IN DISCRETE STABLE COLORING,0.12040133779264214,"Theorem 4.2. Consider a discrete signal y(x) sampled at n points xi (i = 1, ..., n), and let the
sequence p(xi) = (y(xi−t), .., y(xi−1), y(xi+1), .., y(xi+t)) be a patch of neighborhood values
around each xi for some context window length t. NLM denoises the signal y(xi), i = 1, ..., n with
iterative updates. The ﬁxed point denoised version of the signal yd can be written as follows:"
ALGORITHMS WITH ROOTS IN DISCRETE STABLE COLORING,0.12374581939799331,"yd(xi) =
1
D(xi) n
X"
ALGORITHMS WITH ROOTS IN DISCRETE STABLE COLORING,0.12709030100334448,"j=1
K(pd(xi), pd(xj))yd(xj),
(2)"
ALGORITHMS WITH ROOTS IN DISCRETE STABLE COLORING,0.13043478260869565,"where D(xi) = Pn
j=1 K(yd(xi), yd(xj)) and K is an arbitrary kernel function. Let the graph
Gnlm = (Vnlm, Enlm) where each xi is a node ui in Vnlm and each pair (ui, uj) with |i −j| ≤t
is represented as a directed edge in Enlm with label (i −j). Then the ﬁxed point solution of NLM
with Kronecker delta kernel Kδ, yd : N →R deﬁnes a weak-stable coloring over the graph Gnlm."
ALGORITHMS WITH ROOTS IN DISCRETE STABLE COLORING,0.13377926421404682,"An image is an example of a discrete signal with pixel intensities as signal values. With Kronecker
delta kernel, NLM terminates with the intensity value of yd(xi) = yd(xj) only if pd(xi) = pd(xj).
Assigning color to node ni as xi’s ﬁnal denoised intensity value, one can observe that NLM only
terminates when a weak-stable colored graph is deﬁned over the ﬁnal image. Note that if the pixel
values have high variety, then the probability to get a stable-colored graph is high."
ALGORITHMS WITH ROOTS IN DISCRETE STABLE COLORING,0.13712374581939799,"From the above two examples, it is interesting to observe that a stable colored graph G emerges from
the underlying principles used by two different domains. Both in DH and NLM, the words/pixels
that end up with the same color have same colored neighborhood around them. In order to ﬁnd
more connections to stable coloring, especially that of embedding models, we need a continuous
representation of color and hence in next section we start building on a novel notion of color."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.14046822742474915,"5
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.14381270903010032,"As shown in the earlier sections, even discrete stable coloring can be connected to various concepts
in NLP and image processing. We ﬁnd even more deep rooted links between stable coloring and
unsupervised learning algorithms in structured domains. In fact, we can view embeddings of each
node as a continuous ‘color’ assigned to each node. In order to show these links, we ﬁrst need to
generalize the idea of SC to a setting where we can talk about continuously comparable colors. This
section is organised as follows. We ﬁrst deﬁne a CSC problem analogous to SC . We show that this
is a strict generalization of SC (discrete) problem. Then, inspired by the GAU for stable coloring, we
propose a series of learning problems having GAU at their core (called General Aggregate Learning
, GAL in short) . We end this section with a generalized algorithm whose parameters, as we show in
section 6, can be initialized in various ways to obtain algorithms in varied domains that have been a
de facto standard in those domains since a long time."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.14715719063545152,Under review as a conference paper at ICLR 2022
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.1505016722408027,"5.1
CONTINUOUS STABLE COLORING (CSC )"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.15384615384615385,"Let us consider graph G = (V, E) representing the structured domain under consideration. In SC ,
we assign categorical colors to the nodes of the graph. However, for most analytical tasks, including
machine learning, we need to assign continuous labels (or embedding) to the nodes. Hence, we
introduce the problem of CSC . Let the domain of colors assigned be a metric space L (eg. Rd)
associated with a distance metric D. There are various ways to deﬁne similarity metrics in liter-
ature based on D. For the sake of discussion in this section, we would use a simple deﬁnition of
S(x, y) = e−D(x,y). While the theorems with other deﬁnitions will change in appearance, they will
still maintain the spirit of analysis. We denote the neighbourhood N(u) = {v|(u, v) ∈E}. Let
C : V →L denote the coloring of nodes. We overload the function C, C : 2V →NL, to oper-
ate on subset of nodes as given in the following equation. We use the notation NL to denote all
multi-subsets of L.
C(V ) = {{C(v)|v ∈V ⊆V}}
(3)
We deﬁne SN : NL × NL →R as the similarity metric over the multi-subsets of L via the same
similarity S over L and a permutation invariant and injective aggregator function fagg : N L →L
as
SN (A, B) = S(fagg(A), fagg(B))
where, A, B ∈NL.
(4)
We refer to this embedding as the continuous color under CSC . We use this terminology inter-
changeably as is best for the context. We denote nodes by small case letters (u, v, ...) and the
subset/multi-subsets of nodes by upper case letters (A, B, ..) etc. Let us now take a look at the
continuous stable coloring formulation (CSC ).
Deﬁnition 5.1. (Continuous Stable Coloring (L, S, fagg)). The coloring C : V →L of nodes
in graph G = (V, E) is called continuous stable coloring parameterised by similarity kernel S :
L × L →R for some metric space L and an injective aggregator function fagg : NL →L if the
following holds:"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.15719063545150502,"S(C(u), C(v)) = SN (C(N(u)), C(N(v))),
∀u, v ∈V.
(5)"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.1605351170568562,"In most applications, we look at L = Rd for some d > 0. Essentially, CSC states that the similarity
between embeddings of two nodes should be equal to the similarity between the neighborhoods of
the two nodes. Thus CSC relaxes the Kronecker delta function of comparison over categories in SC
to a general similarity metric over L. In fact CSC is a strict generalization of SC which we show in
the next theorem.
Theorem 5.1. (SC is a special case of CSC ). Stable coloring (discrete) problem is an instance of
continuous stable coloring problem with L = N, S(i, j) = 1(i = j) and SN (s1, s2) = 1(s1 = s2)
where i, j ∈N and s1, s2 ∈NN. In this case fagg : NN →N function is an injective hash function
which maps multi-subsets of N to N."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.16387959866220736,"It is easy to verify the validity of this theorem by using correct values for L, S and fagg as mentioned
in the theorem. Next, we deﬁne a learning problem GAL which solves the CSC problem."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.16722408026755853,"5.2
GENERAL AGGREGATE LEARNING (GAL )"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.1705685618729097,"Consider G = (V, E) representing the structured domain under consideration. In (Xu et al., 2019b),
authors showed that GAU framework (equation 1) with an injective fmerge and fagg operations is
equivalent to 1-WL which solves SC problem. We deﬁne a solution to the CSC problem that is
similar in spirit of GAU . However, instead of providing an iterative algorithm like GAU , we pose
an optimization objective which, in essence, learns the stable solution directly."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.17391304347826086,"First, we deﬁne the notations. Recall that the task is to assign continuous labels to graph nodes in
metric space L with distance metric D and the similarity metric is given as S(x, y) = e−D(x,y).
Notation for the embedding matrix E is deﬁned in the same way as that of a coloring function C,
such that E : V →L and E : 2V →{{L}}:"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.17725752508361203,"E : Embedding matrix,
E(u) : embedding of node u ∈V, and
E(V ) = {{E(v)|v ∈V ⊆V}}.
(6)"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.1806020066889632,"E stores the color assignments of all nodes in V and is learned in our setting. Let us look at our ﬁrst
optimization objective which follows naturally from the deﬁnition of CSC ."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.18394648829431437,Under review as a conference paper at ICLR 2022
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.18729096989966554,"Deﬁnition 5.2. (Global GAL formulation) Let matrix E ∈R|V|×d store embeddings (or colors) of
nodes in graph G = (V, E) . Then the optimization objective of Global GAL (G-GAL ) parameter-
ized by an injective function fagg : NL →L (where L = Rd) is to learn an embedding matrix E
that minimizes the following"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.19063545150501673,"E = arg min
E X"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.1939799331103679,"u,v∈V
abs

−lnS
 
E(u), E(v)

+ lnS
 
fagg(E(N(u))), fagg(E(N(v)))
 (7)"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.19732441471571907,"The function fagg can be as simple as a sum operation or as complex as a neural network architecture
with learnable parameters. The global formulation follows naturally from the deﬁnition of CSC
(Rd, S, fagg) problem. Note that we project neighborhood embeddings( or coloring) into the same
space as node embeddings. This will be important for subsequent formulations."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.20066889632107024,"Research Question: Can an effective learning strategy be formulated for minimizing G-GAL
loss? An effective algorithm for G-GAL formulation can stimulate further research. To the best
of our knowledge, we do not know of an algorithm in any domain which uses this formulation.
We think a possible reason why it is difﬁcult to learn is that in most applications, we only have
access to subgraph samples of the true underlying graph. Working with a sample of neighbour-
hood
ˆ
N(u) instead of the complete neighborhood implies we are estimating ˆfagg(E(N(u))). The
errors in S( ˆfagg(E(N(u))), ˆfagg(E(N(v)))) increase super-linearly with error in ˆfagg(E(N(u)).
Nonetheless, this formulation can be of independent interest."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.2040133779264214,"The above mentioned issue of multiplying noisy neighborhood estimates motivates us to ﬁnd a
simpler learning problem. We formulate a different learning problem and show in theorem 5.2
that this simpler problem, which we call Node-local GAL (L-GAL ) problem also solves G-GAL
problem."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.20735785953177258,"Deﬁnition 5.3. (L-GAL formulation). Let matrix E ∈R|V|×d store embeddings (or colors) of nodes
in graph G = (V, E) . Then the optimization objective of L-GAL parameterized by an injective
function fagg : NL →L (where L = Rd) is to learn an embedding matrix E that minimizes the
following"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.21070234113712374,"E = arg min
E X"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.2140468227424749,"v∈V
−lnS(E(v), fagg(E(N(v)))).
(8)"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.21739130434782608,"In the above L-GAL formulation, we uncoupled the G-GAL formulation and thus eliminated the
issue of multiplying noisy neighborhood estimates. However, we would like to emphasize that the
solution to L-GAL is also a good solution to G-GAL . We quantify this relation in the next theorem."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.22073578595317725,"Theorem 5.2. (L-GAL solves G-GAL ). Let the solution E to L-GAL upper-bounds each term in
the summation of loss in equation 8 by some ϵ > 0; thus it upper-bounds total loss by |V|ϵ. Then
the same solution matrix E is a solution to G-GAL with each term in summation upper-bounded by
2ϵ and thus upper-bounding the total loss by |V|2ϵ."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.22408026755852842,"The proof of theorem 5.2 can be found in appendix. Uncoupling works because if you alternately
bring E(u) and E(v) closer to fagg(E(N(u))) and fagg(E(N(v))) respectively, it forces the dis-
tances between (E(u), E(v)) and (fagg(E(N(u))), fagg(E(N(v)))) to be nearly equal."
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.22742474916387959,"5.3
L-GAL WITH SAMPLES FROM G = (V, E)"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.23076923076923078,"Often when trying to solve the L-GAL problem, one will be forced to work on sub-graph samples
of the graph instead of the entire graph. This can happen for multiple practical reasons : (1) True
graph is not known and we have access to the graph only through instances of sub-graphs. For
example, true NLP graph is not known. But we have access to NLP text which hints at the graph. (2)
Performing gradient descent on the entire graph is computationally prohibitive. For example social
networking graphs are massive. In this case one need to sample nodes from V and then sample
the neighborhood of these nodes to estimate fagg(N(u)) for all sampled nodes u. To account for
this practical scenario, we propose an optimization objective for working with sub-graphs. Let X
be the data available for a particular domain, Gx = Vx, Ex be the sub-graph corresponding to the
example x ∈X. Let the neighborhood function restricted to this sub-graph be Nx. We deﬁne the"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.23411371237458195,Under review as a conference paper at ICLR 2022
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.23745819397993312,"optimization objective as,"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.2408026755852843,"E = arg min
E X x∈X X"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.24414715719063546,"v∈Vx
−lnS(E(v), fagg(E( ˆ
Nx(v))))
(9)"
GENERAL AGGREGATE LEARNING FRAMEWORK FOR CSC,0.24749163879598662,"where ˆN(u) is neighborhood induced by the sub-graph. Essentially, we consider each node in the
sub-graph and its induced neighbours ˆ
N from the sub-graph as an example and optimize the loss."
CSC WITH CONSTRAINTS,0.2508361204013378,"5.4
CSC WITH CONSTRAINTS"
CSC WITH CONSTRAINTS,0.25418060200668896,"The problem of CSC , as are most other learning algorithms, is under-speciﬁed (particularly when
fagg is a highly expressive function). Also, sometimes there is additional information that one wants
to induct in the loss function which is not present in graph structure. For example, in commercial
product search settings, “nike” and “adidas” get similar embeddings due to similar neighborhood
graph structure but one might want that the embeddings learned distinguish between these entities.
In such cases, a constraint needs to be imposed in CSC to minimize similarity between such pairs.
This concept is generalized by using a negative sampling function NS : V →2V which deﬁnes a
set of nodes in 2V on which a constraint needs to be imposed for any given node in V. We write the
problem of constrained CSC as follows:"
CSC WITH CONSTRAINTS,0.25752508361204013,"C∗= arg min
C X"
CSC WITH CONSTRAINTS,0.2608695652173913,"u∈V,v∈NS(u)
−ln(1 −S(C(u), C(v)))
subject to the CSC condition:"
CSC WITH CONSTRAINTS,0.26421404682274247,"S(C(u), C(v)) = SN (C(N(u)), C(N(v))) ∀u, v ∈V.
(10)"
CSC WITH CONSTRAINTS,0.26755852842809363,"Using the Lagrangian multiplier we can re-write the constrained optimization objective of L-GAL
as"
CSC WITH CONSTRAINTS,0.2709030100334448,"E = arg min
E
λ
X"
CSC WITH CONSTRAINTS,0.27424749163879597,"v∈V
−lnS(E(v), fagg(E(N(v)))) +
X"
CSC WITH CONSTRAINTS,0.27759197324414714,"v∈V,u∈NS(v)
−ln(1 −S(E(u), E(v)).
(11)"
CSC WITH CONSTRAINTS,0.2809364548494983,"In most applications, λ is set to 1. There are variants of inducing negative loss into a system, e.g.,
softmax (or sampled softmax) is one such popular variant."
OPTIMIZATION ALGORITHM TO SOLVE L-GAL AND ITS VARIANTS,0.2842809364548495,"5.5
OPTIMIZATION ALGORITHM TO SOLVE L-GAL AND ITS VARIANTS"
OPTIMIZATION ALGORITHM TO SOLVE L-GAL AND ITS VARIANTS,0.28762541806020064,"We have developed a series of optimization objectives to solve the problem posed by CSC in the
previous subsections. We close this section by discussing the algorithms used to solve these opti-
mization objectives. We can use any of the standard optimization algorithms: ﬁrst order algorithms
like gradient descent (Courant et al., 1994), stochastic gradient descent (Kiefer & Wolfowitz, 1952)
or second order algorithms like Adam (Kingma & Ba, 2015), Adagrad (Duchi et al., 2011), etc.
When solving problem on complete graph, we denote the algorithm as A( G = (V, E) , S, fagg, NS)
which is parameterized by underlying graph G = (V, E) , similarity metric S, aggregation function
fagg, and negative sampling function NS : V →2V. Whenever working with sub-graph samples
of the true graph G, we denote the algorithm as as As(X, S, fagg, NS) where X is the data, each
element x of which, induces a sub-graph Gx ( The precise deﬁnition of this in provided in section
6.1)"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.2909698996655518,"6
INSTANTIATIONS OF L-GAL IN THE LITERATURE
Firstly, we describe the construction of the graph for a particular domain and sub-graph induction
based on samples from the data. Secondly we summarize the graph construction and instantiations
of L-GAL optimization objective for different embedding models in table 1 and discuss some aspects
of it in section 6.2."
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.29431438127090304,"6.1
GRAPH CONSTRUCTION.
We provide a generic recipe of graph construction on structured domains. Consider for example a
domain with token set T . Our graph will have these tokens as nodes. However, we can introduce
even higher order tokens by combining tokens. For example, the set T 2 will denote a set of all
bi-gram tokens in the domain. We can extend this idea to n-grams by considering the set T n. Thus,
the set of all nodes in the graph of domain can be written as"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.2976588628762542,"V = ∪k
i=1T i,
(12)"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3010033444816054,Under review as a conference paper at ICLR 2022
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.30434782608695654,"Table 1: Examples of instantiation of As for L-GAL for various state-of-the art embedding mod-
els - word2vec (Mikolov et al., 2013a), AWE (Sonkar et al., 2020), BERT (Devlin et al., 2019),
ViT (Dosovitskiy et al., 2020), and Node2vec (Grover & Leskovec, 2016). Note that the token in
BERT/ViT includes the position."
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3076923076923077,"Parameters of As
Word2Vec
AWE
BERT
ViT
Node2Vec
T : Tokens
words
words
word-pieces ×N
(16 x 16 patches) ×N
nodes
Gram-depth
1
1
BERT depth
ViT depth
1
V : Nodes
T
T
∪depth
i=1 T i
∪depth
i=1 T i
T"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3110367892976589,E : Edges
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.31438127090301005,"co-occurence
with freq as
weights"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3177257525083612,"co-occurence
with freq as
weights"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3210702341137124,"co-occurence
with freq as
weights"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.32441471571906355,"co-occurence
with freq as
weights"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3277591973244147,"co-occurence in
random walks
with freq as
weights
S(x, y)
exp (⟨x, y⟩)
1
1+exp (−⟨x,y⟩)
exp (⟨x, y⟩)
exp (⟨x, y⟩)
exp (⟨x, y⟩)
Final loss with
Negative Sampling loss"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3311036789297659,"hierarchical
softmax"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.33444816053511706,"sigmoid
sampled loss
softmax
softmax
sampled
softmax"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3377926421404682,"Negative Sampling
NS(u)
T /{u}"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3411371237458194,"frequency based
sampling
to choose
NS"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.34448160535117056,"T /{u}
T /{u}"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.34782608695652173,"random
sample from
T /{u}"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3511705685618729,"fagg(N(u))
P"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.35451505016722407,"v∈N(u)
w(u,v)E(v)
P"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.35785953177257523,"v∈N(u)
w(u,v)E(v)
P"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3612040133779264,"v∈N(u)
w(u,v)E(v)
P"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.36454849498327757,"v∈N(u)
w(u,v)E(v)
P"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.36789297658862874,"v∈N(u)
w(u,v)E(v)"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3712374581939799,"ˆfagg(N(u))
P"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3745819397993311,"v∈ˆ
N(u)
E(v)
P"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3779264214046823,"v∈ˆ
N(u)
ˆw(u, v)E(v)
P"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.38127090301003347,"v∈ˆ
N(u)
ˆw(u, v)E(v)
P"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.38461538461538464,"v∈ˆ
N(u)
ˆw(u, v)E(v)
P"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3879598662207358,"v∈ˆ
N(u)
E(v)"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.391304347826087,"x ∈X
x = subgraph induced
on G=(V, E) by"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.39464882943143814,"words in a
sentence"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.3979933110367893,"words in a
sentence"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.4013377926421405,"word-pieces
with position
in a sentence"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.40468227424749165,"patches with position
in an image"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.4080267558528428,"nodes in a
random
walk sequence"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.411371237458194,"where k is the gram-depth. The edges in this graph of gram-depth k depend on the application. This
is illustrated in ﬁgure 1a. In most cases, including the models discussed in this paper, the edges are
added on basis of co-occurrence statistics in the data. For example, let us consider the case of NLP.
The sentence “The tree fell” in a graph with a gram-depth of 3 will include the nodes {the, tree,
fell, {the, tree}, {the, fell}, {tree, fell}, {the, tree, fell}}. If the edges are added on the basis of co-
occurrence, then based on the above sentence we add the edges between “The” and “fell,” “The tree”
and “fell”, “The fell” and “tree,” and so on. Additionally, edges can have weights corresponding to
the frequency of co-occurrence."
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.41471571906354515,"While the graph on tokens represents the true graph, the algorithm As which solves the sampled
version of L-GAL problem uses the sub-graph of G = (V, E) which is induced by the sample.
Considering the sample x ∈X as a set of tokens x ⊆T , the graph induced by this set of tokens,
Gx = (Vx, Ex) with neighborhood function Nx where"
INSTANTIATIONS OF L-GAL IN THE LITERATURE,0.4180602006688963,"Vx = {v|v ∈∪k
i=1xi, v ∈V},
Ex = {(u, v)|u, v ∈∪k
i=1xi, (u, v) ∈E}.
(13)"
CURRENT STATE-OF-THE-ART EMBEDDING MODELS ARE INSTANTIATIONS OF L-GAL,0.4214046822742475,"6.2
CURRENT STATE-OF-THE-ART EMBEDDING MODELS ARE INSTANTIATIONS OF L-GAL
Now, we summarize the instantiations of As for variety of algorithms in domains of NLP, images,
and graphs. For details on working of each algorithm, we direct the reader to the corresponding
original papers. We want to point out the usage of exp⟨x, y⟩
 
or
1
1+exp (−⟨x,y⟩)

as similarity
metrics. Under the unit-norm assumption on x and y, a standard assumption for analysis of embed-
dings, exp ⟨x, y⟩= exp (1 −D2(x, y)/2) which is only a function of the l2 norm D and inversely
proportional to it. Thus, exp⟨x, y⟩( or
1
1+exp (−⟨x,y⟩) ) is a valid similarity metric under unit-norm
assumption."
CURRENT STATE-OF-THE-ART EMBEDDING MODELS ARE INSTANTIATIONS OF L-GAL,0.42474916387959866,"6.2.1
WORD2VEC, NODE2VEC, AWE
Theorem 6.1. The algorithm As for L-GAL initialized with parameters from table 1 for Word2Vec,
Node2Vec and AWE leads to exactly the algorithms (with possibly minor variations) as proposed in
the original papers of Word2Vec, Node2Vec and AWE"
CURRENT STATE-OF-THE-ART EMBEDDING MODELS ARE INSTANTIATIONS OF L-GAL,0.4280936454849498,"The proof of the above theorem is quite straightforward and is provided in the appendix. The for-
mulation of sampled L-GAL problem provides a natural explanation of why AWE performs better
than Word2Vec. Both Word2Vec and AWE operate on the same graph G = (V, E) . However, they
compute their estimates of fagg( ˆ
N(u)) differently. While word2vec performs simple sum, AWE
performs weighted sum. One can prove that the MSE error with simple sum is larger than with
weighted sum when weights are equal to the co-occurrence frequency. AWE models these weights
as these weights are not known apriori and the authors in the paper hint at the learned weights being
representative of the these co-occurrence frequencies."
CURRENT STATE-OF-THE-ART EMBEDDING MODELS ARE INSTANTIATIONS OF L-GAL,0.431438127090301,Under review as a conference paper at ICLR 2022
CURRENT STATE-OF-THE-ART EMBEDDING MODELS ARE INSTANTIATIONS OF L-GAL,0.43478260869565216,"(a) Graph Construction : higher
order tokens in a graph"
CURRENT STATE-OF-THE-ART EMBEDDING MODELS ARE INSTANTIATIONS OF L-GAL,0.43812709030100333,(b) Illustration of how Bert/ ViT follows L-GAL framework
BERT AND VIT,0.4414715719063545,"6.2.2
BERT AND VIT
Theorem 6.2. Considering BERT and ViT as stacks of attention layers without any non-linearity
and initializing the algorithm As with the parameters mentioned in the table 1 leads to the algorithms
(with possibly minor variations) as proposed in the original papers of BERT and ViT."
BERT AND VIT,0.44481605351170567,"The proof of the theorem is provided in the appendix. For this analysis, we will assume that BERT
is a stack of attention layers without any non-linearity. BERT and ViT have similar architecture and
hence we provide a single proof for them. Let us consider the sub-graph induced for BERT by a
sample x, which means it essentially has the nodes ∪k
i=1xi. The sub-graph will have every node
connect with every other node. Thus when we look at the embedding of a particular masked word,
according to deﬁnition of fagg for BERT we will get,"
BERT AND VIT,0.44816053511705684,"fagg(N(u)) =
X"
BERT AND VIT,0.451505016722408,"v∈N(u)
(w(u,v)E(v)),
ˆfagg(N(u)) =
X"
BERT AND VIT,0.45484949832775917,"v∈N(u)∩(∪k
i=1xi)
(α(u,v)E(v)).
(14)"
BERT AND VIT,0.45819397993311034,"Now if this is exactly the computation that BERT (or ViT) performs then with softmax style neg-
ative sampling loss, BERT will follow the L-GAL optimization. We claim that BERT does indeed
compute this ˆfagg. Ideally, BERT should learn the embedding matrix for all the nodes including
tokens and higher order tokens. However, this is computationally expensive. Hence, BERT actually
models the higher order tokens in terms of its component tokens. It can be seen that if the modelling
is a weighted linear sum of the components, then BERT essentially ends up computing equation 14.
More details can be found in appendix. Also, as in AWE it models the weights in the summation of
fagg (via the attention mechanism). The information propagated through BERT can be visualized
as shown in ﬁgure 1b. At each layer i ∈{1, .., k}, BERT computes the token embeddings of order i
and this information from each layer ﬂows to the masked word node and gets aggregated."
IMPLICATIONS AND CONCLUSIONS,0.46153846153846156,"7
IMPLICATIONS AND CONCLUSIONS"
IMPLICATIONS AND CONCLUSIONS,0.46488294314381273,"In this paper, we deﬁne a notion of CSC to understand the internals of wide range of embedding
models across diverse structured domains. Grounded on this notion, we propose a generalized L-
GAL optimization problem that solves for CSC on graphs induced by any structured domain. We
thereby prove equivalence between loss functions of popular NLP, image, and graph embedding
models and our proposed constrained L-GAL optimization loss operating on domain-speciﬁc graphs.
The methodology of construction of these graphs is also presented for each structured domain."
IMPLICATIONS AND CONCLUSIONS,0.4682274247491639,"Our proposed framework with its robust mathematical founding on CSC us graph theoretical per-
spective to these unsupervised embedding models. We have already seen great improvements in
supervised models based on the connection between 1-WL and GNNs. We believe that our for-
mulation will stimulate further research in embedding models. Insights from the underlying graph
view of the computation in embedding models can give us new directions to improve quality and
scalability and training efﬁciency of these models."
IMPLICATIONS AND CONCLUSIONS,0.47157190635451507,Under review as a conference paper at ICLR 2022
REFERENCES,0.47491638795986624,REFERENCES
REFERENCES,0.4782608695652174,"Carl Allen and Timothy Hospedales. Analogies explained: Towards understanding word embed-
dings. In International Conference on Machine Learning, pp. 223–231. PMLR, 2019."
REFERENCES,0.4816053511705686,"Carl Allen, Ivana Balazevic, and Timothy Hospedales. What the vec? towards probabilistically
grounded embeddings.
Advances in Neural Information Processing Systems, 32:7467–7477,
2019."
REFERENCES,0.48494983277591974,"Suyash P Awate and Ross T Whitaker. Unsupervised, information-theoretic, adaptive image ﬁltering
for image restoration. IEEE Transactions on pattern analysis and machine intelligence, 28(3):
364–376, 2006."
REFERENCES,0.4882943143812709,"L´aszl´o Babai and Ludik Kucera. Canonical labelling of graphs in linear average time. In 20th
Annual Symposium on Foundations of Computer Science (sfcs 1979), pp. 39–46. IEEE, 1979."
REFERENCES,0.4916387959866221,"Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew Zhai, and Dmitry Kislyuk. Toward
transformer-based object detection. ArXiv, abs/2012.09958, 2020."
REFERENCES,0.49498327759197325,"Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with
subword information. Transactions of the Association for Computational Linguistics, 5:135–146,
2017."
REFERENCES,0.4983277591973244,"Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image denoising. In 2005
IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05),
volume 2, pp. 60–65. IEEE, 2005."
REFERENCES,0.5016722408026756,"Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on
graph classiﬁcation. arXiv preprint arXiv:1905.04579, 2019."
REFERENCES,0.5050167224080268,"Richard Courant et al. Variational methods for the solution of problems of equilibrium and vibra-
tions. Lecture notes in pure and applied mathematics, pp. 1–1, 1994."
REFERENCES,0.5083612040133779,"Scott C Deerwester, Susan T Dumais, George W Furnas, Richard A Harshman, Thomas K Landauer,
Karen E Lochbaum, and Lynn A Streeter. Computer information retrieval using latent semantic
structure, June 13 1989. US Patent 4,839,853."
REFERENCES,0.5117056856187291,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019."
REFERENCES,0.5150501672240803,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.5183946488294314,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011."
REFERENCES,0.5217391304347826,"Kawin Ethayarajh, David Duvenaud, and Graeme Hirst. Towards understanding linear word analo-
gies. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-
tics, pp. 3253–3262, Florence, Italy, July 2019. Association for Computational Linguistics. doi:
10.18653/v1/P19-1315. URL https://aclanthology.org/P19-1315."
REFERENCES,0.5250836120401338,"Martin Grohe. Word2vec, node2vec, graph2vec, x2vec: Towards a theory of vector embeddings
of structured data. In Proceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Symposium on
Principles of Database Systems, PODS’20, pp. 1–16, New York, NY, USA, 2020. Association
for Computing Machinery. ISBN 9781450371087. URL https://doi.org/10.1145/
3375395.3387641."
REFERENCES,0.5284280936454849,Under review as a conference paper at ICLR 2022
REFERENCES,0.5317725752508361,"Aditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for networks. In Pro-
ceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, KDD ’16, pp. 855–864, New York, NY, USA, 2016. Association for Com-
puting Machinery.
ISBN 9781450342322.
doi: 10.1145/2939672.2939754.
URL https:
//doi.org/10.1145/2939672.2939754."
REFERENCES,0.5351170568561873,"William L Hamilton, Rex Ying, and Jure Leskovec.
Inductive representation learning on large
graphs. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, pp. 1025–1035, 2017."
REFERENCES,0.5384615384615384,"Zellig S Harris. Distributional structure. Word, 10(2-3):146–162, 1954."
REFERENCES,0.5418060200668896,"Tatsunori B Hashimoto, David Alvarez-Melis, and Tommi S Jaakkola. Word embeddings as metric
recovery in semantic spaces. Transactions of the Association for Computational Linguistics, 4:
273–286, 2016."
REFERENCES,0.5451505016722408,"J. Kiefer and J. Wolfowitz. Stochastic estimation of the maximum of a regression function. The
Annals of Mathematical Statistics, 23(3):462–466, 1952. ISSN 00034851. URL http://www.
jstor.org/stable/2236690."
REFERENCES,0.5484949832775919,"Diederik P. Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980."
REFERENCES,0.5518394648829431,"Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Interna-
tional conference on machine learning, pp. 1188–1196. PMLR, 2014."
REFERENCES,0.5551839464882943,"Omer Levy and Yoav Goldberg.
Neural word embedding as implicit matrix factoriza-
tion.
In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger
(eds.), Advances in Neural Information Processing Systems, volume 27. Curran Asso-
ciates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/
feab05aa91085b7a8012516bc3533958-Paper.pdf."
REFERENCES,0.5585284280936454,"Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Topical word embeddings. In AAAI,
2015."
REFERENCES,0.5618729096989966,"Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
bb04af0f7ecaee4aae62035497da1387-Paper.pdf."
REFERENCES,0.5652173913043478,"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word represen-
tations in vector space. In Yoshua Bengio and Yann LeCun (eds.), 1st International Conference
on Learning Representations, ICLR 2013, 2013a."
REFERENCES,0.568561872909699,"Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, pp. 3111–3119, 2013b."
REFERENCES,0.5719063545150501,"Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In
International workshop on artiﬁcial intelligence and statistics, pp. 246–252. PMLR, 2005."
REFERENCES,0.5752508361204013,"Christopher Morris, Kristian Kersting, and Petra Mutzel. Glocalized weisfeiler-lehman graph ker-
nels: Global-local feature maps of graphs.
In 2017 IEEE International Conference on Data
Mining (ICDM), pp. 327–336, 2017. doi: 10.1109/ICDM.2017.42."
REFERENCES,0.5785953177257525,"Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gau-
rav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural net-
works. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 33(01):4602–4609, Jul.
2019. doi: 10.1609/aaai.v33i01.33014602. URL https://ojs.aaai.org/index.php/
AAAI/article/view/4384."
REFERENCES,0.5819397993311036,Under review as a conference paper at ICLR 2022
REFERENCES,0.5852842809364549,"Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and leman go sparse: Towards
scalable higher-order graph embeddings. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
21824–21840. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.
cc/paper/2020/file/f81dee42585b3814de199b2e88757f5c-Paper.pdf."
REFERENCES,0.5886287625418061,"Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and leman go sparse: Towards
scalable higher-order graph embeddings. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
21824–21840. Curran Associates, Inc., 2020b. URL https://proceedings.neurips.
cc/paper/2020/file/f81dee42585b3814de199b2e88757f5c-Paper.pdf."
REFERENCES,0.5919732441471572,"Christopher Morris, Matthias Fey, and Nils Kriege. The power of the weisfeiler-leman algorithm
for machine learning with graphs. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth Interna-
tional Joint Conference on Artiﬁcial Intelligence, IJCAI-21, pp. 4543–4550. International Joint
Conferences on Artiﬁcial Intelligence Organization, 8 2021. Survey Track."
REFERENCES,0.5953177257525084,"Korawit Orkphol and Wu Yang. Word sense disambiguation using cosine similarity collaborates
with word2vec and wordnet. Future Internet, 11(5):114, 2019."
REFERENCES,0.5986622073578596,"Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532–1543, 2014a."
REFERENCES,0.6020066889632107,"Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532–1543, 2014b."
REFERENCES,0.6053511705685619,"Magnus Sahlgren. The distributional hypothesis. The Italian Journal of Linguistics, 20:33–54, 2008."
REFERENCES,0.6086956521739131,"Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borg-
wardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011."
REFERENCES,0.6120401337792643,"Shashank Sonkar, Andrew Waters, and Richard Baraniuk. Attention word embedding. In Pro-
ceedings of the 28th International Conference on Computational Linguistics, pp. 6894–6902,
Barcelona, Spain (Online), December 2020. International Committee on Computational Linguis-
tics. doi: 10.18653/v1/2020.coling-main.608. URL https://aclanthology.org/2020.
coling-main.608."
REFERENCES,0.6153846153846154,"Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale hetero-
geneous text networks. In Proceedings of the 21th ACM SIGKDD international conference on
knowledge discovery and data mining, pp. 1165–1174, 2015."
REFERENCES,0.6187290969899666,"Bruna Thalenberg. Distinguishing antonyms from synonyms in vector space models of semantics.
Technical report, Technical report, 2016."
REFERENCES,0.6220735785953178,"Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra
which appears therein. NTI, Series, 2(9):12–16, 1968."
REFERENCES,0.6254180602006689,"Hu Xu, Bing Liu, Lei Shu, and Philip Yu.
BERT post-training for review reading compre-
hension and aspect-based sentiment analysis.
In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 2324–2335, Minneapolis, Minnesota,
June 2019a. Association for Computational Linguistics.
doi: 10.18653/v1/N19-1242.
URL
https://aclanthology.org/N19-1242."
REFERENCES,0.6287625418060201,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
How powerful are graph neural
networks?
In International Conference on Learning Representations, 2019b. URL https:
//openreview.net/forum?id=ryGs6iA5Km."
REFERENCES,0.6321070234113713,"Guido Zuccon, Bevan Koopman, Peter Bruza, and Leif Azzopardi. Integrating and evaluating neural
word embeddings in information retrieval. In Proceedings of the 20th Australasian document
computing symposium, pp. 1–8, 2015."
REFERENCES,0.6354515050167224,Under review as a conference paper at ICLR 2022
REFERENCES,0.6387959866220736,"A
APPENDIX"
REFERENCES,0.6421404682274248,"A.1
A WALK THROUGH POPULAR EMBEDDING MODELS AND UNDERSTANDING THEIR
DESIGN PRINCIPLES AGAINST THE BACKDROP OF OUR FRAMEWORK"
REFERENCES,0.6454849498327759,"The proposed L-GAL formulation abstracts out different components of embedding models and thus
enables design of better embedding models by focusing on these components. The components are
as follows,"
REFERENCES,0.6488294314381271,• Domain Graphs
REFERENCES,0.6521739130434783,• fagg Neighborhood aggregation function.
REFERENCES,0.6555183946488294,• NS : Negative Sampling set.
REFERENCES,0.6588628762541806,• Estimation of fagg(N(u)) via sampling.
REFERENCES,0.6622073578595318,"One can think of the strategies focusing on these components as ,"
REFERENCES,0.6655518394648829,1. Better sampling/ estimates of a node’s neighborhoods.
REFERENCES,0.6688963210702341,2. Better design of fagg function.
REFERENCES,0.6722408026755853,3. Construction of more expressive graphs that embed higher order tokens.
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.6755852842809364,"4. Importance based negative sampling to inject knowledge that best negates the neighbor
information given by our proposed domain graph."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.6789297658862876,"With these four strategies in place, next we understand the development of NLP models over time
by choosing word2vec, one of the most widely-used embedding model, as our starting reference.
We demonstrate how our proposed strategies have manifested in different embedding models that
have improved over word2vec in last decade."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.6822742474916388,"Recall from table 1, that word2vec uses"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.68561872909699,"• G = (V, E) where V is the set of all single words as nodes and E is the edge between
words, say u and v, with co-occurrence as the weight."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.6889632107023411,• fagg(N(u)) weighted sum of neighbour embeddings
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.6923076923076923,• NS(u): All words except u
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.6956521739130435,• Sampling technique : sentences
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.6989966555183946,"Strategy 1 : Better estimation of fagg(N(u)) - Glove vs Word2vec
Word2vec is an iterative
algorithm over sentences. It uses sentence as a sample to guess word’s neighborhood which induces
a lot of noise. Our framework would suggest to use the entire computation of fagg instead of
sampling if possible for a given fagg and graph. Glove does exactly this. Glove (Pennington et al.,
2014a) improves upon word2vec by using global co-occurrence statistics to eliminate noise in these
neighborhood estimates. Glove’s optimization objective (given by equation 12 which is simpliﬁed
to equation 15 in original paper) is as follows: J = V
X i=1 V
X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7023411371237458,"j=1
Xij log Qij,"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.705685618729097,"where V is the vocabulary, Xij is the co-occurrence count of word i with word j in the corpus,"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7090301003344481,"and Qij =
exp wT
i wj
PV
k=1 exp wT
i wk . Note that Glove simpliﬁes Qij = exp wT
i wj which is identical to the
similarity kernel we use in our analysis with unit norm assumption."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7123745819397993,"Strategy 1, thus, helps us to understand why Glove improves upon word2vec."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7157190635451505,"Strategy 2 : Better fagg - AWE vs Word2vec
Let us now consider another word embedding
model, AWE (Attention Word Embedding), that improves upon the CBOW model of word2vec.
CBOW model weighs each word equally to compute the context embedding, however, AWE weighs"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7190635451505016,Under review as a conference paper at ICLR 2022
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7224080267558528,"each context word differently as some context words are more important for prediction of the masked
word than others. The context vector in AWE is given by:"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.725752508361204,"cawe =
X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7290969899665551,"i∈[−b,b]−{0}
awiuwi where awi = exp
 
kT
w0qwi

,"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7324414715719063,"where where b is the size of the context window and wi is the index of each word (w0 is the index of
the masked word; the rest are the indices of the context words), awi is the attention weight of each
context word vector uwi calculated using the key matrix K and the query matrix Q. Note that in
word2vec awi = 1."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7357859531772575,"Strategy 2 helps us understand why AWE outperforms Word2vec. AWE formulates a better fagg
function as compared to Word2Vec."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7391304347826086,"Strategy 3 : More informative domain graphs - BERT vs Word2vec
As discussed in section ??,
BERT uses a graph with higher order tokens. In fact, the highest order of token can be interpreted as
the number of layers in ther BERT. It also uses the AWE style weighting of the neighbours. Strategy
3 (along with strategy 2) can help us understand how BERT improves over Word2vec embedding
model. Strategy 3 can also help us understand why (Le & Mikolov, 2014; Tang et al., 2015; Liu
et al., 2015) can improve over Word2Vec."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7424749163879598,"Strategy 4 : Better NS - WSD-W2V vs Word2Vec
Strategy 4 helps to understand yet another
shortcoming of word2vec. Word2vec struggles with modeling antonyms and often brings antonyms
close to each other (Thalenberg, 2016). This is not surprising since antonyms tend to occur in
similar contexts, i.e., identical neighborhoods. With our proposed framework, one can arrive at
this conclusion from a graph theoretic perspective. Antonym subgraphs are isomorphic, and thus
antonyms get assigned similar continuous stable colors. To overcome this, one needs to introduce
information into the setup using variants of negative sampling. Thus external datasets like wordnet
(Orkphol & Yang, 2019) need to complement word2vec’s training strategy for efﬁcient modeling of
antonyms."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.745819397993311,"To conclude, the model design changes that our framework suggest (strategies 1-4) can be found in
existing works."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7491638795986622,"A.2
THEOREM 4.1"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7525083612040134,"Let {w1, w2, .., wn} be words in vocabulary, and M ∈Rn×n is a co-occurrence matrix with entries
eij ∈N containing the number of times wi co-occurs with wj within a ﬁxed context window. Let
there be a function f : w →N which takes a word and maps it to a color c ∈N (assigns meaning
in accordance with Distributional Hypothesis), such that f(wi) = f(wj) only if row i is same as
row j in matrix M. Construct a graph GDH with words as nodes and its adjacency matrix given by
M. Then, function f deﬁnes a stable coloring on graph GDH."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7558528428093646,Proof: The proof follows trivially from the deﬁnition of f.
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7591973244147158,"A.3
THEOREM 4.2"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7625418060200669,"Consider a discrete signal y(x) sampled at n points xi (i = 1, ..., n), and let the sequence p(xi) =
(y(xi−t), .., y(xi−1), y(xi+1), .., y(xi+t)) be a patch of neighborhood values around each xi for
some context window length t. NLM denoises the signal y(xi), i = 1, ..., n with iterative updates.
The ﬁxed point denoised version of the signal yd can be written as follows:"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7658862876254181,"yd(xi) =
1
D(xi) n
X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7692307692307693,"j=1
K(pd(xi), pd(xj))yd(xj),
(15)"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7725752508361204,"where D(xi) = Pn
j=1 K(yd(xi), yd(xj)) and K is an arbitrary kernel function. Let the graph
Gnlm = (Vnlm, Enlm) where each xi is a node ui in Vnlm and each pair (ui, uj) with |i −j| ≤t
is represented as a directed edge in Enlm with label (i −j). Then the ﬁxed point solution of NLM
with Kronecker delta kernel Kδ, yd : N →R deﬁnes a weak-stable coloring over the graph Gnlm"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7759197324414716,Under review as a conference paper at ICLR 2022
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7792642140468228,Proof:
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.782608695652174,"In order to prove a weak coloring, we need to prove that if the neighbourhoods of a pixel are
equal then its value is equal. Consider any two pixels xi and xj with same neighbourhoods, Then
Kδ(pd(xi), pd(xj) = 1. Let {xk1, xk2, xk3, ...} be the set of size L of pixels whose neighbourhoods
that match with neighbourhood of xi and xj. Thus, by equation of ﬁnal stable solution, both the
pixel values are equal to yd(xi) = yd(xj) = 1"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7859531772575251,"L
PL
l=1 Kδ(pd(xi), pd(xkl))yd(xkl)"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7892976588628763,"A.4
THEOREM 5.1"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7926421404682275,"(SC is a special case of CSC ). Stable coloring (discrete) problem is an instance of continuous stable
coloring problem with L = N, S(i, j) = 1(i = j) and SN (s1, s2) = 1(s1 = s2) where i, j ∈N
and s1, s2 ∈NN. In this case fagg : NN →N function is essentially an injective hash function
which maps multi-subsets of N to N."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7959866220735786,"Proof: The proof of the theorem trivially follows by using the parameters of CSC as mentioned in
the theorem."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.7993311036789298,"A.5
THEOREM 5.2"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.802675585284281,"Let the solution E to L-GAL upper-bounds each term in the summation of loss in equation 8 by
some ϵ > 0; thus it upper-bounds total loss by |V|ϵ. Then the same solution matrix E is a solution to
G-GAL with each term in summation upper-bounded by 2ϵ and thus upper-bounding the total loss
by |V|2ϵ."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8060200668896321,Proof:
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8093645484949833,Figure 2: Illustration of theorem 5.2
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8127090301003345,"Loss(G-GAL ) =
X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8160535117056856,"u,v∈V
abs(−ln S(E(u), E(v)) + ln S(fagg(E(N(u))), fagg(E(N(v))))) (16) =
X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8193979933110368,"u,v∈V
abs(D(E(u), E(v)) −D(fagg(E(N(u))), fagg(E(N(v)))))
(17)"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.822742474916388,"As D is a metric, we have"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8260869565217391,"D(fagg(E(N(u))), fagg(E(N(v)))) ≤D(fagg(E(N(u))), E(u))+D(E(u), E(v))+D(E(v), fagg(E(N(v))))
(18)
D(fagg(E(N(u))), fagg(E(N(v))))−D(E(u), E(v)) ≤D(fagg(E(N(u))), E(u))+D(E(v), fagg(E(N(v))))
(19)"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8294314381270903,Under review as a conference paper at ICLR 2022
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8327759197324415,"By hypothesis, that the each term in the loss of L-GAL is bound by ϵ"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8361204013377926,"D(fagg(E(N(u))), fagg(E(N(v)))) −D(E(u), E(v)) ≤2ϵ
(20)"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8394648829431438,"Thus ,"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.842809364548495,"Loss(G-GAL ) ≤
X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8461538461538461,"u,v∈V
2ϵ = |V|(|V| −1)ϵ ≤|V|2ϵ
(21)"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8494983277591973,"A.6
THEOREM 6.1"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8528428093645485,"Theorem A.1. The algorithm As for L-GAL initialized with parameters from table 1 for Word2Vec,
Node2Vec and AWE leads to exactly the algorithms (with possibly minor variations) as proposed in
the original papers of Word2Vec, Node2Vec and AWE"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8561872909698997,"proof It is easy to check that the parameters speciﬁed in the table 1 exactly provides the algorithms
of corresponding models. Some more details can be found in appendix B"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8595317725752508,"A.7
THEOREM 6.2"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.862876254180602,"Considering BERT and ViT as stacks of attention layers without any non-linearity and initializing
the algorithm As with the parameters mentioned in the table 1 leads to the algorithms (with possibly
minor variations) as proposed in the original papers of BERT and ViT with the linear modelling on
embedding of higher order tokens."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8662207357859532,"Proof: The parameters of BERT / ViT such as loss used, tokens can be veriﬁed from the table. It
only remains to prove that BERT computation is same as that of fagg mentioned."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8695652173913043,"This is done under two assumptions, (1) BERT is just a stack of attention layers. (2) each higher
order token’s embedding is modelled as a linear model of it constituent embeddings."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8729096989966555,"Let us consider the token (u, i) to be masked. Without loss of generality let the position in the token
be last position. Let each path reaching the level k at masked word be denoted with the locations
of the words the path takes at each level. So a path of (0,1,0,2) says that it takes the route of words
w0, w1, w0, w2 to reach the masked word at the ﬁnal level."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8762541806020067,Then we can write the new computation of reaching BERT at level k as X
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8795986622073578,"p∈P AT HS(k) k
X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.882943143812709,"i=1
αipE(w[p(i)])
(22)"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8862876254180602,"This can be seen as the embedding of the path as BERT models it (linear model assumption). It is
easy to see that each path of length k maps one-to-one to a higher order token in T k. Under this
relation PATHS(k) is exactly the set of all nodes in Sk The information also ﬂows from masked
words which aggregates the embeddings of Si, i < k. Thus the ﬁnal computation of bert at level K
looks like K
X k=1 X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8896321070234113,"p∈P AT HS(k) k
X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8929765886287625,"i=1
αipE(w[p(i)]) =
X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8963210702341137,"v∈SK
βvE(v)
(23)"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.8996655518394648,for some βs
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.903010033444816,"B
EXAMPLES OF GRAPH CONSTRUCTION"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9063545150501672,"B.1
VARIOUS DOMAINS AS GRAPHS"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9096989966555183,"Any general set of tokens say T and associated relation R ⊆T ×T can give us the graph G = (V, E)
with V = T and (u, v) ∈E iff R(u, v) = 1. While the underlying graph of NLP is not explicitly
known some examples of relations that can be drawn from the sentence structure are as follows."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9130434782608695,Under review as a conference paper at ICLR 2022
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9163879598662207,"• Let V be the set of all words {w1, w2, ...w|V|}. (w1, w2) ∈E iff the words w1 and w2 co-occur in a
sentence. Additionally, we can assign a weight to the edge equal to the frequency of co-occurrence
observed in some natural language corpus.
• Let V be a set of all subsets of T of size less than or equal to k. Let t1, t2 ∈V then the relation
(t1, t2) ∈E iff t1 ∪t2 appear together in some sentence in natural language."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.919732441471572,"Similarly we can also view the entire image collection as a graph G = (V, E) where V is a set of
all k × k patches observed in all images ( or alternatively images of a particular category ). Some
examples of relations might be,"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9230769230769231,"• R(p1, p2) = 1 iff p1 occurs in a larger patch K × K around p2 in some images
• R(p1, p2) = 1 iff p1 adjacent to p2 in four directions (left, right, top, down)"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9264214046822743,"Graph domain naturally maps to the graph data structure. However, we can construct more informa-
tive graphs from the basic graph structure. For example, we can add hyper-edges to add hyper-graph
structure to the same graph."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9297658862876255,Under review as a conference paper at ICLR 2022
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9331103678929766,"B.2
WORD2VEC"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9364548494983278,"Word2vec is one of the standard word embedding models (Mikolov et al., 2013a;b). The continuous
bag-of-words (CBOW) model of word2vec predicts a masked word using its context. Let E =
[v1, ..., vN]T ∈RN×d be the word embedding matrix where N is the size of the vocabulary, and d
is the size of the word vector. CBOW looks at the set of sentences drawn from NLP corpus X and
uses it to learn E. For each sentence x of size n, CBOW creates n examples of (u, ctx(u) = {v|v ∈
x, v ̸= u}) pairs by choosing one word and using rest of the words as context ctx."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.939799331103679,"It uses the following formulation,"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9431438127090301,"E = arg min
E X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9464882943143813,"(u,ctx(u)∈x∈X) "
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9498327759197325,"−ln
1
1 + exp (−⟨E(u), E(ctx(u))⟩)+
(24) X"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9531772575250836,"v∈NS(u)
−ln

1 −
1
1 + exp (−⟨E(v), E(ctx(u))⟩)  ! (25)"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9565217391304348,where E(ctx(u)) = P
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.959866220735786,"v∈ctx(u) E(v). Note that original formulation of CBOW uses softmax which
is a variant of the negative sampling loss formulation."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9632107023411371,"We will show in theorem ?? that Word2Vec is indeed same as As using some speciﬁc initialization
of its parameters. We ﬁrst deﬁne the parameters."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9665551839464883,"• Graph Gw2v. Let {w1, w2, .., w|V |} be words in vocabulary V , and M ∈R|V |×|V | is a co-
occurrence matrix with entries eij ∈N containing the number of times wi co-occurs with wj
within a ﬁxed context window. Construct a graph Gw2v with words as nodes and its adjacency
matrix given by M
• Data Xw2v. Every example is a set of words (sentence) and the graph induced by these words is
a sub-graph in Gw2v with every node connecting every other node.
• Aggregation function fw2v. Let fagg(ctx(u)) = P"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9698996655518395,v∈ctx(u) E(v).
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9732441471571907,"• Similarity Metric Ssgm. Let S(x, y) be σ(x, y). Under unit-norm assumption, this is a valid
similarity metric derived from l2 norm distance metric."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9765886287625418,"B.3
ATTENTION WORD EMBEDDINGS (AWE)"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.979933110367893,"As can be observed in fw2v, CBOW model equally weights the context words when making a
prediction. To address this limitation of CBOW, AWE augments CBOW model with an attention
mechanism to attend to context words that are most relevant for prediction of masked word. AWE
also uses the same formulation as word2vec given by equation 25, except that context vector is a
weighted sum of context word embeddings."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9832775919732442,"In AWE, E(ctx(u)) = P
v∈ctx(u) wuvE(v), where wuv = exp (< ku, qv >) models the impor-
tance of context word v for predicting the masked word u using key and query word embedding
matrices given by K = [k1, ..., kN]T ∈RN×d and Q = [q1, ..., qN]T ∈RN×d respectively."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9866220735785953,"As we did for word2vec, we will show in theorem ?? that AWE is indeed same as As using some
speciﬁc initialization of its parameters. We ﬁrst deﬁne the parameters."
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9899665551839465,"• Graph Gawe. Gawe = Gw2v.
• Data Xawe. Similar to word2vec, every example is a set of words (sentence) but unlike the case
of word2vec, the graph induced by these words is a weighted sub-graph in Gawe.
• Aggregation function fawe. Let fagg(ctx(u)) = P"
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9933110367892977,v∈ctx(u) wuvE(v).
IMPORTANCE BASED NEGATIVE SAMPLING TO INJECT KNOWLEDGE THAT BEST NEGATES THE NEIGHBOR,0.9966555183946488,"• Similarity Metric Ssgm. Let S(x, y) be σ(x, y). Under unit-norm assumption, this is a valid
similarity metric derived from l2 norm distance metric."
