Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018115942028985507,"Whitening loss provides theoretical guarantee in avoiding feature collapse for
1"
ABSTRACT,0.0036231884057971015,"self-supervised learning (SSL) using joint embedding architectures. One typical
2"
ABSTRACT,0.005434782608695652,"implementation of whitening loss is hard whitening that designs whitening transfor-
3"
ABSTRACT,0.007246376811594203,"mation over embedding and imposes the loss on the whitened output. In this paper,
4"
ABSTRACT,0.009057971014492754,"we propose spectral transformation (ST) framework to map the spectrum of embed-
5"
ABSTRACT,0.010869565217391304,"ding to a desired distribution during forward pass, and to modulate the spectrum
6"
ABSTRACT,0.012681159420289856,"of embedding by implicit gradient update during backward pass. We show that
7"
ABSTRACT,0.014492753623188406,"whitening transformation is a special instance of ST by definition, and there exist
8"
ABSTRACT,0.016304347826086956,"other instances that can avoid collapse by our empirical investigation. Furthermore,
9"
ABSTRACT,0.018115942028985508,"we propose a new instance of ST, called IterNorm with trace loss (INTL). We
10"
ABSTRACT,0.019927536231884056,"theoretically prove that INTL can avoid collapse and modulate the spectrum of
11"
ABSTRACT,0.021739130434782608,"embedding towards an equal-eigenvalue distribution during the course of opti-
12"
ABSTRACT,0.02355072463768116,"mization. Moreover, INTL achieves 76.6% top-1 accuracy in linear evaluation on
13"
ABSTRACT,0.025362318840579712,"ImageNet using ResNet-50, which exceeds the performance of the supervised base-
14"
ABSTRACT,0.02717391304347826,"line, and this result is obtained by using a batch size of only 256. Comprehensive
15"
ABSTRACT,0.028985507246376812,"experiments show that INTL is a promising SSL method in practice.
16"
INTRODUCTION,0.030797101449275364,"1
Introduction
17"
INTRODUCTION,0.03260869565217391,"Self-supervised learning (SSL) via joint embedding architectures to learn visual representations has
18"
INTRODUCTION,0.034420289855072464,"made significant progress over the last several years [1, 18, 7, 9, 2, 30], almost outperforming their
19"
INTRODUCTION,0.036231884057971016,"supervised counterpart on many downstream tasks [28, 23, 32]. This paradigm addresses to train
20"
INTRODUCTION,0.03804347826086957,"a dual pair of networks to produce similar embeddings for different views of the same image [9].
21"
INTRODUCTION,0.03985507246376811,"One main challenge with the joint embedding architectures is how to prevent a collapse of the
22"
INTRODUCTION,0.041666666666666664,"representation, in which the two branches ignore the inputs and produce identical and constant
23"
INTRODUCTION,0.043478260869565216,"outputs [9]. A variety of methods have been proposed to successfully avoid collapse, including
24"
INTRODUCTION,0.04528985507246377,"contrastive learning methods [41, 18, 34] that attract different views from the same image (positive
25"
INTRODUCTION,0.04710144927536232,"pairs) while pull apart different images (negative pairs), and non-contrastive methods [16, 9] that
26"
INTRODUCTION,0.04891304347826087,"directly match the positive targets without introducing negative pairs.
27"
INTRODUCTION,0.050724637681159424,"The collapse problem is further generalized into dimensional collapse [20, 24] (or informational
28"
INTRODUCTION,0.05253623188405797,"collapse [2]), where the embedding vectors only span a lower-dimensional subspace and would be
29"
INTRODUCTION,0.05434782608695652,"highly correlated. In this case, the covariance matrix of embedding has certain zero eigenvalues, which
30"
INTRODUCTION,0.05615942028985507,"degenerates the representation in SSL. To prevent dimensional collapse, a theoretically motivated
31"
INTRODUCTION,0.057971014492753624,"paradigm, called whitening loss, is proposed by minimizing the distance between embeddings of
32"
INTRODUCTION,0.059782608695652176,"positive pairs under the condition that embeddings from different views are whitened [12, 20].
33"
INTRODUCTION,0.06159420289855073,"One typical implementation of whitening loss is hard whitening [12, 40] that designs whitening
34"
INTRODUCTION,0.06340579710144928,"transformation over mini-batch data and imposes the loss on the whitened output [12, 20, 40]. We note
35"
INTRODUCTION,0.06521739130434782,"that the whitening transformation is a function over embedding during forward pass, and modulates
36"
INTRODUCTION,0.06702898550724638,"the spectrum of embedding implicitly during backward pass when minimizing the objective. This
37"
INTRODUCTION,0.06884057971014493,"raises questions whether there exist other functions over embedding can avoid collapse? If yes, how
38"
INTRODUCTION,0.07065217391304347,"the function affects the spectrum of embedding?
39"
INTRODUCTION,0.07246376811594203,X: input �~� �1
INTRODUCTION,0.07427536231884058,"�2
joint architecture �1 �2 �1 �2 ��(∙)"
INTRODUCTION,0.07608695652173914,"��′
′  (∙)"
INTRODUCTION,0.07789855072463768,�: embedding ST ST ℒ
INTRODUCTION,0.07971014492753623,�: transformed output �ℒ ��2 �ℒ ��1
INTRODUCTION,0.08152173913043478,"�ℒ
�Z1
�ℒ
�Z2"
INTRODUCTION,0.08333333333333333,"Figure 1: The framework using spectral transformation
(ST) to modulate the spectrum of embedding in joint em-
bedding architecture for SSL."
INTRODUCTION,0.08514492753623189,"This paper proposes spectral transforma-
40"
INTRODUCTION,0.08695652173913043,"tion (ST), a framework to modulate the
41"
INTRODUCTION,0.08876811594202899,"spectrum of embedding in joint embed-
42"
INTRODUCTION,0.09057971014492754,"ding architecture.
ST maps the spec-
43"
INTRODUCTION,0.09239130434782608,"trum of embedding to a desired distribu-
44"
INTRODUCTION,0.09420289855072464,"tion during forward pass, and modulates
45"
INTRODUCTION,0.09601449275362318,"the spectrum of embedding by implicit
46"
INTRODUCTION,0.09782608695652174,"gradient update during backward pass
47"
INTRODUCTION,0.09963768115942029,"(Figure 1). This framework provides a
48"
INTRODUCTION,0.10144927536231885,"way to seek for functions beyond whiten-
49"
INTRODUCTION,0.10326086956521739,"ing transformation that can avoid dimen-
50"
INTRODUCTION,0.10507246376811594,"sional collapse. We show that whitening
51"
INTRODUCTION,0.1068840579710145,"transformation is a special instance of ST
52"
INTRODUCTION,0.10869565217391304,"using a power function by definition, and there exist other power functions that can avoid dimen-
53"
INTRODUCTION,0.1105072463768116,"sional collapse by our empirical investigation (see Section 3.2 for details). We demonstrate that
54"
INTRODUCTION,0.11231884057971014,"IterNorm [22], an approximating whitening method by using Newton’s iterations [3, 42], is also an
55"
INTRODUCTION,0.11413043478260869,"instance of ST, and show that IterNorm with different iteration number corresponds to different ST
56"
INTRODUCTION,0.11594202898550725,"(see Section 3.3 for details). We further theoretically characterize how the spectrum evolves as the
57"
INTRODUCTION,0.11775362318840579,"increasing of iteration number of IterNorm.
58"
INTRODUCTION,0.11956521739130435,"We empirically observe that IterNorm suffers from severe dimensional collapse and mostly fails to
59"
INTRODUCTION,0.1213768115942029,"train the model in SSL unexpectedly, unlike its benefits in approximating whitening for supervised
60"
INTRODUCTION,0.12318840579710146,"learning [22]. We thus propose IterNorm with trace loss (INTL), a simple solution to address the
61"
INTRODUCTION,0.125,"failure of IterNorm, by adding an extra penalty on the transformed output. Moreover, we theoretically
62"
INTRODUCTION,0.12681159420289856,"demonstrate that INTL can avoid dimensional collapse, and reveal its mechanism in encouraging the
63"
INTRODUCTION,0.1286231884057971,"covariance matrix of embedding to have equal eigenvalues. We conduct comprehensive experiments
64"
INTRODUCTION,0.13043478260869565,"and show that INTL is a promising SSL method in practice. E.g., INTL achieves 76.6% top-1
65"
INTRODUCTION,0.1322463768115942,"accuracy in linear evaluation on ImageNet using ResNet-50, which exceeds the performance of
66"
INTRODUCTION,0.13405797101449277,"the supervised baseline, and this result is obtained by using a batch size of only 256. Our main
67"
INTRODUCTION,0.1358695652173913,"contributions are summarized as follows:
68"
INTRODUCTION,0.13768115942028986,"• We propose spectral transformation, a framework to modulate the spectrum of embedding
69"
INTRODUCTION,0.13949275362318841,"and to seek for functions beyond whitening that can avoid dimensional collapse. We show
70"
INTRODUCTION,0.14130434782608695,"there exist other functions that can avoid dimensional collapse by empirical observations.
71"
INTRODUCTION,0.1431159420289855,"• We propose a new instance of ST, called IterNorm with trace loss (INTL). We theoretically
72"
INTRODUCTION,0.14492753623188406,"prove that INTL can avoid collapse and modulate the spectrum of embedding towards an
73"
INTRODUCTION,0.14673913043478262,"equal-eigenvalue distribution during the course of optimization.
74"
INTRODUCTION,0.14855072463768115,"• INTL is a promising SSL method in practice. INTL is on par with or outperforms the state-
75"
INTRODUCTION,0.1503623188405797,"of-the-art SSL methods on standard benchmarks. Furthermore, these results are obtained by
76"
INTRODUCTION,0.15217391304347827,"using a relatively small batch size.
77"
RELATED WORK,0.1539855072463768,"2
Related Work
78"
RELATED WORK,0.15579710144927536,"Our work is related to the SSL methods that address the feature collapse problem when using joint
79"
RELATED WORK,0.15760869565217392,"embedding architectures. Contrastive learning prevents collapse by attracting positive samples closer,
80"
RELATED WORK,0.15942028985507245,"and spreading negative samples apart [41, 43]. In these methods, negative samples play an important
81"
RELATED WORK,0.161231884057971,"role and need to be well designed [29, 1, 19]. MoCos [18, 8] builds a memory bank with a momentum
82"
RELATED WORK,0.16304347826086957,"encoder to provide consistent negative samples, while SimCLR [7] addresses that more negative
83"
RELATED WORK,0.16485507246376813,"samples in a batch with strong data augmentations perform better. Our proposed INTL can avoid
84"
RELATED WORK,0.16666666666666666,"collapse and work well without negative samples.
85"
RELATED WORK,0.16847826086956522,"Non-contrastive methods by designing asymmetric network architecture avoid feature collapse
86"
RELATED WORK,0.17028985507246377,"without introducing negative pairs explicitly [4, 5, 26, 16, 9]. BYOL [16] appends a predictor after
87"
RELATED WORK,0.1721014492753623,"the online network and introduce momentum into the target network. SimSiam [9] further generalizes
88"
RELATED WORK,0.17391304347826086,"BYOL by empirically showing that stop-gradient is essential for preventing trivial solutions. Other
89"
RELATED WORK,0.17572463768115942,"progresses include a cluster assignment prediction using Sinkhorn-Knopp algorithm [5], and an
90"
RELATED WORK,0.17753623188405798,"asymmetric pipeline with a self-distillation loss for Vision Transformers [6]. It remains not clear
91"
RELATED WORK,0.1793478260869565,"how the asymmetric network avoids collapse without negative pairs, leaving the debates on batch
92"
RELATED WORK,0.18115942028985507,"normalization (BN) [13, 39, 33] and stop-gradient [9, 45], even though preliminary works have
93"
RELATED WORK,0.18297101449275363,"attempted to analyze the training dynamics [38] and build a connection between non-contrastive
94"
RELATED WORK,0.18478260869565216,"and contrastive methods [36, 14]. Our work addresses the more challenging dimensional collapse
95"
RELATED WORK,0.18659420289855072,"problem, and theoretically shows that our INTL can avoid dimensional collapse.
96"
RELATED WORK,0.18840579710144928,"Whitening loss is a theoretically motivated paradigm to prevent dimensional collapse [12]. One
97"
RELATED WORK,0.19021739130434784,"typical implementation of whitening loss is hard whitening that designs whitening transformation over
98"
RELATED WORK,0.19202898550724637,"mini-batch data and imposes the loss on the whitened output. The designed whitening transformation
99"
RELATED WORK,0.19384057971014493,"includes batch whitening in W-MSE [12] and Shuffled-DBN [20], channel whitening in CW-RGP [40],
100"
RELATED WORK,0.1956521739130435,"and the combination of both in Zero-CL [47]. Our proposed ST generalizes whitening transformation
101"
RELATED WORK,0.19746376811594202,"and provides a frame to modulate the spectrum of embedding. Our proposed INTL can improve these
102"
RELATED WORK,0.19927536231884058,"work in training stability and performance, by replacing whitening transformation with IterNorm [22]
103"
RELATED WORK,0.20108695652173914,"and imposing an additional trace loss on the transformed output. Furthermore, we theoretically show
104"
RELATED WORK,0.2028985507246377,"that our proposed INTL encourages the covariance matrix of embedding having equal eigenvalues.
105"
RELATED WORK,0.20471014492753623,"Another way to implement whitening loss is soft whitening that imposes a whitening penalty as
106"
RELATED WORK,0.20652173913043478,"regularization on the embedding, including Barlow Twins [44], VICReg [2] and CCA-SSG [46].
107"
RELATED WORK,0.20833333333333334,"Different from these works, our proposed INTL imposes the trace loss on the approximated whitened
108"
RELATED WORK,0.21014492753623187,"output, which implicitly encourages the covariance matrix of embedding having equal eigenvalues to
109"
RELATED WORK,0.21195652173913043,"avoid dimensional collapse.
110"
RELATED WORK,0.213768115942029,"There are also theoretical works analyzing how dimensional collapse occurs [20, 24] and how it
111"
RELATED WORK,0.21557971014492755,"can be avoided by using whitening loss [20, 40]. The recent works [17, 15] further discuss how to
112"
RELATED WORK,0.21739130434782608,"characterize the magnitude of dimensional collapse, and connect the spectrum of a representation
113"
RELATED WORK,0.21920289855072464,"to a power law. They show the coefficient of the power law is a strong indicator for the effects of
114"
RELATED WORK,0.2210144927536232,"the representation. Different from these works, our theoretical analysis presents a new thought in
115"
RELATED WORK,0.22282608695652173,"demonstrating how to avoid dimensional collapse, which provides theoretical basis for our proposed
116"
RELATED WORK,0.2246376811594203,"INTL.
117"
SPECTRAL TRANSFORMATION BEYOND WHITENING,0.22644927536231885,"3
Spectral Transformation beyond Whitening
118"
PRELIMINARY AND NOTATION,0.22826086956521738,"3.1
Preliminary and Notation
119"
PRELIMINARY AND NOTATION,0.23007246376811594,"Joint embedding architectures.
Let x denote the input sampled uniformly from a set of images
120"
PRELIMINARY AND NOTATION,0.2318840579710145,"D, and T denote the set of data transformations available for augmentation. We consider a pair of
121"
PRELIMINARY AND NOTATION,0.23369565217391305,"neural networks Fθ and F ′
θ′, parameterized by θ and θ′ respectively. They take as input two randomly
122"
PRELIMINARY AND NOTATION,0.23550724637681159,"augmented views, x(1) = T1(x) and x(2) = T2(x), where T1,2 ∈T; and they output the embedding
123"
PRELIMINARY AND NOTATION,0.23731884057971014,"z(1) = Fθ(x(1)) and z(2) = F ′
θ′(x(2)). The networks are trained with an objective function that
124"
PRELIMINARY AND NOTATION,0.2391304347826087,"minimizes the distance between embeddings obtained from different views of the same image:
125"
PRELIMINARY AND NOTATION,0.24094202898550723,"L(x, θ) = Ex∼D, T1,2∼T ℓ
 
Fθ(T1(x)), F ′
θ′(T2(x))

.
(1)"
PRELIMINARY AND NOTATION,0.2427536231884058,"where ℓ(·, ·) is a loss function.
The mean square error (MSE) of L2−normalized vectors as
126"
PRELIMINARY AND NOTATION,0.24456521739130435,"ℓ(z(1), z(2)) = ∥
z(1)"
PRELIMINARY AND NOTATION,0.2463768115942029,"∥z(1)∥2 −
z(2)"
PRELIMINARY AND NOTATION,0.24818840579710144,"∥z(2)∥2 ∥2
2 is usually used as the loss function [9]. This loss is also equiva-
127"
PRELIMINARY AND NOTATION,0.25,"lent to the negative cosine similarity, up to a scale of 1"
PRELIMINARY AND NOTATION,0.25181159420289856,"2 and an optimization irrelevant constant [9].
128"
PRELIMINARY AND NOTATION,0.2536231884057971,"This architecture is also called Siamese Network [9], if Fθ = F ′
θ′. Another variant distinguishes the
129"
PRELIMINARY AND NOTATION,0.2554347826086957,"networks into target network F ′
θ′ and online network Fθ, and updates the weight θ′ of target network
130"
PRELIMINARY AND NOTATION,0.2572463768115942,"through exponential moving average (EMA) [8, 16] over θ of online network.
131"
PRELIMINARY AND NOTATION,0.25905797101449274,"Feature collapse.
While minimizing Eqn. 1, a trivial solution known as complete collapse could
132"
PRELIMINARY AND NOTATION,0.2608695652173913,"occur such that Fθ(x) ≡c, ∀x ∈D. Moreover, a weaker collapse condition called dimensional
133"
PRELIMINARY AND NOTATION,0.26268115942028986,"collapse can be easily arrived, for which the projected features collapse into a low-dimensional
134"
PRELIMINARY AND NOTATION,0.2644927536231884,"manifold. To express dimensional collapse more mathematically, we refer to dimensional collapse as
135"
PRELIMINARY AND NOTATION,0.266304347826087,"the phenomenon that one or certain eigenvalues of the covariance matrix of feature vectors degenerate
136"
PRELIMINARY AND NOTATION,0.26811594202898553,"to 0. Therefore, we can determine the occurrence of dimensional collapse by observing the spectrum
137"
PRELIMINARY AND NOTATION,0.26992753623188404,"of the covariance matrix.
138"
PRELIMINARY AND NOTATION,0.2717391304347826,"Whitening loss.
To address the collapse problem, whitening loss [12] is proposed to minimize
139"
PRELIMINARY AND NOTATION,0.27355072463768115,"Eqn. 1, under the condition that embeddings from different views are whitened. Whitening loss
140"
PRELIMINARY AND NOTATION,0.2753623188405797,"provides theoretical guarantee in avoiding (dimensional) collapse, since the embedding is whitened
141"
PRELIMINARY AND NOTATION,0.27717391304347827,"with all axes decorrelated [12, 20]. Ermolov et al. [12] propose to whiten the mini-batch embedding
142"
PRELIMINARY AND NOTATION,0.27898550724637683,"Z ∈Rd×m using batch whitening (BW) [21, 35] and impose the loss on the whitened output
143"
PRELIMINARY AND NOTATION,0.2807971014492754,"bZ ∈Rd×m, given the mini-batch inputs X with size of m, as follows:
144"
PRELIMINARY AND NOTATION,0.2826086956521739,"min
θ
L(X; θ) = EX∼D, T1,2∼T ∥bZ(1) −bZ(2)∥2
F"
PRELIMINARY AND NOTATION,0.28442028985507245,with bZ(v) = Σ−1
PRELIMINARY AND NOTATION,0.286231884057971,"2 Z(v), v ∈{1, 2},
(2)"
PRELIMINARY AND NOTATION,0.28804347826086957,"where Σ =
1
mZZT is the covariance matrix of embedding1. Σ−1"
PRELIMINARY AND NOTATION,0.2898550724637681,"2 is called the whitening matrix,
145"
PRELIMINARY AND NOTATION,0.2916666666666667,"and is calculated either by Cholesky decomposition in [12] or by eigen-decomposition in [20].
146"
PRELIMINARY AND NOTATION,0.29347826086956524,"E.g., zero-phase component analysis (ZCA) whitening [21] calculates Σ−1"
PRELIMINARY AND NOTATION,0.29528985507246375,2 = UΛ−1
PRELIMINARY AND NOTATION,0.2971014492753623,"2 UT , where
147"
PRELIMINARY AND NOTATION,0.29891304347826086,"Λ = diag(λ1, . . . , λd) and U = [u1, ..., ud] are the eigenvalues and associated eigenvectors of Σ,
148"
PRELIMINARY AND NOTATION,0.3007246376811594,"i.e., UΛUT = Σ. One intriguing result shown in [40] is that hard whitening can avoid collapse by
149"
PRELIMINARY AND NOTATION,0.302536231884058,"only constraining the embedding Z to be full-rank, but not whitened.
150"
PRELIMINARY AND NOTATION,0.30434782608695654,"We note that the whitening transformation is a function over embedding Z during forward pass, and
151"
PRELIMINARY AND NOTATION,0.3061594202898551,"modulates the spectrum of embedding Z implicitly during backward pass when minimizing MSE
152"
PRELIMINARY AND NOTATION,0.3079710144927536,"loss imposed on the whitened output. This raises questions whether there are other functions over
153"
PRELIMINARY AND NOTATION,0.30978260869565216,"embedding Z can avoid collapse? If yes, how the function affects the spectrum of embedding Z?
154"
SPECTRAL TRANSFORMATION,0.3115942028985507,"3.2
Spectral Transformation
155"
SPECTRAL TRANSFORMATION,0.3134057971014493,"In this section, we extend the whitening transformation to spectral transformation, a more general
156"
SPECTRAL TRANSFORMATION,0.31521739130434784,"view to characterize the modulation on the spectrum of embedding, and empirically investigate the
157"
SPECTRAL TRANSFORMATION,0.3170289855072464,"interaction between the spectrum of the covariance matrix of bZ and collapse of the SSL model.
158"
SPECTRAL TRANSFORMATION,0.3188405797101449,"Definition 1. (Spectral Transformation) Given any one-variable mapping function g(·) in the
159"
SPECTRAL TRANSFORMATION,0.32065217391304346,"definition domain λ(Z) = {λ1, λ2, . . . , λd}, spectral transformation (ST) maps the spectrum λ(Z)
160"
SPECTRAL TRANSFORMATION,0.322463768115942,"to g(λ(Z)) = {g(λ1), g(λ2), . . . , g(λd)}. Accordingly, for a d × d real symmetric matrix Σ, spectral
161"
SPECTRAL TRANSFORMATION,0.3242753623188406,"transformation g(·) on Σ =
dP"
SPECTRAL TRANSFORMATION,0.32608695652173914,"i=1
λiuiuT
i is defined as g(Σ) =
dP"
SPECTRAL TRANSFORMATION,0.3278985507246377,"i=1
g(λi)uiuT
i . We denote g(Λ) =
162"
SPECTRAL TRANSFORMATION,0.32971014492753625,"diag(g(λ(Z))), and ΦST = g(Σ) = Ug(Λ)UT is the transformation matrix.
163"
SPECTRAL TRANSFORMATION,0.33152173913043476,"The output of spectral transformation is calculated by bZ = ΦST Z = Ug(Λ)UT Z. The covariance
164"
SPECTRAL TRANSFORMATION,0.3333333333333333,"matrix of bZ is:
165"
SPECTRAL TRANSFORMATION,0.3351449275362319,ΣbZ = 1
SPECTRAL TRANSFORMATION,0.33695652173913043,"m bZbZT = UΛg2(Λ)UT .
166"
SPECTRAL TRANSFORMATION,0.338768115942029,"Based on this formula,
the essence of spectral transformation is mapping the spec-
167"
SPECTRAL TRANSFORMATION,0.34057971014492755,"trum
λ(Z)
=
{λ1, λ2, . . . , λd}
to
λ(bZ)
=

λ1g2(λ1), λ2g2(λ2), . . . , λdg2(λd)
	
.
168 169"
SPECTRAL TRANSFORMATION,0.3423913043478261,"�= 1.50
�= 0.55
�= 0.50"
SPECTRAL TRANSFORMATION,0.3442028985507246,"�= 0.45
�= 0.40
�= 0.30 (a)"
SPECTRAL TRANSFORMATION,0.34601449275362317,"p=0.0
0.3
0.40
0.45
0.50
0.55
1.0
1.5
0 20 40 60 80"
SPECTRAL TRANSFORMATION,0.34782608695652173,Top-1 accuracy % 8 6 4 2 0
SPECTRAL TRANSFORMATION,0.3496376811594203,"Lg of c
1"
SPECTRAL TRANSFORMATION,0.35144927536231885,"Acc
Lg of c
1 (b)"
SPECTRAL TRANSFORMATION,0.3532608695652174,"Figure 2: Investigate ST using power functions.
We
choose several p from 0 to 1.5. We show (a) the visu-
alization of the toy model output; (b) top-1 accuracy and
condition indicator (we use the inverse of the condition
number c−1 = λd"
SPECTRAL TRANSFORMATION,0.35507246376811596,"λ1 ) on CIFAR-10 with ResNet-18. The
results on CIFAR-10 are averaged over five runs, with stan-
dard deviation shown as error bars and we show the details
of experimental setup in supplementary materials. Similar
phenomena can be observed when using other datasets
(e.g., ImageNet) and other networks (e.g., ResNet-50)."
SPECTRAL TRANSFORMATION,0.35688405797101447,"ST using power functions.
Whitening
170"
SPECTRAL TRANSFORMATION,0.358695652173913,"is a special instance of spectral transfor-
171"
SPECTRAL TRANSFORMATION,0.3605072463768116,"mation, where g(·) is a power function
172"
SPECTRAL TRANSFORMATION,0.36231884057971014,g(λ) = λ−1
SPECTRAL TRANSFORMATION,0.3641304347826087,"2 .
We further study the
173"
SPECTRAL TRANSFORMATION,0.36594202898550726,"mechanism of this power transformation,
174"
SPECTRAL TRANSFORMATION,0.3677536231884058,"where we consider a more general trans-
175"
SPECTRAL TRANSFORMATION,0.3695652173913043,"formation g(λ) = λ−p, p ∈(−∞, +∞)
176"
SPECTRAL TRANSFORMATION,0.3713768115942029,"for ST. Based on Definition. 1, this
177"
SPECTRAL TRANSFORMATION,0.37318840579710144,"general power transformation is map-
178"
SPECTRAL TRANSFORMATION,0.375,"ping the spectrum λ(Z) to λ(bZ) =
179

λ1
1−2p, λ2
1−2p, . . . , λd
1−2p	
,
e.g.,
180"
SPECTRAL TRANSFORMATION,0.37681159420289856,"λ(bZ)
=
{1, 1, . . . , 1} when using
181"
SPECTRAL TRANSFORMATION,0.3786231884057971,whitening with p = 1
SPECTRAL TRANSFORMATION,0.3804347826086957,"2.
182"
SPECTRAL TRANSFORMATION,0.3822463768115942,"We first conduct experiments on the 2D
183"
SPECTRAL TRANSFORMATION,0.38405797101449274,"dataset with varying p and visualize out-
184"
SPECTRAL TRANSFORMATION,0.3858695652173913,"puts of the toy models in Figure 2(a). We
185"
SPECTRAL TRANSFORMATION,0.38768115942028986,"observe the toy model seems to perform
186"
SPECTRAL TRANSFORMATION,0.3894927536231884,"well to avoid collapse although the trans-
187"
SPECTRAL TRANSFORMATION,0.391304347826087,"formed output is not ideally whitened, when p is in the neighborhood of 0.5, e.g. 0.45 ∼0.55. But
188"
SPECTRAL TRANSFORMATION,0.39311594202898553,"when p gradually deviates from 0.5, collapse occurs. We then conduct experiments on real-world
189"
SPECTRAL TRANSFORMATION,0.39492753623188404,"datasets to confirm these phenomena. The results shown in Figure 2(b) are consistent with the above
190"
SPECTRAL TRANSFORMATION,0.3967391304347826,"phenomena. When p is set to 0.45 or 0.55, the model remains high evaluation performance as the
191"
SPECTRAL TRANSFORMATION,0.39855072463768115,"one of p = 0.5. When p is in the neighborhood of 0.5, the transformed output has a well-conditioned
192"
SPECTRAL TRANSFORMATION,0.4003623188405797,"spectrum that each eigenvalue approaches 1. When p deviates from 0.5 to a certain extent, the
193"
SPECTRAL TRANSFORMATION,0.40217391304347827,1The embedding is usually centralized by performing Z := Z(I −1
SPECTRAL TRANSFORMATION,0.40398550724637683,"m1 · 1T ) for whitening, and we assume
Z is centralized in this paper for simplifying discussion."
SPECTRAL TRANSFORMATION,0.4057971014492754,"spectrum of the transformed output is not well-conditioned, which is closely related to collapse of
194"
SPECTRAL TRANSFORMATION,0.4076086956521739,"the embedding. Therefore, we empirically show that if a certain ST could obtain a well-conditioned
195"
SPECTRAL TRANSFORMATION,0.40942028985507245,"spectrum of transformed output, collapse could be avoided.
196"
SPECTRAL TRANSFORMATION,0.411231884057971,"Based on the above results, we empirically observe that the spectral transformation g(λ) = λ−p with
197"
SPECTRAL TRANSFORMATION,0.41304347826086957,"p around 0.5 can avoid collapse. Therefore, we can design new algorithms based on our framework
198"
SPECTRAL TRANSFORMATION,0.4148550724637681,"to avoid collapse.
199"
SPECTRAL TRANSFORMATION,0.4166666666666667,"3.3
Implicit Spectral Transformation using Newton’s Iteration
200"
SPECTRAL TRANSFORMATION,0.41847826086956524,"One problem of ST using power functions g(λ) = λ−p (p is around 0.5) is the numerical instability,
201"
SPECTRAL TRANSFORMATION,0.42028985507246375,"when calculating eigenvalues λ and eigenvectors U using eigen-decomposition if the covariance
202"
SPECTRAL TRANSFORMATION,0.4221014492753623,"matrix is ill-conditioned [31]. We provide detailed experiments and analysis in supplementary
203"
SPECTRAL TRANSFORMATION,0.42391304347826086,"materials to confirm the existence of this problem in SSL.
204"
SPECTRAL TRANSFORMATION,0.4257246376811594,"Naturally, if we can implement a spectral transformation that can modulate the spectrum without
205"
SPECTRAL TRANSFORMATION,0.427536231884058,"explicitly calculating λ or U, this problem can be solved. Indeed, we note an approximate whitening
206"
SPECTRAL TRANSFORMATION,0.42934782608695654,"method by using Newton’s iteration, called iterative normalization (IterNorm) [22], is proposed
207"
SPECTRAL TRANSFORMATION,0.4311594202898551,"to address the numerical problem of batch whitening in supervised learning. Specifically, given
208"
SPECTRAL TRANSFORMATION,0.4329710144927536,"the centralized embedding Z, iteration number T and the trace-normalized covariance matrix
209"
SPECTRAL TRANSFORMATION,0.43478260869565216,"ΣN = Σ/tr(Σ), it performs Newton’s iteration as follows.
210"
SPECTRAL TRANSFORMATION,0.4365942028985507,"211
P0 = I
Pk = 1"
SPECTRAL TRANSFORMATION,0.4384057971014493,"2(3Pk−1 −P3
k−1ΣN), k = 1, 2, ..., T.
(3)"
SPECTRAL TRANSFORMATION,0.44021739130434784,The whitening matrix Σ−1
SPECTRAL TRANSFORMATION,0.4420289855072464,"2 is approximated by ΦT = PT /
p"
SPECTRAL TRANSFORMATION,0.4438405797101449,"tr(Σ) and we have the whitened output
212"
SPECTRAL TRANSFORMATION,0.44565217391304346,"bZ = ΦT Z. When T →+∞, ΦT →Σ−1"
SPECTRAL TRANSFORMATION,0.447463768115942,"2 and the covariance matrix of bZ will be an identity matrix.
213"
SPECTRAL TRANSFORMATION,0.4492753623188406,"Here, we theoretically show that IterNorm is also an instance of spectral transformation as follows.
214"
SPECTRAL TRANSFORMATION,0.45108695652173914,"Theorem 1. Define one-variable iterative function fT (x), satisfying
215"
SPECTRAL TRANSFORMATION,0.4528985507246377,fk+1(x) = 3
SPECTRAL TRANSFORMATION,0.45471014492753625,2fk(x) −1
XFK,0.45652173913043476,"2xfk
3(x), k ≥0; f0(x) = 1.
216"
XFK,0.4583333333333333,"The mapping function of IterNorm is
217"
XFK,0.4601449275362319,"g(λ) = fT (
λ
tr(Σ))/
p"
XFK,0.46195652173913043,"tr(Σ),
218"
XFK,0.463768115942029,"Without calculating λ or U, IterNorm implicitly maps ∀λi ∈λ(Z) to bλi =
λi
tr(Σ)fT
2(
λi
tr(Σ)).
219"
XFK,0.46557971014492755,"The proof is shown in supplementary materials. For simplicity, we define the T-whitening function of
220"
XFK,0.4673913043478261,"IterNorm hT (x) = xfT
2(x), which obtains the spectrum of transformed output. Based on the fact
221"
XFK,0.4692028985507246,"that the covariance matrix of transformed output will be identity when T of IterNorm increases to
222"
XFK,0.47101449275362317,"infinity [3], we thus have
223"
XFK,0.47282608695652173,"∀λi > 0, lim
T →∞hT (
λi
tr(Σ)) = 1.
(4)"
XFK,0.4746376811594203,"Different iteration numbers T of IterNorm imply different T-whitening functions hT (·). It is interest-
224"
XFK,0.47644927536231885,"ing to analyze the characteristics of hT (·).
225"
XFK,0.4782608695652174,"Proposition 1. Given x ∈(0, 1), ∀T ∈N we have hT (x) ∈(0, 1) and h′
T (x) > 0.
226"
XFK,0.48007246376811596,"The proof of Proposition 1 is shown in supplementary materials. Proposition 1 states hT (x) is a
227"
XFK,0.48188405797101447,"monotone increasing function for x ∈(0, 1) and its range is also in (0, 1). Since
λi
tr(Σ) ∈(0, 1),
228"
XFK,0.483695652173913,"∀λi > 0, we have
229"
XFK,0.4855072463768116,"∀T ∈N, λi > λj > 0 =⇒1 > bλi > bλj > 0.
(5)"
XFK,0.48731884057971014,"Formula 5 indicates that IterNorm maps all non-zero eigenvalues to (0, 1) and preserves monotonicity.
230"
XFK,0.4891304347826087,"Proposition 2. Given x ∈(0, 1), ∀T ∈N, we have hT +1(x) > hT (x).
231"
XFK,0.49094202898550726,"The proof of Proposition 2 is shown in supplementary materials. Proposition 2 indicates that IterNorm
232"
XFK,0.4927536231884058,"gradually stretches the eigenvalues towards one as the iteration number T increases. This property
233"
XFK,0.4945652173913043,"of IterNorm theoretically shows that the spectrum of bZ will have better condition if we use a larger
234"
XFK,0.4963768115942029,"iteration number T of IterNorm.
235"
XFK,0.49818840579710144,"In summary, our analyses theoretically show that IterNorm gradually stretches the eigenvalues towards
236"
XFK,0.5,"one as the iteration number T increases, and the smaller the eigenvalue is, the larger T is required to
237"
XFK,0.5018115942028986,"approach one.
238"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5036231884057971,"4
Iterative Normalization with Trace Loss
239"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5054347826086957,"It is expected that IterNorm, as a kind of spectral transformation, can avoid collapse and obtain good
240"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5072463768115942,"performance in SSL, due to its benefits in approximating whitening for supervised learning [22].
241"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5090579710144928,"However, we empirically observe that IterNorm suffers severe dimensional collapse and mostly
242"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5108695652173914,"fails to train the model in SSL (we postpone the details in Section 4.2.). Based on the analyses in
243"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5126811594202898,"Section 3.2 and 3.3, we propose a simple solution by adding an extra penalty named trace loss on the
244"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5144927536231884,"transformed output bZ by IterNorm to ensure a well-conditioned spectrum. It is clear that the sum of
245"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5163043478260869,"eigenvalues of ΣbZ is less than or equal to d, we thus propose a trace loss that encourages the trace of
246"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5181159420289855,"ΣbZ to be its maximum d, when d ≤m. In particular, we design a new method called IterNorm with
247"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.519927536231884,"trace loss (INTL) for optimizing the SSL model as2:
248"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5217391304347826,"min
θ∈Θ
INTL(Z) = d
X"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5235507246376812,"j=1
(1 −(ΣbZ)jj)2,
(6)"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5253623188405797,"where Z = Fθ(·) and bZ = IterNorm(Z). Eqn. 6 can be viewed as an optimization problem over θ
249"
ITERATIVE NORMALIZATION WITH TRACE LOSS,0.5271739130434783,"to encourage the trace of bZ to be d.
250"
THEORETICAL ANALYSIS,0.5289855072463768,"4.1
Theoretical Analysis
251"
THEORETICAL ANALYSIS,0.5307971014492754,"In this section, we theoretically prove that INTL can avoid collapse, and INTL modulates the spectrum
252"
THEORETICAL ANALYSIS,0.532608695652174,"of embedding towards an equal-eigenvalue distribution during the course of optimization.
253"
THEORETICAL ANALYSIS,0.5344202898550725,"Note that ΣbZ can be expressed using the T-whitening function hT (·) as ΣbZ =
dP"
THEORETICAL ANALYSIS,0.5362318840579711,"i=1
hT (xi)uiuT
i ,
254"
THEORETICAL ANALYSIS,0.5380434782608695,"where xi = λi/tr(Σ) ≥0 and
dP"
THEORETICAL ANALYSIS,0.5398550724637681,"i=1
xi = 1. When the range of Fθ(·) is wide enough, the optimization
255"
THEORETICAL ANALYSIS,0.5416666666666666,"problem over θ (Eqn. 6) can be transformed as the following optimization problem over x (Eqn. 7)
256"
THEORETICAL ANALYSIS,0.5434782608695652,"without changing the optimal value (please see supplementary materials for the details of derivation):
257






"
THEORETICAL ANALYSIS,0.5452898550724637,"




"
THEORETICAL ANALYSIS,0.5471014492753623,"min
x
INTL(x) =
dP j=1  dP"
THEORETICAL ANALYSIS,0.5489130434782609,"i=1
[1 −hT (xi)]u2
ji 2"
THEORETICAL ANALYSIS,0.5507246376811594,"s.t.
dP"
THEORETICAL ANALYSIS,0.552536231884058,"i=1
xi = 1"
THEORETICAL ANALYSIS,0.5543478260869565,"xi ≥0, i = 1, · · · , d, (7)"
THEORETICAL ANALYSIS,0.5561594202898551,"where uji is the j-th elements of vector ui. In this formulation, we can prove that our proposed INTL
258"
THEORETICAL ANALYSIS,0.5579710144927537,"can theoretically avoid collapse, as long as the iteration number T of IterNorm is larger than zero.
259"
THEORETICAL ANALYSIS,0.5597826086956522,"Theorem 2. Let x ∈[0, 1]d, ∀T ∈N+, INTL(x) shown in Eqn. 7 is a strictly convex function.
260"
THEORETICAL ANALYSIS,0.5615942028985508,x∗= [ 1
THEORETICAL ANALYSIS,0.5634057971014492,"d, · · · , 1"
THEORETICAL ANALYSIS,0.5652173913043478,"d]T is the unique minimum point as well as the optimal solution to INTL(x).
261"
THEORETICAL ANALYSIS,0.5670289855072463,"The proof is shown in supplementary materials. Based on Theorem 2, INTL promotes the equality of
262"
THEORETICAL ANALYSIS,0.5688405797101449,"all eigenvalues of the covariance matrix of embedding Z during the course of optimization, which
263"
THEORETICAL ANALYSIS,0.5706521739130435,"provides a theoretical guarantee to avoid dimensional collapse.
264"
THEORETICAL ANALYSIS,0.572463768115942,"Connection to hard whitening.
Hard whitening methods, like W-MSE [12] and shuffle-DBN [20],
265"
THEORETICAL ANALYSIS,0.5742753623188406,"design a whitening transformation over each view and minimize the distances between the whitened
266"
THEORETICAL ANALYSIS,0.5760869565217391,"outputs from different views. This mechanism encourages the covariance matrix of embedding to be
267"
THEORETICAL ANALYSIS,0.5778985507246377,"full-rank [40]. Our INTL designs an approximated whitening transformation using IterNorm and
268"
THEORETICAL ANALYSIS,0.5797101449275363,"imposes an additional trace loss penalty on the (approximately) whitened output. This encourages the
269"
THEORETICAL ANALYSIS,0.5815217391304348,"covariance matrix of embedding having equal eigenvalues.
270"
THEORETICAL ANALYSIS,0.5833333333333334,"Connection to soft whitening.
Soft whitening methods, like Barlow-Twins [44] and VICReg [2]
271"
THEORETICAL ANALYSIS,0.5851449275362319,"directly impose a whitening penalty as a regularization on the embedding. This encourages the
272"
THEORETICAL ANALYSIS,0.5869565217391305,"covariance matrix of the embedding to be identity (with a fixed scalar γ, e.g., γI). Our INTL imposes
273"
THEORETICAL ANALYSIS,0.5887681159420289,"2Without losing validity, we ignore the MSE term for simplifying discussion."
THEORETICAL ANALYSIS,0.5905797101449275,"0
20
40
60
80
100
120
Eigenvalue index 10 8 6 4 2 0"
THEORETICAL ANALYSIS,0.592391304347826,Lg of eigenvalue
THEORETICAL ANALYSIS,0.5942028985507246,"T=1
T=3
T=5 (a)"
THEORETICAL ANALYSIS,0.5960144927536232,"0
20
40
60
80
100
120
Eigenvalue index 5 0 5"
THEORETICAL ANALYSIS,0.5978260869565217,Lg of eigenvalue
THEORETICAL ANALYSIS,0.5996376811594203,"T=1
T=3
T=5 (b)"
THEORETICAL ANALYSIS,0.6014492753623188,"0
20
40
60
80
Epochs 20 40 60 80"
THEORETICAL ANALYSIS,0.6032608695652174,Top-1 accuracy %
THEORETICAL ANALYSIS,0.605072463768116,"T=1
T=3
T=5 (c)"
THEORETICAL ANALYSIS,0.6068840579710145,"T=1
T=3
T=5
T=7
T=9
Iteration number 10 5 0 5"
THEORETICAL ANALYSIS,0.6086956521739131,Lg of eigenvalue
THEORETICAL ANALYSIS,0.6105072463768116,"maximum eigenvalue
minimum eigenvalue"
THEORETICAL ANALYSIS,0.6123188405797102,"(d)
Figure 3: Investigate the effectiveness of IterNorm with and without trace loss. We train the models on
CIFAR-10 with ResNet-18 for 100 epochs (details of experimental setup are shown in supplementary
materials). We apply IterNorm with various iteration numbers T, and show the results with (solid
lines) and without (dashed lines) trace loss respectively. (a) The spectrum of the transformed output
bZ; (b) The spectrum of the embedding Z; (c) The top-1 accuracy. (d) indicates that IterNorm (without
trace loss) suffers from numeric divergence when using a large iteration number, e.g. T = 9. It is
noteworthy that when T ≥11, the loss values are all NAN, making the model unable to be trained.
Similar phenomena can be observed when using other datasets (e.g., ImageNet) and other networks
(e.g., ResNet-50)."
THEORETICAL ANALYSIS,0.6141304347826086,"the penalty on the transformed output, but can be viewed as implicitly encouraging the covariance
274"
THEORETICAL ANALYSIS,0.6159420289855072,"matrix of the embedding to be identity with a free scalar (i.e., having equal eigenvalues).
275"
THEORETICAL ANALYSIS,0.6177536231884058,"Intuitively, INTL provides the equal-eigenvalues constraint on the covariance matrix of embedding,
276"
THEORETICAL ANALYSIS,0.6195652173913043,"which is a stronger constraint than hard whitening (the full-rank constraint), but a weaker constraint
277"
THEORETICAL ANALYSIS,0.6213768115942029,"than soft whitening (the whitening constraint). This preliminary but new comparison provides a new
278"
THEORETICAL ANALYSIS,0.6231884057971014,"way to understand the whitening loss in SSL.
279"
EMPIRICAL ANALYSIS,0.625,"4.2
Empirical Analysis
280"
EMPIRICAL ANALYSIS,0.6268115942028986,"In this section, we empirically show that IterNorm-only fails to avoid collapse, but IterNorm with
281"
EMPIRICAL ANALYSIS,0.6286231884057971,"trace loss can well avoid collapse.
282"
EMPIRICAL ANALYSIS,0.6304347826086957,"IterNorm fails to avoid collapse.
In theory, IterNorm can map all non-zero eigenvalues to approach
283"
EMPIRICAL ANALYSIS,0.6322463768115942,"one, with a large enough T. In practice, it usually uses a fixed T, and it is very likely to encounter
284"
EMPIRICAL ANALYSIS,0.6340579710144928,"small eigenvalues during training. In this case, IterNorm cannot ensure the transformed output has
285"
EMPIRICAL ANALYSIS,0.6358695652173914,"a well-conditioned spectrum (Figure 3(a)), which potentially results in dimensional collapse. One
286"
EMPIRICAL ANALYSIS,0.6376811594202898,"may use a large T, however, IterNorm will encounter numeric divergence upon further increasing the
287"
EMPIRICAL ANALYSIS,0.6394927536231884,"iteration number T, even though it has converged. E.g., IterNorm suffers from numeric divergence
288"
EMPIRICAL ANALYSIS,0.6413043478260869,"in Figure 3(d) when using T = 9, since the maximum eigenvalue of whitened output is around
289"
EMPIRICAL ANALYSIS,0.6431159420289855,"107, significantly large than 1 (we attribute to the numeric divergence, since this result goes against
290"
EMPIRICAL ANALYSIS,0.644927536231884,"Proposition 1 and 2, and we further validate it by monitoring the transformed output). It is noteworthy
291"
EMPIRICAL ANALYSIS,0.6467391304347826,"that when T ≥11, the loss values are all NAN, making the model unable to be trained. These
292"
EMPIRICAL ANALYSIS,0.6485507246376812,"problems make IterNorm difficult to avoid dimensional collapse in practice.
293"
EMPIRICAL ANALYSIS,0.6503623188405797,"The magic of trace loss for IterNorm.
IterNorm with trace loss works significantly different from
294"
EMPIRICAL ANALYSIS,0.6521739130434783,"IterNorm-only. Our experimental results (Figure 3(b)) empirically show that INTL avoid dimensional
295"
EMPIRICAL ANALYSIS,0.6539855072463768,"collapse, which is consistent with our Theorem 2. INTL encourages the equality of all eigenvalues
296"
EMPIRICAL ANALYSIS,0.6557971014492754,"of the covariance matrix of embedding Z and achieve good evaluation performance (Figure 3(c))
297"
EMPIRICAL ANALYSIS,0.657608695652174,"even when the iteration number T is only 1. This equal-eigenvalues optimal solution is derived
298"
EMPIRICAL ANALYSIS,0.6594202898550725,"from the combination of IterNorm and trace loss. Benefiting from trace loss, IterNorm can obtain
299"
EMPIRICAL ANALYSIS,0.6612318840579711,"well-conditioned spectra for the transformed output during training (Figure 3(a)), which helps to
300"
EMPIRICAL ANALYSIS,0.6630434782608695,"avoid collapse.
301"
EXPERIMENTS ON STANDARD SSL BENCHMARK,0.6648550724637681,"5
Experiments on Standard SSL Benchmark
302"
EXPERIMENTS ON STANDARD SSL BENCHMARK,0.6666666666666666,"In this section, we conduct experiments on standard SSL benchmarks to validate the effectiveness of
303"
EXPERIMENTS ON STANDARD SSL BENCHMARK,0.6684782608695652,"our proposed INTL. We first evaluate the performance of INTL for classification on ImageNet [11],
304"
EXPERIMENTS ON STANDARD SSL BENCHMARK,0.6702898550724637,"CIFAR-10/100 [25] and ImageNet-100 [37]. Then we evaluate the effectiveness in transfer learning,
305"
EXPERIMENTS ON STANDARD SSL BENCHMARK,0.6721014492753623,"for a pre-trained model using INTL. We provide details of implementation and training protocol as
306"
EXPERIMENTS ON STANDARD SSL BENCHMARK,0.6739130434782609,"well as computational overhead and the full PyTorch-style algorithm in supplementary materials.
307"
EVALUATION FOR CLASSIFICATION,0.6757246376811594,"5.1
Evaluation for Classification
308"
EVALUATION FOR CLASSIFICATION,0.677536231884058,"Evaluation on ImageNet.
We train our INTL using ResNet-50 backbone and evaluate the perfor-
309"
EVALUATION FOR CLASSIFICATION,0.6793478260869565,"mance using the common linear evaluation protocol on ImageNet. The results are shown in Table 1.
310"
EVALUATION FOR CLASSIFICATION,0.6811594202898551,"Our INTL achieves a top-1 accuracy of 76.6% that exceeds the performance of the supervised base-
311"
EVALUATION FOR CLASSIFICATION,0.6829710144927537,"line [7] and other SSL methods. Moreover, this result is obtained under a batch size of only 256,
312"
EVALUATION FOR CLASSIFICATION,0.6847826086956522,"Table 1: Evaluation on ImageNet. All results are based on ResNet-50 backbone: (1) linear classifica-
tion on top of the frozen representations from ImageNet; (2) semi-supervised classification on top of
the fine-tuned representations from 1% and 10% of ImageNet samples. Following MoCo [8] and
SimSiam [9], our INTL is trained for 800 epochs with a batch size of 256 on 2 A100-40GB GPUs."
EVALUATION FOR CLASSIFICATION,0.6865942028985508,Method
EVALUATION FOR CLASSIFICATION,0.6884057971014492,"Linear
Semi-supervised
top-1
top-5
top-1
top-5
1%
10%
1%
10%
Supervised
76.5
-
25.4
56.4
48.4
80.4
SimCLR [7]
69.3
89.0
48.3
65.6
75.5
87.8
MoCo v2 [8]
71.1
-
-
BYOL [16]
74.3
91.6
53.2
68.8
78.4
89.0
SwAV [5]
75.3
-
53.9
70.2
78.5
89.9
SimSiam [9]
71.3
-
-
W-MSE [12]
72.6
-
-
DINO [6]
75.3
-
-
Barlow Twins [44]
73.2
91.0
55.0
69.7
79.2
89.3
VICReg [2]
73.2
91.1
54.8
69.5
79.4
89.5
INTL (ours)
76.6
93.1
61.7
72.0
84.6
90.9"
EVALUATION FOR CLASSIFICATION,0.6902173913043478,"Table 2: Classification accuracy (top 1 and top-5) of a linear classifier and a 5-nearest neighbors
classifier for different loss functions and datasets. The table is mostly inherited from [10]. All
methods are based on ResNet-18 with two views and are trained for 1000-epoch on CIFAR-10/100
with a batch size of 256 and 400-epoch on ImageNet-100 with a batch size of 128."
EVALUATION FOR CLASSIFICATION,0.6920289855072463,"Method
CIFAR-10
CIFAR-100
ImageNet-100
top-1
5-nn
top-5
top-1
5-nn
top-5
top-1
5-nn
top-5
SimCLR [7]
90.74
85.13
99.75
65.78
53.19
89.04
77.64
65.78
94.06
MoCo V2 [8]
92.94
88.95
99.79
69.89
58.09
91.65
79.28
70.46
95.18
BYOL [16]
92.58
87.40
99.79
70.46
56.46
91.96
80.32
68.94
94.94
SwAV [5]
89.17
84.18
99.68
64.88
53.32
88.78
74.28
63.84
92.84
SimSiam [9]
90.51
86.82
99.72
66.04
55.79
89.62
78.72
67.92
94.78
W-MSE [12]
88.67
84.95
99.68
61.33
49.65
87.26
69.06
58.44
91.22
DINO [6]
89.52
86.13
99.71
66.76
56.24
90.34
74.92
64.30
92.78
Barlow Twins [44]
92.10
88.09
99.73
70.90
59.40
91.91
80.16
72.14
95.14
VICReg [2]
92.07
87.38
99.74
68.54
56.32
90.83
79.40
71.94
95.02
INTL (ours)
92.60
90.03
99.80
70.88
61.90
92.13
81.68
73.46
95.42"
EVALUATION FOR CLASSIFICATION,0.6938405797101449,"achieving one goal of SSL community that seeks for obtaining good performance with a small batch
313"
EVALUATION FOR CLASSIFICATION,0.6956521739130435,"size.
314"
EVALUATION FOR CLASSIFICATION,0.697463768115942,"Semi-supervised training on ImageNet.
Furthermore, we fine-tune our pre-trained INTL model
315"
EVALUATION FOR CLASSIFICATION,0.6992753623188406,"on a subset of ImageNet. We use subsets of size 1% and 10% using the same split as SimCLR. The
316"
EVALUATION FOR CLASSIFICATION,0.7010869565217391,"semi-supervised results obtained on the ImageNet validation set are also reported in Table 1. It shows
317"
EVALUATION FOR CLASSIFICATION,0.7028985507246377,"that INTL outperforms other baselines with a significant margin.
318"
EVALUATION FOR CLASSIFICATION,0.7047101449275363,"Evaluation on small and medium size datasets.
In order to further test the generality of INTL, we
319"
EVALUATION FOR CLASSIFICATION,0.7065217391304348,"also provide the linear evaluation results of INTL on CIFAR-10/100 [25] and ImageNet-100 [37] with
320"
EVALUATION FOR CLASSIFICATION,0.7083333333333334,"ResNet-18 as the backbone. We strictly follow the experimental settings in solo-learn [10] for these
321"
EVALUATION FOR CLASSIFICATION,0.7101449275362319,"datasets. As shown in Table 2, INTL achieves a top-1 accuracy of 92.60% on CIFAR-10, 70.88%
322"
EVALUATION FOR CLASSIFICATION,0.7119565217391305,"on CIFAR-100 and 81.68% on ImageNet-100 which is on par with or exceeds the state-of-the-art
323"
EVALUATION FOR CLASSIFICATION,0.7137681159420289,"methods reproduced by solo-learn. Meanwhile, INTL outperforms other baselines with a significant
324"
EVALUATION FOR CLASSIFICATION,0.7155797101449275,"margin when using 5-nearest neighbors classifier which also indicates that INTL has learned good
325"
EVALUATION FOR CLASSIFICATION,0.717391304347826,"representations.
326"
TRANSFER TO DOWNSTREAM TASKS,0.7192028985507246,"5.2
Transfer to Downstream Tasks
327"
TRANSFER TO DOWNSTREAM TASKS,0.7210144927536232,"We examine the representation quality by transferring our pre-trained model to other tasks, including
328"
TRANSFER TO DOWNSTREAM TASKS,0.7228260869565217,"COCO [27] object detection and instance segmentation. We use the baseline of the detection codebase
329"
TRANSFER TO DOWNSTREAM TASKS,0.7246376811594203,"from MoCo [18] for INTL. The results of baselines shown in Table 3 are mostly inherited from [9].
330"
TRANSFER TO DOWNSTREAM TASKS,0.7264492753623188,"We observe that INTL performs much better than other state-of-the-art approaches on COCO object
331"
TRANSFER TO DOWNSTREAM TASKS,0.7282608695652174,"detection and instance segmentation, which shows the great potential of INTL in transferring to
332"
TRANSFER TO DOWNSTREAM TASKS,0.730072463768116,"downstream tasks.
333"
TRANSFER TO DOWNSTREAM TASKS,0.7318840579710145,"Table 3: Transfer Learning. All competitive unsupervised methods are based on 200-epoch pre-
training on ImageNet (IN). The table are mostly inherited from [9]. Our INTL is performed with 3
random seeds, with mean and standard deviation reported."
TRANSFER TO DOWNSTREAM TASKS,0.7336956521739131,"Method
COCO detection
COCO instance seg.
AP50
AP
AP75
AP50
AP
AP75
Scratch
44.0
26.4
27.8
46.9
29.3
30.8
Supervised
58.2
38.2
41.2
54.7
33.3
35.2
SimCLR [7]
57.7
37.9
40.9
54.6
33.3
35.3
MoCo v2 [8]
58.8
39.2
42.5
55.5
34.3
36.6
BYOL [16]
57.8
37.9
40.9
54.3
33.2
35.0
SwAV [5] (repro.)
60.2
39.8
43.0
56.6
34.6
36.8
SimSiam [9]
57.5
37.9
40.9
54.2
33.2
35.2
W-MSE [12] (repro.)
60.1
39.2
42.8
56.8
34.8
36.7
Barlow Twins [44]
59.0
39.2
42.5
56.0
34.3
36.5
INTL (ours)
61.2±0.08
41.2±0.12
44.7±0.19
57.8±0.04
35.7±0.04
38.1±0.12"
ABLATION STUDY,0.7355072463768116,"5.3
Ablation Study
334"
ABLATION STUDY,0.7373188405797102,"Table 4: Effect of batch sizes for INTL. We
train 100 epoch on ImageNet and provide the
Top-1 accuracy using linear evaluation. The
embedding dimension is fixed to 8192."
ABLATION STUDY,0.7391304347826086,"Bs
32
64
128
256
512
1024
acc.(%)
64.2
66.4
68.1
68.7
69.5
69.7"
ABLATION STUDY,0.7409420289855072,"Batch size.
Most SSL methods, including certain
335"
ABLATION STUDY,0.7427536231884058,"whitening-based methods, are known to be sensitive
336"
ABLATION STUDY,0.7445652173913043,"to batch sizes, e.g. SimCLR [7], SwAV [5] and W-
337"
ABLATION STUDY,0.7463768115942029,"MSE [12] all require a large batch size (e.g. 4096)
338"
ABLATION STUDY,0.7481884057971014,"to work well. We then test the robustness of INTL
339"
ABLATION STUDY,0.75,"to batch sizes. We train INTL on ImageNet for 100
340"
ABLATION STUDY,0.7518115942028986,"epochs with various batch sizes ranging from 32 to
341"
ABLATION STUDY,0.7536231884057971,"1024. As shown in Table. 4, even if the batch size is as low as 32 or 64, INTL still maintains good
342"
ABLATION STUDY,0.7554347826086957,"performance. At the same time, when the batch size increases, the accuracy of INTL is also improved.
343"
ABLATION STUDY,0.7572463768115942,"These results indicate that INTL has good robustness to batch sizes and can adapt to various scenarios
344"
ABLATION STUDY,0.7590579710144928,"that constrain the training batch size.
345"
ABLATION STUDY,0.7608695652173914,"64
128
256
512
1024
2048
4096
8192 16384
Embedding dimension 57.5 60.0 62.5 65.0 67.5 70.0 72.5"
ABLATION STUDY,0.7626811594202898,Top-1 accuracy %
ABLATION STUDY,0.7644927536231884,"INTL (ours)
Barlow Twins
SimCLR"
ABLATION STUDY,0.7663043478260869,"Figure 4: Ablation experiments for varying em-
bedding dimensions. The batch size is fixed to
256."
ABLATION STUDY,0.7681159420289855,"Embedding dimension.
Embedding dimension,
346"
ABLATION STUDY,0.769927536231884,"the output dimension of the projection, is also a key
347"
ABLATION STUDY,0.7717391304347826,"element for most self-supervised learning methods,
348"
ABLATION STUDY,0.7735507246376812,"which may have a significant impact on training
349"
ABLATION STUDY,0.7753623188405797,"results. As illustrated in [44], Barlow Twins is very
350"
ABLATION STUDY,0.7771739130434783,"sensitive to embedding dimension and it requires a
351"
ABLATION STUDY,0.7789855072463768,"large dimension (e.g. 8192 or 16384) to work well.
352"
ABLATION STUDY,0.7807971014492754,"We also test the robustness of INTL to embedding
353"
ABLATION STUDY,0.782608695652174,"dimensions. Following the setup of [7] and [44],
354"
ABLATION STUDY,0.7844202898550725,"we train INTL on ImageNet for 300 epochs with
355"
ABLATION STUDY,0.7862318840579711,"the dimension ranging from 64 to 16384. As shown
356"
ABLATION STUDY,0.7880434782608695,"in Figure. 4, even when the embedding dimension
357"
ABLATION STUDY,0.7898550724637681,"is low as 64 or 128, INTL still achieves good re-
358"
ABLATION STUDY,0.7916666666666666,"sults. These results show that INTL also has strong
359"
ABLATION STUDY,0.7934782608695652,"robustness to embedding dimensions.
360"
CONCLUSION AND LIMITATION,0.7952898550724637,"6
Conclusion and Limitation
361"
CONCLUSION AND LIMITATION,0.7971014492753623,"In this paper, we proposed spectral transformation (ST) framework to modulate the spectrum of
362"
CONCLUSION AND LIMITATION,0.7989130434782609,"embedding and to seek for functions beyond whitening that can avoid dimensional collapse. Our
363"
CONCLUSION AND LIMITATION,0.8007246376811594,"proposed IterNorm with trace loss (INTL) is well-motivated, theoretically demonstrated, and em-
364"
CONCLUSION AND LIMITATION,0.802536231884058,"pirically validated in avoiding dimension collapse. Comprehensive experiments have shown the
365"
CONCLUSION AND LIMITATION,0.8043478260869565,"merits of INTL for achieving state-of-the-art performance for SSL in practice. We showed that INTL
366"
CONCLUSION AND LIMITATION,0.8061594202898551,"provides the equal-eigenvalues constraint on the covariance matrix of embedding, which is a stronger
367"
CONCLUSION AND LIMITATION,0.8079710144927537,"constraint than hard whitening (the full-rank constraint), but a weaker constraint than soft whitening
368"
CONCLUSION AND LIMITATION,0.8097826086956522,"(the whitening constraint). This preliminary but new results provides a potential way to understand
369"
CONCLUSION AND LIMITATION,0.8115942028985508,"and compare SSL methods.
370"
CONCLUSION AND LIMITATION,0.8134057971014492,"Limitation.
Our work only explores the mechanism of ST using power function and Newton’s
371"
CONCLUSION AND LIMITATION,0.8152173913043478,"iteration for SSL. As a general concept, we believe that more functions in our ST framework can be
372"
CONCLUSION AND LIMITATION,0.8170289855072463,"designed to avoid collapse in the future. Besides, our theoretical work mainly revolves around the ST
373"
CONCLUSION AND LIMITATION,0.8188405797101449,"using Newton’s iteration, without providing theoretical analysis for more general ST. There are still
374"
CONCLUSION AND LIMITATION,0.8206521739130435,"mysteries about modulation of the spectrum in more general ST during backpropagation.
375"
REFERENCES,0.822463768115942,"References
376"
REFERENCES,0.8242753623188406,"[1] Bachman, P., Hjelm, R.D., Buchwalter, W.: Learning representations by maximizing mutual
377"
REFERENCES,0.8260869565217391,"information across views. In: NeurIPS (2019)
378"
REFERENCES,0.8278985507246377,"[2] Bardes, A., Ponce, J., LeCun, Y.: Vicreg: Variance-invariance-covariance regularization for
379"
REFERENCES,0.8297101449275363,"self-supervised learning. In: ICLR (2022)
380"
REFERENCES,0.8315217391304348,"[3] Bini, D.A., Higham, N.J., Meini, B.: Algorithms for the matrix pth root. Numerical Algorithms
381"
REFERENCES,0.8333333333333334,"(2005)
382"
REFERENCES,0.8351449275362319,"[4] Caron, M., Bojanowski, P., Joulin, A., Douze, M.: Deep clustering for unsupervised learning of
383"
REFERENCES,0.8369565217391305,"visual features. In: ECCV (2018)
384"
REFERENCES,0.8387681159420289,"[5] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised learning of
385"
REFERENCES,0.8405797101449275,"visual features by contrasting cluster assignments. In: NeurIPS (2020)
386"
REFERENCES,0.842391304347826,"[6] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging
387"
REFERENCES,0.8442028985507246,"properties in self-supervised vision transformers. In: ICCV (2021)
388"
REFERENCES,0.8460144927536232,"[7] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning
389"
REFERENCES,0.8478260869565217,"of visual representations. In: ICML (2020)
390"
REFERENCES,0.8496376811594203,"[8] Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive
391"
REFERENCES,0.8514492753623188,"learning. arXiv preprint arXiv:2003.04297 (2020)
392"
REFERENCES,0.8532608695652174,"[9] Chen, X., He, K.: Exploring simple siamese representation learning. In: CVPR (2021)
393"
REFERENCES,0.855072463768116,"[10] da Costa, V.G.T., Fini, E., Nabi, M., Sebe, N., Ricci, E.: solo-learn: A library of self-supervised
394"
REFERENCES,0.8568840579710145,"methods for visual representation learning. Journal of Machine Learning Research 23(56), 1–6
395"
REFERENCES,0.8586956521739131,"(2022), http://jmlr.org/papers/v23/21-1155.html
396"
REFERENCES,0.8605072463768116,"[11] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-Scale
397"
REFERENCES,0.8623188405797102,"Hierarchical Image Database. In: CVPR (2009)
398"
REFERENCES,0.8641304347826086,"[12] Ermolov, A., Siarohin, A., Sangineto, E., Sebe, N.: Whitening for self-supervised representation
399"
REFERENCES,0.8659420289855072,"learning. In: ICML (2021)
400"
REFERENCES,0.8677536231884058,"[13] Fetterman, A., Albrecht, J.: Understanding self-supervised and contrastive learning with ”boot-
401"
REFERENCES,0.8695652173913043,"strap your own latent” (byol). Technical Report (2020)
402"
REFERENCES,0.8713768115942029,"[14] Garrido, Q., Chen, Y., Bardes, A., Najman, L., LeCun, Y.: On the duality between contrastive
403"
REFERENCES,0.8731884057971014,"and non-contrastive self-supervised learning. In: ICLR (2023)
404"
REFERENCES,0.875,"[15] Ghosh, A., Mondal, A.K., Agrawal, K.K., Richards, B.A.: Investigating power laws in deep
405"
REFERENCES,0.8768115942028986,"representation learning. arXiv preprint arXiv:2202.05808 (2022)
406"
REFERENCES,0.8786231884057971,"[16] Grill, J.B., Strub, F., Altch´e, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C.,
407"
REFERENCES,0.8804347826086957,"Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., Piot, B., kavukcuoglu, k., Munos, R., Valko, M.:
408"
REFERENCES,0.8822463768115942,"Bootstrap your own latent - a new approach to self-supervised learning. In: NeuraIPS (2020)
409"
REFERENCES,0.8840579710144928,"[17] He, B., Ozay, M.: Exploring the gap between collapsed and whitened features in self-supervised
410"
REFERENCES,0.8858695652173914,"learning. In: ICML (2022)
411"
REFERENCES,0.8876811594202898,"[18] He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised visual
412"
REFERENCES,0.8894927536231884,"representation learning. In: CVPR (2020)
413"
REFERENCES,0.8913043478260869,"[19] Henaff, O.: Data-efficient image recognition with contrastive predictive coding. In: ICML
414"
REFERENCES,0.8931159420289855,"(2020)
415"
REFERENCES,0.894927536231884,"[20] Hua, T., Wang, W., Xue, Z., Ren, S., Wang, Y., Zhao, H.: On feature decorrelation in self-
416"
REFERENCES,0.8967391304347826,"supervised learning. In: ICCV (2021)
417"
REFERENCES,0.8985507246376812,"[21] Huang, L., Yang, D., Lang, B., Deng, J.: Decorrelated batch normalization. In: CVPR (2018)
418"
REFERENCES,0.9003623188405797,"[22] Huang, L., Zhou, Y., Zhu, F., Liu, L., Shao, L.: Iterative normalization: Beyond standardization
419"
REFERENCES,0.9021739130434783,"towards efficient whitening. In: CVPR (2019)
420"
REFERENCES,0.9039855072463768,"[23] Jaiswal, A., Babu, A.R., Zadeh, M.Z., Banerjee, D., Makedon, F.: A survey on contrastive
421"
REFERENCES,0.9057971014492754,"self-supervised learning. arXiv preprint arXiv:2011.00362 (2020)
422"
REFERENCES,0.907608695652174,"[24] Jing, L., Vincent, P., LeCun, Y., Tian, Y.: Understanding dimensional collapse in contrastive
423"
REFERENCES,0.9094202898550725,"self-supervised learning. In: ICLR (2022)
424"
REFERENCES,0.9112318840579711,"[25] Krizhevsky, A.: Learning multiple layers of features from tiny images. Tech. rep. (2009)
425"
REFERENCES,0.9130434782608695,"[26] Li, J., Zhou, P., Xiong, C., Hoi, S.C.: Prototypical contrastive learning of unsupervised repre-
426"
REFERENCES,0.9148550724637681,"sentations. In: ICLR (2021)
427"
REFERENCES,0.9166666666666666,"[27] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, L.:
428"
REFERENCES,0.9184782608695652,"Microsoft coco: Common objects in context. In: ECCV (2014)
429"
REFERENCES,0.9202898550724637,"[28] Liu, Y., Pan, S., Jin, M., Zhou, C., Xia, F., Yu, P.S.: Graph self-supervised learning: A survey.
430"
REFERENCES,0.9221014492753623,"arXiv e-prints pp. arXiv–2103 (2021)
431"
REFERENCES,0.9239130434782609,"[29] Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predictive coding.
432"
REFERENCES,0.9257246376811594,"arXiv preprint arXiv:1807.03748 (2018)
433"
REFERENCES,0.927536231884058,"[30] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P.,
434"
REFERENCES,0.9293478260869565,"Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without
435"
REFERENCES,0.9311594202898551,"supervision. arXiv preprint arXiv:2304.07193 (2023)
436"
REFERENCES,0.9329710144927537,"[31] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,
437"
REFERENCES,0.9347826086956522,"Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning
438"
REFERENCES,0.9365942028985508,"library. Advances in neural information processing systems 32 (2019)
439"
REFERENCES,0.9384057971014492,"[32] Ranasinghe, K., Naseer, M., Khan, S., Khan, F.S., Ryoo, M.: Self-supervised video transformer.
440"
REFERENCES,0.9402173913043478,"In: CVPR (2022)
441"
REFERENCES,0.9420289855072463,"[33] Richemond, P.H., Grill, J.B., Altch´e, F., Tallec, C., Strub, F., Brock, A., Smith, S., De,
442"
REFERENCES,0.9438405797101449,"S., Pascanu, R., Piot, B., et al.: Byol works even without batch statistics. arXiv preprint
443"
REFERENCES,0.9456521739130435,"arXiv:2010.10241 (2020)
444"
REFERENCES,0.947463768115942,"[34] Saunshi, N., Plevrakis, O., Arora, S., Khodak, M., Khandeparkar, H.: A theoretical analysis of
445"
REFERENCES,0.9492753623188406,"contrastive unsupervised representation learning. In: ICML (2019)
446"
REFERENCES,0.9510869565217391,"[35] Siarohin, A., Sangineto, E., Sebe, N.: Whitening and coloring batch transform for gans. In:
447"
REFERENCES,0.9528985507246377,"ICLR (2019)
448"
REFERENCES,0.9547101449275363,"[36] Tao, C., Wang, H., Zhu, X., Dong, J., Song, S., Huang, G., Dai, J.: Exploring the equivalence of
449"
REFERENCES,0.9565217391304348,"siamese self-supervised learning via A unified gradient framework. In: CVPR (2022)
450"
REFERENCES,0.9583333333333334,"[37] Tian, Y., Krishnan, D., Isola, P.: Contrastive multiview coding. In: European conference on
451"
REFERENCES,0.9601449275362319,"computer vision (2020)
452"
REFERENCES,0.9619565217391305,"[38] Tian, Y., Chen, X., Ganguli, S.: Understanding self-supervised learning dynamics without
453"
REFERENCES,0.9637681159420289,"contrastive pairs. In: ICML (2021)
454"
REFERENCES,0.9655797101449275,"[39] Tian, Y., Yu, L., Chen, X., Ganguli, S.: Understanding self-supervised learning with dual deep
455"
REFERENCES,0.967391304347826,"networks. CoRR abs/2010.00578 (2020)
456"
REFERENCES,0.9692028985507246,"[40] Weng, X., Huang, L., Zhao, L., Anwer, R.M., Khan, S., Khan, F.: An investigation into
457"
REFERENCES,0.9710144927536232,"whitening loss for self-supervised learning. In: NeurIPS (2022)
458"
REFERENCES,0.9728260869565217,"[41] Wu, Z., Xiong, Y., Yu, S.X., Lin, D.: Unsupervised feature learning via non-parametric instance
459"
REFERENCES,0.9746376811594203,"discrimination. In: CVPR (2018)
460"
REFERENCES,0.9764492753623188,"[42] Ye, C., Evanusa, M., He, H., Mitrokhin, A., Goldstein, T., Yorke, J.A., Fermuller, C., Aloimonos,
461"
REFERENCES,0.9782608695652174,"Y.: Network deconvolution. In: ICLR (2020)
462"
REFERENCES,0.980072463768116,"[43] Ye, M., Zhang, X., Yuen, P.C., Chang, S.F.: Unsupervised embedding learning via invariant and
463"
REFERENCES,0.9818840579710145,"spreading instance feature. In: CVPR (2019)
464"
REFERENCES,0.9836956521739131,"[44] Zbontar, J., Jing, L., Misra, I., Lecun, Y., Deny, S.: Barlow twins: Self-supervised learning via
465"
REFERENCES,0.9855072463768116,"redundancy reduction. In: ICML (2021)
466"
REFERENCES,0.9873188405797102,"[45] Zhang, C., Zhang, K., Zhang, C., Pham, T.X., Yoo, C.D., Kweon, I.S.: How does simsiam avoid
467"
REFERENCES,0.9891304347826086,"collapse without negative samples? a unified understanding with self-supervised contrastive
468"
REFERENCES,0.9909420289855072,"learning. In: ICLR (2022)
469"
REFERENCES,0.9927536231884058,"[46] Zhang, H., Wu, Q., Yan, J., Wipf, D., Yu, P.S.: From canonical correlation analysis to self-
470"
REFERENCES,0.9945652173913043,"supervised graph neural networks. In: NeurIPS (2021)
471"
REFERENCES,0.9963768115942029,"[47] Zhang, S., Zhu, F., Yan, J., Zhao, R., Yang, X.: Zero-CL: Instance and feature decorrelation for
472"
REFERENCES,0.9981884057971014,"negative-free symmetric contrastive learning. In: ICLR (2022)
473"
