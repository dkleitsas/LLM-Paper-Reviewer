Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016501650165016502,"Privacy and communication efﬁciency are important challenges in federated train-
ing of neural networks, and combining them is still an open problem. In this
work, we develop a method that uniﬁes highly compressed communication and
differential privacy (DP). We introduce a compression technique based on Relative
Entropy Coding (REC) to the federated setting. With a minor modiﬁcation to
REC, we obtain a provably differentially private learning algorithm, DP-REC,
and show how to compute its privacy guarantees. Our experiments demonstrate
that DP-REC drastically reduces communication costs while providing privacy
guarantees comparable to the state-of-the-art."
INTRODUCTION,0.0033003300330033004,"1
INTRODUCTION"
INTRODUCTION,0.0049504950495049506,"The performance of modern neural-network-based machine learning models scales exceptionally well
with the amount of data that they are trained on (Kaplan et al., 2020; Henighan et al., 2020). At the
same time, industry (Xiao & Karlin), legislators (Dwork, 2019; Voigt & Von dem Bussche, 2017) and
consumers (Laziuk, 2021) have become more conscious about the need to protect the privacy of the
data that might be used in training such models. Federated learning (FL) describes a machine learning
principle that enables learning on decentralized data by computing updates on-device. Instead of
sending its data to a central location, a “client” in a federation of devices sends model updates
computed on its data to the central server. Such an approach to learning from decentralized data
promises to unlock the computing capabilities of billions of edge devices, enable personalized models
and new applications in e.g. healthcare due to the inherently more private nature of the approach."
INTRODUCTION,0.006600660066006601,"On the other hand, the federated paradigm brings challenges along many dimensions such as learning
from non-i.i.d. data, resource-constrained devices, heterogeneous compute and communication
capabilities, questions of fairness and representation, as well as the focus of our paper: communication
overhead and characterization of privacy. Neural network training requires many passes over the data,
resulting in repeated transfer of the model and updates between the server and the clients, potentially
making communication a primary bottleneck (Kairouz et al., 2019; Wang et al., 2021). Compressing
updates is an active area of research in FL and an essential step in “untethering” edge devices from
WiFi. Moreover, while FL is intuitively more private through keeping data on-device, client updates
have been shown to reveal sensitive information, even allowing to reconstruct a client’s training
data (Geiping et al., 2020). To truly protect client privacy, a more rigorous mathematical notion of
Differential Privacy (DP) is widely adopted as the de-facto standard in FL. More speciﬁcally, DP for
FL is usually deﬁned at a client-level, which provides plausible deniability of an individual client’s
contribution to the federated model. Both aspects of FL, communication efﬁciency and differential
privacy, have been extensively studied in separation. Effective combination of the two, however, is an
open problem and an active area of research (Kairouz et al., 2021; Chen et al., 2020; Girgis et al.,
2020). Some methods offer very limited compression capabilities for comparable utility (Kairouz
et al., 2021), whereas others (Girgis et al., 2020) offer signiﬁcant compression but require additional
compromises, due to a looser privacy composition."
INTRODUCTION,0.00825082508250825,"In this work, we present Differentially Private Relative Entropy Coding (DP-REC), a uniﬁed approach
to jointly tackle privacy and communication efﬁciency. DP-REC makes use of the information-
limiting constraints of DP to encode the client updates in FL with extremely short messages. First, we
build our method based on compression without quantization; namely, the lossy variant of Relative
Entropy Coding (REC), recently proposed by Flamich et al. (2020). Second, we show that it can be"
INTRODUCTION,0.009900990099009901,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01155115511551155,"modiﬁed to satisfy DP, and we provide a proof of its privacy guarantees along with the appropriate
accounting technique. Third, we run extensive evaluation on 4 datasets and 3 types of models to
demonstrate that our algorithm achieves extreme compression of client-to-server updates (down to 7
bits per tensor) at privacy levels ε < 1 with a small impact on utility (< 6.5% accuracy reduction
on FEMNIST compared to DP-FedAvg). Additionally, we show how to reduce server-to-client
communication by sending a history of updates accumulated since the client’s last participation."
DP-REC FOR PRIVATE AND EFFICIENT COMMUNICATION IN FL,0.013201320132013201,"2
DP-REC FOR PRIVATE AND EFFICIENT COMMUNICATION IN FL"
DP-REC FOR PRIVATE AND EFFICIENT COMMUNICATION IN FL,0.01485148514851485,"Federated learning has been described by McMahan et al. (2016) in the form of the FedAvg
algorithm. At each communication round t, the server sends the current model parameters w(t) to a
subset S(t) of all clients S. Each chosen client s updates the server-provided model w(t), e.g., via
stochastic gradient descent, to better ﬁt its local dataset Ds = {dsi}Ns
i=1 with a given loss function"
DP-REC FOR PRIVATE AND EFFICIENT COMMUNICATION IN FL,0.0165016501650165,"Ls(Ds, w) := 1 Ns Ns
X"
DP-REC FOR PRIVATE AND EFFICIENT COMMUNICATION IN FL,0.018151815181518153,"i=1
L(dsi, w).
(1)"
DP-REC FOR PRIVATE AND EFFICIENT COMMUNICATION IN FL,0.019801980198019802,"After E epochs of optimization on the local dataset, the client-side optimization procedure results in
an updated model w(t)
s , based on which the client computes its update to the global model"
DP-REC FOR PRIVATE AND EFFICIENT COMMUNICATION IN FL,0.02145214521452145,"∆(t)
s
= w(t)
s
−w(t);
(2)"
DP-REC FOR PRIVATE AND EFFICIENT COMMUNICATION IN FL,0.0231023102310231,"and sends it to the server. The server then aggregates client-speciﬁc updates to get the new global
model w(t+1) = w(t) +
1
|S(t)|
P"
DP-REC FOR PRIVATE AND EFFICIENT COMMUNICATION IN FL,0.024752475247524754,"s ∆(t)
s
=
1
|S(t)|
P"
DP-REC FOR PRIVATE AND EFFICIENT COMMUNICATION IN FL,0.026402640264026403,"s w(t)
s . Outside the context of differential privacy,
client updates are usually weighted by the size Ns of the local dataset. A generalization of this server-
side scheme (Reddi et al., 2020) interprets
1
|S(t)|
P
s∈S(t) ∆(t)
s
as a “gradient” for the server-side
model and introduces more advanced updating schemes, such as Adam (Kingma & Ba, 2014)."
DP-REC FOR PRIVATE AND EFFICIENT COMMUNICATION IN FL,0.028052805280528052,"Federated training involves repeated communication of model updates from clients to the server and
vice versa. These updates can reveal sensitive information about the client data, so there is a need for
formal privacy guarantees. The total communication cost can be signiﬁcant, thus constraining FL to
the use of unmetered channels, such as WiFi. Hence, compressing the update messages in a privacy-
preserving way plays an important role in moving FL to a truly mobile use-case. In the following
sections, we ﬁrst describe the lossy version of Relative Entropy Coding (REC) (Flamich et al., 2020),
then show how to extend it to the FL scenario before discussing it in the context of differential privacy.
Finally, we show how DP-REC can be used to additionally compress server-to-client messages."
REC FOR EFFICIENT COMMUNICATION,0.0297029702970297,"2.1
REC FOR EFFICIENT COMMUNICATION"
REC FOR EFFICIENT COMMUNICATION,0.03135313531353135,"Lossy REC, and its predecessor minimal random code learning (MIRACLE) (Havasi et al., 2018),
have been originally proposed as a way to compress a random sample w from a distribution qφ(w)
parameterized with φ, i.e., w ∼qφ(w), by using information that is “shared” between the sender
and the receiver. This information is given in terms of a shared prior distribution pθ(w) with
parameters θ along with a shared random seed R. The sender proceeds by generating K independent
random samples, w1, . . . , wK, from the prior distribution pθ(w) according to the random seed R.
Subsequently, it forms a categorical distribution ˜qπ(w1:K) over the K samples with the probability
of each sample being proportional to the likelihood ratio πk ∝qφ(w = wk)/pθ(w = wk). Finally,
it draws a random sample wk∗from ˜qπ(w1:K), corresponding to the k∗’th sample drawn from the
shared prior. The sender can then communicate to the receiver the index k∗with log2 K bits. On
the receiver side, wk∗can be reconstructed by initializing the random number generator with R and
sampling the ﬁrst k∗samples from pθ(w). These procedures are described in Algorithms 1 and 2."
REC FOR EFFICIENT COMMUNICATION,0.033003300330033,"Havasi et al. (2018) set K to the exponential of the Kullback-Leibler (KL) divergence of qφ(w) to
the prior pθ(w) with an extra constant c, i.e., K = exp(KL(qφ(w)||pθ(w)) + c). In this case, the
message length is at least O(KL(qφ(w)||pθ(w))). This can be theoretically motivated based on the
work by Harsha et al. (2007); when the sender and the receiver share a source of randomness, under
some assumptions, this KL divergence is a lower bound on the expected message length (Flamich
et al., 2020). This brings forth an intuitive notion of compression that connects the compression rate"
REC FOR EFFICIENT COMMUNICATION,0.034653465346534656,Under review as a conference paper at ICLR 2022
REC FOR EFFICIENT COMMUNICATION,0.036303630363036306,"Algorithm 1 The sender-side algorithm for lossy
REC with the shared random seed R, the shared
prior pθ(w) and the number of prior samples K."
REC FOR EFFICIENT COMMUNICATION,0.037953795379537955,"Set state of pseudo-RNG to R
Draw w1, . . . , wK
i.i.d.
∼pθ(w)
αk ←qφ(w = wk)/pθ(w = wk)
πk ←αk/ P"
REC FOR EFFICIENT COMMUNICATION,0.039603960396039604,"j αj
wk∗∼˜qπ(w1:K)
return k∗encoded with log2 K bits"
REC FOR EFFICIENT COMMUNICATION,0.041254125412541254,"Algorithm 2 The receiver-side algorithm for
lossy REC."
REC FOR EFFICIENT COMMUNICATION,0.0429042904290429,"Receive k∗from the sender
Set state of pseudo-RNG to R
Draw w1, . . . , wk∗i.i.d.
∼pθ(w)
Use wk∗for downstream task"
REC FOR EFFICIENT COMMUNICATION,0.04455445544554455,"with the amount of additional information encoded in qφ(w) relative to the information in pθ(w);
the smaller the amount of extra information the shorter the message length will be and, in the extreme
case where qφ(w) = pθ(w), the message length will be O(1). Of course, achieving this efﬁciency
is meaningless if the bias of this procedure is high; fortunately, Havasi et al. (2018) show that for
appropriate values of c and under mild assumptions, the bias, namely |E˜qπ(w1:K)[f] −Eqφ(w)[f]| for
arbitrary functions f, can be sufﬁciently small. In all of our subsequent discussions, we parametrize
K as a function of a binary bit-width b, i.e., K = 2b, and treat b as the hyperparameter."
REC FOR EFFICIENT COMMUNICATION IN FL,0.0462046204620462,"2.2
REC FOR EFFICIENT COMMUNICATION IN FL"
REC FOR EFFICIENT COMMUNICATION IN FL,0.04785478547854786,"We can adapt this procedure to FL by appropriately choosing distributions over client-to-server
messages (model updates), q(t)
φs(∆(t)
s ) , along with the prior distribution p(t)
θ (∆(t)
s ) on each round t:"
REC FOR EFFICIENT COMMUNICATION IN FL,0.04950495049504951,"p(t)
θ (∆(t)
s ) := N(∆(t)
s |0, σ2I),
q(t)
φs(∆(t)
s ) := N(∆(t)
s |w(t)
s
−w(t), σ2I),
(3)"
REC FOR EFFICIENT COMMUNICATION IN FL,0.05115511551155116,"i.e., for the prior we use a Gaussian distribution centered at zero with appropriately chosen σ and for
the message distribution we opt for a Gaussian with the same standard deviation centered at the model
update. The form of q is chosen to provide a plug-in solution to potentially resource constrained
devices, as well as to readily satisfy the differential privacy constrains discussed in Section 2.3. Note
that, as opposed to the FedAvg client update deﬁnition in (2), here we consider ∆(t)
s
to be a random
variable and the difference w(t)
s
−w(t) to be the mean of the client-update distribution q over ∆(t)
s ."
REC FOR EFFICIENT COMMUNICATION IN FL,0.052805280528052806,"Let us now see how communication efﬁciency is realized in the FL pipeline. The length of the message
will be a function of how much “extra” information about the local dataset Ds is encoded in w(t)
s ,
measured by KL divergence. As we show later, this has a nice interplay with DP: the DP constraints
bound the amount of information encoded in each update, resulting in highly compressible messages.
It is also worth noting that this procedure can be done parameter-wise (i.e., communicate log2 K
bits per parameter), layer-wise (log2 K bits for each layer in the network) or even network-wise
(log2 K bits in total). Any arbitrary intermediate vector size is also possible. This is done by splitting
∆(t)
s
into M independent groups (which is possible due to our assumption of factorial distributions
over the dimensions of the vector) and applying compression independently to each group (see also
Theorem 3). If we have many groups (e.g., when we perform per-parameter compression), we can
boost the compression even further by entropy coding the indices with their empirical distribution."
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC,0.054455445544554455,"2.3
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC"
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC,0.056105610561056105,"In order to make the compression procedure described in Section 2.2 differentially private, we
need to bound the sensitivity of the mechanism and quantify the noise inherent to it. Similarly to
DP-FedAvg, bounding the sensitivity consists of clipping the norm of client updates w(t)
s −w(t). In
the context of REC, this means that the client message distribution q(t)
φ cannot be too different from"
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC,0.057755775577557754,"the server prior p(t)
θ in any given round t. After this step, instead of explicitly injecting additional
noise to the updates, we make use of the fact that the procedure in itself is stochastic. Two sources
of randomness play a role in each round t: (1) drawing K samples from the prior p(t)
θ ; (2) drawing
an update from the importance sampling distribution ˜q(t)
π . We coin the name Differentially-Private
Relative Entropy Coding (DP-REC) for the resulting mechanism, outlined in Algorithms 3 and 4."
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC,0.0594059405940594,Under review as a conference paper at ICLR 2022
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC,0.06105610561056106,"Algorithm 3 The client-side algorithm for DP-REC at a
given round t. R(t)
s
is the client-chosen shared random seed,
w(t) is the server model, w(t)
s
is the ﬁne-tuned model of
client s, K is the number of prior samples, C is the clipping
threshold and σ is the prior standard deviation."
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC,0.0627062706270627,"R(t)
s
←choose and set client-speciﬁc seed for round t
φ(t)
s
←w(t)
s
−w(t)"
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC,0.06435643564356436,"∆1, . . . , ∆K
i.i.d.
∼N(∆(t)
s |0, σ2I)
ˆφ(t)
s
= φ(t)
s min(1, C/∥φ(t)
s ∥2)
αk ←N(∆(t)
s
= ∆k| ˆφ(t)
s , σ2I)/N(∆(t)
s
= ∆k|0, σ2I)
πk ←αk/ P"
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC,0.066006600660066,"j αj
∆k∗∼˜q(t)
π (∆1:K)
return k∗encoded with log2 K bits, R(t)
s"
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC,0.06765676567656766,"Algorithm 4 The server side algo-
rithm for DP-REC."
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC,0.06930693069306931,"Receive k∗, R(t)
s
from client s
Set state of pseudo-RNG to R(t)
s
∆1, . . . , ∆k∗i.i.d.
∼N(∆(t)
s |0, σ2I)
Use ∆k∗for downstream task"
PRIVATE & EFFICIENT COMMUNICATION FOR FL WITH DP-REC,0.07095709570957096,"Finally, to prove that DP-REC is differentially private and calculate the corresponding ε, δ, we build
upon prior work in privacy accounting for ML and FL; particularly, on Rényi differential privacy
(RDP) (Mironov, 2017) and the moments accountant (Abadi et al., 2016). Our analysis in the next
section follows the established approach based on tail bounds of the privacy loss random variable,
addressing such important differences as two sources of randomness and non-Gaussianity of ˜q(t)
π .
Additional discussion on other aspects of our method can be found in Appendices B.1 and D.2."
PRIVACY ANALYSIS OF DP-REC,0.07260726072607261,"2.4
PRIVACY ANALYSIS OF DP-REC"
PRIVACY ANALYSIS OF DP-REC,0.07425742574257425,"A possible avenue to analyze privacy for DP-REC is to derive DP bounds relying on Theorem 3.2 of
Havasi et al. (2018). However, obtaining reasonable guarantees can be challenging, as the probability
bound in (Havasi et al., 2018, Theorem 3.2) has to be incorporated in δ and it is at least e−0.125 ln K.
As such, a fairly common value δ = 10−5 would require
∼e92 samples from the prior. To overcome
this issue, we consider an importance sampling bound by Agapiou et al. (2017, Section 2.2). It scales
(inversely) linearly with the number of samples, helping for smaller sample sizes."
PRIVACY ANALYSIS OF DP-REC,0.07590759075907591,"Let us restate the bound for completeness. For some test function ζ, one can characterize the bound
between the true expectation and the importance sampling estimate in the following way:"
PRIVACY ANALYSIS OF DP-REC,0.07755775577557755,"sup
|ζ|<1"
PRIVACY ANALYSIS OF DP-REC,0.07920792079207921,"E

µN(ζ) −µ(ζ)
 ≤12"
PRIVACY ANALYSIS OF DP-REC,0.08085808580858085,"K eD2(qφ||pθ),
(4)"
PRIVACY ANALYSIS OF DP-REC,0.08250825082508251,"where K is the number of samples, µN(·) = E˜qπ[·], µ(·) = Eqφ[·] and D2(·||·) denotes the Rényi
divergence of order 2 between the target qφ and the proposal pθ. Our choice of ζ (cf. Appendix A.1)
guarantees |ζ| < 1 for any input and enables the use of this bound."
PRIVACY ANALYSIS OF DP-REC,0.08415841584158416,"Theorem 1 summarizes our main theoretical result. It enables DP-REC to combine the server-side
privacy accounting in the central model of DP with extreme model update compression, while
allowing clients to privatize their updates locally and protect against an honest-but-curious server.
Theorem 1. After T rounds, with the client-to-server bitrate b, DP-REC is differentially private with"
PRIVACY ANALYSIS OF DP-REC,0.0858085808580858,"δ ≤min
λ e−λε
T
Y"
PRIVACY ANALYSIS OF DP-REC,0.08745874587458746,"t=1
e(λ−1)c(t)
λ +λc(t)
λ+1 + 12"
B,0.0891089108910891,"2b T
X"
B,0.09075907590759076,"t=1
ec(t)
2 ,"
B,0.0924092409240924,"where the constant c(t)
λ
≥Dλ

q(t)
φ ||p(t)
θ

is the upper bound on the Rényi divergence of order λ"
B,0.09405940594059406,"between the client model update distribution q(t)
φ and the server prior p(t)
θ in any given round t."
B,0.09570957095709572,Proof. See Appendix A.1.
B,0.09735973597359736,"An attentive reader may notice a strong resemblance between Theorem 1 and the moments accoun-
tant (Abadi et al., 2016) and Rényi DP (Mironov, 2017) results. Indeed, it turns out that the DP-REC"
B,0.09900990099009901,Under review as a conference paper at ICLR 2022
B,0.10066006600660066,"compression procedure yields the privacy loss that can be bound by almost the double of the normal
continuous Gaussian mechanism loss, plus a small overhead, and composed in a very similar manner."
B,0.10231023102310231,"Shared random seed Prior work relying on shared random seeds has been shown to be vulnerable
to attacks due to the possibility of “inverting” the noise (Kairouz et al., 2021). DP-REC is not
susceptible to this for two reasons. First, the randomness of the method comes from two sources
instead of one, and knowing one seed does not permit to fully reconstruct the un-noised sample.
In our guarantee, δ corresponds to the probability of the mechanism failure w.r.t. both sources of
randomness, making it as secure as the standard Gaussian mechanism even when one of the seeds
is shared. However, this alone is not sufﬁcient because the shared seed R could be manipulated to
generate a set of samples on which to encode the update such that the guarantee would not hold (e.g.,
in the tails of the privacy loss distribution). Hence, we add the second line of defense as we shift the
task of picking R to a client, who then transmits the seed along with the index k∗for decoding. This
way, neither the server nor any other entity can manipulate the seed to break the privacy guarantee."
B,0.10396039603960396,"Privacy ampliﬁcation Like many recent approaches, including (Kairouz et al., 2021), DP-REC can
be seen as a hybrid method, privatizing updates locally and providing an ampliﬁed central guarantee.
A key difference, however, is that our method does not calibrate noise to the aggregate. Namely, the
scale of noise and the guarantee do not depend on whether a thousand, a hundred, or just one client is
sampled in a round. This is due to non-additive nature of our mechanism, which does not let us easily
beneﬁt from the variance reduction as the Gaussian mechanism does. As a result, we always calibrate
to an “aggregate” of one client. While this property increases the necessary noise, it has an upside of
allowing stronger privacy ampliﬁcation. Substituting the typical sampling without replacement by
sampling with replacement, we can use the ampliﬁcation factor of 1/|S| while accounting for T|S′|
rounds, instead of |S′|/|S| while accounting for T rounds, and obtain a tighter overall guarantee."
B,0.10561056105610561,"Secure aggregation One of the considerations in FL is that the server might be honest-but-curious
rather than fully trusted. In DP-FedAvg (McMahan et al., 2017), the noise addition is delegated to
the server, and thus, secure aggregation (Bonawitz et al., 2017) is necessary to prevent the server from
receiving client updates in the clear. DDGauss (Kairouz et al., 2021) and analogous methods mitigate
this problem by shifting the noise addition to the client side, but still use secure aggregation for the
reasons stated in the previous paragraph: noise is calibrated to the aggregate and may be insufﬁcient
to protect some clients. In DP-REC, because of the noise calibration to individual updates, we believe
that the provided local guarantee (which can be computed by removing subsampling ampliﬁcation)
is sufﬁcient against non-malicious servers. To further boost privacy protection, an important future
work direction is integrating the DP-REC compression scheme with secure aggregation."
B,0.10726072607260725,"Interplay of privacy and compression Theorem 1 relates privacy to our compression scheme in
an intuitive manner. In order to get meaningful privacy guarantees, we have to bound the Rényi
divergence in any given round t for any λ ≥1, which limits the amount of information encoded
in q(t)
φ relative to p(t)
θ . As a result, enforcing DP guarantees also implies that our scheme will have
highly compressible messages. We formalize this interplay in the following lemma."
B,0.10891089108910891,"Lemma 2. Consider compressing the expected value of ζ under the qφs(∆s) at (3), and let ξ be a
desired average compression bias of REC for ζ. To achieve this target, a sufﬁcient bitrate b is at most"
B,0.11056105610561057,"b ≤log2 12 +
1
log 2D2 (qφs||pθ) −log2
ξ
G,"
B,0.11221122112211221,"where |ζ| ≤G and D2 (qφs||pθ), and thus the maximum bitrate b, is controlled by the chosen (ε, δ)."
B,0.11386138613861387,"Proof. The proof follows from (4), by setting K = 2b, rearranging the terms and adjusting for G."
B,0.11551155115511551,"We also verify the claim empirically (Figure 2), ﬁxing the bound on the Rényi divergence and showing
that the performance with extreme compression (i.e., 7 bits/tensor) and no compression is similar.
Finally, to conﬁrm that various granularity of compression (per-parameter, per-layer, per-network,
etc.) does not interfere with our privacy analysis, we introduce and prove the following theorem."
B,0.11716171617161716,"Theorem 3 (Compression-by-parts). Expectation of an arbitrary function ζ(∆[1], . . . , ∆[M]) over
the importance sampling distributions q[1]
π , . . . , q[M]
π
, built for M non-intersecting parameter groups"
B,0.1188118811881188,Under review as a conference paper at ICLR 2022
B,0.12046204620462046,"independently, is equivalent to the expectation over the joint distribution qπ built on 2
PM
i=1 bi samples:"
B,0.12211221122112212,"E∆[1]∼q[1]
π"
B,0.12376237623762376,"h
. . . E∆[M]∼q[M]
π"
B,0.1254125412541254,"h
ζ(∆[1], . . . , ∆[M])
i
. . .
i
= E∆[1:M]∼qπ
h
ζ(∆[1:M])
i
."
B,0.12706270627062707,Proof. See Appendix A.2.
COMPRESSING SERVER-TO-CLIENT MESSAGES,0.12871287128712872,"2.5
COMPRESSING SERVER-TO-CLIENT MESSAGES"
COMPRESSING SERVER-TO-CLIENT MESSAGES,0.13036303630363036,"The compression procedure described in Section 2.2 is a speciﬁc example of (stochastic) vector
quantization where the shared codebook is determined by a shared random seed. Here we show
how the principle of communicating indices into such a shared codebook additionally allows for the
compression of the server-to-client communication. Instead of sending the full server-side model
to a speciﬁc client, the server can choose to collect all updates to the global model in-between two
subsequent rounds that the client participates in. Based on this history of codebook indices, the
client can deterministically reconstruct the current state of the server model before beginning local
optimization. Clearly, the expected length of the history is proportional to the total number of clients
and the amount of client subsampling we perform during training. At the beginning of a round, the
server can therefore compare the bit-size of the history and choose to send the full-precision model
w(t) instead. Taking a model with 1k parameters as an example, a single uncompressed model update
is approximately equal to 4k communicated indices when using 8-bit codebook compression of the
whole model. Crucially, compressing server-to-client messages this way has no inﬂuence on the DP
nature of DP-REC since the local model of DP ensures privacy of client information (and the stronger
central guarantee applies as long as the secrecy of the sample is preserved within the updates history)."
COMPRESSING SERVER-TO-CLIENT MESSAGES,0.132013201320132,"For clients participating in their ﬁrst round of training, the ﬁrst seed without accompanying indices
can be understood as seeding the random initialization of the server-side model. Algorithms 5 and 6
(in Appendix B.2) describe this scheme. It is important to note that the client-side update rule must
be equal to the server-side update rule, i.e. in generalized FedAvg (Reddi et al., 2020) it might be
necessary to additionally send the optimizer state when sending the current global model w(t)."
RELATED WORK,0.13366336633663367,"3
RELATED WORK"
RELATED WORK,0.1353135313531353,"The privacy promise of federated learning relies heavily on the use of additional techniques, such as
differential privacy and secure multi-party computation, to provide rigorous theoretical guarantees.
Without these techniques, FL has been shown to be vulnerable to attacks (Geiping et al., 2020) and
unintended leakage of sensitive information (Thakkar et al., 2020). One of the main open challenges
is to reduce the communication cost while preserving DP guarantees (Kairouz et al., 2019)."
RELATED WORK,0.13696369636963696,"McMahan et al. (2017) outlined DP-FedAvg, which has since been the staple of DP federated
learning. In this default scheme, clients clip norms of their updates before submitting them to the
server (preferably, using a secure aggregation protocol (Bonawitz et al., 2017)). The server then
completes the DP mechanism by adding Gaussian noise to the aggregate of multiple clients. Without
secure aggregation, this method allows clients to compress their updates and reduce communication,
but becomes vulnerable to a malicious or honest-but-curious server. One could use the local model of
DP (Dwork et al., 2014) instead of the central model to address this issue: clients would add noise
locally before sending the updates. But this leads to a pronounced drop in accuracy due to the larger
scale of noise necessary in the local model (Kairouz et al., 2019). The few existing examples that use
the local model operate on very large numbers of clients and updates (Pihur et al., 2018)."
RELATED WORK,0.13861386138613863,"To overcome this problem of excessive noise, researchers are increasingly looking in the direction
of hybrid solutions (Shi et al., 2011; Rastogi & Nath, 2010; Agarwal et al., 2018; Truex et al.,
2019). The key idea of these techniques is to reduce the variance of the locally added noise by
taking into account the larger global number of clients and accounting DP in the central model.
Since the smaller noise variance is insufﬁcient to protect individual updates, secure aggregation is
necessary for these methods. In this context, the local noise distributions have to be discrete, such
that their sum after secure aggregation is also discrete and is of known shape, allowing the server to
compute guarantees centrally. As a result, until recently, most of these methods relied on the binomial
distribution. Compared to the traditional Gaussian mechanisms, it is at a disadvantage because it does
not yield Renyi or concentrated DP, and thus cannot beneﬁt from tighter adaptive composition and"
RELATED WORK,0.14026402640264027,Under review as a conference paper at ICLR 2022
RELATED WORK,0.1419141914191419,"ampliﬁcation through sampling, which is particularly important in ML applications (Kairouz et al.,
2021). Kairouz et al. (2021) presented a novel analysis of the discrete Gaussian mechanism (Canonne
et al., 2020) in terms of concentrated DP. Their mechanism, named Discrete Distributed Gaussian
(DDGauss), is adapted to the context of FL, can provide accuracy comparable to the centralized
Gaussian mechanism, and enables parameter-wise quantization. However, at higher compression
rates (e.g., 12 bits per parameter), DDGauss fails to train a model for reasonable ε values."
RELATED WORK,0.14356435643564355,"Finally, there is recent work featuring extreme compression rates for DP algorithms, bearing a
resemblance to our solution. Girgis et al. (2020) proposed to use a locally private mechanism along
with a secure shufﬂer to communicate models one (privatized) parameter at a time. It compresses
client messages to log d bits for a model of d parameters. This approach, however, seems to require a
large number of clients to participate in a single round (10k for their MNIST experiment), which is
impractical in realistic scenarios and detrimental to the total communication cost (upload + download).
Chen et al. (2020) achieve similarly high compression rates as (Girgis et al., 2020); however, they do
not consider the context of federated learning but rather focus on frequency and mean estimation. As
we did not have an FL baseline for (Chen et al., 2020) and could not reliably reproduce the results of
Girgis et al. (2020), we focused our comparison with these methods on mean estimation. Details can
be found in Appendix D.2. In short, we ﬁnd that the method of Chen et al. (2020) has an edge over
DP-REC if a good prior is lacking, making it preferable for low-information, “one-shot” scenarios,
such as mean estimation. On the other hand, with a good prior (or one that is learned during training),
DP-REC performs better and thus is more appropriate in federated settings."
EXPERIMENTS,0.14521452145214522,"4
EXPERIMENTS"
EXPERIMENTS,0.14686468646864687,"We evaluate DP-REC on several non-i.i.d. FL tasks that provide client-level privacy guarantees:
image classiﬁcation on a non-i.i.d. split of MNIST (LeCun et al., 1998) into 100 clients and FEM-
NIST (Caldas et al., 2018) into 3.5k clients, along with next character prediction on the Shakespeare
dataset (Caldas et al., 2018) with LSTMs (Hochreiter & Schmidhuber, 1997) and 660 clients as well
as tag prediction on the StackOverﬂow (SO-LR) dataset (TFF Authors, 2019) with a logistic regres-
sion model (Kairouz et al., 2021) and 342477 clients. For baselines, we consider DP-FedAvg as the
gold standard without compression and DDGauss as a baseline that involves parameter quantization.
Both of these methods target central DP guarantees and employ secure aggregation. All exeriments
were implemented in PyTorch (Paszke et al., 2019). We provide experimental details in Appendix C."
RESULTS AND DISCUSSION,0.1485148514851485,"4.1
RESULTS AND DISCUSSION"
RESULTS AND DISCUSSION,0.15016501650165018,"We plot the global model accuracy for different privacy budgets ε as a function of the total communi-
cation cost in Figure 1; this highlights how efﬁciently each method spends bits of communication
in order to reach a target accuracy. Due to the substantial compression by DP-REC relative to
the baselines, we use log-scale for the x-axis of communication costs. Additionally, Figure 3 in
Appendix D shows the accuracy achieved as a function of the privacy budget ε; this highlights how
efﬁciently each method spends its privacy budget in order to reach a speciﬁc target accuracy. Note
that in certain cases the training is stopped before convergence, due to exhausted privacy budget."
RESULTS AND DISCUSSION,0.15181518151815182,"In Table 1, we report the ﬁnal model performance and total communication costs for different privacy
budgets. It should be mentioned that the evaluation loop for StackOverﬂow is prohibitively expensive
to be done in each round due to having
∼1.6M datapoints. We thus pick 10k random datapoints
for evaluation during training to plot the learning curves (not necessarily the same for each run),
following (Reddi et al., 2020). The numbers in Table 1 refer to the model performance at the end
of training, where we evaluate on the entire test set for DP-FedAvg and DP-REC. For DDGauss,
since Kairouz et al. (2021) do not report the model performance on the full test set, we use the
numbers shown at the end of their plot. Finally, results for some settings are omitted because of
either (i) convergence issues for stricter privacy budgets (a typical phenomenon for ε < 1 on smaller
federations); (ii) not being reported by the related work we compare with; or (iii) not being necessary
in light of sufﬁcient performance under better privacy guarantees (e.g., on StackOverﬂow)."
RESULTS AND DISCUSSION,0.15346534653465346,"Observing the experimental results, we clearly see the trade-offs. DP-REC can drastically reduce the
total communication costs (download and upload) of federated training depending on the subsampling
rate and amount of clients in a federation. For certain cases, we can get extreme reduction, such"
RESULTS AND DISCUSSION,0.1551155115511551,Under review as a conference paper at ICLR 2022
RESULTS AND DISCUSSION,0.15676567656765678,"Table 1: Performance (± standard error obtained via multiple runs) and total communication (in
GB), achieved for different privacy guarantees ε. Results for DDGauss (marked DDGx for x bits)
are taken from (Kairouz et al., 2021), which uses a slightly smaller version of the FEMNIST dataset
(3.4k clients instead of 3.5k)."
RESULTS AND DISCUSSION,0.15841584158415842,"MNIST
DP-FedAvg
DP-REC"
RESULTS AND DISCUSSION,0.16006600660066006,"Acc. (ε=3)
82.1 ± 0.8
66.5 ± 1.5
Acc. (ε=6)
90.0 ± 0.4
79.0 ± 1.9
Comm.
43
0.01"
RESULTS AND DISCUSSION,0.1617161716171617,"FEMNIST
DP-FedAvg DDG16 DDG12
DP-REC"
RESULTS AND DISCUSSION,0.16336633663366337,"Acc. (ε=1)
65.7 ± 0.3
–
–
59.3 ± 0.1
Acc. (ε=3)
74.2 ± 0.1
∼72
∼25
67.0 ± 0.1
Acc. (ε=6)
75.5 ± 0.2
∼77
∼71
69.1 ± 0.1
Comm.
259
194
178
14.2"
RESULTS AND DISCUSSION,0.16501650165016502,"Shakespeare DP-FedAvg
DP-REC"
RESULTS AND DISCUSSION,0.16666666666666666,"Acc. (ε=3)
39.0 ± 0.1
29.0 ± 0.1
Comm.
81
0.1"
RESULTS AND DISCUSSION,0.16831683168316833,"SO LR
DP-FedAvg DDG16 DDG12
DP-REC"
RESULTS AND DISCUSSION,0.16996699669966997,"R@5 (ε=1)
19.3 ± 0.0
∼14
∼10
18.4 ± 0.7
Comm.
3356
2517
2307
32"
RESULTS AND DISCUSSION,0.1716171617161716,"(a) MNIST
(b) FEMNIST"
RESULTS AND DISCUSSION,0.17326732673267325,"(c) Shakespeare
(d) StackOverﬂow"
RESULTS AND DISCUSSION,0.17491749174917492,"Figure 1: Test accuracy (%) with ±1 standard error as a function of communication (in GB, in
log-scale). For StackOverﬂow, we report compression at the end of training, as the learning curves
are based on different subsets of the validation set."
RESULTS AND DISCUSSION,0.17656765676567657,"as MNIST with 4300x compression. But even for other cases, the reduction is still several orders
of magnitude (105x for SO-LR and 18x on FEMNIST). Nevertheless, DP-FedAvg and DDGauss
(depending on the task) can reach better model performance for a given privacy budget. This is
primarily due to two reasons. Firstly, both DP-FedAvg and DDGauss add noise calibrated to the
number of clients in a given round to get a central DP guarantee on the aggregate; in contrast, DP-REC
calibrates noise to the contribution of individual updates (see Section 2.4). The signal-to-noise-ratio
for the model updates is thus worse for DP-REC. Secondly, in DP-REC we have to clip client updates
more aggressively to account for the privacy loss overhead incurred from the REC compression
(roughly double for each iteration compared to Gaussian mechanism). This can be mitigated in larger
federations, since the clipping can be reduced based on stronger privacy ampliﬁcation for the central
model of DP. For example, observe the StackOverﬂow experiment, where the accuracy delta between
DP-REC and DP-FedAvg is smaller. Furthermore, it is precisely in those cases that reducing the
communication cost is important from a practical perspective. When we target a speciﬁc accuracy"
RESULTS AND DISCUSSION,0.1782178217821782,Under review as a conference paper at ICLR 2022
RESULTS AND DISCUSSION,0.17986798679867988,"within the reach of both DP-REC and DP-FedAvg, we compress between 8.3x (FEMNIST) to
1533x (MNIST), at the additional cost of 1.9 to 2.2 in ε, relative to DP-FedAvg."
RESULTS AND DISCUSSION,0.18151815181518152,"Comparison with local DP guarantees
The nature of the DP-REC privacy mechanism requires
calibrating noise to individual client updates rather than their contribution to the aggregate. This
property warrants a different kind of comparison: equalizing local guarantees for each individual
update (i.e., as seen when the secrecy of the sample in client sampling is not preserved). We use
our non-i.i.d. FEMNIST setting with 1.5k rounds and tune the noise of DP-FedAvg such that
a single client update, without any privacy ampliﬁcation, has the same εlocal guarantee. When
training DP-FedAvg with a local-DP guarantee that DP-REC obtains when targeting central-DP
with ε = 3, we see that DP-FedAvg obtains 68.9% accuracy, whereas DP-REC gets 63.4%. On
a setting where we target the local-DP guarantee that DP-REC gets when aiming for central-DP
with ε = 1, DP-FedAvg achieves 60.1% accuracy compared to DP-REC’s 57.4%. In both cases,
DP-REC achieves an overall compression rate of 72.5x with 7-bits per tensor, while losing 2.7% to
5.5% in accuracy compared to DP-FedAvg (depending on the privacy target) due to more aggressive
clipping. It is also worth noting that the performance delta relative to the results shown in Table 1 is
smaller, highlighting the negative effects of calibrating the noise to an “aggregate” of a single client."
RESULTS AND DISCUSSION,0.18316831683168316,"Figure 2: Effect of the bit-width on
model performance with DP-REC
for clipping targeting ε = 3 after
1.5k rounds. The “no-compression”
variant communicates a random sam-
ple directly from q(t)
φ instead of com-
pressing it via DP-REC."
RESULTS AND DISCUSSION,0.1848184818481848,"Compressibility of differentially private updates
As we
noted in Section 2.4, differential privacy has a positive inter-
play with the REC compression scheme. DP upper-bounds
the Rényi divergence for any order λ between the proposal
distribution p(t)
θ
and the model update distribution q(t)
φ and,
according to lemma 2, this limits the maximum bitrate b of in-
formation to be communicated. In order to empirically verify
this claim, we consider the non-i.i.d. FEMNIST classiﬁcation
with DP-REC, and hyperparameters that target a model with
ε = 3 after 1.5k rounds. We then maintain the same bound on
Rényi divergence and vary the number of bits per tensor from
4 to 7. We also include a variant without compression, which
directly communicates a random sample from the clipped up-
date distribution q(t)
φ . The results appear to verify our claim
and can be seen in Figure 2. After 4 bits, we see very small
improvements with adding additional bits and, with 7 bits, the
performance is very similar to the uncompressed baseline."
CONCLUSION,0.18646864686468648,"5
CONCLUSION"
CONCLUSION,0.18811881188118812,"With DP-REC, we formalized the intuitive notion that ε, δ differentially private messages necessarily
contain a small amount of information and can therefore be compressed signiﬁcantly. A bound on the
Rényi divergence between the server-side prior p and the locally optimized q implies a small message
size for REC, elegantly tying together DP and communication efﬁciency for federated learning. The
nature of our bound in Theorem 1 reveals the ﬂipside of DP-REC. As opposed to the standard
Gaussian mechanism in central DP, it requires stronger clipping, effectively reducing the utility of
models trained with DP-REC for a given privacy budget. Our experiments with the StackOverﬂow
dataset show that these limitations can be mitigated by the stronger privacy ampliﬁcation in situations
with large numbers of clients. Contrary to intuition, spending more bits to communicate updates from
clients to the server cannot recover this utility as the necessary information is lost in clipping, i.e.,
before forming the importance sampling distribution ˜q."
CONCLUSION,0.18976897689768976,"There are several important directions left for future work. The ﬁrst would be to investigate the
convergence of DP-REC. Intuitively, we can expect the algorithm to be convergent if DP-FedAvg
is convergent. The main difference of DP-REC is that the gradient is sampled using importance
weights rather than the true Gaussian probabilities. Asymptotically, these two distributions should be
close, and the bias of the compressed gradients would be bounded by a small value (as discussed in
Section 2.4). Empirically, we observe no issues with convergence. The second would be to investigate
secure shufﬂing, similarly to Girgis et al. (2020) as a way to amplify our privacy further; this can lead
to less aggressive clipping and thus improved performance for a given privacy budget."
CONCLUSION,0.19141914191419143,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.19306930693069307,ETHICS STATEMENT
ETHICS STATEMENT,0.19471947194719472,"This paper addresses a signiﬁcant, present-day problem and provides a clear, detailed explanation
of our solution to facilitate reproducibility and promote research integrity. We expect our work
to have an overall beneﬁcial societal impact through its main contributions of communication
reduction and differential privacy. One potential concern worth mentioning is the interplay between
differential privacy and fairness in FL, an actively researched open problem. Bagdasaryan et al.
(2019) demonstrate that DP training can have a disproportionate effect on underrepresented and more
complex sub-populations, resulting in a disparate accuracy reduction. At the same time, Hooker
et al. (2020) showed that compression of models can have negative impacts by amplifying biases. In
the context of FL, compression is performed on model updates instead of a ﬁnal model, therefore
requiring further research on its inﬂuence on bias ampliﬁcation."
REPRODUCIBILITY STATEMENT,0.19636963696369636,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.19801980198019803,"For the theoretical results, we clearly state our assumptions and present complete proofs in the
appendix. For experimental evaluation, although we cannot provide the source code at the time of the
submission because it is proprietary, we include extensive information on algorithms and settings in
the appendix, and repeat our experiments with multiple random seeds to aid reproducibility."
REFERENCES,0.19966996699669967,REFERENCES
REFERENCES,0.20132013201320131,"Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308–318, 2016."
REFERENCES,0.20297029702970298,"Sergios Agapiou, Omiros Papaspiliopoulos, Daniel Sanz-Alonso, and AM Stuart. Importance
sampling: Intrinsic dimension and computational cost. Statistical Science, pp. 405–431, 2017."
REFERENCES,0.20462046204620463,"Naman Agarwal, Ananda Theertha Suresh, Felix Yu, Sanjiv Kumar, and H Brendan Mcma-
han. cpsgd: Communication-efﬁcient and differentially-private distributed sgd. arXiv preprint
arXiv:1805.10559, 2018."
REFERENCES,0.20627062706270627,"Galen Andrew, Om Thakkar, H Brendan McMahan, and Swaroop Ramaswamy. Differentially private
learning with adaptive clipping. arXiv preprint arXiv:1905.03871, 2019."
REFERENCES,0.2079207920792079,"Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate
impact on model accuracy. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
fc0de4e0396fff257ea362983c2dda5a-Paper.pdf."
REFERENCES,0.20957095709570958,"Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pp. 1175–1191, 2017."
REFERENCES,0.21122112211221122,"Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMa-
han, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv
preprint arXiv:1812.01097, 2018."
REFERENCES,0.21287128712871287,"Clément Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential
privacy. arXiv preprint arXiv:2004.00010, 2020."
REFERENCES,0.2145214521452145,"Wei-Ning Chen, Peter Kairouz, and Ayfer Ozgur. Breaking the communication-privacy-accuracy
trilemma.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 3312–3324. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
222afbe0d68c61de60374b96f1d86715-Paper.pdf."
REFERENCES,0.21617161716171618,Under review as a conference paper at ICLR 2022
REFERENCES,0.21782178217821782,"Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921–2926. IEEE, 2017."
REFERENCES,0.21947194719471946,"Cynthia Dwork. Differential privacy and the us census. In Proceedings of the 38th ACM SIGMOD-
SIGACT-SIGAI Symposium on Principles of Database Systems, pp. 1–1, 2019."
REFERENCES,0.22112211221122113,"Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211–407, 2014."
REFERENCES,0.22277227722772278,"Gergely Flamich, Marton Havasi, and José Miguel Hernández-Lobato. Compressing images by
encoding their latent representations with relative entropy coding. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.22442244224422442,"Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. Inverting gradients -
how easy is it to break privacy in federated learning? In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 16937–16947. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/c4ede56bbd98819ae6112b20ac6bf145-Paper.pdf."
REFERENCES,0.22607260726072606,"Antonious M Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and Ananda Theertha Suresh.
Shufﬂed model of federated learning: Privacy, communication and accuracy trade-offs. arXiv
preprint arXiv:2008.07180, 2020."
REFERENCES,0.22772277227722773,"Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communica-
tion complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational
Complexity (CCC’07), pp. 10–23. IEEE, 2007."
REFERENCES,0.22937293729372937,"Marton Havasi, Robert Peharz, and José Miguel Hernández-Lobato. Minimal random code learning:
Getting bits back from compressed model parameters. arXiv preprint arXiv:1810.00440, 2018."
REFERENCES,0.23102310231023102,"Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Rad-
ford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam
McCandlish. Scaling laws for autoregressive generative modeling, 2020."
REFERENCES,0.23267326732673269,"Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.23432343234323433,"Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. Characterising
bias in compressed models. arXiv preprint arXiv:2010.03058, 2020."
REFERENCES,0.23597359735973597,"Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classiﬁcation. arXiv preprint arXiv:1909.06335, 2019."
REFERENCES,0.2376237623762376,"Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019."
REFERENCES,0.23927392739273928,"Peter Kairouz, Ziyu Liu, and Thomas Steinke. The distributed discrete gaussian mechanism for
federated learning with secure aggregation. arXiv preprint arXiv:2102.06387, 2021."
REFERENCES,0.24092409240924093,"Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361, 2020."
REFERENCES,0.24257425742574257,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.24422442244224424,"Estelle Laziuk. Daily ios 14.5 opt-in rate, 2021. URL https://www.flurry.com/blog/ios-
14-5-opt-in-rate-att-restricted-app-tracking-transparency-
worldwide-us-daily-latest-update/."
REFERENCES,0.24587458745874588,Under review as a conference paper at ICLR 2022
REFERENCES,0.24752475247524752,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.24917491749174916,"H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efﬁcient
learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016."
REFERENCES,0.2508250825082508,"H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models. arXiv preprint arXiv:1710.06963, 2017."
REFERENCES,0.2524752475247525,"Ilya Mironov. Rényi differential privacy. In 2017 IEEE 30th Computer Security Foundations
Symposium (CSF), pp. 263–275. IEEE, 2017."
REFERENCES,0.25412541254125415,"Ilya Mironov, Kunal Talwar, and Li Zhang. Rényi differential privacy of the sampled gaussian
mechanism. arXiv preprint arXiv:1908.10530, 2019."
REFERENCES,0.25577557755775576,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran
Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-
an-imperative-style-high-performance-deep-learning-library.pdf."
REFERENCES,0.25742574257425743,"Vasyl Pihur, Aleksandra Korolova, Frederick Liu, Subhash Sankuratripati, Moti Yung, Dachuan
Huang, and Ruogu Zeng. Differentially-private"" draw and discard"" machine learning. arXiv
preprint arXiv:1807.04369, 2018."
REFERENCES,0.2590759075907591,"Vibhor Rastogi and Suman Nath. Differentially private aggregation of distributed time-series with
transformation and encryption. In Proceedings of the 2010 ACM SIGMOD International Confer-
ence on Management of data, pp. 735–746, 2010."
REFERENCES,0.2607260726072607,"Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn`y,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint
arXiv:2003.00295, 2020."
REFERENCES,0.2623762376237624,"Elaine Shi, TH Hubert Chan, Eleanor Rieffel, Richard Chow, and Dawn Song. Privacy-preserving
aggregation of time-series data. In Proc. NDSS, volume 2, pp. 1–17. Citeseer, 2011."
REFERENCES,0.264026402640264,"TFF
Authors.
Tensorﬂow
federated
stack
overﬂow
dataset.
Online:
https:
//www.tensorflow.org/federated/api_docs/python/tff/simulation/
datasets/stackoverflow, 2019."
REFERENCES,0.26567656765676567,"Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, and Françoise Beaufays. Understanding
unintended memorization in federated learning. arXiv preprint arXiv:2006.07490, 2020."
REFERENCES,0.26732673267326734,"Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and
Yi Zhou. A hybrid approach to privacy-preserving federated learning. In Proceedings of the 12th
ACM Workshop on Artiﬁcial Intelligence and Security, pp. 1–11, 2019."
REFERENCES,0.26897689768976896,"Paul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). A Practical
Guide, 1st Ed., Cham: Springer International Publishing, 10:3152676, 2017."
REFERENCES,0.2706270627062706,"Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated
optimization. arXiv preprint arXiv:2107.06917, 2021."
REFERENCES,0.2722772277227723,"Yao Xiao and Josh Karlin. Federated learning of cohorts. URL https://wicg.github.io/
floc/."
REFERENCES,0.2739273927392739,Under review as a conference paper at ICLR 2022
REFERENCES,0.2755775577557756,APPENDIX
REFERENCES,0.27722772277227725,"A
PROOFS"
REFERENCES,0.27887788778877887,"A.1
PROOF OF THEOREM 1"
REFERENCES,0.28052805280528054,"Theorem 1. After T rounds, with the client-to-server bitrate b, DP-REC is differentially private with"
REFERENCES,0.28217821782178215,"δ ≤min
λ e−λε
T
Y"
REFERENCES,0.2838283828382838,"t=1
e(λ−1)c(t)
λ +λc(t)
λ+1 + 12"
B,0.2854785478547855,"2b T
X"
B,0.2871287128712871,"t=1
ec(t)
2 ,"
B,0.2887788778877888,"where the constant c(t)
λ
≥Dλ

q(t)
φ ||p(t)
θ

is the upper bound on the Rényi divergence of order λ"
B,0.29042904290429045,"between the client model update distribution q(t)
φ and the server prior p(t)
θ in any given round t."
B,0.29207920792079206,"Proof. Consider a privacy mechanism A : D →Rd, mapping a dataset D ∈D to a d-dimensional
model update ∆∈Rd. Recall that our mechanism features two sources of randomness: drawing
from distributions pθ and then ˜qπ (which is based on qφ and pθ)."
B,0.29372937293729373,"Similarly to the derivations for the moments accountant (Abadi et al., 2016), we can write"
B,0.2953795379537954,"Pr[A(D) ∈Ω] = Pr[A(D) ∈Ω∩¯S] + Pr[A(D) ∈Ω∩S]
(5)"
B,0.297029702970297,"≤eε Pr[A(D′) ∈Ω] + Pr[A(D) ∈S],
(6)"
B,0.2986798679867987,"where Ω⊂Rd is an arbitrary set of outcomes, S = {∆k∗: L˜qπ(∆k∗, D, D′) > ε} is the set of
outcomes where the bound on privacy loss is violated, and ¯S denotes a complement. For multiple
iterations, k∗can be viewed as the multi-index picked from the importance sampling distribution
across all iterations."
B,0.30033003300330036,"First, let us make a brief side-note on the privacy loss. As we pursue the goal of client-level privacy,
we consider two clients with different local datasets D and D′. Their update distributions are
parameterized by φ for one client and φ′ for another: q(1:T )
φ
(·) and q(1:T )
φ′
(·). We will denote the"
B,0.30198019801980197,"corresponding importance sampling distributions as ˜q(1:T )
π
(·) and ˜q(1:T )
π′
(·). Then the privacy loss for
these two clients is"
B,0.30363036303630364,"L˜q(1:T )
π
|| ˜q(1:T )
π′"
B,0.30528052805280526,"
∆(1:T )
k∗
, D, D′
= log
˜q(1:T )
π

∆(1:T )
k∗
"
B,0.3069306930693069,"˜q(1:T )
π′

∆(1:T )
k∗

(7)"
B,0.3085808580858086,"= log ˜q(1:T )
π
(·)"
B,0.3102310231023102,"˜p(1:T )
π
(·)
+ log ˜q(1:T )
π′
(·)"
B,0.3118811881188119,"˜p(1:T )
π
(·)
(8)"
B,0.31353135313531355,"= log ˜q(1:T )
π
(·)"
B,0.31518151815181517,"˜p(1:T )
π
(·)
+ log ˜q(1:T )
π′
(·)"
B,0.31683168316831684,"˜p(1:T )
π′
(·)
(9)"
B,0.3184818481848185,"= L˜q(1:T )
π
(·) + L˜q(1:T )
π′
(·) ,
(10)"
B,0.3201320132013201,"where ˜p(1:T )
π
= ˜p(1:T )
π′
is the uniform distribution over K samples from p(1:T )
θ
. Thus, it is sufﬁcient to
bound the privacy loss L˜q(1:T )
π
(·) for the worst-case φ with some ε, δ. For the centralized guarantee,
it would correspond to bounding the inﬂuence of adding or removing one client. If the local
guarantee, or bounding the inﬂuence of substituting a client, is necessary, it will be given by 2ε (and
correspondingly, 2δ). This is consistent with prior work where bounds on substitution are also double
the bounds on addition/removal."
B,0.3217821782178218,Under review as a conference paper at ICLR 2022
B,0.3234323432343234,Consider the second term of Eq. 6 for T rounds of training:
B,0.3250825082508251,"Pr
h
L˜q(1:T )
π
> ε
i
(11)"
B,0.32673267326732675,"=
Z
p(1:T )
θ

∆(1)
1:K . . . ∆(T )
1:K

K
X"
B,0.32838283828382836,"k1=1
˜q(1)
π

∆(1)
k1"
B,0.33003300330033003,"
. . . K
X"
B,0.3316831683168317,"kT =1
˜q(T )
π

∆(T )
kT"
B,0.3333333333333333,"∆(1:T −1)
1:K

1{L˜qπ >ε}d∆(1:T )
1:K ,
(12) where"
B,0.334983498349835,"L(1:T )
˜qπ
= log
˜qπ

∆(1:T )
1:K
"
B,0.33663366336633666,"˜pπ

∆(1:T )
1:K

(13)"
B,0.33828382838283827,"is the total privacy loss across all samples from all iterations. Note also that by ˜pπ(·) we denote an
importance sampling distribution formed when the proposal and the target are the same, which is
essentially uniform over the K samples."
B,0.33993399339933994,"Since 1{·} ≤1, we can employ the bound (4) directly. However, due to the iterative importance
sampling over rounds and possible dependencies between rounds, we have to use the law of total
expectation and apply the bound recursively. Namely, denoting ζ(T )(∆(1:T )
1:K ) = 1{L˜qπ >ε},"
B,0.3415841584158416,"Pr[L˜q(1:T )
π
> ε]"
B,0.3432343234323432,"=
Z
p(1)
θ

∆(1)
1:K

K
X"
B,0.3448844884488449,"k1=1
˜q(1)
π

∆(1)
k1"
B,0.3465346534653465,"
. . .
(14)"
B,0.3481848184818482,"Z
p(T )
θ

∆(T )
1:K
 ∆(1:T −1)
1:K

K
X"
B,0.34983498349834985,"kT =1
˜q(T )
π

∆(T )
kT"
B,0.35148514851485146,"∆(1:T −1)
1:K

ζ(T ) 
∆(1:T )
1:K

d∆(1:T )
1:K ,"
B,0.35313531353135313,"≤
Z
p(1)
θ

∆(1)
1:K

K
X"
B,0.3547854785478548,"k1=1
˜q(1)
π

∆(1)
k1"
B,0.3564356435643564,"
. . .
(15)"
B,0.3580858085808581,"Z
p(T )
θ

∆(T )
1:K
 ∆(1:T −1)
1:K
Z
q(T )
φ

∆(T ) ∆(1:T −1)
1:K

ζ(T ) 
∆(1:T )
1:K

d∆(T )+ 12 K ρT"
B,0.35973597359735976,"
d∆(1:T )
1:K"
B,0.3613861386138614,"≤
Z
p(1)
θ

∆(1)
1:K

K
X"
B,0.36303630363036304,"k1=1
˜q(1)
π

∆(1)
k1"
B,0.36468646864686466,"
. . .
(16)"
B,0.36633663366336633,"Z
p(T )
θ

∆(T )
1:K
 ∆(1:T −1)
1:K
 Z
q(T )
φ

∆(T ) ∆(1:T −1)
1:K

ζ(T ) 
∆(1:T )
1:K

d∆(T )d∆(T )
1:K
|
{z
}"
B,0.367986798679868,"ζ(T −1)

∆(1:T −1)
1:K
"
B,0.3696369636963696,"d∆(1:T −1)
1:K + 12"
B,0.3712871287128713,"K ρT ,
(17)"
B,0.37293729372937295,where we can pull (17) out due to its independence from the rest of the expectation.
B,0.37458745874587457,"As noted in line (16), one can then treat the inner expectations (over the distributions from round T)
as a new function ζ(T −1)(∆(1:T −1)
1:K
), which is still bounded by 1 because it’s an expectation of an
indicator function. Repeating the procedure, we get"
B,0.37623762376237624,"Pr[L˜q(1:T )
π
> ε]"
B,0.3778877887788779,"≤Ep(1:T )
θ "
B,0.3795379537953795,"Eq(1:T )
φ  1("
B,0.3811881188118812,"log
˜qπ(∆(1:T )
1:K )"
B,0.38283828382838286,"˜
pπ(∆(1:T )
1:K )
>ε )   "
B,0.3844884488448845,+ 12
B,0.38613861386138615,"2b T
X"
B,0.38778877887788776,"t=1
ρt"
B,0.38943894389438943,"| {z }
ρ(1:T )"
B,0.3910891089108911,",
(18)"
B,0.3927392739273927,Under review as a conference paper at ICLR 2022
B,0.3943894389438944,"with ρt = e
D2

q(t)
φ ||p(t)
θ

. It is worth noting one more time that q(t)
φ and p(t)
θ
are conditional distribu-
tions, depending on rounds 1...t −1, and that we do not assume independence between rounds."
B,0.39603960396039606,"Applying Chernoff inequality to (18):
Pr[L˜q(1:T )
π
> ε]"
B,0.3976897689768977,"≤e−λεEp(1:T )
θ "
B,0.39933993399339934,"Eq(1:T )
φ "
B,0.400990099009901,"e
λ log
˜qπ(∆(1:T )
1:K )"
B,0.40264026402640263,"˜
pπ(∆(1:T )
1:K )   "
B,0.4042904290429043,+ 12
B,0.40594059405940597,"2b ρ(1:T )
(19)"
B,0.4075907590759076,"= e−λεEp(1:T )
θ "
B,0.40924092409240925,"Eq(1:T )
φ   "
B,0.41089108910891087,"
˜qπ

∆(1:T )
1:K
"
B,0.41254125412541254,"˜pπ

∆(1:T )
1:K
   λ  "
B,0.4141914191419142,+ 12
B,0.4158415841584158,"2b ρ(1:T ).
(20)"
B,0.4174917491749175,"As we have done above in (14), we re-arrange expectations using the law of total expectation:"
B,0.41914191419141916,"= e−λεEp(1)
θ "
B,0.4207920792079208,"Eq(1)
φ "
B,0.42244224422442245,". . . Ep(T )
θ "
B,0.4240924092409241,"Eq(T )
φ   "
B,0.42574257425742573,"
˜qπ

∆(1:T )
1:K
"
B,0.4273927392739274,"˜pπ

∆(1:T )
1:K
   λ  "
B,0.429042904290429,. . .   
B,0.4306930693069307,+ 12
B,0.43234323432343236,"2b ρ(1:T ).
(21)"
B,0.43399339933993397,"Analogously, let us apply the chain rule to the inner expression:
"
B,0.43564356435643564,"
˜qπ

∆(1:T )
k∗
"
B,0.4372937293729373,"˜pπ

∆(1:T )
k∗
   λ = "
B,0.4389438943894389,"
˜qπ

∆(1)
k∗
"
B,0.4405940594059406,"˜pπ

∆(1)
k∗
   λ"
B,0.44224422442244227,"|
{z
}
ℓ
˜q(1)
π"
B,0.4438943894389439,· . . . · 
B,0.44554455445544555,"
˜qπ

∆(T )
k∗
 ∆(1:T −1)
k∗
"
B,0.4471947194719472,"˜pπ

∆(T )
k∗
 ∆(1:T −1)
k∗
   λ"
B,0.44884488448844884,"|
{z
}
ℓ
˜q(T )
π (22)"
B,0.4504950495049505,Continuing on (18):
B,0.4521452145214521,"= e−λεEp(1)
θ"
B,0.4537953795379538,"h
Eq(1)
φ"
B,0.45544554455445546,"h
. . . Ep(T )
θ"
B,0.4570957095709571,"h
Eq(T )
φ"
B,0.45874587458745875,"h
ℓ˜q(1)
π · . . . · ℓ˜q(T )
π"
B,0.4603960396039604,"ii
. . .
ii
+ 12"
B,0.46204620462046203,"2b ρ(1:T )
(23)"
B,0.4636963696369637,"= e−λεEp(1)
θ "
B,0.46534653465346537,"
Eq(1)
φ "
B,0.466996699669967,"
ℓ˜q(1)
π
. . . Ep(T )
θ"
B,0.46864686468646866,"h
Eq(T )
φ"
B,0.47029702970297027,"h
ℓ˜q(T )
π ii"
B,0.47194719471947194,"|
{z
}"
B,0.4735973597359736,"L(T )
˜qπ ≤κλ . . .   "
B,0.4752475247524752,"
+ 12"
B,0.4768976897689769,"2b ρ(1:T ).
(24)"
B,0.47854785478547857,"If we bound the quantity L(T )
˜qπ by some constant κλ, independent of all the previous samples ∆(1:T −1)
k∗
,
we can bring it in front of the rest of the expectation. Note that this quantity is not exactly the privacy
loss of the mechanism in round T, and the slight abuse of notation is for simplicity purposes."
B,0.4801980198019802,"By again performing this operation recursively,"
B,0.48184818481848185,"≤e−λεκT
λ + 12"
B,0.4834983498349835,"2b ρ(1:T ).
(25)"
B,0.48514851485148514,"Let us therefore consider any of such terms in isolation. To proceed, observe that we can switch from
importance sampling to the original continuous distributions inside the expectation in the following
way:"
B,0.4867986798679868,"˜qπ(∆(t)
k∗)"
B,0.4884488448844885,"˜pπ(∆(t)
k∗)
= qφ(∆(t)
k∗)/pθ(∆(t)
k∗)"
B,0.4900990099009901,"pθ(∆(t)
k∗)/pθ(∆(t)
k∗) P"
B,0.49174917491749176,"k pθ(∆(t)
k )/pθ(∆(t)
k )
P"
B,0.4933993399339934,"k qφ(∆(t)
k )/pθ(∆(t)
k )"
B,0.49504950495049505,"= qφ(∆(t)
k∗)"
B,0.4966996699669967,"pθ(∆(t)
k∗)"
P,0.49834983498349833,"1
P"
P,0.5,"k qφ(∆(t)
k )/pθ(∆(t)
k ) X k"
P,0.5016501650165016,"pθ(∆(t)
k )"
P,0.5033003300330033,"qφ(∆(t)
k )"
P,0.504950495049505,"qφ(∆(t)
k )"
P,0.5066006600660066,"pθ(∆(t)
k )"
P,0.5082508250825083,"= qφ(∆(t)
k∗)"
P,0.5099009900990099,"pθ(∆(t)
k∗)
E∆(t)
k′ ∼˜qπ"
P,0.5115511551155115,"""
pθ(∆(t)
k′ )"
P,0.5132013201320133,"qφ(∆(t)
k′ ) # (26)"
P,0.5148514851485149,Under review as a conference paper at ICLR 2022
P,0.5165016501650165,"Hence, keeping in mind that expectations and distributions are conditioned on the previous t −1
rounds,"
P,0.5181518151815182,"L(t)
˜qπ = Ep(t)
θ "
P,0.5198019801980198,"Eq(t)
φ "
P,0.5214521452145214,"e
λ log
qφ(∆(t))"
P,0.523102310231023,"pθ(∆(t))
E
˜q(t)
π"
P,0.5247524752475248,"""
pθ(∆(t)
k′ )"
P,0.5264026402640264,"qφ(∆(t)
k′ ) #  "
P,0.528052805280528,"
(27)"
P,0.5297029702970297,"= Eq(t)
φ "
P,0.5313531353135313,"e
λ log
qφ(∆(t))"
P,0.533003300330033,pθ(∆(t)) 
P,0.5346534653465347,"Ep(t)
θ "
P,0.5363036303630363,"e
λ log E
˜q(t)
π"
P,0.5379537953795379,"""
pθ(∆(t)
k′ )"
P,0.5396039603960396,"qφ(∆(t)
k′ ) #"
P,0.5412541254125413,"
(28)"
P,0.5429042904290429,"= Eq(t)
φ  "
P,0.5445544554455446,"qφ
 
∆(t)"
P,0.5462046204620462,"pθ
 
∆(t) !λ"
P,0.5478547854785478,"Ep(t)
θ "
P,0.5495049504950495,"E˜q(t)
π "
P,0.5511551155115512,"
pθ

∆(t)
k′
"
P,0.5528052805280528,"qφ

∆(t)
k′
   λ"
P,0.5544554455445545,".
(29)"
P,0.5561056105610561,"The ﬁrst expectation is equivalent to the one in DP-SGD and is basically a moment-generating
function of the privacy loss random variable between two sequences of Gaussian distributions (or
mixtures when subsampling is used) over T rounds."
P,0.5577557755775577,"Let us consider the second expectation, which requires further manipulation to avoid using the privacy
sensitive importance weights. We cannot employ the bound by Agapiou et al. (2017) because the"
P,0.5594059405940595,Under review as a conference paper at ICLR 2022
P,0.5610561056105611,"function inside not bounded. However, we can utilize the special form of this function:"
P,0.5627062706270627,"Ep(t)
θ "
P,0.5643564356435643,"E˜q(t)
π "
P,0.566006600660066,"
pθ

∆(t)
k′
"
P,0.5676567656765676,"qφ

∆(t)
k′
   λ"
P,0.5693069306930693,"
(30)"
P,0.570957095709571,"=
E
∆(t)
1:K∼p(t)
θ   P"
P,0.5726072607260726,"l qφ(∆(t)
l )/pθ(∆(t)
l )
K
K
P"
P,0.5742574257425742,"l qφ(∆(t)
l )/pθ(∆(t)
l )
E˜q(t)
π "
P,0.5759075907590759,"
pθ

∆(t)
k′
"
P,0.5775577557755776,"qφ

∆(t)
k′
   λ"
P,0.5792079207920792,"
(31)"
P,0.5808580858085809,"=
E
∆(t)
1:K∼p(t)
θ   P"
P,0.5825082508250825,"l qφ(∆(t)
l )/pθ(∆(t)
l )
K X k"
P,0.5841584158415841,"qφ(∆(t)
k )"
P,0.5858085808580858,"pθ(∆(t)
k )"
P,0.5874587458745875,"pθ(∆(t)
k )"
P,0.5891089108910891,"qφ(∆(t)
k )
P"
P,0.5907590759075908,"l qφ(∆(t)
l )/pθ(∆(t)
l )
E˜q(t)
π "
P,0.5924092409240924,"
pθ

∆(t)
k′
"
P,0.594059405940594,"qφ

∆(t)
k′
   λ"
P,0.5957095709570958,(32)
P,0.5973597359735974,"=
E
∆(t)
1:K∼p(t)
θ   P"
P,0.599009900990099,"l qφ(∆(t)
l )/pθ(∆(t)
l )
K
E˜q(t)
π "
P,0.6006600660066007,"
pθ

∆(t)
k′
"
P,0.6023102310231023,"qφ

∆(t)
k′
   λ+1"
P,0.6039603960396039,"
(33)"
P,0.6056105610561056,"≤
E
∆(t)
1:K∼p(t)
θ  "
P,0.6072607260726073,"P
l qφ(∆(t)
l )/pθ(∆(t)
l )
K
E˜q(t)
π   "
P,0.6089108910891089,"
pθ

∆(t)
k′
"
P,0.6105610561056105,"qφ

∆(t)
k′
   λ+1  "
P,0.6122112211221122,"
(34)"
P,0.6138613861386139,"=
E
∆(t)
1:K∼p(t)
θ  
P"
P,0.6155115511551155,"l qφ(∆(t)
l )/pθ(∆(t)
l )
K X k"
P,0.6171617161716172,"qφ(∆(t)
k )/pθ(∆(t)
k )
P"
P,0.6188118811881188,"l qφ(∆(t)
l )/pθ(∆(t)
l )"
P,0.6204620462046204,"pθ(∆(t)
k )"
P,0.6221122112211221,"qφ(∆(t)
k ) !λ+1"
P,0.6237623762376238,"
(35)"
P,0.6254125412541254,"=
E
∆(t)
1:K∼p(t)
θ  1 K X k"
P,0.6270627062706271,"qφ(∆(t)
k )"
P,0.6287128712871287,"pθ(∆(t)
k )"
P,0.6303630363036303,"pθ(∆(t)
k )"
P,0.6320132013201321,"qφ(∆(t)
k ) !λ+1"
P,0.6336633663366337,"
(36) = 1 K X"
P,0.6353135313531353,"k
E
∆(t)
1:K∼p(t)
θ "
P,0.636963696369637,"qφ(∆(t)
k )"
P,0.6386138613861386,"pθ(∆(t)
k )"
P,0.6402640264026402,"pθ(∆(t)
k )"
P,0.641914191419142,"qφ(∆(t)
k ) !λ+1"
P,0.6435643564356436,"
(37) = 1 K X"
P,0.6452145214521452,"k
E
∆(t)
k ∼qφ  "
P,0.6468646864686468,"pθ(∆(t)
k )"
P,0.6485148514851485,"qφ(∆(t)
k ) !λ+1"
P,0.6501650165016502,"
(38)"
P,0.6518151815181518,"=
E
∆(t)∼qφ"
P,0.6534653465346535,"""pθ(∆(t))"
P,0.6551155115511551,qφ(∆(t)) λ+1# (39)
P,0.6567656765676567,"= eλDλ+1(pθ||qφ)
(40)"
P,0.6584158415841584,"Putting everything together, we have"
P,0.6600660066006601,"L(t)
˜qπ ≤e(λ−1)Dλ(qφ||pθ)+λDλ+1(pθ||qφ) ≤κλ,
(41)"
P,0.6617161716171617,"where κλ does not depend on any of the previous rounds, since we can bound Rényi divergences
(e.g., by clipping the model updates), and is deﬁned as"
P,0.6633663366336634,"κλ ≡max
φ,θ e(λ−1)Dλ(qφ||pθ)+λDλ+1(pθ||qφ).
(42)"
P,0.665016501650165,"It then satisﬁes the conditions to obtain Eq. 25, and consequently, proves the theorem with"
P,0.6666666666666666,"c(t)
λ = max"
P,0.6683168316831684,"(
maxθ,φ Dλ(q(t)
φ ||p(t)
θ )
maxθ,φ Dλ(p(t)
θ ||q(t)
φ )
(43)"
P,0.66996699669967,Under review as a conference paper at ICLR 2022
P,0.6716171617161716,"A.2
COMPRESSION BY PARTS"
P,0.6732673267326733,"Let us consider the setting where parts of the model (such as tensors, or even individual parameters)
are compressed independently. Assume all model parameters ∆are split into M non-intersecting
groups ∆[1], . . . , ∆[M] and are encoded using b1, . . . , bM bits correspondingly. We use square
brackets to distinguish these indices from the round indices. The following holds."
P,0.6749174917491749,"Theorem 3. Expectation of an arbitrary function ζ(∆[1], . . . , ∆[M]) over the importance sampling
distributions q[1]
π , . . . , q[M]
π
, built according to the outlined procedure for each parameter group
independently, is equivalent to the expectation over the joint importance sampling distribution qπ
built on 2
PM
i=1 bi samples, i.e.,"
P,0.6765676567656765,"E∆[1]∼q[1]
π"
P,0.6782178217821783,"h
. . . E∆[M]∼q[M]
π"
P,0.6798679867986799,"h
ζ(∆[1], . . . , ∆[M])
i
. . .
i
= E∆[1:M]∼qπ
h
ζ(∆[1:M])
i
."
P,0.6815181518151815,Proof. To show that the above is true it is sufﬁcient to write down these expectations:
P,0.6831683168316832,"E∆[1]∼q[1]
π"
P,0.6848184818481848,"h
. . . E∆[M]∼q[M]
π"
P,0.6864686468646864,"h
ζ(∆[1], . . . , ∆[M])
i
. . .
i
(44) = 2b1
X k1=1"
P,0.6881188118811881,"qφ(∆[1]
k1)/pθ(∆[1]
k1)
P"
P,0.6897689768976898,"l1 qφ(∆[1]
l1 )/pθ(∆[1]
l1 ) 2b2
X"
P,0.6914191419141914,"k2=1
. . ."
"BM
X",0.693069306930693,"2bM
X kM=1"
"BM
X",0.6947194719471947,"qφ(∆[M]
kM )/pθ(∆[M]
kM )
P"
"BM
X",0.6963696369636964,"lM qφ(∆[M]
lM )/pθ(∆[M]
lM )
ζ(∆[1]
k1, . . . , ∆[M]
kM ) (45) = 2b1
X k1=1 2b2
X"
"BM
X",0.698019801980198,"k2=1
. . ."
"BM
X",0.6996699669966997,"2bM
X kM=1"
"BM
X",0.7013201320132013,"qφ(∆[1]
k1)/pθ(∆[1]
k1)
P"
"BM
X",0.7029702970297029,"l1 qφ(∆[1]
l1 )/pθ(∆[1]
l1 )
. . .
qφ(∆[M]
kM )/pθ(∆[M]
kM )
P"
"BM
X",0.7046204620462047,"lM qφ(∆[M]
lM )/pθ(∆[M]
lM )
ζ(∆[1]
k1, . . . , ∆[M]
kM ) (46) = 2b1
X k1=1 2b2
X"
"BM
X",0.7062706270627063,"k2=1
. . ."
"BM
X",0.7079207920792079,"2bM
X kM=1"
"BM
X",0.7095709570957096,"qφ(∆[1]
k1)...qφ(∆[M]
kM )"
"BM
X",0.7112211221122112,"pθ(∆[1]
k1)...pθ(∆[M]
kM )
P"
"BM
X",0.7128712871287128,l1 . . . P lM
"BM
X",0.7145214521452146,"qφ(∆[1]
l1 )...qφ(∆[M]
lM )"
"BM
X",0.7161716171617162,"pθ(∆[1]
l1 )...pθ(∆[M]
lM )"
"BM
X",0.7178217821782178,"ζ(∆[1]
k1, . . . , ∆[M]
kM )
(47) ="
"BM
X",0.7194719471947195,"2b1+...+bM
X k∗=1"
"BM
X",0.7211221122112211,"qφ(∆[1:M]
k∗
)"
"BM
X",0.7227722772277227,"pθ(∆[1:M]
k∗
)
P2b1+...+bM
l∗=1
qφ(∆[1:M]
l∗
)"
"BM
X",0.7244224422442245,"pθ(∆[1:M]
l∗
)"
"BM
X",0.7260726072607261,"ζ(∆[1:M]
k∗
)
(48)"
"BM
X",0.7277227722772277,"= E∆[1:M]∼qπ
h
ζ(∆[1:M])
i
,
(49)"
"BM
X",0.7293729372937293,"where k∗= (k1, k2, . . . , kM) and l∗= (l1, l2, . . . , lM) are multi-indexes. The line (48) is because
distributions q[1]
φ , . . . , q[M]
φ
and p[1]
θ , . . . , p[M]
θ
for different parts of the model are parameterized by the
model updates / learnt parameters, and hence, are independent given these updates / parameters."
"BM
X",0.731023102310231,"B
ADDITIONAL DISCUSSION AND ALGORITHMS"
"BM
X",0.7326732673267327,"B.1
ADDITIONAL DISCUSSION"
"BM
X",0.7343234323432343,"Computational overhead DP-REC does introduce additional computational requirements compared
to standard FedAvg and compared to DP-FedAvg. Every line in Algorithm 3 involves some
additional computation due to the REC compression. More speciﬁcally, REC involves locally sampling
K i.i.d. normal samples of dimensionality of the update. After clipping updates, REC computes the
importance samples αk, essentially calculating log-probabilities of the samples under two Gaussian
distributions. Finally, sampling from the categorical distribution ˜qπ, which is relatively cheap since we
perform per-tensor compression. It is difﬁcult to benchmark the real-world impact of this additional
computational overhead given heterogeneous hardware. A practical implementation would sample the
standard-normal samples during training (or even during idle-time), as well as the Gumbel-samples
for the categorical distribution. The log-probabilities of the prior can equally be pre-computed.
The index-selection procedure can be parallelized if the hardware supports it. In our simulated"
"BM
X",0.735973597359736,Under review as a conference paper at ICLR 2022
"BM
X",0.7376237623762376,"environment on a RTX2080 GPU, which runs everything sequentially (training, then per-tensor
Gaussian sampling, per-tensor α computation and index-selection), for the Cifar10 experiment with
K = 27, a single-client’s epoch has a roughly 70%:30% ratio of training-to-compression, with some
variance due to the different local data-set sizes."
"BM
X",0.7392739273927392,"Increasing accuracy at the cost of communication There is a noticeable accuracy gap between
DP-REC and DP-FedAvg. A reasonable question to ask is whether it is possible to trade higher
communication spending for better accuracy? Unfortunately, it is not as simple. The reason for
observing the gap lies not in compression but rather in privacy accounting. Figure 2 shows empirically
that increasing bit-width has marginal returns, up until doing no compression at all. Any conﬁguration
in terms of higher bit-widths or more ﬁne-grained vector quantization can be expected to perform
between 7-bit per-tensor quantization and no-compression. Compression-only experiments in Ap-
pendix D.4 also corroborate this point, as the non-private compressed model achieves performance
much closer to FedAvg. Expressing the divergence bound of discrete distributions through continu-
ous distributions leads to nearly double the amount of noise necessary for an equivalent guarantee
compared to the normal continuous Gaussian mechanism. Thus, we would need to relax privacy to
close the accuracy gap. This bound with overhead appears to be tight too, judging by some of our
synthetic experiments measuring how close it comes to the divergence computed from actual discrete
distributions. However, there is still a possible way to reduce the accuracy difference by employing
stronger privacy ampliﬁcation (e.g. adding a secure shufﬂer), which will counter the overhead of the
bound. This effect is seen in StackOverﬂow dataset, where subsampling ampliﬁcation is stronger due
to a large number of users and the accuracy gap between DP-FedAvg and DP-REC is drastically
smaller."
"BM
X",0.740924092409241,Under review as a conference paper at ICLR 2022
"BM
X",0.7425742574257426,"B.2
ADDITIONAL ALGORITHMS"
"BM
X",0.7442244224422442,"B.2.1
SERVER-TO-CLIENT MESSAGE COMPRESSION"
"BM
X",0.7458745874587459,"Algorithm
5
and
Algorithm
6
formalize
the
process
described
in
Section
2.5."
"BM
X",0.7475247524752475,"Algorithm 5 The server side algorithm for the com-
pression of server-to-client communication. R(t)
s
is
the client-chosen shared random seed for round t. dec.
describes Alg. 4. O(t) describes the sever-side opti-
mizer including its state (e.g. momenta)"
"BM
X",0.7491749174917491,"Hs = {(R′, O(0))} ∀s ∈S ▷History with model
initial seed R′
for t ∈{0 . . . T} do"
"BM
X",0.7508250825082509,"Sample set S′ of participating clients
M ←{}
▷Round memory
for s ∈S′ in parallel do"
"BM
X",0.7524752475247525,"msgs ←create_message_for_client(s, Hs)
Send msgs to client s
Receive k∗
s, R(t)
s
▷client-chosen index, seed
M ←M ∪{(k∗
s, R(t)
s )}
end for
for s ∈S do"
"BM
X",0.7541254125412541,"Hs ←update_history(s, S′, Hs, M))
end for
∆(t) ←
1
S′
P"
"BM
X",0.7557755775577558,"k∗∈M dec.(R(t), k∗)
w(t+1) ←w(t) −O(t)(∆(t))
end for"
"BM
X",0.7574257425742574,"procedure CREATE MESSAGE FOR CLIENT(s, Hs)"
"BM
X",0.759075907590759,if size(Hs) > size(w(t)) + size(O(t)) then
"BM
X",0.7607260726072608,"msgs ←(w(t), O(t))
else"
"BM
X",0.7623762376237624,"msgs ←Hs
end if
Hs ←{}
▷Reset client history
return msgs
end procedure"
"BM
X",0.764026402640264,"procedure UPDATE HISTORY(s, S′, Hs, M)"
"BM
X",0.7656765676567657,if s ∈S′ then
"BM
X",0.7673267326732673,"Hs ←Hs ∪{M\{(k∗
s, R(t)
s )})}
else"
"BM
X",0.768976897689769,"Hs ←Hs ∪{M)}
end if
return Hs
end procedure"
"BM
X",0.7706270627062707,"Algorithm 6 The client side algorithm for
the decompression of server-to-client com-
munication. dec. describes Alg. 4"
"BM
X",0.7722772277227723,"if msgs = (w(t), O) then"
"BM
X",0.7739273927392739,"w(t) ←w(t)
O(t)
s
←O(t)
else"
"BM
X",0.7755775577557755,"H ←msgs
w(t) ←w(prev)
s
▷Last-known server
model"
"BM
X",0.7772277227722773,for M ∈H do
"BM
X",0.7788778877887789,"if M = (R′, O0) then"
"BM
X",0.7805280528052805,"w(t) ←initialize(R′)
Os ←O(0)
else"
"BM
X",0.7821782178217822,"∆←
1
|M|
P
(k,R)∈Mdec.(R, k)"
"BM
X",0.7838283828382838,"w(t) ←w(t) −Os(∆)
end if
end for
end if
return w(t)"
"BM
X",0.7854785478547854,"B.2.2
ALGORITHM FOR ACCOUNTING PRIVACY IN DP-REC"
"BM
X",0.7871287128712872,"Algorithm 7 describes how we compute ε for a target δ in DP-REC in general. More speciﬁcally,
in our experiments, p(t)
θ
= N(0, σ2I) and q(t)
φ
= N(φq, σ2I). For such distributions, the Rényi
divergence between them evaluates to"
"BM
X",0.7887788778877888,"Dλ

q(t)
φ ||p(t)
θ

=
λ
2σ2 ∥φq∥2
2.
(50)"
"BM
X",0.7904290429042904,Under review as a conference paper at ICLR 2022
"BM
X",0.7920792079207921,"Thus, for a given λ and σ, bounding this divergence corresponds to clipping the norm of φq, i.e.,"
"BM
X",0.7937293729372937,"clipping ∥φq∥2 to C corresponds to Dλ

q(t)
φ ||p(t)
θ

≤λC2"
"BM
X",0.7953795379537953,"2σ2 , ∀φ, θ. In order to allow for privacy"
"BM
X",0.7970297029702971,"ampliﬁcation with subsampling, we also need to bound the Rényi divergence between the prior p(t)
θ
and the mixture B"
"BM
X",0.7986798679867987,"N qt
φ + N−B"
"BM
X",0.8003300330033003,"N
p(t)
θ where N corresponds to the number of participants in the federation
and B/N to the subsampling rate (Abadi et al., 2016; Mironov et al., 2019). In the general case, we
have to consider the maximum over the two possible directions: Dλ"
"BM
X",0.801980198019802, N −B
"BM
X",0.8036303630363036,"N
p(t)
θ + B"
"BM
X",0.8052805280528053,"N q(t)
φ"
"BM
X",0.806930693069307,"p(t)
θ"
"BM
X",0.8085808580858086,"
,
Dλ"
"BM
X",0.8102310231023102,"
p(t)
θ
 N −B"
"BM
X",0.8118811881188119,"N
p(t)
θ + B"
"BM
X",0.8135313531353136,"N q(t)
φ"
"BM
X",0.8151815181518152,"
.
(51)"
"BM
X",0.8168316831683168,"However, the calculation can be simpliﬁed, at least for certain choices of the distributions p(t)
θ , q(t)
φ ,
as (Mironov et al., 2019) show that Dλ"
"BM
X",0.8184818481848185, N −B
"BM
X",0.8201320132013201,"N
p(t)
θ + B"
"BM
X",0.8217821782178217,"N q(t)
φ"
"BM
X",0.8234323432343235,"p(t)
θ 
≥Dλ"
"BM
X",0.8250825082508251,"
p(t)
θ
 N −B"
"BM
X",0.8267326732673267,"N
p(t)
θ + B"
"BM
X",0.8283828382838284,"N q(t)
φ"
"BM
X",0.83003300330033,"
,
(52)"
"BM
X",0.8316831683168316,"allowing us to focus on the ﬁrst term. Furthermore, again following (Abadi et al., 2016; Mironov
et al., 2019), this divergence can be simpliﬁed for a general mixture with weights α and (1 −α), our
speciﬁc choice of q(t)
φ and p(t)
θ , and for integer λ:"
"BM
X",0.8333333333333334,"Dλ

(1 −α)p(t)
θ + αq(t)
φ
 p(t)
θ

=
1
λ −1 log "
"BM
X",0.834983498349835,"Ep(t)
θ  "
"BM
X",0.8366336633663366,"(1 −α)p(t)
θ + αq(t)
φ
p(t)
θ !λ  "
"BM
X",0.8382838283828383,"
(53)"
"BM
X",0.8399339933993399,"=
1
λ −1 log "
"BM
X",0.8415841584158416,"Ek∼B(λ,α) "
"BM
X",0.8432343234323433,"Ep(t)
θ  "
"BM
X",0.8448844884488449,"q(t)
φ
p(t)
θ !k    "
"BM
X",0.8465346534653465,"
(54)"
"BM
X",0.8481848184818482,"≤
1
λ −1 log

Ek∼B(λ,B/N) 
e k2−k"
"BM
X",0.8498349834983498,"2σ2 C2
.
(55)"
"BM
X",0.8514851485148515,This allows us to readily calculate all of the terms in Algorithm 7.
"BM
X",0.8531353135313532,"Algorithm 7 Privacy accounting for DP-REC with subsampling for privacy ampliﬁcation. Λ are the
Rényi orders λ > 1 that will be considered and b are the total number of bits used to represent the
neural network update. Furthermore, T are the total communication rounds for training, B is the
number of users considered on each round and N is the total number of users in the federation. δ is
the target probability of DP failing to provide a guarantee."
"BM
X",0.8547854785478548,"ρ(0) ←0
ˆk(0)
λ
←0, ∀λ ∈Λ
ˆm(0)
λ
←0, ∀λ ∈Λ
for t ←1, . . . , TB do"
"BM
X",0.8564356435643564,"ρ(t) ←ρ(t−1) + maxθ,φ e
D2

q(t)
φ ||p(t)
θ
"
"BM
X",0.858085808580858,for λ ∈Λ do
"BM
X",0.8597359735973598,"ˆk(t)
λ
←ˆk(t−1)
λ
+ max 
 "
"BM
X",0.8613861386138614,"maxθ,φ Dλ

N−1"
"BM
X",0.863036303630363,"N p(t)
θ + 1"
"BM
X",0.8646864686468647,"N q(t)
φ
 p(t)
θ
"
"BM
X",0.8663366336633663,"maxθ,φ Dλ

p(t)
θ
 N−1"
"BM
X",0.8679867986798679,"N p(t)
θ + 1"
"BM
X",0.8696369636963697,"N q(t)
φ
"
"BM
X",0.8712871287128713,"ˆm(t)
λ ←ˆm(t−1)
λ
+ max 
 "
"BM
X",0.8729372937293729,"maxθ,φ Dλ+1

N−1"
"BM
X",0.8745874587458746,"N p(t)
θ + 1"
"BM
X",0.8762376237623762,"N q(t)
φ
 p(t)
θ
"
"BM
X",0.8778877887788779,"maxθ,φ Dλ+1

p(t)
θ
 N−1"
"BM
X",0.8795379537953796,"N p(t)
θ + 1"
"BM
X",0.8811881188118812,"N q(t)
φ
"
"BM
X",0.8828382838283828,"end for
end for"
"BM
X",0.8844884488448845,"ε ←min
λ λ −1"
"BM
X",0.8861386138613861,"λ
ˆk(T B)
λ
+ ˆm(T B)
λ
−1"
"BM
X",0.8877887788778878,"λ log

δ −12"
"BM
X",0.8894389438943895,"2b ρ(T B)
"
"BM
X",0.8910891089108911,Under review as a conference paper at ICLR 2022
"BM
X",0.8927392739273927,"B.2.3
OVERALL ALGORITHM FOR FEDERATED TRAINING WITH DP-REC"
"BM
X",0.8943894389438944,"Algorithm 8 Overall federated training pipeline when using DP-REC. S corresponds to all clients in
the federation and s to a speciﬁc client. C corresponds to the desired sensitivity."
"BM
X",0.8960396039603961,"procedure SERVER TRAINING(T, σ)"
"BM
X",0.8976897689768977,"Hs = {(R′, O(0))} ∀s ∈S
▷History with model initial seed R′
for t ∈{0 . . . T} do"
"BM
X",0.8993399339933993,"S′ ←Sample set of participating clients with replacement
M ←{}
▷Round memory
for s ∈S′ in parallel do"
"BM
X",0.900990099009901,"msgs ←create_message_for_client(s, Hs)
k∗
s, R(t)
s
←client_training(msgs)
M ←M ∪{(k∗
s, R(t)
s )}
end for
for s ∈S do"
"BM
X",0.9026402640264026,"Hs ←update_history(s, S′, Hs, M))
end for
∆(t) ←
1
S′
P"
"BM
X",0.9042904290429042,"k∗∈M dec.(R(t), k∗)
w(t+1) ←w(t) −O(t)(∆(t))
end for
ε ←compute ε guarantee for a given δ.
▷Using Alg. 7
end procedure"
"BM
X",0.905940594059406,procedure CLIENT TRAINING(msgs)
"BM
X",0.9075907590759076,"w(t) ←receive_message(msgs)
▷Refers to Alg. 6
w(t)
s
←w(t)
for local epoch e ∈{1, . . . , E} do"
"BM
X",0.9092409240924092,for batch b ∈B do
"BM
X",0.9108910891089109,"w(t)
s
←w(t)
s
−∇ℓ(w(t)
s , b)
▷ℓcorresponds to the local loss
end for
end for
φ(t)
s
←w(t)
s
−w(t)
ˆφ(t)
s
= φ(t)
s min(1, C/∥φ(t)
s ∥2)
R(t)
s
←Choose a random seed
k∗
s ←enc.( ˆφ(t)
s , R(t)
s )
▷Using Alg. 1
return k∗
s, R(t)
s
end procedure"
"BM
X",0.9125412541254125,"C
EXPERIMENTAL DETAILS"
"BM
X",0.9141914191419142,"The experiments in the main text were performed on two Nvidia RTX 2080Ti GPUs, as well on
several Nvidia Tesla V100 GPU’s available on an internal cluster over the span of two weeks."
"BM
X",0.9158415841584159,"C.1
DATASETS & MODELS"
"BM
X",0.9174917491749175,"MNIST
For MNIST, we consider a LeNet-5 (LeCun et al., 1998) model trained on a federated
version of the original 50k training images. We split the data across 100 clients in a non-i.i.d. way
where the label proportions on each client are determined by sampling a Dirichlet distribution with
concentration α = 1.0 (Hsu et al., 2019). For evaluation, we assume the standard validation split of
MNIST to be available at the server. We run all experiments four times with different seeds, except
the one with DP-REC and ε = 3 which we run with eleven seeds, as it had an “outlier” run which
skewed the average and increased considerably the standard error."
"BM
X",0.9191419141914191,"FEMNIST
FEMNIST is a federated version of the extended MNIST (EMNIST) dataset (Cohen
et al., 2017). It consists of MNIST-like images of handwritten letters and digits belonging to one"
"BM
X",0.9207920792079208,Under review as a conference paper at ICLR 2022
"BM
X",0.9224422442244224,Table 2: Federated training sets
"BM
X",0.9240924092409241,"Dataset
Number of devices
Total samples
Samples per device
mean
std"
"BM
X",0.9257425742574258,"MNIST
100
50, 000
500.00
73.10
FEMNIST
3500
705, 595
201.60
78.92
Shakespeare
660
3, 678, 451
5573.41
6460.77
StackOverﬂow
342, 477
135, 818, 730
396.58
1278.94"
"BM
X",0.9273927392739274,"of 62 classes. The federated nature of the dataset is naturally determined by the writer for a given
datapoint. Additionally, the size of the individual clients’ datasets differ signiﬁcantly. In the literature,
there are two versions of this dataset used for experimentation. Originally published by (Caldas et al.,
2018), their published code1 provides a recipe to pre-process the dataset into the federated version.
Unfortunately, however, the statistics reported in the paper do not align with the result of this recipe.
We repeat the statistics as we use them for our experiments in Table 2. As mentioned in the main
text, some works such as DDGauss (Kairouz et al., 2021) use the FEMNIST version provided by
tensorﬂow federated2. It consists of a smaller subset of 3400 clients. For the model architecture,
we consider the convolutional network described in (Kairouz et al., 2021), albeit without dropout
regularization. We run four seeds for DP-FedAvg and DP-REC with ε = 1, 3, 6."
"BM
X",0.929042904290429,"Shakespeare
For the Shakespeare dataset we closely follow (Caldas et al., 2018). Each client
corresponds to a unique character across the collection of Shakespeare’s plays with a minimum
number of spoken lines. The non-i.i.d. characteristics of this dataset are due to the different
”speaking"" styles of the resulting 660 roles. We use the same 2-layer LSTM model as in (Caldas et al.,
2018) for this next-character-prediction task (considering a library of 80 characters). Each client
predicts the next character following the LSTM encoding of the previous 80 characters. For Table 2
we consider each pair of 80 character plus next character as a single sample. The statistics we report
differ markedly from the statistics in (Caldas et al., 2018), as reported on a corresponding issue raised
in their code base3. We run four seeds for both DP-FedAvg and DP-REC."
"BM
X",0.9306930693069307,"StackOverﬂow
The StackOverﬂow dataset (TFF Authors, 2019) consists of a collection of ques-
tions and answers posted on the StackOverﬂow website during a certain time window. Each user of
that website who posted there in that time-frame is considered a client with their aggregated posts
as the client’s dataset. Here, we consider the task of tag prediction described in (Reddi et al., 2020).
Associated with each posting (irrespective of whether it is a question or an answer) is associated at
least one of 500 tags. Each client is therefore performing one-vs-all classiﬁcation corresponding to
500 binary classiﬁcations. We pre-process each post by creating a bag-of-words representation of the
10, 000 most frequent words, normalized to 1. As model, we consider logistic regression. Learning
curves in the main paper were created by selecting the ﬁrst 10, 000 data-points when iterating over
a shufﬂed list of hold out clients. As noted in the main text, each run selected a different seed for
shufﬂing, resulting in non directly comparable learning curves. For the results in Table 1, the ﬁnal
model was evaluated on all 16, 586, 035 datapoints across all hold out client. Note that with 1, 500
communication rounds and 60 clients per round, only ∼26% of all clients participate in training. We
run two seeds for both DP-FedAvg and DP-REC."
"BM
X",0.9323432343234324,"C.2
HYPERPARAMETERS"
"BM
X",0.933993399339934,"For all of the experiments in the main text we used 7-bit per tensor quantization for DP-REC. This
was determined after the ablation study on the FEMNIST dataset, shown at Figure 2. The only
difference was at the StackOverﬂow experiment where, to have a reasonable ε DP guarantee, we used
5 quantization groups for the weight matrix of the logistic regression model (each with 106 entries)
and did per-tensor quantization for the biases (i.e., we had 6 quantization groups in total). As we"
"BM
X",0.9356435643564357,"1https://github.com/TalwalkarLab/leaf
2https://www.tensorflow.org/federated/api_docs/python/tff/simulation/
datasets/emnist
3https://github.com/TalwalkarLab/leaf/issues/13"
"BM
X",0.9372937293729373,Under review as a conference paper at ICLR 2022
"BM
X",0.9389438943894389,"mentioned in the main text, for DP-REC we performed sampling with replacement to select clients for
each round on all tasks, since the improved privacy ampliﬁcation was beneﬁcial. For DP-FedAvg
we use the traditional sampling without replacement on each round but with replacement across
rounds."
"BM
X",0.9405940594059405,"Tuning privacy hyperparameters Privacy guarantees depend essentially on the ratio of the clipping
threshold and the prior noise scale σ. For all experiments, we ﬁxed the ratio, determined by our
accounting upfront, in order to ensure a chosen ε at the end of the experiment. More speciﬁcally,
for DP-FedAvg we tune the clipping threshold for each task and pick the appropriate noise scale
for a given ε guarantee. For DP-REC the clipping threshold was a multiplicative factor of the
standard deviation of the prior over the deltas, i.e., cσ, and c was tuned in order to yield a speciﬁc
ε guarantee. The free parameter that we thus optimize is σ. For σ, we considered values ranging
from 10−5 up to 1, ﬁnding the right order of magnitude and then ﬁne-tuning within that order if
necessary (e.g. considering 0.001, 0.003, 0.005, etc.), based on validation performance. Of course,
in a practical deployment such tuning would need to be taken into account when computing ﬁnal
privacy parameters, as discussed by Abadi et al. (2016, Appendix D)."
"BM
X",0.9422442244224423,"MNIST For this task we used SGD with a learning rate of 0.01 for the client optimizer and
Adam (Kingma & Ba, 2014) with a learning rate of 2e −3 for the server optimizer for all of
the experiments. The β1, β2 parameters of Adam were kept at the default values (0.9 and 0.999) and
we trained for 1k global communication rounds rounds. We used 10 clients on each round, where
each client performed 1 local epoch with a batch size of 20. For DP-FedAvg the clipping threshold
was 0.01 whereas for DP-REC the prior standard deviation was ﬁxed to σ = 0.005. In order to get
ε = 3, 6 for DP-FedAvg we used a noise scale of 3.8 and 2.15, whereas for DP-REC we used a
c = 0.5 and c = 0.7625 respectively."
"BM
X",0.9438943894389439,"FEMNIST For optimization we used SGD with a learning rate of 0.05 locally and SGD with a
learning rate of 1.0 globally, i.e., we averaged the local parameters. For all of the methods we
sampled 100 clients for each round and each client performed 1 local epoch with a batch size of 20.
For DP-FedAvg we trained for 1.5k rounds with a clipping threshold of 0.1 and for DP-REC we
found it beneﬁcial to train for 4k rounds (and thus we had to clip more aggressively on each round)
and the σ was ﬁxed to 0.03. In order to get ε of 1, 3 and 6 we used a noise scale of 5, 1.85 and 1.15
for DP-FedAvg and a c of 0.75, 1.35 and 1.66 for DP-REC."
"BM
X",0.9455445544554455,"Shakespeare Both the global and local optimizers for this task were SGD with a learning rate of 1.0.
We trained for 200 rounds where on each round we sampled 66 clients and each client performed
1 local epoch with a batch size of 10. For DP-FedAvg the clipping threshold was 0.3 whereas for
DP-REC the prior standard deviation was ﬁxed to σ = 0.1. In order to get a ε of 3 we used a noise
scale of 2.15 for DP-FedAvg and a c of 1.36 for DP-REC."
"BM
X",0.9471947194719472,"StackOverﬂow For this task we mainly used the hyperparameters provided at (Andrew et al., 2019).
More speciﬁcally, for the local optimizer we used SGD with a learning rate of 102.5 whereas for the
global optimizer we used SGD with a learning rate of 10−0.25 and a momentum of 0.9. We sampled
60 clients per round and each client performed 1 local epoch with a batch size of 100. We trained the
logistic regression model for 1.5k rounds. For DP-FedAvg the clipping threshold was 0.05 whereas
for DP-REC the prior standard deviation was σ = 0.05. For a ε of 1 we used a noise scale of 0.957
for DP-FedAvg and a c of 1.227 for DP-REC."
"BM
X",0.9488448844884488,"D
ADDITIONAL RESULTS"
"BM
X",0.9504950495049505,"D.1
ACCURACY FOR A FIXED PRIVACY BUDGET"
"BM
X",0.9521452145214522,"Figure 3 shows the accuracy achieved as a function of the privacy budget ε. For a discussion of these
results refer to the main text."
"BM
X",0.9537953795379538,"D.2
PRIVATE MEAN ESTIMATION"
"BM
X",0.9554455445544554,"We ran an experiment to compare DP-REC with the method of Chen et al. (2020) and see the effects of
Kashin’s representation and shared randomness. Removing privacy from the equation and comparing
compression methods head-to-head with 1-bit communication (+ bits for shared randomness, as we"
"BM
X",0.9570957095709571,Under review as a conference paper at ICLR 2022
"BM
X",0.9587458745874587,"(a) MNIST
(b) Femnist"
"BM
X",0.9603960396039604,"(c) Shakespeare
(d) StackOverﬂow"
"BM
X",0.9620462046204621,Figure 3: Test accuracy (%) as a function of the privacy budget ε (for a ﬁxed δ = 1/N 1.1).
"BM
X",0.9636963696369637,"explain below), we ﬁnd that (Chen et al., 2020) performs slightly better than DP-REC in terms of
mean squared error (MSE), when the latter uses poorly informed prior (e.g. a zero-mean Gaussian).
When DP-REC is equipped with a better prior (e.g. Gaussian with the mean 1/
√"
"BM
X",0.9653465346534653,"d in all dimensions),
it is comparable or outperforms (Chen et al., 2020). For example, estimating a 200-dimensional mean
from 10k samples, MSE of (Chen et al., 2020) is 0.07, for DP-REC with a poor prior it is ∼0.3, and
for DP-REC with a better prior it is ∼0.03. There are a few points to elaborate on:"
"BM
X",0.966996699669967,"• Shared randomness. Similarly to DP-REC, (Chen et al., 2020) have a choice between public
randomness (deﬁned by the server) or a shared randomness (deﬁned by clients). As we
explained in our paper, the latter is a preferred choice in terms of privacy, but adds a few
bits to client-server communication (for the random seed). We found that it also improves
performance and that the method of Chen et al. (2020) did not work well in few-bit settings
with public randomness (MSE > 10.0 in the above example)."
"BM
X",0.9686468646864687,"• Prior. DP-REC is a method that’s more dependent on a good prior. With a poor choice,
in one-shot settings like mean estimation, performance can be compromised. However, it
makes it more suitable for FL scenarios where prior is gradually improved with every round."
"BM
X",0.9702970297029703,"• Adding privacy to the mix. It is important to note that DP-REC communication gets
somewhat penalized due to the overhead term in our Theorem 1, which for the above setting
requires at least 21 bits per message to achieve δ < 10−5. Nonetheless, this is not a problem
in FL, since practical communication bit-width would be larger in any case. Moreover,
DP-REC privacy can be further ampliﬁed by implementing the secure shufﬂer, similarly to
Girgis et al. (2020), resulting in even tighter privacy guarantees."
"BM
X",0.971947194719472,"D.3
ADDITIONAL BASELINES"
"BM
X",0.9735973597359736,"A curious reader may wonder how would a simple baseline of compressed gradients combined
with DP-FedAvg fare against our method. Unfortunately, combining DP-FedAvg (or differential"
"BM
X",0.9752475247524752,Under review as a conference paper at ICLR 2022
"BM
X",0.976897689768977,"privacy in general) with compression is not a straightforward task, which is why it motivates our
paper. There are two options: (i) ensure DP, then compress the update; (ii) compress, then ensure DP.
Each option has its challenges."
"BM
X",0.9785478547854786,"First, consider (i). Adding noise at the client and then compressing the update before transmission,
might not allow to calibrate noise to the aggregate and the use tighter composition theorems (e.g.,
Rényi accountant), degrading the privacy guarantee. This is what essentially led to the development
of hybrid methods such as cpSGD and DDGauss. As this direction was researched in the line of
work leading up to DDGauss, we take their method as the best representation of such approach.
Let us now consider (ii). Once updates are compressed by the client, we cannot add noise directly,
because it would negate any compression. Therefore, the client needs to transmit the update ﬁrst,
using secure aggregation to protect against honest-but-curious server. We can compress updates using
scalar quantization, but secure aggregation might add communication overhead countering some of
the effects of compression (see Bonawitz et al. (2017)). Even without the additional communication
overhead of secure aggregation, we empirically observed that such a method can have both worse
accuracy and worse communication efﬁciency than DP-REC and represents a rather weak baseline.
We ran an experiment on our MNIST task and combined 8-bit scalar quantization of the client updates
(in a way that satisﬁes the desired sensitivity) with DP-FedAvg with ε = 3; there we observed
that the global accuracy reached ∼58% after 1k rounds, which is both smaller than the DP-REC
result and signiﬁcantly more expensive in terms of communication. Lastly, if we were to consider
vector quantization to get ahead in terms of compression ratios, it would hinder application of secure
aggregation, leaving us without any protection against honest-but-curious server (since adding noise
is not possible either, as it would nullify compression)."
"BM
X",0.9801980198019802,"Of course, there exists a number of diverse gradient compression methods that could be considered,
and that could be more easily combined with either SecAgg or DP, but we leave exploration of these
methods for future work."
"BM
X",0.9818481848184818,"D.4
COMPRESSION-ONLY PERFORMANCE OF OUR METHOD"
"BM
X",0.9834983498349835,"In order to investigate the behaviour of the compression part of DP-REC, we performed some
experiments on our 4k round FEMNIST task where we omit the clipping part of DP-REC. We
consider three runs:"
"BM
X",0.9851485148514851,"1. A baseline of vanilla FedAvg on this task without compression,"
REC COMPRESSION WITH THE SAME HYPERPARAMETERS AS FOR THE WITH-DP EXPERIMENTS BUT,0.9867986798679867,"2. REC compression with the same hyperparameters as for the with-DP experiments but
without clipping,"
REC COMPRESSION WITH THE SAME HYPERPARAMETERS AS FOR THE WITH-DP EXPERIMENTS BUT,0.9884488448844885,"3. REC compression but with quantization of groups of size 64 (instead of per-tensor
quantization),"
REC COMPRESSION WITH THE SAME HYPERPARAMETERS AS FOR THE WITH-DP EXPERIMENTS BUT,0.9900990099009901,"The results can be found at Table 3, where we also include the results from DP-REC for reference.
We can see that the compression part of DP-REC performs quite well, reaching performance similar
to FedAvg while signiﬁcantly reducing the total communication costs. It is worthwhile to note that
there is a multitude of other hyperparameters that could be tuned to further reﬁne the compression-
only performance, such as: tuning the prior σ; varying the vector-size over which σ is computed; the
bit-width b; additional application of entropy-coding for the larger number of indices; adaptive K per
round. We leave a more detailed compression-only evaluation of our method to future work."
REC COMPRESSION WITH THE SAME HYPERPARAMETERS AS FOR THE WITH-DP EXPERIMENTS BUT,0.9917491749174917,Under review as a conference paper at ICLR 2022
REC COMPRESSION WITH THE SAME HYPERPARAMETERS AS FOR THE WITH-DP EXPERIMENTS BUT,0.9933993399339934,"Method
Accuracy
GB"
REC COMPRESSION WITH THE SAME HYPERPARAMETERS AS FOR THE WITH-DP EXPERIMENTS BUT,0.995049504950495,"DP-REC (ε=1)
59.3 ± 0.1
14.2
DP-REC (ε=3)
67.0 ± 0.1
14.2
DP-REC (ε=6)
69.1 ± 0.1
14.2"
REC COMPRESSION WITH THE SAME HYPERPARAMETERS AS FOR THE WITH-DP EXPERIMENTS BUT,0.9966996699669967,"FedAvg (no DP / compression)
86.28
690.6
REC (no DP clipping)
80.69
14.2
REC (no DP clipping, 64)
84.14
332.2"
REC COMPRESSION WITH THE SAME HYPERPARAMETERS AS FOR THE WITH-DP EXPERIMENTS BUT,0.9983498349834984,"Table 3: Compression-only (i.e., no DP) ablation studies, all run for 4k rounds. ∗was still improving
after 4k rounds."
