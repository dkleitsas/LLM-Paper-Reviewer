Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024449877750611247,"We present AutoOED, an Automated Optimal Experimental Design platform
powered by machine learning to accelerate discovering solutions with optimal ob-
jective trade-offs. To solve expensive multi-objective problems in a data-efﬁcient
manner, we implement popular multi-objective Bayesian optimization (MOBO)
algorithms with state-of-the-art performance in a modular framework. To further
accelerate the optimization in a time-efﬁcient manner, we propose a novel strategy
called Believer-Penalizer (BP), which allows batch experiments to be accelerated
asynchronously without affecting performance. AutoOED serves as a testbed for
machine learning researchers to quickly develop and evaluate their own MOBO
algorithms. We also provide a graphical user interface (GUI) for users with little
or no experience with coding, machine learning, or optimization to visualize and
guide the experiment design intuitively. Finally, we demonstrate that AutoOED
can control and guide real-world hardware experiments in a fully automated way
without human intervention."
INTRODUCTION,0.004889975550122249,"1
INTRODUCTION"
INTRODUCTION,0.007334963325183374,"Optimal Experimental Design (OED) problems in science and engineering often require satisfying
several conﬂicting objectives simultaneously. These problems aim to solve a multi-objective opti-
mization system and discover a set of optimal solutions, called Pareto optimal. Furthermore, the
objectives are typically black-box functions whose evaluations are time-consuming and costly (e.g.,
measuring real experiments or running expensive numerical simulations). Thus, the budget that de-
termines the number of experiments can be heavily constrained. Hence, an efﬁcient strategy for
guiding the experimental design towards Pareto optimal solutions is necessary. Recent advances
in machine learning have facilitated optimization of various design problems, including chemical
design (Grifﬁths & Hern´andez-Lobato, 2017), material design (Zhang et al., 2020), resource al-
location (Wu et al., 2013), environmental monitoring (Marchant & Ramos, 2012), recommender
systems (Chapelle & Li, 2011) and robotics (Martinez-Cantin et al., 2009). A machine learning
concept that enables automatic guidance of the design process is Bayesian optimization (Shahriari
et al., 2016). This concept is extensively studied in the machine learning community from a theo-
retical aspect and in the single-objective case. However, its practical applications in multi-objective
problems are still not widely explored due to the lack of easy-to-use and open-source software."
INTRODUCTION,0.009779951100244499,"In this paper, we present AutoOED1, an open-source platform for efﬁciently optimizing multi-
objective problems with a restricted budget of experiments. The key features of AutoOED include:"
INTRODUCTION,0.012224938875305624,"• Data-efﬁcient experimentation: AutoOED employs state-of-the-art MOBO strategies that
rapidly advances the Pareto front with a small set of evaluated experiments.
• Time-efﬁcient experimentation: AutoOED supports both synchronous and asynchronous
batch optimization to accelerate the optimization. We propose a novel and robust asyn-
chronous optimization strategy named Believer-Penalizer (BP), which is instrumental when
multiple workers run experiments in parallel, but their evaluations drastically vary in time."
INTRODUCTION,0.014669926650366748,"1Code, screenshots, detailed documentation and tutorials can be found at https://sites.google.
com/view/autooed."
INTRODUCTION,0.017114914425427872,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.019559902200488997,"• Intuitive GUI: An easy-to-use graphical user interface (GUI) is provided to directly visu-
alize and guide the optimization progress and facilitate the operation for users with little or
no experience with coding, optimization, or machine learning."
INTRODUCTION,0.022004889975550123,"• Modular structure: A highly modular Python codebase enables easy extensions and re-
placements of MOBO algorithm components. AutoOED can serve as a testbed for machine
learning researchers to easily develop and evaluate their own MOBO algorithms."
INTRODUCTION,0.02444987775061125,"• Automation of experimental design: The platform is designed for straightforward in-
tegration into a fully automated experimental design pipeline as long as the experiment
evaluations (either in simulation or physical) can be controlled via computer programs."
RELATED WORK,0.02689486552567237,"2
RELATED WORK"
RELATED WORK,0.029339853300733496,"Bayesian optimal experimental design
Optimal experimental design (OED) is the process of de-
signing the sequence of experiments to maximize speciﬁc objectives in a data- or time-efﬁcient man-
ner. Therefore, Bayesian optimization (BO) (Shahriari et al., 2016) is usually applied to ﬁnd optimal
solutions with a minimal number of evaluations. Essentially, BO relies on surrogate models like the
Gaussian process (GP) to accurately model the experimental process and proposes new experimental
designs based on deﬁned acquisition functions that trade-off between exploration and exploitation.
Popular choices of the acquisition functions include Expected Improvement (EI) (Moˇckus, 1975),
Upper Conﬁdence Bound (UCB) (Srinivas et al., 2010), Thompson Sampling (TS) (Thompson,
1933). Bayesian OED has found success in a wide range of applications (Greenhill et al., 2020) and
is the main methodology of AutoOED. To further speed up when evaluations can be carried out in
parallel, asynchronous BO approaches have been developed (Ginsbourger et al., 2010; Kandasamy
et al., 2018; Alvi et al., 2019). However, all of the previous literature focuses on single-objective
BO rather than the multi-objective scenario. In this paper, we extend several single-objective asyn-
chronous BO methods to multi-objective versions and propose a novel asynchronous method named
Believer-Penalizer (BP) with the stablest performance on multi-objective benchmark problems."
RELATED WORK,0.03178484107579462,"Multi-objective Bayesian optimization
MOBO is developed to optimize for a set of Pareto
optimal solutions while minimizing the number of experimental evaluations. Early approaches
solve multi-objective problems by scalarizing them into single-objective ones using random
weights (Knowles, 2006). Instead of scalarization, some acquisition functions are proposed to com-
pute a single objective, e.g., entropy-based or hypervolume-based (Russo & Van Roy, 2014; Be-
lakaria et al., 2019; Emmerich & Klinkenberg, 2008; Daulton et al., 2020a). Alternatively, MOBO
can be solved by deﬁning a separate acquisition function per objective, optimizing using cheap
multi-objective solvers (usually evolutionary algorithms like NSGA-II (Deb et al., 2002)) and ﬁnally
selecting one or a batch of designs to evaluate next (Bradford et al., 2018; Belakaria & Deshwal,
2020; Konakovic Lukovic et al., 2020). AutoOED implements many of them in a modular way and
allows easily changing modules in an uniﬁed MOBO framework (see Section 3.2)."
RELATED WORK,0.034229828850855744,"Open-source Bayesian optimization platform
There are many existing Python libraries for
Bayesian optimization including Spearmint (Snoek et al., 2012), HyperOpt (Bergstra et al., 2013),
GPyOpt (authors, 2016), GPﬂowOpt (Knudde et al., 2017), Dragonﬂy (Kandasamy et al., 2020),
AX (Bakshy et al., 2018), Optuna (Akiba et al., 2019), HyperMapper (Nardi et al., 2019),
BoTorch (Balandat et al., 2020a), SMAC3 (Lindauer et al., 2021) and OpenBox (Li et al., 2021).
These Python libraries are designed for general applications and have different algorithmic features
supported. The feature comparison between AutoOED and these libraries is shown in Table 1 and
is further discussed in Section 5.2. However, they are all targeted for experts in coding without an
intuitive GUI. In contrast, there are also software platforms that provide intuitive user interface and
visualization to speciﬁc domain experts but the platforms cannot be used for other general applica-
tions, for example, Auto-QChem (Shields et al., 2021) for chemical synthesis and GeoBO (Haan,
2021) for geoscience. Combining powerful Bayesian optimization algorithms and an intuitive GUI,
AutoOED is designed to be a general optimization platform that can be easily used by anyone for
applications in any ﬁeld."
RELATED WORK,0.03667481662591687,"2The comparison is based on AutoOED’s core features. ”∼” means the package only supports a single multi-
objective algorithm rather than a modular multi-objective framework with several state-of-the-art algorithms."
RELATED WORK,0.039119804400977995,Under review as a conference paper at ICLR 2022
RELATED WORK,0.04156479217603912,Table 1: Feature comparison between different Bayesian optimization platforms.2
RELATED WORK,0.044009779951100246,"Name
GUI
Multiple
objectives
Multiple
domains
Asynchronous
optimization
External
evaluation
Modular
framework
Built-in
visualization"
RELATED WORK,0.04645476772616137,"Spearmint
✓
GPyOpt
✓
✓
✓
✓
GPﬂowOpt
∼
✓
Dragonﬂy
∼
✓
✓
BoTorch
✓
✓
✓
AutoOED
✓
✓
✓
✓
✓
✓
✓"
DATA-EFFICIENT MULTI-OBJECTIVE OPTIMIZATION,0.0488997555012225,"3
DATA-EFFICIENT MULTI-OBJECTIVE OPTIMIZATION"
PROBLEM FORMULATION,0.05134474327628362,"3.1
PROBLEM FORMULATION ℝ! !"
PROBLEM FORMULATION,0.05378973105134474,Design Space
PROBLEM FORMULATION,0.05623471882640587,"Pareto Set !
ℝ"" !(!)"
PROBLEM FORMULATION,0.05867970660146699,Performance Space
PROBLEM FORMULATION,0.061124694376528114,"Pareto Front
Pareto Set
Pareto Front"
PROBLEM FORMULATION,0.06356968215158924,"Design Space
Performance Space"
PROBLEM FORMULATION,0.06601466992665037,"Optimal experiment design problems involving mul-
tiple conﬂicting objectives can be formulated as a
multi-objective optimization on design parameters
as data- and time-efﬁcient as possible. More for-
mally, we consider a optimization problem over a
set of design variables X ⊂Rd, called design space.
The goal is to simultaneously minimize m ≥2 ob-
jective functions f1, ..., fm : X →R.
Representing the vector of all objectives as f(x) =
(f1(x), ..., fm(x)), the performance space is then an m-dimensional image f(X) ⊂Rm. Conﬂict-
ing objectives result in a set of optimal solutions rather than a single best solution. These optimal
solutions are referred to as Pareto set Ps ⊆X in the design space, and the corresponding images in
performance space are Pareto front Pf = f(Ps) ⊂Rm."
PROBLEM FORMULATION,0.06845965770171149,"To measure the quality of an approximated Pareto front, hypervolume (Zitzler & Thiele, 1999) is
the most commonly used metric in multi-objective optimization (Riquelme et al., 2015). Let Pf be
a Pareto front approximation in an m-dimensional performance space and given a reference point
r ∈Rm, the hypervolume H(Pf) is deﬁned as H(Pf) =
R"
PROBLEM FORMULATION,0.07090464547677261,"Rm 1H(Pf )(z)dz, where H(Pf) = {z ∈
Z | ∃1 ≤i ≤|Pf| : r ⪯z ⪯Pf(i)}. ⪯is the relation operator of objective dominance and 1H(Pf )
is a Dirac delta function that equals 1 if z ∈H(Pf) and 0 otherwise. The higher the hypervolume,
the better Pf approximates the true Pareto front."
MODULAR ALGORITHM FRAMEWORK,0.07334963325183375,"3.2
MODULAR ALGORITHM FRAMEWORK"
MODULAR ALGORITHM FRAMEWORK,0.07579462102689487,"Pareto front
Observations f1 f2"
MODULAR ALGORITHM FRAMEWORK,0.07823960880195599,Selected points f1 f2 x fj(x)
MODULAR ALGORITHM FRAMEWORK,0.08068459657701711,Evaluate proposed designs
MODULAR ALGORITHM FRAMEWORK,0.08312958435207823,"Multi-objective solver
Selection
Acquisition function
Surrogate model"
MODULAR ALGORITHM FRAMEWORK,0.08557457212713937,Approximate Pareto set and
MODULAR ALGORITHM FRAMEWORK,0.08801955990220049,"front over all    a
fj"
MODULAR ALGORITHM FRAMEWORK,0.09046454767726161,Propose a batch of designs to
MODULAR ALGORITHM FRAMEWORK,0.09290953545232274,"evaluate next
Define       that trades off mean"
MODULAR ALGORITHM FRAMEWORK,0.09535452322738386,and uncertainty of surrogates
MODULAR ALGORITHM FRAMEWORK,0.097799511002445,"fj
Fit surrogate models for each"
MODULAR ALGORITHM FRAMEWORK,0.10024449877750612,objective on evaluated data
MODULAR ALGORITHM FRAMEWORK,0.10268948655256724,"Observations
Mean
Uncertainty
x fj(x)"
MODULAR ALGORITHM FRAMEWORK,0.10513447432762836,Gaussian Process
MODULAR ALGORITHM FRAMEWORK,0.10757946210268948,Neural Network …
MODULAR ALGORITHM FRAMEWORK,0.1100244498777506,"Expected Improvement 
Upper Confidence Bound …"
MODULAR ALGORITHM FRAMEWORK,0.11246943765281174,"NSGA-II 
MOEA/D …"
MODULAR ALGORITHM FRAMEWORK,0.11491442542787286,Hypervolume Improvement
MODULAR ALGORITHM FRAMEWORK,0.11735941320293398,Uncertainty …
MODULAR ALGORITHM FRAMEWORK,0.1198044009779951,Acquisition value
MODULAR ALGORITHM FRAMEWORK,0.12224938875305623,Figure 1: Algorithmic pipeline and core modules of multi-objective Bayesian optimization.
MODULAR ALGORITHM FRAMEWORK,0.12469437652811736,Under review as a conference paper at ICLR 2022
MODULAR ALGORITHM FRAMEWORK,0.1271393643031785,"Multi-objective Bayesian optimization (MOBO) is a data-driven approach that attempts to learn the
black-box objective functions f(x) from available data and ﬁnd Pareto optimal solutions in an iter-
ative and data-efﬁcient manner. MOBO typically consists of four core modules: (i) an inexpensive
surrogate model for the black-box objective functions; (ii) an acquisition function that deﬁnes sam-
pling from the surrogate model and trade-off between exploration and exploitation of the design
space; (iii) a cheap multi-objective solver to approximate the Pareto set and front; (iv) a selection
strategy that proposes a single or a batch of experiments to evaluate next. These four modules (see
Figure 1) are implemented as core and independent building blocks of the AutoOEDs, making it
highly modularized and easy to develop new algorithms and modules. The whole pipeline starts
from a given small dataset or a set of random evaluated samples, then it works iteratively by propos-
ing new design samples and evaluating them until the stopping criterion is met."
MODULAR ALGORITHM FRAMEWORK,0.1295843520782396,"For each module in this framework, AutoOED supports following choices:"
MODULAR ALGORITHM FRAMEWORK,0.13202933985330073,"• Surrogate model: Gaussian process, neural network (multi-layer perceptron), Bayesian
neural network (DNGO (Snoek et al., 2015))
• Acquisition function: Expected Improvement, Probability of Improvement, Upper Conﬁ-
dence Bound, Thompson Sampling, identity function
• Multi-objective solver: NSGA-II, MOEA/D, ParetoFrontDiscovery (Schulz et al., 2018)
• Selection: Hypervolume improvement, uncertainty, random, etc.
• Stopping criterion: Time, number of evaluations, hypervolume convergence"
MODULAR ALGORITHM FRAMEWORK,0.13447432762836187,"class TSEMO(MOBO):
’’’
[Bradford et al. 2018]
’’’
spec = {"
MODULAR ALGORITHM FRAMEWORK,0.13691931540342298,"’surrogate’: ’gp’,
’acquisition’: ’ts’,
’solver’: ’nsga2’,
’selection’: ’hvi’,
}"
MODULAR ALGORITHM FRAMEWORK,0.1393643031784841,"class USEMO_EI(MOBO):
’’’
[Belakaria and Deshwal 2020]
’’’
spec = {"
MODULAR ALGORITHM FRAMEWORK,0.14180929095354522,"’surrogate’: ’gp’,
’acquisition’: ’ei’,
’solver’: ’nsga2’,
’selection’: ’uncertainty’,
}"
MODULAR ALGORITHM FRAMEWORK,0.14425427872860636,"class DGEMO(MOBO):
’’’
[Lukovic et al. 2020]
’’’
spec = {"
MODULAR ALGORITHM FRAMEWORK,0.1466992665036675,"’surrogate’: ’gp’,
’acquisition’: ’identity’,
’solver’: ’discovery’,
’selection’: ’direct’,
}"
MODULAR ALGORITHM FRAMEWORK,0.1491442542787286,Code Example 1: Creating algorithms in AutoOED by simply specifying module combinations.
MODULAR ALGORITHM FRAMEWORK,0.15158924205378974,"Based on this framework, we implement several popular and state-of-the-art MOBO methods, in-
cluding ParEGO (Knowles, 2006), MOEA/D-EGO (Zhang et al., 2009), TSEMO (Bradford et al.,
2018), USeMO (Belakaria & Deshwal, 2020), DGEMO (Konakovic Lukovic et al., 2020). DGEMO
exhibits state-of-the-art performance for data-efﬁcient, multi-objective problems with batch evalua-
tions to the best of our knowledge. With necessary modules of the MOBO framework implemented,
the algorithms can be easily composed by specifying the choice of each module and inheriting the
base class MOBO, see Code Example 1. Supported choices of each module can be found in our
documentation. Users can select an algorithm from this library that best ﬁts the characteristics of
their physical system or optimization goals, or they can easily create new algorithms by specifying
novel combinations of existing modules in just a few lines of code."
TIME-EFFICIENT MULTI-OBJECTIVE OPTIMIZATION,0.15403422982885084,"4
TIME-EFFICIENT MULTI-OBJECTIVE OPTIMIZATION"
BATCH OPTIMIZATION,0.15647921760391198,"4.1
BATCH OPTIMIZATION"
BATCH OPTIMIZATION,0.15892420537897312,"While standard MOBO optimizes for the Pareto front in a data-efﬁcient manner, often, when mul-
tiple experiment setups are available, evaluations can be executed in batch by parallel workers to
further speed up the whole optimization process. To leverage this speed-up, all the algorithms in
AutoOED are implemented to support batch evaluation."
BATCH OPTIMIZATION,0.16136919315403422,"However, if parallel workers evaluate in different speeds, some workers are left idle when they
ﬁnish evaluations earlier than others. Therefore, asynchronous optimization is desired to maximize
the utilization of workers and is able to evaluate many more designs than synchronous optimization
in a ﬁxed amount of time, as also illustrated by Kandasamy et al. (2018) and Alvi et al. (2019).
Nevertheless, while some designs are being evaluated (i.e., busy designs), how to propose the next"
BATCH OPTIMIZATION,0.16381418092909536,Under review as a conference paper at ICLR 2022
BATCH OPTIMIZATION,0.16625916870415647,"design that (i) avoids being similar to the busy designs and (ii) incorporates knowledge from busy
designs to reach better regions in the performance space is the key question that we want to explore."
BATCH OPTIMIZATION,0.1687041564792176,"To develop efﬁcient asynchronous strategy for multi-objective optimization, we borrow ideas from
previous literature in the single-objective setting."
BATCH OPTIMIZATION,0.17114914425427874,Lower bound
BATCH OPTIMIZATION,0.17359413202933985,"Busy design
Evaluated design
Posterior mean
Posterior std x f"
BATCH OPTIMIZATION,0.17603911980440098,"(a) The failure case of KB when believing
overestimated busy designs. A B A B H1 H2 H1 H2"
BATCH OPTIMIZATION,0.1784841075794621,"Penalizing
Believing"
BATCH OPTIMIZATION,0.18092909535452323,"Busy design
Proposed design"
BATCH OPTIMIZATION,0.18337408312958436,Current Pareto front
BATCH OPTIMIZATION,0.18581907090464547,"Hypervolume improvement f2 f1
f1 f2"
BATCH OPTIMIZATION,0.1882640586797066,"H1 > H2
H1 < H2"
BATCH OPTIMIZATION,0.19070904645476772,"(b) The sub-optimality of LP in multi-objective scenario when be-
lieving busy designs affects the selection result."
BATCH OPTIMIZATION,0.19315403422982885,Figure 2: Analysis of KB and LP strategies for asynchronous optimization.
FAILURES OF EXISTING STRATEGIES,0.19559902200489,"4.2
FAILURES OF EXISTING STRATEGIES"
FAILURES OF EXISTING STRATEGIES,0.1980440097799511,"Kriging Believer
(KB) (Ginsbourger et al., 2010) is a simple yet effective approach that believes
the performance of busy designs is their posterior mean of the surrogate model when optimizing
for new designs. In other words, it treats the mean prediction of the busy designs as their real
performance and eliminates their posterior variance to prevent acquisition functions from preferring
those regions. However, failure case happens when it believes an overestimated design, it might
become difﬁcult to ﬁnd designs better than this overestimated one and make further improvement,
see Figure 2a. Especially, when the posterior mean of the busy design is extremely small and even
exceeds the lower bound of the objective, subsequent optimization can hardly ﬁnd a better solution.
In other words, subsequent optimization will only propose more overestimated designs with even
lower predicted performance to ”make improvement”, even though they are even farther from the
ground truth and drive the optimization away from the real meaningful regions. This overestimation
issue has not been studied in the past literature to the best of our knowledge, though KB is still the
strategy used in popular BO packages (Kandasamy et al., 2020; Balandat et al., 2020b)."
FAILURES OF EXISTING STRATEGIES,0.20048899755501223,"Local Penalization
(LP) (Gonz´alez et al., 2016) is another widely used approach that directly
penalizes the nearby region of the busy designs to prevent similar designs from being evaluated next.
However, extending this approach to the multi-objective scenario sometimes leads to sub-optimal
selection of new designs, as explained in Figure 2b. Intuitively, this sub-optimality comes from the
failure of leveraging the accurate predictions from the surrogate model. Consider when selecting the
best design to evaluate from a set of candidate designs (A and B) proposed by the multi-objective
solver using hypervolume improvement criterion, while a busy design is in evaluation. LP penalizes
the nearby regions of the busy design in the design space but has no control over the performance
space, which means that designs with similar performance as the busy design could still be selected
(design A). Ideally, if the surrogate prediction of the busy design is certain, we can leverage this to
avoid proposing designs with little performance gain. For example, simply believing the prediction
of the busy design leads to selecting design B that has a higher hypervolume improvement."
BELIEVER-PENALIZER STRATEGY,0.20293398533007334,"4.3
BELIEVER-PENALIZER STRATEGY"
BELIEVER-PENALIZER STRATEGY,0.20537897310513448,"In conclusion, we observe that the failure case of KB is due to the trust of uncertain predictions
while the sub-optimality of LP comes from not believing the certain prediction. Therefore, we
propose a novel strategy BP that naturally combines KB and LP by applying KB to designs with
certain predictions and LP to designs with uncertain predictions. Here, certainty of prediction is
simply deﬁned as the posterior standard deviation from the surrogate model which can be Gaussian
processes, Bayesian neural networks or other type of model that computes standard deviation of"
BELIEVER-PENALIZER STRATEGY,0.2078239608801956,Under review as a conference paper at ICLR 2022
BELIEVER-PENALIZER STRATEGY,0.21026894865525672,"predictions. Though the idea of BP is general and one can use any analytical expression to determine
the certainty threshold for applying KB or LP, in practice, we ﬁnd a simple probabilistic form which
works well: Pi(x) = max(1 −2σi(x), 0) where Pi is the probability of believing x for the i-th
objective and σi is the posterior std of x from the surrogate model of the i-th objective. Because
the objective data is normalized as zero with mean unit variance before ﬁtting the surrogate models,
σi(x) is generally between 0 and 1. As a result, BP generalizes more robustly than both KB and LP
to most of the benchmark problems as empirically demonstrated in Section 6.1."
THE AUTOOED PLATFORM,0.21271393643031786,"5
THE AUTOOED PLATFORM"
OVERALL WORKFLOW,0.21515892420537897,"5.1
OVERALL WORKFLOW GUI"
OVERALL WORKFLOW,0.2176039119804401,Optimization
OVERALL WORKFLOW,0.2200488997555012,"Automatic 
Evaluation
Database
Update visualization"
OVERALL WORKFLOW,0.22249388753056235,Launch
OVERALL WORKFLOW,0.22493887530562348,Propose designs
OVERALL WORKFLOW,0.2273838630806846,Store results
OVERALL WORKFLOW,0.22982885085574573,Interact User
OVERALL WORKFLOW,0.23227383863080683,Scheduler
OVERALL WORKFLOW,0.23471882640586797,Assign tasks
OVERALL WORKFLOW,0.2371638141809291,Update status
OVERALL WORKFLOW,0.2396088019559902,"GUI
Optimization
Manual 
Evaluation
Database"
OVERALL WORKFLOW,0.24205378973105135,"Launch
Propose designs
Store results"
OVERALL WORKFLOW,0.24449877750611246,Update visualization
OVERALL WORKFLOW,0.2469437652811736,Interact User
OVERALL WORKFLOW,0.24938875305623473,Perform experiments
OVERALL WORKFLOW,0.25183374083129584,Manual Mode
OVERALL WORKFLOW,0.254278728606357,Auto Mode
OVERALL WORKFLOW,0.2567237163814181,Figure 3: The manual and automatic workﬂows of the platform design.
OVERALL WORKFLOW,0.2591687041564792,"As a full-stack software, the overall workﬂows of AutoOED are presented in Figure 3. The manual
mode and auto mode are distinguished by whether the experimental evaluations need to be done
manually or can be done automatically through programs. Key components in this workﬂow include:"
OVERALL WORKFLOW,0.2616136919315403,"• GUI: An intuitive graphical interface between users and optimization, evaluation and
database, with many powerful functionalities supported as described in Section 5.2."
OVERALL WORKFLOW,0.26405867970660146,"• Optimization: Once the user sends the optimization request, the MOBO algorithm will be
running in back-end and proposing next promising designs."
OVERALL WORKFLOW,0.2665036674816626,"• Evaluation: AutoOED supports two different ways of performing evaluations: manually
by hand and automatically by evaluation programs. The evaluation module receives designs
proposed by the optimization algorithms and outputs the corresponding objective values."
OVERALL WORKFLOW,0.26894865525672373,"• Database: The SQL database stores information of each design including design parame-
ters, predicted and real objective values in a transactional manner which can be distributed."
OVERALL WORKFLOW,0.2713936430317848,"Next, we illustrate how the workﬂows of AutoOED combine these individual components."
OVERALL WORKFLOW,0.27383863080684595,"Manual mode
When the experimental evaluation must be performed by hand, the manual mode
has to be selected. In this case, users need to interface with both the GUI and evaluation. When
GUI receives the optimization request from the user, it launches the optimization worker to propose
designs for the user to evaluate. Users will see the new proposed designs from the database GUI, then
they can enter the evaluation results in the same interface. Once AutoOED receives the evaluation
results, visualizations and statistics will be updated to inform users the latest status."
OVERALL WORKFLOW,0.2762836185819071,Under review as a conference paper at ICLR 2022
OVERALL WORKFLOW,0.2787286063569682,"Auto mode
If the evaluation program is available (in Python/C/C++/Matlab), AutoOED can au-
tomatically guide the experiments by alternating between optimization and evaluation through a
scheduler. In this case, upon receiving the optimization request and stopping criteria from the user,
the scheduler will automatically repeat the optimization-evaluation cycle until one of the stopping
criteria is met. The scheduler automatically launches evaluations after designs are proposed by
optimization workers, and restarts optimization after evaluations are done, thus the whole loop of
experimental design can be executed in an automated way without human intervention."
FEATURE COMPARISON WITH OTHER PLATFORMS,0.28117359413202936,"5.2
FEATURE COMPARISON WITH OTHER PLATFORMS"
FEATURE COMPARISON WITH OTHER PLATFORMS,0.28361858190709044,"As shown in Table 1, we compare AutoOED with existing BO platforms according to the following
important criteria. For other features of AutoOED, please refer to Appendix A."
FEATURE COMPARISON WITH OTHER PLATFORMS,0.2860635696821516,"Graphical user interface and visualization
The GUI guides the user through a set of simple steps
to conﬁgure the problem, such as the number of design and performance parameters, the parameter
bounds and constraints, parallelization settings, and selection of the optimization algorithm without
the need of coding. The GUI also includes a real-time display of the design and performance space
which allows users to easily understand the current status of optimization. We also support display-
ing and exporting the whole optimization history (including database and statistics). All previous
platforms do not offer such a convenient GUI and even the visualizations need to be written by the
user, except GPyOpt has a built-in tool for plotting the acquisition function and convergence."
FEATURE COMPARISON WITH OTHER PLATFORMS,0.2885085574572127,"Multiple objectives and multiple domains
For multi-objectivity, GPﬂowOpt implements
HVPOI (Couckuyt et al., 2014) and Dragonﬂy implements MOORS (Paria et al., 2020) without
the ﬂexibility of incorporating other algorithms or modules. BoTorch supports MESMO (Belakaria
et al., 2019), qEHVI (Daulton et al., 2020b) and qParEGO. Although algorithm implementations
differ across platforms, AutoOED covers a wider range of algorithms and incorporates them into a
more uniﬁed modular framework. Except continuous designs, AutoOED supports discrete, binary,
categorical designs and a mix of them by applying discrete or one-hot transformation in ﬁtting and
evaluating the surrogate model, as suggested by Garrido-Merch´an & Hern´andez-Lobato (2020)."
FEATURE COMPARISON WITH OTHER PLATFORMS,0.29095354523227385,"Asynchronous optimization
As demonstrated in Section 4, AutoOED supports different asyn-
chronous techniques including KB, LP and BP, while Dragonﬂy and BoTorch only implements KB
and all other platforms do not support asynchronous optimization."
FEATURE COMPARISON WITH OTHER PLATFORMS,0.293398533007335,"External evaluation
There are many real-world experimental design problems where the experi-
mental evaluation must be performed by hand or external lab equipment thus it is hard to write an
analytical objective function in code. Though simple to implement, surprisingly, among all of the
existing platforms surveyed in this paper, only GPyOpt supports evaluating externally and suggest-
ing designs to evaluate purely based on a given dataset. AutoOED allows users to see the suggested
designs and enter the evaluation results easily in the database interface, as described in Section 5.1."
EXPERIMENTS,0.29584352078239606,"6
EXPERIMENTS"
EXPERIMENTS,0.2982885085574572,"We conduct experiments across 12 standard multi-objective benchmark problems: ZDT1-4 (Zitzler
et al., 2000), DTLZ1-4 (Deb et al., 2005), OKA1-2 (Okabe et al., 2004) and VLMOP2-3 (Van Veld-
huizen & Lamont, 1999). For each algorithm, we run experiments with 20 different random seeds
and a budget of 100 samples. The initial 20 samples of each run are generated by Latin hypercube
sampling (McKay et al., 1979). We measure as the performance criterion the log of the difference
between the hypervolume of the ground-truth Pareto front and hypervolume of the best Pareto front
approximation found by the algorithms (thus lower is better). The curves are averaged over 20
random seeds and the variance is shown as shaded regions. Detailed problem information and hy-
perparameters are described in Appendix B. Additional ablation studies are included in Appendix D."
ASYNCHRONOUS MOBO ALGORITHMS,0.30073349633251834,"6.1
ASYNCHRONOUS MOBO ALGORITHMS"
ASYNCHRONOUS MOBO ALGORITHMS,0.30317848410757947,"To test whether Believer-Penalizer is effective, we compare four asynchronous MOBO algorithms on
all benchmark problems. Async simply ignores the busy designs while optimizing asynchronously"
ASYNCHRONOUS MOBO ALGORITHMS,0.3056234718826406,Under review as a conference paper at ICLR 2022
ASYNCHRONOUS MOBO ALGORITHMS,0.3080684596577017,"20
40
60
80
100
−0.4 −0.2 0.0 0.2 0.4"
ASYNCHRONOUS MOBO ALGORITHMS,0.3105134474327628,Log hypervolume difference zdt1
ASYNCHRONOUS MOBO ALGORITHMS,0.31295843520782396,"20
40
60
80
100 0.40 0.45 0.50 0.55 0.60 zdt2"
ASYNCHRONOUS MOBO ALGORITHMS,0.3154034229828851,"20
40
60
80
100 0.2 0.3 0.4 0.5 zdt3"
ASYNCHRONOUS MOBO ALGORITHMS,0.31784841075794623,"20
40
60
80
100 1.8 1.9 2.0 2.1 zdt4"
ASYNCHRONOUS MOBO ALGORITHMS,0.3202933985330073,"20
40
60
80
100 3.6 3.8 4.0 4.2"
ASYNCHRONOUS MOBO ALGORITHMS,0.32273838630806845,Log hypervolume difference dtlz1
ASYNCHRONOUS MOBO ALGORITHMS,0.3251833740831296,"20
40
60
80
100 −0.8 −0.6 −0.4 −0.2 dtlz2"
ASYNCHRONOUS MOBO ALGORITHMS,0.3276283618581907,"20
40
60
80
100
4.0 4.2 4.4 4.6 4.8 5.0 dtlz3"
ASYNCHRONOUS MOBO ALGORITHMS,0.33007334963325186,"20
40
60
80
100 −1.1 −1.0 −0.9 −0.8 dtlz4"
ASYNCHRONOUS MOBO ALGORITHMS,0.33251833740831294,"20
40
60
80
100
Number of samples 0.7 0.8 0.9 1.0 1.1"
ASYNCHRONOUS MOBO ALGORITHMS,0.33496332518337407,Log hypervolume difference oka1
ASYNCHRONOUS MOBO ALGORITHMS,0.3374083129584352,"20
40
60
80
100
Number of samples 0.7 0.8 0.9 1.0 1.1 1.2 oka2"
ASYNCHRONOUS MOBO ALGORITHMS,0.33985330073349634,"20
40
60
80
100
Number of samples −2.25 −2.00 −1.75 −1.50 −1.25 −1.00 −0.75"
ASYNCHRONOUS MOBO ALGORITHMS,0.3422982885085575,vlmop2
ASYNCHRONOUS MOBO ALGORITHMS,0.34474327628361856,"20
40
60
80
100
Number of samples −0.5 0.0 0.5 1.0 1.5"
ASYNCHRONOUS MOBO ALGORITHMS,0.3471882640586797,vlmop3
ASYNCHRONOUS MOBO ALGORITHMS,0.34963325183374083,"Async
Async KB
Async LP
Async BP"
ASYNCHRONOUS MOBO ALGORITHMS,0.35207823960880197,Figure 4: Performance comparison between variants of asynchoronous MOBO algorithms.
ASYNCHRONOUS MOBO ALGORITHMS,0.3545232273838631,"and the remaining algorithms are described in Section 4. Figure 4 shows that Async BP consistently
outperforms other methods and follows the best of Async KB and Async LP."
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.3569682151589242,"6.2
PERFORMANCE COMPARISON ACROSS PLATFORMS"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.3594132029339853,"20
40
60
80
100 −0.2 0.0 0.2 0.4"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.36185819070904646,Log hypervolume difference zdt1
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.3643031784841076,"20
40
60
80
100 0.1 0.2 0.3 0.4 0.5 0.6 zdt2"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.36674816625916873,"20
40
60
80
100 0.2 0.3 0.4 0.5 zdt3"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.3691931540342298,"20
40
60
80
100 1.6 1.8 2.0 zdt4"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.37163814180929094,"20
40
60
80
100 3.6 3.8 4.0 4.2"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.3740831295843521,Log hypervolume difference dtlz1
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.3765281173594132,"20
40
60
80
100 −1.5 −1.0 −0.5 dtlz2"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.37897310513447435,"20
40
60
80
100 3.5 4.0 4.5 5.0 dtlz3"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.38141809290953543,"20
40
60
80
100
−1.1 −1.0 −0.9 −0.8 dtlz4"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.38386308068459657,"20
40
60
80
100
Number of samples 0.7 0.8 0.9 1.0 1.1"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.3863080684596577,Log hypervolume difference oka1
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.38875305623471884,"20
40
60
80
100
Number of samples 0.6 0.7 0.8 0.9 1.0 1.1 1.2 oka2"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.39119804400978,"20
40
60
80
100
Number of samples −3.0 −2.5 −2.0 −1.5 −1.0"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.39364303178484106,vlmop2
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.3960880195599022,"20
40
60
80
100
Number of samples −0.5 0.0 0.5 1.0 1.5"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.39853300733496333,vlmop3
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.40097799511002447,"BoTorch
GPflowOpt
Dragonfly
Hypermapper
OpenBox
AutoOED"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.4034229828850856,"Figure 5: Performance of AutoOED on the benchmark problems compared to other BO platforms.
The result of BoTorch on VLMOP3 is omitted as the algorithm fails to stop within 24 hours."
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.4058679706601467,"We compare AutoOED against other open-source BO platforms with multi-objective optimization
capabilities, including BoTorch, GPﬂowOpt, Dragonﬂy, HyperMapper and OpenBox on the afore-
mentioned benchmark problems. Our baseline implementation follows the default recommended"
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.4083129584352078,Under review as a conference paper at ICLR 2022
PERFORMANCE COMPARISON ACROSS PLATFORMS,0.41075794621026895,"settings in the original documentation and tutorials. For BoTorch, we use the qEHVI acquisition
function. In Figure 5, we demonstrate the competitive performance of AutoOED against other BO
platforms using our benchmark. AutoOED takes a major lead in several challenging problems such
as ZDT1, ZDT3, and VLMOP3, which shows that our platform handles high-dimensional MOBO
problems very well with the proposed asynchronous BP strategy. Besides, the performance of Au-
toOED is generally stable and ends up either the best or comparable on most benchmark problems.
We conduct additional ranked and paired comparisons between AutoOED and all the baseline plat-
forms in Appendix C to further demonstrate AutoOED’s robustness and competitive performance."
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.4132029339853301,"6.3
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN"
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.4156479217603912,(a) Experiment setup.
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.4180929095354523,"0
5
10
15
20
25
30
35
Overshoot 20 40 60 80 100 120"
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.42053789731051344,Response time (s)
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.4229828850855746,"MOBO
Random"
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.4254278728606357,(b) Discovered solutions.
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.4278728606356968,"5
10
15
20
25
30
Number of samples 1050 1100 1150 1200 1250 1300 1350"
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.43031784841075793,Hypervolume
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.43276283618581907,"MOBO
Random"
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.4352078239608802,(c) Hypervolume.
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.43765281173594134,"Figure 6: Physical setup, optimization process and solutions of the PID heater control experiment."
REAL-WORLD OPTIMAL EXPERIMENTAL DESIGN,0.4400977995110024,"We further demonstrate the real-world applicability of AutoOED through applying to optimize a
PID heater controller to have optimal response time and minimal overshoot in a fully automated
way. Details of the experimental setup can be found in Appendix B.3. To automate the experiment,
we simply link the Python evaluation program of this experiment setup to AutoOED through GUI
then start the iterative optimization. Finally, the results are exported as shown in Figure 6, where a
set of solutions are discovered with optimal trade-offs between minimal response time and minimal
overshoot. Using MOBO algorithms provided by AutoOED is able to discover better designs com-
pared to random sampling at the presence of evaluation noise (temperature measurement error, lack
of precise initial temperature control, fabrication differences between heater blocks). This example,
with all the components that people can easily buy off-the-shelf, serves as a simple proof of concept
that AutoOED is applicable to optimize real physical systems. More examples, including optimiz-
ing material structure based on FEM simulation and optimizing a physical motor’s rotation, can be
found in our documentation with detailed instructions on how to interact with GUI and compose the
evaluation program for fully automated OED."
CONCLUSION AND FUTURE WORK,0.44254278728606355,"7
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.4449877750611247,"We introduced an open-source platform for automated optimal experimental design of multi-
objective problems. The platform is of modular structure, facilitating the implementation of dif-
ferent multi-objective Bayesian optimization (MOBO) algorithms and enabling both data- and time-
efﬁcient optimization. In addition, the platform includes a novel strategy for asynchronous batch
optimization for improved time efﬁciency. We performed extensive experiments on standard bench-
mark to demonstrate the robust performance against other relevant methods. Furthermore, we con-
ducted real-world physical experiments to showcase the automated usage of our platform."
CONCLUSION AND FUTURE WORK,0.4474327628361858,"From the research and engineering perspective, future work includes implementing additional fea-
tures, such as expensive constraint handling, advanced noise handling and extending AutoOED’s
framework to incorporate a even wider range of MOBO algorithms. From a practical standpoint,
we are also interested in applying AutoOED to automate more real-world experimental design prob-
lems in science and engineering. And we believe AutoOED will gradually lower the barrier between
MOBO research and practical applications."
CONCLUSION AND FUTURE WORK,0.44987775061124696,Under review as a conference paper at ICLR 2022
REFERENCES,0.45232273838630804,REFERENCES
REFERENCES,0.4547677261613692,"Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A
next-generation hyperparameter optimization framework, 2019."
REFERENCES,0.4572127139364303,"Ahsan S Alvi, Binxin Ru, Jan Calliess, Stephen J Roberts, and Michael A Osborne. Asynchronous
batch bayesian optimisation with improved local penalisation. arXiv preprint arXiv:1901.10452,
2019."
REFERENCES,0.45965770171149145,"The GPyOpt authors. GPyOpt: A bayesian optimization framework in python. http://github.
com/SheffieldML/GPyOpt, 2016."
REFERENCES,0.4621026894865526,"Eytan Bakshy, Lili Dworkin, Brian Karrer, Konstantin Kashin, Benjamin Letham, Ashwin Murthy,
and Shaun Singh. Ae: A domain-agnostic platform for adaptive experimentation. 2018."
REFERENCES,0.46454767726161367,"Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham,
Andrew Gordon Wilson,
and Eytan Bakshy.
BoTorch:
A Framework for Efﬁcient
Monte-Carlo Bayesian Optimization.
In Advances in Neural Information Processing Sys-
tems 33, 2020a.
URL https://proceedings.neurips.cc/paper/2020/hash/
f5b1b89d98b7286673128a5fb112cb9a-Abstract.html."
REFERENCES,0.4669926650366748,"Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, An-
drew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efﬁcient Monte-Carlo
Bayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020b. URL
http://arxiv.org/abs/1910.06403."
REFERENCES,0.46943765281173594,"Syrine Belakaria and Aryan Deshwal.
Uncertainty-aware search framework for multi-objective
bayesian optimization. In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2020."
REFERENCES,0.4718826405867971,"Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search for multi-
objective bayesian optimization. In Advances in Neural Information Processing Systems, pp.
7823–7833, 2019."
REFERENCES,0.4743276283618582,"James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter
optimization in hundreds of dimensions for vision architectures. In International conference on
machine learning, pp. 115–123. PMLR, 2013."
REFERENCES,0.4767726161369193,"Eric Bradford, Artur M Schweidtmann, and Alexei Lapkin. Efﬁcient multiobjective optimization
employing gaussian processes, spectral sampling and a genetic algorithm.
Journal of global
optimization, 71(2):407–438, 2018."
REFERENCES,0.4792176039119804,"Olivier
Chapelle
and
Lihong
Li.
An
empirical
evaluation
of
thompson
sampling.
In
J.
Shawe-Taylor,
R.
S.
Zemel,
P.
L.
Bartlett,
F.
Pereira,
and
K.
Q.
Wein-
berger
(eds.),
Advances
in
Neural
Information
Processing
Systems
24,
pp.
2249–
2257. Curran Associates,
Inc.,
2011.
URL http://papers.nips.cc/paper/
4321-an-empirical-evaluation-of-thompson-sampling.pdf."
REFERENCES,0.48166259168704156,"Ivo Couckuyt, Dirk Deschrijver, and Tom Dhaene. Fast calculation of multiobjective probability
of improvement and expected improvement criteria for pareto optimization. Journal of Global
Optimization, 60(3):575–594, 2014."
REFERENCES,0.4841075794621027,"Samuel Daulton, Maximilian Balandat, and Eytan Bakshy.
Differentiable expected hyper-
volume improvement for parallel multi-objective bayesian optimization.
In H. Larochelle,
M.
Ranzato,
R.
Hadsell,
M.
F.
Balcan,
and
H.
Lin
(eds.),
Advances
in
Neu-
ral Information Processing Systems,
volume 33,
pp.
9851–9864. Curran Associates,
Inc.,
2020a.
URL
https://proceedings.neurips.cc/paper/2020/file/
6fec24eac8f18ed793f5eaad3dd7977c-Paper.pdf."
REFERENCES,0.48655256723716384,"Samuel Daulton, Maximilian Balandat, and Eytan Bakshy. Differentiable expected hypervolume im-
provement for parallel multi-objective bayesian optimization. arXiv preprint arXiv:2006.05078,
2020b."
REFERENCES,0.4889975550122249,"Kalyanmoy Deb, Ram Bhushan Agrawal, et al. Simulated binary crossover for continuous search
space. Complex systems, 9(2):115–148, 1995."
REFERENCES,0.49144254278728605,Under review as a conference paper at ICLR 2022
REFERENCES,0.4938875305623472,"Kalyanmoy Deb, Mayank Goyal, et al. A combined genetic adaptive search (geneas) for engineering
design. Computer Science and informatics, 26:30–45, 1996."
REFERENCES,0.4963325183374083,"Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist mul-
tiobjective genetic algorithm: Nsga-ii. IEEE transactions on evolutionary computation, 6(2):
182–197, 2002."
REFERENCES,0.49877750611246946,"Kalyanmoy Deb, Lothar Thiele, Marco Laumanns, and Eckart Zitzler. Scalable test problems for
evolutionary multiobjective optimization. In Evolutionary multiobjective optimization, pp. 105–
145. Springer, 2005."
REFERENCES,0.5012224938875306,"Michael Emmerich and Jan-willem Klinkenberg. The computation of the expected improvement in
dominated hypervolume of pareto front approximations. Rapport technique, Leiden University,
34:7–3, 2008."
REFERENCES,0.5036674816625917,"Eduardo C Garrido-Merch´an and Daniel Hern´andez-Lobato. Dealing with categorical and integer-
valued variables in bayesian optimization with gaussian processes. Neurocomputing, 380:20–35,
2020."
REFERENCES,0.5061124694376528,"Michael A Gelbart, Jasper Snoek, and Ryan P Adams. Bayesian optimization with unknown con-
straints. arXiv preprint arXiv:1403.5607, 2014."
REFERENCES,0.508557457212714,"David Ginsbourger, Rodolphe Le Riche, and Laurent Carraro. Kriging is well-suited to parallelize
optimization. In Computational intelligence in expensive optimization problems, pp. 131–162.
Springer, 2010."
REFERENCES,0.511002444987775,"Javier Gonz´alez, Zhenwen Dai, Philipp Hennig, and Neil Lawrence. Batch bayesian optimization
via local penalization. In Artiﬁcial intelligence and statistics, pp. 648–657. PMLR, 2016."
REFERENCES,0.5134474327628362,"Stewart Greenhill, Santu Rana, Sunil Gupta, Pratibha Vellanki, and Svetha Venkatesh. Bayesian
optimization for adaptive experimental design: A review. IEEE access, 8:13937–13948, 2020."
REFERENCES,0.5158924205378973,"Ryan-Rhys Grifﬁths and Jos´e Miguel Hern´andez-Lobato. Constrained bayesian optimization for
automatic chemical design. arXiv preprint arXiv:1709.05501, 2017."
REFERENCES,0.5183374083129584,"Sebastian Haan. Geobo: Python package for multi-objective bayesian optimisation and joint inver-
sion in geosciences. Journal of Open Source Software, 6(57):2690, 2021. doi: 10.21105/joss.
02690. URL https://doi.org/10.21105/joss.02690."
REFERENCES,0.5207823960880196,"Kirthevasan Kandasamy, Akshay Krishnamurthy, Jeff Schneider, and Barnab´as P´oczos. Parallelised
bayesian optimisation via thompson sampling. In International Conference on Artiﬁcial Intelli-
gence and Statistics, pp. 133–142. PMLR, 2018."
REFERENCES,0.5232273838630807,"Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R.
Collins, Jeff Schneider, Barnabas Poczos, and Eric P. Xing. Tuning hyperparameters without grad
students: Scalable and robust bayesian optimisation with dragonﬂy. Journal of Machine Learning
Research, 21(81):1–27, 2020. URL http://jmlr.org/papers/v21/18-223.html."
REFERENCES,0.5256723716381418,"Joshua Knowles. Parego: a hybrid algorithm with on-line landscape approximation for expensive
multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10(1):
50–66, 2006."
REFERENCES,0.5281173594132029,"Nicolas Knudde, Joachim van der Herten, Tom Dhaene, and Ivo Couckuyt.
GPﬂowOpt: A
Bayesian Optimization Library using TensorFlow.
arXiv preprint – arXiv:1711.03845, 2017.
URL https://arxiv.org/abs/1711.03845."
REFERENCES,0.530562347188264,"Mina Konakovic Lukovic, Yunsheng Tian, and Wojciech Matusik. Diversity-guided multi-objective
bayesian optimization with batch evaluations. Advances in Neural Information Processing Sys-
tems, 33, 2020."
REFERENCES,0.5330073349633252,"Yang Li, Yu Shen, Wentao Zhang, Yuanwei Chen, Huaijun Jiang, Mingchao Liu, Jiawei Jiang,
Jinyang Gao, Wentao Wu, Zhi Yang, and et al. Openbox: A generalized black-box optimization
service.
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery Data
Mining, Aug 2021. doi: 10.1145/3447548.3467061. URL http://dx.doi.org/10.1145/
3447548.3467061."
REFERENCES,0.5354523227383863,Under review as a conference paper at ICLR 2022
REFERENCES,0.5378973105134475,"Marius Lindauer, Katharina Eggensperger, Matthias Feurer, Andr´e Biedenkapp, Difan Deng, Car-
olin Benjamins, Ren´e Sass, and Frank Hutter. Smac3: A versatile bayesian optimization package
for hyperparameter optimization, 2021."
REFERENCES,0.5403422982885085,"Roman Marchant and Fabio Ramos. Bayesian optimisation for intelligent environmental monitoring.
pp. 2242–2249, 10 2012. ISBN 978-1-4673-1737-5. doi: 10.1109/IROS.2012.6385653."
REFERENCES,0.5427872860635696,"Ruben Martinez-Cantin, Nando Freitas, Eric Brochu, Jose Castellanos, and Arnaud Doucet.
A
bayesian exploration-exploitation approach for optimal online sensing and planning with a visu-
ally guided mobile robot. Auton. Robots, 27:93–103, 08 2009. doi: 10.1007/s10514-009-9130-2."
REFERENCES,0.5452322738386308,"Michael D McKay, Richard J Beckman, and William J Conover. Comparison of three methods for
selecting values of input variables in the analysis of output from a computer code. Technometrics,
21(2):239–245, 1979."
REFERENCES,0.5476772616136919,"Jonas Moˇckus. On bayesian methods for seeking the extremum. In Optimization techniques IFIP
technical conference, pp. 400–404. Springer, 1975."
REFERENCES,0.5501222493887531,"Luigi Nardi, Artur Souza, David Koeplinger, and Kunle Olukotun. Hypermapper: a practical design
space exploration framework. In 2019 IEEE 27th International Symposium on Modeling, Anal-
ysis, and Simulation of Computer and Telecommunication Systems (MASCOTS), pp. 425–426,
2019. doi: 10.1109/MASCOTS.2019.00053."
REFERENCES,0.5525672371638142,"Tatsuya Okabe, Yaochu Jin, Markus Olhofer, and Bernhard Sendhoff. On test functions for evolu-
tionary multi-objective optimization. In International Conference on Parallel Problem Solving
from Nature, pp. 792–802. Springer, 2004."
REFERENCES,0.5550122249388753,"Biswajit Paria, Kirthevasan Kandasamy, and Barnab´as P´oczos. A ﬂexible framework for multi-
objective bayesian optimization using random scalarizations.
In Ryan P. Adams and Vibhav
Gogate (eds.), Proceedings of The 35th Uncertainty in Artiﬁcial Intelligence Conference, vol-
ume 115 of Proceedings of Machine Learning Research, pp. 766–776. PMLR, 22–25 Jul 2020.
URL https://proceedings.mlr.press/v115/paria20a.html."
REFERENCES,0.5574572127139364,"Nery Riquelme, Christian Von L¨ucken, and Benjamin Baran. Performance metrics in multi-objective
optimization. In 2015 Latin American Computing Conference (CLEI), pp. 1–11. IEEE, 2015."
REFERENCES,0.5599022004889975,"Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. In
Advances in Neural Information Processing Systems, pp. 1583–1591, 2014."
REFERENCES,0.5623471882640587,"Adriana Schulz, Harrison Wang, Eitan Grinspun, Justin Solomon, and Wojciech Matusik. Interactive
exploration of design trade-offs. ACM Transactions on Graphics (TOG), 37(4):1–14, 2018."
REFERENCES,0.5647921760391198,"B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of the
loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016."
REFERENCES,0.5672371638141809,"Benjamin J Shields, Jason Stevens, Jun Li, Marvin Parasram, Farhan Damani, Jesus I Martinez
Alvarado, Jacob M Janey, Ryan P Adams, and Abigail G Doyle. Bayesian reaction optimization
as a tool for chemical synthesis. Nature, 590(7844):89–96, 2021."
REFERENCES,0.5696821515892421,"Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. Advances in neural information processing systems, 25, 2012."
REFERENCES,0.5721271393643031,"Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, Mr Prabhat, and Ryan Adams.
Scalable bayesian optimization using deep
neural networks. In International conference on machine learning, pp. 2171–2180. PMLR, 2015."
REFERENCES,0.5745721271393643,"Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimiza-
tion in the bandit setting: no regret and experimental design. In Proceedings of the 27th Interna-
tional Conference on International Conference on Machine Learning, pp. 1015–1022, 2010."
REFERENCES,0.5770171149144254,"William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285–294, 1933."
REFERENCES,0.5794621026894865,Under review as a conference paper at ICLR 2022
REFERENCES,0.5819070904645477,"David A Van Veldhuizen and Gary B Lamont. Multiobjective evolutionary algorithm test suites. In
Proceedings of the 1999 ACM symposium on Applied computing, pp. 351–357, 1999."
REFERENCES,0.5843520782396088,"J. Wu, W.Y. Zhang, S. Zhang, Y.N. Liu, and X.H. Meng. A matrix-based bayesian approach for
manufacturing resource allocation planning in supply chain management. International Journal
of Production Research, 51(5):1451–1463, 2013. doi: 10.1080/00207543.2012.693966. URL
https://doi.org/10.1080/00207543.2012.693966."
REFERENCES,0.58679706601467,"Qingfu Zhang, Wudong Liu, Edward Tsang, and Botond Virginas. Expensive multiobjective opti-
mization by moea/d with gaussian process model. IEEE Transactions on Evolutionary Computa-
tion, 14(3):456–474, 2009."
REFERENCES,0.589242053789731,"Yichi Zhang, Daniel W Apley, and Wei Chen. Bayesian optimization for materials design with
mixed quantitative and qualitative variables. Scientiﬁc Reports, 10(1):1–13, 2020."
REFERENCES,0.5916870415647921,"Eckart Zitzler and Lothar Thiele. Multiobjective evolutionary algorithms: a comparative case study
and the strength pareto approach. IEEE transactions on Evolutionary Computation, 3(4):257–
271, 1999."
REFERENCES,0.5941320293398533,"Eckart Zitzler, Kalyanmoy Deb, and Lothar Thiele. Comparison of multiobjective evolutionary
algorithms: Empirical results. Evolutionary computation, 8(2):173–195, 2000."
REFERENCES,0.5965770171149144,Under review as a conference paper at ICLR 2022
REFERENCES,0.5990220048899756,"A
PLATFORM DETAILS"
REFERENCES,0.6014669926650367,"A.1
ADDITIONAL FEATURES"
REFERENCES,0.6039119804400978,"Cross-platform
AutoOED is a cross-platform software that can be installed as an executable on
computers with either Windows/MacOS/Linux system. Installing from source code is also supported
for extra ﬂexibility."
REFERENCES,0.6063569682151589,"Surrogate prediction
Users may ﬁnd AutoOED useful not only in optimization but also in predic-
tion. In addition to the set of optimal solutions, our platform’s ﬁnal product is the learned prediction
models of the unknown objectives, which can be used easily from GUI to predict the objectives for
a given design from the user. The prediction provides the users with more insights into the poten-
tial outcomes of experiments. It helps them better understand the optimization problem to make
informed decisions and guide the optimization process towards their preference."
REFERENCES,0.60880195599022,"Constraints
Besides, AutoOED supports inexpensive black-box constraints on the design space
in addition to the bounds. To handle expensive black-box design constraints or objective constraints,
it is possible to learn a constraint model similar to learning the objective function, as implemented
by Spearmint (Gelbart et al., 2014)."
REFERENCES,0.6112469437652812,"B
EXPERIMENT SETUP"
REFERENCES,0.6136919315403423,"B.1
BENCHMARK PROBLEMS"
REFERENCES,0.6161369193154034,"In this section, we brieﬂy introduce the properties of each benchmark problem, including the di-
mensions of the design space X ⊂Rd and performance space f(X) ⊂Rm, and the reference
points we use for calculating the hypervolume indicator, which are shown in Table 2. We perform
20 independent test runs with 20 different random seeds for each problem on each algorithm. For
each test run of one problem, we use the same initial set of samples for every algorithm, which is
generated by Latin hypercube sampling (McKay et al., 1979) using a same random seed. To have
a fair comparison, we simply set the reference point r ∈Rm as a vector containing the maximum
value of each objective over the initial set of samples {x1, ..., xk}:"
REFERENCES,0.6185819070904646,"r = ( max
1≤i≤k f1(xi), ..., max
1≤i≤k fm(xi))."
REFERENCES,0.6210268948655256,Table 2: Basic descriptions of all the benchmark problems.
REFERENCES,0.6234718826405868,"Name
d
m
r"
REFERENCES,0.6259168704156479,"ZDT1
30
2
(0.9699, 6.0445)
ZDT2
30
2
(0.9699, 6.9957)
ZDT3
30
2
(0.9699, 6.0236)
ZDT4
10
2
(0.9699, 199.6923)
DTLZ1
6
2
(360.7570, 343.4563)
DTLZ2
6
2
(1.7435, 1,6819)
DTLZ3
6
2
(706.5260, 746.2411)
DTLZ4
6
2
(1.8111, 0.7776)
OKA1
2
2
(7.4051, 4.3608)
OKA2
3
2
(3.1315, 4.6327)
VLMOP2
6
2
(1.0, 1.0)
VLMOP3
2
3
(8.1956, 53.2348, 0.1963)"
REFERENCES,0.628361858190709,"B.2
HYPERPARAMETERS"
REFERENCES,0.6308068459657702,Here we present all the common hyperparameters that AutoOED uses in the experiments.
REFERENCES,0.6332518337408313,Under review as a conference paper at ICLR 2022
REFERENCES,0.6356968215158925,"Surrogate model
We use the same Gaussian process model as the surrogate for all experiments.
We use zero mean function and anisotropic Matern 1/2 kernel, which empirically is numerically
stable than popular Matern 5/2 kernel in our experiments. The corresponding hyperparameters are
speciﬁed in Table 3, which are suggested by TSEMO."
REFERENCES,0.6381418092909535,Table 3: GP hyperparameters.
REFERENCES,0.6405867970660146,"parameter name
value"
REFERENCES,0.6430317848410758,"initial l
(1, ..., 1) ∈Rd"
REFERENCES,0.6454767726161369,"l range
(
√"
REFERENCES,0.6479217603911981,"10−3,
√"
REFERENCES,0.6503667481662592,"103)
initial σf
1
σf range
(
√"
REFERENCES,0.6528117359413202,"10−3,
√"
REFERENCES,0.6552567237163814,"103)
initial σn
10−2"
REFERENCES,0.6577017114914425,"σn range
(e−6, 1)"
REFERENCES,0.6601466992665037,"Multi-objective evolutionary algorithm
The cheap NSGA-II solver employed in AutoOED’s
MOBO algorithms by default uses simulated binary crossover (Deb et al., 1995) and polynomial
mutation (Deb et al., 1996) for ﬁnding the Pareto front of acquisition functions. The initial popula-
tion is obtained from the best current samples determined by non-dominated sort (Deb et al., 2002).
The other hyperparameters are speciﬁed in Table 4."
REFERENCES,0.6625916870415648,Table 4: NSGA-II hyperparameters.
REFERENCES,0.6650366748166259,"parameter name
value"
REFERENCES,0.6674816625916871,"population size
100
number of generations
200
crossover ηc
15
mutation ηm
20"
REFERENCES,0.6699266503667481,"B.3
REAL-WORLD EXPERIMENT SETUP"
REFERENCES,0.6723716381418093,"In our real-world experiment setup, overall, a PID controller is employed to regulate the temperature
of the heater block with proportional, integral, and differential constants. To ﬁnd the optimal set of
constants a number of heating cycles are done asynchronously with the controller using AutoOED.
The experiment is comprised of setting the PID constants, then heating the block up to a set temper-
ature, and keeping them at the set duration for 2 minutes. During this time the response time and
overshoot are measured. After the heating cycle, the heater is then cooled back down to a starting
temperature to prepare for another test with new PID constants."
REFERENCES,0.6748166259168704,"Speciﬁcally, the experimental setup is comprised of 3 heaters with identical dimensions and charac-
teristics. Each heater is comprised of a heater block, heating element, temperature sensor, solid-state
relay, power supply, and a controller, shown in Figure 7. To run an experiment, AutoOED sends the
PID constants to a controller that is free. Next, the PID controller becomes active and starts regu-
lating the temperature of the heater. The temperature sensor measures the temperature of the heater
block. Depending on the constants of the PID controller and the temperautre of the heater block, the
controller turns the heating element on or off via the solid-state relay. After a period of 2 minutes
where the PID controller is active, the controller stops actively regulating the temperature of the
heater allowing the heater to cool. Next, the controller starts to monitor the cooling of the heater via
the temperature sensor. It monitors it until the heater block cools to a temperature below a threshold.
The amount of time it takes to cool the heater block depends on the temperature that the block was
heated to during the active period. Once it sufﬁciently cools, the controller sends the calculated
overshoot and response time to AutoOED and notiﬁes that it is ready to run another experiment."
REFERENCES,0.6772616136919315,Under review as a conference paper at ICLR 2022
REFERENCES,0.6797066014669927,"Temp.
Sensor"
REFERENCES,0.6821515892420538,"Heating 
Element"
REFERENCES,0.684596577017115,Heater 1
REFERENCES,0.687041564792176,"Solid-State 
Relay"
REFERENCES,0.6894865525672371,AutoOED
REFERENCES,0.6919315403422983,"PID 
Controller"
REFERENCES,0.6943765281173594,Heater 2
REFERENCES,0.6968215158924206,Heater 3
REFERENCES,0.6992665036674817,"Solid-State 
Relay"
REFERENCES,0.7017114914425427,"Solid-State 
Relay"
REFERENCES,0.7041564792176039,"PID 
Controller"
REFERENCES,0.706601466992665,"PID 
Controller"
REFERENCES,0.7090464547677262,Figure 7: A schematic of the setup used for the real-world experiment.
REFERENCES,0.7114914425427873,"C
ADDITIONAL COMPARISONS"
REFERENCES,0.7139364303178484,"BoTorch
GPflowOpt
Dragonfly Hypermapper
OpenBox
AutoOED 1 2 3 4 5 6"
REFERENCES,0.7163814180929096,"Figure 8: Performance rank of platforms on the 12 benchmark problems (lower is better). The box
extends from the lower to the upper quartile values, with a solid line at the median and a dashed line
at the mean. The whiskers that extend the box show the range of the data."
REFERENCES,0.7188264058679706,"We conduct ranked and paired comparisons between AutoOED and all the baseline platforms based
on the 12 benchmark problems, as shown in Figure 8 and Figure 9. The performance rank compar-
ison in Figure 8 suggests that BoTorch, OpenBox and AutoOED outperform other platforms by a
great margin overall. While BoTorch and OpenBox share a better median rank, AutoOED appears
to be the stablest platform that consistently ranks between 1 and 3 on all problems and has a higher
lower-bound performance than BoTorch and OpenBox. Figure 9 also suggest that AutoOED has
a competitive performance to BoTorch and OpenBox, but outperforms other baselines on a wider
range of benchmarks."
REFERENCES,0.7212713936430318,Under review as a conference paper at ICLR 2022
REFERENCES,0.7237163814180929,BoTorch
REFERENCES,0.726161369193154,GPflowOpt
REFERENCES,0.7286063569682152,Dragonfly
REFERENCES,0.7310513447432763,Hypermapper
REFERENCES,0.7334963325183375,OpenBox
REFERENCES,0.7359413202933985,AutoOED
REFERENCES,0.7383863080684596,Platform B
REFERENCES,0.7408312958435208,BoTorch
REFERENCES,0.7432762836185819,GPflowOpt
REFERENCES,0.7457212713936431,Dragonfly
REFERENCES,0.7481662591687042,Hypermapper
REFERENCES,0.7506112469437652,OpenBox
REFERENCES,0.7530562347188264,AutoOED
REFERENCES,0.7555012224938875,Platform A
REFERENCES,0.7579462102689487,"0
10
6
10
0
0"
REFERENCES,0.7603911980440098,"-10
0
2
6
-8
-10"
REFERENCES,0.7628361858190709,"-6
-2
0
8
-6
-8"
REFERENCES,0.7652811735941321,"-10
-6
-8
0
-12
-12"
REFERENCES,0.7677261613691931,"0
8
6
12
0
0"
REFERENCES,0.7701711491442543,"0
10
8
12
0
0
10 5 0 5 10"
REFERENCES,0.7726161369193154,Number of benchmarks A outperform B
REFERENCES,0.7750611246943765,"Figure 9: Performance comparison between each pair of platforms on the 12 benchmark problems.
Each value in the matrix shows the number of benchmarks that platform A (associated with the row)
outperforms platform B (associated with the column), the higher the better."
REFERENCES,0.7775061124694377,"0
5
10
15
20 −0.2 0.0 0.2 0.4"
REFERENCES,0.7799511002444988,Log hypervolume difference zdt1
REFERENCES,0.78239608801956,"0
5
10
15
20 0.40 0.45 0.50 0.55 0.60 zdt2"
REFERENCES,0.784841075794621,"0
5
10
15
20
25 0.2 0.3 0.4 0.5 zdt3"
REFERENCES,0.7872860635696821,"0
5
10
15
20 1.80 1.85 1.90 1.95 2.00 2.05 2.10 zdt4"
REFERENCES,0.7897310513447433,"0
5
10
15
20 3.6 3.8 4.0 4.2"
REFERENCES,0.7921760391198044,Log hypervolume difference dtlz1
REFERENCES,0.7946210268948656,"0
5
10
15
20 −0.8 −0.6 −0.4 −0.2 dtlz2"
REFERENCES,0.7970660146699267,"0
5
10
15
20 4.2 4.4 4.6 4.8 5.0 dtlz3"
REFERENCES,0.7995110024449877,"0
5
10
15
20 −1.1 −1.0 −0.9 −0.8 dtlz4"
REFERENCES,0.8019559902200489,"0
5
10
15
20
Time (s) 0.7 0.8 0.9 1.0 1.1"
REFERENCES,0.80440097799511,Log hypervolume difference oka1
REFERENCES,0.8068459657701712,"0
5
10
15
20
Time (s) 0.8 0.9 1.0 1.1 1.2 oka2"
REFERENCES,0.8092909535452323,"0
5
10
15
20
Time (s) −2.25 −2.00 −1.75 −1.50 −1.25 −1.00 −0.75"
REFERENCES,0.8117359413202934,vlmop2
REFERENCES,0.8141809290953546,"0
5
10
15
20
Time (s) 0.00 0.25 0.50 0.75 1.00 1.25 1.50"
REFERENCES,0.8166259168704156,vlmop3
REFERENCES,0.8190709046454768,"Sync
Async"
REFERENCES,0.8215158924205379,"Figure 10: Performance comparison between synchronous and naive asynchronous MOBO algo-
rithms."
REFERENCES,0.823960880195599,Under review as a conference paper at ICLR 2022
REFERENCES,0.8264058679706602,"D
ABLATION STUDIES"
REFERENCES,0.8288508557457213,"D.1
SYNCHRONOUS AND ASYNCHRONOUS MOBO"
REFERENCES,0.8312958435207825,"Following the experiment settings in Section 6, we addtionally compare the performance of syn-
chornous and asynchronous MOBO. As shown in Figure 10, they achieve similar hypervolumes
whereas asynchronous MOBO spends less than half of the time of its synchronous counterpart."
REFERENCES,0.8337408312958435,"D.2
BATCH SIZE"
REFERENCES,0.8361858190709046,"20
40
60
80
100
−0.4 −0.2 0.0 0.2 0.4"
REFERENCES,0.8386308068459658,Log hypervolume difference zdt1
REFERENCES,0.8410757946210269,"20
40
60
80
100
0.40 0.45 0.50 0.55 0.60 zdt2"
REFERENCES,0.843520782396088,"20
40
60
80
100 0.1 0.2 0.3 0.4 0.5 zdt3"
REFERENCES,0.8459657701711492,"20
40
60
80
100 1.7 1.8 1.9 2.0 2.1 zdt4"
REFERENCES,0.8484107579462102,"20
40
60
80
100
3.4 3.6 3.8 4.0 4.2 4.4"
REFERENCES,0.8508557457212714,Log hypervolume difference dtlz1
REFERENCES,0.8533007334963325,"20
40
60
80
100 −0.8 −0.6 −0.4 −0.2 dtlz2"
REFERENCES,0.8557457212713936,"20
40
60
80
100 4.2 4.4 4.6 4.8 5.0 dtlz3"
REFERENCES,0.8581907090464548,"20
40
60
80
100 −1.2 −1.1 −1.0 −0.9 −0.8 −0.7 dtlz4"
REFERENCES,0.8606356968215159,"20
40
60
80
100
Number of samples 0.7 0.8 0.9 1.0 1.1"
REFERENCES,0.863080684596577,Log hypervolume difference oka1
REFERENCES,0.8655256723716381,"20
40
60
80
100
Number of samples 0.7 0.8 0.9 1.0 1.1 1.2 oka2"
REFERENCES,0.8679706601466992,"20
40
60
80
100
Number of samples −2.0 −1.5 −1.0"
REFERENCES,0.8704156479217604,vlmop2
REFERENCES,0.8728606356968215,"20
40
60
80
100
Number of samples 0.0 0.5 1.0 1.5"
REFERENCES,0.8753056234718827,vlmop3
REFERENCES,0.8777506112469438,"Async
Async KB
Async LP
Async BP"
REFERENCES,0.8801955990220048,"Figure 11: Performance comparison between variants of asynchoronous MOBO algorithms with a
batch size of 4."
REFERENCES,0.882640586797066,"Ablation studies are also conducted on the batch size in asynchronous MOBO. For this category
of experiments, we repeat our practice in Section 6.1 while changing the batch size to 4 and 16,
respectively. The results are demonstrated in Figure 11 and 12. Our proposed BP strategy maintains
its relative lead in VLMOP3 and performs comparably with other variants on the rest of the test
problems."
REFERENCES,0.8850855745721271,"D.3
ACQUISITION FUNCTION"
REFERENCES,0.8875305623471883,"Lastly, we evaluate the asynchronous MOBO variants using the EI acquisition function. Although
the change in acquisition function has a clear inﬂuence on hypervolume growth, the proposed BP
variant still performs favorably in problems such as ZDT2, DTLZ2, and VLMOP2. The performance
of BP on the other problems remain comparable to the alternative strategies."
REFERENCES,0.8899755501222494,Under review as a conference paper at ICLR 2022
REFERENCES,0.8924205378973105,"20
40
60
80
100 −0.2 0.0 0.2 0.4"
REFERENCES,0.8948655256723717,Log hypervolume difference zdt1
REFERENCES,0.8973105134474327,"20
40
60
80
100 0.40 0.45 0.50 0.55 0.60 zdt2"
REFERENCES,0.8997555012224939,"20
40
60
80
100 0.2 0.3 0.4 0.5 zdt3"
REFERENCES,0.902200488997555,"20
40
60
80
100 1.8 1.9 2.0 2.1 zdt4"
REFERENCES,0.9046454767726161,"20
40
60
80
100 3.6 3.8 4.0 4.2 4.4"
REFERENCES,0.9070904645476773,Log hypervolume difference dtlz1
REFERENCES,0.9095354523227384,"20
40
60
80
100 −0.8 −0.7 −0.6 −0.5 −0.4 −0.3 −0.2 dtlz2"
REFERENCES,0.9119804400977995,"20
40
60
80
100 4.2 4.4 4.6 4.8 5.0 dtlz3"
REFERENCES,0.9144254278728606,"20
40
60
80
100 −1.0 −0.9 −0.8 dtlz4"
REFERENCES,0.9168704156479217,"20
40
60
80
100
Number of samples 0.7 0.8 0.9 1.0 1.1"
REFERENCES,0.9193154034229829,Log hypervolume difference oka1
REFERENCES,0.921760391198044,"20
40
60
80
100
Number of samples 0.6 0.7 0.8 0.9 1.0 1.1 1.2 oka2"
REFERENCES,0.9242053789731052,"20
40
60
80
100
Number of samples −2.25 −2.00 −1.75 −1.50 −1.25 −1.00 −0.75"
REFERENCES,0.9266503667481663,vlmop2
REFERENCES,0.9290953545232273,"20
40
60
80
100
Number of samples −0.5 0.0 0.5 1.0 1.5"
REFERENCES,0.9315403422982885,vlmop3
REFERENCES,0.9339853300733496,"Async
Async KB
Async LP
Async BP"
REFERENCES,0.9364303178484108,"Figure 12: Performance comparison between variants of asynchoronous MOBO algorithms with a
batch size of 16."
REFERENCES,0.9388753056234719,"20
40
60
80
100 −0.6 −0.4 −0.2 0.0 0.2 0.4"
REFERENCES,0.941320293398533,Log hypervolume difference zdt1
REFERENCES,0.9437652811735942,"20
40
60
80
100 −0.2 0.0 0.2 0.4 0.6 zdt2"
REFERENCES,0.9462102689486552,"20
40
60
80
100 0.0 0.2 0.4 zdt3"
REFERENCES,0.9486552567237164,"20
40
60
80
100 1.80 1.85 1.90 1.95 2.00 2.05 2.10 zdt4"
REFERENCES,0.9511002444987775,"20
40
60
80
100 3.4 3.6 3.8 4.0 4.2 4.4"
REFERENCES,0.9535452322738386,Log hypervolume difference dtlz1
REFERENCES,0.9559902200488998,"20
40
60
80
100
−0.375"
REFERENCES,0.9584352078239609,−0.350
REFERENCES,0.960880195599022,−0.325
REFERENCES,0.9633251833740831,−0.300
REFERENCES,0.9657701711491442,−0.275
REFERENCES,0.9682151589242054,−0.250
REFERENCES,0.9706601466992665,−0.225 dtlz2
REFERENCES,0.9731051344743277,"20
40
60
80
100
4.0 4.2 4.4 4.6 4.8 5.0 dtlz3"
REFERENCES,0.9755501222493888,"20
40
60
80
100 −1.0 −0.9 −0.8 dtlz4"
REFERENCES,0.9779951100244498,"20
40
60
80
100
Number of samples 0.8 0.9 1.0 1.1"
REFERENCES,0.980440097799511,Log hypervolume difference oka1
REFERENCES,0.9828850855745721,"20
40
60
80
100
Number of samples 0.6 0.8 1.0 1.2 oka2"
REFERENCES,0.9853300733496333,"20
40
60
80
100
Number of samples −1.1 −1.0 −0.9 −0.8 −0.7"
REFERENCES,0.9877750611246944,vlmop2
REFERENCES,0.9902200488997555,"20
40
60
80
100
Number of samples −1.0 −0.5 0.0 0.5 1.0 1.5"
REFERENCES,0.9926650366748166,vlmop3
REFERENCES,0.9951100244498777,"Async
Async KB
Async LP
Async BP"
REFERENCES,0.9975550122249389,"Figure 13: Performance comparison between variants of asynchoronous MOBO algorithms with EI
as acquisition function."
