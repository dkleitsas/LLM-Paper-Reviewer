Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002352941176470588,"Achieving structured, data-free sparsity of deep neural networks (DNNs) remains
an open area of research. In this work, we address the challenge of pruning
filters without access to the original training set or loss function. We propose the
discriminative filters hypothesis, that well-trained models possess discriminative
filters, and any non discriminative filters can be pruned without impacting the
predictive performance of the classifier. Based on this hypothesis, we propose a
new paradigm for pruning neural networks: distributional pruning, wherein we only
require access to the distributions that generated the original datasets. Our approach
to solving the problem of formalising and quantifying the discriminating ability
of filters is through the total variation (TV) distance between the class-conditional
distributions of the filter outputs. We present empirical results that, using this
definition of discriminability, support our hypothesis on a variety of datasets and
architectures. Next, we define the LDIFF score, a heuristic to quantify the extent to
which a layer possesses a mixture of discriminative and non-discriminative filters.
We empirically demonstrate that the LDIFF score is indicative of the performance
of random pruning for a given layer, and thereby indicates the extent to which a
layer may be pruned. Our main contribution is a novel one-shot pruning algorithm,
called TVSPrune, that identifies non-discriminative filters for pruning. We extend
this algorithm to IterTVSPrune, wherein we iteratively apply TVSPrune, thereby
enabling us to achieve greater sparsity. Last, we demonstrate the efficacy of the
TVSPrune on a variety of datasets, and show that in some cases, we can prune up to
60% of parameters with only a 2% loss of accuracy without any fine-tuning of the
model, beating the nearest baseline by almost 10%. Our code is available here1."
INTRODUCTION,0.004705882352941176,"1
INTRODUCTION"
INTRODUCTION,0.007058823529411765,"Deep neural networks are, in general, highly overparameterized, leading to significant implementation
challenges in terms of reducing inference time, power consumption, and memory footprint. This
is especially crucial for deployment on real world, resource constrained devices (Molchanov et al.,
2019b; Prakash et al., 2019). A variety of solutions have been proposed to solve this problem, which
can broadly be grouped into quantization, sparsification or pruning, knowledge distillation, and neural
architecture search (NAS) (Hoefler et al., 2021)."
INTRODUCTION,0.009411764705882352,"Pruning can be further divided into unstructured pruning, wherein individual parameters are set to zero,
or structured pruning, wherein entire filters or channels are removed from the architecture (Hoefler
et al., 2021). Structured pruning yields immediate improvements in inference time, memory footprint,
and power consumption, without requiring any specialized software frameworks. Unstructured
pruning, on the other hand, typically yields models that are significantly more sparse than those"
INTRODUCTION,0.011764705882352941,"∗Work done at the Department of Computer Science and Automation, Indian Institute of Science.
1Link to github: https://github.com/chaimurti/TVSPrune"
INTRODUCTION,0.01411764705882353,Published as a conference paper at ICLR 2023
INTRODUCTION,0.01647058823529412,"obtained with structured pruning, but do not provide the same improvements in inference time without
specialized sparse linear algebra implementations (Hoefler et al., 2021; Blalock et al., 2020)."
INTRODUCTION,0.018823529411764704,"In this work, we consider the problem of pruning CNNs without access to the training set or loss
function. Data-free pruning is an important problem due to concerns such as privacy and security Yin
et al. (2020), as well as the cost of retraining models (Tanaka et al., 2020; Hoefler et al., 2021). We
offer a new perspective to this problem, which we call distributional pruning, wherein we have no
access to the training data or the loss function, but have access to data distribution, either through it’s
moments, or through additional samples separate from the training set."
INTRODUCTION,0.021176470588235293,"To facilitate distributional pruning, two crucial questions need to be answered. First, what makes a
filter valuable to the classification performance of the model?, and second, how do we characterize
which layers can be effectively sparsified? To answer these questions, we first identify discriminative
filters, which are filters with class-conditional outputs that are well-separated in terms of the total
variation. We propose the discriminative filters hypothesis, which states that well-trained models
possess a mix of discriminative and non discriminative filters, and that discriminative filters are
useful for generalization of the classifier. Based on this hypothesis, discriminative filters are useful
for classification purposes whereas non-discriminative filters are not, thus allowing us to prune the
latter. Furthermore, layers that possess a mix of discriminative and non-discriminative filters can be
effectively pruned, thereby providing a method to identify difficult-to-prune layers. We formally state
our contributions below."
INTRODUCTION,0.023529411764705882,"1. We begin by proposing a quantitative measure of discriminative ability of filters in terms of
the TV distance between their class-conditional outputs. Specifically, we say a filter is TV-
separable if the pairwise minimum TV distance between class-conditional distributions of
the outputs is larger than a given threshold. If the class conditional distributions are gaussian
- a common assumption as noted in Wong et al. (2021); Wen et al. (2016) - we can compute
the Hellinger distance based lower bound to estimate whether filters are TV-separable using
easily computed class-conditional moments. We describe this in section 4."
INTRODUCTION,0.02588235294117647,"2. We produce the empirical observation that the classwise outputs of at least some filters
present in CNNs that generalize well are TV-separable, and that untrained models, or models
that generalize poorly do not possess discriminative filters; these are presented in section 7.
Based on these observations, in section 2, we propose the discriminative filters hypothesis,
which states that well-trained convolutional neural networks possess a mix of discriminative
and non-discriminative filters, and discriminative filters are useful for classification whereas
the latter are not. We use this hypothesis to motivate a distributional approach to pruning."
INTRODUCTION,0.02823529411764706,"3. Based on the discriminative filters hypothesis, we aim to use TV separation to identify which
filters to prune in a model. We assume the class-conditional distributions are Gaussian, and,
using the Hellinger lower bounds discussed in section 4, we compute lower bounds on the
TV-separation for each filter. We identify important filters (those that cannot be pruned)
as those filters with the Hellinger lower bound of the TV-separation that are greater than a
separation threshold; those filters that are not discriminative with respect to the separation
threshold can be pruned."
INTRODUCTION,0.03058823529411765,"4. As noted in Hoefler et al. (2021); Liebenwein et al. (2019), some layers are more difficult to
prune than others. We address the problem of identifying which layers can be effectively
pruned using TV-separability. Based on the discriminative filters hypothesis, a layer can be
effectively pruned if it possesses a mixture of discriminative and non-discriminative filters.
Thus, in section 5, we propose an informative heuristic, which we call the LDIFFscore,
that quantifies the extent to which a layer possesses a mixture of discriminative and non-
discriminative filters. We empirically validate this heuristic in section 7."
INTRODUCTION,0.03294117647058824,"5. We use TV-separability and LDIFF scores to develop TVSPRUNE, a layer-wise, threshold
based method for structured pruning, requiring no fine tuning and only the class-conditional
moments of the outputs of each filter. We also extend this algorithm to an iterative variant,
ITERTVSPRUNE, which enables superior sparsification of the model. We formally state
these algorithms in section 6, in Algorithms 1 and 2. We show that on the CIFAR-10
dataset, our method achieves over 40% sparsification with minimal reduction in accuracy on
VGG models without any fine tuning; furthermore, our method outperforms contemporary
methods such as (Sui et al., 2021; Molchanov et al., 2019b) in this regime."
INTRODUCTION,0.03529411764705882,Published as a conference paper at ICLR 2023
INTRODUCTION,0.03764705882352941,"Our paper is organized as follows. In Section 2, we outline our problem statement. In Section 3, we
detail the notation used in the paper, and review Total Variation distances and the associated Hellinger
distance bounds, and in Section 4, we describe how we utilize the TV-distance to measure the discrim-
inative capacity of filters. In Section 5, we propose the LDIFF score for quantifying the difficulty in
pruning a given layer, and in Section 6, we present the TVSPRUNE and ITERTVSPRUNEalgorithms.
In Section 7, we detail our experimental results. We produce a detailed related work section in
Appendix A. We also provide additional discussions in B, variants of the LDIFF score and the
TVSPRUNE algorithms in sections C and D, and a variety of additional empirical results in section
E."
LIT REVIEW,0.04,"2
FILTER PRUNING WITHOUT TRAINING DATA AND FINE TUNING"
LIT REVIEW,0.042352941176470586,"Structured pruning can be thought of as two specific optimization problems: finding the most accurate
model that satisfies a sparsity model, and finding the sparsest network that satisfies an accuracy
constraint. We define the latter problem below."
LIT REVIEW,0.04470588235294118,"W∗= arg min
W
{∥W∥0 | f(W) ≤t, t > 0} .
(P)"
LIT REVIEW,0.047058823529411764,"Our goal is to solve equation P using structured pruning techniques; that is, we aim to remove entire
filters at each pruning iteration. As mentioned previously, typical methods to solve this problem
require extensive retraining to overcome the losses in accuracy. In this work, we consider the scenario
wherein neither the training data nor the loss function are available during the pruning process."
LIT REVIEW,0.04941176470588235,"Motivated by empirical results, we see that models that generalize well tend to have filters with
well-separated output features (in the distributional sense), whereas models that do not generalize well
possess filters with output features that are not as well separated. We refer the reader to Section 7.1
for these results. This prompts us to formulate the following hypothesis."
LIT REVIEW,0.05176470588235294,"Hypothesis 1 (Discriminative Filters Hypothesis). “Models that generalize well possess a rich
mixture of filters that discriminate between classes and those that don’t. Furthermore, we hypothesize
that for generalization purposes, it is sufficient to retain the filters that discriminate between classes
well, in those layers that have a large variation in discriminative ability.”"
LIT REVIEW,0.05411764705882353,"Note that we also observe that some layers have filters with output features that cannot discriminate
well, and yet cannot be pruned."
LIT REVIEW,0.05647058823529412,"3
PRELIMINARIES AND BACKGROUND"
LIT REVIEW,0.058823529411764705,"3.1
NOTATION"
LIT REVIEW,0.0611764705882353,"Let [n] = {1, · · · , n} ⊂N. We the define parameters of a neural network with L as W =
{W1, · · · , WL}, where each Wi = {Wi,j}Nl
j=1 is a collection of tensors. For a convolutional neural"
LIT REVIEW,0.06352941176470588,"network (CNN), each Wl ∈RNl
in×Nl
out×Kl×Kl . In this formulation, the ith filter of layer l is a
tensor f l
i ∈RNin×K×K, and the jth kernel in this filter is a matrix f l
i,j ∈RK×K. Note that given a
model, we can use the pair (l, j) to identify a filter - l denotes the layer, and j the filter index."
LIT REVIEW,0.06588235294117648,"We vectorize this model as follows. First, suppose the vectorized input to layer l + 1 ∈[L] is
ϕl(X) ∈Rm2Nin, where Nin is the number of channels and m is the dimension of the input (say, the
number of pixels in each dimension of an image). Note that we ignore the superscript indicating the
layer for the sake of brevity. Suppose each filter in layer l + 1 has kernel size K × K, that is, for each
j ∈[J], the we have the vectorized W l
j ∈RK2Nin. Next, we assume that there are P patches, with
ϕl
p(X) being the pth patch in the input. In the sequel, we drop the layer superscript for convenience.
In this work, we assume we draw samples (X, u), where X is the datum and u is the class label,
from a distribution D. We write Dα is the class conditional distribution of class α. Here X ∈RN for
some integer N is the input, and u ∈[Nclasses] ⊂N is the integer class."
LIT REVIEW,0.06823529411764706,Published as a conference paper at ICLR 2023
LIT REVIEW,0.07058823529411765,"3.2
REVIEW OF TOTAL VARIATION DISTANCE"
LIT REVIEW,0.07294117647058823,"In this section, we formally define the Total Variation and Hellinger distances. We define this formally
in the sequel. The results that follow are discussed in, say, Kraft (1955)."
LIT REVIEW,0.07529411764705882,"Definition 1. Let Q1 and Q1 be two probability measures supported on Rd. We define the Total
Variation Distance TV as"
LIT REVIEW,0.07764705882352942,"TV (Q1, Q2) = sup
A⊂Rd |Q1(A) −Q2(A)|"
LIT REVIEW,0.08,"Unfortunately, no closed form expression exists for the Total Variation distance, even when Q1, Q2
are Gaussian. This motivates us to use the Hellinger distance, which we define below."
LIT REVIEW,0.08235294117647059,"Definition 2. Let Q1, Q2 be two probability measures supported on Rd, and let q1 and q2 be the
corresponding densities. We define the squared Hellinger distance as"
LIT REVIEW,0.08470588235294117,"HELLD2(Q1, Q2) = 1 2 Z Rd p"
LIT REVIEW,0.08705882352941176,"q1(x) −
p"
LIT REVIEW,0.08941176470588236,"q2(x)
2
dx"
LIT REVIEW,0.09176470588235294,"Crucially, the Hellinger distance provides us upper and lower bounds on the TV-Distance."
LIT REVIEW,0.09411764705882353,"HELLD2(Q1, Q2) ≤TV(Q1, Q2) ≤
√"
LIT REVIEW,0.09647058823529411,"2 HELLD(Q1, Q2).
(1)"
LIT REVIEW,0.0988235294117647,"If we have Q1 ∼N(µ1, σ2
1I) and Q2 ∼N(µ2, σ2
2I) are Gaussian, we get"
LIT REVIEW,0.1011764705882353,"HELLD2 (Q1, Q2) = 1 −
 2σ1σ2"
LIT REVIEW,0.10352941176470588,"σ2
1 + σ2
2  d 2
e−∆"
LIT REVIEW,0.10588235294117647,"4 , ∆= ∥µ1 −µ2∥2"
LIT REVIEW,0.10823529411764705,"σ2
1 + σ2
2
(2)"
IMPLEMENTATION/METHODS,0.11058823529411765,"4
MEASURING DISCRIMINATIVE ABILITY OF FILTERS"
IMPLEMENTATION/METHODS,0.11294117647058824,"In this work, we take a distributional view of the discriminative ability of filters. Previously, we have
described the TV-distance, and the Hellinger lower bound that can be efficiently estimated. In this
section, we apply the TV-distance to arriving at a means to quantify the discriminative ability of
individual filters by using the class-conditional distributions of the outputs. We begin by recasting the
dot products used in the convolution operations with vectorized filters and feature maps."
IMPLEMENTATION/METHODS,0.11529411764705882,"Define yj,p(X) to be the dot product of the jth filter and the pth patch of ϕl−1(X); that is"
IMPLEMENTATION/METHODS,0.11764705882352941,"yl,j,p(X) = ⟨ϕl−1
p
(X), W l
j⟩."
IMPLEMENTATION/METHODS,0.12,"Following from this, we can write"
IMPLEMENTATION/METHODS,0.1223529411764706,"Yl,j(X) = ⟨Φl−1(X), W l
j⟩,
(3)"
IMPLEMENTATION/METHODS,0.12470588235294118,"where Φl−1(X) = [ϕl−1
1
(X), · · · , ϕl−1
P
(X)] We define the Class Conditional Means and Variances
for Y (X), the output of a given filter as follows."
IMPLEMENTATION/METHODS,0.12705882352941175,"¯Y α = E(X,u)∼Dα [Y (X)] and σ2
l,j,α = E(X,u)∼Dα 
∥Y (X) −¯Y α∥2
(4)"
IMPLEMENTATION/METHODS,0.12941176470588237,"Assumption 1. We assume that the class conditional distributions of each filter is Gaussian, that is
Yl,j(X|α) ∼N( ¯Y α
l,j, σ2
l,j,α)"
IMPLEMENTATION/METHODS,0.13176470588235295,"This assumption enables us to use the closed form expressions described in equation 2. While
this assumption is a strong one, it is well motivated in the literature. The assumption that class
conditional distributions are gaussian is used in a variety of settings, including Sun et al. (2020);
Lee et al. (2020); Seetharaman et al. (2019). Our goal is to quantify the discriminative ability of the
outputs of the individual filters, and to do so, we use the TV-Distance between the class conditional
distributions. Assumption 1 enables us to estimate lower bounds on the TV distance between any two
class-conditional distributions of a filter’s output easily, using only samples from that distribution.
With that in mind, we define the Minumum TV Separation (MinTVS) between classes of a given
filter."
IMPLEMENTATION/METHODS,0.13411764705882354,Published as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.13647058823529412,"Definition 3. Suppose Asssumption 1 holds. Let Dα
l,j be the class-conditional distribution of Yl,j(X)
where (X, u) ∼Dα. For each layer l and filter j, and each pair α, β, we define the Minimum TV
Separation of filter j in layer l as"
IMPLEMENTATION/METHODS,0.1388235294117647,"MinTVS(l, j) = min
α,β TV

Dα
l,j, Dβ
l,j

≥min
α,β HELLD2 
Dα
l,j, Dβ
l,j

.
(5)"
IMPLEMENTATION/METHODS,0.1411764705882353,"Furthermore, since the harmonic mean in equation 2 is upper bounded by 1, we have"
IMPLEMENTATION/METHODS,0.14352941176470588,"MinTVS (l, j) ≥1 −exp

−min
α,β ∆α,β
l,j /4

, ∆α,β
l,j = ∥¯Y α
l,j −¯Y β
l,j∥2 
σ2
l,j,α + σ2
l,j,β
−1"
IMPLEMENTATION/METHODS,0.14588235294117646,"thus allowing us to use ∆α,β
l,j as a surrogate for MinTVS(l, j) during experimentation. Using this
quantity, we define TV-Separability with respect to some η > 0, which we call the separation
threshold.
Definition 4. For each layer l and filter j, and separation threshold η > 0 we say the filter is η-TV
Separable, or TVSEP(η) if
MinTVS(l, j) ≥η."
IMPLEMENTATION/METHODS,0.14823529411764705,"Since we cannot directly compute the pairwise minimum of the TV-separations between the class-
conditional distributions of a filter’s output, we instead rely on the lower bound proposed above. The
value of η that indicates a “well separated” filter will vary from dataset to dataset, or architecture to
architecture; for instance, for the VGG models trained on CIFAR10 used in our experiments, we find
η = 0.05 is useful."
IMPLEMENTATION/METHODS,0.15058823529411763,"5
IDENTIFYING DIFFICULT-TO-PRUNE LAYERS AND THE LDIFF SCORE"
IMPLEMENTATION/METHODS,0.15294117647058825,"As noted in Hoefler et al. (2021); Liebenwein et al. (2019), in a given model, some layers can be
effectively pruned, and others cannot. In this section, we investigate the problem of identifying which
layers of a given neural network can be pruned effectively, through the lens of the discriminative filters
hypothesis. Specifically, we argue that discriminative filters contribute to generalization performance
whereas non-discriminative filters do not, and argue that we can effectively prune layers containing
a mix of discriminative and non discriminative filters. We desire an informative heuristic that can
(a) indicate the extent a layer can be randomly pruned, thus informing pruning ratios for random
pruning; and (b) indicate whether or not a layer should be pruned at all. Note that we leverage the
latter purpose in our proposed algorithms in subsequent sections."
IMPLEMENTATION/METHODS,0.15529411764705883,"We define LDIFF, a heuristic score assigned to each layer based on the TV separations of the
constituent filters to the aggregate TV-separation and a given separation threshold η. Our goal is
to capture the mix of discriminative and non-discriminative filters in a single heuristic indicator.
Thus, given a separation threshold η, suppose the fraction of discriminative filters is τ(η). We need
a function that penalizes τ(η) close to either 1 or 0 (as having no discriminative filters in a layer
indicates difficulty in pruning). With these requirements in mind, we define"
IMPLEMENTATION/METHODS,0.15764705882352942,"LDIFF(l, η) = 4τ(η)(1 −τ(η))
(6)"
IMPLEMENTATION/METHODS,0.16,"If this quantity is close to 1, then that layer has a mix of discriminative and non-discriminative filters.
If not , then there is either a majority of discriminative filters, or non-discriminative filters; in either
case, pruning that layer would be challenging. Thus, this heuristic can also be used to inform random
pruning ratios. For instance, we may choose γLDIFF(l, η), for suitable γ ∈(0, 1), to be the fraction of
filters to be randomly pruned. Further refinements of this heuristic include layerwise thresholds, and
using statistics of the MinTVS scores to determine discriminability. We refer readers to Section C
for further discussion on such refinements, as well as empirical support for the use of LDIFF scores
for indicating pruneability of layers."
IMPLEMENTATION/METHODS,0.1623529411764706,"6
DISTRIBUTIONAL APPROACHES TO STRUCTURED PRUNING"
IMPLEMENTATION/METHODS,0.16470588235294117,"In this section, we discuss our solution to the problem of structured pruning of neural networks
without access to the loss function or the original data, and only with access to the distributions of the"
IMPLEMENTATION/METHODS,0.16705882352941176,Published as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.16941176470588235,"outputs of the individual filters and layers. Our proposed algorithm has three steps. First, for a given
separation threshold η, we estimate the η-TV-separability of the filter outputs; second, we decide
which layers we can prune; third, in those layers which we can prune from, we remove all filters with
TV-separability less than η."
IMPLEMENTATION/METHODS,0.17176470588235293,Algorithm 1: TVSPRUNE
IMPLEMENTATION/METHODS,0.17411764705882352,"Input: Dataset ˆD = {Xi, ui}M
i=1, Pretrained
CNN with parameters
W = (W1, · · · , WL),TVSEP threshold η,
layer difficulty threshold ν
Compute ¯Y l,α
j
, σ2
l,j,α for all l, j, and α .
Compute ∆α,β
l,j for each l, j, α, β.
for l ∈[L] do"
IMPLEMENTATION/METHODS,0.17647058823529413,"Compute MinTVS(l, j) for all l, j
Compute LDIFF(l)
if LDIFF(l) < ν ∨LDIFF(l) = 0 then"
IMPLEMENTATION/METHODS,0.17882352941176471,"for j ∈[N l
out] do
if MinTVS(l, j) < η then"
IMPLEMENTATION/METHODS,0.1811764705882353,"W l
j ←0"
IMPLEMENTATION/METHODS,0.18352941176470589,"Output: ˆ
W = ( ˆ
W1, · · · , ˆ
WL), where
supp( ˆ
Wl) ≤supp(Wl) ∀l
return ˆ
W"
IMPLEMENTATION/METHODS,0.18588235294117647,Algorithm 2: ITERTVSPRUNE
IMPLEMENTATION/METHODS,0.18823529411764706,"Input: Dataset ˆD = {Xi, ui}M
i=1, Pretrained
CNN W = (W1, · · · , WL), initial
TVSEP threshold η, LDIFF threshold ν,
δη > 0, accuracy threshold t, ηmin < η.
Set k = 0, W(0) = W
while η ≥ηmin"
IMPLEMENTATION/METHODS,0.19058823529411764,"˜
W = TVSPRUNE(D, W(k), η, ν)
if f( ˜
W) ≤t then
W(k+1) ←˜
W
else"
IMPLEMENTATION/METHODS,0.19294117647058823,η ←η −δη
IMPLEMENTATION/METHODS,0.1952941176470588,"W(k+1) ←W(k)
k ←k + 1
Output: ˆ
W = ( ˆ
W1, · · · , ˆ
WL),f( ˆ
W) ≤t,
supp( ˆ
Wl) ≤supp(Wl) ∀l
return ˆ
W"
IMPLEMENTATION/METHODS,0.1976470588235294,"6.1
THE TVSPRUNE ALGORITHM - VARIABLE PRUNING RATIOS PER LAYER"
IMPLEMENTATION/METHODS,0.2,"The goal of this algorithm is to leverage the discriminative ability of filters to decide whether a given
filter should be pruned. Thus far, we have defined the MinTVS value, which is a lower bound on
the least classwise TV-separation of a filter’s output, as well as the LDIFF score, which, in a sense,
quantifies the extent to which a layer can be sparsified."
IMPLEMENTATION/METHODS,0.2023529411764706,"The algorithm begins by computing the TV-separability of all the filters using {MinTVS(l, j)}Nl
out
j=1
as defined in equation 10. Next, we compute the LDIFF values as described in equation 6. We then
check the LDIFF score agains the threshold ν, and thereby decide whether or not to prune the given
layer. Then, all filters in each prunable layer (that satisfies LDIFF(l) > ν) that are not η-TV Separable
be pruned. The value of η dictates the aggressiveness of the pruning as it is a measure of how “well
separated” we desire - if η is small, fewer filters are pruned. This approach is useful since it ensures
that the pruning ratio varies from layer to layer, and that we prune those layers that are relatively
difficult to sparsify far less aggressively."
IMPLEMENTATION/METHODS,0.20470588235294118,"Furthermore, we observe that some layers are dramatically more difficult to prune from than oth-
ers. Therefore, we use the LDIFF scores to decide which layers to prune. We now present the
TVSPRUNE algorithm. We assume that we have a labeled dataset ˆD of pairs (X, u) that was
not used for training. The TVSPRUNE algorithm then uses this dataset to compute the moments
¯Y α
l,j, σ2
l,j,α for each layer l, filter j, and class α. Using this data, TVSPRUNE then computes
MinTVS(l, j) for each l, j; after this, the mean ml and standard deviation σl are computed for the
TVSEPvalues {MinTVS(l, j)} for each layer l. After these are computed, those filters that satisfy
MinTVS(l, j) ≤η are pruned."
IMPLEMENTATION/METHODS,0.20705882352941177,"6.2
ITERATIVE PRUNING WITH TV SEPARATION"
IMPLEMENTATION/METHODS,0.20941176470588235,"We motivate this section by noting that the TVSPRUNE algorithm does not give us control over the
accuracy of the pruned model, or the extent that a model is sparsified. Therefore, it is unsuitable
to solve either equation P . In this section, we describe ITERTVSPRUNE, an iterative algorithm
that builds upon TVSPRUNE, and which attempts to solve equation P. We also propose a variant
of ITERTVSPRUNE that attempts to solve the problem of finding the most accurate model given
a sparsity budget, without training data or access to the loss function. We refer readers to the
supplemental material for this algorithm description."
IMPLEMENTATION/METHODS,0.21176470588235294,Published as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.21411764705882352,"The ITERTVSPRUNE algorithm builds upon the fact that the distribution over Y l(X) changes
each time the previous layers are pruned. Thus, we may recompute the TV-Separation values, and
iteratively prune the model. Note that as was the case with TVSPRUNE, this algorithm does not
require any fine-tuning, and does not even require the training data."
IMPLEMENTATION/METHODS,0.2164705882352941,"The ITERTVSPRUNE algorithm takes an initial TVSEP threshold η, a convolutional neural network
with parameters W, an LDIFF threhsold ν, and an accuracy threshold t. At each step k, we run the
TVSPRUNE algorithm given the current parameters W(k) and a current TVSEP threshold ηk; that is
¯
W = TVSPRUNE(W(k), ηk, D). After running TVSPRUNE, we check the accuracy of the pruned
model - if f( ¯
W) ≤t, then we repeat the pruning step, else we reduce η and run TVSPRUNE with
W(k) and the new value of η. The utility of varying the TVSEP threshold is that as we increase the
threshold, the number of filters whose MinTVS value exceeds the threshold reduces. Thus, as we
reduce η, we increase the acceptable degree of TV-separation, thus making the pruning more gentle.
We state the algorithm formally in Algorithm 5."
RESULTS/EXPERIMENTS,0.2188235294117647,"7
EXPERIMENTAL RESULTS"
RESULTS/EXPERIMENTS,0.2211764705882353,"In this section, we detail our experimental results. We aim to compare our method with various
existing structured pruning techniques without any fine-tuning. Our experiments utilize the standard
CIFAR-10 (Krizhevsky et al., 2009) and Imagenet (Russakovsky et al., 2015) datasets under the MIT
license. For computing moments, we partition the test set, which contains 10,000 samples, into two
subsets of size 7,500 and 2,500 in the case of CIFAR10; for Imagenet, we partition the validation set
into two subsets of size 40000 and 10000. We use the larger subset for the computation of moments,
and the smaller subset for testing the accuracy of the pruned model. For our hardware setup, we refer
readers to section E."
RESULTS/EXPERIMENTS,0.2235294117647059,"We aim to answer three broad questions with our slate of experiments. First, do well trained CNNs
have more filters with well-separated filter outputs than models that are not well trained? Second,
using the LDIFF mechanism that utilizes the separability of intermediate features, can we gauge the
“difficulty” of pruning a given layer? And last, how does our pruning mechanism compare with other
existing methods in the scenario where training data and the loss function are unavailable?"
RESULTS/EXPERIMENTS,0.22588235294117648,"For further experimental results, we refer the reader to the supplementary material."
RESULTS/EXPERIMENTS,0.22823529411764706,"7.1
VALIDATING THE DISCRIMINATIVE FILTERS HYPOTHESIS: WELL-TRAINED MODELS
HAVE FILTERS WITH WELL SEPARATED OUTPUTS"
RESULTS/EXPERIMENTS,0.23058823529411765,"In this set of experiments, we aim to show that models that generalize well have filters with well-
separated output features, whereas models that generalize poorly do not. To measure separation, we
compute"
RESULTS/EXPERIMENTS,0.23294117647058823,"MinTVS(l) =
1
N L
out"
RESULTS/EXPERIMENTS,0.23529411764705882,"NL
out
X"
RESULTS/EXPERIMENTS,0.2376470588235294,"j=1
MinTVS(l, j),"
RESULTS/EXPERIMENTS,0.24,"that is, the mean minimum classwise TV-separation of filters in each layer. In order to obtain models
that do not generalize well, we modify the weights of a model with high test accuracy by adding
zero-mean gaussian noise to the weights. In our experiments, we progressively increase the variance
of the gaussian noise, in order to increase the test error. Furthermore, measure MinTVS(l) values
for untrained models for additional comparison. Our experiments focus on VGG16 and ResNet18
models trained on the CIFAR10 and Imagenet datasets. For both architectures, and for both datasets,
we observe that as the variance of the noise increases (and thus, the test error decreases), the average
TV-separation per layer decreases. We also observe that the average TV-separation for untrained
models remains low, and almost constant. Curiously, we note that even with the addition of Gaussian
noise, the separation does not significantly change in the layers close to the input, particularly for
VGG models. This therefore supports our hypothesis that models which generalize well possess
discriminative filters, whereas models that do not generalize well do not."
RESULTS/EXPERIMENTS,0.24235294117647058,"7.2
DETERMINING WHICH LAYERS ARE DIFFICULT TO PRUNE WITH LDIFF"
RESULTS/EXPERIMENTS,0.2447058823529412,"In this section, we investigate the utility of the LDIFF score for determining which layers can be
extensively pruned, and which can’t. The experiment we conduct is as follows. First, we compute the"
RESULTS/EXPERIMENTS,0.24705882352941178,Published as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.24941176470588236,"(a)
VGG16
trained
on
CIFAR10 with Gaussian
Noise"
RESULTS/EXPERIMENTS,0.25176470588235295,"(b) ResNet18 trained on
CIFAR10 with Gaussian
Noise"
RESULTS/EXPERIMENTS,0.2541176470588235,"(c) VGG16 trained on Ima-
genet with Gaussian Noise"
RESULTS/EXPERIMENTS,0.2564705882352941,"(d) ResNet18 trained on
Imagenet with Gaussian
Noise"
RESULTS/EXPERIMENTS,0.25882352941176473,"Figure 1: Comparison of accuracies of models with weights perturbed by Gaussian noise with fully
trained and untrained models."
RESULTS/EXPERIMENTS,0.2611764705882353,"(a) Random Pruning on
VGG-16, CIFAR10"
RESULTS/EXPERIMENTS,0.2635294117647059,"(b) Random Pruning on
VGG-16, CIFAR100"
RESULTS/EXPERIMENTS,0.26588235294117646,"(c) Random Pruning on
VGG-19, CIFAR10"
RESULTS/EXPERIMENTS,0.26823529411764707,"(d) Random Pruning on
VGG-19, CIFAR100"
RESULTS/EXPERIMENTS,0.27058823529411763,"(e) Layerwise LDIFF scores
for VGG-16, CIFAR10"
RESULTS/EXPERIMENTS,0.27294117647058824,"(f) Layerwise LDIFF scores
for VGG-16, CIFAR100"
RESULTS/EXPERIMENTS,0.2752941176470588,"(g) Layerwise LDIFF scores
for VGG19, CIFAR10"
RESULTS/EXPERIMENTS,0.2776470588235294,"(h) Layerwise LDIFF scores
for VGG19, CIFAR100"
RESULTS/EXPERIMENTS,0.28,"Figure 2: Top row: Effect of uniform random unstructured pruning of single layers on VGG16/19
models trained on CIFAR10/100. ‘Prn_frac’ refers to the percent of the weights removed from the
layer. Bottom row: LDIFF scores for corresponding unpruned models."
RESULTS/EXPERIMENTS,0.2823529411764706,"LDIFF scores for the VGG16 -19 models trained on the CIFAR10 and CIFAR100 datasets. We use
512 samples for CIFAR10, sampled from the appropriate partition of the test set, and 4096 samples
for CIFAR100. Then, we prune weights randomly from each layer in isolation; that is, for each
experiment, we only prune weights from a single layer. We then measure the test accuracies. We
observe that the LDIFF scores are small for the initial layers, thus indicating that those layers cannot
be randomly pruned. This is borne out by our experimental results for both models trained on the
CIFAR10 and the CIFAR100 dataset. Last, we infer, based on the LDIFF scores and test accuracies
of randomly pruned models, that pruning ratios of γLDIFF(l, η), where γ < 1/2 would result in
negligible loss of accuracy. Collectively, these observations support the use of the LDIFF score for
determining the extent to which a layer can be pruned. These results are displayed in 5. Further
discussion and experiments are provided in the supplemental material in C."
RESULTS/EXPERIMENTS,0.2847058823529412,"7.3
EFFECTIVENESS OF ITERTVSPRUNE FOR STRUCTURED PRUNING WITHOUT
FINE-TUNING"
RESULTS/EXPERIMENTS,0.28705882352941176,"In this section, we compare the ITERTVSPRUNE algorithm with some existing baselines, in the
setting where we do not have access to the training set or loss function. We consider VGG16, and
-19 models trained on CIFAR10, and ResNet50 and -56 models trained on Imagenet and CIFAR10
respectively. In this set of experiments, we run ITERTVSPRUNE to solve equation P for each model
and dataset; then, having obtained the pruned model satisfying the accuracy constraint, we compare
with three baselines, using the same sparsity pattern as obtained by ITERTVSPRUNE. The baselines"
RESULTS/EXPERIMENTS,0.28941176470588237,Published as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.2917647058823529,"(a) VGG11 on CIFAR-10
(b) VGG16 on CIFAR10
(c) VGG19 on CIFAR-10"
RESULTS/EXPERIMENTS,0.29411764705882354,Figure 3: Comparison of accuracies of ITERTVSPRUNE on VGG models trained on CIFAR10
RESULTS/EXPERIMENTS,0.2964705882352941,"Table 1: Pruning ResNet Models with no training dataset and no finetuning on CIFAR-10 and
Imagenet"
RESULTS/EXPERIMENTS,0.2988235294117647,"Model
Dataset
Param. Sparsity
ITERTVSPRUNE
CHIP
L1
Rand.-3"
RESULTS/EXPERIMENTS,0.30117647058823527,"4.76%
-3.02%
-3.41%
-7.86%
-13.9%
ResNet50
ImageNet
9.98%
-10.21%
-10.08%
-48.2%
43.1%
24.65%
-31.3%
-34.2%
-
-"
RESULTS/EXPERIMENTS,0.3035294117647059,"3.5%
-1.47%
-1.36%
-5.46%
-7.12%
ResNet56
CIFAR10
7.6%
-4.82%
-5.56%
N/A
-9.41%
12.3%
-9.86%
-10.22%
-17.41%
-21.30%"
RESULTS/EXPERIMENTS,0.3058823529411765,"chosen were L1 based pruning, CHIP (Sui et al., 2021) (for ResNet models) and a first order gradient
based score based on (Prakash et al., 2019; Molchanov et al., 2019a) (for VGG nets), and uniform
random pruning (accuracy is the average of three trials of random pruning); all were run without fine
tuning. Modifications to implementations are discussed in Appendix E. We report our results for
VGG nets on CIFAR10 in Figure 7, and for ResNet models in Table 1. We observe that our method
consistently outperforms the baselines in the task of structured pruning of models without fine tuning
the model with the training set. Notably, for VGG19 trained on CIFAR10, we are able to remove
more than 60% of parameters with minimal loss in accuracy, far exceeding the nearest baseline."
CONCLUSION/DISCUSSION ,0.30823529411764705,"8
CONCLUSIONS AND DISCUSSION"
CONCLUSION/DISCUSSION ,0.31058823529411766,"In this work, we propose a new paradigm for pruning, which we call distributional pruning, which
only requires access to the data distribution. We make the observation that models with high predictive
performance possess a mixture of filters that discriminate well, and those that discriminate poorly.
Motivated by these observations, we argue that we can prune non-discriminative filters to avoid
significant loss of test error. We use the TV Distance to quantify the discriminative ability of filters,
using which we define the heuristic LDIFF score and derive the TVSPRUNE Algorithm."
CONCLUSION/DISCUSSION ,0.3129411764705882,"The TVSPRUNE algorithm’s drawbacks are as follows. First, since classwise distances must be
computed, the number of separations for each filter that the algorithm needs to estimate are quadratic
in the number of classes; limiting the scalability of the algorithm. Next, the relationship between
TV-separability and generalization in DNNs is not fully understood, and requires further investigation.
Future research directions include (a) improving the scalability of this algorithm to massive datasets,
(b) evaluating other measures of similarity between distributions, and (c) analyzing parameter
quantization through the lens of distributional separability."
CONCLUSION/DISCUSSION ,0.31529411764705884,Published as a conference paper at ICLR 2023
CONCLUSION/DISCUSSION ,0.3176470588235294,ACKNOWLEDGEMENTS
CONCLUSION/DISCUSSION ,0.32,"We authors gratefully acknowledge AMD for their support. The authors also thank Ramaswamy
Govindarajan (Professor, IISc), Himanshu Jain (IISc), Raghavendra Prakash (AMD), and Ramasamy
Chandra Kumar (AMD) for their insight and assistance in this work."
CONCLUSION/DISCUSSION ,0.32235294117647056,The authors thank the reviewers for their valuable feedback which has helped us improve our work.
REFERENCES,0.3247058823529412,REFERENCES
REFERENCES,0.3270588235294118,"David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection:
Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 6541–6549, 2017."
REFERENCES,0.32941176470588235,"Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela Rus. Data-dependent
coresets for compressing neural networks with applications to generalization bounds. In Interna-
tional Conference on Learning Representations, 2018."
REFERENCES,0.33176470588235296,"Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of
neural network pruning? Proceedings of machine learning and systems, 2:129–146, 2020."
REFERENCES,0.3341176470588235,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.33647058823529413,"Thomas Gebhart, Udit Saxena, and Paul Schrater.
A unified paths perspective for pruning at
initialization. arXiv preprint arXiv:2101.10552, 2021."
REFERENCES,0.3388235294117647,"Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016."
REFERENCES,0.3411764705882353,"Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015."
REFERENCES,0.34352941176470586,"Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon.
Advances in neural information processing systems, 5, 1992."
REFERENCES,0.3458823529411765,"Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating
deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018."
REFERENCES,0.34823529411764703,"Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for
deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 4340–4349, 2019."
REFERENCES,0.35058823529411764,"Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep
learning: Pruning and growth for efficient inference and training in neural networks. Journal of
Machine Learning Research, 22(241):1–124, 2021."
REFERENCES,0.35294117647058826,"Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size.
arXiv preprint arXiv:1602.07360, 2016."
REFERENCES,0.3552941176470588,"Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. Advances in neural information
processing systems, 32, 2019."
REFERENCES,0.35764705882352943,"Saumya Jetley, Nicholas Lord, and Philip Torr. With friends like these, who needs adversaries?
Advances in neural information processing systems, 31, 2018."
REFERENCES,0.36,"Donggyu Joo, Eojindl Yi, Sunghyun Baek, and Junmo Kim. Linearly replaceable filters for deep
network channel pruning. In The 34th AAAI Conference on Artificial Intelligence,(AAAI), 2021."
REFERENCES,0.3623529411764706,"Charles Kraft. Some conditions for consistency and uniform consistency of statistical procedures.
University of California Publication in Statistics, 2:125–141, 1955."
REFERENCES,0.36470588235294116,Published as a conference paper at ICLR 2023
REFERENCES,0.36705882352941177,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.36941176470588233,"Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information
processing systems, 2, 1989."
REFERENCES,0.37176470588235294,"Dongha Lee, Sehun Yu, and Hwanjo Yu. Multi-class data description for out-of-distribution detection.
In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pp. 1362–1370, 2020."
REFERENCES,0.37411764705882355,"Seungmin Lee, Dongwan Kim, Namil Kim, and Seong-Gyun Jeong. Drop to adapt: Learning
discriminative features for unsupervised domain adaptation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp. 91–100, 2019."
REFERENCES,0.3764705882352941,"Bailin Li, Bowen Wu, Jiang Su, and Guangrun Wang. Eagleeye: Fast sub-net evaluation for efficient
neural network pruning. In European conference on computer vision, pp. 639–654. Springer, 2020."
REFERENCES,0.3788235294117647,"Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficient convnets. arXiv preprint arXiv:1608.08710, 2016."
REFERENCES,0.3811764705882353,"Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus. Provable filter pruning
for efficient neural networks. In International Conference on Learning Representations, 2019."
REFERENCES,0.3835294117647059,"Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling
Shao. Hrank: Filter pruning using high-rank feature map. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pp. 1529–1538, 2020a."
REFERENCES,0.38588235294117645,"Mingbao Lin, Rongrong Ji, Yuxin Zhang, Baochang Zhang, Yongjian Wu, and Yonghong Tian.
Channel pruning via automatic structure search. arXiv preprint arXiv:2001.08565, 2020b."
REFERENCES,0.38823529411764707,"Jing Liu, Bohan Zhuang, Zhuangwei Zhuang, Yong Guo, Junzhou Huang, Jinhui Zhu, and Mingkui
Tan. Discrimination-aware network pruning for deep model compression. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2021a."
REFERENCES,0.3905882352941176,"Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin
Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang. Group fisher pruning for practical
network compression. In International Conference on Machine Learning, pp. 7021–7032. PMLR,
2021b."
REFERENCES,0.39294117647058824,"Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket
hypothesis: Pruning is all you need. In International Conference on Machine Learning, pp.
6682–6691. PMLR, 2020."
REFERENCES,0.3952941176470588,"Fanxu Meng, Hao Cheng, Ke Li, Huixiang Luo, Xiaowei Guo, Guangming Lu, and Xing Sun.
Pruning filter in filter. Advances in Neural Information Processing Systems, 33:17629–17640,
2020."
REFERENCES,0.3976470588235294,"P Molchanov, S Tyree, T Karras, T Aila, and J Kautz. Pruning convolutional neural networks for
resource efficient inference. In 5th International Conference on Learning Representations, ICLR
2017-Conference Track Proceedings, 2019a."
REFERENCES,0.4,"Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation
for neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 11264–11272, 2019b."
REFERENCES,0.4023529411764706,"Ben Mussay, Dan Feldman, Samson Zhou, Vladimir Braverman, and Margarita Osadchy. Data-
independent structured pruning of neural networks via coresets. IEEE Transactions on Neural
Networks and Learning Systems, 2021."
REFERENCES,0.4047058823529412,"Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi, and Pascal Frossard. Hold
me tight! influence of discriminative features on deep network boundaries. Advances in Neural
Information Processing Systems, 33:2935–2946, 2020."
REFERENCES,0.40705882352941175,"Shreyas Malakarjun Patil and Constantine Dovrolis. Phew: Constructing sparse networks that learn
fast and generalize well without training data. In International Conference on Machine Learning,
pp. 8432–8442. PMLR, 2021."
REFERENCES,0.40941176470588236,Published as a conference paper at ICLR 2023
REFERENCES,0.4117647058823529,"Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris Papailiopoulos.
Optimal lottery tickets via subset sum: Logarithmic over-parameterization is sufficient. Advances
in Neural Information Processing Systems, 33:2599–2610, 2020."
REFERENCES,0.41411764705882353,"Prafull Prakash, Chaitanya Murti, Saketha Nath, and Chiranjib Bhattacharyya. Optimizing dnn
architectures for high speed autonomous navigation in gps denied environments on edge devices.
In Pacific Rim International Conference on Artificial Intelligence, pp. 468–481. Springer, 2019."
REFERENCES,0.4164705882352941,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International journal of computer vision, 115(3):211–252, 2015."
REFERENCES,0.4188235294117647,"Prem Seetharaman, Gordon Wichern, Shrikant Venkataramani, and Jonathan Le Roux. Class-
conditional embeddings for music source separation. In ICASSP 2019-2019 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 301–305. IEEE, 2019."
REFERENCES,0.4211764705882353,"Suraj Srinivas and R Venkatesh Babu. Data-free parameter pruning for deep neural networks. arXiv
preprint arXiv:1507.06149, 2015."
REFERENCES,0.4235294117647059,"Yang Sui, Miao Yin, Yi Xie, Huy Phan, Saman Aliari Zonouz, and Bo Yuan. Chip: Channel
independence-based pruning for compact neural networks. Advances in Neural Information
Processing Systems, 34, 2021."
REFERENCES,0.4258823529411765,"Xin Sun, Zhenning Yang, Chi Zhang, Keck-Voon Ling, and Guohao Peng. Conditional gaussian
distribution learning for open set recognition. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 13480–13489, 2020."
REFERENCES,0.42823529411764705,"Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks with-
out any data by iteratively conserving synaptic flow. Advances in Neural Information Processing
Systems, 33:6377–6389, 2020."
REFERENCES,0.43058823529411766,"Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach
for deep face recognition. In European conference on computer vision, pp. 499–515. Springer,
2016."
REFERENCES,0.4329411764705882,"Eric Wong, Shibani Santurkar, and Aleksander Madry. Leveraging sparse linear layers for debuggable
deep networks. In International Conference on Machine Learning, pp. 11205–11216. PMLR,
2021."
REFERENCES,0.43529411764705883,"Sarthak Yadav and Atul Rai. Learning discriminative features for speaker identification and verifica-
tion. In Interspeech, pp. 2237–2241, 2018."
REFERENCES,0.4376470588235294,"Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K
Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
8715–8724, 2020."
REFERENCES,0.44,"Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han, Mingfei Gao,
Ching-Yung Lin, and Larry S Davis. Nisp: Pruning networks using neuron importance score
propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 9194–9203, 2018."
REFERENCES,0.4423529411764706,"Shaochen Zhong, Guanqun Zhang, Ningjia Huang, and Shuai Xu. Revisit kernel pruning with lottery
regulated grouped convolutions. In International Conference on Learning Representations, 2021."
REFERENCES,0.4447058823529412,"Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang,
and Jinhui Zhu. Discrimination-aware channel pruning for deep neural networks. Advances in
neural information processing systems, 31, 2018."
REFERENCES,0.4470588235294118,"Zhen Zuo, Gang Wang, Bing Shuai, Lifan Zhao, Qingxiong Yang, and Xudong Jiang. Learning
discriminative and shareable features for scene classification. In European Conference on Computer
Vision, pp. 552–568. Springer, 2014."
APPENDIX,0.44941176470588234,Published as a conference paper at ICLR 2023
APPENDIX,0.45176470588235296,APPENDIX
APPENDIX,0.4541176470588235,"In this supplemental material, we provide additional detail and empirical results to support the claims
of the main paper. We organize this document as follows."
APPENDIX,0.45647058823529413,"1. In section A, for the sake of comprehensiveness, we provide further discussion of relevant
literature, including a discussion on the discriminative ability of convolutional filters."
APPENDIX,0.4588235294117647,"2. In section B, we present additional discussion required for clarification purposes, including
a brief discussion adopting the machinery used in this paper to feedforward models."
APPENDIX,0.4611764705882353,"3. In section C, we provide further details regarding the derivation of the LDIFF scores."
APPENDIX,0.46352941176470586,"4. In section E, we provide additional experiments in using the TVSPRUNE and TVSPRUNE al-
gorithms for structured pruning. We provide plots showing the number of filters pruned in
each layer for the same experiments presented in Section 7.3 of the original paper."
APPENDIX,0.46588235294117647,"5. In section D we state a simple variation of the ITERTVSPRUNE algorithm that aims to solve
a slight variation of the pruning problem."
APPENDIX,0.4682352941176471,"In the sequel, we list additional material added to this document."
APPENDIX,0.47058823529411764,"6. In section E, we provide additional experiments illustrating the utility and effectiveness of
the ITERTVSPRUNE algorithm."
APPENDIX,0.47294117647058825,"A
ADDITIONAL LITERATURE SURVEY"
APPENDIX,0.4752941176470588,"In this section, we survey contemporary and historic results in the area of neural network pruning.
We focus on filter pruning in this work, but there exists a wide variety of works addressing problem
of unstructured, or weight pruning. For a more detailed discussion on relevant results in unstructured
pruning, refer to (Hoefler et al., 2021; Blalock et al., 2020)."
APPENDIX,0.4776470588235294,"Filter Pruning
Filter pruning refers to processes by which entire filters or channels are pruned
from a model; this can be further extended to individual filters(Zhong et al., 2021) or ’stripes’ (Meng
et al., 2020) within filters. Initial results on structured pruning removed filters based on the L1
norm of the weight tensors (Li et al., 2016). More recent advances involve using gradient based
scores (Molchanov et al., 2019b; Liu et al., 2021b; Molchanov et al., 2019a), rank minimization
(Lin et al., 2020a), and linear replaceability and channel independence (Joo et al., 2021; Sui et al.,
2021), and other geometric properties (He et al., 2018; 2019; Li et al., 2020; Lin et al., 2020b; Yu
et al., 2018). Other works, such as (Liebenwein et al., 2019; Baykal et al., 2018) developed sampling
based methods for structured pruning that also provided generalization bounds. Lastly, in (Zhuang
et al., 2018; Liu et al., 2021a), the discriminative capacity of filters is used to inform the pruning
strategy by introducing discrimination-aware loss functions. Our work differs from these in two key
respects - first, we rank filters based on the discriminative ability of their outputs, and second, we use
a novel metric based on the discriminative ability of filters, based on classwise distances between
class-conditional output distributions of filters, to decide which layers can be pruned effectively."
APPENDIX,0.48,"Data-Free Pruning
Data free pruning has recently gained significant interest (Hoefler et al., 2021);
however, while there are a variety of methods that do not use data for the task of pruning, there are
few methods that do not require fine tuning or retraining. Early works in this regime include (Srinivas
& Babu, 2015), which measured similarity between neurons, and merged similar neurons together.
More recently, (Tanaka et al., 2020) proposed the SynFlow method, an unstructured, data free pruning
method that relied on preserving gradient flow. This work was expanded upon in (Gebhart et al.,
2021; Patil & Dovrolis, 2021), which utilize the Neural Tangent Kernel to improve upon SynFlow.
Other approaches include (Mussay et al., 2021), which uses coresets of weights to perform structured
pruning. Our work differs from this since we utilize the class conditional distributions of the filter
outputs to rank the filters, which we either obtain a priori, or compute using previously unseen data.
In this section, we briefly survey works pertaining to discriminative ability of neural networks, and
unstructured pruning. We do so for the sake of comprehensiveness, and to discuss works that are
tangentially relevant to ours."
APPENDIX,0.4823529411764706,Published as a conference paper at ICLR 2023
APPENDIX,0.48470588235294115,"Discriminative Ability of Convolutional Neural Networks
Studying the discriminative ability
of features and representation at either the layer level or the filter level is an active area of research,
with applications in speech identification (Yadav & Rai, 2018), scene classification (Zuo et al., 2014),
face recognition (Wen et al., 2016), and domain adaptation (Lee et al., 2019). Understanding the
discriminative capacity of features is also crucial toward understanding adversarial robustness (Jetley
et al., 2018; Ilyas et al., 2019) and model interpretability (Bau et al., 2017). In (Wong et al., 2021),
pretrained set of convolutional layers is given as the input to a linear classifier which is trained with a
sparsity-inducing regularizer to identify discriminative filters. In (Ortiz-Jimenez et al., 2020), the
discriminative capacity of intermediate representations is used to analyze decision boundaries of
CNNs. However, to the best of our knowledge, there are no works that utilize the discriminative
capacity of filters for pruning models."
APPENDIX,0.48705882352941177,"Discriminative Filters in Pruning
Discriminative filters have also been applied to structured
pruning of neural networks. In (Zhuang et al., 2018; Liu et al., 2021a), the discriminative capacity of
filters is used to inform the pruning strategy. In particular, the authors define the discriminative ability
of a filter as the impact of it’s ouptut on the final classifer, and introduce a discrimination aware
loss function, with which the model is fine tuned. Once this is done, the authors propose a greedy
algorithm for channel selection that makes use of this loss function and the associated gradients.
This method differs from this work, as we explicitly define discriminative ability in terms of the TV
separation, and not on the impact on the final classifier."
APPENDIX,0.4894117647058824,"Unstructured Pruning
In this section, we provide a brief overview of unstructured pruning
techniques. While unstructured pruning is not the focus of this work, we provide this summary in the
interest of completeness. and Unstructured pruning refers to processes by which individual weights
are pruned from a model (Hoefler et al., 2021). Early works on unstructured pruning include (LeCun
et al., 1989; Hassibi & Stork, 1992), which relied on diagonal approximations of the Hessian of the
loss function. However, such methods are not scalable to very large models, prompting the creation
of unstructured pruning methods that married a variety of techniques to achieve dramatic parametric
sparsity (Han et al., 2015; Iandola et al., 2016). More recently, (Frankle & Carbin, 2018) proposed
the Lottery Ticket Hypothesis, which asserts that networks contain subnetworks which, when trained
from scratch, achieve similar accuracies to the original model; stronger versions of this result were
proved in (Malach et al., 2020; Pensia et al., 2020). This remains an active area of research (Blalock
et al., 2020)."
APPENDIX,0.49176470588235294,"B
ADDITIONAL DISCUSSION"
APPENDIX,0.49411764705882355,"In this section, we include additional discussion on certain aspects of this work. In particular, we
briefly discuss the extension of our method to linear, feedforward networks, and provide additional
discussion on the definitions and use of patches."
APPENDIX,0.4964705882352941,"B.1
EXTENSION OF PROPOSED METHODOLOGY TO LINEAR LAYERS"
APPENDIX,0.4988235294117647,"Our proposed methodology can easily be extended to feedforward/linear layers as well. Broadly
speaking, we consider the layerwise outputs to be the outputs of linear layers, as opposed to convolu-
tional layers. In that case, it is possible to derive the same measures of distributional discrepancy. We
provide a more rigorous derivation below:"
APPENDIX,0.5011764705882353,"Let Φl−1(X) be the output of the l −1-th linear layer, and let W l be the weight tensor of the l-th
layer. Then,
Yl(X) = ⟨Φl−1(X), W l⟩.
(7)
If Yl(X) is a vector, let Y j
l (X) be the jth element (the jth neuron). Then, we simply define
¯Y α = E(X,u)∼Dα [Y (X)] and σ2
l,j,α = E(X,u)∼Dα 
∥Y (X) −¯Y α∥2
(8)
as we did for convolutional layers. However, in this case, outputs are vectors. Thus, each Y j
l is a
single real number (along with the expectations and the variances). Using this, we can then define"
APPENDIX,0.5035294117647059,"HELLD2 (Q1, Q2) = 1 −
 2σ1σ2"
APPENDIX,0.5058823529411764,"σ2
1 + σ2
2  d 2
e−∆"
APPENDIX,0.508235294117647,"4 , ∆= (µ1 −µ2)2"
APPENDIX,0.5105882352941177,"σ2
1 + σ2
2
(9)"
APPENDIX,0.5129411764705882,"and
MinTVS(l, j) = min
α,β TV

Dα
l,j, Dβ
l,j

≥min
α,β 1 −exp(−∆α,β
l,j /4)
(10)"
APPENDIX,0.5152941176470588,Published as a conference paper at ICLR 2023
APPENDIX,0.5176470588235295,"where
∆α,β
l,j = ( ¯Y α
l,j −¯Y β
l,j)2 
σ2
l,j,α + σ2
l,j,β
−1 .
(11)"
APPENDIX,0.52,"B.2
DESCRIPTION OF PATCHES"
APPENDIX,0.5223529411764706,"A patch is a subset of the input datum with which a dot product with the convolutional filter is taken.
The input datum is made up of several patches which may or may not overlap. We describe this
formally in the sequel."
APPENDIX,0.5247058823529411,"For simplicity, assume that the input is a in RM×M, and each convolutional filter is a tensor in RK×K
where K < M. In a convolution operation, the filter is applied to subsets of size K × K of the
input; that is, the output is a dot product of the filter and that subset of elements of the input. Each
such subset is called a patch. We describe this further in Section H of the supplemental material.
Furthermore, we refer the reader to, for example, (Goodfellow et al., 2016)."
APPENDIX,0.5270588235294118,"C
THE LDIFF SCORE: ADDITIONAL CONTEXT AND EMPIRICAL ANALYSIS"
APPENDIX,0.5294117647058824,"In this section, we provide additional context and information regarding the LDIFF score described in
Section 6.1, which is a heuristic used to quantify the extent to which a layer may be randomly pruned."
APPENDIX,0.5317647058823529,"C.1
REFINEMENTS OF THE LDIFF SCORE"
APPENDIX,0.5341176470588235,"In this section, we discuss refinements of the LDIFF score presented in section 5. Recall that in this
work, we focus on identifying and pruning non-discriminative filters from individual layers. Thus, as
mentioned previously, our goal is to derive a score, leveraging the aforementioned hypothesis and
using the discriminative ability of the filters in a given layer, that we can use to deduce whether or not
a layer is difficult to prune."
APPENDIX,0.5364705882352941,"Assuming the hypothesis is true, a good measure of the extent to which a model can be pruned could
be given by the fraction of filters that are highly discriminative. Thus, if a layer has only a few highly
discriminative filters, it should be possible to sparsify that layer extensively."
APPENDIX,0.5388235294117647,"In the LDIFF score defined in equation 6, we fix the threshold to identify discriminative filters. We
first present methods with fixed thresholds that utilize statistics of the layerwise MinTVS scores
here."
APPENDIX,0.5411764705882353,"C.1.1
MEDIAN-BASED LDIFF SCORES WITH A FIXED THRESHOLD"
APPENDIX,0.5435294117647059,We begin by recalling definitions from the original paper.
APPENDIX,0.5458823529411765,"Let
\
MinTVS(l) be the median value of MinTVS(l, j) for a given l (that is, with respect to j). We
recall that a filter is discriminative if it is TVSEP(η) for some η > 0; furthermore, recall that η can
vary from model to model, and dataset to dataset. With this, we first define the following sets."
APPENDIX,0.548235294117647,"• Suppose that for at least one j, the corresponding filter is TVSEP(η) for some η > 0. We
define the sets
Sη,l = {j : ∆∗
l,j < η}, and ˆSl = {j : ∆∗
l,j ≥ˆ∆l.}"
APPENDIX,0.5505882352941176,"• The set Sη,l captures the number of filters that are not TVSEP(η), and thus non-
discriminative."
APPENDIX,0.5529411764705883,"• The set ˆSl is the set of ∆∗
l,j greater than the median."
APPENDIX,0.5552941176470588,Let |S| denote the cardinality of a set S. We then define
APPENDIX,0.5576470588235294,"LDIFF0(l) = 1 −|Sη,l ∩ˆSl|"
APPENDIX,0.56,"| ˆSl|
.
(12)"
APPENDIX,0.5623529411764706,"The set Sη,l ∩ˆSl captures the number of non-discriminative filters in the top two quantiles. Thus,
LDIFF0(l) captures the fraction of filters in the top two quantiles that are discriminative. If this
quantity equals 1, we conclude that at least half the filters are TVSEP(η); thus LDIFF0 captures the
mix between discriminative and non-discriminative filters. Thus, if LDIFF0(l) is close to 1, then the
layer is difficult to prune."
APPENDIX,0.5647058823529412,Published as a conference paper at ICLR 2023
APPENDIX,0.5670588235294117,"This is illustrated in Figure 9, where we plot the LDIFF0(l) scores for each layer for VGG16 and
VGG19 models trained on the CIFAR10 and CIFAR100 datasets. We choose η = 0.025, and we
see that the initial layers of VGG16 and VGG19 models trained on CIFAR10 appear to be very easy
to prune, since there should be no discriminative filters. Instead, however, those layers cannot be
effectively sparsified."
APPENDIX,0.5694117647058824,"In order to compensate for this, we chose another way to quantify the mix between discriminative and
non-discriminative filters in a layer. Specifically, we define LDIFF as in equation 6, which captures
the ratio of the separations that are below the median ˆ∆l, and those above it. As we will show in the
sequel, this is a more effective variant for capturing the difficulty of pruning filters, or indeed weights,
from a layer."
APPENDIX,0.571764705882353,"(a)
Layerwise
LD-
IFF0 scores for VGG-16,
CIFAR10"
APPENDIX,0.5741176470588235,"(b)
Layerwise
LD-
IFF0 scores for VGG-19,
CIFAR10"
APPENDIX,0.5764705882352941,"(c)
Layerwise
LD-
IFF0 scores for VGG16,
CIFAR100"
APPENDIX,0.5788235294117647,"(d)
Layerwise
LD-
IFF0 scores for VGG19,
CIFAR100"
APPENDIX,0.5811764705882353,"Here, we observe that the LDIFF0 score is effective at determining whether a layer is difficult to prune
for models trained on CIFAR100, but is seemingly not as effective for models trained on CIFAR10.
In particular, for models trained on CIFAR10, we observe that the initial layers receive low scores -
since filter outputs for those models are not well separated. This lends credence to the notion that if
there are no filters in a layer that are TVSEP(η), then we should avoid pruning that layer entirely. In
the sequel, we discuss variants of the scores defined herein and propose variants, including the score
proposed in the final paper."
APPENDIX,0.5835294117647059,"C.2
FLEXIBLE THRESHOLD LDIFF SCORES"
APPENDIX,0.5858823529411765,"A potential drawback of using fixed thresholds for discriminative ability, however, is that the scale
of the distributional separation may vary from layer to layer. That is a value of η that ensures
discriminative ability in one layer may not apply to another. In this section, we extend this definition
to a more flexible variant, wherein we utilize layerwise thresholds. We consider multiple variants,
including cases where thresholds are fixed a priori and variants where thresholds are fixed based on
statistics of the MinTVS values for a given layer. Furthermore, layerwise thresholds for discriminative
ability provide the added benefit of being able to capture the ‘mix’ of discriminative and non-
discriminative filters."
APPENDIX,0.5882352941176471,"C.2.1
COUNTING-BASED LDIFFSCORES USING LAYERWISE MinTVS STATISTICS"
APPENDIX,0.5905882352941176,"In this section, we focus on methods that utilize the statistics of MinTVS values for a given layer to
determine pruning thresholds. A variant of the method used in the main paper is as follows."
APPENDIX,0.5929411764705882,"Define MinTVS(l) to be the mean MinTVS for a given l (that is, with respect to j), and let γ > 0
be a constant. Then, define"
APPENDIX,0.5952941176470589,"LDIFFC(l) =
P 1{MinTVS(l, j) < γMinTVS(l)}
P 1{MinTVS(l, j) > γMinTVS(l)}
(13)"
APPENDIX,0.5976470588235294,"The LDIFFC(l) score utilizes the mean MinTVS score for a layer to capture the relative discriminative
ability of filters in a layer, along with a constant γl. For each layer, this measure counts the number of
filters that are discriminative relative to the MinTVS score for that layer. As mentioned previously,
useful thresholds of discriminative ability can vary significantly from architecture to architecture,
layer to layer, and dataset to dataset."
APPENDIX,0.6,Published as a conference paper at ICLR 2023
APPENDIX,0.6023529411764705,"C.2.2
MASS-BASED LDIFFSCORES USING LAYERWISE MinTVS STATISTICS"
APPENDIX,0.6047058823529412,"The previous counting-based measures of pruneability, even with flexible thresholds, fail to account
for the wide range of values that MinTVS scores can take, particularly in wide networks with a small
number of very discriminative filters. In order to address this issue, we define the following variant of
the LDIFF score."
APPENDIX,0.6070588235294118,"LDIFF(l) =
P MinTVS(l, j)1{MinTVS(l, j) < γMinTVS(l)}
P MinTVS(l, j)1{MinTVS(l, j) > γMinTVS(l)}
(14)"
APPENDIX,0.6094117647058823,"This variant of LDIFF score compares the ratio of MinTVS(l, j) scores that lie below γMinTVS(l)
to the sum of MinTVS(l, j) scores that lie above γMinTVS(l). Thus, we compare the ‘mass’ of
the discriminative ability of scores below the (variable) threshold to those above it. Hence, we use
the term “Mass-based” LDIFF scores. This captures the spread of discriminative ability, and can be
shown to more effectively predict which layers can be randomly pruned."
APPENDIX,0.611764705882353,"(a) Layerwise LDIFF scores
for VGG-16, CIFAR10"
APPENDIX,0.6141176470588235,"(b) Layerwise LDIFF scores
for VGG-16, CIFAR100"
APPENDIX,0.6164705882352941,"(c) Layerwise LDIFF scores
for VGG19, CIFAR10"
APPENDIX,0.6188235294117647,"(d) Layerwise LDIFF scores
for VGG19, CIFAR100"
APPENDIX,0.6211764705882353,"Figure 5: Mass based LDIFF scores. Here, a higher score means it is more difficult to prune."
APPENDIX,0.6235294117647059,"D
VARIANTS OF ITERTVSPRUNE"
APPENDIX,0.6258823529411764,"In this section, we describe variants of the TVSPRUNE algorithm. First, we describe a method that
utilizes flexible thresholds, and second, we describe a variant to solve the pruning problem with a
fixed sparsity budget."
APPENDIX,0.6282352941176471,"D.1
TVSPRUNE WITH FLEXIBLE THRESHOLDS"
APPENDIX,0.6305882352941177,".
This section describes a variant of the TVSPRUNE
and ITERTVSPRUNE
algorithms
that utilize variable thresholds for discriminative ability.
In this case, we specifically use
the mean MinTVS score to determine layerwise pruning thresholds.
Furthermore, we use
the mass-based LDIFF score described in equation 14 to determine which layers to prune."
APPENDIX,0.6329411764705882,Published as a conference paper at ICLR 2023
APPENDIX,0.6352941176470588,Algorithm 3: TVSPRUNEmean
APPENDIX,0.6376470588235295,"Input: Dataset ˆD = {Xi, ui}M
i=1, Pretrained
CNN with parameters
W = (W1, · · · , WL), γ > 0, layer
difficulty threshold ν
Compute ¯Y l,α
j
, σ2
l,j,α for all l, j, and α .
Compute ∆α,β
l,j for each l, j, α, β.
for l ∈[L] do"
APPENDIX,0.64,"Compute MinTVS(l, j) for all l, j
Compute MinTVS(l) ="
APPENDIX,0.6423529411764706,"1
Nl
out
PNl
out
j=1 MinTVS(l, j)"
APPENDIX,0.6447058823529411,"Compute LDIFF(l) from equation 14
if LDIFF(l) < ν then"
APPENDIX,0.6470588235294118,"for j ∈[N l
out] do
if MinTVS(l, j) < γMinTVS(l)
then"
APPENDIX,0.6494117647058824,"W l
j ←0"
APPENDIX,0.6517647058823529,"Output: ˆ
W = ( ˆ
W1, · · · , ˆ
WL), where
supp( ˆ
Wl) ≤supp(Wl) ∀l
return ˆ
W"
APPENDIX,0.6541176470588236,Algorithm 4: ITERTVSPRUNEmean
APPENDIX,0.6564705882352941,"Input: Dataset ˆD = {Xi, ui}M
i=1, Pretrained
CNN W = (W1, · · · , WL), γ0 > 0,
LDIFF threshold ν, δγ > 0, accuracy
threshold t, γmin < γ.
Set k = 0, W(0) = W
while γ ≥γmin"
APPENDIX,0.6588235294117647,"˜
W = TVSPRUNEmean(D, W(k), γ, ν)
if f( ˜
W) ≤t then
W(k+1) ←˜
W
else"
APPENDIX,0.6611764705882353,"γ ←γ −δγ
W(k+1) ←W(k)
k ←k + 1
Output: ˆ
W = ( ˆ
W1, · · · , ˆ
WL),f( ˆ
W) ≤t,
supp( ˆ
Wl) ≤supp(Wl) ∀l
return ˆ
W"
APPENDIX,0.6635294117647059,"This algorithm functions in a similar fashion to the TVSPRUNE and ITERTVSPRUNE algorithms,
except that the thresholds for discriminative ability are flexible (varying between layers), and are
decided by the mean of the MinTVS scores for each layer. Furthermore, just as the threshold for
discriminative ability were reduced at each iteration of ITERTVSPRUNE, we vary the constant γ,
thus enabling us to flexibly prune each layer even as the distribution of MinTVS scores changes with
each pruning iteration."
APPENDIX,0.6658823529411765,"D.2
FIXED-BUDGET PRUNING"
APPENDIX,0.668235294117647,"In this section, we briefly describe a variant of the ITERTVSPRUNE algorithm that, using only
the data-generating distribution, prunes a model with a fixed sparsity budget using only the data-
generating distribution. This variant attempts to solve the problem of finding the most accurate model
subject to a sparsity budget. Formally speaking, this variant of the algorithm attempts to solve
arg min {f(W) | ∥W∥0 ≤K, K > 0} .
(P2)
where ∥W∥0 = P"
APPENDIX,0.6705882352941176,"l ∥Wl∥0 and where K is a sparsity budget satisfying K < ∥W∥0. While a simple
option is to simply increase η in the TVSPRUNE algorithm (thus raising the threshold as to which
filters are discriminative), the proposed variant makes use of the fact that the distributions of the
filter outputs change at each iteration, owing to the filters pruned in the previous step This algorithm,"
APPENDIX,0.6729411764705883,Algorithm 5: ITERTVSPRUNE-SB
APPENDIX,0.6752941176470588,"Input: Dataset ˆD = {Xi, ui}M
i=1, Pretrained CNN W = (W1, · · · , WL), initial TVSEP threshold η,
LDIFF threshold ν, δη > 0, sparsity budget K, ηmin < η.
Set k = 0, W(0) = W
while η ≥ηmin"
APPENDIX,0.6776470588235294,"˜
W = TVSPRUNE(D, W(k), η, ν)
if ∥˜
W∥0 ≥K then"
APPENDIX,0.68,"W(k+1) ←˜
W
else"
APPENDIX,0.6823529411764706,"η ←η −δη
W(k+1) ←W(k)
k ←k + 1
Output: ˆ
W = ( ˆ
W1, · · · , ˆ
WL), ∥ˆ
W∥0 ≤K,
return ˆ
W"
APPENDIX,0.6847058823529412,"ITERTVSPRUNE-SB (SB refers to sparsity budget), iteratively calls the TVSPRUNE algorithm to"
APPENDIX,0.6870588235294117,Published as a conference paper at ICLR 2023
APPENDIX,0.6894117647058824,"prune a fraction of filters at each step. At the kth iteration, if ∥˜
W∥0 < K; that is, the fraction of
parameters drops below the sparsity budget K, we rewind to the previous value of Wk, and reduce η,
thereby ensuring that we prune more gently. The drawback of using this algorithm is that ηmin needs
to be chosen carefully so that we are guaranteed to prune some filters at each iteration. While we do
not directly control for accuracy, we rely on our hypothesis that we may remove non-discriminative
filters without suffering catastrophic losses in accuracy."
APPENDIX,0.691764705882353,"E
ADDITIONAL EXPERIMENTS USING ITERTVSPRUNE"
APPENDIX,0.6941176470588235,"In this section, we provide additional experimental details on the use of the ITERTVSPRUNE algo-
rithm."
APPENDIX,0.6964705882352941,HARDWARE SETUP
APPENDIX,0.6988235294117647,"Our experiments were conducted on two systems, one with an NVIDIA GTX1060 GPU and an Intel
i7-7700, and one with dual NVIDIA RTX3090 GPUs and an Intel i9-11900F. All experiments are
done using the PyTorch framework, in particular, for obtaining the output features for each filter.
Moments and TVSEPvalues were computed using standard numpy packages."
APPENDIX,0.7011764705882353,DATA SET SPLITS
APPENDIX,0.7035294117647058,"We briefly discuss the split of the data used in our experiments. As mentioned in the main paper, we
do not make use of the training data at all. However, since we are unable to obtain sufficient data that
would be drawn from the class conditional distributions of the CIFAR10 and CIFAR100 datasets (i.e.
obtaining a large number of images of cats, planes, horses etc), we utilize the test data. We partition
the test data into two, with one partition receiving 7500 images, and the other 2500. We describe this
in Table 2."
APPENDIX,0.7058823529411765,Table 2: Breakdown of dataset splits used in our experiments.
APPENDIX,0.7082352941176471,"Dataset
Training Set
Mean Computation Set
Test Set
CIFAR10
Not used
7500 images from test set
2500 images from Test set
CIFAR100
Not used
7500 images from test set
2500 images from Test set
Imagenet
Not used
40000 images from validation set
10000 images from validation set"
APPENDIX,0.7105882352941176,"Hyperparameters
For all experiments, the value of η as used in Algorithm 2 of the main manuscript
is chosen empirically, and varies for each experiment. The value for δ is chosen to be 0.15. The value
of ν is chosen to be 0.5. We state the hyperparameters used in or experiments using fine-tuning in
Table 3."
APPENDIX,0.7129411764705882,"Table 3: Hyperparameters chosen for fine-tuning pruning model with the CIFAR10 and Imagenet
datasets."
APPENDIX,0.7152941176470589,"Hyperparameter
Learning Rate
Batch Size
Epochs
Momentum
Scheduler
Weight Decay
Value/CIFAR10
0.001
128
50
0.05
Cosine
0.9
Value/Imagenet
0.001
256
70
10−6
Cosine
.0.99"
APPENDIX,0.7176470588235294,BASELINE METHODS
APPENDIX,0.72,"In this section, we briefly describe our modifications to common baselines."
APPENDIX,0.7223529411764706,"1. CHIP Sui et al. (2021): We ensure that the algorithm only has access to the validation set,
which is split into a 40000/10000 pair. When running the algorithm, we limit the number of
samples seen by the algorithm to 512 (1 batch of 512) for CIFAR10. For Imagenet results,
we reduce the number of repetitions to 1."
APPENDIX,0.7247058823529412,Published as a conference paper at ICLR 2023
APPENDIX,0.7270588235294118,"2. Taylor first order methods: we produce a bespoke implementation of the methods described
in Molchanov et al. (2019a;b). We allow the algorithm to see 512 samples for CIFAR 10 to
estimate gradients, before pruning. We treat this as a one-shot pruning method.
3. Other methods: for L1 and random pruning, we use native PyTorch implementations."
APPENDIX,0.7294117647058823,"E.1
SUMMARY OF EXPERIMENTAL RESULTS AND METHODOLOGY"
APPENDIX,0.731764705882353,"In this section, we provide tables summarizing our empirical results."
APPENDIX,0.7341176470588235,"We apply ITERTVSPRUNE to achieve accuracy drops of at most 1.5%, 5%, and 10%, on a variety of
architectures and datasets. We then check the accuracy observed when we prune the same networks,
on the same datasets, with baseline pruning algorithms. We detail this list below. All experiments do
not make use of the training set, except experiments where we apply CHIP Sui et al. (2021) toward
pruning the models."
APPENDIX,0.7364705882352941,"1. ResNet18/CIFAR10
Baselines: L1 Pruning, Random Pruning, Taylor First Order Pruning
2. ResNet50/CIFAR10
Baselines: L1 Pruning, Random Pruning, Taylor First Order
3. ResNet56/CIFAR10
Baselines: L1 Pruning, Random Pruning, Taylor First Order, CHIP Sui et al. (2021)
4. VGG16/CIFAR10
Baselines: L1 Pruning, Random Pruning, Taylor First Order, CHIP Sui et al. (2021)"
APPENDIX,0.7388235294117647,"E.2
CIFAR10 RESULTS"
APPENDIX,0.7411764705882353,"In this subsection, we detail our experiments on models trained using the CIFAR10 dataset."
APPENDIX,0.7435294117647059,"E.2.1
VGG MODELS"
APPENDIX,0.7458823529411764,"In this subsection, we compare VGG models trained on the CIFAR10 dataset. Specifically, we
compare the efficacy of ITERTVSPRUNE on VGG11, VGG16 and VGG19 models, with standard
baselines. In our experiment, we first use ITERTVSPRUNEmean to prune the models to a given
accuracy threshold; that is, we solve equation P for different values of t. Then, using the same sparsity
patterns, we apply L1 pruning, random pruning, and first-order gradient-based pruning respectively.
Note that here too, we do not fine tune the models after pruning. We plot these results in Figure 6.
For clarity, we also present these results in Table 4."
APPENDIX,0.7482352941176471,"Parametric sparsity
We observe that using ITERTVSPRUNE dramatically outperforms several
standard baselines. Indeed, we are able to prune more than 60% of parameters in VGG19 trained
on the CIFAR10 dataset with only a 2% drop in accuracy. We also observe, from table 4, that on
VGG16, ITERTVSPRUNE also outperforms contemporary baselines such as CHIPSui et al. (2021)."
APPENDIX,0.7505882352941177,"Layerwise Structured Sparsity
In Section 7.3, we report the parametric sparsity - that is, the
number of parameters pruned from the model using our proposed algorithms. In this section, we
consider the layerwise structured sparsity achieved using the ITERTVSPRUNE algorithm. In our
experiments, we observe that irrespective of accuracy threshold, most of the pruned filters lie in the
final layers of the VGG models, and almost no filters are pruned in the initial layers."
APPENDIX,0.7529411764705882,"Key Takeaways
The key takeaway of this slate of experiments is that VGG models trained on
CIFAR10 can be sparsified extensively with ITERTVSPRUNE. We see that our method consistently
outperforms the closest baselines, often by double digit percentages (in terms of test accuracy) for
the same sparsity."
APPENDIX,0.7552941176470588,"E.2.2
RESNET EXPERIMENTS"
APPENDIX,0.7576470588235295,"In this section, we detail our experiments with ResNet models trained on the CIFAR10 dataset. The
models we consider are the ResNet18, -20, -50, and -56 models. Our procedure mirrors that used"
APPENDIX,0.76,Published as a conference paper at ICLR 2023
APPENDIX,0.7623529411764706,"(a) Parametric Sparsity compari-
son for VGG-11 on CIFAR-10"
APPENDIX,0.7647058823529411,"(b) Parametric Sparsity compari-
son for VGG-16 on CIFAR-10"
APPENDIX,0.7670588235294118,"(c) Parametric Sparsity compari-
son for VGG-19 on CIFAR-10"
APPENDIX,0.7694117647058824,"(d) Structural Sparsity compari-
son for VGG-11 on CIFAR-10"
APPENDIX,0.7717647058823529,"(e) Structural Sparsity compari-
son for VGG-16 on CIFAR-10"
APPENDIX,0.7741176470588236,"(f) Structural Sparsity comparison
for VGG-19 on CIFAR-10"
APPENDIX,0.7764705882352941,"Figure 6: Comparison of accuracies of different pruning algorithms applied to VGG models trained
on CIFAR-10"
APPENDIX,0.7788235294117647,"(a) VGG-11 on CIFAR-10
(b) VGG-16 on CIFAR-10
(c) VGG-19 on CIFAR-10"
APPENDIX,0.7811764705882352,"(d) VGG-11 on CIFAR-100
(e) VGG-16 on CIFAR-100
(f) VGG-19 on CIFAR-100"
APPENDIX,0.7835294117647059,"Figure 7: Top row: Structured sparsity pattern after ITERTVSPRUNEfor VGG models trained on
CIFAR10. Bottom row: Structured sparsity pattern after ITERTVSPRUNEfor VGG models trained
on CIFAR100."
APPENDIX,0.7858823529411765,"in the previous section, with VGG nets: we use ITERTVSPRUNE to prune models with different
accuracy thresholds, and then apply baseline methods with the same sparsity patterns obtained via
ITERTVSPRUNE. We present the results of our experiments in Table 5."
APPENDIX,0.788235294117647,"Key takeaways
The key takeaway from this slate of experiments is that ITERTVSPRUNE is
effective on residual networks as well. However, it is not as effective on narrow resnet models trained
on CIFAR10 as it is on VGG models trained on the same dataset. We observe that the extent to"
APPENDIX,0.7905882352941176,Published as a conference paper at ICLR 2023
APPENDIX,0.7929411764705883,Table 4: Pruning VGG Models with no training dataset and no finetuning on CIFAR-10
APPENDIX,0.7952941176470588,"Model/Param. Sparsity
Pruning Method
Acc. Drop
Comp. with ITERTVSPRUNE"
APPENDIX,0.7976470588235294,"ITERTVSPRUNE
0.52 %
-
Random-3
2.12%
-1.6%
VGG-11/5.99%
L1
2.02%
-1.5%
Gradient-FO
0.32%
0.2%
CHIP
-
-"
APPENDIX,0.8,"ITERTVSPRUNE
3.52 %
-
Random-3
21.32%
-17.8%
VGG-11/24.0%
L1
24.02%
-20.5%
Gradient-FO
8.72%
-5.2%
CHIP
-
-"
APPENDIX,0.8023529411764706,"ITERTVSPRUNE
13.52 %
-
Random-3
61.32%
-47.8%
VGG-11/31.8%
L1
53.12%
-39.6%
Gradient-FO
33.92%
-20.4%
CHIP
-
-"
APPENDIX,0.8047058823529412,"ITERTVSPRUNE
1.9%
-
Random-3
50.6%
-48.7%
VGG-16/37.6%
L1
25.3%
-23.4%
Gradient-FO
10.6%
-6.5%
CHIP
4.1%
-2.2%"
APPENDIX,0.8070588235294117,"ITERTVSPRUNE
4.9%
-
Random-3
66.3%
-61.4%
VGG-16/52.5%
L1
35.6%
-30.7%
Gradient-FO
31.3%
-26.4%
CHIP
7.9%
-3.0%"
APPENDIX,0.8094117647058824,"ITERTVSPRUNE
12.9%
-
Random-3
72.7%
-69.8%
VGG-16/67.5%
L1
47.8
-34.9%
Gradient-FO
43.2%
-30.3%
CHIP
16.6%
-3.7%"
APPENDIX,0.8117647058823529,"ITERTVSPRUNE
1.3%
-
Random-3
22.9%
-21.6%
VGG-19/49.0%
L1
25.8%
-24.5%
Gradient-FO
6.0%
-4.7%
CHIP
-
-"
APPENDIX,0.8141176470588235,"ITERTVSPRUNE
4.3%
-
Random-3
30.0%
-27.7%
VGG-19/64.5%
L1
28.7%
-24.4%
Gradient-FO
8.7%
-4.4%
CHIP
-
-"
APPENDIX,0.8164705882352942,"ITERTVSPRUNE
70%
-
Random-3
51.8%
-40.5%
VGG-19/72.3%
L1
33.7%
-22.4%
Gradient-FO
18.6%
-7.3%
CHIP
-
-"
APPENDIX,0.8188235294117647,"which “narrow” ResNet models, such as ResNet20, can be sparsified is significantly less than “wider”
ResNet models, such as ResNet18, adapted for CIFAR10."
APPENDIX,0.8211764705882353,"E.3
CIFAR100 RESULTS"
APPENDIX,0.8235294117647058,"In this section, we detail our experiments on models trained on the CIFAR100 dataset. The architec-
tures considered were VGG11, VGG16, and VGG19. The experimental setup was identical to the
previously presented experiments with the same models trained on the CIFAR10 dataset. We plot
those results below in Figure 8, and provided a detailed snapshot in Table 6."
APPENDIX,0.8258823529411765,"E.4
EFFECT OF SIZE OF VALIDATION SET"
APPENDIX,0.8282352941176471,"We conduct a brief experiment to illustrate the extent to which the size of the set of samples used
to compute the class-conditional moments, and the MinTVS scores, affects the ranking in terms
of MinTVS. For a VGG16 model trained on the CIFAR10 dataset, we sample 1024, 768, and 512
samples, and compute the minα,β ∆α,β
l,j , where α and β are classes, l denotes the layer, and j denotes
the filter, as defined in Section 5 of the main manuscript. We present plots for three layers below."
APPENDIX,0.8305882352941176,Published as a conference paper at ICLR 2023
APPENDIX,0.8329411764705882,Table 5: Pruning ResNet Models with no training dataset and no finetuning on CIFAR-10
APPENDIX,0.8352941176470589,"Model/Param. Sparsity
Pruning Method
Acc. Drop
Comp. with ITERTVSPRUNE"
APPENDIX,0.8376470588235294,"ITERTVSPRUNE
1.50%
-
Random-3
13.21%
-11.71%
ResNet18/19.12%
L1
7.88%
-6.38%
Gradient-FO
4.56%
-3.06%"
APPENDIX,0.84,"ITERTVSPRUNE
4.99%
-
Random-3
-19.89%
-15.33%
ResNet18/33.91%
L1
14.92%
10.36%
Gradient-FO
-10.02%
-5.46%"
APPENDIX,0.8423529411764706,"ITERTVSPRUNE
10.0%
-
Random-3
38.91%
-28.90%
ResNet18/39.21%
L1
28.76%
-18.76%
Gradient-FO
18.44%
-8.43%"
APPENDIX,0.8447058823529412,"ITERTVSPRUNE
1.4%
-
Random-3
5.2%
-3.8%
ResNet20/2.4%
L1
3.5%
-18.76%
Gradient-FO
2.8%
-1.4%"
APPENDIX,0.8470588235294118,"ITERTVSPRUNE
4.7%
-
Random-3
13.2%
-8.5%
ResNet20/4.8%
L1
8.9%
-4.2%
Gradient-FO
6.23%
-1.63%"
APPENDIX,0.8494117647058823,"ITERTVSPRUNE
9.41%
-
Random-3
29.2%
-19.8%
ResNet20/7.1%
L1
17.3%
-7.89%
Gradient-FO
14.8
-5.39%"
APPENDIX,0.851764705882353,"ITERTVSPRUNE
1.50%
-
Random-3
25.5%
-24.0%
ResNet50/24.0%
L1
15.1%
-13.60%
Gradient-FO
9.82%
-7.32%"
APPENDIX,0.8541176470588235,"ITERTVSPRUNE
4.62%
-
Random-3
26.2%
-21.58%
ResNet50/29%
L1
17.3%
-14.68%
Gradient-FO
7.2%
-2.58%"
APPENDIX,0.8564705882352941,"ITERTVSPRUNE
9.94%
-
Random-3
44.45%
-34.51%
ResNet50/34.1%
L1
24.3%
-14.36%
Gradient-FO
14.25%
-4.31%"
APPENDIX,0.8588235294117647,"ITERTVSPRUNE
1.47%
-
Random-3
7.12%
-5.65%
ResNet56/3.4%
L1
5.46%
-3.09%
Gradient-FO
4.12%
-2.65%
CHIP
1.36
0.11"
APPENDIX,0.8611764705882353,"ITERTVSPRUNE
4.82%
-
Random-3
23.42%
-18.60%
ResNet56/7.6%
L1
N/A
-
Gradient-FO
9.41%
-4.59%
CHIP
5.56
-0.74"
APPENDIX,0.8635294117647059,"We observe that even when we reduce the number of samples by a factor of 2, we see that the ordering
of filters according to minα,β ∆l,j
α,β (and thus, MinTVS(l, j)) is retained even for comparatively
smaller sample sizes."
APPENDIX,0.8658823529411764,"E.5
EXPERIMENTS WITH FINE TUNING"
APPENDIX,0.8682352941176471,"In this section, we detail our results on using ITERTVSPRUNE in the setting wherein we have access
to the training set and loss function, thereby enabling us to fine-tune our models after pruning. Our
experiments involve fine-tuning models trained on CIFAR10 and Imagenet. Broadly speaking, our
experiments demonstrate two facts."
APPENDIX,0.8705882352941177,"1. Models pruned by our algorithm are able to return to the accuracy of the original pretrained
model,"
APPENDIX,0.8729411764705882,"2. Fine-tuning our models yield models that are competitive in accuracy with those obtained
using current, state-of-the-art methods such as Sui et al. (2021)."
APPENDIX,0.8752941176470588,Published as a conference paper at ICLR 2023
APPENDIX,0.8776470588235294,"(a) Parametric Sparsity compari-
son for VGG11 on CIFAR10"
APPENDIX,0.88,"(b) Parametric Sparsity compari-
son for VGG16 on CIFAR10"
APPENDIX,0.8823529411764706,"(c) Parametric Sparsity compari-
son for VGG19 on CIFAR10"
APPENDIX,0.8847058823529412,"(d) Structural Sparsity compari-
son for VGG11 on CIFAR10"
APPENDIX,0.8870588235294118,"(e) Structural Sparsity compari-
son for VGG16 on CIFAR10"
APPENDIX,0.8894117647058823,"(f) Structural Sparsity comparison
for VGG19 on CIFAR100"
APPENDIX,0.8917647058823529,"Figure 8: Comparison of accuracies of different pruning algorithms applied to VGG models trained
on CIFAR100"
APPENDIX,0.8941176470588236,"(a) minα,β ∆α,β
l,j
for layer 0 of
VGG16"
APPENDIX,0.8964705882352941,"(b) minα,β ∆α,β
l,j
for layer 1 of
VGG16"
APPENDIX,0.8988235294117647,"(c) minα,β ∆α,β
l,j
for layer 2 of
VGG16"
APPENDIX,0.9011764705882352,"Figure 9: Effect of sample size on ranking of minα,β ∆α,β
l,j"
APPENDIX,0.9035294117647059,"E.5.1
RESULTS OF PRUNING WITH FINE TUNING ON CIFAR10"
APPENDIX,0.9058823529411765,"We now present the results of fine-tuning models trained on the CIFAR10 dataset. We chose to
fine-tune VGG16 models pruned using ITERTVSPRUNE and CHIP(Sui et al., 2021), and a ResNet18
model pruned with ITERTVSPRUNE. We detail our results below in Table 7."
APPENDIX,0.908235294117647,"We
observe
that
after
fine-tuning
for
50
epochs,
we
are
able
to
recover
the
accuracy
of
the
original
model,
even
when
the
model
is
pruned
substan-
tially,
with
67.5%
of
parameters
removed,
as
is
the
case
with
VGG16."
APPENDIX,0.9105882352941177,"E.5.2
RESULTS OF PRUNING WITH FINE TUNING ON IMAGENET"
APPENDIX,0.9129411764705883,"We now present the results of fine-tuning models trained on the Imagenet dataset. We chose to
fine-tune VGG16 models pruned using ITERTVSPRUNE and CHIP(Sui et al., 2021), and a ResNet18
model pruned with ITERTVSPRUNE. We detail our results below in Table 8."
APPENDIX,0.9152941176470588,"As was the case with models trained on CIFAR10, we observe that our models recover the original
accuracy after substantial fine-tuning. However, the extent of fine-tuning required to achieve this is
substantially greater for models trained on Imagenet than for models trained on CIFAR10."
APPENDIX,0.9176470588235294,Published as a conference paper at ICLR 2023
APPENDIX,0.92,Table 6: Pruning VGG Models with no training dataset and no finetuning on CIFAR-100
APPENDIX,0.9223529411764706,"Model Name /Param. Sparsity
Pruning Method
Acc. Drop
Comp. w/ ITERTVSPRUNE"
APPENDIX,0.9247058823529412,"ITERTVSPRUNE
1.6 %
-
Random-3
4.2%
-2.6%
VGG-11/5.99%
L1
5.7%
-4.1%
Gradient-FO
2.4%
-0.8%
CHIP
-
-"
APPENDIX,0.9270588235294117,"ITERTVSPRUNE
5.6 %
-
Random-3
30.4%
-24.8%
VGG-11/17.9%
L1
21.1%
-15.5%
Gradient-FO
12.8%
-7.2%
CHIP
-
-"
APPENDIX,0.9294117647058824,"ITERTVSPRUNE
12.6 %
-
Random-3
45.4%
-32.8%
VGG-11/23.3%
L1 37.2%
-24.6%
Gradient-FO
24.0%
-11.4%
CHIP
-
-"
APPENDIX,0.9317647058823529,"ITERTVSPRUNE
5.56 %
-
Random-3
42.74%
-37.18%
VGG-16/23.6% %
L1
34.26%
-28.7%
Gradient-FO
21.96%
-16.4%
CHIP
-
-"
APPENDIX,0.9341176470588235,"ITERTVSPRUNE
4.9%
-
Random-3
66.3%
-61.4%
VGG-16 / 31.9 %
L1
35.6%
-30.7%
Gradient-FO
31.3%
-26.4%
CHIP
-
-"
APPENDIX,0.9364705882352942,"ITERTVSPRUNE
18.56 %
-
Random-3
50.08%
-31.52%
VGG-16 / 40.2%
L1
40.38%
-21.82%
Gradient-FO
32.36%
-13.8%
CHIP
-
-"
APPENDIX,0.9388235294117647,"ITERTVSPRUNE
0.2%
-
Random-3
38.3%
-38.1%
VGG-19 / 37.8%
L1
26.6%
-26.4%
Gradient-FO
6.2%
-6.0%
CHIP
-
-"
APPENDIX,0.9411764705882353,"ITERTVSPRUNE
5.2%
-
Random-3
40.9%
-35.7%
VGG-19 59.0%
L1
29.8%
-24.6%
Gradient-FO
12.1%
-6.9%
CHIP
-
-"
APPENDIX,0.9435294117647058,"ITERTVSPRUNE
18.2%
-
Random-3
41.1%
-22.9%
VGG-19 72.3%
L1
34.7%
-16.5%
Gradient-FO
22.8%
-4.6%
CHIP
-
-"
APPENDIX,0.9458823529411765,"Table 7: Accuracy of Fine-Tuned models trained on CIFAR10. “Acc. Drop w/o FT” refers to the
accuracy drop obtained without fine-tuning the model post pruning, and “Acc. Drop w/ FT ” denotes
the accuracy drop after retraining the model."
APPENDIX,0.9482352941176471,"Model Name /Param. Sparsity
Pruning Method
Acc. Drop w/o FT
Acc. Drop w/ FT"
APPENDIX,0.9505882352941176,"VGG-16/67.5%
ITERTVSPRUNE
12.9 %
0.21%
VGG-16/67.5%
CHIP
16.6%
0.64%
ResNet18/39.2%
ITERTVSPRUNE
10.0%
0.06%"
APPENDIX,0.9529411764705882,"E.6
VALIDATING USING MinTVS FOR PRUNING"
APPENDIX,0.9552941176470588,"In this section, we conduct experiments validating the us MinTVS scores, as well as the key
hypothesis of the paper. We show that for models trained on the CIFAR10 dataset, pruning filters
with the smallest MinTVS scores achieves superior accuracy compared to pruning filters with the
largest MinTVS scores. We detail this experiment below."
APPENDIX,0.9576470588235294,"E.6.1
EXPERIMENTAL SETUP"
APPENDIX,0.96,"We consider VGG16 and VGG19 models trained on the CIFAR10 dataset. We consider models with
LDIFF scores that are greater than 0.4; that is, LDIFF(ηl, l) > 0.4, where ηl = 0.75MinTVS(l) is"
APPENDIX,0.9623529411764706,Published as a conference paper at ICLR 2023
APPENDIX,0.9647058823529412,"Table 8: Accuracy of Fine-Tuned models trained on Imagenet. “Acc. Drop w/o FT” refers to the
accuracy drop obtained without fine-tuning the model post pruning, and “Acc. Drop w/ FT ” denotes
the accuracy drop after retraining the model."
APPENDIX,0.9670588235294117,"Model Name /Param. Sparsity
Pruning Method
Acc. Drop w/o FT
Acc. Drop w/ FT"
APPENDIX,0.9694117647058823,"ResNet50/24.7%
ITERTVSPRUNE
31.2 %
0.03%
ResNet50/24.7%
CHIP
34.2%
0.26%"
APPENDIX,0.971764705882353,"0.75 times the mean MinTVS score for the layer. The partition of the validation set for accuracy
measurement and score computation is given in 2. We do so since the variation in MinTVS scores
for layers with low LDIFF scores is minimal; thus, there is little difference between the largest scores
and the smallest."
APPENDIX,0.9741176470588235,"E.6.2
EXPERIMENTAL RESULTS"
APPENDIX,0.9764705882352941,"We now present our experimental results. We observe that pruning discriminative filters in general
yields mode dramatic drops in accuracy when compared to pruning non-discriminative filters, though
the effect is slightly less for deeper layers (those closer to the output). This observation supports
the hypothesis that, given layers with a mix of discriminative and non-discriminative filters, we can
safely prune the non-discriminative filters, but not the former. We present these results in Figure 10."
APPENDIX,0.9788235294117648,"(a) VGG16, 10% of filters
pruned"
APPENDIX,0.9811764705882353,"(b) VGG16, 25% of filters
pruned"
APPENDIX,0.9835294117647059,"(c) VGG16, 50% of filters
pruned"
APPENDIX,0.9858823529411764,"(d) VGG16, 75% of filters
pruned"
APPENDIX,0.9882352941176471,"(e) VGG19, 10% of filters
pruned"
APPENDIX,0.9905882352941177,"(f) VGG19, 25% of filters
pruned"
APPENDIX,0.9929411764705882,"(g) VGG19, 50% of filters
pruned"
APPENDIX,0.9952941176470588,"(h) VGG19, 75% of filters
pruned"
APPENDIX,0.9976470588235294,"Figure 10: Comparison of accuracies of when pruning fractions of filters with least MinTVS scores
with pruning fraction of filters with greatest MinTVS scores for VGG16 and VGG19 trained on
CIFAR10. “Dist. max” refers to accuracies when most discriminative filters are pruned, and “Dist.
min” refers to accuracies when least discriminative filters are pruned"
