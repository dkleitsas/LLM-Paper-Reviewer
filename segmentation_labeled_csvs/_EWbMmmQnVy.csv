Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.005319148936170213,"The increased interest and importance of explaining neural networks’ predictions, especially
in the medical community, associated with the known unreliability of saliency maps, the
most common explainability method, has sparked research into other types of explanations.
Natural Language Explanations (NLEs) emerge as an alternative, with the advantage of
being inherently understandable by humans and the standard way that radiologists explain
their diagnoses. We extend upon previous work on NLE generation for multi-label chest
X-ray diagnosis by replacing the traditional decoder-only NLE generator with an encoder-
decoder architecture. This constitutes a first step towards Reinforcement Learning-free
adversarial generation of NLEs when no (or few) ground-truth NLEs are available for
training, since the generation is done in the continuous encoder latent space, instead of
in the discrete decoder output space. However, in the current scenario, large amounts of
annotated examples are still required, which are especially costly to obtain in the medical
domain, given that they need to be provided by clinicians. Thus, we explore how the recent
developments in Parameter-Efficient Fine-Tuning (PEFT) can be leveraged for this use-
case. We compare different PEFT methods and find that integrating the visual information
into the NLE generator layers instead of only at the input achieves the best results, even
outperforming the fully fine-tuned encoder-decoder-based model, while only training 12% of
the model parameters. Additionally, we empirically demonstrate the viability of supervising
the NLE generation process on the encoder latent space, thus laying the foundation for RL-
free adversarial training in low ground-truth NLE availability regimes. The code is publicly
available at https://github.com/icrto/peft-nles.
Keywords: parameter-efficient, natural language explanations, chest x-ray diagnosis"
INTRODUCTION,0.010638297872340425,1. Introduction
INTRODUCTION,0.015957446808510637,"The need for providing explanations is essential in domains like medicine, given the life-or-
death nature of the decisions involved (Vayena et al., 2018). Given their training-free nature,
saliency maps are, by far, the most commonly employed explanation method, despite their
known problems (Adebayo et al., 2018; Camburu et al., 2019; Rudin, 2019).
Among other explanation modalities are Natural Language Explanations (NLEs). These
are textual descriptions of the decision-making process of a neural network for a given input"
INTRODUCTION,0.02127659574468085,Rio-Torto Cardoso Teixeira
INTRODUCTION,0.026595744680851064,"image, thus combining image descriptions (e.g.
obtained via image captioning models)
- image-relevance - and model predictions - class-relevance (Hendricks et al., 2016; Rio-
Torto et al., 2022).
NLEs are intrinsically understandable by humans and can provide
complementary insights to those of other types of explanations. For example, while visual
explanations are more spatially precise, NLEs can be more semantically meaningful (Rio-
Torto et al., 2022).
In the medical domain in general, and in chest X-ray diagnosis in
particular, NLEs can be even more useful, given that they correspond to how radiologists
explain their diagnoses (Gale et al., 2018; Miller, 2019).
There are several works on NLEs for natural images (Hendricks et al., 2016; Park et al.,
2018; Wu and Mooney, 2019; Do et al., 2020; Marasovi´c et al., 2020; Kayser et al., 2021;
Majumder et al., 2022; Pl¨uster et al., 2022; Sammani et al., 2022) and for text-only models
(Camburu et al., 2018; Rajani et al., 2019; Kumar and Talukdar, 2020; Narang et al., 2020);
they usually apply the traditional cross entropy loss on the generated NLEs and condition
the NLE generator on information from the predictions of the model that is being explained.
We highlight the work by Hendricks et al. (Hendricks et al., 2016) since it explicitly imposes
class-relevance on the generated NLEs through the proposal of a Reinforcement Learning-
based (RL) loss. Inspired by this and by the work on adversarial text generation (Donahue
and Rumshisky, 2018), we propose replacing the traditional decoder-only NLE
generator with an encoder-decoder architecture (see Figure 1). This paves the way
for the RL-free adversarial generation of NLEs, when no (or few) ground-truth data is
available to train the model.
In the medical domain, text generation mostly consists of report generation approaches
(Hou et al., 2021; Monshi et al., 2020; Ouis and A. Akhloufi, 2024). To the best of our
knowledge, Kayser et al. (Kayser et al., 2022) is the first and only work on NLE generation
for the medical domain. Therefore, we build upon this work; namely, we use the dataset
the authors introduce, MIMIC-NLE, and their proposed evaluation framework.
However, contrary to the models proposed by Kayser et al. (Kayser et al., 2022), we
adopt Parameter-Efficient Fine-Tuning (PEFT) when training our NLE genera-
tor to receive visual features and classification predictions. Instead of fine-tuning all model
parameters, PEFT methods fine-tune only a small percentage, while still performing com-
petitively (Zhang et al., 2023). A plethora of PEFT methods has been proposed, not only
for single modality use-cases (Rebuffi et al., 2017; Houlsby et al., 2019; Lester et al., 2021;
Li and Liang, 2021; Liu et al., 2021; Chen et al., 2023; Yu et al., 2023; Zhang et al., 2023),
but also for multi-modal applications (Luo et al., 2023; Zhang et al., 2023; Gao et al., 2024),
and in the medical domain as well (Dutt et al., 2023). Applying PEFT to NLE generation
is still an emergent topic, with work only on uni-modal applications (Solano et al., 2023).
Therefore, our contributions are the following:"
INTRODUCTION,0.031914893617021274,"• Replacement of the decoder-only NLE generator of Kayser et al. (Kayser et al., 2022)
by an encoder-decoder architecture that lays the foundation for the RL-free adversarial
generation of NLEs when no (or few) ground-truth data is available during training."
INTRODUCTION,0.03723404255319149,"• Introduction of the first work exploring different PEFT methods for NLE generation
for chest X-ray classification."
INTRODUCTION,0.0425531914893617,PEFT Generation of NLEs for Chest X-ray Classification
INTRODUCTION,0.047872340425531915,"Vision
Feature
Extractor"
INTRODUCTION,0.05319148936170213,Vision
INTRODUCTION,0.05851063829787234,Classifier
INTRODUCTION,0.06382978723404255,"0
1
0
0
1
0
0"
INTRODUCTION,0.06914893617021277,"NLE
Generator"
INTRODUCTION,0.07446808510638298,Encoder
INTRODUCTION,0.0797872340425532,"NLE
Generator"
INTRODUCTION,0.0851063829787234,Decoder
INTRODUCTION,0.09042553191489362,Findings compatible with pulmonary
INTRODUCTION,0.09574468085106383,congestion Edema
INTRODUCTION,0.10106382978723404,"Figure 1: Block diagram of our NLE generation architecture for the pathology ”Edema“.
The encoder receives the image features and the prediction information, while
the decoder generates the NLE from the latent space of the encoder."
IMPLEMENTATION/METHODS,0.10638297872340426,2. Methodology
IMPLEMENTATION/METHODS,0.11170212765957446,2.1. MIMIC-NLE Dataset
IMPLEMENTATION/METHODS,0.11702127659574468,"NLEs are fundamentally different from image descriptions, since the former need to be not
only image-relevant but also decision-relevant, i.e. take into account the decision of the
underlying model they are trying to explain (Hendricks et al., 2016; Rio-Torto et al., 2022).
Although there are some datasets for chest X-ray diagnosis that include medical reports
in text form (i.e. image descriptions) (Demner-Fushman et al., 2015; Johnson et al., 2019;
Bustos et al., 2020), the work by Kayser et al. (Kayser et al., 2022) was the first and, to
the best of our knowledge, the only one to propose a dataset with NLEs.
The MIMIC-NLE dataset
(Kayser et al., 2022) is obtained automatically from the
MIMIC-CXR dataset (Johnson et al., 2019) by using a BERT-based labeler in conjunction
with clinically validated extraction rules. The dataset comprises 38003 image-NLE pairs
or 44935 image-diagnosis-NLE triplets (some NLEs explain multiple diagnoses). Note that
each image can simultaneously be associated with 10 pathologies (Atelectasis, Consolidation,
Edema, Enlarged Cardiomediastinum, Lung Lesion, Lung Opacity, Pleural Effusion, Pleural
Other, Pneumonia, and Pneumothorax). The data is split into 37016, 273, and 714 NLEs
for training, validation and testing, respectively."
IMPLEMENTATION/METHODS,0.12234042553191489,2.2. Baselines
IMPLEMENTATION/METHODS,0.1276595744680851,"Besides introducing MIMIC-NLE, Kayser et al. (Kayser et al., 2022) also tested different
architectures to generate NLEs for this multi-label scenario. These architectures, which we
describe below, are used here as baselines to compare our proposals with. All architectures
work in a similar fashion: a vision model (DenseNet-121 (Huang et al., 2017)) extracts
features and classifies the input image; the features and a vector representation of the
prediction, together with the pathology for which the NLE is being generated, are fed
into an autoregressive language model, which generates the NLE. During training, instead
of using the prediction vector, the ground-truth label is used, and the vision model is
trainable. Thus, this can be considered an in-model explanation architecture that works
within a predict-explain paradigm (Do et al., 2020).
RAdiological Text Captioning for Human Examined Thoraxes (RATCHET) was first
proposed by Hou et al. (Hou et al., 2021) as a chest X-ray captioning method. Kayser et
al. (Kayser et al., 2022) leverage this architecture to generate NLEs instead. It consists of"
IMPLEMENTATION/METHODS,0.13297872340425532,Rio-Torto Cardoso Teixeira
IMPLEMENTATION/METHODS,0.13829787234042554,"a GPT-2 (Radford et al., 2018) model with cross-attention layers, that receive the vision
classifier’s features concatenated with a projection of the prediction vector, as depicted in
Figure 2 (top left). As input, the model receives the NLE-related pathology.
DPT (DenseNet-121 + GPT-2) is proposed by Kayser et al. (Kayser et al., 2022) and it
consists of a GPT-2 model that receives as input the vision features, the prediction vector,
and the NLE-related pathology. An illustration of this architecture can be found in Figure
2 (top right)."
IMPLEMENTATION/METHODS,0.14361702127659576,2.3. Proposed Approach
IMPLEMENTATION/METHODS,0.14893617021276595,"Architecture
Our approach builds upon the work of Kayser et al. (Kayser et al., 2022),
but replaces the NLE decoder with a denoising auto-encoder (see Figure 1). Not only does
this increase the representation capabilities of the NLE generator, but, most importantly,
it lays the foundation for the RL-free adversarial generation of NLEs when no (or few)
ground-truth NLEs are available for training. In this scenario, teacher forcing is unfeasible.
While an adversarial approach can solve the problem, if the generator is a decoder-only
architecture (e.g. DPT), RL is needed, since its output (and the input to the discriminator)
is discrete and, as such, no gradient would flow from the discriminator to the generator
(Donahue and Rumshisky, 2018).
By using an encoder-decoder architecture, where the
adversarial learning happens on the continuous latent space of the encoder at the sentence
level, gradient propagation is ensured without needing RL, thus making the training process
faster (obtaining an NLE now only involves a single encoder forward pass instead of N
decoder passes, where N is the number of tokens in the generated sentence) and more
stable.
Finally, decoding this latent space into text to obtain the NLE is still possible
thanks to the pre-trained decoder.
We use the auto-encoder proposed by (Montero et al., 2021), which consists of a frozen
BERT-based encoder, a multi-head attention mechanism as a bottleneck layer, and a single
transformer decoder layer with cross attention. Since this cross attention layer receives the
same input at every timestep, the authors introduce a gating mechanism that controls the
amount of information to keep at each timestep. Trained on MIMIC-NLE (Kayser et al.,
2022), it achieves a reconstruction BLEU-4 score of 93.9."
IMPLEMENTATION/METHODS,0.15425531914893617,"PEFT
Our auto-encoder is trained on text inputs, but it needs to be adapted to incor-
porate the products of the vision model we want to explain. Recently, the transfer learning
paradigm has shifted from fine-tuning entire models to only certain layers. This is especially
important in NLE generation, and even more so in post-model methods, where given a pre-
trained model, we want to obtain explanations for its decision process with the shortest
training time possible. Therefore, we experiment with applying several PEFT techniques
to the encoder of our NLE Generator, while keeping its decoder module frozen.
These
methods are depicted in Figure 2 (last three rows).
In total, we try five PEFT methods, four of which are built on top of an input con-
figuration similar to that of DPT, i.e. where both image features, prediction vector and
associated pathology are given as inputs to the encoder.
LoRA (Yu et al., 2023) represents, through low-rank decomposition, the model’s weight
updates into two smaller matrices, which are trained, while the original model weights are
kept frozen. After training, both weight matrices are combined."
IMPLEMENTATION/METHODS,0.1595744680851064,PEFT Generation of NLEs for Chest X-ray Classification
IMPLEMENTATION/METHODS,0.16489361702127658,NLE Generator Decoder Layer 0 …
IMPLEMENTATION/METHODS,0.1702127659574468,Visual Features
IMPLEMENTATION/METHODS,0.17553191489361702,NLE Generator Decoder Layer 1
IMPLEMENTATION/METHODS,0.18085106382978725,NLE Generator Decoder Cross Attn 1
IMPLEMENTATION/METHODS,0.18617021276595744,NLE Generator Decoder Layer N
IMPLEMENTATION/METHODS,0.19148936170212766,"Findings compatible with pulmonary
BOS Pred"
IMPLEMENTATION/METHODS,0.19680851063829788,"SEP
Edema"
IMPLEMENTATION/METHODS,0.20212765957446807,congestion
IMPLEMENTATION/METHODS,0.2074468085106383,NLE Generator Decoder Cross Attn 0
IMPLEMENTATION/METHODS,0.2127659574468085,"BOS
Visual Features
Pred
SEP
SEP
Findings compatible with pulmonary
SEP
Edema"
IMPLEMENTATION/METHODS,0.21808510638297873,NLE Generator Decoder
IMPLEMENTATION/METHODS,0.22340425531914893,congestion
IMPLEMENTATION/METHODS,0.22872340425531915,"NLE 
Generator"
IMPLEMENTATION/METHODS,0.23404255319148937,Encoder
IMPLEMENTATION/METHODS,0.2393617021276596,"BOS
Visual Features
Pred
SEP
SEP
SEP
Edema LoRA LoRA"
IMPLEMENTATION/METHODS,0.24468085106382978,Findings compatible with pulmonary
IMPLEMENTATION/METHODS,0.25,"NLE
Generator"
IMPLEMENTATION/METHODS,0.2553191489361702,Decoder BOS
IMPLEMENTATION/METHODS,0.26063829787234044,congestion
IMPLEMENTATION/METHODS,0.26595744680851063,"BOS
Visual Features
Pred
SEP
SEP
SEP
Edema"
IMPLEMENTATION/METHODS,0.2712765957446808,NLE Generator Encoder Layer N
IMPLEMENTATION/METHODS,0.2765957446808511,NLE Generator Encoder Layer 0 …
IMPLEMENTATION/METHODS,0.28191489361702127,NLE Generator Encoder Layer 1
IMPLEMENTATION/METHODS,0.2872340425531915,Findings compatible with pulmonary
IMPLEMENTATION/METHODS,0.2925531914893617,"NLE
Generator"
IMPLEMENTATION/METHODS,0.2978723404255319,Decoder BOS
IMPLEMENTATION/METHODS,0.30319148936170215,congestion
IMPLEMENTATION/METHODS,0.30851063829787234,Soft Prompt
IMPLEMENTATION/METHODS,0.31382978723404253,"RATCHET
DPT LoRA"
IMPLEMENTATION/METHODS,0.3191489361702128,Prefix/Prompt Tuning & uni-modal LLaMA-Adapter
IMPLEMENTATION/METHODS,0.324468085106383,Multi-modal LLaMA-Adapter
IMPLEMENTATION/METHODS,0.32978723404255317,"Visual Features
Pred"
IMPLEMENTATION/METHODS,0.3351063829787234,Vision-to-Adapter
IMPLEMENTATION/METHODS,0.3404255319148936,NLE Generator Encoder Layer N
IMPLEMENTATION/METHODS,0.34574468085106386,NLE Generator Encoder Layer 0 …
IMPLEMENTATION/METHODS,0.35106382978723405,NLE Generator Encoder Layer 1
IMPLEMENTATION/METHODS,0.35638297872340424,"BOS
SEP
Edema
Findings compatible with pulmonary"
IMPLEMENTATION/METHODS,0.3617021276595745,"NLE
Generator"
IMPLEMENTATION/METHODS,0.3670212765957447,Decoder BOS
IMPLEMENTATION/METHODS,0.3723404255319149,Soft Prompt
IMPLEMENTATION/METHODS,0.3776595744680851,congestion
IMPLEMENTATION/METHODS,0.3829787234042553,"Figure 2: The different types of (PEFT) architectures of the NLE generator. Trainable
layers/parameters are represented by the fire icon, while frozen blocks are repre-
sented by the snowflake."
IMPLEMENTATION/METHODS,0.3882978723404255,"Prompt and Prefix Tuning (Lester et al., 2021; Li and Liang, 2021) insert a given number
of learnable soft prompts into the frozen model. While Prompt Tuning (Lester et al., 2021)
only appends these prompts to the input embeddings, Prefix Tuning (Li and Liang, 2021)
applies these prompts to several layers of the frozen model (in our case, to all layers)."
IMPLEMENTATION/METHODS,0.39361702127659576,"LlaMA-Adapter (Zhang et al., 2023) was proposed to efficiently fine-tune a large lan-
guage model, LlaMA (Touvron et al., 2023), and it differs from Prefix Tuning in the way the
learnable adapters are merged into the frozen model. The authors propose to separate the
adapter prompts from the word tokens and introduce a zero-initialized gating mechanism
that adaptively controls the influence of the adapter prompts during training. This way,"
IMPLEMENTATION/METHODS,0.39893617021276595,Rio-Torto Cardoso Teixeira
IMPLEMENTATION/METHODS,0.40425531914893614,"in the early training stages, the influence of the adapter prompt is very small, and it is
progressively increased, which guarantees training stability while still ensuring the model is
gradually being adapted and simultaneously retaining the knowledge of the original model.
However, none of the aforementioned methods take into account the multi-modality of
our scenario, as they were developed for uni-modal applications. To tackle this, the authors
of LlaMA-Adapter also propose a multi-modal version. Instead of providing the image-
related information through the input of the model, it is directly added to the adaptation
tokens. To ensure the image features have the same dimensions as the adaptation prompts,
a Transformer encoder is used, as shown in Figure 2 (last row)."
IMPLEMENTATION/METHODS,0.4095744680851064,"NLE Generation Loss Function
As previously mentioned, using an encoder-decoder
architecture opens the door to the RL-free adversarial generation of NLEs in low ground-
truth availability regimes. To test the viability of such an approach despite having, as we
do in this work, ground-truth NLEs for the whole dataset, we experiment with supervis-
ing the NLE generation directly in the latent space of the encoder, as this is where the
adversarial learning would happen. Thus, in this experiment, we replace the cross entropy
loss computed between the discrete output of the decoder and the ground-truth NLE, by
the Mean Squared Error (MSE) computed between the encoder latent representation of the
ground-truth NLE and the continuous output of the encoder."
RESULTS/EXPERIMENTS,0.4148936170212766,3. Experimental Setup
RESULTS/EXPERIMENTS,0.42021276595744683,"Evaluation Metrics
We follow the evaluation protocol proposed in the MIMIC-NLE
paper (Kayser et al., 2022); several aspects are evaluated, namely the chest X-ray multi-
label classification performance (through balanced AUC - Area Under the Curve), and
the NLEs’ quality (only for correctly predicted instances). Given that there is evidence
that automated natural language generation (NLG) metrics correlate badly with the way
humans evaluate how similar two NLEs are (Kayser et al., 2021), because two NLEs can
be simultaneously correct while having very different syntactic forms, the authors propose
the CLEV (CLinical EVidence) score. This score uses the CheXbert labeler (Smit et al.,
2020) to verify if an NLE refers to the same clinical evidence as the ground-truth NLE.
Thus, it corresponds to the accuracy regarding clinical evidence.
Several NLG metrics
are considered, with a bigger importance given to BERTScore (Zhang et al., 2020) and
METEOR (Lavie and Agarwal, 2007) since it has been shown that these exhibit the highest
correlation with human evaluation (Kayser et al., 2021)."
RESULTS/EXPERIMENTS,0.425531914893617,"Implementation Details
Regarding the pre-training of the auto-encoder, the decoder
is a single transformer layer, as proposed in the original AUTOBOT paper (Montero et al.,
2021), and the encoder is the CXR-BERT model (Boecking et al., 2022). CXR-BERT is
a BERT masked language model trained on PubMed abstracts (U.S. National Library of
Medicine), clinical notes from MIMIC-III (Johnson et al., 2016) and MIMIC-CXR (Johnson
et al., 2019). The model is trained with the AdamW optimizer for 1 million steps with a
batch size of 64 and a linearly decayed learning rate of 0.001 with 1000 warmup steps. The
best model is chosen based on the reconstruction BLEU-4.
For the explanation generation model, we follow the training details described in the
MIMIC-NLE paper (Kayser et al., 2022): for RATCHET, the learning rate is 5×10−5, and"
RESULTS/EXPERIMENTS,0.4308510638297872,PEFT Generation of NLEs for Chest X-ray Classification
RESULTS/EXPERIMENTS,0.43617021276595747,"5 × 10−4 for all remaining experiments. All models are trained with the AdamW optimizer
for 50 epochs with a batch size of 16 and a linearly decayed learning rate with 1000 warmup
steps. We use Early Stopping with a patience of 25.
LoRA parameters follow the original LoRA paper (Yu et al., 2023), i.e. an alpha of
32 and a dropout of 0.01. For the remaining PEFT methods, the learnable prompts have
10 tokens (as in the original LlaMA-Adapter (Zhang et al., 2023)), and for both LlaMA-
Adapter models, the prompts are injected at every layer of the NLE generator encoder.
The transformer used in the multi-modal LlaMA-Adapter has the following configuration:
2 transformer layers with 4 attention heads, an embedding dimension of 768, and the linear
layers have 3072 neurons.
The best model and corresponding hyperparameters are chosen with the same criteria
as in the MIMIC-NLE paper (Kayser et al., 2022): the product of the classification score
(balanced AUC), CLEV score, and the average of BERTScore (Zhang et al., 2020) and
METEOR (Lavie and Agarwal, 2007), computed on the validation set.
We learn the weighting scheme of the classification and text losses, following the method-
ology proposed in (Cipolla et al., 2018). Thus, our loss function is given by L = Lclf +
1
σ2 Ltext + log(σ), where σ is learned during training and has a starting value of 1.0."
RESULTS/EXPERIMENTS,0.44148936170212766,4. Results and Discussion
RESULTS/EXPERIMENTS,0.44680851063829785,"Table 1 contains the results of the different NLE generation frameworks evaluated in terms of
balanced AUC, CLEV and NLG metrics. We also present the number of correctly classified
instances (for which the CLEV and NLG metrics are computed), the number of additional
trainable parameters introduced, and the overall performance.
Besides the base DenseNet-121 model, we present the results of two of the models of
(Kayser et al., 2022), RATCHET and DPT, trained with our loss weighting scheme. The
obtained results follow the trends verified in the original paper, i.e. RATCHET improves
AUC over the DenseNet-121, while DPT does not, and the former is better overall than the
latter.
Regarding our proposals, replacing the decoder-only NLE generator (i.e. DPT) with
the encoder-decoder (Base AE) improves the overall performance: while the AUC slightly
decreases, the CLEV and BERT scores increase. However, this comes at a cost: 145M
trainable parameters are added, 20M more than DPT introduces.
Uni-modal PEFT techniques slightly decrease or perform on par with the Base AE model
in terms of overall performance. However, these methods introduce much less trainable
parameters (11M in the worst case). Given the small loss in performance but the high gain
in training efficiency, PEFT methods are, in fact, suitable for NLE generation for chest
X-ray diagnosis, as had already been shown in other domains. Our results also validate the
findings of the original LlaMA-Adapter (Zhang et al., 2023), i.e. that the zero-initialized
gating mechanism surpasses traditional Prefix Tuning.
The multi-modal version of LLaMA-Adapter achieves the best overall results among
our proposals, even surpassing the fully fine-tuned model. This confirms our hypothesis
that incorporating the visual information throughout the NLE generator layers instead of
just at the input is beneficial. Indeed, this is probably the same reason why RATCHET
outperforms DPT. This result demonstrates that explainability and task performance are"
RESULTS/EXPERIMENTS,0.4521276595744681,Rio-Torto Cardoso Teixeira
RESULTS/EXPERIMENTS,0.4574468085106383,"Table 1: Results obtained on the MIMIC-NLE (Kayser et al., 2022) test set. Evaluation
metrics include the balanced AUC, the CLEV score, and several NLG metrics.
CLEV and NLG metrics are only computed for correctly classified instances. This
value is presented in column # instances, for which the total is 835 (one NLE
can explain more than one prediction). The # params column corresponds to the
number of trainable parameters (in millions, M) relative to the 7M DenseNet-121
model. The last column shows the overall performance, given by the product of
the AUC, the CLEV score, and the average of the BERT and METEOR scores.
In bold are the best results for each metric across all models, while underlined are
the best results among our models."
RESULTS/EXPERIMENTS,0.4627659574468085,"Model
AUC
CLEV
BERTS
METEOR
BLEU-1
BLEU-4
ROUGE
CIDEr
SPICE
# instances
# params. (↓)
overall"
RESULTS/EXPERIMENTS,0.46808510638297873,"DenseNet-121
64.45
7M
RATCHET
65.19
74.81
77.49
13.28
19.92
3.933
21.53
37.71
20.04
540
+90.5M
221337
DPT
63.24
73.90
77.97
13.57
21.02
4.364
22.65
40.67
19.25
498
+125M
213903"
RESULTS/EXPERIMENTS,0.4734042553191489,"Base AE
63.01
74.19
78.42
13.10
15.31
2.497
16.58
11.81
15.02
527
+145M
213915
LoRA AE
63.98
73.99
77.51
15.32
20.38
4.002
20.60
24.44
20.11
546
+1.1M
219723
Prompt AE
61.31
74.49
77.16
14.43
19.49
3.773
19.76
23.48
18.10
541
+0.8M
209145
Prefix AE
65.19
67.96
77.56
14.70
20.06
3.739
19.77
21.53
18.84
568
+1.2M
204370
LLaMA-Adapt AE
63.87
71.05
77.17
14.61
23.06
4.356
21.47
29.74
19.00
532
+1.0M
208247
+ multi-modal
64.86
74.63
76.95
14.12
18.77
3.098
18.65
18.40
16.29
536
+11M
220412
+ MSE loss
62.32
77.78
78.05
10.04
16.46
2.075
14.20
13.97
9.710
495
+11M
213497"
RESULTS/EXPERIMENTS,0.4787234042553192,"not always at odds with each other and, in fact, explainability might improve classification
accuracy.
Finally, we train the best PEFT configuration (multi-modal LLaMA-Adapter) directly
with MSE on the latent space of the encoder.
While the decrease in NLG metrics is
expected since supervision now occurs at the sentence level instead of at the token level, the
CLEV score significantly increases, which shows that, although the generated NLEs might
by syntactically different from the ground-truth NLEs, they contain the correct clinical
evidence. Thus, this confirms that our proposed approach is viable, laying the foundation
for the RL-free adversarial generation of NLEs.
We present qualitative examples in Figure 3 of the Appendix. They corroborate the
quantitative results, mainly that the PEFT approaches produce NLEs consistent with fully
fine-tuned models, even while training only a fraction of the parameters, and that reasonable
NLG metric values do not always correlate with correct clinical evidence."
CONCLUSION/DISCUSSION,0.48404255319148937,5. Conclusion
CONCLUSION/DISCUSSION,0.48936170212765956,"We build upon the seminal work of Kayser et al. (Kayser et al., 2022), which tackled the
problem of generating NLEs for chest X-ray diagnosis, by replacing the decoder-only NLE
generator by an encoder-decoder, coupled with PEFT techniques to drastically reduce the
number of trainable parameters while maintaining competitive performance. We show that
using the multi-modal-aware LLaMA-Adapter (Zhang et al., 2023) achieves the best overall
results, even outperforming the fully fine-tuned encoder-decoder model. We also empir-
ically demonstrate that directly supervising the NLE generation process on the encoder
latent space at the sentence level is viable, thus consolidating this approach as a first step
towards the RL-free adversarial generation of NLEs when no (or few) ground-truth NLEs
are available."
CONCLUSION/DISCUSSION,0.4946808510638298,PEFT Generation of NLEs for Chest X-ray Classification
CONCLUSION/DISCUSSION,0.5,Acknowledgments
CONCLUSION/DISCUSSION,0.5053191489361702,"This work is financed by National Funds through the FCT - Funda¸c˜ao para a Ciˆencia e
a Tecnologia, I.P. (Portuguese Foundation for Science and Technology) within the project
CAGING, with reference 2022.10486.PTDC (DOI 10.54499/2022.10486.PTDC), and through
the Ph.D. Grant 2020.07034.BD."
CONCLUSION/DISCUSSION,0.5106382978723404,Appendix A. Qualitative Examples
CONCLUSION/DISCUSSION,0.5159574468085106,"In Figure 3 we present examples of the NLEs generated by all models. These examples show
that, although using only a small percentage of trainable parameters, PEFT approaches pro-
duce NLEs that are consistent with NLEs produced by fully fine-tuned models (RATCHET,
DPT, and Base AE). Furthermore, they corroborate the known fact that NLG metrics are
not good measures of the correctness of a given text: the Base AE model produces the same
sentence for all instances, which can be explained by the high imbalance of MIMIC-NLE
(evidence labels are predominantly ‘Lung Opacity’) (Kayser et al., 2022). For example,
consider the last example where ‘Atelectasis’ is not present in the labels, but the NLE
generated by the Base AE mentions that ‘There is a new opacity in the right lung base
which may represent atelectasis’. However, the majority of the NLG metrics (BERTScore,
METEOR, BLEU-4, ROUGE, and SPICE) for the Base AE surpass those of the MSE
loss trained model. On the other hand, both versions of the multi-modal LlaMA-Adapter
present higher CLEV scores, indicating that they contain the correct clinical evidence, even
though the syntax of the generated NLEs might deviate more from the ground-truth NLEs."
CONCLUSION/DISCUSSION,0.5212765957446809,Rio-Torto Cardoso Teixeira
CONCLUSION/DISCUSSION,0.526595744680851,"Labels: Atelectasis (U), Lung Opacity (P) and Pneumonia (U)"
CONCLUSION/DISCUSSION,0.5319148936170213,"GT: There is new opacity at the right cardiophrenic angle, which may be atelectasis but could 
also represent pneumonia in the appropriate clinical setting."
CONCLUSION/DISCUSSION,0.5372340425531915,RATCHET: Subtle opacity in the left lower lobe is concerning for pneumonia.
CONCLUSION/DISCUSSION,0.5425531914893617,DPT: New right lower lobe opacity concerning for pneumonia.
CONCLUSION/DISCUSSION,0.5478723404255319,"Base AE: There is a new opacity in the right lung base which may represent atelectasis, but 
cannot exclude pneumonia or aspiration in the right clinical setting."
CONCLUSION/DISCUSSION,0.5531914893617021,LoRA: New patchy opacities in the right upper lobe are concerning for pneumonia.
CONCLUSION/DISCUSSION,0.5585106382978723,"Prompt: Patchy opacity in the left lower lobe is likely due to atelectasis, but infection cannot 
be excluded in the appropriate clinical setting."
CONCLUSION/DISCUSSION,0.5638297872340425,"Prefix: There is a new focal opacity in the right upper lobe, concerning for pneumonia or 
aspiration in the appropriate clinical setting."
CONCLUSION/DISCUSSION,0.5691489361702128,"LLaMA Uni: Subtle opacity in the left lower lobe is likely atelectasis versus scarring in the 
appropriate clinical setting, a possible right middle lobe pneumonia cannot be excluded."
CONCLUSION/DISCUSSION,0.574468085106383,LLaMA Mult: Subtle opacity at the right lung base likely represents atelectasis or pneumonia.
CONCLUSION/DISCUSSION,0.5797872340425532,"LLaMA Mult MSE: There is patchy left basilar opacity likely reflecting atelectasis, but appear 
free air."
CONCLUSION/DISCUSSION,0.5851063829787234,"Labels: Atelectasis (U), Edema (U), Lung Opacity (P), Pleural Effusion (U) and Pneumonia (U)
GT: Retrocardiac opacity which may be due in part to pulmonary edema and atelectasis 
limits evaluation of the left lower lung, and superimposed pneumonia cannot be excluded."
CONCLUSION/DISCUSSION,0.5904255319148937,"RATCHET: There is prominence of the pulmonary vasculature consistent with mild 
pulmonary edema."
CONCLUSION/DISCUSSION,0.5957446808510638,DPT: There is likely due to pulmonary edema.
CONCLUSION/DISCUSSION,0.601063829787234,"Base AE: There is a new opacity in the right lung base which may represent atelectasis, but 
cannot exclude pneumonia or aspiration in the right clinical setting."
CONCLUSION/DISCUSSION,0.6063829787234043,"LoRA: There is mild interstitial prominence, which could reflect mild pulmonary edema, but 
not entirely excluded."
CONCLUSION/DISCUSSION,0.6117021276595744,"Prompt: There is mild interstitial prominence of the hila bilaterally, consistent with pulmonary 
edema."
CONCLUSION/DISCUSSION,0.6170212765957447,"Prefix: There is persistent bilateral perihilar opacities, likely representing asymmetric 
edema."
CONCLUSION/DISCUSSION,0.6223404255319149,LLaMA Uni: There is persistent mild interstitial edema.
CONCLUSION/DISCUSSION,0.6276595744680851,"LLaMA Mult: There is increased interstitial markings in the left lower lobe, which may 
represent a component of pulmonary edema and atelectasis, less likely pneumonia."
CONCLUSION/DISCUSSION,0.6329787234042553,"LLaMA Mult MSE: There is hazy associated opacity in the left base likely representing 
atelectasis."
CONCLUSION/DISCUSSION,0.6382978723404256,"Labels: Lung Opacity (P) and Pneumonia (P)
GT: A new right lower lobe airspace opacity is likely due to aspiration pneumonia."
CONCLUSION/DISCUSSION,0.6436170212765957,RATCHET: Right lower lobe consolidation is worrisome for pneumonia.
CONCLUSION/DISCUSSION,0.648936170212766,DPT: New right upper lobe opacity concerning for pneumonia.
CONCLUSION/DISCUSSION,0.6542553191489362,"Base AE: There is a new opacity in the right lung base which may represent atelectasis, but 
cannot exclude pneumonia or aspiration in the right clinical setting."
CONCLUSION/DISCUSSION,0.6595744680851063,"LoRA: There is persistent consolidation in the right upper lobe, which may represent 
aspiration or pneumonia in the appropriate clinical setting, worsening of the right lung base, 
and could be due to aspiration or pneumonia"
CONCLUSION/DISCUSSION,0.6648936170212766,Prompt: New opacification at the right lung base is compatible with pneumonia.
CONCLUSION/DISCUSSION,0.6702127659574468,"Prefix: There is a new focal opacity in the right upper lobe, concerning for pneumonia or 
aspiration."
CONCLUSION/DISCUSSION,0.675531914893617,"LLaMA Uni: The opacity in the right upper lobe is concerning for pneumonia or aspiration, 
and less likely atelectasis."
CONCLUSION/DISCUSSION,0.6808510638297872,"LLaMA Mult: There is a new heterogeneous opacification in the right mid and lower lung 
zones concerning for pneumonia."
CONCLUSION/DISCUSSION,0.6861702127659575,"LLaMA Mult MSE: Given that the right basilar opacity is concerning for pneumonia, less 
likely atelectasis given the clinical setting."
CONCLUSION/DISCUSSION,0.6914893617021277,"Figure 3: Illustrative examples of NLEs generated by all tested models for correctly pre-
dicted images. For each image we provide the ground-truth labels, where: U
- uncertain and P - positive. The labels highlighted in bold correspond to the
disease being explained in the NLE."
CONCLUSION/DISCUSSION,0.6968085106382979,PEFT Generation of NLEs for Chest X-ray Classification
REFERENCES,0.7021276595744681,References
REFERENCES,0.7074468085106383,"Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been
Kim. Sanity Checks for Saliency Maps. In Advances in Neural Information Processing
Systems, volume 31, 2018."
REFERENCES,0.7127659574468085,"Benedikt
Boecking,
Naoto
Usuyama,
Shruthi
Bannur,
Daniel
C.
Castro,
Anton
Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori,
Javier Alvarez-Valle, Hoifung Poon, and Ozan Oktay. Making the Most of Text Semantics
to Improve Biomedical Vision–Language Processing. In Computer Vision – ECCV 2022,
pages 1–21, 2022."
REFERENCES,0.7180851063829787,"Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria de la Iglesia-Vay´a. Pad-
Chest: A large chest x-ray image dataset with multi-label annotated reports. Medical
Image Analysis, 66:101797, 2020."
REFERENCES,0.723404255319149,"Oana-Maria Camburu, Tim Rockt¨aschel, Thomas Lukasiewicz, and Phil Blunsom. e-SNLI:
Natural Language Inference with Natural Language Explanations. In Advances in Neural
Information Processing Systems, volume 31, 2018."
REFERENCES,0.7287234042553191,"Oana-Maria Camburu, Eleonora Giunchiglia, Jakob Foerster, Thomas Lukasiewicz, and
Phil Blunsom. Can I Trust the Explainer? Verifying Post-hoc Explanatory Methods. In
NeurIPS 2019 Workshop on Safety and Robustness in Decision Making, 2019."
REFERENCES,0.7340425531914894,"Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vi-
sion Transformer Adapter for Dense Predictions. In International Conference on Learning
Representations, 2023."
REFERENCES,0.7393617021276596,"Roberto Cipolla, Yarin Gal, and Alex Kendall.
Multi-task Learning Using Uncertainty
to Weigh Losses for Scene Geometry and Semantics. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 7482–7491, 2018."
REFERENCES,0.7446808510638298,"Dina Demner-Fushman, Marc D. Kohli, Marc B. Rosenman, Sonya E. Shooshan, Laritza
Rodriguez, Sameer Antani, George R. Thoma, and Clement J. McDonald. Preparing a
collection of radiology examinations for distribution and retrieval. Journal of the Amer-
ican Medical Informatics Association, 23(2):304–310, 2015."
REFERENCES,0.75,"Virginie Do,
Oana-Maria Camburu,
Zeynep Akata,
and Thomas Lukasiewicz.
e-
SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations.
arXiv:2004.03744, 2020."
REFERENCES,0.7553191489361702,"David Donahue and Anna Rumshisky. Adversarial Text Generation Without Reinforcement
Learning. arXiv:1810.06640, 2018."
REFERENCES,0.7606382978723404,"Raman Dutt,
Linus Ericsson,
Pedro Sanchez,
Sotirios A. Tsaftaris,
and Timothy
Hospedales. Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed
Opportunity. arXiv:2305.08252, 2023."
REFERENCES,0.7659574468085106,Rio-Torto Cardoso Teixeira
REFERENCES,0.7712765957446809,"William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P Bradley, and Lyle J
Palmer.
Producing radiologist-quality reports for interpretable artificial intelligence.
arXiv:1806.00340, 2018."
REFERENCES,0.776595744680851,"Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng
Li, and Yu Qiao. CLIP-Adapter: Better Vision-Language Models with Feature Adapters.
International Journal of Computer Vision, 132(2):581–595, 2024."
REFERENCES,0.7819148936170213,"Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and
Trevor Darrell. Generating Visual Explanations. In Computer Vision – ECCV 2016,
pages 3–19, 2016."
REFERENCES,0.7872340425531915,"Benjamin Hou, Georgios Kaissis, Ronald M. Summers, and Bernhard Kainz. RATCHET:
Medical Transformer for Chest X-ray Diagnosis and Reporting. In Medical Image Com-
puting and Computer Assisted Intervention – MICCAI 2021, pages 293–303, 2021."
REFERENCES,0.7925531914893617,"Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Larous-
silhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Trans-
fer Learning for NLP. In International Conference on Machine Learning, volume 97,
pages 2790–2799, 2019."
REFERENCES,0.7978723404255319,"Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely
Connected Convolutional Networks.
In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2017."
REFERENCES,0.8031914893617021,"Alistair E. W. Johnson, Tom J. Pollard, Lu Shen, Li-wei H. Lehman, Mengling Feng, Mo-
hammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G.
Mark. MIMIC-III, a freely accessible critical care database. Scientific Data, 3(1):160035,
2016."
REFERENCES,0.8085106382978723,"Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum,
Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. MIMIC-CXR,
a de-identified publicly available database of chest radiographs with free-text reports.
Scientific Data, 6(1):317, 2019."
REFERENCES,0.8138297872340425,"Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do,
Zeynep Akata, and Thomas Lukasiewicz. e-ViL: A Dataset and Benchmark for Natural
Language Explanations in Vision-Language Tasks.
In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 1244–1254, 2021."
REFERENCES,0.8191489361702128,"Maxime Kayser, Cornelius Emde, Oana-Maria Camburu, Guy Parsons, Bartlomiej Papiez,
and Thomas Lukasiewicz. Explaining Chest X-Ray Pathologies in Natural Language. In
Medical Image Computing and Computer Assisted Intervention – MICCAI 2022, pages
701–713, 2022."
REFERENCES,0.824468085106383,"Sawan Kumar and Partha Talukdar. NILE: Natural Language Inference with Faithful Natu-
ral Language Explanations. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pages 8730–8742, 2020."
REFERENCES,0.8297872340425532,PEFT Generation of NLEs for Chest X-ray Classification
REFERENCES,0.8351063829787234,"Alon Lavie and Abhaya Agarwal. METEOR: An Automatic Metric for MT Evaluation with
Improved Correlation with Human Judgments. In Proceedings of the Second Workshop
on Statistical Machine Translation, page 228–231, 2007."
REFERENCES,0.8404255319148937,"Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient
Prompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing, pages 3045–3059, 2021."
REFERENCES,0.8457446808510638,"Xiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing Continuous Prompts for Gen-
eration. In Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing,
pages 4582–4597, 2021."
REFERENCES,0.851063829787234,"Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie
Tang.
P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally
Across Scales and Tasks. arXiv:2110.07602, 2021."
REFERENCES,0.8563829787234043,"Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji. Cheap
and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models.
arXiv:2305.15023, 2023."
REFERENCES,0.8617021276595744,"Bodhisattwa Prasad Majumder, Oana Camburu, Thomas Lukasiewicz, and Julian Mcauley.
Knowledge-Grounded Self-Rationalization via Extractive and Natural Language Explana-
tions. In Proceedings of the 39th International Conference on Machine Learning, volume
162, pages 14786–14801, 2022."
REFERENCES,0.8670212765957447,"Ana Marasovi´c, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras, Noah A Smith,
and Yejin Choi. Natural Language Rationales with Full-Stack Visual Reasoning: From
Pixels to Semantic Frames to Commonsense Graphs. In Findings of the Association for
Computational Linguistics: EMNLP 2020, pages 2810–2829, 2020."
REFERENCES,0.8723404255319149,"Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. Artificial
Intelligence, 267:1–38, 2019."
REFERENCES,0.8776595744680851,"Maram Mahmoud A. Monshi, Josiah Poon, and Vera Chung. Deep learning in generating
radiology reports: A survey. Artificial Intelligence in Medicine, 106:101878, 2020."
REFERENCES,0.8829787234042553,"Ivan Montero, Nikolaos Pappas, and Noah A. Smith. Sentence Bottleneck Autoencoders
from Transformer Language Models. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, pages 1822–1831, 2021."
REFERENCES,0.8882978723404256,"Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Kar-
ishma Malkan.
WT5?!
Training Text-to-Text Models to Explain their Predictions.
arXiv:2004.14546, 2020."
REFERENCES,0.8936170212765957,"Mohammed Yasser Ouis and Moulay A. Akhloufi. Deep learning for report generation on
chest X-ray images. Computerized Medical Imaging and Graphics, 111:102320, 2024."
REFERENCES,0.898936170212766,Rio-Torto Cardoso Teixeira
REFERENCES,0.9042553191489362,"Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor
Darrell, and Marcus Rohrbach.
Multimodal Explanations:
Justifying Decisions and
Pointing to the Evidence. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2018."
REFERENCES,0.9095744680851063,"Bj¨orn Pl¨uster, Jakob Ambsdorf, Lukas Braach, Jae Hee Lee, and Stefan Wermter. Har-
nessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language
Explanations. arXiv:2212.04231, 2022."
REFERENCES,0.9148936170212766,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. 2018."
REFERENCES,0.9202127659574468,"Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain
Yourself! Leveraging Language Models for Commonsense Reasoning. In Proceedings of
the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932–
4942, 2019."
REFERENCES,0.925531914893617,"Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual do-
mains with residual adapters. In Advances in Neural Information Processing Systems,
page 506–516, 2017."
REFERENCES,0.9308510638297872,"Isabel Rio-Torto, Jaime S. Cardoso, and Lu´ıs F. Teixeira.
From Captions to Explana-
tions: A Multimodal Transformer-based Architecture for Natural Language Explanation
Generation. In Pattern Recognition and Image Analysis, pages 54–65, 2022."
REFERENCES,0.9361702127659575,"Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions
and use interpretable models instead. Nature machine intelligence, 1(5):206–215, 2019."
REFERENCES,0.9414893617021277,"Fawaz Sammani, Tanmoy Mukherjee, and Nikos Deligiannis.
NLX-GPT: A Model for
Natural Language Explanations in Vision and Vision-Language Tasks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8322–
8332, 2022."
REFERENCES,0.9468085106382979,"Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Ng, and Matthew
Lungren. Combining Automatic Labelers and Expert Annotations for Accurate Radiol-
ogy Report Labeling Using BERT. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing, pages 1500–1519, 2020."
REFERENCES,0.9521276595744681,"Jesus Solano, Oana-Maria Camburu, and Pasquale Minervini.
SPARSEFIT: Few-shot
Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Lan-
guage Explanations. arXiv:2305.13235, 2023."
REFERENCES,0.9574468085106383,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971, 2023."
REFERENCES,0.9627659574468085,U.S. National Library of Medicine. Pubmed. URL https://pubmed.ncbi.nlm.nih.gov/.
REFERENCES,0.9680851063829787,"Effy Vayena, Alessandro Blasimme, and I. Glenn Cohen. Machine learning in medicine:
Addressing ethical challenges. PLOS Medicine, 15:1–4, 2018."
REFERENCES,0.973404255319149,PEFT Generation of NLEs for Chest X-ray Classification
REFERENCES,0.9787234042553191,"Jialin Wu and Raymond Mooney. Faithful Multimodal Explanation for Visual Question
Answering.
In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and
Interpreting Neural Networks for NLP, pages 103–112, 2019."
REFERENCES,0.9840425531914894,"Yu Yu, Chao-Han Huck Yang, Jari Kolehmainen, Prashanth G. Shivakumar, Yile Gu,
Sungho Ryu Roger Ren, Qi Luo, Aditya Gourav, I-Fan Chen, Yi-Chieh Liu, Tuan Dinh,
Ankur Gandhe Denis Filimonov, Shalini Ghosh, Andreas Stolcke, Ariya Rastow, and
Ivan Bulyko. Low-Rank Adaptation of Large Language Model Rescoring for Parameter-
Efficient Speech Recognition. In 2023 IEEE Automatic Speech Recognition and Under-
standing Workshop, pages 1–8, 2023."
REFERENCES,0.9893617021276596,"Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng
Li, Peng Gao, and Yu Qiao. LLaMA-Adapter: Efficient Fine-tuning of Language Models
with Zero-init Attention. arXiv:2303.16199, 2023."
REFERENCES,0.9946808510638298,"Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.
BERTScore: Evaluating Text Generation with BERT. In International Conference on
Learning Representations, 2020."
