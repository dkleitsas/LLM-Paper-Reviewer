Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005235602094240838,"Evading adversarial example detection defenses requires ﬁnding adversarial ex-
amples that must simultaneously (a) be misclassiﬁed by the model and (b) be
detected as non-adversarial. We ﬁnd that existing attacks that attempt to satisfy
multiple simultaneous constraints often over-optimize against one constraint at the
cost of satisfying another. We introduce Selective Projected Gradient Descent and
Orthogonal Projected Gradient Descent, improved attack techniques to generate
adversarial examples that avoid this problem by orthogonalizing the gradients when
running standard gradient-based attacks. We use our technique to evade four state-
of-the-art detection defenses, reducing their accuracy to 0% while maintaining a
0% detection rate."
INTRODUCTION,0.010471204188481676,"1
INTRODUCTION"
INTRODUCTION,0.015706806282722512,"Generating adversarial examples (SZS+14; BCM+13), inputs designed by an adversary to cause a
neural network to behave incorrectly, is straightforward. By performing input-space gradient descent
(CW17b; MMS+17), it is possible to maximize the loss of arbitrary test examples. This process is
both efﬁcient and highly effective. But despite much effort by the community, attempts at designing
defenses against adversarial examples have been largely unsuccessful and gradient-descent attacks
continue to circumvent new defenses (ACW18; TCBM20)."
INTRODUCTION,0.020942408376963352,"As a result, many defenses aim to make generating adversarial examples more difﬁcult by requiring
additional constraints on inputs for them to be considered successful. Defenses that rely on detection,
for example, will reject inputs if a secondary detector model determines the input is adversarial
(MGFB17; XEQ17). Turning a benign input x into an adversarial example x′ thus now requires
fooling both the original classiﬁer, f, and the detector, g, simultaneously."
INTRODUCTION,0.02617801047120419,"Traditionally, this is done by constructing a single loss function L that jointly penalizes the loss on f
and the loss on g (CW17a), e.g., by deﬁning L(x′) = L(f) + λL(g) and then minimizing L(x′) with
gradient descent. Unfortunately, many evaluations using this strategy have had limited success—not
only must λ be tuned appropriately, but the gradients of f and g must also be well behaved."
INTRODUCTION,0.031413612565445025,"Our contributions. We develop a new attack technique designed to construct adversarial examples
that simultaneously satisfy multiple constraints. Our attack approach is a modiﬁcation of standard
gradient descent (MMS+17) and requires changing just a few lines of code. Given two objective
functions f and g, instead of taking gradient descent steps that optimize the joint loss function f +λg,
we selectively take gradient descent steps on either f or g. This makes our attack both simpler and
easier to analyze than prior attack approaches."
INTRODUCTION,0.03664921465968586,"We use our technique to evade four state-of-the-art and previously-unbroken defenses to adver-
sarial examples: the Honeypot defense (CCS’20) (SWW+20), Dense Layer Analysis (IEEE Euro
S&P’20) (SKCB19), Sensitivity Inconsistency Detector (AAAI’21) (TZLD21), and the SPAM de-
tector presented in Detection by Steganalysis (CVPR’19) (LZZ+19). In all cases, we successfully
reduce the accuracy of the protected classiﬁer to 0% while maintaining a detection AUC of less than
0.5—meaning the detector performs worse than random guessing."
INTRODUCTION,0.041884816753926704,∗Equal contributions. Authored alphabetically.
INTRODUCTION,0.04712041884816754,Published as a conference paper at ICLR 2022
BACKGROUND,0.05235602094240838,"2
BACKGROUND"
NOTATION,0.05759162303664921,"2.1
NOTATION"
NOTATION,0.06282722513089005,"We consider classiﬁcation neural networks f : Rd →Rn that receive a d-dimensional input vector
(in this paper, images) x ∈Rd and output an n-dimensional prediction vector f(x) ∈Rn. We let
g : Rd →R denote some other function which also must be considered, where g(x) ≤0 when the
constraint is satisﬁed and g(x) > 0 if it is violated. Without loss of generality, in a detection defense
this function g is the detector and higher values corresponding to higher likelihood of the input being
an adversarial example. To denote the true label of x is given by y we write c(x) = y. In an abuse of
notation, write y = f(x) to denote the arg-max most likely label under the model f."
ADVERSARIAL EXAMPLES,0.06806282722513089,"2.2
ADVERSARIAL EXAMPLES"
ADVERSARIAL EXAMPLES,0.07329842931937172,"Adversarial examples (SZS+14; BCM+13) have been demonstrated in nearly every domain in which
neural networks are used. (ASE+18; CW18; HPG+17) Given an input x corresponding to label c(x)
and classiﬁer f, an adversarial example is a perturbation x′ of the input such that d(x, x′) < ϵ and
f(x′) ̸= c(x) for some metric d. Additionally, an adversarial example can be targeted if, given a
target label t ̸= c(x) we have f(x′) = t with d(x, x′) < ϵ. The metric d is most often that induced
by a p-norm, typically either || · ||2 or || · ||∞. With small enough perturbations under these metrics,
the adversarial example x′ is not perceptibly different from the original input x."
ADVERSARIAL EXAMPLES,0.07853403141361257,"Datasets.
We attack each defense on the dataset that it performs best on. All of our defenses
operate on images. For three of these defenses, this is the CIFAR-10 dataset (KH09), and for one,
it is the ImageNet dataset (DDS+09). For each defense we attack, we constrain our adversarial
examples to the threat model originally considered to perform a fair re-evaluation, but also generate
adversarial examples with standard norms used extensively in prior work in order to make cross-
defense evaluations meaningful. We perform all evaluations on a single GPU. Our attacks on
CIFAR-10 require just a few minutes, and for ImageNet a few hours (primarily due to the defense
having a throughput of one image per second)."
DETECTION DEFENSES,0.08376963350785341,"2.3
DETECTION DEFENSES"
DETECTION DEFENSES,0.08900523560209424,"We focus our study on detection defenses. Rather than directly improve the robustness of the model
(MMS+17; RSL18; LAG+19; CRK19), detection defenses classify inputs as adversarial or benign
(MGFB17; XEQ17) so they can be rejected. While there have been several different strategies
attempted to detect adversarial examples in the past (GSS15; MGFB17; FCSG17; XEQ17; MC17;
MLW+18; RKH19), many of these approaches were broken with adaptive attacks that designed new
loss functions tailored to each defense (CW17a; TCBM20)."
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.09424083769633508,"2.4
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT"
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.09947643979057591,"Projected Gradient Descent (MMS+17) is a powerful ﬁrst-order method for ﬁnding such adversarial
examples. Given a loss L(f, x, t) that takes a classiﬁer, input, and desired target label, we optimize
over the constraint set Sϵ = {z : d(x, z) < ϵ} and solve"
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.10471204188481675,"x′ = arg min
z∈Sϵ
L(f, z, t)
(1)"
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.1099476439790576,"by taking steps xi+1 = ΠSϵ (xi −α∇xiL(f, xi, t)). Here ΠSϵ denotes projection onto the set Sϵ, α
is the step size, and x0 is randomly initialized (MMS+17). This paper adapts PGD in order to solve
optimization problems which involve minimizing multiple objective functions simultaneously. For
notational simplicity, in the remainder of this paper we will omit the projection operator ΠSϵ."
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.11518324607329843,"Attacks using PGD.
Recent work has shown that it is possible to attack models with adaptive
attacks that target speciﬁc aspects of defenses. For detection defenses this process is often ad hoc,
involving alterations speciﬁc to each given defense (TCBM20). An independent line of work develops
automated attack techniques that are reliable indicators of robustness (CH20); however, in general,
these attack approaches are difﬁcult to apply to detection defenses."
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.12041884816753927,Published as a conference paper at ICLR 2022
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.1256544502617801,f(x) < 0
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.13089005235602094,g(x) < 0
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.13612565445026178,f(x) < 0
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.14136125654450263,g(x) < 0
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.14659685863874344,f(x) < 0
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.1518324607329843,g(x) < 0
GENERATING ADVERSARIAL EXAMPLES WITH PROJECTED GRADIENT DESCENT,0.15706806282722513,"Figure 1: A visualization showing how a standard Lagrangian attack fails when ours succeeds
over a non-convex loss landscape. Given two circular regions corresponding to when f(x) < 0
(above) and g(x) < 0 (below), we would like to ﬁnd the central region where both are satisﬁed.
(left) With Lagrangian PGD, the attack falls in a local minimum and fails to satisfy both constraints
simultaneously regardless of the value λ selected. (middle) Our S-PGD attack ﬁrst moves towards
the upper region by minimizing f(x). Once this constraint is satisﬁed (and f(x) < 0), we begin
to minimize g(x); however this overshoots to a point where f(x) > 0. A ﬁnal step recovers a
valid solution to both. (right) Our O-PGD attack follows the same trajectory for the ﬁrst two steps
optimizing f(x). However after this it takes steps orthogonal to f(x) maintaining the constraint
f(x) < 0 while simultaneously minimizing g(x), giving a valid solution more quickly."
RETHINKING ADVERSARIAL EXAMPLE DETECTION,0.16230366492146597,"3
RETHINKING ADVERSARIAL EXAMPLE DETECTION"
RETHINKING ADVERSARIAL EXAMPLE DETECTION,0.16753926701570682,"Before we develop our improved attack technique to break adversarial example detectors, it will be
useful to understand why evaluating adversarial example detectors is more difﬁcult than evaluating
standard classiﬁers."
RETHINKING ADVERSARIAL EXAMPLE DETECTION,0.17277486910994763,"Early work on adversarial examples often set up the problem slightly differently than we do above
in Equation 1. The initial formulation of an adversarial example (SZS+14; CW17b) asks for the
smallest perturbation δ such that f(x + δ) is misclassiﬁed. In the targeted case, this means solving
arg min ∥δ∥2 such that f(x + δ) = t
for a given target t that is not equal to the original correct label for x. Solving this problem as stated
is intractable. It requires searching over a nonlinear constraint set, which is not feasible for standard
gradient descent. As a result, detection papers typically (SWW+20; SKCB19) reformulate the search
with a Lagrangian relaxation
arg min ∥δ∥2 + λL(f, x + δ, t)
(2)
This formulation is simpler, but still (a) requires tuning λ to work well, and (b) is only guaranteed to
be correct for convex functions L—that it works for non-convex models like deep neural networks is
not theoretically justiﬁed. It additionally requires carefully constructing loss functions L (CW17b)."
RETHINKING ADVERSARIAL EXAMPLE DETECTION,0.17801047120418848,"Equation 1 simpliﬁes the setup considerably by exchanging the constraint and objective. Whereas in
Equation 2 we search for the smallest perturbation that results in misclassiﬁcation, Equation 1 instead
ﬁnds an input x + δ with δ < ϵ that minimizes the classiﬁer’s loss. This is a simpler formulation
because now the constraint is convex, and so we can run standard gradient descent optimization."
RETHINKING ADVERSARIAL EXAMPLE DETECTION,0.18324607329842932,"Evading detection defenses is difﬁcult because there are now two non-linear constraints. Not
only must the input be constrained by a distortion bound and be misclassiﬁed by the base classiﬁer,
but we must also have that they are not detected, i.e., with g(x) < 0. This new requirement is
nonlinear, and now it becomes impossible to side-step the problem by merely swapping the objective
and the constraint as we did before: there will always be at least one constraint that is a non-linear
function, and so standard gradient descent techniques can not directly apply."
RETHINKING ADVERSARIAL EXAMPLE DETECTION,0.18848167539267016,"In order to resolve this difﬁculty, the existing literature applies the same Lagrangian relaxation as
was previously applied to constructing minimum-distortion adversarial examples. That is, breaking a
detection scheme involves solving
arg min
x∈Sϵ
L(f, x, t) + λg(x)
(3)"
RETHINKING ADVERSARIAL EXAMPLE DETECTION,0.193717277486911,"where λ is a hyperparameter that controls the relative importance of fooling the classiﬁer versus fool-
ing the detector. However, this formulation again brings back all of the reasons why the community
moved past minimum-distortion adversarial examples."
RETHINKING ADVERSARIAL EXAMPLE DETECTION,0.19895287958115182,Published as a conference paper at ICLR 2022
A MOTIVATING EXAMPLE,0.20418848167539266,"3.1
A MOTIVATING EXAMPLE"
A MOTIVATING EXAMPLE,0.2094240837696335,"Let f(⃗x) = exp(−1) −exp(−∥⃗x −⃗e∥2
2) −ε and g(⃗x) = exp(−1) −exp(−∥⃗x + ⃗e∥2
2) −ε where
⃗e ∈RN and ∥⃗e∥= 1, as visualized in Figure 1. By setting ε to a small constant, the only solution
that satisﬁes both f(⃗x) < 0 and g(⃗x) < 0 can be made arbitrarily close to the origin ⃗x = ⃗0."
A MOTIVATING EXAMPLE,0.21465968586387435,"However, no standard Lagrangian formulation will be able to ﬁnd this solution. Consider the sum
h(⃗x; λ) = f(⃗x) + λg(⃗x); then we can show that for all λ we will have arg min⃗xh(⃗x; λ) ̸= 0. To
see this, observe that while it is possible for the gradient ∇h(⃗0; λ) = 0 (one of the conditions for a
value to be a local minima), the loss surface is always “concave down” at the origin. It will always
be possible to move slightly closer to ⃗e or −⃗e and decrease the loss. Therefore, minimizing h(x)
will never be able to ﬁnd a valid stable solution to this extremely simple problem, as it will always
collapse to ﬁnding a solution of either ⃗e or −⃗e, which only satisﬁes one of the two equations."
OUR ATTACK APPROACHES,0.2198952879581152,"4
OUR ATTACK APPROACHES"
OUR ATTACK APPROACHES,0.225130890052356,"We now present our attack strategy designed to generate adversarial examples that satisfy two
constraints. As we have been doing, each of our attack strategies deﬁned below generates a targeted
adversarial example x′ so that f(x′) = t but g(x′) < 0. Constructing an untargeted attack is nearly
identical except for the substitution of maximization instead of minimization."
SELECTIVE GRADIENT DESCENT,0.23036649214659685,"4.1
SELECTIVE GRADIENT DESCENT"
SELECTIVE GRADIENT DESCENT,0.2356020942408377,"Instead of minimizing the weighted sum of f and g, our ﬁrst attack never optimizes against a
constraint once it becomes satisﬁed. That is, we write our attack as"
SELECTIVE GRADIENT DESCENT,0.24083769633507854,"A(x, t) =
arg min
x′:∥x−x′∥<ϵ
L(f, x′, t) · 1[f(x) ̸= t] + g(x′) · 1[f(x) = t]
|
{z
}
Lupdate(x,t) .
(4)"
SELECTIVE GRADIENT DESCENT,0.24607329842931938,"The idea here is that instead of minimizing a convex combination of the two loss functions, we
selectively optimize either f or g depending on if f(x) = t, ensuring that updates are always helping
to improve either the loss on f or the loss on g."
SELECTIVE GRADIENT DESCENT,0.2513089005235602,"Another beneﬁt of this style is that it decomposes the gradient step into two updates, which prevents
imbalanced gradients, where the gradients for two loss functions are not of the same magnitude and
result in unstable optimization (JMW+20). In fact, our loss function can be viewed directly in this
lens as following the margin decomposition proposal (JMW+20) by observing that"
SELECTIVE GRADIENT DESCENT,0.25654450261780104,"∇Lupdate(x, t) =
∇L(f, x, t)
if f(x) ̸= t
∇g(x)
if f(x) = t.
(5)"
SELECTIVE GRADIENT DESCENT,0.2617801047120419,"That is, each iteration either take gradients on f or on g depending on whether f(x) = t or not."
SELECTIVE GRADIENT DESCENT,0.2670157068062827,"Recalling the motivating example from Figure 1, this selective optimization formulation would
be able to ﬁnd a valid solution. No matter where we initialized our adversarial example search,
minimizing with respect to f(x) will eventually give a valid solution near e. Once this happens, we
then switch to optimizing against g(x) (because f(x) is satisﬁed). From here we will eventually
converge on the solution x ≈⃗0."
ORTHOGONAL GRADIENT DESCENT,0.27225130890052357,"4.2
ORTHOGONAL GRADIENT DESCENT"
ORTHOGONAL GRADIENT DESCENT,0.2774869109947644,"The prior attack, while mathematically correct, might encounter numerical stability difﬁculties. Often,
the gradients of f and g point in opposite directions, that is, ∇f ≈−∇g. As a result, every step
spent optimizing f causes backwards progress on optimizing against g. This results in the optimizer
constantly “undoing” its own progress after each step that is taken. To address this problem, we
would like to ""remove"" the portion of one gradient optimization step that ""undoes"" the progress of a
previous optimization step."
ORTHOGONAL GRADIENT DESCENT,0.28272251308900526,"To do this, we call on vector projections. Note that ∇g(x)⊥= ∇L(f, x, t) −proj∇g(x)∇L(f, x, t)
is orthogonal to the gradient ∇g(x), and similarly ∇L(f, x, t)⊥is orthogonal to ∇L(f, x, t). We"
ORTHOGONAL GRADIENT DESCENT,0.2879581151832461,Published as a conference paper at ICLR 2022
ORTHOGONAL GRADIENT DESCENT,0.2931937172774869,"integrate this fact with Equation 5 to give a slightly different update rule that again solves Equation 4,
however this time by optimizing:"
ORTHOGONAL GRADIENT DESCENT,0.29842931937172773,"Lupdate(x, t) ="
ORTHOGONAL GRADIENT DESCENT,0.3036649214659686,"(
∇L(f, x, t) −proj∇g(x)∇L(f, x, t)
if f(x) ̸= t
∇g(x) −proj∇L(f,x,t)∇g(x)
if f(x) = t.
(6)"
ORTHOGONAL GRADIENT DESCENT,0.3089005235602094,"The purpose of this update is to take gradient descent steps with respect to one of f or g in such a
way that we do not signiﬁcantly disturb the loss of the function not chosen. In this way, we prevent
our attack from taking steps that undo work done in previous iterations of the attack."
ORTHOGONAL GRADIENT DESCENT,0.31413612565445026,"It is also important to note that, in the high-dimensional space in which a typical neural network
operates, the gradients of f and g are practically never exactly opposite, that is a situation where
∇f = −∇g. In this case, the projection of ∇f onto ∇g and ∇g onto ∇f would be 0 and we could
not make any meaningful optimizations towards satisfying either constraint with OPGD."
ORTHOGONAL GRADIENT DESCENT,0.3193717277486911,"Again recalling Figure 1, by taking steps that are orthogonal to f(x) we can ensure that once we reach
the acceptable region for f, we never leave it, and much more quickly converge on an adversarial
example that evades detection."
CASE STUDIES,0.32460732984293195,"5
CASE STUDIES"
CASE STUDIES,0.3298429319371728,"We validate the efﬁcacy of our attack by using it to circumvent four previously unbroken, state-of-the-
art defenses accepted at top computer security or machine learning venues. Three of the case studies
utilizes models and code obtained directly from their respective authors. In the ﬁnal case the original
authors provided us with matlab source code that was not easily used, which we re-implemented."
CASE STUDIES,0.33507853403141363,"Attack Success Rate Deﬁnition. We evaluate the success of our attack by a standard metric called
attack success rate at N (SR@N for short) (SKCB19). We use SR@N to ensure comparability across
different case studies but more importantly between our results and our case studies’ original results.
SR@N is deﬁned as the fraction of targeted attacks that succeed when the defense’s false positive
rate is set to N%. (To adjust a defense’s false positive rate it sufﬁces to adjust the detection threshold
φ so that inputs are rejected as adversarial when g(x) > φ.) For example, a 94% SR@5 could either
be achieved through 94% of inputs being misclassiﬁed as the target class and 0% being detected as
adversarial, or by 100% of inputs being misclassiﬁed as the target class and 6% being detected as
adversarial, or some combination thereof. We report SR@5 and SR@50 for our main results 1, and
for completeness also give the full ROC curve of the detection rate for a more complete analysis."
CASE STUDIES,0.3403141361256545,"Attack Hyperparameters. We use the same hyperparmaeter setting for all attacks shown below. We
set the distortion bound ε to 0.01 and .031; several of these papers exclusively make claims using the
value of 0.01 (SWW+20; SKC+20; TZLD21), but the value 0.031 = 8/255 is more typical in the
literature (TCBM20). We run our attack for N = 1000 iterations of gradient descent with a step size
α =
ε
10 (that is, the step size changes as a function of ε which follows standard advice (MMS+17))."
HONEYPOT DEFENSE,0.34554973821989526,"5.1
HONEYPOT DEFENSE"
HONEYPOT DEFENSE,0.3507853403141361,"The ﬁrst paper we consider is the Honeypot Defense (SWW+20). Instead of preventing attackers
from directly constructing adversarial examples, the authors propose to lure attackers into producing
speciﬁc perturbations that are easy to ﬁnd and hard to ignore. These perturbations are called
“honeypots” or trapdoors and can be easily identiﬁed by a detector. For their evaluation on the MNIST
and CIFAR-10 dataset, the authors use 5 sets of randomly selected 3 × 3 squares per label."
HONEYPOT DEFENSE,0.35602094240837695,"Formally, consider an input x to the classiﬁer, f. During training, f is injected with a honeypot,
∆. The signature of a particular honeypot, S∆, is the expectation of the neuron activations of f
over multiple sample inputs containing ∆. During inference, the internal neuron activation pattern
e(x) is compared to S∆using cosine similarity. Speciﬁcally, for a predeﬁned threshold φ, if
cos(e(x), S∆) > φ, then x is ﬂagged as adversarial. One additional modiﬁcation the authors make is
to use neuron randomization. When creating the signatures, a random sample of neuron activations"
HONEYPOT DEFENSE,0.3612565445026178,"1The value 5% is used in many prior defenses in the literature (MLW+18; XEQ17), and 50% is an extreme
upper bound and would reduce the model’s accuracy by half."
HONEYPOT DEFENSE,0.36649214659685864,Published as a conference paper at ICLR 2022
HONEYPOT DEFENSE,0.3717277486910995,"Attack
eps=0.01
eps=0.031
SR@5
SR@50
SR@5
SR@50"
HONEYPOT DEFENSE,0.3769633507853403,"(SWW+20)
0.02
-
-
-
Reproduction
0.13∗
-
-
-"
HONEYPOT DEFENSE,0.38219895287958117,"Orthogonal
1.0
0.93
1.0
0.92
Selective
0.998
0.505
0.996
0.628"
HONEYPOT DEFENSE,0.387434554973822,"(a) Attack success rate for our two proposed attacks. ∗When we
reproduce the authors attack, we reach slightly a higher attack
success rate of 13%."
HONEYPOT DEFENSE,0.39267015706806285,"0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0"
HONEYPOT DEFENSE,0.39790575916230364,True Positive Rate
HONEYPOT DEFENSE,0.4031413612565445,"Orthogonal
Selective"
HONEYPOT DEFENSE,0.4083769633507853,"(b) Defense ROC, ε = 0.01, AUC of 0.30"
HONEYPOT DEFENSE,0.41361256544502617,"Figure 2: Honeypot attack evaluation. Compared to the originally reported 2% success rate, our
attack reaches a 100% attack success rate under the same distortion bound. While the ROC curve
does cross over the x = y line, this only occurs after a FPR of 70% which is completely unusable."
HONEYPOT DEFENSE,0.418848167539267,"is ignored and the same set of neurons is ignored when calculating the cosine similarity during
adversarial attack detection."
HONEYPOT DEFENSE,0.42408376963350786,"We test our method of generating adversarial images on CIFAR-10 with an l∞bound of ϵ =
8
255.
The original model has a baseline accuracy of .87. To test the defense, we generate targeted attacks
choosing the target from among the incorrect labels. As a simple baseline attack, we conﬁrm that
FGSM achieves an accuracy of .06 on the classiﬁer but the ROC AUC of the detector is .994."
HONEYPOT DEFENSE,0.4293193717277487,"To evaluate the efﬁcacy of our attack, we use the authors pre-trained code, and convert their models
from TensorFlow to PyTorch to run our attacks. We ﬁnd that our attack is extremely effective and has
an attack success rate above 99.9%, even at a 50% false positive rate, and an AUC of 0.30. In Table
2 we give the attack results for several conﬁgurations of our attack. We shared our results with the
original defense authors who conﬁrmed our adversarial examples successfully evaded their detection
(an important step, given that we had converted the model to PyTorch)."
DENSE LAYER ANALYSIS,0.43455497382198954,"5.2
DENSE LAYER ANALYSIS"
DENSE LAYER ANALYSIS,0.4397905759162304,"Many recent defenses analyze the hidden activations of neural networks while processing benign
and adversarial inputs (e.g., following (MGFB17)). These defenses aim to capitalize on differences
in activation patterns among benign and adversarial inputs to train a separate classiﬁer capable of
detecting inputs as adversarial."
DENSE LAYER ANALYSIS,0.44502617801047123,"The most recent paper in this direction Sperl et al. extract dense layer activation patterns among
benign and adversarial inputs and train a secondary binary classiﬁer that detects adversarial examples
(SKCB19). The authors do this by ﬁrst performing a forward pass through a target neural network
with both adversarial and benign inputs to create a mixed-feature dataset of activation-label pairs.
Then, using the mixed-feature dataset, they train a secondary binary classiﬁer capable of discerning
between adversarial and benign inputs. When evaluating their models, the authors pass an input
through the target model to obtain the activation feature vectors for a particular input as well as a
potential classiﬁcation. They then pass this feature vector through the secondary classiﬁer. If the
secondary classiﬁer alerts that the input was adversarial, the classiﬁcation is thrown away. Otherwise,
classiﬁcation proceeds as normal."
DENSE LAYER ANALYSIS,0.450261780104712,"Sperl et al. evaluate this defense with 5 leading adversarial attacks on the MNIST and CIFAR-10
datasets using several models and report high accuracies for benign inputs and high detection rates
for adversarial inputs. The authors report a worst-case individual attack accuracy of 0.739."
DENSE LAYER ANALYSIS,0.45549738219895286,"In accordance with our framework, we assign the cross entropy loss of the classiﬁer to our primary
function and binary cross entropy loss of the detector as our secondary function."
DENSE LAYER ANALYSIS,0.4607329842931937,"We obtain source code and pre-trained defense models from the authors in order to ensure that our
attack matches the defense as closely as possible. We now detail the results of our attack at ϵ = .01"
DENSE LAYER ANALYSIS,0.46596858638743455,Published as a conference paper at ICLR 2022
DENSE LAYER ANALYSIS,0.4712041884816754,"Attack
eps=0.01
eps=0.031
SR@5
SR@50
SR@5
SR@50"
DENSE LAYER ANALYSIS,0.47643979057591623,"(SKC+20)
≤0.13∗
-
-
-
Reproduction
0.20+
-
-
-"
DENSE LAYER ANALYSIS,0.4816753926701571,"Orthogonal
0.374
0.163
1.0
0.718
Selective
0.83
0.441
1.0
0.865"
DENSE LAYER ANALYSIS,0.4869109947643979,"(a) Attack success rate for our two proposed attacks. ∗The original
paper reported only at a 20% FPR, we take this as an upper bound
for what could be achieved at 5% FPR. +When we reproduce the
authors attack, we reach slightly higher success rate of 20%."
DENSE LAYER ANALYSIS,0.49214659685863876,"0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0"
DENSE LAYER ANALYSIS,0.4973821989528796,True Positive Rate
DENSE LAYER ANALYSIS,0.5026178010471204,"Orthogonal
Selective"
DENSE LAYER ANALYSIS,0.5078534031413613,"(b) Defense ROC, ε = 0.01, AUC of 0.38"
DENSE LAYER ANALYSIS,0.5130890052356021,"Figure 3: DLA attack evaluation. Our attack succeeds with 83% probability compared to the original
evaluation of 13% (with ε = 0.01), and 100% of the time under the more typical 8/255 constraint."
DENSE LAYER ANALYSIS,0.518324607329843,"and at ϵ = .03 at false positive rates of 5% and 50% in Figure 3. We ﬁnd that our attack is extremely
effective, resulting in an accuracy of 0 at a detection rate of 0 with a false positive rate of 5% under
ϵ = .03 bounds and an AUC of 0.38. Finally, to validate that our attack succeeded, we again shared
the resulting adversarial examples with the authors who conﬁrmed our attack results."
SENSITIVITY INCONSISTENCY OF SPATIAL-TRANSFORM DOMAIN,0.5235602094240838,"5.3
SENSITIVITY INCONSISTENCY OF SPATIAL-TRANSFORM DOMAIN"
SENSITIVITY INCONSISTENCY OF SPATIAL-TRANSFORM DOMAIN,0.5287958115183246,"We next evaluated our attack on the Sensitivity Inconsistency Detector (SID) proposed by Tian et
al. (TZLD21). This defense relies on the observations of Fawzi et al. (FMDFS18) that adversarial
examples are movements, in the form of perturbations, of benign inputs in a decision space along
an adversarial direction. Tian et al. then conjecture that, because adversarial examples are likely
to lie near highly-curved decision boundaries, and benign inputs lie away from such boundaries,
ﬂuctuations in said boundaries will often result in a change in classiﬁcation of adversarial examples
but not in classiﬁcation of benign inputs."
SENSITIVITY INCONSISTENCY OF SPATIAL-TRANSFORM DOMAIN,0.5340314136125655,"To measure sensitivity against decision boundary transformations, Tian et al. design a dual classiﬁer
which is the composition of a weighted additive wavelet transform layer and a DNN classiﬁer with
the same structure as the original classiﬁer. When doing a forward pass of the system, the authors
run an input through both the primal and the dual model, then pass both results to the detector that
discriminates among adversarial and benign classes. With these models, the authors then deﬁne their
so-called feature of sensitivity inconsistency S(x0)."
SENSITIVITY INCONSISTENCY OF SPATIAL-TRANSFORM DOMAIN,0.5392670157068062,"S(x0) = {fi(x0) −gi(x0)}K
i=1
where fi(x0) and gi(x0) are the predictions of the primal and the dual respectively. Input x0 is
classiﬁed as adversarial if S(x0) is greater than a threshold φ. SID achieves improved adversarial
example detection performance, especially in cases with small perturbations in inputs. The authors
report a worst-case, individual attack detection AUC % of 0.95."
SENSITIVITY INCONSISTENCY OF SPATIAL-TRANSFORM DOMAIN,0.5445026178010471,"Now, we want to create adversarial examples that are misclassiﬁed by the original model and not
ﬂagged as adversarial by the Sensitivity Inconsistency Detector. We assign the loss of our target
model to our primary function and the loss of the Sensitivity Inconsistency Detector as our secondary
function. The initial target model had an accuracy of .94 and deemed .06 of all inputs adversarial."
SENSITIVITY INCONSISTENCY OF SPATIAL-TRANSFORM DOMAIN,0.5497382198952879,"We again obtain source code from the authors along with pre-trained models to ensure evaluation
correctness. We describe our attack’s results at ϵ = .01 and at ϵ = .03 at false positive rates of 5%
and 50% in Figure 4. Our attack works well in this case and induces an accuracy of 0 at a detection
rate of 0 with a false positive rate of 5% under ϵ = .03 bounds with an AUC of 0.25."
DETECTION THROUGH STEGANALYSIS,0.5549738219895288,"5.4
DETECTION THROUGH STEGANALYSIS"
DETECTION THROUGH STEGANALYSIS,0.5602094240837696,"Since adversarial perturbations alter the dependence between pixels in an image, Liu et al. (LZZ+19)
propose a defense which uses a steganalysis-inspired approach to detect “hidden features” within an"
DETECTION THROUGH STEGANALYSIS,0.5654450261780105,Published as a conference paper at ICLR 2022
DETECTION THROUGH STEGANALYSIS,0.5706806282722513,"Attack
eps=0.01
eps=0.031
SR@5
SR@50
SR@5
SR@50"
DETECTION THROUGH STEGANALYSIS,0.5759162303664922,"(TZLD21)
≤0.09∗
-
-
-"
DETECTION THROUGH STEGANALYSIS,0.581151832460733,"Orthogonal
0.931
0.766
1.0
0.984
Selective
0.911
0.491
1.0
0.886"
DETECTION THROUGH STEGANALYSIS,0.5863874345549738,"(a) Attack success rate for our two proposed attacks. ∗The orig-
inal paper only reports AUC values and does not report true
positive/false positive rates. The value of 9% was obtained by
running PGD on the author’s defense implementation."
DETECTION THROUGH STEGANALYSIS,0.5916230366492147,"0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0"
DETECTION THROUGH STEGANALYSIS,0.5968586387434555,True Positive Rate
DETECTION THROUGH STEGANALYSIS,0.6020942408376964,"Orthogonal
Selective"
DETECTION THROUGH STEGANALYSIS,0.6073298429319371,"(b) Defense ROC, ε = 0.01, AUC of 0.25"
DETECTION THROUGH STEGANALYSIS,0.612565445026178,"Figure 4: SID attack evaluation. Our attack succeeds with 93% probability compared to the original
evaluation of 9% under a ε = 0.01-norm constraint, and 100% under a ε = 0.031."
DETECTION THROUGH STEGANALYSIS,0.6178010471204188,"image. These features are then used to train binary classiﬁers to detect the perturbations. Unlike the
prior defenses, this paper evaluates on ImageNet, reasoning that small images such as those from
CIFAR-10 and MNIST do not provide enough inter-pixel dependency samples to construct efﬁcient
features for adversarial detection, so we attack this defense on ImageNet."
DETECTION THROUGH STEGANALYSIS,0.6230366492146597,"As a baseline, the authors use two feature extraction methods: SPAM and Spatial Rich Model. For
each pixel Xi,j of an image X, SPAM takes the difference between adjacent pixels along 8 directions.
For the rightward direction, a difference matrix A→is computed so that A→
i,j = Xi,j −Xi,j+1. A
transition probability matrix M →between pairs of differences can then be computed with"
DETECTION THROUGH STEGANALYSIS,0.6282722513089005,"M →
x,y = Pr(A→
i,j+1 = x|A→
i,j = y)"
DETECTION THROUGH STEGANALYSIS,0.6335078534031413,"where x, y ∈{−T, ..., T}, with T being a parameter used to control the dimensionality of the ﬁnal
feature set F. We use T = 3 in accordance with that used by the authors. The features themselves
are calculated by concatenating the average of the non-diagonal matrices with the average of the
diagonal matrices:"
DETECTION THROUGH STEGANALYSIS,0.6387434554973822,"F1,...,k = M →+ M ←+ M ↑+ M ↓"
DETECTION THROUGH STEGANALYSIS,0.643979057591623,"4
Fk+1,...,2k = M ↗+ M ↖+ M ↘+ M ↙"
DETECTION THROUGH STEGANALYSIS,0.6492146596858639,"4
In order to use the same attack implementation across all defenses, we reimplemented this defense
in PyTorch (the authors implementation was in matlab). Instead of re-implementing the full Fisher
Linear Discriminant (FLD) ensemble (KFH12) used by the authors, we train a 3-layer fully connected
neural network on SPAM features and use this as the detector. This allows us to directly investigate
the claim that SPAM features can be reliably used to detect adversarial examples, as FLD is a highly
non-differentiable operation and is not a fundamental component of the defense proposal."
DETECTION THROUGH STEGANALYSIS,0.6544502617801047,"The paper also proposes a second feature extraction method named “Spatial Rich Model” (SRM)
that we do not evaluate against. This scheme follows the same fundamental principle as SPAM in
modeling inter-pixel dependencies—there is only a marginal beneﬁt from using these more complex
models, and so we analyze the simplest variant of the scheme."
DETECTION THROUGH STEGANALYSIS,0.6596858638743456,"Notice that SPAM requires the difference matrices A to be discretized in order for the dimensionality
of the transition probability matrices M to be ﬁnite. To make this discretization step differentiable
and compatible with our attacks, we deﬁne a count matrix X where, for example, X→
x,y counts, for
every pair i, j, the number of occurrences of y in A→
i,j and x in A→
i,j+1. M →
x,y is then deﬁned by:"
DETECTION THROUGH STEGANALYSIS,0.6649214659685864,"M →
x,y = P(A→
i,j+1 = x|A→
i,j = y) =
X→
x,y
P"
DETECTION THROUGH STEGANALYSIS,0.6701570680628273,"x′ X→
x′,y"
DETECTION THROUGH STEGANALYSIS,0.675392670157068,"To construct a differentiable approximation, consider without loss of generality the rightward dif-
ference matrix A→
1 for an image. We construct a shifted copy of it A→
2 so that A→
2i,j = A→
1i,j+1. We
then deﬁne a mask K so that"
DETECTION THROUGH STEGANALYSIS,0.680628272251309,"Ki,j = 1[x ≤A→
2i,j < x + 1 ∩y ≤A→
1i,j < y + 1]"
DETECTION THROUGH STEGANALYSIS,0.6858638743455497,Published as a conference paper at ICLR 2022
DETECTION THROUGH STEGANALYSIS,0.6910994764397905,"Attack
eps=0.01
eps=0.031
SR@5
SR@50
SR@5
SR@50"
DETECTION THROUGH STEGANALYSIS,0.6963350785340314,"(LZZ+19)
0.03
-
.03
-"
DETECTION THROUGH STEGANALYSIS,0.7015706806282722,"Orthogonal
0.988
0.54
1.0
0.62"
DETECTION THROUGH STEGANALYSIS,0.7068062827225131,"(a) Attack success rate for our proposed attack. For computational
efﬁciency, we only run our Orthogonal attack as the detection
model has a throughput of one image per second."
DETECTION THROUGH STEGANALYSIS,0.7120418848167539,"0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0"
DETECTION THROUGH STEGANALYSIS,0.7172774869109948,True Positive Rate
DETECTION THROUGH STEGANALYSIS,0.7225130890052356,Orthogonal
DETECTION THROUGH STEGANALYSIS,0.7277486910994765,"(b) Defense ROC, ε = 0.01, AUC of 0.44"
DETECTION THROUGH STEGANALYSIS,0.7329842931937173,"Figure 5: Steganalysis attack evaluation. We ﬁnd it difﬁcult to decrease the detection score lower
than the original score on the non-adversarial input, thus the AUC is almost exactly 0.5."
DETECTION THROUGH STEGANALYSIS,0.7382198952879581,"Each element of the intermediate matrix X→
x,y counts the number of pairs in A→
1 and A→
2 which
would be rounded to x and y respectively after discretization:"
DETECTION THROUGH STEGANALYSIS,0.743455497382199,"X→
x,y = P"
DETECTION THROUGH STEGANALYSIS,0.7486910994764397,"i,j (K ◦A→
2 )i,j
x
where ◦is the Hadamard product. If we normalize X→so that the sum of elements in each column
is equal to 1, we get the probability of difference values x ∈A→
2 conditioned on column y ∈A→
1 .
Thus, for any pair of indices i, j,"
DETECTION THROUGH STEGANALYSIS,0.7539267015706806,"M →
x,y = P(A→
2i,j = x|A→
1i,j = y) =
X→
x,y
P"
DETECTION THROUGH STEGANALYSIS,0.7591623036649214,"x′ X→
x′,y
Using this differentiable formulation of SPAM feature extraction, we train an auxillary detector as
described above and use its gradients to apply our attack on the original, non-differentiable detector."
DETECTION THROUGH STEGANALYSIS,0.7643979057591623,"The authors evaluate their defense on 4 adversarial attacks and report high accuracy for benign inputs
and high detection rates for adversarial inputs. The best attack they develop still has a success rate
less than 3%. In contrast, our attack on SPAM using the differentiable approximation has a success
rate of 98.8% when considering a 5% false positive rate, with an AUC of 0.44, again less than the
random guessing threshold of 0.5."
CONCLUSION,0.7696335078534031,"6
CONCLUSION"
CONCLUSION,0.774869109947644,"Generating adversarial examples that satisfy multiple constraints simultaneously (e.g., requiring
that an input is both misclassiﬁed and deemed non-adversarial) requires more care than generating
adversarial examples that satisfy only one constraint (e.g., requiring only that an input is misclassiﬁed).
We ﬁnd that prior attacks unnecessarily over-optimizes one constraint when another constraint has
not yet been satisﬁed. Our new attack methogology is designed to avoid this weakness, and as a
result can reduce the accuracy of four previously-unbroken detection methods to 0% accuracy while
maintaining a 0% detection rate at 5% false positive rates."
CONCLUSION,0.7801047120418848,"We believe our attack approach is generally useful, but it is not a substitute for trying other attack
techniques. We do not envision this attack as a complete replacement for standard Lagrangian-based
attacks, but rather a complement; defenses must carefully consider their robustness to both prior
attacks as well as this new one. Notice, for example, that for one of the four defenses we study
Selective PGD performs better than Orthogonal PGD—indicating these attacks are complementary to
each other. Additionally, automated attack tools (CH20) would beneﬁt from adding our optimization
trick to their collection of known techniques that could (optionally) compose with other attacks. We
discourage future work from blindly applying this attack without properly understanding its design
criteria. While this attack is effective for the defenses we consider, it is not the only way to do so, and
may not be the correct way to do so in future defense evaluations. Evaluating adversarial example
defenses will necessarily require adapting any attack strategies to the defense’s design."
CONCLUSION,0.7853403141361257,Published as a conference paper at ICLR 2022
CONCLUSION,0.7905759162303665,ACKNOWLEDGEMENTS
CONCLUSION,0.7958115183246073,"We thank the authors of the papers we use in the case studies, who helped us answer questions
speciﬁc to their respective defenses and agreed to share their code with us. We are also grateful to
Alex Kurakin and the anonymous reviewers for comments on drafts of this paper."
ETHICS STATEMENT,0.8010471204188482,ETHICS STATEMENT
ETHICS STATEMENT,0.806282722513089,"All work that improves adversarial attacks has potential negative societal impacts. Yet, we believe
that it is better for those vulnerabilities to be known rather than relying on security through obscurity.
We have attacked no deployed system, and so cause no direct harm; and by describing how our attack
works, future defenses will be stronger. We have communicated the results of our attack to the authors
of the papers we break as a form of responsible disclosure, and also to ensure the correctness of our
results."
REPRODUCIBILITY STATEMENT,0.8115183246073299,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.8167539267015707,"All of the code we used to generate our results will be made open source in a GitHub repository. The
datasets we use (MNIST, CIFAR10, ImageNet) are available online and widely studied. We obtained
original copies of the code associated with 3 of the 4 case studies. We used code either directly from
the authors or code released publicly alongside an academic paper. In the case of steganalysis, we
reimplemented the paper to the best of our ability. We also provide a Python class constructor so that
future work can test or improve our results. Again, we relayed results to the authors of each paper
and received conﬁrmation that our adversarial examples were indeed adversarial and not detected by
the author’s original implementations."
REFERENCES,0.8219895287958116,REFERENCES
REFERENCES,0.8272251308900523,"[ACW18] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false
sense of security: Circumventing defenses to adversarial examples. In International
Conference on Machine Learning, 2018."
REFERENCES,0.8324607329842932,"[ASE+18] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivas-
tava, and Kai-Wei Chang. Generating natural language adversarial examples. CoRR,
abs/1804.07998, 2018."
REFERENCES,0.837696335078534,"[BCM+13] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndi´c, Pave˜l
Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at
test time. In Joint European conference on machine learning and knowledge discovery
in databases, pages 387–402. Springer, 2013."
REFERENCES,0.8429319371727748,"[CH20] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with
an ensemble of diverse parameter-free attacks. In Proceedings of the 37th International
Conference on Machine Learning, pages 2206–2216. PMLR, 2020."
REFERENCES,0.8481675392670157,"[CRK19] Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certiﬁed adversarial robustness
via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019."
REFERENCES,0.8534031413612565,"[CW17a] Nicholas Carlini and David Wagner. Adversarial examples are not easily detected:
Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on
Artiﬁcial Intelligence and Security, pages 3–14, 2017."
REFERENCES,0.8586387434554974,"[CW17b] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural
networks. In 2017 IEEE symposium on security and privacy, pages 39–57. IEEE, 2017."
REFERENCES,0.8638743455497382,"[CW18] Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on
speech-to-text. In 2018 IEEE Security and Privacy Workshops (SPW), pages 1–7, 2018."
REFERENCES,0.8691099476439791,Published as a conference paper at ICLR 2022
REFERENCES,0.8743455497382199,"[DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A
large-scale hierarchical image database. In 2009 IEEE conference on computer vision
and pattern recognition, pages 248–255. Ieee, 2009."
REFERENCES,0.8795811518324608,"[FCSG17] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting
adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017."
REFERENCES,0.8848167539267016,"[FMDFS18] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano
Soatto. Empirical study of the topology and geometry of deep networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018."
REFERENCES,0.8900523560209425,"[GSS15] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples. International Conference on Learning Representations, 2015."
REFERENCES,0.8952879581151832,"[HPG+17] Sandy H. Huang, Nicolas Papernot, Ian J. Goodfellow, Yan Duan, and Pieter Abbeel.
Adversarial attacks on neural network policies. CoRR, abs/1702.02284, 2017."
REFERENCES,0.900523560209424,"[JMW+20] Linxi Jiang, Xingjun Ma, Zejia Weng, James Bailey, and Yu-Gang Jiang. Imbal-
anced gradients: A new cause of overestimated adversarial robustness. arXiv preprint
arXiv:2006.13726, 2020."
REFERENCES,0.9057591623036649,"[KFH12] Jan Kodovsky, Jessica Fridrich, and Vojtˇech Holub. Ensemble classiﬁers for steganalysis
of digital media. In IEEE Transactions on Information Forensics and Security, pages
432–444, 2012."
REFERENCES,0.9109947643979057,"[KH09] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images.
Master’s thesis, Department of Computer Science, University of Toronto, 2009."
REFERENCES,0.9162303664921466,"[LAG+19] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana.
Certiﬁed robustness to adversarial examples with differential privacy. In 2019 IEEE
Symposium on Security and Privacy (SP), pages 656–672. IEEE, 2019."
REFERENCES,0.9214659685863874,"[LZZ+19] Jiayang Liu, Weiming Zhang, Yiwei Zhang, Dongdong Hou, Yujia Liu, Hongyue
Zha, and Nenghai Yu. Detection based defense against adversarial examples from the
steganalysis point of view. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 4825–4834, 2019."
REFERENCES,0.9267015706806283,"[MC17] Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial
examples. In Proceedings of the 2017 ACM SIGSAC conference on computer and
communications security, pages 135–147, 2017."
REFERENCES,0.9319371727748691,"[MGFB17] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting
adversarial perturbations. arXiv preprint arXiv:1702.04267, 2017."
REFERENCES,0.93717277486911,"[MLW+18] Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant
Schoenebeck, Dawn Song, Michael E Houle, and James Bailey. Characterizing adver-
sarial subspaces using local intrinsic dimensionality. arXiv preprint arXiv:1801.02613,
2018."
REFERENCES,0.9424083769633508,"[MMS+17] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. International
Conference on Learning Representations, 2017."
REFERENCES,0.9476439790575916,"[RKH19] Kevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd: A statistical test
for detecting adversarial examples. In International Conference on Machine Learning,
pages 5498–5507. PMLR, 2019."
REFERENCES,0.9528795811518325,"[RSL18] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against
adversarial examples. arXiv preprint arXiv:1801.09344, 2018."
REFERENCES,0.9581151832460733,"[SKC+20] Philip Sperl, Ching-Yu Kao, Peng Chen, Xiao Lei, and Konstantin Böttinger. Dla:
Dense-layer-analysis for adversarial example detection.
In 2020 IEEE European
Symposium on Security and Privacy (EuroS&P), pages 198–215. IEEE, 2020."
REFERENCES,0.9633507853403142,Published as a conference paper at ICLR 2022
REFERENCES,0.9685863874345549,"[SKCB19] Philip Sperl, Ching-yu Kao, Peng Chen, and Konstantin Böttinger. DLA: dense-layer-
analysis for adversarial example detection. CoRR, abs/1911.01921, 2019."
REFERENCES,0.9738219895287958,"[SWW+20] Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y Zhao.
Gotta catch’em all: Using honeypots to catch adversarial attacks on neural networks. In
Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications
Security, pages 67–83, 2020."
REFERENCES,0.9790575916230366,"[SZS+14] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
and Rob Goodfellow, Ian an d Fergus. Intriguing properties of neural networks. In
International Conference on Learning Representations (ICLR), 2014."
REFERENCES,0.9842931937172775,"[TCBM20] Florian Tramèr, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive
attacks to adversarial example defenses. CoRR, abs/2002.08347, 2020."
REFERENCES,0.9895287958115183,"[TZLD21] Jinyu Tian, Jiantao Zhou, Yuanman Li, and Jia Duan.
Detecting adversarial ex-
amples from sensitivity inconsistency of spatial-transform domain. arXiv preprint
arXiv:2103.04302, 2021."
REFERENCES,0.9947643979057592,"[XEQ17] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial
examples in deep neural networks. arXiv preprint arXiv:1704.01155, 2017."
