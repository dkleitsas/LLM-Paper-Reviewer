Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0031645569620253164,"In this paper, we propose THOMAS, a joint multi-agent trajectory prediction
framework allowing for an efÔ¨Åcient and consistent prediction of multi-agent multi-
modal trajectories. We present a uniÔ¨Åed model architecture for simultaneous agent
future heatmap estimation, in which we leverage hierarchical and sparse image
generation for fast and memory-efÔ¨Åcient inference. We propose a learnable tra-
jectory recombination model that takes as input a set of predicted trajectories for
each agent and outputs its consistent reordered recombination. This recombina-
tion module is able to realign the initially independent modalities so that they do
no collide and are coherent with each other. We report our results on the Interac-
tion multi-agent prediction challenge and rank 1st on the online test leaderboard."
INTRODUCTION,0.006329113924050633,"1
INTRODUCTION"
INTRODUCTION,0.00949367088607595,"Motion forecasting is an essential step in the pipeline of an autonomous driving vehicle, transform-
ing perception data into future prediction which are then leveraged to plan the future moves of the
autonomous cars. The self-driving stacks needs to predict the future trajectories for all the neighbor
agents, in a fast and coherent way."
INTRODUCTION,0.012658227848101266,"The interactivity between agents plays an important role for accurate trajectory prediction. Agents
need to be aware of their neighbors in order to adapt their speed, yield right of way and merge in
neighbor lanes. To do so, different interaction mechanisms have been developed, such as social
pooling (Alahi et al., 2016; Lee et al., 2017; Deo & Trivedi, 2018), graphs (Salzmann et al., 2020;
Zeng et al., 2021) or attention (Mercat et al., 2020; Messaoud et al., 2020; Luo et al., 2020; Gao
et al., 2020; Liang et al., 2020), beneÔ¨Åting from the progress of powerful transformer architectures
(Li et al., 2020; Yuan et al., 2021; Girgis et al., 2021; Ngiam et al., 2021) . These mechanisms
allow agents to look at and share features with neighbors and to take them into account in their own
predictions."
INTRODUCTION,0.015822784810126583,"Multi-modality is another important aspect of the possible future trajectories. A car can indeed chose
to turn right or left, or decide to realise a certain maneuver in various ways. Uncertainty modeled
as variance of Gaussians is insufÔ¨Åcient to model these multiple cases, as it can only represent a
continuous spread and cannot show multiple discrete possibilities. Therefore, current state-of-the-art
produces not one but K possible trajectories for each agent predicted, and most recent benchmarks
(Caesar et al., 2020; Chang et al., 2019; Zhan et al., 2019; Ettinger et al., 2021) include multi-
modality in their metrics, taking only the minimum error over a predicted set of K trajectories."
INTRODUCTION,0.0189873417721519,"However, up until very recently and the opening of multi-agent joint interaction challenges (Ettinger
et al., 2021; Zhan et al., 2021), no motion forecasting prediction datasets were taking into account
the coherence of modalities between different agents predicted at the same time. As a result, the
most probable predicted modality of a given agent could crash with the most probable modality of
another agent."
INTRODUCTION,0.022151898734177215,"Our THOMAS model encodes the past trajectories of all the agents present in the scene, as well
as the HD-Map lanelet graph, and then predicts for each agent a sparse heatmap representing the
future probability distribution at a Ô¨Åxed timestep in the future, using hierarchical reÔ¨Ånement for very
efÔ¨Åcient decoding. A deterministic sampling algorithm then iteratively selects the best K trajectory"
INTRODUCTION,0.02531645569620253,Published as a conference paper at ICLR 2022
INTRODUCTION,0.028481012658227847,"endpoints according to the heatmap for each agent, in order to represent a wide and diverse spec-
trum of modalities. Given this wide spectrum of endpoints, a recombination module takes care of
addressing consistency in the scene among agents."
INTRODUCTION,0.03164556962025317,Our contributions are summarized as follow:
INTRODUCTION,0.03481012658227848,"‚Ä¢ We propose a hierarchical heatmap decoder allowing for unconstrained heatmap genera-
tion with optimized computational costs, enabling efÔ¨Åcient simultaneous multi-agent pre-
diction."
INTRODUCTION,0.0379746835443038,"‚Ä¢ We design a novel recombination model able to recombine the sampled endpoints to obtain
scene-consistent trajectories across the agents."
RELATED WORK,0.04113924050632911,"2
RELATED WORK"
RELATED WORK,0.04430379746835443,"Learning-based models have quickly overtaken physics-based methods for trajectory prediction for
several reasons. First, the sequential nature of trajectories is a logical application for recurrent
architectures (Alahi et al., 2016; Altch¬¥e & de La Fortelle, 2017; Lee et al., 2017; Mercat et al.,
2020; Khandelwal et al., 2020). Then, beneÔ¨Åting from the latest progresses in computer vision,
convolutional layers can easily be applied to bird-view rasters of the map context (Lee et al., 2017;
Tang & Salakhutdinov, 2019; Cui et al., 2019; Hong et al., 2019; Salzmann et al., 2020; Chai et al.,
2020; Gilles et al., 2021b). A more compact representation closer to the trajectory space can encode
surrounding HD-Maps (usually formalized as connected lanelets), using Graph Neural Networks
(Gao et al., 2020; Liang et al., 2020; Zeng et al., 2021; Gilles et al., 2021a). Finally, some point-
based approaches (Ye et al., 2021) can be applied in a broader way to trajectory prediction, as both
lanes and trajectories can be considered as ordered set of points."
RELATED WORK,0.04746835443037975,"Multi-modality in prediction can be obtained through a multiple prediction head in the model (Cui
et al., 2019; Liang et al., 2020; Ngiam et al., 2021; Deo et al., 2021). However, some methods
rather adopt a candidate-based approach where potential endpoints are obtained either from anchor
trajectories through clustering (Chai et al., 2020; Phan-Minh et al., 2020) or from a model-based
generator (Song et al., 2021). Other approaches use a broader set of candidates from the context
graph (Zhang et al., 2020; Zhao et al., 2020; Zeng et al., 2021; Kim et al., 2021) or a dense grid
around the target agent (Deo & Trivedi, 2020; Gu et al., 2021; Gilles et al., 2021b;a). Another family
of approaches use variational inference to generate diverse predictions through latent variables (Lee
et al., 2017; Rhinehart et al., 2018; Tang & Salakhutdinov, 2019; Casas et al., 2020) or GAN (Gupta
et al., 2018; Rhinehart et al., 2018; Sadeghian et al., 2019) but the sampling of these trajectories is
stochastic and does not provide any probability value for each sample."
RELATED WORK,0.05063291139240506,"While very little work has directly tackled multi-agent prediction and evaluation so far, multiple
methods hint at the ability to predict multiple agents at the same time (Liang et al., 2020; Ivanovic
et al., 2020; Zeng et al., 2021) even if they then focus on a more single-agent oriented framework.
Other works (Alahi et al., 2016; Tang & Salakhutdinov, 2019; Rhinehart et al., 2019; Girgis et al.,
2021) use autoregressive roll-outs to condition the future step of an agent on the previous steps
of all the other agents. SceneTransformer (Ngiam et al., 2021) repeats each agent features across
possible modalities, and performs self-attention operations inside each modality before using a loss
computed jointly among agents to train a model and evaluate on the WOMD dataset (Ettinger et al.,
2021) interaction track. ILVM (Casas et al., 2020) uses scene latent representations conditioned
on all agents to generate scene-consistent samples, but its variational inference does not provide a
conÔ¨Ådence score for each modality, hence LookOut (Cui et al., 2021) proposes a scenario scoring
function and a diverse sampler to improve sample efÔ¨Åciency. AIR2 (Wu & Wu, 2021) extends
Multipath (Chai et al., 2020) and produces a cross-distribution for two agents along all possible
trajectory anchors, but it scales exponentially with the number of agents, making impractical for a
real-time implementation that could encounter more than 10 agents at the same time."
RELATED WORK,0.05379746835443038,"The approach most related to this paper is GOHOME (Gilles et al., 2021a), which uses a similar
graph encoder and then leverage lane rasters to generate a probability heatmap in a sparse manner.
However, this lane-generated heatmap remains constrained to the drivable area and to Ô¨Åxed lane-
widths. Another close approach is the one of DenseTNT (Gu et al., 2021), which also uses attention
to a grid of points in order to obtain a dense prediction, but their grid also remains constrained to a"
RELATED WORK,0.056962025316455694,Published as a conference paper at ICLR 2022
RELATED WORK,0.060126582278481014,Heatmap
RELATED WORK,0.06329113924050633,estimation
RELATED WORK,0.06645569620253164,Endpoint
RELATED WORK,0.06962025316455696,sampling
RELATED WORK,0.07278481012658228,Endpoint
RELATED WORK,0.0759493670886076,"recombination 1 2 3 2
1
3 3 2 1 1
3 2 2
1
3 3 2
1"
RELATED WORK,0.07911392405063292,Figure 1: Illustration of the THOMAS multi-agent prediction pipeline
RELATED WORK,0.08227848101265822,"neighborhood of the drivable area. Finally, none of these previous methods considers the problems
of scene consistency for multi-agent prediction."
METHOD,0.08544303797468354,"3
METHOD"
METHOD,0.08860759493670886,"Our goal is to predict the future T timesteps of A agents using their past history made of H timesteps
and the HD-Map context. Similar to recent works (Zhao et al., 2020; Zeng et al., 2021; Gu et al.,
2021), we will divide the problem into goal-based prediction followed by full trajectory reconstruc-
tion. Our prediction pipeline is displayed in Fig. 1. We Ô¨Årst encode each agent trajectory and the
HD-Map context graph into a common representation. We then decode a future probability heatmap
for each agent in the scene, which we sample heuristically to maximize coverage. Finally, we re-
combine the sampled endpoints into scene-consistent modalities across agents and build the full
trajectories for each agent."
METHOD,0.09177215189873418,"Our pipeline shares the same graph encoder, sampling algorithm and full trajectory generation as
GOHOME (Gilles et al., 2021a), but uses a novel efÔ¨Åcient hierarchical heatmap process that enables
to scale to simultaneous multi-agent prediction. Furthermore, we add a novel scene-consistency
module that recombines the marginal outputs into a joint prediction."
MODEL BACKBONE,0.0949367088607595,"3.1
MODEL BACKBONE"
GRAPH ENCODER,0.0981012658227848,"3.1.1
GRAPH ENCODER"
GRAPH ENCODER,0.10126582278481013,"We use the same encoder as the GOHOME model (Gilles et al., 2021a). The agent trajectories are
encoded though TrajEncoder using a 1D CNN followed by a UGRU recurrent layer (Rozenberg
et al., 2021), and the HD-Map is encoded as a lanelet graph using a GNN GraphEncoder made of
graph convolutions. We then run cross-attention Lanes2Agents to add context information to the
agent features, followed by self-attention Agents2Agents to observe interaction between agents.
The Ô¨Ånal result is an encoding Fa for each agent, where history, context and interactions have been
summarized. This encoding Fa is used in the next decoder operations and is also stored to be
potentially used in modality recombination described in Sec. 3.2. The resulting architecture of these
encoding operations is illustrated in the Ô¨Årst half of Fig. 2."
GRAPH ENCODER,0.10443037974683544,"Lane 
Features"
GRAPH ENCODER,0.10759493670886076,Agents past trajectories
GRAPH ENCODER,0.11075949367088607,GraphEncoder
GRAPH ENCODER,0.11392405063291139,TrajEncoder
GRAPH ENCODER,0.11708860759493671,Lanes2Agents
GRAPH ENCODER,0.12025316455696203,Agents2Agents
GRAPH ENCODER,0.12341772151898735,HD Map
GRAPH ENCODER,0.12658227848101267,Map features
GRAPH ENCODER,0.12974683544303797,"Agents 
features"
GRAPH ENCODER,0.13291139240506328,"Map aware
Agents features Graph"
GRAPH ENCODER,0.1360759493670886,"Common 
context"
GRAPH ENCODER,0.13924050632911392,"GraphEncoder2
HierarchicalDecoder"
GRAPH ENCODER,0.14240506329113925,Heatmap 1
GRAPH ENCODER,0.14556962025316456,GraphEncoder2
GRAPH ENCODER,0.14873417721518986,Heatmap N
GRAPH ENCODER,0.1518987341772152,Agent 1 Graph
GRAPH ENCODER,0.1550632911392405,Agent 1
GRAPH ENCODER,0.15822784810126583,Agent N ‚ãÆ
GRAPH ENCODER,0.16139240506329114,Agent N Graph
GRAPH ENCODER,0.16455696202531644,LaneEncoder
GRAPH ENCODER,0.16772151898734178,HierarchicalDecoder
GRAPH ENCODER,0.17088607594936708,Figure 2: Model architecture for multi-agent prediction with shared backbone
GRAPH ENCODER,0.17405063291139242,Published as a conference paper at ICLR 2022
GRAPH ENCODER,0.17721518987341772,"Figure 3: Hierarchical iterative reÔ¨Ånement of the grid probabilities. First, the full grid is evaluated
at a very low resolution, then the highest cells are up-sampled and evaluated at a higher resolution,
until Ô¨Ånal resolution is reached. We highlight in grey the restricted area considered for reÔ¨Ånement at
each step."
HIERARCHICAL GRID DECODER,0.18037974683544303,"3.1.2
HIERARCHICAL GRID DECODER"
HIERARCHICAL GRID DECODER,0.18354430379746836,"Our aim here is to decode each agent encoding into a heatmap representing its future probability
distribution at prediction horizon T. Since we create this heatmap for each agent in the scene, the
decoding process has to be fast so that it can be applied to a great number of agents in parallel. We
use hierarchical predictions at various levels of resolutions so that the decoder has the possibility of
predicting over the full surroundings of the agent but learns to reÔ¨Åne with more precision only in
places where the agent will end up with high probability. This hierarchical process is illustrated in
Fig. 3."
HIERARCHICAL GRID DECODER,0.18670886075949367,"Starting from an initial full dense grid probability at low resolution R0 √óR0 by pixels, we iteratively
reÔ¨Åne the resolution by a Ô¨Åxed factor f until we reach the desired Ô¨Ånal resolution Rfinal √ó Rfinal.
At each iteration i, we select only the Ni highest ranking grid points from the previous iteration,
and upsample only these points to the Ri √ó Ri = Ri‚àí1/f √ó Ri‚àí1/f. At each step, the grid points
features are computed by a 2-layer MLP applied on the point coordinates, these features are then
concatenated to the agent encoding and passed through a linear layer, and Ô¨Ånally enriched by a 2-
layer cross-attention on the graph lane features, before applying a linear layer with sigmoid to get
the probability."
HIERARCHICAL GRID DECODER,0.189873417721519,"For a given W output range, this hierarchical process allows the model to only operate on W/R0 √ó
W/R0 + P"
HIERARCHICAL GRID DECODER,0.1930379746835443,"i Ni √ó f 2 grid points instead of the W/Rfinal √ó W/Rfinal available. In practice, for a
Ô¨Ånal output range of 192 meters with desired Rfinal = 0.5m resolution, we start with an initial
resolution of R0 = 8m and use two iterations of (N1, N2) = (16, 64) points each and an upscaling
factor f = 4. This way, we compute only 1856 grid points from the 147 456 available, with no
performance loss."
HIERARCHICAL GRID DECODER,0.1962025316455696,"The heatmap is trained on each resolution level using as pixel-wise focal loss as in Gilles et al.
(2021b), detailed in Appendix A.3, with the ground truth being a Gaussian centered at the target
agent future position."
FULL TRAJECTORY GENERATION,0.19936708860759494,"3.1.3
FULL TRAJECTORY GENERATION"
FULL TRAJECTORY GENERATION,0.20253164556962025,"Having a heatmap as output grants us the good property of letting us decide to privilege coverage in
order to give a wide spectrum of candidates to the scene recombination process. From each heatmap,
we therefore decode K end points using the same MissRate optimization algorithm as Gilles et al.
(2021b). We then generate the full trajectories for each end point using also the same model, a
fully-connected MLP. The MLP takes the car history and predicted endpoint as input and produces
T 2D-coordinates representing the full future trajectory. At training time, this model is trained using
ground-truth endpoints."
FULL TRAJECTORY GENERATION,0.20569620253164558,Published as a conference paper at ICLR 2022 Keys
FULL TRAJECTORY GENERATION,0.2088607594936709,Values
FULL TRAJECTORY GENERATION,0.2120253164556962,Cross-attention
FULL TRAJECTORY GENERATION,0.21518987341772153,Scene modality ùëÜ1
FULL TRAJECTORY GENERATION,0.21835443037974683,Scene modality ùëÜùëô
FULL TRAJECTORY GENERATION,0.22151898734177214,Queries
FULL TRAJECTORY GENERATION,0.22468354430379747,"Agent modality ùê¥1
ùëé"
FULL TRAJECTORY GENERATION,0.22784810126582278,"Agent modality ùê¥ùëò
ùëé"
FULL TRAJECTORY GENERATION,0.2310126582278481,"Agent 1 
Agent a"
FULL TRAJECTORY GENERATION,0.23417721518987342,"Agent modality ùê¥1
1"
FULL TRAJECTORY GENERATION,0.23734177215189872,"Agent modality ùê¥ùëò
1 ‚Ä¶"
FULL TRAJECTORY GENERATION,0.24050632911392406,Scene modality ùëÜ1
FULL TRAJECTORY GENERATION,0.24367088607594936,Scene modality ùëÜùëô
FULL TRAJECTORY GENERATION,0.2468354430379747,"Agent modality ùê¥1
ùëé"
FULL TRAJECTORY GENERATION,0.25,"Agent modality ùê¥ùëò
ùëé"
FULL TRAJECTORY GENERATION,0.25316455696202533,"Agent 1 
Agent a"
FULL TRAJECTORY GENERATION,0.2563291139240506,"Agent modality ùê¥1
1"
FULL TRAJECTORY GENERATION,0.25949367088607594,"Agent modality ùê¥ùëò
1 ‚Ä¶ ùëù1 1 ùëùùëô 1 ùëù1 ùëé ùëùùëô ùëé"
FULL TRAJECTORY GENERATION,0.2626582278481013,"Linear 
recombination L x D"
FULL TRAJECTORY GENERATION,0.26582278481012656,A x K x D
FULL TRAJECTORY GENERATION,0.2689873417721519,L x A x 2
FULL TRAJECTORY GENERATION,0.2721518987341772,"L x A x K ùëêùëô ùëò,ùëé"
FULL TRAJECTORY GENERATION,0.27531645569620256,A x K x D
FULL TRAJECTORY GENERATION,0.27848101265822783,Figure 4: Illustration of THOMAS methods for generation of scene-consistent agent modalities
MODALITY RECOMBINATION FOR MULTI-AGENT CONSISTENT PREDICTION,0.28164556962025317,"3.2
MODALITY RECOMBINATION FOR MULTI-AGENT CONSISTENT PREDICTION"
MODALITY RECOMBINATION FOR MULTI-AGENT CONSISTENT PREDICTION,0.2848101265822785,"The difÔ¨Åculty in multimodal multi-agent prediction resides in having coherent modalities between
each agent. Since the modalities are considered scene-wise, each agent‚Äôs most probable modality
has to match with the most probable prediction of the other agents, and so on. Moreover, these
modalities must not collide with each other, as they should represent realistic scenarios."
MODALITY RECOMBINATION FOR MULTI-AGENT CONSISTENT PREDICTION,0.2879746835443038,"Initially, the prediction output of the model can be deÔ¨Åned as marginal, as all A agents have been
predicted and sampled independently in order to get K possible endpoint each. Our goal is to output
a set of scene predictions J = (L, A) from the marginal prediction M = (A, K), where each
scene modality l belonging to J is an association of endpoints pa
l for each agent a. To achieve
this, our main hypothesis lays in the fact that good trajectory proposals are already present in the
marginal predictions, but they need to be coherently aligned among agents to achieve a set of overall
consistent scene predictions. For a given agent a, the modality selected for the scene l would be
a combination of this agent available marginal modalities pa
k ‚ààM. Each scene modality l would
select a different association between the agents."
MODALITY RECOMBINATION FOR MULTI-AGENT CONSISTENT PREDICTION,0.2911392405063291,"We illustrate our scene modality generation process in Fig. 4. The model learns L scene modality
embeddings Sl of D features each. K √ó A agent modality vectors Aa
k are also derived from each
agent modality endpoint. These vectors are obtained through a 2-layer MLP applied on the agent
modality coordinates pa
k , to which the stored agent encoding Fa (previously described in Sec. 3.1.1)
is concatenated in order to help the model recognise modalities from the same agent. The scene
modality vectors Sl are ‚Äôspecialized‚Äô by querying the available modality proposals Aa
k of each agent
through cross-attention layers. Subsequently, a matching score ck,a
l
between each scene modality Sl
and each agent modality Aa
k is computed. This matching score can be intuitively interpreted as the
selection of the role (maneuver k) that the agent a would play in the overall trafÔ¨Åc scene l:"
MODALITY RECOMBINATION FOR MULTI-AGENT CONSISTENT PREDICTION,0.29430379746835444,"ck,a
l
= Sl.Aa
k
T"
MODALITY RECOMBINATION FOR MULTI-AGENT CONSISTENT PREDICTION,0.2974683544303797,"Since argmax is non-differentiable, we employ a soft argmax as a weighted linear combination of
the agent modalities pa
k using a softmax on the ck,a
l
scores:"
MODALITY RECOMBINATION FOR MULTI-AGENT CONSISTENT PREDICTION,0.30063291139240506,"pa
l =
X"
MODALITY RECOMBINATION FOR MULTI-AGENT CONSISTENT PREDICTION,0.3037974683544304,"k
softmax(ck,a
l
)pa
k"
MODALITY RECOMBINATION FOR MULTI-AGENT CONSISTENT PREDICTION,0.3069620253164557,"The recombination module is trained with the scene-level minimum displacement error, where for
each modality inside a single scene all predicted agent displacement losses are averaged before
taking the minimum (winner-takes-all loss) over the L scene modalities. A formal deÔ¨Åniton of this
loss, also used as a metric, is given in Eq. 2 in Sec. 4.2 (minSFDEk)."
MODALITY RECOMBINATION FOR MULTI-AGENT CONSISTENT PREDICTION,0.310126582278481,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.31329113924050633,"4
EXPERIMENTS"
DATASET,0.31645569620253167,"4.1
DATASET"
DATASET,0.31962025316455694,"We use the Interaction v1.2 dataset that has recently opened a new multi-agent track in the context
of its Interpret challenge. It contains 47 584 training cases, 11 794 validation cases and 2 644 testing
cases, with each case containing between 1 and 40 agents to predict simultaneously."
METRICS,0.3227848101265823,"4.2
METRICS"
METRICS,0.3259493670886076,"For a set of k predictions pa
k for each agent a in a scene, we report the minimum Final Displacement
Error (minFDEk) to the ground truth ÀÜpa and the MissRate (MRk), whose deÔ¨Ånition of a miss is
speciÔ¨Åed according to the evaluated dataset in Appendix A.1. In their usual marginal deÔ¨Ånition,
these metrics are averaged over agents after the minimum operation, which means that the best
modality of each agent is selected independently for each and then averaged:"
METRICS,0.3291139240506329,minFDEk = 1 A X
METRICS,0.3322784810126582,"a
mink‚à•pa
k ‚àíÀÜpa‚à•2 , MRk = 1 A X"
METRICS,0.33544303797468356,"a
mink1a
miss k
(1)"
METRICS,0.33860759493670883,"For consistent scene multi-agent prediction, we report the joint scene-level metrics, where the aver-
age operation over the agents is done before the minimum operator. In this formulation, the mini-
mum is taken over scene modalities, meaning that only the best scene (joint over agents) modality
is taken into account:"
METRICS,0.34177215189873417,"minSFDEk = mink
1
A X"
METRICS,0.3449367088607595,"a
‚à•pa
k ‚àíÀÜpa‚à•2 , SMRk = mink
1
A X"
METRICS,0.34810126582278483,"a
1a
miss k
(2)"
METRICS,0.3512658227848101,"In other words, the marginal metrics pick their optimal solutions in a pool of k to the power of a
predicted solutions, while the joint metrics restrict this pool to only k possibilities, making it a much
more complex problem."
METRICS,0.35443037974683544,"We also report the scene collision rate (SCR) which is the percentage of modalities where two or
more agents collide together, and the consistent collision-free joint Miss Rate (cSMR), which is
SMR where colliding modalities are also counted as misses."
COMPARISON WITH STATE-OF-THE-ART,0.3575949367088608,"4.3
COMPARISON WITH STATE-OF-THE-ART"
COMPARISON WITH STATE-OF-THE-ART,0.36075949367088606,"We compare our THOMAS model performance with other joint predictions methods ILVM (Casas
et al., 2020) and SceneTransformer (Ngiam et al., 2021). For fair comparison, we use a GOHOME
encoder for each of the method, and adapt them accordingly so that they predict only endpoints
similar to our method. For each method, we focus on implementing the key idea meant to solve
scene consistency and keep the remaining part of the model as close as possible to our approach for
fair comparison:"
COMPARISON WITH STATE-OF-THE-ART,0.3639240506329114,"ILVM (Casas et al., 2020) uses variational inference to learn a latent representation of the scene
conditioned of each agent with a Scene Interaction Module, and decodes it with a similar Scene
Interaction Module. The required number of modalities is obtained by sampling the latent space as
many times as require. Even though the sampling is independent for each agent, the latent space is
generated conditionally on all the agents."
COMPARISON WITH STATE-OF-THE-ART,0.3670886075949367,"SceneTransformer (Ngiam et al., 2021) duplicates the agent encoding with the number of modal-
ities required and adds a one-hot encoding speciÔ¨Åc to each modality. They then apply a shared
transformer architecture on all modalities to encode intra-modality interactions between agents and
generate the Ô¨Ånal trajectories."
COMPARISON WITH STATE-OF-THE-ART,0.370253164556962,"More details on the state-of-the-art method and the re-implementations we used are given in Ap-
pendix A.4. The results are reported in Tab. 1. While having comparable marginal distance perfor-
mance (demonstrating that our model is not inherently more powerful or leveraging more informa-
tion), THOMAS signiÔ¨Åcantly outperforms other methods on every joint metric. SMR is improved
by about 25% and SCR by almost 30%, leading to a combined cSMR decreased by also more than
25%. We also report our numbers from the Interpret multi-agent online test set leaderboard in Tab."
COMPARISON WITH STATE-OF-THE-ART,0.37341772151898733,Published as a conference paper at ICLR 2022
COMPARISON WITH STATE-OF-THE-ART,0.37658227848101267,Table 1: Comparison of consistent solutions on Interpret multi-agent validation track
COMPARISON WITH STATE-OF-THE-ART,0.379746835443038,"Marginal metrics
Joint metrics
mADE
mFDE
MR
mFDE
MR
SCR
cMR
ILVM (Casas et al., 2020)
0.30
0.62
10.8
0.84
19.8
5.7
21.3
SceneTranformer (Ngiam et al., 2021)
0.29
0.59
10.5
0.84
15.7
3.4
17.3
THOMAS
0.31
0.60
8.2
0.76
11.8
2.4
12.7"
COMPARISON WITH STATE-OF-THE-ART,0.3829113924050633,"5 of Appendix A.5. For better comparison with existing single-agent trajectory prediction state-of-
the-art, we provide evaluation numbers on the single-agents benchmarks Argoverse (Chang et al.,
2019) and NuScenes (Caesar et al., 2020) in Appendix A.7."
ABLATION STUDIES,0.3860759493670886,"4.4
ABLATION STUDIES"
RECOMBINATION MODULE,0.38924050632911394,"4.4.1
RECOMBINATION MODULE"
RECOMBINATION MODULE,0.3924050632911392,We establish the following baselines to assess the effects our THOMAS recombination.
RECOMBINATION MODULE,0.39556962025316456,"Scalar output: we train a model with the GOHOME graph encoder and a multimodal scalar re-
gression head similar to Liang et al. (2020). We optimize it with either marginal and joint loss
formulation."
RECOMBINATION MODULE,0.3987341772151899,"Heatmap output with deterministic sampling: we try various sampling methods applied on the
heatmap, with either the deterministic sampling as described in Gilles et al. (2021a) or a learned
sampling trained to directly regress the sampled modalities from the input heatmap."
RECOMBINATION MODULE,0.40189873417721517,"We report the comparison between the aforementioned baselines and THOMAS in Tab. 2. Com-
pared to these baselines, THOMAS can be seen as an hybrid sampling method that takes the result
of deterministic sampling as input and learns to recombine it into a more coherent solution. With
regard to the joint algorithmic sampling that only tackled collisions but has little to no effect on
actual consistency, as highlighted by the big drop in collision rate from 7.2% to 2.6% but a similar
joint SMR, THOMAS actually brings a lot of consistency in the multi-agent prediction and drops
the joint SMR from 14.8% to 11.8%."
RECOMBINATION MODULE,0.4050632911392405,Table 2: Comparison of consistent solutions on Interpret multi-agent validation track
RECOMBINATION MODULE,0.40822784810126583,"Marginal metrics
Joint metrics
Output
Sampling
Objective
mADE
mFDE
MR
mFDE
MR
Col
cMR
Scalar
Marg
0.28
0.59
10.4
1.04
23.7
6.4
24.9
Scalar
Joint
0.34
0.77
16.2
0.90
19.9
49
21.7
Heat
Learned
Marg
0.26
0.46
4.9
0.98
20.9
4.1
21.9
Heat
Learned
Joint
0.29
0.58
9.8
0.88
15.2
3.0
16.4
Heat
Algo
Marg
0.29
0.54
3.8
0.83
14.8
7.2
15.9
Heat
Algo
Joint
0.29
0.54
3.8
0.83
14.8
2.6
15.6
Heat
Combi
Joint
0.31
0.60
8.2
0.76
11.8
2.4
12.7"
RECOMBINATION MODULE,0.41139240506329117,"Usually, scalar marginal models already suffer from learning difÔ¨Åculties as only one output modality,
the closest one to ground, can be trained at a time. Some modalities may therefore converge faster
to acceptable solutions, and beneÔ¨Åt from a much increased number of training samples compared
to the others. This problem is aggravated in the joint training case, since the modality selected is
the same for all agents in a training sample. The joint scalar model therefore actually fails to learn
multi-modality as illustrated by a higher marginal minFDE6 than any other model, and an extremely
high crossCollisionRate since some modalities never train and always point to the same coordinates
regardless of the queried agent. Note that, despite a similar training loss, SceneTransformer doesn‚Äôt
suffer of the same pitfalls in Tab. 1 as is shares the same weights between all modalities and only
differentiates them in the initialization of the features."
RECOMBINATION MODULE,0.41455696202531644,Published as a conference paper at ICLR 2022
HIERARCHICAL HEATMAP REFINEMENT,0.4177215189873418,"4.5
HIERARCHICAL HEATMAP REFINEMENT"
HIERARCHICAL HEATMAP REFINEMENT,0.4208860759493671,"We assess the speed gain of our proposed hierarchical decoder compared to the lane rasters of GO-
HOME Gilles et al. (2021a). In Tab. 3. We report training time for 16 epochs with a batchsize of
16, and inference time for 32 and 128 simultaneous agents."
HIERARCHICAL HEATMAP REFINEMENT,0.4240506329113924,Table 3: Comparison of consistent solutions on Interpret multi-agent validation track
HIERARCHICAL HEATMAP REFINEMENT,0.4272151898734177,"Training
Inference 32 agents
Inference 128 agents
GOHOME
12.8 hours
36 ms
90 ms
THOMAS
7.5 hours
20 ms
31 ms"
HIERARCHICAL HEATMAP REFINEMENT,0.43037974683544306,"For additional comparison, the other existing dense prediction method DenseTNT (Gu et al., 2021)
reports an inference speed of 100ms per sample for their model."
QUALITATIVE EXAMPLES,0.43354430379746833,"4.6
QUALITATIVE EXAMPLES"
QUALITATIVE EXAMPLES,0.43670886075949367,"In this section, we will mostly compare the model before recombination, which we will refer by the
Before model, to the model after recombination, referenced as the After model. We display four
qualitative examples in Fig. 5 with colliding modalities in the Before model (in dashed orange) and
the solved modality (in full line orange) after recombination. For each model (Before-dashed or
After-full), the highlighted modality in orange is the best modality according to the SMR6 metric
among the 6 available modalities. We also display in dashed grey the other 5 predicted Before
modalities, and highlight that the recombination model indeed selects modalities already available
in the vanilla set and reorders them so that non-colliding modalities are aligned together."
QUALITATIVE EXAMPLES,0.439873417721519,"Figure 5: Qualitative examples of recombination model assembling collision-free modalities to-
gether compared to initial colliding modalities. For each example we display the general context
with highlighted agents and area of interest, then two zooms in on the agents, one displaying the
initial best modality before recombination in dashed orange and all the other available modalities in
grey. The second zooms shows the best modality after recombination in full line orange."
QUALITATIVE EXAMPLES,0.4430379746835443,"We also show more qualitative examples in Fig. 6, where we highlight the comparison in modality
diversity between the Before model (in dashed lines) and the After model (in full lines). While
the Before model tries to spread the modalities for all agents to minimize marginal miss-rate,
the recombined model presents much less spread compared to the original model, maintaining a
multimodal behavior only in presence of very different possible agent intentions such as different
possible exits or turn choices. For most other agents, almost all modalities are located at the same
position, that is, the one deemed the most likely by the model. Thus, if the truly uncertain agents"
QUALITATIVE EXAMPLES,0.4462025316455696,Published as a conference paper at ICLR 2022
QUALITATIVE EXAMPLES,0.44936708860759494,"Figure 6: Qualitative examples of recombination model selecting fewer but more pronounced and
impactful agent modalities compared to initial colliding modalities. For each example we display
on the left the vanilla model modalities with dashed lines, with the initial best modality in dashed
orange and all the other available modalities in grey. On the right we display the selected modalities
after recombination, where the model focuses on the most likely occurrence in most agents."
QUALITATIVE EXAMPLES,0.4525316455696203,"have to select the second or third most likely modality, the other agents still have their own most
likely modality"
CONCLUSION,0.45569620253164556,"5
CONCLUSION"
CONCLUSION,0.4588607594936709,"We have presented THOMAS, a recombination module that can be added after any trajectory pre-
diction module outputing multi-modal predictions. By design, THOMAS allows to generate scene-
consistent modalities across all agents by making the scene modalities select coherent agent modal-
ities and restricting the modality budget on the agents that truly need it. We show signiÔ¨Åcant per-
formance increase when adding the THOMAS module compared to the vanilla model and achieve
state-of-the-art results compared to already existing methods tackling scene-consistent predictions. ."
CONCLUSION,0.4620253164556962,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.4651898734177215,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.46835443037974683,"We use the publicly available Interaction 1.2 dataset (Zhan et al., 2019) available at http://
challenge.interaction-dataset.com/dataset/download. We detail dataset pre-
processing in Appendix A.2, training process in Appendix A.3 and give a precise illustration of our
model architecture with each layer size in A.8."
REFERENCES,0.47151898734177217,REFERENCES
REFERENCES,0.47468354430379744,"Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio
Savarese. Social lstm: Human trajectory prediction in crowded spaces. In CVPR, 2016."
REFERENCES,0.4778481012658228,"Florent Altch¬¥e and Arnaud de La Fortelle. An lstm network for highway trajectory prediction. In
ITSC, 2017."
REFERENCES,0.4810126582278481,"Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Kr-
ishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes prediction competition. https:
//eval.ai/web/challenges/challenge-page/591/leaderboard/1659.
Ac-
cessed: 2021-11-20."
REFERENCES,0.48417721518987344,"Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush
Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for
autonomous driving. In CVPR, 2020."
REFERENCES,0.4873417721518987,"Sergio Casas, Cole Gulino, Simon Suo, Katie Luo, Renjie Liao, and Raquel Urtasun. Implicit latent
variable model for scene-consistent motion forecasting. In ECCV, 2020."
REFERENCES,0.49050632911392406,"Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple prob-
abilistic anchor trajectory hypotheses for behavior prediction. In CoRL, 2020."
REFERENCES,0.4936708860759494,"Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hart-
nett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, et al.
Argoverse motion fore-
casting competition. https://eval.ai/web/challenges/challenge-page/454/
leaderboard/1279. Accessed: 2021-11-20."
REFERENCES,0.49683544303797467,"Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hart-
nett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d tracking and
forecasting with rich maps. In CVPR, 2019."
REFERENCES,0.5,"Alexander Cui, Sergio Casas, Abbas Sadat, Renjie Liao, and Raquel Urtasun. Lookout: Diverse
multi-future prediction and planning for self-driving. In ICCV, 2021."
REFERENCES,0.5031645569620253,"Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, Tsung-Han Lin, Thi Nguyen, Tzu-Kuo
Huang, Jeff Schneider, and Nemanja Djuric. Multimodal trajectory predictions for autonomous
driving using deep convolutional networks. In ICRA, 2019."
REFERENCES,0.5063291139240507,"Nachiket Deo and Mohan M Trivedi. Convolutional social pooling for vehicle trajectory prediction.
In CVPR, 2018."
REFERENCES,0.509493670886076,"Nachiket Deo and Mohan M Trivedi. Trajectory forecasts in unknown environments conditioned on
grid-based plans. arXiv:2001.00735, 2020."
REFERENCES,0.5126582278481012,"Nachiket Deo, Eric M Wolff, and Oscar Beijbom. Multimodal trajectory prediction conditioned on
lane-graph traversals. In CoRL, 2021."
REFERENCES,0.5158227848101266,"Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning
Chai, Ben Sapp, Charles Qi, Yin Zhou, et al.
Large scale interactive motion forecasting for
autonomous driving: The waymo open motion dataset. arXiv:2104.10133, 2021."
REFERENCES,0.5189873417721519,"Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, and Cordelia
Schmid. Vectornet: Encoding hd maps and agent dynamics from vectorized representation. In
CVPR, 2020."
REFERENCES,0.5221518987341772,Published as a conference paper at ICLR 2022
REFERENCES,0.5253164556962026,"Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, and Fabien Moutarde.
Gohome:
Graph-oriented heatmap output forfuture motion estimation.
arXiv preprint
arXiv:2108.09640, 2021a."
REFERENCES,0.5284810126582279,"Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, and Fabien Moutarde.
Home: Heatmap output for future motion estimation. In ITSC, 2021b."
REFERENCES,0.5316455696202531,"Roger
Girgis,
Florian
Golemo,
Felipe
Codevilla,
Jim
Aldon
D‚ÄôSouza,
Martin
Weiss,
Samira Ebrahimi Kahou, Felix Heide, and Christopher Pal. Autobots: Latent variable sequen-
tial set transformers. arXiv:2104.00563, 2021."
REFERENCES,0.5348101265822784,"Junru Gu, Chen Sun, and Hang Zhao. Densetnt: End-to-end trajectory prediction from dense goal
sets. In ICCV, 2021."
REFERENCES,0.5379746835443038,"Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially
acceptable trajectories with generative adversarial networks. In CVPR, 2018."
REFERENCES,0.5411392405063291,"Joey Hong, Benjamin Sapp, and James Philbin. Rules of the road: Predicting driving behavior with
a convolutional model of semantic interactions. In CVPR, 2019."
REFERENCES,0.5443037974683544,"Boris Ivanovic, Amine Elhafsi, Guy Rosman, Adrien Gaidon, and Marco Pavone. Mats: An inter-
pretable trajectory forecasting representation for planning and control. In CoRL, 2020."
REFERENCES,0.5474683544303798,"Siddhesh Khandelwal, William Qi, Jagjeet Singh, Andrew Hartnett, and Deva Ramanan. What-if
motion prediction for autonomous driving. arXiv:2008.10587, 2020."
REFERENCES,0.5506329113924051,"ByeoungDo Kim, Seong Hyeon Park, Seokhwan Lee, Elbek Khoshimjonov, Dongsuk Kum, Junsoo
Kim, Jeong Soo Kim, and Jun Won Choi. Lapred: Lane-aware prediction of multi-modal future
trajectories of dynamic agents. In CVPR, 2021."
REFERENCES,0.5537974683544303,"Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B Choy, Philip HS Torr, and Manmohan
Chandraker. Desire: Distant future prediction in dynamic scenes with interacting agents. In
CVPR, 2017."
REFERENCES,0.5569620253164557,"Lingyun Luke Li, Bin Yang, Ming Liang, Wenyuan Zeng, Mengye Ren, Sean Segal, and Raquel
Urtasun. End-to-end contextual perception and prediction with interaction transformer. In IROS,
2020."
REFERENCES,0.560126582278481,"Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, and Raquel Urtasun. Learning
lane graph representations for motion forecasting. In ECCV, 2020."
REFERENCES,0.5632911392405063,"Chenxu Luo, Lin Sun, Dariush Dabiri, and Alan Yuille. Probabilistic multi-modal trajectory predic-
tion with lane attention for autonomous vehicles. arXiv:2007.02574, 2020."
REFERENCES,0.5664556962025317,"Jean Mercat, Thomas Gilles, Nicole El Zoghby, Guillaume Sandou, Dominique Beauvois, and
Guillermo Pita Gil. Multi-head attention for multi-modal joint vehicle motion forecasting. In
ICRA, 2020."
REFERENCES,0.569620253164557,"Kaouther Messaoud, Nachiket Deo, Mohan M Trivedi, and Fawzi Nashashibi.
Multi-head at-
tention with joint agent-map representation for trajectory prediction in autonomous driving.
arXiv:2005.02545, 2020."
REFERENCES,0.5727848101265823,"Xiaoyu Mo, Yang Xing, and Chen Lv. Recog: A deep learning framework with heterogeneous graph
for interaction-aware trajectory prediction. arXiv:2012.05032, 2020."
REFERENCES,0.5759493670886076,"Sriram Narayanan, Ramin Moslemi, Francesco Pittaluga, Buyu Liu, and Manmohan Chandraker.
Divide-and-conquer for lane-aware diverse trajectory prediction. In CVPR, 2021."
REFERENCES,0.5791139240506329,"Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zhengdong Zhang, Hao-Tien Lewis Chiang,
Jeffrey Ling, Rebecca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, et al.
Scene
transformer: A uniÔ¨Åed multi-task model for behavior prediction and planning. arXiv preprint
arXiv:2106.08417, 2021."
REFERENCES,0.5822784810126582,"Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton, Oscar Beijbom, and Eric M Wolff.
Covernet: Multimodal behavior prediction using trajectory sets. In CVPR, 2020."
REFERENCES,0.5854430379746836,Published as a conference paper at ICLR 2022
REFERENCES,0.5886075949367089,"Nicholas Rhinehart, Kris M Kitani, and Paul Vernaza. R2p2: A reparameterized pushforward policy
for diverse, precise generative path forecasting. In ECCV, 2018."
REFERENCES,0.5917721518987342,"Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. Precog: Prediction condi-
tioned on goals in visual multi-agent settings. In CVPR, 2019."
REFERENCES,0.5949367088607594,"Rapha¬®el Rozenberg, Joseph Gesnouin, and Fabien Moutarde. Asymmetrical bi-rnn for pedestrian
trajectory encoding. arXiv:2106.04419, 2021."
REFERENCES,0.5981012658227848,"Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid RezatoÔ¨Åghi, and Silvio
Savarese. Sophie: An attentive gan for predicting paths compliant to social and physical con-
straints. In CVPR, 2019."
REFERENCES,0.6012658227848101,"Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone.
Trajectron++:
Dynamically-feasible trajectory forecasting with heterogeneous data. In ECCV, 2020."
REFERENCES,0.6044303797468354,"Haoran Song, Di Luan, Wenchao Ding, Michael Yu Wang, and Qifeng Chen. Learning to predict
vehicle trajectories with model-based planning. In CoRL, 2021."
REFERENCES,0.6075949367088608,"Yichuan Charlie Tang and Ruslan Salakhutdinov. Multiple futures prediction. In NeurIPS, 2019."
REFERENCES,0.6107594936708861,"Chuhua Wang, Yuchen Wang, Mingze Xu, and David J Crandall. Stepwise goal-driven networks for
trajectory prediction. arXiv:2103.14107, 2021."
REFERENCES,0.6139240506329114,"David Wu and Yunnan Wu. Air2 for interaction prediction. Waymo Open Dataset Challenges Re-
ports, CVPR Workshop on Autonomous Driving, http://cvpr2021.wad.vision/, 2021."
REFERENCES,0.6170886075949367,"Maosheng Ye, Tongyi Cao, and Qifeng Chen. Tpcn: Temporal point cloud networks for motion
forecasting. In CVPR, 2021."
REFERENCES,0.620253164556962,"Ye Yuan, Xinshuo Weng, Yanglan Ou, and Kris Kitani. Agentformer: Agent-aware transformers for
socio-temporal multi-agent forecasting. arXiv:2103.14023, 2021."
REFERENCES,0.6234177215189873,"Wenyuan Zeng, Ming Liang, Renjie Liao, and Raquel Urtasun. Lanercnn: Distributed representa-
tions for graph-centric motion forecasting. In IROS, 2021."
REFERENCES,0.6265822784810127,"Wei Zhan, Liting Sun, Di Wang, Haojie Shi, Aubrey Clausse, Maximilian Naumann, Julius Kum-
merle, Hendrik Konigshof, Christoph Stiller, Arnaud de La Fortelle, et al. Interaction dataset:
An international, adversarial and cooperative motion dataset in interactive driving scenarios with
semantic maps. arXiv:1910.03088, 2019."
REFERENCES,0.629746835443038,"Wei Zhan, Liting Sun, Di Wang, Haojie Shi, Aubrey Clausse, Maximilian Naumann, Julius Kum-
merle, Hendrik Konigshof, Christoph Stiller, Arnaud de La Fortelle, et al. Interpret: Interaction-
dataset-based prediction challenge.
http://challenge.interaction-dataset.
com/prediction-challenge/intro, 2021. Accessed: 2021-10-03."
REFERENCES,0.6329113924050633,"Lingyao Zhang, Po-Hsun Su, Jerrick Hoang, Galen Clark Haynes, and Micol Marchetti-Bowick.
Map-adaptive goal-based trajectory prediction. In CoRL, 2020."
REFERENCES,0.6360759493670886,"Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Benjamin Sapp, Balakrishnan Varadarajan, Yue Shen,
Yi Shen, Yuning Chai, Cordelia Schmid, et al. Tnt: Target-driven trajectory prediction. CoRL,
2020."
REFERENCES,0.6392405063291139,Published as a conference paper at ICLR 2022
REFERENCES,0.6424050632911392,"A
APPENDIX"
REFERENCES,0.6455696202531646,"A.1
METRIC DETAILS"
REFERENCES,0.6487341772151899,"We adopt for each dataset evaluation the miss deÔ¨Ånition as speciÔ¨Åed by the dataset benchmark.
Argoverse (Chang et al., 2019) and NuScenes (Caesar et al., 2020) deÔ¨Åne a miss as the prediction
being further than 2 meters to the ground truth, while Waymo (Ettinger et al., 2021) and Interaction
Zhan et al. (2019)count a miss when the prediction is closer than a lateral (1m) and an longitudinal
threshold with regard to speed:"
REFERENCES,0.6518987341772152,"Threshold lon = Ô£±
Ô£≤ Ô£≥"
REFERENCES,0.6550632911392406,"1
v < 1.4m/s
1 + v‚àí1.4"
REFERENCES,0.6582278481012658,"11‚àí1.4
1.4m/s ‚â§v ‚â§11m/s
2
v ‚â•11m/s"
REFERENCES,0.6613924050632911,"A.2
DATASET PROCESSING"
REFERENCES,0.6645569620253164,"We use the training/validation split provided in Interaction 1.2. We select for each scene a reference
agent on which the scene will be centered and oriented according to its heading at prediction time.
At training time, this reference agent is chosen randomly across all agents. At validation and testing
time, we compute the barycenter of all agents and select the closest agent as the scene reference
point. We use for training all agents that have ground-truth available at prediction horizon T=3s."
REFERENCES,0.6677215189873418,"We use the provided dataset HD-Maps to create the context graphs. As in Liang et al. (2020), we
use four relations:{predecessor, successor, left, right} obtained from the lane graph connectivity.
We upsample long lanelets to have a maximum of 10 points in each lanelet."
REFERENCES,0.6708860759493671,"For each agent, we use their trajectory history made of position, yaw and speed in the past second
sampled at 10Hz. We also provide the model a mask indicating if the agent was present in a given
time-frame, and pad with zeros when the agent is not tracked at a speciÔ¨Åc timestep."
REFERENCES,0.6740506329113924,"A.3
TRAINING DETAILS"
REFERENCES,0.6772151898734177,"We train all models with Adam optimizer and batchsize 32. We initialize the learning rate at 1e‚àí3
and divide it by 2 at epochs 3, 6, 9 and 13, before stopping the training at epoch 16. We use ReLU
activation after every linear layer unless speciÔ¨Åed otherwise, and LayerNormalization after every
attention and graph convolution layer."
REFERENCES,0.680379746835443,"During training, since a scene can contain up to 40 agents, which is too much for gradient compu-
tation in batches of 32, we restrict the number of predicted agents to 8 by randomly sampling them
across available agents. We do not use other data augmentation."
REFERENCES,0.6835443037974683,"The Ô¨Ånal heatmap Y predicted cover a range of 192 meters at resolution 0.5m, hence a (384, 384)
image. For heatmap generation, we use the same pixel-wise focal loss over the pixels p as in Gilles
et al. (2021b):"
REFERENCES,0.6867088607594937,L = ‚àí1 P X
REFERENCES,0.689873417721519,"p
(Yp ‚àíÀÜYp)2f(Yp, ÀÜYp) with f(Yp, ÀÜYp) ="
REFERENCES,0.6930379746835443,"(
log( ÀÜYp)
if Yp=1
(1 ‚àíYp)4 log(1 ‚àíÀÜYp)
else"
REFERENCES,0.6962025316455697,"where ÀÜY is deÔ¨Åned as a Gaussian centered on the agent future position at prediction horizon T with
a standard deviation of 4 pixels, equivalent to 2m."
REFERENCES,0.6993670886075949,"A.4
BASELINES IMPLEMENTATIONS"
REFERENCES,0.7025316455696202,"A.4.1
IMPLICIT LATENT VARIABLE MODEL"
REFERENCES,0.7056962025316456,"We use a GOHOME encoder for the prior, posterior and decoder Scene Interaction Modules. We
weight the KL term with Œ≤ = 1 which worked best according to our experiments."
REFERENCES,0.7088607594936709,Published as a conference paper at ICLR 2022
REFERENCES,0.7120253164556962,"A.4.2
SCENE TRANSFORMER"
REFERENCES,0.7151898734177216,"The initial paper applies a transformer architecture on a [F, A, T, D] tensor where F is the potential
modality dimension, A the agent dimension and T the time dimension, with D the feature embed-
ding, with factorized self-attention to the agent and time dimensions separately, so that agents can
look at each-other inside a speciÔ¨Åc scene modality. The resulting output is optimized using a jointly
formalized loss. For our implementation, we get rid of the T dimension as we focus on endpoint
prediction and coherence between the A agents. The initial encoded [A, D] tensor is obtained with
a GOHOME encoder, multiplied across the F futures and concatenated with a modality-speciÔ¨Åc
one-hot encoding as in Ngiam et al. (2021) to obtain the [F, A, D] tensor. We then apply two layers
of agent self-attention similar to the original paper, before decoding the endpoints through a MLP."
REFERENCES,0.7183544303797469,"A.4.3
COLLISION-FREE ENDPOINT SAMPLING BASELINE"
REFERENCES,0.7215189873417721,"We designed a deterministic sampling algorithm based on the heatmaps generated in previous section
in order to sample endpoints for each agent in a collision aware manner. We use the same sampling
algorithm as Gilles et al. (2021a) based on MR optimization, but add a sequential iteration over the
agents for each modalities."
REFERENCES,0.7246835443037974,"For a single modality k, we predict the possible endpoint of a Ô¨Årst agent a by taking the maxi-
mum accumulated predicted probability under an area of radius r. We then not only set to zero the
heatmap values of this agent heatmap Ia
k‚Ä≤ around the sampled location so not to sample it in the next
modalities k‚Ä≤, but we also set to zero the same area on the heatmaps Ia‚Ä≤
k of the other agents a‚Ä≤ on the
same modality k, so that these other agents cannot be sampled at the same position for this modality.
This way, we try to enforce collision-free endpoints, and expect that considering collisions brings
logic to improve the overall consistency of the predictions. However, as will be highlighted in Sec.
4.4, this methods signiÔ¨Åcantly improves the collision rate without the need for any additional learned
model but it does barely improve the multi agent consistency."
REFERENCES,0.7278481012658228,"A.5
RESULTS ON INTERPRET CHALLENGE"
REFERENCES,0.7310126582278481,"We also report our numbers from the Interpret multi-agent track challenge online leaderboard in
Table 5. It is to be noted that the DenseTNT solution explicitely checks for collisions in the search
for its proposed predictions, which we don‚Äôt, hence their 0% collision rate (SCR) and its direct
impact on consistent collision-free joint Miss Rate (cSMR)."
REFERENCES,0.7341772151898734,Table 4: Results on Interpret multi-agent regular scene leaderboard (test set)
REFERENCES,0.7373417721518988,"minSADE
minSFDE
SMR
SCR
cSMR
MoliNet
0.73
2.55
44.4
7.5
47.4
ReCoG2 (Mo et al., 2020)
0.47
1.16
23.8
6.9
26.8
DenseTNT (Gu et al., 2021)
0.42
1.13
22.4
0.0
22.4
THOMAS
0.42
0.97
17.9
12.8
25.2"
REFERENCES,0.740506329113924,Table 5: Results on Interpret multi-agent conditional scene leaderboard (test set)
REFERENCES,0.7436708860759493,"minSADE
minSFDE
SMR
SCR
cSMR
ReCoG2 (Mo et al., 2020)
0.33
0.87
14.98
0.09
15.12
DenseTNT (Gu et al., 2021)
0.28
0.89
15.02
0.0
15.02
THOMAS
0.31
0.72
10.67
0.84
11.63"
REFERENCES,0.7468354430379747,"A.6
SPEED / PERFORMANCE TRADE-OFF WITH HIERARCHICAL REFINEMENT"
REFERENCES,0.75,"We also display the trade-off between inference speed and coverage from hierarchical reÔ¨Ånement
in Fig. 7, evaluated on the Interpret multi-agent dataset with marginal MissRate6. The curve is
obtained setting the number N of upsampled points at the last reÔ¨Ånement iteration from 2 to 128."
REFERENCES,0.7531645569620253,Published as a conference paper at ICLR 2022
REFERENCES,0.7563291139240507,"From N = 16 and lower, coverage performance starts to diminish while little speed gains are made.
We still kept a relatively high N=64 in our model as we wanted to insure a wide coverage, and the
time loss between 41 ms and 46 ms remains acceptable."
REFERENCES,0.759493670886076,"Figure 7: Curve of MissRate6 with regard to inference time with varying number of points upsam-
pled at the last hierarchical reÔ¨Ånement iteration."
REFERENCES,0.7626582278481012,"A.7
ADDITIONAL RESULTS ON SINGLE-AGENT DATASETS"
REFERENCES,0.7658227848101266,Table 6: Argoverse Leaderboard (Chang et al.)
REFERENCES,0.7689873417721519,"K=1
K=6
minFDE
MR
minADE
minFDE
MR
LaneGCN (Liang et al., 2020)
3.78
59.1
0.87
1.36
16.3
Autobot (Girgis et al., 2021)
0.89
1.41
16
TPCN (Ye et al., 2021)
3.64
58.6
0.85
1.35
15.9
Jean (Mercat et al., 2020)
4.24
68.6
1.00
1.42
13.1
SceneTrans (Ngiam et al., 2021)
4.06
59.2
0.80
1.23
12.6
LaneRCNN (Zeng et al., 2021)
3.69
56.9
0.90
1.45
12.3
PRIME (Song et al., 2021)
3.82
58.7
1.22
1.56
11.5
DenseTNT (Gu et al., 2021)
3.70
59.9
0.94
1.49
10.5
GOHOME (Gilles et al., 2021a)
3.65
57.2
0.94
1.45
10.5
HOME (Gilles et al., 2021b)
3.65
57.1
0.93
1.44
9.8
THOMAS
3.59
56.1
0.94
1.44
10.4"
REFERENCES,0.7721518987341772,Published as a conference paper at ICLR 2022
REFERENCES,0.7753164556962026,Table 7: NuScenes Leaderboard (Caesar et al.)
REFERENCES,0.7784810126582279,"K=5
K=10
k=1
minADE
MR
minADE
MR
minFDE
CoverNet (Phan-Minh et al., 2020)
1.96
67
1.48
Trajectron++ (Salzmann et al., 2020)
1.88
70
1.51
57
9.52
ALAN (Narayanan et al., 2021)
1.87
60
1.22
49
9.98
SG-Net (Wang et al., 2021)
1.86
67
1.40
52
9.25
WIMP (Khandelwal et al., 2020)
1.84
55
1.11
43
8.49
MHA-JAM (Messaoud et al., 2020)
1.81
59
1.24
46
8.57
CXX (Luo et al., 2020)
1.63
69
1.29
60
8.86
LaPred (Kim et al., 2021)
1.53
1.12
8.12
P2T (Deo & Trivedi, 2020)
1.45
64
1.16
46
10.50
GOHOME (Gilles et al., 2021a)
1.42
57
1.15
47
6.99
Autobot (Girgis et al., 2021)
1.37
62
1.03
44
8.19
PGP (Deo & Trivedi, 2020)
1.30
57
0.98
37
7.72
THOMAS
1.33
55
1.04
1.04
6.71"
REFERENCES,0.7816455696202531,Published as a conference paper at ICLR 2022
REFERENCES,0.7848101265822784,"A.8
DETAILED ARCHITECTURE"
REFERENCES,0.7879746835443038,TrajEncoder
REFERENCES,0.7911392405063291,"1D Conv, 64"
REFERENCES,0.7943037974683544,"UGRU, 64"
REFERENCES,0.7974683544303798,Agent input A x 4
REFERENCES,0.8006329113924051,Agent features A x 64
REFERENCES,0.8037974683544303,Lanelet input L x 3
REFERENCES,0.8069620253164557,LaneEncoder
REFERENCES,0.810126582278481,"1D Conv, 64"
REFERENCES,0.8132911392405063,"UGRU, 64"
REFERENCES,0.8164556962025317,Lanelet features L x 64
REFERENCES,0.819620253164557,GraphEncoder1
REFERENCES,0.8227848101265823,"GraphConv, 64"
REFERENCES,0.8259493670886076,"GraphConv, 64"
REFERENCES,0.8291139240506329,"GraphConv, 64"
REFERENCES,0.8322784810126582,"GraphConv, 64"
REFERENCES,0.8354430379746836,GraphEncoder2
REFERENCES,0.8386075949367089,"GraphConv, 32"
REFERENCES,0.8417721518987342,"GraphConv, 32"
REFERENCES,0.8449367088607594,"GraphConv, 32"
REFERENCES,0.8481012658227848,"GraphConv, 32"
REFERENCES,0.8512658227848101,Lanes2Agents
REFERENCES,0.8544303797468354,"Attention, 64"
REFERENCES,0.8575949367088608,"Linear, 64"
REFERENCES,0.8607594936708861,Agents2Agents
REFERENCES,0.8639240506329114,"Attention, 64"
REFERENCES,0.8670886075949367,"Linear, 64"
REFERENCES,0.870253164556962,Map-aware Agent features A x 64
REFERENCES,0.8734177215189873,Ego2Lanes
REFERENCES,0.8765822784810127,"Concatenate, 128"
REFERENCES,0.879746835443038,"Linear, 64"
REFERENCES,0.8829113924050633,Lane scoring
REFERENCES,0.8860759493670886,"Linear, 1"
REFERENCES,0.8892405063291139,Sigmoid
REFERENCES,0.8924050632911392,Lanelet scores A x L x 1
REFERENCES,0.8955696202531646,Agent graphs A x L x 64
REFERENCES,0.8987341772151899,Hierarchical decoder
REFERENCES,0.9018987341772152,"Probability heatmap (H, W, 1)"
REFERENCES,0.9050632911392406,Low res grid G0 x 2
REFERENCES,0.9082278481012658,Interaction-aware Agent features A x 64
REFERENCES,0.9113924050632911,Step Grid decoder
REFERENCES,0.9145569620253164,"Probability scores (G, 1)"
REFERENCES,0.9177215189873418,Low res grid G x 2
REFERENCES,0.9208860759493671,"Linear, 32"
REFERENCES,0.9240506329113924,"Concatenate, 96"
REFERENCES,0.9272151898734177,"Linear, 32"
REFERENCES,0.930379746835443,"Attention, 32"
REFERENCES,0.9335443037974683,"Linear, 32"
REFERENCES,0.9367088607594937,"Attention, 32"
REFERENCES,0.939873417721519,"Linear, 1, Sigmoid"
REFERENCES,0.9430379746835443,Step Grid decoder 1
REFERENCES,0.9462025316455697,Top N1
REFERENCES,0.9493670886075949,Upsample
REFERENCES,0.9525316455696202,Mid res grid G1 x 2
REFERENCES,0.9556962025316456,Step Grid decoder 2
REFERENCES,0.9588607594936709,Top N2
REFERENCES,0.9620253164556962,Upsample
REFERENCES,0.9651898734177216,High res grid G2 x 2
REFERENCES,0.9683544303797469,Step Grid decoder 3
REFERENCES,0.9715189873417721,"Final res grid (G2 , 1)"
REFERENCES,0.9746835443037974,Sparse2Dense
REFERENCES,0.9778481012658228,Agent features
REFERENCES,0.9810126582278481,Graph features
REFERENCES,0.9841772151898734,Top K lanelets A x K x 32
REFERENCES,0.9873417721518988,Figure 8: Detailed illustration of our heatmap generator model
REFERENCES,0.990506329113924,Published as a conference paper at ICLR 2022
REFERENCES,0.9936708860759493,"A.9
HEATMAP QUALITATIVE RESULTS"
REFERENCES,0.9968354430379747,"Figure 9: Qualitative examples of heatmap output from our multi-agent model. All the heatmaps
from one scene are the results from one single forward pass in our model predicting all agents
at once. We use matching colors for the agent history, current location and their future predicted
heatmap (best viewed in color)."
