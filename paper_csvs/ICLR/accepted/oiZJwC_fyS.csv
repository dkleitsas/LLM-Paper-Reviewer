Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024096385542168677,"In this work we theoretically contribute to neural network approximation by pro-
viding a novel tropical geometrical viewpoint to structured neural network com-
pression. In particular, we show that the approximation error between two neural
networks with ReLU activations and one hidden layer depends on the Hausdorff
distance of the tropical zonotopes of the networks. This theorem comes as a ﬁrst
step towards a purely geometrical interpretation of neural network approxima-
tion. Based on this theoretical contribution, we propose geometrical methods that
employ the K-means algorithm to compress the fully connected parts of ReLU
activated deep neural networks. We analyze the error bounds of our algorithms
theoretically based on our approximation theorem and evaluate them empirically on
neural network compression. Our experiments follow a proof-of-concept strategy
and indicate that our geometrical tools achieve improved performance over relevant
tropical geometry techniques and can be competitive against non-tropical methods."
INTRODUCTION,0.004819277108433735,"1
INTRODUCTION"
INTRODUCTION,0.007228915662650603,"Tropical geometry (Maclagan & Sturmfels, 2015) is a mathematical ﬁeld based on algebraic geometry
and strongly linked to polyhedral and combinatorial geometry. It is built upon the tropical semiring
which originally refers to the min-plus semiring (Rmin, ∧, +), but may also refer to the max-plus
semiring (Cuninghame-Green, 2012; Butkoviˇc, 2010). In our work, we follow the convention of the
max-plus semiring (Rmax, ∨, +) which replaces the classical operations of addition and multiplication
by max and sum respectively. These operations turn polynomials into piecewise linear functions
making them directly applicable in neural networks."
INTRODUCTION,0.00963855421686747,"Tropical mathematics cover a wide range of applications including dynamical systems on weighted
lattices (Maragos, 2017), ﬁnite state transducers (Theodosis & Maragos, 2018; 2019) and convex
regression (Maragos & Theodosis, 2020; Tsilivis et al., 2021). Recently, there has been remarkable
theoretical impact of tropical geometry in the study of neural networks and machine learning (Maragos
et al., 2021). Zhang et al. (2018) prove the equivalence of ReLU activated neural networks with
tropical rational mappings. Furthermore, they use zonotopes to compute a bound on the number
of the network’s linear regions, which has already been known in (Montúfar et al., 2014). In a
similar context, Charisopoulos & Maragos (2018) compute an upper bound to the number of linear
regions of convolutional and maxout layers and propose a randomized algorithm for linear region
counting. Other works employ tropical geometry to examine the training and further properties of
morphological perceptron (Charisopoulos & Maragos, 2017) and morphological neural networks
(Dimitriadis & Maragos, 2021)."
INTRODUCTION,0.012048192771084338,"Pruning or, generally, compressing neural networks gained interest in recent years due to the surprising
capability of reducing the size of a neural network without compromising performance (Blalock et al.,
2020). As tropical geometry explains the mathematical structure of neural networks, pruning may
also be viewed under the perspective of tropical geometry. Indeed, Alfarra et al. (2020) propose an
unstructured compression algorithm based on sparsifying the zonotope matrices of the network. Also,"
INTRODUCTION,0.014457831325301205,†Conducted research as a student in National Technical University of Athens.
INTRODUCTION,0.016867469879518072,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01927710843373494,"Smyrnis et al. (2020) construct a novel tropical division algorithm that applies to neural network
minimization. A generalization of this applies to multiclass networks (Smyrnis & Maragos, 2020)."
INTRODUCTION,0.021686746987951807,"Contributions
In our work, we contribute to structured neural network approximation from the
mathematical viewpoint of tropical geometry:"
INTRODUCTION,0.024096385542168676,"• We establish a novel bound on the approximation error between two neural networks with
ReLU activations and one hidden layer. To prove this we bound the difference of the
networks’ tropical polynomials via the Hausdorff distance of their respective zonotopes.
• We construct two geometrical neural network compression methods that are based on
zonotope reduction and employ K-means algorithm for clustering. Our algorithms apply on
the fully connected layers of ReLU activated neural networks.
• Our algorithms are analyzed both theoretically and experimentally. The theoretical eval-
uation is based on the theoretical bound of neural network approximation error. On the
experimental part, we examine the performance of our algorithms on retaining the accuracy
of convolutional neural networks when applying compression on their fully connected layers."
BACKGROUND ON TROPICAL GEOMETRY,0.02650602409638554,"2
BACKGROUND ON TROPICAL GEOMETRY"
BACKGROUND ON TROPICAL GEOMETRY,0.02891566265060241,"We study tropical geometry from the viewpoint of the max-plus semiring (Rmax, ∨, +) which is
deﬁned as the set Rmax = R ∪{−∞} equipped with two operations (∨, +). Operation ∨stands for
max and + stands for sum. In max-plus algebra we deﬁne polynomials in the following way."
BACKGROUND ON TROPICAL GEOMETRY,0.03132530120481928,"Tropical polynomials
A tropical polynomial f in d variables x = (x1, x2, ..., xd)T is deﬁned as
the function
f(x) = max
i∈[n]{aT
i x + bi}
(1)"
BACKGROUND ON TROPICAL GEOMETRY,0.033734939759036145,"where [n] = {1, ..., n}, ai are vectors in Rd and bi is the corresponding monomial coefﬁcient in
Rmax = R ∪{−∞}. The set of such polynomials constitutes the semiring Rmax[x] of tropical
polynomials. Note that each term aT
i x + bi corresponds to a hyperplane in Rd. We thus call the
vectors {ai}i∈[n] the slopes of the tropical polynomial, and {bi}i∈[n] the respective biases. We allow
slopes to be vectors with real coefﬁcients rather than integer ones, as it is normally the case for
polynomials in regular algebra. These polynomials are also referred to as signomials (Dufﬁn &
Peterson, 1973) in the literature."
BACKGROUND ON TROPICAL GEOMETRY,0.03614457831325301,"Polytopes
Polytopes have been studied extensively (Ziegler, 2012; Grünbaum, 2013) and occur as
a geometric tool for ﬁelds such as linear programming and optimization. They also have an important
role in the analysis of neural networks. For instance, Zhang et al. (2018); Charisopoulos & Maragos
(2018) show that linear regions of neural networks correspond to vertices of polytopes. Thus, the
counting of linear regions reduces to a combinatorial geometry problem. In what follows, we explore
this connection of tropical geometry with polytopes."
BACKGROUND ON TROPICAL GEOMETRY,0.03855421686746988,"Consider the tropical polynomial deﬁned in (1). The Newton polytope associated to f(x) is deﬁned
as the convex hull of the slopes of the polynomial"
BACKGROUND ON TROPICAL GEOMETRY,0.04096385542168675,Newt (f) := conv{ai : i ∈[n]}
BACKGROUND ON TROPICAL GEOMETRY,0.043373493975903614,"Furthermore, the extended Newton polytope of f(x) is deﬁned as the convex hull of the slopes of the
polynomial extended in the last dimension by the corresponding bias coefﬁcient."
BACKGROUND ON TROPICAL GEOMETRY,0.04578313253012048,"ENewt (f) := conv{(aT
i , bi) : i ∈[n]}"
BACKGROUND ON TROPICAL GEOMETRY,0.04819277108433735,"The following proposition computes the extended Newton polytope that occurs when a tropical
operation is applied between two tropical polynomials. It will allow us to compute the polytope
representation corresponding to a neural network’s hidden layer.
Proposition 1. (Zhang et al., 2018; Charisopoulos & Maragos, 2018) Let f, g ∈Rmax[x] be two
tropical polynomials . Then for the extended Newton polytopes it is true that"
BACKGROUND ON TROPICAL GEOMETRY,0.05060240963855422,"ENewt (f ∨g) = conv{ENewt (f) ∪ENewt (g)}
ENewt (f + g) = ENewt (f) ⊕ENewt (g)"
BACKGROUND ON TROPICAL GEOMETRY,0.05301204819277108,Published as a conference paper at ICLR 2022
BACKGROUND ON TROPICAL GEOMETRY,0.05542168674698795,"ENewt (f)
ENewt (g)
ENewt (f ∨g)
ENewt (f + g)"
BACKGROUND ON TROPICAL GEOMETRY,0.05783132530120482,UF (ENewt(f∨g))
BACKGROUND ON TROPICAL GEOMETRY,0.060240963855421686,"(0,0,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.06265060240963856,"(2,1,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.06506024096385542,"(1,0,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.06746987951807229,"(0,1,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.06987951807228916,"(0,0,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.07228915662650602,"(0,0,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.0746987951807229,"(2,1,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.07710843373493977,"(1,0,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.07951807228915662,"(0,1,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.0819277108433735,"(0,0,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.08433734939759036,"(1,0,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.08674698795180723,"(0,1,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.0891566265060241,"(0,0,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.09156626506024096,"(3,1,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.09397590361445783,"(2,2,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.0963855421686747,"(2,1,2)"
BACKGROUND ON TROPICAL GEOMETRY,0.09879518072289156,"Figure 1: Illustration of tropical operations between polynomials. The polytope of the max (∨) of f
and g corresponds to the convex hull of the union of points of the two polytopes and the polytope of
sum (+) corresponds to their Minkowski sum."
BACKGROUND ON TROPICAL GEOMETRY,0.10120481927710843,"Here ⊕denotes Minkowski addition. In particular, for two sets A, B ⊆Rd it is deﬁned as
A ⊕B := {a + b | a ∈A, b ∈B}
Corollary 1. This result can be extended to any ﬁnite set of polynomials using induction.
Example 1. Let f, g be two tropical polynomials in 2 variables, such that
f(x, y) = max(2x + y + 1, 0),
g(x, y) = max(x, y, 1)
The tropical operations applied to these polynomials give
f ∨g = max(2x + y + 1, 0, x, y, 1)
f + g = max(3x + y + 1, x, 2x + 2y + 1, y, 2x + y + 2, 1)
Fig. 1 illustrates the extended Newton polytopes of the original and the computed polynomials."
BACKGROUND ON TROPICAL GEOMETRY,0.10361445783132531,"The extended Newton polytope provides a geometrical representation of a tropical polynomial. In
addition, it may be used to compute the values that the polynomial attains, as Proposition 2 indicates.
Proposition 2. (Charisopoulos & Maragos, 2018) Let f ∈Rmax[x] be a tropical polynomial in d
variables. Let UF (ENewt (f)) be the points in the upper envelope of ENewt (f), where upward
direction is taken with respect to the last dimension of Rd+1. Then for each i ∈[n] there exists a
linear region of f on which f(x) = aT
i x + bi if and only if (aT
i , bi) is a vertex of UF (ENewt (f)).
Example 2. Using the polynomials from Example 1 we compute a reduced representation for f ∨g.
f ∨g = max(2x + y + 1, 0, x, y, 1) = max(2x + y + 1, x, y, 1)
Indeed, the signiﬁcant terms correspond to the vertices of UF(ENewt (f ∨g)) shown in Fig. 1."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.10602409638554217,"2.1
TROPICAL GEOMETRY OF NEURAL NETWORKS"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.10843373493975904,"Tropical geometry has the capability of expressing the mathematical structure of ReLU activated
neural networks. We review some of the basic properties of neural networks and introduce notation
that will be used in our analysis. For this purpose, we consider the ReLU activated neural network of
Fig. 2 with one hidden layer."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.1108433734939759,"Network tropical equations
The network of Fig. 2 consists of an input layer x = (x1, ..., xd), a
hidden layer f = (f1, ..., fn) with ReLU activations, an output layer v = (v1, ..., vm) and two linear
layers deﬁned by the matrices A, C respectively. As illustrated in Fig. 2 we have Ai,: =
 
aT
i , bi

for
the ﬁrst linear layer and Cj,: = (cj1, cj2, ..., cjn) for the second linear layer, as we ignore its biases.
Furthermore, the output of the i−th component of the hidden layer f is computed as"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.11325301204819277,"fi(x) = max d
X"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.11566265060240964,"k=1
aikxk + bi, 0 !"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.1180722891566265,"= max(aT
i x + bi, 0)
(2)"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.12048192771084337,"We deduce that each fi is a tropical polynomial with two terms. It therefore follows that ENewt (fi)
is a linear segment in Rd+1. The components of the output layer may be computed as"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.12289156626506025,"vj(x) = n
X"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.12530120481927712,"i=1
cjifi(x) =
X"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.12771084337349398,"cji>0
|cji|fi(x) −
X"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.13012048192771083,"cji<0
|cji|fi(x) = pj(x) −qj(x)
(3)"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.13253012048192772,Published as a conference paper at ICLR 2022 x1 x2 ... xk ... xd f1 f2 ... fi ... fn v1 v2 ... vj ... vm ai1 ai2 aik aid cj1 cj2 cji cjn b1 b2 bi bn
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.13493975903614458,"Figure 2: Neural network with one hid-
den ReLU layer. The ﬁrst linear layer has
weights {aT
i } with bias {bi} correspond-
ing to i−th node ∀i ∈[n] and the second
has weights {cji}, ∀j ∈[m], i ∈[n]."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.13734939759036144,"Tropical rational functions
In Eq. (3), functions pj, qj
are both linear combinations of {fi} with positive coef-
ﬁcients, which implies that they are tropical polynomials.
We conclude that every output node vi can be written as
a difference of two tropical polynomials, which is deﬁned
as a tropical rational function. This indicates that the
output layer of the neural network of Fig. 2 is equivalent
to a tropical rational mapping. In fact, this result holds
for deeper networks, in general, as demonstrated by the
following theorem."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.13975903614457832,"Theorem 1. (Zhang et al., 2018) A ReLU activated deep
neural network F : Rd →Rm is equivalent to a tropical
rational mapping."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.14216867469879518,"It is not known whether a tropical rational function r(x)
admits an efﬁcient geometric representation that deter-
mines its values {r(x)} for x ∈Rd, as it holds for tropi-
cal polynomials with their polytopes in Proposition 2. For
this reason, we choose to work separately on the polytopes
of the tropical polynomials pj, qj."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.14457831325301204,"Zonotopes
Zonotopes are deﬁned as the Minkowski
sum of a ﬁnite set of line segments. They are a special
case of polytopes that occur as a building block for our
network. These geometrical structures provide a representation of the polynomials pj, qj in (3)
that further allows us to build our compression algorithms. We use the notation Pj, Qj for the
extended Newton polytopes of tropical polynomials pj, qj, respectively. Notice from (3) that for each
component vj of the output pj, qj are written as linear combinations of tropical polynomials that
correspond to linear segments. Thus Pj and Qj are zonotopes. We call Pj the positive zonotope,
corresponding to the positive polynomial pj and Qj the negative one."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.14698795180722893,"Zonotope Generators
Each neuron of the hidden layer represents geometrically a line segment
contributing to the positive or negative zonotope. We thus call these line segments generators of the
zonotope. The generators further receive the characterization positive or negative depending on the
zonotope they contribute to. It is intuitive to expect that a zonotope gets more complex as its number
of generators increases. In fact, each vertex of the zonotope can be computed as the sum of vertices of
the generators, where we choose a vertex from each generating line segment, either 0 or cji
 
aT
i , bi

.
We summarize the above with the following extension of (Charisopoulos & Maragos, 2018)."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.1493975903614458,"Proposition 3. Pj, Qj are zonotopes in Rd+1. For each vertex v of Pj there exists a subset of indices
I+ of {1, 2, ..., n} with cji > 0, ∀i ∈I+ such that v = P"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.15180722891566265,"i∈I+ cji
 
aT
i , bi

. Similarly, a vertex u of
Qj can be written as u = P"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.15421686746987953,"i∈I−cji
 
aT
i , bi

where I−corresponds to cji < 0, ∀i ∈I−."
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1566265060240964,"3
APPROXIMATION OF TROPICAL POLYNOMIALS"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.15903614457831325,"In this section we present our central theorem that bounds the error between the original and
approximate neural network, when both have the architecture of Fig. 2. To achieve this we need
to derive a bound for the error of approximating a simpler functional structure, namely the tropical
polynomials that represent the neural network. The motivation behind the geometrical bounding of
the error of the polynomials is Proposition 2. It indicates that a polynomial’s values are determined at
each point of the input space by the vertices of the upper envelope of its extended Newton polytope.
Therefore, it is expected that two tropical polynomials with approximately equal extended Newton
polytopes should attain similar values. In fact, this serves as the intuition for our theorem. The metric
we use to deﬁne the distance between extended Newton polytopes is the Hausdorff distance."
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1614457831325301,"Hausdorff distance
The distance of a point u ∈Rd from the ﬁnite set V ⊂Rd is denoted by either
dist (u, V) or dist (V, u) and computed as minv∈V ∥u −v∥which is the Euclidean distance of u"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.163855421686747,Published as a conference paper at ICLR 2022
APPROXIMATION OF TROPICAL POLYNOMIALS,0.16626506024096385,"from its closest point v ∈V. The Hausdorff distance H(V, U) of two ﬁnite point sets V, U ⊂Rd is
deﬁned as"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1686746987951807,"H (V, U) = max

max
v∈V dist (v, U) , max
u∈U dist (V, u)
"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1710843373493976,"Let P, ˜P be two polytopes with their vertex sets denoted by VP , V ˜
P respectively. We deﬁne the"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.17349397590361446,"Hausdorff distance H

P, ˜P

of the two polytopes as the Hausdorff distance of their respective vertex
sets VP , V ˜
P . Namely,"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.17590361445783131,"H

P, ˜P

:= H (VP , V ˜
P )
(4)"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1783132530120482,"Clearly, the Hausdorff distance is a metric of how close two polytopes are to each other. Indeed, it
becomes zero when the two polytopes coincide. According to this metric, the following bound on the
error of tropical polynomial approximation is derived."
APPROXIMATION OF TROPICAL POLYNOMIALS,0.18072289156626506,"Proposition 4. Let p, ˜p ∈Rmax[x] be two tropical polynomials and let P = ENewt (p) , ˜P =
ENewt (˜p). Then,"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.18313253012048192,"max
x∈B |p(x) −˜p(x)| ≤ρ · H

P, ˜P
"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1855421686746988,"where B = {x ∈Rd : ∥x∥≤r} is the hypersphere of radius r, and ρ =
√"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.18795180722891566,r2 + 1.
APPROXIMATION OF TROPICAL POLYNOMIALS,0.19036144578313252,"The above proposition enables us to handle the more general case of neural networks with one hidden
layer, that are equivalent with tropical rational mappings. By repeatedly applying Proposition 4 to
each tropical polynomial corresponding to the networks, we get the following bound."
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1927710843373494,"Theorem 1. Let v, ˜v ∈Rmax[x] be two neural networks with architecture as in Fig. 2. With ˜Pj, ˜Qj
we denote the positive and negative zonotopes of ˜v. The following bound holds."
APPROXIMATION OF TROPICAL POLYNOMIALS,0.19518072289156627,"max
x∈B ∥v(x) −˜v(x)∥1 ≤ρ ·  
m
X"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.19759036144578312,"j=1
H

Pj, ˜Pj

+ H

Qj, ˜Qj

 "
APPROXIMATION OF TROPICAL POLYNOMIALS,0.2,"Remark 1. The reason we choose to compute the error of the approximation on a bounded hyper-
sphere B is twofold. Firstly, the unbounded error of linear terms always diverges to inﬁnity and,
secondly, in practice the working subspace of our dataset is usually bounded, e.g. images."
NEURAL NETWORK COMPRESSION ALGORITHMS,0.20240963855421687,"4
NEURAL NETWORK COMPRESSION ALGORITHMS"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.20481927710843373,"Compression problem formulation
The tropical geometry theorem on the approximation of
neural networks enables us to derive compression algorithms for ReLU activated neural networks.
Suppose that we want to compress the neural network of Fig. 2 by reducing the number of neurons in
the hidden layer, from n to K. Let us assume that the output of the compressed network is the tropical
rational map ˜v = (˜v1, ..., ˜vm). Its j−th component may be written as ˜vj(x) = ˜pj(x) −˜qj(x) where
using Proposition 3 the zonotopes of ˜pj, ˜qj are generated by ˜cji(˜aT
i , ˜bi), ∀i. The generators need
to be chosen in such a way that ˜vj(x) ≈vj(x) for all x ∈B. Due to Theorem 1 it sufﬁces to ﬁnd"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.20722891566265061,"generators such that the resulting zonotopes have H

Pj, ˜Pj

, H

Qj, ˜Qj

as small as possible ∀j.
We thus formulated neural network compression as a geometrical zonotope approximation problem."
NEURAL NETWORK COMPRESSION ALGORITHMS,0.20963855421686747,"Our approaches
Approximating a zonotope with fewer generators is a problem known as zonotope
order reduction (Kopetzki et al., 2017). In our case we approach this problem by manipulating the
zonotope generators cji
 
aT
i , bi

, ∀i, j 1. Each of the algorithms presented will create a subset of
altered generators that approximate the original zonotopes. Ideally, we require the approximation to
hold simultaneously for all positive and negative zonotopes of each output component vj. However,
this is not always possible, as in the case of multiclass neural networks, and it necessarily leads to
heuristic manipulation. Our ﬁrst attempt to tackle this problem applies the K-means algorithm to the"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.21204819277108433,1Dealing with the full generated zonotope would lead to exponential computational overhead.
NEURAL NETWORK COMPRESSION ALGORITHMS,0.21445783132530122,Published as a conference paper at ICLR 2022 x1 x2 f3 f4 f5 f6 f7 f2 f1 v c1 c2 c3 c4 c5 c6 c7 b1 b2 b3 b4 b5 b6 b7
NEURAL NETWORK COMPRESSION ALGORITHMS,0.21686746987951808,"{aji}i∈(1,2), j∈(1,...,7)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.21927710843373494,(a) Original network.
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2216867469879518,"c1(aT
1 , b1)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.22409638554216868,"c2(aT
2 , b2)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.22650602409638554,"c3(aT
3 , b3)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2289156626506024,"c4(aT
4 , b4)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.23132530120481928,"c5(aT
5 , b5)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.23373493975903614,"c6(aT
6 , b6)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.236144578313253,"c7(aT
7 , b7)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2385542168674699,(b) Original zonotopes.
NEURAL NETWORK COMPRESSION ALGORITHMS,0.24096385542168675,"˜c1(˜aT
1 , ˜b1)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2433734939759036,"˜c2(˜aT
2 , ˜b2)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2457831325301205,"˜c4(˜aT
4 , ˜b4) ≡c7(aT
7 , b7)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.24819277108433735,"˜c3(˜aT
3 , ˜b3)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.25060240963855424,(c) Resulting zonotopes. x1 x2 ˜f1 ˜f2 ˜f3 ˜f4 v + + − −
NEURAL NETWORK COMPRESSION ALGORITHMS,0.25301204819277107,˜c1˜b1
NEURAL NETWORK COMPRESSION ALGORITHMS,0.25542168674698795,˜c2˜b2
NEURAL NETWORK COMPRESSION ALGORITHMS,0.25783132530120484,˜c3˜b3
NEURAL NETWORK COMPRESSION ALGORITHMS,0.26024096385542167,"˜c4˜b4
{˜cj˜aji}i∈(1,2), j∈(1,...,4)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.26265060240963856,(d) Compressed network.
NEURAL NETWORK COMPRESSION ALGORITHMS,0.26506024096385544,"Figure 3: Illustration of Zonotope K-means execution. The original zonotope P is generated by
ci
 
aT
i , bi

for i = 1, ..., 4 and the negative zonotope Q generated by the remaining ones i = 5, 6, 7."
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2674698795180723,"The approximation ˜P of P is colored in purple and generated by ˜ci

˜aT
i , ˜bi

, i = 1, 2 where the
ﬁrst generator is the K-means center representing the generators 1, 2 of P and the second is the
representative center of 3, 4. Similarly, the approximation ˜Q of Q is colored in green and deﬁned by
the generators ˜ci

˜aT
i , ˜bi

, i = 3, 4 that stand as representative centers for {5, 6} and 7 respectively."
NEURAL NETWORK COMPRESSION ALGORITHMS,0.26987951807228916,"positive and negative generators, separately. This method is restricted on applying to single output
neural networks. Our second approach further develops this technique to multiclass neural networks.
Speciﬁcally, it utilizes K-means on the vectors associated with the neural paths passing from a node
in the hidden layer, as we deﬁne later. The algorithms we present refer to the neural network of Fig.
2 with one hidden layer, but we may repeatedly apply them to compress deeper networks."
ZONOTOPE APPROXIMATION,0.27228915662650605,"4.1
ZONOTOPE APPROXIMATION"
ZONOTOPE APPROXIMATION,0.2746987951807229,"Zonotope K-means
The ﬁrst compression approach uses K-means to compress each zonotope of
the network, and covers only the case of a single output neural network, e.g as in Fig. 2 but with
m = 1. The algorithm reduces the hidden layer size from n to K neurons. We use the notation
ci, i = 1, ..., n for weights of the second linear layer, connecting the hidden layer with the output
node. Algorithm 1 is presented below and a demonstration of its execution can be found in Fig. 3."
ZONOTOPE APPROXIMATION,0.27710843373493976,Algorithm 1: Zonotope K-means Compression
SPLIT GENERATORS INTO POSITIVE,0.27951807228915665,"1. Split generators into positive

ci
 
aT
i , bi

: ci > 0
	
and negative

ci
 
aT
i , bi

: ci < 0
	
."
APPLY K-MEANS FOR K,0.2819277108433735,2. Apply K-means for K
APPLY K-MEANS FOR K,0.28433734939759037,"2 centers, separately for both sets of generators and receive
n
˜ci

˜aT
i , ˜bi

: ˜ci > 0
o
,
n
˜ci

˜aT
i , ˜bi

: ˜ci < 0
o
as output."
APPLY K-MEANS FOR K,0.28674698795180725,"3. Construct the ﬁnal weights. For the ﬁrst linear layer, the weights and the bias which
correspond to the i−th neuron become the vector ˜ci

˜aT
i , ˜bi

."
APPLY K-MEANS FOR K,0.2891566265060241,"4. The weights of the second linear layer are set to 1 for every hidden layer neuron
where ˜ci

˜aT
i , ˜bi

occurs from positive generators and −1, elsewhere."
APPLY K-MEANS FOR K,0.29156626506024097,Proposition 5. Zonotope K-means produces a compressed neural network with output ˜v satisfying
APPLY K-MEANS FOR K,0.29397590361445786,"1
ρ · max
x∈B |v(x) −˜v(x)| ≤K · δmax +

1 −
1
Nmax 
n
X"
APPLY K-MEANS FOR K,0.2963855421686747,"i=1
|ci|∥
 
aT
i , bi

∥"
APPLY K-MEANS FOR K,0.2987951807228916,"where K is the total number of centers used in both K-means, δmax is the largest distance from a point
to its corresponding cluster center and Nmax is the maximum cardinality of a cluster."
APPLY K-MEANS FOR K,0.30120481927710846,"The above proposition provides an upper bound between the original neural network and the one that
is approximated with Zonotope K-means. In particular, if we use K = n centers the bound of the"
APPLY K-MEANS FOR K,0.3036144578313253,Published as a conference paper at ICLR 2022
APPLY K-MEANS FOR K,0.3060240963855422,"approximation error becomes 0, because then δmax = 0 and Nmax = 1. Also, if K ≈0 the bound gets
a ﬁxed value depending on the magnitude of the weights of the linear layers."
MULTIPLE ZONOTOPE APPROXIMATION,0.30843373493975906,"4.2
MULTIPLE ZONOTOPE APPROXIMATION"
MULTIPLE ZONOTOPE APPROXIMATION,0.3108433734939759,"The exact positive and negative zonotope approximation performed by Zonotope K-means algorithm
has a main disadvantage: it can only be used in single output neural networks. Indeed, suppose
that we want to employ the preceeding algorithm to approach the zonotopes of each output in a
multiclass neural network. That would require 2m separate executions of K-means which are not
necessarily consistent. For instance, it is possible to have cj1i > 0 and cj2i < 0 for some output
components vj1, vj2. That means that in the compression procedure of vj1, the i−th neuron belongs
to the positive generators set, while for vj2, it belongs to the negative one. This makes the two
compressions incompatible. Moreover, the drawback of restricting to single output only allow us to
compress the ﬁnal ReLU layer and not any preceeding ones."
MULTIPLE ZONOTOPE APPROXIMATION,0.3132530120481928,"Neural Path K-means
To overcome this obstacle we apply a simultaneous approximation of the
zonotopes. The method is called Neural Path K-means and directly applies K-means to the vectors of
the weights
 
aT
i , bi, c1i, ..., cmi

associated to each neuron i of the hidden layer. The name of the
algorithm emanates from the fact that the vector associated to each neuron consists of the weights of
all the neural network paths passing from this neuron. The procedure is presented in Algorithm 2."
MULTIPLE ZONOTOPE APPROXIMATION,0.3156626506024096,Algorithm 2: Neural Path K-means Compression
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3180722891566265,"1. Apply K-means for K centers to the vectors
 
aT
i , bi, CT
:,i

, i = 1, ..., n, and get"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3204819277108434,"the centers

˜aT
i , ˜bi, ˜CT
:,i

, i = 1, ..., K."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3228915662650602,"2. Construct the ﬁnal weights. For the ﬁrst linear layer matrix the i −th row becomes

˜aT
i , ˜bi

, while for the second linear layer matrix, the i −th column becomes ˜C:,i."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3253012048192771,"Null Generators
Neural Path K-means does not apply compression directly to each zonotope of
the network, but is rather a heuristic approach for this task. More precisely, if we focus on the set
of generators of the zonotopes of output j, Neural Path K-means might mix positive and negative
generators together in the same cluster. For instance, suppose

˜aT
k , ˜bk, ˜CT
:,k

is the cluster center"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.327710843373494,"corresponding to vectors
 
aT
i , bi, CT
:,i

for i ∈I. Then regarding output j, it is not necessary
that ∀i ∈I all cji have the same sign. Thus, the compressed positive zonotope ˜Pj might contain
generators of the original negative zonotope Qj and vice versa. We will call generators cji
 
aT
i , bi
"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3301204819277108,"contributing to opposite zonotopes, null generators."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3325301204819277,Proposition 6. Neural Path K-means produces a compressed neural network with output ˜v satisfying
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3349397590361446,"1
ρ · max
x∈B ∥v(x) −˜v(x)∥1 ≤√mKδ2
max + √m

1 −
1
Nmax 
n
X"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3373493975903614,"i=1
∥C:,i∥
 
aT
i , bi
 +"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3397590361445783,"√mδmax Nmin n
X i=1"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3421686746987952,"  
aT
i , bi
 + ∥C:,i∥

+ m
X j=1 X"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.344578313253012,"i∈Nj
|cji|
 
aT
i , bi
"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3469879518072289,"where K is the number of K-means clusters, δmax the maximum distance from any point to its
corresponding cluster center, Nmax, Nmin the maximum and minimum cardinality respectively of a
cluster and Nj the set of null generators with respect to output j."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3493975903614458,"The performance of Neural Path K-means is evaluated with Proposition 6. The result we deduce
is analogous to Zonotope K-means. The bound of the approximation error becomes zero when K
approaches n. Indeed, for K = n we get δmax = 0, Nmax = 1 and Nj = ∅, ∀j ∈[m]. For lower
values of K, the upper bound reaches a value depending on the magnitude of the weights of the linear
layers together with weights corresponding to null generators."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.35180722891566263,Published as a conference paper at ICLR 2022
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3542168674698795,Table 1: Reporting accuracy of compressed networks for single output compression methods.
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3566265060240964,"Percentage of
remaining
neurons"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.35903614457831323,"MNIST 3/5
MNIST 4/9"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3614457831325301,"(Smyrnis et al.,
2020)
Zonotope
K-means
Neural Path
K-means
(Smyrnis et al.,
2020)
Zonotope
K-means
Neural Path
K-means"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.363855421686747,"100% (Original)
99.18 ± 0.27
99.38 ± 0.09
99.38 ± 0.09
99.53 ± 0.09
99.53 ± 0.09
99.53 ± 0.09
5%
99.12 ± 0.37
99.42 ± 0.07
99.25 ± 0.04
98.99 ± 0.09
99.52 ± 0.09
99.48 ± 0.15
1%
99.11 ± 0.36
99.39 ± 0.05
99.32 ± 0.03
99.01 ± 0.09
99.46 ± 0.05
99.35 ± 0.17
0.5%
99.18 ± 0.36
99.41 ± 0.05
99.22 ± 0.11
98.81 ± 0.09
99.35 ± 0.24
98.84 ± 1.18
0.3%
99.18 ± 0.36
99.25 ± 0.37
99.19 ± 0.41
98.81 ± 0.09
98.22 ± 1.38
98.22 ± 1.33"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.36626506024096384,"Table 2: Reporting theoretical upper bounds of Propositions 5, 6."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3686746987951807,"Percentage of
remaining neurons
MNIST 3/5
MNIST 4/9"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3710843373493976,"Zonotope K-means
Neural Path K-means
Zonotope K-means
Neural Path K-means"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.37349397590361444,"100%
0.00
0.00
0.00
0.00
10%
17.07
246.74
18.85
229.37
2.5%
15.35
59.42
17.02
63.37
1%
14.79
42.22
16.44
45.58
0.5%
14.57
36.47
16.20
39.71"
EXPERIMENTS,0.3759036144578313,"5
EXPERIMENTS"
EXPERIMENTS,0.3783132530120482,"We conduct experiments on compressing the linear layers of convolutional neural networks. Our
experiments serve as proof-of-concept and indicate that our theoretical claims indeed hold in practice.
The heart of our contribution lies in presenting novel tropical geometrical background for neural
network approximation that will shed light for further research towards tropical mathematics."
EXPERIMENTS,0.38072289156626504,"Our methods compress the linear layers of the network layer by layer. They perform a functional
approximation of the original network and thus they are applicable for both classiﬁcation and
regression tasks. To compare them with other techniques in the literature we choose methods with
similar structure, i.e. structured pruning techniques without re-training. For example, Alfarra et al.
(2020) proposed a compression algorithm based on the sparsiﬁcation of the matrices representing the
zonotopes which served as an intuition for part of our work. However, their method is unstructured
and incompatible for comparison. The methods we choose to compare are two tropical methods for
single-output (Smyrnis et al., 2020) and multi-output (Smyrnis & Maragos, 2020) networks, Random
and L1 Structured, and a modiﬁcation of ThiNet (Luo et al., 2017) adapted to linear layers. Smyrnis
et al. (2020); Smyrnis & Maragos (2020) proposed a novel tropical division framework that aimed on
the reduction of zonotope vertices. Random method prunes neurons according to uniform probability,
while L1 prunes those with the lowest value of L1 norm of their weights. Also, ThiNet uses a greedy
criterion for discarding the neurons that have the smallest contribution to the output of the network."
EXPERIMENTS,0.38313253012048193,"MNIST Dataset, Pairs 3-5 and 4-9
The ﬁrst experiment is performed on the binary classiﬁcation
tasks of pairs 3/5 and 4/9 of the MNIST dataset and so we can utilize both of our proposed methods.
In Table 1, we compare our methods with a tropical geometrical approach of Smyrnis et al. (2020).
Their method is based on a tropical division framework for network minimization. For fair comparison,
we use the same CNN with two fully connected layers and hidden layer of size 1000. According to
Table 1, our techniques seem to have similar performance. They retain the accuracy of the network
while reducing its size. Moreover, in Table 2 we include experimental computation of the theoretical
bounds provided by Proposition 5, 6. We notice that the bounds decrease as the remaining weights
get less. The behaviour of the bounds was expected to be incremental because the less weights we
use, the compression gets worse and the error becomes larger. However, the opposite holds which
means that the bounds are tighter for higher pruning rates. It is also important to mention that the
bounds become 0 when we keep all the weights, as expected."
EXPERIMENTS,0.3855421686746988,"MNIST and Fashion-MNIST Datasets
For the second experiment we employ MNIST and
Fashion-MNIST datasets. The corresponding classiﬁcation is multiclass and thus Neural Path
K-means may only be applied. In Table 3, we compare it with the multiclass tropical method of
Smyrnis & Maragos (2020) using the same CNN architecture they do. Furthermore, in plots 4a,
4b we compare Neural Path K-means with ThiNet and baseline pruning methods by compressing"
EXPERIMENTS,0.38795180722891565,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.39036144578313253,Table 3: Reporting accuracy of compressed networks for multiclass compression methods.
EXPERIMENTS,0.3927710843373494,"Percentage of
remaining neurons
MNIST
Fashion-MNIST"
EXPERIMENTS,0.39518072289156625,"(Smyrnis & Maragos, 2020)
Neural Path K-means
(Smyrnis & Maragos, 2020)
Neural Path K-means"
EXPERIMENTS,0.39759036144578314,"100% (Original)
98.60 ± 0.03
98.61 ± 0.11
88.66 ± 0.54
89.52 ± 0.19
50%
96.39 ± 1.18
98.13 ± 0.28
83.30 ± 2.80
88.22 ± 0.32
25%
95.15 ± 2.36
98.42 ± 0.42
82.22 ± 2.85
86.67 ± 1.12
10%
93.48 ± 2.57
96.89 ± 0.55
80.43 ± 3.27
86.04 ± 0.94
5%
92.93 ± 2.59
96.31 ± 1.29
−
83.68 ± 1.06"
EXPERIMENTS,0.4,"LeNet5 (LeCun et al., 1998). To get a better idea of how our method performs in deeper architectures
we provide plots 4c,4d that illustrate the performance of compressing a deep neural network with
layers of size 28 ∗28, 512, 256, 128 and 10, which we refer to as deepNN. The compression is
executed on all hidden layers beginning from the input and heading to the output. From Table 3, we
deduce that our method performs better than (Smyrnis & Maragos, 2020). Also, it achieves higher
accuracy scores and experience lower variance as shown in plots 4a-4d. Neural Path K-means, overall,
seems to have good performance, even competitive to ThiNet. Its worst performance occurs on low
percentages of remaining weights. An explanation for this is that K-means provides a high-quality
compression as long as the number of centers is not less than the number of ""real"" clusters."
EXPERIMENTS,0.40240963855421685,"(a) LeNet5, MNIST
(b) LeNet5, F-MNIST
(c) deepNN, MNIST
(d) deepNN, F-MNIST"
EXPERIMENTS,0.40481927710843374,"(e) AlexNet, CIFAR10
(f) CIFAR-VGG, CIFAR10
(g) AlexNet, CIFAR100
(h)
CIFAR-VGG,
CI-
FAR100"
EXPERIMENTS,0.4072289156626506,"Figure 4: Neural Path K-means compared with baseline pruning methods and ThiNet. Horizontal
axis shows the ratio of remaining neurons in each hidden layer of the fully connected part."
EXPERIMENTS,0.40963855421686746,"CIFAR Dataset
We conduct our ﬁnal experiment on CIFAR datasets using CIFAR-VGG (Blalock
et al., 2020) and an altered version of AlexNet adapted for CIFAR. The resulting plots are shown in
Fig. 4e-4h. We deduce that Neural Path K-means retains a good performance on larger datasets. In
particular, in most cases it has slightly better accuracy an lower deviation than the baselines, but has
worse behaviour when keeping almost zero weights."
CONCLUSIONS AND FUTURE WORK,0.41204819277108434,"6
CONCLUSIONS AND FUTURE WORK"
CONCLUSIONS AND FUTURE WORK,0.41445783132530123,"We presented a novel theorem on the bounding of the approximation error between two neural
networks. This theorem occurs from the bounding of the tropical polynomials representing the neural
networks via the Hausdorff distance of their extended Newton polytopes. We derived geometrical
compression algorithms for the fully connected parts of ReLU activated deep neural networks, while
application to convolutional layers is an ongoing work. Our algorithms seem to perform well in
practice and motivate further research towards the direction revealed by tropical geometry."
CONCLUSIONS AND FUTURE WORK,0.41686746987951806,Published as a conference paper at ICLR 2022
REFERENCES,0.41927710843373495,REFERENCES
REFERENCES,0.42168674698795183,"Motasem Alfarra, Adel Bibi, Hasan Hammoud, Mohamed Gaafar, and Bernard Ghanem. On the
decision boundaries of deep neural networks: A tropical geometry perspective. arXiv preprint
arXiv:2002.08838, 2020."
REFERENCES,0.42409638554216866,"Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of
neural network pruning? arXiv preprint arXiv:2003.03033, 2020."
REFERENCES,0.42650602409638555,"Peter Butkoviˇc. Max-linear systems: theory and algorithms. Springer monographs in mathematics.
Springer, 2010. ISBN 978-1-84996-298-8."
REFERENCES,0.42891566265060244,"Vasileios Charisopoulos and Petros Maragos. Morphological perceptrons: geometry and training
algorithms. In International Symposium on Mathematical Morphology and Its Applications to
Signal and Image Processing, pp. 3–15. Springer, 2017."
REFERENCES,0.43132530120481927,"Vasileios Charisopoulos and Petros Maragos. A tropical approach to neural networks with piecewise
linear activations. arXiv preprint arXiv:1805.08749, 2018."
REFERENCES,0.43373493975903615,"Raymond A Cuninghame-Green. Minimax algebra, volume 166. Springer Science & Business
Media, 2012."
REFERENCES,0.43614457831325304,"Nikolaos Dimitriadis and Petros Maragos. Advances in morphological neural networks: Training,
pruning and enforcing shape constraints. In Proc. 46th IEEE Int’l Conf. Acoustics, Speech and
Signal Processing (ICASSP-2021), Toronto, June 2021."
REFERENCES,0.43855421686746987,"Richard James Dufﬁn and Elmor L Peterson. Geometric programming with signomials. Journal of
Optimization Theory and Applications, 11(1):3–35, 1973."
REFERENCES,0.44096385542168676,"Branko Grünbaum. Convex polytopes, volume 221. Springer Science & Business Media, 2013."
REFERENCES,0.4433734939759036,"Anna-Kathrin Kopetzki, Bastian Schürmann, and Matthias Althoff. Methods for order reduction of
zonotopes. In 2017 IEEE 56th Annual Conference on Decision and Control (CDC), pp. 5626–5633.
IEEE, 2017."
REFERENCES,0.4457831325301205,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.44819277108433736,"Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. ThiNet: A Filter Level Pruning Method for Deep Neural
Network Compression. In 2017 IEEE International Conference on Computer Vision (ICCV), pp.
5068–5076, 2017. doi: 10.1109/ICCV.2017.541."
REFERENCES,0.4506024096385542,"Diane Maclagan and Bernd Sturmfels. Introduction to tropical geometry, volume 161. American
Mathematical Soc., 2015."
REFERENCES,0.4530120481927711,"Petros Maragos. Dynamical systems on weighted lattices: general theory. Mathematics of Control,
Signals, and Systems, 29(4):1–49, 2017."
REFERENCES,0.45542168674698796,"Petros Maragos and Emmanouil Theodosis. Multivariate tropical regression and piecewise-linear
surface ﬁtting. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pp. 3822–3826. IEEE, 2020."
REFERENCES,0.4578313253012048,"Petros Maragos, Vasileios Charisopoulos, and Emmanouil Theodosis. Tropical geometry and machine
learning. Proceedings of the IEEE, 109(5):728–755, 2021. doi: 10.1109/JPROC.2021.3065238."
REFERENCES,0.4602409638554217,"Guido Montúfar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. arXiv preprint arXiv:1402.1869, 2014."
REFERENCES,0.46265060240963857,"Georgios Smyrnis and Petros Maragos. Multiclass neural network minimization via tropical newton
polytope approximation. In Proc. Int’l Conf. on Machine Learning, PMLR, 2020."
REFERENCES,0.4650602409638554,"Georgios Smyrnis, Petros Maragos, and George Retsinas. Maxpolynomial division with application to
neural network simpliﬁcation. In ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 4192–4196. IEEE, 2020."
REFERENCES,0.4674698795180723,Published as a conference paper at ICLR 2022
REFERENCES,0.46987951807228917,"Emmanouil Theodosis and Petros Maragos. Analysis of the viterbi algorithm using tropical algebra
and geometry. In 2018 IEEE 19th International Workshop on Signal Processing Advances in
Wireless Communications (SPAWC), pp. 1–5. IEEE, 2018."
REFERENCES,0.472289156626506,"Emmanouil Theodosis and Petros Maragos. Tropical modeling of weighted transducer algorithms on
graphs. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 8653–8657, 2019. doi: 10.1109/ICASSP.2019.8683127."
REFERENCES,0.4746987951807229,"Nikos Tsilivis, Anastasios Tsiamis, and Petros Maragos. Sparsity in max-plus algebra and applications
in multivariate convex regression. In Proc. 46th IEEE Int’l Conf. Acoustics, Speech and Signal
Processing (ICASSP-2021), Toronto, June 2021."
REFERENCES,0.4771084337349398,"Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks. In
International Conference on Machine Learning, pp. 5824–5832. PMLR, 2018."
REFERENCES,0.4795180722891566,"Günter M Ziegler. Lectures on polytopes, volume 152. Springer Science & Business Media, 2012."
REFERENCES,0.4819277108433735,Published as a conference paper at ICLR 2022
REFERENCES,0.4843373493975904,"A
PROOFS FOR THE SECTION ""BACKGROUND ON TROPICAL GEOMETRY"""
REFERENCES,0.4867469879518072,"A.1
PROOF OF PROPOSITION 3"
REFERENCES,0.4891566265060241,"Proof. The ﬁrst argument follows from the fact that both pj, qj are linear combinations of tropical
polynomials consisting of two terms. Indeed, we compute"
REFERENCES,0.491566265060241,"pj(x) =
X"
REFERENCES,0.4939759036144578,"cji>0
cji max(aT
i x + bi, 0) =
X"
REFERENCES,0.4963855421686747,"cji>0
max(cjiaT
i x + cjibi, 0)
Prop. 1
====⇒"
REFERENCES,0.4987951807228916,"Pj =
M"
REFERENCES,0.5012048192771085,"cji>0
ENewt
 
max(cjiaT
i x + cjibi, 0)
"
REFERENCES,0.5036144578313253,"Each ENewt
 
max(cjiaT
i x + cjibi, 0)

is a line segment with endpoints 0 and
 
cjiaT
i , cjibi

=
cji
 
aT
i , bi

. Therefore Pj is written as the Minkowski sum of line segments, which is a zonotope by
deﬁnition. Similarly Qj is a zonotope."
REFERENCES,0.5060240963855421,"Furthermore, from the deﬁnition of the Minkowski sum, each point v ∈Pj may be written as
P"
REFERENCES,0.5084337349397591,"cji>0 vi, where each vi is a point in the segment ENewt
 
max(cjiaT
i x + cjibi, 0)

. A vertex of
Pj can only occur if vi is an extreme point of ENewt
 
max(cjiaT
i x + cjibi, 0)

for every i which is
equivalent to either vi = 0 or vi = cji
 
aT
i , bi

. This means that every vertex of Pj corresponds to a
subset I+ ⊆[n] of indices i with cji > 0, for which we choose vi = cji
 
aT
i , bi

and for the rest it
holds vi = 0. Thus,
v =
X"
REFERENCES,0.5108433734939759,"i∈I+
cji
 
aT
i , bi
"
REFERENCES,0.5132530120481927,In the same way we derive the analogous result for the negative zonotope Qj.
REFERENCES,0.5156626506024097,"Corollary 2. The geometric result concerning the structure of zonotopes can be extended to max-
pooling layers. For instance, a max-pooling layer of size 2 × 2 corresponds to a polytope that is
constructed as the Minkowski sum of pyramids which could stand as generalized case of zonotope."
REFERENCES,0.5180722891566265,"B
PROOFS FOR THE SECTION ""APPROXIMATION OF TROPICAL
POLYNOMIALS"""
REFERENCES,0.5204819277108433,"B.1
PROOF OF PROPOSITION 4"
REFERENCES,0.5228915662650603,"Proof. Consider a point x ∈B and assume that p(x) = aT x + b, ˜p(x) = cT x + d. Then,"
REFERENCES,0.5253012048192771,"p(x) −˜p(x) = p(x) −
max
(˜aT ,˜b)∈V˜
p
{˜aT x + ˜b} ≤aT x + b −
 
uT , v
 
x
1 "
REFERENCES,0.5277108433734939,"where
 
uT , v

may be any vertex of ˜P. Similarly, we derive
 
rT , s
 
x
1"
REFERENCES,0.5301204819277109,"
−
 
cT x + d

≤p(x) −˜p(x)"
REFERENCES,0.5325301204819277,"for any vertex
 
rT , s

of P. Therefore, we may select the vertices
 
uT , v

∈˜P,
 
rT , s

∈P so that
their respective distances from
 
aT , b

and
 
cT , d

, respectively, are minimized. Choosing them in
such a way gives"
REFERENCES,0.5349397590361445,"p(x) −˜p(x) ≤aT x + b −
 
uT , v
 
x
1"
REFERENCES,0.5373493975903615,"
=
  
aT , b

−
 
uT , v
 
x
1 
≤"
REFERENCES,0.5397590361445783,"≤
 
aT , b

−
 
uT , v
 
x
1"
REFERENCES,0.5421686746987951," ≤d
 
aT , b

, ˜P
 p"
REFERENCES,0.5445783132530121,"r2 + 1
(5)"
REFERENCES,0.5469879518072289,In similar manner we deduce
REFERENCES,0.5493975903614458,"p(x) −˜p(x) ≥
 
rT , s
 
x
1"
REFERENCES,0.5518072289156627,"
−cT x + d =
  
rT , s

−
 
cT , d
 
x
1 
≥"
REFERENCES,0.5542168674698795,"≥−
 
rT , s

−
 
cT , d
 
x
1"
REFERENCES,0.5566265060240964," ≥−d
 
P,
 
cT , d
 p"
REFERENCES,0.5590361445783133,"r2 + 1
(6)"
REFERENCES,0.5614457831325301,Published as a conference paper at ICLR 2022
REFERENCES,0.563855421686747,"Notice, that for the relations (5) and (6) we used Cauchy-Schwartz inequality"
REFERENCES,0.5662650602409639,"|⟨x, y⟩| ≤∥x∥∥y∥⇔−∥x∥∥y∥≤⟨x, y⟩≤∥x∥∥y∥"
REFERENCES,0.5686746987951807,"Inequality (5) holds at any point x ∈B for some vertex
 
aT , b

∈P, therefore"
REFERENCES,0.5710843373493976,"p(x) −˜p(x) ≤ρ ·
max
(aT ,b)∈VP
d
  
aT , b

, V ˜
P

(7)"
REFERENCES,0.5734939759036145,"for all x ∈B. Similarly, we derive"
REFERENCES,0.5759036144578313,"p(x) −˜p(x) ≥
min
(c,d)∈V ˜
P
−ρ · d
 
VP ,
 
cT , d

= −
max
(cT ,d)∈V ˜
P
ρ · d
 
VP ,
 
cT , d

(8)"
REFERENCES,0.5783132530120482,Combining (7) and (8) gives
REFERENCES,0.5807228915662651,"−
max
(cT ,d)∈V ˜
P
ρ · d
 
VP ,
 
cT , d

≤p(x) −˜p(x) ≤
max
(aT ,b)∈VP
ρ · d
  
aT , b

, V ˜
P

⇔"
REFERENCES,0.5831325301204819,"|p(x) −˜p(x)| ≤ρ · max

max
(aT ,b)∈VP
ρ · d
  
aT , b

, V ˜
P

,
max
(cT ,d)∈V ˜
P
ρ · d
 
VP ,
 
cT , d
"
REFERENCES,0.5855421686746988,"Hence, from the deﬁnition of the Hausdorff distance of two polytopes we derive the desired upper
bound"
REFERENCES,0.5879518072289157,"|p(x) −˜p(x)| ≤ρ · H

P, ˜P

, ∀x ∈B ⇒"
REFERENCES,0.5903614457831325,"max
x∈B |p(x) −˜p(x)| ≤ρ · H

P, ˜P
"
REFERENCES,0.5927710843373494,"Remark 2. Note that with similar proof one may replace the Hausdorff distance of the two polytopes
by the Hausdorff distance of their upper envelopes. This makes our theorem an exact generalization
of Proposition 2. However, this format is difﬁcult to use in practice, because it is computationally
harder to determine the vertices of the upper envelope."
REFERENCES,0.5951807228915663,"B.2
PROOF OF THEOREM 1"
REFERENCES,0.5975903614457831,Proof. Notice that we may write
REFERENCES,0.6,"∥v(x) −˜v(x)∥1 = m
X"
REFERENCES,0.6024096385542169,"j=1
|vj(x) −˜vj(x)| = m
X"
REFERENCES,0.6048192771084338,"j=1
|(pj(x) −qj(x)) −(˜pj(x) −˜qj(x))| = m
X"
REFERENCES,0.6072289156626506,"j=1
|(pj(x) −˜pj(x)) −(qj(x) −˜qj(x))| ≤ m
X"
REFERENCES,0.6096385542168675,"j=1
|pj(x) −˜pj(x)| + |qj(x) −˜qj(x)|"
REFERENCES,0.6120481927710844,Thus from from Proposition 4 we derive
REFERENCES,0.6144578313253012,"max
x∈B ∥v(x) −˜v(x)∥1 ≤ρ ·  
m
X"
REFERENCES,0.6168674698795181,"j=1
H

Pj, ˜Pj

+ H

Qj, ˜Qj

 "
REFERENCES,0.619277108433735,"C
PROOFS AND FIGURES FOR THE SECTION ""NEURAL NETWORK
COMPRESSION ALGORITHMS"""
REFERENCES,0.6216867469879518,"C.1
ILLUSTRATION OF ZONOTOPE K-MEANS"
REFERENCES,0.6240963855421687,Below we present a larger version of Fig. 5 demonstrating the execution of Zonotope K-means .
REFERENCES,0.6265060240963856,Published as a conference paper at ICLR 2022 x1 x2 f3 f4 f5 f6 f7 f2 f1 v c1 c2 c3 c4 c5 c6 c7 b1 b2 b3 b4 b5 b6 b7
REFERENCES,0.6289156626506024,"{aji}i∈(1,2), j∈(1,...,7)"
REFERENCES,0.6313253012048192,(a) Original network. x1 x2 ˜f1 ˜f2 ˜f3 ˜f4 v + + − −
REFERENCES,0.6337349397590362,˜c1˜b1
REFERENCES,0.636144578313253,˜c2˜b2
REFERENCES,0.6385542168674698,˜c3˜b3
REFERENCES,0.6409638554216868,"˜c4˜b4
{˜cj˜aji}i∈(1,2), j∈(1,...,4)"
REFERENCES,0.6433734939759036,(b) Minimized network.
REFERENCES,0.6457831325301204,"c1(aT
1 , b1)"
REFERENCES,0.6481927710843374,"c2(aT
2 , b2)"
REFERENCES,0.6506024096385542,"c3(aT
3 , b3)"
REFERENCES,0.653012048192771,"c4(aT
4 , b4)"
REFERENCES,0.655421686746988,"c5(aT
5 , b5)"
REFERENCES,0.6578313253012048,"c6(aT
6 , b6)"
REFERENCES,0.6602409638554216,"c7(aT
7 , b7)"
REFERENCES,0.6626506024096386,(c) Original zonotopes
REFERENCES,0.6650602409638554,"˜c1(˜aT
1 , ˜b1)"
REFERENCES,0.6674698795180722,"˜c2(˜aT
2 , ˜b2)"
REFERENCES,0.6698795180722892,"˜c4(˜aT
4 , ˜b4) ≡c7(aT
7 , b7)"
REFERENCES,0.672289156626506,"˜c3(˜aT
3 , ˜b3)"
REFERENCES,0.6746987951807228,(d) Approximating zonotopes.
REFERENCES,0.6771084337349398,"Figure 5: Illustration of Zonotope K-means execution. The original zonotope P is generated by
ci
 
aT
i , bi

for i = 1, ..., 4 and the negative zonotope Q generated by the remaining ones i = 5, 6, 7."
REFERENCES,0.6795180722891566,"The approximation ˜P of P is colored in purple and generated by ˜ci

˜aT
i , ˜bi

, i = 1, 2 where the
ﬁrst generator is the K-means center representing the generators 1, 2 of P and the second is the
representative center of 3, 4. Similarly, the approximation ˜Q of Q is colored in green and deﬁned by
the generators ˜ci

˜aT
i , ˜bi

, i = 3, 4 that stand as representative centers for {5, 6} and 7 respectively."
REFERENCES,0.6819277108433734,"C.2
PROOF OF PROPOSITION 5"
REFERENCES,0.6843373493975904,Proof. We remind that for the output functions it holds
REFERENCES,0.6867469879518072,"v(x) = p(x) −q(x) , ˜v(x) = ˜p(x) −˜q(x)"
REFERENCES,0.689156626506024,From triangle inequality we deduce
REFERENCES,0.691566265060241,|v(x) −˜v(x)| = |p(x) −q(x) −(˜p(x) −˜q(x))| < |p(x) −˜p(x)| + |q(x) −˜q(x)|
REFERENCES,0.6939759036144578,"Prop 4 bounds |p(x) −˜p(x)| and |q(x) −˜q(x)| are bounded by H

P, ˜P

and H

Q, ˜Q

respec-
tively. Therefore, it sufﬁces to get an upper bound for these Hausdorff distances. Let us consider any"
REFERENCES,0.6963855421686747,Published as a conference paper at ICLR 2022
REFERENCES,0.6987951807228916,vertex u = P
REFERENCES,0.7012048192771084,"i∈I+ ci
 
aT
i , bi

of P. For the vertex u ∈P we need to choose vertex v ∈˜P as close"
REFERENCES,0.7036144578313253,"to u as possible, in order to provide an upper bound for dist

u, ˜P

. Vertex v is selected as follows."
REFERENCES,0.7060240963855422,"For each i ∈I+ we select k such that ˜ck
 
˜aT
k
˜bk

is the center of the cluster where ci
 
aT
i , bi
"
REFERENCES,0.708433734939759,"belongs to. We denote the set of such clusters by C+, where each cluster center k appears only once.
Then, vertex v is constructed as v = P"
REFERENCES,0.7108433734939759,"k∈C+ ˜ck
 
˜aT
k
˜bk

∈˜P. We have that:"
REFERENCES,0.7132530120481928,"dist

u, ˜P

≤  X"
REFERENCES,0.7156626506024096,"i∈I+
ci
 
aT
i , bi

−
X"
REFERENCES,0.7180722891566265,"k∈C+
˜ck

˜aT
k , ˜bk
 ≤
X k∈C+  X"
REFERENCES,0.7204819277108434,"i∈Ik+
ci
 
aT
i , bi

−˜ck

˜aT
k , ˜bk
 ≤
X k∈C+ X i∈Ik+"
REFERENCES,0.7228915662650602,"ci
 
aT
i , bi

−
˜ck

˜aT
k , ˜bk
 |Ik+|  =
X k∈C+ X i∈Ik+"
REFERENCES,0.7253012048192771,"ci
 
aT
i , bi

−ci
 
aT
i , bi

+ εi
|Ik+|  ≤
X k∈C+ X i∈Ik+"
REFERENCES,0.727710843373494,"
1 −
1
|Ik+|"
REFERENCES,0.7301204819277108,"
|ci|
 
aT
i , bi
 + ∥εi∥ |Ik+| "
REFERENCES,0.7325301204819277,"≤|C+| · δmax +

1 −
1
Nmax  X"
REFERENCES,0.7349397590361446,"i∈I+
|ci|
 
aT
i , bi
"
REFERENCES,0.7373493975903614,"where we denote by Ik+ the set of indices i ∈I+ that belong to the center k ∈C+ and εi =
˜ck

˜aT
k , ˜bk

−ci
 
aT
i , bi

is the vector of the difference of the i−th generator, with its corresponding
K-means cluster center."
REFERENCES,0.7397590361445783,"The maximum value of the upper bound occurs when I+ contains all indices that correspond
to ci > 0.
This value gives us an upper bound for maxu∈P d(u, ˜P).
To compute an up-
per bound for maxv∈V ˜
P d(P, v) we assume v = P"
REFERENCES,0.7421686746987952,"k∈C+ ˜ck
 
˜aT
k
˜bk

and consider the vertex
P"
REFERENCES,0.744578313253012,"i∈I+ ci
 
aT
i , bi

∈P where I+ is the set of indices of positive generators corresponding to the
union of all clusters corresponding to the centers of C+. Note that the occurring distance X"
REFERENCES,0.7469879518072289,"i∈I+
ci
 
aT
i , bi

−
X"
REFERENCES,0.7493975903614458,"k∈C+
˜ck

˜aT
i , ˜bi
"
REFERENCES,0.7518072289156627,"was taken into account when computing the upper bound for maxu∈VP d(u, ˜P), and thus both values
obtain the same upper bound. Therefore,"
REFERENCES,0.7542168674698795,"H

P, ˜P

≤K+ · δmax +

1 −
1
Nmax  X"
REFERENCES,0.7566265060240964,"i∈I+
|ci|
 
aT
i , bi
"
REFERENCES,0.7590361445783133,"where K+ is the number of cluster centers corresponding to ˜P and I+ the indices corresponding to
all positive generators of P. Similarly,"
REFERENCES,0.7614457831325301,"H

Q, ˜Q

≤K−· δmax +

1 −
1
Nmax  X"
REFERENCES,0.763855421686747,"i∈I−
|ci|
 
aT
i , bi
"
REFERENCES,0.7662650602409639,"where K−, I−are deﬁned in analogous way for the negative zonotope. Combining the relations gives
the desired bound."
REFERENCES,0.7686746987951807,"1
ρ · |v(x) −˜v(x)| ≤H

P, ˜P

+ H

Q, ˜Q

≤K · δmax +

1 −
1
Nmax 
n
X"
REFERENCES,0.7710843373493976,"i=1
|ci|
 
aT
i , bi
"
REFERENCES,0.7734939759036145,Published as a conference paper at ICLR 2022
REFERENCES,0.7759036144578313,"C.3
ILLUSTRATION FOR NEURAL PATH K-MEANS ALGORITHM"
REFERENCES,0.7783132530120482,"Below we illustrate the vectors on which K-means is applied for the multi-output case. The vectors
that are compressed are consist of all the edges associated to a hidden layer neuron. The corresponding
edges contain all the possible neural paths that begin from some node of the input, end in some node
of the output and pass through this hidden node. x1 x2 ... xk ... xd f1 f2 ... fi ... fn v1 v2 ... vj ... vm ai1 ai2 aik aid c1i c2i cji cmi b1 b2 bi bn"
REFERENCES,0.7807228915662651,"Figure 6: Neural Path K-means for multi-output neural network compression. In green color we
highlight the weights corresponding to the i−th vector used by Neural Path K-means."
REFERENCES,0.7831325301204819,"In the main text we deﬁned the null generators of zonotopes that occur by the execution of Neural
Path K-means. Below we provide an illustration for them."
REFERENCES,0.7855421686746988,Null Generators
REFERENCES,0.7879518072289157,"Figure 7: Visualization of K-means in Rd+1+n, where d is the input dimension and n the hidden
layer size. We color points according to the j−th output component of the network. Black and white
points correspond to generators of Pj and Qj respectively. White vertices in positive (brown) clusters
and black vertices in negative (blue) clusters are null generators regarding j−th output."
REFERENCES,0.7903614457831325,"C.4
PROOF OF PROPOSITION 6"
REFERENCES,0.7927710843373494,"Proof. Let us ﬁrst focus on a single output, say j−th output. As in the proof for Zonotope K-means,
we will bound H

Pj, ˜Pj

, H

Qj, ˜Qj

for all j ∈[m]. From triangle inequality we get"
REFERENCES,0.7951807228915663,|vj(x) −˜vj(x)| ≤|pj(x) −˜pj(x)| + |qj(x) −˜qj(x)|
REFERENCES,0.7975903614457831,Any vertex of u ∈Pj can be written as u = P
REFERENCES,0.8,"i∈Ij+ cji
 
aT
i , bi

where the set of indices Ij+ satisfy"
REFERENCES,0.8024096385542169,"cji > 0, ∀i ∈Ij+ and thus correspond to positive generators. To choose a nearby vertex from ˜Pj
we perform the following. For each i ∈Ij+ we select the center
 
˜aT
k
˜bk
˜C(k)T 
of the cluster to
where
 
aT
i
bi
C(i)T 
belongs, only if ˜cjk > 0. Such a center only exists if cji
 
aT
i , bi

is not a"
REFERENCES,0.8048192771084337,Published as a conference paper at ICLR 2022
REFERENCES,0.8072289156626506,"null generator. Else, we choose as representation the vector 0. Each cluster center, or 0, is taken into
account once and the vertex P
k∈Cj+ ˜cjk

˜ak, ˜bk

∈˜Pj is formed. We derive:"
REFERENCES,0.8096385542168675,"max
u∈VPj
dist

u, ˜Pj

≤  X"
REFERENCES,0.8120481927710843,"i∈Ij+
cji
 
aT
i , bi

−
X"
REFERENCES,0.8144578313253013,"k∈Cj+
˜cjk

˜aT
k , ˜bk
 ≤
X k∈Cj+  X"
REFERENCES,0.8168674698795181,"i∈Ijk+
cji
 
aT
i , bi

−˜cjk

˜aT
k , ˜bk


+
X"
REFERENCES,0.8192771084337349,"i∈Nj+
|cji|
 
aT
i , bi
 ≤
X k∈Cj+ X"
REFERENCES,0.8216867469879519,i∈Ijk+
REFERENCES,0.8240963855421687,"cji
 
aT
i , bi

−
˜cjk

˜aT
k , ˜bk
"
REFERENCES,0.8265060240963855,"|Ijk+| +
X"
REFERENCES,0.8289156626506025,"i∈Nj+
|cji|
 
aT
i , bi
 ≤
X k∈C X"
REFERENCES,0.8313253012048193,i∈Ijk+
REFERENCES,0.8337349397590361,"cji
 
aT
i , bi

−(cji + εji)
 
aT
i , bi

+ λi
"
REFERENCES,0.8361445783132531,"|Ijk+| +
X"
REFERENCES,0.8385542168674699,"i∈Nj+
|cji|
 
aT
i , bi
 ≤
X k∈Cj+ X"
REFERENCES,0.8409638554216867,i∈Ijk+
REFERENCES,0.8433734939759037,|εji|∥λi∥
REFERENCES,0.8457831325301205,"|Ijk+|
+

1 −
1
|Ijk+|"
REFERENCES,0.8481927710843373,"
|cji|
 
aT
i , bi


+ +
X k∈Cj+ X"
REFERENCES,0.8506024096385543,i∈Ijk+
REFERENCES,0.8530120481927711,"""
|εji|
 
aT
i , bi
 + |cji| ∥λi∥
|Ijk+| # +
X"
REFERENCES,0.8554216867469879,"i∈Nj+
|cji|
 
aT
i , bi
"
REFERENCES,0.8578313253012049,where for all i ∈Ijk+ the i−th vector of K-means is represented by the k−th center k ∈Cj+. We
REFERENCES,0.8602409638554217,"also assumed ˜C(i) = C:,i + ε(i) ⇒˜cji = cji + εji and

˜aT
i , ˜bi

=
 
aT
i , bi

+ λi."
REFERENCES,0.8626506024096385,"The maximum value of the upper bound occurs when Ij+ contains all indices that correspond
to cji > 0. To compute an upper bound for maxv∈V ˜
Pj dist (Pj, v) we write the vertex v as v = P"
REFERENCES,0.8650602409638555,"k∈Cj+ ˜cjk
 
˜aT
k
˜bk

∈˜Pj and choose the vertex u = P"
REFERENCES,0.8674698795180723,"i∈Ij+ cji
 
aT
i , bi

of Pj where
Ij+ is the set of all indices corresponding to generators that belong to these clusters. As in the
proof of Proposition 5, their distance was taken into account when computing the upper bound for
maxu∈VPj dist

u, ˜Pj

. Hence, both obtain the same upper bound which leads to"
REFERENCES,0.8698795180722891,"H

Pj, ˜Pj

≤
X k∈Cj+ X"
REFERENCES,0.8722891566265061,i∈Ijk+
REFERENCES,0.8746987951807229,|εji|∥λi∥
REFERENCES,0.8771084337349397,"|Ijk+|
+

1 −
1
|Ijk+|"
REFERENCES,0.8795180722891566,"
|cji|
 
aT
i , bi


+ +
X k∈Cj+ X"
REFERENCES,0.8819277108433735,i∈Ijk+
REFERENCES,0.8843373493975903,"""
|εji|
 
aT
i , bi
 + |cji| ∥λi∥
|Ijk+| # +
X"
REFERENCES,0.8867469879518072,"i∈Nj+
|cji|
 
aT
i , bi
"
REFERENCES,0.8891566265060241,"where Ij+ contains all indices corresponding to positive cji. Similarly, we deduce"
REFERENCES,0.891566265060241,"H

Qj, ˜Qj

≤
X k∈Cj− X"
REFERENCES,0.8939759036144578,i∈Ijk−
REFERENCES,0.8963855421686747,|εji|∥λi∥
REFERENCES,0.8987951807228916,"|Ijk−|
+

1 −
1
|Ijk−|"
REFERENCES,0.9012048192771084,"
|cji|
 
aT
i , bi


+ +
X k∈Cj− X"
REFERENCES,0.9036144578313253,i∈Ijk−
REFERENCES,0.9060240963855422,"""
|εji|
 
aT
i , bi
 |cji| ∥λi∥
|Ijk−| # +
X"
REFERENCES,0.908433734939759,"i∈Nj−
|cji|
 
aT
i , bi
"
REFERENCES,0.9108433734939759,where Ij−contains all i such that cji < 0. In total these bounds together with Proposition 4 give
REFERENCES,0.9132530120481928,Published as a conference paper at ICLR 2022
REFERENCES,0.9156626506024096,"1
ρ · max
x∈B |vj(x) −˜vj(x)| ≤
X k∈Cj X i∈Ijk"
REFERENCES,0.9180722891566265,|εji|∥λi∥
REFERENCES,0.9204819277108434,"|Ijk|
+

1 −
1
|Ijk|"
REFERENCES,0.9228915662650602,"
|cji|
 
aT
i , bi


+ +
X k∈Cj X i∈Ijk"
REFERENCES,0.9253012048192771,"""
|εji|
 
aT
i , bi
 + |cji| ∥λi∥
|Ijk| # +
X"
REFERENCES,0.927710843373494,"i∈Nj
|cji|
 
aT
i , bi
"
REFERENCES,0.9301204819277108,"Here we used the notation Cj = Cj+ ∪Cj−= {1, 2, ..., K} and Ijk is either equal to Ijk+ or Ijk−
depending on k ∈Cj. Note that {i|i ∈Ijk, k ∈Cj} = {1, 2, ..., n} \ Nj ⊆{1, 2, ..., n}, since
every generator that is not null corresponds to some cluster center with the same sign. Also, using
Nmax ≥|Ijk| ≥Nmin, it follows that"
REFERENCES,0.9325301204819277,"1
ρ · max
x∈B |vj(x) −˜vj(x)| ≤ n
X i=1"
REFERENCES,0.9349397590361446,|εji|∥λi∥
REFERENCES,0.9373493975903614,"Nmin
+

1 −
1
Nmax"
REFERENCES,0.9397590361445783,"
|cji|
 
aT
i , bi


+ + n
X i=1"
REFERENCES,0.9421686746987952,"""
|εji|
 
aT
i , bi
 + |cji| ∥λi∥
Nmin # +
X"
REFERENCES,0.944578313253012,"i∈Nj
|cji|
 
aT
i , bi
"
REFERENCES,0.946987951807229,"We will compute the total cost that combines all outputs by applying the inequality
 
m
X"
REFERENCES,0.9493975903614458,"j=1
|uj|   2 ≤m  
m
X"
REFERENCES,0.9518072289156626,"j=1
|uj|2  ⇔ m
X"
REFERENCES,0.9542168674698795,"j=1
|uj| ≤√m ∥(u1, ..., um)∥"
REFERENCES,0.9566265060240964,"which is a direct application of Cauchy-Schwartz inequality.
Together with the relations
∥ε(i)∥< δmax,
∥λi∥< δmax, we get 1
ρ · m
X"
REFERENCES,0.9590361445783132,"j=1
max
x∈B |vj(x) −˜vj(x)| ≤ m
X j=1 n
X i=1"
REFERENCES,0.9614457831325302,|εji|∥λi∥
REFERENCES,0.963855421686747,"Nmin
+

1 −
1
Nmax"
REFERENCES,0.9662650602409638,"
|cji|
 
aT
i , bi


+ + m
X j=1 n
X i=1"
REFERENCES,0.9686746987951808,"""
|εji|
 
aT
i , bi
 + |cji| ∥λi∥
Nmin # + m
X j=1 X"
REFERENCES,0.9710843373493976,"i∈Nj
|cji|
 
aT
i , bi
 ≤ n
X i=1"
REFERENCES,0.9734939759036144,"""√m
ε(i) ∥λi∥"
REFERENCES,0.9759036144578314,"Nmin
+

1 −
1
Nmax"
REFERENCES,0.9783132530120482," √m ∥C:,i∥
 
aT
i , bi

# + + n
X i=1"
REFERENCES,0.980722891566265,"""√m
ε(i)  
aT
i , bi
 + √m ∥C:,i∥∥λi∥
Nmin # + m
X j=1 X"
REFERENCES,0.983132530120482,"i∈Nj
|cji|
 
aT
i , bi
"
REFERENCES,0.9855421686746988,"≤√mKδ2
max + √m

1 −
1
Nmax 
n
X"
REFERENCES,0.9879518072289156,"i=1
∥C:,i∥
 
aT
i , bi
 +"
REFERENCES,0.9903614457831326,"√mδmax Nmin n
X i=1"
REFERENCES,0.9927710843373494,"  
aT
i , bi
 + ∥C:,i∥

+ m
X j=1 X"
REFERENCES,0.9951807228915662,"i∈Nj
|cji|
 
aT
i , bi
"
REFERENCES,0.9975903614457832,as desired.
