Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016420361247947454,"Creating novel views from a single image has achieved tremendous strides with
1"
ABSTRACT,0.003284072249589491,"advanced autoregressive models. Although recent methods generate high-quality
2"
ABSTRACT,0.0049261083743842365,"novel views, synthesizing with only one explicit or implicit 3D geometry has a
3"
ABSTRACT,0.006568144499178982,"trade-off between two objectives that we call the ‚Äúseesaw‚Äù problem: 1) preserv-
4"
ABSTRACT,0.008210180623973728,"ing reprojected contents and 2) completing realistic out-of-view regions. Also,
5"
ABSTRACT,0.009852216748768473,"autoregressive models require a considerable computational cost. In this paper, we
6"
ABSTRACT,0.011494252873563218,"propose a single-image view synthesis framework for mitigating the seesaw prob-
7"
ABSTRACT,0.013136288998357963,"lem. The proposed model is an efficient non-autoregressive model with implicit and
8"
ABSTRACT,0.014778325123152709,"explicit renderers. Motivated by characteristics that explicit methods well preserve
9"
ABSTRACT,0.016420361247947456,"reprojected pixels and implicit methods complete realistic out-of-view region, we
10"
ABSTRACT,0.0180623973727422,"introduce a loss function to complement two renderers. Our loss function promotes
11"
ABSTRACT,0.019704433497536946,"that explicit features improve the reprojected area of implicit features and implicit
12"
ABSTRACT,0.021346469622331693,"features improve the out-of-view area of explicit features. With the proposed
13"
ABSTRACT,0.022988505747126436,"architecture and loss function, we can alleviate the seesaw problem, outperforming
14"
ABSTRACT,0.024630541871921183,"autoregressive-based state-of-the-art methods and generating an image ‚âà100 times
15"
ABSTRACT,0.026272577996715927,"faster. We validate the efficiency and effectiveness of our method with experiments
16"
ABSTRACT,0.027914614121510674,"on RealEstate10K and ACID datasets.
17"
INTRODUCTION,0.029556650246305417,"1
Introduction
18"
INTRODUCTION,0.031198686371100164,"Single-image view synthesis is the task of generating novel view images from a given single image [5,
19"
INTRODUCTION,0.03284072249589491,"18, 23, 38‚Äì40, 47, 50, 54]. It can enable the movement of the camera from a photograph and bring an
20"
INTRODUCTION,0.034482758620689655,"image to 3D, which are significant for various computer vision applications such as image editing and
21"
INTRODUCTION,0.0361247947454844,"animating. To perform the realistic single-image view synthesis in these applications, we can expect
22"
INTRODUCTION,0.03776683087027915,"that the novel view image has to consist of existing objects and unseen new objects from the reference
23"
INTRODUCTION,0.03940886699507389,"viewpoint. Therefore, for high-quality novel views, the following two goals should be considered: 1)
24"
INTRODUCTION,0.041050903119868636,"preserving 3D transformed seen contents of a single reference image and 2) generating semantically
25"
INTRODUCTION,0.042692939244663386,"compatible pixels for filling the unseen region. To achieve two goals, explicit and implicit methods
26"
INTRODUCTION,0.04433497536945813,"have been proposed.
27"
INTRODUCTION,0.04597701149425287,"With the recent success of differentiable geometric transformation methods [2, 31], explicit meth-
28"
INTRODUCTION,0.047619047619047616,"ods [5, 17, 23, 50, 57] leverage such 3D inductive biases to guide the view synthesis network to
29"
INTRODUCTION,0.04926108374384237,"preserve 3D transformed contents, and various generative models are applied to complete the unseen
30"
INTRODUCTION,0.05090311986863711,"regions. Explicit methods can produce high-quality novel view images in small view changes, where
31"
INTRODUCTION,0.052545155993431854,"the content of the reference viewpoint still occupies a large portion. However, for large view changes,
32"
INTRODUCTION,0.054187192118226604,"the image quality is degraded due to a lack of ability to generate pixels of the unseen region. To deal
33"
INTRODUCTION,0.05582922824302135,"with this problem, outpainting with the autoregressive model is exploited to fill unseen regions [39],
34"
INTRODUCTION,0.05747126436781609,"but generating photo-realistic images remains a challenge for explicit methods.
35 32 37 42 47 52 57 62 67"
INTRODUCTION,0.059113300492610835,"14
14.5
15
15.5
16"
INTRODUCTION,0.060755336617405585,SynSin
INTRODUCTION,0.06239737274220033,SynSin-6x
INTRODUCTION,0.06403940886699508,PixelSynth
INTRODUCTION,0.06568144499178982,"GeoFree
Ours"
INTRODUCTION,0.06732348111658457,PSNR on small viewpoint changes
INTRODUCTION,0.06896551724137931,FID on large viewpoint changes
INTRODUCTION,0.07060755336617405,Better preserve
INTRODUCTION,0.0722495894909688,Better generation
INTRODUCTION,0.07389162561576355,Warped image
INTRODUCTION,0.0755336617405583,"Input image
SynSin
PixelSynth"
INTRODUCTION,0.07717569786535304,"GeoFree
Ours
: Explicit method
: Implicit method"
INTRODUCTION,0.07881773399014778,0.063 sec/img
INTRODUCTION,0.08045977011494253,9.39 sec/img
INTRODUCTION,0.08210180623973727,6.22 sec/img
INTRODUCTION,0.08374384236453201,0.056 sec/img
INTRODUCTION,0.08538587848932677,0.063 sec/img
INTRODUCTION,0.08702791461412152,Trade-off [50] [40] [39]
INTRODUCTION,0.08866995073891626,"Figure 1: Seesaw problem of explicit and implicit methods. Explicit methods well preserve warped
contents but sacrifice to fill unseen pixels (‚ÜëPSNR on small view change, ‚ÜëFID on large view
change). Implicit methods amply fill unseen pixels but fall short of preserving seen contents (‚ÜìPSNR
on small view change, ‚ÜìFID on large view change). Our proposed framework alleviates this seesaw
problem and generates an image faster than the state-of-the-art methods."
INTRODUCTION,0.090311986863711,"On the other side, implicit methods [38, 40, 46] less enforce 3D inductive biases and let the model
36"
INTRODUCTION,0.09195402298850575,"learn the required 3D geometry for view synthesis. Based on the powerful autoregressive trans-
37"
INTRODUCTION,0.09359605911330049,"former [10], recent implicit methods learn the 3D geometry from a reference image and camera
38"
INTRODUCTION,0.09523809523809523,"parameters. Implicitly learned 3D geometry allows the model to synthesize diverse and realistic novel
39"
INTRODUCTION,0.09688013136288999,"view images but fails to preserve the contents of the reference image since they reduce 3D inductive
40"
INTRODUCTION,0.09852216748768473,"biases.
41"
INTRODUCTION,0.10016420361247948,"To sum up, previous single-image view synthesis methods suffer from a trade-off between two
42"
INTRODUCTION,0.10180623973727422,"objectives: 1) preserve seen contents and 2) generate semantically compatible unseen regions. Figure 1
43"
INTRODUCTION,0.10344827586206896,"shows an apparent trade-off that explicit methods well preserve seen contents with sacrificing the
44"
INTRODUCTION,0.10509031198686371,"generation of unseen regions and vice versa for implicit methods. Here, we call this trade-off the
45"
INTRODUCTION,0.10673234811165845,"seesaw problem and emphasize the need for combining solid points of explicit and implicit methods.
46"
INTRODUCTION,0.10837438423645321,"Moreover, recent methods often depend on autoregressive models, which generate individual pixels
47"
INTRODUCTION,0.11001642036124795,"sequentially. Sequential generation causes too slower view synthesis than non-autoregressive methods,
48"
INTRODUCTION,0.1116584564860427,"limiting their application areas, such as image animating in real-time. Therefore, we refocus on a fast
49"
INTRODUCTION,0.11330049261083744,"and efficient non-autoregressive model for single view synthesis.
50"
INTRODUCTION,0.11494252873563218,"In this paper, we present a non-autoregressive framework for alleviating the seesaw problem. Our
51"
INTRODUCTION,0.11658456486042693,"approach aims to design the architecture and loss functions. We design two parallel render blocks
52"
INTRODUCTION,0.11822660098522167,"which explicitly or implicitly learn geometric transformations from point cloud representations.
53"
INTRODUCTION,0.11986863711001643,"To bridge explicit and implicit transformations, we propose a novel loss function that motivates
54"
INTRODUCTION,0.12151067323481117,"explicit features improve seen pixels of implicit features and implicit features improve unseen
55"
INTRODUCTION,0.12315270935960591,"pixels of explicit features. Interestingly, we observe that proposed loss makes two renderers embed
56"
INTRODUCTION,0.12479474548440066,"discriminative features and allow the model to use both renderers in a balanced way to create novel
57"
INTRODUCTION,0.12643678160919541,"views. With the proposed architecture and the loss function, we can merge the pros of both explicit
58"
INTRODUCTION,0.12807881773399016,"and implicit methods, alleviating the seesaw problem. As a result, our non-autoregressive framework
59"
INTRODUCTION,0.1297208538587849,"can better preserve seen contents, better complete unseen pixels, and generate images ‚âà100 times
60"
INTRODUCTION,0.13136288998357964,"faster than autoregressive methods. We validate the efficiency and effectiveness of our framework
61"
INTRODUCTION,0.1330049261083744,"with experiments on the indoor dataset RealEstate10K [58] and the outdoor dataset ACID [23].
62"
RELATED WORKS,0.13464696223316913,"2
Related Works
63"
RELATED WORKS,0.13628899835796388,"Novel view synthesis
Given multiple images from different viewpoints of a scene, novel view
64"
RELATED WORKS,0.13793103448275862,"synthesis aims to generate novel view images. Traditionally, multi-view geometry is utilized for
65"
RELATED WORKS,0.13957307060755336,"synthesizing novel viewpoints [4, 6, 7, 13, 21, 42, 59]. Recently, deep neural networks have been
66"
RELATED WORKS,0.1412151067323481,"used to rendering [15, 28, 29, 32] and several representation for view synthesis such as multi-plane
67"
RELATED WORKS,0.14285714285714285,"image [11, 45, 58], point cloud [1], depth [44], radiance field [30, 49, 55] and voxel [25, 33, 43].
68"
RELATED WORKS,0.1444991789819376,ViewNet
RELATED WORKS,0.14614121510673234,Input Image Iref
RELATED WORKS,0.1477832512315271,DepthNet
RELATED WORKS,0.14942528735632185,Back-projection f0
RELATED WORKS,0.1510673234811166,"D
Xw Ximg he hi"
RELATED WORKS,0.15270935960591134,Decoder
RELATED WORKS,0.15435139573070608,output Image Itgt
RELATED WORKS,0.15599343185550082,"Overlap Patch
Embeddings"
RELATED WORKS,0.15763546798029557,Encoder
RELATED WORKS,0.1592775041050903,Mix-FFN x N
RELATED WORKS,0.16091954022988506,Global Set Attention ISAB
RELATED WORKS,0.1625615763546798,"ùúπùíàùíçùíêùíÉùíÇùíç
gglobal"
RELATED WORKS,0.16420361247947454,Local Set Attention
RELATED WORKS,0.16584564860426929,LSA Layer
RELATED WORKS,0.16748768472906403,ùúπùíçùíêùíÑùíÇùíç
RELATED WORKS,0.16912972085385877,"ùíÇùíÉùíî
ùúπùíçùíêùíÑùíÇùíç ùíìùíÜùíç"
RELATED WORKS,0.17077175697865354,glocal fN
RELATED WORKS,0.1724137931034483,Camera Pose T
RELATED WORKS,0.17405582922824303,Explicit Renderer
RELATED WORKS,0.17569786535303777,"Overlap Patch 
Embeddings"
RELATED WORKS,0.17733990147783252,"Transformer 
Block"
RELATED WORKS,0.17898193760262726,Upsample
RELATED WORKS,0.180623973727422,Warping
RELATED WORKS,0.18226600985221675,Implicit Renderer ùõøpos x M
RELATED WORKS,0.1839080459770115,"Overlap Patch 
Embeddings"
RELATED WORKS,0.18555008210180624,"Transformer 
Block"
RELATED WORKS,0.18719211822660098,Upsample x M
RELATED WORKS,0.18883415435139572,": addition
: concatenation"
RELATED WORKS,0.19047619047619047,"Figure 2: An overview of network architecture. Our network takes a reference image Iref and
a relative camera pose T as inputs. The depth estimation network (DepthNet) first predicts a depth
map D, and the view synthesis network (ViewNet) generates a target image Itgt from Iref, D and
T. Specifically, D is used for calculating the 3D world coordinate Xw and the normalized image
coordinate Ximg at the reference viewpoint, which are passed through various positional encoding
layers in the encoder (e.g., Œ¥global, Œ¥abs
local and Œ¥rel
local) to provide the scene structure representations.
Encoded features fN are then transformed by both Implicit Renderer and Explicit Renderer with T.
Finally, two transformed feature map, hi and he, are concatenated to generate Itgt by the decoder."
RELATED WORKS,0.1921182266009852,"Single-image view synthesis is more challenging than general novel view synthesis since a single
69"
RELATED WORKS,0.19376026272577998,"input image is only available [5, 18, 23, 38‚Äì40, 47, 50, 54]. Explicit methods directly inject 3D
70"
RELATED WORKS,0.19540229885057472,"inductive biases into models. For example, SynSin [50] uses 3D point cloud features with estimated
71"
RELATED WORKS,0.19704433497536947,"depth from the model, projects to novel viewpoints, and refines unseen pixels with recent generative
72"
RELATED WORKS,0.1986863711001642,"models [3]. SynSin works well in small viewpoint changes but degrades in large viewpoint changes
73"
RELATED WORKS,0.20032840722495895,"due to the lack of generating unseen pixels. To deal with this problem, PixelSynth [39] exploits the
74"
RELATED WORKS,0.2019704433497537,"autoregressive outpainting model [37] with 3D point cloud representation. Despite using the slow
75"
RELATED WORKS,0.20361247947454844,"autoregressive model, it cannot generate unseen pixels well. For an implicit method, Rombach et
76"
RELATED WORKS,0.20525451559934318,"al. [40] propose a powerful autoregressive transformer. By less enforcing 3D inductive biases, this
77"
RELATED WORKS,0.20689655172413793,"approach can generate realistic view synthesis and complete the unseen region without explicit
78"
RELATED WORKS,0.20853858784893267,"3D geometry. However, its inference time is long due to the autoregressive model, and it fails to
79"
RELATED WORKS,0.21018062397372742,"preserve seen contents of a reference image. We bridge these implicit and explicit methods as a
80"
RELATED WORKS,0.21182266009852216,"non-autoregressive architecture, which can outperform autoregressive approaches with fast inference.
81"
RELATED WORKS,0.2134646962233169,"Transformer for point cloud
The transformer and self-attention have brought a breakthrough in
82"
RELATED WORKS,0.21510673234811165,"natural language processing [8, 48] and computer vision [9]. Inspired by this success, transformer
83"
RELATED WORKS,0.21674876847290642,"and self-attention networks have been widely applied for point cloud recognition tasks and achieved
84"
RELATED WORKS,0.21839080459770116,"remarkable performance gain. Early methods utilize global attention for all of the point clouds,
85"
RELATED WORKS,0.2200328407224959,"resulting in a large amount of computation and inapplicable for large-scale 3D point cloud [24, 52, 53].
86"
RELATED WORKS,0.22167487684729065,"Lee et al. [20] propose the SetTransformer module suitable for point cloud due to permutation-
87"
RELATED WORKS,0.2233169129720854,"invariant, which uses inducing point methods and reduces computational complexity from quadratic
88"
RELATED WORKS,0.22495894909688013,"to linear in the number of elements. Also, local attention methods is utilized to enable scalability [14,
89"
RELATED WORKS,0.22660098522167488,"34, 56]. Notably, among local attention methods, Fast Point Transformer [34] which uses voxel
90"
RELATED WORKS,0.22824302134646962,"hashing-based architecture, achieves both remarkable performance and computational efficiency.
91"
RELATED WORKS,0.22988505747126436,"Global attention may dilute important content by excessive noises as most neighbors are less relevant,
92"
RELATED WORKS,0.2315270935960591,"and local attention may not have sufficient context due to their scope. Therefore, Our approaches use
93"
RELATED WORKS,0.23316912972085385,"both global and local attention to deal with 3D point cloud representation.
94"
METHODOLOGY,0.2348111658456486,"3
Methodology
95"
METHODOLOGY,0.23645320197044334,"Given a reference image Iref and a relative camera pose T, the goal of single-image view synthesis is
96"
METHODOLOGY,0.23809523809523808,"to create a target image Itgt with keeping visible contents of Iref and completing realistic out-of-view
97"
METHODOLOGY,0.23973727422003285,"pixels. To achieve this, we focus on mitigating the seesaw problem between explicit and implicit
98"
METHODOLOGY,0.2413793103448276,"methods in terms of the network architecture and the loss function. Figure 2 describes an overview of
99"
METHODOLOGY,0.24302134646962234,"our network architecture. The network consists of two sub-networks, the depth estimation network
100"
METHODOLOGY,0.24466338259441708,"(DepthNet) and the view synthesis network (ViewNet). Note that the pre-trained DepthNet generates
101"
METHODOLOGY,0.24630541871921183,"depth map D, which is used for ViewNet to synthesize the photo-realistic Itgt.
102"
METHODOLOGY,0.24794745484400657,"3.1
Depth Estimation Network (DepthNet)
103"
METHODOLOGY,0.24958949096880131,"We train the depth estimation network for explicit 3D geometry since ground-truth depths are not
104"
METHODOLOGY,0.2512315270935961,"available. Following Monodepth2 [12], our DepthNet is trained in a self-supervised manner from
105"
METHODOLOGY,0.25287356321839083,"monocular video sequences. Because a ground-truth relative pose between images is available,
106"
METHODOLOGY,0.2545155993431856,"we substitute the pose estimation network with the ground-truth relative pose. Then, we train the
107"
METHODOLOGY,0.2561576354679803,"network on reprojection losses and smoothness losses with auto-masking in their work. After training
108"
METHODOLOGY,0.25779967159277506,"DepthNet, we fix it during training ViewNet.
109"
METHODOLOGY,0.2594417077175698,"3.2
View Synthesis Network (ViewNet)
110"
METHODOLOGY,0.26108374384236455,"We design a simple view synthesis network built on architectural innovations of recent transformer
111"
METHODOLOGY,0.2627257799671593,"models. Specifically, we exploit 3D point cloud representation to consider the relationship between
112"
METHODOLOGY,0.26436781609195403,"the geometry-aware camera pose information and the input image.
113"
METHODOLOGY,0.2660098522167488,"Encoder
The encoder aims to extract scene representations from a feature point cloud of a reference
114"
METHODOLOGY,0.2676518883415435,"image. To deal with point clouds, we design a Global and Local Set Attention (GLSA) block which
115"
METHODOLOGY,0.26929392446633826,"simultaneously extracts overall contexts and detailed semantics. For efficient input size of transform-
116"
METHODOLOGY,0.270935960591133,"ers, Iref ‚ààRH√óW √ó3 is encoded into f0 ‚ààR
H 4 √ó W"
METHODOLOGY,0.27257799671592775,"4 √óC by an overlapping patch embedding [51],
117"
METHODOLOGY,0.2742200328407225,"where C denotes the channel dimension. Then, the homogeneous coordinates p of a pixel in f0
118"
METHODOLOGY,0.27586206896551724,"are mapped into normalized image coordinates Ximg as Ximg(p) = K‚àí1
‚Üìp, where K‚Üìdenotes the
119"
METHODOLOGY,0.277504105090312,"camera intrinsic matrix of f0. Finally, 3D world coordinates of p are calculated with depth map D as
120"
METHODOLOGY,0.2791461412151067,"Xw(p) = D(p)Ximg(p). Our encoder architecture is N stacked GLSA block, and i-th GLSA block
121"
METHODOLOGY,0.28078817733990147,"receives fi‚àí1, Ximg and Xw and outputs fi with Mix-FFN [51].
122"
METHODOLOGY,0.2824302134646962,"Global Set Attention. We utilize Induced Set Attention Block (ISAB) [20] to extract global set
123"
METHODOLOGY,0.28407224958949095,"attention between the feature point clouds. With positional encoder Œ¥global and vector concatenation
124"
METHODOLOGY,0.2857142857142857,"operator ‚äï, the global attention of i-th GLSA bock is represented as:
125"
METHODOLOGY,0.28735632183908044,"gi
global(p) = ISAB(fi(p) ‚äïŒ¥global(Xw(p)).
(1)"
METHODOLOGY,0.2889983579638752,"Local Set Attention. We use a modified Lightweight Self-Attention (LSA) layer [34] for the set
126"
METHODOLOGY,0.29064039408866993,"attention in r √ó r local window of each pixel point. Unlike the decomposing relative position of
127"
METHODOLOGY,0.2922824302134647,"voxels in [34], we decompose the relative position of 3D world coordinates between neighbor pixels
128"
METHODOLOGY,0.2939244663382594,"using normalized image coordinates as:
129"
METHODOLOGY,0.2955665024630542,"Xw(p) ‚àíXw(q) = (Xw(p) ‚àíXimg(p)) ‚àí(Xw(q) ‚àíXimg(q)) + (Ximg(p) ‚àíXimg(q)),
(2)"
METHODOLOGY,0.29720853858784896,"where q ‚ààN(p) is a neighbor set of homogeneous coordinates in a r √ó r window of p. With
130"
METHODOLOGY,0.2988505747126437,"decomposition in Eq. 2, we can divide the relative positional encoding into an continuous positional
131"
METHODOLOGY,0.30049261083743845,"encoding Œ¥abs
local and a discretized positional encoding Œ¥rel
local.Then, the computation procedures for
132"
METHODOLOGY,0.3021346469622332,"local set attention gi
local of i-th GLSA block is similar to LSA layer as:
133"
METHODOLOGY,0.30377668308702793,"li
local(p) = fi(p) ‚äïŒ¥abs
local(Xw(p) ‚àíXimg(p)),"
METHODOLOGY,0.3054187192118227,"gi
local(p) = Œ£q‚ààN(p)SC(œà(li
local(p)), Œ¥rel
local(Ximg(p) ‚àíXimg(q)))œï(li
local(q)),
(3)"
METHODOLOGY,0.3070607553366174,"where œà and œï are MLP-layers, and Sc(a, b) =
a¬∑b
‚à•a‚à•‚à•b‚à•computes the cosine similarity between
134"
METHODOLOGY,0.30870279146141216,"a and b. As pixel coordinates of p and q are all integer, the encoding of Ximg(p) ‚àíXimg(q) is
135"
METHODOLOGY,0.3103448275862069,"hashed over r2 ‚àí1 values, resulting in a space complexity reduction from O(HW ¬∑ r2 ¬∑ C) to
136"
METHODOLOGY,0.31198686371100165,"O(HW ¬∑ C) + O(r2 ¬∑ C).
137"
METHODOLOGY,0.3136288998357964,"Rendering Module
Given the scene representations of the reference image, the rendering module
138"
METHODOLOGY,0.31527093596059114,"learns 3D transformation from the reference viewpoint to the target viewpoint. Motivated by our
139"
METHODOLOGY,0.3169129720853859,"observations of implicit and explicit methods, we design an Explicit Renderer(ER) and an Implicit
140"
METHODOLOGY,0.3185550082101806,"Renderer(IR) connected in parallel to bypass the seesaw problem. The structure of the two renderers
141"
METHODOLOGY,0.32019704433497537,Explicit Renderer
METHODOLOGY,0.3218390804597701,Implicit Renderer hi he ùë∂
METHODOLOGY,0.32348111658456485,"ùë≥ùíïùíî,ùíäùíè
ùë≥ùíïùíî,ùíêùíñùíï grad grad"
METHODOLOGY,0.3251231527093596,"K, D, T ùüè ùüé"
METHODOLOGY,0.32676518883415434,Out-of-view pixels
METHODOLOGY,0.3284072249589491,Re-projected pixels
METHODOLOGY,0.33004926108374383,"Figure 3: An overview of our transformation similarity loss. Two transformed features, hi and
he, are complemented each other by the transformation similarity loss. Specifically, we first derive
out-of-view mask O from K, D and T. By using O, two transformation similarity loss, i.e., Lts,in
and Lts,out, are applied to encourage the discriminability of hi and he, respectively. To guide the
another renderer as intended, we allow the back-propagated gradients of Lts,in only to the reprojected
regions of hi, and those of Lts,out only to the out-of-view regions of he."
METHODOLOGY,0.33169129720853857,"is similar; they consist of an overlapping patch embedding, GPT architecture [36] and ResNet blocks
142"
METHODOLOGY,0.3333333333333333,"with upsampling layers. Note that the overlapping patch embedding and upsampling layers are
143"
METHODOLOGY,0.33497536945812806,"designed for downsampling and upsampling the input feature with the factor of 4, respectively.
144"
METHODOLOGY,0.3366174055829228,"The major difference between the two renderers is how the relative camera pose T is used for the
145"
METHODOLOGY,0.33825944170771755,"geometric transformation.
146"
METHODOLOGY,0.3399014778325123,"Explicit Renderer (ER). Given the rotation matrix R and translation vector t of relative camera pose T,
147"
METHODOLOGY,0.3415435139573071,"p can be reprojected to the homogeneous coordinates of target viewpoint p‚Ä≤ as p‚Ä≤ = K‚ÜìRXw(p) + t.
148"
METHODOLOGY,0.34318555008210183,"The output of encoder fN is warped by splatting operation [31] with optical flow from p to p‚Ä≤. Then,
149"
METHODOLOGY,0.3448275862068966,"warped fN goes through the explicit renderer to produce explicit feature map he.
150"
METHODOLOGY,0.3464696223316913,"Implicit Renderer (IR). Unlike the explicit renderer, the implicit renderer uses the camera parameter
151"
METHODOLOGY,0.34811165845648606,"itself. Instead of embedding 3x4 camera extrinsic matrix, we use independent 7 parameters to embed
152"
METHODOLOGY,0.3497536945812808,pose information; Translation vector t and axis-angle notation ( u
METHODOLOGY,0.35139573070607555,"‚à•u‚à•, Œ∏) to parameterize rotation matrix
153"
METHODOLOGY,0.3530377668308703,"R. We use a positional encoding layer Œ¥pos to embed these parameters and add them to the input of
154"
METHODOLOGY,0.35467980295566504,"the transformer block. fN passes through the implicit renderer and outputs implicit feature map hi.
155"
METHODOLOGY,0.3563218390804598,"Please refer to the supplementary materials for details to compute the axis-angle notation.
156"
METHODOLOGY,0.3579638752052545,"Decoder
Two feature maps from ER and IR, which are denoted as he and hi, are then concatenated
157"
METHODOLOGY,0.35960591133004927,"before the decoder. We use a simple CNN-based decoder by gradually upsampling the concatenated
158"
METHODOLOGY,0.361247947454844,"feature map with four ResNet blocks. Instead of generating pixels in an auto-regressive manner,
159"
METHODOLOGY,0.36288998357963875,"we directly predict all pixels in the one-path, resulting in more than 110 times faster than the
160"
METHODOLOGY,0.3645320197044335,"state-of-the-art autoregressive methods [38‚Äì40] in generating images.
161"
LOSS DESIGN FOR VIEWNET,0.36617405582922824,"3.3
Loss Design for ViewNet
162"
LOSS DESIGN FOR VIEWNET,0.367816091954023,"Following the previous single-image view synthesis methods [39, 50], we also use the ‚Ñì1-loss,
163"
LOSS DESIGN FOR VIEWNET,0.3694581280788177,"perceptual loss [35] and adversarial loss to learn the network. Specifically, we compute ‚Ñì1-loss and
164"
LOSS DESIGN FOR VIEWNET,0.37110016420361247,"perceptual loss between Itgt and the ground-truth image Igt at the target viewpoint. Also, we use the
165"
LOSS DESIGN FOR VIEWNET,0.3727422003284072,"global and local discriminators [19] with a Projected GAN [41] structure and a hinge loss [22]. We
166"
LOSS DESIGN FOR VIEWNET,0.37438423645320196,"observe that our methods improve the generation performance even through these simple network
167"
LOSS DESIGN FOR VIEWNET,0.3760262725779967,"structural innovations. Furthermore, we introduce a transformation similarity loss Lts to complement
168"
LOSS DESIGN FOR VIEWNET,0.37766830870279144,"two output feature maps he and hi.
169"
LOSS DESIGN FOR VIEWNET,0.3793103448275862,"Transformation Similarity Loss
As an extension of the existing seesaw problem, he may have
170"
LOSS DESIGN FOR VIEWNET,0.38095238095238093,"better discriminability than hi in reprojected regions, conversely, hi has better delineation of out-of-
171"
LOSS DESIGN FOR VIEWNET,0.3825944170771757,"view regions than he. Therefore, as shown in Fig. 3, we design the transformation similarity loss
172"
LOSS DESIGN FOR VIEWNET,0.3842364532019704,"between he and hi, expecting that hi learns to keep reprojected image contests, and he also learn
173"
LOSS DESIGN FOR VIEWNET,0.38587848932676516,"to generate realistic out-of-view pixels. Specifically, we use a negative cosine similarity function
174"
LOSS DESIGN FOR VIEWNET,0.38752052545155996,"Sc for calculating the similarity between two feature maps, and the transformation similarity loss
175"
LOSS DESIGN FOR VIEWNET,0.3891625615763547,"Lts = ŒªinLts,in + ŒªoutLts,out is formulated as:
176"
LOSS DESIGN FOR VIEWNET,0.39080459770114945,"Lts,in = ‚àí
1
P"
LOSS DESIGN FOR VIEWNET,0.3924466338259442,p(1 ‚àíO(p)) X
LOSS DESIGN FOR VIEWNET,0.39408866995073893,"p
(1 ‚àíO(p)) ¬∑ Sc(hi(p), detach(he(p))),"
LOSS DESIGN FOR VIEWNET,0.3957307060755337,"Lts,out = ‚àí
1
P
p O(p) X"
LOSS DESIGN FOR VIEWNET,0.3973727422003284,"p
O(p) ¬∑ Sc(detach(hi(p)), he(p)),
(4)"
LOSS DESIGN FOR VIEWNET,0.39901477832512317,"where O(p) ‚ààR
H 4 √ó W"
DENOTES AN OUT-OF-VIEW MASK WHICH IS DERIVED FROM THE DEPTH MAP D AND,0.4006568144499179,"4 denotes an out-of-view mask which is derived from the depth map D and
177"
DENOTES AN OUT-OF-VIEW MASK WHICH IS DERIVED FROM THE DEPTH MAP D AND,0.40229885057471265,"the relative camera pose T. Note that, without detach operations, our transformation similarity loss
178"
DENOTES AN OUT-OF-VIEW MASK WHICH IS DERIVED FROM THE DEPTH MAP D AND,0.4039408866995074,"performs the same as a simple negative cosine similarity loss between two feature maps. Thus, we
179"
DENOTES AN OUT-OF-VIEW MASK WHICH IS DERIVED FROM THE DEPTH MAP D AND,0.40558292282430214,"detach gradients back-propagated from Lts,in to he and gradients from Lts,out to hi, because the
180"
DENOTES AN OUT-OF-VIEW MASK WHICH IS DERIVED FROM THE DEPTH MAP D AND,0.4072249589490969,"detach operation allows the components of Lts to be applied to the intended area.
181"
DENOTES AN OUT-OF-VIEW MASK WHICH IS DERIVED FROM THE DEPTH MAP D AND,0.4088669950738916,"Final Learning Objective
Taken together, our ViewNet is trained on the weighted sum of a ‚Ñì1-loss
182"
DENOTES AN OUT-OF-VIEW MASK WHICH IS DERIVED FROM THE DEPTH MAP D AND,0.41050903119868637,"L‚Ñì1, a perceptual loss Lc, an adversarial loss Ladv and a transformation similarity loss Lts. The total
183"
DENOTES AN OUT-OF-VIEW MASK WHICH IS DERIVED FROM THE DEPTH MAP D AND,0.4121510673234811,"loss is then L = L‚Ñì1 + ŒªcLc + ŒªadvLadv + Lts. We fix Œªc = 1 and Œªadv = 0.1 for all experiments.
184"
DENOTES AN OUT-OF-VIEW MASK WHICH IS DERIVED FROM THE DEPTH MAP D AND,0.41379310344827586,"Table 1: Types of baselines and our method. Note that InfNat [23] varies according to the number
of steps, so we mark it as
."
DENOTES AN OUT-OF-VIEW MASK WHICH IS DERIVED FROM THE DEPTH MAP D AND,0.4154351395730706,"Types
Methods
Tatarchenko et al. [46]
Viewappearance [57]
SynSin [50]
InfNat [23]
PixelSynth [39]
GeoFree [40]
LookOutside [38]
Ours
Explicit
‚úó
‚úì
‚úì
‚úì
‚úì
‚úó
‚úó
‚úì
Implicit
‚úì
‚úó
‚úó
‚úó
‚úó
‚úì
‚úì
‚úì
Autoregressive
‚úó
‚úó
‚úó
‚úì
‚úì
‚úì
‚úó"
EXPERIMENTAL RESULTS,0.41707717569786534,"4
Experimental Results
185"
EXPERIMENTAL SETTINGS,0.4187192118226601,"4.1
Experimental Settings
186"
EXPERIMENTAL SETTINGS,0.42036124794745483,"We now describe experimental settings, and please refer to the supplementary materials for further
187"
EXPERIMENTAL SETTINGS,0.4220032840722496,"details about datasets, baselines, and our network architecture.
188"
EXPERIMENTAL SETTINGS,0.4236453201970443,"Dataset
We used two standard datasets, RealEstate10K [58] and ACID [23], which are a collection
189"
EXPERIMENTAL SETTINGS,0.42528735632183906,"of videos mostly captured in indoor and outdoor scenes, respectively. We divided train and test
190"
EXPERIMENTAL SETTINGS,0.4269293924466338,"sequences as in [40].
191"
EXPERIMENTAL SETTINGS,0.42857142857142855,"Baselines
To validate the effectiveness of our framework, we compared our method to previous
192"
EXPERIMENTAL SETTINGS,0.4302134646962233,"single-image view synthesis methods : Tatarchenko et al. [46], Viewappearance [57], Synsin [50],
193"
EXPERIMENTAL SETTINGS,0.4318555008210181,"InfNat [23], PixelSynth [39], GeoFree [40] and LookOutside [38]. Table 1 briefly shows whether
194"
EXPERIMENTAL SETTINGS,0.43349753694581283,"each method is an explicit, implicit, and autoregressive model. Compared to previous methods, we
195"
EXPERIMENTAL SETTINGS,0.4351395730706076,"use both explicit and implicit geometric transformations without an autoregressive model.
196"
EXPERIMENTAL SETTINGS,0.4367816091954023,"Evaluation Details
Because explicit and implicit methods are respectively advantageous in small
197"
EXPERIMENTAL SETTINGS,0.43842364532019706,"view change and large view change, methods should be evaluated on several sizes of viewpoint
198"
EXPERIMENTAL SETTINGS,0.4400656814449918,"changes for a fair comparison. Therefore, we used a ratio of out-of-view pixels over all pixels to
199"
EXPERIMENTAL SETTINGS,0.44170771756978655,"quantify view changes, resulting in three splits are categorized into small (20-40%), medium (40-60%)
200"
EXPERIMENTAL SETTINGS,0.4433497536945813,"and large (60-80%). Since evaluation datasets do not have ground-truth depth maps, we used depth
201"
EXPERIMENTAL SETTINGS,0.44499178981937604,"maps from our pre-trained DepthNet to derive the ratio of out-of-view mask pixels. Finally, we used
202"
EXPERIMENTAL SETTINGS,0.4466338259441708,"randomly selected 1,000 image pairs for each test split.
203"
EXPERIMENTAL SETTINGS,0.4482758620689655,"We use PSNR on the small split and FID [16] on the medium and large split as evaluation metrics.
204"
EXPERIMENTAL SETTINGS,0.44991789819376027,"PSNR is a traditional metric for comparing images, which is widely used to evaluate consistency.
205"
EXPERIMENTAL SETTINGS,0.451559934318555,"Nevertheless, PSNR is a poor metric to verify the image quality on large viewpoint changes [39, 40].
206"
EXPERIMENTAL SETTINGS,0.45320197044334976,"Still, it can be a good metric for evaluating the preservation of reprojected pixels on small view
207"
EXPERIMENTAL SETTINGS,0.4548440065681445,"changes. Therefore, we use PSNR on the small split to evaluate the ability to preserve seen contents.
208"
EXPERIMENTAL SETTINGS,0.45648604269293924,"For evaluating images quality of view synthesis, FID is widely used [39, 40, 50]. Especially in
209"
EXPERIMENTAL SETTINGS,0.458128078817734,"the medium and large split with many out-of-view pixels, FID indicates how well the model fills
210"
EXPERIMENTAL SETTINGS,0.45977011494252873,"out-of-view pixels and generates realistic images. We use the PSNR and FID of specific splits as
211"
EXPERIMENTAL SETTINGS,0.4614121510673235,"evaluation metrics, but we report the PSNR and FID of all splits to show the overall trend.
212"
EXPERIMENTAL SETTINGS,0.4630541871921182,"Table 2: Quantitative results on RealEstate10K and ACID. Image quality is measured by PSNR
and FID for three types of view changes, i.e., Small, Medium and Large. Furthermore, we show the
average performance over all view changes at the end. For both datasets, best results in each metric
are in bold, and second best are underlined."
EXPERIMENTAL SETTINGS,0.46469622331691296,"Dataset
Methods
Small
Medium
Large
Average
PSNR‚Üë
FID‚Üì
PSNR‚Üë
FID‚Üì
PSNR‚Üë
FID‚Üì
PSNR‚Üë
FID‚Üì"
EXPERIMENTAL SETTINGS,0.4663382594417077,RealEstate10K [58]
EXPERIMENTAL SETTINGS,0.46798029556650245,"Tatarchenko et al. [46]
11.12
258.75
10.90
248.55
10.80
249.24
10.94
252.18
Viewappearance [57]
12.51
142.93
12.79
110.84
12.44
147.27
12.58
133.68
SynSin [50]
15.38
41.75
14.88
43.06
13.96
61.67
14.74
48.83
SynSin-6x [50]
15.17
33.72
14.99
37.28
14.26
48.29
14.81
39.76
PixelSynth [39]
14.46
37.23
13.46
38.39
12.28
45.44
13.40
40.35
GeoFree [40]
14.16
33.48
13.15
34.21
12.57
35.28
13.29
34.32
LookOutside [38]
12.58
44.87
12.72
43.17
12.11
43.22
12.47
43.75
ours
15.87
32.42
14.65
33.04
13.83
35.26
14.78
33.57"
EXPERIMENTAL SETTINGS,0.4696223316912972,ACID [23]
EXPERIMENTAL SETTINGS,0.47126436781609193,"Tatarchenko et al. [46]
14.43
148.19
14.20
151.24
14.34
150.47
14.32
149.97
Viewappearance [57]
14.46
161.91
13.58
203.19
13.21
218.37
13.75
194.49
SynSin [50]
17.48
55.64
16.49
75.88
16.87
79.04
16.95
70.19
InfNat [23] (1-step)
15.94
64.32
14.40
90.80
13.65
106.28
14.66
87.13
InfNat [23] (5-step)
15.16
64.48
14.79
71.52
14.90
65.45
14.95
67.15
PixelSynth [39]
15.81
53.38
14.33
63.48
13.53
65.60
14.56
60.82
GeoFree [40]
14.80
53.21
14.24
58.92
14.22
54.78
14.42
55.64
ours
17.52
42.52
16.54
51.56
15.81
49.28
16.62
47.79"
EXPERIMENTAL SETTINGS,0.4729064039408867,"(a) Input Image
(b) Warped Image
(c) SynSin [50]
(d) PixelSynth [39]
(e) GeoFree [40]
(f) Ours"
EXPERIMENTAL SETTINGS,0.4745484400656814,"Figure 4: Qualitative Results on RealEstate10K and ACID. We compare baselines to our method.
The top two rows are from RealEstate10K, and the bottom two rows are from ACID.
Implementation Details
We first resized all images into a resolution of 256 √ó 256, and normalized
213"
EXPERIMENTAL SETTINGS,0.47619047619047616,"RGB value following [39, 50]. We trained DepthNet using a batch size 50 for 100k iterations and
214"
EXPERIMENTAL SETTINGS,0.47783251231527096,"ViewNet using a batch size 32 for 150k iterations. Training takes about 3 days on 4 NVIDIA Geforce
215"
EXPERIMENTAL SETTINGS,0.4794745484400657,"RTX 3090 GPUs. We used an AdamW [27] optimizer (with Œ≤1 = 0.5 and Œ≤2 = 0.9) and applied
216"
EXPERIMENTAL SETTINGS,0.48111658456486045,"weight decay of 0.01. We first linearly increased the learning rate from 10‚àí6 to 3 ¬∑ 10‚àí4 during the
217"
EXPERIMENTAL SETTINGS,0.4827586206896552,"first 1.5k steps, and then a cosine-decay learning rate schedule [26] was applied towards zero. In
218"
EXPERIMENTAL SETTINGS,0.48440065681444994,"ViewNet, we used 8 GLSA blocks with local window size r = 5 and 6 transformer blocks in each
219"
EXPERIMENTAL SETTINGS,0.4860426929392447,"renderer for all experiments.
220"
COMPARISON TO BASELINES,0.4876847290640394,"4.2
Comparison to Baselines
221"
COMPARISON TO BASELINES,0.48932676518883417,"We now compare our method with the state-of-the-art methods on RealEstate10K and ACID. Table 2
222"
COMPARISON TO BASELINES,0.4909688013136289,"shows quantitative results for both datasets. The implicit method GeoFree [40] reports a lower
223"
COMPARISON TO BASELINES,0.49261083743842365,"Table 3: Average inference time.
Methods
SynSin
InfNat (5-step)
PixelSynth
Time (s/img)
0.063
1.14
6.22
Methods
GeoFree
LookOutside
Ours
Time (s/img)
9.39
22.15
0.056"
COMPARISON TO BASELINES,0.4942528735632184,Table 4: Ablation study on Lts.
COMPARISON TO BASELINES,0.49589490968801314,"Loss Type
no Lts
Lts,in
Lts,out
Lts(no detach)
Lts
PNSR‚Üë
14.47
14.62
14.73
14.59
14.78
FID‚Üì
40.45
38.05
36.95
40.44
33.57"
COMPARISON TO BASELINES,0.4975369458128079,Table 5: Ablation Study on the Set Attention.
COMPARISON TO BASELINES,0.49917898193760263,"Set Attention
Small
Medium
Large
glocal
gglobal
PSNR‚Üë
FID‚Üì
PSNR‚Üë
FID‚Üì
PSNR‚Üë
FID‚Üì
‚úì
15.69
34.07
14.64
34.81
13.78
37.63
‚úì
15.74
32.80
14.61
34.37
13.88
38.68
‚úì
‚úì
15.87
32.42
14.65
33.04
13.83
35.26"
COMPARISON TO BASELINES,0.5008210180623974,"Table 6: Ablation Study on hyperparame-
ters of transformation similarity loss."
COMPARISON TO BASELINES,0.5024630541871922,"Loss Weight
Small
Medium
Large
Œªin
Œªout
PSNR‚Üë
FID‚Üì
PSNR‚Üë
FID‚Üì
PSNR‚Üë
FID‚Üì
0.1
1
15.78
33.95
14.65
34.10
13.81
37.11
10
1
15.48
37.46
14.39
37.46
13.56
40.69
1
0.1
15.46
34.98
14.37
37.51
13.64
39.81
1
10
15.70
35.03
14.54
35.57
13.77
38.43
1
1
15.87
32.42
14.65
33.04
13.83
35.26"
COMPARISON TO BASELINES,0.5041050903119869,"FID in the medium and large split than explicit methods such as SynSin [50] and PixelSynth [39],
224"
COMPARISON TO BASELINES,0.5057471264367817,"but its PSNR of the small split is lower. This shows that previous methods are suffered from the
225"
COMPARISON TO BASELINES,0.5073891625615764,"seesaw problem. However, our method consistently achieves the highest PSNR in the small split on
226"
COMPARISON TO BASELINES,0.5090311986863711,"both datasets, which means our method better preserves reprojected contents than previous methods.
227"
COMPARISON TO BASELINES,0.5106732348111659,"Moreover, our method also achieves the lowest FID in all splits on both datasets, and this demonstrates
228"
COMPARISON TO BASELINES,0.5123152709359606,"that our method generates better quality images with filling compatible pixels regardless of view
229"
COMPARISON TO BASELINES,0.5139573070607554,"changes. As observed in [38, 39], we note that SynSin and its variant (i.e., SynSin-6x) often produce
230"
COMPARISON TO BASELINES,0.5155993431855501,"entirely gray images, resulting they still performing competitive results in PSNR of the medium and
231"
COMPARISON TO BASELINES,0.5172413793103449,"large split. Considering this, our method stably outperforms previous methods in all splits.
232"
COMPARISON TO BASELINES,0.5188834154351396,"Also, qualitative results in Fig. 4 illustrate that the warped regions are well-preserved and invisible
233"
COMPARISON TO BASELINES,0.5205254515599343,"parts are well-completed in our method, whereas explicit methods do not generate realistic images,
234"
COMPARISON TO BASELINES,0.5221674876847291,"and an implicit method loses the semantic information of visible contents. Specifically, GeoFree [40]
235"
COMPARISON TO BASELINES,0.5238095238095238,"does not preserve the table in the first sample and the ships floating on the sea in the third sample.
236"
COMPARISON TO BASELINES,0.5254515599343186,"Also, explicit methods [39, 50] either make the entire out-of-view regions in one color or produce a
237"
COMPARISON TO BASELINES,0.5270935960591133,"less realistic view than our method.
238"
COMPARISON TO BASELINES,0.5287356321839081,"We confirm that mitigating the seesaw problem by well-bridged explicit and implicit geometric
239"
COMPARISON TO BASELINES,0.5303776683087028,"transformations yields high-quality view synthesis, even acquiring a generation speed of about 110
240"
COMPARISON TO BASELINES,0.5320197044334976,"times faster than the previous autoregressive models, as shown in Table 3. The fast generation of
241"
COMPARISON TO BASELINES,0.5336617405582923,"novel view images allows our method to be scalable to various real-time applications.
242"
COMPARISON TO BASELINES,0.535303776683087,"4.3
Ablation Study: Type of Set Attention
243"
COMPARISON TO BASELINES,0.5369458128078818,"We design the global and local set attention block to simultaneously extract overall contexts and
244"
COMPARISON TO BASELINES,0.5385878489326765,"detailed semantics. Therefore, we conducted an ablation study on RealEstate10K [58] to verify each
245"
COMPARISON TO BASELINES,0.5402298850574713,"attention improves the performance of generating novel views. Table 5 shows the quantitative result
246"
COMPARISON TO BASELINES,0.541871921182266,"for the type of set attention. Interestingly, our local set attention improves the performance relatively
247"
COMPARISON TO BASELINES,0.5435139573070608,"in large view changes, while our global set attention performs well on small view changes. From this
248"
COMPARISON TO BASELINES,0.5451559934318555,"result, we conjecture that local and global set attention are more useful for structural reasoning of
249"
COMPARISON TO BASELINES,0.5467980295566502,"out-of-view regions and 3D scene representation of reprojected regions, respectively. Also, significant
250"
COMPARISON TO BASELINES,0.548440065681445,"performance improvement is achieved when both attentions are used.
251"
COMPARISON TO BASELINES,0.5500821018062397,"4.4
Ablation Study: Transformation Similarity Loss
252"
COMPARISON TO BASELINES,0.5517241379310345,"The transformation similarity loss Lts is weighted combination of Lts,in and Lts,out. To understand
253"
COMPARISON TO BASELINES,0.5533661740558292,"the effect of each component, we conducted ablation studies of transformation similarity loss on the
254"
COMPARISON TO BASELINES,0.555008210180624,"RealEstate10K dataset. Table 4 reports the average PSNR and FID of our model by changing various
255"
COMPARISON TO BASELINES,0.5566502463054187,"components of Lts. Results show that combining with gradient stopping operation, Lts,in, and
256"
COMPARISON TO BASELINES,0.5582922824302134,"Lts,out achieves best results among the five variants. Also, either using Lts,in or Lts,out improves
257"
COMPARISON TO BASELINES,0.5599343185550082,"the performance and shows that guiding one renderer from the other renderer with the proposed
258"
COMPARISON TO BASELINES,0.5615763546798029,"loss function is effective. Notably, transformation similarity loss is not practical when the detach
259"
COMPARISON TO BASELINES,0.5632183908045977,"operation is not used. From this result, it is necessary to selectively guide unseen and seen regions by
260"
COMPARISON TO BASELINES,0.5648604269293924,"detaching the gradient.
261"
COMPARISON TO BASELINES,0.5665024630541872,"We also performed an ablation study on balancing parameter Œªin and Œªout. Table 6 illustrates the
262"
COMPARISON TO BASELINES,0.5681444991789819,"results varying weight of Lts. Results show that the case of Œªin = 1, Œªout = 1 performs best. As
263"
COMPARISON TO BASELINES,0.5697865353037767,"mentioned above, it seems essential to complement each other in a balanced way.
264"
COMPARISON TO BASELINES,0.5714285714285714,"w/o ùêø!""
w ùêø!""
w ùêø!"",$%
w ùêø!"",&'!"
COMPARISON TO BASELINES,0.5730706075533661,"‚Ñé! ùëù
""/ ‚Ñé# ùëù
"""
COMPARISON TO BASELINES,0.5747126436781609,(a) Overall
COMPARISON TO BASELINES,0.5763546798029556,"‚Ñé! ùëù
""/ ‚Ñé# ùëù
"""
COMPARISON TO BASELINES,0.5779967159277504,(b) without Lts
COMPARISON TO BASELINES,0.5796387520525451,"‚Ñé! ùëù
""/ ‚Ñé# ùëù
"""
COMPARISON TO BASELINES,0.5812807881773399,"(c) with Lts,in"
COMPARISON TO BASELINES,0.5829228243021346,"‚Ñé! ùëù
""/ ‚Ñé# ùëù
"""
COMPARISON TO BASELINES,0.5845648604269293,"(d) with Lts,out"
COMPARISON TO BASELINES,0.5862068965517241,"‚Ñé! ùëù
""/ ‚Ñé# ùëù
"""
COMPARISON TO BASELINES,0.5878489326765188,(e) with Lts
COMPARISON TO BASELINES,0.5894909688013136,Figure 5: Histogram of ||he(p)||2/||hi(p)||2 on the small and large split of RealEstate10K dataset.
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.5911330049261084,"4.5
Dependency Analysis between Implicit and Explicit Renderers
265"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.5927750410509032,"Our proposed architecture exploits the implicit and explicit renderer and mixes their outputs for
266"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.5944170771756979,"decoding view synthesis results. To understand the dependency between two renderers, we analyze
267"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.5960591133004927,"the norm of output feature maps. For a spatial position p, the norm ratio of two spatial features
268"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.5977011494252874,"||he(p)||2/||hi(p)||2 can represent how much depends on the explicit feature he(p) compared to
269"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.5993431855500821,"implicit feature hi(p). For example, if the ratio is large, the model depends on the explicit renderer
270"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6009852216748769,"than the implicit renderer at position p. We compare histograms of the norm ratio by changing the
271"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6026272577996716,"components of Lts and data splits as shown in Fig. 5.
272"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6042692939244664,"(a) Input Image
(b) Warp Image"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6059113300492611,"(c) Without Lts
(d) With Lts"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6075533661740559,"Figure 6: Visual ablation study. With-
out the transformation similarity loss,
our model complete textured out-of-view
regions but not realistic enough than our
model trained with the transformation
similarity loss."
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6091954022988506,"Figure 5a depicts that using Lts,out and Lts,in tends to make
273"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6108374384236454,"the model more dependent on explicit and implicit features,
274"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6124794745484401,"respectively, compared to our method trained without Lts.
275"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6141215106732348,"Furthermore, these tendencies are more apparent in difficult
276"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6157635467980296,"cases (i.e., large split) as shown in Fig. 5c‚Äì5d. From our ob-
277"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6174055829228243,"servations, we conjecture that guiding only a specific renderer
278"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6190476190476191,"improves the discriminability of that renderer, resulting in the
279"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6206896551724138,"model depending on the improved renderer. Surprisingly, the
280"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6223316912972086,"model trained on combining all components of Lts uses both
281"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6239737274220033,"renderers in a balanced way, and there is less bias in norm
282"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.625615763546798,"ratio even according to data splits as shown in Fig. 5e.
283"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6272577996715928,"The effectiveness of our transformation similarity loss is con-
284"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6288998357963875,"firmed by comparing it to our method that is trained without
285"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6305418719211823,"Lts. Figure 5b shows that our model trained without Lts has
286"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.632183908045977,"some outliers for large view changes despite there being less
287"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6338259441707718,"bias according to data splits. We observe these outliers are
288"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6354679802955665,"derived when the model fails to generate realistic out-of-view
289"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6371100164203612,"regions, especially in challenging settings, such as the net-
290"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.638752052545156,"work having to create novel views for both indoor and outdoor
291"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6403940886699507,"scenes, as shown in Fig. 6. We also confirm that our model
292"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6420361247947455,"trained with Lts performs well even in extreme cases, inform-
293"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6436781609195402,"ing that Lts improves two renderers to embed discriminative
294"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.645320197044335,"features. Collectively, Lts improves the discriminability of output features from two renderers and
295"
DEPENDENCY ANALYSIS BETWEEN IMPLICIT AND EXPLICIT RENDERERS,0.6469622331691297,"makes the behavior of the model stable, resulting in alleviating the seesaw problem.
296"
CONCLUSION,0.6486042692939245,"5
Conclusion
297"
CONCLUSION,0.6502463054187192,"We have introduced a single-image view synthesis framework by bridging explicit and implicit
298"
CONCLUSION,0.6518883415435139,"renderers. Despite using autoregressive models, previous methods still suffer from the seesaw
299"
CONCLUSION,0.6535303776683087,"problem since they use only one explicit or implicit geometric transformation. Thus, we design two
300"
CONCLUSION,0.6551724137931034,"parallel renderers to mitigate the problem and complement renderers with transformation similarity
301"
CONCLUSION,0.6568144499178982,"loss. Alleviating the seesaw problem allows the network to generate novel view images better than
302"
CONCLUSION,0.6584564860426929,"previous methods, even with a non-autoregressive structure. We note that the effectiveness of bridging
303"
CONCLUSION,0.6600985221674877,"two renderers can be applied in other tasks, such as extrapolation. We believe that our work can
304"
CONCLUSION,0.6617405582922824,"prompt refocusing on non-autoregressive architecture for single-image view synthesis.
305"
REFERENCES,0.6633825944170771,"References
306"
REFERENCES,0.6650246305418719,"[1] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. Neural
307"
REFERENCES,0.6666666666666666,"point-based graphics. In European Conference on Computer Vision, pages 696‚Äì712. Springer, 2020.
308"
REFERENCES,0.6683087027914614,"[2] Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Depth-aware
309"
REFERENCES,0.6699507389162561,"video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
310"
REFERENCES,0.6715927750410509,"Recognition, pages 3703‚Äì3712, 2019.
311"
REFERENCES,0.6732348111658456,"[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
312"
REFERENCES,0.6748768472906403,"image synthesis. arXiv preprint arXiv:1809.11096, 2018.
313"
REFERENCES,0.6765188834154351,"[4] Shenchang Eric Chen and Lance Williams. View interpolation for image synthesis. In Proceedings of the
314"
REFERENCES,0.6781609195402298,"20th annual conference on Computer graphics and interactive techniques, pages 279‚Äì288, 1993.
315"
REFERENCES,0.6798029556650246,"[5] Xu Chen, Jie Song, and Otmar Hilliges. Monocular neural image based rendering with continuous view
316"
REFERENCES,0.6814449917898193,"control. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4090‚Äì4100,
317"
REFERENCES,0.6830870279146142,"2019.
318"
REFERENCES,0.6847290640394089,"[6] Paul Debevec, Yizhou Yu, and George Borshukov. Efficient view-dependent image-based rendering with
319"
REFERENCES,0.6863711001642037,"projective texture-mapping. In Eurographics Workshop on Rendering Techniques, pages 105‚Äì116. Springer,
320"
REFERENCES,0.6880131362889984,"1998.
321"
REFERENCES,0.6896551724137931,"[7] Paul E Debevec, Camillo J Taylor, and Jitendra Malik.
Modeling and rendering architecture from
322"
REFERENCES,0.6912972085385879,"photographs: A hybrid geometry-and image-based approach. In Proceedings of the 23rd annual conference
323"
REFERENCES,0.6929392446633826,"on Computer graphics and interactive techniques, pages 11‚Äì20, 1996.
324"
REFERENCES,0.6945812807881774,"[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-
325"
REFERENCES,0.6962233169129721,"tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
326"
REFERENCES,0.6978653530377669,"[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
327"
REFERENCES,0.6995073891625616,"Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
328"
REFERENCES,0.7011494252873564,"16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
329"
REFERENCES,0.7027914614121511,"[10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image
330"
REFERENCES,0.7044334975369458,"synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
331"
REFERENCES,0.7060755336617406,"pages 12873‚Äì12883, 2021.
332"
REFERENCES,0.7077175697865353,"[11] John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan Overbeck, Noah
333"
REFERENCES,0.7093596059113301,"Snavely, and Richard Tucker. Deepview: View synthesis with learned gradient descent. In Proceedings of
334"
REFERENCES,0.7110016420361248,"the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2367‚Äì2376, 2019.
335"
REFERENCES,0.7126436781609196,"[12] Cl√©ment Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular depth estimation
336"
REFERENCES,0.7142857142857143,"with left-right consistency. In Proceedings of the IEEE Conference on Computer Vision and Pattern
337"
REFERENCES,0.715927750410509,"Recognition, pages 270‚Äì279, 2017.
338"
REFERENCES,0.7175697865353038,"[13] Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen.
The lumigraph.
In
339"
REFERENCES,0.7192118226600985,"Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 43‚Äì54,
340"
REFERENCES,0.7208538587848933,"1996.
341"
REFERENCES,0.722495894909688,"[14] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu. Pct:
342"
REFERENCES,0.7241379310344828,"Point cloud transformer. Computational Visual Media, 7(2):187‚Äì199, 2021.
343"
REFERENCES,0.7257799671592775,"[15] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep
344"
REFERENCES,0.7274220032840722,"blending for free-viewpoint image-based rendering. ACM Transactions on Graphics (TOG), 37(6):1‚Äì15,
345"
REFERENCES,0.729064039408867,"2018.
346"
REFERENCES,0.7307060755336617,"[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
347"
REFERENCES,0.7323481116584565,"trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information
348"
REFERENCES,0.7339901477832512,"processing systems, 30, 2017.
349"
REFERENCES,0.735632183908046,"[17] Yuxin Hou, Arno Solin, and Juho Kannala. Novel view synthesis via depth-guided skip connections. In
350"
REFERENCES,0.7372742200328407,"Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3119‚Äì3128,
351"
REFERENCES,0.7389162561576355,"2021.
352"
REFERENCES,0.7405582922824302,"[18] Ronghang Hu, Nikhila Ravi, Alexander C Berg, and Deepak Pathak. Worldsheet: Wrapping the world in a
353"
REFERENCES,0.7422003284072249,"3d sheet for view synthesis from a single image. In Proceedings of the IEEE/CVF International Conference
354"
REFERENCES,0.7438423645320197,"on Computer Vision, pages 12528‚Äì12537, 2021.
355"
REFERENCES,0.7454844006568144,"[19] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and locally consistent image completion.
356"
REFERENCES,0.7471264367816092,"ACM Transactions on Graphics (ToG), 36(4):1‚Äì14, 2017.
357"
REFERENCES,0.7487684729064039,"[20] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer:
358"
REFERENCES,0.7504105090311987,"A framework for attention-based permutation-invariant neural networks. In International Conference on
359"
REFERENCES,0.7520525451559934,"Machine Learning, pages 3744‚Äì3753. PMLR, 2019.
360"
REFERENCES,0.7536945812807881,"[21] Marc Levoy and Pat Hanrahan. Light field rendering. In Proceedings of the 23rd annual conference on
361"
REFERENCES,0.7553366174055829,"Computer graphics and interactive techniques, pages 31‚Äì42, 1996.
362"
REFERENCES,0.7569786535303776,"[22] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017.
363"
REFERENCES,0.7586206896551724,"[23] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa.
364"
REFERENCES,0.7602627257799671,"Infinite nature: Perpetual view generation of natural scenes from a single image. In Proceedings of the
365"
REFERENCES,0.7619047619047619,"IEEE/CVF International Conference on Computer Vision, pages 14458‚Äì14467, 2021.
366"
REFERENCES,0.7635467980295566,"[24] Xinhai Liu, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Point2sequence: Learning the shape
367"
REFERENCES,0.7651888341543513,"representation of 3d point clouds with an attention-based sequence to sequence network. In Proceedings of
368"
REFERENCES,0.7668308702791461,"the AAAI Conference on Artificial Intelligence, volume 33, pages 8778‚Äì8785, 2019.
369"
REFERENCES,0.7684729064039408,"[25] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh.
370"
REFERENCES,0.7701149425287356,"Neural volumes: Learning dynamic renderable volumes from images. arXiv preprint arXiv:1906.07751,
371"
REFERENCES,0.7717569786535303,"2019.
372"
REFERENCES,0.7733990147783252,"[26] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint
373"
REFERENCES,0.7750410509031199,"arXiv:1608.03983, 2016.
374"
REFERENCES,0.7766830870279147,"[27] Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
375"
REFERENCES,0.7783251231527094,"arXiv:1711.05101, 2017.
376"
REFERENCES,0.7799671592775042,"[28] Ricardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel Pidlypenskyi, Jonathan Taylor, Julien Valentin,
377"
REFERENCES,0.7816091954022989,"Sameh Khamis, Philip Davidson, Anastasia Tkach, Peter Lincoln, et al. Lookingood: Enhancing perfor-
378"
REFERENCES,0.7832512315270936,"mance capture with real-time neural re-rendering. arXiv preprint arXiv:1811.05029, 2018.
379"
REFERENCES,0.7848932676518884,"[29] Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely, and
380"
REFERENCES,0.7865353037766831,"Ricardo Martin-Brualla. Neural rerendering in the wild. In Proceedings of the IEEE/CVF Conference on
381"
REFERENCES,0.7881773399014779,"Computer Vision and Pattern Recognition, pages 6878‚Äì6887, 2019.
382"
REFERENCES,0.7898193760262726,"[30] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren
383"
REFERENCES,0.7914614121510674,"Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on
384"
REFERENCES,0.7931034482758621,"computer vision, pages 405‚Äì421. Springer, 2020.
385"
REFERENCES,0.7947454844006568,"[31] Simon Niklaus and Feng Liu. Softmax splatting for video frame interpolation. In Proceedings of the
386"
REFERENCES,0.7963875205254516,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5437‚Äì5446, 2020.
387"
REFERENCES,0.7980295566502463,"[32] David Novotny, Ben Graham, and Jeremy Reizenstein. Perspectivenet: A scene-consistent image generator
388"
REFERENCES,0.7996715927750411,"for new view synthesis in real indoor environments. Advances in Neural Information Processing Systems,
389"
REFERENCES,0.8013136288998358,"32, 2019.
390"
REFERENCES,0.8029556650246306,"[33] Kyle Olszewski, Sergey Tulyakov, Oliver Woodford, Hao Li, and Linjie Luo. Transformable bottleneck
391"
REFERENCES,0.8045977011494253,"networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7648‚Äì
392"
REFERENCES,0.80623973727422,"7657, 2019.
393"
REFERENCES,0.8078817733990148,"[34] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jaesik Park. Fast point transformer. arXiv preprint
394"
REFERENCES,0.8095238095238095,"arXiv:2112.04702, 2021.
395"
REFERENCES,0.8111658456486043,"[35] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-
396"
REFERENCES,0.812807881773399,"adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern
397"
REFERENCES,0.8144499178981938,"recognition, pages 2337‚Äì2346, 2019.
398"
REFERENCES,0.8160919540229885,"[36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
399"
REFERENCES,0.8177339901477833,"models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
400"
REFERENCES,0.819376026272578,"[37] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2.
401"
REFERENCES,0.8210180623973727,"Advances in neural information processing systems, 32, 2019.
402"
REFERENCES,0.8226600985221675,"[38] Xuanchi Ren and Xiaolong Wang. Look outside the room: Synthesizing a consistent long-term 3d scene
403"
REFERENCES,0.8243021346469622,"video from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
404"
REFERENCES,0.825944170771757,"Recognition (CVPR), 2022.
405"
REFERENCES,0.8275862068965517,"[39] Chris Rockwell, David F Fouhey, and Justin Johnson. Pixelsynth: Generating a 3d-consistent experience
406"
REFERENCES,0.8292282430213465,"from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
407"
REFERENCES,0.8308702791461412,"pages 14104‚Äì14113, 2021.
408"
REFERENCES,0.8325123152709359,"[40] Robin Rombach, Patrick Esser, and Bj√∂rn Ommer. Geometry-free view synthesis: Transformers and
409"
REFERENCES,0.8341543513957307,"no 3d priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
410"
REFERENCES,0.8357963875205254,"14356‚Äì14366, 2021.
411"
REFERENCES,0.8374384236453202,"[41] Axel Sauer, Kashyap Chitta, Jens M√ºller, and Andreas Geiger. Projected gans converge faster. Advances in
412"
REFERENCES,0.8390804597701149,"Neural Information Processing Systems, 34, 2021.
413"
REFERENCES,0.8407224958949097,"[42] Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. A comparison and
414"
REFERENCES,0.8423645320197044,"evaluation of multi-view stereo reconstruction algorithms. In 2006 IEEE computer society conference on
415"
REFERENCES,0.8440065681444991,"computer vision and pattern recognition (CVPR‚Äô06), volume 1, pages 519‚Äì528. IEEE, 2006.
416"
REFERENCES,0.8456486042692939,"[43] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie√üner, Gordon Wetzstein, and Michael Zollhofer.
417"
REFERENCES,0.8472906403940886,"Deepvoxels: Learning persistent 3d feature embeddings. In Proceedings of the IEEE/CVF Conference on
418"
REFERENCES,0.8489326765188834,"Computer Vision and Pattern Recognition, pages 2437‚Äì2446, 2019.
419"
REFERENCES,0.8505747126436781,"[44] Pratul P Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi Ramamoorthi, and Ren Ng. Learning to
420"
REFERENCES,0.8522167487684729,"synthesize a 4d rgbd light field from a single image. In Proceedings of the IEEE International Conference
421"
REFERENCES,0.8538587848932676,"on Computer Vision, pages 2243‚Äì2251, 2017.
422"
REFERENCES,0.8555008210180624,"[45] Pratul P Srinivasan, Richard Tucker, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng, and Noah Snavely.
423"
REFERENCES,0.8571428571428571,"Pushing the boundaries of view extrapolation with multiplane images. In Proceedings of the IEEE/CVF
424"
REFERENCES,0.8587848932676518,"Conference on Computer Vision and Pattern Recognition, pages 175‚Äì184, 2019.
425"
REFERENCES,0.8604269293924466,"[46] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox. Multi-view 3d models from single images
426"
REFERENCES,0.8620689655172413,"with a convolutional network. In European Conference on Computer Vision, pages 322‚Äì337. Springer,
427"
REFERENCES,0.8637110016420362,"2016.
428"
REFERENCES,0.8653530377668309,"[47] Shubham Tulsiani, Richard Tucker, and Noah Snavely. Layer-structured 3d scene inference via view
429"
REFERENCES,0.8669950738916257,"synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), pages 302‚Äì317, 2018.
430"
REFERENCES,0.8686371100164204,"[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
431"
REFERENCES,0.8702791461412152,"Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,
432"
REFERENCES,0.8719211822660099,"30, 2017.
433"
REFERENCES,0.8735632183908046,"[49] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron,
434"
REFERENCES,0.8752052545155994,"Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based
435"
REFERENCES,0.8768472906403941,"rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
436"
REFERENCES,0.8784893267651889,"pages 4690‚Äì4699, 2021.
437"
REFERENCES,0.8801313628899836,"[50] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis
438"
REFERENCES,0.8817733990147784,"from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
439"
REFERENCES,0.8834154351395731,"Recognition, pages 7467‚Äì7477, 2020.
440"
REFERENCES,0.8850574712643678,"[51] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:
441"
REFERENCES,0.8866995073891626,"Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information
442"
REFERENCES,0.8883415435139573,"Processing Systems, 34, 2021.
443"
REFERENCES,0.8899835796387521,"[52] Saining Xie, Sainan Liu, Zeyu Chen, and Zhuowen Tu. Attentional shapecontextnet for point cloud
444"
REFERENCES,0.8916256157635468,"recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
445"
REFERENCES,0.8932676518883416,"4606‚Äì4615, 2018.
446"
REFERENCES,0.8949096880131363,"[53] Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, and Qi Tian. Modeling
447"
REFERENCES,0.896551724137931,"point clouds with self-attention and gumbel subset sampling. In Proceedings of the IEEE/CVF Conference
448"
REFERENCES,0.8981937602627258,"on Computer Vision and Pattern Recognition, pages 3323‚Äì3332, 2019.
449"
REFERENCES,0.8998357963875205,"[54] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or
450"
REFERENCES,0.9014778325123153,"few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
451"
REFERENCES,0.90311986863711,"pages 4578‚Äì4587, 2021.
452"
REFERENCES,0.9047619047619048,"[55] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural
453"
REFERENCES,0.9064039408866995,"radiance fields. arXiv preprint arXiv:2010.07492, 2020.
454"
REFERENCES,0.9080459770114943,"[56] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In
455"
REFERENCES,0.909688013136289,"Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16259‚Äì16268, 2021.
456"
REFERENCES,0.9113300492610837,"[57] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A Efros. View synthesis by
457"
REFERENCES,0.9129720853858785,"appearance flow. In European conference on computer vision, pages 286‚Äì301. Springer, 2016.
458"
REFERENCES,0.9146141215106732,"[58] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification:
459"
REFERENCES,0.916256157635468,"Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018.
460"
REFERENCES,0.9178981937602627,"[59] C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele, Simon Winder, and Richard Szeliski. High-
461"
REFERENCES,0.9195402298850575,"quality video view interpolation using a layered representation. ACM transactions on graphics (TOG), 23
462"
REFERENCES,0.9211822660098522,"(3):600‚Äì608, 2004.
463"
REFERENCES,0.922824302134647,"Checklist
464"
REFERENCES,0.9244663382594417,"The checklist follows the references. Please read the checklist guidelines carefully for information on how to
465"
REFERENCES,0.9261083743842364,"answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or [N/A] . You are
466"
REFERENCES,0.9277504105090312,"strongly encouraged to include a justification to your answer, either by referencing the appropriate section of
467"
REFERENCES,0.9293924466338259,"your paper or providing a brief inline description. For example:
468"
REFERENCES,0.9310344827586207,"‚Ä¢ Did you include the license to the code and datasets? [Yes]
469"
REFERENCES,0.9326765188834154,"‚Ä¢ Did you include the license to the code and datasets? [No] The code and the data are proprietary.
470"
REFERENCES,0.9343185550082101,"‚Ä¢ Did you include the license to the code and datasets? [N/A] ?
471"
REFERENCES,0.9359605911330049,"Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist
472"
REFERENCES,0.9376026272577996,"section does not count towards the page limit. In your paper, please delete this instructions block and only keep
473"
REFERENCES,0.9392446633825944,"the Checklist section heading above along with the questions/answers below.
474"
REFERENCES,0.9408866995073891,"1. For all authors...
475"
REFERENCES,0.9425287356321839,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper‚Äôs contribu-
476"
REFERENCES,0.9441707717569786,"tions and scope? [Yes]
477"
REFERENCES,0.9458128078817734,"(b) Did you describe the limitations of your work? [Yes] We deal with our limitations in conclusion.
478"
REFERENCES,0.9474548440065681,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] Potential privacy
479"
REFERENCES,0.9490968801313628,"concern is discussed in conclusion.
480"
REFERENCES,0.9507389162561576,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
481"
REFERENCES,0.9523809523809523,"2. If you are including theoretical results...
482"
REFERENCES,0.9540229885057471,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
483"
REFERENCES,0.9556650246305419,"(b) Did you include complete proofs of all theoretical results? [N/A]
484"
REFERENCES,0.9573070607553367,"3. If you ran experiments...
485"
REFERENCES,0.9589490968801314,"(a) Did you include the code, data, and instructions needed to reproduce the main experimental
486"
REFERENCES,0.9605911330049262,"results (either in the supplemental material or as a URL)? [Yes]
487"
REFERENCES,0.9622331691297209,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
488"
REFERENCES,0.9638752052545156,"[Yes] In section 4 and a supplementary material, training details are presented.
489"
REFERENCES,0.9655172413793104,"(c) Did you report error bars (e.g., with respect to the random seed after running experiments
490"
REFERENCES,0.9671592775041051,"multiple times)? [No] We observe stable performance from ablation study.
491"
REFERENCES,0.9688013136288999,"(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs,
492"
REFERENCES,0.9704433497536946,"internal cluster, or cloud provider)? [Yes] As mention in section 4, we use 4 RTX 3090 GPUs.
493"
REFERENCES,0.9720853858784894,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
494"
REFERENCES,0.9737274220032841,"(a) If your work uses existing assets, did you cite the creators? [Yes] Please see supplementary
495"
REFERENCES,0.9753694581280788,"material.
496"
REFERENCES,0.9770114942528736,"(b) Did you mention the license of the assets? [Yes] We use publicly available data for experiments.
497"
REFERENCES,0.9786535303776683,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] In the
498"
REFERENCES,0.9802955665024631,"supplementary material, we provide the code.
499"
REFERENCES,0.9819376026272578,"(d) Did you discuss whether and how consent was obtained from people whose data you‚Äôre us-
500"
REFERENCES,0.9835796387520526,"ing/curating? [N/A]
501"
REFERENCES,0.9852216748768473,"(e) Did you discuss whether the data you are using/curating contains personally identifiable informa-
502"
REFERENCES,0.986863711001642,"tion or offensive content? [N/A]
503"
REFERENCES,0.9885057471264368,"5. If you used crowdsourcing or conducted research with human subjects...
504"
REFERENCES,0.9901477832512315,"(a) Did you include the full text of instructions given to participants and screenshots, if applicable?
505"
REFERENCES,0.9917898193760263,"[N/A]
506"
REFERENCES,0.993431855500821,"(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB)
507"
REFERENCES,0.9950738916256158,"approvals, if applicable? [N/A]
508"
REFERENCES,0.9967159277504105,"(c) Did you include the estimated hourly wage paid to participants and the total amount spent on
509"
REFERENCES,0.9983579638752053,"participant compensation? [N/A]
510"
