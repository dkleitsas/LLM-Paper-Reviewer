Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018315018315018315,"In this paper, Spectral Bridges, a novel clustering algorithm, is introduced. This algorithm
builds upon the traditional k-means and spectral clustering frameworks by subdividing data
into small VoronoÃ¯ regions, which are subsequently merged according to a connectivity measure.
Drawing inspiration from Support Vector Machineâ€™s margin concept, a non-parametric clustering
approach is proposed, building an affinity margin between each pair of VoronoÃ¯ regions. This
approach is characterized by minimal hyperparameters and delineation of intricate, non-convex
cluster structures.
The numerical experiments underscore Spectral Bridges as a fast, robust, and versatile tool
for sophisticated clustering tasks spanning diverse domains. Its efficacy extends to large-scale
scenarios encompassing both real-world and synthetic datasets.
The Spectral Bridge algorithm is implemented both in Python (https://pypi.org/project/spect
ral-bridges) and R https://github.com/cambroise/spectral-bridges-Rpackage)."
ABSTRACT,0.003663003663003663,"Keywords: spectral clustering, vector quantization, scalable, non-parametric"
OTHER,0.005494505494505495,"Contents
1"
OTHER,0.007326007326007326,"1
Introduction
2
2"
OTHER,0.009157509157509158,"2
Related Work
3
3"
OTHER,0.01098901098901099,"3
Spectral Bridges
3
4"
OTHER,0.01282051282051282,"3.1
Bridge affinity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
5"
OTHER,0.014652014652014652,"3.2
Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
6"
OTHER,0.016483516483516484,1Corresponding author: christophe.ambroise@univ-evry.fr
OTHER,0.018315018315018316,"4
Numerical experiments
7
7"
OTHER,0.020146520146520148,"4.1
Datasets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
8"
OTHER,0.02197802197802198,"4.1.1
Real-world data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
9"
OTHER,0.023809523809523808,"4.1.2
Synthetic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
10"
OTHER,0.02564102564102564,"4.1.3
Datasets Summary & Class Balance . . . . . . . . . . . . . . . . . . . . . . .
8
11"
OTHER,0.027472527472527472,"4.2
Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
12"
OTHER,0.029304029304029304,"4.3
Platform
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
13"
OTHER,0.031135531135531136,"4.4
Hyperparameter settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
14"
OTHER,0.03296703296703297,"4.5
Time complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
15"
OTHER,0.0347985347985348,"4.6
Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
16"
OTHER,0.03663003663003663,"4.7
Noise robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
17"
OTHER,0.038461538461538464,"4.8
Hyperparameter values effect on accuracy
. . . . . . . . . . . . . . . . . . . . . . .
13
18"
OTHER,0.040293040293040296,"5
Conclusive remarks
14
19"
OTHER,0.04212454212454213,"6
Appendix
14
20"
OTHER,0.04395604395604396,"6.1
Derivation of the bridge affinity . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
21"
OTHER,0.045787545787545784,"6.2
Code
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
22"
OTHER,0.047619047619047616,"6.2.1
Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
23"
OTHER,0.04945054945054945,"6.2.2
Affinity matrix computation . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
24"
OTHER,0.05128205128205128,"References
16
25"
OTHER,0.05311355311355311,"Session information
17
26"
INTRODUCTION,0.054945054945054944,"1
Introduction
27"
INTRODUCTION,0.056776556776556776,"Clustering is a fundamental technique for exploratory data analysis, organizing a set of objects into
28"
INTRODUCTION,0.05860805860805861,"distinct homogeneous groups known as clusters. It is extensively utilized across various fields, such
29"
INTRODUCTION,0.06043956043956044,"as biology for gene expression analysis (Eisen et al. 1998), social sciences for community detection in
30"
INTRODUCTION,0.06227106227106227,"social networks (Latouche, BirmelÃ©, and Ambroise 2011), and psychology for identifying behavioral
31"
INTRODUCTION,0.0641025641025641,"patterns. Clustering is often employed alongside supervised learning as a pre-processing step, helping
32"
INTRODUCTION,0.06593406593406594,"to structure and simplify data, thus enhancing the performance and interpretability of subsequent
33"
INTRODUCTION,0.06776556776556776,"predictive models (Verhaak et al. 2010). Additionally, clustering can be integrated into supervised
34"
INTRODUCTION,0.0695970695970696,"learning algorithms, such as mixture of experts (Jacobs et al. 1991), as part of a multi-objective
35"
INTRODUCTION,0.07142857142857142,"strategy.
36"
INTRODUCTION,0.07326007326007326,"There are various approaches to clustering, and the quality of the results is largely determined by
37"
INTRODUCTION,0.07509157509157509,"how the similarity between objects is defined, either through a similarity measure or a distance
38"
INTRODUCTION,0.07692307692307693,"metric. Clustering techniques originate from diverse fields of research, such as genetics, psychometry,
39"
INTRODUCTION,0.07875457875457875,"statistics, and computer science. Some methods are entirely heuristic, while others aim to optimize
40"
INTRODUCTION,0.08058608058608059,"specific criteria and can be related to statistical models.
41"
INTRODUCTION,0.08241758241758242,"Density-based methods identify regions within the data with a high concentration of points, corre-
42"
INTRODUCTION,0.08424908424908426,"sponding to the modes of the joint density. A notable non-parametric example of this approach is
43"
INTRODUCTION,0.08608058608058608,"DBSCAN (Ester et al. 1996). In contrast, model-based clustering, such as Gaussian mixture models,
44"
INTRODUCTION,0.08791208791208792,"represents a parametric approach to density-based methods. Model-based clustering assumes that
45"
INTRODUCTION,0.08974358974358974,"the data is generated from a mixture of underlying probability distributions, typically Gaussian
46"
INTRODUCTION,0.09157509157509157,"distributions. Each cluster is viewed as a component of this mixture model, and the Expectation-
47"
INTRODUCTION,0.09340659340659341,"Maximization (EM) algorithm is often used to estimate the parameters. This approach provides a
48"
INTRODUCTION,0.09523809523809523,"probabilistic framework for clustering, allowing for the incorporation of prior knowledge and the
49"
INTRODUCTION,0.09706959706959707,"ability to handle more complex cluster shapes and distributions (McLachlan and Peel 2000).
50"
INTRODUCTION,0.0989010989010989,"Geometric approaches, such as k-means (MacQueen et al. 1967), are distance-based methods that aim
51"
INTRODUCTION,0.10073260073260074,"to partition data by optimizing a criterion reflecting group homogeneity. The k-means++ algorithm
52"
INTRODUCTION,0.10256410256410256,"(Arthur and Vassilvitskii 2006) enhances this approach by providing faster and more reliable results.
53"
INTRODUCTION,0.1043956043956044,"However, a key limitation of these methods is the assumption of linear boundaries between clusters,
54"
INTRODUCTION,0.10622710622710622,"implying that clusters are convex. To address non-convex clusters, the kernel trick can be applied,
55"
INTRODUCTION,0.10805860805860806,"allowing for a more flexible k-means algorithm. This approach is comparable to spectral clustering in
56"
INTRODUCTION,0.10989010989010989,"handling complex cluster boundaries (Dhillon, Guan, and Kulis 2004). The k-means algorithm can also
57"
INTRODUCTION,0.11172161172161173,"be interpreted within the framework of model-based clustering under specific assumptions (Govaert
58"
INTRODUCTION,0.11355311355311355,"and Nadif 2003), revealing that it is essentially a special case of the more general Gaussian mixture
59"
INTRODUCTION,0.11538461538461539,"models, where clusters are assumed to be spherical Gaussian distributions with equal variance.
60"
INTRODUCTION,0.11721611721611722,"Graph-based methods represent data as a graph, with vertices symbolizing data points and edges
61"
INTRODUCTION,0.11904761904761904,"weighted to indicate the affinity between these points. Spectral clustering can be seen as a relaxed
62"
INTRODUCTION,0.12087912087912088,"version of the graph cut algorithm (Shi and Malik 2000). However, traditional spectral clustering faces
63"
INTRODUCTION,0.1227106227106227,"significant limitations due to its high time and space complexity, greatly hindering its applicability
64"
INTRODUCTION,0.12454212454212454,"to large-scale problems (Von Luxburg 2007).
65"
INTRODUCTION,0.12637362637362637,"The method we propose aims to find non-convex clusters in large datasets, without relying on a
66"
INTRODUCTION,0.1282051282051282,"parametric model, by using spectral clustering based on an affinity that characterizes the local density
67"
INTRODUCTION,0.13003663003663005,"of the data. The algorithm described in this paper draws from numerous clustering approaches. The
68"
INTRODUCTION,0.13186813186813187,"initial intuition is to detect high-density areas. To this end, vector quantization is used to divide the
69"
INTRODUCTION,0.1336996336996337,"space into a VoronoÃ¯ tessellation. An original geometric criterion is then employed to detect pairs
70"
INTRODUCTION,0.13553113553113552,"of VoronoÃ¯ regions that are either distant from each other or separated by a low-density boundary.
71"
INTRODUCTION,0.13736263736263737,"Finally, this affinity measure is considered as the weight of an edge in a complete graph connecting
72"
INTRODUCTION,0.1391941391941392,"the centroids of the tessellation, and a spectral clustering algorithm is used to find a partition of this
73"
INTRODUCTION,0.14102564102564102,"graph. The only parameters of the algorithm are the number of VoronoÃ¯ Cells and the number of
74"
INTRODUCTION,0.14285714285714285,"clusters.
75"
INTRODUCTION,0.1446886446886447,"The paper begins with a section dedicated to presenting the context and related algorithms, followed
76"
INTRODUCTION,0.14652014652014653,"by a detailed description of the proposed algorithm. Experiments and comparisons with reference
77"
INTRODUCTION,0.14835164835164835,"algorithms are then conducted on both real and synthetic data.
78"
LIT REVIEW,0.15018315018315018,"2
Related Work
79"
LIT REVIEW,0.152014652014652,"Spectral clustering is a graph-based approach that computes the eigen-vectors of the graphâ€™s Laplacian
80"
LIT REVIEW,0.15384615384615385,"matrix. This technique transforms the data into a lower-dimensional space, making the clusters
81"
LIT REVIEW,0.15567765567765568,"more discernible. A standard algorithm like k-means is then applied to these transformed features
82"
LIT REVIEW,0.1575091575091575,"to identify the clusters (Von Luxburg 2007). Spectral clustering enables capturing complex data
83"
LIT REVIEW,0.15934065934065933,"structures and discerning clusters based on the connectivity of data points in a transformed space,
84"
LIT REVIEW,0.16117216117216118,"effectively treating it as a relaxed graph cut problem.
85"
LIT REVIEW,0.163003663003663,"Classical spectral clustering involves two phases: construction of the affinity matrix and eigen-
86"
LIT REVIEW,0.16483516483516483,"decomposition. Constructing the affinity matrix requires ğ‘‚(ğ‘›2ğ‘‘) time and ğ‘‚(ğ‘›2) memory, while
87"
LIT REVIEW,0.16666666666666666,"eigen-decomposition demands ğ‘‚(ğ‘›3) time and ğ‘‚(ğ‘›2) memory, where ğ‘›is the data size and ğ‘‘is the
88"
LIT REVIEW,0.1684981684981685,"dimension. As ğ‘›increases, the computational load escalates significantly (Von Luxburg 2007).
89"
LIT REVIEW,0.17032967032967034,"To mitigate this computational burden, one common approach is to sparsify the affinity matrix and
90"
LIT REVIEW,0.17216117216117216,"use sparse eigen-solvers, reducing memory costs but still requiring computation of all original matrix
91"
LIT REVIEW,0.17399267399267399,"entries (Von Luxburg 2007). Another strategy is sub-matrix construction. The NystrÃ¶m method
92"
LIT REVIEW,0.17582417582417584,"randomly selects ğ‘šrepresentatives from the dataset to form an ğ‘›Ã— ğ‘šaffinity sub-matrix (Chen et
93"
LIT REVIEW,0.17765567765567766,"al. 2010). Cai et al. extended this with the landmark-based spectral clustering method, which uses
94"
LIT REVIEW,0.1794871794871795,"k-means to determine ğ‘šcluster centers as representatives (Cai and Chen 2014). Ultra-scalable spectral
95"
LIT REVIEW,0.1813186813186813,"clustering (U-SPEC) employs a hybrid representative selection strategy and a fast approximation
96"
LIT REVIEW,0.18315018315018314,"method for constructing a sparse affinity sub-matrix (Huang et al. 2019).
97"
LIT REVIEW,0.184981684981685,"Other approaches use the properties of the small initial clusters for the affinity computation. Cluster-
98"
LIT REVIEW,0.18681318681318682,"ing Based on Graph of Intensity Topology (GIT) estimates for example a global topological graph
99"
LIT REVIEW,0.18864468864468864,"(topo-graph) between local clusters (Gao et al. 2021). It then uses the Wasserstein Distance between
100"
LIT REVIEW,0.19047619047619047,"predicted and prior class proportions to automatically cut noisy edges in the topo-graph and merge
101"
LIT REVIEW,0.19230769230769232,"connected local clusters into final clusters.
102"
LIT REVIEW,0.19413919413919414,"The issue of characterizing the affinity between two clusters to create an edge weight is central to
103"
LIT REVIEW,0.19597069597069597,"the efficiency of a spectral clustering algorithm operating from a submatrix.
104"
LIT REVIEW,0.1978021978021978,"Notice that the clustering robustness of many Spectral clustering algorithms heavily relies on the
105"
LIT REVIEW,0.19963369963369965,"proper selection of kernel parameter, which is difficult to find without prior knowledge (Ng, Jordan,
106"
LIT REVIEW,0.20146520146520147,"and Weiss 2001).
107"
IMPLEMENTATION/METHODS,0.2032967032967033,"3
Spectral Bridges
108"
IMPLEMENTATION/METHODS,0.20512820512820512,"The proposed algorithm uses k-means centroids for vector quantization defining VoronoÃ¯ region, and
109"
IMPLEMENTATION/METHODS,0.20695970695970695,"a strategy is proposed to link these regions, with an â€œaffinityâ€ gauged in terms of minimal margin
110"
IMPLEMENTATION/METHODS,0.2087912087912088,"between pairs of classes. These affinities are considered as weight of edges defining a completely
111"
IMPLEMENTATION/METHODS,0.21062271062271062,"connected graph whose vertices are the regions. Spectral clustering on the region provide a partition
112"
IMPLEMENTATION/METHODS,0.21245421245421245,"of the input space. The sole parameters of the algorithm are the number of VoronoÃ¯ region and the
113"
IMPLEMENTATION/METHODS,0.21428571428571427,"number of final cluster.
114"
IMPLEMENTATION/METHODS,0.21611721611721613,"3.1
Bridge affinity
115"
IMPLEMENTATION/METHODS,0.21794871794871795,"The basic idea involves calculating the difference in inertia achieved by projecting onto a segment
116"
IMPLEMENTATION/METHODS,0.21978021978021978,"connecting two centroids, rather than using the two centroids separately (see Figure 1). If the
117"
IMPLEMENTATION/METHODS,0.2216117216117216,"difference is small, it suggests a low density between the classes. Conversely, if this diffrence is large,
118"
IMPLEMENTATION/METHODS,0.22344322344322345,"it indicates that the two classes may reside within the same densely populated region.
119"
IMPLEMENTATION/METHODS,0.22527472527472528,"Let us consider a sample ğ‘‹= (ğ‘¥ğ‘–)ğ‘–âˆˆ{1,â‹¯,ğ‘›} of vectors ğ‘¥ğ‘–âˆˆâ„ğ‘‘and a set of ğ‘šcoding vectors (ğœ‡ğ‘˜)ğ‘˜âˆˆ{1,â‹¯,ğ‘š}
120"
IMPLEMENTATION/METHODS,0.2271062271062271,"defining a partition ğ‘ƒ= {ğ’±1, â‹¯, ğ’±ğ‘š} of â„ğ‘‘into ğ‘šVoronoÃ¯ regions:
121"
IMPLEMENTATION/METHODS,0.22893772893772893,ğ’±ğ‘˜= {x âˆˆâ„ğ‘‘âˆ£â€–x âˆ’ğœ‡ğ‘˜â€– â‰¤â€–x âˆ’ğœ‡ğ‘—â€– for all ğ‘—â‰ ğ‘˜} .
IMPLEMENTATION/METHODS,0.23076923076923078,"In the following a ball denotes the subset of ğ‘‹in a VoronoÃ¯ region. The inertia of two balls ğ’±ğ‘˜and
122"
IMPLEMENTATION/METHODS,0.2326007326007326,"ğ’±ğ‘™is
123"
IMPLEMENTATION/METHODS,0.23443223443223443,ğ¼ğ‘˜ğ‘™= âˆ‘
IMPLEMENTATION/METHODS,0.23626373626373626,"ğ‘¥ğ‘–âˆˆğ’±ğ‘˜
â€–ğ‘¥ğ‘–âˆ’ğœ‡ğ‘˜â€–2 + âˆ‘"
IMPLEMENTATION/METHODS,0.23809523809523808,"ğ‘¥ğ‘–âˆˆğ’±ğ‘™
â€–ğ‘¥ğ‘–âˆ’ğœ‡ğ‘™â€–2."
IMPLEMENTATION/METHODS,0.23992673992673993,"We define a bridge as a structure defined by a segment connecting two centroids ğœ‡ğ‘˜and ğœ‡ğ‘™. The
124"
IMPLEMENTATION/METHODS,0.24175824175824176,"inertia of a bridge between ğ’±ğ‘˜and ğ’±ğ‘™is defined as
125"
IMPLEMENTATION/METHODS,0.24358974358974358,"ğµğ‘˜ğ‘™=
âˆ‘
ğ‘¥ğ‘–âˆˆğ’±ğ‘˜âˆªğ’±ğ‘™
â€–ğ‘¥ğ‘–âˆ’ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–)â€–2,"
IMPLEMENTATION/METHODS,0.2454212454212454,"where
126"
IMPLEMENTATION/METHODS,0.24725274725274726,"ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–) = ğœ‡ğ‘˜+ ğ‘¡ğ‘–(ğœ‡ğ‘™âˆ’ğœ‡ğ‘˜),"
IMPLEMENTATION/METHODS,0.2490842490842491,"with
127"
IMPLEMENTATION/METHODS,0.2509157509157509,"ğ‘¡ğ‘–= min (1, max (0, âŸ¨ğ‘¥ğ‘–âˆ’ğœ‡ğ‘˜|ğœ‡ğ‘™âˆ’ğœ‡ğ‘˜âŸ©"
IMPLEMENTATION/METHODS,0.25274725274725274,"â€–ğœ‡ğ‘™âˆ’ğœ‡ğ‘˜â€–2
)) ."
IMPLEMENTATION/METHODS,0.25457875457875456,"Figure 1: Balls (left) versus Bridge (right). The inertia of each structure is the sum of the squared
distances represented by grey lines."
IMPLEMENTATION/METHODS,0.2564102564102564,"Considering two centroÃ¯ds, the normalized average of the difference betweenn Bridge and balls
128"
IMPLEMENTATION/METHODS,0.25824175824175827,"inertia (See Appendix) constitutes the basis of our affinity measure between two regions:
129"
IMPLEMENTATION/METHODS,0.2600732600732601,"ğµğ‘˜ğ‘™âˆ’ğ¼ğ‘˜ğ‘™
(ğ‘›ğ‘˜+ ğ‘›ğ‘™)â€–ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™â€–2 =
âˆ‘ğ‘¥ğ‘–âˆˆğ’±ğ‘˜âŸ¨ğ‘¥ğ‘–âˆ’ğœ‡ğ‘˜|ğœ‡ğ‘™âˆ’ğœ‡ğ‘˜âŸ©2+ âˆ‘ğ‘¥ğ‘–âˆˆğ’±ğ‘™âŸ¨ğ‘¥ğ‘–âˆ’ğœ‡ğ‘™|ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™âŸ©2+"
IMPLEMENTATION/METHODS,0.2619047619047619,"(ğ‘›ğ‘˜+ ğ‘›ğ‘™)â€–ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™â€–4
,"
IMPLEMENTATION/METHODS,0.26373626373626374,"=
âˆ‘ğ‘¥ğ‘–âˆˆğ’±ğ‘˜âˆªğ’±ğ‘™ğ›¼2
ğ‘–
ğ‘›ğ‘˜+ ğ‘›ğ‘™
,"
IMPLEMENTATION/METHODS,0.26556776556776557,"where
130"
IMPLEMENTATION/METHODS,0.2673992673992674,"ğ›¼ğ‘–= {ğ‘¡ğ‘–,
if ğ‘¡ğ‘–âˆˆ[0, 1/2],
1 âˆ’ğ‘¡ğ‘–,
if ğ‘¡ğ‘–âˆˆ]1/2, 1]."
IMPLEMENTATION/METHODS,0.2692307692307692,"The basic intuition behind this affinity is that ğ‘¡ğ‘–represents the relative position of the projection of ğ‘¥ğ‘–
131"
IMPLEMENTATION/METHODS,0.27106227106227104,"on the segment [ğœ‡ğ‘˜, ğœ‡ğ‘™]. ğ›¼ğ‘–represents the relative position on the segment, with the centroid of the
132"
IMPLEMENTATION/METHODS,0.27289377289377287,"class to which ğ‘¥ğ‘–belongs as the reference point.
133"
IMPLEMENTATION/METHODS,0.27472527472527475,"The boundary that separates the two clusters defined by centroids ğœ‡ğ‘˜and ğœ‡ğ‘™is a hyperplane. This
134"
IMPLEMENTATION/METHODS,0.2765567765567766,"hyperplane is orthogonal to the line segment connecting the centroids and intersects this segment at
135"
IMPLEMENTATION/METHODS,0.2783882783882784,"its midpoint.
136"
IMPLEMENTATION/METHODS,0.2802197802197802,"If we consider all points ğ‘¥ğ‘–âˆˆğ’±ğ‘˜âˆªğ’±ğ‘™which are not projected on centroids but somewhere on the
137"
IMPLEMENTATION/METHODS,0.28205128205128205,"segment, the distance from a point to the hyperplane is
138"
IMPLEMENTATION/METHODS,0.2838827838827839,â€–ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–) âˆ’ğœ‡ğ‘˜ğ‘™â€– = (1/2 âˆ’ğ›¼ğ‘–)â€–ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™â€–.
IMPLEMENTATION/METHODS,0.2857142857142857,"This distance is similar to the concept of margin in Support Vector Machine (Cortes and Vapnik 1995).
139"
IMPLEMENTATION/METHODS,0.2875457875457875,"When the ğ›¼ğ‘–values are small (close to zero since ğ›¼ğ‘–âˆˆ[0, 1/2]), the margins to the hyperplane are
140"
IMPLEMENTATION/METHODS,0.2893772893772894,"large, indicating a low density between the classes. Conversely, if the margins are small, it suggests
141"
IMPLEMENTATION/METHODS,0.29120879120879123,"that the two classes may reside within the same densely populated region. Consequently, the sum of
142"
IMPLEMENTATION/METHODS,0.29304029304029305,"the ğ›¼ğ‘–or ğ›¼2
ğ‘–increases with the density of the region between the classes.
143"
IMPLEMENTATION/METHODS,0.2948717948717949,"Note that the criterion is local and indicates the relative difference in densities between the balls and
144"
IMPLEMENTATION/METHODS,0.2967032967032967,"the bridge, rather than evaluating a global score for the densities of the structures.
145"
IMPLEMENTATION/METHODS,0.29853479853479853,"Eventually, we define the bridge affinity between centroids ğ‘˜and ğ‘™as:
146"
IMPLEMENTATION/METHODS,0.30036630036630035,"ğ‘ğ‘˜ğ‘™= {
0,
if ğ‘˜= ğ‘™,
âˆ‘ğ‘¥ğ‘–âˆˆğ’±ğ‘˜âˆªğ’±ğ‘™ğ›¼2
ğ‘–
ğ‘›ğ‘˜+ğ‘›ğ‘™
,
otherwise."
IMPLEMENTATION/METHODS,0.3021978021978022,"To allow points with large margin to dominate and make the algorithm more robust to noise and
147"
IMPLEMENTATION/METHODS,0.304029304029304,"outliers we consider the following exponential transformation:
148"
IMPLEMENTATION/METHODS,0.3058608058608059,Ìƒğ‘ğ‘˜ğ‘™= ğ‘”(ğ‘ğ‘˜ğ‘™) = exp(ğ›¾âˆšğ‘ğ‘˜ğ‘™).
IMPLEMENTATION/METHODS,0.3076923076923077,"where ğ›¾is a scaling factor. This factor is set to ensure a large enough separation between the final
149"
IMPLEMENTATION/METHODS,0.30952380952380953,"coefficients. This factor is determined by the equation:
150"
IMPLEMENTATION/METHODS,0.31135531135531136,"ğ›¾=
ğ‘™ğ‘œğ‘”(ğ‘€)"
IMPLEMENTATION/METHODS,0.3131868131868132,âˆšğ‘90 âˆ’âˆšğ‘10
IMPLEMENTATION/METHODS,0.315018315018315,"where ğ‘10 and ğ‘90 are respectively the 10th and 90th percentiles of the original affinity matrix
151"
IMPLEMENTATION/METHODS,0.31684981684981683,"and ğ‘€> 1. Thus, since the transformation is order-preserving, the 90th percentile of the newly
152"
IMPLEMENTATION/METHODS,0.31868131868131866,"constructed matrix is ğ‘€times greater than the 10th percentile. By default, ğ‘€is arbitrarily set to a
153"
IMPLEMENTATION/METHODS,0.32051282051282054,"large value of 104.
154"
IMPLEMENTATION/METHODS,0.32234432234432236,"The inclusion of the square root can be understood as redefining the affinity measure. Instead of
155"
IMPLEMENTATION/METHODS,0.3241758241758242,"considering the variance and the squared Euclidean norm, we interpret the affinity as the ratio
156"
IMPLEMENTATION/METHODS,0.326007326007326,"between the standard deviation and the length of the segment connecting two centroids. This
157"
IMPLEMENTATION/METHODS,0.32783882783882784,"reinterpretation greatly enhances numerical stability, contributing to more reliable clustering results.
158"
IMPLEMENTATION/METHODS,0.32967032967032966,"3.2
Algorithm
159"
IMPLEMENTATION/METHODS,0.3315018315018315,"The Spectral Bridges algorithm first identifies local clusters to define VoronoÃ¯ regions, computes
160"
IMPLEMENTATION/METHODS,0.3333333333333333,"edges with affinity weights between these regions, and ultimately cuts edges between regions with
161"
IMPLEMENTATION/METHODS,0.33516483516483514,"low inter-region density to determine the final clusters (See Algorithm 1 and Figure 2).
162"
IMPLEMENTATION/METHODS,0.336996336996337,"In spectral clustering, the time complexity is usually dominated by the eigen-decomposition step,
163"
IMPLEMENTATION/METHODS,0.33882783882783885,"which is ğ‘‚(ğ‘›3). However, in the case of Spectral Bridges, the k-means algorithm has a time complexity
164"
IMPLEMENTATION/METHODS,0.34065934065934067,"of ğ‘‚(ğ‘›Ã—ğ‘šÃ—ğ‘‘). For datasets with large ğ‘›, this can be more significant than the ğ‘‚(ğ‘š3) time complexity
165"
IMPLEMENTATION/METHODS,0.3424908424908425,"of the Spectral Bridges eigen-decomposition. As for the affinity matrix construction, there are ğ‘š2
166"
IMPLEMENTATION/METHODS,0.3443223443223443,"coefficients to be calculated. Each ğ‘ğ‘˜ğ‘™coefficient requires the computation of ğ‘›ğ‘˜+ ğ‘›ğ‘™dot products as
167"
IMPLEMENTATION/METHODS,0.34615384615384615,"well as the norm â€–ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™â€–, the latter often being negligeable. Assuming that the VoronoÃ¯ regions are
168"
IMPLEMENTATION/METHODS,0.34798534798534797,"roughly balanced in cardinality, we have ğ‘›ğ‘˜â‰ˆğ‘›"
IMPLEMENTATION/METHODS,0.3498168498168498,"ğ‘š. Since ğ‘šshould always be less than ğ‘›, therefore
169"
IMPLEMENTATION/METHODS,0.3516483516483517,"ğ‘›
ğ‘š> 1 and the time complexity of the affinity matrix is ğ‘‚( ğ‘›"
IMPLEMENTATION/METHODS,0.3534798534798535,"ğ‘šÃ— ğ‘š2 Ã— ğ‘‘) = ğ‘‚(ğ‘›Ã— ğ‘šÃ— ğ‘‘) given the
170"
IMPLEMENTATION/METHODS,0.3553113553113553,"acceptable range of values for ğ‘š. Nonetheless, this is rarely the bottleneck.
171"
IMPLEMENTATION/METHODS,0.35714285714285715,Algorithm 1 Spectral Bridges
IMPLEMENTATION/METHODS,0.358974358974359,"1: procedure SpectralBridges(ğ‘‹, ğ‘˜, ğ‘š) â–·ğ‘‹: input dataset, ğ‘˜: number of clusters, ğ‘š: number of
VoronoÃ¯ regions"
IMPLEMENTATION/METHODS,0.3608058608058608,"2:
Step 1: Vector Quantization"
IMPLEMENTATION/METHODS,0.3626373626373626,"3:
centroids, voronoiRegions â†KMeans(ğ‘‹, ğ‘š)
â–·Initial centroids and Voronoi regions using
k-means++"
IMPLEMENTATION/METHODS,0.36446886446886445,"4:
Step 2: Affinity Computation"
IMPLEMENTATION/METHODS,0.3663003663003663,"5:
ğ´= {ğ‘”(ğ‘ğ‘˜ğ‘™)}ğ‘˜ğ‘™â†Affinity(ğ‘‹, centroids, voronoiRegions)
â–·Compute affinity matrix ğ´"
IMPLEMENTATION/METHODS,0.36813186813186816,"6:
Step 3: Spectral Clustering
â–·Assign each region to a cluster"
IMPLEMENTATION/METHODS,0.36996336996337,"7:
labels â†SpectralClustering(ğ´, ğ‘˜)"
IMPLEMENTATION/METHODS,0.3717948717948718,"8:
Step 4: Propagate
â–·Assign each data point to the cluster of its region"
IMPLEMENTATION/METHODS,0.37362637362637363,"9:
clusters â†Propagate(ğ‘‹, labels, voronoiRegions)"
IMPLEMENTATION/METHODS,0.37545787545787546,"10:
return clusters
â–·Return cluster labels for data points in ğ‘‹"
IMPLEMENTATION/METHODS,0.3772893772893773,11: end procedure
IMPLEMENTATION/METHODS,0.3791208791208791,"(a) Vector quantization
(b) Affinity computation
(c) Spectral clustering"
IMPLEMENTATION/METHODS,0.38095238095238093,"Figure 2: Illustration of the Spectral bridges algorithm with the Iris dataset (first principal plane).
Vector quantization (Step 1 of Algorithm 1 ), Affinity computation (Step 2 of Algorithm 1 ), Spectral
clustering and spreading (Step 3-4 of Algorithm 1 )."
RESULTS/EXPERIMENTS,0.38278388278388276,"4
Numerical experiments
172"
RESULTS/EXPERIMENTS,0.38461538461538464,"In this section, the results obtained from testing the Spectral Bridges algorithm on various datasets,
173"
RESULTS/EXPERIMENTS,0.38644688644688646,"both small and large scale, including real-world and well-known synthetic datasets, are presented.
174"
RESULTS/EXPERIMENTS,0.3882783882783883,"These experiments assess the accuracy, time and space complexity, ease of use, robustness, and adapt-
175"
RESULTS/EXPERIMENTS,0.3901098901098901,"ability of our algorithm. We compare Spectral Bridges (SB) against several state-of-the-art methods,
176"
RESULTS/EXPERIMENTS,0.39194139194139194,"including k-means++ (KM) (MacQueen et al. 1967; Arthur and Vassilvitskii 2006), Expectation-
177"
RESULTS/EXPERIMENTS,0.39377289377289376,"Maximization (EM) (Dempster, Laird, and Rubin 1977), Ward Clustering (WC) (Ward Jr 1963), and
178"
RESULTS/EXPERIMENTS,0.3956043956043956,"DBSCAN (DB) (Ester et al. 1996). This comparison establishes baselines across centroid-based
179"
RESULTS/EXPERIMENTS,0.3974358974358974,"clustering algorithms, hierarchical methods, and density-based methods.
180"
RESULTS/EXPERIMENTS,0.3992673992673993,"The algorithms are evaluated on both raw and PCA-processed data with varying dimensionality.
181"
RESULTS/EXPERIMENTS,0.4010989010989011,"For synthetic datasets, Gaussian and/or uniform noise is introduced to assess the robustness of the
182"
RESULTS/EXPERIMENTS,0.40293040293040294,"algorithm.
183"
RESULTS/EXPERIMENTS,0.40476190476190477,"4.1
Datasets
184"
RESULTS/EXPERIMENTS,0.4065934065934066,"4.1.1
Real-world data
185"
RESULTS/EXPERIMENTS,0.4084249084249084,"â€¢ MNIST: A large dataset containing 60,000 handwritten digit images in ten balanced classes,
186"
RESULTS/EXPERIMENTS,0.41025641025641024,"commonly used for image processing benchmarks. Each image consists of 28 Ã— 28 = 784 pixels.
187"
RESULTS/EXPERIMENTS,0.41208791208791207,"â€¢ UCI ML Breast Cancer Wisconsin: A dataset featuring computed attributes from digitized
188"
RESULTS/EXPERIMENTS,0.4139194139194139,"images of fine needle aspirates (FNA) of breast masses, used to predict whether a tumor is
189"
RESULTS/EXPERIMENTS,0.4157509157509158,"malignant or benign.
190"
RESULTS/EXPERIMENTS,0.4175824175824176,"4.1.2
Synthetic data
191"
RESULTS/EXPERIMENTS,0.4194139194139194,"â€¢ Impossible: A synthetic dataset designed to challenge clustering algorithms with complex
192"
RESULTS/EXPERIMENTS,0.42124542124542125,"patterns.
193"
RESULTS/EXPERIMENTS,0.4230769230769231,"â€¢ Moons: A two-dimensional dataset with two interleaving half-circles.
194"
RESULTS/EXPERIMENTS,0.4249084249084249,"â€¢ Circles: A synthetic dataset of points arranged in two non-linearly separable circles.
195"
RESULTS/EXPERIMENTS,0.4267399267399267,"â€¢ Smile: A synthetic dataset with points arranged in the shape of a smiling face, used to test the
196"
RESULTS/EXPERIMENTS,0.42857142857142855,"separation of non-linearly separable data.
197"
RESULTS/EXPERIMENTS,0.43040293040293043,"4.1.3
Datasets Summary & Class Balance
198"
RESULTS/EXPERIMENTS,0.43223443223443225,Table 1: Datasets Summary & Class Balance
RESULTS/EXPERIMENTS,0.4340659340659341,"Dataset
#Dims
#Samples
#Classes
Class Proportions"
RESULTS/EXPERIMENTS,0.4358974358974359,"MNIST
784
60000
10
9.9%, 11.2%, 9.9%, 10.3%, 9.7%, 9%, 9.9%,
10.4%, 9.7%, 9.9%
Breast Cancer
30
569
2
37.3%, 62.7%
Impossible
2
3594
7
24.8%, 18.8%, 11.3%, 7.5%, 12.5%, 12.5%,
12.5%
Moons
2
1000
2
50%, 50%
Circles
2
1000
2
50%, 50%
Smile
2
1000
4
25%, 25%, 25%, 25%"
RESULTS/EXPERIMENTS,0.43772893772893773,"Class proportions are presented in ascending order starting from label 0.
199"
RESULTS/EXPERIMENTS,0.43956043956043955,"4.2
Metrics
200"
RESULTS/EXPERIMENTS,0.4413919413919414,"To evaluate the performance of the clustering algorithm, the Adjusted Rand Index (ARI) (Halkidi,
201"
RESULTS/EXPERIMENTS,0.4432234432234432,"Batistakis, and Vazirgiannis 2002) and Normalized Mutual Information (NMI) (Cover and Thomas
202"
RESULTS/EXPERIMENTS,0.44505494505494503,"1991) are used. ARI measures the similarity between two clustering results, ranging from -0.5 to 1,
203"
RESULTS/EXPERIMENTS,0.4468864468864469,"with 1 indicating perfect agreement. NMI ranges from 0 to 1, with higher values indicating better
204"
RESULTS/EXPERIMENTS,0.44871794871794873,"clustering quality. In some tests, the variability of scores across multiple runs is also reported due to
205"
RESULTS/EXPERIMENTS,0.45054945054945056,"the random initialization in k-means, though k-means++ generally provides stable and reproducible
206"
RESULTS/EXPERIMENTS,0.4523809523809524,"results.
207"
RESULTS/EXPERIMENTS,0.4542124542124542,"4.3
Platform
208"
RESULTS/EXPERIMENTS,0.45604395604395603,"All experiments were conducted on an Archlinux machine with Linux 6.9.3 Kernel, 8GB of RAM, and
209"
RESULTS/EXPERIMENTS,0.45787545787545786,"an AMD Ryzen 3 7320U processor.
210"
RESULTS/EXPERIMENTS,0.4597069597069597,"4.4
Hyperparameter settings
211"
RESULTS/EXPERIMENTS,0.46153846153846156,"The hyperparameters of the Spectral Bridges algorithm were based on the size of each dataset, ğ‘›,
212"
RESULTS/EXPERIMENTS,0.4633699633699634,"and the number of clusters, ğ¾. A larger number of clusters typically suggests that a higher value
213"
RESULTS/EXPERIMENTS,0.4652014652014652,"for the number of VoronoÃ¯ regions is optimal. Conversely, using a high number of VoronoÃ¯ regions
214"
RESULTS/EXPERIMENTS,0.46703296703296704,"for a small dataset might result in nearly empty regions that do not adequately represent any local
215"
RESULTS/EXPERIMENTS,0.46886446886446886,"structure.
216"
RESULTS/EXPERIMENTS,0.4706959706959707,"A good yet not very precise way of setting the number of VoronoÃ¯ regions ğ‘šis to observe the Within
217"
RESULTS/EXPERIMENTS,0.4725274725274725,"Cluster Sum of Squares (WCSS) or inertia in a way akin to the elbow method. Since ğ‘šshould be set
218"
RESULTS/EXPERIMENTS,0.47435897435897434,"to a value strictly greater than ğ¾, we plot the WCSS for varying values of ğ‘š, and find a value such
219"
RESULTS/EXPERIMENTS,0.47619047619047616,"that the WCSS-ğ‘šrelationship becomes quasi-linear.
220"
RESULTS/EXPERIMENTS,0.47802197802197804,"By adjusting ğ‘šin this manner, we aim to balance the need for detailed representation with the
221"
RESULTS/EXPERIMENTS,0.47985347985347987,"risk of overfitting, ensuring that each VoronoÃ¯ region meaningfully captures the underlying data
222"
RESULTS/EXPERIMENTS,0.4816849816849817,"distribution. The sensitivity or lack thereof is illustrated later on by Figure 10.
223"
RESULTS/EXPERIMENTS,0.4835164835164835,"For other algorithms, such as DBSCAN, labels were used to determine the best hyperparameter
224"
RESULTS/EXPERIMENTS,0.48534798534798534,"values to compare our method against the â€œbest case scenarioâ€, thus putting the Spectral Bridges
225"
RESULTS/EXPERIMENTS,0.48717948717948717,"algorithm at a voluntary disadvantage.
226"
RESULTS/EXPERIMENTS,0.489010989010989,"4.5
Time complexity
227"
RESULTS/EXPERIMENTS,0.4908424908424908,"To assess the algorithmâ€™s time complexity, the average execution times over 50 runs were computed
228"
RESULTS/EXPERIMENTS,0.4926739926739927,"for varying numbers of VoronoÃ¯ regions ğ‘šas well as dataset sizes. With a constant number of clusters
229"
RESULTS/EXPERIMENTS,0.4945054945054945,"ğ¾= 5 and an embedding dimension of ğ‘‘= 10, the results (see Figure 3) highlight Spectral Bridges
230"
RESULTS/EXPERIMENTS,0.49633699633699635,"algorihtmâ€™s efficacy. As discussed previously, we observe a linear relationship between ğ‘šand the
231"
RESULTS/EXPERIMENTS,0.4981684981684982,"execution time because the matrix construction is highly optimized and the time taken is almost
232"
RESULTS/EXPERIMENTS,0.5,"negligeable compared to that of the initial k-means++ centroids initalization.
233"
RESULTS/EXPERIMENTS,0.5018315018315018,"0
20k
40k
60k
80k
100k
0 0.05 0.1 0.15 0.2"
RESULTS/EXPERIMENTS,0.5036630036630036,Dataset Size
RESULTS/EXPERIMENTS,0.5054945054945055,Average Time per Run (seconds)
RESULTS/EXPERIMENTS,0.5073260073260073,"(a) Varying ğ‘›, fixed ğ‘š= 10"
RESULTS/EXPERIMENTS,0.5091575091575091,"0
200
400
600
800
1000
0 0.2 0.4 0.6 0.8 1 m"
RESULTS/EXPERIMENTS,0.510989010989011,Average Time per Run (seconds)
RESULTS/EXPERIMENTS,0.5128205128205128,"(b) Varying ğ‘š, fixed ğ‘›= 5000"
RESULTS/EXPERIMENTS,0.5146520146520146,Figure 3: Average time taken per model fit.
RESULTS/EXPERIMENTS,0.5164835164835165,"4.6
Accuracy
234"
RESULTS/EXPERIMENTS,0.5183150183150184,"The algorithmâ€™s accuracy was first evaluated on the MNIST dataset. Metrics were collected to
235"
RESULTS/EXPERIMENTS,0.5201465201465202,"compare our method with k-means++, EM, and Ward clustering. Metric were estimated by taking
236"
RESULTS/EXPERIMENTS,0.521978021978022,"the empirical average over 10 consecutive runs with the same random seed for each method. Since
237"
RESULTS/EXPERIMENTS,0.5238095238095238,"our computational capabilites were too limited, a sample of 20,000 (one third) data points was chosen
238"
RESULTS/EXPERIMENTS,0.5256410256410257,"at random for each iteration.
239"
RESULTS/EXPERIMENTS,0.5274725274725275,"Let â„denote the embedding dimension of the dataset. Spectral Bridges was tested both on the raw
240"
RESULTS/EXPERIMENTS,0.5293040293040293,"MNIST dataset without preprocessing (â„= 784) and after reducing its dimension using PCA to
241"
RESULTS/EXPERIMENTS,0.5311355311355311,"â„âˆˆ{8, 16, 32, 64} (see Figure 4).
242 h=8 h=16 h=32 h=64"
RESULTS/EXPERIMENTS,0.532967032967033,h=784 (full) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 h=8 h=16 h=32 h=64
RESULTS/EXPERIMENTS,0.5347985347985348,h=784 (full) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
RESULTS/EXPERIMENTS,0.5366300366300366,"0.8
KM
EM
WC
SB"
RESULTS/EXPERIMENTS,0.5384615384615384,ARI Score
RESULTS/EXPERIMENTS,0.5402930402930403,NMI Score
RESULTS/EXPERIMENTS,0.5421245421245421,"Figure 4: ARI and NMI scores of k-means++ (blue), EM (green), Ward Clustering (red), and Spectral
Bridges (purple) on PCA embedding and full MNIST."
RESULTS/EXPERIMENTS,0.5439560439560439,"For visualization purposes, the predicted clusters by Spectral Bridges and k-means++ were projected
243"
RESULTS/EXPERIMENTS,0.5457875457875457,"using UMAP to compare them against the ground truth labels and to better understand the cluster
244"
RESULTS/EXPERIMENTS,0.5476190476190477,"shapes (see Figure 5). Note that the projection was not used in the experiments as an embedding, and
245"
RESULTS/EXPERIMENTS,0.5494505494505495,"thus does not play any role in the clustering process itself. As a matter of fact, the embedding used
246"
RESULTS/EXPERIMENTS,0.5512820512820513,"was obtained with PCA, â„= 32 and 250 VoronoÃ¯ regions. Note that the label colors match the legend
247"
RESULTS/EXPERIMENTS,0.5531135531135531,"only in the case of the ground truth data. Indeed, the ordering of the labels have no significance on
248"
RESULTS/EXPERIMENTS,0.554945054945055,"clustering quality.
249"
RESULTS/EXPERIMENTS,0.5567765567765568,"âˆ’5
0
5
10
15 âˆ’5 0 5 10 15"
RESULTS/EXPERIMENTS,0.5586080586080586,(a) k-means++
RESULTS/EXPERIMENTS,0.5604395604395604,"âˆ’5
0
5
10
15 âˆ’5 0 5 10 15"
RESULTS/EXPERIMENTS,0.5622710622710623,(b) Spectral Bridges
RESULTS/EXPERIMENTS,0.5641025641025641,"âˆ’5
0
5
10
15 âˆ’5 0 5 10 15 Label 0 1 2 3 4 5 6 7 8 9"
RESULTS/EXPERIMENTS,0.5659340659340659,(c) Ground Truth
RESULTS/EXPERIMENTS,0.5677655677655677,Figure 5: UMAP projection of predicted clusters against the ground truth labels.
RESULTS/EXPERIMENTS,0.5695970695970696,"The Spectral Bridges algorithm was also put to the test against the same competitors using scikit-
250"
RESULTS/EXPERIMENTS,0.5714285714285714,"learnâ€™s UCI Breast Cancer data. Once again, this new method performs well although the advantage
251"
RESULTS/EXPERIMENTS,0.5732600732600732,"is not as obvious in this case (see Figure 6). However, in none of our tests has it ranked worse than
252"
RESULTS/EXPERIMENTS,0.575091575091575,"k-means++. The results are displayed as a boxplot generated from 200 iterations of each algorithm
253"
RESULTS/EXPERIMENTS,0.5769230769230769,"using a different seed, in order to better grasp the variability lying in the seed dependent nature of
254"
RESULTS/EXPERIMENTS,0.5787545787545788,"the k-means++, Expectation Maximization and Spectral Bridges algorithms.
255"
RESULTS/EXPERIMENTS,0.5805860805860806,"Since the Spectral Bridges algorithm is expected to excel at discerning complex and intricate cluster
256"
RESULTS/EXPERIMENTS,0.5824175824175825,"KM
EM
WC
SB 0.3 0.4 0.5 0.6 0.7 0.8"
RESULTS/EXPERIMENTS,0.5842490842490843,"KM
EM
WC
SB
0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7"
RESULTS/EXPERIMENTS,0.5860805860805861,UCI Breast Cancer
RESULTS/EXPERIMENTS,0.5879120879120879,"ARI
NMI"
RESULTS/EXPERIMENTS,0.5897435897435898,"Figure 6: ARI and NMI scores of k-means++ (blue), EM (green), Ward Clustering (red), and Spectral
Bridges (purple) on the UCI Breast Cancer dataset."
RESULTS/EXPERIMENTS,0.5915750915750916,"structures, an array of four toy datasets was collected, as illustrated in Figure 7.
257"
RESULTS/EXPERIMENTS,0.5934065934065934,"âˆ’10
âˆ’5
0
5
10 âˆ’10 âˆ’5 0 5 10"
RESULTS/EXPERIMENTS,0.5952380952380952,(a) Impossible
RESULTS/EXPERIMENTS,0.5970695970695971,"âˆ’1
0
1
2 âˆ’0.5 0 0.5 1"
RESULTS/EXPERIMENTS,0.5989010989010989,(b) Moons
RESULTS/EXPERIMENTS,0.6007326007326007,"âˆ’1
âˆ’0.5
0
0.5
1 âˆ’1 âˆ’0.5 0 0.5 1"
RESULTS/EXPERIMENTS,0.6025641025641025,(c) Circles
RESULTS/EXPERIMENTS,0.6043956043956044,"âˆ’1
âˆ’0.8
âˆ’0.6
âˆ’0.4
âˆ’0.2
0 0 0.2 0.4 0.6 0.8 1"
RESULTS/EXPERIMENTS,0.6062271062271062,(d) Smile
RESULTS/EXPERIMENTS,0.608058608058608,Figure 7: Four toy datasets.
RESULTS/EXPERIMENTS,0.6098901098901099,"Multiple algorithms, including the proposed one, were benchmarked in the exact same manner
258"
RESULTS/EXPERIMENTS,0.6117216117216118,"as for the UCI Breast Cancer data. The results show that the proposed method outperforms all
259"
RESULTS/EXPERIMENTS,0.6135531135531136,"tested algorithms (DBSCAN, k-means++, Expectation Maximization, and Ward Clustering) while
260"
RESULTS/EXPERIMENTS,0.6153846153846154,"requiring few hyperparameters. As previously discussed, DBSCANâ€™s parameters were optimized
261"
RESULTS/EXPERIMENTS,0.6172161172161172,"using the ground truth labels to represent a best-case scenario; however, in practical applications,
262"
RESULTS/EXPERIMENTS,0.6190476190476191,"suboptimal performance is more likely. Despite this optimization, the Spectral-Bridge algorithm still
263"
RESULTS/EXPERIMENTS,0.6208791208791209,"demonstrates superior ability to capture and represent the underlying cluster structures.
264 0.5 0.6 0.7 0.8 0.9 1 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1"
RESULTS/EXPERIMENTS,0.6227106227106227,"DB
KM
EM
WC
SB 0.6 0.7 0.8 0.9 1"
RESULTS/EXPERIMENTS,0.6245421245421245,"DB
KM
EM
WC
SB
0.6 0.7 0.8 0.9 1"
RESULTS/EXPERIMENTS,0.6263736263736264,"Impossible
Moons
Circles
Smile"
RESULTS/EXPERIMENTS,0.6282051282051282,"ARI
NMI"
RESULTS/EXPERIMENTS,0.63003663003663,Figure 8: ARI and NMI scores of Spectral Bridges and competitors on standard synthetic toy datasets.
RESULTS/EXPERIMENTS,0.6318681318681318,"4.7
Noise robustness
265"
RESULTS/EXPERIMENTS,0.6336996336996337,"To evaluate the noise robustness of the algorithm, two experimental setups were devised: one involved
266"
RESULTS/EXPERIMENTS,0.6355311355311355,"introducing Gaussian-distributed perturbations to the data, and the other involved concatenating
267"
RESULTS/EXPERIMENTS,0.6373626373626373,"uniformly distributed points within a predefined rectangular region (determined by the span of the
268"
RESULTS/EXPERIMENTS,0.6391941391941391,"dataset) to the existing dataset. As illustrated in Figure 9, the tests demonstrate that in both scenarios,
269"
RESULTS/EXPERIMENTS,0.6410256410256411,"the algorithm exhibits a high degree of insensitivity to noise.
270"
RESULTS/EXPERIMENTS,0.6428571428571429,"âˆ’10
âˆ’5
0
5
10 âˆ’10 âˆ’5 0 5 10 X Y"
RESULTS/EXPERIMENTS,0.6446886446886447,(a) Clean
RESULTS/EXPERIMENTS,0.6465201465201466,"âˆ’10
âˆ’5
0
5
10 âˆ’10 âˆ’5 0 5 10 X Y"
RESULTS/EXPERIMENTS,0.6483516483516484,(b) Uniform noise
RESULTS/EXPERIMENTS,0.6501831501831502,"âˆ’10
âˆ’5
0
5
10 âˆ’10 âˆ’5 0 5 10 X Y"
RESULTS/EXPERIMENTS,0.652014652014652,(c) Gaussian noise
RESULTS/EXPERIMENTS,0.6538461538461539,"Figure 9: Three representations of the algorithmâ€™s predicted cluster centers are displayed as colored
dots, with each point of the Impossible dataset shown as a small black dot. In the left graph, the
dataset is unmodified. In the center graph, 250 uniformly distributed samples were added. In the
right graph, Gaussian noise perturbations with ğœ= 0.1 were applied."
RESULTS/EXPERIMENTS,0.6556776556776557,"4.8
Hyperparameter values effect on accuracy
271"
RESULTS/EXPERIMENTS,0.6575091575091575,"To better understand and measure the significance of choosing the right values for the hyper-
272"
RESULTS/EXPERIMENTS,0.6593406593406593,"parameters of the proposed algorithm, that it to say the number of VoronoÃ¯ regions ğ‘š, Spec-
273"
RESULTS/EXPERIMENTS,0.6611721611721612,"tral Bridges was run on the PCA â„= 32 embedded MNIST dataset with varying values of ğ‘šâˆˆ
274"
RESULTS/EXPERIMENTS,0.663003663003663,"{10, 120, 230, 340, 450, 560, 670, 780, 890, 1000}. The case ğ‘š= 10 is equivalent to the k-means++ algo-
275"
RESULTS/EXPERIMENTS,0.6648351648351648,"rithm. ARI and NMI scores are recorded over 20 consecutive iterations and subsequently plotted. As
276"
RESULTS/EXPERIMENTS,0.6666666666666666,"shown by Figure 10, the accuracy seems to be consistently increasing with values of ğ‘š, although
277"
RESULTS/EXPERIMENTS,0.6684981684981685,"the largest observed gap occurs between values of ğ‘š= 10 and ğ‘š= 120, indicating a tremendous im-
278"
RESULTS/EXPERIMENTS,0.6703296703296703,"provement over the classical k-means++ framework even for empirically suboptimal hyperparameter
279"
RESULTS/EXPERIMENTS,0.6721611721611722,"values.
280"
RESULTS/EXPERIMENTS,0.673992673992674,"0
200
400
600
800
1000 0.4 0.5 0.6 0.7 0.8"
RESULTS/EXPERIMENTS,0.6758241758241759,"0
200
400
600
800
1000
0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 m
m"
RESULTS/EXPERIMENTS,0.6776556776556777,ARI Score
RESULTS/EXPERIMENTS,0.6794871794871795,NMI Score
RESULTS/EXPERIMENTS,0.6813186813186813,Figure 10: ARI and NMI scores of Spectral Bridges with varying values of ğ‘š.
CONCLUSION/DISCUSSION ,0.6831501831501832,"5
Conclusive remarks
281"
CONCLUSION/DISCUSSION ,0.684981684981685,"Spectral Bridges is an original clustering algorithm which presents a novel approach by integrating
282"
CONCLUSION/DISCUSSION ,0.6868131868131868,"the strengths of traditional k-means and spectral clustering frameworks. This algorithm utilizes a
283"
CONCLUSION/DISCUSSION ,0.6886446886446886,"simple affinity measure for spectral clustering, which is derived from the minimal margin between
284"
CONCLUSION/DISCUSSION ,0.6904761904761905,"pairs of VoronoÃ¯ regions.
285"
CONCLUSION/DISCUSSION ,0.6923076923076923,"The algorithm demonstrates scalability, handling large datasets efficiently through a balanced com-
286"
CONCLUSION/DISCUSSION ,0.6941391941391941,"putational complexity between the k-means clustering and eigen-decomposition steps. As a non-
287"
CONCLUSION/DISCUSSION ,0.6959706959706959,"parametric method, Spectral Bridges does not rely on strong assumptions about data distribution,
288"
CONCLUSION/DISCUSSION ,0.6978021978021978,"enhancing its versatility across various data types. It performs exceptionally well with both syn-
289"
CONCLUSION/DISCUSSION ,0.6996336996336996,"thetic and real-world data and consistently outperforms conventional clustering algorithms such as
290"
CONCLUSION/DISCUSSION ,0.7014652014652014,"k-means, DBSCAN, and mixture models.
291"
CONCLUSION/DISCUSSION ,0.7032967032967034,"The design of Spectral Bridges ensures robustness to noise, a significant advantage in real-world
292"
CONCLUSION/DISCUSSION ,0.7051282051282052,"applications. Additionally, the algorithm requires minimal hyperparameters, primarily the number
293"
CONCLUSION/DISCUSSION ,0.706959706959707,"of VoronoÃ¯ regions, making it straightforward to tune and deploy.
294"
CONCLUSION/DISCUSSION ,0.7087912087912088,"Furthermore, Spectral Bridges can be kernelized, allowing it to handle data in similarity space directly,
295"
CONCLUSION/DISCUSSION ,0.7106227106227107,"which enhances its flexibility and applicability. Overall, Spectral Bridges is a powerful, robust, and
296"
CONCLUSION/DISCUSSION ,0.7124542124542125,"scalable clustering algorithm that offers significant improvements over traditional methods, making
297"
CONCLUSION/DISCUSSION ,0.7142857142857143,"it an excellent tool for advanced clustering tasks across numerous domains.
298"
APPENDIX,0.7161172161172161,"6
Appendix
299"
APPENDIX,0.717948717948718,"6.1
Derivation of the bridge affinity
300"
APPENDIX,0.7197802197802198,"We denote a bridge as a segment connecting two centroids ğœ‡ğ‘˜and ğœ‡ğ‘™. The inertia of a bridge between
301"
APPENDIX,0.7216117216117216,"ğ’±ğ‘˜and ğ’±ğ‘™is defined as
302"
APPENDIX,0.7234432234432234,"ğµğ‘˜ğ‘™=
âˆ‘
ğ‘¥ğ‘–âˆˆğ’±ğ‘˜âˆªğ’±ğ‘™
â€–ğ‘¥ğ‘–âˆ’ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–)â€–2,"
APPENDIX,0.7252747252747253,"where
303"
APPENDIX,0.7271062271062271,"ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–) = ğœ‡ğ‘˜+ ğ‘¡ğ‘–(ğœ‡ğ‘™âˆ’ğœ‡ğ‘˜),"
APPENDIX,0.7289377289377289,"with
304"
APPENDIX,0.7307692307692307,"ğ‘¡ğ‘–= min (1, max (0, âŸ¨ğ‘¥ğ‘–âˆ’ğœ‡ğ‘˜|ğœ‡ğ‘™âˆ’ğœ‡ğ‘˜âŸ©"
APPENDIX,0.7326007326007326,"â€–ğœ‡ğ‘™âˆ’ğœ‡ğ‘˜â€–2
)) ."
APPENDIX,0.7344322344322345,"ğµğ‘˜ğ‘™, the bridge inertia between centroids ğ‘˜and ğ‘™, can be expressed as the sum of three terms, which
305"
APPENDIX,0.7362637362637363,"represents the projection onto each centroÃ¯ds and onto the segment:
306"
APPENDIX,0.7380952380952381,"ğµğ‘˜ğ‘™=
âˆ‘
ğ‘–âˆ£ğ‘¡ğ‘–=0
â€–ğ‘¥ğ‘–âˆ’ğœ‡ğ‘˜â€–2 + âˆ‘
ğ‘–âˆ£ğ‘¡ğ‘–=1
â€–ğ‘¥ğ‘–âˆ’ğœ‡ğ‘™â€–2 +
âˆ‘
ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0,1[
â€–ğ‘¥ğ‘–âˆ’ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–)â€–2."
APPENDIX,0.73992673992674,"The last term may be decomposed in two parts corresponding to the points of the two VoronoÃ¯
307"
APPENDIX,0.7417582417582418,"regions which are projected on the segment:
308"
APPENDIX,0.7435897435897436,"âˆ‘
ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0,1[
â€–ğ‘¥ğ‘–âˆ’ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–)â€–2 =
âˆ‘"
APPENDIX,0.7454212454212454,"ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0, 1"
APPENDIX,0.7472527472527473,"2[
â€–ğ‘¥ğ‘–âˆ’ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–)â€–2 +
âˆ‘"
APPENDIX,0.7490842490842491,ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ[ 1
APPENDIX,0.7509157509157509,"2,1[
â€–ğ‘¥ğ‘–âˆ’ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–)â€–2"
APPENDIX,0.7527472527472527,"and each part further decomposed using Pythagore
309 âˆ‘"
APPENDIX,0.7545787545787546,"ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0, 1"
APPENDIX,0.7564102564102564,"2[
â€–ğ‘¥ğ‘–âˆ’ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–)â€–2 =
âˆ‘"
APPENDIX,0.7582417582417582,"ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0, 1"
APPENDIX,0.76007326007326,"2[
â€–ğ‘¥ğ‘–âˆ’ğœ‡ğ‘˜â€–2 âˆ’
âˆ‘"
APPENDIX,0.7619047619047619,"ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0, 1"
APPENDIX,0.7637362637362637,"2[
â€–ğœ‡ğ‘˜âˆ’ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–)â€–2 =
âˆ‘"
APPENDIX,0.7655677655677655,"ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0, 1"
APPENDIX,0.7673992673992674,"2[
â€–ğ‘¥ğ‘–âˆ’ğœ‡ğ‘˜â€–2 âˆ’
âˆ‘"
APPENDIX,0.7692307692307693,"ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0, 1"
APPENDIX,0.7710622710622711,"2[
â€–ğ‘¡ğ‘–(ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™)â€–2, âˆ‘"
APPENDIX,0.7728937728937729,ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ] 1
APPENDIX,0.7747252747252747,"2,1[
â€–ğ‘¥ğ‘–âˆ’ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–)â€–2 =
âˆ‘"
APPENDIX,0.7765567765567766,"ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0, 1"
APPENDIX,0.7783882783882784,"2[
â€–ğ‘¥ğ‘–âˆ’ğœ‡ğ‘™â€–2 âˆ’
âˆ‘"
APPENDIX,0.7802197802197802,"ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0, 1"
APPENDIX,0.782051282051282,"2[
â€–ğœ‡ğ‘™âˆ’ğ‘ğ‘˜ğ‘™(ğ‘¥ğ‘–)â€–2 =
âˆ‘"
APPENDIX,0.7838827838827839,ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ] 1
APPENDIX,0.7857142857142857,"2,1[
â€–ğ‘¥ğ‘–âˆ’ğœ‡ğ‘˜â€–2 âˆ’
âˆ‘"
APPENDIX,0.7875457875457875,"ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0, 1"
APPENDIX,0.7893772893772893,"2[
â€–(1 âˆ’ğ‘¡ğ‘–)(ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™)â€–2"
APPENDIX,0.7912087912087912,"Thus
310"
APPENDIX,0.793040293040293,"ğµğ‘˜ğ‘™âˆ’ğ¼ğ‘˜ğ‘™=
âˆ‘"
APPENDIX,0.7948717948717948,"ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0, 1"
APPENDIX,0.7967032967032966,"2[
ğ‘¡2
ğ‘–â€–ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™â€–2 +
âˆ‘"
APPENDIX,0.7985347985347986,ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ] 1
APPENDIX,0.8003663003663004,"2,1[
(1 âˆ’ğ‘¡ğ‘–)2â€–ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™â€–2,"
APPENDIX,0.8021978021978022,"ğµğ‘˜ğ‘™âˆ’ğ¼ğ‘˜ğ‘™
â€–ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™â€–2 =
âˆ‘"
APPENDIX,0.8040293040293041,"ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ]0, 1"
APPENDIX,0.8058608058608059,"2[
ğ‘¡2
ğ‘–+
âˆ‘"
APPENDIX,0.8076923076923077,ğ‘–âˆ£ğ‘¡ğ‘–âˆˆ] 1
APPENDIX,0.8095238095238095,"2,1[
(1 âˆ’ğ‘¡ğ‘–)2,"
APPENDIX,0.8113553113553114,"ğµğ‘˜ğ‘™âˆ’ğ¼ğ‘˜ğ‘™
(ğ‘›ğ‘˜+ ğ‘›ğ‘™)â€–ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™â€–2 =
âˆ‘ğ‘¥ğ‘–âˆˆğ’±ğ‘˜âŸ¨ğ‘¥ğ‘–âˆ’ğœ‡ğ‘˜|ğœ‡ğ‘™âˆ’ğœ‡ğ‘˜âŸ©2+ âˆ‘ğ‘¥ğ‘–âˆˆğ’±ğ‘™âŸ¨ğ‘¥ğ‘–âˆ’ğœ‡ğ‘™|ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™âŸ©2+"
APPENDIX,0.8131868131868132,"(ğ‘›ğ‘˜+ ğ‘›ğ‘™)â€–ğœ‡ğ‘˜âˆ’ğœ‡ğ‘™â€–4
."
APPENDIX,0.815018315018315,"6.2
Code
311"
APPENDIX,0.8168498168498168,"6.2.1
Implementation
312"
APPENDIX,0.8186813186813187,"Numerical experiments have been conducted in Python. The python scripts to reproduce the
313"
APPENDIX,0.8205128205128205,"simulations and figures are available at https://github.com/flheight/Spectral-Bridges. The Spectral
314"
APPENDIX,0.8223443223443223,"Bridge algorithm is implemented both in
315"
APPENDIX,0.8241758241758241,"â€¢ Python: https://pypi.org/project/spectral-bridges, and
316"
APPENDIX,0.826007326007326,"â€¢ R: https://github.com/cambroise/spectral-bridges-Rpackage.
317"
APPENDIX,0.8278388278388278,"6.2.2
Affinity matrix computation
318"
APPENDIX,0.8296703296703297,"Taking a closer look at the second step of Algorithm 1 , that is the affinity matrix calculation
319"
APPENDIX,0.8315018315018315,"with a ğ‘‚(ğ‘›Ã— ğ‘šÃ— ğ‘‘) time complexity, most operations can be parallelized leaving a single loop,
320"
APPENDIX,0.8333333333333334,"bundling together ğ‘š2 dot products into only ğ‘šmatrix multiplications, thus allowing for an efficient
321"
APPENDIX,0.8351648351648352,"construction in both high and low level programming languages. Though the complexity of the
322"
APPENDIX,0.836996336996337,"algorithm remains unchanged, libraries such as Basic Linear Algebra Subprograms can render the
323"
APPENDIX,0.8388278388278388,"calculations orders of magnitude faster. Moreover, the symmetrical nature of the bridge affinity can
324"
APPENDIX,0.8406593406593407,"be used to effectively halve the computation time.
325"
APPENDIX,0.8424908424908425,"The calculation of the affinity matrix is highlighted by the Python code Listing 1. Though it could
326"
APPENDIX,0.8443223443223443,"be even more optimized, the following code snippet is approximately 200 times faster than a naive
327"
APPENDIX,0.8461538461538461,"implementation on a small dataset comprised of ğ‘›= 3594, ğ‘‘= 2 points, and a value of ğ‘š= 250.
328"
APPENDIX,0.847985347985348,"Notice that the Python code is significantly faster than the R code.
329"
APPENDIX,0.8498168498168498,"Listing 1 Python code for affinity matrix computation
# Initialize the affinity matrix
affinity = np.empty((self.n_nodes, self.n_nodes))"
APPENDIX,0.8516483516483516,"# Center each Voronoi region around its centroid
X_centered = ["
APPENDIX,0.8534798534798534,"X[kmeans.labels_ == i] - kmeans.cluster_centers_[i] for i in range(self.n_nodes)
]"
APPENDIX,0.8553113553113553,"# Count the total number of points in each pair of regions
counts = np.array([X_centered[i].shape[0] for i in range(self.n_nodes)])
counts = counts[np.newaxis, :] + counts[:, np.newaxis]"
APPENDIX,0.8571428571428571,"# Compute the segments between each pair of centroids and their squared Euclidean norm
segments = ("
APPENDIX,0.8589743589743589,"kmeans.cluster_centers_[np.newaxis, :] - kmeans.cluster_centers_[:, np.newaxis]
)
dists = np.einsum(""ijk,ijk->ij"", segments, segments)
np.fill_diagonal(dists, 1)
# Avoid dividing by zero"
APPENDIX,0.8608058608058609,"# Assign each row of the affinity matrix
for i in range(self.n_nodes):"
APPENDIX,0.8626373626373627,"projs = np.maximum(np.dot(X_centered[i], segments[i].T), 0)
affinity[i] = np.einsum(""ij,ij->j"", projs, projs)"
APPENDIX,0.8644688644688645,"# Symmetrize the matrix and normalize, as well as taking the element-wise square root
affinity = np.sqrt(affinity + affinity.T) / (np.sqrt(counts) * dists)
affinity -= 0.5 * affinity.max()
# For numerical stability"
APPENDIX,0.8663003663003663,"# Apply the exponential transformation
q10, q90 = np.quantile(affinity, [0.1, 0.9])"
APPENDIX,0.8681318681318682,"gamma = np.log(self.M) / (q90 - q10)
affinity = np.exp(gamma * affinity)"
REFERENCES,0.86996336996337,"References
330"
REFERENCES,0.8717948717948718,"Arthur, David, and Sergei Vassilvitskii. 2006. â€œK-Means++: The Advantages of Careful Seeding.â€
331"
REFERENCES,0.8736263736263736,"Technical Report 2006-13. Stanford InfoLab; Stanford. http://ilpubs.stanford.edu:8090/778/.
332"
REFERENCES,0.8754578754578755,"Cai, Deng, and Xinlei Chen. 2014. â€œLarge Scale Spectral Clustering via Landmark-Based Sparse
333"
REFERENCES,0.8772893772893773,"Representation.â€ IEEE Transactions on Cybernetics 45 (8): 1669â€“80.
334"
REFERENCES,0.8791208791208791,"Chen, Wen-Yen, Yangqiu Song, Hongjie Bai, Chih-Jen Lin, and Edward Y Chang. 2010. â€œParallel
335"
REFERENCES,0.8809523809523809,"Spectral Clustering in Distributed Systems.â€ IEEE Transactions on Pattern Analysis and Machine
336"
REFERENCES,0.8827838827838828,"Intelligence 33 (3): 568â€“86.
337"
REFERENCES,0.8846153846153846,"Cortes, Corinna, and Vladimir Vapnik. 1995. â€œSupport-Vector Networks.â€ Machine Learning 20 (3):
338"
REFERENCES,0.8864468864468864,"273â€“97.
339"
REFERENCES,0.8882783882783882,"Cover, Thomas M, and Joy A Thomas. 1991. â€œInformation Theory and the Stock Market.â€ Elements of
340"
REFERENCES,0.8901098901098901,"Information Theory. Wiley Inc., New York, 543â€“56.
341"
REFERENCES,0.891941391941392,"Dempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. â€œMaximum Likelihood from Incomplete
342"
REFERENCES,0.8937728937728938,"Data via the EM Algorithm.â€ Journal of the Royal Statistical Society: Series B (Methodological) 39
343"
REFERENCES,0.8956043956043956,"(1): 1â€“22.
344"
REFERENCES,0.8974358974358975,"Dhillon, Inderjit S, Yuqiang Guan, and Brian Kulis. 2004. â€œKernel k-Means, Spectral Clustering
345"
REFERENCES,0.8992673992673993,"and Normalized Cuts.â€ In Proceedings of the Tenth ACM SIGKDD International Conference on
346"
REFERENCES,0.9010989010989011,"Knowledge Discovery and Data Mining, 551â€“56. ACM.
347"
REFERENCES,0.9029304029304029,"Eisen, Michael B., Paul T. Spellman, Patrick O. Brown, and David Botstein. 1998. â€œCluster Analysis
348"
REFERENCES,0.9047619047619048,"and Display of Genome-Wide Expression Patterns.â€ Proceedings of the National Academy of
349"
REFERENCES,0.9065934065934066,"Sciences 95 (25): 14863â€“68.
350"
REFERENCES,0.9084249084249084,"Ester, Martin, Hans-Peter Kriegel, JÃ¶rg Sander, Xiaowei Xu, et al. 1996. â€œA Density-Based Algorithm
351"
REFERENCES,0.9102564102564102,"for Discovering Clusters in Large Spatial Databases with Noise.â€ In Kdd, 96:226â€“31.
352"
REFERENCES,0.9120879120879121,"Gao, Zhangyang, Haitao Lin, Cheng Tan, Lirong Wu, Stan Li, et al. 2021. â€œGit: Clustering Based on
353"
REFERENCES,0.9139194139194139,"Graph of Intensity Topology.â€ arXiv Preprint arXiv:2110.01274.
354"
REFERENCES,0.9157509157509157,"Govaert, GÃ©rard, and Mohamed Nadif. 2003. â€œClustering with Block Mixture Models.â€ Pattern
355"
REFERENCES,0.9175824175824175,"Recognition 36 (2): 463â€“73.
356"
REFERENCES,0.9194139194139194,"Halkidi, Maria, Yannis Batistakis, and Michalis Vazirgiannis. 2002. â€œCluster Validity Methods: Part i.â€
357"
REFERENCES,0.9212454212454212,"ACM SIGMOD Record 31 (2): 40â€“45.
358"
REFERENCES,0.9230769230769231,"Huang, Dong, Chang-Dong Wang, Jian-Sheng Wu, Jian-Huang Lai, and Chee-Keong Kwoh. 2019.
359"
REFERENCES,0.924908424908425,"â€œUltra-Scalable Spectral Clustering and Ensemble Clustering.â€ IEEE Transactions on Knowledge
360"
REFERENCES,0.9267399267399268,"and Data Engineering 32 (6): 1212â€“26.
361"
REFERENCES,0.9285714285714286,"Jacobs, Robert A, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. â€œAdaptive Mixtures
362"
REFERENCES,0.9304029304029304,"of Local Experts.â€ Neural Computation 3 (1): 79â€“87.
363"
REFERENCES,0.9322344322344323,"Latouche, Pierre, Etienne BirmelÃ©, and Christophe Ambroise. 2011. â€œOverlapping stochastic block
364"
REFERENCES,0.9340659340659341,"models with application to the French political blogosphere.â€ The Annals of Applied Statistics 5
365"
REFERENCES,0.9358974358974359,"(1): 309â€“36. https://doi.org/10.1214/10-AOAS382.
366"
REFERENCES,0.9377289377289377,"MacQueen, James et al. 1967. â€œSome Methods for Classification and Analysis of Multivariate
367"
REFERENCES,0.9395604395604396,"Observations.â€ In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and
368"
REFERENCES,0.9413919413919414,"Probability, 1:281â€“97. Oakland, CA, USA.
369"
REFERENCES,0.9432234432234432,"McLachlan, Geoffrey J., and David Peel. 2000. Finite Mixture Models. New York: Wiley-Interscience.
370"
REFERENCES,0.945054945054945,"Ng, Andrew, Michael Jordan, and Yair Weiss. 2001. â€œOn Spectral Clustering: Analysis and an
371"
REFERENCES,0.9468864468864469,"Algorithm.â€ Advances in Neural Information Processing Systems 14.
372"
REFERENCES,0.9487179487179487,"Shi, Jianbo, and Jitendra Malik. 2000. â€œNormalized Cuts and Image Segmentation.â€ IEEE Transactions
373"
REFERENCES,0.9505494505494505,"on Pattern Analysis and Machine Intelligence 22 (8): 888â€“905.
374"
REFERENCES,0.9523809523809523,"Verhaak, Roel G. W., Katherine A. Hoadley, Elizabeth Purdom, Victoria Wang, Yuexin Qi, Matthew
375"
REFERENCES,0.9542124542124543,"D. Wilkerson, Charlie R. Miller, et al. 2010. â€œIntegrated Genomic Analysis Identifies Clinically
376"
REFERENCES,0.9560439560439561,"Relevant Subtypes of Glioblastoma Characterized by Abnormalities in PDGFRA, IDH1, EGFR,
377"
REFERENCES,0.9578754578754579,"and NF1.â€ Cancer Cell 17 (1): 98â€“110.
378"
REFERENCES,0.9597069597069597,"Von Luxburg, Ulrike. 2007. â€œA Tutorial on Spectral Clustering.â€ Statistics and Computing 17: 395â€“416.
379"
REFERENCES,0.9615384615384616,"Ward Jr, Joe H. 1963. â€œHierarchical Grouping to Optimize an Objective Function.â€ Journal of the
380"
REFERENCES,0.9633699633699634,"American Statistical Association 58 (301): 236â€“44.
381"
OTHER,0.9652014652014652,"Session information
382"
OTHER,0.967032967032967,"R version 4.3.2 (2023-10-31)
383"
OTHER,0.9688644688644689,"Platform: x86_64-apple-darwin20 (64-bit)
384"
OTHER,0.9706959706959707,"Running under: macOS Sonoma 14.3.1
385 386"
OTHER,0.9725274725274725,"Matrix products: default
387"
OTHER,0.9743589743589743,"BLAS:
/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib
388"
OTHER,0.9761904761904762,"LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;
LAPACK
389 390"
OTHER,0.978021978021978,"locale:
391"
OTHER,0.9798534798534798,"[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
392 393"
OTHER,0.9816849816849816,"time zone: Europe/Paris
394"
OTHER,0.9835164835164835,"tzcode source: internal
395 396"
OTHER,0.9853479853479854,"attached base packages:
397"
OTHER,0.9871794871794872,"[1] stats
graphics
grDevices utils
datasets
methods
base
398 399"
OTHER,0.989010989010989,"loaded via a namespace (and not attached):
400"
OTHER,0.9908424908424909,"[1] digest_0.6.34
fastmap_1.1.1
xfun_0.42
Matrix_1.6-5
401"
OTHER,0.9926739926739927,"[5] lattice_0.22-5
reticulate_1.37.0 knitr_1.45
htmltools_0.5.7
402"
OTHER,0.9945054945054945,"[9] png_0.1-8
rmarkdown_2.26
cli_3.6.2
grid_4.3.2
403"
OTHER,0.9963369963369964,"[13] compiler_4.3.2
rstudioapi_0.15.0 tools_4.3.2
evaluate_0.23
404"
OTHER,0.9981684981684982,"[17] Rcpp_1.0.12
yaml_2.3.8
rlang_1.1.3
jsonlite_1.8.8
405"
