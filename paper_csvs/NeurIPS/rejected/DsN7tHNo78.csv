Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007331378299120235,"Composed Image Retrieval (CIR) facilitates retrieving an image matching a refer-
1"
ABSTRACT,0.001466275659824047,"ence image while incorporating specified textual modifications, which is crucial
2"
ABSTRACT,0.0021994134897360706,"for internet searches and e-commerce. Traditional supervised CIR methods rely
3"
ABSTRACT,0.002932551319648094,"on annotated triplets, which are labor-intensive and limit generalizability. Recent
4"
ABSTRACT,0.0036656891495601175,"advances in Zero-Shot Composed Image Retrieval (ZS-CIR) address the challenge
5"
ABSTRACT,0.004398826979472141,"of performing this task without annotated triplets. A key challenge in ZS-CIR
6"
ABSTRACT,0.005131964809384164,"is training models on limited intention-relevant datasets to understand human
7"
ABSTRACT,0.005865102639296188,"intention implicitly expressed in textual modifications for accurately retrieving
8"
ABSTRACT,0.006598240469208211,"target images. In this paper, we introduce an image-text dataset incorporated
9"
ABSTRACT,0.007331378299120235,"with pseudo-manipulation intentions to enhance the training of ZS-CIR models
10"
ABSTRACT,0.008064516129032258,"in understanding human manipulation intents. Based on our dataset, we propose
11"
ABSTRACT,0.008797653958944282,"a novel framework, De-MINDS, for capturing the intent humans aim to modify,
12"
ABSTRACT,0.009530791788856305,"thereby enhancing the ZS-CIR model’s ability to understand human manipulation
13"
ABSTRACT,0.010263929618768328,"descriptions. Specifically, a simple mapping network first maps image information
14"
ABSTRACT,0.010997067448680353,"into language space and forms a target description with a manipulation descrip-
15"
ABSTRACT,0.011730205278592375,"tion. Subsequently, De-MINDS captures intention-relevant information from tar-
16"
ABSTRACT,0.012463343108504398,"get descriptions and converts them into several pseudo-word tokens for accurate
17"
ABSTRACT,0.013196480938416423,"ZS-CIR. The De-MINDS model exhibits robust generalization and significant
18"
ABSTRACT,0.013929618768328446,"improvements in performance across four ZS-CIR tasks. It achieves performance
19"
ABSTRACT,0.01466275659824047,"improvements from 2.05% to 4.35% over the best methods and establishes new
20"
ABSTRACT,0.015395894428152493,"state-of-the-art results with comparable inference times. Our code is available at
21"
ABSTRACT,0.016129032258064516,"https://anonymous.4open.science/r/De-MINDS/.
22"
INTRODUCTION,0.01686217008797654,"1
Introduction
23"
INTRODUCTION,0.017595307917888565,"Composed Image Retrieval (CIR) [55] aims to retrieve an image that is visually similar to a reference
24"
INTRODUCTION,0.018328445747800588,"image while having visual modification according to the manipulation text. Different from traditional
25"
INTRODUCTION,0.01906158357771261,"image retrieval [15], CIR offers more flexibility and accuracy by enabling users to integrate both
26"
INTRODUCTION,0.019794721407624633,"visual and textual information into their search intent. This approach has gained emerging attention
27"
INTRODUCTION,0.020527859237536656,"in internet searches and e-commerce applications [12, 45]. Various supervised methods have been
28"
INTRODUCTION,0.02126099706744868,"proposed to solve CIR problem [12, 33, 19, 4], which requires a large amount of annotated triplets,
29"
INTRODUCTION,0.021994134897360705,"i.e., a reference image, a manipulated description, and a target image, for training task-specific
30"
INTRODUCTION,0.022727272727272728,"retrieval models. However, these supervised methods are labor-intensive for data annotation and tend
31"
INTRODUCTION,0.02346041055718475,"to suffer from limited generalization capabilities due to bias in human annotation. To enhance model
32"
INTRODUCTION,0.024193548387096774,"generalization and perform CIR tasks without annotated triplets, recent research [45, 3, 52, 25, 20]
33"
INTRODUCTION,0.024926686217008796,"introduce Zero-Shot Composed Image Retrieval (ZS-CIR). Existing solutions for ZS-CIR map an
34"
INTRODUCTION,0.025659824046920823,"image to the language space, combining it with text to form a query. This query retrieves target
35"
INTRODUCTION,0.026392961876832845,"images from the shared semantic space of a pre-trained vision-language model by calculating semantic
36"
INTRODUCTION,0.027126099706744868,"similarity. These methods typically involve a pre-trained mapping network that converts the reference
37"
INTRODUCTION,0.02785923753665689,"image into a pseudo-word token S∗. During retrieval, this token S∗is merged with the manipulation
38"
INTRODUCTION,0.028592375366568914,"description to construct a target description, which a pre-trained CLIP model [41] then encodes,
39"
INTRODUCTION,0.02932551319648094,"leveraging its comprehensive pre-trained knowledge across image candidates for retrieval.
40"
INTRODUCTION,0.030058651026392963,"Despite remarkable advancement, the pre-trained mapping networks are not satisfactory for CIR due
41"
INTRODUCTION,0.030791788856304986,"to the following reasons:
42"
INTRODUCTION,0.03152492668621701,"(1) There exists a discrepancy between the retrieval and pre-training stages in ZS-CIR models. During
43"
INTRODUCTION,0.03225806451612903,"retrieval, the mapping network is tasked with aligning intent-specific visual information (e.g., objects,
44"
INTRODUCTION,0.032991202346041054,"scenes, colors, and styles) in language space to form a composed image description query (e.g.,
45"
INTRODUCTION,0.03372434017595308,"change to a man playing the accordion joyfully in the street) for calculating semantic similarity with
46"
INTRODUCTION,0.0344574780058651,"the target image. However, in the pre-training phase, the mapping network aligns general visual
47"
INTRODUCTION,0.03519061583577713,"information with textual descriptions of the image content (e.g., a musician plays the piano). Without
48"
INTRODUCTION,0.03592375366568915,"intent-specific mapping, the pseudo-token S∗contains heavy information redundancy involving most
49"
INTRODUCTION,0.036656891495601175,"objects, background/foreground, color, and style, leading to inaccurate retrieval.
50"
INTRODUCTION,0.0373900293255132,"(2) Accurately understanding the intention a user intends to modify in manipulation descriptions
51"
INTRODUCTION,0.03812316715542522,"presents substantial challenges. These intentions are implicitly expressed in users’ manipulation
52"
INTRODUCTION,0.038856304985337244,"descriptions. For instance, the manipulation intention embedded in the request to “make this photo
53"
INTRODUCTION,0.039589442815249266,"feel like early fall” may involve changing colors (e.g., orange and yellow), adjusting the scene (e.g.,
54"
INTRODUCTION,0.04032258064516129,"fallen leaves), and adding specific objects (e.g., autumnal trees). However, existing ZS-CIR models
55"
INTRODUCTION,0.04105571847507331,"rely on the CLIP language encoder, which challenges capturing fine-grained/long information from
56"
INTRODUCTION,0.041788856304985335,"text [51, 58], facing difficulties in accurately understanding these manipulation intentions.
57"
INTRODUCTION,0.04252199413489736,"In this work, we introduce the intent-CC3M, an intention-based dataset for training mapping net-
58"
INTRODUCTION,0.04325513196480939,"works capable of aligning intention-relevant visual information within the language space, thus
59"
INTRODUCTION,0.04398826979472141,"addressing the gap between pre-training and retrieval in ZS-CIR models. We incorporate pseudo-
60"
INTRODUCTION,0.04472140762463343,"manipulation descriptions in CC3M [47], the widely used ZS-CIR training dataset [45, 52]. These
61"
INTRODUCTION,0.045454545454545456,"pseudo descriptions, reflecting potential user intention to manipulate images, are reasoned through
62"
INTRODUCTION,0.04618768328445748,"chain-of-thought prompting using an off-the-shelf Multi-modal Large Language Model (MLLM),
63"
INTRODUCTION,0.0469208211143695,"facilitating the learning of intent-specific mapping capabilities. Furthermore, to overcome the chal-
64"
INTRODUCTION,0.047653958944281524,"lenge of existing ZS-CIR models in understanding manipulation intention within descriptions, we
65"
INTRODUCTION,0.04838709677419355,"propose a novel unDErstanding of Manipulation INtention from target Description before Searching
66"
INTRODUCTION,0.04912023460410557,"approach, named De-MINDS. We leverage pseudo-manipulation descriptions to train De-MINDS
67"
INTRODUCTION,0.04985337243401759,"to capture manipulation intention from various aspects (e.g., objects, scenes, colors, styles) guided
68"
INTRODUCTION,0.050586510263929615,"by multiple learnable queries. This intention information is mapped to several pseudo-word tokens,
69"
INTRODUCTION,0.051319648093841645,"which are subsequently input into the CLIP language encoder, enhancing its ability to understand
70"
INTRODUCTION,0.05205278592375367,"users’ intention to modify and thereby improving the accuracy of CIR.
71"
INTRODUCTION,0.05278592375366569,"The main contributions of this work are summarized as follows: (1) We introduce intent-CC3M, a
72"
INTRODUCTION,0.053519061583577714,"novel dataset with pseudo-manipulation descriptions reasoned by an MLLM to bridge the gap between
73"
INTRODUCTION,0.054252199413489736,"pre-training and retrieval in ZS-CIR models. Our experiments demonstrate that baseline models
74"
INTRODUCTION,0.05498533724340176,"trained with our dataset are capable of aligning intention-relevant visual information, achieving
75"
INTRODUCTION,0.05571847507331378,"consistent performance improvements. (2) We propose a novel manipulation intention understanding
76"
INTRODUCTION,0.056451612903225805,"network. We extract intentions in manipulation descriptions under the guidance of learnable queries
77"
INTRODUCTION,0.05718475073313783,"and map to several pseudo-word tokens for retrieval, enhancing the CLIP’s ability to understand users’
78"
INTRODUCTION,0.05791788856304985,"intentions. It sheds new light on intention-based image retrieval. (3) Our De-MINDS are consistently
79"
INTRODUCTION,0.05865102639296188,"effective and generalizable across diverse ZS-CIR tasks. It significantly improves CIR performance
80"
INTRODUCTION,0.0593841642228739,"from 2.05% to 4.35% across four CIR tasks, establishing new state-of-the-art results with comparable
81"
INTRODUCTION,0.060117302052785926,"inference time, further impacting vision and language applications.
82"
RELATED WORKS,0.06085043988269795,"2
Related Works
83"
RELATED WORKS,0.06158357771260997,"Composed Image Retrieval. Composed Image Retrieval (CIR) integrates image and text for retrieval
84"
RELATED WORKS,0.062316715542521994,"[54]. Current models typically employ late fusion for integrating visual and language features
85"
RELATED WORKS,0.06304985337243402,"separately [4, 33, 4]. In contrast, zero-shot CIR models like Pic2Word [45], SEARLE [3], and
86"
RELATED WORKS,0.06378299120234604,"Context-I2W [52] train on image-text pairs, bypassing the need for costly CIR datasets. Pic2Word
87"
RELATED WORKS,0.06451612903225806,"aligns entire images into text features, SEARLE adds a pseudo-word token to GPT-based captions,
88"
RELATED WORKS,0.06524926686217009,"and Context-I2W employs context-dependent word mapping for accurate retrieval. However, these
89"
RELATED WORKS,0.06598240469208211,"methods rely on the pre-trained CLIP language encoder, which struggles to understand intentions
90"
RELATED WORKS,0.06671554252199413,"within manipulation descriptions. To tackle this issue, we propose a novel model that effectively
91"
RELATED WORKS,0.06744868035190615,A rugby player passes the
RELATED WORKS,0.06818181818181818,ball with his teammate.
RELATED WORKS,0.0689149560117302,Input Image
RELATED WORKS,0.06964809384164222,Original Caption
RELATED WORKS,0.07038123167155426,A rugby player in a purple and orange jersey is in the
RELATED WORKS,0.07111436950146628,"foreground running with the ball, while three other 
players in similar jerseys are in the background running 
towards him, indicating a game in progress. The field is"
RELATED WORKS,0.0718475073313783,"green, and the focus is on the action of the players."
RELATED WORKS,0.07258064516129033,Rewritten Caption
RELATED WORKS,0.07331378299120235,Original Caption Rewriting
RELATED WORKS,0.07404692082111437,Rewrite the original caption like a human
RELATED WORKS,0.0747800586510264,"description in: 1. Describe the object's 
appearance, position, and relationship. 2."
RELATED WORKS,0.07551319648093842,"Describe what is in the background and 
foreground. 3. Focusing on colors, styles, and 
materials. 4. Identify the domain of the image."
RELATED WORKS,0.07624633431085044,Manipulation Intention Reasoning
RELATED WORKS,0.07697947214076246,You are a powerful human manipulation intents
RELATED WORKS,0.07771260997067449,analyst. Infer human possible manipulation
RELATED WORKS,0.07844574780058651,intents in an image caption.
RELATED WORKS,0.07917888563049853,"In a game, a rugby player in a purple and orange jersey is"
RELATED WORKS,0.07991202346041056,running with the ball on a green field while the other
RELATED WORKS,0.08064516129032258,three in similar jerseys are running toward him.
RELATED WORKS,0.0813782991202346,Pseudo-Manipulation Description ① ① ① ② ② ②
RELATED WORKS,0.08211143695014662,🌋LLaVA
RELATED WORKS,0.08284457478005865,"①
②Original Caption Rewriting Process"
RELATED WORKS,0.08357771260997067,Pseudo-Manipulation Description Reasoning Process
RELATED WORKS,0.08431085043988269,"Figure 1: Illustration of using LLaVA to create our intent-CC3M dataset. We first use a prompt to
guide the LLaVA model in generating rewritten captions with multi-view visual descriptions. Then,
we leverage another prompt to reason pseudo-manipulation descriptions with potential intentions."
RELATED WORKS,0.08504398826979472,"understands these intentions, thereby improving the ZS-CIR model’s ability to retrieve images based
92"
RELATED WORKS,0.08577712609970674,"on human manipulation intents accurately. Unlike CIReVL [25], which employs LLMs during
93"
RELATED WORKS,0.08651026392961877,"inference for composed retrieval, introducing non-negligible computational overhead, our model is
94"
RELATED WORKS,0.0872434017595308,"lightweight and achieves comparable inference time to recent approaches.
95"
RELATED WORKS,0.08797653958944282,"Vision and Language Pre-training Models. Vision and Language Pre-training (VLP) models, like
96"
RELATED WORKS,0.08870967741935484,"CLIP [41], leverage extensive image-text pair training to achieve implicit alignment. Recent VLP
97"
RELATED WORKS,0.08944281524926687,"advancements [60, 49] utilize static models to integrate encoded image and text features, enabling
98"
RELATED WORKS,0.09017595307917889,"various zero-shot tasks [29, 49, 48]. However, current CLIP-based zero-shot learning struggles with
99"
RELATED WORKS,0.09090909090909091,"manipulation description in CIR tasks, motivating our approach, which enhances CLIP’s capabilities
100"
RELATED WORKS,0.09164222873900293,"of understanding user intentions to modify from fine-grained/long descriptions. Moreover, recent
101"
RELATED WORKS,0.09237536656891496,"studies [1, 28, 38, 37], inspired by DETR [7], employ learnable queries to select image and text
102"
RELATED WORKS,0.09310850439882698,"information. In our work, we utilize multiple learnable queries to guide the extraction of manipulation
103"
RELATED WORKS,0.093841642228739,"intentions from target descriptions, providing explanatory cues for more accurate ZS-CIR.
104"
RELATED WORKS,0.09457478005865103,"Image-text Dataset Enhancement. In the field of vision-language learning, various endeavors
105"
RELATED WORKS,0.09530791788856305,"[17, 27, 18, 39, 10] aim to enhance caption quality within existing image-text datasets. LaCLIP [17]
106"
RELATED WORKS,0.09604105571847507,"utilizes LLMs to refine raw captions. VeCLIP [27] integrates insights from raw and synthetic sources
107"
RELATED WORKS,0.0967741935483871,"using LLMs. The latest approach, ShareGPT4V [10], leverages MLLMs to generate descriptive
108"
RELATED WORKS,0.09750733137829912,"captions from deliberate prompts and corresponding image inputs. However, these methods ignore
109"
RELATED WORKS,0.09824046920821114,"human manipulation intentions, which are crucial for CIR tasks. To bridge this gap, we introduce a
110"
RELATED WORKS,0.09897360703812316,"novel dataset infused with pseudo-manipulation intentions reasoned by MLLMs.
111"
METHODOLOGY,0.09970674486803519,"3
Methodology
112"
PRELIMINARY,0.10043988269794721,"3.1
Preliminary
113"
PRELIMINARY,0.10117302052785923,"Given a reference image space I and a text description space T , Composed Image Retrieval (CIR)
114"
PRELIMINARY,0.10190615835777127,"involves a user manipulation text T ∈T describing hypothetical semantic changes to a reference
115"
PRELIMINARY,0.10263929618768329,"image Ir ∈I, aiming to retrieve a target image with its closest context from an image database
116"
PRELIMINARY,0.10337243401759531,"D = {Ii, . . . , In}. Zero-Shot CIR (ZS-CIR) approaches [45, 3, 52] sidestep this requirement
117"
PRELIMINARY,0.10410557184750734,"by training a mapping network to map the reference image into an associated text representation.
118"
PRELIMINARY,0.10483870967741936,"Specifically, these methods learn a mapping function fθ : I →Z, where Z is a pre-defined text-
119"
PRELIMINARY,0.10557184750733138,"token embedding space. fθ is trained using intermediate image representations from a specific image
120"
PRELIMINARY,0.1063049853372434,"encoder ΨI, often part of a pre-trained vision-language representation system. Template filling
121"
PRELIMINARY,0.10703812316715543,"around the manipulation text over the pseudo token embedding S∗= fθ(ΨI(Ir)) is then employed
122"
PRELIMINARY,0.10777126099706745,"to aggregate information into a target description P (e.g., “a photo of S∗, {T}).” This target
123"
PRELIMINARY,0.10850439882697947,"description serves as input for target image retrieval, encoding it using the associated pre-trained text
124"
PRELIMINARY,0.1092375366568915,"encoder ΨT . The respective matching score is cos_sim(ΨI(Ir), ΨT (P)) using cosine similarity.
125"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.10997067448680352,"3.2
Creating Intention-based Image-text Aliagment Dataset
126"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.11070381231671554,"To address the discrepancy between pre-training and retrieval in existing ZS-CIR models, we aim
127"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.11143695014662756,"to develop an intention-based image-text dataset for training mapping networks capable of aligning
128"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.11217008797653959,"intent-relevant visual information within the language space. To make a fair comparison and mitigate
129"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.11290322580645161,"the bias in human annotation, we propose to augment the widely used ZS-CIR training image-text
130"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.11363636363636363,"dataset, CC3M, through LLaVA [32], an open-source, state-of-the-art Multi-modal Large Language
131"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.11436950146627566,"Model (MLLM) known for its robust performance in vision-language tasks. However, reasoning
132"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.11510263929618768,"potential manipulation intentions from image-text pairs remains a challenging task for LLaVA.
133"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.1158357771260997,"Recent advancements in MLLMs include the development of Chain-of-Thought (CoT) prompting
134"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.11656891495601172,"[56], which enables MLLMs to produce a sequence of reasoning steps, breaking down multi-step
135"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.11730205278592376,"problems into intermediate stages and enhancing performance in complex tasks [24]. Inspired by the
136"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.11803519061583578,"CoT prompting mechanism, we explore a novel multimodal CoT prompting strategy using LLaVA to
137"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.1187683284457478,"reason pseudo-manipulation descriptions with potential intentions from image-text pairs effectively.
138"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.11950146627565983,"As illustrated in Figure 1, we divide the process of reasoning pseudo-manipulation descriptions
139"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.12023460410557185,"into two stages: the Caption Rewriting stage rewrites the original caption with multi-view visual
140"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.12096774193548387,"information for CIR tasks. The Intention Reasoning stage further understands the manipulation
141"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.1217008797653959,"intentions from rewritten captions to reason pseudo-manipulation descriptions. Specifically, in the
142"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.12243401759530792,"caption rewriting stage, we utilize the i-th image Ii and its original caption T i
ori from the CC3M,
143"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.12316715542521994,"denoted as D = {(Ii
r, T i
ori), . . . , (In
r , T n
ori)}. We guide the LLaVA model with a prompt to generate
144"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.12390029325513197,"a rewritten caption T i
rew for each image. These rewritten captions, averaging 65 tokens, include
145"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.12463343108504399,"various aspects of visual information (e.g., object, foreground/background, color, and domain style).
146"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.125366568914956,"In the intention reasoning stage, we apply an additional prompt to reason manipulation intention for
147"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.12609970674486803,"rewritten captions. This results in a more effective pseudo-manipulation description T i
int, averaging 27
148"
CREATING INTENTION-BASED IMAGE-TEXT ALIAGMENT DATASET,0.12683284457478006,"tokens. The result dataset is represented as ˜D = {(Ii
r, T i
ori, T i
rew, T i
int), . . . , (In
r , T n
ori, T n
rew, T n
int)}.
149"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.12756598240469208,"3.3
Manipulation Intention Understanding From Descriptions Before Searching
150"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1282991202346041,"Since ZS-CIR models leverage the CLIP language encoder, there is a challenge in understanding
151"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.12903225806451613,"manipulation intentions that are implicitly expressed in user descriptions. To address this challenge,
152"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.12976539589442815,"we propose a method to understand the manipulation intention before feeding into the CLIP language
153"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.13049853372434017,"encoder for accurate ZS-CIR in two modules: the Manipulation Intention Understanding captures
154"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1312316715542522,"manipulation intentions and maps them into several pseudo tokens. The Reasoning Distillation
155"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.13196480938416422,"further aligns the context of desired pseudo-word tokens closely with human intention by leveraging
156"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.13269794721407624,"pseudo-manipulation description to enhance the models’ ability to understand human intention.
157"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.13343108504398826,"Image and Context Encoding. For a given sample (Ir, Tori, Trew, Tint) from intent-CC3M. Since
158"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.13416422287390029,"the pre-trained vision-language models are strong at modeling the cross-modal implicit alignment.
159"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1348973607038123,"Initially, we employ the frozen image encoder ΨI from the CLIP model to encode the global image
160"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.13563049853372433,"feature of the reference image Ir as v = ΨI(Ir) = {vi}d
i=1 ∈Rd×1. Subsequently, we apply a
161"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.13636363636363635,"simple mapping network fθ with parameters θ to extract a pseudo token embedding S∗= fθ(v).
162"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.13709677419354838,"Considering our focus on manipulation intention understanding for ZS-CIR, fθ is structured as a
163"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1378299120234604,"simple three-layer fully-connected network. We then construct a target description P formatted
164"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.13856304985337242,"as “a photo of S∗, {T}”. We consider two scenarios for manipulation intention understanding:
165"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.13929618768328444,"deducing intention information from concise texts (e.g., original caption) or integrating it from
166"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.14002932551319647,"lengthy texts(e.g., rewritten caption). Accordingly, the text T is composed randomly within a batch
167"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.14076246334310852,"according to the following distribution: 50% original caption Trew and 30% rewritten caption Tori to
168"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.14149560117302054,"learn manipulation intention understanding, 20% pseudo-manipulation description Tint to ensure
169"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.14222873900293256,"training stability (details are in Appendix C). We feed the target description to the language encoder
170"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1429618768328446,"ΨT of frozen CLIP to represent the target description P by a set of language feature vectors T
171"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1436950146627566,"={ti}m
i=1 ⊆Rd×m. t1 represents the [CLS] embedding tcls with global information of image and
172"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.14442815249266863,"caption, while other ones denote word embeddings ˜T ={ti}m
i=2.
173"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.14516129032258066,"Manipulation Intentions Understanding. Given the word embeddings of the target descriptions,
174"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.14589442815249268,"this module aims to capture different manipulation intentions, thereby enhancing the CLIP lan-
175"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1466275659824047,"guage encoder’s capability to understand users’ intents for manipulation. To capture different
176"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.14736070381231672,"manipulation intentions, we introduce a set of learnable query embeddings for guidance, denoted
177"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.14809384164222875,"as X = {xk}n
k=1 ∈Rd×n, where d is the embedding dimension and n is the number of queries.
178"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.14882697947214077,"Each query xk represents a kind of manipulation intention. As depicted in Figure 2(left), we im-
179"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1495601173020528,"plement cross-attention mechanisms to extract intention-relevant contextual information from the
180"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.15029325513196481,"word embeddings ˜T = {ti}m
i=2 using the learnable queries X. The cross-attention operation in-
181"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.15102639296187684,"volves three primary steps. First, we compute the query, key and value through linear projections,
182"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.15175953079178886,Image and Context Encoding
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.15249266862170088,Input Image
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1532258064516129,Language
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.15395894428152493,Encoder
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.15469208211143695,"Image
Encoder"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.15542521994134897,"a  photo
of  𝑺∗"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.156158357771261,Target Description
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.15689149560117302,",  A rugby
… ... ... ..."
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.15762463343108504,Cross-Attention
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.15835777126099707,Feed Forward 𝑞 #𝑻 ×N
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1590909090909091,"a  
photo"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1598240469208211,"of  𝑺∗
,  In
a …"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.16055718475073313,Language
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.16129032258064516,Encoder
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.16202346041055718,Distill Loss
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1627565982404692,"Mapping
Network"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.16348973607038123,Manipulation Intention Understanding Training
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.16422287390029325,Pseudo-Manipulation
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.16495601173020527,Description
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1656891495601173,Candidate Images
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.16642228739002932,"Text-to-image
Retrieval"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.16715542521994134,Inference with De-MINDS
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.16788856304985336,Reference Image 𝑓$
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.16862170087976538,Language
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1693548387096774,Encoder
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.17008797653958943,"Image
Encoder"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.17082111436950145,"Mapping
Network 𝑓$"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.17155425219941348,"is
a  photo  of  
𝑺∗
smaller
and
not
eating
...
De
-MINDS"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.17228739002932553,Language
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.17302052785923755,Encoder
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.17375366568914957,"Image
Encoder
Retrieved Image"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1744868035190616,Target Description
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.17521994134897362,"Gate
Alignment Loss"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.17595307917888564,Manipulation Intention
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.17668621700879766,Understanding Gate game
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1774193548387097,"Reasoning Distillation ①
②"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1781524926686217,"Manipulation Intention Understanding Process
Reasoning Distillation Process ①
② ②
① ① ① ② ① ① ①"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.17888563049853373,"Figure 2: An overview of our De-MINDS. Pre-training (left): Map the image to a pseudo token S∗,
and understand the intention from the target description. Inference (right): Map the inference image
to S∗to construct the target description and understand manipulation intention for ZS-CIR."
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.17961876832844575,"i.e., Q = XW Q, K = [X, ˜T ]W K, V = [X, ˜T ]W V . [X, ˜T ] denotes concatenating the two
183"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.18035190615835778,"matrices, which enhances the interaction between learnable queries and word embeddings with better
184"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1810850439882698,"performance. Then, the learnable queries from the current cross-attention block Xi is calculated as:
185"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.18181818181818182,"Xi
att = Att(Q, K, V ) = softmax QK⊤ √ d !"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.18255131964809385,"V , Xi = FFW(Xi
att + Xi−1) + Xi
att
(1)"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.18328445747800587,"where Xi−1 are learnable queries from the previous block and FFW(·) denotes 2-layer feed-forward
186"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1840175953079179,"networks. the refined query embeddings X are then fed into the frozen language encoder ΨT of
187"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.18475073313782991,"CLIP to extract the intention embedding as t∗= ΨT (Xn) = {ti
∗}d
i=1 ∈Rd×1 (d = 768).
188"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.18548387096774194,"Reasoning Distillation. Given the intention embedding t∗, the AI agent needs to further align with
189"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.18621700879765396,"human manipulation intention. Specifically, we aim to reduce the distance between the intention
190"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.18695014662756598,"embedding and the corresponding pseudo-manipulation description’s [CLS] word embedding, which
191"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.187683284457478,"represents the MLLM’s intention embedding while ensuring that each embedding remains distinct
192"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.18841642228739003,"and discriminative. Given the intention embeddings Tint = {ti
∗}N
i=1, where N is the number of
193"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.18914956011730205,"images in ˜D, and the corresponding MLLM’s intention embeddings ˜t∗= ΨT (Tint) ∈˜Tint we
194"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.18988269794721407,"employ a symmetric contrastive loss inspired by SimCLR [11, 13, 45] as follows:
195"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1906158357771261,"Ldistil = Ls2t(t∗,˜t∗) + Lt2s(˜t∗, t∗)
(2)"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.19134897360703812,"The two contrastive loss terms are defined as:
196"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.19208211143695014,"Ls2t(t∗,˜t∗) = −1 |B| X"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.19281524926686217,"i∈B
log
eτ(ti
∗)T ˜ti
∗
P"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1935483870967742,"j∈B eτ(ti∗)T ˜tj
∗, Lt2s(ˆt∗,˜t∗) = −1 |B| X"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1942815249266862,"i∈B
log
eτ(˜ti
∗)T ti
∗
P"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.19501466275659823,"j∈B eτ(˜ti
∗)T tj
∗
(3)"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.19574780058651026,"where B is the number of images in a batch and τ is a temperature hyper-parameter that controls the
197"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.19648093841642228,"strength of penalties on hard negative samples.
198"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.1972140762463343,"Cross-Modal Alignment. Given the embedding of user manipulation intention, this module aims
199"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.19794721407624633,"to form a target embedding optimized for retrieval. Since the nature of CIR, both the reference
200"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.19868035190615835,"image and the manipulation intention form a comprehensive context that defines the target image. To
201"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.19941348973607037,"dynamically control the influence of manipulation intentions on the retrieval process, we introduce a
202"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.2001466275659824,"learnable scalar gate that decides the contribution of the manipulation intention information t∗and
203"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.20087976539589442,"integrates the global information tcls to form the final target embedding ˆt as follows:
204"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.20161290322580644,ˆt = tcls + gate · t∗
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.20234604105571846,"Then, we aim to match a target image to its paired target embedding while separating unpaired
205"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.20307917888563048,"ones. We minimize the symmetric contrastive loss between the image embedding v and the target
206"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.20381231671554254,"embedding ˆt as follows:
207"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.20454545454545456,"Lalign = Ls2t(ˆt, v) + Lt2s(v,ˆt)
(4)
where Ls2t and Lt2s are two contrastive loss terms as Eq.3. The final loss used to optimize is:
208"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.20527859237536658,"L = Ldistill + Lalign
(5)"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.2060117302052786,"Inference with De-MINDS. In the inference stage, we compose the reference image with the paired
209"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.20674486803519063,"manipulation description and compare the composed query with candidate images for retrieval. As
210"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.20747800586510265,"shown in Figure 2 (right), we compose the pseudo token embedding S∗of the image from the
211"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.20821114369501467,"mapping network with the text description and feed it to the pre-trained language encoder of CLIP.
212"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.2089442815249267,"The result is embedded by the text encoder and compared to the visual features of candidate images.
213"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.20967741935483872,"Since we focus on studying the manipulation intention understanding searching for ZS-CIR, we utilize
214"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.21041055718475074,"the same prompt in the most recent works [45, 52] for a fair comparison. We show prompt examples
215"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.21114369501466276,"for different ZS-CIR tasks. In all examples, [*] indicates the pseudo token from the mapping
216"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.2118768328445748,"network: (a) Domain conversion aims to modify the domain of the reference image. The prompt
217"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.2126099706744868,"is defined as a [domain tag] of [*]; (b) Object composition retrieves an image that contains
218"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.21334310850439883,"an object in the reference image and other object tags. The prompt is in the format of a photo
219"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.21407624633431085,"of [*], [obj1 tag] and [obj2 tag], . . . , and [objn tag]; (c) Sentence manipulation
220"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.21480938416422288,"modifies the reference image based on a sentence. We simply append the sentence with the special
221"
MANIPULATION INTENTION UNDERSTANDING FROM DESCRIPTIONS BEFORE SEARCHING,0.2155425219941349,"token as a photo of [*], [sentence]. More details are in Appendix D.3.
222"
EXPERIMENTS,0.21627565982404692,"4
Experiments
223"
EXPERIMENTS,0.21700879765395895,"Datasets. We evaluate our model on four ZS-CIR datasets, i.e., COCO [31] for object composition,
224"
EXPERIMENTS,0.21774193548387097,"ImageNet [16, 21] for domain conversion, CIRR [33] for object/scene manipulation, and Fashion-IQ
225"
EXPERIMENTS,0.218475073313783,"[57] for attribute manipulation. All the dataset settings and evaluation metrics (Recall@K) follow the
226"
EXPERIMENTS,0.21920821114369501,"recent works [45, 52] for a fair comparison.
227"
EXPERIMENTS,0.21994134897360704,"(1) Domain conversion. This dataset comprises 16,983 images of 200 classes from four domains,
228"
EXPERIMENTS,0.22067448680351906,"i.e., cartoon, origami, toy, and sculpture. We use the prompt (a) in inference. (2) Object composition.
229"
EXPERIMENTS,0.22140762463343108,"The dataset contains images with corresponding lists of object labels and instance masks of query
230"
EXPERIMENTS,0.2221407624633431,"images. We randomly crop one object and mask its background using its instance mask to create a
231"
EXPERIMENTS,0.22287390029325513,"reference image. We use the prompt (b) in inference. (3) Object/scene manipulation. A reference
232"
EXPERIMENTS,0.22360703812316715,"image is an instruction for manipulating an object or the background scene. We apply the prompt
233"
EXPERIMENTS,0.22434017595307917,"(c) in inference. (4) Attribute manipulation. This dataset includes various description sentences for
234"
EXPERIMENTS,0.2250733137829912,"manipulating image attributes. We utilize the prompt (c) in inference. More details in Appendix D.2.
235"
EXPERIMENTS,0.22580645161290322,"Implementation Details. Generating one pseudo-manipulation description through LLaVA-1.6-13B
236"
EXPERIMENTS,0.22653958944281524,"[32] for the entire Conceptual Caption dataset [47], which comprises 3M images (CC3M), requires
237"
EXPERIMENTS,0.22727272727272727,"approximately 625 hours on 5 A100 (80G) GPUs. For training De-MINDS, We utilize the CC3M and
238"
EXPERIMENTS,0.2280058651026393,"adopt ViT-L/14 CLIP [41] pre-trained on 400M image-text paired data. We employ AdamW [34] with
239"
EXPERIMENTS,0.2287390029325513,"a learning rate of 1 × 10−6, weight decay of 0.1, and a linear warmup of 10000 steps. The number
240"
EXPERIMENTS,0.22947214076246333,"of cross-attention blocks is 6. The number of learnable queries is 4. The batch size for contrastive
241"
EXPERIMENTS,0.23020527859237536,"learning is 1024. To improve training stability, we initialize the learnable scalar of tanh-gating to 0
242"
EXPERIMENTS,0.23093841642228738,"[2]. For training Context-I2W and SEARLE, we keep the same setting reported in their paper, only
243"
EXPERIMENTS,0.2316715542521994,"replacing the original captions with our pseudo-manipulation descriptions. All models are trained on
244"
EXPERIMENTS,0.23240469208211142,"4 NVIDIA A100 (80G) GPUs. To ensure reliable results, we report the performance averaged over
245"
EXPERIMENTS,0.23313782991202345,"three trials. More details are in Appendix D.1.
246"
QUANTITATIVE AND QUALITATIVE RESULTS,0.23387096774193547,"4.1
Quantitative and Qualitative Results
247"
QUANTITATIVE AND QUALITATIVE RESULTS,0.23460410557184752,"We compare De-MINDS with several ZS-CIR methods, including: 1) Pic2Word [45]: Maps the
248"
QUANTITATIVE AND QUALITATIVE RESULTS,0.23533724340175954,"visual features of a reference image into a pseudo-word token within the CLIP token embedding
249"
QUANTITATIVE AND QUALITATIVE RESULTS,0.23607038123167157,"space; 2) SEARLE-XL [3]: Similar to Pic2Word, further integrating the pseudo-word token with the
250"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2368035190615836,"caption generated by GPT [6] and distilled for efficiency; 3) Context-I2W [52]: Selectively extracts
251"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2375366568914956,"text-relevant visual information from the reference image before mapping it into a pseudo-word
252"
QUANTITATIVE AND QUALITATIVE RESULTS,0.23826979472140764,"token; 4) CIReVL [25]: Uses LLMs to enhance the manipulation description during inference; and
253"
QUANTITATIVE AND QUALITATIVE RESULTS,0.23900293255131966,"5) LinCIR [20]: Masks subjects in captions from various image-text datasets for training. For a fair
254"
QUANTITATIVE AND QUALITATIVE RESULTS,0.23973607038123168,"comparison, we present the reported results of methods relying on the ViT-L/14 CLIP model.
255"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2404692082111437,"Moreover, we compare De-MINDS with 6) SEARLE-XL* and Context-I2W*: Replace the original
256"
QUANTITATIVE AND QUALITATIVE RESULTS,0.24120234604105573,"captions with our pseudo-manipulation description, and standard ZS-CIR methods, including 7)
257"
QUANTITATIVE AND QUALITATIVE RESULTS,0.24193548387096775,"Text-only: Computes similarity based on the CLIP features of descriptions and candidate images; 8)
258"
QUANTITATIVE AND QUALITATIVE RESULTS,0.24266862170087977,"Image-only: Retrieves the most similar images to the reference image; and 9) Image + Text: Sums
259"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2434017595307918,"the CLIP features of the reference image and the description.
260"
QUANTITATIVE AND QUALITATIVE RESULTS,0.24413489736070382,Table 1: Results on Fashion-IQ for attribute manipulation.
QUANTITATIVE AND QUALITATIVE RESULTS,0.24486803519061584,"Dress
Shrit
TopTee
Average"
QUANTITATIVE AND QUALITATIVE RESULTS,0.24560117302052786,"Methods
Conferences
R10
R50
R10
R50
R10
R50
R10
R50"
QUANTITATIVE AND QUALITATIVE RESULTS,0.24633431085043989,"Image-only
–
5.4
13.9
9.9
20.8
8.3
17.7
7.9
17.5
Text-only
–
13.6
29.7
18.9
31.8
19.3
37.0
17.3
32.9
Image+Text
–
16.3
33.6
21.0
34.5
22.2
39.0
19.8
35.7
Pic2Word [45]
CVPR 2023
20.0
40.2
26.2
43.6
27.9
47.4
24.7
43.7
CIReVL [25]
ICLR 2024
24.6
44.8
29.5
47.4
31.4
53.7
28.6
48.6
LinCIR [20]
CVPR 2024
20.9
42.4
29.1
46.8
28.8
50.2
26.3
46.5"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2470674486803519,"SEARLE-XL [3]
ICCV 2023
20.3
43.2
27.4
45.7
29.3
50.2
25.7
46.3
SEARLE-XL*
–
22.7
45.0
29.4
47.9
30.2
51.4
27.4
48.1"
QUANTITATIVE AND QUALITATIVE RESULTS,0.24780058651026393,"Context-I2W [52]
AAAI 2024
23.1
45.3
29.7
48.6
30.6
52.9
27.8
48.9
Context-I2W*
–
23.9
46.9
30.4
49.7
31.1
53.8
28.5
50.1"
QUANTITATIVE AND QUALITATIVE RESULTS,0.24853372434017595,"De-MINDS
–
25.2
48.7
31.0
51.2
32.9
55.7
29.7
51.9 Ours"
QUANTITATIVE AND QUALITATIVE RESULTS,0.24926686217008798,"is black and long 
sleeves with red 
and white designs"
QUANTITATIVE AND QUALITATIVE RESULTS,0.25,at the center Query
QUANTITATIVE AND QUALITATIVE RESULTS,0.250733137829912,B00AN545PI.png
QUANTITATIVE AND QUALITATIVE RESULTS,0.25146627565982405,"has longer sleeves 
and color is blue with"
QUANTITATIVE AND QUALITATIVE RESULTS,0.25219941348973607,"buttoned front and 
has double pockets"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2529325513196481,"is a shorter, sexier,"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2536656891495601,"tighter fit and a 
lighter color with a"
QUANTITATIVE AND QUALITATIVE RESULTS,0.25439882697947214,waistband
QUANTITATIVE AND QUALITATIVE RESULTS,0.25513196480938416,Context-I2W
QUANTITATIVE AND QUALITATIVE RESULTS,0.2558651026392962,Figure 3: Results on the attribute manipulation task Ours
QUANTITATIVE AND QUALITATIVE RESULTS,0.2565982404692082,Origami Toy Query
QUANTITATIVE AND QUALITATIVE RESULTS,0.25733137829912023,Cartoon
QUANTITATIVE AND QUALITATIVE RESULTS,0.25806451612903225,Sculpture
QUANTITATIVE AND QUALITATIVE RESULTS,0.2587976539589443,Context-I2W
QUANTITATIVE AND QUALITATIVE RESULTS,0.2595307917888563,Figure 4: Results on the domain conversion task.
QUANTITATIVE AND QUALITATIVE RESULTS,0.2602639296187683,"Tables 1 to 4 present the quantitative results, while Figures 3 to 6 display the corresponding qualitative
261"
QUANTITATIVE AND QUALITATIVE RESULTS,0.26099706744868034,"results of our model and the most recent works, CIReVL and Context-I2W. The attribute manipulation
262"
QUANTITATIVE AND QUALITATIVE RESULTS,0.26173020527859236,"task requires accurately localizing specific attributes within the entire image. As demonstrated in Table
263"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2624633431085044,"1, De-MINDS outperforms existing ZS-CIR models significantly, achieving an average improvement
264"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2631964809384164,"of 2.20% over the State-of-the-Art (SoTA) model, CIReVL. CIReVL’s dependency on an LLM at
265"
QUANTITATIVE AND QUALITATIVE RESULTS,0.26392961876832843,"inference introduces substantial computational overhead during retrieval. De-MINDS tackles this
266"
QUANTITATIVE AND QUALITATIVE RESULTS,0.26466275659824046,"challenge by extracting fashion-relevant intention within manipulation descriptions into a series of
267"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2653958944281525,"implicit pseudo-tokens for CLIP retrieval. This approach is more efficient and suitable for models than
268"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2661290322580645,"relying on explicit, often noisy, LLM analysis results. Figure 3 further illustrates how De-MINDS
269"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2668621700879765,"effectively understand complex fashion-relevant attributes in manipulation descriptions, such as a
270"
QUANTITATIVE AND QUALITATIVE RESULTS,0.26759530791788855,"sexier style with a waistband (row 1), black color with a special design in the center (row 2), and
271"
QUANTITATIVE AND QUALITATIVE RESULTS,0.26832844574780057,"longer sleeves with two pockets in blue (row 3), facilitating more accurate searching.
272"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2690615835777126,"We further assess De-MINDS’ capability in foreground/background differentiation and fine-grained
273"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2697947214076246,"image editing through the object/scene manipulation task (Table 2). De-MINDS consistently surpasses
274"
QUANTITATIVE AND QUALITATIVE RESULTS,0.27052785923753664,"existing ZS-CIR models, achieving an average performance improvement of 2.05% over the best
275"
QUANTITATIVE AND QUALITATIVE RESULTS,0.27126099706744866,"model. This enhancement is attributed to De-MINDS’ approach of extracting human intention from
276"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2719941348973607,"manipulation descriptions before searching, enhancing the ability of the CLIP language encoder
277"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2727272727272727,"to understand the user’s intention to modify. In Figure 5, De-MINDS accurately understands
278"
QUANTITATIVE AND QUALITATIVE RESULTS,0.27346041055718473,"manipulation intention to change the number of an object and modify the background (row 1), alter
279"
QUANTITATIVE AND QUALITATIVE RESULTS,0.27419354838709675,"the stage and remove an overlapping object (row 2), adjust the camera focus, age of a dog, and
280"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2749266862170088,"remove a specific object (row 3), and modify the style of an image with a specific design (row 4).
281"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2756598240469208,"In the object composition experiments (Table 3), De-MINDS significantly outperforms the current
282"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2763929618768328,"SoTA model by an average of 4.30%. These results prove the effectiveness of De-MINDS in
283"
QUANTITATIVE AND QUALITATIVE RESULTS,0.27712609970674484,"accurately mapping visual information to the language token space via bridges the gap between
284"
QUANTITATIVE AND QUALITATIVE RESULTS,0.27785923753665687,"pre-training and retrieval, which facilitates the combination of multiple objects, as shown in Figure 6.
285"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2785923753665689,"Moreover, in the domain conversion results (Table 4), De-MINDS consistently outperforms existing
286"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2793255131964809,"approaches and notably surpasses the SoTA Context-I2W by an average of 4.35%. As illustrated in
287"
QUANTITATIVE AND QUALITATIVE RESULTS,0.28005865102639294,"Figure 4, De-MINDS accurately maps objects within complex scenes (e.g., a saxophonist in the street,
288"
QUANTITATIVE AND QUALITATIVE RESULTS,0.28079178885630496,"a bald eagle on wood, a monkey in the forest, and a sea lion in the water). In contrast, Context-I2W
289"
QUANTITATIVE AND QUALITATIVE RESULTS,0.28152492668621704,"struggles to select the intention-relevant local visual features due to its reliance on image caption
290"
QUANTITATIVE AND QUALITATIVE RESULTS,0.28225806451612906,"without intention, whereas our pseudo-manipulation descriptions are effectively addressed.
291"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2829912023460411,"Table 2: Results on CIRR for object
manipulation task."
QUANTITATIVE AND QUALITATIVE RESULTS,0.2837243401759531,"Methods
R1
R5
R10 R50"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2844574780058651,"Image-only
7.4
23.6 34.0 57.4
Text-only
20.9 44.8 55.5 79.1
Image+Text
12.4 36.2 49.1 78.2
Pic2Word [45]
23.9 51.7 65.3 87.8
CIReVL [25]
24.6 52.3 64.9 86.3
LinCIR [20]
25.0 53.3 66.7
–"
QUANTITATIVE AND QUALITATIVE RESULTS,0.28519061583577715,"SEARLE-XL [3]
24.2 52.4 66.3 88.6
SEARLE-XL*
25.4 54.1 66.9 89.3"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2859237536656892,"Context-I2W [52] 25.6 55.1 68.5 89.8
Context-I2W*
26.3 55.7 69.0 90.2"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2866568914956012,"De-MINDS
27.3 57.0 71.3 91.6"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2873900293255132,"Table 3: Results on COCO for object
composition task."
QUANTITATIVE AND QUALITATIVE RESULTS,0.28812316715542524,"Methods
R1
R5
R10"
QUANTITATIVE AND QUALITATIVE RESULTS,0.28885630498533726,"Image-only
8.6
15.4
18.9
Text-only
6.1
15.7
23.5
Image+Text
10.2
20.2
26.6
Pic2Word [45]
11.5
24.8
33.4"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2895894428152493,"Context-I2W [52]
13.5
28.5
38.1
Context-I2W*
14.3
29.7
40.5"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2903225806451613,"De-MINDS
15.7
33.2
44.1"
QUANTITATIVE AND QUALITATIVE RESULTS,0.29105571847507333,"Ours
Context-I2W
Query"
QUANTITATIVE AND QUALITATIVE RESULTS,0.29178885630498536,Target two animals
QUANTITATIVE AND QUALITATIVE RESULTS,0.2925219941348974,"resting on white 
towel rather showing"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2932551319648094,one black
QUANTITATIVE AND QUALITATIVE RESULTS,0.2939882697947214,"Take the picture 
closer, make the dog 
younger, and remove"
QUANTITATIVE AND QUALITATIVE RESULTS,0.29472140762463345,the person
QUANTITATIVE AND QUALITATIVE RESULTS,0.29545454545454547,dev-224-2-img1.png
QUANTITATIVE AND QUALITATIVE RESULTS,0.2961876832844575,Make dog sleep in
QUANTITATIVE AND QUALITATIVE RESULTS,0.2969208211143695,"couch or ground 
and remove objects"
QUANTITATIVE AND QUALITATIVE RESULTS,0.29765395894428154,from its mouth
QUANTITATIVE AND QUALITATIVE RESULTS,0.29838709677419356,make it a poster of
QUANTITATIVE AND QUALITATIVE RESULTS,0.2991202346041056,"the dog, and have"
QUANTITATIVE AND QUALITATIVE RESULTS,0.2998533724340176,"text above and 
below the animal
Figure 5: Retrieved results on the object manipulation task Ours"
QUANTITATIVE AND QUALITATIVE RESULTS,0.30058651026392963,"train, light, 
people, railway,"
QUANTITATIVE AND QUALITATIVE RESULTS,0.30131964809384165,"package, sky Query"
QUANTITATIVE AND QUALITATIVE RESULTS,0.3020527859237537,"man, woman, table,"
QUANTITATIVE AND QUALITATIVE RESULTS,0.3027859237536657,"bottle, food, knife,"
QUANTITATIVE AND QUALITATIVE RESULTS,0.3035190615835777,"fork, wine"
QUANTITATIVE AND QUALITATIVE RESULTS,0.30425219941348974,"leaves, person, food,"
QUANTITATIVE AND QUALITATIVE RESULTS,0.30498533724340177,"chair, table, plate,"
QUANTITATIVE AND QUALITATIVE RESULTS,0.3057184750733138,"fork, bread"
QUANTITATIVE AND QUALITATIVE RESULTS,0.3064516129032258,Context-I2W
QUANTITATIVE AND QUALITATIVE RESULTS,0.30718475073313783,"Figure 6: Retrieved results on the object composition task.
Table 4: Results on ImageNet for domain conversion."
QUANTITATIVE AND QUALITATIVE RESULTS,0.30791788856304986,"Cartoon
Origami
Toy
Sculpture
Average"
QUANTITATIVE AND QUALITATIVE RESULTS,0.3086510263929619,"Methods
Conferences
R10
R50
R10
R50
R10
R50
R10
R50
R10
R50"
QUANTITATIVE AND QUALITATIVE RESULTS,0.3093841642228739,"Image-only
–
0.3
4.5
0.2
1.8
0.6
5.7
0.3
4.0
0.4
4.0
Text-only
–
0.2
1.1
0.8
3.7
0.8
2.4
0.4
2.0
0.5
2.3
Image+Text
–
2.2
13.3
2.0
10.3
1.2
9.7
1.6
11.6
1.7
11.2
Pic2Word [45]
CVPR 2023
8.0
21.9
13.5
25.6
8.7
21.6
10.0
23.8
10.1
23.2"
QUANTITATIVE AND QUALITATIVE RESULTS,0.3101173020527859,"Context-I2W [52]
AAAI 2024
10.2
26.1
17.5
28.7
11.6
27.4
12.1
28.2
12.9
27.6
Context-I2W*
–
11.2
27.4
18.7
30.4
12.5
29.8
13.7
31.4
14.0
29.8"
QUANTITATIVE AND QUALITATIVE RESULTS,0.31085043988269795,"De-MINDS
–
13.3
31.2
20.3
34.5
14.7
31.7
16.5
34.7
16.2
33.0"
ABLATION STUDY,0.31158357771260997,"4.2
Ablation Study
292"
ABLATION STUDY,0.312316715542522,"In Table 5, we evaluate the contributions of De-MINDS components on the CIRR and FashionIQ
293"
ABLATION STUDY,0.313049853372434,"datasets. (1) In models ‘2-3’, we assess the significance of the intent-CC3M dataset. Replacing the
294"
ABLATION STUDY,0.31378299120234604,"pseudo-manipulation description with original captions (model ‘2’) results in an average performance
295"
ABLATION STUDY,0.31451612903225806,"drop of 3.80%, demonstrating training with intent-CC3M benefit for aligning intention-relevant
296"
ABLATION STUDY,0.3152492668621701,"visual information. Using a single prompt for pseudo-manipulation descriptions (model ‘3’) causes a
297"
ABLATION STUDY,0.3159824046920821,"3.14% performance decline, indicating that CoT prompting enhances MLLM in reasoning potential
298"
ABLATION STUDY,0.31671554252199413,"manipulation intention. (2) In models ‘4-6’, we evaluate key modules in the manipulation intention
299"
ABLATION STUDY,0.31744868035190615,"understanding process. Without intention embeddings from De-MINDS (model ‘4’), performance
300"
ABLATION STUDY,0.3181818181818182,"drops by 4.02% on average, proving De-MINDS’s importance in CIR. Removing the global feature
301"
ABLATION STUDY,0.3189149560117302,"tcls (model ‘5’) leads to a 2.38% performance decline, highlighting the necessity of comprehensive
302"
ABLATION STUDY,0.3196480938416422,"both global and intention information. Summing global and intention features directly (model
303"
ABLATION STUDY,0.32038123167155425,"‘6’) causes a 1.64% performance drop, indicating the need for adaptive capture of complementary
304"
ABLATION STUDY,0.32111436950146627,"information. (3) In models ‘7-9’, we assess De-MINDS’s training strategies. Using only original
305"
ABLATION STUDY,0.3218475073313783,"captions as T (model ‘7’) reduces training stability, resulting in a 1.62% performance drop. Without
306"
ABLATION STUDY,0.3225806451612903,"the distillation loss (model ‘8’) or replacing it with a cosine loss (model ‘9’) leads to performance
307"
ABLATION STUDY,0.32331378299120234,"drops of 3.58% and 1.54%, respectively, indicating the necessity of symmetric contrastive loss for
308"
ABLATION STUDY,0.32404692082111436,"distilling MLLM’s reasoning ability. In models ‘10-12’, we evaluate alternative solutions. Not
309"
ABLATION STUDY,0.3247800586510264,"utilizing T for image-to-text mapping (model ‘10’) results in a 2.30% performance drop, confirming
310"
ABLATION STUDY,0.3255131964809384,"the effectiveness of our pseudo-manipulation descriptions. Applying MiniGPT-4 [61] to generate the
311"
ABLATION STUDY,0.3262463343108504,"intent-CC3M dataset (model ‘11’) results in a 1.18% performance drop, suggesting that a superior
312"
ABLATION STUDY,0.32697947214076245,"MLLM model benefits pseudo-manipulation description quality. Leveraging the LLaMA [53] rewrite
313"
ABLATION STUDY,0.3277126099706745,"Table 5: Ablation study of main components
on CIRR and FashionIQ."
ABLATION STUDY,0.3284457478005865,"CIRR
Fashion-IQ"
ABLATION STUDY,0.3291788856304985,"Methods
R1
R5
R10
R10
R50"
FULL MODEL,0.32991202346041054,"1.
full model
27.3 57.0 71.3 29.7
51.9
Significant of inetent-CC3M
2.
w/o intent-CC3M
24.6 53.7 67.1 26.0
46.8
3.
w/o CoT
25.2 54.3 67.8 26.7
47.5
Key modules of De-MINDS process
4.
w/o De-MINDS
24.0 53.5 67.2 25.8
46.6
5.
w/o global feature
25.5 55.2 68.0 27.3
49.6
6.
w/o gate
25.9 55.3 69.5 27.9
50.4
Training Strategies
7.
w/o construct T
26.2 55.6 69.3 27.8
50.2
8.
w/o distil
24.8 53.9 67.3 26.3
47.0
9.
cos distll
26.2 55.5 69.7 27.9
50.2
Alternative solutions
10. a photo of S∗
25.5 55.2 67.9 27.5
49.6
11. MiniGPT4’s caption 26.4 55.7 70.2 28.2
50.8
12. LLM’s caption
25.2 53.7 67.2 26.9
47.2"
FULL MODEL,0.33064516129032256,Shows another room with a
FULL MODEL,0.3313782991202346,"side table and chair, except 
they are each in front of two 
windows in a corner and the"
FULL MODEL,0.3321114369501466,chairs have cushions.
FULL MODEL,0.33284457478005863,Manipulation Description
FULL MODEL,0.33357771260997066,"Remove all dogs and basket,"
FULL MODEL,0.3343108504398827,Add adult dog standing and
FULL MODEL,0.3350439882697947,"alert, Place dog on cement"
FULL MODEL,0.3357771260997067,"pavement with handler 
seated behind dog's head."
FULL MODEL,0.33651026392961875,Learnable Queries
FULL MODEL,0.33724340175953077,"Standing guinea pig on the 
background of toys instead of"
FULL MODEL,0.3379765395894428,a white-red puppy sleeping
FULL MODEL,0.3387096774193548,on a boot on the ground.
FULL MODEL,0.33944281524926684,"Reference Image
Retrieved Image"
FULL MODEL,0.34017595307917886,"Place dog standing on hind 
legs, Add another dog, and 
Place dogs in a commercial, 
industrial setting with orange"
FULL MODEL,0.3409090909090909,background.
FULL MODEL,0.3416422287390029,"Figure 7: Visualization of the top two attention
words for each learnable query, different colors
denoting the results corresponding to each query."
FULL MODEL,0.34237536656891493,"CC3M dataset [17] (model ‘12’) causes a 3.40% performance drop, indicating the necessity of MLLM
314"
FULL MODEL,0.34310850439882695,"for generating pseudo-manipulation description with multi-view supplementary image detail.
315"
ANALYSIS,0.34384164222873903,"4.3
Analysis
316"
ANALYSIS,0.34457478005865105,"Interpretability of Learnable Query. In Figure 7, we visualize the top two attention words of each
317"
ANALYSIS,0.3453079178885631,"learnable query from the last block, demonstrating the distinct focus of the four queries. Specifically,
318"
ANALYSIS,0.3460410557184751,"the first two queries mainly focus on object and attribute information, while the last two queries
319"
ANALYSIS,0.3467741935483871,"mostly consider foreground/background and relation information. These attention maps substantiate
320"
ANALYSIS,0.34750733137829914,"De-MINDS’s interpretability in extracting specific intention across various descriptions, supporting
321"
ANALYSIS,0.34824046920821117,"the understanding of intention from manipulation descriptions.
322"
ANALYSIS,0.3489736070381232,"Effectiveness and Efficiency Analysis. Our approach achieves significant improvements on four
323"
ANALYSIS,0.3497067448680352,"widely compared ZR-CIR tasks from 2.05% to 4.35% over the SoTA models. Designed for under-
324"
ANALYSIS,0.35043988269794724,"standing manipulation intention, the model size of De-MINDS(58.5M) is larger than the simple
325"
ANALYSIS,0.35117302052785926,"3-layer MLP mapping (0.9M) of Pic2Word. Consequently, our training time (20 hours) is 6 hours
326"
ANALYSIS,0.3519061583577713,"longer than Pic2Word under the same settings. Notably, our inference time (0.017s) is ×58 faster
327"
ANALYSIS,0.3526392961876833,"than CIReVL (∼1s), which uses LLM for inference, and only 0.005s slower than Pic2Word. It’s
328"
ANALYSIS,0.3533724340175953,"worth noting that our model using just 50% of the pre-training data achieves comparable performance
329"
ANALYSIS,0.35410557184750735,"to SoTA models (details are in Appendix A.2).
330"
ANALYSIS,0.3548387096774194,"Limitation. While the training process for De-MINDS does not introduce significant additional
331"
ANALYSIS,0.3555718475073314,"memory or computational overhead, generating pseudo-manipulation descriptions using MLLMs
332"
ANALYSIS,0.3563049853372434,"can be computationally intensive. Moreover, these pseudo descriptions are not filtered, potentially
333"
ANALYSIS,0.35703812316715544,"introducing irrelevant details that do not align with actual human manipulation intention. Our paper
334"
ANALYSIS,0.35777126099706746,"aims to bridge the gap between pre-training and retrieval in ZS-CIR models and introduce a novel
335"
ANALYSIS,0.3585043988269795,"framework to enhance the model’s capability to understand user intention. Future work could explore
336"
ANALYSIS,0.3592375366568915,"more efficient methods to generate pseudo-manipulation descriptions while maintaining performance.
337"
CONCLUSION,0.35997067448680353,"5
Conclusion
338"
CONCLUSION,0.36070381231671556,"In this paper, we introduce intent-CC3M, an intention-based dataset featuring pseudo-manipulation
339"
CONCLUSION,0.3614369501466276,"descriptions reasoned through chain-of-thought prompting by an MLLM for training mapping
340"
CONCLUSION,0.3621700879765396,"networks to align intention-relevant visual information. Leveraging intent-CC3M, we propose a
341"
CONCLUSION,0.3629032258064516,"novel manipulation intention understanding network that employs learnable queries to enhance the
342"
CONCLUSION,0.36363636363636365,"models’ capability to understand user intention from manipulation descriptions for accurate CIR.
343"
CONCLUSION,0.36436950146627567,"De-MINDS shows strong generalization ability and remarkably improves the best performance of
344"
CONCLUSION,0.3651026392961877,"existing approaches on four diverse ZS-CIR tasks with comparable inference times. Our work inspires
345"
CONCLUSION,0.3658357771260997,"intention-based image retrieval and impacts diverse vision and language applications.
346"
REFERENCES,0.36656891495601174,"References
347"
REFERENCES,0.36730205278592376,"[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
348"
REFERENCES,0.3680351906158358,"Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,
349"
REFERENCES,0.3687683284457478,"Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud,
350"
REFERENCES,0.36950146627565983,"Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikoł aj Bi´nkowski, Ricardo Barreira, Oriol Vinyals,
351"
REFERENCES,0.37023460410557185,"Andrew Zisserman, and Karén Simonyan. Flamingo: a visual language model for few-shot learning.
352"
REFERENCES,0.3709677419354839,"In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural
353"
REFERENCES,0.3717008797653959,"Information Processing Systems, volume 35, pages 23716–23736, 2022.
354"
REFERENCES,0.3724340175953079,"[2] Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley.
355"
REFERENCES,0.37316715542521994,"Rezero is all you need: Fast convergence at large depth. In Uncertainty in Artificial Intelligence, pages
356"
REFERENCES,0.37390029325513197,"1352–1361, 2021.
357"
REFERENCES,0.374633431085044,"[3] Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. Zero-shot composed image
358"
REFERENCES,0.375366568914956,"retrieval with textual inversion. arXiv:2303.15247, 2023.
359"
REFERENCES,0.37609970674486803,"[4] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. Effective conditioned and
360"
REFERENCES,0.37683284457478006,"composed image retrieval combining clip-based features. In Proceedings of the IEEE/CVF Conference on
361"
REFERENCES,0.3775659824046921,"Computer Vision and Pattern Recognition, pages 21466–21474, June 2022.
362"
REFERENCES,0.3782991202346041,"[5] Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov.
363"
REFERENCES,0.3790322580645161,"Knowledge distillation: A good teacher is patient and consistent. In Proceedings of the IEEE/CVF
364"
REFERENCES,0.37976539589442815,"conference on computer vision and pattern recognition, pages 10925–10934, 2022.
365"
REFERENCES,0.38049853372434017,"[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
366"
REFERENCES,0.3812316715542522,"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
367"
REFERENCES,0.3819648093841642,"Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
368"
REFERENCES,0.38269794721407624,"Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
369"
REFERENCES,0.38343108504398826,"Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models
370"
REFERENCES,0.3841642228739003,"are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances
371"
REFERENCES,0.3848973607038123,"in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.
372"
REFERENCES,0.38563049853372433,"[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
373"
REFERENCES,0.38636363636363635,"Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision,
374"
REFERENCES,0.3870967741935484,"pages 213–229, 2020.
375"
REFERENCES,0.3878299120234604,"[8] Akshay Chawla, Hongxu Yin, Pavlo Molchanov, and Jose Alvarez. Data-free knowledge distillation for
376"
REFERENCES,0.3885630498533724,"object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
377"
REFERENCES,0.38929618768328444,"pages 3289–3298, 2021.
378"
REFERENCES,0.39002932551319647,"[9] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efficient object
379"
REFERENCES,0.3907624633431085,"detection models with knowledge distillation. In Proc. of Advances in Neural Information Processing
380"
REFERENCES,0.3914956011730205,"Systems (NeurIPS), volume 30, 2017.
381"
REFERENCES,0.39222873900293254,"[10] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.
382"
REFERENCES,0.39296187683284456,"Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793,
383"
REFERENCES,0.3936950146627566,"2023.
384"
REFERENCES,0.3944281524926686,"[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
385"
REFERENCES,0.3951612903225806,"contrastive learning of visual representations. In Proc. of International Conference on Machine Learning
386"
REFERENCES,0.39589442815249265,"(ICML), pages 1597–1607. PMLR, 2020.
387"
REFERENCES,0.3966275659824047,"[12] Yanbei Chen, Shaogang Gong, and Loris Bazzani. Image search with text feedback by visiolinguistic atten-
388"
REFERENCES,0.3973607038123167,"tion learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
389"
REFERENCES,0.3980938416422287,"pages 3001–3011, 2020.
390"
REFERENCES,0.39882697947214074,"[13] Niv Cohen, Rinon Gal, Eli A. Meirom, Gal Chechik, and Yuval Atzmon. ""This is my unicorn, Fluffy"":
391"
REFERENCES,0.39956011730205276,"Personalizing frozen vision-language representations. In Proc. of the European Conference on Computer
392"
REFERENCES,0.4002932551319648,"Vision (ECCV), 2022.
393"
REFERENCES,0.4010263929618768,"[14] Niv Cohen, Rinon Gal, Eli A. Meirom, Gal Chechik, and Yuval Atzmon. “this is my unicorn, fluffy”:
394"
REFERENCES,0.40175953079178883,"Personalizing frozen vision-language representations. In European conference on computer vision, pages
395"
REFERENCES,0.40249266862170086,"558–577, 2022.
396"
REFERENCES,0.4032258064516129,"[15] Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z Wang. Image retrieval: Ideas, influences, and trends of
397"
REFERENCES,0.4039589442815249,"the new age. ACM Computing Surveys, 40(2):1–60, 2008.
398"
REFERENCES,0.4046920821114369,"[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
399"
REFERENCES,0.40542521994134895,"image database. In Computer Vision and Pattern Recognition, pages 248–255, 2009.
400"
REFERENCES,0.40615835777126097,"[17] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with
401"
REFERENCES,0.40689149560117305,"language rewrites. Advances in Neural Information Processing Systems, 36, 2024.
402"
REFERENCES,0.40762463343108507,"[18] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen,
403"
REFERENCES,0.4083577712609971,"Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next
404"
REFERENCES,0.4090909090909091,"generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024.
405"
REFERENCES,0.40982404692082114,"[19] Sonam Goenka, Zhaoheng Zheng, Ayush Jaiswal, Rakesh Chada, Yue Wu, Varsha Hedau, and Pradeep
406"
REFERENCES,0.41055718475073316,"Natarajan. Fashionvlp: Vision language transformer for fashion retrieval with feedback. In Proceedings of
407"
REFERENCES,0.4112903225806452,"the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14105–14115, June 2022.
408"
REFERENCES,0.4120234604105572,"[20] Geonmo Gu, Sanghyuk Chun, Wonjae Kim, , Yoohoon Kang, and Sangdoo Yun. Language-only efficient
409"
REFERENCES,0.41275659824046923,"training of zero-shot composed image retrieval. In Conference on Computer Vision and Pattern Recognition
410"
REFERENCES,0.41348973607038125,"(CVPR), 2024.
411"
REFERENCES,0.4142228739002933,"[21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,
412"
REFERENCES,0.4149560117302053,"Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces
413"
REFERENCES,0.4156891495601173,"of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF
414"
REFERENCES,0.41642228739002934,"International Conference on Computer Vision, pages 8340–8349, 2021.
415"
REFERENCES,0.41715542521994137,"[22] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS
416"
REFERENCES,0.4178885630498534,"Deep Learning and Representation Learning Workshop, 2015.
417"
REFERENCES,0.4186217008797654,"[23] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,
418"
REFERENCES,0.41935483870967744,"1997.
419"
REFERENCES,0.42008797653958946,"[24] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei
420"
REFERENCES,0.4208211143695015,"Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with
421"
REFERENCES,0.4215542521994135,"language models. Advances in Neural Information Processing Systems, 36, 2024.
422"
REFERENCES,0.4222873900293255,"[25] Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, and Zeynep Akata. Vision-by-language
423"
REFERENCES,0.42302052785923755,"for training-free compositional image retrieval. In The Twelfth International Conference on Learning
424"
REFERENCES,0.4237536656891496,"Representations, 2024.
425"
REFERENCES,0.4244868035190616,"[26] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept
426"
REFERENCES,0.4252199413489736,"customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision
427"
REFERENCES,0.42595307917888564,"and Pattern Recognition, pages 1931–1941, 2023.
428"
REFERENCES,0.42668621700879766,"[27] Zhengfeng Lai, Haotian Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan,
429"
REFERENCES,0.4274193548387097,"Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, et al. From scarcity to efficiency: Improving clip training via
430"
REFERENCES,0.4281524926686217,"visual-enriched captions. arXiv preprint arXiv:2310.07699, 2023.
431"
REFERENCES,0.42888563049853373,"[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training
432"
REFERENCES,0.42961876832844575,"with frozen image encoders and large language models, 2023.
433"
REFERENCES,0.4303519061583578,"[29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pre-training
434"
REFERENCES,0.4310850439882698,"for unified vision-language understanding and generation. In Proceedings of the 39th International
435"
REFERENCES,0.4318181818181818,"Conference on Machine Learning, pages 12888–12900, 2022.
436"
REFERENCES,0.43255131964809385,"[30] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong
437"
REFERENCES,0.43328445747800587,"Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In
438"
REFERENCES,0.4340175953079179,"European Conference on Computer Vision, pages 121–137, 2020.
439"
REFERENCES,0.4347507331378299,"[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
440"
REFERENCES,0.43548387096774194,"and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla,
441"
REFERENCES,0.43621700879765396,"Bernt Schiele, and Tinne Tuytelaars, editors, European Conference on Computer Vision, pages 740–755,
442"
REFERENCES,0.436950146627566,"2014.
443"
REFERENCES,0.437683284457478,"[32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural
444"
REFERENCES,0.43841642228739003,"information processing systems, 36, 2024.
445"
REFERENCES,0.43914956011730205,"[33] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. Image retrieval on real-life
446"
REFERENCES,0.4398826979472141,"images with pre-trained vision-and-language models. In Proceedings of the IEEE/CVF International
447"
REFERENCES,0.4406158357771261,"Conference on Computer Vision, pages 2125–2134, October 2021.
448"
REFERENCES,0.4413489736070381,"[34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on
449"
REFERENCES,0.44208211143695014,"Learning Representations, 2018.
450"
REFERENCES,0.44281524926686217,"[35] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim
451"
REFERENCES,0.4435483870967742,"Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on
452"
REFERENCES,0.4442815249266862,"Computer Vision and Pattern Recognition, pages 14297–14306, 2023.
453"
REFERENCES,0.44501466275659823,"[36] Ron Mokady, Amir Hertz, and Amit H. Bermano. Clipcap: Clip prefix for image captioning, 2021.
454"
REFERENCES,0.44574780058651026,"[37] Muhammad Ferjad Naeem, Muhammad Gul Zain Ali Khan, Yongqin Xian, Muhammad Zeshan Afzal,
455"
REFERENCES,0.4464809384164223,"Didier Stricker, Luc Van Gool, and Federico Tombari. I2mvformer: Large language model generated
456"
REFERENCES,0.4472140762463343,"multi-view document supervision for zero-shot image classification. In Proceedings of the IEEE/CVF
457"
REFERENCES,0.4479472140762463,"Conference on Computer Vision and Pattern Recognition, pages 15169–15179, 2023.
458"
REFERENCES,0.44868035190615835,"[38] Muhammad Ferjad Naeem, Yongqin Xian, Luc V Gool, and Federico Tombari. I2dformer: Learning image
459"
REFERENCES,0.44941348973607037,"to document attention for zero-shot image classification. Advances in Neural Information Processing
460"
REFERENCES,0.4501466275659824,"Systems, 35:12283–12294, 2022.
461"
REFERENCES,0.4508797653958944,"[39] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving
462"
REFERENCES,0.45161290322580644,"multimodal datasets with image captioning. Advances in Neural Information Processing Systems, 36, 2024.
463"
REFERENCES,0.45234604105571846,"[40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
464"
REFERENCES,0.4530791788856305,"Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
465"
REFERENCES,0.4538123167155425,"learning library. NeurIPS, 32, 2019.
466"
REFERENCES,0.45454545454545453,"[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
467"
REFERENCES,0.45527859237536655,"Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning trans-
468"
REFERENCES,0.4560117302052786,"ferable visual models from natural language supervision. In Proceedings of the International Conference
469"
REFERENCES,0.4567448680351906,"on Machine Learning, pages 8748–8763, 2021.
470"
REFERENCES,0.4574780058651026,"[42] Aditya Ramesh, Mukul Goyal, and Rob Fergus. Dall-e: Creating images from text. OpenAI Blog, 2021.
471"
REFERENCES,0.45821114369501464,"[43] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua
472"
REFERENCES,0.45894428152492667,"Bengio. FitNets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.
473"
REFERENCES,0.4596774193548387,"[44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-
474"
REFERENCES,0.4604105571847507,"booth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the
475"
REFERENCES,0.46114369501466274,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500–22510, 2023.
476"
REFERENCES,0.46187683284457476,"[45] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister.
477"
REFERENCES,0.4626099706744868,"Pic2word: Mapping pictures to words for zero-shot composed image retrieval. In Proceedings of the
478"
REFERENCES,0.4633431085043988,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19305–19314, 2023.
479"
REFERENCES,0.4640762463343108,"[46] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation.
480"
REFERENCES,0.46480938416422285,"arXiv preprint arXiv:2311.17042, 2023.
481"
REFERENCES,0.4655425219941349,"[47] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,
482"
REFERENCES,0.4662756598240469,"hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Association
483"
REFERENCES,0.4670087976539589,"for Computational Linguistics, pages 2556–2565, 2018.
484"
REFERENCES,0.46774193548387094,"[48] Jiangming Shi, Yachao Zhang, Xiangbo Yin, Yuan Xie, Zhizhong Zhang, Jianping Fan, Zhongchao Shi,
485"
REFERENCES,0.46847507331378296,"and Yanyun Qu. Dual pseudo-labels interactive self-training for semi-supervised visible-infrared person
486"
REFERENCES,0.46920821114369504,"re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
487"
REFERENCES,0.46994134897360706,"11218–11228, 2023.
488"
REFERENCES,0.4706744868035191,"[49] Haoyu Song, Li Dong, Wei-Nan Zhang, Ting Liu, and Furu Wei. Clip models are few-shot learners:
489"
REFERENCES,0.4714076246334311,"Empirical studies on vqa and visual entailment, 2022.
490"
REFERENCES,0.47214076246334313,"[50] Derek Tam, Colin Raffel, and Mohit Bansal. Simple weakly-supervised image captioning via CLIP’s
491"
REFERENCES,0.47287390029325516,"multimodal embeddings. In The AAAI-23 Workshop on Creative AI Across Modalities, 2023.
492"
REFERENCES,0.4736070381231672,"[51] Yingtian Tang, Yutaro Yamada, Yoyo Zhang, and Ilker Yildirim. When are lemons purple? the concept
493"
REFERENCES,0.4743401759530792,"association bias of vision-language models. In Proceedings of the 2023 Conference on Empirical Methods
494"
REFERENCES,0.4750733137829912,"in Natural Language Processing, pages 14333–14348, 2023.
495"
REFERENCES,0.47580645161290325,"[52] Yuanmin Tang, Jing Yu, Keke Gai, Jiamin Zhuang, Gang Xiong, Yue Hu, and Qi Wu. Context-i2w: Map-
496"
REFERENCES,0.47653958944281527,"ping images to context-dependent words for accurate zero-shot composed image retrieval. In Proceedings
497"
REFERENCES,0.4772727272727273,"of the AAAI Conference on Artificial Intelligence, volume 38, pages 5180–5188, 2024.
498"
REFERENCES,0.4780058651026393,"[53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
499"
REFERENCES,0.47873900293255134,"Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
500"
REFERENCES,0.47947214076246336,"language models. arXiv preprint arXiv:2302.13971, 2023.
501"
REFERENCES,0.4802052785923754,"[54] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing text
502"
REFERENCES,0.4809384164222874,"and image for image retrieval - an empirical odyssey. In Proceedings of the IEEE/CVF Conference on
503"
REFERENCES,0.48167155425219943,"Computer Vision and Pattern Recognition, pages 6439–6448, 2019.
504"
REFERENCES,0.48240469208211145,"[55] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing text and
505"
REFERENCES,0.4831378299120235,"image for image retrieval-an empirical odyssey. In Proceedings of the IEEE/CVF conference on computer
506"
REFERENCES,0.4838709677419355,"vision and pattern recognition, pages 6439–6448, 2019.
507"
REFERENCES,0.4846041055718475,"[56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
508"
REFERENCES,0.48533724340175954,"Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural
509"
REFERENCES,0.48607038123167157,"information processing systems, 35:24824–24837, 2022.
510"
REFERENCES,0.4868035190615836,"[57] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris.
511"
REFERENCES,0.4875366568914956,"Fashion iq: A new dataset towards retrieving images by natural language feedback. In Proceedings of the
512"
REFERENCES,0.48826979472140764,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11307–11317, 2021.
513"
REFERENCES,0.48900293255131966,"[58] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the
514"
REFERENCES,0.4897360703812317,"long-text capability of clip, 2024.
515"
REFERENCES,0.4904692082111437,"[59] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and
516"
REFERENCES,0.4912023460410557,"Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the
517"
REFERENCES,0.49193548387096775,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5579–5588, 2021.
518"
REFERENCES,0.49266862170087977,"[60] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for
519"
REFERENCES,0.4934017595307918,"vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
520"
REFERENCES,0.4941348973607038,"Recognition, pages 16816–16825, 2022.
521"
REFERENCES,0.49486803519061584,"[61] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing
522"
REFERENCES,0.49560117302052786,"vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,
523"
REFERENCES,0.4963343108504399,"2023.
524"
REFERENCES,0.4970674486803519,"[62] Wanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, and William Yang Wang.
525"
REFERENCES,0.49780058651026393,"Visualize before you write: Imagination-guided open-ended text generation, 2023.
526"
REFERENCES,0.49853372434017595,"A
Extended Analysis
527"
REFERENCES,0.499266862170088,"A.1
Analysis of the number of learnable queries.
528"
REFERENCES,0.5,"We conduct analysis on the number of learnable query embedding X = {xk}n
k=1 ∈Rd×n as shown
529"
REFERENCES,0.500733137829912,"in Figure 8. We find that n = 2 results in not learning sufficient intentions for manipulation, but
530 26.5 27.3 25.8 25.1 24.5 29.7 31 29.1 28.4 27.5 31.8 33"
REFERENCES,0.501466275659824,"31.1
30.7 30.1 32.5 33.2 31.8 31.2 30.6 24 25 26 27 28 29 30 31 32 33 34"
QUERIES,0.5021994134897361,"2 queries
4 queries
8 queries
16 queries
32 queries"
QUERIES,0.5029325513196481,"CIRR R@1
Shirt R@10
ImageNet R@50
COCO R@5"
QUERIES,0.5036656891495601,Figure 8: Analysis of the number of learnable queries.
QUERIES,0.5043988269794721,"when n is added to 32, it is redundant and unhelpful for the CLIP model to understand manipulation
531"
QUERIES,0.5051319648093842,"intentions. We finally choose n = 4, which gives the best result among different settings.
532"
QUERIES,0.5058651026392962,Table 6: Results on ImageNet for domain conversion.
QUERIES,0.5065982404692082,"Cartoon
Origami
Toy
Sculpture
Average"
QUERIES,0.5073313782991202,"Methods
Conferences
R10
R50
R10
R50
R10
R50
R10
R50
R10
R50"
QUERIES,0.5080645161290323,"Pic2Word [45]
CVPR 2023
8.0
21.9
13.5
25.6
8.7
21.6
10.0
23.8
10.1
23.2"
QUERIES,0.5087976539589443,"Context-I2W [52]
AAAI 2024
10.2
26.1
17.5
28.7
11.6
27.4
12.1
28.2
12.9
27.6
Context-I2W*
–
11.2
27.4
18.7
30.4
12.5
29.8
13.7
31.4
14.0
29.8"
QUERIES,0.5095307917888563,"Context-I2W(50 %)
AAAI 2024
9.0
23.0
14.3
25.6
10.7
25.0
11.0
25.5
11.3
24.8
De-MINDS(50 %)
–
11.7
28.3
19.2
30.9
12.8
30.2
14.2
32.0
14.5
30.4
De-MINDS(100 %)
–
13.3
31.2
20.3
34.5
14.7
31.7
16.5
34.7
16.2
33.0"
QUERIES,0.5102639296187683,"Table 7: Results on CIRR for object manipu-
lation task."
QUERIES,0.5109970674486803,"Methods
R1
R5
R10
R50"
QUERIES,0.5117302052785924,"Pic2Word [45]
23.9
51.7
65.3
87.8
CIReVL [25]
24.6
52.3
64.9
86.3
LinCIR [20]
25.0
53.3
66.7
–"
QUERIES,0.5124633431085044,"SEARLE-XL [3]
24.2
52.4
66.3
88.6
SEARLE-XL*
25.4
54.1
66.9
89.3"
QUERIES,0.5131964809384164,"Context-I2W [52]
25.6
55.1
68.5
89.8
Context-I2W*
26.3
55.7
69.0
90.2"
QUERIES,0.5139296187683284,"Context-I2W(50%)
24.8
53.6
67.1
88.9
De-MINDS (50%)
26.5
56.0
69.3
90.5
De-MINDS
27.3
57.0
71.3
91.6"
QUERIES,0.5146627565982405,"Table 8: Results on COCO for object composition
task."
QUERIES,0.5153958944281525,"Methods
R1
R5
R10"
QUERIES,0.5161290322580645,"Pic2Word [45]
11.5
24.8
33.4"
QUERIES,0.5168621700879765,"Context-I2W [52]
13.5
28.5
38.1
Context-I2W*
14.3
29.7
40.5"
QUERIES,0.5175953079178885,"Context-I2W(50%)
12.1
25.6
34.4
De-MINDS (50%)
14.6
30.4
40.8
De-MINDS (100%)
15.7
33.2
44.1"
QUERIES,0.5183284457478006,"A.2
More Effectiveness and Efficiency Analysis
533"
QUERIES,0.5190615835777126,"In Table 6 to 9, we present more evidence supporting the efficacy and efficiency of our De-MINDS.
534"
QUERIES,0.5197947214076246,"With only 50% of the training data, De-MINDS matches and exceeds the performance of the state-
535"
QUERIES,0.5205278592375366,"of-the-art (SoTA) Context-I2W model by 0.83% to 2.20%. Remarkably, De-MINDS outperforms
536"
QUERIES,0.5212609970674487,"reported results of the SoTA model by 1.98% to 4.57% under the same 50% training data, underscoring
537"
QUERIES,0.5219941348973607,"our method’s superiority.
538"
QUERIES,0.5227272727272727,Table 9: Results on Fashion-IQ for attribute manipulation.
QUERIES,0.5234604105571847,"Dress
Shrit
TopTee
Average"
QUERIES,0.5241935483870968,"Methods
Conferences
R10
R50
R10
R50
R10
R50
R10
R50"
QUERIES,0.5249266862170088,"Pic2Word [45]
CVPR 2023
20.0
40.2
26.2
43.6
27.9
47.4
24.7
43.7
CIReVL [25]
ICLR 2024
24.6
44.8
29.5
47.4
31.4
53.7
28.6
48.6
LinCIR [20]
CVPR 2024
20.9
42.4
29.1
46.8
28.8
50.2
26.3
46.5"
QUERIES,0.5256598240469208,"SEARLE-XL [3]
ICCV 2023
20.3
43.2
27.4
45.7
29.3
50.2
25.7
46.3
SEARLE-XL*
–
22.7
45.0
29.4
47.9
30.2
51.4
27.4
48.1"
QUERIES,0.5263929618768328,"Context-I2W [52]
AAAI 2024
23.1
45.3
29.7
48.6
30.6
52.9
27.8
48.9
Context-I2W*
–
23.9
46.9
30.4
49.7
31.1
53.8
28.5
50.1"
QUERIES,0.5271260997067448,"Context-I2W(50%)
AAAI 2024
21.4
43.7
28.1
46.9
29.7
51.4
26.4
47.3
De-MINDS (50%)
–
24.3
47.5
30.6
50.0
31.3
54.0
28.7
50.5
De-MINDS (100%)
–
25.2
48.7
31.0
51.2
32.9
55.7
29.7
51.9"
QUERIES,0.5278592375366569,Algorithm 1 Manipulation Intention Understanding’s process.
QUERIES,0.5285923753665689,"Input: batch of word embeddings of target descriptions ˜T = {ti}m
i=1, where t1 is the global feature
tcls, Nlayer, the frozen CLIP language encoder ΨT
Parameter: a set of learnable embeddings X ∈Rd×n , 8-heads attention layer Attn, 3-layers FC
layers fM, gateα.
Output: target embedding ˆt"
QUERIES,0.5293255131964809,"1: Initialize X ∈Rd×n, Attn, fM randomly.
2: Let Xi
att = {ti}m
i=2, t = 1
3: while t ≤Nlayer do
4:
Xi+1
att = Xi
att + Attnt(q=q, k=concat([Xi
att, q]), v=concat([Xi
att, q]))
5:
Xi+1
att = Xi+1
att + fMt(Xi+1
att )
6:
t = t + 1
7: end while
t∗= ΨT (Xoutput)
ˆt = tcls + tanh(gateα) · t∗
8: return ˆt"
QUERIES,0.5300586510263929,"A.3
Broader Impact
539"
QUERIES,0.530791788856305,"We propose a novel image-text dataset augmentation strategy that generates diverse rewrites for
540"
QUERIES,0.531524926686217,"any given image-text pair. This approach not only bolsters the performance of vision-language
541"
QUERIES,0.532258064516129,"models but also enhances capabilities in textual inversion [44], including text-to-image generation
542"
QUERIES,0.532991202346041,"via diffusion models and personalized image retrieval. However, it is crucial to note that MLLMs are
543"
QUERIES,0.533724340175953,"trained on extensive web data, which may incorporate factual inaccuracies and hallucinatory content.
544"
QUERIES,0.5344574780058651,"Consequently, the intention-infused versions of texts could inherit these flaws. We advocate for
545"
QUERIES,0.5351906158357771,"the implementation of rigorous data filtering methods before these models’ deployment in practical
546"
QUERIES,0.5359237536656891,"settings. Furthermore, while the MLLM-based rewriting strategy demands substantial GPU/TPU
547"
QUERIES,0.5366568914956011,"computational resources, potentially increasing the carbon footprint.
548"
QUERIES,0.5373900293255132,"A.4
Qualitative Results of intent-CC3M
549"
QUERIES,0.5381231671554252,"Figure 9 to 10 we leverage DALL-E [42] to generate images of each caption for qualitative experiment.
550"
QUERIES,0.5388563049853372,"We compare intent-CC3M with the CC3M dataset and GPT4’s rewritten captions. We found that
551"
QUERIES,0.5395894428152492,"the captions of Intent-CC3M, which contain potential manipulation intentions, provide better visual
552"
QUERIES,0.5403225806451613,"information compared to the original captions and those rewritten by a large language model. This
553"
QUERIES,0.5410557184750733,"improvement is due to incorporating diverse visual perspectives (e.g., colors, scenes, and objects)
554"
QUERIES,0.5417888563049853,"using a multi-model language model, which enhances the training of text-to-image generation tasks.
555"
QUERIES,0.5425219941348973,"Notably, our pseudo-manipulation descriptions are shorter than the rewritten captions. The results
556"
QUERIES,0.5432551319648093,"show that pseudo-manipulation descriptions serve as more effective prompts, enabling DALL-E to
557"
QUERIES,0.5439882697947214,"generate results that are closer to the original images. This demonstrates the high quality of our
558"
QUERIES,0.5447214076246334,"pseudo-manipulation descriptions.
559"
QUERIES,0.5454545454545454,Original Caption
QUERIES,0.5461876832844574,a street musician
QUERIES,0.5469208211143695,plays an accordion. LLM’s
QUERIES,0.5476539589442815,Rewritten Caption
QUERIES,0.5483870967741935,A street performer
QUERIES,0.5491202346041055,serenades
QUERIES,0.5498533724340176,passersby with the
QUERIES,0.5505865102639296,melodies of an
QUERIES,0.5513196480938416,accordion.
QUERIES,0.5520527859237536,"Generated Image
Pseudo-Manipulation"
QUERIES,0.5527859237536656,Description
QUERIES,0.5535190615835777,Man plays accordion
QUERIES,0.5542521994134897,"joyfully in the street,"
QUERIES,0.5549853372434017,with cafe background
QUERIES,0.5557184750733137,"suggesting a lively,"
QUERIES,0.5564516129032258,public musical
QUERIES,0.5571847507331378,performance.
QUERIES,0.5579178885630498,Generated Image
QUERIES,0.5586510263929618,A man is playing an accordion on a
QUERIES,0.5593841642228738,"street. He is standing in the foreground,"
QUERIES,0.5601173020527859,holding the accordion in his hands.
QUERIES,0.5608504398826979,There are several chairs and tables in the
QUERIES,0.5615835777126099,"background, suggesting a cafe or"
QUERIES,0.5623167155425219,"outdoor seating area. The man is smiling,"
QUERIES,0.5630498533724341,indicating he is enjoying his
QUERIES,0.5637829912023461,performance. The accordion is black and
QUERIES,0.5645161290322581,has a metallic finish. The street is lined
QUERIES,0.5652492668621701,"with buildings, and there are a few other"
QUERIES,0.5659824046920822,people in the background.
QUERIES,0.5667155425219942,"Our Rewritten Caption
Generated Image"
QUERIES,0.5674486803519062,Automotive industry
QUERIES,0.5681818181818182,business now offers
QUERIES,0.5689149560117303,a new tuning
QUERIES,0.5696480938416423,package for
QUERIES,0.5703812316715543,automobile model.
QUERIES,0.5711143695014663,The automotive
QUERIES,0.5718475073313783,industry now
QUERIES,0.5725806451612904,introduces a fresh
QUERIES,0.5733137829912024,tuning package for a
QUERIES,0.5740469208211144,specific car model.
QUERIES,0.5747800586510264,Highlights Black VW
QUERIES,0.5755131964809385,Beetle with tuning
QUERIES,0.5762463343108505,"package, sleek design,"
QUERIES,0.5769794721407625,"and glossy finish,"
QUERIES,0.5777126099706745,emphasizing driving on
QUERIES,0.5784457478005866,rural road.
QUERIES,0.5791788856304986,A black Volkswagen Beetle in motion on
QUERIES,0.5799120234604106,a road. The car is equipped with a new
QUERIES,0.5806451612903226,"tuning package, featuring a body kit with"
QUERIES,0.5813782991202346,"a front bumper, side skirts, and a rear"
QUERIES,0.5821114369501467,"bumper, all in the same black color. The"
QUERIES,0.5828445747800587,car's sleek and modern design has a
QUERIES,0.5835777126099707,glossy finish that reflects the
QUERIES,0.5843108504398827,surrounding environment. The
QUERIES,0.5850439882697948,"background is a blur of greenery,, which"
QUERIES,0.5857771260997068,contrasts with the car's urban aesthetic.
QUERIES,0.5865102639296188,"he domain of the image is photography,"
QUERIES,0.5872434017595308,capturing a real-life scene.
QUERIES,0.5879765395894428,"on a sunny winter day.
Underneath the crisp"
QUERIES,0.5887096774193549,winter sun.
QUERIES,0.5894428152492669,"A large, round, stone"
QUERIES,0.5901759530791789,castle with multiple
QUERIES,0.5909090909090909,"towers, highlighted by"
QUERIES,0.591642228739003,stands in the middle of
QUERIES,0.592375366568915,"a lush green field,"
QUERIES,0.593108504398827,"surrounded by a moat,"
QUERIES,0.593841642228739,under a clear blue sky.
QUERIES,0.594574780058651,"A large, round, stone castle with"
QUERIES,0.5953079178885631,multiple towers stands in the middle of
QUERIES,0.5960410557184751,a lush green field. The castle is
QUERIES,0.5967741935483871,"surrounded by a moat, and the sky is"
QUERIES,0.5975073313782991,clear and blue. The sun is shining
QUERIES,0.5982404692082112,"brightly, casting long shadows and"
QUERIES,0.5989736070381232,highlighting the textures of the stone.
QUERIES,0.5997067448680352,The castle is the central focus of the
QUERIES,0.6004398826979472,"image, with the open field stretching out"
QUERIES,0.6011730205278593,around it.
QUERIES,0.6019061583577713,I start the season
QUERIES,0.6026392961876833,with a big tree.
QUERIES,0.6033724340175953,I kick off the season
QUERIES,0.6041055718475073,with a towering tree.
QUERIES,0.6048387096774194,A large tree in the center of
QUERIES,0.6055718475073314,"a town square, surrounded"
QUERIES,0.6063049853372434,by white buildings with
QUERIES,0.6070381231671554,"blue accents, under a blue"
QUERIES,0.6077712609970675,sky. The square is bustling
QUERIES,0.6085043988269795,"with people, showcasing a"
QUERIES,0.6092375366568915,European-influenced
QUERIES,0.6099706744868035,architecture.
QUERIES,0.6107038123167156,"A large, leafy tree stands prominently in"
QUERIES,0.6114369501466276,"the center of a town square, surrounded"
QUERIES,0.6121700879765396,by white buildings with blue accents. The
QUERIES,0.6129032258064516,"tree is the focal point, with its vibrant"
QUERIES,0.6136363636363636,green leaves contrasting against the clear
QUERIES,0.6143695014662757,blue sky. The town square is bustling with
QUERIES,0.6151026392961877,"people, adding life to the scene. The"
QUERIES,0.6158357771260997,architecture of the buildings suggests a
QUERIES,0.6165689149560117,"European influence, and the overall"
QUERIES,0.6173020527859238,atmosphere is serene and picturesque.
QUERIES,0.6180351906158358,Original Image
QUERIES,0.6187683284457478,"Figure 9: Qualitative results of our intent-CC3M dataset. We leverage DALL-E to generate images of
the captions. We compare intent-CC3M with the CC3M dataset and LLM’s rewritten captions."
QUERIES,0.6195014662756598,Original Caption
QUERIES,0.6202346041055719,A toddler laughs in
QUERIES,0.6209677419354839,enjoyment on the
QUERIES,0.6217008797653959,playground.
QUERIES,0.6224340175953079,A young child
QUERIES,0.6231671554252199,giggles with delight
QUERIES,0.623900293255132,while playing on the
QUERIES,0.624633431085044,playground.
QUERIES,0.625366568914956,Generated Image
QUERIES,0.626099706744868,A young girl is laughing
QUERIES,0.6268328445747801,"on a swing, wearing a"
QUERIES,0.6275659824046921,"colorful dress, with two"
QUERIES,0.6282991202346041,adults watching her.
QUERIES,0.6290322580645161,The swing is yellow and
QUERIES,0.6297653958944281,wooden. They are in a
QUERIES,0.6304985337243402,park-like setting.
QUERIES,0.6312316715542522,Generated Image
QUERIES,0.6319648093841642,A young girl is sitting on a playground
QUERIES,0.6326979472140762,"swing, laughing and enjoying herself."
QUERIES,0.6334310850439883,She is wearing a colorful dress and has
QUERIES,0.6341642228739003,blonde hair. Two adults are standing
QUERIES,0.6348973607038123,"nearby, watching her with smiles. The"
QUERIES,0.6356304985337243,swing is yellow and has a wooden seat.
QUERIES,0.6363636363636364,"In the background, there is a sandy area"
QUERIES,0.6370967741935484,and a fence. The scene is set in a park-
QUERIES,0.6378299120234604,like environment.
QUERIES,0.6385630498533724,"Our Rewritten Caption
Generated Image"
QUERIES,0.6392961876832844,a shepherd pictured
QUERIES,0.6400293255131965,with his flock of
QUERIES,0.6407624633431085,sheep in north.
QUERIES,0.6414956011730205,A shepherd
QUERIES,0.6422287390029325,captured alongside
QUERIES,0.6429618768328446,his flock of sheep in
QUERIES,0.6436950146627566,the northern region.
QUERIES,0.6444281524926686,Man in plaid shirt and
QUERIES,0.6451612903225806,flat cap stands in field
QUERIES,0.6458944281524927,"with sheep, smiling,"
QUERIES,0.6466275659824047,"looking at camera, on"
QUERIES,0.6473607038123167,"clear, blue day."
QUERIES,0.6480938416422287,A man in a plaid shirt and a flat cap
QUERIES,0.6488269794721407,stands in a field with a flock of sheep. He
QUERIES,0.6495601173020528,is smiling and looking directly at the
QUERIES,0.6502932551319648,camera. The sheep are scattered around
QUERIES,0.6510263929618768,"him, grazing on the grass. The sky is"
QUERIES,0.6517595307917888,"clear and blue, suggesting a sunny day."
QUERIES,0.6524926686217009,The man's attire and the presence of the
QUERIES,0.6532258064516129,sheep suggest a rural or pastoral setting.
QUERIES,0.6539589442815249,The image captures a moment of
QUERIES,0.6546920821114369,peaceful coexistence between the man
QUERIES,0.655425219941349,and his flock.
QUERIES,0.656158357771261,A child touches an ice
QUERIES,0.656891495601173,sculpture.
QUERIES,0.657624633431085,A child reaches out to
QUERIES,0.658357771260997,touch an ice sculpture.
QUERIES,0.6590909090909091,Child reaches out to touch ice
QUERIES,0.6598240469208211,"dragon sculpture, intricately"
QUERIES,0.6605571847507331,designed with scales and
QUERIES,0.6612903225806451,"spikes, standing in front of it"
QUERIES,0.6620234604105572,on sandy ground.
QUERIES,0.6627565982404692,A child reaches out to touch a large ice
QUERIES,0.6634897360703812,"sculpture of a dragon, which is"
QUERIES,0.6642228739002932,intricately designed with scales and
QUERIES,0.6649560117302052,spikes. The sculpture is positioned on a
QUERIES,0.6656891495601173,"sandy ground, and the child is standing"
QUERIES,0.6664222873900293,"in front of it. The background is dark,"
QUERIES,0.6671554252199413,highlighting the brightness of the ice
QUERIES,0.6678885630498533,sculpture. The dragon sculpture is made
QUERIES,0.6686217008797654,of ice and is the main focus of the image.
QUERIES,0.6693548387096774,The child's interaction with the sculpture
QUERIES,0.6700879765395894,suggests a sense of wonder and curiosity.
QUERIES,0.6708211143695014,office worker sitting at
QUERIES,0.6715542521994134,the table and working
QUERIES,0.6722873900293255,on the computer .
QUERIES,0.6730205278592375,An office worker sits
QUERIES,0.6737536656891495,"at their desk,"
QUERIES,0.6744868035190615,diligently working on
QUERIES,0.6752199413489736,their computer.
QUERIES,0.6759530791788856,A man works on his
QUERIES,0.6766862170087976,computer at a desk with a
QUERIES,0.6774193548387096,"monitor, keyboard , and"
QUERIES,0.6781524926686217,mouse. He is dressed
QUERIES,0.6788856304985337,"casually, wearing glasses,"
QUERIES,0.6796187683284457,and has a slight smile. The
QUERIES,0.6803519061583577,background is minimalistic.
QUERIES,0.6810850439882697,The style is a flat illustration
QUERIES,0.6818181818181818,with a limited color palette.
QUERIES,0.6825513196480938,"A man is seated at a desk, engrossed in"
QUERIES,0.6832844574780058,work on his computer. The desk is
QUERIES,0.6840175953079178,"equipped with a monitor, keyboard, and"
QUERIES,0.6847507331378299,mouse. The man is dressed in a casual
QUERIES,0.6854838709677419,"outfit, wearing glasses and has a slight"
QUERIES,0.6862170087976539,smile on his face. The background is
QUERIES,0.6869501466275659,"minimalistic with a light beige color, and"
QUERIES,0.6876832844574781,there's a window that lets in natural light.
QUERIES,0.6884164222873901,The overall style of the image is a flat
QUERIES,0.6891495601173021,"illustration with a limited color palette,"
QUERIES,0.6898826979472141,giving it a modern and clean look.
QUERIES,0.6906158357771262,"Original Image
Pseudo-Manipulation"
QUERIES,0.6913489736070382,"Description
LLM’s"
QUERIES,0.6920821114369502,Rewritten Caption
QUERIES,0.6928152492668622,"Figure 10: Qualitative results of our intent-CC3M dataset. We leverage DALL-E to generate images
of the captions. We compare intent-CC3M with the CC3M dataset and LLM’s rewritten captions."
QUERIES,0.6935483870967742,"B
Algorithm of Manipulation Intention Understanding’s Process.
560"
QUERIES,0.6942815249266863,"Algorithm 1 outlines the pseudo-code for the manipulation intention understanding process. We
561"
QUERIES,0.6950146627565983,"create a fixed number of learnable embeddings as latent queries to capture intentions that the user
562"
QUERIES,0.6957478005865103,"aims to modify within manipulation descriptions. These learnable embeddings are then employed in
563"
QUERIES,0.6964809384164223,"a Transformer to execute cross-attention with the target descriptions word embedding {ti}m
i=2. The
564"
QUERIES,0.6972140762463344,"number of output tokens produced by the De-MINDS matches the count of learnable embeddings. To
565"
QUERIES,0.6979472140762464,"enhance the interaction between learnable embeddings and word embeddings, we concatenate the
566"
QUERIES,0.6986803519061584,"learnable embeddings with keys and values during the cross-attention process. Each learned query
567"
QUERIES,0.6994134897360704,"interacts with different intentions, as shown in Figure 2. To achieve a dynamic ratio during the fusion
568"
QUERIES,0.7001466275659824,"of global and intention embeddings, we utilize a tanh-gating mechanism [23].
569"
QUERIES,0.7008797653958945,Table 10: More ablation study on CIRR and FashionIQ.
QUERIES,0.7016129032258065,"CIRR
Fashion-IQ"
QUERIES,0.7023460410557185,"Methods
R1
R5
R10
R10
R50"
QUERIES,0.7030791788856305,"1.
100% original caption
26.2
55.5
69.5
26.8
49.9
2.
100% rewritten caption
25.8
55.4
69.0
26.5
49.6
3.
100% pseudo-manipulation description
25.3
54.5
68.0
26.9
49.7
4.
50% original, 50% rewritten
26.5
55.9
70.3
27.7
50.9
5.
50% original, 50% pseudo
25.5
55.2
68.6
27.0
50.1
6.
50% rewritten, 50% pseudo
25.9
55.8
69.7
27.4
50.5
7.
40% original , 30% rewritten , 30% pseudo
26.1
55.7
69.2
28.1
50.1
8.
50% original , 25% rewritten , 25% pseudo
26.7
56.5
70.4
29.2
51.4
9.
50% original , 30% rewritten , 20% pseudo
27.3
57.0
71.3
29.7
51.9
10.
w/o align loss
20.6
45.2
57.3
23.6
42.8"
QUERIES,0.7038123167155426,"C
Further Ablation Studies on the Training Strategy
570"
QUERIES,0.7045454545454546,"Table 10 details additional ablation analyses of the training strategy in De-MINDS. In model
571"
QUERIES,0.7052785923753666,"‘1-10’, we evaluate the necessity of constructs T for pre-training Our method supports two
572"
QUERIES,0.7060117302052786,"scenarios in manipulation intention understanding: integrating intention information from lengthy
573"
QUERIES,0.7067448680351907,"texts and deducing it from concise texts. We evaluated the utility of the original caption Trew, the
574"
QUERIES,0.7074780058651027,"rewritten caption Tori, and the pseudo-manipulation description Tint in fostering an understanding of
575"
QUERIES,0.7082111436950147,"manipulation intentions and ensuring training stability. Our experiments led to the optimal ratio of
576"
QUERIES,0.7089442815249267,"50% original caption, 30% rewritten caption, and 20% pseudo-manipulation description. Moreover,
577"
QUERIES,0.7096774193548387,"in model ‘9-10’, we assess the significance of the alignment loss. The absence of alignment
578"
QUERIES,0.7104105571847508,"between the original image embedding and the target embedding in pre-training results in a notable
579"
QUERIES,0.7111436950146628,"decrease in average performance by 9.54%. This highlights the crucial role of aligning the original
580"
QUERIES,0.7118768328445748,"image during training, as in CIR, both the reference image and the manipulation intention together
581"
QUERIES,0.7126099706744868,"create a comprehensive context that defines the target image.
582"
QUERIES,0.7133431085043989,"D
More Details of De-MINDS
583"
QUERIES,0.7140762463343109,"D.1
More Implementation Details For Baseline Models And Mapping Network
584"
QUERIES,0.7148093841642229,"Generating one intention caption through LLaVA-1.6-13B [32] for the entire Conceptual Caption
585"
QUERIES,0.7155425219941349,"dataset [47], which comprises 3M images (CC3M) dataset requires approximately 625 hours on 5
586"
QUERIES,0.716275659824047,"A100 GPUs. By leveraging the capabilities of LLaVA, we ensure that each text sample within the
587"
QUERIES,0.717008797653959,"dataset is enriched with diverse and contextually intent-relevant text rewrites, significantly enhancing
588"
QUERIES,0.717741935483871,"the dataset’s utility for composed image retrieval tasks. For training De-MINDS, we utilize the CC3M
589"
QUERIES,0.718475073313783,"and adopt ViT-L/14 CLIP [41] pre-trained on 400M image-text paired data. We employ AdamW [34]
590"
QUERIES,0.719208211143695,"with a learning rate of 1 × 10−6, weight decay of 0.1, and a linear warmup of 10000 steps. The batch
591"
QUERIES,0.7199413489736071,"size for contrastive learning is 1024. To improve training stability, we initialize the learnable scalar of
592"
QUERIES,0.7206744868035191,"tanh-gating to 0 [2]. For training Context-I2W, we only replace the original captions of CC3M with
593"
QUERIES,0.7214076246334311,"our pseudo-manipulation descriptions. Specifically, we employ AdamW [34] with a learning rate of
594"
QUERIES,0.7221407624633431,"1 × 10−5, weight decay of 0.1, and a linear warmup of 10000 steps. The batch size for contrastive
595"
QUERIES,0.7228739002932552,"learning is 1024. For training SEARLE, we utilize the ImageNet1K [16] test set, which comprises
596"
QUERIES,0.7236070381231672,"100K images, and leverage LLaVA to generate intention captions as detailed in Section 3.2. We
597"
QUERIES,0.7243401759530792,"employ AdamW, with a learning rate of 5 × 10−5 and a batch size of 256. All models are trained
598"
QUERIES,0.7250733137829912,"on 4 NVIDIA A100 (80G) GPUs. Moreover, we conduct ablation studies on CIRR test sets and
599"
QUERIES,0.7258064516129032,"FashionIQ validation sets. For FashionIQ, we consider the average recall. To ensure reliable results,
600"
QUERIES,0.7265395894428153,"we report the performance averaged over three trials.
601"
QUERIES,0.7272727272727273,"Mapping network design. Table 11 summarizes the mapping network fθ architecture we employ.
602"
QUERIES,0.7280058651026393,"Table 11: Pytorch-style[40] model description of the mapping network fθ. The output is fed into the
CLIP language encoder."
QUERIES,0.7287390029325513,"Layer
Module
Output
nn.Linear(512, 768)
ReLU2
nn.ReLU
Dropout2
nn.Dropout(0.1)
FC2
nn.Linear(512, 512)
ReLU1
nn.ReLU
Dropout1
nn.Dropout(0.1)
FC1
nn.Linear(512, 512)"
QUERIES,0.7294721407624634,"D.2
More Evaluation Datasets Details of Query and Candidate Images.
603"
QUERIES,0.7302052785923754,"We evaluate our model on four ZS-CIR datasets, i.e., COCO [31] for object composition, ImageNet
604"
QUERIES,0.7309384164222874,"[16, 21] for domain conversion, CIRR [33] for object/scene manipulation, and Fashion-IQ [57] for
605"
QUERIES,0.7316715542521994,"attribute manipulation. All the dataset settings and evaluation metrics (Recall@K) follow the recent
606"
QUERIES,0.7324046920821115,"works [45, 52] for a fair comparison. The evaluation datasets are preprocessed, as explained in the
607"
QUERIES,0.7331378299120235,"main paper, we describe the details of the dataset, i.e., number of query images and candidate images
608"
QUERIES,0.7338709677419355,"used for evaluation.
609"
QUERIES,0.7346041055718475,Table 12: The number of images used for evaluation in each dataset.
QUERIES,0.7353372434017595,"Dataset
Query images
Candidate images"
QUERIES,0.7360703812316716,"ImageNet
10,000
16,983
COCO
4,766
4,766
CIRR (test)
4,148
2,315
Fashion (Dress)
2,017
3,817
Fashion (Shirt)
2,038
6,346
Fashion (TopTee)
1,961
5,373"
QUERIES,0.7368035190615836,"D.3
More Inference Details of Prompts for Different Evaluate Tasks
610"
QUERIES,0.7375366568914956,"(1) Domain conversion. This setup evaluates the ability to compose real images and domain infor-
611"
QUERIES,0.7382697947214076,"mation to retrieve corresponding domain-specific images. We utilize ImageNet [16] and ImageNet-R
612"
QUERIES,0.7390029325513197,"[21], which comprises 200 classes with diverse domains and has domain annotations. Following
613"
QUERIES,0.7397360703812317,"Pic2Word, we pick cartoon, origami, toy, and sculpture as the evaluation target to avoid noise in the
614"
QUERIES,0.7404692082111437,"annotations. With this selection, we have 16,983 images as candidates. In the evaluation, given the
615"
QUERIES,0.7412023460410557,"real image from ImageNet and target domain names, we compose the query following the procedure
616"
QUERIES,0.7419354838709677,"in (a) in the Inference section. e.g., a cartoon of [*].
617"
QUERIES,0.7426686217008798,"(2) Object composition. We evaluate the validation split (5000 images) of COCO [31], which
618"
QUERIES,0.7434017595307918,"dataset contains images with corresponding lists of object classes and instance mask of query images.
619"
QUERIES,0.7441348973607038,"Following Pic2Word, we randomly crop one object and mask its background using its instance mask
620"
QUERIES,0.7448680351906158,"to create a query for each image. The list of object classes is used as text specification. Given the
621"
QUERIES,0.7456011730205279,"reference image and class list, we compose a query by following (b) in the Inference section. e.g., a
622"
QUERIES,0.7463343108504399,"photo of [*], [cat] and [dog].
623"
QUERIES,0.7470674486803519,"(3) Object/scene manipulation by text description. In this setup, a reference image is provided
624"
QUERIES,0.7478005865102639,"alongside a text description containing instructions for manipulating either an object or the background
625"
QUERIES,0.748533724340176,"scene depicted in the reference image. This composition of the reference image and text description
626"
QUERIES,0.749266862170088,"enables the retrieval of manipulated images. We evaluate the test split of CIRR [33] using the standard
627"
QUERIES,0.75,"evaluation protocol following previous works [45, 3, 52], and query texts are composed following the
628"
QUERIES,0.750733137829912,"procedure in (c) of the Inference section.
629"
QUERIES,0.751466275659824,"(4) Attribute manipulation. We employ Fashion-IQ [57], which includes various modification texts
630"
QUERIES,0.7521994134897361,"related to image attributes. These attribute manipulations are given as a sentence. As with CIRR, we
631"
QUERIES,0.7529325513196481,"adopt the standard evaluation protocol and create query texts following the procedure provided in
632"
QUERIES,0.7536656891495601,"(c) of the Inference section. In evaluation, we employ the validation set, following previous works
633"
QUERIES,0.7543988269794721,"[4, 45, 3, 52].
634"
QUERIES,0.7551319648093842,"E
Extended Related Works
635"
QUERIES,0.7558651026392962,"Mapping Image as One Word. Several methods [30, 59] represent image regions as word tokens via
636"
QUERIES,0.7565982404692082,"VLP models, which rely on object detector efficacy. However, ZR-CIR tasks extend the alignment
637"
QUERIES,0.7573313782991202,"ability beyond objects to scenes, styles, attributes, ect. Our method addresses this issue by employing
638"
QUERIES,0.7580645161290323,"pseudo triplet data, which maps a pseudo reference image to a pseudo word token and combines it
639"
QUERIES,0.7587976539589443,"with the caption to align with the target image. PALAVRA [14] proposes personalized image retrieval
640"
QUERIES,0.7595307917888563,"via cycle contrastive loss, requiring class-wise and caption annotations. In contrast, our model
641"
QUERIES,0.7602639296187683,"facilitates fine-grained image-to-word mapping without additional annotations. Other approaches
642"
QUERIES,0.7609970674486803,"[26, 36, 62, 50] utilize a single word token to represent multiple images of the same object for
643"
QUERIES,0.7617302052785924,"text-to-image generation. Our model obviates the need for costly image-supervised training.
644"
QUERIES,0.7624633431085044,"Knowledge Distillation. Knowledge distillation is a machine learning technique wherein a simpler
645"
QUERIES,0.7631964809384164,"model, known as the student, learns to mimic the behavior of a more complex model, known as
646"
QUERIES,0.7639296187683284,"the teacher, by learning from its predictions [22]. This approach has demonstrated efficacy across
647"
QUERIES,0.7646627565982405,"various computer vision tasks, including image classification [22, 43, 5], object detection [9, 8],
648"
QUERIES,0.7653958944281525,"and text-to-image synthesis [35, 46], resulting in improved model compression, computational
649"
QUERIES,0.7661290322580645,"efficiency, and accuracy. In our study, we employ knowledge distillation to transfer knowledge from
650"
QUERIES,0.7668621700879765,"a computationally expensive optimization method (teacher) to a more lightweight neural network
651"
QUERIES,0.7675953079178885,"(student). Specifically, we train a manipulation intention understanding network to replicate the
652"
QUERIES,0.7683284457478006,"reasoning ability of an MLLM using a distillation loss. Alternatively, our lightweight network can be
653"
QUERIES,0.7690615835777126,"interpreted as a surrogate model of the more resource-intensive technique.
654"
QUERIES,0.7697947214076246,"NeurIPS Paper Checklist
655"
CLAIMS,0.7705278592375366,"1. Claims
656"
CLAIMS,0.7712609970674487,"Question: Do the main claims made in the abstract and introduction accurately reflect the
657"
CLAIMS,0.7719941348973607,"paper’s contributions and scope?
658"
CLAIMS,0.7727272727272727,"Answer: [Yes]
659"
CLAIMS,0.7734604105571847,"Justification: The abstract and introduction are include the claims made in the paper
660"
CLAIMS,0.7741935483870968,"Guidelines:
661"
CLAIMS,0.7749266862170088,"• The answer NA means that the abstract and introduction do not include the claims
662"
CLAIMS,0.7756598240469208,"made in the paper.
663"
CLAIMS,0.7763929618768328,"• The abstract and/or introduction should clearly state the claims made, including the
664"
CLAIMS,0.7771260997067448,"contributions made in the paper and important assumptions and limitations. A No or
665"
CLAIMS,0.7778592375366569,"NA answer to this question will not be perceived well by the reviewers.
666"
CLAIMS,0.7785923753665689,"• The claims made should match theoretical and experimental results, and reflect how
667"
CLAIMS,0.7793255131964809,"much the results can be expected to generalize to other settings.
668"
CLAIMS,0.7800586510263929,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
669"
CLAIMS,0.780791788856305,"are not attained by the paper.
670"
LIMITATIONS,0.781524926686217,"2. Limitations
671"
LIMITATIONS,0.782258064516129,"Question: Does the paper discuss the limitations of the work performed by the authors?
672"
LIMITATIONS,0.782991202346041,"Answer: [Yes]
673"
LIMITATIONS,0.783724340175953,"Justification: Our paper has limitation in our main paper.
674"
LIMITATIONS,0.7844574780058651,"Guidelines:
675"
LIMITATIONS,0.7851906158357771,"• The answer NA means that the paper has no limitation while the answer No means that
676"
LIMITATIONS,0.7859237536656891,"the paper has limitations, but those are not discussed in the paper.
677"
LIMITATIONS,0.7866568914956011,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
678"
LIMITATIONS,0.7873900293255132,"• The paper should point out any strong assumptions and how robust the results are to
679"
LIMITATIONS,0.7881231671554252,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
680"
LIMITATIONS,0.7888563049853372,"model well-specification, asymptotic approximations only holding locally). The authors
681"
LIMITATIONS,0.7895894428152492,"should reflect on how these assumptions might be violated in practice and what the
682"
LIMITATIONS,0.7903225806451613,"implications would be.
683"
LIMITATIONS,0.7910557184750733,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
684"
LIMITATIONS,0.7917888563049853,"only tested on a few datasets or with a few runs. In general, empirical results often
685"
LIMITATIONS,0.7925219941348973,"depend on implicit assumptions, which should be articulated.
686"
LIMITATIONS,0.7932551319648093,"• The authors should reflect on the factors that influence the performance of the approach.
687"
LIMITATIONS,0.7939882697947214,"For example, a facial recognition algorithm may perform poorly when image resolution
688"
LIMITATIONS,0.7947214076246334,"is low or images are taken in low lighting. Or a speech-to-text system might not be
689"
LIMITATIONS,0.7954545454545454,"used reliably to provide closed captions for online lectures because it fails to handle
690"
LIMITATIONS,0.7961876832844574,"technical jargon.
691"
LIMITATIONS,0.7969208211143695,"• The authors should discuss the computational efficiency of the proposed algorithms
692"
LIMITATIONS,0.7976539589442815,"and how they scale with dataset size.
693"
LIMITATIONS,0.7983870967741935,"• If applicable, the authors should discuss possible limitations of their approach to
694"
LIMITATIONS,0.7991202346041055,"address problems of privacy and fairness.
695"
LIMITATIONS,0.7998533724340176,"• While the authors might fear that complete honesty about limitations might be used by
696"
LIMITATIONS,0.8005865102639296,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
697"
LIMITATIONS,0.8013196480938416,"limitations that aren’t acknowledged in the paper. The authors should use their best
698"
LIMITATIONS,0.8020527859237536,"judgment and recognize that individual actions in favor of transparency play an impor-
699"
LIMITATIONS,0.8027859237536656,"tant role in developing norms that preserve the integrity of the community. Reviewers
700"
LIMITATIONS,0.8035190615835777,"will be specifically instructed to not penalize honesty concerning limitations.
701"
THEORY ASSUMPTIONS AND PROOFS,0.8042521994134897,"3. Theory Assumptions and Proofs
702"
THEORY ASSUMPTIONS AND PROOFS,0.8049853372434017,"Question: For each theoretical result, does the paper provide the full set of assumptions and
703"
THEORY ASSUMPTIONS AND PROOFS,0.8057184750733137,"a complete (and correct) proof?
704"
THEORY ASSUMPTIONS AND PROOFS,0.8064516129032258,"Answer: [Yes]
705"
THEORY ASSUMPTIONS AND PROOFS,0.8071847507331378,"Justification: All the formulas in the paper be numbered and cross-referenced
706"
THEORY ASSUMPTIONS AND PROOFS,0.8079178885630498,"Guidelines:
707"
THEORY ASSUMPTIONS AND PROOFS,0.8086510263929618,"• The answer NA means that the paper does not include theoretical results.
708"
THEORY ASSUMPTIONS AND PROOFS,0.8093841642228738,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
709"
THEORY ASSUMPTIONS AND PROOFS,0.8101173020527859,"referenced.
710"
THEORY ASSUMPTIONS AND PROOFS,0.8108504398826979,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
711"
THEORY ASSUMPTIONS AND PROOFS,0.8115835777126099,"• The proofs can either appear in the main paper or the supplemental material, but if
712"
THEORY ASSUMPTIONS AND PROOFS,0.8123167155425219,"they appear in the supplemental material, the authors are encouraged to provide a short
713"
THEORY ASSUMPTIONS AND PROOFS,0.8130498533724341,"proof sketch to provide intuition.
714"
THEORY ASSUMPTIONS AND PROOFS,0.8137829912023461,"• Inversely, any informal proof provided in the core of the paper should be complemented
715"
THEORY ASSUMPTIONS AND PROOFS,0.8145161290322581,"by formal proofs provided in appendix or supplemental material.
716"
THEORY ASSUMPTIONS AND PROOFS,0.8152492668621701,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
717"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8159824046920822,"4. Experimental Result Reproducibility
718"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8167155425219942,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
719"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8174486803519062,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
720"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8181818181818182,"of the paper (regardless of whether the code and data are provided or not)?
721"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8189149560117303,"Answer: [Yes]
722"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8196480938416423,"Justification: The code and sample dataset are provided in our supplementary. We describe
723"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8203812316715543,"the steps taken to make the results reproducible or verifiable.
724"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8211143695014663,"Guidelines:
725"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8218475073313783,"• The answer NA means that the paper does not include experiments.
726"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8225806451612904,"• If the paper includes experiments, a No answer to this question will not be perceived
727"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8233137829912024,"well by the reviewers: Making the paper reproducible is important, regardless of
728"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8240469208211144,"whether the code and data are provided or not.
729"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8247800586510264,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
730"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8255131964809385,"to make their results reproducible or verifiable.
731"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8262463343108505,"• Depending on the contribution, reproducibility can be accomplished in various ways.
732"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8269794721407625,"For example, if the contribution is a novel architecture, describing the architecture fully
733"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8277126099706745,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
734"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8284457478005866,"be necessary to either make it possible for others to replicate the model with the same
735"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8291788856304986,"dataset, or provide access to the model. In general. releasing code and data is often
736"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8299120234604106,"one good way to accomplish this, but reproducibility can also be provided via detailed
737"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8306451612903226,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8313782991202346,"of a large language model), releasing of a model checkpoint, or other means that are
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8321114369501467,"appropriate to the research performed.
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8328445747800587,"• While NeurIPS does not require releasing code, the conference does require all submis-
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8335777126099707,"sions to provide some reasonable avenue for reproducibility, which may depend on the
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8343108504398827,"nature of the contribution. For example
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8350439882697948,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8357771260997068,"to reproduce that algorithm.
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8365102639296188,"(b) If the contribution is primarily a new model architecture, the paper should describe
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8372434017595308,"the architecture clearly and fully.
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8379765395894428,"(c) If the contribution is a new model (e.g., a large language model), then there should
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8387096774193549,"either be a way to access this model for reproducing the results or a way to reproduce
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8394428152492669,"the model (e.g., with an open-source dataset or instructions for how to construct
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8401759530791789,"the dataset).
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8409090909090909,"(d) We recognize that reproducibility may be tricky in some cases, in which case
752"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.841642228739003,"authors are welcome to describe the particular way they provide for reproducibility.
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.842375366568915,"In the case of closed-source models, it may be that access to the model is limited in
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.843108504398827,"some way (e.g., to registered users), but it should be possible for other researchers
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.843841642228739,"to have some path to reproducing or verifying the results.
756"
OPEN ACCESS TO DATA AND CODE,0.844574780058651,"5. Open access to data and code
757"
OPEN ACCESS TO DATA AND CODE,0.8453079178885631,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
758"
OPEN ACCESS TO DATA AND CODE,0.8460410557184751,"tions to faithfully reproduce the main experimental results, as described in supplemental
759"
OPEN ACCESS TO DATA AND CODE,0.8467741935483871,"material?
760"
OPEN ACCESS TO DATA AND CODE,0.8475073313782991,"Answer: [Yes]
761"
OPEN ACCESS TO DATA AND CODE,0.8482404692082112,"Justification: Our paper provides open access to the code for creating the dataset and
762"
OPEN ACCESS TO DATA AND CODE,0.8489736070381232,"reproducing the main experimental results. We will provide the entire dataset after our paper
763"
OPEN ACCESS TO DATA AND CODE,0.8497067448680352,"is accepted.
764"
OPEN ACCESS TO DATA AND CODE,0.8504398826979472,"Guidelines:
765"
OPEN ACCESS TO DATA AND CODE,0.8511730205278593,"• The answer NA means that paper does not include experiments requiring code.
766"
OPEN ACCESS TO DATA AND CODE,0.8519061583577713,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
767"
OPEN ACCESS TO DATA AND CODE,0.8526392961876833,"public/guides/CodeSubmissionPolicy) for more details.
768"
OPEN ACCESS TO DATA AND CODE,0.8533724340175953,"• While we encourage the release of code and data, we understand that this might not be
769"
OPEN ACCESS TO DATA AND CODE,0.8541055718475073,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
770"
OPEN ACCESS TO DATA AND CODE,0.8548387096774194,"including code, unless this is central to the contribution (e.g., for a new open-source
771"
OPEN ACCESS TO DATA AND CODE,0.8555718475073314,"benchmark).
772"
OPEN ACCESS TO DATA AND CODE,0.8563049853372434,"• The instructions should contain the exact command and environment needed to run to
773"
OPEN ACCESS TO DATA AND CODE,0.8570381231671554,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
774"
OPEN ACCESS TO DATA AND CODE,0.8577712609970675,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
775"
OPEN ACCESS TO DATA AND CODE,0.8585043988269795,"• The authors should provide instructions on data access and preparation, including how
776"
OPEN ACCESS TO DATA AND CODE,0.8592375366568915,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
777"
OPEN ACCESS TO DATA AND CODE,0.8599706744868035,"• The authors should provide scripts to reproduce all experimental results for the new
778"
OPEN ACCESS TO DATA AND CODE,0.8607038123167156,"proposed method and baselines. If only a subset of experiments are reproducible, they
779"
OPEN ACCESS TO DATA AND CODE,0.8614369501466276,"should state which ones are omitted from the script and why.
780"
OPEN ACCESS TO DATA AND CODE,0.8621700879765396,"• At submission time, to preserve anonymity, the authors should release anonymized
781"
OPEN ACCESS TO DATA AND CODE,0.8629032258064516,"versions (if applicable).
782"
OPEN ACCESS TO DATA AND CODE,0.8636363636363636,"• Providing as much information as possible in supplemental material (appended to the
783"
OPEN ACCESS TO DATA AND CODE,0.8643695014662757,"paper) is recommended, but including URLs to data and code is permitted.
784"
OPEN ACCESS TO DATA AND CODE,0.8651026392961877,"6. Experimental Setting/Details
785"
OPEN ACCESS TO DATA AND CODE,0.8658357771260997,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
786"
OPEN ACCESS TO DATA AND CODE,0.8665689149560117,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
787"
OPEN ACCESS TO DATA AND CODE,0.8673020527859238,"results?
788"
OPEN ACCESS TO DATA AND CODE,0.8680351906158358,"Answer: [Yes]
789"
OPEN ACCESS TO DATA AND CODE,0.8687683284457478,"Justification: Our paper specifies all the training and test details in the main paper and
790"
OPEN ACCESS TO DATA AND CODE,0.8695014662756598,"appendix. We also provide the pseudo-code for our method in our appendix.
791"
OPEN ACCESS TO DATA AND CODE,0.8702346041055719,"Guidelines:
792"
OPEN ACCESS TO DATA AND CODE,0.8709677419354839,"• The answer NA means that the paper does not include experiments.
793"
OPEN ACCESS TO DATA AND CODE,0.8717008797653959,"• The experimental setting should be presented in the core of the paper to a level of detail
794"
OPEN ACCESS TO DATA AND CODE,0.8724340175953079,"that is necessary to appreciate the results and make sense of them.
795"
OPEN ACCESS TO DATA AND CODE,0.8731671554252199,"• The full details can be provided either with the code, in appendix, or as supplemental
796"
OPEN ACCESS TO DATA AND CODE,0.873900293255132,"material.
797"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.874633431085044,"7. Experiment Statistical Significance
798"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.875366568914956,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
799"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.876099706744868,"information about the statistical significance of the experiments?
800"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8768328445747801,"Answer: [No]
801"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8775659824046921,"Justification: Error bars are not reported because it would be too computationally expensive
802"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8782991202346041,"for four datasets.
803"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8790322580645161,"Guidelines:
804"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8797653958944281,"• The answer NA means that the paper does not include experiments.
805"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8804985337243402,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
806"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8812316715542522,"dence intervals, or statistical significance tests, at least for the experiments that support
807"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8819648093841642,"the main claims of the paper.
808"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8826979472140762,"• The factors of variability that the error bars are capturing should be clearly stated (for
809"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8834310850439883,"example, train/test split, initialization, random drawing of some parameter, or overall
810"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8841642228739003,"run with given experimental conditions).
811"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8848973607038123,"• The method for calculating the error bars should be explained (closed form formula,
812"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8856304985337243,"call to a library function, bootstrap, etc.)
813"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8863636363636364,"• The assumptions made should be given (e.g., Normally distributed errors).
814"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8870967741935484,"• It should be clear whether the error bar is the standard deviation or the standard error
815"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8878299120234604,"of the mean.
816"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8885630498533724,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
817"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8892961876832844,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
818"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8900293255131965,"of Normality of errors is not verified.
819"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8907624633431085,"• For asymmetric distributions, the authors should be careful not to show in tables or
820"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8914956011730205,"figures symmetric error bars that would yield results that are out of range (e.g. negative
821"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8922287390029325,"error rates).
822"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8929618768328446,"• If error bars are reported in tables or plots, The authors should explain in the text how
823"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8936950146627566,"they were calculated and reference the corresponding figures or tables in the text.
824"
EXPERIMENTS COMPUTE RESOURCES,0.8944281524926686,"8. Experiments Compute Resources
825"
EXPERIMENTS COMPUTE RESOURCES,0.8951612903225806,"Question: For each experiment, does the paper provide sufficient information on the com-
826"
EXPERIMENTS COMPUTE RESOURCES,0.8958944281524927,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
827"
EXPERIMENTS COMPUTE RESOURCES,0.8966275659824047,"the experiments?
828"
EXPERIMENTS COMPUTE RESOURCES,0.8973607038123167,"Answer: [Yes]
829"
EXPERIMENTS COMPUTE RESOURCES,0.8980938416422287,"Justification: We indicate the type of compute workers and compute time for dataset
830"
EXPERIMENTS COMPUTE RESOURCES,0.8988269794721407,"generation and training.
831"
EXPERIMENTS COMPUTE RESOURCES,0.8995601173020528,"Guidelines:
832"
EXPERIMENTS COMPUTE RESOURCES,0.9002932551319648,"• The answer NA means that the paper does not include experiments.
833"
EXPERIMENTS COMPUTE RESOURCES,0.9010263929618768,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
834"
EXPERIMENTS COMPUTE RESOURCES,0.9017595307917888,"or cloud provider, including relevant memory and storage.
835"
EXPERIMENTS COMPUTE RESOURCES,0.9024926686217009,"• The paper should provide the amount of compute required for each of the individual
836"
EXPERIMENTS COMPUTE RESOURCES,0.9032258064516129,"experimental runs as well as estimate the total compute.
837"
EXPERIMENTS COMPUTE RESOURCES,0.9039589442815249,"• The paper should disclose whether the full research project required more compute
838"
EXPERIMENTS COMPUTE RESOURCES,0.9046920821114369,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
839"
EXPERIMENTS COMPUTE RESOURCES,0.905425219941349,"didn’t make it into the paper).
840"
CODE OF ETHICS,0.906158357771261,"9. Code Of Ethics
841"
CODE OF ETHICS,0.906891495601173,"Question: Does the research conducted in the paper conform, in every respect, with the
842"
CODE OF ETHICS,0.907624633431085,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
843"
CODE OF ETHICS,0.908357771260997,"Answer: [Yes]
844"
CODE OF ETHICS,0.9090909090909091,"Justification: We have reviewed the NeurIPS Code of Ethics.
845"
CODE OF ETHICS,0.9098240469208211,"Guidelines:
846"
CODE OF ETHICS,0.9105571847507331,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
847"
CODE OF ETHICS,0.9112903225806451,"• If the authors answer No, they should explain the special circumstances that require a
848"
CODE OF ETHICS,0.9120234604105572,"deviation from the Code of Ethics.
849"
CODE OF ETHICS,0.9127565982404692,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
850"
CODE OF ETHICS,0.9134897360703812,"eration due to laws or regulations in their jurisdiction).
851"
BROADER IMPACTS,0.9142228739002932,"10. Broader Impacts
852"
BROADER IMPACTS,0.9149560117302052,"Question: Does the paper discuss both potential positive societal impacts and negative
853"
BROADER IMPACTS,0.9156891495601173,"societal impacts of the work performed?
854"
BROADER IMPACTS,0.9164222873900293,"Answer: [Yes]
855"
BROADER IMPACTS,0.9171554252199413,"Justification: We discuss both potential positive societal impacts and negative societal
856"
BROADER IMPACTS,0.9178885630498533,"impacts of the work performed in our appendix.
857"
BROADER IMPACTS,0.9186217008797654,"Guidelines:
858"
BROADER IMPACTS,0.9193548387096774,"• The answer NA means that there is no societal impact of the work performed.
859"
BROADER IMPACTS,0.9200879765395894,"• If the authors answer NA or No, they should explain why their work has no societal
860"
BROADER IMPACTS,0.9208211143695014,"impact or why the paper does not address societal impact.
861"
BROADER IMPACTS,0.9215542521994134,"• Examples of negative societal impacts include potential malicious or unintended uses
862"
BROADER IMPACTS,0.9222873900293255,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
863"
BROADER IMPACTS,0.9230205278592375,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
864"
BROADER IMPACTS,0.9237536656891495,"groups), privacy considerations, and security considerations.
865"
BROADER IMPACTS,0.9244868035190615,"• The conference expects that many papers will be foundational research and not tied
866"
BROADER IMPACTS,0.9252199413489736,"to particular applications, let alone deployments. However, if there is a direct path to
867"
BROADER IMPACTS,0.9259530791788856,"any negative applications, the authors should point it out. For example, it is legitimate
868"
BROADER IMPACTS,0.9266862170087976,"to point out that an improvement in the quality of generative models could be used to
869"
BROADER IMPACTS,0.9274193548387096,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
870"
BROADER IMPACTS,0.9281524926686217,"that a generic algorithm for optimizing neural networks could enable people to train
871"
BROADER IMPACTS,0.9288856304985337,"models that generate Deepfakes faster.
872"
BROADER IMPACTS,0.9296187683284457,"• The authors should consider possible harms that could arise when the technology is
873"
BROADER IMPACTS,0.9303519061583577,"being used as intended and functioning correctly, harms that could arise when the
874"
BROADER IMPACTS,0.9310850439882697,"technology is being used as intended but gives incorrect results, and harms following
875"
BROADER IMPACTS,0.9318181818181818,"from (intentional or unintentional) misuse of the technology.
876"
BROADER IMPACTS,0.9325513196480938,"• If there are negative societal impacts, the authors could also discuss possible mitigation
877"
BROADER IMPACTS,0.9332844574780058,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
878"
BROADER IMPACTS,0.9340175953079178,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
879"
BROADER IMPACTS,0.9347507331378299,"feedback over time, improving the efficiency and accessibility of ML).
880"
SAFEGUARDS,0.9354838709677419,"11. Safeguards
881"
SAFEGUARDS,0.9362170087976539,"Question: Does the paper describe safeguards that have been put in place for responsible
882"
SAFEGUARDS,0.9369501466275659,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
883"
SAFEGUARDS,0.9376832844574781,"image generators, or scraped datasets)?
884"
SAFEGUARDS,0.9384164222873901,"Answer: [No]
885"
SAFEGUARDS,0.9391495601173021,"Justification: our paper poses no such risks.
886"
SAFEGUARDS,0.9398826979472141,"Guidelines:
887"
SAFEGUARDS,0.9406158357771262,"• The answer NA means that the paper poses no such risks.
888"
SAFEGUARDS,0.9413489736070382,"• Released models that have a high risk for misuse or dual-use should be released with
889"
SAFEGUARDS,0.9420821114369502,"necessary safeguards to allow for controlled use of the model, for example by requiring
890"
SAFEGUARDS,0.9428152492668622,"that users adhere to usage guidelines or restrictions to access the model or implementing
891"
SAFEGUARDS,0.9435483870967742,"safety filters.
892"
SAFEGUARDS,0.9442815249266863,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
893"
SAFEGUARDS,0.9450146627565983,"should describe how they avoided releasing unsafe images.
894"
SAFEGUARDS,0.9457478005865103,"• We recognize that providing effective safeguards is challenging, and many papers do
895"
SAFEGUARDS,0.9464809384164223,"not require this, but we encourage authors to take this into account and make a best
896"
SAFEGUARDS,0.9472140762463344,"faith effort.
897"
LICENSES FOR EXISTING ASSETS,0.9479472140762464,"12. Licenses for existing assets
898"
LICENSES FOR EXISTING ASSETS,0.9486803519061584,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
899"
LICENSES FOR EXISTING ASSETS,0.9494134897360704,"the paper, properly credited and are the license and terms of use explicitly mentioned and
900"
LICENSES FOR EXISTING ASSETS,0.9501466275659824,"properly respected?
901"
LICENSES FOR EXISTING ASSETS,0.9508797653958945,"Answer: [Yes]
902"
LICENSES FOR EXISTING ASSETS,0.9516129032258065,"Justification: the creators or original owners of assets are the license and terms of use
903"
LICENSES FOR EXISTING ASSETS,0.9523460410557185,"explicitly mentioned and properly respected.
904"
LICENSES FOR EXISTING ASSETS,0.9530791788856305,"Guidelines:
905"
LICENSES FOR EXISTING ASSETS,0.9538123167155426,"• The answer NA means that the paper does not use existing assets.
906"
LICENSES FOR EXISTING ASSETS,0.9545454545454546,"• The authors should cite the original paper that produced the code package or dataset.
907"
LICENSES FOR EXISTING ASSETS,0.9552785923753666,"• The authors should state which version of the asset is used and, if possible, include a
908"
LICENSES FOR EXISTING ASSETS,0.9560117302052786,"URL.
909"
LICENSES FOR EXISTING ASSETS,0.9567448680351907,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
910"
LICENSES FOR EXISTING ASSETS,0.9574780058651027,"• For scraped data from a particular source (e.g., website), the copyright and terms of
911"
LICENSES FOR EXISTING ASSETS,0.9582111436950147,"service of that source should be provided.
912"
LICENSES FOR EXISTING ASSETS,0.9589442815249267,"• If assets are released, the license, copyright information, and terms of use in the
913"
LICENSES FOR EXISTING ASSETS,0.9596774193548387,"package should be provided. For popular datasets, paperswithcode.com/datasets
914"
LICENSES FOR EXISTING ASSETS,0.9604105571847508,"has curated licenses for some datasets. Their licensing guide can help determine the
915"
LICENSES FOR EXISTING ASSETS,0.9611436950146628,"license of a dataset.
916"
LICENSES FOR EXISTING ASSETS,0.9618768328445748,"• For existing datasets that are re-packaged, both the original license and the license of
917"
LICENSES FOR EXISTING ASSETS,0.9626099706744868,"the derived asset (if it has changed) should be provided.
918"
LICENSES FOR EXISTING ASSETS,0.9633431085043989,"• If this information is not available online, the authors are encouraged to reach out to
919"
LICENSES FOR EXISTING ASSETS,0.9640762463343109,"the asset’s creators.
920"
NEW ASSETS,0.9648093841642229,"13. New Assets
921"
NEW ASSETS,0.9655425219941349,"Question: Are new assets introduced in the paper well documented and is the documentation
922"
NEW ASSETS,0.966275659824047,"provided alongside the assets?
923"
NEW ASSETS,0.967008797653959,"Answer: [No]
924"
NEW ASSETS,0.967741935483871,"Justification: Our paper does not release new assets.
925"
NEW ASSETS,0.968475073313783,"Guidelines:
926"
NEW ASSETS,0.969208211143695,"• The answer NA means that the paper does not release new assets.
927"
NEW ASSETS,0.9699413489736071,"• Researchers should communicate the details of the dataset/code/model as part of their
928"
NEW ASSETS,0.9706744868035191,"submissions via structured templates. This includes details about training, license,
929"
NEW ASSETS,0.9714076246334311,"limitations, etc.
930"
NEW ASSETS,0.9721407624633431,"• The paper should discuss whether and how consent was obtained from people whose
931"
NEW ASSETS,0.9728739002932552,"asset is used.
932"
NEW ASSETS,0.9736070381231672,"• At submission time, remember to anonymize your assets (if applicable). You can either
933"
NEW ASSETS,0.9743401759530792,"create an anonymized URL or include an anonymized zip file.
934"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9750733137829912,"14. Crowdsourcing and Research with Human Subjects
935"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758064516129032,"Question: For crowdsourcing experiments and research with human subjects, does the paper
936"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9765395894428153,"include the full text of instructions given to participants and screenshots, if applicable, as
937"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772727272727273,"well as details about compensation (if any)?
938"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780058651026393,"Answer: [No]
939"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787390029325513,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
940"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9794721407624634,"Guidelines:
941"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802052785923754,"• The answer NA means that the paper does not involve crowdsourcing nor research with
942"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809384164222874,"human subjects.
943"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816715542521994,"• Including this information in the supplemental material is fine, but if the main contribu-
944"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824046920821115,"tion of the paper involves human subjects, then as much detail as possible should be
945"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831378299120235,"included in the main paper.
946"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838709677419355,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
947"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846041055718475,"or other labor should be paid at least the minimum wage in the country of the data
948"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853372434017595,"collector.
949"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860703812316716,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
950"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868035190615836,"Subjects
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875366568914956,"Question: Does the paper describe potential risks incurred by study participants, whether
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882697947214076,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890029325513197,"approvals (or an equivalent approval/review based on the requirements of your country or
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897360703812317,"institution) were obtained?
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904692082111437,"Answer: [No]
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912023460410557,"Justification: Our paper does not involve crowdsourcing nor research with human subjects
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9919354838709677,"Guidelines:
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926686217008798,"• The answer NA means that the paper does not involve crowdsourcing nor research with
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934017595307918,"human subjects.
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941348973607038,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948680351906158,"may be required for any human subjects research. If you obtained IRB approval, you
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956011730205279,"should clearly state this in the paper.
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9963343108504399,"• We recognize that the procedures for this may vary significantly between institutions
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970674486803519,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
965"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978005865102639,"guidelines for their institution.
966"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998533724340176,"• For initial submissions, do not include any information that would break anonymity (if
967"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.999266862170088,"applicable), such as the institution conducting the review.
968"
