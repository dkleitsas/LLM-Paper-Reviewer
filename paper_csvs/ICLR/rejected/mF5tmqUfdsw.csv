Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0037174721189591076,"Evolution based zeroth-order optimization methods and policy gradient based
ﬁrst-order methods are two promising alternatives to solve reinforcement learn-
ing (RL) problems with complementary advantages. The former work with ar-
bitrary policies, drive state-dependent and temporally-extended exploration, pos-
sess robustness-seeking property, but suffer from high sample complexity, while
the latter are more sample efﬁcient but restricted to differentiable policies and the
learned policies are less robust. We propose Zeroth-Order Actor-Critic algorithm
(ZOAC) that uniﬁes these two methods into an on-policy actor-critic architec-
ture to preserve the advantages from both. ZOAC conducts rollouts collection
with timestep-wise perturbation in parameter space, ﬁrst-order policy evaluation
(PEV) and zeroth-order policy improvement (PIM) alternately in each iteration.
The modiﬁed rollouts collection strategy and the introduced critic network help
to reduce the variance of zeroth-order gradient estimators and improve the sam-
ple efﬁciency and stability of the algorithm. We evaluate our proposed method
using two different types of policies, linear policies and neural networks, on a
range of challenging continuous control benchmarks, where ZOAC outperforms
zeroth-order and ﬁrst-order baseline algorithms."
INTRODUCTION,0.007434944237918215,"1
INTRODUCTION"
INTRODUCTION,0.011152416356877323,"Reinforcement learning (RL) has achieved great success in a wide range of challenging domains,
including video games (Mnih et al., 2015), robotic control (Schulman et al., 2017), autonomous driv-
ing (Kendall et al., 2019), etc. The majority of RL methods formulate the environment as Markov
decision process (MDP) and leverage the temporal structure to design learning algorithms such as
Q-learning and policy gradient (Sutton & Barto, 2018). Actor-critic methods are among the most
popular RL algorithms, which usually introduce two function approximators, one for value func-
tion estimation (critic) and another for optimal policy approximation (actor), and optimize these two
approximators by alternating between policy evaluation (PEV) and policy improvement (PIM). On-
policy actor-critic methods, e.g., A3C (Mnih et al., 2016) and PPO (Schulman et al., 2017), often
use critics to construct advantage functions and substitute them for the Monte Carlo return used in
vanilla policy gradient (Williams, 1992), which signiﬁcantly reduces the variance of gradient esti-
mation and improve learning speed and stability. Among existing actor-critic algorithms, a common
choice is to use deep neural networks as the function approximators and conduct both PEV and PIM
using ﬁrst-order optimization techniques."
INTRODUCTION,0.01486988847583643,"An alternative approach for RL, though less popular, is to ignore the underlying MDP structures
and regard RL problems as black-box optimization, and to directly search for the optimal policy in
a zeroth-order way, i.e., without using the ﬁrst-order gradient information. Recent researches have
shown that zeroth-order optimization (ZOO) methods, e.g., ES (Salimans et al., 2017), ARS (Mania
et al., 2018) and GA (Such et al., 2017), are competitive on common RL benchmarks, even when
applied to deep neural network with millions of parameters. ZOO has several advantages compared
to ﬁrst-order MDP-based RL methods (Sehnke et al., 2010; Salimans et al., 2017; Such et al., 2017;
Lehman et al., 2018; Khadka & Tumer, 2018; Qian & Yu, 2021): (1) ZOO is not restricted to
differentiable policies; (2) ZOO perturbs the policy in parameter space rather than in action space,
which leads to state-dependent and temporally-extended exploration; (3) Zeroth-order population-
based optimization possesses robustness-seeking property and diverse policy behaviors."
INTRODUCTION,0.01858736059479554,"Despite these attractive advantages, the main limitation of ZOO is its high sample complexity and
high variance of the parameter update process, especially in high-dimensional problems. Recent"
INTRODUCTION,0.022304832713754646,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.026022304832713755,"researches have proposed various techniques to improve ZOO, e.g., using orthogonal or antithetic
sampling methods (Sehnke et al., 2010; Salimans et al., 2017; Choromanski et al., 2018; Mania
et al., 2018), identifying a low-dimensional search subspace (Maheswaranathan et al., 2019; Choro-
manski et al., 2019; Sener & Koltun, 2020), or subtracting a baseline for variance reduction (Sehnke
et al., 2010; Grathwohl et al., 2018). One of the major reasons for the sample inefﬁciency of ZOO is
its ignorance of the MDP temporal structures. Many recent researches have tried to combine ZOO
and ﬁrst-order MDP-based RL into hybrid methods, e.g., run evolutionary algorithms in parallel
with off-policy RL algorithms and optimize the population of policies with information from both
sides (Khadka & Tumer, 2018; Pourchot & Sigaud, 2018; Bodnar et al., 2020), or inject parameter
noise into existing RL algorithms for efﬁcient exploration (Plappert et al., 2018; Fortunato et al.,
2018). However, existing hybrid methods still conduct ﬁrst-order gradient-based policy improve-
ment (at least as a part), which reimposes differentiable requirement on the policy."
INTRODUCTION,0.02973977695167286,"In this paper, we propose the Zeroth-Order Actor-Critic algorithm (ZOAC), which uniﬁes ﬁrst-order
and zeroth-order RL methods into an actor-critic architecture by conducting ﬁrst-order PEV to up-
date the critic and zeroth-order PIM to update the actor. In such a way, complementary advantages
of both methods are preserved, e.g., wide adaptability to policy parameterization, robustness seeking
property, state-dependent and temporally-extended exploration. We modify the rollouts collection
strategy from episode-wise perturbation as in traditional zeroth-order methods to timestep-wise per-
turbation, which results in higher sample efﬁciency and better exploration. We derive the zeroth-
order policy gradient under this setting and point out that a critic network can be introduced to
estimate the state-value function and trade-off between bias and variance. We then propose a prac-
tical algorithm that utilizes several parallelized rollout workers and alternates between ﬁrst-order
PEV and zeroth-order PIM based on generated experiences in each iteration. We evaluate ZOAC on
a range of challenging continuous control benchmarks from OpenAI gym (Brockman et al., 2016),
using two different types of policies, linear policies and neural networks. Experiment results show
that ZOAC outperforms zeroth-order and ﬁrst-order baseline algorithms in sample efﬁciency, ﬁnal
performance, and the robustness of the learned policies. We visualize the polices learned in an
environment with sparse and delayed reward, which indicates sufﬁcient exploration driven by pa-
rameter noise in ZOAC. Furthermore, we conduct ablation studies to demonstrate the indispensable
contribution of the modiﬁed rollouts collection strategy and the introduced critic network to ZOAC."
PRELIMINARIES,0.03345724907063197,"2
PRELIMINARIES"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.03717472118959108,"2.1
FROM POLICY GRADIENT TO ACTOR-CRITIC"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.040892193308550186,"In standard MDP-based RL settings, the environment is usually formulated as an MDP deﬁned as
(S, A, P, r), where S is the state space, A is the action space, P : S × A × S →R is the transition
probability matrix, r : S × A →R is the reward function. The return is deﬁned as the total
discounted future reward Gt = P∞
i=0 γir(st+i, at+i), where γ ∈(0, 1) is the discounting factor.
The behavior of the agent is controlled by a policy π(a|s) : S × A →[0, 1], which maps states to a
probability distribution over actions. The state-value function is deﬁned as the expected return under
policy π starting from a certain state: V π(s) = Ea∼π{Gt|st = s}. The goal of MDP-based RL is
to ﬁnd an optimal policy that maximizes the expectation of state-value function under a certain state
distribution. Denoting a policy parameterized with θ as πθ, the objective function can be written as:"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.04460966542750929,"JPG(θ) = Es∼d[V πθ(s)],
d = d0 or dπθ
(1)"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.048327137546468404,"where d0 is the initial state distribution and dπθ is the stationary state distribution of Markov chain
under policy πθ. Generally, the former is used for episodic tasks with ﬁnite horizon and the latter
is used for continuing tasks with inﬁnite horizon. For any differentiable policy πθ, and for con-
tinuing or episodic tasks, the same form of policy gradient can be derived from the policy gradient
theorem (Sutton & Barto, 2018). This vanilla policy gradient given by Williams (1992) is as follows:"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.05204460966542751,"∇θJPG(θ) = Est∼dπθ ,at∼πθ[Gt∇θ log πθ(at|st)]
(2)"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.055762081784386616,"Vanilla policy gradient suffers from high variance since it directly uses Monte Carlo return from
sampled trajectories. Actor-critic methods improved upon it, which usually introduce a critic net-
work to estimate the value function and serve as a baseline to substitute the expected return Gt with
a proper form of advantage function At, for example, TD residual (Mnih et al., 2016), or gener-
alized advantage estimation (GAE) (Schulman et al., 2015). However, the above policy gradient"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.05947955390334572,Under review as a conference paper at ICLR 2022
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.06319702602230483,"based methods can only be applied to differentiable policies, and may be unavailable when a non-
differentiable controller needs to be optimized."
EVOLUTION STRATEGIES,0.06691449814126393,"2.2
EVOLUTION STRATEGIES"
EVOLUTION STRATEGIES,0.07063197026022305,"Existing ZOO methods focus on episodic RL problems with ﬁnite horizon and treat them as black-
box optimization. In these cases, the length of trajectories is limited and the discounting factor γ
is usually set as 1. Evolution strategies (ES) is one of the most popular algorithms of ZOO, which
optimizes a Gaussian smoothed objective function:"
EVOLUTION STRATEGIES,0.07434944237918216,"JES(θ) = Eϵ∼N(0,I)Es∼d0[V πθ+σϵ(s)]
(3)"
EVOLUTION STRATEGIES,0.07806691449814127,"where d0 is the initial state distribution and σ is the standard deviation of the Gaussian noise added
to the policy. The zeroth-order gradient can be derived using the log-likelihood ratio trick and the
probability density function of Gaussian distribution (Nesterov & Spokoiny, 2017):"
EVOLUTION STRATEGIES,0.08178438661710037,∇θJES(θ) = 1
EVOLUTION STRATEGIES,0.08550185873605948,"σ Eϵ∼N(0,I)Es∼d0[V πθ+σϵ(s)ϵ]
(4)"
EVOLUTION STRATEGIES,0.08921933085501858,"In practice, the expectation over Gaussian distribution can be approximated by sampling n noise
samples {ϵi}i=1,...,n, and the corresponding state value V πθ+σϵi can be approximated by the
episodic return Gi = PT
t=0 γtr(st, at) of the sample trajectory of length T collected with policy
πθ+σϵi:"
EVOLUTION STRATEGIES,0.09293680297397769,"∇θJES(θ) ≈1 nσ n
X"
EVOLUTION STRATEGIES,0.09665427509293681,"i=1
Giϵi
(5)"
EVOLUTION STRATEGIES,0.10037174721189591,"The zeroth-order gradient estimator in Equation (5) only relies on the episodic return of each evalu-
ated random directions, so it is applicable to non-differentiable policies. Besides, each perturbed pol-
icy remains deterministic in one trajectory, which leads to state-dependent and temporally-extended
exploration. Furthermore, the Gaussian smoothed objective also improves robustness of the learned
policies in parameter space."
ZEROTH-ORDER ACTOR-CRITIC,0.10408921933085502,"3
ZEROTH-ORDER ACTOR-CRITIC"
FROM ES TO ZOAC,0.10780669144981413,"3.1
FROM ES TO ZOAC"
FROM ES TO ZOAC,0.11152416356877323,"In this section, we will derive an improved zeroth-order gradient combining the actor-critic archi-
tecture for policy improvement. We start from improving the sample efﬁciency and stability of ES.
Most of the existing ES methods applied to RL optimize a deterministic policy (Salimans et al.,
2017; Mania et al., 2018), where the exploration is driven by noise in parameter space. Without loss
of generality, we follow them in the following derivations and algorithm design. A deterministic
policy parameterized with θ is denoted as πθ : S →A, which directly maps states to actions."
FROM ES TO ZOAC,0.11524163568773234,"In ES, the policy is perturbed in parameter space at the beginning of an episode and remains un-
changed throughout the trajectories. If a large number of random directions n is evaluated, the
sample complexity will increase signiﬁcantly. However, since the zeroth-order gradient is estimated
as the weighted sum of several random directions, it exhibit excessively high variance when n is
small (Berahas et al., 2021), which may greatly harm the performance. Therefore, it is essential to
trade-off this contradictory between sample efﬁciency and variance."
FROM ES TO ZOAC,0.11895910780669144,"To encourage sufﬁcient exploration and low variance while maintaining high sample efﬁciency, here
we consider perturbating the policy at every timestep, i.e., the Gaussian noise ϵ is sampled identically
and independently at every timesteps. We regard it as a stochastic exploration policy β = πθ+σϵ,
where ϵ ∼N(0, I) is Gaussian noise in parameter space and σ is the standard deviation. Our
objective is to maximize the expectation of the state-value under stationary distribution dβ of the ex-
ploration policy β. We can leverage Bellman equation to estimate the state-value via bootstrapping,
in which the one-step reward can be replaced with sampled experiences:"
FROM ES TO ZOAC,0.12267657992565056,"JZOAC(θ) = Est∼dβ[V β(st)] = Eϵ∼N(0,I)Est∼dβEst+1∼P[r(st, πθ+σϵ(st)) + γV β(st+1)]
(6)"
FROM ES TO ZOAC,0.12639405204460966,"Since all the contents in the outer expectation Eϵ∼N(0,I)[·] can be regarded as a function of ϵ, the
zeroth-order gradient of this objective function can be derived in exactly the same way as in ES."
FROM ES TO ZOAC,0.13011152416356878,Under review as a conference paper at ICLR 2022
FROM ES TO ZOAC,0.13382899628252787,"Moreover, V β(st) can be subtracted as a baseline for variance reduction because of its uncorrelation
to ϵ in the outer expectation and the zero mean property of the Gaussian noise ϵ:"
FROM ES TO ZOAC,0.137546468401487,∇θJZOAC(θ) = 1
FROM ES TO ZOAC,0.1412639405204461,"σ Eϵ∼N(0,I)Est∼dβEst+1∼P{[r(st, πθ+σϵ(st)) + γV β(st+1) −V β(st)]ϵ}
(7)"
FROM ES TO ZOAC,0.1449814126394052,"Compared to ES which uses unbiased but high variance Monte Carlo return to evaluate each per-
turbed policy, the performance of each random direction here is estimated by one-step TD residual
with low variance. In practice, a common approach is to introduce a critic network Vw(s) to estimate
the state-value function V β, which may lead to high bias in this form of advantage estimation."
FROM ES TO ZOAC,0.14869888475836432,"To trade-off between bias and variance, we consider extending our derivation further to the case
where each perturbed policies (i.e., each sampled random noise) run forward N timesteps instead of
one timestep only. Equation (7) can be extended to:
∇θJZOAC(θ) = 1"
FROM ES TO ZOAC,0.1524163568773234,"σ Eϵ∼N(0,I)Est∼dβEP"
FROM ES TO ZOAC,0.15613382899628253,"(""N−1
X"
FROM ES TO ZOAC,0.15985130111524162,"i=0
γir(st+i, πθ+σϵ(st+i)) + γNV β(st+N) −V β(st) # ϵ )
(8)"
FROM ES TO ZOAC,0.16356877323420074,"where EP refers to expectation over N-step transition dynamics. Similar to one-step case, the
cumulative reward within N step can be estimated from sampled experiences when st is the ﬁrst
state of the trajectory fragment collected with a certain perturbed policy πθ+σϵ. By introducing
a critic network and choosing an appropriate length N, this N-step residual advantage function
contributes to achieving a good trade-off between the bias and variance."
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.16728624535315986,"3.2
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.17100371747211895,"We analyze the variance of these two types of gradient estimators, ZOAC gradient and ES gradient.
The budget of timestep of one trajectory N × H is identical for both algorithms: in ZOAC, each
perturbed policy run forward N steps, and H is the number of sampled random directions in one
trajectory; in ES, only one perturbed policy is sampled and run forward N × H steps. If we denote
the accumulative reward obtained within N × H timesteps in ES as ˆV πθ+σϵ
NH
and the N-step TD
residual in ZOAC as ˆAπθ+σϵ
N
. We can then estimate the zeroth-order gradient according to Equation
(4) and (8) respectively:"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.17472118959107807,"∇θ ˆJES(θ) = 1 nσ n
X"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.17843866171003717,"i=1
ˆV
πθ+σϵi
NH
ϵi
(9)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.1821561338289963,"∇θ ˆJZOAC(θ) =
1
nHσ nH
X"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.18587360594795538,"i=1
ˆA
πθ+σϵi
N
ϵi
(10)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.1895910780669145,"We now give the upper bound of variance for these two gradient estimators. Variance is deﬁned
as the trace of the convariance matrix of gradient vectors Var(g) = Pd
l=1 E[g2
l ] −(Egl)2, where
g = (g1, g2, ..., gd)⊤(Zhao et al., 2011). We can derive the variance bound as follows if both the
reward and the critic network output is bounded (detailed derivation is provided in Appendix A.3,
vectors are bolded in the appendix for clarity but not in the main text)."
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.19330855018587362,"Theorem 1. If the reward |r(s, a)| < α, the critic network output |Vw(s)| < β, and n trajectories
with length of N × H timesteps are collected in one iteration, the upper bounds of the variance for
gradient estimators (Equation (9) and (10)) are:"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.1970260223048327,Var[∇θ ˆJES(θ)] ≤(1 −γNH)2α2d
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.20074349442379183,"nσ2(1 −γ)2
(11)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.20446096654275092,Var[∇θ ˆJZOAC(θ)] ≤((1 −γN)α + (1 −γ)(1 + γN)β)2d
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.20817843866171004,"nHσ2(1 −γ)2
(12)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.21189591078066913,"We can compare their variance in a more intuitive way: if N × H = 1000, γ = 0.99, and assume
that β ≈
α
1−γ , the difference of variance bounds becomes Var[∇θ ˆJZOAC(θ)] −Var(∇θ ˆJES(θ)) ≈ ( 4"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.21561338289962825,H −1) 10000α2d
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.21933085501858737,"nσ2
, which decreases with H and drops below zero when 4 < H ≤1000 (i.e., N
is smaller than 250). This suggests that although same amount of data is collected, an appropriate
rollout length N can indeed reduce variance of the gradient estimators. Besides, both variance bound
are inversely proportional to n, which urges us to collect more trajectories."
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.22304832713754646,Under review as a conference paper at ICLR 2022
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.22676579925650558,"Terminal 
Condition"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.23048327137546468,"Generated
Experiences
(𝑠𝑡, 𝑎𝑡, 𝑟𝑡, 𝑠𝑡+1)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2342007434944238,First-order PEV
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2379182156133829,"(𝑠, 𝐺)
𝐺𝑡= 𝑉𝑤𝑠𝑡+
(𝛾𝜆)𝑘𝛿𝑡+𝑘 𝑇−𝑡−1"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.241635687732342,"𝑘=0
∇𝑤𝐽critic 𝑤= 𝔼(𝑠,𝐺)
𝑉𝑤𝑠−𝐺∇𝑤𝑉𝑤𝑠"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.24535315985130113,Zeroth-order PIM
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.24907063197026022,∇𝜃𝐽actor 𝜃= 1
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2527881040892193,"𝜎𝔼(𝜖,𝐴) 𝐴𝜖"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.25650557620817843,"(𝜖, 𝐴)
𝐴𝑖,𝑗=
(𝛾𝜆)𝑘𝛿𝑖,𝑗 𝑁−1 𝑘=0"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.26022304832713755,"Rollouts Collection 𝑎 𝑠 𝑟, 𝑠′"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.26394052044609667,"Environment
Environment
Environment"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.26765799256505574,"Behavior Policy 𝛽= 𝜋𝜃+𝜎𝜖
Behavior Policy 𝛽= 𝜋𝜃+𝜎𝜖
Behavior Policy 𝛽= 𝜋𝜃+𝜎𝜖"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.27137546468401486,"𝜖~𝒩(0, 𝐼)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.275092936802974,Parallelized
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2788104089219331,workers
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2825278810408922,(a) Overall framework of ZOAC
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2862453531598513,"𝜖~𝒩(0, 𝐼)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2899628252788104,"𝑠1,0
𝑠1,𝑁
𝜋𝜃+𝜎𝜖1,0"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2936802973977695,"𝜋𝜃+𝜎𝜖1,1 …"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.29739776951672864,"𝑠1,(𝐻−1)𝑁
𝑠1,2𝑁 …"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.30111524163568776,"𝑠𝑛,0
𝑠𝑛,𝑁
𝜋𝜃+𝜎𝜖𝑛,0 …"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.3048327137546468,"𝜋𝜃+𝜎𝜖1,𝐻−1 𝑠1,𝐻𝑁 𝑠𝑛,𝐻𝑁"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.30855018587360594,"𝜋𝜃+𝜎𝜖𝑛,𝐻−1"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.31226765799256506,"𝑠𝑛,(𝐻−1)𝑁"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.3159851301115242,"𝜋𝜃+𝜎𝜖𝑛 … 𝑠𝑛,0 𝑠1,0 … … ES ZOAC"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.31970260223048325,𝜋𝜃+𝜎𝜖1
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.32342007434944237,(b) Rollouts collection strategies: ES vs. ZOAC
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.3271375464684015,"Figure 1: (a) The overall framework of our proposed algorithm ZOAC; (b) Comparison of rollouts
collection strategies (with n parallelized samplers): On the top is ES, which performs episode-wise
perturbation; on the bottom is ZOAC, which performs N timestep-wise perturbation."
PRACTICAL ALGORITHM,0.3308550185873606,"3.3
PRACTICAL ALGORITHM"
PRACTICAL ALGORITHM,0.3345724907063197,"We propose the Zeroth-Order Actor-Critic (ZOAC) algorithm, which uniﬁes ﬁrst-order and zeroth-
order methods into an on-policy actor-critic architecture by conducting rollouts collection with
timestep-wise perturbation in parameter space, ﬁrst-order policy evaluation (PEV) and zeroth-order
policy improvement (PIM) alternately in each iteration. The overall framework of ZOAC is shown
in Figure 1a and the pseudocode is summarized in Appendix A.1. In each iteration, parallelized
workers will collect rollouts in the environment with perturbed policies, then the agent train the
critic network to estimate state-value function under the exploration policy, and ﬁnally improve the
policy along the zeroth-order gradient direction."
PRACTICAL ALGORITHM,0.3382899628252788,"Rollouts collection.
The rollouts collection strategy is illustrated brieﬂy in Figure 1b, which is
a parallelized version with n workers. If we denote the t-th state sampled by the i-th worker as
si,t, the rollout strategy can be described as: when reaching states in {si,jN}, where j ∈N, a new
random direction ϵi,j is sampled and the behavior policy is perturbed; when reaching other states,
the deterministic behavior policy remains unchanged. It’s worth noting that the notation is only for
continuing case where an episode is never done. In episodic tasks, the rollout length 1 ≤Ni,j ≤N
actually varies between different perturbed policies πθ+σϵi,j since an episode may terminate at any
time. However, we still use N to denote the rollout length of each perturbed policy for brevity."
PRACTICAL ALGORITHM,0.3420074349442379,"A limit case of our proposed strategy is that when N is chosen as the episode length and the critic
network is turned off (i.e., Vw(s) ≡0), the algorithm actually degenerate into ES, since all perturbed
policies are evaluated by running a whole episode, and the episodic return is used as the ﬁtness score."
PRACTICAL ALGORITHM,0.34572490706319703,"First-order PEV.
The state-value function ˆV (s) can be estimated by a jointly optimized critic
network Vw(s), which aims to minimize the MSE loss between the network output and state-value
target. In each iteration, in total n × N × H states and the corresponding target values (s, ˆG) are
calculated and used for critic training. In a trajectory with length T, the target value ˆGt for each
state st is calculated as (Schulman et al., 2015; Andrychowicz et al., 2021):"
PRACTICAL ALGORITHM,0.34944237918215615,ˆGt = Vw(st) +
PRACTICAL ALGORITHM,0.35315985130111527,"T −t−1
X"
PRACTICAL ALGORITHM,0.35687732342007433,"k=0
(γλ)k[rt+k + γVw(st+k+1) −Vw(st+k)]
(13)"
PRACTICAL ALGORITHM,0.36059479553903345,"where 0 < λ < 1 is a hyperparameter to control the trade-off between bias and variance of the value
target. In Figure 1a, the one-step TD residual of each state s is denoted as δ for simplicity. The
objective function of PEV can be written as:"
PRACTICAL ALGORITHM,0.3643122676579926,"Jcritic(w) = E(s, ˆ
G) 1"
PRACTICAL ALGORITHM,0.3680297397769517,"2[Vw(s) −ˆG]2

(14)"
PRACTICAL ALGORITHM,0.37174721189591076,"In practice, the critic network is constructed as a neural network and updated through several epoches
of stochastic gradient descent in each iteration."
PRACTICAL ALGORITHM,0.3754646840148699,Under review as a conference paper at ICLR 2022
PRACTICAL ALGORITHM,0.379182156133829,"Figure 2: Learning curves on MuJoCo benchmarks. Exploration noise are turned off for evaluation.
The solid lines correspond to the mean and the shaded regions to the 95% conﬁdence interval over
5 trials using a ﬁxed set of random seeds. All curves are smoothed uniformly for visual clarity."
PRACTICAL ALGORITHM,0.3828996282527881,Table 1: Max total average return within certain environmental steps (mean±std over 5 trials).
PRACTICAL ALGORITHM,0.38661710037174724,"Environment
Inv.D.P.-v2
Hopper-v2
HalfCheetah-v2
Ant-v2
Timesteps
1e6
2e6
2e6
2e7
ZOAC(Linear)
9359.93±0.01
3333.60±97.85
5190.82±196.99
4495.33±100.37
ZOAC(Neural)
9339.66±5.29
3195.15±91.76
5339.95±140.46
4292.09±108.69
ARS(Linear)
9359.86±0.06
2891.28±305.61
2967.98±889.42
3427.85±765.62
ES(Neural)
9155.73±404.25
1065.34±49.14
2349.11±444.77
3274.89±519.66
PPO(Neural)
9350.89±0.04
3178.60±270.09
5219.61±677.90
3796.13±754.78"
PRACTICAL ALGORITHM,0.3903345724907063,"Zeroth-order PIM.
We calculate the zeroth-order gradient with n × H random directions and
the corresponding advantage function as (ϵ, ˆA). Similar to state value estimation, we leverage the
generalized advantage estimation (GAE) trick (Schulman et al., 2015) to further control the bias-
variance trade-off. We also perform advatage normalization to ensure consistent gradient length
during training. Following the notations in Figure 1b, the advantage function can be written as:"
PRACTICAL ALGORITHM,0.3940520446096654,"ˆA
πθ+σϵi,j
N
= N−1
X"
PRACTICAL ALGORITHM,0.39776951672862454,"k=0
(γλ)k[ri,jN+k + γVw(si,jN+k+1) −Vw(si,jN+k)]
(15)"
PRACTICAL ALGORITHM,0.40148698884758366,"where λ is the same as in Equation (13). The zeroth-order gradient can be then estimated as the
weighted sum of the sampled random directions:"
PRACTICAL ALGORITHM,0.4052044609665427,"∇θJactor(θ) ≈
1
nHσ n
X i=1 H−1
X"
PRACTICAL ALGORITHM,0.40892193308550184,"j=0
ˆA
πθ+σϵi,j
N
ϵi,j
(16)"
EXPERIMENTS,0.41263940520446096,"4
EXPERIMENTS"
PERFORMANCE EVALUATION,0.4163568773234201,"4.1
PERFORMANCE EVALUATION"
PERFORMANCE EVALUATION,0.4200743494423792,"We evaluate the performance of ZOAC on the MuJoCo continuous control benchmarks (Todorov
et al., 2012) in OpenAI Gym (Brockman et al., 2016). We choose Evolution Strategies (ES) (Sali-
mans et al., 2017; Liang et al., 2018) and Augmented Random Search (ARS) (Mania et al., 2018) as
zeroth-order baselines and proximal policy optimization (PPO) (Schulman et al., 2017; Rafﬁn et al.,
2019) as a ﬁrst-order actor-critic baseline."
PERFORMANCE EVALUATION,0.42379182156133827,"We use two different types of policies: linear policies for ARS and ZOAC (linear), neural networks
with (64, 64) hidden nodes and tanh nonlinearities for ES, PPO and ZOAC (neural). For a fair
comparison, we enable observation normalization for all methods, which has been proved effective
no matter in ﬁrst-order methods or zeroth-order methods (Mania et al., 2018; Andrychowicz et al.,
2021). When using neural networks as actors, we also use layer normalization (Ba et al., 2016)
in ZOAC and virtual batch normalization in ES (Salimans et al., 2017). Both of them ensure the
diversity of behaviors among the population, while the former is less computationally expensive.
We summarize the implementation details of ZOAC in Appendix A.2 and follow the recommended
hyperparameter settings listed in the related papers or code repositories."
PERFORMANCE EVALUATION,0.4275092936802974,Under review as a conference paper at ICLR 2022
PERFORMANCE EVALUATION,0.4312267657992565,"Figure 3: Visualization of the policies learned by different methods on MountainCarContinuous-v0.
Color represents the action output, from −1.0 (red) to 1.0 (blue). The horizontal lines in solid indi-
cate the initial car position distribution x ∼U(−0.6, −0.4), and the vertical lines in solid indicate
the goal x > 0.45. The dashed curves are trajectories starting from the same initial state."
PERFORMANCE EVALUATION,0.4349442379182156,"Table 2: Robustness comparison of the learned policies on HalfCheetah-v2. Best performing poli-
cies from Figure 2 are tested (4e7 timestep limitation for ARS and ES). Each policy is evaluated on
400 trajectories in the same environment and the average return over 5 policies are listed."
PERFORMANCE EVALUATION,0.43866171003717475,"Noise type
No extra noise
Obs. noise (σ = 0.1)
Para. noise (σ = 0.05)
ZOAC(Linear)
4909.53
4402.54 (-10.3%)
3289.15 (-33.0%)
ZOAC(Neural)
5179.39
5057.07 (-2.4%)
4247.08 (-18.0%)
ARS(Linear)
4529.30
1771.23 (-60.9%)
1724.40 (-61.9%)
ES(Neural)
5478.01
4550.22 (-16.9%)
1527.93 (-72.1%)
PPO(Neural)
4668.55
3342.23 (-28.4%)
972.08 (-79.2%)"
PERFORMANCE EVALUATION,0.4423791821561338,"Figure 2 presents the learning curves on four continuous control tasks. Table 1 summarizes max total
average return within the timestep threshold over 5 trials. Additional results including state-value
estimation and performance comparison of different policies are attached in Appendix A.4 and A.6."
PERFORMANCE EVALUATION,0.44609665427509293,"ZOAC matches or outperforms baseline algorithms across tasks in learning speed, ﬁnal performance,
and variance over trials. One thing worth mentioning is that both the zeroth-order baseline methods
perform reward shaping to resolve the local optima problem: ARS subtracts the survival bonus from
rewards (1 in Hopper and Ant), while ES transforms the episodic returns into rankings. Although
these tricks improve the performance, they also alter the update directions of the policies and make
it difﬁcult to determine what is the real objective function being optimized. ZOAC, however, sur-
passes ES and ARS without relying on speciﬁc exploration tricks, which can be attributed to the
introduction of critic network and the construction of advantage estimations in policy improvement."
PERFORMANCE EVALUATION,0.44981412639405205,"Robustness Comparison.
The objective function of ZOAC aims to maximize the expected state-
value of the stochastic behavior policy that contains parameter noise all the time, which intuitively
encourages the agent to ﬁnd a wider optima and leads to better generalization and robustness. Hence,
we evaluate the learned policies under two types of noise, observation noise and parameter noise.
Extra observation noise is added to the normalized observation at each timestep, which leads to
a slightly different observation distribution. Extra parameter noise is added at the beginning of
each trajectories, which pushes the learned policy to its neighborhood. The result in HalfCheetah-
v2 is presented in Table 2. Results show that in general the policies learned by ZOAC possess
higher robustness against both observation noise and parameter noise, which can be ascribed to
the robustness-seeking property of our method. The linear policies learned by ARS seem very
fragile and suffer signiﬁcant performance degradation under extra noise, far inferior to the linear
ones learned by ZOAC. Additional results and discussions are attached in Appendix A.5."
PERFORMANCE EVALUATION,0.45353159851301117,"Visualization of the learned policies.
In order to intuitively observe the behaviors of the learned
policies, we apply all methods on MountainCarContinuous-v0, in which the car is rewarded +100
only when it achieves the goal and penalized by the action output at every timestep. Policy gradient
methods usually struggle on this problem because the reward is sparse and delayed, while zeroth-
order methods can better handle reward sparsity by nature. We visualize the policies learned by each
algorithm in Figure 3. Among these learned policies, the neural policy learned by ES obtains the
highest average return, while the neural policy learned by ZOAC obtains the second highest average"
PERFORMANCE EVALUATION,0.45724907063197023,Under review as a conference paper at ICLR 2022
PERFORMANCE EVALUATION,0.46096654275092935,"Figure 4: Inﬂuence of the rollout length
N with each perturbed policies."
PERFORMANCE EVALUATION,0.4646840148698885,"Figure 5: Ablation studies on ZOAC with neural network
(left) and linear poicy (right)."
PERFORMANCE EVALUATION,0.4684014869888476,"return within the shortest episode length. The latter has a similar but much steeper terrain compared
to the former one and implies a larger control action in most areas. As for linear policies, the one
learned by ZOAC also tends to achieve the goal in a shorter episode length than the one learned by
ARS. We attribute this to the usage of discounting factor, which pushes the agent to perform higher
actions and achieve the goal as early as possible. ZOAC outperforms PPO both in ﬁnal performance
and training stability over different random seeds, due to its state-dependent exploration in parameter
space, which is more efﬁcient than action noise and is essential to solve this task."
ABLATION STUDIES,0.4721189591078067,"4.2
ABLATION STUDIES"
ABLATION STUDIES,0.4758364312267658,"Appropriate rollout length.
As illustrated in Section 3.2, choosing appropriate rollout length N
of each perturbed policies may achieve a good trade-off between bias and variance. We perform
an ablation study to understand the effect of timestep-wise perturbation strategy and choice of N.
We compare ZOAC and ARS on HalfCheetah-v2, the former conducting timstep-wise perturbation
and the latter conducting episode-wise perturbation. We use linear policies in both methods and set
hyperparameters to the same, including the standard deviation of parameter noise σ, the learning rate
of policy αactor, and also the budget of timesteps within one iteration. Figure 4 shows the inﬂuence
of N on performance. Results show that under this setting, both 10 and 20 are good choices for N
which lead to better performance, while other two choices, 5 and 50, perform similarly to ARS. Note
that here we sample as many experiences as ARS per iteration for comparison, while in practice, the
number of timesteps collected in each iteration is highly tunable in ZOAC. In fact, we found that
rollout length N in a large range, approximately from 5 to 50, perform quite well across tasks."
ABLATION STUDIES,0.4795539033457249,"Analysis on different components.
To evaluate the contribution of each individual component
and also the potential of additional techniques, we perform ablation studies and present the results
in Figure 5. Results demonstrate that critic network is a crucial part of ZOAC, i.e., N-step accu-
mulative reward without bootstrapping is not sufﬁcient to guide policy improvement. Observation
normalization technique is also essential to zeroth-order methods, which helps to generate diverse
policies via isotropic Gaussian noise. Besides, GAE trick and layer normalization trick slightly im-
prove the performance. Mania et al. (2018) propose to use only the top performing directions in
policy update to relieve the bad inﬂuence of noisy evaluation results and validate its effectiveness
on ARS. Here we perform a similar direction sifting technique, using only the directions that have
the highest advantage in policy improvement, but it seems to pull down the learning performance
of ZOAC. Moreover, results show that additional action noise is not helpful to the performance,
indicating that the exploration driven by parameter noise is sufﬁcient."
RELATED WORK,0.483271375464684,"5
RELATED WORK"
RELATED WORK,0.48698884758364314,"ZOO and its applications in RL.
At each iteration, ZOO samples several random directions
from a certain distribution, and then the distribution is updated according to the evaluation results
over these directions. Sehnke et al. (2010) derive parameter-exploring policy gradients (PGPE)
for episodic RL problems, which has reduced variance and higher performance than vanilla policy
gradient. Salimans et al. (2017) and Such et al. (2017) propose highly scalable evolution strategies
(ES) and genetic algorithms (GA) respectively, both of which can be applied to deep neural networks
and achieve competitive performance with MDP-based RL algorithms. Mania et al. (2018) propose"
RELATED WORK,0.49070631970260226,Under review as a conference paper at ICLR 2022
RELATED WORK,0.4944237918215613,"augmented random search (ARS), which applied ZOO to linear policies with techniques including
observation normalization, reward scaling, top performing directions sifting, and achieve astonishing
performance on RL benchmarks considering its simplicity. ZOO has regained popularity in recent
years because of its special advantages when applied in RL, including wide adaptability to policy
parameterization (e.g., deterministic or stochastic, differentiable or non-differentiable), robustness
seeking property, state-dependent and temporally-extended exploration."
RELATED WORK,0.49814126394052044,"Improved techniques for ZOO.
The main limitation of ZOO is its high sample complexity. Re-
searchers have proposed various improved techniques for ZOO from different perspectives. One way
is to adopt advanced Monte Carlo sampling methods to reduce variance of the zeroth-order gradient
estimation, e.g., antithetic sampling (Sehnke et al., 2010; Salimans et al., 2017; Mania et al., 2018),
orthogonal and Quasi Monte Carlo exploration (Choromanski et al., 2018). Constructing control
variates (i.e., subtracting a baseline) is another popular variance reduction technique. Sehnke et al.
(2010) adopt a moving-average baseline in PGPE heuristically, while Zhao et al. (2011) derive the
optimal baseline for PGPE in an analytical form that minimizes the variance. Moreover, the sample
complexity of zeroth-order methods will further increase with the dimension of the optimization
problem (Nesterov & Spokoiny, 2017), therefore some researches aim to identify a low-dimensional
search space and guide the search towards faster convergence. Guided ES (Maheswaranathan et al.,
2019) and ASEBO (Choromanski et al., 2019) are proposed based on a similar idea: to identify
linear subspaces and adapt the search distribution from recent history of descent directions. Sener &
Koltun (2020) propose LMRS, which uses more expressive neural networks to represent subspaces
and jointly learns the underlying subspace and optimizes the objective function."
RELATED WORK,0.5018587360594795,"Hybridization of ZOO and ﬁrst-order MDP-based RL.
These two methods have complemen-
tary advantages when applied to RL problems, and recent researches have tried to combine them for
better performance. Khadka & Tumer (2018) propose the ERL framework that runs evolutionary
algorithms (EA) and DDPG (Lillicrap et al., 2015) concurrently with bidirectional information ﬂow,
i.e., the DDPG agent is trained with experiences generated by the EA population and reinserted into
the population periodically to guide the evolution process. CEM-RL (Pourchot & Sigaud, 2018) and
Proximal Distilled ERL (Bodnar et al., 2020) adopt similar hybridization framework, but use differ-
ent algorithms as components and improve training techniques. Fortunato et al. (2018) and Plappert
et al. (2018) inject parameter noises into existing ﬁrst-order MDP-based RL algorithms to drive more
efﬁcient exploration, and demonstrate that existing RL algorithms can indeed beneﬁt from parameter
space exploration through comparative experiments. Some other hybrid methods (Grathwohl et al.,
2018; Tang et al., 2020) leverage policy gradient and reparameterization trick to construct control
variates, which leads to unbiased, low variance gradient estimators. Our proposed method, however,
uniﬁes ﬁrst-order and zeroth-order methods into an on-policy actor-critic architecture by conducting
ﬁrst-order PEV and zeroth-order PIM alternately in each iteration. The state-value function network
does not only serve as a baseline to reduce variance, but also as a critic used for bootstrapping, which
leads to reduced variance and accelerated learning (Sutton & Barto, 2018). The policy is updated in
a zeroth-order way, which implies wide adaptability to different forms of policies."
CONCLUSION,0.5055762081784386,"6
CONCLUSION"
CONCLUSION,0.5092936802973977,"In this paper, we propose Zeroth-Order Actor-Critic algorithm (ZOAC) that uniﬁes evolution based
zeroth-order and policy gradient based ﬁrst-order methods into an on-policy actor-critic architecture
to preserve the advantages from both, including the ability to handle different forms of policies, state-
dependent exploration, robustness-seeking property from the former and high sample efﬁciency
from the latter. ZOAC conducts rollouts collection with timestep-wise perturbation in parameter
space, ﬁrst-order policy evaluation (PEV) and zeroth-order policy improvement (PIM) alternately
in each iteration. Experimental results in a range of challenging continuous control tasks show that
ZOAC outperforms zeroth-order and ﬁrst-order baselines. Robustness analysis and ablation studies
on hyperparameters and components are also performed to show the properties of ZOAC."
CONCLUSION,0.5130111524163569,"Moreover, our methods achieve such improvement while still using traditional isotropic Gaussian
noise for perturbation, so in principle those improved techniques for ZOO from sampling perspec-
tives can be further integrated, e.g., Monte Carlo sampling techniques, low-dimensional subspace
identiﬁcation, adaptive perturbation scale, which may lead to even higher performance."
CONCLUSION,0.516728624535316,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY,0.5204460966542751,"7
REPRODUCIBILITY"
REPRODUCIBILITY,0.5241635687732342,"The code of ZOAC will be released after the author notiﬁcation in https://anonymous.
4open.science/r/Zeroth-Order-Actor-Critic-1A71. We summarize the algorithm
in Appendix A.1 and describe the implementation details in Appendix A.2."
REFERENCES,0.5278810408921933,REFERENCES
REFERENCES,0.5315985130111525,"Marcin Andrychowicz, Anton Raichuk, Piotr Sta´nczyk, Manu Orsini, Sertan Girgin, Rapha¨el
Marinier, Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly,
and Olivier Bachem. What matters for on-policy deep actor-critic methods? a large-scale study.
In International Conference on Learning Representations, 2021."
REFERENCES,0.5353159851301115,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016."
REFERENCES,0.5390334572490706,"Albert S Berahas, Liyuan Cao, Krzysztof Choromanski, and Katya Scheinberg. A theoretical and
empirical comparison of gradient approximations in derivative-free optimization. Foundations of
Computational Mathematics, pp. 1–54, 2021."
REFERENCES,0.5427509293680297,"Cristian Bodnar, Ben Day, and Pietro Li´o. Proximal distilled evolutionary reinforcement learning. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 3283–3290, 2020."
REFERENCES,0.5464684014869888,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.550185873605948,"Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard Turner, and Adrian Weller.
Structured evolution with compact architectures for scalable policy optimization. In International
Conference on Machine Learning, pp. 970–978. PMLR, 2018."
REFERENCES,0.5539033457249071,"Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, and Vikas Sindhwani.
From complexity to simplicity: Adaptive es-active subspaces for blackbox optimization. Ad-
vances in Neural Information Processing Systems, 32:10299–10309, 2019."
REFERENCES,0.5576208178438662,"Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Os-
band, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles
Blundell, and Shane Legg.
Noisy networks for exploration.
In International Conference on
Learning Representations, 2018."
REFERENCES,0.5613382899628253,"Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. In International
Conference on Learning Representations, 2018."
REFERENCES,0.5650557620817844,"Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen,
Vinh-Dieu Lam, Alex Bewley, and Amar Shah. Learning to drive in a day. In 2019 International
Conference on Robotics and Automation (ICRA), pp. 8248–8254. IEEE, 2019."
REFERENCES,0.5687732342007435,"Shauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learning.
In Proceedings of the 32nd International Conference on Neural Information Processing Systems,
pp. 1196–1208, 2018."
REFERENCES,0.5724907063197026,"Joel Lehman, Jay Chen, Jeff Clune, and Kenneth O Stanley. Es is more than just a traditional
ﬁnite-difference approximator.
In Proceedings of the Genetic and Evolutionary Computation
Conference, pp. 450–457, 2018."
REFERENCES,0.5762081784386617,"Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gon-
zalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement learning.
In International Conference on Machine Learning, pp. 3053–3062. PMLR, 2018."
REFERENCES,0.5799256505576208,"Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015."
REFERENCES,0.5836431226765799,Under review as a conference paper at ICLR 2022
REFERENCES,0.587360594795539,"Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, and Jascha Sohl-Dickstein. Guided
evolutionary strategies: Augmenting random search with surrogate gradients. In International
Conference on Machine Learning, pp. 4264–4273. PMLR, 2019."
REFERENCES,0.5910780669144982,"Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is
competitive for reinforcement learning. In Proceedings of the 32nd International Conference on
Neural Information Processing Systems, pp. 1805–1814, 2018."
REFERENCES,0.5947955390334573,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.5985130111524164,"Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928–1937. PMLR, 2016."
REFERENCES,0.6022304832713755,"Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang,
Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. Ray: A distributed frame-
work for emerging {AI} applications.
In 13th {USENIX} Symposium on Operating Systems
Design and Implementation ({OSDI} 18), pp. 561–577, 2018."
REFERENCES,0.6059479553903345,"Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics, 17(2):527–566, 2017."
REFERENCES,0.6096654275092936,"Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.
In International Conference on Learning Representations, 2018."
REFERENCES,0.6133828996282528,"Alo¨ıs Pourchot and Olivier Sigaud. Cem-rl: Combining evolutionary and gradient-based methods
for policy search. arXiv preprint arXiv:1810.01222, 2018."
REFERENCES,0.6171003717472119,"Hong Qian and Yang Yu.
Derivative-free reinforcement learning: A review.
arXiv preprint
arXiv:2102.05710, 2021."
REFERENCES,0.620817843866171,"Antonin Rafﬁn, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dor-
mann. Stable baselines3. GitHub repository, 2019."
REFERENCES,0.6245353159851301,"Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017."
REFERENCES,0.6282527881040892,"John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.
High-
dimensional continuous control using generalized advantage estimation.
arXiv preprint
arXiv:1506.02438, 2015."
REFERENCES,0.6319702602230484,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.6356877323420075,"Frank Sehnke, Christian Osendorfer, Thomas R¨uckstieß, Alex Graves, Jan Peters, and J¨urgen
Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551–559, 2010."
REFERENCES,0.6394052044609665,"Ozan Sener and Vladlen Koltun. Learning to guide random search. In International Conference on
Learning Representations, 2020."
REFERENCES,0.6431226765799256,"Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and
Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training
deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017."
REFERENCES,0.6468401486988847,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.6505576208178439,"Yunhao Tang, Krzysztof Choromanski, and Alp Kucukelbir. Variance reduction for evolution strate-
gies via structured control variates. In International Conference on Artiﬁcial Intelligence and
Statistics, pp. 646–656. PMLR, 2020."
REFERENCES,0.654275092936803,Under review as a conference paper at ICLR 2022
REFERENCES,0.6579925650557621,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.
IEEE, 2012."
REFERENCES,0.6617100371747212,"Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3):229–256, 1992."
REFERENCES,0.6654275092936803,"Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama. Analysis and improvement
of policy gradient estimation. Advances in Neural Information Processing Systems, 24:262–270,
2011."
REFERENCES,0.6691449814126395,Under review as a conference paper at ICLR 2022
REFERENCES,0.6728624535315985,"A
APPENDIX"
REFERENCES,0.6765799256505576,"A.1
PSEUDOCODE OF ZOAC"
REFERENCES,0.6802973977695167,Algorithm 1 Zeroth-Order Actor-Critic (ZOAC)
REFERENCES,0.6840148698884758,"1: Initialize: policy parameters θ, critic network parameters w
2: for each iteration do
3:
for each worker i = 1, 2, ..., n do
4:
for j = 0, 1, ..., H −1 do
5:
Sample ϵi,j ∼N(0, I)
6:
Run perturbed policy πθ+σϵi,j in environment for N timesteps"
REFERENCES,0.6877323420074349,"7:
Compute advantage function ˆAπθ+σϵi,j according to Equation (15)
8:
end for
9:
Compute the state-value target ˆGt for each state st according to Equation (13)
10:
end for
11:
Collect (s, ˆG) for critic update and (ϵ, ˆA) for actor update
12:
Update w with batch size L through SGD by minimizing Equation (14) for M epoches
13:
Update θ along the zeroth-order gradient direction estimated in Equation (16)
14: end for"
REFERENCES,0.6914498141263941,"A.2
IMPLEMENTATION DETAILS"
REFERENCES,0.6951672862453532,"We implemented ZOAC with parallelized workers (Algorithm 1) using the distributed framework
Ray (Moritz et al., 2018). We follow the parallelization techniques used in ES (Salimans et al., 2017)
and ARS (Mania et al., 2018). Firstly, we created a shared noise table before training starts, then the
workers communicate indices in the shared table but not the perturbation vectors, so as to avoid high
communication cost. Besides, random seeds for constructing parallelized training environments and
the evaluation environment are different and generated from a single seed designated before hand."
REFERENCES,0.6988847583643123,"We use two different types of policies: linear policies for ARS and ZOAC (linear), neural networks
with (64, 64) hidden nodes and tanh nonlinearities for ES, PPO and ZOAC (neural). For actor-critic
algorithms, we use neural networks with (256, 256) hidden nodes and tanh nonlinearities as critics
to estimate state-value function."
REFERENCES,0.7026022304832714,"Both the zeroth-order baseline methods perform reward shaping to resolve the local optima problem
as described in the original paper: ARS subtracts the survival bonus from rewards (1 in Hopper and
Ant), while ES transforms the episodic returns into rankings. ES further discretize the actions to
encourage exploration in Hopper but we do not reserve this trick for comparison since discretization
will lead to a different policy architecture."
REFERENCES,0.7063197026022305,"We summarize the hyperparameters used in ZOAC in Table 3 and list their values that are used to
produce the results in Figure 2. We tune several important hypermarameters (n, N, H, σ) via coarse
grid search and select the best performing setting to produce the ﬁnal results. During evaluation,
exploration noise are turned off and the reported total average return is averaged over 10 episodes.
Table 1 summarizes the maximum value of the total average return within the timestep threshold,
averaged over 5 trials."
REFERENCES,0.7100371747211895,"A.3
DERIVATION OF THE VARIANCE BOUND"
REFERENCES,0.7137546468401487,"Theorem 1. If the reward |r(s, a)| < α, the critic network output |Vw(s)| < β, and n trajectories
with length of N × H timesteps are collected in one iteration, the upper bounds of the variance of
gradient estimators (Equation (9) and (10)) are:"
REFERENCES,0.7174721189591078,Var(∇θ ˆJES(θ)) ≤(1 −γNH)2α2d
REFERENCES,0.7211895910780669,nσ2(1 −γ)2
REFERENCES,0.724907063197026,Var[∇θ ˆJZOAC(θ)] ≤((1 −γN)α + (1 −γ)(1 + γN)β)2d
REFERENCES,0.7286245353159851,nHσ2(1 −γ)2
REFERENCES,0.7323420074349443,Under review as a conference paper at ICLR 2022
REFERENCES,0.7360594795539034,Table 3: Hyperparameters of ZOAC for the learning curves shown in Figure 2
REFERENCES,0.7397769516728625,"Environment
Inv.D.P.-v2
Hopper-v2
HalfCheetah-v2
Ant-v2
Policy type
Linear
Neural
Linear
Neural
Linear
Neural
Linear
Neural
Num. of workers n
4
8
4
8
4
8
8
Rollout length N
10
20
10
20
20
Train frequency H
16
16
32
16
256
Para. noise std. σ
0.02
0.04
0.04
0.06
0.02
Batch size L
64
128
Num. of epoches M
8
4
Actor optimizer
Adam(αactor = 0.005, β1 = 0.9, β2 = 0.999)
Critic optimizer
Adam(αcritic = 0.0003, β1 = 0.9, β2 = 0.999)
Discount factor γ
0.99
GAE coeff. λ
0.95"
REFERENCES,0.7434944237918215,Proof. (1) Variance bound for ES gradient estimators
REFERENCES,0.7472118959107806,"Under the setting described in Section 3.2, the state-value under policy πθ+σϵ is estimated by the
accumulative return over NH timesteps, which is denoted as ˆV πθ+σϵ
NH
. The isotropic Gaussian noise
added to the policy can be presented as ϵ = (ϵ1, ϵ2, ..., ϵd)⊤, where ϵl ∼N(0, 1), l ∈{1, 2, ..., d}."
REFERENCES,0.7509293680297398,"Var[ ˆV πθ+σϵ
NH
ϵ] ≤ d
X"
REFERENCES,0.7546468401486989,"l=1
E[( ˆV πθ+σϵ
NH
ϵl)2] = d
X l=1"
REFERENCES,0.758364312267658,"Z
p(ϵl) NH
X"
REFERENCES,0.7620817843866171,"t=1
γt−1r(st, at) !2"
REFERENCES,0.7657992565055762,"ϵ2
l dϵl ≤ d
X l=1"
REFERENCES,0.7695167286245354,"Z
p(ϵl) NH
X"
REFERENCES,0.7732342007434945,"t=1
γt−1α !2"
REFERENCES,0.7769516728624535,"ϵ2
l dϵl"
REFERENCES,0.7806691449814126,= (1 −γNH)2α2
REFERENCES,0.7843866171003717,"(1 −γ)2 d
X l=1"
REFERENCES,0.7881040892193308,"Z
p(ϵl)ϵ2
l dϵl"
REFERENCES,0.79182156133829,= (1 −γNH)2α2
REFERENCES,0.7955390334572491,"(1 −γ)2 d
X"
REFERENCES,0.7992565055762082,"l=1
Eϵl∼N(0,1)ϵ2
l"
REFERENCES,0.8029739776951673,= (1 −γNH)2α2d
REFERENCES,0.8066914498141264,(1 −γ)2
REFERENCES,0.8104089219330854,"The last equality holds because ϵ2
l ∼χ2(1) when ϵl ∼N(0, 1), and E[ϵ2
l ] = 1 for all l. Since n
random directions is sampled and evaluated, the ES gradient estimator is given according to Equation
(9):"
REFERENCES,0.8141263940520446,"∇θ ˆJES(θ) = 1 nσ n
X"
REFERENCES,0.8178438661710037,"i=1
ˆV
πθ+σϵi
NH
ϵi"
REFERENCES,0.8215613382899628,Therefore the variance bound for ES can be derived as in Theorem 1:
REFERENCES,0.8252788104089219,"Var[∇θ ˆJES(θ)] =
1
nσ2 Var[ ˆV πθ+σϵ
NH
ϵ]"
REFERENCES,0.828996282527881,≤(1 −γNH)2α2d
REFERENCES,0.8327137546468402,nσ2(1 −γ)2
REFERENCES,0.8364312267657993,(2) Variance bound for ZOAC gradient estimators
REFERENCES,0.8401486988847584,"Under the setting described in Section 3.2, the performance under policy πθ+σϵ is estimated by the
N-step TD residual, which is denoted as ˆAπθ+σϵ
N
. The isotropic Gaussian noise ϵ is added to the
policy as well."
REFERENCES,0.8438661710037175,Under review as a conference paper at ICLR 2022
REFERENCES,0.8475836431226765,"Var[ ˆAπθ+σϵ
N
ϵ] ≤ d
X"
REFERENCES,0.8513011152416357,"l=1
E[( ˆAπθ+σϵ
N
ϵl)2] = d
X l=1"
REFERENCES,0.8550185873605948,"Z
p(ϵl) N
X"
REFERENCES,0.8587360594795539,"t=1
γt−1r(st, at) + γNVw(st+N) −Vw(st) !2"
REFERENCES,0.862453531598513,"ϵ2
l dϵl ≤ d
X l=1"
REFERENCES,0.8661710037174721,"Z
p(ϵl) N
X"
REFERENCES,0.8698884758364313,"t=1
γt−1α + (1 + γN)β !2"
REFERENCES,0.8736059479553904,"ϵ2
l dϵl"
REFERENCES,0.8773234200743495,"=
(1 −γN)α + (1 −γ)(1 + γN)β"
REFERENCES,0.8810408921933085,(1 −γ)
REFERENCES,0.8847583643122676,"2
d
X l=1"
REFERENCES,0.8884758364312267,"Z
p(ϵl)ϵ2
l dϵl"
REFERENCES,0.8921933085501859,"=
(1 −γN)α + (1 −γ)(1 + γN)β"
REFERENCES,0.895910780669145,"(1 −γ) 2
d"
REFERENCES,0.8996282527881041,"Totally n × H random directions is sampled and evaluated, and the ZOAC gradient estimator is
given according to Equation (10):"
REFERENCES,0.9033457249070632,"∇θ ˆJZOAC(θ) ≈
1
nHσ nH
X"
REFERENCES,0.9070631970260223,"i=1
ˆA
πθ+σϵi
N
ϵi"
REFERENCES,0.9107806691449815,Therefore the variance bound for ZOAC can be derived:
REFERENCES,0.9144981412639405,"Var[∇θ ˆJZOAC(θ)] =
1
nHσ2 Var[ ˆAπθ+σϵ
N
ϵ]"
REFERENCES,0.9182156133828996,≤((1 −γN)α + (1 −γ)(1 + γN)β)2d
REFERENCES,0.9219330855018587,nHσ2(1 −γ)2
REFERENCES,0.9256505576208178,"A.4
STATE-VALUE FUNCTION ESTIMATION"
REFERENCES,0.929368029739777,"Figure 6: Average state-value estimation difference Vw(s) −V πθ(s) in evaluation during training
in Figure 2. The solid lines correspond to the mean and the shaded regions to the 95% conﬁdence
interval over 5 trials using a ﬁxed set of random seeds. All curves are smoothed uniformly for visual
clarity."
REFERENCES,0.9330855018587361,"We plot the average state-value estimation difference Vw(s) −V πθ(s) in evaluation during training
in Figure 6. Since we turn off the exploration noise for evaluation, which means that the trajectories
are collected under the deterministic policy πθ, the discounted sum of reward-to-go can be regarded
as an estimate of the true state-value."
REFERENCES,0.9368029739776952,"Results show that the critic networks converge, but in most cases to an underestimated value. This is
because the critic network is trained to ﬁt the state-value function V β(s) of the stochastic exploration
policy β rather than V πθ(s) of the deterministic policy πθ. The underestimate bias vary in different
tasks and when using different forms of policies, which is related to the local shape of the optima
found by the RL agent. However, due to the objective function used in ZOAC, intuitively, the agent
tend to ﬁnd wide optima during training, which ﬁnally result in more robust policies."
REFERENCES,0.9405204460966543,Under review as a conference paper at ICLR 2022
REFERENCES,0.9442379182156134,"A.5
ADDITIONAL RESULTS OF ROBUSTNESS COMPARISON"
REFERENCES,0.9479553903345725,"Table 4 shows the result of robustness comparison in all tested MuJoCo environments. Results show
that in general neural policies are more robust, which can be ascribed to their strong expressive
ability. InvertedDoublePendulum-v2 is the exception, in which the robustness of the neural network
policies is no where near the linear ones, no matter what RL algorithms is used. Since this environ-
ment is the simplest one among these environments, we guess that this is due to the overﬁtting of
neural networks, and that a matrix is enough to represent an optimal policy."
REFERENCES,0.9516728624535316,"When comparing between linear policies, the policies learned by ZOAC yield higher robustness to
both observation noise and parameter noise in all environments, compared to those learned by ARS.
One possible reason is that ARS subtracts the survival bonus from rewards, which actually alter the
objective function being optimized, as described in Section 4.1. As for neural policies, the policies
learned by ZOAC are also shown to be more robust to both observation noise and parameter noise.
One reason is that parameter noise used in zeroth-order methods encourages the agent to ﬁnd a
wide optima which is robust to parameter perturbations, while gradient-based methods focus on the
performance of a particular point. Besides, ZOAC perform timestep-wise perturbation rather than
episode-wise perturbation, which explores along more random directions and collects more diverse
trajectories (i.e., data) than ARS or ES. All these features ﬁnally lead to better generalization and
robustness of the policies learned by ZOAC."
REFERENCES,0.9553903345724907,"Table 4: Robustness comparison of the learned policies. Best performing policies from Figure 2
are tested (4e7 timestep limitation for ARS and ES in HalfCheetah-v2 and Ant-v2 till convergence).
Each policy is evaluated on 400 trajectories in the same environment (using the same random seed
that has never been used during training) and the average return over 5 policies are listed."
REFERENCES,0.9591078066914498,"Env.
Noise type
No extra noise
Obs. noise (σ = 0.1)
Para. noise (σ = 0.05)
ZOAC(Linear)
8744.11
8615.50 (-1.5%)
3643.49 (-58.3%)
ZOAC(Neural)
8419.16
908.95 (-89.2%)
1344.54 (-84.0%)
Inv.D.P.
ARS(Linear)
8883.13
8339.77 (-6.1%)
3396.74 (-61.8%)
ES(Neural)
7333.34
3593.76 (-51.0%)
927.02 (-87.4%)
PPO(Neural)
7477.48
3537.34 (-52.7%)
3115.82 (-58.3%)
ZOAC(Linear)
2885.10
2733.24 (-5.3%)
1872.07 (-35.1%)
ZOAC(Neural)
2417.33
2284.12 (-5.5%)
1644.34 (-32.0%)
Hopper
ARS(Linear)
2587.06
1194.43 (-53.8%)
781.99 (-69.8%)
ES(Neural)
1443.77
1330.98 (-7.8%)
915.38 (-36.6%)
PPO(Neural)
2596.02
2185.02 (-15.8%)
395.80 (-84.8%)
ZOAC(Linear)
4909.53
4402.54 (-10.3%)
3289.15 (-33.0%)
ZOAC(Neural)
5179.39
5057.07 (-2.4%)
4247.08 (-18.0%)
HalfC.
ARS(Linear)
4529.30
1771.23 (-60.9%)
1724.40 (-61.9%)
ES(Neural)
5478.01
4550.22 (-16.9%)
1527.93 (-72.1%)
PPO(Neural)
4668.55
3342.23 (-28.4%)
972.08 (-79.2%)
ZOAC(Linear)
4134.79
3460.67 (-16.3%)
-2901.51 (-170.2%)
ZOAC(Neural)
4013.57
3650.79 (-9.0%)
976.57 (-75.6%)
Ant
ARS(Linear)
3749.73
2868.18 (-23.5%)
-4269.38 (-213.9%)
ES(Neural)
4029.82
3902.70 (-3.2%)
970.30 (-75.9%)
PPO(Neural)
3103.79
2792.79 (-10.0%)
393.20 (-87.3%)"
REFERENCES,0.9628252788104089,"A.6
PERFORMANCE COMPARISON OF DIFFERENT POLICY PARAMETERIZATIONS"
REFERENCES,0.966542750929368,"The derivative-free nature of ZOAC allows us to estimate the zeroth-order policy gradient to im-
prove the policy without considering the speciﬁc policy architecture. Hence, ZOAC can be applied
seamlessly to arbitrary parameterized policies in theory."
REFERENCES,0.9702602230483272,"We further apply ZOAC on two more different policies and conduct additional experiments on the
same environments with four different policies, from fewer parameters to more parameters: Toeplitz
matrix, matrix, network with (64, 64) units, network with (128, 128) units, and to see the perfor-
mance of ZOAC when optimizing these policies. We listed the dimension of the parameter space of
different policies in Table 5."
REFERENCES,0.9739776951672863,Under review as a conference paper at ICLR 2022
REFERENCES,0.9776951672862454,"Toeplitz matrix is a kind of compact policies with parameter sharing schemes, each element depends
only on the difference between the row index and the column index. A general dense matrix M ∈
Rm×n has m × n parameters, while a Toeplitz matrix T ∈Rm×n has only m + n −1 parameters.
Since a general dense matrix performs quite well in all tested environments, we wonder whether a
more compact policy still work. Besides, as derived in Theorem 1, the variance bound of zeroth-
order gradient increases proportional to the dimension of parameter space, and it is a common view
that zeroth-order methods are more suitable for low-dimensional problems. We wonder whether a
larger network will improve or harm the performance."
REFERENCES,0.9814126394052045,"We use the same set of hyperparameters for linear policies, and another set for networks, as summa-
rized in Table 3. Results in Figure 7 show that a Toeplitz matrix can obtain average return around
2000 in challenging environments like HalfCheetah-v2 and Ant-v2 with only very few parameters.
However, it is far inferior to a general dense matrix, indicating that this type of policy is not sufﬁ-
cient to represent an optimal policy. Network with (64, 64) and (128, 128) hidden nodes performs
quite similarly to each other."
REFERENCES,0.9851301115241635,"The additional results demonstrate the wide adaptability of ZOAC to different forms of policies.
Zeroth-order policy update makes it very useful when gradient information is hard to obtain or even
unavailable, like low precision neural networks, hierarchical policies, or even rule-based controllers.
In the future, we may apply ZOAC to speciﬁc problems where ﬁrst-order methods can not handle
and to improve existing results where the policies are trained with traditional zeroth-order methods
like ES and ARS."
REFERENCES,0.9888475836431226,"Table 5: Dimension of the state space, action space and parameter space of different policies."
REFERENCES,0.9925650557620818,"Environment
Inv.Dou.Pen.-v2
Hopper-v2
HalfCheetah-v2
Ant-v2
State space
11
11
17
111
Action space
1
3
6
8
Toeplitz matrix
11
13
22
118
Matrix
11
33
102
888
Network (64, 64)
4993
5123
5702
11848
Network (128, 128)
18177
18435
19590
31880"
REFERENCES,0.9962825278810409,"Figure 7: Learning curves on MuJoCo benchmarks with different policies. The solid lines corre-
spond to the mean and the shaded regions to the 95% conﬁdence interval over 5 trials using a ﬁxed
set of random seeds. All curves are smoothed uniformly for visual clarity."
