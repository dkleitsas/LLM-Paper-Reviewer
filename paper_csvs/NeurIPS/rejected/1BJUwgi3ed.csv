Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001567398119122257,"We establish new generalisation bounds for multiclass classification by abstracting
1"
ABSTRACT,0.003134796238244514,"to a more general setting of discretised error types. Extending the PAC-Bayes
2"
ABSTRACT,0.004702194357366771,"theory, we are hence able to provide fine-grained bounds on performance for multi-
3"
ABSTRACT,0.006269592476489028,"class classification, as well as applications to other learning problems including
4"
ABSTRACT,0.007836990595611285,"discretisation of regression losses. Tractable training objectives are derived from
5"
ABSTRACT,0.009404388714733543,"the bounds. The bounds are uniform over all weightings of the discretised error
6"
ABSTRACT,0.0109717868338558,"types and thus can be used to bound weightings not foreseen at training, including
7"
ABSTRACT,0.012539184952978056,"the full confusion matrix in the multiclass classification case.
8"
INTRODUCTION,0.014106583072100314,"1
Introduction
9"
INTRODUCTION,0.01567398119122257,"Generalisation bounds are a core component of the theoretical understanding of machine learning
10"
INTRODUCTION,0.017241379310344827,"algorithms. For over two decades now, the PAC-Bayesian theory has been at the core of studies
11"
INTRODUCTION,0.018808777429467086,"on generalisation abilities of machine learning algorithms. PAC-Bayes originates in the seminal
12"
INTRODUCTION,0.02037617554858934,"work of [24, 25] and was further developed by citepcatoni2003pac,catoni2004statistical,catoni2007,
13"
INTRODUCTION,0.0219435736677116,"among other authors—we refer to the recent surveys [16] and [1] for an introduction to the field. The
14"
INTRODUCTION,0.023510971786833857,"outstanding empirical successes of deep neural networks in the past decade call for better theoretical
15"
INTRODUCTION,0.025078369905956112,"understanding of deep learning, and PAC-Bayes emerged as one of the few frameworks allowing
16"
INTRODUCTION,0.02664576802507837,"the derivation of meaningful (and non-vacuous) generalisation bounds for neural networks: the
17"
INTRODUCTION,0.02821316614420063,"pioneering work of [13] has been followed by a number of contributions, including [28], [35], [19],
18"
INTRODUCTION,0.029780564263322883,"[30, 31] and [4, 6, 5], to name but a few.
19"
INTRODUCTION,0.03134796238244514,"Much of the PAC-Bayes literature focuses on the case of binary classification, or of multiclass
20"
INTRODUCTION,0.032915360501567396,"classification where one only distinguishes whether each classification is correct or incorrect. This is
21"
INTRODUCTION,0.034482758620689655,"in stark contrast to the complexity of contemporary real-world learning problems. This work aims to
22"
INTRODUCTION,0.03605015673981191,"bridge this gap via generalisation bounds that provide information rich measures of performance at test
23"
INTRODUCTION,0.03761755485893417,"time by controlling the probabilities of errors of any finite number of types, bounding combinations
24"
INTRODUCTION,0.03918495297805643,"of these probabilities uniformly over all weightings.
25"
INTRODUCTION,0.04075235109717868,"Previous results. We believe our framework of discretised error types to be novel. In the particular
26"
INTRODUCTION,0.04231974921630094,"case of multiclass classification, little is known from a theoretical perspective and, to the best of our
27"
INTRODUCTION,0.0438871473354232,"knowledge, only a handful of relevant strategies or generalisation bounds can be compared to the
28"
INTRODUCTION,0.045454545454545456,"present paper. The closest is the work of [27] on a PAC-Bayes generalisation bound on the operator
29"
INTRODUCTION,0.047021943573667714,"norm of the confusion matrix, to train a Gibbs classifier. We focus on a different performance metric,
30"
INTRODUCTION,0.048589341692789965,"in the broader setting of discretised error types. [17] suggest to minimise the confusion matrix norm
31"
INTRODUCTION,0.050156739811912224,"with a focus on the imbalance between classes; their treatment is not done through PAC-Bayes. [18]
32"
INTRODUCTION,0.05172413793103448,"extend the celebrated C-bound in PAC-Bayes to weighted majority votes of classifiers, to perform
33"
INTRODUCTION,0.05329153605015674,"multiclass classification. [3] present a streamlined version of some of the results from [27] in the
34"
INTRODUCTION,0.054858934169279,"case where some examples are voluntarily not classified (e.g., in the case of too large uncertainty).
35"
INTRODUCTION,0.05642633228840126,"More recently, [15] derive bounds for a majority vote classifier where the confusion matrix serves as
36"
INTRODUCTION,0.05799373040752351,"an error indicator: they conduct a study of the Bayes classifier.
37"
INTRODUCTION,0.05956112852664577,"From binary to multiclass classification. A number of PAC-Bayesian bounds have been unified by a
38"
INTRODUCTION,0.061128526645768025,"single general bound, found in [7]. Stated as Theorem 1 below, it applies to binary classification. We
39"
INTRODUCTION,0.06269592476489028,"use it as a basis to prove our Theorem 3, a more general bound that can be applied to, amongst other
40"
INTRODUCTION,0.06426332288401254,"things, multiclass classification and discretised regression. While the proof of Theorem 3 follows
41"
INTRODUCTION,0.06583072100313479,"similar lines to that given in [7], our generalisation to ‘soft’ hypotheses incurring any finite number of
42"
INTRODUCTION,0.06739811912225706,"error types requires a non-trivial extension of a result found in [22]. This extension (Lemma 5), along
43"
INTRODUCTION,0.06896551724137931,"with its corollary (Corollary 6) may be of independent interest. The generalisation bound in [22],
44"
INTRODUCTION,0.07053291536050156,"stated below as Corollary 2, is shown in [7] to be a corollary of their bound. In a similar manner, we
45"
INTRODUCTION,0.07210031347962383,"derive Corollary 7 from Theorem 3. Obtaining this corollary is significantly more involved than the
46"
INTRODUCTION,0.07366771159874608,"analogous derivation in [7] or the original proof in [22], requiring a number of technical results found
47"
INTRODUCTION,0.07523510971786834,"in Appendix B.
48"
INTRODUCTION,0.0768025078369906,"Briefly, the results in [7] and [22] consider an arbitrary input set X, output set Y = {−1, 1},
49"
INTRODUCTION,0.07836990595611286,"hypothesis space H ⊆YX and i.i.d. sample S ∈(X × Y)m. They then establish high probability
50"
INTRODUCTION,0.07993730407523511,"bounds on the discrepancy between the risk (probability of error an a new datapoint) of any stochastic
51"
INTRODUCTION,0.08150470219435736,"classifier Q (namely, a distribution on H) and its empirical counterpart (the fraction of the sample Q
52"
INTRODUCTION,0.08307210031347963,"misclassifies). The bounds hold uniformly over all Q and contain a complexity term involving the
53"
INTRODUCTION,0.08463949843260188,"Kullback-Leibler (KL) divergence between Q and a reference distribution P on H (often referred to
54"
INTRODUCTION,0.08620689655172414,"as a prior by analogy with Bayesian inference—see the discussion in 16).
55"
INTRODUCTION,0.0877742946708464,"There are two ways in which the results in [7] and [22] can be described as binary. First, as Y
56"
INTRODUCTION,0.08934169278996865,"contains two elements, this is obviously an instance of binary classification. But a more interesting
57"
INTRODUCTION,0.09090909090909091,"and subtle way to look at this is that only two cases are distinguished—correct classification and
58"
INTRODUCTION,0.09247648902821316,"incorrect classification. Specifically, since the two different directions in which misclassification can
59"
INTRODUCTION,0.09404388714733543,"be made are counted together, the bound gives no information on which direction is more likely.
60"
INTRODUCTION,0.09561128526645768,"More generally, the aforementioned bounds can be applied in the context of multiclass classification
61"
INTRODUCTION,0.09717868338557993,"provided one maintains the second binary characteristic by only distinguishing correct and incorrect
62"
INTRODUCTION,0.0987460815047022,"classifications rather than considering the entire confusion matrix. However, note that these bounds
63"
INTRODUCTION,0.10031347962382445,"will not give information on the relative likelihood of the different errors. In contrast, our new
64"
INTRODUCTION,0.10188087774294671,"results can consider the entire confusion matrix, bounding how far the true (read “expected over the
65"
INTRODUCTION,0.10344827586206896,"data-generating distribution”) confusion matrix differs from the empirical one, according to some
66"
INTRODUCTION,0.10501567398119123,"metric. In fact, our results extend to the case of arbitrary label set Y, provided the number of different
67"
INTRODUCTION,0.10658307210031348,"errors one distinguishes is finite.
68"
INTRODUCTION,0.10815047021943573,"Formally, we let SM
j=1 Ej be a user-specified disjoint partition of Y2 into a finite number of M
69"
INTRODUCTION,0.109717868338558,"error types, where we say that a hypothesis h ∈H makes an error of type j on datapoint (x, y)
70"
INTRODUCTION,0.11128526645768025,"if (h(x), y) ∈Ej (by convention, every pair (ˆy, y) ∈Y2 is interpreted as a predicted value ˆy
71"
INTRODUCTION,0.11285266457680251,"followed by a true value y, in that order). It should be stressed that some Ej need not correspond
72"
INTRODUCTION,0.11442006269592477,"to mislabellings—indeed, some of the Ej may distinguish different correct labellings. We then
73"
INTRODUCTION,0.11598746081504702,"count up the number of errors of each type that a hypothesis makes on a sample, and bound how
74"
INTRODUCTION,0.11755485893416928,"far this empirical distribution of errors is from the expected distribution under the data-generating
75"
INTRODUCTION,0.11912225705329153,"distribution (Theorem 3). Thus, in our generalisation, the (scalar) risk and empirical risk (RD(Q) and
76"
INTRODUCTION,0.1206896551724138,"RS(Q), defined in the next section) are replaced by M-dimensional vectors (RD(Q) and RS(Q)),
77"
INTRODUCTION,0.12225705329153605,"and our discrepancy measure d is a divergence between discrete distributions on M elements. Our
78"
INTRODUCTION,0.1238244514106583,"generalisation therefore allows us to bound how far the true distribution of errors can be from the
79"
INTRODUCTION,0.12539184952978055,"observed distribution of errors. If we then associate a loss value ℓj ∈[0, ∞) to each Ej we can derive
80"
INTRODUCTION,0.12695924764890282,"a bound on the total risk, defined as the sum of the true error probabilities weighted by the loss values.
81"
INTRODUCTION,0.12852664576802508,"In fact, the total risk is bounded with high probability uniformly over all such weightings. The loss
82"
INTRODUCTION,0.13009404388714735,"values need not be distinct; we may wish to understand the distribution of error types even across
83"
INTRODUCTION,0.13166144200626959,"error types that incur the same loss.
84"
INTRODUCTION,0.13322884012539185,"For example, in the case of binary classification with Y = {−1, 1}, we can take the usual partition
85"
INTRODUCTION,0.13479623824451412,"into E1 = {(−1, −1), (1, 1)} and E2 = {(−1, 1), (1, −1)} and loss values ℓ1 = 0, ℓ2 = 1, or the
86"
INTRODUCTION,0.13636363636363635,"fine-grained partition Y2 = {(0, 0)} ∪{(1, 1)} ∪{(0, 1)} ∪{(1, 0)} and the loss values ℓ1 = ℓ2 =
87"
INTRODUCTION,0.13793103448275862,"0, ℓ3 = 1, ℓ4 = 2. More generally, for multiclass classification with N classes and Y = [N], one may
88"
INTRODUCTION,0.13949843260188088,"take the usual coarse partition into E1 = {(ˆy, y) ∈Y2 : ˆy = y} and E2 = {(ˆy, y) ∈Y2 : ˆy ̸= y}
89"
INTRODUCTION,0.14106583072100312,"(with ℓ1 = 0 and ℓ2 = 1), or the fully refined partition into Ei,j = {(i, j)} for i, j ∈[N] (with
90"
INTRODUCTION,0.1426332288401254,"correspondingly greater choice of the associated loss values), or something in-between. Note that we
91"
INTRODUCTION,0.14420062695924765,"still refer to Ej as an “error type” even if it contains elements that correspond to correct classification,
92"
INTRODUCTION,0.14576802507836992,"namely if there exists y ∈Y such that (y, y) ∈Ej. As we will see later, a more fine-grained
93"
INTRODUCTION,0.14733542319749215,"partition will allow more error types to be distinguished and bounded, at the expense of a looser
94"
INTRODUCTION,0.14890282131661442,"bound. As a final example, for regression with Y = R, we may fix M strictly increasing thresholds
95"
INTRODUCTION,0.15047021943573669,"0 = λ1 < λ2 < · · · < λM and partition Y2 into Ej = {(y1, y2) ∈Y2 : λj ≤|y1 −y2| < λj+1} for
96"
INTRODUCTION,0.15203761755485892,"j ∈[M −1], and EM = {(y1, y2) ∈Y2 : |y1 −y2| ≥λM}.
97"
INTRODUCTION,0.1536050156739812,"Outline. We set our notation in Section 2. In Section 3 we state and prove generalisation bounds in
98"
INTRODUCTION,0.15517241379310345,"the setting of discretised error types: this significantly expands the previously known results from [7]
99"
INTRODUCTION,0.15673981191222572,"by allowing for generic output sets Y. Our main results are Theorem 3 and Corollary 7. To make
100"
INTRODUCTION,0.15830721003134796,"our findings profitable to the broader machine learning community we then discuss how these new
101"
INTRODUCTION,0.15987460815047022,"bounds can be turned into tractable training objectives in Section 4 (with a general recipe described
102"
INTRODUCTION,0.1614420062695925,"in greater detail in Appendix A). The paper closes with perspectives for follow-up work in Section 5
103"
INTRODUCTION,0.16300940438871472,"and we defer to Appendix B the proofs of technical results.
104"
NOTATION,0.164576802507837,"2
Notation
105"
NOTATION,0.16614420062695925,"For any set A, let M(A) be the set of probability measures on A. For any M ∈Z>0, define
106"
NOTATION,0.1677115987460815,"[M] := {1, 2, . . . , M}, the M-dimensional simplex △M := {u ∈[0, 1]M : u1 + · · · + uM = 1}
107"
NOTATION,0.16927899686520376,"and its interior △>0
M := △M ∩(0, 1)M. For m, M ∈Z>0, define the integer counterparts Sm,M :=
108

(k1, . . . , kM) ∈ZM
≥0 : k1 + · · · + kM = m
	
and S>0
m,M := Sm,M ∩ZM
>0. The set Sm,M is the
109"
NOTATION,0.17084639498432602,"domain of the multinomial distribution with parameters m, M and some r ∈△M, which is denoted
110"
NOTATION,0.1724137931034483,"Mult(m, M, r) and has probability mass function for k ∈Sm,M given by
111"
NOTATION,0.17398119122257052,"Mult(k; m, M, r) :=

m
k1 k2 · · · kM  M
Y"
NOTATION,0.1755485893416928,"j=1
rkj
j ,
where

m
k1 k2 · · · kM"
NOTATION,0.17711598746081506,"
:=
m!
QM
j=1 kj!
."
NOTATION,0.1786833855799373,"For q, p ∈△M, let kl(q∥p) denote the KL-divergence of Mult(1, M, q) from Mult(1, M, p), namely
112"
NOTATION,0.18025078369905956,"kl(q∥p) := PM
j=1 qj ln qj"
NOTATION,0.18181818181818182,"pj , with the convention that 0 ln 0"
NOTATION,0.1833855799373041,x = 0 for x ≥0 and x ln x
NOTATION,0.18495297805642633,"0 = ∞for x > 0.
113"
NOTATION,0.1865203761755486,"For M = 2 we abuse notation and abbreviate kl((q, 1 −q)∥(p, 1 −p)) to kl(q∥p), which is then the
114"
NOTATION,0.18808777429467086,"conventional definition of kl(·∥·) : [0, 1]2 →[0, ∞] found in the PAC-Bayes literature [as in 33, for
115"
NOTATION,0.1896551724137931,"example].
116"
NOTATION,0.19122257053291536,"Let X and Y be arbitrary input (e.g., feature) and output (e.g., label) sets respectively. Let SM
j=1 Ej
117"
NOTATION,0.19278996865203762,"be a partition of Y2 into a finite sequence of M error types, and to each Ej associate a loss value
118"
NOTATION,0.19435736677115986,"ℓj ∈[0, ∞). The only restriction we place on the loss values ℓj is that they are not all equal. This is
119"
NOTATION,0.19592476489028213,"not a strong assumption, since if they were all equal then all hypotheses would incur equal loss and
120"
NOTATION,0.1974921630094044,"there would be no learning problem: we are effectively ruling out trivial cases.
121"
NOTATION,0.19905956112852666,"Let H ⊆YX denote a hypothesis class, D ∈M(X × Y) a data-generating distribution and
122"
NOTATION,0.2006269592476489,"S ∼Dm an i.i.d. sample of size m drawn from D. For h ∈H and j ∈[M] we define the
123"
NOTATION,0.20219435736677116,"empirical j-risk and true j-risk of h to be Rj
S(h) := 1 m
P"
NOTATION,0.20376175548589343,"(x,y)∈S 1[(h(x), y) ∈Ej] and Rj
D(h) :=
124"
NOTATION,0.20532915360501566,"E(x,y)∼D[1[(h(x), y) ∈Ej]], respectively, namely, the proportion of the sample S on which h makes
125"
NOTATION,0.20689655172413793,"an error of type Ej and the probability that h makes an error of type Ej on a new (x, y) ∼D.
126"
NOTATION,0.2084639498432602,"More generally, suppose H ⊆M(Y)X is a class of soft hypotheses of the form H : X →M(Y),
127"
NOTATION,0.21003134796238246,"where, for any A ⊆Y, H(x)[A] is interpreted as the probability according to H that the label of
128"
NOTATION,0.2115987460815047,"x is in A. It is worth stressing that a soft hypothesis is still deterministic since a prediction is not
129"
NOTATION,0.21316614420062696,"drawn from the distribution it returns. We then define the empirical j-risk of H to be Rj
S(H) :=
130"
"M
P",0.21473354231974923,"1
m
P"
"M
P",0.21630094043887146,"(x,y)∈S H(x)

{ˆy ∈Y : (ˆy, y) ∈Ej}

, namely the mean—over the elements (x, y) of S—
131"
"M
P",0.21786833855799373,"probability mass H assigns to predictions ˆy ∈Y incurring an error of type Ej when labelling each x.
132"
"M
P",0.219435736677116,"Further, we define the true j-risk of H to be Rj
D(H) := E(x,y)∼D

H(x)

{ˆy ∈Y : (ˆy, y) ∈Ej}

,
133"
"M
P",0.22100313479623823,"namely the mean—over (x, y) ∼D—probability mass H assigns to predictions ˆy ∈Y incurring an
134"
"M
P",0.2225705329153605,"error of type Ej when labelling each x. We will see in Section 4 that the more general hypothesis
135"
"M
P",0.22413793103448276,"class H ⊆M(Y)X is necessary for constructing a differentiable training objective.
136"
"M
P",0.22570532915360503,"To each ordinary hypothesis h ∈YX there corresponds a soft hypothesis H ∈M(Y)X that, for each
137"
"M
P",0.22727272727272727,"x ∈X, returns a point mass on h(x). In this case, it is straightforward to show that Rj
S(h) = Rj
S(H)
138"
"M
P",0.22884012539184953,"and Rj
D(h) = Rj
D(H) for all j ∈[M], where we have used the corresponding definitions above for
139"
"M
P",0.2304075235109718,"ordinary and soft hypotheses. Since, in addition, our results hold identically for both ordinary and
140"
"M
P",0.23197492163009403,"soft hypotheses, we henceforth use the same notation h for both ordinary and soft hypotheses and
141"
"M
P",0.2335423197492163,"their associated values Rj
S(h) and Rj
D(h). It will always be clear from the context whether we are
142"
"M
P",0.23510971786833856,"dealing with ordinary or soft hypotheses and thus which of the above definitions of the empirical and
143"
"M
P",0.23667711598746083,"true j-risks is being used.
144"
"M
P",0.23824451410658307,"We define the empirical risk and true risk of a (ordinary or soft) hypothesis h to be RS(h) :=
145"
"M
P",0.23981191222570533,"(R1
S(h), . . . , RM
S (h)) and RD(h) := (R1
D(h), . . . , RM
D (h)), respectively. It is straightforward to
146"
"M
P",0.2413793103448276,"show that RS(h) and RD(h) are elements of △M. Since S is drawn i.i.d. from D, the expectation
147"
"M
P",0.24294670846394983,"of the empirical risk is equal to the true risk, namely ES[Rj
S(h)] = Rj
D(h) for all j and thus
148"
"M
P",0.2445141065830721,"ES[RS(h)] = RD(h). Finally, we generalise to stochastic hypotheses Q ∈M(H), which predict
149"
"M
P",0.24608150470219436,"by first drawing a deterministic hypothesis h ∼Q and then predicting according to h, where a new
150"
"M
P",0.2476489028213166,"h is drawn for each prediction. Thus, we define the empirical j-risk and true j-risk of Q to be
151"
"M
P",0.24921630094043887,"the scalars Rj
S(Q) := Eh∼Q[Rj
S(h)] and Rj
D(Q) := Eh∼Q[Rj
D(h)], for j ∈[M], and simply the
152"
"M
P",0.2507836990595611,"empirical risk and true risk of Q to be the elements of △M defined by RS(Q) := Eh∼Q[RS(h)]
153"
"M
P",0.25235109717868337,"and RD(Q) := Eh∼Q[RD(h)]. As before, since S is i.i.d., we have (using Fubini this time) that
154"
"M
P",0.25391849529780564,"ES[RS(Q)] = RD(Q). Finally, given a loss vector ℓ∈[0, ∞)M, we define the total risk of Q by
155"
"M
P",0.2554858934169279,"the scalar RT
D(Q) := PM
j=1 ℓjRj
D(Q). As is conventional in the PAC-Bayes literature, we refer to
156"
"M
P",0.25705329153605017,"sample independent and dependent distributions on M(H) (i.e. stochastic hypotheses) as priors
157"
"M
P",0.25862068965517243,"(denoted P) and posteriors (denoted Q) respectively, even if they are not related by Bayes’ theorem.
158"
INSPIRATION AND MAIN RESULTS,0.2601880877742947,"3
Inspiration and Main Results
159"
INSPIRATION AND MAIN RESULTS,0.2617554858934169,"We first state the existing results in [7] and [22] that we will generalise from just two error types
160"
INSPIRATION AND MAIN RESULTS,0.26332288401253917,"(correct and incorrect) to any finite number of error types. These results are stated in terms of
161"
INSPIRATION AND MAIN RESULTS,0.26489028213166144,"the scalars RS(Q) :=
1
m
P"
INSPIRATION AND MAIN RESULTS,0.2664576802507837,"(x,y)∈S 1[h(x) ̸= y] and RD(Q) := E(x,y)∼D1[h(x) ̸= y] and, as we
162"
INSPIRATION AND MAIN RESULTS,0.26802507836990597,"demonstrate, correspond to the case M = 2 of our generalisations.
163"
INSPIRATION AND MAIN RESULTS,0.26959247648902823,"Theorem 1. (7, Theorem 4) Let X be an arbitrary set and Y = {−1, 1}. Let D ∈M(X × Y)
164"
INSPIRATION AND MAIN RESULTS,0.2711598746081505,"be a data-generating distribution and H ⊆YX be a hypothesis class. For any prior P ∈M(H),
165"
INSPIRATION AND MAIN RESULTS,0.2727272727272727,"δ ∈(0, 1], convex function d : [0, 1]2 →R, sample size m and β ∈(0, ∞), with probability at least
166"
INSPIRATION AND MAIN RESULTS,0.274294670846395,"1 −δ over the random draw S ∼Dm, we have that simultaneously for all posteriors Q ∈M(H)
167"
INSPIRATION AND MAIN RESULTS,0.27586206896551724,"d
 
RS(Q), RD(Q)

≤1 β"
INSPIRATION AND MAIN RESULTS,0.2774294670846395,"
KL(Q∥P) + ln Id(m, β) δ 
,"
INSPIRATION AND MAIN RESULTS,0.27899686520376177,"with Id(m, β) := supr∈[0,1]
hPm
k=0 Bin(k; m, r) exp

βd
  k"
INSPIRATION AND MAIN RESULTS,0.28056426332288403,"m, r
 i
, where Bin(k; m, r) is the bi-
168"
INSPIRATION AND MAIN RESULTS,0.28213166144200624,"nomial probability mass function Bin(k; m, r) :=
 m
k

rk(1 −r)m−k.
169"
INSPIRATION AND MAIN RESULTS,0.2836990595611285,"Note the original statement in [7] is for a positive integer m′, but the proof trivially generalises to any
170"
INSPIRATION AND MAIN RESULTS,0.2852664576802508,"β ∈(0, ∞). One of the bounds that Theorem 1 unifies—which we also generalise—is that of [33],
171"
INSPIRATION AND MAIN RESULTS,0.28683385579937304,"later tightened in [22], which we now state. It can be recovered from Theorem 1 by setting β = m
172"
INSPIRATION AND MAIN RESULTS,0.2884012539184953,"and d(q, p) = kl(q∥p) := q ln q"
INSPIRATION AND MAIN RESULTS,0.28996865203761757,p + (1 −q) ln 1−q
INSPIRATION AND MAIN RESULTS,0.29153605015673983,"1−p.
173"
INSPIRATION AND MAIN RESULTS,0.29310344827586204,"Corollary 2. (22, Theorem 5) Let X be an arbitrary set and Y = {−1, 1}. Let D ∈M(X × Y)
174"
INSPIRATION AND MAIN RESULTS,0.2946708463949843,"be a data-generating distribution and H ⊆YX be a hypothesis class. For any prior P ∈M(H),
175"
INSPIRATION AND MAIN RESULTS,0.2962382445141066,"δ ∈(0, 1] and sample size m, with probability at least 1 −δ over the random draw S ∼Dm, we
176"
INSPIRATION AND MAIN RESULTS,0.29780564263322884,"have that simultaneously for all posteriors Q ∈M(H)
177"
INSPIRATION AND MAIN RESULTS,0.2993730407523511,"kl
 
RS(Q), RD(Q)

≤1 m"
INSPIRATION AND MAIN RESULTS,0.30094043887147337,"
KL(Q∥P) + ln 2√m δ 
."
INSPIRATION AND MAIN RESULTS,0.30250783699059564,"We wish to bound the deviation of the empirical vector RS(Q) from the unknown vector RD(Q).
178"
INSPIRATION AND MAIN RESULTS,0.30407523510971785,"Since in general the stochastic hypothesis Q we learn will depend on the sample S, it is useful
179"
INSPIRATION AND MAIN RESULTS,0.3056426332288401,"to obtain bounds on the deviation of RS(Q) from RD(Q) that are uniform over Q, just as in
180"
INSPIRATION AND MAIN RESULTS,0.3072100313479624,"Theorem 1 and Corollary 2. In Theorem 1, the deviation d(RS(Q), RD(Q)) between the scalars
181"
INSPIRATION AND MAIN RESULTS,0.30877742946708464,"RS(Q), RD(Q) ∈[0, 1] is measured by some convex function d : [0, 1]2 →R. In our case, the
182"
INSPIRATION AND MAIN RESULTS,0.3103448275862069,"deviation d(RS(Q), RD(Q)) between the vectors RS(Q), RD(Q) ∈△M is measured by some
183"
INSPIRATION AND MAIN RESULTS,0.31191222570532917,"convex function d : △2
M →R. In Section 3.2 we will derive Corollary 7 from Theorem 3 by selecting
184"
INSPIRATION AND MAIN RESULTS,0.31347962382445144,"β = m and d(q, p) := kl(q∥p), analogous to how Corollary 2 is obtained from Theorem 1.
185"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.31504702194357365,"3.1
Statement and proof of the generalised bound
186"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3166144200626959,"We now state and prove our generalisation of Theorem 1. The proof follows identical lines to that
187"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3181818181818182,"of Theorem 1 given in [7], but with additional non-trivial steps to account for the greater number of
188"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.31974921630094044,"error types and the possibility of soft hypotheses.
189"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3213166144200627,"Theorem 3. Let X and Y be arbitrary sets and SM
j=1 Ej be a disjoint partition of Y2. Let D ∈
190"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.322884012539185,"M(X × Y) be a data-generating distribution and H ⊆M(Y)X be a hypothesis class. For any
191"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.32445141065830724,"prior P ∈M(H), δ ∈(0, 1], jointly convex function d : △2
M →R, sample size m and β ∈(0, ∞),
192"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.32601880877742945,"with probability at least 1 −δ over the random draw S ∼Dm, we have that simultaneously for all
193"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3275862068965517,"posteriors Q ∈M(H)
194"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.329153605015674,"d
 
RS(Q), RD(Q)

≤1 β"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.33072100313479624,"
KL(Q∥P) + ln Id(m, β) δ"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3322884012539185,"
,
(1)"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3338557993730408,"where Id(m, β) := supr∈△M
hP"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.335423197492163,"k∈Sm,M Mult(k; m, M, r) exp

βd
  k"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.33699059561128525,"m, r
i
. Further, the bounds
195"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3385579937304075,"are unchanged if one restricts to an ordinary hypothesis class, namely if H ⊆YX .
196"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3401253918495298,"The proof begins on the following page after a discussion and some auxiliary results. One can
197"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.34169278996865204,"derive multiple bounds from this theorem, all of which then hold simultaneously with probability
198"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3432601880877743,"at least 1 −δ. For example, one can derive bounds on the individual error probabilities Rj
D(Q) or
199"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3448275862068966,"combinations thereof. It is this flexibility that allows Theorem 3 to provide far richer information
200"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3463949843260188,"on the performance of the posterior Q on unseen data. For a more in depth discussion of how such
201"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.34796238244514105,"bounds can be derived, including a recipe for transforming the bound into a differentiable training
202"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3495297805642633,"objective, see Section 4 and Appendix A.
203"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3510971786833856,"To see that Theorem 3 is a generalisation of Theorem 1, note that we can recover it by setting
204"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.35266457680250785,"Y = {−1, 1}, M = 2, E1 = {(−y, y) : y ∈Y} and E2 = {(y, y) : y ∈Y}. Then, for any
205"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3542319749216301,"convex function d : [0, 1]2 →R, apply Theorem 3 with the convex function d′ : △2
M →R
206"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3557993730407524,"defined by d′((u1, u2), (v1, v2)) := d(u1, v1) so that Theorem 3 bounds d′ 
RS(Q), RD(Q)

=
207"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3573667711598746,"d
 
R1
S(Q), R1
D(Q)

which equals d(RS(Q), RD(Q)) in the notation of Theorem 1. Further,
208 X"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.35893416927899685,"k∈Sm,2
Mult(k; m, 2, r) exp

βd′  k"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3605015673981191,"m, r
 
= m
X"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3620689655172414,"k=0
Bin(k; m, r1) exp

βd
  k"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.36363636363636365,"m, r1
 
,"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3652037617554859,"so that the supremum over r1 ∈[0, 1] of the right hand side equals the supremum over r ∈△2 of the
209"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3667711598746082,"left hand side, which, when substituted into (1), yields the bound given in Theorem 1.
210"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3683385579937304,"Our proof of Theorem 3 follows the lines of the proof of Theorem 1 in [7], making use of the change
211"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.36990595611285265,"of measure inequality Lemma 4. However, a complication arises from the use of soft classifiers
212"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3714733542319749,"h ∈M(Y)X . A similar problem is dealt with in [22] when proving Corollary 2 by means of a
213"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3730407523510972,"Lemma permitting the replacement of [0, 1]-valued random variables by corresponding {0, 1}-valued
214"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.37460815047021945,"random variables with the same mean. We use a generalisation of this, stated as Lemma 5 (Lemma
215"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3761755485893417,"3 in 22 corresponds to the case M = 2), the proof of which is not insightful for our purposes and
216"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3777429467084639,"thus deferred to Appendix B.1. An immediate consequence of Lemma 5 is Corollary 6, which is a
217"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3793103448275862,"generalisation of the first half of Theorem 1 in [22]. While we only use it implicitly in the remainder
218"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.38087774294670845,"of the paper, we state it as it may be of broader interest.
219"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3824451410658307,"The consequence of Lemma 5 is that the worst case (in terms of bounding d(RS(Q), RD(Q))) occurs
220"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.384012539184953,"when R{(x,y)}(h) is a one-hot vector for all (x, y) ∈S and h ∈H, namely when H ⊆M(Y)X only
221"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.38557993730407525,"contains hypotheses that, when labelling S, put all their mass on elements ˆy ∈Y that incur the same
222"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3871473354231975,"error type1. In particular, this is the case for hypotheses that put all their mass on a single element of
223"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3887147335423197,"Y, equivalent to the simpler case H ⊆YX as discussed in Section 2. Thus, Lemma 5 shows that the
224"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.390282131661442,"bound given in Theorem 3 cannot be made tighter only by restricting to such hypotheses.
225"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.39184952978056425,"Lemma 4. (Change of measure, 10, 11) For any set H, any P, Q ∈M(H) and any measurable
226"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3934169278996865,"function ϕ : H →R, E
h∼Qϕ(h) ≤KL(Q∥P) + ln E
h∼P exp(ϕ(h)).
227"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3949843260188088,"Lemma 5. (Generalisation of Lemma 3 in 22) Let X1, . . . , Xm be i.i.d △M-valued random vectors
228"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.39655172413793105,"with mean µ and suppose that f : △m
M →R is convex. If X′
1, . . . , X′
m are i.i.d. Mult(1, M, µ)
229"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3981191222570533,"random vectors, then E[f(X1, . . . , Xm)] ≤E[f(X′
1, . . . , X′
m)].
230"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.3996865203761755,"1More precisely, when ∀h ∈H ∀(x, y) ∈S ∃j ∈[M] such that h(x)[{ˆy ∈Y : (ˆy, y) ∈Ej)}] = 1."
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4012539184952978,"Corollary 6. (Generalisation of Theorem 1 in 22) Let X1, . . . , Xm be i.i.d △M-valued random
231"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.40282131661442006,"vectors with mean µ, and X′
1, . . . , X′
m be i.i.d. Mult(1, M, µ). Define ¯
X :=
1
m
Pm
i=1 Xi and
232"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4043887147335423,"¯
X′ := 1"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4059561128526646,"m
Pm
i=1 X′
i. Then E[exp(mkl( ¯
X∥µ)] ≤E[exp(mkl( ¯
X′∥µ)].
233"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.40752351097178685,"Proof. (of Corollary 6) This is immediate from Lemma 5 since the average is linear, the kl-divergence
234"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4090909090909091,"is convex and the exponential is non-decreasing and convex.
235"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4106583072100313,"Proof. (of Theorem 3) The case H ⊆YX follows directly from the more general case by taking
236"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4122257053291536,"H′ := {h′ ∈M(Y)X : ∃h ∈H such that ∀x ∈X h′(x) = δh(x)}, where δh(x) ∈M(Y) denotes a
237"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.41379310344827586,"point mass on h(x). For the general case H ⊆M(Y)X , using Jensen’s inequality with the convex
238"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4153605015673981,"function d(·, ·) and Lemma 4 with ϕ(h) = βd(RS(h), RD(h)), we see that for all Q ∈M(H)
239"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4169278996865204,"βd
 
RS(Q), RD(Q)

= βd

E
h∼QRS(h), E
h∼QRD(h)
"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.41849529780564265,"≤
E
h∼Qβd
 
RS(h), RD(h)
"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4200626959247649,"≤KL(Q∥P) + ln

E
h∼P exp

βd
 
RS(h), RD(h)
"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4216300940438871,"= KL(Q∥P) + ln(ZP (S)),"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4231974921630094,"where ZP (S) := Eh∼P exp
 
βd(RS(h), RD(h))

. Note that ZP (S) is a non-negative random
240"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.42476489028213166,"variable, so that by Markov’s inequality
P
S∼Dm"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4263322884012539,"
ZP (S) ≤ES′∼DmZP (S′)"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4278996865203762,"δ

≥1−δ. Thus, since ln(·)
241"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.42946708463949845,"is strictly increasing, with probability at least 1 −δ over S ∼Dm, we have that simultaneously for
242"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.43103448275862066,"all Q ∈M(H)
243"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.43260188087774293,"βd
 
RS(Q), RD(Q)

≤KL(Q∥P) + ln
E
S′∼DmZP (S′)"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4341692789968652,"δ
.
(2)"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.43573667711598746,"To bound ES′∼DmZP (S′), let Xi := R{(xi,yi)′}(h) ∈△M for i ∈[m], where (xi, yi)′ is the
244"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4373040752351097,"i’th element of the dummy sample S′. Noting that each Xi has mean RD(h), define the random
245"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.438871473354232,"vectors X′
i ∼Mult(1, M, RD(h)) and Y := Pm
i=1 X′
i ∼Mult(m, M, RD(h)). Finally let f :
246"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.44043887147335425,"△m
M →R be defined by f(x1, . . . , xm) := exp
 
βd
  1"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.44200626959247646,"m
Pm
i=1 xi, RD(h)

, which is convex since
247"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.44357366771159873,"the average is linear, d is convex and the exponential is non-decreasing and convex. Then, by
248"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.445141065830721,"swapping expectations (which is permitted by Fubini’s theorem since the argument is non-negative)
249"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.44670846394984326,"and applying Lemma 5, we have that ES′∼DmZP (S′) can be written as
250"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4482758620689655,"ES′∼DmZP (S′) =
E
S′∼Dm
E
h∼P exp

βd
 
RS′(h), RD(h)
"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.4498432601880878,"= E
h∼P
E
S′∼Dm exp

βd
 
RS′(h), RD(h)
"
STATEMENT AND PROOF OF THE GENERALISED BOUND,0.45141065830721006,"= E
h∼P
E
X1,...,Xm exp  βd"
M,0.45297805642633227,"1
m m
X"
M,0.45454545454545453,"i=1
Xi, RD(h) !!"
M,0.4561128526645768,"≤E
h∼P
E
X′
1,...,X′
m
exp  βd"
M,0.45768025078369906,"1
m m
X"
M,0.4592476489028213,"i=1
X′
i, RD(h) !!"
M,0.4608150470219436,"= E
h∼P E
Y exp

βd
 1"
M,0.46238244514106586,"mY , RD(h)
"
M,0.46394984326018807,"= E
h∼P X"
M,0.46551724137931033,"k∈Sm,M
Mult
 
k; m, M, RD(h)

exp

βd
  k"
M,0.4670846394984326,"m, RD(h)
"
M,0.46865203761755486,"≤sup
r∈△M  
X"
M,0.4702194357366771,"k∈Sm,M
Mult
 
k; m, M, r

exp

βd
  k"
M,0.4717868338557994,"m, r

 ."
M,0.47335423197492166,"Which is the definition of Id(m, β). Inequality (1) then follows by substituting this bound on
251"
M,0.47492163009404387,"ES′∼DmZP (S′) into (2) and dividing by β.
252"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.47648902821316613,"3.2
Statement and proof of the generalised corollary
253"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.4780564263322884,"We now apply our generalised theorem with β = m and d(q, p) = kl(q∥p). This results in the
254"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.47962382445141066,"following corollary, analogous to Corollary 2 (although the multi-dimensionality makes the proof
255"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.48119122257053293,"much more involved, requiring multiple lemmas and extra arguments to make the main idea go
256"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.4827586206896552,"through). We give two forms of the bound since, while the second is looser, the first is not practical
257"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.4843260188087774,"to calculate except when m is very small.
258"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.48589341692789967,"Corollary 7. Let X and Y be arbitrary sets and SM
j=1 Ej be a disjoint partition of Y2. Let
259"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.48746081504702193,"D ∈M(X × Y) be a data-generating distribution and H ⊆M(Y)X be a hypothesis class. For any
260"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.4890282131661442,"prior P ∈M(H), δ ∈(0, 1] and sample size m, with probability at least 1 −δ over the random
261"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.49059561128526646,"draw S ∼Dm, we have that simultaneously for all posteriors Q ∈M(H)
262"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.49216300940438873,"kl
 
RS(Q)∥RD(Q)

≤1 m "
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.493730407523511,"KL(Q∥P) + ln  m! δmm
X"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.4952978056426332,"k∈Sm,M M
Y j=1"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.49686520376175547,"kkj
j
kj!    
(3) ≤1 m """
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.49843260188087773,KL(Q∥P) + ln
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5,"1
δ
√πe1/(12m) m 2  M−1"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5015673981191222,"2
M−1
X z=0 M
z 
1"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5031347962382445,"(πm)z/2 Γ
  M−z 2
 !# ,
(4)"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5047021943573667,"where the second inequality holds provided m ≥M. Further, the bounds are unchanged if one
263"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5062695924764891,"restricts to an ordinary hypothesis class, namely if H ⊆YX .
264"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5078369905956113,"While analogous corollaries can be obtained from Theorem 3 by other choices of convex function d,
265"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5094043887147336,"the kl-divergence leads to convenient cancellations that remove the dependence of Ikl(m, β, r) on
266"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5109717868338558,"r, making Ikl(m, β) := supr∈△M Ikl(m, β, r) simple to evaluate. Note (4) is logarithmic in 1/δ
267"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.512539184952978,"(typical of PAC-Bayes bounds) and thus the confidence can be increased very cheaply. Ignoring
268"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5141065830721003,"logarithmic terms, (4) is O(1/m), also as expected. As for M, a simple analysis shows that (4) grows
269"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5156739811912225,"only sublinearly in M, meaning M can be made quite large provided one has a reasonable amount of
270"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5172413793103449,"data. To prove Corollary 7 we require Lemma 8, the proof of which is deferred to Appendix B.2.
271"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5188087774294671,"Lemma 8. For integers M ≥1 and m ≥M, P"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5203761755485894,"k∈S>0
m,M
1
QM
j=1
√"
STATEMENT AND PROOF OF THE GENERALISED COROLLARY,0.5219435736677116,"kj ≤π
M"
M,0.5235109717868338,"2 m
M−2"
M,0.5250783699059561,"2
Γ( M"
M,0.5266457680250783,"2 )
.
272"
M,0.5282131661442007,"Proof. (of Corollary 7) Applying Theorem 3 with d(q, p) = kl(q∥p) (defined in Section 2) and
273"
M,0.5297805642633229,"β = m gives that with probability at least 1 −δ over S ∼Dm, simultaneously for all pos-
274"
M,0.5313479623824452,"teriors Q ∈M(H), kl
 
RS(Q)∥RD(Q)

≤
1
m[KL(Q∥P) + ln Ikl(m,m)"
M,0.5329153605015674,"δ
], where Ikl(m, m) :=
275"
M,0.5344827586206896,supr∈△M [P
M,0.5360501567398119,"k∈Sm,M Mult(k; m, M, r) exp
 
mkl( k"
M,0.5376175548589341,"m, r

)]. Thus, to establish the first inequality of
276"
M,0.5391849529780565,"the corollary, it suffices to show that
277"
M,0.5407523510971787,"Ikl(m, m) ≤m! mm
X"
M,0.542319749216301,"k∈Sm,M M
Y j=1"
M,0.5438871473354232,"kkj
j
kj! .
(5)"
M,0.5454545454545454,"To see this, for each fixed r = (r1, . . . , rM) ∈△M let Jr = {j ∈[M] : rj = 0}. Then
278"
M,0.5470219435736677,"Mult(k; m, M, r) = 0 for any k ∈Sm,M such that kj ̸= 0 for some j ∈Jr. For the other
279"
M,0.54858934169279,"k ∈Sm,M, namely those such that kj = 0 for all j ∈Jr, the probability term can be written as
280"
M,0.5501567398119123,"Mult(k; m, M, r) =
m!
QM
j=1 kj!
QM
j=1 rkj
j
=
m!
Q"
M,0.5517241379310345,"j̸∈Jr kj!
Q"
M,0.5532915360501567,"j̸∈Jr rkj
j , and (recalling the convention that
281"
M,0.554858934169279,0 ln 0
M,0.5564263322884012,0 = 0) the term exp(mkl( k
M,0.5579937304075235,"m, r)) can be written as
282 exp  m M
X j=1"
M,0.5595611285266457,"kj
m ln"
M,0.5611285266457681,"kj
m
rj "
M,0.5626959247648903,= exp  X
M,0.5642633228840125,"j̸∈Jr
kj ln kj mrj  =
Y j̸∈Jr  kj mrj"
M,0.5658307210031348,"kj
=
1
mm
Y j̸∈Jr kj rj kj
,"
M,0.567398119122257,"where the last equality is obtained by recalling that the kj sum to m. Substituting these two
283"
M,0.5689655172413793,"expressions into the definition of Ikl(m, m) and only summing over those k ∈Sm,M with non-zero
284"
M,0.5705329153605015,"probability, we obtain
285 X"
M,0.5721003134796239,"k∈Sm,M
Mult(k; m, M, r) exp
 
mkl
  k"
M,0.5736677115987461,"m, r

=
X"
M,0.5752351097178683,"k∈Sm,M :
∀j∈Jr kj=0"
M,0.5768025078369906,"Mult(k; m, M, r) exp
 
mkl
  k"
M,0.5783699059561128,"m, r
 =
X"
M,0.5799373040752351,"k∈Sm,M :
∀j∈Jr kj=0 m!
Q"
M,0.5815047021943573,j̸∈Jr kj! Y
M,0.5830721003134797,"j̸∈Jr
rkj
j
1
mm
Y j̸∈Jr kj rj kj = m! mm
X"
M,0.5846394984326019,"k∈Sm,M :
∀j∈Jr kj=0 Y j̸∈Jr"
M,0.5862068965517241,"kkj
j
kj! = m! mm
X"
M,0.5877742946708464,"k∈Sm,M :
∀j∈Jr kj=0 M
Y j=1"
M,0.5893416927899686,"kkj
j
kj!
(because 00"
M,0.5909090909090909,"0! = 1) ≤m! mm
X"
M,0.5924764890282131,"k∈Sm,M M
Y j=1"
M,0.5940438871473355,"kkj
j
kj! ."
M,0.5956112852664577,"Since this is independent of r, it also holds after taking the supremum over r ∈△M of the left
286"
M,0.5971786833855799,"hand side. We have thus established (5) and hence (3). Now, defining f : S∞
M=2 Sm,M →R by
287"
M,0.5987460815047022,"f(k) = Q|k|
j=1 kkj
j /kj!, we see that to establish inequality (4) it suffices to show that
288"
M,0.6003134796238244,"m!
mm
X"
M,0.6018808777429467,"k∈Sm,M
f(k) ≤√πe1/12m m 2  M−1"
M,0.603448275862069,"2
M−1
X z=0 M
z 
1"
M,0.6050156739811913,"(πm)z/2 Γ
  M−z"
M,0.6065830721003135,"2
.
(6)"
M,0.6081504702194357,"We show this by upper bounding each f(k) individually using Stirling’s formula: ∀n ≥1
289
√"
M,0.609717868338558,"2πn
  n"
M,0.6112852664576802,"e
n < n! <
√"
M,0.6128526645768025,"2πn
  n"
M,0.6144200626959248,"e
n e
1
12n . Since we cannot use this to upper bound 1/kj! when
290"
M,0.6159874608150471,"kj = 0, we partition the sum above according to the number of coordinates of k at which kj = 0. Let
291"
M,0.6175548589341693,"z index the number of such coordinates. Since f is symmetric under permutations of its arguments,
292 X"
M,0.6191222570532915,"k∈Sm,M
f(k) = M−1
X z=0 M
z 
X"
M,0.6206896551724138,"k∈S>0
m,M−z"
M,0.622257053291536,"f(k).
(7)"
M,0.6238244514106583,"For k ∈S>0
m,M Stirling’s formula yields f(k) ≤QM
j=1
k
kj
j
√"
M,0.6253918495297806,"2πkj
 kj"
M,0.6269592476489029,"e
kj
= QM
j=1
ekj
√"
M,0.6285266457680251,"2πkj
=
293 em"
M,0.6300940438871473,"(2π)M/2
QM
j=1
1
√"
M,0.6316614420062696,"kj . An application of Lemma 8 now gives
294 X"
M,0.6332288401253918,"k∈S>0
m,M−z"
M,0.6347962382445141,"f(k) ≤
em"
M,0.6363636363636364,"(2π)M/2
X"
M,0.6379310344827587,"k∈S>0
m,M−z M
Y j=1"
P,0.6394984326018809,"1
p"
P,0.6410658307210031,"kj
≤
em"
P,0.6426332288401254,"(2π)
M"
P,0.6442006269592476,"2
π
M−z"
M,0.64576802507837,"2
m
M−z−2 2"
M,0.6473354231974922,"Γ
  M−z"
M,0.6489028213166145,"2

=
emm
M−2 2"
M,0.6504702194357367,"2
M"
M,0.6520376175548589,"2 (πm)z/2 Γ
  M−z 2
."
M,0.6536050156739812,"Substituting this into equation (7) and bounding m! using Stirling’s formula, we have
295"
M,0.6551724137931034,"m!
mm
X"
M,0.6567398119122257,"k∈Sm,M
f(k) ≤ √"
M,0.658307210031348,"2πme1/12m em M−1
X z=0 M
z"
M,0.6598746081504702,"
emm
M−2 2"
M,0.6614420062695925,"2M/2 (πm)z/2 Γ
  M−z 2
"
M,0.6630094043887147,= √πe1/12m m 2  M−1
M,0.664576802507837,"2
M−1
X z=0 M
z 
1"
M,0.6661442006269592,"(πm)z/2 Γ
  M−z 2
"
M,0.6677115987460815,"which is (6), establishing (4) and therefore completing the proof.
296"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6692789968652038,"4
Implied Bounds and Construction of a Differentiable Training Objective
297"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.670846394984326,"As already discussed, a multitude of bounds can be derived from Theorem 3 and Corollary 7, all of
298"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6724137931034483,"which then hold simultaneously with high probability. For example, suppose after a use of Corollary
299"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6739811912225705,"7 we have a bound of the form kl(RS(Q)||RD(Q)) ≤B. The following proposition then yields the
300"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6755485893416928,"bounds Lj ≤Rj
D(Q) ≤Uj, where Lj := inf{p ∈[0, 1] : kl(Rj
S(Q)∥p) ≤B} and Uj := sup{p ∈
301"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.677115987460815,"[0, 1] : kl(Rj
S(Q)∥p) ≤B}. Moreover, since in the worst case we have kl(RS(Q)||RD(Q)) = B,
302"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6786833855799373,"the proposition shows that the lower and upper bounds Lj and Uj are the tightest possible, since if
303"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6802507836990596,"Rj
D(Q) ̸∈[Lj, Uj] then kl(Rj
S(Q)∥Rj
D(Q)) > B implying kl(RS(Q)||RD(Q)) > B. For a more
304"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6818181818181818,"precise version of this argument and a proof of Proposition 9, see Appendix B.3.
305"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6833855799373041,"Proposition 9. Let q, p ∈△M. Then kl(qj∥pj) ≤kl(q∥p) for all j ∈[M], with equality when
306"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6849529780564263,pi = 1−pj
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6865203761755486,"1−qj qi. for all i ̸= j.
307"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6880877742946708,"As a second much more interesting example, suppose we can quantify how bad an error of each type
308"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6896551724137931,"is by means of a loss vector ℓ∈[0, ∞)M, where ℓj is the loss we attribute to an error of type Ej. We
309"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6912225705329154,"may then be interested in bounding the total risk RT
D(Q) ∈[0, ∞) of Q which, recall, is defined by
310"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6927899686520376,"RT
D(Q) := PM
j=1 ℓjRj
D(Q). Indeed, given a bound of the form kl(RS(Q)||RD(Q)) ≤B, we can
311"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6943573667711599,"derive RT
D(Q) ≤sup{PM
j=1 ℓjrj : r ∈△M, kl(RS(Q)||r) ≤B}. This motivates the following
312"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6959247648902821,"definition of kl−1
ℓ(u|c). To see that this is indeed well-defined (at least when u ∈△>0
M ), see the
313"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6974921630094044,"discussion at the beginning of Appendix B.4.
314"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.6990595611285266,"Definition 10. For u ∈△M, c ∈[0, ∞) and ℓ∈[0, ∞)M, define kl−1
ℓ(u|c) = sup{PM
j=1 ℓjvj :
315"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.700626959247649,"v ∈△M, kl(u∥v) ≤c}.
316"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7021943573667712,"Can we calculate kl−1
ℓ(u|c) and hence fℓ(kl−1
ℓ(u|c)) in order to evaluate the bound on the total risk?
317"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7037617554858934,"Additionally, if we wish to use the bound on the total risk as a training objective, can we calculate
318"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7053291536050157,"the partial derivatives of f ∗
ℓ(u, c) := fℓ(kl−1
ℓ(u|c)) with respect to the uj and c so that we can use
319"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7068965517241379,"gradient descent? Our Proposition 11 answers both of these questions in the affirmative, at least in
320"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7084639498432602,"the sense that it provides a speedy method for approximating these quantities to arbitrary precision
321"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7100313479623824,"provided uj > 0 for all j ∈[M] and c > 0. Indeed, the only approximation step required is that of
322"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7115987460815048,"approximating the unique root of a continuous and strictly increasing scalar function. Thus, provided
323"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.713166144200627,"the uj themselves are differentiable, Corollary 7 combined with Proposition 11 yields a tractable
324"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7147335423197492,"and fully differentiable objective that can be used for training. More details on how this can be
325"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7163009404388715,"done, including an algorithm written in pseudocode, can be found in Appendix A. While somewhat
326"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7178683385579937,"analogous to the technique used in [9] to obtain derivatives of the one-dimensional kl-inverse, our
327"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.719435736677116,"proposition directly yields derivatives on the total risk by (implicitly) employing the envelope theorem
328"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7210031347962382,"(see for example 34). Since the proof of Proposition 11 is rather long and technical, we defer it to
329"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7225705329153606,"Appendix B.4.
330"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7241379310344828,"Proposition 11. Fix ℓ∈[0, ∞)M such that not all ℓj are equal, and define fℓ: △M →[0, ∞) by
331"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.725705329153605,"fℓ(v) := PM
j=1 ℓjvj. For all ˜u = (u, c) ∈△>0
M ×(0, ∞), define v∗(˜u) := kl−1
ℓ(u|c) ∈△M and let
332"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7272727272727273,"µ∗(˜u) ∈(−∞, −maxj ℓj) be the unique solution to c = ϕℓ(µ), where ϕℓ: (−∞, −maxj ℓj) →R
333"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7288401253918495,"is given by ϕℓ(µ) := ln(−PM
j=1
uj
µ+ℓj ) + PM
j=1 uj ln(−(µ + ℓj)), which is continuous and strictly
334"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7304075235109718,"increasing. Then v∗(˜u) = kl−1
ℓ(u|c) is given by
335"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.731974921630094,"v∗(˜u)j =
λ∗(˜u)uj
µ∗(˜u) + ℓj
for j ∈[M], where
λ∗(˜u) =  
M
X j=1"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7335423197492164,"uj
µ∗(˜u) + ℓj   −1 ."
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7351097178683386,"Further, defining f ∗
ℓ: △>0
M × (0, ∞) →[0, ∞) by f ∗
ℓ(˜u) := fℓ(v∗(˜u)), we have that
336"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7366771159874608,"∂f ∗
ℓ
∂uj
(˜u) = λ∗(˜u)

1 + ln
uj
v∗(˜u)j"
IMPLIED BOUNDS AND CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.7382445141065831,"
and
∂f ∗
ℓ
∂c (˜u) = −λ∗(˜u)."
PERSPECTIVES,0.7398119122257053,"5
Perspectives
337"
PERSPECTIVES,0.7413793103448276,"By abstracting to a general setting of discretised error types, we established a novel type of generalisa-
338"
PERSPECTIVES,0.7429467084639498,"tion bound (Theorem 3) providing far richer information than existing PAC-Bayes bounds. Through
339"
PERSPECTIVES,0.7445141065830722,"our Corollary 7 and Proposition 11, our bound inspires a training algorithm (see Appendix A) suitable
340"
PERSPECTIVES,0.7460815047021944,"for many different learning problems, including structured output prediction [as investigated by 8, in
341"
PERSPECTIVES,0.7476489028213166,"the PAC-Bayes setting], multi-task learning and learning-to-learn [see e.g. 23]. We will demonstrate
342"
PERSPECTIVES,0.7492163009404389,"these applications and our bound’s utility for real-world learning problems in an empirical follow-up
343"
PERSPECTIVES,0.7507836990595611,"study. Note we require i.i.d. data, which in practice is frequently not the case or is hard to verify.
344"
PERSPECTIVES,0.7523510971786834,"Further, the number of error types M must be finite. While in continuous scenarios it would be
345"
PERSPECTIVES,0.7539184952978056,"preferable to be able to quantify the entire distribution of loss values without having to discretise into
346"
PERSPECTIVES,0.7554858934169278,"finitely many error types, in the multiclass setting our framework is entirely suitable.
347"
REFERENCES,0.7570532915360502,"References
348"
REFERENCES,0.7586206896551724,"[1] Alquier, P. (2021).
User-friendly introduction to PAC-Bayes bounds.
arXiv preprint
349"
REFERENCES,0.7601880877742947,"arXiv:2110.11216.
350"
REFERENCES,0.7617554858934169,"[2] Ambroladze, A., Parrado-Hernández, E., and Shawe-Taylor, J. (2006). Tighter PAC-Bayes
351"
REFERENCES,0.7633228840125392,"bounds. In Schölkopf, B., Platt, J. C., and Hofmann, T., editors, Advances in Neural Information
352"
REFERENCES,0.7648902821316614,"Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information
353"
REFERENCES,0.7664576802507836,"Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006, pages 9–16. MIT
354"
REFERENCES,0.768025078369906,"Press.
355"
REFERENCES,0.7695924764890282,"[3] Benabbou, L. and Lang, P. (2017). PAC-Bayesian generalization bound for multi-class learning.
356"
REFERENCES,0.7711598746081505,"In NIPS 2017 Workshop. (Almost) 50 Shades of Bayesian Learning: PAC-Bayesian trends and
357"
REFERENCES,0.7727272727272727,"insights.
358"
REFERENCES,0.774294670846395,"[4] Biggs, F. and Guedj, B. (2021). Differentiable PAC-Bayes objectives with partially aggregated
359"
REFERENCES,0.7758620689655172,"neural networks. Entropy, 23(10):1280.
360"
REFERENCES,0.7774294670846394,"[5] Biggs, F. and Guedj, B. (2022a). Non-vacuous generalisation bounds for shallow neural networks.
361"
REFERENCES,0.7789968652037618,"arXiv preprint arXiv:2202.01627.
362"
REFERENCES,0.780564263322884,"[6] Biggs, F. and Guedj, B. (2022b). On margins and derandomisation in PAC-Bayes. In AISTATS.
363"
REFERENCES,0.7821316614420063,"[7] Bégin, L., Germain, P., Laviolette, F., and Roy, J.-F. (2016). PAC-Bayesian Bounds based
364"
REFERENCES,0.7836990595611285,"on the Rényi Divergence. In Gretton, A. and Robert, C. C., editors, Proceedings of the 19th
365"
REFERENCES,0.7852664576802508,"International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of
366"
REFERENCES,0.786833855799373,"Machine Learning Research, pages 435–444, Cadiz, Spain. PMLR.
367"
REFERENCES,0.7884012539184952,"[8] Cantelobre, T., Guedj, B., Pérez-Ortiz, M., and Shawe-Taylor, J. (2020). A pac-bayesian perspec-
368"
REFERENCES,0.7899686520376176,"tive on structured prediction with implicit loss embeddings. arXiv preprint arXiv:2012.03780.
369"
REFERENCES,0.7915360501567398,"[9] Clerico, E., Deligiannidis, G., and Doucet, A. (2021). Conditional Gaussian PAC-Bayes. arXiv
370"
REFERENCES,0.7931034482758621,"preprint arXiv:2110.11886.
371"
REFERENCES,0.7946708463949843,"[10] Csiszár, I. (1975). I-divergence geometry of probability distributions and minimization problems.
372"
REFERENCES,0.7962382445141066,"The Annals of Probability, pages 146–158.
373"
REFERENCES,0.7978056426332288,"[11] Donsker, M. and Varadhan, S. (1975). Large deviations for Markov processes and the asymptotic
374"
REFERENCES,0.799373040752351,"evaluation of certain markov process expectations for large times. In Probabilistic Methods in
375"
REFERENCES,0.8009404388714734,"Differential Equations, pages 82–88. Springer.
376"
REFERENCES,0.8025078369905956,"[12] Dziugaite, G. K., Hsu, K., Gharbieh, W., Arpino, G., and Roy, D. (2021). On the role of data in
377"
REFERENCES,0.8040752351097179,"PAC-Bayes. In Banerjee, A. and Fukumizu, K., editors, The 24th International Conference on
378"
REFERENCES,0.8056426332288401,"Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, volume 130
379"
REFERENCES,0.8072100313479624,"of Proceedings of Machine Learning Research, pages 604–612. PMLR.
380"
REFERENCES,0.8087774294670846,"[13] Dziugaite, G. K. and Roy, D. M. (2017). Computing nonvacuous generalization bounds for
381"
REFERENCES,0.8103448275862069,"deep (stochastic) neural networks with many more parameters than training data. In Conference
382"
REFERENCES,0.8119122257053292,"on Uncertainty in Artificial Intelligence [UAI].
383"
REFERENCES,0.8134796238244514,"[14] Dziugaite, G. K. and Roy, D. M. (2018). Entropy-SGD optimizes the prior of a PAC-Bayes
384"
REFERENCES,0.8150470219435737,"bound: Generalization properties of entropy-SGD and data-dependent priors. In Dy, J. G. and
385"
REFERENCES,0.8166144200626959,"Krause, A., editors, Proceedings of the 35th International Conference on Machine Learning, ICML
386"
REFERENCES,0.8181818181818182,"2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of
387"
REFERENCES,0.8197492163009404,"Machine Learning Research, pages 1376–1385. PMLR.
388"
REFERENCES,0.8213166144200627,"[15] Feofanov, V., Devijver, E., and Amini, M.-R. (2019). Transductive bounds for the multi-
389"
REFERENCES,0.822884012539185,"class majority vote classifier. Proceedings of the AAAI Conference on Artificial Intelligence,
390"
REFERENCES,0.8244514106583072,"33:3566–3573.
391"
REFERENCES,0.8260188087774295,"[16] Guedj, B. (2019). A Primer on PAC-Bayesian Learning. In Proceedings of the second congress
392"
REFERENCES,0.8275862068965517,"of the French Mathematical Society.
393"
REFERENCES,0.829153605015674,"[17] Koço, S. and Capponi, C. (2013). On multi-class classification through the minimization of
394"
REFERENCES,0.8307210031347962,"the confusion matrix norm. In Ong, C. S. and Ho, T. B., editors, Proceedings of the 5th Asian
395"
REFERENCES,0.8322884012539185,"Conference on Machine Learning, volume 29 of Proceedings of Machine Learning Research,
396"
REFERENCES,0.8338557993730408,"pages 277–292, Australian National University, Canberra, Australia. PMLR.
397"
REFERENCES,0.835423197492163,"[18] Laviolette, F., Morvant, E., Ralaivola, L., and Roy, J.-F. (2017). Risk upper bounds for general
398"
REFERENCES,0.8369905956112853,"ensemble methods with an application to multiclass classification. Neurocomputing, 219:15–25.
399"
REFERENCES,0.8385579937304075,"[19] Letarte, G., Germain, P., Guedj, B., and Laviolette, F. (2019). Dichotomize and generalize:
400"
REFERENCES,0.8401253918495298,"PAC-Bayesian binary activated deep neural networks. In Wallach, H., Larochelle, H., Beygelzimer,
401"
REFERENCES,0.841692789968652,"A., dAlché Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing
402"
REFERENCES,0.8432601880877743,"Systems 32, pages 6872–6882. Curran Associates, Inc.
403"
REFERENCES,0.8448275862068966,"[20] Lever, G., Laviolette, F., and Shawe-Taylor, J. (2010). Distribution-dependent PAC-Bayes
404"
REFERENCES,0.8463949843260188,"priors. In International Conference on Algorithmic Learning Theory, pages 119–133. Springer.
405"
REFERENCES,0.8479623824451411,"[21] Lever, G., Laviolette, F., and Shawe-Taylor, J. (2013). Tighter PAC-Bayes bounds through
406"
REFERENCES,0.8495297805642633,"distribution-dependent priors. Theoretical Computer Science, 473:4–28.
407"
REFERENCES,0.8510971786833855,"[22] Maurer, A. (2004). A note on the PAC-Bayesian theorem. arXiv preprint cs/0411099.
408"
REFERENCES,0.8526645768025078,"[23] Maurer, A., Pontil, M., and Romera-Paredes, B. (2016). The benefit of multitask representation
409"
REFERENCES,0.85423197492163,"learning. J. Mach. Learn. Res., 17:81:1–81:32.
410"
REFERENCES,0.8557993730407524,"[24] McAllester, D. A. (1998). Some PAC-Bayesian theorems. In Proceedings of the eleventh annual
411"
REFERENCES,0.8573667711598746,"conference on Computational Learning Theory, pages 230–234. ACM.
412"
REFERENCES,0.8589341692789969,"[25] McAllester, D. A. (1999). PAC-Bayesian model averaging. In Proceedings of the twelfth annual
413"
REFERENCES,0.8605015673981191,"conference on Computational Learning Theory, pages 164–170. ACM.
414"
REFERENCES,0.8620689655172413,"[26] Mohamed, S., Rosca, M., Figurnov, M., and Mnih, A. (2020). Monte carlo gradient estimation
415"
REFERENCES,0.8636363636363636,"in machine learning. J. Mach. Learn. Res., 21(132):1–62.
416"
REFERENCES,0.8652037617554859,"[27] Morvant, E., Koço, S., and Ralaivola, L. (2012). PAC-Bayesian generalization bound on
417"
REFERENCES,0.8667711598746082,"confusion matrix for multi-class classification. In Proceedings of the 29th International Conference
418"
REFERENCES,0.8683385579937304,"on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc /
419"
REFERENCES,0.8699059561128527,"Omnipress.
420"
REFERENCES,0.8714733542319749,"[28] Neyshabur, B., Bhojanapalli, S., and Srebro, N. (2018). A PAC-Bayesian approach to spectrally-
421"
REFERENCES,0.8730407523510971,"normalized margin bounds for neural networks. In 6th International Conference on Learning
422"
REFERENCES,0.8746081504702194,"Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
423"
REFERENCES,0.8761755485893417,"Proceedings. OpenReview.net.
424"
REFERENCES,0.877742946708464,"[29] Parrado-Hernández, E., Ambroladze, A., Shawe-Taylor, J., and Sun, S. (2012). PAC-Bayes
425"
REFERENCES,0.8793103448275862,"bounds with data dependent priors. J. Mach. Learn. Res., 13:3507–3531.
426"
REFERENCES,0.8808777429467085,"[30] Pérez-Ortiz, M., Rivasplata, O., Guedj, B., Gleeson, M., Zhang, J., Shawe-Taylor, J., Bober, M.,
427"
REFERENCES,0.8824451410658307,"and Kittler, J. (2021). Learning pac-bayes priors for probabilistic neural networks. arXiv preprint
428"
REFERENCES,0.8840125391849529,"arXiv:2109.10304.
429"
REFERENCES,0.8855799373040752,"[31] Perez-Ortiz, M., Rivasplata, O., Shawe-Taylor, J., and Szepesvari, C. (2021). Tighter risk
430"
REFERENCES,0.8871473354231975,"certificates for neural networks. Journal of Machine Learning Research, 22(227):1–40.
431"
REFERENCES,0.8887147335423198,"[32] Rivasplata, O., Szepesvári, C., Shawe-Taylor, J., Parrado-Hernández, E., and Sun, S. (2018).
432"
REFERENCES,0.890282131661442,"PAC-Bayes bounds for stable algorithms with instance-dependent priors. In Bengio, S., Wallach,
433"
REFERENCES,0.8918495297805643,"H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in
434"
REFERENCES,0.8934169278996865,"Neural Information Processing Systems 31: Annual Conference on Neural Information Processing
435"
REFERENCES,0.8949843260188087,"Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 9234–9244.
436"
REFERENCES,0.896551724137931,"[33] Seeger, M. (2002). PAC-Bayesian generalisation error bounds for Gaussian process classification.
437"
REFERENCES,0.8981191222570533,"Journal of Machine Learning Research, 3(Oct):233–269.
438"
REFERENCES,0.8996865203761756,"[34] Takayama, A. and Akira, T. (1985). Mathematical economics. Cambridge university press.
439"
REFERENCES,0.9012539184952978,"[35] Zhou, W., Veitch, V., Austern, M., Adams, R. P., and Orbanz, P. (2019). Non-vacuous general-
440"
REFERENCES,0.9028213166144201,"ization bounds at the ImageNet scale: a PAC-Bayesian compression approach. In 7th International
441"
REFERENCES,0.9043887147335423,"Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
442"
REFERENCES,0.9059561128526645,"OpenReview.net.
443"
REFERENCES,0.9075235109717869,"Checklist
444"
REFERENCES,0.9090909090909091,"1. For all authors...
445"
REFERENCES,0.9106583072100314,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
446"
REFERENCES,0.9122257053291536,"contributions and scope? [Yes]
447"
REFERENCES,0.9137931034482759,"(b) Did you describe the limitations of your work? [Yes] We touch upon limitations in
448"
REFERENCES,0.9153605015673981,"Section 5.
449"
REFERENCES,0.9169278996865203,"(c) Did you discuss any potential negative societal impacts of your work? [N/A] Due to
450"
REFERENCES,0.9184952978056427,"the theoretical nature of our contributions, we do not foresee any immediate societal
451"
REFERENCES,0.9200626959247649,"impacts of our work. However, we very much hope that a better understanding of how
452"
REFERENCES,0.9216300940438872,"algorithms generalise in the multiclass setting can inspire a more informed utilisation of
453"
REFERENCES,0.9231974921630094,"these algorithms, and eventually benefit the many people impacted by the deployment
454"
REFERENCES,0.9247648902821317,"of these methods, ultimately leading to a positive societal impact.
455"
REFERENCES,0.9263322884012539,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
456"
REFERENCES,0.9278996865203761,"them? [Yes]
457"
REFERENCES,0.9294670846394985,"2. If you are including theoretical results...
458"
REFERENCES,0.9310344827586207,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] All assumptions
459"
REFERENCES,0.932601880877743,"are present in the statements of the theoretical results.
460"
REFERENCES,0.9341692789968652,"(b) Did you include complete proofs of all theoretical results? [Yes] As follows:
461"
REFERENCES,0.9357366771159875,"i. Theorem 1 is not proved here as it is due to 7, where it appears as Theorem 4.
462"
REFERENCES,0.9373040752351097,"ii. Corollary 2 is not proved here as it is due to 22, where it appears as Theorem 5.
463"
REFERENCES,0.9388714733542319,"iii. Theorem 3 is proved in Section 3.1 after the statement of the necessary Lemmas 4
464"
REFERENCES,0.9404388714733543,"and 5 and Corollary 6. The proof starts on page 6.
465"
REFERENCES,0.9420062695924765,"iv. Lemma 4 is not proved here as it is a known result. See the given references (10,
466"
REFERENCES,0.9435736677115988,"11).
467"
REFERENCES,0.945141065830721,"v. The proof of Lemma 5 is not insightful for our purposes and is thus deferred to
468"
REFERENCES,0.9467084639498433,"Appendix B.1 of the supplementary material.
469"
REFERENCES,0.9482758620689655,"vi. The proof of Corollary 6 in Section 3.1 can be found directly after the corollary
470"
REFERENCES,0.9498432601880877,"statement.
471"
REFERENCES,0.95141065830721,"vii. The proof of Corollary 7 is found in the same section as the statement (3.2), starting
472"
REFERENCES,0.9529780564263323,"on the same page after stating the helping Lemma 8.
473"
REFERENCES,0.9545454545454546,"viii. The proof of Lemma 8 can be found in Appendix B.2 of the supplementary material.
474"
REFERENCES,0.9561128526645768,"ix. The proof of Proposition 9 can be found in Appendix B.3 of the supplementary
475"
REFERENCES,0.957680250783699,"material.
476"
REFERENCES,0.9592476489028213,"x. The proof of Proposition 11 can be found in Appendix B.4 of the supplementary
477"
REFERENCES,0.9608150470219435,"material.
478"
REFERENCES,0.9623824451410659,"3. If you ran experiments...
479"
REFERENCES,0.9639498432601881,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
480"
REFERENCES,0.9655172413793104,"mental results (either in the supplemental material or as a URL)? [N/A]
481"
REFERENCES,0.9670846394984326,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
482"
REFERENCES,0.9686520376175548,"were chosen)? [N/A]
483"
REFERENCES,0.9702194357366771,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
484"
REFERENCES,0.9717868338557993,"ments multiple times)? [N/A]
485"
REFERENCES,0.9733542319749217,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
486"
REFERENCES,0.9749216300940439,"of GPUs, internal cluster, or cloud provider)? [N/A]
487"
REFERENCES,0.9764890282131662,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
488"
REFERENCES,0.9780564263322884,"(a) If your work uses existing assets, did you cite the creators? [N/A]
489"
REFERENCES,0.9796238244514106,"(b) Did you mention the license of the assets? [N/A]
490"
REFERENCES,0.9811912225705329,"(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
491 492"
REFERENCES,0.9827586206896551,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
493"
REFERENCES,0.9843260188087775,"using/curating? [N/A]
494"
REFERENCES,0.9858934169278997,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
495"
REFERENCES,0.987460815047022,"information or offensive content? [N/A]
496"
REFERENCES,0.9890282131661442,"5. If you used crowdsourcing or conducted research with human subjects...
497"
REFERENCES,0.9905956112852664,"(a) Did you include the full text of instructions given to participants and screenshots, if
498"
REFERENCES,0.9921630094043887,"applicable? [N/A]
499"
REFERENCES,0.9937304075235109,"(b) Did you describe any potential participant risks, with links to Institutional Review
500"
REFERENCES,0.9952978056426333,"Board (IRB) approvals, if applicable? [N/A]
501"
REFERENCES,0.9968652037617555,"(c) Did you include the estimated hourly wage paid to participants and the total amount
502"
REFERENCES,0.9984326018808778,"spent on participant compensation? [N/A]
503"
