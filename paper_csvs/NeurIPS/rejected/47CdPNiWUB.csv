Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004132231404958678,"Labeling errors in datasets are common, if not systematic, in practice. They nat-
1
urally arise in a variety of contexts—human labeling, noisy labeling, and weak
2
labeling (i.e., image classiﬁcation), for example. This presents a persistent and
3
pervasive stress on machine learning practice. In particular, neural network (NN)
4
architectures can withstand minor amounts of dataset imperfection with traditional
5
countermeasures such as regularization, data augmentation, and batch normaliza-
6
tion. However, major dataset imperfections often prove insurmountable. We pro-
7
pose and study the implementation of Rockafellian Relaxation (RR), a new loss
8
reweighting, architecture-independent methodology, for neural network training.
9
Experiments indicate RR can enhance standard neural network methods to achieve
10
robust performance across classiﬁcation tasks in computer vision and natural lan-
11
guage processing (sentiment analysis). We ﬁnd that RR can mitigate the effects
12
of dataset corruption due to both (heavy) labeling error and/or adversarial pertur-
13
bation, demonstrating effectiveness across a variety of data domains and machine
14
learning tasks.
15"
INTRODUCTION,0.008264462809917356,"1
Introduction
16"
INTRODUCTION,0.012396694214876033,"Labeling errors are systematic in practice, stemming from various sources. For example, the re-
17
liability of human-generated labels can be negatively impacted by incomplete information, or the
18
subjectivity of the labeling task - as is commonly seen in medical contexts, in which experts can
19
often disagree on matters such as the location of electrocardiogram signal boundaries [8], prostate
20
tumor region delineation, and tumor grading [20]. As well, labeling systems, such as Mechanical
21
Turk1 often ﬁnd expert labelers being replaced with unreliable non-experts [27]. For all these rea-
22
sons, it would be advisable for any practitioner to operate under the assumption that their dataset is
23
corrupted with labeling errors, and possibly to a large degree.
24"
INTRODUCTION,0.01652892561983471,"In this paper, we propose a loss-reweighting methodology for the task of training a classiﬁer on data
25
having higher levels of labeling errors. We show that our method relates to optimistic and robust dis-
26
tributional optimization formulations aimed at addressing adversarial training (AT). These ﬁndings
27
underscore our numerical experiments on NNs that suggest this method of training can provide test
28
performance robust to high levels of labeling error, and to some extent, feature perturbation. Over-
29
all, we tackle the prevalent challenges of label corruption and class imbalance in training datasets,
30
which are critical obstacles for deploying robust machine learning models. Our proposed approach
31
implements Rockafellian Relaxations [23] to address corrupted labels and automatically manage
32
class imbalances without the need for clean validation sets or sophisticated hyper-parameters - com-
33
mon constraints of current methodologies. This distinct capability represents our key contribution,
34
making our approach more practical for handling large industrial datasets.
35"
INTRODUCTION,0.02066115702479339,1http://mturk.com
INTRODUCTION,0.024793388429752067,"We proceed to discuss related works in section 2, and our speciﬁc contributions to the literature.
36
In section 3 we discuss our methodology in detail and provide some theoretical justiﬁcations that
37
motivate the effectiveness of our methodology. The datasets and NN model architectures upon which
38
our experimental results are based are discussed in sections 4 and 5, respectively. We then conclude
39
with numerical experiments and results in section 6.
40"
RELATED WORK,0.028925619834710745,"2
Related Work
41"
RELATED WORK,0.03305785123966942,"Corrupted datasets are of concern, as they potentially pose severe threats to classiﬁcation perfor-
42
mance of numerous machine-learning approaches [36], including, most notably, NNs [15, 33]. Nat-
43
urally, there have been numerous efforts to mitigate this effect [28, 8]. These efforts can be cate-
44
gorized into robust architectures, robust regularization, robust loss function, loss adjustment, and
45
sample selection [28]. Robust architecture methods focus on developing custom NN layers and
46
dedicated NN architectures. This differs from our approach, which is architecture agnostic and
47
could potentially ""wrap around"" these methods. While robust regularization methods like data aug-
48
mentation [26], weight decay [16], dropout [29], and batch normalization [14] can help to bolster
49
performance, they generally do so under lower levels of dataset corruption. Our approach, on the
50
other hand, is capable of handling high levels of corruption, and can seamlessly incorporate methods
51
such as these. In label corruption settings, it has been shown that loss functions, such as robust mean
52
absolute error (MAE) [10] and generalized cross entropy (GCE) [35] are more robust than categor-
53
ical cross entropy (CCE). Again, our method is not dependent on a particular loss function, and it
54
is possible that arbitrary loss functions, including robust MAE and GCE, can be swapped into our
55
methodology with ease. Our approach resembles the loss adjustment methods most closely, where
56
the overall loss is adjusted based on a (re)weighting scheme applied to training examples.
57"
RELATED WORK,0.0371900826446281,"In loss adjustment methods, individual training example losses are typically adjusted multiple times
58
throughout the training process prior to NN updates. These methods can be further grouped into
59
loss correction, loss reweighting, label refurbishment, and meta-learning [28]. Our approach most
60
closely resembles the loss reweighting methods. Under this scheme each training example is as-
61
signed a unique weight, where smaller weights are assigned to examples that have likely been cor-
62
rupted. This reduces the inﬂuence of corrupted examples. A training example can be completely
63
removed if its corresponding weight becomes zero. Indeed, a number of loss reweighting methods
64
are similar to our approach. For example, Ren et al., [22] learn sample weights through the use of
65
a noise-free validation set. Chang et al. [5] assign sample weights based on prediction variances,
66
and Zhang et al. [34] examine the structural relationship among labels to assign sample weights.
67
However, we view the need for a clean dataset, or at least one with sufﬁcient class balance, by these
68
methods as a shortcoming, and our method, in contrast, makes no assumption on the availability of
69
such a dataset.
70"
RELATED WORK,0.04132231404958678,"Satoshi et al. [12] propose a two-phased approach to noise cleaning. The ﬁrst phase trains a standard
71
neural network to determine the top-m most inﬂuential training instances that inﬂuence the decision
72
boundary; these are subsequently removed from the training set to create a cleaner dataset. In the
73
second phase, the neural network is retrained using the cleansed training set. Their method demon-
74
strates superior validation accuracy for various values of m on MNIST and CIFAR-10. Although
75
impressive, their method does not address the fact that most industrial datasets have a reasonably
76
large amount of label corruption [28] which, upon complete cleansing, could also remove informa-
77
tive examples that lie close to the decision boundary. Additionally, the value of m is an additional
78
hyper-parameter that could require signiﬁcant tuning on different datasets and sources.
79"
RELATED WORK,0.045454545454545456,"Mengye et al. [22] propose dealing with label noise and class imbalance by learning exemplar
80
weights automatically. They propose doing so in the following steps: a) Create a pristine noise-free
81
validation set. b) Initially train on a large, noisy training dataset, compute the training loss on the
82
training set, train on the clean validation set, and compute the training loss on the validation set.
83
c) Finally, compute the exemplar weights that temper the training loss computed in step two with
84
validation loss. This approach is algorithmically the most similar to ours, with some key differences.
85
The major difference is that it treats noise and class imbalance similarly. Our approach deals with
86
noisy labels explicitly and can cope with almost any amount of class imbalance automatically, as
87
tested in our experiments with the open-source Hate-Speech dataset, where we experimented with
88
different prevalence levels of Hate-Speech text. The biggest drawback of the method proposed
89
by Mengye et al. is that it requires a clean validation set, which in practice is almost impossible
90"
RELATED WORK,0.049586776859504134,"to obtain; if it were possible, it would not be very prohibitive to clean the entire dataset. Noise,
91
typically, is an artifact of the generative distribution which cannot be cherry-picked as easily in
92
practice. Our approach does not require a clean dataset to be operational or effective.
93"
METHODOLOGY,0.05371900826446281,"3
Methodology
94"
MISLABELING,0.05785123966942149,"3.1
Mislabeling
95"
MISLABELING,0.06198347107438017,"Let X denote a feature space, with Y a corresponding label space. Then Z := X × Y will be
96
a collection of feature-label pairs, with an unknown probability distribution D. Throughout the
97
forthcoming discussions, {(xi, yi)}N
i=1 will denote a sample of N feature-label pairs, for which
98
some pairs will have a mislabeling. More precisely, we begin with a collection (xi, ˜yi) drawn i.i.d.
99
from D, but there is some unknown set C ⊊{1, . . . , N} denoting (corrupted) indices for which
100
yi = ˜yi if and only if i /∈C. For those i ∈C, yi is some incorrect label, selected uniformly at
101
random, following the Noise Completely at Random (NCAR) model [8] also known as uniform
102
label noise.
103"
MISLABELING,0.06611570247933884,"3.2
Rockafellian Relaxation Method (RRM)
104"
MISLABELING,0.07024793388429752,"We adopt the empirical risk minimization (ERM) [31] problem formulation:
105"
MISLABELING,0.0743801652892562,"min
θ
1
N N
X"
MISLABELING,0.07851239669421488,"i=1
J(θ; xi, yi) + r(θ)
(1)"
MISLABELING,0.08264462809917356,"as a baseline against which our method is measured. Given an NN architecture with (learned) param-
106
eter setting θ that takes as input any feature x and outputs a prediction ˆy, J(θ; x, y) is the loss with
107
which we evaluate the prediction ˆy with respect to y. Finally, r(θ) denotes a regularization term.
108"
MISLABELING,0.08677685950413223,"In ERM it is common practice to assign each training observation i a probability pi = 1/N. How-
109
ever, when given a corrupted dataset, we may desire to remove those samples that are affected; in
110
other words, if C ⊊{1, ..., N} is the set of corrupted training observations, then we would desire to
111
set the probabilities in the following alternative way:
112"
MISLABELING,0.09090909090909091,"p = (p1, ..., pN) with pi ="
MISLABELING,0.09504132231404959,"(
0,
if i ∈C
1
N−|C|,
if i ∈{1, ..., N} \ C,
(2)"
MISLABELING,0.09917355371900827,"where |C| is the cardinality of the unknown set C. In this work, we provide a procedure - the Rock-
113
afellian Relaxation Method (RRM) - with the intention of aligning the pi values closer to the desired
114
(but unknown) p of (2) in self-guided, automated fashion. It does so by adopting the Rockafellian
115
Relaxation approach of [23]. More precisely, we consider the problem
116 min
θ"
MISLABELING,0.10330578512396695,"h
v(θ) := min
u∈U N
X"
MISLABELING,0.10743801652892562,"i=1
( 1"
MISLABELING,0.1115702479338843,"N + ui) · J(θ; xi, yi) + γ∥u∥1
i
,
(3)"
MISLABELING,0.11570247933884298,"where U := {u ∈RN : PN
i=1 ui = 0, 1"
MISLABELING,0.11983471074380166,"N + ui ≥0 ∀i = 1, . . . , N}, and some γ > 0.
117"
MISLABELING,0.12396694214876033,"We proceed to comment on this problem that is nonconvex in general, before providing an algorithm.
118"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.128099173553719,"3.3
Analysis and Interpretation of Rockafellian Relaxation
119"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.1322314049586777,"Although problem (3) is nonconvex in general, the computation of v(θ) for any ﬁxed θ amounts
120
to a linear program. The following result characterizes the complete set of solutions to this linear
121
program, and in doing so, provides an interpretation of the role that γ plays in the loss-reweighting
122
action of RRM.
123"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.13636363636363635,"Theorem 3.1. Let γ > 0 and c = (c1, . . . , cN) ∈RN, with cmin := mini ci, and cmax := maxi ci.
124
Write Imin := {i : ci = cmin}, Ibig := {i : ci = cmin + 2γ}, and for any S1 ⊆Imin, S2 ⊆Ibig,
125"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.14049586776859505,"deﬁne the polytope U ∗
S1,S2 :="
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.1446280991735537,"



"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.1487603305785124,"


"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.15289256198347106,"u∗
i ≥0 ∀i : ci = cmin
u∗
i = 0 ∀i : ci ∈(cmin, cmin + 2γ)
u∗∈U :
u∗
i = −1"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.15702479338842976,"N ∀i ∈Ibig \ S2
u∗
i = −1"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.16115702479338842,"N ∀i : ci > cmin + 2γ
u∗
i = 0 ∀i ∈S1 ∪S2"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.1652892561983471,"



"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.16942148760330578,"


"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.17355371900826447,". Then
126"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.17768595041322313,"conv
 
∪S1,S2U ∗
S1,S2

= arg min
u∈U N
X"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.18181818181818182,"i=1
( 1"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.1859504132231405,"N + ui) · ci + γ∥u∥1.
(4)"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.19008264462809918,"The theorem explains that the construction of any optimal solution u∗essentially reduces to cate-
127
gorizing each of the losses among {ci = J(θ; xi, yi)}N
i=1 as “small"" or “big"", according to their
128
position in the partitioning of [cmin, ∞) = [cmin, cmin + 2γ) ∪[cmin + 2γ, ∞). For losses that
129
occur at the break points of cmin and cmin + 2γ, this classiﬁcation can be arbitrary - hence, the use
130
of S1 and S2 set conﬁgurations to capture this degree of freedom.
131"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.19421487603305784,"In particular, those points with losses ci exceeding cmin + 2γ are down-weighted to zero and ef-
132
fectively removed from the dataset. And in the event that cmax −cmin < 2γ, no loss reweighting
133
occurs. In this manner, while lasso produces sparse solutions in the model parameter space, RRM
134
produces sparse weight vectors by assigning zero weight to data points with high losses.
135"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.19834710743801653,"Consequently, if χ := {i : ci ∈(cmin + 2γ, ∞)} converges over the course of any algorithmic
136
scheme, e.g., Algorithm 1, to some set C, then we can conclude that these data points are effectively
137
removed from the dataset even if the training of θ might proceed. This convergence was observed in
138
the experiments of Section 6. It is hence of possible consideration to tune γ for consistency with an
139
estimate α ∈[0, 1] of labeling error in the dataset {(xi, yi)}N
i=1. More precisely, we may tune γ so
140
that |χ|"
ANALYSIS AND INTERPRETATION OF ROCKAFELLIAN RELAXATION,0.2024793388429752,"N ≈α.
141"
RRM AND OPTIMISTIC WASSERSTEIN DISTRIBUTIONALLY ROBUST OPTIMIZATION,0.2066115702479339,"3.4
RRM and Optimistic Wasserstein Distributionally Robust Optimization
142"
RRM AND OPTIMISTIC WASSERSTEIN DISTRIBUTIONALLY ROBUST OPTIMIZATION,0.21074380165289255,"In this section, we discuss RRM’s relation to distributionally robust and optimistic optimization
143
formulations. Indeed, (3)’s formulation as a min-min problem bears resemblance to optimistic for-
144
mulations of recent works, e.g., [19]. We will see as well that the minimization in u, as considered
145
in Theorem 3.1, relates to an approximation of a data-driven Wasserstein Distributionally Robust
146
Optimization (DRO) formulation [30].
147"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.21487603305785125,"3.4.1
Loss-reweighting via Data-Driven Wasserstein Formulation
148"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.2190082644628099,"For this discussion, as it relates to reweighting, we will lift the feature-label space Z = X × Y.
149
More precisely, we let W := R+ denote a space of weights. Next, we say W × Z has an unknown
150
probability distribution D such that πZD = D and ΠWD({1}) = 1. In words, all possible (w.r.t.
151
D) feature-label pairs have a weight of 1. Finally, we deﬁne an auxiliary loss ℓ: W × Z × Θ by
152
ℓ(w, z; θ) := w · J(x, y; θ), for any z = (x, y) ∈Z.
153"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.2231404958677686,"Given a sample {(1, xi, yi)}N
i=1, just as in Section 3.2, we can opt not to take as granted the result-
154
ing empirical distribution DN because of the possibility that |C|-many have incorrect labels (i.e.,
155
yi ̸= ˜yi). Instead, we will admit alternative distributions obtained by shifting the DN’s probability
156
mass off “corrupted"" tuples (1, xi, yi)i∈C to possibly (0, xi, yi), (1, xi, ˜yi), or even some other tuple
157
(1, xj, ˜yj) with j /∈C for example - equivalently, eliminating, correcting, or replacing the sample,
158
respectively. In order to admit such favorable corrections to DN, we can consider the optimistic
159
[19, 30] data-driven problem
160 min
θ "
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.22727272727272727,"vN(θ) :=
min
˜
D:W1(DN, ˜
D)≤ǫ
E ˜
D [ℓ(w, z; θ)] ! ,
(5)"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.23140495867768596,"in which for each parameter tuning θ, vN(θ) measures the expected auxiliary loss with respect to
161
the most favorable distribution within an ǫ - prescribed W1 (1- Wasserstein) distance of DN. It turns
162
out that a budgeted deviation of the weights alone (and not the feature-label pairs) can approximate
163
(up to an error diminishing in N) vN(θ). More precisely, we derive the following approximation
164
along similar lines to [30].
165"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.23553719008264462,"Proposition 3.2. Let ǫ > 0, and suppose for any θ, max(x,y)∈Z |J(θ; x, y)| < ∞. Then there exists
166
κ ≥0 such that for any θ, the following problem
167"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.2396694214876033,"vMIX
N
(θ) :=
min
u1,...,uN N
X"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.24380165289256198,"i=1
( 1"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.24793388429752067,"N + ui) · J(θ; xi, yi) + γθ N
X"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.25206611570247933,"i=1
|ui|"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.256198347107438,s.t. ui + 1
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.2603305785123967,"N ≥0 i = 1, . . . , N"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.2644628099173554,satisﬁes vN(θ) + κ
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.26859504132231404,"N ≥vMIX
N
(θ) ≥vN(θ).
168"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.2727272727272727,"In particular, −γθ ≤mini J(θ; xi, yi), and {i : J(θ; xi, yi) > γθ} are all down-weighted to zero,
169
i.e., u∗
i = −1"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.2768595041322314,"N for any u∗solving vMIX
N
(θ).
170"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.2809917355371901,"In summary, while the optimistic Wasserstein formulation would permit correction to DN with a
171
combination of reweighting and/or feature-label revision, the above indicates that a process focused
172
on reweighting alone could accomplish a reasonable approximation; further, upon comparison to
173
(3), we see that RRM is a constrained version of this approximating problem, that is,
174"
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.28512396694214875,"v(θ) ≥vMIX
N
(θ) ≥vN(θ)."
LOSS-REWEIGHTING VIA DATA-DRIVEN WASSERSTEIN FORMULATION,0.2892561983471074,"Hence, in some sense, we can conﬁrm that RRM is an optimistic methodology but that it is less
175
optimistic than the data-driven Wasserstein approach.
176"
RRM ALGORITHM,0.29338842975206614,"3.5
RRM Algorithm
177"
RRM ALGORITHM,0.2975206611570248,"Towards solving problem (3) in the two decisions θ and u, we proceed iteratively with a block-
178
coordinate descent heuristic outlined in Algorithm 1, whereby we update the two separately in cycli-
179
cal fashion. In other words, we update θ while holding u ﬁxed, and we update u whilst holding θ
180
ﬁxed. The update of θ is an SGD step on a batch of s−many samples. The update of u reduces
181
to a linear program. In light of the discussion in 3.4, we also outline an Adversarial Rockafellian
182
Relaxation method (A-RRM), an execution of RRM that includes a perturbation (parameterized by
183
ǫ ≥0) to the feature x of a sample (x, y), for the purposes of adversarial training.
184"
RRM ALGORITHM,0.30165289256198347,"Algorithm 1 (Adversarial) Rockafellian Relaxation Algorithm (A-RRM/RRM)
Require: Perturbation Multiplier ǫ ∈[0, 1], Number of epochs σ, Batch size s ≥1, learning rate
η > 0, regularization parameter γ > 0, reweighting step µ ∈(0, 1).
u ←0 ∈RN
repeat
for e = 1, . . . , σ do
for b = 1, . . . , ⌈N"
RRM ALGORITHM,0.30578512396694213,"s ⌉do
{(xb
i, yb
i )}s
i=1 ←Draw Batch of size s from {(xi, yi)}N
i=1
for i = 1, . . . , s do
xb
i ←xb
i + ǫ · sign
 
∇xJ(θ; (xb
i, yb
i ))
"
RRM ALGORITHM,0.30991735537190085,"end for
θ ←θ −η Ps
i=1
  1"
RRM ALGORITHM,0.3140495867768595,"N + ui

· ∇θJ(θ; (xb
i, yb
i ))
end for
end for
u∗←minu∈U
PN
i=1
  1"
RRM ALGORITHM,0.3181818181818182,"N + ui

· J(θ; xi, yi) + γ∥u∥1
u ←µu∗+ (1 −µ)u
until Desired Validation Accuracy or Loss"
RRM ALGORITHM,0.32231404958677684,"The stepsize parameters µ, η and the regularization parameter γ are hyper-parameters that may be
185
tuned, or guided by the general discussions above in Section 3.3.
186"
RRM ALGORITHM,0.32644628099173556,"The RRM algorithm, in which ǫ = 0, is meant for contexts in which only label corruption and no
187
feature corruption occurs. The A-RRM algorithm, for which ǫ > 0, is intended for contexts in which
188
both label and feature corruption is anticipated.
189"
DATASETS,0.3305785123966942,"4
Datasets
190"
DATASETS,0.3347107438016529,"We select several datasets to evaluate RRM. In some cases, the selected dataset is nearly pristine. In
191
these cases we perturb the dataset to achieve various types and levels of corruption. Other datasets
192
consist of weakly labeled examples, which we maintain unaltered. The varied data domains and
193
regimes of corruption enable a robust evaluation of RRM.
194"
DATASETS,0.33884297520661155,"MNIST [17]: A multi-class classiﬁcation dataset consisting of 70000 images of digits zero through
195
nine. 60000 digits are set aside for training and 10000 for testing. 0%, 5%, 10%, 20%, and 30%
196
of the training labels are swapped for different, randomly selected digits. The test set labels are
197
unmodiﬁed.
198"
DATASETS,0.34297520661157027,"Toxic Comments [6]: A multi-label classiﬁcation problem from JIGSAW that consists of Wikipedia
199
comments labeled by humans for toxic behavior. Comments can be any number (including zero) of
200
six categories: toxic, severe toxic, obscene, threat, insult, and identity hate. We convert this into a
201
binary classiﬁcation problem by treating the label as either none of the six categories or at least one
202
of the six categories. This dataset is a public dataset used as part of the Kaggle Toxic Comment
203
Classiﬁcation Challenge.
204"
DATASETS,0.34710743801652894,"IMDb [18]: A binary classiﬁcation dataset consisting of 50000 movie reviews each assigned a posi-
205
tive or negative sentiment label. 25000 reviews are selected randomly for training and the remaining
206
are used for testing. 25%, 30%, 40%, and 45% of the labels of the training reviews are randomly
207
selected and swapped from positive sentiment to negative sentiment, and vice versa, to achieve four
208
training datasets of desired levels of label corruption. The test set labels are unmodiﬁed.
209"
DATASETS,0.3512396694214876,"Tissue Necrosis: A binary classiﬁcation dataset consisting of 7874 256x256-pixel hematoxylin and
210
eosin (H&E) stained RGB images derived from [2]. The training dataset consists of 3156 images
211
labeled non-necrotic, as well as 3156 images labeled necrotic. The training images labeled non-
212
necrotic contain no necrosis. However, only 25% of the images labeled necrotic contain necrotic
213
tissue. This type of label error can be expected in cases of weakly-labeled Whole Slide Imagery
214
(WSI). Here, an expert pathologist will provide a slide-level label for a potentially massive slide
215
consisting of gigapixels, but they lack time or resources to provide granular, segmentation-level
216
annotations of the location of the pathology in question. Also, the diseased tissue often occupies
217
a small portion of the WSI, with the remainder consisting of normal tissue. When the gigapixel-
218
sized WSI is subsequently divided into sub-images of manageable size for typical machine-learning
219
workﬂows, many of the sub-images will contain no disease, but will be assigned the ""weak"" label
220
chosen by the expert for the WSI. The test dataset consists of 718 necrosis and 781 non-necrosis
221
256x256-pixel H&E images, which were also derived from [2]. For both the training and test images,
222
[2] provide segmentation-level necrosis annotations, so we are able to ensure a pristine test set, and,
223
in the case of the training set, we were able to identify the corrupted images for the purpose of
224
algorithm evaluation.
225"
ARCHITECTURES,0.35537190082644626,"5
Architectures
226"
ARCHITECTURES,0.359504132231405,"We do not strive to develop a novel NN architectures capable of defeating current state-of-the-art
227
(SOA) performance in each data domain. Nor do we focus on developing robust architectures as
228
described in [28]. Rather, we select a reasonable NN architecture and measure model performance
229
with and without the application of RRM. This approach enables us to demonstrate the general
230
superiority of RRM under varied data domains and NN architectures. We discuss the underlying
231
NN architectures that we employ in this section.
232"
ARCHITECTURES,0.36363636363636365,"MNIST: The MNIST dataset has been studied extensively and harnessed to investigate novel
233
machine-learning methods, including CNNs [4]. We adopt a basic CNN architecture with a few
234
convolutional layers. The ﬁrst layer has a depth of 32, and the next two layers have a depth of 64.
235
Each convolutional layer employs a kernel of size three and the ReLU activation function followed
236
by a max-pooling layer employing a kernal of size 2. The last convolutional layer is connected to a
237
classiﬁcation head consisting of a 100-unit dense layer with ReLU activation, followed by a 10-unit
238
dense layer with softmax activation. In total, there are 159254 trainable parameters. Categorical
239
cross-entropy is employed for the loss function.
240"
ARCHITECTURES,0.3677685950413223,"Toxic Comments: We use a simple model with only a single convolutional layer. A pretrained
241
embedding from FastText is ﬁrst used to map the comments into a 300 dimension embedding space,
242
followed by a single convolutional layer with a kernel size of two with a ReLU activation layer
243
followed by a max-pooling layer. We then apply a 36-unit dense layer, followed by a 6 unit dense
244
layer with sigmoid activation. Binary cross-entropy is used for the loss function.
245"
ARCHITECTURES,0.371900826446281,"IMDb: Transformer architectures have achieved SOA performance on the IMDb dataset sentiment
246
analysis task [7, 32]. As such, we a adopt a reasonable transformer architecture to assess RRM. We
247
utilize the DistilBERT [25] architecture with low-rank adaptation (LoRA) [13] for large language
248
models, which reduces the number of trainable weights from 67584004 to 628994. In this manner,
249
we reduce the computational burden, while maintaining excellent sentiment analysis performance.
250
Binary cross-entropy is employed for the loss function.
251"
ARCHITECTURES,0.3760330578512397,"Tissue Necrosis: Consistent with the computational histopathology literature [21], we employ a
252
convolutional neural network (CNN) architecture for this classiﬁcation task. In particular, a ResNet-
253
50 architecture with pre-trained ImageNet weights is harnessed. The classiﬁcation head is removed
254
and replaced with a dense layer of 512 units and ReLU activation function, followed by an output
255
layer with a single unit using a sigmoid activation function. All weights, with the exception of
256
the new classiﬁcation head are frozen, resulting in 1050114 trainable parameters out of 24637826.
257
Binary cross-entropy is employed for the loss function.
258"
EXPERIMENTS AND RESULTS,0.38016528925619836,"6
Experiments and Results
259"
EXPERIMENTS AND RESULTS,0.384297520661157,"In this work, we have discussed errors/perturbations/corruption to features and labels. We now
260
perform experiments to see how RRM performs under one or the other, or both. The MNIST ex-
261
periments are performed under a setting of both adversarial perturbation, as well as label corruption.
262
The Toxic Comments experiments are performed under settings of label corruption only. All ex-
263
periments are performed using a combination of GPU resources, both cloud-base, as well as access
264
to an on-premise high-performance computing (HPC) facility. We refer the reader to the Appendix
265
(Sections 6.3 and 6.4) for the experiments on IMDb and Tissue Necrosis.
266"
MNIST,0.3884297520661157,"6.1
MNIST
267"
MNIST,0.3925619834710744,"Twenty percent of the training data is set aside for validation purposes. Using Tensorﬂow 2.10 [1],
268
50 iterations of RRM are executed with σ = 10 epochs per iteration for a total of 500 epochs for
269
a given hyperparameter setting. For RRM, the hyperparameter settings of µ and γ at 0.5 and 2.0,
270
respectively, are based on a search to optimize validation set accuracy. For contrast, we perform a
271
comparable 500 epochs using ERM. Both ERM and RRM employ stochastic gradient descent (SGD)
272
with a learning rate (η) of 0.1. Each time a batch is drawn, each training image is perturbed using
273
the Fast Gradient Sign Method (FGSM) [11] adversarial attack: advx = x + ǫ · sign(∇xJ(θ, x, y)),
274
where advx is the resulting perturbed image, x is the original image, y is the image label, ǫ is a
275
multiplier controlling the magnitude of the image perturbation, θ are the model parameters, and J is
276
the loss. An ǫ = 1.0 is used for all training image perturbations.
277"
MNIST,0.39669421487603307,"For each of the 0%, 5%, 10%, 20%, and 30% training label corruption levels, we compare ad-
278
versarial training (AT) and adversarial RRM (A-RRM) performance under varios regimes of test set
279
perturbation (ǫtest ∈0.0, 0.1, 0.25, 0.5, 1.0). In Table 1 we show the test set accuracy achieved when
280
validation set accuracy peaks. We can see that training with an ǫtrain = 1.0 and testing with lower
281
ǫtest levels of 0.00, 0.10, and 0.25, results in a drastic degradation in accuracy for AT for corruption
282
levels greater than 0%. This performance collapse is not observed when using A-RRM. Given that
283
it may be difﬁcult to anticipate the adversarial regime in production environments, A-RRM seems
284
to confer a greater beneﬁt than AT.
285"
MNIST,0.40082644628099173,"We examine the ui-value associated with each training observation, i, from iteration-to-iteration of
286
the heuristic algorithm. Table 2 summarizes the progression of the ui-vector across its 49 updates
287
for the dataset corruption level of 20%. Column “1. iteration” shows the distribution of ui-values
288
following the ﬁrst u-optimization for both the 9600 corrupted training observations and the 38400
289
clean training observations. Initially, all ui-values are approximately equal to 0.0. It is once again
290
observed that, over the course of iterations, the ui-values noticeably change. In column “10. itera-
291
tion” it can be seen that a signiﬁcant number of the ui-values of the corrupted training observations
292"
MNIST,0.4049586776859504,"Table 1: Test accuracy (%) for AT and A-RRM on MNIST under different levels of corruption C
and test-set adversarial perturbation ǫtest."
MNIST,0.4090909090909091,"Percentage Corrupted Training Data
C
0%
5%
10%
20%
30%
ǫtest
AT
A-RRM
AT
A-RRM
AT
A-RRM
AT
A-RRM
AT
A-RRM
0.00
97
96
63
95
57
97
58
96
26
86
0.10
95
93
64
92
71
94
61
93
20
82
0.25
93
90
83
91
88
92
84
90
74
81
50
91
88
94
91
94
90
90
88
97
80
1.00
86
83
95
90
94
86
88
83
98
77"
MNIST,0.4132231404958678,"achieve negative values, while a large majority of the ui-values for the clean training observations
293
remain close to 0.0. Finally, column “49. iteration” displays the ﬁnal ui-values. 9286 out of 9600 of
294
the corrupted training observations have achieved a ui ∈(−2.08, −1.56] · 10 −5. This means these
295
training observations are removed, or nearly-so, from consideration because this value cancels the
296
nominal probability 1/N = 2.08 · 10-5. It is observed that a large majority (35246/38400) clean train-
297
ing observations remain with their nominal probability. This helps explain the performance beneﬁt
298
of A-RRM over AT. A-RRM ""removes"" the corrupted data points in-situ, whereas AT does not. It
299
appears that under adversarial training regimes with corrupted training data, it is essential to identify
300
and ""remove"" the corrupted examples, especially if the level adversarial perturbation encountered in
301
the test set is unknown, or possibly lower than the level of adversarial perturbation applied to the
302
training set.
303"
MNIST,0.41735537190082644,"Table 2: Evolution of u-vector across 9600 corrupted data points and 38400 clean data points. Note
that 1/(9600 + 38400) = 2.08 · 10-5."
ITERATION,0.4214876033057851,"1. iteration
10. iteration
49. iteration
ui value
corrupted
data points"
ITERATION,0.4256198347107438,"clean
data
points"
ITERATION,0.4297520661157025,"corrupted
data points"
ITERATION,0.43388429752066116,"clean
data
points"
ITERATION,0.4380165289256198,"corrupted
data points"
ITERATION,0.44214876033057854,"clean
data
points
≫0
0
1
0
4.
0
25
≈0
8844
38385
2058
37524
91
35246
(-0.52, 0.00) · 10-5
0
0
7
36
146
1655
(-1.04, -0.52] · 10-5
0
0
41
45
43
155
(-1.56, -1.04] · 10-5
756
14
415
174
34
168
(-2.08, -1.56] · 10-5
0
0
7079
617
9286
1151"
TOXIC COMMENT,0.4462809917355372,"6.2
Toxic Comment
304"
TOXIC COMMENT,0.45041322314049587,"We use the Toxic Comment dataset to test the efﬁcacy of RRM on low prevalence text data. The
305
positive (toxic) comments consist of only 3% of the data and we corrupt anywhere from 1% to 20%
306
of the labels. There are a total of 148,000 samples, and we set aside 80% for training and 20% for
307
test. σ = 2 with 3 iterations of the heuristic algorithm results in a total of 6 epochs, and ERM is
308
run for a total of 6 epochs to make the results comparable. Since the data is highly imbalanced,
309
we look at the area under the curve of the precision/recall curve to assess the performance of the
310
models. Unsurprisingly, as the noise increase, the model performance decreases. We note that RRM
311
outperforms ERM across all noise levels tested, though as the noise increase, the gap between RRM
312
and ERM decreases.
313"
TOXIC COMMENT,0.45454545454545453,"Table 3: Comparison of training and test area under the precision/recall curve for ERM and RRM at
noise levels ranging from 1% to 20%."
TOXIC COMMENT,0.45867768595041325,"Method
Percentage Corrupted Training Data
1%
5%
7%
10%
15%
20%
ERM (train)
0.2904
0.2006
0.1589
0.1302
0.1073
0.0920
RRM (train)
0.6875
0.4458
0.3805
0.3087
0.2438
0.1966
ERM (test)
0.5861
0.3970
0.3246
0.2550
0.2013
0.1717
RRM (test)
0.6705
0.4338
0.3619
0.2824
0.2208
0.1861"
IMDB,0.4628099173553719,"6.3
IMDb
314"
IMDB,0.4669421487603306,"Twenty percent of the training data is set aside for validation purposes. Using Pytorch 2.1.0 [3],
315
30 iterations of RRM are executed, with σ = 10 epochs per iteration for a total of 300 epochs for
316
a given hyperparameter setting. For RRM, the hyperparameter settings of µ and γ at 0.5 and 0.4,
317
respectively, are based on a search to optimize validation set accuracy. For contrast, we perform
318
a comparable 300 epochs using ERM. Both ERM and RRM employ stochastic gradient descent
319
(SGD) with a learning rate (η) of 0.001. In Table 4 we record both the test set accuracy achieved
320
when validation set accuracy peaks, as well as the maximum test set accuracy. At these high levels
321
of corruption RRM consistently achieves a better maximum test set accuracy.
322"
IMDB,0.47107438016528924,"Table 4: Test accuracy (%) for ERM and RRM on IMDb under different levels of corruption. Test
set accuracy at peak validation accuracy and maximum test set accuracy are recorded."
IMDB,0.47520661157024796,"Method
Percentage Corrupted Training Data
25%
30%
40%
45%
ERM
90.2, 90.2
89.5, 89.6
86.4, 86.6
80.7, 81.1
RRM
90.1, 90.4
90.2, 90.4
88.4, 88.7
76.9, 82.6"
TISSUE NECROSIS,0.4793388429752066,"6.4
Tissue Necrosis
323"
TISSUE NECROSIS,0.4834710743801653,"Twenty percent of the training data is set aside for validation purposes, including hyperparameter
324
selection. 60 iterations of RRM are executed, with σ = 10 epochs per iteration, for a total of 600
325
epochs for a given hyperparameter setting. For RRM, the hyperparameter settings of µ and γ at
326
0.5 and 0.016, respectively, are based on a search to optimize validation set accuracy. For contrast,
327
we perform a comparable 600 epochs using ERM. Both ERM and RRM employ stochastic gradient
328
descent (SGD) with a learning rate (η) of 5.0 and 1.0, respectively. RRM achieves a test set accuracy
329
at peak validation accuracy of 74.6, and a maximum test set accuracy 77.2, whereas ERM achieves
330
71.7 and 73.2, respectively. RRM appears to confer a performance beneﬁt under this regime of
331
weakly labeled data.
332"
CONCLUSION,0.48760330578512395,"7
Conclusion
333"
CONCLUSION,0.49173553719008267,"In this study, we demonstrate the robustness of the A-RRM algorithm in a variety of data domains,
334
data corruption schemes, model architectures and machine learning applications. In the MNIST
335
example we show that conducting training in preparation for deployment environments with varied
336
levels of adversarial attacks, one can beneﬁt from implementation of the A-RRM algorithm. This
337
can lead to a model more robust across levels of both feature perturbation and high levels of label
338
corruption. We also demonstrate the mechanism by which A-RRM operates and confers superior
339
results: by automatically identifying and removing the corrupted training observations at training
340
time execution.
341"
CONCLUSION,0.49586776859504134,"The Toxic Comment example presents another challenging classiﬁcation problem, characterized by
342
a low prevalence target class amidst label noise. Our experiments demonstrate that as the amount of
343
label noise increases, standard methods become increasingly ineffective. However, RRM remains
344
reasonably robust under varying degrees of label corruption. Therefore, RRM could be a valuable
345
addition to the set of tools being developed to enhance the robustness of AI-based decision engines.
346"
CONCLUSION,0.5,"In the IMDb example we demonstrate that RRM can confer beneﬁts to the sentiment analysis classi-
347
ﬁcation task using pre-trained large models under conditions of high label corruption. The success
348
of ﬁne-tuning in LLMs depends, in large part, on access to high quality training examples. We have
349
shown that RRM can mitigate this need by allowing effective training in scenarios of high training
350
data corruption. As such, resource allocation dedicated to dataset curation may be lessened by the
351
usage of RRM.
352"
CONCLUSION,0.5041322314049587,"In the Tissue Necrosis example, we demonstrate that RRM also confers accuracy beneﬁts to the
353
necrosis identiﬁcation task provided weakly labeled WSIs. Again, RRM can mitigate the need for
354
expert-curated, detailed pathology annotations, which are costly and time-consuming to generate.
355"
REFERENCES,0.5082644628099173,"References
356"
REFERENCES,0.512396694214876,"[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
357
Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfel-
358
low, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz
359
Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore,
360
Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,
361
Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol
362
Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Ten-
363
sorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available
364
from tensorﬂow.org.
365"
REFERENCES,0.5165289256198347,"[2] Mohamed Amgad, Habiba Elfandy, Hagar Hussein, Lamees A Atteya, Mai A T Elsebaie,
366
Lamia S Abo Elnasr, Rokia A Sakr, Hazem S E Salem, Ahmed F Ismail, Anas M Saad,
367
Joumana Ahmed, Maha A T Elsebaie, Mustaﬁjur Rahman, Inas A Ruhban, Nada M Elgazar,
368
Yahya Alagha, Mohamed H Osman, Ahmed M Alhusseiny, Mariam M Khalaf, Abo-Alela F
369
Younes, Ali Abdulkarim, Duaa M Younes, Ahmed M Gadallah, Ahmad M Elkashash, Salma Y
370
Fala, Basma M Zaki, Jonathan Beezley, Deepak R Chittajallu, David Manthey, David A Gut-
371
man, and Lee A D Cooper. Structured crowdsourcing enables convolutional segmentation of
372
histology images. Bioinformatics, 35(18):3461–3467, 02 2019.
373"
REFERENCES,0.5206611570247934,"[3] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesen-
374
sky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia,
375
Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong,
376
Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch,
377
Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, CK Luk, Bert Maher,
378
Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Sarouﬁm, Marcos Yukio Siraichi, Helen
379
Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang,
380
Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, and Soumith
381
Chintala. PyTorch 2: Faster machine learning through dynamic python bytecode transforma-
382
tion and graph compilation. In 29th ACM International Conference on Architectural Support
383
for Programming Languages and Operating Systems, Volume 2 (ASPLOS ’24). ACM, 4 2024.
384"
REFERENCES,0.5247933884297521,"[4] Alejandro Baldominos, Yago Saez, and Pedro Isasi. A survey of handwritten character recog-
385
nition with mnist and emnist. Applied Sciences, 9(15), 2019.
386"
REFERENCES,0.5289256198347108,"[5] Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more
387
accurate neural networks by emphasizing high variance samples, 2018.
388"
REFERENCES,0.5330578512396694,"[6] cjadams, Jeffrey Sorensen, Julia Elliott, Lucas Dixon, Mark McDonald, nithum, and Will
389
Cukierski. Toxic comment classiﬁcation challenge, 2017.
390"
REFERENCES,0.5371900826446281,"[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
391
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
392
2018.
393"
REFERENCES,0.5413223140495868,"[8] Benoit Frenay and Michel Verleysen. Classiﬁcation in the presence of label noise: A survey.
394
IEEE Transactions on Neural Networks and Learning Systems, 25(5):845–869, 2014.
395"
REFERENCES,0.5454545454545454,"[9] Rui Gao and Anton Kleywegt. Distributionally robust stochastic optimization with wasserstein
396
distance. Mathematics of Operations Research, 48(2):603–655, 2023.
397"
REFERENCES,0.5495867768595041,"[10] Aritra Ghosh, Himanshu Kumar, and P. S. Sastry. Robust loss functions under label noise
398
for deep neural networks. In Proceedings of the Thirty-First AAAI Conference on Artiﬁcial
399
Intelligence, AAAI’17, page 1919–1925. AAAI Press, 2017.
400"
REFERENCES,0.5537190082644629,"[11] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adver-
401
sarial examples. In 3rd International Conference on Learning Representations, 2015.
402"
REFERENCES,0.5578512396694215,"[12] Satoshi Hara, Atsushi Nitanda, and Takanori Maehara. Data cleansing for models trained with
403
SGD. Curran Associates Inc., Red Hook, NY, USA, 2019.
404"
REFERENCES,0.5619834710743802,"[13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
405
Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In In-
406
ternational Conference on Learning Representations, 2022.
407"
REFERENCES,0.5661157024793388,"[14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
408
by reducing internal covariate shift. In International conference on machine learning, pages
409
448–456. pmlr, 2015.
410"
REFERENCES,0.5702479338842975,"[15] Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander Toshev, Tom
411
Duerig, James Philbin, and Li Fei-Fei. The unreasonable effectiveness of noisy data for ﬁne-
412
grained recognition. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam,
413
The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 301–320. Springer,
414
2016.
415"
REFERENCES,0.5743801652892562,"[16] Anders Krogh and John Hertz. A simple weight decay can improve generalization. Advances
416
in neural information processing systems, 4, 1991.
417"
REFERENCES,0.5785123966942148,"[17] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.
418"
REFERENCES,0.5826446280991735,"[18] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christo-
419
pher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual
420
Meeting of the Association for Computational Linguistics: Human Language Technologies,
421
pages 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguis-
422
tics.
423"
REFERENCES,0.5867768595041323,"[19] Viet Anh Nguyen, Soroosh Shaﬁeezadeh Abadeh, Man-Chung Yue, Daniel Kuhn, and Wol-
424
fram Wiesemann. Optimistic distributionally robust optimization for nonparametric likelihood
425
approximation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
426
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran
427
Associates, Inc., 2019.
428"
REFERENCES,0.5909090909090909,"[20] Guy Nir, Soheil Hor, Davood Karimi, Ladan Fazli, Brian F. Skinnider, Peyman Tavassoli,
429
Dmitry Turbin, Carlos F. Villamil, Gang Wang, R. Storey Wilson, Kenneth A. Iczkowski,
430
M. Scott Lucia, Peter C. Black, Purang Abolmaesumi, S. Larry Goldenberg, and Septimiu E.
431
Salcudean. Automatic grading of prostate cancer in digitized histopathology images: Learning
432
from multiple experts. Medical Image Analysis, 50:167–180, 2018.
433"
REFERENCES,0.5950413223140496,"[21] Dominika Petríková and Ivan Cimrák. Survey of recent deep neural networks with strong
434
annotated supervision in histopathology. Computation, 11(4), 2023.
435"
REFERENCES,0.5991735537190083,"[22] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples
436
for robust deep learning. In International Conference on Machine Learning, 2018.
437"
REFERENCES,0.6033057851239669,"[23] Johannes O. Royset, Louis L. Chen, and Eric Eckstrand. Rockafellian relaxation and stochastic
438
optimization under perturbations. Mathematics of Operations Research (to appear), 2023.
439"
REFERENCES,0.6074380165289256,"[24] Johannes O. Royset and Roger J-B Wets. An Optimization Primer. Springer, 2021.
440"
REFERENCES,0.6115702479338843,"[25] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
Distilbert, a distilled
441
version of bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108, 2019.
442"
REFERENCES,0.6157024793388429,"[26] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep
443
learning. Journal of big data, 6(1):1–48, 2019.
444"
REFERENCES,0.6198347107438017,"[27] Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. Cheap and fast – but is it
445
good? evaluating non-expert annotations for natural language tasks. In Mirella Lapata and
446
Hwee Tou Ng, editors, Proceedings of the 2008 Conference on Empirical Methods in Natu-
447
ral Language Processing, pages 254–263, Honolulu, Hawaii, October 2008. Association for
448
Computational Linguistics.
449"
REFERENCES,0.6239669421487604,"[28] Hwanjun Song, Minseok Kim, Dongmin Park, and Jae-Gil Lee. Learning from noisy labels
450
with deep neural networks: A survey. CoRR, abs/2007.08199, 2020.
451"
REFERENCES,0.628099173553719,"[29] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
452
nov.
Dropout: a simple way to prevent neural networks from overﬁtting.
The journal of
453
machine learning research, 15(1):1929–1958, 2014.
454"
REFERENCES,0.6322314049586777,"[30] Matthew Staib and Stefanie Jegelka. Distributionally robust deep learning as a generaliza-
455
tion of adversarial training. In NIPS workshop on Machine Learning and Computer Security,
456
volume 3, page 4, 2017.
457"
REFERENCES,0.6363636363636364,"[31] V. Vapnik. Principles of risk minimization for learning theory. In Proceedings of the 4th
458
International Conference on Neural Information Processing Systems, NIPS’91, page 831–838,
459
San Francisco, CA, USA, 1991. Morgan Kaufmann Publishers Inc.
460"
REFERENCES,0.640495867768595,"[32] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data
461
augmentation for consistency training. arXiv preprint arXiv:1904.12848, 2019.
462"
REFERENCES,0.6446280991735537,"[33] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
463
ing deep learning requires rethinking generalization. In International Conference on Learning
464
Representations, 2017.
465"
REFERENCES,0.6487603305785123,"[34] HaiYang Zhang, XiMing Xing, and Liang Liu. Dualgraph: A graph-based method for rea-
466
soning about label noise. In 2021 IEEE/CVF Conference on Computer Vision and Pattern
467
Recognition (CVPR), pages 9649–9658, 2021.
468"
REFERENCES,0.6528925619834711,"[35] Zhilu Zhang and Mert R. Sabuncu. Generalized cross entropy loss for training deep neural
469
networks with noisy labels. In Proceedings of the 32nd International Conference on Neural In-
470
formation Processing Systems, NIPS’18, page 8792–8802, Red Hook, NY, USA, 2018. Curran
471
Associates Inc.
472"
REFERENCES,0.6570247933884298,"[36] Xingquan Zhu and Xindong Wu. Class noise vs. attribute noise: A quantitative study. Artiﬁcial
473
intelligence review, 22:177–210, 2004.
474"
REFERENCES,0.6611570247933884,"A
Appendix / supplemental material
475"
REFERENCES,0.6652892561983471,"A.1
Section 3 Proofs
476"
REFERENCES,0.6694214876033058,"Theorem 3.1. Let γ > 0 and c = (c1, . . . , cN) ∈RN, with cmin := mini ci, and cmax := maxi ci.
477
Write Imin := {i : ci = cmin}, Ibig := {i : ci = cmin + 2γ}, and for any S1 ⊆Imin, S2 ⊆Ibig,
478"
REFERENCES,0.6735537190082644,"deﬁne the polytope U ∗
S1,S2 :="
REFERENCES,0.6776859504132231,"



"
REFERENCES,0.6818181818181818,"


"
REFERENCES,0.6859504132231405,"u∗
i ≥0 ∀i : ci = cmin
u∗
i = 0 ∀i : ci ∈(cmin, cmin + 2γ)
u∗∈U :
u∗
i = −1"
REFERENCES,0.6900826446280992,"N ∀i ∈Ibig \ S2
u∗
i = −1"
REFERENCES,0.6942148760330579,"N ∀i : ci > cmin + 2γ
u∗
i = 0 ∀i ∈S1 ∪S2"
REFERENCES,0.6983471074380165,"



"
REFERENCES,0.7024793388429752,"


"
REFERENCES,0.7066115702479339,". Then
479"
REFERENCES,0.7107438016528925,"conv
 
∪S1,S2U ∗
S1,S2

= arg min
u∈U N
X"
REFERENCES,0.7148760330578512,"i=1
( 1"
REFERENCES,0.71900826446281,"N + ui) · ci + γ∥u∥1.
(4)"
REFERENCES,0.7231404958677686,"Proof. For any set C, let ιC(x) = 0 and ιC(x) = ∞otherwise. We recognize that u⋆is a solution
480
of the minimization problem if and only if it is a minimizer of the function h given by
481"
REFERENCES,0.7272727272727273,"h(u) = N
X i=1"
REFERENCES,0.731404958677686,"
ci/N + uici + γ|ui| + ι[0,∞)(1/N + ui)

+ ι{0}
 N
X"
REFERENCES,0.7355371900826446,"i=1
ui
"
REFERENCES,0.7396694214876033,"Thus, because h(u) > −∞for all u ∈RN and h is convex, u⋆is a solution of the minimization
482
problem if and only if 0 ∈∂h(u⋆) by Theorem 2.19 in [24]. We proceed by characterizing ∂h.
483"
REFERENCES,0.743801652892562,"Consider the univariate function hi given by
484"
REFERENCES,0.7479338842975206,"hi(ui) = ci/N + uici + γ|ui| + ι[0,∞)(1/N + ui)."
REFERENCES,0.7520661157024794,"For ui ≥−1/N, the Moreau-Rockafellar sum rule (see, e.g, [24, Theorem 2.26]) gives that
485"
REFERENCES,0.756198347107438,∂hi(ui) = ci +
REFERENCES,0.7603305785123967,"


 

"
REFERENCES,0.7644628099173554,"{γ}
if ui > 0
[−γ, γ]
if ui = 0
{−γ}
if −1/N < ui < 0
(−∞, −γ]
if ui = −1/N."
REFERENCES,0.768595041322314,"For u = (u1, . . . , uN) ∈[−1/N, ∞)N, we obtain by Proposition 4.63 in [24] that
486"
REFERENCES,0.7727272727272727,"∂
 N
X"
REFERENCES,0.7768595041322314,"i=1
hi

(u) = ∂h1(u1) × · · · × ∂hN(uN)."
REFERENCES,0.78099173553719,"Let h0 be the function given by h0(u) = ι{0}(PN
i=1 ui). Again invoking the Moreau-Rockafellar
487
sum rule while recognizing that the interior of the domain of PN
i=1 hi intersects with the domain of
488
h0, we obtain
489"
REFERENCES,0.7851239669421488,"∂h(u) = ∂
 N
X"
REFERENCES,0.7892561983471075,"i=1
hi

(u) + ∂h0(u) = ∂h1(u1) × · · · × ∂hN(uN) +  "
REFERENCES,0.7933884297520661,"1
...
1  R"
REFERENCES,0.7975206611570248,"for any u = (u1, . . . , uN) with ui ≥−1/N, i = 1, . . . , N, and PN
i=1 ui = 0. Hence, u∗∈U is
490
optimal if and only if for some λ ∈R,
491 λ ∈"
REFERENCES,0.8016528925619835,"


 

"
REFERENCES,0.8057851239669421,"{ci + γ}
if u⋆
i > 0
[ci −γ, ci + γ]
if u⋆
i = 0
{ci −γ}
if u⋆
i ∈(−1/N, 0)
(−∞, ci −γ]
if u⋆
i = −1/N."
REFERENCES,0.8099173553719008,"It follows that λ = cmin + γ can accompany any optimal u∗in satisfying the above; hence, the
492
result follows.
493 494"
REFERENCES,0.8140495867768595,"Proposition A.1. Let ǫ > 0, and suppose for any θ, max(x,y)∈Z |J(θ; x, y)| < ∞. Then there exists
495
κ ≥0 such that for any θ, the following problem
496"
REFERENCES,0.8181818181818182,"vMIX
N
(θ) :=
min
u1,...,uN N
X"
REFERENCES,0.8223140495867769,"i=1
( 1"
REFERENCES,0.8264462809917356,"N + ui) · J(θ; xi, yi) + γθ N
X"
REFERENCES,0.8305785123966942,"i=1
|ui|"
REFERENCES,0.8347107438016529,s.t. ui + 1
REFERENCES,0.8388429752066116,"N ≥0 i = 1, . . . , N"
REFERENCES,0.8429752066115702,satisﬁes vN(θ) + κ
REFERENCES,0.8471074380165289,"N ≥vMIX
N
(θ) ≥vN(θ).
497"
REFERENCES,0.8512396694214877,"In particular, −γθ ≤mini J(θ; xi, yi), and {i : J(θ; xi, yi) > γθ} are all down-weighted to zero,
498
i.e., u∗
i = −1"
REFERENCES,0.8553719008264463,"N for any u∗solving vMIX
N
(θ).
499"
REFERENCES,0.859504132231405,"Proof. Fix θ. Then for any z = (x, y) ∈Z, the function ℓ(·, z, θ) is linear, and hence Lipschitz with
500
constant ℓ(1, z, θ) = J(θ; x, y) ≤max(x,y)∈Z |J(θ; x, y)| < ∞.
501"
REFERENCES,0.8636363636363636,"By Lemma 3.1 of [30] and/or Corollary 2 of [9],
502"
REFERENCES,0.8677685950413223,"vMIX
N
(θ) :=
min
˜
w1,..., ˜
wN≥0
1
N N
X"
REFERENCES,0.871900826446281,"i=1
ℓ( ˜wi, zi; θ)"
REFERENCES,0.8760330578512396,"s.t.
1
N N
X"
REFERENCES,0.8801652892561983,"i=1
| ˜wi −wi| ≤ǫ"
REFERENCES,0.8842975206611571,"provides the stated approximation of v(θ).
503"
REFERENCES,0.8884297520661157,"Upon introducing the change of variable ui =
˜
wi N −1"
REFERENCES,0.8925619834710744,"N , and applying a Lagrange multiplier γθ to
504
the ǫ−budget constraint (any convex dual optimal multiplier), we recover
505"
REFERENCES,0.8966942148760331,"min
u1,...,uN N
X"
REFERENCES,0.9008264462809917,"i=1
ℓ(ui + 1"
REFERENCES,0.9049586776859504,"N , zi; θ) + γθ N
X"
REFERENCES,0.9090909090909091,"i=1
|ui|"
REFERENCES,0.9132231404958677,s.t. ui + 1
REFERENCES,0.9173553719008265,"N ≥0 i = 1, . . . , N 506"
REFERENCES,0.9214876033057852,"NeurIPS Paper Checklist
507"
CLAIMS,0.9256198347107438,"1. Claims
508
Question: Do the main claims made in the abstract and introduction accurately reﬂect the
509
paper’s contributions and scope?
510
Answer: [Yes]
511
Justiﬁcation: Sections 6.1, 6.2, 6.3, 6.4
512
Guidelines:
513
• The answer NA means that the abstract and introduction do not include the claims
514
made in the paper.
515
• The abstract and/or introduction should clearly state the claims made, including the
516
contributions made in the paper and important assumptions and limitations. A No or
517
NA answer to this question will not be perceived well by the reviewers.
518
• The claims made should match theoretical and experimental results, and reﬂect how
519
much the results can be expected to generalize to other settings.
520
• It is ﬁne to include aspirational goals as motivation as long as it is clear that these
521
goals are not attained by the paper.
522
2. Limitations
523
Question: Does the paper discuss the limitations of the work performed by the authors?
524
Answer: [Yes]
525
Justiﬁcation: The paper has focused more on label corruption, rather than feature perturba-
526
tion settings.
527
Guidelines:
528
• The answer NA means that the paper has no limitation while the answer No means
529
that the paper has limitations, but those are not discussed in the paper.
530
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
531
• The paper should point out any strong assumptions and how robust the results are to
532
violations of these assumptions (e.g., independence assumptions, noiseless settings,
533
model well-speciﬁcation, asymptotic approximations only holding locally). The au-
534
thors should reﬂect on how these assumptions might be violated in practice and what
535
the implications would be.
536
• The authors should reﬂect on the scope of the claims made, e.g., if the approach was
537
only tested on a few datasets or with a few runs. In general, empirical results often
538
depend on implicit assumptions, which should be articulated.
539
• The authors should reﬂect on the factors that inﬂuence the performance of the ap-
540
proach. For example, a facial recognition algorithm may perform poorly when image
541
resolution is low or images are taken in low lighting. Or a speech-to-text system might
542
not be used reliably to provide closed captions for online lectures because it fails to
543
handle technical jargon.
544
• The authors should discuss the computational efﬁciency of the proposed algorithms
545
and how they scale with dataset size.
546
• If applicable, the authors should discuss possible limitations of their approach to ad-
547
dress problems of privacy and fairness.
548
• While the authors might fear that complete honesty about limitations might be used by
549
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
550
limitations that aren’t acknowledged in the paper. The authors should use their best
551
judgment and recognize that individual actions in favor of transparency play an impor-
552
tant role in developing norms that preserve the integrity of the community. Reviewers
553
will be speciﬁcally instructed to not penalize honesty concerning limitations.
554
3. Theory Assumptions and Proofs
555
Question: For each theoretical result, does the paper provide the full set of assumptions and
556
a complete (and correct) proof?
557
Answer: [Yes]
558"
CLAIMS,0.9297520661157025,"Justiﬁcation: Section 3
559
Guidelines:
560
• The answer NA means that the paper does not include theoretical results.
561
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
562
referenced.
563
• All assumptions should be clearly stated or referenced in the statement of any theo-
564
rems.
565
• The proofs can either appear in the main paper or the supplemental material, but if
566
they appear in the supplemental material, the authors are encouraged to provide a
567
short proof sketch to provide intuition.
568
• Inversely, any informal proof provided in the core of the paper should be comple-
569
mented by formal proofs provided in appendix or supplemental material.
570
• Theorems and Lemmas that the proof relies upon should be properly referenced.
571
4. Experimental Result Reproducibility
572
Question: Does the paper fully disclose all the information needed to reproduce the main
573
experimental results of the paper to the extent that it affects the main claims and/or conclu-
574
sions of the paper (regardless of whether the code and data are provided or not)?
575
Answer: [Yes]
576
Justiﬁcation: Sections 3.2, 4, 5, 6
577
Guidelines:
578
• The answer NA means that the paper does not include experiments.
579
• If the paper includes experiments, a No answer to this question will not be perceived
580
well by the reviewers: Making the paper reproducible is important, regardless of
581
whether the code and data are provided or not.
582
• If the contribution is a dataset and/or model, the authors should describe the steps
583
taken to make their results reproducible or veriﬁable.
584
• Depending on the contribution, reproducibility can be accomplished in various ways.
585
For example, if the contribution is a novel architecture, describing the architecture
586
fully might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation,
587
it may be necessary to either make it possible for others to replicate the model with
588
the same dataset, or provide access to the model. In general. releasing code and data
589
is often one good way to accomplish this, but reproducibility can also be provided via
590
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
591
the case of a large language model), releasing of a model checkpoint, or other means
592
that are appropriate to the research performed.
593
• While NeurIPS does not require releasing code, the conference does require all sub-
594
missions to provide some reasonable avenue for reproducibility, which may depend
595
on the nature of the contribution. For example
596
(a) If the contribution is primarily a new algorithm, the paper should make it clear
597
how to reproduce that algorithm.
598
(b) If the contribution is primarily a new model architecture, the paper should describe
599
the architecture clearly and fully.
600
(c) If the contribution is a new model (e.g., a large language model), then there should
601
either be a way to access this model for reproducing the results or a way to re-
602
produce the model (e.g., with an open-source dataset or instructions for how to
603
construct the dataset).
604
(d) We recognize that reproducibility may be tricky in some cases, in which case au-
605
thors are welcome to describe the particular way they provide for reproducibility.
606
In the case of closed-source models, it may be that access to the model is limited in
607
some way (e.g., to registered users), but it should be possible for other researchers
608
to have some path to reproducing or verifying the results.
609
5. Open access to data and code
610
Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
611
tions to faithfully reproduce the main experimental results, as described in supplemental
612
material?
613"
CLAIMS,0.9338842975206612,"Answer: [No]
614
Justiﬁcation: The datasets are open-source, and the code will be made available pending
615
conference review of this work
616
Guidelines:
617
• The answer NA means that paper does not include experiments requiring code.
618
• Please
see
the
NeurIPS
code
and
data
submission
guidelines
619
(https://nips.cc/public/guides/CodeSubmissionPolicy)
for
more
de-
620
tails.
621
• While we encourage the release of code and data, we understand that this might not
622
be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
623
including code, unless this is central to the contribution (e.g., for a new open-source
624
benchmark).
625
• The instructions should contain the exact command and environment needed to run
626
to reproduce the results.
See the NeurIPS code and data submission guidelines
627
(https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
628
• The authors should provide instructions on data access and preparation, including how
629
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
630
• The authors should provide scripts to reproduce all experimental results for the new
631
proposed method and baselines. If only a subset of experiments are reproducible, they
632
should state which ones are omitted from the script and why.
633
• At submission time, to preserve anonymity, the authors should release anonymized
634
versions (if applicable).
635
• Providing as much information as possible in supplemental material (appended to the
636
paper) is recommended, but including URLs to data and code is permitted.
637
6. Experimental Setting/Details
638
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
639
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
640
results?
641
Answer: [Yes]
642
Justiﬁcation: Sections 4, 5, 6
643
Guidelines:
644
• The answer NA means that the paper does not include experiments.
645
• The experimental setting should be presented in the core of the paper to a level of
646
detail that is necessary to appreciate the results and make sense of them.
647
• The full details can be provided either with the code, in appendix, or as supplemental
648
material.
649
7. Experiment Statistical Signiﬁcance
650
Question: Does the paper report error bars suitably and correctly deﬁned or other appropri-
651
ate information about the statistical signiﬁcance of the experiments?
652
Answer: [No]
653
Justiﬁcation: Error bars are not reported because it would be too computationally expen-
654
sive.
655
Guidelines:
656
• The answer NA means that the paper does not include experiments.
657
• The authors should answer ""Yes"" if the results are accompanied by error bars, conﬁ-
658
dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
659
the main claims of the paper.
660
• The factors of variability that the error bars are capturing should be clearly stated (for
661
example, train/test split, initialization, random drawing of some parameter, or overall
662
run with given experimental conditions).
663
• The method for calculating the error bars should be explained (closed form formula,
664
call to a library function, bootstrap, etc.)
665"
CLAIMS,0.9380165289256198,"• The assumptions made should be given (e.g., Normally distributed errors).
666
• It should be clear whether the error bar is the standard deviation or the standard error
667
of the mean.
668
• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
669
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
670
Normality of errors is not veriﬁed.
671
• For asymmetric distributions, the authors should be careful not to show in tables or
672
ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
673
error rates).
674
• If error bars are reported in tables or plots, The authors should explain in the text how
675
they were calculated and reference the corresponding ﬁgures or tables in the text.
676
8. Experiments Compute Resources
677
Question: For each experiment, does the paper provide sufﬁcient information on the com-
678
puter resources (type of compute workers, memory, time of execution) needed to reproduce
679
the experiments?
680
Answer: [Yes]
681
Justiﬁcation: See section 6
682
Guidelines:
683
• The answer NA means that the paper does not include experiments.
684
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
685
or cloud provider, including relevant memory and storage.
686
• The paper should provide the amount of compute required for each of the individual
687
experimental runs as well as estimate the total compute.
688
• The paper should disclose whether the full research project required more compute
689
than the experiments reported in the paper (e.g., preliminary or failed experiments
690
that didn’t make it into the paper).
691
9. Code Of Ethics
692
Question: Does the research conducted in the paper conform, in every respect, with the
693
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
694
Answer: [Yes]
695
Justiﬁcation: The paper conforms with the NeurIPS Code of Ethics
696
Guidelines:
697
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
698
• If the authors answer No, they should explain the special circumstances that require a
699
deviation from the Code of Ethics.
700
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
701
eration due to laws or regulations in their jurisdiction).
702
10. Broader Impacts
703
Question: Does the paper discuss both potential positive societal impacts and negative
704
societal impacts of the work performed?
705
Answer: [No]
706
Justiﬁcation: The work in the paper is foundational research and is not tied to a particular
707
application or deployment.
708
Guidelines:
709
• The answer NA means that there is no societal impact of the work performed.
710
• If the authors answer NA or No, they should explain why their work has no societal
711
impact or why the paper does not address societal impact.
712
• Examples of negative societal impacts include potential malicious or unintended uses
713
(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
714
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
715
ciﬁc groups), privacy considerations, and security considerations.
716"
CLAIMS,0.9421487603305785,"• The conference expects that many papers will be foundational research and not tied
717
to particular applications, let alone deployments. However, if there is a direct path to
718
any negative applications, the authors should point it out. For example, it is legitimate
719
to point out that an improvement in the quality of generative models could be used to
720
generate deepfakes for disinformation. On the other hand, it is not needed to point out
721
that a generic algorithm for optimizing neural networks could enable people to train
722
models that generate Deepfakes faster.
723
• The authors should consider possible harms that could arise when the technology is
724
being used as intended and functioning correctly, harms that could arise when the
725
technology is being used as intended but gives incorrect results, and harms following
726
from (intentional or unintentional) misuse of the technology.
727
• If there are negative societal impacts, the authors could also discuss possible mitiga-
728
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
729
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
730
feedback over time, improving the efﬁciency and accessibility of ML).
731"
SAFEGUARDS,0.9462809917355371,"11. Safeguards
732"
SAFEGUARDS,0.9504132231404959,"Question: Does the paper describe safeguards that have been put in place for responsible
733
release of data or models that have a high risk for misuse (e.g., pretrained language models,
734
image generators, or scraped datasets)?
735"
SAFEGUARDS,0.9545454545454546,"Answer: [No]
736"
SAFEGUARDS,0.9586776859504132,"Justiﬁcation: No models are released as part of this work, and the datasets are publicly
737
available.
738"
SAFEGUARDS,0.9628099173553719,"Guidelines:
739"
SAFEGUARDS,0.9669421487603306,"• The answer NA means that the paper poses no such risks.
740
• Released models that have a high risk for misuse or dual-use should be released with
741
necessary safeguards to allow for controlled use of the model, for example by re-
742
quiring that users adhere to usage guidelines or restrictions to access the model or
743
implementing safety ﬁlters.
744
• Datasets that have been scraped from the Internet could pose safety risks. The authors
745
should describe how they avoided releasing unsafe images.
746
• We recognize that providing effective safeguards is challenging, and many papers do
747
not require this, but we encourage authors to take this into account and make a best
748
faith effort.
749"
LICENSES FOR EXISTING ASSETS,0.9710743801652892,"12. Licenses for existing assets
750"
LICENSES FOR EXISTING ASSETS,0.9752066115702479,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
751
the paper, properly credited and are the license and terms of use explicitly mentioned and
752
properly respected?
753"
LICENSES FOR EXISTING ASSETS,0.9793388429752066,"Answer: [Yes]
754"
LICENSES FOR EXISTING ASSETS,0.9834710743801653,"Justiﬁcation: Citations for publicly available datasets and code are provided.
755"
LICENSES FOR EXISTING ASSETS,0.987603305785124,"Guidelines:
756"
LICENSES FOR EXISTING ASSETS,0.9917355371900827,"• The answer NA means that the paper does not use existing assets.
757
• The authors should cite the original paper that produced the code package or dataset.
758
• The authors should state which version of the asset is used and, if possible, include a
759
URL.
760
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
761
• For scraped data from a particular source (e.g., website), the copyright and terms of
762
service of that source should be provided.
763
• If assets are released, the license, copyright information, and terms of use in the pack-
764
age should be provided. For popular datasets, paperswithcode.com/datasets has
765
curated licenses for some datasets. Their licensing guide can help determine the li-
766
cense of a dataset.
767
• For existing datasets that are re-packaged, both the original license and the license of
768
the derived asset (if it has changed) should be provided.
769"
LICENSES FOR EXISTING ASSETS,0.9958677685950413,"• If this information is not available online, the authors are encouraged to reach out to
770
the asset’s creators.
771
13. New Assets
772
Question: Are new assets introduced in the paper well documented and is the documenta-
773
tion provided alongside the assets?
774
Answer: [No]
775
Justiﬁcation: No new assets are introduced in the paper.
776
Guidelines:
777
• The answer NA means that the paper does not release new assets.
778
• Researchers should communicate the details of the dataset/code/model as part of their
779
submissions via structured templates. This includes details about training, license,
780
limitations, etc.
781
• The paper should discuss whether and how consent was obtained from people whose
782
asset is used.
783
• At submission time, remember to anonymize your assets (if applicable). You can
784
either create an anonymized URL or include an anonymized zip ﬁle.
785
14. Crowdsourcing and Research with Human Subjects
786
Question: For crowdsourcing experiments and research with human subjects, does the pa-
787
per include the full text of instructions given to participants and screenshots, if applicable,
788
as well as details about compensation (if any)?
789
Answer: [NA]
790
Justiﬁcation: No crowdsourcing experiments or research with human subjects was con-
791
ducted.
792
Guidelines:
793
• The answer NA means that the paper does not involve crowdsourcing nor research
794
with human subjects.
795
• Including this information in the supplemental material is ﬁne, but if the main contri-
796
bution of the paper involves human subjects, then as much detail as possible should
797
be included in the main paper.
798
• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
799
tion, or other labor should be paid at least the minimum wage in the country of the
800
data collector.
801
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
802
Subjects
803
Question: Does the paper describe potential risks incurred by study participants, whether
804
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
805
approvals (or an equivalent approval/review based on the requirements of your country or
806
institution) were obtained?
807
Answer: [NA]
808
Justiﬁcation: The paper does not involve research with human subjects.
809
Guidelines:
810
• The answer NA means that the paper does not involve crowdsourcing nor research
811
with human subjects.
812
• Depending on the country in which research is conducted, IRB approval (or equiva-
813
lent) may be required for any human subjects research. If you obtained IRB approval,
814
you should clearly state this in the paper.
815
• We recognize that the procedures for this may vary signiﬁcantly between institutions
816
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
817
guidelines for their institution.
818
• For initial submissions, do not include any information that would break anonymity
819
(if applicable), such as the institution conducting the review.
820"
