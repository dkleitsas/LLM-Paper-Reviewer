Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0072992700729927005,"Automatic segmentation of anatomical structures in laparoscopic images or videos is an im-
portant prerequisite for visual assistance tools which are designed to increase efficiency and
safety during an intervention. In order to be used in a realistic clinical scenario, both high
accuracy and real-time capability are required. Current deep learning networks for anatomy
segmentation show high accuracy, but are not suitable for real-time clinical application due
to their large size. As smaller, real-time capable deep learning networks show lower segmen-
tation performance, we propose a multi-teacher knowledge distillation approach applicable
to partially labeled datasets.
We leverage the knowledge of multiple anatomy-specific,
high-accuracy teacher networks to improve the segmentation performance of a single and
efficient student network capable of segmenting multiple anatomies simultaneously. To do
so, we minimize the Kullback-Leibler divergence between the normalized anatomy-specific
teacher logits and the respective normalized logits of the student. We conduct experiments
on the Dresden Surgical Anatomy Dataset, which provides multiple subsets of binary seg-
mented anatomical structures. Results show that our approach can increase the overall
Dice score for different real-time capable network architectures for anatomy segmentation.
Keywords: Anatomy Segmentation, Real-Time, Surgical Computer Vision, Knowledge
Distillation"
INTRODUCTION,0.014598540145985401,1. Introduction
INTRODUCTION,0.021897810218978103,"Postoperative complications remain a major problem for both the healthcare system and
the individual patient and are associated with higher healthcare costs and poorer outcomes
(Dencker et al., 2021). An important factor in decreasing postoperative complications is the
reduction of technical errors, which are defined as adverse events directly related to manual
errors of the surgeon (Fecso et al., 2017; Suliburk et al., 2019).
The increasing adoption of minimally invasive procedures, which rely on the visualization by
endoscopic cameras, leads to an increasing amount of available surgical video data. This en-
ables the data-driven analysis of surgical video data using computer vision techniques, such
as deep learning, to visually assist surgeons (Mascagni et al., 2022). An essential prerequi-
site for computer vision-based surgical assistance applications is the accurate and real-time"
INTRODUCTION,0.029197080291970802,"perception of the intraoperative environment, e.g. by segmenting the surgical instruments
and anatomical structures. To evaluate the accuracy of computer vision-based methods for
anatomy segmentation, various data sets with pixel-wise annotations of anatomical struc-
tures were introduced (Bamba et al., 2021; Madani et al., 2020; Allan et al., 2020). As
these data sets have little diversity due to their size or are based solely on simpler porcine
tissue data, the applicability of deep learning models trained with such data sets in a real
clinical setting is limited.
Therefore, Carstens et al.
(Carstens et al., 2023) published
the largest public data set of laparoscopic images to date, namely the Dresden Surgical
Anatomy Dataset (DSAD). It is divided into partially annotated sub-datasets, containing
overall 13, 195 laparoscopic images with pixel-wise annotations of eleven anatomical struc-
tures. As only binary annotations for one anatomy are provided in a sub-dataset, although
several other anatomical structures without annotation are visible, complete information
about the background remains unknown. To tackle the problem of training networks with
partially labeled datasets, the usage of annotation adaptive loss functions has been proposed
(Vu et al., 2021; Ulrich et al., 2023). In their recent work, Kolbinger et al. (Kolbinger et al.,
2023) trained a combined network with a shared encoder and multiple decoders for each of
the sub-datasets in the DSAD. In addition, they made use of mutual-exclusivity by incor-
porating the information of a positive annotation of one class as a negative annotation for
all other classes. Despite achieving segmentation accuracy comparable to human experts,
there are two limitations. First, the binary segmentation accuracy of the combined network
is inferior compared to anatomy-specific single-encoder, single-decoder networks (Kolbinger
et al., 2023). Second, due to the large size of the encoder and decoder networks, achieving
a sufficiently high frame rate for delay-free segmentation of anatomical structures on dedi-
cated hardware in the operating room can not be guaranteed. Methods for real-time capable
segmentation in surgical videos include the development of lightweight convolution-based
architectures for faster segmentation of surgical instruments in laparoscopy (Tomasini et al.,
2022; Pakhomov and Navab, 2020; Jha et al., 2021). While networks with a low amount
of parameters can provide good segmentation performance for simpler tasks such as surgi-
cal instrument segmentation, they are ineffective at learning complex features required for
accurate segmentation of anatomies in surgical videos. Knowledge Distillation has drawn
attention to overcome the dilemma of decreasing performance when using smaller models
capable of faster inference speed. Xie et al. (Xie et al., 2018) transfer the zero- and first-
order knowledge from a strong teacher network to guide the fast student network. Qin et al.
(Qin et al., 2021) improve the segmentation performance on public CT datasets by propos-
ing a novel module that encodes regional knowledge for a student network. To overcome the
dependency of student networks on a single teacher network, Amirkhani et al. (Amirkhani
et al., 2021) include multiple teacher networks trained with the same input but different
style transfers and data augmentations.
In this work, we propose a multi-teacher knowledge distillation (MT-KD) approach that
leverages the knowledge of multiple anatomy-specific, high-accuracy teacher networks to
tackle the problem of training a single network with partially labeled datasets. Specifically,
we use MT-KD to improve the segmentation performance of a real-time capable student
network with a small number of parameters.
In a first step, multiple teacher networks
are trained to obtain high anatomy-specific accuracy.
In a second step, the Kullback-
Leibler (KL) divergence is minimized between the normalized output logits of the individ-"
INTRODUCTION,0.0364963503649635,Efficient Anatomy Segmentation with KD
INTRODUCTION,0.043795620437956206,"ual anatomy-specific teacher models and the normalized output logits of the corresponding
anatomy-specific decoder of the student model. This way, the teacher networks guide the
student to pay more attention to the most salient regions in order to accurately segment
the anatomies. By improving the segmentation accuracy of small student networks, capa-
ble of segmenting multiple anatomies simultaneously, we aim to increase the applicability
of computer vision methods in realistic surgical scenarios. The comprehensive evaluation
of our MT-KD approach shows increased segmentation performance across various network
architectures with a small number of parameters, as shown in Figure 1."
INTRODUCTION,0.051094890510948905,"100
101
102
103"
INTRODUCTION,0.058394160583941604,Number of Parameters (Million) 35 45 55 65 70
INTRODUCTION,0.06569343065693431,Dice Score
INTRODUCTION,0.072992700729927,MiniNetv2
INTRODUCTION,0.08029197080291971,DeepLabV3/ResNet18
INTRODUCTION,0.08759124087591241,"DeepLabV3/
ResNet50
SegFormer-B0"
INTRODUCTION,0.0948905109489051,SegFormer-B3
INTRODUCTION,0.10218978102189781,MiniNetv2
INTRODUCTION,0.10948905109489052,"DeepLabV3
/ResNet18"
INTRODUCTION,0.11678832116788321,"DeepLabV3/
ResNet50"
INTRODUCTION,0.12408759124087591,SegFormer-B0
INTRODUCTION,0.13138686131386862,SegFormer-B3
INTRODUCTION,0.1386861313868613,MiniNetv2
INTRODUCTION,0.145985401459854,"DeepLabv3/
ResNet18"
INTRODUCTION,0.15328467153284672,SegFormer-B0
INTRODUCTION,0.16058394160583941,"Figure 1: Performance of different segmentation networks presented as the mean Dice score
over all eleven anatomies in the DSAD. The red stars indicate the performance of
the network trained with Multi-Teacher Knowledge Distillation (MT-KD). The
red arrows indicate the respective improvement.
⋆refers to models using a
common encoder and eleven anatomy-specific decoders. ■indicates segmentation
models that are trained on each anatomy separately."
IMPLEMENTATION/METHODS,0.1678832116788321,2. Method
IMPLEMENTATION/METHODS,0.17518248175182483,"Our multi-teacher knowledge distillation approach with its two stages is schematically il-
lustrated in Figure 2 and explained in more detail in the following."
IMPLEMENTATION/METHODS,0.18248175182481752,"Stage 1:
During the first stage, D = {Ai=1, Ai=2, .., Ai=N} corresponds to the overall data set, where"
IMPLEMENTATION/METHODS,0.1897810218978102,"Ai denotes the anatomy-specific data set of the ith anatomy of N anatomies. In the follow-
ing, only one of the anatomy-specific data sets is considered.
Let A be denoted as A = {(xj, yj)}M
j=1, where xj ∈R3×H×W and yj ∈R1×H×W correspond
to the input image and binary segmentation mask, respectively. M denotes the overall
number of images in A. Further, a teacher segmentation model T, consisting of one encoder
F enc and one decoder F dec, is trained using the standard pixel-wise binary cross entropy
loss function formulated as:"
IMPLEMENTATION/METHODS,0.19708029197080293,"LCE = − M
X j=1 H×W
X"
IMPLEMENTATION/METHODS,0.20437956204379562,"k=1
[yj,k log(T(xj,k)) + (1 −yj,k) log(1 −T(xj,k))]
(1)"
IMPLEMENTATION/METHODS,0.2116788321167883,"Stage 2:
In this stage, the same data set D is utilized as in stage 1.
This time, a student seg-
mentation model S that consists of one encoder F enc and N anatomy-specific decoders
{F dec
1
, F dec
2
, .., F dec
N } is optimized using two different objective functions.
For the first objective function, we follow the description from Kolbinger et al. (Kolbinger
et al., 2023). There, the pixel-wise binary cross entropy loss according to Equation 1 is
calculated separately using the output probabilities of each of the N anatomy-specific de-
coders. In detail, the loss is calculated for each pixel, only if the annotated anatomy i in
the input image corresponds to the respective anatomy-specific decoder F dec
i
of S. For all
other decoders F dec
̸=i , only the pixels in xj that belong to the anatomy i are considered for
the loss calculation as the false positive class, as shown in Figure 2. The remaining pixels
are not considered for the loss calculation, as only binary segmentation masks are used in
DSAD and several anatomies can appear per image and it cannot be ruled out that all other
pixels do not contain the anatomy i.
The second objective function utilizes the anatomy-specific knowledge of the various teacher
models with frozen parameters from stage 1. Similar as in work by Shu et al. (Shu et al.,
2021), where the normalized activations of corresponding channels between the teacher and
student network are aligned using the KL divergence, we utilize the normalized output logits
of each of the anatomy-specific teacher models and minimize the discrepancy to the normal-
ized output logits of the corresponding anatomy-specific decoder of the student model. As
the logits of a well-trained, anatomy-specific teacher model generally show salient anatom-
ical regions, the student model, capable of segmenting multiple anatomies simultaneously,
can be guided. This results in overall higher segmentation performance of the student. Let
zT
i,C and zS
i,C be the output logits of the anatomy-specific teacher model Ti and the student
decoder F dec
i
of anatomy i, with C being either the anatomy or false positive/background
class. First, the output logits zT
i,C and zS
i,C are divided by a temperature value T and then
normalized using the softmax function σ(z) =
ez
P(ez). The temperature value T is used
to control the softness of the probability distribution. Second, to evaluate the discrepancy"
IMPLEMENTATION/METHODS,0.21897810218978103,"between the two probability distributions pT
i,C = σ(
zT
i,C
T ) and pS
i,C = σ(
zS
i,C
T ), we utilize the
KL divergence (Kullback and Leibler, 1951).
The second objective function during stage 2 can therefore be denoted as:"
IMPLEMENTATION/METHODS,0.22627737226277372,"LKLi(pS
i,C, pT
i,C) = pT
i,C · log(
pT
i,C
pS
i,C
)
(2)"
IMPLEMENTATION/METHODS,0.23357664233576642,Efficient Anatomy Segmentation with KD
IMPLEMENTATION/METHODS,0.24087591240875914,"With λ as a weighting parameter, the overall objective function of stage 2 can be formulated
as: L = N
X"
IMPLEMENTATION/METHODS,0.24817518248175183,"i=1
Li = N
X"
IMPLEMENTATION/METHODS,0.25547445255474455,"i=1
LCEi · λLKLi
(3) .
.
."
IMPLEMENTATION/METHODS,0.26277372262773724,Student Training (Stage 2)
IMPLEMENTATION/METHODS,0.27007299270072993,False Positive Class
IMPLEMENTATION/METHODS,0.2773722627737226,Anatomy
IMPLEMENTATION/METHODS,0.2846715328467153,Not considered for
IMPLEMENTATION/METHODS,0.291970802919708,loss calculation
IMPLEMENTATION/METHODS,0.29927007299270075,Anatomy-Specific Teacher Training
IMPLEMENTATION/METHODS,0.30656934306569344,(Stage 1)
IMPLEMENTATION/METHODS,0.31386861313868614,Segmentation Mask
IMPLEMENTATION/METHODS,0.32116788321167883,Anatomy 1
IMPLEMENTATION/METHODS,0.3284671532846715,TAnatomy 1
IMPLEMENTATION/METHODS,0.3357664233576642,Logits
IMPLEMENTATION/METHODS,0.34306569343065696,Logits
IMPLEMENTATION/METHODS,0.35036496350364965,Segmentation Mask
IMPLEMENTATION/METHODS,0.35766423357664234,Anatomy i . . . . . .
IMPLEMENTATION/METHODS,0.36496350364963503,Frozen Weights
IMPLEMENTATION/METHODS,0.3722627737226277,T Anatomy i
IMPLEMENTATION/METHODS,0.3795620437956204,"Pixel Cross
Entropy Loss"
IMPLEMENTATION/METHODS,0.38686131386861317,Logits
IMPLEMENTATION/METHODS,0.39416058394160586,Segmentation Mask
IMPLEMENTATION/METHODS,0.40145985401459855,Anatomy N
IMPLEMENTATION/METHODS,0.40875912408759124,Input Image
IMPLEMENTATION/METHODS,0.41605839416058393,Anatomy 1
IMPLEMENTATION/METHODS,0.4233576642335766,Input Image
IMPLEMENTATION/METHODS,0.4306569343065693,Anatomy N
IMPLEMENTATION/METHODS,0.43795620437956206,"Fenc
Pixel Cross
Entropy Loss"
IMPLEMENTATION/METHODS,0.44525547445255476,Student
IMPLEMENTATION/METHODS,0.45255474452554745,Frozen Weights
IMPLEMENTATION/METHODS,0.45985401459854014,T Anatomy 1
IMPLEMENTATION/METHODS,0.46715328467153283,Logits
IMPLEMENTATION/METHODS,0.4744525547445255,Input Image
IMPLEMENTATION/METHODS,0.48175182481751827,Anatomy i
IMPLEMENTATION/METHODS,0.48905109489051096,Input Image
IMPLEMENTATION/METHODS,0.49635036496350365,Anatomy i
IMPLEMENTATION/METHODS,0.5036496350364964,Logits
IMPLEMENTATION/METHODS,0.5109489051094891,"Pixel Cross
Entropy Loss"
IMPLEMENTATION/METHODS,0.5182481751824818,"Pixel Cross
Entropy Loss
KL Divergence Loss"
IMPLEMENTATION/METHODS,0.5255474452554745,KL Divergence Loss
IMPLEMENTATION/METHODS,0.5328467153284672,TAnatomy N
IMPLEMENTATION/METHODS,0.5401459854014599,"Fenc
Fdec Fdec Fenc Fi dec F1 dec"
IMPLEMENTATION/METHODS,0.5474452554744526,Logits
IMPLEMENTATION/METHODS,0.5547445255474452,"Figure 2: Schematic of the proposed MT-KD approach. In stage 1, multiple teacher net-
works are trained to obtain high anatomy-specific accuracy. In stage 2, the nor-
malized output logits of the individual anatomy-specific teacher models are used
to guide the training of the student models using the KL-divergence."
RESULTS/EXPERIMENTS,0.5620437956204379,3. Experimental Setup
RESULTS/EXPERIMENTS,0.5693430656934306,3.1. Data
RESULTS/EXPERIMENTS,0.5766423357664233,"To evaluate our MT-KD approach, we use the DSAD that consists of 13, 195 high-quality
laparoscopic images with pixel-wise annotations of eleven intra-abdominal anatomical struc-
tures, i.e., abdominal wall, colon, inferior mesenteric artery, intestinal veins, liver, pancreas,
small intestine, spleen, stomach, ureter and vesicular glands (Carstens et al., 2023). We
follow the training, validation and test splits as well as the pre-processing steps and aug-
mentations as in the original work from (Kolbinger et al., 2023)."
RESULTS/EXPERIMENTS,0.583941605839416,3.2. Implementation details
RESULTS/EXPERIMENTS,0.5912408759124088,"For all experiments with the proposed approach, we utilize the Adam optimizer (Kingma
and Ba, 2014), using a learning rate of 5e-4 during stage 1 and a learning rate of 1e-3 for"
RESULTS/EXPERIMENTS,0.5985401459854015,"stage 2. Additionally, an exponential learning rate scheduler is used. We train our models
for 100 epochs and 60 epochs in stage 1 and 2 and end up with a final learning rate of 1.5e-6
and 2.5e-5, respectively. During the teacher-student knowledge distillation, we follow the
implementation details from (Shu et al., 2021) and use the temperature value T = 4.0 for
the calculation of the KL divergence and the weighting parameter λ = 3. During all training
experiments, a batch size of 8 and an input image size of 640 × 512 is utilized. The utilized
convolutional encoder architectures are pretrained on either the COCO or Cityscapes data
set (Lin et al., 2014; Cordts et al., 2016). For the transformer-based segmentation networks,
i.e., SegFormer, we use pretrained weights from the Cityscape data set. For the evaluation
on the test data set, the model that obtained the best results in terms of the Dice score on
the validation set is used. We implement all models in Pytorch 1."
RESULTS/EXPERIMENTS,0.6058394160583942,4. Results
RESULTS/EXPERIMENTS,0.6131386861313869,"To evaluate the proposed approach in this work, the mean Dice score among all eleven
anatomies in the DSAD is used. Furthermore, the number of parameters of each segmen-
tation model as well as the inference time in form of time for computing the segmentation
mask of one input image of size 1280 × 720 (High Definition) is determined."
RESULTS/EXPERIMENTS,0.6204379562043796,"Image
Ground Truth
SegFormerB3
MiniNetv2
MiniNetv2
with our MT-KD approach"
RESULTS/EXPERIMENTS,0.6277372262773723,"Abdominal
Wall"
RESULTS/EXPERIMENTS,0.635036496350365,"Intestinal 
Veins"
RESULTS/EXPERIMENTS,0.6423357664233577,Stomach
RESULTS/EXPERIMENTS,0.6496350364963503,"Abdominal
Wall"
RESULTS/EXPERIMENTS,0.656934306569343,Figure 3: Qualitative segmentation results for different anatomies.
RESULTS/EXPERIMENTS,0.6642335766423357,"In order to assess the improvements of our the approach, we evaluate previous deep
learning models applied to the DSAD regarding segmentation accuracy and inference time,
as well as another state-of-the-art network for efficient surgical segmentation, as shown in"
RESULTS/EXPERIMENTS,0.6715328467153284,1. Code available at:
RESULTS/EXPERIMENTS,0.6788321167883211,https://github.com/lennart-maack/Efficient-Anatomy-Segmentation-w-Multi-Teacher-KD
RESULTS/EXPERIMENTS,0.6861313868613139,Efficient Anatomy Segmentation with KD
RESULTS/EXPERIMENTS,0.6934306569343066,"Table 1: Segmentation and inference speed results. ■indicates segmentation models that
are trained on each anatomy separately, consisting of one encoder and one de-
coder. ⋆indicates models using a common encoder and eleven anatomy-specific
decoders. Dice score (%) is calculated as the mean over all eleven anatomies. Time
corresponds to the inference time in ms for one image (1280 × 720) on a NVIDIA
RTX3090. SegFormer-B3 ■is used as a teacher network for our Multi-Teacher
Knowledge Distillation (MT-KD) approach."
RESULTS/EXPERIMENTS,0.7007299270072993,"Architecture
Encoder
Params(M)
Dice (↑)
Time (↓)
FPS (↑)
DeepLabv3 ■
ResNet18
174.8
53.8
160
6
(Chen et al., 2017)
ResNet50
435.9
65.3
367
3
EfficientNetb0
80.3
62.8
262
4
EfficientNetb3
159.1
66.3
479
2
SegFormer ■
SegFormerB0
40.8
64.0
180
6
(Xie et al., 2021)
SegFormerB3
519.5
69.7
548
2
MiniNetv2 ■
MiniNetv2
5.5
43.2
35
28
(Tomasini et al., 2022)"
RESULTS/EXPERIMENTS,0.708029197080292,"ResNet18
63.1
58.7
42
23
DeepLabv3 ⋆
ResNet50
200.0
59.8
112
9
(Chen et al., 2017)
EfficientNetb0
40.0
60.3
45
22
EfficientNetb3
52.2
64.2
67
15
SegFormer ⋆
SegFormerB0
7.7
60.5
39
25
(Xie et al., 2021)
SegFormerB3
78.7
66.9
153
6
MiniNetv2 ⋆
MiniNetv2
1.1
36.1
26
38
(Tomasini et al., 2022)"
RESULTS/EXPERIMENTS,0.7153284671532847,"DeepLabv3 ⋆
EfficientNetb0
40.0
64.5
45
22
(with our MT-KD approach)
ResNet18
63.1
64.4
42
23
SegFormer ⋆
SegFormerB0
7.7
64.9
39
25
(with our MT-KD approach)"
RESULTS/EXPERIMENTS,0.7226277372262774,"MiniNetv2 ⋆
MiniNetv2
1.1
60.5
26
38
(with our MT-KD approach)"
RESULTS/EXPERIMENTS,0.7299270072992701,"Table 1. Both anatomy-specific models with one encoder and one decoder (■), as well as
models with one common encoder and multiple anatomy-specific decoders (⋆) are evalu-
ated. From the results in Table 1, we observe that segmentation models trained on each
anatomy separately outperform networks with the same architecture but using a common
encoder and eleven anatomy-specific decoders in terms of Dice score. However, the number
of parameters increases significantly when using multiple anatomy-specific models which
leads to low inference speed. Furthermore, the results show superior performance in terms
of Dice score when DeepLabv3 is used with EfficientNet as an encoder compared to ResNet
encoders. Small network architectures such as MiniNetv2 enable real-time capabilities, but
show significantly lower segmentation performance. The transformer-based architectures
SegFormer achieves higher segmentation accuracy in comparison to convolutional-based"
RESULTS/EXPERIMENTS,0.7372262773722628,"DeepLabv3 networks. Our proposed MT-KD approach increases the segmentation perfor-
mance of both convolutional-based segmentation networks and transformer-based segmen-
tation networks. The most significant increase due to our MT-KD approach can be shown
for small models, i.e.
MiniNetv2.
In this case, the Dice score increases from 36.1% to
60.5%. Qualitative results in Figure 3 show more accurate segmentation of anatomies for
MiniNetv2 when trained with our MT-KD. Especially for smaller details, the segmentation
accuracy can be increased by using large and accurate teacher models. A uniform increase
in segmentation performance can be observed across all eleven anatomies.
The specific
segmentation results in terms of Dice score for individual anatomies can be found in the
Appendix."
CONCLUSION/DISCUSSION,0.7445255474452555,5. Discussion and Conclusion
CONCLUSION/DISCUSSION,0.7518248175182481,"Current deep learning networks for anatomy segmentation suffer from two problems. Either
they show good segmentation performance, similar to human experts, but are too large for
real-time applications or they are efficient enough for real-time applications but do not show
sufficient performance for high-accuracy segmentation. In this work, we propose a multi-
teacher knowledge distillation (MT-KD) approach that leverages the knowledge of multiple
anatomy-specific, high-accuracy teacher networks to tackle the problem of training a sin-
gle and efficient network with partially labeled datasets. By minimizing the discrepancy
between the normalized logits of anatomy-specific, high-accuracy teacher networks and a
single and efficient student network, the segmentation accuracy of various small, real-time
capable network architectures is improved while retaining high inference speed.
Our results demonstrate highest segmentation performance with 66.3% and 69.7% Dice
score for anatomy-specific, high capacity teacher networks such as DeepLabv3/Eff.Netb3 ■
and SegFormerB3 ■. We assume higher capacity to learn complex, anatomy-specific fea-
tures. The results further demonstrate that anatomy-specific, low capacity networks such
as MiniNetv3 ■only achieve an overall Dice Score of 43.2%, failing to learn valuable fea-
tures in the data. For application in realistic surgical scenarios, anatomy-specific models
need to be operated sequentially in order to segment several anatomies. A combined ar-
chitecture with one common encoder and anatomy-specifc decoders enables simultaneous
anatomy segmentation. Although the segmentation performance of the combined architec-
ture decreases only by 2.1% and 2.8%, for DeepLabv3/EfficientNetb3 ⋆and SegFormerB3
⋆, respectively, models with large encoder and decoder architectures still do not achieve
a sufficiently high frame rate (6 FPS and 15 FPS). With our approach, the segmentation
performance of models with smaller and thus faster architectures increases by up to 24.4%.
Especially, the accurate segmentation of smaller and more complex anatomies can be im-
proved for smaller segmentation networks when guided by accurate teacher models with our
MT-KD approach. Overall, the segmentation accuracy of high-capacity, anatomy-specific
networks remains at least 1.8% higher, however, almost two orders of magnitude fewer pa-
rameters are required. We evaluated our approach on the recently published DSAD. In
order to evaluate whether the generalization ability of smaller student models changes sim-
ilarly to that of larger teacher models when applied to other laparoscopic data sets, it is of
interest to perform further comprehensive studies on different laparoscopic data sets."
CONCLUSION/DISCUSSION,0.7591240875912408,Efficient Anatomy Segmentation with KD
REFERENCES,0.7664233576642335,References
REFERENCES,0.7737226277372263,"Max Allan, Satoshi Kondo, Sebastian Bodenstedt, Stefan Leger, Rahim Kadkhodamoham-
madi, Imanol Luengo, Felix Fuentes, Evangello Flouty, Ahmed Mohammed, Marius Ped-
ersen, Avinash Kori, Varghese Alex, Ganapathy Krishnamurthi, David Rauber, Robert
Mendel, Christoph Palm, Sophia Bano, Guinther Saibro, Chi-Sheng Shih, Hsun-An Chi-
ang, Juntang Zhuang, Junlin Yang, Vladimir Iglovikov, Anton Dobrenkii, Madhu Red-
diboina, Anubhav Reddy, Xingtong Liu, Cong Gao, Mathias Unberath, Myeonghyeon
Kim, Chanho Kim, Chaewon Kim, Hyejin Kim, Gyeongmin Lee, Ihsan Ullah, Miguel
Luna, Sang Hyun Park, Mahdi Azizian, Danail Stoyanov, Lena Maier-Hein, and Stefanie
Speidel. 2018 robotic scene segmentation challenge, 2020."
REFERENCES,0.781021897810219,"Abdollah Amirkhani, Amir Khosravian, Masoud Masih-Tehrani, and Hossein Kashiani.
Robust semantic segmentation with multi-teacher knowledge distillation. IEEE Access,
9:119049–119066, 2021."
REFERENCES,0.7883211678832117,"Yoshiko Bamba, Shimpei Ogawa, Michio Itabashi, Hironari Shindo, Shingo Kameoka,
Takahiro Okamoto, and Masakazu Yamamoto.
Object and anatomical feature recog-
nition in surgical video images based on a convolutional neural network. International
Journal of Computer Assisted Radiology and Surgery, 16(11):2045–2054, June 2021."
REFERENCES,0.7956204379562044,"Matthias Carstens, Franziska M. Rinner, Sebastian Bodenstedt, Alexander C. Jenke, J¨urgen
Weitz, Marius Distler, Stefanie Speidel, and Fiona R. Kolbinger. The dresden surgical
anatomy dataset for abdominal organ segmentation in surgical data science. Scientific
Data, 10(1), January 2023."
REFERENCES,0.8029197080291971,"Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking
atrous convolution for semantic image segmentation. arXiv preprint, 2017."
REFERENCES,0.8102189781021898,"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Ro-
drigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for
semantic urban scene understanding. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 3213–3223, 2016."
REFERENCES,0.8175182481751825,"Emilie Even Dencker, Alexander Bonde, Anders Troelsen, Kartik Mangudi Varadarajan,
and Martin Sillesen. Postoperative complications: an observational study of trends in
the united states from 2012 to 2018. BMC Surgery, 21(1), November 2021."
REFERENCES,0.8248175182481752,"Andras B. Fecso, Peter Szasz, Georgi Kerezov, and Teodor P. Grantcharov. The effect of
technical performance on patient outcomes in surgery: A systematic review. Annals of
Surgery, 265(3):492–501, March 2017."
REFERENCES,0.8321167883211679,"Debesh Jha, Nikhil Kumar Tomar, Sharib Ali, Michael A Riegler, H˚avard D Johansen, Dag
Johansen, Thomas de Lange, and P˚al Halvorsen. Nanonet: Real-time polyp segmentation
in video capsule endoscopy and colonoscopy. In 2021 IEEE 34th International Symposium
on Computer-Based Medical Systems (CBMS), pages 37–43. IEEE, 2021."
REFERENCES,0.8394160583941606,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014."
REFERENCES,0.8467153284671532,"Fiona R Kolbinger, Franziska M Rinner, Alexander C Jenke, Matthias Carstens, Stefanie
Krell, Stefan Leger, Marius Distler, J¨urgen Weitz, Stefanie Speidel, and Sebastian Boden-
stedt. Anatomy segmentation in laparoscopic surgery: comparison of machine learning
and human expertise–an experimental study. International Journal of Surgery, 109(10):
2962–2974, 2023."
REFERENCES,0.8540145985401459,"Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of
mathematical statistics, 22(1):79–86, 1951."
REFERENCES,0.8613138686131386,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,
Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, Septem-
ber 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014."
REFERENCES,0.8686131386861314,"Amin Madani, Babak Namazi, Maria S. Altieri, Daniel A. Hashimoto, Angela Maria Rivera,
Philip H. Pucher, Allison Navarrete-Welton, Ganesh Sankaranarayanan, L. Michael
Brunt, Allan Okrainec, and Adnan Alseidi.
Artificial intelligence for intraoperative
guidance: Using semantic segmentation to identify surgical anatomy during laparoscopic
cholecystectomy. Annals of Surgery, 276(2):363–369, November 2020."
REFERENCES,0.8759124087591241,"Pietro Mascagni, Deepak Alapatt, Luca Sestini, Maria S. Altieri, Amin Madani, Yusuke
Watanabe, Adnan Alseidi, Jay A. Redan, Sergio Alfieri, Guido Costamagna, Ivo Boˇskoski,
Nicolas Padoy, and Daniel A. Hashimoto. Computer vision in surgery: from potential to
clinical value. npj Digital Medicine, 5(1), October 2022."
REFERENCES,0.8832116788321168,"Daniil Pakhomov and Nassir Navab.
Searching for efficient architecture for instrument
segmentation in robotic surgery. Medical Image Computing and Computer Assisted In-
tervention – MICCAI, 2020."
REFERENCES,0.8905109489051095,"Dian Qin, Jia-Jun Bu, Zhe Liu, Xin Shen, Sheng Zhou, Jing-Jun Gu, Zhi-Hua Wang,
Lei Wu, and Hui-Fen Dai. Efficient medical image segmentation based on knowledge
distillation. IEEE Transactions on Medical Imaging, 40(12):3820–3831, 2021."
REFERENCES,0.8978102189781022,"Changyong Shu, Yifan Liu, Jianfei Gao, Zheng Yan, and Chunhua Shen. Channel-wise
knowledge distillation for dense prediction. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pages 5311–5320, 2021."
REFERENCES,0.9051094890510949,"James W. Suliburk, Quentin M. Buck, Chris J. Pirko, Nader N. Massarweh, Neal R. Barshes,
Hardeep Singh, and Todd K. Rosengart.
Analysis of human performance deficiencies
associated with surgical adverse events. JAMA Network Open, 2(7):e198067, July 2019."
REFERENCES,0.9124087591240876,"Clara Tomasini, Luis Riazuelo, AC Murillo, and I˜nigo Alonso. Efficient tool segmentation
for endoscopic videos in the wild. Proceedings of Machine Learning Research 172:1–17,
2022."
REFERENCES,0.9197080291970803,"Constantin Ulrich, Fabian Isensee, Tassilo Wald, Maximilian Zenk, Michael Baumgart-
ner, and Klaus H Maier-Hein. Multitalent: A multi-dataset approach to medical image
segmentation. In International Conference on Medical Image Computing and Computer-
Assisted Intervention, pages 648–658. Springer, 2023."
REFERENCES,0.927007299270073,Efficient Anatomy Segmentation with KD
REFERENCES,0.9343065693430657,"Minh H Vu, Gabriella Norman, Tufve Nyholm, and Tommy L¨ofstedt. A data-adaptive loss
function for incomplete data and incremental learning in semantic image segmentation.
IEEE Transactions on Medical Imaging, 41(6):1320–1330, 2021."
REFERENCES,0.9416058394160584,"Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping
Luo. Segformer: Simple and efficient design for semantic segmentation with transformers.
Advances in Neural Information Processing Systems, 34:12077–12090, 2021."
REFERENCES,0.948905109489051,"Jiafeng Xie, Bing Shuai, Jianfang Hu, Jingyang Lin, and Weishi Zheng. Improving fast
segmentation with teacher-student learning. In British Machine Vision Conference, 2018."
OTHER,0.9562043795620438,Appendix A. Anatomy specific results
OTHER,0.9635036496350365,"Architecture
Encoder
Abdominal
Colon
Inferior
Intestinal
Liver
Pancreas
Small
Spleen
Stomach
Ureter
Vesicular
wall
mesenteric
veins
intestine
glands
artery
DeepLabv3 ■
ResNet18
83
69
44
39
65
28
79
56
59
33
37
(Chen et al., 2017)
ResNet50
90
79
54
54
80
37
87
79
71
47
40
Eff.Netb0
88
77
51
49
71
42
86
74
66
40
47
Eff.Netb3
90
79
54
56
76
43
88
78
66
48
52
SegFormer ■
SegFormerB0
89
76
51
51
78
45
85
73
64
44
48
(Xie et al., 2021)
SegFormerB3
91
79
58
58
83
46
89
81
75
52
55
MiniNetv2 ■
MiniNetv2
80
55
34
24
61
25
70
38
44
20
25
(Tomasini et al., 2022)"
OTHER,0.9708029197080292,"ResNet18
83
70
44
47
76
37
77
73
61
32
46
DeepLabv3 ⋆
ResNet50
83
72
49
47
68
35
79
73
63
35
47
(Chen et al., 2017)
Eff.Netb0
83
72
48
47
71
37
80
75
64
39
48
Eff.Netb3
84
75
53
57
72
43
81
76
70
47
51
SegFormer ⋆
SegFormerB0
83
73
42
53
75
42
79
69
62
42
46
(Xie et al., 2021)
SegFormerB3
89
77
55
53
78
45
84
81
71
49
54
MiniNetv2 ⋆
MiniNetv2
61
46
18
36
52
29
51
36
34
13
21
(Tomasini et al., 2022)"
OTHER,0.9781021897810219,"DeepLabv3 ⋆
Eff.Netb0
86
74
55
54
69
41
84
81
68
48
49
(w/ our KD-approach)
ResNet18
87
76
55
54
77
37
81
78
70
43
51
SegFormer ⋆
SegFormerB0
84
75
51
53
76
45
81
81
74
45
49
(w/ our KD-approach)"
OTHER,0.9854014598540146,"MiniNetv2 ⋆
MiniNetv2
83
74
46
49
72
40
80
74
64
41
42
(w/ our KD-approach)"
OTHER,0.9927007299270073,Table 2: Segmentation results for all individual anatomies in terms of Dice score (%).
