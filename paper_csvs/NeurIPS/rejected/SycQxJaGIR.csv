Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017152658662092624,"Multi-agent Pathfinding (MAPF) problem generally asks to find a set of conflict-
1"
ABSTRACT,0.003430531732418525,"free paths for a set of agents confined to a graph. In conventional MAPF scenarios,
2"
ABSTRACT,0.005145797598627788,"the graph and the agents’ start and goal locations are known in advance. Thus,
3"
ABSTRACT,0.00686106346483705,"a centralized planning algorithm can be utilized to generate a solution. In this
4"
ABSTRACT,0.008576329331046312,"work, we investigate the decentralized MAPF setting, in which the agents can
5"
ABSTRACT,0.010291595197255575,"not share the information and must independently navigate toward their goals
6"
ABSTRACT,0.012006861063464836,"without knowing the other agents’ goals or paths. We focus on the lifelong variant
7"
ABSTRACT,0.0137221269296741,"of MAPF, which involves continuously assigning new goals to the agents upon
8"
ABSTRACT,0.015437392795883362,"arrival to the previous ones. To address this complex problem, we propose a
9"
ABSTRACT,0.017152658662092625,"method that integrates two complementary approaches: planning with heuristic
10"
ABSTRACT,0.018867924528301886,"search and reinforcement learning (RL) through policy optimization. Planning
11"
ABSTRACT,0.02058319039451115,"is utilized to maintain an individual path, while RL is employed to discover the
12"
ABSTRACT,0.022298456260720412,"collision avoidance policies that effectively guide an agent along the path. This
13"
ABSTRACT,0.024013722126929673,"decomposition and intrinsic motivation specific for multi-agent scenarios allows
14"
ABSTRACT,0.025728987993138937,"leveraging replanning with learnable policies. We evaluate our method on a wide
15"
ABSTRACT,0.0274442538593482,"range of setups and compare it to the state-of-the-art competitors (both learnable
16"
ABSTRACT,0.029159519725557463,"and search-based). The results show that our method consistently outperforms the
17"
ABSTRACT,0.030874785591766724,"competitors in challenging setups when the number of agents is high.
18"
INTRODUCTION,0.032590051457975985,"1
Introduction
19"
INTRODUCTION,0.03430531732418525,"Multi-agent pathfinding (MAPF) [1] is a challenging problem that gets increasing attention recently.
20"
INTRODUCTION,0.036020583190394515,"It is often studied in the AI community with the following assumptions. The agents are confined to a
21"
INTRODUCTION,0.03773584905660377,"graph, and at each time step an agent can either move to an adjacent vertex or stay at the current one.
22"
INTRODUCTION,0.03945111492281304,"A central controller possesses information about the graph and the agents’ start and goal locations.
23"
INTRODUCTION,0.0411663807890223,"This unit is in charge of constructing a set of conflict-free plans for all the agents. Thus, a typical
24"
INTRODUCTION,0.04288164665523156,"setting for MAPF can be attributed as centralized and fully observable.
25"
INTRODUCTION,0.044596912521440824,"In many real-world domains, however, the central controller does not exist, or, even if it does, it may
26"
INTRODUCTION,0.04631217838765009,"not possess full information about the environment. For example, consider a fleet of service robots
27"
INTRODUCTION,0.048027444253859346,"delivering some items in a human-shared environment, e.g., the robots delivering drugs in the hospital.
28"
INTRODUCTION,0.04974271012006861,"Each of these robots is likely to have access to the global map of the environment (e.g., the floor
29"
INTRODUCTION,0.051457975986277875,"plan), possibly refined through the robot’s sensors. However, the connection to the central controller
30"
INTRODUCTION,0.05317324185248713,"may not be consistent. Thus, the latter may not have accurate data on the robots’ locations and,
31"
INTRODUCTION,0.0548885077186964,"consequently, cannot provide valid MAPF solutions. In such scenarios, decentralized approaches
32"
INTRODUCTION,0.05660377358490566,"to the MAPF problems, when the robots themselves have to decide their future paths, are essential.
33"
INTRODUCTION,0.058319039451114926,"Moreover, decentralized approaches may be preferable due to the poor scalability of the centralized
34"
INTRODUCTION,0.060034305317324184,"ones. In this work, we aim to develop such an efficient decentralized approach.
35 1 2 3"
INTRODUCTION,0.06174957118353345,"Figure 1: An example of a decentralized
LMAPF instance is depicted below. Each
agent is represented by a filled circle. The red
agent has visibility limited to the positions of
other agents within its field of view, indicated
by a dotted red line. The red circles with num-
bers represent the goals that the agent needs
to reach. The next goal is revealed only after
the previous one is achieved. The small red
circle indicates the subgoal the agent needs to
accomplish to progress towards its goal."
INTRODUCTION,0.0634648370497427,"It is natural to frame the decentralized MAPF as a
36"
INTRODUCTION,0.06518010291595197,"sequential decision-making problem where at each
37"
INTRODUCTION,0.06689536878216124,"time step, each agent must choose and execute an
38"
INTRODUCTION,0.0686106346483705,"action that will advance it to the goal and, at the same
39"
INTRODUCTION,0.07032590051457976,"time, will not disallow other agents to reach their
40"
INTRODUCTION,0.07204116638078903,"goals as well. The result of solving this problem
41"
INTRODUCTION,0.07375643224699828,"is a policy that, at each moment, tells which action
42"
INTRODUCTION,0.07547169811320754,"to execute. To form such a policy, learnable meth-
43"
INTRODUCTION,0.07718696397941681,"ods are commonly used, for example, reinforcement
44"
INTRODUCTION,0.07890222984562607,"learning (RL), which is especially beneficial in tasks
45"
INTRODUCTION,0.08061749571183534,"with incomplete information [2, 3, 4]. However, even
46"
INTRODUCTION,0.0823327615780446,"state-of-the-art model-free RL methods generally can-
47"
INTRODUCTION,0.08404802744425385,"not efficiently solve long-horizon problems with the
48"
INTRODUCTION,0.08576329331046312,"involved casual structure [5, 6], and they are often
49"
INTRODUCTION,0.08747855917667238,"inferior to the seach-based methods when solving
50"
INTRODUCTION,0.08919382504288165,"problems with hard combinatorial structure.
51"
INTRODUCTION,0.09090909090909091,"The additional challenges that make the MAPF prob-
52"
INTRODUCTION,0.09262435677530018,"lems challenging for RL are as follows. First, we
53"
INTRODUCTION,0.09433962264150944,"want the policy to be highly generalizable to previ-
54"
INTRODUCTION,0.09605488850771869,"ously unseen environments, which may differ sig-
55"
INTRODUCTION,0.09777015437392796,"nificantly in scale and topology from the ones used
56"
INTRODUCTION,0.09948542024013722,"during the learning stage. In MAPF, our primary in-
57"
INTRODUCTION,0.10120068610634649,"terest lies not in how well the agents learn to behave
58"
INTRODUCTION,0.10291595197255575,"in the environment(s) used for training, but rather how
59"
INTRODUCTION,0.10463121783876501,"well they perform in any arbitrary (even out-of-the-
60"
INTRODUCTION,0.10634648370497427,"distribution) environment. Second, MAPF problems
61"
INTRODUCTION,0.10806174957118353,"are naturally dependent on the goal locations of the
62"
INTRODUCTION,0.1097770154373928,"agents, meaning that even in the same environment
63"
INTRODUCTION,0.11149228130360206,"(map), the goals may vary significantly. Finally, effectively training in a complex observation and
64"
INTRODUCTION,0.11320754716981132,"action spaces poses challenges even for state-of-the-art multi-agent reinforcement learning (MARL)
65"
INTRODUCTION,0.11492281303602059,"methods.
66"
INTRODUCTION,0.11663807890222985,"To this end, in this work we suggest not to solve the MAPF problem directly by RL but rather to
67"
INTRODUCTION,0.1183533447684391,"decompose it into a series of sub-tasks utilizing heuristic search algorithms and then solve these
68"
INTRODUCTION,0.12006861063464837,"sub-tasks efficiently with a learnable policy, that is obtained through the decentralized training. The
69"
INTRODUCTION,0.12178387650085763,"general pipeline of our solution is the following. Each agent plans an individual path to its goal by
70"
INTRODUCTION,0.1234991423670669,"the conventional heuristic search algorithm without considering the other agents (we also introduce
71"
INTRODUCTION,0.12521440823327615,"an additional technique to penalize paths that are likely to cause deadlocks). Then a waypoint on this
72"
INTRODUCTION,0.1269296740994854,"path is chosen in some vicinity of the agent, which becomes its local goal. To reach it, a learnable
73"
INTRODUCTION,0.12864493996569468,"policy is utilized, which takes both static obstacles and the locally observable agents into account.
74"
INTRODUCTION,0.13036020583190394,"Once a waypoint is reached, or the agent goes too far away from it the cycle repeats.
75"
INTRODUCTION,0.1320754716981132,"Empirically we compare our method, which we name FOLLOWER, to a range of both learnable and
76"
INTRODUCTION,0.13379073756432247,"non-learnable state-of-the-art competitors and show that it i) consistently outperforms the competitors
77"
INTRODUCTION,0.13550600343053174,"when the number of agents is high; ii) better generalizes to unseen environments compared to other
78"
INTRODUCTION,0.137221269296741,"learnable solvers; iii) may outperform a centralized search-based solver in certain setups.
79"
RELATED WORKS,0.13893653516295026,"2
Related Works
80"
RELATED WORKS,0.14065180102915953,"Lifelong MAPF
LMAPF is an extension of MAPF when the agents are assigned new goals upon
81"
RELATED WORKS,0.1423670668953688,"reaching their current ones. Similarly, in (online) multi-agent pickup and delivery (MAPD), agents
82"
RELATED WORKS,0.14408233276157806,"are continuously assigned tasks, comprising two locations that the agent has to visit in a strict order
83"
RELATED WORKS,0.1457975986277873,"– pickup location and delivery location. Typically, the assignment problem is not considered in
84"
RELATED WORKS,0.14751286449399656,"LMAPF/MAPD. However, there exist works that include the assignment task into the problem,
85"
RELATED WORKS,0.14922813036020582,"see [7, 8] for example.
86"
RELATED WORKS,0.1509433962264151,"In [9], several variants to tackle MAPD were proposed differing in the amount of data the agents
87"
RELATED WORKS,0.15265866209262435,"share. Yet, even the decoupled (as attributed by the authors) algorithms based on Token Swapping
88"
RELATED WORKS,0.15437392795883362,"rely on global information, i.e., the one provided by the central unit. An enhanced Token Swapping
89"
RELATED WORKS,0.15608919382504288,"variant that considers kinematic constraints was introduced in [10]. In [11], an efficient rule-based re-
90"
RELATED WORKS,0.15780445969125215,"planning approach to solve MAPF was introduced that is naturally capable of solving LMAPD/MAPD
91"
RELATED WORKS,0.1595197255574614,"problems. It did not rely on the several restrictive assumptions of Token Swapping and was empirically
92"
RELATED WORKS,0.16123499142367068,"shown to outperform the latter.
93"
RELATED WORKS,0.16295025728987994,"Finally, one of the most recent and effective LMAPF solvers is the RHCR algorithm presented in [12].
94"
RELATED WORKS,0.1646655231560892,"It relies on the idea of bounded planning, i.e., constructing not a complete plan but rather its initial
95"
RELATED WORKS,0.16638078902229847,"part. RHCR is a centralized solver that relies on the full knowledge of the agents’ locations, their
96"
RELATED WORKS,0.1680960548885077,"current paths, goals, etc. In this work we empirically compare with RHCR and show that our method
97"
RELATED WORKS,0.16981132075471697,"is superior when the number of agents is high.
98"
RELATED WORKS,0.17152658662092624,"Decentralized MAPF
This setting entails that the paths/actions of the agents are decided not by a
99"
RELATED WORKS,0.1732418524871355,"central unit but by the agents themselves. Numerous approaches, especially the ones tailored to the
100"
RELATED WORKS,0.17495711835334476,"robotics applications, boil this problem down to reactive control, see [13, 14, 15] for example. These
101"
RELATED WORKS,0.17667238421955403,"methods, however, are often prone to deadlocks. Several MAPF algorithms can also be implemented
102"
RELATED WORKS,0.1783876500857633,"in a decentralized manner. For example, in [16] MAPP algorithm was introduced that relies on the
103"
RELATED WORKS,0.18010291595197256,"individual pathfinding for each agent and a set of rules to determine priorities and choose actions to
104"
RELATED WORKS,0.18181818181818182,"avoid conflicts when they happen along the paths. In [11] PIBT algorithm was introduced in which
105"
RELATED WORKS,0.1835334476843911,"the agents also pick their actions individually (at each time step) based on specific rules. In general,
106"
RELATED WORKS,0.18524871355060035,"most rule-based MAPF solvers, like [17], can be implemented in such a way that each agent decides
107"
RELATED WORKS,0.18696397941680962,"its actions. However, in this case, the implicit assumption is that the agents can communicate to share
108"
RELATED WORKS,0.18867924528301888,"the relevant information (or that they have access to the global MAPF-related data). In contrast, our
109"
RELATED WORKS,0.19039451114922812,"work assumes that the agents are unable to communicate with one another or a central unit, which
110"
RELATED WORKS,0.19210977701543738,"significantly increases the complexity of the problem.
111"
RELATED WORKS,0.19382504288164665,"Learnable MAPF
This direction has been getting increased attention recently. In [18], a seminal
112"
RELATED WORKS,0.1955403087478559,"PRIMAL method that utilized reinforcement learning and imitation learning to solve MAPF in a
113"
RELATED WORKS,0.19725557461406518,"decentralized fashion was introduced. Later in [19], it was also tailored to solve LMAPF. The new
114"
RELATED WORKS,0.19897084048027444,"version got the name PRIMAL2. Since that, numerous learning-based MAPF solvers emerged, and
115"
RELATED WORKS,0.2006861063464837,"it became common to compare against PRIMAL/PRIMAL2 (we also compare with it in our work).
116"
RELATED WORKS,0.20240137221269297,"For example, in [20], another learning-based approach was proposed, tailored explicitly to agents
117"
RELATED WORKS,0.20411663807890223,"with a non-trivial dynamic model, such as quadrotors. In [21] DHC method that utilized the agents’
118"
RELATED WORKS,0.2058319039451115,"communications to solve decentralized MAPF efficiently was described. Another communication-
119"
RELATED WORKS,0.20754716981132076,"based learnable approach, PICO, was presented in [22]. Overall, currently, a wide range of learnable
120"
RELATED WORKS,0.20926243567753003,"decentralized MAPF solvers exist. However, to the best of our knowledge, they all rely on the
121"
RELATED WORKS,0.2109777015437393,"communication between the agents or on access to the global MAPF-related data (like in PRIMAL,
122"
RELATED WORKS,0.21269296740994853,"where each agent knows the goal locations of the others). We lift these assumptions in this work.
123"
RELATED WORKS,0.2144082332761578,"MARL
A separate direction in RL can be distinguished that specifically considers the multi-agent
124"
RELATED WORKS,0.21612349914236706,"setting (MARL) [23]. Mainly these approaches consider game environments (like Starcraft [24])
125"
RELATED WORKS,0.21783876500857632,"in which pathfinding is not of the primary importance. However, several MARL methods, such as
126"
RELATED WORKS,0.2195540308747856,"QMIX [3], MAPPO [25], have been adapted specifically for the MAPF task [26]. However they rely
127"
RELATED WORKS,0.22126929674099485,"on the information sharing between agents.
128"
RELATED WORKS,0.22298456260720412,"Much attention is paid to multi-agent learnable methods in robotics [27]. Often, the value-based
129"
RELATED WORKS,0.22469982847341338,"approaches are used to control small groups of agents on simple maps lile in [28] where a group
130"
RELATED WORKS,0.22641509433962265,"of 4 agents is considered. In [29], a combination of Particle Swarm Optimization and Q-Learning
131"
RELATED WORKS,0.2281303602058319,"controlling up to 100 agents is used. In [30], the model-based DynaQ method is used to learn agents
132"
RELATED WORKS,0.22984562607204118,"in the knowledge exchange mode. Some works [31, 32] use value-based approaches with prior
133"
RELATED WORKS,0.23156089193825044,"knowledge of how to interact with other agents. In [33] (MAPPER) an evolutionary reinforcement
134"
RELATED WORKS,0.2332761578044597,"learning was used for MAPF task. This work also uses a global planner to determine sub-goals in
135"
RELATED WORKS,0.23499142367066894,"learning one agent. In multi-agent mode, agents using ineffective polices are eliminated and only
136"
RELATED WORKS,0.2367066895368782,"successful agents continue to be trained.
137"
BACKGROUND,0.23842195540308747,"3
Background
138"
BACKGROUND,0.24013722126929674,"Multi-agent Pathfinding
In (Classical) Multi-agent pathfinding [1], the timeline is discretized to
139"
BACKGROUND,0.241852487135506,"the time steps, T = 0, 1, 2, ... and the workspace, where K agents operate, is discretized to a graph
140"
BACKGROUND,0.24356775300171526,"G = (V, E), whose vertices correspond to the locations and the edges to the transitions between
141"
BACKGROUND,0.24528301886792453,"these locations. K start and goal vertices are given and each agent i has to reach its goal gi ∈V
142"
BACKGROUND,0.2469982847341338,"from the start si ∈V . At each time step, an agent can either stay in its current vertex or move to an
143"
BACKGROUND,0.24871355060034306,"adjacent one. An individual plan for an agent pi1 is a sequence of actions that transfers it between
144"
BACKGROUND,0.2504288164665523,"two designated vertices. The plan’s cost is the time step when the agent reaches the goal.
145"
BACKGROUND,0.2521440823327616,"The MAPF problem asks to find a set of K plans s.t. each agent reaches the goal without colliding
146"
BACKGROUND,0.2538593481989708,"with other agents. Formally, two collisions are usually distinguished: vertex collision, when the
147"
BACKGROUND,0.2555746140651801,"agents occupy the same vertex at the same time step, and edge collision, when the agents use the
148"
BACKGROUND,0.25728987993138935,"same edge at the same time step.
149"
BACKGROUND,0.25900514579759865,"Lifelong MAPF (LMAPF) is a variant of MAPF where immediately after an agent reaches its goal, it
150"
BACKGROUND,0.2607204116638079,"is assigned to another one (via an external assignment procedure) and has to continue its operation.
151"
BACKGROUND,0.2624356775300172,"Thus, LMAPF generally asks to find not a fixed set of K plans but rather to i) find a set of K initial
152"
BACKGROUND,0.2641509433962264,"plans and ii) update each agent’s plan when it reaches the current goal and receives a new one. In
153"
BACKGROUND,0.2658662092624357,"extreme cases, when some goal is reached at each step, the plans’ updates are needed constantly (i.e.,
154"
BACKGROUND,0.26758147512864494,"at each time step).
155"
BACKGROUND,0.2692967409948542,"The Considered Decentralized LMAPF Problem
Consider a set of agents operating in the shared
156"
BACKGROUND,0.27101200686106347,"environment, represented as a graph G = (V, E). The timeline is discretized to the time steps
157"
BACKGROUND,0.2727272727272727,"T = 0, 1, ..., Tmax, where Tmax is the episode length. Each agent is located initially at the start
158"
BACKGROUND,0.274442538593482,"vertex and is assigned to the current goal vertex. If it reaches the latter before the episode ends, it is
159"
BACKGROUND,0.27615780445969124,"immediately assigned another goal vertex. We assume that the goal assignment unit is external to the
160"
BACKGROUND,0.27787307032590053,"system, and the agents’ behavior does not influence the goal assignments. An agent can reach the
161"
BACKGROUND,0.27958833619210977,"goal by performing the following actions: wait at the current vertex, and move to an adjacent vertex.
162"
BACKGROUND,0.28130360205831906,"The duration of each action is uniform, i.e., 1 time step. We assume that the outcomes of the actions
163"
BACKGROUND,0.2830188679245283,"are deterministic and no inaccuracies occur when executing the actions.
164"
BACKGROUND,0.2847341337907376,"Each agent has complete knowledge of the graph G. However, it can observe the other agents only
165"
BACKGROUND,0.2864493996569468,"locally. When observing them, no communication is happening. Thus an agent does not know the
166"
BACKGROUND,0.2881646655231561,"(current) goals or intended paths of the other agents. It observes only their locations. The observation
167"
BACKGROUND,0.28987993138936535,"function can be defined differently depending on the type of graph. In our experiments, we use
168"
BACKGROUND,0.2915951972555746,"4-connected grids and assume that an agent observes the other agents in the area of the size m × m,
169"
BACKGROUND,0.2933104631217839,"centered at the agent’s current position.
170"
BACKGROUND,0.2950257289879931,"Our task is to construct an individual policy π for each agent, i.e., the function that takes as input
171"
BACKGROUND,0.2967409948542024,"a graph (global information) and (a history of) observations (local information) and outputs a
172"
BACKGROUND,0.29845626072041165,"distribution over actions. Equipped with such policy, an agent at each time step samples an action
173"
BACKGROUND,0.30017152658662094,"from the distribution suggested by π and executes it in the environment. This continues until time
174"
BACKGROUND,0.3018867924528302,"step Tmax is reached when the episode ends. Upon that, we compute the throughput as the ratio of
175"
BACKGROUND,0.30360205831903947,"the episode length to the number of goals achieved by all agents. This metric is used to compare
176"
BACKGROUND,0.3053173241852487,"different policies: we say that π1 outperforms π2 (in a particular episode) if the throughput of the
177"
BACKGROUND,0.307032590051458,"former is higher.
178"
BACKGROUND,0.30874785591766724,"Partially Observable Markov Decision Process
We consider a partially observable multi-agent
179"
BACKGROUND,0.31046312178387653,"Markov decision process [34, 35]: M = ⟨S, A, U, P, R, O, γ⟩. At each timestep, each agent u ∈U,
180"
BACKGROUND,0.31217838765008576,"where U = 1, . . . , n, chooses an action au ∈A, forming a joint action j ∈J = Jn. This joint action
181"
BACKGROUND,0.313893653516295,"leads to a change in the environment according to the transition function P(s′|s, j) : S×J×S →[0, 1].
182"
BACKGROUND,0.3156089193825043,"After that each agent receives individual observations ou ∈O based on the global observation function
183"
BACKGROUND,0.31732418524871353,"G(s, a) : S × A →O. And individual reward R(s, u, j) : S × U × J →R, based on the current
184"
BACKGROUND,0.3190394511149228,"state, agent, and joint action. To make decisions each agent maintains an action-observation history
185"
BACKGROUND,0.32075471698113206,"τ u ∈T = (O × A)∗, which is used to condition a stochastic policy πu(au|τ u) : T × A →[0, 1].
186"
BACKGROUND,0.32246998284734135,"The task of the learning process is to optimize the policy πu for individual each agent in order to
187"
BACKGROUND,0.3241852487135506,"maximize the expected cumulative reward over time.
188"
LEARN TO FOLLOW,0.3259005145797599,"4
Learn to Follow
189"
LEARN TO FOLLOW,0.3276157804459691,"The suggested approach to solve the considered LMAPF problem, which we dub FOLLOWER,
190"
LEARN TO FOLLOW,0.3293310463121784,"comprises of the two complimentary modules combined into a coherent pipeline shown in Fig. 2.
191"
LEARN TO FOLLOW,0.33104631217838765,"1In MAPF literature, a plan is typically denoted with π. However, in RL, this is reserved to denote the policy.
As we use both MAPF and RL approaches in this work, we denote a plan as p."
LEARN TO FOLLOW,0.33276157804459694,"Spatial 
Encoder"
LEARN TO FOLLOW,0.3344768439108062,"Action 
Decoder"
LEARN TO FOLLOW,0.3361921097770154,"Concat 4
2"
LEARN TO FOLLOW,0.3379073756432247,"position
goal 0
1"
LEARN TO FOLLOW,0.33962264150943394,"0
0
0
0
0"
LEARN TO FOLLOW,0.34133790737564323,"0
0
0
0
0"
LEARN TO FOLLOW,0.34305317324185247,"0
0
0
0
0"
LEARN TO FOLLOW,0.34476843910806176,"0
0
0
0
0"
LEARN TO FOLLOW,0.346483704974271,"0
1
0
1
1"
LEARN TO FOLLOW,0.3481989708404803,"0
0
0
0
0"
LEARN TO FOLLOW,0.34991423670668953,"0
0
0
0
0"
LEARN TO FOLLOW,0.3516295025728988,"1
0
1
0
0"
LEARN TO FOLLOW,0.35334476843910806,"1
0
0
0
1"
LEARN TO FOLLOW,0.35506003430531735,"1
0
0
0
0"
LEARN TO FOLLOW,0.3567753001715266,"0
0
0
0
0"
LEARN TO FOLLOW,0.3584905660377358,"0
0
0
0
0"
LEARN TO FOLLOW,0.3602058319039451,"0
0
0
0
0"
LEARN TO FOLLOW,0.36192109777015435,"0
0
0
0
0"
LEARN TO FOLLOW,0.36363636363636365,"0
0
1
0
0"
LEARN TO FOLLOW,0.3653516295025729,"local obstacles
local agents
waypoint"
LEARN TO FOLLOW,0.3670668953687822,global map
LEARN TO FOLLOW,0.3687821612349914,Heuristic
LEARN TO FOLLOW,0.3704974271012007,Sub-goal
LEARN TO FOLLOW,0.37221269296740994,Setter
LEARN TO FOLLOW,0.37392795883361923,"position, goal"
LEARN TO FOLLOW,0.37564322469982847,learning-based policy
LEARN TO FOLLOW,0.37735849056603776,setting subgoal
LEARN TO FOLLOW,0.379073756432247,"Figure 2: The general pipeline of the FOLLOWER approach. The action selection policy for each
agent is decentralized and consists of two modules: Heuristic Sub-goal Decider, which address
long-term path planning problem and Learning-based Policy optimization module, which addresses
the short-term conflict resolution task."
LEARN TO FOLLOW,0.38078902229845624,"First, a Heuristic Sub-goal Decider is used to construct an individual path to the goal and choose a
192"
LEARN TO FOLLOW,0.38250428816466553,"waypoint on this path that becomes the agent’s local goal, which we also call a sub-goal. Second, a
193"
LEARN TO FOLLOW,0.38421955403087477,"Learnable Follower is invoked to reach the sub-goal. This module decides which actions to take at
194"
LEARN TO FOLLOW,0.38593481989708406,"each time step until the sub-goal is reached or until the agent gets too far away from it. In both cases
195"
LEARN TO FOLLOW,0.3876500857632933,"the sub-goal decider in called again and the cycle repeats.
196"
HEURISTIC SUB-GOAL DECIDER,0.3893653516295026,"4.1
Heuristic Sub-goal Decider
197"
HEURISTIC SUB-GOAL DECIDER,0.3910806174957118,"In essence the purpose of this module is to provide a waypoint (sub-goal) in the vicinity of the agent,
198"
HEURISTIC SUB-GOAL DECIDER,0.3927958833619211,"pursuing which will allow agent to progress towards its (global) goal. A conventional heuristic
199"
HEURISTIC SUB-GOAL DECIDER,0.39451114922813035,"search algorithm, i.e. A*, is used to construct a path to the latter from the current location. Global
200"
HEURISTIC SUB-GOAL DECIDER,0.39622641509433965,"information on the locations of the static obstacles, i.e. the map, is used for pathfinding. The other
201"
HEURISTIC SUB-GOAL DECIDER,0.3979416809605489,"agents are not taken into account at this stage, thus the constructed path may go through them. Once
202"
HEURISTIC SUB-GOAL DECIDER,0.3996569468267582,"the path is built a node node located K steps away from the current position is chosen as the current
203"
HEURISTIC SUB-GOAL DECIDER,0.4013722126929674,"sub-goal. Here K is the user-specified parameter.
204"
HEURISTIC SUB-GOAL DECIDER,0.40308747855917665,"An crucial design choice for this module is what individual path to build. On the one hand, A* finds
205"
HEURISTIC SUB-GOAL DECIDER,0.40480274442538594,"the shortest (individual) path to the goal. On the other, as we noted empirically, when the number
206"
HEURISTIC SUB-GOAL DECIDER,0.4065180102915952,"of agents is very high and each agent is following the shortest path, congestion often arise in the
207"
HEURISTIC SUB-GOAL DECIDER,0.40823327615780447,"bottleneck parts of the map, such as corridors or doors. This degrades the performance dramatically.
208"
HEURISTIC SUB-GOAL DECIDER,0.4099485420240137,"To this end we suggest to search not for the shortest paths but rather the evenly dispersed paths. This
209"
HEURISTIC SUB-GOAL DECIDER,0.411663807890223,"is implemented as follows.
210"
HEURISTIC SUB-GOAL DECIDER,0.41337907375643224,"At each time step the information on the locations of the locally observed agents is stored, in what we
211"
HEURISTIC SUB-GOAL DECIDER,0.41509433962264153,"call a heatmap, and used to compute the additional transition costs for individual pathfinding. The
212"
HEURISTIC SUB-GOAL DECIDER,0.41680960548885077,"number of times the other agents were seen in a certain location (grid cell in our experiments) is
213"
HEURISTIC SUB-GOAL DECIDER,0.41852487135506006,"multiplied by the user-defined parameter C and added to the transition cost to that location. Intuitively,
214"
HEURISTIC SUB-GOAL DECIDER,0.4202401372212693,"if many agents are noticed in particular areas of the map the transition costs of the latter are increased
215"
HEURISTIC SUB-GOAL DECIDER,0.4219554030874786,"so A* will avoid them. This balances the distribution of the agents’ paths across the map and
216"
HEURISTIC SUB-GOAL DECIDER,0.4236706689536878,"contributes to collision avoidance. Indeed, each agent maintains its own heatmap and performs
217"
HEURISTIC SUB-GOAL DECIDER,0.42538593481989706,"pathfinding individually, thus the assumption that the agents do not share any data is not violated.
218"
LEARNABLE FOLLOWER,0.42710120068610635,"4.2
Learnable Follower
219"
LEARNABLE FOLLOWER,0.4288164665523156,"This module implements a learnable policy that is tailored to achieve the provided sub-goals while
220"
LEARNABLE FOLLOWER,0.4305317324185249,"avoiding collision with the other agents. The policy function is approximated by a (deep) neural
221"
LEARNABLE FOLLOWER,0.4322469982847341,"network and, as the agents are assumed to be homogeneous, a single network is utilized during
222"
LEARNABLE FOLLOWER,0.4339622641509434,"training (a technique referred to as policy sharing). This approach is beneficial for complex tasks
223"
LEARNABLE FOLLOWER,0.43567753001715265,"and large maps where it would be infeasible to learn a separate neural network for each agent, as the
224"
LEARNABLE FOLLOWER,0.43739279588336194,"number of parameters increases linearly with the number of agents.
225"
LEARNABLE FOLLOWER,0.4391080617495712,"The input to the neural network represents the local observation of an agent and is comprised of a
226"
LEARNABLE FOLLOWER,0.44082332761578047,"3 × m × m tensor, where m × m is the observation range. The channels of the tensor encode the
227"
LEARNABLE FOLLOWER,0.4425385934819897,"locations of the static obstacles, other agents and the current sub-goal respectively – see Fig. 2. If the
228"
LEARNABLE FOLLOWER,0.444253859348199,"latter is out of the agent’s field of view, it is projected into the nearest cell (similarily to [19]).
229"
LEARNABLE FOLLOWER,0.44596912521440824,"The input goes through the Spatial Encoder first, then Concat block combines both spatial and
230"
LEARNABLE FOLLOWER,0.44768439108061747,"non-spatial features (the position of the agent on a map and its global goal). This is followed by an
231"
LEARNABLE FOLLOWER,0.44939965694682676,"Action Decoder that uses the function f for approximating the state using observation history (the
232"
LEARNABLE FOLLOWER,0.451114922813036,"positions of other agents and the presence of obstacles) to make a decision. The network’s output is a
233"
LEARNABLE FOLLOWER,0.4528301886792453,"probability distribution over possible actions.
234"
LEARNABLE FOLLOWER,0.45454545454545453,"The whole pipeline is trained with a policy optimization algorithm using the reward function separated
235"
LEARNABLE FOLLOWER,0.4562607204116638,"into the two components: upon reaching a sub-goal an agent receives a small intrinsic positive reward
236"
LEARNABLE FOLLOWER,0.45797598627787306,"of rs, whose value was determined empirically; upon reaching the global goal a conventional RL
237"
LEARNABLE FOLLOWER,0.45969125214408235,"reward rg = 1 is received. If while reaching the current goal the agent goes too far away from it, the
238"
LEARNABLE FOLLOWER,0.4614065180102916,"heuristic sub-goal decider is invoked again. This mechanism is helpful in scenarios when to progress
239"
LEARNABLE FOLLOWER,0.4631217838765009,"towards the global goal it is actually more beneficial to make a detour to avoid congestion with the
240"
LEARNABLE FOLLOWER,0.4648370497427101,"other agents. Practically wise, a goal is recalculated if the agent’s distance from its target exceeds a
241"
LEARNABLE FOLLOWER,0.4665523156089194,"certain threshold, which is determined by a hyperparameter H.
242"
LEARNABLE FOLLOWER,0.46826758147512865,"The task of the learning process is to optimize the shared policy πu
θ (i.e. the same policy for each
243"
LEARNABLE FOLLOWER,0.4699828473413379,"agent) to maximize the expected cumulative reward. During the training process, rollouts (sequences
244"
LEARNABLE FOLLOWER,0.4716981132075472,"of observation and action pairs) are gathered asynchronously from multiple environments with
245"
LEARNABLE FOLLOWER,0.4734133790737564,"varying numbers of agents. The shared policy πθ (actor network) is continually updated using the
246"
LEARNABLE FOLLOWER,0.4751286449399657,PPO clipped loss [36]: maxθ 1
LEARNABLE FOLLOWER,0.47684391080617494,"N
Pn
u=1
P j
P"
LEARNABLE FOLLOWER,0.47855917667238423,"τ u πθ(au|τ u) ˆAclip(τ u, au) −βH(πθ(·|τ u)).
247"
LEARNABLE FOLLOWER,0.48027444253859347,"Here, β is a coefficient that controls the entropy H, and ˆA denotes the unclipped advantage function
248"
LEARNABLE FOLLOWER,0.48198970840480276,"calculated using returns ˆR for each step t with observation history τ u: ˆA(τ u, au) = ˆRu
t −V ϕ(τ u),
249"
LEARNABLE FOLLOWER,0.483704974271012,"with ˆRu
t = PT −1
k=0 γkru
t+k. Here, we have a shared critic value function Vϕ, which is optimized using
250"
LEARNABLE FOLLOWER,0.4854202401372213,the following equation: minϕ 1
LEARNABLE FOLLOWER,0.48713550600343053,"N
Pn
u=1
P j
P"
LEARNABLE FOLLOWER,0.4888507718696398,"τ u

Vϕ(τ u) −ˆRu
t
2
.
251"
LEARNABLE FOLLOWER,0.49056603773584906,"In practice, the observation history τ u is effectively modeled using a recurrent neural network (RNN)
252"
LEARNABLE FOLLOWER,0.4922813036020583,"integrated into the actor and critic networks. The actor network is parameterized by θ, while the critic
253"
LEARNABLE FOLLOWER,0.4939965694682676,"network is parameterized by ϕ. In our approach, we specifically utilize the GRU architecture [37].
254"
LEARNABLE FOLLOWER,0.4957118353344768,"The introduced intrinsic reward function allows the efficient training of an agent using relatively short
255"
LEARNABLE FOLLOWER,0.4974271012006861,"rollouts, as evidenced by our experimental results, which demonstrate that a rollout length of 8 is
256"
LEARNABLE FOLLOWER,0.49914236706689535,"sufficient for training. This is crucial for ensuring lifelong learning, as episodes may not have a clear
257"
LEARNABLE FOLLOWER,0.5008576329331046,"ending point.
258"
LEARNABLE FOLLOWER,0.5025728987993139,"During the inference phase, each agent uses a copy of the trained weights, and other parameters
259"
LEARNABLE FOLLOWER,0.5042881646655232,"remain unchanged. The proposed FOLLOWER scheme, despite its simplicity, allows the agent to
260"
LEARNABLE FOLLOWER,0.5060034305317325,"separate the two components of the overall policy transparently and does not require the involvement
261"
LEARNABLE FOLLOWER,0.5077186963979416,"of any expert data for training. The learning process is end-to-end and the number of hyperparameters
262"
LEARNABLE FOLLOWER,0.5094339622641509,"(such as K and H) that affect the result is relatively small. Finally, the reward function used is simple
263"
LEARNABLE FOLLOWER,0.5111492281303602,"and does not require involved manual shaping.
264"
EXPERIMENTAL EVALUATION,0.5128644939965694,"5
Experimental Evaluation
265"
EXPERIMENTAL EVALUATION,0.5145797598627787,"To evaluate the efficiency of the proposed method2, we have conducted a set of experiments, compar-
266"
EXPERIMENTAL EVALUATION,0.516295025728988,"ing it with the existing learnable and search-based algorithms on different grid maps. The episode
267"
EXPERIMENTAL EVALUATION,0.5180102915951973,"length was set to 512 in all experiments. The agents field-of-view was 11 × 11. When training
268"
EXPERIMENTAL EVALUATION,0.5197255574614065,"FOLLOWER we used the following values of the reward components: rg = +1 and rs = +0.1. The
269"
EXPERIMENTAL EVALUATION,0.5214408233276158,"Spatial Encoder was realized as ResNet neural model [38], the Concat block – as a Multi-Layer
270"
EXPERIMENTAL EVALUATION,0.5231560891938251,"Perceptron (MLP), and the Action Decoder – as a recurrent neural network, separated for actor
271"
EXPERIMENTAL EVALUATION,0.5248713550600344,"and critic and based on GRU [37]. In the experiments, values of 2 and 10 were used for K and
272"
EXPERIMENTAL EVALUATION,0.5265866209262435,"H respectively. The weighting coefficient C for sub-goal setter was set to 0.4. More information
273"
EXPERIMENTAL EVALUATION,0.5283018867924528,"about which (hyper) parameters were tuned and how is provided in the Appendix. After fixing all the
274"
EXPERIMENTAL EVALUATION,0.5300171526586621,"parameters, the final policy was trained using a single TITAN RTX GPU in approximately 1 hour.
275"
EXPERIMENTAL EVALUATION,0.5317324185248714,2We are committed to open-source FOLLOWER.
COMPARISON WITH THE LEARNABLE METHODS,0.5334476843910806,"5.1
Comparison With the Learnable Methods
276"
COMPARISON WITH THE LEARNABLE METHODS,0.5351629502572899,"In the first series of experiments, we have compared FOLLOWER with the two state-of-the-art
277"
COMPARISON WITH THE LEARNABLE METHODS,0.5368782161234992,"learnable MAPF solvers, i.e. PRIMAL2 [19] and PICO [22]. Similarly to FOLLOWER, both are
278"
COMPARISON WITH THE LEARNABLE METHODS,0.5385934819897084,"decentralized and rely on the local observations of the other agents. However, PRIMAL2 assumes
279"
COMPARISON WITH THE LEARNABLE METHODS,0.5403087478559176,"that the local observations contain not only information about the current locations of the agents but
280"
COMPARISON WITH THE LEARNABLE METHODS,0.5420240137221269,"also about their goals on the global map. PICO assumes that the agents can communicate, through
281"
COMPARISON WITH THE LEARNABLE METHODS,0.5437392795883362,"selected central agent. Recall that our solver has access neither to any information about the other
282"
COMPARISON WITH THE LEARNABLE METHODS,0.5454545454545454,"agents except their current locations nor to communication between the agents.
283"
COMPARISON WITH THE LEARNABLE METHODS,0.5471698113207547,"2 3264
128
256
Number of Agents 0.0 0.5 1.0 1.5 2.0"
COMPARISON WITH THE LEARNABLE METHODS,0.548885077186964,Average Throughput Mazes
COMPARISON WITH THE LEARNABLE METHODS,0.5506003430531733,"Follower
Primal2"
COMPARISON WITH THE LEARNABLE METHODS,0.5523156089193825,"Figure 3: Average throughput
on maze-like environments.
The shaded area indicates 95%
confidence intervals."
COMPARISON WITH THE LEARNABLE METHODS,0.5540308747855918,"As learnable methods assume training on a certain maps topology,
284"
COMPARISON WITH THE LEARNABLE METHODS,0.5557461406518011,"we use the maps suggested by the authors of the respective baselines
285"
COMPARISON WITH THE LEARNABLE METHODS,0.5574614065180102,"for a fair comparison. Specifically, we compare with PRIMAL2 on
286"
COMPARISON WITH THE LEARNABLE METHODS,0.5591766723842195,"the maze-like maps of size 65 × 65 on which PRIMAL2 was trained,
287"
COMPARISON WITH THE LEARNABLE METHODS,0.5608919382504288,"and we compare with PICO on the maps with random obstacles
288"
COMPARISON WITH THE LEARNABLE METHODS,0.5626072041166381,"described in the PICO paper. The visualizations of the maps are
289"
COMPARISON WITH THE LEARNABLE METHODS,0.5643224699828473,"given in Appendix. We used the readily available weights for PRI-
290"
COMPARISON WITH THE LEARNABLE METHODS,0.5660377358490566,"MAL2 neural network (from the authors’ repository). PICO was
291"
COMPARISON WITH THE LEARNABLE METHODS,0.5677530017152659,"trained by us using the open-source code of its authors. Our method,
292"
COMPARISON WITH THE LEARNABLE METHODS,0.5694682675814752,"FOLLOWER, was trained using the hyperparameters described in
293"
COMPARISON WITH THE LEARNABLE METHODS,0.5711835334476844,"Appendix, and only PRIMAL2 maps were used for training. When
294"
COMPARISON WITH THE LEARNABLE METHODS,0.5728987993138936,"training FOLLOWER, we vary the number of agents in range: 16, 32,
295"
COMPARISON WITH THE LEARNABLE METHODS,0.5746140651801029,"64, 128. After the training phase, we run the solvers on ten different
296"
COMPARISON WITH THE LEARNABLE METHODS,0.5763293310463122,"maze-like/random maps that were not used while training. Each map
297"
COMPARISON WITH THE LEARNABLE METHODS,0.5780445969125214,"was populated with varying numbers of agents: from 2 to 256. The
298"
COMPARISON WITH THE LEARNABLE METHODS,0.5797598627787307,"goals for LMAPF were generated and assigned to agents randomly.
299"
COMPARISON WITH THE LEARNABLE METHODS,0.58147512864494,"FOLLOWER vs. PRIMAL2 results
are depicted on Fig. 3. The OX axis shows the number of
300"
COMPARISON WITH THE LEARNABLE METHODS,0.5831903945111492,"agents and OY axis shows the average throughput. Indeed, when the number of agents is low both
301"
COMPARISON WITH THE LEARNABLE METHODS,0.5849056603773585,"algorithms demonstrate similar results. However, with an increasing number of agents the gap
302"
COMPARISON WITH THE LEARNABLE METHODS,0.5866209262435678,"in performance is getting pronounced. The throughput of the FOLLOWER is 11% better for 128
303"
COMPARISON WITH THE LEARNABLE METHODS,0.5883361921097771,"agents and 17% better for 256 agents. Overall, one can claim that despite having access to less
304"
COMPARISON WITH THE LEARNABLE METHODS,0.5900514579759862,"MAPF-related data FOLLOWER outperforms PRIMAL2 when the number of agents is not low, i.e. in
305"
COMPARISON WITH THE LEARNABLE METHODS,0.5917667238421955,"cases where the potential conflicts between the agents are not rare.
306"
COMPARISON WITH THE LEARNABLE METHODS,0.5934819897084048,"FOLLOWER vs. PICO results
are presented in Table 1. Clearly, FOLLOWER demonstrates a
307"
COMPARISON WITH THE LEARNABLE METHODS,0.5951972555746141,"superior performance across all scenarios. The poor performance of PICO can be attributed to the
308"
COMPARISON WITH THE LEARNABLE METHODS,0.5969125214408233,"inherent difficulties in learning effective communication strategies for prioritizing large number
309"
COMPARISON WITH THE LEARNABLE METHODS,0.5986277873070326,"of agents. Authors of PICO trained their method on 8 agents. We hypothesize that this limited
310"
COMPARISON WITH THE LEARNABLE METHODS,0.6003430531732419,"population size may have impeded the acquisition of knowledge necessary for effective coordination
311"
COMPARISON WITH THE LEARNABLE METHODS,0.6020583190394511,"among a larger number of agents. Both FOLLOWER and PRIMAL2 outperform PICO showing their
312"
COMPARISON WITH THE LEARNABLE METHODS,0.6037735849056604,"ability to generalize (as they were not trained on PICO type of maps), with FOLLOWER being the
313"
COMPARISON WITH THE LEARNABLE METHODS,0.6054888507718696,"ultimate winner.
314"
COMPARISON WITH THE LEARNABLE METHODS,0.6072041166380789,"Table 1: The comparison of FOLLOWER with PICO on random maps with different obstacle densities,
taken from PICO evaluation setup."
COMPARISON WITH THE LEARNABLE METHODS,0.6089193825042881,Obstacle Density
COMPARISON WITH THE LEARNABLE METHODS,0.6106346483704974,"Algorithm
Agents
0%
10%
20%
30%"
COMPARISON WITH THE LEARNABLE METHODS,0.6123499142367067,"FOLLOWER
8
0.61 (±0.01)
0.57 (±0.02)
0.49 (±0.04)
0.38 (±0.21)
PRIMAL2
8
0.44 (±0.03)
0.39 (±0.04)
0.3 (±0.05)
0.19 (±0.11)
PICO
8
0.19 (±0.01)
0.18 (±0.03)
0.14 (±0.04)
0.05 (±0.05)"
COMPARISON WITH THE LEARNABLE METHODS,0.614065180102916,"FOLLOWER
16
1.1 (±0.03)
0.96 (±0.05)
0.85 (±0.19)
0.56 (±0.34)
PRIMAL2
16
0.79 (±0.03)
0.67 (±0.06)
0.51 (±0.08)
0.31 (±0.14)
PICO
16
0.31 (±0.03)
0.25 (±0.04)
0.23 (±0.06)
0.08 (±0.06)"
COMPARISON WITH THE LEARNABLE METHODS,0.6157804459691252,"FOLLOWER
32
1.81 (±0.05)
1.45 (±0.15)
1.21 (±0.27)
0.84 (±0.39)
PRIMAL2
32
1.25 (±0.04)
1.02 (±0.11)
0.71 (±0.12)
0.4 (±0.16)
PICO
32
0.46 (±0.05)
0.35 (±0.1)
0.28 (±0.12)
0.12 (±0.09)"
COMPARISON WITH THE LEARNABLE METHODS,0.6174957118353345,"FOLLOWER
64
2.6 (±0.11)
1.88 (±0.33)
1.24 (±0.23)
0.71 (±0.36)
PRIMAL2
64
1.63 (±0.05)
1.15 (±0.15)
0.73 (±0.13)
0.44 (±0.18)
PICO
64
0.42 (±0.07)
0.41 (±0.13)
0.28 (±0.12)
0.11 (±0.1)"
COMPARISON WITH THE LEARNABLE METHODS,0.6192109777015438,"2 32 64
128
256
Number of Agents 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
COMPARISON WITH THE LEARNABLE METHODS,0.6209262435677531,Average Throughput
COMPARISON WITH THE LEARNABLE METHODS,0.6226415094339622,a) MovingAI lak303
COMPARISON WITH THE LEARNABLE METHODS,0.6243567753001715,"Follower
Primal2"
COMPARISON WITH THE LEARNABLE METHODS,0.6260720411663808,"2 32 64
128
256
Number of Agents 0.0 0.5 1.0 1.5"
COMPARISON WITH THE LEARNABLE METHODS,0.62778730703259,Average Throughput
COMPARISON WITH THE LEARNABLE METHODS,0.6295025728987993,b) MovingAI warehouse
COMPARISON WITH THE LEARNABLE METHODS,0.6312178387650086,"Follower
Primal2"
COMPARISON WITH THE LEARNABLE METHODS,0.6329331046312179,"Figure 4: The results on a) lak303d and b) warehouse
maps. The shaded area indicates 95% confidence intervals."
COMPARISON WITH THE LEARNABLE METHODS,0.6346483704974271,"Out of the distribution evaluation.
315"
COMPARISON WITH THE LEARNABLE METHODS,0.6363636363636364,"An important attribute of any learn-
316"
COMPARISON WITH THE LEARNABLE METHODS,0.6380789022298456,"able algorithm is the so-called gen-
317"
COMPARISON WITH THE LEARNABLE METHODS,0.6397941680960549,"eralization, i.e. the ability to solve
318"
COMPARISON WITH THE LEARNABLE METHODS,0.6415094339622641,"problem instances that are not alike
319"
COMPARISON WITH THE LEARNABLE METHODS,0.6432246998284734,"to the ones that were used for training.
320"
COMPARISON WITH THE LEARNABLE METHODS,0.6449399656946827,"We have already seen that FOLLOWER
321"
COMPARISON WITH THE LEARNABLE METHODS,0.6466552315608919,"generalizes better than PRIMAL2 to
322"
COMPARISON WITH THE LEARNABLE METHODS,0.6483704974271012,"the PICO type of maps (random ones).
323"
COMPARISON WITH THE LEARNABLE METHODS,0.6500857632933105,"Now we run an additional test when
324"
COMPARISON WITH THE LEARNABLE METHODS,0.6518010291595198,"we evaluated both algorithms on two
325"
COMPARISON WITH THE LEARNABLE METHODS,0.6535162950257289,"(unseen while learning) maps from the
326"
COMPARISON WITH THE LEARNABLE METHODS,0.6552315608919382,"well-known in the MAPF community
327"
COMPARISON WITH THE LEARNABLE METHODS,0.6569468267581475,"MovingAI benchmark [1]: warehouse-10-20-10-2-1 and lak303d. The former map is 63 × 171
328"
COMPARISON WITH THE LEARNABLE METHODS,0.6586620926243568,"in size and represents the warehouse environment. It is similar to a certain extent to the maze-maps
329"
COMPARISON WITH THE LEARNABLE METHODS,0.660377358490566,"on which FOLLOWER and PRIMAL2 were trained. The latter map is a video-game map that was
330"
COMPARISON WITH THE LEARNABLE METHODS,0.6620926243567753,"downscaled by us to have size 95 × 95. Its topology is quite different from the one of the maps used
331"
COMPARISON WITH THE LEARNABLE METHODS,0.6638078902229846,"for training FOLLOWER and PRIMAL2. The results of these experiments are presented in Fig. 4.
332"
COMPARISON WITH THE LEARNABLE METHODS,0.6655231560891939,"Note that we did not evaluate PICO on out-of-the-distribution maps due to its poor performance in
333"
COMPARISON WITH THE LEARNABLE METHODS,0.6672384219554031,"the previous experiment.
334"
COMPARISON WITH THE LEARNABLE METHODS,0.6689536878216124,"On warehouse-10-20-10-2-1 FOLLOWER and PRIMAL2 demonstrate similar performance, with
335"
COMPARISON WITH THE LEARNABLE METHODS,0.6706689536878216,"our method achieving slightly higher throughput. I.e. the FOLLOWER’S throughput is 9.5% higher
336"
COMPARISON WITH THE LEARNABLE METHODS,0.6723842195540308,"for 128 agents and 4% higher for the 256 agents. The results on lak303d are quite different. First
337"
COMPARISON WITH THE LEARNABLE METHODS,0.6740994854202401,"of all, both algorithms have much lower average throughput compared to maze-like and warehouse
338"
COMPARISON WITH THE LEARNABLE METHODS,0.6758147512864494,"environments. This is expected, as the topology of the game map differs a lot from the latter maps.
339"
COMPARISON WITH THE LEARNABLE METHODS,0.6775300171526587,"Second, the throughput of FOLLOWER is significantly higher, providing another evidence (in addition
340"
COMPARISON WITH THE LEARNABLE METHODS,0.6792452830188679,"to the results on PICO maps) that the generalization ability of our approach is better.
341"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.6809605488850772,"5.2
Comparison With the Centralized Search-based Solver
342"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.6826758147512865,"While most of the learnable approaches compare their results only with other learnable methods, we
343"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.6843910806174958,"have also compared FOLLOWER with the state-of-the-art search-based algorithm for solving LMAPF
344"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.6861063464837049,"– RHCR3 [12]. In contrast to the proposed method, RHCR is a centralized approach that coordinates
345"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.6878216123499142,"all the agents and does not restrict the observation and/or communication abilities of the agents. This
346"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.6895368782161235,"planner has several parameters that influence its performance. We have varied the planning horizon
347"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.6912521440823327,"(2, 5, 10, 20), the re-planning rate (1, 5) and found that the best results are achieved when the first
348"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.692967409948542,"parameter is set to 20 and the second one to 5. The time limit for re-planning was set to either 1 or 10
349"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.6946826758147513,"seconds. The MAPF solver used in RHCR was set to PBS [39]. The rest parameters were left default.
350"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.6963979416809606,"The comparison was conducted on the same warehouse map as in the original paper [12]. The
351"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.6981132075471698,"possible placements of start and goal locations were also restricted in the same way as in the original
352"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.6998284734133791,"paper. The number of agents in this experiment reached 192 as no more agents are able to be placed
353"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7015437392795884,"with the given restrictions to start locations. We generated 10 random instances per each number of
354"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7032590051457976,"agents.
355"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7049742710120068,"Besides RHCR we have also evaluated different versions of our solver. First, we want to assess the
356"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7066895368782161,"impact of the learnable component on the FOLLOWER’S performance. To this end, we removed it
357"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7084048027444254,"from FOLLOWER and let each agent simply plan its path with A* and perform the first action. In case
358"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7101200686106347,"the path can not be found the action is selected randomly. We refer to this approach as Randomized
359"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7118353344768439,"A*. We evaluated two versions of Randomized A*: the one that treats the other agents (within
360"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7135506003430532,"field-of-view) as obstacles and the one that does not. Next, we were interested in how weighting the
361"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7152658662092625,"transition costs for A* affects the FOLLOWER’S performance. Thus, we have created a version of
362"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7169811320754716,"FOLLOWER that doesn’t include this weighting (i.e. C = 0) and all transitions have uniform cost.
363"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7186963979416809,"The results of this experiment are depicted in Fig. 5. Clearly both version of Randomized A* are
364"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7204116638078902,"outperformed by FOLLOWER. This confirms that the learnable policy is crucial to FOLLOWER.
365"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7221269296740995,"The introduced technique of penalizing the transitions to the areas where the other agents are often
366"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7238421955403087,"observed is also important, as in its absence the results of FOLLOWER are on par with Randomized A*.
367"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.725557461406518,3We used an implementation of RHCR from the authors’ repository
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7272727272727273,"Only combining the learnable module with the weighting technique we end with the best-performing
368"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7289879931389366,"method.
369"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7307032590051458,"32
64
96
128
160
192
Number of Agents 1 2 3 4 5"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7324185248713551,Average Throughput
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7341337907375644,a) Average Throughput
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7358490566037735,"Follower
Follower (no weighting)
Randomized A* (agents as obstacles)
Randomized A* (ignoring other agents)
RHCR (1s limit)
RHCR (10s limit)"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7375643224699828,"Figure 5:
Average throughput on
warehouse map. The shaded area in-
dicates 95% confidence intervals."
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7392795883361921,"32
64
128
Number of Agents 1 4 16 64 256"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7409948542024014,Runtime (seconds)
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7427101200686106,b) Runtime
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7444253859348199,"Follower
Follower (no weighting)
Randomized A* (agents as obstacles)
Randomized A* (ignoring other agents)
RHCR (1s limit)
RHCR (10s limit)"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7461406518010292,"Figure
6:
Average
runtime
on
warehouse map."
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7478559176672385,"Both versions of RHCR significantly outperform competitors in instances with up to 128 agents.
370"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7495711835334476,"However, when the number of agents increases to 160 and 192 the performance of RHCR with
371"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7512864493996569,"1s time cap for re-planning degrades dramatically and it gets outperformed by FOLLOWER. This
372"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7530017152658662,"pinpoints the principal limitation of the centralized approach – it does not scale well to large number
373"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7547169811320755,"of agents when the time limit for finding a MAPF solution is imposed. To better understand how the
374"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7564322469982847,"runtime of the evaluated methods is affected by the increasing number of agents, see Fig. 6. Here
375"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.758147512864494,"each data point says how much time on average was spent to solve a LMAPF instance (on a single
376"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7598627787307033,"CPU, 1 thread). Indeed, FOLLOWER scales much better compared to RHCR. Moreover in practice it
377"
COMPARISON WITH THE CENTRALIZED SEARCH-BASED SOLVER,0.7615780445969125,"can be parallelized, i.e. run on each agent individually, while RHCR – can not.
378"
SUMMARY,0.7632933104631218,"5.3
Summary
379"
SUMMARY,0.7650085763293311,"The observed results let us infer the following conclusions. First, the suggested approach outperforms
380"
SUMMARY,0.7667238421955404,"the learnable decentralized competitors, when it comes to a large number of agents and/or maps that
381"
SUMMARY,0.7684391080617495,"are different from the ones used for training. Second, the learnable component of FOLLOWER is
382"
SUMMARY,0.7701543739279588,"crucial to its high performance (as well as the introduced weighting technique). Third, when the
383"
SUMMARY,0.7718696397941681,"number of agents is significantly high, FOLLOWER can outperform the centralized LMAPF solver
384"
SUMMARY,0.7735849056603774,"when a (reasonable) time cap for the latter is introduced.
385"
CONCLUSION,0.7753001715265866,"6
Conclusion
386"
CONCLUSION,0.7770154373927959,"This study addresses the challenging problem of decentralized lifelong multi-agent pathfinding. The
387"
CONCLUSION,0.7787307032590052,"proposed FOLLOWER approach utilizes a combination of a planning algorithm for constructing a
388"
CONCLUSION,0.7804459691252144,"long-term plan and reinforcement learning for reaching short-term sub-goals and resolving local
389"
CONCLUSION,0.7821612349914236,"conflicts. The proposed method consistently outperforms decentralized learnable competitors in
390"
CONCLUSION,0.7838765008576329,"challenging scenarios. Moreover, our approach can show better results, compared to state-of-the-art
391"
CONCLUSION,0.7855917667238422,"centralized planner in certain setups. Directions for future research may include: enriching the action
392"
CONCLUSION,0.7873070325900514,"space of the agents, handling uncertain observations and external (stochastic) events.
393"
LIMITATIONS,0.7890222984562607,"7
Limitations
394"
LIMITATIONS,0.79073756432247,"As in many other works on that topic (including the ones we compare with) we rely on the following
395"
LIMITATIONS,0.7924528301886793,"assumptions. The map of the environment is accurate and the configuration of the static obstacles
396"
LIMITATIONS,0.7941680960548885,"does not change. The agents are assumed to have perfect localization and mapping abilities. The
397"
LIMITATIONS,0.7958833619210978,"agents execute actions accurately and their moves are synchronized. All these may be considered as
398"
LIMITATIONS,0.7975986277873071,"the limitations as in real world, e.g. in robotic applications, many of the assumptions do not hold.
399"
REFERENCES,0.7993138936535163,"References
400"
REFERENCES,0.8010291595197255,"[1] Roni Stern, Nathan R Sturtevant, Ariel Felner, Sven Koenig, Hang Ma, Thayne T Walker,
401"
REFERENCES,0.8027444253859348,"Jiaoyang Li, Dor Atzmon, Liron Cohen, TK Satish Kumar, et al. Multi-agent pathfinding:
402"
REFERENCES,0.8044596912521441,"Definitions, variants, and benchmarks. In Proceedings of the 12th Annual Symposium on
403"
REFERENCES,0.8061749571183533,"Combinatorial Search (SoCS 2019), pages 151–158, 2019.
404"
REFERENCES,0.8078902229845626,"[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei a Rusu, Joel Veness, Marc G
405"
REFERENCES,0.8096054888507719,"Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Pe-
406"
REFERENCES,0.8113207547169812,"tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan
407"
REFERENCES,0.8130360205831904,"Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement
408"
REFERENCES,0.8147512864493996,"learning. Nature, 518(7540):529–533, 2015.
409"
REFERENCES,0.8164665523156089,"[3] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob
410"
REFERENCES,0.8181818181818182,"Foerster, and Shimon Whiteson. QMIX: Monotonic value function factorisation for deep
411"
REFERENCES,0.8198970840480274,"multi-agent reinforcement Learning. In 35th International Conference on Machine Learning,
412"
REFERENCES,0.8216123499142367,"ICML 2018, volume 10, pages 6846–6859, 2018.
413"
REFERENCES,0.823327615780446,"[4] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with
414"
REFERENCES,0.8250428816466552,"discrete world models. In ICLR, 2021.
415"
REFERENCES,0.8267581475128645,"[5] Stephanie Milani, Nicholay Topin, Brandon Houghton, William H. Guss, Sharada P. Mohanty,
416"
REFERENCES,0.8284734133790738,"Oriol Vinyals, and Noboru Sean Kuno. The MineRL Competition on Sample-Efficient Rein-
417"
REFERENCES,0.8301886792452831,"forcement Learning Using Human Priors: A Retrospective. In NeurIPS Competition track,
418"
REFERENCES,0.8319039451114922,"2020.
419"
REFERENCES,0.8336192109777015,"[6] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering Diverse Domains
420"
REFERENCES,0.8353344768439108,"through World Models. 2023.
421"
REFERENCES,0.8370497427101201,"[7] Minghua Liu, Hang Ma, Jiaoyang Li, and Sven Koenig. Task and path planning for multi-agent
422"
REFERENCES,0.8387650085763293,"pickup and delivery. In Proceedings of the 18th International Conference on Autonomous
423"
REFERENCES,0.8404802744425386,"Agents and Multiagent Systems (AAMAS 2019), pages 1152–1160, 2019.
424"
REFERENCES,0.8421955403087479,"[8] Zhe Chen, Javier Alonso-Mora, Xiaoshan Bai, Daniel D Harabor, and Peter J Stuckey. Integrated
425"
REFERENCES,0.8439108061749572,"task assignment and path planning for capacitated multi-agent pickup and delivery. IEEE
426"
REFERENCES,0.8456260720411664,"Robotics and Automation Letters, 6(3):5816–5823, 2021.
427"
REFERENCES,0.8473413379073756,"[9] Hang Ma, Jiaoyang Li, TK Kumar, and Sven Koenig. Lifelong multi-agent path finding for
428"
REFERENCES,0.8490566037735849,"online pickup and delivery tasks. In Proceedings of the 16th Conference on Autonomous Agents
429"
REFERENCES,0.8507718696397941,"and MultiAgent Systems (AAMAS 2017), pages 837–845, 2017.
430"
REFERENCES,0.8524871355060034,"[10] H. Ma, W. Hönig, T. K. S. Kumar, N. Ayanian, and S. Koenig. Lifelong path planning with
431"
REFERENCES,0.8542024013722127,"kinematic constraints for multi-agent pickup and delivery. In Proceedings of the 33rd AAAI
432"
REFERENCES,0.855917667238422,"Conference on Artificial Intelligence (AAAI 2019), pages 7651–7658, 2019.
433"
REFERENCES,0.8576329331046312,"[11] Keisuke Okumura, Manao Machida, Xavier Défago, and Yasumasa Tamura. Priority inheritance
434"
REFERENCES,0.8593481989708405,"with backtracking for iterative multi-agent path finding. In Proceedings of the 28th International
435"
REFERENCES,0.8610634648370498,"Joint Conference on Artificial Intelligence (IJCAI 2019), pages 535–542, 2019.
436"
REFERENCES,0.8627787307032591,"[12] Jiaoyang Li, Andrew Tinka, Scott Kiesel, Joseph W Durham, TK Satish Kumar, and Sven
437"
REFERENCES,0.8644939965694682,"Koenig. Lifelong multi-agent path finding in large-scale warehouses. In Proceedings of the
438"
REFERENCES,0.8662092624356775,"35th AAAI Conference on Artificial Intelligence (AAAI 2021), pages 11272–11281, 2021.
439"
REFERENCES,0.8679245283018868,"[13] Vladimir J. Lumelsky and KR Harinarayan. Decentralized motion planning for multiple mobile
440"
REFERENCES,0.869639794168096,"robots: The cocktail party model. Autonomous Robots, 4(1):121–135, 1997.
441"
REFERENCES,0.8713550600343053,"[14] Jur Van den Berg, Ming Lin, and Dinesh Manocha. Reciprocal velocity obstacles for real-time
442"
REFERENCES,0.8730703259005146,"multi-agent navigation. In Proceedings of The 2008 IEEE International Conference on Robotics
443"
REFERENCES,0.8747855917667239,"and Automation (ICRA 2008), pages 1928–1935. IEEE, 2008.
444"
REFERENCES,0.8765008576329331,"[15] Hai Zhu, Bruno Brito, and Javier Alonso-Mora. Decentralized probabilistic multi-robot collision
445"
REFERENCES,0.8782161234991424,"avoidance using buffered uncertainty-aware voronoi cells. Autonomous Robots, 46(2):401–420,
446"
REFERENCES,0.8799313893653516,"2022.
447"
REFERENCES,0.8816466552315609,"[16] Ko-Hsin Cindy Wang and Adi Botea. Mapp: a scalable multi-agent path planning algorithm
448"
REFERENCES,0.8833619210977701,"with tractability and completeness guarantees. Journal of Artificial Intelligence Research,
449"
REFERENCES,0.8850771869639794,"42:55–90, 2011.
450"
REFERENCES,0.8867924528301887,"[17] Boris de Wilde, Adriaan W ter Mors, and Cees Witteveen. Push and rotate: cooperative multi-
451"
REFERENCES,0.888507718696398,"agent path planning. In Proceedings of the 12th International Conference on Autonomous
452"
REFERENCES,0.8902229845626072,"Agents and Multiagent Systems (AAMAS 2013), pages 87–94, 2013.
453"
REFERENCES,0.8919382504288165,"[18] Guillaume Sartoretti, Justin Kerr, Yunfei Shi, Glenn Wagner, TK Satish Kumar, Sven Koenig,
454"
REFERENCES,0.8936535162950258,"and Howie Choset. Primal: Pathfinding via reinforcement and imitation multi-agent learning.
455"
REFERENCES,0.8953687821612349,"IEEE Robotics and Automation Letters, 4(3):2378–2385, 2019.
456"
REFERENCES,0.8970840480274442,"[19] Mehul Damani, Zhiyao Luo, Emerson Wenzel, and Guillaume Sartoretti. Primal _2: Pathfinding
457"
REFERENCES,0.8987993138936535,"via reinforcement and imitation multi-agent learning-lifelong. IEEE Robotics and Automation
458"
REFERENCES,0.9005145797598628,"Letters, 6(2):2666–2673, 2021.
459"
REFERENCES,0.902229845626072,"[20] Benjamin Riviere, Wolfgang Hönig, Yisong Yue, and Soon-Jo Chung. Glas: Global-to-local
460"
REFERENCES,0.9039451114922813,"safe autonomy synthesis for multi-robot motion planning with end-to-end learning. IEEE
461"
REFERENCES,0.9056603773584906,"Robotics and Automation Letters, 5(3):4249–4256, 2020.
462"
REFERENCES,0.9073756432246999,"[21] Ziyuan Ma, Yudong Luo, and Hang Ma. Distributed heuristic multi-agent path finding with
463"
REFERENCES,0.9090909090909091,"communication. In 2021 IEEE International Conference on Robotics and Automation (ICRA),
464"
REFERENCES,0.9108061749571184,"pages 8699–8705. IEEE, 2021.
465"
REFERENCES,0.9125214408233276,"[22] Wenhao Li, Hongjun Chen, Bo Jin, Wenzhe Tan, Hong Zha, and Xiangfeng Wang. Multi-
466"
REFERENCES,0.9142367066895368,"agent path finding with prioritized communication learning. 2022 International Conference on
467"
REFERENCES,0.9159519725557461,"Robotics and Automation (ICRA), pages 10695–10701, 2022.
468"
REFERENCES,0.9176672384219554,"[23] Annie Wong, Thomas Bäck, Anna V. Kononova, and Aske Plaat. Deep multiagent reinforcement
469"
REFERENCES,0.9193825042881647,"learning: challenges and directions. Artificial Intelligence Review, (0123456789), oct 2022.
470"
REFERENCES,0.9210977701543739,"[24] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas
471"
REFERENCES,0.9228130360205832,"Nardelli, Tim G.J. Rudner, Chia Man Hung, Philip H.S. Torr, Jakob Foerster, and Shimon
472"
REFERENCES,0.9245283018867925,"Whiteson. The StarCraft multi-agent challenge. In Proceedings of the International Joint
473"
REFERENCES,0.9262435677530018,"Conference on Autonomous Agents and Multiagent Systems, AAMAS, volume 4, pages 2186–
474"
REFERENCES,0.9279588336192109,"2188, 2019.
475"
REFERENCES,0.9296740994854202,"[25] Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The Surprising
476"
REFERENCES,0.9313893653516295,"Effectiveness of PPO in Cooperative Multi-Agent Games. 2021.
477"
REFERENCES,0.9331046312178388,"[26] Alexey Skrynnik, Alexandra Yakovleva, Vasilii Davydov, Konstantin Yakovlev, and Aleksandr I.
478"
REFERENCES,0.934819897084048,"Panov. Hybrid Policy Learning for Multi-Agent Pathfinding. IEEE Access, 9:126034–126047,
479"
REFERENCES,0.9365351629502573,"2021.
480"
REFERENCES,0.9382504288164666,"[27] Mike Wesselhöft, Johannes Hinckeldeyn, and Jochen Kreutzfeldt. Controlling fleets of au-
481"
REFERENCES,0.9399656946826758,"tonomous mobile robots with reinforcement learning: A brief survey. Robotics, 11(5), 2022.
482"
REFERENCES,0.9416809605488851,"[28] Jixuan Zhi and Jyh-Ming Lien. Learning to herd agents amongst obstacles: Training robust
483"
REFERENCES,0.9433962264150944,"shepherding behaviors using deep reinforcement learning. IEEE Robotics and Automation
484"
REFERENCES,0.9451114922813036,"Letters, 6(2):4163–4168, 2021.
485"
REFERENCES,0.9468267581475128,"[29] Syed Irfan Ali Meerza, Moinul Islam, and Md. Mohiuddin Uzzal. Q-learning based particle
486"
REFERENCES,0.9485420240137221,"swarm optimization algorithm for optimal path planning of swarm of mobile robots. In 2019
487"
REFERENCES,0.9502572898799314,"1st International Conference on Advances in Science, Engineering and Robotics Technology
488"
REFERENCES,0.9519725557461407,"(ICASERT), pages 1–5, 2019.
489"
REFERENCES,0.9536878216123499,"[30] Emanuele Vitolo, Alberto San Miguel, Javier Civera, and Cristian Mahulea. Performance
490"
REFERENCES,0.9554030874785592,"evaluation of the dyna-q algorithm for robot navigation. In 2018 IEEE 14th International
491"
REFERENCES,0.9571183533447685,"Conference on Automation Science and Engineering (CASE), pages 322–327, 2018.
492"
REFERENCES,0.9588336192109777,"[31] Bo Li and Hongbin Liang. Multi-robot path planning method based on prior knowledge and
493"
REFERENCES,0.9605488850771869,"q-learning algorithms. Journal of Physics: Conference Series, 1624(4):042008, oct 2020.
494"
REFERENCES,0.9622641509433962,"[32] Hyansu Bae, Gidong Kim, Jonguk Kim, Dianwei Qian, and Sukgyu Lee. Multi-robot path
495"
REFERENCES,0.9639794168096055,"planning method using reinforcement learning. Applied Sciences, 9(15), 2019.
496"
REFERENCES,0.9656946826758147,"[33] Zuxin Liu, Baiming Chen, Hongyi Zhou, Guru Koushik, Martial Hebert, and Ding Zhao.
497"
REFERENCES,0.967409948542024,"Mapper: Multi-agent path planning with evolutionary reinforcement learning in mixed dynamic
498"
REFERENCES,0.9691252144082333,"environments. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems
499"
REFERENCES,0.9708404802744426,"(IROS), pages 11748–11754, 2020.
500"
REFERENCES,0.9725557461406518,"[34] Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity
501"
REFERENCES,0.9742710120068611,"of decentralized control of markov decision processes. Mathematics of operations research,
502"
REFERENCES,0.9759862778730704,"27(4):819–840, 2002.
503"
REFERENCES,0.9777015437392796,"[35] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in
504"
REFERENCES,0.9794168096054888,"partially observable stochastic domains. Artificial Intelligence, 101(1-2):99–134, may 1998.
505"
REFERENCES,0.9811320754716981,"[36] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
506"
REFERENCES,0.9828473413379074,"Policy Optimization Algorithms. pages 1–12, 2017.
507"
REFERENCES,0.9845626072041166,"[37] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation
508"
REFERENCES,0.9862778730703259,"of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555,
509"
REFERENCES,0.9879931389365352,"2014.
510"
REFERENCES,0.9897084048027445,"[38] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
511"
REFERENCES,0.9914236706689536,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
512"
REFERENCES,0.9931389365351629,"pages 770–778, 2016.
513"
REFERENCES,0.9948542024013722,"[39] Hang Ma, Daniel Harabor, Peter J Stuckey, Jiaoyang Li, and Sven Koenig. Searching with
514"
REFERENCES,0.9965694682675815,"consistent prioritization for multi-agent path finding. In Proceedings of the AAAI Conference
515"
REFERENCES,0.9982847341337907,"on Artificial Intelligence, volume 33, pages 7643–7650, 2019.
516"
