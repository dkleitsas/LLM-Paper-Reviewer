Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0008347245409015025,"Minwise hashing (MinHash) is an important and practical algorithm for generat-
ing random hashes to approximate the Jaccard (resemblance) similarity in mas-
sive binary (0/1) data. The basic theory of MinHash requires applying hundreds
or even thousands of independent random permutations to each data vector in the
dataset, in order to obtain reliable results for (e.g.,) building large-scale learning
models or approximate near neighbor search in massive data. In this paper, we
propose Circulant MinHash (C-MinHash) and provide the surprising theoret-
ical results that using only two independent random permutations in a circulant
manner leads to uniformly smaller Jaccard estimation variance than that of the
classical MinHash with K independent permutations. Experiments are conducted
to show the effectiveness of the proposed method. We also analyze a more conve-
nient C-MinHash variant which reduces two permutations to just one, with exten-
sive numerical results to validate that it achieves essentially the same estimation
accuracy as using two permutations with rigorous theory."
INTRODUCTION,0.001669449081803005,"1
INTRODUCTION"
INTRODUCTION,0.0025041736227045075,"Given two D-dimensional binary vectors v, w âˆˆ{0, 1}D, the Jaccard similarity is deï¬ned as"
INTRODUCTION,0.00333889816360601,"J(v, w) =
PD
i=1 1{vi = wi = 1}
PD
i=1 1{vi + wi â‰¥1}
,
(1)"
INTRODUCTION,0.004173622704507512,"which is a commonly used similarity metric in machine learning and web search applications. The
vectors v and w can also be viewed as two sets of items (which represent the locations of non-zero
entries), the Jaccard similarity can be equivalently viewed as the size of set intersection over the size
of set union. The well-known method of â€œminwise hashingâ€ (MinHash) (Broder, 1997; Broder et al.,
1997; 1998; Li & Church, 2005; Li & KÃ¶nig, 2011) is a standard technique for computing/estimating
the Jaccard similarity in massive binary datasets, with numerous applications such as near neighbor
search, duplicate detection, malware detection, web search, clustering, large-scale learning, social
networks, computer vision, etc. (Charikar, 2002; Fetterly et al., 2003; Henzinger, 2006; Das et al.,
2007; Buehrer & Chellapilla, 2008; Bendersky & Croft, 2009; Chierichetti et al., 2009; Lee et al.,
2010; Li et al., 2011; Deng et al., 2012; Chum & Matas, 2012; Shrivastava & Li, 2012; He et al.,
2013; Tamersoy et al., 2014; Shrivastava & Li, 2014; Zamora et al., 2016)."
INTRODUCTION,0.005008347245409015,"1.1
A REVIEW OF MINWISE HASHING (MINHASH)"
INTRODUCTION,0.005843071786310518,"Algorithm 1 Minwise-hashing (MinHash)
Input: Binary data vector v âˆˆ{0, 1}D,
K independent permutations Ï€1, ..., Ï€K: [D] â†’[D]."
INTRODUCTION,0.00667779632721202,"Output: K hash values h1(v), ..., hK(v).
For k = 1 to K
hk(v) â†mini:viÌ¸=0 Ï€k(i)
End For"
INTRODUCTION,0.007512520868113523,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.008347245409015025,"For simplicity, Algorithm 1 considers just one vector v âˆˆ{0, 1}D. In order to generate K hash val-
ues for v, we assume K independent permutations: Ï€1, ..., Ï€K : [D] 7â†’[D]. For each permutation,
the hash value is the ï¬rst non-zero location in the permuted vector, i.e., hk(v) = mini:viÌ¸=0 Ï€k(i),
âˆ€k = 1, ..., K. Similarly, for another binary vector w âˆˆ{0, 1}D, using the same K permutations,
we can also obtain K hash values, hk(w). The estimator of J(v, w) is simply"
INTRODUCTION,0.009181969949916527,"Ë†JMH(v, w) = 1 K K
X"
INTRODUCTION,0.01001669449081803,"k=1
1{hk(v) = hk(w)},
(2)"
INTRODUCTION,0.010851419031719533,"where 1{Â·} is the indicator function. By fundamental probability and the independence among the
permutations, it is easy to show that"
INTRODUCTION,0.011686143572621035,"E[ Ë†JMH] = J,
V ar[ Ë†JMH] = J(1 âˆ’J)"
INTRODUCTION,0.012520868113522538,"K
.
(3)"
INTRODUCTION,0.01335559265442404,"How large is K? The answer depends on the application domains. For example, for training large-
scale machine learning models, it appears that K = 512 or K = 1024 might be sufï¬cient (Li et al.,
2011). However, for approximate near neighbor search using many hash tables (Indyk & Motwani,
1998), it is likely that K might have to be much larger than 1024 (Shrivastava & Li, 2012; 2014)."
INTRODUCTION,0.014190317195325543,"In the early work of MinHash (Broder, 1997; Broder et al., 1997), actually only one permutation
was used by storing the ï¬rst K non-zero locations after the permutation. Later Li & Church (2005)
proposed better estimators to improve the estimation accuracy. The major drawback of the original
scheme was that the hashed values did not form a metric space (e.g., satisfying the triangle inequal-
ity) and hence could not be used in many algorithms/applications. We believe this was the main
reason why the original authors moved to using K permutations (Broder et al., 1998)."
OUTLINE OF MAIN RESULTS,0.015025041736227046,"1.2
OUTLINE OF MAIN RESULTS"
OUTLINE OF MAIN RESULTS,0.015859766277128547,"From K Permutations to two. The main idea of this work, is to replace the independent per-
mutations in MinHash with â€œcirculantâ€ permutations. Thus, we name the proposed framework C-
MinHash (circulant MinHash). The â€œcirculantâ€ trick was used in the literature of random projec-
tions. For example, Yu et al. (2017) showed that using circulant projections hurts the estimation
accuracy, but not by too much when the data are sparse. In Section 3, we present some (perhaps
surprising) theoretical ï¬ndings that we just need 2 permutations in MinHash and the results (esti-
mation variances) are even more accurate. Basically, with the initial permutation (denoted by Ïƒ),
we randomly shufï¬‚e the data to break whatever structure which might exist in the original data,
and then the second permutation (denoted by Ï€) is applied and re-used K times to generate K
hash values, via circulation. This method is called C-MinHash-(Ïƒ, Ï€). Before that, in Section 2, we
analyze a simpler variant C-MinHash-(0, Ï€) without initial permutation Ïƒ. Although it is not our
recommended method, our analysis for C-MinHash-(0, Ï€) provides the necessary preparation for
later methods and the intuition to understand the need for the initial permutation."
OUTLINE OF MAIN RESULTS,0.01669449081803005,"From Two Permutations to one. In Section 5, we provide a more convenient variant, C-MinHash-
(Ï€, Ï€), that only needs one permutation Ï€ for both pre-processing and hashing. Although the theo-
retical analysis becomes very complicated, we are able to explicitly write down the expectation of
the estimator, which is no longer unbiased but the bias is extremely small and has essentially no im-
pact on the estimation accuracy (mean square errors). Extensive experiments are provided to verify
that this variant has same estimation accuracy as C-MinHash-(Ïƒ, Ï€) using two permutations."
OUTLINE OF MAIN RESULTS,0.017529215358931552,"In this paper, we mainly focus on presenting the fundamental idea of C-MinHash and the theoretical
analysis on its uniform variance reduction with non-trivial efforts. Besides improving the Jaccard
estimation accuracy when directly implemented in place of MinHash, the proposed C-MinHash
mechanism can also be conveniently used as a tool to improve more MinHash based algorithms,
for example, the One Permutation Hashing (OPH, Li et al. (2012)). When combined with OPH, C-
MinHash can reduce the required one permutation to effectively 1/K permutation only. Since such
combination is a research direction which requires independent introduction and efforts, we present
the work in a separate manuscript (which could be anonymously shared with Referees if needed)."
OUTLINE OF MAIN RESULTS,0.018363939899833055,Under review as a conference paper at ICLR 2022
OUTLINE OF MAIN RESULTS,0.019198664440734557,"Algorithm 2 C-MinHash-(0, Ï€)"
OUTLINE OF MAIN RESULTS,0.02003338898163606,"Input: Binary data vector v âˆˆ{0, 1}D,
Permutation vector Ï€: [D] â†’[D]
Output: Hash values h1(v), ..., hK(v)"
OUTLINE OF MAIN RESULTS,0.020868113522537562,"For k = 1 to K
Shift Ï€ circulantly rightwards by k units: Ï€k = Ï€â†’k
hk(v) â†mini:viÌ¸=0 Ï€â†’k(i)
End For ğŸ‘"
OUTLINE OF MAIN RESULTS,0.021702838063439065,Circular
OUTLINE OF MAIN RESULTS,0.022537562604340568,"Shift
ğŸ
ğŸ”"
OUTLINE OF MAIN RESULTS,0.02337228714524207,"ğ…â†’ğ’Œ= {ğŸ“, ğŸ–, ğŸ, ğŸ•, ğŸ‘, ğŸ’, ğŸ”, ğŸ} ğŸ ğŸ– ğ’—ğŸ–
ğŸ” ğŸ•
ğŸ‘ ğŸ“"
OUTLINE OF MAIN RESULTS,0.024207011686143573,"ğ…â†’ğ’Œ+ğŸ= {ğŸ, ğŸ“, ğŸ–, ğŸ, ğŸ•, ğŸ‘, ğŸ’, ğŸ”} ğŸ’"
OUTLINE OF MAIN RESULTS,0.025041736227045076,"ğŸ
ğ’—ğŸ
ğ’—ğŸ ğ’—ğŸ‘"
OUTLINE OF MAIN RESULTS,0.02587646076794658,"ğ’—ğŸ’
ğ’—ğŸ“
ğ’—ğŸ” ğ’—ğŸ• ğ’—ğŸ– ğ’—ğŸ
ğ’—ğŸ ğ’—ğŸ‘"
OUTLINE OF MAIN RESULTS,0.02671118530884808,"ğ’—ğŸ’
ğ’—ğŸ“
ğ’—ğŸ” ğ’—ğŸ• ğŸ“ ğŸ– ğŸ•
ğŸ’ ğŸ"
OUTLINE OF MAIN RESULTS,0.027545909849749584,"Figure 1: An illustration of the idea of C-MinHash. The data vector has three non-zeros, v2 = v4 =
v5 = 1. In this example, hk(v) = 3, hk+1(v) = 1."
OUTLINE OF MAIN RESULTS,0.028380634390651086,"2
C-MINHASH-(0, Ï€) WITHOUT INITIAL PERMUTATION"
OUTLINE OF MAIN RESULTS,0.02921535893155259,"As shown in Algorithm 2, the C-MinHash algorithm has similar operations as MinHash. The differ-
ence lies in the permutations used in the hashing process. To generate each hash hk(v), we permute
the data vector using Ï€â†’k, which is the permutation shifted k units circulantly towards right based
on Ï€. For example, Ï€ = [3, 1, 2, 4], Ï€â†’1 = [4, 3, 1, 2], Ï€â†’2 = [2, 4, 3, 1], etc. Conceptually, we may
think of circulation as concatenating the ï¬rst and last elements of a vector to form a circle; see Fig-
ure 1. We set the hash value hk(v) as the position of the ï¬rst non-zero after being permuted by Ï€â†’k.
Analogously, we deï¬ne the C-MinHash-(0, Ï€) estimator of the Jaccard similarity J(v, w) as"
OUTLINE OF MAIN RESULTS,0.03005008347245409,"Ë†J0,Ï€ = 1 K K
X"
OUTLINE OF MAIN RESULTS,0.030884808013355594,"k=1
1{hk(v) = hk(w)},
(4)"
OUTLINE OF MAIN RESULTS,0.03171953255425709,"where h is the hash value output by Algorithm 2. In this paper, for simplicity, we assume K â‰¤D."
OUTLINE OF MAIN RESULTS,0.0325542570951586,"Next, we present the theoretical analysis for Algorithm 2, in terms of the expectation (mean) and the
variance of the estimator Ë†J0,Ï€. Our results reveal that the estimation accuracy depends on the initial
data distribution, which may lead to undesirable performance behaviors when real-world datasets
exhibit various structures. On the other hand, while it is not our recommended method, the analysis
serves a preparation (and insight) for the C-MinHash-(Ïƒ, Ï€) which will soon be described."
OUTLINE OF MAIN RESULTS,0.0333889816360601,"First, we introduce some notations and deï¬nitions. Given v, w âˆˆ{0, 1}D, we deï¬ne a and f as a = D
X"
OUTLINE OF MAIN RESULTS,0.034223706176961605,"i=1
1{vi = 1 and wi = 1},
f = D
X"
OUTLINE OF MAIN RESULTS,0.035058430717863104,"i=1
1{vi = 1 or wi = 1}.
(5)"
OUTLINE OF MAIN RESULTS,0.03589315525876461,"We say that (v, w) is a (D, f, a)-data pair, whose Jaccard similarity can also be written as J = a/f.
Deï¬nition 2.1. Consider v, w âˆˆ{0, 1}D. Deï¬ne the location vector as x âˆˆ{O, Ã—, âˆ’}D, with xi
being â€œOâ€, â€œÃ—â€, â€œâˆ’â€, when vi = wi = 1, vi + wi = 1 and vi = wi = 0, respectively."
OUTLINE OF MAIN RESULTS,0.03672787979966611,"The location vector x can fully characterize a hash collision. When a permutation Ï€â†’k is applied,
the hashes hk(v) and hk(w) would collide if after permutation, the ï¬rst â€œOâ€ is placed before the
ï¬rst â€œÃ—â€ (counting from small to large). This observation will be the key in our theoretical analysis."
OUTLINE OF MAIN RESULTS,0.037562604340567615,Under review as a conference paper at ICLR 2022
OUTLINE OF MAIN RESULTS,0.038397328881469114,"Deï¬nition 2.2. For A, B âˆˆ{O, Ã—, âˆ’}, let {(A, B)|â–³} denote the set {(i, j) : (xi, xj) =
(A, B), j âˆ’i = â–³}. For each 1 â‰¤â–³â‰¤K âˆ’1, deï¬ne
L0(â–³) = {(O, O)|â–³}, L1(â–³) = {(O, Ã—)}, L2(â–³) = {(O, âˆ’)},
G0(â–³) = {(âˆ’, O)|â–³}, G1(â–³) = {(âˆ’, Ã—)} , G2(â–³) = {(âˆ’, âˆ’)},
H0(â–³) = {(Ã—, O)|â–³}, H1(â–³) = {(Ã—, Ã—)}, H2(â–³) = {(Ã—, âˆ’)}.
Remark 2.1. For the ease of notation, by circulation we write xj = xjâˆ’D when D < j < 2D."
OUTLINE OF MAIN RESULTS,0.03923205342237062,"Deï¬nition 2.2 measures the relative location of different types of points in the location vector, for a
speciï¬c pair of data vectors. One can easily verify that for âˆ€1 â‰¤â–³â‰¤K âˆ’1,
|L0| + |L1| + |L2| = |L0| + |G0| + |H0| = a,
|G0| + |G1| + |G2| = |L2| + |G2| + |H2| = D âˆ’f,
|H0| + |H1| + |H2| = |L1| + |G1| + |H1| = f âˆ’a,
(6)"
OUTLINE OF MAIN RESULTS,0.04006677796327212,"which is the intrinsic constraints on the size of above sets. We are now ready to analyze the expecta-
tion and variance of Ë†J0,Ï€. It is easy to see that Ë†J0,Ï€ is still unbiased, i.e., E[ Ë†J0,Ï€] = J, by linearity of
expectation. Lemma 2.1 provides an important quantity that leads to V ar[ Ë†J0,Ï€] as in Theorem 2.2.
Lemma 2.1. For any 1 â‰¤s < t â‰¤K with t âˆ’s = â–³, we have"
OUTLINE OF MAIN RESULTS,0.040901502504173626,"EÏ€

1{hs(v) = hs(w)}1{ht(v) = ht(w)}

= |L0(â–³)| + (|G0(â–³)| + |L2(â–³)|)J"
OUTLINE OF MAIN RESULTS,0.041736227045075125,"f + |G0(â–³)| + |G1(â–³)|
,"
OUTLINE OF MAIN RESULTS,0.04257095158597663,"where the sets are deï¬ned in Deï¬nition 2.2 and hs, ht are the hash values as in Algorithm 2.
Theorem 2.2. Under the same setting as in Lemma 2.1, the variance of Ë†J0,Ï€ is"
OUTLINE OF MAIN RESULTS,0.04340567612687813,"V ar[ Ë†J0,Ï€] = J"
OUTLINE OF MAIN RESULTS,0.04424040066777963,"K + 2 PK
s=2(s âˆ’1)Î˜Kâˆ’s+1"
OUTLINE OF MAIN RESULTS,0.045075125208681135,"K2
âˆ’J2,"
OUTLINE OF MAIN RESULTS,0.045909849749582635,"where Î˜â–³â‰œEÏ€

1{hs(v) = hs(w)}1{ht(v) = ht(w)}

as in Lemma 2.1 with any t âˆ’s = â–³."
OUTLINE OF MAIN RESULTS,0.04674457429048414,"From Theorem 2.2, we see that the variance of Ë†J0,Ï€ depends on a, f, and the size of sets Lâ€™s and
Gâ€™s as in Deï¬nition 2.1, which is determined by the location vector x. Since we use the original
data vectors without randomly permuting the entries beforehand, V ar[ Ë†J0,Ï€] is called â€œlocation-
dependentâ€ as it is dependent on the location of non-zero entries of the original data."
OUTLINE OF MAIN RESULTS,0.04757929883138564,"3
C-MINHASH-(Ïƒ, Ï€) WITH INDEPENDENT INITIAL PERMUTATION"
OUTLINE OF MAIN RESULTS,0.048414023372287146,"Algorithm 3 C-MinHash-(Ïƒ, Ï€)"
OUTLINE OF MAIN RESULTS,0.049248747913188645,"Input: Binary data vector v âˆˆ{0, 1}D,
Permutation vectors Ï€ and Ïƒ: [D] â†’[D]
Output: Hash values h1(v), ..., hK(v)"
OUTLINE OF MAIN RESULTS,0.05008347245409015,"Initial permutation: vâ€² = Ïƒ(v)
For k = 1 to K
Shift Ï€ circulantly rightwards by k units: Ï€k = Ï€â†’k
hk(v) â†mini:vâ€²
iÌ¸=0 Ï€â†’k(i)
End For"
OUTLINE OF MAIN RESULTS,0.05091819699499165,"The method C-MinHash-(Ïƒ, Ï€) is summarized in Algorithm 3, which is very similar to Algorithm 2.
This time, as pre-processing, we apply an initial permutation Ïƒ |="
OUTLINE OF MAIN RESULTS,0.05175292153589316,"Ï€ on the data to break whatever
structures which might exist. Similarly, we deï¬ne the C-MinHash-(Ïƒ, Ï€) estimator of J as"
OUTLINE OF MAIN RESULTS,0.052587646076794656,"Ë†JÏƒ,Ï€ = 1 K K
X"
OUTLINE OF MAIN RESULTS,0.05342237061769616,"k=1
1{hk(v) = hk(w)},
(7)"
OUTLINE OF MAIN RESULTS,0.05425709515859766,"where hkâ€™s are the hash values output by Algorithm 3. We will present our detailed theoretical
analysis and the main result (Theorem 3.5). First, by linearity of expectation and that Ïƒ and Ï€ are
independent, Ë†JÏƒ,Ï€ is still an unbiased estimator E[ Ë†JÏƒ,Ï€] = E[1{hi(v) = hi(w)}] = J. To provide
an immediate intuition, the following proposition emphasizes that the collision indicator variables in
(7) are pairwise negatively correlated, which intuitively explains the source of variance reduction."
OUTLINE OF MAIN RESULTS,0.05509181969949917,Under review as a conference paper at ICLR 2022
OUTLINE OF MAIN RESULTS,0.055926544240400666,"Proposition 3.1. In (7), Cov(1{hi(v) = hi(w)}, 1{hj(v) = hj(w)}) < 0, âˆ€i Ì¸= j."
OUTLINE OF MAIN RESULTS,0.05676126878130217,"Proof.
Denote ËœE = EiÌ¸=j[1{hi(v) = hi(w)}1{hj(v) = hj(w)}], for i Ì¸= j. The task is then
to show that Cov(1{hi(v) = hi(w)}, 1{hj(v) = hj(w)}) = ËœE âˆ’J2 < 0. By Theorem 3.5,
V ar[ Ë†JMH] < V ar[ Ë†JÏƒ,Ï€], where V ar[ Ë†JMH] = J(1âˆ’J) K
and"
OUTLINE OF MAIN RESULTS,0.05759599332220367,"V ar[ Ë†JÏƒ,Ï€] = E

Ë†JÏƒ,Ï€
2
âˆ’J2 = E ï£® ï£°"
K,0.05843071786310518,"1
K K
X"
K,0.05926544240400668,"k=1
1{hk(v) = hk(w)} !2ï£¹ ï£»âˆ’J2 =
P"
K,0.06010016694490818,"i E[1{hi(v) = hi(w)}] K2
+ P"
K,0.06093489148580968,iÌ¸=j E[1{hi(v) = hi(w)}1{hj(v) = hj(w)}]
K,0.06176961602671119,"K2
âˆ’J2 = J"
K,0.06260434056761269,K + K(K âˆ’1)E[1{hi(v) = hi(w)}1{hj(v) = hj(w)}]
K,0.06343906510851419,"K2
âˆ’J2,
where i Ì¸= j = J"
K,0.0642737896494157,K + (K âˆ’1) ËœE
K,0.0651085141903172,"K
âˆ’J2 < J(1 âˆ’J)"
K,0.0659432387312187,"K
=â‡’

ËœE âˆ’J2 
1 âˆ’1 K"
K,0.0667779632721202,"
< 0 =â‡’ËœE âˆ’J2 < 0.
â–¡"
K,0.0676126878130217,"Next, we formally present the path to our main result. The next Theorem gives the variance of Ë†JÏƒ,Ï€.
Theorem 3.2. Let a, f be deï¬ned as in (5). When 0 < a < f â‰¤D (J /âˆˆ{0, 1}), we have"
K,0.06844741235392321,"V ar[ Ë†JÏƒ,Ï€] = J"
K,0.06928213689482471,K + (K âˆ’1) ËœE
K,0.07011686143572621,"K
âˆ’J2,
(8)"
K,0.0709515859766277,"where with l = max(0, D âˆ’2f + a), and"
K,0.07178631051752922,"ËœE =EiÌ¸=j[1{hi(v) = hi(w)}1{hj(v) = hj(w)}] =
X"
K,0.07262103505843072,"{l0,l2,g0,g1}"
K,0.07345575959933222,"( 
l0
f + g0 + g1
+
a(g0 + l2)
(f + g0 + g1)f  Ã—"
K,0.07429048414023372,"Dâˆ’fâˆ’1
X s=l"
K,0.07512520868113523," Dâˆ’f
s
"
K,0.07595993322203673," Dâˆ’aâˆ’1
Dâˆ’fâˆ’1
"
K,0.07679465776293823," 
fâˆ’aâˆ’1
Dâˆ’fâˆ’sâˆ’1
  s
n1
 Dâˆ’fâˆ’s
n2
 Dâˆ’fâˆ’s
n3
 fâˆ’aâˆ’(Dâˆ’fâˆ’s)
n4
 
aâˆ’1
aâˆ’l1âˆ’l2
"
K,0.07762938230383973," Dâˆ’1
a
 ) . (9)"
K,0.07846410684474124,"The feasible set {l0, l2, g0, g1} satisï¬es the intrinsic constraints (6), and"
K,0.07929883138564274,"n1 = g0 âˆ’(D âˆ’f âˆ’s âˆ’g1),
n2 = D âˆ’f âˆ’s âˆ’g1,
n3 = l2 âˆ’g0 + (D âˆ’f âˆ’s âˆ’g1), n4 = l1 âˆ’(D âˆ’f âˆ’s âˆ’g1)."
K,0.08013355592654424,"Also, when a = 0 or f = a (J = 0 or J = 1), we have V ar[ Ë†JÏƒ,Ï€] = 0."
K,0.08096828046744574,"Since the original locational structure of the data is broken by the initial permutation Ïƒ, V ar[ Ë†JÏƒ,Ï€]
only depends on the value of (D, f, a), i.e., it is â€œlocation-independentâ€. This would make the
performance of C-MinHash-(Ïƒ, Ï€) consistent in different tasks. Same as MinHash, Proposition 3.3
states that given D and f, the variance of Ë†JÏƒ,Ï€ is symmetric about J = 0.5, as illustrated in Figure 2,
which also shows that the variance of Ë†JÏƒ,Ï€ is smaller than the variance of the original MinHash."
K,0.08180300500834725,"Proposition 3.3 (Symmetry). V ar[ Ë†JÏƒ,Ï€] is the same for the (D, f, a)-data pair and the (D, f, f âˆ’
a)-data pair, âˆ€0 â‰¤a â‰¤f â‰¤D."
K,0.08263772954924875,"0
0.2
0.4
0.6
0.8
1
J 1 2 3"
K,0.08347245409015025,"4
5
10-4"
K,0.08430717863105175,"MinHash
 0.2 0.8"
K,0.08514190317195326,f/D = 1
K,0.08597662771285476,D = 1000  K = 500
K,0.08681135225375626,"0
0.2
0.4
0.6
0.8
1
J 1 2 3 10-4"
K,0.08764607679465776,MinHash
K,0.08848080133555926,"0.2
 0.4
 0.6 0.8"
K,0.08931552587646077,f/D = 1
K,0.09015025041736227,D = 1000  K = 800
K,0.09098497495826377,"Figure 2: V ar[ Ë†JÏƒ,Ï€] versus J, with D = 1000 and varying f. Left: K = 500. Right: K = 800."
K,0.09181969949916527,Under review as a conference paper at ICLR 2022
K,0.09265442404006678,"101
102
103 D 10-3 10-2 10-1 100"
K,0.09348914858096828,J = 0.1
K,0.09432387312186978,J = 0.3
K,0.09515859766277128,J = 0.5
K,0.09599332220367279,J = 0.9
K,0.09682804674457429,f = 10
K,0.09766277128547579,"0
0.2
0.4
0.6
0.8
1
J 2 3 4 5"
K,0.09849749582637729,Variance Ratio
K,0.0993322203672788,"0.2
 0.4
 0.6 0.8"
K,0.1001669449081803,f/D = 1
K,0.1010016694490818,"0
0.2
0.4
0.6
0.8
1
K/D 100 101 102"
K,0.1018363939899833,Variance Ratio
K,0.10267111853088481,f/D = 0.1
K,0.10350584307178631,"0.7
0.9"
K,0.10434056761268781,"f/D = 1
D = 1000"
K,0.10517529215358931,"Figure 3: Left: Theoretical ËœE, f = 10. Each dash line represents the corresponding J2. Mid:
Variance ratio V ar[ Ë†
JMH]
V ar[ Ë†
JÏƒ,Ï€] , D = 1000, K = 800. Right: Variance ratio V ar[ Ë†
JMH]
V ar[ Ë†
JÏƒ,Ï€] , D = 1000."
K,0.10601001669449082,"A rigorous comparison of V ar[ Ë†JÏƒ,Ï€] and V ar[ Ë†JMH] appears to be a challenging task given the
complicated combinatorial form of V ar[ Ë†JÏƒ,Ï€]. The following lemma characterizes an important
property of ËœE in (9), that it is monotone in D when a and f are ï¬xed, as illustrated in Figure 3 (left)."
K,0.10684474123539232,"Lemma 3.4 (Increasing Increment). Assume f > a > 0 are arbitrary and ï¬xed. Denote ËœED as in
(9) in Theorem 3.2, with D treated as a parameter. Then we have ËœED+1 > ËœED for âˆ€D â‰¥f."
K,0.10767946577629382,"Equipped with Lemma 3.4, we arrive at the following main theoretical result of this work, on the
uniform variance reduction of C-MinHash-(Ïƒ, Ï€).
Theorem 3.5 (Uniform Superiority). For any two binary vectors v, w âˆˆ{0, 1}D with J Ì¸= 0 or 1,
it holds that V ar[ Ë†JÏƒ,Ï€(v, w)] < V ar[ Ë†JMH(v, w)]."
K,0.10851419031719532,"That is, using merely two permutations as C-MinHash-(Ïƒ, Ï€) improves the Jaccard estimation vari-
ance of classical MinHash, in all cases. Next, in Figure 3 (mid) and Theorem 3.6, we show that
interestingly, the relative improvement is actually the same for any J, for given D, f and K.
Proposition 3.6 (Consistent Improvement). Suppose f is ï¬xed. In terms of a, the variance ratio
V ar[ Ë†
JMH(v,w)]
V ar[ Ë†
JÏƒ,Ï€(v,w)] is constant for any 0 < a < f."
K,0.10934891485809682,"How is the improvement affected by the sparsity (i.e., f) and the number of hashes K? In Figure 3
(right), we plot the variance ratio V ar[ Ë†
JMH]
V ar[ Ë†
JÏƒ,Ï€] with different f and K, given ï¬xed D. We see that the
improvement in variance increases with K (more hashes) and f (more non-zero entries). Note that,
by Proposition 3.6, here we do not need to consider a which does not affect the variance ratio. The
results in Figure 3 again verify Theorem 3.5, as the variance ratio is always greater than 1."
NUMERICAL EXPERIMENTS,0.11018363939899833,"4
NUMERICAL EXPERIMENTS"
NUMERICAL EXPERIMENTS,0.11101836393989983,"In this section, we provide numerical experiments to validate our theoretical ï¬ndings and demon-
strate that C-MinHash can indeed lead to smaller Jaccard estimation errors."
NUMERICAL EXPERIMENTS,0.11185308848080133,"4.1
SANITY CHECK: A SIMULATION STUDY"
NUMERICAL EXPERIMENTS,0.11268781302170283,"A simulation study is conducted on synthetic data to verify the theoretical variances given by The-
orem 2.2 and Theorem 3.2. We simulate D = 128 dimensional binary vector pairs (v, w) with
different f and a, which have a special locational structure that the location vector x is such that a
â€œOâ€â€™s are followed by (f âˆ’a) â€œÃ—â€â€™s and then followed by (D âˆ’f) â€œâˆ’â€â€™s sequentially. We plot the
empirical and theoretical mean square errors (MSE = variance + bias2) in Figure 4:"
NUMERICAL EXPERIMENTS,0.11352253756260434,"â€¢ The theoretical variance agrees with the empirical observation, as the curves overlap, con-
ï¬rming Theorem 2.2 and Theorem 3.2. The variance reduction increases with larger K."
NUMERICAL EXPERIMENTS,0.11435726210350584,"â€¢ V ar[ Ë†JÏƒ,Ï€] (C-MinHash-(Ïƒ, Ï€)) is always smaller than V ar[ Ë†JMH], as stated by Theo-
rem 3.5. In contrast, V ar[ Ë†J0,Ï€] (C-MinHash-(0, Ï€)) varies signiï¬cantly depending on dif-
ferent data structures, as mentioned in Section 2."
NUMERICAL EXPERIMENTS,0.11519198664440734,Under review as a conference paper at ICLR 2022
NUMERICAL EXPERIMENTS,0.11602671118530884,"1
20
40
60
80
100 120
K 10-4 10-3 10-2 10-1 MSE"
NUMERICAL EXPERIMENTS,0.11686143572621036,"(D,f,a) = (128,8,1)"
NUMERICAL EXPERIMENTS,0.11769616026711185,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.11853088480801335,"1
20
40
60
80
100 120
K 10-3 10-2 10-1 MSE"
NUMERICAL EXPERIMENTS,0.11936560934891485,"(D,f,a) = (128,8,2)"
NUMERICAL EXPERIMENTS,0.12020033388981637,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.12103505843071787,"1
20
40
60
80
100 120
K 10-3 10-2 10-1 100 MSE"
NUMERICAL EXPERIMENTS,0.12186978297161936,"(D,f,a) = (128,8,4)"
NUMERICAL EXPERIMENTS,0.12270450751252086,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.12353923205342238,"1
20
40
60
80
100 120
K 10-4 10-3 10-2 10-1 MSE"
NUMERICAL EXPERIMENTS,0.12437395659432388,"(D,f,a) = (128,64,2)"
NUMERICAL EXPERIMENTS,0.12520868113522537,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.12604340567612687,"1
20
40
60
80
100 120
K 10-4 10-3 10-2 10-1 MSE"
NUMERICAL EXPERIMENTS,0.12687813021702837,"(D,f,a) = (128,64,8)"
NUMERICAL EXPERIMENTS,0.12771285475792987,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.1285475792988314,"1
20
40
60
80
100 120
K 10-6 10-4 10-2 100 MSE"
NUMERICAL EXPERIMENTS,0.1293823038397329,"(D,f,a) = (128,128,16)"
NUMERICAL EXPERIMENTS,0.1302170283806344,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.1310517529215359,"Figure 4: Empirical vs. theoretical variance of Ë†J0,Ï€ (C-MinHash-(0, Ï€)) and Ë†JÏƒ,Ï€ (C-MinHash-
(Ïƒ, Ï€)), on synthetic binary data vector pairs with different data patterns."
NUMERICAL EXPERIMENTS,0.1318864774624374,"28
 29
210
211
212 K 0.4 0.6 0.8"
NUMERICAL EXPERIMENTS,0.1327212020033389,Mean Absolute Error 10-2 BBC
NUMERICAL EXPERIMENTS,0.1335559265442404,"MinHash
C-MinHash-(0, )
C-MinHash-( , )"
NUMERICAL EXPERIMENTS,0.1343906510851419,"28
 29
210
211
212 K 0.5 1 1.5"
NUMERICAL EXPERIMENTS,0.1352253756260434,Mean Absolute Error 10-2 NIPS
NUMERICAL EXPERIMENTS,0.13606010016694492,"MinHash
C-MinHash-(0, )
C-MinHash-( , )"
NUMERICAL EXPERIMENTS,0.13689482470784642,"25
26
27
28
29 K 10-2 10-1"
NUMERICAL EXPERIMENTS,0.13772954924874792,Mean Absolute Error MNIST
NUMERICAL EXPERIMENTS,0.13856427378964942,"MinHash
C-MinHash-(0, )
C-MinHash-( , )"
NUMERICAL EXPERIMENTS,0.13939899833055092,"26
 27
 28
 29
210 K 10-2 10-1"
NUMERICAL EXPERIMENTS,0.14023372287145242,Mean Absolute Error CIFAR
NUMERICAL EXPERIMENTS,0.14106844741235391,"MinHash
C-MinHash-(0, )
C-MinHash-( , )"
NUMERICAL EXPERIMENTS,0.1419031719532554,Figure 5: Mean Absolute Error ( MAE) of MinHash and C-MinHash on real-world datasets.
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14273789649415694,"4.2
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14357262103505844,"We test C-MinHash on four datasets, including two text datasets: the NIPS full paper dataset from
UCI repository (Dua & Graff, 2017), and the BBC News dataset (Greene & Cunningham, 2006),
and two popular image datasets: the MNIST dataset (LeCun et al., 1998) with hand-written digits,
and the CIFAR dataset (Krizhevsky, 2009) containing natural images. All the datasets are processed
to be binary. For each dataset with n data vectors, there are in total n(n âˆ’1)/2 data vector pairs.
We estimate the Jaccard similarities for all the pairs and report the mean absolute errors (MAE). The
results are averaged over 10 independent repetitions, for each dataset, as shown in Figure 5:"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14440734557595994,"â€¢ The MAE of C-MinHash-(Ïƒ, Ï€) is consistently smaller than that of MinHash, again con-
ï¬rming Theorem 3.5 on the beneï¬t of the improved variance. The improvements become
more substantial with larger K, which is consistent with the trend in Figure 3.
â€¢ Without the initial permutation Ïƒ, the accuracy of C-MinHash-(0, Ï€) is affected by the
distribution of the original data, and it is worse than C-MinHash-(Ïƒ, Ï€) on all these four
datasets. Also, the performance of C-MinHash-(0, Ï€) on image data seems much worse
than that on text data, which we believe is because the image datasets contain more struc-
tural patterns. This again suggests that the initial permutation Ïƒ might be needed in practice."
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14524207011686144,"In summary, the simulation study has veriï¬ed the correctness of our theoretical ï¬ndings in Theo-
rem 2.2 and Theorem 3.2. The experiments with Jaccard estimation on four datasets conï¬rm that
C-MinHash is more accurate than the original MinHash. The initial permutation Ïƒ is recommended."
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14607679465776294,"5
C-MINHASH-(Ï€, Ï€): PRACTICALLY REDUCING TO ONE PERMUTATION"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14691151919866444,"We propose a more convenient variant, C-MinHash-(Ï€, Ï€), which only requires one permutation.
That is, Ï€ is used for both pre-processing and circulant hashing. The procedure is the same as Al-
gorithm 3, except that the initial permutation Ïƒ is replaced by Ï€. Denote the Jaccard estimator Ë†JÏ€,Ï€."
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14774624373956594,Under review as a conference paper at ICLR 2022
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14858096828046743,"The complicated dependency between Ï€ (initial permutation) and Ï€â†’k (hashing) makes the theoret-
ical analysis challenging. In the following, we provide the mean of each hash collision indicator.
Theorem 5.1. Assume K â‰¤D and let a, f be deï¬ned as (5). The location vector x is deï¬ned in
Deï¬nition 2.1. Denote B1 = {i : xi = O}, B2 = {i : xi = Ã—} and B3 = {i : xi = âˆ’}. For
a â‰¤j â‰¤D and 1 â‰¤k â‰¤K, deï¬ne"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14941569282136896,"Aâˆ’(j) = {xi : (i + k âˆ’1 mod D) + 1 â‰¤j},
A+(j) = {xi : (i + k âˆ’1 mod D) + 1 > j}."
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15025041736227046,"Let nâˆ’,1(j) = |{xi = O : i âˆˆAâˆ’(j)}| be the number of â€œOâ€ points in Aâˆ’(j). Analogously let
nâˆ’,2(j), nâˆ’,3(j) be the number of â€œÃ—â€ and â€œâˆ’â€ points in Aâˆ’(j), and n+,1(j), n+,2(j), n+,3(j)
be the number of â€œOâ€, â€œÃ—â€ and â€œâˆ’â€ points in A+(j). Then, for the k-th C-MinHash-(Ï€, Ï€) hash,"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15108514190317196,"E[1{hk(v) = hk(w)}] = D
X j=1 X"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15191986644407346,"ZâˆˆÎ˜j
Pj(Z) Â·
n
3
X"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15275459098497496,"i=1
Î¨i(j) +
X"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15358931552587646,"iâˆˆB1
(1 âˆ’
zâˆ’,1
nâˆ’,1(iâˆ—)) ËœP1
o
,"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15442404006677796,"where Pj(Z) is the density function of Z
= (zâˆ’,k|3
1, z+,k|3
1) which follows hyper(D, D âˆ’
f, nâˆ’,k(j)|3
1, n+,k(j)|3
1), with domain Î˜j. For q = 1, 2, 3, denote 1#
q = 1{j# âˆˆBq}, and"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15525876460767946,"Î¨q(j) =
X"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15609348914858096,"iâˆˆBq,j<iâˆ—"
X,0.15692821368948248,"3
X"
X,0.15776293823038398,"p=1
1#
p
 
1 âˆ’
zâˆ’,p
nâˆ’,p(j) + 1{q = 3}(2
zâˆ’,p
nâˆ’,p(j) âˆ’1)

(1 âˆ’
z+,q
n+,q(j)) ËœPq Â¯Jq +
X"
X,0.15859766277128548,"iâˆˆBq,j>iâˆ—"
X,0.15943238731218698,"h
1#
q (1 âˆ’
zâˆ’,q
nâˆ’,q(j))(1 âˆ’
zâˆ’,q
nâˆ’,q(j) âˆ’1) +"
X,0.16026711185308848,"3
X"
X,0.16110183639398998,"pÌ¸=q
1#
p (1 âˆ’
zâˆ’,q
nâˆ’,q(j))(1 âˆ’
zâˆ’,p
nâˆ’,p(j))
i
ËœPqJâˆ—,"
X,0.16193656093489148,"where iâˆ—= (i + k âˆ’1 mod D) + 1, i# = (i âˆ’k âˆ’1 mod D) + 1, âˆ€i. Deï¬ne Jâˆ—=
aâˆ’r1
fâˆ’r1âˆ’r2 , and"
X,0.16277128547579298,"ËœP1 = ËœP2 =
1
r1 + r2"
X,0.1636060100166945," b0
r3
 Dâˆ’jâˆ’b0
r1+r2âˆ’1
"
X,0.164440734557596," Dâˆ’f
r3
 
f
r1+r2
,
ËœP3 = 1 r3"
X,0.1652754590984975,"  b0
r3âˆ’1
 Dâˆ’jâˆ’b0
r1+r2
"
X,0.166110183639399," Dâˆ’f
r3
 
f
r1+r2
 ,"
X,0.1669449081803005,Â¯Jq = r1 âˆ’1{q = 1}
X,0.167779632721202,"D âˆ’j âˆ’b0
+ (1 âˆ’r1 + r2 âˆ’1{q Ì¸= 3}"
X,0.1686143572621035,"D âˆ’j âˆ’b0
)Jâˆ—, q = 1, 2, 3,"
X,0.169449081803005,"where b0 = P3
k=1 z+,k, r1 = aâˆ’zâˆ’,1âˆ’z+,1, r2 = fâˆ’aâˆ’zâˆ’,2âˆ’z+,2 and r3 = Dâˆ’fâˆ’zâˆ’,3âˆ’z+,3."
X,0.17028380634390652,"Theorem 5.1 says that E[1{hk(v) = hk(w)}] would be different for different k. Figure 6 presents
numerical examples to validate the theory and demonstrate the magnitude of bias2 (recall MSE =
bias2 + variance), where the simulations match perfectly Theorem 5.1. As the bias of E[1{hk(v) ="
X,0.17111853088480802,"0
20
40
60
K 0 1 2 3 Bias2 10-6"
X,0.17195325542570952,"(64,4,1)"
X,0.17278797996661102,"Simulation
Theory"
X,0.17362270450751252,"0
20
40
60
K 0 1 2 3 4 Bias2 10-6"
X,0.17445742904841402,"(64,4,2)"
X,0.17529215358931552,"Simulation
Theory"
X,0.17612687813021702,"0
20
40
60
K 0 0.5 1 Bias2 10-5"
X,0.17696160267111852,"(64,32,16)"
X,0.17779632721202004,"Simulation
Theory"
X,0.17863105175292154,"0
20
40
60
k 0.24 0.245 0.25 0.255 0.26"
X,0.17946577629382304,Mean Estimate
X,0.18030050083472454,"(D,f,a) = (64,4,1)
Simulation
Theory
True J"
X,0.18113522537562604,"0
20
40
60
k 0.49 0.495 0.5 0.505 0.51"
X,0.18196994991652754,Mean Estimate
X,0.18280467445742904,"(64,4,2)"
X,0.18363939899833054,"Simulation
Theory
True J"
X,0.18447412353923207,"0
20
40
60
k 0.49 0.495 0.5 0.505 0.51"
X,0.18530884808013356,Mean Estimate
X,0.18614357262103506,"(64,32,16)"
X,0.18697829716193656,"Simulation
Theory
True J"
X,0.18781302170283806,"Figure 6: 1st row: bias2 = (E[ Ë†JÏ€,Ï€] âˆ’J)2 vs. number of hashes K on simulated (D, f, a)-data
pairs, D = 64. 2nd row: mean of each collision indicator."
X,0.18864774624373956,Under review as a conference paper at ICLR 2022
X,0.18948247078464106,"hk(w)}] can be positive or negative, the overall bias of Ë†JÏ€,Ï€ would approach 0 as K increases. From
the 2nd row, we see that bias2 is very small (10âˆ’5 or even smaller) and indeed converges to 0 with
more hashes, i.e., the averaging effect. Also, from the proof (see Appendix A.8) we can know that
when a and f are ï¬xed, as D increases, E[ Ë†JÏ€,Ï€] would converge to J."
X,0.19031719532554256,"We found through extensive numerical experiments that, the MSE of Ë†JÏ€,Ï€ is essentially the same
as Ë†JÏƒ,Ï€. Figure 7 (1st row) compares the empirical of C-MinHash-(Ï€, Ï€) with the theoretical vari-
ances of C-MinHash-(Ïƒ, Ï€), where the simulated data has the same special locational structure as in
Section 4.1. In the 2nd row, we present the MAE comparison on real datasets, where we see that
the curves for these two estimators ( Ë†JÏƒ,Ï€ and Ë†JÏ€,Ï€) match well. In all ï¬gures, the overlapping MSE
curves verify our claim that we just need one permutation Ï€. Due to the space limitation, we provide
more empirical justiï¬cations in Appendix B."
X,0.1911519198664441,"100
101
102 K 10-3 10-2 10-1 MSE"
X,0.19198664440734559,"(128,8,2)"
PERM,0.19282136894824708,"1 Perm
2 Perm Theo."
PERM,0.19365609348914858,"100
101
102 K 10-3 10-2 10-1 100 MSE"
PERM,0.19449081803005008,"(128,8,4)"
PERM,0.19532554257095158,"1 Perm
2 Perm Theo."
PERM,0.19616026711185308,"100
101
102 K 10-4 10-3 10-2 10-1 MSE"
PERM,0.19699499165275458,"(128,64,8)"
PERM,0.19782971619365608,"1 Perm
2 Perm Theo."
PERM,0.1986644407345576,"100
101
102 K 10-6 10-4 10-2 100 MSE"
PERM,0.1994991652754591,"(128,128,16)"
PERM,0.2003338898163606,"1 Perm
2 Perm Theo."
PERM,0.2011686143572621,"26
 27
 28
 29
210
211
212 K 0.5 1 1.5"
PERM,0.2020033388981636,Mean Absolute Error 10-2 BBC
PERM,0.2028380634390651,"2 Perm
1 Perm"
PERM,0.2036727879799666,"28
 29
210
211
212 K 0.5 1 1.5"
PERM,0.2045075125208681,Mean Absolute Error 10-2 NIPS
PERM,0.20534223706176963,"2 Perm
1 Perm"
PERM,0.20617696160267113,"25
26
27
28
29 K 10-2 10-1"
PERM,0.20701168614357263,Mean Absolute Error MNIST
PERM,0.20784641068447413,"1 Perm
2 Perm"
PERM,0.20868113522537562,"26
 27
 28
 29
210 K 10-2 10-1"
PERM,0.20951585976627712,Mean Absolute Error CIFAR
PERM,0.21035058430717862,"2 Perm
1 Perm"
PERM,0.21118530884808012,"Figure 7: 1st row: estimator MSE vs. K on simulated data pairs. 2nd row: MAE of Jaccard
estimation on four datasets. â€œ1 Permâ€ is C-MinHash-(Ï€, Ï€), and â€œ2 Permâ€ is C-MinHash-(Ïƒ, Ï€)."
CONCLUSION,0.21202003338898165,"6
CONCLUSION"
CONCLUSION,0.21285475792988315,"The method of minwise hashing (MinHash), from the seminal works of Broder and his colleagues,
has become standard in industrial practice. One fundamental reason for its wide applicability is
that the binary (0/1) high-dimensional representation is convenient and suitable for a wide range of
practical scenarios. The so-called Jaccard similarity is a popular measure of similarity in binary data."
CONCLUSION,0.21368948247078465,"To estimate the Jaccard similarity J, standard MinHash requires to use K independent permuta-
tions, where K, the number of hashes, can be several hundreds or even thousands in practice. In this
paper, we proposed Circulant MinHash (C-MinHash) present the surprising theoretical results that,
with merely 2 permutations, we still obtain an unbiased estimate of the Jaccard similarity with the
variance strictly smaller than that of the original MinHash, as conï¬rmed by numerical experiments
on simulated and real datasets. The initial permutation is applied to break whatever structure the
original data may exhibit. The second permutation is re-used K times in a circulant shifting fash-
ion. Moreover, we analyze a more convenient C-MinHash variance which uses only 1 permutation
for both pre-processing and circulant hashing. We derive the complicated mean estimation, and
validate through extensive experiments that it has the same Jaccard estimation accuracy as using 2
permutations with strict theoretical guarantee."
CONCLUSION,0.21452420701168615,"Practically speaking, our theoretical results may reveal a useful direction for designing hashing
methods. For example, in many applications, using permutation vectors of length (e.g.,) 230 might
be sufï¬cient. While it is perhaps unrealistic to store (e.g.,) K = 1024 such permutation vectors in
the memory, one can afford to store two such permutations (even in GPU memory). Using perfectly
random permutations in lieu of approximate permutations would simplify the design and analysis of
randomized algorithms and ensure that the practical performance strictly matches the theory."
CONCLUSION,0.21535893155258765,"Finally, we mention that our work can be used as a building block to improve Li et al. (2012),
reducing the required one permutation to effectively 1/K permutation only. If needed, we would be
happy to anonymously share the technical manuscript with the related results."
CONCLUSION,0.21619365609348914,Under review as a conference paper at ICLR 2022
REFERENCES,0.21702838063439064,REFERENCES
REFERENCES,0.21786310517529214,"Michael Bendersky and W. Bruce Croft. Finding text reuse on the web. In Proceedings of the
Second International Conference on Web Search and Web Data Mining (WSDM), pp. 262â€“271,
Barcelona, Spain, 2009."
REFERENCES,0.21869782971619364,"Andrei Z. Broder. On the resemblance and containment of documents. In Proceedings of the Con-
ference on Compression and Complexity of SEQUENCES, pp. 21â€“29, Positano, Amalï¬tan Coast,
Salerno, Italy, 1997."
REFERENCES,0.21953255425709517,"Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. Syntactic clustering
of the web. Comput. Networks, 29(8-13):1157â€“1166, 1997."
REFERENCES,0.22036727879799667,"Andrei Z. Broder, Moses Charikar, Alan M. Frieze, and Michael Mitzenmacher. Min-wise inde-
pendent permutations. In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of
Computing (STOC), pp. 327â€“336, Dallas, TX, 1998."
REFERENCES,0.22120200333889817,"Gregory Buehrer and Kumar Chellapilla. A scalable pattern mining approach to web graph com-
pression with communities. In Proceedings of the International Conference on Web Search and
Web Data Mining (WSDM), pp. 95â€“106, Stanford, CA, 2008."
REFERENCES,0.22203672787979967,"Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings on
34th Annual ACM Symposium on Theory of Computing (STOC), pp. 380â€“388, Montreal, Canada,
2002."
REFERENCES,0.22287145242070117,"Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, Michael Mitzenmacher, Alessandro Panconesi, and
Prabhakar Raghavan. On compressing social networks. In Proceedings of the 15th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (KDD), pp. 219â€“228, Paris,
France, 2009."
REFERENCES,0.22370617696160267,"Ondrej Chum and Jiri Matas. Fast computation of min-hash signatures for image collections. In
Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 3077â€“3084, Providence, RI, 2012."
REFERENCES,0.22454090150250416,"Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyamsundar Rajaram.
Google news per-
sonalization: scalable online collaborative ï¬ltering. In Proceedings of the 16th International
Conference on World Wide Web (WWW), pp. 271â€“280, Banff, Alberta, Canada, 2007."
REFERENCES,0.22537562604340566,"Fan Deng, Stefan Siersdorfer, and Sergej Zerr. Efï¬cient jaccard-based diversity analysis of large
document collections. In Proceedings of the 21st ACM International Conference on Information
and Knowledge Management (CIKM), pp. 1402â€“1411, Maui, HI, 2012."
REFERENCES,0.2262103505843072,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.2270450751252087,"Dennis Fetterly, Mark Manasse, Marc Najork, and Janet L. Wiener. A large-scale study of the
evolution of web pages. In Proceedings of the Twelfth International World Wide Web Conference
(WWW), pp. 669â€“678, Budapest, Hungary, 2003."
REFERENCES,0.2278797996661102,"Derek Greene and Padraig Cunningham. Practical solutions to the problem of diagonal dominance
in kernel document clustering. In Proceedings of the Twenty-Third International Conference on
Machine Learning (ICML), pp. 377â€“384, Pittsburgh, PA, 2006."
REFERENCES,0.2287145242070117,"Kaiming He, Fang Wen, and Jian Sun.
K-means hashing: An afï¬nity-preserving quantization
method for learning binary compact codes. In Proceedings of the 2013 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 2938â€“2945, Portland, OR, 2013."
REFERENCES,0.2295492487479132,"Monika Rauch Henzinger. Finding near-duplicate web pages: a large-scale evaluation of algorithms.
In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR), pp. 284â€“291, Seattle, WA, 2006."
REFERENCES,0.2303839732888147,"Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse
of dimensionality. In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of
Computing (STOC), pp. 604â€“613, Dallas, TX, 1998."
REFERENCES,0.23121869782971619,Under review as a conference paper at ICLR 2022
REFERENCES,0.23205342237061768,Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.2328881469115192,"Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278â€“2324, 1998."
REFERENCES,0.2337228714524207,"David C. Lee, Qifa Ke, and Michael Isard. Partition min-hash for partial duplicate image discovery.
In Proceedings of the 11th European Conference on Computer Vision (ECCV), Part I, pp. 648â€“
662, Heraklion, Crete, Greece, 2010."
REFERENCES,0.2345575959933222,"Ping Li and Kenneth Ward Church. Using sketches to estimate associations. In Proceedings of
the Conference on Human Language Technology and the Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP), pp. 708â€“715, Vancouver, Canada, 2005."
REFERENCES,0.2353923205342237,"Ping Li and Arnd Christian KÃ¶nig. Theory and applications of b-bit minwise hashing. Commun.
ACM, 54(8):101â€“109, 2011."
REFERENCES,0.2362270450751252,"Ping Li, Anshumali Shrivastava, Joshua Moore, and Arnd Christian KÃ¶nig. Hashing algorithms for
large-scale learning. In Advances in Neural Information Processing Systems (NIPS), pp. 2672â€“
2680, Granada, Spain, 2011."
REFERENCES,0.2370617696160267,"Ping Li, Art B Owen, and Cun-Hui Zhang.
One permutation hashing.
In Advances in Neural
Information Processing Systems (NIPS), pp. 3122â€“3130, Lake Tahoe, NV, 2012."
REFERENCES,0.2378964941569282,"Anshumali Shrivastava and Ping Li. Fast near neighbor search in high-dimensional binary data.
In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in
Databases (ECML-PKDD), pp. 474â€“489, Bristol, UK, 2012."
REFERENCES,0.2387312186978297,"Anshumali Shrivastava and Ping Li. In defense of minhash over simhash. In Proceedings of the
Seventeenth International Conference on Artiï¬cial Intelligence and Statistics (AISTATS), pp. 886â€“
894, Reykjavik, Iceland, 2014."
REFERENCES,0.2395659432387312,"Acar Tamersoy, Kevin A. Roundy, and Duen Horng Chau. Guilt by association: large scale malware
detection by mining ï¬le-relation graphs. In Proceedings of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD), pp. 1524â€“1533, New York, NY,
2014."
REFERENCES,0.24040066777963273,"Felix X. Yu, Aditya Bhaskara, Sanjiv Kumar, Yunchao Gong, and Shih-Fu Chang. On binary em-
bedding using circulant matrices. J. Mach. Learn. Res., 18:150:1â€“150:30, 2017."
REFERENCES,0.24123539232053423,"Juan Zamora, Marcelo Mendoza, and HÃ©ctor Allende. Hashing-based clustering in high dimensional
data. Expert Syst. Appl., 62:202â€“211, 2016."
REFERENCES,0.24207011686143573,Under review as a conference paper at ICLR 2022
REFERENCES,0.24290484140233723,"A
PROOFS OF TECHNICAL RESULTS"
REFERENCES,0.24373956594323873,"Notations. In our analysis, we will use 1s to denote 1{hs(v) = hs(w)} in for âˆ€1 â‰¤s â‰¤K, where
h is the hash value. Given two data vectors v, w âˆˆ{0, 1}D. Recall in (5): a = PD
i=1 1{vi =
1 and wi = 1}, f = PD
i=1 1{vi = 1 or wi = 1}. Thus, the Jaccard similarity J = a/f. We also
deï¬ne ËœJ = (a âˆ’1)/(f âˆ’1)."
REFERENCES,0.24457429048414023,"Deï¬nition A.1. Let v, w âˆˆ{0, 1}D. Deï¬ne the location vector as x âˆˆ{O, Ã—, âˆ’}D, with xi being
â€œOâ€, â€œÃ—â€, â€œâˆ’â€ when vi = wi = 1, vi + wi = 1 and vi = wi = 0, respectively."
REFERENCES,0.24540901502504173,"Deï¬nition A.2. For A, B âˆˆ{O, Ã—, âˆ’}, let {(i, j) : (A, B)|â–³} denote a pair of indices {(i, j) :
(xi, xj) = (A, B), j âˆ’i = â–³}. Deï¬ne"
REFERENCES,0.24624373956594323,"L0(â–³) = {(i, j) : (O, O)|â–³},
L1(â–³) = {(i, j) : (O, Ã—)|â–³},
L2(â–³) = {(i, j) : (O, âˆ’)|â–³},
G0(â–³) = {(i, j) : (âˆ’, O)|â–³},
G1(â–³) = {(i, j) : (âˆ’, Ã—)|â–³} , G2(â–³) = {(i, j) : (âˆ’, âˆ’)|â–³},
H0(â–³) = {(i, j) : (Ã—, O)|â–³},
H1(â–³) = {(i, j) : (Ã—, Ã—)|â–³},
H2(â–³) = {(i, j) : (Ã—, âˆ’)|â–³}."
REFERENCES,0.24707846410684475,"Remark A.1. For the ease of notation, by circulation we write xj = xjâˆ’D when D < j < 2D."
REFERENCES,0.24791318864774625,"One can easily verify that given ï¬xed a, f, D, it holds that for âˆ€1 â‰¤â–³â‰¤K âˆ’1,"
REFERENCES,0.24874791318864775,"|L0(â–³)| + |L1(â–³)| + |L2(â–³)| = |L0(â–³)| + |G0(â–³)| + |H0(â–³)| = a,
|G0(â–³)| + |G1(â–³)| + |G2(â–³)| = |L2(â–³)| + |G2(â–³)| + |H2(â–³)| = D âˆ’f,
|H0(â–³)| + |H1(â–³)| + |H2(â–³)| = |L1(â–³)| + |G1(â–³)| + |H1(â–³)| = f âˆ’a.
(10)"
REFERENCES,0.24958263772954925,We will refer this as the intrinsic constraints on the size of above sets.
REFERENCES,0.25041736227045075,"A.1
PROOF OF LEMMA 2.1"
REFERENCES,0.25125208681135225,"Lemma 2.1. For any 1 â‰¤s < t â‰¤K with t âˆ’s = â–³, we have"
REFERENCES,0.25208681135225375,"EÏ€

1{hs(v) = hs(w)}1{ht(v) = ht(w)}

= |L0(â–³)| + (|G0(â–³)| + |L2(â–³)|)J"
REFERENCES,0.25292153589315525,"f + |G0(â–³)| + |G1(â–³)|
,"
REFERENCES,0.25375626043405675,"where the sets are deï¬ned in Deï¬nition 2.2 and hs, ht are the hash values as in Algorithm 2."
REFERENCES,0.25459098497495825,"Proof. To check whether a hash sample generated by MinHash collides (under some permutation
Ï€), it sufï¬ces to look at the permuted location vector x. If a collision happens, after permuted by Ï€,
type â€œOâ€ point must appear before the ï¬rst â€œÃ—â€ point. That said, the minimal permutation index of
â€œOâ€ elements must be smaller than that of â€œÃ—â€ elements. If the hash sample does not collide, then
the ï¬rst â€œÃ—â€ must appear before the ï¬rst â€œOâ€. Note that â€œâˆ’â€ points does not affect the collision."
REFERENCES,0.25542570951585974,"To compute the variance of the estimator, it sufï¬ces to compute E[1s1t]. Let L, G and H be the
union of Lâ€™s, Gâ€™s and Hâ€™s, respectively. In the following, we say that an index i belongs to a set if i
is the ï¬rst term of an element in that set. We have"
REFERENCES,0.25626043405676124,"|L| = a,
|H| = f âˆ’a,
|G| = D âˆ’f."
REFERENCES,0.2570951585976628,"One key observation is that, for a pair (i, j) in above sets, the hash index Ï€s(i) will be the hash index
of Ï€t(j). We begin by decomposing the expectation into"
REFERENCES,0.2579298831385643,"E[1s1t] = P[collision s, collision t] =
X"
REFERENCES,0.2587646076794658,"iâˆ—sâˆˆL
P[collision s at iâˆ—
s, collision t] ="
X,0.2595993322203673,"2
X p=0 X"
X,0.2604340567612688,"iâˆ—
sâˆˆLp
P[collision s at iâˆ—
s, collision t].
(11)"
X,0.2612687813021703,"where iâˆ—
s is the location of the original â€œOâ€ in vector x that collides for s-th hash sample. It is
different from the exact location of collision in x(Ï€s). Note that the permutation is totally random,
so the location of collision is independent of 1s, and uniformly distributed among all type â€œOâ€ pairs."
X,0.2621035058430718,Under review as a conference paper at ICLR 2022
X,0.2629382303839733,"1) When iâˆ—
s âˆˆL0. In this case, the minimum index of the type â€œOâ€ pair in x(Ï€s), Ï€s(iâˆ—
s), is shifted
to another type â€œOâ€ pair in x(Ï€t). Therefore, the indices of pairs with the ï¬rst element being â€œOâ€
or â€œÃ—â€ originally in x(Ï€s) will still be greater than Ï€t(iâˆ—
s). If sample s collides at iâˆ—
s, hash sample t
will collide when"
X,0.2637729549248748,"1. All the points in G1, after permutation Ï€s, is greater than Ï€s(iâˆ—
s). In this case, regardless of
the permuted G0, hash t will always collide."
X,0.2646076794657763,"2. There exist points in G1 after permutation Ï€s smaller than Ï€s(iâˆ—
s), and also points in G0 that
is smaller than the minimum of permuted G1."
X,0.2654424040066778,"Consequently, we have for iâˆ—
s âˆˆL0,"
X,0.2662771285475793,"P[collision s at iâˆ—
s, collision t]
= P[Ï€s(iâˆ—
s) < Ï€s(i), âˆ€i âˆˆH âˆªL/iâˆ—
s, and min
jâˆˆG1 Ï€s(j) > Ï€s(iâˆ—
s)]"
X,0.2671118530884808,"+ P[Ï€s(iâˆ—
s) < Ï€s(i), âˆ€i âˆˆH âˆªL/iâˆ—
s, and min
jâˆˆG0 Ï€s(j) < min
jâˆˆG1 Ï€s(j) < Ï€s(iâˆ—
s)] = 1"
X,0.2679465776293823,"a Â·
a
f + |G1| +
|G0|
f + |G0| + |G1| Â·
|G1|
f + |G1| Â· a f Â· 1 a"
X,0.2687813021702838,"=
1
f + |G1| +
|G0| Â· |G1|
(f + |G0| + |G1|)(f + |G1|)f .
(12)"
X,0.2696160267111853,"This probability holds for âˆ€iâˆ—
s âˆˆL0."
X,0.2704507512520868,"2) When iâˆ—
s âˆˆL1. Similarly, we consider the condition where iâˆ—
s âˆˆL1, and both hash samples
collide. In this case, Ï€s(iâˆ—
s) would be shifted to a â€œÃ—â€ pair in x(Ï€t). That is, the indices of pairs with
the ï¬rst element being â€œOâ€ or â€œÃ—â€ originally in x(Ï€s) will all become greater than Ï€s(iâˆ—
s), which
now is the location of a â€œÃ—â€ pair in x(Ï€t). Thus, to make hash t collide, we will need:"
X,0.27128547579298834,"â€¢ At least one point from G0 is smaller than any other points in H âˆªL âˆªG1 after permutation
Ï€s."
X,0.27212020033388984,"Therefore, for any iâˆ—
s âˆˆL1,"
X,0.27295492487479134,"P[collision s at iâˆ—
s, collision t]
= P[Ï€s(iâˆ—
s) < Ï€s(i), âˆ€i âˆˆH âˆªL/iâˆ—
s, and min
jâˆˆG0 Ï€s(j) < min{Ï€s(iâˆ—
s), min
jâˆˆG1 Ï€s(j)}]"
X,0.27378964941569284,"=
|G0|
f + |G0| + |G1| Â· a f Â· 1 a"
X,0.27462437395659434,"=
|G0|
(f + |G0| + |G1|)f ,
(13)"
X,0.27545909849749584,"which is true for âˆ€iâˆ—
s âˆˆL1."
X,0.27629382303839733,"3) When iâˆ—
s âˆˆL2."
X,0.27712854757929883,"In this scenario, Ï€s(iâˆ—
s) would be shifted to a â€œâˆ’â€ pair in x(Ï€t). Therefore, if hash s collides, hash t
will also collide when:"
X,0.27796327212020033,"â€¢ After applying Ï€s, the minimum of L0 âˆªH0 âˆªG0 is smaller than the minimum of L1 âˆª
H1 âˆªG1."
X,0.27879799666110183,"Thus, we obtain that for any iâˆ—
s âˆˆL2,"
X,0.27963272120200333,"P[collision s at iâˆ—
s, collision t]
= P[Ï€s(iâˆ—
s) < Ï€s(i), âˆ€i âˆˆH âˆªL/iâˆ—
s, and
min
jâˆˆL0âˆªG0âˆªH0 Ï€s(j) <
min
jâˆˆL1âˆªG1âˆªH1 Ï€s(j)]"
X,0.28046744574290483,â‰œP[â„¦].
X,0.28130217028380633,Under review as a conference paper at ICLR 2022
X,0.28213689482470783,"Let 1s,iâˆ—s denote the event {Ï€s(iâˆ—
s) < Ï€s(i), âˆ€i âˆˆH âˆªL/iâˆ—
s}. Then â„¦can be separated into the
following several cases:"
X,0.28297161936560933,"1. â„¦1:
1s,iâˆ—s, minjâˆˆL0âˆªH0 Ï€s(j)
<
minjâˆˆL1âˆªH1 Ï€s(j), and minjâˆˆL0âˆªH0 Ï€s(j)
<
minjâˆˆG1 Ï€s(j)."
X,0.2838063439065108,"2. â„¦2:
1s,iâˆ—s, minjâˆˆL0âˆªH0 Ï€s(j)
<
minjâˆˆL1âˆªH1 Ï€s(j), and minjâˆˆL0âˆªH0 Ï€s(j)
>
minjâˆˆG1 Ï€s(j) > minjâˆˆG0 Ï€s(j) > Ï€s(iâˆ—
s)."
X,0.2846410684474123,"3. â„¦3:
1s,iâˆ—s, minjâˆˆL0âˆªH0 Ï€s(j)
<
minjâˆˆL1âˆªH1 Ï€s(j), and minjâˆˆL0âˆªH0 Ï€s(j)
>
minjâˆˆG1 Ï€s(j) > Ï€s(iâˆ—
s) > minjâˆˆG0 Ï€s(j)."
X,0.2854757929883139,"4. â„¦4: 1s,iâˆ—s, minjâˆˆL0âˆªH0 Ï€s(j) < minjâˆˆL1âˆªH1 Ï€s(j), and minjâˆˆL0âˆªH0 Ï€s(j) > Ï€s(iâˆ—
s) >
minjâˆˆG1 Ï€s(j) > minjâˆˆG0 Ï€s(j)."
X,0.2863105175292154,"5. â„¦5: 1s,iâˆ—s, minjâˆˆL0âˆªH0 Ï€s(j) > minjâˆˆL1âˆªH1 Ï€s(j), and Ï€s(iâˆ—
s) < minjâˆˆG0 Ï€s(j) <
minjâˆˆL1âˆªH1âˆªG1 Ï€s(j)."
X,0.2871452420701169,"6. â„¦6: 1s,iâˆ—s, minjâˆˆL0âˆªH0 Ï€s(j) > minjâˆˆL1âˆªH1 Ï€s(j), and minjâˆˆG0 Ï€s(j) < Ï€s(iâˆ—
s) <
minjâˆˆL1âˆªH1âˆªG1 Ï€s(j)."
X,0.2879799666110184,We can compute the probability of each event as
X,0.2888146911519199,P[â„¦1] = 1
X,0.2896494156928214,"a Â·
a
f + |G1| Â·
|L0| + |H0|
|L0| + |H0| + |L1| + |H1| + |G1|,"
X,0.2904841402337229,"=
a âˆ’|G0|
(f âˆ’|G0|)(f + |G1|),"
X,0.2913188647746244,P[â„¦2] = 1
X,0.2921535893155259,"a Â·
a
f + |G0| + |G1| Â·
|G0|
|G0| + |G1| + |L0| + |H0| + |L1| + |H1|"
X,0.2929883138564274,"Â·
|G1|
|L0| + |H0| + |L1| + |H1| + |G1| Â·
|L0| + |H0|
|L0| + |H0| + |L1| + |H1|"
X,0.2938230383973289,"=
1
f + |G0| + |G1| Â· |G0|"
X,0.29465776293823037,"f
Â·
|G1|
f âˆ’|G0| Â·
a âˆ’|G0|
f âˆ’|G0| âˆ’|G1|"
X,0.29549248747913187,"=
|G0| Â· |G1| Â· (a âˆ’|G0|)
(f + |G0| + |G1|)(f âˆ’|G0|)(f âˆ’|G0| âˆ’|G1|)f ,"
X,0.29632721202003337,"P[â„¦3] =
|G0|
f + |G0| + |G1| Â·
1
f + |G1| Â·
|G1|
|L0| + |H0| + |L1| + |H1| + |G1| Â·
|L0| + |H0|
|L0| + |H0| + |L1| + |H1|"
X,0.29716193656093487,"=
|G0| Â· |G1| Â· (a âˆ’|G0|)
(f + |G0| + |G1|)(f + |G1|)(f âˆ’|G0|)(f âˆ’|G0| âˆ’|G1|),"
X,0.29799666110183637,"P[â„¦4] =
|G0|
f + |G0| + |G1| Â·
|G1|
f + |G1| Â· 1"
X,0.2988313856427379,"f Â·
|L0| + |H0|
|L0| + |H0| + |L1| + |H1|"
X,0.2996661101836394,"=
|G0| Â· |G1| Â· (a âˆ’|G0|)
(f + |G0| + |G1|)(f + |G1|)(f âˆ’|G0| âˆ’|G1|)f ,"
X,0.3005008347245409,"P[â„¦5] =
1
f + |G0| + |G1| Â·
|G0|
|G0| + |G1| + |L0| + |H0| + |L1| + |H1| Â·
|L1| + |H1|
|L0| + |H0| + |L1| + |H1|"
X,0.3013355592654424,"=
|G0| Â· (f âˆ’a âˆ’|G1|)
(f + |G0| + |G1|)(f âˆ’|G0| âˆ’|G1|)f ,"
X,0.3021702838063439,"P[â„¦6] =
|G0|
f + |G0| + |G1| Â· 1"
X,0.3030050083472454,"f Â·
|L1| + |H1|
|L0| + |H0| + |L1| + |H1|"
X,0.3038397328881469,"=
|G0| Â· (f âˆ’a âˆ’|G1|)
(f + |G0| + |G1|)(f âˆ’|G0| âˆ’|G1|)f ."
X,0.3046744574290484,Under review as a conference paper at ICLR 2022
X,0.3055091819699499,Note that
X,0.3063439065108514,P[â„¦2] + P[â„¦3] + P[â„¦4]
X,0.3071786310517529,"=
|G0| Â· |G1| Â· (a âˆ’|G0|)
(f + |G0| + |G1|)(f âˆ’|G0| âˆ’|G1|)"
X,0.3080133555926544,"
1
(f âˆ’|G0|)f +
1
(f âˆ’|G0|)(f + |G1|) +
1
f(f + |G1|) "
X,0.3088480801335559,"=
|G0| Â· |G1| Â· (a âˆ’|G0|)(3f âˆ’|G0| + |G1|)
(f + |G0| + |G1|)(f âˆ’|G0| âˆ’|G1|)f(f âˆ’|G0|)(f + |G1|)."
X,0.3096828046744574,"Summing up all the terms together, we obtain P[â„¦] as"
X,0.3105175292153589,"6
X"
X,0.3113522537562604,"n=1
P[â„¦n] = f(f + |G0| + |G1|)(f âˆ’|G0| âˆ’|G1|)(a âˆ’|G0|) + |G0||G1|(a âˆ’|G0|)(3f âˆ’|G0| + |G1|)"
X,0.3121869782971619,(f + |G0| + |G1|)(f âˆ’|G0| âˆ’|G1|)(f âˆ’|G0|)(f + |G1|)f
X,0.31302170283806346,"+
2|G0|(f âˆ’a âˆ’|G1|)(f âˆ’|G0|)(f + |G1|)
(f + |G0| + |G1|)(f âˆ’|G0| âˆ’|G1|)(f âˆ’|G0|)(f + |G1|)f"
X,0.31385642737896496,= (a âˆ’|G0|)(f + |G0| âˆ’|G1|)(f âˆ’|G0|)(f + |G1|) + 2|G0|(f âˆ’a âˆ’|G1|)(f âˆ’|G0|)(f + |G1|)
X,0.31469115191986646,(f + |G0| + |G1|)(f âˆ’|G0| âˆ’|G1|)(f âˆ’|G0|)(f + |G1|)f
X,0.31552587646076796,"=
(a + |G0|)(f âˆ’|G0| âˆ’|G1|)(f âˆ’|G0|)(f + |G1|)
(f + |G0| + |G1|)(f âˆ’|G0| âˆ’|G1|)(f âˆ’|G0|)(f + |G1|)f"
X,0.31636060100166946,"=
a + |G0|
(f + |G0| + |G1|)f ,
(14)"
X,0.31719532554257096,"which holds for âˆ€iâˆ—
s âˆˆL2. Now combining (12), (13), (14) with (11), we obtain"
X,0.31803005008347246,"E[1s1t] =
|L0|
f + |G1| +
|G0||G1||L0|
(f + |G0| + |G1|)(f + |G1|)f +
|G0||L1|
(f + |G0| + |G1|)f +
(a + |G0|)|L2|
(f + |G0| + |G1|)f . (15)"
X,0.31886477462437396,"Here, recall that the sets are associated with all 1 â‰¤s < t â‰¤K such that â–³= t âˆ’s. Using the
intrinsic constraints (10), after some calculation we can simplify (15) as"
X,0.31969949916527546,"EÏ€[1s1t] =
|L0|
f + |G0| + |G1| +
a(|G0| + |L2|)
(f + |G0| + |G1|)f ,"
X,0.32053422370617696,which completes the proof.
X,0.32136894824707846,"A.2
PROOF OF THEOREM 2.2"
X,0.32220367278797996,"Theorem 2.2. Under the same setting as in Lemma 2.1, the variance of Ë†J0,Ï€ is"
X,0.32303839732888145,"V ar[ Ë†J0,Ï€] = J"
X,0.32387312186978295,"K + 2 PK
s=2(s âˆ’1)Î˜Kâˆ’s+1"
X,0.32470784641068445,"K2
âˆ’J2,"
X,0.32554257095158595,"where Î˜â–³â‰œEÏ€

1{hs(v) = hs(w)}1{ht(v) = ht(w)}

as in Lemma 2.1 with any t âˆ’s = â–³."
X,0.32637729549248745,"Proof. By the expansion of variance formula, since E[12
s] = E[1s] = J, we have"
X,0.327212020033389,"V ar[ Ë†J0,Ï€] = J K +"
X,0.3280467445742905,"PK
s=1
PK
tÌ¸=s E[1s1t]"
X,0.328881469115192,"K2
âˆ’J2.
(16)"
X,0.3297161936560935,"Note here that for âˆ€t > s, the t-th hash sample uses Ï€t as the permutation, which is shifted right-
wards by â–³= t âˆ’s from Ï€s. Thus, we have E[1s1t] = E[1sâˆ’i1tâˆ’i] for âˆ€0 < i < s âˆ§t, which
implies E[1s1t] = E[111tâˆ’s+1], âˆ€s < t. Since by assumption K â‰¤D, we have K
X s K
X"
X,0.330550918196995,"tÌ¸=s
E[1s1t] = 2E

(1112 + 1113 + ... + 111K) + (1213 + ... + 121K) + ... + 1Kâˆ’11K
"
X,0.3313856427378965,"= 2E

(1112 + 1113 + ... + 111K) + (1112 + ... + 111Kâˆ’1) + ... + 1112
"
X,0.332220367278798,"Under review as a conference paper at ICLR 2022 = 2 K
X"
X,0.3330550918196995,"s=2
(s âˆ’1)E[111Kâˆ’s+2] â‰œ2 K
X"
X,0.333889816360601,"s=2
(s âˆ’1)Î˜Kâˆ’s+1.
(17)"
X,0.3347245409015025,"Finally, integrating (16), (17) and Lemma 2.1 completes the proof."
X,0.335559265442404,"A.3
PROOF OF THEOREM 3.2"
X,0.3363939899833055,"Theorem 3.2. Let a, f be deï¬ned as in (5). When 0 < a < f â‰¤D (J /âˆˆ{0, 1}), we have"
X,0.337228714524207,"V ar[ Ë†JÏƒ,Ï€] = J"
X,0.3380634390651085,K + (K âˆ’1) ËœE
X,0.33889816360601,"K
âˆ’J2,
(18)"
X,0.3397328881469115,"where with l = max(0, D âˆ’2f + a), and"
X,0.34056761268781305,"ËœE =
X"
X,0.34140233722871455,"{l0,l2,g0,g1}"
X,0.34223706176961605,"( 
l0
f + g0 + g1
+
a(g0 + l2)
(f + g0 + g1)f  Ã—"
X,0.34307178631051755,"Dâˆ’fâˆ’1
X s=l"
X,0.34390651085141904," Dâˆ’f
s
"
X,0.34474123539232054," Dâˆ’aâˆ’1
Dâˆ’fâˆ’1
"
X,0.34557595993322204," 
fâˆ’aâˆ’1
Dâˆ’fâˆ’sâˆ’1
  s
n1
 Dâˆ’fâˆ’s
n2
 Dâˆ’fâˆ’s
n3
 fâˆ’aâˆ’(Dâˆ’fâˆ’s)
n4
 
aâˆ’1
aâˆ’l1âˆ’l2
"
X,0.34641068447412354," Dâˆ’1
a
 ) . (19)"
X,0.34724540901502504,"The feasible set {l0, l2, g0, g1} satisï¬es the intrinsic constraints (6), and"
X,0.34808013355592654,"n1 = g0 âˆ’(D âˆ’f âˆ’s âˆ’g1),
n2 = D âˆ’f âˆ’s âˆ’g1,
n3 = l2 âˆ’g0 + (D âˆ’f âˆ’s âˆ’g1), n4 = l1 âˆ’(D âˆ’f âˆ’s âˆ’g1)."
X,0.34891485809682804,"Also, when a = 0 or f = a (J = 0 or J = 1), we have V ar[ Ë†JÏƒ,Ï€] = 0."
X,0.34974958263772954,"Proof. Similar to the proof of Theorem 2.2, we denote Î˜â–³= EÏƒ,Ï€[1s1t] with |t âˆ’s| = â–³. Note
that now the expectation is taken w.r.t. both two independent permutations Ïƒ and Ï€. Since Ïƒ is
random, we know that Î˜1 = Î˜2 = Â· Â· Â· = Î˜Kâˆ’1. Then by the variance formula, we have"
X,0.35058430717863104,"V ar[ Ë†JÏƒ,Ï€] = J2"
X,0.35141903171953254,K âˆ’(K âˆ’1)Î˜1
X,0.35225375626043404,"K
âˆ’J2
(20)"
X,0.35308848080133554,"Hence, it sufï¬ces to consider Î˜1. In this proof, we will set â–³= 1 and drop the notation â–³for
conciseness, and denote ËœE = Î˜1 from now on. First, we note that Lemma 2.1 gives the desired
quantity conditional on Ïƒ. By the law of total probability, we have"
X,0.35392320534223703,ËœE = EÏƒ
X,0.3547579298831386,"
|L0|
f + |G0| + |G1| +
a(|G0| + |L2|)
(f + |G0| + |G1|)f"
X,0.3555926544240401,"
,
(21)"
X,0.3564273789649416,"where the sizes of sets are random depending on the initial permutation Ïƒ (i.e.
counted after
permuting by Ïƒ). As a result, the problem turns into deriving the distribution of |L0|, |L1|, |L2|, |G0|
and |G1| under random permutation Ïƒ, and then taking expectation of (21) with respect to this
additional randomness."
X,0.3572621035058431,"When a = 0, we know that |L0| = |L2| = |G0| = 0, hence the expectation ËœE is trivially 0. Thus, the
V ar[ Ë†JÏƒ,Ï€] = 0. When f = a, |G1| = 0, and the constraint on the sets becomes"
X,0.3580968280467446,"|L0| + |G0| = |L0| + |L2| = f,
|L2| + |G2| = |G0| + |G2| = D âˆ’f."
X,0.3589315525876461,Then (21) becomes
X,0.3597662771285476,ËœE = EÏƒ
X,0.3606010016694491,"
|L0|
f + |G0| + |G0| + |L2|"
X,0.3614357262103506,f + |G0| 
X,0.3622704507512521,Under review as a conference paper at ICLR 2022 = EÏƒ
X,0.3631051752921536,|L0| + |G0| + |L2|
X,0.3639398998330551,"f + |G0| 
â‰¡1."
X,0.3647746243739566,"Therefore, when f = a, we also have V ar[ Ë†JÏƒ,Ï€] = 0."
X,0.3656093489148581,"Next, we will consider the general case where 0 < a < f â‰¤D. This can be considered as a
combinatorial problem where we randomly arrange a type â€œOâ€, (f âˆ’a) type â€œÃ—â€ and (D âˆ’f)
type â€œâˆ’â€ points in a circle. We are interested in the distribution of the number of {O, O}, {O, Ã—},
{O, âˆ’}, {âˆ’, O} and {âˆ’, Ã—} pairs of consecutive points in clockwise direction. We consider this
procedure in two steps, where we ï¬rst place â€œÃ—â€ and â€œâˆ’â€ points, and then place â€œOâ€ points."
X,0.3664440734557596,Step 1. Randomly place â€œÃ—â€ and â€œâˆ’â€ points on the circle.
X,0.3672787979966611,"In this step, four types of pairs may appear: {âˆ’, âˆ’}, {âˆ’, Ã—}, {Ã—, Ã—} and {Ã—, âˆ’}. Denote C1, C2,
C3 and C4 as the collections of above pairs. Since"
X,0.36811352253756263,"|C1| + |C4| = |C1| + |C2| = D âˆ’f,
|C2| + |C3| = |C2| + |C4| = f âˆ’a,"
X,0.36894824707846413,"knowing the size of one set gives information on the size of all the sets. Thus, we can characterize the
joint distribution by analyzing the distribution of |C1|. First, placing (D âˆ’f) â€œâˆ’â€ points on a circle
leads to (Dâˆ’f) number of {âˆ’, âˆ’} pairs. This (Dâˆ’f) elements can be regarded as the borders that
split the circle into (D âˆ’f) bins. Now, we randomly throw (f âˆ’a) number of â€œÃ—â€ points into these
bins. If at least one â€œÃ—â€ falls into one bin, then the number of {âˆ’, âˆ’} pairs (|C1|) would reduce by
1, while |C2| and |C4| would increase by 1. If z â€œÃ—â€ points fall into one bin, then the number of
{Ã—, Ã—} (|C3|) would increase by (z âˆ’1). Notice that since s â‰¤D âˆ’f and D âˆ’f âˆ’s â‰¤f âˆ’a, we
have max(0, D âˆ’2f + a) â‰¤s â‰¤D âˆ’f. Consequently, for s in this range, we have"
X,0.36978297161936563,"P
n
|C1| = s
o
= P
n
|C1| = s, |C3| = f âˆ’a âˆ’(D âˆ’f âˆ’s)
o ="
X,0.37061769616026713,"  Dâˆ’f
Dâˆ’fâˆ’s
 
fâˆ’aâˆ’1
Dâˆ’fâˆ’sâˆ’1
"
X,0.37145242070116863," Dâˆ’aâˆ’1
Dâˆ’fâˆ’1
 ="
X,0.3722871452420701," Dâˆ’f
s
 
fâˆ’aâˆ’1
Dâˆ’fâˆ’sâˆ’1
"
X,0.3731218697829716," Dâˆ’aâˆ’1
Dâˆ’fâˆ’1

.
(22)"
X,0.3739565943238731,"The second line is due to the stars and bars problem that the number of ways to place n unlabeled
balls in m distinct bins such that each bin has at least one ball is
  nâˆ’1
mâˆ’1

. For |C1| = s, we need
n = f âˆ’a (number of â€œÃ—â€) and m = |C2| = D âˆ’f âˆ’s. Moreover, the number of ways to
place n balls in m distinct bins is
 n+mâˆ’1
mâˆ’1

. When counting the total number of possibilities, we
have n = f âˆ’a and m = D âˆ’f. This gives the denominator. We notice that (22) is actually a
hyper-geometric distribution."
X,0.3747913188647746,Step 2. Randomly place â€œOâ€ points on the circle.
X,0.3756260434056761,We have the probability mass function
X,0.3764607679465776,"P[Î¨] â‰œP
n
|L1| = l1, |L2| = l2, |G0| = g0, |G1| = g1
o ="
X,0.3772954924874791,"Dâˆ’fâˆ’1
X"
X,0.3781302170283806,"s=Dâˆ’2f+a
P
n
|L1| = l1, |L2| = l2, |G0| = g0, |G1| = g1
|C1| = s
o
P
n
|C1| = s
o
.
(23)"
X,0.3789649415692821,"Now it remains to compute the distribution conditional on |C1|. Here we drop |L0| because it is
intrinsically determined by |L1| and |L2|. Again, given a placement of all â€œÃ—â€ and â€œâˆ’â€ points, each
consecutive pair can be regarded as a distinct bin. Therefore, now the problem is to randomly throw
a type â€œOâ€ points into that (D âˆ’a) bins, given that we have placed type â€œÃ—â€ and â€œâˆ’â€ points on the
circle with |C1| = s (and thus |C2| = |C3| = D âˆ’f âˆ’s and |C4| = f âˆ’a âˆ’(D âˆ’f âˆ’s) are also
determined correspondingly). In the following, we count the number of â€œOâ€ points that fall in Ci,
i = 1, 2, 3, 4, to make the event Î¨ happen. Note that"
X,0.3797996661101836,Under review as a conference paper at ICLR 2022
X,0.3806343906510851,"â€¢ When at least one â€œOâ€ point falls into C1 (between {âˆ’, âˆ’}), |L2| and |G0| increase by 1."
X,0.3814691151919866,"â€¢ When at least one â€œOâ€ point falls into C2 (between {âˆ’, Ã—}), |L1| and |G0| increase by 1,
while |G1| decreases by 1."
X,0.3823038397328882,"â€¢ When at least one â€œOâ€ point falls into C3 (between {Ã—, âˆ’}), |L2| increases by 1."
X,0.3831385642737897,"â€¢ When at least one â€œOâ€ point falls into C4 (between {Ã—, Ã—}), |L1| increases by 1."
X,0.38397328881469117,"We denote the number of bins in Ci, i = 1, 2, 3, 4 that contain at least one â€œOâ€ point as n1, n2, n3, n4,
respectively. As a result of above reasoning, in the event Î¨, we have
ï£±
ï£´
ï£´
ï£² ï£´
ï£´
ï£³"
X,0.38480801335559267,"n1 + n3 = l2,
n2 + n4 = l1,
n1 + n2 = g0,
D âˆ’f âˆ’s âˆ’n2 = g1."
X,0.38564273789649417,"Solving the equations gives
ï£±
ï£´
ï£´
ï£² ï£´
ï£´
ï£³"
X,0.38647746243739567,"n1 = g0 âˆ’(D âˆ’f âˆ’s âˆ’g1),
n2 = D âˆ’f âˆ’s âˆ’g1,
n3 = l2 âˆ’g0 + (D âˆ’f âˆ’s âˆ’g1),
n4 = l1 âˆ’(D âˆ’f âˆ’s âˆ’g1)."
X,0.38731218697829717,"Note that P4
i=1 ni = l1 + l2. Therefore, event Î¨ is equivalent to randomly pick n1, n2, n3 and
n4 bins in C1,...,C4, and then distribute a type â€œOâ€ points in these (l1 + l2) bins such that each bin
contains at least one â€œOâ€. Hence, we obtain"
X,0.38814691151919867,"P
n
|L1| = l1, |L2| = l2, |G0| = g0, |G1| = g1
|C1| = s
o
="
X,0.38898163606010017,"  s
n1
 Dâˆ’fâˆ’s
n2
 Dâˆ’fâˆ’s
n3
 fâˆ’aâˆ’(Dâˆ’fâˆ’s)
n4
 
aâˆ’1
l1+l2âˆ’1
"
X,0.38981636060100167,"  Dâˆ’1
Dâˆ’aâˆ’1
 ="
X,0.39065108514190316,"  s
n1
 Dâˆ’fâˆ’s
n2
 Dâˆ’fâˆ’s
n3
 fâˆ’aâˆ’(Dâˆ’fâˆ’s)
n4
 
aâˆ’1
aâˆ’l1âˆ’l2
"
X,0.39148580968280466," Dâˆ’1
a

, (24)"
X,0.39232053422370616,"which is also multi-variate hyper-geometric distributed. Now combining (22), (23) and (24), we
obtain the joint distribution of |L0|, |L1|, |L2|, |G0| and |G1| as"
X,0.39315525876460766,"P
n
|L1| = l1, |L2| = l2, |G0| = g0, |G1| = g1
o ="
X,0.39398998330550916,"Dâˆ’fâˆ’1
X"
X,0.39482470784641066,"s=max(0,Dâˆ’2f+a)"
X,0.39565943238731216,"  s
n1
 Dâˆ’fâˆ’s
n2
 Dâˆ’fâˆ’s
n3
 fâˆ’aâˆ’(Dâˆ’fâˆ’s)
n4
 
aâˆ’1
aâˆ’l1âˆ’l2
"
X,0.3964941569282137," Dâˆ’1
a

Â·"
X,0.3973288814691152," Dâˆ’f
s
 
fâˆ’aâˆ’1
Dâˆ’fâˆ’sâˆ’1
"
X,0.3981636060100167," Dâˆ’aâˆ’1
Dâˆ’fâˆ’1

. (25)"
X,0.3989983305509182,"Now let Î be the feasible set of (l0, l1, g0, g1, g2) that satisï¬es the intrinsic constraints (10). The
desired expectation w.r.t. both Ï€ and Ïƒ can thus be written as"
X,0.3998330550918197,"ËœE =
X Î"
X,0.4006677796327212,"
l0
f + g0 + g1
+
a(g0 + l2)
(f + g0 + g1)f 
Â· ï£« ï£­"
X,0.4015025041736227,"Dâˆ’fâˆ’1
X"
X,0.4023372287145242,"s=max(0,Dâˆ’2f+a)"
X,0.4031719532554257,"  s
n1
 Dâˆ’fâˆ’s
n2
 Dâˆ’fâˆ’s
n3
 fâˆ’aâˆ’(Dâˆ’fâˆ’s)
n4
 
aâˆ’1
aâˆ’l1âˆ’l2
"
X,0.4040066777963272," Dâˆ’1
a

Â·"
X,0.4048414023372287," Dâˆ’f
s
 
fâˆ’aâˆ’1
Dâˆ’fâˆ’sâˆ’1
"
X,0.4056761268781302," Dâˆ’aâˆ’1
Dâˆ’fâˆ’1
 ï£¶ ï£¸."
X,0.4065108514190317,The desired result can then follows by (20).
X,0.4073455759599332,"A.4
PROOF OF PROPOSITION 3.3"
X,0.4081803005008347,"Proposition 3.3 (Symmetry). V ar[ Ë†JÏƒ,Ï€] is the same for the (D, f, a)-data pair and the (D, f, f âˆ’
a)-data pair, âˆ€0 â‰¤a â‰¤f â‰¤D."
X,0.4090150250417362,Under review as a conference paper at ICLR 2022
X,0.40984974958263776,"Proof. For ï¬xed a, f, D, let ËœE1 be the expectation deï¬ned in Theorem 3.2 for (v1, w1), and ËœE2 be
that for (v2, w2). From Theorem 3.2 we know that"
X,0.41068447412353926,"ËœE1 = E(l0,l2,g0,g1)
h
l0
f + g0 + g1
+
a(g0 + l2)
(f + g0 + g1)f i
,"
X,0.41151919866444076,"where (l0, l2, g0, g1) follows the distribution of (|L0|, |L2|, |G0|, |G1|) associated with the location
vector x1 of (v1, v2). For data pair (v2, w2), we can consider its location vector x2 as swapping
the â€œOâ€ and â€œÃ—â€ entries of x1. Now we denote the size of the corresponding sets (Deï¬nition 2.2)
of x2 as lâ€²
is, gâ€²
is, hâ€²
is, for i = 0, 1, 2. Since Ïƒ is applied before hashing, by symmetry there is a
one-to-one correspondence between the two location vectors. More speciï¬cally, lâ€²
0 corresponds to
h1, gâ€²
0 corresponds to g1, gâ€²
1 corresponds to g0, and lâ€²
2 corresponds to h2. Therefore, in probability
we can write"
X,0.41235392320534225,"ËœE2 = E(lâ€²
0,lâ€²
2,gâ€²
0,gâ€²
1)
h
lâ€²
0
f + gâ€²
0 + gâ€²
1
+
a(gâ€²
0 + lâ€²
2)
(f + gâ€²
0 + gâ€²
1)f i"
X,0.41318864774624375,"= E(h1,h2,g0,g1)
h
h1
f + g0 + g1
+ (f âˆ’a)(g1 + h2)"
X,0.41402337228714525,"(f + g0 + g1)f i
."
X,0.41485809682804675,"Consequently, we have"
X,0.41569282136894825,"ËœE1 âˆ’ËœE2 = E(l0,l2,h1,h2,g0,g1)
h
l0 âˆ’h1
f + g0 + g1
+ a(g0 + l2) âˆ’(f âˆ’a)(g1 + h2)"
X,0.41652754590984975,"(f + g0 + g1)f i
."
X,0.41736227045075125,"In the sequel, the subscript of expectation is suppressed for conciseness. Exploiting the constraints
(10), we deduce that h1 = (f âˆ’a) âˆ’l1 âˆ’g1, h2 = l0 + g0 + l1 + g1 âˆ’a and l0 + l1 = a âˆ’l2.
Using these facts we obtain"
X,0.41819699499165275,"ËœE1 âˆ’ËœE2 = E
h(l0 âˆ’(f âˆ’a) + l1 + g1)f + a(g0 + l2) âˆ’(f âˆ’a)(l0 + g0 + l1 + 2g1 âˆ’a)"
X,0.41903171953255425,(f + g0 + g1)f i
X,0.41986644407345575,"= E
h(2a âˆ’f + g1 âˆ’l2)f + a(g0 + l2) âˆ’(f âˆ’a)(2g1 + g0 âˆ’l2)"
X,0.42070116861435725,(f + g0 + g1)f i
X,0.42153589315525875,"= E
h2(f + g0 + g1)a âˆ’(f + g0 + g1)f"
X,0.42237061769616024,(f + g0 + g1)f i
X,0.42320534223706174,= 2J âˆ’1.
X,0.4240400667779633,"Comparing the variances of Ë†JÏƒ,Ï€(v1, w1) and Ë†JÏƒ,Ï€(v2, w2), we derive"
X,0.4248747913188648,"V ar[ Ë†JÏƒ,Ï€(v1, w1)] âˆ’V ar[ Ë†JÏƒ,Ï€(v2, w2)] = ( J"
X,0.4257095158597663,K + (K âˆ’1) ËœE1
X,0.4265442404006678,"K
âˆ’J2) âˆ’(1 âˆ’J"
X,0.4273789649415693,"K
+ (K âˆ’1) ËœE2"
X,0.4282136894824708,"K
âˆ’(1 âˆ’J)2)"
X,0.4290484140233723,= âˆ’K âˆ’1
X,0.4298831385642738,"K
(2J âˆ’1) + K âˆ’1"
X,0.4307178631051753,"K
( ËœE1 âˆ’ËœE2) = 0."
X,0.4315525876460768,This completes the proof.
X,0.4323873121869783,"A.5
PROOF OF LEMMA 3.4"
X,0.4332220367278798,"Lemma 3.4 (Increasing Increment). Assume a > 0 and f > a are arbitrary and ï¬xed. Denote ËœED
as in (19) in Theorem 3.2, with D treated as a parameter. Then we have ËœED+1 > ËœED for âˆ€D â‰¥f."
X,0.4340567612687813,"Proof. Let the probability mass function (25) with a, f and dimension D be Pa,f,D(l0, l2, g0, g1).
Conditional on l0, l2, g0, g1 with D elements, the possible values lâ€²
0, lâ€²
2, gâ€²
0, gâ€²
1 when adding a â€œâˆ’â€
are"
X,0.4348914858096828,"â€¢ gâ€²
0 = g0 + 1, lâ€²
0 = l0, lâ€²
2 = l2, gâ€²
1 = g1. This is true when the new elements falls between a
pair of (Ã—, O), with probability l1+l2âˆ’g0 D
."
X,0.4357262103505843,Under review as a conference paper at ICLR 2022
X,0.4365609348914858,"â€¢ gâ€²
1 = g1 + 1, lâ€²
0 = l0, lâ€²
2 = l2, gâ€²
0 = g0, when the new elements falls between a pair of
(Ã—, Ã—), with probability fâˆ’aâˆ’l1âˆ’g1 D
."
X,0.4373956594323873,"â€¢ gâ€²
1 = g1 + 1, lâ€²
2 = l2 + 1, lâ€²
0 = l0, gâ€²
0 = g0, when the new elements falls between a pair of
(O, Ã—), with probability l1 D."
X,0.43823038397328884,"â€¢ lâ€²
0 = l0 âˆ’1, lâ€²
2 = l2 + 1, gâ€²
0 = g0 + 1, gâ€²
1 = g1, when the new elements falls between a pair
of (O, O), with probability l0 D."
X,0.43906510851419034,"â€¢ All values unchanged, when the â€œâˆ’â€ falls between other types of pairs, with probability
Dâˆ’f+g0+g1 D
."
X,0.43989983305509184,"Denote ÎD as the feasible set satisfying (10) with dimension D â‰¥f. Above reasoning builds a
correspondence between ÎD and ÎD+1. More precisely, we have"
X,0.44073455759599334,"ËœED+1 =
X ÎD+1"
X,0.44156928213689484,"
lâ€²
0
f + gâ€²
0 + gâ€²
1
+
a(gâ€²
0 + lâ€²
2)
(f + gâ€²
0 + gâ€²
1)f"
X,0.44240400667779634,"
Pa,f,D+1(lâ€²
0, lâ€²
2, gâ€²
0, gâ€²
1) =
X ÎD"
X,0.44323873121869783,"
l0
f + g0 + g1 + 1 +
a(g0 + l2 + 1)
(f + g0 + g1 + 1)f"
X,0.44407345575959933,l1 + l2 âˆ’g0
X,0.44490818030050083,"D
Pa,f,D(l0, l2, g0, g1)"
X,0.44574290484140233,"+

l0
f + g0 + g1 + 1 +
a(g0 + l2)
(f + g0 + g1 + 1)f"
X,0.44657762938230383,f âˆ’a âˆ’l1 âˆ’g1
X,0.44741235392320533,"D
Pa,f,D(l0, l2, g0, g1)"
X,0.44824707846410683,"+

l0
f + g0 + g1 + 1 +
a(g0 + l2 + 1)
(f + g0 + g1 + 1)f  l1"
X,0.44908180300500833,"DPa,f,D(l0, l2, g0, g1)"
X,0.44991652754590983,"+

l0 âˆ’1
f + g0 + g1 + 1 +
a(g0 + l2 + 2)
(f + g0 + g1 + 1)f  l0"
X,0.4507512520868113,"DPa,f,D(l0, l2, g0, g1)"
X,0.4515859766277129,"+

l0
f + g0 + g1
+
a(g0 + l2)
(f + g0 + g1)f"
X,0.4524207011686144,D âˆ’f + g0 + g1
X,0.4532554257095159,"D
Pa,f,D(l0, l2, g0, g1)

."
X,0.4540901502504174,The increment can be computed as
X,0.4549248747913189,"ËœÎ´D â‰œËœED+1 âˆ’ËœED =
X ÎD"
X,0.4557595993322204,f âˆ’g0 âˆ’g1 D
X,0.4565943238731219,"h 
l0
f + g0 + g1 + 1 âˆ’
l0
f + g0 + g1"
X,0.4574290484140234,"
+
  a(g0 + l2 + 1)"
X,0.4582637729549249,f + g0 + g1 + 1 âˆ’a(g0 + l2)
X,0.4590984974958264,f + g0 + g1 i
X,0.4599332220367279,"âˆ’
l0
D(f + g0 + g1 + 1) âˆ’a(f âˆ’a âˆ’l1 âˆ’g1) âˆ’al0"
X,0.4607679465776294,Df(f + g0 + g1 + 1)
X,0.46160267111853087,"
Pa,f,D(l0, l2, g0, g1) =
X ÎD"
X,0.46243739565943237,(f âˆ’g0 âˆ’g1)[a(f + g1 âˆ’l2) âˆ’fl0]
X,0.46327212020033387,Df(f + g0 + g1)(f + g0 + g1 + 1) âˆ’(f âˆ’a)l0 + a(f âˆ’a âˆ’l1 âˆ’g1)
X,0.46410684474123537,Df(f + g0 + g1 + 1)
X,0.46494156928213687,"
Pa,f,D(l0, l2, g0, g1) =
X ÎD"
X,0.4657762938230384,2af(l1 + g1) âˆ’2f(f âˆ’a)l0 âˆ’2a(f âˆ’a)(g0 + g1)
X,0.4666110183639399,"Df(f + g0 + g1)(f + g0 + g1 + 1)
Pa,f,D(l0, l2, g0, g1)"
X,0.4674457429048414,"= E
h2af(l1 + g1) âˆ’2f(f âˆ’a)l0 âˆ’2a(f âˆ’a)(g0 + g1)"
X,0.4682804674457429,Df(f + g0 + g1)(f + g0 + g1 + 1) i
X,0.4691151919866444,"= E
h2af(f âˆ’a âˆ’h1) âˆ’2f(f âˆ’a)l0 âˆ’2a(f âˆ’a)(g0 + g1 + f âˆ’f)"
X,0.4699499165275459,Df(f + g0 + g1)(f + g0 + g1 + 1) i
X,0.4707846410684474,"= E
h
4a(f âˆ’a)
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.4716193656093489,"i
âˆ’E
h
2ah1 + 2(f âˆ’a)l0
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.4724540901502504,"i
âˆ’E
h
2a(f âˆ’a)
Df(f + g0 + g1 + 1) i"
X,0.4732888146911519,"â‰œ4a(f âˆ’a)E0 âˆ’2aE1 âˆ’2(f âˆ’a)E2 âˆ’2a(f âˆ’a)E3,
(26) where"
X,0.4741235392320534,"E0 = E
h
1
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.4749582637729549,"i
, E1 = E
h
h1
D(f + g0 + g1)(f + g0 + g1 + 1) i
,"
X,0.4757929883138564,Under review as a conference paper at ICLR 2022
X,0.4766277128547579,"E2 = E
h
l0
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.4774624373956594,"i
, E3 = E
h
g2
Df(f + g0 + g1 + 1) i
."
X,0.4782971619365609,"Note that here the expectations are taken w.r.t. the set size distribution with a, f, D. We can expand
the terms of density function (25) to derive"
X,0.4791318864774624,"Pa,f,D(l0, l2, g0, g1) ="
X,0.47996661101836396,"Dâˆ’fâˆ’1
X"
X,0.48080133555926546,"s=max(0,Dâˆ’2f+a)"
X,0.48163606010016696,"(D âˆ’f âˆ’s)(D âˆ’f)!(f âˆ’a âˆ’1)!
[D âˆ’(f + g0 + g1)]![(f + g0 + g1) âˆ’D + s]!g1!(D âˆ’f âˆ’s âˆ’g1)!"
X,0.48247078464106846,"(a âˆ’1)!
(g0 + g1 âˆ’l2)![D âˆ’s + l2 âˆ’(f + g0 + g1)]!(f âˆ’a âˆ’l1 âˆ’g1)!(f + g1 + l1 âˆ’D + s)!l0!(a âˆ’l0 âˆ’1)!
a!(f âˆ’a)!(D âˆ’f âˆ’1)!"
X,0.48330550918196996,"(D âˆ’1)!
."
X,0.48414023372287146,"Denote aâ€² = a âˆ’1, f â€² = f âˆ’1, Dâ€² = D âˆ’1 and lâ€²
0 = l0 âˆ’1. We have"
X,0.48497495826377296,"E2 =
X ÎD"
X,0.48580968280467446,"l0
D(f + g0 + g1)(f + g0 + g1 + 1)Pa,f,D(l0, l2, g0, g1) =
X ÎD"
X,0.48664440734557596,a(a âˆ’1)
X,0.48747913188647746,"D âˆ’1
Â·
1
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.48831385642737896,"Dâ€²âˆ’f â€²âˆ’1
X"
X,0.48914858096828046,"s=max(0,Dâ€²âˆ’2f â€²+aâ€²)"
X,0.48998330550918195,"(Dâ€² âˆ’f â€² âˆ’s)(Dâ€² âˆ’f â€²)!(f â€² âˆ’aâ€² âˆ’1)!
[Dâ€² âˆ’(f â€² + g0 + g1)]![(f â€² + g0 + g1) âˆ’Dâ€² + s]!g1!(Dâ€² âˆ’f â€² âˆ’s âˆ’g1)!"
X,0.49081803005008345,"(aâ€² âˆ’1)!
(g0 + g1 âˆ’l2)![Dâ€² âˆ’s + l2 âˆ’(f â€² + g0 + g1)]!(f â€² âˆ’aâ€² âˆ’l1 âˆ’g1)!(f â€² + g1 + l1 âˆ’Dâ€² + s)!lâ€²
0!(aâ€² âˆ’lâ€²
0 âˆ’1)!
aâ€²!(f â€² âˆ’aâ€²)!(Dâ€² âˆ’f â€² âˆ’1)!"
X,0.49165275459098495,"(Dâ€² âˆ’1)! =
X ÎDâˆ’1"
X,0.49248747913188645,a(a âˆ’1)
X,0.493322203672788,"D âˆ’1
1
D(f + g0 + g1)(f + g0 + g1 + 1)Paâˆ’1,fâˆ’1,Dâˆ’1(l0, l2, g0, g1)"
X,0.4941569282136895,= a(a âˆ’1)
X,0.494991652754591,"D âˆ’1 Eaâˆ’1,fâˆ’1,Dâˆ’1
h
1
D(f + g0 + g1)(f + g0 + g1 + 1) i"
X,0.4958263772954925,â‰œa(a âˆ’1)
X,0.496661101836394,"D âˆ’1
Â¯E."
X,0.4974958263772955,"Here the subscript means that we are taking expectation w.r.t the set sizes when the number of â€œOâ€,
â€œÃ—â€ and â€œâˆ’â€ points is (a âˆ’1, f âˆ’1, D âˆ’1). By symmetry, it can be shown similarly that"
X,0.498330550918197,E1 = (f âˆ’a)(f âˆ’a âˆ’1)
X,0.4991652754590985,"D âˆ’1
Ea,fâˆ’1,Dâˆ’1
h
1
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.5,"i
= (f âˆ’a)(f âˆ’a âˆ’1)"
X,0.5008347245409015,"D âˆ’1
Â¯E."
X,0.501669449081803,"Substituting above results into (26), we obtain"
X,0.5025041736227045,ËœÎ´D = 2a(f âˆ’a)[2E0 âˆ’f âˆ’2
X,0.503338898163606,"D âˆ’1
Â¯E âˆ’E3]."
X,0.5041736227045075,"To compute E0, note that with a, f and D, variable g2 is distributed as hyper(Dâˆ’1, Dâˆ’f, Dâˆ’fâˆ’1).
For Â¯E, the distribution becomes hyper(D âˆ’2, D âˆ’f, D âˆ’f âˆ’1). Since f + g0 + g1 = D âˆ’g2, we
deduce E0 ="
X,0.505008347245409,"Dâˆ’fâˆ’1
X"
X,0.5058430717863105,"s=max(0,Dâˆ’2f)"
X,0.506677796327212,"1
D(D âˆ’s)(D âˆ’s + 1)"
X,0.5075125208681135," Dâˆ’fâˆ’1
s
 
f
Dâˆ’fâˆ’s
"
X,0.508347245409015," Dâˆ’1
Dâˆ’f
 ="
X,0.5091819699499165,"Dâˆ’fâˆ’1
X"
X,0.510016694490818,"s=max(0,Dâˆ’2f)"
X,0.5108514190317195,"1
D(D âˆ’s)(D âˆ’s + 1)
(D âˆ’f âˆ’1)!f!
s!(D âˆ’f âˆ’s âˆ’1)!(D âˆ’f âˆ’s)!(âˆ’D + 2f + s)!
(D âˆ’f)!(f âˆ’1)!"
X,0.511686143572621,"(D âˆ’1)!
,"
X,0.5125208681135225,Under review as a conference paper at ICLR 2022 and Â¯E =
X,0.513355592654424,"Dâˆ’fâˆ’1
X"
X,0.5141903171953256,"s=max(0,Dâˆ’2f+1)"
X,0.5150250417362271,"1
D(D âˆ’s)(D âˆ’s + 1)"
X,0.5158597662771286," Dâˆ’fâˆ’1
s
 
fâˆ’1
Dâˆ’fâˆ’s
"
X,0.5166944908180301," Dâˆ’2
Dâˆ’f
 ="
X,0.5175292153589316,"Dâˆ’fâˆ’1
X"
X,0.5183639398998331,"s=max(0,Dâˆ’2f+1)"
X,0.5191986644407346,"1
D(D âˆ’s)(D âˆ’s + 1)
(D âˆ’f)!(f âˆ’2)!"
X,0.5200333889816361,"(D âˆ’2)!
Â·"
X,0.5208681135225376,"(D âˆ’f âˆ’1)!(f âˆ’1)!
s!(D âˆ’f âˆ’s âˆ’1)!(D âˆ’f âˆ’s)!(âˆ’D + 2f + s âˆ’1)!."
X,0.5217028380634391,"For âˆ€D â‰¥f, we have"
X,0.5225375626043406,"f âˆ’2
D âˆ’1
Â¯E â‰¤"
X,0.5233722871452421,"Dâˆ’fâˆ’1
X"
X,0.5242070116861436,"s=max(0,Dâˆ’2f)"
X,0.5250417362270451,"(f âˆ’2)(D âˆ’1)(âˆ’D + 2f + s)
D(D âˆ’1)f(f âˆ’1)(D âˆ’s)(D âˆ’s + 1)"
X,0.5258764607679466,"(D âˆ’f âˆ’1)!f!
s!(D âˆ’f âˆ’s âˆ’1)!(D âˆ’f âˆ’s)!(âˆ’D + 2f + s)!
(D âˆ’f)!(f âˆ’1)!"
X,0.5267111853088481,(D âˆ’1)!
X,0.5275459098497496,"â‰¤E
h
(f âˆ’2)(f âˆ’(D âˆ’f âˆ’g2))
Df(f âˆ’1)(D âˆ’g2)(D âˆ’g2 + 1) i"
X,0.5283806343906511,"< E
h
(f âˆ’g0 âˆ’g1)
Df(f + g0 + g1)(f + g0 + g1 + 1) i
."
X,0.5292153589315526,"Consequently, we have"
X,0.5300500834724541,"ËœÎ´D > 2a(f âˆ’a)E
h
2
D(f + g0 + g1)(f + g0 + g1 + 1) âˆ’
f âˆ’g0 âˆ’g1
Df(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.5308848080133556,"âˆ’
f + g0 + g1
Df(f + g0 + g1)(f + g0 + g1 + 1) i = 0,"
X,0.5317195325542571,and note that this holds for âˆ€D â‰¥K. The proof is complete.
X,0.5325542570951586,"A.6
PROOF OF THEOREM 3.5"
X,0.5333889816360601,"Theorem 3.5 (Uniform Superiority). For any two binary vectors v, w âˆˆ{0, 1}D with J Ì¸= 0 or 1,
it holds that V ar[ Ë†JÏƒ,Ï€(v, w)] < V ar[ Ë†JMH(v, w)]."
X,0.5342237061769616,"Proof. By assumption we have 0 < a < f. To compare V ar[ Ë†JÏƒ,Ï€] with V ar[ Ë†JMH] = J(1âˆ’J) K
="
X,0.5350584307178631,"J
K + (Kâˆ’1)J2"
X,0.5358931552587646,"K
âˆ’J2, it sufï¬ces to compare ËœE with J2. When D = f, we know that the location vector
x of (v, w) contains no â€œâˆ’â€ elements. It is easy to verify that in this case, |G0| = |G1| = |L2| = 0,
and |L0| follows hyper(f âˆ’1, a, a âˆ’1). By Theorem 3.2, it follows that when D = f,"
X,0.5367278797996661,ËœED = 1
X,0.5375626043405676,f E[|L0|] = a(a âˆ’1)
X,0.5383973288814691,f(f âˆ’1) = J ËœJ < J2.
X,0.5392320534223706,Recall the deï¬nition ËœJ = aâˆ’1
X,0.5400667779632721,"fâˆ’1, which is always less than J. On the other hand, as D â†’âˆ, we have
|L0| â†’0, |L2| â†’a, |G0| â†’a and |G1| â†’f âˆ’a. We can easily show that"
X,0.5409015025041736,"ËœED â†’J2,
as D â†’âˆ."
X,0.5417362270450752,"By Lemma 3.4, the sequence ( ËœEf, ËœEf+1, ËœEf+2, ...) is strictly increasing. Since it is convergent with
limit J2, by the Monotone Convergence Theorem we know that ËœED < J2, âˆ€D â‰¥f."
X,0.5425709515859767,"A.7
PROOF OF PROPOSITION 3.6"
X,0.5434056761268782,"Proposition 3.6 (Consistent Improvement). Suppose f is ï¬xed. In terms of a, the variance ratio
Ï(a) = V ar[ Ë†
JMH(v,w)]
V ar[ Ë†
JÏƒ,Ï€(v,w)] is constant for any 0 < a < f."
X,0.5442404006677797,Under review as a conference paper at ICLR 2022
X,0.5450751252086812,"Proof. Let ËœE be deï¬ned as in Theorem 3.2. Assume that D and f are ï¬xed and a is variable. Firstly,
we can write the variance ratio explicitly as"
X,0.5459098497495827,Ï(a) = Jâˆ’J2
X,0.5467445742904842,"K
J
K + (K+1) ËœE"
X,0.5475792988313857,"K
âˆ’J2 =
1 âˆ’J"
X,0.5484140233722872,"1 âˆ’J âˆ’(K âˆ’1)(J âˆ’
ËœE
J )
."
X,0.5492487479131887,"We now show that the term J âˆ’
ËœE
J = C(1 âˆ’J), where C is some constant independent of J (i.e.,
a). Then, for ï¬xed D and f, by cancellation Ï(a) would be constant for all 0 < a < f. We have"
X,0.5500834724540902,"J âˆ’
ËœE
J = a"
X,0.5509181969949917,"f âˆ’Ea,f,D
h
fl0
a(f + g0 + g1) +
g0 + l2
f + g0 + g1 i"
X,0.5517529215358932,"= E
ha2(f + g0 + g1) âˆ’f 2l0 âˆ’af(g0 + l2)"
X,0.5525876460767947,af(f + g0 + g1) i
X,0.5534223706176962,"= E
ha(a âˆ’f)(g0 + g1) + a2f + afg1 âˆ’f 2l0 âˆ’afl2"
X,0.5542570951585977,af(f + g0 + g1) i
X,0.5550918196994992,"= E
ha(a âˆ’f)(g0 + g1) + af(l0 + l1) + afg1 âˆ’f 2l0"
X,0.5559265442404007,af(f + g0 + g1) i
X,0.5567612687813022,"= E
ha(a âˆ’f)(g0 + g1) + f(a âˆ’f)l0 + af(f âˆ’a âˆ’h1)"
X,0.5575959933222037,af(f + g0 + g1)
X,0.5584307178631052,"i
,
(27)"
X,0.5592654424040067,"where we use the constraints (10) that l0 + l1 + l2 = a and l1 + g1 + h1 = f âˆ’a. We now study the
three terms respectively. We have"
X,0.5601001669449082,"E
ha(a âˆ’f)(g0 + g1)"
X,0.5609348914858097,af(f + g0 + g1)
X,0.5617696160267112,"i
= âˆ’(1 âˆ’J)E
h
g0 + g1
f + g0 + g1"
X,0.5626043405676127,"i
â‰œâˆ’Eâ€²(1 âˆ’J)."
X,0.5634390651085142,We have shown in the proof of Lemma 3.4 that
X,0.5642737896494157,"Ea,f,D
h
l0
f + g0 + g1"
X,0.5651085141903172,"i
= a(a âˆ’1)"
X,0.5659432387312187,"D âˆ’1 Eaâˆ’1,fâˆ’1,Dâˆ’1
h
1
f + g0 + g1"
X,0.5667779632721202,"i
â‰œa(a âˆ’1)"
X,0.5676126878130217,"D âˆ’1 Eâˆ—,"
X,0.5684474123539232,and by symmetry it holds that
X,0.5692821368948247,"Ea,f,D
h
h1
f + g0 + g1"
X,0.5701168614357263,"i
= (f âˆ’a)(f âˆ’a âˆ’1)"
X,0.5709515859766278,"D âˆ’1
Eâˆ—."
X,0.5717863105175293,"Note that Since f is ï¬xed, (|G0| + |G1|) is distributed independent of a. Consequently, Eâ€² and Eâˆ—
are both independent of a. Next, we obtain"
X,0.5726210350584308,"E
h
f(a âˆ’f)l0
af(f + g0 + g1)"
X,0.5734557595993323,"i
= âˆ’(1 âˆ’J)f(a âˆ’1)"
X,0.5742904841402338,"D âˆ’1 Eâˆ—, and"
X,0.5751252086811353,"E
h af(f âˆ’a âˆ’h1)"
X,0.5759599332220368,af(f + g0 + g1)
X,0.5767946577629383,"i
= (1 âˆ’J)fEâˆ—âˆ’(1 âˆ’J)f(f âˆ’a âˆ’1)"
X,0.5776293823038398,"D âˆ’1
Eâˆ—."
X,0.5784641068447413,"Summing up the terms and substituting into (27), we derive"
X,0.5792988313856428,"J âˆ’
ËœE
J = C(1 âˆ’J),"
X,0.5801335559265443,where C = âˆ’Eâ€² + (f âˆ’f(fâˆ’2)
X,0.5809682804674458,"Dâˆ’1 )Eâˆ—, which is independent of a. Taking into Ï(a), we get"
X,0.5818030050083473,"Ï(a) =
1 âˆ’J
1 âˆ’J âˆ’(K âˆ’1)C(1 âˆ’J) =
1
1 âˆ’(K âˆ’1)C ,"
X,0.5826377295492488,"which is a constant only depending on f, D and K. This completes the proof."
X,0.5834724540901502,Under review as a conference paper at ICLR 2022
X,0.5843071786310517,"A.8
PROOF OF THEOREM 5.1"
X,0.5851419031719532,"Proof. Denote E[1k] := E[1{hk(v) = hk(w)}] for any 1 â‰¤k â‰¤K. We ï¬rst recall some notations.
We have v, w âˆˆ{0, 1}D, and a and f are deï¬ned in (5). Denote B1 = {i : xi = O}, B2 = {i :
xi = Ã—} and B3 = {i : xi = âˆ’} as the sets of three types of points, respectively. For a â‰¤j â‰¤D
and 1 â‰¤k â‰¤K, deï¬ne"
X,0.5859766277128547,"Aâˆ’(j) = {xi : (i + k âˆ’1 mod D) + 1 â‰¤j},
A+(j) = {xi : (i + k âˆ’1 mod D) + 1 > j}."
X,0.5868113522537562,"Let nâˆ’,1(j) = |{xi = O : i âˆˆAâˆ’(j)}| be the number of â€œOâ€ points in Aâˆ’(j). Analogously let
nâˆ’,2(j), nâˆ’,3(j) be the number of â€œÃ—â€ and â€œâˆ’â€ points in Aâˆ’(j), and n+,1(j), n+,2(j), n+,3(j) be
the number of â€œOâ€, â€œÃ—â€ and â€œâˆ’â€ points in A+(j). For any i, denote iâˆ—= (i + k âˆ’1 mod D) + 1,
i# = (i âˆ’k âˆ’1 mod D) + 1."
X,0.5876460767946577,"Our analysis starts with the decomposition of hash collision probability,"
X,0.5884808013355592,"E[1k] = P
h
hk(v) = hk(w)
i
= D
X"
X,0.5893155258764607,"j=1
P
h
hk(v) = hk(w) = j
i
,
(28)"
X,0.5901502504173622,"where recall h(Â·) is the hash sample. Consider the process for generating the hash. As before, we
look at the location vector x. In Method 2, we ï¬rst permute x by Ï€ to get Ï€(x). Then the k-th
hash samples collide if the minimum of Ï€â†’k(Ï€(x)) is â€œOâ€. One key observation is that, when
applying Ï€â†’k, the random index for the i-th element in Ï€(x) is exactly the one used for xi# (shifted
backwards) in the initial permutation. A toy example in provided in Figure 8 to help understand the
reasoning."
X,0.5909849749582637,"Figure 8: Illustration of C-MinHash-(Ï€, Ï€) hash collision, with k = 2. Here, â€œcirculant rightâ€ means
â€œcirculant downâ€. Small indices correspond to upper elements."
X,0.5918196994991652,"Further denote set M = {Ï€(i) : Ï€(i) /âˆˆB3} be the collection of indices of initially permuted vector
Ï€(x) that are not â€œâˆ’â€ points, and Mâ†k = (M âˆ’k âˆ’1 mod D) + 1 be the corresponding indices
shifted backwards. In Figure 8, M = {2, 5, 8, 11}. Also, Ï€(6) = 4, and the permutation maps are
described by the red arrows. Consequently, in Ï€â†’k(Ï€(x)), the 8-th element (â€œOâ€) in Ï€(x) will be
permuted to the same index of the 8 âˆ’2 = 6-th element in x, which equals to 4. It is important to
notice that, when considering the k-th collision, only points in Mâ†k matters. Hence, we deduct:"
X,0.5926544240400667,"â€¢ (Collision condition) Denote i = arg mintâˆˆMâ†k Ï€(xt) be the location of minimal permu-
tation index in Mâ†k. The k-th collision occurs at j i.f.f. Ï€(i) = j, and the iâˆ—-th element
in Ï€(x) must be a â€œOâ€ point. Recall the deï¬nition iâˆ—= (i + k âˆ’1 mod D) + 1."
X,0.5934891485809682,Under review as a conference paper at ICLR 2022
X,0.5943238731218697,"In Figure 8, consider i = 6 and j = 4 for example. Above condition means that the i = 6-th element
in x (â€œâˆ’â€ in red bold border) is permuted to the j = 4-th position, and it is above all other permuted
elements with red bold borders. Meanwhile, the i + k = 6 + 2 = 8-th element in Ï€(x) must be a
â€œOâ€. Figure 8 exactly satisï¬es the condition, so it depicts a collision. Mathematically, we have"
X,0.5951585976627712,"E[1k] = D
X"
X,0.5959933222036727,"j=1
P
h
hk(v) = hk(w) = j
i = D
X j=1 D
X"
X,0.5968280467445742,"i=1
P
h
Ï€(i) = j, Ï€âˆ’1(iâˆ—) âˆˆB1
i
,
(29)"
X,0.5976627712854758,"where i = arg mintâˆˆMâ†k Ï€(xt). In this expression, everything is random of Ï€, except for the set
B1 which is ï¬xed given the data."
X,0.5984974958263773,"Now we will focus on deriving the probability for a ï¬xed i and j in (29). Our analysis will be
conditional on the collection of variables Z which is deï¬ned as follows. Let zâˆ’,1, zâˆ’,2 and zâˆ’,3 be
the number of â€œOâ€, â€œÃ—â€ and â€œâˆ’â€ points in Aâˆ’(j) âˆ©Mc
â†k, and z+,1, z+,2 and z+,3 be the number
of â€œOâ€, â€œÃ—â€ and â€œâˆ’â€ points in A+(j)âˆ©Mc
â†k, respectively. Here Mc
â†k represents the complement
of Mâ†k. Notice that Z (and its density function) depends on different j since Aâˆ’(j) and A+(j)
depends on j. For the ease of notation we suppress the information of j in Z (and zâ€™s). It is easy to
see that Z = (zâˆ’,k|3
1, z+,k|3
1) follows hyper(D, D âˆ’f, nâˆ’,k(j)|3
1, n+,k(j)|3
1). Denote the domain of
Z and Î˜j. Conditional on Z, we obtain"
X,0.5993322203672788,"E[1k] = D
X j=1 X ZâˆˆÎ˜j D
X"
X,0.6001669449081803,"i=1
P
h
Ï€(i) = j, Ï€âˆ’1(iâˆ—) âˆˆB1|Z
i
Pj
h
Z
i = D
X j=1 X ZâˆˆÎ˜j D
X"
X,0.6010016694490818,"i=1
P
h
Ï€âˆ’1(iâˆ—) âˆˆB1|Ï€(i) = j, Z
i
P
h
Ï€(i) = j|Z
i
Pj
h
Z
i â‰œ D
X j=1 X ZâˆˆÎ˜j D
X"
X,0.6018363939899833,"i=1
Î“(i, j)Pj
h
Z
i
,
(30)"
X,0.6026711185308848,"with i = arg mintâˆˆMâ†k Ï€(xt). We will carefully compute the probabilities in the summation.
Basically, the key is that elements in M need to be controlled, i.e. smaller than j, and other positions
can be arbitrary. Given Z, this means that we need to put r1 = a âˆ’zâˆ’,1 âˆ’z+,1 type â€œOâ€ points,
r2 = f âˆ’aâˆ’zâˆ’,2 âˆ’z+,2 type â€œÃ—â€ points and r3 = D âˆ’f âˆ’zâˆ’,3 âˆ’z+,3 type â€œâˆ’â€ points no smaller
than j, with Ï€(i) = j exactly. Also note that there are ï¬xed b0 = P3
k=1 z+,k type â€œâˆ’â€ points no
smaller than j."
X,0.6035058430717863,"With all these deï¬nitions and reasoning, we are ready to proceed with the proof. Based on xi, we
have three general cases."
X,0.6043405676126878,1) xi âˆˆB1. The ï¬rst case is that xi = O.
X,0.6051752921535893,"Case 1a) j < iâˆ—. Firstly, we consider the case where j < iâˆ—. By combinatorial theory we have"
X,0.6060100166944908,"P
h
Ï€(i) = j|Z
i
= P
h
Ï€(i) = j|Ï€âˆ’1(j) /âˆˆB3, Z
i
P
h
Ï€âˆ’1(j) /âˆˆB3|Z
i"
X,0.6068447412353923,"=
1
r1 + r2"
X,0.6076794657762938," b0
r3
 Dâˆ’jâˆ’b0
r1+r2âˆ’1
"
X,0.6085141903171953," Dâˆ’f
r3
 
f
r1+r2
P
h
Ï€âˆ’1(j) /âˆˆB3|Z
i"
X,0.6093489148580968,"â‰œËœP1 Â· P
h
Ï€âˆ’1(j) /âˆˆB3|Z
i
,
(31)"
X,0.6101836393989983,"where the second probability is that the j-th element in Ï€(x) is not â€œâˆ’â€. Conditional on Z, the
probability is dependent on j#:"
X,0.6110183639398998,"P
h
Ï€âˆ’1(j) /âˆˆB3|Z
i
="
X,0.6118530884808013,"3
X"
X,0.6126878130217028,"p=1
1{j# âˆˆBp}(1 âˆ’
zâˆ’,p
nâˆ’,p(j))."
X,0.6135225375626043,Under review as a conference paper at ICLR 2022
X,0.6143572621035058,Combining with (31) we obtain
X,0.6151919866444073,"P
h
Ï€(i) = j|Z
i
= ËœP1"
X,0.6160267111853088,"3
X"
X,0.6168614357262103,"p=1
1{j# âˆˆBp}(1 âˆ’
zâˆ’,p
nâˆ’,p(j)).
(32)"
X,0.6176961602671118,"Next we compute P[Ï€âˆ’1(iâˆ—) âˆˆB1|Ï€(i) = j, Z]. Note that given the conditions, Ï€âˆ’1(iâˆ—) has two
cases: 1) it comes from Mâ†k (i.e. it is one of the elements with red bold border); 2) Otherwise. We
then can write"
X,0.6185308848080133,"P
h
Ï€âˆ’1(iâˆ—) âˆˆB1|Ï€(i) = j, Z
i"
X,0.6193656093489148,"= P
h
Ï€âˆ’1(iâˆ—) âˆˆB1|Ï€âˆ’1(iâˆ—) âˆˆMâ†k, Ï€(i) = j, Z
i
P
h
Ï€âˆ’1(iâˆ—) âˆˆMâ†k|Ï€(i) = j, Z
i"
X,0.6202003338898163,"+ P
h
Ï€âˆ’1(iâˆ—) âˆˆB1|Ï€âˆ’1(iâˆ—) /âˆˆMâ†k, Ï€(i) = j, Z
i
P
h
Ï€âˆ’1(iâˆ—) /âˆˆMâ†k|Ï€(i) = j, Z
i"
X,0.6210350584307178,"= (1 âˆ’
z+,1
n+,1(j))
r1 + r2 âˆ’1"
X,0.6218697829716193,D âˆ’j âˆ’b0
X,0.6227045075125208,"r1 âˆ’1
r1 + r2 âˆ’1 + (1 âˆ’r1 + r2 âˆ’1"
X,0.6235392320534223,"D âˆ’j âˆ’b0
)
a âˆ’r1
f âˆ’r1 âˆ’r2 "
X,0.6243739565943238,"â‰œ(1 âˆ’
z+,1
n+,1(j))

r1 âˆ’1
D âˆ’j âˆ’b0
+ (1 âˆ’r1 + r2 âˆ’1"
X,0.6252086811352254,"D âˆ’j âˆ’b0
)Jâˆ—
"
X,0.6260434056761269,"â‰œ(1 âˆ’
z+,1
n+,1(j)) Â¯J1.
(33)"
X,0.6268781302170284,"Combining (32) and (33), we obtain when i âˆˆB1 and j < iâˆ—,"
X,0.6277128547579299,"Î“(i, j) ="
X,0.6285475792988314,"3
X"
X,0.6293823038397329,"p=1
1{j# âˆˆBp}(1 âˆ’
zâˆ’,p
nâˆ’,p(j))(1 âˆ’
z+,1
n+,1(j)) ËœP1 Â¯J1.
(34)"
X,0.6302170283806344,"Case 1b) j = iâˆ—. Similarly approach also applies to the situation with j = iâˆ—. In this case,"
X,0.6310517529215359,"P
h
Ï€âˆ’1(j) /âˆˆB3|Z
i
= (1 âˆ’
zâˆ’,1
nâˆ’,1(j)) ËœP1,
P[Ï€âˆ’1(iâˆ—) âˆˆB1|Ï€(i) = j, Z] = 1.
(35)"
X,0.6318864774624374,"The equations are because (iâˆ—)# = i âˆˆB1, and equivalently, Ï€âˆ’1(iâˆ—) = Ï€âˆ’1(j) = i âˆˆB1."
X,0.6327212020033389,"Case 1c) j > iâˆ—. I this case, we still have"
X,0.6335559265442404,"P
h
Ï€(i) = j|Z
i
= ËœP1"
X,0.6343906510851419,"3
X"
X,0.6352253756260434,"p=1
1{j# âˆˆBp}(1 âˆ’
zâˆ’,p
nâˆ’,p(j)),"
X,0.6360601001669449,"but the probability of Ï€âˆ’1(iâˆ—) being â€œOâ€ is different. Since j > iâˆ—, this event now depends on zâˆ’,p,
p = 1, 2, 3. More speciï¬cally,"
X,0.6368948247078464,"P[Ï€âˆ’1(iâˆ—) âˆˆB1|Ï€(i) = j, Z] = """
X,0.6377295492487479,"1{j# âˆˆB1}(1 âˆ’
zâˆ’,1
nâˆ’,1(j) âˆ’1) +
X"
X,0.6385642737896494,"p=2,3
1{j# âˆˆBp}(1 âˆ’
zâˆ’,p
nâˆ’,p(j)) # Jâˆ—."
X,0.6393989983305509,"Therefore, when i âˆˆB1 and j > jâˆ—, it holds that"
X,0.6402337228714524,"Î“(i, j) = (1 âˆ’
zâˆ’,1
nâˆ’,1(j)) """
X,0.6410684474123539,"1{j# âˆˆB1}(1 âˆ’
zâˆ’,1
nâˆ’,1(j) âˆ’1) +
X"
X,0.6419031719532554,"p=2,3
1{j# âˆˆBp}(1 âˆ’
zâˆ’,p
nâˆ’,p(j)) # Jâˆ—. (36)"
X,0.6427378964941569,"2) xi âˆˆB2. The case where xi âˆˆB2 can be analyzed using similar arguments. For conciseness, we
mainly present the ï¬nal results."
X,0.6435726210350584,Case 2a) j < iâˆ—. The calculation ends up in the same form. We have
X,0.6444073455759599,"P
h
Ï€(i) = j|Z
i
= ËœP2"
X,0.6452420701168614,"3
X"
X,0.6460767946577629,"p=1
1{j# âˆˆBp}(1 âˆ’
zâˆ’,p
nâˆ’,p(j)),"
X,0.6469115191986644,Under review as a conference paper at ICLR 2022
X,0.6477462437395659,"with ËœP2 = ËœP1. In addition,"
X,0.6485809682804674,"P
h
Ï€âˆ’1(iâˆ—) âˆˆB1|Ï€(i) = j, Z
i
= (1 âˆ’
z+,2
n+,2(j)) Â¯J2,"
X,0.6494156928213689,"where Â¯J2 =
r1
Dâˆ’jâˆ’b0 + (1 âˆ’r1+r2âˆ’1"
X,0.6502504173622704,"Dâˆ’jâˆ’b0 )Jâˆ—. Hence, when xi âˆˆB2 and j < iâˆ—, we have"
X,0.6510851419031719,"Î“(i, j) ="
X,0.6519198664440734,"3
X"
X,0.6527545909849749,"p=1
1{j# âˆˆBp}(1 âˆ’
zâˆ’,p
nâˆ’,p(j))(1 âˆ’
z+,2
n+,2(j)) ËœP2 Â¯J2.
(37)"
X,0.6535893155258765,"Case 2b) j = iâˆ—. In this case, Î“(i, j) simply equals to 0, since Ï€âˆ’1(iâˆ—) = Ï€âˆ’1(j) âˆˆB2. The
probability of Ï€âˆ’1(iâˆ—) being â€œOâ€ is 0."
X,0.654424040066778,"Case 2c) j > iâˆ—. Omitting the details, we have"
X,0.6552587646076795,"Î“(i, j) = (1 âˆ’
zâˆ’,2
nâˆ’,2(j)) """
X,0.656093489148581,"1{j# âˆˆB2}(1 âˆ’
zâˆ’,2
nâˆ’,2(j) âˆ’1) +
X"
X,0.6569282136894825,"p=1,3
1{j# âˆˆBp}(1 âˆ’
zâˆ’,p
nâˆ’,p(j)) # Jâˆ—. (38)"
X,0.657762938230384,2) xi âˆˆB3.
X,0.6585976627712855,"Case 3a) j < iâˆ—. The expression is different from previous two, in that we need Ï€âˆ’1(j) âˆˆB3."
X,0.659432387312187,"P
h
Ï€(i) = j|Z
i
= P
h
Ï€(i) = j|Ï€âˆ’1(j) âˆˆB3, Z
i
P
h
Ï€âˆ’1(j) âˆˆB3|Z
i = ËœP3"
X,0.6602671118530885,"3
X"
X,0.66110183639399,"p=1
1{j# âˆˆBp}
zâˆ’,p
nâˆ’,p(j), where"
X,0.6619365609348915,ËœP3 = 1 r3
X,0.662771285475793,"  b0
r3âˆ’1
 Dâˆ’jâˆ’b0
r1+r2
"
X,0.6636060100166945," Dâˆ’f
r3
 
f
r1+r2
 ."
X,0.664440734557596,"Moreover, we have"
X,0.6652754590984975,"P
h
Ï€âˆ’1(iâˆ—) âˆˆB1|Ï€(i) = j, Z
i
= (1 âˆ’
z+,3
n+,3(j)) Â¯J3,"
X,0.666110183639399,"with Â¯J3 =
r1
Dâˆ’jâˆ’b0 + (1 âˆ’
r1+r2
Dâˆ’jâˆ’b0 )Jâˆ—. Combining parts together we obtain"
X,0.6669449081803005,"Î“(i, j) ="
X,0.667779632721202,"3
X"
X,0.6686143572621035,"p=1
1{j# âˆˆBp}
zâˆ’,p
nâˆ’,p(j)(1 âˆ’
z+,3
n+,3(j)) ËœP3 Â¯J3.
(39)"
X,0.669449081803005,"Case 3b) j = iâˆ—. By same reasoning as Case 2a), Î“(i, j) = 0."
X,0.6702838063439065,Case 3c) j > iâˆ—. We have in this case
X,0.671118530884808,"Î“(i, j) = """
X,0.6719532554257095,"1{j# âˆˆB3}
zâˆ’,3
nâˆ’,3(j)
nâˆ’,3(j) âˆ’zâˆ’,3"
X,0.672787979966611,"nâˆ’,3(j) âˆ’1
+ (1 âˆ’
zâˆ’,3
nâˆ’,3(j))
X"
X,0.6736227045075125,"p=1,2
1{j# âˆˆBp}
zâˆ’,p
nâˆ’,p(j) # Jâˆ— ="
X,0.674457429048414,"3
X"
X,0.6752921535893155,"p=1
1{j# âˆˆBp}
zâˆ’,p
nâˆ’,p(j)(1 âˆ’zâˆ’,3 âˆ’1{p = 3}"
X,0.676126878130217,"nâˆ’,3(j)
âˆ’1{p = 3})Jâˆ—.
(40)"
X,0.6769616026711185,"Finally, combining (30), (34), (35), (36), (37), (38), (39) and (40) and re-organizing terms, the proof
is complete."
X,0.67779632721202,Under review as a conference paper at ICLR 2022
X,0.6786310517529215,"B
MORE NUMERICAL JUSTIFICATION ON C-MINHASH-(Ï€, Ï€)"
X,0.679465776293823,"The â€œWordsâ€ dataset (Li & Church, 2005) (which is publicly available) contains a large number of
word vectors, with the i-th entry indicating whether this word appears in the i-th document, for a
total of D = 216 documents. The key statistics of the 120 selected word pairs are presented in
Table 1. Those 120 pairs of words are more or less randomly selected except that we make sure
they cover a wide spectrum of data distributions. Denote d as the number of non-zero entries in the
vector. Table 1 reports the density Ëœd = d/D for each word vector, ranging from 0.0006 to 0.6. The
Jaccard similarity J ranges from 0.002 to 0.95."
X,0.6803005008347245,"In Figures 9 - 16, we plot the empirical MSE along with the empirical bias2 for Ë†JÏ€,Ï€, as well as
the empirical MSE for Ë†JÏƒ,Ï€. Note that for D this large, it is numerically difï¬cult to evaluate the
theoretical variance formulas. From the results in the Figures, we can observe"
X,0.6811352253756261,"â€¢ For all the data pairs, the MSE of C-MinHash-(Ï€, Ï€) estimator overlaps with the empirical
MSE of C-MinHash-(Ïƒ, Ï€) estimator for all K from 1 up to 4096."
X,0.6819699499165276,"â€¢ The bias2 is several orders of magnitudes smaller than the MSE, in all data pairs. This
veriï¬es that the bias of Ë†JÏ€,Ï€ is extremely small in practice and can be safely neglected."
X,0.6828046744574291,"We have many more plots on more data pairs. Nevertheless, we believe the current set of experiments
on this â€œWordsâ€ dataset should be sufï¬cient to verify that, the proposed C-MinHash-(Ï€, Ï€) could
give indistinguishable Jaccard estimation accuracy in practice compared with C-MinHash-(Ïƒ, Ï€)."
X,0.6836393989983306,Under review as a conference paper at ICLR 2022
X,0.6844741235392321,"Table 1: 120 selected word pairs from the Words dataset (Li & Church, 2005). For each pair, we
report the density Ëœd (number of non-zero entries divided by D = 216) for each word as well as the
Jaccard similarity J. Both Ëœd and J cover a wide range of values."
X,0.6853088480801336,"Ëœ
d1
Ëœ
d2
J
Ëœ
d1
Ëœ
d2
J
ABOUT - INTO
0.302
0.125
0.258
NEW - WEB
0.291
0.194
0.224
ABOUT - LIKE
0.302
0.140
0.281
NEWS - LIKE
0.168
0.140
0.172
ACTUAL - DEVELOPED
0.017
0.030
0.071
NO - WELL
0.220
0.120
0.244
ACTUAL - GRABBED
0.017
0.002
0.016
NOT - IT
0.281
0.295
0.437
AFTER - OR
0.103
0.356
0.220
NOTORIOUSLY - LOCK
0.0006
0.006
0.004
AND - PROBLEM
0.554
0.044
0.070
OF - THEN
0.570
0.104
0.168
AS - NAME
0.280
0.144
0.204
OF - WE
0.570
0.226
0.361
AT - CUT
0.374
0.242
0.052
OPPORTUNITY - COUNTRIES
0.029
0.024
0.066
BE - ONE
0.323
0.221
0.403
OUR - THAN
0.244
0.125
0.245
BEST - AND
0.136
0.554
0.228
OVER - BACK
0.148
0.160
0.233
BRAZIL - OH
0.010
0.031
0.019
OVER - TWO
0.148
0.121
0.289
BUT - MANY
0.167
0.116
0.340
PEAK - SHOWS
0.006
0.033
0.026
CALLED - BUSINESSES
0.016
0.018
0.043
PEOPLE - BY
0.121
0.425
0.228
CALORIES - MICROSOFT
0.002
0.045
0.0003
PEOPLE - INFO
0.121
0.138
0.117
CAN - FROM
0.243
0.326
0.444
PICKS - BOOST
0.007
0.005
0.007
CAN - SEARCH
0.243
0.214
0.237
PLANET - REWARD
0.013
0.003
0.018
COMMITTED - PRODUCTIVE
0.013
0.004
0.029
PLEASE - MAKE
0.168
0.141
0.195
CONTEMPORARY - FLASH
0.011
0.021
0.013
PREFER - PUEDE
0.010
0.003
0.0001
CONVENIENTLY - INDUSTRIES
0.003
0.011
0.009
PRIVACY - FOUND
0.126
0.136
0.053
COPYRIGHT - AN
0.218
0.290
0.209
PROSECUTION - MAXIMIZE
0.002
0.003
0.006
CREDIT - CARD
0.046
0.041
0.285
RECENTLY - INT
0.028
0.007
0.014
DE - WEB
0.117
0.194
0.091
REPLY - ACHIEVE
0.013
0.012
0.023
DO - GOOD
0.174
0.102
0.276
RESERVED - BEEN
0.172
0.141
0.108
EARTH - GROUPS
0.021
0.035
0.056
RIGHTS - FIND
0.187
0.144
0.166
EXPRESSED - FRUSTRATED
0.010
0.002
0.024
RIGHTS - RESERVED
0.187
0.172
0.877
FIND - HAS
0.144
0.228
0.214
SCENE - ABOUT
0.012
0.301
0.029
FIND - SITE
0.144
0.275
0.212
SEE - ALSO
0.138
0.166
0.291
FIXED - SPECIFIC
0.011
0.039
0.054
SEIZE - ANYTHING
0.0007
0.037
0.012
FLIGHT - TRANSPORTATION
0.011
0.018
0.040
SHOULDERS - GORGEOUS
0.003
0.004
0.028
FOUND - DE
0.136
0.117
0.039
SICK - FELL
0.008
0.008
0.085
FRANCISCO - SAN
0.025
0.049
0.476
SITE - CELLULAR
0.275
0.006
0.010
GOOD - BACK
0.102
0.160
0.220
SOLD - LIVE
0.018
0.064
0.055
GROUPS - ORDERED
0.035
0.011
0.034
SOLO - CLAIMS
0.010
0.012
0.007
HAPPY - CONCEPT
0.029
0.013
0.054
SOON - ADVANCE
0.040
0.017
0.057
HAVE - FIRST
0.267
0.151
0.320
SPECIALIZES - ACTUAL
0.003
0.017
0.008
HAVE - US
0.267
0.284
0.349
STATE - OF
0.101
0.570
0.165
HILL - ASSURED
0.020
0.004
0.011
STATES - UNITED
0.061
0.062
0.591
HOME - SYNTHESIS
0.365
0.002
0.003
TATTOO - JEANS
0.002
0.004
0.035
HONG - KONG
0.014
0.014
0.925
THAT - ALSO
0.301
0.166
0.376
HOSTED - DRUGS
0.016
0.013
0.013
THIS - CITY
0.423
0.123
0.132
INTERVIEWS - FOURTH
0.012
0.011
0.031
THEIR - SUPPORT
0.165
0.117
0.189
KANSAS - PROPERTY
0.017
0.045
0.052
THEIR - VIEW
0.165
0.103
0.151
KIRIBATI - GAMBIA
0.003
0.003
0.712
THEM - OF
0.112
0.570
0.187
LAST - THIS
0.135
0.423
0.221
THEN - NEW
0.104
0.291
0.192
LEAST - ROMANCE
0.046
0.007
0.019
THINKS - LOT
0.007
0.040
0.079
LIME - REGISTERED
0.002
0.030
0.004
TIME - OUT
0.189
0.191
0.366
LINKS - TAKE
0.191
0.105
0.134
TIME - WELL
0.189
0.120
0.299
LINKS - THAN
0.191
0.125
0.141
TOP - AS
0.140
0.280
0.217
MAIL - AND
0.160
0.554
0.192
TOP - COPYRIGHT
0.140
0.218
0.149
MAIL - BACK
0.160
0.160
0.132
TOP - NEWS
0.140
0.168
0.192
MAKE - LIKE
0.141
0.140
0.297
UP - AND
0.200
0.554
0.334
MANAGING - LOCK
0.010
0.006
0.010
UP - HAS
0.200
0.228
0.312
MANY - US
0.116
0.284
0.210
US - BE
0.284
0.323
0.335
MASS - DREAM
0.016
0.017
0.048
VIEW - IN
0.103
0.540
0.153
MAY - HELP
0.184
0.156
0.206
VIEW - PEOPLE
0.103
0.121
0.138
MOST - HOME
0.141
0.365
0.207
WALKED - ANTIVIRUS
0.006
0.002
0.002
NAME - IN
0.144
0.540
0.207
WEB - GO
0.194
0.111
0.138
NEITHER - FIGURE
0.011
0.016
0.085
WELL - INFO
0.120
0.138
0.110
NET - SO
0.101
0.154
0.112
WELL - NEWS
0.120
0.168
0.161
NEW - PLEASE
0.291
0.168
0.205
WEEKS - LONDON
0.028
0.032
0.050"
X,0.6861435726210351,Under review as a conference paper at ICLR 2022
X,0.6869782971619366,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
X,0.6878130217028381,WELL - INFO
PERM,0.6886477462437396,"2 Perm
1 Perm Bias2"
PERM,0.6894824707846411,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.6903171953255426,ABOUT - INTO
PERM,0.6911519198664441,"2 Perm
1 Perm Bias2"
PERM,0.6919866444073456,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.6928213689482471,ABOUT - LIKE
PERM,0.6936560934891486,"2 Perm
1 Perm Bias2"
PERM,0.6944908180300501,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.6953255425709516,WELL - NEWS
PERM,0.6961602671118531,"2 Perm
1 Perm Bias2"
PERM,0.6969949916527546,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.6978297161936561,ACTUAL - DEVELOPED
PERM,0.6986644407345576,"2 Perm
1 Perm Bias2"
PERM,0.6994991652754591,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7003338898163606,ACTUAL - GRABBED
PERM,0.7011686143572621,"2 Perm
1 Perm Bias2"
PERM,0.7020033388981636,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7028380634390651,AFTER - OR
PERM,0.7036727879799666,"2 Perm
1 Perm Bias2"
PERM,0.7045075125208681,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7053422370617696,AND - PROBLEM
PERM,0.7061769616026711,"2 Perm
1 Perm Bias2"
PERM,0.7070116861435726,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7078464106844741,AS - NAME
PERM,0.7086811352253757,"2 Perm
1 Perm Bias2"
PERM,0.7095158597662772,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7103505843071787,AT - CUT
PERM,0.7111853088480802,"2 Perm
1 Perm Bias2"
PERM,0.7120200333889817,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7128547579298832,BE - ONE
PERM,0.7136894824707847,"2 Perm
1 Perm Bias2"
PERM,0.7145242070116862,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7153589315525877,BEST - AND
PERM,0.7161936560934892,"2 Perm
1 Perm Bias2"
PERM,0.7170283806343907,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7178631051752922,BRAZIL - OH
PERM,0.7186978297161937,"2 Perm
1 Perm Bias2"
PERM,0.7195325542570952,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7203672787979967,BUT - MANY
PERM,0.7212020033388982,"2 Perm
1 Perm Bias2"
PERM,0.7220367278797997,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7228714524207012,CALLED - BUSINESSES
PERM,0.7237061769616027,"2 Perm
1 Perm Bias2"
PERM,0.7245409015025042,"Figure 9: Empirical MSEs of C-MinHash-(Ï€, Ï€) (â€œ1 Permâ€, red, solid) vs. C-MinHash-(Ïƒ, Ï€) (â€œ2
Permâ€, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(Ï€, Ï€) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.7253756260434057,Under review as a conference paper at ICLR 2022
PERM,0.7262103505843072,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7270450751252087,CALORIES - MICROSOFT
PERM,0.7278797996661102,"2 Perm
1 Perm Bias2"
PERM,0.7287145242070117,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7295492487479132,CAN - FROM
PERM,0.7303839732888147,"2 Perm
1 Perm Bias2"
PERM,0.7312186978297162,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7320534223706177,CAN - SEARCH
PERM,0.7328881469115192,"2 Perm
1 Perm Bias2"
PERM,0.7337228714524207,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7345575959933222,COMMITTED - PRODUCTIVE
PERM,0.7353923205342237,"2 Perm
1 Perm Bias2"
PERM,0.7362270450751253,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7370617696160268,CONTEMPORARY - FLASH
PERM,0.7378964941569283,"2 Perm
1 Perm Bias2"
PERM,0.7387312186978298,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7395659432387313,CONVENIENTLY - INDUSTRIES
PERM,0.7404006677796328,"2 Perm
1 Perm Bias2"
PERM,0.7412353923205343,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7420701168614358,COPYRIGHT - AN
PERM,0.7429048414023373,"2 Perm
1 Perm Bias2"
PERM,0.7437395659432388,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7445742904841403,CREDIT - CARD
PERM,0.7454090150250418,"2 Perm
1 Perm Bias2"
PERM,0.7462437395659433,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7470784641068448,DE - WEB
PERM,0.7479131886477463,"2 Perm
1 Perm Bias2"
PERM,0.7487479131886478,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7495826377295493,DO - GOOD
PERM,0.7504173622704507,"2 Perm
1 Perm Bias2"
PERM,0.7512520868113522,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7520868113522537,EARTH - GROUPS
PERM,0.7529215358931552,"2 Perm
1 Perm Bias2"
PERM,0.7537562604340567,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7545909849749582,EXPRESSED - FRUSTRATED
PERM,0.7554257095158597,"2 Perm
1 Perm Bias2"
PERM,0.7562604340567612,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7570951585976627,FIND - HAS
PERM,0.7579298831385642,"2 Perm
1 Perm Bias2"
PERM,0.7587646076794657,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7595993322203672,FIND - SITE
PERM,0.7604340567612687,"2 Perm
1 Perm Bias2"
PERM,0.7612687813021702,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7621035058430717,FIXED - SPECIFIC
PERM,0.7629382303839732,"2 Perm
1 Perm Bias2"
PERM,0.7637729549248747,"Figure 10: Empirical MSEs of C-MinHash-(Ï€, Ï€) (â€œ1 Permâ€, red, solid) vs. C-MinHash-(Ïƒ, Ï€) (â€œ2
Permâ€, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(Ï€, Ï€) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.7646076794657763,Under review as a conference paper at ICLR 2022
PERM,0.7654424040066778,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7662771285475793,FLIGHT - TRANSPORTATION
PERM,0.7671118530884808,"2 Perm
1 Perm Bias2"
PERM,0.7679465776293823,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7687813021702838,FOUND - DE
PERM,0.7696160267111853,"2 Perm
1 Perm Bias2"
PERM,0.7704507512520868,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7712854757929883,FRANCISCO - SAN
PERM,0.7721202003338898,"2 Perm
1 Perm Bias2"
PERM,0.7729549248747913,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7737896494156928,GOOD - BACK
PERM,0.7746243739565943,"2 Perm
1 Perm Bias2"
PERM,0.7754590984974958,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7762938230383973,GROUPS - ORDERED
PERM,0.7771285475792988,"2 Perm
1 Perm Bias2"
PERM,0.7779632721202003,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7787979966611018,HAPPY - CONCEPT
PERM,0.7796327212020033,"2 Perm
1 Perm Bias2"
PERM,0.7804674457429048,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7813021702838063,HAVE - FIRST
PERM,0.7821368948247078,"2 Perm
1 Perm Bias2"
PERM,0.7829716193656093,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7838063439065108,HAVE - US
PERM,0.7846410684474123,"2 Perm
1 Perm Bias2"
PERM,0.7854757929883138,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7863105175292153,HILL - ASSURED
PERM,0.7871452420701168,"2 Perm
1 Perm Bias2"
PERM,0.7879799666110183,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7888146911519198,HOME - SYNTHESIS
PERM,0.7896494156928213,"2 Perm
1 Perm Bias2"
PERM,0.7904841402337228,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7913188647746243,HONG - KONG
PERM,0.7921535893155259,"2 Perm
1 Perm Bias2"
PERM,0.7929883138564274,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7938230383973289,HOSTED - DRUGS
PERM,0.7946577629382304,"2 Perm
1 Perm Bias2"
PERM,0.7954924874791319,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7963272120200334,INTERVIEWS - FOURTH
PERM,0.7971619365609349,"2 Perm
1 Perm Bias2"
PERM,0.7979966611018364,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7988313856427379,KANSAS - PROPERTY
PERM,0.7996661101836394,"2 Perm
1 Perm Bias2"
PERM,0.8005008347245409,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8013355592654424,KIRIBATI - GAMBIA
PERM,0.8021702838063439,"2 Perm
1 Perm Bias2"
PERM,0.8030050083472454,"Figure 11: Empirical MSEs of C-MinHash-(Ï€, Ï€) (â€œ1 Permâ€, red, solid) vs. C-MinHash-(Ïƒ, Ï€) (â€œ2
Permâ€, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(Ï€, Ï€) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.8038397328881469,Under review as a conference paper at ICLR 2022
PERM,0.8046744574290484,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8055091819699499,LAST - THIS
PERM,0.8063439065108514,"2 Perm
1 Perm Bias2"
PERM,0.8071786310517529,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8080133555926544,LEAST - ROMANCE
PERM,0.8088480801335559,"2 Perm
1 Perm Bias2"
PERM,0.8096828046744574,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8105175292153589,LIME - REGISTERED
PERM,0.8113522537562604,"2 Perm
1 Perm Bias2"
PERM,0.8121869782971619,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8130217028380634,LINKS - TAKE
PERM,0.8138564273789649,"2 Perm
1 Perm Bias2"
PERM,0.8146911519198664,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8155258764607679,LINKS - THAN
PERM,0.8163606010016694,"2 Perm
1 Perm Bias2"
PERM,0.8171953255425709,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8180300500834724,MAIL - AND
PERM,0.8188647746243739,"2 Perm
1 Perm Bias2"
PERM,0.8196994991652755,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.820534223706177,MAIL - BACK
PERM,0.8213689482470785,"2 Perm
1 Perm Bias2"
PERM,0.82220367278798,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8230383973288815,MAKE - LIKE
PERM,0.823873121869783,"2 Perm
1 Perm Bias2"
PERM,0.8247078464106845,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.825542570951586,MANAGING - LOCK
PERM,0.8263772954924875,"2 Perm
1 Perm Bias2"
PERM,0.827212020033389,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8280467445742905,MANY - US
PERM,0.828881469115192,"2 Perm
1 Perm Bias2"
PERM,0.8297161936560935,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.830550918196995,MASS - DREAM
PERM,0.8313856427378965,"2 Perm
1 Perm Bias2"
PERM,0.832220367278798,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8330550918196995,MAY - HELP
PERM,0.833889816360601,"2 Perm
1 Perm Bias2"
PERM,0.8347245409015025,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.835559265442404,MOST - HOME
PERM,0.8363939899833055,"2 Perm
1 Perm Bias2"
PERM,0.837228714524207,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8380634390651085,NAME - IN
PERM,0.83889816360601,"2 Perm
1 Perm Bias2"
PERM,0.8397328881469115,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.840567612687813,NEITHER - FIGURE
PERM,0.8414023372287145,"2 Perm
1 Perm Bias2"
PERM,0.842237061769616,"Figure 12: Empirical MSEs of C-MinHash-(Ï€, Ï€) (â€œ1 Permâ€, red, solid) vs. C-MinHash-(Ïƒ, Ï€) (â€œ2
Permâ€, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(Ï€, Ï€) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.8430717863105175,Under review as a conference paper at ICLR 2022
PERM,0.843906510851419,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8447412353923205,NET - SO
PERM,0.845575959933222,"2 Perm
1 Perm Bias2"
PERM,0.8464106844741235,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8472454090150251,NEW - PLEASE
PERM,0.8480801335559266,"2 Perm
1 Perm Bias2"
PERM,0.8489148580968281,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8497495826377296,NEW - WEB
PERM,0.8505843071786311,"2 Perm
1 Perm Bias2"
PERM,0.8514190317195326,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8522537562604341,NEWS - LIKE
PERM,0.8530884808013356,"2 Perm
1 Perm Bias2"
PERM,0.8539232053422371,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8547579298831386,NO - WELL
PERM,0.8555926544240401,"2 Perm
1 Perm Bias2"
PERM,0.8564273789649416,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8572621035058431,NOT - IT
PERM,0.8580968280467446,"2 Perm
1 Perm Bias2"
PERM,0.8589315525876461,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8597662771285476,NOTORIOUSLY - LOCK
PERM,0.8606010016694491,"2 Perm
1 Perm Bias2"
PERM,0.8614357262103506,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8622704507512521,OF - THEN
PERM,0.8631051752921536,"2 Perm
1 Perm Bias2"
PERM,0.8639398998330551,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8647746243739566,OF - WE
PERM,0.8656093489148581,"2 Perm
1 Perm Bias2"
PERM,0.8664440734557596,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8672787979966611,OPPORTUNITY - COUNTRIES
PERM,0.8681135225375626,"2 Perm
1 Perm Bias2"
PERM,0.8689482470784641,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8697829716193656,WALKED - ANTIVIRUS
PERM,0.8706176961602671,"2 Perm
1 Perm Bias2"
PERM,0.8714524207011686,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8722871452420701,OUR - THAN
PERM,0.8731218697829716,"2 Perm
1 Perm Bias2"
PERM,0.8739565943238731,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8747913188647746,OVER - BACK
PERM,0.8756260434056762,"2 Perm
1 Perm Bias2"
PERM,0.8764607679465777,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8772954924874792,OVER - TWO
PERM,0.8781302170283807,"2 Perm
1 Perm Bias2"
PERM,0.8789649415692822,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8797996661101837,PEAK - SHOWS
PERM,0.8806343906510852,"2 Perm
1 Perm Bias2"
PERM,0.8814691151919867,"Figure 13: Empirical MSEs of C-MinHash-(Ï€, Ï€) (â€œ1 Permâ€, red, solid) vs. C-MinHash-(Ïƒ, Ï€) (â€œ2
Permâ€, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(Ï€, Ï€) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.8823038397328882,Under review as a conference paper at ICLR 2022
PERM,0.8831385642737897,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8839732888146912,PEOPLE - BY
PERM,0.8848080133555927,"2 Perm
1 Perm Bias2"
PERM,0.8856427378964942,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8864774624373957,PEOPLE - INFO
PERM,0.8873121869782972,"2 Perm
1 Perm Bias2"
PERM,0.8881469115191987,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8889816360601002,PICKS - BOOST
PERM,0.8898163606010017,"2 Perm
1 Perm Bias2"
PERM,0.8906510851419032,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8914858096828047,PLANET - REWARD
PERM,0.8923205342237062,"2 Perm
1 Perm Bias2"
PERM,0.8931552587646077,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8939899833055092,PLEASE - MAKE
PERM,0.8948247078464107,"2 Perm
1 Perm Bias2"
PERM,0.8956594323873122,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8964941569282137,PREFER - PUEDE
PERM,0.8973288814691152,"2 Perm
1 Perm Bias2"
PERM,0.8981636060100167,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8989983305509182,PRIVACY - FOUND
PERM,0.8998330550918197,"2 Perm
1 Perm Bias2"
PERM,0.9006677796327212,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9015025041736227,PROSECUTION - MAXIMIZE
PERM,0.9023372287145242,"2 Perm
1 Perm Bias2"
PERM,0.9031719532554258,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9040066777963273,RECENTLY - INT
PERM,0.9048414023372288,"2 Perm
1 Perm Bias2"
PERM,0.9056761268781303,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9065108514190318,REPLY - ACHIEVE
PERM,0.9073455759599333,"2 Perm
1 Perm Bias2"
PERM,0.9081803005008348,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9090150250417363,RESERVED - BEEN
PERM,0.9098497495826378,"2 Perm
1 Perm Bias2"
PERM,0.9106844741235393,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9115191986644408,RIGHTS - FIND
PERM,0.9123539232053423,"2 Perm
1 Perm Bias2"
PERM,0.9131886477462438,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9140233722871453,RIGHTS - RESERVED
PERM,0.9148580968280468,"2 Perm
1 Perm Bias2"
PERM,0.9156928213689483,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9165275459098498,SCENE - ABOUT
PERM,0.9173622704507512,"2 Perm
1 Perm Bias2"
PERM,0.9181969949916527,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9190317195325542,SEE - ALSO
PERM,0.9198664440734557,"2 Perm
1 Perm Bias2"
PERM,0.9207011686143572,"Figure 14: Empirical MSEs of C-MinHash-(Ï€, Ï€) (â€œ1 Permâ€, red, solid) vs. C-MinHash-(Ïƒ, Ï€) (â€œ2
Permâ€, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(Ï€, Ï€) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.9215358931552587,Under review as a conference paper at ICLR 2022
PERM,0.9223706176961602,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9232053422370617,SEIZE - ANYTHING
PERM,0.9240400667779632,"2 Perm
1 Perm Bias2"
PERM,0.9248747913188647,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9257095158597662,SHOULDERS - GORGEOUS
PERM,0.9265442404006677,"2 Perm
1 Perm Bias2"
PERM,0.9273789649415692,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9282136894824707,SICK - FELL
PERM,0.9290484140233722,"2 Perm
1 Perm Bias2"
PERM,0.9298831385642737,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9307178631051753,SITE - CELLULAR
PERM,0.9315525876460768,"2 Perm
1 Perm Bias2"
PERM,0.9323873121869783,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9332220367278798,SOLD - LIVE
PERM,0.9340567612687813,"2 Perm
1 Perm Bias2"
PERM,0.9348914858096828,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9357262103505843,SOLO - CLAIMS
PERM,0.9365609348914858,"2 Perm
1 Perm Bias2"
PERM,0.9373956594323873,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9382303839732888,SOON - ADVANCE
PERM,0.9390651085141903,"2 Perm
1 Perm Bias2"
PERM,0.9398998330550918,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9407345575959933,SPECIALIZES - ACTUAL
PERM,0.9415692821368948,"2 Perm
1 Perm Bias2"
PERM,0.9424040066777963,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9432387312186978,STATE - OF
PERM,0.9440734557595993,"2 Perm
1 Perm Bias2"
PERM,0.9449081803005008,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9457429048414023,STATES - UNITED
PERM,0.9465776293823038,"2 Perm
1 Perm Bias2"
PERM,0.9474123539232053,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9482470784641068,TATTOO - JEANS
PERM,0.9490818030050083,"2 Perm
1 Perm Bias2"
PERM,0.9499165275459098,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9507512520868113,THAT - ALSO
PERM,0.9515859766277128,"2 Perm
1 Perm Bias2"
PERM,0.9524207011686143,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9532554257095158,THIS - CITY
PERM,0.9540901502504173,"2 Perm
1 Perm Bias2"
PERM,0.9549248747913188,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9557595993322203,THEIR - SUPPORT
PERM,0.9565943238731218,"2 Perm
1 Perm Bias2"
PERM,0.9574290484140233,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9582637729549248,THEIR - VIEW
PERM,0.9590984974958264,"2 Perm
1 Perm Bias2"
PERM,0.9599332220367279,"Figure 15: Empirical MSEs of C-MinHash-(Ï€, Ï€) (â€œ1 Permâ€, red, solid) vs. C-MinHash-(Ïƒ, Ï€) (â€œ2
Permâ€, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(Ï€, Ï€) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.9607679465776294,Under review as a conference paper at ICLR 2022
PERM,0.9616026711185309,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9624373956594324,THEM - OF
PERM,0.9632721202003339,"2 Perm
1 Perm Bias2"
PERM,0.9641068447412354,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9649415692821369,THEN - NEW
PERM,0.9657762938230384,"2 Perm
1 Perm Bias2"
PERM,0.9666110183639399,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9674457429048414,THINKS - LOT
PERM,0.9682804674457429,"2 Perm
1 Perm Bias2"
PERM,0.9691151919866444,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9699499165275459,TIME - OUT
PERM,0.9707846410684474,"2 Perm
1 Perm Bias2"
PERM,0.9716193656093489,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9724540901502504,TIME - WELL
PERM,0.9732888146911519,"2 Perm
1 Perm Bias2"
PERM,0.9741235392320534,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9749582637729549,TOP - AS
PERM,0.9757929883138564,"2 Perm
1 Perm Bias2"
PERM,0.9766277128547579,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9774624373956594,TOP - COPYRIGHT
PERM,0.9782971619365609,"2 Perm
1 Perm Bias2"
PERM,0.9791318864774624,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9799666110183639,TOP - NEWS
PERM,0.9808013355592654,"2 Perm
1 Perm Bias2"
PERM,0.9816360601001669,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9824707846410684,UP - AND
PERM,0.9833055091819699,"2 Perm
1 Perm Bias2"
PERM,0.9841402337228714,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9849749582637729,UP - HAS
PERM,0.9858096828046744,"2 Perm
1 Perm Bias2"
PERM,0.986644407345576,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9874791318864775,US - BE
PERM,0.988313856427379,"2 Perm
1 Perm Bias2"
PERM,0.9891485809682805,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.989983305509182,VIEW - IN
PERM,0.9908180300500835,"2 Perm
1 Perm Bias2"
PERM,0.991652754590985,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9924874791318865,VIEW - PEOPLE
PERM,0.993322203672788,"2 Perm
1 Perm Bias2"
PERM,0.9941569282136895,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.994991652754591,WEB - GO
PERM,0.9958263772954925,"2 Perm
1 Perm Bias2"
PERM,0.996661101836394,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9974958263772955,WEEKS - LONDON
PERM,0.998330550918197,"2 Perm
1 Perm Bias2"
PERM,0.9991652754590985,"Figure 16: Empirical MSEs of C-MinHash-(Ï€, Ï€) (â€œ1 Permâ€, red, solid) vs. C-MinHash-(Ïƒ, Ï€) (â€œ2
Permâ€, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(Ï€, Ï€) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
