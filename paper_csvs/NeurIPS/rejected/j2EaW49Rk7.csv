Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002183406113537118,"Gradient boosting of decision rules is an efficient approach to find interpretable yet
1"
ABSTRACT,0.004366812227074236,"accurate machine learning models. However, in practice, interpretability requires
2"
ABSTRACT,0.006550218340611353,"to limit the number and size of the generated rules, and existing boosting variants
3"
ABSTRACT,0.008733624454148471,"are not designed for this purpose. Through their strict greedy approach, they can
4"
ABSTRACT,0.010917030567685589,"increase accuracy only by adding further rules, even when the same gains can
5"
ABSTRACT,0.013100436681222707,"be achieved, in a more interpretable form, by altering already discovered rules.
6"
ABSTRACT,0.015283842794759825,"Here we address this shortcoming by adopting a weight correction step in each
7"
ABSTRACT,0.017467248908296942,"boosting round to maximise the predictive gain per added rule. This leads to a
8"
ABSTRACT,0.019650655021834062,"new objective function for rule selection that, based on orthogonal projections,
9"
ABSTRACT,0.021834061135371178,"anticipates the subsequent weight correction. This approach does not only correctly
10"
ABSTRACT,0.024017467248908297,"approximate the ideal update of adding the risk gradient itself to the model, it
11"
ABSTRACT,0.026200873362445413,"also favours the inclusion of more general and thus shorter rules. Additionally,
12"
ABSTRACT,0.028384279475982533,"we derive a fast incremental algorithm for rule evaluation, as necessary to enable
13"
ABSTRACT,0.03056768558951965,"efficient single-rule optimisation through either the greedy or the branch-and-
14"
ABSTRACT,0.03275109170305677,"bound approach. As we demonstrate on a range of classification, regression,
15"
ABSTRACT,0.034934497816593885,"and Poisson regression tasks, the resulting rule learner significantly improves the
16"
ABSTRACT,0.03711790393013101,"comprehensibility/accuracy trade-off of the fitted ensemble. At the same time, it
17"
ABSTRACT,0.039301310043668124,"has comparable computational cost to previous branch-and-bound rule learners.
18"
INTRODUCTION,0.04148471615720524,"1
Introduction
19"
INTRODUCTION,0.043668122270742356,"Algorithms for learning additive rule ensembles (or rule sets) are an active area of research, because
20"
INTRODUCTION,0.04585152838427948,"they are intrinsically interpretable yet relatively accurate due to their modularity and ability to
21"
INTRODUCTION,0.048034934497816595,"represent interaction effects. While there is an emerging consensus that rule ensembles should
22"
INTRODUCTION,0.05021834061135371,"optimize the trade-off between statistical risk and cognitive complexity in terms of number and lengths
23"
INTRODUCTION,0.05240174672489083,"of rules (see Fig. 1), there is a multitude of diverse approaches for performing this optimization.
24"
INTRODUCTION,0.05458515283842795,"This ranges from computationally inexpensive generate-and-select approaches [10; 14], over more
25"
INTRODUCTION,0.056768558951965066,"expensive minimum-description length and Bayesian approaches [29], to expensive full-fledged
26"
INTRODUCTION,0.05895196506550218,"discrete optimization methods [6; 30]. Within this range of options, methods based on gradient
27"
INTRODUCTION,0.0611353711790393,"boosting [9] are of special interest because of their robustness against changes in the training data,
28"
INTRODUCTION,0.06331877729257641,"flexibility to adapt to various response variable types and loss functions, and finally their good model
29"
INTRODUCTION,0.06550218340611354,"performance relative to their computational cost.
30"
INTRODUCTION,0.06768558951965066,"On the other hand, state-of-the-art rule boosting approaches are based on design choices that compro-
31"
INTRODUCTION,0.06986899563318777,"mise their risk/complexity trade-off. The traditional gradient boosting adaption [8] resorts to greedy
32"
INTRODUCTION,0.07205240174672489,"optimization of the individual rules, which results in additional rules and additional conditions per
33"
INTRODUCTION,0.07423580786026202,"rule to reach a desired statistical risk level. The more recent optimal rule boosting approach [3]
34"
INTRODUCTION,0.07641921397379912,"partially addresses this issue, but it is based on the uncorrected weight updates of the extreme gradient
35"
INTRODUCTION,0.07860262008733625,"boosting framework [4]. This too results in the inclusion of unnecessary extra rules, especially for
36"
INTRODUCTION,0.08078602620087336,"0
20
40
60
80
100
Cognitive Complexity 10
2 10
1 100 Risk"
INTRODUCTION,0.08296943231441048,"Gradient Boosting / XGBoost:
+18418 if PS
120 & year
2005
+13919 if count
69 & km
125000 & PS
220 & year
2011
- 8545  if 120
powerPS
160 & 2005
year
2011
+2769  if True
- 4150  if count
18 & km
50000 & PS
120 & year
2005
+2826  if count
18 & km
100000 & year
2008
+5878  if count
150 & km
125000 & PS
220 & year
2005
- 10356 if km
125000 & PS=220 & year
2011"
INTRODUCTION,0.0851528384279476,"Proposed algorithm:
+5245  if PS
120 & year
2005
+6779  if km
125000 & year
2011
+7451  if count
150 & PS
220
+3932  if year
2000
+6001  if PS
160 & year
2008
+12621 if count
150 & PS
160 & year
2015"
INTRODUCTION,0.08733624454148471,used cars
INTRODUCTION,0.08951965065502183,"SIRUS
GS
GB / XGB
FCOGB
train
test"
INTRODUCTION,0.09170305676855896,"0
20
40
60
80
100
Cognitive Complexity"
INTRODUCTION,0.09388646288209607,"Gradient Boosting:
- 3.03 if var
1.24
+3.47 if skew
7.62 & var
-0.50
+5.65 if curt
-2.18 & skew
7.62 & -0.50
var
3.38
- 4.01 if curt
-2.18 & skew
3.42 & -2.26
var
1.24
- 5.38 if curt
4.57 & skew
-3.38 & var
1.24
+3.41 if curt
4.57 & ent
-0.19 & skew
0.76 & -0.50
var
1.24
- 3.92 if curt
-2.18 & skew
7.62 & var
1.24
- 2.76 if curt
-0.18 & ent
-0.19 & skew
0.76 & -0.507
var
1.24
+5.05 if -2.18
curt
-0.18 & ent
0.54 & skew
3.42 
         & 1.24
var
3.38
- 3.47 if curt
1.40 & skew
7.62 & var
-2.26"
INTRODUCTION,0.09606986899563319,"Proposed algorithm:
- 4.80 if var
1.24
+5.20 if skew
7.62 & var
-0.50
+7.68 if curt
-2.18 & skew
7.62 & -0.50
var
3.38
- 4.36 if skew
3.42 & -2.26
var
1.24
- 6.81 if curt
4.57 & skew
-3.38 & var
1.24
+3.34 if curt
4.57 & ent
-0.19 & skew
0.76 & 
            -0.50
var
1.24"
INTRODUCTION,0.0982532751091703,banknote
INTRODUCTION,0.10043668122270742,"SIRUS
GS
GB
XGB
FCOGB
train
test"
INTRODUCTION,0.10262008733624454,"Figure 1: Risk/complexity curves for previous rule boosting variants (green) and proposed orthog-
onalization approach (red) for dataset used_cars and banknote. The two highlighted corners
correspond to rule ensembles with roughly equivalent training risk but substantially reduced cognitive
complexity for the proposed algorithm.
loss functions with unbounded second derivatives like the Poisson loss. Most importantly, both
37"
INTRODUCTION,0.10480349344978165,"approaches use the strict stagewise fitting approach where rules are not revised after they are added to
38"
INTRODUCTION,0.10698689956331878,"the ensemble. Thus, they can increase accuracy only by adding further rules, even when the same
39"
INTRODUCTION,0.1091703056768559,"gains can be achieved, in a more interpretable form, by altering those already present in the model.
40"
INTRODUCTION,0.11135371179039301,"Here we develop the first rule boosting algorithm that consistently optimizes the accuracy/complexity
41"
INTRODUCTION,0.11353711790393013,"trade-off of the produced rule sets. For that, we adopt the fully corrective boosting approach [26]
42"
INTRODUCTION,0.11572052401746726,"where all rule consequents are re-optimized in every boosting round, which can be done with
43"
INTRODUCTION,0.11790393013100436,"only little computational extra effort given the usual convex loss functions. We then derive a new
44"
INTRODUCTION,0.12008733624454149,"objective function for selecting individual rule bodies that anticipates the subsequent consequent
45"
INTRODUCTION,0.1222707423580786,"re-optimization. This function is based on considering only the part of a rule body orthogonal to the
46"
INTRODUCTION,0.12445414847161572,"already selected rules, which, as we show, correctly identifies the best approximation to the ideal space
47"
INTRODUCTION,0.12663755458515283,"for consequent optimization defined by the risk gradient. Finally, we derive a corresponding efficient
48"
INTRODUCTION,0.12882096069868995,"algorithm for cut-point search, which is crucial for, either greedy or branch-and-bound, single rule
49"
INTRODUCTION,0.13100436681222707,"optimization. As we demonstrate on a wide range of datasets, the resulting rule boosting algorithm
50"
INTRODUCTION,0.1331877729257642,"significantly outperforms the previous boosting variants in terms of risk/complexity trade-off, which
51"
INTRODUCTION,0.13537117903930132,"can be attributed to a better risk reduction per rule as well as an affinity to select simpler rules. At the
52"
INTRODUCTION,0.13755458515283842,"same time, the computational cost remains comparable to the previous branch-and-bound rule learner.
53"
INTRODUCTION,0.13973799126637554,"The paper is organized as follows. After giving a brief overview of the wider literature on intepretable
54"
INTRODUCTION,0.14192139737991266,"machine learning and additive rule ensembles (Sec. 2), we recall the formal basics of rule ensembles
55"
INTRODUCTION,0.14410480349344978,"and gradient boosting in Sec. 3. We then present our main technical contributions in Sec. 4 and their
56"
INTRODUCTION,0.1462882096069869,"empirical evaluation in Sec. 5, before concluding in Sec. 6.
57"
RELATED LITERATURE,0.14847161572052403,"2
Related Literature
58"
RELATED LITERATURE,0.15065502183406113,"In contrast to post-hoc explanations of blackbox models [e.g., 28; 23], which are often unfaithful to
59"
RELATED LITERATURE,0.15283842794759825,"the original model [21; 24; 13], interpretable machine learning methods aim to produce intrinsically
60"
RELATED LITERATURE,0.15502183406113537,"intelligible, yet accurate, models. Additive models that compose terms in a simple summation are
61"
RELATED LITERATURE,0.1572052401746725,"particularly useful in this context, because of their modularity, i.e., the possibility to comprehend the
62"
RELATED LITERATURE,0.15938864628820962,"terms in isolation. As long as the individual terms are not too numerous and simulatable, i.e., their
63"
RELATED LITERATURE,0.1615720524017467,"output can be approximately computed by a human, the resulting model is highly interpretable.
64"
RELATED LITERATURE,0.16375545851528384,"Good examples for this are (generalized) linear models [GLMs, 19], or generalized additive mod-
65"
RELATED LITERATURE,0.16593886462882096,"els [GAMs, 12; 16]. However, they do not model variable interactions, at least not of higher order.
66"
RELATED LITERATURE,0.16812227074235808,"Conjunctive propositional rules, on the other hand, have this ability, explaining their longstanding
67"
RELATED LITERATURE,0.1703056768558952,"popularity in machine learning and related fields. Additive rule ensembles, which are closely related
68"
RELATED LITERATURE,0.17248908296943233,"Figure 2: Illustration of output space for toy regression example with three data points with target
values y1 = −10, y2 = −6, y3 = 5 and three queries with output vectors q1 = (1, 1, 0), i.e., q1
selects the first two data points, q2 = (0, 0, 1), and q3 = (0, 1, 1). The gradient boosting objective
selects q1 with weight β1 = −8 as first rule, resulting in a negative gradient vector −g = (−2, 2, 5).
Left: Approximations (4) to target subspace (blue) spanned by q1 and −g. The subspace (green)
spanned by q3 and q1 is a better approximation than the subspace (orange) spanned by q2 and q1.
However, the latter is selected by standard gradient boosting.Right: After projection onto orthogonal
complement of already selected query, angle between q3 and −g is smaller than that between q2 and
−g and is thus successfully selected by orthogonal gradient boosting objective."
RELATED LITERATURE,0.17467248908296942,"to non-modular rule lists [e.g., 31; 22], thus provide a unique combination of interpretability and
69"
RELATED LITERATURE,0.17685589519650655,"predictive power. There is a wide range of algorithms for learning additive rule ensembles. One
70"
RELATED LITERATURE,0.17903930131004367,"approach is to generate a candidate set and then sub-selecting a rule ensemble, e.g. via sub-modular
71"
RELATED LITERATURE,0.1812227074235808,"optimization [14; 32], or—as in RuleFit [10] or SIRUS [2]—via a sparse linear model, which is
72"
RELATED LITERATURE,0.18340611353711792,"especially computationally inexpensive. However, these approaches are typically highly sensitive to
73"
RELATED LITERATURE,0.185589519650655,"the randomness in the generation of the candidate set. Alternatively, finding an optimal rule ensemble
74"
RELATED LITERATURE,0.18777292576419213,"can be expressed as an integer program. Its relaxation as linear problem can then be solved via the
75"
RELATED LITERATURE,0.18995633187772926,"column generation framework [6; 30], making the problem tractable. This approach is robust and
76"
RELATED LITERATURE,0.19213973799126638,"flexible, but the full optimization remains computationally expensive.
77"
RELATED LITERATURE,0.1943231441048035,"Early approaches to additive rule ensemble learning that avoid initial candidate generation are based
78"
RELATED LITERATURE,0.1965065502183406,"on the separate-and-conquer framework [11] and later on the original boosting algorithm [5; 17].
79"
RELATED LITERATURE,0.19868995633187772,"However, the first typically leads to non-modular rule lists and the second are designed for specific
80"
RELATED LITERATURE,0.20087336244541484,"learning tasks only, typically classification. This problem is overcome with the gradient boosting
81"
RELATED LITERATURE,0.20305676855895197,"framework [9], which generalizes the original AdaBoost algorithm [25] and allows fitting arbitrary
82"
RELATED LITERATURE,0.2052401746724891,"differentiable loss functions. With this framework, rules are fitted stagewise based on their effect
83"
RELATED LITERATURE,0.2074235807860262,"on the training loss when added to the ensemble [7; 8]. Extreme gradient boosting [4] increases
84"
RELATED LITERATURE,0.2096069868995633,"the scalability of gradient boosting by avoiding numerical weight optimization. It is applicable
85"
RELATED LITERATURE,0.21179039301310043,"whenever the loss function is twice differentiable. Fully-corrective boosting recalculate the weight of
86"
RELATED LITERATURE,0.21397379912663755,"all weak learners after adding one weak learner into the ensemble model [26; 27]. It overcomes the
87"
RELATED LITERATURE,0.21615720524017468,"drawback of the original gradient boosting algorithm that the weak learners are not changed after
88"
RELATED LITERATURE,0.2183406113537118,"being generated. However, it is a high-level framework and does not solve the problem of how to
89"
RELATED LITERATURE,0.2205240174672489,"select individual base learners.
90"
RULE BOOSTING,0.22270742358078602,"3
Rule Boosting
91"
RULE BOOSTING,0.22489082969432314,"An additive ensemble of k rules can be represented by Boolean query functions q1, . . . , qk and a
92"
RULE BOOSTING,0.22707423580786026,"weight vector β = (β1, . . . , βk)T ∈Rk that jointly describe a function f(x) = Pk
i=1 βiqi(x), the
93"
RULE BOOSTING,0.2292576419213974,"output of which can be mapped to the conditional distribution of a target variable Y |X = x. That is,
94"
RULE BOOSTING,0.2314410480349345,"the queries define the rule antecedents (rule bodies), and the coefficients β define the rule consequents,
95"
RULE BOOSTING,0.2336244541484716,"i.e., the output of rule i for input x ∈Rd is βi if x satisfies the antecedent, i.e., qi(x) = 1 (and 0
96"
RULE BOOSTING,0.23580786026200873,"otherwise). Moreover, each query function qi : Rd →{0, 1} is a conjunction of ci propositions, i.e.,
97"
RULE BOOSTING,0.23799126637554585,"qi(x) = pi,1(x)pi,2(x) . . . pi,ci(x) where the pi,j are typically a threshold function on an individual
98"
RULE BOOSTING,0.24017467248908297,"input variable, e.g., pi,j(x) = δ(xl ≥t). We denote the set of available propositions by P and the
99"
RULE BOOSTING,0.2423580786026201,"query language of all conjunctions that can be formed from P as Q.
100"
RULE BOOSTING,0.2445414847161572,"We are concerned with two properties of an additive rule ensemble: its (empirical) prediction risk1
101"
RULE BOOSTING,0.24672489082969432,R(f) = 1
RULE BOOSTING,0.24890829694323144,"n
Pn
i=1 l(f(xi), yi), measured by some positive loss function l(f(x), y) averaged over a
102"
RULE BOOSTING,0.25109170305676853,"dataset {(x1, y1), . . . , (xn, yn)}, and its cognitive complexity C(f) = k + Pk
i=1 ci, measuring
103"
RULE BOOSTING,0.25327510917030566,"the cognitive effort required to parse all rule consequents and antecedents. Here we consider loss
104"
RULE BOOSTING,0.2554585152838428,"functions that can be derived as negative log likelihood (or rather deviance function) when interpreting
105"
RULE BOOSTING,0.2576419213973799,"the rule ensemble output as natural parameter of an exponential family model of the target variable,
106"
RULE BOOSTING,0.259825327510917,"which guarantees that the loss function is strictly convex and twice differentiable. Specifically, we
107"
RULE BOOSTING,0.26200873362445415,"consider the cases of squared loss lsqr(f(xi), yi) = (f(xi) −yi)2, the logistic loss llog(f(xi), yi) =
108"
RULE BOOSTING,0.26419213973799127,"log(1 + exp(−yif(xi))), and the Poisson loss lpoi(f(xi), yi) = log yi −f(xi) −yi + exp(f(xi)).
109"
RULE BOOSTING,0.2663755458515284,"Gradient boosting
Gradient boosting [9] is a “stagewise” fitting scheme for additive models that,
110"
RULE BOOSTING,0.2685589519650655,"in our context, produces a sequence of rule ensembles f (0), f (1), . . . , f (k) such that f (0)(x) = 0 and,
111"
RULE BOOSTING,0.27074235807860264,"for t ∈[1, k], f (t)(x) = f (t−1)(x) + βtqt(x). Specifically, the term βtqt(x) is chosen to perform an
112"
RULE BOOSTING,0.27292576419213976,"approximate gradient descent with respect to the risk function R(f) = R(f) considered as a function
113"
RULE BOOSTING,0.27510917030567683,"of the model output vector f = (f(x1), . . . , f(xn)). The exact gradient descent update would be
114"
RULE BOOSTING,0.27729257641921395,"−α∗g where g is the gradient vector with components gi = ∂l(f(xi), yi)/∂f(xi) and α∗is the
115"
RULE BOOSTING,0.2794759825327511,"step length that minimizes the empicical risk R(f −αg). However, since in general there is no query
116"
RULE BOOSTING,0.2816593886462882,"q for which the output vector q = (q(x1), . . . , q(xn)) is equal to the gradient g, the goal is to select
117"
RULE BOOSTING,0.2838427947598253,"q∗that best approximates g in the sense that it minimizes the squared projection error
118"
RULE BOOSTING,0.28602620087336245,"min
β∈R ∥−α∗g −βq∥2 = α2
∗"
RULE BOOSTING,0.28820960698689957,"
∥g∥2 −(qT g)2 ∥q∥2"
RULE BOOSTING,0.2903930131004367,"
.
(1)"
RULE BOOSTING,0.2925764192139738,"This is achieved by choosing qt to maximize the standard gradient boosting objective [8] objgb(q) =
119"
RULE BOOSTING,0.29475982532751094,"|qT g|/∥q∥and to find βt = arg minβ∈R R(f + βqt) via a line search. Note that this βt is not
120"
RULE BOOSTING,0.29694323144104806,"generally equal to the minimizing β in (1), because the optimal update in direction q can be better
121"
RULE BOOSTING,0.29912663755458513,"than the best geometric approximation to the gradient descent update in direction q. A derivation of
122"
RULE BOOSTING,0.30131004366812225,"this objective function is the gradient sum objective [8; 26] objgs(q) = |qT g|, which always selects
123"
RULE BOOSTING,0.3034934497816594,"more general rules than the gradient boosting objective [8, Thm. 1], however, typically at the expense
124"
RULE BOOSTING,0.3056768558951965,"of an increased risk per rule, because the correction of data points with large gradient elements has to
125"
RULE BOOSTING,0.3078602620087336,"be toned down to avoid over-correction of other selected data points with small gradient elements.
126"
RULE BOOSTING,0.31004366812227074,"Finally, an adaption of “extreme gradient boosting” [4] to rule ensembles yields the extreme boosting
127"
RULE BOOSTING,0.31222707423580787,"objective [3] objxgb(q) = (qT g)2/qT h where h = diag(∇2
f(x)R(f)) is the diagonal vector of the
128"
RULE BOOSTING,0.314410480349345,"risk Hessian again with respect to the output vector f. This approach starts from the second order
129"
RULE BOOSTING,0.3165938864628821,"approximation of R(f + βq) for which also yields a closed form weight update βt = −qT g/qT h.
130"
RULE BOOSTING,0.31877729257641924,"This approach is well-defined for our loss functions derived from exponential family response models,
131"
RULE BOOSTING,0.32096069868995636,"which guarantee defined and positive h. For the squared loss, it is equivalent to standard gradient
132"
RULE BOOSTING,0.3231441048034934,"boosting, because the second order approximation is exact for lsqr and h is constant.
133"
RULE BOOSTING,0.32532751091703055,"Single rule optimization
While the rule optimization literature can be neatly divided into heuristic
134"
RULE BOOSTING,0.32751091703056767,"(greedy / beam search) and exact branch-and-bound search, these approaches are actually closely
135"
RULE BOOSTING,0.3296943231441048,"related: they can both be described as traversing a lattice on the query language Q imposed by a
136"
RULE BOOSTING,0.3318777292576419,"specialization relation q ⪯q′ that holds if the propositions in q′ are a superset of those mentioned in
137"
RULE BOOSTING,0.33406113537117904,"q, and q′ thus logically implies q. The difference between the approaches is under what conditions
138"
RULE BOOSTING,0.33624454148471616,"they discard specializations of candidate queries and how those specializations are generated.
139"
RULE BOOSTING,0.3384279475982533,"Here, we build on the branch-and-bound framework presented in Boley et al. [3] that allows to
140"
RULE BOOSTING,0.3406113537117904,"efficiently search for optimal conjunctive queries in a condensed search space, given that there is
141"
RULE BOOSTING,0.34279475982532753,"an admissible, effective, and efficiently computable bounding function for the employed objective.
142"
RULE BOOSTING,0.34497816593886466,"Specifically, let obj : Q →R denote the objective function to be maximized. Then a bounding
143"
RULE BOOSTING,0.3471615720524017,"function bnd : Q →R is admissible if for all q ∈Q it holds that bnd(q) ≥bst(q) where
144"
RULE BOOSTING,0.34934497816593885,"bst(q) = maxq⪯q′∈Q obj(q′) denotes the objective value of the best specialization of q. A bounding
145"
RULE BOOSTING,0.35152838427947597,"function is effective in allowing to prune the search space if the difference bnd(q) −bst(q) tends
146"
RULE BOOSTING,0.3537117903930131,"1Note that for all algorithms discussed here, the l2-regularized risk can also be considered with only light
modifications. However, for ease of exposition and since regularization typically is not crucial for the rather
small rule ensembles considered here, we focus on the unregularized case."
RULE BOOSTING,0.3558951965065502,"to be small, rendering bst(q) itself the theoretically most effective bounding function. However,
147"
RULE BOOSTING,0.35807860262008734,"computing bst(q) is as hard as the overall optimization problem.
148"
RULE BOOSTING,0.36026200873362446,"A frequently applied recipe for constructing an admissible bounding function that is also effective and
149"
RULE BOOSTING,0.3624454148471616,"efficiently computable is to relax the quantifier in the definition of bst and instead of bounding the
150"
RULE BOOSTING,0.3646288209606987,"value of the best specialization in the search space, bound the value of the best subset of data points
151"
RULE BOOSTING,0.36681222707423583,"of those selected by q [20]. This results in the tight bounding function when unaware of selectability
152"
RULE BOOSTING,0.36899563318777295,"bnd(q) = max{obj(q′) : q′ ≤q, q′ ∈{0, 1}n} ≥bst(q) .
(2)
Here, q ≤q′ refers to the component-wise less or equals relation on the binary output vector of q
153"
RULE BOOSTING,0.37117903930131,"and q′. This function can be efficiently computed for many objective functions by pre-sorting the
154"
RULE BOOSTING,0.37336244541484714,"data in time O(n log n) that has to be carried out only once per fitted rule [15]. For instance for the
155"
RULE BOOSTING,0.37554585152838427,"extreme boosting objective, the optimum q′ ∈{0, 1}n can be found as a prefix or suffix of all data
156"
RULE BOOSTING,0.3777292576419214,"points after sorting them according to the ratio gi/hi of first and second order loss derivatives.
157"
FULLY-CORRECTIVE ORTHOGONAL GRADIENT BOOSTING,0.3799126637554585,"4
Fully-corrective Orthogonal Gradient Boosting
158"
FULLY-CORRECTIVE ORTHOGONAL GRADIENT BOOSTING,0.38209606986899564,"Having reviewed the existing rule boosting approaches, we now turn to improving them in terms of
159"
FULLY-CORRECTIVE ORTHOGONAL GRADIENT BOOSTING,0.38427947598253276,"their risk/complexity trade-off. Our approach to this is to improve the risk reduction per rule added to
160"
FULLY-CORRECTIVE ORTHOGONAL GRADIENT BOOSTING,0.3864628820960699,"the ensemble, which directly affects the number of rules needed to achieve a certain risk. As it turns
161"
FULLY-CORRECTIVE ORTHOGONAL GRADIENT BOOSTING,0.388646288209607,"out, this typically coincides with preferring the addition of more general and hence simpler rules.
162"
FULLY-CORRECTIVE ORTHOGONAL GRADIENT BOOSTING,0.39082969432314413,"Thus, it also positively affects the cognitive complexity on the level of the lengths of individual rules.
163"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.3930131004366812,"4.1
Weight Correction and Subspace Approximations
164"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.3951965065502183,"A natural idea to reduce the ensemble risk per rule added is to relax the strict stagewise fitting
165"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.39737991266375544,"approach of traditional gradient boosting and to allow the whole weight vector β to be adjusted in
166"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.39956331877729256,"every round t, i.e., to set
167"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4017467248908297,"β(t) = arg min
β∈Rt
R(Qtβ) ,
(3)"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4039301310043668,"where Qt = [q1, . . . , qt] is the n × t query matrix with the output vectors of all selected queries as
168"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.40611353711790393,"columns. In contrast to a full joint optimization of queries and weights, this intermediate solution
169"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.40829694323144106,"still retains the computational benefits of gradient boosting for small rule ensembles: Given that our
170"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4104803493449782,"loss function l and therefore the empirical risk R are convex, optimizing the weights in round t is a
171"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4126637554585153,"convex optimization problem in t variables, equivalent to fitting a small generalized linear model.
172"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4148471615720524,"Using Newton-Raphson (“iterated least squares”) type algorithms, the computational cost of this is
173"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4170305676855895,"usually negligible compared to the more expensive query optimization step, especially when aiming
174"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4192139737991266,"for optimal individual queries for their reduced cognitive complexity.
175"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.42139737991266374,"Re-optimizing the weights, which is sometimes referred to as fully corrective boosting [26], ef-
176"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.42358078602620086,"fectively turns boosting into a form of forward variable selection for linear models. However, in
177"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.425764192139738,"contrast to conventional variable selection where all variables are given explicitly, we still have to
178"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4279475982532751,"identify a good query qt in each boosting iteration, and it turns out that finding the appropriate query
179"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.43013100436681223,"is more complicated as in the case of single weight optimization characterized by (1). We still would
180"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.43231441048034935,"like to add the direction of steepest descent, i.e., the negative gradient −g, to the subsequent risk
181"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4344978165938865,"optimization step and approximate as closely as possible the outcome [Qt−1; g]α∗where α∗∈Rt
182"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4366812227074236,"are the risk minimizing weights for q1, . . . , qt−1, g. Therefore, the best approximating query q is
183"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4388646288209607,"now given by
184"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4410480349344978,"arg min
q∈Q
min
β∈Rt ∥[Qt−1; g]α∗−[Qt−1; q]β∥2 .
(4)"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4432314410480349,"It is an important observation that the standard gradient boosting objective does not correctly identify
185"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.44541484716157204,"this optimally approximating query. This is demonstrated by the example illustrated in Fig. 2. In
186"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.44759825327510916,"the left sub-figure it can be seen that the green plane, span{q1, q3}, is a better approximation to
187"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4497816593886463,"span{q1, −g} (blue) than the orange plain, span{q1, q2}. However, the latter is selected by standard
188"
WEIGHT CORRECTION AND SUBSPACE APPROXIMATIONS,0.4519650655021834,"gradient boosting, because the angle between q2 and −g is smaller than that between q3 and −g.
189"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.45414847161572053,"4.2
An Objective Function to Identify the Best Approximating Subspace
190"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.45633187772925765,"The intuitive reason for the gradient boosting objective failing to identify the correct query in Fig. 2
191"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4585152838427948,"is that selecting x2 in addition to x3 is not beneficial for the overall risk reduction if we are only
192"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4606986899563319,"allowed to set the weight for the newly selected query. This is because then this weight has to be a
193"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.462882096069869,"compromise between the two different magnitudes of correction required for x2, which only needs a
194"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4650655021834061,"small positive correction, and x3, which needs a large positive correction. If we, however, are allowed
195"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4672489082969432,"to change the weight of the previously selected query this consideration changes, because we can
196"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.46943231441048033,"now balance an over-correction for x2 by adjusting the weight of the first rule. While on first glance
197"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.47161572052401746,"it seems unclear how much of such re-balancing can be applied without harming the overall risk, it
198"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4737991266375546,"turns out that this is captured by a simple criterion based on the norm of the part of the newly selected
199"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4759825327510917,"query that is orthogonal to the already selected ones.
200"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4781659388646288,"Lemma 4.1. For g∈Rn, Q=[q1, . . . , qt−1] ∈Rn×(t−1), and f ∈span{q1, . . . , qt−1, g}, we have
201"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.48034934497816595,"arg min
q∈Q
min
β∈Rt ∥f −[q1, . . . , qt−1, q]β∥2 = arg max
q∈Q"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.48253275109170307,"|gT
⊥q|
∥q⊥∥.
(5)"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4847161572052402,"where for a vector v ∈Rn we denote by v⊥its projection onto the orthogonal complement of
202"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4868995633187773,"range Q. (All proofs of lemmas and theorems are in SI [1])
203"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4890829694323144,"From this result we can directly derive |gT
⊥q|/∥q⊥∥as suitable objective function for fully corrective
204"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4912663755458515,"gradient boosting. However, it is worth incorporating two further observations. Firstly, we can show
205"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.49344978165938863,"that, after applying the weight correction step (3), the gradient vector satisfies g = g⊥, i.e., it is
206"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.49563318777292575,"orthogonal to the subspace spanned by the selected queries, and therefore can be used in the objective
207"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.4978165938864629,"function without projection.
208"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5,"Lemma 4.2. Let g be the gradient vector after the application of the weight correction step (3) for
209"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5021834061135371,"selected queries q1, . . . , qt. Then g ⊥span{q1, . . . , qt}.
210"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5043668122270742,"Moreover, the right hand side of Eq. (4) is technically undefined for redundant query vectors q that
211"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5065502183406113,"lie in range Q and therefore have ∥q⊥∥= 0. Through Lm. 4.2 we know that for such queries we
212"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5087336244541485,"also have gT q = 0, which suggests to simply fix this issue by defining the objective value in this
213"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5109170305676856,"case to be 0. However, this solution would not fix numerical instabilities when ∥q⊥∥is close to
214"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5131004366812227,"zero. A better solution is therefore to add a small positive value ϵ to the denominator, which can
215"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5152838427947598,"be considered a regularization parameter. With this we arrive at the final form of our objective
216"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.517467248908297,"function, which we state along with some of its basic properties in the following theorem.
217"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.519650655021834,"Theorem 4.3. Let Q = [q1, . . . , qt−1] ∈Rn×(t−1) be the selected query matrix and g the cor-
218"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5218340611353712,"responding gradient vector after full weight correction, and let us denote by q = q⊥+ q∥the
219"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5240174672489083,"orthogonal decomposition of q with respect to range Q. Then we have for a maximizer q∗of the
220"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5262008733624454,"orthogonal gradient boosting objective objogb(q) = |gT q|/(∥q⊥∥+ ϵ):
221"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5283842794759825,"a) For ϵ →0, span{q1, . . . , qt−1, q∗} is the best approximation to span{q1, . . . , qt−1, g}.
222"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5305676855895196,"b) For ϵ →∞, q∗maximizes objgs and any maximizer of objgs maximizes objogb.
223"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5327510917030568,"c) For ϵ = 0 and ∥q⊥∥> 0, the ratio (objogb(q)/objgb(q))2 is equal to 1 + (∥q∥∥/∥q⊥∥)2.
224"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5349344978165939,"d) The objective value objogb(q) is upper bounded by ∥g∥.
225"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.537117903930131,"Intuitively, the orthogonal gradient boosting objective function measures the cosine of the angle
226"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5393013100436681,"between the gradient vector and the orthogonal projection of a candidate query vector q. This is in
227"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5414847161572053,"contrast to the standard gradient boosting objective, which considers the angle of the unprojected
228"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5436681222707423,"query vector instead. In the example in Fig. 2 we can observe that this difference leads to successfully
229"
AN OBJECTIVE FUNCTION TO IDENTIFY THE BEST APPROXIMATING SUBSPACE,0.5458515283842795,"identifying the best approximating subspace, and Thm. 4.3a) guarantees this property.
230"
EFFICIENT IMPLEMENTATION,0.5480349344978166,"4.3
Efficient Implementation
231"
EFFICIENT IMPLEMENTATION,0.5502183406113537,"To develop an efficient optimization algorithm for the orthogonal gradient boosting objective, we
232"
EFFICIENT IMPLEMENTATION,0.5524017467248908,"recall that projections q⊥on the orthogonal complement of range Q can be naively computed via
233"
EFFICIENT IMPLEMENTATION,0.5545851528384279,"q⊥= Q((QT Q)−1(QT q)) where we placed the parentheses to emphasize that only matrix-vector
234"
EFFICIENT IMPLEMENTATION,0.5567685589519651,"products are involved in the computation—at least once the inverse of the Gram matrix QT Q is
235"
EFFICIENT IMPLEMENTATION,0.5589519650655022,"computed. This approach allows to compute projections, and thus objective values, in time O(nt+t2)
236"
EFFICIENT IMPLEMENTATION,0.5611353711790393,"per candidate query after an initial preprocessing per boosting round of cost O(t2n + t3).
237"
EFFICIENT IMPLEMENTATION,0.5633187772925764,"In a first step, this naive approach can be improved by maintaining an orthonormal basis of the range
238"
EFFICIENT IMPLEMENTATION,0.5655021834061136,"of the query matrix throughout the boosting rounds, resulting in a Gram-Schmidt-type procedure.
239"
EFFICIENT IMPLEMENTATION,0.5676855895196506,Algorithm 1 Fully-corrective Orthogonal Gradient Boosting
EFFICIENT IMPLEMENTATION,0.5698689956331878,"Input: dataset (xi, yi)n
i=1, desired number of rules k
Initialise f (0) = 0
for t = 1 to k do"
EFFICIENT IMPLEMENTATION,0.5720524017467249,"g =

∂l(f (t−1)(x1),y1)"
EFFICIENT IMPLEMENTATION,0.574235807860262,"∂f (t−1)(x1)
, . . . , ∂l(f (t−1)(xn),yn)"
EFFICIENT IMPLEMENTATION,0.5764192139737991,"∂f (t−1)(xn)
"
EFFICIENT IMPLEMENTATION,0.5786026200873362,"qt = arg maxq∈Q
|qT g|
∥q⊥∥via beam(g, Ot−1) or bb(g, Ot−1)
ot = qt⊥/∥qt⊥∥and Ot = [Ot−1; ot]
βt = arg minβ∈Rt R([q1, . . . , qt]β) via convex_opt
f (t) = [q1, . . . , qt]βt
Output: f (k)(·) = βk,1q1(·) + · · · + βk,kqk(·)"
EFFICIENT IMPLEMENTATION,0.5807860262008734,"Table 1: Comparison of normalised training risks and computation times for rule ensembles, averaged
over cognitive complexities between 1 and 50, using SIRUS(SRS), Gradient Sum(GS), Gradient
boosting (GB), XGBoost (XGB) and FCOGB, for benchmark datasets of classification (upper),
regression (middle) and Poisson regression problems (lower)."
EFFICIENT IMPLEMENTATION,0.5829694323144105,"TRAINING RISKS
TESTING RISKS
COMPUTATION TIMES
DATASET
d
n
SRS GS
GB XGB FCOGB SRS
GS
GB
XGB FCOGB SRS
GS
GB
XGB FCOGB"
EFFICIENT IMPLEMENTATION,0.5851528384279476,"TITANIC
7
1043 .895 .662 .635 .637
.610
.894 .723
.712
.721
.707
7.077 2.624 9.858 10.21
25.71
TIC-TAC-TOE
27
958
.892 .741 .627 .640
.587
.885 .800
.722
.689
.669
12.59 3.971 10.34 6.09
13.99
IRIS
4
150
.685 .253 .222 .287
.218
.745 .384
.429
.408
.511
11.02 0.775 1.099 1.453
2.487
BREAST
30
569
.569 .273 .291 .314
.292
.627 .273
.370
.376
.348
11.48 6.744 74.43 74.83
239.2
WINE
13
178
.578 .162 .216 .192
.146
.621 .340
.471
.402
.242
9.456 1.530 4.432 2.154
55.18
IBM HR
32 1470 .980 .572 .560 .573
.560
.974 .607
.618
.626
.606
11.15 17.24 10.99 12.92
12.03
TELCO CHURN
18 7043 .944 .679 .683 .679
.670
.945 .663
.677
.673
.663
50.83 40.01 1883 1485
3039
GENDER
20 3168 .566 .996 .996 .996
.996
.570 1.000 1.000 1.000
1.000
22.42 22.73 25.49 24.27
32.95
BANKNOTE
4
1372 .854 .303 .264 .288
.227
.858 .310
.263
.297
.228
8.933 6.298 5.648 7.060
8.444
LIVER
6
345
.908 .809 .800 .787
.777
.917 .913 1.000 .928
1.000
9.734 1.997 99.72 124.1
193.9
MAGIC
10 19020 .906 .720 .709 .710
.707
.903 .702
.693
.693
.687
1.364 75.14 89.18 101.9
352.2
ADULT
11 30162 .804 .594 .599 .594
.582
.802 .603
.615
.607
.597
2.169 121.0 136.7 146.0
728.3
DIGITS5
64 3915 .248 .331 .312 .335
.353
.262 .329
.314
.330
.350
52.60 110.8 72.74 101.5
97.4"
EFFICIENT IMPLEMENTATION,0.5873362445414847,"INSURANCE
6
1338 .169 .144 .143 .146
.126
.177 .134
.137
.140
.126
14.06 7.507 15.94 12.98
39.53
FRIEDMAN1
10 2000 .180 .069 .073 .071
.068
.165 .072
.080
.079
.074
16.79 2.514 4.302 3.171
6.915
FRIEDMAN2
4 10000 .082 .092 .119 .116
.101
.082 .094
.120
.117
.101
47.33 11.79 17.56 13.18
28.4
FRIEDMAN3
4
5000 .093 .044 .043 .043
.041
.092 .046
.046
.046
.044
29.86 6.243 10.61 8.559
17.65
WAGE
5
1379 .427 .368 .366 .355
.342
.341 .377
.397
.394
.396
14.18 5.605 12.12 13.17
25.19
DEMOGRAPHICS 13 6876 .219 .214 .213 .213
.212
.209 .217
.217
.217
.216
38.24 36.80 29.40 33.04
72.42
GDP
1
35
.063 .024 .024 .024
.024
.059 .025
.025
.025
.025
7.974 .261
.351
.282
.488
USED CARS
4
1770 .175 .266 .250 .251
.225
.172 .289
.265
.271
.241
15.00 8.371 12.10 9.484
20.27
DIABETES
10
442
.156 .137 .137 .136
.130
.188 .148
.150
.155
.158
10.50 2.204 3.574 3.920
7.591
BOSTON
13
506
.101 .089 .090 .087
.081
.105 .078
.086
.086
.081
10.96 3.055 6.731 5.285
10.44
HAPPINESS
8
315
.109 .031 .031 .032
.030
.109 .033
.038
.038
.037
6.344 1.160 11.37 11.31
26.43
LIFE EXPECT.
21 1649 .109 .026 .026 .026
.025
.110 .027
.027
.027
.026
21.44 16.16 58.43 63.82
131.2
MOBILE PRICES 20 2000 .148 .131 .137 .137
.135
.140 .136
.143
.145
.142
33.81 15.03 367.7 442.5
815.4
SUICIDE RATE
5 27820 .547 .543 .540 .541
.534
.514 .521
.521
.521
.515
52.35 109.6 117.1 139.6
644.6
VIDEOGAME
6 16327 .895 .953 .953 .953
.953
.850 .720
.720
.720
.720
1.171 41.91 34.38 45.90
119.1
RED WINE
11 1599 .072 .034 .035 .035
.034
.073 .035
.035
.036
.035
19.94 9.149 15.32 21.99
35.34"
EFFICIENT IMPLEMENTATION,0.5895196506550219,"COVID VIC
4
85
NA .144 .130 .368
.125
NA
.152
.127
.382
.130
NA
.523
.600
.628
.854
COVID
2
225
NA .341 .374 2.893
.347
NA
.447
.482 4.115
.469
NA
.701
.690
.682
1.143
BICYCLE
4
122
NA .352 .317 .310
.300
NA
.413
.439
.440
.467
NA
.695 1.103 1.105
2.124
SHIPS
4
34
NA .155 .168 87.31
.145
NA
.222
.288 109.1
.420
NA
.235
.296
.311
.448
SMOKING
2
36
NA .114 .109 .165
.090
NA
.130
.193
.258
.169
NA
.266
.256
.208
.301"
EFFICIENT IMPLEMENTATION,0.5917030567685589,"Since the projections qt⊥of the selected queries naturally form an orthogonal basis of range Q
240"
EFFICIENT IMPLEMENTATION,0.5938864628820961,"this only requires normalization, which can be done essentially without additional cost. Formally,
241"
EFFICIENT IMPLEMENTATION,0.5960698689956332,"by storing ot = qt⊥/∥qt⊥∥in all boosting rounds t, subsequent projections can be computed
242"
EFFICIENT IMPLEMENTATION,0.5982532751091703,"via q⊥= q −O(OT q) where O = [o1, . . . , ot]. This reduces the computational complexity per
243"
EFFICIENT IMPLEMENTATION,0.6004366812227074,"candidate query to O(tn) without requiring any additional preprocessing.
244"
EFFICIENT IMPLEMENTATION,0.6026200873362445,"While this looks like the optimal complexity for evaluating objogb in isolation, it leads to a prohibitive
245"
EFFICIENT IMPLEMENTATION,0.6048034934497817,"complexity for large n for finding the optimal query in a given round. Specifically, branch-and-bound
246"
EFFICIENT IMPLEMENTATION,0.6069868995633187,"with the tight bounding function (2) evaluates O(n) queries per expanded search node and beam
247"
EFFICIENT IMPLEMENTATION,0.6091703056768559,"search even O(d2n). In both cases, using the expression for q⊥above repeatedly results in a quadratic
248"
EFFICIENT IMPLEMENTATION,0.611353711790393,"cost in n per search node. To circumvent this, we have to exploit the structure of candidate evaluations,
249"
EFFICIENT IMPLEMENTATION,0.6135371179039302,"similar to other efficient implementations of rule and tree learning algorithms [18].
250"
EFFICIENT IMPLEMENTATION,0.6157205240174672,"The candidates evaluated per search node of both branch-and-bound and beam search have query
251"
EFFICIENT IMPLEMENTATION,0.6179039301310044,"vectors that are prefixes of an ordered sub-selection of data points, in beam search because optimum
252"
EFFICIENT IMPLEMENTATION,0.6200873362445415,"cut-off values are sought for each of the d input variables, in branch-and-bound because the optimum
253"
EFFICIENT IMPLEMENTATION,0.6222707423580786,"in Eq. (2) is attained or approximated by a prefix with respect to some presorting order. Hence, we
254"
EFFICIENT IMPLEMENTATION,0.6244541484716157,"need to solve the following prefix optimization problem: given an ordered sub-selection of l of the
255"
EFFICIENT IMPLEMENTATION,0.6266375545851528,"n data points, represented by an injective map σ: {1, . . . , l} →{1, . . . , n}, find the optimal prefix
256"
EFFICIENT IMPLEMENTATION,0.62882096069869,"i∗= arg max
i∈{1,...,l}"
EFFICIENT IMPLEMENTATION,0.631004366812227,gT q(i)
EFFICIENT IMPLEMENTATION,0.6331877729257642,"∥q(i)
⊥∥+ ϵ
(6)"
EFFICIENT IMPLEMENTATION,0.6353711790393013,"where q(0) = 0 and q(i) = q(i−1) + eσ(i). The following proof shows how the computational
257"
EFFICIENT IMPLEMENTATION,0.6375545851528385,"complexity for solving (6) can be substantially reduced compared to the direct approach above. It
258"
EFFICIENT IMPLEMENTATION,0.6397379912663755,"uses an incremental computation of projections that works directly on the available orthonormal basis
259"
EFFICIENT IMPLEMENTATION,0.6419213973799127,"vectors o instead of computing matrix-vector products or, even worse, the whole projection matrix.
260"
EFFICIENT IMPLEMENTATION,0.6441048034934498,"Theorem 4.4. Given a gradient vector g ∈Rn, an orthonormal basis o1, . . . , ot ∈Rn of the
261"
EFFICIENT IMPLEMENTATION,0.6462882096069869,"subspace spanned by the queries of the first t rules, and a sub-selection of l candidate points
262"
EFFICIENT IMPLEMENTATION,0.648471615720524,"σ: [l] →[n], the best prefix selection problem (6) can be solved in time O(tn).
263"
EFFICIENT IMPLEMENTATION,0.6506550218340611,"Proof sketch. We write the objective value of prefix i in terms of incrementally computable quantities:
264"
EFFICIENT IMPLEMENTATION,0.6528384279475983,gT q(i)
EFFICIENT IMPLEMENTATION,0.6550218340611353,"∥q(i)
⊥∥+ ϵ
=
gT q(i)"
EFFICIENT IMPLEMENTATION,0.6572052401746725,"∥q(i)∥−∥q(i)
∥∥+ ϵ
=
gT q(i)"
EFFICIENT IMPLEMENTATION,0.6593886462882096,"∥q(i)∥−
qPt
k=1 ∥okoT
k q(i)∥2 + ϵ
."
EFFICIENT IMPLEMENTATION,0.6615720524017468,"In particular, the t sequences of norms ∥okoT
k q(i)∥can be computed in time O(n) via cumulative
265"
EFFICIENT IMPLEMENTATION,0.6637554585152838,"summation of the k-th basis vector elements in the order given by σ:
266"
EFFICIENT IMPLEMENTATION,0.665938864628821,"∥okoT
k q(i)∥= ∥ok∥ i
X"
EFFICIENT IMPLEMENTATION,0.6681222707423581,"j=1
oT
k eσ(j) = i
X"
EFFICIENT IMPLEMENTATION,0.6703056768558951,"j=1
ok,σ(j) 267"
EFFICIENT IMPLEMENTATION,0.6724890829694323,"We close this section with a pseudocode (Alg. 1) that summarizes the main ideas of the orthogonal
268"
EFFICIENT IMPLEMENTATION,0.6746724890829694,"gradient boosting and refer to the literature for details about the beam-search/branch-and-bound step.
269"
EXPERIMENTS,0.6768558951965066,"5
Experiments
270"
EXPERIMENTS,0.6790393013100436,"In this section, we present empirical results comparing the proposed fully corrective orthogonal
271"
EXPERIMENTS,0.6812227074235808,"gradient boosting (FCOGB) to the standard gradient boosting algorithms [8] using greedy optimization
272"
EXPERIMENTS,0.6834061135371179,"of objgb (GB) and objgs (GS), to extreme gradient booosting [3] using branch-and-bound optimisation
273"
EXPERIMENTS,0.6855895196506551,"of objxgb (XGB), and finally to SIRUS [2] as the state-of-the-art generate-and-filter approach.
274"
EXPERIMENTS,0.6877729257641921,"We investigate the risk/complexity trade-off, the affinity to select general rules, as well as the
275"
EXPERIMENTS,0.6899563318777293,"computational complexity. The datasets used are those of Boley et al. [3] augmented by three
276"
EXPERIMENTS,0.6921397379912664,"additional classification datasets from the UCI machine learning repository and, to introduce a novel
277"
EXPERIMENTS,0.6943231441048034,"modelling task to the rule learning literature, five counting regression datasets from public sources.
278"
EXPERIMENTS,0.6965065502183406,"This results in a total of 34 datasets (13 for classification, 16 for regression, and 5 for counting/Poisson
279"
EXPERIMENTS,0.6986899563318777,"regression, see Tab. 1). All algorithms were run five times on all datasets using 5 random 80/20
280"
EXPERIMENTS,0.7008733624454149,"train/test splits to calculate robust estimates of all considered metrics. In all cases, the number of
281"
EXPERIMENTS,0.7030567685589519,"gradient boosting iterations was chosen to produce ensembles with cognitive complexity of at most
282"
EXPERIMENTS,0.7052401746724891,"50. The experiment code and further information about the datasets are available on GitHub[1].
283"
EXPERIMENTS,0.7074235807860262,"Cognitive complexity versus risk
Tab. 1 compares the complexity/risk trade-off of the boosting
284"
EXPERIMENTS,0.7096069868995634,"variants and SIRUS by the normalized risk averaged across all considered cognitive complexity levels
285"
EXPERIMENTS,0.7117903930131004,"(where normalization is performed by the risk of the empty rule ensemble). FCOGB has the smallest
286"
EXPERIMENTS,0.7139737991266376,"training risk for 26 of the 34 datasets, occasionally outperforming the second best algorithm by a
287"
EXPERIMENTS,0.7161572052401747,"wide margin (tic-tac-toe, wine, banknote, insurance, boston, ships, smoking). For the test risk the
288"
EXPERIMENTS,0.7183406113537117,"picture is more ambiguous, however, FCOGB retains a relative majority of datasets won. Performing
289"
EXPERIMENTS,0.7205240174672489,"one-sided paired t-tests at significance level 0.05 (with Bonferroni correction for 8 hypotheses) reveals
290"
EXPERIMENTS,0.722707423580786,"that FCOGB significantly outperforms all other variants with a margin of at least 0.001 average
291"
EXPERIMENTS,0.7248908296943232,"normalized training risk (while there is no significant winner in terms of test risk—likely due to a
292"
EXPERIMENTS,0.7270742358078602,"lack of regularization for larger ensemble sizes).
293"
EXPERIMENTS,0.7292576419213974,"0.0
0.5
1.0
Coverage Rate Gradient Boosting 0.0 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.7314410480349345,Coverage Rate FCOGB
EXPERIMENTS,0.7336244541484717,"0.0
0.5
1.0
Coverage Rate XGBoosting"
EXPERIMENTS,0.7358078602620087,"0.0
0.5
1.0
Coverage Rate Gradient Sum"
EXPERIMENTS,0.7379912663755459,"0
10
20
30
40
50
cognitive complexity 0 100 200 300 400 500 600 700 800 900"
EXPERIMENTS,0.740174672489083,time (seconds)
EXPERIMENTS,0.74235807860262,breast
EXPERIMENTS,0.7445414847161572,"Gradient Sum
Gradient Boosting
XGBoost
Naive FCOGB
FCOGB"
EXPERIMENTS,0.7467248908296943,"0
10
20
30
40
50
cognitive complexity 0 2 4 6 8 10 12"
EXPERIMENTS,0.7489082969432315,diabetes
EXPERIMENTS,0.7510917030567685,"Figure 3: First three: the comparison of the coverage rate of Gradient Boosting, XGBoost, Gradient
Sum and FCOGB. The upper (resp. lower) half of the green line means the coverage rate of FCOGB is
higher (resp. lower) than the other method. Last two: the comparison of the running time of Gradient
boosting, XGBoost and FCOGB for the benchmark datasets breast cancer and diabetes of
generating rule ensembles with cognitive complexity 50."
EXPERIMENTS,0.7532751091703057,"Coverage
To compare the generality of the rules learned by the new objective function in com-
294"
EXPERIMENTS,0.7554585152838428,"parison to the existing ones, we performed an additional experiment where we first used one of the
295"
EXPERIMENTS,0.75764192139738,"previous objective functions to generate rule ensembles with ten rules for all datasets. Then for
296"
EXPERIMENTS,0.759825327510917,"each partial rule ensemble, we applied the orthogonal gradient boosting objective function to find
297"
EXPERIMENTS,0.7620087336244541,"an alternative rule. Importantly, we used branch-and-bound with admissible bounding functions for
298"
EXPERIMENTS,0.7641921397379913,"all the alternative objectives to avoid confounding through sub-optimal greedy solutions. In Fig. 3
299"
EXPERIMENTS,0.7663755458515283,"we compare the relative coverage, i.e., the relative number of selected data points ∥q∥2/n, of the
300"
EXPERIMENTS,0.7685589519650655,"rules discovered by the original algorithms to the ones discovered by FCOGB. The outcome is that
301"
EXPERIMENTS,0.7707423580786026,"81.1% of the FCOGB rules covers more data points than gradient boosting, and similarly 71.3% of
302"
EXPERIMENTS,0.7729257641921398,"its rules cover more data points than those generated by XGBoost. In contrast, only 47.2% of the
303"
EXPERIMENTS,0.7751091703056768,"FCOGB rules cover more datapoint than the ones discovered by gradient sum. These results are in
304"
EXPERIMENTS,0.777292576419214,"alignment with the theoretical expectation in terms of the influence of the coverage on the objective
305"
EXPERIMENTS,0.7794759825327511,"values where gradient sum is completely unaffected, whereas orthogonal gradient boosting has a
306"
EXPERIMENTS,0.7816593886462883,"denominator that tends to grow with coverage albeit less than the one of gradient boosting.
307"
EXPERIMENTS,0.7838427947598253,"Computation time
We also compare the computational cost of generating rule ensembles with
308"
EXPERIMENTS,0.7860262008733624,"cognitive complexity 50 by different algorithms in Tab 1. Comparing the computational cost of
309"
EXPERIMENTS,0.7882096069868996,"FCOGB to XGB, the other algorithm utilizing the more expensive branch-and-bound search, the
310"
EXPERIMENTS,0.7903930131004366,"costs are in the same order of magnitude except for one extreme case (wine) where FCOGB is a
311"
EXPERIMENTS,0.7925764192139738,"factor of 26 slower. Comparing to the two greedy variants, FCOGB is in the same order of magnitude
312"
EXPERIMENTS,0.7947598253275109,"as gradient boosting for most datasets. Unsuprisingly, there are a few examples where greedy search
313"
EXPERIMENTS,0.7969432314410481,"vastly outperforms branch-and-bound, in one case (telco churn) by a factor of around 76. However,
314"
EXPERIMENTS,0.7991266375545851,"overall, the results confirm that branch-and-bound search is a practical algorithm in absolute terms:
315"
EXPERIMENTS,0.8013100436681223,"For 23 benchmark dataset, FCOGB is able to finish training a model of cognitive complexity of 50
316"
EXPERIMENTS,0.8034934497816594,"within one minute. Most of the other experiments run within 15 minutes except one dataset (telco
317"
EXPERIMENTS,0.8056768558951966,"churn) which require longer running time. Finally, Fig. 3 shows the detailed computation time of
318"
EXPERIMENTS,0.8078602620087336,"all algorithms in terms of cognitive complexity, including the naive implementation of FCOGB for
319"
EXPERIMENTS,0.8100436681222707,"breast cancer and diabetes, which shows that the performance improvement through Thm. 4.4 is
320"
EXPERIMENTS,0.8122270742358079,"critical to bring the computational complexity on par with XGB.
321"
CONCLUSION,0.8144104803493449,"6
Conclusion
322"
CONCLUSION,0.8165938864628821,"The proposed fully corrective orthogonal boosting approach is a worthwhile alternative to previously
323"
CONCLUSION,0.8187772925764192,"published boosting variants for rule learning, especially when targeting a beneficial risk-complexity
324"
CONCLUSION,0.8209606986899564,"trade-off and an overall small number of rules. The present work provided a relatively detailed
325"
CONCLUSION,0.8231441048034934,"theoretical analysis of the newly developed rule objective function. However, some interesting
326"
CONCLUSION,0.8253275109170306,"questions were left open. While the presorting-based approach to the bounding function performs
327"
CONCLUSION,0.8275109170305677,"extremely well in synthetic experiments, a theoretical approximation guarantee for this algorithm has
328"
CONCLUSION,0.8296943231441049,"yet to be derived. Another interesting direction for future work is the extension of the introduced
329"
CONCLUSION,0.8318777292576419,"approximating subspace paradigm to the extreme gradient boosting approach, which, due to the
330"
CONCLUSION,0.834061135371179,"utilization of higher order information, should principally be able to produce even better risk-
331"
CONCLUSION,0.8362445414847162,"complexity trade-offs.
332"
REFERENCES,0.8384279475982532,"References
333"
REFERENCES,0.8406113537117904,"[1] Anonymous. Fully-corrective orthogonal gradient boosting: Code, datasets, and supplementary
334"
REFERENCES,0.8427947598253275,"information. https://anonymous.4open.science/r/FCOGB-BF6D, 2023.
335"
REFERENCES,0.8449781659388647,"[2] C. Bénard, G. Biau, S. Da Veiga, and E. Scornet. Interpretable random forests via rule extraction.
336"
REFERENCES,0.8471615720524017,"In International Conference on Artificial Intelligence and Statistics, pages 937–945. PMLR,
337"
REFERENCES,0.8493449781659389,"2021.
338"
REFERENCES,0.851528384279476,"[3] M. Boley, S. Teshuva, P. L. Bodic, and G. I. Webb. Better short than greedy: Interpretable models
339"
REFERENCES,0.8537117903930131,"through optimal rule boosting. In Proceedings of the 2021 SIAM International Conference on
340"
REFERENCES,0.8558951965065502,"Data Mining (SDM), pages 351–359. SIAM, 2021.
341"
REFERENCES,0.8580786026200873,"[4] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd
342"
REFERENCES,0.8602620087336245,"acm sigkdd international conference on knowledge discovery and data mining, pages 785–794,
343"
REFERENCES,0.8624454148471615,"2016.
344"
REFERENCES,0.8646288209606987,"[5] W. W. Cohen and Y. Singer. A simple, fast, and effective rule learner. AAAI/IAAI, 99(335-342):
345"
REFERENCES,0.8668122270742358,"3, 1999.
346"
REFERENCES,0.868995633187773,"[6] S. Dash, O. Gunluk, and D. Wei. Boolean decision rules via column generation. Advances in
347"
REFERENCES,0.87117903930131,"neural information processing systems, 31, 2018.
348"
REFERENCES,0.8733624454148472,"[7] K. Dembczy´nski, W. Kotłowski, and R. Słowi´nski. Maximum likelihood rule ensembles. In
349"
REFERENCES,0.8755458515283843,"Proceedings of the 25th international conference on Machine learning, pages 224–231, 2008.
350"
REFERENCES,0.8777292576419214,"[8] K. Dembczy´nski, W. Kotłowski, and R. Słowi´nski. Ender: a statistical framework for boosting
351"
REFERENCES,0.8799126637554585,"decision rules. Data Mining and Knowledge Discovery, 21(1):52–90, 2010.
352"
REFERENCES,0.8820960698689956,"[9] J. H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of
353"
REFERENCES,0.8842794759825328,"statistics, pages 1189–1232, 2001.
354"
REFERENCES,0.8864628820960698,"[10] J. H. Friedman and B. E. Popescu. Predictive learning via rule ensembles. The annals of applied
355"
REFERENCES,0.888646288209607,"statistics, pages 916–954, 2008.
356"
REFERENCES,0.8908296943231441,"[11] J. Fürnkranz. Separate-and-conquer rule learning. Artificial Intelligence Review, 13(1):3–54,
357"
REFERENCES,0.8930131004366813,"1999.
358"
REFERENCES,0.8951965065502183,"[12] T. Hastie and R. Tibshirani.
Generalized Additive Models.
Routledge, 1990.
ISBN
359"
REFERENCES,0.8973799126637555,"9780203753781.
360"
REFERENCES,0.8995633187772926,"[13] I. Kumar, C. Scheidegger, S. Venkatasubramanian, and S. Friedler. Shapley residuals: Quantify-
361"
REFERENCES,0.9017467248908297,"ing the limits of the shapley value for explanations. Advances in Neural Information Processing
362"
REFERENCES,0.9039301310043668,"Systems, 34:26598–26608, 2021.
363"
REFERENCES,0.9061135371179039,"[14] H. Lakkaraju, S. H. Bach, and J. Leskovec. Interpretable decision sets: A joint framework for
364"
REFERENCES,0.9082969432314411,"description and prediction. In Proceedings of the 22nd ACM SIGKDD international conference
365"
REFERENCES,0.9104803493449781,"on knowledge discovery and data mining, pages 1675–1684, 2016.
366"
REFERENCES,0.9126637554585153,"[15] F. Lemmerich, M. Atzmueller, and F. Puppe. Fast exhaustive subgroup discovery with numerical
367"
REFERENCES,0.9148471615720524,"target concepts. Data Mining and Knowledge Discovery, 30:711–762, 2016.
368"
REFERENCES,0.9170305676855895,"[16] Y. Lou, R. Caruana, J. Gehrke, and G. Hooker. Accurate intelligible models with pairwise
369"
REFERENCES,0.9192139737991266,"interactions. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge
370"
REFERENCES,0.9213973799126638,"discovery and data mining, pages 623–631, 2013.
371"
REFERENCES,0.9235807860262009,"[17] D. Malioutov and K. Varshney. Exact rule learning via boolean compressed sensing. In
372"
REFERENCES,0.925764192139738,"International conference on machine learning, pages 765–773. PMLR, 2013.
373"
REFERENCES,0.9279475982532751,"[18] M. Mampaey, S. Nijssen, A. Feelders, R. Konijn, and A. Knobbe. Efficient algorithms for finding
374"
REFERENCES,0.9301310043668122,"optimal binary features in numeric and nominal labeled data. Knowledge and Information
375"
REFERENCES,0.9323144104803494,"Systems, 42:465–492, 2015.
376"
REFERENCES,0.9344978165938864,"[19] P. McCullagh and J. A. Nelder. Generalized linear models. Routledge, 2019.
377"
REFERENCES,0.9366812227074236,"[20] S. Morishita and J. Sese. Transversing itemset lattices with statistical metric pruning. In
378"
REFERENCES,0.9388646288209607,"Proceedings of the nineteenth ACM SIGMOD-SIGACT-SIGART symposium on Principles of
379"
REFERENCES,0.9410480349344978,"database systems, pages 226–236, 2000.
380"
REFERENCES,0.9432314410480349,"[21] W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu. Interpretable machine learning:
381"
REFERENCES,0.9454148471615721,"definitions, methods, and applications. arXiv preprint arXiv:1901.04592, 2019.
382"
REFERENCES,0.9475982532751092,"[22] H. M. Proença and M. van Leeuwen. Interpretable multiclass classification by mdl-based rule
383"
REFERENCES,0.9497816593886463,"lists. Information Sciences, 512:1372–1393, 2020.
384"
REFERENCES,0.9519650655021834,"[23] M. T. Ribeiro, S. Singh, and C. Guestrin. "" why should i trust you?"" explaining the predictions of
385"
REFERENCES,0.9541484716157205,"any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge
386"
REFERENCES,0.9563318777292577,"discovery and data mining, pages 1135–1144, 2016.
387"
REFERENCES,0.9585152838427947,"[24] C. Rudin. Stop explaining black box machine learning models for high stakes decisions and use
388"
REFERENCES,0.9606986899563319,"interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019.
389"
REFERENCES,0.962882096069869,"[25] R. E. Schapire and Y. Freund. Boosting: Foundations and algorithms. Kybernetes, 42(1):
390"
REFERENCES,0.9650655021834061,"164–166, 2013.
391"
REFERENCES,0.9672489082969432,"[26] S. Shalev-Shwartz, N. Srebro, and T. Zhang. Trading accuracy for sparsity in optimization
392"
REFERENCES,0.9694323144104804,"problems with sparsity constraints. SIAM Journal on Optimization, 20(6):2807–2832, 2010.
393"
REFERENCES,0.9716157205240175,"[27] C. Shen and H. Li. On the dual formulation of boosting algorithms. IEEE Transactions on
394"
REFERENCES,0.9737991266375546,"Pattern Analysis and Machine Intelligence, 32(12):2216–2231, 2010.
395"
REFERENCES,0.9759825327510917,"[28] E. Strumbelj and I. Kononenko. An efficient explanation of individual classifications using
396"
REFERENCES,0.9781659388646288,"game theory. The Journal of Machine Learning Research, 11:1–18, 2010.
397"
REFERENCES,0.980349344978166,"[29] T. Wang, C. Rudin, F. Doshi-Velez, Y. Liu, E. Klampfl, and P. MacNeille. A bayesian framework
398"
REFERENCES,0.982532751091703,"for learning rule sets for interpretable classification. The Journal of Machine Learning Research,
399"
REFERENCES,0.9847161572052402,"18(1):2357–2393, 2017.
400"
REFERENCES,0.9868995633187773,"[30] D. Wei, S. Dash, T. Gao, and O. Gunluk. Generalized linear rule models. In International
401"
REFERENCES,0.9890829694323144,"Conference on Machine Learning, pages 6687–6696. PMLR, 2019.
402"
REFERENCES,0.9912663755458515,"[31] H. Yang, C. Rudin, and M. Seltzer. Scalable bayesian rule lists. In International conference on
403"
REFERENCES,0.9934497816593887,"machine learning, pages 3921–3930. PMLR, 2017.
404"
REFERENCES,0.9956331877729258,"[32] G. Zhang and A. Gionis.
Diverse rule sets.
In Proceedings of the 26th ACM SIGKDD
405"
REFERENCES,0.9978165938864629,"International Conference on Knowledge Discovery & Data Mining, pages 1532–1541, 2020.
406"
