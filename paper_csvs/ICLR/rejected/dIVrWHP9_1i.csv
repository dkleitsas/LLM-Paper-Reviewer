Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0033003300330033004,"This work develops mixup to graph data. Mixup has shown superiority in improv-
ing the generalization and robustness of neural networks by interpolating features
and labels of random two samples. Traditionally, Mixup can operate on regu-
lar, grid-like, and Euclidean data such as image or tabular data. However, it is
challenging to directly adopt Mixup to augment graph data because two graphs
typically: 1) have different numbers of nodes; 2) are not readily aligned; and
3) have unique topologies in non-Euclidean space. To this end, we propose G-
Mixup to augment graphs for graph classiﬁcation by interpolating the generator
(i.e., graphon) of different classes of graphs. Speciﬁcally, we ﬁrst use graphs
within the same class to estimate a graphon. Then, instead of directly manipu-
lating graphs, we interpolate graphons of different classes in the Euclidean space
to get mixed graphons, where the synthetic graphs are generated through sam-
pling based on the mixed graphons. Extensive experiments show that G-Mixup
substantially improves the generalization and robustness of GNNs."
INTRODUCTION,0.006600660066006601,"1
INTRODUCTION"
INTRODUCTION,0.009900990099009901,"Recently deep learning has been widely adopted to graph analysis.
Graph Neural Networks
(GNNs) (Wu et al., 2020; Zhou et al., 2020b; Zhang et al., 2020; Xu et al., 2018) have made many
signiﬁcant breakthroughs on graph classiﬁcation.
Meanwhile, data augmentation (e.g., DropE-
dge (Rong et al., 2020), Subgraph (You et al., 2020) ) has also been adopted to graph analysis by
generating synthetic graphs to create more training data for improving the generalization of graph
classiﬁcation models. However, existing graph data augmentation strategies typically aim to aug-
ment graphs at a within-graph level by either modifying edges or nodes in individual graph, which
limits them to only generating new graphs based on one individual graph. The between-graph aug-
mentation methods (i.e., augmenting graphs between graphs) are still under-explored."
INTRODUCTION,0.013201320132013201,"In parallel with the development of graph neural networks, Mixup (Zhang et al., 2017) and its vari-
ants (e.g., Manifold Mixup (Verma et al., 2019)), as data augmentation methods, have been theoret-
ically and empirically shown to improve the generalization and robustness of deep neural networks
in image recognition (Zhang et al., 2017; Verma et al., 2019; Zhang et al., 2021) and natural lan-
guage processing (Guo et al., 2019; Guo, 2020). The basic idea of Mixup is to linearly interpolate
continuous values of random sample pairs to generate more synthetic training data. The formal
mathematical expression of Mixup is xnew = λxi + (1 −λ)xj, ynew = λyi + (1 −λ)yj, where
(xi, yi) and (xj, yj) are two samples drawn at random from training data and the target y are one-
hot labels. With graph neural network and mixup in mind, the following question naturally arises,"
INTRODUCTION,0.0165016501650165,Can we mix up graph data to improve the generalization and robustness of GNNs?
INTRODUCTION,0.019801980198019802,"It remains an open and challenging problem to mix up the graph data due to the characteristics
of graphs and the requirements of applying Mixup. Typically, Mixup requires that original data in-
stances are regular and well-aligned in Euclidean space, such as image data and table data. However,
graph data is distinctly different from image data due to the following characteristics: (i) graph data
is irregular. The number of nodes in different graphs are typically different to others; (ii) graph
data is not well-aligned. The nodes in different graphs can not be aligned well directly; (iii) graph
topology between classes are divergent. The topologies of a pair of graphs from different classes are
usually different while the topologies of those from the same class are usually similar. thus make it
challenging to directly adopt the Mixup strategy to graph data."
INTRODUCTION,0.0231023102310231,Under review as a conference paper at ICLR 2022 WI WH WG
INTRODUCTION,0.026402640264026403,"I = {I1, I2, · · · , Ik} with label (0.5, 0.5)"
INTRODUCTION,0.0297029702970297,"H = {H1, H2, · · · , Hk} with label (0, 1)"
INTRODUCTION,0.033003300330033,"G = {G1, G2, · · · , Gk} with label (1, 0) … … …"
INTRODUCTION,0.036303630363036306,"1) graphon
estimation"
INTRODUCTION,0.039603960396039604,"3) graph
sampling"
INTRODUCTION,0.0429042904290429,= 0.5 ⇤WG + 0.5 ⇤WH
INTRODUCTION,0.0462046204620462,"1) graphon
estimation"
INTRODUCTION,0.04950495049504951,2) graphon mixup
INTRODUCTION,0.052805280528052806,"Figure 1: An overview of G-Mixup. The task is binary graph classiﬁcation. We have two classes of
graphs G and H with different topologies (G has two communities while H has eight communities).
G and H have different graphons. We mix up the graphons WG and WH to obtain a mixed graphon
WI, and then sample new graphs from the mixed graphon. Intuitively, the synthetic graphs have two
major communities and each of which has four sub-communities, demonstrating that the generated
graphs preserve the structure of original graphs from both classes."
INTRODUCTION,0.056105610561056105,"To tackle the aforementioned problems, we propose G-Mixup, a class-level graph data augmenta-
tion method, to mix up graph data based on graphons. The graphs within one class have the same
generator (i.e., graphon). We mix up the graphons of different classes and then generate synthetic
graphs. Informally, a graphon can be thought of as a probability matrix (e.g., the matrix WG and
WH in Figure 1), where W(i, j) represents the probability of edge between node i and j. The real-
world graphs can be regraded as generated from graphons. In this way, we can mix two classes of
graphs by mixing their generators. Since the graphons of different graphs is regular, well-aligned,
and is deﬁned in Euclidean space, it is easy and natural to mix up the graphons and then gener-
ate the synthetic graphs therefrom. We also provide theoretical analysis of graphons mixup, which
guarantees that the generated graphs will preserve the key characteristics of both original classes.
Our proposed method is illustrated in Figure 1 with an example. Given two graph training sets
G = {G1, G2, · · · , Gk} and H = {H1, H2, · · · , Hk} with different labels and distinct topologies
(i.e., G has two communities while H has eight communities), we estimate graphons WG and WH
respectively from G and H. We then mix up the two graphons and obtain a mixed graphon WI.
After that, we sample synthetic graphs from WI as additional training graphs. The generated syn-
thetic graphs have two major communities and each of them have four sub-communities, which is a
mixture of the two graph sets. It thus shows that G-Mixup is capable of mixing up graphs."
INTRODUCTION,0.0594059405940594,"In summary, our main contributions are three-fold. Firstly, we propose G-Mixup to augment the
training graphs for graph classiﬁcation. Since directly mixing up graphs is intractable, G-Mixup
mixes the graphons of different classes of graphs to generate synthetic graphs. Secondly, we the-
oretically prove that the synthetic graph will be the mixture of the original graphs, where the key
topology (i.e., discriminative motif) of source graphs will be preserved. Thirdly, we demonstrate
the efﬁcacy of the proposed G-Mixup on various graph neural network backbones and datasets. Ex-
tensive experimental results show the proposed G-Mixup substantially improves the performance of
graph neural networks in terms of enhancing their generalization and robustness."
PRELIMINARIES,0.0627062706270627,"2
PRELIMINARIES"
PRELIMINARIES,0.066006600660066,"In this section, we ﬁrst go over the notations used in this paper, and then introduce graph related
concepts including graph homomorphism and graphons, which will be used for theoretical analysis
in this work. Finally, we brieﬂy review the graph neural networks for graph classiﬁcation."
NOTATIONS,0.06930693069306931,"2.1
NOTATIONS"
NOTATIONS,0.07260726072607261,"Given a graph G, we use V (G) and E(G) to denote its nodes and edges, respectively. The number
of nodes is v(G) = |V (G)|, and the number of edges is e(G) = |E(G)|. We use G, H, I to denote
graphs and G, H, I to denote graph set. yG ∈RC denotes the label of graph set G, where C is
number of classes of graphs. A graph could contain some interesting and frequent patterns and
subgraphs which are called motifs. The motifs in graph G is denoted as FG. The set of motifs
in graph set G is denoted as FG. WG denotes the graphon of graph set G. W denotes the step"
NOTATIONS,0.07590759075907591,Under review as a conference paper at ICLR 2022
NOTATIONS,0.07920792079207921,"function. Unif[0,1] denotes the uniform distribution between 0 and 1. Bern(·) denotes the Bernoulli
distribution. G(n, W) denotes the random graph with n nodes based on graphon W."
GRAPH HOMOMORPHISM AND GRAPHONS,0.08250825082508251,"2.2
GRAPH HOMOMORPHISM AND GRAPHONS"
GRAPH HOMOMORPHISM AND GRAPHONS,0.0858085808580858,"Graph Homomorphism. A graph homomorphism is an adjacency-preserving mapping between
two graphs, i.e., mapping adjacent vertices in one graph to adjacent vertices in the other. Formally,
a graph homomorphism φ: F →G is a map from V (F) to V (G), where if {u, v} ∈E(F), then
{φ(u), φ(v)} ∈E(G). For two graphs H and G, there could be multiple graph homomorphisms
between them. Let hom(H, G) denotes the total number of graph homomorphisms from graph
H to graph G. For example, hom( , G) = |V (G)| if graph H is
, hom(
, G) = 2|E(G)|
if graph H is
, and hom(
, G) is six times the number of triangles in G. There are in total
|V (G)||V (H)| mappings from H to G, but only some of them are homomorphisms. Thus, we deﬁne
homomorphism density to measure the relative frequency that the graph H appears in graph G:"
GRAPH HOMOMORPHISM AND GRAPHONS,0.0891089108910891,"t(H, G) = hom(H, G)"
GRAPH HOMOMORPHISM AND GRAPHONS,0.0924092409240924,|V (G)||V (H)| .
GRAPH HOMOMORPHISM AND GRAPHONS,0.09570957095709572,"For example, t( , G) = |V (G)|/n1 = 1, t(
, G) = 2|E(G)|/n2."
GRAPH HOMOMORPHISM AND GRAPHONS,0.09900990099009901,"Graphon. A graphon (Airoldi et al., 2013) is a continuous, bounded and symmetric function W :
[0, 1]2 →[0, 1] which may be thought of as the weight matrix of a graph with inﬁnite number of
nodes. Then, given two points ui, uj ∈[0, 1], W(i, j) represents the probability that nodes i and j
be related with an edge. Various quantities of a graph can be calculated as a function of the graphon.
For example, the degree of nodes in graphs can be easily extended to a degree distribution function
in graphons, which is characterized by its graphon marginal dW (x) =
R 1
0 W(x, y)dy. Similarly, the
concept of homomorphism density can be naturally extended from graphs to graphons. Given an
arbitrary graph motif F, its homomorphism density with respect to graphon W is deﬁned by"
GRAPH HOMOMORPHISM AND GRAPHONS,0.10231023102310231,"t(F, W) =
Z"
GRAPH HOMOMORPHISM AND GRAPHONS,0.10561056105610561,"[0,1]V (F ) Y"
GRAPH HOMOMORPHISM AND GRAPHONS,0.10891089108910891,"i,j∈E(F )
W(xi, xj)
Y"
GRAPH HOMOMORPHISM AND GRAPHONS,0.11221122112211221,"i∈V (F )
dxi."
GRAPH HOMOMORPHISM AND GRAPHONS,0.11551155115511551,"For example, the edge density of graphon W is t(
, W) =
R"
GRAPH HOMOMORPHISM AND GRAPHONS,0.1188118811881188,"[0,1]2 W(x, y) dxdy, and the triangle
density of graphon W is t(
, W) =
R"
GRAPH HOMOMORPHISM AND GRAPHONS,0.12211221122112212,"[0,1]3 W(x, y)W(x, z)W(y, z) dxdydz."
GRAPH CLASSIFICATION WITH GRAPH NEURAL NETWORKS,0.1254125412541254,"2.3
GRAPH CLASSIFICATION WITH GRAPH NEURAL NETWORKS"
GRAPH CLASSIFICATION WITH GRAPH NEURAL NETWORKS,0.12871287128712872,"Given a set of graphs, graph classiﬁcation aims to assign a class label for each graph G. Recently,
graph neural networks have become the state-of-the-art approach for graph classiﬁcation. Without
loss of generalization, we present the formal expression of a graph convolution network (GCN) (Kipf
& Welling, 2016). The forward propagation at k-th layer is described as the following:"
GRAPH CLASSIFICATION WITH GRAPH NEURAL NETWORKS,0.132013201320132,"a(k)
i
= AGG(k) n
h(k−1)
j
: j ∈N(i)"
GRAPH CLASSIFICATION WITH GRAPH NEURAL NETWORKS,0.1353135313531353,"o
,
h(k)
i
= COMBINE(k) 
h(k−1)
i
, a(k)
i

,
(1)"
GRAPH CLASSIFICATION WITH GRAPH NEURAL NETWORKS,0.13861386138613863,"where h(k)
i
∈Rn×dk is the intermediate representation of node i at the k-th layer, N(i) denotes the
neighbors of node i. AGG(·) is an aggregation function to collect embedding representations from
neighbors, and COMBINE(·) combines neighbor representation and its representation at (k −1)-th
layer followed by nonlinear transformation. For graph classiﬁcation, a graph-level representation is
obtained by summarizing all node-level representations in the graph by a readout function:"
GRAPH CLASSIFICATION WITH GRAPH NEURAL NETWORKS,0.1419141914191419,"hG = READOUT
n
h(k)
i
: i ∈E(G)
o
,
ˆy = softmax(hG),
(2)"
GRAPH CLASSIFICATION WITH GRAPH NEURAL NETWORKS,0.14521452145214522,"where READOUT(·) is the readout function, which can be a simple function such as average or
sophisticated pooling function (Gao & Ji, 2019; Ying et al., 2018), hG is the representation of graph
G, and ˆy ∈RC is the output estimating the probability that G belongs to each of the C classes."
METHODOLOGY,0.1485148514851485,"3
METHODOLOGY"
METHODOLOGY,0.15181518151815182,"In this section, we ﬁrst formally introduce the proposed G-Mixup, and then present theoretical analy-
sis of graph generation via graphon interpolation from the graph homomorphism density perspective."
METHODOLOGY,0.1551155115511551,Under review as a conference paper at ICLR 2022
G -MIXUP,0.15841584158415842,"3.1
G -MIXUP"
G -MIXUP,0.1617161716171617,"Different from interpolation of image data in Euclidean space, adopting Mixup to graph data is
nontrivial since graphs are irregular, unaligned and non-Euclidean data. In this work, we show that
this challenge could be tackled via graphon theories. By intuition, graphon can be thought of as a
generator to generate graphs. The real-world graphs can be regraded as generated form a graphon,
which has same homomorphism density of arbitrary motif to that of graphon. With this in mind, we
propose G-Mixup, a class-level data augmentation via graphon interpolation. G-Mixup interpolates
different graph generator to obtain a new generator. Then, synthetic graphs are sampled based on
the mixed graphon for graph data augmentation. The graphs sampled from this generator partially
possess properties of all the original graphs. Formally, G-Mixup can be formulated as the following:
Graphon Estimation:
G →WG, H →WH,
(3)
Graphon Mixup:
WI = λWG + (1 −λ)WH,
(4)"
G -MIXUP,0.16501650165016502,"Graph Generation:
{I1, I2, · · · , Ik}
i.i.d
∼G(k, WI),
(5)
Label Mixup:
yI = λyG + (1 −λ)yH,
(6)"
G -MIXUP,0.16831683168316833,"where WG, WH are graphons of the graph set G and H. The mixed graphon is denoted by WI, and
λ ∈[0, 1] is the trade-off hyperparameter to control the contributions from different original graphs
for interpolation. The set of synthetic graphs generated from WI is I = {I1, I2, · · · , Ik}. The
yG ∈RC and yH ∈RC are vectors containing ground-truth labels for graph G and H respectively.
The label vector of a generated graph in I is set as yI ∈RC, where C is the total classes of graphs."
G -MIXUP,0.1716171617161716,"As illustrated in Figure 1 and the above equations, the proposed G-Mixup includes three key steps:
i) estimate the graphon for each class of graphs with the same label, ii) mix up the graphons of
different classes of graphs, and iii) generate the synthetic graphs based on the mixed graphon.
Speciﬁcally, we have G = {G1, G2, · · · , Gk} with label yG, and H = {H1, H2, · · · , Hk} with
label yH. Graphons WG and WH are estimated from graph set G and H, then we mix them up
though linearly interpolating two graphons and their training labels and obtain WI and yI. The
synthetic graph set I is sampled based on WI, which will be used as additional training graphs."
IMPLEMENTATION,0.17491749174917492,"3.2
IMPLEMENTATION"
IMPLEMENTATION,0.1782178217821782,"In this section, we introduce the details of the implementation of estimating graphon from the ob-
served graphs and generating synthetic graphs in the real-world scenario."
IMPLEMENTATION,0.18151815181518152,"Graphon Estimation and Mixup. Estimating graphons from observed graphs is a prerequisite
for G-Mixup, however, it is intractable because graphon is an unknown function without closed-
form solution in real-world graphs. Therefore, we use step function (Lov´asz, 2012; Xu et al.,
2021) as an approximation of graphon. 1 The step function estimation methods are well-studied,
which ﬁrst aligns the nodes of a series of graphs based on simple node measurements (e.g., de-
gree) and then estimates the step function from all the aligned adjacency matrices.
The step
function estimation methods used includes sorting-and-smoothing (SAS) method (Chan & Airoldi,
2014), stochastic block approximation (SBA) (Airoldi et al., 2013), “largest gap” (LG) (Channarond
et al., 2012), matrix completion (MC) (Keshavan et al., 2010), universal singular value threshold-
ing (USVT) (Chatterjee et al., 2015). Formally, a step function WP : [0, 1]2 7→[0, 1] is deﬁne as"
IMPLEMENTATION,0.1848184818481848,"WP (x, y) =
XK"
IMPLEMENTATION,0.18811881188118812,"k,k′=1 wkk′1Pk×Pk′(x, y), where P = (P1, .., PK) denotes the partition of [0, 1]"
IMPLEMENTATION,0.19141914191419143,"into K adjacent intervals of length 1/K, wkk′ ∈[0, 1], and indicator function 1Pk×Pk′(x, y) equals
to 1 if (x, y) ∈Pk ×Pk′ and otherwise it is 0. Essentially, the step function can be seen as a matrix
W = [wkk′] ∈[0, 1]K×K, where Wij is the probability of edge between node i and j. In practice,
we use the matrix-form step function as graphon to mix up and generation synthetic graphs."
IMPLEMENTATION,0.19471947194719472,"For binary classiﬁcation, we have G = {G1, G2, · · · , Gk} and H = {H1, H2, · · · , Hk} with dif-
ferent labels, we estimate their step functions WG ∈RK×K and WH ∈RK×K We let K be the
average number of nodes in all graphs. For multi-class classiﬁcation, we ﬁrst estimate the step func-
tion for each class of graphs and then randomly select two to perform mix-up. The resultant step
function is WI = λWG +(1−λ)WH ∈RK×K, which serves as the generator of synthetic graphs."
IMPLEMENTATION,0.19801980198019803,"1Because weak regularity lemma of graphon (Frieze & Kannan, 1999) indicates that an arbitrary graphon
can be approximated well by step function. Detailed discussion is in Appendix A.4."
IMPLEMENTATION,0.20132013201320131,Under review as a conference paper at ICLR 2022
IMPLEMENTATION,0.20462046204620463,"Synthetic Graphs Generation. A graphon W provides a distribution to generating arbitrarily sized
graphs. Speciﬁcally, a k-node random graph G(k, WI) can be generated following the process:"
IMPLEMENTATION,0.2079207920792079,"u1, . . . , uk
iid∼Unif[0,1],
(G(k, W)ij|u1, . . . , uk)
iid∼Bern(W(ui, uj)), ∀i, j ∈[k].
(7)"
IMPLEMENTATION,0.21122112211221122,"Since we only estimate the step function W to approximate the graph W, we set W(ui, uj) =
WI[⌊1/ui⌋, ⌊1/uj⌋], and ⌊·⌋is the ﬂoor function. The ﬁrst step samples K nodes independently
from a uniform distribution on [0, 1]. The second step generates an adjacency matrix A = [aij] ∈
{0, 1}K×K, whose element values follow the Bernoulli distributions determined by the step func-
tion. A graph is thus obtained as G where V (G) = {1, ..., K} and E(G) = {(i, j) | aij = 1}.
A set of synthetic graphs can be generated by conducting the above process independently multiple
times. For node features, we generate them of synthetic graphs based the original two sets of graphs.
Speciﬁcally, we generate the node feature of each graphons. In the graphon estimation, we align the
node features with the process of the adjacency matrix. For each graphon we have a set of node
features, we can pooling the node features to obtain the graphon features. The the node features
would inherit form the graphon features."
IMPLEMENTATION,0.2145214521452145,"Table 1: Computational com-
plexity of graphon estima-
tion (Xu et al., 2021)"
IMPLEMENTATION,0.21782178217821782,"Method
Complexity"
IMPLEMENTATION,0.22112211221122113,"MC
O(N3)
USVT
O(N3)
LG
O(MN2)
SBA
O(MKN log N)
SAS
O(MN log N + K2 log K2)"
IMPLEMENTATION,0.22442244224422442,"Computational Complexity Analysis. We now discuss computa-
tional complexity of the proposed G-Mixup. The major additional
computation costs come from graphon estimation and graph gen-
eration. For graphon estimation, suppose we have M graphs and
each of them has N nodes and E edges, and estimate step function
with K partitions to approximate a graphon, the complexity of used
graphon estimation methods (Xu et al., 2021) is in Table 1. For
graph generation, suppose we need to generate K graphs with N
nodes, the computational complexity is O(KN) for node genera-
tion and O(KN 2) for edge generation, so the overall complexity of graph generation is O(KN 2)."
THEORETICAL JUSTIFICATION,0.22772277227722773,"4
THEORETICAL JUSTIFICATION"
THEORETICAL JUSTIFICATION,0.23102310231023102,"In the following, we will theoretically prove that, the synthetic graphs generated by G-Mixup will
be a mixture of original graphs. We ﬁrst deﬁne the discriminative motif, and then we justify the
graphon mixup operation (Equation 4) and graph generation operation (Equation 5) by analysing
the homomorphism density of discriminative motifs of the original graphs and the synthetic graphs."
THEORETICAL JUSTIFICATION,0.23432343234323433,"Deﬁnition 1 (Discriminative Motif) A discriminative motif FG of graph G is the subgraph, with
the minimal number of nodes and edges, that can decide the class the graph G. Discriminative
motifs FG is the set of the discriminative motif of every graph in the graph set G."
THEORETICAL JUSTIFICATION,0.2376237623762376,"Intuitively, the discriminative motif is the key topology of a graph, by which the graph can be
distinguished. We assume that (i) every graph G has a discriminative motif FG , and (ii) a set
of graphs G has a ﬁnite set of discriminative motifs FG. The goal of graph classiﬁcation is to
ﬁlter out structural noise in graphs (Fox & Rajamanickam, 2019) and recognize the key typologies
(discriminative motifs) to predict the class label. For example, benzene (a compound in chemistry)
is distinguished by the discriminative motif
(benzene ring)."
THEORETICAL JUSTIFICATION,0.24092409240924093,"4.1
WILL DISCRIMINATIVE MOTIFS FG AND FH EXIST IN λWG + (1 −λ)WH ?"
THEORETICAL JUSTIFICATION,0.24422442244224424,"We answer this question by exploring the difference in homomorphism density of discriminative
motifs between the original graphon and mixed graphon. We propose the following theorem,"
THEORETICAL JUSTIFICATION,0.24752475247524752,"Theorem 1 Given two sets of graphs G and H, the corresponding graphons are WG and WH, and
the corresponding discriminative motif set FG and FH. For every discriminative motif FG ∈FG
and FH ∈FH, the difference between the homomorphism density of FG/FH in the mixed graphon
WI = λWG + (1 −λ)WH and that of the graphon WH/WG is upper bounded by
|t(FG, WI) −t(FG, WG)| ≤(1 −λ)e(FG)||WH −WG||□,
|t(FH, WI) −t(FH, WH)| ≤λe(FH)||WH −WG||□
(8)"
THEORETICAL JUSTIFICATION,0.2508250825082508,where e(F) is the number of nodes in graph F and ||WH −WG||□is the cut norm 2.
THEORETICAL JUSTIFICATION,0.25412541254125415,2Details about cut norm are in Appendic A.1
THEORETICAL JUSTIFICATION,0.25742574257425743,Under review as a conference paper at ICLR 2022
THEORETICAL JUSTIFICATION,0.2607260726072607,"Proof Sketch. The proof follows the derivation of Counting Lemma for Graphons (Lemma 10.23
in Lov´asz (2012)), which relates the homomorphism density and the cut distance ||WH −WG||
of graphons. Speciﬁcally, we take the two graphons in this Lemma to deduce the bound of the
difference of homomorphism densities of WI and WG/WH. Detailed proof are in Appendix A.2. ■"
THEORETICAL JUSTIFICATION,0.264026402640264,"Theorem 1 suggests that the difference in the homomorphism densities of the mixed graphon and
original graphons is upper bounded. Note that difference depends on the hyperparameter λ, the
edge number e(FG)/e(FH) and the cut norm ||WH −WG||□. Since the e(FG)/e(FH) and the cut
norm ||WH −WG||□are decided by the dataset ( can be regraded as a constant), the difference in
homomorphism densities will be decided by λ. On this basis, the corresponding label of the graphon
is set to λyG + (1−λ)yH. Therefore, G-Mixup can preserve the different discriminative motifs
of the two different graphons into one mixed graphon."
WILL THE GENERATED GRAPHS FROM GRAPHON WI PRESERVE THE MIXTURE OF,0.26732673267326734,"4.2
WILL THE GENERATED GRAPHS FROM GRAPHON WI PRESERVE THE MIXTURE OF
DISCRIMINATIVE MOTIFS?"
WILL THE GENERATED GRAPHS FROM GRAPHON WI PRESERVE THE MIXTURE OF,0.2706270627062706,"Ideally, the generated graphs should inherit the homomorphism density of discriminative motifs
from the graphon. To verify this, we propose the following theorem."
WILL THE GENERATED GRAPHS FROM GRAPHON WI PRESERVE THE MIXTURE OF,0.2739273927392739,"Theorem 2 Let WI be the mixed graphon , n ≥1, 0 < ε < 1, and let FI be the mixed discrimina-
tive motif, then the WI-random graph G = G(n, WI) satisﬁes"
WILL THE GENERATED GRAPHS FROM GRAPHON WI PRESERVE THE MIXTURE OF,0.27722772277227725,"P (|t(FI, G) −t(FI, WI)| > ε) ≤2exp

−
ε2n
8v(FI)2"
WILL THE GENERATED GRAPHS FROM GRAPHON WI PRESERVE THE MIXTURE OF,0.28052805280528054,"
.
(9)"
WILL THE GENERATED GRAPHS FROM GRAPHON WI PRESERVE THE MIXTURE OF,0.2838283828382838,"Theorem 2 states that for any nonzero speciﬁed margin ε, no matter how small, with a sufﬁciently
large samples of graph from mixed graphon, the homomorphism density of discriminative motif
in synthetic graphs will approximately equal to that in graphon t(FI, G) ≈t(FI, WI) with high
probability. In other words, it reads the synthetic graphs will preserve the discriminative motif
of the mixed graphon with a very high probability if the sample number n is large enough. The
detailed proof is presented in Appendix A.3. Therefore, G-Mixup can preserve the different
discriminative motifs of the two different graphs into one mixed graph."
DISCUSSION,0.2871287128712871,"4.3
DISCUSSION"
DISCUSSION,0.29042904290429045,"This section discusses the differences and relations between G-Mixup and other graph augmentation
strategies, such as DropEdge (Rong et al., 2020), and Manifold Mixup (Wang et al., 2021)."
DISCUSSION,0.29372937293729373,"Relation to Edge Perturbation Method . Edge perturbation methods is to randomly perturb the
edges to improve the GNNs, inlcuding DropEdge (Rong et al., 2020), Graphon-based edge pertur-
bation (Hu et al., 2021). DropEdge drops graph edges independently with a speciﬁed probability,
aiming to prevent over-smoothing and over-ﬁtting issues in GNNs. Graphon-based edge perturba-
tion (Rong et al., 2020) improves the Dropedge by dropping edge based on an estimated probability.
One of the limitations of such methods is that the edge permutation is based on the one individual
graph, the graphs will not mix up. Edge perturbation methods will not mix the two discriminative
motifs together and only randomly add remove some edges while the graphon will add some edges
based on the graphon. DropEdge and Graphon-based edge perturbation (Hu et al., 2021) are spe-
cial cases of G-Mixup while setting different hyperparameter λ. i) G-Mixup will degenerate into
Graphon-based edge perturbation, while λ = 0 in Equation 4. The mathematical expression is
WI = WH, {I1, I2, · · · , Ik}
i.i.d
∼G(k, WI), yI = yH. ii) G-Mixup will degenerate into DropEdge,
while setting λ = 0 and masking graphons with adjacency matrix A in Equation 4. The expression is
WI = A⊙WH, {I1, I2, · · · , Ik}
i.i.d
∼G(k, WI), yI = yH, where ⊙is element-wise multiplication."
DISCUSSION,0.297029702970297,"Relation to Manifold Mixup for Graph. The methods proposed by Wang et al. (2021) is to adopt
Manifold Mixup to graph data, which mix up graphs representation in the embedding space. The
Manifold Mixup is to stabilize the model training by interpolating hidden representation. Interpo-
lating hidden representation limits its applications by that 1) learning algorithms must have hidden
representation of graphs and 2) models must be modiﬁed to adapt Manifold Mixup. In contrast, G-
Mixup is capable of generating synthetic graphs without modifying models. As a data augmentation
method, our proposal has broader applications, e.g., creating graphs for graph contrastive learning."
DISCUSSION,0.30033003300330036,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.30363036303630364,"5
EXPERIMENTS"
EXPERIMENTS,0.3069306930693069,"We evaluate the performance of the proposed G-Mixup in this section. First, we visualize graphons
and graph generation results to investigate what G-Mixup actually do on real-world datasets in Sec-
tion 5.1 and Section 5.2. Then, we evaluate the effectiveness of G-Mixup in graph classiﬁcation
with various datasets and GNN backbones in Section 5.3, as well as how it improve the robustness
of GNNs against label corruption and adversarial examples in Section 5.4."
EXPERIMENTS,0.3102310231023102,"5.1
DOES DIFFERENT CLASSES OF REAL-WORLD GRAPHS HAVE DIFFERENT GRAPHONS?"
EXPERIMENTS,0.31353135313531355,"We visualize the estimated graphon in Figure 2 to examine whether there are different graphons for
different classes of graphs. Observation 1: different classes of graphs have different graphons in
real-world dataset. As shown in Figure 2, the graphons of different class of graphs in one dataset
are distinctly different. The graphons of IMDB-BINAERY in Figure 2 shows that the graphon
of class 1 has larger dense area, which indicates that the graphs in this class have a more large
communities than the graphs of class 0. The graphons of REDDIT-BINARY in Figure 2 shows that
graphs of class 0 have one high-degree nodes while the graphs of class 1 have two. This observation
validates that real-world graphs of different classes have distinctly different graphons, which lays a
solid foundation for generating the mixture of graphs by mixing up graphons."
EXPERIMENTS,0.31683168316831684,IMDB-BINARY
EXPERIMENTS,0.3201320132013201,"Class 0
Class 1"
EXPERIMENTS,0.3234323432343234,REDDIT-BINARY
EXPERIMENTS,0.32673267326732675,"Class 0
Class 1"
EXPERIMENTS,0.33003300330033003,IMDB-MULTI
EXPERIMENTS,0.3333333333333333,"Class 0
Class 1
Class 2"
EXPERIMENTS,0.33663366336633666,"Figure 2: Estimated graphons on IMDB-BINARY, REDDIT-BINARY, and IMDB-MULTI. Obvi-
ously, graphons of different classes of graphs are quiet different. This observation validates the
divergence of graphons between different classes of graphs, which is the basis of the G-Mixup. The
graphons are estimated by LG. More estimated graphons via various methods are in Appendix B.3."
EXPERIMENTS,0.33993399339933994,"5.2
WHAT IS G-MIXUP DOING? A CASE STUDY"
EXPERIMENTS,0.3432343234323432,"To investigate the outcome of G-Mixup in real-world scenarios, we visualize the generated synthetic
graphs in REDDIT-BINARY dataset in Figure 3. Observation 2: synthetic graphs are indeed the
mixture of the original graphs. Original graphs and the generated synthetic graphs are visualized
in Figure 3(a)(b) and Figure 3(c)(d)(e), respectively. Figure 3 demonstrates that mixed graphon
0.5∗WG +0.5∗WH is able to generate graphs with a high-degree node and a dense subgraph, which
can be regarded as the mixture of graphs with one high-degree node and two high-degree nodes. It
validates that G-Mixup prefer to preserve the discriminative motifs from the original graphs."
EXPERIMENTS,0.3465346534653465,original graphs W0
EXPERIMENTS,0.34983498349834985,"(a) graphs of class 0 and the graphon W0
(b) graphs of class 1 and the graphon W1 W1"
EXPERIMENTS,0.35313531353135313,generated graphs
EXPERIMENTS,0.3564356435643564,"(c) graphs generated from 1 ⇤W0 + 0 ⇤W1
(d) graphs generated from 0 ⇤W0 + 1 ⇤W1
(e) graphs generated from 0.5 ⇤W0 + 0.5 ⇤W1"
EXPERIMENTS,0.35973597359735976,"Figure 3: The visualization of generated synthetic graphs on dataset REDDIT-BINARY. The ﬁrst
row is the original graphs in the dataset while the second row is the generated graphs through the
proposed G-Mixup. The graphs in (a) and (b) are the original graphs of class 0 and class 1. The
distinct difference of these two classes of graphs is that graphs of class 0 have one high-degree node
while graphs of class 1 have two ( marked with
in (a) and (b) ). (c)/(d) shows graphs generated
with the mixed graphon (1 ∗W0 + 0 ∗W1) / (0 ∗W0 + 1 ∗W1), which have one/two high-degree
node/nodes (marked with
in (c) and (d)) because the mixed graphon only contains W0/W1. The
synthetic graphs generated from (0.5 ∗W0 + 0.5 ∗W1) is the mixture of graphs of class 0 and
class 1, which appears as a high-degree node and a dense subgraph ( marked with
and
in (e),
respectively). The visualization shows that synthetic graphs are the mixture of the original graphs."
EXPERIMENTS,0.36303630363036304,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.36633663366336633,"IMDB-BINARY
IMDB-MULTI
REDDIT-BINARY
REDDIT-MULTI-5K"
EXPERIMENTS,0.3696369636963696,"Epoch
Epoch
Epoch
Epoch"
EXPERIMENTS,0.37293729372937295,Cross-entropy Loss
EXPERIMENTS,0.37623762376237624,"Figure 4:
The training/validation/test curves on IMDB-BINARY, IMDB-MULTI, REDDIT-
BINARY and REDDIT-MULTI-5K with GCN as backbone. The curves are depicted on ten runs."
EXPERIMENTS,0.3795379537953795,"Table 2: Performance comparisons of G-Mixup with different graph neural networks on different
dataset. The metric is the classiﬁcation accuracy and its standard deviation. The best performance is
in boldface. Experimental settings are in Appendix B.1. Experiments with more backbones (Diff-
Pool, MincutPool, GMT) and molecular property prediction are in Appendix C.1 and Appendix C.2."
EXPERIMENTS,0.38283828382838286,"Dataset
IMDB-B
IMDB-M
REDDIT-B
REDD-M5k
REDD-M12k"
EXPERIMENTS,0.38613861386138615,"#graphs
1000
1500
2000
4999
11929
#classes
2
3
2
5
11
#avg.nodes
19.77
13.00
429.63
508.52
391.41
#avg.edges
96.53
65.94
497.75
594.87
456.89"
EXPERIMENTS,0.38943894389438943,"GCN
vanilla
72.18±1.55
48.79±2.72
78.82±1.33
45.07±1.70
46.90±0.73
w/ Dropedge
72.50±0.31
49.08±1.89
81.25±8.15
51.35±1.54
47.08±0.55
w/ NodeDropping
72.00±4.09
48.58±2.85
79.25±0.35
49.35±1.80
47.93±0.64
w/ Subgraph
68.50±4.76
49.58±2.61
74.33±2.88
48.70±1.63
47.49±0.93
w/ ManfoldMixup 72.83±1.75
49.50±1.97
75.75±4.53
49.82±0.85
46.92±1.05
w/ G-Mixup
72.87±3.85
51.30±2.14
89.81±0.74
51.51±1.70
48.06±0.53"
EXPERIMENTS,0.3927392739273927,"GIN
vanilla
71.55±3.53
48.83±2.75
92.59±0.86
55.19±1.02
50.23±0.83
w/ Dropedge
72.20±1.82
48.83±3.02
92.00±1.13
55.10±0.44
49.77±0.76
w/ NodeDropping
72.16±0.28
48.33±0.98
90.25±0.98
53.26±4.99
49.95±1.70
w/ Subgraph
68.50±0.86
47.25±3.78
90.33±0.87
54.60±3.15
49.67±0.90
w/ ManfoldMixup 70.83±1.04
49.88±1.34
90.75±1.78
54.95±0.86
49.81±0.80
w/ G-Mixup
71.94±3.00
50.46±1.49
92.90±0.87
55.49±0.53
50.50±0.41"
EXPERIMENTS,0.39603960396039606,"TopKPool vanilla
72.37±5.01
50.57±1.62
90.30±1.47
45.07±1.70
45.06±1.70
w/ Dropedge
71.75±2.18
48.75±2.94
88.96±1.90
47.43±1.82
44.56±1.41
w/ NodeDropping
69.16±1.04
48.50±2.50
81.33±4.48
46.15±2.28
44.49±1.15
w/ Subgraph
67.83±4.01
50.83±2.38
86.08±2.12
45.75±2.47
46.18±1.53
w/ ManfoldMixup 71.83±3.03
51.22±1.17
87.58±3.16
45.60±2.35
43.81±0.95
w/ G-Mixup
72.80±3.33
51.30±2.14
90.40±0.89
46.48±1.70
43.72±1.65"
EXPERIMENTS,0.39933993399339934,"5.3
CAN G-MIXUP IMPROVE THE PERFORMANCE AND GENERALIZATION OF GNNS?"
EXPERIMENTS,0.40264026402640263,"To validate the effectiveness of our proposal, we experiment to compare the performance with vari-
ous backbones of GNNs on various datasets, and summarize results in Table 2 as well as the training
curves in Figure 4. Our main observations are: Observation 3: G-Mixup can signiﬁcantly im-
proves the performance of various graph neural networks on various datasets. From Table 2,
our proposal gain 12 best performances among 15 reported accuracies, which substantially improve
the performance of GNNs. Overall, our proposal performs 2.84% better than vanilla model. Note
that G-Mixup and baseline models adopt the same architecture of GNNs (e.g., layers, activation
functions) and the same training hyperparameters (e.g., optimizer, learning rate). Considering both
model performance and experimental setting, the improvement adequately validates the effective-
ness of our proposal. Observation 4: G-Mixup can signiﬁcantly improve the generalization of
various backbones of graph neural networks. From the loss curve on test data (green line) in
Figure 4, the loss of test data of G-Mixup (dashed green lines) are consistently lower than the vanilla
model (solid green lines). Considering both the better performance and the better test loss curves,
our proposal substantially is able to improve the generalization of GNNs. Observation 5: G-Mixup
largely stabilizes the model training. As shown in Table 2, G-Mixup achieves 11 lower standard
deviation among total 15 reported numbers than vanilla model. Additionally, the train/validation/test"
EXPERIMENTS,0.40594059405940597,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.40924092409240925,"curves of G-Mixup (dashed line) in Figure 4 are more stable than vanilla model (solid line). These
all indicate that our proposal G-Mixup is capable of stabilizing the training of graph neural networks."
EXPERIMENTS,0.41254125412541254,"5.4
CAN G -MIXUP VIRTUALLY IMPROVE THE ROBUSTNESS OF GNNS?"
EXPERIMENTS,0.4158415841584158,"We investigate the two kinds of robustness of the proposed G-Mixup, including Label Corruption
Robustness and Topology Corruption Robustness, and report the results in Table 3 and 4, respec-
tively. More experimental settings are presented in Appendix B.2. Observation 6: G-Mixup im-
proves the robustness of graph neural networks. Table 3 shows our proposal gains the better
performance, indicating it is more robust to noisy label than vanilla baseline. Table 4 shows that
G-Mixup is more robust when graph topology is corrupted since the accuracy is consistently better
than baselines. This can be an advantage of G-Mixup when graph labels or topologies are noisy."
EXPERIMENTS,0.41914191419141916,"Table 3: Robustness to label corruption with
different corruption ratio."
EXPERIMENTS,0.42244224422442245,"Models
Methods
10%
20%
30%
40%"
EXPERIMENTS,0.42574257425742573,"IMDB-B
vanilla
72.30±3.67
69.43±4.80
63.65±8.87
55.21±8.75"
EXPERIMENTS,0.429042904290429,"w/ Dropedge
72.00±2.44
69.52±3.25
64.12±3.44
48.50±0.00"
EXPERIMENTS,0.43234323432343236,"w/ ManfoldMixup
71.87±3.56
69.03±4.85
65.62±9.89
48.50±0.00"
EXPERIMENTS,0.43564356435643564,"w/ G-Mixup
72.56±3.08
69.87±5.41
65.50±8.90
52.56±6.97"
EXPERIMENTS,0.4389438943894389,"REDDIT-B
vanilla
73.90±1.43
75.68±2.75
68.12±0.81
46.50±0.00"
EXPERIMENTS,0.44224422442244227,"w/ Dropedge
73.75±1.28
72.06±1.42
46.50±0.00
46.50±0.00"
EXPERIMENTS,0.44554455445544555,"w/ ManfoldMixup
71.96±1.97
76.00±2.24
54.43±1.09
46.50±0.00"
EXPERIMENTS,0.44884488448844884,"w/ G-Mixup
71.94±3.00
76.34±1.49
74.21±1.85
53.50±0.00"
EXPERIMENTS,0.4521452145214521,"Table 4: Robustness to topology corruption with
different corruption ratio."
EXPERIMENTS,0.45544554455445546,"Models
Methods
10%
20%
30%
40%"
EXPERIMENTS,0.45874587458745875,"Removing edges
vanilla
77.96±3.71
67.59±5.73
64.96±8.87
65.71±8.31"
EXPERIMENTS,0.46204620462046203,"w/ Dropedge
74.40±2.26
65.12±3.51
65.93±2.32
57.87±4.14"
EXPERIMENTS,0.46534653465346537,"w/ ManfoldMixup
75.62±1.59
65.81±3.84
59.81±9.45
57.31±3.15"
EXPERIMENTS,0.46864686468646866,"w/ G-Mixup
81.46±3.08
71.12±7.47
67.46±8.90
66.25±7.78"
EXPERIMENTS,0.47194719471947194,"Adding edges
vanilla
76.12±5.73
74.37±6.48
72.31±2.69
72.00±2.92"
EXPERIMENTS,0.4752475247524752,"w/ Dropedge
70.53±1.47
70.18±1.29
71.18±1.53
70.90±1.53"
EXPERIMENTS,0.47854785478547857,"w/ ManfoldMixup
73.41±2.40
71.87±1.28
71.50±2.03
71.21±2.00"
EXPERIMENTS,0.48184818481848185,"w/ G-Mixup
84.31±3.21
82.21±4.31
77.00±2.25
75.56±3.05"
RELATED WORKS,0.48514851485148514,"6
RELATED WORKS"
RELATED WORKS,0.4884488448844885,"Graph Data Augmentation
Graph neural networks (GNNs) also achieve the state-of-the-art per-
formance on graph classiﬁcation task (Kipf & Welling, 2016; Veliˇckovi´c et al., 2017; Hamilton et al.,
2017; Xu et al., 2018; Zhang et al., 2018). In parallel, graph data augmentation methods are also
proposes to improve the performance of GNNs. There are three categories of graph data augmen-
tation, including node perturbation (You et al., 2020), edge perturbation (Rong et al., 2020; You
et al., 2020), and subgraph sampling (You et al., 2020). We have discussed the detailed differences
to the mainstream graph augmentation method in Section 4.3. However, the common limitation of
the existing graph data augmentation methods is that these methods are based on one single graph
while G-Mixup is able to augment new graphs using multiple input graphs. Besides, there are model-
independent graph data augmentation methods (Zhou et al., 2020a; Zhao et al., 2021) and graph data
augmentation method for node classiﬁcation, which is not applicable while our methods is genral
plug-in model-agnostic graph data augmentation methods for graph classiﬁcation."
RELATED WORKS,0.49174917491749176,"Graphon Estimation.
Graphons and convergent graph sequences have been broadly studied in
mathematics (Lov´asz, 2012; Lov´asz & Szegedy, 2006; Borgs et al., 2008) and have been applied to to
network science (Avella-Medina et al., 2018; Vizuete et al., 2021) and graph neural networks (Ruiz
et al., 2020a;b). There are tow lines of works to estimate step functions, one is based on stochastic
block models, such as sorting-and-smoothing (SAS) method (Chan & Airoldi, 2014), stochastic
block approximation (SBA) (Airoldi et al., 2013), “largest gap” (LG) (Channarond et al., 2012);
another one is based on matrix decomposition, such as matrix completion (MC) (Keshavan et al.,
2010), universal singular value thresholding (USVT) (Chatterjee et al., 2015). We estimate the step
function using the above ﬁve methods (Xu et al., 2021) and the estimation are in Appendix B.3."
CONCLUSION,0.49504950495049505,"7
CONCLUSION"
CONCLUSION,0.49834983498349833,"This work develops G-Mixup to augment graph data. Unlike image data, graph data is irregular,
unaligned and in non-Euclidean space, making it hard to be mixed up. However, the graphs within
one class have the same generator (i.e., graphon), which is regular, well-aligned and in Euclidean
space. Thus we turn to mix up the graphons of different classes then generate synthetic graphs.
G-Mixup is a new graph data augmentation algorithm which mix up the input graph to interpolate
the topologies of different classes of graphs. A variety of experiments have shown that graph neural
networks trained with G-Mixup achieve better performance and generalization in terms of model
accuracy and model loss, and improve the robustness to noisy labels and corrupted topologies."
CONCLUSION,0.5016501650165016,Under review as a conference paper at ICLR 2022
REFERENCES,0.504950495049505,REFERENCES
REFERENCES,0.5082508250825083,"Edoardo M Airoldi, Thiago B Costa, and Stanley H Chan. Stochastic blockmodel approximation of
a graphon: Theory and consistent estimation. arXiv preprint arXiv:1311.1731, 2013."
REFERENCES,0.5115511551155115,"Marco Avella-Medina, Francesca Parise, Michael T Schaub, and Santiago Segarra. Centrality mea-
sures for graphons: Accounting for uncertainty in networks. IEEE Transactions on Network
Science and Engineering, 7(1):520–537, 2018."
REFERENCES,0.5148514851485149,"Jinheon Baek, Minki Kang, and Sung Ju Hwang. Accurate learning of graph representations with
graph multiset pooling. In International Conference on Learning Representations, 2020."
REFERENCES,0.5181518151815182,"Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. Spectral clustering with graph neural
networks for graph pooling. In International Conference on Machine Learning, pp. 874–883.
PMLR, 2020."
REFERENCES,0.5214521452145214,"Christian Borgs, Jennifer T Chayes, L´aszl´o Lov´asz, Vera T S´os, and Katalin Vesztergombi. Conver-
gent sequences of dense graphs i: Subgraph frequencies, metric properties and testing. Advances
in Mathematics, 219(6):1801–1851, 2008."
REFERENCES,0.5247524752475248,"Stanley Chan and Edoardo Airoldi. A consistent histogram estimator for exchangeable graph mod-
els. In International Conference on Machine Learning, pp. 208–216, 2014."
REFERENCES,0.528052805280528,"Antoine Channarond, Jean-Jacques Daudin, St´ephane Robin, et al. Classiﬁcation and estimation
in the stochastic blockmodel based on the empirical degrees. Electronic Journal of Statistics, 6:
2574–2601, 2012."
REFERENCES,0.5313531353135313,"Sourav Chatterjee et al. Matrix estimation by universal singular value thresholding. The Annals of
Statistics, 43(1):177–214, 2015."
REFERENCES,0.5346534653465347,"James Fox and Sivasankaran Rajamanickam. How robust are graph neural networks to structural
noise? arXiv preprint arXiv:1912.10206, 2019."
REFERENCES,0.5379537953795379,"Alan Frieze and Ravi Kannan. Quick approximation to matrices and applications. Combinatorica,
19(2):175–220, 1999."
REFERENCES,0.5412541254125413,"Hongyang Gao and Shuiwang Ji. Graph u-nets. arXiv preprint arXiv:1905.05178, 2019."
REFERENCES,0.5445544554455446,"Hongyu Guo.
Nonlinear mixup: Out-of-manifold data augmentation for text classiﬁcation.
In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2020."
REFERENCES,0.5478547854785478,"Hongyu Guo, Yongyi Mao, and Richong Zhang. Augmenting data with mixup for sentence classiﬁ-
cation: An empirical study. arXiv preprint arXiv:1905.08941, 2019."
REFERENCES,0.5511551155115512,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, pp. 1024–1034, 2017."
REFERENCES,0.5544554455445545,"Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Ad-
vances in Neural Information Processing Systems, volume 33, pp. 22118–22133, 2020."
REFERENCES,0.5577557755775577,"Ziqing Hu, Yihao Fang, and Lizhen Lin. Training graph neural networks by graphon estimation,
2021."
REFERENCES,0.5610561056105611,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015."
REFERENCES,0.5643564356435643,"Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few
entries. IEEE transactions on information theory, 56(6):2980–2998, 2010."
REFERENCES,0.5676567656765676,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),
2015."
REFERENCES,0.570957095709571,Under review as a conference paper at ICLR 2022
REFERENCES,0.5742574257425742,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016."
REFERENCES,0.5775577557755776,"L´aszl´o Lov´asz. Large networks and graph limits, volume 60. American Mathematical Soc., 2012."
REFERENCES,0.5808580858085809,"L´aszl´o Lov´asz and Bal´azs Szegedy. Limits of dense graph sequences. Journal of Combinatorial
Theory, Series B, 96(6):933–957, 2006."
REFERENCES,0.5841584158415841,"Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.
In Icml, 2010."
REFERENCES,0.5874587458745875,"Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classiﬁcation. In ICLR 2020 : Eighth International Conference
on Learning Representations, 2020."
REFERENCES,0.5907590759075908,"Luana Ruiz, Luiz Chamon, and Alejandro Ribeiro. Graphon neural networks and the transferability
of graph neural networks. Advances in Neural Information Processing Systems, 33, 2020a."
REFERENCES,0.594059405940594,"Luana Ruiz, Zhiyang Wang, and Alejandro Ribeiro. Graph and graphon neural network stability.
arXiv preprint arXiv:2010.12529, 2020b."
REFERENCES,0.5973597359735974,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017."
REFERENCES,0.6006600660066007,"Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najaﬁ, Ioannis Mitliagkas, David Lopez-
Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states.
In International Conference on Machine Learning, pp. 6438–6447. PMLR, 2019."
REFERENCES,0.6039603960396039,"Renato Vizuete, Federica Garin, and Paolo Frasca. The laplacian spectrum of large graphs sampled
from graphons. IEEE Transactions on Network Science and Engineering, 2021."
REFERENCES,0.6072607260726073,"Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph
classiﬁcation. In Proceedings of the Web Conference 2021, pp. 3663–3674, 2021."
REFERENCES,0.6105610561056105,"Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE transactions on neural networks and
learning systems, 2020."
REFERENCES,0.6138613861386139,"Hongteng Xu, Dixin Luo, Lawrence Carin, and Hongyuan Zha. Learning graphons via structured
gromov-wasserstein barycenters. In Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, pp. 10505–10513, 2021."
REFERENCES,0.6171617161716172,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
How powerful are graph neural
networks? In International Conference on Learning Representations, 2018."
REFERENCES,0.6204620462046204,"Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, pp. 4805–4815, 2018."
REFERENCES,0.6237623762376238,"Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph
contrastive learning with augmentations. Advances in Neural Information Processing Systems,
33, 2020."
REFERENCES,0.6270627062706271,"Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empiri-
cal risk minimization. In International Conference on Learning Representations, 2017."
REFERENCES,0.6303630363036303,"Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup
help with robustness and generalization? In International Conference on Learning Representa-
tions, 2021."
REFERENCES,0.6336633663366337,"Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
architecture for graph classiﬁcation. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018."
REFERENCES,0.636963696369637,Under review as a conference paper at ICLR 2022
REFERENCES,0.6402640264026402,"Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. IEEE Transactions
on Knowledge and Data Engineering, 2020."
REFERENCES,0.6435643564356436,"Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data aug-
mentation for graph neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intel-
ligence, volume 35, pp. 11015–11023, 2021."
REFERENCES,0.6468646864686468,"Yufei Zhao. Graph theory and additive combinatorics, 2019. URL https://yufeizhao.com/
gtac/."
REFERENCES,0.6501650165016502,"Jiajun Zhou, Jie Shen, and Qi Xuan. Data augmentation for graph classiﬁcation. In Proceedings of
the 29th ACM International Conference on Information & Knowledge Management, pp. 2341–
2344, 2020a."
REFERENCES,0.6534653465346535,"Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,
Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applica-
tions. AI Open, 1:57–81, 2020b."
REFERENCES,0.6567656765676567,Under review as a conference paper at ICLR 2022
REFERENCES,0.6600660066006601,"A
PROOF OF THEOREM"
REFERENCES,0.6633663366336634,"In the appendix, we ﬁrst present the preliminaries in Appendix A.1. And then we present complete
proof for Theorem 1 and Theorem 2 in Section A.3 and A.2, respectively."
REFERENCES,0.6666666666666666,"A.1
PRELIMINARIES"
REFERENCES,0.66996699669967,"Cut norm (Lov´asz, 2012; Zhao, 2019) is used to measure structural similarity of two graphons. The
deﬁnition of cut norm is as follow:"
REFERENCES,0.6732673267326733,Deﬁnition 2 The cut norm of grapon W is deﬁned as
REFERENCES,0.6765676567656765,"∥W∥□= supS,T ⊂[0,1]

Z"
REFERENCES,0.6798679867986799,"S×T
W(x, y)dxdy
,
(10)"
REFERENCES,0.6831683168316832,where the supremum is taken over all measurable subsets S and T .
REFERENCES,0.6864686468646864,"The following lemma follows the derivation of counting lemma for graphons, are known in the
paper (Lov´asz, 2012). It will be used to prove the Theorem 1."
REFERENCES,0.6897689768976898,"Lemma 1 Let F be a simple graph and let W, W ′ ∈W. Then"
REFERENCES,0.693069306930693,"|t(F, W) −t(F, W ′)| ≤e(F)||W −W ′||□
(11)"
REFERENCES,0.6963696369636964,"Proof of Lemma 1. The proof follows Zhao (2019). For an arbitrary simple graph F, by the triangle
inequality we have"
REFERENCES,0.6996699669966997,"|t(F, W) −t(F, W ′)| =  Z  Y"
REFERENCES,0.7029702970297029,"uivi∈E
W (ui, vi) −
Y"
REFERENCES,0.7062706270627063,"uivi∈E
W ′ (ui, vi) ! Y"
REFERENCES,0.7095709570957096,"v∈V
dv  ≤ |E|
X i=1  Z "
REFERENCES,0.7128712871287128,"
i−1
Y"
REFERENCES,0.7161716171617162,"j=1
W ′ (uj, vj) (W (ui, vi) −W ′ (ui, vi)) |E|
Y"
REFERENCES,0.7194719471947195,"k=i+1
W (uk, vk)  Y"
REFERENCES,0.7227722772277227,"v∈V
dv  (12)"
REFERENCES,0.7260726072607261,"Here, each absolute value term in the sum is bounded by the cut norm ∥W −W ′∥□if we ﬁx all
other irrelavant variables (everything except ui and vi for the i-th term), altogether implying that"
REFERENCES,0.7293729372937293,"| t(F, W) −t(F, W ′)| ≤e(F)||W −W ′||□
(13) ■"
REFERENCES,0.7326732673267327,"Lemma 2 (Corollary 10.4 in Lov´asz & Szegedy (2006)) Let W be a graphon, n ≥1, 0 < ε < 1,
and let F be a simple graph, then the W-random graph G = G(n, W) satisﬁes"
REFERENCES,0.735973597359736,"P (|t(F, G) −t(F, W)| > ε) ≤2exp

−
ε2n
8v(F)2"
REFERENCES,0.7392739273927392,"
(14)"
REFERENCES,0.7425742574257426,"A.2
PROOF OF THEOREM 1"
REFERENCES,0.7458745874587459,"We have the mixed graphon WI = λWG + (1 −λ)WH. Let W = WI, W ′ = WG, and F = FG in
Lemma 1, we have,"
REFERENCES,0.7491749174917491,"|t(FG, WI) −t(FG, WG)| ≤e(FG)||WI −WG||□
|t(FG, λWG + (1 −λ)WH) −t(F, WG)| ≤e(FG)||λWG + (1 −λ)WH −WG||□
≤e(FG)||(1 −λ)(WH −WG)||□ (15)"
REFERENCES,0.7524752475247525,"Recall that the cut norm ∥W∥□= supS,T ⊆[0,1]

R"
REFERENCES,0.7557755775577558,"S×T W
 ."
REFERENCES,0.759075907590759,Under review as a conference paper at ICLR 2022
REFERENCES,0.7623762376237624,"obviously, suppose α ∈R, we have"
REFERENCES,0.7656765676567657,"∥αW∥□=
sup
S,T ⊆[0,1]  Z"
REFERENCES,0.768976897689769,"S×T
αW
 =
sup
S,T ⊆[0,1] α
Z"
REFERENCES,0.7722772277227723,"S×T
W
 = α∥W∥□
(16)"
REFERENCES,0.7755775577557755,"Based on Equation 15 and Equation 16, we have"
REFERENCES,0.7788778877887789,"|t(FG, λWG + (1 −λ)WH) −t(FG, WG)| ≤e(FG)||(1 −λ)(WH −WG)||□
≤(1 −λ)e(FG)||WH −WG||□
(17)"
REFERENCES,0.7821782178217822,"Similarly, let W = WI, W ′ = WH and F = FH in Lemma 1, We can also easily obtain"
REFERENCES,0.7854785478547854,"|t(FH, λWG + (1 −λ)WH) −t(FH, WH)| ≤λe(FH)||WH −WG||□
(18)
Equation 17 and 18 produce the upper bound in Theorem 8.
■"
REFERENCES,0.7887788778877888,"A.3
PROOF OF THEOREM 2"
REFERENCES,0.7920792079207921,"Let F and W be the discriminative motif FG and the mixed graphon WI in Lemma 2, we will have"
REFERENCES,0.7953795379537953,"P (|t(FI, G) −t(FI, WI)| > ε) ≤2exp

−
ε2n
8v(FI)2"
REFERENCES,0.7986798679867987,"
(19)"
REFERENCES,0.801980198019802,"which produces the result in 9.
■"
REFERENCES,0.8052805280528053,"A.4
GRAPHONS ESTIMATION BY STEP FUNCTIONS"
REFERENCES,0.8085808580858086,"The proof follows Xu et al. (2021). A graphon can always be approximated by a step function in the
cut norm (Frieze & Kannan, 1999)."
REFERENCES,0.8118811881188119,"Let P = (P1, .., PK) be a partition of Ωinto K measurable sets. We deﬁne a step function WP :
Ω2 7→[0, 1] as"
REFERENCES,0.8151815181518152,"WP(x, y) =
XK"
REFERENCES,0.8184818481848185,"k,k′=1 wkk′1Pk×Pk′(x, y),
(20)"
REFERENCES,0.8217821782178217,"where each wkk′ ∈[0, 1] and the indicator function 1Pk×Pk′(x, y) is 1 if (x, y) ∈Pk × Pk′,
otherwise it is 0. The weak regularity lemma Lov´asz (2012) shown below guarantees that every
graphon can be approximated well in the cut norm by step functions."
REFERENCES,0.8250825082508251,"Theorem 3 (Weak Regularity Lemma (Lemma 9.9 in (Lov´asz, 2012)) ) For every graphon W
and K ≥1, there always exists a step function W with |P| = K steps such that"
REFERENCES,0.8283828382838284,"∥W −W∥□≤
2
√log K ∥W∥L2.
(21)"
REFERENCES,0.8316831683168316,"B
EXPERIMENTS SETTING"
REFERENCES,0.834983498349835,"B.1
EXPERIMENTAL SETTING"
REFERENCES,0.8382838283828383,"To ensure a fair comparison, we use the same hyperparater for modeling training and the same
architecture for vanilla model and other baselines. For model training, we use the Adam opti-
mizer(Kingma & Ba, 2015). The initial learning rate is 0.01 and will drop the learning rate by half
every 100 epochs. The batch size is set to 128. We split the dataset into train/val/test data by 7 : 1 : 2.
The best epoch are determined by the best validation accuracy. Note that best test epoch is selected
on a validation set. We also report the test accuracy on ten runs."
REFERENCES,0.8415841584158416,"For architecture of graph neural networks, the details are listed as follows,"
REFERENCES,0.8448844884488449,"• GCN (Kipf & Welling, 2016). Four GNN layers and global mean pooling are applied. All the
hidden units is set to 64. The activation is ReLU (Nair & Hinton, 2010)."
REFERENCES,0.8481848184818482,Under review as a conference paper at ICLR 2022
REFERENCES,0.8514851485148515,"• TopKPool (Gao & Ji, 2019). Three GNN layers and three TopK pooling are applied. A there-
layer percetron are adopted to predict the labels. All the hidden units is set to 64. The activation
is ReLU (Nair & Hinton, 2010).
• GIN (Xu et al., 2018). We apply ﬁve GNN layers and all MLPs have two layers. Batch normal-
ization (Ioffe & Szegedy, 2015) is applied on every hidden layer. All hidden units are set to 64.
The activation is ReLU (Nair & Hinton, 2010)."
REFERENCES,0.8547854785478548,"For hyperparemeter in G-Mixup, we generate 20% more graph for training graph. The graphons are
estimated based on the training graphs. We use different λ ∈[0.1, 0.2] to mix up the graphon and
generate synthetic with different strength of mixing up."
REFERENCES,0.858085808580858,"B.2
EXPERIMENTAL SETTING OF ROBUSTNESS"
REFERENCES,0.8613861386138614,"The graph neural network adopted in this experiment is GCN, the architecture of which is as
above. For label corruption, we randomly corrupt the graph labels with different corruption ratio
10%, 20%, 30%, 40%. For topology corruption, we we randomly remove/add edges with different
corruption ratio 10%, 20%, 30%, 40%. The dataset for topology corruption is REDDIT-BINARY."
REFERENCES,0.8646864686468647,"B.3
VISUALIZATION OF GRAPHONS ON MORE REAL-WORLD DATASET"
REFERENCES,0.8679867986798679,"G-Mixup explores ﬁve graphon estimation methods, including sorting-and-smoothing (SAS)
method (Chan & Airoldi, 2014), stochastic block approximation (SBA) (Airoldi et al., 2013),
“largest gap” (LG) (Channarond et al., 2012), matrix completion (MC) (Keshavan et al., 2010) and
the universal singular value thresholding (USVT) (Chatterjee et al., 2015). We present the estimated
graphon by LG in Figure 2. Here we present more visualization of graphons on IMDB-BINARY,
REDDIT-BINARY and IMDB-MULTI dataset. An obvious observation is that graphons of different
classes of graphs are different. This observation further validates the divergence of graphon between
different classes of graphs."
REFERENCES,0.8712871287128713,"LG
MC
SAS
SBA
USVT"
REFERENCES,0.8745874587458746,"IMDB-B
REDDIT-B
IMDB-M"
REFERENCES,0.8778877887788779,Figure 5: The estimated graphon on various dataset with different graphon estimation methods.
REFERENCES,0.8811881188118812,Under review as a conference paper at ICLR 2022
REFERENCES,0.8844884488448845,"C
ADDITIONAL EXPERIMENTS FOR REBUTTAL"
REFERENCES,0.8877887788778878,"In this appendix, we conduct additional experiments to further investigate the proposed method. The
additional experiments include 1) more graph neural networks ((DiffPool, MincutPool, GMT)) in
Appendix C.1, 2) molecular property prediction task with OGB datasets in Appendix C.2, 3) exper-
iments on the impact of the nodes number of generated graphs in Appendix C.3 and 4) experiments
on the performance of GCN with different layers in Appendix C.4."
REFERENCES,0.8910891089108911,"C.1
EXPERIMENT ON MORE GRAPH NEURAL NETWORKS (DIFFPOOL, MINCUTPOOL,
GMT)"
REFERENCES,0.8943894389438944,"To further validate the effectiveness of G-Mixup on more graph neural networks, we experiment
with DiffPool (Ying et al., 2018), MincutPool (Bianchi et al., 2020) and GMT (Baek et al., 2020).
For GMT, we use their released code and the recommended hyperparameters for their used datasets
(D&D, MUTAG, PROTEINS, IMDB-B, IMDB-M) in their paper. To reproduce its results, we use
their ofﬁcial code and the above datasets. The results are presented in Tables 5 and 6."
REFERENCES,0.8976897689768977,The details of backbones are listed as follows:
REFERENCES,0.900990099009901,"• DiffPool (Ying et al., 2018) is a differentiable graph pooling methods that can be adapted to
various GNN architectures, which maps nodes to clusters based on their learned embeddings.
• MincutPool (Bianchi et al., 2020) is a differentiable pooling baselines. It learns a clustering
function that can be quickly evaluated on out-of-sample graphs.
• GMT (Baek et al., 2020) is a multi-head attention based global pooling layer to generate graph
representation, which captures the interaction between nodes according to their structure."
REFERENCES,0.9042904290429042,"Our main observations are: Observation 7: G-Mixup improves the performance of DiffPool and
MincutPool on various datasets. From Table 5, our proposal gains 7 best performances among
8 reported accuracies, which substantially improve the performance of DiffPool and MincutPool.
Observation 8: G-Mixup can signiﬁcantly improve the performance of GMT. Table 6 shows
that G-Mixup outperform all the baselines on all datasets. Overall, G-Mixup outperform vanilla,
Dropedge, ManifoldMixup by 1.44%, 1.28%, 2.01%, respectively. This indicates the superiority of
G-Mixup for graph classiﬁcation task."
REFERENCES,0.9075907590759076,"Table 5: Performance comparisons of G-Mixup with DiffPool and MincutPool on different datasets.
The metric is classiﬁcation accuracy and its standard deviation. The best performance is in boldface."
REFERENCES,0.9108910891089109,"Backbone
Method
IMDB-B
IMDB-M
REDDIT-B
REDDIT-M5k"
REFERENCES,0.9141914191419142,"DiffPool
vanilla
71.68±3.40
47.75±2.34
78.40±4.38
31.61±5.95
w/ Dropedge
69.16±2.51
49.44±2.50
76.00±5.50
34.46±6.80
w/ NodeDropping
70.25±3.01
46.83±1.34
76.68±2.57
33.10±5.53
w/ Subgraph
69.50±2.16
46.00±4.43
76.06±2.81
31.65±4.43
w/ ManfoldMixup
66.50±4.04
45.16±4.63
78.37±2.29
34.46±6.80
w/ G-Mixup
73.25±3.89
50.70±2.79
78.87±2.27
38.42±6.51"
REFERENCES,0.9174917491749175,"MincutPool vanilla
73.25±3.27
49.04±3.57
84.95±3.25
49.32±2.67
w/ Dropedge
69.16±2.51
49.66±1.73
81.37±1.59
47.20±1.10
w/ NodeDropping
73.50±3.89
49.91±2.83
85.68±2.04
46.82±4.60
w/ Subgraph
70.25±1.84
48.18±1.10
84.91±2.50
49.22±2.49
w/ ManfoldMixup
70.62±2.09
49.96±1.86
85.12±2.29
47.20±1.10
w/ G-Mixup
73.93±2.84
50.29±2.30
85.87±1.37
50.12±2.47"
REFERENCES,0.9207920792079208,"Table 6: Performance comparisons of G-Mixup with GMT on different dataset. The metric is the
classiﬁcation accuracy and its standard deviation. The best performance is in boldface."
REFERENCES,0.9240924092409241,"Backbone Method
D&D
MUTAG
PROTEINS
IMDB-B
IMDB-M"
REFERENCES,0.9273927392739274,"GMT
vanilla
78.29±5.77
82.77±6.30
74.59±5.29
73.60±3.87
50.73±3.03
w/ Dropedge
78.37±4.17
82.22±8.88
74.32±5.42
73.40±3.85
50.73±3.09
w/ ManfoldMixup 77.69±3.81
82.22±10.48
74.41±3.97
73.70±3.79
49.93±3.49
w/ G-Mixup
79.57±3.69
84.44±8.88
75.13±5.06
74.70±3.76
51.33±3.52"
REFERENCES,0.9306930693069307,Under review as a conference paper at ICLR 2022
REFERENCES,0.933993399339934,"C.2
EXPERIMENT ON MOLECULAR PROPERTY PREDICTION"
REFERENCES,0.9372937293729373,"We experiment on molecular property prediction task (Hu et al., 2020), including ogbg-molhiv,
ogbg-molbace, ogbg-molbbbp. In these dataset, each graph represents a molecule, where nodes are
atoms, and edges are chemical bonds. We adopte ofﬁcial reference graph neural network backbones
(gcn, gcn-vitual, gin, gin-vitual) as our backbones, and we generate the edge attributes randomly
for synthetic graphs. The results are presented in Table 7. Observation 9: G-Mixup can improve
the performance of GNNs on molecular property prediction task with the experimental setting
for a fair comparison. Table 7 shows that G-Mixup gains 9 best performances among 12 reported
AUCs."
REFERENCES,0.9405940594059405,"Table 7: Performance comparisons of G-Mixup on molecular property prediction task. The metric
is AUROC 3 and its standard deviation. The best performance is in boldface."
REFERENCES,0.9438943894389439,"Backbones
Mehtods
ogbg-molhiv
ogbg-molbbbp
ogbg-molbace"
REFERENCES,0.9471947194719472,"GCN
vanilla
76.24±0.98
68.05±1.52
80.36±1.56
w/ Dropedge
75.93±0.76
68.02±0.95
80.22±1.59
w/ ManifoldMixup
76.24±1.40
68.36±2.05
80.46±2.05
w/ G-Mixup
76.29±0.80
68.45±0.84
80.73±2.06"
REFERENCES,0.9504950495049505,"GCN-virtual
vanilla
75.62±1.65
65.13±1.11
74.49±3.04
w/ Dropedge
74.64±1.32
66.46±1.61
69.75±3.47
w/ ManifoldMixup
74.04±2.06
65.51±1.74
73.10±4.97
w/ G-Mixup
76.56±0.80
67.20±1.30
73.55±4.79"
REFERENCES,0.9537953795379538,"GIN
vanilla
77.08±1.96
68.42±2.31
75.91±1.01
w/ Dropedge
75.77±1.75
66.16±2.96
70.50±6.24
w/ ManifoldMixup
75.73±1.25
68.15±2.04
77.44±4.13
w/ G-Mixup
77.14±0.45
69.28±1.24
77.79±3.34"
REFERENCES,0.9570957095709571,"GIN-virtual
vanilla
77.52±1.56
67.10±2.10
74.19±4.99
w/ Dropedge
76.83±1.11
68.87±1.17
72.20±3.37
w/ ManifoldMixup
76.51±2.22
68.04±2.87
74.17±1.38
w/ G-Mixup
77.09±1.07
70.02±1.68
73.53±3.98"
REFERENCES,0.9603960396039604,"C.3
WHAT IS THE IMPACT OF NUMBER OF NODES OF GENERATED GRAPHS?"
REFERENCES,0.9636963696369637,"We investigate the impact of number of nodes of generated synthetic graphs by G -Mixup and report
the results in Figure 6. Speciﬁcally, G -Mixup generates synthetic graphs with different the numbers
(hyperparameters K) of nodes and use them to train graph neural networks. From Figure 6, we can
see that average number of nodes in the original graphs is a better choice for hyperparameters K for
G -Mixup, which is accords with intuition."
REFERENCES,0.966996699669967,"C.4
HOW G -MIXUP PERFORM WHEN GRAPH NEURAL NETWORK GOES DEEPER?"
REFERENCES,0.9702970297029703,"We investigate the performance of G-Mixup when the GCN goes deeper and report the results in
Figure 7. We experimented with different numbers (2−9) of layers to investigate the performance of
G -Mixup. Observation 10: G-Mixup improves the performance of graph neural networks with
different layers. The left ﬁgure in Figure 7 shows G -Mixup gains better performance for IMDB-
BINARY dataset while the depth of GCNs is 2 −6. The performance with deeper GCN (7 −9) are
comparable to baselines, however, the accuracy of deeper GCN is much lower than shallow ones.
The right ﬁgure in Figure 7 shows G -Mixup gains better performance by a signiﬁcant margin for
REDDIT-BINARY dataset while the depth of GCNs is 2 −9. This validates the effectiveness of
G-Mixup when graph neural network goes deeper."
AREA UNDER RECEIVER OPERATING CHARACTERISTIC,0.9735973597359736,3Area Under Receiver Operating Characteristic
AREA UNDER RECEIVER OPERATING CHARACTERISTIC,0.976897689768977,Under review as a conference paper at ICLR 2022
AREA UNDER RECEIVER OPERATING CHARACTERISTIC,0.9801980198019802,"IMDB-BINARY
REDDIT-BINARY"
AREA UNDER RECEIVER OPERATING CHARACTERISTIC,0.9834983498349835,"Avg. #Nodes of
Original Graphs"
AREA UNDER RECEIVER OPERATING CHARACTERISTIC,0.9867986798679867,"Avg. #Nodes of
Original Graphs"
AREA UNDER RECEIVER OPERATING CHARACTERISTIC,0.9900990099009901,"Figure 6: The impact of the node numbers of generated synthetic graphs on IMDB-BINARY and
REDDIT-BINARY datasets. The red vertical line indicates the average number of all the original
graphs. The blue line represents that classiﬁcation accuracy with different number of nodes of
generated graphs. Obviously, the accuracy reach the maximum values around the red line on both
two datasets"
AREA UNDER RECEIVER OPERATING CHARACTERISTIC,0.9933993399339934,"IMDB-BINARY
REDDIT-BINARY"
AREA UNDER RECEIVER OPERATING CHARACTERISTIC,0.9966996699669967,"Figure 7: The performance of G-mixup using GCNs with different depth on IMDB-BINARY and
REDDIT-BINARY. This ﬁgure show that G-Mixup consistently improve GCN when it goes deeper."
