Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029411764705882353,"Do neural networks generalise because of bias in the functions returned by gradient
descent, or bias already present in the network architecture?"
ABSTRACT,0.0058823529411764705,¿Por qué no los dos?
ABSTRACT,0.008823529411764706,"This paper ﬁnds that while typical networks that ﬁt the training data already gener-
alise fairly well, gradient descent can further improve generalisation by selecting
networks with a large margin. This conclusion is based on a careful study of the
behaviour of inﬁnite width networks trained by Bayesian inference and ﬁnite width
networks trained by gradient descent. To measure the implicit bias of architecture,
new technical tools are developed to both analytically bound and consistently
estimate the average test error of the neural network–Gaussian process (NNGP)
posterior. This error is found to be already better than chance, corroborating the
ﬁndings of Valle-Pérez et al. (2019) and underscoring the importance of archi-
tecture. Going beyond this result, this paper ﬁnds that test performance can be
substantially improved by selecting a function with much larger margin than is typi-
cal under the NNGP posterior. This highlights a curious fact: minimum a posteriori
functions can generalise best, and gradient descent can select for those functions. In
summary, new technical tools suggest a nuanced portrait of generalisation involving
both the implicit biases of architecture and gradient descent."
INTRODUCTION,0.011764705882352941,"1
INTRODUCTION"
INTRODUCTION,0.014705882352941176,"Following an inﬂuential paper by Zhang et al. (2017), the basic question of why neural networks
generalise is generally regarded to be open. The authors demonstrate a surprising fact: deep learning
generalises even when the neural network is expressive enough to represent functions that do not
generalise. In turn, this implies that the theory of Vapnik & Chervonenkis (1971)—based on uniform
convergence of train error to population error—does not explain generalisation in neural networks."
INTRODUCTION,0.01764705882352941,"Several hypotheses have since been proposed to ﬁll this theoretical vacuum. One prominent hypothesis
states that, while most neural network solutions do not generalise well, there is an implicit bias in
the kinds of functions returned by gradient descent (Soudry et al., 2018). In sharp contrast, a second
hypothesis states that the solution space of a neural network is dominated by simple functions, while
the complex kinds of functions that overﬁt are relatively rare (Valle-Pérez et al., 2019)."
INTRODUCTION,0.020588235294117647,"This latter hypothesis dovetails with a particular “PAC-Bayesian” theorem of McAllester (1998),
which bounds the average population error of all classiﬁers consistent with a training sample. If this
bound is small, then the complex functions that overﬁt must indeed be rare. In more technical terms,
the PAC-Bayesian theorem can provide a meaningful certiﬁcate of generalisation even for machine
learning models with an inﬁnite Vapnik-Chervonenkis dimension, or an arbitrarily large number of
parameters, by properly accounting for the measure of those very complex functions."
INTRODUCTION,0.023529411764705882,"This paper takes a more nuanced position between these two hypotheses. While the average pop-
ulation error of all neural networks that ﬁt a training sample is found to be good (and certiﬁably
good by the PAC-Bayesian theorem), it is still possible for certain networks with special properties
to perform substantially better than average (and likewise substantially better than the PAC-Bayes
bound). Moreover, gradient descent may be used to speciﬁcally target these special networks. This
subtly counters a position put forward by Mingard et al. (2021), which suggests that gradient descent
may be well-modelled as sampling randomly from a particular Bayesian posterior distribution."
INTRODUCTION,0.026470588235294117,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.029411764705882353,"To support these claims, a careful study of the behaviour of both inﬁnite width and ﬁnite width neural
networks is conducted. In particular, this paper makes the following technical contributions:"
INTRODUCTION,0.03235294117647059,"Section 3 Implicit Bias of Architecture. A purely analytical PAC-Bayes bound (Theorem 2) on the
population error of the neural network–Gaussian process (NNGP) posterior for binary
classiﬁcation is derived. The bound furnishes an interpretable measure of model complexity
that depends on both the architecture and the training data. This exact analytical bound
improves upon an approximate computational approach due to Valle-Pérez et al. (2019).
Section 4 Testing the Bound. The new bound is found to be both non-vacuous and correlated with the
test error of ﬁnite width multilayer perceptrons trained by gradient descent. This provides
supporting evidence for the important role of architecture in generalisation, as put forward
by Valle-Pérez et al. (2019). Still, a gap exists with the performance of gradient descent.
Section 5 Implicit Bias of Gradient Descent. Going further beyond the work of Valle-Pérez et al.
(2019), a new theoretical tool (Theorem 3) is developed to enable consistent estimation
of the average error of the NNGP posterior on a given holdout set. The average is found
to be signiﬁcantly worse than the holdout performance of Gaussian process draws with
large margin. The experiment is repeated for ﬁnite width neural networks trained by
gradient descent, and the same qualitative phenomenon persists. This ﬁnding demonstrates
the ability of gradient descent to select large margin functions with vanishing posterior
probability that nonetheless generalise signiﬁcantly better than the posterior average."
RELATED WORK,0.03529411764705882,"2
RELATED WORK"
RELATED WORK,0.03823529411764706,"Margin-based generalisation theory
A rich body of work explores the connection between margin
and generalisation. For instance, Bartlett et al. (2017) propose a margin-based complexity measure
for neural networks derived via Rademacher complexity analysis, and Neyshabur et al. (2018) derive
a similar result via PAC-Bayes analysis. Neyshabur et al. (2019) test these bounds experimentally,
ﬁnding them to be vacuous and to scale poorly with network width. Biggs & Guedj (2021) further
develop this style of bound. Margin-based PAC-Bayes bounds go back at least to the work of Herbrich
(2001), who derived such a bound for linear classiﬁcation. The standard idea is that a solution with
large margin implies the existence of nearby solutions with the same training error (but perhaps
smaller margin), facilitating a more spread out PAC-Bayes posterior. This style of margin-based
PAC-Bayes bound does not provide guidance on whether the original large margin classiﬁer should
generalise better or worse than the additional nearby classiﬁers included in the posterior."
RELATED WORK,0.041176470588235294,"Non-vacuous bounds for neural networks
Aside from PAC-Bayes, many styles of generalisation
bound for neural networks are vacuous (Dziugaite & Roy, 2017). Even many PAC-Bayes bounds
are vacuous (Achille & Soatto, 2018; Foret et al., 2021) due to their choice of PAC-Bayes prior.
Dziugaite & Roy (2017) construct a non-vacuous weight space PAC-Bayes bound by iteratively
optimising both the PAC-Bayes prior and posterior by gradient descent. Wu et al. (2020) extend
this approach to involve Hessians. Meanwhile, Valle-Pérez et al. (2019) instantiate a non-vacuous
PAC-Bayes bound in the function space of neural networks via the NNGP correspondence (Neal,
1994; Lee et al., 2018). The evaluation of this bound involves an iterative and statistically inconsistent
expectation-propagation approximation. Unlike this paper’s purely analytical bound in Theorem 2,
these non-vacuous bounds are all evaluated by iterative computational procedures."
RELATED WORK,0.04411764705882353,"Implicit bias of gradient descent
Much research has gone into the role that gradient descent plays
in neural network generalisation. For instance, Zhang et al. (2017) suggest that gradient descent may
converge to solutions with special generalisation properties, while Wilson et al. (2017) suggest that
gradient descent may have a more favourable implicit bias than the Adam optimiser. Azizan et al.
(2021) make a related study in the context of mirror descent. Keskar et al. (2017) point to an implicit
bias present in mini-batch stochastic gradient descent, but Geiping et al. (2021) argue against the
importance of stochasticity. Meanwhile, Soudry et al. (2018) observe how gradient descent combined
with certain loss functions converges to max-margin solutions; Wei et al. (2018) make a similar
observation. On a different tack, Mingard et al. (2021), Valle-Pérez & Louis (2020) and Valle-Pérez
et al. (2019) argue that the implicit bias of neural architecture is more important than that of gradient
descent for understanding generalisation, and so long as gradient descent does not select solutions
pathologically, generalisation should occur."
RELATED WORK,0.047058823529411764,Under review as a conference paper at ICLR 2022
RELATED WORK,0.05,"weight space Ω
volume P[Ω] = 1"
RELATED WORK,0.052941176470588235,solution space VS
RELATED WORK,0.05588235294117647,volume P[VS]
RELATED WORK,0.058823529411764705,f(x(1); ·)
RELATED WORK,0.061764705882352944,f(x(2); ·)
RELATED WORK,0.06470588235294118,solution space VS
RELATED WORK,0.06764705882352941,volume P[VS]
RELATED WORK,0.07058823529411765,function space FS
RELATED WORK,0.07352941176470588,volume P[FS] = 1
RELATED WORK,0.07647058823529412,"Figure 1: The solution space VS of a learning task denotes the subset of classiﬁers that attain zero
error on a dataset S. While the solution space can have a complicated geometry when described in
weight space, in function space the solution space of a binary classiﬁcation task is just an orthant."
IMPLICIT BIAS OF ARCHITECTURE,0.07941176470588235,"3
IMPLICIT BIAS OF ARCHITECTURE"
IMPLICIT BIAS OF ARCHITECTURE,0.08235294117647059,"This section derives an analytical bound on the population error of an inﬁnitely wide neural network
averaged over all weight conﬁgurations that attain 0% train error—in other words, the average
population error of the NNGP posterior. Since this quantity assesses the performance of all solutions
rather than just those returned by gradient descent, it is intended to measure the implicit bias of
architecture. In Section 4, the bound is found to be non-vacuous for multilayer perceptrons. The
source of a substantial gap with the results of gradient descent training is investigated in Section 5,
and ultimately attributed to an additional implicit bias of gradient descent: namely margin."
PRELIMINARY NOTATION,0.08529411764705883,"3.1
PRELIMINARY NOTATION"
PRELIMINARY NOTATION,0.08823529411764706,"Consider a training dataset S of n input–label pairs: S = {(x(1), y(1)), ..., (x(n), y(n))}. The inputs
x(i) ∈Rd0 are embedded in Euclidean d0-space, while to simplify the analysis the paper shall restrict
to binary labels y(i) ∈{±1}. A classiﬁer f shall depend on a weight vector w ∈Ω, where Ωdenotes
the weight space. In particular, for an input x the prediction is given by sign f(x; w)."
PRELIMINARY NOTATION,0.09117647058823529,"One is interested in the relationship between the train error εS of a classiﬁer on a dataset S to the
population error εD on a data distribution D. For a weight vector w ∈Ω, these are deﬁned as:"
PRELIMINARY NOTATION,0.09411764705882353,"εS(w) := 1 n n
X"
PRELIMINARY NOTATION,0.09705882352941177,"i=1
I
h
sign f(x(i), w) ̸= y(i)i
;
εD(w) := P(x,y)∼D [sign f(x; w) ̸= y] .
(1)"
PRELIMINARY NOTATION,0.1,The solution space VS := {w ∈Ω| εS(w) = 0} denotes those classiﬁers that attain zero train error.
GEOMETRY OF SOLUTIONS IN FUNCTION SPACE,0.10294117647058823,"3.2
GEOMETRY OF SOLUTIONS IN FUNCTION SPACE"
GEOMETRY OF SOLUTIONS IN FUNCTION SPACE,0.10588235294117647,The function space FS is deﬁned to be the set of outputs that the classiﬁer f can realise on dataset S:
GEOMETRY OF SOLUTIONS IN FUNCTION SPACE,0.10882352941176471,"FS :=
n
f(x(1), w), ..., f(x(n), w)
  w ∈Ω
o
⊂Rn.
(2)"
GEOMETRY OF SOLUTIONS IN FUNCTION SPACE,0.11176470588235295,"In weight space the geometry of solutions can be arbitrarily complicated, but in function space the
solutions occupy the single orthant picked out by the binary training labels."
GEOMETRY OF SOLUTIONS IN FUNCTION SPACE,0.11470588235294117,"To measure the volume of the solution space, one can deﬁne a measure P on weight space. It
is convenient to enforce P[Ω] = 1 so that P is a probability measure. In the absence of a better
alternative, P is usually set to a multivariate Gaussian or uniform distribution. The measure P on
weight space induces a measure P on function space via the relation:"
GEOMETRY OF SOLUTIONS IN FUNCTION SPACE,0.11764705882352941,"P[F ⊂FS] := P
n
w ∈Ω


f(x(1), w), ..., f(x(n), w)

∈F
o
.
(3)"
GEOMETRY OF SOLUTIONS IN FUNCTION SPACE,0.12058823529411765,"The volume of solutions is denoted P[VS], and can be computed either in weight or function space.
In function space, P[VS] is an orthant probability. The situation is visualised in Figure 1."
GEOMETRY OF SOLUTIONS IN FUNCTION SPACE,0.12352941176470589,Under review as a conference paper at ICLR 2022
PAC-BAYES THEORY,0.1264705882352941,"3.3
PAC-BAYES THEORY"
PAC-BAYES THEORY,0.12941176470588237,"PAC-Bayes relates the volume of solutions to their average population error. The following result
was derived by Valle-Pérez et al. (2019) as a corollary of a theorem due to Langford & Seeger (2001).
The result is similar in form to a theorem of McAllester (1998).
Theorem 1 (A PAC-Bayesian theorem). First, ﬁx a probability measure P over the weight space Ω
of a classiﬁer. Let S denote a training set of n datapoints sampled iid from the data distribution D
and let VS denote the corresponding solution space. Consider the population error 0 ≤εD(w) ≤1
of weight setting w ∈Ω, and its average over the solution space εD(VS) := Ew∼P[εD(w) | w ∈VS].
Then, for a proportion 1 −δ of draws of the training set S,"
PAC-BAYES THEORY,0.1323529411764706,"εD(VS) ≤ln
1
1 −εD(VS) ≤
ln
1
P[VS] + ln 2n"
PAC-BAYES THEORY,0.13529411764705881,"δ
n −1
.
(4)"
PAC-BAYES THEORY,0.13823529411764707,"For large n, the ln 2n/δ term is negligible and the result says that the population error averaged over
solutions is less than the ratio of the negative log volume of solutions to the number of training points."
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.1411764705882353,"3.4
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES"
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.14411764705882352,"Since inﬁnitely wide neural networks induce a Gaussian measure on function space, computing the
volume of the solution space P[VS] amounts to computing a Gaussian orthant probability."
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.14705882352941177,"In more detail, let P denote a measure on the weight space Ωof an inﬁnitely wide neural network
satisfying the conditions of the NNGP correspondence—see Theorem 4 in Appendix A.3 for an
example. Then for a weight vector w ∼P, the network outputs on a training set S are distributed:"
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.15,"f(x(1); w), ..., f(x(n); w) ∼N(0, Σ),
(5)"
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.15294117647058825,with covariance Σij := Ew∼P[f(x(i); w)f(x(j); w)].
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.15588235294117647,"Therefore, under the NNGP correspondence, the volume of solutions computed in function space is
just the Gaussian probability of the orthant picked out by the binary training labels:"
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.1588235294117647,"P[VS] = Pϕ∼N(0,Σ)[sign ϕ1 = y(1), ..., sign ϕn = y(n)].
(6)
To facilitate estimating and bounding this probability, this paper has derived the following lemma.
Lemma 1 (Gaussian orthant probability). For a covariance matrix Σ ∈Rn×n, and a binary vector
y ∈{±1}n, let p denote the corresponding Gaussian orthant probability:
p := Pϕ∼N(0,Σ)[sign(ϕ) = y].
(7)"
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.16176470588235295,"Letting I denote the n×n identity matrix, ⊙the elementwise product and |·| the elementwise absolute
value, then p may be equivalently expressed as: p = 1"
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.16470588235294117,"2n Eu∼N(0,I)
h
e−1"
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.1676470588235294,"2 (y⊙|u|)T(
n√"
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.17058823529411765,"det ΣΣ−1−I)(y⊙|u|)i
=: e−C0(Σ,y),
(8)"
VOLUME OF SOLUTIONS VIA GAUSSIAN ORTHANT PROBABILITIES,0.17352941176470588,and p may be bounded as follows: p ≥1
"N E
N",0.17647058823529413,"2n e
n
2 −n√"
"N E
N",0.17941176470588235,det Σ[( 1 2 −1
"N E
N",0.18235294117647058,π ) tr(Σ−1)+ 1
"N E
N",0.18529411764705883,π yT Σ−1y] =: 1
"N E
N",0.18823529411764706,"2n e
n
2 −C1(Σ,y).
(9)"
"N E
N",0.19117647058823528,"The proof is given in Appendix A.2. Equation 8 yields an unbiased Monte Carlo estimator of Gaussian
orthant probabilities, and Inequality 9 yields a lower bound. The complexity measures C0 and C1 are
deﬁned for later use."
"N E
N",0.19411764705882353,"To gain intuition about the lemma, observe that 1/2n is the orthant probability for an isotropic
Gaussian. Depending on the degree of anisotropy
n√"
"N E
N",0.19705882352941176,"det ΣΣ−1 −I inherent in the covariance
matrix Σ, Equation 8 captures how the orthant probability may either be exponentially ampliﬁed or
suppressed compared to 1/2n."
"N E
N",0.2,"As an aside, using Inequality 9 to lower bound Equation 6 has a Bayesian interpretation. Since
the volume of solutions may be written P[VS] =
R"
"N E
N",0.20294117647058824,"ΩdP(w) I[w ∈VS], it may be interpreted as the
Bayesian evidence for the network architecture under the likelihood function I[w ∈VS]. A Bayesian
would then refer to Inequality 9 as an evidence lower bound."
"N E
N",0.20588235294117646,"The following generalisation bound is a basic consequence of Theorem 1, Equation 6 and Lemma 1:"
"N E
N",0.2088235294117647,Under review as a conference paper at ICLR 2022
"N E
N",0.21176470588235294,"Theorem 2 (Upper bound on the average population error of an inﬁnitely wide neural network).
First, ﬁx a probability measure P over the weight space Ωof an inﬁnitely wide neural network. Let S
denote a training set of n datapoints sampled iid from the data distribution D, let y ∈{±1}n denote
the binary vector of training labels, and let VS denote the corresponding solution space. Consider the
population error 0 ≤εD(w) ≤1 of weight setting w ∈Ω, and its average over the solution space
εD(VS) := Ew∼P[εD(w) | w ∈VS]. Let Σ denote the NNGP covariance matrix (Equation 5). Then,
for a proportion 1 −δ of draws of the training set S,"
"N E
N",0.21470588235294116,"εD(VS) ≤ln
1
1 −εD(VS) ≤C0(Σ, y) + ln 2n"
"N E
N",0.21764705882352942,"δ
n −1
≤"
"N E
N",0.22058823529411764,"n
5 + C1(Σ, y) + ln 2n"
"N E
N",0.2235294117647059,"δ
n −1
,
(10)"
"N E
N",0.22647058823529412,where the complexity measures C0 and C1 are deﬁned in Lemma 1.
"N E
N",0.22941176470588234,"Since the complexity measure C1 is an analytical function of the NNGP covariance matrix Σ and the
binary vector of training labels y, Theorem 2 is an analytical generalisation bound for the NNGP
posterior. For large n, the result simpliﬁes to: εD(VS) ⪅C0(Σ, y)/n ≤1/5 + C1(Σ, y)/n. So the
bound depends on the ratio of complexity measures C0 and C1 to the number of datapoints n."
"N E
N",0.2323529411764706,"To gain further intuition about Theorem 2, consider two special cases. First, suppose that the neural
architecture induces no correlation between any pair of distinct data points, such that Σ = I. Then
εD(VS) ⪅C0(I, y)/n = ln 2 ≈0.7 and the bound is worse than chance. This corresponds to
pure memorisation of the training labels. Next, suppose that the neural architecture induces strong
intra-class correlations and strong inter-class anti-correlations, such that Σij = yiyj. Although this Σ
is singular, it may be seen directly that P[VS] = 1/2. Then by Theorem 1, εD(VS) ⪅ln 2/n which
is much better than chance for large n. This corresponds to pure generalisation from the training
labels. Interpolating between these two extremes would suggest that a good neural architecture would
impose a prior on functions with strong intra-class and weak inter-class correlations."
TESTING THE BOUND,0.23529411764705882,"4
TESTING THE BOUND"
TESTING THE BOUND,0.23823529411764705,"This section compares the generalisation bound for inﬁnite width networks (Theorem 2) to the
performance of ﬁnite width multilayer perceptrons (MLPs) trained by gradient descent. The bound is
found to be non-vacuous and correlated with the effects of varying depth and dataset complexity. Still,
there is a substantial gap between the bound and gradient descent, which is investigated in Section 5."
TESTING THE BOUND,0.2411764705882353,"Three modiﬁed versions of the MNIST handwritten digit dataset (LeCun et al., 1998) of varying
“hardness” were used in the experiments, as detailed in Figure 2 (top left). MLPs were trained with L
layers and W hidden units per layer. Speciﬁcally, each 28px × 28px input image x was ﬂattened
to lie in R784, and normalised to satisfy ∥x∥2 =
√"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.24411764705882352,"784. The networks consisted of an input layer
in R784×W , (L −2) layers in RW ×W , and an output layer in RW ×1. The nonlinearity ϕ was set
to ϕ(z) :=
√"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.24705882352941178,"2 · max(0, z). For this architecture, the width W →∞kernel is the compositional
arccosine kernel described in Theorem 4 in Appendix A.3. For the ﬁnite width networks, the training
loss was set to square loss using the binary training labels as regression targets, and the networks
were trained for 100 epochs using the Nero optimiser (Liu et al., 2021) with an initial learning rate
of 0.01 decayed by a factor of 0.9 every epoch, and a mini-batch size of min(50, training set size)
data points. The ﬁnal train error was 0% in all reported experiments. All bounds were computed
with a failure probability of δ = 0.01, and C0 was estimated using 106 Monte-Carlo samples. All
experiments were run on one NVIDIA Titan RTX GPU."
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.25,"The generalisation bound of Theorem 2 was ﬁrst compared across three datasets of varying complexity.
The network architecture was set to a depth L = 7 MLP, and the bound was computed via Monte-
Carlo estimation of C0. The results are shown in Figure 2 (top right). The bound was found to reﬂect
the relative hardness of the datasets. For random labels, the bound was vacuous as desired. Next,
the generalisation bound was compared against the empirical performance of a depth L = 7 MLP
trained on the decimal digits dataset. The results are shown in Figure 2 (bottom left). While loose
compared to the holdout error of the ﬁnite width network, the bound is still non-vacuous. Finally,
the effect of varying network depth was investigated on the decimal digits dataset. Two depths
were compared: L = 2 and L = 7. The results are shown in Figure 2 (bottom right). The bounds
(computed via Monte-Carlo estimation of C0) appear to predict the relative holdout performance of
the two architectures at ﬁnite width. These results corroborate those of Valle-Pérez & Louis (2020)
but without the use of the statistically inconsistent expectation-propagation approximation."
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.2529411764705882,Under review as a conference paper at ICLR 2022
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.25588235294117645,"MNIST Variant
Inputs
Labels"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.25882352941176473,"random labels
{0, 1, ..., 9}
coin ﬂip
decimal digits
{0, 1, ..., 9}
parity
binary digits
{0, 1}
parity"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.26176470588235295,"100
200
300
400
500
Number of training examples 0.0 0.2 0.4 0.6 Error"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.2647058823529412,Varying the dataset complexity
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.2676470588235294,"Random labels
Chance
Decimal digits
Binary digits"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.27058823529411763,"100
200
300
400
500
Number of training examples 0.0 0.2 0.4 0.6 Error"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.2735294117647059,Comparing to gradient descent
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.27647058823529413,"Chance
W=
 bound
W=
 estimator"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.27941176470588236,"W=5000
W=10000"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.2823529411764706,"100
200
300
400
500
Number of training examples 0.0 0.2 0.4 0.6 Error"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.2852941176470588,Varying the number of layers
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.28823529411764703,"Chance
L=2, W=
 bound"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.2911764705882353,"L=2, W=5000
L=7, W=
 bound"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.29411764705882354,"L=7, W=5000"
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.29705882352941176,"Figure 2: Testing Theorem 2—the bound on the average population error of the NNGP posterior.
The bound is found to be non-vacuous and correlated with the effects of varying network depth and
dataset complexity. For all curves, the mean and range are plotted over three global random seeds. A
substantial gap is visible between the bound and the results of gradient descent training, which is
investigated in Section 5."
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.3,"Top left: The three datasets used in the experiments, listed in order of hardness. For random labels
there is no meaningful relationship between image and label, so generalisation is impossible. Binary
digits is easier than decimal digits because each class is less diverse."
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.3029411764705882,"Top right: Comparing generalisation bounds from Theorem 2 for datasets of varying hardness. The
curves are computed by Monte-Carlo estimation of C0 for an inﬁnite width depth 7 MLP. The ordering
of the bounds reﬂects the dataset hardness. For random labels the bound is rightfully vacuous."
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.3058823529411765,"Bottom left: Comparing generalisation bounds from Theorem 2 to the results of training ﬁnite
width (W = 5000 and W = 10000) networks by gradient descent. The bounds are computed by
both Monte-Carlo estimation of C0 (referred to as W = ∞estimator) and exact computation of C1
(referred to as W = ∞bound). The comparison is made for depth 7 MLPs on the decimal digits
dataset. The exact bound computed via C1 is looser than the C0 bound computed via Monte-Carlo
estimation but only slightly, and both are non-vacuous above 150 datapoints. While the bounds follow
the same trend as the results of training ﬁnite width networks, a substantial gap is visible."
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.3088235294117647,"Bottom right: Comparing generalisation bounds from Theorem 2 to the results of training ﬁnite
width networks by gradient descent on decimal digits, for networks of varying depth. The bounds are
computed by exact computation of C1. The ordering of the bounds matches the ﬁnite width results,
but again a substantial gap is visible between the bounds and the results of gradient descent training."
THE NETWORKS CONSISTED OF AN INPUT LAYER,0.31176470588235294,Under review as a conference paper at ICLR 2022
IMPLICIT BIAS OF GRADIENT DESCENT,0.31470588235294117,"5
IMPLICIT BIAS OF GRADIENT DESCENT"
IMPLICIT BIAS OF GRADIENT DESCENT,0.3176470588235294,"In Section 3, a bound was derived on the average population error of all inﬁnitely wide neural
networks that ﬁt a certain training set. Since the bounded quantity measures the performance of all
solutions rather than just those returned by gradient descent, it is intended to assess the implicit bias
of architecture. In Section 4, this bound was tested and found to be non-vacuous. Still a substantial
gap was found between the bound and the performance of ﬁnite width networks trained by gradient
descent. This gap could arise for several potential reasons:"
IMPLICIT BIAS OF GRADIENT DESCENT,0.3205882352941177,i) slackness in the bounding technique;
IMPLICIT BIAS OF GRADIENT DESCENT,0.3235294117647059,ii) a difference between inﬁnite width and ﬁnite width neural networks;
IMPLICIT BIAS OF GRADIENT DESCENT,0.3264705882352941,iii) an additional implicit bias of gradient descent.
IMPLICIT BIAS OF GRADIENT DESCENT,0.32941176470588235,"This section tests these various possibilities, ultimately concluding that gradient descent does have an
important additional implicit bias: the ability to control the margin of the returned network."
AN EXACT FORMULA FOR THE AVERAGE HOLDOUT ERROR OF SOLUTIONS,0.3323529411764706,"5.1
AN EXACT FORMULA FOR THE AVERAGE HOLDOUT ERROR OF SOLUTIONS"
AN EXACT FORMULA FOR THE AVERAGE HOLDOUT ERROR OF SOLUTIONS,0.3352941176470588,"To investigate whether slackness in the bounding technique is responsible for the gap, an additional
theoretical tool was developed: a formula for the holdout error of a binary classiﬁer averaged over the
solution space. In contrast to Theorem 2 which gives an upper bound on population error, Theorem 3
is an equality and therefore does not suffer from slackness. The proof is given in Appendix A.1."
AN EXACT FORMULA FOR THE AVERAGE HOLDOUT ERROR OF SOLUTIONS,0.3382352941176471,"Theorem 3 (Average holdout error of a binary classiﬁer). First, ﬁx a probability measure P over the
weight space Ωof a binary classiﬁer. Let S denote a training set and let VS denote the solution space.
For a holdout set T of m datapoints, consider the holdout error 0 ≤εT (w) ≤1 of weight setting
w ∈Ω, and its average over the solution space εT (VS) := Ew∼P[εT (w) | w ∈VS]. Then:"
AN EXACT FORMULA FOR THE AVERAGE HOLDOUT ERROR OF SOLUTIONS,0.3411764705882353,εT (VS) = 1 m X
AN EXACT FORMULA FOR THE AVERAGE HOLDOUT ERROR OF SOLUTIONS,0.34411764705882353,"(x,y)∈T"
AN EXACT FORMULA FOR THE AVERAGE HOLDOUT ERROR OF SOLUTIONS,0.34705882352941175,"P[VS∪(x,−y)]"
AN EXACT FORMULA FOR THE AVERAGE HOLDOUT ERROR OF SOLUTIONS,0.35,"P[VS]
.
(11)"
AN EXACT FORMULA FOR THE AVERAGE HOLDOUT ERROR OF SOLUTIONS,0.35294117647058826,"In words, the average holdout error over solutions equals the reduction in the volume of solutions
when the training set is augmented with a negated holdout point, averaged over the holdout set. For an
inﬁnitely wide neural network, Equation 11 involves computing a sum of ratios of Gaussian orthant
probabilities. These probabilities can be consistently estimated by Equation 8 in Lemma 1."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.3558823529411765,"5.2
THREE-WAY COMPARISON OF HOLDOUT ERROR"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.3588235294117647,"Armed with Theorem 3, this subsection makes a three-way comparison between the holdout error of
inﬁnite width networks averaged over solutions, the holdout error of ﬁnite width networks averaged
over solutions, and the holdout error of ﬁnite width networks trained by gradient descent."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.36176470588235293,"To compute the holdout error of ﬁnite width networks averaged over solutions, weight vectors were
randomly sampled and all non-solutions were discarded. To make this process computationally
tractable, a very small training set was used consisting of only 5 samples from binary digits. A
holdout set of 50 datapoints was used, and the network was set to a 7-layer MLP. The results were:"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.36470588235294116,"average holdout error at inﬁnite width:
0.337 ± 0.001;
average holdout error at width 10,000:
0.33 ± 0.01;
gradient descent holdout error at width 10,000:
0.178 ± 0.007."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.36764705882352944,"Based on these results, three comments are in order. First, the close agreement between the average
holdout error at ﬁnite and inﬁnite width suggests that the inﬁnite width limit may not be responsible
for the signiﬁcant gap observed in Section 4. Second, the signiﬁcant gap between the gradient descent
holdout error and the holdout error averaged over solutions suggests slackness in the bounding
technique in Theorem 2 may also not be the main culprit. Third, the signiﬁcant gap between the
gradient descent holdout error and the holdout error averaged over solutions suggests that gradient
descent does have an extra implicit bias. The next subsection attempts to diagnose this implicit bias."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.37058823529411766,Under review as a conference paper at ICLR 2022
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.3735294117647059,f(x(1); ·)
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.3764705882352941,f(x(2); ·)
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.37941176470588234,"label
ray"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.38235294117647056,"10
2
10
1
100
101
102"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.38529411764705884,Label scale 0.0 0.2 0.4 0.6 Error
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.38823529411764707,Holdout error along the label ray
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.3911764705882353,"Chance
Average holdout error over orthant
Network holdout error at scale 
Network train error at scale 
NNGP holdout error at scale"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.3941176470588235,"100
200
300
400
500
Number of training examples 0.0 0.2 0.4 0.6 Error"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.39705882352941174,Varying the label scale at depth 2
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4,"Chance
W=
 estimator
W=5000, label scale 0.1
W=5000, label scale 1.0"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.40294117647058825,"100
200
300
400
500
Number of training examples 0.0 0.2 0.4 0.6 Error"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.40588235294117647,Varying the label scale at depth 7
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4088235294117647,"Figure 3: Exploring generalisation performance along the label ray deﬁned in Section 5.3. Functions
further along the ray are found to generalise substantially better than average, for both NNGP draws
and ﬁnite width networks trained by gradient descent."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4117647058823529,"Top left: Schematic diagram illustrating the label ray in function space. While all functions in the grey
shaded orthant attain zero train error, the darker shaded solutions have small margin and thus—under
a Gaussian process model—holdout predictions are expected to be driven by random ﬂuctuations."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4147058823529412,"Top right: Holdout error along the label ray of the small learning task described in Section 5.2.
Results are shown for both networks trained by gradient descent and NNGP posterior samples. The
label scale α is deﬁned in Section 5.3 and measures the distance of the function along the label
ray. NNGP posterior samples were generated by sampling from a Gaussian distribution with mean
and covariance given by Equations 12 and 13. Networks trained by gradient descent were found by
training with the loss function given in Equation 14. Shading shows the standard deviation over 100
random intialisations or posterior samples. Deep into function space (large α) the holdout error is
signiﬁcantly better than for functions close to the origin (small α). The average holdout error over
the orthant, as computed in Section 5.2, is also plotted. Large margin networks and NNGP posterior
samples both signiﬁcantly outperform the orthant average. Finally, a gap is visible between large
margin NNGP posterior samples and large margin networks trained by gradient descent—this may be
due to an additional and undiagnosed implicit bias of gradient descent."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4176470588235294,"Bottom: Repeating experiments from Section 4 with a smaller label scale to sanity check the ﬁndings.
For a depth 2 and a depth 7 MLP on the decimal digits dataset, the experiment from Section 4 was
repeated with the loss function set to Lα=0.1 as deﬁned in Equation 14. The inﬁnite width W = ∞
curve shows the Theorem 2 bound estimated via C0. The other two curves show the results of
networks trained by gradient descent with Lα=0.1 and Lα=1. Despite all networks attaining 0% train
error, holdout error was signiﬁcantly worse for networks trained using Lα=0.1. Also, the networks
trained using Lα=0.1 exhibit a substantially smaller gap with the Theorem 2 upper bound."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.42058823529411765,Under review as a conference paper at ICLR 2022
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4235294117647059,"5.3
DIAGNOSIS: MARGIN"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4264705882352941,"This subsection ﬁnds that gradient descent has an important implicit bias in determining the margin
of the returned network. This conclusion is made by studying the generalisation error of functions
along the label ray in function space—depicted in Figure 3 (top left). For a training set S with a
vector of binary labels y ∈{±1}n, a function α-far along the label ray refers to the point αy ∈FS."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4294117647058823,"For the case of a zero mean NNGP, the predictive distribution on a holdout set T conditioned on
training labels α-far along the label ray is given by Bishop (2006, Chapter 2.3.1):"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4323529411764706,"posterior mean = α × ΣT SΣ−1
SSy,
(12)"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.43529411764705883,"posterior covariance = ΣT T −ΣT SΣ−1
SSΣST ,
(13)"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.43823529411764706,"where ΣT S is the covariance between holdout and train inputs, and ΣST , ΣSS and ΣT T are deﬁned
analogously. For large α, predictions are driven by the posterior mean, whereas for small α, they are
driven by random ﬂuctuations. So one expects that letting α →∞should improve holdout error."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4411764705882353,"For the case of gradient descent training, functions α-far along the label ray can be returned by
minimising the following loss function:"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4441176470588235,"Lα(W) := 1 n n
X i=1"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.4470588235294118,"
f(x(i); W) −αy(i)2
.
(14)"
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.45,"A subtle but important point is that for gradient descent training with Nero (Liu et al., 2021) the
norms of the weight matrices are constrained, thus α controls a properly normalised notion of margin."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.45294117647058824,"An experiment was performed to measure the holdout error of networks α-far along the label ray—for
both NNGP draws and neural networks selected by Nero—for α ranging from 10−2 to 102. As can
be seen in Figure 3 (top right), varying α appears to directly control the holdout error, despite all
solutions attaining 0% train error. Moreover, large α solutions signiﬁcantly outperform the average
holdout error over the solution space. This suggests that gradient descent possesses a signiﬁcant
implicit bias in its ability to control margin—going beyond the implicit bias of architecture."
THREE-WAY COMPARISON OF HOLDOUT ERROR,0.45588235294117646,"To sanity check this result, experiments from Section 4 were repeated with the loss function Lα=0.1
replacing Lα=1. As can be seen in Figure 3 (bottom), the holdout performance was signiﬁcantly
diminshed at α = 0.1, as was the gap with the PAC-Bayes bound from Theorem 2."
DISCUSSION AND CONCLUSION,0.4588235294117647,"6
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.46176470588235297,"This paper has explored the separate implicit biases of architecture and gradient descent. Section 3
derived an analytical generalisation bound on the NNGP posterior, Section 4 found this bound to be
non-vacuous, while Section 5 showed that large margin functions substantially outperform the bound."
DISCUSSION AND CONCLUSION,0.4647058823529412,"The ﬁndings in this paper support the importance of the implicit bias of architecture: reproducing the
ﬁndings in Valle-Pérez & Louis (2020) but with improved technical tools, the average generalisation
performance of architecture was already found to be good. But the implicit bias of gradient descent
was also found to be important. In particular, in contrast to an assumption made by Valle-Pérez &
Louis (2020) that gradient descent “samples the zero-error region close to uniformly”, it was found
that gradient descent can be used to target zero-error functions with large margin. This also subtly
counters a proposal by Mingard et al. (2021) that gradient descent acts like a “Bayesian sampler”.
In particular, gradient descent can target functions with margin α →∞for which the Bayesian
posterior probability →0. And indeed, these minimum a posteriori functions seem to generalise best."
DISCUSSION AND CONCLUSION,0.4676470588235294,"One direction of future work suggested by these results is an improvement to the function space
PAC-Bayes theory to account for margin. While margin-based PAC-Bayes bounds do already exist
(Herbrich, 2001; Langford & Shawe-Taylor, 2003; Neyshabur et al., 2018), these bounds operate in
weight space and further do not seem to imply an advantage to selecting the max-margin classiﬁer
over the other classiﬁers included in the PAC-Bayes posterior."
DISCUSSION AND CONCLUSION,0.47058823529411764,"Ultimately, a generalisation theory that properly accounts for the various implicit biases of deep
learning could provide a more principled basis for both neural architecture design as well as the design
of new regularisation schemes. It is hoped that the new analytical results as well as experimental
insights included in this paper contribute a step towards reaching that goal."
DISCUSSION AND CONCLUSION,0.47352941176470587,Under review as a conference paper at ICLR 2022
REFERENCES,0.4764705882352941,REFERENCES
REFERENCES,0.47941176470588237,"Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep
representations. Journal of Machine Learning Research, 2018."
REFERENCES,0.4823529411764706,"Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent on overparameterized
nonlinear models. IEEE Transactions on Neural Networks and Learning Systems, 2021."
REFERENCES,0.4852941176470588,"Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Neural Information Processing Systems, 2017."
REFERENCES,0.48823529411764705,"Felix Biggs and Benjamin Guedj. On margins and derandomisation in PAC-Bayes. arXiv:2107.03955,
2021."
REFERENCES,0.49117647058823527,"Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag, 2006."
REFERENCES,0.49411764705882355,"Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. In Neural Information
Processing Systems, 2009."
REFERENCES,0.4970588235294118,"Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Uncertainty in
Artiﬁcial Intelligence, 2017."
REFERENCES,0.5,"Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efﬁciently improving generalization. In International Conference on Learning Representations,
2021."
REFERENCES,0.5029411764705882,"Jonas Geiping, Micah Goldblum, Phillip E. Pope, Michael Moeller, and Tom Goldstein. Stochastic
training is not necessary for generalization. arXiv:2109.14119, 2021."
REFERENCES,0.5058823529411764,"Ralf Herbrich. Learning Kernel Classiﬁers: Theory and Algorithms. MIT Press, 2001."
REFERENCES,0.5088235294117647,"Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
International Conference on Learning Representations, 2017."
REFERENCES,0.5117647058823529,"John Langford and Matthias Seeger. Bounds for averaging classiﬁers. Technical report, Carnegie
Mellon University, 2001."
REFERENCES,0.5147058823529411,"John Langford and John Shawe-Taylor. PAC-Bayes & margins. In Neural Information Processing
Systems, 2003."
REFERENCES,0.5176470588235295,"Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. MNIST handwritten digit database, 1998."
REFERENCES,0.5205882352941177,"Jaehoon Lee, Jascha Sohl-Dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as Gaussian processes. In International Conference on
Learning Representations, 2018."
REFERENCES,0.5235294117647059,"Yang Liu, Jeremy Bernstein, Markus Meister, and Yisong Yue.
Learning by turning: Neural
architecture aware optimisation. In International Conference on Machine Learning, 2021."
REFERENCES,0.5264705882352941,"David A. McAllester. Some PAC-Bayesian theorems. In Conference on Computational Learning
Theory, 1998."
REFERENCES,0.5294117647058824,"Chris Mingard, Guillermo Valle-Pérez, Joar Skalse, and Ard A. Louis. Is SGD a Bayesian sampler?
Well, almost. Journal of Machine Learning Research, 2021."
REFERENCES,0.5323529411764706,"Radford M. Neal. Bayesian Learning for Neural Networks. Ph.D. thesis, Department of Computer
Science, University of Toronto, 1994."
REFERENCES,0.5352941176470588,"Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
A PAC-Bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learning
Representations, 2018."
REFERENCES,0.538235294117647,Under review as a conference paper at ICLR 2022
REFERENCES,0.5411764705882353,"Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role
of over-parametrization in generalization of neural networks. In International Conference on
Learning Representations, 2019."
REFERENCES,0.5441176470588235,"Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. Journal of Machine Learning Research, 2018."
REFERENCES,0.5470588235294118,"Guillermo Valle-Pérez and Ard A. Louis. Generalization bounds for deep learning. arXiv:2012.04115,
2020."
REFERENCES,0.55,"Guillermo Valle-Pérez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because
the parameter–function map is biased towards simple functions. In International Conference on
Learning Representations, 2019."
REFERENCES,0.5529411764705883,"Aad W. van der Vaart. Asymptotic Statistics. Cambridge University Press, 1998."
REFERENCES,0.5558823529411765,"Vladimir N. Vapnik and Alexey Ya. Chervonenkis. On the uniform convergence of relative frequencies
of events to their probabilities. Theory of Probability & Its Applications, 1971."
REFERENCES,0.5588235294117647,"Colin Wei, J. Lee, Qiang Liu, and Tengyu Ma. On the margin theory of feedforward neural networks.
arXiv:1810.05369, 2018."
REFERENCES,0.5617647058823529,"Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Neural Information Processing Systems,
2017."
REFERENCES,0.5647058823529412,"Yikai Wu, Xingyu Zhu, Chenwei Wu, Annie Wang, and Rong Ge. Dissecting Hessian: Understanding
common structure of Hessian in neural networks. arXiv:2010.04261, 2020."
REFERENCES,0.5676470588235294,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations, 2017."
REFERENCES,0.5705882352941176,Under review as a conference paper at ICLR 2022
REFERENCES,0.5735294117647058,"APPENDIX A
PROOFS"
REFERENCES,0.5764705882352941,"A.1
FUNCTION SPACE ORTHANT ARITHMETIC"
REFERENCES,0.5794117647058824,"The following PAC-Bayesian theorem is due to Valle-Pérez et al. (2019):
Theorem 1 (A PAC-Bayesian theorem). First, ﬁx a probability measure P over the weight space Ω
of a classiﬁer. Let S denote a training set of n datapoints sampled iid from the data distribution D
and let VS denote the corresponding solution space. Consider the population error 0 ≤εD(w) ≤1
of weight setting w ∈Ω, and its average over the solution space εD(VS) := Ew∼P[εD(w) | w ∈VS].
Then, for a proportion 1 −δ of draws of the training set S,"
REFERENCES,0.5823529411764706,"εD(VS) ≤ln
1
1 −εD(VS) ≤
ln
1
P[VS] + ln 2n"
REFERENCES,0.5852941176470589,"δ
n −1
.
(4)"
REFERENCES,0.5882352941176471,"Proof. The ﬁrst inequality is a basic property of logarithms. The second inequality follows from
Theorem 3 of Langford & Seeger (2001), by setting the prior measure to P(·) = P(·) and the posterior
measure to the conditional Q(·) = P(·|VS). Under these settings, the average training error rate over
the posterior is zero and KL(Q|P) = ln
1
P[VS]."
REFERENCES,0.5911764705882353,"Theorem 3 (Average holdout error of a binary classiﬁer). First, ﬁx a probability measure P over the
weight space Ωof a binary classiﬁer. Let S denote a training set and let VS denote the solution space.
For a holdout set T of m datapoints, consider the holdout error 0 ≤εT (w) ≤1 of weight setting
w ∈Ω, and its average over the solution space εT (VS) := Ew∼P[εT (w) | w ∈VS]. Then:"
REFERENCES,0.5941176470588235,εT (VS) = 1 m X
REFERENCES,0.5970588235294118,"(x,y)∈T"
REFERENCES,0.6,"P[VS∪(x,−y)]"
REFERENCES,0.6029411764705882,"P[VS]
.
(11)"
REFERENCES,0.6058823529411764,"Proof. The result follows by interchanging the order of expectations, rewriting the expectation of an
indicator variable as a probability and ﬁnally applying the deﬁnition of conditional probability:"
REFERENCES,0.6088235294117647,εT (VS) := Ew∼P[εT (w) | w ∈VS] = 1 m X
REFERENCES,0.611764705882353,"(x,y)∈T
Ew∼P [I[sign f(x, w) = −y] | w ∈VS] = 1 m X"
REFERENCES,0.6147058823529412,"(x,y)∈T
P [sign f(x, w) = −y | w ∈VS] = 1 m X"
REFERENCES,0.6176470588235294,"(x,y)∈T"
REFERENCES,0.6205882352941177,"P [sign f(x, w) = −y and w ∈VS]"
REFERENCES,0.6235294117647059,P [w ∈VS] = 1 m X
REFERENCES,0.6264705882352941,"(x,y)∈T"
REFERENCES,0.6294117647058823,"P[VS∪(x,−y)]"
REFERENCES,0.6323529411764706,"P[VS]
."
REFERENCES,0.6352941176470588,"A.2
GAUSSIAN ORTHANT PROBABILITIES"
REFERENCES,0.638235294117647,"Lemma 1 (Gaussian orthant probability). For a covariance matrix Σ ∈Rn×n, and a binary vector
y ∈{±1}n, let p denote the corresponding Gaussian orthant probability:"
REFERENCES,0.6411764705882353,"p := Pϕ∼N(0,Σ)[sign(ϕ) = y].
(7)"
REFERENCES,0.6441176470588236,"Letting I denote the n×n identity matrix, ⊙the elementwise product and |·| the elementwise absolute
value, then p may be equivalently expressed as: p = 1"
REFERENCES,0.6470588235294118,"2n Eu∼N(0,I)
h
e−1"
REFERENCES,0.65,"2 (y⊙|u|)T(
n√"
REFERENCES,0.6529411764705882,"det ΣΣ−1−I)(y⊙|u|)i
=: e−C0(Σ,y),
(8)"
REFERENCES,0.6558823529411765,and p may be bounded as follows: p ≥1
"N E
N",0.6588235294117647,"2n e
n
2 −n√"
"N E
N",0.6617647058823529,det Σ[( 1 2 −1
"N E
N",0.6647058823529411,π ) tr(Σ−1)+ 1
"N E
N",0.6676470588235294,π yT Σ−1y] =: 1
"N E
N",0.6705882352941176,"2n e
n
2 −C1(Σ,y).
(9)"
"N E
N",0.6735294117647059,Under review as a conference paper at ICLR 2022
"N E
N",0.6764705882352942,"Proof. The orthant probability may ﬁrst be expressed using the probability density function of the
multivariate Normal distribution as follows:"
"N E
N",0.6794117647058824,"p =
1
p"
"N E
N",0.6823529411764706,(2π)n det Σ Z
"N E
N",0.6852941176470588,"y⊙ϕ≥0
e−1"
"N E
N",0.6882352941176471,2 ϕT Σ−1ϕ dϕ.
"N E
N",0.6911764705882353,"By the change of variables u =
y⊙ϕ
2n√"
"N E
N",0.6941176470588235,"det Σ or equivalently ϕ =
2n√"
"N E
N",0.6970588235294117,"det Σ(y ⊙u), the orthant probability
may be expressed as:"
"N E
N",0.7,"p =
1
p (2π)n Z"
"N E
N",0.7029411764705882,"u≥0
e−1"
"N E
N",0.7058823529411765,2 (y⊙u)T n√
"N E
N",0.7088235294117647,det ΣΣ−1(y⊙u) du = 1
N,0.711764705882353,"2n
1
p (2π)n Z"
N,0.7147058823529412,Rn e−1
N,0.7176470588235294,2 (y⊙|u|)T n√
N,0.7205882352941176,det ΣΣ−1(y⊙|u|) du = 1
N,0.7235294117647059,"2n Eu∼N(0,I)
h
e−1"
N,0.7264705882352941,"2 (y⊙|u|)T(
n√"
N,0.7294117647058823,"det ΣΣ−1−I)(y⊙|u|)i
,"
N,0.7323529411764705,"where the second equality follows by symmetry, and the third equality follows by inserting a factor of
e−u2/2e+u2/2 = 1 into the integrand."
N,0.7352941176470589,"Next, by Jensen’s inequality, p ≥1"
N,0.7382352941176471,2n e−1
N,0.7411764705882353,"2 Eu∼N (0,I)[(y⊙|u|)T(
n√"
N,0.7441176470588236,det ΣΣ−1−I)(y⊙|u|)] = 1
N,0.7470588235294118,2n e−1
P,0.75,"2
P"
P,0.7529411764705882,"ij Eu∼N (0,I)[yiyj|ui||uj|(
n√"
P,0.7558823529411764,"det ΣΣ−1
ij −δij)] = 1"
P,0.7588235294117647,"2n e−1 2[
P i(
n√"
P,0.7617647058823529,"det ΣΣ−1
ii −1)+ 2 π
P"
P,0.7647058823529411,"i̸=j yiyj
n√"
P,0.7676470588235295,"det ΣΣ−1
ij ] = 1"
P,0.7705882352941177,"2n e−1 2[
P"
P,0.7735294117647059,i((1−2
P,0.7764705882352941,π ) n√
P,0.7794117647058824,"det ΣΣ−1
ii −1)+ 2 π
P"
P,0.7823529411764706,"ij yiyj
n√"
P,0.7852941176470588,"det ΣΣ−1
ij ] = 1"
P,0.788235294117647,2n e−1
P,0.7911764705882353,2[(1−2
P,0.7941176470588235,π ) n√
P,0.7970588235294118,"det Σ tr(Σ−1)−n+ 2 π
n√"
P,0.8,det ΣyT Σ−1y] = 1
"N E
N",0.8029411764705883,"2n e
n
2 −n√"
"N E
N",0.8058823529411765,det Σ[( 1 2 −1
"N E
N",0.8088235294117647,π ) tr(Σ−1)+ 1
"N E
N",0.8117647058823529,π yT Σ−1y].
"N E
N",0.8147058823529412,"The third equality follows by noting Eu∼N(0,I) |ui||ui| = Eu∼N(0,1) u2 = 1, while for i ̸= j,
Eu∼N(0,I) |ui||uj| = [Eu∼N(0,1) |u|]2 = 2/π."
"N E
N",0.8176470588235294,"A.3
NEURAL NETWORKS AS GAUSSIAN PROCESSES"
"N E
N",0.8205882352941176,"The essence of the following lemma is due to Neal (1994). The lemma will be used in the proof of
Theorem 4.
Lemma 2 (NNGP correspondence). For the neural network layer given by Equation 18, consider
randomly sampling the weight matrix W (l). If the following hold:"
"N E
N",0.8235294117647058,"(i) for every x ∈Rd0, the activations ϕ

z(l−1)
1
(x)

, ..., ϕ

z(l−1)
dl−1 (x)

are iid with ﬁnite ﬁrst
and second moment;"
"N E
N",0.8264705882352941,"(ii) the weights W (l)
ij are drawn iid with zero mean and ﬁnite variance;"
"N E
N",0.8294117647058824,"(iii) for any random variable z with ﬁnite ﬁrst and second moment, ϕ(z) also has ﬁnite ﬁrst and
second moment;"
"N E
N",0.8323529411764706,"then, in the limit that dl−1 →∞, the following also hold:"
"N E
N",0.8352941176470589,"(1) for every x ∈Rd0, the activations ϕ

z(l)
1 (x)

, ..., ϕ

z(l)
dl (x)

are iid with ﬁnite ﬁrst and
second moment;"
"N E
N",0.8382352941176471,"(2) for any collection of k inputs x(1), ..., x(k), the distribution of the ith pre-activations
z(l)
i (x(1)), ..., z(l)
i (x(k)) is jointly Normal."
"N E
N",0.8411764705882353,Under review as a conference paper at ICLR 2022
"N E
N",0.8441176470588235,"While condition (i) may seem non-trivial, notice that the lemma propagates this condition to the next
layer via entailment (1). This means that provided condition (i) holds for ϕ(z(1)(x)) at the ﬁrst layer,
then recursive application of the lemma implies that the network’s pre-activations are jointly Normal
at all layers via entailment (2)."
"N E
N",0.8470588235294118,"Proof of Lemma 2. To establish entailment (1), consider the dl-dimensional vector Z1
:=
h
z(l)
1 (x), ..., z(l)
dl (x)
i
. Observe that Z1 satisﬁes:"
"N E
N",0.85,"Z1 =
1
p dl−1"
"N E
N",0.8529411764705882,"dl−1
X j=1"
"N E
N",0.8558823529411764,"h
W (l)
1j ϕ

z(l−1)
j
(x)

, ..., W (l)
dljϕ

z(l−1)
j
(x)
i
.
(15)"
"N E
N",0.8588235294117647,"By conditions (i) and (ii), the summands in Equation 15 are iid random vectors with zero mean,
and any two distinct components of the same vector summand have the same variance and zero
covariance. Then by the multivariate central limit theorem (van der Vaart, 1998, p. 16), in the limit
that dl−1 →∞, the components of Z1 are Gaussian with a covariance equal to a scaled identity
matrix. In particular, the components of Z1 are iid with ﬁnite ﬁrst and second moment. Applying
condition (iii) then implies that the same holds for ϕ(Z1). This establishes entailment (1)."
"N E
N",0.861764705882353,"To
establish
entailment
(2),
consider
instead
the
k-dimensional
vector
Z2
:=
h
z(l)
i (x(1)), ..., z(l)
i (x(k))
i
. Observe that Z2 satisﬁes:"
"N E
N",0.8647058823529412,"Z2 =
1
p dl−1"
"N E
N",0.8676470588235294,"dl−1
X j=1"
"N E
N",0.8705882352941177,"h
W (l)
ij ϕ

z(l−1)
j
(x(1))

, ..., W (l)
ij ϕ

z(l−1)
j
(x(k))
i
.
(16)"
"N E
N",0.8735294117647059,"Again by combining conditions (i) and (ii), the summands in Equation 16 are iid random vectors with
ﬁnite mean and ﬁnite covariance. Then as dl−1 →∞, the distribution of Z2 is jointly Normal—again
by the multivariate central limit theorem. This establishes entailment (2)."
"N E
N",0.8764705882352941,"The essence of the following theorem appears in a paper by Lee et al. (2018), building on the work of
Cho & Saul (2009). The theorem and its proof are included for completeness.
Theorem 4 (NNGP for relu networks). Consider an L-layer MLP deﬁned recursively via:"
"N E
N",0.8794117647058823,"z(1)
i
(x) =
1
√d0 d0
X"
"N E
N",0.8823529411764706,"j=1
W (1)
ij xj,
(17)"
"N E
N",0.8852941176470588,"z(l)
i (x) =
1
p dl−1"
"N E
N",0.888235294117647,"dl−1
X"
"N E
N",0.8911764705882353,"j=1
W (l)
ij ϕ

z(l−1)
j
(x)

,
(18)"
"N E
N",0.8941176470588236,"where x ∈Rd0 denotes an input, z(l)(x) ∈Rdl denotes the pre-activations at the lth layer, W (l)
denotes the weight matrix at the lth layer, and ϕ denotes the nonlinearity."
"N E
N",0.8970588235294118,"Set the output dimension dL = 1 and set the nonlinearity to a scaled relu ϕ(z) :=
√"
"N E
N",0.9,"2 · max(0, z).
Suppose that the weight matrices W (1), ..., W (L) have entries drawn iid N(0, 1), and consider any
collection of k inputs x(1), ..., x(k) each with Euclidean norm √d0."
"N E
N",0.9029411764705882,"If d1, ..., dL−1 →∞, the distribution of outputs z(L)(x(1)), ..., z(L)(x(k)) ∈R induced by random
sampling of the weights is jointly Normal with mean zero and covariance:"
"N E
N",0.9058823529411765,"E
h
z(L)(x)z(L)(x′)
i
= h ◦... ◦h
|
{z
}
L−1 times"
"N E
N",0.9088235294117647,xT x′ d0
"N E
N",0.9117647058823529,"
;
(19)"
"N E
N",0.9147058823529411,"where h(t) := 1 π
√"
"N E
N",0.9176470588235294,"1 −t2 + t · (π −arccos t)

."
"N E
N",0.9205882352941176,"Proof. Condition (ii) of Lemma 2 holds at all layers for iid standard Normal weights, and condition
(iii) holds trivially for the scaled relu nonlinearity. Provided one can establish condition (i) for
the ﬁrst layer activations ϕ

z(1)
1 (x)

, ..., ϕ

z(1)
d1 (x)

, then condition (i) will hold at all layers by"
"N E
N",0.9235294117647059,Under review as a conference paper at ICLR 2022
"N E
N",0.9264705882352942,"recursive application of Lemma 2, thus establishing joint Normality of the pre-activations at all layers
(including the network outputs). But condition (i) holds at the ﬁrst layer, since it is quick to check
by Equation 17 that for any x satisfying ∥x∥2 = √d0, the pre-activations z(1)
1 (x), ..., z(1)
d1 (x) are iid
N (0, 1), and ϕ preserves both iid-ness and ﬁnite-ness of the ﬁrst and second moment."
"N E
N",0.9294117647058824,"Since the pre-activations at any layer are jointly Normal, all that remains is to compute their ﬁrst and
second moments. For the ith hidden unit in the lth layer, the ﬁrst moment E[z(l)
i (x)] = 0. This can
be seen by taking the expectation of Equation 18 and using the fact that the W (l)
ij are independent of
the previous layer’s activations and have mean zero."
"N E
N",0.9323529411764706,"Since the pre-activations z(l)
i (x) and z(l)
i (x′) are jointly Normal with mean zero, their distribution is
completely described by their covariance matrix Σl(x, x′), deﬁned by:"
"N E
N",0.9352941176470588,"ρl(x, x′) := E
h
z(l)
i (x)z(l)
i (x′)
i"
"N E
N",0.9382352941176471,"Σl(x, x′) :=

ρl(x, x)
ρl(x, x′)
ρl(x, x′)
ρl(x′, x′) 
,"
"N E
N",0.9411764705882353,"where the hidden unit index i is unimportant since hidden units in the same layer are identically
distributed."
"N E
N",0.9441176470588235,"The theorem statement will follow from an effort to express Σl(x, x′) in terms of Σl−1(x, x′),
and then recursing back through the network. By Equation 18 and independence of the W (l)
ij , the
covariance ρl(x, x′) may be expressed as:"
"N E
N",0.9470588235294117,"ρl(x, x′) = E
h
ϕ

z(l−1)
j
(x)

ϕ

z(l−1)
j
(x′)
i
,
(20)"
"N E
N",0.95,"where j indexes an arbitrary hidden unit in the (l −1)th layer. To make progress, it helps to ﬁrst
evaluate:"
"N E
N",0.9529411764705882,"ρl(x, x) = E

ϕ

z(l−1)
j
(x)
2
= 1"
"N E
N",0.9558823529411765,"2 · 2 · ρl−1(x, x),"
"N E
N",0.9588235294117647,"which follows by the deﬁnition of ϕ and symmetry of the Gaussian expectation around zero. Then,
by recursion:
ρl(x, x) = ρl−1(x, x) = ... = ρ1(x, x) = 1,
where the ﬁnal equality holds because the ﬁrst layer pre-activations are iid N(0, 1) by Equation 17.
Therefore, the covariance Σl−1 at layer l −1 is just:"
"N E
N",0.961764705882353,"Σl−1(x, x′) =

1
ρl−1(x, x′)
ρl−1(x, x′)
1 
,"
"N E
N",0.9647058823529412,"Equation 20 may now be used to express ρl(x, x′) in terms of ρl−1(x, x′). Dropping the (x, x′)
indexing for brevity:
ρl = Eu,v∼N(0,Σl−1) [ϕ (u) ϕ (v)] =
1 π
q"
"N E
N",0.9676470588235294,"1 −ρ2
l−1 ZZ"
"N E
N",0.9705882352941176,"u,v≥0
du dv exp

−u2 −2ρl−1uv + v2"
"N E
N",0.9735294117647059,"2(1 −ρ2
l−1) 
uv."
"N E
N",0.9764705882352941,"By making the substitution ρl−1 = cos θ, this integral becomes equivalent to 1"
"N E
N",0.9794117647058823,"πJ1(θ) as expressed in
Equation 15 of Cho & Saul (2009). Substituting in the evaluation of this integral (Cho & Saul, 2009,
Equation 6), one obtains:
ρl(x, x′) = h(ρl−1(x, x′)),
(21)"
"N E
N",0.9823529411764705,"where h(t) := 1 π
√"
"N E
N",0.9852941176470589,"1 −t2 + t · (π −arccos t)

."
"N E
N",0.9882352941176471,"All that remains is to evaluate ρ1(x, x′). Since E
h
W (1)
ij W (1)
ik
i
= δjk, this is given by:"
"N E
N",0.9911764705882353,"ρ1(x, x′) := E
h
z(1)
i
(x)z(1)
i
(x′)
i = 1 d0 d0
X"
"N E
N",0.9941176470588236,"j,k=1
E
h
W (1)
ij W (1)
ik
i
xjx′
k = xT x′ d0
."
"N E
N",0.9970588235294118,The proof is completed by combining this expression with the recurrence relation in Equation 21.
