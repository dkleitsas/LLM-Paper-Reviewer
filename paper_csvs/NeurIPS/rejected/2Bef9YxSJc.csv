Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007716049382716049,"Recent studies empirically indicate that language models (LMs) encode rich world
1"
ABSTRACT,0.0015432098765432098,"knowledge beyond mere semantics, attracting significant attention across various
2"
ABSTRACT,0.0023148148148148147,"fields. However, in the recommendation domain, it remains uncertain whether
3"
ABSTRACT,0.0030864197530864196,"LMs implicitly encode user preference information. Contrary to the prevailing
4"
ABSTRACT,0.0038580246913580245,"understanding that LMs and traditional recommender models learn two distinct rep-
5"
ABSTRACT,0.004629629629629629,"resentation spaces due to a huge gap in language and behavior modeling objectives,
6"
ABSTRACT,0.005401234567901234,"this work rethinks such understanding and explores extracting a recommendation
7"
ABSTRACT,0.006172839506172839,"space directly from the language representation space. Surprisingly, our findings
8"
ABSTRACT,0.006944444444444444,"demonstrate that item representations, when linearly mapped from advanced LM
9"
ABSTRACT,0.007716049382716049,"representations, yield superior recommendation performance. This outcome sug-
10"
ABSTRACT,0.008487654320987654,"gests a homomorphic relationship between the language representation space and
11"
ABSTRACT,0.009259259259259259,"an effective recommendation space, implying that collaborative signals may indeed
12"
ABSTRACT,0.010030864197530864,"be encoded within advanced LMs. Motivated by these findings, we propose a
13"
ABSTRACT,0.010802469135802469,"simple yet effective collaborative filtering (CF) model named AlphaRec, which
14"
ABSTRACT,0.011574074074074073,"utilizes language representations of item textual metadata (e.g., titles) instead of tra-
15"
ABSTRACT,0.012345679012345678,"ditional ID-based embeddings. Specifically, AlphaRec is comprised of three main
16"
ABSTRACT,0.013117283950617283,"components: a multilayer perceptron (MLP), graph convolution, and contrastive
17"
ABSTRACT,0.013888888888888888,"learning (CL) loss function, making it extremely easy to implement and train. Our
18"
ABSTRACT,0.014660493827160493,"empirical results show that AlphaRec outperforms leading ID-based CF models
19"
ABSTRACT,0.015432098765432098,"on multiple datasets, marking the first instance of such a recommender with text
20"
ABSTRACT,0.016203703703703703,"embeddings achieving this level of performance. Moreover, AlphaRec introduces
21"
ABSTRACT,0.016975308641975308,"a new text-based CF paradigm with several desirable advantages: being easy to
22"
ABSTRACT,0.017746913580246913,"implement, lightweight, rapid convergence, superior zero-shot recommendation
23"
ABSTRACT,0.018518518518518517,"abilities in new domains, and being aware of user intention.
24"
INTRODUCTION,0.019290123456790122,"1
Introduction
25"
INTRODUCTION,0.020061728395061727,"Language models (LMs) have achieved great success across various domains [3–7], prompting a
26"
INTRODUCTION,0.020833333333333332,"critical question about the knowledge encoded within their representation spaces. Recent studies
27"
INTRODUCTION,0.021604938271604937,"empirically find that LMs extend beyond semantic understanding to encode comprehensive world
28"
INTRODUCTION,0.022376543209876542,"knowledge about various domains, including game states [8], lexical attributes [9], and even concepts
29"
INTRODUCTION,0.023148148148148147,"of space and time [10] through language modeling. However, in the domain of recommendation
30"
INTRODUCTION,0.023919753086419752,"where the integration of LMs is attracting widespread interest [11–15], it remains unclear whether
31"
INTRODUCTION,0.024691358024691357,"LMs inherently encode relevant information on user preferences and behaviors. One possible reason
32"
INTRODUCTION,0.02546296296296296,"is the significant difference between the objectives of language modeling for LMs and user behavior
33"
INTRODUCTION,0.026234567901234566,"modeling for recommenders [16–19].
34"
INTRODUCTION,0.02700617283950617,"Currently, one prevailing understanding holds that general LMs and traditional recommenders
35"
INTRODUCTION,0.027777777777777776,"encode two distinct representation spaces: the language space and the recommendation space
36"
INTRODUCTION,0.02854938271604938,"(i.e., user and item representation space), each offering potential enhancements to the other for
37"
INTRODUCTION,0.029320987654320986,(a) Linearly mapping language representations into the recommendation space
INTRODUCTION,0.03009259259259259,"(b) Performance comparison
(c) The t-SNE representations of movies and user intention in two spaces.
Figure 1: Linearly mapping item titles in language representation space into recommendation space
yields superior recommendation performance on Movies & TV [1] dataset. (1a) The framework
of linear mapping. (1b) The recommendation performance comparison between leading CF rec-
ommenders and linear mapping. (1c) The t-SNE [2] visualizations of movie representations, with
colored lines linking identical movies or user intention across language space (left) and linearly
projected recommendation space (right)."
INTRODUCTION,0.030864197530864196,"recommendation tasks [17, 20]. On the one hand, when using LMs as recommenders, aligning the
38"
INTRODUCTION,0.031635802469135804,"language space with the recommendation space could significantly improve the performance of
39"
INTRODUCTION,0.032407407407407406,"LM-based recommendation [14, 21–23]. Various alignment strategies are proposed, including fine-
40"
INTRODUCTION,0.033179012345679014,"tuning LMs with recommendation data [15, 16, 24–26], incorporating embeddings from traditional
41"
INTRODUCTION,0.033950617283950615,"recommenders as a new modality of LMs [17, 20, 27], and extending the vocabulary of LMs with item
42"
INTRODUCTION,0.034722222222222224,"tokens [18, 19, 28–31]. On the other hand, when using LMs as the enhancer, traditional recommenders
43"
INTRODUCTION,0.035493827160493825,"greatly benefit from from leveraging text representations [32–45], semantic and reasoning information
44"
INTRODUCTION,0.036265432098765434,"[46–49], and generated user behaviors [50, 51]. Despite these efforts, explicit explorations of the
45"
INTRODUCTION,0.037037037037037035,"relationship between language and recommendation spaces remain largely unexplored.
46"
INTRODUCTION,0.03780864197530864,"In this work, we rethink the prevailing understanding and explore whether LMs inherently encode
47"
INTRODUCTION,0.038580246913580245,"user preferences through language modeling. Specifically, we test the possibility of directly deriving a
48"
INTRODUCTION,0.03935185185185185,"recommendation space from the language representation space, assessing whether the representations
49"
INTRODUCTION,0.040123456790123455,"of item textual metadata (e.g., titles) obtained from LMs can independently achieve satisfactory
50"
INTRODUCTION,0.04089506172839506,"recommendation performance. Positive results would imply that user behavioral patterns, such as
51"
INTRODUCTION,0.041666666666666664,"collaborative signals (i.e., user preference similarities between items) [52, 53], may be implicitly
52"
INTRODUCTION,0.04243827160493827,"encoded by LMs. To test this hypothesis, we employ linear mapping to project the language
53"
INTRODUCTION,0.043209876543209874,"representations of item titles into a recommendation space (see Figure 1a). Our observations include:
54"
INTRODUCTION,0.04398148148148148,"• Surprisingly, this simple linear mapping yields high-quality item representations, which achieve
55"
INTRODUCTION,0.044753086419753084,"exceptional recommendation performance (see Figure 1b and experimental results in Section 2).
56"
INTRODUCTION,0.04552469135802469,"• The clustering of items is generally preserved from the language space to the recommendation
57"
INTRODUCTION,0.046296296296296294,"space (see Figure 1c). For example, movies with the theme of superheroes and monsters are
58"
INTRODUCTION,0.0470679012345679,"gathering in both language and recommendation spaces.
59"
INTRODUCTION,0.047839506172839504,"• Interestingly, the linear mapping effectively reveals preference similarities that may be implicit
60"
INTRODUCTION,0.04861111111111111,"or even obscure in the language space. For instance, while certain movies, such as those of
61"
INTRODUCTION,0.04938271604938271,"homosexual movies (illustrated in Figure 1c), show dispersed representations in the language space,
62"
INTRODUCTION,0.05015432098765432,"their projections through linear mapping tend to cluster together, reflecting their genres affiliation.
63"
INTRODUCTION,0.05092592592592592,"These findings indicate a homomorphic relationship between the language representation space of
64"
INTRODUCTION,0.05169753086419753,"LMs and an effective item representation space for recommendation. Motivated by this insight, we
65"
INTRODUCTION,0.05246913580246913,"propose a new text-based recommendation paradigm for general collaborative filtering (CF), which
66"
INTRODUCTION,0.05324074074074074,"utilizes the pre-trained language representations of item titles as the item input and the average
67"
INTRODUCTION,0.05401234567901234,"historical interactions’ representations as the user input. Different from traditional ID-based CF
68"
INTRODUCTION,0.05478395061728395,"models [54, 55, 52] that heavily rely on trainable user and item IDs, this paradigm solely uses
69"
INTRODUCTION,0.05555555555555555,"pre-trained LM embeddings and completely abandons ID-based embeddings. In this paper, to fully
70"
INTRODUCTION,0.05632716049382716,"explore the potential of advanced language representations, we adopt a simple model architecture
71"
INTRODUCTION,0.05709876543209876,"consisting of a two-layer MLP with graph convolution, and the popular contrastive loss, InfoNCE
72"
INTRODUCTION,0.05787037037037037,"[56–58], as the objective function. This model is named AlphaRec for its originality and a series of
73"
INTRODUCTION,0.05864197530864197,"good properties.
74"
INTRODUCTION,0.05941358024691358,"Benefiting from paradigm shifts from ID-based embeddings to language representations, AlphaRec
75"
INTRODUCTION,0.06018518518518518,"presents three desirable advantages. First, AlphaRec is notable for its simplicity, lightweight, rapid
76"
INTRODUCTION,0.06095679012345679,"convergence, and exceptional recommendation performance (see Section 4.1). We empirically
77"
INTRODUCTION,0.06172839506172839,"demonstrate that, for the first time, such a simple model with embeddings from pre-trained LMs can
78"
INTRODUCTION,0.0625,"outperform leading CF models on multiple datasets. This finding strongly supports the possibility
79"
INTRODUCTION,0.06327160493827161,"for developing language-representation-based recommender systems. Second, AlphaRec exhibits
80"
INTRODUCTION,0.06404320987654322,"a strong zero-shot recommendation capability across untrained domains (see Section 4.2). By
81"
INTRODUCTION,0.06481481481481481,"co-training on three Amazon datasets (Books, Movies & TV, and Video Games) [1], AlphaRec
82"
INTRODUCTION,0.06558641975308642,"can achieve performance comparable to the fully-trained LightGCN on entirely different platforms
83"
INTRODUCTION,0.06635802469135803,"(MovieLens-1M [59] and BookCrossing [60]), and even exceed LightGCN in a completely new
84"
INTRODUCTION,0.06712962962962964,"domain (Amazon Industrial), without additional training on these target datasets. This capability
85"
INTRODUCTION,0.06790123456790123,"underscores AlphaRec’s potential to develop more general recommenders. Third, AlphaRec is user-
86"
INTRODUCTION,0.06867283950617284,"friendly, offering a new research paradigm that enhances recommendation by leveraging language-
87"
INTRODUCTION,0.06944444444444445,"based user feedback (see Section 4.3). Endowed with its inherent semantic comprehension of
88"
INTRODUCTION,0.07021604938271606,"language representations, AlphaRec can refine recommendations based on user intentions expressed
89"
INTRODUCTION,0.07098765432098765,"in natural language, enabling traditional CF recommenders to evolve into intention-aware systems
90"
INTRODUCTION,0.07175925925925926,"through a straightforward paradigm shift.
91"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.07253086419753087,"2
Uncovering Collaborative Signals in LMs via Linear Mapping
92"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.07330246913580248,"In this section, we aim to explore whether LMs implicitly encode collaborative signals in their
93"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.07407407407407407,"representation spaces. We first formulate the personalized item recommendation task, then detail the
94"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.07484567901234568,"linear mapping and its empirical findings. Empirical evidence indicates a homomorphic relationship
95"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.07561728395061729,"between the representation spaces of advanced LMs and effective recommendation spaces.
96"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.0763888888888889,"Task formulation. Personalized item recommendation with implicit feedback aims to select items
97"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.07716049382716049,"i ∈I that best match user u’s preferences based on binary interaction data Y = [yui], where yui = 1
98"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.0779320987654321,"(yui = 0) indicates user u ∈U has (has not) interacted with item i [58]. The primary objective of
99"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.0787037037037037,"recommendation is to model the user-item interaction matrix Y using a scoring function ˆy : U × I →
100"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.07947530864197531,"R, where ˆyui measures u’s preference for i. The scoring function ˆyui = s ◦ϕθ(xu, xi) comprises
101"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08024691358024691,"three key components: pre-existing features xu and xi for user u and item i, a representation learning
102"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08101851851851852,"module ϕθ(·, ·) parametrized by θ, and a similarity function s(·, ·). The representation learning
103"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08179012345679013,"module ϕθ transfers u and i into representations eu and ei for similarity matching s(eu, ei), and the
104"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08256172839506173,"Top-K highest scoring items are recommended to u.
105"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08333333333333333,"Different recommenders employ various pre-existing features xu, xi and representation learning
106"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08410493827160494,"architecture ϕθ(·, ·). Traditional ID-based recommenders use one-hot vectors as pre-existing features
107"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08487654320987655,"xu, xi. The choice of ID-based representation learning architecture ϕθ can vary widely, including
108"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08564814814814815,"ID-based embedding matrix [54], multilayer perception [61], graph neural network [52, 62], and
109"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08641975308641975,"variational autoencoder [63]. The commonly used similarity function is cosine similarity [64, 57]
110"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08719135802469136,"s(eu, ei) =
eu
⊤ei
∥eu∥·∥ei∥, which we adopt in this paper.
111"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08796296296296297,"Linear mapping. Building on the extensive knowledge encoded by LMs, we explore utilizing LMs
112"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08873456790123457,"as feature extractors, leveraging the language representations of item titles as initial item feature xi.
113"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.08950617283950617,"For initial user feature xu, we use the average of the title representations of historically interacted
114"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.09027777777777778,"items, defined as xu =
1
|Nu|
P"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.09104938271604938,"i∈Nu xi, where Nu is the set of items user u has interacted with.
115"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.09182098765432099,"Detailed procedures for obtaining these language-based features are provided in Appendix B.2.
116"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.09259259259259259,"We select a trainable linear mapping matrix W as the representation learning module ϕθ, setting
117"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.0933641975308642,"eu = W xu and ei = W xi. To learn the linear mapping W , we adopt the InfoNCE loss [56] as the
118"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.0941358024691358,"objective function, which has demonstrated state-of-the-art performance in both ID-based [65, 66]
119"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.09490740740740741,"and LM-enhanced collaborative filtering (CF) recommendations [47] (refer to Equation (4) for the
120"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.09567901234567901,"formula). The overall framework of the linear mapping process is illustrated in Figure 1a. We directly
121"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.09645061728395062,"use linearly mapped representations eu and ei to calculate the user-item similarity s(eu, ei) for
122"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.09722222222222222,"recommendation. High performance on the test set would suggest that collaborative signals (i.e., user
123"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.09799382716049383,Table 1: The recommendation performance of linear mapping comparing with classical CF baselines.
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.09876543209876543,"Books
Movies & TV
Video Games
Recall
NDCG
HR
Recall
NDCG
HR
Recall
NDCG
HR"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.09953703703703703,"MF (Rendle et al., 2012)
0.0437
0.0391
0.2476
0.0568
0.0519
0.3377
0.0323
0.0195
0.0864
MultVAE (Liang et al., 2018)
0.0722
0.0597
0.3418
0.0853
0.0776
0.4434
0.0908
0.0531
0.2211
LightGCN (He et al., 2021)
0.0723
0.0608
0.3489
0.0849
0.0747
0.4397
0.1007
0.0590
0.2281"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.10030864197530864,"Linear Mapping
BERT
0.0226
0.0194
0.1240
0.0415
0.0399
0.2362
0.0524
0.0309
0.1245
RoBERTa
0.0247
0.0209
0.1262
0.0406
0.0387
0.2277
0.0578
0.0338
0.1339
Llama2-7B
0.0662
0.0559
0.3176
0.1027
0.0955
0.4952
0.1249
0.0729
0.2746
Mistral-7B
0.0650
0.0544
0.3124
0.1039
0.0963
0.4994
0.1270
0.0687
0.2428
text-embedding-ada-v2
0.0515
0.0436
0.2570
0.0926
0.0874
0.4563
0.1176
0.0683
0.2579
text-embeddings-3-large
0.0735
0.0608
0.3355
0.1109
0.1023
0.5200
0.1367
0.0793
0.2928
SFR-Embedding-Mistral
0.0738
0.0610
0.3371
0.1152
0.1065
0.5327
0.1370
0.0787
0.2927"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.10108024691358025,"preference similarities between items) have been implicitly encoded in the language representation
124"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.10185185185185185,"space [67, 10].
125"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.10262345679012345,"Empirical findings. We compare the recommendation performance of the linear mapping method
126"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.10339506172839506,"with three classical CF baselines, matrix factorization (MF) [54, 68], MultVAE [63], and LightGCN
127"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.10416666666666667,"[55] (see more details about baselines in Appendix C.2.1). We report three widely used metrics Hit
128"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.10493827160493827,"Ratio (HR@K), Recall@K, Normalized Discounted Cumulative Gain (NDCG@K)) to evaluate
129"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.10570987654320987,"the effectiveness of linear mapping, with K set by default at 20. We evaluate a wide range of LMs,
130"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.10648148148148148,"including BERT-style models [4, 5], decoder-only language models [6, 69], and LM-based text
131"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.10725308641975309,"embedding models [70, 71] (see Appendix B.1 for details about used LMs).
132"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.10802469135802469,"Table 1 reports the recommendation performance yielded by the linear mapping on three Amazon
133"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.1087962962962963,"datasets [1], comparing with classic CF baselines. We observe that the performance of most advanced
134"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.1095679012345679,"text embedding models (e.g., text-embeddings-3-large [70] and SFR-Embedding-Mistral [71]) exceed
135"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.11033950617283951,"LightGCN on all datasets. We further empirically prove that these improvements do not merely
136"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.1111111111111111,"come from the better feature encoding ability (refer to Appendix B.3). These findings indicate
137"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.11188271604938271,"the homomorphic relationship between the language representation space of advanced LMs and an
138"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.11265432098765432,"effective item representation space for recommendation. Moreover, with the advances in LMs, the
139"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.11342592592592593,"performance of item representation linearly mapped from LMs exhibits a rising trend, gradually
140"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.11419753086419752,"surpassing traditional ID-based CF models. Representations from early BERT-style models (e.g.,
141"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.11496913580246913,"BERT [4] and RoBERTa [5]) only show weaker or equal capabilities compared with MF, while the
142"
UNCOVERING COLLABORATIVE SIGNALS IN LMS VIA LINEAR MAPPING,0.11574074074074074,"performance of decoder-only LMs (e.g., Llama-7B [6] ) start to match MultVAE and LightGCN.
143"
ALPHAREC,0.11651234567901235,"3
AlphaRec
144"
ALPHAREC,0.11728395061728394,"This finding of space homomorphic relationship sheds light on building advanced CF models purely
145"
ALPHAREC,0.11805555555555555,"based on LM representations without introducing ID-based embeddings. To be specific, we try to
146"
ALPHAREC,0.11882716049382716,"incorporate only three simple components (i.e., nonlinear projection [61], graph convolution [55]
147"
ALPHAREC,0.11959876543209877,"and contrastive learning (CL) objectives [56]), to develop a simple yet effective CF model called
148"
ALPHAREC,0.12037037037037036,"AlphaRec. It is important to highlight that our approach is centered on exploring the potential of
149"
ALPHAREC,0.12114197530864197,"LM representations for CF by integrating essential components from leading CF models, rather than
150"
ALPHAREC,0.12191358024691358,"deliberately inventing new CF mechanisms. We present the model structure of AlphaRec in Section
151"
ALPHAREC,0.12268518518518519,"3.1, and compare AlphaRec with two popular recommendation paradigms in Section 3.2.
152"
METHOD,0.12345679012345678,"3.1
Method
153"
METHOD,0.12422839506172839,"We present how AlphaRec is designed and trained. Generally, the representation learning architecture
154"
METHOD,0.125,"ϕθ(·, ·) of AlphaRec is simple, which only contains a two-layer MLP and the basic graph convolution
155"
METHOD,0.1257716049382716,"operation, with language representations as the input features xu, xi. The cosine similarity is used as
156"
METHOD,0.12654320987654322,"the similarity function s(·, ·), and the contrastive loss InfoNCE [56, 57] is adopted for optimization.
157"
METHOD,0.12731481481481483,"For simplicity, we consistently adopt text-embeddings-3-large [70] as the language representation
158"
METHOD,0.12808641975308643,"model, for its excellent language understanding and representation capabilities.
159"
METHOD,0.12885802469135801,"Nonlinear projection. In AlphaRec, we substitute the linear mapping matrix delineated in Section 2
160"
METHOD,0.12962962962962962,"with a nonlinear MLP. This conversion from linear to nonlinear is non-trivial, for the paradigm shift
161"
METHOD,0.13040123456790123,"from ID-based embeddings to LM representations, since nonlinear transformation helps in excavating
162"
METHOD,0.13117283950617284,"more comprehensive collaborative signals from the LM representation space with rich semantics (see
163"
METHOD,0.13194444444444445,"discussions about this in Appendix C.2.3) [61]. Specifically, we project the language representation
164"
METHOD,0.13271604938271606,"xi of the item title to an item space for recommendation with the two-layer MLP, and obtain user
165"
METHOD,0.13348765432098766,"representations as the average of historical items:
166"
METHOD,0.13425925925925927,"e(0)
i
= W2 LeakyReLU (W1xi + b1) + b2,
e(0)
u
=
1
|Nu| X"
METHOD,0.13503086419753085,"i∈Nu
e(0)
i .
(1)"
METHOD,0.13580246913580246,"Graph convolution. Graph neural networks (GNNs) have shown superior effectiveness for recom-
167"
METHOD,0.13657407407407407,"mendation [52, 55], owing to the natural user-item graph structure in recommender systems [72].
168"
METHOD,0.13734567901234568,"In AlphaRec, we employ a minimal graph convolution operation [55] to capture more complicated
169"
METHOD,0.1381172839506173,"collaborative signals from high-order connectivity [55, 73, 74, 72] as follows:
170"
METHOD,0.1388888888888889,"e(k+1)
u
=
X i∈Nu"
P,0.1396604938271605,"1
p"
P,0.1404320987654321,"|Nu|
p"
P,0.1412037037037037,"|Ni|
e(k)
i
,
e(k+1)
i
=
X u∈Ni"
P,0.1419753086419753,"1
p"
P,0.1427469135802469,"|Ni|
p"
P,0.14351851851851852,"|Nu|
e(k)
u .
(2)"
P,0.14429012345679013,"The information of connected neighbors is aggregated with a symmetric normalization term
171 1
√ |Nu|√"
P,0.14506172839506173,"|Ni|. Here Nu (Ni) denotes the historical item (user) set that user u (item i) has inter-
172"
P,0.14583333333333334,"acted with. The features e(0)
u
and e(0)
i
projected from the MLP are used as the input of the first layer.
173"
P,0.14660493827160495,"After propagating for K layers, the final representation of a user (item) is obtained as the average of
174"
P,0.14737654320987653,"features from each layer:
175"
P,0.14814814814814814,"eu =
1
K + 1 K
X"
P,0.14891975308641975,"k=0
e(k)
u ,
ei =
1
K + 1 K
X"
P,0.14969135802469136,"k=0
e(k)
i
.
(3)"
P,0.15046296296296297,"Contrastive learning objective. The introduction of contrasting learning is another key element for
176"
P,0.15123456790123457,"the success of leading CF models. Recent research suggests that the contrast learning objective, rather
177"
P,0.15200617283950618,"than data augmentation, plays a more significant role in improving recommendation performance
178"
P,0.1527777777777778,"[66, 75, 65]. Therefore, we simply use the contrast learning object InfoNCE [56] as the loss function
179"
P,0.15354938271604937,"without any additional data augmentation on the graph [76, 57]. With cosine similarity as the
180"
P,0.15432098765432098,"similarity function s(eu, ei) =
eu
⊤ei
∥eu∥·∥ei∥, the InfoNCE loss [56, 76, 77] is written as:
181"
P,0.1550925925925926,"LInfoNCE = −
X"
P,0.1558641975308642,"(u,i)∈O+
log
exp (s(u, i)/τ)
exp (s(u, i)/τ) + P"
P,0.1566358024691358,"j∈Su exp (s(u, j)/τ).
(4)"
P,0.1574074074074074,"Here, τ is a hyperparameter called temperature [78], O+ = {(u, i)|yui = 1} denoting the observed
182"
P,0.15817901234567902,"interactions between users U and items I. And Su is a randomly sampled subset of negative items
183"
P,0.15895061728395063,"that user u does not adopt.
184"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.1597222222222222,"3.2
Discussion of Recommendation Paradigms
185"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16049382716049382,"We compare the language-representation-based AlphaRec with two popular recommendation
186"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16126543209876543,"paradigms in Table 2 (see more discussion about related works in Appendix A).
187"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16203703703703703,"ID-based recommendation (ID-Rec) [52, 54]. In the traditional ID-based recommendation paradigm,
188"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16280864197530864,"users and items are represented by ID-based learnable embeddings derived from a large number of
189"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16358024691358025,"user interactions. While ID-Rec exhibits excellent recommendation capabilities with low training and
190"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16435185185185186,"inference costs [62, 76], it also has two significant drawbacks. Firstly, these ID-based embeddings
191"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16512345679012347,"learned in specific domains are difficult to transfer to new domains without overlapping users
192"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16589506172839505,"and items [37], thereby hindering zero-shot recommendation capabilities. Additionally, there is a
193"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16666666666666666,"substantial gap between ID-Rec and natural languages [34], which makes ID-based recommenders
194"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16743827160493827,"hard to incorporate language-based user intentions and further refine recommendations accordingly.
195"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16820987654320987,"LM-based recommendation (LM-Rec) [15, 16, 24]. Benefitting from the extensive world knowledge
196"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.16898148148148148,"and powerful reasoning capabilities of LMs [7, 79], the LM-based recommendation paradigm has
197"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.1697530864197531,"gained widespread attention [11, 13]. LM-Rec tends to convert user interaction history into text
198"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.1705246913580247,"prompts as input for LMs, utilizing pre-trained or fine-tuned LMs in a text generation pattern to
199"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.1712962962962963,"recommend items. LM-Rec demonstrates zero-shot and few-shot abilities and can easily understand
200"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.1720679012345679,"language-based user intentions. However, LM-Rec faces significant challenges. Firstly, the LM-based
201"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.1728395061728395,"model architecture leads to huge training and inference costs, with real-world deployment difficulties.
202"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.1736111111111111,Table 2: Comparison of recommendation paradigms
DISCUSSION OF RECOMMENDATION PARADIGMS,0.1743827160493827,"Recommendation Paradigms
Training Cost
Zero-shot Ability
Intention-aware Ability"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.17515432098765432,"ID-based
Low
%
%
LLM-based
High
""
""
Language-representation-based
Low
""
"""
DISCUSSION OF RECOMMENDATION PARADIGMS,0.17592592592592593,"Additionally, limited by the text generation paradigm, LM-based models tend to perform candidate
203"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.17669753086419754,"selection [17] or generate a single next item [24]. It remains difficult for LM-Rec to comprehensively
204"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.17746913580246915,"rank the entire item corpus or recommend multiple items that align with user interests.
205"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.17824074074074073,"Language-representation-based recommendation. We argue that AlphaRec follows a new CF
206"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.17901234567901234,"paradigm, which we term the language-representation-based paradigm. This paradigm replaces
207"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.17978395061728394,"the ID-based embeddings in ID-Rec with representations from pre-trained LMs, employing feature
208"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.18055555555555555,"encoders to map LM representations directly into the recommendation space. Few early studies lie in
209"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.18132716049382716,"this paradigm, including using BERT-style LMs to learn universal sequence representations [37, 44],
210"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.18209876543209877,"or adopting the same model architecture as ID-Rec with simple input features replacement [34, 35].
211"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.18287037037037038,"These early explorations, which are mostly based on BERT-style LMs, are usually only applicable in
212"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.18364197530864199,"certain specific scenarios, such as the transductive setting with the help of ID-based embeddings [37].
213"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.18441358024691357,"This phenomenon is consistent with our previous findings in Section 2, indicating that BERT-style
214"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.18518518518518517,"LMs may fail to effectively encode collaborative signals. We point out that AlphaRec is the first
215"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.18595679012345678,"recommender in the language-representation-based paradigm to surpass the traditional ID-based
216"
DISCUSSION OF RECOMMENDATION PARADIGMS,0.1867283950617284,"paradigm on multiple tasks, faithfully demonstrating the effectiveness and potential of this paradigm.
217"
EXPERIMENTS,0.1875,"4
Experiments
218"
EXPERIMENTS,0.1882716049382716,"In this section, we aim to explore the effectiveness of AlphaRec. Specifically, we are trying to answer
219"
EXPERIMENTS,0.18904320987654322,"the following research questions:
220"
EXPERIMENTS,0.18981481481481483,"• RQ1: How does AlphaRec perform compared with leading ID-based CF methods?
221"
EXPERIMENTS,0.19058641975308643,"• RQ2: Can AlphaRec learn general item representations, and achieve good zero-shot recommenda-
222"
EXPERIMENTS,0.19135802469135801,"tion performance on entirely new datasets?
223"
EXPERIMENTS,0.19212962962962962,"• RQ3: Can AlphaRec capture user intention described in natural language and adjust the recom-
224"
EXPERIMENTS,0.19290123456790123,"mendation results accordingly?
225"
EXPERIMENTS,0.19367283950617284,"4.1
General Recommendation Performance (RQ1)
226"
EXPERIMENTS,0.19444444444444445,"Motivation. We aim to explore whether the language-representation-based recommendation paradigm
227"
EXPERIMENTS,0.19521604938271606,"can outperform the ID-Rec paradigm. An excellent performance of AlphaRec would shed light on
228"
EXPERIMENTS,0.19598765432098766,"the research line of building representation-based recommenders in the future.
229"
EXPERIMENTS,0.19675925925925927,"Baselines. We only consider ID-based baselines in this section. We ignore LM-based methods due to
230"
EXPERIMENTS,0.19753086419753085,"two practical difficulties: the huge inference cost on datasets with millions of interactions and the
231"
EXPERIMENTS,0.19830246913580246,"task limitation of candidate selection or next item prediction. In addition to classic baselines (i.e., MF,
232"
EXPERIMENTS,0.19907407407407407,"MultVAE, and LightGCN) introduced in section 2, we consider two categories of leading ID-based
233"
EXPERIMENTS,0.19984567901234568,"CF baselines: CL-based CF methods: SGL [80], BC Loss [76], XSimGCL [66] and LM-enhanced
234"
EXPERIMENTS,0.2006172839506173,"methods: KAR [48], RLMRec [47]. See more details about baselines in Appendix C.2.1.
235"
EXPERIMENTS,0.2013888888888889,"Results. Table 3 presents the performance of AlphaRec compared with leading CF baselines. The
236"
EXPERIMENTS,0.2021604938271605,"best-performing methods are bold, while the second-best methods are underlined. Figure 2a and
237"
EXPERIMENTS,0.2029320987654321,"Figure 2b report the training efficiency and ablation results. We observe that:
238"
EXPERIMENTS,0.2037037037037037,"• AlphaRec consistently outperforms leading CF baselines by a large margin across all metrics
239"
EXPERIMENTS,0.2044753086419753,"on all datasets. AlphaRec shows an improvement ranging from 6.79% to 9.75% on Recall@20
240"
EXPERIMENTS,0.2052469135802469,"compared to the best baseline RLMRec [47]. We further conduct the ablation study to explore the
241"
EXPERIMENTS,0.20601851851851852,"reason for its success (see more ablation results in Appendix C.2.2). As shown in Figure 2b, each
242"
EXPERIMENTS,0.20679012345679013,"component in AlphaRec contributes positively. Specifically, the performance degradation caused by
243"
EXPERIMENTS,0.20756172839506173,"replacing the MLP with a linear weight matrix (w/o MLP) indicates that nonlinear transformations
244"
EXPERIMENTS,0.20833333333333334,"can further extract the implicit collaborative signals encoded in the LM representation space.
245"
EXPERIMENTS,0.20910493827160495,"Table 3: The performance comparison with ID-based CF baselines. The improvement achieved by
AlphaRec is significant (p-value << 0.05)."
EXPERIMENTS,0.20987654320987653,"Books
Movies & TV
Video Games
Recall
NDCG
HR
Recall
NDCG
HR
Recall
NDCG
HR"
EXPERIMENTS,0.21064814814814814,"MF (Rendle et al., 2012)
0.0437
0.0391
0.2476
0.0568
0.0519
0.3377
0.0323
0.0195
0.0864
MultVAE (Liang et al., 2018)
0.0722
0.0597
0.3418
0.0853
0.0776
0.4434
0.0908
0.0531
0.2211
LightGCN (He et al., 2021)
0.0723
0.0608
0.3489
0.0849
0.0747
0.4397
0.1007
0.0590
0.2281"
EXPERIMENTS,0.21141975308641975,"SGL (Wu et al., 2021)
0.0789
0.0657
0.3734
0.0916
0.0838
0.4680
0.1089
0.0634
0.2449
BC Loss (Zhang et al., 2022)
0.0915
0.0779
0.4045
0.1039
0.0943
0.5037
0.1145
0.0668
0.2561
XSimGCL (Yu et al., 2024)
0.0879
0.0745
0.3918
0.1057
0.0984
0.5128
0.1138
0.0662
0.2550"
EXPERIMENTS,0.21219135802469136,"KAR (Xi et al., 2023)
0.0852
0.0734
0.3834
0.1084
0.1001
0.5134
0.1181
0.0693
0.2571
RLMRec (Ren et al., 2024)
0.0928
0.0774
0.4092
0.1119
0.1013
0.5301
0.1384
0.0809
0.2997"
EXPERIMENTS,0.21296296296296297,"AlphaRec
0.0991*
0.0828*
0.4185*
0.1221*
0.1144*
0.5587*
0.1519*
0.0894*
0.3207*"
EXPERIMENTS,0.21373456790123457,"Imp.% over the best baseline
6.79%
5.34%
2.27%
9.12%
10.75%
5.40%
9.75%
10.51%
7.01%"
EXPERIMENTS,0.21450617283950618,"(a) Training efficiency comparison
(b) Ablation study on Books
Figure 2: (2a) The bar charts show the number of epochs needed for each model to converge.
AlphaRec tends to exhibit an extremely fast convergence speed. (2b) The effect of each component
in AlphaRec on Books dataset."
EXPERIMENTS,0.2152777777777778,"Moreover, the performance drop from replacing InfoNCE loss [57] with BPR loss [68] (w/o CL)
246"
EXPERIMENTS,0.21604938271604937,"and removing the graph convolution (w/o GCN) suggests that explicitly modeling the collaborative
247"
EXPERIMENTS,0.21682098765432098,"relationships through the loss function and model architecture can further enhance recommendation
248"
EXPERIMENTS,0.2175925925925926,"performance. These findings suggest that, by carefully designing the model to extract collaborative
249"
EXPERIMENTS,0.2183641975308642,"signals, the language-representation-based paradigm can surpass the ID-Rec paradigm.
250"
EXPERIMENTS,0.2191358024691358,"• The incorporation of semantic LM representations into traditional ID-based CF methods can
251"
EXPERIMENTS,0.2199074074074074,"lead to significant performance improvements. We note that two LM-enhanced CF methods,
252"
EXPERIMENTS,0.22067901234567902,"KAR and RLMRec, both show improvements over CL-based CF methods. Nevertheless, the com-
253"
EXPERIMENTS,0.22145061728395063,"bination of ID-based embeddings and LM representations in these methods does not yield higher
254"
EXPERIMENTS,0.2222222222222222,"results than purely language-representation-based AlphaRec. We attribute this phenomenon to the
255"
EXPERIMENTS,0.22299382716049382,"fact that the performance contribution of these methods mainly comes from the LM representations,
256"
EXPERIMENTS,0.22376543209876543,"which is consistent with the previous findings [34, 44].
257"
EXPERIMENTS,0.22453703703703703,"• AlphaRec exhibits fast convergence speed. We find that the convergence speed of AlphaRec is
258"
EXPERIMENTS,0.22530864197530864,"comparable with, or even surpasses, CL-based methods with data augmentation (e.g., SGL [80]
259"
EXPERIMENTS,0.22608024691358025,"and XSimGCL [66]). Meanwhile, methods based solely on graph convolution (LightGCN [55]) or
260"
EXPERIMENTS,0.22685185185185186,"CL objective (BC Loss [76]) show relatively slow convergence speed, indicating that introducing
261"
EXPERIMENTS,0.22762345679012347,"these modules may not lead to convergence speed improvement. Therefore, we attribute the fast
262"
EXPERIMENTS,0.22839506172839505,"convergence speed of AlphaRec to the homomorphic relationship between the LM representation
263"
EXPERIMENTS,0.22916666666666666,"space and a good recommendation space, so only minor adjustments to the LM representations are
264"
EXPERIMENTS,0.22993827160493827,"needed for recommendation.
265"
EXPERIMENTS,0.23070987654320987,"4.2
Zero-shot Recommendation Performance on Entirely New Datasets (RQ2)
266"
EXPERIMENTS,0.23148148148148148,"Motivation. We aim to explore whether AlphaRec has learned general item representations [37],
267"
EXPERIMENTS,0.2322530864197531,"which enables it to perform well on entirely new datasets without any user and item overlap.
268"
EXPERIMENTS,0.2330246913580247,"Task and datasets. In zero-shot recommendation [38], there is not any item or user overlap between
269"
EXPERIMENTS,0.2337962962962963,"the training set and test set [38, 33], which is different from the research line of cross-domain
270"
EXPERIMENTS,0.2345679012345679,"recommendation in ID-Rec [81]. We jointly train AlphaRec on three source datasets (i.e., Books,
271"
EXPERIMENTS,0.2353395061728395,"Movies & TV, and Video Games), while testing it on three completely new target datasets (i.e.,
272"
EXPERIMENTS,0.2361111111111111,"Table 4: The zero-shot recommendation performance comparison on entirely new datasets. The
improvement achieved by AlphaRec is significant (p-value << 0.05)."
EXPERIMENTS,0.2368827160493827,"Industrial
MovieLens-1M
Book Crossing
Recall
NDCG
HR
Recall
NDCG
HR
Recall
NDCG
HR full"
EXPERIMENTS,0.23765432098765432,"MF (Rendle et al., 2012)
0.0344
0.0225
0.0521
0.1855
0.3765
0.9634
0.0316
0.0317
0.2382
MultVAE (Liang et al., 2018)
0.0751
0.0459
0.1125
0.2039
0.3741
0.9740
0.0736
0.0634
0.3716
LightGCN (He et al., 2021)
0.0785
0.0533
0.1078
0.2019
0.4017
0.9715
0.0630
0.0588
0.3475"
EXPERIMENTS,0.23842592592592593,zero-shot
EXPERIMENTS,0.23919753086419754,"Random
0.0148
0.0061
0.0248
0.0068
0.0185
0.2611
0.0039
0.0036
0.0443
Pop
0.0216
0.0087
0.0396
0.0253
0.0679
0.5439
0.0119
0.0101
0.1157
ZESRec (Ding et al., 2021)
0.0326
0.0272
0.0628
0.0274
0.0787
0.5786
0.0155
0.0143
0.1347
UniSRec (Hou et al., 2022)
0.0453
0.0350
0.0863
0.0578
0.1412
0.7135
0.0396
0.0332
0.2454
AlphaRec
0.0913*
0.0573
0.1277*
0.1486*
0.3215*
0.9296*
0.0660*
0.0545*
0.3381*"
EXPERIMENTS,0.23996913580246915,"Imp.% over the best zero-shot baseline
157.09%
127.69%
30.29%
66.67%
64.16%
37.78%
101.55%
63.71%
47.97%"
EXPERIMENTS,0.24074074074074073,"Movielens-1M [59], Book Crossing [60], and Industrial [1]) without further training on these new
273"
EXPERIMENTS,0.24151234567901234,"datasets. (see more details about how we train AlphaRec on multiple datasets in Appendix C.3.1).
274"
EXPERIMENTS,0.24228395061728394,"Baselines. Due to the lack of zero-shot recommenders in the field of general recommendation, we
275"
EXPERIMENTS,0.24305555555555555,"slightly modify two zero-shot methods in the sequential recommendation [82], ZESRec [37] and
276"
EXPERIMENTS,0.24382716049382716,"UniSRec [37], as baselines. We also incorporate two strategy-based CF methods, Random and Pop
277"
EXPERIMENTS,0.24459876543209877,"(see more details about these baselines in Appendix C.3.2).
278"
EXPERIMENTS,0.24537037037037038,"Results. Table 4 presents the zero-shot recommendation performance comparison on entirely new
279"
EXPERIMENTS,0.24614197530864199,"datasets. The best-performing methods are bold and starred, while the second-best methods are
280"
EXPERIMENTS,0.24691358024691357,"underlined. We observe that:
281"
EXPERIMENTS,0.24768518518518517,"• AlphaRec demonstrates strong zero-shot recommendation capabilities, comparable to or even
282"
EXPERIMENTS,0.24845679012345678,"surpassing the fully trained LightGCN. On datasets from completely different platforms (e.g.,
283"
EXPERIMENTS,0.2492283950617284,"MovieLens-1M and Book Crossing), AlphaRec is comparable with the fully trained LightGCN.
284"
EXPERIMENTS,0.25,"On the same Amazon platform dataset, Industrial, AlphaRec even surpasses LightGCN, which we
285"
EXPERIMENTS,0.2507716049382716,"attribute to the possibility that AlphaRec implicitly learns unique user behavioral patterns on the
286"
EXPERIMENTS,0.2515432098765432,"Amazon platform [1]. Conversely, ZESRec and UniSRec exhibit a marked performance decrement
287"
EXPERIMENTS,0.2523148148148148,"compared with AlphaRec. We attribute this phenomenon to two aspects. On the one hand, BERT-
288"
EXPERIMENTS,0.25308641975308643,"style LMs [4, 5] used in these works may not have effectively encoded collaborative signals, which
289"
EXPERIMENTS,0.25385802469135804,"is consistent with our findings in Section 2. On the other hand, components designed for the
290"
EXPERIMENTS,0.25462962962962965,"next item prediction task in sequential recommendation [83] may not be suitable for capturing the
291"
EXPERIMENTS,0.25540123456790126,"general preferences of users in CF scenarios.
292"
EXPERIMENTS,0.25617283950617287,"• The zero-shot recommendation capability of AlphaRec generally benefits from an increased
293"
EXPERIMENTS,0.2569444444444444,"amount of training data, without harming the performance on source datasets. As illustrated
294"
EXPERIMENTS,0.25771604938271603,"in Figure 8, the zero-shot performance of AlphaRec, when trained on a mixed dataset, is generally
295"
EXPERIMENTS,0.25848765432098764,"superior to training on one single dataset [37]. Additionally, we also note that training data with
296"
EXPERIMENTS,0.25925925925925924,"themes similar to the target domain contributes more to the zero-shot performance. For instance, the
297"
EXPERIMENTS,0.26003086419753085,"zero-shot capability on MovieLens-1M may primarily stem from Movies & TV. Furthermore, we
298"
EXPERIMENTS,0.26080246913580246,"discover that AlphaRec, when trained jointly on multiple datasets, hardly experiences a performance
299"
EXPERIMENTS,0.26157407407407407,"decline on each source dataset. These findings further point to the general recommendation
300"
EXPERIMENTS,0.2623456790123457,"capability of a single pre-trained AlphaRec across multiple datasets. The above findings also offer
301"
EXPERIMENTS,0.2631172839506173,"a potential research path to achieve general recommendation capabilities, by incorporating more
302"
EXPERIMENTS,0.2638888888888889,"training data with more themes. See more details about these results in Appendix C.3.3.
303"
EXPERIMENTS,0.2646604938271605,"4.3
User Intention Capture Performance (RQ3)
304"
EXPERIMENTS,0.2654320987654321,"Motivation. We aim to investigate whether a straightforward paradigm shift enables pre-trained
305"
EXPERIMENTS,0.2662037037037037,"AlphaRec to perceive text-based user intentions and refine recommendations.
306"
EXPERIMENTS,0.26697530864197533,"Task and datasets. We test the user intention capture ability of AlphaRec on MovieLens-1M and
307"
EXPERIMENTS,0.26774691358024694,"Video Games. In the test set, only one target item remains for each user [84], with one intention
308"
EXPERIMENTS,0.26851851851851855,"query generated by ChatGPT [85, 40] (see the details about how to generate and check these intention
309"
EXPERIMENTS,0.2692901234567901,"queries in Appendix C.4.1). In the training stage, we follow the same procedure as illustrated in
310"
EXPERIMENTS,0.2700617283950617,"Section 2 to train AlphaRec. In the inference stage, we obtain the LM representation eIntention
u
311"
EXPERIMENTS,0.2708333333333333,"for each user intention query and combine it with the original user representation to get a new user
312"
EXPERIMENTS,0.2716049382716049,"representation as ˜e(0)
u
= (1−α)e(0)
u +αeIntention
u
[84]. This new user representation is sent into the
313"
EXPERIMENTS,0.27237654320987653,"freezed AlphaRec for recommendation. We report a relatively small K = 5 for all metrics to better
314"
EXPERIMENTS,0.27314814814814814,"reflect the intention capture accuracy.
315"
EXPERIMENTS,0.27391975308641975,Table 5: The performance comparison in user intention capture.
EXPERIMENTS,0.27469135802469136,"MovieLens-1M
Video Games
HR@5
NDCG@5
HR@5
NDCG@5"
EXPERIMENTS,0.27546296296296297,"TEM (Bi et al., 2020)
0.2738
0.1973
0.2212
0.1425
AlphaRec (w/o Intention)
0.0793
0.0498
0.0663
0.0438
AlphaRec (w Intention)
0.4704*
0.3738*
0.2569*
0.1862*"
EXPERIMENTS,0.2762345679012346,"(a) Case study of user intention capture
(b) Effect of α
Figure 3: User intention capture experiments on MovieLens-1M. (3a) AlphaRec refines the recom-
mendations according to language-based user intention. (3b) The effect of user intention strength α."
EXPERIMENTS,0.2770061728395062,"User intention capture results. Table 5 represents the user intention capture experiment results,
316"
EXPERIMENTS,0.2777777777777778,"compared with the baseline TEM [86]. Clearly, the introduction of user intention (w Intention)
317"
EXPERIMENTS,0.2785493827160494,"significantly refines the recommendations of the pre-trained AlphaRec (w/o Intention). Moreover,
318"
EXPERIMENTS,0.279320987654321,"AlphaRec outperforms the baseline model TEM by a large margin, even without additional training
319"
EXPERIMENTS,0.2800925925925926,"on search tasks. We further conduct a case study on MovieLens-1M to demonstrate how AlphaRec
320"
EXPERIMENTS,0.2808641975308642,"captures the user (see more case study results in Appendix C.4.3). As shown in Figure 3a, AlphaRec
321"
EXPERIMENTS,0.2816358024691358,"accurately captures the hidden user intention for “Godfather”, while keeping most of the recommen-
322"
EXPERIMENTS,0.2824074074074074,"dation results unchanged. This indicates that AlphaRec captures the user intention and historical
323"
EXPERIMENTS,0.283179012345679,"interests simultaneously.
324"
EXPERIMENTS,0.2839506172839506,"Effect of the intention strength α. By controlling the value of α, AlphaRec can provide better
325"
EXPERIMENTS,0.2847222222222222,"recommendation results, with a balance between user historical interests and user intent capture.
326"
EXPERIMENTS,0.2854938271604938,"Figure 3b depicts the effect of α. Initially, as α increases, the recommendation performance rises
327"
EXPERIMENTS,0.2862654320987654,"accordingly, indicating that incorporating user intention enables AlphaRec to provide better rec-
328"
EXPERIMENTS,0.28703703703703703,"ommendation results. However, as the α approaches 1, the recommendation performance starts to
329"
EXPERIMENTS,0.28780864197530864,"decrease, which suggests that the user historical interests learned by AlphaRec also play a vital role.
330"
EXPERIMENTS,0.28858024691358025,"The similar effect of α on Video Games is discussed in Appendix C.4.4.
331"
LIMITATIONS,0.28935185185185186,"5
Limitations
332"
LIMITATIONS,0.29012345679012347,"There are several limitations not addressed in this paper. On the one hand, although we have demon-
333"
LIMITATIONS,0.2908950617283951,"strated the excellence of AlphaRec for multiple tasks on various offline datasets, the effectiveness of
334"
LIMITATIONS,0.2916666666666667,"online employment remains unclear. On the other hand, although we have successfully explored the
335"
LIMITATIONS,0.2924382716049383,"potential of language-representation-based recommenders by incorporating essential components in
336"
LIMITATIONS,0.2932098765432099,"leading CF models, we do not elaboratively focus on designing new components for CF models.
337"
CONCLUSION,0.29398148148148145,"6
Conclusion
338"
CONCLUSION,0.29475308641975306,"In this paper, we explored what knowledge about recommendations has been encoded in the LM
339"
CONCLUSION,0.29552469135802467,"representation space. Specifically, we found that the advanced LMs representation space exhibits
340"
CONCLUSION,0.2962962962962963,"a homomorphic relationship with an effective recommendation space. Based on this finding, we
341"
CONCLUSION,0.2970679012345679,"developed a simple yet effective CF model called AlphaRec, which exhibits good recommendation
342"
CONCLUSION,0.2978395061728395,"performance with zero-shot recommendation and user intent capture ability. We pointed out that
343"
CONCLUSION,0.2986111111111111,"AlphaRec follows a new recommendation paradigm, language-representation-based recommendation,
344"
CONCLUSION,0.2993827160493827,"which uses language representations from LMs to represent users and items and completely abandons
345"
CONCLUSION,0.3001543209876543,"ID-based embeddings. We believed that AlphaRec is an important stepping stone towards building
346"
CONCLUSION,0.30092592592592593,"general recommenders in the future.1
347"
THC BROADER IMPACT OF ALPHAREC WILL BE DETAILED IN APPENDIX E,0.30169753086419754,1Thc broader impact of AlphaRec will be detailed in Appendix E
REFERENCES,0.30246913580246915,"References
348"
REFERENCES,0.30324074074074076,"[1] Jianmo Ni, Jiacheng Li, and Julian J. McAuley. Justifying recommendations using distantly-
349"
REFERENCES,0.30401234567901236,"labeled reviews and fine-grained aspects. In EMNLP, 2019.
350"
REFERENCES,0.30478395061728397,"[2] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
351"
REFERENCES,0.3055555555555556,"learning research, 9(11), 2008.
352"
REFERENCES,0.30632716049382713,"[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
353"
REFERENCES,0.30709876543209874,"Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.
354"
REFERENCES,0.30787037037037035,"[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of
355"
REFERENCES,0.30864197530864196,"deep bidirectional transformers for language understanding. In ACL, 2019.
356"
REFERENCES,0.30941358024691357,"[5] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
357"
REFERENCES,0.3101851851851852,"Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT
358"
REFERENCES,0.3109567901234568,"pretraining approach. CoRR, abs/1907.11692, 2019.
359"
REFERENCES,0.3117283950617284,"[6] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
360"
REFERENCES,0.3125,"Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
361"
REFERENCES,0.3132716049382716,"Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,
362"
REFERENCES,0.3140432098765432,"Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony
363"
REFERENCES,0.3148148148148148,"Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
364"
REFERENCES,0.31558641975308643,"Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut
365"
REFERENCES,0.31635802469135804,"Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mi-
366"
REFERENCES,0.31712962962962965,"haylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi
367"
REFERENCES,0.31790123456790126,"Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,
368"
REFERENCES,0.31867283950617287,"Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
369"
REFERENCES,0.3194444444444444,"Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
370"
REFERENCES,0.32021604938271603,"Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open
371"
REFERENCES,0.32098765432098764,"foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023.
372"
REFERENCES,0.32175925925925924,"[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
373"
REFERENCES,0.32253086419753085,"Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
374"
REFERENCES,0.32330246913580246,"wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
375"
REFERENCES,0.32407407407407407,"Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
376"
REFERENCES,0.3248456790123457,"Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
377"
REFERENCES,0.3256172839506173,"dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.
378"
REFERENCES,0.3263888888888889,"In NeurIPS, 2020.
379"
REFERENCES,0.3271604938271605,"[8] Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda B. Viégas, Hanspeter Pfister, and
380"
REFERENCES,0.3279320987654321,"Martin Wattenberg. Emergent world representations: Exploring a sequence model trained on a
381"
REFERENCES,0.3287037037037037,"synthetic task. In ICLR, 2023.
382"
REFERENCES,0.32947530864197533,"[9] Ivan Vulic, Edoardo Maria Ponti, Robert Litschko, Goran Glavas, and Anna Korhonen. Probing
383"
REFERENCES,0.33024691358024694,"pretrained language models for lexical semantics. In EMNLP, 2020.
384"
REFERENCES,0.33101851851851855,"[10] Wes Gurnee and Max Tegmark.
Language models represent space and time.
CoRR,
385"
REFERENCES,0.3317901234567901,"abs/2310.02207, 2023.
386"
REFERENCES,0.3325617283950617,"[11] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang,
387"
REFERENCES,0.3333333333333333,"and Qing Li. Recommender systems in the era of large language models (llms). CoRR,
388"
REFERENCES,0.3341049382716049,"abs/2307.02046, 2023.
389"
REFERENCES,0.33487654320987653,"[12] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. Large language models for generative
390"
REFERENCES,0.33564814814814814,"recommendation: A survey and visionary discussions. CoRR, abs/2309.01157, 2023.
391"
REFERENCES,0.33641975308641975,"[13] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin,
392"
REFERENCES,0.33719135802469136,"Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. A survey on large language
393"
REFERENCES,0.33796296296296297,"models for recommendation. CoRR, abs/2305.19860, 2023.
394"
REFERENCES,0.3387345679012346,"[14] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu,
395"
REFERENCES,0.3395061728395062,"Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang. How can recommender systems
396"
REFERENCES,0.3402777777777778,"benefit from large language models: A survey. CoRR, abs/2306.05817, 2023.
397"
REFERENCES,0.3410493827160494,"[15] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. Rec-
398"
REFERENCES,0.341820987654321,"ommendation as instruction following: A large language model empowered recommendation
399"
REFERENCES,0.3425925925925926,"approach. CoRR, abs/2305.07001, 2023.
400"
REFERENCES,0.3433641975308642,"[16] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. Tallrec: An
401"
REFERENCES,0.3441358024691358,"effective and efficient tuning framework to align large language model with recommendation.
402"
REFERENCES,0.3449074074074074,"In RecSys, 2023.
403"
REFERENCES,0.345679012345679,"[17] Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, and Xiang Wang. Llara:
404"
REFERENCES,0.3464506172839506,"Aligning large language models with sequential recommenders. CoRR, abs/2312.02445, 2023.
405"
REFERENCES,0.3472222222222222,"[18] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language
406"
REFERENCES,0.3479938271604938,"model for recommender systems. CoRR, abs/2311.01343, 2023.
407"
REFERENCES,0.3487654320987654,"[19] Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, Ming Chen, and
408"
REFERENCES,0.34953703703703703,"Ji-Rong Wen. Adapting large language models by integrating collaborative semantics for
409"
REFERENCES,0.35030864197530864,"recommendation. CoRR, abs/2311.09049, 2023.
410"
REFERENCES,0.35108024691358025,"[20] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. Collm:
411"
REFERENCES,0.35185185185185186,"Integrating collaborative embeddings into large language models for recommendation. CoRR,
412"
REFERENCES,0.35262345679012347,"abs/2310.19488, 2023.
413"
REFERENCES,0.3533950617283951,"[21] Arpita Vats, Vinija Jain, Rahul Raja, and Aman Chadha. Exploring the impact of large
414"
REFERENCES,0.3541666666666667,"language models on recommender systems: An extensive review. CoRR, abs/2402.18590,
415"
REFERENCES,0.3549382716049383,"2024.
416"
REFERENCES,0.3557098765432099,"[22] Chengkai Huang, Tong Yu, Kaige Xie, Shuai Zhang, Lina Yao, and Julian J. McAuley. Founda-
417"
REFERENCES,0.35648148148148145,"tion models for recommender systems: A survey and new perspectives. CoRR, abs/2402.11143,
418"
REFERENCES,0.35725308641975306,"2024.
419"
REFERENCES,0.35802469135802467,"[23] Lanling Xu, Junjie Zhang, Bingqian Li, Jinpeng Wang, Mingchen Cai, Wayne Xin Zhao, and
420"
REFERENCES,0.3587962962962963,"Ji-Rong Wen. Prompting large language models for recommender systems: A comprehensive
421"
REFERENCES,0.3595679012345679,"framework and empirical analysis. CoRR, abs/2401.04997, 2024.
422"
REFERENCES,0.3603395061728395,"[24] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation
423"
REFERENCES,0.3611111111111111,"as language processing (RLP): A unified pretrain, personalized prompt & predict paradigm
424"
REFERENCES,0.3618827160493827,"(P5). In RecSys, 2022.
425"
REFERENCES,0.3626543209876543,"[25] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. M6-rec: Generative
426"
REFERENCES,0.36342592592592593,"pretrained language models are open-ended recommender systems. CoRR, abs/2205.08084,
427"
REFERENCES,0.36419753086419754,"2022.
428"
REFERENCES,0.36496913580246915,"[26] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming
429"
REFERENCES,0.36574074074074076,"Tang, Yong Yu, and Weinan Zhang. Rella: Retrieval-enhanced large language models for
430"
REFERENCES,0.36651234567901236,"lifelong sequential behavior comprehension in recommendation. CoRR, abs/2308.11131, 2023.
431"
REFERENCES,0.36728395061728397,"[27] Zhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang
432"
REFERENCES,0.3680555555555556,"Wang, and Xiangnan He. Large language model can interpret latent space of sequential
433"
REFERENCES,0.36882716049382713,"recommender. CoRR, abs/2310.20487, 2023.
434"
REFERENCES,0.36959876543209874,"[28] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan, Trung Vu,
435"
REFERENCES,0.37037037037037035,"Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q. Tran, Jonah Samost, Maciej Kula, Ed H. Chi,
436"
REFERENCES,0.37114197530864196,"and Mahesh Sathiamoorthy. Recommender systems with generative retrieval. In Alice Oh,
437"
REFERENCES,0.37191358024691357,"Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,
438"
REFERENCES,0.3726851851851852,"NeurIPS, 2023.
439"
REFERENCES,0.3734567901234568,"[29] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. ONCE: boosting content-based
440"
REFERENCES,0.3742283950617284,"recommendation with both open- and closed-source large language models. In Luz Angelica
441"
REFERENCES,0.375,"Caudillo-Mata, Silvio Lattanzi, Andrés Muñoz Medina, Leman Akoglu, Aristides Gionis, and
442"
REFERENCES,0.3757716049382716,"Sergei Vassilvitskii, editors, WSDM, 2024.
443"
REFERENCES,0.3765432098765432,"[30] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong,
444"
REFERENCES,0.3773148148148148,"Fangda Gu, Michael He, Yinghai Lu, and Yu Shi. Actions speak louder than words: Trillion-
445"
REFERENCES,0.37808641975308643,"parameter sequential transducers for generative recommendations. CoRR, abs/2402.17152,
446"
REFERENCES,0.37885802469135804,"2024.
447"
REFERENCES,0.37962962962962965,"[31] Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. How to index item ids for
448"
REFERENCES,0.38040123456790126,"recommendation foundation models. In Qingyao Ai, Yiqin Liu, Alistair Moffat, Xuanjing
449"
REFERENCES,0.38117283950617287,"Huang, Tetsuya Sakai, and Justin Zobel, editors, SIGIR-AP, 2023.
450"
REFERENCES,0.3819444444444444,"[32] Guanghu Yuan, Fajie Yuan, Yudong Li, Beibei Kong, Shujie Li, Lei Chen, Min Yang, Chenyun
451"
REFERENCES,0.38271604938271603,"Yu, Bo Hu, Zang Li, Yu Xu, and Xiaohu Qie. Tenrec: A large-scale multipurpose benchmark
452"
REFERENCES,0.38348765432098764,"dataset for recommender systems. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle
453"
REFERENCES,0.38425925925925924,"Belgrave, K. Cho, and A. Oh, editors, NeurIPS, 2022.
454"
REFERENCES,0.38503086419753085,"[33] Jiaqi Zhang, Yu Cheng, Yongxin Ni, Yunzhu Pan, Zheng Yuan, Junchen Fu, Youhua Li,
455"
REFERENCES,0.38580246913580246,"Jie Wang, and Fajie Yuan. Ninerec: A benchmark dataset suite for evaluating transferable
456"
REFERENCES,0.38657407407407407,"recommendation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.
457"
REFERENCES,0.3873456790123457,"[34] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin
458"
REFERENCES,0.3881172839506173,"Ni. Where to go next for recommender systems? ID- vs. modality-based recommender models
459"
REFERENCES,0.3888888888888889,"revisited. In SIGIR, 2023.
460"
REFERENCES,0.3896604938271605,"[35] Ruyu Li, Wenhao Deng, Yu Cheng, Zheng Yuan, Jiaqi Zhang, and Fajie Yuan. Exploring the
461"
REFERENCES,0.3904320987654321,"upper limits of text-based collaborative filtering using large language models: Discoveries and
462"
REFERENCES,0.3912037037037037,"insights. CoRR, abs/2305.11700, 2023.
463"
REFERENCES,0.39197530864197533,"[36] Youhua Li, Hanwen Du, Yongxin Ni, Pengpeng Zhao, Qi Guo, Fajie Yuan, and Xiaofang Zhou.
464"
REFERENCES,0.39274691358024694,"Multi-modality is all you need for transferable recommender systems. CoRR, abs/2312.09602,
465"
REFERENCES,0.39351851851851855,"2023.
466"
REFERENCES,0.3942901234567901,"[37] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen.
467"
REFERENCES,0.3950617283950617,"Towards universal sequence representation learning for recommender systems. In KDD, pages
468"
REFERENCES,0.3958333333333333,"585–593. ACM, 2022.
469"
REFERENCES,0.3966049382716049,"[38] Hao Ding, Yifei Ma, Anoop Deoras, Yuyang Wang, and Hao Wang. Zero-shot recommender
470"
REFERENCES,0.39737654320987653,"systems. CoRR, abs/2105.08318, 2021.
471"
REFERENCES,0.39814814814814814,"[39] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian J. McAuley.
472"
REFERENCES,0.39891975308641975,"Text is all you need: Learning language representations for sequential recommendation. In
473"
REFERENCES,0.39969135802469136,"KDD, 2023.
474"
REFERENCES,0.40046296296296297,"[40] Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian J. McAuley. Bridging
475"
REFERENCES,0.4012345679012346,"language and items for retrieval and recommendation. CoRR, abs/2403.03952, 2024.
476"
REFERENCES,0.4020061728395062,"[41] Yupeng Hou, Zhankui He, Julian J. McAuley, and Wayne Xin Zhao. Learning vector-quantized
477"
REFERENCES,0.4027777777777778,"item representation for transferable sequential recommenders. In Ying Ding, Jie Tang, Juan F.
478"
REFERENCES,0.4035493827160494,"Sequeda, Lora Aroyo, Carlos Castillo, and Geert-Jan Houben, editors, WWW, 2023.
479"
REFERENCES,0.404320987654321,"[42] Zhiming Mao, Huimin Wang, Yiming Du, and Kam-Fai Wong. Unitrec: A unified text-to-text
480"
REFERENCES,0.4050925925925926,"transformer and joint contrastive learning framework for text-based recommendation. In ACL,
481"
REFERENCES,0.4058641975308642,"2023.
482"
REFERENCES,0.4066358024691358,"[43] Zhaopeng Qiu, Xian Wu, Jingyue Gao, and Wei Fan. U-BERT: pre-training user representations
483"
REFERENCES,0.4074074074074074,"for improved recommendation. In Thirty-Fifth AAAI Conference on Artificial Intelligence,
484"
REFERENCES,0.408179012345679,"AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI
485"
REFERENCES,0.4089506172839506,"2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021,
486"
REFERENCES,0.4097222222222222,"Virtual Event, February 2-9, 2021. AAAI, 2021.
487"
REFERENCES,0.4104938271604938,"[44] Lingzi Zhang, Xin Zhou, Zhiwei Zeng, and Zhiqi Shen. Are ID embeddings necessary?
488"
REFERENCES,0.4112654320987654,"whitening pre-trained text embeddings for effective sequential recommendation.
CoRR,
489"
REFERENCES,0.41203703703703703,"abs/2402.10602, 2024.
490"
REFERENCES,0.41280864197530864,"[45] Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. Parameter-efficient
491"
REFERENCES,0.41358024691358025,"transfer from sequential behaviors for user modeling and recommendation. In SIGIR, 2020.
492"
REFERENCES,0.41435185185185186,"[46] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang,
493"
REFERENCES,0.41512345679012347,"Dawei Yin, and Chao Huang. Llmrec: Large language models with graph augmentation for
494"
REFERENCES,0.4158950617283951,"recommendation. In Luz Angelica Caudillo-Mata, Silvio Lattanzi, Andrés Muñoz Medina,
495"
REFERENCES,0.4166666666666667,"Leman Akoglu, Aristides Gionis, and Sergei Vassilvitskii, editors, WSDM, 2024.
496"
REFERENCES,0.4174382716049383,"[47] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and
497"
REFERENCES,0.4182098765432099,"Chao Huang. Representation learning with large language models for recommendation. CoRR,
498"
REFERENCES,0.41898148148148145,"abs/2310.15950, 2024.
499"
REFERENCES,0.41975308641975306,"[48] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang,
500"
REFERENCES,0.42052469135802467,"Rui Zhang, and Yong Yu. Towards open-world recommendation with knowledge augmentation
501"
REFERENCES,0.4212962962962963,"from large language models. CoRR, abs/2306.10933, 2023.
502"
REFERENCES,0.4220679012345679,"[49] Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou,
503"
REFERENCES,0.4228395061728395,"and Linjian Mo. Breaking the length barrier: Llm-enhanced CTR prediction in long textual
504"
REFERENCES,0.4236111111111111,"user behaviors. CoRR, abs/2403.19347, 2024.
505"
REFERENCES,0.4243827160493827,"[50] An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua.
506"
REFERENCES,0.4251543209876543,"On generative agents in recommendation. CoRR, abs/2310.10108, 2023.
507"
REFERENCES,0.42592592592592593,"[51] Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian J. McAuley, Wayne Xin Zhao,
508"
REFERENCES,0.42669753086419754,"Leyu Lin, and Ji-Rong Wen. Agentcf: Collaborative learning with autonomous language
509"
REFERENCES,0.42746913580246915,"agents for recommender systems. CoRR, abs/2310.09233, 2023.
510"
REFERENCES,0.42824074074074076,"[52] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph
511"
REFERENCES,0.42901234567901236,"collaborative filtering. In SIGIR, 2019.
512"
REFERENCES,0.42978395061728397,"[53] Yang Li, Tong Chen, Yadan Luo, Hongzhi Yin, and Zi Huang. Discovering collaborative
513"
REFERENCES,0.4305555555555556,"signals for next POI recommendation with iterative seq2graph augmentation. In IJCAI, 2021.
514"
REFERENCES,0.43132716049382713,"[54] Yehuda Koren, Robert M. Bell, and Chris Volinsky. Matrix factorization techniques for
515"
REFERENCES,0.43209876543209874,"recommender systems. Computer, 42(8):30–37, 2009.
516"
REFERENCES,0.43287037037037035,"[55] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng Wang. Lightgcn:
517"
REFERENCES,0.43364197530864196,"Simplifying and powering graph convolution network for recommendation. In SIGIR, 2021.
518"
REFERENCES,0.43441358024691357,"[56] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive
519"
REFERENCES,0.4351851851851852,"predictive coding. CoRR, abs/1807.03748, 2018.
520"
REFERENCES,0.4359567901234568,"[57] Jiancan Wu, Xiang Wang, Xingyu Gao, Jiawei Chen, Hongcheng Fu, Tianyu Qiu, and Xi-
521"
REFERENCES,0.4367283950617284,"angnan He. On the effectiveness of sampled softmax loss for item recommendation. CoRR,
522"
REFERENCES,0.4375,"abs/2201.02327, 2022.
523"
REFERENCES,0.4382716049382716,"[58] Steffen Rendle. Item recommendation from implicit feedback. In Recommender Systems
524"
REFERENCES,0.4390432098765432,"Handbook. Springer US, 2022.
525"
REFERENCES,0.4398148148148148,"[59] F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. ACM
526"
REFERENCES,0.44058641975308643,"Trans. Interact. Intell. Syst., 5(4):19:1–19:19, 2016.
527"
REFERENCES,0.44135802469135804,"[60] Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. Melu: Meta-
528"
REFERENCES,0.44212962962962965,"learned user preference estimator for cold-start recommendation. In KDD, 2019.
529"
REFERENCES,0.44290123456790126,"[61] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural
530"
REFERENCES,0.44367283950617287,"collaborative filtering. In WWW, 2017.
531"
REFERENCES,0.4444444444444444,"[62] Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. Lightgcl: Simple yet effective graph
532"
REFERENCES,0.44521604938271603,"contrastive learning for recommendation. In ICLR, 2023.
533"
REFERENCES,0.44598765432098764,"[63] Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. Variational
534"
REFERENCES,0.44675925925925924,"autoencoders for collaborative filtering. In WWW, 2018.
535"
REFERENCES,0.44753086419753085,"[64] Jiawei Chen, Junkang Wu, Jiancan Wu, Xuezhi Cao, Sheng Zhou, and Xiangnan He. Adap-τ :
536"
REFERENCES,0.44830246913580246,"Adaptively modulating embedding magnitude for recommendation. In WWW, 2023.
537"
REFERENCES,0.44907407407407407,"[65] An Zhang, Leheng Sheng, Zhibo Cai, Xiang Wang, and Tat-Seng Chua. Empowering collabo-
538"
REFERENCES,0.4498456790123457,"rative filtering with principled adversarial contrastive loss. In NeurIPS, 2023.
539"
REFERENCES,0.4506172839506173,"[66] Junliang Yu, Xin Xia, Tong Chen, Lizhen Cui, Nguyen Quoc Viet Hung, and Hongzhi Yin.
540"
REFERENCES,0.4513888888888889,"Xsimgcl: Towards extremely simple graph contrastive learning for recommendation. IEEE
541"
REFERENCES,0.4521604938271605,"Trans. Knowl. Data Eng., 36(2):913–926, 2024.
542"
REFERENCES,0.4529320987654321,"[67] Abhilasha Ravichander, Yonatan Belinkov, and Eduard H. Hovy. Probing the probing paradigm:
543"
REFERENCES,0.4537037037037037,"Does probing accuracy entail task relevance? In EACL, 2021.
544"
REFERENCES,0.45447530864197533,"[68] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. BPR:
545"
REFERENCES,0.45524691358024694,"bayesian personalized ranking from implicit feedback. CoRR, abs/1205.2618, 2012.
546"
REFERENCES,0.45601851851851855,"[69] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
547"
REFERENCES,0.4567901234567901,"Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu-
548"
REFERENCES,0.4575617283950617,"cile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
549"
REFERENCES,0.4583333333333333,"Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. CoRR,
550"
REFERENCES,0.4591049382716049,"abs/2310.06825, 2023.
551"
REFERENCES,0.45987654320987653,"[70] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek,
552"
REFERENCES,0.46064814814814814,"Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav
553"
REFERENCES,0.46141975308641975,"Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr,
554"
REFERENCES,0.46219135802469136,"Felipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov,
555"
REFERENCES,0.46296296296296297,"Joanne Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive
556"
REFERENCES,0.4637345679012346,"pre-training. CoRR, abs/2201.10005, 2022.
557"
REFERENCES,0.4645061728395062,"[71] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfr-
558"
REFERENCES,0.4652777777777778,"embedding-mistral:enhance text retrieval with transfer learning. Salesforce AI Research Blog,
559"
REFERENCES,0.4660493827160494,"2024. URL https://blog.salesforceairesearch.com/sfr-embedded-mistral/.
560"
REFERENCES,0.466820987654321,"[72] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in
561"
REFERENCES,0.4675925925925926,"recommender systems: A survey. ACM Comput. Surv., 55(5):97:1–97:37, 2023.
562"
REFERENCES,0.4683641975308642,"[73] Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q.
563"
REFERENCES,0.4691358024691358,"Weinberger. Simplifying graph convolutional networks. In ICML, 2019.
564"
REFERENCES,0.4699074074074074,"[74] Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. Revisiting graph based
565"
REFERENCES,0.470679012345679,"collaborative filtering: A linear residual graph convolutional network approach. In AAAI, 2020.
566"
REFERENCES,0.4714506172839506,"[75] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen.
567"
REFERENCES,0.4722222222222222,"Are graph augmentations necessary?: Simple graph contrastive learning for recommendation.
568"
REFERENCES,0.4729938271604938,"In SIGIR, 2022.
569"
REFERENCES,0.4737654320987654,"[76] An Zhang, Wenchang Ma, Xiang Wang, and Tat-Seng Chua. Incorporating bias-aware margins
570"
REFERENCES,0.47453703703703703,"into contrastive loss for collaborative filtering. In NeurIPS, 2022.
571"
REFERENCES,0.47530864197530864,"[77] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
572"
REFERENCES,0.47608024691358025,"Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and
573"
REFERENCES,0.47685185185185186,"Ilya Sutskever. Learning transferable visual models from natural language supervision. In
574"
REFERENCES,0.47762345679012347,"ICML, 2021.
575"
REFERENCES,0.4783950617283951,"[78] Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In CVPR,
576"
REFERENCES,0.4791666666666667,"2021.
577"
REFERENCES,0.4799382716049383,"[79] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian
578"
REFERENCES,0.4807098765432099,"Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng
579"
REFERENCES,0.48148148148148145,"Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie,
580"
REFERENCES,0.48225308641975306,"and Ji-Rong Wen. A survey of large language models. CoRR, abs/2303.18223, 2023.
581"
REFERENCES,0.48302469135802467,"[80] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie.
582"
REFERENCES,0.4837962962962963,"Self-supervised graph learning for recommendation. In SIGIR, 2021.
583"
REFERENCES,0.4845679012345679,"[81] Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, and Guanfeng Liu. Cross-domain
584"
REFERENCES,0.4853395061728395,"recommendation: Challenges, progress, and prospects. In IJCAI, 2021.
585"
REFERENCES,0.4861111111111111,"[82] Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z. Sheng, and Mehmet A. Orgun.
586"
REFERENCES,0.4868827160493827,"Sequential recommender systems: Challenges, progress and prospects. In IJCAI, 2019.
587"
REFERENCES,0.4876543209876543,"[83] Wang-Cheng Kang and Julian J. McAuley. Self-attentive sequential recommendation. In
588"
REFERENCES,0.48842592592592593,"ICDM, 2018.
589"
REFERENCES,0.48919753086419754,"[84] Qingyao Ai, Yongfeng Zhang, Keping Bi, Xu Chen, and W. Bruce Croft. Learning a hierarchi-
590"
REFERENCES,0.48996913580246915,"cal embedding model for personalized product search. In SIGIR, 2017.
591"
REFERENCES,0.49074074074074076,"[85] OpenAI. GPT-4 technical report. CoRR, 2023.
592"
REFERENCES,0.49151234567901236,"[86] Keping Bi, Qingyao Ai, and W. Bruce Croft. A transformer-based embedding model for
593"
REFERENCES,0.49228395061728397,"personalized product search. In SIGIR, 2020.
594"
REFERENCES,0.4930555555555556,"[87] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier
595"
REFERENCES,0.49382716049382713,"probes. In ICLR (Workshop), 2017.
596"
REFERENCES,0.49459876543209874,"[88] Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces. In
597"
REFERENCES,0.49537037037037035,"ICLR, 2022.
598"
REFERENCES,0.49614197530864196,"[89] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the
599"
REFERENCES,0.49691358024691357,"geometry of large language models. CoRR, abs/2311.03658, 2023.
600"
REFERENCES,0.4976851851851852,"[90] Xubin Ren, Wei Wei, Lianghao Xia, and Chao Huang. A comprehensive survey on self-
601"
REFERENCES,0.4984567901234568,"supervised learning for recommendation. arXiv preprint arXiv:2404.03354, 2024.
602"
REFERENCES,0.4992283950617284,"[91] Zheng Chen. PALR: personalization aware llms for recommendation. CoRR, abs/2305.07622,
603"
REFERENCES,0.5,"2023.
604"
REFERENCES,0.5007716049382716,"[92] Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou,
605"
REFERENCES,0.5015432098765432,"Liang Pang, and Xiao Wang. Can small language models be good reasoners for sequential
606"
REFERENCES,0.5023148148148148,"recommendation? CoRR, abs/2403.04260, 2024.
607"
REFERENCES,0.5030864197530864,"[93] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing
608"
REFERENCES,0.503858024691358,"Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.
609"
REFERENCES,0.5046296296296297,"[94] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian J. McAuley, and
610"
REFERENCES,0.5054012345679012,"Wayne Xin Zhao. Large language models are zero-shot rankers for recommender systems. In
611"
REFERENCES,0.5061728395061729,"ECIR, 2024.
612"
REFERENCES,0.5069444444444444,"[95] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. Is chatgpt a good recommender?
613"
REFERENCES,0.5077160493827161,"A preliminary study. CoRR, abs/2304.10149, 2023.
614"
REFERENCES,0.5084876543209876,"[96] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun,
615"
REFERENCES,0.5092592592592593,"Xiao Zhang, and Jun Xu. Uncovering chatgpt’s capabilities in recommender systems. In
616"
REFERENCES,0.5100308641975309,"RecSys, 2023.
617"
REFERENCES,0.5108024691358025,"[97] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. Chat-
618"
REFERENCES,0.5115740740740741,"rec: Towards interactive and explainable llms-augmented recommender system.
CoRR,
619"
REFERENCES,0.5123456790123457,"abs/2303.14524, 2023.
620"
REFERENCES,0.5131172839506173,"[98] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. Ctrl: Connect tabular and language
621"
REFERENCES,0.5138888888888888,"model for ctr prediction. arXiv preprint arXiv:2306.02841, 2023.
622"
REFERENCES,0.5146604938271605,"[99] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
623"
REFERENCES,0.5154320987654321,"thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez,
624"
REFERENCES,0.5162037037037037,"Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
625"
REFERENCES,0.5169753086419753,"language models. CoRR, abs/2302.13971, 2023.
626"
REFERENCES,0.5177469135802469,"[100] Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David
627"
REFERENCES,0.5185185185185185,"Bau. Function vectors in large language models. CoRR, abs/2310.15213, 2023.
628"
REFERENCES,0.5192901234567902,"[101] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,
629"
REFERENCES,0.5200617283950617,"Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language
630"
REFERENCES,0.5208333333333334,"models. In NeurIPS, 2022.
631"
REFERENCES,0.5216049382716049,"[102] Julian J. McAuley, Rahul Pandey, and Jure Leskovec. Inferring networks of substitutable and
632"
REFERENCES,0.5223765432098766,"complementary products. In KDD, 2015.
633"
REFERENCES,0.5231481481481481,"[103] Walid Krichene and Steffen Rendle. On sampled metrics for item recommendation. In KDD,
634"
REFERENCES,0.5239197530864198,"2020.
635"
REFERENCES,0.5246913580246914,"A
Related Works
636"
REFERENCES,0.5254629629629629,"Representations in LMs. The impressive capabilities demonstrated by LMs across various tasks
637"
REFERENCES,0.5262345679012346,"raise a wide concern about what they have learned in the representation space. An important and
638"
REFERENCES,0.5270061728395061,"effective approach for interpreting and analyzing representations of LMs is linear probing [67, 87].
639"
REFERENCES,0.5277777777777778,"The main idea of linear probing is simple: training linear classifiers to predict some specific attributes
640"
REFERENCES,0.5285493827160493,"or concepts (e.g., lexical structure [9] ) from the representations in the hidden layers of LMs. A high
641"
REFERENCES,0.529320987654321,"probing result (e.g., classification accuracy on the out-of-sample test set) tends to imply relevant
642"
REFERENCES,0.5300925925925926,"information has been implicitly encoded in the representation space of LMs, although this does
643"
REFERENCES,0.5308641975308642,"not imply LMs directly use these representations [67, 10]. Recent studies empirically demonstrate
644"
REFERENCES,0.5316358024691358,"that concepts such as color [88], game states [8]. and geographic position are encoded in LMs.
645"
REFERENCES,0.5324074074074074,"Furthermore, these concepts may even be linearly encoded in the representation space of LMs [8, 89].
646"
REFERENCES,0.533179012345679,"Collaborative filtering. Collaborative filtering (CF) [90] is an advanced technique in modern
647"
REFERENCES,0.5339506172839507,"recommender systems. The prevailing CF methods tend to adopt an ID-based paradigm, where users
648"
REFERENCES,0.5347222222222222,"and items are typically represented as one-hot vectors, with an embedding table used for lookup [54].
649"
REFERENCES,0.5354938271604939,"Usually, these embedding parameters are learned by optimizing specific loss functions to reconstruct
650"
REFERENCES,0.5362654320987654,"the history interaction pattern [68]. Recent advances in CF mainly benefit from two aspects, graph
651"
REFERENCES,0.5370370370370371,"convolution [72] and contrastive learning [90]. These CF models exhibit superior recommendation
652"
REFERENCES,0.5378086419753086,"performance by conducting the embedding propagation [52, 55] and applying contrastive learning
653"
REFERENCES,0.5385802469135802,"objectives [80, 62, 66]. However, although effective, these methods are still limited, due to the
654"
REFERENCES,0.5393518518518519,"ID-based paradigm. Since one-hot vectors contain no feature information beyond being identifiers, it
655"
REFERENCES,0.5401234567901234,"is challenging to transfer pre-trained ID embeddings to other domains [37] or to leverage leading
656"
REFERENCES,0.5408950617283951,"techniques from computer vision (CV) and natural language processing (NLP) [34].
657"
REFERENCES,0.5416666666666666,"LMs for recommendation. The remarkable language understanding and reasoning ability shown by
658"
REFERENCES,0.5424382716049383,"LMs has attracted extensive attention in the field of recommendation. The application of LMs in rec-
659"
REFERENCES,0.5432098765432098,"ommendation can be categorized into three main approaches: LM-enhanced recommendation, LM as
660"
REFERENCES,0.5439814814814815,"the modality encoder, and LLM-based recommendation. The first research direction, LLM-enhanced
661"
REFERENCES,0.5447530864197531,"recommendation, focuses on empowering traditional recommenders with the semantic representations
662"
REFERENCES,0.5455246913580247,"from LMs [48, 47, 46, 49, 91, 92]. Specifically, these methods introduce representations from LMs as
663"
REFERENCES,0.5462962962962963,"additional features for traditional ID-based recommenders, to capture complicated user preferences.
664"
REFERENCES,0.5470679012345679,"The second research line lies in adopting the LM as the text modality encoder, which is also known
665"
REFERENCES,0.5478395061728395,"as a kind of modality-based recommendation (MoRec) [34, 35]. These methods tend to train the
666"
REFERENCES,0.5486111111111112,"LM as the text modality encoder together with the traditional recommender. In previous studies,
667"
REFERENCES,0.5493827160493827,"BERT-style LMs are widely used as the text modality encoder. The third research line, LLM-based
668"
REFERENCES,0.5501543209876543,"recommendation, directly uses LLMs as the recommender and recommends items in a text generation
669"
REFERENCES,0.5509259259259259,"paradigm. Early attempts focus on adopting in-context learning (ICL) [93] and prompting pre-trained
670"
REFERENCES,0.5516975308641975,"LLMs [94–97]. However, such naive methods tend to yield poor performance compared to traditional
671"
REFERENCES,0.5524691358024691,"models. Therefore, recent studies concentrate on fine-tuning LLMs on recommendation-related cor-
672"
REFERENCES,0.5532407407407407,"pus [16, 15, 26, 25, 29] and align the LLMs with the representations from traditional recommenders
673"
REFERENCES,0.5540123456790124,"as the additional modality [17, 20, 27, 98].
674"
REFERENCES,0.5547839506172839,"B
Linear Mapping
675"
REFERENCES,0.5555555555555556,"B.1
Brief of Used LMs
676"
REFERENCES,0.5563271604938271,"We briefly introduce the LMs we use for linear mapping in Section 2.
677"
REFERENCES,0.5570987654320988,"• BERT [4] is an encoder-only language model based on the transformer architecture [3], pre-trained
678"
REFERENCES,0.5578703703703703,"on text corpus with unsupervised tasks. BERT adopts bidirectional self-attention heads to learn
679"
REFERENCES,0.558641975308642,"bidirectional representations.
680"
REFERENCES,0.5594135802469136,"• RoBERTa [5] is an enhanced version of BERT. RoBERTa preserves the architecture of BERT but
681"
REFERENCES,0.5601851851851852,"improves it by training with more data and large batches, adopting dynamic masking, and removing
682"
REFERENCES,0.5609567901234568,"the next sentence prediction objective.
683"
REFERENCES,0.5617283950617284,"• Llama2-7B [6] is an open-source decoder-only LLM with 7 billion parameters. Llama2 adopts
684"
REFERENCES,0.5625,"grouped-query attention, with longer context length and larger size of the pre-training corpus
685"
REFERENCES,0.5632716049382716,"compared with Llama-7B [99].
686"
REFERENCES,0.5640432098765432,Table 6: Linear mapping performance of randomly shuffled item representations
REFERENCES,0.5648148148148148,"Books
Movies & TV
Video Games
Recall
NDCG
HR
Recall
NDCG
HR
Recall
NDCG
HR"
REFERENCES,0.5655864197530864,"BERT
0.0226
0.0194
0.1240
0.0415
0.0399
0.2362
0.0524
0.0309
0.1245
text-embeddings-3-large (Random)
0.0200
0.0197
0.1316
0.0559
0.0528
0.3204
0.0562
0.0328
0.1351
text-embeddings-3-large
0.0735
0.0608
0.3355
0.1109
0.1023
0.5200
0.1367
0.0793
0.2928"
REFERENCES,0.566358024691358,Table 7: Dataset statistics.
REFERENCES,0.5671296296296297,"Books
Movies & TV
Video Games
Industrial
MovieLens-1M
Book Crossing"
REFERENCES,0.5679012345679012,"#Users
7,176
14,382
40,834
15,141
6,040
6,273
#Items
10,728
1,000
14,344
5,163
3,043
5,335
#Interactions
1,304,453
129,748
390,013
82,578
995,492
253,057
Density
0.0169
0.0090
0.0701
0.0010
0.0542
0.0076"
REFERENCES,0.5686728395061729,"• Mistral-7B [69] is an open-source pre-trained decoder-only LLM with 7 billion parameters. Mistral
687"
REFERENCES,0.5694444444444444,"7B leverages grouped-query attention, coupled with sliding window attention for faster and lower
688"
REFERENCES,0.5702160493827161,"cost inference.
689"
REFERENCES,0.5709876543209876,"• text-embedding-ada-v2 & text-embeddings-3-large [70] are leading text embedding models
690"
REFERENCES,0.5717592592592593,"released by OpenAI. These models are built upon decoder-only GPT models, pre-trained on
691"
REFERENCES,0.5725308641975309,"unsupervised data at scale with contrastive learning objectives.
692"
REFERENCES,0.5733024691358025,"• SFR-Embedding-Mistral [71] is a decoder-based text embedding model built upon the open-
693"
REFERENCES,0.5740740740740741,"source LLM Mixtral-7B [69]. SFR-Embedding-Mistral introduces task-homogeneous batching and
694"
REFERENCES,0.5748456790123457,"computes contrastive loss on “hard negatives”, which brings a better performance than the vanilla
695"
REFERENCES,0.5756172839506173,"Mixtral-7B model.
696"
REFERENCES,0.5763888888888888,"B.2
Extracting Representations from LMs
697"
REFERENCES,0.5771604938271605,"We present how to extract representations from LMs. For encoder-based LMs (e.g., BERT [4]
698"
REFERENCES,0.5779320987654321,"and RoBERTa [5]), we use the output representation corresponding to the [CLS] token [40]. For
699"
REFERENCES,0.5787037037037037,"decoder-based models (e.g., Llama-7B [6, 69], Mistral-7B, and SFR-Embedding-Mistral [71]),
700"
REFERENCES,0.5794753086419753,"we use the representation in the last transformer block [3], corresponding to the last input token
701"
REFERENCES,0.5802469135802469,"[10, 100, 70]. Especially, for the commercial closed-source model (e.g., text-embedding-ada-v2 and
702"
REFERENCES,0.5810185185185185,"text-embeddings-3-large 2 [70]), we directly call the API interface to obtain representations.
703"
REFERENCES,0.5817901234567902,"B.3
Empirical Findings
704"
REFERENCES,0.5825617283950617,"We find more evidence about representations in leading LM encode collaborative signals beyond
705"
REFERENCES,0.5833333333333334,"better feature encoding ability. We randomly shuffle item representations and conduct the same linear
706"
REFERENCES,0.5841049382716049,"mapping experiment. As illustrated in Table 6, randomly shuffled representations, text-embeddings-
707"
REFERENCES,0.5848765432098766,"3-large (Random), yield similar performance with BERT, lagging largely behind the vanilla linear
708"
REFERENCES,0.5856481481481481,"mapping method. These results indicate that BERT may only serve as a good feature encoder, while
709"
REFERENCES,0.5864197530864198,"the latest LM may further encode collaborative signals beyond naive feature encoding.
710"
REFERENCES,0.5871913580246914,"C
Experiments
711"
REFERENCES,0.5879629629629629,"C.1
Datasets
712"
REFERENCES,0.5887345679012346,"We incorporate six datasets in this paper, including four datasets from the Amazon platform 3 [1]
713"
REFERENCES,0.5895061728395061,"(i.e., Books, Movies & TV, Video Games, and Industrial), and two datasets from other platforms (i.e.,
714"
REFERENCES,0.5902777777777778,"MovieLens-1M and Book Crossing). Table 7 reports the data statistics of each dataset.
715"
REFERENCES,0.5910493827160493,"We divide the history interaction of each user into training, validation, and testing sets with a ratio
716"
REFERENCES,0.591820987654321,"of 4:3:3, and remove users with less than 20 interactions following previous studies [50]. We also
717"
REFERENCES,0.5925925925925926,"remove items from the testing and validation sets that do not appear in the training set, to address the
718"
REFERENCES,0.5933641975308642,"cold start problem.
719"
REFERENCES,0.5941358024691358,"2https://platform.openai.com/docs/guides/embeddings
3www.amazon.com"
REFERENCES,0.5949074074074074,Item Title Examples
REFERENCES,0.595679012345679,"Books:
Dismissed with Prejudice: A J.P. Beaumont Novel; Die for Love: A Jacqueline
Kirby Novel of Suspense; The Cloud; Memories Before and After the Sound of Music: An
Autobiography; Harry Potter and the Sorcerer’s Stone;
Movies & TV: Batman Begins; Fantastic Four; Max Headroom: The Complete Series;
Madagascar; Land of the Dead; King Kong;
Video Games: Fighting Force; Tomb Raider II; Tomb Raider; WWF Warzone; Kartia: The
Word of Fate; Snowboard Kids; Command & amp; Conquer: Tiberian Sun - PC; Final Fantasy
VII; Grim Fandango - PC; Half-Life - PC;
MovieLens-1M: Basquiat (1996); Tin Cup (1996); Godfather, The (1972); Supercop (1992);
Manny & Lo (1996); Bound (1996); Carpool (1996);
Book Crossing:
Prague : A Novel; Chocolate Jesus; Wie Barney es sieht; To Kill a
Mockingbird; Sturmzeit. Roman; A Soldier of the Great War; Pride and Prejudice (Dover
Thrift Editions);
Industrial: Jurassic Perisphinctes Ammonites from France; FS9140: Spinosaurus - Dinosaur
Tooth 20-30mm; FS9410: USA Eocene, Fossil Fish (Knightia alt), A-grade; Delta 50-857
Charcoal Filter for 50-868; Hitachi RP30SA 7-1/2 Gallon Stainless Steel Industrial Shop
Vacuum (Discontinued by Manufacturer); Makita 632002-4 14-Inch Cut-Off Wheels (5-Pack)
(Discontinued by Manufacturer); PORTER-CABLE 740001801 4 1/2-Inch by 10yd 180 Grit
Adhesive-Backed Sanding Roll;"
REFERENCES,0.5964506172839507,Figure 4: Example of item titles.
REFERENCES,0.5972222222222222,"In this paper, we only use the item titles as the text description. Figure 4 gives some item title
720"
REFERENCES,0.5979938271604939,"examples from different datasets.
721"
REFERENCES,0.5987654320987654,"C.2
General Recommendation
722"
REFERENCES,0.5995370370370371,"C.2.1
Baselines
723"
REFERENCES,0.6003086419753086,"We incorporate a series of CF models as our baselines for general recommendation. These models
724"
REFERENCES,0.6010802469135802,"are classified as classical CF methods (MF, MultVAE, and LightGCN), CL-based CF methods (SGL,
725"
REFERENCES,0.6018518518518519,"BC Loss, and XSimGCL), and LM-enhanced CF methods (KAR, RLMRec). For these LM-enhanced
726"
REFERENCES,0.6026234567901234,"CF methods, we adopt the leading CF method XSimGCL as the backbone.
727"
REFERENCES,0.6033950617283951,"• MF [54, 68] is the most basic CF model. It denotes users and items with ID-based embeddings and
728"
REFERENCES,0.6041666666666666,"conducts matrix factorization with Bayesian personalized ranking (BPR) loss.
729"
REFERENCES,0.6049382716049383,"• MultVAE [63] is a traditional CF model based on the variational autoencoder (VAE). It regards the
730"
REFERENCES,0.6057098765432098,"item recommendation as a generative process from a multinomial distribution and uses variational
731"
REFERENCES,0.6064814814814815,"inference to estimate parameters. We adopt the same model structure as suggested in the paper:
732"
REFERENCES,0.6072530864197531,"600 →200 →600.
733"
REFERENCES,0.6080246913580247,"• LightGCN [55] is a light graph convolution network tailored for the recommendation, which
734"
REFERENCES,0.6087962962962963,"deletes redundant feature transformation and activation function in NGCF [52].
735"
REFERENCES,0.6095679012345679,"• SGL [80] introduces graph contrastive learning into recommender models for the first time. By
736"
REFERENCES,0.6103395061728395,"employing node or edge dropout to generate augmented graph views and conduct contrastive
737"
REFERENCES,0.6111111111111112,"learning between two views, SGL achieves better performance than LightGCN.
738"
REFERENCES,0.6118827160493827,"• BC Loss [76] introduces a robust and model-agnostic contrastive loss, handling various data biases
739"
REFERENCES,0.6126543209876543,"in recommendation, especially for popularity bias.
740"
REFERENCES,0.6134259259259259,"• XSimGCL [66] directly generates augmented views by adding noise into the inner layer of
741"
REFERENCES,0.6141975308641975,"LightGCN without graph augmentation. The simplicity of XSimGCL leads to a faster convergence
742"
REFERENCES,0.6149691358024691,"speed and better performance.
743"
REFERENCES,0.6157407407407407,"• KAR [48] enhances recommender models by integrating knowledge from large language models
744"
REFERENCES,0.6165123456790124,"(LLMs). It generates textual descriptions of users and items and combine the LM representations
745"
REFERENCES,0.6172839506172839,"with traditional recommenders using a hybrid-expert adaptor.
746"
REFERENCES,0.6180555555555556,"(a) Ablation study on Movies & TV
(b) Ablation study on Video Games"
REFERENCES,0.6188271604938271,Figure 5: Ablation study
REFERENCES,0.6195987654320988,"(a) LM representations
(b) AlphaRec (w/o MLP)
(c) AlphaRec"
REFERENCES,0.6203703703703703,"Figure 6: The t-SNE visualization of representations on Movies & TV. (6a) The item representations
in the LM space. (6b) The item representations obtained by replacing the MLP with a linear mapping
matrix in AlphaRec. (6c) The item representations obtained from AlphaRec."
REFERENCES,0.621141975308642,"• RLMRec [47] aligns semantic representations of users and items with the representations in CF
747"
REFERENCES,0.6219135802469136,"models through a contrastive loss, as an additional loss trained together with the CF model. The
748"
REFERENCES,0.6226851851851852,"fusion of semantic information and collaborative information brings performance improvement.
749"
REFERENCES,0.6234567901234568,"C.2.2
Ablation Study
750"
REFERENCES,0.6242283950617284,"We conduct the same ablation study as introduced in Section 4.1 on Movies & TV and Video Games
751"
REFERENCES,0.625,"datasets. As illustrated in Figure 5, each component in AlphaRec contributes positively, which is
752"
REFERENCES,0.6257716049382716,"consistent with our findings in Section 4.1.
753"
REFERENCES,0.6265432098765432,"C.2.3
The t-SNE Visualization Comparison
754"
REFERENCES,0.6273148148148148,"In this section, we aim to intuitively explore how the MLP in AlphaRec further helps in excavating
755"
REFERENCES,0.6280864197530864,"collaborative signals in language representations, compared to the linear mapping matrix. We
756"
REFERENCES,0.628858024691358,"visualize the item representations from LMs, AlphaRec (w/o MLP), and AlphaRec in Figure 6, where
757"
REFERENCES,0.6296296296296297,"AlphaRec (w/o MLP) denotes replacing the MLP with a linear mapping matrix. We observed that
758"
REFERENCES,0.6304012345679012,"movies about superhero and monster cluster in all representation spaces, indicating both AlphaRec
759"
REFERENCES,0.6311728395061729,"(w/o MLP) and AlphaRec capture the preference similarities between these items and preserve
760"
REFERENCES,0.6319444444444444,"the clustering relationship. The difference between AlphaRec (w/o MLP) and AlphaRec may lie
761"
REFERENCES,0.6327160493827161,"in the ability to capture obscure preference similarities among items. As shown in Figure 6a,
762"
REFERENCES,0.6334876543209876,"homosexual movies are dispersed in the language space, indicating the possible semantic differences
763"
REFERENCES,0.6342592592592593,"between them. AlphaRec successfully captures the preference similarities and gathers these items
764"
REFERENCES,0.6350308641975309,"in the representation space, while AlphaRec (w/o MLP) remains some items dispersed. Moreover,
765"
REFERENCES,0.6358024691358025,"AlphaRec outperforms AlphaRec (w/o MLP) by a large margin, as indicated in Figure 5a. These
766"
REFERENCES,0.6365740740740741,"results indicate that AlphaRec exhibits a more fine-grained preference capture ability with the help of
767"
REFERENCES,0.6373456790123457,"nonlinear transformation.
768"
REFERENCES,0.6381172839506173,Table 8: The effect of the training dataset on zero-shot recommendation
REFERENCES,0.6388888888888888,"Industrial
MovieLens-1M
Book Crossing
Recall
NDCG
HR
Recall
NDCG
HR
Recall
NDCG
HR"
REFERENCES,0.6396604938271605,"AlphaRec (trained on Books)
0.0896
0.0562
0.1256
0.1218
0.2619
0.8942
0.0646
0.0532
0.3346"
REFERENCES,0.6404320987654321,"AlphaRec (trained on Movies & TV)
0.0909
0.0581
0.1266
0.1438
0.3122
0.9200
0.0471
0.0406
0.2600"
REFERENCES,0.6412037037037037,"AlphaRec (trained on Video Games)
0.0905
0.0567
0.1225
0.1221
0.2313
0.9034
0.0412
0.0378
0.2585"
REFERENCES,0.6419753086419753,"AlphaRec (trained on mixed dataset)
0.0913
0.0573
0.1277
0.1486
0.3215
0.9296
0.0660
0.0545
0.3381"
REFERENCES,0.6427469135802469,Table 9: Performance comparison between training on the single dataset and the mixed dataset
REFERENCES,0.6435185185185185,"Books
Movies & TV
Video Games
Recall
NDCG
HR
Recall
NDCG
HR
Recall
NDCG
HR"
REFERENCES,0.6442901234567902,"AlphaRec (trained on single dataset)
0.0991
0.0828
0.4185
0.1221
0.1144
0.5587
0.1519
0.0894
0.3207"
REFERENCES,0.6450617283950617,"AlphaRec (trained on mixed dataset)
0.0979
0.0818
0.4147
0.1194
0.1107
0.5463
0.1381
0.0827
0.2985"
REFERENCES,0.6458333333333334,"C.3
Zero-shot Recommendation
769"
REFERENCES,0.6466049382716049,"C.3.1
Co-training on Multiple Datasets
770"
REFERENCES,0.6473765432098766,"Co-training on multiple datasets is similar to training on one single dataset, where the only difference
771"
REFERENCES,0.6481481481481481,"lies in the negative sampling. When co-training on multiple datasets, the negative items are restricted
772"
REFERENCES,0.6489197530864198,"to the same dataset as the positive item rather than the full item pool. The other training procedures
773"
REFERENCES,0.6496913580246914,"remain the same with training on one single dataset.
774"
REFERENCES,0.6504629629629629,"C.3.2
Baselines
775"
REFERENCES,0.6512345679012346,"Since previous works about zero-shot recommendation mostly focus on sequential recommendation
776"
REFERENCES,0.6520061728395061,"[83, 82], we slightly modify two methods in sequential recommendation, ZESRec [38] and UniSRec
777"
REFERENCES,0.6527777777777778,"[37] as our baselines. Specifically, we maintain the model structure as provided in the paper, and
778"
REFERENCES,0.6535493827160493,"adopt the training paradigm of CF.
779"
REFERENCES,0.654320987654321,"• Random denotes randomly recommending items from the entire item pool.
780"
REFERENCES,0.6550925925925926,"• Pop denotes randomly recommending from the most popular items. Here popularity denotes the
781"
REFERENCES,0.6558641975308642,"number of users that have interacted with the item.
782"
REFERENCES,0.6566358024691358,"• ZESRec [38] is the first work that defines the problem of zero-shot recommendation. To address
783"
REFERENCES,0.6574074074074074,"this problem, this work introduces a hierarchical Bayesian model with representations from the
784"
REFERENCES,0.658179012345679,"pre-trained BERT.
785"
REFERENCES,0.6589506172839507,"• UniSRec [37] aims to learn universal item representations from BERT, with parametric whitening
786"
REFERENCES,0.6597222222222222,"and a MoE-enhanced adaptor. By pre-training on multiple source datasets, UniSRec can conduct
787"
REFERENCES,0.6604938271604939,"zero-shot recommendation on various datasets in a transductive or inductive paradigm.
788"
REFERENCES,0.6612654320987654,"C.3.3
The Effect of Training Datasets
789"
REFERENCES,0.6620370370370371,"The effect of the training dataset on zero-shot recommendation.
We report the zero-shot
790"
REFERENCES,0.6628086419753086,"recommendation performance differences trained on different datasets in Table 8. Here AlphaRec
791"
REFERENCES,0.6635802469135802,"(trained on Books) denotes training on a single Books dataset, while AlphaRec (trained on mixed
792"
REFERENCES,0.6643518518518519,"dataset) denotes co-training on three Amazon datasets. Generally, training on more datasets lead to a
793"
REFERENCES,0.6651234567901234,"better zero-shot performance.
794"
REFERENCES,0.6658950617283951,"The performance comparison between training on the single dataset and the mixed dataset. In
795"
REFERENCES,0.6666666666666666,"Table 9, AlphaRec (trained on single dataset) denotes training and testing on the same single dataset,
796"
REFERENCES,0.6674382716049383,"while AlphaRec (trained on mixed dataset) denotes training on three Amazon datasets and testing
797"
REFERENCES,0.6682098765432098,"on one single dataset. Generally, co-training on three Amazon datasets yields similar performance
798"
REFERENCES,0.6689814814814815,"compared with training on one single dataset. The only exception lies in Video Games, which shows
799"
REFERENCES,0.6697530864197531,"some performance degradation. We attribute this to the difference between the selection of τ. We use
800"
REFERENCES,0.6705246913580247,"τ = 0.15 when trained on the mixed dataset, while the optimal τ for Video Games lies around 0.2.
801"
REFERENCES,0.6712962962962963,"These results indicate that a single AlphaRec can capture user preferences among various datasets,
802"
REFERENCES,0.6720679012345679,"showcasing a general collaborative signal capture ability.
803"
REFERENCES,0.6728395061728395,"C.4
User Intention Capture
804"
REFERENCES,0.6736111111111112,"C.4.1
Intention Query Generation
805"
REFERENCES,0.6743827160493827,Intention Query Generation
REFERENCES,0.6751543209876543,"Input
You are an expert in generating queries for a target movie. Please help me generate the most
suitable query for the target movie within one sentence, following the given example.
Example:
TARGET: BUG-A-SALT 3.0 Black Fly Edition.
QUERY: I want a gun that I can use while gardening to get rid of stink bugs, ants, flies, and
spiders in my house. It needs to be amazing and help me feel less scared.
TARGET: Toy Story (1995)."
REFERENCES,0.6759259259259259,"Output
QUERY: I’m looking for a heartwarming animated movie that follows the adventures of a
group of toys who come to life when their owner is not around."
REFERENCES,0.6766975308641975,Figure 7: Example of item query generation.
REFERENCES,0.6774691358024691,"The user intention query is a natural language sentence implying the target item of interest. For
806"
REFERENCES,0.6782407407407407,"each item in the dataset, we generate a fixed user intention query. Following the previous work
807"
REFERENCES,0.6790123456790124,"[40], we generate user intention queries with the help of ChatGPT [85]. As shown in Figure 7, we
808"
REFERENCES,0.6797839506172839,"prompt ChatGPT in a Chain-of-Thought (CoT) [101] paradigm and adopt the output as the user
809"
REFERENCES,0.6805555555555556,"intention query. We adopt a rule-based strategy to ensure that the output query is in first person, and
810"
REFERENCES,0.6813271604938271,"regenerate the wrong query. Considering the huge amount of item title text, we use ChatGPT3.5 API
811"
REFERENCES,0.6820987654320988,"for generating all queries for the budget’s sake.
812"
REFERENCES,0.6828703703703703,"C.4.2
Baseline
813"
REFERENCES,0.683641975308642,"AlphaRec exhibits user intention capture abilities, although not specially designed for search tasks.
814"
REFERENCES,0.6844135802469136,"We compare AlphaRec with TEM [86] which falls in the field of personalized search [84, 102].
815"
REFERENCES,0.6851851851851852,"• TEM [86] uses a transformer to encode the intention query together with user history behaviors,
816"
REFERENCES,0.6859567901234568,"which enables it to achieve better search results by considering the user’s historical interest.
817"
REFERENCES,0.6867283950617284,"C.4.3
Case Study
818"
REFERENCES,0.6875,"We conduct two more case studies to verify the user intention capture ability of AlphaRec. As
819"
REFERENCES,0.6882716049382716,"illustrated in Figure 8 and Figure 9, AlphaRec provides proper recommendation results, including the
820"
REFERENCES,0.6890432098765432,"target item for the user intention at the top.
821"
REFERENCES,0.6898148148148148,"C.4.4
Effect of the Intention Strength Alpha
822"
REFERENCES,0.6905864197530864,"The value of α controls the balance between the user’s historical interests and the user intention
823"
REFERENCES,0.691358024691358,"query. A larger α incorporates more about the user intention while considering less about the user’s
824"
REFERENCES,0.6921296296296297,"historical interests. As shown in Figure 10, the effect of α on Video Games shows a similar trend
825"
REFERENCES,0.6929012345679012,"with MovieLens-1M.
826"
REFERENCES,0.6936728395061729,"C.5
Trainig Cost
827"
REFERENCES,0.6944444444444444,"We report the training cost of AlphaRec in this section. Table 10 reports the seconds needed per
828"
REFERENCES,0.6952160493827161,"epoch and the total training cost until convergence. Here Amazon-Mix denotes the mixed dataset of
829"
REFERENCES,0.6959876543209876,"Books, Movies & TV, and Video Games. It’s worth noting that AlphaRec converges quickly and only
830"
REFERENCES,0.6967592592592593,"requires a small amount of training time.
831"
REFERENCES,0.6975308641975309,Figure 8: Case study of user intention capture on MovieLens-1M
REFERENCES,0.6983024691358025,Figure 9: Case study of user intention capture on Video Games
REFERENCES,0.6990740740740741,"D
Hyperparameter Settings and Implementation Details
832"
REFERENCES,0.6998456790123457,"We conduct all the experiments in PyTorch with a single NVIDIA RTX A5000 (24G) GPU and a
833"
REFERENCES,0.7006172839506173,"64 AMD EPYC 7543 32-Core Processor CPU. We optimize all methods with the Adam optimizer.
834"
REFERENCES,0.7013888888888888,"For all ID-based CF methods, we set the layer numbers of graph propagation by default at 2, with
835"
REFERENCES,0.7021604938271605,"the embedding size as 64 and the size of sampled negative items |Su| as 256. We use the early stop
836"
REFERENCES,0.7029320987654321,"strategy to avoid overfitting. We stop the training process if the Recall@20 metric on the validation
837"
REFERENCES,0.7037037037037037,"set does not increase for 20 successive evaluations. In AlphaRec, the dimensions of the input and
838"
REFERENCES,0.7044753086419753,"output in the two-layer MLP are 3072 and 64 respectively, with the hidden layer dimension as 1536.
839"
REFERENCES,0.7052469135802469,"We apply the all-ranking strategy [103] for all experiments, which ranks all items except positive ones
840"
REFERENCES,0.7060185185185185,"in the training set for each user. We search hyperparameters for baselines according to the suggestion
841"
REFERENCES,0.7067901234567902,"in the literature. The hyperparameter search space is reported in Table 11. For these LM-enhanced
842"
REFERENCES,0.7075617283950617,"models, KAR and RLMRec, we also search the hyperparameter of their backbone XSimGCL.
843"
REFERENCES,0.7083333333333334,"For AlphaRec, the only hyperparameter is the temperature τ and we search it in [0.05, 2]. We report
844"
REFERENCES,0.7091049382716049,"the temperature τ we used for each dataset in Table 12. For the mixed dataset Amazon-Mix in
845"
REFERENCES,0.7098765432098766,"Section 4.2, we use a universal τ = 0.15. We adopt τ = 0.2 for the MovieLens-1M dataset for the user
846"
REFERENCES,0.7106481481481481,"intention capture experiment in Section 4.3.
847"
REFERENCES,0.7114197530864198,"E
Broader Impact
848"
REFERENCES,0.7121913580246914,"The proposed AlphaRec can significantly improve the performance of zero-shot recommendation
849"
REFERENCES,0.7129629629629629,"and the capability of user intent capture, offering a good approach to crafting more personalized
850"
REFERENCES,0.7137345679012346,"recommendation results. One concern of AlphaRec is the potential for the representations generated
851"
REFERENCES,0.7145061728395061,"by language models can be maliciously attacked, which may result in erroneous or unexpected
852"
REFERENCES,0.7152777777777778,"recommendations. Therefore, we kindly advise researchers to cautiously check the quality of the
853"
REFERENCES,0.7160493827160493,"language representations before using AlphaRec.
854"
REFERENCES,0.716820987654321,Figure 10: Effect of α on Video Games
REFERENCES,0.7175925925925926,Table 10: Training cost of AlphaRec (seconds per epoch/in total).
REFERENCES,0.7183641975308642,"Books
Movies & TV
Video Games
Amazon-Mix"
REFERENCES,0.7191358024691358,"AlphaRec
40.1 / 1363.4
12.3 / 479.7
7.4 / 214.6
107.2 / 5788.8"
REFERENCES,0.7199074074074074,Table 11: Hyperparameters search spaces for baselines.
REFERENCES,0.720679012345679,Hyperparameter space
REFERENCES,0.7214506172839507,"MF & LightGCN
lr ∼{1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3}"
REFERENCES,0.7222222222222222,"MultVAE
dropout ratio ∼{0, 0.2, 0.5}, β ∼{0.2, 0.4, 0.6, 0.8}"
REFERENCES,0.7229938271604939,"SGL
τ ∼[0.05, 2], λ1 ∼{0.005, 0.01, 0.05, 0.1, 0.5, 1.0}, ρ ∼{0, 0.1, 0.2, 0.3, 0.4, 0.5}"
REFERENCES,0.7237654320987654,"BC Loss
τ1 ∼[0.05, 3], τ2 ∼[0.05, 2]"
REFERENCES,0.7245370370370371,"XSimGCL
τ ∼[0.05, 2], ϵ ∼{0.01, 0.05, 0.1, 0.2, 0.5, 1.0}, λ ∼{0.005, 0.01, 0.05, 0.1, 0.5, 1.0}, l∗= 1"
REFERENCES,0.7253086419753086,"KAR
No. shared experts ∼{3, 4, 5}, No. preference experts ∼{4, 5}"
REFERENCES,0.7260802469135802,"RLMRec
kd weight ∼[0.05, 2], kd temperature ∼[0.01, 0.05, 0.1, 0.15, 0.2, 0.5, 1]"
REFERENCES,0.7268518518518519,"ZESRec
λu ∼{0.01, 0.05, 0.1, 0.5, 1.0}, λv ∼{0.01, 0.05, 0.1, 0.5, 1.0}"
REFERENCES,0.7276234567901234,"UniSRec
lr ∼{3e-4, 1e-3, 3e-3, 1e-2}"
REFERENCES,0.7283950617283951,"TEM
l ∼{2,3}, head h ∼{4, 8}"
REFERENCES,0.7291666666666666,"AlphaRec
τ ∼[0.05, 2]"
REFERENCES,0.7299382716049383,Table 12: The hyperparameters of AlphaRec
REFERENCES,0.7307098765432098,"Books
Movies & TV
Video Games
Amazon-Mix"
REFERENCES,0.7314814814814815,"τ
0.15
0.15
0.2
0.15"
REFERENCES,0.7322530864197531,"NeurIPS Paper Checklist
855"
REFERENCES,0.7330246913580247,"The checklist is designed to encourage best practices for responsible machine learning research,
856"
REFERENCES,0.7337962962962963,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
857"
REFERENCES,0.7345679012345679,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
858"
REFERENCES,0.7353395061728395,"follow the references and follow the (optional) supplemental material. The checklist does NOT count
859"
REFERENCES,0.7361111111111112,"towards the page limit.
860"
REFERENCES,0.7368827160493827,"Please read the checklist guidelines carefully for information on how to answer these questions. For
861"
REFERENCES,0.7376543209876543,"each question in the checklist:
862"
REFERENCES,0.7384259259259259,"• You should answer [Yes] , [No] , or [NA] .
863"
REFERENCES,0.7391975308641975,"• [NA] means either that the question is Not Applicable for that particular paper or the
864"
REFERENCES,0.7399691358024691,"relevant information is Not Available.
865"
REFERENCES,0.7407407407407407,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
866"
REFERENCES,0.7415123456790124,"The checklist answers are an integral part of your paper submission. They are visible to the
867"
REFERENCES,0.7422839506172839,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
868"
REFERENCES,0.7430555555555556,"(after eventual revisions) with the final version of your paper, and its final version will be published
869"
REFERENCES,0.7438271604938271,"with the paper.
870"
REFERENCES,0.7445987654320988,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
871"
REFERENCES,0.7453703703703703,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
872"
REFERENCES,0.746141975308642,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
873"
REFERENCES,0.7469135802469136,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
874"
REFERENCES,0.7476851851851852,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
875"
REFERENCES,0.7484567901234568,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
876"
REFERENCES,0.7492283950617284,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
877"
REFERENCES,0.75,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
878"
REFERENCES,0.7507716049382716,"please point to the section(s) where related material for the question can be found.
879"
REFERENCES,0.7515432098765432,"IMPORTANT, please:
880"
REFERENCES,0.7523148148148148,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
881"
REFERENCES,0.7530864197530864,"• Keep the checklist subsection headings, questions/answers and guidelines below.
882"
REFERENCES,0.753858024691358,"• Do not modify the questions and only use the provided macros for your answers.
883"
CLAIMS,0.7546296296296297,"1. Claims
884"
CLAIMS,0.7554012345679012,"Question: Do the main claims made in the abstract and introduction accurately reflect the
885"
CLAIMS,0.7561728395061729,"paper’s contributions and scope?
886"
CLAIMS,0.7569444444444444,"Answer: [Yes]
887"
CLAIMS,0.7577160493827161,"Justification: We clearly state the claims made in the abstract and introduction.
888"
CLAIMS,0.7584876543209876,"Guidelines:
889"
CLAIMS,0.7592592592592593,"• The answer NA means that the abstract and introduction do not include the claims
890"
CLAIMS,0.7600308641975309,"made in the paper.
891"
CLAIMS,0.7608024691358025,"• The abstract and/or introduction should clearly state the claims made, including the
892"
CLAIMS,0.7615740740740741,"contributions made in the paper and important assumptions and limitations. A No or
893"
CLAIMS,0.7623456790123457,"NA answer to this question will not be perceived well by the reviewers.
894"
CLAIMS,0.7631172839506173,"• The claims made should match theoretical and experimental results, and reflect how
895"
CLAIMS,0.7638888888888888,"much the results can be expected to generalize to other settings.
896"
CLAIMS,0.7646604938271605,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
897"
CLAIMS,0.7654320987654321,"are not attained by the paper.
898"
LIMITATIONS,0.7662037037037037,"2. Limitations
899"
LIMITATIONS,0.7669753086419753,"Question: Does the paper discuss the limitations of the work performed by the authors?
900"
LIMITATIONS,0.7677469135802469,"Answer: [Yes]
901"
LIMITATIONS,0.7685185185185185,"Justification: We discuss the limitations of this work in the Section 5.
902"
LIMITATIONS,0.7692901234567902,"Guidelines:
903"
LIMITATIONS,0.7700617283950617,"• The answer NA means that the paper has no limitation while the answer No means that
904"
LIMITATIONS,0.7708333333333334,"the paper has limitations, but those are not discussed in the paper.
905"
LIMITATIONS,0.7716049382716049,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
906"
LIMITATIONS,0.7723765432098766,"• The paper should point out any strong assumptions and how robust the results are to
907"
LIMITATIONS,0.7731481481481481,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
908"
LIMITATIONS,0.7739197530864198,"model well-specification, asymptotic approximations only holding locally). The authors
909"
LIMITATIONS,0.7746913580246914,"should reflect on how these assumptions might be violated in practice and what the
910"
LIMITATIONS,0.7754629629629629,"implications would be.
911"
LIMITATIONS,0.7762345679012346,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
912"
LIMITATIONS,0.7770061728395061,"only tested on a few datasets or with a few runs. In general, empirical results often
913"
LIMITATIONS,0.7777777777777778,"depend on implicit assumptions, which should be articulated.
914"
LIMITATIONS,0.7785493827160493,"• The authors should reflect on the factors that influence the performance of the approach.
915"
LIMITATIONS,0.779320987654321,"For example, a facial recognition algorithm may perform poorly when image resolution
916"
LIMITATIONS,0.7800925925925926,"is low or images are taken in low lighting. Or a speech-to-text system might not be
917"
LIMITATIONS,0.7808641975308642,"used reliably to provide closed captions for online lectures because it fails to handle
918"
LIMITATIONS,0.7816358024691358,"technical jargon.
919"
LIMITATIONS,0.7824074074074074,"• The authors should discuss the computational efficiency of the proposed algorithms
920"
LIMITATIONS,0.783179012345679,"and how they scale with dataset size.
921"
LIMITATIONS,0.7839506172839507,"• If applicable, the authors should discuss possible limitations of their approach to
922"
LIMITATIONS,0.7847222222222222,"address problems of privacy and fairness.
923"
LIMITATIONS,0.7854938271604939,"• While the authors might fear that complete honesty about limitations might be used by
924"
LIMITATIONS,0.7862654320987654,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
925"
LIMITATIONS,0.7870370370370371,"limitations that aren’t acknowledged in the paper. The authors should use their best
926"
LIMITATIONS,0.7878086419753086,"judgment and recognize that individual actions in favor of transparency play an impor-
927"
LIMITATIONS,0.7885802469135802,"tant role in developing norms that preserve the integrity of the community. Reviewers
928"
LIMITATIONS,0.7893518518518519,"will be specifically instructed to not penalize honesty concerning limitations.
929"
THEORY ASSUMPTIONS AND PROOFS,0.7901234567901234,"3. Theory Assumptions and Proofs
930"
THEORY ASSUMPTIONS AND PROOFS,0.7908950617283951,"Question: For each theoretical result, does the paper provide the full set of assumptions and
931"
THEORY ASSUMPTIONS AND PROOFS,0.7916666666666666,"a complete (and correct) proof?
932"
THEORY ASSUMPTIONS AND PROOFS,0.7924382716049383,"Answer: [NA]
933"
THEORY ASSUMPTIONS AND PROOFS,0.7932098765432098,"Justification: This is an empirical article and contains no theoretical results.
934"
THEORY ASSUMPTIONS AND PROOFS,0.7939814814814815,"Guidelines:
935"
THEORY ASSUMPTIONS AND PROOFS,0.7947530864197531,"• The answer NA means that the paper does not include theoretical results.
936"
THEORY ASSUMPTIONS AND PROOFS,0.7955246913580247,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
937"
THEORY ASSUMPTIONS AND PROOFS,0.7962962962962963,"referenced.
938"
THEORY ASSUMPTIONS AND PROOFS,0.7970679012345679,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
939"
THEORY ASSUMPTIONS AND PROOFS,0.7978395061728395,"• The proofs can either appear in the main paper or the supplemental material, but if
940"
THEORY ASSUMPTIONS AND PROOFS,0.7986111111111112,"they appear in the supplemental material, the authors are encouraged to provide a short
941"
THEORY ASSUMPTIONS AND PROOFS,0.7993827160493827,"proof sketch to provide intuition.
942"
THEORY ASSUMPTIONS AND PROOFS,0.8001543209876543,"• Inversely, any informal proof provided in the core of the paper should be complemented
943"
THEORY ASSUMPTIONS AND PROOFS,0.8009259259259259,"by formal proofs provided in appendix or supplemental material.
944"
THEORY ASSUMPTIONS AND PROOFS,0.8016975308641975,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
945"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8024691358024691,"4. Experimental Result Reproducibility
946"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8032407407407407,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
947"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8040123456790124,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
948"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8047839506172839,"of the paper (regardless of whether the code and data are provided or not)?
949"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8055555555555556,"Answer: [Yes]
950"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8063271604938271,"Justification: We present all the experiment details and datasets in Appendix C, and Hyper-
951"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8070987654320988,"parameters settings are reported in Appendix D. Moreover, we have uploaded the code and
952"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8078703703703703,"data we used in the supplementary material.
953"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.808641975308642,"Guidelines:
954"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8094135802469136,"• The answer NA means that the paper does not include experiments.
955"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8101851851851852,"• If the paper includes experiments, a No answer to this question will not be perceived
956"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8109567901234568,"well by the reviewers: Making the paper reproducible is important, regardless of
957"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8117283950617284,"whether the code and data are provided or not.
958"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8125,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
959"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8132716049382716,"to make their results reproducible or verifiable.
960"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8140432098765432,"• Depending on the contribution, reproducibility can be accomplished in various ways.
961"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8148148148148148,"For example, if the contribution is a novel architecture, describing the architecture fully
962"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8155864197530864,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
963"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.816358024691358,"be necessary to either make it possible for others to replicate the model with the same
964"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8171296296296297,"dataset, or provide access to the model. In general. releasing code and data is often
965"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8179012345679012,"one good way to accomplish this, but reproducibility can also be provided via detailed
966"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8186728395061729,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
967"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8194444444444444,"of a large language model), releasing of a model checkpoint, or other means that are
968"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8202160493827161,"appropriate to the research performed.
969"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8209876543209876,"• While NeurIPS does not require releasing code, the conference does require all submis-
970"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8217592592592593,"sions to provide some reasonable avenue for reproducibility, which may depend on the
971"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8225308641975309,"nature of the contribution. For example
972"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8233024691358025,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
973"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8240740740740741,"to reproduce that algorithm.
974"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8248456790123457,"(b) If the contribution is primarily a new model architecture, the paper should describe
975"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8256172839506173,"the architecture clearly and fully.
976"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8263888888888888,"(c) If the contribution is a new model (e.g., a large language model), then there should
977"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8271604938271605,"either be a way to access this model for reproducing the results or a way to reproduce
978"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8279320987654321,"the model (e.g., with an open-source dataset or instructions for how to construct
979"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8287037037037037,"the dataset).
980"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8294753086419753,"(d) We recognize that reproducibility may be tricky in some cases, in which case
981"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8302469135802469,"authors are welcome to describe the particular way they provide for reproducibility.
982"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8310185185185185,"In the case of closed-source models, it may be that access to the model is limited in
983"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8317901234567902,"some way (e.g., to registered users), but it should be possible for other researchers
984"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8325617283950617,"to have some path to reproducing or verifying the results.
985"
OPEN ACCESS TO DATA AND CODE,0.8333333333333334,"5. Open access to data and code
986"
OPEN ACCESS TO DATA AND CODE,0.8341049382716049,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
987"
OPEN ACCESS TO DATA AND CODE,0.8348765432098766,"tions to faithfully reproduce the main experimental results, as described in supplemental
988"
OPEN ACCESS TO DATA AND CODE,0.8356481481481481,"material?
989"
OPEN ACCESS TO DATA AND CODE,0.8364197530864198,"Answer: [Yes]
990"
OPEN ACCESS TO DATA AND CODE,0.8371913580246914,"Justification: We provide access to the data and code we used in the supplementary material.
991"
OPEN ACCESS TO DATA AND CODE,0.8379629629629629,"Guidelines:
992"
OPEN ACCESS TO DATA AND CODE,0.8387345679012346,"• The answer NA means that paper does not include experiments requiring code.
993"
OPEN ACCESS TO DATA AND CODE,0.8395061728395061,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
994"
OPEN ACCESS TO DATA AND CODE,0.8402777777777778,"public/guides/CodeSubmissionPolicy) for more details.
995"
OPEN ACCESS TO DATA AND CODE,0.8410493827160493,"• While we encourage the release of code and data, we understand that this might not be
996"
OPEN ACCESS TO DATA AND CODE,0.841820987654321,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
997"
OPEN ACCESS TO DATA AND CODE,0.8425925925925926,"including code, unless this is central to the contribution (e.g., for a new open-source
998"
OPEN ACCESS TO DATA AND CODE,0.8433641975308642,"benchmark).
999"
OPEN ACCESS TO DATA AND CODE,0.8441358024691358,"• The instructions should contain the exact command and environment needed to run to
1000"
OPEN ACCESS TO DATA AND CODE,0.8449074074074074,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
1001"
OPEN ACCESS TO DATA AND CODE,0.845679012345679,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
1002"
OPEN ACCESS TO DATA AND CODE,0.8464506172839507,"• The authors should provide instructions on data access and preparation, including how
1003"
OPEN ACCESS TO DATA AND CODE,0.8472222222222222,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
1004"
OPEN ACCESS TO DATA AND CODE,0.8479938271604939,"• The authors should provide scripts to reproduce all experimental results for the new
1005"
OPEN ACCESS TO DATA AND CODE,0.8487654320987654,"proposed method and baselines. If only a subset of experiments are reproducible, they
1006"
OPEN ACCESS TO DATA AND CODE,0.8495370370370371,"should state which ones are omitted from the script and why.
1007"
OPEN ACCESS TO DATA AND CODE,0.8503086419753086,"• At submission time, to preserve anonymity, the authors should release anonymized
1008"
OPEN ACCESS TO DATA AND CODE,0.8510802469135802,"versions (if applicable).
1009"
OPEN ACCESS TO DATA AND CODE,0.8518518518518519,"• Providing as much information as possible in supplemental material (appended to the
1010"
OPEN ACCESS TO DATA AND CODE,0.8526234567901234,"paper) is recommended, but including URLs to data and code is permitted.
1011"
OPEN ACCESS TO DATA AND CODE,0.8533950617283951,"6. Experimental Setting/Details
1012"
OPEN ACCESS TO DATA AND CODE,0.8541666666666666,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
1013"
OPEN ACCESS TO DATA AND CODE,0.8549382716049383,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
1014"
OPEN ACCESS TO DATA AND CODE,0.8557098765432098,"results?
1015"
OPEN ACCESS TO DATA AND CODE,0.8564814814814815,"Answer: [Yes]
1016"
OPEN ACCESS TO DATA AND CODE,0.8572530864197531,"Justification: Datasets and data split are presented in Appendix C.1, and hyperparameters
1017"
OPEN ACCESS TO DATA AND CODE,0.8580246913580247,"are searched according to the suggestion in the literature. See more details in Appendix D.
1018"
OPEN ACCESS TO DATA AND CODE,0.8587962962962963,"Guidelines:
1019"
OPEN ACCESS TO DATA AND CODE,0.8595679012345679,"• The answer NA means that the paper does not include experiments.
1020"
OPEN ACCESS TO DATA AND CODE,0.8603395061728395,"• The experimental setting should be presented in the core of the paper to a level of detail
1021"
OPEN ACCESS TO DATA AND CODE,0.8611111111111112,"that is necessary to appreciate the results and make sense of them.
1022"
OPEN ACCESS TO DATA AND CODE,0.8618827160493827,"• The full details can be provided either with the code, in appendix, or as supplemental
1023"
OPEN ACCESS TO DATA AND CODE,0.8626543209876543,"material.
1024"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8634259259259259,"7. Experiment Statistical Significance
1025"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8641975308641975,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1026"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8649691358024691,"information about the statistical significance of the experiments?
1027"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8657407407407407,"Answer: [Yes]
1028"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8665123456790124,"Justification: We validate the p-value to support the main claims of this paper.
1029"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8672839506172839,"Guidelines:
1030"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8680555555555556,"• The answer NA means that the paper does not include experiments.
1031"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8688271604938271,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
1032"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8695987654320988,"dence intervals, or statistical significance tests, at least for the experiments that support
1033"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8703703703703703,"the main claims of the paper.
1034"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.871141975308642,"• The factors of variability that the error bars are capturing should be clearly stated (for
1035"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8719135802469136,"example, train/test split, initialization, random drawing of some parameter, or overall
1036"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8726851851851852,"run with given experimental conditions).
1037"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8734567901234568,"• The method for calculating the error bars should be explained (closed form formula,
1038"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8742283950617284,"call to a library function, bootstrap, etc.)
1039"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.875,"• The assumptions made should be given (e.g., Normally distributed errors).
1040"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8757716049382716,"• It should be clear whether the error bar is the standard deviation or the standard error
1041"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8765432098765432,"of the mean.
1042"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8773148148148148,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1043"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8780864197530864,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1044"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.878858024691358,"of Normality of errors is not verified.
1045"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8796296296296297,"• For asymmetric distributions, the authors should be careful not to show in tables or
1046"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8804012345679012,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1047"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8811728395061729,"error rates).
1048"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8819444444444444,"• If error bars are reported in tables or plots, The authors should explain in the text how
1049"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8827160493827161,"they were calculated and reference the corresponding figures or tables in the text.
1050"
EXPERIMENTS COMPUTE RESOURCES,0.8834876543209876,"8. Experiments Compute Resources
1051"
EXPERIMENTS COMPUTE RESOURCES,0.8842592592592593,"Question: For each experiment, does the paper provide sufficient information on the com-
1052"
EXPERIMENTS COMPUTE RESOURCES,0.8850308641975309,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1053"
EXPERIMENTS COMPUTE RESOURCES,0.8858024691358025,"the experiments?
1054"
EXPERIMENTS COMPUTE RESOURCES,0.8865740740740741,"Answer: [Yes]
1055"
EXPERIMENTS COMPUTE RESOURCES,0.8873456790123457,"Justification: We conduct all the experiments in PyTorch with a single NVIDIA RTX A5000
1056"
EXPERIMENTS COMPUTE RESOURCES,0.8881172839506173,"(24G) GPU and a 64 AMD EPYC 7543 32-Core Processor CPU. And Detailed time costs
1057"
EXPERIMENTS COMPUTE RESOURCES,0.8888888888888888,"are shown in Appendix C.5.
1058"
EXPERIMENTS COMPUTE RESOURCES,0.8896604938271605,"Guidelines:
1059"
EXPERIMENTS COMPUTE RESOURCES,0.8904320987654321,"• The answer NA means that the paper does not include experiments.
1060"
EXPERIMENTS COMPUTE RESOURCES,0.8912037037037037,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1061"
EXPERIMENTS COMPUTE RESOURCES,0.8919753086419753,"or cloud provider, including relevant memory and storage.
1062"
EXPERIMENTS COMPUTE RESOURCES,0.8927469135802469,"• The paper should provide the amount of compute required for each of the individual
1063"
EXPERIMENTS COMPUTE RESOURCES,0.8935185185185185,"experimental runs as well as estimate the total compute.
1064"
EXPERIMENTS COMPUTE RESOURCES,0.8942901234567902,"• The paper should disclose whether the full research project required more compute
1065"
EXPERIMENTS COMPUTE RESOURCES,0.8950617283950617,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1066"
EXPERIMENTS COMPUTE RESOURCES,0.8958333333333334,"didn’t make it into the paper).
1067"
CODE OF ETHICS,0.8966049382716049,"9. Code Of Ethics
1068"
CODE OF ETHICS,0.8973765432098766,"Question: Does the research conducted in the paper conform, in every respect, with the
1069"
CODE OF ETHICS,0.8981481481481481,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1070"
CODE OF ETHICS,0.8989197530864198,"Answer: [Yes]
1071"
CODE OF ETHICS,0.8996913580246914,"Justification: The research adheres to all ethical guidelines outlined by NeurIPS. Specifically,
1072"
CODE OF ETHICS,0.9004629629629629,"we have ensured that our data collection methods are ethical, our experiments are conducted
1073"
CODE OF ETHICS,0.9012345679012346,"responsibly, and all potential biases are addressed. Additionally, we have considered the
1074"
CODE OF ETHICS,0.9020061728395061,"broader impacts of our work and have taken steps to mitigate any negative consequences.
1075"
CODE OF ETHICS,0.9027777777777778,"Guidelines:
1076"
CODE OF ETHICS,0.9035493827160493,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1077"
CODE OF ETHICS,0.904320987654321,"• If the authors answer No, they should explain the special circumstances that require a
1078"
CODE OF ETHICS,0.9050925925925926,"deviation from the Code of Ethics.
1079"
CODE OF ETHICS,0.9058641975308642,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1080"
CODE OF ETHICS,0.9066358024691358,"eration due to laws or regulations in their jurisdiction).
1081"
BROADER IMPACTS,0.9074074074074074,"10. Broader Impacts
1082"
BROADER IMPACTS,0.908179012345679,"Question: Does the paper discuss both potential positive societal impacts and negative
1083"
BROADER IMPACTS,0.9089506172839507,"societal impacts of the work performed?
1084"
BROADER IMPACTS,0.9097222222222222,"Answer: [Yes]
1085"
BROADER IMPACTS,0.9104938271604939,"Justification: We consider both the potential societal impacts and negative societal impacts,
1086"
BROADER IMPACTS,0.9112654320987654,"and also discuss possible mitigation strategies. Details are shown in Appendix E.
1087"
BROADER IMPACTS,0.9120370370370371,"Guidelines:
1088"
BROADER IMPACTS,0.9128086419753086,"• The answer NA means that there is no societal impact of the work performed.
1089"
BROADER IMPACTS,0.9135802469135802,"• If the authors answer NA or No, they should explain why their work has no societal
1090"
BROADER IMPACTS,0.9143518518518519,"impact or why the paper does not address societal impact.
1091"
BROADER IMPACTS,0.9151234567901234,"• Examples of negative societal impacts include potential malicious or unintended uses
1092"
BROADER IMPACTS,0.9158950617283951,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1093"
BROADER IMPACTS,0.9166666666666666,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1094"
BROADER IMPACTS,0.9174382716049383,"groups), privacy considerations, and security considerations.
1095"
BROADER IMPACTS,0.9182098765432098,"• The conference expects that many papers will be foundational research and not tied
1096"
BROADER IMPACTS,0.9189814814814815,"to particular applications, let alone deployments. However, if there is a direct path to
1097"
BROADER IMPACTS,0.9197530864197531,"any negative applications, the authors should point it out. For example, it is legitimate
1098"
BROADER IMPACTS,0.9205246913580247,"to point out that an improvement in the quality of generative models could be used to
1099"
BROADER IMPACTS,0.9212962962962963,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1100"
BROADER IMPACTS,0.9220679012345679,"that a generic algorithm for optimizing neural networks could enable people to train
1101"
BROADER IMPACTS,0.9228395061728395,"models that generate Deepfakes faster.
1102"
BROADER IMPACTS,0.9236111111111112,"• The authors should consider possible harms that could arise when the technology is
1103"
BROADER IMPACTS,0.9243827160493827,"being used as intended and functioning correctly, harms that could arise when the
1104"
BROADER IMPACTS,0.9251543209876543,"technology is being used as intended but gives incorrect results, and harms following
1105"
BROADER IMPACTS,0.9259259259259259,"from (intentional or unintentional) misuse of the technology.
1106"
BROADER IMPACTS,0.9266975308641975,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1107"
BROADER IMPACTS,0.9274691358024691,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1108"
BROADER IMPACTS,0.9282407407407407,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1109"
BROADER IMPACTS,0.9290123456790124,"feedback over time, improving the efficiency and accessibility of ML).
1110"
SAFEGUARDS,0.9297839506172839,"11. Safeguards
1111"
SAFEGUARDS,0.9305555555555556,"Question: Does the paper describe safeguards that have been put in place for responsible
1112"
SAFEGUARDS,0.9313271604938271,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1113"
SAFEGUARDS,0.9320987654320988,"image generators, or scraped datasets)?
1114"
SAFEGUARDS,0.9328703703703703,"Answer: [NA]
1115"
SAFEGUARDS,0.933641975308642,"Justification: The paper poses no such risks.
1116"
SAFEGUARDS,0.9344135802469136,"Guidelines:
1117"
SAFEGUARDS,0.9351851851851852,"• The answer NA means that the paper poses no such risks.
1118"
SAFEGUARDS,0.9359567901234568,"• Released models that have a high risk for misuse or dual-use should be released with
1119"
SAFEGUARDS,0.9367283950617284,"necessary safeguards to allow for controlled use of the model, for example by requiring
1120"
SAFEGUARDS,0.9375,"that users adhere to usage guidelines or restrictions to access the model or implementing
1121"
SAFEGUARDS,0.9382716049382716,"safety filters.
1122"
SAFEGUARDS,0.9390432098765432,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1123"
SAFEGUARDS,0.9398148148148148,"should describe how they avoided releasing unsafe images.
1124"
SAFEGUARDS,0.9405864197530864,"• We recognize that providing effective safeguards is challenging, and many papers do
1125"
SAFEGUARDS,0.941358024691358,"not require this, but we encourage authors to take this into account and make a best
1126"
SAFEGUARDS,0.9421296296296297,"faith effort.
1127"
LICENSES FOR EXISTING ASSETS,0.9429012345679012,"12. Licenses for existing assets
1128"
LICENSES FOR EXISTING ASSETS,0.9436728395061729,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1129"
LICENSES FOR EXISTING ASSETS,0.9444444444444444,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1130"
LICENSES FOR EXISTING ASSETS,0.9452160493827161,"properly respected?
1131"
LICENSES FOR EXISTING ASSETS,0.9459876543209876,"Answer: [Yes]
1132"
LICENSES FOR EXISTING ASSETS,0.9467592592592593,"Justification: We incorporate six datasets, including four datasets from the Amazon
1133"
LICENSES FOR EXISTING ASSETS,0.9475308641975309,"platform[1](Books, Movies & TV, Video Games, and Industrial), Movielens-1M[59], and
1134"
LICENSES FOR EXISTING ASSETS,0.9483024691358025,"Book Crossing[60], all of which are open-source. The backend language models used in our
1135"
LICENSES FOR EXISTING ASSETS,0.9490740740740741,"research are BERT [4], RoBERTa [5], Llama2-7B [6], Mistral-7B [69], text-embedding-ada-
1136"
LICENSES FOR EXISTING ASSETS,0.9498456790123457,"v2 & text-embeddings-3-large [70], and SFR-Embedding-Mistral [71].
1137"
LICENSES FOR EXISTING ASSETS,0.9506172839506173,"Guidelines:
1138"
LICENSES FOR EXISTING ASSETS,0.9513888888888888,"• The answer NA means that the paper does not use existing assets.
1139"
LICENSES FOR EXISTING ASSETS,0.9521604938271605,"• The authors should cite the original paper that produced the code package or dataset.
1140"
LICENSES FOR EXISTING ASSETS,0.9529320987654321,"• The authors should state which version of the asset is used and, if possible, include a
1141"
LICENSES FOR EXISTING ASSETS,0.9537037037037037,"URL.
1142"
LICENSES FOR EXISTING ASSETS,0.9544753086419753,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1143"
LICENSES FOR EXISTING ASSETS,0.9552469135802469,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1144"
LICENSES FOR EXISTING ASSETS,0.9560185185185185,"service of that source should be provided.
1145"
LICENSES FOR EXISTING ASSETS,0.9567901234567902,"• If assets are released, the license, copyright information, and terms of use in the
1146"
LICENSES FOR EXISTING ASSETS,0.9575617283950617,"package should be provided. For popular datasets, paperswithcode.com/datasets
1147"
LICENSES FOR EXISTING ASSETS,0.9583333333333334,"has curated licenses for some datasets. Their licensing guide can help determine the
1148"
LICENSES FOR EXISTING ASSETS,0.9591049382716049,"license of a dataset.
1149"
LICENSES FOR EXISTING ASSETS,0.9598765432098766,"• For existing datasets that are re-packaged, both the original license and the license of
1150"
LICENSES FOR EXISTING ASSETS,0.9606481481481481,"the derived asset (if it has changed) should be provided.
1151"
LICENSES FOR EXISTING ASSETS,0.9614197530864198,"• If this information is not available online, the authors are encouraged to reach out to
1152"
LICENSES FOR EXISTING ASSETS,0.9621913580246914,"the asset’s creators.
1153"
NEW ASSETS,0.9629629629629629,"13. New Assets
1154"
NEW ASSETS,0.9637345679012346,"Question: Are new assets introduced in the paper well documented and is the documentation
1155"
NEW ASSETS,0.9645061728395061,"provided alongside the assets?
1156"
NEW ASSETS,0.9652777777777778,"Answer: [NA]
1157"
NEW ASSETS,0.9660493827160493,"Justification: This paper does not release new assets.
1158"
NEW ASSETS,0.966820987654321,"Guidelines:
1159"
NEW ASSETS,0.9675925925925926,"• The answer NA means that the paper does not release new assets.
1160"
NEW ASSETS,0.9683641975308642,"• Researchers should communicate the details of the dataset/code/model as part of their
1161"
NEW ASSETS,0.9691358024691358,"submissions via structured templates. This includes details about training, license,
1162"
NEW ASSETS,0.9699074074074074,"limitations, etc.
1163"
NEW ASSETS,0.970679012345679,"• The paper should discuss whether and how consent was obtained from people whose
1164"
NEW ASSETS,0.9714506172839507,"asset is used.
1165"
NEW ASSETS,0.9722222222222222,"• At submission time, remember to anonymize your assets (if applicable). You can either
1166"
NEW ASSETS,0.9729938271604939,"create an anonymized URL or include an anonymized zip file.
1167"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737654320987654,"14. Crowdsourcing and Research with Human Subjects
1168"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9745370370370371,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1169"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9753086419753086,"include the full text of instructions given to participants and screenshots, if applicable, as
1170"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760802469135802,"well as details about compensation (if any)?
1171"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768518518518519,"Answer: [NA]
1172"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776234567901234,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1173"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783950617283951,"Guidelines:
1174"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791666666666666,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1175"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799382716049383,"human subjects.
1176"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9807098765432098,"• Including this information in the supplemental material is fine, but if the main contribu-
1177"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814814814814815,"tion of the paper involves human subjects, then as much detail as possible should be
1178"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822530864197531,"included in the main paper.
1179"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830246913580247,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1180"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837962962962963,"or other labor should be paid at least the minimum wage in the country of the data
1181"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845679012345679,"collector.
1182"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853395061728395,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1183"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861111111111112,"Subjects
1184"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868827160493827,"Question: Does the paper describe potential risks incurred by study participants, whether
1185"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876543209876543,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1186"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884259259259259,"approvals (or an equivalent approval/review based on the requirements of your country or
1187"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891975308641975,"institution) were obtained?
1188"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9899691358024691,"Answer: [NA]
1189"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907407407407407,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1190"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915123456790124,"Guidelines:
1191"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922839506172839,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1192"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930555555555556,"human subjects.
1193"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938271604938271,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1194"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945987654320988,"may be required for any human subjects research. If you obtained IRB approval, you
1195"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953703703703703,"should clearly state this in the paper.
1196"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996141975308642,"• We recognize that the procedures for this may vary significantly between institutions
1197"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969135802469136,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1198"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976851851851852,"guidelines for their institution.
1199"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9984567901234568,"• For initial submissions, do not include any information that would break anonymity (if
1200"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9992283950617284,"applicable), such as the institution conducting the review.
1201"
