Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0036900369003690036,"Policy Optimization (PO) algorithms have been proven particularly suited to handle
the high-dimensionality of real-world continuous control tasks. In this context,
Trust Region Policy Optimization methods represent a popular approach to stabilize
the policy updates. These usually rely on the Kullback-Leibler (KL) divergence
to limit the change in the policy. The Wasserstein distance represents a natural
alternative, in place of the KL divergence, to deﬁne trust regions or to regular-
ize the objective function. However, state-of-the-art works either resort to its
approximations or do not provide an algorithm for continuous state-action spaces,
reducing the applicability of the method. In this paper, we explore optimal trans-
port discrepancies (which include the Wasserstein distance) to deﬁne trust regions,
and we propose a novel algorithm – Optimal Transport Trust Region Policy Op-
timization (OT-TRPO) – for continuous state-action spaces. We circumvent the
inﬁnite-dimensional optimization problem for PO by providing a one-dimensional
dual reformulation for which strong duality holds. We then analytically derive
the optimal policy update given the solution of the dual problem. This way, we
bypass the computation of optimal transport costs and of optimal transport maps,
which we implicitly characterize by solving the dual formulation. Finally, we
provide an experimental evaluation of our approach across various control tasks.
Our results show that optimal transport discrepancies can offer an advantage over
state-of-the-art approaches."
INTRODUCTION,0.007380073800738007,"1
Introduction"
INTRODUCTION,0.01107011070110701,"Reinforcement Learning (RL) has achieved outstanding results in numerous ﬁelds, from resource
management [16], recommendation systems [42], and optimization of chemical reactions [44], to
video-games [18, 43, 10] and board games [39], without sparing the world’s champion of GO [32].
Many of these successful applications rely on Policy Optimization (PO) algorithms, a family of RL
methods that are particularly suited to handle the high-dimensionality of real-world control tasks. PO
algorithms approach the RL setting as an optimization problem in the policy space. In this context, the
main challenge is to provide policy improvement guarantees. One remarkable option in this direction"
INTRODUCTION,0.014760147601476014,⇤Equal contribution.
INTRODUCTION,0.01845018450184502,"is represented by Trust Region Policy Optimization (TRPO) [29], which constrains the optimization
problem to policies that are “close” to the current one, whereby the Kullback-Leibler (KL) divergence
is used as a similarity measure. Nevertheless, “closeness” in the policy space can also be quantiﬁed via
other functions. Recent work [22, 33, 20] proposed to replace the KL divergence with the Wasserstein
distance, a particular instance of optimal transport discrepancy (or cost). Besides being very natural
and expressive, optimal transport discrepancies enjoy powerful topological, differential, geometrical,
computational, and statistical features and guarantees [37, 3, 14]. In particular, (i) optimal transport
discrepancies allow us to compare probability measures (and thus policies) not sharing the same
support (for which the KL divergence is inﬁnity); and (ii) they encapsulate the geometry encoded
by the transport cost in the action space: the discrepancy between two actions coincides with the
discrepancy between the corresponding deterministic policies (whereas the KL divergence is again
inﬁnity). These reasons make optimal transport discrepancies particularly attractive for RL. However,
the mere evaluation of optimal transport discrepancies entails solving a transportation problem
(e.g., see [23, 34]), which poses signiﬁcant computational challenges for its deployment. Most of
the previous work on the topic [22, 20] overcomes the computational burden via approximation,
effectively changing the original problem. Conversely, [33] proposes two algorithms to solve the PO
problem exactly, studying the trust regions described via the Wasserstein distance and the Sinkhorn
divergence. However, their analysis is limited to discrete (and ﬁnite) settings. In our work, we
consider optimal transport discrepancies to construct the trust region in settings where actions and
states take value in general compact Polish spaces. This allows tackling many applications of
interest involving continuous domains such as physical control tasks. We derive and leverage a dual
reformulation of the PO problem to ensure an optimal policy update within the trust region, without
any additional need for line searches (conversely to [29]). We circumvent the computation of optimal
transport discrepancies via an analytical expression of the transport maps, which are characterized
thanks to the dual reformulation. Notably, our analysis enables a practical and efﬁcient algorithm that
encompasses both discrete and continuous settings."
INTRODUCTION,0.02214022140221402,"Contributions. Our contributions are summarized as follows:
1. We derive the dual of the optimal transport trust region policy optimization problem and we show"
INTRODUCTION,0.025830258302583026,"that strong duality holds for general compact metric state-action spaces. We further characterize
the optimal policy update given the solution to the dual problem. We show that policy updates can
result in monotonic improvement of the performance function.
2. We propose a novel PO algorithm for continuous spaces, Optimal Transport Trust Region Policy"
INTRODUCTION,0.02952029520295203,"Optimization (OT-TRPO). Herein, we leverage the derived duality theory to provide policy updates
that satisfy the optimal transport discrepancy constraint while circumventing its computation.
3. We conduct experiments in several RL benchmarks in both discrete and continuous state-action"
INTRODUCTION,0.033210332103321034,"spaces, comparing our method to state-of-the-art approaches. Our results show the effectiveness
of our approach for PO and the beneﬁts of using optimal transport discrepancies."
LIT REVIEW,0.03690036900369004,"2
Related Works"
LIT REVIEW,0.04059040590405904,"Optimal transport, and in particular the Wasserstein distance, has found various applications in RL and
in particular in PO algorithms. In this section, we discuss the most relevant for our work; a broader
overview is postponed to Appendix A.1. In [22], the authors propose Behavior Guided Policy Gradient
(BGPG), whereby they replace the KL divergence trust region from TRPO [29] by a Wasserstein
distance penalty in a behavioral space. Although our alternating procedure in Section 5 may resemble
the approach of [22] in spirit, our approach is fundamentally different; cfr. [22, Algorithms 1 and 3]
with Algorithm 1. In [20], the authors further suggested incorporating additional information about
the local behavior of policies encapsulated in the so-called Wasserstein Information Matrix, in the
attempt to speed up the PO using a Wasserstein Natural Policy Gradient (WNPG). However, these
approaches are relatively slow, compared to the traditional Proximal Policy Optimization (PPO) and
TRPO. Conversely to our work, they do not build on the idea of trust regions: we instead guarantee
that the policy update is “close” to the previous one, where the “closeness” is deﬁned via an optimal
transport discrepancy (e.g., the Wasserstein distance). Accordingly, the closest related work to ours
is the recent paper [33] which studied Wasserstein Policy Optimization (WPO) for discrete action
spaces. In contrast to [33], the present work addresses the general setting of compact Polish spaces
encompassing the cases of continuous and discrete state-action spaces as particular cases. While
we also adopt a duality approach, our level of generality induces many challenges compared to
the discrete action space setting (see Remark 2), and it is compatible even with non-direct policy"
LIT REVIEW,0.04428044280442804,"parametrizations. Finally, our work is closely connected with Wasserstein Distributionally Robust
Optimization (DRO) [8, 19, 4, 25, 13]. Albeit our duality results are inspired from this literature,
DRO is concerned with quantifying the worst-case risk of a cost functional over an ambiguity set
of probability measures, which is a fundamentally different setting; see Remark 1. Conversely to
all the previous work, we show how to perform exact optimal transport-based TRPO in continuous
settings: we exploit optimal transport theory to circumvent the computational burden of evaluating
optimal transport discrepancies while still performing exact policy updates within the trust regions.
Accordingly, to the best of our knowledge, our practical algorithm is completely novel."
LIT REVIEW,0.04797047970479705,"3
Preliminaries"
LIT REVIEW,0.05166051660516605,We brieﬂy introduce useful background and notation for the remainder of the paper.
LIT REVIEW,0.055350553505535055,"Notation. For every Polish space X (i.e., completely metrizable separable topological space),
the set of Borel probability measures on X is denoted by P(X). The Dirac measure at some
point x 2 X is denoted by δx. Given two Polish spaces X, Y, a Borel probability measures
µ 2 P(X), and a Borel map T : X ! Y, the pushforward measure of µ, denoted by T#µ, is deﬁned
by (T#µ)(A) := µ(T −1(A)) for all A 2 B(Y), where B(Y) is the collection of Borel subsets of Y.
The set of probability measures on a ﬁnite set X coincides with the probability simplex and will also
be denoted by P(X). For a given function f : X ! R, the notation kfk1 refers to supx2X |f(x)|."
LIT REVIEW,0.05904059040590406,"Markov Decision Process. We consider an inﬁnite-horizon discounted Markov Decision Process
(MDP) [24] M = (S, A, P, r, ⇢, γ), where S is the state space, A is the action space, P : S⇥A⇥S !
R≥0 is the state transition probability kernel, r : S⇥A ! R is the reward function, ⇢is the initial state
probability distribution, and γ 2 [0, 1) is the discount factor. A randomized stationary Markovian
policy, which we will simply call a policy in the rest of the paper, is a mapping ⇡: S ! P(A)
specifying for each s 2 S a probability measure over the set of actions A by ⇡(·|s) 2 P(A).
The set of all policies is denoted by ⇧. Each policy ⇡2 ⇧induces a discrete-time Markov
reward process {(st, r(st, at))}t2N, where st 2 S represents the state of the system at time t
and r(st, at) corresponds to the reward received when executing action at 2 A in state st. We
denote by P⇢,⇡the probability distribution of the Markov chain (st, at) issued from the MDP
controlled by the policy ⇡with initial state distribution ⇢. The associated expectation is denoted
by E⇢,⇡and the notation E⇡is used whenever there is no dependence on ⇢. The state-value function
V ⇡: S ! R and the action-value function Q⇡: S ⇥A ! R are deﬁned for all s 2 S, a 2 A
by V ⇡(s) := E⇡[P1"
LIT REVIEW,0.06273062730627306,"t=0 γtr(st, at)|s0 = s] and Q⇡(s, a) := E⇡[P1"
LIT REVIEW,0.06642066420664207,"t=0 γtr(st, at)|s0 = s, a0 = a].
We also deﬁne the advantage function A⇡: S ⇥A ! R by A⇡(s, a) := Q⇡(s, a) −V ⇡(s). Given
an initial state probability distribution ⇢, our goal is to ﬁnd a policy ⇡maximizing the expected
long-term return"
LIT REVIEW,0.07011070110701106,"J(⇡) := E⇢,⇡ "" 1
X t=0"
LIT REVIEW,0.07380073800738007,"γtr(st, at) # ,"
LIT REVIEW,0.07749077490774908,"which is well-deﬁned when, e.g., the reward function is bounded. To solve this PO problem, we only
have access to the observed state, action, and reward st, at, rt at each time step t, whereas the state
transition kernel P is unknown. When the state and action spaces (S and A) are ﬁnite, an optimal
policy ⇡⇤is guaranteed to exist. When S and A are continuous, a (measurable) optimal policy is also
guaranteed to exist (see [24, Theorem 6.11.11, p. 262]) under appropriate assumptions on the state
and action spaces, the reward function and the transition kernel; we will explicit these later on. In
this paper, we focus on the continuous state-action space setting and comment on the discrete (non
necessarily ﬁnite) setting as a special case."
LIT REVIEW,0.08118081180811808,"Optimal transport. Consider a Polish space X and a continuous non-negative function c : X ⇥X !
R≥0, referred to as transport cost. Let µ, ⌫2 P(X) and deﬁne the set of joint probability measures
on X ⇥X with marginals µ and ⌫:"
LIT REVIEW,0.08487084870848709,"Γ(µ, ⌫) := {γ 2 P(X ⇥X) : γ(A ⇥X) = µ(A), γ(X ⇥B) = ⌫(B) 8 A, B 2 B(X)}.
We deﬁne the optimal transport discrepancy on P(X) for every probability measures µ and ⌫by"
LIT REVIEW,0.08856088560885608,"C(µ, ⌫) :=
min
γ2Γ(µ,⌫) Z X⇥X"
LIT REVIEW,0.09225092250922509,"c(x, x0) dγ(x, x0).
(1)"
LIT REVIEW,0.0959409594095941,"Notice that this deﬁnition is valid for both discrete and continuous measures. When c = dp,
where d is a distance on X and p ≥1, then C(µ, ⌫)1/p reduces to the celebrated type-p Wasserstein"
LIT REVIEW,0.0996309963099631,"distance [37, 2]. In our PO context, we will use this discrepancy to compare two probability
measures ⇡(·|s) 2 P(A) and ˜⇡(·|s) 2 P(A) for every s 2 S, where ⇡, ˜⇡2 ⇧are two policies."
IMPLEMENTATION/METHODS,0.1033210332103321,"4
Optimal Transport for Trust Region Policy Optimization"
IMPLEMENTATION/METHODS,0.1070110701107011,"In this section, we study the TRPO algorithm with a trust region deﬁned using an optimal transport
discrepancy as a measure of closeness between policies. We prove that the arising optimization
problem admits an amenable dual reformulation. Importantly, we show that, given the dual optimal
solution, the primal solution has an analytical expression, which can lead to monotonic improvements
of the performance index."
IMPLEMENTATION/METHODS,0.11070110701107011,"4.1
Policy iteration algorithm with optimal transport-based trust regions"
IMPLEMENTATION/METHODS,0.11439114391143912,"By the policy difference lemma [11, Lemma 6.1], the difference between the expected returns of two
policies ⇡, ˜⇡2 ⇧reads"
IMPLEMENTATION/METHODS,0.11808118081180811,J(˜⇡) = J(⇡) + Z S Z A
IMPLEMENTATION/METHODS,0.12177121771217712,"A⇡(s, a)d˜⇡(a|s)d⇢˜⇡(s),
(2)"
IMPLEMENTATION/METHODS,0.12546125461254612,"where ⇢˜⇡is the discounted state-occupancy measure [11]. The complex dependency of the discounted
visitation frequency ⇢˜⇡on the policy ˜⇡hampers the direct optimization of (2); see [29, Sec. 2].
Following previous work, we consider instead a local approximation of the expected return J, deﬁned
by"
IMPLEMENTATION/METHODS,0.12915129151291513,L⇡(˜⇡) := J(⇡) + Z S Z A
IMPLEMENTATION/METHODS,0.13284132841328414,"A⇡(s, a)d˜⇡(a|s)d⇢⇡(s).
(3)"
IMPLEMENTATION/METHODS,0.13653136531365315,"Observe that this approximation uses the discounted state-occupancy measure ⇢⇡(which can be
estimated) instead of ⇢˜⇡(see (2)). In other words, the inﬂuence of a policy change on the discounted
state-occupancy measure is neglected. Moreover, this surrogate function coincides with the expected
return J when ˜⇡= ⇡. Then, (3) motivates a policy update rule maximizing at each time step the
approximation L⇡(˜⇡) over ˜⇡, where ⇡is the current policy that we want to improve upon (see
also [33, Section 2, Eq. (1)]). To ensure stability of the update, we conservatively update the policy
using a discrepancy constraint between the current and the new one. Unlike TRPO, we do not use
the KL divergence to deﬁne the trust region, but instead an optimal transport discrepancy. Then, at
each time step, our method solves"
IMPLEMENTATION/METHODS,0.14022140221402213,"sup
˜⇡2⇧ Z S Z A"
IMPLEMENTATION/METHODS,0.14391143911439114,"A⇡(s, a)d˜⇡(a|s)d⇢⇡(s),"
IMPLEMENTATION/METHODS,0.14760147601476015,"s.t. ˜⇡2 T""(⇡) := ⇢"
IMPLEMENTATION/METHODS,0.15129151291512916,˜⇡2 ⇧: Z S
IMPLEMENTATION/METHODS,0.15498154981549817,"C(⇡(·|s), ˜⇡(·|s))d⇢⇡(s) "" ' , (P)"
IMPLEMENTATION/METHODS,0.15867158671586715,"where "" > 0 is a parameter deﬁning the radius of the trust region T""(⇡). Similarly to [29, Eq. (12)]
and [33, Problem (4)], we consider the average optimal transport discrepancy over the state space
as optimization constraint. Accordingly, the OT-TRPO policy optimization results from iteratively
solving Problem (P)."
IMPLEMENTATION/METHODS,0.16236162361623616,"4.2
Dual of the trust-region constrained problem (P)"
IMPLEMENTATION/METHODS,0.16605166051660517,"Problem (P) is intractable for two main reasons. First, as soon as the state or action space is continuous,
it is an inﬁnite-dimensional optimization problem. Second, the mere evaluation of the trust-region
constraint (e.g., for line search as in TRPO [29]) needs (possibly) inﬁnitely many computations of
the optimal transport discrepancy, which is itself already challenging to estimate. However, inspired
by prior works on Wasserstein DRO [8, 19, 41, 4], we show that problem (P) admits a tractable
one-dimensional convex dual reformulation. This duality theorem is the cornerstone of the design of
our algorithm. Before stating the result, we make the following assumptions."
IMPLEMENTATION/METHODS,0.16974169741697417,"Assumption 1. The state space S is a compact subset of an Euclidean space, the action space A
is a compact subset of a Polish space, the reward function r is a continuous function and for every
continuous function w on S, R"
IMPLEMENTATION/METHODS,0.17343173431734318,"S w(u)dP(u|s, a) is continuous in both s and a."
IMPLEMENTATION/METHODS,0.17712177121771217,"Under this assumption, there exists an optimal measurable (stationary) policy to the PO problem
formulated in Section 3. We refer the reader to [24, Theorem 6.11.11, p. 262] for a statement of this
result and milder assumptions. In particular, our duality result continues to hold if S is a compact
Polish space (i.e., not necessarily Euclidean).
Assumption 2. For every policy ⇡2 ⇧, the advantage function A⇡: S ⇥A ! R is continuous.
Moreover, the transport cost c : A ⇥A ! R≥0 is continuous and satisﬁes c(a, a) = 0 for all a 2 A."
IMPLEMENTATION/METHODS,0.18081180811808117,"In the next theorem, we show that under these assumptions Problem (P) admits a dual reformulation
for which strong duality holds.
Theorem 1 (Dual formulation). For every "" > 0 and for every policy ⇡2 ⇧, under Assumptions 1
and 2 the following strong duality result holds: max ˜⇡2⇧ ⇢Z S Z A"
IMPLEMENTATION/METHODS,0.18450184501845018,"A⇡(s, a) d˜⇡(a|s)d⇢⇡(s) : Z S"
IMPLEMENTATION/METHODS,0.1881918819188192,"C(⇡(·|s), ˜⇡(·|s))d⇢⇡(s) "" ' (P) = min λ≥0 ⇢ λ"" + Z S Z A"
IMPLEMENTATION/METHODS,0.1918819188191882,"max
a02A{A⇡(s, a0) −λc(a, a0)}d⇡(a|s)d⇢⇡(s) ' .
(D)"
IMPLEMENTATION/METHODS,0.19557195571955718,"Moreover, the primal and dual problems (P) and (D) admit a maximizer and a minimizer, respectively."
IMPLEMENTATION/METHODS,0.1992619926199262,"Remarkably, Problem (D) is one-dimensional and convex, and it only involves the current policy
⇡, advantage function A⇡, and visitation frequency ⇢⇡. The proof of Theorem 1 is constructive. In
particular, we derive a closed-form solution of problem (P) as a function of the optimal Lagrange
multiplier λ⇤solving problem (D) and the policy ⇡deﬁning the problem. Even if the closed-form
policy is part of Theorem 1 and its proof, we present it separately for clarity and for later reference.
To do so, we introduce some additional notation, which is instrumental to derive a practical algorithm
(similarly to [33])."
IMPLEMENTATION/METHODS,0.2029520295202952,"Under Assumptions 1 and 2, deﬁne for every λ ≥0 the λ-regularized advantage Φλ : S ⇥A ! R
and its associated set of maximizers for every s 2 S, a 2 A as follows:"
IMPLEMENTATION/METHODS,0.2066420664206642,"Φλ(s, a) := max"
IMPLEMENTATION/METHODS,0.21033210332103322,"a02A{A⇡(s, a0) −λc(a, a0)},"
IMPLEMENTATION/METHODS,0.2140221402214022,"Dλ(s, a) := arg max a02A"
IMPLEMENTATION/METHODS,0.2177121771217712,"{A⇡(s, a0) −λc(a, a0)}.
(4)"
IMPLEMENTATION/METHODS,0.22140221402214022,"Corollary 2 (Optimal policy). Under the setting and assumptions of Theorem 1, for any policy ⇡2 ⇧,
let λ⇤≥0 be the minimizer of the dual problem (D). Then, the following statements hold:"
IMPLEMENTATION/METHODS,0.22509225092250923,"1. For every λ ≥0, there exist two measurable selection maps T λ : S⇥A ! A and T λ : S⇥A ! A"
IMPLEMENTATION/METHODS,0.22878228782287824,"such that for every s 2 S, a 2 A"
IMPLEMENTATION/METHODS,0.23247232472324722,"T λ(s, a) 2 arg min"
IMPLEMENTATION/METHODS,0.23616236162361623,"a02Dλ(s,a)"
IMPLEMENTATION/METHODS,0.23985239852398524,"c(a, a0),
T λ(s, a) 2 arg max"
IMPLEMENTATION/METHODS,0.24354243542435425,"a02Dλ(s,a)"
IMPLEMENTATION/METHODS,0.24723247232472326,"c(a, a0).
(5)"
IMPLEMENTATION/METHODS,0.25092250922509224,"2. If λ⇤> 0, there exists t⇤2 [0, 1] such that t⇤ Z S Z A"
IMPLEMENTATION/METHODS,0.25461254612546125,"c(a, T λ⇤(s, a))d⇡(a|s)d⇢⇡(s) + (1 −t⇤) Z S Z A"
IMPLEMENTATION/METHODS,0.25830258302583026,"c(a, T λ⇤(s, a))d⇡(a|s)d⇢⇡(s) = ""."
IMPLEMENTATION/METHODS,0.26199261992619927,"(6)
3. There exists an optimal feasible policy ˜⇡for problem (P) deﬁned for every s 2 S by"
IMPLEMENTATION/METHODS,0.2656826568265683,"˜⇡(·|s) := t⇤T λ⇤(s, ·)#⇡(·|s) + (1 −t⇤)T λ⇤(s, ·)#⇡(·|s),
(7)"
IMPLEMENTATION/METHODS,0.2693726937269373,where t⇤results from (6) if λ⇤> 0 and t⇤= 0 if λ⇤= 0.
IMPLEMENTATION/METHODS,0.2730627306273063,"Intuitively, Corollary 2 suggests that the optimal policy results from displacing the probability mass
⇡(a|s) to the maximizers of the λ⇤-regularized advantage Φλ⇤(s, a), where λ⇤≥0 is the optimal
dual solution. Since maximizers are generally not unique (i.e., Dλ⇤(s, a) is not a singleton), one
needs to balance between the closest (i.e., T λ⇤(s, a)) and the furthest apart (i.e., T λ⇤(s, a)) to satisfy
the trust region constraint. In the special case λ⇤= 0, the trust region constraint is either not active
(i.e., the optimal policy lies within the trust region) or it does not affect the optimal trust region
constraint (i.e., the optimal policy would lie at the boundary of the trust region even if the constraint
is removed). In this case, it sufﬁces to displace all probability mass to the closest maximizer of the
advantage functions (i.e., T 0(s, a) 2 D0(s, a)). The complete proof of the results of this section is
deferred to Appendix B.1."
IMPLEMENTATION/METHODS,0.2767527675276753,"Remark 1. Similar results were previously established in the literature in the context of DRO (e.g.,
see [8, Theorem 1] and [4, Theorem 1]). While these results closely inspire our proof, there is
a major difference: in DRO, one seeks to evaluate the worst-case cost over an ambiguous set of
probability distributions, expressed in terms of the optimal transport discrepancy. As such, the
average optimal transport discrepancy in the trust region constraint is replaced by a single optimal
transport discrepancy. Thus, one does not need to ensure the regularity of the problem with respect to
the state (e.g., measurability of T λ w.r.t. s). This is reﬂected in our assumption of joint continuity of
the advantage in state and action and the state space being compact. To readily deploy existing results
in DRO, one needs (i) to consider a single state only (i.e., S = {s}) or (ii) to deﬁne a trust region
for each state (at the price of inﬁnitely many constraints). To transform (P) into a DRO, one might
alternatively identify a policy as a probability measure over Q"
IMPLEMENTATION/METHODS,0.28044280442804426,"s2S A, and hope to deploy standard
duality arguments in DRO. However, the uncountable product of Polish spaces is not a Polish space,
which makes all results in DRO, and more generally in optimal transport [37], inapplicable.
Remark 2. A similar result in the discrete case was presented in [33]. We highlight four major
differences. First, in the discrete setting, problem (P) is a ﬁnite-dimensional linear optimization
problem, for which strong duality holds. Thus, linear programming arguments can be used to derive
the dual reformulation. In the continuous setting, the same proof strategy would require to mobilize
the abstract machinery of inﬁnite-dimensional linear programming [15]. Second, in the discrete
setting, continuity and measurability of all functions are “for free”. On the contrary, the continuous
case imposes a careful analysis of these issues. Third, the optimal policy update [33] implicitly
assumes that the set Dλ(s, a) (see (4)) is a singleton, which is rarely satisﬁed in practice. Fourth, as a
byproduct of our proof, we show that (D) is a (one-dimensional) convex optimization problem, which
can be solved efﬁciently via off-the-shelf solvers. This way, we do not need to resort to approximation
techniques [33, Section 6.1] for the optimal dual multiplier."
IMPLEMENTATION/METHODS,0.28413284132841327,"Discrete state-action spaces. In the remainder of this section, we specialize our results to discrete
(ﬁnite) state-action spaces (which trivially satisfy Assumptions 1 and 2). Without loss of generality, we
represent the state and action spaces by S = {s1, . . . , sM} and A = {a1, . . . , aN} where M and N
are two positive integers, and we describe any policy ⇡2 ⇧and its corresponding state-occupancy
measure ⇢⇡as discrete measures:"
IMPLEMENTATION/METHODS,0.2878228782287823,"⇡(·|si) = N
X j=1"
IMPLEMENTATION/METHODS,0.2915129151291513,"⇡i,jδaj
8i 2 {1, . . . , M},
⇢⇡= M
X i=1"
IMPLEMENTATION/METHODS,0.2952029520295203,"⇢iδsi,
(8)"
IMPLEMENTATION/METHODS,0.2988929889298893,"where ⇢i, ⇡i,j ≥0 for every i 2 {1, . . . , M}, j 2 {1, . . . , N} , PM"
IMPLEMENTATION/METHODS,0.3025830258302583,i=1 ⇢i = 1 and PN
IMPLEMENTATION/METHODS,0.3062730627306273,"j=1 ⇡i,j = 1 for
every i 2 {1, . . . , M}.2 The analogous results to Theorem 1 and Corollary 2 are as follows.
Corollary 3 (Dual formulation - discrete setting). Let "" > 0. For every policy ⇡2 ⇧, the following
strong duality result holds:"
IMPLEMENTATION/METHODS,0.30996309963099633,"max
t2[0,1],bi,j,bi,j2A,
i2{1,...M}, j2{1,...,N} 8
< : M
X i=1 ⇢i N
X j=1 ⇡i,j -"
IMPLEMENTATION/METHODS,0.31365313653136534,"tA⇡(si, bi,j) + (1 −t)A⇡(si, bi,j) . : M
X i=1 ⇢i N
X j=1 ⇡i,j -"
IMPLEMENTATION/METHODS,0.3173431734317343,"tc(aj, bi,j) + (1 −t)c(aj, bi,j) . "" 9
= ;"
IMPLEMENTATION/METHODS,0.3210332103321033,"(discrete-P) = min λ≥0 8
< :λ"" + M
X i=1 ⇢i N
X j=1"
IMPLEMENTATION/METHODS,0.3247232472324723,"⇡i,jΦλ(si, aj) 9
="
IMPLEMENTATION/METHODS,0.3284132841328413,"; .
(discrete-D)"
IMPLEMENTATION/METHODS,0.33210332103321033,"In particular, let λ⇤≥0 be a solution to (discrete-D), and given for every i 2 {1, . . . , M}, j 2
{1, . . . , N}, select any b⇤"
IMPLEMENTATION/METHODS,0.33579335793357934,"i,j 2
arg min
a02Dλ⇤(si,aj)"
IMPLEMENTATION/METHODS,0.33948339483394835,"c(aj, a0),
b"
IMPLEMENTATION/METHODS,0.34317343173431736,"⇤
i,j 2
arg max
a02Dλ⇤(si,aj)"
IMPLEMENTATION/METHODS,0.34686346863468637,"c(aj, a0),
(9)"
IMPLEMENTATION/METHODS,0.3505535055350554,"and let c := M
X i=1 ⇢i N
X j=1"
IMPLEMENTATION/METHODS,0.35424354243542433,"⇡i,jc(aj, b⇤"
IMPLEMENTATION/METHODS,0.35793357933579334,"i,j),
c := M
X i=1 ⇢i N
X j=1"
IMPLEMENTATION/METHODS,0.36162361623616235,"⇡i,jc(aj, b"
IMPLEMENTATION/METHODS,0.36531365313653136,"⇤
i,j)."
IMPLEMENTATION/METHODS,0.36900369003690037,"2This representation is also valid beyond the ﬁnite state-action space setting when the policies and the
state-occupancy measures are empirical distributions with ﬁnitely many samples."
IMPLEMENTATION/METHODS,0.3726937269372694,"Then, an optimal policy ˜⇡is given by"
IMPLEMENTATION/METHODS,0.3763837638376384,"˜⇡(·|si) = N
X j=1 ⇡i,j ⇣ t⇤δb⇤"
IMPLEMENTATION/METHODS,0.3800738007380074,"i,j + (1 −t⇤)δb ⇤
i,j ⌘"
IMPLEMENTATION/METHODS,0.3837638376383764,",
8i 2 {1, . . . , M},
(10)"
IMPLEMENTATION/METHODS,0.3874538745387454,"with t⇤= (c −"")/(c −c) 2 [0, 1] (and t⇤2 [0, 1] if c = c = "") if λ⇤> 0 and t⇤= 0 if λ⇤= 0."
IMPLEMENTATION/METHODS,0.39114391143911437,"The proof of this result stems from substituting the discrete measures as deﬁned in (8) in problems (P)
and (D), and observing that the images of the mappings T λ⇤and T λ⇤have ﬁnite support in the current
setting. Notably, Corollary 3 directly provides an implementable algorithm for the policy update,
circumventing the difﬁculty of the mixed-integer optimization problem (discrete-P): solving the
one-dimensional convex program (discrete-D) provides the optimal Lagrange multiplier associated
to the trust region constraint of the primal problem which can be directly used to compute the
actions b⇤"
IMPLEMENTATION/METHODS,0.3948339483394834,"i,j, b"
IMPLEMENTATION/METHODS,0.3985239852398524,"⇤
i,j via (9), and thus the policy ˜⇡via (10).
Remark 3. The policy update suggested by (10) differs from the one in [33] (see [33, Theorem 1, (5)]
where f ⇤"
IMPLEMENTATION/METHODS,0.4022140221402214,"s (i, j) 2 {0, 1} with their notation). Indeed, our policy update relies on “splitting the
probability mass”: the probability mass ⇡(·|si) is displaced to b⇤"
IMPLEMENTATION/METHODS,0.4059040590405904,"i,j and b"
IMPLEMENTATION/METHODS,0.4095940959409594,"⇤
i,j with weights t⇤and 1 −t⇤,
respectively. This result is consistent with the Wasserstein DRO literature (e.g., see [4, Remark 2]).
The result provided in [33] corresponds to the particular case where b⇤"
IMPLEMENTATION/METHODS,0.4132841328413284,"i,j = b"
IMPLEMENTATION/METHODS,0.41697416974169743,"⇤
i,j which amounts to
supposing that the set Dλ(si, aj) deﬁned in (4) is a singleton. We provide further comments and
examples in Appendix A.2 to illustrate the importance of this “mass splitting”."
IMPLEMENTATION/METHODS,0.42066420664206644,"4.3
Policy improvement"
IMPLEMENTATION/METHODS,0.42435424354243545,"In the next result, we show that our policy update leads to a monotonic improvement of the perfor-
mance function J up to the advantage function estimation error.
Proposition 4 (Performance improvement). Let ⇡2 ⇧. Consider solutions ˜⇡⇤2 ⇧and λ⇤≥0
of problems (P) and (D), respectively. If the true advantage function A⇡is approximated by some
estimated advantage function ˆA⇡such that kA⇡−ˆA⇡k1 < 1, then the following bound holds:"
IMPLEMENTATION/METHODS,0.4280442804428044,"J(˜⇡⇤) ≥J(⇡) +
λ⇤ 1 −γ Z S"
IMPLEMENTATION/METHODS,0.4317343173431734,"C(⇡(·|s), ˜⇡⇤(·|s))d⇢˜⇡⇤(s) −2kA⇡−ˆA⇡k1"
IMPLEMENTATION/METHODS,0.4354243542435424,"1 −γ
.
(11)"
IMPLEMENTATION/METHODS,0.43911439114391143,"Proposition 4 indicates that optimal transport-based trust region policy optimization leads to mono-
tonic improvement of the performance function when we have access to the true advantage function.
The proof, postponed to Appendix B.2, of this result builds on the performance difference lemma
(see (2)) and uses the closed-form expression of the optimal policy solving problem (P) as constructed
in the proof of Theorem 1 (see Corollary 2). The analog of this result for a ﬁnite action space was
proved in [33, Theorem 2, p. 5]. To the best of our knowledge, this result is novel for the continuous
state-action space setting."
IMPLEMENTATION/METHODS,0.44280442804428044,"5
Practical Optimal Transport Trust Region Policy Optimization Algorithm"
IMPLEMENTATION/METHODS,0.44649446494464945,"In this section, we use the duality results on Section 4 to derive a practical algorithm for OT-
TRPO. Herein, we restrict the policy search set ⇧to the set of policies ⇡✓parametrized by a
vector ✓2 Rd for some integer d > 0. We require the policy parametrization to be continuously
differentiable with respect to ✓(for every state and action). This way, we simultaneously cover
the direct parametrization (for which Corollary 3 directly provides a policy update) as well as
commonly used policy parametrizations (e.g., softmax and the Gaussian policies). Accordingly, the
dual problem (D) can be reformulated as follows for every ✓2 Rd: min"
IMPLEMENTATION/METHODS,0.45018450184501846,"λ≥0 G(λ, ✓) := λ"" + Z S Z A"
IMPLEMENTATION/METHODS,0.45387453874538747,"max
a02A{A⇡✓(s, a0) −λc(a, a0)}d⇡✓(a|s)d⇢⇡✓(s).
(12)"
IMPLEMENTATION/METHODS,0.4575645756457565,"Given a current policy represented by the vec-
tor ✓, we ﬁrst solve the one-dimensional convex
problem (12) to obtain its solution λ⇤. Then, we
use the optimal dual multiplier λ⇤to derive the
optimal policy update within the trust region. The
procedure is summarized in Algorithm 1.
De-
pending on the parametrization of the policy, the
steps of Section 5 are as follows:"
IMPLEMENTATION/METHODS,0.4612546125461255,Algorithm 1 OT-TRPO.
IMPLEMENTATION/METHODS,0.46494464944649444,"1: Initialize ⇡✓0
2: for all t = 0, 1, . . . do
3:
Estimate A⇡✓t and ⇢⇡✓t .
4:
Compute λ⇤2 argminλ≥0G(λ, ✓t).
5:
Update ✓t to ✓t+1 using λ⇤.
6: end for"
IMPLEMENTATION/METHODS,0.46863468634686345,"Algorithm 1 - step 3. In the discrete setting, the visitation frequency is estimated via Monte Carlo
methods. In the continuous case, we weight every visited state equally. We propose three ways to
estimate the unknown advantage function via samples3:
1. Monte Carlo methods or TD-learning (for discrete settings only);
2. General Advantage Estimation (GAE) [30], using a neural network to approximate the value"
IMPLEMENTATION/METHODS,0.47232472324723246,"function like in standard actor-critic methods; and
3. Direct estimation via non-linear approximators (e.g., using directly a neural network for the"
IMPLEMENTATION/METHODS,0.47601476014760147,"advantage function).
Algorithm 1 - step 4 (evaluation of G). Depending on the setting, we propose various ways to
evaluate G(λ, ✓). They all apply to both continuous and discrete states.
1. Finite actions: Since the maximization in (12) is over ﬁnitely many actions, we can directly"
IMPLEMENTATION/METHODS,0.4797047970479705,"evaluate (12) for any λ ≥0.
2. Gaussian policy parametrization: With m(s) being the mean of the Gaussian policy (with ﬁxed"
IMPLEMENTATION/METHODS,0.4833948339483395,"variance), we approximate G(λ, ✓) by"
IMPLEMENTATION/METHODS,0.4870848708487085,"G(λ, ✓) ⇡ ("
IMPLEMENTATION/METHODS,0.4907749077490775,"λ"" + P"
IMPLEMENTATION/METHODS,0.4944649446494465,"s2 ˆ
S maxa02{a,m(s)}{A⇡✓(s, a0) −λc(m✓(s), a0)}⇢⇡✓(s)
if A⇡✓via GAE,
λ"" + P"
IMPLEMENTATION/METHODS,0.4981549815498155,"s2 ˆ
S maxa02 ˆ
A(s){A⇡✓(s, a0) −λc(m✓(s), a0)}⇢⇡✓(s)
if A⇡✓via NN,"
IMPLEMENTATION/METHODS,0.5018450184501845,"where ˆS are the states visited in the trajectory and ˆ
A(s) is a (possibly state-dependent) collection
of samples from A.
3. Arbitrary policy parametrization: For a neural network approximation of the advantage function,"
IMPLEMENTATION/METHODS,0.5055350553505535,"we approximate G(λ, ✓) by"
IMPLEMENTATION/METHODS,0.5092250922509225,"G(λ, ✓) ⇡λ"" + P"
IMPLEMENTATION/METHODS,0.5129151291512916,"s2 ˆ
S P"
IMPLEMENTATION/METHODS,0.5166051660516605,"a2 ˆ
A1(s) maxa02 ˆ
A2(s){A⇡✓(s, a0) −λc(a, a0)}⇡✓(a|s)⇢⇡✓(s),"
IMPLEMENTATION/METHODS,0.5202952029520295,"where ˆS are the states visited in the trajectory and ˆ
Ai(s) are (possibly state-dependent) collections
of samples from A.
Algorithm 1 - step 4 (solving for λ⇤). Since (D) is a one-dimensional convex optimization problem,
λ⇤can be found using any solver for convex optimization problems."
IMPLEMENTATION/METHODS,0.5239852398523985,"Algorithm 1 - step 5. Update the parameter vector ✓.
1. Direct parametrization (ﬁnite spaces): Update ✓according to (10) and (9).
2. Direct parametrization via policy network (continuous states, discrete actions): Use (10) and (9)"
IMPLEMENTATION/METHODS,0.5276752767527675,"to compute the optimal policy update at the visited states, denoted by ⇡⇤"
IMPLEMENTATION/METHODS,0.5313653136531366,"✓t. Then, update the policy"
IMPLEMENTATION/METHODS,0.5350553505535055,network by performing gradient descent on the loss L(✓) = P
IMPLEMENTATION/METHODS,0.5387453874538746,"s2 ˆ
S ⇢⇡✓t (s)"
IMPLEMENTATION/METHODS,0.5424354243542435,55⇡✓(·|s) −⇡⇤
IMPLEMENTATION/METHODS,0.5461254612546126,✓t(·|s) 552
IMPLEMENTATION/METHODS,0.5498154981549815,to steer ⇡✓towards the optimal policy update ⇡⇤
IMPLEMENTATION/METHODS,0.5535055350553506,"✓t within the trust region.
3. Arbitrary policy parametrization: Since there are inﬁnitely many actions, the computation of the"
IMPLEMENTATION/METHODS,0.5571955719557196,"maximization is computationally demanding, and so Corollary 2 cannot be directly utilized for
the policy update. Yet, we can update the policy via gradient ascent. The intuition is as follows:
according to Corollary 2, the optimal policy update attains the maximum maxa02A{A⇡✓t (s, a0) −
λ⇤c(a, a0)} at each state. Thus, we can steer the policy ⇡✓to maximize ✓7! P"
IMPLEMENTATION/METHODS,0.5608856088560885,"s2 ˆ
S R"
IMPLEMENTATION/METHODS,0.5645756457564576,"A maxa02A{A⇡✓t (s, a0) −λ⇤c(a✓, a0)}d⇡✓(a✓|s)⇢⇡✓t (s)."
IMPLEMENTATION/METHODS,0.5682656826568265,"In the particular case of a Gaussian policy with parametrized mean (and ﬁxed variance), combined
with GAE estimate of the advantage function, one can maximize ✓7! P"
IMPLEMENTATION/METHODS,0.5719557195571956,"s2 ˆ
S max{A⇡✓t (s, a0) −λ⇤c(m✓(s), a0), 0}⇢⇡✓t (s)."
IMPLEMENTATION/METHODS,0.5756457564575646,"3In the experiments reported in the main paper we used the ﬁrst and the second method for discrete and
continuous environments, respectively. In Appendix A.5 we further comment on the different methods."
IMPLEMENTATION/METHODS,0.5793357933579336,"Intuitively, this update implicitly estimates the transport maps T λ⇤and T λ⇤, which are needed for
the optimal policy update. This way, we steer the policy network towards the optimal policy update
within the trust region. Among others, this policy update allows for the following interpretation:
imposing an optimal transport-based trust region constraints is, at least formally, equivalent to
maximizing a regularized advantage function, where the value of the regularization λ⇤≥0 is
based on the transport cost c and the radius of the trust region ""."
RESULTS/EXPERIMENTS,0.5830258302583026,"6
Experiments and Insights"
RESULTS/EXPERIMENTS,0.5867158671586716,"In this section, we evaluate the performance of OT-TRPO across a variety of environments [5, 36]
of increasing complexity, ranging from discrete to continuous settings. We compare it to the
classical TRPO [29, 9] and PPO [31, 9], with Advantage Actor Critic (A2C) [17, 9], with the
recent approaches leveraging the Wasserstein distance, BGPG [22] and WNPG [20] (in continuous
settings), and with WPO [33] (in discrete settings). The training curves are shown in Fig. 1;
see Appendix A.3 for implementation details, Appendix A.6 for further details on the experimental
results, and Appendix A.4 for an ablation study on our algorithm."
RESULTS/EXPERIMENTS,0.5904059040590406,"0
20
40
−4,000"
RESULTS/EXPERIMENTS,0.5940959409594095,"−2,000 0"
RESULTS/EXPERIMENTS,0.5977859778597786,Learning steps (103) J(✓⇤)
RESULTS/EXPERIMENTS,0.6014760147601476,CliffWalking-v0
RESULTS/EXPERIMENTS,0.6051660516605166,"0
200
400 −600 −400 −200 0"
RESULTS/EXPERIMENTS,0.6088560885608856,Learning steps (103)
RESULTS/EXPERIMENTS,0.6125461254612546,Taxi-v3
RESULTS/EXPERIMENTS,0.6162361623616236,"0
20
40
60
80
100 −100 0 100"
RESULTS/EXPERIMENTS,0.6199261992619927,Learning steps (103)
RESULTS/EXPERIMENTS,0.6236162361623616,MountainCarContinuous-v0
RESULTS/EXPERIMENTS,0.6273062730627307,"0
0.2
0.4
0.6
0.8
1 0 1,000 2,000 3,000"
RESULTS/EXPERIMENTS,0.6309963099630996,Learning steps (106) J(✓⇤)
RESULTS/EXPERIMENTS,0.6346863468634686,Hopper-v3
RESULTS/EXPERIMENTS,0.6383763837638377,"0
0.2
0.4
0.6
0.8
1 0 200 400"
RESULTS/EXPERIMENTS,0.6420664206642066,Learning steps (106)
RESULTS/EXPERIMENTS,0.6457564575645757,Swimmer-v3
RESULTS/EXPERIMENTS,0.6494464944649446,"0
0.2
0.4
0.6
0.8
1 0 2,000 4,000 6,000"
RESULTS/EXPERIMENTS,0.6531365313653137,Learning steps (106)
RESULTS/EXPERIMENTS,0.6568265682656826,HalfCheetah-v3
RESULTS/EXPERIMENTS,0.6605166051660517,"OT-TRPO
TRPO
WPO
PPO
A2C
BGPG
WNPG"
RESULTS/EXPERIMENTS,0.6642066420664207,"Figure 1: Cumulative rewards during the training process in different environments. The shaded area
represents the mean ± the standard deviation across 10 independent runs. Every policy evaluation in
each run is averaged over 10 sampled trajectories."
RESULTS/EXPERIMENTS,0.6678966789667896,"Our approach is shown to consistently improve over the other algorithms: OT-TRPO leads to larger
ﬁnal returns, with lower variance, and only in few cases at the expense of a slightly slower learning
curve. Four remarks are in order. First, the performance gain of OT-TRPO compared to BGPG and
WNPG conﬁrms that trust regions help stabilize training, as already observed in [29]. Second, optimal
transport discrepancies induce a more natural notion of “closeness” between policies compared to the
KL divergence (e.g., see [3, Example 2.1]). For instance, in CliffWalking-v0, consider the optimal
policy ⇡⇤and the candidate policy ⇡depicted below, which differ at one state only (see ﬁgure)."
RESULTS/EXPERIMENTS,0.6715867158671587,"⇡⇤: optimal policy, ⇡: “close” policy"
RESULTS/EXPERIMENTS,0.6752767527675276,"The optimal transport discrepancy between ⇡and ⇡⇤is
⇢⇡(s)C(⇡(·|s), ⇡⇤(·|s)) = ⇢⇡(s)c(Down, Right). When us-
ing the KL divergence, instead, the discrepancy is inﬁnite, since
the two policies do not share the same support. In particular, if
initialized with ⇡, TRPO cannot converge to the optimal policy,
regardless of the radius of the trust region. Third, OT-TRPO
improves on WPO, in two ways: (i) it leads to superior per-
formances of the trained policies and (ii) it does not violate"
RESULTS/EXPERIMENTS,0.6789667896678967,"the trust region constraint (which, e.g., in Taxi-v3 is the case for 72% of the updates of WPO).
This performance improvement results from the “mass splitting” (see Remark 3), which is the only
difference between the two algorithms (in discrete settings). Fourth, in continuous settings, the per-
formance of OT-TRPO is aligned with the best-performing alternative approach. In the environment
Swimmer-v3, it even yields an improvement of more than 50% in the performance of the trained
agent."
CONCLUSION/DISCUSSION,0.6826568265682657,"7
Conclusion and Future Work"
CONCLUSION/DISCUSSION,0.6863468634686347,"We studied trust region policy optimization for continuous state-action spaces whereby the trust
region is deﬁned in terms of a general optimal transport discrepancy. Our analysis bases on a one-
dimensional convex dual reformulation of the optimization problem for the policy update which
(i) enjoys strong duality and (ii) directly characterizes the optimal policy update, bypassing the
computational burden of evaluating optimal transport discrepancies. Moreover, we show that the
policy update can yield a monotonic improvement of the performance index. Empowered by our
theoretic results, we propose a novel algorithm, OT-TRPO, for trust region policy optimization with
optimal transport discrepancies. We evaluate its performance across several environments. Our
results reveal that trust regions deﬁned by optimal transport discrepancies can offer advantages over
the KL divergence or non-trust region methods."
CONCLUSION/DISCUSSION,0.6900369003690037,"There are several research directions that merit further investigation. We highlight two. First, transport
costs provide us actionable knobs to shape the geometry of the trust region, and can be used to encode
prior knowledge on the environment or preferred exploration strategies. Second, we would like to
study the convergence properties of the proposed algorithm."
CONCLUSION/DISCUSSION,0.6937269372693727,Acknowledgements
CONCLUSION/DISCUSSION,0.6974169741697417,"This project has received funding from Google Brain, Swiss National Science Foundation under the
NCCR Automation (grant agreement 51NF40_180545), and it was partially supported by the ETH
AI Center."
REFERENCES,0.7011070110701108,References
REFERENCES,0.7047970479704797,[1] Charalambos D. Aliprantis and Kim C. Border. Inﬁnite Dimensional Analysis: a Hitchhiker’s
REFERENCES,0.7084870848708487,"Guide. Springer, Berlin; London, 2006."
REFERENCES,0.7121771217712177,"[2] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré. Gradient ﬂows: In Metric Spaces and in"
REFERENCES,0.7158671586715867,"the Space of Probability Measures. Springer, 2008."
REFERENCES,0.7195571955719557,"[3] Liviu Aolaritei, Nicolas Lanzetti, Hongruyu Chen, and Florian Dörﬂer. Uncertainty propagation"
REFERENCES,0.7232472324723247,"via optimal transport ambiguity sets. arXiv preprint arXiv:2205.00343, 2022."
REFERENCES,0.7269372693726938,[4] Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport.
REFERENCES,0.7306273062730627,"Mathematics of Operations Research, 44(2):565–600, 2019."
REFERENCES,0.7343173431734318,"[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,"
REFERENCES,0.7380073800738007,"and Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.7416974169741697,[6] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances
REFERENCES,0.7453874538745388,"in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013."
REFERENCES,0.7490774907749077,"[7] Rui Gao, Xi Chen, and Anton J. Kleywegt. Wasserstein distributionally robust optimization and"
REFERENCES,0.7527675276752768,"variation regularization. arXiv preprint arXiv:1712.06050, 2017."
REFERENCES,0.7564575645756457,[8] Rui Gao and Anton J. Kleywegt. Distributionally robust stochastic optimization with Wasser-
REFERENCES,0.7601476014760148,"stein distance. arXiv preprint arXiv:1604.02199, 2016."
REFERENCES,0.7638376383763837,"[9] Ashley Hill, Antonin Rafﬁn, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene"
REFERENCES,0.7675276752767528,"Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert,
Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https:
//github.com/hill-a/stable-baselines, 2018."
REFERENCES,0.7712177121771218,"[10] Łukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Bła˙zej Osi´nski, Roy H Campbell, Konrad"
REFERENCES,0.7749077490774908,"Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model based
reinforcement learning for Atari. In International Conference on Learning Representations,
2019."
REFERENCES,0.7785977859778598,[11] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning.
REFERENCES,0.7822878228782287,"In International Conference on Machine Learning, pages 267–274. PMLR, 2002."
REFERENCES,0.7859778597785978,"[12] Achim Klenke. Probability Theory: A Comprehensive Course. Springer, 2008."
REFERENCES,0.7896678966789668,"[13] Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shaﬁeezadeh-"
REFERENCES,0.7933579335793358,"Abadeh. Wasserstein distributionally robust optimization: Theory and applications in machine
learning. In Operations research & management science in the age of analytics, pages 130–166.
Informs, 2019."
REFERENCES,0.7970479704797048,"[14] Nicolas Lanzetti, Saverio Bolognani, and Florian Dörﬂer. First-order conditions for optimization"
REFERENCES,0.8007380073800738,"in the Wasserstein space. arXiv preprint arXiv:2209.12197, 2022."
REFERENCES,0.8044280442804428,"[15] David G. Luenberger. Optimization by vector space methods. John Wiley & Sons, 1997."
REFERENCES,0.8081180811808119,"[16] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. Resource manage-"
REFERENCES,0.8118081180811808,"ment with deep reinforcement learning. In 15th ACM workshop on hot topics in networks, pages
50–56, 2016."
REFERENCES,0.8154981549815498,"[17] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,"
REFERENCES,0.8191881918819188,"Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-
ment learning. In International Conference on Machine Learning, pages 1928–1937. PMLR,
2016."
REFERENCES,0.8228782287822878,"[18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan"
REFERENCES,0.8265682656826568,"Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013."
REFERENCES,0.8302583025830258,[19] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
REFERENCES,0.8339483394833949,"using the Wasserstein metric: performance guarantees and tractable reformulations. Mathemati-
cal Programming, 171(1):115–166, 2018."
REFERENCES,0.8376383763837638,"[20] Ted Moskovitz, Michael Arbel, Ferenc Huszar, and Arthur Gretton. Efﬁcient Wasserstein"
REFERENCES,0.8413284132841329,"natural gradients for reinforcement learning. arXiv preprint arXiv:2010.05380, 2020."
REFERENCES,0.8450184501845018,"[21] James R. Munkres. Topology. Featured Titles for Topology. Prentice Hall, Incorporated, 2000."
REFERENCES,0.8487084870848709,"[22] Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Anna Choromanska, Krzysztof Choroman-"
REFERENCES,0.8523985239852399,"ski, and Michael I. Jordan. Learning to score behaviors for guided policy optimization. In
International Conference on Machine Learning, pages 7401–7410. PMLR, 2020."
REFERENCES,0.8560885608856088,[23] Gabriel Peyré and Marco Cuturi. Computational optimal transport. Foundations and Trends in
REFERENCES,0.8597785977859779,"Machine Learning, 11(5-6):1–257, 2019."
REFERENCES,0.8634686346863468,[24] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.
REFERENCES,0.8671586715867159,"John Wiley & Sons, 2005."
REFERENCES,0.8708487084870848,[25] Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv
REFERENCES,0.8745387453874539,"preprint arXiv:1908.05659, 2019."
REFERENCES,0.8782287822878229,[26] Pierre H. Richemond and Brendan Maginnis. On Wasserstein reinforcement learning and the
REFERENCES,0.8819188191881919,"Fokker-Planck equation. arXiv preprint arXiv:1712.07185, 2017."
REFERENCES,0.8856088560885609,"[27] Ralph Tyrell Rockafellar. Convex Analysis. Princeton University Press, 2015."
REFERENCES,0.8892988929889298,"[28] Walter Rudin. Real and Complex Analysis, 3rd Ed. McGraw-Hill, Inc., USA, 1987."
REFERENCES,0.8929889298892989,"[29] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust"
REFERENCES,0.8966789667896679,"region policy optimization. In International Conference on Machine Learning, pages 1889–1897.
PMLR, 2015."
REFERENCES,0.9003690036900369,"[30] John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-"
REFERENCES,0.9040590405904059,"dimensional continuous control using generalized advantage estimation. In International
Conference on Learning Representations, 2016."
REFERENCES,0.9077490774907749,"[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal"
REFERENCES,0.9114391143911439,"policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.915129151291513,"[32] D. Silver, A. Huang, and C. et al. Maddison. Mastering the game of go with deep neural"
REFERENCES,0.9188191881918819,"networks and tree search. Nature 529, 484–489, 2016."
REFERENCES,0.922509225092251,"[33] Jun Song, Chaoyue Zhao, and Niao He. Efﬁcient Wasserstein and Sinkhorn policy optimization, 2022."
REFERENCES,0.9261992619926199,"[34] Bahar Taskesen, Soroosh Shaﬁeezadeh-Abadeh, and Daniel Kuhn. Semi-discrete optimal"
REFERENCES,0.9298892988929889,"transport: Hardness, regularization and numerical solution. arXiv preprint arXiv:2103.06263,
2021."
REFERENCES,0.933579335793358,[35] Dávid Terjék and Diego González-Sánchez. Optimal transport with f-divergence regularization
REFERENCES,0.9372693726937269,"and generalized Sinkhorn algorithm. In International Conference on Artiﬁcial Intelligence and
Statistics, pages 5135–5165. PMLR, 2022."
REFERENCES,0.940959409594096,"[36] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based"
REFERENCES,0.9446494464944649,"control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages
5026–5033. IEEE, 2012."
REFERENCES,0.948339483394834,"[37] Cédric Villani. Optimal Transport: Old and New. Springer-Verlag Berlin Heidelberg, 2008."
REFERENCES,0.9520295202952029,"[38] Patrick Nadeem Ward, Ariella Smofsky, and Avishek Joey Bose. Improving exploration in"
REFERENCES,0.955719557195572,"soft-actor-critic with normalizing ﬂows policies. arXiv preprint arXiv:1906.02771, 2019."
REFERENCES,0.959409594095941,"[39] Konstantia Xenou, Georgios Chalkiadakis, and Stergos Afantenos. Deep reinforcement learning"
REFERENCES,0.9630996309963099,"in strategic board game environments. Springer International Publishing, 2019."
REFERENCES,0.966789667896679,"[40] Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as"
REFERENCES,0.9704797047970479,"wasserstein gradient ﬂows. In International Conference on Machine Learning, pages 5737–
5746. PMLR, 2018."
REFERENCES,0.974169741697417,[41] Chaoyue Zhao and Yongpei Guan. Data-driven risk-averse stochastic optimization with Wasser-
REFERENCES,0.977859778597786,"stein metric. Operations Research Letters, 46(2):262–267, 2018."
REFERENCES,0.981549815498155,"[42] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and"
REFERENCES,0.985239852398524,"Zhenhui Li. DRN: A deep reinforcement learning framework for news recommendation. In
World Wide Web Conference, pages 167–176, 2018."
REFERENCES,0.988929889298893,"[43] Yue Zheng. Reinforcement learning and video games. arXiv preprint arXiv:1909.04751, 2019."
REFERENCES,0.992619926199262,"[44] Zhenpeng Zhou, Xiaocheng Li, and Richard N Zare. Optimizing chemical reactions with deep"
REFERENCES,0.996309963099631,"reinforcement learning. ACS Central Science, 3(12):1337–1344, 2017."
