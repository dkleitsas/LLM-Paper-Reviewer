Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002304147465437788,"Existing ofÔ¨Çine reinforcement learning (RL) methods face a few major challenges,
particularly the distributional shift between the learned policy and the behavior
policy. OfÔ¨Çine Meta-RL is emerging as a promising approach to address these
challenges, aiming to learn an informative meta-policy from a collection of tasks.
Nevertheless, as shown in our empirical studies, ofÔ¨Çine Meta-RL could be outper-
formed by ofÔ¨Çine single-task RL methods on tasks with good quality of datasets,
indicating that a right balance has to be delicately calibrated between ‚Äúexploring‚Äù
the out-of-distribution state-actions by following the meta-policy and ‚Äúexploiting‚Äù
the ofÔ¨Çine dataset by staying close to the behavior policy. Motivated by such em-
pirical analysis, we propose model-based ofÔ¨Çine Meta-RL with regularized Policy
Optimization (MerPO), which learns a meta-model for efÔ¨Åcient task structure in-
ference and an informative meta-policy for safe exploration of out-of-distribution
state-actions. In particular, we devise a new meta-Regularized model-based Actor-
Critic (RAC) method for within-task policy optimization, as a key building block
of MerPO, using both conservative policy evaluation and regularized policy im-
provement; and the intrinsic tradeoff therein is achieved via striking the right bal-
ance between two regularizers, one based on the behavior policy and the other on
the meta-policy. We theoretically show that the learnt policy offers guaranteed
improvement over both the behavior policy and the meta-policy, thus ensuring the
performance improvement on new tasks via ofÔ¨Çine Meta-RL. Experiments corrob-
orate the superior performance of MerPO over existing ofÔ¨Çine Meta-RL methods."
INTRODUCTION,0.004608294930875576,"1
INTRODUCTION"
INTRODUCTION,0.0069124423963133645,"OfÔ¨Çine reinforcement learning (a.k.a., batch RL) has recently attracted extensive attention by learn-
ing from ofÔ¨Çine datasets previously collected via some behavior policy (Kumar et al., 2020). How-
ever, the performance of existing ofÔ¨Çine RL methods could degrade signiÔ¨Åcantly due to the following
issues: 1) the possibly poor quality of ofÔ¨Çine datasets (Levine et al., 2020) and 2) the inability to
generalize to different environments (Li et al., 2020b). To tackle these challenges, ofÔ¨Çine Meta-RL
(Li et al., 2020a; Dorfman & Tamar, 2020; Mitchell et al., 2020; Li et al., 2020b) has emerged very
recently by leveraging the knowledge of similar ofÔ¨Çine RL tasks (Yu et al., 2021a). The main aim
of these studies is to enable quick policy adaptation for new ofÔ¨Çine tasks, by learning a meta-policy
with robust task structure inference that captures the structural properties across training tasks."
INTRODUCTION,0.009216589861751152,Figure 1: FOCAL vs. COMBO.
INTRODUCTION,0.01152073732718894,"Because tasks are trained on ofÔ¨Çine datasets, value
overestimation (Fujimoto et al., 2019) inevitably oc-
curs in dynamic programming based ofÔ¨Çine Meta-RL,
resulted from the distribution shift between the be-
havior policy and the learnt task-speciÔ¨Åc policy. To
guarantee the learning performance on new ofÔ¨Çine
tasks, a right balance has to be carefully calibrated be-
tween ‚Äúexploring‚Äù the out-of-distribution state-actions
by following the meta-policy, and ‚Äúexploiting‚Äù the of-
Ô¨Çine dataset by staying close to the behavior policy.
However, such a unique ‚Äúexploration-exploitation‚Äù tradeoff has not been considered in existing of-
Ô¨Çine Meta-RL approaches, which would likely limit their ability to handle diverse ofÔ¨Çine datasets"
INTRODUCTION,0.013824884792626729,Published as a conference paper at ICLR 2022
INTRODUCTION,0.016129032258064516,"particularly towards those with good behavior policies. To illustrate this issue more concretely, we
compare the performance between a state-of-the-art ofÔ¨Çine Meta-RL algorithm FOCAL (Li et al.,
2020b) and an ofÔ¨Çine single-task RL method COMBO (Yu et al., 2021b) in two new ofÔ¨Çine tasks.
As illustrated in Figure 1, while FOCAL performs better than COMBO on the task with a bad-quality
dataset (left plot in Figure 1), it is outperformed by COMBO on the task with a good-quality dataset
(right plot in Figure 1). Clearly, existing ofÔ¨Çine Meta-RL fails in several standard environments (see
Figure 1 and Figure 11) to generalize universally well over datasets with varied quality. In order to
Ô¨Åll such a substantial gap, we seek to answer the following key question in ofÔ¨Çine Meta-RL:"
INTRODUCTION,0.018433179723502304,"How to design an efÔ¨Åcient ofÔ¨Çine Meta-RL algorithm to strike the right balance between exploring
with the meta-policy and exploiting the ofÔ¨Çine dataset?"
INTRODUCTION,0.020737327188940093,"To this end, we propose MerPO, a model-based ofÔ¨Çine Meta-RL approach with regularized Policy
Optimization, which learns a meta-model for efÔ¨Åcient task structure inference and an informa-
tive meta-policy for safe exploration of out-of-distribution state-actions. Compared to existing ap-
proaches, MerPO achieves: (1) safe policy improvement: performance improvement can be guar-
anteed for ofÔ¨Çine tasks regardless of the quality of the dataset, by strike the right balance between
exploring with the meta-policy and exploiting the ofÔ¨Çine dataset; and (2) better generalization ca-
pability: through a conservative utilization of the learnt model to generate synthetic data, MerPO
aligns well with a recently emerging trend in supervised meta-learning to improve the generaliza-
tion ability by augmenting the tasks with ‚Äúmore data‚Äù (Rajendran et al., 2020; Yao et al., 2021). Our
main contributions can be summarized as follows:"
INTRODUCTION,0.02304147465437788,"(1) Learnt dynamics models not only serve as a natural remedy for task structure inference in of-
Ô¨Çine Meta-RL, but also facilitate better exploration of out-of-distribution state-actions by generating
synthetic rollouts. With this insight, we develop a model-based approach, where an ofÔ¨Çine meta-
model is learnt to enable efÔ¨Åcient task model learning for each ofÔ¨Çine task. More importantly,
we propose a meta-regularized model-based actor-critic method (RAC) for within-task policy opti-
mization, where a novel regularized policy improvement module is devised to calibrate the unique
‚Äúexploration-exploitation‚Äù tradeoff by using an interpolation between two regularizers, one based
on the behavior policy and the other on the meta-policy. Intuitively, RAC generalizes COMBO to
the multi-task setting, with introduction of a novel regularized policy improvement module to strike
a right balance between the impacts of the meta-policy and the behavior policy."
INTRODUCTION,0.02534562211981567,"(2) We theoretically show that under mild conditions, the learnt task-speciÔ¨Åc policy based on MerPO
offers safe performance improvement over both the behavior policy and the meta-policy with high
probability. Our results also provide a guidance for the algorithm design in terms of how to ap-
propriately select the weights in the interpolation, such that the performance improvement can be
guaranteed for new ofÔ¨Çine RL tasks."
INTRODUCTION,0.027649769585253458,"(3) We conduct extensive experiments to evaluate the performance of MerPO. More speciÔ¨Åcally, the
experiments clearly show the safe policy improvement offered in MerPO, corroborating our theo-
retical results. Further, the superior performance of MerPO over existing ofÔ¨Çine Meta-RL methods
suggests that model-based approaches can be more beneÔ¨Åcial in ofÔ¨Çine Meta-RL."
RELATED WORK,0.029953917050691243,"2
RELATED WORK"
RELATED WORK,0.03225806451612903,"OfÔ¨Çine single-task RL. Many existing model-free ofÔ¨Çine RL methods regularize the learnt policy
to be close to the behavior policy by, e.g., distributional matching (Fujimoto et al., 2019), support
matching (Kumar et al., 2019), importance sampling (Nachum et al., 2019; Liu et al., 2020), learning
lower bounds of true Q-values (Kumar et al., 2020). Along a different avenue, model-based algo-
rithms learn policies by leveraging a dynamics model obtained with the ofÔ¨Çine dataset. (Matsushima
et al., 2020) directly constrains the learnt policy to the behavior policy as in model-free algorithms.
To penalize the policy for visiting states where the learnt model is likely to be incorrect, MOPO
(Yu et al., 2020) and MoREL (Kidambi et al., 2020) modify the learnt dynamics such that the value
estimates are conservative when the model uncertainty is above a threshold. To remove the need
of uncertainty quantiÔ¨Åcation, COMBO (Yu et al., 2021b) is proposed by combining model-based
policy optimization (Janner et al., 2019) and conservative policy evaluation (Kumar et al., 2020)."
RELATED WORK,0.03456221198156682,"OfÔ¨Çine Meta-RL. A few very recent studies have explored the ofÔ¨Çine Meta-RL. Particularly, (Li
et al., 2020a) considers a special scenario where the task identity is spuriously inferred due to biased"
RELATED WORK,0.03686635944700461,Published as a conference paper at ICLR 2022
RELATED WORK,0.03917050691244239,"datasets, and applies the triplet loss to robustify the task inference with reward relabelling. (Dorfman
& Tamar, 2020) extends an online Meta-RL method VariBAD (Zintgraf et al., 2019) to the ofÔ¨Çine
setup, and assumes known reward functions and shared dynamics across tasks. Based on MAML
(Finn et al., 2017), (Mitchell et al., 2020) proposes an ofÔ¨Çine Meta-RL algorithm with advantage
weighting loss, and learns initializations for both the value function and the policy, where they
consider the ofÔ¨Çine dataset in the format of full trajectories in order to evaluate the advantage. Based
on the off-policy Meta-RL method PEARL (Rakelly et al., 2019), (Li et al., 2020b) combines the idea
of deterministic context encoder and behavior regularization, under the assumption of deterministic
MDP. Different from the above works, we study a more general ofÔ¨Çine Meta-RL problem. More
importantly, MerPO strikes a right balance between exploring with the meta-policy and exploiting
the ofÔ¨Çine dataset, which guarantees safe performance improvement for new ofÔ¨Çine tasks."
PRELIMINARIES,0.041474654377880185,"3
PRELIMINARIES"
PRELIMINARIES,0.04377880184331797,"Consider a Markov decision process (MDP) M = (S, A, T, r, ¬µ0, Œ≥) with state space S, action
space A, the environment dynamics T(s‚Ä≤|s, a), reward function r(s, a), initial state distribution
¬µ0, and Œ≥ ‚àà(0, 1) is the discount factor. Without loss of generality, we assume that |r(s, a)| ‚â§
Rmax. Given a policy œÄ, let dœÄ
M(s) := (1 ‚àíŒ≥) P‚àû
t=0 Œ≥tPM(st = s|œÄ) denote the discounted
marginal state distribution, where PM(st = s|œÄ) denotes the probability of being in state s at
time t by rolling out œÄ in M. Accordingly, let dœÄ
M(s, a) := dœÄ
M(s)œÄ(a|s) denote the discounted
marginal state-action distribution, and J(M, œÄ) :=
1
1‚àíŒ≥ E(s,a)‚àºdœÄ
M(s,a)[r(s, a)] denote the expected
discounted return. The goal of RL is to Ô¨Ånd the optimal policy that maximizes J(M, œÄ). In ofÔ¨Çine
RL, no interactions with the environment are allowed, and we only have access to a Ô¨Åxed dataset
D = {(s, a, r, s‚Ä≤)} collected by some unknown behavior policy œÄŒ≤. Let dœÄŒ≤
M(s) be the discounted
marginal state distribution of œÄŒ≤. The dataset D is indeed sampled from dœÄŒ≤
M(s, a) = dœÄŒ≤
M(s)œÄŒ≤(a|s).
Denote M as the empirical MDP induced by D and d(s, a) as a sample-based version of dœÄŒ≤
M(s, a)."
PRELIMINARIES,0.04608294930875576,"In ofÔ¨Çine Meta-RL, consider a distribution of RL tasks p(M) as in standard Meta-RL (Finn et al.,
2017; Rakelly et al., 2019), where each task Mn is an MDP, i.e., Mn = (S, A, Tn, rn, ¬µ0,n, Œ≥), with
task-shared state and action spaces, and unknown task-speciÔ¨Åc dynamics and reward function.For
each task Mn, no interactions with the environment are allowed and we only have access to an
ofÔ¨Çine dataset Dn, collected by some unknown behavior policy œÄŒ≤,n. The main objective is to learn
a meta-policy based on a set of ofÔ¨Çine training tasks {Mn}N
n=1."
PRELIMINARIES,0.04838709677419355,"Conservative OfÔ¨Çine Model-Based Policy Optimization (COMBO). Recent model-based ofÔ¨Çine
RL algorithms, e.g., COMBO (Yu et al., 2021b), have demonstrated promising performance on
a single ofÔ¨Çine RL task by combining model-based policy optimization (Janner et al., 2019) and
conservative policy evaluation (CQL (Kumar et al., 2020)). Simply put, COMBO Ô¨Årst trains a
dynamics model bTŒ∏(s‚Ä≤|s, a) parameterized by Œ∏, via supervised learning on the ofÔ¨Çine dataset D.
The learnt MDP is constructed as c
M := (S, A, bT, r, ¬µ0, Œ≥). Then, the policy is learnt using D and
model-generated rollouts. SpeciÔ¨Åcally, deÔ¨Åne the action-value function (Q-function) as QœÄ(s, a) :=
E [P‚àû
t=0 Œ≥tr(st, at)|s0 = s, a0 = a], and the empirical Bellman operator as: bBœÄQ(s, a) = r(s, a)+
Œ≥E(s,a,s‚Ä≤)‚àºD[Q(s‚Ä≤, a‚Ä≤)], for a‚Ä≤ ‚àºœÄ(¬∑|s‚Ä≤). To penalize the Q functions in out-of-distribution state-
action tuples, COMBO employs conservative policy evaluation based on CQL:
bQk+1 ‚Üêarg min
Q(s,a) Œ≤(Es,a‚àºœÅ[Q(s, a)] ‚àíEs,a‚àºD[Q(s, a)]) + 1"
PRELIMINARIES,0.05069124423963134,"2Es,a,s‚Ä≤‚àºdf [(Q(s, a) ‚àíbBœÄ bQk(s, a))2] (1)"
PRELIMINARIES,0.052995391705069124,"where œÅ(s, a) := dœÄ
c
M(s)œÄ(a|s) is the discounted marginal distribution when rolling out œÄ in c
M,"
PRELIMINARIES,0.055299539170506916,"and df(s, a) = fdœÄŒ≤
M(s, a) + (1 ‚àíf)œÅ(s, a) for f ‚àà[0, 1]. The Bellman backup bBœÄ over df can
be interpreted as an f-interpolation of the backup operators under the empirical MDP (denoted by
BœÄ"
PRELIMINARIES,0.0576036866359447,"M) and the learnt MDP (denoted by BœÄ
c
M). Given the Q-estimation bQœÄ, the policy can be learnt by:"
PRELIMINARIES,0.059907834101382486,"œÄ‚Ä≤ ‚Üêarg max
œÄ
Es‚àºœÅ(s),a‚àºœÄ(¬∑|s)[ bQœÄ(s, a)].
(2)"
PRELIMINARIES,0.06221198156682028,"4
MERPO: MODEL-BASED OFFLINE META-RL WITH REGULARIZED
POLICY OPTIMIZATION"
PRELIMINARIES,0.06451612903225806,"Learnt dynamics models not only serves as a natural remedy for task structure inference in ofÔ¨Çine
Meta-RL, but also facilitates better exploration of out-of-distribution state-actions by generating"
PRELIMINARIES,0.06682027649769585,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.06912442396313365,Figure 2: Model-based ofÔ¨Çine Meta-RL with learning of ofÔ¨Çine meta-model and ofÔ¨Çine meta-policy.
PRELIMINARIES,0.07142857142857142,"synthetic rollouts (Yu et al., 2021b). Thus motivated, we propose a general framework of model-
based ofÔ¨Çine Meta-RL, as depicted in Figure 2. More speciÔ¨Åcally, the ofÔ¨Çine meta-model is Ô¨Årst
learnt by using supervised meta-learning, based on which the task-speciÔ¨Åc model can be quickly
adapted. Then, the main attention of this study is devoted to the learning of an informative meta-
policy via bi-level optimization, where 1) a model-based policy optimization approach is leveraged
in the inner loop for each task to learn a task-speciÔ¨Åc policy; and 2) the meta-policy is then updated
in the outer loop based on the learnt task-speciÔ¨Åc policies."
OFFLINE META-MODEL LEARNING,0.07373271889400922,"4.1
OFFLINE META-MODEL LEARNING"
OFFLINE META-MODEL LEARNING,0.07603686635944701,"Learning a meta-model based on the set of ofÔ¨Çine dataset {Dn}N
n=1 can be carried out via supervised
meta-learning. Many gradient-based meta-learning techniques can be applied here, e.g., MAML
(Finn et al., 2017) and Reptile (Nichol et al., 2018). In what follows, we outline the basic idea to
leverage the higher-order information of the meta-objective function. SpeciÔ¨Åcally, we consider a
proximal meta-learning approach, following the same line as in (Zhou et al., 2019):"
OFFLINE META-MODEL LEARNING,0.07834101382488479,"min
œÜm
Lmodel(œÜm) = EMn"
OFFLINE META-MODEL LEARNING,0.08064516129032258,"
min
Œ∏n"
OFFLINE META-MODEL LEARNING,0.08294930875576037,"h
E(s,a,s‚Ä≤)‚àºDn[log bTŒ∏n(s‚Ä≤|s, a)] + Œ∑‚à•Œ∏n ‚àíœÜm‚à•2
2
i
(3)"
OFFLINE META-MODEL LEARNING,0.08525345622119816,"where the learnt dynamics for each task Mn is parameterized by Œ∏n and the meta-model is parame-
terized by œÜm. Solving Eq. (3) leads to an ofÔ¨Çine meta-model."
OFFLINE META-MODEL LEARNING,0.08755760368663594,"Given the learnt meta-model TœÜ‚àóm, the dynamics model for an individual ofÔ¨Çine task j can be found
by solving the following problem via gradient descent with initialization TœÜ‚àóm using Dj, i.e.,
min
Œ∏j
E(s,a,s‚Ä≤)‚àºDj[log bTŒ∏j(s‚Ä≤|s, a)] + Œ∑‚à•Œ∏j ‚àíœÜ‚àó
m‚à•2
2.
(4)"
OFFLINE META-MODEL LEARNING,0.08986175115207373,"Compared to learning the dynamics model from scratch, adapting from TœÜ‚àóm can quickly generate
a dynamics model for task identity inference by leveraging the knowledge from similar tasks, and
hence improve the sample efÔ¨Åciency (Finn et al., 2017; Zhou et al., 2019)."
OFFLINE META-POLICY LEARNING,0.09216589861751152,"4.2
OFFLINE META-POLICY LEARNING"
OFFLINE META-POLICY LEARNING,0.0944700460829493,"In this section, we turn attention to tackle one main challenge in this study: How to learn an infor-
mative ofÔ¨Çine meta-policy in order to achieve the optimal tradeoff between ‚Äúexploring‚Äù the out-of-
distribution state-actions by following the meta-policy and ‚Äúexploiting‚Äù the ofÔ¨Çine dataset by staying
close to the behavior policy? Clearly, it is highly desirable for the meta-policy to safely ‚Äòexplore‚Äô
out-of-distribution state-action pairs, and for each task to utilize the meta-policy to mitigate the issue
of value overestimation."
OFFLINE META-POLICY LEARNING,0.0967741935483871,"4.2.1
HOW DO EXISTING PROXIMAL META-RL APPROACHES PERFORM?"
OFFLINE META-POLICY LEARNING,0.09907834101382489,"Proximal Meta-RL approaches have demonstrated remarkable performance in the online setting
(e.g., (Wang et al., 2020)), by explicitly regularizing the task-speciÔ¨Åc policy close to the meta-policy.
We Ô¨Årst consider the approach that applies the online Proximal Meta-RL method directly to devise
ofÔ¨Çine Meta-RL, which would lead to:"
OFFLINE META-POLICY LEARNING,0.10138248847926268,"max
œÄc
EMn"
OFFLINE META-POLICY LEARNING,0.10368663594470046,"
max
œÄn"
OFFLINE META-POLICY LEARNING,0.10599078341013825,"
E
s‚àºœÅn,
a‚àºœÄn(¬∑|s)"
OFFLINE META-POLICY LEARNING,0.10829493087557604,"h
ÀÜQœÄ
n(s, a)
i
‚àíŒªD(œÄn, œÄc)

(5)"
OFFLINE META-POLICY LEARNING,0.11059907834101383,"where œÄc is the ofÔ¨Çine meta-policy, œÄn is the task-speciÔ¨Åc policy, œÅn is the state marginal of œÅn(s, a)
for task n and D(¬∑, ¬∑) is some distance measure between two probability distributions. To alleviate
value overestimation, conservative policy evaluation can be applied to learn ÀÜQœÄ
n by using Eq. (1)."
OFFLINE META-POLICY LEARNING,0.11290322580645161,Published as a conference paper at ICLR 2022
OFFLINE META-POLICY LEARNING,0.1152073732718894,"Intuitively, Eq. (5) corresponds to generalizing COMBO to the multi-task setting, where a meta
policy œÄc is learned to regularize the within-task policy optimization."
OFFLINE META-POLICY LEARNING,0.1175115207373272,"Figure 3:
Performance of
proximal Meta-RL Eq. (5)."
OFFLINE META-POLICY LEARNING,0.11981566820276497,"To get a sense of how the meta-policy learnt using Eq. (5) performs,
we evaluate its performance in an ofÔ¨Çine variant of standard Meta-RL
benchmark Walker-2D-Params with good-quality datasets, and evalu-
ate the testing performance of the task-speciÔ¨Åc policy after Ô¨Åne-tuning
based on the learnt meta-policy, with respect to the meta-training
steps. As can be seen in Figure 3, the proximal Meta-RL algorithm
Eq. (5) performs surprisingly poorly and fails to learn an informative
meta-policy, despite conservative policy evaluation being applied in
within-task policy optimization to deal with the value overestimation.
In particular, the testing performance degrades along with the meta-
training process, implying that the quality of the learnt meta-policy is
in fact decreasing."
OFFLINE META-POLICY LEARNING,0.12211981566820276,"Why does the proximal Meta-RL method in Eq. (5) perform poorly in ofÔ¨Çine Meta-RL, even with
conservative policy evaluation? To answer this, it is worth to take a closer look at the within-task
policy optimization in Eq. (5), which is given as follows:
œÄn ‚Üêarg max
œÄn Es‚àºœÅn,a‚àºœÄn(¬∑|s)[ bQœÄ
n(s, a)] ‚àíŒªD(œÄn, œÄc).
(6)"
OFFLINE META-POLICY LEARNING,0.12442396313364056,"Clearly, the performance of Eq. (6) depends heavily on the quality of the meta-policy œÄc. A poor
meta-policy may have negative impact on the performance and result in a task-speciÔ¨Åc policy œÄn that
is even outperformed by the behaviour policy œÄŒ≤,n. Without online exploration, the quality of œÄn
could not be improved, which in turn leads to a worse meta-policy œÄc through Eq. (5). The iterative
meta-training process would eventually result in the performance degradation in Figure 3."
OFFLINE META-POLICY LEARNING,0.12672811059907835,"In a nutshell, simply following the meta-policy may lead to worse performance of ofÔ¨Çine tasks when
œÄŒ≤ is a better policy than œÄc. Since it is infeasible to guarantee the superiority of the meta-policy a
priori, it is necessary to balance the tradeoff between exploring with the meta-policy and exploiting
the ofÔ¨Çine dataset, in order to guarantee the performance improvement of new ofÔ¨Çine tasks."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.12903225806451613,"4.2.2
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1313364055299539,"To tackle the above challenge, we next devise a novel regularized policy improvement for within-
task policy optimization of task n, through a weighted interpolation of two different regularizers
based on the behavior policy œÄŒ≤,n and the meta-policy œÄc, given as follows:
œÄn ‚Üêarg max
œÄn Es‚àºœÅn,a‚àºœÄn(¬∑|s)[ bQœÄ
n(s, a)] ‚àíŒªŒ±D(œÄn, œÄŒ≤,n) ‚àíŒª(1 ‚àíŒ±)D(œÄn, œÄc),
(7)"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1336405529953917,"for some Œ± ‚àà[0, 1]. Here, Œ± controls the trade-off between staying close to the behavior policy and
following the meta-policy to ‚Äúexplore‚Äù out-of-distribution state-actions. Intuitively, as Œ± is closer
to 0, the policy improvement is less conservative and tends to improve the task-speciÔ¨Åc policy œÄn
towards the actions in œÄc that have highest estimated Q-values. Compared to Eq. (6), the exploration
penalty induced by D(œÄn, œÄŒ≤,n) serves as a safeguard and stops œÄn following œÄc over-optimistically."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1359447004608295,Algorithm 1 RAC
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1382488479262673,"1: Train dynamics model bTŒ∏n using Dn;
2: for k = 1, 2, ... do
3:
Perform model rollouts starting from
states in Dn and add into Dmodel,n;
4:
Policy evaluation by recursively solv-
ing Eq. (1) using Dn ‚à™Dmodel,n;
5:
Improve policy by solving Eq. (7);
6: end for"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.14055299539170507,"Safe Policy Improvement Guarantee. Based on con-
servative policy evaluation Eq. (1) and regularized pol-
icy improvement Eq. (7), we have the meta-regularized
model-based actor-critic method (RAC), as outlined in
Algorithm 1. Note that different distribution distance
measures can be used in Eq. (7). In this work, we theo-
retically show that the policy œÄn(a|s) learnt by RAC is
a safe improvement over both the behavior policy œÄŒ≤,n
and the meta-policy œÄc on the underlying MDP Mn,
when using the maximum total-variation distance for
D(œÄ1, œÄ2), i.e., D(œÄ1, œÄ2) := maxs DT V (œÄ1||œÄ2)."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.14285714285714285,"For convenience, deÔ¨Åne ŒΩn(œÅ, f) = EœÅ [(œÅ(s, a) ‚àídn(s, a))/df,n(s, a)], and let Œ¥ ‚àà(0, 1/2). We
have the following important result on the safe policy improvement achieved by œÄn(a|s)."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.14516129032258066,"Theorem 1. (a) Let œµ =
Œ≤[ŒΩn(œÅœÄn,f)‚àíŒΩn(œÅœÄŒ≤,n,f)]"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.14746543778801843,"2Œª(1‚àíŒ≥)D(œÄn,œÄŒ≤,n)
. If ŒΩn(œÅœÄn, f) ‚àíŒΩn(œÅœÄŒ≤,n, f) > 0 and Œ± ‚àà
 
max{ 1"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1497695852534562,"2 ‚àíœµ, 0}, 1"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.15207373271889402,"2

, then J(Mn, œÄn) ‚â•max{J(Mn, œÄc) + Œæ1, J(Mn, œÄŒ≤,n) + Œæ2} holds with
probability at least 1 ‚àí2Œ¥, where both Œæ1 and Œæ2 are positive for large enough Œ≤ and Œª;"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1543778801843318,Published as a conference paper at ICLR 2022
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.15668202764976957,"(b) More generally, we have that J(Mn, œÄn) ‚â•max{J(Mn, œÄc) + Œæ1, J(Mn, œÄŒ≤,n) + Œæ2} holds
with probability at least 1 ‚àí2Œ¥, when Œ± ‚àà(0, 1/2), where Œæ1 is positive for large enough Œª."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.15898617511520738,"Remark 1. The expressions of Œæ1 and Œæ2 are involved and can be found in Eq. (14) and Eq. (15) in
the appendix. In part (a) of Theorem 1, both Œæ1 and Œæ2 are positive for large enough Œ≤ and Œª, pointing
to guaranteed improvements over œÄc and œÄŒ≤,n. Due to the fact that the dynamics T d
Mn learnt via
supervised learning is close to the true dynamics TMn on the states visited by the behavior policy
œÄŒ≤,n, dœÄŒ≤,n
d
Mn (s, a) is close to dœÄŒ≤,n"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.16129032258064516,"Mn (s, a) and œÅœÄŒ≤,n is close to dn(s, a), indicating that the condition
ŒΩn(œÅœÄn, f)‚àíŒΩn(œÅœÄŒ≤,n, f) > 0 is expected to hold in practical scenarios (Yu et al., 2021b). For more
general cases, a slightly weaker result can be obtained in part (b) of Theorem 1, where Œæ1 is positive
for large enough Œª and Œæ2 can be negative."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.16359447004608296,"Remark 2. Intuitively, the selection of Œ± balances the impact of œÄŒ≤,n and œÄc, while delicately
leaning toward the meta-policy œÄc because œÄŒ≤,n has played an important role in policy evaluation
to Ô¨Ånd a lower bound of Q-value. As a result, Eq. (7) maximizes the true Q-value while implic-
itly regularized by a weighted combination, instead of Œ±-interpolation, between D(œÄn, œÄŒ≤,n) and
D(œÄn, œÄc), where the weights are carefully balanced through Œ±. In particular, in the tabular setting,
the conservative policy evaluation in Eq. (1) corresponds to penalizing the Q estimation (Yu et al.,
2021b):
bQk+1
n
(s, a) = bBœÄ bQk
n(s, a) ‚àíŒ≤[œÅ(s, a) ‚àídn(s, a)]"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.16589861751152074,"df,n(s, a)
.
(8)"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.16820276497695852,"Clearly, œµ increases with the value of the penalty term in Eq. (8). As a result, when the policy
evaluation Eq. (1) is overly conservative, the lower bound of Œ± will be close to 0, and hence the
regularizer based on the meta-policy œÄc can play a bigger role so as to encourage the ‚Äúexploration‚Äù
of out-of-distribution state-actions following the guidance of œÄc. On the other hand, when the policy
evaluation Eq. (1) is less conservative, the lower bound of Œ± will be close to 1"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.17050691244239632,"2, and the regularizer
based on œÄŒ≤,n will have more impact, leaning towards ‚Äúexploiting‚Äù the ofÔ¨Çine dataset. In fact,
the introduction of 1) behavior policy-based regularizer and 2) the interpolation for modeling the
interaction between the behavior policy and the meta-policy, is the key to prove Theorem 1."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1728110599078341,"Practical Implementation. In practice, we can use the KL divergence to replace the total variation
distance between policies, based on Pinsker‚Äôs Inequality: ‚à•œÄ1‚àíœÄ2‚à•‚â§
p"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.17511520737327188,"2DKL(œÄ1||œÄ2). Moreover,
since the behavior policy œÄŒ≤,n is typically unknown, we can use the reverse KL-divergence between
œÄn and œÄŒ≤,n to circumvent the estimation of œÄŒ≤,n, following the same line as in (Fakoor et al., 2021):
DKL(œÄŒ≤,n||œÄn) = Ea‚àºœÄŒ≤,n[log œÄŒ≤,n(a|s)] ‚àíEa‚àºœÄŒ≤,n[log œÄn(a|s)]"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1774193548387097,"‚àù‚àíEa‚àºœÄŒ≤,n[log œÄn(a|s)] ‚âà‚àíE(s,a)‚àºDn[log œÄn(a|s)]."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.17972350230414746,"Then, the task-speciÔ¨Åc policy can be learnt by solving the following problem:"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.18202764976958524,"max
œÄn
Es‚àºœÅn,a‚àºœÄn(¬∑|s)
h
bQœÄ
n(s, a)
i
+ ŒªŒ±E(s,a)‚àºDn[log œÄn(a|s)] ‚àíŒª(1 ‚àíŒ±)DKL(œÄn||œÄc).
(9)"
OFFLINE META-POLICY UPDATE,0.18433179723502305,"4.2.3
OFFLINE META-POLICY UPDATE"
OFFLINE META-POLICY UPDATE,0.18663594470046083,"Built on RAC, the ofÔ¨Çine meta-policy œÄc is updated by taking the following two steps, in an iterative
manner: 1) (inner loop) given the meta-policy œÄc, RAC is run for each training task to obtain the
task-speciÔ¨Åc policy œÄn; 2) (outer loop) based on {œÄn}n, œÄc is updated by solving:"
OFFLINE META-POLICY UPDATE,0.1889400921658986,"max
œÄc
EMn"
OFFLINE META-POLICY UPDATE,0.1912442396313364,"
E
s‚àºœÅn,
a‚àºœÄn(¬∑|s)"
OFFLINE META-POLICY UPDATE,0.1935483870967742,"h
ÀÜQœÄ
n(s, a)
i
+ ŒªŒ±E(s,a)‚àºDn[log œÄn(a|s)] ‚àíŒª(1 ‚àíŒ±)DKL(œÄn||œÄc)

(10)"
OFFLINE META-POLICY UPDATE,0.195852534562212,"where both œÅn and ÀÜQœÄ
n are from the last iteration of the inner loop for each training task. By
using RAC in the inner loop for within-task policy optimization, the learnt task-speciÔ¨Åc policy œÄn
and the meta-policy œÄc work in concert to regularize the policy search for each other, and improve
akin to ‚Äòpositive feedback‚Äô. Here the regularizer based on the behavior policy serves an important
initial force to boost the policy optimization against the ground: RAC in the inner loop aims to
improve the task-speciÔ¨Åc policy over the behavior policy at the outset and the improved task-speciÔ¨Åc
policy consequently regularizes the meta-policy search as in Eq. (10), leading to a better meta-policy
eventually. Noted that a meta-Q network is learnt using Ô¨Årst-order meta-learning to initialize task-
speciÔ¨Åc Q networks. It is worth noting that different tasks can have different values of Œ± to capture
the heterogeneity of dataset qualities across tasks."
OFFLINE META-POLICY UPDATE,0.19815668202764977,Published as a conference paper at ICLR 2022
OFFLINE META-POLICY UPDATE,0.20046082949308755,"Figure 4: Performance comparison among COMBO, COMBO-3 and RAC, with good-quality meta-
policy (two Ô¨Ågures on the left) and poor-quality meta-policy (two Ô¨Ågures on the right)."
OFFLINE META-POLICY UPDATE,0.20276497695852536,"In a nutshell, the proposed model-based ofÔ¨Çine Meta-RL with regularized Policy Optimization
(MerPO) is built on two key steps: 1) learning the ofÔ¨Çine meta-model via Eq. (3) and 2) learning the
ofÔ¨Çine meta-policy via Eq. (10). The details are presented in Algorithm 2 in the appendix."
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.20506912442396313,"4.3
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK"
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.2073732718894009,"Let TœÜ‚àóm and œÄ‚àó
c be the ofÔ¨Çine meta-model and the ofÔ¨Çine meta-policy learnt by MerPO. For a new
ofÔ¨Çine RL task, the task model can be quickly adapted based on Eq. (4), and the task-speciÔ¨Åc policy
can be obtained based on œÄ‚àó
c using the within-task policy optimization module RAC. Appealing to
Theorem 1, we have the following result on MerPO-based policy learning on a new task."
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.20967741935483872,"Proposition 1. Consider a new ofÔ¨Çine RL task with the true MDP M.
Suppose œÄo is the
MerPO-based task-speciÔ¨Åc policy, learnt by running RAC over the meta-policy œÄ‚àó
c.
If œµ =
Œ≤[ŒΩ(œÅœÄo,f)‚àíŒΩ(œÅœÄŒ≤ ,f)]"
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.2119815668202765,"2Œª(1‚àíŒ≥)D(œÄo,œÄŒ≤)
‚â•0 and Œ± ‚àà
 
max{ 1"
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.21428571428571427,"2 ‚àíœµ, 0}, 1"
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.21658986175115208,"2

, then œÄo achieves the safe performance
improvement over both œÄ‚àó
c and œÄŒ≤, i.e., J(M, œÄo) > max{J(M, œÄ‚àó
c), J(M, œÄŒ≤)} holds with prob-
ability at least 1 ‚àí2Œ¥, for large enough Œ≤ and Œª."
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.21889400921658986,"Proposition 1 indicates that MerPO-based policy optimization for learning task-speciÔ¨Åc policy guar-
antees a policy with higher rewards than both the behavior policy and the meta-policy. This is
particularly useful in the following two scenarios: 1) the ofÔ¨Çine dataset is collected by some poor
behavior policy, but the meta-policy is a good policy; and 2) the meta-policy is inferior to a good
behavior policy."
EXPERIMENTS,0.22119815668202766,"5
EXPERIMENTS"
EXPERIMENTS,0.22350230414746544,"In what follows, we Ô¨Årst evaluate the performance of RAC for within-task policy optimization on
an ofÔ¨Çine RL task to validate the safe policy improvement, and then examine how MerPO performs
when compared to state-of-art ofÔ¨Çine Meta-RL algorithms. Due to the space limit, we relegate
additional experiments to the appendix."
PERFORMANCE EVALUATION OF RAC,0.22580645161290322,"5.1
PERFORMANCE EVALUATION OF RAC"
PERFORMANCE EVALUATION OF RAC,0.22811059907834103,"Setup.
We evaluate RAC on several continuous control tasks in the D4RL benchmark (Fu et al.,
2020) from the Open AI Gym (Brockman et al., 2016), and compare its performance to 1) COMBO
(where no meta-policy is leveraged) and 2) COMBO with policy improvement Eq. (6) (namely,
COMBO-3), under different qualities of ofÔ¨Çine datasets and different qualities of meta-policy (good
and poor). For illustrative purpose, we use a random policy as a poor-quality meta-policy, and
choose the learnt policy after 200 episodes as a better-quality meta-policy. We evaluate the average
return over 4 random seeds after each episode with 1000 gradient steps."
PERFORMANCE EVALUATION OF RAC,0.2304147465437788,"Results. As shown in Figure 4, RAC can achieve comparable performance with COMBO-3 given
a good-quality meta-policy, and both clearly outperform COMBO. Besides, the training procedure
is also more stable and converges more quickly as expected when regularized with the meta-policy.
When regularized by a poor-quality meta-policy, that is signiÔ¨Åcantly worse than the behavior policy
in all environments, the performance of COMBO-3 degrades dramatically. However, RAC outper-
forms COMBO even when the meta-policy is a random policy. In a nutshell, RAC consistently"
PERFORMANCE EVALUATION OF RAC,0.23271889400921658,Published as a conference paper at ICLR 2022
PERFORMANCE EVALUATION OF RAC,0.2350230414746544,"achieves the best performance in various setups and demonstrates compelling robustness against the
quality of the meta-policy, for suitable parameter selections (Œ± = 0.4 in Figure 4)."
PERFORMANCE EVALUATION OF RAC,0.23732718894009217,"Figure 5: Impact of Œ± on the performance of RAC under
different qualities of ofÔ¨Çine datasets."
PERFORMANCE EVALUATION OF RAC,0.23963133640552994,"Impact of Œ±.
As shown in Theorem
1, the selection of Œ± is important to
guarantee the safe policy improvement
property of RAC. Therefore, we next
examine the impact of Œ± on the per-
formance of RAC under different qual-
ities of datasets and meta-policy. More
speciÔ¨Åcally, we consider four choices
of Œ±: Œ± = 0, 0.4, 0.7, 1. Here, Œ± =
0 corresponds to COMBO-3, i.e., reg-
ularized by the meta-policy only, and
the policy improvement step is regular-
ized by the behavior policy only when
Œ± = 1. Figure 5 shows the average return of RAC over different qualities of meta-policies under
different qualities of the ofÔ¨Çine datasets. It is clear that RAC achieves the best performance when
Œ± = 0.4 among the four selections of Œ±, corroborating the result in Theorem 1. In general, the
performance of RAC is stable for Œ± ‚àà[0.3, 0.5] in our experiments."
PERFORMANCE EVALUATION OF MERPO,0.24193548387096775,"5.2
PERFORMANCE EVALUATION OF MERPO"
PERFORMANCE EVALUATION OF MERPO,0.24423963133640553,"Setup. To evaluate the performance of MerPO, we follow the setups in the literature (Rakelly et al.,
2019; Li et al., 2020b) and consider continuous control meta-environments of robotic locomotion.
More speciÔ¨Åcally, tasks has different transition dynamics in Walker-2D-Params and Point-Robot-
Wind, and different reward functions in Half-Cheetah-Fwd-Back and Ant-Fwd-Back. We collect
the ofÔ¨Çine dataset for each task by following the same line as in (Li et al., 2020b). We consider the
following baselines: (1) FOCAL (Li et al., 2020b), a model-free ofÔ¨Çine Meta-RL approach based
on a deterministic context encoder that achieves the state-of-the-art performance; (2) MBML (Li
et al., 2020a), an ofÔ¨Çine multi-task RL approach with metric learning; (3) Batch PEARL, which
modiÔ¨Åes PEARL (Rakelly et al., 2019) to train and test from ofÔ¨Çine datasets without exploration;
(4) Contextual BCQ (CBCQ), which is a task-augmented variant of the ofÔ¨Çine RL algorithm BCQ
(Fujimoto et al., 2019) by integrating a task latent variable into the state information. We train on
a set of ofÔ¨Çine RL tasks, and evaluate the performance of the learnt meta-policy during the training
process on a set of unseen testing ofÔ¨Çine RL tasks."
PERFORMANCE EVALUATION OF MERPO,0.2465437788018433,"Fixed Œ± vs Adaptive Œ±. We consider two implementations of MerPO based on the selection of Œ±.
1) MerPO: Œ± is Ô¨Åxed as 0.4 for all tasks; 2) MerPO-Adp: at each iteration k, given the task-policy
œÄk
n for task n and the meta-policy œÄk
c at iteration k, we update Œ±k
n using one-step gradient descent to
minimize the following problem.
min
Œ±kn
(1 ‚àíŒ±k
n)(D(œÄk
n, œÄŒ≤,n) ‚àíD(œÄk
n, œÄk
c )), s.t. Œ±k
n ‚àà[0.1, 0.5].
(11)"
PERFORMANCE EVALUATION OF MERPO,0.2488479262672811,"The idea is to adapt Œ±k
n in order to balance between D(œÄk
n, œÄŒ≤,n) and D(œÄk
n, œÄk
c ), because Theorem
1 implies that the safe policy improvement can be achieved when the impacts of the meta-policy and
the behavior policy are well balanced. SpeciÔ¨Åcally, at iteration k for each task n, Œ±k
n is increased
when the task-policy œÄk
n is closer to the meta-policy œÄk
c , and is decreased when œÄk
n is closer to the
behavior policy. Note that Œ±k
n is constrained in the range [0.1, 0.5] as suggested by Theorem 1."
PERFORMANCE EVALUATION OF MERPO,0.2511520737327189,"Results.
As illustrated in Figure 6, MerPO-Adp yields the best performance, and both MerPO-
Adp and MerPO achieve better or comparable performance in contrast to existing ofÔ¨Çine Meta-RL
approaches. Since the meta-policy changes during the learning process and the qualities of the
behavior policies vary across different tasks, MerPO-Adp adapts Œ± across different iterations and
tasks so as to achieve a ‚Äòlocal‚Äô balance between the impacts of the meta-policy and the behavior
policy. As expected, MerPO-Adp can perform better than MerPO with a Ô¨Åxed Œ±. Here the best
testing performance for the baseline algorithms is selected over different qualities of ofÔ¨Çine datasets."
PERFORMANCE EVALUATION OF MERPO,0.2534562211981567,Ablation Study. We next provide ablation studies by answering the following questions.
PERFORMANCE EVALUATION OF MERPO,0.2557603686635945,"(1) Is RAC important for within-task policy optimization? To answer this question, we compare
MerPO with the approach Eq. (5) where the within-task policy optimization is only regularized by"
PERFORMANCE EVALUATION OF MERPO,0.25806451612903225,Published as a conference paper at ICLR 2022
PERFORMANCE EVALUATION OF MERPO,0.26036866359447003,"Figure 6: Performance comparison in terms of the average return in different environments. Clearly,
MerPO Adp and MerPO achieve better or comparable performance than the baselines."
PERFORMANCE EVALUATION OF MERPO,0.2626728110599078,"(a) Impact of RAC mod-
ule."
PERFORMANCE EVALUATION OF MERPO,0.26497695852534564,"(b) Impact of model uti-
lization."
PERFORMANCE EVALUATION OF MERPO,0.2672811059907834,"(c) Performance under dif-
ferent data qualities."
PERFORMANCE EVALUATION OF MERPO,0.2695852534562212,"(d) Testing
performance
for expert dataset."
PERFORMANCE EVALUATION OF MERPO,0.271889400921659,Figure 7: Ablation study of MerPO in Walker-2D-Params.
PERFORMANCE EVALUATION OF MERPO,0.27419354838709675,"the meta-policy. As shown in Figure 7(a), with the regularization based on the behavior policy in
RAC, MerPO performs signiÔ¨Åcantly better than Eq. (5), implying that the safe policy improvement
property of RAC enables MerPO to continuously improve the meta-policy."
PERFORMANCE EVALUATION OF MERPO,0.2764976958525346,"(2) Is learning the dynamics model important? Without the utilization of models, the within-task
policy optimization degenerates to CQL (Kumar et al., 2020) and the Meta-RL algorithm becomes a
model-free approach. Figure 7(b) shows the performance comparison between the cases whether the
dynamics model is utilized. It can be seen that the performance without model utilization is much
worse than that of MerPO. This indeed makes sense because the task identity inference (Dorfman
& Tamar, 2020; Li et al., 2020a;b) is a critical problem in Meta-RL. Such a result also aligns well
with a recently emerging trend in supervised meta-learning to improve the generalization ability by
augmenting the tasks with ‚Äúmore data‚Äù (Rajendran et al., 2020; Yao et al., 2021)."
PERFORMANCE EVALUATION OF MERPO,0.27880184331797236,"(3) How does MerPO perform in unseen ofÔ¨Çine tasks under different data qualities? We eval-
uate the average return in unseen ofÔ¨Çine tasks with different data qualities, and compare the per-
formance between (1) MerPO with Œ± = 0.4 (‚ÄúWith meta‚Äù) and (2) Run a variant of COMBO with
behavior-regularized policy improvement, i.e., Œ± = 1 (‚ÄúWith beha only‚Äù). For a fair comparison, we
initialize the policy network with the meta-policy in both cases. As shown in Figure 7(c), the aver-
age performance of ‚ÄúWith meta‚Äù over different data qualities is much better than that of ‚ÄúWith beha
only‚Äù. More importantly, for a new task with expert data, MerPO (‚ÄúWith meta‚Äù) clearly outperforms
COMBO as illustrated in Figure 7(d), whereas the performance of FOCAL is worse than COMBO."
CONCLUSION,0.28110599078341014,"6
CONCLUSION"
CONCLUSION,0.2834101382488479,"In this work, we study ofÔ¨Çine Meta-RL aiming to strike a right balance between ‚Äúexploring‚Äù the
out-of-distribution state-actions by following the meta-policy and ‚Äúexploiting‚Äù the ofÔ¨Çine dataset
by staying close to the behavior policy. To this end, we propose a model-based ofÔ¨Çine Meta-RL
approach, namely MerPO, which learns a meta-model to enable efÔ¨Åcient task model learning and a
meta-policy to facilitate safe exploration of out-of-distribution state-actions. Particularly, we devise
RAC, a meta-regularized model-based actor-critic method for within-task policy optimization, by
using a weighted interpolation between two regularizers, one based on the behavior policy and
the other on the meta-policy. We theoretically show that the learnt task-policy via MerPO offers
safe policy improvement over both the behavior policy and the meta-policy. Compared to existing
ofÔ¨Çine Meta-RL methods, MerPO demonstrates superior performance on several benchmarks, which
suggests a more prominent role of model-based approaches in ofÔ¨Çine Meta-RL."
CONCLUSION,0.2857142857142857,Published as a conference paper at ICLR 2022
CONCLUSION,0.2880184331797235,ACKNOWLEDGEMENT
CONCLUSION,0.2903225806451613,"This work is supported in part by NSF Grants CNS-2003081, CNS-2203239, CPS-1739344, and
CCSS-2121222."
REPRODUCIBILITY STATEMENT,0.2926267281105991,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.29493087557603687,"For the theoretical results presented in the main text, we state the full set of assumptions of all
theoretical results in Appendix B, and include the complete proofs of all theoretical results in Ap-
pendix C. For the experimental results presented in the main text, we include the code in the supple-
mental material, and specify all the training details in Appendix A. For the datasets used in the main
text, we also give a clear explanation in Appendix A."
REFERENCES,0.29723502304147464,REFERENCES
REFERENCES,0.2995391705069124,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.30184331797235026,"Ron Dorfman and Aviv Tamar.
OfÔ¨Çine meta reinforcement learning.
arXiv preprint
arXiv:2008.02598, 2020."
REFERENCES,0.30414746543778803,"Rasool Fakoor, Jonas Mueller, Pratik Chaudhari, and Alexander J Smola. Continuous doubly con-
strained batch reinforcement learning. arXiv preprint arXiv:2102.09225, 2021."
REFERENCES,0.3064516129032258,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1126‚Äì1135. JMLR. org, 2017."
REFERENCES,0.3087557603686636,"Justin Fu, Aviral Kumar, OÔ¨År Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.31105990783410137,"Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587‚Äì1596. PMLR, 2018."
REFERENCES,0.31336405529953915,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052‚Äì2062. PMLR, 2019."
REFERENCES,0.315668202764977,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861‚Äì1870. PMLR, 2018."
REFERENCES,0.31797235023041476,"Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. arXiv preprint arXiv:1906.08253, 2019."
REFERENCES,0.32027649769585254,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofÔ¨Çine reinforcement learning. arXiv preprint arXiv:2005.05951, 2020."
REFERENCES,0.3225806451612903,"Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019."
REFERENCES,0.3248847926267281,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofÔ¨Çine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.3271889400921659,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. OfÔ¨Çine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.3294930875576037,"Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Henrik Christensen, and Hao Su.
Multi-task batch reinforcement learning with metric learning. Advances in Neural Information
Processing Systems, 33, 2020a."
REFERENCES,0.3317972350230415,"Lanqing Li, Rui Yang, and Dijun Luo. EfÔ¨Åcient fully-ofÔ¨Çine meta-reinforcement learning via dis-
tance metric learning and behavior regularization. arXiv preprint arXiv:2010.01112, 2020b."
REFERENCES,0.33410138248847926,Published as a conference paper at ICLR 2022
REFERENCES,0.33640552995391704,"Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient
with stationary distribution correction. In Uncertainty in ArtiÔ¨Åcial Intelligence, pp. 1180‚Äì1190.
PMLR, 2020."
REFERENCES,0.3387096774193548,"Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, OÔ¨År Nachum, and Shixiang Gu. Deployment-
efÔ¨Åcient reinforcement learning via model-based ofÔ¨Çine optimization.
arXiv preprint
arXiv:2006.03647, 2020."
REFERENCES,0.34101382488479265,"Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. OfÔ¨Çine meta-
reinforcement learning with advantage weighting. arXiv preprint arXiv:2008.06043, 2020."
REFERENCES,0.3433179723502304,"OÔ¨År Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019."
REFERENCES,0.3456221198156682,"Alex Nichol, Joshua Achiam, and John Schulman. On Ô¨Årst-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018."
REFERENCES,0.347926267281106,"Janarthanan Rajendran, Alex Irpan, and Eric Jang. Meta-learning requires meta-augmentation. arXiv
preprint arXiv:2007.05549, 2020."
REFERENCES,0.35023041474654376,"Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. EfÔ¨Åcient off-policy
meta-reinforcement learning via probabilistic context variables. arXiv preprint arXiv:1903.08254,
2019."
REFERENCES,0.35253456221198154,"Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. On the global optimality of model-
agnostic meta-learning.
In International Conference on Machine Learning, pp. 9837‚Äì9846.
PMLR, 2020."
REFERENCES,0.3548387096774194,"Huaxiu Yao, Long-Kai Huang, Linjun Zhang, Ying Wei, Li Tian, James Zou, Junzhou Huang, et al.
Improving generalization in meta-learning via task augmentation. In International Conference on
Machine Learning, pp. 11887‚Äì11897. PMLR, 2021."
REFERENCES,0.35714285714285715,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma.
Mopo:
Model-based ofÔ¨Çine policy optimization.
arXiv preprint
arXiv:2005.13239, 2020."
REFERENCES,0.35944700460829493,"Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey Levine, and Chelsea
Finn. Conservative data sharing for multi-task ofÔ¨Çine reinforcement learning. arXiv preprint
arXiv:2109.08128, 2021a."
REFERENCES,0.3617511520737327,"Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
Combo: Conservative ofÔ¨Çine model-based policy optimization. arXiv preprint arXiv:2102.08363,
2021b."
REFERENCES,0.3640552995391705,"Pan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng. EfÔ¨Åcient meta learning via
minibatch proximal update. Advances in Neural Information Processing Systems, 32:1534‚Äì1544,
2019."
REFERENCES,0.3663594470046083,"Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,
and Shimon Whiteson.
Varibad: A very good method for bayes-adaptive deep rl via meta-
learning. arXiv preprint arXiv:1910.08348, 2019."
REFERENCES,0.3686635944700461,Published as a conference paper at ICLR 2022
REFERENCES,0.3709677419354839,Table 1: Hyperparameters for RAC.
REFERENCES,0.37327188940092165,"Hyperparameters
Halfcheetah
Hopper
Walker2d
Discount factor
0.99
0.99
0.99
Sample batch size
256
256
256
Real data ratio
0.5
0.5
0.5
Model rollout length
5
5
1
Critic lr
3e-4
3e-4
1e-4
Actor lr
1e-4
1e-4
1e-5
Model lr
1e-3
1e-3
1e-3
Optimizer
Adam
Adam
Adam
Œ≤
1
1
10
Max entropy
True
True
True
Œª
1
1
1"
REFERENCES,0.37557603686635943,"A
EXPERIMENTAL DETAILS"
REFERENCES,0.3778801843317972,"A.1
META ENVIRONMENT DESCRIPTION"
REFERENCES,0.38018433179723504,"‚Ä¢ Walker-2D-Params: Train an agent to move forward. Different tasks correspond to different
randomized dynamcis parameters."
REFERENCES,0.3824884792626728,"‚Ä¢ Half-Cheeta-Fwd-Back: Train a Cheetah robot to move forward or backward, and the re-
ward function depends on the moving direction. All tasks have the same dynamics model
but different reward functions."
REFERENCES,0.3847926267281106,"‚Ä¢ Ant-Fwd-Back: Train an Ant robot to move forward or backward, and the reward function
depends on the moving direction. All tasks have the same dynamics model but different
reward functions."
REFERENCES,0.3870967741935484,"‚Ä¢ Poing-Robot-Wind: Point-Robot-Wind is a variant of Sparse-Point-Robot (Li et al., 2020b),
a 2D navigation problem introduced in (Rakelly et al., 2019), where each task is to guide
a point robot to navigate to a speciÔ¨Åc goal location on the edge of a semi-circle from the
origin. In Point-Robot-Wind, each task is affected by a distinct ‚Äúwind‚Äù uniformly sampled
from [‚àí0.05, 0.05]2, and hence differs in the transition dynamics."
REFERENCES,0.38940092165898615,"A.2
IMPLEMENTATION DETAILS AND MORE EXPERIMENTS"
REFERENCES,0.391705069124424,"A.2.1
EVALUATION OF RAC"
REFERENCES,0.39400921658986177,"Model learning. Following the same line as in (Janner et al., 2019; Yu et al., 2020; 2021b), the
dynamics model for each task is represented as a probabilistic neural network that takes the current
state-action as input and outputs a Gaussian distribution over the next state and reward:
bTŒ∏(st+1, r|s, a) = N(¬µŒ∏(st, at), Œ£Œ∏(st, at)).
An ensemble of 7 models is trained independently using maximum likelihood estimation, and the
best 5 models are picked based on the validation prediction error using a held-our set of the ofÔ¨Çine
dataset. Each model is represented by a 4-layer feedforward neural network with 256 hidden units.
And one model will be randomly selected from the best 5 models for model rollout."
REFERENCES,0.39631336405529954,"Policy optimization. We represent both Q-network and policy network as a 4-layer feedforward
neural network with 256 hidden units, and use clipped double Q-learning (Fujimoto et al., 2018) for
Q backup update. A max entropy term is also included to the value function for computing the target
Q value as in SAC (Haarnoja et al., 2018). The hyperparameters used for evaluating the performance
of RAC are described in Table 1."
REFERENCES,0.3986175115207373,"Additional experiments. We also evaluate the performance of RAC in Walker2d under different
qualities of the meta-policy. As shown in Figure 8(a) and 8(b), RAC achieves the best performance
in both scenarios, compared to COMBO and COMBO-3. Particularly, the performance of COMBO-
3 in Figure 8(a) degrades in the later stage of training because the meta-policy is not superior over
the behavior policy in this case. In stark contrast, the performance of RAC is consistently better, as
it provides a safe policy improvement guarantee over both the behavior policy and the meta-policy."
REFERENCES,0.4009216589861751,Published as a conference paper at ICLR 2022
REFERENCES,0.4032258064516129,"(a)
Performance
in
Walker2d
with
a
good
meta-policy."
REFERENCES,0.4055299539170507,"(b)
Performance
in
Walker2d with a random
meta-policy."
REFERENCES,0.4078341013824885,"(c)
Performance
in
HalfCheetah with expert
ofÔ¨Çine dataset."
REFERENCES,0.41013824884792627,"(d) Average return over
different
qualities
of
meta-policies
under
ex-
pert dataset for different
choices of Œ±."
REFERENCES,0.41244239631336405,Figure 8: Performance evaluation of RAC.
REFERENCES,0.4147465437788018,"Beside, we also compare the performance of these three algorithms under an expert behavior policy
in Figure 8(c), where a meta-policy usually interferes the policy optimization and drags the learnt
policy away from the expert policy. As expected, RAC can still achieve comparable performance
with COMBO, as a result of safe policy improvement over the behavior policy for suitable parameter
selections."
REFERENCES,0.41705069124423966,"We examine the impact of Œ± on the performance of RAC under different qualities of the meta-policy
for HalfCheetah with expert data. In this case, the meta-policy is a worse policy compared to the
behavior policy. As shown in Figure 8(d), the performance Œ± = 0.4 is comparable to the case of
Œ± = 1 where the policy improvement step is only regularized based on the behavior policy, and
clearly better than the other two cases."
REFERENCES,0.41935483870967744,"A.2.2
EVALUATION OF MERPO"
REFERENCES,0.4216589861751152,"Data collection. We collect the ofÔ¨Çine dataset for each task by training a stochastic policy network
using SAC (Haarnoja et al., 2018) for that task and rolling out the policies saved at each checkpoint
to collect trajectories. Different checkpoints correspond to different qualities of the ofÔ¨Çine datasets.
When training with MerPO, we break the trajectories into independent tuples {si, ai, ri, s‚Ä≤
i} and
store in a replay buffer. Therefore, the ofÔ¨Çine dataset for each task does not contain full trajectories
over entire episodes, but merely individual transitions collected by some behavior policy."
REFERENCES,0.423963133640553,"Setup. For each testing task, we obtain the task-speciÔ¨Åc policy through quick adaptation using the
within-task policy optimization method RAC, based on its own ofÔ¨Çine dataset and the learnt meta-
policy, and evaluate the average return of the adapted policy over 4 random seeds. As shown earlier,
we take Œ± = 0.4 for all experiments about MerPO. In MerPO-Adp, we initialize Œ± with 0.4 and
update with a learning rate of 1e ‚àí4."
REFERENCES,0.42626728110599077,"Meta-model learning. Similar as in section A.2.1, for each task we quickly adapt from the meta-
model to obtain an ensemble of 7 models and pick the best 5 models based on the validation error.
The neural network used for representing the dynamics model is same with that in section A.2.1."
REFERENCES,0.42857142857142855,"Meta-policy learning. As in RAC, we represent the task Q-network, the task policy network and
the meta-policy network as a 5-layer feedforward neural network with 300 hidden units, and use
clipped double Q-learning (Fujimoto et al., 2018) for within task Q backup update. For each task,
we also use dual gradient descent to automatically tune both the parameter Œ≤ for conservative policy
evaluation and the parameter Œª for regularized policy improvement:"
REFERENCES,0.4308755760368664,"‚Ä¢ Tune Œ≤. Before optimizing the Q-network in policy evaluation, we Ô¨Årst optimize Œ≤ by
solving the following problem:"
REFERENCES,0.43317972350230416,"min
Q max
Œ≤‚â•0 Œ≤(Es,a‚àºœÅ[Q(s, a)] ‚àíEs,a‚àºD[Q(s, a)] ‚àíœÑ) + 1"
REFERENCES,0.43548387096774194,"2Es,a,s‚Ä≤‚àºdf [(Q(s, a) ‚àíbBœÄ bQk(s, a))2]."
REFERENCES,0.4377880184331797,"Intuitively, the value of Œ≤ will be increased to penalty the Q-values for out-of-distribution
state-actions if the difference Es,a‚àºœÅ[Q(s, a)]‚àíEs,a‚àºD[Q(s, a)] is larger than some thresh-
old value œÑ."
REFERENCES,0.4400921658986175,Published as a conference paper at ICLR 2022
REFERENCES,0.4423963133640553,Table 2: Hyperparameters for MerPO.
REFERENCES,0.4447004608294931,"Hyperparameters
Walker-2D-Params
Half-Cheetah-Fwd-Back
Ant-Fwd-Back
Point-Robot-Wind
Discount factor
0.99
0.99
0.99
0.9
Sample batch size
256
256
256
256
Task batch size
8
2
2
8
Real data ratio
0.5
0.5
0.5
0.5
Model rollout length
1
1
1
1
Inner critic lr
1e-3
1e-3
8e-4
1e-3
Inner actor lr
1e-3
5e-4
5e-4
1e-3
Inner steps
10
10
10
10
Outer critic lr
1e-3
1e-3
1e-3
1e-3
Outer actor lr
1e-3
1e-3
1e-3
1e-3
Meta-q lr
1e-3
1e-3
1e-3
1e-3
Task model lr
1e-4
1e-4
1e-4
1e-4
Meta-model lr
5e-2
5e-2
5e-2
5e-2
Model adaptation steps
25
25
25
25
Optimizer
Adam
Adam
Adam
Adam
Auto-tune Œª
True
True
True
True
Œª lr
1
1
1
1
Œª initial
5
100
100
5
Target divergence
0.05
0.05
0.05
0.05
Auto-tune Œ≤
True
True
True
True
log Œ≤ lr
1e-3
1e-3
1e-3
1e-3
log Œ≤ initial
0
0
0
0
Q difference threshold
5
10
10
10
Max entropy
True
True
True
True
Œ±
0.4
0.4
0.4
0.4
Testing adaptation steps
100
100
100
100
# training tasks
20
2
2
40
# testing tasks
5
2
2
10"
REFERENCES,0.4470046082949309,"‚Ä¢ Tune Œª. Similarly, we can optimize Œª in policy improvement by solving the following
problem:
max
œÄ
min
Œª‚â•0 Es‚àºœÅ(s),a‚àºœÄ(¬∑|s)[ bQœÄ(s, a)] ‚àíŒªŒ±[D(œÄ, œÄŒ≤) ‚àíDtarget] ‚àíŒª(1 ‚àíŒ±)[D(œÄ, œÄc) ‚àíDtarget]."
REFERENCES,0.44930875576036866,"Intuitively, the value of Œª will be increased so as to have stronger regularizations if the
divergence is larger than some threshold Dtarget, and decreased if the divergence is smaller
than Dtarget."
REFERENCES,0.45161290322580644,"Besides, we also build a meta-Q network Qmeta over the training process as an initialization of
the task Q networks to facilitate the within task policy optimization. At the k-th meta-iteration for
meta-policy update, the meta-Q network is also updated using the average Q-values of current batch
B of training tasks with meta-q learning rate Œæq, i.e.,"
REFERENCES,0.4539170506912442,"Qk+1
meta = Qk
meta ‚àíŒæq[Qk
meta ‚àí1 |B| X"
REFERENCES,0.45622119815668205,"n‚ààB
Qn]."
REFERENCES,0.45852534562211983,"Therefore, we initialize the task Q networks and the task policy with the meta-Q network and the
meta-policy, respectively, for within task policy optimization during both meta-training and meta-
testing. The hyperparameters used in evaluation of MerPO are listed in Table 2."
REFERENCES,0.4608294930875576,"For evaluating the performance improvement in a single new ofÔ¨Çine task, we use a smaller learning
rate of 8e ‚àí5 for the Q network and the policy network update."
REFERENCES,0.4631336405529954,"A.2.3
MORE EXPERIMENTS."
REFERENCES,0.46543778801843316,"We also evaluate the impact of the utilization extent of the learnt model, by comparing the perfor-
mance of MerPO under different cases of real data ratio, i.e., the ratio of the data from the ofÔ¨Çine
dataset in the data batch for training. As shown in Figure 9(a), the performance of MerPO can be
further boosted with a more conservative utilization of the model."
REFERENCES,0.46774193548387094,"To understand how much beneÔ¨Åt MerPO can bring for policy learning in unseen ofÔ¨Çine RL tasks,
we compare the performance of the following cases with respect to the gradient steps taken for
learning in unseen ofÔ¨Çine RL tasks: (1) Initialize the task policy network with the meta-policy and
run RAC (‚ÄúWith meta‚Äù); (2) Run RAC using the meta-policy without network initialization (‚ÄúWith
meta (no init)‚Äù); (3) Run RAC with a single regularization based on behavior policy without network
initialization, i.e., Œ± = 1 (‚ÄúWith beha only‚Äù); (4) Run COMBO (‚ÄúNo regul.‚Äù). As shown in Figure
9(b), ‚ÄúWith meta‚Äù achieves the best performance and improves signiÔ¨Åcantly over ‚ÄúNo regul.‚Äù and
‚ÄúWith beha only‚Äù, i.e., learning alone without any guidance of meta-policy, which implies that the"
REFERENCES,0.4700460829493088,Published as a conference paper at ICLR 2022
REFERENCES,0.47235023041474655,"(a) Impact of real data ratio.
(b) Performance comparison for
unseen tasks."
REFERENCES,0.47465437788018433,"(c) Training sample efÔ¨Åciency in
Point-Robot-Wind."
REFERENCES,0.4769585253456221,Figure 9: Ablation study of MerPO.
REFERENCES,0.4792626728110599,"(a) Training efÔ¨Åciency.
(b) Testing efÔ¨Åciency."
REFERENCES,0.4815668202764977,Figure 10: Sample efÔ¨Åciency.
REFERENCES,0.4838709677419355,"learnt meta-policy can efÔ¨Åciently guide the exploration of out-of-distribution state-actions. Without
network initialization, ‚ÄúWith meta (no init)‚Äù and ‚ÄúWith beha only‚Äù achieve similar performance
because good ofÔ¨Çine dataset is considered here. Such a result is also consistent with Figure 8(d)."
REFERENCES,0.4861751152073733,"We evaluate the testing performance of MerPO, by changing sample size of all tasks. Figure 10(a)
shows that the performance of MerPO is stable even if we decrease the number of trajectories for
each task to be around 200. In contrast, the number of trajectories collected in other baselines is
of the order 103. Figure 10(b) illustrates the testing sample efÔ¨Åciency of MerPO, by evaluating the
performance at new ofÔ¨Çine tasks under different sample sizes. Clearly, a good task-speciÔ¨Åc policy
can be quickly adapted at a new task even with 5 trajectories (1000 samples) of ofÔ¨Çine data. We also
evaluate the training sample efÔ¨Åciency of MerPO in Point-Robot-Wind. As shown in Figure 9(c) the
performance of MerPO is stable even if we decrease the number of trajectories for each task to be
around 200."
REFERENCES,0.48847926267281105,"A.2.4
MORE COMPARISON BETWEEN FOCAL AND COMBO"
REFERENCES,0.49078341013824883,"Following the setup as in Figure 1, we compare the performance between FOCAL and COMBO
in two more environments: Half-Cheetah-Fwd-Back and Ant-Fwd-Back. As shown in Figure 11,
although FOCAL performs better than COMBO on the task with a bad-quality dataset, it is outper-
formed by COMBO on the task with a good-quality dataset. This further conÔ¨Årms the observation
made in Figure 1."
REFERENCES,0.4930875576036866,"A.3
ALGORITHMS"
REFERENCES,0.49539170506912444,We include the details of MerPO in Algorithm 2.
REFERENCES,0.4976958525345622,Published as a conference paper at ICLR 2022
REFERENCES,0.5,"(a) Performance comparison in Half-Cheetah-Fwd-
Back."
REFERENCES,0.5023041474654378,(b) Performance comparison in Ant-Fwd-Back.
REFERENCES,0.5046082949308756,Figure 11: FOCAL vs. COMBO.
REFERENCES,0.5069124423963134,Algorithm 2 Regularized policy optimization for model-based ofÔ¨Çine Meta-RL (MerPO)
REFERENCES,0.5092165898617511,"1: Initialize the dynamics, actor and critic for each task, and initialize the meta-model and the
meta-policy;
2: for k = 1, 2, ... do
3:
for each training task Mn do
4:
Solve the following problem with gradient descent for h steps to compute the dynamics
model bTŒ∏k
n based on the ofÔ¨Çine dataset Di:
min
Œ∏n
E(s,a,s‚Ä≤)‚àºDn[log bTŒ∏n(s‚Ä≤|s, a)] + Œ∑‚à•Œ∏n ‚àíœÜm(k)‚à•2
2;"
REFERENCES,0.511520737327189,"5:
end for
6:
Update œÜm(k + 1) = œÜm(k) ‚àíŒæ1[œÜm(k) ‚àí1"
REFERENCES,0.5138248847926268,"N
PN
n=1 Œ∏k
n];
7: end for
8: Quickly obtain the estimated dynamics model bTn for each training task by solving Eq. (4) with
t steps gradient descent;
9: for k = 1, 2, ... do
10:
for each training task Mn do
11:
for j = 1, ..., J do
12:
Perform model rollouts with bTn starting from states in Dn and add model rollouts to
Dn
model;
13:
Policy evaluation by recursively solving Eq. (1) using data from Dn ‚à™Dn
model;
14:
Given the meta-policy œÄk
c , improve policy œÄk
n by solving Eq. (7);
15:
end for
16:
end for
17:
Given the learnt policy œÄk
n for each task, update the meta-policy œÄk+1
c
by solving Eq. (10)
with one step gradient descent;
18: end for"
REFERENCES,0.5161290322580645,"B
PRELIMINARIES"
REFERENCES,0.5184331797235023,"For ease of exposition, let TM and rM denote the dynamics and reward function of the underlying
MDP M, TM and rM denote the dynamics and reward function of the empirical MDP M induced
by the dataset D, and T c
M and r c
M denote the dynamics and reward function of the learnt MDP c
M.
To prevent any trivial bound with ‚àûvalues, we assume that the cardinality of a state-action pair in
the dataset D, i.e., |D(s, a)|, in the denominator, is non-zero, by setting |D(s, a)| to be a small value
less than 1 when (s, a) /‚ààD."
REFERENCES,0.5207373271889401,"Following the same line as in (Kumar et al., 2020; Yu et al., 2021b), we make the following standard
assumption on the concentration properties of the reward and dynamics for the empirical MDP M
to characterize the sampling error."
REFERENCES,0.5230414746543779,"Assumption 1. For any (s, a) ‚ààM, the following inequalities hold with probability 1 ‚àíŒ¥:"
REFERENCES,0.5253456221198156,"‚à•TM(s‚Ä≤|s, a) ‚àíTM(s‚Ä≤|s, a)‚à•1 ‚â§
CT,Œ¥
p"
REFERENCES,0.5276497695852534,"|D(s, a)|
;
|rM(s, a) ‚àírM| ‚â§
Cr,Œ¥
p"
REFERENCES,0.5299539170506913,"|D(s, a)|
where CT,Œ¥ and Cr,Œ¥ are some constants depending on Œ¥ via a
p"
REFERENCES,0.532258064516129,log(1/Œ¥) dependency.
REFERENCES,0.5345622119815668,Published as a conference paper at ICLR 2022
REFERENCES,0.5368663594470046,"Based on Assumption 1, we can bound the estimation error induced by the empirical Bellman backup
operator for any (s, a) ‚ààM:
BœÄ"
REFERENCES,0.5391705069124424,"M bQk(s, a) ‚àíBœÄ
M bQk(s, a) ="
REFERENCES,0.5414746543778802,"rM(s, a) ‚àírM(s, a) + Œ≥
X"
REFERENCES,0.543778801843318,"s‚Ä≤
(TM(s‚Ä≤|s, a) ‚àíTM(s‚Ä≤|s, a))EœÄ(a‚Ä≤|s‚Ä≤)[ bQk(s‚Ä≤, a‚Ä≤)] "
REFERENCES,0.5460829493087558,"‚â§|rM(s, a) ‚àírM(s, a)| + Œ≥  X"
REFERENCES,0.5483870967741935,"s‚Ä≤
(TM(s‚Ä≤|s, a) ‚àíTM(s‚Ä≤|s, a))EœÄ(a‚Ä≤|s‚Ä≤)[ bQk(s‚Ä≤, a‚Ä≤)] "
REFERENCES,0.5506912442396313,"‚â§
Cr,Œ¥
p"
REFERENCES,0.5529953917050692,"|D(s, a)|
+ Œ≥‚à•TM(s‚Ä≤|s, a) ‚àíTM(s‚Ä≤|s, a)‚à•1‚à•EœÄ(a‚Ä≤|s‚Ä≤)[ bQk(s‚Ä≤, a‚Ä≤)]‚à•‚àû"
REFERENCES,0.5552995391705069,"‚â§Cr,Œ¥ + Œ≥CT,Œ¥Rmax/(1 ‚àíŒ≥)
p"
REFERENCES,0.5576036866359447,"|D(s, a)|"
REFERENCES,0.5599078341013825,"=((1 ‚àíŒ≥)Cr,Œ¥/Rmax + Œ≥CT,Œ¥)Rmax"
REFERENCES,0.5622119815668203,"(1 ‚àíŒ≥)
p"
REFERENCES,0.5645161290322581,"|D(s, a)|"
REFERENCES,0.5668202764976958,"‚â§(Cr,Œ¥/Rmax + CT,Œ¥)Rmax"
REFERENCES,0.5691244239631337,"(1 ‚àíŒ≥)
p"
REFERENCES,0.5714285714285714,"|D(s, a)|
‚âú
Cr,T,Œ¥Rmax
(1 ‚àíŒ≥)
p"
REFERENCES,0.5737327188940092,"|D(s, a)|
."
REFERENCES,0.576036866359447,"Similarly, we can bound the difference between the Bellman backup induced by the learnt MDP c
M
and the underlying Bellman backup:
BœÄ
c
M bQk(s, a) ‚àíBœÄ
M bQk(s, a)"
REFERENCES,0.5783410138248848,"‚â§|r c
M(s, a) ‚àírM(s, a)| + Œ≥Rmax"
REFERENCES,0.5806451612903226,"1 ‚àíŒ≥ Dtv(T c
M, TM)"
REFERENCES,0.5829493087557603,"where Dtv(T c
M, TM) is the total-variation distance between T c
M and TM."
REFERENCES,0.5852534562211982,"For any two MDPs, M1 and M2, with the same state space, action space and discount factor Œ≥,
and a given fraction f ‚àà(0, 1), deÔ¨Åne the f-interpolant MDP Mf as the MDP with dynamics:
TMf = fTM1 + (1 ‚àíf)TM2 and reward function: rMf = frM1 + (1 ‚àíf)rM2, which has the
same state space, action space and discount factor with M1 and M2. Let T œÄ be the transition matrix
on state-action pairs induced by a stationary policy œÄ, i.e.,
T œÄ = T(s‚Ä≤|s, a)œÄ(a‚Ä≤|s‚Ä≤)."
REFERENCES,0.5875576036866359,"To prove the main result, we Ô¨Årst restate the following lemma from (Yu et al., 2021b) to be used
later."
REFERENCES,0.5898617511520737,"Lemma 1. For any policy œÄ, its returns in any MDP M, denoted by J(M, œÄ), and in Mf, denoted
by J(M1, M2, f, œÄ), satisfy the following:
J(M, œÄ) ‚àíŒ∑ ‚â§J(M1, M2, f, œÄ) ‚â§J(M, œÄ) + Œ∑
where"
REFERENCES,0.5921658986175116,Œ∑ =2Œ≥(1 ‚àíf)
REFERENCES,0.5944700460829493,"(1 ‚àíŒ≥)2 RmaxDtv(TM2, TM) +
Œ≥f
1 ‚àíŒ≥ |EdœÄ
M[(T œÄ
M ‚àíT œÄ
M1)QœÄ
M]|"
REFERENCES,0.5967741935483871,"+
f
1 ‚àíŒ≥ Es,a‚àºdœÄ
M[|rM1(s, a) ‚àírM(s, a)|] + 1 ‚àíf"
REFERENCES,0.5990783410138248,"1 ‚àíŒ≥ Es,a‚àºdœÄ
M [|rM2(s, a) ‚àírM(s, a)|]."
REFERENCES,0.6013824884792627,"Lemma 1 characterizes the relationship between policy returns in different MDPs in terms of the
corresponding reward difference and dynamics difference."
REFERENCES,0.6036866359447005,"C
PROOF OF THEOREM 1"
REFERENCES,0.6059907834101382,"Let d(s, a) := dœÄŒ≤
M(s, a). In the setting without function approximation, by setting the derivation of
Equation Eq. (1) to 0, we have that
bQk+1(s, a) = bBœÄ bQk(s, a) ‚àíŒ≤ œÅ(s, a) ‚àíd(s, a)"
REFERENCES,0.6082949308755761,"df(s, a)
."
REFERENCES,0.6105990783410138,Published as a conference paper at ICLR 2022
REFERENCES,0.6129032258064516,"Denote ŒΩ(œÅ, f) = EœÅ
h
œÅ(s,a)‚àíd(s,a)"
REFERENCES,0.6152073732718893,"df (s,a)
i
as the expected penalty on the Q-value. It can be shown (Yu"
REFERENCES,0.6175115207373272,"et al., 2021b) that ŒΩ(œÅ, f) ‚â•0 and increases with f, for any œÅ and f ‚àà(0, 1). Then, RAC optimizes
the return of a policy in a f-interpolant MDP induced by the empirical MDP M and the learnt MDP
c
M, which is regularized by both the behavior policy œÄŒ≤ and the meta-policy œÄc:"
REFERENCES,0.619815668202765,"max
œÄ
J(M, c
M, f, œÄ) ‚àíŒ≤ ŒΩ(œÅœÄ, f)"
REFERENCES,0.6221198156682027,"1 ‚àíŒ≥
‚àíŒªŒ±D(œÄ, œÄŒ≤) ‚àíŒª(1 ‚àíŒ±)D(œÄ, œÄc).
(12)"
REFERENCES,0.6244239631336406,"Denote œÄo as the solution to the above optimization problem. Based on Lemma 1, we can Ô¨Årst
characterize the return of the learnt policy œÄo in the underlying MDP M in terms of its return in the
f-interpolant MDP:
J(M, œÄo) + Œ∑1 ‚â•J(M, c
M, f, œÄo)
(13)
where"
REFERENCES,0.6267281105990783,Œ∑1 =2Œ≥(1 ‚àíf)
REFERENCES,0.6290322580645161,"(1 ‚àíŒ≥)2 RmaxDtv(T c
M, TM) +
Œ≥f
1 ‚àíŒ≥ |EdœÄo
M[(T œÄo
M ‚àíT œÄo"
REFERENCES,0.631336405529954,"M)QœÄo
M]|"
REFERENCES,0.6336405529953917,"+
f
1 ‚àíŒ≥ Es,a‚àºdœÄo
M[|rM(s, a) ‚àírM(s, a)|] + 1 ‚àíf"
REFERENCES,0.6359447004608295,"1 ‚àíŒ≥ Es,a‚àºdœÄo
M[|r c
M(s, a) ‚àírM(s, a)|]"
REFERENCES,0.6382488479262672,‚â§2Œ≥(1 ‚àíf)
REFERENCES,0.6405529953917051,"(1 ‚àíŒ≥)2 RmaxDtv(T c
M, TM) + Œ≥2fCT,Œ¥Rmax"
REFERENCES,0.6428571428571429,"(1 ‚àíŒ≥)2
Es‚àºdœÄo
M(s) ""s"
REFERENCES,0.6451612903225806,"|A|
|D(s)| q"
REFERENCES,0.6474654377880185,"DCQL(œÄo, œÄŒ≤)(s) + 1 #"
REFERENCES,0.6497695852534562,"+ Cr,Œ¥"
REFERENCES,0.652073732718894,"1 ‚àíŒ≥ Es,a‚àºdœÄo
M ""
1
p"
REFERENCES,0.6543778801843319,"|D(s, a)| #"
REFERENCES,0.6566820276497696,"+
1
1 ‚àíŒ≥ Es,a‚àºdœÄo
M[|r c
M(s, a) ‚àírM(s, a)|]"
REFERENCES,0.6589861751152074,"‚âúŒ∑c
1.
Note that the inequality above holds because the following is true for the empirical MDP M (Kumar
et al., 2020):"
REFERENCES,0.6612903225806451,"|EdœÄ
M[(T œÄ
M ‚àíT œÄ"
REFERENCES,0.663594470046083,"M)QœÄ
M]| ‚â§Œ≥CT,Œ¥Rmax"
REFERENCES,0.6658986175115207,"1 ‚àíŒ≥
Es‚àºdœÄ
M(s) ""s"
REFERENCES,0.6682027649769585,"|A|
|D(s)| q"
REFERENCES,0.6705069124423964,"DCQL(œÄ, œÄŒ≤)(s) + 1 #"
REFERENCES,0.6728110599078341,"for DCQL(œÄ1, œÄ2)(s) := P"
REFERENCES,0.6751152073732719,"a œÄ1(a|s)

œÄ1(a|s)
œÄ2(a|s) ‚àí1

."
REFERENCES,0.6774193548387096,"C.1
SAFE IMPROVEMENT OVER œÄc"
REFERENCES,0.6797235023041475,"We Ô¨Årst show that the learnt policy offers safe improvement over the meta-policy œÄc. Following the
same line as in Eq. (13), we next bound the return of the meta-policy œÄc in the underlying MDP M
from above, in terms of its return in the f-interpolant MDP:
J(M, c
M, f, œÄc) ‚â•J(M, œÄc) ‚àíŒ∑2
where"
REFERENCES,0.6820276497695853,Œ∑2 ‚â§2Œ≥(1 ‚àíf)
REFERENCES,0.684331797235023,"(1 ‚àíŒ≥)2 RmaxDtv(T c
M, TM) + Œ≥2fCT,Œ¥Rmax"
REFERENCES,0.6866359447004609,"(1 ‚àíŒ≥)2
Es‚àºdœÄc
M(s) ""s"
REFERENCES,0.6889400921658986,"|A|
|D(s)| q"
REFERENCES,0.6912442396313364,"DCQL(œÄc, œÄŒ≤)(s) + 1 #"
REFERENCES,0.6935483870967742,"+ Cr,Œ¥"
REFERENCES,0.695852534562212,"1 ‚àíŒ≥ Es,a‚àºdœÄc
M ""
1
p"
REFERENCES,0.6981566820276498,"|D(s, a)| #"
REFERENCES,0.7004608294930875,"+
1
1 ‚àíŒ≥ Es,a‚àºdœÄc
M[|r c
M(s, a) ‚àírM(s, a)|]"
REFERENCES,0.7027649769585254,"‚âúŒ∑c
2."
REFERENCES,0.7050691244239631,It follows that
REFERENCES,0.7073732718894009,"J(M, œÄo) + Œ∑c
1 ‚àíŒ≤ ŒΩ(œÅœÄo, f)"
REFERENCES,0.7096774193548387,"1 ‚àíŒ≥
‚àíŒªŒ±D(œÄo, œÄŒ≤) ‚àíŒª(1 ‚àíŒ±)D(œÄo, œÄc)"
REFERENCES,0.7119815668202765,"‚â•J(M, c
M, f, œÄo) ‚àíŒ≤ ŒΩ(œÅœÄo, f)"
REFERENCES,0.7142857142857143,"1 ‚àíŒ≥
‚àíŒªŒ±D(œÄo, œÄŒ≤) ‚àíŒª(1 ‚àíŒ±)D(œÄo, œÄc)"
REFERENCES,0.716589861751152,"‚â•J(M, c
M, f, œÄc) ‚àíŒ≤ ŒΩ(œÅœÄc, f)"
REFERENCES,0.7188940092165899,"1 ‚àíŒ≥
‚àíŒªŒ±D(œÄc, œÄŒ≤)"
REFERENCES,0.7211981566820277,"‚â•J(M, œÄc) ‚àíŒ∑c
2 ‚àíŒ≤ ŒΩ(œÅœÄc, f)"
REFERENCES,0.7235023041474654,"1 ‚àíŒ≥
‚àíŒªŒ±D(œÄc, œÄŒ≤),"
REFERENCES,0.7258064516129032,Published as a conference paper at ICLR 2022
REFERENCES,0.728110599078341,"where the second inequality is true because œÄo is the solution to Eq. (12). This gives us a lower
bound on J(M, œÄo) in terms of J(M, œÄc):"
REFERENCES,0.7304147465437788,"J(M, œÄo) ‚â•J(M, œÄc)‚àíŒ∑c
1 ‚àíŒ∑c
2 +
Œ≤
1 ‚àíŒ≥ [ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄc, f)]"
REFERENCES,0.7327188940092166,"+ ŒªŒ±D(œÄo, œÄŒ≤) + Œª(1 ‚àíŒ±)D(œÄo, œÄc) ‚àíŒªŒ±D(œÄc, œÄŒ≤)."
REFERENCES,0.7350230414746544,"It is clear that Œ∑c
1 and Œ∑c
2 are independent to Œ≤ and Œª. To show the performance improvement of œÄo
over the meta-policy œÄc, it sufÔ¨Åces to guarantee that for appropriate choices of Œ≤ and Œª,"
REFERENCES,0.7373271889400922,"‚àÜc = ŒªŒ±D(œÄo, œÄŒ≤) + Œª(1 ‚àíŒ±)D(œÄo, œÄc) ‚àíŒªŒ±D(œÄc, œÄŒ≤) +
Œ≤
1 ‚àíŒ≥ [ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄc, f)] > 0."
REFERENCES,0.7396313364055299,"To this end, the following lemma Ô¨Årst provides an upper bound on |ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄc, f)|:"
REFERENCES,0.7419354838709677,"Lemma 2. There exist some positive constants L1 and L2 such that
|ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄc, f)| ‚â§2(L1 + L2)Dtv(œÅœÄo(s, a)||œÅœÄc(s, a))."
REFERENCES,0.7442396313364056,"Proof. First, we have that
|ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄc, f)|"
REFERENCES,0.7465437788018433,"=
EœÅœÄo

œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a)"
REFERENCES,0.7488479262672811,"
‚àíEœÅœÄc

œÅœÄc(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄc(s, a)  =  X (s,a)"
REFERENCES,0.7511520737327189,"
œÅœÄo(s, a)
œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a) ‚àíœÅœÄc(s, a)
œÅœÄc(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄc(s, a)  ‚â§  X (s,a)"
REFERENCES,0.7534562211981567,"
œÅœÄo(s, a)
œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a) ‚àíœÅœÄc(s, a)
œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a)  +  X (s,a)"
REFERENCES,0.7557603686635944,"
œÅœÄc(s, a)
œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a) ‚àíœÅœÄc(s, a)
œÅœÄc(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄc(s, a)  =  X"
REFERENCES,0.7580645161290323,"(s,a)
[œÅœÄo(s, a) ‚àíœÅœÄc(s, a)]
œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a)  +  X"
REFERENCES,0.7603686635944701,"(s,a)
œÅœÄc(s, a)

œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a) ‚àí
œÅœÄc(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄc(s, a)  ‚â§
X"
REFERENCES,0.7626728110599078,"(s,a)
|œÅœÄo(s, a) ‚àíœÅœÄc(s, a)|

œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a)  +
X"
REFERENCES,0.7649769585253456,"(s,a)
œÅœÄc(s, a)

œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a) ‚àí
œÅœÄc(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄc(s, a) ."
REFERENCES,0.7672811059907834,"First, observe that for the term

œÅœÄo(s,a)‚àíd(s,a)
fd(s,a)+(1‚àíf)œÅœÄo(s,a)
,"
REFERENCES,0.7695852534562212,"‚Ä¢ If œÅœÄo(s, a) ‚â•d(s, a), then

œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a) "
REFERENCES,0.771889400921659,"‚â§

œÅœÄo(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a)"
REFERENCES,0.7741935483870968,"‚â§

œÅœÄo(s, a)
(1 ‚àíf)œÅœÄo(s, a)"
REFERENCES,0.7764976958525346,"=
1
1 ‚àíf ."
REFERENCES,0.7788018433179723,"‚Ä¢ If œÅœÄo(s, a) < d(s, a), then

œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a)"
REFERENCES,0.7811059907834101,"‚â§

d(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a)"
REFERENCES,0.783410138248848,"‚â§

d(s, a)
fd(s, a) = 1 f ."
REFERENCES,0.7857142857142857,Published as a conference paper at ICLR 2022
REFERENCES,0.7880184331797235,"Therefore,

œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a)"
REFERENCES,0.7903225806451613,"‚â§max
 1"
REFERENCES,0.7926267281105991,"f ,
1
1 ‚àíf"
REFERENCES,0.7949308755760369,"
‚âúL1."
REFERENCES,0.7972350230414746,"Next, for the term

œÅœÄo(s,a)‚àíd(s,a)
fd(s,a)+(1‚àíf)œÅœÄo(s,a) ‚àí
œÅœÄc(s,a)‚àíd(s,a)
fd(s,a)+(1‚àíf)œÅœÄc(s,a)
, consider the function g(x) ="
REFERENCES,0.7995391705069125,"x‚àíd
fd+(1‚àíf)x for x ‚àà[0, 1]. Clearly, when d(s, a) = 0,

œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a) ‚àí
œÅœÄc(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄc(s, a) = 0."
REFERENCES,0.8018433179723502,"For any (s, a) that d(s, a) > 0, it can be shown that g(x) is continuous and has bounded gradient,
i.e., |‚àág(x)| ‚â§
1
f 2d ‚âúL2. Hence, it follows that

œÅœÄo(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄo(s, a) ‚àí
œÅœÄc(s, a) ‚àíd(s, a)
fd(s, a) + (1 ‚àíf)œÅœÄc(s, a)"
REFERENCES,0.804147465437788,"‚â§L2|œÅœÄo(s, a) ‚àíœÅœÄc(s, a)|."
REFERENCES,0.8064516129032258,"Therefore, we can conclude that
|ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄc, f)| ‚â§L1
X"
REFERENCES,0.8087557603686636,"(s,a)
|œÅœÄo(s, a) ‚àíœÅœÄc(s, a)| + L2
X"
REFERENCES,0.8110599078341014,"(s,a)
œÅœÄc(s, a)|œÅœÄo(s, a) ‚àíœÅœÄc(s, a)|"
REFERENCES,0.8133640552995391,"‚â§(L1 + L2)
X"
REFERENCES,0.815668202764977,"s,a
|œÅœÄo(s, a) ‚àíœÅœÄc(s, a)|"
REFERENCES,0.8179723502304147,"=2(L1 + L2)Dtv(œÅœÄo(s, a)||œÅœÄc(s, a))."
REFERENCES,0.8202764976958525,"Recall that
œÅœÄo(s, a) = dœÄo
c
M(s)œÄo(a|s), œÅœÄc(s, a) = dœÄc
c
M(s)œÄc(a|s),
which denote the marginal state-action distributions by rolling out œÄo and œÄc in the learnt model
c
M, respectively. Lemma 2 gives an upper bound on the difference between the expected penalties
induced under œÄo and œÄc, with regard to the difference between the marginal state-action distribu-
tions. Next, we need to characterize the relationship between the marginal state-action distribution
difference and the corresponding policy distance, which is captured in the following lemma."
REFERENCES,0.8225806451612904,"Lemma 3. Let D(œÄ1||œÄ2) = maxs Dtv(œÄ1||œÄ2) denote the maximum total-variation distance be-
tween two policies œÄ1 and œÄ2. Then, we can have that"
REFERENCES,0.8248847926267281,"Dtv(œÅœÄo(s, a)||œÅœÄc(s, a)) ‚â§
1
1 ‚àíŒ≥ max
s
Dtv(œÄo(a|s)||œÄc(a|s))."
REFERENCES,0.8271889400921659,Proof. Note that
REFERENCES,0.8294930875576036,"Dtv(œÅœÄo(s, a)||œÅœÄc(s, a)) ‚â§(1 ‚àíŒ≥) ‚àû
X"
REFERENCES,0.8317972350230415,"t=0
Œ≥tDtv(œÅœÄo
t (s, a)||œÅœÄc
t (s, a))."
REFERENCES,0.8341013824884793,"It then sufÔ¨Åces to bound the state-action marginal difference at time t. Since both state-action
marginals here correspond to rolling out œÄo and œÄc in the same MDP c
M, based on Lemma B.1
and B.2 in (Janner et al., 2019), we can obtain that
Dtv(œÅœÄo
t (s, a)||œÅœÄc
t (s, a))
‚â§Dtv(œÅœÄo
t (s)||œÅœÄc
t (s)) + max
s
Dtv(œÄo(a|s)||œÄc(a|s))"
REFERENCES,0.836405529953917,"‚â§t max
s
Dtv(œÄo(a|s)||œÄc(a|s)) + max
s
Dtv(œÄo(a|s)||œÄc(a|s))"
REFERENCES,0.8387096774193549,"=(t + 1) max
s
Dtv(œÄo(a|s)||œÄc(a|s)),
which indicates that"
REFERENCES,0.8410138248847926,"Dtv(œÅœÄo(s, a)||œÅœÄc(s, a)) ‚â§(1 ‚àíŒ≥) ‚àû
X"
REFERENCES,0.8433179723502304,"t=0
Œ≥t(t + 1) max
s
Dtv(œÄo(a|s)||œÄc(a|s))"
REFERENCES,0.8456221198156681,"=
1
1 ‚àíŒ≥ max
s
Dtv(œÄo(a|s)||œÄc(a|s))."
REFERENCES,0.847926267281106,Published as a conference paper at ICLR 2022
REFERENCES,0.8502304147465438,"Building on Lemma 2 and Lemma 3, we can show that"
REFERENCES,0.8525345622119815,"|ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄc, f)| ‚â§2(L1 + L2)"
REFERENCES,0.8548387096774194,"1 ‚àíŒ≥
max
s
Dtv(œÄo(a|s)||œÄc(a|s))"
REFERENCES,0.8571428571428571,"‚âúC max
s
Dtv(œÄo(a|s)||œÄc(a|s))."
REFERENCES,0.8594470046082949,"Let D(¬∑, ¬∑) = maxs Dtv(¬∑||¬∑). It is clear that for Œª ‚â•Œª0 where Œª0 >
CŒ≤
(1‚àíŒ≥)(1‚àí2Œ±) and Œ± < 1 2,"
REFERENCES,0.8617511520737328,"‚àÜc =ŒªŒ±D(œÄo, œÄŒ≤) + Œª(1 ‚àíŒ±)D(œÄo, œÄc) ‚àíŒªŒ±D(œÄc, œÄŒ≤) +
Œ≤
1 ‚àíŒ≥ [ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄc, f)]"
REFERENCES,0.8640552995391705,"=ŒªŒ±D(œÄo, œÄŒ≤) + ŒªŒ±D(œÄo, œÄc) ‚àíŒªŒ±D(œÄc, œÄŒ≤) + Œª(1 ‚àí2Œ±)D(œÄo, œÄc)"
REFERENCES,0.8663594470046083,"+
Œ≤
1 ‚àíŒ≥ [ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄc, f)]"
REFERENCES,0.868663594470046,"‚â•Œª(1 ‚àí2Œ±)D(œÄo, œÄc) +
Œ≤
1 ‚àíŒ≥ [ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄc, f)]"
REFERENCES,0.8709677419354839,"‚â•Œª(1 ‚àí2Œ±)D(œÄo, œÄc) ‚àíCŒ≤"
REFERENCES,0.8732718894009217,"1 ‚àíŒ≥ D(œÄo, œÄc)"
REFERENCES,0.8755760368663594,"=(Œª ‚àíŒª0)(1 ‚àí2Œ±)D(œÄo, œÄc) +

Œª0(1 ‚àí2Œ±) ‚àíCŒ≤ 1 ‚àíŒ≥"
REFERENCES,0.8778801843317973,"
D(œÄo, œÄc) > 0."
REFERENCES,0.880184331797235,"In a nutshell, we can conclude that with probability 1 ‚àíŒ¥"
REFERENCES,0.8824884792626728,"J(M, œÄo) ‚â•J(M, œÄc) ‚àíŒ∑c
1 ‚àíŒ∑c
2
|
{z
}
(a)"
REFERENCES,0.8847926267281107,"+(Œª ‚àíŒª0)(1 ‚àí2Œ±)D(œÄo, œÄc)
|
{z
}
(b)"
REFERENCES,0.8870967741935484,"+

Œª0(1 ‚àí2Œ±) ‚àíCŒ≤ 1 ‚àíŒ≥"
REFERENCES,0.8894009216589862,"
D(œÄo, œÄc)
|
{z
}
(c) ,"
REFERENCES,0.8917050691244239,"where (a) depends on Œ¥ but is independent to Œª, (b) is positive and increases with Œª, and (c) is
positive. This implies that an appropriate choice of Œª will make term (b) large enough to counteract
term (a) and lead to the performance improvement over the meta-policy œÄc:
J(M, œÄo) ‚â•J(M, œÄc) + Œæ1
where Œæ1 ‚â•0."
REFERENCES,0.8940092165898618,"C.2
SAFE IMPROVEMENT OVER œÄŒ≤"
REFERENCES,0.8963133640552995,"Next, we show that the learnt policy œÄo achieves safe improvement over the behavior policy œÄŒ≤.
Based on Lemma 1, we have
J(M1, M2, f, œÄŒ≤) ‚â•J(M, œÄŒ≤) ‚àíŒ∑3
where"
REFERENCES,0.8986175115207373,Œ∑3 =2Œ≥(1 ‚àíf)
REFERENCES,0.9009216589861752,"(1 ‚àíŒ≥)2 RmaxDtv(T c
M, TM) +
Œ≥f
1 ‚àíŒ≥ |Ed
œÄŒ≤
M [(T œÄŒ≤
M ‚àíT œÄŒ≤"
REFERENCES,0.9032258064516129,"M )QœÄŒ≤
M]|"
REFERENCES,0.9055299539170507,"+
f
1 ‚àíŒ≥ Es,a‚àºd
œÄŒ≤
M [|rM(s, a) ‚àírM(s, a)|] + 1 ‚àíf"
REFERENCES,0.9078341013824884,"1 ‚àíŒ≥ Es,a‚àºd
œÄŒ≤
M [|r c
M(s, a) ‚àírM(s, a)|]"
REFERENCES,0.9101382488479263,‚â§2Œ≥(1 ‚àíf)
REFERENCES,0.9124423963133641,"(1 ‚àíŒ≥)2 RmaxDtv(T c
M, TM) + Œ≥2fCT,Œ¥Rmax"
REFERENCES,0.9147465437788018,"(1 ‚àíŒ≥)2
Es‚àºd
œÄŒ≤
M (s) ""s"
REFERENCES,0.9170506912442397,"|A|
|D(s)| #"
REFERENCES,0.9193548387096774,"+ Cr,Œ¥"
REFERENCES,0.9216589861751152,"1 ‚àíŒ≥ Es,a‚àºd
œÄŒ≤
M ""
1
p"
REFERENCES,0.923963133640553,"|D(s, a)| #"
REFERENCES,0.9262672811059908,"+
1
1 ‚àíŒ≥ Es,a‚àºd
œÄŒ≤
M [|r c
M(s, a) ‚àírM(s, a)|]"
REFERENCES,0.9285714285714286,"‚âúŒ∑Œ≤
3 ."
REFERENCES,0.9308755760368663,Published as a conference paper at ICLR 2022
REFERENCES,0.9331797235023042,"Therefore, it follows that"
REFERENCES,0.9354838709677419,"J(M, œÄo) + Œ∑c
1 ‚àíŒ≤ ŒΩ(œÅœÄo, f)"
REFERENCES,0.9377880184331797,"1 ‚àíŒ≥
‚àíŒªŒ±D(œÄo, œÄŒ≤) ‚àíŒª(1 ‚àíŒ±)D(œÄo, œÄc)"
REFERENCES,0.9400921658986175,"‚â•J(M, c
M, f, œÄo) ‚àíŒ≤ ŒΩ(œÅœÄo, f)"
REFERENCES,0.9423963133640553,"1 ‚àíŒ≥
‚àíŒªŒ±D(œÄo, œÄŒ≤) ‚àíŒª(1 ‚àíŒ±)D(œÄo, œÄc)"
REFERENCES,0.9447004608294931,"‚â•J(M, c
M, f, œÄŒ≤) ‚àíŒ≤ ŒΩ(œÅœÄŒ≤, f)"
REFERENCES,0.9470046082949308,"1 ‚àíŒ≥
‚àíŒª(1 ‚àíŒ±)D(œÄŒ≤, œÄc)"
REFERENCES,0.9493087557603687,"‚â•J(M, œÄŒ≤) ‚àíŒ∑Œ≤
3 ‚àíŒ≤ ŒΩ(œÅœÄŒ≤, f)"
REFERENCES,0.9516129032258065,"1 ‚àíŒ≥
‚àíŒª(1 ‚àíŒ±)D(œÄŒ≤, œÄc),"
REFERENCES,0.9539170506912442,"which indicates that with probability 1 ‚àíŒ¥
J(M, œÄc) ‚â•J(M, œÄŒ≤) ‚àíŒ∑c
1 ‚àíŒ∑Œ≤
3 +ŒªŒ±D(œÄo, œÄŒ≤) + Œª(1 ‚àíŒ±)D(œÄo, œÄc) ‚àíŒª(1 ‚àíŒ±)D(œÄŒ≤, œÄc)"
REFERENCES,0.956221198156682,"+
Œ≤
1 ‚àíŒ≥ [ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄŒ≤, f)],"
REFERENCES,0.9585253456221198,"where Œ∑Œ≤
3 is some constant that depends on Œ¥ but is independent to Œ≤ and Œª."
REFERENCES,0.9608294930875576,"To conclude, we can have that with probability 1 ‚àí2Œ¥
J(M, œÄo) ‚â•max{J(M, œÄc) + Œæ1, J(M, œÄŒ≤) + Œæ2}
where"
REFERENCES,0.9631336405529954,"Œæ1 = ‚àíŒ∑c
1 ‚àíŒ∑c
2 + (Œª ‚àíŒª0)(1 ‚àí2Œ±)D(œÄo, œÄc) +

Œª0(1 ‚àí2Œ±) ‚àíCŒ≤ 1 ‚àíŒ≥"
REFERENCES,0.9654377880184332,"
D(œÄo, œÄc)
(14)"
REFERENCES,0.967741935483871,"and
Œæ2 = ‚àíŒ∑c
1 ‚àíŒ∑Œ≤
3 + ŒªŒ±D(œÄo, œÄŒ≤)+Œª(1 ‚àíŒ±)D(œÄo, œÄc) ‚àíŒª(1 ‚àíŒ±)D(œÄŒ≤, œÄc)
(15)"
REFERENCES,0.9700460829493087,"+
Œ≤
1 ‚àíŒ≥ [ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄŒ≤, f)].
(16)"
REFERENCES,0.9723502304147466,"Moreover, as we noted earlier, Œæ1 > 0 for a suitably selected Œª and Œ± < 1"
REFERENCES,0.9746543778801844,"2. For the term ŒΩ(œÅœÄo, f) ‚àí"
REFERENCES,0.9769585253456221,"ŒΩ(œÅœÄŒ≤, f) in Œæ2 where ŒΩ(œÅœÄ, f) is deÔ¨Åned as EœÅœÄ
h
œÅœÄ(s,a)‚àíd(s,a)"
REFERENCES,0.9792626728110599,"df (s,a)
i
, as noted in (Yu et al., 2021b),"
REFERENCES,0.9815668202764977,"ŒΩ(œÅœÄŒ≤, f) is expected to be smaller than ŒΩ(œÅœÄo, f) in practical scenarios, due to the fact that the
dynamics T c
M learnt via supervised learning is close to the underlying dynamics TM on the states
visited by the behavior policy œÄŒ≤. This directly indicates that dœÄŒ≤
c
M(s, a) is close to dœÄŒ≤"
REFERENCES,0.9838709677419355,"M(s, a) and œÅœÄŒ≤"
REFERENCES,0.9861751152073732,"is close to d(s, a). In this case, let"
REFERENCES,0.988479262672811,"œµ = Œ≤[ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄŒ≤, f)]"
REFERENCES,0.9907834101382489,"2Œª(1 ‚àíŒ≥)D(œÄo, œÄŒ≤)
."
REFERENCES,0.9930875576036866,"We can show that for Œ± > 1 2 ‚àíœµ,"
REFERENCES,0.9953917050691244,"‚àÜŒ≤ =ŒªŒ±D(œÄo, œÄŒ≤) + Œª(1 ‚àíŒ±)D(œÄo, œÄc) ‚àíŒª(1 ‚àíŒ±)D(œÄc, œÄŒ≤) +
Œ≤
1 ‚àíŒ≥ [ŒΩ(œÅœÄo, f) ‚àíŒΩ(œÅœÄŒ≤, f)]"
REFERENCES,0.9976958525345622,"=ŒªŒ±D(œÄo, œÄŒ≤) + Œª(1 ‚àíŒ±)D(œÄo, œÄc) ‚àíŒª(1 ‚àíŒ±)D(œÄc, œÄŒ≤) + 2œµŒªD(œÄo, œÄŒ≤)
=Œª [(2œµ + Œ±)D(œÄo, œÄŒ≤) + (1 ‚àíŒ±)D(œÄo, œÄc) ‚àí(1 ‚àíŒ±)D(œÄc, œÄŒ≤)]
>Œª(1 ‚àíŒ±)[D(œÄo, œÄŒ≤) + D(œÄo, œÄc) ‚àíD(œÄc, œÄŒ≤)]
>0,
and ‚àÜŒ≤ increases with Œª, which implies that
J(M, œÄo) ‚â•J(M, œÄŒ≤) + Œæ2 = J(M, œÄŒ≤) ‚àíŒ∑c
1 ‚àíŒ∑Œ≤
3 + ‚àÜŒ≤ > J(M, œÄŒ≤)
for an appropriate choice of Œª."
