Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021551724137931034,"3D point-clouds and 2D images are different visual representations of the physical
world. While human vision can understand both representations, computer vision
models designed for 2D image and 3D point-cloud understanding are quite different.
Our paper explores the potential of transferring 2D model architectures and weights
to understand 3D point-clouds, by empirically investigating the feasibility of the
transfer, the beneﬁts of the transfer, and shedding light on why the transfer works.
We discover that we can indeed use the same architecture and pretrained weights
of a neural net model to understand both images and point-clouds. Speciﬁcally,
we transfer the image-pretrained model to a point-cloud model by inﬂating 2D
convolutional ﬁlters to 3D convolutional ﬁlters and finetuning the inﬂated image-
pretrained models (FIP). We ﬁnd that models with minimal ﬁnetuning efforts —
only on input, output, and optionally, batch normalization layers — can achieve
competitive performance on 3D point-cloud classiﬁcation, beating a wide range
of point-cloud models that adopt task-speciﬁc architectures and use a variety of
tricks. When ﬁnetuning the whole model, the performance gets further improved.
Meanwhile, FIP improves data efﬁciency, reaching up to 10.0 points top-1 accuracy
gain on few-shot classiﬁcation. It also speeds up training of point-cloud models by
up to 11.1x for a target accuracy."
INTRODUCTION,0.004310344827586207,"1
INTRODUCTION"
INTRODUCTION,0.00646551724137931,"Point-cloud is an important visual representation for 3D computer vision. It is widely used in
applications such as autonomous driving (Behley et al., 2019; Caesar et al., 2020; Yue et al., 2018),
robotics (Armeni et al., 2017; Pomerleau et al., 2015; Xu et al., 2021), augmented and virtual reality
(Sketchup, 2021; Wu et al., 2015; Shi et al., 2015), etc. A point-cloud represents visual information
in a highly different way from a 2D image. A point-cloud consists of a set of unordered points
lying on the object’s surface, with each point encoding its spatial x, y, z coordinates and potentially
other features. In contrast, a 2D image organizes visual features as a dense 2D pixel array. Due to
the representation differences, 2D image and 3D point-cloud understanding are treated as separate
problems. Image models and point-cloud models are designed to have different architectures and are
trained on different types of data. Few research efforts have tried to directly transfer models from
images to point-clouds or vice versa."
INTRODUCTION,0.008620689655172414,"Intuitively, both 3D point-clouds and 2D images are visual representations of the physical world.
Their low-level representations are drastically different, but they can represent the same underlying
visual concept. Furthermore, human vision has no problem understanding both representations.
However, can computer vision models trained on one modality understand the other?"
INTRODUCTION,0.010775862068965518,"Somewhat remarkably, the answer to the question above is: Yes. 2D image models trained on image
datasets can be transferred to understand 3D point-clouds with minimal efforts. As illustrated in
Figure 1, we transfer a 2D ConvNet to a 3D ConvNet whose input is a 3D voxel representation
converted from a point-cloud. Based on a pretrained 2D ConvNet, we inﬂate its 2D convolutional
ﬁlters to 3D by copying the ﬁlter weights along a third dimension. We add linear input and output
layers to the network; and on a target point-cloud dataset, we only ﬁnetune the input/output layers,
and optionally, the normalization layers, while keeping the pretrained model weights untouched. We
term such partially-ﬁnetuned-image-pretrained models as FIP-IO (ﬁnetuning only input and output
layers) or FIP-IO+BN (ﬁnetuning input, output, and BN layers). FIP-IO+BN can achieve competitive"
INTRODUCTION,0.01293103448275862,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.015086206896551725,Pretrained 2D ConvNet
D SPARSE CONVNET,0.017241379310344827,3D Sparse ConvNet
D SPARSE CONVNET,0.01939655172413793,"Classifier
Dog Chair"
D SPARSE CONVNET,0.021551724137931036,Inflation
D SPARSE CONVNET,0.023706896551724137,"Conv2D
Norm
Conv2D
Norm
…"
D SPARSE CONVNET,0.02586206896551724,"Sparse
Conv3D
Norm
…
Sparse
Conv3D
Norm"
D SPARSE CONVNET,0.028017241379310345,Input Layer
D SPARSE CONVNET,0.03017241379310345,Input Layer
D SPARSE CONVNET,0.032327586206896554,Input Layer
D SPARSE CONVNET,0.034482758620689655,Input Layer
D SPARSE CONVNET,0.036637931034482756,Classifier
D SPARSE CONVNET,0.03879310344827586,Decoder
D SPARSE CONVNET,0.040948275862068964,Decoder
D SPARSE CONVNET,0.04310344827586207,"Copy
Inflation
Copy"
D SPARSE CONVNET,0.04525862068965517,"Figure 1: We investigate the feasibility of pretrained 2D ConvNets transferring to 3D sparse ConvNets.
With ﬁlter inﬂation and ﬁnetuning only the input, output layer (classiﬁer for classiﬁcation task and
decoder for semantic segmentation task), and optionally, normalization layers, 3D Sparse ConvNets
are capable of dealing with point-cloud classiﬁcation, indoor, and driving scene segmentation."
D SPARSE CONVNET,0.04741379310344827,"performance up to 90.8% top-1 accuracy on the ModelNet 3D Warehouse dataset, on top a ResNet50,
outperforming previous point-cloud models that adopt task-speciﬁc model architectures and tricks."
D SPARSE CONVNET,0.04956896551724138,"Most point-cloud models except projection-based models are only trained from scratch. Based on
our discovery, we further investigate fully-ﬁnetuned-image-pretrained models (termed as FIP-ALL).
We observe that FIP-ALL brings signiﬁcant improvement on top of ResNet series. Besides applying
FIP-ALL to voxel-based method, we also ﬁnd that it generalizes to other popular methods, such
as point-based method (PointNet++ (Qi et al., 2017)) and projection-based method (SimpleView
(Goyal et al., 2021a)), as well as current popular vision transformers (ViT (Dosovitskiy et al.,
2020)). Speciﬁcally, FIP-ALL largely outperforms the training-from-scratch by 0.88, 0.50, 3.50, 4.18
points top-1 accuracy on top of PointNet++, SimpleView, ViT-B-16, and ViT-L-16, respectively. In
addition to the performance gain, FIP-ALL exhibits superior data efﬁciency with up to 10.0 points
improvement in few-shot classiﬁcation on the ModelNet 3D Warehouse dataset. Comparing with
training-from-scratch, FIP-ALL also dramatically speeds up the training by using 11.1 times fewer
epochs to reach a target validation accuracy."
D SPARSE CONVNET,0.05172413793103448,"In order to understand why the image pretraining can be utilized to beneﬁt point-cloud understanding,
we conduct experiment studying the network dissection (Bau et al., 2017), text-shape representation
transferring (Geirhos et al., 2018), and distribution distance."
RELATED WORK,0.05387931034482758,"2
RELATED WORK"
POINT-CLOUD PROCESSING MODEL,0.05603448275862069,"2.1
POINT-CLOUD PROCESSING MODEL"
D CONVOLUTION-BASED METHOD IS ONE OF THE MAINSTREAM POINT-CLOUD PROCESSING APPROACHES WHICH,0.05818965517241379,"3D convolution-based method is one of the mainstream point-cloud processing approaches which
efﬁciently process point-clouds based on voxelization. In this approach, voxelization is used to
rasterize point-clouds into regular grids (called voxels) so that conventional 3D convolutions can be
applied. Sparse convolution is proposed to apply on the non-empty voxels (Liu et al., 2015; Choy
et al., 2019; Tang et al., 2020; Zhou et al., 2020; Yan et al., 2018; Feng et al., 2021), largely improving
the efﬁciency of 3D convolutions."
D CONVOLUTION-BASED METHOD IS ONE OF THE MAINSTREAM POINT-CLOUD PROCESSING APPROACHES WHICH,0.0603448275862069,"Projection-based method attempts to project a 3D point-cloud to a 2D plane and uses 2D convolution
to extract features (Wang et al., 2018; Wu et al., 2018; 2019; Xu et al., 2020; Su et al., 2015; Lawin
et al., 2017; Boulch et al., 2017). Speciﬁcally, bird-eye-view projection (Yang et al., 2018; Lang et al.,
2019) and spherical projection (Wu et al., 2018; 2019; Xu et al., 2020; Milioto et al., 2019) make
great progress in outdoor point-cloud tasks."
D CONVOLUTION-BASED METHOD IS ONE OF THE MAINSTREAM POINT-CLOUD PROCESSING APPROACHES WHICH,0.0625,"Point-based method directly processes the point-cloud data. The most classic methods, PointNet
(Qi et al., 2016) and PointNet++ (Qi et al., 2017), consume points by customized feature aggregation.
Many works further develop advanced local-feature aggregation operators that mimic the convolution"
D CONVOLUTION-BASED METHOD IS ONE OF THE MAINSTREAM POINT-CLOUD PROCESSING APPROACHES WHICH,0.06465517241379311,Under review as a conference paper at ICLR 2022
D CONVOLUTION-BASED METHOD IS ONE OF THE MAINSTREAM POINT-CLOUD PROCESSING APPROACHES WHICH,0.0668103448275862,"to structure data (Xu et al., 2021; Li et al., 2018b; Hua et al., 2018; Liu et al., 2019; 2020; Wang et al.,
2017; Li et al., 2018a; Komarichev et al., 2019)."
D CONVOLUTION-BASED METHOD IS ONE OF THE MAINSTREAM POINT-CLOUD PROCESSING APPROACHES WHICH,0.06896551724137931,"2.2
PRETRAINING IN 2D AND 3D VISION"
D CONVOLUTION-BASED METHOD IS ONE OF THE MAINSTREAM POINT-CLOUD PROCESSING APPROACHES WHICH,0.07112068965517242,"Pretraining in 2D vision has shown effectiveness under supervised (Dosovitskiy et al., 2020; Gir-
shick et al., 2014), self-supervised (Jing & Tian, 2020; Goyal et al., 2021b), and unsupervised
contrastive approach (He et al., 2020; Bachman et al., 2019; Chen et al., 2020a; Caron et al., 2020;
Chen et al., 2020c; Hjelm et al., 2018). After pretraining on a large amount of data, a 2D model re-
quires much fewer computational resources and data for ﬁnetuning to reach competitive performance
on downstream tasks (Kataoka et al., 2020; Caron et al., 2019; Chen et al., 2020b; Henaff, 2020)."
D CONVOLUTION-BASED METHOD IS ONE OF THE MAINSTREAM POINT-CLOUD PROCESSING APPROACHES WHICH,0.07327586206896551,"Pretraining in 3D vision has been studied similarly as pretraining in 2D vision: both self-supervised
and contrastive pretraining (Xie et al., 2020) show promising results. Due to the lack of large,
annotated point-cloud datasets, pretraining in 3D vision is motivated to achieve data efﬁciency (Xu &
Lee, 2020). Recent works (Hou et al., 2020; Zhang et al., 2021b) consider pretraining methods with
data efﬁciency in mind, for example, Contrastive Scene Contents which makes use of both point-level
correspondences and spatial contexts."
CROSS-MODAL TRANSFER LEARNING,0.07543103448275862,"2.3
CROSS-MODAL TRANSFER LEARNING"
CROSS-MODAL TRANSFER LEARNING,0.07758620689655173,"Cross-modal transfer learning attempts to take advantage of data from different modalities (Dai &
Nießner, 2018; Liu et al., 2021b). For example, Liu et al. (2021a) proposed pixel-to-point knowledge
transfer (PPKT) from 2D to 3D which uses aligned RGB and RGB-D images during pretraining.
Our work does not rely on joint image-point-cloud pretraining. Instead, we directly transfer an
image-pretrained model to point-cloud with the simplest pretraining-ﬁnetuning scheme."
CROSS-MODAL TRANSFER LEARNING,0.07974137931034483,"Some of the previous works for video and medical images (Carreira & Zisserman, 2017; Shan et al.,
2018) have adopted the method of simply extending a pretrained 2D convolutional ﬁlter along time
or depth direction for transferring to 3D models. Between language and image modality, transfer
learning with minimal ﬁnetuning also shows a competitive performance (Lu et al., 2021)."
CROSS-MODAL TRANSFER LEARNING,0.08189655172413793,"3
CONVERTING A 2D CONVNET TO A 3D CONVNET"
CROSS-MODAL TRANSFER LEARNING,0.08405172413793104,"In this paper, we primarily focus on the 3D sparse-convolution based method to process point-clouds
because it can be extended to all point-cloud tasks. As discussed in 2.1, we consider a set of points
where each point is represented by its 3D coordinates and additional features such as intensity and
RGB. We then voxelize/quantize these points into voxels according to their 3D space coordinates,
following Choy et al. (2019). A voxel’s feature is inherited from the point that lies in the voxel. If
there are multiple points in a voxel, then we average all points’ feature and assign the mean to the
voxel. If there is no point in the voxel, then we simply set the voxel’s feature to 0. When using sparse
convolution, we skip the computation on empty voxels."
CROSS-MODAL TRANSFER LEARNING,0.08620689655172414,"Given a pretrained 2D ConvNet, we convert it to a 3D ConvNet that takes 3D voxels as input. The
key element of this procedure is to convert 2D convolution ﬁlters to 3D, i.e. constructing 3D ﬁlters
with the weights directly inherited from 2D ﬁlters. A 2D convolutional ﬁlter can be represented
with a 4D tensor of shape [M, N, K, K], representing output dimension, input dimension, and two
spatial kernel sizes, respectively. A 3D convolutional ﬁlter has an extra dimension, and its shape is
[M, N, K, K, K]. To better illustrate, we ignore the output and input dimensions and only consider a
spatial slice of the 2D ﬁlter with shape [K, K]. The simplest way to convert this 2D ﬁlter to 3D is
to copy the 2D ﬁlter and repeat it by K times along a third dimension. This operation is the same
as the inﬂation technique used by (Carreira & Zisserman, 2017) to initialize a video model with a
pretrained 2D ConvNet."
CROSS-MODAL TRANSFER LEARNING,0.08836206896551724,"Besides convolution, other operations such as downsampling, BN, nonlinear activation can be easily
migrated to 3D. Our 3D model inherits the architecture of the original 2D ConvNet, but we also add a
linear layer as the input layer and an output layer depending on the target task. For classiﬁcation, we
use a global average pooling layer followed by one fully connected layer to get the ﬁnal prediction.
For semantic segmentation, the output layer is a U-Net style decoder (Ronneberger et al., 2015). The
architecture of the input/output layers is described in more detail in Appendix A.9."
CROSS-MODAL TRANSFER LEARNING,0.09051724137931035,Under review as a conference paper at ICLR 2022 //
CROSS-MODAL TRANSFER LEARNING,0.09267241379310345,FIP-IO (71.03)
CROSS-MODAL TRANSFER LEARNING,0.09482758620689655,FIP-IO+BN (88.75)
CROSS-MODAL TRANSFER LEARNING,0.09698275862068965,FIP-IO (81.20)
CROSS-MODAL TRANSFER LEARNING,0.09913793103448276,FIP-IO (73.74)
CROSS-MODAL TRANSFER LEARNING,0.10129310344827586,FIP-IO+BN (90.44)
CROSS-MODAL TRANSFER LEARNING,0.10344827586206896,FIP-IO+BN (90.80)
CROSS-MODAL TRANSFER LEARNING,0.10560344827586207,FIP-IO (64.63)
CROSS-MODAL TRANSFER LEARNING,0.10775862068965517,FIP-IO+BN (89.87) 60 65 70 75 80 85 90 95
CROSS-MODAL TRANSFER LEARNING,0.10991379310344827,"0
0.001
0.002
0.003
0.004"
CROSS-MODAL TRANSFER LEARNING,0.11206896551724138,"ResNet18 90.39
ResNet50 90.32
ResNet152 90.28 1"
X,0.11422413793103449,345.5 x
X,0.11637931034482758,ResNet18 on ImageNet1K
X,0.11853448275862069,ResNet50 on ImageNet1K
X,0.1206896551724138,ResNet50 on ImageNet21K
X,0.12284482758620689,ResNet152 on ImageNet1K
X,0.125,Train from scratch
X,0.1271551724137931,Trainable parameters/Total parameters
X,0.12931034482758622,Top 1 Accuracy
X,0.1314655172413793,"77
77.6"
X,0.1336206896551724,"89.2
90.7
90.7
90.1
90.6
90.316
90.8 83.35 65 70 75 80 85 90"
DSHAPENETS,0.13577586206896552,3DShapeNets
DSHAPENETS,0.13793103448275862,DeepPano
DSHAPENETS,0.1400862068965517,PointNet
DSHAPENETS,0.14224137931034483,PointNet++ DGCNN MVCNN KDNet
DSHAPENETS,0.14439655172413793,ResNet50 (baseline)
DSHAPENETS,0.14655172413793102,ResNet50 FIP-IO
DSHAPENETS,0.14870689655172414,ResNet50 FIP-IO+BN
DSHAPENETS,0.15086206896551724,"Train from scratch
Tiny-ImageNet pretrain
ImageNet1K pretrain"
DSHAPENETS,0.15301724137931033,"ImageNet21K pretrain
FractalDB1K pretrain
FractalDB10K pretrain"
DSHAPENETS,0.15517241379310345,"(a) Trainable Parameters vs. Accuracy
(b) Pretrained Dataset"
DSHAPENETS,0.15732758620689655,"Figure 2: a) the left ﬁgure shows the trainable parameters ratio w.r.t top-1 accuracy on ModelNet 3D
Warehouse dataset. b) the right ﬁgure shows the performance of FIP-IO and FIP-IO+BN on top of
ResNet50 pretrained on different datasets."
EMPIRICAL EVALUATION,0.15948275862068967,"4
EMPIRICAL EVALUATION"
EMPIRICAL EVALUATION,0.16163793103448276,"To explore the image to point-cloud transfer, we study three settings: 1) partially-ﬁnetuned-image-
pretrained model, only ﬁnetuning input and output layers (FIP-IO), 2) ﬁnetuning input, output, and
batch normalization layers (FIP-IO+BN), and 3) ﬁnetuning the whole pretrained network (FIP-ALL).
Under the three settings, we extensively explore the feasibility of transferring the image-pretrained
model for point-cloud understanding and its beneﬁts. The entire empirical evaluation is organized as
four questions: 1) Can we transfer pretrained-image models to recognize point-clouds? (Section 4.1)
2) Can image-pretraining beneﬁt the performance of point-cloud recognition? (Section 4.2) 3) Can
image-pretrained model improve the data efﬁciency on point-cloud recognition? (Section 4.3) 4) Can
image-pretrained model accelerate training point-cloud models? (Section 4.4)"
EMPIRICAL EVALUATION,0.16379310344827586,"Datasets.
We benchmark the transferred models on ModelNet 3D Warehouse classiﬁcation (Wu
et al., 2015), S3DIS indoor segmentation (Armeni et al., 2017), and SemanticKITTI outdoor segmen-
tation (Behley et al., 2019) tasks. ModelNet 3D Warehouse is a CAD model classiﬁcation dataset
that consists of point-clouds with 40 categories. CAD models in this benchmark come from 3D
Warehouse (Sketchup, 2021). In this benchmark, we only utilize x, y, z coordinates as features. S3DIS
is a dataset collected from real-world indoor scenes and includes 3D scans of Matterport Scanners
from 6 areas. It provides point-wise annotations for indoor objects like chair, table, and bookshelf,
etc. SemanticKITTI dataset from KITTI Vision Odometry (Geiger et al., 2012) is a driving scene
dataset. It provides dense point-wise annotations for the complete 360 degrees ﬁeld-of-view of the
deployed automotive lidar, which is currently one of the most challenging datasets."
EMPIRICAL EVALUATION,0.16594827586206898,"ResNet (He et al., 2016a) series is used mostly throughout our experiments. Depending on the
experiments, ResNets are pretrained on Tiny-ImageNet, ImageNet-1K, ImageNet-21K (Deng et al.,
2009), and Fractal database (FractalDB) (Kataoka et al., 2020). Our pretrained models are directly
downloaded from various sources, with detailed links provided in Section A.1. To study the beneﬁts
of using pretrained image models, we also utilize PointNet++ (Qi et al., 2017), ViT (Dosovitskiy
et al., 2020), and SimpleView (Goyal et al., 2021a) as our baselines."
EMPIRICAL EVALUATION,0.16810344827586207,"4.1
CAN WE TRANSFER PRETRAINED-IMAGE MODELS TO RECOGNIZE POINT-CLOUDS?"
EMPIRICAL EVALUATION,0.17025862068965517,"To evaluate the feasibility of transferring pretrained 2D image models to 3D point-cloud tasks, we
conduct experiments on top of the ResNet series since there are abundant open-source pretrained
ResNet available. In particular, we convert 2D ConvNets into 3D ConvNets using the procedure
described in Section 3. We hypothesize that, if a pretrained 2D image model is capable of understand-
ing point-clouds directly, we can see a non-trivial performance by only ﬁnetuning input and output
layers of the transferred model. Further, as we gradually relax the frozen parameters, ﬁnetuning
BN parameters as well, the transferred model can achieve better performance, even surpassing
training-from-scratch performance."
EMPIRICAL EVALUATION,0.1724137931034483,Under review as a conference paper at ICLR 2022
EMPIRICAL EVALUATION,0.17456896551724138,"Table 1: ModelNet 3D Warehouse classiﬁcation results (top-1 accuracy %) of fully-ﬁnetuned-image-
pretrained models (FIP-ALL) based on different pretrained models. We include 2021 SOTAs, such as
RSMix (Lee et al., 2021), Point Transformer (Point-Trans) (Zhao et al., 2021), DRNet (Qiu et al.,
2021), and PointCutMix (Zhang et al., 2021a), for comparison."
EMPIRICAL EVALUATION,0.17672413793103448,"Method
ResNet18
ResNet50
ResNet152
ResNet101×2"
EMPIRICAL EVALUATION,0.1788793103448276,"From Scratch
90.39
90.32
90.28
90.03
FIP-ALL on ImageNet1K
90.52 (+0.13)
90.92 (+0.60)
91.09 (+0.81)
90.52 (+0.49)
FIP-ALL on ImageNet21K
-
91.05 (+0.73)
-
-"
EMPIRICAL EVALUATION,0.1810344827586207,"Method
PointNet++(SSG)
ViT-B-16
ViT-L-16
SimpleView"
EMPIRICAL EVALUATION,0.18318965517241378,"From Scratch
90.34
84.27
83.48
93.3
FIP-ALL on ImageNet1K
91.22 (+0.88)
-
-
93.8 (+0.50)
FIP-ALL on ImageNet21K
-
87.77 (+3.50)
87.66 (+4.18)
-"
EMPIRICAL EVALUATION,0.1853448275862069,"Method
RSMix
Point-Trans
DRNet
PointCutMix"
EMPIRICAL EVALUATION,0.1875,"From Scratch
93.5
93.7
93.1
93.4"
EMPIRICAL EVALUATION,0.1896551724137931,"Table 2: Indoor scene and outdoor scene segmentation results (mIoU %) of fully-ﬁnetuned-image-
pretrained Model (FIP-ALL). In this table, all image-pretrained models are pretrained on ImageNet1K."
EMPIRICAL EVALUATION,0.19181034482758622,"Method
S3DIS (mIoU %)
SemanticKITTI (mIoU %)"
EMPIRICAL EVALUATION,0.1939655172413793,"PointNet++(SSG)
ResNet18
HRNetV2-W48
ResNet18"
EMPIRICAL EVALUATION,0.1961206896551724,"From Scratch
52.45
55.09
44.12
64.75
FIP-ALL on ImageNet1K
55.01 (+2.56)
56.62 (+1.53)
47.53 (+3.41)
65.57 (+0.82)"
EMPIRICAL EVALUATION,0.19827586206896552,"We conduct two groups of experiments with FIP-IO and FIP-IO+BN, with the results shown in
Figure 2. The ﬁrst is to evaluate the performance as the trainable parameters gradually increase. As
shown in Figure 2 (a), training no more than 0.3 % (345.5x fewer) of the whole parameters, the
image pretraining even beats the training-from-scratch (100 % trainable parameters). Speciﬁcally,
ResNet152 FIP-IO+BN with ImageNet1K pretraining improves training-from-scratch by 0.16 points,
and ResNet50 FIP-IO+BN with ImageNet21K pretraining improves 0.48 points. Meanwhile, FIP-IO
also reaches a non-trivial performance. ResNet50 FIP-IO pretrained on ImageNet1K achieves 81.20
% top-1 accuracy, only 9.12 points worse than training-from-scratch with approximately 0.1 %
trainable parameters."
EMPIRICAL EVALUATION,0.20043103448275862,"Furthermore, to investigate the effect of different datasets, as shown in the right ﬁgure of Figure 2, we
inﬂate ResNet50 pretrained from different image datasets, including Tiny-ImageNet, ImageNet1K,
ImageNet21K, FractalDB1K, and FractalDB10K, then evaluate on the ModelNet 3D Warehouse."
EMPIRICAL EVALUATION,0.2025862068965517,"We discover that, even if we only ﬁnetune the input and output layers while keeping the image-
pretrained weights frozen, the FIP-IO pretrained from ImageNet1K, FractalDB1K, and FractalDB10K
achieves competitive performance. Speciﬁcally, ResNet50 FIP-IO with ImageNet1K pretraining
outperforms 3D ShapeNet (Wu et al., 2015) and DeepPano (Shi et al., 2015), which were the state-of-
the-arts in 2015, by 4.2 and 3.6 points respectively in top-1 accuracy on ModelNet 3D Warehouse.
More importantly, with ImageNet21K pretrained model, ResNet50 FIP-IO+BN surpasses training-
from-scratch by 0.48 points, even beating a variety of well-known methods including PointNet (Qi
et al., 2016), MVCNN (Su et al., 2015), DGCNN (Wang et al., 2019), KDNet (Klokov & Lempitsky,
2017), etc."
EMPIRICAL EVALUATION,0.20474137931034483,"Notably, we ﬁnd out the answer to ""Can we transfer pretrained-image models to recognize point-
clouds?"": Yes. The pretrained 2D image models can be directly used for recognizing point-clouds. It
is also noteworthy that the pretraining dataset is not restricted to natural but also synthetic images
like those in FractalDB1K/10K."
EMPIRICAL EVALUATION,0.20689655172413793,"4.2
CAN IMAGE-PRETRAINING BENEFIT POINT-CLOUD RECOGNITION?"
EMPIRICAL EVALUATION,0.20905172413793102,"From the previous subsection, we ﬁnd unexpectedly that image-pretrained model can be directly used
for point-cloud understanding. In this subsection, we investigate whether image-pretrained model"
EMPIRICAL EVALUATION,0.21120689655172414,Under review as a conference paper at ICLR 2022
EMPIRICAL EVALUATION,0.21336206896551724,"Table 3: Comparison with PointContrast (Xie et al., 2020) on the ModelNet 3D Warehouse. PointCon-
trast provides two different pretrained models with using PointInfoNCE loss and Hardest Contrastive
loss, respectively."
EMPIRICAL EVALUATION,0.21551724137931033,"From scratch
PointInfoNCE
Hardest Contrastive
ImageNet1K pretrain (Ours)"
EMPIRICAL EVALUATION,0.21767241379310345,"89.95
90.24 (+0.29)
90.15 (+0.20)
90.88 (+0.93)"
EMPIRICAL EVALUATION,0.21982758620689655,"is helpful to improve the performance on point-cloud tasks. We use different baselines, including
voxelization-based method (simply ResNet), point-based method (PointNet++ (Qi et al., 2017)),
projection-based method (SimpleView (Goyal et al., 2021a)), and current popular transformer-based
method (ViT-B-16 and ViT-L-16 (Dosovitskiy et al., 2020)), and fully ﬁnetune them on three point-
cloud datasets: classiﬁcation on ModelNet 3D Warehouse, indoor scene segmentation on S3DIS, and
outdoor scene segmentation on SemanticKITTI, as shown in Table 1 and Table 2."
EMPIRICAL EVALUATION,0.22198275862068967,"For PointNet++, we use ImageNet1K to pretrain: we break each image into pixels and regard it as a
point-cloud. For ViT, we directly use the open-source pretrained model and ﬁnetune it on ModelNet
3D Warehouse. All the implementation details are illustrated in Appendix A.1."
EMPIRICAL EVALUATION,0.22413793103448276,"Table 1 presents performance on ModelNet 3D Warehouse dataset. We observe that FIP-ALL
improves all baselines steadily and signiﬁcantly. Besides, pretraining brings more improvements
to deeper models. For example, ResNet18 can only be improved by 0.13% top-1 accuracy, but
pretraining on ImageNet1K leads to 0.81 points top-1 accuracy improvement on top of ResNet152.
Moreover, larger pretrained datasets also lead to better performance. Speciﬁcally, ResNet50 FIP-ALL
from ImageNet21K can reach 91.05% top-1 acc, with 0.73 points improvement over training-from-
scratch. Such FIP-ALL signiﬁcantly outperforms a series of well-known methods such as (Qi et al.,
2016; 2017; Klokov & Lempitsky, 2017; Wang et al., 2019; Su et al., 2015; Li et al., 2018a)."
EMPIRICAL EVALUATION,0.22629310344827586,"We also explore FIP-ALL on different architectures, as shown in the second group of Table 1.
In particular, FIP-ALL on top of PointNet++, ViT-B-16, ViT-L-16 and SimpleView with image
dataset pretraining improve the training-from-scratch by 0.88, 3.50, 4.18, 0.50 points, respectively.
Especially for the current superior baseline in image recognition, ViT-B-16 and ViT-L-16, the
improved performance is quite signiﬁcant, revealing the huge potential of using image-pretrained
models for point cloud recognition."
EMPIRICAL EVALUATION,0.22844827586206898,"For the challenging indoor and outdoor scene segmentation, using ImageNet1K pretrained models
(FIP-ALL on ImageNet1K) also improve the training-from-scratch consistently, as shown in Table 2.
PointNet++ (resp. ResNet18) pretrained on ImageNet1K outperforms the training-from-scratch by
2.56 points (resp. 1.53 points) mIoU on S3DIS dataset. For SemanticKITTI, we utilize the commonly
used projection-based method with 2D ConvNet HRNet. With ImageNet1K pretraining, we observe
3.41 points mIoU improvement, a large margin in such a challenging task. Since HRNetV2-W48 has
rich pretrained models, we ﬁnetune Cityscapes pretrained HRNetV2-W48 and observe this enhances
more (5.25% mIoU improvement over training from scratch). Even for the ResNet18 with a high
from-scratch performance of 64.75% mIoU, the ImageNet1K pretraining can also bring 0.82 points
mIoU improvement."
EMPIRICAL EVALUATION,0.23060344827586207,"Finally, we compare the performance gain with the well-known point-cloud self-supervised method
PointContrast (Xie et al., 2020), as presented in Table 3. We use the same model architecture and
ﬁnetuning recipe, and the only difference is the pretraining weights. We can observe that image-
pretraining on ImageNet1K signiﬁcantly boosts the training-from-scratch by 0.93 points, surpassing
the PointContrast by at least 0.64 points."
EMPIRICAL EVALUATION,0.23275862068965517,"Therefore, the answer to ""Can image-pretraining beneﬁt point-cloud recognition"" is: Yes. Image-
pretraining can indeed improve point-cloud recognition, which can generalize to a wide range of
backbones and beneﬁt more challenging tasks."
CAN IMAGE-PRETRAINED MODELS IMPROVE THE DATA EFFICIENCY ON POINT-CLOUD,0.2349137931034483,"4.3
CAN IMAGE-PRETRAINED MODELS IMPROVE THE DATA EFFICIENCY ON POINT-CLOUD
RECOGNITION?"
CAN IMAGE-PRETRAINED MODELS IMPROVE THE DATA EFFICIENCY ON POINT-CLOUD,0.23706896551724138,"Data efﬁciency is extremely important in point-cloud understanding due to the huge labor of collecting
and annotating point-cloud data. In this subsection, we investigate whether the image-pretrained"
CAN IMAGE-PRETRAINED MODELS IMPROVE THE DATA EFFICIENCY ON POINT-CLOUD,0.23922413793103448,Under review as a conference paper at ICLR 2022
CAN IMAGE-PRETRAINED MODELS IMPROVE THE DATA EFFICIENCY ON POINT-CLOUD,0.2413793103448276,Table 4: Few-shot experiments on top of different ResNets on the ModelNet 3D Warehouse dataset.
CAN IMAGE-PRETRAINED MODELS IMPROVE THE DATA EFFICIENCY ON POINT-CLOUD,0.2435344827586207,"Few-shot
ResNet18
ResNet50
ResNet152
(from scratch/FIP-ALL)"
-SHOT,0.24568965517241378,"10-shot
72.2±0.8/73.2±0.6 (+1.0)
71.7±0.7/74.1±0.8 (+2.4)
69.8±1.1/73.9±0.4 (+4.1)
5-shot
63.7±1.6/66.6±0.8 (+2.9)
62.4±1.1/66.0±2.2 (+3.6)
59.4±0.8/66.5±0.9 (+7.1)
1-shot
26.8±4.4/36.8±0.6 (+10.0)
28.1±0.4/34.1±0.2 (+6.0)
23.3±4.3/33.2±1.3 (+9.9) 70 75 80 85 90 95"
-SHOT,0.2478448275862069,"1
11
21
31
41
51
61
71
81
91 101 111 121"
-SHOT,0.25,Validation (top 1 accuracy) vs. Epoch
-SHOT,0.2521551724137931,ResNet 152 train from scratch
-SHOT,0.2543103448275862,ResNet 152 FIP-ALL on ImageNet1K 70 75 80 85 90 95
-SHOT,0.25646551724137934,"1
11
21
31
41
51
61
71
81
91 101 111 121"
-SHOT,0.25862068965517243,Validation (top 1 accuracy) vs. Epoch
-SHOT,0.2607758620689655,ResNet 18 train from scratch
-SHOT,0.2629310344827586,ResNet 18 FIP-ALL on ImageNet1K
EPOCH,0.2650862068965517,19 epoch
EPOCH,0.2672413793103448,56 epoch
EPOCH,0.26939655172413796,1th epoch: 79.34
EPOCH,0.27155172413793105,"1th epoch: 13.94
70 75 80 85 90 95"
EPOCH,0.27370689655172414,"1
11
21
31
41
51
61
71
81
91 101 111 121"
EPOCH,0.27586206896551724,Validation (top 1 accuracy) vs. Epoch
EPOCH,0.27801724137931033,ResNet 50 train from scratch
EPOCH,0.2801724137931034,ResNet 50 FIP-ALL on ImageNet1K
EPOCH,0.2823275862068966,1th epoch: 81.65
EPOCH,0.28448275862068967,1th epoch: 70.22
EPOCH,0.28663793103448276,28 epoch
EPOCH,0.28879310344827586,60 epoch
EPOCH,0.29094827586206895,1th epoch: 80.11
EPOCH,0.29310344827586204,1th epoch: 28.48
EPOCH,0.2952586206896552,11 epoch
EPOCH,0.2974137931034483,122 epoch
EPOCH,0.2995689655172414,"Epoch
Epoch
Epoch"
EPOCH,0.3017241379310345,"Figure 3: The curves of validation accuracy w.r.t training epoch. We compare the results be-
tween training-from-scratch and FIP-ALL on the ImageNet1K, on top of ResNet18, ResNet50, and
ResNet152, respectively."
EPOCH,0.30387931034482757,"model can help to improve the data efﬁciency by conducting few-shot setting experiments, including
1-shot, 5-shot, and 10-shot. We conduct 3 trials for each setting and report the results as mean ± std."
EPOCH,0.30603448275862066,"In detail, for each class (ModelNet 3D Warehouse involves 40 classes), we randomly choose a few
point-clouds as training data, and still evaluate on the whole test set. We compare the results between
training-from-scratch and FIP-ALL pretrained on the ImageNet1K dataset. The experimental results
are shown in Table 4. We observe that FIP-ALL dramatically surpasses training-from-scratch on the
low data regime (1-shot): pretraining on ImageNet1K brings 10.0, 6.0, and 9.9 points top-1 accuracy
improvement for ResNet18, ResNet50, and ResNet152, respectively. For 5-shot and 10-shot settings,
using ImageNet1K pretraining can still consistently improve the performance. However, we also
observe that as the amount of training data increases, the performance gain becomes saturated."
EPOCH,0.3081896551724138,"Therefore, our answer to ""Can image-pretrained model improve the data efﬁciency on point-cloud
recognition?"" is: Yes. Image-pretrained model can improve the data efﬁciency on point-cloud
recognition, especially on low data regime. When the training data increases, it can still improve the
performance, but the gain becomes marginal."
EPOCH,0.3103448275862069,"4.4
CAN IMAGE-PRETRAINED MODELS ACCELERATE POINT-CLOUD TRAINING?"
EPOCH,0.3125,"We also investigate whether image-pretrained model can help point-cloud task train faster. The results
are shown in Figure 3."
EPOCH,0.3146551724137931,"We discover that, after training only one epoch on ModelNet 3D Warehouse dataset, FIP-ALL on
ImageNet1K achieves very impressive performance, yet the performance of training-from-scratch is
still at a low level. For example, after the ﬁrst epoch, ResNet50 (resp. ResNet152) with training from
scratch can only achieve 28.48% (resp. 13.94%) top-1 accuracy while ResNet50 (resp. ResNet152)
with ImageNet1K pretraining reaches 80.11% (resp. 79.34%) top-1 accuracy. Moreover, to reach 90%
top-1 accuracy, a non-trivial performance, FIP-ALL signiﬁcantly accelerates the training by 2.14x
(28 vs. 60 epoch), 11.1x (11 vs. 122 epoch), 2.95x (19 vs. 56 epoch) over training-from-scratch, on
top of ResNet18, ResNet50, and ResNet152, respectively."
EPOCH,0.3168103448275862,"Therefore, our answer to ""Can image-pretrained model accelerate point-cloud training?"" is still: Yes.
The image-pretrained model can signiﬁcantly accelerate the training speed of point-cloud tasks."
EPOCH,0.31896551724137934,Under review as a conference paper at ICLR 2022
EPOCH,0.32112068965517243,Table 5: Texture-shape representation transferring experiment on ModelNet 3D Warehouse.
EPOCH,0.3232758620689655,"Method
Pretrained dataset
top-1 accuracy"
EPOCH,0.3254310344827586,"ResNet50 FIP-IO
ImageNet1K
81.20"
EPOCH,0.3275862068965517,"ResNet50 FIP-IO
Stylized-ImageNet
83.52"
EPOCH,0.3297413793103448,"BagNet17 FIP-IO
ImageNet1K
57.53
BagNet33 FIP-IO
ImageNet1K
68.40"
EPOCH,0.33189655172413796,Unit 161
EPOCH,0.33405172413793105,"Broden
Modelnet"
EPOCH,0.33620689655172414,Unit 508
EPOCH,0.33836206896551724,Unit 157
EPOCH,0.34051724137931033,Modelnet
EPOCH,0.3426724137931034,Unit 166
EPOCH,0.3448275862068966,Broden
EPOCH,0.34698275862068967,"Unit 190
Unit 322"
EPOCH,0.34913793103448276,"Figure 4: Network dissection of FIP-IO+BN. The visualization displays what the units are attending
to on both image dataset and ModelNet 3D Warehouse dataset."
DISCUSSION,0.35129310344827586,"5
DISCUSSION"
DISCUSSION,0.35344827586206895,"In this section, we attempt to shed light on why transferring image-pretrained models for point-cloud
understanding works. Inspired by recent related works (Geirhos et al., 2018; Brendel & Bethge, 2019;
He et al., 2015; 2016b; Bau et al., 2017), we explore this from the aspects of the network dissection,
texture-shape representation transferring, and feature distribution distance."
DISCUSSION,0.35560344827586204,"5.1
WHAT DO IMAGE-PRETRAINED MODELS TRANSFER TO POINT-CLOUD MODEL?"
DISCUSSION,0.3577586206896552,"Does the image-pretrained model transfer the visual concepts? Inspired by the network dissec-
tion for 2D Broden dataset (Bau et al., 2017), we also attempt to explore what the transferred units
are focus on in the ModelNet 3D Warehouse dataset. We present the visualization of FIP-IO+BN
pretrained on ImageNet1K. The visualization shows the most activated cases when the whole dataset
passes through each unit of the last model stage, as displayed in Figure 4. More visualization can
be found in Appendix 7. From the visualization alone, we do not get obvious cues of what visual
concepts are transferred between the two modalities. For example, unit 161 strongly activating to
computer screens in the Broden, yet it attends to cars and shelves in the ModelNet 3D Warehouse.
Surprisingly, we ﬁnd that the pretrained units are prone to cluster similar objects. In fact, such
clustering ability is an important cue of performing well on classiﬁcation tasks (Hartigan & Wong,
1979; Caron et al., 2021)."
DISCUSSION,0.3599137931034483,"Do pretrained-image models transfer shape or texture representation? Recent work (Geirhos
et al., 2018) proposed that models learn texture and shape from ImageNet. We follow this direction
to explore what pretrained-image mode transfers. For the experiment, we take two image-pretrained
models with either more shape or texture representation and compare their FIP-IO performance."
DISCUSSION,0.3620689655172414,"In order to force the pretrained models to acquire more shape representation, Geirhos et al. (2018)
stylizes the images in ImageNet into artwork style, such that the models trained on those images
are confused by variant textures, hence having stronger shape representation. We directly take the
pretrained ResNet50 on stylized ImageNet as the stronger shape representation model."
DISCUSSION,0.3642241379310345,Under review as a conference paper at ICLR 2022
DISCUSSION,0.36637931034482757,"To get a stronger texture representation model, we are inspired by BagNet (Brendel & Bethge, 2019).
By controlling the receptive ﬁeld, BagNet breaks the shape in an image and focuses more on the
texture information. Our experimental result is shown in Table 5. BagNet17 means the size of
attended patches is 17 × 17, and BagNet33 means the size of attended patches is 33 × 33. Note that
after inﬂating the BagNet17 and BagNet33, both architectures are the same as inﬂated ResNet50.
Besides, both BagNet17/33 and ResNet pretrained on stylized-ImageNet1K perform worse than the
original ResNet50 on ImageNet (Geirhos et al., 2018; Brendel & Bethge, 2019)."
DISCUSSION,0.36853448275862066,"We can observe that the ResNet50 FIP-IO on Stylized-ImageNet (with stronger shape representation)
outperforms the baseline ResNet50 FIP-IO on ImageNet1K over 2.32 points top-1 accuracy, while
both inﬂated BagNets perform dramatically worse than the baseline. This shows that the shape
representations are transferred."
DISCUSSION,0.3706896551724138,"To make clear what parts of the image are critical for the transfer, we further conduct experiments
using different image processing methods for pretraining. The results are reported in Appendix A.7."
DISCUSSION,0.3728448275862069,"5.2
WHY DOES FINETUNING BATCH NORMALIZATION HELP THE TRANSFERRING?"
DISCUSSION,0.375,"Our experiment in Figure 2 shows that ﬁnetuning BN, in addition to the input and output layer, can
greatly improve the transfer performance. It is interesting why such a small part of the network, in
terms of parameter size and FLOPs, can have a big impact."
DISCUSSION,0.3771551724137931,"We calculate the ﬁrst-wasserstein distance (FWD) (Rubner et al., 2000), a measurement of Gaussian
distribution distance, between the 2D image and 3D point-cloud feature distributions. For estimating
the 2D feature distribution, we pass the whole ImageNet1K dataset into the pretrained-image models,
collect the pre-activation features after each convolution layer, then sample 15,000 data points from
the element-wise distribution of collected features (we assume the pre-activation features present
Gaussian distribution). For 3D feature distribution, we use FIP-IO, FIP-IO+BN, and FIP-ALL to
conduct the same operation on the ModelNet 3D Warehouse dataset and also collect 15,000 data
points. We then calculate the FWD between the element-wise feature distribution of image-pretrained
model and each of the FIP-IO, FIP-IO+BN, FIP-ALL model."
DISCUSSION,0.3793103448275862,"We observe that the layer-wise average FWD between FIP-IO (point-cloud features) and image
model (image features) is 2.1 × 102 , yet after ﬁnetuning batch normalization layers, the distance is
dramatically reduced. Particularly, average FWD between FIP-IO+BN and image model is 0.27, and
FWD between FIP-ALL and image model is only 0.093. Complete FWD for each layer is shown in
Appendix Table 11. This suggests that batch normalization plays a critical role in transforming the
point-cloud representation to be closer to the image representation."
CONCLUSION,0.38146551724137934,"6
CONCLUSION"
CONCLUSION,0.38362068965517243,"In this work, we use ﬁnetuned-image-pretrained models (FIP) to explore the feasibility of transferring
image-pretrained models for point-cloud understanding and the beneﬁts of using image-pretrained
models on point-cloud tasks. We surprisingly discover that, with simply inﬂating a 2D pretrained
ConvNet and minimal ﬁnetuning — input, output, and optionally, batch normalization layer (FIP-IO
or FIP-IO+BN), FIP can achieve very competitive performance on 3D point-cloud classiﬁcation,
beating a wide range of point-cloud models that adopt a variety of tricks. Moreover, we ﬁnd that
when ﬁnetuning all the parameters of the pretrained models (FIP-ALL), the performance can be
signiﬁcantly improved on point-cloud classiﬁcation, indoor and outdoor scene segmentation. Fully
ﬁnetuned models generalizes to most of the popular point-cloud methods. We also ﬁnd that FIP-ALL
can improve the data efﬁciency on few-shot learning and accelerate the training speed by a large
margin. Additionally, we shed light on why image-pretrained models can be used for point-cloud
understanding from three aspects: network dissection, texture-shape representation transferring,
and feature distribution distance. Compared with previous works that seek improvements from
designing architectures and pretraining only on point-cloud modality, our work is not limited by the
architecture design and the small-scale point-cloud dataset. We believe that image pretraining is one
of the solutions to the bottleneck of point-cloud understanding and hope this direction can inspire the
research community."
CONCLUSION,0.3857758620689655,Under review as a conference paper at ICLR 2022
REFERENCES,0.3879310344827586,REFERENCES
REFERENCES,0.3900862068965517,"Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese. Joint 2d-3d-semantic data for indoor
scene understanding. arXiv preprint arXiv:1702.01105, 2017."
REFERENCES,0.3922413793103448,"Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. arXiv preprint arXiv:1906.00910, 2019."
REFERENCES,0.39439655172413796,"David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection:
Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 6541–6549, 2017."
REFERENCES,0.39655172413793105,"J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall. SemanticKITTI:
A Dataset for Semantic Scene Understanding of LiDAR Sequences. In Proc. of the IEEE/CVF
International Conf. on Computer Vision (ICCV), 2019."
REFERENCES,0.39870689655172414,"Alexandre Boulch, Bertrand Le Saux, and Nicolas Audebert. Unstructured point cloud semantic
labeling using deep segmentation networks. 3DOR, 2:7, 2017."
REFERENCES,0.40086206896551724,"Wieland Brendel and Matthias Bethge. Approximating cnns with bag-of-local-features models works
surprisingly well on imagenet. International Conference on Learning Representations, 2019. URL
https://openreview.net/pdf?id=SkfMWhAqYQ."
REFERENCES,0.40301724137931033,"Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush
Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for
autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 11621–11631, 2020."
REFERENCES,0.4051724137931034,"John Canny. A computational approach to edge detection. IEEE Transactions on pattern analysis
and machine intelligence, (6):679–698, 1986."
REFERENCES,0.4073275862068966,"Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of
image features on non-curated data. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 2959–2968, 2019."
REFERENCES,0.40948275862068967,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 2020."
REFERENCES,0.41163793103448276,"Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint
arXiv:2104.14294, 2021."
REFERENCES,0.41379310344827586,"Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics
dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
6299–6308, 2017."
REFERENCES,0.41594827586206895,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pp.
1597–1607. PMLR, 2020a."
REFERENCES,0.41810344827586204,"Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big
self-supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029,
2020b."
REFERENCES,0.4202586206896552,"Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020c."
REFERENCES,0.4224137931034483,"Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski
convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3075–3084, 2019."
REFERENCES,0.4245689655172414,"Angela Dai and Matthias Nießner. 3dmv: Joint 3d-multi-view prediction for 3d semantic scene
segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pp.
452–468, 2018."
REFERENCES,0.4267241379310345,Under review as a conference paper at ICLR 2022
REFERENCES,0.42887931034482757,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.43103448275862066,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.4331896551724138,"Di Feng, Yiyang Zhou, Chenfeng Xu, Masayoshi Tomizuka, and Wei Zhan. A simple and ef-
ﬁcient multi-task network for 3d object detection and road understanding.
arXiv preprint
arXiv:2103.04056, 2021."
REFERENCES,0.4353448275862069,"A. Geiger, P. Lenz, and R. Urtasun. Are we ready for Autonomous Driving? The KITTI Vision
Benchmark Suite. In Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),
pp. 3354–3361, 2012."
REFERENCES,0.4375,"Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves
accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018."
REFERENCES,0.4396551724137931,"Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate
object detection and semantic segmentation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 580–587, 2014."
REFERENCES,0.4418103448275862,"Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. Revisiting point cloud shape
classiﬁcation with a simple and effective baseline. arXiv preprint arXiv:2106.05304, 2021a."
REFERENCES,0.44396551724137934,"Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat
Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual
features in the wild. arXiv preprint arXiv:2103.01988, 2021b."
REFERENCES,0.44612068965517243,"John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm. Journal
of the royal statistical society. series c (applied statistics), 28(1):100–108, 1979."
REFERENCES,0.4482758620689655,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.4504310344827586,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016a."
REFERENCES,0.4525862068965517,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630–645. Springer, 2016b."
REFERENCES,0.4547413793103448,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729–9738, 2020."
REFERENCES,0.45689655172413796,"Olivier Henaff. Data-efﬁcient image recognition with contrastive predictive coding. In International
Conference on Machine Learning, pp. 4182–4192. PMLR, 2020."
REFERENCES,0.45905172413793105,"R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018."
REFERENCES,0.46120689655172414,"Ji Hou, Benjamin Graham, Matthias Nießner, and Saining Xie. Exploring data-efﬁcient 3d scene
understanding with contrastive scene contexts. arXiv preprint arXiv:2012.09165, 2020."
REFERENCES,0.46336206896551724,"Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Pointwise convolutional neural networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 984–993,
2018."
REFERENCES,0.46551724137931033,Under review as a conference paper at ICLR 2022
REFERENCES,0.4676724137931034,"Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks: A
survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020."
REFERENCES,0.4698275862068966,"Hirokatsu Kataoka, Kazushige Okayasu, Asato Matsumoto, Eisuke Yamagata, Ryosuke Yamada,
Nakamasa Inoue, Akio Nakamura, and Yutaka Satoh. Pre-training without natural images. In
Proceedings of the Asian Conference on Computer Vision, 2020."
REFERENCES,0.47198275862068967,"Roman Klokov and Victor Lempitsky. Escape from cells: Deep kd-networks for the recognition of
3d point cloud models. In Proceedings of the IEEE International Conference on Computer Vision,
pp. 863–872, 2017."
REFERENCES,0.47413793103448276,"Artem Komarichev, Zichun Zhong, and Jing Hua. A-cnn: Annularly convolutional neural networks
on point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 7421–7430, 2019."
REFERENCES,0.47629310344827586,"Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Point-
pillars: Fast encoders for object detection from point clouds. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 12697–12705, 2019."
REFERENCES,0.47844827586206895,"Felix Järemo Lawin, Martin Danelljan, Patrik Tosteberg, Goutam Bhat, Fahad Shahbaz Khan, and
Michael Felsberg. Deep projective 3d semantic segmentation. In International Conference on
Computer Analysis of Images and Patterns, pp. 95–107. Springer, 2017."
REFERENCES,0.48060344827586204,"Dogyoon Lee, Jaeha Lee, Junhyeop Lee, Hyeongmin Lee, Minhyeok Lee, Sungmin Woo, and
Sangyoun Lee. Regularization strategy for point cloud via rigidly mixed sample. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15900–15909, 2021."
REFERENCES,0.4827586206896552,"Jiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self-organizing network for point cloud analysis.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9397–9406,
2018a."
REFERENCES,0.4849137931034483,"Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolu-
tion on χ-transformed points. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, pp. 828–838, 2018b."
REFERENCES,0.4870689655172414,"Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolu-
tional neural networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 806–814, 2015."
REFERENCES,0.4892241379310345,"Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming Xiang, and Chunhong Pan. Densepoint:
Learning densely contextual representation for efﬁcient point cloud processing. In Proceedings of
the IEEE/CVF International Conference on Computer Vision, pp. 5239–5248, 2019."
REFERENCES,0.49137931034482757,"Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang, Hung-Ting Su, Zhe-Yu Liu, Chin-Tang Chen,
Ching-Yu Tseng, and Winston H Hsu. Learning from 2d: Pixel-to-point knowledge transfer for 3d
pretraining. arXiv preprint arXiv:2104.04687, 2021a."
REFERENCES,0.49353448275862066,"Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong. A closer look at local aggregation operators
in point cloud analysis. In European Conference on Computer Vision, pp. 326–342. Springer, 2020."
REFERENCES,0.4956896551724138,"Zhengzhe Liu, Xiaojuan Qi, and Chi-Wing Fu. 3d-to-2d distillation for indoor scene parsing. arXiv
preprint arXiv:2104.02243, 2021b."
REFERENCES,0.4978448275862069,"Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal
computation engines. arXiv preprint arXiv:2103.05247, 2021."
REFERENCES,0.5,"Andres Milioto, Ignacio Vizzo, Jens Behley, and Cyrill Stachniss. Rangenet++: Fast and accurate
lidar semantic segmentation. In 2019 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pp. 4213–4220. IEEE, 2019."
REFERENCES,0.5021551724137931,"François Pomerleau, Francis Colas, and Roland Siegwart. A review of point cloud registration
algorithms for mobile robotics. Foundations and Trends in Robotics, 4(1):1–104, 2015."
REFERENCES,0.5043103448275862,Under review as a conference paper at ICLR 2022
REFERENCES,0.5064655172413793,"Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets
for 3d classiﬁcation and segmentation, 2016. URL http://arxiv.org/abs/1612.00593.
cite arxiv:1612.00593."
REFERENCES,0.5086206896551724,"Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning
on point sets in a metric space. arXiv preprint arXiv:1706.02413, 2017."
REFERENCES,0.5107758620689655,"Shi Qiu, Saeed Anwar, and Nick Barnes. Dense-resolution network for point cloud classiﬁcation and
segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision, pp. 3813–3822, 2021."
REFERENCES,0.5129310344827587,"Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for
the masses, 2021."
REFERENCES,0.5150862068965517,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234–241. Springer, 2015."
REFERENCES,0.5172413793103449,"Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric for
image retrieval. International journal of computer vision, 40(2):99–121, 2000."
REFERENCES,0.5193965517241379,"Hongming Shan, Yi Zhang, Qingsong Yang, Uwe Kruger, Mannudeep K Kalra, Ling Sun, Wenxiang
Cong, and Ge Wang. 3-d convolutional encoder-decoder network for low-dose ct via transfer
learning from a 2-d trained network. IEEE transactions on medical imaging, 37(6):1522–1534,
2018."
REFERENCES,0.521551724137931,"Baoguang Shi, Song Bai, Zhichao Zhou, and Xiang Bai. Deeppano: Deep panoramic representation
for 3-d shape recognition. IEEE Signal Processing Letters, 22(12):2339–2343, 2015. doi: 10.
1109/LSP.2015.2480802."
REFERENCES,0.5237068965517241,"Sketchup. 3d modeling online free|3d warehouse models. https://3dwarehouse.sketchup.
com, 2021."
REFERENCES,0.5258620689655172,"Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional
neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on
computer vision, pp. 945–953, 2015."
REFERENCES,0.5280172413793104,"Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for
human pose estimation. In CVPR, 2019."
REFERENCES,0.5301724137931034,"Kamrul Hasan Talukder and Koichi Harada. Haar wavelet based approach for image compression
and quality assessment of compressed image. arXiv preprint arXiv:1010.4084, 2010."
REFERENCES,0.5323275862068966,"Haotian* Tang, Zhijian* Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han.
Searching efﬁcient 3d architectures with sparse point-voxel convolution. In European Conference
on Computer Vision, 2020."
REFERENCES,0.5344827586206896,"Carlo Tomasi and Roberto Manduchi.
Bilateral ﬁltering for gray and color images.
In Sixth
international conference on computer vision (IEEE Cat. No. 98CH36271), pp. 839–846. IEEE,
1998."
REFERENCES,0.5366379310344828,"Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. O-cnn: Octree-based
convolutional neural networks for 3d shape analysis. ACM Transactions on Graphics (TOG), 36
(4):1–11, 2017."
REFERENCES,0.5387931034482759,"Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon.
Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5):1–12,
2019."
REFERENCES,0.540948275862069,"Zining Wang, Wei Zhan, and Masayoshi Tomizuka. Fusing bird’s eye view lidar point cloud and
front view camera image for 3d object detection. In 2018 IEEE Intelligent Vehicles Symposium
(IV), pp. 1–6. IEEE, 2018."
REFERENCES,0.5431034482758621,Under review as a conference paper at ICLR 2022
REFERENCES,0.5452586206896551,"Bichen Wu, Alvin Wan, Xiangyu Yue, and Kurt Keutzer. Squeezeseg: Convolutional neural nets with
recurrent crf for real-time road-object segmentation from 3d lidar point cloud. In ICRA, 2018."
REFERENCES,0.5474137931034483,"Bichen Wu, Xuanyu Zhou, Sicheng Zhao, Xiangyu Yue, and Kurt Keutzer. Squeezesegv2: Improved
model structure and unsupervised domain adaptation for road-object segmentation from a lidar
point cloud. In ICRA, 2019."
REFERENCES,0.5495689655172413,"Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2015."
REFERENCES,0.5517241379310345,"Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast:
Unsupervised pre-training for 3d point cloud understanding. In European Conference on Computer
Vision, pp. 574–591. Springer, 2020."
REFERENCES,0.5538793103448276,"Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt Keutzer, and Masayoshi
Tomizuka. Squeezesegv3: Spatially-adaptive convolution for efﬁcient point-cloud segmentation.
In European Conference on Computer Vision, pp. 1–19. Springer, 2020."
REFERENCES,0.5560344827586207,"Chenfeng Xu, Bohan Zhai, Bichen Wu, Tian Li, Wei Zhan, Peter Vajda, Kurt Keutzer, and Masayoshi
Tomizuka. You only group once: Efﬁcient point-cloud processing with token representation and
relation inference module. arXiv preprint arXiv:2103.09975, 2021."
REFERENCES,0.5581896551724138,"Xun Xu and Gim Hee Lee. Weakly supervised semantic point cloud segmentation: Towards 10x fewer
labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 13706–13715, 2020."
REFERENCES,0.5603448275862069,"Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection. Sensors, 18
(10):3337, 2018."
REFERENCES,0.5625,"Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-time 3d object detection from point
clouds. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp.
7652–7660, 2018."
REFERENCES,0.5646551724137931,"Xiangyu Yue, Bichen Wu, Sanjit A Seshia, Kurt Keutzer, and Alberto L Sangiovanni-Vincentelli. A
lidar point cloud generator: from a virtual world to autonomous driving. In Proceedings of the
2018 ACM on International Conference on Multimedia Retrieval, pp. 458–464, 2018."
REFERENCES,0.5668103448275862,"Jinlai Zhang, Lyujie Chen, Bo Ouyang, Binbin Liu, Jihong Zhu, Yujing Chen, Yanmei Meng, and
Danfeng Wu. Pointcutmix: Regularization strategy for point cloud classiﬁcation. arXiv preprint
arXiv:2101.01461, 2021a."
REFERENCES,0.5689655172413793,"Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan Misra. Self-supervised pretraining of 3d
features on any point-cloud. arXiv preprint arXiv:2101.02691, 2021b."
REFERENCES,0.5711206896551724,"Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16259–16268,
2021."
REFERENCES,0.5732758620689655,"Hui Zhou, Xinge Zhu, Xiao Song, Yuexin Ma, Zhe Wang, Hongsheng Li, and Dahua Lin. Cylin-
der3d: An effective 3d framework for driving-scene lidar semantic segmentation. arXiv preprint
arXiv:2008.01550, 2020."
REFERENCES,0.5754310344827587,Under review as a conference paper at ICLR 2022
REFERENCES,0.5775862068965517,"A
APPENDIX"
REFERENCES,0.5797413793103449,"A.1
IMPLEMENTATION DETAIL."
REFERENCES,0.5818965517241379,"Our experiments are conducted on ModelNet 3D Warehouse, S3DIS, and SemanticKITTI datasets.
For the ModelNet 3D Warehouse dataset, we train all models on the train set and evaluate on the
validation set. For the S3DIS, we train all models on area 1, 2, 3, 4, 6 and evaluate on area 5. For the
SemanticKITTI dataset, we train all models on splits 00-10 except 08 which is used for evaluation.
For each of the dataset, all ResNet series models use the same training scheme, and all experiments
are implemented with PyTorch."
REFERENCES,0.584051724137931,"When training on the ModelNet 3D Warehouse dataset, coordinates of point-clouds are randomly
scaled, translated, and jittered. We use SGD optimizer with momentum 0.9, weight-decay 10−4, and
initial learning rate 0.1 with cosine learning rate scheduler. Each mini batch is set to 32, and models
are trained for 300 epochs. For both training and inference phase, we only utilize x, y, z coordinates
without other features and set voxel size to 0.05. The experiments for ModelNet 3D Warehouse are
all conducted on a Titan RTX GPU."
REFERENCES,0.5862068965517241,"When training on the S3DIS dataset, we concatenate all subparts of an indoor scene to train and
validate on. Along x, y directions, scenes are applied horizontal ﬂip randomly. RGB features are
randomly jittered, translated, and auto contrasted. Finally, we normalize and clip point-clouds. We
set voxel size to 0.05, use SGD optimizer with momentum 0.9, weight-decay 10−4, and initialize
learning rate to 0.1 with polynomial learning rate scheduler. Each mini batch is set to 3, and models
are trained for 400 epochs on 2 Titan RTX GPUs."
REFERENCES,0.5883620689655172,"When training on the SemanticKITTI dataset, coordinates of each point-cloud are randomly scaled
and rotated. We use SGD optimizer with momentum 0.9, weight-decay 10−4, and initial learning
rate 0.24 with cosine warmup learning rate scheduler. Each mini batch is set to 2, and models are
trained for 15 epochs on 4 Titan RTX GPUs. For both training and inference phase, we utilize x, y, z
coordinates as well as intensity feature and set voxel size to 0.05."
REFERENCES,0.5905172413793104,"Most of our pretrained models come from open-sources, 1 2 3 4 5 6 , so we do not need to take time
and computational resources for pretraining. We use torchsparse 7 to produce sparse 3D convolutions."
REFERENCES,0.5926724137931034,"Details of Section 4.1. In this section, we take the ResNet architecture, inﬂate the pretrained models
of different image datasets, and add linear input and output layers as shown in Section A.9. The
ResNet50 pretrained on ImageNet1K is directly taken from PyTorch. We use the same training
recipe provided by PyTorch to train the ResNet50 on Tiny-ImageNet. The pretrained ResNet50 on
ImageNet21K comes from Ridnik et al. (2021)."
REFERENCES,0.5948275862068966,"Details of Section 4.2, Section 4.3 and 4.4. In this section, the ResNet pretrained models are taken
from the same sources as illustrated above."
REFERENCES,0.5969827586206896,"For PointNet++ pretraining on ImageNet1K, we utilize the PointNet++ SSG version (Qi et al., 2017).
We break the image into pixels and regard the group of pixels as a point-cloud with coordinates of x,
y positions in the original image and appending z = 1 to all pixels. Then, we set center sampling
number to 1024 and 256 for ﬁrst and second stage, and the radius is set into 8 and 64, respectively.
For each center point, we query 64 neighbouring points. The training recipe is also provided by
PyTorch."
REFERENCES,0.5991379310344828,"For ViT models, we directly take the pretrained weights from Dosovitskiy et al. (2020). To apply it
on ModelNet 3D Warehouse, we sample 256 centers and group 64 nearby points, regarding these
as ""point-cloud patches"". Then, we use a linear embedding to project the point-cloud patches into a
sequence, and ViT processes them same as image patches. Except the linear embedding and the ﬁnal"
REFERENCES,0.6012931034482759,"1https://pytorch.org/vision/stable/models.html
2https://github.com/Alibaba-MIIL/ImageNet21K
3https://github.com/hirokatsukataoka16/FractalDB-Pretrained-ResNet-PyTorch
4https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/pytorch-v1.1
5https://github.com/rgeirhos/Stylized-ImageNet
6https://github.com/wielandbrendel/bag-of-local-features-models
7https://github.com/mit-han-lab/torchsparse"
REFERENCES,0.603448275862069,Under review as a conference paper at ICLR 2022
REFERENCES,0.6056034482758621,"Table 6: ResNet50 results (evaluated on ModelNet 3D Warehouse) of ﬁntuning the mean and variance
in batch normalization layers on different datasets. IO indicates ﬁnetuning input and output layer,
IOms indicates updating input, output, mean and variance, IOmsWb indicates ﬁnetuning input, output
and the whole BN."
REFERENCES,0.6077586206896551,"Layers
Tiny-ImageNet
ImageNet1K
ImageNet21K
FractalDB1K
FractalDB10K"
REFERENCES,0.6099137931034483,"IO
67.666
81.199
73.744
83.347
80.105"
REFERENCES,0.6120689655172413,"IOms
83.793
82.942
84.076
72.326
79.66"
REFERENCES,0.6142241379310345,"IOmsWb
89.992
89.87
90.721
89.263
89.344"
REFERENCES,0.6163793103448276,"From scratch
90.316
90.316
90.316
90.316
90.316"
REFERENCES,0.6185344827586207,"Table 7: ResNet18, 50, 152 results (evaluated on ModelNet 3D Warehouse) of ﬁnetuing the mean
and variance in batch normalization layers."
REFERENCES,0.6206896551724138,"Layers
ResNet18
ResNet50
ResNet152"
REFERENCES,0.6228448275862069,"IO
71.029
81.199
64.627"
REFERENCES,0.625,"IOmv
81.888
82.942
82.658"
REFERENCES,0.6271551724137931,"IOmvWb
88.574
89.87
90.438"
REFERENCES,0.6293103448275862,"From Scratch
90.397
90.316
90.276"
REFERENCES,0.6314655172413793,"output classiﬁer, all the models are kept same as the origin version. For the experiments on S3DIS
and SemanticKITTI, the architecture detail of ResNet18 is shown in A.5 listing 2."
REFERENCES,0.6336206896551724,"For SimpleView model, all the experiment settings are the same as Goyal et al. (2021a). The only
difference is whether to use the pretrained ResNet18. For HRNetV2-W48, we directly use the
ImageNet1K and Cityscape pretrained models from (Sun et al., 2019)."
REFERENCES,0.6357758620689655,"We conduct three trials on the few-shot experiments. For each trial, we change the random seed but
keep all the other settings the same. To plot the training speed curve, we directly use the training log
without any other changes, such as smoothing."
REFERENCES,0.6379310344827587,"A.2
FINETUNING THE MEAN AND VARIANCE OF BATCH NORMALIZATION."
REFERENCES,0.6400862068965517,"For the ﬁrst group of experiments, ResNet50 FIP either has IO or IO+BN ﬁnetuned. In addition to
these two experimental settings, we also investigate ﬁntuning input, output layers, and mean, variance
of normalization layers, while ﬁxing the convolution layer weights, normalization layer weights, and
bias. The full experiment results with this extra setting is reported in Table 6 and 7. We can observe
that compared with only ﬁnetuning input and output layers, updating mean and variance can also
largely improve the performance of point-cloud recognition. As suggested in Section 5.2 in the main
paper, updating mean and variance is to push the FIP models to generate point-cloud representation
that is similar to image representation."
REFERENCES,0.6422413793103449,"A.3
ABLATION STUDY OF INFLATING TOWARDS DIFFERENT DIRECTIONS."
REFERENCES,0.6443965517241379,"We conduct experiments of inﬂating ﬁlters along different directions, the illustration ﬁgure is shown
in Figure 5, and the results are shown in Table 8. We ﬁnd that the performance is different when
using different inﬂation methods. In particular, with ResNet50 pretrained on ImageNet1K, inﬂating
along x axis and the y axis leads to better performance compared with inﬂating along z axis for both
FIP-IO and FIP-IO+BN. More importantly, the minimally ﬁnetuned FIP-IO+BN with inﬂating along
the x and y axis even surpasses the training-from-scratch."
REFERENCES,0.646551724137931,"A.4
ABLATION STUDY OF LOADING DIFFERENT STAGES OF IMAGE-PRETRAINED MODEL."
REFERENCES,0.6487068965517241,"We investigate the effect of loading different subset of stages. The results are shown in Table 9. In
detail, we load the pretrained weights partially while keeping the other weights randomly initialized."
REFERENCES,0.6508620689655172,Under review as a conference paper at ICLR 2022
D FILTER,0.6530172413793104,"2D Filter
3D Filter"
D FILTER,0.6551724137931034,"Z axis
Y axis
X axis"
D FILTER,0.6573275862068966,Figure 5: Visualization of ﬁlter inﬂation along different axis.
D FILTER,0.6594827586206896,"Table 8: ModelNet 3D Warehouse results (Top-1 accuracy) of partially ﬁnetuning ResNet50 pretrained
on ImageNet1K with inﬂation along the x, y, z axis."
D FILTER,0.6616379310344828,"Method
x axis
y axis
z axis"
D FILTER,0.6637931034482759,"FIP-IO
82.17
81.73
81.20
FIP-IO+BN
90.44
90.84
89.87"
D FILTER,0.665948275862069,"From Scratch
90.32"
D FILTER,0.6681034482758621,"Table 9: ModelNet 3D Warehouse results (Top-1 accuracy) of ﬁnetuning ResNet50 pretrained on
ImageNet1K with only subset of stages loaded."
D FILTER,0.6702586206896551,"Loaded Stages
1
1 2
1 2 3
1 2 3 4
2 3 4
3 4
4"
D FILTER,0.6724137931034483,"FIP-ALL
89.91
90.36
90.64
90.92
91.09
90.19
89.99"
D FILTER,0.6745689655172413,"From Scratch
90.32"
D FILTER,0.6767241379310345,"We observe that excluding the weights of the ﬁrst stage achieves the best performance, bringing 0.77
points improvement."
D FILTER,0.6788793103448276,"A.5
AN INTUITION OF WHY THE IMAGE-PRETRAINED WEIGHTS CAN BE USED FOR
POINT-CLOUD RECOGNITION."
D FILTER,0.6810344827586207,"In this subsection, we try to give an intuition of why the weights pretrained on images can be used
for point-cloud tasks from the aspect of local feature projection. Let’s only look at the one time
convolution on a local point-cloud feature Xlocal ∈RCin×Cout×K3. In fact, a ﬁlter inﬂation can
be represented as linear projection, given by W2d ⊗T = W3d , where W2d ∈RCin×Cout×K2"
D FILTER,0.6831896551724138,"is pretrained 2D weight, W3d ∈RCin×Cout×K3 is the transformed 3D weight from W2d, and
T ∈RK2×K3 is the projection matrix. Therefore, for a local 3D feature Xlocal ∈RCin×Cout×K3,
the convolution can be formed as W3d ⊗Xlocal = W2d ⊗T ⊗Xlocal = W2d ⊗Xlocal−2d, where
Xlocal−2d ∈RCin×Cout×K2 is the 2D feature projected from Xlocal. Essentially, applying the
inﬂated ﬁlters on 3D point-clouds is equal to the local projection of 3D point-clouds to 2D then
applying the pretrained 2D ﬁlters. A visual explanation is in Figure 6."
"D FILTER
LOCAL VOXELS",0.6853448275862069,"3D Filter
Local Voxels
2D Filter
Default T
Local Voxels
Projected Voxels
2D Filter"
"D FILTER
LOCAL VOXELS",0.6875,"Linear Projection
3D Convolution
2D Convolution on Projected Voxels"
"D FILTER
LOCAL VOXELS",0.6896551724137931,Figure 6: Visualization of ﬁlter inﬂation explained with linear projection matrix.
"D FILTER
LOCAL VOXELS",0.6918103448275862,Under review as a conference paper at ICLR 2022
"D FILTER
LOCAL VOXELS",0.6939655172413793,"Table 10: ModelNet 3D Warehouse results (Top-1 accuracy) of ﬁnetuning ResNet18 pretrained on
different Tiny-ImageNet."
"D FILTER
LOCAL VOXELS",0.6961206896551724,"Method
Original image
Canny edge
detection
Bilateral ﬁlters
Haar
transformation"
"D FILTER
LOCAL VOXELS",0.6982758620689655,"FIP-IO
78.89
78.85 (-0.04)
78.97 (+0.08)
78.49 (-0.40)"
"D FILTER
LOCAL VOXELS",0.7004310344827587,"Method
Shufﬂe patch 56
Shufﬂe patch 28
Shufﬂe patch 14
Shufﬂe patch 7"
"D FILTER
LOCAL VOXELS",0.7025862068965517,"FIP-IO
78.44(-0.45)
78.00 (-0.89)
76.21 (-2.68)
74.60 (-4.29)"
"D FILTER
LOCAL VOXELS",0.7047413793103449,"A.6
MORE VISUALIZATION OF NETWORK DISSECTION."
"D FILTER
LOCAL VOXELS",0.7068965517241379,"We present more visualization results based on the technique of network dissection, as shown in
Figure 7. We can observe that for most cases, although there is no obvious visual concept transferred,
the pretrained ﬁlters are prone to cluster similar objects on ModelNet 3D Warehouse dataset."
"D FILTER
LOCAL VOXELS",0.709051724137931,"A.7
USING DIFFERENT IMAGE PROCESSING METHODS FOR PRETRAINING."
"D FILTER
LOCAL VOXELS",0.7112068965517241,"To further investigate the image-to-point-cloud transfer in a more rigorous manner, we design a new
set of experiments and report them in Appendix A.7 in the paper revision. The basic idea is that we
perform a transformation (such as a low-pass ﬁlter) on the original image to destroy certain properties
(high frequency components). Then, we train a ResNet18 on the transformed images, then transfer to
point-cloud recognition. This way, we can better attribute which parts of the images are useful when
transferred to point-cloud recognition. We perform the following transformations. Implementation
code is taken from external sources8 9 10."
"D FILTER
LOCAL VOXELS",0.7133620689655172,"1) Canny edge detection (Canny, 1986) transforms the images into high-frequency edge maps,
discarding the low-frequency such as the surface of objects;"
"D FILTER
LOCAL VOXELS",0.7155172413793104,"2) Bilateral ﬁlters (Tomasi & Manduchi, 1998) preserve the low-frequency properties of images while
ﬁltering out the high-frequency noise. Note that the high-frequency edges are still preserved and even
sharpened;"
"D FILTER
LOCAL VOXELS",0.7176724137931034,"3) Haar wavelet image compression (Talukder & Harada, 2010). The wavelet transform divides the
information of an image into approximation and detailed sub-signals. If the detail is small, then it
would be thrown;"
"D FILTER
LOCAL VOXELS",0.7198275862068966,"4) Patch shufﬂing divides an image into patches with different patch sizes, shufﬂe the patches, and
rearrange them into a new image. This transformation preserves the statistics of the images, while
destroying the original order of the pixels. “Shufﬂe patch 56” means the patch size is 56 × 56."
"D FILTER
LOCAL VOXELS",0.7219827586206896,"The results are reported in the Table 10. Speciﬁcally, the edge images from canny edge detection
are dramatically different from original images while keeping minimal performance drop. The
bilateral ﬁlters discard the high-frequency noise, sharpening the high-frequency edge lead to the best
performance, even outperforming the result of ﬁnetuning from the original image pretraining. As
details of images are thrown (haar transformation), the performance slightly drops. On the other hand,
shufﬂing the patches results in worse performance as the patch size gradually becomes small, which
indicates that the order of the pixels are important to the transfer."
"D FILTER
LOCAL VOXELS",0.7241379310344828,"Therefore, we may infer that the structured (ordered) edges are important to the transfer."
"D FILTER
LOCAL VOXELS",0.7262931034482759,"A.8
DETAILS OF FIRST-WASSERSTEIN DISTANCE ON RESNET18."
"D FILTER
LOCAL VOXELS",0.728448275862069,"We list all the results in each layer of ResNet18, as shown in Table 11. We can observe that for each
layer, the ﬁrst-wasserstein distance is largely reduced when ﬁnetuning more parameters."
"D FILTER
LOCAL VOXELS",0.7306034482758621,"8https://www.geeksforgeeks.org/implement-canny-edge-detector-in-python-using-opencv/
9https://docs.opencv.org/3.4/d4/d13/tutorial_py_ﬁltering.html
10https://medium.com/@digitalpadm/image-compression-haar-wavelet-transform-5d7be3408aa"
"D FILTER
LOCAL VOXELS",0.7327586206896551,Under review as a conference paper at ICLR 2022
"D FILTER
LOCAL VOXELS",0.7349137931034483,"Table 11: First-wasserstein distance between image model features and different 3D model features
of all 16 layers in ResNet18"
"D FILTER
LOCAL VOXELS",0.7370689655172413,"Image Model
FIP/IO
FIP/IO-BN
FIP/ALL
Training-from-
scratch"
"D FILTER
LOCAL VOXELS",0.7392241379310345,"Average
2.1 × 102
0.27
0.093
0.15"
"D FILTER
LOCAL VOXELS",0.7413793103448276,"Layer 1
1.9
0.2
0.094
0.12"
"D FILTER
LOCAL VOXELS",0.7435344827586207,"Layer 2
1.1
0.064
0.11
0.048"
"D FILTER
LOCAL VOXELS",0.7456896551724138,"Layer 3
1.3
0.24
0.068
0.095"
"D FILTER
LOCAL VOXELS",0.7478448275862069,"Layer 4
2.6
0.14
0.051
0.077"
"D FILTER
LOCAL VOXELS",0.75,"Layer 5
1.1
0.23
0.039
0.082"
"D FILTER
LOCAL VOXELS",0.7521551724137931,"Layer 6
3
0.21
0.025
0.063"
"D FILTER
LOCAL VOXELS",0.7543103448275862,"Layer 7
5.4
0.29
0.05
0.15"
"D FILTER
LOCAL VOXELS",0.7564655172413793,"Layer 8
13
0.26
0.051
0.097"
"D FILTER
LOCAL VOXELS",0.7586206896551724,"Layer 9
12
0.28
0.04
0.095"
"D FILTER
LOCAL VOXELS",0.7607758620689655,"Layer 10
26
0.2
0.077
0.13"
"D FILTER
LOCAL VOXELS",0.7629310344827587,"Layer 11
54
0.27
0.032
0.13"
"D FILTER
LOCAL VOXELS",0.7650862068965517,"Layer 12
2.2 × 102
0.25
0.076
0.094"
"D FILTER
LOCAL VOXELS",0.7672413793103449,"Layer 13
1.9 × 102
0.29
0.028
0.11"
"D FILTER
LOCAL VOXELS",0.7693965517241379,"Layer 14
7.7 × 102
0.16
0.075
0.041"
"D FILTER
LOCAL VOXELS",0.771551724137931,"Layer 15
9.5 × 102
0.29
0.081
0.088"
"D FILTER
LOCAL VOXELS",0.7737068965517241,"Layer 16
1.2 × 103
0.89
0.59
1"
"D FILTER
LOCAL VOXELS",0.7758620689655172,"A.9
DETAILS OF USED ARCHITECTURES."
"D FILTER
LOCAL VOXELS",0.7780172413793104,1 Class 3DRes_cls(nn.Module):
"D FILTER
LOCAL VOXELS",0.7801724137931034,"2
def __init__(self, res_block):"
"D FILTER
LOCAL VOXELS",0.7823275862068966,"3
super().__init__() # res_block means the residual block as same
as the conventional ResNet."
"D FILTER
LOCAL VOXELS",0.7844827586206896,"4
self.input_layer = nn.Sequential("
"D FILTER
LOCAL VOXELS",0.7866379310344828,"5
sparse_conv3d(input_dim, layer1_Idim, k=3, s=1),"
"D FILTER
LOCAL VOXELS",0.7887931034482759,"6
sparse_bn(layer1_Idim)) 7"
"D FILTER
LOCAL VOXELS",0.790948275862069,"8
self.layer1 = inflated_resnet_layer1(res_block, layer1_Idim,
layer1_Odim)"
"D FILTER
LOCAL VOXELS",0.7931034482758621,"9
self.layer2 = inflated_resnet_layer2(res_block, layer2_Idim,
layer2_Odim)"
"D FILTER
LOCAL VOXELS",0.7952586206896551,"10
self.layer3 = inflated_resnet_layer3(res_block, layer3_Idim,
layer3_Odim)"
"D FILTER
LOCAL VOXELS",0.7974137931034483,"11
self.layer4 = inflated_resnet_layer4(res_block, layer4_Idim,
layer4_Odim) 12"
"D FILTER
LOCAL VOXELS",0.7995689655172413,"13
self.output_layer = nn.Sequential("
"D FILTER
LOCAL VOXELS",0.8017241379310345,"14
global_average_pooling,"
"D FILTER
LOCAL VOXELS",0.8038793103448276,"15
nn.Linear(layer4_Odim, class_num),"
"D FILTER
LOCAL VOXELS",0.8060344827586207,"16
nn.bn(class_num)) 17"
"D FILTER
LOCAL VOXELS",0.8081896551724138,"18
def forward(self, x):"
"D FILTER
LOCAL VOXELS",0.8103448275862069,"19
x = self.input_layer(x)"
"D FILTER
LOCAL VOXELS",0.8125,"20
x = self.layer1(x)"
"D FILTER
LOCAL VOXELS",0.8146551724137931,"21
x = self.layer2(x)"
"D FILTER
LOCAL VOXELS",0.8168103448275862,"22
x = self.layer3(x)"
"D FILTER
LOCAL VOXELS",0.8189655172413793,"23
x = self.layer4(x)"
"D FILTER
LOCAL VOXELS",0.8211206896551724,Under review as a conference paper at ICLR 2022
"D FILTER
LOCAL VOXELS",0.8232758620689655,"24
return self.output_layer(x)
Listing 1: Pseudo code of inﬂated ResNet with linear input and output for classiﬁcation"
"D FILTER
LOCAL VOXELS",0.8254310344827587,1 Class 3DRes_seg(nn.Module):
"D FILTER
LOCAL VOXELS",0.8275862068965517,"2
def __init__(self, res_block):"
"D FILTER
LOCAL VOXELS",0.8297413793103449,"3
super().__init__() # res_block means the residual block as same
as the conventional ResNet."
"D FILTER
LOCAL VOXELS",0.8318965517241379,"4
self.input_layer = nn.Sequential("
"D FILTER
LOCAL VOXELS",0.834051724137931,"5
sparse_conv3d(input_dim, layer1_Idim, k=3, s=1),"
"D FILTER
LOCAL VOXELS",0.8362068965517241,"6
sparse_bn(layer1_Idim),"
"D FILTER
LOCAL VOXELS",0.8383620689655172,"7
sparse_ReLU(True),"
"D FILTER
LOCAL VOXELS",0.8405172413793104,"8
sparse_conv3d(layer1_Idim, layer1_Idim, k=3, s=1),"
"D FILTER
LOCAL VOXELS",0.8426724137931034,"9
sparse_bn(layer1_Idim),"
"D FILTER
LOCAL VOXELS",0.8448275862068966,"10
sparse_ReLU(True),"
"D FILTER
LOCAL VOXELS",0.8469827586206896,"11
sparse_conv3d(layer1_Idim, layer1_Idim, k=3, s=2),"
"D FILTER
LOCAL VOXELS",0.8491379310344828,"12
sparse_bn(layer1_Idim),"
"D FILTER
LOCAL VOXELS",0.8512931034482759,"13
sparse_ReLU(True)) 14"
"D FILTER
LOCAL VOXELS",0.853448275862069,"15
self.layer1 = inflated_resnet_layer1(res_block, layer1_Idim,
layer1_Odim)"
"D FILTER
LOCAL VOXELS",0.8556034482758621,"16
self.layer2 = inflated_resnet_layer2(res_block, layer2_Idim,
layer2_Odim)"
"D FILTER
LOCAL VOXELS",0.8577586206896551,"17
self.layer3 = inflated_resnet_layer3(res_block, layer3_Idim,
layer3_Odim)"
"D FILTER
LOCAL VOXELS",0.8599137931034483,"18
self.layer4 = inflated_resnet_layer4(res_block, layer4_Idim,
layer4_Odim) 19"
"D FILTER
LOCAL VOXELS",0.8620689655172413,"20
self.up1 = sparse_deconv(layer4_Odim, layer4_Odim, k=2, s=2),"
"D FILTER
LOCAL VOXELS",0.8642241379310345,"21
self.decode1 = self.Sequential("
"D FILTER
LOCAL VOXELS",0.8663793103448276,"22
res_block(layer4_Odim+layer3_Odim, layer3_Odim),"
"D FILTER
LOCAL VOXELS",0.8685344827586207,"23
res_block(layer3_Odim, layer3_Odim)) 24"
"D FILTER
LOCAL VOXELS",0.8706896551724138,"25
self.up2 = sparse_deconv(layer3_Odim, layer3_Odim, k=2, s=2)"
"D FILTER
LOCAL VOXELS",0.8728448275862069,"26
self.decode2 = self.Sequential("
"D FILTER
LOCAL VOXELS",0.875,"27
res_block(layer3_Odim+layer2_Odim, layer2_Odim),"
"D FILTER
LOCAL VOXELS",0.8771551724137931,"28
res_block(layer2_Odim, layer2_Odim)) 29"
"D FILTER
LOCAL VOXELS",0.8793103448275862,"30
self.up3 = sparse_deconv(layer2_Odim, layer2_Odim, k=2, s=2)"
"D FILTER
LOCAL VOXELS",0.8814655172413793,"31
self.decode3 = self.Sequential("
"D FILTER
LOCAL VOXELS",0.8836206896551724,"32
res_block(layer2_Odim+layer1_Odim, layer1_Odim),"
"D FILTER
LOCAL VOXELS",0.8857758620689655,"33
res_block(layer1_Odim, layer1_Odim)) 34"
"D FILTER
LOCAL VOXELS",0.8879310344827587,"35
self.up4 = sparse_deconv(layer1_Odim, layer1_Odim, k=2, s=2)"
"D FILTER
LOCAL VOXELS",0.8900862068965517,"36
self.decode4 = self.Sequential("
"D FILTER
LOCAL VOXELS",0.8922413793103449,"37
res_block(layer1_Odim+layer1_Odim, layer1_Odim),"
"D FILTER
LOCAL VOXELS",0.8943965517241379,"38
res_block(layer1_Odim, layer1_Odim)) 39"
"D FILTER
LOCAL VOXELS",0.896551724137931,"40
self.output_layer = nn.Sequential("
"D FILTER
LOCAL VOXELS",0.8987068965517241,"41
nn.Linear(layer1_Odim, class_num)) 42"
"D FILTER
LOCAL VOXELS",0.9008620689655172,"43
def forward(self, x):"
"D FILTER
LOCAL VOXELS",0.9030172413793104,"44
x_i = self.input_layer(x)"
"D FILTER
LOCAL VOXELS",0.9051724137931034,"45
x1 = self.layer1(x_i)"
"D FILTER
LOCAL VOXELS",0.9073275862068966,"46
x2 = self.layer2(x1)"
"D FILTER
LOCAL VOXELS",0.9094827586206896,"47
x3 = self.layer3(x2)"
"D FILTER
LOCAL VOXELS",0.9116379310344828,"48
x4 = self.layer4(x3) 49"
"D FILTER
LOCAL VOXELS",0.9137931034482759,"50
x3_ = self.decoder1(cat(x3, self.up1(x4)))"
"D FILTER
LOCAL VOXELS",0.915948275862069,"51
x2_ = self.decoder2(cat(x2, self.up2(x3_)))"
"D FILTER
LOCAL VOXELS",0.9181034482758621,"52
x1_ = self.decoder3(cat(x1, self.up3(x2_)))"
"D FILTER
LOCAL VOXELS",0.9202586206896551,"53
xi_ = self.decoder4(cat(x_i, self.up4(x1_)))"
"D FILTER
LOCAL VOXELS",0.9224137931034483,"54
return self.output_layer(xi_)"
"D FILTER
LOCAL VOXELS",0.9245689655172413,Listing 2: Pseudo code of inﬂated ResNet for segmentation
"D FILTER
LOCAL VOXELS",0.9267241379310345,Under review as a conference paper at ICLR 2022
"D FILTER
LOCAL VOXELS",0.9288793103448276,Unit 17
"D FILTER
LOCAL VOXELS",0.9310344827586207,"Broden
Modelnet"
"D FILTER
LOCAL VOXELS",0.9331896551724138,Unit 127
"D FILTER
LOCAL VOXELS",0.9353448275862069,Unit 16
"D FILTER
LOCAL VOXELS",0.9375,Modelnet
"D FILTER
LOCAL VOXELS",0.9396551724137931,Unit 38
"D FILTER
LOCAL VOXELS",0.9418103448275862,Broden
"D FILTER
LOCAL VOXELS",0.9439655172413793,"Unit 78
Unit 82"
"D FILTER
LOCAL VOXELS",0.9461206896551724,Unit 42
"D FILTER
LOCAL VOXELS",0.9482758620689655,"Broden
Modelnet"
"D FILTER
LOCAL VOXELS",0.9504310344827587,Unit 420
"D FILTER
LOCAL VOXELS",0.9525862068965517,Unit 35
"D FILTER
LOCAL VOXELS",0.9547413793103449,Modelnet
"D FILTER
LOCAL VOXELS",0.9568965517241379,Unit 102
"D FILTER
LOCAL VOXELS",0.959051724137931,Broden
"D FILTER
LOCAL VOXELS",0.9612068965517241,"Unit 227
Unit 394"
"D FILTER
LOCAL VOXELS",0.9633620689655172,Unit 177
"D FILTER
LOCAL VOXELS",0.9655172413793104,"Broden
Modelnet"
"D FILTER
LOCAL VOXELS",0.9676724137931034,Unit 487
"D FILTER
LOCAL VOXELS",0.9698275862068966,Unit 169
"D FILTER
LOCAL VOXELS",0.9719827586206896,Modelnet
"D FILTER
LOCAL VOXELS",0.9741379310344828,Unit 198
"D FILTER
LOCAL VOXELS",0.9762931034482759,Broden
"D FILTER
LOCAL VOXELS",0.978448275862069,"Unit 427
Unit 438"
"D FILTER
LOCAL VOXELS",0.9806034482758621,Unit 98
"D FILTER
LOCAL VOXELS",0.9827586206896551,"Broden
Modelnet"
"D FILTER
LOCAL VOXELS",0.9849137931034483,Unit 246
"D FILTER
LOCAL VOXELS",0.9870689655172413,Unit 80
"D FILTER
LOCAL VOXELS",0.9892241379310345,Modelnet
"D FILTER
LOCAL VOXELS",0.9913793103448276,Unit 109
"D FILTER
LOCAL VOXELS",0.9935344827586207,Broden
"D FILTER
LOCAL VOXELS",0.9956896551724138,"Unit 129
Unit 141"
"D FILTER
LOCAL VOXELS",0.9978448275862069,Figure 7: More network dissection results.
