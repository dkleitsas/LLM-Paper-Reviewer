Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0015432098765432098,"We focus on the setting of contextual batched bandit (CBB), where a batch of re-
wards is observed from the environment in each episode. But these rewards are
partial-information feedbacks where the rewards of the non-executed actions are
unobserved. Existing approaches for CBB usually ignore the potential rewards of
the non-executed actions, resulting in feedback information being underutilized.
In this paper, we propose an efﬁcient reward imputation approach using sketch-
ing in CBB, which completes the unobserved rewards with the imputed rewards
approximating the full-information feedbacks. Speciﬁcally, we formulate the re-
ward imputation as a problem of imputation regularized ridge regression, which
captures the feedback mechanisms of both the non-executed and executed actions.
To reduce the time complexity of reward imputation on a large batch of data, we
use randomized sketching for solving the regression problem of imputation. We
prove that the proposed reward imputation approach obtains a relative-error bound
for sketching approximation, achieves an instantaneous regret with a controllable
bias and a smaller variance than that without reward imputation, and enjoys a
sublinear regret bound against the optimal policy. Moreover, we present two ex-
tensions of our approach, including the rate-scheduled version and the version for
nonlinear rewards, which makes our approach more feasible. Experimental results
demonstrated that our approach can outperform the state-of-the-art baselines on a
synthetic dataset, the Criteo dataset, and a dataset from a commercial app."
INTRODUCTION,0.0030864197530864196,"1
INTRODUCTION"
INTRODUCTION,0.004629629629629629,"Contextual bandits have been widely used in real-world sequential decision-making problems (Li
et al., 2010; Lan & Baraniuk, 2016; Yom-Tov et al., 2017; Yang et al., 2021), where the agen-
t updates the decision-making policy fully online (i.e., at each step) according to the context and
corresponding reward feedback so as to maximize the cumulative reward. In this paper, we con-
sider a more complex setting—contextual batched bandits (CBB), where the decision process is
partitioned into N episodes, the agent interacts with the environment for B steps in one episode,
collects the reward feedbacks and contexts at the end of the episode, and then updates the policy
using the collected data for the next episode. CBB is more practical in some real-world applications,
since updating the policy once receiving the reward feedback is rather unrealistic due to its high
computational cost and decision instability."
INTRODUCTION,0.006172839506172839,"In bandit settings, it is inevitable that the environment only reveals the rewards of the executed
actions to the agent as the feedbacks, while hiding the rewards of non-executed actions. We refer to
this category of limited feedback as the partial-information feedback (also called “bandit feedback”).
Existing batched bandit approaches in the CBB setting discard the information contained in the
potential rewards of the non-executed actions, address the problem of partial-information feedback
using an exploitation-exploration tradeoff on the context space and reward space (Han et al., 2020;
Zhang et al., 2020).But in the CBB setting, the agent usually estimates and maintains reward models
for the action-selection policy, and the potential rewards of the non-executed actions have been
somehow captured by the policy. This additional reward structure information is estimated and
available in each episode, however, are not utilized by existing batched bandit approaches."
INTRODUCTION,0.007716049382716049,"In contextual bandit settings where the policy is updated fully online, several bias-correction ap-
proaches have been introduced to address the partial-information feedback. Dimakopoulou et al."
INTRODUCTION,0.009259259259259259,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.010802469135802469,"(2019) presented linear contextual bandits integrating the balancing approach from causal inference,
which reweight the contexts and rewards by the inverse propensity scores. Chou et al. (2015) de-
signed pseudo-reward algorithms for contextual bandits, which use a direct method to estimate the
unobserved rewards for the upper conﬁdence bound (UCB) strategy. Kim & Paik (2019) focused
on the feedback bias-correction for LASSO bandit with high-dimensional contexts, and applied
the doubly-robust approach to the reward modiﬁcation using average contexts. Although these ap-
proaches have been demonstrated to be effective in contextual bandit settings, little efforts have been
spent to address the under-utilization of partial-information feedback in the CBB setting."
INTRODUCTION,0.012345679012345678,"Theoretical and experimental analyses in Section 2 indicate that better performance of CBB is
achievable if the rewards of the non-executed actions can be received. Motivated by these observa-
tions, we propose a novel reward imputation approach for the non-executed actions, which mimics
the reward generation mechanisms of environments. We conclude our contributions as follows.
• To fully utilize feedback information in CBB, we formulate the reward imputation as a problem of
imputation regularized ridge regression, where the policy can be updated efﬁciently using sketching.
• We prove that our reward imputation approach obtains a relative-error bound for sketching ap-
proximation, achieves an instantaneous regret with a controllable bias and a smaller variance than
that without reward imputation, has a lower bound of the sketch size independently of the overall
number of steps, enjoys a sublinear regret bound against the optimal policy, and reduces the time
complexity from O(Bd2) to O(cd2) for each action in one episode, where B denotes the batch size,
c the sketch size, and d the dimensionality of inputs, satisfying d < c < B.
• We present two practical variants of our reward imputation approach, including the rate-scheduled
version in which the imputation rate is set without tuning, and the version for nonlinear rewards.
• We carried out extensive experiments on the synthetic data, public benchmark, and the data col-
lected from a real commercial product to demonstrate our performance, empirically analyzed the
inﬂuence of different parameters, and veriﬁed the correctness of the theoretical results."
INTRODUCTION,0.013888888888888888,"Related Work. Recently, batched bandit has become an active research topic in statistics and learn-
ing theory including 2-armed bandit (Perchet et al., 2016), multi-armed bandit (Gao et al., 2019;
Zhang et al., 2020; Wang & Cheng, 2020), and contextual bandit (Han et al., 2020; Ren & Zhou,
2020; Gu et al., 2021). Han et al. (2020) deﬁned linear contextual bandits, and designed UCB-type
algorithms for both stochastic and adversarial contexts, where true rewards of different actions have
the same parameters. Zhang et al. (2020) provided methods for inference on data collected in batch-
es using bandits, and introduced a batched least squares estimator for both multi-arm and contextual
bandits. Recently, Esfandiari et al. (2021) proved reﬁned regret upper bounds of batched bandits in
stochastic and adversarial settings. There are several recent works that consider similar settings to
CBB, e.g., episodic Markov decision process (Jin et al., 2018), LASSO bandits (Wang & Cheng,
2020). Sketching is another related technology that compresses a large matrix to a much smaller
one by multiplying a (usually) random matrix with certain properties (Woodruff, 2014), which has
been used in online convex optimization (Calandriello et al., 2017; Zhang & Liao, 2019)."
PROBLEM FORMULATION AND ANALYSIS,0.015432098765432098,"2
PROBLEM FORMULATION AND ANALYSIS"
PROBLEM FORMULATION AND ANALYSIS,0.016975308641975308,"First, we introduce some notations. Let [x] = {1, 2, . . . , x}, S ⊆Rd be the context space, A =
{Aj}j∈[M] the action space containing M actions,[A; B] = [A⊺, B⊺]⊺, ∥A∥F, ∥A∥1 ∥A∥2 denote
the Frobenius norm, 1-norm, and spectral norm of a matrix A, respectively, ∥a∥1 and ∥a∥2 be the
ℓ1-norm and the ℓ2-norm of a vector a, σmin(A) and σmax(A) denote the minimum and maximum
of the of singular values of A. In this paper, we focus on the setting of Contextual Batched Bandits
(CBB) (see Algorithm 1), where the decision process is partitioned into N episodes, and in each
episode, CBB consists of two phases: 1) the policy updating approximates the optimal policy based
on the received contexts and rewards; 2) the online decision selects actions for execution following
the updated and ﬁxed policy p for B steps (also called the batch size is B), and stores the context-
action pairs and the observed rewards of the executed actions into a data buffer D. The reward R in
CBB is a partial-information feedback where rewards are unobserved for the non-executed actions."
PROBLEM FORMULATION AND ANALYSIS,0.018518518518518517,"Different from existing batch bandit setting (Han et al., 2020; Esfandiari et al., 2021), where the
true reward feedbacks for all actions are controlled by the same parameter vector while the contexts
received differ by actions at each step, we assume that in the CBB setting, the mechanism of true
reward feedback differs by actions and the received context is shared by actions. Formally, for any
context si ∈S ⊆Rd and action A ∈A, we assume that the expectation of the true reward Rtrue
i,A"
PROBLEM FORMULATION AND ANALYSIS,0.020061728395061727,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION AND ANALYSIS,0.021604938271604937,Algorithm 1 Contextual Batched Bandit (CBB)
PROBLEM FORMULATION AND ANALYSIS,0.023148148148148147,"INPUT: Batch size B, number of episodes N, action space A = {Aj}j∈[M], context space S ⊆Rd"
PROBLEM FORMULATION AND ANALYSIS,0.024691358024691357,"1: Initialize policy p0 ←1/M, sample data buffer D1 = {(s0,b, AI0,b, R0,b)}b∈[B] using initial policy p0
2: for n = 1 to N do
3:
Update the policy pn on Dn
{Policy Updating}
4:
for b = 1 to B do
5:
Observe context sn,b
6:
Choose AIn,b ∈A following the updated policy pn(sn,b) {Online Decision}
7:
end for
8:
Dn+1 ←{(sn,b, AIn,b, Rn,b}b∈[B], where Rn,b denotes the reward of action AIn,b on context sn,b
9: end for"
PROBLEM FORMULATION AND ANALYSIS,0.026234567901234566,"is determined by an unknown action-speciﬁc reward parameter vector θ∗
A ∈Rd: E

Rtrue
i,A | si

=
⟨θ∗
A, si⟩(the linear reward will be extended to the nonlinear case in Section 5). This setting for
reward feedback matches many real-world applications, e.g., each action corresponds to a different
category of candidate coupons in coupon recommendation, and the reward feedback mechanism of
each category differs due to the different discount pricing strategies."
PROBLEM FORMULATION AND ANALYSIS,0.027777777777777776,"Next, we provide deeper understandings of the inﬂuence of unobserved feedbacks on the perfor-
mance of policy updating in the CBB setting. We ﬁrst conducted an empirical comparison by
applying the batch UCB policy (Han et al., 2020) to environments under different proportion-
s of received reward feedbacks. In particular, the agent under full-information feedback can re-
ceive all the rewards of the executed and non-executed actions, called Full-Information CBB (FI-
CBB) setting.
From Figure 1, we can observe that the partial-information feedbacks could be
damaging in terms of hurting the policy updating, and batched bandit could beneﬁt from more
reward feedbacks, where the performance of 80% feedback is very close to that of FI-CBB."
PROBLEM FORMULATION AND ANALYSIS,0.029320987654320986,"10
20
30
40
50
60
70
80
90
Number of Episodes N 0.236 0.238 0.240 0.242 0.244 0.246 0.248 0.250 0.252"
PROBLEM FORMULATION AND ANALYSIS,0.030864197530864196,Average Reward
PROBLEM FORMULATION AND ANALYSIS,0.032407407407407406,"partial-information feedback (CBB)
40% feedback
60% feedback
80% feedback
full-information feedback (FI-CBB)"
PROBLEM FORMULATION AND ANALYSIS,0.033950617283950615,"Figure 1: Average rewards of batch UCB
policy (Han et al., 2020) under different pro-
portions of received reward feedbacks, in-
teracting with the synthetic environment in
Section 6, where x% feedback means that
x% of actions can receive their true rewards"
PROBLEM FORMULATION AND ANALYSIS,0.035493827160493825,"Then, we demonstrate the difference of instantaneous
regrets between the CBB and FI-CBB settings as in
Theorem 1.
The detailed description and proof of
Theorem 1 can be found in Appendix A.
Theorem 1. For any action A ∈A and contex-
t si ∈S, let θn
A be the reward parameter vec-
tor estimated by the batched UCB policy in the n-
th episode. The upper bound of instantaneous regret
(deﬁned by |⟨θn
A, si⟩−⟨θ∗
A, si⟩|) in the FI-CBB set-
ting is tighter than that in CBB setting (i.e., using the
partial-information feedback)."
PROBLEM FORMULATION AND ANALYSIS,0.037037037037037035,"From Theorem 1, we can conclude that the price
paid for having access only to the partial-information
feedbacks is the deterioration in the regret bound.
Ideally, the policy would be updated using the full-
information feedback.
In CBB, however, the full-
information feedback is unaccessible. Fortunately, in
CBB, different reward parameter vectors need to be maintained and estimated separately for each
action, and the potential reward structures of the non-executed actions have been somehow cap-
tured. So why don’t we use these maintained reward parameters to estimate the unknown rewards
for the non-executed actions? Next, we propose an efﬁcient reward imputation approach that uses
this additional reward structure information for improving the performance of the bandit policy."
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.038580246913580245,"3
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.040123456790123455,"In this section, we present an efﬁcient reward imputation approach tailored for policy updating in
the CBB setting."
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.041666666666666664,"Formulation of Reward Imputation. Since the true reward parameters differ by actions in the CBB
setting, we need to maintain a different estimated reward parameter vector for each action. As shown
in Figure 2, in contrast to CBB that ignores the contexts and rewards of the non-executed steps of
Aj, our reward imputation approach completes the missing values using the imputed contexts and
rewards, approximating the full-information CBB setting. Speciﬁcally, in the (n+1)-th episode, for"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.043209876543209874,Under review as a conference paper at ICLR 2022 … …
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.044753086419753084,Context
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.046296296296296294,Matrix
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.047839506172839504,Imputed
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.04938271604938271,Context
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.05092592592592592,"Matrix
Reward"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.05246913580246913,Vector
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.05401234567901234,Imputed
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.05555555555555555,Reward
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.05709876543209876,"Vector B … B
…
…"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.05864197530864197,Context Matrix
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.06018518518518518,Reward
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.06172839506172839,"Vector B … B
…"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.06327160493827161,Context
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.06481481481481481,Matrix
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.06635802469135803,Reward
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.06790123456790123,"Vector n
j
N … n
j
N"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.06944444444444445,"n
j
N
ˆ n j
N n
j
N ˆ n j
N"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.07098765432098765,"CBB with Our Reward Imputation
Full-Information CBB
CBB"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.07253086419753087,"j
n
A
S"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.07407407407407407,"j
n
A
R"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.07561728395061729,"j
n
A
S
ˆ"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.07716049382716049,"j
n
A
S"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.0787037037037037,"j
n
A
R ˆ"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.08024691358024691,"j
n
A
R"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.08179012345679013,"Figure 2: Comparison of the stored data corresponding to the action Aj ∈A in CBB, CBB with our
reward imputation, and full-information CBB, in the (n + 1)-th episode"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.08333333333333333,"each action Aj ∈A, j ∈[M], we store context vectors and rewards corresponding to the steps in
which the action Aj is executed, into a context matrix Sn
Aj ∈RNn
j ×d and a reward vector Rn
Aj ∈
RNn
j , respectively, where N n
j denotes the number of steps (in episode n+1) in which the action Aj is
executed. Additionally, for any Aj ∈A, j ∈[M], we store context vectors corresponding to the non-
executed steps of action Aj (denote the number of non-executed steps by b
N n
j , i.e., b
N n
j = B −N n
j ),"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.08487654320987655,"into an imputed context matrix bSn
Aj ∈R
b
Nn
j ×d, and compute the imputed reward vector as follows:"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.08641975308641975,"b
Rn
Aj = {rn,1(Aj), rn,2(Aj), . . . , rn, b
Nn
j (Aj)} ∈R
b
Nn
j ,
j ∈[M],"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.08796296296296297,"where rn,b(Aj) := ⟨¯θn
Aj, sn,b⟩denotes the imputed reward for each step b ∈[ b
N n
j ], and sn,b is the"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.08950617283950617,"b-th row of bSn
Aj. Then, we obtain several block matrices by concatenating the context and reward
matrices from the previous episodes: Ln
Aj = [S0
Aj; · · · ; Sn
Aj] ∈RLn
j ×d, T n
Aj = [R0
Aj; · · · ; Rn
Aj] ∈"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.09104938271604938,"RLn
j , Ln
j = Pn
k=0 N k
j , bLn
Aj = [ bS0
Aj; · · · ; bSn
Aj] ∈R
bLn
j ×d, bT n
Aj = [ b
R0
Aj; · · · ; b
Rn
Aj] ∈R
bLn
j , bLn
j =
Pn
k=0 b
N k
j . In the (n + 1)-th episode, the estimated parameter vector ¯θn+1
Aj
of the imputed reward
for action Aj can be updated by solving the following imputation regularized ridge regression:"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.09259259259259259,"¯θn+1
Aj
= arg min
θ∈Rd"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.0941358024691358,"Ln
Ajθ −T n
Aj

2"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.09567901234567901,"2
|
{z
}
Observed Term"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.09722222222222222,"+ γ
bLn
Ajθ −bT n
Aj

2"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.09876543209876543,"2
|
{z
}
Imputation Term"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.10030864197530864,"+λ∥θ∥2
2,
n = 0, 1, . . . , N −1,
(1)"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.10185185185185185,"where γ ∈[0, 1] is the imputation rate which controls the degree of reward imputation and measures
a trade-off between bias and variance (Remark 1&2), λ > 0 is the regularization parameter, yielding"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.10339506172839506,"¯θn+1
Aj
=

Ψn+1
Aj"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.10493827160493827,"−1 
bn+1
Aj
+ γˆbn+1
Aj"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.10648148148148148,"
,
(2)"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.10802469135802469,"that can be obtained by the closed least squares solution, where Ψn+1
Aj
:= λId + Φn+1
Aj
+ γ bΦn+1
Aj ,"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.1095679012345679,"Φn+1
Aj
= Φn
Aj + Sn⊺
Aj Sn
Aj,
bn+1
Aj
= bn
Aj + Sn⊺
Aj Rn
Aj,
(3)"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.1111111111111111,"bΦn+1
Aj
= η bΦn
Aj + bSn⊺
Aj bSn
Aj,
ˆbn+1
Aj
= ηˆbn
Aj + bSn⊺
Aj b
Rn
Aj,
(4)"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.11265432098765432,"and η ∈(0, 1) is the discount parameter which controls how fast the previous imputed rewards are
forgotten, and can help guaranteeing the regret bound in Theorem 2."
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.11419753086419752,"Efﬁcient Reward Imputation using Sketching. As shown in the ﬁrst 4 columns in Table 1, the
overall time complexity of the imputation for each action is O(Bd2) in each episode, where B
represents the batch size, and d the dimensionality of the input. Thus, for all the M actions in one
episode, reward imputation increases the time complexity from O(Bd2) of the approach without
imputation to O(MBd2). To address this issue, we design an efﬁcient reward imputation approach
using sketching, which reduces the time complexity of each action in one episode from O
 
Bd2"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.11574074074074074,"to O
 
cd2
, where c denotes the sketch size satisfying d < c < B and cd > B. Speciﬁcally, in
the (n + 1)-th episode, the imputation regularized ridge regression Eq.(1) can be approximated by a
sketched ridge regression as"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.11728395061728394,"˜θn+1
Aj
= arg min
θ∈Rd"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.11882716049382716,"Πn
Aj

Ln
Ajθ −T n
Aj

2"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.12037037037037036,"2 + γ
 bΠn
Aj

bLn
Ajθ −bT n
Aj

2"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.12191358024691358,"2 + λ∥θ∥2
2,
(5)"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.12345679012345678,Under review as a conference paper at ICLR 2022
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.125,"Table 1: The time complexities of the original reward imputation in Eq.(1) (ﬁrst 4 columns) and the
reward imputation using sketching in Eq.(5) (last 4 columns) for action Aj in the (n+1)-th episode,
where N n
j ( b
N n
j ) denotes the number of steps in which the action Aj is executed (non-executed) in
episode n+1, b
N n
j +N n
j = B, and the sketch size c satisfying d < c < B and cd > B (MM: matrix
multiplication; MI: matrix inversion; Overall: overall time complexity for action Aj in one episode)"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.12654320987654322,"Original reward imputation in Eq.(1)
Reward imputation using sketching in Eq.(5)
Item
Operation
Eq.
Time
Item
Operation
Eq.
Time
Φn+1
Aj , bΦn+1
Aj
MM
(3), (4)
O(Bd2)
Gn+1
Aj , b
Gn+1
Aj
MM
(7), (8)
O(cd2)
bn+1
Aj , ˆbn+1
Aj
MM
(3), (4)
O(Bd)
pn+1
Aj , ˆpn+1
Aj
MM
(7), (8)
O(cd)
(Ψn+1
Aj )−1
MI
(2)
O(d3)
(W n+1
Aj
)−1
MI
(6)
O(d3)
–
Γn
Aj, Λn
Aj
Sketching
–
O(N n
j d)
–
bΓn
Aj, bΛn
Aj
Sketching
–
O( b
N n
j d)
Overall
–
–
O(Bd2)
Overall
–
–
O(cd2)"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.12808641975308643,"where ˜θn+1
A
denotes the estimated parameter vector of the imputed reward using sketching for action
A ∈A, Cn
Aj ∈Rc×Nn
j and b
Cn
Aj ∈Rc× b
Nn
j are the sketch submatrices for the observed term and the
imputation term, respectively, and the sketch matrices for the two terms can be represented as"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.12962962962962962,"Πn
Aj =
h
C0
Aj, C1
Aj, · · · , Cn
Aj
i
∈Rc×Ln
j ,
bΠn
Aj =
h
b
C0
Aj, b
C1
Aj, · · · , b
Cn
Aj
i
∈Rc×bLn
j ."
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.13117283950617284,"We denote the sketches of the context matrix and the reward vector by Γn
Aj := Cn
AjSn
Aj ∈Rc×d"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.13271604938271606,"and Λn
Aj := Cn
AjRn
Aj ∈Rc, the sketches of the imputed context matrix and the imputed reward"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.13425925925925927,"vector by bΓn
Aj := b
Cn
Aj bSn
Aj ∈Rc×d and bΛn
Aj := b
Cn
Aj b
Rn
Aj ∈Rc, and obtain the solution of Eq.(5):"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.13580246913580246,"˜θn+1
Aj
=

W n+1
Aj"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.13734567901234568,"−1 
pn+1
Aj
+ γ ˆpn+1
Aj"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.1388888888888889,"
,
(6)"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.1404320987654321,"where η ∈(0, 1) denotes the discount parameter, W n+1
Aj
:= λId + Gn+1
Aj
+ γ b
Gn+1
Aj , and"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.1419753086419753,"Gn+1
Aj
= Gn
Aj + Γn⊺
AjΓn
Aj,
pn+1
Aj
= pn
Aj + Γn⊺
AjΛn
Aj,
(7)"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.14351851851851852,"b
Gn+1
Aj
= η b
Gn
Aj + bΓn⊺
Aj bΓn
Aj,
ˆpn+1
Aj
= η ˆpn
Aj + bΓn⊺
Aj bΛn
Aj.
(8)"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.14506172839506173,"Using the parameter ˜θn+1
Aj , we obtain the sketched version of imputed reward as ˜rn,b(Aj) :="
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.14660493827160495,"⟨˜θn
Aj, sn,b⟩at step b ∈[ b
N n
j ]. Finally, we specify that the sketch submatrices {Cn
A}A∈A,n∈[N] and"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.14814814814814814,"{ b
Cn
Aj}A∈A,n∈[N] are the block construction of Sparser Johnson-Lindenstrauss Transform (SJLT)
(Kane & Nelson, 2014), where the sketch size c is divisible by the number of blocks D1. As shown
in the last 4 columns in Table 1, sketching reduces the time complexity from O(MBd2) to O(Mcd2)
for reward imputation of all M actions in one episode, where c < B. When Mc ≈B, the overall
time complexity of our reward imputation using sketching is even comparable to that without reward
imputation which has a O(Bd2) time complexity."
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.14969135802469136,"Updated Policy using Imputed Rewards. Inspired by the UCB strategy (Li et al., 2010), the updat-
ed policy for online decision of the (n + 1)-th episode can be formulated using the imputed rewards
(parameterized by ¯θn+1
A
in Eq.(2)) or the sketched version of imputed rewards (parameterized by
˜θn+1
A
in Eq.(6)). Speciﬁcally, for a new context s,
origin policy ¯pn+1 selects the action following A ←arg max
A∈A
⟨¯θn+1
A
, s⟩+ ω[s⊺(Ψn+1
A
)−1s]
1
2 ,"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.15123456790123457,"sketched policy ˜pn+1 selects the action following A ←arg max
A∈A
⟨˜θn+1
A
, s⟩+ α[s⊺(W n+1
A
)−1s]
1
2 ,"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.1527777777777778,"where ω ≥0 and α ≥0 are the regularization parameters in policy and their theoretical values are
given in Theorem 4. We summarize the reward imputation using sketching and the sketched poli-
cy into Algorithm 2, called SPUIR. Similarly, we call the updating of the original policy that uses
reward imputation without sketching, the Policy Updating with Imputed Rewards (PUIR)."
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.15432098765432098,"1Since we set the number of blocks of SJLT as D < d, we omit D in the complexity analysis."
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.1558641975308642,Under review as a conference paper at ICLR 2022
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.1574074074074074,Algorithm 2 Sketched Policy Updating with Imputed Rewards (SPUIR) in the (n + 1)-th episode
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.15895061728395063,"INPUT: Policy ˜pn, Dn+1, A = {Aj}j∈[M], α ≥0, η ∈(0, 1), γ ∈[0, 1], λ > 0, W 0
Aj = λId, G0
Aj =
b
G0
Aj = Od, p0
Aj = ˆp0
Aj = 0, ˜θ0
Aj = 0, j ∈[M], batch size B, sketch size c, number of block D
OUTPUT: Updated policy ˜pn+1"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.16049382716049382,"1: For all j ∈[M], store context vectors and rewards corresponding to the steps in which the action Aj is
executed, into Γn
Aj ∈RNn
j ×d and Λn
Aj ∈RNn
j"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.16203703703703703,"2: For all j ∈[M], store context vectors corresponding to the steps in which the action Aj is not executed
into bΓn
Aj ∈R
b
Nn
j ×d, where b
N n
j ←B −N n
j
3: ˜rn,b(Aj) ←⟨˜θn
Aj, sn,b⟩, for all Aj ∈A and b ∈[ b
N n
j ], where sn,b is the b-th row of bΓn
Aj
4: Compute imputed reward vector b
Rn
Aj ←{˜rn,1(Aj), . . . , ˜rn, b
Nn
j (Aj)} ∈R
b
Nn
j for any j ∈[M]
5: for all action Aj ∈A do
6:
Gn+1
Aj
←Gn
Aj + Γn⊺
AjΓn
Aj,
pn+1
Aj
←pn
Aj + Γn⊺
AjΛn
Aj
{Eq.(7)}"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.16358024691358025,"7:
b
Gn+1
Aj
←η b
Gn
Aj + bΓn⊺
Aj bΓn
Aj,
ˆpn+1
Aj
←η ˆpn
Aj + bΓn⊺
Aj bΛn
Aj
{Eq.(8)}"
EFFICIENT REWARD IMPUTATION FOR POLICY UPDATING,0.16512345679012347,"8:
W n+1
Aj
←λId + Gn+1
Aj
+ γ b
Gn+1
Aj ,
˜θn+1
Aj
←(W n+1
Aj
)−1(pn+1
Aj
+ γ ˆpn+1
Aj ) {Eq.(6)}
9: end for
10: ˜pn+1(s) selects action A ←arg maxA∈A⟨˜θn+1
A
, s⟩+ α[s⊺ 
W n+1
A
−1 s]
1
2 for a new context s
11: return {˜θn+1
A
}A∈A, {
 
W n+1
A
−1}A∈A"
THEORETICAL ANALYSIS,0.16666666666666666,"4
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.16820987654320987,"We provide the instantaneous regret bound, prove the approximation error of sketching, and analyze
the regret in the CBB setting. The detailed proofs can be found in Appendix B. We ﬁrst demonstrate
the instantaneous regret bound of the original solution ¯θn
A in Eq.(1).
Theorem 2 (Instantaneous Regret Bound). Let η ∈(0, 1) be the discount parameter, γ ∈[0, 1] the
imputation rate. In the n-th episode, if the rewards {Rn,b}b∈[B] are independent2 and bounded by
CR, then, for any b ∈[B] and ∀A ∈A, with probability at least 1 −δ,

¯θn
A, sn,b

−⟨θ∗
A, sn,b⟩
 ≤
h
λ∥θ∗
A∥2 + ν + γ
1
2 η−1"
"CIMP
I",0.1697530864197531,"2 CImp
i
[s⊺
n,b (Ψn
A)−1 sn,b]
1
2 ,
(9)"
"CIMP
I",0.1712962962962963,"where Ψn
A = λId + Φn
A + γ bΦn
A, ν = [2C2
R log(2MB/δ)]
1
2 , and CImp > 0. The ﬁrst term on the
right-hand side of Eq.(9) can be seen as the bias term for the reward imputation, while the second
term is the variance term. The variance term of our algorithm is not larger than that without the
reward imputation, i.e, for any s ∈Rd, [s⊺(Ψn
A)−1 s]
1
2 ≤[s⊺(λId + Φn
A)−1 s]
1
2 . Further, a larger
imputation rate γ leads to a smaller variance term [s⊺(Ψn
A)−1 s]
1
2 .
Remark 1 (Smaller Variance). From Theorem 2, we can observe that the proposed reward impu-
tation achieves a smaller variance ([s⊺
n,b (Ψn
A)−1 sn,b]
1
2 ) than that without the reward imputation."
"CIMP
I",0.1728395061728395,"Remark 2 (Controllable Bias). Our reward imputation approach incurs a bias term γ
1
2 η−1"
CIMP IN,0.1743827160493827,"2 CImp in
addition to the two bias terms λ∥θ∗
A∥2, ν that exist in every UCB-based policy. But this additional
bias term is controllable due to the presence of imputation rate γ that can help controlling the addi-
tional bias. Moreover, the term CImp in the additional bias can be replaced by a function fImp(n),
and fImp(n) is monotonic decreasing w.r.t. number of episodes n provided that the mild condition
√η = Θ(d−1) holds (the deﬁnition and analysis about fImp can be found in Appendix B.1). Over-
all, the imputation rate γ controls a trade-off between the bias term and the variance term, and we
will design a rate-scheduled approach for choosing the imputation rate γ in Section 5."
CIMP IN,0.17592592592592593,"Although some approximation error bounds using SJLT have been proposed (Nelson & Nguyˆen,
2013; Kane & Nelson, 2014; Zhang & Liao, 2019), it is still unknown what is the lower bound of
the sketch size while applying SJLT to the sketched ridge regression problem in our SPUIR. Next,
we prove the approximation error as well as the lower bound of the sketch size for the sketched ridge
regression problem. For convenience, we drop all the superscripts and subscripts in this result.
Theorem 3 (Approximation Error Bound of Imputation using Sketching). Denote the imputation
regularized ridge regression function by F(θ) (deﬁned in Eq.(1)) and the sketched ridge regression"
CIMP IN,0.17746913580246915,"2The assumption about conditional independence of the rewards is commonly used in the bandits literature,
which can be ensured using a master technology as a theoretical construction (Auer, 2002; Chu et al., 2011)."
CIMP IN,0.17901234567901234,Under review as a conference paper at ICLR 2022
CIMP IN,0.18055555555555555,"function by F S(θ) (deﬁned in Eq.(5)) for reward imputation, whose solutions (i.e., the estimated re-
ward parameter vectors) are ¯θ = arg minθ∈Rd F(θ) and ˜θ = arg minθ∈Rd F S(θ). Let γ ∈[0, 1] be
the imputation rate, λ > 0 the regularization parameter, δ ∈(0, 0.1], ε ∈(0, 1), Lall = [L; √γ bL],
and ρλ = ∥Lall∥2
2/(∥Lall∥2
2+λ). If Π and bΠ are SJLT, assuming that D = Θ(ε−1 log3(dδ−1)) and
the sketch size c = Ω
 
d polylog
 
dδ−1
/ε2
, with probability at least 1−δ, F(˜θ) ≤(1+ρλε)F(¯θ)
and ∥˜θ −¯θ∥2 = O
 √ρλε

hold."
CIMP IN,0.18209876543209877,"To measure the convergence of approximating the optimal policy in sequential decision-making, we
deﬁne the regret of SPUIR against the optimal policy as follows:"
CIMP IN,0.18364197530864199,"Reg
 
{AIn,b}n∈[N],b∈[B]

:= maxA∈A
P"
CIMP IN,0.18518518518518517,"n∈[N],b∈[B][⟨θ∗
A, sn,b⟩−⟨θ∗
AIn,b , sn,b⟩],"
CIMP IN,0.1867283950617284,"where In,b denotes the index of the executed action using the sketched policy ˜pn (parameterized by
{˜θn
A}A∈A) at step b in the n-th episode. Finally, we demonstrate the regret bound of SPUIR.
Theorem 4 (Regret Bound of SPUIR). Let T = BN be the overall number of steps, η ∈(0, 1)
be the discount parameter, γ ∈[0, 1] the imputation rate, λ > 0 the regularization parameter,
Cmax
θ∗
= maxA∈A ∥θ∗
A∥2, CImp be a positive constant. Assume that the conditional independence
assumption in Theorem 2 holds and the upper bound of rewards is CR, M = O(poly(d)), T ≥d2,
ν = [2C2
R log(2MB/δ1)]
1
2 with δ1 ∈(0, 1),"
CIMP IN,0.1882716049382716,"ω = λCmax
θ∗
+ ν + γ
1
2 η−1"
CIMP IN,0.18981481481481483,"2 CImp,
α = ωCα,
where Cα > 0 which decreases with increase of 1/ε and ε ∈(0, 1). Let δ2 ∈(0, 0.1], ρλ < 1 be the
constant deﬁned in Theorem 3, and Creg be a positive constant that decreases with increase of 1/ε.
For the sketch matrices {Πn
A}A∈A,n∈[N] and { bΠ}A∈A,n∈[N], assuming that the number of blocks in
SJLT D = Θ(ε−1 log3(dδ−1
2 )), and the sketch size satisfying c = Ω
 
d polylog
 
dδ−1
2

/ε2
, then,
for an arbitrary sequence of contexts {sn,b}n∈[N],b∈[B], with probability at least 1 −N(δ1 + δ2),"
CIMP IN,0.19135802469135801,"Reg({AIn,b}n∈[N],b∈[B]) ≤2αCreg
√"
CIMP IN,0.19290123456790123,"10M log(T + 1)(
√"
CIMP IN,0.19444444444444445,"dT + dB) + O

T
p"
CIMP IN,0.19598765432098766,"ρλϵd/B

.
(10)"
CIMP IN,0.19753086419753085,"Remark 3. Setting B = O(
p"
CIMP IN,0.19907407407407407,"T/d) and ρλϵ = 1/d yields a sublinear upper bound of regret of
order eO(
√"
CIMP IN,0.2006172839506173,"MdT)3 provided that the sketch size c = Ω(ρ2
λd3 polylog(dδ−1
2 )). We can observe that
the lower bound of c is independent of the overall number of steps T, and a theoretical value of
the batch size is B = CB
p"
CIMP IN,0.2021604938271605,"T/d = C2
BN/d, where setting CB ≈25 is a suitable choice that has
been veriﬁed in the experiments in Section 6. In particular, when ρλ = O(1/d), the sketch size of
order c = Ω(d polylogd) is sufﬁcient to achieve a sublinear regret, which has been demonstrated
in our experimental results. Since the lower bound of regret for contextual batched bandit in (Han
et al., 2020) assumes that there are only two actions and both the actions share the same true reward
model, it can not be applied to our CBB setting where each action corresponds to a different reward
model. Despite the lack of the lower bound in CBB setting, from the theoretical results of regret,
we can observe that our SPUIR admits several advantages: (a) The order of our regret bound is
not higher than those in the literature of contextual bandits in the fully-online setting (i.e., B = 1)
(Li et al., 2019; Dimakopoulou et al., 2019), which is a more simple setting than ours; (b) The
ﬁrst term in the regret bound Eq.(10) measures the performance of policy updating using imputed
rewards (called “policy error”). From Theorem 2 and Remark 1&2, we obtain that, in each episode,
our policy updating has a smaller variance than the policy without the reward imputation, and
incurs a decreasing additional bias under mild conditions, leading to a tighter regret (i.e., smaller
policy error) after some number of episodes. (c) The second term on the right-hand side of Eq.(10)
is of order O(T√ρλϵd/B), which is incurred by the sketching approximation using SJLT (called
“sketching error”). This sketching error does not have any inﬂuence on the order of regret of SPUIR,
which may even have a lower order with a suitable choice of ρλε, e.g., setting ρλε = T −1/4d−1"
CIMP IN,0.2037037037037037,"yields a sketching error of order O(T 3/8d1/2) provided that c = Ω(ρ2
λd3 polylog(dδ−1
2 )
√ T)."
EXTENSIONS OF OUR APPROACH,0.2052469135802469,"5
EXTENSIONS OF OUR APPROACH"
EXTENSIONS OF OUR APPROACH,0.20679012345679013,"To make the proposed reward imputation approach more feasible and practical, we tackle the follow-
ing two research questions by designing variants of our approach following the theoretical results:"
EXTENSIONS OF OUR APPROACH,0.20833333333333334,3We use the notation of eO to suppress logarithmic factors in the overall number of steps T.
EXTENSIONS OF OUR APPROACH,0.20987654320987653,Under review as a conference paper at ICLR 2022
EXTENSIONS OF OUR APPROACH,0.21141975308641975,"RQ1 (Parameter Selection): Can we set the imputation rate γ without tuning?
RQ2 (Nonlinear Reward): Can we apply the proposed reward imputation approach to the case
where the expectation of true rewards is nonlinear?
Rate-Scheduled Approach. For RQ1, we equip PUIR and SPUIR with a rate-scheduled approach,
called PUIR-RS and SPUIR-RS, respectively. From Remark 1&2, a larger imputation rate γ leads
to a smaller variance while increasing the bias, while the bias term includes a monotonic decreasing
function w.r.t. number of episodes under mild conditions. Therefore, we can gradually increase γ
with the number of episodes, avoiding the large bias at the beginning of reward imputation. Speciﬁ-
cally, we set γ = X% for episodes from (X −10)% × N to X% × N, where X ∈[10, 100].
Application to Nonlinear Rewards.
For RQ2, we provide nonlinear versions of reward imputa-
tion. We use linearization technologies of nonlinear rewards, rather than directly setting the rewards
as nonlinear functions (Valko et al., 2013; Chatterji et al., 2019), avoiding the linear regret or curse
of kernelization. Speciﬁcally, instead of using the linear imputed reward ˜rn,b(Aj) := ⟨˜θn
Aj, sn,b⟩,
we use the following linearized nonlinear imputed rewards, denotes by Tn,b(θ, A):
1) SPUIR-Exp. We assume that the expected reward is an exponential function as GE(θ, s) =
exp (θ⊺s) . Then Tn,b(θ, A) = ⟨θ, ∇θGE(θ, sn,b)⟩, where ∇θGE(θ, sn,b) = exp (θ⊺sn,b) sn,b.
2) SPUIR-Poly. When the expected reward is a polynomial function as GP(θ, s) = (θ⊺s)2 . Then
Tn,b(θ, A) = ⟨θ, ∇θGP(θ, sn,b)⟩, where ∇θGP(θ, sn,b) = 2 (θ⊺sn,b) sn,b.
3) SPUIR-Kernel. Consider that the underlying expected reward in a Gaussian reproducing kernel
Hilbert space (RKHS). We use Tn,b(θ, A) = ⟨θ, φ(sn,b)⟩in random feature space, where the ran-
dom feature mapping φ can be explicitly deﬁned as in (Rahimi & Recht, 2007).
For SPUIR-Exp and SPUIR-Poly, combining the linearization of convex functions (Shalev-Shwartz,
2011) with Theorem 4 yields the regret bounds of the same order. For SPUIR-Kernel, using the
approximation error of random features (Rahimi & Recht, 2008), we can obtain a regret bound with
an additional error of order O(B/
√"
EXTENSIONS OF OUR APPROACH,0.21296296296296297,"dr), where dr is the dimension of the random features."
EXPERIMENTS,0.21450617283950618,"6
EXPERIMENTS"
EXPERIMENTS,0.21604938271604937,"We empirically evaluated the performance of our algorithms on 3 datasets: the synthetic dataset,
publicly available Criteo dataset4 (Criteo-recent, Criteo-all), and dataset collected from
a real commercial app for coupon recommendation (commercial product).
Experimental Settings. We compared our algorithms with: Sequential Batch UCB (SBUCB) (Han
et al., 2020), Batched linear EXP3 (BEXP3) (Neu & Olkhovskaya, 2020), Batched linear EXP3
using Inverse Propensity Weighting (BEXP3-IPW) (Bistritz et al., 2019), Batched Balanced Linear
Thompson Sampling (BLTS-B) (Dimakopoulou et al., 2019), and Sequential version of Delayed
Feedback Model (DFM-S) (Chapelle, 2014). We implemented all the algorithms on Intel(R) X-
eon(R) Silver 4114 CPU@2.20GHz, and repeated the experiments 20 times. We tested the perfor-
mance of algorithms in streaming recommendation scenarios, where the reward is represented by
a linear combination of the click and conversion behaviors of users. According to Remark 3, we
set the batch size as B = C2
BN/d, the constant CB ≈25, and the sketch size c = 150 on all the
datasets. The average reward was used to evaluate the accuracy of algorithms.
Performance Evaluation. Figure 3(a)–(c) reports the average reward of SPUIR with its variants and
the baselines. We observed that SPUIR and its variants achieved higher average rewards, demon-
strating the effectiveness of our reward imputation. Moreover, SPUIR and its rate-scheduled version
SPUIR-RS had similar performances compared with the origin PUIR, which indicates the practi-
cal effectiveness of our variants and veriﬁes the correctness of the theoretical analyses. The results
on commercial product in Table 2 indicate that SPUIR outperformed the second-best base-
line with the improvements of 1.07% CVR (conversion rate) and 1.12% CTCVR (post-view click-
through&conversion rate). Besides, our reward imputation approaches were more efﬁcient than
DFM-S, BLTS-B. The variants using sketching of our algorithms (SPUIR, SPUIR-RS) signiﬁcantly
reduced the time costs of reward imputation, and took less than twice as long to run compared to the
baselines without reward imputation (SBUCB, BEXP3, BEXP3-IPW). Figure 3(d) illustrates per-
formances of SPUIR and its nonlinear variants, where SPUIR-Kernel achieved the highest rewards
indicating the effectiveness of the nonlinear generalization of our approach. For different decision
tasks, a suitable nonlinear reward model needs to be selected for better performances.
Parameter Inﬂuence. From the regret bound Eq.(10), we can observe that a larger batch size B
results in a larger ﬁrst term (of order O(B), called policy error) but a smaller second term (of order"
EXPERIMENTS,0.2175925925925926,4https://labs.criteo.com/2013/12/conversion-logs-dataset/
EXPERIMENTS,0.2191358024691358,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.22067901234567902,Table 2: Performance comparison of coupon recommendation on commercial product
EXPERIMENTS,0.2222222222222222,"Algorithm
CVR (mean ± std)
CTCVR (mean ± std)
Time (sec., mean ± std)"
EXPERIMENTS,0.22376543209876543,"DFM-S
0.8656 ± 0.0473
0.3317 ± 0.0218
302.3140 ± 8.3045
SBUCB
0.8569 ± 0.0037
0.4277 ± 0.0084
43.5435 ± 0.3659
BEXP3
0.4846 ± 0.0205
0.2425 ± 0.0116
53.5001 ± 0.9220
BEXP3-IPW
0.4862 ± 0.0187
0.2436 ± 0.0113
56.0101 ± 1.4142
BLTS-B
0.8663 ± 0.0178
0.4285 ± 0.0157
218.2109 ± 1.8198
PUIR
0.8807 ± 0.0053
0.4411 ± 0.0029
184.3575 ± 2.2346
SPUIR
0.8770 ± 0.0059
0.4397 ± 0.0032
81.5753 ± 1.5879
PUIR-RS
0.8763 ± 0.0056
0.4389 ± 0.0030
180.4999 ± 1.7763
SPUIR-RS
0.8758 ± 0.0058
0.4391 ± 0.0031
80.8003 ± 2.9030"
EXPERIMENTS,0.22530864197530864,"10
20
30
40
50
60
70
80
90
Number of Episodes N 0.220 0.225 0.230 0.235 0.240 0.245 0.250"
EXPERIMENTS,0.22685185185185186,Average Reward
EXPERIMENTS,0.22839506172839505,"PUIR
SPUIR
PUIR-RS
SPUIR-RS
BLTS-B
SBUCB"
EXPERIMENTS,0.22993827160493827,(a) synthetic data
EXPERIMENTS,0.23148148148148148,"0
50
100
150
200
250
300
Number of Episodes N 0.250 0.252 0.254 0.256 0.258"
EXPERIMENTS,0.2330246913580247,Average Reward
EXPERIMENTS,0.2345679012345679,"PUIR
SPUIR
PUIR-RS
SPUIR-RS
BLTS-B
SBUCB
BEXP3-IPW
BEXP3"
EXPERIMENTS,0.2361111111111111,(b) Criteo-all
EXPERIMENTS,0.23765432098765432,"20
40
60
80
100
120
Number of Episodes N 0.39 0.40 0.41 0.42 0.43 0.44"
EXPERIMENTS,0.23919753086419754,Average Reward
EXPERIMENTS,0.24074074074074073,"PUIR
SPUIR
PUIR-RS
SPUIR-RS
BLTS-B
SBUCB"
EXPERIMENTS,0.24228395061728394,(c) commercial product
EXPERIMENTS,0.24382716049382716,"0
20
40
60
80
Number of Episodes N 0.20 0.21 0.22 0.23 0.24 0.25 0.26"
EXPERIMENTS,0.24537037037037038,Average Reward
EXPERIMENTS,0.24691358024691357,"SPUIR
SPUIR-Exp
SPUIR-Poly
SPUIR-Kernel"
EXPERIMENTS,0.24845679012345678,(d) nonlinear reward
EXPERIMENTS,0.25,"10%N 20%N 30%N 40%N 50%N 60%N 70%N 80%N 90%N
N
N 0.252 0.254 0.256 0.258 0.260 0.262"
EXPERIMENTS,0.2515432098765432,Average Reward
EXPERIMENTS,0.25308641975308643,"B=600
B=800
B=1000
B=1200
B=1400"
EXPERIMENTS,0.25462962962962965,(e) batch size B 5.0%B 7.5%B
EXPERIMENTS,0.25617283950617287,10.0%B
EXPERIMENTS,0.25771604938271603,12.5%B
EXPERIMENTS,0.25925925925925924,15.0%B
EXPERIMENTS,0.26080246913580246,17.5%B
EXPERIMENTS,0.2623456790123457,20.0%B
EXPERIMENTS,0.2638888888888889,22.5%B
EXPERIMENTS,0.2654320987654321,25.0%B
EXPERIMENTS,0.26697530864197533,27.5%B
EXPERIMENTS,0.26851851851851855,30.0%B
EXPERIMENTS,0.2700617283950617,Sketch Size c 0.230 0.235 0.240 0.245 0.250
EXPERIMENTS,0.2716049382716049,Average Reward
EXPERIMENTS,0.27314814814814814,"SPUIR-RS
SPUIR"
EXPERIMENTS,0.27469135802469136,(f) sketch size c
EXPERIMENTS,0.2762345679012346,"Figure 3: (a), (b), (c): Average rewards of the compared algorithms, the proposed SPUIR and
its variants on synthetic dataset, Criteo dataset, and the real commercial product data, where we
omitted the curves of algorithms whose average rewards are 5% lower than the highest reward; (d):
SPUIR and its three nonlinear variants on synthetic dataset; (e): SPUIR with different batch sizes
on Criteo-recent; (f): SPUIR and SPUIR-RS with different sketch sizes on synthetic dataset"
EXPERIMENTS,0.2777777777777778,"O(1/B), called sketching error), indicating that a suitable batch size B needs to be set. This conclu-
sion was empirically veriﬁed in Figure 3(e), where B = 1, 000 (CB = 25) yields better empirical
performance in terms of the average reward. Similar phenomenon can also be observed on Criteo
dataset and commercial product. All of the results veriﬁed the theoretical results in Remark 3:
B = CB
p"
EXPERIMENTS,0.279320987654321,"T/d = C2
BN/d is a suitable choice while setting CB ≈25. From the results in Fig-
ure 3(f) we observe that, for our SPUIR and SPUIR-RS, the performances signiﬁcantly increased
when the sketch size c reached 10%B (≈d log d), which demonstrates the conclusion in Remark 3
that only the sketch size of order c = Ω(d polylogd) is needed for satisfactory performance."
CONCLUSION,0.2808641975308642,"7
CONCLUSION"
CONCLUSION,0.2824074074074074,"Partial-information feedback is ubiquitous in real-world applications, where reward feedback is usu-
ally underutilized for learning. This paper proposes a theoretically sound and computationally ef-
ﬁcient reward imputation approach for contextual batched bandits, which mimics the reward gen-
eration mechanism of the environment approximating the setting of full-information feedback. The
proposed reward imputation approach reduces the time complexity of imputation on large batches
of data using sketching, achieves a relative-error bound for sketching approximation, has an instan-
taneous regret with a controllable bias and a smaller variance, and enjoys a sublinear regret bound
against the optimal policy. The theoretical formulation and algorithmic implementation may provide
an efﬁcient reward imputation scheme for online learning under limited feedback."
CONCLUSION,0.2839506172839506,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.2854938271604938,ETHICS STATEMENT
ETHICS STATEMENT,0.28703703703703703,"To verify the effectiveness and efﬁciency of our algorithms on real products, we conducted experi-
ments on a real dataset collected from a commercial social app. We call this dataset commercial
product, where the data were collected after the users gave consent, and did not contain any
personally identiﬁable information or offensive content."
REPRODUCIBILITY STATEMENT,0.28858024691358025,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.29012345679012347,"The source code of the proposed algorithms is submitted as supplementary materials. For theoretical
results, clear explanations of any assumptions and a complete proof of the claims are included as an
appendix. The detailed experimental settings are also provided in the appendix. Since the dataset
commercial product is non-public, we did not provide a URL. We will release this non-public
dataset after the publication of this paper, and the link to download this non-public dataset is to be
included in the ﬁnal paper."
REFERENCES,0.2916666666666667,REFERENCES
REFERENCES,0.2932098765432099,"Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3:397–422, 2002."
REFERENCES,0.29475308641975306,"Ilai Bistritz, Zhengyuan Zhou, Xi Chen, Nicholas Bambos, and Jose Blanchet. Online EXP3 learn-
ing in adversarial bandits with delayed feedback. In Advances in Neural Information Processing
Systems 32, pp. 11349–11358, 2019."
REFERENCES,0.2962962962962963,"Jean Bourgain, Sjoerd Dirksen, and Jelani Nelson. Toward a uniﬁed theory of sparse dimensionality
reduction in Euclidean space. In Proceedings of the 47th Annual ACM on Symposium on Theory
of Computing, pp. 499–508, 2015."
REFERENCES,0.2978395061728395,"Daniele Calandriello, Alessandro Lazaric, and Michal Valko. Efﬁcient second-order online kernel
learning with adaptive embedding. In Advances in Neural Information Processing Systems 30,
pp. 6140–6150, 2017."
REFERENCES,0.2993827160493827,"Olivier Chapelle. Modeling delayed feedback in display advertising. In Proceedings of the 20th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1097–
1105, 2014."
REFERENCES,0.30092592592592593,"Niladri S. Chatterji, Aldo Pacchiano, and Peter L. Bartlett. Online learning with kernel losses. In
Proceedings of the 36th International Conference on Machine Learning, pp. 971–980, 2019."
REFERENCES,0.30246913580246915,"Ku-Chun Chou, Hsuan-Tien Lin, Chao-Kai Chiang, and Chi-Jen Lu. Pseudo-reward algorithms for
contextual bandits with linear payoff functions. In Proceedings of the 6th Asian Conference on
Machine Learning, pp. 344–359, 2015."
REFERENCES,0.30401234567901236,"Wei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandits with linear payoff
functions. In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and
Statistics, pp. 208–214, 2011."
REFERENCES,0.3055555555555556,"Maria Dimakopoulou, Zhengyuan Zhou, Susan Athey, and Guido Imbens. Balanced linear contextu-
al bandits. In Proceedings of the 33rd AAAI Conference on Artiﬁcial Intelligence, pp. 3445–3453,
2019."
REFERENCES,0.30709876543209874,"Hossein Esfandiari, Amin Karbasi, Abbas Mehrabian, and Vahab S. Mirrokni. Regret bounds for
batched bandits. In Proceedings of the 35th AAAI Conference on Artiﬁcial Intelligence, pp. 7340–
7348, 2021."
REFERENCES,0.30864197530864196,"Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed bandits problem.
In Advances in Neural Information Processing Systems 32, pp. 501–511, 2019."
REFERENCES,0.3101851851851852,"Quanquan Gu, Amin Karbasi, Khashayar Khosravi, Vahab Mirrokni, and Dongruo Zhou. Batched
neural bandits. arXiv preprint arXiv:2102.13028, 2021."
REFERENCES,0.3117283950617284,Under review as a conference paper at ICLR 2022
REFERENCES,0.3132716049382716,"Yanjun Han, Zhengqing Zhou, Zhengyuan Zhou, Jose H. Blanchet, Peter W. Glynn, and Yinyu Ye.
Sequential batch learning in ﬁnite-action linear contextual bandits. CoRR, abs/2004.06321, 2020."
REFERENCES,0.3148148148148148,"Chi Jin, Zeyuan Allen-Zhu, S´ebastien Bubeck, and Michael I. Jordan. Is q-learning provably efﬁ-
cient? In Advances in Neural Information Processing Systems 31, 2018."
REFERENCES,0.31635802469135804,"Daniel M Kane and Jelani Nelson. Sparser Johnson-Lindenstrauss transforms. Journal of the ACM,
61(1):4:1–4:23, 2014."
REFERENCES,0.31790123456790126,"Gi-Soo Kim and Myunghee Cho Paik. Doubly-robust lasso bandit. In Advances in Neural Informa-
tion Processing Systems 32, pp. 5869–5879, 2019."
REFERENCES,0.3194444444444444,"Andrew S. Lan and Richard G. Baraniuk. A contextual bandits framework for personalized learning
action selection. In Proceedings of the 9th International Conference on Educational Data Mining,
pp. 424–429, 2016."
REFERENCES,0.32098765432098764,"Lihong Li, Wei Chu, John Langford, and Robert E Schapire.
A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th International Conference
on World Wide Web, pp. 661–670, 2010."
REFERENCES,0.32253086419753085,"Shuai Li, Wei Chen, Shuai Li, and Kwong-Sak Leung. Improved algorithm on online clustering of
bandits. In Sarit Kraus (ed.), Proceedings of the 28th International Joint Conference on Artiﬁcial
Intelligence, volume Li2019Improved, pp. 2923–2929, 2019."
REFERENCES,0.32407407407407407,"Jelani Nelson and Huy L Nguyˆen. OSNAP: Faster numerical linear algebra algorithms via sparser
subspace embeddings. In Proceedings of the 54th Annual Symposium on Foundations of Comput-
er Science, pp. 117–126, 2013."
REFERENCES,0.3256172839506173,"Gergely Neu and Julia Olkhovskaya. Efﬁcient and robust algorithms for adversarial linear contextual
bandits. In Proceedings of the 33rd Conference on Learning Theory, pp. 3049–3068, 2020."
REFERENCES,0.3271604938271605,"Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit prob-
lems. The Annals of Statistics, 44(2):660–681, 2016."
REFERENCES,0.3287037037037037,"Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems 20, pp. 1177–1184, 2007."
REFERENCES,0.33024691358024694,"Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In Advances in Neural Information Processing Systems 21, pp.
1313–1320, 2008."
REFERENCES,0.3317901234567901,"Zhimei Ren and Zhengyuan Zhou. Dynamic batch learning in high-dimensional sparse linear con-
textual bandits. arXiv preprint arXiv:2008.11918, 2020."
REFERENCES,0.3333333333333333,"Yuta Saito, Gota Morishita, and Shota Yasui. Dual learning algorithm for delayed conversions. In
Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in
Information Retrieval, pp. 1849–1852, 2020."
REFERENCES,0.33487654320987653,"Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends R⃝
in Machine Learning, 4(2):107–194, 2011."
REFERENCES,0.33641975308641975,"Michal Valko, Nathaniel Korda, R´emi Munos, Ilias N. Flaounas, and Nello Cristianini. Finite-time
analysis of kernelised contextual bandits. In Proceedings of the 29th Conference on Uncertainty
in Artiﬁcial Intelligence, 2013."
REFERENCES,0.33796296296296297,"Chi-Hua Wang and Guang Cheng. Online batch decision-making with high-dimensional covariates.
In Proceedings of the 23rd International Conference on Artiﬁcial Intelligence and Statistics, pp.
3848–3857, 2020."
REFERENCES,0.3395061728395062,"Shusen Wang, Alex Gittens, and Michael W. Mahoney. Sketched ridge regression: Optimization
perspective, statistical perspective, and model averaging. In Proceedings of the 34th International
Conference on Machine Learning, pp. 3608–3616, 2017."
REFERENCES,0.3410493827160494,"David P Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends R⃝in
Theoretical Computer Science, 10(1–2):1–157, 2014."
REFERENCES,0.3425925925925926,Under review as a conference paper at ICLR 2022
REFERENCES,0.3441358024691358,"Jiaqi Yang, Wei Hu, Jason D. Lee, and Simon Shaolei Du. Impact of representation learning in
linear bandits. In Proceedings of the 9th International Conference on Learning Representations,
2021."
REFERENCES,0.345679012345679,"E. Yom-Tov, G. Feraru, M. Kozdoba, S. Mannor, and I. Hochberg. Encouraging physical activity in
patients with diabetes: Intervention using a reinforcement learning system. Journal of Medical
Internet Research, 19(10):e338, 2017."
REFERENCES,0.3472222222222222,"Yuya Yoshikawa and Yusaku Imai. A nonparametric delayed feedback model for conversion rate
prediction. arXiv:1802.00255v1, 2018."
REFERENCES,0.3487654320987654,"Kelly W. Zhang, Lucas Janson, and Susan A. Murphy. Inference for batched bandits. In Advances
in Neural Information Processing Systems 33, pp. 9818–9829, 2020."
REFERENCES,0.35030864197530864,"Xiao Zhang and Shizhong Liao. Incremental randomized sketching for online kernel learning. In
Proceedings of the 36th International Conference on Machine Learning, pp. 7394–7403, 2019."
REFERENCES,0.35185185185185186,Under review as a conference paper at ICLR 2022
REFERENCES,0.3533950617283951,"A
DETAILED PROBLEM FORMULATION AND DETAILED PROOF IN PROBLEM
ANALYSIS"
REFERENCES,0.3549382716049383,"In this part, we give the detailed problem formulation and the detailed proof of Theorem 1 in problem
analysis in Section 2."
REFERENCES,0.35648148148148145,"A.1
DETAILED PROBLEM FORMULATION OF CBB"
REFERENCES,0.35802469135802467,"In this paper, we focus on the setting of contextual batched bandits (CBB), which can be formulated
as a 6-tuple ⟨S, A, p, R, N, B⟩:
Context space S ⊆Rd means a vector space containing the context information received at each
step, e.g., context summarizes the information of both the user and items in recommendation sce-
narios.
Action space A = {Aj}j∈[M] contains M candidate actions for execution. As an example, in rec-
ommender systems, each action corresponds to a candidate item, and selecting an action means that
the corresponding item is recommended.
Policy p determines which action to take at each step, which is a function of the context s ∈S and
outputs an action for execution (or a selection distribution over action space A).
Reward R in CBB is a partial-information feedback where rewards are unobserved for the non-
executed actions. Consider a stochastic bandit setting, where the expectation of the true reward is
assumed to be a function of the context s ∈S. In particular, different from the shared expectation
function of true rewards in existing batch bandits (Han et al., 2020), we assume that the expectation
functions of true rewards are different for each action, where each expectation function corresponds
to an unknown parameter vector θ∗
A ∈Rd, A ∈A. This setting for rewards matches many real-
world applications, e.g., each action corresponds to a different category of candidate coupons in
coupon recommendation.
Number of episodes N. The decision process in CBB is partitioned into N episodes. Within one
episode, the agent updates the policy using the collected data, and then interacts with the environ-
ment for multiple steps using the updated and ﬁxed policy.
Batch size B is the number of steps in each episode. That is, in each episode, the agent interacts
with the environment B times using a ﬁxed policy, and stores the contexts, executed actions, and
observed rewards into a data buffer D at the end of each episode."
REFERENCES,0.3595679012345679,"A.2
DETAILED DESCRIPTION AND PROOF OF THEOREM 1 IN PROBLEM ANALYSIS"
REFERENCES,0.3611111111111111,"We present some theoretical ﬁndings about the regret difference between the partial-information
feedback and the full-information feedback. Assuming that the agent in the CBB setting can observe
the rewards of all the candidate actions from the environment at each step, we apply the batched UCB
policy (Han et al., 2020) to this setting (see Algorithm 3), We demonstrate an instantaneous regret
bound in Theorem 5, where Theorem 5 is a detailed version of Theorem 1 in Section 2."
REFERENCES,0.3626543209876543,Algorithm 3 Batch UCB Policy Updating in the (n+1)-th episode in Full-Information CBB Setting
REFERENCES,0.36419753086419754,"INPUT: Policy pn, data buffer Dn+1, action space A = {Aj}j∈[M], θ0
Aj = 0, j ∈[M], batch size B
OUTPUT: Updated policy pn+1"
REFERENCES,0.36574074074074076,"1: Let eLn ∈R(n+1)B×d be the matrix that stores all the context vectors till the n-th episode as the row
vectors
2: For ∀A ∈A, let eT n
A ∈R(n+1)B be the reward vector that stores all the rewards of action A ∈A till the
n-th episode
3: // Policy Updating
4: Υn+1 ←eL⊺
n eLn
5: for all action A ∈A do
6:
θn+1
A
←(Id + Υn+1)−1 eL⊺
n eT n
A
7: end for
8: For a new context s, pn+1(s) is to choose the action following: A ←arg max
A∈A"
REFERENCES,0.36728395061728397,"θn+1
A
, s"
REFERENCES,0.36882716049382713,"9: return

θn+1
A A∈A"
REFERENCES,0.37037037037037035,Under review as a conference paper at ICLR 2022
REFERENCES,0.37191358024691357,"Theorem 5 (Instantaneous Regret Bound in Full-Information CBB Setting, Detailed Version of
Theorem 1). Let eLn−1 ∈RnB×d be the matrix that stores all the context vectors till the (n −1)-th
episode as the row vectors, and eT n−1
A
∈RnB be the reward vector that stores all the rewards of
action A ∈A till the (n −1)-th episode. Given the action space A = {Aj}j∈[M], in the n-th
episode, assume that the rewards are independent and bounded by CR. Then, with probability at
least 1 −δ, for any b ∈[B] and ∀A ∈A, we have the following instantaneous regret bound in the
n-th episode"
REFERENCES,0.3734567901234568,"|⟨θn
A, sn,b⟩−⟨θ∗
A, sn,b⟩| ≤

∥θ∗
A∥2 +
q"
REFERENCES,0.375,"2C2
R log(2MB/δ)
 q"
REFERENCES,0.3765432098765432,"s⊺
n,b (Id + Υn)−1 sn,b,
(11)"
REFERENCES,0.37808641975308643,"where Υn = eL⊺
n−1 eLn−1 and the parameter of reward model θn
A in the batched UCB policy is
obtained by"
REFERENCES,0.37962962962962965,"θn
A := arg min
θ∈Rd"
REFERENCES,0.38117283950617287,"eLn−1θ −eT n−1
A

2"
REFERENCES,0.38271604938271603,"2 + ∥θ∥2
2 = (Id + Υn)−1 eL⊺
n−1 eT n−1
A
."
REFERENCES,0.38425925925925924,"Further, the instantaneous regret bound Eq.(11) in FI-CBB setting is tighter than that in
CBB setting (i.e., using the partial-information feedback).
In particular, the variance term
q"
REFERENCES,0.38580246913580246,"s⊺
n,b (Id + Υn)−1 sn,b is smaller than that in CBB setting."
REFERENCES,0.3873456790123457,"Proof of Theorem 1. By the formulation of θn
A and the triangle inequality, we ﬁrst obtain that"
REFERENCES,0.3888888888888889,"|⟨θn
A, sn,b⟩−⟨θ∗
A, sn,b⟩|"
REFERENCES,0.3904320987654321,"=
s⊺
n,b (Id + Υn)−1 eL⊺
n−1 eT n−1
A
−s⊺
n,bθ∗
A"
REFERENCES,0.39197530864197533,"=
s⊺
n,b (Id + Υn)−1 h
eL⊺
n−1 eT n−1
A
−(Id + Υn) θ∗
A
i"
REFERENCES,0.39351851851851855,"=
s⊺
n,b (Id + Υn)−1 h
eL⊺
n−1 eT n−1
A
−

Id + eL⊺
n−1 eLn−1

θ∗
A
i"
REFERENCES,0.3950617283950617,"=
s⊺
n,b (Id + Υn)−1 eL⊺
n−1

eT n−1
A
−eLn−1θ∗
A

−s⊺
n,b (Id + Υn)−1 θ∗
A"
REFERENCES,0.3966049382716049,"≤
s⊺
n,b (Id + Υn)−1 eL⊺
n−1

eT n−1
A
−eLn−1θ∗
A
 +
s⊺
n,b (Id + Υn)−1 θ∗
A (12)"
REFERENCES,0.39814814814814814,"Next, we bound the two terms in the last row of Eq.(12)."
REFERENCES,0.39969135802469136,"Bounding
s⊺
n,b (Id + Υn)−1 eL⊺
n−1

eT n−1
A
−eLn−1θ∗
A
:"
REFERENCES,0.4012345679012346,"Since E
h
eT n−1
A
i
= eLn−1θ∗
A and the received rewards are independent, by the Azuma-Hoeffding
bound, we have"
REFERENCES,0.4027777777777778,"Pr
s⊺
n,b (Id + Υn)−1 eL⊺
n−1

eT n−1
A
−eLn−1θ∗
A
 ≥ν
q"
REFERENCES,0.404320987654321,"s⊺
n,b (Id + Υn)−1 sn,b "
REFERENCES,0.4058641975308642,≤2 exp (
REFERENCES,0.4074074074074074,"−
ν2s⊺
n,b (Id + Υn)−1 sn,b"
REFERENCES,0.4089506172839506,"2C2
R∥eLn−1 (Id + Υn)−1 sn,b∥2
2 )"
REFERENCES,0.4104938271604938,",
(13)"
REFERENCES,0.41203703703703703,"where ν > 0 is some constant. Since
∥eLn−1 (Id + Υn)−1 sn,b∥2
2 = s⊺
n,b (Id + Υn)−1 eL⊺
n−1 eLn−1 (Id + Υn)−1 sn,b"
REFERENCES,0.41358024691358025,"≤s⊺
n,b (Id + Υn)−1 
Id + eL⊺
n−1 eLn−1

(Id + Υn)−1 sn,b"
REFERENCES,0.41512345679012347,"≤s⊺
n,b (Id + Υn)−1 (Id + Υn) (Id + Υn)−1 sn,b"
REFERENCES,0.4166666666666667,"= s⊺
n,b (Id + Υn)−1 sn,b,"
REFERENCES,0.4182098765432099,combing with Eq.(13) implies the following results
REFERENCES,0.41975308641975306,"Pr
s⊺
n,b (Id + Υn)−1 eL⊺
n−1

eT n−1
A
−eLn−1θ∗
A
 ≥ν
q"
REFERENCES,0.4212962962962963,"s⊺
n,b (Id + Υn)−1 sn,b "
REFERENCES,0.4228395061728395,"≤2 exp

−ν2 2C2
R"
REFERENCES,0.4243827160493827,"
.
(14)"
REFERENCES,0.42592592592592593,Under review as a conference paper at ICLR 2022
REFERENCES,0.42746913580246915,"Combing Eq.(14) with the union bound, yields that, with probability at least 1 −δ, for any b ∈[B]
and ∀A ∈A,
s⊺
n,b (Id + Υn)−1 eL⊺
n−1

eT n−1
A
−eLn−1θ∗
A
 ≤ν
q"
REFERENCES,0.42901234567901236,"s⊺
n,b (Id + Υn)−1 sn,b,
(15)"
REFERENCES,0.4305555555555556,where the failure probability is
REFERENCES,0.43209876543209874,"δ = 2MB exp

−ν2 2C2
R 
,"
REFERENCES,0.43364197530864196,"yielding that ν =
p"
REFERENCES,0.4351851851851852,"2C2
R log(2MB/δ)."
REFERENCES,0.4367283950617284,"Bounding
s⊺
n,b (Id + Υn)−1 θ∗
A
:"
REFERENCES,0.4382716049382716,"Since Υn is positive semi-deﬁnite, combining with the H¨older inequality, we obtain
s⊺
n,b (Id + Υn)−1 θ∗
A
 ≤∥θ∗
A∥2
(Id + Υn)−1 sn,b

2"
REFERENCES,0.4398148148148148,"= ∥θ∗
A∥2 q"
REFERENCES,0.44135802469135804,"sn,b (Id + Υn)−1 (Id + Υn)−1 sn,b"
REFERENCES,0.44290123456790126,"≤∥θ∗
A∥2 q"
REFERENCES,0.4444444444444444,"sn,b (Id + Υn)−1 (Id + Υn) (Id + Υn)−1 sn,b"
REFERENCES,0.44598765432098764,"= ∥θ∗
A∥2 q"
REFERENCES,0.44753086419753085,"sn,b (Id + Υn)−1 sn,b. (16)"
REFERENCES,0.44907407407407407,Combing Eq.(15) and Eq.(16) concludes the proof.
REFERENCES,0.4506172839506173,"Similarly to the proof of Eq.(29), we can obtain that the variance term in full-information setting is
smaller than that in partial-information setting."
REFERENCES,0.4521604938271605,"B
DETAILED PROOFS IN THEORETICAL ANALYSIS"
REFERENCES,0.4537037037037037,"In this section, we provide the instantaneous regret bound in each episode, prove the approximation
error of sketching, and analyze the regret for policy updating in the CBB setting. Figure 4 describes
the dependence structure of our theoretical results."
REFERENCES,0.45524691358024694,Regret Bound
REFERENCES,0.4567901234567901,of SPUIR
REFERENCES,0.4583333333333333,Approximation Properties
REFERENCES,0.45987654320987653,of Sketching
REFERENCES,0.46141975308641975,Approximation Error Bound
REFERENCES,0.46296296296296297,of Reward Imputation
REFERENCES,0.4645061728395062,Instantaneous Regret Bound
REFERENCES,0.4660493827160494,"Figure 4: The dependence structure of our theoretical results, where the proof of instantaneous re-
gret bound (Theorem 2) is provided in Appendix B.1, the analysis of approximation properties of
sketching (Theorem 6) is given in Appendix B.2, the approximation error bound of reward imputa-
tion (Theorem 3) is proven in Appendix B.3, and the regret of SPUIR (Theorem 4) is analyzed in
Appendix B.4"
REFERENCES,0.4675925925925926,Under review as a conference paper at ICLR 2022
REFERENCES,0.4691358024691358,"B.1
PROOF OF THEOREM 2"
REFERENCES,0.470679012345679,"Before we provide the detailed proof of Theorem 2, we ﬁrst demonstrate a lemma about the conver-
gence and monotonicity of the sum of functions, which is the main tool for analyzing the additional
bias of reward imputation."
REFERENCES,0.4722222222222222,"Lemma 1 (Convergence and Monotonicity). Let f(n) = Pn
j=1 an−j · g(j), where a ∈(0, 1) and
n is a positive integer. Then,"
REFERENCES,0.4737654320987654,"1) when g(j) is convergent, the limit limn→∞f(n) exists. Moreover,"
REFERENCES,0.47530864197530864,"lim
n→∞f(n) =
1
1 −a lim
n→∞g(n).
(17)"
REFERENCES,0.47685185185185186,"2) f(n) is a monotonic decreasing function if and only if g(j) satisﬁes, for any positive integer
j ≥2,"
REFERENCES,0.4783950617283951,"g(j) ≤ 
 "
REFERENCES,0.4799382716049383,"(j −1)aj−1g(1)
a = 1/2,
(1 −a)

aj−1 −(1 −a)j−1"
REFERENCES,0.48148148148148145,"2a −1
g(1)
a ̸= 1/2.
(18)"
REFERENCES,0.48302469135802467,"Proof. Letting b(j) = a−j · g(j), ∀j ∈[n], and S(n) = Pn
j=1 b(j), f(n) can be rewritten as
f(n) = anS(n)."
REFERENCES,0.4845679012345679,"1) Rewriting f(n) = S(n)/a−n, from the Stolz’s theorem, we have"
REFERENCES,0.4861111111111111,"lim
n→∞f(n) = lim
n→∞
S(n) −S(n −1)"
REFERENCES,0.4876543209876543,a−n −a−(n−1)
REFERENCES,0.48919753086419754,"= lim
n→∞
b(n)
a−n −a−(n−1)"
REFERENCES,0.49074074074074076,"= lim
n→∞
a−n · g(n)
a−n −a−(n−1)"
REFERENCES,0.49228395061728397,"=
1
1 −a lim
n→∞g(n)."
REFERENCES,0.49382716049382713,"2) The condition that f(·) is a monotonic decreasing function is equivalent to the following condi-
tion: for and positive integer n,"
REFERENCES,0.49537037037037035,f(n + 1) ≤f(n)
REFERENCES,0.49691358024691357,"⇔an+1S(n + 1) ≤anS(n)
⇔a[S(n) + b(n + 1)] ≤S(n)
⇔b(n + 1) ≤(1/a −1) S(n).
(19)"
REFERENCES,0.4984567901234568,"From the equivalent condition Eq.(19), we obtain the following recursion formula:"
REFERENCES,0.5,b(n + 1) ≤(1/a −1) S(n)
REFERENCES,0.5015432098765432,"b(n) ≤(1/a −1) n−1
X"
REFERENCES,0.5030864197530864,"j=1
b(j)"
REFERENCES,0.5046296296296297,"...
b(3) ≤(1/a −1) [b(1) + b(2)]
b(2) ≤(1/a −1) b(1),"
REFERENCES,0.5061728395061729,"yielding that, for any positive integer j ≥2,"
REFERENCES,0.5077160493827161,"b(j) ≤

(1/a −1) + (1/a −1)2 + · · · + (1/a −1)j−1
b(1).
(20)"
REFERENCES,0.5092592592592593,Under review as a conference paper at ICLR 2022
REFERENCES,0.5108024691358025,"From Eq.(20) , for a ̸= 1/2,"
REFERENCES,0.5123456790123457,"b(j) ≤(1/a −1)

1 −(1/a −1)j−1"
REFERENCES,0.5138888888888888,"1 −(1/a −1)
b(1)"
REFERENCES,0.5154320987654321,"= (1 −a)

1 −(1/a −1)j−1"
REFERENCES,0.5169753086419753,"2a −1
b(1), (21)"
REFERENCES,0.5185185185185185,and substituting the deﬁnition of b(j) into Eq.(21) yields the equivalent condition
REFERENCES,0.5200617283950617,"g(j) ≤(1 −a)

1 −(1/a −1)j−1"
REFERENCES,0.5216049382716049,"2a −1
aj−1 g(1)."
REFERENCES,0.5231481481481481,"For a = 1/2, we have the condition b(j) ≤(j −1)b(1), which is equivalent to"
REFERENCES,0.5246913580246914,a−j · g(j) ≤(j −1)a−1 · g(1) ⇔g(j) ≤(j −1)aj−1g(1).
REFERENCES,0.5262345679012346,"Next, we provide the detailed proof of Theorem 2."
REFERENCES,0.5277777777777778,"Proof of Theorem 2. From the formulation of ¯θn
A and the triangle inequality, we can obtain that, for
each action A ∈A,

¯θn
A, sn,b

−⟨θ∗
A, sn,b⟩"
REFERENCES,0.529320987654321,"=
s⊺
n,b (Ψn
A)−1 
bn
A + γˆbn
A

−s⊺
n,bθ∗
A"
REFERENCES,0.5308641975308642,"=
s⊺
n,b (Ψn
A)−1 h 
Ln−1
A
⊺T n−1
A
+ γ

bLn−1
A
⊺bT n−1
A
−Ψn
Aθ∗
A
i"
REFERENCES,0.5324074074074074,"=
s⊺
n,b (Ψn
A)−1 h 
Ln−1
A
⊺T n−1
A
+ γ

bLn−1
A
⊺bT n−1
A
−

λId + Φn
A + γ bΦn
A

θ∗
A
i"
REFERENCES,0.5339506172839507,"=
s⊺
n,b (Ψn
A)−1 n 
Ln−1
A
⊺T n−1
A
+ γ

bLn−1
A
⊺bT n−1
A
−
h
λId +
 
Ln−1
A
⊺Ln−1
A
+ γ

bLn−1
A
⊺bLn−1
A
i
θ∗
A
o"
REFERENCES,0.5354938271604939,"=
s⊺
n,b (Ψn
A)−1  
Ln−1
A
⊺ 
T n−1
A
−Ln−1
A
θ∗
A

−λs⊺
n,b (Ψn
A)−1 θ∗
A+"
REFERENCES,0.5370370370370371,"s⊺
n,b (Ψn
A)−1 γ

bLn−1
A
⊺
bT n−1
A
−bLn−1
A
θ∗
A
"
REFERENCES,0.5385802469135802,"≤
s⊺
n,b (Ψn
A)−1  
Ln−1
A
⊺ 
T n−1
A
−Ln−1
A
θ∗
A

|
{z
}"
REFERENCES,0.5401234567901234,"X(1)
A"
REFERENCES,0.5416666666666666,"+ λ
s⊺
n,b (Ψn
A)−1 θ∗
A

|
{z
}"
REFERENCES,0.5432098765432098,"X(2)
A +"
REFERENCES,0.5447530864197531,"s⊺
n,b (Ψn
A)−1 γ

bLn−1
A
⊺
bT n−1
A
−bLn−1
A
θ∗
A

|
{z
}"
REFERENCES,0.5462962962962963,"X(3)
A ."
REFERENCES,0.5478395061728395,"Next, we bound X(1)
A , X(2)
A , and X(3)
A . For convenience, we drop all the superscripts and subscripts
about n and b. Similarly to the proof of Theorem 1, we bound X(1)
A
+ X(2)
A
as follows: with
probability at least 1 −δ,"
REFERENCES,0.5493827160493827,"X(1)
A + X(2)
A
≤(λ∥θ∗
A∥2 + ν)
q"
REFERENCES,0.5509259259259259,"s⊺Ψ−1
A s,
(22)"
REFERENCES,0.5524691358024691,"where ν =
p"
REFERENCES,0.5540123456790124,"2C2
R log(2MB/δ). For X(3)
A , using the Cauchy-Schwarz inequality, we have"
REFERENCES,0.5555555555555556,"X(3)
A
≤γ
bLAΨ−1
A s

2"
REFERENCES,0.5570987654320988,"bTA −bLAθ∗
A

2 = √γ r"
REFERENCES,0.558641975308642,"s⊺Ψ−1
A

γ bL⊺
A bLA

Ψ−1
A s
 bTA −bLAθ∗
A

2 ≤√γ
q"
REFERENCES,0.5601851851851852,"s⊺Ψ−1
A s
 bTA −bLAθ∗
A

2 . (23)"
REFERENCES,0.5617283950617284,Under review as a conference paper at ICLR 2022
REFERENCES,0.5632716049382716,"Now we need to bound the term
 bTA −bLAθ∗
A

2. Since using the discount parameter η ∈(0, 1)
in Eq.(4) is equivalent to multiplying both the imputed contexts and the imputed rewards by the
parameter √η in each episode, we have, in the n-th episode,
 bT n−1
A
−bLn−1
A
θ∗
A

2 =
∆η
n−1

2 ,
(24)"
REFERENCES,0.5648148148148148,"where ∆η
n−1 = {η(n−i−1)/2 IRi,b}i∈[n−1],b∈[B] denotes an exponential-decay vector of the in-
stantaneous regrets, and IRi,b denotes the instantaneous regret at step b in the i-th episode, i.e,
IRi,b =

¯θi
A, si,b

−⟨θ∗
A, si,b⟩
. From Eq.(24), letting"
REFERENCES,0.566358024691358,"CIRi =
X"
REFERENCES,0.5679012345679012,"b∈[B]
IRi,b
(25)"
REFERENCES,0.5694444444444444,"be the cumulative instantaneous regret in the i-th episode, we can obtain the upper bound of Eq.(24)
as follows:
 bT n−1
A
−bLn−1
A
θ∗
A

2 =
∆η
n−1

2"
REFERENCES,0.5709876543209876,"≤
∆η
n−1

1 =
X"
REFERENCES,0.5725308641975309,"i∈[n−1],b∈[B]"
REFERENCES,0.5740740740740741,"η(n−i−1)/2 IRi,b =
X"
REFERENCES,0.5756172839506173,"i∈[n−1]
η(n−i−1)/2 CIRi = η−1"
REFERENCES,0.5771604938271605,"2 fImp(n), (26)"
REFERENCES,0.5787037037037037,"where
fImp(n) :=
X"
REFERENCES,0.5802469135802469,"i∈[n−1]
(√η)n−i CIRi.
(27)"
REFERENCES,0.5817901234567902,"From monotone bounded theorem, we have that the limit of CIRi exists. From Eq.(17) in Lemma 1,
we get that fImp(n) is convergent and then has an upper bound. We denotes the upper bound of
fImp(n) by CImp > 0, and then from Eq.(26) we have
 bT n−1
A
−bLn−1
A
θ∗
A

2 ≤η−1"
REFERENCES,0.5833333333333334,"2 CImp.
(28)"
REFERENCES,0.5848765432098766,"Substituting Eq.(28) into Eq.(23) yields the upper bound of X(3)
A ."
REFERENCES,0.5864197530864198,"Then, we prove that
q"
REFERENCES,0.5879629629629629,"s⊺(Ψn
A)−1 s ≤
q"
REFERENCES,0.5895061728395061,"s⊺(λId + Φn
A)−1 s.
(29)
holds, which is equivalent to"
REFERENCES,0.5910493827160493,"s⊺
λId + Φ + γ bΦ
−1
s ≤s⊺(λId + Φ)−1 s.
(30)"
REFERENCES,0.5925925925925926,"Letting Θ = λId + Φ, by Sherman-Morrison-Woodbury formula, we have

Θ + γ bΦ
−1
=

Θ + γ bS⊺bS
−1
= Θ−1 −γΘ−1 bS⊺
Id + γ bSΘ−1 bS⊺−1 bSΘ−1"
REFERENCES,0.5941358024691358,"= Θ−1 −Θ−1 bS⊺
Id"
REFERENCES,0.595679012345679,"γ + bSΘ−1 bS⊺
−1
bSΘ−1,
(31)"
REFERENCES,0.5972222222222222,"yielding that Eq.(30) is equivalent to
s⊺Γs ≥0,
(32)
where
Γ = Θ−1 bS⊺
Id/γ + bSΘ−1 bS⊺−1 bSΘ−1."
REFERENCES,0.5987654320987654,"Let S = UdΣ1/2
d
V ⊺
d , bS = bUd bΣ1/2
d
bV ⊺
d be the Singular Value Decomposition (SVD) of S and bS,
respectively. Note that Φ = VdΣdV ⊺
d , bΦ = bVd bΣd bV ⊺
d . We can obtain that Γ is a square symmetric
positive semi-deﬁnite matrix, since Γ can be decomposed into"
REFERENCES,0.6003086419753086,"Γ = Q⊺Q,"
REFERENCES,0.6018518518518519,Under review as a conference paper at ICLR 2022
REFERENCES,0.6033950617283951,"where PγΛγP ⊺
γ is the SVD of Id/γ + bSΘ−1 bS⊺and"
REFERENCES,0.6049382716049383,"Q = Λ−1/2
γ
P ⊺
γ bSΘ−1."
REFERENCES,0.6064814814814815,"Thus, Eq.(32) holds, yielding that Eq.(30) also holds."
REFERENCES,0.6080246913580247,"Finally, we prove that a larger imputation rate γ leads to a smaller variance term
q"
REFERENCES,0.6095679012345679,"s⊺(Ψ)−1 s.
From Eq.(31), the variance term can be represented as follows:
q"
REFERENCES,0.6111111111111112,"s⊺(Ψ)−1 s =
h
s⊺Θ−1s −s⊺Θ−1 bS⊺M −1
γ
bSΘ−1s
i1/2
,
(33)"
REFERENCES,0.6126543209876543,"where Mγ = Id/γ + bSΘ−1 bS⊺. Letting Mγ = UMγΛMγU ⊺
Mγ be the SVD of Mγ, and z ="
REFERENCES,0.6141975308641975,"U ⊺
Mγ bSΘ−1s, from Eq.(33) we can written the variance term as follows:
q"
REFERENCES,0.6157407407407407,"s⊺(Ψ)−1 s =
h
s⊺Θ−1s −z⊺Λ−1
Mγz
i1/2
.
(34)"
REFERENCES,0.6172839506172839,"In Eq.(34), we can observed that"
REFERENCES,0.6188271604938271,"z⊺ΛMγz = ∥(ΛMγ)−1/2z∥2
2 ∈

1
σmax(M) + 1/γ ∥z∥2
2,
1
σmin(M) + 1/γ ∥z∥2
2 "
REFERENCES,0.6203703703703703,"where M = bSΘ−1 bS⊺, which indicates that a larger imputation rate γ leads to a smaller variance
term."
REFERENCES,0.6219135802469136,"Finally, we provide a deeper understanding of the additional bias in Theorem 2."
REFERENCES,0.6234567901234568,"Remark 4 (Controllable Bias). Our reward imputation approach incurs a bias term γ
1
2 η−1"
CIMP,0.625,"2 CImp
in addition to the two bias terms λ∥θ∗
A∥2 and ν that exist in every UCB-based policy. But this
additional bias term is controllable due to the presence of imputation rate γ that can help controlling
the additional bias. Moreover, from the proof of Eq.(26), we can obtain that, the term CImp in the
additional bias can be replaced by a function fImp(n) (deﬁned in Eq.(27)), and the additional bias
term turns out to be γ
1
2 η−1"
CIMP,0.6265432098765432,"2 fImp(n). Since fImp(n) has the same functional form as the function
f(n) in Lemma 1, we can ﬁnd the conditions that fImp(n) is monotonic decreasing following Eq.(18)
in Lemma 1. Speciﬁcally, letting CIRi be the cumulative instantaneous regret in the i-th episode
deﬁned in Eq.(25),"
CIMP,0.6280864197530864,"1) when √η ̸= 1/2, the condition of a monotonic decreasing function fImp(·) is equivalent to, for
any positive integer i ≥2,"
CIMP,0.6296296296296297,"CIRi ≤
(1 −√η)
h√ηi−1 −(1 −√η)i−1i"
CIMP,0.6311728395061729,"2√η −1
CIR1,"
CIMP,0.6327160493827161,indicating that the regret after N episodes satisﬁes X
CIMP,0.6342592592592593,"2≤i≤N
CIRi ≤CIR1
X 2≤i≤N"
CIMP,0.6358024691358025,"(1 −√η)
h√ηi−1 −(1 −√η)i−1i"
CIMP,0.6373456790123457,2√η −1
CIMP,0.6388888888888888,"= CIR1
1 −√η
2√η −1 X 2≤i≤N"
CIMP,0.6404320987654321,h√ηi−1 −(1 −√η)i−1i
CIMP,0.6419753086419753,"= CIR1
1
√η(2√η −1)

2√η −1 + (1 −√η)N+1 −(√η)N+1"
CIMP,0.6435185185185185,"= CIR1
1
√η"
CIMP,0.6450617283950617,"
1 + (1 −√η)N+1 −(√η)N+1"
CIMP,0.6466049382716049,2√η −1
CIMP,0.6481481481481481,"
.
(35)"
CIMP,0.6496913580246914,"2) for the case √η = 1/2, the condition of a monotonic decreasing function fImp(·) is equivalent
to CIRi ≤(i −1)(√η)i−1CIR1 for any positive integer i ≥2, indicating that the regret after"
CIMP,0.6512345679012346,Under review as a conference paper at ICLR 2022
CIMP,0.6527777777777778,"N episodes satisﬁes
X"
CIMP,0.654320987654321,"2≤i≤N
CIRi ≤CIR1
X"
CIMP,0.6558641975308642,"2≤i≤N
(i −1)(√η)i−1"
CIMP,0.6574074074074074,"=
√η
(1 −√η)2 CIR1 −

1
(1 −√η)2 + N −1 1 −√η"
CIMP,0.6589506172839507,"
(√η)NCIR1"
CIMP,0.6604938271604939,"=

2 −1 + N 2N−1"
CIMP,0.6620370370370371,"
CIR1"
CIMP,0.6635802469135802,"=
 1
√η −(1 + N)√ηN−1

CIR1.
(36)"
CIMP,0.6651234567901234,"From Eq.(35) and Eq.(36), we can conclude that a monotonic decreasing function fImp(·) indicates
the upper bound of regret after N episodes is of order O(CIR1/√η). The conclusion also indicates
that setting the discount parameter as √η = Θ(CIR1/N) achieves a O(N) regret bound (i.e., a
˜O(
√"
CIMP,0.6666666666666666,"dT) regret bound following Remark 3). Note that setting the discount parameter as √η =
Θ(CIR1/N) is a mild condition, since the cumulative instantaneous regret CIR1 is typically of
order O(B) (B = O(
p"
CIMP,0.6682098765432098,"T/d) in Remark 3) yielding that √η = Θ(d−1). Overall, since a larger
imputation rate γ leads to a smaller variance while increasing the bias (variance analysis can be
found in Remark 2), γ controls a trade-off between the bias term and the variance term. When
fImp is a monotonic decreasing function w.r.t. number of episodes n, the additional bias term
γ
1
2 η−1"
CIMP,0.6697530864197531,"2 fImp(n) can be easily controlled, e.g., gradually increasing γ with the number of episodes,
avoiding the large bias from fImp(n) at the beginning of reward imputation. We design a rate-
scheduled approach for choosing the imputation rate γ in Section 5.
Remark 5 (Relationship to Exploration and Exploitation Trade-off). Exploration-exploitation
dilemma is the key challenge in online learning under bandit settings. In the full-information set-
ting, agent (e.g., UCB policy) receives the rewards from all the actions, does not need to consider
the choice of exploring the feedback mechanisms, and achieves a lower variance part in the re-
gret upper bound (Theorem 1). Along this line, our reward computation approach is proposed to
approximate the setting of full-information feedback, which somewhat relaxes the explore/exploit
dilemma and also brings a lower variance part and a controllable additional bias part in the regret.
Extra information that pushes the policy towards exploitation and away from exploration comes
from the estimated reward structures of the non-executed actions maintained in each episode, and
the proposed reward imputation can be seen as an effective and efﬁcient tool to capture this extra
information."
CIMP,0.6712962962962963,"B.2
APPROXIMATION PROPERTIES OF SKETCHING"
CIMP,0.6728395061728395,"Although some error bounds of approximation using SJLT have been proposed (Nelson & Nguyˆen,
2013; Kane & Nelson, 2014; Bourgain et al., 2015), it is still unknown what is the lower bound
of the sketch size while applying SJLT to the sketched ridge regression problem in our SPUIR.
To address this issue, we ﬁrst prove two approximation properties of SJLT which are necessary to
achieve approximation error bound of the sketched ridge regression using SJLT. For convenience,
we drop all the superscripts and subscripts in these theoretical results.
Lemma 2 ((Nelson & Nguyˆen, 2013)). Let U ∈RL×d be a matrix with orthonormal columns,
Π ∈Rc×L the SJLT. Assuming that D = Θ(ε−1
σ log3(dδ−1
0 )) for Π, εσ ∈(0, 1) and d ≤c, with
probability at least 1 −δ0 all singular values of ΠU"
CIMP,0.6743827160493827,"σi(ΠU) = 1 ± εσ,
i ∈[d],"
CIMP,0.6759259259259259,as long as
CIMP,0.6774691358024691,"c ≥d log8  
dδ−1
0
 ε2σ
."
CIMP,0.6790123456790124,"Further, this holds if the hash function h and σ deﬁning the Π is Ω
 
log(dδ−1
0 )

-wise independent."
CIMP,0.6805555555555556,"Theorem 6 (Approximation Properties of SJLT). Let U ∈RL×d be a matrix with orthonormal
columns, and A be a matrix of any proper size. If Π ∈Rc×L is the SJLT satisfying the assumptions
in Lemma 2, and d ≤c ≤L, then Π has the following two properties:"
CIMP,0.6820987654320988,Under review as a conference paper at ICLR 2022
CIMP,0.683641975308642,"1) Subspace embedding property: set c = Ω
 
d polylog
 
dδ−1
s

/ε2
s

, for εs ∈(0, 1), with proba-
bility at least 1 −δs,
∥U ⊺Π⊺ΠU −Id∥2 ≤εs;"
CIMP,0.6851851851851852,"2) Matrix multiplication property: set c = Ω(d/(εmδm)), for εm ∈(0, 1), with probability at least
1 −δm,
∥U ⊺Π⊺ΠA −U ⊺A∥2
F ≤εm∥A∥2
F."
CIMP,0.6867283950617284,"Proof of Theorem 6. 1) From Lemma 2, by setting c = Ω
 
d polylog
 
dδ−1
s

/ε2
0

, we can obtain
the upper bounds of eigenvalues: with probability at least 1 −δs,"
CIMP,0.6882716049382716,"λi (U ⊺Π⊺ΠU) = σ2
i (ΠU) ∈[(1 −εσ)2, (1 + εσ)2] ⊆[1 −2εσ, 1 + 3εσ],
(37)"
CIMP,0.6898148148148148,"which yields that
|λi (U ⊺Π⊺ΠU −Id)| ≤3εσ.
(38)"
CIMP,0.691358024691358,"Eq.(39) is equivalent to
∥U ⊺Π⊺ΠU −Id∥2 ≤3εσ."
CIMP,0.6929012345679012,"Letting εs = 3εσ and εσ ∈(0, 1/3) yields the subspace embedding property."
CIMP,0.6944444444444444,"2) From Lemma 1 in (Zhang & Liao, 2019), we have"
CIMP,0.6959876543209876,"E

∥U ⊺Π⊺ΠA −U ⊺A∥2
F

≤2"
CIMP,0.6975308641975309,"c ∥U∥2
F∥A∥2
F = 2d"
CIMP,0.6990740740740741,"c ∥A∥2
F.
(39)"
CIMP,0.7006172839506173,"Combining Eq.(39) with the Markov’s inequality, we obtain that, with probability at least 1−δm,"
CIMP,0.7021604938271605,"∥U ⊺Π⊺ΠA −U ⊺A∥2
F ≤2d"
CIMP,0.7037037037037037,"δmc∥A∥2
F."
CIMP,0.7052469135802469,Letting εm = 2d
CIMP,0.7067901234567902,δmc yields the matrix multiplication property.
CIMP,0.7083333333333334,"B.3
PROOF OF THEOREM 3"
CIMP,0.7098765432098766,"Next, using the approximation properties of SJLT in Theorem 6, we prove that the objective function
value of the imputation regularized ridge regression problem for reward imputation can be approx-
imated well with a relative-error bound. Moreover, we prove that the solution solving the sketched
ridge regression problem for reward imputation is also a good approximation of the solution solving
the imputation regularized ridge regression. The following theorem is a detailed version of Theo-
rem 3."
CIMP,0.7114197530864198,"Theorem 7 (Approximation Error Bound of Imputation using Sketching, Detailed Version of The-
orem 3). Let γ ∈[0, 1] be the imputation rate, λ > 0 the regularization parameter, Π ∈Rc×L"
CIMP,0.7129629629629629,"and bΠ ∈Rc×bL be the SJLT, and L ∈RL×d, bL ∈RbL×d, T ∈RL, bT ∈RbL, θ ∈Rd. Denote the
imputation regularized ridge regression function F and sketched ridge regression function F S for
reward imputation by"
CIMP,0.7145061728395061,"F(θ) = ∥Lθ −T ∥2
2 + γ
bLθ −bT

2"
CIMP,0.7160493827160493,"2 + λ∥θ∥2
2,"
CIMP,0.7175925925925926,"F S(θ) = ∥Π (Lθ −T )∥2
2 + γ
 bΠ

bLθ −bT

2"
CIMP,0.7191358024691358,"2 + λ∥θ∥2
2,"
CIMP,0.720679012345679,and the solutions of these regression problems by
CIMP,0.7222222222222222,"¯θ = arg min
θ∈Rd
F(θ)
and
˜θ = arg min
θ∈Rd
F S(θ)."
CIMP,0.7237654320987654,"Let δ ∈(0, 0.1], ε ∈(0, 1), ρλ = ∥Lall∥2
2/(∥Lall∥2
2 + λ). For Π and bΠ, assuming that D =
Θ(ε−1 log3(dδ−1)) and
c = Ω
 
d polylog
 
dδ−1
/ε2
,"
CIMP,0.7253086419753086,Under review as a conference paper at ICLR 2022
CIMP,0.7268518518518519,"with probability at least 1 −δ,"
CIMP,0.7283950617283951,"F(˜θ) ≤(1 + ρλε)F(¯θ),
(40)"
CIMP,0.7299382716049383,∥˜θ −¯θ∥2 ≤ q
CIMP,0.7314814814814815,"ρλεF
 ¯θ
"
CIMP,0.7330246913580247,"σmin
 
Lλ
all
 ,
(41)"
CIMP,0.7345679012345679,"where Lλ
all =
h
L; √γ bL;
√"
CIMP,0.7361111111111112,"λId
i
∈R(L+bL+d)×d. Furthermore, if there is a constant fraction of"
CIMP,0.7376543209876543,"the norm of T 0
all lies in the column space of Lλ
all, then Eq.(41) can be strengthened. Formally,
assuming that a mild structural assumption on the context matrix and the reward vector is satisﬁed,
i.e., ∥UallU ⊺
allT 0
all∥2 ≥ξ∥T 0
all∥2 with a constant ξ ∈(0, 1], then with probability at least 1 −δ,"
CIMP,0.7391975308641975,"∥˜θ −¯θ∥2 ≤

κ(Lλ
all)
p"
CIMP,0.7407407407407407,"ξ−2 −1
 √ρλϵ∥¯θ∥2,
(42)"
CIMP,0.7422839506172839,"where κ(A) denotes the condition number of A, T 0
all = [T ; bT ; 0d] ∈R(L+bL+d), and Lλ
all =
UallΣallV ⊺
all is the SVD of Lλ
all."
CIMP,0.7438271604938271,"Proof of Theorem 7. We ﬁrst introduce some more notation of block matrices that will simplify
the proof of the theorem:"
CIMP,0.7453703703703703,"Πall =
 Π
O
O
bΠ"
CIMP,0.7469135802469136,"
,
Lall =

L
√γ bL"
CIMP,0.7484567901234568,"
,
Tall =
 T
bT"
CIMP,0.75,"
.
(43)"
CIMP,0.7515432098765432,Then the regression functions can be rewritten as follows:
CIMP,0.7530864197530864,"F(θ) = ∥Lallθ −Tall∥2
2 + λ∥θ∥2
2,
F S(θ) = ∥Πall (Lallθ −Tall)∥2
2 + λ∥θ∥2
2."
CIMP,0.7546296296296297,"Obviously, Πall is still an SJLT. Combining Theorem 6 with theorem 19 in (Wang et al., 2017), we
can obtain, setting
c = Ω
 
max{d polylog
 
dδ−1
s

/ε2
s, d/(εmδm)}

,"
CIMP,0.7561728395061729,"with probability at least 1 −(δs + δm),"
CIMP,0.7577160493827161,"F(˜θ) −F(¯θ) ≤ρλτF(¯θ),
(44)"
CIMP,0.7592592592592593,"where ρλ =
∥Lall∥2
2
∥Lall∥2
2+λ and τ = 2 max{ε2
s ,εm}
1−εs
. Letting εs = εm := ε0, Eq.(44) can be rewritten as"
CIMP,0.7608024691358025,F(˜θ) −F(¯θ) ≤2ρλε0
CIMP,0.7623456790123457,"1 −ε0
F(¯θ),
(45)"
CIMP,0.7638888888888888,"Assuming that δs = δm := δ/2 ∈(0, 0.1] and ε0 ∈(0, 1/3), setting ϵ =
2ε0
1−ε0 ∈(0, 1), from
Eq.(45) we obtain the upper bound Eq.(40)."
CIMP,0.7654320987654321,"Next, we bound the difference between the solutions solving the sketched ridge regression problem
and the original regression problem. Since σ2
min(A)∥x∥2
2 ≤∥Ax∥2
2 for any A and x with proper
sizes, we have"
CIMP,0.7669753086419753,"σ2
min(Lall)∥˜θ −¯θ∥2
2 ≤
Lall(˜θ −¯θ)

2"
CIMP,0.7685185185185185,"2 .
(46)"
CIMP,0.7700617283950617,"The key ingredient of bounding ∥˜θ−¯θ∥2 is to bound ∥Lall(˜θ−¯θ)∥2. Let Lλ
all =
h
L; √γ bL;
√"
CIMP,0.7716049382716049,"λId
i
∈"
CIMP,0.7731481481481481,"R(L+bL+d)×d, T 0
all = [T ; bT ; 0d] ∈R(L+bL+d), Lλ
all = UallΣallV ⊺
all be the SVD of Lλ
all, and denote
a matrix with orthonormal columns by U ⊥
all ∈R(L+bL+d)×(L+bL) which satisﬁes"
CIMP,0.7746913580246914,"UallU ⊺
all + U ⊥
all(U ⊥
all)⊺= IL+bL+d
and
U ⊺
allU ⊥
all = O."
CIMP,0.7762345679012346,"Then, we can rewrite the solution ¯θ as follows:"
CIMP,0.7777777777777778,"¯θ = arg min
θ∈Rd
F(θ) = arg min
θ∈Rd"
CIMP,0.779320987654321,"Lλ
allθ −T 0
all
2 2"
CIMP,0.7808641975308642,"= (Lλ
all)†T 0
all = VallΣ−1
all U ⊺
allT 0
all,"
CIMP,0.7824074074074074,Under review as a conference paper at ICLR 2022
CIMP,0.7839506172839507,"which yields that
T 0
all −Lλ
all ¯θ = T 0
all −Lλ
allVallΣ−1
all U ⊺
allT 0
all
= T 0
all −UallΣallV ⊺
allVallΣ−1
all U ⊺
allT 0
all
= T 0
all −UallU ⊺
allT 0
all
= U ⊥
all(U ⊥
all)
⊺T 0
all. (47)"
CIMP,0.7854938271604939,"Thus, T 0
all −Lλ
all ¯θ is orthogonal to Uall, and consequently to Lλ
all(˜θ −¯θ), and we can obtain the
following equality by Pythagoras’s theorem:
Lλ
all(˜θ −¯θ)

2"
CIMP,0.7870370370370371,"2 =
Lλ
all ˜θ −T 0
all

2"
CIMP,0.7885802469135802,"2 −
Lλ
all ¯θ −T 0
all
2"
CIMP,0.7901234567901234,"2 .
(48)"
CIMP,0.7916666666666666,"Combining Eq.(48) with Eq.(40) yields that
Lλ
all(˜θ −¯θ)

2"
CIMP,0.7932098765432098,"2 = F(˜θ) −F(¯θ) ≤ρλεF(¯θ).
(49)"
CIMP,0.7947530864197531,Substituting Eq.(49) into Eq.(46) concludes the proof of Eq.(41).
CIMP,0.7962962962962963,"If we make a mild structural assumption on the context matrix and the reward vector, we can provide
a stronger bound of ∥˜θ −¯θ∥2. Speciﬁcally, assuming that ∥UallU ⊺
allT 0
all∥2 ≥ξ∥T 0
all∥2 with a
constant ξ ∈(0, 1], from Eq.(47) and Pythagoras’s theorem we have"
CIMP,0.7978395061728395,"F(¯θ) = ∥Lλ
all ¯θ −T 0
all∥2
2
= ∥T 0
all∥2
2 −∥UallU ⊺
allT 0
all∥2
2
≤(ξ−2 −1)∥UallU ⊺
allT 0
all∥2
2
= (ξ−2 −1)∥Lλ
all ¯θ∥2
2
≤(ξ−2 −1)∥Lλ
all∥2
2 ∥¯θ∥2
2
≤(ξ−2 −1)σ2
max(Lλ
all)∥¯θ∥2
2. (50)"
CIMP,0.7993827160493827,Combining Eq.(50) with Eq.(41) yields Eq.(42).
CIMP,0.8009259259259259,"B.4
PROOF OF THEOREM 4"
CIMP,0.8024691358024691,"Proof of Theorem 4. In our sketched policy, letting Cmax
θ∗
= maxA∈A ∥θ∗
A∥2, CImp > 0, ν =
p"
CIMP,0.8040123456790124,"2C2
R log(2MB/δ), and
ω = λCmax
θ∗
+ ν + γ
1
2 η−1"
CIMP,0.8055555555555556,"2 CImp,"
CIMP,0.8070987654320988,"from Eq.(9) in Theorem 2 we obtain that

¯θn
A, sn,b

−⟨θ∗
A, sn,b⟩
 ≤ω
q"
CIMP,0.808641975308642,"s⊺
n,b (Ψn
A)−1 sn,b.
(51)"
CIMP,0.8101851851851852,"Before proving the upper bound of

D
˜θn
A, sn,b
E
−⟨θ∗
A, sn,b⟩
, we need to provide a technical tool
as follows. For convenience, we also drop all the superscripts and subscripts. The goal is to ﬁnd a
constant Cα such that
√"
CIMP,0.8117283950617284,"s⊺Ψ−1s ≤Cα
√"
CIMP,0.8132716049382716,"s⊺W −1s,
(52)"
CIMP,0.8148148148148148,"which is equivalent to the condition that the matrix C2
αW −1 −Ψ−1 is positive semideﬁnite.
Let Lall and Πall be the matrices deﬁned in Eq.(43), Lall = eUall eΣall eV ⊺
all be the SVD of Lall,
and ˜σ1 ≥˜σ2 · · · ≥˜σd be the singular values of Lall.
Then the i-th eigenvalue of Ψ−1 =
(λId + L⊺
allLall)−1 can be represented as λi(Ψ−1) = 1/(˜σ2
i + λ), and the i-th eigenvalue of
W −1 = (λId + L⊺
allΠ⊺
allΠallLall)−1 is λi(W −1) = 1/(ˆλi + λ), where ˆλi is the i-th eigenvalue of
eΣall eU ⊺
allΠ⊺
allΠall eUall eΣall."
CIMP,0.816358024691358,"From the Lidskii’s theorem and Eq.(37), we have"
CIMP,0.8179012345679012,"ˆλi ∈[˜σ2
d(1 −2εσ), ˜σ2
1(1 + 3εσ)].
(53)"
CIMP,0.8194444444444444,Under review as a conference paper at ICLR 2022
CIMP,0.8209876543209876,"Assuming that the positive semi-deﬁniteness of C2
αW −1 −Ψ−1 is satisﬁed, we obtain that
C2
αλi(W −1) −λi(Ψ−1) ≥0 for i ∈[d], and combining this inequality with Eq.(53) yields that"
CIMP,0.8225308641975309,"Cα =
q"
CIMP,0.8240740740740741,"[˜σ2
1(1 + 3εσ) + λ]/(˜σ2
d + λ)."
CIMP,0.8256172839506173,"From the proof of Theorem 6 and Theorem 3, we can obtain that εσ = ε/(6 + 3ε), yielding that Cα = s"
CIMP,0.8271604938271605,"˜σ2
1[1 + ε/(2 + ε)] + λ"
CIMP,0.8287037037037037,"˜σ2
d + λ
,"
CIMP,0.8302469135802469,"which decreases with increase of 1/ε. Similarly to the proof of Cα satisfying Eq.(52), we can obtain
that
√"
CIMP,0.8317901234567902,"s⊺W −1s ≤Creg
√"
CIMP,0.8333333333333334,"s⊺Ψ−1s,
(54)"
CIMP,0.8348765432098766,provided that
CIMP,0.8364197530864198,Creg = s
CIMP,0.8379629629629629,"˜σ2
1 + λ
˜σ2
d[1 −2ε/(6 + 3ε)] + λ."
CIMP,0.8395061728395061,"Obviously, Creg also decreases with increase of 1/ε."
CIMP,0.8410493827160493,"Then, letting α = ωCα, from Eq.(51) and Eq.(52) we have"
CIMP,0.8425925925925926,"D
˜θn
A, sn,b
E
−⟨θ∗
A, sn,b⟩
 ≤

D
˜θn
A, sn,b
E
−

¯θn
A, sn,b
 +

¯θn
A, sn,b

−⟨θ∗
A, sn,b⟩"
CIMP,0.8441358024691358,"≤

D
˜θn
A −¯θn
A, sn,b
E + ω
q"
CIMP,0.845679012345679,"s⊺
n,b (Ψn
A)−1 sn,b"
CIMP,0.8472222222222222,"≤Yn,b + α
q"
CIMP,0.8487654320987654,"s⊺
n,b (W n
A)−1 sn,b, (55)"
CIMP,0.8503086419753086,"where Yn,b denotes the upper bound of

D
˜θn
A −¯θn
A, sn,b
E for any A ∈A."
CIMP,0.8518518518518519,"Next, using the compatibility of norm, we give a speciﬁc representation of the sum of Yn,b as follows: X"
CIMP,0.8533950617283951,"b∈[B]
Yn,b = max
A∈A ∥Sn
A(˜θn
A −¯θn
A)∥1"
CIMP,0.8549382716049383,"≤max
A∈A ∥Sn
A∥1∥˜θn
A −¯θn
A∥1"
CIMP,0.8564814814814815,"≤max
A∈A ∥Sn
A∥1
√"
CIMP,0.8580246913580247,"d∥˜θn
A −¯θn
A∥2. (56)"
CIMP,0.8595679012345679,"Further, we give a more speciﬁc upper bound in Eq.(56) under mild structural assumption in Theo-
rem 3. Let κmax
all
denote the maximum of the condition numbers of Lλ
all(A, n) for A ∈A, n ∈[N],"
CIMP,0.8611111111111112,"and Lλ
all(A, n) =
h
Ln
A; √γ bLn
A;
√"
CIMP,0.8626543209876543,"λId
i
, and Uall(A, n) be the left singular matrix of Lλ
all(A, n)."
CIMP,0.8641975308641975,"Letting T 0
all(A, n)
=
[T n
A; bT n
A; 0d], assuming that ∥Uall(A, n)Uall(A, n)⊺T 0
all(A, n)∥2
≥
ξ∥T 0
all(A, n)∥2 with a constant ξ ∈(0, 1], substituting the upper bound Eq.(42) in Theorem 3 into
Eq.(56) yields that X"
CIMP,0.8657407407407407,"b∈[B]
Yn,b ≤

κmax
all
p"
CIMP,0.8672839506172839,"ξ−2 −1

CSCmax
¯θ
p"
CIMP,0.8688271604938271,"ρλϵd,
(57)"
CIMP,0.8703703703703703,"where CS = maxn∈[N],A∈A ∥Sn
A∥1, Cmax
¯θ
= maxA∈A,n∈[N] ∥¯θn
A∥2."
CIMP,0.8719135802469136,Under review as a conference paper at ICLR 2022
CIMP,0.8734567901234568,"From Eq.(54), Eq.(55), Eq.(57) and the deﬁnition of our sketched policy, letting CY
=

κmax
all
p"
CIMP,0.875,"ξ−2 −1

CSCmax
¯θ
, we obtain that"
CIMP,0.8765432098765432,"Reg({AIn,b}n∈[N],b∈[B]) =
X"
CIMP,0.8780864197530864,"n∈[N],b∈[B]"
CIMP,0.8796296296296297,"
max
A∈A ⟨θ∗
A, sn,b⟩−
D
θ∗
AIn,b , sn,b
E ≤
X"
CIMP,0.8811728395061729,"n∈[N],b∈[B]"
CIMP,0.8827160493827161,"
max
A∈A"
CIMP,0.8842592592592593,"D
˜θn
A, sn,b
E
+ α
q"
CIMP,0.8858024691358025,"s⊺
n,b (W n
A)−1 sn,b"
CIMP,0.8873456790123457,"
+ Yn,b −
D
θ∗
AIn,b , sn,b
E =
X"
CIMP,0.8888888888888888,"n∈[N],b∈[B]"
CIMP,0.8904320987654321,"""D
˜θn
AIn,b , sn,b
E
+ α r"
CIMP,0.8919753086419753,"s⊺
n,b

W n
AIn,b"
CIMP,0.8935185185185185,"−1
sn,b + Yn,b −
D
θ∗
AIn,b , sn,b
E# ≤2α
X"
CIMP,0.8950617283950617,"n∈[N],b∈[B] r"
CIMP,0.8966049382716049,"s⊺
n,b

W n
AIn,b"
CIMP,0.8981481481481481,"−1
sn,b + 2
X"
CIMP,0.8996913580246914,"n∈[N],b∈[B]
Yn,b"
CIMP,0.9012345679012346,"≤2αCreg
√ B
X n∈[N]"
CIMP,0.9027777777777778,"v
u
u
t X"
CIMP,0.904320987654321,"b∈[B]
s⊺
n,b

Ψn
AIn,b"
CIMP,0.9058641975308642,"−1
sn,b + 2NCY
p ρλϵd"
CIMP,0.9074074074074074,"= 2αCreg
√ B
X n∈[N]"
CIMP,0.9089506172839507,"v
u
u
t X b∈[B]"
CIMP,0.9104938271604939,"sn,b s⊺
n,b,

Ψn
AIn,b"
CIMP,0.9120370370370371,"−1
+ 2NCY
p ρλϵd"
CIMP,0.9135802469135802,"= 2αCreg
√ B
X n∈[N] s X A∈A"
CIMP,0.9151234567901234,"D
Sn⊺
A Sn
A, (Ψn
A)−1E
+ 2NCY
p ρλϵd"
CIMP,0.9166666666666666,"= 2αCreg
√ B
X n∈[N] s X"
CIMP,0.9182098765432098,"A∈A
tr

Sn⊺
A Sn
A (Ψn
A)−1
+ O(N
p"
CIMP,0.9197530864197531,"ρλϵd),"
CIMP,0.9212962962962963,"≤2αCreg
√ BM
X n∈[N] r"
CIMP,0.9228395061728395,"max
A∈A"
CIMP,0.9243827160493827,"n
tr

Sn⊺
A Sn
A (Ψn
A)−1o
+ O(N
p"
CIMP,0.9259259259259259,"ρλϵd).
(58)"
CIMP,0.9274691358024691,"When the structural assumption in Theorem 3 is not satisﬁed, from Eq.(41), we can obtain that the
second term in Eq.(58) is also of order O(√ρλϵd), which does not inﬂuence the order of the ﬁnal
regret bound. Finally, combining Eq.(58) with lemma 3 in (Han et al., 2020) gives the ﬁnal regret
bound."
CIMP,0.9290123456790124,"C
DETAILED EXPERIMENTAL SETTINGS AND MORE EXPERIMENTAL
RESULTS"
CIMP,0.9305555555555556,"In this section, we provide more details and results in the experiments."
CIMP,0.9320987654320988,"C.1
DESCRIPTION OF DATASETS"
CIMP,0.933641975308642,Table 3 summarizes the description of datasets used in the experiments.
CIMP,0.9351851851851852,"Next, we provide more details about the three datasets."
CIMP,0.9367283950617284,"Synthetic Data. Inspired by the experiments in (Saito et al., 2020), the synthetic data generation
procedure was formulated as follows, which simulates the streaming recommendation environment."
CIMP,0.9382716049382716,"• Context si ∈Rd: we drew elements of si independently from a Gaussian distribution
N(0.1, 0.22), where d = 40;"
CIMP,0.9398148148148148,"• Click-Through-Rate (CTR): the CTRs for the 5 actions were respectively set as
{10%, 15%, 25%, 20%, 30%};"
CIMP,0.941358024691358,Under review as a conference paper at ICLR 2022
CIMP,0.9429012345679012,"Table 3: Description of datasets in the experiments (T: number of instances; B: batch size; N:
number of episodes; d: dimensionality of context; M: number of actions; CB satisfying B =
C2
BN/d)"
CIMP,0.9444444444444444,"Dataset
T
B
N
d
M
CB"
CIMP,0.9459876543209876,"synthetic data
126,000
1,400
90
40
5
25.00
Criteo-recent
75,000
1,000
75
50
5
25.82
Criteo-all
1,276,000
4,000
319
50
15
25.04
commercial product
216,568
1,700
128
50
5
25.83"
CIMP,0.9475308641975309,• The indicator variables of click events:
CIMP,0.9490740740740741,"Ci =
1
a click occurs in context si,
0
otherwise."
CIMP,0.9506172839506173,"We sampled the click index set according to the uniform distribution.
• Conversion rate (CVR) in context si: when Ci = 1,"
CIMP,0.9521604938271605,"CVR(si) := sigmoid(⟨wc, si⟩), =
1
1 + exp(−⟨wc, si⟩),"
CIMP,0.9537037037037037,"where the coefﬁcient vector wc ∈Rd is sampled according to a Gaussian distribution as
wc ∼N(κc1d, σ2
cId), and we set different means and standard deviations for different
action with κc ∈[0 : −0.2 : −0.8] and σc ∈[0.01 : +0.01 : 0.05];"
CIMP,0.9552469135802469,"Criteo Data. We used the publicly available Criteo dataset5, consisting of Criteo’s trafﬁc on display
ads over a period of two months (Chapelle, 2014), where each context consists of 8 integer features
and 9 categorical features. Following the experiments in (Yoshikawa & Imai, 2018), the categorical
features were represented as one-hot vectors and then concatenated to the integer features. We
reduced the dimensionality of the feature vectors to 50 using principal component analysis (PCA).
All of the algorithms were tested in a simulated online environment that was trained on users’ logs
in the Criteo dataset. Speciﬁcally, we chose several campaigns from the Criteo dataset, where each
campaign represents a category of items and corresponds to an action. This online environment
contains a prediction model for the CVR, which was well trained by applying DFM (Chapelle,
2014) using the true user feedbacks. This environment model was trained for each chosen campaign,
whose AUCs are ranging from 70% to 90%, assuring that the online environment can provide nearly
realistic feedbacks. To simulate the uncertainty of user behaviors, Gaussian noises with zero-mean
were added to the model parameters. At each step, the online environment randomly selected a
campaign and samples one context from this campaign, and revealed the context to the agent with
a preset CTR. To generate a reasonable sequence of instances, the environment kept the order of
timestamps of the contexts in each campaign. We tested our algorithms and the baselines with the
following two online environments on the Criteo dataset: Criteo-recent contains 5 campaigns
(75, 000 instances) chosen from the recent campaigns, corresponding to 5 actions; Criteo-all
contains 15 campaigns (1, 276, 000 instances) chosen from all the campaigns, corresponding to 15
actions."
CIMP,0.9567901234567902,"Data Collected from a Real Commercial App for Coupon Recommendation. To verify the ef-
fectiveness and efﬁciency of our algorithms on real products, we conducted experiments on a real
dataset collected from a commercial social app. We call this dataset commercial product,
where the data were collected after the users gave consent, and did not contain any personally iden-
tiﬁable information or offensive content. Since this dataset from a commercial app is proprietary,
we did not provide a URL. We will release this dataset after the publication of this paper. In this
commercial app, after clicking a recommended coupon, a user may convert the coupon after some
time, or just leave it there. The dataset was collected during a 1-month period with a subsampling,
and consists of 216, 568 instances from 5 categories of coupons, where each context is described by
86 numerical features and 16 categorical features. The timestamps of clicks and conversions were"
CIMP,0.9583333333333334,5https://labs.criteo.com/2013/12/conversion-logs-dataset/
CIMP,0.9598765432098766,Under review as a conference paper at ICLR 2022
CIMP,0.9614197530864198,"also recorded. Following the settings on the Criteo data, we also represented the categorical features
as a one-hot vector, reduced the dimensionality of the feature vectors to 50 by PCA. The action
space contains 5 actions, where each corresponding to one coupon category. Due to the limitation
of real online experiments, in this experiment, we still trained DFM using the true user feedbacks as
the online environment, where AUCs range from 75% to 90%."
CIMP,0.9629629629629629,"To simulate the real environment under partial-information feedback, The experiments were con-
ducted in environments where the distribution of the initialization data is atypical. Speciﬁcally,
in the experiments, we set different numbers of the initialization instances for each action. In
the synthetic environment, we set the number of the initial instances as 140, 210, 350, 280, 420
for the 5 actions, respectively.
In Criteo-recent, we set the proportion of the initial in-
stances as 0.1, 0.15, 0.25, 0.2, 0.3 for the 5 actions, and set the number of the initial instances as
[100 : 23 : 423] for the 15 actions in Criteo-all."
CIMP,0.9645061728395061,"C.2
DETAILED SPECIFICATION OF HYPERPARAMETERS"
CIMP,0.9660493827160493,"In these experiments, the true reward is deﬁned by Rtrue
i,A = λcCi,A + (1 −λc)Vi,A (Ci,A and Vi,A
denote true binary variables of user click and conversion when executing action A given context si),
where λc = 0.01 on the synthetic data, Criteo Data, and commercial product data, respectively. As in
most contextual bandit literature (Li et al., 2010; Chu et al., 2011), we set the regularization parame-
ter λ = 1 in the Euclidean regularization. According to theoretical analysis in Remark 3, we set the
batch size as B = C2
BN/d, set the constant CB ≈25 and the sketch size c = 150 on all the dataset-
s (B = 1400, 1000, 4000, 1700 for synthetic data, Criteo-recent, Criteo-all, and
commercial product). The regularization parameters ω, α in our policy and that in the batch
UCB policy were tuned in [0.2 : +0.2 : 1.2]. For the SJLT in SPUIR and its variants, sketch
size was set as c = 150 and the number of block D was selected in {1, 2, 4, 6}. Except for the
rate-scheduled variants of our approaches, the imputation rate γ was selected in [0.1 : +0.2 : 0.9].
Besides, the discount parameter η was tuned in [0.1 : +0.2 : 0.9]. In the nonlinear variant of our
approach SPUIR-Kernel, we selected the dimension of the random features dr in {50, 100, 200} and
the kernel width of Gaussian kernel in {2−(i+1)/2, i = [−12 : 2 : 12]}."
CIMP,0.9675925925925926,"Rate-Scheduled Approach.
We equip PUIR and SPUIR with a rate-scheduled approach, called
PUIR-RS and SPUIR-RS, respectively. We design a rate-scheduled approach following the theoret-
ical results about the imputation rate γ. From Remark 1&2, we can obtain that a larger imputation
rate γ leads to a smaller variance while increasing the bias. From Remark 4, we conclude that the
additional bias term includes a monotonic decreasing function w.r.t. number of episodes under mild
conditions. Therefore, instead of using a ﬁxed imputation rate, we can gradually increase γ with the
number of episodes, avoiding the large bias at the beginning of the reward imputation while achiev-
ing a small variance. Speciﬁcally, we set γ = X% for episodes from (X −10)% × N to X% × N,
where X ∈[10, 100]."
CIMP,0.9691358024691358,"C.3
MORE EXPERIMENTAL RESULTS"
CIMP,0.970679012345679,"For better illustration, in Figure 3 of the manuscript, we omitted the curves of algorithms whose av-
erage rewards are 5% lower than the highest reward. Now we provide the curves of all the algorithms
in Figure 5."
CIMP,0.9722222222222222,Under review as a conference paper at ICLR 2022
CIMP,0.9737654320987654,"10
20
30
40
50
60
70
80
90
Number of Episodes N 0.12 0.14 0.16 0.18 0.20 0.22 0.24"
CIMP,0.9753086419753086,Average Reward
CIMP,0.9768518518518519,"PUIR
SPUIR
PUIR-RS
SPUIR-RS
BLTS-B
SBUCB
BEXP3-IPW
DFM-S
BEXP3"
CIMP,0.9783950617283951,(a) synthetic data
CIMP,0.9799382716049383,"0
10
20
30
40
50
60
70
Number of Episodes N 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.27"
CIMP,0.9814814814814815,Average Reward
CIMP,0.9830246913580247,"PUIR
SPUIR
PUIR-RS
SPUIR-RS
BLTS-B
SBUCB
BEXP3-IPW
DFM-S
BEXP3"
CIMP,0.9845679012345679,(b) Criteo-recent
CIMP,0.9861111111111112,"0
50
100
150
200
250
300
Number of Episodes N 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26"
CIMP,0.9876543209876543,Average Reward
CIMP,0.9891975308641975,"PUIR
SPUIR
PUIR-RS
SPUIR-RS
BLTS-B
SBUCB
BEXP3-IPW
DFM-S
BEXP3"
CIMP,0.9907407407407407,(c) Criteo-all
CIMP,0.9922839506172839,"20
40
60
80
100
120
Number of Episodes N 0.250 0.275 0.300 0.325 0.350 0.375 0.400 0.425 0.450"
CIMP,0.9938271604938271,Average Reward
CIMP,0.9953703703703703,"PUIR
SPUIR
PUIR-RS
SPUIR-RS
BLTS-B
SBUCB
BEXP3-IPW
DFM-S
BEXP3"
CIMP,0.9969135802469136,(d) commercial product
CIMP,0.9984567901234568,"Figure 5: Average rewards of the compared algorithms, the proposed SPUIR and its variants on
synthetic dataset, Criteo dataset, and the real commercial product data"
