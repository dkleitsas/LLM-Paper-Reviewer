Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008583690987124463,"Bayesian optimization (BO) mainly uses Gaussian processes (GP) with a stationary
1"
ABSTRACT,0.0017167381974248926,"and separable kernel function (e.g., the squared-exponential kernel with automatic
2"
ABSTRACT,0.002575107296137339,"relevance determination [SE-ARD]) as the surrogate model. However, such lo-
3"
ABSTRACT,0.0034334763948497852,"calized kernel specifications are deficient in learning complex functions that are
4"
ABSTRACT,0.004291845493562232,"non-stationary, non-separable and multi-modal. In this paper, we propose using
5"
ABSTRACT,0.005150214592274678,"Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate model for
6"
ABSTRACT,0.006008583690987125,"Bayesian optimization (BO) in a D-dimensional grid with both continuous and
7"
ABSTRACT,0.0068669527896995704,"categorical variables. Our key idea is to approximate the underlying D-dimensional
8"
ABSTRACT,0.007725321888412017,"solid with a fully Bayesian low-rank tensor CP decomposition, in which we place
9"
ABSTRACT,0.008583690987124463,"GP priors on the latent basis functions for each dimension to encode local consis-
10"
ABSTRACT,0.00944206008583691,"tency and smoothness. With this formulation, the information from each sample
11"
ABSTRACT,0.010300429184549357,"can be shared not only with neighbors but also across dimensions, thus fostering a
12"
ABSTRACT,0.011158798283261802,"more global search strategy. Although BKTF no longer has an analytical posterior,
13"
ABSTRACT,0.01201716738197425,"we efficiently approximate the posterior distribution through Markov chain Monte
14"
ABSTRACT,0.012875536480686695,"Carlo (MCMC). We conduct numerical experiments on several test functions with
15"
ABSTRACT,0.013733905579399141,"continuous variables and two machine learning hyperparameter tuning problems
16"
ABSTRACT,0.014592274678111588,"with mixed variables. The results show that BKTF offers a flexible and highly
17"
ABSTRACT,0.015450643776824034,"effective approach to characterizing and optimizing complex functions, especially
18"
ABSTRACT,0.01630901287553648,"in cases where the initial sample size and budget are severely limited.
19"
INTRODUCTION,0.017167381974248927,"1
Introduction
20"
INTRODUCTION,0.018025751072961373,"For many applications in science and engineering, such as emulation-based studies, experiment
21"
INTRODUCTION,0.01888412017167382,"design, and automated machine learning, the goal is to optimize a complex black-box function f(x)
22"
INTRODUCTION,0.019742489270386267,"in a D-dimensional space, for which we have limited prior knowledge. The main challenge in
23"
INTRODUCTION,0.020600858369098713,"such optimization problems is that we aim to efficiently find the global optima, while the objective
24"
INTRODUCTION,0.02145922746781116,"function f is often gradient-free, multimodal and computationally expensive to evaluate. Bayesian
25"
INTRODUCTION,0.022317596566523604,"optimization (BO) offers a powerful statistical approach to these problems, particularly when the
26"
INTRODUCTION,0.02317596566523605,"observation budgets are limited [1, 2, 3]. A typical BO framework consists of two components—a
27"
INTRODUCTION,0.0240343347639485,"surrogate model and an acquisition function (AF)—to balance exploitation and exploration. The
28"
INTRODUCTION,0.024892703862660945,"surrogate is a probabilistic model that allows us to estimate f(x) with uncertainty at a new location
29"
INTRODUCTION,0.02575107296137339,"x, and the AF is used to determine which location to query next.
30"
INTRODUCTION,0.026609442060085836,"Gaussian process (GP) regression is the most widely used surrogate for BO [3, 4], thanks to its
31"
INTRODUCTION,0.027467811158798282,"appealing properties in providing analytical derivations and uncertainty quantification (UQ). The
32"
INTRODUCTION,0.02832618025751073,"choice of kernel/covariance function is a critical decision in GP models; for multidimensional
33"
INTRODUCTION,0.029184549356223177,"BO problems, perhaps the most popular kernel is the ARD (automatic relevance determination)—
34"
INTRODUCTION,0.030042918454935622,"Squared-Exponential (SE) or Matérn kernel [4]. Although this specification has certain numerical
35"
INTRODUCTION,0.030901287553648068,"advantages and can help automatically learn the importance of input variables, a key limitation is
36"
INTRODUCTION,0.03175965665236052,"Figure 1: BO for a 2D function: (a) True function surface, where the global maximum is marked; (b)
Comparison between BO models using GP surrogates (with two AFs) and BKTF with 30 random
initial observations, averaged over 20 replications; (c) Specific results of one run, including the final
estimated mean surface for f, in which green dots denote the locations of the selected candidates,
and the corresponding AF surface."
INTRODUCTION,0.03261802575107296,"that it implies/assumes that the underlying stochastic process is stationary and separable, and the
37"
INTRODUCTION,0.03347639484978541,"value of the covariance function between two random points quickly goes to zero with the increase
38"
INTRODUCTION,0.034334763948497854,"of input dimensionality. These assumptions can be problematic for complex real-world processes
39"
INTRODUCTION,0.0351931330472103,"with long-range dependencies, because estimating the underlying function with a simple ARD kernel
40"
INTRODUCTION,0.036051502145922745,"would require a large number of observations. A potential solution to address this issue is to use
41"
INTRODUCTION,0.03690987124463519,"more flexible kernel structures. The additive kernel, for example, is designed to characterize a more
42"
INTRODUCTION,0.03776824034334764,"“global” structure by restricting variable interactions [5]. However, in practice using additive kernels
43"
INTRODUCTION,0.03862660944206009,"requires strong prior knowledge to determine the proper interactions and involves a large number
44"
INTRODUCTION,0.039484978540772535,"of kernel hyperparameters to learn [6]. Another emerging solution is to use deep GP [7], such as in
45"
INTRODUCTION,0.04034334763948498,"[8, 9]; however, learning deep GP often becomes a more challenging task due to the inference of
46"
INTRODUCTION,0.041201716738197426,"latent layers. In addition, these GP related surrogates can be more deficient to tune when taking into
47"
INTRODUCTION,0.04206008583690987,"account both continuous and categorical inputs.
48"
INTRODUCTION,0.04291845493562232,"In this paper, we propose using Bayesian Kernelized Tensor Factorization (BKTF) as a flexible
49"
INTRODUCTION,0.04377682403433476,"and adaptive surrogate model for BO in a D-dimensional Cartesian product space (i.e., grid) when
50"
INTRODUCTION,0.04463519313304721,"D is relatively small (say D ≤10). BKTF is initially developed for modeling multidimensional
51"
INTRODUCTION,0.045493562231759654,"spatiotemporal data with UQ, for tasks such as spatiotemporal kriging/cokriging [10, 11]. This
52"
INTRODUCTION,0.0463519313304721,"paper adapts BKTF to the BO setting, and our key idea is to characterize the multivariate objective
53"
INTRODUCTION,0.04721030042918455,"function f (x) = f (x1, . . . , xD) for a specific BO problem using the low-rank tensor CANDE-
54"
INTRODUCTION,0.048068669527897,"COMP/PARAFAC (CP) factorization with random basis functions. Unlike other basis-function
55"
INTRODUCTION,0.048927038626609444,"models that rely on known/deterministic basis functions [12], BKTF uses a hierarchical Bayesian
56"
INTRODUCTION,0.04978540772532189,"framework to achieve high-quality UQ in a more flexible way—GP priors are used to model the basis
57"
INTRODUCTION,0.050643776824034335,"functions, and hyperpriors are used to model kernel hyperparameters in particular for the lengthscale
58"
INTRODUCTION,0.05150214592274678,"that characterizes the scale of variation. In addition, BKTF also provides a natural solution for
59"
INTRODUCTION,0.05236051502145923,"categorical variables, for which we can simply introduce an inverse-Wishart prior on the covariance
60"
INTRODUCTION,0.05321888412017167,"matrix of the basis functions.
61"
INTRODUCTION,0.05407725321888412,"Figure 1 shows the comparison between BKTF and GP surrogates when optimizing a bivariate
62"
INTRODUCTION,0.054935622317596564,"function (D = 2) that is nonstationary, nonseparable, and multimodal. The details of this function
63"
INTRODUCTION,0.055793991416309016,"and the BO experiments are provided in Appendix C. This 2D case clearly shows that GP surrogate
64"
INTRODUCTION,0.05665236051502146,"is limited by the local kernel and becomes ineffective in finding the global solution, while BKTF
65"
INTRODUCTION,0.05751072961373391,"offers superior flexibility and adaptability to characterize the multidimensional process from limited
66"
INTRODUCTION,0.05836909871244635,"data. Unlike GP-based surrogate models, BKTF no longer has an analytical posterior; however,
67"
INTRODUCTION,0.0592274678111588,"efficient inference and acquisition can be achieved through Markov chain Monte Carlo (MCMC)
68"
INTRODUCTION,0.060085836909871244,"in an element-wise learning way, in which we update basis functions and kernel hyperparameters
69"
INTRODUCTION,0.06094420600858369,"using Gibbs sampling and slice sampling, respectively [11]. For optimization, we first use MCMC
70"
INTRODUCTION,0.061802575107296136,"samples to approximate the posterior distribution of the entire tensor and then naturally define the
71"
INTRODUCTION,0.06266094420600858,"upper confidence bound (UCB) of the posterior as AF. This process is feasible for many real-world
72"
INTRODUCTION,0.06351931330472103,"applications that can be studied in a discretized tensor product space, such as experimental design.
73"
INTRODUCTION,0.06437768240343347,"We conduct extensive experiments on both standard optimization and ML hyperparameter tuning
74"
INTRODUCTION,0.06523605150214593,"tasks. Our results show that BKTF achieves a fast global search for optimizing complex objective
75"
INTRODUCTION,0.06609442060085836,"functions with limited initial data and budget.
76"
BAYESIAN OPTIMIZATION,0.06695278969957082,"2
Bayesian optimization
77"
BAYESIAN OPTIMIZATION,0.06781115879828326,"Let f : X = X1 × . . . × XD →R be a black-box function that could be nonconvex, derivative-free,
78"
BAYESIAN OPTIMIZATION,0.06866952789699571,"and expensive to evaluate. BO aims to address the global optimization problem:
79"
BAYESIAN OPTIMIZATION,0.06952789699570816,"x⋆= arg max
x∈X
f(x).
(1)"
BAYESIAN OPTIMIZATION,0.0703862660944206,"BO solves this problem by first building a probabilistic model for f(x) (i.e., surrogate model)
80"
BAYESIAN OPTIMIZATION,0.07124463519313305,"based on initial observations and then using the predictive distribution to decide where in X to
81"
BAYESIAN OPTIMIZATION,0.07210300429184549,"evaluate/query next. The overall goal of BO is to find the global optimum of the objective function
82"
BAYESIAN OPTIMIZATION,0.07296137339055794,"using as few evaluations as possible. Most BO models rely on a GP prior for f(x) to achieve
83"
BAYESIAN OPTIMIZATION,0.07381974248927038,"prediction and UQ:
84"
BAYESIAN OPTIMIZATION,0.07467811158798283,"f(x) = f(x1, . . . , xD) ∼GP (m (x) , k (x, x′)) , with xd ∈Xd, d = 1, . . . , D,
(2)
where k (·, ·) is a valid kernel/covariance function and m (·) is a mean function that can be generally
85"
BAYESIAN OPTIMIZATION,0.07553648068669527,"assumed to be 0. Given a finite set of observation points {xi}n
i=1 with xi =
 
xi
1, . . . , xi
D
⊤,
86"
BAYESIAN OPTIMIZATION,0.07639484978540773,"the vector of function values f = (f(x1), . . . , f(xn))⊤has a multivariate Gaussian distribution
87"
BAYESIAN OPTIMIZATION,0.07725321888412018,"f ∼N (0, K), where K is the n×n covariance matrix. For a set of observed data Dn = {xi, yi}n
i=1
88"
BAYESIAN OPTIMIZATION,0.07811158798283262,"with i.i.d. Gaussian noise, i.e., yi = f(xi) + ϵi where ϵi ∼N(0, τ −1), GP gives an analytical
89"
BAYESIAN OPTIMIZATION,0.07896995708154507,"posterior distribution of f(x) at an unobserved point x∗:
90"
BAYESIAN OPTIMIZATION,0.07982832618025751,"f(x∗) | Dn ∼N

kx∗X
 
K + τ −1In
−1 y, k(x∗, x∗) −kx∗X
 
K + τ −1In
−1 k⊤
x∗X

, (3)"
BAYESIAN OPTIMIZATION,0.08068669527896996,"where kx∗X = [k(x∗, x1), . . . , k(x∗, xn)]⊤and y = (y1, . . . , yn)⊤.
91"
BAYESIAN OPTIMIZATION,0.0815450643776824,"Algorithm 1: Basic BO process.
Input: Initial dataset D0 and a trained
surrogate model; total budget N.
for n = 1, . . . , N do"
BAYESIAN OPTIMIZATION,0.08240343347639485,"Approximate the posterior distribution of
f using the surrogate model based on
Dn−1;
Find next evaluation point xn by
optimizing the AF;
Augment data Dn = Dn−1 ∪{xn, yn},
update the surrogate model."
BAYESIAN OPTIMIZATION,0.08326180257510729,"Based on the posterior distributions of f, one can
92"
BAYESIAN OPTIMIZATION,0.08412017167381974,"compute an AF, denoted by α : X →R, for a
93"
BAYESIAN OPTIMIZATION,0.08497854077253218,"new candidate x∗and evaluate how promising x∗
94"
BAYESIAN OPTIMIZATION,0.08583690987124463,"is. In BO, the next query point is often determined
95"
BAYESIAN OPTIMIZATION,0.08669527896995709,"by maximizing a selected/predefined AF, i.e., xn+1 =
96"
BAYESIAN OPTIMIZATION,0.08755364806866953,"arg maxx∈X α (x | Dn). Most AFs are based on the
97"
BAYESIAN OPTIMIZATION,0.08841201716738198,"predictive mean and variance; for example, a com-
98"
BAYESIAN OPTIMIZATION,0.08927038626609442,"monly used AF is the expected improvement (EI)
99"
BAYESIAN OPTIMIZATION,0.09012875536480687,"[1]:
100"
BAYESIAN OPTIMIZATION,0.09098712446351931,"αEI (x | Dn) = σ(x)φ
∆(x) σ(x)"
BAYESIAN OPTIMIZATION,0.09184549356223176,"
+|∆(x)| Φ
∆(x) σ(x) 
,"
BAYESIAN OPTIMIZATION,0.0927038626609442,"(4)
where ∆(x) = µ(x) −f ⋆
n is the expected difference between the proposed point x and the current
101"
BAYESIAN OPTIMIZATION,0.09356223175965665,"best solution, f ⋆
n = maxx∈{xi}n
i=1 f(x) denotes the best function value obtained so far; µ(x) and
102"
BAYESIAN OPTIMIZATION,0.0944206008583691,"σ(x) are the predictive mean and predictive standard deviation at x, respectively; and φ(·) and Φ(·)
103"
BAYESIAN OPTIMIZATION,0.09527896995708154,"denote the probability density function (PDF) and the cumulative distribution function (CDF) of
104"
BAYESIAN OPTIMIZATION,0.096137339055794,"standard normal, respectively. Another widely applied AF for maximization problems is the upper
105"
BAYESIAN OPTIMIZATION,0.09699570815450644,"confidence bound (UCB) [13]:
106"
BAYESIAN OPTIMIZATION,0.09785407725321889,"αUCB (x | Dn, β) = µ(x) + βσ(x),
(5)
where β is a tunable parameter that balances exploration and exploitation. The general BO procedure
107"
BAYESIAN OPTIMIZATION,0.09871244635193133,"can be summarized as Algorithm 1.
108"
BKTF FOR BAYESIAN OPTIMIZATION,0.09957081545064378,"3
BKTF for Bayesian optimization
109"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.10042918454935622,"3.1
Bayesian hierarchical model specification
110"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.10128755364806867,"Before introducing BKTF, we first construct a D-dimensional grid space corresponding to
111"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.10214592274678111,"the search space X, where Xd could be continuous, integer-valued, or categorical.
We de-
112"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.10300429184549356,"fine it on D sets {S1, . . . , SD} and denote the whole grid by QD
d=1 Sd: S1 × . . . × SD =
113"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.10386266094420601,"{(s1, . . . , sD) | ∀d ∈{1, . . . , D}, sd ∈Sd}. For dimensions with integer-valued and categorical
114"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.10472103004291845,"input, we consider Sd the set of corresponding discrete values. For dimensions with continuous input,
115"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.1055793991416309,"the coordinate set Sd is formed by md interpolation points cd
i that are distributed over the bounded
116"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.10643776824034334,"interval Xd = [ad, bd], i.e., Sd =

cd
i
	md
i=1 with cd
i ∈Xd. The size of Sd becomes |Sd| = md, and
117"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.1072961373390558,"size of the entire grid space is QD
d=1 |Sd|. Note that we do not restrict Sd to be uniformly distributed.
118"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.10815450643776824,"We assume the underlying function f as a stochastic process that is zero-centered. We randomly
119"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.10901287553648069,"sample an initial dataset including n0 input-output data pairs D0 = {xi, yi}n0
i=1, where {xi}n0
i=1
120"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.10987124463519313,"are located in QD
d=1 Sd, and this yields an incomplete D-dimensional tensor Y ∈R|S1|×···×|SD|
121"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.11072961373390558,"with n0 observed points. BKTF approximates the entire data tensor Y by a kernelized tensor CP
122"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.11158798283261803,"decomposition:
123 Y = R
X"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.11244635193133047,"r=1
λr · gr
1 ◦gr
2 ◦· · · ◦gr
D + E,
(6)"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.11330472103004292,"where R is a pre-specified tensor CP rank, λ = (λ1, . . . , λR)⊤denote weight coefficients that capture
124"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.11416309012875536,"the magnitude/importance of each rank in the factorization, gr
d = [gr
d(sd) : sd ∈Sd] ∈R|Sd| denotes
125"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.11502145922746781,"the rth latent factor for the dth dimension, entries in E are i.i.d. white noises following N(0, τ −1).
126"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.11587982832618025,"It should be particularly noted that both the coefficients {λr}R
r=1 and the latent basis functions
127"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.1167381974248927,"{gr
1, . . . , gr
D}R
r=1 are random variables. The function approximation for x = (x1, . . . , xD)⊤is:
128"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.11759656652360514,"f(x) = R
X"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.1184549356223176,"r=1
λrgr
1 (x1) · · · gr
D (xD) = R
X"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.11931330472103004,"r=1
λr D
Y"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.12017167381974249,"d=1
gr
d (xd) .
(7)"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.12103004291845494,"For priors, we assume λr ∼N (0, 1) for r = 1, . . . , R and use a GP prior on the latent factors for
129"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.12188841201716738,"dimension d with continuous input:
130"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.12274678111587983,"gr
d (xd) | lr
d ∼GP (0, kr
d (xd, x′
d; lr
d)) , for r = 1, . . . , R,
(8)
where kr
d is a valid kernel function. In this paper, we choose a Matérn 3/2 kernel kr
d (xd, x′
d; lr
d) =
131"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.12360515021459227,"σ2 
1 + √"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.12446351931330472,"3|xd−x′
d|
lr
d"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.12532188841201716,"
exp

− √"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.12618025751072962,"3|xd−x′
d|
lr
d"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.12703862660944207,"
. We fix the kernel variance of kr
d as σ2 = 1, and only learn
132"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.1278969957081545,"the lengthscale hyperparameters lr
d, since the variances of the model can be captured by λ. One can
133"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.12875536480686695,"also exclude λ but introduce variance σ2 as a kernel hyperparameter on one of the basis functions;
134"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.1296137339055794,"however, learning kernel hyperparameters is computationally more expensive than learning λ. For
135"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.13047210300429185,"simplicity, we can also assume the lengthscale parameters to be identical, i.e., l1
d = . . . = lR
d = ld,
136"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.1313304721030043,"for each dimension d. The prior distribution for the corresponding latent factor gr
d is then a Gaussian
137"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.13218884120171673,"distribution: gr
d ∼N (0, Kr
d), where Kr
d is the |Sd| × |Sd| covariance matrix computed from kr
d.
138"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.13304721030042918,"We place Gaussian hyperpriors on the log-transformed kernel hyperparameters to ensure positive
139"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.13390557939914163,"values, i.e., log (lr
d) ∼N
 
µl, τ −1
l

. For categorical input, we assume that the corresponding latent
140"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.13476394849785409,"basis functions gr
d | Λd ∼N
 
0, Λ−1
d

for r = 1, . . . , R, where the precision matrix Λd follows a
141"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.1356223175965665,"Wishart prior with an identity scale matrix and |Sd| degrees of freedom, i.e., Λd ∼W
 
I|Sd|, |Sd|

.
142"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.13648068669527896,"For noise precision τ, we assume a conjugate Gamma prior τ ∼Gamma (a0, b0). For dimensions
143"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.13733905579399142,"with integer variables, we could model the covariance of the basis functions either using a kernel
144"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.13819742489270387,"matrix or with an inverse-Wishart prior, depending on specific situations.
145"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.13905579399141632,"For observations, we assume each yi in the initial dataset D0 follows a Gaussian distribution:
146"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.13991416309012875,"yi
 {gr
d
 
xi
d

}, {λr}, τ ∼N
 
f (xi) , τ −1
.
(9)"
BKTF AS A TWO-LAYER DEEP GP,0.1407725321888412,"3.2
BKTF as a two-layer deep GP
147"
BKTF AS A TWO-LAYER DEEP GP,0.14163090128755365,"Here we show the representation of BKTF as a two-layer deep GP. The first layer characterizes
148"
BKTF AS A TWO-LAYER DEEP GP,0.1424892703862661,"the generation of latent functions {gr
d}R
r=1 for dimension d. For the second layer, if we consider
149"
BKTF AS A TWO-LAYER DEEP GP,0.14334763948497853,"{gr
1, . . . , gr
D}R
r=1 as parameters and rewrite the functional decomposition in Eq. (7) as a linear function
150"
BKTF AS A TWO-LAYER DEEP GP,0.14420600858369098,"f (x; {λr}) = PR
r=1 λr
QD
d=1 gr
d (xd) with λr
iid
∼N (0, 1), we can marginalize {λr} and obtain a
151"
BKTF AS A TWO-LAYER DEEP GP,0.14506437768240343,"fully symmetric multilinear kernel/covariance function for any two data points x = (x1, . . . , xD)⊤
152"
BKTF AS A TWO-LAYER DEEP GP,0.1459227467811159,"and x′ = (x′
1, . . . , x′
D)⊤:
153"
BKTF AS A TWO-LAYER DEEP GP,0.14678111587982834,"k
 
x, x′; {gr
1, . . . , gr
D}R
r=1

= R
X r=1 D
Y"
BKTF AS A TWO-LAYER DEEP GP,0.14763948497854076,"d=1
gr
d (xd) gr
d (x′
d) .
(10)"
BKTF AS A TWO-LAYER DEEP GP,0.14849785407725322,"As can be seen, the second layer has a multilinear product kernel function parameterized by
154"
BKTF AS A TWO-LAYER DEEP GP,0.14935622317596567,"{gr
1, . . . , gr
D}R
r=1. There are some properties to highlight: (i) the kernel is nonstationary since
155"
BKTF AS A TWO-LAYER DEEP GP,0.15021459227467812,"the value of gr
d(·) is location specific, and (ii) the kernel is nonseparable when R > 1. Therefore,
156"
BKTF AS A TWO-LAYER DEEP GP,0.15107296137339055,"this specification is very different from traditional GP surrogates, such as:
157




"
BKTF AS A TWO-LAYER DEEP GP,0.151931330472103,"


"
BKTF AS A TWO-LAYER DEEP GP,0.15278969957081545,"GP ARD:
k (x, x′) = QD
d=1 kd (xd, x′
d) ,
kernel function is stationary and separable
Additive GP (2nd order):
k (x, x′) = PD
d=1 k1
d (xd, x′
d) + PD−1
d=1
PD
e=d+1 k2
d (xd, x′
d) k2
e (xe, x′
e) ,
kerenl is stationary and nonseparable
where all kernel functions are stationary with different hyperparameters (e.g., length scale and
158"
BKTF AS A TWO-LAYER DEEP GP,0.1536480686695279,"variance). Compared to the GP-based kernel specification, the multilinear kernel in Eq. (10) has a
159"
BKTF AS A TWO-LAYER DEEP GP,0.15450643776824036,"much larger set of hyperparameters and becomes more flexible and adaptive for the data. From a GP
160"
BKTF AS A TWO-LAYER DEEP GP,0.15536480686695278,"perspective, learning the hyperparameter in the kernel function in Eq. (10) will be computationally
161"
BKTF AS A TWO-LAYER DEEP GP,0.15622317596566523,"expensive; however, we can achieve efficient Bayesian inference of {λr, gr
1, . . . , gr
D}R
r=1 under a
162"
BKTF AS A TWO-LAYER DEEP GP,0.1570815450643777,"kernelized tensor factorization framework.
163"
MODEL INFERENCE,0.15793991416309014,"3.3
Model inference
164"
MODEL INFERENCE,0.15879828326180256,"Unlike GP, BKTF no longer enjoys an analytical posterior distribution. Based on the aforementioned
165"
MODEL INFERENCE,0.15965665236051502,"prior and hyperprior settings, we adapt the MCMC updating procedure in Ref. [10, 11] to an efficient
166"
MODEL INFERENCE,0.16051502145922747,"Gibbs sampling algorithm for model inference. This allows us to accommodate observations that are
167"
MODEL INFERENCE,0.16137339055793992,"not located in the grid space QD
d=1 Sd. The detailed derivation of the sampling algorithm is given in
168"
MODEL INFERENCE,0.16223175965665235,"Appendix A. In terms of computational cost, we note that updating gr
d and kernel hyperparameters
169"
MODEL INFERENCE,0.1630901287553648,"requires min

O(n3), O(|Sd|3)
	
in time. Sparse GP (such as [14]) could be introduced when n, |Sd|
170"
MODEL INFERENCE,0.16394849785407725,"become large. See Appendix A, F for detailed discussion/comparison about computation complexity.
171"
PREDICTION AND AF COMPUTATION,0.1648068669527897,"3.4
Prediction and AF computation
172"
PREDICTION AND AF COMPUTATION,0.16566523605150216,"In each step of function evaluation, we run the MCMC sampling process K iterations for model
173"
PREDICTION AND AF COMPUTATION,0.16652360515021458,"inference, where the first K0 samples are taken as burn-in and the last K −K0 samples are
174"
PREDICTION AND AF COMPUTATION,0.16738197424892703,"used for posterior approximation. The predictive distribution for any entry f ∗in the defined grid
175"
PREDICTION AND AF COMPUTATION,0.1682403433476395,"space conditioned on the observed dataset Dn can be obtained by the Monte Carlo approximation
176"
PREDICTION AND AF COMPUTATION,0.16909871244635194,"p (f ∗| Dn, θ0) ≈
1
K−K0 × PK
k=K0+1 p

f ∗ (gr
d)(k) , λ(k), τ (k)
, where θ0 = {µl, τl, a0, b0} is
177"
PREDICTION AND AF COMPUTATION,0.16995708154506436,"the set of all parameters used in hyperpriors. Although a direct analytical predictive distribution does
178"
PREDICTION AND AF COMPUTATION,0.17081545064377682,"not exist in BKTF, we can use MCMC samples to obtain the mean and variance of all points on the
179"
PREDICTION AND AF COMPUTATION,0.17167381974248927,"grid, thus offering an enumeration-based approach to define AF.
180"
PREDICTION AND AF COMPUTATION,0.17253218884120172,"We define a Bayesian variant of the UCB as the AF by approximating the predictive mean and variance
181"
PREDICTION AND AF COMPUTATION,0.17339055793991417,"(or uncertainty) in ordinary GP-based UCB with the values calculated from MCMC sampling.
182"
PREDICTION AND AF COMPUTATION,0.1742489270386266,"For every MCMC sample after burn-in, i.e., k > K0, we can estimate an output tensor ˜F
(k)
183"
PREDICTION AND AF COMPUTATION,0.17510729613733905,"over the entire grid space using the latent factors (gr
d)(k) and the weight vector λ(k): ˜F
(k) =
184
PR
r=1 λ(k)
r
(gr
1)(k) ◦(gr
2)(k) ◦· · · ◦(gr
D)(k). We can then compute the corresponding mean and
185"
PREDICTION AND AF COMPUTATION,0.1759656652360515,"variance tensors of the (K −K0) samples { ˜F
(k)}K
k=K0+1, and denote the two tensors by U and
186"
PREDICTION AND AF COMPUTATION,0.17682403433476396,"V, respectively. The approximated predictive distribution at each point x in the space becomes
187"
PREDICTION AND AF COMPUTATION,0.17768240343347638,"˜f(x) ∼N (u(x), v(x)). Following the definition of UCB in Eq. (5), we define Bayesian UCB
188"
PREDICTION AND AF COMPUTATION,0.17854077253218884,"(B-UCB) at location x as αB-UCB (x | D, β, gr
d, λ) = u(x) + β
p"
PREDICTION AND AF COMPUTATION,0.1793991416309013,"v(x). The next search/query point
189"
PREDICTION AND AF COMPUTATION,0.18025751072961374,"can be determined via xnext = arg maxx∈{QD
d=1 Sd−Dn−1} αB-UCB (x).
190"
PREDICTION AND AF COMPUTATION,0.1811158798283262,"We summarize the implementation procedure of BKTF for BO in Appendix B (see Algorithm 2).
191"
PREDICTION AND AF COMPUTATION,0.18197424892703862,"Given the sequential nature of BO, when a new data point arrives at step n, we can start the MCMC
192"
PREDICTION AND AF COMPUTATION,0.18283261802575107,"with the last iteration of the Markov chains at step n −1 to accelerate model convergence. The main
193"
PREDICTION AND AF COMPUTATION,0.18369098712446352,"computational and storage cost of BKTF is to interpolate and save the tensors ˜F ∈R|S1|×···×|SD|
194"
PREDICTION AND AF COMPUTATION,0.18454935622317598,"over (K −K0) iterations for Bayesian AF estimation. This could be prohibitive when the MCMC
195"
PREDICTION AND AF COMPUTATION,0.1854077253218884,"sample size K or the dimensionality D becomes large. To avoid saving the tensors, in practice, we
196"
PREDICTION AND AF COMPUTATION,0.18626609442060085,"can simply use the maximum values of each entry over the (K −K0) iterations through iterative
197"
PREDICTION AND AF COMPUTATION,0.1871244635193133,"pairwise comparison. The number of samples after burn-in then implies the value of β in αB-UCB. We
198"
PREDICTION AND AF COMPUTATION,0.18798283261802576,"adopt this simple AF in our numerical experiments.
199"
PREDICTION AND AF COMPUTATION,0.1888412017167382,"A critical challenge in BKTF is that tensor size grows exponentially with the number of dimensions.
200"
PREDICTION AND AF COMPUTATION,0.18969957081545064,"To decrease the computational burden of enumeration-based AF, we also implement BKTF with
201"
PREDICTION AND AF COMPUTATION,0.1905579399141631,"random discretization—randomly selecting candidate points instead of reconstructing the whole
202"
PREDICTION AND AF COMPUTATION,0.19141630901287554,"space, denoted as BKTFrandom. BKTFrandom can be applied to functions with higher dimensions
203"
PREDICTION AND AF COMPUTATION,0.192274678111588,"(e.g., D > 10). We discuss the comparison between BKTF and BKTFrandom in Experiments on test
204"
PREDICTION AND AF COMPUTATION,0.19313304721030042,"functions, see Section 5.1.
205"
RELATED WORK,0.19399141630901287,"4
Related work
206"
RELATED WORK,0.19484978540772532,"The key of BO is to effectively characterize the posterior distribution of the objective function
207"
RELATED WORK,0.19570815450643778,"from a limited number of observations. The most relevant work to our study is the Bayesian
208"
RELATED WORK,0.1965665236051502,"Kernelized Factorization (BKF) framework, which has been mainly used for modeling large-scale and
209"
RELATED WORK,0.19742489270386265,"multidimensional spatiotemporal data with UQ. The key idea is to parameterize the multidimensional
210"
RELATED WORK,0.1982832618025751,"stochastic processes using a factorization model, in which specific priors are used to encode spatial
211"
RELATED WORK,0.19914163090128756,"and temporal dependencies. Signature examples of BKF include spatial dynamic factor model
212"
RELATED WORK,0.2,"(SDFM) [15], variational Gaussian process factor analysis (VGFA) [16], and Bayesian kernelized
213"
RELATED WORK,0.20085836909871244,"matrix/tensor factorization (BKMF/BKTF) [10, 11, 17]. A common solution in these models is to
214"
RELATED WORK,0.2017167381974249,"use GP prior to modeling the factor matrices, thus encoding spatial and temporal dependencies. In
215"
RELATED WORK,0.20257510729613734,"addition, for categorical dimensions, BKTF uses an inverse-Wishart prior to modeling the covariance
216"
RELATED WORK,0.2034334763948498,"matrix for the latent factors. A key difference among these methods is how inference is performed.
217"
RELATED WORK,0.20429184549356222,"SDFM and BKMF/BKTF are fully Bayesian hierarchical models and they rely on MCMC for model
218"
RELATED WORK,0.20515021459227467,"inference, where the factors can be updated via Gibbs sampling with conjugate priors; for learning
219"
RELATED WORK,0.20600858369098712,"the posterior distributions of kernel hyperparameters, SDFM uses the Metropolis-Hastings sampling,
220"
RELATED WORK,0.20686695278969958,"while BKMF/BKTF uses the more efficient slice sampling. On the other hand, VGFA uses variational
221"
RELATED WORK,0.20772532188841203,"inference to learn factor matrices, while kernel hyperparameters are learned through maximum a
222"
RELATED WORK,0.20858369098712445,"posteriori (MAP) estimation without UQ. Overall, BKTF has shown superior performance in modeling
223"
RELATED WORK,0.2094420600858369,"multidimensional spatiotemporal processes with high-quality UQ for 2D/3D spaces [11, 17].
224"
RELATED WORK,0.21030042918454936,"The proposed BKTF surrogate models the objective function—as a single realization of a random
225"
RELATED WORK,0.2111587982832618,"process—using low-rank tensor factorization with random basis functions. This basis function-
226"
RELATED WORK,0.21201716738197424,"based specification is closely related to multidimensional Karhunen-Loève (KL) expansion [18] for
227"
RELATED WORK,0.2128755364806867,"stochastic (spatial, temporal, and spatiotemporal) processes. Empirical analysis of KL expansion is
228"
RELATED WORK,0.21373390557939914,"also known as proper orthogonal decomposition (POD). With a known kernel/covariance function,
229"
RELATED WORK,0.2145922746781116,"truncated KL expansion allows us to approximate the underlying random process using a set of
230"
RELATED WORK,0.21545064377682405,"eigenvalues and eigenfunctions derived from the kernel function. Numerical KL expansion is often
231"
RELATED WORK,0.21630901287553647,"referred to as the Garlekin method, and in practice, the basis functions are often chosen as prespecified
232"
RELATED WORK,0.21716738197424892,"and deterministic functions [12, 19], such as Fourier basis, wavelet basis, orthogonal polynomials,
233"
RELATED WORK,0.21802575107296138,"B-splines, empirical orthogonal functions, radial basis functions (RBF), and Wendland functions
234"
RELATED WORK,0.21888412017167383,"(i.e., compactly supported RBF) (see, e.g., [20], [21], [22], [23]). However, the quality of UQ will be
235"
RELATED WORK,0.21974248927038625,"undermined as the randomness is fully attributed to the coefficients {λr}; in addition, these methods
236"
RELATED WORK,0.2206008583690987,"also require a large number of basis functions (or a large rank) to fit complex stochastic processes.
237"
RELATED WORK,0.22145922746781116,"Different from methods with fixed/known basis functions, BKTF uses a Bayesian hierarchical
238"
RELATED WORK,0.2223175965665236,"modeling framework to better capture the randomness and uncertainty in the data, in which GP priors
239"
RELATED WORK,0.22317596566523606,"are used to model the latent factors (i.e., basis functions are also random processes) on different
240"
RELATED WORK,0.2240343347639485,"dimensions, and hyperpriors are introduced on the kernel hyperparameters. Therefore, BKTF becomes
241"
RELATED WORK,0.22489270386266094,"a fully Bayesian version of multidimensional KL expansion for stochastic processes with unknown
242"
RELATED WORK,0.2257510729613734,"covariance from partially observed data, however, without imposing any orthogonal constraint on
243"
RELATED WORK,0.22660944206008585,"the basis functions. Following the analysis in section 3.2, BKTF is also a special case of a two-layer
244"
RELATED WORK,0.22746781115879827,"deep Gaussian process [24, 7], where the first layer produces latent factors for each dimension, and
245"
RELATED WORK,0.22832618025751072,"the second layer has a multilinear kernel parameterized by all latent factors.
246"
EXPERIMENTS,0.22918454935622318,"5
Experiments
247"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.23004291845493563,"5.1
Optimization for benchmark test functions
248"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.23090128755364808,"We test the proposed BKTF model for BO on seven benchmark functions that are used for global
249"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2317596566523605,"optimization problems [25], with dimension D ranging from 2 to 10. All selected standard functions
250"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.23261802575107296,"are multimodal; detailed descriptions are given in Appendix D. In fact, we can visually see that
251"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2334763948497854,"the standard Damavandi/Schaffer/Griewank functions in Figure 7 (see Appendix D) indeed have a
252"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.23433476394849787,"Figure 2: Optimization on benchmark test functions, where medians with 25% and 75% quartiles of
10 runs are compared."
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2351931330472103,"low-rank structure. For each function, we assume the initial dataset D0 contains n0 = D observed
253"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.23605150214592274,"data pairs, and we set the total number of query points to N = 80 for 4D Griewank and 6D Hartmann
254"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2369098712446352,"function, N = 200 for 10D Griewank, and N = 50 for others. We rescale the input search range to
255"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.23776824034334765,"[0, 1] for all dimensions and normalize the output data using z-score normalization.
256"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.23862660944206007,"Model configuration
When applying BKTF to continuous test functions, we introduce md interpo-
257"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.23948497854077253,"lation points for the dth dimension of the input space. The values of md used for each benchmark
258"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.24034334763948498,"function are predefined and given in Table 1 (see Appendix D). Setting the resolution grid will require
259"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.24120171673819743,"certain prior knowledge (e.g., smoothness of the function); and it also depends on the available
260"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.24206008583690988,"computational resources and the number of entries in the tensor, which grows exponentially with md.
261"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2429184549356223,"In practice, we find that setting md = 10 ∼100 is sufficient for most problems. We set the CP rank
262"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.24377682403433476,"R = 2, and for each BO function evaluation run 400 MCMC iterations for model inference where
263"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2446351931330472,"the first 200 iterations are taken as burn-in. We use Matérn 3/2 kernel as the covariance function for
264"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.24549356223175967,"all the test functions.
265"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2463519313304721,"Note that for the 10D Griewank function, the grid-based models do not work, and only models built
266"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.24721030042918454,"in continuous space and BKTFrandom can be performed. For BKTFrandom, in each evaluation we
267"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.248068669527897,"randomly select 20k points in the search space as candidates and choose the one with the best AF as
268"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.24892703862660945,"the next evaluation location.
269"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2497854077253219,"Baselines
We compare BKTF with the following BO methods that use GP as the surrogate model:
270"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2506437768240343,"(1) GP αEI and (2) GPgrid αEI: GP as the surrogate model and EI as the AF, in continuous space
271
QD
d=1 Xd and Cartesian grid space QD
d=1 Sd, respectively; (3) GP αUCB and (4) GPgrid αUCB: GP
272"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2515021459227468,"as the surrogate model with UCB as the AF where β = 2, in QD
d=1 Xd and QD
d=1 Sd, respectively;
273"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.25236051502145923,"(5) additive GP: the sum of two 1st-order additive kernels per dimension as the surrogate with EI
274"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2532188841201717,"as the AF, in continuous space. This baseline has the same number of latent functions as BKTF
275"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.25407725321888414,"(R = 2) but in a sum-based manner; (6) deepGP: a two-layer fully-Bayesian deep GP with EI as the
276"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2549356223175966,"AF, implemented with the deepgp package1.
277"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.255793991416309,"Similar as the setting for BKTF, we use Matérn 3/2 kernel in all GP models. Given the computational
278"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.25665236051502144,"cost, we only compare deepGP on 2D functions [9]. For AF optimization in GP αEI and GP αUCB, we
279"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2575107296137339,"first use the DIRECT algorithm [26] and then apply the Nelder-Mead algorithm [27] to further search
280"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.25836909871244634,"if there exist better solutions. We also compare with SAASBO (Sparse Axis-Aligned Subspace) [28]
281"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2592274678111588,"with Hamiltonian Monte Carlo sampling, implemented using BoTorch [29], on the 6D Hartmann and
282"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.26008583690987125,"10D Griewank functions.
283"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2609442060085837,"Results
To compare the optimization performance of different models on the benchmark functions,
284"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.26180257510729615,"we define the absolute error between the global optimum f ⋆and the current estimated global
285"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2626609442060086,"optimum ˆf ⋆, i.e.,
f ⋆−ˆf ⋆, as the regret, and examine how regret varies with the number of function
286"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.263519313304721,1https://CRAN.R-project.org/package=deepgp
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.26437768240343346,"evaluations. We run the optimization 10 times for every test function with a different set of initial
287"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2652360515021459,"observations. The results are summarized in Figure 2. We see that for the 2D functions Branin
288"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.26609442060085836,"and Schaffer, BKTF clearly finds the global optima much faster than GP surrogate-based baselines.
289"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2669527896995708,"For Damavandi function, where the global minimum (f(x⋆) = 0) is located in a small sharp area
290"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.26781115879828327,"while the local optimum (f(x) = 2) is located at a large smooth area (see Figure 7 in Appendix D),
291"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2686695278969957,"GP-based models are trapped around the local optima in most cases ( i.e., regret = 2) and cannot
292"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.26952789699570817,"jump out. In contrast, BKTF explores the global characteristics of the objective function over the
293"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2703862660944206,"entire search space and reaches the global optimum within 10 iterations of function evaluations.
294"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.271244635193133,"For higher dimensional Griewank and Hartmann functions, BKTF successfully arrives at the global
295"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2721030042918455,"optima under the given observation budgets, while GP-based comparison methods are prone to be
296"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2729613733905579,"stuck around local optima. We compare the regret at the last iteration in Table 2 (Appendix E.2),
297"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2738197424892704,"along with the average cost of evaluations. The enumeration-based GP surrogates, i.e., GPgrid αEI
298"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.27467811158798283,"and GPgrid αUCB, perform a little better than direct GP-based search, i.e., GP αEI and GP αUCB on
299"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2755364806866953,"Damavandi function, but worse on others. This means that the discretization, to some extent, offers
300"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.27639484978540774,"possibilities for searching all the alternative points in the space, since in each function evaluation,
301"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2772532188841202,"every sample in the space is equally compared solely based on the predictive distribution. Additive
302"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.27811158798283264,"GP is comparable with R = 2 BKTF; while the results demonstrate that BKTF can be much more
303"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.27896995708154504,"flexible than additive GP. As for BKTFrandom, we see that it can alleviate the curse of dimensionality
304"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2798283261802575,"and be applied for higher-dimensional problems that may not be performed with a grid but at the
305"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.28068669527896994,"cost of more evaluation budgets, particularly it costs more iterations for lower-dimensional functions
306"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2815450643776824,"compared with BKTF.
307"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.28240343347639485,"Overall, BKTF reaches the global optimum for every test function and shows superior performance
308"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2832618025751073,"for complex objective functions with a faster convergence rate. To intuitively compare the overall per-
309"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.28412017167381975,"formance of different models across multiple experiments/functions, we further estimate performance
310"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2849785407725322,"profiles (PPs) [30] (see Appendix E.1), and compute the area under the curve (AUC) for quantitative
311"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.28583690987124466,"analysis (see bottom right of Figure 2 and Table 2 in Appendix E.2). Our results show that BKTF
312"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.28669527896995706,"obtains the best performance across all functions.
313"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2875536480686695,"Interpretable latent space. The sampled latent functions are interpretable and imply the under-
314"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.28841201716738196,"lying correlations of the objective function. We illustrate the learned periodic (global) patterns in
315"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2892703862660944,"Appendix E.3. Effects of hyperpriors. Since we build a fully Bayesian model, the hyperparameters
316"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.29012875536480687,"of the covariance functions can be updated automatically from the data likelihood and hyperprior.
317"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2909871244635193,"Note that in optimization scenarios where the observations are scarce, the prediction performance of
318"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2918454935622318,"BKTF highly depends on the hyperprior settings, i.e., θ0 = {µl, τl, a0, b0}. We discuss the effects of
319"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2927038626609442,"hyperpriors in Appendix E.4. Effects of rank. The only predefined model parameter is the model
320"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2935622317596567,"rank, all other model parameters and hyperparameters are sampled with MCMC. We discuss the
321"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2944206008583691,"effects of rank on the 2D nonstationary nonseparable function defined in Introduction (see Figure 1)
322"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.2952789699570815,"in Appendix E.5. We see that BKTF is robust to rank specification and can find the global solution
323"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.296137339055794,"efficiently with rank set as 2, 4, 6 and 8.
324"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.29699570815450643,"5.2
Hyperparameter tuning for machine learning
325"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.2978540772532189,"In this section, we evaluate the performance of BKTF for automatic machine learning (ML) tasks.
326"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.29871244635193134,"We compare different models to optimize the hyperparameters of two ML algorithms—random forest
327"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.2995708154506438,"(RF) and neural network (NN)—on classification for the MNIST database of handwritten digits2
328"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.30042918454935624,"and regression for the Boston housing dataset3. The tuning tasks involve both integer-valued and
329"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3012875536480687,"categorical parameters, and the details are given in the Appendix G. We treat those integer-valued
330"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3021459227467811,"dimensions as continuous and use a Matérn 3/2 kernel for the basis functions. Given the size of the
331"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.30300429184549355,"hyperparameter space, we perform BKTFrandom for classification and BKTF for regression. We
332"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.303862660944206,"assume that the number of initial observations D0 equals the number of tuning hyperparameters.
333"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.30472103004291845,"The total budget N is 20 for the classification task and 50 for the regression. We implement RF
334"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3055793991416309,"algorithms using the scikit-learn package and construct NN models by Keras with 2 hidden layers.
335"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.30643776824034336,"All other model hyperparameters are set as the default values.
336"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3072961373390558,"Model configuration
We treat all the integer hyperparameters as samples from a continuous
337"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.30815450643776826,"space and then generate the corresponding Cartesian product space QD
d=1 Sd. One can interpret
338"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3090128755364807,"2http://yann.lecun.com/exdb/mnist/
3https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3098712446351931,Figure 3: Comparison of hyperparameter tuning for ML tasks: (a) classification; (b) regression.
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.31072961373390556,"the candidate values for each hyperparameter as the interpolation points in the corresponding input
339"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.311587982832618,"dimension. The size of the spanned space Q Sd is 91 × 46 × 64 × 10 × 11 × 2 and 91 × 46 × 13 × 10
340"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.31244635193133047,"for RF classifier and RF regressor, respectively; 91 × 49 × 31 × 18 × 3 × 2 and 91 × 49 × 31 for
341"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3133047210300429,"NN classifier and NN regressor respectively. We set the tensor rank R = 4 for BKTF, set K = 400
342"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3141630901287554,"and K0 = 200 for MCMC inference, and use the Matérn 3/2 kernel for capturing correlations.
343"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3150214592274678,"Baselines
In addition to GP surrogate-based GP αEI and GP αUCB, we also compare with a baseline
344"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3158798283261803,"method: random search (RS), and two non-GP models: particle swarm optimization (PSO) [31] and
345"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.31673819742489273,"Tree-structured Parzen Estimator (BO-TPE) [32], which are common methods for hyperparameter
346"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.31759656652360513,"tuning. We exclude grid-based GP models as sampling the entire grid becomes infeasible.
347"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3184549356223176,"Results
We compare the accuracy for MNIST classification and MSE (mean squared error) for
348"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.31931330472103003,"Boston housing regression both in terms of the number of function evaluations and still run the
349"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3201716738197425,"optimization processes ten times with different initial datasets D0. The results obtained by different
350"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.32103004291845494,"BO models are given in Figure 3, and the final classification accuracy and regression MSE are
351"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3218884120171674,"compared in Table 5 (see Appendix H). For BKTF, we see from Figure 3 that the width between the
352"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.32274678111587984,"two quartiles of accuracy and error decreases as more iterations are evaluated, and the median curves
353"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3236051502145923,"present better convergence rates compared to the baselines. Table 5 also shows that the proposed
354"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.3244635193133047,"BKTF surrogate achieves the best final mean accuracy and regression error with small standard
355"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.32532188841201715,"deviations. The results above demonstrate the advantage of BKTF as a surrogate.
356"
CONCLUSION,0.3261802575107296,"6
Conclusion
357"
CONCLUSION,0.32703862660944205,"This paper proposes to use Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate
358"
CONCLUSION,0.3278969957081545,"model for Bayesian optimization with mixed variables (both discrete/categorical and continuous)
359"
CONCLUSION,0.32875536480686696,"when the dimensionality is relatively small (e.g., say D < 10). Compared with traditional GP surro-
360"
CONCLUSION,0.3296137339055794,"gates, the BKTF surrogate is more flexible and adaptive to data thanks to the Bayesian hierarchical
361"
CONCLUSION,0.33047210300429186,"specification, which provides high-quality UQ for BO tasks. The tensor factorization model behind
362"
CONCLUSION,0.3313304721030043,"BKTF offers an effective solution to capture global/long-range correlations and cross-dimension cor-
363"
CONCLUSION,0.3321888412017167,"relations. The inference of BKTF is achieved through MCMC, which provides a natural solution for
364"
CONCLUSION,0.33304721030042916,"acquisition. Experiments on both test function optimization and ML hyperparameter tuning confirm
365"
CONCLUSION,0.3339055793991416,"the superiority of BKTF as a surrogate for BO. Moreover, the tensor factorization framework makes
366"
CONCLUSION,0.33476394849785407,"it straightforward to adapt BKTF to handle multivariate and functional output (see e.g., [33, 34]), by
367"
CONCLUSION,0.3356223175965665,"directly treating the output coordinates as part of the input. A limitation of BKTF is that we restrict
368"
CONCLUSION,0.336480686695279,"BO to a grid search space in order to leverage tensor factorization; however, we believe that designing
369"
CONCLUSION,0.3373390557939914,"a compatible grid space based on prior knowledge is not a challenging task.
370"
CONCLUSION,0.3381974248927039,"There are several directions to be explored to make BKTF more scalable. Scalable GP solutions, such
371"
CONCLUSION,0.33905579399141633,"as sparse GP [14] and Gaussian Markov Random Field (GMRF) [35], can be introduced to reduce
372"
CONCLUSION,0.33991416309012873,"the inference cost when |Sd| becomes large. The current MCMC-based acquisition method requires
373"
CONCLUSION,0.3407725321888412,"explicit reconstruction of the whole tensor, which quickly becomes infeasible when D becomes large
374"
CONCLUSION,0.34163090128755363,"(e.g., D > 20). A natural question is whether it is possible to achieve efficient acquisition directly
375"
CONCLUSION,0.3424892703862661,"using the basis functions and the corresponding weights without explicitly constructing the tensors.
376"
CONCLUSION,0.34334763948497854,"This problem corresponds to finding/locating the maximum entry of a tensor given its low-rank
377"
CONCLUSION,0.344206008583691,"decomposition (see e.g., [36]).
378"
CONCLUSION,0.34506437768240344,"This work aims to advance the field of probabilistic machine learning, particularly Bayesian opti-
379"
CONCLUSION,0.3459227467811159,"mization. Regardless of the model limitations, it has the potential of misuse for ML algorithms.
380"
REFERENCES,0.34678111587982835,"References
381"
REFERENCES,0.34763948497854075,"[1] Roman Garnett. Bayesian Optimization. Cambridge University Press, 2023.
382"
REFERENCES,0.3484978540772532,"[2] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking
383"
REFERENCES,0.34935622317596565,"the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE,
384"
REFERENCES,0.3502145922746781,"104(1):148–175, 2015.
385"
REFERENCES,0.35107296137339056,"[3] Robert B Gramacy. Surrogates: Gaussian Process Modeling, Design, and Optimization for the
386"
REFERENCES,0.351931330472103,"Applied Sciences. Chapman and Hall/CRC, 2020.
387"
REFERENCES,0.35278969957081546,"[4] Christopher KI Williams and Carl Edward Rasmussen. Gaussian Processes for Machine
388"
REFERENCES,0.3536480686695279,"Learning. MIT Press, Cambridge, MA, 2006.
389"
REFERENCES,0.35450643776824037,"[5] David K Duvenaud, Hannes Nickisch, and Carl Rasmussen. Additive Gaussian processes.
390"
REFERENCES,0.35536480686695276,"Advances in neural information processing systems, 24, 2011.
391"
REFERENCES,0.3562231759656652,"[6] Mickael Binois and Nathan Wycoff. A survey on high-dimensional Gaussian process modeling
392"
REFERENCES,0.35708154506437767,"with application to Bayesian optimization. ACM Transactions on Evolutionary Learning and
393"
REFERENCES,0.3579399141630901,"Optimization, 2(2):1–26, 2022.
394"
REFERENCES,0.3587982832618026,"[7] Andreas Damianou and Neil D Lawrence. Deep Gaussian processes. In International Conference
395"
REFERENCES,0.35965665236051503,"on Artificial Intelligence and Statistics, pages 207–215, 2013.
396"
REFERENCES,0.3605150214592275,"[8] Ali Hebbal, Loïc Brevault, Mathieu Balesdent, El-Ghazali Talbi, and Nouredine Melab.
397"
REFERENCES,0.36137339055793993,"Bayesian optimization using deep gaussian processes with applications to aerospace system
398"
REFERENCES,0.3622317596566524,"design. Optimization and Engineering, 22:321–361, 2021.
399"
REFERENCES,0.3630901287553648,"[9] Annie Sauer, Robert B Gramacy, and David Higdon. Active learning for deep gaussian process
400"
REFERENCES,0.36394849785407724,"surrogates. Technometrics, 65(1):4–18, 2023.
401"
REFERENCES,0.3648068669527897,"[10] Mengying Lei, Aurelie Labbe, Yuankai Wu, and Lijun Sun. Bayesian kernelized matrix
402"
REFERENCES,0.36566523605150214,"factorization for spatiotemporal traffic data imputation and kriging. IEEE Transactions on
403"
REFERENCES,0.3665236051502146,"Intelligent Transportation Systems, 23(10):18962–18974, 2022.
404"
REFERENCES,0.36738197424892705,"[11] Mengying Lei, Aurelie Labbe, and Lijun Sun. Bayesian complementary kernelized learning for
405"
REFERENCES,0.3682403433476395,"multidimensional spatiotemporal data. arXiv preprint arXiv:2208.09978, 2022.
406"
REFERENCES,0.36909871244635195,"[12] Noel Cressie, Matthew Sainsbury-Dale, and Andrew Zammit-Mangion. Basis-function models
407"
REFERENCES,0.3699570815450644,"in spatial statistics. Annual Review of Statistics and Its Application, 9:373–400, 2022.
408"
REFERENCES,0.3708154506437768,"[13] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
409"
REFERENCES,0.37167381974248925,"Learning Research, 3(Nov):397–422, 2002.
410"
REFERENCES,0.3725321888412017,"[14] Joaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approxi-
411"
REFERENCES,0.37339055793991416,"mate Gaussian process regression. The Journal of Machine Learning Research, 6:1939–1959,
412"
REFERENCES,0.3742489270386266,"2005.
413"
REFERENCES,0.37510729613733906,"[15] Hedibert Freitas Lopes, Esther Salazar, and Dani Gamerman. Spatial dynamic factor analysis.
414"
REFERENCES,0.3759656652360515,"Bayesian Analysis, 3(4):759–792, 2008.
415"
REFERENCES,0.37682403433476397,"[16] Jaakko Luttinen and Alexander Ilin. Variational Gaussian-process factor analysis for modeling
416"
REFERENCES,0.3776824034334764,"spatio-temporal data. Advances in Neural Information Processing Systems, 22:1177–1185,
417"
REFERENCES,0.3785407725321888,"2009.
418"
REFERENCES,0.37939914163090127,"[17] Mengying Lei, Aurelie Labbe, and Lijun Sun. Scalable spatiotemporally varying coefficient
419"
REFERENCES,0.3802575107296137,"modeling with bayesian kernelized tensor regression. arXiv preprint arXiv:2109.00046, 2021.
420"
REFERENCES,0.3811158798283262,"[18] Limin Wang. Karhunen-Loeve expansions and their applications. London School of Economics
421"
REFERENCES,0.38197424892703863,"and Political Science (United Kingdom), 2008.
422"
REFERENCES,0.3828326180257511,"[19] Holger Wendland. Scattered Data Approximation, volume 17. Cambridge university press,
423"
REFERENCES,0.38369098712446353,"2004.
424"
REFERENCES,0.384549356223176,"[20] Rommel G Regis and Christine A Shoemaker. A stochastic radial basis function method for the
425"
REFERENCES,0.38540772532188844,"global optimization of expensive functions. INFORMS Journal on Computing, 19(4):497–509,
426"
REFERENCES,0.38626609442060084,"2007.
427"
REFERENCES,0.3871244635193133,"[21] Gregory Beylkin, Jochen Garcke, and Martin J Mohlenkamp. Multivariate regression and
428"
REFERENCES,0.38798283261802574,"machine learning with sums of separable functions. SIAM Journal on Scientific Computing,
429"
REFERENCES,0.3888412017167382,"31(3):1840–1857, 2009.
430"
REFERENCES,0.38969957081545065,"[22] Christopher K Wikle and Noel Cressie. A dimension-reduced approach to space-time kalman
431"
REFERENCES,0.3905579399141631,"filtering. Biometrika, 86(4):815–829, 1999.
432"
REFERENCES,0.39141630901287555,"[23] Mathilde Chevreuil, Régis Lebrun, Anthony Nouy, and Prashant Rai. A least-squares method
433"
REFERENCES,0.392274678111588,"for sparse low rank approximation of multivariate functions. SIAM/ASA Journal on Uncertainty
434"
REFERENCES,0.3931330472103004,"Quantification, 3(1):897–921, 2015.
435"
REFERENCES,0.39399141630901285,"[24] Alexandra M Schmidt and Anthony O’Hagan. Bayesian inference for non-stationary spatial
436"
REFERENCES,0.3948497854077253,"covariance structure via spatial deformations. Journal of the Royal Statistical Society: Series B
437"
REFERENCES,0.39570815450643776,"(Statistical Methodology), 65(3):743–758, 2003.
438"
REFERENCES,0.3965665236051502,"[25] Momin Jamil and Xin-She Yang.
A literature survey of benchmark functions for global
439"
REFERENCES,0.39742489270386266,"optimization problems. arXiv preprint arXiv:1308.4008, 2013.
440"
REFERENCES,0.3982832618025751,"[26] Donald R Jones, Cary D Perttunen, and Bruce E Stuckman. Lipschitzian optimization without
441"
REFERENCES,0.39914163090128757,"the lipschitz constant. Journal of optimization Theory and Applications, 79(1):157–181, 1993.
442"
REFERENCES,0.4,"[27] John A Nelder and Roger Mead. A simplex method for function minimization. The computer
443"
REFERENCES,0.4008583690987124,"journal, 7(4):308–313, 1965.
444"
REFERENCES,0.40171673819742487,"[28] David Eriksson and Martin Jankowiak. High-dimensional bayesian optimization with sparse
445"
REFERENCES,0.4025751072961373,"axis-aligned subspaces. In Uncertainty in Artificial Intelligence, pages 493–503. PMLR, 2021.
446"
REFERENCES,0.4034334763948498,"[29] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wil-
447"
REFERENCES,0.40429184549356223,"son, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization.
448"
REFERENCES,0.4051502145922747,"Advances in neural information processing systems, 33:21524–21538, 2020.
449"
REFERENCES,0.40600858369098713,"[30] Elizabeth D Dolan and Jorge J Moré. Benchmarking optimization software with performance
450"
REFERENCES,0.4068669527896996,"profiles. Mathematical programming, 91(2):201–213, 2002.
451"
REFERENCES,0.40772532188841204,"[31] Jun Tang, Gang Liu, and Qingtao Pan. A review on representative swarm intelligence algorithms
452"
REFERENCES,0.40858369098712444,"for solving optimization problems: Applications and trends. IEEE/CAA Journal of Automatica
453"
REFERENCES,0.4094420600858369,"Sinica, 8(10):1627–1643, 2021.
454"
REFERENCES,0.41030042918454934,"[32] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-
455"
REFERENCES,0.4111587982832618,"parameter optimization. Advances in neural information processing systems, 24, 2011.
456"
REFERENCES,0.41201716738197425,"[33] Dave Higdon, James Gattiker, Brian Williams, and Maria Rightley. Computer model calibration
457"
REFERENCES,0.4128755364806867,"using high-dimensional output. Journal of the American Statistical Association, 103(482):570–
458"
REFERENCES,0.41373390557939915,"583, 2008.
459"
REFERENCES,0.4145922746781116,"[34] Shandian Zhe, Wei Xing, and Robert M Kirby. Scalable high-order gaussian process regression.
460"
REFERENCES,0.41545064377682406,"In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2611–2620.
461"
REFERENCES,0.41630901287553645,"PMLR, 2019.
462"
REFERENCES,0.4171673819742489,"[35] Havard Rue and Leonhard Held. Gaussian Markov random fields: theory and applications.
463"
REFERENCES,0.41802575107296136,"Chapman and Hall/CRC, 2005.
464"
REFERENCES,0.4188841201716738,"[36] Anastasiia Batsheva, Andrei Chertkov, Gleb Ryzhakov, and Ivan Oseledets. Protes: probabilistic
465"
REFERENCES,0.41974248927038627,"optimization with tensor sampling. Advances in Neural Information Processing Systems, 36:808–
466"
REFERENCES,0.4206008583690987,"823, 2023.
467"
REFERENCES,0.42145922746781117,"Appendix
468"
REFERENCES,0.4223175965665236,"Contents (Appendix)
469"
REFERENCES,0.4231759656652361,"A Model inference
12
470"
REFERENCES,0.4240343347639485,"A.1
Sampling latent functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
471"
REFERENCES,0.4248927038626609,"A.2
Sampling kernel hyperparameters
. . . . . . . . . . . . . . . . . . . . . . . . . .
13
472"
REFERENCES,0.4257510729613734,"A.3
Sampling Λd for latent functions on categorical inputs
. . . . . . . . . . . . . . .
13
473"
REFERENCES,0.42660944206008583,"A.4
Sampling weight vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
474"
REFERENCES,0.4274678111587983,"A.5
Sampling model noise precision
. . . . . . . . . . . . . . . . . . . . . . . . . . .
14
475"
REFERENCES,0.42832618025751074,"B Algorithm of BKTF for BO
15
476"
REFERENCES,0.4291845493562232,"C Optimization for the 2D nonstationary and nonseparable function
15
477"
REFERENCES,0.43004291845493564,"C.1
Data generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
478"
REFERENCES,0.4309012875536481,"C.2
Experimental setting
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
479"
REFERENCES,0.4317596566523605,"C.3
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
480"
REFERENCES,0.43261802575107294,"D Benchmark test functions
17
481"
REFERENCES,0.4334763948497854,"E
Supplementary results on benchmark test functions
19
482"
REFERENCES,0.43433476394849785,"E.1
Performance profiles
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
483"
REFERENCES,0.4351931330472103,"E.2
Quantitative comparison
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
484"
REFERENCES,0.43605150214592275,"E.3
Interpretation of results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
485"
REFERENCES,0.4369098712446352,"E.4
Effects of hyperpriors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
486"
REFERENCES,0.43776824034334766,"E.5
Effects of rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
487"
REFERENCES,0.4386266094420601,"F
Comparison of computational complexity
24
488"
REFERENCES,0.4394849785407725,"G Hyperparameter tuning for machine learning
25
489"
REFERENCES,0.44034334763948496,"H Supplementary results on ML hyperparameter tuning
26
490"
REFERENCES,0.4412017167381974,"A
Model inference
491"
REFERENCES,0.44206008583690987,"Assume an observation dataset Dn = {xi, yi}n
i=1, we exploit an efficient element-wise Gibbs
492"
REFERENCES,0.4429184549356223,"sampling algorithm for model inference.
493"
REFERENCES,0.44377682403433477,"A.1
Sampling latent functions
494"
REFERENCES,0.4446351931330472,"Given the Gaussian prior and Gaussian likelihood of the latent factors gr
d, their posterior distributions
495"
REFERENCES,0.4454935622317597,"are still Gaussian. Let yi
r = yi −PR
h=1
h̸=r
λh
QD
d=1 gh
d
 
xi
d

and yr = [y1
r, . . . , yn
r ]⊤∈Rn for
496"
REFERENCES,0.44635193133047213,"r = 1, . . . , R. Every yr generates a D-dimensional tensor Yr ∈R|S1|×···×|SD| in the Cartesian
497"
REFERENCES,0.4472103004291845,"product space QD
d=1 Sd. We define a binary tensor O with the same size of Y indicating the locations
498"
REFERENCES,0.448068669527897,"of the observation data, where o (xi) = 1 for i ∈[1, n], and other values are zero. The posterior of
499"
REFERENCES,0.44892703862660943,"gr
d is given by
500"
REFERENCES,0.4497854077253219,"p (gr
d | −) = N

gr
d
 [µr
d]∗,
 
[Λr
d]∗−1
,
(11)"
REFERENCES,0.45064377682403434,"where
501"
REFERENCES,0.4515021459227468,"[µr
d]∗=
 
[Λr
d]∗−1 "
REFERENCES,0.45236051502145924,"

τ
 
Y r(d) ⊛O(d)
 "
REFERENCES,0.4532188841201717,"

λr"
O,0.45407725321888415,"1
O"
O,0.45493562231759654,"h=D
h̸=d gr
h  

  

"
O,0.455793991416309,"|
{z
}
a∈R|Sd|"
O,0.45665236051502145,",
(12) 502"
O,0.4575107296137339,"[Λr
d]∗= τ diag "
O,0.45836909871244635,"

O(d) "
O,0.4592274678111588,"

λr"
O,0.46008583690987126,"1
O"
O,0.4609442060085837,"h=D
h̸=d gr
h  

 2 

"
O,0.46180257510729616,"|
{z
}
b∈R|Sd|"
O,0.46266094420600856,"+ (Kr
d)−1 .
(13)"
O,0.463519313304721,"Y r(d) and O(d) are mode-d unfoldings of Yr and O, respectively, with the size of |Sd| ×
503"
O,0.46437768240343347,"(QD
h=1
h̸=d
|Sh|). Note that the vector term a in [µr
d]∗and b in [Λr
d]∗, which are only relevant to
504"
O,0.4652360515021459,"the n observations and corresponding function values, can be computed element-wise instead of
505"
O,0.4660944206008584,"using matrix multiplication and Kronecker product. The point-wise computation can dramatically
506"
O,0.4669527896995708,"reduce the computational cost, especially for a relatively large D, since in such case the number of
507"
O,0.4678111587982833,"observations in BO can be much smaller compared to the number of samples in the entire grid space,
508"
O,0.46866952789699573,"i.e., n ≪QD
d=1 |Sd|.
509"
O,0.4695278969957081,"A.2
Sampling kernel hyperparameters
510"
O,0.4703862660944206,"We update the kernel lengthscale hyperparameters lr
d from their marginal posteriors by integrating
511"
O,0.47124463519313303,"out the latent factors. Let [yr]d = Od vec
 
Y r(d)

∈Rn, where Od ∈Rn×(
QD
d=1 |Sd|) is a binary
512"
O,0.4721030042918455,"matrix obtained by removing the rows corresponding to zeros in vec
 
O(d)

from IQD
d=1 |Sd|. When
513"
O,0.47296137339055794,"sampling the posteriors for kernel hyperparameters under a given d and r, their marginal likelihoods
514"
O,0.4738197424892704,"only relate to [yr]d. The log marginal likelihood of lr
d, for example, is:
515"
O,0.47467811158798284,"log p ([yr]d | lr
d) ∝−1"
O,0.4755364806866953,"2 ([yr]d)⊤Σ−1
[yr]d|lr
d [yr]d −1"
LOG DET,0.47639484978540775,"2 log det

Σ [yr]d|lr
d  ∝τ 2"
LOG DET,0.47725321888412015,"2 ([yr]d)⊤H
 
[Λr
d]∗−1 H⊤[yr]d
|
{z
}
c −1"
LOG DET,0.4781115879828326,"2 log det
 
[Λr
d]∗
−1"
LOG DET,0.47896995708154505,"2 log det (Kr
d) , (14)"
LOG DET,0.4798283261802575,where H = Od
LOG DET,0.48068669527896996,"
λr
N1
h=D
h̸=d
gr
h ⊗I(|Sd|)"
LOG DET,0.4815450643776824,"
∈Rn×|Sd| and Σ [yr]d|lr
d = HKr
dH⊤+ τ −1In. The
516"
LOG DET,0.48240343347639486,"term c in Eq. (14) is a scalar that can be computed with u⊤u, where u = (Lr
d)−1 a is a vector
517"
LOG DET,0.4832618025751073,"of length |Sd|; Lr
d = chol
 
[Λr
d]∗
is the Cholesky factor matrix of [Λr
d]∗. This means that the
518"
LOG DET,0.48412017167381977,"complicated term c can also be calculated element-wise, and it leads to a fast learning process. With
519"
LOG DET,0.48497854077253216,"the marginal likelihood and predefined log-normal hyperpriors, we can get the marginal posteriors
520"
LOG DET,0.4858369098712446,"of the kernel hyperparameters straightforwardly; and we update them by using the slice sampling
521"
LOG DET,0.48669527896995707,"algorithm presented in [17].
522"
LOG DET,0.4875536480686695,"A.3
Sampling Λd for latent functions on categorical inputs
523"
LOG DET,0.488412017167382,"For latent factors in dimensions with categorical variables/inputs, we sample the precision hyperpa-
524"
LOG DET,0.4892703862660944,"rameter Λd in its prior distribution from a Wishart distribution:
525"
LOG DET,0.4901287553648069,"Λd | −∼W

GdG⊤
d + I|Sd|
−1
, |Sd| + R

.
(15)"
LOG DET,0.49098712446351933,"A.4
Sampling weight vector
526"
LOG DET,0.4918454935622318,"Every observed data point has the following distribution:
527 yi ∼N R
X"
LOG DET,0.4927038626609442,"r=1
λr D
Y"
LOG DET,0.49356223175965663,"d=1
gr
d
 
xi
d

= "" D
Y"
LOG DET,0.4944206008583691,"d=1
g1
d
 
xi
d

, . . . , D
Y"
LOG DET,0.49527896995708154,"d=1
gR
d
 
xi
d

#"
LOG DET,0.496137339055794,"λ, τ −1
!"
LOG DET,0.49699570815450644,", i = 1, . . . , n.
(16)"
LOG DET,0.4978540772532189,"Let g (xi) =
QD
d=1 g1
d
 
xi
d

, . . . , QD
d=1 gR
d
 
xi
d
⊤
∈RR and y = (y1, . . . , yn)⊤∈Rn; we then
528"
LOG DET,0.49871244635193135,"have y ∼N

˜G
⊤λ, τ −1In

, where ˜G = [g(x1), . . . , g(xn)] ∈RR×n. The posterior of λ follows
529"
LOG DET,0.4995708154506438,"a Gaussian distribution
530"
LOG DET,0.5004291845493563,"p (λ | −) ∼N

µ∗
λ, (Λ∗
λ)−1
,
(17)"
LOG DET,0.5012875536480687,"where
531"
LOG DET,0.5021459227467812,"µ∗
λ = τ (Λ∗
λ)−1 ˜Gy,
(18)
532"
LOG DET,0.5030042918454936,"Λ∗
λ = τ ˜G ˜G
⊤+ IR.
(19)"
LOG DET,0.503862660944206,"A.5
Sampling model noise precision
533"
LOG DET,0.5047210300429185,"For precision τ, we have a Gamma posterior
534"
LOG DET,0.5055793991416309,"p (τ | −) = Gamma (τ | a∗, b∗) ,
(20)"
LOG DET,0.5064377682403434,"where
535"
LOG DET,0.5072961373390558,a∗= a0 + 1
LOG DET,0.5081545064377683,"2n,
(21) 536"
LOG DET,0.5090128755364807,"b∗= b0 + 1 2 n
X i=1  yi − R
X"
LOG DET,0.5098712446351932,"r=1
λr D
Y"
LOG DET,0.5107296137339056,"d=1
gr
d
 
xi
d

!2"
LOG DET,0.511587982832618,".
(22)"
LOG DET,0.5124463519313305,"B
Algorithm of BKTF for BO
537"
LOG DET,0.5133047210300429,Algorithm 2: BKTF for BO
LOG DET,0.5141630901287554,"Input: Initial dataset D0.
for n = 1 to N do"
LOG DET,0.5150214592274678,for k = 1 to K do
LOG DET,0.5158798283261803,for r = 1 to R do
LOG DET,0.5167381974248927,for d = 1 to D do
LOG DET,0.5175965665236052,"Draw kernel lengthscale hyperparameter lr
d or precision hyperparameter Λd;
Draw latent factors (gr
d)(k);
end for
end for
Draw model noise precision τ (k);
Draw weight vector λ(k);
if k > K0 then"
LOG DET,0.5184549356223176,"Compute and collect ˜F
(k).
end if
end for
Compute mean U and variance V of { ˜F
(k)};
Compute αB-UCB (x | Dn−1, β) as a tensor;
Find next xn = arg maxx αB-UCB (x | Dn−1, β);
Augment the data Dn = Dn−1 ∪{xn, yn}.
end for 538"
LOG DET,0.51931330472103,"C
Optimization for the 2D nonstationary and nonseparable function
539"
LOG DET,0.5201716738197425,"C.1
Data generation
540"
LOG DET,0.5210300429184549,"The function in Figure 1(a) of the paper (see Section 1 Introduction) is a modification of the case
541"
LOG DET,0.5218884120171674,"study used in [11]. It is a 40 × 40 2D process generated in a [1, 2] × [−1, 0] square, with
542"
LOG DET,0.5227467811158798,"Y (x1, x2) = (cos (4 [f1(x1) + f2(x2)]) + sin (4 [f1(x2) −f2(x1)]) −1)"
LOG DET,0.5236051502145923,"× exp

−(x1 −0.5)2 + (x2 −1)2 5"
LOG DET,0.5244635193133047,"
,
(23)"
LOG DET,0.5253218884120172,"where f(x1) = x1 (sin 2x1 + 2), f(x2) = 0.2x2
p"
LOG DET,0.5261802575107296,"99(x2 + 1) + 4, x1 ∈[1, 2], x2 ∈[−1, 0]. This
543"
LOG DET,0.527038626609442,"is a nonstationary, nonseparable, and multimodel function, with the global maximum f(x⋆) = 0.6028
544"
LOG DET,0.5278969957081545,"at x⋆= (1.75, −0.55).
545"
LOG DET,0.5287553648068669,"C.2
Experimental setting
546"
LOG DET,0.5296137339055794,"We randomly select n0 = 30 data points as the initial data and run 50 iterations (i.e., budget)
547"
LOG DET,0.5304721030042918,"of evaluation for optimization. To compare different surrogates, we run the optimization for 20
548"
LOG DET,0.5313304721030043,"replications with different initial datasets. For the proposed BKTF surrogate, we place Matérn 3/2
549"
LOG DET,0.5321888412017167,"kernel functions on the latent factors, set the rank R = 4, and run 1000 MCMC samples for model
550"
LOG DET,0.5330472103004292,"inference where the first 600 samples are burn-in. For comparison of BO methods, we consider
551"
LOG DET,0.5339055793991416,"typical GP surrogate with both EI and UCB (β = 2) as the AF, denoted by GP αEI and GP αUCB
552"
LOG DET,0.534763948497854,"respectively, and use the same Matérn 3/2 kernel for GP surrogate.
553"
LOG DET,0.5356223175965665,"C.3
Results
554"
LOG DET,0.5364806866952789,"Figure 1(b) in Section 1 Introduction of the paper shows the medians along with the 25% and 75%
555"
LOG DET,0.5373390557939914,"quantiles of the optimization results from 20 runs. We see that GP αEI and GP αUCB cannot find the
556"
LOG DET,0.5381974248927038,"global optimum in most cases, and they easily get stuck in the lower left flat area which contains
557"
LOG DET,0.5390557939914163,"easily find local optima. Figure 1(c) illustrates the estimation surface of the function and the estimated
558"
LOG DET,0.5399141630901287,"AF surface from one run. It is clear that BKTF can capture global correlations with limited data. The
559"
LOG DET,0.5407725321888412,"Figure 4: Optimization on the 2D function (Eq. 23): Posterior distributions of model hyperparameters
learned by BKTF."
LOG DET,0.5416309012875536,Figure 5: Optimization on the 2D function (Eq. 23): Mean of latent factors sampled by BKTF.
LOG DET,0.542489270386266,"search points contain areas of almost every peak in the true function, and the peaks of its AF surface
560"
LOG DET,0.5433476394849786,"also reflect the peak area in f.
561"
LOG DET,0.544206008583691,"Figure 4 shows the approximated posterior distributions of model parameters in BKTF. Figure 5 and
562"
LOG DET,0.5450643776824035,"Figure 6 illustrate the posterior mean and the last 20 MCMC samples of the latent factors, respectively.
563"
LOG DET,0.5459227467811159,"Samples of the latent factors in panels (a) and (b) of Figure 6 explain the uncertainty learned by
564"
LOG DET,0.5467811158798284,"BKTF for this optimization problem.
565"
LOG DET,0.5476394849785408,(a) MCMC samples for latent factors (d = 1).
LOG DET,0.5484978540772533,(b) MCMC samples for latent factors (d = 2).
LOG DET,0.5493562231759657,"Figure 6: Optimization on the 2D function (Eq. 23): The last 20 MCMC samples of the latent factors
in the two dimensions learned by BKTF."
LOG DET,0.5502145922746781,"D
Benchmark test functions
566"
LOG DET,0.5510729613733906,"We summarize the characteristics of the benchmark functions tested in Table 1. Figure 7 shows
567"
LOG DET,0.551931330472103,"the functions with 2-dimensional inputs together with the 2D Griewank function. The functional
568"
LOG DET,0.5527896995708155,"expressions and more detailed features of these test functions are provided in the following.
569"
LOG DET,0.5536480686695279,Table 1: Summary of the studied benchmark functions.
LOG DET,0.5545064377682404,"Function
D
Search space
md
Characteristics"
LOG DET,0.5553648068669528,"Branin
2
[−5, 10] × [0, 15]
14
3 global minima, flat
Damavandi
2
[0, 14]2
71
multimodal, global minimum located in small area
Schaffer
2
[−10, 10]2
11
multimodal, global optimum located close to local minima"
LOG DET,0.5562231759656653,Griewank
LOG DET,0.5570815450643777,"3
[−10, 10]3
11
multimodal, many widespread and regularly distributed
4
[−10, 10]4
11
local optima
10
[−10, 10]10
-"
LOG DET,0.5579399141630901,"Hartmann
6
[0, 1]6
12
multimodal, multi-input"
LOG DET,0.5587982832618026,Figure 7: Examples of the tested benchmark functions.
LOG DET,0.559656652360515,(1) Branin function (D = 2)
LOG DET,0.5605150214592275,"f(x1, x2) =

x2 −5.1"
LOG DET,0.5613733905579399,"4π x2
1 + 5"
LOG DET,0.5622317596566524,"π x1 −6
2
+ 10

1 −1 8π"
LOG DET,0.5630901287553648,"
cos(x1) + 10,
(24)"
LOG DET,0.5639484978540773,"where x1 ∈[−5, 10] and x2 ∈[0, 15]. It is a smooth but multimodal function with global minima
570"
LOG DET,0.5648068669527897,"f(x∗) = 0.3978873 at three input points x∗= (−π, 12.275), (π, 2.275), (3π, 2.425).
571"
LOG DET,0.5656652360515021,(2) Damavandi function (D = 2)
LOG DET,0.5665236051502146,"f(x1, x2) = """
LOG DET,0.567381974248927,"1 −

sin[π(x1 −2)] sin[π(x2 −2)]"
LOG DET,0.5682403433476395,π2(x1 −2)(x2 −2) 
LOG DET,0.5690987124463519,"5#

2 + (x1 −7)2 + 2(x2 −7)2
,
(25)"
LOG DET,0.5699570815450644,"where xd ∈[0, 14]. This is a multimodal function with the global minimum f(x∗) = 0 at x∗= (2, 2).
572"
LOG DET,0.5708154506437768,(3) Schaffer function (D = 2)
LOG DET,0.5716738197424893,"f(x1, x2) = 0.5 + sin2 p"
LOG DET,0.5725321888412017,"x2
1 + x2
2 −0.5"
LOG DET,0.5733905579399141,"[1 + 0.001 (x2
1 + x2
2)]2 ,
(26)"
LOG DET,0.5742489270386266,"where xd ∈[−10, 10]. The global minimum value is f(x∗) = 0 at x∗= (0, 0). One characteristic
573"
LOG DET,0.575107296137339,"of this function is that the global minimum is located very close to the local minima.
574"
LOG DET,0.5759656652360515,"(4) Griewank function (D = 3, 4, 10)"
LOG DET,0.5768240343347639,"f(x) = 1 + D
X d=1"
LOG DET,0.5776824034334764,"x2
d
4000 − D
Y"
LOG DET,0.5785407725321888,"d=1
cos
 xd
√ d"
LOG DET,0.5793991416309013,"
,
(27)"
LOG DET,0.5802575107296137,"where xd ∈[−10, 10]. This is a multimodal function with global minimum f(x∗) = 0 at x∗= (0, 0).
575"
LOG DET,0.5811158798283261,"(5) Hartmann function (D = 6)
A nonseparable function with multidimensional inputs and
576"
LOG DET,0.5819742489270386,"multiple local minima.
577"
LOG DET,0.582832618025751,f(x) = −
X,0.5836909871244635,"4
X"
X,0.5845493562231759,"j=1
cj exp  −"
X,0.5854077253218885,"6
X"
X,0.5862660944206008,"d=1
ajd(xd −bjd)2
!"
X,0.5871244635193134,",
(28)"
X,0.5879828326180258,"where
578"
X,0.5888412017167381,A = [ajd] =  
X,0.5896995708154507,"10
3
17
3.5
1.7
8
0.05
10
17
0.1
8
14
3
3.5
1.7
10
17
8
17
8
0.05
10
0.1
14 "
X,0.590557939914163,", c = [cj] =  "
X,0.5914163090128756,"1
1.2
3
3.2  ,"
X,0.592274678111588,B = [bjd] =  
X,0.5931330472103005,"0.1312
0.1696
0.5569
0.0124
0.8283
0.5586
0.2329
0.4135
0.8307
0.3736
0.1004
0.9991
0.2348
0.1451
0.3522
0.2833
0.3047
0.6650
0.4047
0.8828
0.8732
0.5743
0.1091
0.0381  , (29)"
X,0.5939914163090129,"xd ∈[0, 1]. The 6-dimensional case has 6 local minima, and the global minimum is f(x∗) =
579"
X,0.5948497854077254,"−3.32237 at x∗= (0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.657301).
580"
X,0.5957081545064378,"Note that all these minimization problems can be easily transformed as a maximization optimization
581"
X,0.5965665236051502,"problem, i.e., max −f(x).
582"
X,0.5974248927038627,"E
Supplementary results on benchmark test functions
583"
X,0.5982832618025751,"E.1
Performance profiles
584"
X,0.5991416309012876,"When computing the performance profiles (PPs), i.e., Dolan-Moré curves [30], we consider the
585"
X,0.6,"number of function evaluations to find the global optimum as the performance measure. Specifically,
586"
X,0.6008583690987125,"let tp,a denote the number of evaluations used by method/solver a to reach the global solution in
587"
X,0.6017167381974249,"experiment p (a lower value is better). The value is equal to Np + 100 if the method cannot find the
588"
X,0.6025751072961374,"global optimum with Np being the given observation budget for experiment p. The performance ratio
589"
X,0.6034334763948498,"γp,a =
tp,a
min{tp,a : a ∈A},
(30)"
X,0.6042918454935622,"where A represents the set that includes all comparing models, and the performance profile for each
590"
X,0.6051502145922747,"method is the distribution of p (γp,a ≤ρ) in terms of a factor ρ. We set ρ = 1 : Nmax + 1, where
591"
X,0.6060085836909871,"Nmax = max Np is the largest observation budget assumed for the compared experiments. We define
592"
X,0.6068669527896996,"the problem set {P | ∀p ∈P} as the 10 runs for every function and draw the performance profiles
593"
X,0.607725321888412,"of each model, also set P as the 70 experiments in the 7 tested functions and estimate the overall
594"
X,0.6085836909871245,"performance profiles.
595"
X,0.6094420600858369,"We show the obtained PPs across test functions (i.e., overall PPs) in Figure 2 (bottom right) for
596"
X,0.6103004291845494,"illustration (see Section 5.1 of the paper); the results of all the tested functions are shown below in
597"
X,0.6111587982832618,"Figure 8. Note we only compute PPs for the methods that compared on all test functions, i.e., deepGP
598"
X,0.6120171673819742,"and SAASBO are not considered.
599"
X,0.6128755364806867,"Table 2 gives the AUC (area under the curve) values of these curves, where the AUC of the overall
600"
X,0.6137339055793991,"performance profiles is taken as the metric to compare the overall performances. As can be seen,
601"
X,0.6145922746781116,"BKTF obtains the best performance across all the considered functions. In addition, for most of
602"
X,0.615450643776824,"the test functions, the AUC of grid-based baseline models is comparable with those of continuous
603"
X,0.6163090128755365,"GP-based models, suggesting that discretization of the continuous space is feasible to simplify the
604"
X,0.6171673819742489,"optimization problem.
605"
X,0.6180257510729614,Figure 8: Performance profiles on the standard test functions.
X,0.6188841201716738,"E.2
Quantitative comparison
606"
X,0.6197424892703862,"We compare the last step regret, average costs of evaluations, and AUC of PPs of different methods
607"
X,0.6206008583690987,"on benchmark test functions in Table 2.
608"
X,0.6214592274678111,"Table 2: Optimization on test functions: regret when n = N (mean ± std.) / Average costs of
evaluations / AUC of PPs."
X,0.6223175965665236,"f(x)
GP
GP
GPgrid
GPgrid
additive
BKTF
BKTF
deepGP
SAAS
(D)
αEI
αUCB
αEI
αUCB
GP
random
BO B (2)"
X,0.623175965665236,regret
X,0.6240343347639485,"0.01
0.01
0.31
0.24
0.05
0.00
0.00
0.02
-
±
±
±
±
±
±
±
±
0.01
0.01
0.62
0.64
0.09
0.00
0.00
0.02"
X,0.6248927038626609,"Cost
≈44
≈42
≈23
≈36
≈100
≈47
≈4
≈47
-"
X,0.6257510729613734,"AUC
32.14
32.14
44.86
42.29
2.14
43.40
48.44
-
- D (2)"
X,0.6266094420600858,regret
X,0.6274678111587982,"2.00
2.00
1.60
2.00
2.00
0.60
0.00
3.00
-
±
±
±
±
±
±
±
±
0.00
0.00
0.80
0.00
0.00
0.92
0.00
0.007"
X,0.6283261802575107,"Cost
-
-
-
-
-
≈48
≈5
-
-"
X,0.6291845493562231,"AUC
12.17
12.17
19.95
17.10
12.17
37.05
49.82
-
- S (2)"
X,0.6300429184549357,regret
X,0.630901287553648,"0.02
0.02
0.10
0.09
0.03
0.00
0.00
0.08
-
±
±
±
±
±
±
±
±
0.02
0.02
0.15
0.07
0.03
0.00
0.00
0.04"
X,0.6317596566523606,"Cost
≈36
≈44
> 50
> 50
≈43
≈54
≈22
> 50
-"
X,0.632618025751073,"AUC
41.73
39.51
31.74
29.74
37.00
44.60
48.80
-
- G (3)"
X,0.6334763948497855,regret
X,0.6343347639484979,"0.14
0.25
0.23
0.22
0.10
0.00
0.00
-
-
±
±
±
±
±
±
±
0.14
0.10
0.13
0.12
0.09
0.00
0.00"
X,0.6351931330472103,"Cost
> 100
> 100
> 100
> 100
> 100
≈47
≈43
-
-"
X,0.6360515021459228,"AUC
48.90
47.69
47.69
47.69
47.69
50.44
50.78
-
- G (4)"
X,0.6369098712446352,"regret
0.10
0.19
0.38
0.27
0.13
0.00
0.00
-
-
±
±
±
±
±
±
±
0.07
0.12
0.19
0.17
0.11
0.00
0.00"
X,0.6377682403433477,"Cost
> 100
> 100
> 100
> 100
> 100
≈87
≈68
-
-"
X,0.6386266094420601,"AUC
79.32
77.61
77.61
77.61
78.17
80.01
80.40
-
- H (6)"
X,0.6394849785407726,regret
X,0.640343347639485,"0.12
0.07
0.70
0.79
0.48
1e-5
0.00
-
0.19
±
±
±
±
±
±
±
±
0.07
0.07
0.70
0.61
0.17
1e-5
0.00
0.48"
X,0.6412017167381975,"Cost
> 100
> 100
> 100
> 100
> 100
≈154
≈60
-
> 100"
X,0.6420600858369099,"AUC
78.11
78.11
79.18
78.11
78.11
78.55
80.78
-
-"
X,0.6429184549356223,G (10)
X,0.6437768240343348,regret
X,0.6446351931330472,"0.36
0.38
-
-
0.25
0.00
-
-
0.14
±
±
-
-
±
±
±
0.07
0.10
-
-
0.30
0.00
0.10"
X,0.6454935622317597,"Cost
> 200
> 200
-
-
> 150
≈124
-
-
> 200"
X,0.6463519313304721,"AUC
197.55
197.55
-
-
199.38
200.77
-
-
-"
X,0.6472103004291846,"Overall
AUC
186.49
185.75
188.80
187.59
185.40
196.40
199.51
-
-
Best results are highlighted in bold fonts. B: Branin; D: Damavandi; S: Schaffer; G: Griewank; H: Hartmann."
X,0.648068669527897,"E.3
Interpretation of results
609"
X,0.6489270386266094,"The basis functions learned by BKTF are interpretable. For example, Figure 9 shows the latent factors
610"
X,0.6497854077253219,"(r = 1) obtained at the last iteration of function evaluation in one run on 3D Griewank function.
611"
X,0.6506437768240343,"We see that BKTF can learn the periodicity (global structure) of the function benefited from the
612"
X,0.6515021459227468,"low-rank modeling. On the other hand, a stationary and separable GP cannot, other than using a
613"
X,0.6523605150214592,"specific kernel function such as the periodic kernel, which however requires strong prior knowledge
614"
X,0.6532188841201717,"to set the periodicity kernel hyperparameter.
615"
X,0.6540772532188841,Figure 9: Examples of latent factors learned by BKTF on 3D Griewank function.
X,0.6549356223175966,Figure 10: Effects of hyperpriors on Branin function: Optimization with different hyperpriors.
X,0.655793991416309,"E.4
Effects of hyperpriors
616"
X,0.6566523605150214,"We compare the optimization performance on Branin function with different hyperprior settings in
617"
X,0.6575107296137339,"Figure 10 as an example to illustrate the effects of hyperpriors. Specifically, we compare the optimiza-
618"
X,0.6583690987124463,"tion results under several hyperprior assumptions of µl, when τ −1
l
is set as 0.5. As can be seen, BKTF
619"
X,0.6592274678111588,"is not able to reach the global minimum with too small or too large mean assumptions (comparable to
620"
X,0.6600858369098712,"[0, 1]) on the kernel lengthscales l, for example in the cases where µl = {log (0.05), log (2)}. In con-
621"
X,0.6609442060085837,"trast, it finds the global optimum after 4 iterations of function evaluations when µl = log (0.5), see the
622"
X,0.6618025751072961,"purple line. These imply the importance of hyperprior selection. The reason is that in the first several
623"
X,0.6626609442060086,"evaluations, since the observations are rare, the prior basically determines the exploration-exploitation
624"
X,0.663519313304721,"balance and guides the search process.
625"
X,0.6643776824034334,"Figure 11 shows the approximated posterior distributions for kernel hyperparameters and model noise
626"
X,0.6652360515021459,"variance when τ −1
l
= 0.5, µl = log (0.5). We see that for the re-scaled input space and normalized
627"
X,0.6660944206008583,"function output, the sampled length scales are around half of the input domain. Such settings are
628"
X,0.6669527896995708,"reasonable to capture the correlations between the observations and are also interpretable.
629"
X,0.6678111587982832,"The effects of hyper-priors on other functions are similar, and we choose an appropriate setting
630"
X,0.6686695278969957,"relevant to the input range. The hyper-prior on τ impacts the uncertainty of the latent factors, for
631"
X,0.6695278969957081,"example a large model noise assumption allows more variances in the factors. The role of {a0, b0}
632"
X,0.6703862660944206,"becomes more important when the objective function is complex that BKTF cannot well describe the
633"
X,0.671244635193133,"function with limited observations. Generally, we select the priors that make the noise variances not
634"
X,0.6721030042918454,"quite large, such as the results of τ −1 shown in Figure 4 and Figure 11. An example of the uncertainty
635"
X,0.672961373390558,"provided by BKTF is explained in Appendix C (see Figure 6).
636"
X,0.6738197424892703,"Figure 11: Effects of hyperpriors on Branin function: Posterior probability distributions of length-
scales and model noise variance when τ −1
l
= 0.5, µl = log(0.5)."
X,0.6746781115879829,"E.5
Effects of rank
637"
X,0.6755364806866953,"Under a fully Bayesian treatment, kernel hyperparameters of BKTF are automatically sampled by
638"
X,0.6763948497854078,"MCMC with proper priors; the only selected parameter is the model rank. We test the effects of rank
639"
X,0.6772532188841202,"specification for the proposed BKTF surrogate on the 2D nonstationary nonseparable function defined
640"
X,0.6781115879828327,"in Section 1 Introduction (see Figure 1 and Appendix C). We use the same experiment settings as in
641"
X,0.6789699570815451,"Appendix C, i.e., 30 initial observations and 50 budget.
642"
X,0.6798283261802575,"The results are given in Figure 12 and 13, where we compare the optimization performance of BKTF
643"
X,0.68068669527897,"with rank R = {2, 4, 6, 8} and two GP-based surrogate models: GP αEI and GP αUCB. To clearly
644"
X,0.6815450643776824,"illustrate the results, we only show the comparison on one run. In Figure 12, we compare the regret
645"
X,0.6824034334763949,"from different models, and in Figure 13 we compute and compare the mean CRPS (continuous ranked
646"
X,0.6832618025751073,"probability score) on the unobserved points.
647"
X,0.6841201716738198,"CRPS is a widely applied metric for evaluating the performance of UQ for probabilistic models. With
648"
X,0.6849785407725322,"Gaussian likelihoods, CRPS can be defined as:
649"
X,0.6858369098712447,"CRPS = −1 n′ n′
X"
X,0.6866952789699571,"i=1
σi"
X,0.6875536480686695," 1
√π −2ψ
fi −ˆyi σi"
X,0.688412017167382,"
−fi −ˆyi σi"
X,0.6892703862660944,"
2Φ
fi −ˆyi σi"
X,0.6901287553648069,"
−1

,
(31)"
X,0.6909871244635193,"where n′ is the number of unknown points in the defined space, i.e., n′ = QD
d=1 md −n, ˆyi and
650"
X,0.6918454935622318,"σi are the approximated posterior mean and std. for the ith data point, respectively, fi denotes the
651"
X,0.6927038626609442,"true value for the ith point, and ψ (·) and Φ (·) are the PDF (probability density function) and CDF
652"
X,0.6935622317596567,"(cumulative distribution function) of standard normal, respectively.
653"
X,0.6944206008583691,"We see that BKTF successfully finds the global optimum with rank from 2 to 8, and obtains better
654"
X,0.6952789699570815,"(lower) CRPS values than GP baseline surrogates during the search processes. These results indicate
655"
X,0.696137339055794,"that the proposed fully Bayesian framework is robust to the rank setting and can avoid overfitting.
656"
X,0.6969957081545064,"Figure 12: Effects of rank specification on the test function defined in Introduction (see Appendix C,
Eq. 23): Comparison of optimization performance."
X,0.6978540772532189,"Figure 13: Effects of rank specification on the test function defined in Introduction (see Appendix C,
Eq. 23): Comparison of CRPS."
X,0.6987124463519313,"F
Comparison of computational complexity
657"
X,0.6995708154506438,"We compare the computational complexity of BKTF, BKTFrandom and the baseline methods applied
658"
X,0.7004291845493562,"on test functions in Table 3, where n is the number of observations, md is the number of interpo-
659"
X,0.7012875536480687,"lation points for the dth dimension, and nrandom denotes the number of candidates we selected in
660"
X,0.7021459227467811,"BKTFrandom, which is 20k in the conducted experiments.
661"
X,0.7030042918454935,"As can be seen, theoretically the proposed model BKTF/BKTFrandom has the lowest computational
662"
X,0.703862660944206,"complexity.
663"
X,0.7047210300429184,"Table 3: Comparison of model complexity.
Model
Complexity"
X,0.7055793991416309,"GP αEI
O
 
n3"
X,0.7064377682403433,"GP αUCB
O
 
n3"
X,0.7072961373390558,"GPgrid αEI
O
QD
d=1 md
3"
X,0.7081545064377682,"GPgrid αUCB
O
QD
d=1 md
3"
X,0.7090128755364807,"additive GP
O
 
n3"
X,0.7098712446351931,"BKTF
min
n
O
 
n3
, O
PD
d=1 m3
d
o"
X,0.7107296137339055,"BKTFrandom
O
 
n3
random
"
X,0.711587982832618,"deepGP
O
QD
d=1 md
3"
X,0.7124463519313304,"G
Hyperparameter tuning for machine learning
664"
X,0.7133047210300429,"Table 4 lists all the hyperparameters in the tuning tasks.
665"
X,0.7141630901287553,"Table 4: Hyperparameters of the tested ML algorithms.
Dataset
Algorithm
Hyperparameters
Type
Search space MNIST"
X,0.7150214592274678,RF classifier
X,0.7158798283261802,"no. of estimators
integer
[10, 100]
max depth
integer
[5, 50]
max features
integer
[1, 64]
min samples split
integer
[2, 11]
min samples leaf
integer
[1,11]
criterion
categorical
gini, entropy"
X,0.7167381974248928,NN classifier
X,0.7175965665236052,"neurons
integer
[10, 100]
batch size
integer
[16, 64]
epochs
integer
[20, 50]
patience
integer
[3, 20]
optimizer
categorical
adam, rmsprop, sgd
activation
categorical
relu, tanh"
X,0.7184549356223175,Boston housing
X,0.7193133047210301,RF regressor
X,0.7201716738197425,"no. of estimators
integer
[10, 100]
max depth
integer
[5, 50]
max features
integer
[1, 13]
min samples split
integer
[2, 11]"
X,0.721030042918455,NN regressor
X,0.7218884120171674,"neurons
integer
[10, 100]
batch size
integer
[16, 64]
epochs
integer
[20, 50]"
X,0.7227467811158799,"H
Supplementary results on ML hyperparameter tuning
666"
X,0.7236051502145923,"We summarize the final accuracy obtained with different BO methods for MNIST classification and
667"
X,0.7244635193133048,"the final MSE for the regression task in Table 5.
668"
X,0.7253218884120172,Table 5: Final accuracy for (a) MNIST classification and MSE for (b) Boston housing regression.
X,0.7261802575107296,"(a) MNIST
(b) Boston housing
RF classifier
NN classifier
RF regressor
NN regressor"
X,0.7270386266094421,"RS
96.09 ± 0.05
97.59 ± 0.37
26.10 ± 0.45
40.43 ± 3.91
PSO
95.84 ± 0.14
97.54 ± 0.51
26.07 ± 0.44
44.40 ± 6.29
GP αEI
96.10 ± 0.07
98.04 ± 0.05
26.19 ± 0.45
38.46 ± 3.31
GP αUCB
96.06 ± 0.05
97.46 ± 0.64
26.34 ± 0.35
36.78 ± 1.91
BO-TPE
96.10 ± 0.06
97.52 ± 0.06
26.27 ± 0.31
36.40 ± 4.72
BKTF
-
-
25.03 ± 0.18
30.84 ± 1.13
BKTFrandom
96.38 ± 0.04
98.16 ± 0.09
-
-
The values are presented as mean ± std. Best results are highlighted in bold fonts."
X,0.7278969957081545,"NeurIPS Paper Checklist
669"
CLAIMS,0.728755364806867,"1. Claims
670"
CLAIMS,0.7296137339055794,"Question: Do the main claims made in the abstract and introduction accurately reflect the
671"
CLAIMS,0.7304721030042919,"paper’s contributions and scope?
672"
CLAIMS,0.7313304721030043,"Answer: [Yes]
673"
CLAIMS,0.7321888412017168,"Justification: Please see Abstract and Section Introduction for more details.
674"
CLAIMS,0.7330472103004292,"Guidelines:
675"
CLAIMS,0.7339055793991416,"• The answer NA means that the abstract and introduction do not include the claims
676"
CLAIMS,0.7347639484978541,"made in the paper.
677"
CLAIMS,0.7356223175965665,"• The abstract and/or introduction should clearly state the claims made, including the
678"
CLAIMS,0.736480686695279,"contributions made in the paper and important assumptions and limitations. A No or
679"
CLAIMS,0.7373390557939914,"NA answer to this question will not be perceived well by the reviewers.
680"
CLAIMS,0.7381974248927039,"• The claims made should match theoretical and experimental results, and reflect how
681"
CLAIMS,0.7390557939914163,"much the results can be expected to generalize to other settings.
682"
CLAIMS,0.7399141630901288,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
683"
CLAIMS,0.7407725321888412,"are not attained by the paper.
684"
LIMITATIONS,0.7416309012875536,"2. Limitations
685"
LIMITATIONS,0.7424892703862661,"Question: Does the paper discuss the limitations of the work performed by the authors?
686"
LIMITATIONS,0.7433476394849785,"Answer: [Yes]
687"
LIMITATIONS,0.744206008583691,"Justification: We discuss the limitations in Section 6 Conclusion.
688"
LIMITATIONS,0.7450643776824034,"Guidelines:
689"
LIMITATIONS,0.7459227467811159,"• The answer NA means that the paper has no limitation while the answer No means that
690"
LIMITATIONS,0.7467811158798283,"the paper has limitations, but those are not discussed in the paper.
691"
LIMITATIONS,0.7476394849785408,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
692"
LIMITATIONS,0.7484978540772532,"• The paper should point out any strong assumptions and how robust the results are to
693"
LIMITATIONS,0.7493562231759656,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
694"
LIMITATIONS,0.7502145922746781,"model well-specification, asymptotic approximations only holding locally). The authors
695"
LIMITATIONS,0.7510729613733905,"should reflect on how these assumptions might be violated in practice and what the
696"
LIMITATIONS,0.751931330472103,"implications would be.
697"
LIMITATIONS,0.7527896995708154,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
698"
LIMITATIONS,0.7536480686695279,"only tested on a few datasets or with a few runs. In general, empirical results often
699"
LIMITATIONS,0.7545064377682403,"depend on implicit assumptions, which should be articulated.
700"
LIMITATIONS,0.7553648068669528,"• The authors should reflect on the factors that influence the performance of the approach.
701"
LIMITATIONS,0.7562231759656652,"For example, a facial recognition algorithm may perform poorly when image resolution
702"
LIMITATIONS,0.7570815450643776,"is low or images are taken in low lighting. Or a speech-to-text system might not be
703"
LIMITATIONS,0.7579399141630901,"used reliably to provide closed captions for online lectures because it fails to handle
704"
LIMITATIONS,0.7587982832618025,"technical jargon.
705"
LIMITATIONS,0.759656652360515,"• The authors should discuss the computational efficiency of the proposed algorithms
706"
LIMITATIONS,0.7605150214592274,"and how they scale with dataset size.
707"
LIMITATIONS,0.76137339055794,"• If applicable, the authors should discuss possible limitations of their approach to
708"
LIMITATIONS,0.7622317596566524,"address problems of privacy and fairness.
709"
LIMITATIONS,0.7630901287553649,"• While the authors might fear that complete honesty about limitations might be used by
710"
LIMITATIONS,0.7639484978540773,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
711"
LIMITATIONS,0.7648068669527897,"limitations that aren’t acknowledged in the paper. The authors should use their best
712"
LIMITATIONS,0.7656652360515022,"judgment and recognize that individual actions in favor of transparency play an impor-
713"
LIMITATIONS,0.7665236051502146,"tant role in developing norms that preserve the integrity of the community. Reviewers
714"
LIMITATIONS,0.7673819742489271,"will be specifically instructed to not penalize honesty concerning limitations.
715"
THEORY ASSUMPTIONS AND PROOFS,0.7682403433476395,"3. Theory Assumptions and Proofs
716"
THEORY ASSUMPTIONS AND PROOFS,0.769098712446352,"Question: For each theoretical result, does the paper provide the full set of assumptions and
717"
THEORY ASSUMPTIONS AND PROOFS,0.7699570815450644,"a complete (and correct) proof?
718"
THEORY ASSUMPTIONS AND PROOFS,0.7708154506437769,"Answer: [Yes]
719"
THEORY ASSUMPTIONS AND PROOFS,0.7716738197424893,"Justification: We illustrate methodology and technical details of the proposed model in
720"
THEORY ASSUMPTIONS AND PROOFS,0.7725321888412017,"Section 3 BKTF for Bayesian optimization and Appendix A Model inference.
721"
THEORY ASSUMPTIONS AND PROOFS,0.7733905579399142,"Guidelines:
722"
THEORY ASSUMPTIONS AND PROOFS,0.7742489270386266,"• The answer NA means that the paper does not include theoretical results.
723"
THEORY ASSUMPTIONS AND PROOFS,0.7751072961373391,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
724"
THEORY ASSUMPTIONS AND PROOFS,0.7759656652360515,"referenced.
725"
THEORY ASSUMPTIONS AND PROOFS,0.776824034334764,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
726"
THEORY ASSUMPTIONS AND PROOFS,0.7776824034334764,"• The proofs can either appear in the main paper or the supplemental material, but if
727"
THEORY ASSUMPTIONS AND PROOFS,0.7785407725321889,"they appear in the supplemental material, the authors are encouraged to provide a short
728"
THEORY ASSUMPTIONS AND PROOFS,0.7793991416309013,"proof sketch to provide intuition.
729"
THEORY ASSUMPTIONS AND PROOFS,0.7802575107296137,"• Inversely, any informal proof provided in the core of the paper should be complemented
730"
THEORY ASSUMPTIONS AND PROOFS,0.7811158798283262,"by formal proofs provided in appendix or supplemental material.
731"
THEORY ASSUMPTIONS AND PROOFS,0.7819742489270386,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
732"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7828326180257511,"4. Experimental Result Reproducibility
733"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7836909871244635,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
734"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.784549356223176,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
735"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7854077253218884,"of the paper (regardless of whether the code and data are provided or not)?
736"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7862660944206008,"Answer: [Yes]
737"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7871244635193133,"Justification: We illustrate detailed information for experiment implementation in Section 5
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7879828326180257,"Experiments and Appendices C-G.
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7888412017167382,"Guidelines:
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7896995708154506,"• The answer NA means that the paper does not include experiments.
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7905579399141631,"• If the paper includes experiments, a No answer to this question will not be perceived
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7914163090128755,"well by the reviewers: Making the paper reproducible is important, regardless of
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.792274678111588,"whether the code and data are provided or not.
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7931330472103004,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7939914163090128,"to make their results reproducible or verifiable.
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7948497854077253,"• Depending on the contribution, reproducibility can be accomplished in various ways.
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7957081545064377,"For example, if the contribution is a novel architecture, describing the architecture fully
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7965665236051502,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7974248927038626,"be necessary to either make it possible for others to replicate the model with the same
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7982832618025751,"dataset, or provide access to the model. In general. releasing code and data is often
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7991416309012875,"one good way to accomplish this, but reproducibility can also be provided via detailed
752"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8008583690987124,"of a large language model), releasing of a model checkpoint, or other means that are
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8017167381974248,"appropriate to the research performed.
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8025751072961373,"• While NeurIPS does not require releasing code, the conference does require all submis-
756"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8034334763948497,"sions to provide some reasonable avenue for reproducibility, which may depend on the
757"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8042918454935623,"nature of the contribution. For example
758"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8051502145922746,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
759"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8060085836909872,"to reproduce that algorithm.
760"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8068669527896996,"(b) If the contribution is primarily a new model architecture, the paper should describe
761"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8077253218884121,"the architecture clearly and fully.
762"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8085836909871245,"(c) If the contribution is a new model (e.g., a large language model), then there should
763"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8094420600858369,"either be a way to access this model for reproducing the results or a way to reproduce
764"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8103004291845494,"the model (e.g., with an open-source dataset or instructions for how to construct
765"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8111587982832618,"the dataset).
766"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8120171673819743,"(d) We recognize that reproducibility may be tricky in some cases, in which case
767"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8128755364806867,"authors are welcome to describe the particular way they provide for reproducibility.
768"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8137339055793992,"In the case of closed-source models, it may be that access to the model is limited in
769"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8145922746781116,"some way (e.g., to registered users), but it should be possible for other researchers
770"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8154506437768241,"to have some path to reproducing or verifying the results.
771"
OPEN ACCESS TO DATA AND CODE,0.8163090128755365,"5. Open access to data and code
772"
OPEN ACCESS TO DATA AND CODE,0.8171673819742489,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
773"
OPEN ACCESS TO DATA AND CODE,0.8180257510729614,"tions to faithfully reproduce the main experimental results, as described in supplemental
774"
OPEN ACCESS TO DATA AND CODE,0.8188841201716738,"material?
775"
OPEN ACCESS TO DATA AND CODE,0.8197424892703863,"Answer: [Yes]
776"
OPEN ACCESS TO DATA AND CODE,0.8206008583690987,"Justification: We provide the code for the 2D test function in supplementary material.
777"
OPEN ACCESS TO DATA AND CODE,0.8214592274678112,"Guidelines:
778"
OPEN ACCESS TO DATA AND CODE,0.8223175965665236,"• The answer NA means that paper does not include experiments requiring code.
779"
OPEN ACCESS TO DATA AND CODE,0.8231759656652361,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
780"
OPEN ACCESS TO DATA AND CODE,0.8240343347639485,"public/guides/CodeSubmissionPolicy) for more details.
781"
OPEN ACCESS TO DATA AND CODE,0.8248927038626609,"• While we encourage the release of code and data, we understand that this might not be
782"
OPEN ACCESS TO DATA AND CODE,0.8257510729613734,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
783"
OPEN ACCESS TO DATA AND CODE,0.8266094420600858,"including code, unless this is central to the contribution (e.g., for a new open-source
784"
OPEN ACCESS TO DATA AND CODE,0.8274678111587983,"benchmark).
785"
OPEN ACCESS TO DATA AND CODE,0.8283261802575107,"• The instructions should contain the exact command and environment needed to run to
786"
OPEN ACCESS TO DATA AND CODE,0.8291845493562232,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
787"
OPEN ACCESS TO DATA AND CODE,0.8300429184549356,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
788"
OPEN ACCESS TO DATA AND CODE,0.8309012875536481,"• The authors should provide instructions on data access and preparation, including how
789"
OPEN ACCESS TO DATA AND CODE,0.8317596566523605,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
790"
OPEN ACCESS TO DATA AND CODE,0.8326180257510729,"• The authors should provide scripts to reproduce all experimental results for the new
791"
OPEN ACCESS TO DATA AND CODE,0.8334763948497854,"proposed method and baselines. If only a subset of experiments are reproducible, they
792"
OPEN ACCESS TO DATA AND CODE,0.8343347639484978,"should state which ones are omitted from the script and why.
793"
OPEN ACCESS TO DATA AND CODE,0.8351931330472103,"• At submission time, to preserve anonymity, the authors should release anonymized
794"
OPEN ACCESS TO DATA AND CODE,0.8360515021459227,"versions (if applicable).
795"
OPEN ACCESS TO DATA AND CODE,0.8369098712446352,"• Providing as much information as possible in supplemental material (appended to the
796"
OPEN ACCESS TO DATA AND CODE,0.8377682403433476,"paper) is recommended, but including URLs to data and code is permitted.
797"
OPEN ACCESS TO DATA AND CODE,0.8386266094420601,"6. Experimental Setting/Details
798"
OPEN ACCESS TO DATA AND CODE,0.8394849785407725,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
799"
OPEN ACCESS TO DATA AND CODE,0.8403433476394849,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
800"
OPEN ACCESS TO DATA AND CODE,0.8412017167381974,"results?
801"
OPEN ACCESS TO DATA AND CODE,0.8420600858369098,"Answer: [Yes]
802"
OPEN ACCESS TO DATA AND CODE,0.8429184549356223,"Justification: We specify the experimental setting and implementation details in Section 5
803"
OPEN ACCESS TO DATA AND CODE,0.8437768240343347,"Experiments and Appendices C-G.
804"
OPEN ACCESS TO DATA AND CODE,0.8446351931330472,"Guidelines:
805"
OPEN ACCESS TO DATA AND CODE,0.8454935622317596,"• The answer NA means that the paper does not include experiments.
806"
OPEN ACCESS TO DATA AND CODE,0.8463519313304722,"• The experimental setting should be presented in the core of the paper to a level of detail
807"
OPEN ACCESS TO DATA AND CODE,0.8472103004291845,"that is necessary to appreciate the results and make sense of them.
808"
OPEN ACCESS TO DATA AND CODE,0.848068669527897,"• The full details can be provided either with the code, in appendix, or as supplemental
809"
OPEN ACCESS TO DATA AND CODE,0.8489270386266095,"material.
810"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8497854077253219,"7. Experiment Statistical Significance
811"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8506437768240344,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
812"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8515021459227468,"information about the statistical significance of the experiments?
813"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8523605150214593,"Answer: [Yes]
814"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8532188841201717,"Justification: We repeat the experiments certain times and report the mean with std. results
815"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8540772532188842,"in Figure 1, 2, 3, and Table 2, 5.
816"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8549356223175966,"Guidelines:
817"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.855793991416309,"• The answer NA means that the paper does not include experiments.
818"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8566523605150215,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
819"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8575107296137339,"dence intervals, or statistical significance tests, at least for the experiments that support
820"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8583690987124464,"the main claims of the paper.
821"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8592274678111588,"• The factors of variability that the error bars are capturing should be clearly stated (for
822"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8600858369098713,"example, train/test split, initialization, random drawing of some parameter, or overall
823"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8609442060085837,"run with given experimental conditions).
824"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8618025751072962,"• The method for calculating the error bars should be explained (closed form formula,
825"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8626609442060086,"call to a library function, bootstrap, etc.)
826"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.863519313304721,"• The assumptions made should be given (e.g., Normally distributed errors).
827"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8643776824034335,"• It should be clear whether the error bar is the standard deviation or the standard error
828"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8652360515021459,"of the mean.
829"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8660944206008584,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
830"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8669527896995708,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
831"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8678111587982833,"of Normality of errors is not verified.
832"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8686695278969957,"• For asymmetric distributions, the authors should be careful not to show in tables or
833"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8695278969957082,"figures symmetric error bars that would yield results that are out of range (e.g. negative
834"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8703862660944206,"error rates).
835"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.871244635193133,"• If error bars are reported in tables or plots, The authors should explain in the text how
836"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8721030042918455,"they were calculated and reference the corresponding figures or tables in the text.
837"
EXPERIMENTS COMPUTE RESOURCES,0.8729613733905579,"8. Experiments Compute Resources
838"
EXPERIMENTS COMPUTE RESOURCES,0.8738197424892704,"Question: For each experiment, does the paper provide sufficient information on the com-
839"
EXPERIMENTS COMPUTE RESOURCES,0.8746781115879828,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
840"
EXPERIMENTS COMPUTE RESOURCES,0.8755364806866953,"the experiments?
841"
EXPERIMENTS COMPUTE RESOURCES,0.8763948497854077,"Answer: [Yes]
842"
EXPERIMENTS COMPUTE RESOURCES,0.8772532188841202,"Justification: All the experiments can be performed with a 16-core 2.40 GHz CPU and 32
843"
EXPERIMENTS COMPUTE RESOURCES,0.8781115879828326,"GB RAM.
844"
EXPERIMENTS COMPUTE RESOURCES,0.878969957081545,"Guidelines:
845"
EXPERIMENTS COMPUTE RESOURCES,0.8798283261802575,"• The answer NA means that the paper does not include experiments.
846"
EXPERIMENTS COMPUTE RESOURCES,0.8806866952789699,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
847"
EXPERIMENTS COMPUTE RESOURCES,0.8815450643776824,"or cloud provider, including relevant memory and storage.
848"
EXPERIMENTS COMPUTE RESOURCES,0.8824034334763948,"• The paper should provide the amount of compute required for each of the individual
849"
EXPERIMENTS COMPUTE RESOURCES,0.8832618025751073,"experimental runs as well as estimate the total compute.
850"
EXPERIMENTS COMPUTE RESOURCES,0.8841201716738197,"• The paper should disclose whether the full research project required more compute
851"
EXPERIMENTS COMPUTE RESOURCES,0.8849785407725322,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
852"
EXPERIMENTS COMPUTE RESOURCES,0.8858369098712446,"didn’t make it into the paper).
853"
CODE OF ETHICS,0.886695278969957,"9. Code Of Ethics
854"
CODE OF ETHICS,0.8875536480686695,"Question: Does the research conducted in the paper conform, in every respect, with the
855"
CODE OF ETHICS,0.8884120171673819,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
856"
CODE OF ETHICS,0.8892703862660944,"Answer: [Yes]
857"
CODE OF ETHICS,0.8901287553648068,"Justification: The research conducted in this paper conform with the NeurIPS code of ethics.
858"
CODE OF ETHICS,0.8909871244635194,"Guidelines:
859"
CODE OF ETHICS,0.8918454935622318,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
860"
CODE OF ETHICS,0.8927038626609443,"• If the authors answer No, they should explain the special circumstances that require a
861"
CODE OF ETHICS,0.8935622317596567,"deviation from the Code of Ethics.
862"
CODE OF ETHICS,0.894420600858369,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
863"
CODE OF ETHICS,0.8952789699570816,"eration due to laws or regulations in their jurisdiction).
864"
BROADER IMPACTS,0.896137339055794,"10. Broader Impacts
865"
BROADER IMPACTS,0.8969957081545065,"Question: Does the paper discuss both potential positive societal impacts and negative
866"
BROADER IMPACTS,0.8978540772532189,"societal impacts of the work performed?
867"
BROADER IMPACTS,0.8987124463519314,"Answer: [Yes]
868"
BROADER IMPACTS,0.8995708154506438,"Justification: This paper presents work whose goal is to advance the field of probabilistic
869"
BROADER IMPACTS,0.9004291845493563,"Machine Learning, particularly Bayesian Optimization. We discussed such impacts in
870"
BROADER IMPACTS,0.9012875536480687,"Section 6 Conclusion.
871"
BROADER IMPACTS,0.9021459227467811,"Guidelines:
872"
BROADER IMPACTS,0.9030042918454936,"• The answer NA means that there is no societal impact of the work performed.
873"
BROADER IMPACTS,0.903862660944206,"• If the authors answer NA or No, they should explain why their work has no societal
874"
BROADER IMPACTS,0.9047210300429185,"impact or why the paper does not address societal impact.
875"
BROADER IMPACTS,0.9055793991416309,"• Examples of negative societal impacts include potential malicious or unintended uses
876"
BROADER IMPACTS,0.9064377682403434,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
877"
BROADER IMPACTS,0.9072961373390558,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
878"
BROADER IMPACTS,0.9081545064377683,"groups), privacy considerations, and security considerations.
879"
BROADER IMPACTS,0.9090128755364807,"• The conference expects that many papers will be foundational research and not tied
880"
BROADER IMPACTS,0.9098712446351931,"to particular applications, let alone deployments. However, if there is a direct path to
881"
BROADER IMPACTS,0.9107296137339056,"any negative applications, the authors should point it out. For example, it is legitimate
882"
BROADER IMPACTS,0.911587982832618,"to point out that an improvement in the quality of generative models could be used to
883"
BROADER IMPACTS,0.9124463519313305,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
884"
BROADER IMPACTS,0.9133047210300429,"that a generic algorithm for optimizing neural networks could enable people to train
885"
BROADER IMPACTS,0.9141630901287554,"models that generate Deepfakes faster.
886"
BROADER IMPACTS,0.9150214592274678,"• The authors should consider possible harms that could arise when the technology is
887"
BROADER IMPACTS,0.9158798283261803,"being used as intended and functioning correctly, harms that could arise when the
888"
BROADER IMPACTS,0.9167381974248927,"technology is being used as intended but gives incorrect results, and harms following
889"
BROADER IMPACTS,0.9175965665236051,"from (intentional or unintentional) misuse of the technology.
890"
BROADER IMPACTS,0.9184549356223176,"• If there are negative societal impacts, the authors could also discuss possible mitigation
891"
BROADER IMPACTS,0.91931330472103,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
892"
BROADER IMPACTS,0.9201716738197425,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
893"
BROADER IMPACTS,0.9210300429184549,"feedback over time, improving the efficiency and accessibility of ML).
894"
SAFEGUARDS,0.9218884120171674,"11. Safeguards
895"
SAFEGUARDS,0.9227467811158798,"Question: Does the paper describe safeguards that have been put in place for responsible
896"
SAFEGUARDS,0.9236051502145923,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
897"
SAFEGUARDS,0.9244635193133047,"image generators, or scraped datasets)?
898"
SAFEGUARDS,0.9253218884120171,"Answer: [Yes]
899"
SAFEGUARDS,0.9261802575107296,"Justification: This work has the potential of misuse for machine learning algorithms. How-
900"
SAFEGUARDS,0.927038626609442,"ever the current model has certain limitations on applying for high-dimensional problems,
901"
SAFEGUARDS,0.9278969957081545,"thus such risks are low. We mentioned such risks in the last paragraph in Section 6 Conclu-
902"
SAFEGUARDS,0.9287553648068669,"sion.
903"
SAFEGUARDS,0.9296137339055794,"Guidelines:
904"
SAFEGUARDS,0.9304721030042918,"• The answer NA means that the paper poses no such risks.
905"
SAFEGUARDS,0.9313304721030042,"• Released models that have a high risk for misuse or dual-use should be released with
906"
SAFEGUARDS,0.9321888412017167,"necessary safeguards to allow for controlled use of the model, for example by requiring
907"
SAFEGUARDS,0.9330472103004291,"that users adhere to usage guidelines or restrictions to access the model or implementing
908"
SAFEGUARDS,0.9339055793991416,"safety filters.
909"
SAFEGUARDS,0.934763948497854,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
910"
SAFEGUARDS,0.9356223175965666,"should describe how they avoided releasing unsafe images.
911"
SAFEGUARDS,0.936480686695279,"• We recognize that providing effective safeguards is challenging, and many papers do
912"
SAFEGUARDS,0.9373390557939915,"not require this, but we encourage authors to take this into account and make a best
913"
SAFEGUARDS,0.9381974248927039,"faith effort.
914"
LICENSES FOR EXISTING ASSETS,0.9390557939914163,"12. Licenses for existing assets
915"
LICENSES FOR EXISTING ASSETS,0.9399141630901288,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
916"
LICENSES FOR EXISTING ASSETS,0.9407725321888412,"the paper, properly credited and are the license and terms of use explicitly mentioned and
917"
LICENSES FOR EXISTING ASSETS,0.9416309012875537,"properly respected?
918"
LICENSES FOR EXISTING ASSETS,0.9424892703862661,"Answer: [Yes]
919"
LICENSES FOR EXISTING ASSETS,0.9433476394849786,"Justification: We include the URLs for the datasets we used in this work.
920"
LICENSES FOR EXISTING ASSETS,0.944206008583691,"Guidelines:
921"
LICENSES FOR EXISTING ASSETS,0.9450643776824035,"• The answer NA means that the paper does not use existing assets.
922"
LICENSES FOR EXISTING ASSETS,0.9459227467811159,"• The authors should cite the original paper that produced the code package or dataset.
923"
LICENSES FOR EXISTING ASSETS,0.9467811158798283,"• The authors should state which version of the asset is used and, if possible, include a
924"
LICENSES FOR EXISTING ASSETS,0.9476394849785408,"URL.
925"
LICENSES FOR EXISTING ASSETS,0.9484978540772532,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
926"
LICENSES FOR EXISTING ASSETS,0.9493562231759657,"• For scraped data from a particular source (e.g., website), the copyright and terms of
927"
LICENSES FOR EXISTING ASSETS,0.9502145922746781,"service of that source should be provided.
928"
LICENSES FOR EXISTING ASSETS,0.9510729613733906,"• If assets are released, the license, copyright information, and terms of use in the
929"
LICENSES FOR EXISTING ASSETS,0.951931330472103,"package should be provided. For popular datasets, paperswithcode.com/datasets
930"
LICENSES FOR EXISTING ASSETS,0.9527896995708155,"has curated licenses for some datasets. Their licensing guide can help determine the
931"
LICENSES FOR EXISTING ASSETS,0.9536480686695279,"license of a dataset.
932"
LICENSES FOR EXISTING ASSETS,0.9545064377682403,"• For existing datasets that are re-packaged, both the original license and the license of
933"
LICENSES FOR EXISTING ASSETS,0.9553648068669528,"the derived asset (if it has changed) should be provided.
934"
LICENSES FOR EXISTING ASSETS,0.9562231759656652,"• If this information is not available online, the authors are encouraged to reach out to
935"
LICENSES FOR EXISTING ASSETS,0.9570815450643777,"the asset’s creators.
936"
NEW ASSETS,0.9579399141630901,"13. New Assets
937"
NEW ASSETS,0.9587982832618026,"Question: Are new assets introduced in the paper well documented and is the documentation
938"
NEW ASSETS,0.959656652360515,"provided alongside the assets?
939"
NEW ASSETS,0.9605150214592275,"Answer: [Yes]
940"
NEW ASSETS,0.9613733905579399,"Justification: We submit partial of the code in supplementary material and select a license
941"
NEW ASSETS,0.9622317596566523,"when submitting the paper.
942"
NEW ASSETS,0.9630901287553648,"Guidelines:
943"
NEW ASSETS,0.9639484978540772,"• The answer NA means that the paper does not release new assets.
944"
NEW ASSETS,0.9648068669527897,"• Researchers should communicate the details of the dataset/code/model as part of their
945"
NEW ASSETS,0.9656652360515021,"submissions via structured templates. This includes details about training, license,
946"
NEW ASSETS,0.9665236051502146,"limitations, etc.
947"
NEW ASSETS,0.967381974248927,"• The paper should discuss whether and how consent was obtained from people whose
948"
NEW ASSETS,0.9682403433476395,"asset is used.
949"
NEW ASSETS,0.9690987124463519,"• At submission time, remember to anonymize your assets (if applicable). You can either
950"
NEW ASSETS,0.9699570815450643,"create an anonymized URL or include an anonymized zip file.
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708154506437768,"14. Crowdsourcing and Research with Human Subjects
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9716738197424892,"Question: For crowdsourcing experiments and research with human subjects, does the paper
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9725321888412017,"include the full text of instructions given to participants and screenshots, if applicable, as
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9733905579399141,"well as details about compensation (if any)?
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742489270386266,"Answer: [NA]
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975107296137339,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9759656652360515,"Guidelines:
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.976824034334764,"• The answer NA means that the paper does not involve crowdsourcing nor research with
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776824034334763,"human subjects.
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785407725321889,"• Including this information in the supplemental material is fine, but if the main contribu-
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793991416309012,"tion of the paper involves human subjects, then as much detail as possible should be
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802575107296138,"included in the main paper.
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811158798283262,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819742489270387,"or other labor should be paid at least the minimum wage in the country of the data
965"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828326180257511,"collector.
966"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836909871244636,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
967"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984549356223176,"Subjects
968"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854077253218884,"Question: Does the paper describe potential risks incurred by study participants, whether
969"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862660944206009,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871244635193133,"approvals (or an equivalent approval/review based on the requirements of your country or
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9879828326180258,"institution) were obtained?
972"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888412017167382,"Answer: [NA]
973"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896995708154507,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
974"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905579399141631,"Guidelines:
975"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914163090128756,"• The answer NA means that the paper does not involve crowdsourcing nor research with
976"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992274678111588,"human subjects.
977"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931330472103004,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
978"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9939914163090129,"may be required for any human subjects research. If you obtained IRB approval, you
979"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948497854077253,"should clearly state this in the paper.
980"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957081545064378,"• We recognize that the procedures for this may vary significantly between institutions
981"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965665236051502,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
982"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974248927038627,"guidelines for their institution.
983"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982832618025751,"• For initial submissions, do not include any information that would break anonymity (if
984"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991416309012876,"applicable), such as the institution conducting the review.
985"
