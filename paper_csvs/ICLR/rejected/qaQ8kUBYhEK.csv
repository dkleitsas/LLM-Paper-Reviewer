Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0005530973451327434,"In this paper, we study the generalization risk of ridge and ridgeless linear regres-
sion. We assume that the data features follow a multivariate normal distribution
and that the spectrum of the covariance matrix consists of a given set of eigenval-
ues of proportionally growing multiplicity. We characterize the limiting bias and
variance when the dimension and the number of training samples tend to inﬁnity
proportionally. Exact formulae for the bias and variance are derived using the ran-
dom matrix theory and convex Gaussian min-max theorem. Based on these formu-
lae, we study the sample-wise multiple descent phenomenon of the generalization
risk curve, i.e., with more data, the generalization risk can be non-monotone, and
speciﬁcally, can increase and then decrease multiple times with more training data
samples. We prove that sample-wise multiple descent occurs when the spectrum
of the covariance matrix is highly ill-conditioned. We also present numerical re-
sults to conﬁrm the values of the bias and variance predicted by our theory and
illustrate the multiple descent of the generalization risk curve. Moreover, we the-
oretically show that the ridge estimator with optimal regularization can result in a
monotone generalization risk curve and thereby eliminate multiple descent under
some assumptions."
INTRODUCTION,0.0011061946902654867,"1
INTRODUCTION"
INTRODUCTION,0.00165929203539823,"The double/multiple descent phenomenon attracted recent research attention due to (Belkin et al.,
2019). This line of work focuses on the parameter-wise double/multiple descent phenomenon of
the risk curve (Bartlett et al., 2020; Tsigler & Bartlett, 2020; Belkin et al., 2019; 2020; Chen et al.,
2020a; Liang et al., 2020; Advani et al., 2020; B¨os & Opper, 1998; Krogh & Hertz, 1992; Le Cun
et al., 1991; Mei & Montanari, 2019; Opper et al., 1990; Vallet et al., 1989; Watkin et al., 1993). The
classical learning theory shows that when the number of parameters (which reﬂects the model com-
plexity) increases, the test error (generalization risk) ﬁrst decreases due to more ﬁtting power, and
then increases due to overﬁtting. The generalization risk attains a peak at the interpolation threshold
(the number of parameters equals the number of data points so that the model interpolates the data).
This results in a U-shaped risk curve if we plot the test error versus the number of parameters. The
double descent risk curve posits that the risk will decrease (again) if one further increases the model
complexity beyond the interpolation threshold (Belkin et al., 2019). Thus there is a second descent
in addition to the ﬁrst one in the U-shaped stage of the curve. Belkin et al. (2019) presented empiri-
cal results and showed the existence of such double descent behavior in the random Fourier features
model, the fully connected neural network, and the random forest model. Prior to (Belkin et al.,
2019), earlier studies of the shape and features of the risk curve in a number of contexts include
(Vallet et al., 1989; Opper et al., 1990; Le Cun et al., 1991; Krogh & Hertz, 1992; B¨os & Opper,
1998; Watkin et al., 1993; Advani et al., 2020). Loog et al. (2020) presented a prehistory of the
double descent phenomenon. Belkin et al. (2020) proved the double descent curve in the Gaussian
model and the Fourier series model. Mei & Montanari (2019) theoretically established the double
descent curve of the random features regression. Bartlett et al. (2020); Tsigler & Bartlett (2020)
characterized the conditions for ridgeless and ridge linear regression problems, respectively, under
which the minimum-norm interpolants achieve near-optimal generalization risk. Liang et al. (2020)
showed that the test error of the minimum-norm interpolator of data in reproducing kernel Hilbert
space is upper bounded by a multiple descent curve as the model complexity increases. They also
presented a numerical result supporting that the test error itself exhibits a multiple descent curve."
INTRODUCTION,0.0022123893805309734,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0027654867256637168,"Chen et al. (2020a) proved that the multiple descent curve does exist for the minimum-norm inter-
polator in linear regression and that the curve can be even designed."
INTRODUCTION,0.00331858407079646,"Following the parameter-wise double descent, research interest extended to epoch-wise and sample-
wise double descent (Nakkiran et al., 2020; Chen et al., 2020b; Min et al., 2021; Nakkiran et al.,
2021). Nakkiran et al. (2020) observed from their numerical result that the generalization risk ex-
periences a double descent as one keeps the model size ﬁxed and increases the training time. They
called this observation epoch-wise double descent. Nakkiran et al. (2020) also noted sample-wise
non-monotonicity, which means that more data can hurt generalization. Nakkiran et al. (2021)
proved that for isotropic features, optimally regularized ridge regression yields a monotonic gener-
alization risk curve with more samples. Nakkiran et al. (2021) also showed that if the features are
formed by projecting high-dimensional isotropic data to a random low-dimensional space (say, d-
dimensional), the optimally regularized ridge regression has a monotonic generalization risk curve
with increasing d (the model size). Sample-wise non-monotonicity and double descent was also
observed in (Chen et al., 2020b; Min et al., 2021) in adversarially trained models. C ompared to
(Wu & Xu, 2020; ichi Amari et al., 2021; Dobriban & Wager, 2018; Richards et al., 2021), in what
follows, we highlight our contributions and the differences from them. First, our major contribu-
tion is providing a rigorous proof for the existence of sample-wise (test error vs. the number of
training samples) double and multiple descent in linear regression. However, (Richards et al., 2021)
only mentioned parameter-wise double descent (test error vs. model capacity) in their related work.
(ichi Amari et al., 2021) only mentioned epoch-wise (test error vs. training time) double descent
in Appendix A.2. Neither (Richards et al., 2021) nor (ichi Amari et al., 2021) mentioned multiple
descent. Second, we made and theoretically proved the observation that an ill-conditioned covari-
ance matrix is a sufﬁcient condition for the existence of sample-wise multiple descent. To the best
of our knowledge, our work is the ﬁrst paper that pointed this out. Third, we solved the Stieltjes
transform explicitly and derived explicit formulae for the risk and variance in our setup. In addition,
we also provided rigorous treatment to the ridgeless setting and also obtained explicit formulae for
it. Fourth, there is another difference between our paper and the papers that the reviewer mentioned.
(Wu & Xu, 2020; ichi Amari et al., 2021; Dobriban & Wager, 2018; Richards et al., 2021) assumed
a prior on the true linear model and takes expectation over the prior. In our paper, we do not assume
a prior on the true linear model and our risk does not take the expectation over a random true linear
model."
INTRODUCTION,0.0038716814159292035,"In the setting of generally anisotropic features, this paper gives an asymptotic characterization of the
generalization risk curve with more samples. The asymptotic regime is an approximation for large
n, d and can also shed light on practical machine learning problems. We ﬁrst introduce our problem
setup."
PROBLEM SETUP,0.004424778761061947,"1.1
PROBLEM SETUP"
PROBLEM SETUP,0.00497787610619469,"Data Distribution
Let Σ ∈Rd×d be a positive semi-deﬁnite matrix which is termed the covari-
ance matrix, and let θ∗∈Rd. The eigenvalues of Σ are λ1, . . . , λm with multiplicity d1, . . . , dm,
respectively. We have d = Pm
i=1 di. Assume that λ1, . . . , λm are ﬁxed, distinct, all positive, and
do not depend on d (i.e., for all d, the eigenvalue of Σ are always λ1, . . . , λm). We assume the
following data distribution D for (x, y) ∈Rd × R:"
PROBLEM SETUP,0.0055309734513274336,"x ∼N(0, Σ) ,
y = x⊤θ∗+ ϵ ,"
PROBLEM SETUP,0.006084070796460177,"where x and ϵ are independent and ϵ ∼N(0, σ2). In practice, there are natural random variables
x that satisfy our assumption. For example, assume that we want to use machine A to measure the
length of several objects and use machine B to measure their temperature. The measured lengths
and temperatures follow an i.i.d. Gaussian distribution. However, the variance of measurement of
machine A is different from that of machine B. Then we consider the random vector formed by the
measurements x = (l1, . . . , ln, t1, . . . , tn), where li and ti are the length and temperature of object i,
respectively. This results in a block-structured covariance matrix. When we measure more objects,
the size of the covariance matrix tends to inﬁnity. Second, the motivation came from (Nakkiran
et al., 2021). (Nakkiran et al., 2021) observed empirically in their Figure 2 that when the covariance
matrix has a block structure (speciﬁcally, there are only two ﬁxed different eigenvalues 10 and 1),
the expected excess risk exhibits multiple descent. We quantitatively studied this observation and"
PROBLEM SETUP,0.00663716814159292,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.007190265486725664,obtained the related formulae. The excess risk of an estimator θ ∈Rd is given by
PROBLEM SETUP,0.007743362831858407,"R(θ) = Ex,y∼D
h 
y −x⊤θ
2 −
 
y −x⊤θ∗2i
."
PROBLEM SETUP,0.008296460176991151,"Assume that the training data {(xi, yi)}n
i=1 ⊆Rd × R is drawn i.i.d. from D. Write X =  
"
PROBLEM SETUP,0.008849557522123894,"x⊤
1...
x⊤
n "
PROBLEM SETUP,0.009402654867256638,"
∈Rn×d ,
y =  
"
PROBLEM SETUP,0.00995575221238938,"y1
...
yn "
PROBLEM SETUP,0.010508849557522125,"
∈Rn .
(1)"
PROBLEM SETUP,0.011061946902654867,"We have y = Xθ∗+ ϵ, where ϵ ∼N(0, σ2In)."
PROBLEM SETUP,0.011615044247787611,Ridge Estimator and Minimum-Norm Estimator
PROBLEM SETUP,0.012168141592920354,"Deﬁnition 1 (Ridge estimator). The ridge estimator ˆθλ,n,d ∈Rd (λ > 0) solves the following
minimization problem"
PROBLEM SETUP,0.012721238938053098,"min
θ∈Rd
1
n ∥Xθ −y∥2
2 + λ ∥θ∥2
2 ."
PROBLEM SETUP,0.01327433628318584,"Deﬁnition 2 (Minimum-norm estimator). The minimum-norm estimator (also known as the ridge-
less estimator) ˆθ0,n,d ∈Rd solves the following minimization problem"
PROBLEM SETUP,0.013827433628318585,"min
θ∈Rd ∥θ∥2
such that
∥Xθ −y∥2 = min
θ∈Rd ∥Xθ −y∥2 ."
PROBLEM SETUP,0.014380530973451327,"We are interested in the expected excess risk of ˆθλ,n,d, which is given by"
PROBLEM SETUP,0.014933628318584071,"Rλ,n,d = E
h
R

ˆθλ,n,d
i
."
PROBLEM SETUP,0.015486725663716814,"The expectation is taken over the randomness of the training data {(xi, yi)}n
i=1."
PROBLEM SETUP,0.016039823008849558,"Asymptotic Regime
Let Πi ∈Rd×d be the orthogonal projection to the eigenspace of λi. This
paper focuses on the asymptotic behavior of the expected excess risk of ˆθλ,n,d where n, di →+∞,
di/n →zi (zi is a ﬁxed positive constant), and ∥Πiθ∗∥2 →ηi. In other words, we are interested in"
PROBLEM SETUP,0.016592920353982302,"lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
PROBLEM SETUP,0.017146017699115043,"Rλ,n,d ."
OUR CONTRIBUTIONS,0.017699115044247787,"1.2
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.01825221238938053,Our contributions are summarized as follows.
OUR CONTRIBUTIONS,0.018805309734513276,"1. We obtain the formulae for the limiting bias and variance, and thereby the limiting risk.
We use two methods to obtain these formulae. Speciﬁcally, we obtain the limiting bias and
variance by solving the Stieltjes transform and computing its derivatives and antideriva-
tives. We also use convex Gaussian min-max theorem (CGMT) (Thrampoulidis et al.,
2015) to compute the limiting variance. The advantage of the CGMT method is that it is
more mathematically tractable for the ridgeless estimator. Through the CGMT approach,
we obtain a closed-form formula for the variance in the underparameterized regime and
simplify the formula for the variance in the overparameterized regime. Moreover, based on
the simpliﬁed formula, we deduce a closed-form expression for the variance if the covari-
ance matrix of the data distribution has two different eigenvalues.
2. We ﬁnd and theoretically prove that sample-wise multiple descent happens when the co-
variance matrix has eigenvalues of very different orders of magnitude (thus the covariance
matrix is highly ill-conditioned)."
OUR CONTRIBUTIONS,0.019358407079646017,"3. We show that if the true linear model θ∗satisﬁes ∥Πiθ∗∥2 =
q di"
OUR CONTRIBUTIONS,0.01991150442477876,"d , optimal regulariza-"
OUR CONTRIBUTIONS,0.020464601769911505,"tion (i.e., pick λ that minimizes the generalization risk of ˆθλ,n,d) results in a monotone
generalization risk curve—in other words, with optimal regularization, more data samples"
OUR CONTRIBUTIONS,0.02101769911504425,Under review as a conference paper at ICLR 2022
OUR CONTRIBUTIONS,0.02157079646017699,"always improve generalization. Thus there is no sample-wise double or multiple descent.
This provides a theoretical proof of a phenomenon observed in (Nakkiran et al., 2021) that
optimal regularization can mitigate double descent for anisotropic data. Note that without
regularization, there will be a blow-up in expected excess risk when n = d (the linear
model exactly interpolates the data) and therefore, there is no samplewise descent across
the under- and over-parameterized regimes."
PRELIMINARIES,0.022123893805309734,"2
PRELIMINARIES"
PRELIMINARIES,0.02267699115044248,"Notation
Write [m] for {1, 2, . . . , m}. Let i denote the imaginary unit. If x ∈Rn and Σ ∈Rn×n"
PRELIMINARIES,0.023230088495575223,"is a positive semideﬁnite matrix, write ∥x∥Σ ≜
√"
PRELIMINARIES,0.023783185840707963,"x⊤Σx. For a vector x, let ∥·∥1 and ∥·∥2 denote
the ℓ1 and ℓ2 norm, respectively. Let ⊙denote the Hadamard (entry-wise) product between vectors.
Write ∥· ∥2 and ∥· ∥F for the spectral matrix norm and Frobenius matrix norm, respectively. Let
≼denotes the Loewner order. For two square matrices A and B of the same size, write A ≼B
if B −A is positive semideﬁnite. Deﬁne spec (A) as the set of all eigenvalues of A. Let O(d) =

A ∈Rd×d | AA⊤= A⊤A = Id
	
denote the set of d × d orthogonal matrices. Deﬁne Sd−1(r) ≜
{x ∈Rd | ∥x∥2 = r}. Denote almost sure convergence by
a.s.
→, and convergence in probability plim
and
P→."
PRELIMINARIES,0.024336283185840708,"Ridge Estimator and Minimum-Norm Estimator
We begin with the equivalent characteriza-
tions of the ridge and minimum-norm estimator. An equivalent characterization of the ridge estima-
tor ˆθλ,n,d is
ˆθλ,n,d =
 
X⊤X + λnId
−1 X⊤y = X⊤ 
λnIn + XX⊤−1 y .
(2)
The second equality in Equation (2) is because of the Sherman–Morrison–Woodbury formula. A
proof of Equation (2) can be found in (Tsigler & Bartlett, 2020)."
PRELIMINARIES,0.024889380530973452,"An equivalent deﬁnition of the minimum-norm estimator ˆθ0,n,d is that ˆθ0,n,d solves the following
minimization problem
min
θ∈Rd ∥θ∥2
such that
X⊤Xθ = X⊤y ."
PRELIMINARIES,0.025442477876106196,"Thus we have
ˆθ0,n,d =
 
X⊤X
+ X⊤y = X⊤ 
XX⊤+ y = X+y ,
where A+ denotes the pseudo-inverse of A. The second and third equalities are because of the
identity X+ =
 
X⊤X
+ X⊤= X⊤ 
XX⊤+. The minimum-norm estimator is the limit of the
ridge estimator ˆθλ,n,d as λ →0+:
ˆθ0,n,d = lim
λ→0+ ˆθλ,n,d ."
PRELIMINARIES,0.025995575221238937,"This
is
because
of
the
identity
limλ→0+  
X⊤X + λnId
−1 X⊤
="
PRELIMINARIES,0.02654867256637168,"limλ→0+ X⊤ 
λnIn + XX⊤−1 = X+."
PRELIMINARIES,0.027101769911504425,"Bias-Variance Decomposition of Expected Excess Risk
We ﬁrst show that the excess risk of an
estimator θ equals the norm of θ −θ∗:"
PRELIMINARIES,0.02765486725663717,"R(θ) = E(x,y)∼D
h 
y −x⊤θ
2 −
 
y −x⊤θ∗i
= Ex
h 
x⊤(θ∗−θ)
2i"
PRELIMINARIES,0.02820796460176991,"= E
h
(θ∗−θ)⊤Σ (θ∗−θ)
i
= E
h
∥θ∗−θ∥2
Σ
i
."
PRELIMINARIES,0.028761061946902654,"For the ridge estimator, the expected excess risk is"
PRELIMINARIES,0.0293141592920354,"Rλ,d,n =E

∥θ∗−X⊤(nλIn + XX⊤)−1(Xθ∗+ ϵ)∥2
Σ
"
PRELIMINARIES,0.029867256637168143,"=E

∥(Id −X⊤(nλIn + XX⊤)−1X)θ∗−X⊤(nλIn + XX⊤)−1ϵ∥2
Σ
"
PRELIMINARIES,0.030420353982300884,"=E

∥(Id −X⊤(nλIn + XX⊤)−1X)θ∗∥2
Σ

+ E
hX⊤(nλIn + XX⊤)−1ϵ
2 Σ i"
PRELIMINARIES,0.030973451327433628,"=E

∥(Id −X⊤(nλIn + XX⊤)−1X)θ∗∥2
Σ

+ σ2E tr

XΣX⊤(nλIn + XX⊤)−2"
PRELIMINARIES,0.03152654867256637,"≜Bλ,d,n + Vλ,d,n .
(3)"
PRELIMINARIES,0.032079646017699116,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.03263274336283186,"For the minimum-norm estimator, the expected excess risk is
R0,d,n = E

∥θ∗−X+(Xθ∗+ ϵ)∥2
Σ
"
PRELIMINARIES,0.033185840707964605,"= E
h 
Id −X+X

θ∗−X+ϵ
2
Σ i"
PRELIMINARIES,0.03373893805309734,"= E
h 
Id −X+X

θ∗2
Σ"
PRELIMINARIES,0.034292035398230086,"i
+ E
hX+ϵ
2
Σ i"
PRELIMINARIES,0.03484513274336283,"= E
h 
Id −X+X

θ∗2
Σ"
PRELIMINARIES,0.035398230088495575,"i
+ σ2E tr
h 
X+⊤ΣX+i"
PRELIMINARIES,0.03595132743362832,"≜B0,d,n + V0,d,n .
(4)
We call Bλ,d,n and B0,d,n the bias term, and call Vλ,d,n and V0,d,n the variance term. The bias and
variance for the minimum-norm estimator are the limit of their counterpart for the ridge estimator
as λ →0+, i.e., limλ→0+ Bλ,d,n = B0,d,n and limλ→0+ Vλ,d,n = V0,d,n (this can be shown by
Lebesgue’s dominated convergence theorem, see our proof in Lemma 5 and Lemma 6, respectively)."
MAIN RESULTS,0.03650442477876106,"3
MAIN RESULTS"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.03705752212389381,"3.1
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.03761061946902655,"We study the limiting bias and variance for a linear regression problem in which the data distribution
follows a multivariate normal distribution, the spectrum of the covariance matrix exhibits a block
structure and tends to a discrete distribution. Thanks to the random matrix theory, we obtain the
formulae (presented in Theorem 1) for the limiting bias and variance, and thereby the total risk."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.03816371681415929,"We use two methods to obtain these formulae. The ﬁrst method is through the Stieltjes transform
of the matrix 1"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.03871681415929203,"nXX⊤. The central quantity for computing the limiting bias and variance through
the ﬁrst method is the solution ρ∗to the optimization problem Equation (5) in Item 1 of Theorem 1.
Item 1 guarantees the existence of a solution and determines its optimality condition Equation (6).
Item 2 computes the Jacobian matrix of ρ∗with respect to λi and provides a closed-form formula
to compute the Jacobian matrix. Equation (9) and Equation (10) in Item 4 give the formulae for
the limiting bias obtained by the ﬁrst method. Equation (11) and Equation (12) give the limiting
variance."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.03926991150442478,"The second method is through the convex Gaussian min-max theorem (CGMT) (Thrampoulidis
et al., 2015). The central quantity is the solution r∗to the minimax optimization problem Equa-
tion (8) in Item 3. We use CGMT to obtain the formulae for the variance term. They are presented
in Equation (13) and Equation (14) in Item 4.
Theorem 1. The following statements hold:"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.03982300884955752,"1. There exists a minimizer ρ ∈Rm
+ that solves"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.040376106194690266,"inf
ρ∈Rm
+  log  λ + m
X"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.04092920353982301,"j=1
λjρj  + m
X j=1"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.041482300884955754,"
ρj −zj(log ρj"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.0420353982300885,"zj
+ 1)
"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.042588495575221236,".
(5)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.04314159292035398,"The minimizer ρ∗satisﬁes
λi
λ + Pm
j=1 λjρ∗
j
+ 1 −zi"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.043694690265486724,"ρi
= 0 ,
∀i ∈[m] .
(6)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.04424778761061947,"2. Let ρ∗∈Rm be a minimizer of Equation (5) and J =
∂ρ∗"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.04480088495575221,"∂λ ∈Rm×m be the Jacobian
matrix Jij = ∂ρ∗
i
∂λj . Then J is given by"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.04535398230088496,"J =
 
diag (λ) +
 
λ + λ⊤ρ∗
Im −(z −ρ∗) λ⊤−1  
(z −ρ∗) ρ∗⊤−diag (ρ∗)
"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.0459070796460177,"and the matrix
 
diag (λ) +
 
λ + λ⊤ρ∗
Im −(z −ρ∗) λ⊤
is always invertible."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.046460176991150445,"3. Deﬁne r = (r1, . . . , rm), λ = (λ1, . . . , λm), and"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.04701327433628318,"ϑ(rt, r, λ) = 2rt
s 1 +
X"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.04756637168141593,"i∈[m]
r2
i −2rt
X i∈[m]"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.04811946902654867,"√ziri +
X i∈[m]"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.048672566371681415,"1
λi
r2
i −λr2
t .
(7)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.04922566371681416,Under review as a conference paper at ICLR 2022
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.049778761061946904,For any Kt ≥2
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.05033185840707965,"λ and Ku ≥
2λ+(2+√γ)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.05088495575221239,"λ
, we have"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.05143805309734513,"max
0≤rt≤Kt
min
0≤ri≤Ku ϑ(rt, r, λ) =
min
0≤ri≤Ku
max
0≤rt≤Kt ϑ(rt, r, λ) = max
rt≥0 min
ri≥0 ϑ(rt, r, λ) = min
ri≥0 max
rt≥0 ϑ(rt, r, λ)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.051991150442477874,"(8)
and the above optimization problem has a solution."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.05254424778761062,"4. Let r∗= (r∗
1, . . . , r∗
m) solve Equation (8). Deﬁne q =
 
η2
1/z1, . . . , η2
m/zm
⊤and view
λ = (λ1, . . . , λm)⊤as a column vector. The limiting bias is given by"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.05309734513274336,"lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.053650442477876106,"Bλ,d,n = q⊤(λ ⊙ρ∗+ Jλ⊙2) ,
(9)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.05420353982300885,"lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.054756637168141595,"B0,d,n = lim
λ→0+ q⊤(λ ⊙ρ∗+ Jλ⊙2) .
(10)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.05530973451327434,The limiting variance is given by
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.055862831858407076,"lim
n,di→+∞
di/n→zi"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.05641592920353982,"Vλ,d,n = σ2 λ⊙2⊤(ρ∗+ J⊤λ)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.056969026548672565,"(λ + λ⊤ρ∗)2
,
(11)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.05752212389380531,"lim
n,di→+∞
di/n→zi"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.05807522123893805,"V0,d,n = σ2 lim
λ→0+
λ⊙2⊤(ρ∗+ J⊤λ)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.0586283185840708,"(λ + λ⊤ρ∗)2
,
(12)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.05918141592920354,"lim
n,di→+∞
di/n→zi"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.059734513274336286,"Vλ,d,n = σ2
m
X"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06028761061946902,"i=1
r∗2
i ,
(13)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06084070796460177,"lim
n,di→+∞
di/n→zi"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06139380530973451,"V0,d,n = σ2 lim
λ→0+ m
X"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.061946902654867256,"i=1
r∗2
i .
(14)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.0625,"Figure 1 illustrates the theoretical and numerical values of the bias, variance, and total risk. We
observe a triple descent in Figure 1a where the covariance matrix has three blocks, and a quadruple
descent in Figure 1b where the covariance has four blocks. In the three-block example, we set λ3 ≫
λ2 ≫λ1 (λ1 = 1, λ2 = 100, λ3 = 1000). In the four-block example, we set λ4 ≫λ3 ≫λ2 ≫λ1
(λ1 = 1, λ2 = 100, λ3 = 104, λ4 = 107). For the values of other parameters, please refer to the
caption of Figure 1 Our ﬁndings provide an explanation for the occurrence of sample-wise multiple
descent: it occurs when the covariance matrix is highly ill-conditioned. Moreover, we ﬁnd that the
generalization risk curve is continuous in ridge regression (λ > 0) while it blows up at n = d
in ridgeless regression (λ = 0). We can see the singularity (at n = d = 200) of the ridgeless
generalization risk curve in Figure 2a."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06305309734513274,"Following Theorem 1, we focus on the variance in the ridgeless case (λ = 0) and further study
the expressions in Equation (13) and Equation (14). We ﬁnd that the variance exhibits sharply
different behaviors in the underparameterized and overparameterized regimes. Recall that we will
let n, di →+∞and keep di/n →zi. Then d/n →P
i∈[m] zi. If lim d/n = P
i∈[m] zi > 1, we
are in the underparameterized regime. In this regime, the bias vanishes and therefore the risk equals
the variance. If lim d/n < 1, we are in the overparameterized regime."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06360619469026549,Theorem 2. If d/n →P
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06415929203539823,"i∈[m] zi > 1 and r∗= (r∗
1, . . . , r∗
m) solves"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06471238938053098,"min
ri≥0 X i∈[m]"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06526548672566372,"1
λi
r2
i
subject to
s X"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06581858407079647,"i∈[m]
r2
i + 1 =
X i∈[m]"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06637168141592921,"√ziri ,"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06692477876106195,then we have an optimality condition for r∗:
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06747787610619468,"r∗
i
r∗
j
= λi"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06803097345132743,"λj
·
√ziA∗−r∗
i
√zjA∗−r∗
j
,
i, j ∈[m] ,
(15)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06858407079646017,Under review as a conference paper at ICLR 2022
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06913716814159292,"0
50
100
150
200
250
300
Number of Samples 0 50 100 150 200"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.06969026548672566,Expected Risk
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.0702433628318584,"T. Bias (  = 0.01)
T. Var1 (  = 0.01)
T. Var2 (  = 0.01)
T. Risk (  = 0.01)
N. Bias (  = 0.01)
N. Var (  = 0.01)
N. Risk (  = 0.01)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07079646017699115,(a) Sample-wise Triple Descent
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.0713495575221239,"0
50
100
150
200
250
300
Number of Samples 0 200 400 600 800 1000"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07190265486725664,Expected Risk
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07245575221238938,"T. Bias (  = 0.01)
T. Var (  = 0.01)
T. Risk (  = 0.01)
N. Bias (  = 0.01)
N. Var (  = 0.01)
N. Risk (  = 0.01)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07300884955752213,"(b) Sample-wise quadruple de-
scent"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07356194690265487,"Figure 1: Figure 1a and Figure 1b illustrate sample-wise triple and quadruple descent, respectively.
We specify the parameters that we used as follows. Figure 1a: There are 3 blocks. We set d1 = 60 ,
d2 = d3 = 40, λ1 = 1, λ2 = 100, λ3 = 1000, ∥Π1θ∗∥2 = ∥Π3θ∗∥2 = 0.1 and ∥Π2θ∗∥2 = 1. The
three descents occur at n = 36, 80, 136, respectively. Figure 1b: There are 4 blocks. We set d1 =
d2 = d3 = d4 = 40, λ1 = 1, λ2 = 100, λ3 = 104, λ4 = 107, and ∥Πiθ∗∥2 = 0.01(i ∈[4]). The
four descents occur at around n = 1, 37, 80, 120, 150, respectively. In the legend, the items starting
with “T.” are theoretical values predicted by Theorem 1. Items starting with “N.” are numerical
values. We plot two curves for the variance in Figure 1a. “T. Var1” is obtained by Equation (11) of
Theorem 1. “T. Var2” is obtained by Equation (13)."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07411504424778761,"where A∗=
qP"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07466814159292036,"i∈[m] r∗2
i
+ 1. Moreover, we have limn,di→+∞
di/n→zi
V0,d,n = σ2 limλ→0+ Pm
i=1 r∗2
i ."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.0752212389380531,If d/n →P
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07577433628318585,"i∈[m] zi < 1, then we have"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07632743362831858,"lim
n,di→+∞
di/n→zi"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07688053097345132,"V0,d,n = σ2
P"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07743362831858407,"i∈[m] zi
1 −P"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07798672566371681,"i∈[m] zi
. ."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07853982300884955,"Corollary 1. If m = 1 and d/n →z1 > 1, we have limn,di→+∞
di/n→zi
V0,d,n = σ2
1
z1−1."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.0790929203539823,"Proof. In the case m = 1, we have r∗
1 solves minr1≥0
1
λ1 r2
1 subject to
p"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.07964601769911504,"r2
1 + 1 = √z1r1. The
equality constraint gives r∗2
1 =
1
z1−1. Then by Theorem 2, the limiting variance is σ2r∗2
1 = σ2
1
z1−1."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08019911504424779,"In Theorem 2, we ﬁnd that in the underparameterized regime, r∗solves an equality-constrained
minimization problem. In the proof of Theorem 2, we see that the equality constraint is feasible
in the underparameterized regime but infeasible in the overparameterized regime. Moreover, we
present an optimality condition for r∗, which will be used in Theorem 3 to study the two-block
(m = 2) case. If the data distribution is isotropic (which means that the covariance matrix is a scalar
matrix), Collorary 1 shows that the limiting variance is σ2
1
z1−1, which agrees with (Hastie et al.,
2019, Theorem 1)."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08075221238938053,"In the overparameterized regime, however, we ﬁnd that the limiting variance does not depend on the
spectrum {λ1, . . . , λm}of the covariance matrix and only depends on the noise intensity σ and the
ratios zi = lim di/n. This agrees with (Hastie et al., 2019, Proposition 2)."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08130530973451328,"In Theorem 3, we study the case m = 2 and present a concrete closed-form formula for the limiting
variance in the overparameterized regime. Recall that the limiting variance in the underparameter-"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08185840707964602,"ized regime has a closed-form σ2
P"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08241150442477876,"i∈[m] zi
1−P"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08296460176991151,"i∈[m] zi for general m, as shown in Theorem 2."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08351769911504425,"Theorem 3. If m = 2 and d/n →z1 + z2 > 1, we have"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.084070796460177,"lim
n,di→+∞
di/n→zi"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08462389380530974,"V0,d,n = σ2
q2 + 1
q2(z1 −1) + 2q√z1z2 + z2 −1 ."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08517699115044247,Under review as a conference paper at ICLR 2022
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08573008849557522,"0
100
200
300
400
Number of Samples 0 5 10 15 20 25"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08628318584070796,Expected Variance
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.0868362831858407,"T. Var (  = 0)
N. Var (  = 0) (a)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08738938053097345,"0
1
2
3
4
5
0 5 10 15 20 ζ f(ζ) (b)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08794247787610619,"Figure 2: Figure 2a: We illustrate sample-wise triple descent of the variance term in ridgeless
regression (λ = 0). There are 2 blocks. We set d1 = 80 , d2 = 120, λ1 = 1 and λ2 = 100.
The two descents occur at around n = 125, 200, respectively. In the legend, “T. Var” denotes
the theoretical values predicted by Theorem 3. “N. Var” denotes the numerical values. Figure 2b:
Function f(ζ) deﬁned in Equation (17) with σ = 1. where"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08849557522123894,"q = λ1 (z1 −1) + λ2 (1 −z2) +
p"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08904867256637168,(λ1 (z1 −1) + λ2 (1 −z2)) 2 + 4λ1λ2z1z2
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.08960176991150443,"2λ2√z1z2
.
(16)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09015486725663717,"We illustrate the theoretical values predicted by Theorem 3 (overparameterized regime) and Theo-
rem 2 (underparameterized regime) in Figure 2a and compare it to the numerical values.
Corollary 2 (Triple descent in the two-block case). Assume m = 2, z1 = z2, d/n →ζ = 2z1, and
λ2/λ1 = ϱ. Deﬁne fϱ(ζ) = limn,di→+∞
di/n→zi
V0,d,n. We have"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09070796460176991,"f(ζ) ≜
lim
ϱ→+∞fρ(ζ) = 

 
"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09126106194690266,"σ2
ζ
1−ζ
ζ < 1 ,"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.0918141592920354,"σ2 
1
ζ−1 +
2
2−ζ −1

1 < ζ < 2"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09236725663716815,"σ2
2
ζ−2
ζ > 2"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09292035398230089,".
(17)"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09347345132743363,"There exists ζ1, ζ2, ζ3, ζ4 and ϱ0 such that for all ϱ > ϱ0, we have f ′
ϱ(ζ1) < 0, f ′
ϱ(ζ2) > 0,
f ′
ϱ(ζ3) < 0, and f ′
ϱ(ζ4) < 0."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09402654867256637,"Proof. The case ζ < 1 is already given in Theorem 2. In the sequel, assume ζ > 1. Deﬁne q as in
Equation (16). We have"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09457964601769911,"q = ζ +
p"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09513274336283185,"ζ2(ϱ + 1)2 −4ζ(ϱ −1)2 + 4(ϱ −1)2 −(ζ −2)ϱ −2 2ζϱ
."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.0956858407079646,"Recall Theorem 3, we get"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09623893805309734,"fϱ(ζ) =
lim
n,di→+∞
di/n→zi"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09679203539823009,"V0,d,n =
2
 
q2 + 1
"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09734513274336283,"ζ (q + 1)2 −2 (q2 + 1)
=
2σ2"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09789823008849557,ζ (q+1)2
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09845132743362832,"q2+1 −2
."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09900442477876106,Direct calculation yields
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.09955752212389381,"lim
ϱ→+∞fϱ(ζ) = 

 
"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.10011061946902655,"σ2
ζ
1−ζ
ζ < 1 ,"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.1006637168141593,"σ2 
1
ζ−1 +
2
2−ζ −1

1 < ζ < 2"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.10121681415929204,"σ2
2
ζ−2
ζ > 2 . ,"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.10176991150442478,"g(ζ) ≜
lim
ϱ→+∞f ′
ϱ(ζ) = 

 
"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.10232300884955753,"σ2
1
(ζ−1)2
ζ < 1 ,"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.10287610619469026,"σ2
ζ2−2
(ζ2−3ζ+2)2
1 < ζ < 2"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.103429203539823,"σ2
−2
(ζ−2)2
ζ > 2 . ,"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.10398230088495575,Under review as a conference paper at ICLR 2022
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.10453539823008849,"The function g(ζ) > 0 if ζ ∈
 √"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.10508849557522124,"2, 2

and we have g(ζ) < 0 if ζ <
√"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.10564159292035398,"2 or ζ > 2. Pick ζ1 > 2 >
ζ2 >
√"
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.10619469026548672,"2 > ζ3 > 1 > ζ4. Then we have g(ζ1) < 0, g(ζ2) > 0, g(ζ3) < 0, and g(ζ4) > 0. There
exists ϱ0 such that for all ϱ > ϱ0, we have f ′
ϱ(ζ1) < 0, f ′
ϱ(ζ2) > 0, f ′
ϱ(ζ3) < 0, and f ′
ϱ(ζ4) < 0."
LIMITING RISK AND SAMPLE-WISE MULTIPLE DESCENT,0.10674778761061947,"Collorary 2 theoretically proves that there exists triple descent when m = 2 and λ2 ≫λ1. Note
that a larger ζ = lim d/n reﬂects a relatively smaller n. If f ′
ϱ(ζ) < 0, then fϱ(ζ) decreases on
a neighborhood of ζ and therefore the limiting variance increases with a relatively larger n. As n
becomes relatively larger, we see an increasing stage, a decreasing stage, and ﬁnally an increasing
stage in order in the overparameterized regime (n < d). When we further increase n and enter
the underparameterized regime, we observe a decreasing stage. We illustrate f(ζ) in Figure 2b. In
Figure 2b, we observe two singularities at ζ = 1 and ζ = 2."
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.10730088495575221,"3.2
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE"
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.10785398230088496,"Recall the deﬁnition of the ridge estimator in Deﬁnition 1. Since this subsection concerns sample-
wise monotonicity, we add a subscript n to X and y (they are deﬁned by Equation (1) in Section 1.1)
to emphasize that they consist of n data items. Therefore we write"
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.1084070796460177,"ˆθλ,n,d ≜arg min
θ"
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.10896017699115045,"1
n ∥yn −Xnθ∥2
2 + λ ∥θ∥2
2 ."
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.10951327433628319,"In this subsection, under an assumption, we show that optimal regularization (i.e., pick λ that min-
imizes the generalization risk of ˆθλ,n,d) results in a monotone generalization risk curve—in other
words, with optimal regularization, more data always reduces the generalization risk. The assump-"
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.11006637168141593,"tion is that ∥Πiθ∗∥2 =
q di"
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.11061946902654868,"d , i.e., the squared norm of the projection of θ∗onto each eigenspace of
the covariance matrix is proportional to the dimension of that eigenspace. (Nakkiran et al., 2021)
showed by numerical results that optimal regularization can mitigate double descent for anisotropic
data distribution. We give a partial theoretical proof of their observed phenomenon."
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.11117256637168142,"To ease the notation, we use γi ≜lim n/di rather than zi ≜lim di/n in Theorem 4 because a larger
γ reﬂects a relatively larger n (in the limit). Theorem 4 shows that with the optimal regularization,
the limiting risk is an increasing function of γ1, . . . , γm."
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.11172566371681415,Theorem 4 (Optimal regularization). If
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.1122787610619469,∥Πiθ∗∥2 = r di
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.11283185840707964,"d ,
(18)"
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.11338495575221239,"then there exists a function g(γ1, . . . , γm) such that g(γ1, . . . , γm) is increasing in every γi and"
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.11393805309734513,"lim
n,di→∞
n/di→γi"
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.11449115044247787,"inf
λ>0 EXn,yn
ˆθλ,n −θ∗
2"
OPTIMAL REGULARIZATION MONOTONIZES GENERALIZATION RISK CURVE,0.11504424778761062,"Σ = g(γ1, . . . , γm) ."
CONCLUSION,0.11559734513274336,"4
CONCLUSION"
CONCLUSION,0.1161504424778761,"We studied the generalization risk (test error) versus the number of training samples in ridgeless
regression. Under the assumption that the data distribution is Gaussian and the spectrum distribu-
tion of its covariance matrix converges to a discrete distribution, we obtained the exact formulae
for the limiting bias and variance terms using the random matrix theory when the dimension and
the number of training samples go to inﬁnity in a proportional manner. Using these formulae, we
proved the sample-wise multiple descent phenomenon of the generalization risk curve.Moreover, we
theoretically showed that the ridge estimator with optimal regularization can result in a monotone
generalization risk curve and thereby eliminate multiple descent under some assumptions."
REFERENCES,0.11670353982300885,REFERENCES
REFERENCES,0.1172566371681416,"Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of gener-
alization error in neural networks. Neural Networks, 132:428–446, 2020."
REFERENCES,0.11780973451327434,Under review as a conference paper at ICLR 2022
REFERENCES,0.11836283185840708,"Zhi-Dong Bai and Yong-Qua Yin. Limit of the smallest eigenvalue of a large dimensional sample
covariance matrix. In Advances In Statistics, pp. 108–127. World Scientiﬁc, 2008."
REFERENCES,0.11891592920353983,"Zhidong Bai and Jack W Silverstein.
Spectral analysis of large dimensional random matrices,
volume 20. Springer, 2010."
REFERENCES,0.11946902654867257,"Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overﬁtting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020."
REFERENCES,0.12002212389380532,"Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
Reconciling modern machine-
learning practice and the classical bias–variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849–15854, 2019."
REFERENCES,0.12057522123893805,"Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
Journal on Mathematics of Data Science, 2(4):1167–1180, 2020."
REFERENCES,0.12112831858407079,"Siegfried B¨os and Manfred Opper. Dynamics of batch training in a perceptron. Journal of Physics
A: Mathematical and General, 31(21):4835, 1998."
REFERENCES,0.12168141592920353,"Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi.
Multiple descent: Design your own
generalization curve. arXiv preprint arXiv:2008.01036, 2020a."
REFERENCES,0.12223451327433628,"Lin Chen, Yifei Min, Mingrui Zhang, and Amin Karbasi. More data can expand the generalization
gap between adversarially robust and standard models. In International Conference on Machine
Learning, pp. 1670–1680. PMLR, 2020b."
REFERENCES,0.12278761061946902,"Donald L Cohn. Measure theory. Springer, 2013."
REFERENCES,0.12334070796460177,"Edgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: Ridge regression
and classiﬁcation. The Annals of Statistics, 46(1):247–279, 2018."
REFERENCES,0.12389380530973451,"Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.
Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019."
REFERENCES,0.12444690265486726,"Shun ichi Amari, Jimmy Ba, Roger Baker Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny
Wu, and Ji Xu. When does preconditioning help or hurt generalization? In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
S724o4_WB3."
REFERENCES,0.125,"Anders Krogh and John A Hertz. Generalization in a linear perceptron in the presence of noise.
Journal of Physics A: Mathematical and General, 25(5):1135, 1992."
REFERENCES,0.12555309734513273,"Yann Le Cun, Ido Kanter, and Sara A Solla. Eigenvalues of covariance matrices: Application to
neural-network learning. Physical Review Letters, 66(18):2396, 1991."
REFERENCES,0.1261061946902655,"Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm
interpolants and restricted lower isometry of kernels. In Conference on Learning Theory, pp.
2683–2711. PMLR, 2020."
REFERENCES,0.12665929203539822,"F. Liese and K.J. Miescke. Statistical Decision Theory: Estimation, Testing, and Selection. Springer
Series in Statistics. Springer New York, 2008. ISBN 9780387731940."
REFERENCES,0.12721238938053098,"Marco Loog, Tom Viering, Alexander Mey, Jesse H Krijthe, and David MJ Tax. A brief prehistory of
double descent. Proceedings of the National Academy of Sciences, 117(20):10625–10626, 2020."
REFERENCES,0.1277654867256637,"Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and the double descent curve. Communications on Pure and Applied Mathematics,
2019."
REFERENCES,0.12831858407079647,"Yifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More
data can help, double descend, or hurt generalization. In Conference on Uncertainty in Artiﬁcial
Intelligence, 2021."
REFERENCES,0.1288716814159292,Under review as a conference paper at ICLR 2022
REFERENCES,0.12942477876106195,"Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learn-
ing Representations, 2020. URL https://openreview.net/forum?id=B1g5sA4twr."
REFERENCES,0.12997787610619468,"Preetum Nakkiran, Prayaag Venkat, Sham M. Kakade, and Tengyu Ma. Optimal regularization can
mitigate double descent. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=7R7fAoUygoa."
REFERENCES,0.13053097345132744,"M Opper, W Kinzel, J Kleinz, and R Nehl. On the ability of the optimal perceptron to generalise.
Journal of Physics A: Mathematical and General, 23(11):L581, 1990."
REFERENCES,0.13108407079646017,"Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge (less) regres-
sion under general source condition. In International Conference on Artiﬁcial Intelligence and
Statistics, pp. 3889–3897. PMLR, 2021."
REFERENCES,0.13163716814159293,"Walter Rudin. Principles of mathematical analysis. New York, NY: McGraw-Hill, Inc., 3 edition,
1976."
REFERENCES,0.13219026548672566,"Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. Regularized linear regression: A precise
analysis of the estimation error.
Proceedings of Machine Learning Research, 40:1683–1709,
2015."
REFERENCES,0.13274336283185842,"Alexander Tsigler and Peter L Bartlett.
Benign overﬁtting in ridge regression.
arXiv preprint
arXiv:2009.14286, 2020."
REFERENCES,0.13329646017699115,"F Vallet, J-G Cailton, and Ph Refregier. Linear and nonlinear extension of the pseudo-inverse solu-
tion for learning boolean functions. EPL (Europhysics Letters), 9(4):315, 1989."
REFERENCES,0.1338495575221239,"Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010."
REFERENCES,0.13440265486725664,"Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018."
REFERENCES,0.13495575221238937,"Timothy LH Watkin, Albrecht Rau, and Michael Biehl. The statistical mechanics of learning a rule.
Reviews of Modern Physics, 65(2):499, 1993."
REFERENCES,0.13550884955752213,"Denny Wu and Ji Xu. On the optimal weighted ℓ2 regularization in overparameterized linear re-
gression. arXiv preprint arXiv:2006.05800, 2020."
REFERENCES,0.13606194690265486,Under review as a conference paper at ICLR 2022
REFERENCES,0.13661504424778761,"Notation
Comments
Σ = PΛP ⊤
Rd×d
Covariance matrix of data
P
O(d)
Λ = diag(λ1Id1, . . . , λmIdm)
Rd×d"
REFERENCES,0.13716814159292035,"θ′ ≜P ⊤θ∗
Rd"
REFERENCES,0.1377212389380531,"X = Z⊤Λ1/2P ⊤= (x1, . . . , xn)⊤
Rn×d
xi ∼N(0, Σ)
Z = (Z1, . . . , Zm)⊤
Rd×n
Zi ∈Rn×di. Each entry follows N(0, 1)."
REFERENCES,0.13827433628318583,Table 1: Notation
REFERENCES,0.1388274336283186,"A
EIGENDECOMPOSITION AND MORE NOTATION"
REFERENCES,0.13938053097345132,"Write Σ = PΛP ⊤, where P is an orthogonal matrix and Λ = diag (λ1Id1, . . . , λmIdm) ∈Rd×d is
a diagonal matrix. Write λ−= mini∈[m] λi and λ+ = maxi∈[m] λi. We can generate x1, . . . , xn
from standard normal random vector zi ∼N(0, Id) by setting xi = PΛ1/2zi. Therefore, if Z =
( z1
. . .
zn ) ∈Rd×n,we get"
REFERENCES,0.13993362831858408,"X⊤= ( x1
. . .
xn ) = PΛ1/2 ( z1
. . .
zn ) = PΛ1/2Z ."
REFERENCES,0.1404867256637168,"Take the transpose gives X = Z⊤Λ1/2P ⊤. Note that every entry of Z ∈Rd×n follows i.i.d.
N(0, 1). Write Z in a row-partitioned form Z =  
"
REFERENCES,0.14103982300884957,"Z⊤
1...
Z⊤
m  
,"
REFERENCES,0.1415929203539823,where Zi ∈Rn×di. Write P in a column-partitioned form
REFERENCES,0.14214601769911506,"P = ( P1
. . .
Pm ) ,"
REFERENCES,0.1426991150442478,"where Pi ∈Rd×di. Recall that Πi ∈Rd×d denotes the orthogonal projection to the eigenspace of
λi. We have Πi = PiP ⊤
i . Deﬁne θ′ ≜P ⊤θ∗and write it in a row-partitioned form θ′ =  
"
REFERENCES,0.14325221238938052,"P ⊤
1 θ∗
...
P ⊤
mθ∗  
=  
"
REFERENCES,0.14380530973451328,"θ′
1...
θ′
m "
REFERENCES,0.144358407079646,"
,
(19)"
REFERENCES,0.14491150442477876,"where θ′
i ∈Rdi. Then ∥θ′
i∥2 =
P ⊤
i θ∗
2 =
PiP ⊤
i θ∗
2 = ∥Πiθ∗∥2. We summarize part of the
notation above in Table 1."
REFERENCES,0.1454646017699115,"B
BIAS AND VARIANCE UNDER EIGENDECOMPOSITION"
REFERENCES,0.14601769911504425,Lemma 1 characterizes the smallest and largest eigenvalue of Z⊤Z
REFERENCES,0.14657079646017698,"d
(if n/d →γ < 1) and ZZ⊤"
REFERENCES,0.14712389380530974,"n
(if
n/d →γ > 1). Recall that we study the asymptotic regime di/n →zi. Therefore γ =
1
P"
REFERENCES,0.14767699115044247,j∈[m] zj .
REFERENCES,0.14823008849557523,"Lemma 1 ((Bai & Yin, 2008, Theorem 2)). Let Z ∈Rd×n be a random matrix whose entries are
i.i.d. N(0, 1) random variables. As n, d →+∞, n/d →γ ∈(0, 1), we have"
REFERENCES,0.14878318584070796,lim λmin Z⊤Z d
REFERENCES,0.14933628318584072,"
= (1 −√γ)2 ,
lim λmax Z⊤Z d"
REFERENCES,0.14988938053097345,"
= (1 + √γ)2"
REFERENCES,0.1504424778761062,"almost surely. If γ ∈(1, ∞), as n, d →+∞, n/d →γ, we have"
REFERENCES,0.15099557522123894,lim λmin ZZ⊤ n
REFERENCES,0.1515486725663717,"
=

1 −
p"
REFERENCES,0.15210176991150443,"1/γ
2
,
lim λmax ZZ⊤ n"
REFERENCES,0.15265486725663716,"
=

1 +
p"
REFERENCES,0.1532079646017699,"1/γ
2"
REFERENCES,0.15376106194690264,almost surely.
REFERENCES,0.1543141592920354,Under review as a conference paper at ICLR 2022
REFERENCES,0.15486725663716813,"Lemma 2 (Corollary 5.35 (Vershynin, 2010)). Let A be an N × n matrix whose entries are in-
dependent standard normal random variables. Then for every t ≥0, with probability at least
1 −2 exp
 
−t2/2

one has
√"
REFERENCES,0.1554203539823009,"N −√n −t ≤smin(A) ≤smax(A) ≤
√"
REFERENCES,0.15597345132743362,"N + √n + t ,"
REFERENCES,0.15652654867256638,"where smin(A) and smax(A) are the smallest and largest singular value of A.
Lemma 3. Let Z ∈Rd×n be a random matrix whose entries are i.i.d. N(0, 1) random variables,
where d = d(n) satisﬁes limn→∞
n
d(n) = γ. There exists universal positive constants C1, C2, N
such that for all n > N, we have"
REFERENCES,0.1570796460176991,0 < C1 < 1
REFERENCES,0.15763274336283187,"ns2
min(Z) ≤1"
REFERENCES,0.1581858407079646,"ns2
max(Z) < C2 ."
REFERENCES,0.15873893805309736,"Proof. Since d(n) ≍n, with loss of generality, we assume n/d →γ ∈(0, 1). Take t = c1
√n in"
REFERENCES,0.1592920353982301,"Lemma 2, where c1 = 1"
REFERENCES,0.15984513274336284,"2

1
√γ −1

> 0. With probability at least 1 −2e−c2
1n/2, we have √"
REFERENCES,0.16039823008849557,"d −√n −c1
√n ≤smin(Z) ≤smax(Z) ≤
√"
REFERENCES,0.1609513274336283,"d + √n + c1
√n ."
REFERENCES,0.16150442477876106,"Therefore, we deduce
 r"
REFERENCES,0.1620575221238938,"d
n −1 −c1 !2 ≤1"
REFERENCES,0.16261061946902655,"ns2
min(Z) ≤1"
REFERENCES,0.16316371681415928,"ns2
max(Z) ≤ r"
REFERENCES,0.16371681415929204,"d
n + 1 + c1 !2 ."
REFERENCES,0.16426991150442477,Deﬁne C1 = 1
REFERENCES,0.16482300884955753,"8

1
√γ −1
2
> 0 and C2 =

3
√γ + 1
2
. Then there exists a universal constant N1"
REFERENCES,0.16537610619469026,"such that for all n > N1, with probability at least 1 −2e−c2
1n/2, we have"
REFERENCES,0.16592920353982302,0 < C1 < 1
REFERENCES,0.16648230088495575,"ns2
min(Z) ≤1"
REFERENCES,0.1670353982300885,"ns2
max(Z) < C2 ."
REFERENCES,0.16758849557522124,"Deﬁne event En =

C1 < 1"
REFERENCES,0.168141592920354,"ns2
min(Z) ≤1"
REFERENCES,0.16869469026548672,"ns2
max(Z) < C2
	c. Then we have Pr {En} ≤2e−c2
1n/2.
Since P"
REFERENCES,0.16924778761061948,n≥1 Pr {En} ≤P
REFERENCES,0.1698008849557522,"n≥1 2e−c2
1n/2 < ∞, then the probability that inﬁnitely many of En
occur is 0, i.e.,"
REFERENCES,0.17035398230088494,"Pr

lim sup
n
En"
REFERENCES,0.1709070796460177,"
= 0 ."
REFERENCES,0.17146017699115043,"Therefore, there exists a universal constant N2 such that for all n > N2, En does not happen, in
other words,"
REFERENCES,0.1720132743362832,0 < C1 < 1
REFERENCES,0.17256637168141592,"ns2
min(Z) ≤1"
REFERENCES,0.17311946902654868,"ns2
max(Z) < C2"
REFERENCES,0.1736725663716814,holds.
REFERENCES,0.17422566371681417,"Lemma 4. Let Z ∈Rd×n be a random matrix whose entries are i.i.d. N(0, 1) random variables,
and let p be a ﬁxed positive integer which is viewed as a constant and hidden in ≲. If n ≍d, we
have E tr
 
ZZ⊤
≍n2, E tr
 
ZZ⊤2 ≍n3, and E ∥Z∥p
2 ≲np/2."
REFERENCES,0.1747787610619469,"Proof. We have
E tr
 
ZZ⊤
= E ∥Z∥2
F =
X"
REFERENCES,0.17533185840707965,"i∈[d],j∈[n]
Ez2
ij = nd ≍n2 ."
REFERENCES,0.17588495575221239,"Write Z =  
"
REFERENCES,0.17643805309734514,"z⊤
1...
z⊤
d "
REFERENCES,0.17699115044247787,"
, where zi ∈Rn and zi ∼N(0, In). We have E
 
z⊤
i zi
2 = E ∥zi∥4
2 ="
REFERENCES,0.17754424778761063,"n(n + 2). For i ̸= j, we deduce E
 
z⊤
i zj
2 = E
 
∥zi∥2 ∥zj∥2 u⊤v
2 where u, v ∼Unif
 
Sn−1"
REFERENCES,0.17809734513274336,"and ∥zi∥2 , ∥zj∥2 , u, v are independent. Then we get"
REFERENCES,0.1786504424778761,"E
 
∥zi∥∥zj∥s⊤
i sj
2 = E ∥zi∥2
2 ∥zj∥2
2
 
s⊤
i sj
2 = n2Eu2
1 = n2 · 1"
REFERENCES,0.17920353982300885,n = n .
REFERENCES,0.17975663716814158,Under review as a conference paper at ICLR 2022
REFERENCES,0.18030973451327434,"As a result, we have"
REFERENCES,0.18086283185840707,"E tr
 
ZZ⊤2 = E
ZZ⊤2 F =
X"
REFERENCES,0.18141592920353983,"i,j∈[d]
E
 
z⊤
i zj
2 = dn(n + 2) +
 
d2 −d

n ≍n3 ."
REFERENCES,0.18196902654867256,"By (Vershynin, 2018), there exists a universal constant C
> 0 such that for any t > 0,
P
n
∥Z∥2 > C
√n +
√"
REFERENCES,0.18252212389380532,"d + t
o
< 2e−t2. Deﬁne K = C
√n +
√"
REFERENCES,0.18307522123893805,"d

. Then we have"
REFERENCES,0.1836283185840708,"P {∥Z∥2 > K + t} < 2e−t2/C2 .
(20)"
REFERENCES,0.18418141592920353,"Recall Γ(z) =
R ∞
0
xz−1e−xdx. Setting t = C√u in the equation below yields
Z ∞"
REFERENCES,0.1847345132743363,"0
e−t2/C2tp−1dt ≲
Z ∞"
REFERENCES,0.18528761061946902,"0
e−uu
p
2 −1du = Γ
p 2"
REFERENCES,0.18584070796460178,"
≍1 ."
REFERENCES,0.1863938053097345,"Then we can bound the following integral
Z ∞"
REFERENCES,0.18694690265486727,"K
P {∥Z∥2 ≥t} ptp−1dt =
Z ∞"
REFERENCES,0.1875,"0
P {∥Z∥2 ≥K + t} p (t + K)p−1 dt ≲
Z ∞"
REFERENCES,0.18805309734513273,"0
e−t2/C2 (t + K)p−1 dt ≲
Z ∞"
REFERENCES,0.1886061946902655,"0
e−t2/C2  
tp−1 + Kp−1
dt =
Z ∞"
REFERENCES,0.18915929203539822,"0
e−t2/C2tp−1dt + Kp−1
Z ∞"
REFERENCES,0.18971238938053098,"0
e−t2/C2dt"
REFERENCES,0.1902654867256637,"≲n
p−1 2 ,"
REFERENCES,0.19081858407079647,"where the ﬁrst inequality is because of Equation (20). We are in a position to bound E ∥Z∥p
2:"
REFERENCES,0.1913716814159292,"E ∥Z∥p
2 =
Z ∞"
REFERENCES,0.19192477876106195,"0
P {∥Z∥p
2 ≥u} du =
Z ∞"
REFERENCES,0.19247787610619468,"0
P {∥Z∥2 ≥t} ptp−1dt =
Z K"
REFERENCES,0.19303097345132744,"0
P {∥Z∥2 ≥t} ptp−1dt +
Z ∞"
REFERENCES,0.19358407079646017,"K
P {∥Z∥2 ≥t} ptp−1dt"
REFERENCES,0.19413716814159293,≲np/2 + n(p−1)/2
REFERENCES,0.19469026548672566,"≲np/2 ,"
REFERENCES,0.19524336283185842,"where the ﬁrst inequality is because
Z K"
REFERENCES,0.19579646017699115,"0
P {∥Z∥2 ≥t} ptp−1dt ≤
Z K"
REFERENCES,0.1963495575221239,"0
ptp−1dt = Kp ≲np/2 ."
REFERENCES,0.19690265486725664,"Lemma 5. The following equation for the bias term Bλ,d,n (deﬁned in Equation (3)) holds"
REFERENCES,0.19745575221238937,"Bλ,d,n =E
h
∥Λ1/2 
Id −Λ1/2Z
 
nλIn + Z⊤ΛZ
−1 Z⊤Λ1/2
θ′∥2
2
i
(21) =E """
REFERENCES,0.19800884955752213,"∥Λ1/2

Id + 1"
REFERENCES,0.19856194690265486,"nλΛ1/2ZZ⊤Λ1/2
−1
θ′∥2
2 #"
REFERENCES,0.19911504424778761,".
(22)"
REFERENCES,0.19966814159292035,"Moreover, we have |Bλ,d,n| ≲∥θ∗∥2
2 and limλ→0+ Bλ,d,n = B0,d,n. For all sufﬁciently large n and
d such that n/d →γ ∈(0, 1), we have 0 ≤
d
dλBλ,d,n ≲∥θ∗∥2
2. Therefore, {Bλ,d,n} is uniformly
bounded and uniformly equicontinuous with respect to λ ∈(0, ∞)."
REFERENCES,0.2002212389380531,Under review as a conference paper at ICLR 2022
REFERENCES,0.20077433628318583,"Proof. Introduce the shorthand notation M = Λ1/2ZZ⊤Λ1/2 ∈Rd×d, A = Id +
1
nλM ∈Rd×d,
N = nλIn + Z⊤ΛZ ∈Rn×n, and Q = Id −Λ1/2ZN −1Z⊤Λ1/2 ∈Rd×d. Because"
REFERENCES,0.2013274336283186,"X⊤(nλIn + XX⊤)−1X = PΛ1/2Z
 
nλIn + Z⊤ΛZ
−1 Z⊤Λ1/2P ⊤,"
REFERENCES,0.20188053097345132,we have
REFERENCES,0.20243362831858408,"Bλ,d,n =E
h
∥(Id −PΛ1/2Z
 
nλIn + Z⊤ΛZ
−1 Z⊤Λ1/2P ⊤)θ∗∥2
P ΛP ⊤
i"
REFERENCES,0.2029867256637168,"=E
h
∥Λ1/2 
Id −Λ1/2Z
 
nλIn + Z⊤ΛZ
−1 Z⊤Λ1/2
θ′∥2
2
i"
REFERENCES,0.20353982300884957,"=E
h
∥Λ1/2Qθ′∥2
2
i
."
REFERENCES,0.2040929203539823,Using the Sherman–Morrison-Woodbury formula yields
REFERENCES,0.20464601769911506,N −1 = 1
REFERENCES,0.2051991150442478,"nλIn −
1"
REFERENCES,0.20575221238938052,"(nλ)2 Z⊤Λ1/2

I + 1"
REFERENCES,0.20630530973451328,"nλΛ1/2ZZ⊤Λ1/2
−1
Λ1/2Z = 1 nλ"
REFERENCES,0.206858407079646,"
In −Z⊤Λ1/2 
nλId + Λ1/2ZZ⊤Λ1/2−1
Λ1/2Z
 = 1 nλ"
REFERENCES,0.20741150442477876,"
In −Z⊤Λ1/2 (nλId + M)−1 Λ1/2Z

.
(23)"
REFERENCES,0.2079646017699115,It follows that
REFERENCES,0.20851769911504425,Q =Id −Λ1/2ZN −1Z⊤Λ1/2
REFERENCES,0.20907079646017698,=Id −1
REFERENCES,0.20962389380530974,"nλΛ1/2Z

In −Z⊤Λ1/2 (nλId + M)−1 Λ1/2Z

Z⊤Λ1/2"
REFERENCES,0.21017699115044247,=Id −M nλ
REFERENCES,0.21073008849557523,"
Id −(nλId + M)−1 M
"
REFERENCES,0.21128318584070796,=Id −M nλ
REFERENCES,0.21183628318584072,"
Id −(nλId + M)−1 (nλId + M −nλId)
"
REFERENCES,0.21238938053097345,=Id −M (nλId + M)−1
REFERENCES,0.2129424778761062,"=

Id + 1"
REFERENCES,0.21349557522123894,"nλM
−1"
REFERENCES,0.2140486725663717,=A−1 .
REFERENCES,0.21460176991150443,"Therefore, we deduce"
REFERENCES,0.21515486725663716,"Bλ,d,n = E """
REFERENCES,0.2157079646017699,"∥Λ1/2

Id + 1"
REFERENCES,0.21626106194690264,"nλM
−1
θ′∥2
2 #"
REFERENCES,0.2168141592920354,"= E
h
∥Λ1/2A−1θ′∥2
2
i
."
REFERENCES,0.21736725663716813,"Because
Λ1/2
2 ≲1 and

 
Id +
1
nλΛ1/2ZZ⊤Λ1/2−1
2 ≤1, we have"
REFERENCES,0.2179203539823009,"∥Λ1/2A−1θ′∥2
2 ≲∥θ′∥2
2 = ∥θ∗∥2
2 ."
REFERENCES,0.21847345132743362,"Therefore |Bλ,d,n| ≲∥θ∗∥2
2. Moreover, by the dominated convergence theorem,"
REFERENCES,0.21902654867256638,"lim
λ→0+ Bλ,d,n = B0,d,n ."
REFERENCES,0.2195796460176991,We compute the derivative of A−1: dA−1
REFERENCES,0.22013274336283187,"dλ
= −A−1 dA"
REFERENCES,0.2206858407079646,"dλ A−1 = MA−2 nλ2
."
REFERENCES,0.22123893805309736,The matrix M
REFERENCES,0.2217920353982301,n = Λ1/2ZZ⊤Λ1/2
REFERENCES,0.22234513274336284,"n
∈Rd×d is positive semideﬁnite and its d −n smallest eigenvalues
are zeros. Its non-zero eigenvalues are the same as the non-zero eigenvalues of Z⊤ΛZ"
REFERENCES,0.22289823008849557,"n
. Because all
eigenvalues of Z⊤ΛZ"
REFERENCES,0.2234513274336283,"n
are positive almost surely, the spectrum of M"
REFERENCES,0.22400442477876106,n consists of d −n zeros and the
REFERENCES,0.2245575221238938,Under review as a conference paper at ICLR 2022
REFERENCES,0.22511061946902655,spectrum of Z⊤ΛZ
REFERENCES,0.22566371681415928,"n
. We study the range of the spectrum of Z⊤ΛZ"
REFERENCES,0.22621681415929204,"n
. Because λ−Z⊤Z"
REFERENCES,0.22676991150442477,"n
≼Z⊤ΛZ"
REFERENCES,0.22732300884955753,"n
≼
λ+ Z⊤Z"
REFERENCES,0.22787610619469026,"n , we deduce λmin Z⊤ΛZ n"
REFERENCES,0.22842920353982302,"
≥λ−λmin Z⊤Z n"
REFERENCES,0.22898230088495575,"
→λ−

1 −
p"
REFERENCES,0.2295353982300885,"1/γ
2
(24) λmax Z⊤ΛZ n"
REFERENCES,0.23008849557522124,"
≤λ+λmax Z⊤Z n"
REFERENCES,0.230641592920354,"
→λ+

1 +
p"
REFERENCES,0.23119469026548672,"1/γ
2
.
(25)"
REFERENCES,0.23174778761061948,"Deﬁne L1
=
λ−λmin

Z⊤Z"
REFERENCES,0.2323008849557522,"n

and L2
=
λ+λmax

Z⊤Z"
REFERENCES,0.23285398230088494,"n

.
We get lim n,d→+∞
n/d→γ<1
L1
="
REFERENCES,0.2334070796460177,"λ−

1 −
p"
REFERENCES,0.23396017699115043,"1/γ
2
, lim n,d→+∞
n/d→γ<1
L2 = λ+

1 +
p"
REFERENCES,0.2345132743362832,"1/γ
2
and"
REFERENCES,0.23506637168141592,"spec
Z⊤ΛZ n"
REFERENCES,0.23561946902654868,"
⊆[L1, L2] ."
REFERENCES,0.2361725663716814,"We bound
MA−3
2"
REFERENCES,0.23672566371681417,"MA−3
2 =n M n"
REFERENCES,0.2372787610619469,"
Id + M nλ"
REFERENCES,0.23783185840707965,"−3
2"
REFERENCES,0.23838495575221239,"=n
max
s∈spec( M n ) s"
REFERENCES,0.23893805309734514,(1 + s/λ)3
REFERENCES,0.23949115044247787,"=n
max
s∈{0}∪spec

Z⊤ΛZ"
REFERENCES,0.24004424778761063,"n
.

s"
REFERENCES,0.24059734513274336,(1 + s/λ)3
REFERENCES,0.2411504424778761,"=n
max
s∈spec

Z⊤ΛZ"
REFERENCES,0.24170353982300885,"n
.

s"
REFERENCES,0.24225663716814158,(1 + s/λ)3
REFERENCES,0.24280973451327434,"≤n
max
s∈[L1,L2]
s"
REFERENCES,0.24336283185840707,(1 + s/λ)3 .
REFERENCES,0.24391592920353983,"We compute
d
dλ∥Λ1/2A−1θ′∥2
2:"
REFERENCES,0.24446902654867256,"d
dλ∥Λ1/2A−1θ′∥2
2 = 1"
REFERENCES,0.24502212389380532,"nλ2 θ′⊤ 
A−1ΛMA−2 + MA−2ΛA−1
θ′ = 1"
REFERENCES,0.24557522123893805,"nλ2
 
A−1θ′⊤ 
ΛMA−1 + MA−1Λ
  
A−1θ′"
REFERENCES,0.2461283185840708,"Next, we bound
 d"
REFERENCES,0.24668141592920353,"dλ∥Λ1/2A−1θ′∥2
2
:

d
dλBλ,d,n  ≤1"
REFERENCES,0.2472345132743363,"nλ2
MA−2ΛA−1 + A−1ΛMA−2
2 ∥θ′∥2
2 ≤2"
REFERENCES,0.24778761061946902,"nλ2
MA−2ΛA−1
2 ∥θ′∥2
2 = 2"
REFERENCES,0.24834070796460178,"nλ2
MA−3AΛA−1
2 ∥θ′∥2
2 ≤2"
REFERENCES,0.2488938053097345,"nλ2
MA−3
2
AΛA−1
2 ∥θ′∥2
2 ≲1"
REFERENCES,0.24944690265486727,"λ2
max
s∈[L1,L2]
s"
REFERENCES,0.25,"(1 + s/λ)3 ∥θ′∥2
2 ,"
REFERENCES,0.25055309734513276,"where the last inequality is because
AΛA−1
2 = ∥Λ∥2 ≤λ+ ≲1. Deﬁne f(s) =
s
(1+s/λ)3 ."
REFERENCES,0.25110619469026546,Because f ′(s) = λ3(λ−2s)
REFERENCES,0.2516592920353982,"(λ+s)4 , the function f is increasing on [0, λ/2] and decreasing on [λ/2, +∞)."
REFERENCES,0.252212389380531,Under review as a conference paper at ICLR 2022
REFERENCES,0.25276548672566373,"If λ ≤2L1, we have"
REFERENCES,0.25331858407079644,"max
s∈[L1,L2]
s"
REFERENCES,0.2538716814159292,"(1 + s/λ)3 =
L1
(1 + L1/λ)3 ."
REFERENCES,0.25442477876106195,It follows that
REFERENCES,0.2549778761061947,"1
λ2 ·
L1
(1 + L1/λ)3 =
L1λ"
REFERENCES,0.2555309734513274,"(λ + L1)3 ≤
max
λ∈[0,2L1]
L1λ"
REFERENCES,0.2560840707964602,"(λ + L1)3 ≲1 L1
."
REFERENCES,0.25663716814159293,"If λ ≥2L2, we get"
REFERENCES,0.2571902654867257,"1
λ2
max
s∈[L1,L2]
s"
REFERENCES,0.2577433628318584,(1 + s/λ)3 = 1
REFERENCES,0.25829646017699115,"λ2 ·
L2
(1 + L2/λ)3 ≤
max
λ∈[2L2,∞)
L2λ"
REFERENCES,0.2588495575221239,"(λ + L2)3 ≲1 L2
≤1 L1
."
REFERENCES,0.2594026548672566,"If 2L1 < λ < 2L2, we obtain"
REFERENCES,0.25995575221238937,"1
λ2
max
s∈[L1,L2]
s"
REFERENCES,0.2605088495575221,"(1 + s/λ)3 ≲1 λ ≲1 L1
."
REFERENCES,0.2610619469026549,"In all three cases, we show that
1
λ2 maxs∈[L1,L2]
s
(1+s/λ)3 ≲
1
L1 . It follows that"
REFERENCES,0.2616150442477876,"d
dλ∥Λ1/2A−1θ′∥2
2 ≲1"
REFERENCES,0.26216814159292035,"L1
∥θ′∥2
2 =
∥θ′∥2
2
λ−λmin

Z⊤Z"
REFERENCES,0.2627212389380531,"n
 ≍
∥θ′∥2
2
λmin

Z⊤Z n
 ."
REFERENCES,0.26327433628318586,"By Lemma 3, there exists a universal constant n0 such that for all n > n0, one has
1
λmin

Z⊤Z"
REFERENCES,0.26382743362831856,"n
 ≲1."
REFERENCES,0.2643805309734513,"Thus we conclude that

d
dλ∥Λ1/2A−1θ′∥2
2"
REFERENCES,0.2649336283185841,"≲∥θ′∥2
2 ."
REFERENCES,0.26548672566371684,We can exchange differentiation and expectation and get
REFERENCES,0.26603982300884954,"d
dλBλ,d,n = E
 d"
REFERENCES,0.2665929203539823,"dλ∥Λ1/2A−1θ′∥2
2 "
REFERENCES,0.26714601769911506,"and

d
dλBλ,d,n"
REFERENCES,0.2676991150442478,"= E

d
dλ∥Λ1/2A−1θ′∥2
2 "
REFERENCES,0.2682522123893805,"
≲∥θ′∥2
2 ."
REFERENCES,0.2688053097345133,Lemma 6. The following equation for the variance term holds
REFERENCES,0.26935840707964603,"Vλ,d,n =σ2E∥Λ1/2 
λnId + Λ1/2ZZ⊤Λ1/2−1
Λ1/2Z∥2
2"
REFERENCES,0.26991150442477874,"=σ2E∥ΛZ
 
λnIn + Z⊤ΛZ
−1 ∥2
2 ."
REFERENCES,0.2704646017699115,"Moreover, for all sufﬁciently large n and d such that n/d →γ ̸= 1, we have limλ→0+ Vλ,d,n =
V0,d,n, |Vλ,d,n| ≲1 and
 d"
REFERENCES,0.27101769911504425,"dλVλ,d,n
 ≲1. Therefore, {Vλ,d,n} is uniformly bounded and uniformly
equicontinuous with respect to λ ∈(0, ∞)."
REFERENCES,0.271570796460177,"Proof. As in the proof of Lemma 5, deﬁne M = Λ1/2ZZ⊤Λ1/2 ∈Rd×d and N = nλIn+Z⊤ΛZ ∈
Rn×n. Recalling Σ = PΛP ⊤and X = Z⊤Λ1/2P ⊤, we have"
REFERENCES,0.2721238938053097,"Vλ,d,n =σ2E tr

XΣX⊤(nλIn + XX⊤)−2"
REFERENCES,0.27267699115044247,"=σ2E tr

Z⊤Λ2ZN −2"
REFERENCES,0.27323008849557523,"=σ2E tr

N −1Z⊤Λ2ZN −1"
REFERENCES,0.273783185840708,"=σ2E
ΛZN −12
F ."
REFERENCES,0.2743362831858407,Under review as a conference paper at ICLR 2022
REFERENCES,0.27488938053097345,Recalling Equation (23) yields
REFERENCES,0.2754424778761062,ΛZN −1 = 1
REFERENCES,0.27599557522123896,"nλΛZ

In −Z⊤Λ1/2 (nλId + M)−1 Λ1/2Z
 = 1"
REFERENCES,0.27654867256637167,"nλΛ1/2 
In −M (nλId + M)−1
Λ1/2Z"
REFERENCES,0.2771017699115044,=Λ1/2 (nλId + M)−1 Λ1/2Z .
REFERENCES,0.2776548672566372,Deﬁne R = Λ1/2Z ∈Rd×n. We get
REFERENCES,0.2782079646017699,"(nλId + M)−1 Λ1/2Z =
 
nλId + RR⊤−1 R ."
REFERENCES,0.27876106194690264,"Notice that if 0 < a < b, then aId + RR⊤≼bId + RR⊤. We deduce
 
bId + RR⊤2 −
 
aId + RR⊤2 =
 
b2 −a2
Id + 2 (b −a) RR⊤≽0 ."
REFERENCES,0.2793141592920354,"Thus
 
bId + RR⊤2 ≽
 
aId + RR⊤2, which implies
 
bId + RR⊤−2 ≼
 
aId + RR⊤−2. We
get"
REFERENCES,0.27986725663716816,"R⊤ 
bId + RR⊤−2 R ≼R⊤ 
aId + RR⊤−2 R ,"
REFERENCES,0.28042035398230086,"tr

R⊤ 
bId + RR⊤−2 R

≤tr

R⊤ 
aId + RR⊤−2 R
"
REFERENCES,0.2809734513274336,"Let λ0 (·) denote the smallest non-zero eigenvalue of a positive semideﬁnite matrix. We bound the
Frobenius norm
(nλId + M)−1 Λ1/2Z

2 F"
REFERENCES,0.2815265486725664,"= tr

R⊤ 
nλId + RR⊤−1 R
"
REFERENCES,0.28207964601769914,"≤tr

lim
λ→0+ R⊤ 
nλId + RR⊤−2 R
"
REFERENCES,0.28263274336283184,"= tr
 
R⊤R
+"
REFERENCES,0.2831858407079646,"= tr
 
Z⊤ΛZ
+"
REFERENCES,0.28373893805309736,"≲tr
 
Z⊤Z
+ .
It follows that
ΛZN −12
F =
Λ1/2 (nλId + M)−1 Λ1/2Z

2"
REFERENCES,0.2842920353982301,"F ≲
(nλId + M)−1 Λ1/2Z

2"
REFERENCES,0.2848451327433628,"F ≲tr
 
Z⊤Z
+ = tr
 
ZZ⊤+ ."
REFERENCES,0.2853982300884956,"If n/d →γ < 1, the matrix Z⊤Z is full-rank almost surely. Then, using the formula for the mean"
REFERENCES,0.28595132743362833,"of inverse Wishart distribution, we have E tr
 
Z⊤Z
+ = tr E
 
Z⊤Z
−1 = tr

In
d−n−1

≍1. If"
REFERENCES,0.28650442477876104,"n/d →γ > 1, the matrix ZZ⊤is full-rank almost surely. Similarly, we have E tr
 
ZZ⊤+ ="
REFERENCES,0.2870575221238938,"tr E
 
ZZ⊤−1 ≍1. By the dominated convergence theorem, we have limλ→0+ Vλ,d,n = V0,d,n."
REFERENCES,0.28761061946902655,"Moreover, Vλ,d,n ≲E
ΛZN −12
F ≲1."
REFERENCES,0.2881637168141593,"Next we bound
d
dλV (ˆθ). Because dN−1"
REFERENCES,0.288716814159292,"dλ
= −N −1 dN"
REFERENCES,0.28926991150442477,"dλ N −1 = −nN −2, we deduce
d
dλ"
REFERENCES,0.28982300884955753,"ΛZN −12
2 = d"
REFERENCES,0.2903761061946903,"dλ tr
 
N −1Z⊤Λ2ZN −1
= −2n tr
 
Z⊤Λ2ZN −3
≤0 ."
REFERENCES,0.290929203539823,"On the other hand, we have
tr
 
Z⊤Λ2ZN −3"
REFERENCES,0.29148230088495575,"= tr

N −3/2Z⊤Λ2ZN −3/2"
REFERENCES,0.2920353982300885,"≲tr

N −3/2Z⊤ΛZN −3/2"
REFERENCES,0.29258849557522126,"= tr
 
Z⊤ΛZN −3 =
X"
REFERENCES,0.29314159292035397,s∈spec(Z⊤ΛZ) s
REFERENCES,0.2936946902654867,(λn + s)3 .
REFERENCES,0.2942477876106195,Under review as a conference paper at ICLR 2022
REFERENCES,0.2948008849557522,"Because the number of non-zero eigenvalues of Z⊤ΛZ equals rank
 
Z⊤ΛZ

= n ∧d ≍n, we get

d
dλ"
REFERENCES,0.29535398230088494,"ΛZN −12
2"
REFERENCES,0.2959070796460177,"≍n tr
 
Z⊤Λ2ZN −3
≲n2
max
s∈spec(Z⊤ΛZ)
s"
REFERENCES,0.29646017699115046,"(λn + s)3 =
max
s∈spec

Z⊤ΛZ"
REFERENCES,0.29701327433628316,"n

\{0} s"
REFERENCES,0.2975663716814159,(λ + s)3 .
REFERENCES,0.2981194690265487,"If γ < 1, the matrix Z⊤ΛZ"
REFERENCES,0.29867256637168144,"n
is full-rank almost surely. By Equation (24) and Equation (25) in the"
REFERENCES,0.29922566371681414,"proof of Lemma 5, there exists universal positive constants C1 and C2 such that spec

Z⊤ΛZ n

⊆"
REFERENCES,0.2997787610619469,"[C1, C2] for all sufﬁciently large n and d such that n/d →γ < 1. If γ > 1, the non-zero eigenvalues"
REFERENCES,0.30033185840707965,of Z⊤ΛZ
REFERENCES,0.3008849557522124,"n
and M"
REFERENCES,0.3014380530973451,n are the same. The matrix M
REFERENCES,0.3019911504424779,"n is full-rank almost surely. Thus spec

Z⊤ΛZ"
REFERENCES,0.30254424778761063,"n

\{0} ="
REFERENCES,0.3030973451327434,"spec
  M"
REFERENCES,0.3036504424778761,"n

. Because y⊤Λ−1y ≲y⊤y, λmin M n"
REFERENCES,0.30420353982300885,"
= min
x̸=0
x⊤Λ1/2ZZ⊤Λ1/2"
REFERENCES,0.3047566371681416,"n
x
x⊤x
= min
y̸=0
y⊤ZZ⊤"
REFERENCES,0.3053097345132743,"n y
y⊤Λ−1y ≳min
y̸=0
y⊤ZZ⊤"
REFERENCES,0.30586283185840707,"n y
y⊤y
= λmin ZZ⊤ n 
."
REFERENCES,0.3064159292035398,"Similarly, we get λmax M n"
REFERENCES,0.3069690265486726,"
≲λmax ZZ⊤ n 
."
REFERENCES,0.3075221238938053,"Therefore, there exists universal positive constants C1 and C2 such that"
REFERENCES,0.30807522123893805,"spec
Z⊤ΛZ n"
REFERENCES,0.3086283185840708,"
\ {0} ⊆

C1λmin Z⊤Z n"
REFERENCES,0.30918141592920356,"
, C2λmax Z⊤Z n 
."
REFERENCES,0.30973451327433627,"Thus in both cases, we have shown that there exists universal positive constants C1 and C2 such that"
REFERENCES,0.310287610619469,"spec
Z⊤ΛZ n"
REFERENCES,0.3108407079646018,"
\ {0} ⊆

C1λmin Z⊤Z n"
REFERENCES,0.31139380530973454,"
, C2λmax Z⊤Z n 
."
REFERENCES,0.31194690265486724,"Deﬁne L1 = C1λmin

Z⊤Z"
REFERENCES,0.3125,"n

and L2 = C2λmax

Z⊤Z"
REFERENCES,0.31305309734513276,"n

. As a result, we get

d
dλ"
REFERENCES,0.31360619469026546,"ΛZN −12
2"
REFERENCES,0.3141592920353982,"≲
max
s∈[L1,L2]
s"
REFERENCES,0.314712389380531,(λ + s)3 .
REFERENCES,0.31526548672566373,"Deﬁne f(s) =
s
(λ+s)3 . Because f ′(s) =
λ−2s
(λ+s)4 , the function f is increasing on [0, λ/2] and
decreasing on [λ/2, +∞). If λ ≥2L2 or λ ≤2L1, we get"
REFERENCES,0.31581858407079644,"max
s∈[L1,L2]
s"
REFERENCES,0.3163716814159292,"(λ + s)3 ≤
L1
(λ + L1)3 ∨
L2
(λ + L2)3 ≤1"
REFERENCES,0.31692477876106195,"L2
1
."
REFERENCES,0.3174778761061947,"If 2L1 < λ < 2L2, we get"
REFERENCES,0.3180309734513274,"max
s∈[L1,L2]
s"
REFERENCES,0.3185840707964602,(λ + s)3 ≲1 λ2 ≲1
REFERENCES,0.31913716814159293,"L2
1
."
REFERENCES,0.3196902654867257,"As a result, for all sufﬁciently large n, we have

d
dλ"
REFERENCES,0.3202433628318584,"ΛZN −12
2"
REFERENCES,0.32079646017699115,"=
max
s∈[L1,L2]
s"
REFERENCES,0.3213495575221239,(λ + s)3 ≲1
REFERENCES,0.3219026548672566,"L2
1
≲
1"
REFERENCES,0.32245575221238937,"λ2
min

Z⊤Z"
REFERENCES,0.3230088495575221,"n
 ≲1 ,"
REFERENCES,0.3235619469026549,"where the ﬁnal inequality is because of Lemma 3. We can exchange the expectation and differenti-
ation and obtain
d
dλVλ,d,n = σ2E d dλ"
REFERENCES,0.3241150442477876,"ΛZN −12
2
and

d
dλVλ,d,n"
REFERENCES,0.32466814159292035,"≤σ2E

d
dλ"
REFERENCES,0.3252212389380531,"ΛZN −12
2 ≲1 ."
REFERENCES,0.32577433628318586,Under review as a conference paper at ICLR 2022
REFERENCES,0.32632743362831856,"C
LEMMAS ON STIELTJES TRANSFORM"
REFERENCES,0.3268805309734513,"Deﬁnition 3 (Stieltjes transform). The Stieltjes transform of a distribution with cumulative distri-
bution function F is deﬁned by"
REFERENCES,0.3274336283185841,"sF (z) =
Z
1
λ −z dF(λ)
(z ∈H ≜{z ∈C | ℑz > 0}) ."
REFERENCES,0.32798672566371684,"Lemma 7 (Theorem 4.3 (Bai & Silverstein, 2010)). Suppose that the entries of Xn ∈Cn×p are
complex random variables that are independent for each n and identically distributed for all n
and satisfy E
h
|x11 −Ex11|2i
= 1. Also, assume that Tn = diag(τ1, . . . , τp), τi is real, and the"
REFERENCES,0.32853982300884954,"empirical distribution function of {τ1, . . . , τp} converges almost surely to a probability distribution
function H as n →∞. The entries of both Xn and Tn may depend on n, which is suppressed for
brevity. Set Bn = An + 1"
REFERENCES,0.3290929203539823,"nXnTnX∗
n, where X∗
n is the conjugate transpose of Xn, An is Hermitian,
n × n satisfying F An →F A almost surely, where F A is a distribution function (possibly defective)
on the real line. Assume also that Xn, Tn, and An are independent. When p = p(n) with p/n →
y > 0 as n →∞, then, almost surely, F Bn, the empirical spectral distribution of the eigenvalues
of Bn, converges vaguely, as n →∞, to a (nonrandom) distribution function F, where for any
z ∈C+ = {z ∈C | ℑz > 0}, its Stieltjes transform s = s(z) is the unique solution in C+ to the
equation"
REFERENCES,0.32964601769911506,s = sA
REFERENCES,0.3301991150442478,"
z −y
Z τdH(τ)"
REFERENCES,0.3307522123893805,"1 + τs 
,"
REFERENCES,0.3313053097345133,where sA is the Stieltjes transform of F A.
REFERENCES,0.33185840707964603,"Lemma 8. If the functions fα, gα : I →R satisfy fα(x) −gα(x) →0 uniformly as α →+∞, then
limα→+∞(infx∈I f(x) −infx∈I g(x)) = 0."
REFERENCES,0.33241150442477874,"Proof. Because fα(x) −gα(x) →0 uniformly as α →+∞, we have for ∀ϵ > 0, there exists N(ϵ)
such that for ∀α > N(ϵ) and ∀x ∈I, it holds that |fα(x) −gα(x)| < ϵ. Therefore, we get"
REFERENCES,0.3329646017699115,gα(x) −ϵ < fα(x) < gα(x) + ϵ .
REFERENCES,0.33351769911504425,Thus we obtain
REFERENCES,0.334070796460177,"inf
x∈I fα(x) ≤fα(x) < gα(x) + ϵ"
REFERENCES,0.3346238938053097,"inf
x∈I gα(x) −ϵ ≤gα(x) −ϵ < fα(x) ,"
REFERENCES,0.33517699115044247,which in turn implies
REFERENCES,0.33573008849557523,"inf
x∈I fα(x) ≤inf
x∈I gα(x) + ϵ"
REFERENCES,0.336283185840708,"inf
x∈I gα(x) −ϵ ≤inf
x∈I fα(x) ."
REFERENCES,0.3368362831858407,"It follows that |infx∈I fα(x) −infx∈I gα(x)| ≤ϵ. In other words, we proved"
REFERENCES,0.33738938053097345,"lim
α→+∞"
REFERENCES,0.3379424778761062,"
inf
x∈I f(x) −inf
x∈I g(x)

= 0 ."
REFERENCES,0.33849557522123896,Lemma 9. Deﬁne N = λnIn + Z⊤ΛZ. Then we have
REFERENCES,0.33904867256637167,"lim
n,di→+∞
di/n→zi"
REFERENCES,0.3396017699115044,"tr
 
N −1
= d"
REFERENCES,0.3401548672566372,"dλ inf
ρ∈Rm
+  log "
REFERENCES,0.3407079646017699,"λ +
X"
REFERENCES,0.34126106194690264,"i∈[m]
λiρi  +
X i∈[m]"
REFERENCES,0.3418141592920354,"
ρi −zi"
REFERENCES,0.34236725663716816,"
log ρi"
REFERENCES,0.34292035398230086,"zi
+ 1
 , (26)"
REFERENCES,0.3434734513274336,"lim
n,di→+∞
di/n→zi"
N LOG DET N,0.3440265486725664,"1
n log det N"
N LOG DET N,0.34457964601769914,"n = inf
ρ∈Rm
+  log "
N LOG DET N,0.34513274336283184,"λ +
X"
N LOG DET N,0.3456858407079646,"i∈[m]
λiρi  +
X i∈[m]"
N LOG DET N,0.34623893805309736,"
ρi −zi"
N LOG DET N,0.3467920353982301,"
log ρi"
N LOG DET N,0.3473451327433628,"zi
+ 1
"
N LOG DET N,0.3478982300884956,. (27)
N LOG DET N,0.34845132743362833,Under review as a conference paper at ICLR 2022
N LOG DET N,0.34900442477876104,"Proof. Proof of Equation (26). We apply Lemma 7 with An = 0n×n, Xn = Z⊤∈Rn×d,
Tn = Λ, and Bn =
1
nZ⊤ΛZ.
The distribution function of 0n×n converges to 1t≤0 and
its Stieltjes transform is sA(z) =
R
1
λ−zd1λ≤0 = −1"
N LOG DET N,0.3495575221238938,"z.
The empirical distribution function
of {λ1, . . . , λ1
|
{z
}
d1"
N LOG DET N,0.35011061946902655,", . . . , λm . . . , λm
|
{z
}
dm"
N LOG DET N,0.3506637168141593,"} is Hn,di(t) = P
i∈[m]
di"
N LOG DET N,0.351216814159292,"d 1t≤λi.
Recall di/n →zi.
Thus"
N LOG DET N,0.35176991150442477,"di/d →zi/K, where d/n →y = P"
N LOG DET N,0.35232300884955753,"j∈[m] zj. The empirical distribution function converges to
H(t) = P"
N LOG DET N,0.3528761061946903,"i∈[m]
zi"
N LOG DET N,0.353429203539823,y 1t≤λi. Then the empirical spectral distribution of the eigenvalues of 1
N LOG DET N,0.35398230088495575,"nZ⊤ΛZ
converges vaguely to a nonrandom distribution function F and its Stieltjes transform is"
N LOG DET N,0.3545353982300885,"s = s(z) =
lim
n,di→+∞
di/n=zi"
N TR,0.35508849557522126,"1
n tr
 1"
N TR,0.35564159292035397,nZ⊤ΛZ −zIn
N TR,0.3561946902654867,"−1
=
lim
n,di→+∞
di/n=zi"
N TR,0.3567477876106195,"tr
 
Z⊤ΛZ −znIn
−1"
N TR,0.3573008849557522,"(this is because of (Bai & Silverstein, 2010, Theorem B.9)). By Lemma 7, s(z) is the unique solution
in C+ to the equation"
N TR,0.35785398230088494,s(z) = sA
N TR,0.3584070796460177,"
z −y
Z τdH(τ)"
N TR,0.35896017699115046,1 + τs
N TR,0.35951327433628316,"
= −
1
z −P"
N TR,0.3600663716814159,"i∈[m]
λizi
1+λis(z)
,"
N TR,0.3606194690265487,which gives s(z) 
N TR,0.36117256637168144,"z −
X i∈[m]"
N TR,0.36172566371681414,"λizi
1 + λis(z) "
N TR,0.3622787610619469,= −1 .
N TR,0.36283185840707965,We want to prove Equation (26) ﬁrst. The lefthand side of Equation (26) equals
N TR,0.3633849557522124,"lim
n,di→+∞
di/n=zi"
N TR,0.3639380530973451,"tr
 
λnIn + Z⊤ΛZ
−1 = s(−λ) ."
N TR,0.3644911504424779,Because the matrix 1
N TR,0.36504424778761063,"nZ⊤ΛZ is positive semideﬁnite and thereby all of its eigenvalues are non-
negative, its limiting spectral distribution is supported on [0, ∞). The Stieltjes transform s(z) of
the limiting spectral distribution can be continuously extended to (−∞, 0). Therefore, for ∀λ > 0,
s(−λ) is the unique solution to the following equation s(−λ)  λ + m
X i=1"
N TR,0.3655973451327434,"λizi
1 + λis(−λ) !"
N TR,0.3661504424778761,"= 1 .
(28)"
N TR,0.36670353982300885,We will verify that
N TR,0.3672566371681416,"d
dλ inf
ρ∈Rm
+  log  λ + m
X"
N TR,0.3678097345132743,"j=1
λjρj  + m
X j=1"
N TR,0.36836283185840707,"
ρj −zj(log ρj"
N TR,0.3689159292035398,"zj
+ 1)
 "
N TR,0.3694690265486726,satisﬁes Equation (28). Take a minimizer ρ∗of Equation (5). Using the envelope theorem yields
N TR,0.3700221238938053,"d
dλ inf
ρ∈Rm
+  log  λ + m
X"
N TR,0.37057522123893805,"j=1
λjρj  + m
X j=1"
N TR,0.3711283185840708,"
ρj −zj(log ρj"
N TR,0.37168141592920356,"zj
+ 1)
"
N TR,0.37223451327433627,"=
1
λ + Pm
j=1 λjρ∗
j
.
(29)"
N TR,0.372787610619469,"Plugging the righthand side of Equation (29) into Equation (28), we get"
N TR,0.3733407079646018,"1
λ + Pm
j=1 λjρ∗
j  λ + m
X i=1"
N TR,0.37389380530973454,"λizi
1 + λi ·
1
λ+Pm
j=1 λjρ∗
j ! = 1 ."
N TR,0.37444690265486724,"Rewriting the above equation yields m
X i=1"
N TR,0.375,"λizi
1 + λi ·
1
λ+Pm
j=1 λjρ∗
j
= m
X"
N TR,0.37555309734513276,"i=1
λiρ∗
i ."
N TR,0.37610619469026546,"It sufﬁces to show that each summand on the lefthand side equals its counterpart on the righthand
side
λizi
1 + λi ·
1
λ+Pm
j=1 λjρ∗
j
= λiρ∗
i ."
N TR,0.3766592920353982,Under review as a conference paper at ICLR 2022
N TR,0.377212389380531,"We need to show
zi
ρ∗
i
= 1 + λi ·
1
λ + Pm
j=1 λjρ∗
j
,"
N TR,0.37776548672566373,which is equivalent to Equation (6) and therefore holds. Hence we have proved Equation (26).
N TR,0.37831858407079644,"Proof of Equation (27). We use α to denote the indices n, di. Deﬁne"
N TR,0.3788716814159292,"h(λ) = inf
ρ∈Rm
+  log  λ + m
X"
N TR,0.37942477876106195,"j=1
λjρj  + m
X j=1"
N TR,0.3799778761061947,"
ρj −zj(log ρj"
N TR,0.3805309734513274,"zj
+ 1)
 ."
N TR,0.3810840707964602,"First, we want to show that limλ0→+∞(h(λ0) −log λ0) = 0. Deﬁne"
N TR,0.38163716814159293,lλ0(ρ) = log 
N TR,0.3821902654867257,"1 + 1 λ0 m
X"
N TR,0.3827433628318584,"j=1
λjρj  + m
X j=1"
N TR,0.38329646017699115,"
ρj −zj"
N TR,0.3838495575221239,"
log ρj"
N TR,0.3844026548672566,"zj
+ 1

,"
N TR,0.38495575221238937,"q(ρ) = m
X j=1"
N TR,0.3855088495575221,"
ρj −zj"
N TR,0.3860619469026549,"
log ρj"
N TR,0.3866150442477876,"zj
+ 1

."
N TR,0.38716814159292035,"The Hessian matrix of q(p) is diag

z1
ρ2
1 , . . . , zm ρ2m"
N TR,0.3877212389380531,"
, which is positive deﬁnite since zi, ρi > 0. There-"
N TR,0.38827433628318586,"fore, q(p) is convex and the minimum of q(ρ) on Rm
+ is attained at ρ = z, where z = (z1, . . . , zm)⊤.
The minimum is infρ∈Rm
+ q(ρ) = q(z) = 0.
Because lim∥ρ∥2→+∞lλ0(ρ) = +∞, there ex-
ists a universal constant K1 > ∥z∥2 > 0 such that lλ0(ρ) > lλ0(z) for all ∥ρ∥2 > K1. De-
ﬁne E =

ρ ∈Rm
+ | ∥ρ∥2 ≤K1
	
.
We have z ∈E , infρ∈E lλ0(ρ) = infρ∈Rm
+ lλ0(ρ), and
infρ∈E q(ρ) = infρ∈Rm
+ q(ρ) = 0. Therefore, we get"
N TR,0.38882743362831856,"h(λ0) −log λ0 = inf
ρ∈Rm
+
lλ0(ρ) = inf
ρ∈E lλ0(ρ) −inf
ρ∈E q(ρ) .
(30)"
N TR,0.3893805309734513,"On E, there exists a universal constant K2 > 0 such that P"
N TR,0.3899336283185841,"j∈[m] λjρj < K2. Thus on E, we
deduce"
N TR,0.39048672566371684,0 < lλ0(ρ) −q(ρ) = log 
N TR,0.39103982300884954,"1 + 1 λ0 m
X"
N TR,0.3915929203539823,"j=1
λjρj "
N TR,0.39214601769911506,"< log

1 + K2 λ0 
."
N TR,0.3926991150442478,"The right-hand side log

1 + K2 λ0"
N TR,0.3932522123893805,"
→0 as λ0 →+∞. Thus limλ0→+∞(lλ0(ρ) −q(ρ)) = 0
uniformly for ρ ∈E. By Lemma 8, we get"
N TR,0.3938053097345133,"lim
λ0→+∞"
N TR,0.39435840707964603,"
inf
ρ∈E lλ0(ρ) −inf
ρ∈E q(ρ)

= 0 ."
N TR,0.39491150442477874,Recalling Equation (30) yields
N TR,0.3954646017699115,"lim
λ0→+∞(h(λ0) −log λ0) = 0 .
(31)"
N TR,0.39601769911504425,"Deﬁne fα(λ) =
1
n log det N"
N TR,0.396570796460177,"n . Second, we want to show limα fα(λ) = h(λ), where limα means
limn,di→+∞
di/n=zi
. We have fα(λ) −fα(λ0) =
R λ
λ0 f ′
α(x)dx for ∀λ, λ0 > 0. It follows that"
N TR,0.3971238938053097,"|fα(λ) −h(λ)| ≤|fα(λ) −h(λ) + h(λ0) −fα(λ0) + fα(λ0) −log λ0 + log λ0 −h(λ0)|
≤|fα(λ) −h(λ) + h(λ0) −fα(λ0)| + |fα(λ0) −log λ0| + |log λ0 −h(λ0)| =  Z λ"
N TR,0.39767699115044247,"λ0
f ′
α(x)dx −(h(λ) −h(λ0))"
N TR,0.39823008849557523,+ |fα(λ0) −log λ0| + |log λ0 −h(λ0)| .
N TR,0.398783185840708,Taking lim supα on both sides gives
N TR,0.3993362831858407,"lim sup
α
|fα(λ) −h(λ)| ≤lim sup
α  Z λ"
N TR,0.39988938053097345,"λ0
f ′
α(x)dx −(h(λ) −h(λ0))"
N TR,0.4004424778761062,"+lim sup
α
|fα(λ0) −log λ0|+|log λ0 −h(λ0)| . (32)"
N TR,0.40099557522123896,Under review as a conference paper at ICLR 2022
N TR,0.40154867256637167,"Recall f ′
α(λ) = tr N −1 and limα f ′
α(λ) = h′(λ) (this is exactly Equation (26)).
Because
tr N −1 = tr N −1 ≤1"
N TR,0.4021017699115044,"λ and
R λ
λ0
1
xdx < +∞, by the dominated convergence theorem, we have lim
α Z λ"
N TR,0.4026548672566372,"λ0
f ′
α(x)dx =
Z λ"
N TR,0.4032079646017699,"λ0
h′(x)dx = h(λ) −h(λ0) ."
N TR,0.40376106194690264,It follows that
N TR,0.4043141592920354,"lim sup
α  Z λ"
N TR,0.40486725663716816,"λ0
f ′
α(x)dx −(h(λ) −h(λ0))"
N TR,0.40542035398230086,"= lim
α  Z λ"
N TR,0.4059734513274336,"λ0
f ′
α(x)dx −(h(λ) −h(λ0))"
N TR,0.4065265486725664,"= 0 .
(33) Since"
N TR,0.40707964601769914,fα(λ0)−log λ0 = 1
N TR,0.40763274336283184,"n log det

λ0In + 1"
N TR,0.4081858407079646,"nZ⊤ΛZ

−1"
N TR,0.40873893805309736,n log det (λ0In) = 1
N TR,0.4092920353982301,"n log det

In +
1
nλ0
Z⊤ΛZ
"
N TR,0.4098451327433628,"and the matrix
1
nλ0 Z⊤ΛZ is positive semideﬁnite, we have"
N TR,0.4103982300884956,fα(λ0) −log λ0 ≥0 .
N TR,0.41095132743362833,We have
N TR,0.41150442477876104,fα(λ0) −log λ0 = 1
N TR,0.4120575221238938,"n log det

In +
1
nλ0
Z⊤ΛZ
 ≤1"
N TR,0.41261061946902655,"n log det

In + λ+"
N TR,0.4131637168141593,"nλ0
Z⊤Z
"
N TR,0.413716814159292,"≤log

1 + λ+"
N TR,0.41426991150442477,"λ0
λmax Z⊤Z n  ≤λ+"
N TR,0.41482300884955753,"λ0
λmax Z⊤Z n 
."
N TR,0.4153761061946903,"Then taking lim supα, we get"
N TR,0.415929203539823,"lim sup
α
|fα(λ0) −log λ0| = lim sup
α
(fα(λ0) −log λ0) ≤λ+"
N TR,0.41648230088495575,"λ0
lim sup
α
λmax Z⊤Z n 
≲1 λ0
, (34)"
N TR,0.4170353982300885,"where the last inequality is because lim supα λmax

Z⊤Z"
N TR,0.41758849557522126,"n

=

1 +
q γ ∨1"
N TR,0.41814159292035397,"γ
2
≍1 by Lemma 1.
Using Equation (32), Equation (33) and Equation (34) gives"
N TR,0.4186946902654867,"lim sup
α
|fα(λ) −h(λ)| ≲1"
N TR,0.4192477876106195,"λ0
+ |log λ0 −h(λ0)| ."
N TR,0.4198008849557522,Then taking limλ0→+∞and recalling Equation (31) yields
N TR,0.42035398230088494,"lim
α |fα(λ) −h(λ)| = lim sup
α
|fα(λ) −h(λ)| = 0 ."
N TR,0.4209070796460177,"Therefore, we conclude limα fα(λ) = h(λ)."
N TR,0.42146017699115046,Lemma 10. Deﬁne N = λnIn +Z⊤ΛZ = λnIn +P
N TR,0.42201327433628316,"i∈[m] λiZiZ⊤
i . The following equation holds"
N TR,0.4225663716814159,"lim
n,di→+∞
di/n→zi E
 ∂ ∂λi"
N LOG DET N,0.4231194690265487,"1
n log det N n"
N LOG DET N,0.42367256637168144,"
=
∂
∂λi
inf
ρ∈Rm
+  log "
N LOG DET N,0.42422566371681414,"λ +
X"
N LOG DET N,0.4247787610619469,"i∈[m]
λiρi  +
X i∈[m]"
N LOG DET N,0.42533185840707965,"
ρi −zi"
N LOG DET N,0.4258849557522124,"
log ρi"
N LOG DET N,0.4264380530973451,"zi
+ 1
 , (35)"
N LOG DET N,0.4269911504424779,"lim
n,di→+∞
di/n→zi"
N LOG DET N,0.42754424778761063,"E

∂2"
N LOG DET N,0.4280973451327434,∂λj∂λi
N LOG DET N,0.4286504424778761,"1
n log det N n"
N LOG DET N,0.42920353982300885,"
=
∂2"
N LOG DET N,0.4297566371681416,"∂λj∂λi
inf
ρ∈Rm
+  log "
N LOG DET N,0.4303097345132743,"λ +
X"
N LOG DET N,0.43086283185840707,"i∈[m]
λiρi  +
X i∈[m]"
N LOG DET N,0.4314159292035398,"
ρi −zi"
N LOG DET N,0.4319690265486726,"
log ρi"
N LOG DET N,0.4325221238938053,"zi
+ 1
 . (36)"
N LOG DET N,0.43307522123893805,Under review as a conference paper at ICLR 2022
N LOG DET N,0.4336283185840708,"Proof. Proof of Equation (35). We use α to denote the indices n, di and use limα to denote
limn,di→+∞
di/n=zi
. Deﬁne fα(λi) = E
 1"
N LOG DET N,0.43418141592920356,n log det N
N LOG DET N,0.43473451327433627,"n

, f ′
α(λi) =
∂
∂λi E
 1"
N LOG DET N,0.435287610619469,n log det N
N LOG DET N,0.4358407079646018,"n

, and"
N LOG DET N,0.43639380530973454,"h(λi) = inf
ρ∈Rm
+  log "
N LOG DET N,0.43694690265486724,"λ +
X"
N LOG DET N,0.4375,"i∈[m]
λiρi  +
X i∈[m]"
N LOG DET N,0.43805309734513276,"
ρi −zi"
N LOG DET N,0.43860619469026546,"
log ρi"
N LOG DET N,0.4391592920353982,"zi
+ 1
 ."
N LOG DET N,0.439712389380531,"We have

1
n log det N n ≤1"
N LOG DET N,0.44026548672566373,"n log det

λIn + λ+
Z⊤Z n"
N LOG DET N,0.44081858407079644,"
= log λ + 1"
N LOG DET N,0.4413716814159292,"n log det

In + λ+"
N LOG DET N,0.44192477876106195,"nλ Z⊤Z

."
N LOG DET N,0.4424778761061947,"By Lemma 3, there exists a universal constant C > 0 such that for all sufﬁciently large n,"
N LOG DET,0.4430309734513274,"1
n log det

In + λ+"
N LOG DET,0.4435840707964602,"nλ Z⊤Z

≤log

1 + C λ 
."
N LOG DET,0.44413716814159293,"Therefore, we get

1
n log det N n"
N LOG DET,0.4446902654867257,≤log (λ + C) .
N LOG DET,0.4452433628318584,"By the dominated convergence theorem and Lemma 9 (speciﬁcally, Equation (27)), we obtain"
N LOG DET,0.44579646017699115,"lim
α fα(λi) = h(λi) .
(37)"
N LOG DET,0.4463495575221239,"Because

∂
∂λi"
N LOG DET N,0.4469026548672566,"1
n log det N n = 1"
N LOG DET N,0.44745575221238937,"n tr
 
Z⊤
i N −1Zi

≤
1
λn2 tr
 
Z⊤
i Zi
"
N LOG DET N,0.4480088495575221,"and E

1
λn2 tr
 
Z⊤
i Zi

< +∞, we can interchange the differentiation and the expectation and get"
N LOG DET N,0.4485619469026549,"f ′
α(λi) =
∂
∂λi
E
 1"
N LOG DET N,0.4491150442477876,n log det N n
N LOG DET N,0.44966814159292035,"
= E
 ∂ ∂λi"
N LOG DET N,0.4502212389380531,"1
n log det N n"
N LOG DET N,0.45077433628318586,"
.
(38)"
N LOG DET N,0.45132743362831856,"Thus we deduce

∂
∂λi
E
 1"
N LOG DET N,0.4518805309734513,n log det N n
N LOG DET N,0.4524336283185841," =
E
 ∂ ∂λi"
N LOG DET N,0.45298672566371684,"1
n log det N n"
N LOG DET N,0.45353982300884954," ≤E

∂
∂λi"
N LOG DET N,0.4540929203539823,"1
n log det N n"
N LOG DET N,0.45464601769911506,"≤E
 1"
N LOG DET N,0.4551991150442478,"λn2 tr
 
Z⊤
i Zi

."
N LOG DET N,0.4557522123893805,"By Lemma 4, E tr
 
Z⊤
i Zi

≍n2 and therefore E

1
λn2 tr
 
Z⊤
i Zi

≲
1
λ. The function sequence
{f ′
α} is uniformly bounded."
N LOG DET N,0.4563053097345133,"Then we want to show that {f ′
α} is uniformly equicontinuous by showing that {f ′′
α} is uniformly
bounded. Because

∂2 ∂λ2
i"
N LOG DET N,0.45685840707964603,"1
n log det N n = 1"
N LOG DET N,0.45741150442477874,"n tr
 
Z⊤
i N −1Zi
2 ≤
1
nλ2 tr
Z⊤
i Zi n 2"
N LOG DET N,0.4579646017699115,"and E

1
nλ2 tr

Z⊤
i Zi"
N LOG DET N,0.45851769911504425,"n
2
< +∞, we can interchange the differentiation and the expectation and"
N LOG DET N,0.459070796460177,"get
∂2"
N LOG DET N,0.4596238938053097,"∂λ2
i
E
 1"
N LOG DET N,0.46017699115044247,n log det N n
N LOG DET N,0.46073008849557523,"
=
∂
∂λi
E
 ∂ ∂λi"
N LOG DET N,0.461283185840708,"1
n log det N n"
N LOG DET N,0.4618362831858407,"
= E
 ∂2 ∂λ2
i"
N LOG DET N,0.46238938053097345,"1
n log det N n 
."
N LOG DET N,0.4629424778761062,"Therefore, we deduce

∂2"
N LOG DET N,0.46349557522123896,"∂λ2
i
E
 1"
N LOG DET N,0.46404867256637167,n log det N n
N LOG DET N,0.4646017699115044," ≤E

∂2 ∂λ2
i"
N LOG DET N,0.4651548672566372,"1
n log det N n"
N LOG DET N,0.4657079646017699,"≤
1
nλ2 E tr
Z⊤
i Zi n 2
."
N LOG DET N,0.46626106194690264,"Again, by Lemma 4, tr

Z⊤
i Zi"
N LOG DET N,0.4668141592920354,"n
2
≍n. It follows that
1
nλ2 E tr

Z⊤
i Zi"
N LOG DET N,0.46736725663716816,"n
2
≲
1
λ2 . Therefore {f ′
α} is
uniformly equicontinuous."
N LOG DET N,0.46792035398230086,"We want to show limα f ′
α(λi) = h′(λi) by contradiction. If it is not true, there exists ϵ > 0 and
a subsequence {f ′
αk} such that
f ′
αk(λi) −h′(λi)
 ≥ϵ. Let E = [a, b] ∋λi (b > a > 0) be"
N LOG DET N,0.4684734513274336,Under review as a conference paper at ICLR 2022
N LOG DET N,0.4690265486725664,"a closed interval that contains λi. The subsequence {f ′
αk} is uniformly bounded and uniformly"
N LOG DET N,0.46957964601769914,"equicontinuous. By the Arzela-Ascoli theorem, there exists a subsequence
n
f ′
αkj"
N LOG DET N,0.47013274336283184,"o
that converges"
N LOG DET N,0.4706858407079646,"uniformly on λi ∈E. Recall limα fα(λi) = h(λi) (Equation (37)). Thus limj fαkj (λi) = h(λi).
By (Rudin, 1976, Theorem 7.17), for λi ∈E, we have"
N LOG DET N,0.47123893805309736,"lim
j f ′
αkj (λi) = h′(λi) ."
N LOG DET N,0.4717920353982301,"This is a contradiction. Hence, we have shown that limα f ′
α(λi) = h′(λi), which is exactly Equa-"
N LOG DET N,0.4723451327433628,"tion (35) (recall f ′
α(λi) =
∂
∂λi E
 1"
N LOG DET N,0.4728982300884956,n log det N
N LOG DET N,0.47345132743362833,"n

= E
h
∂
∂λi
1
n log det N"
N LOG DET N,0.47400442477876104,"n
i
in Equation (38))."
N LOG DET N,0.4745575221238938,"Proof of Equation (36). Deﬁne gα(λj) =
∂
∂λi E
 1"
N LOG DET N,0.47511061946902655,n log det N
N LOG DET N,0.4756637168141593,"n

= E
h
∂
∂λi
1
n log det N"
N LOG DET N,0.476216814159292,"n
i
. Then"
N LOG DET N,0.47676991150442477,"g′
α(λj) =
∂2
∂λj∂λi E
 1"
N LOG DET N,0.47732300884955753,n log det N
N LOG DET N,0.4778761061946903,"n

=
∂
∂λj E
h
∂
∂λi
1
n log det N"
N LOG DET N,0.478429203539823,"n
i
. We have ∂2"
N LOG DET N,0.47898230088495575,∂λj∂λi
N LOG DET N,0.4795353982300885,"1
n log det N n  = 1"
N LOG DET N,0.48008849557522126,"n tr
 
ZiZ⊤
i N −1ZjZ⊤
j N −1 = 1"
N LOG DET N,0.48064159292035397,"n tr
 
Z⊤
i N −1ZjZ⊤
j N −1Zi
 = 1 n"
N LOG DET N,0.4811946902654867,"Z⊤
j N −1Zi
2 F ≤1"
N LOG DET N,0.4817477876106195,"n ∥Zj∥2
2 ∥Zi∥2
2
N −12
F"
N LOG DET N,0.4823008849557522,"≤
1
λ2n2 ∥Zj∥2
2 ∥Zi∥2
2 ."
N LOG DET N,0.48285398230088494,"where the last inequality is because
N −12
F ≤
 1"
N LOG DET N,0.4834070796460177,"λnIn
2
F =
1
λ2n. If i ̸= j, by Lemma 4, we have"
N LOG DET N,0.48396017699115046,"1
λ2n2 E ∥Zj∥2
2 ∥Zi∥2
2 =
1
λ2n2 E ∥Zj∥2
2 · E ∥Zi∥2
2 ≲1 λ2 ."
N LOG DET N,0.48451327433628316,"If i = j, by Lemma 4, we have"
N LOG DET N,0.4850663716814159,"1
λ2n2 E ∥Zi∥4
2 ≲
1
λ2n2 · n2 = 1 λ2 ."
N LOG DET N,0.4856194690265487,"As a result, we get ∂2"
N LOG DET N,0.48617256637168144,∂λj∂λi
N LOG DET N,0.48672566371681414,"1
n log det N n ≲1"
N LOG DET N,0.4872787610619469,"n · n2 ·
1
λ2n = 1 λ2 ."
N LOG DET N,0.48783185840707965,"Thus we can interexchange
∂
∂λj and expectation, and get g′
α(λj) = E
h
∂2
∂λj∂λi
1
n log det N"
N LOG DET N,0.4883849557522124,"n
i
. Be-"
N LOG DET N,0.4889380530973451,"cause |g′
α(λj)| ≤E

∂2
∂λj∂λi
1
n log det N"
N LOG DET N,0.4894911504424779,"n
 ≲
1
λ2 , the function sequence {g′
α} is uniformly bounded
for λj."
N LOG DET N,0.49004424778761063,Under review as a conference paper at ICLR 2022
N LOG DET N,0.4905973451327434,"Deﬁne L = Z⊤
j N −1Zi and W = Z⊤
j N −1Zj. We have

∂3"
N LOG DET N,0.4911504424778761,"∂λ2
j∂λi"
N LOG DET N,0.49170353982300885,"1
n log det N n  = 2"
N LOG DET N,0.4922566371681416,"n tr
 
L⊤WL
 ≲1"
N LOG DET N,0.4928097345132743,"λn2 tr
 
L⊤Z⊤
j ZjL
 = 1"
N LOG DET N,0.49336283185840707,"λn2 tr

Z⊤
i N −1  
ZjZ⊤
j
2 N −1Zi

. = 1"
N LOG DET N,0.4939159292035398,"λn2
ZjZ⊤
j N −1Zi
2 F ≤1"
N LOG DET N,0.4944690265486726,"λn2
N −12
F
ZjZ⊤
j
2"
N LOG DET N,0.4950221238938053,"2 ∥Zi∥2
2"
N LOG DET N,0.49557522123893805,"≤
1
λ3n3
ZjZ⊤
j
2"
N LOG DET N,0.4961283185840708,"2 ∥Zi∥2
2"
N LOG DET N,0.49668141592920356,"=
1
λ3n3 ∥Zj∥4
2 ∥Zi∥2
2 ,"
N LOG DET N,0.49723451327433627,"where the ﬁrst inequality is because W ≼
1
λnZ⊤
j Zj and the third inequality is because N −1 ≼
1
λnIn
and then
N −12
F ≤
 1"
N LOG DET N,0.497787610619469,"λnIn
2
F ≤
1
λ2n. By Lemma 4, we have E ∥Zj∥4
2 ≲n2 and E ∥Zi∥2
2 ≲n. If
i ̸= j, then Zj and Zi are independent, and we deduce"
N LOG DET N,0.4983407079646018,"1
λ3n3 E ∥Zj∥4
2 ∥Zi∥2
2 ≲1 λ3 ."
N LOG DET N,0.49889380530973454,"If i = j, we have
1
λ3n3 E ∥Zi∥4
2 ∥Zi∥2
2 =
1
λ3n3 E ∥Zi∥6
2 ≲1 λ3 ."
N LOG DET N,0.49944690265486724,"As a result, we deduce E
h
∂3"
N LOG DET N,0.5,"∂λ2
j∂λi
1
n log det N"
N LOG DET N,0.5005530973451328,"n
i
=
∂
∂λj E
h
∂2
∂λj∂λi
1
n log det N"
N LOG DET N,0.5011061946902655,"n
i
= g′′
α(λj). Moreover,
we have"
N LOG DET N,0.5016592920353983,"|g′′
α(λj)| ≤E ∂3"
N LOG DET N,0.5022123893805309,"∂λ2
j∂λi"
N LOG DET N,0.5027654867256637,"1
n log det N n ≲1 λ3 ."
N LOG DET N,0.5033185840707964,"Therefore {g′
α} is uniformly equicontinuous. Deﬁne"
N LOG DET N,0.5038716814159292,"w(λj) =
∂
∂λi
inf
ρ∈Rm
+  log "
N LOG DET N,0.504424778761062,"λ +
X"
N LOG DET N,0.5049778761061947,"i∈[m]
λiρi  +
X i∈[m]"
N LOG DET N,0.5055309734513275,"
ρi −zi"
N LOG DET N,0.5060840707964602,"
log ρi"
N LOG DET N,0.5066371681415929,"zi
+ 1
 ."
N LOG DET N,0.5071902654867256,"We want to show by contradiction that limα g′
α(λj) = w′(λj). Assume that it is not true. Then
there exists ϵ > 0 and a subsequence

g′
αk
	
such that
g′
αk(λj) −w′(λj)
 > ϵ. Since

g′
αk
	
is
uniformly bounded and uniformly equicontinuous, by the Arzela-Ascoli theorem, there is a subse-
quence
n
g′
αkr"
N LOG DET N,0.5077433628318584,"o
that converges uniformly on a closed interval E containing λj. Equation (35) shows"
N LOG DET N,0.5082964601769911,"that limα gα(λj) = w(λj). It follows that limr gαkr (λj) = w(λj). By (Rudin, 1976, Theorem
7.17), for λi ∈E, we have"
N LOG DET N,0.5088495575221239,"lim
r g′
αkr (λj) = w′(λj) ,"
N LOG DET N,0.5094026548672567,"which is a contradiction. Therefore, we have shown that limα g′
α(λj) = w′(λj), which is exactly
Equation (36)."
N LOG DET N,0.5099557522123894,Under review as a conference paper at ICLR 2022
N LOG DET N,0.5105088495575221,"D
PROOF OF THEOREM 1"
N LOG DET N,0.5110619469026548,"D.1
PROOF OF ITEM 1"
N LOG DET N,0.5116150442477876,"Deﬁne
g(ρ)
=
log

λ + Pm
j=1 λjρj

+ Pm
j=1

ρj −zj(log ρj"
N LOG DET N,0.5121681415929203,"zj + 1)

.
The
func-"
N LOG DET N,0.5127212389380531,"tion g(ρ) is continuously differentiable on Rm
+.
The boundary of R+
m
is ∂Rm
+
=
{ρ ∈Rm | (∀i ∈[m], ρi ≥0) ∧(∃i ∈[m], ρi = 0)}.
Because
limRm
+ ∋ρ→ρ0∈∂Rm
+ g(ρ)
=
limRm
+ ∋ρ→∞g(ρ) = +∞, there exists a minimizer ρ∗∈Rm
+ of g(ρ)."
N LOG DET N,0.5132743362831859,"Taking the derivative with respect to ρi gives ∂
∂ρi  log  λ + m
X"
N LOG DET N,0.5138274336283186,"j=1
λjρj  + m
X j=1"
N LOG DET N,0.5143805309734514,"
ρj −zj(log ρj"
N LOG DET N,0.514933628318584,"zj
+ 1)
"
N LOG DET N,0.5154867256637168,"=
λi
λ + Pm
j=1 λjρj
+ 1 −zi ρi
."
N LOG DET N,0.5160398230088495,Setting it to zero gives Equation (6).
N LOG DET N,0.5165929203539823,"D.2
PROOF OF ITEM 2"
N LOG DET N,0.5171460176991151,"Recall Equation (6)
λi
λ + Pm
j=1 λjρ∗
j
+ 1 −zi"
N LOG DET N,0.5176991150442478,"ρ∗
i
= 0 ,
∀i ∈[m] ."
N LOG DET N,0.5182522123893806,Rewriting the above equation gives
N LOG DET N,0.5188053097345132,"(zi −ρ∗
i )  λ + m
X"
N LOG DET N,0.519358407079646,"k=1
λkρ∗
k !"
N LOG DET N,0.5199115044247787,"= λiρ∗
i ,
∀i ∈[m] ."
N LOG DET N,0.5204646017699115,Rewriting it in the linear algebraic form yields
N LOG DET N,0.5210176991150443,"(z −ρ∗)
 
λ + λ⊤ρ∗
= λ ⊙ρ∗."
N LOG DET N,0.521570796460177,"Applying
∂
∂λ to both sides and using the implicit function theorem, we get"
N LOG DET N,0.5221238938053098,"(z −ρ∗)
 
ρ∗⊤+ λ⊤J

−J
 
λ + λ⊤ρ∗
= diag (λ) J + diag (ρ∗) ."
N LOG DET N,0.5226769911504425,"Arranging the above equation yields
 
diag (λ) +
 
λ + λ⊤ρ∗
Im −(z −ρ∗) λ⊤
J = (z −ρ∗) ρ∗⊤−diag (ρ∗) ."
N LOG DET N,0.5232300884955752,"Deﬁne a = λ + λ⊤ρ∗, A = diag (λ) +
 
λ + λ⊤ρ∗
Im = diag (λ) + aIm and B = diag (λ) +
 
λ + λ⊤ρ∗
Im −(z −ρ∗) λ⊤= A −(z −ρ∗) λ⊤. The matrix determinant lemma gives"
N LOG DET N,0.5237831858407079,"det (B) =
 
1 −λ⊤A−1 (z −ρ∗)

det (A) ."
N LOG DET N,0.5243362831858407,Recall Equation (6) again and we have
N LOG DET N,0.5248893805309734,λi + a = zia
N LOG DET N,0.5254424778761062,"ρ∗
i
."
N LOG DET N,0.525995575221239,"We have a −
X"
N LOG DET N,0.5265486725663717,"i∈[m]
λiρ∗
i (1 −ρ∗
i /zi) = "
N LOG DET N,0.5271017699115044,"λ +
X"
N LOG DET N,0.5276548672566371,"i∈[m]
λiρ∗
i  −
X"
N LOG DET N,0.5282079646017699,"i∈[m]
λiρ∗
i (1 −ρ∗
i /zi)"
N LOG DET N,0.5287610619469026,"=λ +
X"
N LOG DET N,0.5293141592920354,"i∈[m]
λi
(ρ∗
i )2"
N LOG DET N,0.5298672566371682,"zi
> 0 ."
N LOG DET N,0.5304203539823009,"It follows that
P
i∈[m] λiρ∗
i (1 −ρ∗
i /zi)"
N LOG DET N,0.5309734513274337,"a
< 1 ."
N LOG DET N,0.5315265486725663,Under review as a conference paper at ICLR 2022
N LOG DET N,0.5320796460176991,Then we compute λ⊤A−1 (z −ρ∗):
N LOG DET N,0.5326327433628318,"λ⊤A−1 (z −ρ∗) =
X i∈[m]"
N LOG DET N,0.5331858407079646,"λi (zi −ρ∗
i )
λi + a =
X i∈[m]"
N LOG DET N,0.5337389380530974,"λi (zi −ρ∗
i )
zia ρ∗
i = P"
N LOG DET N,0.5342920353982301,"i∈[m] λiρ∗
i (1 −ρ∗
i /zi)"
N LOG DET N,0.5348451327433629,"a
< 1 ."
N LOG DET N,0.5353982300884956,"Thus we get 1 −λ⊤A−1 (z −ρ∗) > 0. Therefore, det B ̸= 0 and the matrix B is invertible."
N LOG DET N,0.5359513274336283,"D.3
PROOF OF ITEM 3"
N LOG DET N,0.536504424778761,"Lemma 11. Deﬁne N = λnIn + Z⊤ΛZ, γ = P"
N LOG DET N,0.5370575221238938,"i∈[m] zi, r = (r1, . . . , rm), λ = (λ1, . . . , λm),
and"
N LOG DET N,0.5376106194690266,"ϑ(rt, r, λ) = 2rt
s 1 +
X"
N LOG DET N,0.5381637168141593,"i∈[m]
r2
i −2rt
X i∈[m]"
N LOG DET N,0.5387168141592921,"√ziri +
X i∈[m]"
N LOG DET N,0.5392699115044248,"1
λi
r2
i −λr2
t ."
N LOG DET N,0.5398230088495575,For any Kt ≥2
N LOG DET N,0.5403761061946902,"λ and Ku ≥
2λ+(2+√γ)"
N LOG DET N,0.540929203539823,"λ
, we have"
N LOG DET N,0.5414823008849557,"lim
n,di→+∞
di/n→zi"
N LOG DET N,0.5420353982300885,"tr N −1 =
lim
n,di→+∞
di/n→zi"
N LOG DET N,0.5425884955752213,E tr N −1
N LOG DET N,0.543141592920354,"=
max
0≤rt≤Kt
min
0≤ri≤Ku ϑ(rt, r, λ) =
min
0≤ri≤Ku
max
0≤rt≤Kt ϑ(rt, r, λ) = max
rt≥0 min
ri≥0 ϑ(rt, r, λ) = min
ri≥0 max
rt≥0 ϑ(rt, r, λ) . (39)"
N LOG DET N,0.5436946902654868,"If r∗is a solution to the optimization problem in Equation (39), then 1 + m
X"
N LOG DET N,0.5442477876106194,"j=1
r∗2
j
=  
m
X"
N LOG DET N,0.5448008849557522,"j=1
r∗
j
√zj + λr∗
t   2"
N LOG DET N,0.5453539823008849,",
(40)"
N LOG DET N,0.5459070796460177,"r∗
t
r∗
i
q"
N LOG DET N,0.5464601769911505,"1 + Pm
j=1 r∗2
j
= r∗
t
√zi −r∗
i
λi
.
(41)"
N LOG DET N,0.5470132743362832,"Moreover, we have
∂
∂λi
max
rt≥0 min
ri≥0 ϑ(rt, r, λ) = −r∗2
i
λ2
i
."
N LOG DET N,0.547566371681416,"Proof. Let g ∼N(0, In) be a multivariate standard normal random vector. We have"
N LOG DET N,0.5481194690265486,tr N −1
N LOG DET N,0.5486725663716814,=Egg⊤N −1g
N LOG DET N,0.5492256637168141,"=Eg sup
t∈Rn
 
2g⊤t −t⊤Nt
"
N LOG DET N,0.5497787610619469,"=Eg sup
t∈Rn"
N LOG DET N,0.5503318584070797,"
2g⊤t −t⊤Z⊤ΛZt −nλ ∥t∥2
2
"
N LOG DET N,0.5508849557522124,"=Eg sup
t∈Rn inf
u∈Rd"
N LOG DET N,0.5514380530973452,"
2g⊤t −2u⊤ΛZt + u⊤Λu −nλ ∥t∥2
2
"
N LOG DET N,0.5519911504424779,"= −2Eg inf
t∈Rn sup
u∈Rd"
N LOG DET N,0.5525442477876106,"
u⊤ΛZt −g⊤t −1"
N LOG DET N,0.5530973451327433,2u⊤Λu + 1
N LOG DET N,0.5536504424778761,"2nλ ∥t∥2
2 "
N LOG DET N,0.5542035398230089,"= −2Eg inf
t∈Rn sup
u∈Rd"
N LOG DET N,0.5547566371681416,"
u⊤Zt −g⊤t −1"
N LOG DET N,0.5553097345132744,2u⊤Λ−1u + 1
N LOG DET N,0.5558628318584071,"2nλ ∥t∥2
2 "
N LOG DET N,0.5564159292035398,Under review as a conference paper at ICLR 2022
N LOG DET N,0.5569690265486725,We view
N LOG DET N,0.5575221238938053,"inf
t∈Rn sup
u∈Rd"
N LOG DET N,0.558075221238938,"
u⊤Zt −g⊤t −1"
N LOG DET N,0.5586283185840708,2u⊤Λ−1u + 1
N LOG DET N,0.5591814159292036,"2nλ ∥t∥2
2"
N LOG DET N,0.5597345132743363,"
(42)"
N LOG DET N,0.5602876106194691,"as the primal optimization (PO) problem in the convex Gaussian min-max theorem (CGMT) (Thram-
poulidis et al., 2015)."
N LOG DET N,0.5608407079646017,The KKT conditions for Equation (42) give
N LOG DET N,0.5613938053097345,"Z⊤u −g + nλt = 0 ,"
N LOG DET N,0.5619469026548672,Zt −Λ−1u = 0 .
N LOG DET N,0.5625,Solving the above equations gives
N LOG DET N,0.5630530973451328,"t = N −1g ,
u = ΛZN −1g ."
N LOG DET N,0.5636061946902655,"With probability at least 1 −4 exp(−cn) (c > 0 is a universal constant), we have ∥g∥2 ≤2√n and
∥Z∥≤
√"
N LOG DET N,0.5641592920353983,"d + 2√n ≤
 
2 + √γ
 √n. Therefore, we get"
N LOG DET N,0.5647123893805309,"∥t∥2 ≤
N −1 ∥g∥2 ≤1"
N LOG DET N,0.5652654867256637,"λn · 2√n =
2
λ√n ,"
N LOG DET N,0.5658185840707964,"∥u∥2 ≤λ+ ∥Z∥∥t∥2 ≤λ+ (2 + √γ) √n ·
2
λ√n = 2λ+
 
2 + √γ
 λ
."
N LOG DET N,0.5663716814159292,"Write u =  
"
N LOG DET N,0.566924778761062,"u1
...
um "
N LOG DET N,0.5674778761061947,"
, where ui ∈Rdi.
For all Kt ≥
2
λ, Ku ≥
2λ+(2+√γ)"
N LOG DET N,0.5680309734513275,"λ
, the opti-"
N LOG DET N,0.5685840707964602,"mal solutions t∗and u∗to Equation (42) satisfy √n ∥t∗∥2 ≤Kt and ∥ui∥2 ≤Ku for all
i ∈[m] with probability at least 1 −4 exp(−cn). Deﬁne St = {t ∈Rn | √n ∥t∥2 ≤Kt} and
Su =

u ∈Rd | ∥ui∥≤Ku, ∀i ∈[m]
	
. We use α to denote the indices n, di and use limα to
denote limn,di→+∞
di/n=zi
. Deﬁne event"
N LOG DET N,0.5691371681415929,"Eα =

inf
t∈Rn sup
u∈Rd"
N LOG DET N,0.5696902654867256,"
u⊤Zt −g⊤t −1"
N LOG DET N,0.5702433628318584,2u⊤Λ−1u + 1
N LOG DET N,0.5707964601769911,"2nλ ∥t∥2
2"
N LOG DET N,0.5713495575221239,"
= inf
t∈St sup
u∈Su"
N LOG DET N,0.5719026548672567,"
u⊤Zt −g⊤t −1"
N LOG DET N,0.5724557522123894,2u⊤Λ−1u + 1
N LOG DET N,0.5730088495575221,"2nλ ∥t∥2
2 
."
N LOG DET N,0.5735619469026548,"Then with probability at least 1 −4 exp(−cn), we have t∗∈St and u∗∈Su. Therefore the event
Eα occurs with probability at least 1 −4 exp(−cn), which yields"
N LOG DET N,0.5741150442477876,"P {Ec
α} ≤4 exp(−cn) ."
N LOG DET N,0.5746681415929203,Since P
N LOG DET N,0.5752212389380531,"n≥1 4 exp(−cn) < +∞, by Borel-Cantelli lemma, we have"
N LOG DET N,0.5757743362831859,"P

lim sup
α
Ec
α"
N LOG DET N,0.5763274336283186,"
= P
n
lim inf
α
Eα
co
= 0 ."
N LOG DET N,0.5768805309734514,"Then with probability 1, all but ﬁnitely many Eα occur. Then almost surely there exists n0 such that
for all n > n0, Eα occurs."
N LOG DET N,0.577433628318584,The auxiliary optimization (AO) problem is
N LOG DET N,0.5779867256637168,"inf
t∈St sup
u∈Su"
N LOG DET N,0.5785398230088495,"
∥t∥2 g⊤
1 u + ∥u∥2 g⊤
2 t −g⊤t −1"
N LOG DET N,0.5790929203539823,2u⊤Λ−1u + 1
N LOG DET N,0.5796460176991151,"2nλ ∥t∥2
2 "
N LOG DET N,0.5801991150442478,"=
inf
0≤rt≤Kt
sup
0≤ri≤Ku  
−"
N LOG DET N,0.5807522123893806,"g −
qP"
N LOG DET N,0.5813053097345132,"i∈[m] r2
i g2

2
√n
rt + rt
X i∈[m]"
N LOG DET N,0.581858407079646,"∥g1,i∥2
√n
ri −1 2 X i∈[m]"
N LOG DET N,0.5824115044247787,"1
λi
r2
i + 1"
N LOG DET N,0.5829646017699115,"2λr2
t  
"
N LOG DET N,0.5835176991150443,"=
inf
0≤rt≤Kt
sup
0≤ri≤Ku  −
s 1 +
X"
N LOG DET N,0.584070796460177,"i∈[m]
r2
i
∥g3∥2
√n rt + rt
X i∈[m]"
N LOG DET N,0.5846238938053098,"∥g1,i∥2
√n
ri −1 2 X i∈[m]"
N LOG DET N,0.5851769911504425,"1
λi
r2
i + 1"
N LOG DET N,0.5857300884955752,"2λr2
t  ,"
N LOG DET N,0.5862831858407079,"where g1 ∼N(0, Id), g2 ∼N(0, In), and g3 ∼N(0, In)."
N LOG DET N,0.5868362831858407,Under review as a conference paper at ICLR 2022
N LOG DET N,0.5873893805309734,"Taking n, di →+∞with di/n →zi constant, the strong law of large numbers gives
s 1 +
X"
N LOG DET N,0.5879424778761062,"i∈[m]
r2
i
∥g3∥2
√n"
N LOG DET N,0.588495575221239,"a.s.
→
s 1 +
X"
N LOG DET N,0.5890486725663717,"i∈[m]
r2
i ,"
N LOG DET N,0.5896017699115044,"∥g1,j∥2
√n
= r dj"
N LOG DET N,0.5901548672566371,"n
∥g1,j∥2
p dj"
N LOG DET N,0.5907079646017699,"a.s.
→√zj . Deﬁne"
N LOG DET N,0.5912610619469026,"Xα (rt, r) = −
s 1 +
X"
N LOG DET N,0.5918141592920354,"i∈[m]
r2
i
∥g3∥2
√n rt + rt
X i∈[m]"
N LOG DET N,0.5923672566371682,"∥g1,i∥2
√n
ri −1 2 X i∈[m]"
N LOG DET N,0.5929203539823009,"1
λi
r2
i + 1"
N LOG DET N,0.5934734513274337,"2λr2
t ."
N LOG DET N,0.5940265486725663,"It is a stochastic process on (rt, r) ∈[0, Kt] × [0, Ku]m. We have"
N LOG DET N,0.5945796460176991,"lim
α Xα (rt, r) = X (rt, r) := −rt
s 1 +
X"
N LOG DET N,0.5951327433628318,"i∈[m]
r2
i + rt
X i∈[m]"
N LOG DET N,0.5956858407079646,√ziri −1 2 X i∈[m]
N LOG DET N,0.5962389380530974,"1
λi
r2
i + 1"
N LOG DET N,0.5967920353982301,"2λr2
t"
N LOG DET N,0.5973451327433629,"almost surely.
Since
√"
N LOG DET N,0.5978982300884956,"1 + x2 is convex and increasing and the function ∥r∥2 is convex,"
N LOG DET N,0.5984513274336283,"thus
q"
N LOG DET N,0.599004424778761,"1 + ∥r∥2
2 is convex in r and then −
q 1 + P"
N LOG DET N,0.5995575221238938,"i∈[m] r2
i
∥g3∥2
√n rt
= −
q"
N LOG DET N,0.6001106194690266,"1 + ∥r∥2
2
∥g3∥2
√n rt"
N LOG DET N,0.6006637168141593,"is concave in r.
Because −1"
P,0.6012168141592921,"2
P"
P,0.6017699115044248,"i∈[m]
1
λi r2
i is concave in r and rt
P"
P,0.6023230088495575,"i∈[m]
∥g1,i∥2
√n ri is lin-
ear in r, we deduce that Xα (rt, r) is concave in r.
By (Liese & Miescke, 2008, Lemma
7.75), supr∈[0,Ku]m |Xα (rt, r) −X (rt, r)| →0 almost surely. Then for ∀ϵ > 0, there exists
n0(ϵ), d0,i(ϵ), δ0,i(ϵ) such that for all n > n0(ϵ), di > d0,i(ϵ), |di/n −zi| < δ0,i(ϵ) and for all
r ∈[0, Ku]m, we have"
P,0.6028761061946902,"X (rt, r) −ϵ < Xα(rt, r) < X (rt, r) + ϵ ."
P,0.603429203539823,Thus we obtain
P,0.6039823008849557,"X (rt, r) −ϵ < Xα(rt, r) ≤
sup
r∈[0,Ku]m Xα(rt, r)"
P,0.6045353982300885,"Xα(rt, r) < X (rt, r) + ϵ ≤
sup
r∈[0,Ku]m X(rt, r) + ϵ ,"
P,0.6050884955752213,which in turn implies
P,0.605641592920354,"sup
r∈[0,Ku]m X (rt, r) −ϵ ≤
sup
r∈[0,Ku]m Xα(rt, r)"
P,0.6061946902654868,"sup
r∈[0,Ku]m Xα(rt, r) ≤
sup
r∈[0,Ku]m X(rt, r) + ϵ ."
P,0.6067477876106194,"It follows that
supr∈[0,Ku]m Xα(rt, r) −supr∈[0,Ku]m X(rt, r)
 ≤ϵ. In other words, we showed"
P,0.6073008849557522,|Yα (rt) −Y (rt)| →0
P,0.6078539823008849,"almost surely, where Y (rt) := supr∈[0,Ku]m Xα(rt, r) and Y (rt) := supr∈[0,Ku]m X(rt, r)."
P,0.6084070796460177,"Because Xα (rt, r) is convex in rt, then Y (rt) = supr∈[0,Ku]m Xα(rt, r) is convex in rt. By (Liese
& Miescke, 2008, Lemma 7.75) again, suprt∈[0,Kt] |Yα (rt) −Y (rt)| →0 almost surely. A similar
argument shows that"
P,0.6089601769911505,"inf
rt∈[0,Kt] Yα (rt) −
inf
rt∈[0,Kt] Y (rt)
 ="
P,0.6095132743362832,"inf
rt∈[0,Kt]
sup
r∈[0,Ku]m Xα(rt, r) −
inf
rt∈[0,Kt]
sup
r∈[0,Ku]m X(rt, r) →0"
P,0.610066371681416,almost surely.
P,0.6106194690265486,Under review as a conference paper at ICLR 2022
P,0.6111725663716814,"Therefore, we obtain"
P,0.6117256637168141,"inf
t∈St sup
u∈Su"
P,0.6122787610619469,"
∥t∥2 g⊤
1 u + ∥u∥2 g⊤
2 t −g⊤t −1"
P,0.6128318584070797,2u⊤Λ−1u + 1
P,0.6133849557522124,"2nλ ∥t∥2
2 "
P,0.6139380530973452,"=
inf
0≤rt≤Kt
sup
0≤ri≤Ku  −
s 1 +
X"
P,0.6144911504424779,"i∈[m]
r2
i
∥g3∥2
√n rt + rt
X i∈[m]"
P,0.6150442477876106,"∥g1,i∥2
√n
ri −1 2 X i∈[m]"
P,0.6155973451327433,"1
λi
r2
i + 1"
P,0.6161504424778761,"2λr2
t  "
P,0.6167035398230089,"a.s.
→
inf
0≤rt≤Kt
sup
0≤ri≤Ku "
P,0.6172566371681416,"−rt
s 1 +
X"
P,0.6178097345132744,"i∈[m]
r2
i + rt
X i∈[m]"
P,0.6183628318584071,√ziri −1 2 X i∈[m]
P,0.6189159292035398,"1
λi
r2
i + 1"
P,0.6194690265486725,"2λr2
t "
P,0.6200221238938053,"
(43) =:µ ."
P,0.620575221238938,Deﬁne event
P,0.6211283185840708,"Aα =
 inf
t∈Rn sup
u∈Rd"
P,0.6216814159292036,"
u⊤Zt −g⊤t −1"
P,0.6222345132743363,2u⊤Λ−1u + 1
P,0.6227876106194691,"2nλ ∥t∥2
2"
P,0.6233407079646017,"
−µ
 > τ

,"
P,0.6238938053097345,"Bα =
 inf
t∈St sup
u∈Su"
P,0.6244469026548672,"
u⊤Zt −g⊤t −1"
P,0.625,2u⊤Λ−1u + 1
P,0.6255530973451328,"2nλ ∥t∥2
2"
P,0.6261061946902655,"
−µ
 > τ

,"
P,0.6266592920353983,"Cα =
 inf
t∈St sup
u∈Su"
P,0.6272123893805309,"
∥t∥2 g⊤
1 u + ∥u∥2 g⊤
2 t −g⊤t −1"
P,0.6277654867256637,2u⊤Λ−1u + 1
P,0.6283185840707964,"2nλ ∥t∥2
2"
P,0.6288716814159292,"
−µ
 > τ

."
P,0.629424778761062,Recall
P,0.6299778761061947,"Eα =

inf
t∈Rn sup
u∈Rd"
P,0.6305309734513275,"
u⊤Zt −g⊤t −1"
P,0.6310840707964602,2u⊤Λ−1u + 1
P,0.6316371681415929,"2nλ ∥t∥2
2"
P,0.6321902654867256,"
= inf
t∈St sup
u∈Su"
P,0.6327433628318584,"
u⊤Zt −g⊤t −1"
P,0.6332964601769911,2u⊤Λ−1u + 1
P,0.6338495575221239,"2nλ ∥t∥2
2 
."
P,0.6344026548672567,"We have Aα ∩Eα ⊆Bα. Equation (43) gives limα P {Cα} = 0 for any τ > 0 because almost
sure convergence implies convergence in probability. By the convex Gaussian min-max theorem
(Thrampoulidis et al., 2015), we have"
P,0.6349557522123894,P {Bα} ≤2P {Cα} .
P,0.6355088495575221,It follows that
P,0.6360619469026548,"P {Aα} ≤P {Aα ∩Eα} + P {Ec
α} ≤P {Bα} + P {Ec
α} ≤2P {Cα} + P {Ec
α} ."
P,0.6366150442477876,"Taking lim supα on both sides, because lim supα P {Bα} ≤2 lim supα P {Cα} = 0, we get"
P,0.6371681415929203,"lim sup
α
P {Aα} ≤lim sup
α
P {Ec
α} ≤P

lim sup
α
Ec
α"
P,0.6377212389380531,"
= 0 ,"
P,0.6382743362831859,where the second inequality is because of the reverse Fatou’s lemma. Thus
P,0.6388274336283186,"inf
t∈Rn sup
u∈Rd"
P,0.6393805309734514,"
u⊤Zt −g⊤t −1"
P,0.639933628318584,2u⊤Λ−1u + 1
P,0.6404867256637168,"2nλ ∥t∥2
2"
P,0.6410398230088495,"
P→µ ."
P,0.6415929203539823,"Therefore, we deduce"
P,0.6421460176991151,"g⊤N −1g
P→−2
inf
0≤rt≤Kt
sup
0≤ri≤Ku "
P,0.6426991150442478,"−rt
s 1 +
X"
P,0.6432522123893806,"i∈[m]
r2
i + rt
X i∈[m]"
P,0.6438053097345132,√ziri −1 2 X i∈[m]
P,0.644358407079646,"1
λi
r2
i + 1"
P,0.6449115044247787,"2λr2
t  "
P,0.6454646017699115,"=
sup
0≤rt≤Kt
inf
0≤ri≤Ku "
P,0.6460176991150443,"2rt
s 1 +
X"
P,0.646570796460177,"i∈[m]
r2
i −2rt
X i∈[m]"
P,0.6471238938053098,"√ziri +
X i∈[m]"
P,0.6476769911504425,"1
λi
r2
i −λr2
t  "
P,0.6482300884955752,"=
sup
0≤rt≤Kt
inf
0≤ri≤Ku ϑ(rt, r, λ) .
(44)"
P,0.6487831858407079,"Because
g⊤N −1g
 ≤
1
λn ∥g∥2
2 and E 1"
P,0.6493362831858407,"λn ∥g∥2
2 = 1"
P,0.6498893805309734,"λ < ∞, by the dominated convergence theorem
for convergence in probability (Cohn, 2013, Proposition 3.1.6), we get"
P,0.6504424778761062,"lim
α tr N −1 = lim
α Eg

g⊤N −1g

=
max
0≤rt≤Kt
min
0≤ri≤Ku ϑ(rt, r, λ) .
(45)"
P,0.650995575221239,Under review as a conference paper at ICLR 2022
P,0.6515486725663717,"Note that 2rt
q 1 + P"
P,0.6521017699115044,"i∈[m] r2
i is convex in r, −2rt
P"
P,0.6526548672566371,"i∈[m]
√ziri is linear in r, and P"
P,0.6532079646017699,"i∈[m]
1
λi r2
i"
P,0.6537610619469026,"is strongly convex in r.
Thus ϑ is strongly convex in r.
Note that 2rt
q 1 + P"
P,0.6543141592920354,"i∈[m] r2
i −"
"RT
P",0.6548672566371682,"2rt
P"
"RT
P",0.6554203539823009,"i∈[m]
√ziri is linear in rt and that −λr2
t is strongly concave in rt. Thus ϑ is strongly concave
in rt. Then ϑ has a unique saddle point (r∗
t , r∗) on [0, Kt] × [0, Ku]m that satisﬁes"
"RT
P",0.6559734513274337,"max
rt∈[0,Kt]
min
r∈[0,Ku]m ϑ (rt, r) =
min
r∈[0,Ku]m
max
rt∈[0,Kt] ϑ (rt, r) = ϑ (r∗
t , r∗) ,
(46)"
"RT
P",0.6565265486725663,where the ﬁrst equality is due to Sion’s minimax theorem.
"RT
P",0.6570796460176991,"Since
tr N −1 ≤1"
"RT
P",0.6576327433628318,"λ, using the dominated convergence theorem and combining Equation (45) and
Equation (46) yields"
"RT
P",0.6581858407079646,"lim
α E tr N −1 =
max
0≤rt≤Kt
min
0≤ri≤Ku ϑ(rt, r, λ) =
min
0≤ri≤Ku
max
0≤rt≤Kt ϑ(rt, r, λ) ."
"RT
P",0.6587389380530974,"By the uniqueness of the limit, the right-hand side max0≤rt≤Kt min0≤ri≤Ku ϑ(rt, r, λ) and
min0≤ri≤Ku max0≤rt≤Kt ϑ(rt, r, λ) do not depend on Kt and Ku as long as Kt ≥2"
"RT
P",0.6592920353982301,λandKu ≥
"RT
P",0.6598451327433629,2λ+(2+√γ)
"RT
P",0.6603982300884956,"λ
. Thus we have"
"RT
P",0.6609513274336283,"lim
α E tr N −1 = max
rt≥0 min
ri≥0 ϑ(rt, r, λ) = min
ri≥0 max
rt≥0 ϑ(rt, r, λ) ."
"RT
P",0.661504424778761,"If r∗
t = 0, then ϑ (0, r∗) = minr∈[0,Ku]m P"
"RT
P",0.6620575221238938,"i∈[m]
1
λi r2
i = 0. Thus r∗must be zero. However,
ϑ
  1"
"RT
P",0.6626106194690266,"2λ, 0

=
3
4λ > ϑ (0, r∗). Therefore r∗
t > 0. We compute the partial derivative"
"RT
P",0.6631637168141593,"∂ϑ
∂ri
= 2rt
ri
q 1 + P"
"RT
P",0.6637168141592921,"i∈[m] r2
i
−2rt
√zi + 2 ri λi
."
"RT
P",0.6642699115044248,"If r∗
i = 0, we have
∂ϑ
∂ri"
"RT
P",0.6648230088495575,"ri=0,rt=r∗
t
= −2r∗
t
√zi < 0 ."
"RT
P",0.6653761061946902,"Therefore, one can increase r∗
i and make maxrt∈[0,Kt] minr∈[0,Ku]m ϑ (rt, r) smaller, which results
in a contradiction. Thus r∗
i > 0. Thus the minimax value is attained when rt, ri > 0 for all i ∈[m]."
"RT
P",0.665929203539823,"To obtain the optimality condition, we compute the partial derivatives"
"RT
P",0.6664823008849557,"∂ϑ
∂rt
= 2
s 1 +
X"
"RT
P",0.6670353982300885,"i∈[m]
r2
i −2  X"
"RT
P",0.6675884955752213,"i∈[m]
ri
√zi "
"RT
P",0.668141592920354,"−2λrt ,"
"RT
P",0.6686946902654868,"∂ϑ
∂ri
= 2rt
ri
q 1 + P"
"RT
P",0.6692477876106194,"i∈[m] r2
i
−2rt
√zi + 2 ri λi
."
"RT
P",0.6698008849557522,"Setting them to zero gives the optimality condition for r∗
t , r∗
1, . . . , r∗
m and yields Equation (40) and
Equation (41)."
"RT
P",0.6703539823008849,"Using the envelope theorem, we get
∂
∂λi
max
rt∈[0,Kt]
min
r∈[0,Ku]m ϑ(rt, r, λ)"
"RT
P",0.6709070796460177,"=∂ϑ(r∗
t , r∗, λ1, . . . , λm) ∂λi"
"RT
P",0.6714601769911505,"= −r∗2
i
λ2
i
."
"RT
P",0.6720132743362832,Lemma 12. Deﬁne N = λnIn + Z⊤ΛZ. The following equation holds
"RT
P",0.672566371681416,"lim
n,di→+∞
di/n→zi E
 ∂"
"RT
P",0.6731194690265486,"∂λi
tr
 
N −1
=
∂2"
"RT
P",0.6736725663716814,"∂λi∂λ inf
ρ∈Rm
+  log  λ + m
X"
"RT
P",0.6742256637168141,"j=1
λjρj  + m
X j=1"
"RT
P",0.6747787610619469,"
ρj −zj(log ρj"
"RT
P",0.6753318584070797,"zj
+ 1)
"
"RT
P",0.6758849557522124,"= −r∗2
i
λ2
i
,"
"RT
P",0.6764380530973452,Under review as a conference paper at ICLR 2022
"RT
P",0.6769911504424779,"where r∗is a solution to suprt>0 infr1,...,rm>0 ϑ(rt, r1, . . . , rm, λ1, . . . , λm) and"
"RT
P",0.6775442477876106,"ϑ(rt, r1, . . . , rm, λ1, . . . , λm) = 2rt
s 1 +
X"
"RT
P",0.6780973451327433,"i∈[m]
r2
i −2rt
X i∈[m]"
"RT
P",0.6786504424778761,"√ziri +
X i∈[m]"
"RT
P",0.6792035398230089,"1
λi
r2
i −λr2
t ."
"RT
P",0.6797566371681416,"Proof. Since

∂
∂λi
tr
 
N −1 = tr
 
Z⊤
i N −2Zi

≤
1"
"RT
P",0.6803097345132744,"(λn)2 tr
 
Z⊤
i Zi

(47)"
"RT
P",0.6808628318584071,"and E
1
(λn)2 tr
 
Z⊤
i Zi

≍
1
λ2 (by Lemma 4), using the dominated convergence theorem gives E
 ∂"
"RT
P",0.6814159292035398,"∂λi
tr
 
N −1
=
∂
∂λi
E tr
 
N −1
."
"RT
P",0.6819690265486725,"We use α to denote the indices n, di and use limα to denote limn,di→+∞
di/n=zi
.
Deﬁne fα(λi) ="
"RT
P",0.6825221238938053,"E tr
 
N −1
, g(λi) =
∂
∂λ infρ∈Rm
+
h
log

λ + P"
"RT
P",0.683075221238938,"i∈[m] λiρi

+ P"
"RT
P",0.6836283185840708,"i∈[m]

ρi −zi

log ρi"
"RT
P",0.6841814159292036,"zi + 1
i
,"
"RT
P",0.6847345132743363,"and h(λi)
=
suprt>0 infr1,...,rm>0 ϑ(rt, r1, . . . , rm, λ1, . . . , λm).
Because
tr
 
N −1
≤
tr
  1"
"RT
P",0.6852876106194691,"λnIn

≤
1
λ and limα tr
 
N −1
= g(λi) (by Lemma 9), we have Lemma 9 limα fα(λi) =
limα E tr
 
N −1
= g(λi). Lemma 11 shows limα fα(λi) = h(λi). Therefore limα fα(λi) =
g(λi) = h(λi)."
"RT
P",0.6858407079646017,"Because of Equation (47), we have f ′
α(λi) =
∂
∂λi E tr
 
N −1
and"
"RT
P",0.6863938053097345,"|f ′
α(λi)| =

∂
∂λi
E tr
 
N −1 =
E
 ∂"
"RT
P",0.6869469026548672,"∂λi
tr
 
N −1 ≤E

∂
∂λi
tr
 
N −1 ≲1 λ2"
"RT
P",0.6875,"and therefore {f ′
α} is uniformly bounded for λi. Because

∂2"
"RT
P",0.6880530973451328,"∂λ2
i
tr
 
N −1"
"RT
P",0.6886061946902655,"=2 tr
 
N −1ZiZ⊤
i N −1ZiZ⊤
i N −1 ≲1"
"RT
P",0.6891592920353983,"λn tr

N −1  
ZiZ⊤
i
2 N −1 = 1"
"RT
P",0.6897123893805309,"λn tr
 
ZiZ⊤
i N −2ZiZ⊤
i
 ≤
1"
"RT
P",0.6902654867256637,"(λn)3 tr
 
ZiZ⊤
i
2 ,"
"RT
P",0.6908185840707964,"and E
1
(λn)3 tr
 
ZiZ⊤
i
2 ≍
1
λ3 (by Lemma 4), using the dominated convergence theorem yields"
"RT
P",0.6913716814159292,"E
 ∂2"
"RT
P",0.691924778761062,"∂λ2
i
tr
 
N −1
=
∂
∂λi
E
 ∂"
"RT
P",0.6924778761061947,"∂λi
tr
 
N −1
= ∂2"
"RT
P",0.6930309734513275,"∂λ2
i
E

tr
 
N −1
= f ′′
α(λi) ."
"RT
P",0.6935840707964602,"Moreover, we have"
"RT
P",0.6941371681415929,"|f ′′
α(λi)| ≤E

∂2"
"RT
P",0.6946902654867256,"∂λ2
i
tr
 
N −1 ≲1 λ3 ."
"RT
P",0.6952433628318584,"Thus {f ′
α} is uniformly equicontinuous for λi. We want to show that limα f ′
α(λi) = g′(λi) by
contradiction. Assume that it is not true. Then there exists ϵ > 0 and a subsequence

f ′
αk
	
such
that
f ′
αk(λi) −g′(λi)
 > ϵ. Since

f ′
αk
	
is uniformly bounded and uniformly equicontinuous for
λi ∈E (E is any closed ﬁnite interval containing λi), by the Arzela-Ascoli theorem, there exists
a subsequence
n
f ′
αkr"
"RT
P",0.6957964601769911,"o
that converges uniformly on E. Since limα fαkr (λi) = g(λi), by (Rudin,
1976, Thoerem 7.17), we have
lim
r f ′
αkr (λi) = g′(λi) ,"
"RT
P",0.6963495575221239,"which yields a contradiction. Therefore, we have limα f ′
α(λi) = g′(λi). Recall g(λi) = h(λi)
for any λi > 0. Then by the ﬁnal part of Lemma 11, we have limα f ′
α(λi) = g′(λi) = h′(λi) =
−r∗2
i
λ2
i ."
"RT
P",0.6969026548672567,Under review as a conference paper at ICLR 2022
"RT
P",0.6974557522123894,"D.4
BIAS"
"RT
P",0.6980088495575221,Lemma 13. Suppose that U ∼L
"RT
P",0.6985619469026548,i∈[m] Unif (O (di)) and V are two independent d × d random
"RT
P",0.6991150442477876,"matrices such that V
d= UV U ⊤, where d = Pm
i=1 di. Let θ ∈Rd be a ﬁxed vector. Write θ =  "
"RT
P",0.6996681415929203,"θ1
...
θm "
"RT
P",0.7002212389380531,", where θi ∈Rdi. Let φ ∼
M"
"RT
P",0.7007743362831859,"i∈[m]
Unif
 
Sdi−1 (∥θi∥2)
"
"RT
P",0.7013274336283186,"be a random vector independent of V and let Λ = diag (λ1Id1, . . . , λmIdm) ∈Rd×d. Then we have"
"RT
P",0.7018805309734514,"E
Λ1/2V θ

2 2"
"RT
P",0.702433628318584,"
= E
Λ1/2V φ

2 2 
."
"RT
P",0.7029867256637168,"Proof. Recall UΛU ⊤= Λ and noticing U ⊤θ
d= φ, we get"
"RT
P",0.7035398230088495,"E
Λ1/2V θ

2 2 "
"RT
P",0.7040929203539823,"=E
Λ1/2UV U ⊤θ

2 2 "
"RT
P",0.7046460176991151,"=E

θ⊤UV ⊤U ⊤ΛUV U ⊤θ
"
"RT
P",0.7051991150442478,"=E

θ⊤UV ⊤ΛV U ⊤θ
"
"RT
P",0.7057522123893806,"=E
Λ1/2V U ⊤θ

2 2 "
"RT
P",0.7063053097345132,"=E
Λ1/2V φ

2 2 
."
"RT
P",0.706858407079646,"Lemma
14.
Deﬁne
˜Θ
=
diag
 
∥θ′
1∥2
2/d1Id1, . . . , ∥θ′
m∥2
2/dmIdm

and
S
="
"RT
P",0.7074115044247787,"Λ1/2Z
 
nλIn + Z⊤ΛZ
−1 Z⊤Λ1/2. Then we have"
"RT
P",0.7079646017699115,"Bλ,d,n = ∥θ∗∥2
Σ −2E tr

ΛS ˜Θ

+ E tr

SΛS ˜Θ

."
"RT
P",0.7085176991150443,Proof. Recall Equation (22) in Lemma 5
"RT
P",0.709070796460177,"Bλ,d,n = E """
"RT
P",0.7096238938053098,"∥Λ1/2

Id + 1"
"RT
P",0.7101769911504425,"nλΛ1/2ZZ⊤Λ1/2
−1
θ′∥2
2 # ."
"RT
P",0.7107300884955752,Let U ∼L
"RT
P",0.7112831858407079,"i∈[m] Unif (O(di)) be a random matrix independent of Z. Because UZ
d= Z, we have"
"RT
P",0.7118362831858407,Id + 1
"RT
P",0.7123893805309734,nλΛ1/2ZZ⊤Λ1/2 d= Id + 1
"RT
P",0.7129424778761062,"nλΛ1/2UZZ⊤U ⊤Λ1/2 = U

Id + 1"
"RT
P",0.713495575221239,"nλΛ1/2ZZ⊤Λ1/2

U ⊤."
"RT
P",0.7140486725663717,Deﬁne ˜θ ∼L
"RT
P",0.7146017699115044,"i∈[m] Unif(Sdi−1(∥θ′
i∥2)). Lemma 13 gives"
"RT
P",0.7151548672566371,"Bλ,d,n =E """
"RT
P",0.7157079646017699,"∥Λ1/2

Id + 1"
"RT
P",0.7162610619469026,"nλΛ1/2ZZ⊤Λ1/2
−1
˜θ∥2
2 #"
"RT
P",0.7168141592920354,"=E
h
∥Λ1/2 (Id −S) ˜θ∥2
2
i"
"RT
P",0.7173672566371682,"=E
h
∥(Id −S) ˜θ∥2
Λ
i"
"RT
P",0.7179203539823009,"=E
˜θ

2"
"RT
P",0.7184734513274337,"Λ −E
h
˜θ⊤ΛS˜θ
i
−E
h
˜θ⊤SΛ˜θ
i
+ E
h
˜θ⊤SΛS˜θ
i
."
"RT
P",0.7190265486725663,Under review as a conference paper at ICLR 2022
"RT
P",0.7195796460176991,"Notice that
˜θ

2"
"RT
P",0.7201327433628318,"Λ = ∥θ′∥2
Λ and ˜Θ = E
h
˜θ˜θ⊤i
. Because ˜Θ commutes with Λ, we have tr

SΛ˜Θ

="
"RT
P",0.7206858407079646,"tr

S ˜ΘΛ

= tr

ΛS ˜Θ

. In light of these, we deduce"
"RT
P",0.7212389380530974,"Bλ,d,n = ∥θ′∥2
Λ −E tr

ΛS ˜Θ

−E tr

SΛ˜Θ

+ E tr

SΛS ˜Θ
"
"RT
P",0.7217920353982301,"= ∥θ′∥2
Λ −2E tr

ΛS ˜Θ

+ E tr

SΛS ˜Θ

."
"RT
P",0.7223451327433629,"Lemma 14 expresses the bias Bλ,d,n as the sum of three terms."
"RT
P",0.7228982300884956,"Computing ∥θ′∥2
Λ
Note that ∥θ′∥2
Λ = θ′⊤Λθ′ = P"
"RT
P",0.7234513274336283,"i∈[m] λi ∥θ′
i∥2
2. Therefore,"
"RT
P",0.724004424778761,"lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
"RT
P",0.7245575221238938,"∥θ′∥2
Λ = q⊤(λ ⊙z) ."
"RT
P",0.7251106194690266,"Computing E tr

ΛS ˜Θ

Deﬁne N = λnIn + Z⊤ΛZ = λnIn + P"
"RT
P",0.7256637168141593,"i∈[m] λiZiZ⊤
i . We have"
"RT
P",0.7262168141592921,"E tr

ΛS ˜Θ
"
"RT
P",0.7267699115044248,"=E tr

Z⊤Λ1/2 ˜ΘΛ3/2Z
 
nλIn + Z⊤ΛZ
−1"
"RT
P",0.7273230088495575,"=E tr

Z⊤Λ2 ˜ΘZN −1 =
X"
"RT
P",0.7278761061946902,"i∈[m]
λ2
i
∥θ′
i∥2
2
di
E tr
 
ZiZ⊤
i N −1 =
X"
"RT
P",0.728429203539823,"i∈[m]
λ2
i ∥θ′
i∥2
2
n
di
E
 ∂ ∂λi"
N LOG DET N,0.7289823008849557,"1
n log det N n  =
X"
N LOG DET N,0.7295353982300885,"i∈[m]
λ2
i ∥θ′
i∥2
2
n
di"
N LOG DET N,0.7300884955752213,"∂
∂λi
E
 1"
N LOG DET N,0.730641592920354,"n log det N n 
,"
N LOG DET N,0.7311946902654868,"where the second inequality is because ˜Θ commutes with Λ3/2 and the ﬁnal equality is because of
Equation (38). Taking lim n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
N LOG DET N,0.7317477876106194,and using Lemma 10 gives
N LOG DET N,0.7323008849557522,"lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
N LOG DET N,0.7328539823008849,"E tr

ΛS ˜Θ

=
X i∈[m]"
N LOG DET N,0.7334070796460177,"λ2
i η2
i
zi"
N LOG DET N,0.7339601769911505,"∂
∂λi
inf
ρ∈Rm
+  log "
N LOG DET N,0.7345132743362832,"λ +
X"
N LOG DET N,0.735066371681416,"i∈[m]
λiρi  +
X i∈[m]"
N LOG DET N,0.7356194690265486,"
ρi −zi"
N LOG DET N,0.7361725663716814,"
log ρi"
N LOG DET N,0.7367256637168141,"zi
+ 1
 ."
N LOG DET N,0.7372787610619469,Using the envelope theorem yields
N LOG DET N,0.7378318584070797,"∂
∂λi
inf
ρ∈Rm
+  log "
N LOG DET N,0.7383849557522124,"λ +
X"
N LOG DET N,0.7389380530973452,"i∈[m]
λiρi  +
X i∈[m]"
N LOG DET N,0.7394911504424779,"
ρi −zi"
N LOG DET N,0.7400442477876106,"
log ρi"
N LOG DET N,0.7405973451327433,"zi
+ 1
"
N LOG DET N,0.7411504424778761,"=
ρ∗
i
λ + P"
N LOG DET N,0.7417035398230089,"i∈[m] λiρ∗
i
= zi −ρ∗
i
λi
,"
N LOG DET N,0.7422566371681416,"where the ﬁnal equality is because of Equation (6) in Item 1. Therefore, we deduce"
N LOG DET N,0.7428097345132744,"lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
N LOG DET N,0.7433628318584071,"E tr

ΛS ˜Θ

=
X i∈[m]"
N LOG DET N,0.7439159292035398,"λ2
i η2
i
zi"
N LOG DET N,0.7444690265486725,"zi −ρ∗
i
λi
=
X"
N LOG DET N,0.7450221238938053,"i∈[m]
λiη2
i"
N LOG DET N,0.745575221238938,"
1 −ρ∗
i
zi"
N LOG DET N,0.7461283185840708,"
= q⊤(λ ⊙(z −ρ∗)) ."
N LOG DET N,0.7466814159292036,Under review as a conference paper at ICLR 2022
N LOG DET N,0.7472345132743363,"Computing E tr

SΛS ˜Θ

We have"
N LOG DET N,0.7477876106194691,"E tr

SΛS ˜Θ
"
N LOG DET N,0.7483407079646017,"=E tr
h
Λ1/2ZN −1Z⊤Λ2ZN −1Z⊤Λ1/2 ˜Θ
i"
N LOG DET N,0.7488938053097345,"=E tr
h
Z⊤Λ1/2 ˜ΘΛ1/2ZN −1Z⊤Λ2ZN −1i"
N LOG DET N,0.7494469026548672,"=E tr
h
Z⊤Λ˜ΘZN −1Z⊤Λ2ZN −1i =
X i∈[m]"
N LOG DET N,0.75,"λi∥θ′
i∥2
2
di X"
N LOG DET N,0.7505530973451328,"j∈[m]
λ2
jE tr

ZiZ⊤
i N −1ZjZ⊤
j N −1 = −
X i∈[m]"
N LOG DET N,0.7511061946902655,"λi∥θ′
i∥2
2n
di X"
N LOG DET N,0.7516592920353983,"j∈[m]
λ2
jE

∂2"
N LOG DET N,0.7522123893805309,∂λj∂λi
N LOG DET N,0.7527654867256637,"1
n log det N n 
,"
N LOG DET N,0.7533185840707964,"where the third equality is because ˜Θ commutes with Λ1/2.
Taking lim n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
N LOG DET N,0.7538716814159292,and using
N LOG DET N,0.754424778761062,Lemma 10 gives
N LOG DET N,0.7549778761061947,"lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
N LOG DET N,0.7555309734513275,"E tr

SΛS ˜Θ

= −
X i∈[m]"
N LOG DET N,0.7560840707964602,"λiη2
i
zi X"
N LOG DET N,0.7566371681415929,"j∈[m]
λ2
j
∂2"
N LOG DET N,0.7571902654867256,"∂λj∂λi
inf
ρ∈Rm
+  log "
N LOG DET N,0.7577433628318584,"λ +
X"
N LOG DET N,0.7582964601769911,"l∈[m]
λlρl  +
X l∈[m]"
N LOG DET N,0.7588495575221239,"
ρl −zl"
N LOG DET N,0.7594026548672567,"
log ρl"
N LOG DET N,0.7599557522123894,"zl
+ 1
 ."
N LOG DET N,0.7605088495575221,"Write λ = (λ1, . . . , λm)⊤and z = (z1, . . . , zm)⊤. Let ρ∗∈Rm be a minimizer of Equation (5)
and J = ∂ρ∗"
N LOG DET N,0.7610619469026548,"∂λ ∈Rm×m be the Jacobian matrix Jij = ∂ρ∗
i
∂λj . Recall Item 2
 
diag (λ) +
 
λ + λ⊤ρ∗
Im −(z −ρ∗) λ⊤
J = (z −ρ∗) ρ∗⊤−diag (ρ∗) .
Using the envelope theorem, we have"
N LOG DET N,0.7616150442477876,"∂
∂λi
inf
ρ∈Rm
+  log "
N LOG DET N,0.7621681415929203,"λ +
X"
N LOG DET N,0.7627212389380531,"l∈[m]
λlρl  +
X l∈[m]"
N LOG DET N,0.7632743362831859,"
ρl −zl"
N LOG DET N,0.7638274336283186,"
log ρl"
N LOG DET N,0.7643805309734514,"zl
+ 1
"
N LOG DET N,0.764933628318584,"=
ρ∗
i
λ + P"
N LOG DET N,0.7654867256637168,"l∈[m] λlρ∗
l
=
ρ∗
i
λ + λ⊤ρ∗."
N LOG DET N,0.7660398230088495,"Recall Equation (6) yields
ρ∗
i
λ + λ⊤ρ∗= zi −ρ∗
i
λi
."
N LOG DET N,0.7665929203539823,Differentiating the above equation with respect to λj gives ∂2
N LOG DET N,0.7671460176991151,"∂λj∂λi
inf
ρ∈Rm
+  log "
N LOG DET N,0.7676991150442478,"λ +
X"
N LOG DET N,0.7682522123893806,"l∈[m]
λlρl  +
X l∈[m]"
N LOG DET N,0.7688053097345132,"
ρl −zl"
N LOG DET N,0.769358407079646,"
log ρl"
N LOG DET N,0.7699115044247787,"zl
+ 1
  = ∂ ∂λj"
N LOG DET N,0.7704646017699115,"zi −ρ∗
i
λi"
N LOG DET N,0.7710176991150443,"=−λiJij −(zi −ρ∗
i ) δij
λ2
i
."
N LOG DET N,0.771570796460177,It follows that
N LOG DET N,0.7721238938053098,"lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
N LOG DET N,0.7726769911504425,"E tr

SΛS ˜Θ
 =
X i∈[m]"
N LOG DET N,0.7732300884955752,"λiη2
i
zi X"
N LOG DET N,0.7737831858407079,"j∈[m]
λ2
j
λiJij + (zi −ρ∗
i ) δij
λ2
i =
X"
N LOG DET N,0.7743362831858407,"i,j∈[m]
qiλ2
j"
N LOG DET N,0.7748893805309734,"
Jij + (zi −ρ∗
i ) δij
λi "
N LOG DET N,0.7754424778761062,"=q⊤ 
λ ⊙(z −ρ∗) + Jλ⊙2"
N LOG DET N,0.775995575221239,Under review as a conference paper at ICLR 2022
N LOG DET N,0.7765486725663717,"Putting all three terms together, we have"
N LOG DET N,0.7771017699115044,"lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
N LOG DET N,0.7776548672566371,"Bλ,d,n = q⊤(λ ⊙z)−2q⊤(λ ⊙(z −ρ∗))+q⊤ 
λ ⊙(z −ρ∗) + Jλ⊙2
= q⊤ 
λ ⊙ρ∗+ Jλ⊙2
."
N LOG DET N,0.7782079646017699,"Since {Bλ,d,n} is uniformly bounded and uniformly equicontinuous for λ ∈(0, 1] by Lemma 5,
{Bλ,d,n} can be extended continuously to [0, 1] and the family of extended functions is still uni-
formly bounded and uniformly equicontinuous for λ ∈[0, 1].
By the Arzela-Ascoli theorem,
{Bλ,d,n} converges uniformly to the limit. By the Moore-Osgood theorem, we can exchange the
two limits lim n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
N LOG DET N,0.7787610619469026,and limλ→0+ and get
N LOG DET N,0.7793141592920354,"lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
N LOG DET N,0.7798672566371682,"B0,d,n =
lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
N LOG DET N,0.7804203539823009,"lim
λ→0+ Bλ,d,n = lim
λ→0+
lim
n,di→+∞
di/n→zi
∥Πiθ∗∥2→ηi"
N LOG DET N,0.7809734513274337,"Bλ,d,n = q⊤ 
λ ⊙ρ∗+ Jλ⊙2
|λ=0 ."
N LOG DET N,0.7815265486725663,"D.5
VARIANCE"
N LOG DET N,0.7820796460176991,"Deﬁne N = nλIn + Z⊤ΛZ. Recalling Lemma 6 gives
Vλ,d,n
=σ2E∥ΛZN −1∥2
2"
N LOG DET N,0.7826327433628318,"=σ2
m
X"
N LOG DET N,0.7831858407079646,"i=1
λ2
i E tr
 
ZiZ⊤
i N −2"
N LOG DET N,0.7837389380530974,"= −σ2
m
X"
N LOG DET N,0.7842920353982301,"i=1
λ2
i E
 ∂"
N LOG DET N,0.7848451327433629,"∂λi
tr
 
N −1
."
N LOG DET N,0.7853982300884956,"Using Lemma 12, we get
lim
n,di→∞
di/n→zi"
N LOG DET N,0.7859513274336283,"Vλ,d,n"
N LOG DET N,0.786504424778761,"= −σ2
m
X"
N LOG DET N,0.7870575221238938,"i=1
λ2
i
lim
n,di→∞
di/n→zi E
 ∂"
N LOG DET N,0.7876106194690266,"∂λi
tr
 
N −1"
N LOG DET N,0.7881637168141593,"= −σ2
m
X"
N LOG DET N,0.7887168141592921,"i=1
λ2
i
lim
n,di→∞
di/n→zi E
 ∂"
N LOG DET N,0.7892699115044248,"∂λi
tr
 
N −1"
N LOG DET N,0.7898230088495575,"= −σ2
m
X"
N LOG DET N,0.7903761061946902,"i=1
λ2
i
∂2"
N LOG DET N,0.790929203539823,"∂λi∂λ inf
ρ∈Rm
+  log  λ + m
X"
N LOG DET N,0.7914823008849557,"j=1
λjρj  + m
X j=1"
N LOG DET N,0.7920353982300885,"
ρj −zj(log ρj"
N LOG DET N,0.7925884955752213,"zj
+ 1)
 ."
N LOG DET N,0.793141592920354,"Using the envelope theorem, we deduce"
N LOG DET N,0.7936946902654868,"∂
∂λ inf
ρ∈Rm
+  log  λ + m
X"
N LOG DET N,0.7942477876106194,"j=1
λjρj  + m
X j=1"
N LOG DET N,0.7948008849557522,"
ρj −zj"
N LOG DET N,0.7953539823008849,"
log ρj"
N LOG DET N,0.7959070796460177,"zj
+ 1
"
N LOG DET N,0.7964601769911505,"=
1
λ + Pm
j=1 λjρ∗
j
."
N LOG DET N,0.7970132743362832,"Then we take
∂
∂λi and obtain ∂2"
N LOG DET N,0.797566371681416,"∂λi∂λ inf
ρ∈Rm
+  log  λ + m
X"
N LOG DET N,0.7981194690265486,"j=1
λjρj  + m
X j=1"
N LOG DET N,0.7986725663716814,"
ρj −zj(log ρj"
N LOG DET N,0.7992256637168141,"zj
+ 1)
  = ∂ ∂λi"
N LOG DET N,0.7997787610619469,"1
λ + Pm
j=1 λjρ∗
j"
N LOG DET N,0.8003318584070797,"= −
ρ∗
i + P"
N LOG DET N,0.8008849557522124,"j∈[m] λjJji

λ + P"
N LOG DET N,0.8014380530973452,"j∈[m] λjρ∗
j
2 ."
N LOG DET N,0.8019911504424779,Under review as a conference paper at ICLR 2022
N LOG DET N,0.8025442477876106,"As a result,"
N LOG DET N,0.8030973451327433,"lim
n,di→∞
di/n→zi"
N LOG DET N,0.8036504424778761,"Vλ,d,n = σ2
m
X"
N LOG DET N,0.8042035398230089,"i=1
λ2
i
ρ∗
i + P"
N LOG DET N,0.8047566371681416,"j∈[m] λjJji

λ + P"
N LOG DET N,0.8053097345132744,"j∈[m] λjρ∗
j
2 = σ2
 
λ⊙2⊤ 
ρ∗+ J⊤λ
"
N LOG DET N,0.8058628318584071,"(λ + λ⊤ρ∗)2
."
N LOG DET N,0.8064159292035398,"By Lemma 12, the variance is given by"
N LOG DET N,0.8069690265486725,"lim
n,di→∞
di/n→zi"
N LOG DET N,0.8075221238938053,"Vλ,d,n =
lim
n,di→∞
di/n→zi"
N LOG DET N,0.808075221238938,"−σ2
m
X"
N LOG DET N,0.8086283185840708,"i=1
λ2
i E
 ∂"
N LOG DET N,0.8091814159292036,"∂λi
tr
 
N −1
= σ2
m
X"
N LOG DET N,0.8097345132743363,"i=1
r∗2
i ,"
N LOG DET N,0.8102876106194691,where r∗solves
N LOG DET N,0.8108407079646017,"sup
rt>0
inf
r1,...,rm>0 "
N LOG DET N,0.8113938053097345,"2rt
s 1 +
X"
N LOG DET N,0.8119469026548672,"i∈[m]
r2
i −2rt
X i∈[m]"
N LOG DET N,0.8125,"√ziri +
X i∈[m]"
N LOG DET N,0.8130530973451328,"1
λi
r2
i −λr2
t  ."
N LOG DET N,0.8136061946902655,"Since {Vλ,d,n} is uniformly bounded and uniformly equicontinuous with respect to λ ∈(0, 1] by
Lemma 5, {Vλ,d,n} can be extended continuously to [0, 1] and the family of extended functions
is still uniformly bounded and uniformly equicontinuous. By the Arzela-Ascoli theorem, {Vλ,d,n}
converges uniformly to the limit. By the Moore-Osgood theorem, we can exchange the two limits
limn,di→∞
di/n→zi
and limλ→0+ and get"
N LOG DET N,0.8141592920353983,"lim
n,di→∞
di/n→zi"
N LOG DET N,0.8147123893805309,"lim
λ→0+ Vλ,d,n = lim
λ→0+
lim
n,di→∞
di/n→zi"
N LOG DET N,0.8152654867256637,"Vλ,d,n = σ2
m
X"
N LOG DET N,0.8158185840707964,"i=1
r∗2
i
|λ=0 ."
N LOG DET N,0.8163716814159292,"E
PROOF OF THEOREM 2"
N LOG DET N,0.816924778761062,"We use Theorem 1 to prove Theorem 2. As in Theorem 1, let r∗solve minri≥0 maxrt≥0 ϑ(rt, r, λ),
where ϑ is deﬁned in Equation (7).
Note that ϑ is a quadratic function of rt.
Deﬁne A =
qP"
N LOG DET N,0.8174778761061947,"i∈[m] r2
i + 1, B = P"
N LOG DET N,0.8180309734513275,"i∈[m]
√ziri, A∗=
qP"
N LOG DET N,0.8185840707964602,"i∈[m] r∗2
i
+ 1, and B∗= P"
N LOG DET N,0.8191371681415929,"i∈[m]
√zir∗
i . Then"
N LOG DET N,0.8196902654867256,"r∗
t = A−B"
N LOG DET N,0.8202433628318584,"λ
and we get"
N LOG DET N,0.8207964601769911,"min
ri≥0 max
rt≥0 ϑ(rt, r, λ) = min
ri≥0 "
N LOG DET N,0.8213495575221239,"(A −B)2 λ
+
X i∈[m]"
N LOG DET N,0.8219026548672567,"1
λi
r2
i "
N LOG DET N,0.8224557522123894,"= min
ri≥0 "
N LOG DET N,0.8230088495575221,"(A −B)2 λ
+
X i∈[m]"
N LOG DET N,0.8235619469026548,"1
λi
r2
i  ."
N LOG DET N,0.8241150442477876,"Taking the partial derivative with respect to ri gives ∂
∂ri "
N LOG DET N,0.8246681415929203,"(A −B)2 λ
+
X i∈[m]"
N LOG DET N,0.8252212389380531,"1
λi
r2
i "
N LOG DET N,0.8257743362831859,= 2 · A −B λ ri
N LOG DET N,0.8263274336283186,"A −√zi

+ 2 · ri λi
."
N LOG DET N,0.8268805309734514,"Setting it to zero gives the optimality condition for r∗
i : A∗−B∗ λ"
N LOG DET N,0.827433628318584," r∗
i
A∗−√zi"
N LOG DET N,0.8279867256637168,"
= −r∗
i
λi
,
i ∈[m] .
(48)"
N LOG DET N,0.8285398230088495,"It follows that
r∗
i
A∗−√zi"
N LOG DET N,0.8290929203539823,"r∗
j
A∗−√zj
= r∗
i /λi
r∗
j /λj
,
i, j ∈[m] ."
N LOG DET N,0.8296460176991151,Some algebraic manipulation in the above equation yields
N LOG DET N,0.8301991150442478,"r∗
i
r∗
j
= λi"
N LOG DET N,0.8307522123893806,"λj
·
√ziA∗−r∗
i
√zjA∗−r∗
j
,
i, j ∈[m] ."
N LOG DET N,0.8313053097345132,Under review as a conference paper at ICLR 2022
N LOG DET N,0.831858407079646,"Deﬁne z = (z1, . . . , zm). Then ∥z∥1 = P"
N LOG DET N,0.8324115044247787,"i∈[m] zi. By Cauchy–Schwarz inequality, if d/n →
P"
N LOG DET N,0.8329646017699115,i∈[m] zi < 1
N LOG DET N,0.8335176991150443,"B ≤
s X"
N LOG DET N,0.834070796460177,"i∈[m]
zi ∥r∥2 < ∥r∥2 <
q"
N LOG DET N,0.8346238938053098,"∥r∥2
2 + 1 = A ."
N LOG DET N,0.8351769911504425,Thus there does not exist r such that A = B. If d/n →P
N LOG DET N,0.8357300884955752,"i∈[m] zi > 1, then A = B is feasible for
r. For example, set"
N LOG DET N,0.8362831858407079,"r =
1
p"
N LOG DET N,0.8368362831858407,(∥z∥1 −1) ∥z∥1 √z .
N LOG DET N,0.8373893805309734,We have
N LOG DET N,0.8379424778761062,"B =

r, √z

= s"
N LOG DET N,0.838495575221239,"∥z∥1
∥z∥1 −1 A =
q"
N LOG DET N,0.8390486725663717,"1 + ∥r∥2
2 = s"
N LOG DET N,0.8396017699115044,"1 +
1
∥z∥1 −1 = B ."
N LOG DET N,0.8401548672566371,"If ∥z∥1 > 1, since A = B is feasible, then"
N LOG DET N,0.8407079646017699,"lim
λ→0+ min
ri≥0 "
N LOG DET N,0.8412610619469026,"(A −B)2 λ
+
X i∈[m]"
N LOG DET N,0.8418141592920354,"1
λi
r2
i "
N LOG DET N,0.8423672566371682,"= min
ri≥0
A=B X i∈[m]"
N LOG DET N,0.8429203539823009,"1
λi
r2
i ."
N LOG DET N,0.8434734513274337,"If ∥z∥1 < 1, then A −B always holds. To be precise, we have"
N LOG DET N,0.8440265486725663,"A −B ≥
q"
N LOG DET N,0.8445796460176991,"∥r∥2
2 + 1 −∥r∥2"
N LOG DET N,0.8451327433628318,"
∨

1 −
q ∥z∥1"
N LOG DET N,0.8456858407079646,"
∥r∥2 
."
N LOG DET N,0.8462389380530974,"If ∥r∥2 > 1, then
 
1 −
p"
N LOG DET N,0.8467920353982301,"∥z∥1

∥r∥2 > 1−
p"
N LOG DET N,0.8473451327433629,"∥z∥1. If ∥r∥2 ≤1, then
q"
N LOG DET N,0.8478982300884956,"∥r∥2
2 + 1−∥r∥2 ≥
√ 2−1."
N LOG DET N,0.8484513274336283,"Thus there exists a universal constant C0 =
 
1 −
p"
N LOG DET N,0.849004424778761,"∥z∥1

∨
 √"
N LOG DET N,0.8495575221238938,"2 −1

> 0 such that"
N LOG DET N,0.8501106194690266,A −B ≥C0 .
N LOG DET N,0.8506637168141593,Recall Equation (48). We have
N LOG DET N,0.8512168141592921,"(A∗−B∗)
 r∗
i
A∗−√zi"
N LOG DET N,0.8517699115044248,"
= −λr∗
i
λi
,
i ∈[m] ."
N LOG DET N,0.8523230088495575,"Taking limλ→0+, since A∗−B∗≥C0 does not go to zero, we have"
N LOG DET N,0.8528761061946902,"r∗
i
A∗−√zi = 0 ,
i ∈[m] ."
N LOG DET N,0.853429203539823,Then we get
N LOG DET N,0.8539823008849557,"r∗2
i
1 + P"
N LOG DET N,0.8545353982300885,"j∈[m] r∗2
j
= zi ,
i ∈[m] ."
N LOG DET N,0.8550884955752213,Summing all i ∈[m] yields
N LOG DET N,0.855641592920354,∥z∥1 = P
N LOG DET N,0.8561946902654868,"i∈[m] r∗2
i
1 + P"
N LOG DET N,0.8567477876106194,"i∈[m] r∗2
i
."
N LOG DET N,0.8573008849557522,"Therefore, we have"
N LOG DET N,0.8578539823008849,"lim
n,di→+∞
di/n→zi"
N LOG DET N,0.8584070796460177,"V0,d,n = σ2 lim
λ→0+ m
X"
N LOG DET N,0.8589601769911505,"i=1
r∗2
i
= σ2
P"
N LOG DET N,0.8595132743362832,"i∈[m] zi
1 −P"
N LOG DET N,0.860066371681416,"i∈[m] zi
."
N LOG DET N,0.8606194690265486,Under review as a conference paper at ICLR 2022
N LOG DET N,0.8611725663716814,"F
PROOF OF THEOREM 3"
N LOG DET N,0.8617256637168141,"Deﬁne A∗=
qP"
N LOG DET N,0.8622787610619469,"i∈[m] r∗2
i
+ 1 and B∗= P"
N LOG DET N,0.8628318584070797,"i∈[m]
√zir∗
i . Equation (15) in Theorem 2 yields"
N LOG DET N,0.8633849557522124,"r∗
1
r∗
2
= λ1"
N LOG DET N,0.8639380530973452,"λ2
·
√z1A∗−r∗
1
√z2A∗−r∗
2
."
N LOG DET N,0.8644911504424779,"Using the constraint A∗= B∗, we get"
N LOG DET N,0.8650442477876106,"r∗
1
r∗
2
= λ1
 √z1B∗−r∗
1
"
N LOG DET N,0.8655973451327433,"λ2
 √z2B∗−r∗
2
 ."
N LOG DET N,0.8661504424778761,"Deﬁne q = r∗
1
r∗
2 . We have the following equation"
N LOG DET N,0.8667035398230089,"q = λ1
 
q(z1 −1) + √z1z2
"
N LOG DET N,0.8672566371681416,"λ2
 
q√z1z2 + z2 −1
 ."
N LOG DET N,0.8678097345132744,Solving the above equation yields
N LOG DET N,0.8683628318584071,"q = λ1 (z1 −1) + λ2 (1 −z2) +
p"
N LOG DET N,0.8689159292035398,(λ1 (z1 −1) + λ2 (1 −z2)) 2 + 4λ1λ2z1z2
N LOG DET N,0.8694690265486725,"2λ2√z1z2
.
(49)"
N LOG DET N,0.8700221238938053,"Here we discard the negative root. Let x = r∗2
1 + r∗2
2 = r∗2
2
 
1 + q2
. ?? yields"
N LOG DET N,0.870575221238938,"1 + x = r∗2
2 (q√z1 + √z2)2 =
x
1 + q2 (q√z1 + √z2)2 ."
N LOG DET N,0.8711283185840708,Solving x from the above equation gives
N LOG DET N,0.8716814159292036,"x =
q2 + 1
q2(z1 −1) + 2q√z1z2 + z2 −1 ."
N LOG DET N,0.8722345132743363,"Therefore,"
N LOG DET N,0.8727876106194691,"lim
n,di→+∞
di/n→zi"
N LOG DET N,0.8733407079646017,"V0,d,n =
q2 + 1
q2(z1 −1) + 2q√z1z2 + z2 −1 ."
N LOG DET N,0.8738938053097345,"G
PROOF OF THEOREM 4"
N LOG DET N,0.8744469026548672,"Instead of considering the θ∗speciﬁed in Equation (18), we ﬁrst consider a Bayesian setting where
θ∗∼N
 
0, 1"
N LOG DET N,0.875,"dId

. Later, we will show that the setup in Equation (18) is asymptotically (as di →∞)
equivalent to this Bayesian setting. The precise meaning of equivalence will also be presented later.
Our strategy can be divided into two steps. The ﬁrst step is to show that the Bayes risk of the
Bayes estimator is monotonically decreasing in the sample size n. The second step is to translate
the sample-wise monotonicity of the Bayes estimator to the excess risk of the optimally regularized
estimator ˆθλ,d,n in the setup of Equation (18)."
N LOG DET N,0.8755530973451328,"Recall that since we are interested in sample-wise monotonicity, we add a subscript n to X and y
(they are deﬁned by Equation (1) in Section 1.1) to emphasize that they consist of n data items. In
this Bayesian setting, the likelihood function of θ∗is"
N LOG DET N,0.8761061946902655,"L (θ∗| Xn, yn) =
Y"
N LOG DET N,0.8766592920353983,"i∈[n]
L (θ∗| xi, yi) ∝exp  −"
N LOG DET N,0.8772123893805309,"P
i∈[n] (yi −⟨θ∗, xi⟩)2 2σ2 ! = exp "
N LOG DET N,0.8777654867256637,"−∥Xnθ∗−yn∥2
2
2σ2 ! ."
N LOG DET N,0.8783185840707964,"The density of the prior of θ∗is proportional to exp

−d"
N LOG DET N,0.8788716814159292,"2 ∥θ∗∥2
2

. Therefore, the posterior density
of θ∗is given by"
N LOG DET N,0.879424778761062,"p (θ∗| Xn, yn) ∝exp "
N LOG DET N,0.8799778761061947,"−d ∥θ∗∥2
2
2
−∥Xnθ∗−yn∥2
2
2σ2 ! ."
N LOG DET N,0.8805309734513275,Under review as a conference paper at ICLR 2022
N LOG DET N,0.8810840707964602,"As a result, the posterior distribution of θ∗is Gaussian. The Bayes estimator is"
N LOG DET N,0.8816371681415929,"ˆθBayes (Xn,yn) = arg min
θ
Eθ∗∼p(θ∗|Xn,yn) ∥θ −θ∗∥2
Σ ."
N LOG DET N,0.8821902654867256,Taking the derivative with respect to θ gives
N LOG DET N,0.8827433628318584,"∂
∂θEθ∗∼p(θ∗|Xn,yn) ∥θ −θ∗∥2
Σ = 2Σ (θ −θ∗) ."
N LOG DET N,0.8832964601769911,"Setting the above equation to zero yields Σ

ˆθBayes (Xn,yn) −Eθ∗∼p(θ∗|Xn,yn)θ∗
= 0 and there-
fore"
N LOG DET N,0.8838495575221239,"ˆθBayes (Xn,yn) = Eθ∗∼p(θ∗|Xn,yn)θ∗= E [θ∗| Xn, yn] = arg min
θ "
N LOG DET N,0.8844026548672567,"d ∥θ∗∥2
2 + ∥Xnθ∗−yn∥2
2
σ2 ! ."
N LOG DET N,0.8849557522123894,The ﬁnal equality is because the posterior mean of a Gaussian distribution equals its mode.
N LOG DET N,0.8855088495575221,Deﬁne the Bayes risk
N LOG DET N,0.8860619469026548,"Rn ≜Eθ∗∼N(0, 1"
N LOG DET N,0.8866150442477876,"d Id),Xn,yn"
N LOG DET N,0.8871681415929203,"ˆθBayes (Xn, yn) −θ∗
2 Σ 
."
N LOG DET N,0.8877212389380531,Write X = Rd and Y = R. Deﬁne
N LOG DET N,0.8882743362831859,"R′
n ≜
inf
ˆθ:X n×Yn→R
Eθ∗∼N(0, 1"
N LOG DET N,0.8888274336283186,"d Id),Xn,yn
ˆθ (Xn, yn) −θ∗
2 Σ ."
N LOG DET N,0.8893805309734514,We have
N LOG DET N,0.889933628318584,"R′
n =
inf
ˆθ:X n×Yn→Rd Eθ∗∼N(0, 1"
N LOG DET N,0.8904867256637168,"d Id),Xn,yn
Σ1/2ˆθ (Xn, yn) −Σ1/2θ∗
2 2"
N LOG DET N,0.8910398230088495,"=
inf
ˆθ:X n×Yn→Rd Eθ∗∼N(0, 1"
N LOG DET N,0.8915929203539823,"d Id),Xn,yn
ˆθ (Xn, yn) −Σ1/2θ∗
2 2"
N LOG DET N,0.8921460176991151,"= Eθ∗∼N(0, 1"
N LOG DET N,0.8926991150442478,"d Id),Xn,yn
E
h
Σ1/2θ∗| Xn, yn
i
−Σ1/2θ∗
2 2"
N LOG DET N,0.8932522123893806,"= Eθ∗∼N(0, 1"
N LOG DET N,0.8938053097345132,"d Id),Xn,yn
Σ1/2ˆθBayes (Xn, yn) −Σ1/2θ∗
2"
N LOG DET N,0.894358407079646,"2
= Rn ."
N LOG DET N,0.8949115044247787,"where the third equality is because the conditional expectation minimizes the ℓ2 loss. Next, we want
to show that Rn+1 ≤Rn, i.e., the Bayes risk of the Bayes estimator is monotonically decreasing in
the sample size n."
N LOG DET N,0.8954646017699115,"Rn+1 =
inf
ˆθ:X n+1×Yn+1→Rd Eθ∗∼N(0, 1"
N LOG DET N,0.8960176991150443,"d Id),Xn+1,yn+1"
N LOG DET N,0.896570796460177,"ˆθ (Xn+1, yn+1) −θ∗
2 Σ "
N LOG DET N,0.8971238938053098,"≤
inf
ˆθ:X n×Yn→R
Eθ∗∼N(0, 1"
N LOG DET N,0.8976769911504425,"d Id),Xn+1,yn+1"
N LOG DET N,0.8982300884955752,"ˆθ (Xn, yn) −θ∗
2 Σ "
N LOG DET N,0.8987831858407079,= Rn .
N LOG DET N,0.8993362831858407,"Then we want to show that Rn equals the Bayes risk of the optimally regularized estimator ˆθλ,n,d:"
N LOG DET N,0.8998893805309734,"Rn = inf
λ≥0 Eθ∗∼N(0, 1"
N LOG DET N,0.9004424778761062,"d Id),Xn,yn
ˆθλ,n,d −θ∗
2 Σ ."
N LOG DET N,0.900995575221239,"Since Rn = inf ˆθ:X n×Yn→Rd Eθ∗,Xn,yn
ˆθ (Xn, yn) −θ∗
2"
N LOG DET N,0.9015486725663717,"Σ, we get"
N LOG DET N,0.9021017699115044,"Rn ≤inf
λ≥0 Eθ∗∼N(0, 1"
N LOG DET N,0.9026548672566371,"d Id),Xn,yn
ˆθλ,n,d −θ∗
2 Σ ."
N LOG DET N,0.9032079646017699,Under review as a conference paper at ICLR 2022
N LOG DET N,0.9037610619469026,"On the other hand, recalling ˆθBayes (Xn,yn) = arg minθ

d ∥θ∥2
2 + ∥Xnθ−yn∥2
2
σ2

= ˆθ σ2d"
N LOG DET N,0.9043141592920354,"n ,n,d and"
N LOG DET N,0.9048672566371682,"Rn = Eθ∗∼N(0, 1"
N LOG DET N,0.9054203539823009,"d Id),Xn,yn"
N LOG DET N,0.9059734513274337,"ˆθBayes (Xn, yn) −θ∗
2 Σ"
N LOG DET N,0.9065265486725663,"
, we deduce"
N LOG DET N,0.9070796460176991,"Rn ≥inf
λ≥0 Eθ∗∼N(0, 1"
N LOG DET N,0.9076327433628318,"d Id),Xn,yn
ˆθλ,n,d −θ∗
2 Σ ."
N LOG DET N,0.9081858407079646,"Therefore we deduce Rn = infλ≥0 Eθ∗∼N(0, 1"
N LOG DET N,0.9087389380530974,"d Id),Xn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.9092920353982301,"Σ. As a result, we establish"
N LOG DET N,0.9098451327433629,"sample-wise monotonicity of the Bayes risk of optimal regularized ˆθλ,n,d:"
N LOG DET N,0.9103982300884956,"Rn+1 = inf
λ≥0 Eθ∗,Xn+1,yn+1
ˆθλ,n+1,d −θ∗
2"
N LOG DET N,0.9109513274336283,"Σ ≤inf
λ≥0 Eθ∗,Xn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.911504424778761,"Σ = Rn .
(50)"
N LOG DET N,0.9120575221238938,"In what follows, we show that if θ∗is given by Equation (18), the excess risk of ˆθλ,n,d is asymptoti-
cally equal to its Bayes risk when θ∗∼N(0, 1 dId):"
N LOG DET N,0.9126106194690266,"lim
di→∞"
N LOG DET N,0.9131637168141593,"EXn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.9137168141592921,"Σ −Eθ∗∼N(0, 1"
N LOG DET N,0.9142699115044248,"d Id)EXn,yn
ˆθλ,n,d −θ∗
2 Σ = 0 ."
N LOG DET N,0.9148230088495575,"We abuse the notation in the above equation. The θ∗in EXn,yn
ˆθλ,n −θ∗
2"
N LOG DET N,0.9153761061946902,"Σ satisﬁes Equation (18),"
N LOG DET N,0.915929203539823,"while the θ∗in Eθ∗∼N(0, 1"
N LOG DET N,0.9164823008849557,"d Id)EXn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.9170353982300885,"Σ follows a normal distribution N(0, 1"
N LOG DET N,0.9175884955752213,dId). By
N LOG DET N,0.918141592920354,"Lemma 5 and Lemma 6, if Σ = PΛP ⊤and θ′ = P ⊤θ∗are as deﬁned in Table 1 (where P is an
orthogonal matrix and Λ = diag(λ1Id1, . . . , λmIdm) ∈Rd×d is a diagonal matrix), for ﬁxed θ∗we
have"
N LOG DET N,0.9186946902654868,"EXn,yn
ˆθλ,n −θ∗
2"
N LOG DET N,0.9192477876106194,"Σ =EXn,yn """
N LOG DET N,0.9198008849557522,"∥Λ1/2

Id + 1"
N LOG DET N,0.9203539823008849,"nλΛ1/2ZZ⊤Λ1/2
−1
θ′∥2
2 #"
N LOG DET N,0.9209070796460177,"+ σ2EXn,yn
h
∥ΛZ
 
λnIn + Z⊤ΛZ
−1 ∥2
2
i
,"
N LOG DET N,0.9214601769911505,"where every entry of Z ∈Rd×n follows i.i.d. N(0, 1). If θ∗∼N(0, 1"
N LOG DET N,0.9220132743362832,"dId), we have θ′ ∼N(0, 1 dId)."
N LOG DET N,0.922566371681416,"Since the variance term σ2EXn,yn
h
∥ΛZ
 
λnIn + Z⊤ΛZ
−1 ∥2
2
i
does not depend on θ∗, the two
variance terms cancel out and we get"
N LOG DET N,0.9231194690265486,"EXn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.9236725663716814,"Σ −Eθ∗∼N(0, 1"
N LOG DET N,0.9242256637168141,"d Id)EXn,yn
ˆθλ,n,d −θ∗
2 Σ"
N LOG DET N,0.9247787610619469,"=EXn,yn """
N LOG DET N,0.9253318584070797,"∥Λ1/2

Id + 1"
N LOG DET N,0.9258849557522124,"nλΛ1/2ZZ⊤Λ1/2
−1
θ′∥2
2 #"
N LOG DET N,0.9264380530973452,"−EXn,yn,θ′∼N(0, 1 d Id) """
N LOG DET N,0.9269911504424779,"∥Λ1/2

Id + 1"
N LOG DET N,0.9275442477876106,"nλΛ1/2ZZ⊤Λ1/2
−1
θ′∥2
2 #"
N LOG DET N,0.9280973451327433,For U ∼L
N LOG DET N,0.9286504424778761,"i∈[m] Unif (O (di)), we have

Id + 1"
N LOG DET N,0.9292035398230089,"nλΛ1/2ZZ⊤Λ1/2
−1
d=

Id + 1"
N LOG DET N,0.9297566371681416,"nλΛ1/2UZZ⊤U ⊤Λ1/2
−1
= U

Id + 1"
N LOG DET N,0.9303097345132744,"nλΛ1/2ZZ⊤Λ1/2
−1
U ⊤."
N LOG DET N,0.9308628318584071,"By Lemma 13, for θ∗(and thereby θ′) speciﬁed in Equation (18), we get"
N LOG DET N,0.9314159292035398,"EXn,yn """
N LOG DET N,0.9319690265486725,"∥Λ1/2

Id + 1"
N LOG DET N,0.9325221238938053,"nλΛ1/2ZZ⊤Λ1/2
−1
θ′∥2
2 #"
N LOG DET N,0.933075221238938,"= EXn,yn,φ """
N LOG DET N,0.9336283185840708,"∥Λ1/2

Id + 1"
N LOG DET N,0.9341814159292036,"nλΛ1/2ZZ⊤Λ1/2
−1
φ∥2
2 # ,"
N LOG DET N,0.9347345132743363,"where
φ ∼
M"
N LOG DET N,0.9352876106194691,"i∈[m]
Unif
 
Sdi−1 (∥θ′
i∥2)

=
M"
N LOG DET N,0.9358407079646017,"i∈[m]
Unif

Sdi−1 p"
N LOG DET N,0.9363938053097345,"di/d

."
N LOG DET N,0.9369469026548672,Under review as a conference paper at ICLR 2022
N LOG DET N,0.9375,"In the Bayesian setting, if θ′ ∼N(0, 1"
N LOG DET N,0.9380530973451328,"dId), then U ⊤θ′ ∼N(0, 1"
N LOG DET N,0.9386061946902655,dId). We have
N LOG DET N,0.9391592920353983,"Eθ′∼N(0, 1"
N LOG DET N,0.9397123893805309,"d Id),Xn,yn
ˆθλ,n −θ∗
2"
N LOG DET N,0.9402654867256637,"Σ = Eθ′∼N(0, 1"
N LOG DET N,0.9408185840707964,"d Id),Xn,yn  "
N LOG DET N,0.9413716814159292,"Λ1/2U

Id + 1"
N LOG DET N,0.941924778761062,"nλΛ1/2ZZ⊤Λ1/2
−1
U ⊤θ′ 2 2  "
N LOG DET N,0.9424778761061947,"= EXn,yn,ψ  "
N LOG DET N,0.9430309734513275,"Λ1/2

Id + 1"
N LOG DET N,0.9435840707964602,"nλΛ1/2ZZ⊤Λ1/2
−1
ψ  2 2  ,"
N LOG DET N,0.9441371681415929,"where ψ = U ⊤θ′ ∼N(0, 1 dId)."
N LOG DET N,0.9446902654867256,"Next, we want to couple φ and ψ. Let si
i.i.d.
∼Unif
 
Sdi−1(1)

, hi
i.i.d.
∼χ2(di), and deﬁne φ =   p"
N LOG DET N,0.9452433628318584,"d1/ds1
...
p"
N LOG DET N,0.9457964601769911,dm/dsm 
N LOG DET N,0.9463495575221239,",
ψ =   p"
N LOG DET N,0.9469026548672567,"h1/ds1
...
p"
N LOG DET N,0.9474557522123894,hm/dsm  .
N LOG DET N,0.9480088495575221,We have ∥φ∥2 = 1 and
N LOG DET N,0.9485619469026548,∥ψ∥2 =
N LOG DET N,0.9491150442477876,"v
u
u
t m
X i=1 hi d ="
N LOG DET N,0.9496681415929203,"v
u
u
t m
X i=1 di"
N LOG DET N,0.9502212389380531,"d · hi di
,"
N LOG DET N,0.9507743362831859,∥φ −ψ∥2 =
N LOG DET N,0.9513274336283186,"v
u
u
t m
X i=1 di d  1 − r hi
di !2 ."
N LOG DET N,0.9518805309734514,"By the strong law of large numbers, limdi→+∞hi/di
=
1 almost surely.
Thus we get
limdi→+∞,di/d→νi ∥ψ∥2 =
pPm
i=1 νi and limdi→+∞,di/d→νi ∥φ −ψ∥2 = 0 almost surely (re-
call that we will let di →+∞and di/d →νi for some constant νi > 0.
).
Because
Λ1/2
2 ≲1 and

 
Id +
1
nλΛ1/2ZZ⊤Λ1/2−1
2 ≤∥Id∥2 = 1, we bound the norm of"
N LOG DET N,0.952433628318584,"Q ≜Λ1/2  
Id +
1
nλΛ1/2ZZ⊤Λ1/2−1 as follows"
N LOG DET N,0.9529867256637168,"∥Q∥2 ≤
Λ1/2
2 "
N LOG DET N,0.9535398230088495,"
Id + 1"
N LOG DET N,0.9540929203539823,"nλΛ1/2ZZ⊤Λ1/2
−1
2
≲1 ."
N LOG DET N,0.9546460176991151,"It follows that
EXn,yn,φ
h
∥Qφ∥2
2
i
−EXn,yn,ψ
h
∥Qψ∥2
2
i"
N LOG DET N,0.9551991150442478,"≤EXn,yn,φ,ψ
∥Qφ∥2
2 −∥Qψ∥2
2"
N LOG DET N,0.9557522123893806,"=EXn,ynφ,ψ (∥Qφ∥2 + ∥Qψ∥2) |∥Qφ∥2 −∥Qψ∥2|
≲EXn,ynφ,ψ [(∥φ∥2 + ∥ψ∥2) ∥Q(φ −ψ)∥2]
≲EXn,ynφ,ψ ∥φ −ψ∥2 ,"
N LOG DET N,0.9563053097345132,"where the last inequality is because ∥φ∥2 + ∥ψ∥2 ≲1 for all sufﬁciently large di. We know that
limdi→+∞,di/d→νi ∥φ −ψ∥2 = 0 almost surely. To apply Lebesgue’s dominated convergence
theorem, we need to ﬁnd a dominating integrable random variable. In fact, 1 + ∥ψ∥2 dominates
∥φ −ψ∥2:
∥φ −ψ∥2 ≤∥φ∥2 + ∥ψ∥2 = 1 + ∥ψ∥2 ."
N LOG DET N,0.956858407079646,"It is integrable because E ∥ψ∥2 = E
q χ2(d) d 
≤
q"
N LOG DET N,0.9574115044247787,E[χ2(d)]
N LOG DET N,0.9579646017699115,"d
= 1. Application of Lebesgue’s"
N LOG DET N,0.9585176991150443,dominated convergence theorem yields
N LOG DET N,0.959070796460177,"lim
di→+∞,di/d→νi"
N LOG DET N,0.9596238938053098,"EXn,yn,φ
h
∥Qφ∥2
2
i
−EXn,yn,ψ
h
∥Qψ∥2
2
i = 0 ."
N LOG DET N,0.9601769911504425,Under review as a conference paper at ICLR 2022
N LOG DET N,0.9607300884955752,"Therefore, we conclude that"
N LOG DET N,0.9612831858407079,"lim
di→+∞,di/d→νi"
N LOG DET N,0.9618362831858407,"EXn,yn
ˆθλ,n −θ∗
2"
N LOG DET N,0.9623893805309734,"Σ −Eθ∗∼N(0, 1"
N LOG DET N,0.9629424778761062,"d Id)EXn,yn
ˆθλ,n,d −θ∗
2 Σ = 0"
N LOG DET N,0.963495575221239,"and this convergence is uniform in n and λ ∈(0, ∞). It follows that"
N LOG DET N,0.9640486725663717,"lim
n,di→∞
n/di→γi"
N LOG DET N,0.9646017699115044,"EXn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.9651548672566371,"Σ −Eθ∗∼N(0, 1"
N LOG DET N,0.9657079646017699,"d Id)EXn,yn
ˆθλ,n,d −θ∗
2 Σ = 0"
N LOG DET N,0.9662610619469026,"and this convergence is uniform in λ ∈(0, ∞)."
N LOG DET N,0.9668141592920354,"By Lemma 8 (the proof is similar when we replace α →+∞by n, di →∞, n/di →γi), we have"
N LOG DET N,0.9673672566371682,"lim
n,di→∞
n/di→γi"
N LOG DET N,0.9679203539823009,"inf
λ>0 EXn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.9684734513274337,"Σ −inf
λ>0 Eθ∗∼N(0, 1"
N LOG DET N,0.9690265486725663,"d Id)EXn,yn
ˆθλ,n,d −θ∗
2 Σ"
N LOG DET N,0.9695796460176991,"= 0 .
(51)"
N LOG DET N,0.9701327433628318,"Deﬁne fα(λ) = Eθ∗∼N(0, 1"
N LOG DET N,0.9706858407079646,"d Id)EXn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.9712389380530974,"Σ. We use α to denote the indices n, di. By
Lemma 5 and Lemma 6, we have"
N LOG DET N,0.9717920353982301,"Eθ∗∼N(0, 1"
N LOG DET N,0.9723451327433629,"d Id)EXn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.9728982300884956,"Σ ≲Eθ∗∼N(0, 1"
N LOG DET N,0.9734513274336283,"d Id) ∥θ∗∥2
2 = 1 ."
N LOG DET N,0.974004424778761,"Therefore {fα(λ)} is uniformly bounded for λ > 0. Since

d
dλEXn,yn
ˆθλ,n −θ∗
2 Σ"
N LOG DET N,0.9745575221238938,"≲∥θ∗∥2
2 and"
N LOG DET N,0.9751106194690266,"Eθ∗∼N(0, 1"
N LOG DET N,0.9756637168141593,"d Id) ∥θ∗∥2
2 = 1, we have

d
dλEθ∗∼N(0, 1"
N LOG DET N,0.9762168141592921,"d Id)EXn,yn
ˆθλ,n −θ∗
2 Σ"
N LOG DET N,0.9767699115044248,"=
Eθ∗∼N(0, 1"
N LOG DET N,0.9773230088495575,"d Id)
d
dλEXn,yn
ˆθλ,n −θ∗
2 Σ ≲1 ."
N LOG DET N,0.9778761061946902,"As a result, {fα(λ)} is uniformly equicontinuous for λ > 0, and in particular λ ∈(0, M] for any
M > 0. Therefore {fα(λ)} can be extended continuously to [0, M] and the family of extended
functions is still uniformly bounded and uniformly equicontinuous. Recall that if θ∗∼N(0, 1"
N LOG DET N,0.978429203539823,"dId),
we have θ′ ∼N(0, 1"
N LOG DET N,0.9789823008849557,"dId). As in Equation (19), write θ′ in a row-partitioned form θ′ =  
"
N LOG DET N,0.9795353982300885,"θ′
1...
θ′
m  
,"
N LOG DET N,0.9800884955752213,"where θ′
i ∈Rdi. Then ∥Πiθ∗∥2 = ∥θ′
i∥2 ∼
q"
N LOG DET N,0.980641592920354,"χ2(di) d
=
q"
N LOG DET N,0.9811946902654868,χ2(di)
N LOG DET N,0.9817477876106194,"di
· di"
N LOG DET N,0.9823008849557522,"d →√νi as n, di →+∞"
N LOG DET N,0.9828539823008849,"and n/di →γi, where νi =

γi
P"
N LOG DET N,0.9834070796460177,"j∈[m]
1
γj"
N LOG DET N,0.9839601769911505,"−1
. By Theorem 1, {fα(λ)} converges pointwise, say,"
N LOG DET N,0.9845132743362832,"to h(λ, γ1, . . . , γm). By the Arzela-Ascoli theorem, limα fα(λ) = h(λ, γ1, . . . , γm) uniformly on
λ ∈[0, M]. Therefore, as n, di →∞and n/di →γi, by Lemma 8, we have"
N LOG DET N,0.985066371681416,"inf
λ∈[0,M] Eθ∗∼N(0, 1"
N LOG DET N,0.9856194690265486,"d Id)EXn,yn
ˆθλ,n −θ∗
2"
N LOG DET N,0.9861725663716814,"Σ →
inf
λ∈[0,M] h(λ, γ1, . . . , γm) ."
N LOG DET N,0.9867256637168141,"Recalling ˆθBayes (Xn,yn) = arg minθ

d ∥θ∥2
2 + ∥Xnθ−yn∥2
2
σ2

= ˆθ σ2d"
N LOG DET N,0.9872787610619469,"n ,n and"
N LOG DET N,0.9878318584070797,"Rn = Eθ∗∼N(0, 1"
N LOG DET N,0.9883849557522124,"d Id),Xn,yn"
N LOG DET N,0.9889380530973452,"ˆθBayes (Xn, yn) −θ∗
2 Σ"
N LOG DET N,0.9894911504424779,"
= inf
λ≥0 Eθ∗∼N(0, 1"
N LOG DET N,0.9900442477876106,"d Id),Xn,yn
ˆθλ,n,d −θ∗
2 Σ ."
N LOG DET N,0.9905973451327433,For all M > CM := 2σ2 P
N LOG DET N,0.9911504424778761,"i∈[m]
1
γi ≥σ2d"
N LOG DET N,0.9917035398230089,n (recall d n →P
N LOG DET N,0.9922566371681416,"i∈[m]
1
γi ), we have"
N LOG DET N,0.9928097345132744,"inf
λ∈[0,M] Eθ∗∼N(0, 1"
N LOG DET N,0.9933628318584071,"d Id),Xn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.9939159292035398,"Σ = inf
λ≥0 Eθ∗∼N(0, 1"
N LOG DET N,0.9944690265486725,"d Id)EXn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.9950221238938053,"Σ
→
inf
λ∈[0,M] h(λ, γ1, . . . , γm) ."
N LOG DET N,0.995575221238938,Under review as a conference paper at ICLR 2022
N LOG DET N,0.9961283185840708,"The uniqueness of limits implies that infλ∈[0,M] h(λ, γ1, . . . , γm) is independent of M as
long as M
>
σ2.
As a result, if M
>
CM, we have infλ∈[0,M] h(λ, γ1, . . . , γm)
=
infλ≥0 h(λ, γ1, . . . , γm), which yields"
N LOG DET N,0.9966814159292036,"inf
λ≥0 Eθ∗∼N(0, 1"
N LOG DET N,0.9972345132743363,"d Id),Xn,yn
ˆθλ,n,d −θ∗
2"
N LOG DET N,0.9977876106194691,"Σ →inf
λ≥0 h(λ, γ1, . . . , γm) .
(52)"
N LOG DET N,0.9983407079646017,"Equation (50) implies infλ≥0 h(λ, γ1, . . . , γm) is decreasing in every γi. Combining Equation (51)
and Equation (52) gives"
N LOG DET N,0.9988938053097345,"inf
λ≥0 EXn,yn
ˆθλ,n −θ∗
2"
N LOG DET N,0.9994469026548672,"Σ →inf
λ≥0 h(λ, γ1, . . . , γm) ."
