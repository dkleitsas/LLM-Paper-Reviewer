Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00411522633744856,"When training and evaluating machine learning models on a large number of tasks,
it is important to not only look at average task accuracy—which may be biased by
easy or redundant tasks—but also worst-case accuracy (i.e. the performance on the
task with the lowest accuracy). In this work, we show how to use techniques from
the distributionally robust optimization (DRO) literature to improve worst-case
performance in multitask learning. We highlight several failure cases of DRO
when applied off-the-shelf and present an improved method, Lookahead-DRO
(L-DRO), which mitigates these issues. The core idea of L-DRO is to anticipate the
interaction between tasks during training in order to choose a dynamic re-weighting
of the various task losses, which will (i) lead to minimal worst-case loss and (ii)
train on as many tasks as possible. After demonstrating the efﬁcacy of L-DRO on a
small controlled synthetic setting, we evaluate it on two realistic benchmarks: a
multitask version of the CIFAR-100 image classiﬁcation dataset and a large-scale
multilingual language modeling experiment. Our empirical results show that L-
DRO achieves a better trade-off between average and worst-case accuracy with
little computational overhead compared to several strong baselines."
INTRODUCTION,0.00823045267489712,"1
INTRODUCTION"
INTRODUCTION,0.012345679012345678,"Multitask learning—the process by which a single model is trained to perform a variety of different
tasks—has become a subject of increasing interest with many successful applications in a variety of
domains (Ruder, 2017; Yu et al., 2020; Wang et al., 2021). By and large, multitask learning models
are evaluated by reporting their average performance on all tasks (McCann et al., 2018; Wang et al.,
2019). However, average accuracy does not paint the full picture of a multitask model’s performance.
Ensuring comparable accuracy across tasks, including on tasks from under-represented training data
distributions, is an important issue from both practical and fairness perspectives. For example, in
natural language processing, multilingual models often performs worse on languages with smaller
amounts of resources on the web (e.g., Wikipedia) due to the resulting scarcity of relevant training
data (Hu et al., 2020). In computer vision, facial recognition models have been shown to perform
worse on racial groups that are underrepresented in the training data (Buolamwini & Gebru, 2018)."
INTRODUCTION,0.01646090534979424,"In this paper, we argue that multitask models should also be evaluated by their worst-case accuracy.
Yet commonly used task re-weighting based objectives are inadequate to ensure good worst-case
performance. Rather, a more natural choice is to use a type of min-max objective from the distribu-
tionally robust optimization (DRO) literature (Rahimian & Mehrotra, 2019; Sagawa et al., 2020):
minθ maxi ℓi(θ), where ℓi is the loss of the i-th task and θ are the model parameters. Training with
this objective guarantees at least a certain level of performance on all tasks. However, we observe
that DRO does not always work well in practice. Since the min-max objective optimizes the worst
loss, it tends to do so by sacriﬁcing performance on other tasks, which hurts average accuracy (§2.3)."
INTRODUCTION,0.0205761316872428,"Motivated by this observation, we propose a novel optimization algorithm called Lookahead-DRO
(L-DRO). The intuition behind L-DRO is to anticipate the evolution of the loss of each task depending
on which task we train on. Using this information, L-DRO chooses a weight allocation that (i) leads
to minimal worst-case loss and (ii) trains on the maximal number of tasks possible (§2.4)."
INTRODUCTION,0.024691358024691357,"We evaluate L-DRO on a synthetic task, an image classiﬁcation task, and a language modeling task.
In experiments on a multitask version of the CIFAR-100 dataset, we show that L-DRO consistently
allows us to train models that achieve a good trade-off of worst and average task accuracy. On a"
INTRODUCTION,0.02880658436213992,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03292181069958848,"realistic large-scale multilingual language modeling dataset, we ﬁnd that L-DRO is able to improve
performance on the worst-performing languages at very little cost to average performance."
INTRODUCTION,0.037037037037037035,"Our main contributions are as follows: (i) we show how distributionally robust optimization (DRO)
can be applied to multitask learning to maximize worst-case accuracy; (ii) we highlight failure cases
of the standard DRO formulation in the multitask setting; (iii) we propose a new algorithm, L-DRO
that addresses DRO’s deﬁciencies; and (iv) we demonstrate that L-DRO achieves a better trade-off
between average and worst-case accuracy than strong baselines in a synthetic and two realistic
experiments from two different modalities."
TRAINING MULTITASK MODELS,0.0411522633744856,"2
TRAINING MULTITASK MODELS"
TRAINING MULTITASK MODELS,0.04526748971193416,"Consider a machine learning model parameterized by θ ∈Rdmodel, which is trained to perform N
tasks. In general, let a task refer to the combination of examples drawn from a data distribution and
an objective function. For simplicity, we deﬁne a task solely by its objective function ℓi=1...N. As a
running example, we consider a multilingual language model with dozens of tasks where each task
corresponds to a language (Conneau et al., 2020; Xue et al., 2021)."
AVERAGE TASK LOSS,0.04938271604938271,"2.1
AVERAGE TASK LOSS"
AVERAGE TASK LOSS,0.053497942386831275,The most common objective for multitask learning is to minimize the average of all the task losses:
AVERAGE TASK LOSS,0.05761316872427984,"min
θ
1
N N
X"
AVERAGE TASK LOSS,0.06172839506172839,"i=1
ℓi(θ)"
AVERAGE TASK LOSS,0.06584362139917696,"A downside of this objective is that all tasks are treated equally. In real-world scenarios, we often
encounter a scenario where task difﬁculty is not uniform (i.e., the magnitude of ℓi varies wildly;
Hessel et al. 2019) or we want to pay more attention to certain tasks. If the distribution of all tasks
is not homogeneous, training with Lavg results in a model with performance that is skewed towards
certain clusters of tasks. In our multilingual example, if a large number of languages come from the
same language family such as Indo-European languages, Lavg will choose models which perform
better on these languages at the expense of typologically distinct languages such as Mandarin or
Finnish."
STATIC MIXING,0.06995884773662552,"2.2
STATIC MIXING"
STATIC MIXING,0.07407407407407407,"Rather than simple averaging, prior work in multitask learning often uses arbitrary weightings
w = {w1, . . . , wN} in the probablility simplex ∆N = {w | wi > 0, P"
STATIC MIXING,0.07818930041152264,"i wi = 1} to modulate the
losses of individual tasks: min
θ N
X"
STATIC MIXING,0.0823045267489712,"i=1
wi ℓi(θ)."
STATIC MIXING,0.08641975308641975,"The weights are typically decided based on some statistics of the data. In proportional sampling (Sanh
et al., 2019), the weight of a task i is proportional to the size of its data wi ∝|Di|. This biases the
model towards tasks that are over-represented in the data. To counter-balance this, recent approaches
use an inverse temperature α where wi ∝|Di|α with α typically in the range of [0.2, 0.3] (Aharoni
et al., 2019; Conneau et al., 2020; Xue et al., 2021)."
WORST TASK LOSS,0.09053497942386832,"2.3
WORST TASK LOSS"
WORST TASK LOSS,0.09465020576131687,"A third option that considers the task on which the model performs worst is the min-max objective.
We refer to this objective as DRO because of its prominence in the distributional robustness literature
(Sagawa et al., 2020):"
WORST TASK LOSS,0.09876543209876543,"min
θ max
i
ℓi(θ) = min
θ
max
w∈∆N N
X"
WORST TASK LOSS,0.102880658436214,"i=1
wiℓi(θ)"
WORST TASK LOSS,0.10699588477366255,"|
{z
}
LDRO(θ) ."
WORST TASK LOSS,0.1111111111111111,Under review as a conference paper at ICLR 2022
WORST TASK LOSS,0.11522633744855967,Best Solution
WORST TASK LOSS,0.11934156378600823,"Figure 1: Overview of the Lookahead-DRO procedure. Starting from parameters θ0, the optimal
training weights w are chosen such that the resulting parameters θ∗
w minimize the worst case loss."
WORST TASK LOSS,0.12345679012345678,"The above objective minimizes the worst-case loss of the model at every step. Alternatively, we can
view the objective similar to the previous ones as minimizing the loss under a certain task weighting
w. In this case, the weights w are not static, but rather chosen dynamically to maximize the weighted
loss by allocating the maximum weight(s) to the task(s) on which the model performs worst."
WORST TASK LOSS,0.12757201646090535,"The main advantage of minimizing LDRO is that it ensures a certain level of performance on all tasks.
However, it suffers from a number of drawbacks. First, it is sensitive to task difﬁculty: if one task
ℓhard is much more difﬁcult than the others (i.e., minθ ℓhard(θ) > minθ ℓj(θ) for all other tasks j),
then LDRO is likely to get “stuck” trying to minimize ℓhard at the detriment of other tasks."
WORST TASK LOSS,0.13168724279835392,"In addition, DRO is agnostic to positive (or negative) interaction between tasks. In some cases,
training on one task ℓi may have a positive effect on another task ℓj because they share some
similarities—for example if they represent related languages. In such cases it may be desirable to
train on both tasks rather than on one or the other (as it may reduce the variance of the gradient
estimator), even if one has a higher loss than the other. On the other hand, when two tasks are at odds
with each other (i.e., training on one task decreases performance on the other), the max objective in
LDRO can lead to oscillatory behavior where the weights w alternate between two competing tasks."
LOOKAHEAD DRO,0.13580246913580246,"2.4
LOOKAHEAD DRO"
LOOKAHEAD DRO,0.13991769547325103,"The core limitation of LDRO underpinning the aforementioned issues is the fact that it does not take
the optimization procedure into account (i.e., it will bluntly try to minimize the worst loss). This is
suboptimal because in certain cases, taking gradient steps on the worst task might hurt other tasks
disproportionately. If the worst-performing task has converged already, this weight allocation will
also miss out on optimizing other tasks, which can attain lower losses and are still not converged."
LOOKAHEAD DRO,0.1440329218106996,"Formally, for a given value of the model parameters θ0, LDRO chooses weights maximizing the
model’s loss w∗
0 := arg maxw
P"
LOOKAHEAD DRO,0.14814814814814814,"i wiℓi(θ0). After training with weights w∗
0, we obtain parameters
θ1 with loss LDRO(θ1) = maxw
P"
LOOKAHEAD DRO,0.1522633744855967,"i wiℓi(θ1). However, note that for θ1, the worst-case weight
allocation w∗
1 := arg maxw
P"
LOOKAHEAD DRO,0.15637860082304528,"i wiℓi(θ1) might be different than w∗
0, so we have no guarantees that
LDRO(θ1) ≤LDRO(θ0). In other words, training with the worst-case weights w∗
0 does not guarantee
that LDRO will decrease."
LOOKAHEAD DRO,0.16049382716049382,"This observation suggests an alternative strategy: suppose w, θ0 →θw,θ0 is a ﬁxed—and, for
simplicity, deterministic—gradient descent algorithm minimizing the w-weighted loss P"
LOOKAHEAD DRO,0.1646090534979424,"i wiℓi
starting from parameters θ0. Rather than choosing w to be the worst-case weights corresponding to
θ0, we should pick weights that minimize the worst-case loss of θ∗
w,θ0, i.e. after training. This can
be formalized as solving the min-max problem:"
LOOKAHEAD DRO,0.16872427983539096,"min
w∈∆N max
v∈∆N X"
LOOKAHEAD DRO,0.1728395061728395,"i
viℓi(θw,θ0)
(1)"
LOOKAHEAD DRO,0.17695473251028807,"Intuitively, decoupling the weights w used for training from the weights v used for computing
LDRO(θ∗
w,θ0) removes the restriction of just training on the worst-performing tasks and enables us to
ﬁnd a task weighting which is able to continuously minimize the worst-case loss. Note that in this
case there might be multiple solution w: for instance, it might be that one or more tasks have the same
effect on the resulting worst-case task, and as such any permutation of their weights is an optimal
solution. In such scenarios, we do not want to miss out on training on tasks on which performance"
LOOKAHEAD DRO,0.18106995884773663,Under review as a conference paper at ICLR 2022
LOOKAHEAD DRO,0.18518518518518517,"can still be improved (as long as worst-case performance decreases). This can be formulated as a
maximum entropy principle (Jaynes, 1957): if multiple optimal training weights exist, we should pick
the one with the highest entropy."
LOOKAHEAD DRO,0.18930041152263374,"Because ﬁnding the optimal w necessitates “looking ahead” in the optimization trajectory from θ0
to identify the DRO loss maxv
P
i viℓi(θw,θ0), we call this weight selection principle Lookahead-
DRO (L-DRO). We provide a graphical illustration in Figure 1."
ONE-STEP LOOKAHEAD DRO,0.1934156378600823,"2.5
ONE-STEP LOOKAHEAD DRO"
ONE-STEP LOOKAHEAD DRO,0.19753086419753085,"In practice we cannot explicitly solve the min-max game in Eq. 1 because estimating the payoff
maxv
P"
ONE-STEP LOOKAHEAD DRO,0.20164609053497942,"i viℓi(θw,θ0) would require unrolling the full optimization process to compute θ∗
w,θ0. In-
stead, we simplify and take only one gradient step with a small learning rate λ, i.e. replacing θ∗
w,θ0
with θ0 −λ P
i wi∇ℓi(θ0). Such one-step approximations are often used in the meta-learning litera-
ture (Finn et al., 2017; Liu et al., 2018). Using the ﬁrst order Taylor extension of ℓi enables us to
rewrite the individual lookahead task losses as:"
ONE-STEP LOOKAHEAD DRO,0.205761316872428,"ℓi(θ0 −λ[ N
X"
ONE-STEP LOOKAHEAD DRO,0.20987654320987653,"j=1
wj∇ℓj(θ0)]) ≈ℓi(θ0) −λ N
X"
ONE-STEP LOOKAHEAD DRO,0.2139917695473251,"j=1
wj∇ℓi(θ0)⊺∇ℓj(θ0)"
ONE-STEP LOOKAHEAD DRO,0.21810699588477367,"The resulting payoffs are now linear in w, which reduces the lookahead optimization problem in
Eq. 1 into a bilinear matrix game:"
ONE-STEP LOOKAHEAD DRO,0.2222222222222222,"min
w max
v
v⊺L w
(2)"
ONE-STEP LOOKAHEAD DRO,0.22633744855967078,"with a payoff matrix L, called the “lookahead matrix”, which can be decomposed as follows:"
ONE-STEP LOOKAHEAD DRO,0.23045267489711935,"Lij =
ℓi(θ0)
| {z }
Loss of task i"
ONE-STEP LOOKAHEAD DRO,0.2345679012345679,"−λ ∇ℓi(θ0)⊺∇ℓj(θ0)
|
{z
}
“interaction”
between tasks i and j"
ONE-STEP LOOKAHEAD DRO,0.23868312757201646,"We can ﬁnd a solution (Nash equilibrium) of this game using many available solvers such as ﬁctitious
play (Brown, 1951) or linear programming (Lemke, 1965). In such cases where multiple equilibria
exist, we choose the one with the highest entropy as dictated by our maximum entropy principle. The
lookahead learning rate λ is treated as a hyperparameter. This results in a training algorithm which
we outline in Algorithm 1."
ONE-STEP LOOKAHEAD DRO,0.24279835390946503,"Algorithm 1: Lookahead DRO
Input: Lookahead learning rate λ, initial parameters θ0
θ ←θ0
while training is not over do"
ONE-STEP LOOKAHEAD DRO,0.24691358024691357,"L ←[ℓi(θ) −λ∇ℓi(θ)⊺∇ℓj(θ)]ij
// Compute lookahead matrix
v∗, w∗←maxent_nash_solver(L)
// Find Nash equilibrium of L
θ ←optimizer_step( P"
ONE-STEP LOOKAHEAD DRO,0.25102880658436216,"i w∗
i ℓi)
// Optimizer step on w∗-weighted loss
end"
ONE-STEP LOOKAHEAD DRO,0.2551440329218107,"To build more intuitions, we consider several edge cases and discuss how L-DRO handles them:"
ONE-STEP LOOKAHEAD DRO,0.25925925925925924,"Independent tasks. We have ∇ℓ⊺
i ∇ℓj = 1i=j. In this case, for small enough values of the lookahead
learning rate, the optimal strategy for the row player (v) is to pick the task i∗= arg maxi ℓi with the
highest loss, and the only possible best response for the column player (w) is to match this selection
(since Li∗i = ℓ∗
i −λ < Lij = ℓi for all j ̸= i). In other words, we recover the original DRO objective
of training against the worst-case loss. This can be interpreted as an “implicit assumption” of DRO
that training on one task does not affect the others."
ONE-STEP LOOKAHEAD DRO,0.26337448559670784,"Redundant tasks.
We have ∇ℓ⊺
i ∇ℓj = 1. In this case, the gradients of the tasks are perfectly
aligned. In practice, this means that the payoff is independent of the value of w, so any choice of the
training weights is valid. By the maximum entropy principle, we should pick uniform weights, thus
recovering the average task loss baseline."
ONE-STEP LOOKAHEAD DRO,0.2674897119341564,"The highest loss task has already converged.
Consider the case where ℓ0 is maximal, but the
model has already converged on task 0 (in other words ∇ℓ0 = 0). In this scenario—assuming there"
ONE-STEP LOOKAHEAD DRO,0.2716049382716049,Under review as a conference paper at ICLR 2022
ONE-STEP LOOKAHEAD DRO,0.2757201646090535,"is no strong negative interaction among the remaining tasks—the highest loss will always be ℓ0
irrespective of the training weights w. Again, by the maximum entropy principle, L-DRO advocates
for picking uniform weights. This makes it possible to continue training on the remaining tasks even
though ℓ0 has converged, bypassing the core limitations of distributionally robust optimization (DRO)
identiﬁed in §2.3."
LOOKAHEAD DRO IN PRACTICE,0.27983539094650206,"3
LOOKAHEAD DRO IN PRACTICE"
GRADIENT AND LOSS NOISES,0.2839506172839506,"3.1
GRADIENT AND LOSS NOISES"
GRADIENT AND LOSS NOISES,0.2880658436213992,"In most realistic machine learning scenarios we do not have access to the full losses ℓi(θ) and their
gradients. Rather, we compute stochastic estimates on minibatches ˆℓi(θ). In preliminary experiments,
we ﬁnd that this often leads to instability in the training algorithm, since even little noise in the
lookahead matrix can lead to dramatically different Nash equilibria, which causes high ﬂuctuations
of the training weights w∗."
GRADIENT AND LOSS NOISES,0.29218106995884774,"To mitigate this issue, we adopt an online weight update procedure inspired by the online-DRO
algorithm proposed by Sagawa et al. (2020). Instead of computing the optimal weights w∗at each
step based on noisy losses and gradients, we keep a running set of weights w, which we update
jointly with model parameters θ. Speciﬁcally, we interleave gradient updates on θ with exponentiated
gradient steps (Kivinen & Warmuth, 1997) on w to minimize maxv v⊺L w. We show the weight
update rule in Algorithm 2."
GRADIENT AND LOSS NOISES,0.2962962962962963,"Algorithm 2: Lookahead DRO online weight update
Function update_weights(Lookahead matrix L, weights w, weight update rate η):"
GRADIENT AND LOSS NOISES,0.3004115226337449,"v∗←arg maxv v⊺L w
// Compute best response to w
˜wi ←wie−η[v∗⊺L]i
// Update weights
wi ←
˜
wi
P"
GRADIENT AND LOSS NOISES,0.3045267489711934,"j ˜
wj
// Re-normalize weights
return w"
GRADIENT AND LOSS NOISES,0.30864197530864196,"In practice, we ﬁnd that this online weight update alleviates the instability issues mentioned above.
Note that the choice of exponentiated gradient is not arbitrary. It corresponds to a mirror descent step
(Nemirovskij & Yudin, 1983) using the Kullback-Leibler divergence (Kullback & Leibler, 1951) as
a distance function. Choosing weight updates that locally minimize the change in relative entropy
plays into our maximum entropy guiding principle (Jumarie, 1990; Kapur & Kesavan, 1992)."
LOOKAHEAD MATRIX COMPUTATION,0.31275720164609055,"3.2
LOOKAHEAD MATRIX COMPUTATION"
LOOKAHEAD MATRIX COMPUTATION,0.3168724279835391,"The main computational bottleneck of L-DRO lies in the computation of the lookahead matrix L.
Recall that Lij can be decomposed in two terms: the loss ℓi and an “interaction” term ∇ℓi(θ)⊺∇ℓj(θ).
While the former can be estimated essentially “for free” during the forward pass of back-propagation,
computing the pairwise task gradients dot products is much more computationally demanding. Indeed,
it necessitates computing (and holding in memory) n model gradients, which can be prohibitive
for larger models. To reduce the computational overhead, we only update this second term (which
we call the “interaction matrix” A = [∇ℓi(θ)⊺∇ℓj(θ)]i,j=1...N) periodically, typically every 100
or 1000 training steps. In preliminary experiments, we also ﬁnd that Lookahead-DRO works more
reliably when the lookahead gradients of each task are re-normalized to one. This means that the
interaction matrix is actually computed to be Aij = ∇ℓi(θ)⊺(∇ℓj(θ)/∥∇ℓj(θ)∥). The effect of
these two design choices are investigated in ablation studies in Appendices A.1 and A.2. We show
the ﬁnal algorithm for online L-DRO in Algorithm 3."
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.32098765432098764,"4
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT"
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.32510288065843623,"In this section, we demonstrate the efﬁcacy of L-DRO on a synthetic multitask setting. We design a
toy experiment such that neither DRO nor static mixing is able to reach an optimal solution (Figure 2).
We identify each task i with a point yi on the two dimensional plane R2 and deﬁne their respective loss
functions as the squared Euclidean distance of the model prediction f(θ) to yi, ℓi(θ) = ∥yi −f(θ)∥2
2.
We distribute the tasks such that most points are clustered around (−1, 0), to simulate a grouping of
similar tasks (e.g. Indo-Europoean languages in our multilingual example). We then place a single
point youtlier at (1, 0) to emulate the presence of an outlier task."
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.3292181069958848,Under review as a conference paper at ICLR 2022
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.3333333333333333,"Algorithm 3: Online lookahead DRO training loop
Input: Lookahead learning rate λ, weight update rate η, interaction matrix update interval k
A ←0
// Initialize interaction matrix at 0
w ←
  1"
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.3374485596707819,"N · · · 1 N
"
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.34156378600823045,"// Initialize w as uniform weights
while training is not over do"
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.345679012345679,if i% k == 0 then
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.3497942386831276,"A ←
h
∇ℓi(θ)⊺
∇ℓj(θ)
∥∇ℓj(θ)∥
i"
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.35390946502057613,"1≤i,j≤n
// Update interaction matrix"
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.35802469135802467,"L ←ℓ(θ) −λA
// Compute lookahead matrix
w ←update_weights(w, L)
// Update weights using Algorithm 2
θ ←optimizer_step( P"
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.36213991769547327,"i wiℓi)
// Optimizer step on w-weighted loss"
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.3662551440329218,"We simulate the increased difﬁculty of the outlier task by adopting a parameterization f(θ) that
prevents the model from reaching any position closer than a ﬁxed value R from youtlier. Speciﬁcally
we adopt a polar parameterization θ := (r, φ) and set"
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.37037037037037035,"f(θ) = youtlier + (R + r2)

cos φ
sin φ 
."
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.37448559670781895,"This essentially creates a “forbidden region” of radius R around youtlier of the space that the model
cannot penetrate, thus lower-bounding ℓoutlier to R. In experiments, we set R = 1.25."
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.3786008230452675,"Figure 2a depicts the optimization trajectories when the model is trained either with the average
objective or with DRO. As expected, minimizing the average loss leads to a solution that performs
well on the majority of tasks but poorly on the outlier. On the other hand, the DRO trajectory moves
towards the latter but ends up stuck when it reaches a point where ℓoutlier cannot be improved despite
being the highest loss among all tasks."
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.38271604938271603,"On the other hand, while the L-DRO trajectory shown in Figure 2b follows a similar trajectory as
DRO at ﬁrst, it is able to reach a solution that still achieves the best possible ℓoutlier while achieving
lower loss on the other tasks. Also notice that the online variant follows a similar trajectory, despite
the fact that it does not compute a Nash equilibrium at every step."
SYNTHETIC EXPERIMENTS AS A PROOF OF CONCEPT,0.3868312757201646,"Finally, Figure 2c highlights the importance of the online variant in the presence of noise. We emulate
the stochasticity of SGD by adding a small amount of Gaussian noise to both losses and gradients.
Under these conditions, lookahead DRO becomes very unstable, an issue that is partially mitigated
by the online update."
EXPERIMENTS,0.39094650205761317,"5
EXPERIMENTS"
EXPERIMENTS,0.3950617283950617,"5.1
CIFAR100"
EXPERIMENTS,0.3991769547325103,"As a ﬁrst step towards a more realistic setting, we experiment with a multitask version of the CIFAR-
100 dataset (Krizhevsky et al., 2009), in which the 20 coarse labels are treated as separate 5-way
classiﬁcation tasks (Rosenbaum et al., 2018; Yu et al., 2020). In this setting, we train a small CNN
model with four convolution layers with 32 3 × 3 ﬁlters each followed by a ReLU activation and a
2 × 2 max-pooling layer. The resulting representation is averaged and fed through a 3-layer fully
connected feed-forward network with hidden dimension 128 and ReLU activations. Predictions are
made using a shared softmax layer across all tasks (i.e. across all 100 classes)."
EXPERIMENTS,0.40329218106995884,"In addition to L-DRO, we report results for three baselines. First static-mixing, where all tasks
are weighted equally (§2.2). Another natural baseline is DRO (§2.3) using the online algorithm
proposed by Sagawa et al. (2020). Finally we compare to PCGrad (Yu et al., 2020), a multitask
learning approach, which uses gradient level projections to mitigate interferences between tasks
during learning. We train using regular stochastic gradient descent with a batch size of 128, a
learning rate of 0.025 and with Nesterov accelerated gradients with a momentum rate of 0.9. For
DRO, we sweep over 5 values of the hyper-parameter which controls the online weight update
rate {0, 0.0001, 0.001, 0.01, 0.1}. For L-DRO, we sweep over 3 values of the weight update rate
{0.01, 0.001, 0.0001} and 3 values of the lookahead learning rate {0.001, 0.01, 0.1}. For Static
mixing and PCGrad, there are no additional parameters to sweep over."
EXPERIMENTS,0.4074074074074074,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.411522633744856,"2
1
0
1
2 2 1 0 1 2"
EXPERIMENTS,0.4156378600823045,"Average
DRO"
EXPERIMENTS,0.41975308641975306,(a) DRO vs. Average loss
EXPERIMENTS,0.42386831275720166,"2
1
0
1
2 2 1 0 1 2"
EXPERIMENTS,0.4279835390946502,"L-DRO
Online L-DRO"
EXPERIMENTS,0.43209876543209874,(b) L-DRO
EXPERIMENTS,0.43621399176954734,"2
1
0
1
2 2 1 0 1 2"
EXPERIMENTS,0.4403292181069959,"L-DRO
Online L-DRO"
EXPERIMENTS,0.4444444444444444,(c) L-DRO (with noise)
EXPERIMENTS,0.448559670781893,"Figure 2: Synthetic multitask experiment. The outlier task is shown in red while the others are in
yellow. The grayed out area represents the region in space that cannot be attained by the model with
our chosen parameterization"
EXPERIMENTS,0.45267489711934156,"Figure 3: Average/worst task accuracy
Pareto frontiers on multitask CIFAR-100"
EXPERIMENTS,0.4567901234567901,"To gain a better understanding of the various trade-offs
of each approach in terms of average and worst accuracy,
we do not report a single number. Rather, we periodi-
cally evaluate the trained models on the test set and report
the Pareto frontier of all checkpoints (across epochs and
hyper-parameter conﬁgurations) of each method in the two
dimensional plane of worst and average accuracy."
EXPERIMENTS,0.4609053497942387,"We show the results in Figure 2. We can see that L-DRO
(in blue) consistently achieves a good trade-off between
the two metrics, as its Pareto frontier is situated in the
upper right corner of the space (high average and worst-
case accuracy). In particular it is able to reach similar
worst-case performance as DRO, without incurring a hit
in terms of average performance. We ﬁnd that this trend is consistent across different random seeds
(see Appendix B.1)."
EXPERIMENTS,0.46502057613168724,"An important difference in our experimental setting with Yu et al. (2020) is that the model we train
is comparatively much smaller. In fact, as we increase the size of the model, we observe that the
difference between individual method grows thinner (detailed results are depicted in Appendix B.3).
This ﬁnding suggests that L-DRO is primarily useful when the model’s capacity is a limiting factor
compared to the number of tasks and the difﬁculty of individual tasks, which is not necessarily the
case for the simple CIFAR-100 benchmark."
MULTILINGUAL LANGUAGE MODELING,0.4691358024691358,"5.2
MULTILINGUAL LANGUAGE MODELING"
MULTILINGUAL LANGUAGE MODELING,0.4732510288065844,"To evaluate L-DRO in a real-world scenario, we turn to large-scale multilingual language modeling.
As NLP models are increasingly applied to new settings, pretraining on many languages is becoming
a common scenario (Conneau et al., 2020; Xue et al., 2021). Improved pretraining performance has
generally been found to correlate with better downstream performance (Raffel et al., 2020)."
MULTILINGUAL LANGUAGE MODELING,0.4773662551440329,"Furthermore, multilingual language modeling is a particulary interesting testbed because it exhibits
various properties of real world multitask learning scenarios. For example, there is a large data
imbalance across tasks: the highest resource language (English) contains 55× more data than the
lowest resource language (Yorùbá) in our training corpus. Additionally, there is a difference in
difﬁculty between the various languages. Formally, for a parametric language model pθ, its loss on
the i-th language is the cross-entropy H(pi, pθ) with the underlying distribution of the language, pi.
This can be re-written as H(pi) + DKL(pi||pθ). Since the KL divergence is non-negative, the loss
of each language is lower-bounded by the language-speciﬁc entropy term H(pi), irrespective of the
capacity or the ﬁtness of the model. In other words, languages with higher entropy will be more
difﬁcult to learn (Mielke et al., 2019)."
EXPERIMENTAL SETTING,0.48148148148148145,"5.2.1
EXPERIMENTAL SETTING"
EXPERIMENTAL SETTING,0.48559670781893005,"We train a large autoregressive transformer model (Vaswani et al., 2017) on the multilingual C4
dataset (mC4; Xue et al., 2021). We limit ourselves to the 40 languages that are present in the"
EXPERIMENTAL SETTING,0.4897119341563786,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETTING,0.49382716049382713,"XTREME benchmark (Hu et al., 2020), which belong to 12 different language families and are
written in 12 different scripts. Out of all languages, 40% are Indo-European and 42.5% are written in
the Latin script.1 The mC4 dataset comes with a canonical “training” and “evaluation” split. In order
to distinguish between validation and test set, we modify the evaluation split as follows: for each
language we split the evaluation set into a validation and test set containing up to 5M tokens each
(using mT5 subword tokenization; Kudo 2018), or half the entire evaluation set, whichever is largest.
We limit the size of the validation and test sets to facilitate running validation of our models."
EXPERIMENTAL SETTING,0.49794238683127573,"We train each model for 300, 000 steps with a batch size of 256 elements of 512 tokens using the
Adam optimizer (Kingma & Ba, 2014) and a cosine annealed learning rate schedule (Vaswani et al.,
2017). For each run we select the best model in terms of average validation loss on all languages. For
evaluation, we set the sequence length to 1024."
EXPERIMENTAL SETTING,0.5020576131687243,In this experiment we compare against three baselines:2
EXPERIMENTAL SETTING,0.5061728395061729,"• Static mixing (§2.2). We try 3 different values for the inverse temperature hyper-parameter α:
0 (uniform sampling), 0.3 (a common value in the literature; Xue et al. 2021) and 1 (purely
proportional sampling)
• DRO (§2.3), which minimizes the worst-case loss among all languages using the online algorithm
from Sagawa et al. (2020). We sweep over the weight learning rate η ∈{0.01, 0.001, 0.0001}.
• Baselined-DRO, a variation on DRO proposed by Oren et al. (2019) where the losses of each
language i is adjusted with a “baseline” loss to account for the entropy term which biases DRO
towards selecting languages that are inherently more difﬁcult. In practice, we estimate H(pi)
by training a separate monolingual model on each language, pθ∗
i and using its cross-entropy
H(pi, pθ∗
i ) = minθ H(pi, pθ) as a proxy. We then train to minimize maxi ℓi(θ) −H(pi, pθ∗
i )
using the same algorithm as DRO, sweeping over the same values of the weight learning rate. Note
that this particular variant necessitates training an additional model for each language."
RESULTS,0.5102880658436214,"5.2.2
RESULTS"
RESULTS,0.51440329218107,"We report results in Table 1 in terms of perplexity (Jelinek et al., 1977), the standard metric for
language modeling. Perplexity is the exponential of the (token-wise) cross-entropy. As a result,
perplexities of different languages are not directly comparable. Languages with inherently higher
entropy will tend to contribute more to the average (or worst) perplexity. Therefore, we also report
average and worst-case perplexity relative to a monolingual model. Speciﬁcally, the relative perplexity
of a model θ on language i is deﬁned as:"
RESULTS,0.5185185185185185,"Relative perplexity(θ, i) = Perplexity(θ, i) −Perplexity(θ∗
i , i)
Perplexity(θ∗
i , i)
(3)"
RESULTS,0.522633744855967,"where θ∗
i is a monolingual model trained on language i. In other words, relative perplexity of a
given model on a language quantiﬁes how much worse the multilingual model is compared to a
monolingual model trained on the same language (with the same data)."
RESULTS,0.5267489711934157,"Our results show that L-DRO reaches the best average score both in terms of perplexity and relative
perplexity. While DRO is by far the best alternative in terms of worst perplexity, it ranks lowest in
terms of relative perplexity. This is because its focus on languages with high perplexity comes at the
detriment of low-entropy languages. Unsurprisingly, proportional mixing based methods (α = 0.3 or
1) perform very well on high resource languages (e.g. English or French) but poorly on almost all
other languages, leading to low performance along almost all metrics."
RESULTS,0.5308641975308642,"On the other hand, L-DRO performs much better in terms of relative perplexity: it outperforms both
static mixing and DRO in terms of average and worst-case performance. Note that Baselined-DRO,
which takes the entropy of individual languages into account when computing the maximum, reaches
the lowest worst-case relative perplexity. However, it is a much more computationally demanding
approach: it requires training 39 additional monolingual models to estimate the language entropies.
In comparison, L-DRO comes in as a close second in terms of worst-case relative perplexity, with a
comparatively much smaller computational overhead (periodic computation of L), which makes it"
RESULTS,0.5349794238683128,"1In experiments, we omit the Tagalog language, which is not included in mC4.
2We do not compare against PCGrad due to its underwhelming performance in the CIFAR-100 experiments
and the prohibitive cost of its gradient projection step in this setting."
RESULTS,0.5390946502057613,Under review as a conference paper at ICLR 2022
RESULTS,0.5432098765432098,"absolute perplexity
relative perplexity"
RESULTS,0.5473251028806584,"average
worst
average
worst"
RESULTS,0.551440329218107,"Static Mixing (α = 1)
48.7
135.5
358.3
1170.7
Static Mixing (α = 0.3)
24.0
41.2
130.6
213.7
Static Mixing (α = 0)
23.9
48.7
128.2
198.1
DRO
24.5
35.1
142.7
286.7
Baselined-DRO
23.9
50.7
127.3
168.5
L-DRO
23.6
55.5
126.0
191.7"
RESULTS,0.5555555555555556,"Monolingual
10.6
25.3
0
0"
RESULTS,0.5596707818930041,"Table 1: Results on the multilingual language modeling experiment. We report results both in terms
of absolute and relative perplexity."
RESULTS,0.5637860082304527,"a much more appealing alternative. A more detailed discussion of the computational overhead of
L-DRO can be found in appendix C."
RELATED WORK,0.5679012345679012,"6
RELATED WORK"
RELATED WORK,0.5720164609053497,"Much work in multi-task learning has focused on improving the optimization process when learning
many tasks. Some prior work (Sener & Koltun, 2018; Lin et al., 2019; Ma et al., 2020) casts multi-task
learning as multi-objective optimization, with the goal of ﬁnding a Pareto-optimal solution. Other
work proposes to directly modify the gradients. In order to minimize gradient interference, Yu et al.
(2020) project conﬂicting gradients on the normal plane of the other. Wang et al. (2021) alter both the
direction and magnitude of gradients to achieve a certain gradient similarity."
RELATED WORK,0.5761316872427984,"Many other methods have been proposed to weight the task losses according to different criteria such
as uncertainty (Kendall et al., 2018), reward magnitude (Hessel et al., 2019), and learning speed (Chen
et al., 2018; Liu et al., 2019; Zheng et al., 2019). Most of the latter methods increase a task’s loss
weight when the learning speed for that task is low. Similar to learning speed, a few approaches (Guo
et al., 2018; Jean et al., 2019) weight task losses based on performance, focusing on optimizing tasks
with poor performance. Among these, our proposed method is most similar in spirit to performance
weighting-based methods. Such methods, however, do not consider the optimization procedure and
are not aware of task similarity. Loss weighting can be seen as a continuous relaxation of the task
scheduling and sampling so that most task scheduling methods can be easily adapted to loss weighting
methods, and vice versa (Ruder, 2019; Crawshaw, 2020). Kiperwasser & Ballesteros (2018) proposed
different predeﬁned sampling schedules while Sanh et al. (2019) proposed a proportional sampling
strategy. In multilingual language modeling and machine translation (Aharoni et al., 2019; Conneau
et al., 2020; Xue et al., 2021), proportional sampling with a temperature is typically employed."
RELATED WORK,0.5802469135802469,"The distributional robustness literature focuses on training models that perform well on various
domains (Ben-Tal et al., 2009; Rahimian & Mehrotra, 2019). This is generally performed by
minimizing the worst-case expected risk over a pre-determined family called the “uncertainty” set.
There has been substantial work on formulating appropriate uncertainty sets (Hu & Hong, 2013; Gao
& Kleywegt, 2016; Levy et al., 2020). Most relevant to our multitask setting are group-structured
uncertainty sets, where the maximimum is taken over a mixture of sub-populations (Oren et al., 2019;
Sagawa et al., 2020; Zhou et al., 2021)."
CONCLUSION,0.5843621399176955,"7
CONCLUSION"
CONCLUSION,0.588477366255144,"In this paper, we argued for the importance of looking at worst-case performance when training and
evaluating multi-task models, especially in cases where the distribution of tasks is not homogeneous
(e.g., when some tasks are more difﬁcult than others) or when there is an imbalance towards certain
types of tasks. We proposed L-DRO, which enables efﬁcient optimization of the worst-case loss
without the pitfalls of existing DRO-based max-loss minimization approaches. We demonstrated the
beneﬁts of L-DRO in a synthetic scenario as well as in realistic image classiﬁcation and language
modeling experiments. Overall, we encourage the community to evaluate not only on homogeneous
task collections (e.g., CIFAR-100) but also “heterogeneous” multitask learning scenarios (e.g.,
multilingual language modeling) when assessing the performance of a multitask model."
CONCLUSION,0.5925925925925926,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.5967078189300411,ETHICS STATEMENT
ETHICS STATEMENT,0.6008230452674898,"Measuring models only based on their average performance obfuscates their limitations on under-
represented subpopulations and contributes to the marginalization of said groups. Developing
algorithms that work well not just for the average user but for every type of user is thus important
from an ethical perspective."
REPRODUCIBILITY STATEMENT,0.6049382716049383,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.6090534979423868,"We take several steps to ensure reproducibility of our work. Pseudo-code describing the ﬁnal version
of our algorithm is given in Algorithm 3. Moreover, we detail experimental settings and model
hyper-parameters in §5. Finally, all our experiments are performed on datasets that are openly
available: CIFAR-1003 (Krizhevsky et al., 2009) and mC44 (Xue et al., 2021)."
REFERENCES,0.6131687242798354,REFERENCES
REFERENCES,0.6172839506172839,"Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation.
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies (NAACL-HLT), pp. 3874–3884, Minneapo-
lis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1388.
URL https://aclanthology.org/N19-1388."
REFERENCES,0.6213991769547325,"Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization, volume 28.
Princeton University Press, 2009."
REFERENCES,0.6255144032921811,"George W Brown. Iterative solution of games by ﬁctitious play. Activity analysis of production and
allocation, 13(1):374–376, 1951."
REFERENCES,0.6296296296296297,"Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial
gender classiﬁcation. In Conference on fairness, accountability and transparency, pp. 77–91.
PMLR, 2018."
REFERENCES,0.6337448559670782,"Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient
normalization for adaptive loss balancing in deep multitask networks. In International Conference
on Machine Learning, pp. 794–803. PMLR, 2018."
REFERENCES,0.6378600823045267,"Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,
Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-
supervised cross-lingual representation learning at scale.
In Proceedings of the 8th Annual
Meeting of the Association for Computational Linguistics (ACL), pp. 8440–8451, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL
https://aclanthology.org/2020.acl-main.747."
REFERENCES,0.6419753086419753,"Michael Crawshaw. Multi-task learning with deep neural networks: A survey. arXiv preprint
arXiv:2009.09796, 2020."
REFERENCES,0.6460905349794238,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML),
pp. 1126–1135. PMLR, 2017."
REFERENCES,0.6502057613168725,"Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with wasserstein
distance. arXiv preprint arXiv:1604.02199, 2016."
REFERENCES,0.654320987654321,"Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioriti-
zation for multitask learning. In Proceedings of the European Conference on Computer Vision
(ECCV), pp. 270–287, 2018."
REFERENCES,0.6584362139917695,"3https://www.cs.toronto.edu/~kriz/cifar.html
4https://www.tensorflow.org/datasets/catalog/c4#c4multilingual"
REFERENCES,0.6625514403292181,Under review as a conference paper at ICLR 2022
REFERENCES,0.6666666666666666,"Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van
Hasselt. Multi-task deep reinforcement learning with popart. In Proceedings of the 33rd Meeting
of the Association for Advancement of Artiﬁcial Intelligence (AAAI), pp. 3796–3803, 2019."
REFERENCES,0.6707818930041153,"Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson.
XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Gen-
eralization. In Proceedings of the 37th International Conference on Machine Learning (ICML),
2020."
REFERENCES,0.6748971193415638,"Zhaolin Hu and L Jeff Hong. Kullback-leibler divergence constrained distributionally robust opti-
mization. Available at Optimization Online, 2013."
REFERENCES,0.6790123456790124,"Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957."
REFERENCES,0.6831275720164609,"Sébastien Jean, Orhan Firat, and Melvin Johnson. Adaptive scheduling for multi-task learning. arXiv
preprint arXiv:1909.06434, 2019."
REFERENCES,0.6872427983539094,"Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. Perplexity—a measure of the
difﬁculty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1):
S63–S63, 1977."
REFERENCES,0.691358024691358,"Guy Jumarie. Relative information—what for? In Relative Information, pp. 1–11. Springer, 1990."
REFERENCES,0.6954732510288066,"Jagat Narain Kapur and Hiremaglur K Kesavan. Entropy optimization principles and their applications.
In Entropy and energy dissipation in water resources, pp. 3–20. Springer, 1992."
REFERENCES,0.6995884773662552,"Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses
for scene geometry and semantics. In Proceedings of the 31st IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 7482–7491, 2018."
REFERENCES,0.7037037037037037,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
the International Conference on Learning Representations (ICLR), 2014."
REFERENCES,0.7078189300411523,"Eliyahu Kiperwasser and Miguel Ballesteros.
Scheduled multi-task learning: From syntax to
translation. Transactions of the Association for Computational Linguistics, 6:225–240, 2018."
REFERENCES,0.7119341563786008,"Jyrki Kivinen and Manfred K Warmuth. Exponentiated gradient versus gradient descent for linear
predictors. information and computation, 132(1):1–63, 1997."
REFERENCES,0.7160493827160493,"Alex Krizhevsky et al. Learning multiple layers of features from tiny images. Technical report,
Citeseer, 2009."
REFERENCES,0.720164609053498,"Taku Kudo.
Subword regularization: Improving neural network translation models with mul-
tiple subword candidates.
In Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pp. 66–75, Melbourne, Australia,
July 2018. Association for Computational Linguistics.
doi: 10.18653/v1/P18-1007.
URL
https://aclanthology.org/P18-1007."
REFERENCES,0.7242798353909465,"Solomon Kullback and Richard A Leibler. On information and sufﬁciency. The annals of mathemati-
cal statistics, 22(1):79–86, 1951."
REFERENCES,0.7283950617283951,"Carlton E Lemke. Bimatrix equilibrium points and mathematical programming. Management science,
11(7):681–689, 1965."
REFERENCES,0.7325102880658436,"Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally
robust optimization. Proceedings of the 34th Annual Conference on Neural Information Processing
Systems (NeurIPS), 33, 2020."
REFERENCES,0.7366255144032922,"Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning.
Advances in neural information processing systems, 32:12060–12070, 2019."
REFERENCES,0.7407407407407407,"Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In
Proceedings of the International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.7448559670781894,Under review as a conference paper at ICLR 2022
REFERENCES,0.7489711934156379,"Shengchao Liu, Yingyu Liang, and Anthony Gitter. Loss-balanced task weighting to reduce neg-
ative transfer in multi-task learning. In Proceedings of the 33rd Meeting of the Association for
Advancement of Artiﬁcial Intelligence (AAAI), pp. 9977–9978, 2019."
REFERENCES,0.7530864197530864,"Pingchuan Ma, Tao Du, and Wojciech Matusik. Effcient Continuous Pareto Exploration in Multi-Task
Learning. In Proceedings of ICML 2020, 2020."
REFERENCES,0.757201646090535,"Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language
decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018."
REFERENCES,0.7613168724279835,"Sabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian Roark, and Jason Eisner.
What kind
of language is hard to language-model?
In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pp. 4975–4989, Florence, Italy, July 2019. Association
for Computational Linguistics. doi: 10.18653/v1/P19-1491. URL https://aclanthology.
org/P19-1491."
REFERENCES,0.7654320987654321,"Arkadij Semenoviˇc Nemirovskij and David Borisovich Yudin. Problem complexity and method
efﬁciency in optimization. Wiley-Interscience Series in Discrete Mathematics, 1983."
REFERENCES,0.7695473251028807,"Yonatan Oren, Shiori Sagawa, Tatsunori Hashimoto, and Percy Liang. Distributionally robust
language modeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 4227–4237, 2019. URL https://www.aclweb.org/
anthology/D19-1432."
REFERENCES,0.7736625514403292,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Uniﬁed
Text-to-Text Transformer.
Journal of Machine Learning Research, 21, 2020.
URL http:
//arxiv.org/abs/1910.10683."
REFERENCES,0.7777777777777778,"Hamed Rahimian and Sanjay Mehrotra. Distributionally Robust Optimization: A Review. arXiv
preprint arXiv:1908.05659, 2019."
REFERENCES,0.7818930041152263,"Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of
non-linear functions for multi-task learning. In Proceedings of the International Conference on
Learning Representations (ICLR), 2018."
REFERENCES,0.7860082304526749,"Sebastian Ruder. An Overview of Multi-Task Learning in Deep Neural Networks. In arXiv preprint
arXiv:1706.05098, 2017."
REFERENCES,0.7901234567901234,"Sebastian Ruder. Neural transfer learning for natural language processing. PhD thesis, NUI Galway,
2019."
REFERENCES,0.7942386831275721,"Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generalization.
In Proceedings of the International Conference on Learning Representations (ICLR), 2020. URL
https://arxiv.org/pdf/1911.08731.pdf."
REFERENCES,0.7983539094650206,"Victor Sanh, Thomas Wolf, and Sebastian Ruder. A hierarchical multi-task approach for learning
embeddings from semantic tasks. In Proceedings of the 33rd Meeting of the Association for
Advancement of Artiﬁcial Intelligence (AAAI), pp. 6949–6956, 2019."
REFERENCES,0.8024691358024691,"Ozan Sener and Vladlen Koltun. Multi-Task Learning as Multi-Objective Optimization. In Proceed-
ings of NIPS 2018, 2018."
REFERENCES,0.8065843621399177,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st Annual
Conference on Neural Information Processing Systems (NIPS), pp. 5998–6008, 2017."
REFERENCES,0.8106995884773662,"Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
Glue benchmark leaderboard. https://gluebenchmark.com/leaderboard, 2019. URL https:
//gluebenchmark.com/leaderboard. Accessed: 2021-09-24."
REFERENCES,0.8148148148148148,Under review as a conference paper at ICLR 2022
REFERENCES,0.8189300411522634,"Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao. Gradient Vaccine: Investigating and
Improving Multi-task Optimization in Massively Multilingual Models. In Proceedings of ICLR
2021, 2021."
REFERENCES,0.823045267489712,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer.
In Proceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (NAACL-HLT), pp. 483–498, Online,
June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41.
URL https://aclanthology.org/2021.naacl-main.41."
REFERENCES,0.8271604938271605,"Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
Gradient surgery for multi-task learning. In Proceedings of NeurIPS 2020, 2020."
REFERENCES,0.831275720164609,"Feng Zheng, Cheng Deng, Xing Sun, Xinyang Jiang, Xiaowei Guo, Zongqiao Yu, Feiyue Huang, and
Rongrong Ji. Pyramidal person re-identiﬁcation via multi-loss dynamic training. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8514–8522, 2019."
REFERENCES,0.8353909465020576,"Chunting Zhou, Xuezhe Ma, Paul Michel, and Graham Neubig. Examining and combating spurious
features under distribution shift. In Proceedings of the 38th International Conference on Machine
Learning (ICML), 2021."
REFERENCES,0.8395061728395061,Under review as a conference paper at ICLR 2022
REFERENCES,0.8436213991769548,"Figure 4: Average/worst task accuracy Pareto frontiers on multitask CIFAR-100 for L-DRO with
different interaction matrix update intervals. Each plot corresponds to a different random seed."
REFERENCES,0.8477366255144033,"A
ABLATION STUDIES"
REFERENCES,0.8518518518518519,"A.1
EFFECT OF PERIODIC MATRIX UPDATES"
REFERENCES,0.8559670781893004,"We investigate the effect of periodic matrix updates on the results of L-PDRO. Figure 4 reports the
Pareto frontier of L-DRO over a range of hyper-parameters η and λ on the CIFAR-100 experiment
from Section 5.1. We compare frontiers for 3 different interaction matrix update intervals: 10, 100
and 1000, and report results for 3 random seeds."
REFERENCES,0.8600823045267489,"Although there is some variability across different runs, we do not observe that larger intervals are
consistently worse than smaller intervals. That being said, in experiments with larger models, we
have observed some instability with the larger update intervals. This is evidenced in Figure 5a for a
CIFAR-100 run with the larger CNN model used in Section B.3."
REFERENCES,0.8641975308641975,"A.2
EFFECT OF RENORMALIZATION"
REFERENCES,0.8683127572016461,"Note that there of the two components in each element in the interaction matrix Aij = ∇ℓ⊺
i ∇ℓj and
we only normalize one (∇ℓ⊺
i ∇ℓj →∇ℓ⊺
i
∇ℓj
|∇ℓj|)."
REFERENCES,0.8724279835390947,"This can be interpreted as the ﬁrst order approximation of taking a lookahead step along the *normal-
ized* task gradients:"
REFERENCES,0.8765432098765432,"ℓi(θ0 −λ[ N
X"
REFERENCES,0.8806584362139918,"j=1
wj
∇ℓj(θ0)
∥∇ℓj(θ0)∥]) ≈ℓi(θ0) −λ N
X"
REFERENCES,0.8847736625514403,"j=1
wj∇ℓi(θ0)⊺∇ℓj(θ0)"
REFERENCES,0.8888888888888888,∥∇ℓj(θ0)∥
REFERENCES,0.8930041152263375,"Our intuition with this re-normalization trick is to help mitigate the instability caused by the “stal-
eness” of the interaction matrix with periodic updates. Indeed, the approximation error of using a
stale interaction matrix comes from two sources: outdated alignment between the gradient (cosine
similarity) and outdated gradient norms. With the re-normalization trick, we mitigate the latter by
factoring out the norm of the lookahead step while keeping the original interpretation (as explained
above)."
REFERENCES,0.897119341563786,"In practice, we do observe that this helps improve stability for larger matrix update interval. Indeed,
in an ablation study in CIFAR-100 (see Figure 5b), we ﬁnd that the use of the re-normalization trick
removes the instability observed with larger update intervals (1000)."
REFERENCES,0.9012345679012346,"B
ADDITIONAL CIFAR-100 RESULTS"
REFERENCES,0.9053497942386831,"B.1
MULTIPLE RANDOM SEEDS"
REFERENCES,0.9094650205761317,"Figure 6 depicts results on the CIFAR-100 dataset for 3 different random seeds. We ﬁnd that L-DRO
consistently outperforms baselines."
REFERENCES,0.9135802469135802,Under review as a conference paper at ICLR 2022
REFERENCES,0.9176954732510288,"(a) Without renormalization.
(b) With renormalization."
REFERENCES,0.9218106995884774,"Figure 5: Worst-case accuracy training curves for CIFAR-100 with η = 1, λ = 0.001 and interaction
matrix update intervals 10 (blue), 100 (orange) and 1000 (green). The curves are smoothed with a
running exponential average for readability."
REFERENCES,0.9259259259259259,"Figure 6: Average/worst task accuracy Pareto frontiers across 3 random seeds on multitask CIFAR-
100"
REFERENCES,0.9300411522633745,"B.2
ADDITIONAL BASELINE"
REFERENCES,0.934156378600823,"Another natural baseline for trading off worst and average accuracy is to optimize a weighted average
of average and worst-case task loss:"
REFERENCES,0.9382716049382716,"min
θ α × 1 N N
X"
REFERENCES,0.9423868312757202,"i=1
ℓi(θ) + (1 −α) × max
i
ℓi(θ)"
REFERENCES,0.9465020576131687,"with some interpolation parameter α. A downside of this approach is that, for α > 0, the model cannot,
in general, reach the minimal worst-case loss because of the nonzero “average loss” component."
REFERENCES,0.9506172839506173,"Figure 7 shows results including this additional baseline with α = 0.5 (equal weights to both
the average and worst-case loss). We observe that while interpolating between DRO and Static
mixing is occasionally better than either on their own, it doesn’t achieve the same average/worst-case
performance trade-off as L-DRO."
REFERENCES,0.9547325102880658,"B.3
EXPERIMENTS WITH LARGER CNN"
REFERENCES,0.9588477366255144,"In Figure 8, we report CIFAR-100 results across 3 random seeds using a larger CNN model: 3 layers
of 160 3 × 3 ﬁlters each followed by a ReLU activation and a 2 × 2 max-pooling layer, followed
by a 2 layer multilayer perceptron with hidden dimension 320 and ReLU activations leading into
a ﬁnal fully connected layer mapping to the 100 classes. We ﬁnd that the difference between the"
REFERENCES,0.9629629629629629,"Figure 7: Average/worst task accuracy Pareto frontiers across 3 random seeds on multitask CIFAR-
100, including additional baseline."
REFERENCES,0.9670781893004116,Under review as a conference paper at ICLR 2022
REFERENCES,0.9711934156378601,"Figure 8: Average/worst task accuracy Pareto frontiers across 3 random seeds on multitask CIFAR-
100"
REFERENCES,0.9753086419753086,"various method vanishes. Although L-DRO tends to achieve similar robust accuracy as DRO at higher
average accuracy, the difference is small but not consistent across the different runs."
REFERENCES,0.9794238683127572,"C
TIME AND MEMORY OVERHEAD OF L-DRO"
REFERENCES,0.9835390946502057,"The main overhead of L-DRO is the computation of the interaction matrix A. The most straightforward
approach for doing so is to compute the N task gradients and take their pairwise dot-product. The
overhead incurred is equivalent to taking N forward/backward passes (one to obtain each of the task
gradients), as the cost of the pairwise dot-product is marginal in comparison. In theory, this means
that training with L-DRO becomes approximately N/T times slower, where T is the interaction
matrix update interval."
REFERENCES,0.9876543209876543,"We employed this strategy for the CIFAR-100 experiments in Section 5.1. With N = 20 and T = 100
in that setting, the theoretical slowdown is approximately ×1.2. In practice, for our biggest CNN
model on CIFAR-100, training speed was around 27 steps/s for L-DRO vs 28.5 steps/s for ERM (for
reference, PCGrad ran at 21 steps/s). This means that the effective slowdown is approximately 1.06.
We attribute this improvement over the theoretical limit to the optimized Jacobian computation in our
JAX implementation."
REFERENCES,0.9917695473251029,"The downside of this ﬁrst approach is that it necessitates holding N gradients in device memory. This
can quickly become cumbersome when the number of tasks and the size of the model are large. For
the language modeling experiment in Section 5.2, we adopt the alternate —more memory efﬁcient—
strategy of re-computing each dot-product separately. This means the memory overhead drops to
only holding 2 gradients in memory at each time, but we need to compute O(n2) backward passes.
In practice for the language modeling experiment, there N = 49 tasks and, and the interaction matrix
was updated every T = 1000 steps, leading to a theoretical slowdown of approximately ×3.4. By
optimizing the interaction matrix computation (e.g. computing only the upper triangle, using a (2×)
smaller batch size to estimate the gradients), the ﬁnal training speed was 2.2 steps/s for L-DRO, as
opposed to approximately 3.4 steps/s for ERM, i.e. an effective slowdown of ×1.55."
REFERENCES,0.9958847736625515,"Note that another way to reduce device memory usage would be to use the ﬁrst approach and ofﬂoad
the computation of the gradient dot-products to the CPU (where memory is much less of an issue), in
which case the additional overhead comes from the cost of moving gradients from device to CPU
as well as computing large dot-products on CPU. However, we did not experiment with this last
approach."
