Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00398406374501992,"Advances in differentiable numerical integrators have enabled the use of gradient
descent techniques to learn ordinary differential equations (ODEs), where a ﬂexible
function approximator (often a neural network) is used to estimate the system
dynamics, given as a time derivative. However, these integrators can be unsatisfac-
torily slow and unstable when learning systems of ODEs from long sequences. We
propose to learn an ODE of interest from data by viewing its dynamics as a vector
ﬁeld related to another base vector ﬁeld via a diffeomorphism (i.e., a differentiable
bijection). By learning both the diffeomorphism and the dynamics of the base
ODE, we provide an avenue to ofﬂoad some of the complexity in modelling the
dynamics directly on to learning the diffeomorphism. Consequently, by restricting
the base ODE to be amenable to integration, we can speed up and improve the
robustness of integrating trajectories from the learned system. We demonstrate the
efﬁcacy of our method in training and evaluating benchmark ODE systems, as well
as within continuous-depth neural networks models. We show that our approach
attains speed-ups of up to two orders of magnitude when integrating learned ODEs."
INTRODUCTION,0.00796812749003984,"1
INTRODUCTION"
INTRODUCTION,0.01195219123505976,"The problem of ﬁtting an ordinary differential equation (ODE) to observed data is ubiquitous
throughout many disciplines of the natural sciences and engineering (Perko, 1991). Although
traditional approaches have focused on ﬁxed-form system dynamics and inferring their parameters,
here we consider the more general problem of learning ODEs, when their dynamics are completely
unknown. This problem arises, for example, in robot learning where ODEs are often used to
parameterize learned motion (Sindhwani et al., 2018; Singh et al., 2020). In deep learning, this
problem appears within the context of Neural ODEs (Chen et al., 2018), a family of continuous-depth
models where the evolution of hidden states is an ODE. Recent developments in learning ODEs
have allowed the use of differentiable adaptive step-size numerical integrators to train neural network
dynamics via the adjoint method (Chen et al., 2018)."
INTRODUCTION,0.01593625498007968,"Figure 1: Related vector ﬁelds can be thought
of as a vector ﬁeld “morphed” into another.
(Left) Five integral curves (red) of a vector
ﬁeld of a Linear ODE, overlaid on grid points
(blue); (Right) Corresponding ”morphed” in-
tegrals of the related vector ﬁeld and grid."
INTRODUCTION,0.0199203187250996,"The learned ODE system allows us to integrate contin-
uous trajectories at different initial conditions. To roll
out long and complicated trajectories with a numer-
ical integrator, the neural network dynamics model
is queried sequentially at each step. This can be un-
satisfactorily slow for time critical applications, such
as those in robot control, and is known to suffer from
numerical instabilities (Gholami et al., 2019; Choro-
manski et al., 2020). With these challenges in mind,
we propose an alternative approach to learning ODEs:
we view the desired target ODE dynamics as a vec-
tor ﬁeld that is a “morphed” version of an alternative
base vector ﬁeld via a diffeomorphism, i.e., a bijective
mapping where both the mapping and its inverse are
differentiable. Thus, instead of directly modelling
the time derivative of the desired ODE with a neural
network, we use an invertible neural network to learn the diffeomorphism that relates the target ODE"
INTRODUCTION,0.02390438247011952,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.027888446215139442,"to a base ODE. Crucially, by restricting the base ODE to be less complex and more amenable to
integration, we can obtain a solution to the (more complex) target ODE by integrating the simpler
base ODE and passing its solution through the bijection. Figure 1 shows an example of related
vector ﬁelds: the simple integral curves in the left ﬁgure are passed through a diffeomorphism to
give corresponding complicated curves in the right. Unlike rolling out a trajectory, evaluating the
diffeomorphism needs not to be sequential, allowing for efﬁcient GPU computation."
INTRODUCTION,0.03187250996015936,"We investigate restricting the ﬂexibility of the base ODE to improve integration efﬁciency, and
ofﬂoading more of the representation burden to the diffeomorphism. Speciﬁcally, we assume the
dynamics of the base ODE to be: (1) linear or (2) modelled by a neural network. Restricting the base
ODE to be linear allows the computation of closed-forms solutions, providing major speed beneﬁts.
In this set-up we can achieve signiﬁcant speed-ups of up to two orders of magnitude when employing
GPUs, as compared against differentiable integrators with standard settings. We can also restrict the
learned ODE to be provably asymptotically stable by adding simple constraints to the linear base
ODE. Alternatively, when additional ﬂexibility is required, we remove the restrictive assumption
of a linear base ODE and model the dynamics using a neural network. We show that in this setting
we can improve the performance of learning challenging ODEs compared to existing differentiable
integrators, even when we use a simpler neural network for the base ODE."
INTRODUCTION,0.035856573705179286,"In summary, our main contributions are:"
INTRODUCTION,0.0398406374501992,"1. a novel paradigm to learn ODEs from data: invertible neural networks are trained to “morph”
the target ODE to an alternative related base ODE, which can be more tractably integrated;
2. analysis of the base as (i) a linear ODE and (ii) a non-linear ODE with neural network
dynamics. In the linear case, we demonstrate how by restricting the ﬂexibility of the base
ODE, we can obtain closed-form integrals, providing signiﬁcant speed-ups to integration. In
the non-linear case, we demonstrate that by learning ODEs as related vector ﬁelds, we can
ﬂexibly learn challenging ODEs with simpler networks;
3. a principled method to enforce asymptotic stability of learned ODEs, by adding restrictions
to the base ODE."
INTRODUCTION,0.043824701195219126,Proofs and additional details can be found in the appendices.
RELATED WORK,0.04780876494023904,"2
RELATED WORK"
RELATED WORK,0.05179282868525897,"Learning of ODEs and Neural ODEs: Dynamical systems governed by ODEs can be found
throughout many disciplines of science and engineering. Earlier work on approximating free-form
dynamics of ODEs include gradient matching (Ramsay et al., 2006) and using Gaussian processes
(Heinonen et al., 2018). Most recent work on this problem model the unknown dynamics with
a neural network and leverage differentiable numerical integrators, which use the adjoint method
(Pontryagin et al., 1962) to train in a memory-usage tractable manner (see, e.g., Chen et al., 2018)."
RELATED WORK,0.055776892430278883,"A particular usage of ODE learning is within Neural ODEs, which are neural network models that
model the hidden state as continuous ODEs rather than discrete layers (Chen et al., 2018; Massaroli
et al., 2020). Other continuous neural network models which incorporate an ODE, such as latent
ODEs (Rubanova et al., 2019) have found application in time-series tasks. Subsequent strategies have
been introduced to improve the training of these models, including augmenting the ODE state-space
(Dupont et al., 2019), hyper-network extensions (Choromanski et al., 2020), regularisation techniques
(Finlay et al., 2020; Pal et al., 2021), and investigating integrator step-sizes (Ott et al., 2021). At the
core of all neural ODE models is the differentiable integrator used to learn the underlying ODE. Our
proposed approach improves the learning of the underlying ODE, and is compatible with models
that incorporate learnable ODEs. We note that the term “neural ODE” has typically been used in
the literature to refer to neural networks that incorporate ODEs, including the original work in Chen
et al. (2018). However, “neural ODE” has occasionally been used to refer to an ODE with dynamics
parameterized by a neural network (Norcliffe et al., 2021). To disambiguate, throughout our paper,
we refrained from referring to the latter model as “neural ODEs”, but rather as “ODEs with dynamics
parameterized by a neural network”."
RELATED WORK,0.05976095617529881,"Invertible neural networks and Normalizing Flows: Invertible neural networks (INNs) are a class
of function approximators that learn bijections where the forward and inverse mapping and their"
RELATED WORK,0.06374501992031872,Under review as a conference paper at ICLR 2022
RELATED WORK,0.06772908366533864,"Jacobians can be efﬁciently computed (Ardizzone et al., 2019). INNs are typically constructed by
invertible building blocks, such as those introduced in Kingma et al. (2016); Dinh et al. (2017); Durkan
et al. (2019). Advances in INNs are largely motivated by normalizing ﬂows (Rezende & Mohamed,
2015; Papamakarios et al., 2021), an approach to construct a ﬂexible probability distribution by
ﬁnding a differentiable bijection between the target distribution and a base distribution. Our approach
is similar in spirit to normalizing ﬂows, as we analogously aim to learn a diffeomorphism that
relates the vector ﬁelds of the target ODE and some base ODE. However, unlike normalizing ﬂows,
we do not require the burdensome computation of Jacobian determinants (Karami et al., 2019) to
obtain trajectories. A separate line of work, broadly characterized as continuous normalizing ﬂows,
use ODEs to build invertible approximators (Grathwohl et al., 2019; Chen et al., 2018). Our work
proposes the opposite where invertible approximators are used to learn ODEs."
PRELIMINARIES,0.07171314741035857,"3
PRELIMINARIES"
PRELIMINARIES,0.07569721115537849,"In this section we introduce the problem of learning ODE dynamics with neural networks. We then
describe the notions of tangent spaces and pushforwards, which will be used in section 4 to develop
our method."
LEARNING ODES WITH NEURAL NETWORKS,0.0796812749003984,"3.1
LEARNING ODES WITH NEURAL NETWORKS"
LEARNING ODES WITH NEURAL NETWORKS,0.08366533864541832,Consider a dynamical system given by ordinary differential equations of the form:
LEARNING ODES WITH NEURAL NETWORKS,0.08764940239043825,"y′(t) = f(y(t), t),
y(0) = y0,
(1)"
LEARNING ODES WITH NEURAL NETWORKS,0.09163346613545817,"where t is time, y(t) are the states at time t, and f provides the dynamics. Unlike traditional
approaches where f is assumed known with only a few parameters to estimate from data, here we
consider the more general problem where the dynamics are completely unknown. Thus, we can use a
ﬂexible mapping fω as given by a neural network with parameters ω. Henceforth, we will drop the
explicit dependence on time, and consider the autonomous ODEs given by y′(t) = f(y(t)). Non-
autonomous ODEs, which explicitly depend on time, can be equivalently expressed as autonomous
ODEs by adding a dimension to the states y (Davis et al., 2020). For an initial condition yt0 at
start t0, and some end time te, a solution of the ODE can be evaluated by a numerical integrator
(ODESolve), such as Runge-Kutta methods (Butcher, 1987):"
LEARNING ODES WITH NEURAL NETWORKS,0.09561752988047809,"y(te) = yt0 +
Z te"
LEARNING ODES WITH NEURAL NETWORKS,0.099601593625498,"t0
fω(y(t))dt = ODESolve(fω, yt0, te −t0).
(2)"
LEARNING ODES WITH NEURAL NETWORKS,0.10358565737051793,"The learning problem involves estimating, with fω, the dynamics of the ODE, provided nt ob-
servations yobs
t1 . . . yobs
tnt at speciﬁed times. We can learn the ODE by optimising the parame-
ters ω to minimize a loss between the observations at the given times and the integrated ODE,
ℓ(ω) = Loss({yobs
ti }nt
i=1, {y(ti)}nt
i=1), where {y(ti)}nt
i=1 are obtained by solving eq. (2). Advances
in the neural ODE literature have introduced differentiable numerical integrators, which allow
gradient-based techniques to be used to optimize l(ω). Furthermore, by using the adjoint sensitivity
method as outlined in Chen et al. (2018), the gradients of adaptive integrators can be obtained in a
memory tractable manner, without differentiating through the integrator operations."
LEARNING ODES WITH NEURAL NETWORKS,0.10756972111553785,"Nevertheless, the ﬂexibility of neural network dynamics for ODE learning comes at the expense of
a high-computational cost and potential numerical instabilities, especially when considering long
trajectories. We will develop in section 4 an alternative method that transforms this problem into that
of learning a simpler ODE along with a diffeomorphism, by treating the dynamics of the original
(target) ODE and simpler (base) ODE as related vector ﬁelds."
TANGENT SPACES AND PUSHFORWARDS,0.11155378486055777,"3.2
TANGENT SPACES AND PUSHFORWARDS"
TANGENT SPACES AND PUSHFORWARDS,0.11553784860557768,"As mentioned above, we shall be analysing the system dynamics of ODEs as vector ﬁelds. Here we
brieﬂy introduce the differential geometry notions of tangent spaces and pushforwards, which will be
used to deﬁne related vector ﬁelds, a core concept underpinning our methodology."
TANGENT SPACES AND PUSHFORWARDS,0.11952191235059761,"Tangent Spaces: A manifold is a space that locally resembles Euclidean space. Throughout this
paper, all manifolds will be assumed to be differentiable, with deﬁned tangent spaces. For an"
TANGENT SPACES AND PUSHFORWARDS,0.12350597609561753,Under review as a conference paper at ICLR 2022
TANGENT SPACES AND PUSHFORWARDS,0.12749003984063745,"(a) If F maps points p ∈M to F(p) ∈N, a single
tangent vector at p, Xp ∈TpM, can be mapped to
TF(p)N. However, an entire vector ﬁeld X on M
cannot in general be mapped to a valid vector ﬁeld on
N. The pushforward by a diffeomorphism is a special
case where a valid vector ﬁeld can be obtained. TM
TN M
N DF
X F F −1 Y"
TANGENT SPACES AND PUSHFORWARDS,0.13147410358565736,"(b) If vector ﬁelds X and Y on manifolds M and N
respectively are related by diffeomorphism F, then
they are related via the pushforward of F. If Y is
unknown, we have another path to evaluate Y by"
TANGENT SPACES AND PUSHFORWARDS,0.13545816733067728,"N
F −1
−−−→M
X
−→TM
DF
−−→TN."
TANGENT SPACES AND PUSHFORWARDS,0.1394422310756972,Figure 2: Vector ﬁelds can be related by a diffeomorphisms
TANGENT SPACES AND PUSHFORWARDS,0.14342629482071714,"n-dimensional manifold M, at a point p ∈M, the tangent space TpM is an n-dimensional real
vector space, where each element passes p tangentially and is referred to as a tangent vector. The
tangent space provides a higher-dimensional analogue of a tangent plane at a point on a surface. The
collection of tangent spaces for all points on M is known as the tangent bundle denoted by TM."
TANGENT SPACES AND PUSHFORWARDS,0.14741035856573706,"Pushfoward: For a mapping F : M →N between two manifolds, M and N, the pushforward
by F is a linear mapping between the tangent spaces of the manifolds, DpF : TpM →TF (p)N.
Tangent vectors at p in the domain M can be mapped to tangent vectors at the corresponding point
F(p) in the codomain N via the pushforward. This is computed by the matrix product of the Jacobian
of F at p, JF (p), and a tangent vector at p."
METHODOLOGY,0.15139442231075698,"4
METHODOLOGY"
METHODOLOGY,0.1553784860557769,"We study the dynamics of ODEs, f, as vector ﬁelds, and solutions as their integral curves. We model
the desired ODE dynamics as a target vector ﬁeld that is related to another base vector ﬁeld. First,
we introduce the concept of related vector ﬁelds, outline how they can be learned, and elaborate on
the beneﬁts of learning them. Then, we describe possible choices of base vector ﬁeld models."
RELATED VECTOR FIELDS FOR ODE LEARNING,0.1593625498007968,"4.1
RELATED VECTOR FIELDS FOR ODE LEARNING"
RELATED VECTOR FIELDS FOR ODE LEARNING,0.16334661354581673,"A vector ﬁeld X deﬁned on manifold M is a function that assigns a tangent vector Xp ∈TpM
to each point p ∈M. Intuitively, our aim is to construct a mapping F which shapes the manifold
where a base vector ﬁeld X is deﬁned, such that the pushforward of X by F extrinsically appears
“morphed” to match the data."
RELATED VECTOR FIELDS FOR ODE LEARNING,0.16733067729083664,"What are the requirements of these mappings, for the pushforward of vector ﬁelds to be valid?"
RELATED VECTOR FIELDS FOR ODE LEARNING,0.17131474103585656,"Provided a mapping between manifolds F : M →N, we can push a single vector, Xp ∈TpM, to
the tangent space of N at F(p), TF (p)N, via the pushforward, DpF(Xp). Figure 2a sketches out
an example of the pushforward of a tangent vector between tangent spaces. However, this notion
does not extend in general to vector ﬁelds. If F is injective and non-surjective, the pushforward of
X outside the image of F is not deﬁned. If F is surjective and non-injective, there may be multiple
differing pushforwards for a point. In special cases when the pushforward by F deﬁnes a valid vector
ﬁeld on the codomain N, the vector ﬁeld and its pushforward are known to be F-related. Latent
ODEs, as described in (Rubanova et al., 2019), which use an auto-encoder to map input data to a
latent space, where an ODE is learned is similar in spirit to our method. Latent ODEs, however, do
not deﬁne a valid vector ﬁeld in the input space."
RELATED VECTOR FIELDS FOR ODE LEARNING,0.1752988047808765,"Deﬁnition 4.1 (Related vector ﬁelds). Let F : M →N be a smooth mapping of manifolds. A vector
ﬁeld X on M and a vector ﬁeld Y on N are related by F, or F-related, if for all p ∈M,"
RELATED VECTOR FIELDS FOR ODE LEARNING,0.17928286852589642,"DpF(Xp) = YF (p).
(3)"
RELATED VECTOR FIELDS FOR ODE LEARNING,0.18326693227091634,Under review as a conference paper at ICLR 2022
RELATED VECTOR FIELDS FOR ODE LEARNING,0.18725099601593626,"Related vector ﬁelds arise in particular when F is a diffeomorphism, i.e. a bijective mapping,
where both the mapping itself and its inverse are differentiable.
Proposition 4.1 (Proposition 8.19 in Lee (2012)). Suppose F : M →N is a diffeomorphism
between smooth manifolds M, N. For every vector ﬁeld X on M, there is a unique vector ﬁeld Y
on N that is F-related to X."
RELATED VECTOR FIELDS FOR ODE LEARNING,0.19123505976095617,"By considering the F-related properties of vector ﬁelds, we have a pathway to deﬁne unknown vector
ﬁelds using the pushforward of F, as shown in ﬁg. 2b. If vector ﬁeld X on M is F-related to some
vector ﬁeld Y on N, instead of directly evaluating the vector ﬁeld Y , we can instead obtain tangent"
RELATED VECTOR FIELDS FOR ODE LEARNING,0.1952191235059761,"values for any q ∈N, via N
F −1
−−−→M
X
−→TM
DF
−−→TN. Therefore, the vector attached for each
q ∈N is, Yq = DF −1(q)F(XF −1(q)) = JF (F −1(q))XF −1(q), where JF is the Jacobian of F."
DIFFEOMORPHISM LEARNING VIA INVERTIBLE NEURAL NETWORKS,0.199203187250996,"4.2
DIFFEOMORPHISM LEARNING VIA INVERTIBLE NEURAL NETWORKS"
DIFFEOMORPHISM LEARNING VIA INVERTIBLE NEURAL NETWORKS,0.20318725099601595,"The machinery to learn invertible mappings has seen extensive development with the progress
of normalizing ﬂows for estimating probability distributions. Invertible neural networks (INNs,
Ardizzone et al., 2019) are function approximators which learn differentiable bijections. INNs can be
trained on a forward mapping, and get the inverse with no additional work, by the deﬁnition of their
architecture. In this paper, we use INNs of the type described in Dinh et al. (2017). The basic unit is
a reversible block, where inputs are split into two halves, u1 and u2. The outputs v1 and v2 are:
v1 = u1 ⊙exp(s2(u2)) + t2(u2),
v2 = u2 ⊙exp(s1(v1)) + t1(v1),
(4)
where ⊙indicates element-wise multiplication, and t1, t2 and s1, s2 are functions modelled by
fully-connected neural networks with non-linear activations. These expressions are clearly invertible:
u1 = (v1 −t2(u2)) ⊙exp(−s2(u2)),
u2 = (v2 −t1(v1)) ⊙exp(−s1(v1)).
(5)
Note that the functions t1, t2 and s1, s2 themselves are not required to be invertible."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.20717131474103587,"4.3
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.21115537848605578,"Why would it be beneﬁcial to construct a desired vector ﬁeld Y indirectly, by way of a related X?"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.2151394422310757,"We shall answer this by considering integral curves on Y , which represent solutions to the ODE
associated with Y . An integral curve of Y on N is a differentiable curve y : R →N, whose velocity
at each point is equal to the value of Y at that point, i.e. y′(t) = Yy(t) ∈Ty(t)N, for all t ∈R. The
integral curves of F-related vector ﬁelds are also linked by F: integral curves on one vector ﬁeld are
uniquely mapped to the other via a single pass through F, and the Jacobian JF is not required.
Proposition 4.2 (Proposition 9.6 in Lee (2012)). Suppose X and Y are vector ﬁelds on manifolds
M and N respectively. X and Y are related by mapping F : M →N if and only if for each integral
curve x : R →M, y = F(x) is an integral curve of Y ."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.21912350597609562,"In the ODE learning problem outlined in section 3.1, we denote Y to be the vector ﬁeld associated
with the target ODE. During both training and inference, we need to obtain integral curves y of Y
either by numerical integration, or by y = F(x), where x denotes the corresponding integral curve of
X, related to Y via the diffeomorphism F. Critically, the Jacobian of F does not need to be evaluated
when we are working with the integral curves."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.22310756972111553,"If integral curves of X can be found in a more efﬁcient, or less error-prone manner, than by numerically
integrating curves of Y , we can leverage the relationship y = F(x) for ODE learning. This can be
done by an INN, Fθ, with parameters θ. We denote the base ODE as x′(t) = gϕ(x(t)), where ϕ are
parameters. We can then use the target ODE within some learning problem, minimising a loss over
target ODE trajectories and observations:"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.22709163346613545,"ℓ(θ, ϕ) = Loss
 
yobs
ti
	nt
i=1,

Fθ(F −1
θ
(y0) +
Z ti"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.23107569721115537,"0
gϕ(x(t))dt)
	nt
i=1

,
(6)"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.2350597609561753,"where there are nt data time points, y0 is an initial condition for the system, and yobs
ti
are ob-
served data points at times ti.
After training, the dynamics of the target ODE is given by
y′(t) = JFθ(F −1
θ
(y(t)))gϕ(F −1
θ
(y(t))) and the ODE solutions (integrals) are obtained with:"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.23904382470119523,"y(t) = Fθ(F −1
θ
(y0) +
Z t"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.24302788844621515,"0
gϕ(x(t))dt).
(7)"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.24701195219123506,Under review as a conference paper at ICLR 2022
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.250996015936255,"In practice, we are often required to evaluate an entire trajectory, i.e., y(t) at multiple times
t1, . . . , tend with one initial y0, as outlined in Algorithm 1. This allows us to batch up the pass
through Fθ, which makes this highly efﬁcient when executed on a GPU. The beneﬁts of our method
are apparent when it is advantageous to integrate the base ODE and then pass the solution through
the diffeomorphism, Fθ, rather than numerically integrate the target ODE."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.2549800796812749,"Algorithm 1: Efﬁcient integration of learned ODEs
Input
:Fθ, gϕ, y0, t1, . . . , tend
Output :y(t1), . . . , y(tend)"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.2589641434262948,"1 x0 ←F −1
θ
(y0)"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.26294820717131473,"2 x(ti) ←x0 +
R ti
0 gϕ(x(t))dt, for i = 1, . . . , end ;
// The integral is easier to
solve."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.26693227091633465,"3 y(t1), . . . , y(tend) ←Fθ(x(t1), . . . , x(tend)) ;"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.27091633466135456,"// Batched pass through INN can
be efficiently computed on GPUs."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.2749003984063745,"Next, we investigate restricting the ﬂexi-
bility of the base ODE, so that it is more
amenable to integration, ofﬂoading the
complexity of learning to the diffeomor-
phism. We consider two choices of base
ODEs: (1) Linear ODE; (2) ODE with neu-
ral network dynamics."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.2788844621513944,"4.4
LINEAR ODE AS BASE: FAST
INTEGRATION AND STRAIGHTFORWARD
ASYMPTOTIC STABILITY"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.28286852589641437,"We can speed-up integration signiﬁcantly
by modelling the base as a Linear ODE, of
the form x′(t) = Ax(t), where x(t) ∈Rn
are n-dimensional variables, and A ∈Rn×n. Linear ODEs can be solved very efﬁciently as they
admit closed-form solutions. Provided an initial solution x0, the solution of x(t) and that of the target
ODE y(t) are: x(t) = Pn
k=1(lk · x0)rk exp(λkt), and y(t) = Fθ(x(t)),"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.2868525896414343,"where lk, rk and λk are the corresponding left, right eigenvectors and eigenvalues of matrix A,
respectively. We learn the eigenvalues and eigenvectors of matrix A jointly with diffeomorphism Fθ."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.2908366533864542,"Linear ODEs are also interesting because their long-term behavior, which is determined by their
eigenvalues, is easy to analyse. We shall see how this property allows us to craft the long-term behavior
of the target ODE. In particular, in many applications, consideration is given to the asymptotic
properties of ODEs, namely what happens to the solutions after a long period of time. Will they
converge to equilibrium points, periodic orbits, or diverge and ﬂy off? Our method provides a
straightforward way to restrict the learned ODE to be asymptotically stable. In robot motion
generation problems, such as that in Sindhwani et al. (2018), we aim to learn an asymptotically stable
ODE. We begin by deﬁning equilibrium points and asymptotic stability of ﬁrst order ODEs."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.2948207171314741,"Deﬁnition 4.2 (Equilibrium point). An equilibrium point y∗of an ODE y′(t) = f(y(t)), is a point
where f(y∗) = 0."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.29880478087649404,"Deﬁnition 4.3 (Asymptotic stability). An ODE y′(t) = f(y(t), t) is asymptotically stable if for
every solution y(t), there exists a δ > 0, such that whenever ||y(t0) −y∗||< δ, then y(t) →y∗as
t →∞, where y∗is some equilibrium point."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.30278884462151395,"Intuitively, asymptotically stable systems of ODEs will always settle at some equilibrium points after
a long period of time. In the context of vector ﬁelds related by a diffeomorphism, the asymptotic
stability properties of the ODEs are shared."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.30677290836653387,"Theorem 4.1. Suppose two ODEs x′(t) = g(x(t)), y′(t) = f(y(t)) are related via y(t) = F(x(t)),
where F is a diffeomorphism. If the former ODE is asymptotically stable with ne equilibrium points
x∗
1, . . . , x∗
ne, then the latter is also asymptotically stable, with equilibrium points F(x∗
1), . . . , F(x∗
ne)."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.3107569721115538,"Therefore, if we can restrict the base ODE to be asymptotically stable, then the target ODE learned
by our method is also asymptotically stable. When the base is an n dimensional linear ODE, we can
restrict it to be asymptotically stable by directly learning the eigenvalues, λi for i = 1, . . . , n, and
constraining them to be negative. This can be done by setting λi = −(sλi)2 −ε, where ε is a small
positive constant, and learning sλi instead of learning the eigenvalues."
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.3147410358565737,"4.5
NEURAL NETWORK ODE AS BASE: IMPROVED ROBUSTNESS FOR ‘DIFFICULT’ ODES"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.3187250996015936,"Using linear systems as base ODEs provides a dramatic increase in speed at the cost of ﬂexibility.
We observe that the computation overhead of a single backward pass F −1
θ
and a batched single"
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.32270916334661354,Under review as a conference paper at ICLR 2022
ODE SOLUTIONS AS INTEGRAL CURVES OF RELATED VECTOR FIELDS,0.32669322709163345,"forward pass Fθ is minimal when compared with numerical integration which requires sequential
querying. When the ODE is difﬁcult to learn, we can also parameterize the dynamics of the base
ODE using a neural network. This is particularly appealing for ODEs that are considered stiff, with
rapid varying of the solution in time in one dimension, while the other dimensions remain largely
unchanged. Existing differentiable integrators often struggle to directly learn these ODEs. Directly
learning the target ODE will require exceedingly small step-sizes (Hairer et al., 1993). Although by
simply scaling the data before training, we can lessen the stiffness of the ODE to learn, the relatively
rapid changes isolated in a single dimension can still result in the ODE being hard to learn. To tackle
this, we learn a neural network dynamics of the base ODE jointly with the diffeomorphism. The
burden of accurately representing the stiff dynamics is shared by Fθ, providing additional ﬂexibility.
The diffeomorphism can learn to relate the target ODE to a base that is amenable to integration. This
allows us to more accurately learn challenging ODEs, with a much smaller neural network model of
the base ODE, than that used to directly learn the target ODE. As passes of the INN are inexpensive,
we gain a speed-up during integration."
EXPERIMENTAL RESULTS,0.33067729083665337,"5
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.3346613545816733,"We empirically evaluate the ability of our method to speed up the integration of learned ODEs, along
with the robustness of integration when learning difﬁcult ODE systems. Throughout this section,
we compare the error and integration times of our method against a variety of solvers. We include
ﬁxed step-size solvers: Euler’s, midpoint, and Runge-Kutta 45 (RK4), and the adaptive step-size
solvers Dormand–Prince 5 (DOPRI5) and Dormand–Prince 8 (DOPRI8). Unless speciﬁed otherwise,
for ﬁxed step-size solvers, we set the step-size equal to the smallest time increment of outputs. For
adaptive step-size solvers, we set absolute and relative tolerances to 10−5. We augment the ODE
states for all the systems in accordance to Dupont et al. (2019) in all of the ODEs trained during the
experiments, except when recreating latent ODE results from Rubanova et al. (2019), where we use
the original implementation provided by the authors. The differentiable solvers are implemented in
the torchdiffeq library. Details on experiments available in the appendices."
INTEGRATION SPEED-UPS BY LEARNING WITH A LINEAR ODE BASE,0.3386454183266932,"5.1
INTEGRATION SPEED-UPS BY LEARNING WITH A LINEAR ODE BASE"
INTEGRATION SPEED-UPS BY LEARNING WITH A LINEAR ODE BASE,0.3426294820717131,"We test our hypothesis that the availability of a closed-form expression for the integral, when using a
linear base ODE, can provide signiﬁcant integration speed-ups. We evaluate on learning synthetic
ODE systems, real-world robot demonstrations, and within continuous deep learning models. Here,
we report test error/accuracy and integration times. Training times can be found in the appendices."
INTEGRATION SPEED-UPS BY LEARNING WITH A LINEAR ODE BASE,0.3466135458167331,"Learning 3D Lotka-Volterra: We train and evaluate models on data from the 3D Lokta-Volterra
system, which models the dynamics of predator-prey populations. The data is corrupted by white
noise with standard deviation of 0.05. We train our model using a linear base ODE, and assess the
capability of our model in interpolating the data points at 10x the data resolution, and generalizing to
16 hidden test initial conditions to integrate trajectories, also at 10x the data resolution. We report
the integration time for generalization. Figure 3 shows interpolation results and newly generated
trajectories, where we see that our model is able to capture the dynamics of the system. Furthermore,
table 1 provides a quantitative evaluation, where we see that our method is signiﬁcantly faster"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.350597609561753,"3D Lotka-Volterra
Imitation S
Imitation cube pick
Imitation C
MSE (I)
MSE (G)
Time (ms)
MSE (G)
Time (ms)
MSE (G)
Time (ms)
MSE (G)
Time (ms)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.3545816733067729,"Ours (Lin)
0.14± 0.1
1.5± 0.1
9.3± 0.4
6.1±1.2
6.6± 0.2
18.6± 6.2
7.1 ± 1.6
8.1± 1.6
7.5± 0.8
Euler
4.5± 0.3
4.6± 0.1
385.6± 14.4
10.3±2.9
724.7± 8.3
14.9± 1.4
728.4± 9.5
7.3± 2.0
753.9± 1.4
Midpoint
0.38± 0.05
5.51± 0.1
670.4± 31.3
10.9±3.3
581.6± 13.3
12.9± 1.3
1267.2± 13.6
6.9± 2.2
1305.4± 14.7
RK4
0.35± 0.005
5.6± 0.2
1316.1± 30.8
10.3±3.0
2501.7± 18.9
15.9± 0.9
2522.8± 23.1
7.6± 2.7
1292.3± 22.0
DOPRI5
0.93± 0.05
5.19± 0.5
264.7± 17.0
10.8±2.8
1277.7± 14.3
14.9± 0.9
504.0± 12.3
7.1± 1.9
623.4± 15.6"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.35856573705179284,"Table 1: The mean squared error for interpolation, MSE (I), and generalization, MSE (G), and mean
execution times (in ms, ± 1 standard deviation) on the 3D Lotka-Volterra system and the time critical
motion generation for our method with a linear base ODE and competing augmented ODE models,
with neural network dynamics, with various numerical integrators. Our method, with a linear base
ODE, provides comparable or better accuracy, with signiﬁcant integration speed-ups."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.36254980079681276,Under review as a conference paper at ICLR 2022
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.3665338645418327,"than competing approaches using numerical integrators with speed-ups of more than two orders of
magnitude, while achieving comparable or better accuracy."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.3705179282868526,"Ground truth
Pred.
init. cond."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.3745019920318725,"Figure
3:
Learning
the
3D
Lotka–Volterra.
(Left)
Interpolat-
ing (red) data (blue); (Right) Generating
trajectories (red) at hidden test initial
conditions and the ground truth (blue)."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.3784860557768924,"A Time Critical Application—Robot Motion Genera-
tion as Stable ODE Learning:
The ability to quickly
roll-out trajectories is crucial in motion robotics settings.
We consider the application of generating robot manipula-
tor motion trajectories from provided demonstrations. In
particular, Sindhwani et al. (2018) showed that modelling
the motion as trajectories of a stable ODE is critical for
generalizing and being robust to perturbations in initial
conditions. The goal is to learn an ODE system where
trajectories integrated at different starting points mimic
the shown demonstrations. We use three sets of real-world
data from (Khansari-Zadeh & Billard, 2011) of trajectories:
drawing “S” shapes; placing a cube on a shelf; drawing out
large “C” shapes. We use 70% of the data for training, and
test our generalization on the remaining demonstrations.
In these datasets, the motions are known to converge to equilibrium points. Hence, we constrain the
learned ODE to be asymptotically stable. We report the performance and run-times of generalizing to
new starting points in table 1. We see that our approach is competitive in the quality of generalized
trajectories, while achieving speed-ups of more than two-orders of magnitude."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.38247011952191234,"Figure 4: We model latent dynamics of rotating MNIST
“3” characters, as an ODE. We learn the ODE via a diffeo-
morphism and a linear base ODE. Given an unseen test “3”
character, we efﬁciently generate a image sequence of rotat-
ing “3”s (illustrated here, from upper left to lower right)."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.38645418326693226,"ODE
Learning
for
Continuous
Deep Learning Models: We evaluate
our method as a component of Latent
ODEs (Rubanova et al., 2019), a con-
tinuous deep learning model. Latent
ODEs embed the time series observa-
tions as hidden states via an encoder-
decoder. An ODE, with dynamics pa-
rameterized by a neural network, is ﬁt
on the hidden states which allows for
irregularly sampled series. In these ex-
periments, our method is applied with
a linear ODE base to learn the dynamics governing the hidden states. We report results for the periodic
curves and the human activity classiﬁcation problem used in the original latent ODE paper (Rubanova
et al., 2019), as well as for ECG classiﬁcation and image sequence generation. The periodic curves
problem requires us to reconstruct trajectories at different resolutions, with 100 and 1000 time-steps.
The human activity classiﬁcation dataset contains readings over the body of subjects over time,
and the problem has a sequence-to-sequence setup, requiring us to predict the human activity at
each time point. The ECG problem is a sequence classiﬁcation problem, where the sequences are
ECG signals. In the image sequence generation problem, we train on 100 sequences of rotating “3”
characters, similar to that in Yildiz et al. (2019), learning a high-dimensional ODE which governs
a latent representation of the image rotation over time. We test on 100 unseen “3” characters and
report results at 10x training resolution. The performance and times spent on integrating the hidden
state dynamics are reported in table 2. We observe that by leveraging the closed-form expression of
integrals of the base ODEs, we achieve ODE integration times that are hundreds of times faster, while
achieving competitive performances against compared differentiable integrators. We note that the
main cost of the integral in our method is the pass through the invertible neural network. GPUs allow
us to batch the pass at practically constant cost, whereas the sequential nature of integrators give
linear increases in run-time. A qualitative evaluation of our method on the image sequence problem is
shown in ﬁg. 4. We observe the structure of the rotated “3” character remains consistent with the test
initial image. Our approach, when restricted to a linear base ODE, is able to learn high-dimensional
ODEs which can be integrated signiﬁcantly faster, without compromising performance."
ROBUST INTEGRATION BY LEARNING WITH A NON-LINEAR NEURAL NETWORK BASE,0.3904382470119522,"5.2
ROBUST INTEGRATION BY LEARNING WITH A NON-LINEAR NEURAL NETWORK BASE"
ROBUST INTEGRATION BY LEARNING WITH A NON-LINEAR NEURAL NETWORK BASE,0.3944223107569721,"We test our hypothesis that using a neural network base ODE allows us to learn ODEs that are
difﬁcult to integrate. We learn and evaluate models trained on the chaotic Lorenz system, and the stiff"
ROBUST INTEGRATION BY LEARNING WITH A NON-LINEAR NEURAL NETWORK BASE,0.398406374501992,Under review as a conference paper at ICLR 2022
ROBUST INTEGRATION BY LEARNING WITH A NON-LINEAR NEURAL NETWORK BASE,0.40239043824701193,"Periodic 100
Periodic 1000
Human Activity
ECG
Image Seq.
MSE
Int. time
MSE
Int. time
Acc.
Int. time
Acc.
Int. time
MSE
Int. time
Ours (Lin)
0.030
2.7± 0.6
0.008
2.8± 0.8
0.864
4.2± 1.8
0.966
7.6± 2.5
0.028
5.7 ± 1.3
Euler
0.040
33.7± 2.6
0.043
326.6± 9.5
0.815
67.9± 2.9
0.963
100.0± 2.8
0.028
103.3±2.5
Midpoint
0.032
54.5± 1.8
0.074
510.1± 15.5
0.865
114.2± 2.4
0.963
169.7± 3.8
0.026
187.9±6.8
RK4
0.039
95.6± 1.6
0.052
1020.0± 60.0
0.857
221.2± 4.2
0.963
325.5± 4.8
0.027
401.2±9.1
DOPRI5
0.045
83.4± 2.2
0.050
264.7± 4.6
0.869
67.9± 5.0
0.963
123.3± 2.6
0.025
194.5± 9.2
DOPRI8
0.041
99.6± 2.3
0.049
282.7± 6.4
0.724
94.8± 1.6
0.963
171.6± 3.6
0.026
399±28.4"
ROBUST INTEGRATION BY LEARNING WITH A NON-LINEAR NEURAL NETWORK BASE,0.4063745019920319,"Table 2: The mean squared error, accuracy and mean integration times (in ms, ± 1 standard deviation)
when using latent ODEs on the tasks of periodic curve reconstruction using 100 and 1000 time-steps,
the classiﬁcation problems of human activity and ECG, and the image sequence generation with our
method using a linear base ODE and competing numerical integrators."
ROBUST INTEGRATION BY LEARNING WITH A NON-LINEAR NEURAL NETWORK BASE,0.4103585657370518,"1e-6
1e-4
1e-2
1
1e2
1e4
1e6
Time 0 1 2 3"
E,0.41434262948207173,"1e
5
ROBER system y2(t)"
E,0.41832669322709165,"GT data
Dopri5
Ours"
E,0.42231075697211157,"Lorenz
ROBER
MAE
Time(ms)
MAE
Time(ms)"
E,0.4262948207171315,"Ours
0.20
230±9
0.01
157±8
Euler
10.99
456± 13
0.22
201±6
Midpoint
6.60
805± 43
0.044
340±11
RK4
6.81
1761± 206
0.041
660±18
DOPRI5
7.55
632± 83
0.039
189±7"
E,0.4302788844621514,"Figure 5: Results on chaotic (Lorenz) and stiff (ROBER) systems. Left: A trajectory from the learned
Lorentz system in red, with data in blue. Center: The second dimension of the ROBER system,
against time in logscale. Our method is much capable at capturing the sudden variation. Right:
Quantitative evaluation of our method with a non-linear base ODE and competing integrators on
augmented ODEs with neural network dynamics."
E,0.4342629482071713,"Robertson’s system (ROBER, Robertson, 1966). We use a DOPRI5 adaptive step-size integrator to
learn our base ODE, generate trajectories at 10x data resolution. Figure 5 (left) illustrates a generated
trajectory from the Lorenz system, which closely resembles the data points (blue). We can see that
both the initial large and small variations, indicated by the small dense spiral at the end, are captured.
Figure 5 (center) illustrates the particular rapidly changing second dimension of the ROBER system.
We see that the added ﬂexibility of the diffeomorphism allows us to better capture the rapid variations
over time, while the directly learned ODE struggles to handle the sudden increase. Following the
equation rescaling described in Kim et al. (2021), before we train on our method and comparisons,
we rescale our data by the maximum training value in each dimension and operate in logscale time.
Figure 5 (right) provides the performance and integration times of learning with our method and
baseline integrators, where we see that our method is more accurate than competing approaches while
also requiring shorter integration times, due to having a simpler network modelling the dynamics."
CONCLUSIONS,0.43824701195219123,"6
CONCLUSIONS"
CONCLUSIONS,0.44223107569721115,"We have proposed a novel approach to learning ODEs with unknown dynamics, which uses invertible
neural networks to learn a diffeomorphism relating a desired target ODE to a base ODE that is often
easier to integrate. We have investigated using a base ODE that is linear or parameterized by a neural
network. By leveraging the closed form solution of linear ODEs, our method provides signiﬁcant
speed-ups and allows for asymptotic property constraints on the learned ODEs. Alternatively, by
using a base ODE parameterized by a neural network, our approach can learn “difﬁcult” ODEs,
with simpler networks modeling their dynamics. We have validated our method by learning ODEs
on synthetic and real-world data, on robotic learning problems and within continuous-depth neural
network models. Future work could explore more on how to balance ofﬂoading the burden of learning
to the diffeomorphism and the base ODE."
REPRODUCIBILITY STATEMENT,0.44621513944223107,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.450199203187251,"It is extremely important that the work published in ICLR is reproducible. To this end, we have
included source code for our experiments, including code to generate the benchmark dynamical
systems, as supplementary materials to the submission. Furthermore, additional details of experiments"
REPRODUCIBILITY STATEMENT,0.4541832669322709,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.4581673306772908,"and data, along with clear explanations of any assumptions and a complete proof of the theoretical
claims are included in the appendix."
REFERENCES,0.46215139442231074,REFERENCES
REFERENCES,0.46613545816733065,"Lynton Ardizzone, Jakob Kruse, Sebastian Wirkert, Daniel Rahner, Eric W. Pellegrini, Ralf S. Klessen,
Lena Maier-Hein, Carsten Rother, and Ullrich Köthe. Analyzing inverse problems with invertible
neural networks. In International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.4701195219123506,"J. C. Butcher. The Numerical Analysis of Ordinary Differential Equations: Runge-Kutta and General
Linear Methods. Wiley-Interscience, 1987."
REFERENCES,0.47410358565737054,"Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary
differential equations. Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.47808764940239046,"Krzysztof M Choromanski, Jared Quincy Davis, Valerii Likhosherstov, Xingyou Song, Jean-Jacques
Slotine, Jacob Varley, Honglak Lee, Adrian Weller, and Vikas Sindhwani. Ode to an ode. In
Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.4820717131474104,Kaggle Dataset. ECG heartbeat categorization dataset. 2018.
REFERENCES,0.4860557768924303,"Jared Quincy Davis, Krzysztof Choromanski, Jake Varley, Honglak Lee, Jean-Jacques Slotine,
Valerii Likhosterov, Adrian Weller, Ameesh Makadia, and Vikas Sindhwani. Time dependence in
non-autonomous neural odes. ICLR 2020 Workshop DeepDiffEq, 2020."
REFERENCES,0.4900398406374502,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In
International Conference on Learning Representations (ICLR), 2017."
REFERENCES,0.4940239043824701,"Asen Dontchev and R Rockafellar. Implicit Functions and Solution Mappings: A View from Varia-
tional Analysis. 2009. ISBN 978-0-387-87820-1. doi: 10.1007/978-0-387-87821-8."
REFERENCES,0.49800796812749004,"Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In Advances in Neural
Information Processing Systems, 2019."
REFERENCES,0.50199203187251,"Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline ﬂows. In
Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.5059760956175299,"Chris Finlay, Joern-Henrik Jacobsen, Levon Nurbekyan, and Adam Oberman. How to train your
neural ODE: the world of Jacobian and kinetic regularization. In International Conference on
Machine Learning, 2020."
REFERENCES,0.5099601593625498,"Amir Gholami, K. Keutzer, and G. Biros. Anode: Unconditionally accurate memory-efﬁcient
gradients for neural odes. In IJCAI, 2019."
REFERENCES,0.5139442231075697,"Will Grathwohl, Ricky T. Q. Chen, J. Bettencourt, Ilya Sutskever, and D. Duvenaud. Ffjord: Free-
form continuous dynamics for scalable reversible generative models. International Conference on
Learning Representations (ICLR), 2019."
REFERENCES,0.5179282868525896,"E. Hairer, S. P. Nørsett, and G. Wanner. Solving Ordinary Differential Equations I (2nd Revised. Ed.):
Nonstiff Problems. Springer-Verlag, Berlin, Heidelberg, 1993."
REFERENCES,0.5219123505976095,"Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David
Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti
Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández
del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy,
Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming
with NumPy. Nature, 585(7825), 2020."
REFERENCES,0.5258964143426295,"Markus Heinonen, Çagatay Yildiz, Henrik Mannerström, Jukka Intosalmi, and H. Lähdesmäki.
Learning unknown ode models with gaussian processes. In International Conference on Machine
Learning, 2018."
REFERENCES,0.5298804780876494,"Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 2nd edition,
2012."
REFERENCES,0.5338645418326693,Under review as a conference paper at ICLR 2022
REFERENCES,0.5378486055776892,"J. D. Hunter. Matplotlib: A 2d graphics environment. Computing In Science & Engineering, 2007."
REFERENCES,0.5418326693227091,"Mahdi Karami, Dale Schuurmans, Jascha Sohl-Dickstein, Laurent Dinh, and Daniel Duckworth.
Invertible convolutional ﬂow. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.545816733067729,"S. M. Khansari-Zadeh and A. Billard. Learning stable nonlinear dynamical systems with gaussian
mixture models. IEEE Transactions on Robotics, 2011."
REFERENCES,0.549800796812749,"Suyong Kim, Weiqi Ji, Sili Deng, Yingbo Ma, and Christopher Rackauckas. Stiff neural ordinary
differential equations. Chaos, 2021."
REFERENCES,0.5537848605577689,"Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive ﬂow. In Advances in Neural Information
Processing Systems, 2016."
REFERENCES,0.5577689243027888,"John Lee. Introduction to smooth manifolds. 2nd revised ed. Springer, 2012."
REFERENCES,0.5617529880478087,"S. Lefschetz and R. Alverson. Stability by liapunov’s direct method with applications. Physics Today,
1962."
REFERENCES,0.5657370517928287,"Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. Dissecting
neural odes. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.5697211155378487,"Alexander Norcliffe, Cristian Bodnar, Ben Day, Jacob Moss, and Pietro Liò. Neural ode processes.
In International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.5737051792828686,"K. Ott, P. Katiyar, P. Hennig, and M. Tiemann. Resnet after all: Neural odes and their numerical
solution. In International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.5776892430278885,"Avik Pal, Yingbo Ma, Viral B. Shah, and Chris Rackauckas. Opening the blackbox: Accelerating
neural differential equations by regularizing internal solver heuristics. In International Conference
on Machine Learning, 2021."
REFERENCES,0.5816733067729084,"George Papamakarios, Eric T. Nalisnick, Danilo Jimenez Rezende, S. Mohamed, and Balaji Laksh-
minarayanan. Normalizing ﬂows for probabilistic modeling and inference. Journal of Machine
Learning Research, 2021."
REFERENCES,0.5856573705179283,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems 32. 2019."
REFERENCES,0.5896414342629482,"Lawrence Perko. Differential Equations and Dynamical Systems. Springer-Verlag, 1991."
REFERENCES,0.5936254980079682,"Lev Semenovich Pontryagin, EF Mishchenko, VG Boltyanskii, and RV Gamkrelidze. The mathemat-
ical theory of optimal processes. 1962."
REFERENCES,0.5976095617529881,"J. Ramsay, G. Hooker, D. Campbell, and Jiguo Cao. Parameter estimation for differential equations:
a generalized smoothing approach. Journal of The Royal Statistical Society Series B-statistical
Methodology, 2006."
REFERENCES,0.601593625498008,"Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In International
Conference on Machine Learning, pp. 1530–1538, 2015."
REFERENCES,0.6055776892430279,H.H. Robertson. The solution of a set of reaction rate equations. 1966.
REFERENCES,0.6095617529880478,"Yulia Rubanova, Ricky T. Q. Chen, and D. Duvenaud. Latent odes for irregularly-sampled time series.
Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.6135458167330677,"Vikas Sindhwani, Stephen Tu, and Mohi Khansari. Learning contracting vector ﬁelds for stable
imitation learning, 2018."
REFERENCES,0.6175298804780877,Under review as a conference paper at ICLR 2022
REFERENCES,0.6215139442231076,"Sumeet Singh, Vikas Sindhwani, Jean-Jacques E. Slotine, and Marco Pavone. Learning stabilizable
dynamical systems via control contraction metrics. In Algorithmic Foundations of Robotics XIII,
2020."
REFERENCES,0.6254980079681275,"Cagatay Yildiz, Markus Heinonen, and Harri Lahdesmaki. Ode2vae: Deep generative second order
odes with bayesian neural networks. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.6294820717131474,Under review as a conference paper at ICLR 2022
REFERENCES,0.6334661354581673,"The following sections contain the supplementary text which gives additional results, additional
details about the experiments, and proofs."
REFERENCES,0.6374501992031872,"A
ADDITIONAL RESULTS: REPRESENTATION LOAD OF THE INN"
REFERENCES,0.6414342629482072,"In the setup of using a non-linear neural network base ODE, both the INN and neural network
dynamics are trained jointly. We empirically investigate the representation burden on the invertible
neural network, when using a non-linear neural network base ODE. We train, for 5000 epochs,
augmented ODEs with neural network dynamics models of different parameter sizes, without the
diffeomorphism, on the Lorentz system and report the MAE values of integrating with the dopri5
integrator with the same setup as the Lorentz experiments in Section 5.2. We then compare take the
neural network dynamics model with the smallest number of parameters (with a parameter count of
2256), and add INNs with an increasing number of invertible layers. The results of the ODE with
neural network parameters only, with increasing parameter counts, are tabulated in table 3, while the
results of the neural networks with an increasing number of invertible layers are tabulated in table 4."
REFERENCES,0.6454183266932271,"Each invertible layer contains 15090 parameters. We note here that the querying the INN (and
thereby having a large INN) adds negligible integration time, as during the entire integration we only
need one forward and one inverse pass of the INN, while the system dynamics needs to be queried
sequentially. We see that, in general, adding parameters to the system dynamics of a learned ODE
and to the diffeomorphism adds to the representation power of our ODE models. However, we clearly
see that the addition of an INN improves performance, and additional layers to the INN, up to around
5 invertible layers, improves performance. We see that the model with 5 invertible layers and a base
ODE of 2256 parameters greatly outperforms the ODE model with only a neural network dynamics
of 83006, while the two models are similar in parameter count. This indicates that the invertible
neural network plays a large role in representing the learned ODE."
REFERENCES,0.649402390438247,"NN-dynamics parameter count
2256
8106
17556
30606
47256
67506
83006
MAE
10.95
10.94
10.94
8.04
7.35
7.06
7.08"
REFERENCES,0.6533864541832669,"Table 3: The MAE of interpolating trajectories from the Lorentz system, with ODEs using neural
networks of various sizes."
REFERENCES,0.6573705179282868,"Number of INN Layers
0
1
2
3
4
5
6
7
MAE
10.95
2.9
2.53
0.76
0.45
0.21
0.38
0.29"
REFERENCES,0.6613545816733067,"Table 4: The MAE of interpolating trajectories from the Lorentz system, with a small base ODE with
2256 parameters and an increasing number of invertible layers."
REFERENCES,0.6653386454183267,"B
ADDITIONAL RESULTS: TRAINING TIMES"
REFERENCES,0.6693227091633466,"We present the training times for directly learning ODEs with our method, using a linear base ODE.
These include the training times on the 3D Lotka-Volterra, and the robot imitation datasets, outlined
in Sections 5.1.1 and 5.1.2 of the main paper. We run training for 1000 iterations, where in each
iteration the batch includes the entire training set. We see that, by leveraging the closed-form solution
of linear ODEs, our method is able to also drastically speed up training. Additionally the parallel
nature of passing through the invertible neural network allows more consistent training times across
datasets."
REFERENCES,0.6733067729083665,Under review as a conference paper at ICLR 2022
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.6772908366533864,"3D Lotka-Volterra
Imitation S
Imitation cube pick
Imitation C
Per iter (s)
Total (s)
Per iter (s)
Total (s)
Per iter (s)
Total (s)
Per iter (s)
Total (s)
Ours
0.030± 0.002
29.57
0.029± 0.002
29.35
0.031± 0.007
31.34
0.030± 0.004
29.78
Euler
0.131±0.003
130.94
1.740± 0.017
1740.43
1.713± 0.016
1712.53
1.704± 0.010
1703.85
Midpoint
0.235±0.006
235.07
3.228± 0.027
3227.51
3.177± 0.028
3177.37
3.207± 0.025
3206.92
RK4
0.469±0.005
468.52
6.671± 0.048
6671.44
6.388± 0.046
6388.10
6.441± 0.057
6440.61
Dopri5
0.408±0.037
408.36
1.413± 0.034
1413.34
1.246± 0.023
1245.65
1.247± 0.022
1246.67"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.6812749003984063,"Table 5: The training times in seconds with standard deviations, for 1000 iterations. By leveraging
the closed-form solution of linear ODEs, training time with our method is consistently orders of
magnitude faster than by using a differentiable numerical integrator."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.6852589641434262,"C
ADDITIONAL RESULTS: ABLATION STUDY"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.6892430278884463,"We study the effects of the number of layers in the invertible neural network and number of parameters
in the sub-network, which are the main hyper-parameters of the invertible neural networks used. To
this end, we conduct ablation studies of the speed and performance of our method on the real-world
datasets outlined in section 5.1.2 of the paper. Our basic model uses an invertible neural network
with 5 layers and sub-networks in the invertible network had 1500 hidden dimension size. We alter
the number of layers to be: 2, 3, 4, 5, 6, 7, 8, and hidden dimensions of the sub-networks within the
invertible network to be: 500, 1000, 1500, 2000, 2500. The results are presented below:"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.6932270916334662,"Imitation S
Imitation cube pick
Imitation C
No. Layers
Sub-Net Hid. Dim. Size
Int. time (ms)
MSE
Int. time (ms)
MSE
Int. time (ms)
MSE
2
1500
3.551± 0.585
122.40
2.993±0.103
41.51
2.914±0.049
20.69
3
1500
4.589± 1.193
130.49
4.026±0.129
15.00
5.150±1.407
26.20
4
1500
5.418± 0.746
24.54
6.294±0.939
26.18
5.374±0.382
10.33
5
1500
6.461± 0.686
4.40
7.401±1.531
26.56
7.463±1.929
13.16
6
1500
7.529± 0.698
8.17
8.993±2.212
17.16
8.994±2.781
18.27
7
1500
9.426± 1.664
4.91
9.858±1.828
20.39
9.669±1.240
25.76
8
1500
10.636± 2.541
5.62
10.732±2.111
14.56
10.958±2.518
6.57
5
500
7.159± 1.475
5.62
8.018±2.109
19.37
7.315±1.483
6.22
5
1000
6.972± 1.247
6.04
6.376±0.203
11.02
7.297±1.311
6.37
5
1500
7.031± 1.289
4.40
7.321±1.236
26.56
6.901±0.802
13.16
5
2000
7.787± 1.363
6.23
7.443±1.457
14.05
7.776±2.514
9.48
5
2500
7.208± 1.628
10.92
6.521±0.082
11.66
7.611±1.687
5.59"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.6972111553784861,Table 6: Ablation study results of different conﬁgurations for the invertible neural network model.
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.701195219123506,"We see that as we increase the number of invertible network layers, the integration times increase,
while the hidden dimension size of the sub-networks within the invertible network does not visibly
affect the integration times. Overall, the generalisation performance improves as the number of
invertible layers are used, up to some number of layers. Beyond this number of layers, adding layers
does not vary performance signiﬁcantly. Additionally, the hidden dimension sizes, for the values
tested do not greatly vary the generalisation performance."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7051792828685259,"D
PROOFS"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7091633466135459,Proofs for Propositions 4.1 and 4.2 can be found in Lee (2012) as Propositions 8.19 and 9.6.
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7131474103585658,"Theorem 4.1. Suppose two ODEs x′(t) = g(x(t)), y′(t) = f(y(t)) are related via y(t) = F(x(t)),
where F is a diffeomorphism. If the former ODE is asymptotically stable with ne equilibrium
points x∗
1, . . . , x∗
ne, then the latter ODE is also asymptotically stable, with equilibrium points
F(x∗
1), . . . , F(x∗
ne)."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7171314741035857,"Proof. First we show F(x∗
1), . . . , F(x∗
ne) are equilibrium points of ODE y′(t) = f(y(t)). By
y(t) = F(x(t)), we can write the time derivatives y′ at F(x) as"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7211155378486056,y′(t) = f(F(x(t))) = dF(x(t))
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7250996015936255,"dt
= JF (x(t))g(x(t)),
(8)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7290836653386454,Under review as a conference paper at ICLR 2022
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7330677290836654,"where JF (x(t)) is the Jacobian of F. F is a diffeomorphism and hence invertible over its domain. By
the inverse function theorem (Dontchev & Rockafellar, 2009), the Jacobian JF (x(t)) is invertible,
and furthermore, by the invertible matrix theorem (Horn & Johnson, 2012), it has a null-space
containing only the zero vector. Therefore, y′(t) = f(F(x(t))) = JF (x(t))g(x(t)) = 0 if and only
if g(x(t)) = 0. As g(x∗(t)) = 0 for x∗∈{x∗
1 . . . x∗
ne}, then we also have f(F(x∗(t))) = 0, hence
y∗∈{F(x∗
1), . . . , F(x∗
ne)} gives equilibrium points for y′(t) = f(y(t))."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7370517928286853,"We now show asymptotically stability of y′(t) = f(y(t)), by the existence of a Lyapunov function
(Lefschetz & Alverson, 1962), Vy : Rn →R, where n is the dimension of y, such that ∂Vy(y)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7410358565737052,"∂t
< 0
for all y ∈Rn \ {F(x∗
1), . . . , F(x∗
ne)}, and ∂Vy(y∗)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7450199203187251,"∂t
= 0 for y∗∈{F(x∗
1), . . . , F(x∗
ne)}. The
existence of such a Lyapunov function is a necessary and sufﬁcient condition for stability. We
assume the candidate function to be Vy = Vx(F −1(y)), where Vx is a valid Lyapunov function
of the asymptotically stable x′(t) = g(x(t)), with ∂Vx(x)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.749003984063745,"∂t
< 0 for x ∈Rb \ {x∗
1, . . . , x∗
ne} and
∂Vx(x∗)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7529880478087649,"∂t
= 0 for x∗∈{x∗
1, . . . , x∗
ne}. Consider the time derivative of the candidate function:"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7569721115537849,∂Vy(y)
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7609561752988048,"∂t
= ∂Vy ∂y
∂y"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7649402390438247,∂t = ∂Vy
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7689243027888446,"∂y f(y)
(9)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7729083665338645,"=
∂Vx"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7768924302788844,"∂x
∂F −1 ∂y
∂F"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7808764940239044,"∂x g(x)
"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7848605577689243,"x=F −1(y)
(10)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7888446215139442,"=
∂Vx"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.7928286852589641,"∂x JF (x)−1JF (x)g(x)
"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.796812749003984,"x=F −1(y)
(11)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8007968127490039,"=
∂Vx"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8047808764940239,"∂x g(x)
"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8087649402390438,"x=F −1(y) =
∂Vx(x) ∂t "
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8127490039840638,"x=F −1(y).
(12)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8167330677290837,"Equation (11) by the inverse function theorem (Dontchev & Rockafellar, 2009). Therefore, our
candidate Vy is a valid Lyapunov function for y′(t) = f(y(t)). Thus, the system y′(t) = f(y(t)) is
asymptotically stable."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8207171314741036,"E
ADDITIONAL IMPLEMENTATION DETAILS"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8247011952191236,"We run all of our experiments on a machine with an Intel i7-3770k 3.50GHz processor, 32GB
RAM and an NVIDIA GTX1080 GPU, with 8GB vRAM. For all of our experiments, we use
the optimizer ADAM with step-size 10−4, except for the experiments in the Latent ODE, which
where we use the standard set-up from the Latent-ODE repository (Rubanova et al., 2019).
The dynamics models of compared ODEs have the architecture: Input->dense(Input dimen-
sions, 150)->tanh()->dense(150,150)->tanh()->dense(150,150)->tanh()->dense(150,150)->tanh()-
>dense(150,150)->tanh()->dense(150,output dimensions)->output. Except for the Latent-ODE com-
parisons where settings from the original repository (Rubanova et al., 2019) is used. For all of the
experiments, except latent ODE experiments where we follow the original set-up, we train for 5000
iterations in total."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8286852589641435,"For all the experiments where we directly learn a dynamical system, we use an invertible neu-
ral network with 5 invertible layers, and sub-networks with one hidden layer of 1500 units.
For non-linear base ODEs parameterized with a simple neural networks, we use the architec-
ture: Input->dense(Input dimensions,30)->tanh()->dense(30,30)->tanh()->dense(30,30)->tanh()-
>dense(30,Output dimensions)->Outputs. Additionally, all learned dynamics, both with ours and
compared methods, excepted when adhering to the original Latent-ODE set-up, were augmented
with the same number of additional zeros as original state dimensions, for example 3 dimensional
systems were augmented to 6 dimensions. An exception to this is the high-dimensional image rotation
problem, where we found that adding half as many augmented states as the original state dimensions
was sufﬁcient."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8326693227091634,"The Lotka-Volterra system used has the dynamics:
x′(t) = x(t)(0.75 −0.75y(t))
(13)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8366533864541833,"y′(t) = y(t)(−0.75 + 0.75x(t) −0.75z(t))
(14)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8406374501992032,"z′(t) = z(t)(−0.75 + 0.75y(t))
(15)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8446215139442231,Under review as a conference paper at ICLR 2022
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.848605577689243,"for t ∈[0, 7] with initial conditions {(5, 5, 1), (2, 6, 6), (3, 1, 4), (7, 1, 2), (6, 2, 4),
(3, 3, 1), (2, 2, 2), (4, 4, 3), (3, 3, 4), (1, 1, 5)}."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.852589641434263,The Lorenz system used has the dynamics:
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8565737051792829,"x′(t) = 10(y(t) −x(t))
(16)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8605577689243028,"y′(t) = x(t)(28 −y(t)) −x(t)
(17)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8645418326693227,z′(t) = x(t)y(t) −8
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8685258964143426,"3z(t)
(18)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8725099601593626,"for t ∈[0, 2] with the initial conditions (0.15, 0.15, 0.15)."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8764940239043825,The Robertson’s system used has the dynamics:
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8804780876494024,"x′(t) = −0.04x(t) + 3 × 104y(t)z(t)
(19)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8844621513944223,"y′(t) = 0.04x(t) −3 × 104y(t)2 −104y(t)z(t)
(20)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8884462151394422,"z′(t) = 3 × 104y(t)2
(21)"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.8924302788844621,"for t ∈[0, 120] with the initial conditions (1, 0, 0). During training and evaluation, we rescale the
data dimensions, and roll out the ODE in log-space."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.896414342629482,"In the latent-ODE problem setup, an observable time-series is assumed to have latent variables which
follow some ODE dynamics, and uses an Encoder -> ODE -> Decoder architecture where
an ODE is used to model the hidden latent dimensions between the Encoder and Decoder. Note that
a valid ODE is not guaranteed in the space of observable data, but only in the latent dimensions.
Our set-up follows the repository given by Rubanova et al. (2019), with the training settings for the
Encoder and Decoder architecture as below:"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.900398406374502,"Periodic 100: We train the entire model for 500 epochs with Adamax optimizer and an initial learning
rate of 10−2. We sub-sample 5% of the original time points and the size of the latent state is 10.
The noise weight is set as 0.01 and the total number of time points is 100. For the Neural ODE
architectures, there is one layer in the recognition ODE and one layer in the generative ODE, and 100
unit per layers. For the GRU unit there exists 100 units per layer for the GRU update network. All the
above settings are exactly the same as the conﬁguration given in repository (Rubanova et al., 2019)."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9043824701195219,"Periodic 1000: Settings are the same as Periodic 100, except that the total number of time points is
set as 1000 to predict for ﬁner time steps."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9083665338645418,"Human Activity: The model is trained for 200 epochs, with a dimensionality of 15 in the latent state.
There are 4 layers in the recognition ODE and 2 layers in the generative ODE, and 500 units per layer.
The GRU unit has exists 50 units per layer."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9123505976095617,"ECG: Settings are the same as the classiﬁcation task of Human Activity, except that we use the ECG
Heartbeat data available at Dataset (2018), removing the ‘0’ class."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9163346613545816,"Rotating Image Sequence: We use the ﬁrst 100 MNIST “3” characters as training and the next “3”
characters as test. We create a sequence of 44 images for each initial character until we rotate the
initial image by 180 degrees. During testing, we integrate to obtain a sequence of 440, at 10x data
resolution. We obtain latent representations of each image by training a convolutional autoencoder
to obtain a 64 dimension latent vector. We ﬁt the ODE on these latent dimensions, with an ADAM
optimizer with learning rate 5 × 10−4."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9203187250996016,"ODE dimensions and sequence lengths: The following table contains details on the dimensions
of the ODE models. For latent ODE models, these are the dimensions of the latent state. We also
provide the lengths of the integrated trajectories of the ODEs during inference."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9243027888446215,"F
ADDITIONAL FIGURES"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9282868525896414,"We provide ﬁgures for learning an additional Lorenz system for t ∈[0, 5], with trajectory at initial
condition (−3.1, −1.15, 8.15). We see that our method, with a base ODE parameterized by a neural
network, can generate trajectories that closely match the ground truth:"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9322709163346613,Under review as a conference paper at ICLR 2022
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9362549800796812,"Dimensions of ODE to Learn
Time points in Trajectory
3d-LV
3
700
Imitate S
2
1000
Pick cube
3
1000
Imitate C
3
1000
Periodic 100
10
100
Periodic 1000
10
1000
Human Activity
15
157
ECG
15
188
Rotating MNIST
64
440
Lorenz
3
800
Robertson
3
500"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9402390438247012,"Figure 6: A learned Lorenz system with the generated trajectory, at 10x data resolution, and ground
truth."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9442231075697212,"0
1
2
3
4
5
time 20 10 0 10 20 30 40"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9482071713147411,xyz-coordinates
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.952191235059761,Under review as a conference paper at ICLR 2022
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9561752988047809,"We provide the change in coordinates over time, for the trajectory shown in ﬁgure 5(a) in the paper:"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9601593625498008,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
time 10 0 10 20 30 40 50"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9641434262948207,"Figure 8: The corresponding plot showing the coordinates over the time interval of a learned Lorenz
system over t ∈[0, 2], at 10x data resolution, which corresponds to the 3d ﬁgure shown as ﬁg 5 (left)."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9681274900398407,"We provide additional plots of trajectories, at different start points, from a learned Lotka-Volterra
system. The ground truth data is in blue, while generated trajectory, of 10x data resolution, is in red."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9721115537848606,"Figure 9: We see that trajectories from the learned Lotka-Volterra system, in red, closely matches the
ground truth, in blue."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9760956175298805,"We provide an additional ﬁgure for trajectories generated at unseen starting points after being trained
on the “imitation C” training data. The four generated trajectories are in red, while the ground truths
are in blue. Our generated trajectories match the ground truth, and accurately capture the motion of
drawing a “C” character."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9800796812749004,Under review as a conference paper at ICLR 2022
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9840637450199203,"Figure 10: Robot motion trajectories in red, that imitate drawing a “C” character. The ground truth is
given in blue."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9880478087649402,"G
LICENSES FOR PACKAGES"
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9920318725099602,"Common scientiﬁc packages used in our code include: (i) Numpy (Harris et al., 2020) (BSD license),
for general linear algebra and miscellaneous math operations (ii) Matplotlib (Hunter, 2007) (BSD
compatible custom license), for plotting ﬁgures."
"D LOTKA-VOLTERRA
IMITATION S
IMITATION CUBE PICK
IMITATION C",0.9960159362549801,"More specialized packages used include (i) FrEIA (Ardizzone et al., 2019) (MIT license), for invert-
ible neural networks; (ii) TorchDiffEq (Chen et al., 2018) (MIT license), for differentiable numerical
integrators; (iii) Pytorch (Paszke et al., 2019) (BSD license), for optimisation and automatic differen-
tiation; (iv) Latent-ODE (Rubanova et al., 2019) (MIT license), for latent ODE implementation."
