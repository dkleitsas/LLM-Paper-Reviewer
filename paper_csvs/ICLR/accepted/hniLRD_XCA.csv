Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0022935779816513763,"The Koopman operator theory linearly describes nonlinear dynamical systems in a
high-dimensional functional space and it allows to apply linear control methods to
highly nonlinear systems. However, the Koopman operator does not account for
any uncertainty in dynamical systems, causing it to perform poorly in real-world
applications. Therefore, we propose a deep stochastic Koopman operator (DeSKO)
model in a robust learning control framework to guarantee stability of nonlinear
stochastic systems. The DeSKO model captures a dynamical system’s uncertainty
by inferring a distribution of observables. We use the inferred distribution to design
a robust, stabilizing closed-loop controller for a dynamical system. Modeling and
control experiments on several advanced control benchmarks show that our frame-
work is more robust and scalable than state-of-the-art deep Koopman operators
and reinforcement learning methods. Tested control benchmarks include a soft
robotic arm, a legged robot, and a biological gene regulatory network. We also
demonstrate that this robust control method resists previously unseen uncertainties,
such as external disturbances, with a magnitude of up to ﬁve times the maximum
control input. Our approach opens up new possibilities in learning control for
high-dimensional nonlinear systems while robustly managing internal or external
uncertainty."
INTRODUCTION,0.0045871559633027525,"1
INTRODUCTION"
INTRODUCTION,0.006880733944954129,"Modeling is crucial to the development of intelligent machines and robots. It is needed to predict the
behavior of a dynamical system, analyze a system’s interior properties, realize a reliable planning
and control (M¨uller et al., 2007). For many systems of interest, such as a soft robotic arm that can
interact with an unknown object (George Thuruthel et al., 2018), the governing equations are either
unknown or too complex to be useful (Bakker et al., 2020). Thus, it is essential to use an accurate but
concise model."
INTRODUCTION,0.009174311926605505,"The need for a fast model that is able to represent complex systems has led to recent advancements
in data-driven modeling of complex dynamics, and learning world models. These advancements
have been achieved by constructing surrogate neural network (NN) models (Chua et al., 2018; Ha
& Schmidhuber, 2018; Zhang et al., 2019). By formulating the modeling problem as a supervised
learning task, the neural network can be trained to infer the systems’ dynamics based on the dense
state and action input (Nagabandi et al., 2018b) or high-dimensional image observations (Hafner et al.,
2019). However, as nonlinear systems are often approximated by complex and over-parameterized
neural networks (Morton et al., 2019), the learned models are restrictive when they are used for
dynamical analysis and controller design. Optimal control problems based on NN models are
typically non-convex, and they need to be solved by using inefﬁcient population-based optimization
methods (Bharadhwaj et al., 2020), such as random shooting (Zhang et al., 2019; Nagabandi et al.,
2018a) or cross entropy methods (Chua et al., 2018; Hafner et al., 2019; Wang & Ba, 2019)."
INTRODUCTION,0.011467889908256881,"Recently, the Koopman operator theory has attracted much attention. The Koopman operator can
model complex nonlinear systems by linearly propagating observables in an inﬁnite dimensional
functional space (Koopman, 1931b). This propagation allows us to apply the well-established linear
control and analysis theory to nonlinear systems. Based on a system’s runtime data, a linear operator,
which propagates the observables in time, is then learned. Based on this idea, practical algorithms, like"
INTRODUCTION,0.013761467889908258,Published as a conference paper at ICLR 2022
INTRODUCTION,0.016055045871559634,"dynamic mode decomposition (DMD) and extended dynamic mode decomposition (EDMD), were
developed by selecting a ﬁnite set of parameterized linear and nonlinear observable functions. These
methods have been applied to the modeling of soft robotic arms (Bruder et al., 2020; Haggerty et al.,
2020), multirotors (Folkestad et al., 2020), and mobile robots (Shi & Karydis, 2021). More recently,
Deep-DMD exploits deep learning techniques to automate the design of observable functions (Lusch
et al., 2018; Otto & Rowley, 2019; Han et al., 2020; Yeung et al., 2018; Morton et al., 2018). Deep-
DMD automatically learns a richer set of observable functions than DMD or EDMD, and it can
also be more accurate, even though it uses fewer observables (Bakker et al., 2020). However, these
methods focus on modeling nominal systems with clean data sets, while noise, either in the process
or observation, has not yet been accounted for. Furthermore, it is challenging to ensure closed-loop
stability in a controller design that is based on a learned Koopman operator model (Bakker et al.,
2020)."
INTRODUCTION,0.01834862385321101,"In this work, we learn and control uncertain nonlinear dynamics using the deep stochastic Koopman
operator (DeSKO) model. Instead of encoding the observables deterministically, DeSKO infers the
probabilistic distribution of observables and learns a linear operator that can propagate the inferred
distribution forward in time. Furthermore, we propose an efﬁcient and robust model predictive
controller based on the DeSKO model. In terms of robotic control benchmarks, we show that our
proposed method outperforms state-of-the-art deep Koopman operators (Lusch et al., 2018; Han et al.,
2020) in modeling and in stabilizing systems that have been corrupted by process and observation
noises. Furthermore, we show that the proposed framework is scalable; it can handle high-dimensional
and complex systems, including legged robots, soft robotic arms, and biological gene networks."
INTRODUCTION,0.020642201834862386,"The outline of this paper is as follows. In Section 2, we introduce and discuss the related works.
In Section 3, we introduce the basic notations and concepts of the Koopman operator (KO), and
we present our proposed DeSKO model. In Section 4, we discuss our robust control framework
and prove its stability. In Section 5, we evaluate the DeSKO model in terms of modeling, control,
and robustness in eight different environments, and we compare it to baseline methods. Finally, in
Section 6, we conclude the paper and discuss future challenges."
RELATED WORKS,0.022935779816513763,"2
RELATED WORKS"
RELATED WORKS,0.02522935779816514,"Koopman operators Following the Koopman operator theory formalized in Koopman (1931a),
various practical algorithms have been proposed to extract the Koopman spectral properties from
state-transition data. DMD and EDMD adopt a hand-crafted set of linear and nonlinear functions
to encode the observables and obtain the linear operator through least square optimization (Schmid,
2010; Tu et al., 2014; Williams et al., 2015). A straightforward approach to automate the design of
observable functions is to parameterize the observable functions as neural networks and use deep
learning techniques for training (Lusch et al., 2018; Otto & Rowley, 2019; Yeung et al., 2018; Morton
et al., 2018; Takeishi et al., 2017). Unlike DMD and EDMD, Deep-DMD approaches do not suffer
from the curse of dimensionality; that is, the number of observables does not grow exponentially with
the number of states. Furthermore, Deep-DMD can also express a richer class of observable functions
than DMD or EDMD. Graph neural networks and block-wise Koopman operators are introduced
in (Li et al., 2020) to model objects composed of similar building blocks, such as ropes."
RELATED WORKS,0.027522935779816515,"While the described works have greatly advanced the ﬁeld of modeling and control by applying
the Koopman operator, several challenges still remain. Azencot et al. (2020) proposed a Koopman
autoencoder framework that can learn a Koopman model from noisy data for long horizon prediction,
but only considered autonomous systems without control input. Morton et al. (2019) proposed to
measure uncertainty with an ensemble of Koopman models sampled from the encoded distribution.
However, their work is focused on deterministic systems with clean datasets. Furthermore, how to
guarantee stability with a learned Koopman model remains an open question. Mamakoukas et al.
(2020); Kolter & Manek (2019); Pan & Duraisamy (2020) studied how to learn a stable dynamics
of embeddings on the latent space, while we are focused on stabilizing the original system with
the learned model. Our method differs from previous works by learning a stochastic model from
noisy data, and providing a robust control framework with a stability guarantee. We show that this
framework can easily generalize to systems with process and observation noises."
RELATED WORKS,0.02981651376146789,"Model-based Learning Control In recent years, model-based reinforcement learning has dominated
a large portion of model-based learning control studies (Pascanu et al., 2017; Hamrick et al., 2017;
Racani`ere et al., 2017). Several researchers have combined learned neural network models with
model predictive control (Chua et al., 2018; Zhang et al., 2019; Hafner et al., 2019; Nagabandi et al.,
2018a; Wang & Ba, 2019). In this work, we utilize the Koopman operator’s linear propagation"
RELATED WORKS,0.03211009174311927,Published as a conference paper at ICLR 2022
RELATED WORKS,0.034403669724770644,"property, and we conduct model predictive control efﬁciently. Furthermore, we also show that the
Koopman operator substantially facilitates analyzing and assuring stability of the closed-loop system."
RELATED WORKS,0.03669724770642202,"Gaussian process-based methods are another popular class of learning control methods (Kocijan et al.,
2004; Berkenkamp et al., 2017; Vinogradska et al., 2016; Berkenkamp & Schoellig, 2015). As a type
of non-parametric method, Gaussian process-based methods are efﬁcient in modeling with small data
sets. Indeed, Gaussian process models have been applied to various robotic systems (Chang et al.,
2017; Cao et al., 2017; Kabzan et al., 2019; Chang et al., 2020). Nonetheless, Gaussian process-based
approaches are typically limited to low-dimensional systems and small data sets (Wang et al., 2020).
In this work, we present a framework that is scalable to high-dimensional robotic control tasks."
MODELING,0.0389908256880734,"3
MODELING"
MODELING,0.04128440366972477,"In this section, we will ﬁrst introduce the basic notations and concepts of the Koopman operator
(KO) (Koopman, 1931a). Then the derivation of the DeSKO model is established."
THE KOOPMAN OPERATOR,0.04357798165137615,"3.1
THE KOOPMAN OPERATOR
For the sake of clarity, we introduce the KO using notations that are similar to the ones in Bakker
et al. (2020). Consider the nonlinear discrete time dynamical system"
THE KOOPMAN OPERATOR,0.045871559633027525,"xt+1 = f(xt, ut)"
THE KOOPMAN OPERATOR,0.0481651376146789,"where f is a nonlinear differentiable function. The system can be lifted to an inﬁnite-dimensional
function space F composed of all square-integrable real-valued functions within the compact domain
X × U ⊂Rn+m. Elements of F are called observables. On this space, the ﬂow of the systems is
characterized by the Koopman operator K : F →F, which is an inﬁnite-dimensional linear operator
that satisﬁes
Kψ(xt, ut) = ψ ◦f(xt, ut)
where ψ denotes the observable function. However, the inﬁnite-dimensional function space F and
the KO K are impractical. Therefore, a ﬁnite dimensional function space F ⊂F, spanned by a
linearly independent basis function ψ : Rn →Rh is used instead, where h is the dimensionality
of the observable function speciﬁed by the designer. Here, i denotes the index of each observable
function. The linear property of the KO enables the use of linear control techniques for efﬁcient
control. The Koopman equation can be written as"
THE KOOPMAN OPERATOR,0.05045871559633028,"Aψ(xt) + But = ψ ◦f(xt, ut)
(1)"
THE KOOPMAN OPERATOR,0.052752293577981654,"where the KO is split into the Koopman matrix A ∈Rh×h and the control matrix B ∈Rh×m, . For
the reader’s convenience, we also have also included the details for learning Koopman matrices with
a set of observables in the Appendix."
THE KOOPMAN OPERATOR,0.05504587155963303,"The Koopman operator conducts a deterministic inference of observables and ignores the existence
of any uncertainty that may stem from the system and the lack of data about the system. A controller
design based on such a defective model would inevitably perform suboptimally, and even result in
failure. In this work, we are interested in learning and controlling a nonlinear system that has been
corrupted by additive noise:
xt+1 = f(xt, ut) + wt
(2)
where wt ∼pw denotes random noises subject to the distribution pw. In this case, the transition of
states is probabilistic, and it is denoted by p(xt+1|xt, ut). In the next section, we will discuss how to
overcome this challenge by encoding and propagating the distribution of observables with Koopman
operators."
DEEP STOCHASTIC KOOPMAN OPERATOR,0.05733944954128441,"3.2
DEEP STOCHASTIC KOOPMAN OPERATOR
The deep stochastic Koopman operator (DeSKO) consists of two building blocks: a probabilistic
neural network that encodes the distribution of the observables, and a KO that propagates the
distribution into the future. In the following section, we will describe the construction, implementation,
and optimization of the DeSKO."
DEEP STOCHASTIC KOOPMAN OPERATOR,0.05963302752293578,"Probabilistic neural network To account for uncertainty, we exploit a probabilistic model pθ(ψt|xt)
to encode the distribution of observables given the current state. The probabilistic model is parame-
terized by a probabilistic neural network. The output neurons of the probabilistic NN characterize"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.06192660550458716,Published as a conference paper at ICLR 2022
DEEP STOCHASTIC KOOPMAN OPERATOR,0.06422018348623854,"+
𝑝(𝜑𝑡|𝑥𝑡)
𝑝(𝜑𝑡+1|𝑥𝑡, 𝑢𝑡) 𝑢𝑡×"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.06651376146788991,𝜇𝜃(𝑥𝑡)
DEEP STOCHASTIC KOOPMAN OPERATOR,0.06880733944954129,𝜎𝜃(𝑥𝑡) 𝑥𝑡
DEEP STOCHASTIC KOOPMAN OPERATOR,0.07110091743119266,Recursive Propagation
DEEP STOCHASTIC KOOPMAN OPERATOR,0.07339449541284404,Koopman Matrix 𝐴
DEEP STOCHASTIC KOOPMAN OPERATOR,0.07568807339449542,Control Matrix 𝐵
DEEP STOCHASTIC KOOPMAN OPERATOR,0.0779816513761468,Observation
DEEP STOCHASTIC KOOPMAN OPERATOR,0.08027522935779817,"Matrix 𝐶
𝑥𝑡+1"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.08256880733944955,"Figure 1: Structure of the Deep Stochastic Koopman Operator (DeSKO). With the help of two
neural networks, the DeSKO model encodes the system state xt into the parameters of a Gaussian
distribution of observables, which are the mean vector µθ(xt) and the variance vector σθ(xt). Then,
the learned Koopman matrix A and control matrix B propagate the distribution to the next time step.
This can recursively produce a series of distribution predictions over a given time horizon. Finally,
the learned observation matrix C maps the observables to the state space."
DEEP STOCHASTIC KOOPMAN OPERATOR,0.08486238532110092,"the parameters of the distribution to capture the uncertainty caused by the process and observation
noise. In our particular case, we design two NNs to output a mean vector µθ(xt) and a diagonal
covariance vector σθ(xt). Together, these vectors describe a multivariate Gaussian distribution, which
is deﬁned as pθ(ψt|xt) = N(µθ(xt), σθ(xt)) (see also Figure 1). A Gaussian distribution is often
chosen for continuous-valued states, and it is a reasonable choice if the uncertainty in the system is
unimodal (Chua et al., 2018). While a Gaussian distribution is effective in our experiments, other
tractable distributions can be used instead. In order to expressively encode the dynamics, the distribu-
tion parameters can be represented with an arbitrarily high complexity by using nonlinear functions
that depend on the current state. This makes it feasible to incorporate NNs into the probabilistic
model."
DEEP STOCHASTIC KOOPMAN OPERATOR,0.0871559633027523,"Propagation of observable distributions As shown in Figure 1, given a series of control input
{ut, ut+1, . . . , ut+T }, the distribution of observables encoded by the probabilistic NNs is recursively
propagated into the future with the Koopman matrix A and control matrix B. According to (1),
the KO propagates the observables pointwise on the functional space F. By conducting pointwise
mapping, KO is also able to infer the future distribution of observables. The use of KO to conduct
distribution propagation can also be justiﬁed from the perspective of the Frobenius-Perron Operator
(FPO) and has been discussed in the Appendix. Finally, the decoder maps the observables back
to the state space and produces the prediction results. The decoder is designed to be linear, and
parameterized by an observation matrix C ∈Rn×h. This facilitates controller design, which will be
discussed in the next section."
DEEP STOCHASTIC KOOPMAN OPERATOR,0.08944954128440367,"Optimization procedure To optimize the probabilistic neural network and the Koopman operator,
including the observation matrix, the single-step prediction loss is given as"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.09174311926605505,"Lt(A, B, C, θ) := Epθ(ψt|xt)∥xt+1 −C(Aψt + But)∥"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.09403669724770643,"where E denotes expectation. However, the model trained with single-step optimization may not
satisfactorily perform long-term predictions, since it suffers from the accumulation of prediction
errors during recursive propagation. To this end, we train the model to minimize the following
multi-step prediction loss:"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.0963302752293578,"L(A, B, C, θ) = ED H
X"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.09862385321100918,"k=1
Eψt+k∥xt+k −Cψt+k∥
(3)"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.10091743119266056,"where D denotes the data set composed of multiple T steps input trajectories u0:T and the resulting
state trajectories x0:T . In (3), H denotes the horizon of forward prediction. The distribution of the
observable ψt+k is inferred through recursive propagation with the KO based on the current state
xt and a series of control inputs ut:t+k−1. We exploit the reparameterization trick (Haarnoja et al.,
2018) in the calculation of (3) to achieve a more stable training process (Xu et al., 2019). By injecting
a Gaussian noise vector ϵi ∼N(0, I), the expectation of the observables can be approximated by
1
N
PN
i=0(µθ(xt)+ϵiσθ(xt)). This could produce a lower variance estimator of the gradient, and also
enables the straightforward exploitation of KO in forward propagation. The optimization problem is"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.10321100917431193,Published as a conference paper at ICLR 2022
DEEP STOCHASTIC KOOPMAN OPERATOR,0.10550458715596331,summarized as
DEEP STOCHASTIC KOOPMAN OPERATOR,0.10779816513761468,"min
A,B,C,θ L(A, B, C, θ) = ED H
X k=1 N
X"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.11009174311926606,"i=0
∥xt+k −Cψi
t+k∥"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.11238532110091744,"s.t. ψi
t+k = Aψi
t+k−1 + But+k−1
ψi
t = µθ(xt) + ϵiσθ(xt), ϵi ∼N(0, I) (4)"
DEEP STOCHASTIC KOOPMAN OPERATOR,0.11467889908256881,The parameters in (4) are updated using gradient descent.
DEEP STOCHASTIC KOOPMAN OPERATOR,0.11697247706422019,"Entropy Constraint To further improve the robustness of the DeSKO model, we introduce a con-
straint on the entropy of the observable distributions during optimization (4). During the training
process of probabilistic models, the entropy of the learned distribution naturally falls as the model
becomes more certain about the underlying dynamic. However, the lack of data in some states may
cause the learned model to become overly conﬁdent about the encoding. To this end, we prevent
the model from overﬁtting by constraining the average entropy of the learned distribution above a
minimum entropy threshold, i.e., ED −log p(ψt|xt) ≥H, where H denotes the minimum entropy
threshold. Note that the constraint is valid for the average entropy, rather than a pointwise constraint
on the state space, thus the entropy can vary at different states. In our implementation, we exploit the
Lagrange method to conduct the constrained optimization. The entropy constraint is multiplied by a
Lagrange multiplier and added into the loss function. During the training of NNs, we also adjust the
weight of the entropy constraint by updating the Lagrange multiplier using gradient ascent."
CONTROL,0.11926605504587157,"4
CONTROL"
CONTROL,0.12155963302752294,"In this section, we will present the robust control framework for the proposed DeSKO model. We
exploit the model predictive control (MPC) framework to achieve robust and stabilizing control
guarantees. First, a nominal MPC controller (Borrelli et al., 2017) is designed to solve a ﬁnite
horizon stochastic optimal control problem, minimizing the expectation of cumulative stage cost. As
the DeSKO model encodes a Gaussian distribution of the observables, the nominal dynamic of the
expectation of observables could be characterized by"
CONTROL,0.12385321100917432,"ˆµt+1 = Aˆµt + Bct
(5)"
CONTROL,0.12614678899082568,"where ˆµt is the nominal mean vector encoded by the DeSKO model and ct denotes the nominal
control input. Then the nominal MPC solves the following deterministic optimal control problem"
CONTROL,0.12844036697247707,"V ∗(ˆµt) = min
c0:H−1 H−1
X"
CONTROL,0.13073394495412843,"k=0
∥C ˆµt+k∥2
Q + ∥ct+k∥2
R + ∥C ˆµt+H∥2
P
(6)"
CONTROL,0.13302752293577982,"s.t. ˆµt+k+1 = Aˆµt+k + Bct+k, ct+k ∈U
(7)"
CONTROL,0.1353211009174312,"where Q, R and P are known positive deﬁnite weight matrices, and r denotes the reference signal."
CONTROL,0.13761467889908258,"Ideally, after applying an optimized control input c∗
t to the system, the mean vector encoded at the
next time step µθ(xt+1) should equal to Aˆµt + Bc∗
t . However, this can hardly be true due to the
existence of noise. The actual dynamic of the mean vector is dominated by"
CONTROL,0.13990825688073394,"µt+1 = µθ(f(xt, ut) + wt)
(8)"
CONTROL,0.14220183486238533,"With the learned KO, the evolution of the actual mean vector on the observable space is deﬁned as"
CONTROL,0.1444954128440367,"µt+1 = Aµt + But + g(wt)
(9)"
CONTROL,0.14678899082568808,"where g(wt) = µθ(f(xt, ut) + wt) −µθ(f(xt, ut)) characterizes the unknown effect of wt on the
observable dynamics."
CONTROL,0.14908256880733944,"To compensate for the control error caused by the uncertainty, a stabilizing feedback controller K
is introduced to drive the actual state µt to the nominal trajectory ˆµt. At every time step, the action
input is given by
ut = c∗
t + K(µθ(xt) −ˆµt)
(10)
In this work, the feedback controller K is obtained by using the linear quadratic regulator (LQR)."
CONTROL,0.15137614678899083,"To ensure the closed-loop stability of the nominal system (5), the following constraint needs to be
satisﬁed (Borrelli et al., 2017; Mayne et al., 2000),"
CONTROL,0.1536697247706422,"∥C(A + BK)ˆµt∥2
P −∥C ˆµt∥2
P ≤−∥C ˆµt∥2
Q −∥K ˆµt∥2
R
(11)"
CONTROL,0.1559633027522936,Published as a conference paper at ICLR 2022
CONTROL,0.15825688073394495,"In fact, by exploiting the LQR controller K, this constraint can be easily satisﬁed. The matrix P is
determined by solving the following discrete time Riccati equation,"
CONTROL,0.16055045871559634,"P = CT QC + AT PA −AT PB(R + BT PB)−1BT PA
(12)
and the K is given by
K = −(R + BT PB)−1BT PA
(13)"
CONTROL,0.1628440366972477,The overview of the control procedure is summarized in Algorithm 1.
STABILITY WITH EXACT KOOPMAN OPERATORS,0.1651376146788991,"4.1
STABILITY WITH EXACT KOOPMAN OPERATORS"
STABILITY WITH EXACT KOOPMAN OPERATORS,0.16743119266055045,"First, we investigate the case where the exact Koopman operators are available. To establish the
stability guarantee, we make the following assumptions:
Assumption 1. The probabilistic NN µθ is Lipschitz continuous, ∥µθ(x + y) −µθ(x)∥≤L∥y∥.
Assumption 2. The random noise has bounded energy, i.e., a ﬁnite constant b exists such that
Ew∥w∥≤b."
STABILITY WITH EXACT KOOPMAN OPERATORS,0.16972477064220184,"With the assumptions above, we can establish the stability guarantee as follows:
Proposition 1. Consider system (2) controlled by the Robust MPC controller (6)-(7) and (10). Then
the closed-loop system (2) is uniformly ultimately bounded with bound βσLb"
STABILITY WITH EXACT KOOPMAN OPERATORS,0.1720183486238532,"1−β , where β denotes the
maximum eigenvalue of the closed-loop transition matrix AK = A + BK and σ := ∥C∥."
STABILITY WITH EXACT KOOPMAN OPERATORS,0.1743119266055046,The detailed proof of the proposition is deferred to the Appendix.
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.17660550458715596,"4.2
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS"
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.17889908256880735,"Proposition 1 shows that the controller is stabilizing with the exact Koopman operator. However, in
practice, the exact Koopman matrices A∗, B∗, C∗are generally infeasible and only the sub-optimal
solutions A, B, C can be obtained. Now, we will show that stability could be assured even though
the Koopman matrices are approximated with bounded prediction error."
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.1811926605504587,"First, the nominal system (5) is a conceptual system constructed with the approximated Koopman
matrices and thus its dynamic and the resulting MPC problem remain the same as (5), (6) and (7). On
the other hand, the evolution of the mean vector is characterized with the exact Koopman matrices,
µt+1 = A∗µt + B∗ut + g(wt)
xt = C∗µt
We deﬁne the dynamic residual caused by the sub-optimal approximation ϵt := (A∗−A)µt + (B∗−
B)ut, and the reconstruction residual dt := (C∗−C)µt. Then the above system can be rewritten as"
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.1834862385321101,"µt+1 = Aµt + But + g(wt) + ϵt
xt = Cµt + dt
Assumption 3. There exist positive constants γ, η ∈R+, such that ∥ϵ∥≤γ and ∥d∥≤η."
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.18577981651376146,"With the Assumptions 1-3, we can establish the stability guarantee as follows:
Proposition 2. Consider system (2) controlled by the Robust MPC controller (6)-(7) and (10).
Then the closed-loop system (2) is uniformly ultimately bounded with bound βσ(Lb+γ)"
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.18807339449541285,"1−β
+ η, where
σ := ∥C∥."
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.19036697247706422,"Detailed proof of the above proposition is referred to Appendix. Proposition 2 shows that with the
controller given by (10), the stability of the system (2) can still be guaranteed even with approximated
Koopman matrices, though the uniform ultimate bound is inevitably larger than the ideal case in
Proposition 1.
Remark 1. Assumption 3 is equivalent to assuming that ∥A∗−A∥, ∥B∗−B∥, and ∥C∗−C∥are
bounded respectively, and the state space X and action space U are bounded. To simply the notations,
we chose the form in Assumption 3."
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.1926605504587156,"In addition to the stabilization tasks, we are also interested in how to achieve optimal tracking
control with the DeSKO model. By plugging in a reference signal and modifying the input regulation
accordingly, the proposed method could be extended to deal with set-point and dynamic tracking
problems. The algorithmic and implementation details are detailed in the Appendix."
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.19495412844036697,Published as a conference paper at ICLR 2022
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.19724770642201836,"Algorithm 1 Robust MPC with DeSKO
Require: Weighting matrices Q, R, P, state feedback matrix K, prediction horizon H"
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.19954128440366972,"Initialize ˆµ1 ←µθ(x1)
for t = 1, 2, . . . do"
STABILITY WITH APPROXIMATED KOOPMAN OPERATORS,0.2018348623853211,"Solve (6)-(7) to obtain c∗
t
Apply ut = c∗
t + K(µθ(xt) −ˆµt) to the system (2)
ˆµt+1 ←Aˆµt + Bc∗
t
end for"
EXPERIMENTS,0.20412844036697247,"5
EXPERIMENTS"
EXPERIMENTS,0.20642201834862386,"In this section, we will evaluate the performance of the DeSKO model in terms of modeling, control
and robustness. Speciﬁcally, we evaluate the following aspects: (a) Convergence of the proposed
training algorithm with random parameter initialization; (b) Model performance of the DeSKO
compared to other baselines in learning and predicting diverse dynamics; (c) Reliability of the
control framework achieving successful performance and stability guarantees; (d) Robustness of the
controller when faced with uncertainties unseen during training, such as external disturbances; and
(e) Scalability of the proposed method to high-dimensional complex systems."
EXPERIMENTS,0.20871559633027523,"We illustrate four simulated robotic modeling and control problems to show the general applicability
of DeSKO. First of all, the classic control problem of CartPole balancing from the control and
Reinforcement Learning (RL) literature (Barto et al., 1983) is illustrated. Then, we consider more
complicated high-dimensional continuous control problems of robots, such as the legged robot
HalfCheetah and the soft robotic arm SoPrA. We simulate the HalfCheetah in the MuJoCo physics
engine (Todorov et al., 2012) and the SoPrA (Toshimitsu et al., 2021) in the DRAKE simulation
toolbox (Tedrake & the Drake Development Team, 2019). Lastly, we apply DeSKO to autonomous
systems in cell biology, i.e., biological gene regulatory networks (GRN) (Elowitz & Leibler, 2000).
To further investigate how does DeSKO perform when faced with uncertainty, we introduce process
and observation noise to the CartPole and GRN examples, thus four additional variants of the nominal
systems are included as benchmarks. The environments are detailed in the Appendix. We compare"
EXPERIMENTS,0.21100917431192662,"0
50
100
150
200
250
300
350 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0"
EXPERIMENTS,0.21330275229357798,"MLP
DeSKO
DKO"
EXPERIMENTS,0.21559633027522937,(a) CartPole
EXPERIMENTS,0.21788990825688073,"0
50
100
150
200
250
300
350 3.5 3.0 2.5 2.0 1.5 1.0"
"MLP
DESKO
DKO",0.22018348623853212,"0.5
MLP
DeSKO
DKO"
"MLP
DESKO
DKO",0.22247706422018348,(b) CartPole (Obs Noise)
"MLP
DESKO
DKO",0.22477064220183487,"0
50
100
150
200
250
300
350
3.5 3.0 2.5 2.0 1.5 1.0"
"MLP
DESKO
DKO",0.22706422018348624,"0.5
MLP
DeSKO
DKO"
"MLP
DESKO
DKO",0.22935779816513763,"(c)
CartPole(Process
Noise)"
"MLP
DESKO
DKO",0.231651376146789,"0
50
100
150
200
250
300
350
10 8 6 4 2 0 "
"MLP
DESKO
DKO",0.23394495412844038,"MLP
DeSKO
DKO"
"MLP
DESKO
DKO",0.23623853211009174,(d) GRN
"MLP
DESKO
DKO",0.23853211009174313,"0
50
100
150
200
250
300
350
4.0 3.5 3.0 2.5 2.0 1.5"
"MLP
DESKO
DKO",0.2408256880733945,"MLP
DeSKO
DKO"
"MLP
DESKO
DKO",0.24311926605504589,(e) GRN (Obs Noise)
"MLP
DESKO
DKO",0.24541284403669725,"0
50
100
150
200
250
300
350 1.8 1.6 1.4 1.2 1.0"
"MLP
DESKO
DKO",0.24770642201834864,"0.8
MLP
DeSKO
DKO"
"MLP
DESKO
DKO",0.25,(f) GRN (Process Noise)
"MLP
DESKO
DKO",0.25229357798165136,"0
50
100
150
200
250
300
350 2.50 2.25 2.00 1.75 1.50 1.25"
"MLP
DESKO
DKO",0.2545871559633027,"1.00
MLP
DeSKO
DKO"
"MLP
DESKO
DKO",0.25688073394495414,(g) Halfcheetah
"MLP
DESKO
DKO",0.2591743119266055,"0
50
100
150
200
250
300
350 7 6 5 4 3 2 "
"MLP
DESKO
DKO",0.26146788990825687,"MLP
DKO
DeSKO"
"MLP
DESKO
DKO",0.26376146788990823,(h) SoPrA
"MLP
DESKO
DKO",0.26605504587155965,"Figure 2: Cumulative prediction error on the validation set. The Y-axis indicates the cumulative
mean-squared prediction error in log space over 16 time-steps and the X-axis indicates the training
time steps. The shadowed region shows the conﬁdence interval (one standard deviation) over 10
random seeds."
"MLP
DESKO
DKO",0.268348623853211,"the proposed method with three state-of-the-art baseline methods in terms of modeling and control.
(1) The Deep Koopman Operator (DKO) (Lusch et al., 2018; Han et al., 2020) learns a neural network
as the observable function and the corresponding Koopman operator for forward propagation. In
combination with LQR and MPC, it achieves better control performance than reinforcement learning
methods such as deep deterministic policy gradient (DDPG) (Lillicrap et al., 2019) on OpenAI gym
benchmarks (Brockman et al., 2016). (2) An ensemble of ten Multilayer Perceptrons (MLP) (Chua
et al., 2018) is trained as a baseline for modeling. Each model is a fully connected NN that maps
the current state and action to the next state. The models are trained to minimize the cumulative
prediction error over a prediction horizon. The uncertainty could be quantiﬁed by the range of"
"MLP
DESKO
DKO",0.2706422018348624,Published as a conference paper at ICLR 2022
"MLP
DESKO
DKO",0.27293577981651373,"predictions of the models. (3) Soft actor-critic (SAC) (Haarnoja et al., 2018) is the state-of-the-art
model-free reinforcement learning algorithm. Even though the sample complexity of model-free
approaches is much higher than model-based ones, they can typically converge to solutions with
better performance. SAC updates the controller to minimize cumulative stage costs, thus implicitly
optimizes for a stabilizing controller. Both SAC and DKO could deal with action constraints explicitly.
DKO and MLP also used the multi-step loss for training with the same horizon as DeSKO."
"MLP
DESKO
DKO",0.27522935779816515,"For each environment, a training set composed of 40000 state-action pairs and a validation set of
4000 state-action pairs were collected. The actions were collected by uniformly sampling over the
action space. Both methods were trained to minimize the cumulative prediction error over a time
horizon of 16, and at each update step, a batch of 256 data-points was randomly sampled for the
gradient-descent update. The same learning rate 0.001 and decay strategy were used for both methods.
SAC iteratively interacts with the environments and updates the control policy. For each environment,
1000k steps of state-action-reward pairs were collected for training."
MODELING EVALUATION,0.2775229357798165,"5.1
MODELING EVALUATION
The cumulative prediction error during training on the validation set is shown in Figure 2. As
shown in the plots, DeSKO achieved better or the same prediction performance than DKO. In
particular, in all of the noisy systems, DeSKO achieved better performance; it also scaled better to
high-dimensional systems such as HalfCheetah. Nonetheless, MLP achieved better prediction error
due to its advantage of huge approximation capacity. However, the key drawback of MLP models is
the large computational burden in both forward rollout and backward gradients, making it impossible
to obtain an analytical optimal controller or action input."
MODELING EVALUATION,0.2798165137614679,"0
50
100
150
200
250
t 0.04 0.02 0.00 0.02 0.04 0.06 0.08 0.10"
MODELING EVALUATION,0.28211009174311924,average_path
MODELING EVALUATION,0.28440366972477066,"DeSKO
DKO
SAC
reference"
MODELING EVALUATION,0.286697247706422,(a) CartPole
MODELING EVALUATION,0.2889908256880734,"0
50
100
150
200
250
0.10 0.05 0.00 0.05 0.10 0.15 0.20 0.25"
MODELING EVALUATION,0.29128440366972475,"DeSKO
DKO
SAC
reference t"
MODELING EVALUATION,0.29357798165137616,average_path
MODELING EVALUATION,0.2958715596330275,(b) CartPole (Obs Noise)
MODELING EVALUATION,0.2981651376146789,"0
50
100
150
200
250 0.06 0.04 0.02 0.00 0.02 0.04"
"DESKO
DKO
SAC
REFERENCE",0.30045871559633025,"0.06
DeSKO
DKO
SAC
reference t"
"DESKO
DKO
SAC
REFERENCE",0.30275229357798167,average_path
"DESKO
DKO
SAC
REFERENCE",0.30504587155963303,"(c)
CartPole
(Process
Noise)"
"DESKO
DKO
SAC
REFERENCE",0.3073394495412844,"0
50
100
150
200
250
300
350
400
t 10 0 10 20 30"
"DESKO
DKO
SAC
REFERENCE",0.30963302752293576,average_path
"DESKO
DKO
SAC
REFERENCE",0.3119266055045872,"SAC
DeSKO
DKO
reference"
"DESKO
DKO
SAC
REFERENCE",0.31422018348623854,(d) GRN
"DESKO
DKO
SAC
REFERENCE",0.3165137614678899,"0
50
100
150
200
250
300
350
400 0 5 10 15 20 25"
"DESKO
DKO
SAC
REFERENCE",0.31880733944954126,"DeSKO
DKO
SAC
reference t"
"DESKO
DKO
SAC
REFERENCE",0.3211009174311927,average_path
"DESKO
DKO
SAC
REFERENCE",0.32339449541284404,(e) GRN (Obs Noise)
"DESKO
DKO
SAC
REFERENCE",0.3256880733944954,"0
50
100
150
200
250
300
350
400 0 5 10 15 20 25 30"
"DESKO
DKO
SAC
REFERENCE",0.32798165137614677,"DeSKO
DKO
SAC
reference t"
"DESKO
DKO
SAC
REFERENCE",0.3302752293577982,average_path
"DESKO
DKO
SAC
REFERENCE",0.33256880733944955,(f) GRN (Process Noise)
"DESKO
DKO
SAC
REFERENCE",0.3348623853211009,"0
100
200
300
400
500 2 1 0 1 2 3 4 5"
"DESKO
DKO
SAC
REFERENCE",0.33715596330275227,"DeSKO
DKO
SAC
reference t"
"DESKO
DKO
SAC
REFERENCE",0.3394495412844037,average_path
"DESKO
DKO
SAC
REFERENCE",0.34174311926605505,(g) Halfcheetah
"DESKO
DKO
SAC
REFERENCE",0.3440366972477064,"0
50
100
150
200
250
t 0 1 2 3 4 5 6 7 8"
"DESKO
DKO
SAC
REFERENCE",0.3463302752293578,tracking_error
"DESKO
DKO
SAC
REFERENCE",0.3486238532110092,"DeSKO
DKO
SAC"
"DESKO
DKO
SAC
REFERENCE",0.35091743119266056,(h) SoPrA
"DESKO
DKO
SAC
REFERENCE",0.3532110091743119,"Figure 3: State trajectories (a-g) and mean-square tracking error (h). The y-axis indicates the average
state trajectory (a-g) or tracking error (h) and the x-axis indicates the time steps in each episode. The
shadowed region shows the conﬁdence interval (one standard deviation) over ten random seeds. In
(a-g), only the dimensions where a reference signal is tracked are shown."
CONTROL EVALUATION,0.3555045871559633,"5.2
CONTROL EVALUATION
The control performance of DeSKO and the baselines is compared. A partial state set-point tracking
task is assigned to each testing environment. Further details can be found in the Appendix."
CONTROL EVALUATION,0.3577981651376147,"To evaluate the performance, we observe the angular position of the pole in CartPole, the concentration
of protein 1 in GRN, and the speed in the x-direction in Halfcheetah. For the SoPrA, we record the
sequence of mean-square tracking error, because the reference signal is four-dimensional."
CONTROL EVALUATION,0.36009174311926606,"As shown in Figure 3, DeSKO attained the best control performance with low average tracking error
and variances across different trials in (a-f). In (g) and (h), DeSKO performed comparable to or
slightly worse than SAC. In comparison, DKO failed at (a,b,f) and produced large variances in the
state trajectories in the GRN systems (d,e)."
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.3623853211009174,"5.3
ROBUSTNESS EVALUATION
It is well-known that optimal controllers designed for learned models could be fragile when faced
with unknown impulsive disturbances. Thus, we are also interested in investigating the robustness of
the DeSKO controller when faced with unknown external disturbances. To show this, we introduce"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.3646788990825688,Published as a conference paper at ICLR 2022
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.3669724770642202,"periodic external disturbances (every 20 steps) with different magnitudes in the CartPole and GRN
systems and observe the performance of each controller. These two environments are chosen as
test-beds for robustness due to their higher fragility to disturbances when compared to HalfCheetah
and SoPrA. In CartPole, the pole may fall over when interfered by an external force, ending the
episode in advance. For this reason, we measure the robustness of controllers with the death-rate, i.e.,
the probability of falling over after being disturbed. For GRN where the episodes are always of the
same length, we measure the robustness of controllers by the variation in the cumulative tracking
error. Under each disturbance magnitude, the policies are tested for 100 trials and the performance is
shown in Figure 4."
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.36926605504587157,"80
90
100
110
120
130
140
150
magnitude 20 0 20 40 60 80 100 120"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.37155963302752293,death_rate
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.3738532110091743,"DeSKO
DKO
SAC"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.3761467889908257,(a) CartPole
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.37844036697247707,"80
90
100
110
120
130
140
150
magnitude 20 0 20 40 60 80 100"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.38073394495412843,death_rate
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.3830275229357798,"DeSKO
DKO
SAC"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.3853211009174312,(b) CartPole (Obs Noise)
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.3876146788990826,"80
90
100
110
120
130
140
150
magnitude 20 0 20 40 60 80 100 120"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.38990825688073394,death_rate
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.3922018348623853,"DeSKO
DKO
SAC"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.3944954128440367,(c) CartPole (Process Noise)
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.3967889908256881,"0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
magnitude 5 6 7 8 9 10 11 12"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.39908256880733944,return
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.4013761467889908,"SAC
DKO
DeSKO"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.4036697247706422,(d) GRN
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.4059633027522936,"0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
magnitude 6 7 8 9 10 11 12"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.40825688073394495,return
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.4105504587155963,"SAC
DKO
DeSKO"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.41284403669724773,(e) GRN (Obs Noise)
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.4151376146788991,"0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
magnitude 8 9 10 11 12"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.41743119266055045,return
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.4197247706422018,"SAC
DKO
DeSKO"
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.42201834862385323,(f) GRN (Process Noise)
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.4243119266055046,"Figure 4: Death rate and cumulative tracking error in the presence of persistent disturbances with
different magnitudes. The x-axis indicates the magnitude of the applied disturbances. The y-axis
indicates the death rate in the CartPole systems (a-c) and the cumulative tracking error in log space in
GRN systems (d-e). All of the trained policies are evaluated for 100 trials in each setting."
"ROBUSTNESS EVALUATION
IT IS WELL-KNOWN THAT OPTIMAL CONTROLLERS DESIGNED FOR LEARNED MODELS COULD BE FRAGILE WHEN FACED",0.42660550458715596,"As shown in Figure 4, DeSKO attained the best robustness in all of the tests. In the CartPole examples,
the DeSKO controller can resist disturbances with magnitude up to ﬁve times (100N) of the maximum
control input (20N), without any failure (see (a,b)). In (f), the tracking error of DKO is too high to be
shown in the plots, and the zoomed-out view of (f) can be found in the Appendix."
ABLATION ON THE ENTROPY CONSTRAINT,0.4288990825688073,"5.4
ABLATION ON THE ENTROPY CONSTRAINT"
ABLATION ON THE ENTROPY CONSTRAINT,0.43119266055045874,"80
90
100
110
120
130
140
150
magnitude 0 20 40 60 80 100"
ABLATION ON THE ENTROPY CONSTRAINT,0.4334862385321101,death_rate
ABLATION ON THE ENTROPY CONSTRAINT,0.43577981651376146,"Entropy=-20
Entropy=0
Entropy=10"
ABLATION ON THE ENTROPY CONSTRAINT,0.4380733944954128,"Figure 5: Ablation study on the Entropy
threshold H."
ABLATION ON THE ENTROPY CONSTRAINT,0.44036697247706424,"We are curious to see what is the effect of the entropy
constraint on the robustness of the resulting controller.
To show this, we trained three models in CartPole with
different minimum entropy thresholds H, and constantly
disturb the CartPole to evaluate the robustness of the con-
troller. The death rate of the resulting controllers is shown
in Figure 5. As observed in Figure 5, increasing the value
of H helps the resulting controller to be more robust to
unknown disturbances, which validates the effect of the
entropy constraint."
DISCUSSION AND CONCLUSION,0.4426605504587156,"6
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.44495412844036697,"In this paper, we introduced and discussed an efﬁcient model learning approach called the deep
stochastic Koopman operator (DeSKO). By using deep neural networks to encode the distribution of
observables, the DeSKO model is able to infer and propagate the uncertainty in dynamical systems.
We developed a robust model predictive control framework based on the learned DeSKO model; the
framework guarantees closed-loop stability of the controlled systems. Our experiments show that
DeSKO can be applied to high-dimensional complex nonlinear systems, and it outperforms existing
deep Koopman operator models and RL algorithms in terms of modeling and control. Our control
experiments showed that DeSKO was more robust than the baselines when faced with large external
disturbances."
DISCUSSION AND CONCLUSION,0.44724770642201833,"In the future, it could be interesting to investigate the convergence proof of algorithms based on the
deep Koopman operator. New Koopman representations that have nonlinear control inputs are also
interesting. Another potential future research topic could be control tasks with state constraints."
DISCUSSION AND CONCLUSION,0.44954128440366975,Published as a conference paper at ICLR 2022
DISCUSSION AND CONCLUSION,0.4518348623853211,ACKNOWLEDGMENTS
DISCUSSION AND CONCLUSION,0.4541284403669725,"We thank Lixian Zhang, Miriam Filippi, Lewis Jones, Elvis Nava, Mike Yan Michelis, and Hehui
Zheng for their useful comments and insights. DeSKO is supported by the program of China
Scholarship Council (No.202006120085). We also want to thank the generous gift from Credit Suisse
to the ETH Foundation enabling Soft Robotics Research."
REFERENCES,0.45642201834862384,REFERENCES
REFERENCES,0.45871559633027525,"Omri Azencot, N Benjamin Erichson, Vanessa Lin, and Michael Mahoney. Forecasting sequential
data using consistent koopman autoencoders. In International Conference on Machine Learning,
pp. 475–485. PMLR, 2020."
REFERENCES,0.4610091743119266,"Craig Bakker, Arnab Bhattacharya, Samrat Chatterjee, Casey J. Perkins, and Matthew R. Oster. The
Koopman Operator: Capabilities and Recent Advances. In 2020 Resilience Week (RWS), pp. 34–40,
Salt Lake City, ID, USA, October 2020. IEEE. ISBN 978-1-72818-693-1. doi: 10.1109/RWS50334.
2020.9241276. URL https://ieeexplore.ieee.org/document/9241276/."
REFERENCES,0.463302752293578,"Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can
solve difﬁcult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):
834–846, 1983."
REFERENCES,0.46559633027522934,"Felix Berkenkamp and Angela P. Schoellig. Safe and robust learning control with Gaussian processes.
In 2015 European Control Conference (ECC), pp. 2496–2501, July 2015. doi: 10.1109/ECC.2015.
7330913."
REFERENCES,0.46788990825688076,"Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe Model-based
Reinforcement Learning with Stability Guarantees. arXiv:1705.08551 [cs, stat], November 2017.
URL http://arxiv.org/abs/1705.08551. arXiv: 1705.08551."
REFERENCES,0.4701834862385321,"Homanga Bharadhwaj, Kevin Xie, and Florian Shkurti. Model-predictive control via cross-entropy
and gradient-based optimization. In Learning for Dynamics and Control, pp. 277–286. PMLR,
2020."
REFERENCES,0.4724770642201835,"Francesco Borrelli, Alberto Bemporad, and Manfred Morari. Predictive control for linear and hybrid
systems. Cambridge University Press, 2017."
REFERENCES,0.47477064220183485,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.47706422018348627,"Daniel Bruder, Xun Fu, R. Brent Gillespie, C. David Remy, and Ram Vasudevan. Koopman-based
Control of a Soft Continuum Manipulator Under Variable Loading Conditions. arXiv:2002.01407
[cs], February 2020. URL http://arxiv.org/abs/2002.01407. arXiv: 2002.01407."
REFERENCES,0.4793577981651376,"Gang Cao, Edmund M.-K. Lai, and Fakhrul Alam. Gaussian Process Model Predictive Control
of an Unmanned Quadrotor. Journal of Intelligent & Robotic Systems, 88(1):147–162, October
2017. ISSN 0921-0296, 1573-0409. doi: 10.1007/s10846-017-0549-y. URL http://link.
springer.com/10.1007/s10846-017-0549-y."
REFERENCES,0.481651376146789,"Alexander H. Chang, Christian M. Hubicki, Jeff J. Aguilar, Daniel I. Goldman, Aaron D. Ames,
and Patricio A. Vela. Learning to jump in granular media: Unifying optimal control synthesis
with Gaussian process-based regression. In 2017 IEEE International Conference on Robotics and
Automation (ICRA), pp. 2154–2160, May 2017. doi: 10.1109/ICRA.2017.7989248."
REFERENCES,0.48394495412844035,"Alexander H. Chang, Christian M. Hubicki, Jeffrey J. Aguilar, Daniel I. Goldman, Aaron D. Ames,
and Patricio A. Vela. Learning Terrain Dynamics: A Gaussian Process Modeling and Optimal
Control Adaptation Framework Applied to Robotic Jumping. IEEE Transactions on Control
Systems Technology, pp. 1–16, 2020. ISSN 1558-0865. doi: 10.1109/TCST.2020.3009636.
Conference Name: IEEE Transactions on Control Systems Technology."
REFERENCES,0.48623853211009177,"Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-
ing in a handful of trials using probabilistic dynamics models. arXiv preprint arXiv:1805.12114,
2018."
REFERENCES,0.48853211009174313,Published as a conference paper at ICLR 2022
REFERENCES,0.4908256880733945,"Michael B Elowitz and Stanislas Leibler. A synthetic oscillatory network of transcriptional regulators.
Nature, 403(6767):335–338, 2000."
REFERENCES,0.49311926605504586,"C. Folkestad, D. Pastor, and J. W. Burdick. Episodic Koopman Learning of Nonlinear Robot Dynamics
with Application to Fast Multirotor Landing. In 2020 IEEE International Conference on Robotics
and Automation (ICRA), pp. 9216–9222, May 2020. doi: 10.1109/ICRA40945.2020.9197510.
ISSN: 2577-087X."
REFERENCES,0.4954128440366973,"Thomas George Thuruthel, Yasmin Ansari, Egidio Falotico, and Cecilia Laschi. Control strategies
for soft robotic manipulators: A survey. Soft robotics, 5(2):149–163, 2018."
REFERENCES,0.49770642201834864,"David Ha and J¨urgen Schmidhuber. Recurrent world models facilitate policy evolution. In Proceedings
of the 32nd International Conference on Neural Information Processing Systems, pp. 2455–2467,
2018."
REFERENCES,0.5,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
on Machine Learning, pp. 1861–1870. PMLR, 2018."
REFERENCES,0.5022935779816514,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555–2565. PMLR, 2019."
REFERENCES,0.5045871559633027,"David A. Haggerty, Michael J. Banks, Patrick C. Curtis, Igor Mezi´c, and Elliot W. Hawkes. Modeling,
Reduction, and Control of a Helically Actuated Inertial Soft Robotic Arm via the Koopman
Operator. arXiv:2011.07939 [cs, eess], November 2020. URL http://arxiv.org/abs/
2011.07939. arXiv: 2011.07939."
REFERENCES,0.5068807339449541,"Jessica B. Hamrick, Andrew J. Ballard, Razvan Pascanu, Oriol Vinyals, Nicolas Heess, and Peter W.
Battaglia. Metacontrol for Adaptive Imagination-Based Optimization. arXiv:1705.02670 [cs],
May 2017. URL http://arxiv.org/abs/1705.02670. arXiv: 1705.02670."
REFERENCES,0.5091743119266054,"Yiqiang Han, Wenjian Hao, and Umesh Vaidya. Deep Learning of Koopman Representation for
Control. arXiv:2010.07546 [cs, eess], October 2020. URL http://arxiv.org/abs/2010.
07546. arXiv: 2010.07546."
REFERENCES,0.5114678899082569,"Juraj Kabzan, Lukas Hewing, Alexander Liniger, and Melanie N. Zeilinger. Learning-Based Model
Predictive Control for Autonomous Racing. IEEE Robotics and Automation Letters, 4(4):3363–
3370, October 2019. ISSN 2377-3766. doi: 10.1109/LRA.2019.2926677. Conference Name:
IEEE Robotics and Automation Letters."
REFERENCES,0.5137614678899083,"Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.6980
[cs], 2017. URL http://arxiv.org/abs/1412.6980."
REFERENCES,0.5160550458715596,"Stefan Klus, P´eter Koltai, and Christof Sch¨utte. On the numerical approximation of the Perron-
Frobenius and Koopman operator. Journal of Computational Dynamics, 3(1):1–12, September
2016. ISSN 2158-2491. doi: 10.3934/jcd.2016003. URL http://arxiv.org/abs/1512.
05997. arXiv: 1512.05997."
REFERENCES,0.518348623853211,"J. Kocijan, R. Murray-Smith, C.E. Rasmussen, and A. Girard. Gaussian process model based
predictive control. In Proceedings of the 2004 American Control Conference, volume 3, pp.
2214–2219 vol.3, June 2004. doi: 10.23919/ACC.2004.1383790. ISSN: 0743-1619."
REFERENCES,0.5206422018348624,"J Zico Kolter and Gaurav Manek. Learning stable deep dynamics models. Advances in Neural
Information Processing Systems, 32:11128–11136, 2019."
REFERENCES,0.5229357798165137,"B. O. Koopman.
Hamiltonian Systems and Transformation in Hilbert Space.
Proceedings
of the National Academy of Sciences of the United States of America, 17(5):315–318, May
1931a. ISSN 0027-8424. URL https://www.ncbi.nlm.nih.gov/pmc/articles/
PMC1076052/."
REFERENCES,0.5252293577981652,"Bernard O Koopman. Hamiltonian systems and transformation in hilbert space. Proceedings of the
national academy of sciences of the united states of america, 17(5):315, 1931b."
REFERENCES,0.5275229357798165,Published as a conference paper at ICLR 2022
REFERENCES,0.5298165137614679,"Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, and Antonio Torralba. Learning Compositional
Koopman Operators for Model-Based Control. arXiv:1910.08264 [cs, math, stat], April 2020.
URL http://arxiv.org/abs/1910.08264. arXiv: 1910.08264."
REFERENCES,0.5321100917431193,"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval
Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning.
arXiv:1509.02971 [cs, stat], 2019. URL http://arxiv.org/abs/1509.02971."
REFERENCES,0.5344036697247706,"Bethany Lusch, J. Nathan Kutz, and Steven L. Brunton. Deep learning for universal linear embed-
dings of nonlinear dynamics. Nature Communications, 9(1):4950, December 2018. ISSN 2041-
1723. doi: 10.1038/s41467-018-07210-0. URL http://www.nature.com/articles/
s41467-018-07210-0."
REFERENCES,0.536697247706422,"Giorgos Mamakoukas, Ian Abraham, and Todd D Murphey. Learning data-driven stable koopman
operators. arXiv preprint arXiv:2005.04291, 2020."
REFERENCES,0.5389908256880734,"David Q Mayne, James B Rawlings, Christopher V Rao, and Pierre OM Scokaert. Constrained model
predictive control: Stability and optimality. Automatica, 36(6):789–814, 2000."
REFERENCES,0.5412844036697247,"Jeremy Morton, Freddie D. Witherden, Antony Jameson, and Mykel J. Kochenderfer. Deep Dynamical
Modeling and Control of Unsteady Fluid Flows. arXiv:1805.07472 [cs], November 2018. URL
http://arxiv.org/abs/1805.07472. arXiv: 1805.07472."
REFERENCES,0.5435779816513762,"Jeremy Morton, Freddie D. Witherden, and Mykel J. Kochenderfer. Deep Variational Koopman
Models: Inferring Koopman Observations for Uncertainty-Aware Dynamics Modeling and Control.
arXiv:1902.09742 [cs, stat], June 2019. URL http://arxiv.org/abs/1902.09742.
arXiv: 1902.09742."
REFERENCES,0.5458715596330275,"Dominic M¨uller, Manfred Reichert, and Joachim Herbst. Data-driven modeling and coordination
of large process structures. In OTM Confederated International Conferences” On the Move to
Meaningful Internet Systems”, pp. 131–149. Springer, 2007."
REFERENCES,0.5481651376146789,"Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and
Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-reinforcement
learning. arXiv preprint arXiv:1803.11347, 2018a."
REFERENCES,0.5504587155963303,"Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynam-
ics for model-based deep reinforcement learning with model-free ﬁne-tuning. In 2018 IEEE
International Conference on Robotics and Automation (ICRA), pp. 7559–7566. IEEE, 2018b."
REFERENCES,0.5527522935779816,"Samuel E. Otto and Clarence W. Rowley. Linearly Recurrent Autoencoder Networks for Learning
Dynamics. SIAM Journal on Applied Dynamical Systems, 18(1):558–593, January 2019. ISSN
1536-0040. doi: 10.1137/18M1177846. URL https://epubs.siam.org/doi/10.1137/
18M1177846."
REFERENCES,0.555045871559633,"Shaowu Pan and Karthik Duraisamy. Physics-informed probabilistic learning of linear embeddings
of nonlinear dynamics with guaranteed stability. SIAM Journal on Applied Dynamical Systems, 19
(1):480–509, 2020."
REFERENCES,0.5573394495412844,"Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racani`ere, David
Reichert, Th´eophane Weber, Daan Wierstra, and Peter Battaglia. Learning model-based planning
from scratch. arXiv:1707.06170 [cs, stat], July 2017. URL http://arxiv.org/abs/1707.
06170. arXiv: 1707.06170."
REFERENCES,0.5596330275229358,"S´ebastien Racani`ere, Th´eophane Weber, David P Reichert, Lars Buesing, Arthur Guez, Danilo
Rezende, Adria Puigdomenech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-
augmented agents for deep reinforcement learning. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 5694–5705, 2017."
REFERENCES,0.5619266055045872,"Peter J. Schmid.
Dynamic mode decomposition of numerical and experimental data.
Jour-
nal of Fluid Mechanics, 656:5–28, August 2010.
ISSN 0022-1120, 1469-7645.
doi:
10.1017/S0022112010001217. URL https://www.cambridge.org/core/product/
identifier/S0022112010001217/type/journal_article."
REFERENCES,0.5642201834862385,"Lu Shi and Konstantinos Karydis. Enhancement for robustness of koopman operator-based data-
driven mobile robotic systems. arXiv preprint arXiv:2103.00812, 2021."
REFERENCES,0.5665137614678899,Published as a conference paper at ICLR 2022
REFERENCES,0.5688073394495413,"Aivar Sootla, Natalja Strelkowa, Damien Ernst, Mauricio Barahona, and Guy-Bart Stan. On periodic
reference tracking using batch-mode reinforcement learning with application to gene regulatory
network control. In 52nd IEEE conference on decision and control, pp. 4086–4091. IEEE, 2013."
REFERENCES,0.5711009174311926,"Natalja Strelkowa and Mauricio Barahona. Switchable genetic oscillator operating in quasi-stable
mode. Journal of The Royal Society Interface, 7(48):1071–1082, 2010."
REFERENCES,0.573394495412844,"Naoya Takeishi, Yoshinobu Kawahara, and Takehisa Yairi. Learning koopman invariant subspaces
for dynamic mode decomposition. In Proceedings of the 31st International Conference on Neural
Information Processing Systems, pp. 1130–1140, 2017."
REFERENCES,0.5756880733944955,"Russ Tedrake and the Drake Development Team. Drake: Model-based design and veriﬁcation for
robotics, 2019. URL https://drake.mit.edu."
REFERENCES,0.5779816513761468,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.
IEEE, 2012."
REFERENCES,0.5802752293577982,"Yasunori Toshimitsu, Ki Wan Wong, Thomas Buchner, and Robert Katzschmann. Sopra: Fabrication
& dynamical modeling of a scalable soft continuum robotic arm with integrated proprioceptive
sensing. arXiv preprint arXiv:2103.10726, 2021."
REFERENCES,0.5825688073394495,"Jonathan H. Tu, Clarence W. Rowley, Dirk M. Luchtenburg, Steven L. Brunton, and J. Nathan
Kutz. On Dynamic Mode Decomposition: Theory and Applications. Journal of Computational
Dynamics, 1(2):391–421, 2014. ISSN 2158-2505. doi: 10.3934/jcd.2014.1.391. URL http:
//arxiv.org/abs/1312.0041. arXiv: 1312.0041."
REFERENCES,0.5848623853211009,"Julia Vinogradska, Bastian Bischoff, Duy Nguyen-Tuong, Anne Romer, Henner Schmidt, and Jan
Peters. Stability of controllers for gaussian process forward models. In International Conference
on Machine Learning, pp. 545–554. PMLR, 2016."
REFERENCES,0.5871559633027523,"Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why PINNs fail to train: A neural tangent
kernel perspective. arXiv:2007.14527 [cs, math, stat], 2020. URL http://arxiv.org/abs/
2007.14527."
REFERENCES,0.5894495412844036,"Tingwu Wang and Jimmy Ba. Exploring model-based planning with policy networks. arXiv preprint
arXiv:1906.08649, 2019."
REFERENCES,0.591743119266055,"Matthew O. Williams, Ioannis G. Kevrekidis, and Clarence W. Rowley.
A Data–Driven Ap-
proximation of the Koopman Operator: Extending Dynamic Mode Decomposition.
Jour-
nal of Nonlinear Science, 25(6):1307–1346, December 2015.
ISSN 0938-8974, 1432-
1467. doi: 10.1007/s00332-015-9258-5. URL http://link.springer.com/10.1007/
s00332-015-9258-5."
REFERENCES,0.5940366972477065,"Ming Xu, Matias Quiroz, Robert Kohn, and Scott A Sisson. Variance reduction properties of the
reparameterization trick. In The 22nd International Conference on Artiﬁcial Intelligence and
Statistics, pp. 2711–2720. PMLR, 2019."
REFERENCES,0.5963302752293578,"Enoch Yeung, Zhiyuan Liu, and Nathan O. Hodas. A Koopman Operator Approach for Computing
and Balancing Gramians for Discrete Time Nonlinear Systems. In 2018 Annual American Control
Conference (ACC), pp. 337–344, Milwaukee, WI, June 2018. IEEE. ISBN 978-1-5386-5428-6.
doi: 10.23919/ACC.2018.8431738. URL https://ieeexplore.ieee.org/document/
8431738/."
REFERENCES,0.5986238532110092,"Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, and Sergey Levine.
Solar: Deep structured representations for model-based reinforcement learning. In International
Conference on Machine Learning, pp. 7444–7453. PMLR, 2019."
REFERENCES,0.6009174311926605,Published as a conference paper at ICLR 2022
REFERENCES,0.6032110091743119,"A
THE FROBENIUS-PERRON OPERATOR"
REFERENCES,0.6055045871559633,"Unlike KO, the Frobenius-Perron operator propagates the probabilistic distribution of observables,
thus could better capture the uncertainty in the dynamic."
REFERENCES,0.6077981651376146,"The Frobenius-Perron Operator P : L1(X) →L1(X) is able to characterize the evolution of
probabilistic distributions (Klus et al., 2016), and it is deﬁned as
Z"
REFERENCES,0.6100917431192661,"A
Pg(x)dµ(x) =
Z"
REFERENCES,0.6123853211009175,"f −1(A)
g(x)dµ(x), ∀A ∈B
(14)"
REFERENCES,0.6146788990825688,"where g ∈L1(X) := L1(X, B, µ) is the probabilistic density function (PDF). The measure space
(X, B, µ) is composed of the state space X, the σ-algebra B, and a (probabilistic) measure µ. The FPO
P can be considered a linear, inﬁnite-dimensional representation of the nonlinear, ﬁnite-dimensional
dynamical system shown in (2) (Klus et al., 2016). Furthermore, the FPO is the adjoint operator of
the KO in the following sense (Klus et al., 2016):"
REFERENCES,0.6169724770642202,"⟨ψ, Pg⟩µ =
Z"
REFERENCES,0.6192660550458715,"A
ψ(x)[Pg](x)dµ(x) =
Z"
REFERENCES,0.6215596330275229,"A
[Kψ](x)g(x)dµ(x) = ⟨Kψ, g⟩µ
(15)"
REFERENCES,0.6238532110091743,"where ⟨·, ·⟩µ denotes the duality pairing between the L1 and L∞functions. (15) shows that any
probabilistic distribution g can be propagated with the Koopman operator. In this work, we will
exploit the above relation to propagate the implicit uncertainty by using the KO."
REFERENCES,0.6261467889908257,"B
PROOF OF PROPOSITION 1"
REFERENCES,0.6284403669724771,"Proof. The stability proof is composed of two parts, the stability of the nominal system and the
stability of the error system between the actual system (9) and the nominal system (5). First, we prove
the stability of the nominal system (5). By solving the problem (6)-(7), the optimal control sequence"
REFERENCES,0.6307339449541285,"{c∗
t|t, c∗
t+1|t, . . . , c∗
t+H−1|t}
(16)"
REFERENCES,0.6330275229357798,and the resulting optimal state trajectory
REFERENCES,0.6353211009174312,"{ˆµ∗
t|t, ˆµ∗
t+1|t, . . . , ˆµ∗
t+H−1|t, ˆµ∗
t+H|t}
(17)"
REFERENCES,0.6376146788990825,"at instant t are obtained. By appending the control signal produced by the feedback controller
K ˆµ∗
t+H|t to (16), a suboptimal solution at next time step t + 1 is given by"
REFERENCES,0.6399082568807339,"{c∗
t|t, c∗
t+1|t, . . . , c∗
t+H−1|t, K ˆµ∗
t+H|t}
(18)"
REFERENCES,0.6422018348623854,"and
{ˆµ∗
t|t, ˆµ∗
t+1|t, . . . , ˆµ∗
t+H−1|t, ˆµ∗
t+H|t, AK ˆµ∗
t+H|t}
(19)"
REFERENCES,0.6444954128440367,"where AK := A + BK denotes the closed-loop transition matrix. Based on this suboptimal solution,
we can now prove that the optimal value function V ∗(ˆµt) is decreasing along the trajectory. Due to
the suboptimality of (18) and (19), one has"
REFERENCES,0.6467889908256881,"V ∗(ˆµt+1) ≤ H−1
X"
REFERENCES,0.6490825688073395,"k=1
q(ˆµ∗
t+k|t, c∗
t+k|t) + q(ˆµ∗
t+H|t, K ˆµ∗
t+H|t) + p(AK ˆµ∗
t+H|t)
(20)"
REFERENCES,0.6513761467889908,"= V ∗(ˆµt) + q(ˆµ∗
t+H|t, K ˆµ∗
t+H|t) + p(AK ˆµ∗
t+H|t) −q(ˆµ∗
t|t, c∗
t|t)
(21)"
REFERENCES,0.6536697247706422,"where q(ˆµt, ct) = ∥C ˆµt∥2
Q+∥ct∥2
R denotes the stage cost and p(ˆµt) = ∥C ˆµt∥2
P denotes the terminal
cost. Thus it follows that
V ∗(ˆµt+1) −V ∗(ˆµt) ≤−q(ˆµ∗
t|t, c∗
t|t)"
REFERENCES,0.6559633027522935,"and the optimal value function V ∗(·) is a valid Lyapunov function. Therefore the expectation of
the nominal state E ˆxt = C ˆµt converges to zero as t →∞, that is the nominal state is mean square
stable."
REFERENCES,0.658256880733945,"Second, let’s consider the dynamics of the error system et := µt −ˆµt deﬁned by the difference of (9)
and (5). Substituting the controller (10) into the error system, it follows that"
REFERENCES,0.6605504587155964,"et+1 = (A + BK)et + g(wt)
(22)"
REFERENCES,0.6628440366972477,Published as a conference paper at ICLR 2022
REFERENCES,0.6651376146788991,"Iterate the dynamics of the error system (22) from the initial time instance 1 to t with et+1 =
AKg(wt) + A2
Kg(wt−1) + · · · + At
Kg(w1) + At
Ke1. According to Algorithm 1, the initial instance
e1 equals to zero as ˆµ1 = µθ(x1). Then the L2 norm of the error state is given by
∥et+1∥= ∥AKg(wt) + A2
Kg(wt−1) + · · · + At
Kg(w1)∥"
REFERENCES,0.6674311926605505,"≤∥AKg(wt)∥+ ∥A2
Kg(wt−1)∥+ · · · + ∥At
Kg(w1)∥"
REFERENCES,0.6697247706422018,"≤β∥g(wt)∥+ β2∥g(wt−1)∥+ · · · + βt∥g(w1)∥
Taking the expectation over the random noise wt, and using the fact that the random noise signal at
different time instances are independently distributed, it follows that
E∥et+1∥≤βEwt∥g(wt)∥+ β2Ewt−1∥g(wt−1)∥+ · · · + βtEw1∥g(w1)∥
According to Assumption (1), we can further infer that
E∥et+1∥≤βLEwt∥wt∥+ β2LEwt−1∥wt−1∥+ · · · + βtLEw1∥w1∥"
REFERENCES,0.6720183486238532,≤βLb + β2Lb + · · · + βtLb
REFERENCES,0.6743119266055045,= (β −βt)Lb
REFERENCES,0.676605504587156,"1 −β
where the second inequality is a direct result of Assumption 2. As t →∞, the expectation of the error
state norm is bounded by βLb"
REFERENCES,0.6788990825688074,"1−β . The state of the original system is given by Ext = C(ˆµt + Eet), thus"
REFERENCES,0.6811926605504587,the effect of the error state upon the original state is bounded by βσLb
REFERENCES,0.6834862385321101,"1−β , where σ := ∥C∥. Because the
nominal state is mean square stable and the error between the actual and nominal state is bounded,
the system (2) is proven to be uniformly ultimately bounded."
REFERENCES,0.6857798165137615,"C
PROOF OF PROPOSITION 2"
REFERENCES,0.6880733944954128,"Proof. As the nominal system remains the same as in Proposition 1, the proof for mean square
stability of the nominal system is identical as well. We would focus on proving the uniform ultimate
boundedness of the error system."
REFERENCES,0.6903669724770642,"In presence of the approximation residuals, the dynamic of the error system et := µt −ˆµtis given as
follows,
et+1 = (A + BK)et + g(wt) + ϵt
(23)"
REFERENCES,0.6926605504587156,"Iterate the above equation (22) from the initial time instance 1 to t, one has
et+1 = At
Ke1 + AKϵt + A2
Kϵt−1 + · · · + At
Kϵ1
|
{z
}
Pt
1 Ak
Kϵk"
REFERENCES,0.694954128440367,"+ AKg(wt) + A2
Kg(wt−1) + · · · + At
Kg(w1)
|
{z
}
Pt
1 Ak
Kg(wk)
(24)
. According to Algorithm 1, the initial instance e1 equals to zero as ˆµ1 = µθ(x1). Then the L2 norm
of the error state is given by"
REFERENCES,0.6972477064220184,"∥et+1∥= ∥ t
X"
AK,0.6995412844036697,"1
Ak
Kϵk + t
X"
AK,0.7018348623853211,"1
Ak
Kg(wk)∥ ≤ t
X"
AK,0.7041284403669725,"1
∥Ak
Kϵk∥+ t
X"
AK,0.7064220183486238,"1
∥Ak
Kg(wk)∥ ≤ t
X"
AK,0.7087155963302753,"1
βk∥ϵk∥+ t
X"
AK,0.7110091743119266,"1
βk∥g(wk)∥"
AK,0.713302752293578,"Taking the expectation over the random noise wt, it follows that"
AK,0.7155963302752294,"E∥et+1∥≤ t
X"
AK,0.7178899082568807,"1
βk∥ϵk∥+ t
X"
AK,0.7201834862385321,"1
βk∥g(wk)∥ ≤ t
X"
AK,0.7224770642201835,"1
βkγ + t
X"
AK,0.7247706422018348,"1
βkLb"
AK,0.7270642201834863,= (β −βt)(Lb + γ) 1 −β
AK,0.7293577981651376,Published as a conference paper at ICLR 2022
AK,0.731651376146789,"where the second inequality is a direct result of Assumption 2 and Assumption 3. As t →∞, the
expectation of the error state norm is bounded by βLb+γ"
AK,0.7339449541284404,1−β . Because the nominal state is mean square
AK,0.7362385321100917,stable and the error between the actual and nominal state is bounded by a constant β(Lb+γ)
AK,0.7385321100917431,"1−β
, the
system (2) is proven to be uniformly ultimately bounded."
AK,0.7408256880733946,"D
ROBUST OPTIMAL TRACKING CONTROL"
AK,0.7431192660550459,"We are concerned with two types of tracking problems, the static set-point tracking problems and
the dynamic tracking problems where a time varying reference signal r is given. We will start by
showing how to achieve set-point tracking with the proposed controller."
AK,0.7454128440366973,"For the set-point tracking problems, the nominal MPC controller is formulated as follows"
AK,0.7477064220183486,"V ∗(ˆµt) = min
c0:H−1 H−1
X"
AK,0.75,"k=0
∥C ˆµt+k −r∥2
Q + ∥ct+k −us∥2
R + ∥C ˆµt+H −r∥2
P"
AK,0.7522935779816514,"s.t. ˆµt+k+1 = Aˆµt+k + Bct+k, ct+k ∈U (25)"
AK,0.7545871559633027,"where r refers to the set-point reference signal and us is the set-point control input that sustains the
state at the reference. us could be obtained by solving the following optimization problem"
AK,0.7568807339449541,"min
ˆµs,us∥us∥2"
AK,0.7591743119266054,"C ˆµs = C(Aˆµs + Bus)
C ˆµs = r"
AK,0.7614678899082569,"where ˆµs denotes the unknown nominal set-point observable. Plugging the optimal control input c∗
t
of (25) into the controller (10) yields the used control input. The set-point controller is used in the
SoPrA arm and GRN examples, where the reference signal is constant."
AK,0.7637614678899083,"For dynamic tracking problems like the locomotion of HalfCheetah, the reference signal rt is time
varying and the corresponding MPC is formulated as follows"
AK,0.7660550458715596,"V ∗(ˆµt) = min
c0:H−1 H−1
X"
AK,0.768348623853211,"k=0
∥C ˆµt+k −rt+k∥2
Q + ∥ct+k∥2
R + ∥C ˆµt+H −rt+H∥2
P"
AK,0.7706422018348624,"s.t. ˆµt+k+1 = Aˆµt+k + Bct+k, ct+k ∈U (26)"
AK,0.7729357798165137,There does not exist a set-point control us thus the input regulation is the same as in (6).
AK,0.7752293577981652,"E
EXPERIMENTAL SETUP"
AK,0.7775229357798165,"The experimental evaluation occured in OpenAI Gym (Brockman et al., 2016) or the Drake simula-
tor (Tedrake & the Drake Development Team, 2019). A snapshot of the adopted environments in this
paper can be found in Figure 6."
AK,0.7798165137614679,"(a) Cartpole
(b) HalfCheetah
(c) SoPrA 1 m/s"
AK,0.7821100917431193,"Figure 6: Snapshot of the environments. The CartPole and HaﬂCheetah are simulated in OpenAI
gym. The SoPrA soft robotic arm is visualized using Drake."
AK,0.7844036697247706,Published as a conference paper at ICLR 2022
AK,0.786697247706422,"E.1
CARTPOLE - INVERTED PENDULUM ON A CART"
AK,0.7889908256880734,"We developed a modiﬁed version of CartPole in (Brockman et al., 2016) with a continuous action space
instead of a discrete action space. The system contains a horizontally moving cart and has an inverted
pendulum attached to it. The cart is fully actuated while the inverted pendulum is unactuated. In this
experiment, the controller is expected to maintain the pendulum in its upright, vertical orientation.
The action is the horizontal force applied upon the cart (a ∈[−20, 20]). xthreshold and θthreshold
represents the maximum of position and angle, respectively, xthreshold = 10 and θthreshold = 20◦. The
episode ends if |θ| > θthreshold and the episodes end in advance. The episodes for control evaluation
are of length 250. For robustness evaluation in Section 5.3, we apply an impulsive disturbance force
F on the cart every 20 steps, of which the magnitude ranges from 80 to 150 and the direction is
opposite to the direction of control input."
AK,0.7912844036697247,"E.2
HALFCHEETAH - TWO-LEGGED RUNNING ROBOT"
AK,0.7935779816513762,"HalfCheetah is a legged robot locomotion task adapted from OpenAI Gym (Brockman et al., 2016).
The task is to control a two-legged simulated robot to run at the speed of 1 m/s. The control input
is the torque applied on each joint, ranging from -1 to 1. The episodes for control evaluation are of
length 200."
AK,0.7958715596330275,"To achieve dynamic locomotion, a reference signal is ﬁrst produced for the DeSKO and DKO
controllers. In our case, we trained a model-free RL agent using DDPG (Lillicrap et al., 2019) to run
forward at the desired speed and record its state trajectory as the reference signal. Nonetheless, this
reference signal is suboptimal and could be improved by using model-based planning methods. In
the meantime, SAC is trained directly with the reward to run forward at 1m/s without the need for a
reference signal."
AK,0.7981651376146789,"E.3
SOPRA - SOFT CONTINUUM ROBOTIC ARM"
AK,0.8004587155963303,"SoPrA is a pneumatic two-segment soft continuum robotic arm (Toshimitsu et al., 2021), built and
simulated with the Drake simulation (Tedrake & the Drake Development Team, 2019). The pose of
the SoPrA arm is described by just two conﬁguration variables φ and θ per segment, which are the
relative angle of the plane of bending and the curvature, as is shown in Figure 7. In order to eliminate
a singularity in the representation, the following parameterization is adopted"
AK,0.8027522935779816,"θx := θ cos(φ)
θy := θ sin(φ)"
AK,0.805045871559633,"and the pose vector q = [θx,1, θy,1, θx,2, θy,2], where the subscripts indicate the indexes of the
segments. The state is composed of the pose and its derivative, i.e., x = [q, ˙q]. The controller adjusts
the pressure in the six air chambers of SoPrA. Each segment contains three air chambers. Further
modeling details can be found in (Toshimitsu et al., 2021). The episodes for control evaluation are of
length 250."
AK,0.8073394495412844,"E.4
SYNTHETIC BIOLOGY GENE REGULATORY NETWORKS"
AK,0.8096330275229358,"The gene regulatory networks (GRNs) considered here are in the nano-scale and their physical
properties are vastly different compared to the other examples. Particularly to note is that GRNs can
exhibit interesting oscillatory behavior."
AK,0.8119266055045872,"In this example, we consider a classical dynamical system in systems/synthetic biology which we use
to illustrate the reference tracking task at hand. The GRN is a synthetic three-gene regulatory network
where the dynamics of mRNAs and proteins follow an oscillatory behavior (Elowitz & Leibler,
2000). A discrete-time mathematical description of the GRN, which includes both transcription and"
AK,0.8142201834862385,Published as a conference paper at ICLR 2022
AK,0.8165137614678899,"Figure 7: Conﬁguration space of the two-segment soft robotic arm SoPrA, reproduced from Toshim-
itsu et al. (2021). θi describes the curvature for each bending segment i. ψi describes the relative
angle of the plane of bending for a segment i. mi is the mass of segment i."
AK,0.8188073394495413,"translation dynamics, is given by the following set of discrete-time equations:"
AK,0.8211009174311926,"x1(t + 1) = x1(t) + dt ·

−γ1x1(t) +
a1
K1 + x2
6(t) + u1"
AK,0.823394495412844,"
+ ξ1(t),"
AK,0.8256880733944955,"x2(t + 1) = x2(t) + dt ·

−γ2x2(t) +
a2
K2 + x2
4(t) + u2"
AK,0.8279816513761468,"
+ ξ2(t),"
AK,0.8302752293577982,"x3(t + 1) = x3(t) + dt ·

−γ3x3(t) +
a3
K3 + x2
5(t) + u3"
AK,0.8325688073394495,"
+ ξ3(t),"
AK,0.8348623853211009,"x4(t + 1) = x4(t) + dt · [−c1x4(t) + β1x1(t)] + ξ4(t),
x5(t + 1) = x5(t) + dt · [−c2x5(k) + β2x2(t)] + ξ5(t),
x6(t + 1) = x6(t) + dt · [−c3x6(t) + β3x3(t)] + ξ6(t). (27)"
AK,0.8371559633027523,"Here, x1, x2, x3 (resp. x4, x5, x6) denote the concentrations of the mRNA transcripts (resp. proteins)
of genes 1, 2, and 3, respectively. ξi, ∀i are i.i.d. uniform noise ranging from [−δ, δ], i.e., ξi ∼
U(−δ, δ). During training, δ = 0 and for evaluation δ is set to 0.5 and 1 respectively in Section 5.3.
a1, a2, a3 denote the maximum promoter strength for their corresponding gene, γ1, γ2, γ3 denote the
mRNA degradation rates, c1, c2, c3 denote the protein degradation rates, β1, β2, β3 denote the protein
production rates, and K1, K2, K3 are the dissociation constants. The set of equations in Eq.(27)
corresponds to a topology where gene 1 is repressed by gene 2, gene 2 is repressed by gene 3, and
gene 3 is repressed by gene 1. dt is the discretization time step."
AK,0.8394495412844036,"In practice, only the protein concentrations are observed and given as readouts, for instance via
ﬂuorescent markers (e.g., green ﬂuorescent protein, GFP or red ﬂuorescent protein, mCherry). The
control scheme ui will be implemented by light control signals which can induce the expression of
genes through the activation of their photo-sensitive promoters. To simplify the system dynamics and
as it is usually done for the GRN model (Elowitz & Leibler, 2000), we consider the corresponding
parameters of the mRNA and protein dynamics for different genes to be equal. More background on
mathematical modeling and control of synthetic biology gene regulatory networks can be referred
to (Strelkowa & Barahona, 2010; Sootla et al., 2013). In this example, the parameters are as follows:"
AK,0.841743119266055,"∀i : Ki = 1, ai = 1.6, γi = 0.16, βi = 0.16, ci = 0.06, dt = 1"
AK,0.8440366972477065,"In Figure 8, a single snapshot of the state temporal evolution without ξ is depicted. We uniformly
initialized between 0 to 5, i.e., xi(0) ∼U(0, 5), which is the range we train the policy in Section 5,
persistent oscillatory behavior is also exhibiting similar to the snapshot in Figure 8."
AK,0.8463302752293578,Published as a conference paper at ICLR 2022
AK,0.8486238532110092,"0
200
400
600
800
1000
1200 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5"
AK,0.8509174311926605,"mRNA 1
mRNA 2
mRNA 3
Protein 1
Protein 2
Protein 3"
AK,0.8532110091743119,time steps
AK,0.8555045871559633,Concentration
AK,0.8577981651376146,"Figure 8: A snapshot of the natural oscillatory behavior of a GRN system consisting of 3 genes.
The oscillations have a period of approximately 150 arbitrary time units. The task is to control the
concentration of Protein 1 to track a set-point reference signal. The X-axis denotes time and Y-axis
denotes the value/concentration of each state."
AK,0.8600917431192661,"0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0 20000 40000 60000 80000"
AK,0.8623853211009175,100000
AK,0.8646788990825688,120000
"DESKO
DKO
SAC",0.8669724770642202,"140000
DeSKO
DKO
SAC"
"DESKO
DKO
SAC",0.8692660550458715,magnitude
"DESKO
DKO
SAC",0.8715596330275229,tracking_error
"DESKO
DKO
SAC",0.8738532110091743,"Figure 9: Zoomed-out view of subﬁgure (f) in Figure 4. As shown above, the tracking error of DKO
controller is signiﬁcantly fragile to disturbances."
"DESKO
DKO
SAC",0.8761467889908257,"F
ADDITIONAL SIMULATION RESULTS"
"DESKO
DKO
SAC",0.8784403669724771,"To further show the effectiveness of DeSKO, we also inject noises in the Halfcheetah and SoPrA
environments and test the modeling and control performance of DeSKO and the baselines."
"DESKO
DKO
SAC",0.8807339449541285,"0
50
100
150
200
250
300
350 2.00 1.75 1.50 1.25 1.00 0.75 0.50 0.25"
"MLP
DESKO
DKO",0.8830275229357798,"0.00
MLP
DeSKO
DKO"
"MLP
DESKO
DKO",0.8853211009174312,(a) Halfcheetah (Observation Noise)
"MLP
DESKO
DKO",0.8876146788990825,"0
50
100
150
200
250
300
350
10 8 6 4 2 0 "
"MLP
DESKO
DKO",0.8899082568807339,"DKO
MLP
DeSKO"
"MLP
DESKO
DKO",0.8922018348623854,(b) SoPrA (Observation Noise)
"MLP
DESKO
DKO",0.8944954128440367,"Figure 10: Cumulative prediction error on the validation set. The Y-axis indicates the cumulative
mean-squared prediction error in log space over 16 time-steps and the X-axis indicates the training
time steps. The shadowed region shows the conﬁdence interval (one standard deviation) over 10
random seeds."
"MLP
DESKO
DKO",0.8967889908256881,Published as a conference paper at ICLR 2022
"MLP
DESKO
DKO",0.8990825688073395,"0
100
200
300
400
500 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
"MLP
DESKO
DKO",0.9013761467889908,tracking_error
"MLP
DESKO
DKO",0.9036697247706422,"SAC
DKO
DeSKO"
"MLP
DESKO
DKO",0.9059633027522935,(a) Halfcheetah (Observation Noise)
"MLP
DESKO
DKO",0.908256880733945,"0
50
100
150
200
250 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
"MLP
DESKO
DKO",0.9105504587155964,tracking_error
"MLP
DESKO
DKO",0.9128440366972477,"SAC
DKO
DeSKO"
"MLP
DESKO
DKO",0.9151376146788991,(b) SoPrA (Observation Noise)
"MLP
DESKO
DKO",0.9174311926605505,"Figure 11: Mean-square tracking error. The y-axis indicates the tracking error and the x-axis indicates
the time steps taken during training. The shadowed region shows the conﬁdence interval (one standard
deviation) over ten random seeds."
"MLP
DESKO
DKO",0.9197247706422018,"In terms of modelling, DeSKO is still consistently better than DKO, and even outperforms MLP on
SoPrA with observation noise. In terms of control, DeSKO also shows better performance than DKO
and competitive performance with SAC."
"MLP
DESKO
DKO",0.9220183486238532,"G
REAL-WORLD EXPERIMENT"
"MLP
DESKO
DKO",0.9243119266055045,"To further validate the applicability of DeSKO to real-world control tasks, we tested its control
performance on a real SoPrA arm (Toshimitsu et al., 2021), as shown in Figure 12."
"MLP
DESKO
DKO",0.926605504587156,"The arm was built with silicon and has 6 chambers pneumatically driven by a series of pumps. Each
chamber can hold up to 600 millibars of pressure. The state of the arm is characterized by the
end position and velocity of each segment, and measured by a motion capture system. The control
frequency was set to be 100Hz. We collected a data set of 200000 steps of state-action pairs by
randomly actuating the chambers, which took about 40 minutes."
"MLP
DESKO
DKO",0.9288990825688074,Figure 12: Snapshot of the real SoPrA arm.
"MLP
DESKO
DKO",0.9311926605504587,"G.1
RESULTS"
"MLP
DESKO
DKO",0.9334862385321101,"We train the DeSKO model and the baselines with the same dataset collected in the previous step.
Figure 13 shows that both methods can converge during training, and MLP could still achieve the
lowest prediction error. Then, we exploit the learned model to design a controller and implement it
on the real SoPrA arm on dynamic trajectory tracking problems. The reference trajectories are three
letters on the x-y plane (parallel to the ground), and the robot is expected to follow the trajectories
with its end-point at the speed of 1cm/s. The hyperparameters of the controllers are tuned to reach
their best performance. As shown in Figure 14, DeSKO outperforms DKO in the tracking accuracy
in both scenarios. SAC is not evaluated in the real world, because it requires far more data than the
model-based methods and could likely damage the robot during exploring."
"MLP
DESKO
DKO",0.9357798165137615,Published as a conference paper at ICLR 2022
"MLP
DESKO
DKO",0.9380733944954128,"0
50
100
150
200
250
300
350
0.0 0.2 0.4 0.6 0.8 1.0"
"MLP
DESKO
DKO",0.9403669724770642,"MLP
DKO
DeSKO"
"MLP
DESKO
DKO",0.9426605504587156,"Figure 13: Cumulative prediction error on the validation set. The Y-axis indicates the cumulative
mean-squared prediction error in log space over 16 time-steps and the X-axis indicates the training
time steps. The shadowed region shows the conﬁdence interval (one standard deviation) over 10
random seeds."
"MLP
DESKO
DKO",0.944954128440367,"0.20
0.15
0.10
0.05
0.00
0.05
0.10
0.15
0.20
Y 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20"
"MLP
DESKO
DKO",0.9472477064220184,reference
"MLP
DESKO
DKO",0.9495412844036697,(a) DeSKO-S
"MLP
DESKO
DKO",0.9518348623853211,"0.20
0.15
0.10
0.05
0.00
0.05
0.10
0.15
0.20
Y 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20"
"MLP
DESKO
DKO",0.9541284403669725,reference
"MLP
DESKO
DKO",0.9564220183486238,(b) DeSKO-R
"MLP
DESKO
DKO",0.9587155963302753,"0.20
0.15
0.10
0.05
0.00
0.05
0.10
0.15
0.20
Y 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20"
"MLP
DESKO
DKO",0.9610091743119266,reference
"MLP
DESKO
DKO",0.963302752293578,(c) DeSKO-L
"MLP
DESKO
DKO",0.9655963302752294,"0.20
0.15
0.10
0.05
0.00
0.05
0.10
0.15
0.20
Y 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20"
"MLP
DESKO
DKO",0.9678899082568807,reference
"MLP
DESKO
DKO",0.9701834862385321,(d) DKO-S
"MLP
DESKO
DKO",0.9724770642201835,"0.20
0.15
0.10
0.05
0.00
0.05
0.10
0.15
0.20
Y 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20"
"MLP
DESKO
DKO",0.9747706422018348,reference
"MLP
DESKO
DKO",0.9770642201834863,(e) DKO-R
"MLP
DESKO
DKO",0.9793577981651376,"0.20
0.15
0.10
0.05
0.00
0.05
0.10
0.15
0.20
Y 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20"
"MLP
DESKO
DKO",0.981651376146789,reference
"MLP
DESKO
DKO",0.9839449541284404,(f) DKO-L
"MLP
DESKO
DKO",0.9862385321100917,"Figure 14: Trajectory tracking with DeSKO and DKO. The above ﬁgures show the end-point position
of the SoPrA arm on the x-y plane, controlled by the DeSKO and DKO controllers to track the given
letter trajectories."
"MLP
DESKO
DKO",0.9885321100917431,"H
HYPERPARAMETERS"
"MLP
DESKO
DKO",0.9908256880733946,"The data set is split into the training set composed of 40000 data points, and the validation set
composed of 4000 data points. Each data point contains the current state and action input, and the
resulting state, i.e. {xt, ut, xt+1}. The data is collected by randomly sampling actions from a uniform
distribution over the action space. The DeSKO model is trained with stochastic gradient descent. In
our implementation, the ADAM solver (Kingma & Ba, 2017) is used for optimization. At each step
during training, a batch of 256 data points is sampled from the training set and used for the model
update."
"MLP
DESKO
DKO",0.9931192660550459,Published as a conference paper at ICLR 2022
"MLP
DESKO
DKO",0.9954128440366973,Table 1: Hyperparameters of DeSKO
"MLP
DESKO
DKO",0.9977064220183486,"Hyperparameters
Value
Size of data set D
40000
Batch Size
256
Learning rate
1e-3
Prediction horzion H
16
Structure of µθ(·)
(256,128,64)
Structure of σθ(·)
(256,128,64)
Activation function
ReLU
Dimension of observables
20
Entropy threshold H
-20"
