Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024096385542168677,"In this work we theoretically contribute to neural network approximation by pro-
viding a novel tropical geometrical viewpoint to structured neural network com-
pression. In particular, we show that the approximation error between two neural
networks with ReLU activations and one hidden layer depends on the Hausdorff
distance of the tropical zonotopes of the networks. This theorem comes as a ï¬rst
step towards a purely geometrical interpretation of neural network approxima-
tion. Based on this theoretical contribution, we propose geometrical methods that
employ the K-means algorithm to compress the fully connected parts of ReLU
activated deep neural networks. We analyze the error bounds of our algorithms
theoretically based on our approximation theorem and evaluate them empirically on
neural network compression. Our experiments follow a proof-of-concept strategy
and indicate that our geometrical tools achieve improved performance over relevant
tropical geometry techniques and can be competitive against non-tropical methods."
INTRODUCTION,0.004819277108433735,"1
INTRODUCTION"
INTRODUCTION,0.007228915662650603,"Tropical geometry (Maclagan & Sturmfels, 2015) is a mathematical ï¬eld based on algebraic geometry
and strongly linked to polyhedral and combinatorial geometry. It is built upon the tropical semiring
which originally refers to the min-plus semiring (Rmin, âˆ§, +), but may also refer to the max-plus
semiring (Cuninghame-Green, 2012; ButkoviË‡c, 2010). In our work, we follow the convention of the
max-plus semiring (Rmax, âˆ¨, +) which replaces the classical operations of addition and multiplication
by max and sum respectively. These operations turn polynomials into piecewise linear functions
making them directly applicable in neural networks."
INTRODUCTION,0.00963855421686747,"Tropical mathematics cover a wide range of applications including dynamical systems on weighted
lattices (Maragos, 2017), ï¬nite state transducers (Theodosis & Maragos, 2018; 2019) and convex
regression (Maragos & Theodosis, 2020; Tsilivis et al., 2021). Recently, there has been remarkable
theoretical impact of tropical geometry in the study of neural networks and machine learning (Maragos
et al., 2021). Zhang et al. (2018) prove the equivalence of ReLU activated neural networks with
tropical rational mappings. Furthermore, they use zonotopes to compute a bound on the number
of the networkâ€™s linear regions, which has already been known in (MontÃºfar et al., 2014). In a
similar context, Charisopoulos & Maragos (2018) compute an upper bound to the number of linear
regions of convolutional and maxout layers and propose a randomized algorithm for linear region
counting. Other works employ tropical geometry to examine the training and further properties of
morphological perceptron (Charisopoulos & Maragos, 2017) and morphological neural networks
(Dimitriadis & Maragos, 2021)."
INTRODUCTION,0.012048192771084338,"Pruning or, generally, compressing neural networks gained interest in recent years due to the surprising
capability of reducing the size of a neural network without compromising performance (Blalock et al.,
2020). As tropical geometry explains the mathematical structure of neural networks, pruning may
also be viewed under the perspective of tropical geometry. Indeed, Alfarra et al. (2020) propose an
unstructured compression algorithm based on sparsifying the zonotope matrices of the network. Also,"
INTRODUCTION,0.014457831325301205,â€ Conducted research as a student in National Technical University of Athens.
INTRODUCTION,0.016867469879518072,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01927710843373494,"Smyrnis et al. (2020) construct a novel tropical division algorithm that applies to neural network
minimization. A generalization of this applies to multiclass networks (Smyrnis & Maragos, 2020)."
INTRODUCTION,0.021686746987951807,"Contributions
In our work, we contribute to structured neural network approximation from the
mathematical viewpoint of tropical geometry:"
INTRODUCTION,0.024096385542168676,"â€¢ We establish a novel bound on the approximation error between two neural networks with
ReLU activations and one hidden layer. To prove this we bound the difference of the
networksâ€™ tropical polynomials via the Hausdorff distance of their respective zonotopes.
â€¢ We construct two geometrical neural network compression methods that are based on
zonotope reduction and employ K-means algorithm for clustering. Our algorithms apply on
the fully connected layers of ReLU activated neural networks.
â€¢ Our algorithms are analyzed both theoretically and experimentally. The theoretical eval-
uation is based on the theoretical bound of neural network approximation error. On the
experimental part, we examine the performance of our algorithms on retaining the accuracy
of convolutional neural networks when applying compression on their fully connected layers."
BACKGROUND ON TROPICAL GEOMETRY,0.02650602409638554,"2
BACKGROUND ON TROPICAL GEOMETRY"
BACKGROUND ON TROPICAL GEOMETRY,0.02891566265060241,"We study tropical geometry from the viewpoint of the max-plus semiring (Rmax, âˆ¨, +) which is
deï¬ned as the set Rmax = R âˆª{âˆ’âˆž} equipped with two operations (âˆ¨, +). Operation âˆ¨stands for
max and + stands for sum. In max-plus algebra we deï¬ne polynomials in the following way."
BACKGROUND ON TROPICAL GEOMETRY,0.03132530120481928,"Tropical polynomials
A tropical polynomial f in d variables x = (x1, x2, ..., xd)T is deï¬ned as
the function
f(x) = max
iâˆˆ[n]{aT
i x + bi}
(1)"
BACKGROUND ON TROPICAL GEOMETRY,0.033734939759036145,"where [n] = {1, ..., n}, ai are vectors in Rd and bi is the corresponding monomial coefï¬cient in
Rmax = R âˆª{âˆ’âˆž}. The set of such polynomials constitutes the semiring Rmax[x] of tropical
polynomials. Note that each term aT
i x + bi corresponds to a hyperplane in Rd. We thus call the
vectors {ai}iâˆˆ[n] the slopes of the tropical polynomial, and {bi}iâˆˆ[n] the respective biases. We allow
slopes to be vectors with real coefï¬cients rather than integer ones, as it is normally the case for
polynomials in regular algebra. These polynomials are also referred to as signomials (Dufï¬n &
Peterson, 1973) in the literature."
BACKGROUND ON TROPICAL GEOMETRY,0.03614457831325301,"Polytopes
Polytopes have been studied extensively (Ziegler, 2012; GrÃ¼nbaum, 2013) and occur as
a geometric tool for ï¬elds such as linear programming and optimization. They also have an important
role in the analysis of neural networks. For instance, Zhang et al. (2018); Charisopoulos & Maragos
(2018) show that linear regions of neural networks correspond to vertices of polytopes. Thus, the
counting of linear regions reduces to a combinatorial geometry problem. In what follows, we explore
this connection of tropical geometry with polytopes."
BACKGROUND ON TROPICAL GEOMETRY,0.03855421686746988,"Consider the tropical polynomial deï¬ned in (1). The Newton polytope associated to f(x) is deï¬ned
as the convex hull of the slopes of the polynomial"
BACKGROUND ON TROPICAL GEOMETRY,0.04096385542168675,Newt (f) := conv{ai : i âˆˆ[n]}
BACKGROUND ON TROPICAL GEOMETRY,0.043373493975903614,"Furthermore, the extended Newton polytope of f(x) is deï¬ned as the convex hull of the slopes of the
polynomial extended in the last dimension by the corresponding bias coefï¬cient."
BACKGROUND ON TROPICAL GEOMETRY,0.04578313253012048,"ENewt (f) := conv{(aT
i , bi) : i âˆˆ[n]}"
BACKGROUND ON TROPICAL GEOMETRY,0.04819277108433735,"The following proposition computes the extended Newton polytope that occurs when a tropical
operation is applied between two tropical polynomials. It will allow us to compute the polytope
representation corresponding to a neural networkâ€™s hidden layer.
Proposition 1. (Zhang et al., 2018; Charisopoulos & Maragos, 2018) Let f, g âˆˆRmax[x] be two
tropical polynomials . Then for the extended Newton polytopes it is true that"
BACKGROUND ON TROPICAL GEOMETRY,0.05060240963855422,"ENewt (f âˆ¨g) = conv{ENewt (f) âˆªENewt (g)}
ENewt (f + g) = ENewt (f) âŠ•ENewt (g)"
BACKGROUND ON TROPICAL GEOMETRY,0.05301204819277108,Published as a conference paper at ICLR 2022
BACKGROUND ON TROPICAL GEOMETRY,0.05542168674698795,"ENewt (f)
ENewt (g)
ENewt (f âˆ¨g)
ENewt (f + g)"
BACKGROUND ON TROPICAL GEOMETRY,0.05783132530120482,UF (ENewt(fâˆ¨g))
BACKGROUND ON TROPICAL GEOMETRY,0.060240963855421686,"(0,0,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.06265060240963856,"(2,1,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.06506024096385542,"(1,0,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.06746987951807229,"(0,1,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.06987951807228916,"(0,0,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.07228915662650602,"(0,0,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.0746987951807229,"(2,1,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.07710843373493977,"(1,0,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.07951807228915662,"(0,1,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.0819277108433735,"(0,0,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.08433734939759036,"(1,0,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.08674698795180723,"(0,1,0)"
BACKGROUND ON TROPICAL GEOMETRY,0.0891566265060241,"(0,0,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.09156626506024096,"(3,1,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.09397590361445783,"(2,2,1)"
BACKGROUND ON TROPICAL GEOMETRY,0.0963855421686747,"(2,1,2)"
BACKGROUND ON TROPICAL GEOMETRY,0.09879518072289156,"Figure 1: Illustration of tropical operations between polynomials. The polytope of the max (âˆ¨) of f
and g corresponds to the convex hull of the union of points of the two polytopes and the polytope of
sum (+) corresponds to their Minkowski sum."
BACKGROUND ON TROPICAL GEOMETRY,0.10120481927710843,"Here âŠ•denotes Minkowski addition. In particular, for two sets A, B âŠ†Rd it is deï¬ned as
A âŠ•B := {a + b | a âˆˆA, b âˆˆB}
Corollary 1. This result can be extended to any ï¬nite set of polynomials using induction.
Example 1. Let f, g be two tropical polynomials in 2 variables, such that
f(x, y) = max(2x + y + 1, 0),
g(x, y) = max(x, y, 1)
The tropical operations applied to these polynomials give
f âˆ¨g = max(2x + y + 1, 0, x, y, 1)
f + g = max(3x + y + 1, x, 2x + 2y + 1, y, 2x + y + 2, 1)
Fig. 1 illustrates the extended Newton polytopes of the original and the computed polynomials."
BACKGROUND ON TROPICAL GEOMETRY,0.10361445783132531,"The extended Newton polytope provides a geometrical representation of a tropical polynomial. In
addition, it may be used to compute the values that the polynomial attains, as Proposition 2 indicates.
Proposition 2. (Charisopoulos & Maragos, 2018) Let f âˆˆRmax[x] be a tropical polynomial in d
variables. Let UF (ENewt (f)) be the points in the upper envelope of ENewt (f), where upward
direction is taken with respect to the last dimension of Rd+1. Then for each i âˆˆ[n] there exists a
linear region of f on which f(x) = aT
i x + bi if and only if (aT
i , bi) is a vertex of UF (ENewt (f)).
Example 2. Using the polynomials from Example 1 we compute a reduced representation for f âˆ¨g.
f âˆ¨g = max(2x + y + 1, 0, x, y, 1) = max(2x + y + 1, x, y, 1)
Indeed, the signiï¬cant terms correspond to the vertices of UF(ENewt (f âˆ¨g)) shown in Fig. 1."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.10602409638554217,"2.1
TROPICAL GEOMETRY OF NEURAL NETWORKS"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.10843373493975904,"Tropical geometry has the capability of expressing the mathematical structure of ReLU activated
neural networks. We review some of the basic properties of neural networks and introduce notation
that will be used in our analysis. For this purpose, we consider the ReLU activated neural network of
Fig. 2 with one hidden layer."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.1108433734939759,"Network tropical equations
The network of Fig. 2 consists of an input layer x = (x1, ..., xd), a
hidden layer f = (f1, ..., fn) with ReLU activations, an output layer v = (v1, ..., vm) and two linear
layers deï¬ned by the matrices A, C respectively. As illustrated in Fig. 2 we have Ai,: =
 
aT
i , bi

for
the ï¬rst linear layer and Cj,: = (cj1, cj2, ..., cjn) for the second linear layer, as we ignore its biases.
Furthermore, the output of the iâˆ’th component of the hidden layer f is computed as"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.11325301204819277,"fi(x) = max d
X"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.11566265060240964,"k=1
aikxk + bi, 0 !"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.1180722891566265,"= max(aT
i x + bi, 0)
(2)"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.12048192771084337,"We deduce that each fi is a tropical polynomial with two terms. It therefore follows that ENewt (fi)
is a linear segment in Rd+1. The components of the output layer may be computed as"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.12289156626506025,"vj(x) = n
X"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.12530120481927712,"i=1
cjifi(x) =
X"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.12771084337349398,"cji>0
|cji|fi(x) âˆ’
X"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.13012048192771083,"cji<0
|cji|fi(x) = pj(x) âˆ’qj(x)
(3)"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.13253012048192772,Published as a conference paper at ICLR 2022 x1 x2 ... xk ... xd f1 f2 ... fi ... fn v1 v2 ... vj ... vm ai1 ai2 aik aid cj1 cj2 cji cjn b1 b2 bi bn
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.13493975903614458,"Figure 2: Neural network with one hid-
den ReLU layer. The ï¬rst linear layer has
weights {aT
i } with bias {bi} correspond-
ing to iâˆ’th node âˆ€i âˆˆ[n] and the second
has weights {cji}, âˆ€j âˆˆ[m], i âˆˆ[n]."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.13734939759036144,"Tropical rational functions
In Eq. (3), functions pj, qj
are both linear combinations of {fi} with positive coef-
ï¬cients, which implies that they are tropical polynomials.
We conclude that every output node vi can be written as
a difference of two tropical polynomials, which is deï¬ned
as a tropical rational function. This indicates that the
output layer of the neural network of Fig. 2 is equivalent
to a tropical rational mapping. In fact, this result holds
for deeper networks, in general, as demonstrated by the
following theorem."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.13975903614457832,"Theorem 1. (Zhang et al., 2018) A ReLU activated deep
neural network F : Rd â†’Rm is equivalent to a tropical
rational mapping."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.14216867469879518,"It is not known whether a tropical rational function r(x)
admits an efï¬cient geometric representation that deter-
mines its values {r(x)} for x âˆˆRd, as it holds for tropi-
cal polynomials with their polytopes in Proposition 2. For
this reason, we choose to work separately on the polytopes
of the tropical polynomials pj, qj."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.14457831325301204,"Zonotopes
Zonotopes are deï¬ned as the Minkowski
sum of a ï¬nite set of line segments. They are a special
case of polytopes that occur as a building block for our
network. These geometrical structures provide a representation of the polynomials pj, qj in (3)
that further allows us to build our compression algorithms. We use the notation Pj, Qj for the
extended Newton polytopes of tropical polynomials pj, qj, respectively. Notice from (3) that for each
component vj of the output pj, qj are written as linear combinations of tropical polynomials that
correspond to linear segments. Thus Pj and Qj are zonotopes. We call Pj the positive zonotope,
corresponding to the positive polynomial pj and Qj the negative one."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.14698795180722893,"Zonotope Generators
Each neuron of the hidden layer represents geometrically a line segment
contributing to the positive or negative zonotope. We thus call these line segments generators of the
zonotope. The generators further receive the characterization positive or negative depending on the
zonotope they contribute to. It is intuitive to expect that a zonotope gets more complex as its number
of generators increases. In fact, each vertex of the zonotope can be computed as the sum of vertices of
the generators, where we choose a vertex from each generating line segment, either 0 or cji
 
aT
i , bi

.
We summarize the above with the following extension of (Charisopoulos & Maragos, 2018)."
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.1493975903614458,"Proposition 3. Pj, Qj are zonotopes in Rd+1. For each vertex v of Pj there exists a subset of indices
I+ of {1, 2, ..., n} with cji > 0, âˆ€i âˆˆI+ such that v = P"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.15180722891566265,"iâˆˆI+ cji
 
aT
i , bi

. Similarly, a vertex u of
Qj can be written as u = P"
TROPICAL GEOMETRY OF NEURAL NETWORKS,0.15421686746987953,"iâˆˆIâˆ’cji
 
aT
i , bi

where Iâˆ’corresponds to cji < 0, âˆ€i âˆˆIâˆ’."
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1566265060240964,"3
APPROXIMATION OF TROPICAL POLYNOMIALS"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.15903614457831325,"In this section we present our central theorem that bounds the error between the original and
approximate neural network, when both have the architecture of Fig. 2. To achieve this we need
to derive a bound for the error of approximating a simpler functional structure, namely the tropical
polynomials that represent the neural network. The motivation behind the geometrical bounding of
the error of the polynomials is Proposition 2. It indicates that a polynomialâ€™s values are determined at
each point of the input space by the vertices of the upper envelope of its extended Newton polytope.
Therefore, it is expected that two tropical polynomials with approximately equal extended Newton
polytopes should attain similar values. In fact, this serves as the intuition for our theorem. The metric
we use to deï¬ne the distance between extended Newton polytopes is the Hausdorff distance."
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1614457831325301,"Hausdorff distance
The distance of a point u âˆˆRd from the ï¬nite set V âŠ‚Rd is denoted by either
dist (u, V) or dist (V, u) and computed as minvâˆˆV âˆ¥u âˆ’vâˆ¥which is the Euclidean distance of u"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.163855421686747,Published as a conference paper at ICLR 2022
APPROXIMATION OF TROPICAL POLYNOMIALS,0.16626506024096385,"from its closest point v âˆˆV. The Hausdorff distance H(V, U) of two ï¬nite point sets V, U âŠ‚Rd is
deï¬ned as"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1686746987951807,"H (V, U) = max

max
vâˆˆV dist (v, U) , max
uâˆˆU dist (V, u)
"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1710843373493976,"Let P, ËœP be two polytopes with their vertex sets denoted by VP , V Ëœ
P respectively. We deï¬ne the"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.17349397590361446,"Hausdorff distance H

P, ËœP

of the two polytopes as the Hausdorff distance of their respective vertex
sets VP , V Ëœ
P . Namely,"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.17590361445783131,"H

P, ËœP

:= H (VP , V Ëœ
P )
(4)"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1783132530120482,"Clearly, the Hausdorff distance is a metric of how close two polytopes are to each other. Indeed, it
becomes zero when the two polytopes coincide. According to this metric, the following bound on the
error of tropical polynomial approximation is derived."
APPROXIMATION OF TROPICAL POLYNOMIALS,0.18072289156626506,"Proposition 4. Let p, Ëœp âˆˆRmax[x] be two tropical polynomials and let P = ENewt (p) , ËœP =
ENewt (Ëœp). Then,"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.18313253012048192,"max
xâˆˆB |p(x) âˆ’Ëœp(x)| â‰¤Ï Â· H

P, ËœP
"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1855421686746988,"where B = {x âˆˆRd : âˆ¥xâˆ¥â‰¤r} is the hypersphere of radius r, and Ï =
âˆš"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.18795180722891566,r2 + 1.
APPROXIMATION OF TROPICAL POLYNOMIALS,0.19036144578313252,"The above proposition enables us to handle the more general case of neural networks with one hidden
layer, that are equivalent with tropical rational mappings. By repeatedly applying Proposition 4 to
each tropical polynomial corresponding to the networks, we get the following bound."
APPROXIMATION OF TROPICAL POLYNOMIALS,0.1927710843373494,"Theorem 1. Let v, Ëœv âˆˆRmax[x] be two neural networks with architecture as in Fig. 2. With ËœPj, ËœQj
we denote the positive and negative zonotopes of Ëœv. The following bound holds."
APPROXIMATION OF TROPICAL POLYNOMIALS,0.19518072289156627,"max
xâˆˆB âˆ¥v(x) âˆ’Ëœv(x)âˆ¥1 â‰¤Ï Â· ï£« ï£­
m
X"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.19759036144578312,"j=1
H

Pj, ËœPj

+ H

Qj, ËœQj

ï£¶ ï£¸"
APPROXIMATION OF TROPICAL POLYNOMIALS,0.2,"Remark 1. The reason we choose to compute the error of the approximation on a bounded hyper-
sphere B is twofold. Firstly, the unbounded error of linear terms always diverges to inï¬nity and,
secondly, in practice the working subspace of our dataset is usually bounded, e.g. images."
NEURAL NETWORK COMPRESSION ALGORITHMS,0.20240963855421687,"4
NEURAL NETWORK COMPRESSION ALGORITHMS"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.20481927710843373,"Compression problem formulation
The tropical geometry theorem on the approximation of
neural networks enables us to derive compression algorithms for ReLU activated neural networks.
Suppose that we want to compress the neural network of Fig. 2 by reducing the number of neurons in
the hidden layer, from n to K. Let us assume that the output of the compressed network is the tropical
rational map Ëœv = (Ëœv1, ..., Ëœvm). Its jâˆ’th component may be written as Ëœvj(x) = Ëœpj(x) âˆ’Ëœqj(x) where
using Proposition 3 the zonotopes of Ëœpj, Ëœqj are generated by Ëœcji(ËœaT
i , Ëœbi), âˆ€i. The generators need
to be chosen in such a way that Ëœvj(x) â‰ˆvj(x) for all x âˆˆB. Due to Theorem 1 it sufï¬ces to ï¬nd"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.20722891566265061,"generators such that the resulting zonotopes have H

Pj, ËœPj

, H

Qj, ËœQj

as small as possible âˆ€j.
We thus formulated neural network compression as a geometrical zonotope approximation problem."
NEURAL NETWORK COMPRESSION ALGORITHMS,0.20963855421686747,"Our approaches
Approximating a zonotope with fewer generators is a problem known as zonotope
order reduction (Kopetzki et al., 2017). In our case we approach this problem by manipulating the
zonotope generators cji
 
aT
i , bi

, âˆ€i, j 1. Each of the algorithms presented will create a subset of
altered generators that approximate the original zonotopes. Ideally, we require the approximation to
hold simultaneously for all positive and negative zonotopes of each output component vj. However,
this is not always possible, as in the case of multiclass neural networks, and it necessarily leads to
heuristic manipulation. Our ï¬rst attempt to tackle this problem applies the K-means algorithm to the"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.21204819277108433,1Dealing with the full generated zonotope would lead to exponential computational overhead.
NEURAL NETWORK COMPRESSION ALGORITHMS,0.21445783132530122,Published as a conference paper at ICLR 2022 x1 x2 f3 f4 f5 f6 f7 f2 f1 v c1 c2 c3 c4 c5 c6 c7 b1 b2 b3 b4 b5 b6 b7
NEURAL NETWORK COMPRESSION ALGORITHMS,0.21686746987951808,"{aji}iâˆˆ(1,2), jâˆˆ(1,...,7)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.21927710843373494,(a) Original network.
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2216867469879518,"c1(aT
1 , b1)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.22409638554216868,"c2(aT
2 , b2)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.22650602409638554,"c3(aT
3 , b3)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2289156626506024,"c4(aT
4 , b4)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.23132530120481928,"c5(aT
5 , b5)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.23373493975903614,"c6(aT
6 , b6)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.236144578313253,"c7(aT
7 , b7)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2385542168674699,(b) Original zonotopes.
NEURAL NETWORK COMPRESSION ALGORITHMS,0.24096385542168675,"Ëœc1(ËœaT
1 , Ëœb1)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2433734939759036,"Ëœc2(ËœaT
2 , Ëœb2)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2457831325301205,"Ëœc4(ËœaT
4 , Ëœb4) â‰¡c7(aT
7 , b7)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.24819277108433735,"Ëœc3(ËœaT
3 , Ëœb3)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.25060240963855424,(c) Resulting zonotopes. x1 x2 Ëœf1 Ëœf2 Ëœf3 Ëœf4 v + + âˆ’ âˆ’
NEURAL NETWORK COMPRESSION ALGORITHMS,0.25301204819277107,Ëœc1Ëœb1
NEURAL NETWORK COMPRESSION ALGORITHMS,0.25542168674698795,Ëœc2Ëœb2
NEURAL NETWORK COMPRESSION ALGORITHMS,0.25783132530120484,Ëœc3Ëœb3
NEURAL NETWORK COMPRESSION ALGORITHMS,0.26024096385542167,"Ëœc4Ëœb4
{ËœcjËœaji}iâˆˆ(1,2), jâˆˆ(1,...,4)"
NEURAL NETWORK COMPRESSION ALGORITHMS,0.26265060240963856,(d) Compressed network.
NEURAL NETWORK COMPRESSION ALGORITHMS,0.26506024096385544,"Figure 3: Illustration of Zonotope K-means execution. The original zonotope P is generated by
ci
 
aT
i , bi

for i = 1, ..., 4 and the negative zonotope Q generated by the remaining ones i = 5, 6, 7."
NEURAL NETWORK COMPRESSION ALGORITHMS,0.2674698795180723,"The approximation ËœP of P is colored in purple and generated by Ëœci

ËœaT
i , Ëœbi

, i = 1, 2 where the
ï¬rst generator is the K-means center representing the generators 1, 2 of P and the second is the
representative center of 3, 4. Similarly, the approximation ËœQ of Q is colored in green and deï¬ned by
the generators Ëœci

ËœaT
i , Ëœbi

, i = 3, 4 that stand as representative centers for {5, 6} and 7 respectively."
NEURAL NETWORK COMPRESSION ALGORITHMS,0.26987951807228916,"positive and negative generators, separately. This method is restricted on applying to single output
neural networks. Our second approach further develops this technique to multiclass neural networks.
Speciï¬cally, it utilizes K-means on the vectors associated with the neural paths passing from a node
in the hidden layer, as we deï¬ne later. The algorithms we present refer to the neural network of Fig.
2 with one hidden layer, but we may repeatedly apply them to compress deeper networks."
ZONOTOPE APPROXIMATION,0.27228915662650605,"4.1
ZONOTOPE APPROXIMATION"
ZONOTOPE APPROXIMATION,0.2746987951807229,"Zonotope K-means
The ï¬rst compression approach uses K-means to compress each zonotope of
the network, and covers only the case of a single output neural network, e.g as in Fig. 2 but with
m = 1. The algorithm reduces the hidden layer size from n to K neurons. We use the notation
ci, i = 1, ..., n for weights of the second linear layer, connecting the hidden layer with the output
node. Algorithm 1 is presented below and a demonstration of its execution can be found in Fig. 3."
ZONOTOPE APPROXIMATION,0.27710843373493976,Algorithm 1: Zonotope K-means Compression
SPLIT GENERATORS INTO POSITIVE,0.27951807228915665,"1. Split generators into positive

ci
 
aT
i , bi

: ci > 0
	
and negative

ci
 
aT
i , bi

: ci < 0
	
."
APPLY K-MEANS FOR K,0.2819277108433735,2. Apply K-means for K
APPLY K-MEANS FOR K,0.28433734939759037,"2 centers, separately for both sets of generators and receive
n
Ëœci

ËœaT
i , Ëœbi

: Ëœci > 0
o
,
n
Ëœci

ËœaT
i , Ëœbi

: Ëœci < 0
o
as output."
APPLY K-MEANS FOR K,0.28674698795180725,"3. Construct the ï¬nal weights. For the ï¬rst linear layer, the weights and the bias which
correspond to the iâˆ’th neuron become the vector Ëœci

ËœaT
i , Ëœbi

."
APPLY K-MEANS FOR K,0.2891566265060241,"4. The weights of the second linear layer are set to 1 for every hidden layer neuron
where Ëœci

ËœaT
i , Ëœbi

occurs from positive generators and âˆ’1, elsewhere."
APPLY K-MEANS FOR K,0.29156626506024097,Proposition 5. Zonotope K-means produces a compressed neural network with output Ëœv satisfying
APPLY K-MEANS FOR K,0.29397590361445786,"1
Ï Â· max
xâˆˆB |v(x) âˆ’Ëœv(x)| â‰¤K Â· Î´max +

1 âˆ’
1
Nmax 
n
X"
APPLY K-MEANS FOR K,0.2963855421686747,"i=1
|ci|âˆ¥
 
aT
i , bi

âˆ¥"
APPLY K-MEANS FOR K,0.2987951807228916,"where K is the total number of centers used in both K-means, Î´max is the largest distance from a point
to its corresponding cluster center and Nmax is the maximum cardinality of a cluster."
APPLY K-MEANS FOR K,0.30120481927710846,"The above proposition provides an upper bound between the original neural network and the one that
is approximated with Zonotope K-means. In particular, if we use K = n centers the bound of the"
APPLY K-MEANS FOR K,0.3036144578313253,Published as a conference paper at ICLR 2022
APPLY K-MEANS FOR K,0.3060240963855422,"approximation error becomes 0, because then Î´max = 0 and Nmax = 1. Also, if K â‰ˆ0 the bound gets
a ï¬xed value depending on the magnitude of the weights of the linear layers."
MULTIPLE ZONOTOPE APPROXIMATION,0.30843373493975906,"4.2
MULTIPLE ZONOTOPE APPROXIMATION"
MULTIPLE ZONOTOPE APPROXIMATION,0.3108433734939759,"The exact positive and negative zonotope approximation performed by Zonotope K-means algorithm
has a main disadvantage: it can only be used in single output neural networks. Indeed, suppose
that we want to employ the preceeding algorithm to approach the zonotopes of each output in a
multiclass neural network. That would require 2m separate executions of K-means which are not
necessarily consistent. For instance, it is possible to have cj1i > 0 and cj2i < 0 for some output
components vj1, vj2. That means that in the compression procedure of vj1, the iâˆ’th neuron belongs
to the positive generators set, while for vj2, it belongs to the negative one. This makes the two
compressions incompatible. Moreover, the drawback of restricting to single output only allow us to
compress the ï¬nal ReLU layer and not any preceeding ones."
MULTIPLE ZONOTOPE APPROXIMATION,0.3132530120481928,"Neural Path K-means
To overcome this obstacle we apply a simultaneous approximation of the
zonotopes. The method is called Neural Path K-means and directly applies K-means to the vectors of
the weights
 
aT
i , bi, c1i, ..., cmi

associated to each neuron i of the hidden layer. The name of the
algorithm emanates from the fact that the vector associated to each neuron consists of the weights of
all the neural network paths passing from this neuron. The procedure is presented in Algorithm 2."
MULTIPLE ZONOTOPE APPROXIMATION,0.3156626506024096,Algorithm 2: Neural Path K-means Compression
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3180722891566265,"1. Apply K-means for K centers to the vectors
 
aT
i , bi, CT
:,i

, i = 1, ..., n, and get"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3204819277108434,"the centers

ËœaT
i , Ëœbi, ËœCT
:,i

, i = 1, ..., K."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3228915662650602,"2. Construct the ï¬nal weights. For the ï¬rst linear layer matrix the i âˆ’th row becomes

ËœaT
i , Ëœbi

, while for the second linear layer matrix, the i âˆ’th column becomes ËœC:,i."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3253012048192771,"Null Generators
Neural Path K-means does not apply compression directly to each zonotope of
the network, but is rather a heuristic approach for this task. More precisely, if we focus on the set
of generators of the zonotopes of output j, Neural Path K-means might mix positive and negative
generators together in the same cluster. For instance, suppose

ËœaT
k , Ëœbk, ËœCT
:,k

is the cluster center"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.327710843373494,"corresponding to vectors
 
aT
i , bi, CT
:,i

for i âˆˆI. Then regarding output j, it is not necessary
that âˆ€i âˆˆI all cji have the same sign. Thus, the compressed positive zonotope ËœPj might contain
generators of the original negative zonotope Qj and vice versa. We will call generators cji
 
aT
i , bi
"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3301204819277108,"contributing to opposite zonotopes, null generators."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3325301204819277,Proposition 6. Neural Path K-means produces a compressed neural network with output Ëœv satisfying
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3349397590361446,"1
Ï Â· max
xâˆˆB âˆ¥v(x) âˆ’Ëœv(x)âˆ¥1 â‰¤âˆšmKÎ´2
max + âˆšm

1 âˆ’
1
Nmax 
n
X"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3373493975903614,"i=1
âˆ¥C:,iâˆ¥
 
aT
i , bi
 +"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3397590361445783,"âˆšmÎ´max Nmin n
X i=1"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3421686746987952,"  
aT
i , bi
 + âˆ¥C:,iâˆ¥

+ m
X j=1 X"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.344578313253012,"iâˆˆNj
|cji|
 
aT
i , bi
"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3469879518072289,"where K is the number of K-means clusters, Î´max the maximum distance from any point to its
corresponding cluster center, Nmax, Nmin the maximum and minimum cardinality respectively of a
cluster and Nj the set of null generators with respect to output j."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3493975903614458,"The performance of Neural Path K-means is evaluated with Proposition 6. The result we deduce
is analogous to Zonotope K-means. The bound of the approximation error becomes zero when K
approaches n. Indeed, for K = n we get Î´max = 0, Nmax = 1 and Nj = âˆ…, âˆ€j âˆˆ[m]. For lower
values of K, the upper bound reaches a value depending on the magnitude of the weights of the linear
layers together with weights corresponding to null generators."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.35180722891566263,Published as a conference paper at ICLR 2022
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3542168674698795,Table 1: Reporting accuracy of compressed networks for single output compression methods.
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3566265060240964,"Percentage of
remaining
neurons"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.35903614457831323,"MNIST 3/5
MNIST 4/9"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3614457831325301,"(Smyrnis et al.,
2020)
Zonotope
K-means
Neural Path
K-means
(Smyrnis et al.,
2020)
Zonotope
K-means
Neural Path
K-means"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.363855421686747,"100% (Original)
99.18 Â± 0.27
99.38 Â± 0.09
99.38 Â± 0.09
99.53 Â± 0.09
99.53 Â± 0.09
99.53 Â± 0.09
5%
99.12 Â± 0.37
99.42 Â± 0.07
99.25 Â± 0.04
98.99 Â± 0.09
99.52 Â± 0.09
99.48 Â± 0.15
1%
99.11 Â± 0.36
99.39 Â± 0.05
99.32 Â± 0.03
99.01 Â± 0.09
99.46 Â± 0.05
99.35 Â± 0.17
0.5%
99.18 Â± 0.36
99.41 Â± 0.05
99.22 Â± 0.11
98.81 Â± 0.09
99.35 Â± 0.24
98.84 Â± 1.18
0.3%
99.18 Â± 0.36
99.25 Â± 0.37
99.19 Â± 0.41
98.81 Â± 0.09
98.22 Â± 1.38
98.22 Â± 1.33"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.36626506024096384,"Table 2: Reporting theoretical upper bounds of Propositions 5, 6."
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3686746987951807,"Percentage of
remaining neurons
MNIST 3/5
MNIST 4/9"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.3710843373493976,"Zonotope K-means
Neural Path K-means
Zonotope K-means
Neural Path K-means"
APPLY K-MEANS FOR K CENTERS TO THE VECTORS,0.37349397590361444,"100%
0.00
0.00
0.00
0.00
10%
17.07
246.74
18.85
229.37
2.5%
15.35
59.42
17.02
63.37
1%
14.79
42.22
16.44
45.58
0.5%
14.57
36.47
16.20
39.71"
EXPERIMENTS,0.3759036144578313,"5
EXPERIMENTS"
EXPERIMENTS,0.3783132530120482,"We conduct experiments on compressing the linear layers of convolutional neural networks. Our
experiments serve as proof-of-concept and indicate that our theoretical claims indeed hold in practice.
The heart of our contribution lies in presenting novel tropical geometrical background for neural
network approximation that will shed light for further research towards tropical mathematics."
EXPERIMENTS,0.38072289156626504,"Our methods compress the linear layers of the network layer by layer. They perform a functional
approximation of the original network and thus they are applicable for both classiï¬cation and
regression tasks. To compare them with other techniques in the literature we choose methods with
similar structure, i.e. structured pruning techniques without re-training. For example, Alfarra et al.
(2020) proposed a compression algorithm based on the sparsiï¬cation of the matrices representing the
zonotopes which served as an intuition for part of our work. However, their method is unstructured
and incompatible for comparison. The methods we choose to compare are two tropical methods for
single-output (Smyrnis et al., 2020) and multi-output (Smyrnis & Maragos, 2020) networks, Random
and L1 Structured, and a modiï¬cation of ThiNet (Luo et al., 2017) adapted to linear layers. Smyrnis
et al. (2020); Smyrnis & Maragos (2020) proposed a novel tropical division framework that aimed on
the reduction of zonotope vertices. Random method prunes neurons according to uniform probability,
while L1 prunes those with the lowest value of L1 norm of their weights. Also, ThiNet uses a greedy
criterion for discarding the neurons that have the smallest contribution to the output of the network."
EXPERIMENTS,0.38313253012048193,"MNIST Dataset, Pairs 3-5 and 4-9
The ï¬rst experiment is performed on the binary classiï¬cation
tasks of pairs 3/5 and 4/9 of the MNIST dataset and so we can utilize both of our proposed methods.
In Table 1, we compare our methods with a tropical geometrical approach of Smyrnis et al. (2020).
Their method is based on a tropical division framework for network minimization. For fair comparison,
we use the same CNN with two fully connected layers and hidden layer of size 1000. According to
Table 1, our techniques seem to have similar performance. They retain the accuracy of the network
while reducing its size. Moreover, in Table 2 we include experimental computation of the theoretical
bounds provided by Proposition 5, 6. We notice that the bounds decrease as the remaining weights
get less. The behaviour of the bounds was expected to be incremental because the less weights we
use, the compression gets worse and the error becomes larger. However, the opposite holds which
means that the bounds are tighter for higher pruning rates. It is also important to mention that the
bounds become 0 when we keep all the weights, as expected."
EXPERIMENTS,0.3855421686746988,"MNIST and Fashion-MNIST Datasets
For the second experiment we employ MNIST and
Fashion-MNIST datasets. The corresponding classiï¬cation is multiclass and thus Neural Path
K-means may only be applied. In Table 3, we compare it with the multiclass tropical method of
Smyrnis & Maragos (2020) using the same CNN architecture they do. Furthermore, in plots 4a,
4b we compare Neural Path K-means with ThiNet and baseline pruning methods by compressing"
EXPERIMENTS,0.38795180722891565,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.39036144578313253,Table 3: Reporting accuracy of compressed networks for multiclass compression methods.
EXPERIMENTS,0.3927710843373494,"Percentage of
remaining neurons
MNIST
Fashion-MNIST"
EXPERIMENTS,0.39518072289156625,"(Smyrnis & Maragos, 2020)
Neural Path K-means
(Smyrnis & Maragos, 2020)
Neural Path K-means"
EXPERIMENTS,0.39759036144578314,"100% (Original)
98.60 Â± 0.03
98.61 Â± 0.11
88.66 Â± 0.54
89.52 Â± 0.19
50%
96.39 Â± 1.18
98.13 Â± 0.28
83.30 Â± 2.80
88.22 Â± 0.32
25%
95.15 Â± 2.36
98.42 Â± 0.42
82.22 Â± 2.85
86.67 Â± 1.12
10%
93.48 Â± 2.57
96.89 Â± 0.55
80.43 Â± 3.27
86.04 Â± 0.94
5%
92.93 Â± 2.59
96.31 Â± 1.29
âˆ’
83.68 Â± 1.06"
EXPERIMENTS,0.4,"LeNet5 (LeCun et al., 1998). To get a better idea of how our method performs in deeper architectures
we provide plots 4c,4d that illustrate the performance of compressing a deep neural network with
layers of size 28 âˆ—28, 512, 256, 128 and 10, which we refer to as deepNN. The compression is
executed on all hidden layers beginning from the input and heading to the output. From Table 3, we
deduce that our method performs better than (Smyrnis & Maragos, 2020). Also, it achieves higher
accuracy scores and experience lower variance as shown in plots 4a-4d. Neural Path K-means, overall,
seems to have good performance, even competitive to ThiNet. Its worst performance occurs on low
percentages of remaining weights. An explanation for this is that K-means provides a high-quality
compression as long as the number of centers is not less than the number of ""real"" clusters."
EXPERIMENTS,0.40240963855421685,"(a) LeNet5, MNIST
(b) LeNet5, F-MNIST
(c) deepNN, MNIST
(d) deepNN, F-MNIST"
EXPERIMENTS,0.40481927710843374,"(e) AlexNet, CIFAR10
(f) CIFAR-VGG, CIFAR10
(g) AlexNet, CIFAR100
(h)
CIFAR-VGG,
CI-
FAR100"
EXPERIMENTS,0.4072289156626506,"Figure 4: Neural Path K-means compared with baseline pruning methods and ThiNet. Horizontal
axis shows the ratio of remaining neurons in each hidden layer of the fully connected part."
EXPERIMENTS,0.40963855421686746,"CIFAR Dataset
We conduct our ï¬nal experiment on CIFAR datasets using CIFAR-VGG (Blalock
et al., 2020) and an altered version of AlexNet adapted for CIFAR. The resulting plots are shown in
Fig. 4e-4h. We deduce that Neural Path K-means retains a good performance on larger datasets. In
particular, in most cases it has slightly better accuracy an lower deviation than the baselines, but has
worse behaviour when keeping almost zero weights."
CONCLUSIONS AND FUTURE WORK,0.41204819277108434,"6
CONCLUSIONS AND FUTURE WORK"
CONCLUSIONS AND FUTURE WORK,0.41445783132530123,"We presented a novel theorem on the bounding of the approximation error between two neural
networks. This theorem occurs from the bounding of the tropical polynomials representing the neural
networks via the Hausdorff distance of their extended Newton polytopes. We derived geometrical
compression algorithms for the fully connected parts of ReLU activated deep neural networks, while
application to convolutional layers is an ongoing work. Our algorithms seem to perform well in
practice and motivate further research towards the direction revealed by tropical geometry."
CONCLUSIONS AND FUTURE WORK,0.41686746987951806,Published as a conference paper at ICLR 2022
REFERENCES,0.41927710843373495,REFERENCES
REFERENCES,0.42168674698795183,"Motasem Alfarra, Adel Bibi, Hasan Hammoud, Mohamed Gaafar, and Bernard Ghanem. On the
decision boundaries of deep neural networks: A tropical geometry perspective. arXiv preprint
arXiv:2002.08838, 2020."
REFERENCES,0.42409638554216866,"Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of
neural network pruning? arXiv preprint arXiv:2003.03033, 2020."
REFERENCES,0.42650602409638555,"Peter ButkoviË‡c. Max-linear systems: theory and algorithms. Springer monographs in mathematics.
Springer, 2010. ISBN 978-1-84996-298-8."
REFERENCES,0.42891566265060244,"Vasileios Charisopoulos and Petros Maragos. Morphological perceptrons: geometry and training
algorithms. In International Symposium on Mathematical Morphology and Its Applications to
Signal and Image Processing, pp. 3â€“15. Springer, 2017."
REFERENCES,0.43132530120481927,"Vasileios Charisopoulos and Petros Maragos. A tropical approach to neural networks with piecewise
linear activations. arXiv preprint arXiv:1805.08749, 2018."
REFERENCES,0.43373493975903615,"Raymond A Cuninghame-Green. Minimax algebra, volume 166. Springer Science & Business
Media, 2012."
REFERENCES,0.43614457831325304,"Nikolaos Dimitriadis and Petros Maragos. Advances in morphological neural networks: Training,
pruning and enforcing shape constraints. In Proc. 46th IEEE Intâ€™l Conf. Acoustics, Speech and
Signal Processing (ICASSP-2021), Toronto, June 2021."
REFERENCES,0.43855421686746987,"Richard James Dufï¬n and Elmor L Peterson. Geometric programming with signomials. Journal of
Optimization Theory and Applications, 11(1):3â€“35, 1973."
REFERENCES,0.44096385542168676,"Branko GrÃ¼nbaum. Convex polytopes, volume 221. Springer Science & Business Media, 2013."
REFERENCES,0.4433734939759036,"Anna-Kathrin Kopetzki, Bastian SchÃ¼rmann, and Matthias Althoff. Methods for order reduction of
zonotopes. In 2017 IEEE 56th Annual Conference on Decision and Control (CDC), pp. 5626â€“5633.
IEEE, 2017."
REFERENCES,0.4457831325301205,"Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278â€“2324, 1998."
REFERENCES,0.44819277108433736,"Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. ThiNet: A Filter Level Pruning Method for Deep Neural
Network Compression. In 2017 IEEE International Conference on Computer Vision (ICCV), pp.
5068â€“5076, 2017. doi: 10.1109/ICCV.2017.541."
REFERENCES,0.4506024096385542,"Diane Maclagan and Bernd Sturmfels. Introduction to tropical geometry, volume 161. American
Mathematical Soc., 2015."
REFERENCES,0.4530120481927711,"Petros Maragos. Dynamical systems on weighted lattices: general theory. Mathematics of Control,
Signals, and Systems, 29(4):1â€“49, 2017."
REFERENCES,0.45542168674698796,"Petros Maragos and Emmanouil Theodosis. Multivariate tropical regression and piecewise-linear
surface ï¬tting. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pp. 3822â€“3826. IEEE, 2020."
REFERENCES,0.4578313253012048,"Petros Maragos, Vasileios Charisopoulos, and Emmanouil Theodosis. Tropical geometry and machine
learning. Proceedings of the IEEE, 109(5):728â€“755, 2021. doi: 10.1109/JPROC.2021.3065238."
REFERENCES,0.4602409638554217,"Guido MontÃºfar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. arXiv preprint arXiv:1402.1869, 2014."
REFERENCES,0.46265060240963857,"Georgios Smyrnis and Petros Maragos. Multiclass neural network minimization via tropical newton
polytope approximation. In Proc. Intâ€™l Conf. on Machine Learning, PMLR, 2020."
REFERENCES,0.4650602409638554,"Georgios Smyrnis, Petros Maragos, and George Retsinas. Maxpolynomial division with application to
neural network simpliï¬cation. In ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 4192â€“4196. IEEE, 2020."
REFERENCES,0.4674698795180723,Published as a conference paper at ICLR 2022
REFERENCES,0.46987951807228917,"Emmanouil Theodosis and Petros Maragos. Analysis of the viterbi algorithm using tropical algebra
and geometry. In 2018 IEEE 19th International Workshop on Signal Processing Advances in
Wireless Communications (SPAWC), pp. 1â€“5. IEEE, 2018."
REFERENCES,0.472289156626506,"Emmanouil Theodosis and Petros Maragos. Tropical modeling of weighted transducer algorithms on
graphs. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 8653â€“8657, 2019. doi: 10.1109/ICASSP.2019.8683127."
REFERENCES,0.4746987951807229,"Nikos Tsilivis, Anastasios Tsiamis, and Petros Maragos. Sparsity in max-plus algebra and applications
in multivariate convex regression. In Proc. 46th IEEE Intâ€™l Conf. Acoustics, Speech and Signal
Processing (ICASSP-2021), Toronto, June 2021."
REFERENCES,0.4771084337349398,"Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks. In
International Conference on Machine Learning, pp. 5824â€“5832. PMLR, 2018."
REFERENCES,0.4795180722891566,"GÃ¼nter M Ziegler. Lectures on polytopes, volume 152. Springer Science & Business Media, 2012."
REFERENCES,0.4819277108433735,Published as a conference paper at ICLR 2022
REFERENCES,0.4843373493975904,"A
PROOFS FOR THE SECTION ""BACKGROUND ON TROPICAL GEOMETRY"""
REFERENCES,0.4867469879518072,"A.1
PROOF OF PROPOSITION 3"
REFERENCES,0.4891566265060241,"Proof. The ï¬rst argument follows from the fact that both pj, qj are linear combinations of tropical
polynomials consisting of two terms. Indeed, we compute"
REFERENCES,0.491566265060241,"pj(x) =
X"
REFERENCES,0.4939759036144578,"cji>0
cji max(aT
i x + bi, 0) =
X"
REFERENCES,0.4963855421686747,"cji>0
max(cjiaT
i x + cjibi, 0)
Prop. 1
====â‡’"
REFERENCES,0.4987951807228916,"Pj =
M"
REFERENCES,0.5012048192771085,"cji>0
ENewt
 
max(cjiaT
i x + cjibi, 0)
"
REFERENCES,0.5036144578313253,"Each ENewt
 
max(cjiaT
i x + cjibi, 0)

is a line segment with endpoints 0 and
 
cjiaT
i , cjibi

=
cji
 
aT
i , bi

. Therefore Pj is written as the Minkowski sum of line segments, which is a zonotope by
deï¬nition. Similarly Qj is a zonotope."
REFERENCES,0.5060240963855421,"Furthermore, from the deï¬nition of the Minkowski sum, each point v âˆˆPj may be written as
P"
REFERENCES,0.5084337349397591,"cji>0 vi, where each vi is a point in the segment ENewt
 
max(cjiaT
i x + cjibi, 0)

. A vertex of
Pj can only occur if vi is an extreme point of ENewt
 
max(cjiaT
i x + cjibi, 0)

for every i which is
equivalent to either vi = 0 or vi = cji
 
aT
i , bi

. This means that every vertex of Pj corresponds to a
subset I+ âŠ†[n] of indices i with cji > 0, for which we choose vi = cji
 
aT
i , bi

and for the rest it
holds vi = 0. Thus,
v =
X"
REFERENCES,0.5108433734939759,"iâˆˆI+
cji
 
aT
i , bi
"
REFERENCES,0.5132530120481927,In the same way we derive the analogous result for the negative zonotope Qj.
REFERENCES,0.5156626506024097,"Corollary 2. The geometric result concerning the structure of zonotopes can be extended to max-
pooling layers. For instance, a max-pooling layer of size 2 Ã— 2 corresponds to a polytope that is
constructed as the Minkowski sum of pyramids which could stand as generalized case of zonotope."
REFERENCES,0.5180722891566265,"B
PROOFS FOR THE SECTION ""APPROXIMATION OF TROPICAL
POLYNOMIALS"""
REFERENCES,0.5204819277108433,"B.1
PROOF OF PROPOSITION 4"
REFERENCES,0.5228915662650603,"Proof. Consider a point x âˆˆB and assume that p(x) = aT x + b, Ëœp(x) = cT x + d. Then,"
REFERENCES,0.5253012048192771,"p(x) âˆ’Ëœp(x) = p(x) âˆ’
max
(ËœaT ,Ëœb)âˆˆVËœ
p
{ËœaT x + Ëœb} â‰¤aT x + b âˆ’
 
uT , v
 
x
1 "
REFERENCES,0.5277108433734939,"where
 
uT , v

may be any vertex of ËœP. Similarly, we derive
 
rT , s
 
x
1"
REFERENCES,0.5301204819277109,"
âˆ’
 
cT x + d

â‰¤p(x) âˆ’Ëœp(x)"
REFERENCES,0.5325301204819277,"for any vertex
 
rT , s

of P. Therefore, we may select the vertices
 
uT , v

âˆˆËœP,
 
rT , s

âˆˆP so that
their respective distances from
 
aT , b

and
 
cT , d

, respectively, are minimized. Choosing them in
such a way gives"
REFERENCES,0.5349397590361445,"p(x) âˆ’Ëœp(x) â‰¤aT x + b âˆ’
 
uT , v
 
x
1"
REFERENCES,0.5373493975903615,"
=
  
aT , b

âˆ’
 
uT , v
 
x
1 
â‰¤"
REFERENCES,0.5397590361445783,"â‰¤
 
aT , b

âˆ’
 
uT , v
 
x
1"
REFERENCES,0.5421686746987951," â‰¤d
 
aT , b

, ËœP
 p"
REFERENCES,0.5445783132530121,"r2 + 1
(5)"
REFERENCES,0.5469879518072289,In similar manner we deduce
REFERENCES,0.5493975903614458,"p(x) âˆ’Ëœp(x) â‰¥
 
rT , s
 
x
1"
REFERENCES,0.5518072289156627,"
âˆ’cT x + d =
  
rT , s

âˆ’
 
cT , d
 
x
1 
â‰¥"
REFERENCES,0.5542168674698795,"â‰¥âˆ’
 
rT , s

âˆ’
 
cT , d
 
x
1"
REFERENCES,0.5566265060240964," â‰¥âˆ’d
 
P,
 
cT , d
 p"
REFERENCES,0.5590361445783133,"r2 + 1
(6)"
REFERENCES,0.5614457831325301,Published as a conference paper at ICLR 2022
REFERENCES,0.563855421686747,"Notice, that for the relations (5) and (6) we used Cauchy-Schwartz inequality"
REFERENCES,0.5662650602409639,"|âŸ¨x, yâŸ©| â‰¤âˆ¥xâˆ¥âˆ¥yâˆ¥â‡”âˆ’âˆ¥xâˆ¥âˆ¥yâˆ¥â‰¤âŸ¨x, yâŸ©â‰¤âˆ¥xâˆ¥âˆ¥yâˆ¥"
REFERENCES,0.5686746987951807,"Inequality (5) holds at any point x âˆˆB for some vertex
 
aT , b

âˆˆP, therefore"
REFERENCES,0.5710843373493976,"p(x) âˆ’Ëœp(x) â‰¤Ï Â·
max
(aT ,b)âˆˆVP
d
  
aT , b

, V Ëœ
P

(7)"
REFERENCES,0.5734939759036145,"for all x âˆˆB. Similarly, we derive"
REFERENCES,0.5759036144578313,"p(x) âˆ’Ëœp(x) â‰¥
min
(c,d)âˆˆV Ëœ
P
âˆ’Ï Â· d
 
VP ,
 
cT , d

= âˆ’
max
(cT ,d)âˆˆV Ëœ
P
Ï Â· d
 
VP ,
 
cT , d

(8)"
REFERENCES,0.5783132530120482,Combining (7) and (8) gives
REFERENCES,0.5807228915662651,"âˆ’
max
(cT ,d)âˆˆV Ëœ
P
Ï Â· d
 
VP ,
 
cT , d

â‰¤p(x) âˆ’Ëœp(x) â‰¤
max
(aT ,b)âˆˆVP
Ï Â· d
  
aT , b

, V Ëœ
P

â‡”"
REFERENCES,0.5831325301204819,"|p(x) âˆ’Ëœp(x)| â‰¤Ï Â· max

max
(aT ,b)âˆˆVP
Ï Â· d
  
aT , b

, V Ëœ
P

,
max
(cT ,d)âˆˆV Ëœ
P
Ï Â· d
 
VP ,
 
cT , d
"
REFERENCES,0.5855421686746988,"Hence, from the deï¬nition of the Hausdorff distance of two polytopes we derive the desired upper
bound"
REFERENCES,0.5879518072289157,"|p(x) âˆ’Ëœp(x)| â‰¤Ï Â· H

P, ËœP

, âˆ€x âˆˆB â‡’"
REFERENCES,0.5903614457831325,"max
xâˆˆB |p(x) âˆ’Ëœp(x)| â‰¤Ï Â· H

P, ËœP
"
REFERENCES,0.5927710843373494,"Remark 2. Note that with similar proof one may replace the Hausdorff distance of the two polytopes
by the Hausdorff distance of their upper envelopes. This makes our theorem an exact generalization
of Proposition 2. However, this format is difï¬cult to use in practice, because it is computationally
harder to determine the vertices of the upper envelope."
REFERENCES,0.5951807228915663,"B.2
PROOF OF THEOREM 1"
REFERENCES,0.5975903614457831,Proof. Notice that we may write
REFERENCES,0.6,"âˆ¥v(x) âˆ’Ëœv(x)âˆ¥1 = m
X"
REFERENCES,0.6024096385542169,"j=1
|vj(x) âˆ’Ëœvj(x)| = m
X"
REFERENCES,0.6048192771084338,"j=1
|(pj(x) âˆ’qj(x)) âˆ’(Ëœpj(x) âˆ’Ëœqj(x))| = m
X"
REFERENCES,0.6072289156626506,"j=1
|(pj(x) âˆ’Ëœpj(x)) âˆ’(qj(x) âˆ’Ëœqj(x))| â‰¤ m
X"
REFERENCES,0.6096385542168675,"j=1
|pj(x) âˆ’Ëœpj(x)| + |qj(x) âˆ’Ëœqj(x)|"
REFERENCES,0.6120481927710844,Thus from from Proposition 4 we derive
REFERENCES,0.6144578313253012,"max
xâˆˆB âˆ¥v(x) âˆ’Ëœv(x)âˆ¥1 â‰¤Ï Â· ï£« ï£­
m
X"
REFERENCES,0.6168674698795181,"j=1
H

Pj, ËœPj

+ H

Qj, ËœQj

ï£¶ ï£¸"
REFERENCES,0.619277108433735,"C
PROOFS AND FIGURES FOR THE SECTION ""NEURAL NETWORK
COMPRESSION ALGORITHMS"""
REFERENCES,0.6216867469879518,"C.1
ILLUSTRATION OF ZONOTOPE K-MEANS"
REFERENCES,0.6240963855421687,Below we present a larger version of Fig. 5 demonstrating the execution of Zonotope K-means .
REFERENCES,0.6265060240963856,Published as a conference paper at ICLR 2022 x1 x2 f3 f4 f5 f6 f7 f2 f1 v c1 c2 c3 c4 c5 c6 c7 b1 b2 b3 b4 b5 b6 b7
REFERENCES,0.6289156626506024,"{aji}iâˆˆ(1,2), jâˆˆ(1,...,7)"
REFERENCES,0.6313253012048192,(a) Original network. x1 x2 Ëœf1 Ëœf2 Ëœf3 Ëœf4 v + + âˆ’ âˆ’
REFERENCES,0.6337349397590362,Ëœc1Ëœb1
REFERENCES,0.636144578313253,Ëœc2Ëœb2
REFERENCES,0.6385542168674698,Ëœc3Ëœb3
REFERENCES,0.6409638554216868,"Ëœc4Ëœb4
{ËœcjËœaji}iâˆˆ(1,2), jâˆˆ(1,...,4)"
REFERENCES,0.6433734939759036,(b) Minimized network.
REFERENCES,0.6457831325301204,"c1(aT
1 , b1)"
REFERENCES,0.6481927710843374,"c2(aT
2 , b2)"
REFERENCES,0.6506024096385542,"c3(aT
3 , b3)"
REFERENCES,0.653012048192771,"c4(aT
4 , b4)"
REFERENCES,0.655421686746988,"c5(aT
5 , b5)"
REFERENCES,0.6578313253012048,"c6(aT
6 , b6)"
REFERENCES,0.6602409638554216,"c7(aT
7 , b7)"
REFERENCES,0.6626506024096386,(c) Original zonotopes
REFERENCES,0.6650602409638554,"Ëœc1(ËœaT
1 , Ëœb1)"
REFERENCES,0.6674698795180722,"Ëœc2(ËœaT
2 , Ëœb2)"
REFERENCES,0.6698795180722892,"Ëœc4(ËœaT
4 , Ëœb4) â‰¡c7(aT
7 , b7)"
REFERENCES,0.672289156626506,"Ëœc3(ËœaT
3 , Ëœb3)"
REFERENCES,0.6746987951807228,(d) Approximating zonotopes.
REFERENCES,0.6771084337349398,"Figure 5: Illustration of Zonotope K-means execution. The original zonotope P is generated by
ci
 
aT
i , bi

for i = 1, ..., 4 and the negative zonotope Q generated by the remaining ones i = 5, 6, 7."
REFERENCES,0.6795180722891566,"The approximation ËœP of P is colored in purple and generated by Ëœci

ËœaT
i , Ëœbi

, i = 1, 2 where the
ï¬rst generator is the K-means center representing the generators 1, 2 of P and the second is the
representative center of 3, 4. Similarly, the approximation ËœQ of Q is colored in green and deï¬ned by
the generators Ëœci

ËœaT
i , Ëœbi

, i = 3, 4 that stand as representative centers for {5, 6} and 7 respectively."
REFERENCES,0.6819277108433734,"C.2
PROOF OF PROPOSITION 5"
REFERENCES,0.6843373493975904,Proof. We remind that for the output functions it holds
REFERENCES,0.6867469879518072,"v(x) = p(x) âˆ’q(x) , Ëœv(x) = Ëœp(x) âˆ’Ëœq(x)"
REFERENCES,0.689156626506024,From triangle inequality we deduce
REFERENCES,0.691566265060241,|v(x) âˆ’Ëœv(x)| = |p(x) âˆ’q(x) âˆ’(Ëœp(x) âˆ’Ëœq(x))| < |p(x) âˆ’Ëœp(x)| + |q(x) âˆ’Ëœq(x)|
REFERENCES,0.6939759036144578,"Prop 4 bounds |p(x) âˆ’Ëœp(x)| and |q(x) âˆ’Ëœq(x)| are bounded by H

P, ËœP

and H

Q, ËœQ

respec-
tively. Therefore, it sufï¬ces to get an upper bound for these Hausdorff distances. Let us consider any"
REFERENCES,0.6963855421686747,Published as a conference paper at ICLR 2022
REFERENCES,0.6987951807228916,vertex u = P
REFERENCES,0.7012048192771084,"iâˆˆI+ ci
 
aT
i , bi

of P. For the vertex u âˆˆP we need to choose vertex v âˆˆËœP as close"
REFERENCES,0.7036144578313253,"to u as possible, in order to provide an upper bound for dist

u, ËœP

. Vertex v is selected as follows."
REFERENCES,0.7060240963855422,"For each i âˆˆI+ we select k such that Ëœck
 
ËœaT
k
Ëœbk

is the center of the cluster where ci
 
aT
i , bi
"
REFERENCES,0.708433734939759,"belongs to. We denote the set of such clusters by C+, where each cluster center k appears only once.
Then, vertex v is constructed as v = P"
REFERENCES,0.7108433734939759,"kâˆˆC+ Ëœck
 
ËœaT
k
Ëœbk

âˆˆËœP. We have that:"
REFERENCES,0.7132530120481928,"dist

u, ËœP

â‰¤  X"
REFERENCES,0.7156626506024096,"iâˆˆI+
ci
 
aT
i , bi

âˆ’
X"
REFERENCES,0.7180722891566265,"kâˆˆC+
Ëœck

ËœaT
k , Ëœbk
 â‰¤
X kâˆˆC+  X"
REFERENCES,0.7204819277108434,"iâˆˆIk+
ci
 
aT
i , bi

âˆ’Ëœck

ËœaT
k , Ëœbk
 â‰¤
X kâˆˆC+ X iâˆˆIk+"
REFERENCES,0.7228915662650602,"ci
 
aT
i , bi

âˆ’
Ëœck

ËœaT
k , Ëœbk
 |Ik+|  =
X kâˆˆC+ X iâˆˆIk+"
REFERENCES,0.7253012048192771,"ci
 
aT
i , bi

âˆ’ci
 
aT
i , bi

+ Îµi
|Ik+|  â‰¤
X kâˆˆC+ X iâˆˆIk+"
REFERENCES,0.727710843373494,"
1 âˆ’
1
|Ik+|"
REFERENCES,0.7301204819277108,"
|ci|
 
aT
i , bi
 + âˆ¥Îµiâˆ¥ |Ik+| "
REFERENCES,0.7325301204819277,"â‰¤|C+| Â· Î´max +

1 âˆ’
1
Nmax  X"
REFERENCES,0.7349397590361446,"iâˆˆI+
|ci|
 
aT
i , bi
"
REFERENCES,0.7373493975903614,"where we denote by Ik+ the set of indices i âˆˆI+ that belong to the center k âˆˆC+ and Îµi =
Ëœck

ËœaT
k , Ëœbk

âˆ’ci
 
aT
i , bi

is the vector of the difference of the iâˆ’th generator, with its corresponding
K-means cluster center."
REFERENCES,0.7397590361445783,"The maximum value of the upper bound occurs when I+ contains all indices that correspond
to ci > 0.
This value gives us an upper bound for maxuâˆˆP d(u, ËœP).
To compute an up-
per bound for maxvâˆˆV Ëœ
P d(P, v) we assume v = P"
REFERENCES,0.7421686746987952,"kâˆˆC+ Ëœck
 
ËœaT
k
Ëœbk

and consider the vertex
P"
REFERENCES,0.744578313253012,"iâˆˆI+ ci
 
aT
i , bi

âˆˆP where I+ is the set of indices of positive generators corresponding to the
union of all clusters corresponding to the centers of C+. Note that the occurring distance X"
REFERENCES,0.7469879518072289,"iâˆˆI+
ci
 
aT
i , bi

âˆ’
X"
REFERENCES,0.7493975903614458,"kâˆˆC+
Ëœck

ËœaT
i , Ëœbi
"
REFERENCES,0.7518072289156627,"was taken into account when computing the upper bound for maxuâˆˆVP d(u, ËœP), and thus both values
obtain the same upper bound. Therefore,"
REFERENCES,0.7542168674698795,"H

P, ËœP

â‰¤K+ Â· Î´max +

1 âˆ’
1
Nmax  X"
REFERENCES,0.7566265060240964,"iâˆˆI+
|ci|
 
aT
i , bi
"
REFERENCES,0.7590361445783133,"where K+ is the number of cluster centers corresponding to ËœP and I+ the indices corresponding to
all positive generators of P. Similarly,"
REFERENCES,0.7614457831325301,"H

Q, ËœQ

â‰¤Kâˆ’Â· Î´max +

1 âˆ’
1
Nmax  X"
REFERENCES,0.763855421686747,"iâˆˆIâˆ’
|ci|
 
aT
i , bi
"
REFERENCES,0.7662650602409639,"where Kâˆ’, Iâˆ’are deï¬ned in analogous way for the negative zonotope. Combining the relations gives
the desired bound."
REFERENCES,0.7686746987951807,"1
Ï Â· |v(x) âˆ’Ëœv(x)| â‰¤H

P, ËœP

+ H

Q, ËœQ

â‰¤K Â· Î´max +

1 âˆ’
1
Nmax 
n
X"
REFERENCES,0.7710843373493976,"i=1
|ci|
 
aT
i , bi
"
REFERENCES,0.7734939759036145,Published as a conference paper at ICLR 2022
REFERENCES,0.7759036144578313,"C.3
ILLUSTRATION FOR NEURAL PATH K-MEANS ALGORITHM"
REFERENCES,0.7783132530120482,"Below we illustrate the vectors on which K-means is applied for the multi-output case. The vectors
that are compressed are consist of all the edges associated to a hidden layer neuron. The corresponding
edges contain all the possible neural paths that begin from some node of the input, end in some node
of the output and pass through this hidden node. x1 x2 ... xk ... xd f1 f2 ... fi ... fn v1 v2 ... vj ... vm ai1 ai2 aik aid c1i c2i cji cmi b1 b2 bi bn"
REFERENCES,0.7807228915662651,"Figure 6: Neural Path K-means for multi-output neural network compression. In green color we
highlight the weights corresponding to the iâˆ’th vector used by Neural Path K-means."
REFERENCES,0.7831325301204819,"In the main text we deï¬ned the null generators of zonotopes that occur by the execution of Neural
Path K-means. Below we provide an illustration for them."
REFERENCES,0.7855421686746988,Null Generators
REFERENCES,0.7879518072289157,"Figure 7: Visualization of K-means in Rd+1+n, where d is the input dimension and n the hidden
layer size. We color points according to the jâˆ’th output component of the network. Black and white
points correspond to generators of Pj and Qj respectively. White vertices in positive (brown) clusters
and black vertices in negative (blue) clusters are null generators regarding jâˆ’th output."
REFERENCES,0.7903614457831325,"C.4
PROOF OF PROPOSITION 6"
REFERENCES,0.7927710843373494,"Proof. Let us ï¬rst focus on a single output, say jâˆ’th output. As in the proof for Zonotope K-means,
we will bound H

Pj, ËœPj

, H

Qj, ËœQj

for all j âˆˆ[m]. From triangle inequality we get"
REFERENCES,0.7951807228915663,|vj(x) âˆ’Ëœvj(x)| â‰¤|pj(x) âˆ’Ëœpj(x)| + |qj(x) âˆ’Ëœqj(x)|
REFERENCES,0.7975903614457831,Any vertex of u âˆˆPj can be written as u = P
REFERENCES,0.8,"iâˆˆIj+ cji
 
aT
i , bi

where the set of indices Ij+ satisfy"
REFERENCES,0.8024096385542169,"cji > 0, âˆ€i âˆˆIj+ and thus correspond to positive generators. To choose a nearby vertex from ËœPj
we perform the following. For each i âˆˆIj+ we select the center
 
ËœaT
k
Ëœbk
ËœC(k)T 
of the cluster to
where
 
aT
i
bi
C(i)T 
belongs, only if Ëœcjk > 0. Such a center only exists if cji
 
aT
i , bi

is not a"
REFERENCES,0.8048192771084337,Published as a conference paper at ICLR 2022
REFERENCES,0.8072289156626506,"null generator. Else, we choose as representation the vector 0. Each cluster center, or 0, is taken into
account once and the vertex P
kâˆˆCj+ Ëœcjk

Ëœak, Ëœbk

âˆˆËœPj is formed. We derive:"
REFERENCES,0.8096385542168675,"max
uâˆˆVPj
dist

u, ËœPj

â‰¤  X"
REFERENCES,0.8120481927710843,"iâˆˆIj+
cji
 
aT
i , bi

âˆ’
X"
REFERENCES,0.8144578313253013,"kâˆˆCj+
Ëœcjk

ËœaT
k , Ëœbk
 â‰¤
X kâˆˆCj+  X"
REFERENCES,0.8168674698795181,"iâˆˆIjk+
cji
 
aT
i , bi

âˆ’Ëœcjk

ËœaT
k , Ëœbk


+
X"
REFERENCES,0.8192771084337349,"iâˆˆNj+
|cji|
 
aT
i , bi
 â‰¤
X kâˆˆCj+ X"
REFERENCES,0.8216867469879519,iâˆˆIjk+
REFERENCES,0.8240963855421687,"cji
 
aT
i , bi

âˆ’
Ëœcjk

ËœaT
k , Ëœbk
"
REFERENCES,0.8265060240963855,"|Ijk+| +
X"
REFERENCES,0.8289156626506025,"iâˆˆNj+
|cji|
 
aT
i , bi
 â‰¤
X kâˆˆC X"
REFERENCES,0.8313253012048193,iâˆˆIjk+
REFERENCES,0.8337349397590361,"cji
 
aT
i , bi

âˆ’(cji + Îµji)
 
aT
i , bi

+ Î»i
"
REFERENCES,0.8361445783132531,"|Ijk+| +
X"
REFERENCES,0.8385542168674699,"iâˆˆNj+
|cji|
 
aT
i , bi
 â‰¤
X kâˆˆCj+ X"
REFERENCES,0.8409638554216867,iâˆˆIjk+
REFERENCES,0.8433734939759037,|Îµji|âˆ¥Î»iâˆ¥
REFERENCES,0.8457831325301205,"|Ijk+|
+

1 âˆ’
1
|Ijk+|"
REFERENCES,0.8481927710843373,"
|cji|
 
aT
i , bi


+ +
X kâˆˆCj+ X"
REFERENCES,0.8506024096385543,iâˆˆIjk+
REFERENCES,0.8530120481927711,"""
|Îµji|
 
aT
i , bi
 + |cji| âˆ¥Î»iâˆ¥
|Ijk+| # +
X"
REFERENCES,0.8554216867469879,"iâˆˆNj+
|cji|
 
aT
i , bi
"
REFERENCES,0.8578313253012049,where for all i âˆˆIjk+ the iâˆ’th vector of K-means is represented by the kâˆ’th center k âˆˆCj+. We
REFERENCES,0.8602409638554217,"also assumed ËœC(i) = C:,i + Îµ(i) â‡’Ëœcji = cji + Îµji and

ËœaT
i , Ëœbi

=
 
aT
i , bi

+ Î»i."
REFERENCES,0.8626506024096385,"The maximum value of the upper bound occurs when Ij+ contains all indices that correspond
to cji > 0. To compute an upper bound for maxvâˆˆV Ëœ
Pj dist (Pj, v) we write the vertex v as v = P"
REFERENCES,0.8650602409638555,"kâˆˆCj+ Ëœcjk
 
ËœaT
k
Ëœbk

âˆˆËœPj and choose the vertex u = P"
REFERENCES,0.8674698795180723,"iâˆˆIj+ cji
 
aT
i , bi

of Pj where
Ij+ is the set of all indices corresponding to generators that belong to these clusters. As in the
proof of Proposition 5, their distance was taken into account when computing the upper bound for
maxuâˆˆVPj dist

u, ËœPj

. Hence, both obtain the same upper bound which leads to"
REFERENCES,0.8698795180722891,"H

Pj, ËœPj

â‰¤
X kâˆˆCj+ X"
REFERENCES,0.8722891566265061,iâˆˆIjk+
REFERENCES,0.8746987951807229,|Îµji|âˆ¥Î»iâˆ¥
REFERENCES,0.8771084337349397,"|Ijk+|
+

1 âˆ’
1
|Ijk+|"
REFERENCES,0.8795180722891566,"
|cji|
 
aT
i , bi


+ +
X kâˆˆCj+ X"
REFERENCES,0.8819277108433735,iâˆˆIjk+
REFERENCES,0.8843373493975903,"""
|Îµji|
 
aT
i , bi
 + |cji| âˆ¥Î»iâˆ¥
|Ijk+| # +
X"
REFERENCES,0.8867469879518072,"iâˆˆNj+
|cji|
 
aT
i , bi
"
REFERENCES,0.8891566265060241,"where Ij+ contains all indices corresponding to positive cji. Similarly, we deduce"
REFERENCES,0.891566265060241,"H

Qj, ËœQj

â‰¤
X kâˆˆCjâˆ’ X"
REFERENCES,0.8939759036144578,iâˆˆIjkâˆ’
REFERENCES,0.8963855421686747,|Îµji|âˆ¥Î»iâˆ¥
REFERENCES,0.8987951807228916,"|Ijkâˆ’|
+

1 âˆ’
1
|Ijkâˆ’|"
REFERENCES,0.9012048192771084,"
|cji|
 
aT
i , bi


+ +
X kâˆˆCjâˆ’ X"
REFERENCES,0.9036144578313253,iâˆˆIjkâˆ’
REFERENCES,0.9060240963855422,"""
|Îµji|
 
aT
i , bi
 |cji| âˆ¥Î»iâˆ¥
|Ijkâˆ’| # +
X"
REFERENCES,0.908433734939759,"iâˆˆNjâˆ’
|cji|
 
aT
i , bi
"
REFERENCES,0.9108433734939759,where Ijâˆ’contains all i such that cji < 0. In total these bounds together with Proposition 4 give
REFERENCES,0.9132530120481928,Published as a conference paper at ICLR 2022
REFERENCES,0.9156626506024096,"1
Ï Â· max
xâˆˆB |vj(x) âˆ’Ëœvj(x)| â‰¤
X kâˆˆCj X iâˆˆIjk"
REFERENCES,0.9180722891566265,|Îµji|âˆ¥Î»iâˆ¥
REFERENCES,0.9204819277108434,"|Ijk|
+

1 âˆ’
1
|Ijk|"
REFERENCES,0.9228915662650602,"
|cji|
 
aT
i , bi


+ +
X kâˆˆCj X iâˆˆIjk"
REFERENCES,0.9253012048192771,"""
|Îµji|
 
aT
i , bi
 + |cji| âˆ¥Î»iâˆ¥
|Ijk| # +
X"
REFERENCES,0.927710843373494,"iâˆˆNj
|cji|
 
aT
i , bi
"
REFERENCES,0.9301204819277108,"Here we used the notation Cj = Cj+ âˆªCjâˆ’= {1, 2, ..., K} and Ijk is either equal to Ijk+ or Ijkâˆ’
depending on k âˆˆCj. Note that {i|i âˆˆIjk, k âˆˆCj} = {1, 2, ..., n} \ Nj âŠ†{1, 2, ..., n}, since
every generator that is not null corresponds to some cluster center with the same sign. Also, using
Nmax â‰¥|Ijk| â‰¥Nmin, it follows that"
REFERENCES,0.9325301204819277,"1
Ï Â· max
xâˆˆB |vj(x) âˆ’Ëœvj(x)| â‰¤ n
X i=1"
REFERENCES,0.9349397590361446,|Îµji|âˆ¥Î»iâˆ¥
REFERENCES,0.9373493975903614,"Nmin
+

1 âˆ’
1
Nmax"
REFERENCES,0.9397590361445783,"
|cji|
 
aT
i , bi


+ + n
X i=1"
REFERENCES,0.9421686746987952,"""
|Îµji|
 
aT
i , bi
 + |cji| âˆ¥Î»iâˆ¥
Nmin # +
X"
REFERENCES,0.944578313253012,"iâˆˆNj
|cji|
 
aT
i , bi
"
REFERENCES,0.946987951807229,"We will compute the total cost that combines all outputs by applying the inequality
ï£« ï£­
m
X"
REFERENCES,0.9493975903614458,"j=1
|uj| ï£¶ ï£¸ 2 â‰¤m ï£« ï£­
m
X"
REFERENCES,0.9518072289156626,"j=1
|uj|2 ï£¶ ï£¸â‡” m
X"
REFERENCES,0.9542168674698795,"j=1
|uj| â‰¤âˆšm âˆ¥(u1, ..., um)âˆ¥"
REFERENCES,0.9566265060240964,"which is a direct application of Cauchy-Schwartz inequality.
Together with the relations
âˆ¥Îµ(i)âˆ¥< Î´max,
âˆ¥Î»iâˆ¥< Î´max, we get 1
Ï Â· m
X"
REFERENCES,0.9590361445783132,"j=1
max
xâˆˆB |vj(x) âˆ’Ëœvj(x)| â‰¤ m
X j=1 n
X i=1"
REFERENCES,0.9614457831325302,|Îµji|âˆ¥Î»iâˆ¥
REFERENCES,0.963855421686747,"Nmin
+

1 âˆ’
1
Nmax"
REFERENCES,0.9662650602409638,"
|cji|
 
aT
i , bi


+ + m
X j=1 n
X i=1"
REFERENCES,0.9686746987951808,"""
|Îµji|
 
aT
i , bi
 + |cji| âˆ¥Î»iâˆ¥
Nmin # + m
X j=1 X"
REFERENCES,0.9710843373493976,"iâˆˆNj
|cji|
 
aT
i , bi
 â‰¤ n
X i=1"
REFERENCES,0.9734939759036144,"""âˆšm
Îµ(i) âˆ¥Î»iâˆ¥"
REFERENCES,0.9759036144578314,"Nmin
+

1 âˆ’
1
Nmax"
REFERENCES,0.9783132530120482," âˆšm âˆ¥C:,iâˆ¥
 
aT
i , bi

# + + n
X i=1"
REFERENCES,0.980722891566265,"""âˆšm
Îµ(i)  
aT
i , bi
 + âˆšm âˆ¥C:,iâˆ¥âˆ¥Î»iâˆ¥
Nmin # + m
X j=1 X"
REFERENCES,0.983132530120482,"iâˆˆNj
|cji|
 
aT
i , bi
"
REFERENCES,0.9855421686746988,"â‰¤âˆšmKÎ´2
max + âˆšm

1 âˆ’
1
Nmax 
n
X"
REFERENCES,0.9879518072289156,"i=1
âˆ¥C:,iâˆ¥
 
aT
i , bi
 +"
REFERENCES,0.9903614457831326,"âˆšmÎ´max Nmin n
X i=1"
REFERENCES,0.9927710843373494,"  
aT
i , bi
 + âˆ¥C:,iâˆ¥

+ m
X j=1 X"
REFERENCES,0.9951807228915662,"iâˆˆNj
|cji|
 
aT
i , bi
"
REFERENCES,0.9975903614457832,as desired.
