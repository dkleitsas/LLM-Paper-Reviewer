Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0019083969465648854,"Dictionary learning consists of ﬁnding a sparse representation from noisy data and
is a common way to encode data-driven prior knowledge on signals. Alternating
minimization (AM) is standard for the underlying optimization, where gradient
descent steps alternate with sparse coding procedures. The major drawback of
this method is its prohibitive computational cost, making it unpractical on large
real-world data sets. This work studies an approximate formulation of dictionary
learning based on unrolling and compares it to alternating minimization to ﬁnd
the best trade-off between speed and precision. We analyze the asymptotic behav-
ior and convergence rate of gradients estimates in both methods. We show that
unrolling performs better on the support of the inner problem solution and during
the ﬁrst iterations. Finally, we apply unrolling on pattern learning in magnetoen-
cephalography (MEG) with the help of a stochastic algorithm and compare the
performance to a state-of-the-art method."
INTRODUCTION,0.003816793893129771,"1
INTRODUCTION"
INTRODUCTION,0.0057251908396946565,"Pattern learning provides insightful information on the data in various biomedical applications. Typ-
ical examples include the study of magnetoencephalography (MEG) recordings, where one aims to
analyze the electrical activity in the brain from measurements of the magnetic ﬁeld around the scalp
of the patient (Dupr´e la Tour et al., 2018). One may also mention neural oscillations study in the
local ﬁeld potential (Cole & Voytek, 2017) or QRS complex detection in electrocardiograms (Xiang
et al., 2018) among others."
INTRODUCTION,0.007633587786259542,"Dictionary learning (Olshausen & Field, 1997; Aharon et al., 2006; Mairal et al., 2009) is particu-
larly efﬁcient on pattern learning tasks, such as blood cells detection (Yellin et al., 2017) and MEG
signals analysis (Dupr´e la Tour et al., 2018). This framework assumes that the signal can be de-
composed into a sparse representation in a redundant basis of patterns – also called atoms. In other
words, the goal is to recover a sparse code Z ∈Rn×T and a dictionary D ∈Rm×n from noisy mea-
surements Y ∈Rm×T which are obtained as the linear transformation DZ, corrupted with noise
B ∈Rm×T : Y = DZ + B. Theoretical elements on identiﬁability and local convergence have
been proven in several studies (Gribonval et al., 2015; Haeffele & Vidal, 2015; Agarwal et al., 2016;
Sun et al., 2016). Sparsity-based optimization problems related to dictionary learning generally rely
on the usage of the ℓ0 or ℓ1 regularizations. In this paper, we study Lasso-based (Tibshirani, 1996)
dictionary learning where the dictionary D is learned in a set of constraints C by solving"
INTRODUCTION,0.009541984732824428,"min
Z∈Rn×T ,D∈C F(Z, D) ≜1"
INTRODUCTION,0.011450381679389313,"2 ∥DZ −Y ∥2
2 + λ ∥Z∥1 .
(1)"
INTRODUCTION,0.013358778625954198,Published as a conference paper at ICLR 2022
INTRODUCTION,0.015267175572519083,"Dictionary learning can be written as a bi-level optimization problem to minimize the cost function
with respect to the dictionary only, as mentioned in Mairal et al. (2009),"
INTRODUCTION,0.01717557251908397,"min
D∈C G(D) ≜F(Z∗(D), D)
with
Z∗(D) = arg min
Z∈Rn×T F(Z, D) .
(2)"
INTRODUCTION,0.019083969465648856,"Computing the data representation Z∗(D) is often referred to as the inner problem, while the global
minimization is the outer problem. Classical constraint sets include the unit norm, where each
atom is normalized to avoid scale-invariant issues, and normalized convolutional kernels to perform
Convolutional Dictionary Learning (Grosse et al., 2007)."
INTRODUCTION,0.02099236641221374,"Classical dictionary learning methods solve this bi-convex optimization problem through Alternat-
ing Minimization (AM) (Mairal et al., 2009). It consists in minimizing the cost function F over Z
with a ﬁxed dictionary D and then performing projected gradient descent to optimize the dictionary
with a ﬁxed Z. While AM provides a simple strategy to perform dictionary learning, it can be inefﬁ-
cient on large-scale data sets due to the need to resolve the inner problems precisely for all samples.
In recent years, many studies have focused on algorithm unrolling (Tolooshams et al., 2020; Scetbon
et al., 2021) to overcome this issue. The core idea consists of unrolling the algorithm, which solves
the inner problem, and then computing the gradient with respect to the dictionary with the help of
back-propagation through the iterates of this algorithm. Gregor & LeCun (2010) popularized this
method and ﬁrst proposed to unroll ISTA (Daubechies et al., 2004) – a proximal gradient descent
algorithm designed for the Lasso – to speed up the computation of Z∗(D). The N + 1-th layer
of this network – called LISTA – is obtained as ZN+1 = ST λ"
INTRODUCTION,0.022900763358778626,"L (W 1Y + W 2ZN), with ST be-
ing the soft-thresholding operator. This work has led to many contributions aiming at improving this
method and providing theoretical justiﬁcations in a supervised (Chen et al., 2018; Liu & Chen, 2019)
or unsupervised (Moreau & Bruna, 2017; Ablin et al., 2019) setting. For such unrolled algorithms,
the weights W 1 and W 2 can be re-parameterized as functions of D – as illustrated in Figure A in
appendix – such that the output ZN(D) matches the result of N iterations of ISTA, i.e."
INTRODUCTION,0.02480916030534351,"W 1
D = 1"
INTRODUCTION,0.026717557251908396,"LD⊤
and
W 2
D =

I −1"
INTRODUCTION,0.02862595419847328,"LD⊤D

,
where
L = ∥D∥2 .
(3)"
INTRODUCTION,0.030534351145038167,"Then, the dictionary can be learned by minimizing the loss F(ZN(D), D) over D with back-
propagation. This approach is generally referred to as Deep Dictionary Learning (DDL). DDL and
variants with different kinds of regularization (Tolooshams et al., 2020; Lecouat et al., 2020; Scetbon
et al., 2021), image processing based on metric learning (Tang et al., 2020), and classiﬁcation tasks
with scattering (Zarka et al., 2019) have been proposed in the literature, among others. While these
techniques have achieved good performance levels on several signal processing tasks, the reasons
they speed up the learning process are still unclear."
INTRODUCTION,0.03244274809160305,"In this work, we study unrolling in Lasso-based dictionary learning as an approximate bi-level opti-
mization problem. What makes this work different from Bertrand et al. (2020), Ablin et al. (2020)
and Tolooshams & Ba (2021) is that we study the instability of non-smooth bi-level optimization
and unrolled sparse coding out of the support, which is of major interest in practice with a small
number of layers. In Section 2, we analyze the convergence of the Jacobian computed with auto-
matic differentiation and ﬁnd out that its stability is guaranteed on the support of the sparse codes
only. De facto, numerical instabilities in its estimation make unrolling inefﬁcient after a few dozen
iterations. In Section 3, we empirically show that unrolling leads to better results than AM only with
a small number of iterations of sparse coding, making it possible to learn a good dictionary in this
setting. Then we adapt a stochastic approach to make this method usable on large data sets, and we
apply it to pattern learning in magnetoencephalography (MEG) in Section 4. We do so by adapting
unrolling to rank one convolutional dictionary learning on multivariate time series (Dupr´e la Tour
et al., 2018). We show that there is no need to unroll more than a few dozen iterations to obtain
satisfying results, leading to a signiﬁcant gain of time compared to a state-of-the-art algorithm."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.03435114503816794,"2
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.03625954198473282,"As Z∗(D) does not have a closed-form expression, G cannot be computed directly. A solution is to
replace the inner problem Z∗(D) by an approximation ZN(D) obtained through N iterations of a
numerical optimization algorithm or its unrolled version. This reduces the problem to minimizing
GN(D) ≜F(ZN(D), D). The ﬁrst question is how sub-optimal global solutions of GN are"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.03816793893129771,Published as a conference paper at ICLR 2022
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.04007633587786259,"compared to the ones of G. Proposition 2.1 shows that the global minima of GN converge as fast as
the numerical approximation ZN in function value."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.04198473282442748,"Proposition 2.1 Let D∗= arg minD∈C G(D) and D∗
N = arg minD∈C GN(D), where N is the
number of unrolled iterations. We denote by K(D∗) a constant depending on D∗, and by C(N) the
convergence speed of the algorithm, which approximates the inner problem solution. We have"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.04389312977099236,"GN(D∗
N) −G(D∗) ≤K(D∗)C(N) ."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.04580152671755725,"The proofs of all theoretical results are deferred to Appendix C. Proposition 2.1 implies that when
ZN is computed with FISTA (Beck & Teboulle, 2009), the function value for global minima of
GN converges with speed C(N) =
1
N2 towards the value of the global minima of F. Therefore,
solving the inner problem approximately leads to suitable solutions for equation 2, given that the
optimization procedure is efﬁcient enough to ﬁnd a proper minimum of GN. As the computational
cost of zN increases with N, the choice of N results in a trade-off between the precision of the
solution and the computational efﬁciency, which is critical for processing large data sets."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.04770992366412214,"Moreover, learning the dictionary and computing the sparse codes are two different tasks. The loss
GN takes into account the dictionary and the corresponding approximation ZN(D) to evaluate the
quality of the solution. However, the dictionary evaluation should reﬂect its ability to generate
the same signals as the ground truth data and not consider an approximate sparse code that can be
recomputed afterward. Therefore, we should distinguish the ability of the algorithm to recover a
good dictionary from its ability to learn the dictionary and the sparse codes at the same time. In
this work, we use the metric proposed in Moreau & Gramfort (2020) for convolutions to evaluate
the quality of the dictionary. We compare the atoms using their correlation and denote as C the cost
matrix whose entry i, j compare the atom i of the ﬁrst dictionary and j of the second. We deﬁne a
sign and permutation invariant metric S(C) = maxσ∈Sn
1
n
Pn
i=1 |Cσ(i),i|, where Sn is the group
of permutations of [1, n]. This metric corresponds to the best linear sum assignment on the cost
matrix C, and it can be computed with the Hungarian algorithm. Note that doing so has several
limitations and that evaluating the dictionary is still an open problem. Without loss of generality, let
T = 1 and thus z ∈Rn in the rest of this section."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.04961832061068702,"Gradient estimation in dictionary learning.
Approximate dictionary learning is a non-convex
problem, meaning that good or poor local minima of GN may be reached depending on the initial-
ization, the optimization path, and the structure of the problem. Therefore, a gradient descent on GN
has no guarantee to ﬁnd an adequate minimizer of G. While complete theoretical analysis of these
problems is arduous, we propose to study the correlation between the gradient obtained with GN
and the actual gradient of G, as a way to ensure that the optimization dynamics are similar. Once
z∗(D) is known, Danskin (1967, Thm 1) states that g∗(D) = ∇G(D) is equal to ∇2F(z∗(D), D),
where ∇2 indicates that the gradient is computed relatively to the second variable in F. Even though
the inner problem is non-smooth, this result holds as long as the solution z∗(D) is unique. In the
following, we will assume that D⊤D is invertible on the support of z∗(D), which implies the
uniqueness of z∗(D). This occurs with probability one if D is sampled from a continuous distribu-
tion (Tibshirani, 2013). AM and DDL differ in how they estimate the gradient of G. AM relies on
the analytical formula of g∗and uses an approximation zN of z∗, leading to the approximate gra-
dient g1
N(D) = ∇2F(zN(D), D). We evaluate how well g1
N approximates g∗in Proposition 2.2."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.05152671755725191,"Proposition 2.2 Let D ∈Rm×n. Then, there exists a constant L1 > 0 such that for every number
of iterations N
g1
N −g∗ ≤L1 ∥zN(D) −z∗(D)∥."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.05343511450381679,"Proposition 2.2 shows that g1
N converges as fast as the iterates of ISTA converge. DDL computes
the gradient automatically through zN(D). As opposed to AM, this directly minimizes the loss
GN(D). Automatic differentiation yields a sub-gradient g2
N(D) such that"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.05534351145038168,"g2
N(D) ∈∇2F(zN(D), D) + J+
N

∂1F(zN(D), D)

,
(4)"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.05725190839694656,"where JN : Rm×n →Rn is the weak Jacobian of zN(D) with respect to D and J+
N denotes its
adjoint. The product between J+
N and ∂1F(zN(D), D) is computed via automatic differentiation."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.05916030534351145,Published as a conference paper at ICLR 2022
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.061068702290076333,"Proposition 2.3 Let D ∈Rm×n. Let S∗be the support of z∗(D), SN be the support of zN and
eSN = SN ∪S∗. Let f(z, D) =
1
2 ∥Dz −y∥2
2 be the data-ﬁtting term in F. Let R(J, eS) =
J+ 
∇2
1,1f(z∗, D) ⊙1eS

+ ∇2
2,1f(z∗, D) ⊙1eS. Then there exists a constant L2 > 0 and a sub-
sequence of (F)ISTA iterates zφ(N) such that for all N ∈N:"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.06297709923664122,"∃g2
φ(N) ∈∇2f(zφ(N), D) + J+
φ(N)

∇1f(zφ(N), D) + λ∂∥·∥1(zφ(N))

s.t. :
g2
φ(N) −g∗ ≤
R(Jφ(N), eSφ(N))

zφ(N) −z∗ + L2 2"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.0648854961832061,"zφ(N) −z∗2 ."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.06679389312977099,This sub-sequence zφ(N) corresponds to iterates on the support of z∗.
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.06870229007633588,"Proposition 2.3 shows that g2
N may converge faster than g1
N once the support is reached."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.07061068702290077,"Ablin et al. (2020) and Tolooshams & Ba (2021) have studied the behavior of strongly convex
functions, as it is the case on the support, and found similar results. This allowed Tolooshams &
Ba (2021) to focus on support identiﬁcation and show that automatic differentiation leads to a better
gradient estimation in dictionary learning on the support under minor assumptions."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.07251908396946564,"However, we are also interested in characterizing the behavior outside of the support, where the
gradient estimation is difﬁcult because of the sub-differential. In practice, automatic differenti-
ation uses the sign operator as a sub-gradient of ∥·∥1. The convergence behavior of g2
N is also
driven by R(JN, f
SN) and thus by the weak Jacobian computed via back-propagation. We ﬁrst com-
pute a closed-form expression of the weak Jacobian of z∗(D) and zN(D). We then show that
R(JN, f
SN) ≤L ∥JN −J∗∥and we analyze the convergence of JN towards J∗."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.07442748091603053,"Study of the Jacobian.
The computation of the Jacobian can be done by differentiating through
ISTA. In Theorem 2.4, we show that JN+1 depends on JN and the past iterate zN, and converges
towards a ﬁxed point. This formula can be used to compute the Jacobian during the forward pass,
avoiding the computational cost of back-propagation and saving memory."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.07633587786259542,"Theorem 2.4 At iteration N + 1 of ISTA, the weak Jacobian of zN+1 relatively to Dl, where Dl is
the l-th row of D, is given by induction:
∂(zN+1)"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.07824427480916031,"∂Dl
= 1|zN+1|>0 ⊙
∂(zN)"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.08015267175572519,"∂Dl
−1 L"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.08206106870229007,"
Dlz⊤
N + (D⊤
l zN −yl)In + D⊤D ∂(zN) ∂Dl 
. ∂(zN)"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.08396946564885496,"∂Dl
will be denoted by JN
l . It converges towards the weak Jacobian J∗
l of z∗relatively to Dl,
whose values are
J∗
l S∗= −(D⊤
:,S∗D:,S∗)−1(Dlz∗⊤+ (D⊤
l z∗−yl)In)S∗,
on the support S∗of z∗, and 0 elsewhere. Moreover, R(J∗, S∗) = 0."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.08587786259541985,"This result is similar to Bertrand et al. (2020) where the Jacobian of z is computed over λ to perform
hyper-parameter optimization in Lasso-type models. Using R(J∗, S∗) = 0, we can write
R(JN, eSN)
 ≤
R(JN, eSN) −R(J∗, S∗)
 ≤L ∥JN −J∗∥,
(5)"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.08778625954198473,"as
∇2
1,1f(z∗, D)

2 = L. If the back-propagation were to output an accurate estimate JN of the"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.08969465648854962,"weak Jacobian J∗,
R(JN, f
SN)
 would be 0, and the convergence rate of g2
N could be twice as"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.0916030534351145,"fast as the one of g1
N. To quantify this, we now analyze the convergence of JN towards J∗. In
Proposition 2.5, we compute an upper bound of
JN
l
−J∗
l
 with possible usage of truncated back-
propagation (Shaban et al., 2019). Truncated back-propagation of depth K corresponds to an initial
estimate of the Jacobian JN−K = 0 and iterating the induction in Theorem 2.4."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.09351145038167939,"Proposition 2.5 Let N be the number of iterations and K be the back-propagation depth. We as-
sume that ∀n ≥N −K, S∗⊂Sn. Let ¯EN = Sn\S∗, let L be the largest eigenvalue of D⊤
:,S∗D:,S∗,"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.09541984732824428,"and let µn be the smallest eigenvalue of D⊤
:,SnD:,Sn−1. Let Bn =
PEn −D⊤
:, ¯
EnD†⊤
:,S∗PS∗
, where"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.09732824427480916,PS is the projection on RS and D† is the pseudo-inverse of D. We have
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.09923664122137404,"JN
l
−J∗
l
 ≤ K
Y k=1"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.10114503816793893,"
1 −µN−k L"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.10305343511450382,"
∥J∗
l ∥+ 2"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1049618320610687,"L ∥Dl∥ K−1
X k=0 k
Y"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.10687022900763359,"i=1
(1−µN−i"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.10877862595419847,"L
)
 zN−k
l
−z∗
l
+BN−k ∥z∗
l ∥

."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.11068702290076336,Published as a conference paper at ICLR 2022
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.11259541984732824,"100
102
104"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.11450381679389313,Iterations N 10-3 100 0 20
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.11641221374045801,"100
102
104"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1183206106870229,Iterations N 0 10
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.12022900763358779,"∥J N
l −J ∗
l ∥"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.12213740458015267,"100
102
104"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.12404580152671756,Iterations N 10-8 10-2 0 10 20
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.12595419847328243,"100
102
104"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.12786259541984732,Iterations N 0 20
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1297709923664122,Max BP depth
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1316793893129771,"full
200
50
20
∥J N
l −J ∗
l ∥
∥SN −S ∗∥0"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.13358778625954199,"Figure 1: Average convergence of JN
l
towards J∗
l for two samples from the same data set, gener-
ated with a random Gaussian matrix.
J∗
l −JN
l
 converges linearly on the support in both cases.
However, for sample 2, full back-propagation makes the convergence unstable, and truncated back-
propagation improves its behavior, as described in Proposition 2.5. The proportion of stable and
unstable samples in this particular example is displayed in Figure 2."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.13549618320610687,"Proposition 2.5 reveals multiple stages in the Jacobian estimation. First, one can see that if all
iterates used for the back-propagation lie on the support S∗, the Jacobian estimate has a quasi-linear
convergence, as shown in the following corollary."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.13740458015267176,"Corollary 2.6 Let µ > 0 be the smallest eigenvalue of D⊤
:,S∗D:,S∗. Let K ≤N be the back-
propagation depth and let ∆N = F(zN, D) −F(z∗, D) + L"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.13931297709923665,"2 ∥zN −z∗∥. Suppose that ∀n ∈
[N −K, N]; Sn ⊂S∗. Then, we have
J∗
l −JN
l
 ≤

1 −µ L"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.14122137404580154,"K
∥J∗
l ∥+ K

1 −µ L"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1431297709923664,"K−1
∥Dl∥4∆N−K L2
."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1450381679389313,"Once the support is reached, ISTA also converges with the same linear rate (1 −µ"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.14694656488549618,"L). Thus the
gradient estimate g2
N converges almost twice as fast as g1
N in the best case – with optimal sub-
gradient – as O(K(1 −µ"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.14885496183206107,"L)2K). This is similar to Ablin et al. (2020, Proposition.5) and Tolooshams
& Ba (2021). Second, Proposition 2.5 shows that
J∗
l −JN
l
 may increase when the support is not
well-estimated, leading to a deterioration of the gradient estimate. This is due to an accumulation
of errors materialized by the sum in the right-hand side of the inequality, as the term BN ∥z∗∥may
not vanish to 0 as long as SN ̸⊂S∗. Interestingly, once the support is reached at iteration S < N,
the errors converge linearly towards 0, and we recover the fast estimation of g∗with g2. Therefore,
Lasso-based DDL should either be used with a low number of steps or truncated back-propagation
to ensure stability. These results apply for all linear dictionaries, including convolutions."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.15076335877862596,"101
103"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.15267175572519084,Iterations N 0 10 20 30
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.15458015267175573,"∥J N
l −J ∗
l ∥"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.15648854961832062,"Figure 2:
Average conver-
gence of JN
l
towards J∗
l for
50 samples. In this example,
40% of the Jacobians are un-
stable (red curves)."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.15839694656488548,"Numerical illustrations.
We now illustrate these theoretical re-
sults depending on the number N of unrolled iterations. The data
are generated from a random Gaussian dictionary D of size 30×50,
with Bernoulli-Gaussian sparse codes z (sparsity 0.3, σ2
z = 1), and
Gaussian noise (σ2
noise = 0.1) – more details in Appendix A."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.16030534351145037,"Figure 1 conﬁrms the linear convergence of JN
l
once the support
is reached. However, the convergence might be unstable when the
number of iteration grows, leading to exploding gradient, as illus-
trated in the second case. When this happens, using a small number
of iterations or truncated back-propagation becomes necessary to
prevent accumulating errors. It is also of interest to look at the pro-
portion of unstable Jacobians (see Figure 2). We recover behaviors
observed in the ﬁrst and second case in Figure 1. 40% samples suf-
fer from numerical instabilities in this example. This has a negative
impact on the gradient estimation outside of the support."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.16221374045801526,"We display the convergence behavior of the gradients estimated by
AM and by DDL with different back-propagation depths (20, 50, full) for simulated data and images
in Figure 3. We unroll FISTA instead of ISTA to make the convergence faster. We observed similar"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.16412213740458015,Published as a conference paper at ICLR 2022
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.16603053435114504,"101
103"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.16793893129770993,Iterations N 10-8 10-4 100
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.16984732824427481,"1 −
­
g, g ∗®"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1717557251908397,Gaussian dictionary
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1736641221374046,"101
103"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.17557251908396945,Iterations N 10-3 100
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.17748091603053434,"1 −
­
g, g ∗®"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.17938931297709923,Noisy image
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.18129770992366412,"101
103"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.183206106870229,Iterations N 0.2 0.0 0.2
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1851145038167939,Relative diff.
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.18702290076335878,Noisy image
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.18893129770992367,"BP depth
AM
20
50
full"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.19083969465648856,"Figure 3:
Gradient convergence in angle for 1000 synthetic samples (left) and patches from a
noisy image (center). The image is normalized, decomposed into patches of dimension 10 × 10 and
with additive Gaussian noise (σ2 = 0.1). The dictionary for which the gradients are computed is
composed of 128 patches from the image. (right) Relative difference between angles from DDL and
AM. Convergence is faster with DDL in early iterations, and becomes unstable with too many steps."
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.19274809160305342,"behaviors for both algorithms in early iterations but using ISTA required too much memory to reach
full convergence. As we optimize using a line search algorithm, we are mainly interested in the abil-
ity of the estimate to provide an adequate descent direction. Therefore, we display the convergence
in angle deﬁned as the cosine similarity ⟨g, g∗⟩= T r(gT g∗)"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1946564885496183,"∥g∥∥g∗∥. The angle provides a good metric to
assert that the two gradients are correlated and thus will lead to similar optimization paths. We also
provide the convergence in norm in appendix. We compare g1
N and g2
N with the relative difference"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1965648854961832,"of their angles with g∗, deﬁned as ⟨g2
N,g∗⟩−⟨g1
N,g∗⟩
1−⟨g1
N,g∗⟩
. When its value is positive, DDL provides the"
BI-LEVEL OPTIMIZATION FOR APPROXIMATE DICTIONARY LEARNING,0.1984732824427481,"best descent direction. Generally, when the back-propagation goes too deep, the performance of g2
N
decreases compared to g1
N, and we observe large numerical instabilities. This behavior is coherent
with the Jacobian convergence patterns studied in Proposition 2.5. Once on the support, g2
N reaches
back the performance of g1
N as anticipated. In the case of a real image, unrolling beats AM by up
to 20% in terms of gradient direction estimation when the number of iterations does not exceed 50,
especially with small back-propagation depth. This highlights that the principal interest of unrolled
algorithms is to use them with a small number of layers – i.e., a small number of iterations."
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.20038167938931298,"3
APPROXIMATE DICTIONARY LEARNING IN PRACTICE"
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.20229007633587787,"This section introduces practical guidelines on Lasso-based approximate dictionary learning with
unit norm constraint, and we provide empirical justiﬁcations for its ability to recover the dictionary.
We also propose a strategy to scale DDL with a stochastic optimization method. We provide a full
description of all our experiments in Appendix A. We optimize with projected gradient descent com-
bined to a line search to compute high-quality steps sizes. The computations have been performed
on a GPU NVIDIA Tesla V100-DGXS 32GB using PyTorch (Paszke et al., 2019).1"
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.20419847328244276,"Improvement of precision.
As stated before, a low number of iterations allows for efﬁcient and
stable computations, but this makes the sparse code less precise. One can learn the steps sizes of
(F)ISTA to speed up convergence and compensate for imprecise representations, as proposed by
Ablin et al. (2019) for LISTA. To avoid poor results due to large degrees of freedom in unsuper-
vised learning, we propose a method in two steps to reﬁne the initialization of the dictionary before
relaxing the constraints on the steps sizes:"
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.20610687022900764,1. We learn the dictionary with ﬁxed steps sizes equal to 1
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.20801526717557253,"L where L = ∥D∥2, given by convergence
conditions. Lipschitz constants or upper bounds are computed at each gradient step with norms,
or the FFT for convolutions, outside the scope of the network graph."
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.2099236641221374,"2. Then, once convergence is reached, we jointly learn the step sizes and the dictionary. Both are
still updated using gradient descent with line search to ensure stable optimization."
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.21183206106870228,1Code is available at https://github.com/bmalezieux/unrolled_dl.
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.21374045801526717,Published as a conference paper at ICLR 2022
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.21564885496183206,"101
102
103"
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.21755725190839695,Iterations N 100 200
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.21946564885496184,Number
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.22137404580152673,Gradient steps
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.22328244274809161,"101
102
103"
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.22519083969465647,Iterations N 10-1 102
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.22709923664122136,FN −F ∗ Loss
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.22900763358778625,"101
102
103"
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.23091603053435114,Iterations N 10-3 10-2
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.23282442748091603,SN −S ∗
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.23473282442748092,Rec. score
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.2366412213740458,"AM
DDL
DDL + steps"
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.2385496183206107,"Figure 4: (left) Number of gradient steps performed by the line search before convergence, (center)
distance to the optimal loss, and (right) distance to the optimal dictionary recovery score depending
on the number of unrolled iterations. The data are generated as in Figure 1. We display the mean and
the 10% and 90% quantiles over 50 random experiments. DDL needs less gradient steps to converge
in early iterations, and unrolling obtains high recovery scores with only a few dozens of iterations."
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.24045801526717558,"100
101
102"
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.24236641221374045,Iterations N 20 25 PSNR
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.24427480916030533,Denoising
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.24618320610687022,"AM
DDL
DDL_steps
DL-Oracle"
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.2480916030534351,"10
18
SNR (dB) 21 25 PSNR 0.7 0.9"
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.25,Rec. score
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.25190839694656486,Min. distribution
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.2538167938931298,"2
0
2
Normalized distance Loss"
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.25572519083969464,CDL minima
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.25763358778625955,"Figure 5: We consider a normalized image degraded by Gaussian noise. (left) PSNR depending on
the number of unrolled iterations for σ2
noise = 0.1, i.e. PSNR = 10 dB. DL-Oracle stands for full
AM dictionary learning (103 iterations of FISTA). There is no need to unroll too many iterations to
obtain satisfying results. (center) PSNR and average recovery score between dictionaries depending
on the SNR for 50 random initializations in CDL. (right) 10 loss landscapes in 1D for σ2
noise = 0.1.
DDL is robust to random initialization when there is not too much noise."
APPROXIMATE DICTIONARY LEARNING IN PRACTICE,0.2595419847328244,"The use of LISTA-like algorithms with no ground truth generally aims at improving the speed of
sparse coding when high precision is not required. When it is the case, the ﬁnal sparse codes can be
computed separately with FISTA (Beck & Teboulle, 2009) or coordinate descent (Wu et al., 2008)
to improve the quality of the representation."
OPTIMIZATION DYNAMICS IN APPROXIMATE DICTIONARY LEARNING,0.26145038167938933,"3.1
OPTIMIZATION DYNAMICS IN APPROXIMATE DICTIONARY LEARNING"
OPTIMIZATION DYNAMICS IN APPROXIMATE DICTIONARY LEARNING,0.2633587786259542,"In this part, we study empirical properties of approximate dictionary learning related to global opti-
mization dynamics to put our results on gradient estimation in a broader context."
OPTIMIZATION DYNAMICS IN APPROXIMATE DICTIONARY LEARNING,0.2652671755725191,"Unrolling v. AM.
In Figure 4, we show the number of gradient steps before reaching convergence,
the behavior of the loss FN, and the recovery score deﬁned at the beginning of the section for syn-
thetic data generated by a Gaussian dictionary. As a reminder, S(C) = maxσ∈Sn
1
n
Pn
i=1 |Cσ(i),i|
where C is the correlation matrix between the columns of the true dictionary and the estimate. The
number of iterations corresponds to N in the estimate zN(D). First, DDL leads to fewer gradient
steps than AM in the ﬁrst iterations. This suggests that automatic differentiation better estimates
the directions of the gradients for small depths. However, computing the gradient requires back-
propagating through the algorithm, and DDL takes 1.5 times longer to perform one gradient step
than AM on average for the same number of iterations N. When looking at the loss and the recovery
score, we notice that the advantage of DDL for the minimization of FN is minor without learning
the steps sizes, but there is an increase of performance concerning the recovery score. DDL bet-
ter estimates the dictionary for small depths, inferior to 50. When unrolling more iterations, AM
performs as well as DDL on the approximate problem and is faster."
OPTIMIZATION DYNAMICS IN APPROXIMATE DICTIONARY LEARNING,0.26717557251908397,"Approximate DL.
Figure 4 shows that high-quality dictionaries are obtained before the conver-
gence of FN, either with AM or DDL. 40 iterations are sufﬁcient to reach a reasonable solution"
OPTIMIZATION DYNAMICS IN APPROXIMATE DICTIONARY LEARNING,0.26908396946564883,Published as a conference paper at ICLR 2022
OPTIMIZATION DYNAMICS IN APPROXIMATE DICTIONARY LEARNING,0.27099236641221375,"concerning the recovery score, even though the loss is still very far from the optimum. This suggests
that computing optimal sparse codes at each gradient step is unnecessary to recover the dictionary.
Figure 5 illustrates that by showing the PSNR of a noisy image reconstruction depending on the
number of iterations, compared to full AM dictionary learning with 103 iterations. As for synthetic
data, optimal performance is reached very fast. In this particular case, the model converges after
80 seconds with approximate DL unrolled for 20 iterations of FISTA compared to 600 seconds in
the case of standard DL. Note that the speed rate highly depends on the value of λ. Higher values
of λ tend to make FISTA converge faster, and unrolling becomes unnecessary in this case. On the
contrary, unrolling is more efﬁcient than AM for lower values of λ."
OPTIMIZATION DYNAMICS IN APPROXIMATE DICTIONARY LEARNING,0.2729007633587786,"Loss landscape.
The ability of gradient descent to ﬁnd adequate local minima strongly depends on
the structure of the problem. To quantify this, we evaluate the variation of PSNR depending on the
Signal to Noise Ratio (SNR) (10 log10 (σ2/σ2
b) where σ2
b is the variance of the noise) for 50 random
initializations in the context of convolutional dictionary learning on a task of image denoising, with
20 unrolled iterations. Figure 5 shows that approximate CDL is robust to random initialization when
the level of noise is not too high. In this case, all local minima are similar in terms of reconstruction
quality. We provide a visualization of the loss landscape with the help of ideas presented in Li
et al. (2018). The algorithm computes a minimum, and we chose two properly rescaled vectors to
create a plan from this minimum. The 3D landscape is displayed on this plan in Figure B using the
Python library K3D-Jupyter2. We also compare in Figure 5 (right) the shapes of local minima in 1D
by computing the values of the loss along a line between two local minima. These visualizations
conﬁrm that dictionary learning locally behaves like a convex function with similar local minima."
STOCHASTIC DDL,0.2748091603053435,"3.2
STOCHASTIC DDL"
STOCHASTIC DDL,0.2767175572519084,"0
10
20
30
40
50
Time (s) 0.4 0.6 0.8 1.0"
STOCHASTIC DDL,0.2786259541984733,Rec. score
STOCHASTIC DDL,0.28053435114503816,"Minibatch size
100
500"
STOCHASTIC DDL,0.2824427480916031,"2000
10000"
STOCHASTIC DDL,0.28435114503816794,"Full batch
Complete AM"
STOCHASTIC DDL,0.2862595419847328,"Figure 6: Recovery score vs. time for 10
random Gaussian matrices and 105 samples.
Initialization with random dictionaries. In-
termediate batch sizes offer a good trade-off
between speed and memory usage."
STOCHASTIC DDL,0.2881679389312977,"In order to apply DDL in realistic settings, it
is tempting to adapt Stochastic Gradient Descent
(SGD), commonly used for neural networks. The
major advantage is that the sparse coding is not per-
formed on all data at each forward pass, leading to
signiﬁcant time and memory savings. The issue is
that the choice of gradient steps is critical to the op-
timization process in dictionary learning, and SGD
methods based on simple heuristics like rate decay
are difﬁcult to tune in this context. We propose to
leverage a new optimization scheme introduced in
Vaswani et al. (2019), which consists of performing
a stochastic line search. The algorithm computes a
good step size at each epoch, after which a heuristic
decreases the maximal step. Figure 6 displays the
recovery score function of the time for various mini-
batch sizes on a problem with 105 samples. The data were generated as in Figure 1 but with a
larger dictionary (50×100). The algorithm achieves good performance with small mini-batches and
thus limited memory usage. We also compare this method with Online dictionary learning (Mairal
et al., 2009) in Figure E. It shows that our method speeds up the dictionary recovery, especially
for lower values of λ. This strategy can be adapted very easily for convolutional models by taking
sub-windows of the full signal and performing a stochastic line search, as demonstrated in Section 4.
See Tolooshams et al. (2020) for another unrolled stochastic CDL algorithm applied to medical data."
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.2900763358778626,"4
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS"
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.2919847328244275,"In magnetoencephalography (MEG), the measurements over the scalp consist of hundreds of simul-
taneous recordings, which provide information on the neural activity during a large period. Convo-
lutional dictionary learning makes it possible to learn cognitive patterns corresponding to physiolog-
ical activities (Dupr´e la Tour et al., 2018). As the electromagnetic waves propagate through the brain
at the speed of light, every sensor measures the same waveform simultaneously but not at the same"
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.29389312977099236,2Package available at https://github.com/K3D-tools/K3D-jupyter.
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.2958015267175573,Published as a conference paper at ICLR 2022
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.29770992366412213,"Spatial pattern 0
Spatial pattern 1
Spatial pattern 2"
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.29961832061068705,"0.0
0.5
1.0
Time (s) 0.5 0.0"
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.3015267175572519,Temporal pattern 0
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.30343511450381677,"0.0
0.5
1.0
Time (s) 0.2 0.0"
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.3053435114503817,Temporal pattern 1
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.30725190839694655,"0.0
0.5
1.0
Time (s) 0.0 0.2 0.4"
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.30916030534351147,Temporal pattern 2
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.3110687022900763,"Figure 7:
Stochastic Deep
CDL on 6 minutes of MEG
data (204 channels, sampling
rate of 150Hz).
The algo-
rithm uses 40 atoms, 30 un-
rolled iterations and 100 iter-
ations with batch size 20. We
recover heartbeat (0), blink-
ing (1) artifacts, and an au-
ditory evoked response (2)
among others."
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.31297709923664124,"Minibatch
Time window
Steps learning
Corr. u
Corr. v
Mean corr.
Time
5
20 s
True
0.85 ± 0.02
0.84 ± 0.06
0.845
110 s
5
20 s
False
0.88 ± 0.02
0.78 ± 0.06
0.83
57 s
5
10 s
True
0.83 ± 0.01
0.82 ± 0.09
0.825
56 s
20
10 s
True
0.85 ± 0.01
0.75 ± 0.09
0.80
163 s"
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.3148854961832061,"Table 1: Stochastic Deep CDL on MEG data (as in Figure 7). We compare u and v to 12 important
atoms output by alphacsc (correlation averaged on 5 runs), depending on several hyperparame-
ters, with 30 layers, 10 epochs and 10 iterations per epochs. λrescaled = 0.3λmax, λmax =
DT y

∞.
The best setups achieve 80% – 90% average correlation with alphacsc in around 100 sec. com-
pared to around 1400 sec. Our method is also faster than convolutional K-SVD (Yellin et al., 2017)."
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.31679389312977096,"intensity. The authors propose to rely on multivariate convolutional sparse coding (CSC) with rank-
1 constraint to leverage this physical property and learn prototypical patterns. In this case, space and
time patterns are disjoint in each atom: Dk = ukvT
k where u gathers the spatial activations on each
channel and v corresponds to the temporal pattern. This leads to the model"
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.3187022900763359,"min
zk∈RT ,uk∈RS,vk∈Rt
1
2  n
X"
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.32061068702290074,"k=1
(ukv⊤
k ) ∗zk −y  2 2
+ λ n
X"
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.32251908396946566,"k=1
∥zk∥1 ,
(6)"
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.3244274809160305,"where n is the number of atoms, T is the total recording time, t is the kernel size, and S is the number
of sensors. We propose to learn u and v with Stochastic Deep CDL unrolled for a few iterations to
speed up the computations of the atoms. Figure 7 reproduces the multivariate CSC experiments of
alphacsc3 (Dupr´e la Tour et al., 2018) on the dataset sample of MNE (Gramfort et al., 2013) – 6
minutes of recordings with 204 channels sampled at 150Hz with visual and audio stimuli."
APPLICATION TO PATTERN LEARNING IN MEG SIGNALS,0.32633587786259544,"The algorithm recovers the main waveforms and spatial patterns with approximate sparse codes and
without performing the sparse coding on the whole data set at each gradient iteration, which leads
to a signiﬁcant gain of time. We are able to distinguish several meaningful patterns as heartbeat
and blinking artifacts or auditive evoked response. As this problem is unsupervised, it is difﬁcult
to provide robust quantitative quality measurements. Therefore, we compare our patterns to 12
important patterns recovered by alpahcsc in terms of correlation in Table 1. Good setups achieve
between 80% and 90% average correlation ten times faster."
CONCLUSION,0.3282442748091603,"5
CONCLUSION"
CONCLUSION,0.3301526717557252,"Dictionary learning is an efﬁcient technique to learn patterns in a signal but is challenging to ap-
ply to large real-world problems. This work showed that approximate dictionary learning, which
consists in replacing the optimal solution of the Lasso with a time-efﬁcient approximation, offers
a valuable trade-off between computational cost and quality of the solution compared to complete
Alternating Minimization. This method, combined with a well-suited stochastic gradient descent
algorithm, scales up to large data sets, as demonstrated on a MEG pattern learning problem. This
work provided a theoretical study of the asymptotic behavior of unrolling in approximate dictionary
learning. In particular, we showed that numerical instabilities make DDL usage inefﬁcient when too
many iterations are unrolled. However, the super-efﬁciency of DDL in the ﬁrst iterations remains
unexplained, and our ﬁrst ﬁndings would beneﬁt from theoretical support."
CONCLUSION,0.3320610687022901,3Package and experiments available at https://alphacsc.github.io
CONCLUSION,0.33396946564885494,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.33587786259541985,ETHICS STATEMENT
ETHICS STATEMENT,0.3377862595419847,"The MEG data conform to ethic guidelines (no individual names, collected under individual’s con-
sent, ...)."
REPRODUCIBILITY STATEMENT,0.33969465648854963,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.3416030534351145,"Code is available at https://github.com/bmalezieux/unrolled_dl. We provide a
full description of all our experiments in Appendix A, and the proofs of our theoretical results in
Appendix C."
REPRODUCIBILITY STATEMENT,0.3435114503816794,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.34541984732824427,This work was supported by grants from Digiteo France.
REFERENCES,0.3473282442748092,REFERENCES
REFERENCES,0.34923664122137404,"Pierre Ablin, Thomas Moreau, Mathurin Massias, and Alexandre Gramfort. Learning step sizes
for unfolded sparse coding. In Advances in Neural Information Processing Systems, pp. 13100–
13110, 2019."
REFERENCES,0.3511450381679389,"Pierre Ablin, Gabriel Peyr´e, and Thomas Moreau. Super-efﬁciency of automatic differentiation for
functions deﬁned as a minimum. In Proceedings of the 37th International Conference on Machine
Learning, pp. 32–41, 2020."
REFERENCES,0.3530534351145038,"Alekh Agarwal, Animashree Anandkumar, Prateek Jain, and Praneeth Netrapalli. Learning sparsely
used overcomplete dictionaries via alternating minimization. SIAM Journal on Optimization, 26
(4):2775–2799, 2016."
REFERENCES,0.3549618320610687,"Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for designing overcom-
plete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54:4311 –
4322, 2006."
REFERENCES,0.3568702290076336,"Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM J. Imaging Sciences, 2:183–202, 2009."
REFERENCES,0.35877862595419846,"Quentin Bertrand, Quentin Klopfenstein, Mathieu Blondel, Samuel Vaiter, Alexandre Gramfort, and
Joseph Salmon. Implicit differentiation of lasso-type models for hyperparameter optimization. In
International Conference on Machine Learning, pp. 810–821. PMLR, 2020."
REFERENCES,0.3606870229007634,"Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of
unfolded ista and its practical weights and thresholds. Advances in Neural Information Processing
Systems, 2018."
REFERENCES,0.36259541984732824,"Scott R Cole and Bradley Voytek. Brain oscillations and the importance of waveform shape. Trends
in cognitive sciences, 21(2):137–149, 2017."
REFERENCES,0.36450381679389315,"John M. Danskin. Theory of Max-Min and Its Application to Weapons Allocation Problems. Springer
Berlin Heidelberg, Berlin/Heidelberg, 1967."
REFERENCES,0.366412213740458,"Ingrid Daubechies, Michel Defrise, and Christine Mol. An iterative thresholding algorithm for linear
inverse problems with a sparsity constrains. Communications on Pure and Applied Mathematics,
57, 2004."
REFERENCES,0.3683206106870229,"Charles-Alban Deledalle, Samuel Vaiter, Jalal Fadili, and Gabriel Peyr´e. Stein unbiased gradient
estimator of the risk (sugar) for multiple parameter selection. SIAM Journal on Imaging Sciences,
7(4):2448–2487, 2014."
REFERENCES,0.3702290076335878,"Tom Dupr´e la Tour, Thomas Moreau, Mainak Jas, and Alexandre Gramfort. Multivariate convolu-
tional sparse coding for electromagnetic brain signals. Advances in Neural Information Process-
ing Systems, 31:3292–3302, 2018."
REFERENCES,0.37213740458015265,Published as a conference paper at ICLR 2022
REFERENCES,0.37404580152671757,"Alexandre Gramfort, Martin Luessi, Eric Larson, Denis A Engemann, Daniel Strohmeier, Christian
Brodbeck, Roman Goj, Mainak Jas, Teon Brooks, Lauri Parkkonen, et al. Meg and eeg data
analysis with mne-python. Frontiers in neuroscience, 7:267, 2013."
REFERENCES,0.37595419847328243,"Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. International con-
ference on machine learning, pp. 399–406, 2010."
REFERENCES,0.37786259541984735,"R´emi Gribonval, Rodolphe Jenatton, and Francis Bach. Sparse and spurious: dictionary learning
with noise and outliers. IEEE Transactions on Information Theory, 61(11):6298–6319, 2015."
REFERENCES,0.3797709923664122,"Roger Grosse, Rajat Raina, Helen Kwong, and Andrew Y. Ng. Shift-Invariant Sparse Coding for
Audio Classiﬁcation. Cortex, 8:9, 2007."
REFERENCES,0.3816793893129771,"Benjamin D Haeffele and Ren´e Vidal. Global optimality in tensor factorization, deep learning, and
beyond. arXiv preprint arXiv:1506.07540, 2015."
REFERENCES,0.383587786259542,"Bruno Lecouat, Jean Ponce, and Julien Mairal. A ﬂexible framework for designing trainable priors
with adaptive smoothing and game encoding.
In Advances in neural information processing
systems, 2020."
REFERENCES,0.38549618320610685,"Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. In Advances in neural information processing systems, pp. 6389–6399,
2018."
REFERENCES,0.38740458015267176,"Jialin Liu and Xiaohan Chen. Alista: Analytic weights are as good as learned weights in lista. In
International Conference on Learning Representations, 2019."
REFERENCES,0.3893129770992366,"Julien Mairal, Francis Bach, J. Ponce, and Guillermo Sapiro. Online learning for matrix factorization
and sparse coding. Journal of Machine Learning Research, 11, 2009."
REFERENCES,0.39122137404580154,"Thomas Moreau and Joan Bruna. Understanding neural sparse coding with matrix factorization. In
International Conference on Learning Representation, 2017."
REFERENCES,0.3931297709923664,"Thomas Moreau and Alexandre Gramfort. Dicodile: Distributed convolutional dictionary learning.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020."
REFERENCES,0.3950381679389313,"Bruno A. Olshausen and David J Field. Sparse coding with an incomplete basis set: A strategy
employed by \protect{V1}. Vision Research, 37(23):3311–3325, 1997."
REFERENCES,0.3969465648854962,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems, pp.
8026–8037, 2019."
REFERENCES,0.3988549618320611,"Meyer Scetbon, Michael Elad, and Peyman Milanfar. Deep k-svd denoising. IEEE Transactions on
Image Processing, 30:5944–5955, 2021."
REFERENCES,0.40076335877862596,"Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation
for bilevel optimization. In International Conference on Artiﬁcial Intelligence and Statistics, pp.
1723–1732. PMLR, 2019."
REFERENCES,0.4026717557251908,"Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere i: Overview and
the geometric picture. IEEE Transactions on Information Theory, 63(2):853–884, 2016."
REFERENCES,0.40458015267175573,"Wen Tang, Emilie Chouzenoux, Jean-Christophe Pesquet, and Hamid Krim. Deep transform and
metric learning network: Wedding deep dictionary learning and neural networks. arXiv preprint
arXiv:2002.07898, 2020."
REFERENCES,0.4064885496183206,"Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society Series B, 58:267–288, 1996."
REFERENCES,0.4083969465648855,"Ryan J. Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics, 7(1):1456–
1490, 2013."
REFERENCES,0.41030534351145037,Published as a conference paper at ICLR 2022
REFERENCES,0.4122137404580153,"Bahareh Tolooshams and Demba Ba. Pudle: Implicit acceleration of dictionary learning by back-
propagation. arXiv preprint, 2021."
REFERENCES,0.41412213740458015,"Bahareh Tolooshams, Sourav Dey, and Demba Ba. Deep residual autoencoders for expectation
maximization-inspired dictionary learning. IEEE Transactions on Neural Networks and Learning
Systems, PP:1–15, 2020."
REFERENCES,0.41603053435114506,"Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-
Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. Advances
in neural information processing systems, 32:3732–3745, 2019."
REFERENCES,0.4179389312977099,"Tong Tong Wu, Kenneth Lange, et al. Coordinate descent algorithms for lasso penalized regression.
Annals of Applied Statistics, 2(1):224–244, 2008."
REFERENCES,0.4198473282442748,"Yande Xiang, Zhitao Lin, and Jianyi Meng. Automatic qrs complex detection using two-level con-
volutional neural network. Biomedical engineering online, 17(1):1–17, 2018."
REFERENCES,0.4217557251908397,"Florence Yellin, Benjamin D Haeffele, and Ren´e Vidal. Blood cell detection and counting in holo-
graphic lens-free imaging by convolutional sparse dictionary learning and coding. In International
Symposium on Biomedical Imaging, pp. 650–653. IEEE, 2017."
REFERENCES,0.42366412213740456,"John Zarka, Louis Thiry, Tomas Angles, and Stephane Mallat.
Deep network classiﬁcation by
scattering and homotopy dictionary learning. In International Conference on Learning Represen-
tations, 2019."
REFERENCES,0.4255725190839695,"A
FULL DESCRIPTION OF THE EXPERIMENTS"
REFERENCES,0.42748091603053434,This section provides complementary information on the experiments presented in the paper.
REFERENCES,0.42938931297709926,"A.1
CONVERGENCE OF THE JACOBIANS - FIGURE 1 AND FIGURE 2"
REFERENCES,0.4312977099236641,"We generate a normalized random Gaussian dictionary D of dimension 30 × 50, and sparse codes
z from a Bernoulli Gaussian distribution of sparsity 0.3 and σ2 = 1. The signal to process is y =
Dz + b where b is an additive Gaussian noise with σ2
noise = 0.1. The Jacobians are computed for a
random perturbation D+bD of D where bD is a Gaussian noise of scale 0.5σ2
D. JN
l corresponds to
the approximate Jacobian with N iterations of ISTA with λ = 0.1. J∗
l corresponds the true Jacobian
computed with sparse codes obtained after 104 iterations of ISTA with λ = 0.1."
REFERENCES,0.43320610687022904,"In Figure 2, the norm
JN
l
−J∗
l
 is computed for 50 samples."
REFERENCES,0.4351145038167939,"A.2
CONVERGENCE OF THE GRADIENT ESTIMATES - FIGURE 3"
REFERENCES,0.43702290076335876,"Synthetic data.
We generate a normalized random Gaussian dictionary D of dimension 30 × 50,
and 1000 sparse codes z from a Bernoulli Gaussian distribution of sparsity 0.3 and σ2 = 1. The
signal to process is y = Dz + b where b is an additive Gaussian noise with σ2
noise = 0.1. The
gradients are computed for a random perturbation D + bD of D where bD is a Gaussian noise of
scale 0.5σ2
D."
REFERENCES,0.4389312977099237,"Noisy image.
A 128 × 128 black-and-white image is degraded by a Gaussian noise with σ2
noise =
0.1 and normalized. We processed 1000 patches of dimension 10 × 10 from the image, and we
computed the gradients for a dictionary composed of 128 random patches."
REFERENCES,0.44083969465648853,"gN corresponds to the gradient for N iterations of FISTA with λ = 0.1. g∗corresponds to the true
gradient computed with a sparse code obtained after 104 iterations of FISTA."
REFERENCES,0.44274809160305345,"A.3
OPTIMIZATION DYNAMICS ON SYNTHETIC DATA - FIGURE 4"
REFERENCES,0.4446564885496183,"We generate a normalized random Gaussian dictionary D of dimension 30 × 50, and sparse codes
z from a Bernoulli Gaussian distribution of sparsity 0.3 and σ2 = 1. The signal to process is
y = Dz + b where b is an additive Gaussian noise with σ2
noise = 0.1. The initial dictionary"
REFERENCES,0.44656488549618323,Published as a conference paper at ICLR 2022
REFERENCES,0.4484732824427481,"is taken as a random perturbation D + bD of D where bD is a Gaussian noise of scale 0.5σ2
D.
N corresponds to the number of unrolled iterations of FISTA. F ∗is the value of the loss for 103
iterations minus 10−3. S∗is the score obtained after 103 iterations plus 10−3. The optimization is
done with λ = 0.1. We compare the number of gradient steps (left), the loss values (center), and the
recovery scores (right) for 50 different dictionaries. DDL with steps sizes learning is evaluated on
100 iterations only due to memory and optimization time issues."
REFERENCES,0.45038167938931295,"A.4
OPTIMIZATION DYNAMICS AND LOSS LANDSCAPES ON IMAGES - FIGURE 5"
REFERENCES,0.45229007633587787,A 128 × 128 black-and-white image is degraded by a Gaussian noise and normalized.
REFERENCES,0.4541984732824427,"Left.
In this experiment, σ2
noise = 0.1. We learn a dictionary composed of 128 atoms on 10 × 10
patches with FISTA and λ = 0.1 in all cases. The PSNR is obtained with sparse codes output by
the network. The results are compared to the truth with the Peak Signal to Noise Ratio. Dictionary
learning denoising with 1000 iterations of FISTA is taken as a baseline."
REFERENCES,0.45610687022900764,"Center.
We learn 50 dictionaries from 50 random initializations in convolutional dictionary learn-
ing with 50 kernels of size 8 × 8 with 20 unrolled iterations of FISTA and λ = 0.1. The PSNR is
obtained with sparse codes output by the network. We compare the average, minimal and maximal
PSNR, and recovery scores with all other dictionaries to study the robustness to random initialization
depending on the level of noise (SNR)."
REFERENCES,0.4580152671755725,"Right.
In this experiment, σ2
noise = 0.1. We learn 2 dictionaries from 2 random initializations in
convolutional dictionary learning with 50 kernels of size 8 × 8 with 20 unrolled iterations of FISTA
and λ = 0.1. We display the loss values on the line between these two dictionaries. The experiment
is repeated on 10 different random initializations."
REFERENCES,0.4599236641221374,"A.5
STOCHASTIC DDL ON SYNTHETIC DATA - FIGURE 6"
REFERENCES,0.4618320610687023,"We generate a normalized random Gaussian dictionary D of dimension 50 × 100, and 105 sparse
codes z from a Bernoulli Gaussian distribution of sparsity 0.3 and σ2 = 1. The signal to process
is y = Dz + b where b is an additive Gaussian noise with σ2
noise = 0.1. The initial dictionary is
taken as a random gaussian dictionary. We compare stochastic and full-batch line search projected
gradient descent with 30 unrolled iterations of FISTA and λ = 0.1, without steps sizes learning.
Stochastic DDL is run for 10 epochs with a maximum of 100 iterations for each epoch."
REFERENCES,0.4637404580152672,"A.6
PATTERN LEARNING IN MEG - FIGURE 7"
REFERENCES,0.46564885496183206,"Stochastic Deep CDL on 6 minutes of recordings of MEG data with 204 channels and a sampling
rate of 150Hz. We remove the powerline artifacts and high-pass ﬁlter the signal to remove the
drift which can impact the CSC technique. The signal is also resampled to 150 Hz to reduce the
computational burden. This preprocessing procedure is presented in alphacsc, and available in
the code in the supplementary materials. The algorithm learns 40 atoms of 1 second on mini batches
of 10 seconds, with 30 unrolled iterations of FISTA, λscaled = 0.3, and 10 epochs with 10 iterations
per epoch. The number of mini-batches per iteration is 20, with possible overlap."
REFERENCES,0.4675572519083969,"B
EXTRA FIGURES AND EXPERIMENTAL RESULTS"
REFERENCES,0.46946564885496184,"LISTA - Figure A.
Illustration of LISTA for Dictionary Learning with initialization Z0 = 0 for
N = 3. W 1
D = 1"
REFERENCES,0.4713740458015267,"L(D)⊤, W 2
D = (I −1"
REFERENCES,0.4732824427480916,"L(D)⊤D), where L = ∥D∥2. The result ZN(D) output
by the network is an approximation of the solution of the LASSO."
REFERENCES,0.4751908396946565,"Loss landscape in 2D - Figure B.
We provide a visualization of the loss landscape with the help of
ideas presented in Li et al. (2018). The algorithm computes a minimum, and we chose two properly
rescaled vectors to create a plan from this minimum. The 3D landscape is displayed on this plan in
the appendix using the Python library K3D-Jupyter. This visualization and the visualization in 1D
conﬁrm that (approximate) dictionary learning locally behaves like a convex function with smooth
local minima."
REFERENCES,0.4770992366412214,"Published as a conference paper at ICLR 2022 y W 1
D W 2
D W 1
D W 2
D W 1
D zN(D)"
REFERENCES,0.47900763358778625,Figure A: LISTA
REFERENCES,0.48091603053435117,Figure B: Loss landscape in approximate CDL
REFERENCES,0.48282442748091603,"Gradient convergence in norm - Figure C.
Gradient estimates convergence in norm for synthetic
data (left) and patches from a noisy image (right). The setup is similar to Figure 3. Both gradient
estimates converge smoothly in early iterations. When the back-propagation goes too deep, the
performance of g2
N decreases compared to g1
N, and we observe large numerical instabilities. This
behavior is coherent with the Jacobian convergence patterns studied in Proposition 2.5. Once on the
support, g2
N reaches back the performance of g1
N."
REFERENCES,0.4847328244274809,"100
101
102
103"
REFERENCES,0.4866412213740458,Iterations N 10-1 101 103
REFERENCES,0.48854961832061067,||g ∗−g||
REFERENCES,0.4904580152671756,Gaussian dictionary
REFERENCES,0.49236641221374045,BP depth
REFERENCES,0.49427480916030536,"20
50
full
AM"
REFERENCES,0.4961832061068702,"100
101
102
103"
REFERENCES,0.49809160305343514,Iterations N 101 103
REFERENCES,0.5,||g ∗−g||
REFERENCES,0.5019083969465649,Noisy image
REFERENCES,0.5038167938931297,"Figure C: Gradient estimates convergence in norm for synthetic data (left) and patches from a noisy
image (right). Both gradient estimates converge smoothly in early iterations, after what DDL gradi-
ent becomes unstable. The behavior returns to normal once the algorithm reaches the support."
REFERENCES,0.5057251908396947,"Computation time to reach 0.95 recovery score - Figure D.
The setup is similar to Figure 6. A
random Gaussian dictionary of size 50 × 100 generates the data from 105 sparse codes with sparsity
0.3. The approximate sparse coding is solved with λ = 0.1 and 30 unrolled iterations of FISTA.
The algorithm achieves good performances with small mini-batches and thus limited memory usage.
Stochastic DDL can process large amounts of data and recovers good quality dictionaries faster than
full batch DDL."
REFERENCES,0.5076335877862596,"Sto DDL vs. Online DL - Figure E.
We compare the time Online DL from spams4 (Mairal
et al., 2009) and Stochastic DDL need to reach a recovery score of 0.95 with a batch size of 2000.
Online DL is run with 10 threads. We repeat the experiment 10 times for different values of λ from"
REFERENCES,0.5095419847328244,4package available at http://thoth.inrialpes.fr/people/mairal/spams/
REFERENCES,0.5114503816793893,Published as a conference paper at ICLR 2022
REFERENCES,0.5133587786259542,"100
500
2000
10000
DDL Oracle DL
Minibatch size 0 10 20 30 40 50"
REFERENCES,0.5152671755725191,Time (s)
REFERENCES,0.517175572519084,Timeout
REFERENCES,0.5190839694656488,Time to reach a score of 0.95
REFERENCES,0.5209923664122137,"Figure D: Time to reach a recovery score of 0.95. Intermediate batch sizes offer a good trade-off
between speed and memory usage compared to full-batch DDL."
REFERENCES,0.5229007633587787,"0.1 to 1.0. The setup is similar to Figure D, and we initialize both methods randomly. Stochastic
DDL is more efﬁcient for smaller values of λ, due to the fact that sparse coding is slower in this
case. For higher values of λ, both methods are equivalent. Another advantage of Stochastic DDL
is its modularity. It works on various kinds of dictionary parameterization thanks to automatic
differentiation, as illustrated on 1-rank multivariate convolutional dictionary learning in Figure 7."
REFERENCES,0.5248091603053435,"0.2
0.4
0.6
0.8
1.0
λ 101 102"
REFERENCES,0.5267175572519084,Time (s)
REFERENCES,0.5286259541984732,Time to reach a score of 0.95
REFERENCES,0.5305343511450382,"Sto. DDL
Online DL"
REFERENCES,0.5324427480916031,"Figure E: Comparison between Online DL and Stochastic DDL. Stochastic DDL is more efﬁcient
for smaller values of λ, due to the fact that sparse coding is slower in this case."
REFERENCES,0.5343511450381679,"C
PROOFS OF THEORETICAL RESULTS"
REFERENCES,0.5362595419847328,This section gives the proofs for the various theoretical results in the paper.
REFERENCES,0.5381679389312977,"C.1
PROOF OF PROPOSITION 2.1."
REFERENCES,0.5400763358778626,"Proposition 2.1 Let D∗= arg minD∈C G(D) and D∗
N = arg minD∈C GN(D), where N is the
number of unrolled iterations. We denote by K(D∗) a constant depending on D∗, and by C(N) the
convergence speed of the algorithm, which approximates the inner problem solution. We have"
REFERENCES,0.5419847328244275,"GN(D∗
N) −G(D∗) ≤K(D∗)C(N) ."
REFERENCES,0.5438931297709924,"Let
G(D)
≜
F(Z∗(D), D)
and
GN(D)
≜
F(ZN(D), D)
where
Z∗(D)
=
arg minZ∈Rn×T F(Z, D) and ZN(D) = FISTA(D, N). Let D∗= arg minD∈C G(D) and
D∗
N = arg minD∈C GN(D). We have"
REFERENCES,0.5458015267175572,"GN(D∗
N) −G(D∗) = GN(D∗
N) −GN(D∗) + GN(D∗) −G(D∗)
(7)
= F(ZN(DN), DN) −F(ZN(D∗), D∗)
(8)
+ F(ZN(D∗), D∗) −F(Z(D∗), D∗)
(9)"
REFERENCES,0.5477099236641222,Published as a conference paper at ICLR 2022
REFERENCES,0.549618320610687,"By deﬁnition of D∗
N
F(ZN(D∗
N), D∗
N) −F(ZN(D∗), D∗) ≤0
(10)"
REFERENCES,0.5515267175572519,The convergence rate of FISTA in function value for a ﬁxed dictionary D is
REFERENCES,0.5534351145038168,"F(ZN(D), D) −F(ZN(D), D) ≤K(D)"
REFERENCES,0.5553435114503816,"N 2
(11)"
REFERENCES,0.5572519083969466,Therefore
REFERENCES,0.5591603053435115,"F(ZN(D∗), D∗) −F(Z(D∗), D∗) ≤K(D∗)"
REFERENCES,0.5610687022900763,"N 2
(12) Hence"
REFERENCES,0.5629770992366412,"GN(D∗
N) −G(D∗) ≤K(D∗)"
REFERENCES,0.5648854961832062,"N 2
(13)"
REFERENCES,0.566793893129771,"C.2
PROOF OF PROPOSITION 2.2"
REFERENCES,0.5687022900763359,"Proposition 2.2 Let D ∈Rm×n. Then, there exists a constant L1 > 0 such that for every number
of iterations N
g1
N −g∗ ≤L1 ∥zN(D) −z∗(D)∥."
REFERENCES,0.5706106870229007,We have
REFERENCES,0.5725190839694656,"F(z, D) = 1"
REFERENCES,0.5744274809160306,"2 ∥Dz −y∥2
2 + λ ∥z∥1
(14)"
REFERENCES,0.5763358778625954,"∇2F(z, D) = (Dz −y)z⊤
(15)"
REFERENCES,0.5782442748091603,"z0(D) = 0 and the iterates (zN(D))N∈N converge towards z∗(D). Hence, they are contained in
a closed ball around z∗(D). As ∇2F(·, D) is continuously differentiable, it is locally Lipschitz on
this closed ball, and there exists a constant L1(D) depending on D such that
g1
N −g∗ = ∥∇2F(zN(D), D) −∇2F(z∗(D), D)∥
(16)"
REFERENCES,0.5801526717557252,"≤L1(D) ∥zN(D) −z∗(D)∥
(17)"
REFERENCES,0.5820610687022901,"C.3
PROOF OF PROPOSITION 2.3."
REFERENCES,0.583969465648855,"Proposition 2.3 Let D ∈Rm×n. Let S∗be the support of z∗(D), SN be the support of zN and
eSN = SN ∪S∗. Let f(z, D) =
1
2 ∥Dz −y∥2
2 be the data-ﬁtting term in F. Let R(J, eS) =
J+ 
∇2
1,1f(z∗, D) ⊙1eS

+ ∇2
2,1f(z∗, D) ⊙1eS. Then there exists a constant L2 > 0 and a sub-
sequence of (F)ISTA iterates zφ(N) such that for all N ∈N:"
REFERENCES,0.5858778625954199,"∃g2
φ(N) ∈∇2f(zφ(N), D) + J+
φ(N)

∇1f(zφ(N), D) + λ∂∥·∥1(zφ(N))

s.t. :
g2
φ(N) −g∗ ≤
R(Jφ(N), eSφ(N))

zφ(N) −z∗ + L2 2"
REFERENCES,0.5877862595419847,"zφ(N) −z∗2 ."
REFERENCES,0.5896946564885496,This sub-sequence zφ(N) corresponds to iterates on the support of z∗.
REFERENCES,0.5916030534351145,"We have
g2
N(D) ∈∇2f(zN(D), D) + J+
N
 
∇1f(zN(D), D) + λ∂∥·∥1(zN)

(18)"
REFERENCES,0.5935114503816794,We adapt equation (6) in Ablin et al. (2020)
REFERENCES,0.5954198473282443,"g2
N = g∗+ R(JN, f
SN)(zN −z∗) + RD,z
N
+ J+
NRz,z
N
(19) where"
REFERENCES,0.5973282442748091,"R(J, eS) = J+ 
∇2
1,1f(z∗, D) ⊙1eS

+ ∇2
2,1f(z∗, D) ⊙1eS
(20)"
REFERENCES,0.5992366412213741,"RD,z
N
= ∇2f(zN, D) −∇2f(z∗, D) −∇2
2,1f(z∗, D)(zN −z∗)
(21)"
REFERENCES,0.601145038167939,"Rz,z
N
∈∇1f(zN, D) + λ∂∥·∥1(zN) −∇2
1,1f(z∗, D)(zN −z∗)
(22)"
REFERENCES,0.6030534351145038,Published as a conference paper at ICLR 2022
REFERENCES,0.6049618320610687,"As zN and z∗are on f
SN"
REFERENCES,0.6068702290076335,"∇2
2,1f(z∗, D)(zN −z∗) =

∇2
2,1f(z∗, D) ⊙1 f
SN"
REFERENCES,0.6087786259541985,"
(zN −z∗)
(23)"
REFERENCES,0.6106870229007634,"J+ 
∇2
1,1f(z∗, D)(zN −z∗)

= J+
∇2
1,1f(z∗, D) ⊙1 f
SN (zN −z∗)

(24)"
REFERENCES,0.6125954198473282,"As stated in Proposition 2.2, ∇2f(·, D) is locally Lipschitz, and RD,z
N
is the Taylor rest of
∇2f(·, D). Therefore, there exists a constant LD,z such that"
REFERENCES,0.6145038167938931,"∀N ∈N,
RD,z
N
 ≤LD,z"
REFERENCES,0.6164122137404581,"2
∥zN(D) −z∗(D)∥2
(25)"
REFERENCES,0.6183206106870229,"We know that 0 ∈∇1f(z∗, D)+λ∂∥·∥1(z∗). In other words, ∃u∗∈λ∂∥·∥1(z∗) s.t. ∇1f(z∗, D)+
u∗= 0. Therefore we have:"
REFERENCES,0.6202290076335878,"Rz,z
N
∈∇1f(zN, D) −∇1f(z∗, D) −∇2
1,1f(z∗, x)(zN −z∗) + λ∂∥zN∥1 −u∗
(26)"
REFERENCES,0.6221374045801527,"Let Lz,z be the Lipschitz constant of ∇1f(·, D). (F)ISTA outputs a sequence such that there
exists a sub-sequence (zφ(N))N∈N which has the same support as z∗.
For this sub-sequence,
u∗∈λ∂∥·∥1(zφ(N)). Therefore, there exists Rz,z
φ(N) such that"
REFERENCES,0.6240458015267175,"1. Rz,z
φ(N) ∈∇1f(zφ(N), D) + λ∂∥·∥1(zφ(N)) −∇2
1,1f(z∗, x)(zφ(N) −z∗) 2."
REFERENCES,0.6259541984732825,"Rz,z
φ(N)
 ≤Lz,z"
REFERENCES,0.6278625954198473,"2
zφ(N) −z∗2"
REFERENCES,0.6297709923664122,"For this sub-sequence, we can adapt Proposition 2 from Ablin et al. (2020). Let L2 = LD,z + Lz,z,
we have"
REFERENCES,0.6316793893129771,"∃g2
φ(N) ∈∇2f(zφ(N), D) + Jφ(N)
 
∇1f(zφ(N), D) + λ∂
zφ(N)

1

, s.t. :
(27)
g2
φ(N) −g∗ ≤
R(Jφ(N), ^
Sφ(N))

zφ(N) −z∗ + L2 2"
REFERENCES,0.6335877862595419,"zφ(N) −z∗2
(28)"
REFERENCES,0.6354961832061069,"C.4
PROOF OF THEOREM 2.4."
REFERENCES,0.6374045801526718,"Theorem 2.4 At iteration N + 1 of ISTA, the weak Jacobian of zN+1 relatively to Dl, where Dl is
the l-th row of D, is given by induction:"
REFERENCES,0.6393129770992366,∂(zN+1)
REFERENCES,0.6412213740458015,"∂Dl
= 1|zN+1|>0 ⊙
∂(zN)"
REFERENCES,0.6431297709923665,"∂Dl
−1 L"
REFERENCES,0.6450381679389313,"
Dlz⊤
N + (D⊤
l zN −yl)In + D⊤D ∂(zN) ∂Dl 
. ∂(zN)"
REFERENCES,0.6469465648854962,"∂Dl
will be denoted by JN
l . It converges towards the weak Jacobian J∗
l of z∗relatively to Dl,
whose values are"
REFERENCES,0.648854961832061,"J∗
l S∗= −(D⊤
:,S∗D:,S∗)−1(Dlz∗⊤+ (D⊤
l z∗−yl)In)S∗,"
REFERENCES,0.6507633587786259,"on the support S∗of z∗, and 0 elsewhere. Moreover, R(J∗, S∗) = 0."
REFERENCES,0.6526717557251909,We start by recalling a Lemma from Deledalle et al. (2014).
REFERENCES,0.6545801526717557,"Lemma C.1 The soft-thresholding STµ deﬁned by STµ(z) = sgn(z) ⊙(|z| −µ)+ is weakly dif-
ferentiable with weak derivative dSTµ(z)"
REFERENCES,0.6564885496183206,"dz
= 1|z|>µ."
REFERENCES,0.6583969465648855,"Coordinate-wise, ISTA corresponds to the following equality:"
REFERENCES,0.6603053435114504,zN+1 = STµ((I −1
REFERENCES,0.6622137404580153,LD⊤D)zN + 1
REFERENCES,0.6641221374045801,"LD⊤y)
(29)"
REFERENCES,0.666030534351145,"(zN+1)i = STµ((zN)i −1 L m
X p=1
( n
X"
REFERENCES,0.6679389312977099,"j=1
DjiDjp)(zN)p + 1 L n
X"
REFERENCES,0.6698473282442748,"j=1
Djiyj)
(30)"
REFERENCES,0.6717557251908397,Published as a conference paper at ICLR 2022
REFERENCES,0.6736641221374046,The Jacobian is computed coordinate wise with the chain rule:
REFERENCES,0.6755725190839694,∂(zN+1)i
REFERENCES,0.6774809160305344,"∂Dlk
= 1|(zN+1)i|>0 · (∂(zN)i"
REFERENCES,0.6793893129770993,"∂Dlk
−1"
REFERENCES,0.6812977099236641,"L
∂
∂Dlk
( m
X p=1
( n
X"
REFERENCES,0.683206106870229,"j=1
DjiDjp)(zN)p) + 1"
REFERENCES,0.6851145038167938,"L
∂
∂Dlk n
X"
REFERENCES,0.6870229007633588,"j=1
Djiyj))"
REFERENCES,0.6889312977099237,"(31)
Last term:
∂
∂Dlk n
X"
REFERENCES,0.6908396946564885,"j=1
Djiyj = δikyl
(32)"
REFERENCES,0.6927480916030534,Second term:
REFERENCES,0.6946564885496184,"∂
∂Dlk m
X p=1 n
X"
REFERENCES,0.6965648854961832,"j=1
DjiDjp(zN)p = m
X p=1 n
X"
REFERENCES,0.6984732824427481,"j=1
DjiDjp
∂(zN)p"
REFERENCES,0.700381679389313,"∂Dlk
+ m
X p=1 n
X j=1"
REFERENCES,0.7022900763358778,∂DjiDjp
REFERENCES,0.7041984732824428,"∂Dlk
(zN)p
(33)"
REFERENCES,0.7061068702290076,∂DjiDjp
REFERENCES,0.7080152671755725,"∂Dlk
= 

 
"
DLK,0.7099236641221374,"2Dlk
if j = l and i = p = k
Dlp
if j = l and i = k and p ̸= k
Dli
if j = l and i ̸= k and p = k
0
else (34)"
DLK,0.7118320610687023,"Therefore: m
X p=1 n
X j=1"
DLK,0.7137404580152672,∂DjiDjp
DLK,0.7156488549618321,"∂Dlk
(zN)p = m
X"
DLK,0.7175572519083969,"p=1
(2Dlkδipδik + Dliδpk1i̸=k + Dlpδik1k̸=p)(zN)p
(35)"
DLK,0.7194656488549618,"= 2Dlk(zN)kδik + Dli(zN)k1i̸=k + m
X"
DLK,0.7213740458015268,"p=1
p̸=k"
DLK,0.7232824427480916,"Dlp(zN)pδik
(36)"
DLK,0.7251908396946565,"= Dli(zN)k + δik m
X"
DLK,0.7270992366412213,"p=1
Dlp(zN)p
(37)"
DLK,0.7290076335877863,Hence:
DLK,0.7309160305343512,∂(zN+1)i
DLK,0.732824427480916,"∂Dlk
= 1|(zN+1)i|>0 ·
∂(zN)i"
DLK,0.7347328244274809,"∂Dlk
−1"
DLK,0.7366412213740458,"L(Dli(zN)k+
(38) δik( m
X"
DLK,0.7385496183206107,"p=1
Dlp(zN)p) + m
X p=1 n
X j=1"
DLK,0.7404580152671756,∂(zN)p
DLK,0.7423664122137404,"∂Dlk
DjiDjp −δikyl)
"
DLK,0.7442748091603053,This leads to the following vector formulation:
DLK,0.7461832061068703,∂(zN+1)
DLK,0.7480916030534351,"∂Dl
= 1|zN+1|>0 ⊙
∂(zN)"
DLK,0.75,"∂Dl
−1 L"
DLK,0.7519083969465649,"
Dlz⊤
N + (D⊤
l zN −yl)Im + D⊤D ∂(zN) ∂Dl"
DLK,0.7538167938931297,"
(39)"
DLK,0.7557251908396947,"On the support of z∗, denoted by S∗, this quantity converges towards the ﬁxed point:"
DLK,0.7576335877862596,"J∗
l = −(D⊤
:,S∗D:,S∗)−1(Dlz∗⊤+ (D⊤
l z∗−yl)Im)S∗
(40)"
DLK,0.7595419847328244,"Elsewhere, J∗
l is equal to 0. To prove that R(J∗, S∗) = 0, we use the expression given by equa-
tion 39"
DLK,0.7614503816793893,"J∗= 1S∗⊙

J∗−1"
DLK,0.7633587786259542,"L
 
∇2
2,1f(z∗, Dl)⊤+ ∇2
1,1f(z∗, D)⊤J∗
(41)"
DLK,0.7652671755725191,J∗−1S∗⊙J∗= 1
DLK,0.767175572519084,"L1S∗⊙∇2
2,1f(z∗, Dl)⊤+ 1S∗⊙∇2
1,1f(z∗, D)⊤J∗
(42)"
DLK,0.7690839694656488,"0 = J∗+ 
∇2
1,1f(z∗, D) ⊙1S∗
+ ∇2
2,1f(z∗, D) ⊙1S∗
(43)"
DLK,0.7709923664122137,"0 = R(J∗, S∗)
(44)"
DLK,0.7729007633587787,Published as a conference paper at ICLR 2022
DLK,0.7748091603053435,"C.5
PROOF OF PROPOSITION 2.5 AND COROLLARY 2.6"
DLK,0.7767175572519084,"Proposition 2.5 Let N be the number of iterations and K be the back-propagation depth. We as-
sume that ∀n ≥N −K, S∗⊂Sn. Let ¯EN = Sn\S∗, let L be the largest eigenvalue of D⊤
:,S∗D:,S∗,"
DLK,0.7786259541984732,"and let µn be the smallest eigenvalue of D⊤
:,SnD:,Sn−1. Let Bn =
PEn −D⊤
:, ¯
EnD†⊤
:,S∗PS∗
, where"
DLK,0.7805343511450382,PS is the projection on RS and D† is the pseudo-inverse of D. We have
DLK,0.7824427480916031,"JN
l
−J∗
l
 ≤ K
Y k=1"
DLK,0.7843511450381679,"
1 −µN−k L"
DLK,0.7862595419847328,"
∥J∗
l ∥+ 2"
DLK,0.7881679389312977,"L ∥Dl∥ K−1
X k=0 k
Y"
DLK,0.7900763358778626,"i=1
(1−µN−i"
DLK,0.7919847328244275,"L
)
 zN−k
l
−z∗
l
+BN−k ∥z∗
l ∥

."
DLK,0.7938931297709924,We denote by G the matrix (I −1
DLK,0.7958015267175572,"LD⊤D). For zN with support SN and z∗with support S∗, we
have with the induction in Theorem 2.4"
DLK,0.7977099236641222,"JN
l,SN =
 
GJN−1
l
+ uN−1
l
"
DLK,0.799618320610687,"SN
(45)"
DLK,0.8015267175572519,"J∗
l,S∗=
 
GJ∗
l + u∗
l
"
DLK,0.8034351145038168,"S∗
(46)"
DLK,0.8053435114503816,"where uN
l = −1"
DLK,0.8072519083969466,"L
 
Dlz⊤
N + (D⊤
l zN −yl)I

and the other terms on ¯SN and ¯S∗are 0.
We can thus decompose their difference as the sum of two terms, one on the support S∗and one on
this complement EN = SN \ S∗"
DLK,0.8091603053435115,"J∗
l −JN
l
= (J∗
l −JN
l )S∗+ (J∗
l −JN
l )EN ."
DLK,0.8110687022900763,"Recall that we assume S∗⊂SN. Let’s study the terms separately on S∗and EN = SN \ S∗. These
two terms can be decompose again to constitute a double recursion system,"
DLK,0.8129770992366412,"(JN
l
−J∗
l )S∗= GS∗(JN−1
l
−J∗
l ) + (uN−1
l
−u∗
l )S∗
(47)"
DLK,0.8148854961832062,"= GS∗,S∗(JN−1
l
−J∗
l )S∗+ GS∗,EN−1(JN−1
l
−J∗)EN−1 + (uN−1
l
−u∗
l )S∗,
(48)"
DLK,0.816793893129771,"(JN
l
−J∗
l )EN = (JN
l )EN = GEN (JN−1
l
−J∗
l ) + GEN,S∗J∗
l + (uN−1
l
)EN
(49)"
DLK,0.8187022900763359,"= GEN,S∗(JN−1
l
−J∗
l )S∗+ GEN,EN−1(JN−1
l
−J∗
l )EN−1
(50)"
DLK,0.8206106870229007,"+ (uN−1
l
−u∗
l )EN +

(u∗
l )EN −D⊤
:,EN D:,S∗(D⊤
:,S∗D:,S∗)−1(u∗
l )S∗

."
DLK,0.8225190839694656,"We deﬁne as PSN,EN the operator which projects a vector from EN on (SN, EN) with zeros on
SN. As S∗∪EN = SN, we get by combining these two expressions,"
DLK,0.8244274809160306,"(JN
l
−J∗
l )SN =GSN,SN−1(JN−1
l
−J∗
l )SN−1 + (uN−1
l
−u∗
l )SN
(51)"
DLK,0.8263358778625954,"+ PSN,EN"
DLK,0.8282442748091603,"
(u∗
l )EN −D⊤
:,EN D:,S∗(D⊤
:,S∗D:,S∗)−1(u∗
l )S∗
"
DLK,0.8301526717557252,"Taking the norm yields to the following inequality,
JN
l
−J∗
l
 ≤
GSN,SN−1
 JN−1
l
−J∗
l
 +
uN−1
l
−u∗
l

(52)"
DLK,0.8320610687022901,"+
(u∗
l )EN −D⊤
:,EN D:,S∗(D⊤
:,S∗D:,S∗)−1(u∗
l )S∗
 ."
DLK,0.833969465648855,"Denoting by µN the smallest eigenvalue of D⊤
:,SN D:,SN−1, then
GSN,SN−1
 = (1 −µN"
DLK,0.8358778625954199,"L ) and we
get that"
DLK,0.8377862595419847,"JN
l
−J∗
l
 ≤ K
Y"
DLK,0.8396946564885496,"k=1
(1 −µN−k"
DLK,0.8416030534351145,"L
)
JN−K
l
−J∗
l

(53) + K−1
X k=0 k
Y"
DLK,0.8435114503816794,"i=1
(1 −µN−i"
DLK,0.8454198473282443,"L
)
 uN−k
l
−u∗
l
 +
(u∗
l )EN−k −D⊤
:,EN−kD†⊤
:,S∗(u∗
l )S∗


."
DLK,0.8473282442748091,Published as a conference paper at ICLR 2022
DLK,0.8492366412213741,"The back-propagation is initialized as JN−K
l
= 0. Therefore
JN−K
l
−J∗
l
 = ∥J∗
l ∥. More-"
DLK,0.851145038167939,"over
uN−k
l
−u∗
l
 ≤
2
L ∥Dl∥
zN−k
l
−z∗
l
. Finally,
(u∗
l )EN−k −D⊤
:,EN−kD†⊤
:,S∗(u∗
l )S∗
 can
be rewritten with projection matrices PEN−k and P ¯
S∗to obtain
(u∗
l )EN−k −D⊤
:,EN−kD†⊤
:,S∗(u∗
l )S∗
 ≤
PEN−ku∗
l −D⊤
:,EN−kD†⊤
:,S∗PS∗u∗
l

(54)"
DLK,0.8530534351145038,"≤
PEN−k −D⊤
:,EN−kD†⊤
:,S∗PS∗
 ∥u∗
l ∥
(55)"
DLK,0.8549618320610687,"≤
PEN−k −D⊤
:,EN−kD†⊤
:,S∗PS∗
 2"
DLK,0.8568702290076335,"L ∥Dl∥∥z∗
l ∥.
(56)"
DLK,0.8587786259541985,"Let BN−k =
PEN−k −D⊤
:,EN−kD†⊤
:,S∗PS∗
. We have"
DLK,0.8606870229007634,"JN
l
−J∗
l
 ≤ K
Y"
DLK,0.8625954198473282,"k=1
(1−µN−k"
DLK,0.8645038167938931,"L
) ∥J∗
l ∥+ 2"
DLK,0.8664122137404581,"L ∥Dl∥ K−1
X k=0 k
Y"
DLK,0.8683206106870229,"i=1
(1−µN−i"
DLK,0.8702290076335878,"L
)
 zN−k
l
−z∗
l
+BN−k ∥z∗
l ∥

."
DLK,0.8721374045801527,"(57)
We now suppose that the support is reached at iteration N −s, with s ≥K. Therefore, ∀n ∈
[N −s, N] Sn = S∗. Let ∆n = F(zn, D) −F(z∗, D) + L"
DLK,0.8740458015267175,"2 ∥zn −z∗∥. On the support, F is a
µ-strongly convex function and the convergence rate of (zN) is"
DLK,0.8759541984732825,"∥z∗−zN∥≤
 
1 −µ"
DLK,0.8778625954198473,"L
s 2∆N−s"
DLK,0.8797709923664122,"L
(58)"
DLK,0.8816793893129771,"Thus, we obtain"
DLK,0.8835877862595419,"JN
l
−J∗
l
 ≤ K
Y"
DLK,0.8854961832061069,"k=1
(1 −µN−k"
DLK,0.8874045801526718,"L
) ∥J∗
l ∥
(59) + 2"
DLK,0.8893129770992366,"L ∥Dl∥ K−1
X k=0 k
Y"
DLK,0.8912213740458015,"i=1
(1 −µN−i"
DLK,0.8931297709923665,"L
)
 zN−k
l
−z∗
l
 + BN−k ∥u∗
l ∥
 ≤ K
Y"
DLK,0.8950381679389313,"k=1
(1 −µN−k"
DLK,0.8969465648854962,"L
) ∥J∗
l ∥
(60) + 2"
DLK,0.898854961832061,"L ∥Dl∥ s−1
X"
DLK,0.9007633587786259,"k=0
(1 −µ"
DLK,0.9026717557251909,"L)k zN−k
l
−z∗
l

 + 2"
DLK,0.9045801526717557,L ∥Dl∥(1 −µ
DLK,0.9064885496183206,"L)s
K−1
X k=s−1 k
Y"
DLK,0.9083969465648855,"i=s−1
(1 −µN−i"
DLK,0.9103053435114504,"L
)
 zN−k
l
−z∗
l
 + BN−k ∥(u∗
l )∥
 ≤ K
Y"
DLK,0.9122137404580153,"k=1
(1 −µN−k"
DLK,0.9141221374045801,"L
) ∥J∗
l ∥
(61) + 2"
DLK,0.916030534351145,"L ∥Dl∥ s−1
X"
DLK,0.9179389312977099,"k=0
(1 −µ"
DLK,0.9198473282442748,"L)k 
1 −µ"
DLK,0.9217557251908397,"L
s−1−k 2∆N−s L + 2"
DLK,0.9236641221374046,L ∥Dl∥(1 −µ
DLK,0.9255725190839694,"L)s
K−1
X k=s−1 k
Y"
DLK,0.9274809160305344,"i=s−1
(1 −µN−i"
DLK,0.9293893129770993,"L
)
 zN−k
l
−z∗
l
 + BN−k ∥(u∗
l )∥
 ≤ K
Y"
DLK,0.9312977099236641,"k=1
(1 −µN−k"
DLK,0.933206106870229,"L
) ∥J∗
l ∥
(62)"
DLK,0.9351145038167938,+ ∥Dl∥(1 −µ
DLK,0.9370229007633588,L)s−1s4∆N−s L2 + 2
DLK,0.9389312977099237,L ∥Dl∥(1 −µ
DLK,0.9408396946564885,"L)s
K−1
X k=s−1 k
Y"
DLK,0.9427480916030534,"i=s−1
(1 −µN−i"
DLK,0.9446564885496184,"L
)
 zN−k
l
−z∗
l
 + BN−k ∥(u∗
l )∥
 (63)"
DLK,0.9465648854961832,Published as a conference paper at ICLR 2022
DLK,0.9484732824427481,"Corollary 2.6 Let µ > 0 be the smallest eigenvalue of D⊤
:,S∗D:,S∗. Let K ≤N be the back-
propagation depth and let ∆N = F(zN, D) −F(z∗, D) + L"
DLK,0.950381679389313,"2 ∥zN −z∗∥. Suppose that ∀n ∈
[N −K, N]; Sn ⊂S∗. Then, we have"
DLK,0.9522900763358778,"J∗
l −JN
l
 ≤

1 −µ L"
DLK,0.9541984732824428,"K
∥J∗
l ∥+ K

1 −µ L"
DLK,0.9561068702290076,"K−1
∥Dl∥4∆N−K L2
."
DLK,0.9580152671755725,The term 2
DLK,0.9599236641221374,L ∥Dl∥(1−µ
DLK,0.9618320610687023,"L)s PK−1
k=s−1
Qk
i=s−1(1−µN−i"
DLK,0.9637404580152672,"L
)
 zN−k
l
−z∗
l
+BN−k ∥(u∗
l )∥

vanishes
when the algorithm is initialized on the support. Otherwise, it goes to 0 as s, K →N and N →∞
because ∀n > N −s, µn = µ < 1."
DLK,0.9656488549618321,"D
ITERATIVE ALGORITHMS FOR SPARSE CODING RESOLUTION."
DLK,0.9675572519083969,"ISTA.
Algorithm to solve minz 1"
DLK,0.9694656488549618,"2 ∥y −Dz∥2
2 + λ ∥z∥1"
DLK,0.9713740458015268,Algorithm 1 ISTA
DLK,0.9732824427480916,"y, D, λ, N
z0 = 0, n = 0
Compute the Lipschitz constant L of D⊤D
while n < N do"
DLK,0.9751908396946565,un+1 ←zN −1
DLK,0.9770992366412213,"LD⊤(Dzn −y)
zn+1 ←ST λ"
DLK,0.9790076335877863,"L (un+1)
n ←n + 1
end while"
DLK,0.9809160305343512,"FISTA.
Algorithm to solve minz 1"
DLK,0.982824427480916,"2 ∥y −Dz∥2
2 + λ ∥z∥1"
DLK,0.9847328244274809,Algorithm 2 FISTA
DLK,0.9866412213740458,"y, D, λ, N
z0 = x0 = 0, n = 0, t0 = 1
Compute the Lipschitz constant L of D⊤D
while n < N do"
DLK,0.9885496183206107,un+1 ←zn −1
DLK,0.9904580152671756,"LD⊤(Dzn −y)
xn+1 ←ST λ"
DLK,0.9923664122137404,L (un+1)
DLK,0.9942748091603053,"tn+1 ←
1+√"
DLK,0.9961832061068703,"1+4t2n
2
zn+1 ←xn+1 + tn−1"
DLK,0.9980916030534351,"tn+1 (xn+1 −xn)
n ←n + 1
end while"
