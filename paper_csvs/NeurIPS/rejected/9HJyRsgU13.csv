Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00205761316872428,"Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key sur-
1"
ABSTRACT,0.00411522633744856,"rogate model, mostly with a simple stationary and separable kernel function such as
2"
ABSTRACT,0.006172839506172839,"the squared-exponential kernel with automatic relevance determination (SE-ARD).
3"
ABSTRACT,0.00823045267489712,"However, such simple kernel specifications are deficient in learning functions with
4"
ABSTRACT,0.0102880658436214,"complex features, such as being nonstationary, nonseparable, and multimodal.
5"
ABSTRACT,0.012345679012345678,"Approximating such functions using a local GP, even in a low-dimensional space,
6"
ABSTRACT,0.01440329218106996,"requires a large number of samples, not to mention in a high-dimensional set-
7"
ABSTRACT,0.01646090534979424,"ting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization
8"
ABSTRACT,0.018518518518518517,"(BKTF)—as a new surrogate model—for BO in a D-dimensional Cartesian product
9"
ABSTRACT,0.0205761316872428,"space. Our key idea is to approximate the underlying D-dimensional solid with a
10"
ABSTRACT,0.02263374485596708,"fully Bayesian low-rank tensor CP decomposition, in which we place GP priors
11"
ABSTRACT,0.024691358024691357,"on the latent basis functions for each dimension to encode local consistency and
12"
ABSTRACT,0.026748971193415638,"smoothness. With this formulation, information from each sample can be shared
13"
ABSTRACT,0.02880658436213992,"not only with neighbors but also across dimensions. Although BKTF no longer
14"
ABSTRACT,0.030864197530864196,"has an analytical posterior, we can still efficiently approximate the posterior dis-
15"
ABSTRACT,0.03292181069958848,"tribution through Markov chain Monte Carlo (MCMC) and obtain prediction and
16"
ABSTRACT,0.03497942386831276,"full uncertainty quantification (UQ). We conduct numerical experiments on both
17"
ABSTRACT,0.037037037037037035,"standard BO test functions and machine learning hyperparameter tuning problems,
18"
ABSTRACT,0.03909465020576132,"and our results show that BKTF offers a flexible and highly effective approach for
19"
ABSTRACT,0.0411522633744856,"characterizing complex functions with UQ, especially in cases where the initial
20"
ABSTRACT,0.043209876543209874,"sample size and budget are severely limited.
21"
INTRODUCTION,0.04526748971193416,"1
Introduction
22"
INTRODUCTION,0.047325102880658436,"For many applications in sciences and engineering, such as emulation-based studies, design of
23"
INTRODUCTION,0.04938271604938271,"experiments, and automated machine learning, the goal is to optimize a complex black-box function
24"
INTRODUCTION,0.051440329218107,"f(x) in a D-dimensional space, for which we have limited prior knowledge. The main challenge in
25"
INTRODUCTION,0.053497942386831275,"such optimization problems is that we aim to efficiently find global optima rather than local optima,
26"
INTRODUCTION,0.05555555555555555,"while the objective function f is often gradient-free, multimodal, and computationally expensive
27"
INTRODUCTION,0.05761316872427984,"to evaluate. Bayesian optimization (BO) offers a powerful statistical approach to these problems,
28"
INTRODUCTION,0.059670781893004114,"particularly when the observation budgets are limited [1, 2, 3]. A typical BO framework consists of
29"
INTRODUCTION,0.06172839506172839,"two components to balance exploitation and exploration: the surrogate and the acquisition function
30"
INTRODUCTION,0.06378600823045268,"(AF). The surrogate is a probabilistic model that allows us to estimate f(x) with uncertainty at a new
31"
INTRODUCTION,0.06584362139917696,"location x, and the AF is used to determine which location to query next.
32"
INTRODUCTION,0.06790123456790123,"Gaussian process (GP) regression is the most widely used surrogate for BO [3, 4], thanks to its
33"
INTRODUCTION,0.06995884773662552,"appealing properties in providing analytical derivations and uncertainty quantification (UQ). The
34"
INTRODUCTION,0.0720164609053498,"choice of kernel/covariance function is a critical decision in GP models; for multidimensional
35"
INTRODUCTION,0.07407407407407407,"BO problems, perhaps the most popular kernel is the ARD (automatic relevance determination)—
36"
INTRODUCTION,0.07613168724279835,"Squared-Exponential (SE) or Matérn kernel [4]. Although this specification has certain numerical
37"
INTRODUCTION,0.07818930041152264,"advantages and can help automatically learn the importance of input variables, a key limitation is that
38"
INTRODUCTION,0.08024691358024691,"it implies/assumes that the underlying stochastic process is both stationary and separable, and the
39"
INTRODUCTION,0.0823045267489712,"value of the covariance function between two random points quickly goes to zero with the increase of
40"
INTRODUCTION,0.08436213991769548,"input dimensionality. These assumptions can be problematic for complex real-world processes that
41"
INTRODUCTION,0.08641975308641975,"are nonstationary and nonseparable, as estimating the underlying function with a simple ARD kernel
42"
INTRODUCTION,0.08847736625514403,"would require a large number of observations. A potential solution to address this issue is to use more
43"
INTRODUCTION,0.09053497942386832,"flexible kernel structures. The additive kernel, for example, is designed to characterize a more “global”
44"
INTRODUCTION,0.09259259259259259,"and nonstationary structure by restricting variable interactions [5], and it has demonstrated great
45"
INTRODUCTION,0.09465020576131687,"success in solving high-dimensional BO problems (see, e.g., [6, 7, 8]). However, in practice using
46"
INTRODUCTION,0.09670781893004116,"additive kernels requires strong prior knowledge to determine the proper interactions and involves
47"
INTRODUCTION,0.09876543209876543,"many kernel hyperparameters to learn [9]. Another emerging solution is to use deep GP [10], such as
48"
INTRODUCTION,0.10082304526748971,"in [11]; however, for complex multidimensional functions, learning a deep GP model will require a
49"
INTRODUCTION,0.102880658436214,"large number of samples.
50"
INTRODUCTION,0.10493827160493827,"In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF) [12, 13, 14] as a
51"
INTRODUCTION,0.10699588477366255,"flexible and adaptive surrogate model for BO in a D-dimensional Cartesian product space. BKTF is
52"
INTRODUCTION,0.10905349794238683,"initially developed for modeling multidimensional spatiotemporal data with UQ, for tasks such as
53"
INTRODUCTION,0.1111111111111111,"spatiotemporal kriging/cokriging. This paper adapts BKTF to the BO setting, and our key idea is to
54"
INTRODUCTION,0.11316872427983539,"characterize the multivariate objective function f (x) = f (x1, . . . , xD) for a specific BO problem
55"
INTRODUCTION,0.11522633744855967,"using low-rank tensor CANDECOMP/PARAFAC (CP) factorization with random basis functions.
56"
INTRODUCTION,0.11728395061728394,"Unlike other basis-function models that rely on known/deterministic basis functions [15], BKTF uses
57"
INTRODUCTION,0.11934156378600823,"a hierarchical Bayesian framework to achieve high-quality UQ in a more flexible way—GP priors
58"
INTRODUCTION,0.12139917695473251,"are used to model the basis functions, and hyperpriors are used to model kernel hyperparameters in
59"
INTRODUCTION,0.12345679012345678,"particular for the lengthscale that characterizes the scale of variation.
60"
INTRODUCTION,0.12551440329218108,"Figure 1 shows the comparison between BKTF and GP surrogates when optimizing a 2D function that
61"
INTRODUCTION,0.12757201646090535,"is nonstationary, nonseparable, and multimodal. The details of this function and the BO experiments
62"
INTRODUCTION,0.12962962962962962,"are provided in Appendix 7.3, and related code is given in Supplementary material. For this case,
63"
INTRODUCTION,0.13168724279835392,"GP becomes ineffective in finding the global solution, while BKTF offers superior flexibility and
64"
INTRODUCTION,0.1337448559670782,"adaptability to characterize the multidimensional process from limited data. Different from GP-based
65"
INTRODUCTION,0.13580246913580246,"surrogate models, BKTF no longer has an analytical posterior; however, efficient inference and
66"
INTRODUCTION,0.13786008230452676,"acquisition can be achieved through Markov chain Monte Carlo (MCMC) in an element-wise learning
67"
INTRODUCTION,0.13991769547325103,"way, in which we update basis functions and kernel hyperparameters using Gibbs sampling and slice
68"
INTRODUCTION,0.1419753086419753,"sampling respectively [14]. For the optimization, we first use MCMC samples to approximate the
69"
INTRODUCTION,0.1440329218106996,"posterior distribution of the whole tensor and then naturally define the upper confidence bound (UCB)
70"
INTRODUCTION,0.14609053497942387,"as AF. This process is feasible for many real-world applications that can be studied in a discretized
71"
INTRODUCTION,0.14814814814814814,"tensor product space, such as experimental design and automatic machine learning (ML). We conduct
72"
INTRODUCTION,0.15020576131687244,"extensive experiments on both standard optimization and ML hyperparameter tuning tasks. Our
73"
INTRODUCTION,0.1522633744855967,"results show that BKTF achieves a fast global search for optimizing complex objective functions
74"
INTRODUCTION,0.15432098765432098,"under limited initial data and observation budgets.
75"
INTRODUCTION,0.15637860082304528,"Figure 1: BO for a 2D nonstationary nonseparable function: (a) True function surface, where the
global maximum is marked; (b) Comparison between BO models using GP surrogates (with two AFs)
and BKTF with 30 random initial observations, averaged over 20 replications; (c) Specific results of
one run, including the final mean surface for f, in which green dots denote the locations of selected
candidates, and the corresponding AF surface."
PRELIMINARIES,0.15843621399176955,"2
Preliminaries
76"
PRELIMINARIES,0.16049382716049382,"Throughout this paper, we use lowercase letters to denote scalars, e.g., x, boldface lowercase letters
77"
PRELIMINARIES,0.16255144032921812,"to denote vectors, e.g., x = (x1, . . . , xD)⊤∈RD, and boldface uppercase letters to denote matrices,
78"
PRELIMINARIES,0.1646090534979424,"e.g., X ∈RM×N. For a matrix X, we denote its determinant by det (X). We use IN to represent
79"
PRELIMINARIES,0.16666666666666666,"an identity matrix of size N. Given two matrices A ∈RM×N and B ∈RP ×Q, the Kronecker
80"
PRELIMINARIES,0.16872427983539096,product is defined as A ⊗B = 
PRELIMINARIES,0.17078189300411523,"
a1,1B
· · ·
a1,NB
...
...
...
aM,1B
· · ·
aM,NB "
PRELIMINARIES,0.1728395061728395,"∈RMP ×NQ. The outer product of two
81"
PRELIMINARIES,0.1748971193415638,"vectors a and b is denoted by a ◦b. The vectorization operation vec(X) stacks all column vectors
82"
PRELIMINARIES,0.17695473251028807,"in X as a single vector. Following the tensor notation in [16], we denote a third-order tensor by
83"
PRELIMINARIES,0.17901234567901234,"X ∈RM×N×P and its mode-k (k = 1, 2, 3) unfolding by X(k), which maps a tensor into a matrix.
84"
PRELIMINARIES,0.18106995884773663,"Higher-order tensors can be defined in a similar way.
85"
PRELIMINARIES,0.1831275720164609,"Let f : X = X1 × . . . × XD →R be a black-box function that could be nonconvex, derivative-free,
86"
PRELIMINARIES,0.18518518518518517,"and expensive to evaluate. BO aims to address the global optimization problem:
87"
PRELIMINARIES,0.18724279835390947,"x⋆= arg max
x∈X
f(x), f ⋆= max
x∈X f(x) = f(x⋆).
(1)"
PRELIMINARIES,0.18930041152263374,"BO solves this problem by first building a probabilistic model for f(x) (i.e., surrogate model) based
88"
PRELIMINARIES,0.19135802469135801,"on initial observations and then using the model to decide where in X to evaluate/query next. The
89"
PRELIMINARIES,0.1934156378600823,"overall goal of BO is to find the global optimum of the objective function through as few evaluations
90"
PRELIMINARIES,0.19547325102880658,"as possible. Most BO models rely on a GP prior for f(x) to achieve prediction and UQ:
91"
PRELIMINARIES,0.19753086419753085,"f(x) = f(x1, x2, . . . , xD) ∼GP (m (x) , k (x, x′)) , xd ∈Xd, d = 1, . . . , D,
(2)"
PRELIMINARIES,0.19958847736625515,"where k is a valid kernel/covariance function and m is a mean function that can be generally assumed
92"
PRELIMINARIES,0.20164609053497942,"to be 0. Given a finite set of observation points {xi}n
i=1 with xi =
 
xi
1, . . . , xi
D
⊤, the vector of
93"
PRELIMINARIES,0.2037037037037037,"function values f = (f(x1), . . . , f(xn))⊤has a multivariate Gaussian distribution f ∼N (0, K),
94"
PRELIMINARIES,0.205761316872428,"where K denotes the n × n covariance matrix. For a set of observed data D = {xi, yi}n
i=1 with
95"
PRELIMINARIES,0.20781893004115226,"i.i.d. Gaussian noise, i.e., yi = f(xi) + ϵi where ϵi ∼N(0, τ −1), GP gives an analytical posterior
96"
PRELIMINARIES,0.20987654320987653,"distribution of f(x) at an unobserved point x∗:
97"
PRELIMINARIES,0.21193415637860083,"f(x∗) | Dn ∼N

kx∗X
 
K + τ −1In
−1 y, kx∗x∗−kx∗X
 
K + τ −1In
−1 k⊤
x∗X

,
(3)"
PRELIMINARIES,0.2139917695473251,"where kx∗x∗, kx∗X ∈R1×n are variance of x∗, covariances between x∗and {xi}n
i=1, respectively,
98"
PRELIMINARIES,0.21604938271604937,"and y = (y1, . . . , yn)⊤.
99
Algorithm 1: Basic BO process
Input: Initial dataset D0 and a trained
surrogate model; total budget N.
for n = 1, . . . , N do"
PRELIMINARIES,0.21810699588477367,"Compute the posterior distribution of f
using all available data;
Find next evaluation point xn ∈RD by
optimizing the AF;
Augment data Dn = Dn−1 ∪{xn, yn},
update surrogate model."
PRELIMINARIES,0.22016460905349794,"Based on the posterior distributions of f, one can
100"
PRELIMINARIES,0.2222222222222222,"compute an AF, denoted by α : X →R, for a
101"
PRELIMINARIES,0.2242798353909465,"new candidate x∗and evaluate how promising x∗
102"
PRELIMINARIES,0.22633744855967078,"is. In BO, the next query point is often determined
103"
PRELIMINARIES,0.22839506172839505,"by maximizing a selected/predefined AF, i.e., xn+1 =
104"
PRELIMINARIES,0.23045267489711935,"arg maxx∈X α (x | Dn). Most AFs are built on the
105"
PRELIMINARIES,0.23251028806584362,"predictive mean and variance; for example, a com-
106"
PRELIMINARIES,0.2345679012345679,"monly used AF is the expected improvement (EI)
107"
PRELIMINARIES,0.2366255144032922,"[1]:
108"
PRELIMINARIES,0.23868312757201646,"αEI (x | Dn) = σ(x)φ
∆(x) σ(x)"
PRELIMINARIES,0.24074074074074073,"
+ |∆(x)| Φ
∆(x) σ(x)"
PRELIMINARIES,0.24279835390946503,"
,
(4)"
PRELIMINARIES,0.2448559670781893,"where ∆(x) = µ(x) −f ⋆
n is the expected difference between the proposed point x and the current
109"
PRELIMINARIES,0.24691358024691357,"best solution, f ⋆
n = maxx∈{xi}n
i=1 f(x) denotes the best function value obtained so far; µ(x) and
110"
PRELIMINARIES,0.24897119341563786,"σ(x) are the predictive mean and predictive standard deviation at x, respectively; and φ(·) and Φ(·)
111"
PRELIMINARIES,0.25102880658436216,"denote the probability density function (PDF) and the cumulative distribution function (CDF) of
112"
PRELIMINARIES,0.25308641975308643,"standard normal, respectively. Another widely applied AF for maximization problems is the upper
113"
PRELIMINARIES,0.2551440329218107,"confidence bound (UCB) [17]:
114"
PRELIMINARIES,0.257201646090535,"αUCB (x | Dn, β) = µ(x) + βσ(x),
(5)"
PRELIMINARIES,0.25925925925925924,"where β is a tunable parameter that balances exploration and exploitation. The general BO procedure
115"
PRELIMINARIES,0.2613168724279835,"can be summarized as Algorithm 1.
116"
BAYESIAN KERNELIZED TENSOR FACTORIZATION FOR BO,0.26337448559670784,"3
Bayesian Kernelized Tensor Factorization for BO
117"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.2654320987654321,"3.1
Bayesian Hierarchical Model Specification
118"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.2674897119341564,"Before introducing BKTF, we first construct a D-dimensional Cartesian product space corresponding
119"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.26954732510288065,"to the search space X. We define it over D sets {S1, . . . , SD} and denote as QD
d=1 Sd: S1 × · · · ×
120"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.2716049382716049,"SD = {(s1, . . . , sD) | ∀d ∈{1, . . . , D}, sd ∈Sd}. For ∀d ∈[1, D], the coordinates set Sd is formed
121"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.2736625514403292,"by md interpolation points that are distributed over a bounded interval Xd = [ad, bd], represented by
122"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.2757201646090535,"cd =

cd
1, . . . , cd
md
	
, i.e., Sd =

cd
i
	md
i=1. The size of Sd becomes |Sd| = md, and the entire space
123"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.2777777777777778,"owns QD
d=1 |Sd| samples. Note that Sd could be either uniformly or irregularly distributed.
124"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.27983539094650206,"We randomly sample an initial dataset including n0 input-output data pairs from the pre-defined
125"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.28189300411522633,"space, D0 = {xi, yi}n0
i=1 where {xi}n0
i=1 are located in QD
d=1 Sd, and this yields an incomplete
126"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.2839506172839506,"D-dimensional tensor Y ∈R|S1|×···×|SD| with n0 observed points. BKTF approximates the entire
127"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.28600823045267487,"data tensor Y by a kernelized CANDECOMP/PARAFAC (CP) tensor decomposition:
128 Y = R
X"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.2880658436213992,"r=1
λr · gr
1 ◦gr
2 ◦· · · ◦gr
D + E,
(6)"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.29012345679012347,"where R is a pre-specified tensor CP rank, λ = (λ1, . . . , λR)⊤denote weight coefficients that
129"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.29218106995884774,"capture the magnitude/importance of each rank in the factorization, gr
d = [gr
d(sd) : sd ∈Sd] ∈R|Sd|
130"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.294238683127572,"denotes the rth latent factor for the dth dimension, entries in E are i.i.d. white noises from N(0, τ −1).
131"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.2962962962962963,"It should be particularly noted that both the coefficients {λr}R
r=1 and the latent basis functions
132"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.29835390946502055,"{gr
1, . . . , gr
D}R
r=1 are random variables. The function approximation for x = (x1, . . . , xD)⊤can be
133"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.3004115226337449,"written as:
134"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.30246913580246915,"f(x) = R
X"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.3045267489711934,"r=1
λrgr
1 (x1) gr
2 (x2) · · · gr
D (xD) = R
X"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.3065843621399177,"r=1
λr D
Y"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.30864197530864196,"d=1
gr
d (xd) .
(7)"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.31069958847736623,"For priors, we assume λr ∼N (0, 1) for r = 1, . . . , R and use a GP prior on the latent factors:
135"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.31275720164609055,"gr
d (xd) | lr
d ∼GP (0, kr
d (xd, x′
d; lr
d)) , r = 1, . . . , R, d = 1, . . . , D,
(8)"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.3148148148148148,"where kr
d is a valid kernel function. We fix the variances of kr
d as σ2 = 1, and only learn the
136"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.3168724279835391,"length-scale hyperparameters lr
d, since the variances of the model can be captured by λ. One can
137"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.31893004115226337,"also exclude λ but introduce variance σ2 as a kernel hyperparameter on one of the basis functions;
138"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.32098765432098764,"however, learning kernel hyperparameter is computationally more expensive than learning λ. For
139"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.3230452674897119,"simplicity, we can also assume the lengthscale parameters to be identical, i.e., l1
d = l2
d = . . . =
140"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.32510288065843623,"lR
d = ld, for each dimension d. The prior for the corresponding latent factor gr
d is then a Gaussian
141"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.3271604938271605,"distribution: gr
d ∼N (0, Kr
d), where Kr
d is the |Sd| × |Sd| correlation matrix computed from kr
d.
142"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.3292181069958848,"We place Gaussian hyperpriors on the log-transformed kernel hyperparameters to ensure positive
143"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.33127572016460904,"values, i.e., log (lr
d) ∼N
 
µl, τ −1
l

. For noise precision τ, we assume a conjugate Gamma prior
144"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.3333333333333333,"τ ∼Gamma (a0, b0).
145"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.33539094650205764,"For observations, based on Eq. (7) we assume each yi in the initial dataset D0 to be:
146"
BAYESIAN HIERARCHICAL MODEL SPECIFICATION,0.3374485596707819,"yi
 {gr
d
 
xi
d

}, {λr}, τ ∼N
 
f (xi) , τ −1
.
(9)"
BKTF AS A TWO-LAYER DEEP GP,0.3395061728395062,"3.2
BKTF as a Two-layer Deep GP
147"
BKTF AS A TWO-LAYER DEEP GP,0.34156378600823045,"Here we show the representation of BKTF as a two-layer deep GP. The first layer characterizes the
148"
BKTF AS A TWO-LAYER DEEP GP,0.3436213991769547,"generation of latent functions {gr
d}R
r=1 for coordinate/dimension d and also the generation of random
149"
BKTF AS A TWO-LAYER DEEP GP,0.345679012345679,"weights {λr}R
r=1. For the second layer, if we consider {λr, gr
1, . . . , gr
D}R
r=1 as parameters and rewrite
150"
BKTF AS A TWO-LAYER DEEP GP,0.3477366255144033,"the functional decomposition in Eq. (7) as a linear function f (x; {ξr}) = PR
r=1 ξr|λr| QD
d=1 gr
d (xd)
151"
BKTF AS A TWO-LAYER DEEP GP,0.3497942386831276,"with ξr
iid
∼N (0, 1), we can marginalize {ξr} and obtain a fully symmetric multilinear ker-
152"
BKTF AS A TWO-LAYER DEEP GP,0.35185185185185186,"nel/covariance function for any two data points x = (x1, . . . , xD)⊤and x′ = (x′
1, . . . , x′
D)⊤:
153"
BKTF AS A TWO-LAYER DEEP GP,0.35390946502057613,"k
 
x, x′; {λr, gr
1, . . . , gr
D}R
r=1

= R
X"
BKTF AS A TWO-LAYER DEEP GP,0.3559670781893004,"r=1
λ2
r "" D
Y"
BKTF AS A TWO-LAYER DEEP GP,0.35802469135802467,"d=1
gr
d (xd) gr
d (x′
d) #"
BKTF AS A TWO-LAYER DEEP GP,0.360082304526749,".
(10)"
BKTF AS A TWO-LAYER DEEP GP,0.36213991769547327,"As can be seen, the second layer has a multilinear product kernel function parameterized by
154"
BKTF AS A TWO-LAYER DEEP GP,0.36419753086419754,"{λr, gr
1, . . . , gr
D}R
r=1. There are some properties to highlight: (i) the kernel is nonstationary since
155"
BKTF AS A TWO-LAYER DEEP GP,0.3662551440329218,"the value of gr
d(·) is location-specific, and (ii) the kernel is nonseparable when R > 1. Therefore,
156"
BKTF AS A TWO-LAYER DEEP GP,0.3683127572016461,"this specification is very different from traditional GP surrogates:
157"
BKTF AS A TWO-LAYER DEEP GP,0.37037037037037035,"



"
BKTF AS A TWO-LAYER DEEP GP,0.3724279835390947,"


"
BKTF AS A TWO-LAYER DEEP GP,0.37448559670781895,"GP with SE-ARD:
k (x, x′) = σ2 QD
d=1 kd (xd, x′
d) ,
kernel is stationary and separable
additive GP:
k (x, x′) = PD
d=1 k1st
d (xd, x′
d) + PD−1
d=1
PD
e=d+1 k2nd
d
(xd, x′
d) k2nd
e
(xe, x′
e) ,
(1st/2nd order)
kerenl is stationary and nonseparable"
BKTF AS A TWO-LAYER DEEP GP,0.3765432098765432,"where σ2 represents the kernel variance, and kernel functions

kd(·), k1st
d (·), k2nd
d (·), k2nd
e (·)
	
are
158"
BKTF AS A TWO-LAYER DEEP GP,0.3786008230452675,"stationary with different hyperparameters (e.g., length scale and variance). Compared with GP-based
159"
BKTF AS A TWO-LAYER DEEP GP,0.38065843621399176,"kernel specification, the multilinear kernel in Eq. (10) has a much larger set of hyperparameters and
160"
BKTF AS A TWO-LAYER DEEP GP,0.38271604938271603,"becomes more flexible and adaptive to the data. From a GP perspective, learning the hyperparameter
161"
BKTF AS A TWO-LAYER DEEP GP,0.38477366255144035,"in the kernel function in Eq. (10) will be computationally expensive; however, we can achieve efficient
162"
BKTF AS A TWO-LAYER DEEP GP,0.3868312757201646,"inference of {λr, gr
1, . . . , gr
D}R
r=1 under a tensor factorization framework. Based on the derivation in
163"
BKTF AS A TWO-LAYER DEEP GP,0.3888888888888889,"Eq. (10), we can consider BKTF as a “Bayesian” version of the multidimensional Karhunen-Loève
164"
BKTF AS A TWO-LAYER DEEP GP,0.39094650205761317,"(KL) expansion [18], in which the basis functions {gr
d} are random processes (i.e., GPs) and {λr} are
165"
BKTF AS A TWO-LAYER DEEP GP,0.39300411522633744,"random variables. On the other hand, we can interpret BKTF as a new class of stochastic process that
166"
BKTF AS A TWO-LAYER DEEP GP,0.3950617283950617,"is mainly parameterized by rank R and hyperparameters for those basis functions; however, BKTF
167"
BKTF AS A TWO-LAYER DEEP GP,0.39711934156378603,"does not impose any orthogonal constraints on the latent functions.
168"
MODEL INFERENCE,0.3991769547325103,"3.3
Model Inference
169"
MODEL INFERENCE,0.4012345679012346,"Unlike GP, BKTF no longer enjoys an analytical posterior distribution. Based on the aforementioned
170"
MODEL INFERENCE,0.40329218106995884,"prior and hyperprior settings, we adapt the MCMC updating procedure in [12, 14] to an efficient
171"
MODEL INFERENCE,0.4053497942386831,"element-wise Gibbs sampling algorithm for model inference. This allows us to accommodate
172"
MODEL INFERENCE,0.4074074074074074,"observations that are not located on the grid space QD
d=1 Sd. The detailed derivation of the sampling
173"
MODEL INFERENCE,0.4094650205761317,"algorithm is given in Appendix 7.1.
174"
PREDICTION AND AF COMPUTATION,0.411522633744856,"3.4
Prediction and AF Computation
175"
PREDICTION AND AF COMPUTATION,0.41358024691358025,"In each step of function evaluation, we run the MCMC sampling process K iterations for model
176"
PREDICTION AND AF COMPUTATION,0.4156378600823045,"inference, where the first K0 samples are taken as burn-in and the last K −K0 samples are
177"
PREDICTION AND AF COMPUTATION,0.4176954732510288,"used for posterior approximation. The predictive distribution for any entry f ∗in the defined grid
178"
PREDICTION AND AF COMPUTATION,0.41975308641975306,"space conditioned on the observed dataset D0 can be obtained by the Monte Carlo approximation
179"
PREDICTION AND AF COMPUTATION,0.4218106995884774,"p (f ∗| D0, θ0) ≈
1
K−K0 × PK
k=K0+1 p

f ∗ (gr
d)(k) , λ(k), τ (k)
, where θ0 = {µl, τl, a0, b0} is
180"
PREDICTION AND AF COMPUTATION,0.42386831275720166,"the set of all parameters used in hyperpriors. Although direct analytical predictive distribution does
181"
PREDICTION AND AF COMPUTATION,0.42592592592592593,"not exist in BKTF, the posterior mean and variance estimated from MCMC samples at each location
182"
PREDICTION AND AF COMPUTATION,0.4279835390946502,"naturally offer us a Bayesian approach to define the AFs.
183"
PREDICTION AND AF COMPUTATION,0.43004115226337447,"BKTF provides a fully Bayesian surrogate model. We define a Bayesian variant of UCB as the AF
184"
PREDICTION AND AF COMPUTATION,0.43209876543209874,"by adapting the predictive mean and variance (or uncertainty) in ordinary GP-based UCB with the
185"
PREDICTION AND AF COMPUTATION,0.43415637860082307,"values calculated from MCMC sampling. For every MCMC sample after burn-in, i.e., k > K0, we
186"
PREDICTION AND AF COMPUTATION,0.43621399176954734,"can estimate a output tensor ˜F
(k) over the entire grid space using the latent factors (gr
d)(k) and the
187"
PREDICTION AND AF COMPUTATION,0.4382716049382716,"weight vector λ(k): ˜F
(k) = PR
r=1 λ(k)
r
(gr
1)(k) ◦(gr
2)(k) ◦· · · ◦(gr
D)(k). We can then compute the
188"
PREDICTION AND AF COMPUTATION,0.4403292181069959,"corresponding mean and variance tensors of the (K −K0) samples { ˜F
(k)}K
k=K0+1, and denote the
189"
PREDICTION AND AF COMPUTATION,0.44238683127572015,"two tensors by U and V, respectively. The approximated predictive distribution at each point x in
190"
PREDICTION AND AF COMPUTATION,0.4444444444444444,"the space becomes ˜f(x) ∼N (u(x), v(x)). Following the definition of UCB in Eq. (5), we define
191"
PREDICTION AND AF COMPUTATION,0.44650205761316875,"Bayesian UCB (B-UCB) at location x as αB-UCB (x | D, β, gr
d, λ) = u(x) + β
p"
PREDICTION AND AF COMPUTATION,0.448559670781893,"v(x). The next
192"
PREDICTION AND AF COMPUTATION,0.4506172839506173,"search/query point can be determined via xnext = arg maxx∈{QD
d=1 Sd−Dn−1} αB-UCB (x).
193"
PREDICTION AND AF COMPUTATION,0.45267489711934156,"We summarize the implementation procedure of BKTF for BO in Appendix 7.2 (see Algorithm 2).
194"
PREDICTION AND AF COMPUTATION,0.4547325102880658,"Given the sequential nature of BO, when a new data point arrives at step n, we can start the MCMC
195"
PREDICTION AND AF COMPUTATION,0.4567901234567901,"with the last iteration of the Markov chains at step n −1 to accelerate model convergence. The main
196"
PREDICTION AND AF COMPUTATION,0.4588477366255144,"computational and storage cost of BKTF is to interpolate and save the tensors ˜F ∈R|S1|×···×|SD|
197"
PREDICTION AND AF COMPUTATION,0.4609053497942387,"over (K −K0) iterations for Bayesian AF estimation. This could be prohibitive when the MCMC
198"
PREDICTION AND AF COMPUTATION,0.46296296296296297,"sample size or the dimensionality of input space is large. To avoid saving the tensors, in practice,
199"
PREDICTION AND AF COMPUTATION,0.46502057613168724,"we can simply use the maximum values of each entry over the (K −K0) iterations through iterative
200"
PREDICTION AND AF COMPUTATION,0.4670781893004115,"pairwise comparison. The number of samples after burn-in then implies the value of β in αB-UCB. We
201"
PREDICTION AND AF COMPUTATION,0.4691358024691358,"adopt this simple AF in our numerical experiments.
202"
RELATED WORK,0.4711934156378601,"4
Related Work
203"
RELATED WORK,0.4732510288065844,"The key of BO is to effectively characterize the posterior distribution of the objective function
204"
RELATED WORK,0.47530864197530864,"from a limited number of observations. The most relevant work to our study is the Bayesian
205"
RELATED WORK,0.4773662551440329,"Kernelized Factorization (BKF) framework, which has been mainly used for modeling large-scale and
206"
RELATED WORK,0.4794238683127572,"multidimensional spatiotemporal data with UQ. The key idea is to parameterize the multidimensional
207"
RELATED WORK,0.48148148148148145,"stochastic processes using a factorization model, in which specific priors are used to encode spatial
208"
RELATED WORK,0.4835390946502058,"and temporal dependencies. Signature examples of BKF include spatial dynamic factor model
209"
RELATED WORK,0.48559670781893005,"(SDFM) [19], variational Gaussian process factor analysis (VGFA) [20], and Bayesian kernelized
210"
RELATED WORK,0.4876543209876543,"matrix/tensor factorization (BKMF/BKTF) [12, 14, 13]. A common solution in these models is to
211"
RELATED WORK,0.4897119341563786,"use GP prior to modeling the factor matrices, thus encoding spatial and temporal dependencies. In
212"
RELATED WORK,0.49176954732510286,"addition, for multivariate data with more than one attribute, BKTF also introduces a Wishart prior
213"
RELATED WORK,0.49382716049382713,"to modeling the factors that encode the dependency among features. A key difference among these
214"
RELATED WORK,0.49588477366255146,"methods is how inference is performed. SDFM and BKMF/BKTF are fully Bayesian hierarchical
215"
RELATED WORK,0.49794238683127573,"models and they rely on MCMC for model inference, where the factors can be updated via Gibbs
216"
RELATED WORK,0.5,"sampling with conjugate priors; for learning the posterior distributions of kernel hyperparameters,
217"
RELATED WORK,0.5020576131687243,"SDFM uses the Metropolis-Hastings sampling, while BKMF/BKTF uses the more efficient slice
218"
RELATED WORK,0.5041152263374485,"sampling. On the other hand, VGFA uses variational inference to learn factor matrices, while kernel
219"
RELATED WORK,0.5061728395061729,"hyperparameters are learned through maximum a posteriori (MAP) estimation without UQ. Overall,
220"
RELATED WORK,0.5082304526748971,"BKTF has shown superior performance in modeling multidimensional spatiotemporal processes with
221"
RELATED WORK,0.5102880658436214,"high-quality UQ for 2D and 3D spaces [14] and conducting tensor regression [13].
222"
RELATED WORK,0.5123456790123457,"The proposed BKTF surrogate models the objective function—as a single realization of a random
223"
RELATED WORK,0.51440329218107,"process—using low-rank tensor factorization with random basis functions. This basis function-
224"
RELATED WORK,0.5164609053497943,"based specification is closely related to multidimensional Karhunen-Loève (KL) expansion [18] for
225"
RELATED WORK,0.5185185185185185,"stochastic (spatial, temporal, and spatiotemporal) processes. The empirical analysis of KL expansion
226"
RELATED WORK,0.5205761316872428,"is also known as proper orthogonal decomposition (POD). With a known kernel/covariance function,
227"
RELATED WORK,0.522633744855967,"truncated KL expansion allows us to approximate the underlying random process using a set of
228"
RELATED WORK,0.5246913580246914,"eigenvalues and eigenfunctions derived from the kernel function. Numerical KL expansion is often
229"
RELATED WORK,0.5267489711934157,"referred to as the Garlekin method, and in practice the basis functions are often chosen as prespecified
230"
RELATED WORK,0.5288065843621399,"and deterministic functions [15, 21], such as Fourier basis, wavelet basis, orthogonal polynomials,
231"
RELATED WORK,0.5308641975308642,"B-splines, empirical orthogonal functions, radial basis functions (RBF), and Wendland functions
232"
RELATED WORK,0.5329218106995884,"(i.e., compactly supported RBF) (see, e.g., [22], [23], [24], [25]). However, the quality of UQ will be
233"
RELATED WORK,0.5349794238683128,"undermined as the randomness is fully attributed to the coefficients {λr}; in addition, these methods
234"
RELATED WORK,0.5370370370370371,"also require a large number of basis functions to fit complex stochastic processes. Different from
235"
RELATED WORK,0.5390946502057613,"methods with fixed/known basis functions, BKTF uses a Bayesian hierarchical modeling framework
236"
RELATED WORK,0.5411522633744856,"to better capture the randomness and uncertainty in the data, in which GP priors are used to model the
237"
RELATED WORK,0.5432098765432098,"latent factors (i.e., basis functions are also random processes) on different dimensions, and hyperpriors
238"
RELATED WORK,0.5452674897119342,"are introduced on the kernel hyperparameters. Therefore, BKTF becomes a fully Bayesian version of
239"
RELATED WORK,0.5473251028806584,"multidimensional KL expansion for stochastic processes with unknown covariance from partially
240"
RELATED WORK,0.5493827160493827,"observed data, however, without imposing any orthogonal constraint on the basis functions. Following
241"
RELATED WORK,0.551440329218107,"the analysis in section 3.2, BKTF is also a special case of a two-layer deep Gaussian process [26, 10],
242"
RELATED WORK,0.5534979423868313,"where the first layer produces latent factors for each dimension, and the second layer holds a
243"
RELATED WORK,0.5555555555555556,"multilinear kernel parameterized by all latent factors.
244"
EXPERIMENTS,0.5576131687242798,"5
Experiments
245"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5596707818930041,"5.1
Optimization for Benchmark Test Functions
246"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5617283950617284,"We test the proposed BKTF model for BO on six benchmark functions that are used for global
247"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5637860082304527,"optimization problems [27], which are summarized in Table 1. Figure 2(a) shows those functions with
248"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.565843621399177,"2-dimensional inputs together with the 2D Griewank function. All the selected standard functions
249"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5679012345679012,"are multimodal, more detailed descriptions can be found in Appendix 7.4. In fact, we can visually
250"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5699588477366255,"see that the standard Damavandi/Schaffer/Griewank functions in Figure 2(a) indeed have a low-rank
251"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5720164609053497,Table 1: Summary of the studied benchmark functions.
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5740740740740741,"Function
D
Search space
md
Characteristics"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5761316872427984,"Branin
2
[−5, 10] × [0, 15]
14
3 global minima, flat
Damavandi
2
[0, 14]2
71
multimodal, global minimum located in small area
Schaffer
2
[−10, 10]2
11
multimodal, global optimum located close to local minima"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5781893004115226,"Griewank
3
[−10, 10]3
11
multimodal, many widespread and regularly
4
[−10, 10]4
11
distributed local optima"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5802469135802469,"Hartmann
6
[0, 1]6
12
multimodal, multi-input"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5823045267489712,"structure. For each function, we assume the initial dataset D0 contains n0 = D observed data pairs,
252"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5843621399176955,"and we set the total number of query points to N = 80 for 4D Griewank and 6D Hartmann function
253"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5864197530864198,"and N = 50 for others. We rescale the input search range to [0, 1] for all dimensions and normalize
254"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.588477366255144,"the output data using z-score normalization.
255"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5905349794238683,"Model configuration. When applying BKTF on the continuous test functions, we introduce md
256"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5925925925925926,"interpolation points cd in the dth dimension of the input space. The values of md used for each
257"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5946502057613169,"benchmark function are predefined and given in Table 1. Setting the resolution grid will require
258"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5967078189300411,"certain prior knowledge (e.g., smoothness of the function); and it also depends on the available
259"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.5987654320987654,"computational resources and the number of entries in the tensor which grows exponentially with md.
260"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6008230452674898,"In practice, we find that setting md = 10 ∼100 is sufficient for most problems. We set the CP rank
261"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.602880658436214,"R = 2, and for each BO function evaluation run 400 MCMC iterations for model inference where
262"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6049382716049383,"the first 200 iterations are taken as burn-in. We use Matérn 3/2 kernel as the covariance function for
263"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6069958847736625,"all the test functions. Since we build a fully Bayesian model, the hyperparameters of the covariance
264"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6090534979423868,"functions can be updated automatically from the data likelihood and hyperprior.
265"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6111111111111112,"Effects of hyperpriors. Note that in optimization scenarios where the observation data is scarce, the
266"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6131687242798354,"model performance of BKTF highly depends on the hyperprior settings on the kernel length-scales
267"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6152263374485597,"of the latent factors and the model noise precision τ when proceeding estimation for the unknown
268"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6172839506172839,"points, i.e., θ0 = {µl, τl, a0, b0}. A proper hyper-prior becomes rather important. We discuss the
269"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6193415637860082,"effects of {µl, τl} in Appendix 7.5.1. We see that for the re-scaled input space, a reasonable setting
270"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6213991769547325,"is to suppose the mean prior of the kernel length-scales is around half of the input domain, i.e.,
271"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6234567901234568,"µl = log (0.5). The hyperprior on τ impacts the uncertainty of the latent factors, for example, a large
272"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6255144032921811,"model noise assumption allows more variances in the factors. Generally, we select the priors that
273"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6275720164609053,"make the noise variances not quite large, such as the results shown in Figure 4(a) and Figure 5(b) in
274"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6296296296296297,"Appendix. An example of the uncertainty provided by BKTF is explained in Appendix 7.3.
275"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6316872427983539,"Baselines. We compare BKTF with the following BO methods that use GP as the surrogate model.
276"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6337448559670782,"(1) GP αEI: GP as the surrogate model and EI as the AF in continuous space QD
d=1 Xd; (2) GP αUCB:
277"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6358024691358025,"GP as the surrogate model with UCB as the AF with β = 2, in QD
d=1 Xd; (3) GPgrid αEI: GP as the
278"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6378600823045267,"surrogate model with EI as the AF, in Cartesian grid space QD
d=1 Sd; (4) GPgrid αUCB: GP as the
279"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6399176954732511,"surrogate model with UCB as the AF with β = 2, in QD
d=1 Sd. We use the Matérn 3/2 kernel for all
280"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6419753086419753,"GP surrogates. For AF optimization in GP αEI and GP αUCB, we firstly use the DIRECT algorithm
281"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6440329218106996,"[28] and then apply the Nelder-Mead algorithm [29] to further search if there exist better solutions.
282"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6460905349794238,"Results. To compare optimization performances of different models on the benchmark functions,
283"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6481481481481481,"we consider the absolute error between the global optimum f ⋆and the current estimated global
284"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6502057613168725,"optimum ˆf ⋆, i.e.,
f ⋆−ˆf ⋆, w.r.t. the number of function evaluations. We run the optimization 10
285"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6522633744855967,"times for every test function with a different set of initial observations. The results are summarized in
286"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.654320987654321,"Figure 2(b). We see that for the 2D functions Branin and Schaffer, BKTF clearly finds the global
287"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6563786008230452,"optima much faster than GP surrogate-based baselines. For Damavandi function, where the global
288"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6584362139917695,"minimum (f(x⋆) = 0) is located at a small sharp area while the local optimum (f(x) = 2) is
289"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6604938271604939,"located at a large smooth area (see Figure 2(a)), GP-based models are trapped around the local
290"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6625514403292181,"optima in most cases, i.e.,
f ⋆−ˆf ⋆ = 2, and cannot jump out. On the contrary, BKTF explores the
291"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6646090534979424,"global characteristics of the objective function over the entire search space and reaches the global
292"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6666666666666666,"optimum within 10 iterations of function evaluations. For higher dimensional Griewank and Hartmann
293"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.668724279835391,"functions, BKTF successfully arrives at the global optima under the given observation budgets, while
294"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6707818930041153,"GP-based comparison methods are prone to be stuck around local optima. We illustrate the latent
295"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6728395061728395,"Figure 2: (a) Tested benchmark functions; (b) Optimization results on the six test functions, where
medians with 25% and 75% quartiles of 10 runs are compared; (c) Illustration of performance profiles."
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6748971193415638,"Table 2: Results of
f ⋆−ˆf ⋆ when n = N (mean ± std.) / AUC of PPs on benchmark functions."
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.676954732510288,"Function (D)
GP αEI
GP αUCB
GPgrid αEI
GPgrid αUCB
BKTF αB-UCB"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6790123456790124,"Branin (2)
0.01±0.01/37.7
0.01±0.01/37.7
0.31±0.62/47.8
0.24±0.64/49.2
0.00±0.00/50.5
Damavandi (2)
2.00±0.00/17.6
2.00±0.00/17.6
1.60±0.80/24.2
2.00±0.00/17.6
0.00±0.00/50.6
Schaffer (2)
0.02±0.02/44.9
0.02±0.02/43.1
0.10±0.15/38.3
0.09±0.07/38.0
0.00±0.00/49.6
Griewank (3)
0.14±0.14/48.9
0.25±0.10/47.7
0.23±0.13/47.7
0.22±0.12/47.7
0.00±0.00/50.8
Griewank (4)
0.10±0.07/79.5
0.19±0.12/77.8
0.38±0.19/77.8
0.27±0.17/77.8
0.00±0.00/80.5
Hartmann (6)
0.12±0.07/78.0
0.07±0.07/78.0
0.70±0.70/79.1
0.79±0.61/78.9
0.00±0.00/80.7"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6810699588477366,"Overall
-/70.3
-/69.53
-/71.3
-/70.4
-/80.5"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6831275720164609,Best results are highlighted in bold fonts.
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6851851851851852,"factors of BKTF for 3D Griewank function in Appendix 7.5.3, which shows the periodic (global)
296"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6872427983539094,"patterns automatically learned from the observations. We compare the absolute error between f ⋆and
297"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6893004115226338,"the final estimated ˆf ⋆in Table 2. The enumeration-based GP surrogates, i.e., GPgrid αEI and GPgrid
298"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.691358024691358,"αUCB, perform a little better than direct GP-based search, i.e., GP αEI and GP αUCB on Damavandi
299"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6934156378600823,"function, but worse on others. This means that the discretization, to some extent, offers possibilities
300"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6954732510288066,"for searching all the alternative points in the space, since in each function evaluation, every sample in
301"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6975308641975309,"the space is equally compared solely based on the predictive distribution. Overall, BKTF reaches
302"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.6995884773662552,"the global optimum for every test function and shows superior performance for complex objective
303"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.7016460905349794,"functions with a faster convergence rate. To intuitively compare the overall performances of different
304"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.7037037037037037,"models across multiple experiments/functions, we further estimate performance profiles (PPs) [30]
305"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.7057613168724279,"(see Appendix 7.5.2), and compute the area under the curve (AUC) for quantitative analyses (see
306"
OPTIMIZATION FOR BENCHMARK TEST FUNCTIONS,0.7078189300411523,"Figure 2(c) and Table 2). Clearly, BKTF obtains the best performance across all functions.
307"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7098765432098766,"5.2
Hyperparameter Tuning for Machine Learning
308"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7119341563786008,"In this section, we evaluate the performance of BKTF for automatic machine-learning tasks. Specifi-
309"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7139917695473251,"cally, we compare different models to optimize the hyperparameters of two machine learning (ML)
310"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7160493827160493,"algorithms—random forest (RF) and neural network (NN)—on classification for the MNIST database
311"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7181069958847737,"of handwritten digits1 and housing price regression for the Boston housing dataset2. The details of
312"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.720164609053498,"the hyperparameters that need to learn are given in Appendix 7.6. We assume the number of data
313"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7222222222222222,"points in the initial dataset D0 equals the dimension of hyperparameters need to tune, i.e., n0 = 4 and
314"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7242798353909465,"n0 = 3 for RF and NN, respectively. The total budget is N = 50. We implement the RF algorithms
315"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7263374485596708,"using scikit-learn package and construct NN models through Keras with 2 hidden layers. All other
316"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7283950617283951,"model hyperparameters are set as the default values.
317"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7304526748971193,"Model configuration. We treat all the discrete hyperparameters as samples from a continuous
318"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7325102880658436,"space and then generate the corresponding Cartesian product space QD
d=1 Sd. One can interpret
319"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7345679012345679,"the candidate values for each hyperparameter as the interpolation points in the corresponding input
320"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7366255144032922,"dimension. According to Appendix 7.6, the size of the spanned space Q Sd is 91 × 46 × 64 × 10 and
321"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7386831275720165,"91 × 46 × 13 × 10 for RF classifier and RF regressor, respectively; for the two NN algorithms, the
322"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7407407407407407,"size of parameter space is 91 × 49 × 31. Similar to the settings on standard test functions, we set the
323"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.742798353909465,"tensor rank R = 2, set K = 400 and K0 = 200 for MCMC inference, and use the Matérn 3/2 kernel.
324"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7448559670781894,"1http://yann.lecun.com/exdb/mnist/
2https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7469135802469136,Final accuracy for (a) and MSE for (b).
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7489711934156379,"Model
RF
NN (a)"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7510288065843621,"GP αEI
94.12±0.16
99.96±0.03
GP αUCB
94.07±0.19
99.99±0.02
BO-TPE
94.14±0.20
99.96±0.02
BKTF αB-UCB
94.44±0.15
100.00±0.00 (b)"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7530864197530864,"GP αEI
26.19±0.45
38.46±3.31
GP αUCB
26.29±0.35
36.78±1.91
BO-TPE
26.27±0.31
36.40±4.72
BKTF αB-UCB
25.03±0.18
30.84±1.13"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7551440329218106,"The values are presented as mean±std.
Best results are highlighted in bold fonts."
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.757201646090535,"Figure 3 & Table 3: Results of hyperparameter tuning for automated ML: (a) MNIST classification;
(b) Boston housing regression. The figure compares medians with 25% and 75% quartiles of 10 runs."
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7592592592592593,"Baselines. Other than the GP surrogate-based GP αEI and GP αUCB, we also compare with Tree-
325"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7613168724279835,"structured Parzen Estimator (BO-TPE) [31], which is a widely applied BO approach for hyperparam-
326"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7633744855967078,"eter tuning. We exclude grid-based GP models as sampling the entire grid becomes infeasible.
327"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7654320987654321,"Results. We compare the accuracy for MNIST classification and MSE (mean squared error) for
328"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7674897119341564,"Boston housing regression both in terms of the number of function evaluations and still run the
329"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7695473251028807,"optimization processes ten times with different initial datasets D0. The results obtained by different
330"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7716049382716049,"BO models are given in Figure 3, and the final classification accuracy and regression MSE are
331"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7736625514403292,"compared in Table 3. For BKTF, we see from Figure 3 that the width between the two quartiles of the
332"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7757201646090535,"accuracy and error decreases as more iterations are evaluated, and the median curves present superior
333"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7777777777777778,"convergence rates compared to baselines. For example, BKTF finds the hyperparameters of NN that
334"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.779835390946502,"achieve 100% classification accuracy on MNIST using less than four function evaluations in all ten
335"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7818930041152263,"runs. Table 3 also shows that the proposed BKTF surrogate achieves the best final mean accuracy and
336"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7839506172839507,"regression error with small standard deviations. All these demonstrate the advantage of BKTF as a
337"
HYPERPARAMETER TUNING FOR MACHINE LEARNING,0.7860082304526749,"surrogate for black-box function optimization.
338"
CONCLUSION AND DISCUSSIONS,0.7880658436213992,"6
Conclusion and Discussions
339"
CONCLUSION AND DISCUSSIONS,0.7901234567901234,"In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate
340"
CONCLUSION AND DISCUSSIONS,0.7921810699588477,"model for Bayesian optimization. Compared with traditional GP surrogates, the BKTF surrogate is
341"
CONCLUSION AND DISCUSSIONS,0.7942386831275721,"more flexible and adaptive to data thanks to the Bayesian hierarchical specification, which provides
342"
CONCLUSION AND DISCUSSIONS,0.7962962962962963,"high-quality UQ for BO tasks. The tensor factorization model behind BKTF offers an effective
343"
CONCLUSION AND DISCUSSIONS,0.7983539094650206,"solution to capture global/long-range correlations and cross-dimension correlations. Therefore, it
344"
CONCLUSION AND DISCUSSIONS,0.8004115226337448,"shows superior performance in characterizing complex multidimensional stochastic processes that are
345"
CONCLUSION AND DISCUSSIONS,0.8024691358024691,"nonstationary, nonseparable, and multimodal. The inference of BKTF is achieved through MCMC,
346"
CONCLUSION AND DISCUSSIONS,0.8045267489711934,"which provides a natural solution for acquisition. Experiments on both test function optimization and
347"
CONCLUSION AND DISCUSSIONS,0.8065843621399177,"ML hyperparameter tuning confirm the superiority of BKTF as a surrogate for BO. A limitation of
348"
CONCLUSION AND DISCUSSIONS,0.808641975308642,"BKTF is that we restrict BO to Cartesian grid space to leverage tensor factorization; however, we
349"
CONCLUSION AND DISCUSSIONS,0.8106995884773662,"believe designing a compatible grid space based on prior knowledge is not a challenging task.
350"
CONCLUSION AND DISCUSSIONS,0.8127572016460906,"There are several directions to be explored for future research. A key computational issue of BKTF
351"
CONCLUSION AND DISCUSSIONS,0.8148148148148148,"is that we need to reconstruct the posterior distribution for the whole tensor to obtain the AF. This
352"
CONCLUSION AND DISCUSSIONS,0.8168724279835391,"could be problematic for high-dimensional problems due to the curse of dimensionality. It would be
353"
CONCLUSION AND DISCUSSIONS,0.8189300411522634,"interesting to see whether we can achieve efficient acquisition directly using the basis functions and
354"
CONCLUSION AND DISCUSSIONS,0.8209876543209876,"corresponding weights without constructing the tensors explicitly. In terms of rank determination, we
355"
CONCLUSION AND DISCUSSIONS,0.823045267489712,"can introduce the multiplicative gamma process prior to learn the rank; this will create a Bayesian
356"
CONCLUSION AND DISCUSSIONS,0.8251028806584362,"nonparametric model that can automatically adapt to the data. In terms of surrogate modeling, we can
357"
CONCLUSION AND DISCUSSIONS,0.8271604938271605,"further integrate a local (short-scale) GP component to construct a more precise surrogate model, as
358"
CONCLUSION AND DISCUSSIONS,0.8292181069958847,"presented in [14]. The combined framework would be more expensive in computation, but we expect
359"
CONCLUSION AND DISCUSSIONS,0.831275720164609,"the combination to provide better UQ performance. In terms of parameterization, we also expect that
360"
CONCLUSION AND DISCUSSIONS,0.8333333333333334,"introducing orthogonality prior to the latent factors (basis functions) will improve the inference. This
361"
CONCLUSION AND DISCUSSIONS,0.8353909465020576,"can be potentially achieved through more advanced prior specifications such as the Matrix angular
362"
CONCLUSION AND DISCUSSIONS,0.8374485596707819,"central Gaussian [32]. In addition, for the tensor factorization framework, it is straightforward to
363"
CONCLUSION AND DISCUSSIONS,0.8395061728395061,"adapt the model to handle categorical variables as input and multivariate output by placing a Wishart
364"
CONCLUSION AND DISCUSSIONS,0.8415637860082305,"prior to the latent factors for the categorical/output dimension.
365"
REFERENCES,0.8436213991769548,"References
366"
REFERENCES,0.845679012345679,"[1] Roman Garnett. Bayesian Optimization. Cambridge University Press, 2023.
367"
REFERENCES,0.8477366255144033,"[2] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking
368"
REFERENCES,0.8497942386831275,"the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE,
369"
REFERENCES,0.8518518518518519,"104(1):148–175, 2015.
370"
REFERENCES,0.8539094650205762,"[3] Robert B Gramacy. Surrogates: Gaussian Process Modeling, Design, and Optimization for the
371"
REFERENCES,0.8559670781893004,"Applied Sciences. Chapman and Hall/CRC, 2020.
372"
REFERENCES,0.8580246913580247,"[4] Christopher KI Williams and Carl Edward Rasmussen. Gaussian Processes for Machine
373"
REFERENCES,0.8600823045267489,"Learning. MIT Press, Cambridge, MA, 2006.
374"
REFERENCES,0.8621399176954733,"[5] David K Duvenaud, Hannes Nickisch, and Carl Rasmussen. Additive Gaussian processes.
375"
REFERENCES,0.8641975308641975,"Advances in neural information processing systems, 24, 2011.
376"
REFERENCES,0.8662551440329218,"[6] Kirthevasan Kandasamy, Jeff Schneider, and Barnabás Póczos. High dimensional Bayesian
377"
REFERENCES,0.8683127572016461,"optimisation and bandits via additive models. In International conference on machine learning,
378"
REFERENCES,0.8703703703703703,"pages 295–304. PMLR, 2015.
379"
REFERENCES,0.8724279835390947,"[7] Chun-Liang Li, Kirthevasan Kandasamy, Barnabás Póczos, and Jeff Schneider. High dimen-
380"
REFERENCES,0.8744855967078189,"sional Bayesian optimization via restricted projection pursuit models. In Artificial Intelligence
381"
REFERENCES,0.8765432098765432,"and Statistics, pages 884–892. PMLR, 2016.
382"
REFERENCES,0.8786008230452675,"[8] Paul Rolland, Jonathan Scarlett, Ilija Bogunovic, and Volkan Cevher.
High-dimensional
383"
REFERENCES,0.8806584362139918,"Bayesian optimization via additive models with overlapping groups. In International conference
384"
REFERENCES,0.8827160493827161,"on artificial intelligence and statistics, pages 298–307. PMLR, 2018.
385"
REFERENCES,0.8847736625514403,"[9] Mickael Binois and Nathan Wycoff. A survey on high-dimensional Gaussian process modeling
386"
REFERENCES,0.8868312757201646,"with application to Bayesian optimization. ACM Transactions on Evolutionary Learning and
387"
REFERENCES,0.8888888888888888,"Optimization, 2(2):1–26, 2022.
388"
REFERENCES,0.8909465020576132,"[10] Andreas Damianou and Neil D Lawrence. Deep Gaussian processes. In International Conference
389"
REFERENCES,0.8930041152263375,"on Artificial Intelligence and Statistics, pages 207–215, 2013.
390"
REFERENCES,0.8950617283950617,"[11] Annie Sauer, Robert B Gramacy, and David Higdon. Active learning for deep gaussian process
391"
REFERENCES,0.897119341563786,"surrogates. Technometrics, 65(1):4–18, 2023.
392"
REFERENCES,0.8991769547325102,"[12] Mengying Lei, Aurelie Labbe, Yuankai Wu, and Lijun Sun. Bayesian kernelized matrix
393"
REFERENCES,0.9012345679012346,"factorization for spatiotemporal traffic data imputation and kriging. IEEE Transactions on
394"
REFERENCES,0.9032921810699589,"Intelligent Transportation Systems, 23(10):18962–18974, 2022.
395"
REFERENCES,0.9053497942386831,"[13] Mengying Lei, Aurelie Labbe, and Lijun Sun. Scalable spatiotemporally varying coefficient
396"
REFERENCES,0.9074074074074074,"modeling with bayesian kernelized tensor regression. arXiv preprint arXiv:2109.00046, 2021.
397"
REFERENCES,0.9094650205761317,"[14] Mengying Lei, Aurelie Labbe, and Lijun Sun. Bayesian complementary kernelized learning for
398"
REFERENCES,0.911522633744856,"multidimensional spatiotemporal data. arXiv preprint arXiv:2208.09978, 2022.
399"
REFERENCES,0.9135802469135802,"[15] Noel Cressie, Matthew Sainsbury-Dale, and Andrew Zammit-Mangion. Basis-function models
400"
REFERENCES,0.9156378600823045,"in spatial statistics. Annual Review of Statistics and Its Application, 9:373–400, 2022.
401"
REFERENCES,0.9176954732510288,"[16] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM Review,
402"
REFERENCES,0.9197530864197531,"51(3):455–500, 2009.
403"
REFERENCES,0.9218106995884774,"[17] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
404"
REFERENCES,0.9238683127572016,"Learning Research, 3(Nov):397–422, 2002.
405"
REFERENCES,0.9259259259259259,"[18] Limin Wang. Karhunen-Loeve expansions and their applications. London School of Economics
406"
REFERENCES,0.9279835390946503,"and Political Science (United Kingdom), 2008.
407"
REFERENCES,0.9300411522633745,"[19] Hedibert Freitas Lopes, Esther Salazar, and Dani Gamerman. Spatial dynamic factor analysis.
408"
REFERENCES,0.9320987654320988,"Bayesian Analysis, 3(4):759–792, 2008.
409"
REFERENCES,0.934156378600823,"[20] Jaakko Luttinen and Alexander Ilin. Variational Gaussian-process factor analysis for modeling
410"
REFERENCES,0.9362139917695473,"spatio-temporal data. Advances in Neural Information Processing Systems, 22:1177–1185,
411"
REFERENCES,0.9382716049382716,"2009.
412"
REFERENCES,0.9403292181069959,"[21] Holger Wendland. Scattered Data Approximation, volume 17. Cambridge university press,
413"
REFERENCES,0.9423868312757202,"2004.
414"
REFERENCES,0.9444444444444444,"[22] Rommel G Regis and Christine A Shoemaker. A stochastic radial basis function method for the
415"
REFERENCES,0.9465020576131687,"global optimization of expensive functions. INFORMS Journal on Computing, 19(4):497–509,
416"
REFERENCES,0.948559670781893,"2007.
417"
REFERENCES,0.9506172839506173,"[23] Gregory Beylkin, Jochen Garcke, and Martin J Mohlenkamp. Multivariate regression and
418"
REFERENCES,0.9526748971193416,"machine learning with sums of separable functions. SIAM Journal on Scientific Computing,
419"
REFERENCES,0.9547325102880658,"31(3):1840–1857, 2009.
420"
REFERENCES,0.9567901234567902,"[24] Christopher K Wikle and Noel Cressie. A dimension-reduced approach to space-time kalman
421"
REFERENCES,0.9588477366255144,"filtering. Biometrika, 86(4):815–829, 1999.
422"
REFERENCES,0.9609053497942387,"[25] Mathilde Chevreuil, Régis Lebrun, Anthony Nouy, and Prashant Rai. A least-squares method
423"
REFERENCES,0.9629629629629629,"for sparse low rank approximation of multivariate functions. SIAM/ASA Journal on Uncertainty
424"
REFERENCES,0.9650205761316872,"Quantification, 3(1):897–921, 2015.
425"
REFERENCES,0.9670781893004116,"[26] Alexandra M Schmidt and Anthony O’Hagan. Bayesian inference for non-stationary spatial
426"
REFERENCES,0.9691358024691358,"covariance structure via spatial deformations. Journal of the Royal Statistical Society: Series B
427"
REFERENCES,0.9711934156378601,"(Statistical Methodology), 65(3):743–758, 2003.
428"
REFERENCES,0.9732510288065843,"[27] Momin Jamil and Xin-She Yang.
A literature survey of benchmark functions for global
429"
REFERENCES,0.9753086419753086,"optimization problems. arXiv preprint arXiv:1308.4008, 2013.
430"
REFERENCES,0.977366255144033,"[28] Donald R Jones, Cary D Perttunen, and Bruce E Stuckman. Lipschitzian optimization without
431"
REFERENCES,0.9794238683127572,"the lipschitz constant. Journal of optimization Theory and Applications, 79(1):157–181, 1993.
432"
REFERENCES,0.9814814814814815,"[29] John A Nelder and Roger Mead. A simplex method for function minimization. The computer
433"
REFERENCES,0.9835390946502057,"journal, 7(4):308–313, 1965.
434"
REFERENCES,0.98559670781893,"[30] Elizabeth D Dolan and Jorge J Moré. Benchmarking optimization software with performance
435"
REFERENCES,0.9876543209876543,"profiles. Mathematical programming, 91(2):201–213, 2002.
436"
REFERENCES,0.9897119341563786,"[31] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-
437"
REFERENCES,0.9917695473251029,"parameter optimization. Advances in neural information processing systems, 24, 2011.
438"
REFERENCES,0.9938271604938271,"[32] Michael Jauch, Peter D Hoff, and David B Dunson. Monte carlo simulation on the stiefel
439"
REFERENCES,0.9958847736625515,"manifold via polar expansion. Journal of Computational and Graphical Statistics, 30(3):622–
440"
REFERENCES,0.9979423868312757,"631, 2021.
441"
