Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00303951367781155,"A central question in reinforcement learning (RL) is how to leverage prior knowl-
edge to accelerate learning in new tasks. We propose a Bayesian exploration
method for lifelong reinforcement learning (BLRL) that aims to learn a Bayesian
posterior that distills the common structure shared across different tasks. We further
derive a sample complexity analysis of BLRL in the Ô¨Ånite MDP setting. To scale
our approach, we propose a variational Bayesian Lifelong Learning (VBLRL)
algorithm that is based on Bayesian neural networks, can be combined with recent
model-based RL methods, and exhibits backward transfer. Experimental results on
three challenging domains show that our algorithms adapt to new tasks faster than
state-of-the-art lifelong RL methods."
INTRODUCTION,0.0060790273556231,"1
INTRODUCTION"
INTRODUCTION,0.00911854103343465,"Reinforcement-learning (RL) methods (Sutton & Barto, 1998; Kaelbling et al., 1996) have been
successfully applied to solve challenging individual tasks such as learning robotic control (Duan
et al., 2016) and playing expert-level Go (Silver et al., 2017). However, in the real world, a robot
usually experiences a collection of distinct tasks that arrive sequentially throughout its operational
lifetime; learning each new task from scratch is infeasible, but treating them all as a single task will
fail. Therefore, recent research has focused on algorithms that enable agents to learn across multiple,
sequentially posed tasks, leveraging past knowledge from previous tasks to accelerate the learning of
new tasks. This problem setting is known as lifelong reinforcement learning (Brunskill & Li, 2014;
Wilson et al., 2007b; Isele et al., 2016b). The key questions in lifelong RL research are: How can an
algorithm exploit knowledge gained from past tasks to improve performance in new tasks (forward
transfer), and how can data from new tasks help the agent to perform better on previously learned
tasks (backward transfer)?"
INTRODUCTION,0.0121580547112462,"To answer these two questions, Ô¨Årst consider a simple problem, which is to Ô¨Ånd different items in
different houses. Here, a single task corresponds to Ô¨Ånding items in a speciÔ¨Åc house. Although items
may be stored in different locations in different houses, there still exists some shared information
that connects all houses. For instance, a toothbrush is more likely to be found in a bathroom than
a kitchen, and a room without a window is more likely to be a bathroom than a living room. Such
information can signiÔ¨Åcantly accelerate the search for items in newly encountered houses. We propose
that extracting the common structure existing in previously encountered tasks can help the agent
quickly learn the dynamics of the new tasks. SpeciÔ¨Åcally, this paper considers lifelong RL problems
that can be modeled as hidden-parameter MDPs or HiP-MDPs (Doshi-Velez & Konidaris, 2016;
Killian et al., 2017), where variations among the true task dynamics can be described by a set of
hidden parameters. We model two main categories of learning across multiple tasks: the world-model
distribution, which describes the probability distribution over tasks, and the task-speciÔ¨Åc model, that
deÔ¨Ånes the (stochastic) dynamics within a single task. To enable more accurate sequential knowledge
transfer, we separate the learning process for these two quantities and maintain a hierarchical Bayesian
posterior to approximate them. The world-model posterior is designed to manage the uncertainty in
the world-model distribution, while the task-speciÔ¨Åc posterior handles the uncertainty from the data
collected from only the current task."
INTRODUCTION,0.015197568389057751,"We propose a Bayesian exploration method for lifelong RL (BLRL) that learns a Bayesian world-
model posterior that distills the common structure of previous tasks, and then uses it as a prior to
learn a task-speciÔ¨Åc model in each subsequent task. For the discrete case, we derive an explicit"
INTRODUCTION,0.0182370820668693,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02127659574468085,"performance bound that shows that the task-speciÔ¨Åc model requires fewer samples to become accurate
as the world-model posterior approaches the true underlying world-model distribution. We further
develop VBLRL, a more scalable version of BLRL that uses variational inference to approximate the
world-model distribution and leverages Bayesian Neural Networks (BNNs) to build the hierarchical
Bayesian posterior. Our experimental results on a set of challenging domains show that our algorithms
achieve better forward and backward transfer performance than state-of-the-art lifelong RL algorithms
within limited samples for each task."
BACKGROUND,0.0243161094224924,"2
BACKGROUND"
BACKGROUND,0.02735562310030395,"RL is the problem of maximizing the long-term expected reward of an agent interacting with an
environment (Sutton & Barto, 1998). We usually model the environment as a Markov Decision
Process or MDP (Puterman, 1994), described by a Ô¨Åve tuple: ‚ü®S, A, R, T, Œ≥‚ü©, where S is a Ô¨Ånite set
of states; A is a Ô¨Ånite set of actions; R : S √ó A 7‚Üí[0, 1] is a reward function, with a lower and upper
bound 0 and 1; T : S √ó A 7‚ÜíPr(S) is a transition function, with T(s‚Ä≤|s, a) denoting the probability
of arriving in state s‚Ä≤ ‚ààS after executing action a ‚ààA in state s; and Œ≥ ‚àà[0, 1) is a discount factor,
expressing the agent‚Äôs preference for delayed over immediate rewards."
BACKGROUND,0.030395136778115502,"An MDP is a suitable model for the task facing a single agent.
In the lifelong RL setting,
the agent instead faces a series of tasks œÑ1, ..., œÑn, each of which can be modeled as an MDP:
‚ü®S(i), A(i), R(i), T (i), Œ≥(i)‚ü©. A key question is how these task MDPs are related; we model the
collection of tasks as a HiP-MDP (Doshi-Velez & Konidaris, 2016; Killian et al., 2017), where a
family of tasks is generated by varying a latent task parameter œâ drawn for each task according to
the world-model distribution P‚Ñ¶. Each setting of œâ speciÔ¨Åes a unique MDP, but the agent neither
observes œâ nor has access to the function that generates the task family. Formally, then, the dynamics
T(s‚Ä≤|s, a; œâi) and reward function R(r|s, a; œâi) for task i depend on œâi ‚àà‚Ñ¶, which is Ô¨Åxed for the
duration of the task. For lifelong RL problems, the performance of a speciÔ¨Åc algorithm is usually
evaluated based on both forward transfer and backward transfer results (Lopez-Paz & Ranzato, 2017):"
BACKGROUND,0.03343465045592705,‚Ä¢ Forward transfer: the inÔ¨Çuence that learning task t has on the performance in future task k ‚âªt.
BACKGROUND,0.0364741641337386,‚Ä¢ Backward transfer: the inÔ¨Çuence that learning task t has on the performance in earlier tasks k ‚â∫t.
BAYESIAN EXPLORATION FOR LIFELONG REINFORCEMENT LEARNING,0.03951367781155015,"3
BAYESIAN EXPLORATION FOR LIFELONG REINFORCEMENT LEARNING ùúîùëñ 
Œ® ùëöùëñ ùúèùëó"
BAYESIAN EXPLORATION FOR LIFELONG REINFORCEMENT LEARNING,0.0425531914893617,"ùëñ = 1 ‚ãØùêæ 
ùëó= 1 ‚ãØùëÖ"
BAYESIAN EXPLORATION FOR LIFELONG REINFORCEMENT LEARNING,0.04559270516717325,"Figure 1: Plate representation
for the BLRL approach.
œÑj
denotes trajectory {s, a, r, s‚Ä≤}j.
There are K different tasks and
the agent samples R trajecto-
ries from each task."
BAYESIAN EXPLORATION FOR LIFELONG REINFORCEMENT LEARNING,0.0486322188449848,"The key component of our approach is a hierarchical Bayesian
posterior over task MDPs controlled by the hidden parameter œâ.
Intuitively, we maintain probability distributions that separately
capture two categories of uncertainty within lifelong learning tasks:
the world-model posterior captures the epistemic uncertainty of
the world-model distribution over different tasks controlled by the
hidden parameter. As the learner is exposed to more and more
tasks, this posterior should converge to the world-model distribution
P‚Ñ¶. The task-speciÔ¨Åc posterior captures the epistemic uncertainty
of the current task m. As the learner is exposed to more and
more transitions within the task, this posterior should approach m,
leaving only the aleatoric uncertainty of transitions within the task,
which is independent of other tasks. By sampling from the world-
model posterior, an agent can learn new tasks faster by exploiting
knowledge common to previous tasks, thus exhibiting positive
forward transfer."
BAYESIAN EXPLORATION FOR LIFELONG REINFORCEMENT LEARNING,0.05167173252279635,"We Ô¨Årst consider the Ô¨Ånite MDP setting. Concretely, we use a hi-
erarchical Bayesian model to represent the distribution over MDPs.
Figure 1 shows our generative model in plate notation. Œ® is the parameter set that represents dis-
tribution P‚Ñ¶. It functions as the world-model posterior that aims to capture the common structure
across different tasks. The resulting MDP mi is created based on œâi, which is one hidden parameter
sampled from Œ®. We can sample from our approximation of Œ® to create and solve possible MDPs."
BAYESIAN EXPLORATION FOR LIFELONG REINFORCEMENT LEARNING,0.0547112462006079,Under review as a conference paper at ICLR 2022
BAYESIAN EXPLORATION FOR LIFELONG REINFORCEMENT LEARNING,0.057750759878419454,"The proposed BLRL approach is formalized in Algorithm 3 in the appendix. Initially, before any
MDPs are experienced, the world-model posterior qe(¬∑|st, at) is initialized to an uniformed prior. For
each new task mi, we Ô¨Årst initialize the task-speciÔ¨Åc posterior qmi
Œ∏ (¬∑|st, at) with the parameter values
from the current world-model posterior, and then, for each timestep, select actions using a Bayesian
exploration algorithm based on sampling from this posterior (Thompson, 1933; Asmuth et al., 2009).
A set of sampled MDPs drawn from qmi
Œ∏
is a concrete representation of the uncertainty within the
current task. BLRL samples K models from the task-speciÔ¨Åc posterior whenever the number of
transitions from a state‚Äìaction pair has reached threshold B. Analogously to RMAX (Brafman &
Tennenholtz, 2003) and BOSS (Asmuth et al., 2009), we call a state‚Äìaction pair known whenever
it has been observed Nst,at = B times. For each state‚Äìaction pair, if it is known, we use the task-
speciÔ¨Åc posterior to sample the model. If it is unknown, we instead sample from the world-model
posterior. These models are combined into a merged MDP m#
i and BLRL solves m#
i to get a
policy œÄ‚àó
m#
i . This approach is adopted from BOSS (best of sampled set) to create optimism in the
face of uncertainty, and thereby drive exploration. The new policy œÄ‚àó
m#
i will be used to interact
with the environment until a new state‚Äìaction pair reaches the sampling threshold. The collected
transitions from this task will be used to update the task-speciÔ¨Åc posterior immediately, while the
world-model posterior will be updated using transitions from all the previous tasks at a slower pace.
For simple Ô¨Ånite MDP problems in practice, we use the Dirichlet distribution (the conjugate for the
multinomial) to represent the Bayesian posterior. Thus, the updating process for the posterior is
straightforward to compute. Intuitively, BLRL is able to rapidly adapt to new tasks as long as the
prior of the task-speciÔ¨Åc model (that is, the world-model posterior) is close to the true underlying
model and captures the uncertainty of the common structure of a set of tasks."
SAMPLE COMPLEXITY ANALYSIS,0.060790273556231005,"3.1
SAMPLE COMPLEXITY ANALYSIS"
SAMPLE COMPLEXITY ANALYSIS,0.06382978723404255,"We now provide a simple theoretical analysis of BLRL. First, we use the setting and results of Zhang
(2006) to describe the properties of the Bayesian prior and how it relates to the sample complexity for
the concentration of the Bayesian posterior."
SAMPLE COMPLEXITY ANALYSIS,0.0668693009118541,"Lemma 1. Let œÄ(œâ) denote the prior distribution on the parameter space Œì. We consider a set of
transition-probability densities p(¬∑|œâ) indexed by œâ, and the true underlying density q. DeÔ¨Åne the
prior-mass radius of the transition-probability densities as:"
SAMPLE COMPLEXITY ANALYSIS,0.06990881458966565,"dœÄ = inf{d : d ‚â•‚àíln œÄ({p ‚ààŒì : DKL(q||p) ‚â§d})}.
(1)"
SAMPLE COMPLEXITY ANALYSIS,0.0729483282674772,"Intuitively, this quantity measures the distance between the Bayesian prior we use to initialize the
posterior and the true underlying distribution. Then, ‚àÄœÅ ‚àà(0, 1) and Œ∑ ‚â•1, let"
SAMPLE COMPLEXITY ANALYSIS,0.07598784194528875,Œµn = (1 + 1
SAMPLE COMPLEXITY ANALYSIS,0.0790273556231003,"n)Œ∑dœÄ + (Œ∑ ‚àíœÅ)Œµupper,n((Œ∑ ‚àí1)/(Œ∑ ‚àíœÅ)),
(2)"
SAMPLE COMPLEXITY ANALYSIS,0.08206686930091185,"where Œµupper,n is the critical upper-bracketing radius (Zhang, 2006). The decay rate of Œµupper,n
controls the consistency of the Bayesian posterior distribution (Asmuth et al., 2009). Let œÅ = 1"
SAMPLE COMPLEXITY ANALYSIS,0.0851063829787234,"2, we
have for all t ‚â•0 and Œ¥ ‚àà(0, 1), with probability at least 1 ‚àíŒ¥,"
SAMPLE COMPLEXITY ANALYSIS,0.08814589665653495,"œÄn
n
p ‚ààŒì : ||p ‚àíq||2
1/2 ‚â•2Œµn + (4Œ∑ ‚àí2)t Œ¥/4"
SAMPLE COMPLEXITY ANALYSIS,0.0911854103343465,"oX

‚â§
1
1 + ent .
(3)"
SAMPLE COMPLEXITY ANALYSIS,0.09422492401215805,"Proof (sketch). The proof is similar to that of Corollary 5.2 of Zhang (2006) (see Appendix A.5).
Instead of using the critical prior-mass radius ŒµœÄ,n to describe certain characteristics of the Bayesian
prior, we deÔ¨Åne and use the prior-mass radius dœÄ, which is independent of the sample size n and
measures the distance between the prior and true distribution."
SAMPLE COMPLEXITY ANALYSIS,0.0972644376899696,"Similar to BOSS, for a new MDP m ‚àºM with hidden parameters œâm, we can deÔ¨Åne the Bayesian
concentration sample complexity for the task-speciÔ¨Åc posterior: f(s, a, œµ0, Œ¥0, œÅ0), as the minimum
number c such that, if c IID transitions from (s, a) are observed, then, with probability at least 1 ‚àíŒ¥0,"
SAMPLE COMPLEXITY ANALYSIS,0.10030395136778116,"Prm‚àºposterior(||Tm(s, a, œâm) ‚àíTm‚àó(s, a, œâm)||1 < œµ0) ‚â•1 ‚àíœÅ0.
(4)"
SAMPLE COMPLEXITY ANALYSIS,0.1033434650455927,Under review as a conference paper at ICLR 2022
SAMPLE COMPLEXITY ANALYSIS,0.10638297872340426,"Lemma 2. Assume the posterior is consistent (that is, Œµupper,n = o(1)) and set Œ∑ = 2, then
the Bayesian concentration sample complexity for the task-speciÔ¨Åc posterior f(s, a, œµ, Œ¥, œÅ) ="
SAMPLE COMPLEXITY ANALYSIS,0.1094224924012158,"O
 dœÄ+ln 1"
SAMPLE COMPLEXITY ANALYSIS,0.11246200607902736,"œÅ
œµ2Œ¥‚àídœÄ 
."
SAMPLE COMPLEXITY ANALYSIS,0.11550151975683891,Proof (sketch). This bound can be derived by directly combining Lemma 1 and Equation 4.
SAMPLE COMPLEXITY ANALYSIS,0.11854103343465046,"The above lemma suggests an upper bound of the Bayesian concentration sample complexity using
the prior-mass radius. We can further combine this result with PAC-MDP theory (Strehl et al., 2006)
and derive the sample complexity of the algorithm for each new task."
SAMPLE COMPLEXITY ANALYSIS,0.12158054711246201,"Theorem 1. For each new task, set the sample size K = Œò( S2A"
SAMPLE COMPLEXITY ANALYSIS,0.12462006079027356,"Œ¥
ln SA"
SAMPLE COMPLEXITY ANALYSIS,0.1276595744680851,"Œ¥ ) and the parameters œµ0 =
œµ(1 ‚àíŒ≥)2, Œ¥0 =
Œ¥
SA, œÅ0 =
Œ¥
S2A2K , then, with probability at least 1 ‚àí4Œ¥, V At(st) ‚â•V ‚àó(st) ‚àí4œµ0
in all but ÀúO( S2A2dœÄ"
SAMPLE COMPLEXITY ANALYSIS,0.13069908814589665,"Œ¥œµ3(1‚àíŒ≥)6 ) steps, where ÀúO(¬∑) suppresses logarithmic dependence."
SAMPLE COMPLEXITY ANALYSIS,0.1337386018237082,"Proof (sketch). The proof is based on the PAC-MDP theorem (Strehl et al., 2009) combined with the
new bound for the Bayesian concentration sample complexity we derived in Lemma 2. In general,
we use the same process in BOSS to verify the three required properties of PAC-MDP: optimism,
accuracy and learning complexity. For each new task, the main difference between BLRL and BOSS
is that we use the world-model posterior to initialize the task-speciÔ¨Åc posterior, which results in a
new sample complexity bound based on the prior-mass radius."
SAMPLE COMPLEXITY ANALYSIS,0.13677811550151975,"The result formalizes the intuition that, if we put a larger prior mass at a density that is close to the
true q such that dœÄ is small, the sample complexity of our algorithm will be lower. In the meantime,
the sample complexity is bounded by polynomial functions of the relevant quantities, showing that
our training strategy preserves the properties required by PAC-MDP algorithms (Strehl et al., 2009)."
SAMPLE COMPLEXITY ANALYSIS,0.1398176291793313,"4
SCALING UP: VARIATIONAL BAYESIAN LIFELONG RL"
SAMPLE COMPLEXITY ANALYSIS,0.14285714285714285,"Directly computing the exact posterior is typically not possible for large scale problems. Instead,
we propose a practical approximate algorithm, VBLRL, that uses neural networks and variational
inference (Hinton & van Camp, 1993a). We model the posterior via the transition dynamics using
p(st+1, rt|st, at; Œ∏), Œ∏ ‚ààŒò. The posterior, given a new state‚Äìaction pair, can be rewritten via Bayes‚Äô
rule:"
SAMPLE COMPLEXITY ANALYSIS,0.1458966565349544,"p(Œ∏|Dt, at, st+1, rt) = p(Œ∏|Dt)p(st+1, rt|Dt, at; Œ∏)"
SAMPLE COMPLEXITY ANALYSIS,0.14893617021276595,"p(st+1, rt|Dt, at)
,
(5)"
SAMPLE COMPLEXITY ANALYSIS,0.1519756838905775,"where Dt is the agent‚Äôs history with all the experienced tasks up until time step t. As representing
the posterior p(Œ∏|D) is intractable, we approximate it through an alternative distribution q(Œ∏; œÜ) by
minimizing DKL[q(Œ∏; œÜ)||p(Œ∏|D)], leveraging variational lower bounds (Hinton & van Camp, 1993b;
Houthooft et al., 2016)."
SAMPLE COMPLEXITY ANALYSIS,0.15501519756838905,"We choose Bayesian neural networks (BNN) to approximate the posterior. The intuition is that, in the
context of stochastic outputs, BNNs naturally approximate the hierarchical Bayesian model since
they also maintain a learnable distribution over their weights and biases (Graves, 2011; Houthooft
et al., 2016). We expect the uncertainty embedded in the weights and biases of networks can capture
the epistemic uncertainty introduced by hidden parameters of different tasks, while we also set the
outputs of the neural networks to be stochastic to capture the aleatoric uncertainty within each speciÔ¨Åc
task. In our case, the BNN weights and biases distribution q(Œ∏; œÜ) can be modeled as fully factorized
Gaussian distributions (Blundell et al., 2015):"
SAMPLE COMPLEXITY ANALYSIS,0.1580547112462006,"q(Œ∏; œÜ) = |Œò|
Y"
SAMPLE COMPLEXITY ANALYSIS,0.16109422492401215,"i=1
N(Œ∏i|¬µi, œÉ2
i ),
(6)"
SAMPLE COMPLEXITY ANALYSIS,0.1641337386018237,"where œÜ = {¬µ, œÉ}, and ¬µ is the Gaussian‚Äôs mean vector while œÉ is the covariance matrix diagonal.
Then, the posterior distribution over the model parameters can be computed through:"
SAMPLE COMPLEXITY ANALYSIS,0.16717325227963525,"œÜt = arg min
œÜ"
SAMPLE COMPLEXITY ANALYSIS,0.1702127659574468,"h
DKL[q(Œ∏; œÜ)||p(Œ∏)] ‚àíEŒ∏‚àºq(¬∑;œÜ)[log p(st+1, rt|Dt, at; Œ∏)]
i
,
(7)"
SAMPLE COMPLEXITY ANALYSIS,0.17325227963525835,Under review as a conference paper at ICLR 2022
SAMPLE COMPLEXITY ANALYSIS,0.1762917933130699,"where p(Œ∏) represents the Ô¨Åxed prior distribution of Œ∏. The second term on the right hand side can
be approximated through 1"
SAMPLE COMPLEXITY ANALYSIS,0.17933130699088146,"N
PN
i=1 log p(st+1, rt|Dt, at; Œ∏i) with N samples from Œ∏i ‚àºq(œÜ). This
optimization can be performed in parallel for each s, keeping œÜt‚àí1 Ô¨Åxed. ùëÉŒ© ùúîùëö ‚ãØ ‚ãØ ‚ãØ 
ùúá1 ùúáùëë ùúé1 ùúéùëë ‚ãÆ"
SAMPLE COMPLEXITY ANALYSIS,0.182370820668693,"‚ãÆ 
‚ãÆ 
‚ãÆ ùë∫ ùë® ‚ãØ ‚ãØ ‚ãØ 
ùúá1 ùúáùëë ùúé1 ùúéùëë ‚ãÆ"
SAMPLE COMPLEXITY ANALYSIS,0.18541033434650456,"‚ãÆ 
‚ãÆ 
‚ãÆ ùë∫"
SAMPLE COMPLEXITY ANALYSIS,0.1884498480243161,"ùë® 
ùëÉ(ùë†‚Ä≤, ùëü|ùë†, ùëé; ùúîùëö)"
SAMPLE COMPLEXITY ANALYSIS,0.19148936170212766,Task-specific posterior
SAMPLE COMPLEXITY ANALYSIS,0.1945288753799392,"World-model posterior 
Epistemic 
uncertainty"
SAMPLE COMPLEXITY ANALYSIS,0.19756838905775076,"Epistemic 
uncertainty"
SAMPLE COMPLEXITY ANALYSIS,0.2006079027355623,"Aleatory 
uncertainty"
SAMPLE COMPLEXITY ANALYSIS,0.20364741641337386,"Aleatory 
uncertainty"
SAMPLE COMPLEXITY ANALYSIS,0.2066869300911854,"Figure 2: How VBLRL estimates different kinds of uncertainties in HiP-MDP. The world-model
posterior captures the epistemic uncertainty of the general knowledge distribution (shared across all
tasks controlled by the hidden parameters) via the internal variance of world-model BNN. As the
learner is exposed to more and more tasks, the posterior should converge to P‚Ñ¶. The task-speciÔ¨Åc
posterior captures the epistemic uncertainty of the current task m, which comes from the alleatory
uncertainty of the world model when generating œâm for a new task, via the internal variance of
task-speciÔ¨Åc BNN. The posterior should output the highest probability for œâ near the true œâm as the
agent collects enough data from the task. The aleatory uncertainty of the Ô¨Ånal prediction is measured
by the output variance of the prediction."
SAMPLE COMPLEXITY ANALYSIS,0.20972644376899696,"We provide the intuition of how our design capture the uncertainties of lifelong RL in Figure 2 and
summarize the method in Algorithm 1. The left side of the Ô¨Ågure shows the process of how transitions
are generated from the environment‚Äôs true distribution, while the other parts show how our models
generate transition predictions and how they separately estimate different uncertainties generating
from approximating the true underlying distribution. We employ our posterior knowledge models
in the context of a model-based RL method. When encountering a new task, VBLRL Ô¨Årst uses the
model parameters (that is, {¬µ, œÉ} of weights and biases of BNN) from the general knowledge model
to initialize the task-speciÔ¨Åc posterior network. The task-speciÔ¨Åc model outputs the predicted next
state and reward given a state‚Äìaction pair. Then, we use model-predictive control (Garcia et al., 1989)
to select actions based on the generated transitions."
SAMPLE COMPLEXITY ANALYSIS,0.2127659574468085,"For planning, at each step, we begin by creating P particles from the current state sp
œÑ=t = st‚àÄp. Then
we sample N candidate action sequences at:t+T from a learnable distribution. We propagate the
state-action pairs using the learned task-speciÔ¨Åc model pmi(¬∑|s, a) (BNN) and use the cross entropy
method (Botev et al., 2013) to update the sampling distribution to make the sampled action sequences
close to previous action sequences that achieved high reward. We further calculate the cumulative
reward estimated (via the learned model) for previously sampled sequences and use the mean of that
distribution to select the current action."
SAMPLE COMPLEXITY ANALYSIS,0.21580547112462006,"The task-speciÔ¨Åc posterior is updated using the data collected from only the current task. The
world-model posterior is updated after a few more steps with the collected transitions from all
the visited tasks. The intuition is to guide the two posteriors to separately learn two categories of
uncertainty within lifelong learning tasks. Note that other CEM-based model-based RL algorithms
like PETS (Chua et al., 2018) usually maintain a set of neural networks using the same training
data, and sample action sequences from each of the neural nets to achieve randomness in transitions.
Besides the problem of requiring special training tricks, it is unrealistic to maintain (‚â•30) models for
each task in lifelong RL settings. Our usage of BNNs avoids such problems as we only have to train
one neural network using the same data for each task, and we can sample an unlimited number of
different action sequences to cover more possibilities as needed. In PETS, the epistemic uncertainty"
SAMPLE COMPLEXITY ANALYSIS,0.2188449848024316,Under review as a conference paper at ICLR 2022
SAMPLE COMPLEXITY ANALYSIS,0.22188449848024316,"is estimated via the variance of the output mean of different neural networks, while in VBLRL, it
is estimated via the variance of the weights and biases distribution of BNN during training. This is
implied as the objective function we use to update œÜt is from minimizing DKL[q(Œ∏; œÜ)||p(Œ∏|D)]."
SAMPLE COMPLEXITY ANALYSIS,0.22492401215805471,"Algorithm 1: Variational Bayesian Lifelong RL
Initialize general knowledge model pwm(¬∑|s, a; Œ∏wm), replay buffer Dm1, ¬∑ ¬∑ ¬∑ , DmM
for each task mi from i = 1, 2, 3, ¬∑ ¬∑ ¬∑ , M do"
SAMPLE COMPLEXITY ANALYSIS,0.22796352583586627,"Initialize task-speciÔ¨Åc model pmi(¬∑|s, a; Œ∏mi) with general knowledge model pwm
for each episode do"
SAMPLE COMPLEXITY ANALYSIS,0.23100303951367782,for Time t = 0 to TaskHorizon do
SAMPLE COMPLEXITY ANALYSIS,0.23404255319148937,"Sample Actions at:t+T ‚àºCEM(¬∑)
Propagate state particles sp
œÑ with pmi(s‚Ä≤|s, a)
Evaluate actions as Pt+T
œÑ=t
1
P
PP
p=1 pmi(r|s, a)
Update CEM(¬∑) distribution.
Execute optimal actions a‚àó
t:t+T
end
Add transitions to replay buffer Dmi
Update task-speciÔ¨Åc model according to Equation (7) given Dmi
Update general knowledge model according to Equation (7) given {Dm1, ¬∑ ¬∑ ¬∑ , Dmi}
end
end"
BACKWARD TRANSFER OF VARIATIONAL BAYESIAN LIFELONG RL,0.23708206686930092,"4.1
BACKWARD TRANSFER OF VARIATIONAL BAYESIAN LIFELONG RL"
BACKWARD TRANSFER OF VARIATIONAL BAYESIAN LIFELONG RL,0.24012158054711247,"In our lifelong RL setting, the agent interacts with each task for only a limited number of episodes
and the task-speciÔ¨Åc model stops learning when the next task is initiated. As a result, there may
exist portions of the transition dynamics in which model uncertainty remains high. However, as the
world-model posterior continues to train on new tasks, it gathers more experience in the whole state
space and can provide improvements in its guesses concerning the ‚Äúunknown‚Äù transition dynamics,
even for previously encountered tasks."
BACKWARD TRANSFER OF VARIATIONAL BAYESIAN LIFELONG RL,0.24316109422492402,"Intuitively, the performance of an agent on one task has the potential to be further improved (positive
backward transfer) if there exists a sufÔ¨Åciently large set of state‚Äìaction transition pairs of which the
task-speciÔ¨Åc model‚Äôs predictions are not conÔ¨Ådent due to lack of data. This type of model uncertainty
is sometimes called epistemic uncertainty (Kiureghian & Ditlevsen, 2009; Ciosek et al., 2020). In
our algorithm, the aleatory variability (irreducible chance in the outcome) is measured by the output
variance of the prediction {œÉrp
œÑ , œÉsp
œÑ }, and the epistemic uncertainty (due to lack of experience)
corresponds to the uncertainty of the output mean and variance (see DeÔ¨Ånition 1 below). Thus, a
straightforward method to improve a previously learned task-speciÔ¨Åc model is to Ô¨Ånd the predictions
it needs to make that have high epistemic uncertainty, and replace them with the predictions from the
world-model posterior, which has lower epistemic uncertainty. If we only consider reward prediction,
the conditions for measuring whether a task-speciÔ¨Åc model is sufÔ¨Åciently conÔ¨Ådent are as follows.
DeÔ¨Ånition 1. Assume there exist known constants Œ¥¬µr, Œ¥œÉr. For a given state‚Äìaction pair (s, a), the
task-speciÔ¨Åc model (reward) is proclaimed conÔ¨Ådent when the following conditions are satisÔ¨Åed:
PP
p=1(¬µrp
œÑ ‚àí¬µrp
œÑ )2"
BACKWARD TRANSFER OF VARIATIONAL BAYESIAN LIFELONG RL,0.24620060790273557,"P ‚àí1
< Œ¥¬µr,"
BACKWARD TRANSFER OF VARIATIONAL BAYESIAN LIFELONG RL,0.24924012158054712,"PP
p=1(œÉrp
œÑ ‚àíœÉrp
œÑ )2"
BACKWARD TRANSFER OF VARIATIONAL BAYESIAN LIFELONG RL,0.25227963525835867,"P ‚àí1
< Œ¥œÉr,
(8)"
BACKWARD TRANSFER OF VARIATIONAL BAYESIAN LIFELONG RL,0.2553191489361702,"where P is the number of particles. Similar deÔ¨Ånition applies to the task-speciÔ¨Åc model‚Äôs next-state
prediction. Intuitively, Œ¥¬µr and Œ¥œÉr function as the threshold to judge whether the uncertainty of the
output mean or variance for each dynamic prediction is too high to be called as a conÔ¨Ådent prediction."
BACKWARD TRANSFER OF VARIATIONAL BAYESIAN LIFELONG RL,0.25835866261398177,"The detailed backward transfer testing algorithm can be found in the appendix. In practice, it is
often hard to Ô¨Ånd speciÔ¨Åc conÔ¨Ådence thresholds (Œ¥¬µs, Œ¥¬µr, Œ¥œÉs, Œ¥œÉr) that are effective. Instead, we
implement a simpler approach: During planning, for each prediction, we compare the uncertainty of
the output mean and variance of the world model and the task-speciÔ¨Åc model, and then choose the
one with lower values, which indicates higher conÔ¨Ådence level."
BACKWARD TRANSFER OF VARIATIONAL BAYESIAN LIFELONG RL,0.2613981762917933,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.26443768996960487,"5
EXPERIMENTS"
GRID-WORLD ITEM SEARCHING,0.2674772036474164,"5.1
GRID-WORLD ITEM SEARCHING"
GRID-WORLD ITEM SEARCHING,0.270516717325228,"We Ô¨Årst evaluate BLRL in a simple Grid-World domain. Our testbed consists of a collection of
houses, each of which has four rooms. The goal of each task is to Ô¨Ånd a speciÔ¨Åc object (blue, green or
purple) in the current house. The type of each room is sampled based on an underlying distribution
given by the environment. Each room type has a corresponding probability distribution of which kind
of objects can be found in rooms of this type. Different tasks/houses vary in terms of which rooms
are which types and precisely where objects are located in the room (the task‚Äôs hidden parameters)."
GRID-WORLD ITEM SEARCHING,0.2735562310030395,"To simplify the problem, instead of modeling the whole MDP distribution, we use BLRL to model
the object distribution as the Bayesian posterior and sample MDPs from the distribution. We use
BOSS with a Ô¨Åxed prior (no intertask transfer) as our baseline. The average training performance of
all 300 tasks are shown in Figure 3 top right. Each task consists of 10 epochs, with 21 sample steps
for each epoch. Within the limited steps allotted for each task, BLRL is able to discover and transfer
the common knowledge and helps the agent quickly adapt to new tasks as the training goes on. In
comparison, running BOSS with a Ô¨Åxed prior is able to Ô¨Ånd the optimal policy eventually but needs
more sample steps and learns more slowly than BLRL."
GRID-WORLD ITEM SEARCHING,0.2765957446808511,"Figure 3: Top-left: Grid-World Item Searching; Top-right: Grid-World Item Searching evaluation
results; Bottom-left: Box-jumping Task; Bottom-right: Box-jumping Task evaluation results."
BOX-JUMPING TASK,0.2796352583586626,"5.2
BOX-JUMPING TASK"
BOX-JUMPING TASK,0.2826747720364742,"We use a simpliÔ¨Åed version of the jumping task (des Combes et al., 2018) as a testbed for the proposed
algorithm VBLRL. As shown in Figure 3 bottom left, the goal of the agent is to reach the right side of
the screen by jumping over the obstacle. The agent can only choose from two actions: jump and right.
It will hit the obstacle unless the jump action is chosen at precisely the right time. We set different
obstacle positions as different tasks, constituting the HiP-MDP hidden parameters. The 4-element
state vector describes the (x, y) coordinates of the agent‚Äôs current position, and its velocity in the x
and y directions."
BOX-JUMPING TASK,0.2857142857142857,"Figure 3 bottom right presents the average performance during training across all 300 tasks. Each
task is run for 30 episodes. We compare VBLRL against state-of-the-art lifelong RL methods LPG-
FTW (Mendez et al., 2020) from the multi-model category, EWC (Kirkpatrick et al., 2016) from
the single-model category, singel-task model-base RL baseline (using the same BNN structure and
planning procedures) as well as a HiP-MDP baseline (Killian et al., 2017). For a fair comparison, we
further replace the DDQN algorithm (van Hasselt et al., 2016) used in Killian et al.‚Äôs paper with CEM
planning, and let the transition model also predict the reward for each state‚Äìaction pair. This modiÔ¨Åed"
BOX-JUMPING TASK,0.2887537993920973,Under review as a conference paper at ICLR 2022
BOX-JUMPING TASK,0.2917933130699088,"baseline is similar to the single-model version of VBLRL (i.e., only using the world-model posterior).
VBLRL clearly learns faster than the HiP-MDP baseline and reaches better Ô¨Ånal performance. Thus,
in the lifelong RL setting, separating the updating processes of the world-model posterior and the
task-speciÔ¨Åc posterior can lead to better learning efÔ¨Åciency."
OPENAI GYM MUJOCO DOMAINS,0.2948328267477204,"5.3
OPENAI GYM MUJOCO DOMAINS"
OPENAI GYM MUJOCO DOMAINS,0.2978723404255319,"We evaluate the performance of VBLRL on HiP-MDP versions of several continuous control tasks
from the Mujoco physics simulator (Todorov et al., 2012), HalfCheetah-gravity, HalfCheetah-
bodyparts, Hopper-gravity, Hopper-bodyparts, Walker-gravity, Walker-bodyparts, all of which are
lifelong-RL benchmarks used in prior work1 (Mendez et al., 2020). For each of six different domains,
the task-speciÔ¨Åc hidden parameters correspond to different gravity values or different sizes and
masses of the simulated body parts. More details can be found in the appendix. Compared with prior
work, we substantially reduced the number of iterations that the agent can sample and train on (100
iterations for each task and a horizon of 100/200 for each iteration). We used such settings to increase
the difÔ¨Åculty of lifelong learning and ensure that no learning-from-scratch algorithm could obtain a
good policy in the available training time."
OPENAI GYM MUJOCO DOMAINS,0.3009118541033435,"VBLRL
HiP-MDP baseline
LPG-FTW
EWC
Single-task MBRL"
OPENAI GYM MUJOCO DOMAINS,0.303951367781155,"CG-Start
160.68 ¬± 48.80
126.95 ¬± 31.41
‚àí81.59 ¬± 9.18
‚àí3426.76 ¬± 827.99
‚àí83.96 ¬± 60.10
CG-Train
226.72 ¬± 26.53
170.20 ¬± 39.92
‚àí29.49 ¬± 11.03
‚àí3440.66 ¬± 1007.50
‚àí40.47 ¬± 10.68
CG-Back
231.79 ¬± 23.49
97.84 ¬± 22.04
‚àí29.95 ¬± 11.64
‚àí6672.33 ¬± 3748.63
/
CB-Start
110.74 ¬± 41.96
78.95 ¬± 18.43
‚àí263.94 ¬± 40.80
‚àí5016.93 ¬± 1708.10
‚àí101.02 ¬± 39.11
CB-Train
173.97 ¬± 78.26
87.2 ¬± 9.42
‚àí217.86 ¬± 42.82
‚àí5454.52 ¬± 2145.82
‚àí58.93 ¬± 33.24
CB-Back
181.60 ¬± 67.50
116.03 ¬± 17.35
‚àí116.41 ¬± 65.64
‚àí13889.31 ¬± 6851.05
/
HG-Start
‚àí149.79 ¬± 28.7
‚àí130.54 ¬± 14.86
16.15 ¬± 22.83
‚àí614.80 ¬± 600.14
‚àí408.46 ¬± 36.10
HG-Train
‚àí125.40 ¬± 9.17
‚àí110.19 ¬± 22.03
22.98 ¬± 34.34
‚àí749.49 ¬± 654.04
‚àí386.77 ¬± 65.77
HG-Back
‚àí55.95 ¬± 74.32
‚àí142.82 ¬± 16.30
‚àí252.36 ¬± 106.91
‚àí5816.74 ¬± 4103.66
/
HB-Start
‚àí119.29 ¬± 17.15
‚àí73.41 ¬± 36.99
‚àí15.56 ¬± 48.63
‚àí4701.72 ¬± 1527.74
‚àí402.03 ¬± 30.03
HB-Train
‚àí99.00 ¬± 8.05
‚àí96.68 ¬± 38.725
41.29 ¬± 12.19
‚àí7384.54 ¬± 3232.86
‚àí394.77 ¬± 25.77
HB-Back
‚àí76.47 ¬± 13.27
‚àí92.45 ¬± 41.50
‚àí186.08 ¬± 151.82
‚àí7921.96 ¬± 1147.94
/
WG-Start
‚àí19.53 ¬± 4.76
‚àí20.86 ¬± 5.37
‚àí290.35 ¬± 70.95
‚àí467.54 ¬± 249.19
‚àí440.89 ¬± 59.41
WG-Train
7.77 ¬± 6.38
1.57 ¬± 2.97
‚àí94.34 ¬± 61.36
‚àí361.65 ¬± 260.76
‚àí359.82 ¬± 45.44
WG-Back
18.56 ¬± 7.42
9.79 ¬± 6.32
‚àí90.41 ¬± 64.66
‚àí734.90 ¬± 413.40
/
WB-Start
‚àí43.01 ¬± 10.82
‚àí64.62 ¬± 28.02
‚àí315.60 ¬± 31.66
‚àí1140.62 ¬± 180.21
‚àí437.96 ¬± 26.24
WB-Train
‚àí2.66 ¬± 3.67
‚àí31.29 ¬± 30.07
‚àí187.98 ¬± 72.03
‚àí1131.03 ¬± 451.38
‚àí367.79 ¬± 74.37
WB-Back
4.04 ¬± 2.95
‚àí33.75 ¬± 40.00
‚àí66.97 ¬± 74.71
-‚àí2563.70 ¬± 692.70
/"
OPENAI GYM MUJOCO DOMAINS,0.3069908814589666,"Table 1: Results on OpenAI Gym Mujoco domains. CG denotes Cheetah-Gravity, CB denotes
Cheetah-Bodyparts, HG denotes Hopper-Gravity, HB denotes Hopper-Bodyparts, WG denotes
Walker-Gravity, WB denotes Walker-Bodyparts."
OPENAI GYM MUJOCO DOMAINS,0.3100303951367781,"The results are shown in Table 1. We compare our algorithm against the three algorithms described in
Section 5.2. For all six domains, we report the average performance of all the tasks at the beginning
of training (Start) and after all training for each new task (Train), as well as the average performance
for all previous tasks after training for a given number of tasks, which is the backward transfer test
(Back). As shown in the results, our method VBLRL shows better performance on all three test stages
of the HalfCheetah domain and Walker domain, as well as better backward transfer performance on
Hopper-gravity and Hopper-bodyparts than the other three algorithms. LPG-FTW exhibits better
forward training performance in the Hopper domain, but still shows some signs of catastrophic
forgetting as there is a huge gap between its training performance and backward performance. EWC
fails in most of the tasks as the tasks are diverse and we set very limited sample steps for each task,
which means it is hard to directly learn a single shared policy that achieves good performance. The
HiP-MDP baseline shows good results on some of the tasks because it is more sample-efÔ¨Åcient to
learn a shared model across all the tasks and easier to capture the world-model uncertainty. However,
it cannot achieve as good performance as VBLRL as it is hard to model the task-speciÔ¨Åc uncertainty
using only one model across all tasks, which also leads to the negative backward transfer performance
on Cheetah-Gravity. Comparing VBLRL‚Äôs performance on the Train stage and Back stage, we also
Ô¨Ånd that it shows positive backward transfer results on most of the tasks, without showing patterns
of catastrophic forgetting. Overall, VBLRL‚Äôs world-model posterior contributes to better forward"
WE CHANGED THE ENVIRONMENT SETTINGS FOR HOPPER AND WALKER TO MAKE THEM AMENABLE TO MODEL-BASED RL,0.3130699088145897,"1We changed the environment settings for Hopper and Walker to make them amenable to model-based RL
following Wang et al. (2019)."
WE CHANGED THE ENVIRONMENT SETTINGS FOR HOPPER AND WALKER TO MAKE THEM AMENABLE TO MODEL-BASED RL,0.3161094224924012,Under review as a conference paper at ICLR 2022
WE CHANGED THE ENVIRONMENT SETTINGS FOR HOPPER AND WALKER TO MAKE THEM AMENABLE TO MODEL-BASED RL,0.3191489361702128,"transfer performance (Start), the learning of task-speciÔ¨Åc posterior contributes to better forward
transfer training for each new task (Train), and the combination of these two posteriors guides the
agent to achieve better backward transfer performance (Back)."
RELATED WORK,0.3221884498480243,"6
RELATED WORK"
RELATED WORK,0.3252279635258359,"HiP-MDPs (Doshi-Velez & Konidaris, 2016) provide a framework for studying lifelong RL. Published
HiP-MDP methods use Gaussian Processes (Doshi-Velez & Konidaris, 2016) or Bayesian neural
networks (Killian et al., 2017) to Ô¨Ånd a single model that works for all tasks, which may trigger
catastrophic forgetting. Other single-model lifelong RL algorithms encourage transfer across tasks
by modifying objective functions. EWC (Kirkpatrick et al., 2016) imposes a quadratic penalty
that pulls each weight back towards its old values by an amount proportional to its importance for
performance on previously-learned tasks to avoid forgetting. There are several extensions of this work
based on the core idea of modifying the form of the penalty (Li & Hoiem, 2017; Zenke et al., 2017;
Nguyen et al., 2018). Another category of lifelong RL methods uses multiple models with shared
parameters and task-speciÔ¨Åc parameters to avoid or alleviate the catastrophic problem (Bou-Ammar
et al., 2014; Isele et al., 2016a; Mendez et al., 2020). The drawback of this method is that it is hard to
incorporate the knowledge learned from previous tasks during initial training on a new task (Mendez
et al., 2020).Nagabandi et al. (2019) introduce a model-based continual learning framework based
on MAML, but they focus on discovering when new tasks were encountered without access to task
indicators."
RELATED WORK,0.3282674772036474,"Research in Meta-RL (Wang et al., 2016; Finn et al., 2017) and multi-task RL (Parisotto et al., 2016;
Teh et al., 2017) settings also attempts to Ô¨Ånd ways to accelerate learning by transferring knowledge
from different tasks. Some work employs the MAML framework with Bayesian methods to learn
a stochastic distribution over initial parameter (Yoon et al., 2018; Grant et al., 2018; Finn et al.,
2018). Other work uses the collected trajectories to infer the hidden parameter, which is taken as
an additional input when computing the policy (Rakelly et al., 2019; Zintgraf et al., 2020; Fu et al.,
2021). Our method, however, focuses on problems where the tasks arrive sequentially instead of
having a large number of tasks available at the beginning of training. This sequential setting makes
it hard to accurately infer the hidden parameters, but opens the door for algorithms that support
backward transfer. Further, our method approximates the true HiP-MDP model by learning the
Bayesian posterior over past tasks and uses it to initialize a model for each new task, encouraging the
agent to explore places where the epistemic uncertainty of the world model is high."
RELATED WORK,0.331306990881459,"Some prior work uses Bayesian methods in RL to quantify uncertainty over initial MDP mod-
els (Ghavamzadeh et al., 2015; Asmuth & Littman, 2011; Guez et al., 2012). Several algorithms
start from the idea of sampling from a posterior over MDPs for Bayesian RL, maintaining Bayesian
posteriors and sampling one complete MDP (Strens, 2000; Wilson et al., 2007a) or multiple MDPs
from this distribution (Asmuth et al., 2009). Instead of focusing on single-task RL, our algorithm
aims to Ô¨Ånd a posterior over the common structure among multiple tasks. Wilson et al. (2007a) uses a
hierarchical Bayesian inÔ¨Ånite mixture model to learn a strong prior that allows the agent to rapidly
infer the characteristics of new environment based on previous tasks. However, it only infers the
category label of a new MDP and uses that information to Ô¨Ånd parameter values. Moreover, their
method only works in discrete settings and cannot be applied to the kind of continuous problems
we included in our evaluation. Lifelong learning has also been widely studied within the supervised
learning domain with explicit performance bounds (Baxter, 2000; Pentina & Lampert, 2014). Our
work is one of the Ô¨Årst papers to give explicit sample complexity bounds for lifelong reinforcement
learning algorithm, where data efÔ¨Åciency is essentially important."
CONCLUSION,0.3343465045592705,"7
CONCLUSION"
CONCLUSION,0.3373860182370821,"To address the lifelong RL problems, our work proposed to distill the shared knowledge from similar
MDPs and maintain a Bayesian posterior to approximate the distribution derived from that knowledge.
We gave a sample complexity analysis of the algorithm in the Ô¨Ånite MDP setting. Then, we extended
our method to use variational inference, which scales better and supports both backward and forward
transfer. Our experimental results show that the proposed algorithms enables faster training on new
tasks through collecting and transferring the knowledge learned from preceding tasks."
CONCLUSION,0.3404255319148936,Under review as a conference paper at ICLR 2022
REFERENCES,0.3434650455927052,REFERENCES
REFERENCES,0.3465045592705167,"J. Asmuth and M. Littman. Learning is planning: Near Bayes-optimal reinforcement learning via
Monte-Carlo tree search. In UAI, 2011."
REFERENCES,0.3495440729483283,"John Asmuth, Lihong Li, Michael L. Littman, Ali Nouri, and David Wingate. A Bayesian sampling
approach to exploration in reinforcement learning. In UAI 2009, Proceedings of the Twenty-Fifth
Conference on Uncertainty in ArtiÔ¨Åcial Intelligence, 2009, pp. 19‚Äì26. AUAI Press, 2009."
REFERENCES,0.3525835866261398,"Jonathan Baxter. A model of inductive bias learning. J. Artif. Intell. Res., 12:149‚Äì198, 2000."
REFERENCES,0.3556231003039514,"Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. CoRR, abs/1505.05424, 2015."
REFERENCES,0.3586626139817629,"Z. Botev, Dirk P. Kroese, R. Rubinstein, and P. L‚ÄôEcuyer. Chapter 3 ‚Äì the cross-entropy method for
optimization. Handbook of Statistics, 31:35‚Äì59, 2013."
REFERENCES,0.3617021276595745,"Haitham Bou-Ammar, Eric Eaton, P. Ruvolo, and Matthew E. Taylor. Online multi-task learning for
policy gradient methods. In ICML, 2014."
REFERENCES,0.364741641337386,"Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for
near-optimal reinforcement learning. 3(null), 2003. ISSN 1532-4435."
REFERENCES,0.3677811550151976,"Emma Brunskill and Lihong Li. Pac-inspired option discovery in lifelong reinforcement learning. In
ICML, 2014."
REFERENCES,0.3708206686930091,"G. Carpenter and S. Grossberg. The art of adaptive pattern recognition by a self-organizing neural
network. Computer, 21:77‚Äì88, 1988."
REFERENCES,0.3738601823708207,"Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, pp. 4759‚Äì4770, 2018."
REFERENCES,0.3768996960486322,"Kamil Ciosek, Vincent Fortuin, Ryota Tomioka, Katja Hofmann, and Richard E. Turner. Conservative
uncertainty estimation by Ô¨Åtting prior networks. In 8th International Conference on Learning
Representations, ICLR 2020. OpenReview.net, 2020."
REFERENCES,0.3799392097264438,"Remi Tachet des Combes, Philip Bachman, and Harm van Seijen. Learning invariances for pol-
icy generalization. In 6th International Conference on Learning Representations, ICLR 2018.
OpenReview.net, 2018."
REFERENCES,0.3829787234042553,"Finale Doshi-Velez and George Dimitri Konidaris. Hidden parameter markov decision processes: A
semiparametric regression approach for discovering latent task parametrizations. In Proceedings
of the Twenty-Fifth International Joint Conference on ArtiÔ¨Åcial Intelligence, IJCAI 2016, pp.
1432‚Äì1440. IJCAI/AAAI Press, 2016."
REFERENCES,0.3860182370820669,"Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and P. Abbeel. Benchmarking deep reinforce-
ment learning for continuous control. In ICML, 2016."
REFERENCES,0.3890577507598784,"Chelsea Finn, P. Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In ICML, 2017."
REFERENCES,0.39209726443769,"Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In
Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information
Processing Systems 2018, NeurIPS 2018, pp. 9537‚Äì9548, 2018."
REFERENCES,0.3951367781155015,"Robert M. French. Catastrophic interference in connectionist networks: Can it be predicted, can it
be prevented? In Advances in Neural Information Processing Systems 6, [7th NIPS Conference,
1993], pp. 1176‚Äì1177. Morgan Kaufmann, 1993."
REFERENCES,0.3981762917933131,"Haotian Fu, Hongyao Tang, Jianye Hao, Chen Chen, Xidong Feng, Dong Li, and Wulong Liu.
Towards effective context for meta-reinforcement learning: an approach based on contrastive
learning. In Thirty-Fifth AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2021, pp. 7457‚Äì7465.
AAAI Press, 2021."
REFERENCES,0.4012158054711246,Under review as a conference paper at ICLR 2022
REFERENCES,0.40425531914893614,"C. E. Garcia, D. M. Prett, and M. Morari. Model predictive control: Theory and practice - a survey.
Autom., 25:335‚Äì348, 1989."
REFERENCES,0.4072948328267477,"Mohammed Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Convex optimization:
Algorithms and complexity. Foundations and Trends¬Æ in Machine Learning, 8(5-6):359‚Äì483,
2015. ISSN 1935-8245."
REFERENCES,0.41033434650455924,"Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas L. GrifÔ¨Åths. Recasting
gradient-based meta-learning as hierarchical bayes. In 6th International Conference on Learning
Representations, ICLR 2018. OpenReview.net, 2018."
REFERENCES,0.4133738601823708,"Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information
Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011,
pp. 2348‚Äì2356, 2011."
REFERENCES,0.41641337386018235,"A. Guez, D. Silver, and P. Dayan. EfÔ¨Åcient bayes-adaptive reinforcement learning using sample-based
search. In NIPS, 2012."
REFERENCES,0.4194528875379939,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th
International Conference on Machine Learning, ICML 2018, pp. 1856‚Äì1865. PMLR, 2018."
REFERENCES,0.42249240121580545,"Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing
the description length of the weights. In Proceedings of the Sixth Annual ACM Conference on
Computational Learning Theory, COLT 1993, pp. 5‚Äì13. ACM, 1993a."
REFERENCES,0.425531914893617,"Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In COLT ‚Äô93, 1993b."
REFERENCES,0.42857142857142855,"Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME:
variational information maximizing exploration. In Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016, pp. 1109‚Äì1117,
2016."
REFERENCES,0.4316109422492401,"David Isele, Mohammad Rostami, and Eric Eaton. Using task features for zero-shot knowledge
transfer in lifelong learning. In Proceedings of the Twenty-Fifth International Joint Conference on
ArtiÔ¨Åcial Intelligence, IJCAI 2016, pp. 1620‚Äì1626. IJCAI/AAAI Press, 2016a."
REFERENCES,0.43465045592705165,"David Isele, Mohammad Rostami, and Eric Eaton. Using task features for zero-shot knowledge
transfer in lifelong learning. In IJCAI, 2016b."
REFERENCES,0.4376899696048632,"Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. Reinforcement learning: A
survey. J. Artif. Intell. Res., 4:237‚Äì285, 1996."
REFERENCES,0.44072948328267475,"Taylor W. Killian, Samuel Daulton, Finale Doshi-Velez, and George Dimitri Konidaris. Robust
and efÔ¨Åcient transfer learning with hidden parameter markov decision processes. In Advances in
Neural Information Processing Systems 30: Annual Conference on Neural Information Processing
Systems 2017, pp. 6250‚Äì6261, 2017."
REFERENCES,0.44376899696048633,"James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic
forgetting in neural networks. CoRR, abs/1612.00796, 2016."
REFERENCES,0.44680851063829785,"Armen Der Kiureghian and O. Ditlevsen. Aleatory or epistemic? does it matter? Structural Safety,
31:105‚Äì112, 2009."
REFERENCES,0.44984802431610943,"Zhizhong Li and Derek Hoiem. Learning without forgetting, 2017."
REFERENCES,0.45288753799392095,"David Lopez-Paz and Marc‚ÄôAurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, pp. 6467‚Äì6476, 2017."
REFERENCES,0.45592705167173253,Under review as a conference paper at ICLR 2022
REFERENCES,0.45896656534954405,"Jorge Armando Mendez Mendez, Boyu Wang, and Eric Eaton. Lifelong policy gradient learning of
factored policies for faster training without forgetting. ArXiv, abs/2007.07011, 2020."
REFERENCES,0.46200607902735563,"Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning:
Continual adaptation for model-based rl. ArXiv, abs/1812.07671, 2019."
REFERENCES,0.46504559270516715,"Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning,
2018."
REFERENCES,0.46808510638297873,"Emilio Parisotto, Lei Jimmy Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and
transfer reinforcement learning. In 4th International Conference on Learning Representations,
ICLR 2016, 2016."
REFERENCES,0.47112462006079026,"Anastasia Pentina and Christoph H. Lampert. A pac-bayesian bound for lifelong learning. ArXiv,
abs/1311.2838, 2014."
REFERENCES,0.47416413373860183,"Martin L. Puterman. Markov Decision Processes‚ÄîDiscrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., 1994."
REFERENCES,0.47720364741641336,"Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen.
EfÔ¨Åcient off-
policy meta-reinforcement learning via probabilistic context variables. In Proceedings of the
36th International Conference on Machine Learning, ICML 2019, volume 97 of Proceedings of
Machine Learning Research, pp. 5331‚Äì5340. PMLR, 2019."
REFERENCES,0.48024316109422494,"Anthony V. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connect. Sci., 7:123‚Äì146,
1995."
REFERENCES,0.48328267477203646,"D. Silver, Julian Schrittwieser, K. Simonyan, Ioannis Antonoglou, Aja Huang, A. Guez, Thomas
Hubert, Lucas baker, Matthew Lai, A. Bolton, Yutian Chen, T. Lillicrap, Fan Hui, L. Sifre, G. V. D.
Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge.
Nature, 550:354‚Äì359, 2017."
REFERENCES,0.48632218844984804,"A. Strehl, Lihong Li, and M. Littman. Incremental model-based learners with formal learning-time
guarantees. ArXiv, abs/1206.6870, 2006."
REFERENCES,0.48936170212765956,"Alexander L. Strehl, Lihong Li, and Michael L. Littman. Reinforcement learning in Ô¨Ånite mdps: Pac
analysis. J. Mach. Learn. Res., 10:2413‚Äì2444, 2009."
REFERENCES,0.49240121580547114,"Malcolm J. A. Strens. A Bayesian framework for reinforcement learning. In Proceedings of the
Seventeenth International Conference on Machine Learning (ICML 2000), pp. 943‚Äì950, 2000."
REFERENCES,0.49544072948328266,"Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,
1998."
REFERENCES,0.49848024316109424,"Yee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell,
Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In
Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, pp. 4496‚Äì4506, 2017."
REFERENCES,0.5015197568389058,"William R. Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25:285‚Äì294, 1933."
REFERENCES,0.5045592705167173,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, pp.
5026‚Äì5033. IEEE, 2012."
REFERENCES,0.5075987841945289,"Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the Thirtieth AAAI Conference on ArtiÔ¨Åcial Intelligence,2016, pp.
2094‚Äì2100. AAAI Press, 2016."
REFERENCES,0.5106382978723404,"Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, R¬¥emi Munos,
Charles Blundell, Dharshan Kumaran, and Matthew Botvinick. Learning to reinforcement learn.
CoRR, abs/1611.05763, 2016."
REFERENCES,0.513677811550152,Under review as a conference paper at ICLR 2022
REFERENCES,0.5167173252279635,"Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement
learning. CoRR, abs/1907.02057, 2019."
REFERENCES,0.5197568389057751,"Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning:
A hierarchical Bayesian approach. In Machine Learning, Proceedings of the Twenty-Fourth
International Conference (ICML 2007), 2007, volume 227 of ACM International Conference
Proceeding Series, pp. 1015‚Äì1022. ACM, 2007a."
REFERENCES,0.5227963525835866,"Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a
hierarchical bayesian approach. In ICML ‚Äô07, 2007b."
REFERENCES,0.5258358662613982,"Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. In Advances in Neural Information Processing Systems
31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, pp.
7343‚Äì7353, 2018."
REFERENCES,0.5288753799392097,"Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence,
2017."
REFERENCES,0.5319148936170213,"Tong Zhang. From -entropy to kl-entropy: Analysis of minimum information complexity density
estimation. Annals of Statistics, 34:2180‚Äì2210, 2006."
REFERENCES,0.5349544072948328,"Luisa M. Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,
and Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep RL via meta-learning.
In 8th International Conference on Learning Representations, ICLR 2020. OpenReview.net, 2020."
REFERENCES,0.5379939209726444,Under review as a conference paper at ICLR 2022
REFERENCES,0.541033434650456,"A
APPENDIX"
REFERENCES,0.5440729483282675,"A.1
BACKWARD TRANSFER"
REFERENCES,0.547112462006079,"We take backward transfer to be the inÔ¨Çuence of subsequent learning on the agent‚Äôs performance of a
previous task. Negative backward transfer, which is also known as catastrophic forgetting, describes
the scenario in which the agent‚Äôs progress on a task is erased after learning new tasks (French, 1993;
Carpenter & Grossberg, 1988; Robins, 1995). Our algorithm avoids catastrophic forgetting as it
maintains a separate task-speciÔ¨Åc model for each task. In the mean time, there are circumstances in
which VBLRL exhibits positive backwards transfer as introduced in the main text."
REFERENCES,0.5501519756838906,"Algorithm 2: Variational Bayesian Lifelong RL (Backward transfer)
input :Test task mi, model uncertainty thresholds Œ¥¬µs, Œ¥¬µr, Œ¥œÉs, Œ¥œÉr
for Time t = 0 to TaskHorizon do"
REFERENCES,0.5531914893617021,for Trial k = 1 to K do
REFERENCES,0.5562310030395137,"Sample Actions at:t+T ‚àºCEM(¬∑)
for each action do"
REFERENCES,0.5592705167173252,"Propagate state particles sp
œÑ and reward rp
œÑ with Tmi(s‚Ä≤, r|s, a)
if Tmi(s‚Ä≤, r|s, a) is not conÔ¨Ådent (DeÔ¨Ånition 1) then"
REFERENCES,0.5623100303951368,"Repropagate state particles sp
œÑ and reward rp
œÑ with Te(s‚Ä≤, r|s, a)
end
end
Evaluate actions as Pt+T
œÑ=t
1
P
PP
p=1 Tmi(r|s, a)
Update CEM(¬∑) distribution.
end
Execute optimal actions a‚àó
t:t+T
end"
REFERENCES,0.5653495440729484,"The difference between the backward transfer version and the forward training version is that, when
we do predictions for the state particles, each time we will also calculate the conÔ¨Ådence level
(DeÔ¨Ånition 1) of the task-speciÔ¨Åc model for this particular transition. If the task-speciÔ¨Åc model is not
conÔ¨Ådent enough for this state‚Äìaction pair, we will use the world-model instead and recalculate the
prediction value."
REFERENCES,0.5683890577507599,"In practice, it is often hard to Ô¨Ånd speciÔ¨Åc conÔ¨Ådence thresholds (Œ¥¬µs, Œ¥¬µr, Œ¥œÉs, Œ¥œÉr) that are effective.
Instead, we implement a simpler approach: During planning, for each prediction, we compare the
uncertainty of the output mean and variance of the world model and the task-speciÔ¨Åc model, and
then choose the one with lower values, which indicates higher conÔ¨Ådence level. Here is a link to the
learned behaviors of VBLRL https://youtu.be/I7RAT6g9v5w. The video shows the agent
trained by our algorithm learns some meaningful behaviors at the current setting within limited steps,
and performs better compared with the behaviors learned by other baseline algorithms."
REFERENCES,0.5714285714285714,"A.2
BNN MODEL"
REFERENCES,0.574468085106383,As we model the transition models as Gaussian distributions:
REFERENCES,0.5775075987841946,"T(¬∑|s, a) = N(f ¬µ
Œ∏ (s, a), f œÉ
Œ∏ (s, a))
(9)"
REFERENCES,0.5805471124620061,"The function fŒ∏ represented as a Bayesian neural network parameterized by Œ∏, which is further
modeled as the posterior distribution parameterized by œÜ, predicts the mean ¬µs, ¬µr and variance
œÉs, œÉr given current state and action s, a. To make it clearer, we can view the BNN model in VBLRL
as an inÔ¨Ånite neural network ensemble by integrating out its parameters:"
REFERENCES,0.5835866261398176,"T(s‚Ä≤, r|s, a) =
Z"
REFERENCES,0.5866261398176292,"Œò
T(s‚Ä≤, r|s, a; Œ∏)q(Œ∏; œÜ)dŒ∏
(10)"
REFERENCES,0.5896656534954408,"Compared to previous model-based algorithms that use Ô¨Ånite number of neural network ensembles
(e.g. PETS), our choice of BNN is more suitable for lifelong RL as we only need to maintain one
neural network for each task, and we can sample an unlimited number of predictions from it which
better estimates the uncertainty and is essential in our setting where both dynamic function and
reward function are not given unlike prior model-based RL methods."
REFERENCES,0.5927051671732523,Under review as a conference paper at ICLR 2022
REFERENCES,0.5957446808510638,"A.3
BLRL ALGORITHM"
REFERENCES,0.5987841945288754,"Algorithm 3: Lifelong Bayesian Sampling Approach Algorithm
input :K, B
initialize MDP set, the world-model posterior qe(st+1, rt|st, at);
for each MDP mi do"
REFERENCES,0.601823708206687,"Ns,a ‚Üê0, ‚àÄs, a
do sample ‚ÜêTRUE ;
initialize the task-speciÔ¨Åc posterior qmi
Œ∏ (st+1, rt|st, at) ‚Üêqe(st+1, rt|st, at);
for all timesteps t = 1, 2, 3, ... do"
REFERENCES,0.6048632218844985,if do sample then
REFERENCES,0.60790273556231,"Sample K models mi1, mi2,¬∑¬∑¬∑,miK from the task-speciÔ¨Åc posterior
qmi
Œ∏ (st+1, rt|st, at).;
Merge the models into the mixed MDP m#
i ;
Solve m#
i to obtain œÄ‚àó
m#
i ;"
REFERENCES,0.6109422492401215,"do sample ‚ÜêFALSE
end
Use œÄ‚àó
m#
i for action selection: at ‚ÜêœÄm#
i (st) and observe reward rt and next state st+1 ;"
REFERENCES,0.6139817629179332,"Nst,at ‚ÜêNst,at + 1;
Update the task-speciÔ¨Åc posterior distribution qmi
Œ∏ (st+1, rt|st, at) for the current MDP;
if Nst,at = B then"
REFERENCES,0.6170212765957447,"Update the world-model posterior distribution qe(st+1, rt|st, at) with the collected
transitions;
do sample ‚ÜêTRUE
end
end
end"
REFERENCES,0.6200607902735562,"A.4
EXPERIMENTAL SETTING"
REFERENCES,0.6231003039513677,"A.4.1
GRID-WORLD ITEM SEARCHING"
REFERENCES,0.6261398176291794,"Our testbed consists of a collection of houses, each of which has four rooms. The goal of each
task is to Ô¨Ånd a speciÔ¨Åc object (blue, green or purple) in the current house. The type of each room
is sampled based on an underlying distribution given by the environment. Each room type has a
corresponding probability distribution of which kind of objects can be found in rooms of this type.
Different tasks/houses vary in terms of which rooms are which types and precisely where objects are
located in the room (the task‚Äôs hidden parameters). Room types are sampled from a joint distribution."
REFERENCES,0.6291793313069909,"Room type probability
Room 1
Room 2
Room 3
Room 4"
REFERENCES,0.6322188449848024,"Top-left
0.4
0
0.4
0.2
Bottom-left
0
0.8
0
0.2
Top-right
0.1
0
0
0.9
Bottom-right
0
0
0.8
0.2"
REFERENCES,0.6352583586626139,Table 2: Room type probability distribution
REFERENCES,0.6382978723404256,"A.4.2
BOX-JUMPING TASK"
REFERENCES,0.6413373860182371,"We use a simpliÔ¨Åed version of jumping task (des Combes et al., 2018) as a simple testbed for the
proposed algorithm VBLRL. We select a random position of obstacle between 15 ‚àº33 for each
task. The 4-element state vector describes the (x, y) coordinates of the agent‚Äôs current position, and
its velocity in the x and y directions. The agent can choose from two actions: jump and right. The
reward function for this box-jumping task is:
Rt = I{st reach the right wall} ‚àíI{st+1 hit the obstacle} + Àôxt ¬∑ I{st+1 not hit the obstacle} (11)"
REFERENCES,0.6443768996960486,Under review as a conference paper at ICLR 2022
REFERENCES,0.6474164133738601,"Object type probability
Blue ball
Green box
Purple box"
REFERENCES,0.6504559270516718,"Room 1
0
0.3
0
Room 2
0
0.2
1
Room 3
0.6
0
0
Room 4
0
0
0"
REFERENCES,0.6534954407294833,Table 3: Object type probability distribution
REFERENCES,0.6565349544072948,"A.4.3
OPENAI GYM MUJOCO DOMAINS"
REFERENCES,0.6595744680851063,Figure 4: Left: Walker-2D; Middle: Halfcheetah; Right: Hopper
REFERENCES,0.662613981762918,"Similar to (Mendez et al., 2020), we evaluated on the HalfCheetah, Hopper, and Walker-2D environ-
ments. For the gravity domain, we select a random gravity value between 0.5g and 1.5g for each task.
For the body-parts domain, we set the size and mass of each of the four parts of the body (head, torso,
thigh, and leg) to a random value between 0.5√ó and 1.5√ó its nominal value. As shown in Appendix
C of (Mendez et al., 2020), these changes lead to highly diverse tasks for lifelong RL. Further, as
required by model-based deep RL methods, we change the environment settings for Hopper and
Walker following (Wang et al., 2019). When implementing VBLRL, we found that in the Ô¨Årst few"
REFERENCES,0.6656534954407295,"Name of Environment
Reward function"
REFERENCES,0.668693009118541,"HalfCheetah
Àôxt ‚àí0.1||at||2
2
Hopper
Àôxt ‚àí0.1||at||2
2 ‚àí3.0 √ó (zt ‚àí1.3)2"
REFERENCES,0.6717325227963525,"Walker2D
Àôxt ‚àí0.1||at||2
2 ‚àí3.0 √ó (zt ‚àí1.3)2"
REFERENCES,0.6747720364741642,Table 4: Reward functions for OpenAI Gym domains
REFERENCES,0.6778115501519757,"episodes of each new tasks, the agent hasn‚Äôt collected enough samples of the new task, which results
in overÔ¨Åtting problems when training the task-speciÔ¨Åc. Thus, we use the world-model posterior
instead to do the Ô¨Årst few rounds of predictions and let the task-speciÔ¨Åc model begin training after
collecting enough samples. The world-model has lower possibility of overÔ¨Åtting as its training data
comes from all the previous tasks and has much larger quantity. We list the other implementation
details below. The planning horizons are selected from values suggested by previous model-based
RL papers (Chua et al., 2018; Wang et al., 2019)."
REFERENCES,0.6808510638297872,"For LPG-FTW and EWC, we use the original source code2 with parameters and model architectures
suggested in the original paper. SpeciÔ¨Åcally, we select step size from {0.005, 0.05, 0.5}. For LPG-
FTW, e use Œª = 1e ‚àí5, ¬µ = 1e ‚àí5 and select k from 3,5,10. For EWC, we select Œª from
{1e ‚àí6, 1e ‚àí7, 1e ‚àí4}. For HiP-MDP baseline, we modify the original algorithm for a fair
comparison. We replace the DDQN algorithm used in Killian et al.‚Äôs paper with the exact same CEM
planning method we used in VBLRL as well as the same parameters. And we use the same model
architecture of Bayesian Neural network by modifying the baseline algorithm to also predict reward
for each state-action pair (the original method only considers next-state prediction)."
REFERENCES,0.6838905775075987,2https://github.com/Lifelong-ML/LPG-FTW
REFERENCES,0.6869300911854104,Under review as a conference paper at ICLR 2022
REFERENCES,0.6899696048632219,"Hyper-parameters
CG
CB
HG
HB
WG
WB"
REFERENCES,0.6930091185410334,"# iterations
100
100
100
100
100
100
# Steps (each iteration)
100
100
200
200
200
200
learning rate (world model)
0.001
0.001
0.0005
0.0005
0.0005
0.0005
learning rate (task-speciÔ¨Åc model)
0.0005
0.0005
0.0002
0.0002
0.0002
0.0002
planning horizon
20
20
30
30
1
1
kl-divergence weight
0.0001
0.0001
0.0001
0.0001
0.0001
0.0001
# particles (CEM)
50
50
50
50
50
50
batch size (world-model)
8 √ó 64
8 √ó 64
8 √ó 64
8 √ó 64
8 √ó 64
8 √ó 64
batch size (task-speciÔ¨Åc)
256
256
256
256
256
256
# tasks
40
40
30
30
40
40
search population size
500
500
500
500
500
500
# elites (CEM)
50
50
50
50
50
50"
REFERENCES,0.6960486322188449,Table 5: Room type probability distribution
REFERENCES,0.6990881458966566,"For BOSS and BLRL, we set the number of sampled models K = 5, and Œ≥ = 0.95, ‚àÜ= 0.01 for
value iteration."
REFERENCES,0.7021276595744681,"A.5
DETAILED PROOF FOR LEMMA 1"
REFERENCES,0.7051671732522796,"Following the same settings introduced in section 5 of (Zhang, 2006), we deÔ¨Åne the resolvability of
standard Bayesian posterior as:"
REFERENCES,0.7082066869300911,"rn(q) = inf
œâ [EœÄœâ(Œ∏)DKL(q||p(¬∑|Œ∏)) + 1"
REFERENCES,0.7112462006079028,nDKL(œâdœÄ||dœÄ)] = ‚àí1
REFERENCES,0.7142857142857143,"n ln EœÄe‚àínDKL(q||p(¬∑|Œ∏)).
(12)"
REFERENCES,0.7173252279635258,"We refer the readers to Zhang‚Äôs paper for further explanation of the denotations. Intuitively, the
Bayesian resolvability controls the complexity of the density estimation process. Based on this
deÔ¨Ånition and our previous deÔ¨Ånitions of dœÄ, we can derive a simple and intuitive estimate of the
standard Bayesian resolvability.
Lemma 3. The resolvability of standard Bayesian posterior deÔ¨Åned in (12) can be bounded as"
REFERENCES,0.7203647416413373,"rn(q) ‚â§n + 1 n
dœÄ"
REFERENCES,0.723404255319149,"proof. For all d > 0, we have"
REFERENCES,0.7264437689969605,rn(q) = ‚àí1
REFERENCES,0.729483282674772,n ln EœÄe‚àínDKL(q||p(¬∑|Œ∏)) ‚â§‚àí1
REFERENCES,0.7325227963525835,n ln[e‚àíndœÄ(p ‚ààŒì : DKL(q||p) ‚â§d)]
REFERENCES,0.7355623100303952,= d + 1
REFERENCES,0.7386018237082067,"n √ó [‚àíln œÄ(p ‚ààŒì : DKL(q||p) ‚â§d)] ‚â§n + 1 n
dœÄ"
REFERENCES,0.7416413373860182,"This bound links the Bayesian resolvability to the number of samples n and prior-mass radius
dœÄ which is a Ô¨Åxed property of the density given a speciÔ¨Åc prior and the true underlying density.
Intuitively, the Bayesian posterior is better behaved when the Bayesian prior is closer to the true
distribution (dœÄ is smaller) and more samples are used (n is larger)."
REFERENCES,0.7446808510638298,Now we can prove the main theorem of Lemma 1. Let œÅ = 1
REFERENCES,0.7477203647416414,"2, œµt = 2Œµn+(4Œ∑‚àí2)t"
REFERENCES,0.7507598784194529,"Œ¥/4
, deÔ¨Åne Œì1 =
{p ‚ààŒì : DRe
œÅ (q||p) < œµt} and Œì2 = {p ‚àà: DRe
œÅ (q||p) ‚â•œµt}. We let a = e‚àínt and deÔ¨Åne
œÄ‚Ä≤(Œ∏) = aœÄ(Œ∏)C when Œ∏ ‚ààŒì1 and œÄ‚Ä≤(Œ∏)C when Œ∏ ‚ààŒì2, where the normalization constant
C = (aœÄ(Œì1) + œÄ(Œì2))‚àí1 ‚àà[1, 1/a]. Firstly,"
REFERENCES,0.7537993920972644,EXœÄ‚Ä≤(Œì2|X)œµt ‚â§EXEœÄ‚Ä≤œÄ‚Ä≤(Œ∏|X)1
REFERENCES,0.756838905775076,"2||p ‚àíq||2
1 ‚â§EXEœÄ‚Ä≤œÄ‚Ä≤(Œ∏|X)DKL(q||p)"
REFERENCES,0.7598784194528876,Under review as a conference paper at ICLR 2022
REFERENCES,0.7629179331306991,"according to the Markov inequality (with probability at least 1 ‚àíŒ¥) and Pinsker‚Äôs inequality. Then
according to Theorem 5.2 and Proposition 5.2 in Zhang‚Äôs paper,"
REFERENCES,0.7659574468085106,EXEœÄ‚Ä≤œÄ‚Ä≤(Œ∏|X)DKL(q||p) ‚â§Œ∑ ln EœÄ‚Ä≤e‚àínDKL(q||p(¬∑|œâ))
REFERENCES,0.7689969604863222,œÅ(œÅ ‚àí1)n
REFERENCES,0.7720364741641338,"+
Œ∑ ‚àíœÅ
œÅ(1 ‚àíœÅ)n inf
{Œìj} ln
X"
REFERENCES,0.7750759878419453,"j
œÄ‚Ä≤(Œìj)(Œ∑‚àí1)/(Œ∑‚àíœÅ)(1 + rub(Œìj))n"
REFERENCES,0.7781155015197568,‚â§Œ∑t ‚àí(Œ∑/n) ln EœÄe‚àínDKL(q||p(¬∑|œâ))
REFERENCES,0.7811550151975684,"œÅ(1 ‚àíœÅ)
+
Œ∑ ‚àíœÅ
œÅ(1 ‚àíœÅ)"
REFERENCES,0.78419452887538,h(Œ∑ ‚àí1)t
REFERENCES,0.7872340425531915,"Œ∑ ‚àíœÅ
+ Œµupper,n
Œ∑ ‚àí1 Œ∑ ‚àíœÅ i"
REFERENCES,0.790273556231003,= (2Œ∑ ‚àí1)t
REFERENCES,0.7933130699088146,"œÅ(1 ‚àíœÅ) + ‚àí(Œ∑/n) ln EœÄe‚àínDKL(q||p(¬∑|œâ)) + (Œ∑ ‚àíœÅ)Œµupper,n((Œ∑ ‚àí1)/(Œ∑ ‚àíœÅ))"
REFERENCES,0.7963525835866262,œÅ(1 ‚àíœÅ)
REFERENCES,0.7993920972644377,"Then, using the deÔ¨Ånitions of dœÄ, we further obtain"
REFERENCES,0.8024316109422492,EXœÄ‚Ä≤(Œì2|X)œµt
REFERENCES,0.8054711246200608,‚â§(2Œ∑ ‚àí1)t
REFERENCES,0.8085106382978723,œÅ(1 ‚àíœÅ) + Œ∑ infd>0[d ‚àí1
REFERENCES,0.8115501519756839,"n ln œÄ({p ‚ààŒì : DKL(q||p) ‚â§d})] + (Œ∑ ‚àíœÅ)Œµupper,n((Œ∑ ‚àí1)/(Œ∑ ‚àíœÅ))"
REFERENCES,0.8145896656534954,œÅ(1 ‚àíœÅ)
REFERENCES,0.817629179331307,‚â§(2Œ∑ ‚àí1)t
REFERENCES,0.8206686930091185,œÅ(1 ‚àíœÅ) + Œ∑(1 + 1
REFERENCES,0.8237082066869301,"n)dœÄ + (Œ∑ ‚àíœÅ)Œµupper,n((Œ∑ ‚àí1)/(Œ∑ ‚àíœÅ))"
REFERENCES,0.8267477203647416,œÅ(1 ‚àíœÅ)
REFERENCES,0.8297872340425532,= (2Œ∑ ‚àí1)t + Œµn
REFERENCES,0.8328267477203647,œÅ(1 ‚àíœÅ)
REFERENCES,0.8358662613981763,"We use Œ∑ instead of Œ≥ which is used in the original paper to avoid confusion with the discount factor.
Then we further divide both sides by œµt and obtain œÄ‚Ä≤(Œì|X) ‚â§0.5. Then by deÔ¨Ånition,"
REFERENCES,0.8389057750759878,œÄ(Œì2|X) = aœÄ‚Ä≤(Œì2|X)/(1 ‚àí(1 ‚àía)œÄ‚Ä≤(Œì|X))
REFERENCES,0.8419452887537994,"‚â§
a
a + 1 =
1
1 + ent"
REFERENCES,0.8449848024316109,"Thus, we get the desired bound."
REFERENCES,0.8480243161094225,"A.6
DETAILED PROOF FOR THEOREM 1"
REFERENCES,0.851063829787234,"Theorem 2. (Full version of the bound in Theorem 1) For each new task, set the sample size
K = Œò( S2A"
REFERENCES,0.8541033434650456,"Œ¥
ln SA"
REFERENCES,0.8571428571428571,"Œ¥ ) and the parameters œµ0 = œµ(1 ‚àíŒ≥)2, Œ¥0 =
Œ¥
SA, œÅ0 =
Œ¥
S2A2K , then, with"
REFERENCES,0.8601823708206687,"probability at least 1 ‚àí4Œ¥, V At(st) ‚â•V ‚àó(st) ‚àí4œµ in all but O( S2A2(dœÄ+ln S2A2K"
REFERENCES,0.8632218844984803,"Œ¥
)
Œ¥œµ3(1‚àíŒ≥)6
ln 1"
REFERENCES,0.8662613981762918,"Œ¥ ln
1
œµ(1‚àíŒ≥))
steps."
REFERENCES,0.8693009118541033,"First, we would like to introduce two lemma from BOSS (Asmuth et al., 2009):"
REFERENCES,0.8723404255319149,Lemma 4. The sample size K = Œò( S2A
REFERENCES,0.8753799392097265,"Œ¥
ln SA"
REFERENCES,0.878419452887538,"Œ¥ ) sufÔ¨Åces to guarantee V ‚àó
m(s) ‚â•V ‚àó(s) for all s
during the entire learning process with probability at least 1 ‚àíŒ¥."
REFERENCES,0.8814589665653495,"Lemma 5. If the knownness parameter B = maxs,af(s, a, œµ,
Œ¥
SA,
œÅ
S2A2K ), then the transition
function of all the sampled models are œµ-close (in the l1 sense) to the true transition function for all
the known state-action pairs during the entire learning process with probability at least 1 ‚àíŒ¥ ‚àíœÅ."
REFERENCES,0.8844984802431611,"proof. Given discrete state and action spaces, the proof that BLRL on each new task is PAC-MDP
depends on three main assumptions, following a general PAC-MDP theorem from (Strehl et al.,
2006):"
REFERENCES,0.8875379939209727,"1. Learning complexity condition. In our settings, a state-action pair is claimed to be known after
being visited for B times. In the meantime, there are SA unknown state-action pairs in total at the
beginning of each task, so the bounded discoveries condition is guaranteed."
REFERENCES,0.8905775075987842,Under review as a conference paper at ICLR 2022
REFERENCES,0.8936170212765957,"2. Optimism. This is guaranteed by Lemma 3. We construct a new hyper-model each time a discovery
event occurs. The values for each unknown state in every hyper-model are optimistic."
REFERENCES,0.8966565349544073,"3. Accuracy. For each new task, since the prior œÄ has bounded sample complexity of B for Œ¥0 and
œµ0, a known state-action pair will be locally œµ0-accurate with probability at least 1 ‚àíŒ¥0. Given the
deÔ¨Ånition of Bayesian concentration sample complexity and Lemma 4, œµ0 = œµ(1 ‚àíŒ≥)2 translates into
an œµ error bound in the value function (Asmuth et al., 2009)."
REFERENCES,0.8996960486322189,"Finally, given Lemma 2, we know f(s, a, œµ0, Œ¥0, œÅ0) = O

dœÄ+ln
1
œÅ0
œµ2
0Œ¥0‚àídœÄ"
REFERENCES,0.9027355623100304,"
, let œµ0 = œµ(1 ‚àíŒ≥)2, Œ¥0 ="
REFERENCES,0.9057750759878419,"Œ¥
SA, œÅ0 =
Œ¥
S2A2K , we can get f(s, a, œµ0, Œ¥0, œÅ0) = O

SA(dœÄ+ln S2A2K"
REFERENCES,0.9088145896656535,"Œ¥
)
Œ¥œµ2(1‚àíŒ≥)4"
REFERENCES,0.9118541033434651,"
. We can get the Ô¨Ånal form"
REFERENCES,0.9148936170212766,of the bound by replacing B with these quantities.
REFERENCES,0.9179331306990881,"A.7
ADDITIONAL EXPLANATION OF THE ALGORITHM"
REFERENCES,0.9209726443768997,"Here we Ô¨Årst provide an example to help the readers better understand our plate notation. In our
Gridworld Item Searching case, Œ® represents the parameters of P‚Ñ¶, which is the room-type and
object distribution. For each task, the environment samples a hidden parameter œâ, which is the actual
room and object layout of this house, from this distribution P‚Ñ¶. The sampled œâ then will result in an
MDP m and let the agent interact with it."
REFERENCES,0.9240121580547113,"A.7.1
PLANNING ALGORITHM"
REFERENCES,0.9270516717325228,"With the transition dynamics and reward functions, a planning algorithm like CEM is not the only
way to solve the MDP to get an optimal policy. Another option would be using other Deep RL
algorithms like Soft actor-critic (Haarnoja et al., 2018) with data generated from the model. However,
in this case, incorporating a deep RL algorithm means that we need to introduce additional neural
networks (that is, policy/value networks) for each task. The update signal from the RL loss is usually
stochastic and weak, which is even worse in this case when our model is still far from accurate. So,
here we assume applying a planning algorithm is a better way to get the policy."
REFERENCES,0.9300911854103343,"A.8
COIN EXAMPLE"
REFERENCES,0.9331306990881459,"Consider a coin-Ô¨Çipping environment. We want to Ô¨Ånd the sample complexity of the unbiased coin
(i.e. How many times we need to Ô¨Çip this coin such that our posterior samples are accurate.). Consider
a Dirichlet prior, Œ±0 = (n1, n2) and Œ∏0 = ( 1 2, 1"
REFERENCES,0.9361702127659575,"2). We want to Ô¨Ånd sample complexity B such that
the posterior likelihood for a coin with heads likelihood in [0.5 ‚àíœµ, 0.5 + œµ] is at least 1 ‚àíŒ¥."
REFERENCES,0.939209726443769,"Note that the Dirichlet distribution on the two-dimensional simplex is the Beta distribution. The
Multinomial distribution with two outcomes is the Binomial distribution. That is, given the process"
REFERENCES,0.9422492401215805,"H ‚àºBin(H|œÅ = 0.5, B),
(13)"
REFERENCES,0.9452887537993921,"ÀÜœÅ ‚àºBeta(ÀÜœÅ|Œ± = H + n1, Œ≤ = B ‚àíH + n2),
(14)"
REFERENCES,0.9483282674772037,choose a value B such that
REFERENCES,0.9513677811550152,"P(0.5 ‚àíœµ ‚â§ÀÜœÅ ‚â§0.5 + œµ) ‚â•1 ‚àíŒ¥,
(15) B
X"
REFERENCES,0.9544072948328267,"H=0
Bin(H|œÅ = 0.5, B) ¬∑
Z 0.5+œµ"
REFERENCES,0.9574468085106383,"ÀÜœÅ=0.5‚àíœµ
Beta(ÀÜœÅ|Œ± = H + n1, Œ≤ = B ‚àíH + n2) ‚â•1 ‚àíŒ¥.
(16)"
REFERENCES,0.9604863221884499,"Here, n1 and n2 capture the prior. The smallest B that satisÔ¨Åes Equation 16 can be found numerically."
REFERENCES,0.9635258358662614,"We set œµ = 0.1 and Œ¥ = 0.3. Here are the results of sample complexity B given different values of
n1, n2:"
REFERENCES,0.9665653495440729,"We Ô¨Åx the sum of (n1, n2) as 10. As shown in the results, the value of sample complexity B becomes
lower as we use a more accurate prior (from (10, 0) to (5, 5))."
REFERENCES,0.9696048632218845,Under review as a conference paper at ICLR 2022
REFERENCES,0.9726443768996961,"(n1, n2)
lowest B
(0,10)
78
(1,9)
68
(2,8)
58
(3,7)
49
(4,6)
42
(5,5)
40
(6,4)
42
(7,3)
48
(8,2)
58
(9,1)
68
(10,0)
78"
REFERENCES,0.9756838905775076,"In general, for the task-speciÔ¨Åc posterior, we can relate B, œµ and Œ¥ with the following equation:
Z"
REFERENCES,0.9787234042553191,"P0
Dir(P0|Œ¶T rue)
h
X"
REFERENCES,0.9817629179331308,"N:||N||1=B
Mult(N|P0, B)
h Z"
REFERENCES,0.9848024316109423,"P :||P (Œ¶)‚àíP0(Œ¶)||‚â§œµ
Dir(P|Œ¶old+N)dP
ii
dP0 ‚â•1‚àíŒ¥"
REFERENCES,0.9878419452887538,"(17)
For the world model posterior:
X"
REFERENCES,0.9908814589665653,"N:||N||1=Bw
Mult(N|Pw0, Bw)
h Z"
REFERENCES,0.993920972644377,"P :||Pw(Œ¶)‚àíPw0(Œ¶)||‚â§œµ
Dir(Pw|Œ¶wold + N)dPw
i
‚â•1 ‚àíŒ¥ (18)"
REFERENCES,0.9969604863221885,"For each task, Ô¨Årst we pick a true model P0 according to the true distribution and initialize the
task-speciÔ¨Åc prior Œ¶old = Œ¶w. Then, we make some observations from the world. Once we have the
true model and the observations, we can calculate how many models are œµ-close to the true model,
weighted according to their posterior likelihood."
