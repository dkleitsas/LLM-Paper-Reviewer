Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005076142131979695,"Optimal Mass Transport (OMT) is a well-studied problem with a variety of appli-
cations in a diverse set of ﬁelds, ranging from Physics to Computer Vision and in
particular Statistics and Data Science. Since the original formulation of Monge in
1781 signiﬁcant theoretical progress been made on the existence, uniqueness and
properties of the optimal transport maps. The actual numerical computation of the
transport maps, particularly in high dimensions, remains a challenging problem.
In the past decade several neural network based algorithms have been proposed
to tackle this task. In this paper, building on recent developments of input con-
vex neural networks and physics informed neural networks for solving PDE’s,
we propose a new Deep Learning approach to solve the continuous OMT prob-
lem. Our framework is based on Brenier’s theorem, which reduces the continuous
OMT problem to that of solving a nonlinear PDE of Monge-Ampere type whose
solution is a convex function. To demonstrate the accuracy of our framework we
compare our method to several other deep learning based algorithms. We then fo-
cus on applications to the ubiquitous density estimation and generative modeling
tasks in statistics and machine learning. Finally as an example we present how
our framework can be incorporated with an autoencoder to estimate an effective
probabilistic generative model."
INTRODUCTION,0.01015228426395939,"1
INTRODUCTION"
INTRODUCTION,0.015228426395939087,"Optimal Mass Transport (OMT) is a well-studied problem with a variety of applications in a diverse
set of ﬁelds, ranging from physics to computer vision and in particular statistics and data science. In
this paper we propose a new framework for the estimation of the solution to the L2-optimal transport
problem between two densities. Our algorithm, which is based on Brenier’s theorem, and builds on
recent developments of input convex neural networks and physics-informed neural networks for
solving PDE’s. Before we describe the contributions of the article in more detail, we will brieﬂy
summarize the motivation of our investigations and recent developments in the ﬁeld."
INTRODUCTION,0.02030456852791878,"Density estimation and random sampling: The density estimation problem is to estimate a smooth
probability density based on a discrete ﬁnite set of observations. In traditional parametric density
estimation techniques, the data is assumed to be drawn from a known parametric family of distribu-
tions. One of the most ubiquitous parametric techniques is Gaussian Mixture Modeling (McLachlan
& Basford, 1988)."
INTRODUCTION,0.025380710659898477,"Nonparametric techniques were ﬁrst proposed by Fix & Hodges (1951) (Silverman & Jones (1989))
to move away from rigid distributional assumptions. The most used approach is the kernel density
estimation, which dates back to Rosenblatt (1952a) and Parzen (1962). Many challenges remain
regarding the implementation and practical performance of kernel density estimators, including in
particular, the bandwidth selection and the lack of local adaptivity resulting in a large sensitivity to
outliers (Loader et al., 1999). These problems are particularly exacerbated in high dimensions with
the curse of dimensionality."
INTRODUCTION,0.030456852791878174,"Recently, diffeomorphic transformation-based algorithms have been proposed to tackle this prob-
lem (Dinh et al., 2017; Marzouk et al., 2016; Younes, 2020; Bauer et al., 2017). The basic concept
of transformation-based algorithms is to ﬁnd a diffeomorphic mapping between a reference proba-"
INTRODUCTION,0.03553299492385787,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04060913705583756,"bility distribution and the unknown target distribution, from which the data is drawn. Consequently,
transformation-based density estimation leads at the same time to an efﬁcient generative model,
as new samples from the estimated density can be generated at a low cost by sampling from the
reference density and transforming the samples by the estimated transformation. The fundamental
problem in diffeomorphic transformation-based approaches is how to estimate and select the trans-
formation: from a theoretical point of view there exists an inﬁnite set of transformations that map
two given probability densities onto each other. Recently, several deep learning methods have been
devised for this task, where Normalizing Flows (NF) stand out among these methods. Examples of
such models include Real NVP (Dinh et al., 2017), Masked Autoregressive Flows (Papamakarios
et al., 2017), iResNets (Behrmann et al., 2019), Flow++ (Ho et al., 2019) and Glow (Kingma &
Dhariwal, 2018). For a review of the vast NF literature, we refer to the the overview article (Kobyzev
et al., 2020). Although these methods have shown to perform well in density estimation applications,
the interpretability of the obtained transformation is less clear, e.g. in Real NVP (Dinh et al., 2017),
the solution selection is obtained by restricitng the transformations to the class of diffeomorphisms
with triangular Jacobians that are easy to invert, which is closely related to the Knothe-Rosenblatt
rearrangement (Knothe, 1957; Rosenblatt, 1952b)."
INTRODUCTION,0.04568527918781726,"Optimal mass transport: Optimal mass transport, on the other hand, formulates the transport map
selection as the minimizer of a cost function (Villani, 2008; 2003). The optimal transportation
cost induces a metric structure, the Wasserstein metric, on the space of probability densities and
is sometimes referred to as the Earth Mover’s Distance. This theory, which dates back to 1781,
was originally formulated by the French mathematician Gaspard Monge (1781). The difﬁculty in
applying this framework to the proposed density estimation problem lies in solving the correspond-
ing optimization problem, which in a dimension greater than one is highly non trivial. The fully
discrete OMT problem (optimal assignment problem) can be solved using linear programming and
can be approximated by the Sinkhorn algorithm (Cuturi, 2013a; Papadakis, 2015). However, these
algorithms do not lead to a continuous transformation map and thus can’t be used for the proposed
diffeomorphic density estimation and generative modelling. Previous algorithmic solutions for the
continuous OMT problem include ﬂuid mechanics-based approaches (Benamou & Brenier, 2000),
ﬁnite element or ﬁnite difference-based methods (Benamou et al., 2010; Benamou & Duval, 2019)
and steepest descent-based energy minimization approaches (Angenent et al., 2003; Carlier et al.,
2010; Loeper & Rapetti, 2005)."
INTRODUCTION,0.050761421319796954,"In recent years, several deep learning methods have been deployed for solving the OMT problem.
In these methods, the OMT problem is typically embedded in the loss function for the neural net-
work model. Recent work by Makkuva et al. (2020) proposed to approximate the OMT map as
the solution of min-max optimization using input convex neural networks (ICNN), see Amos et al.
(2017). The min-max nature of this algorithm arises from the need to train an ICNN to represent
a convex function and the conjugate of the convex function. Building upon this approach, Korotin
et al. (2019) imposed a cyclic regularisation that converts the min-max optimization problem to a
standard minimization problem. This change results in a faster converging algorithm that scales well
to higher dimensions and also prevents convergence to local saddle points and instabilities during
training, as is the case in the min-max algorithm."
INTRODUCTION,0.05583756345177665,"Another class of neural networks which have been proposed to solve OMT problems are Generative
Adversarial Networks(GANs) (Goodfellow et al., 2014). GANs are deﬁned through a min-max
game of two neural networks where one of the networks tries to generate new samples from a data
distribution, while the other network judges whether these generated samples originate from the
data population or not. Later, Gulrajani et al. (2017) proposed using the Wasserstein-1 distance in
GANs instead of the Jensen-Shannon divergence between the generated distribution and the data
distribution as in the original formulation. They demonstrated that this new loss functions leads to
better stability of the training of networks attributed to the Wasserstein metric being well deﬁned
even when the two distributions do not share the same support."
INTRODUCTION,0.06091370558375635,"Contributions: In this paper, we propose a different deep learning-based framework to approx-
imate the optimal transport maps. The approach we present relies on Brenier’s celebrated theo-
rem (Brenier, 1991), thereby reducing the optimal transport problem to that of solving a partial
differential equation: a Monge-Ampere type equation. We frame this PDE in the recently developed
paradigm of Physics Informed Neural Networks (PINNs) (Raissi et al., 2017). Similar to other deep
learning-based algorithms, our framework directly inherits the dimensional scalability of neural net-
works (Shin et al., 2020), which traditional ﬁnite element or ﬁnite difference methods for solving"
INTRODUCTION,0.06598984771573604,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.07106598984771574,"PDEs do not possess. Brenier’s theorem further states that the optimal transport map is given by the
gradient of a convex function- the Brenier potential. To incorporate this information in our PINN
approach, we parameterize the Brenier potential using an ICNN, thereby guaranteeing its convexity."
INTRODUCTION,0.07614213197969544,"We test the accuracy of our OMT solver on numerous synthetic examples for which analytical so-
lutions are known. Our experiments show that our algorithm indeed approximates the true solution
well, even in high dimensions. To further quantify the performance of the new framework, we com-
pare it to two other deep learning-based algorithms, for which we guided the selection by the results
of the recent benchmarking paper by Korotin et al. (2021), in which they evaluate the methods pre-
sented in Seguy et al. (2017); Nhan Dam et al. (2019); Taghvaei & Jalali (2019); Makkuva et al.
(2020); Liu et al. (2019); Mallasto et al. (2019); Korotin et al. (2019). We restricted our comparision
to the algorithms of Makkuva et al. (2020) and Korotin et al. (2019), as these two showed the best
performance in this benchmark. Our results showed that the newly proposed method signiﬁcantly
outperforms these methods in terms of accuracy."
INTRODUCTION,0.08121827411167512,"As an explicit application of our solution of OMT, we focus on the density estimation problem. In
synthetic examples, we show that we can estimate the true density based on a limited amount of
samples. We compare the results of the proposed method to four other density estimation algorithm:
the two OMT based algorithms mentioned above, and two methods from the family of normalizing
ﬂows: RealNVP (Dinh et al. (2016)) and iResNet (Behrmann et al. (2019)). Finally we demon-
strate how our framework can be combined with a traditional autoencoder to obtain a generative
framework. In accordance with the best practices for reproducible research, we are providing an
open-source version of the code, which is publicly available on github."
OMT USING DEEP LEARNING,0.08629441624365482,"2
OMT USING DEEP LEARNING"
OMT USING DEEP LEARNING,0.09137055837563451,"In this section, we will present our framework for solving the Optimal Mass Transport (OMT)
problem. Our approach will combine methods of deep learning with the celebrated theorem of
Brenier, which reduces the solution of the OMT problem to solving a Monge-Ampere type equation.
To be more precise, we will tackle this problem by embedding the Monge-Ampere equation into the
broadly applicable concept of Physics Informed Neural Networks."
MATHEMATICAL BACKGROUND OF OMT,0.09644670050761421,"2.1
MATHEMATICAL BACKGROUND OF OMT"
MATHEMATICAL BACKGROUND OF OMT,0.10152284263959391,"We start by summarizing the mathematical background of OMT, including a description of Brenier’s
theorem. For more information we refer to the vast literature on OMT, see e.g., Villani (2003; 2008).
Let Ωbe a convex and bounded domain of Rn and let dx denote the standard measure on Rn. For
simplicity, we restrict our presentation to the set P(Ω) of all absolutely continuous measures on Ω,
i.e., P(Ω) ∋µ = fdx with f ∈L1(Ω), such that
R"
MATHEMATICAL BACKGROUND OF OMT,0.1065989847715736,"Ωfdx = 1. From here, on we will identify
the measure µ with its density function f. We aim to minimize the cost of transporting a density
µ to a density ν using a (transport) map T, which leads to the so-called Monge Optimal Transport
Problem. We will consider only the special case of a quadratic cost function as Brenier’s theorem,
which forms the basis of our algorithm, is only true for this type of cost function."
MATHEMATICAL BACKGROUND OF OMT,0.1116751269035533,"Deﬁnition 2.1 (L2-Monge Optimal Transport Problem) Given µ, ν ∈P(Ω), minimize M(T) =
R"
MATHEMATICAL BACKGROUND OF OMT,0.116751269035533,"Ω∥x −T(x)∥2dµ(x) over all µ-measureable maps T : Ω→Ωsubject to ν = T∗µ. We will call
an optimal T an optimal transport map."
MATHEMATICAL BACKGROUND OF OMT,0.1218274111675127,"Here, the constraint is formulated in terms of the push forward action of a measurable map T : Ω→
Ω, which is deﬁned via T∗µ(B) = µ(T −1(B)), for every measurable set A ⊂Ω. By a change of
coordinates, the constraint T∗µ = T∗(fdx) = ν = gdx can be thus reduced to the equation"
MATHEMATICAL BACKGROUND OF OMT,0.12690355329949238,"f(x) = g(T(x))| det(DT(x))|.
(1)"
MATHEMATICAL BACKGROUND OF OMT,0.1319796954314721,"This equation can also be expressed via the pullback action as µ = T ∗ν := (T −1)∗ν. The existence
and uniqueness of an optimal transport map is not guaranteed. We will see, that in our situation, i.e.,
for absolutely continuous measures, the existence and uniqueness is indeed guaranteed. First, we
will introduce a more general formulation of the Monge problem, called Kantorovich formulation.
Therefore, we deﬁne the space of all transport plans Π(µ, ν), i.e., of all measures on the product
space Ω× Ω, such that the ﬁrst marginal is µ and the second marginal is ν. Then we have:"
MATHEMATICAL BACKGROUND OF OMT,0.13705583756345177,Under review as a conference paper at ICLR 2022
MATHEMATICAL BACKGROUND OF OMT,0.14213197969543148,"Deﬁnition 2.2 (L2-Kantorovich’s Optimal Transport Problem) Given µ, ν ∈P(Ω), minimize
K(π) =
R"
MATHEMATICAL BACKGROUND OF OMT,0.14720812182741116,"Ω×Ω∥x −y∥2dπ(x, y) over all π ∈Π(µ, ν)."
MATHEMATICAL BACKGROUND OF OMT,0.15228426395939088,"Note that the L2-Wasserstein metric W2(µ, ν) between µ and ν is deﬁned as the inﬁmum of K. We
will now formulate Brenier’s theorem, which guarantees the existence of an optimal transport map
and will be the central building block of our algorithm:
Theorem 2.3 (Brenier (1991)) Let µ, ν ∈P(Ω). Then there exists a unique optimal transport plan
π∗∈Π(f, g), which is given by π∗(x, y) = (id ×T) where T = ∇u is the gradient of a convex
function u that pushes µ forward to ν, i.e., (∇u)∗µ = ν. The inverse T −1 is also given by the
gradient of a convex function that is the Legendre transform of the convex function u."
MATHEMATICAL BACKGROUND OF OMT,0.15736040609137056,"Thus, Brenier’s Theorem guarantees the existence and the uniqueness of the optimal transport map
of the OMT problem. Consequently, we can determine this optimal transport map by solving for the
function u in the form of a Monge-Amp`ere equation:
det(D2(u)(x)) · g(∇u(x)) = f(x)
(2)
where D2 is the Hessian, µ = fdx and ν = gdx. We obtain equation 2 directly from equation 1
using the constraint that T = ∇u as required by Brenier’s theorem. We will also refer to this map
as the Brenier map. This map is a diffeomorphism as it is a gradient of a strictly convex function. In
general, the Monge–Amp`ere equation is a nonlinear second-order partial differential equation. If
we limit ourselves to convex solutions, then the differential equation is elliptic. If the two densities f
and g are smooth (C∞), positive and absolutely continuous with respect to each other then an unique
smooth convex solution is guaranteed to exist. See e.g. Forzani & Maldonado (2004); Bakelman
(1983); De Philippis & Figalli (2013) for further details on existence and regularity of solutions
to this equation. Using methods of classical numerical analysis, Brenier’s theorem has been used
e.g. in Peyr´e et al. (2019) to obtain a numerical framework for the continous OMT problem. In
the following section we will propose a new discretization to this problem, which will make use of
recent advances in deep learning."
SOLVING OMT USING PINNS,0.16243654822335024,"2.2
SOLVING OMT USING PINNS"
SOLVING OMT USING PINNS,0.16751269035532995,"Physics Informed Neural Networks (PINNs) were proposed by Raissi et al. (2017) to solve general
nonlinear partial differential equations (PDEs). The basic concept is to use the universal approxi-
mation property of deep neural networks to represent the solution of a PDE via a network. Using
the automatic differentiation capability of modern machine learning frameworks, a loss function is
formulated, such that its minimizer solves the PDE in a weak sense. Such a loss function encodes
the structured information, which results in the ampliﬁcation of the information content of the data
the network sees (Raissi et al., 2017). This formulation of the PDE results in good generalization
even when only few training examples are available. PINNs have found widespread applications in
a short period of time since their introduction. These applications include a wide variety of PDEs,
including the Navier-Stokes equation (Jin et al., 2021), nonlinear stochastic PDEs (Zhang et al.,
2020) or Allen Cahn PDEs (McClenny & Braga-Neto, 2020)."
SOLVING OMT USING PINNS,0.17258883248730963,"In this work, we propose to use the PINN approach to solve the Monge-Ampere equation, as pre-
sented in equation 2, and hence implicitly the OMT problem. This equation has been extensively
studied and the properties of its solutions are well established. By Theorem 2.3, we know that the
solution is given by a convex function u. Recently, Amos et al. (2017) proposed a new architecture
of neural networks, Input Convex Neural Networks (ICNNs), that explicitly constrains the function
approximated by the network to be convex. Furthermore, ICNNs inherit the universal approximation
powers of feedforward networks (Amos et al., 2017, Proposition 2) and thus we can approximate any
convex function arbitrarily well by using an ICNN architecture of sufﬁcient depth and width. Con-
sequently, this architecture naturally lends itself to our proposed application, as it directly encodes
Brenier’s theorem. In the ICNN architecture, the activation function is a nondecreasing convex func-
tion and the internal weights (W (x)
n ) are constrained to be non-negative; see Figure 4 in Appendix A
for a schematic description of this class of networks. This architecture is derived from two simple
facts: non-negative sums of convex functions are also convex, and the composition of a convex and
convex nondecreasing function is again convex."
SOLVING OMT USING PINNS,0.17766497461928935,"We are now ready to describe our procedure for solving the continuous OMT problem: given abso-
lutely continuous densities µ = fdx and ν = gdx we reduce, using Brenier’s theorem, the OMT"
SOLVING OMT USING PINNS,0.18274111675126903,Under review as a conference paper at ICLR 2022
SOLVING OMT USING PINNS,0.18781725888324874,"problem to the Monge-Ampere equation 2. Using the PINN approach to solve this PDE leads to
consider the loss function"
SOLVING OMT USING PINNS,0.19289340101522842,"∥det(D2(u)) · g(∇u) −f∥2
L2,
(3)"
SOLVING OMT USING PINNS,0.19796954314720813,"which corresponds to a weak formulation of the PDE. To incorporate the convexity of u we construct
the solution space using an ICNN of sufﬁcient depth and width. This loss function scales exponen-
tially with respect to dimensions of the domain of integration Ω. This stems from the fact that the
Monge-Ampere equation contains the determinant of the Jacobian. Using the positive deﬁniteness
of the Jacobian (being the Hessian of a convex function) one could potentially make use of efﬁcient
determinant estimators, such as Stochastic Chebyshev Expansions as developed in Han et al. (2018),
which would scales linearly in dimension. Once we have estimated the optimal transport map, the
L2-Wasserstein metric between µ and ν is given by
Z
∥x −∇u(x)∥2 g(x)dx.
(4)"
SOLVING OMT USING PINNS,0.20304568527918782,"We call this combination of the PINN approach with the ICNN structure, Physics Informed Convex
Artiﬁcial Neural Networks (PICANNs)."
SOLVING OMT USING PINNS,0.20812182741116753,"In several applications, we are interested in computing the inverse transformation at the same time.
By a duality argument, we know that this map is also given by the gradient of a convex function.
Thus, we use a second ICNN (with the same architecture) to compute the inverse optimal transport
map (∇v) by solving the minimization problem:"
SOLVING OMT USING PINNS,0.2131979695431472,"∥∇v(∇u(x)) −x∥L2,
(5)"
SOLVING OMT USING PINNS,0.2182741116751269,"where ∇u is the optimal transport map solving (∇u)∗µ = ν, c.f. Figure 4 in Appendix A."
DIFFEOMORPHIC RANDOM SAMPLING AND DENSITY ESTIMATION,0.2233502538071066,"2.3
DIFFEOMORPHIC RANDOM SAMPLING AND DENSITY ESTIMATION"
DIFFEOMORPHIC RANDOM SAMPLING AND DENSITY ESTIMATION,0.22842639593908629,"In many applications, such as the Bayesian estimation, we can evaluate the density rather easily but
generating samples from a given density is not trivial. Traditional methods include Markov Chain
Monte Carlo methods, e.g., the Metropolis Hastings algorithm (Hastings, 1970). An alternative
idea is to use diffeomorphic density matching between the given density ν and a standard density
µ from which samples can be drawn easily. Once we have calculated the transport map, standard
samples are transformed by the push-forward diffeomorphism to generate samples from the target
density ν. This approach has been followed in several articles, where the optimal transport map
selection was based on both, the Fisher-Rao metric (Bauer et al., 2017) and the Knothe–Rosenblatt
rearrangement (Marzouk et al., 2016). The efﬁcient implementation of the present paper directly
leads to an efﬁcient random sampling algorithm in high dimensions."
DIFFEOMORPHIC RANDOM SAMPLING AND DENSITY ESTIMATION,0.233502538071066,"We now recall the density estimation problem using the OMT framework. We are given samples xi
drawn from an unknown density µ ∈P(Ω) that we aim to estimate. The main idea of our algorithm
is to represent the unknown density as the pullback via a (diffeomorphic) Brenier map ∇u of a given
background density ν = gdx, i.e., (∇u)∗ν = µ or equivalently using the push forward action as
(∇u)∗µ = ν, where the pull back and push forward of a density are deﬁned in Section 2.1."
DIFFEOMORPHIC RANDOM SAMPLING AND DENSITY ESTIMATION,0.23857868020304568,"As we do not have an explicit target density, but only a ﬁnite number of samples, we need to ﬁnd a
replacement for the L2-norm used in equation 3 to estimate the transport map ∇u. We do this by
maximizing the log-likelihood of the data with respect to the density (∇u)∗ν:"
N,0.2436548223350254,"1
N X"
N,0.24873096446700507,"i
log
 
det(D2(u(xi))) · g(∇u(xi))

.
(6)"
N,0.25380710659898476,"Using our PINNs framework, we represent the convex function u again via an ICNN of the same
architecture , which serves as an implicit regularizer, see Sivaprasad et al. (2021). This equation
can be alternatively interpreted as minimizing the empirical Kullback-Leibler divergence between µ
and the pullback of the background density ν. To generate new samples from the estimated density,
we use the inverse map to transform the samples from the background density ν. We calculate the
inverse map using a second neural network and explicit loss function given by equation 5."
N,0.25888324873096447,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.2639593908629442,"3
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.26903553299492383,"In this section, we will detail our implementation and present several experiments demonstrating
both the applicability and accuracy of our framework. In particular, we will compare our results in
several experiments to state-of-the-art deep learning-based OMT solvers, and we will show that we
outperform these methods in terms of accuracy."
NETWORK DETAILS,0.27411167512690354,"3.1
NETWORK DETAILS"
NETWORK DETAILS,0.27918781725888325,"As explained in Section 2.2, we use an ICNN architecture for both the forward and the backward
map in all of our experiments, c.f. Figure 4 in Appendix A. As with every deep learning approach,
we need to tune the hyperparameters, including width/depth of the network, activation functions and
batch size. The width of the network needs to increase with the dimension of the ambient space of
the data to ensure sufﬁcient ﬂexibility. For our experiments in lower dimensions, we used a network
with three hidden layers with 128 neurons in each layer, whereas for experiments in 30d, we used a
network with four hidden layers with 128 neurons in each layer. To initialize the network, we ﬁrst
train the networks to learn the identity transformation, i.e., ∇u = I, which we use as the initial
starting point for all our experiments. In all our experiments, 10,000 target samples were used."
NETWORK DETAILS,0.28426395939086296,"To guarantee the convexity of the output function, the activation functions need to be convex and
non-decreasing. Since simple ReLUs are not strictly convex and have a vanishing second derivative
almost everywhere, we experimented with the family of Rectiﬁed Power Units (RePUs), the log
exponential family and the ’Softplus’ function. The Softplus function to the power of α, which is
deﬁned via Softplusα(x) = (log (1 + exp x))α, turned out to be best suited for our applications,
where we chose α = 1.1. In particular our experiments suggested that networks with this activation
function were able to generalize well to regions where no or only limited training data were available."
VALIDATION AND COMPARISON TO OTHER METHODS,0.2893401015228426,"3.2
VALIDATION AND COMPARISON TO OTHER METHODS"
VALIDATION AND COMPARISON TO OTHER METHODS,0.29441624365482233,"To demonstrate the accuracy of our implementation, we present two different types of experiments:
ﬁrst we will conduct a series of experiments in which analytic solutions to the OMT problem are
available, where we compare our results to results obtained with two state-of-the-art deep learning-
based OMT solvers: OT-ICNN from Makkuva et al. (2020) and W2Gen from Korotin et al. (2019).
We choose these two speciﬁc algorithms among the available plethora of available OMT solvers
based on the recent benchmark paper by Korotin et al. (2021). In a second set of experiments we
will consider density estimation problems, where we will compare the quality of our results with four
other methods; the two OMT solvers from above and two methods from the family of normalizing
ﬂows: RealNVP as introduced in Dinh et al. (2016) and iResNet from Behrmann et al. (2019).
Approximating the OMT map: Since both OT-ICNN and W2Gen are also based on an ICNN
structure, we were able to choose the same architecture with same hyperparameters for all three
algorithms, thereby ensuring a fair comparison. Nevertheless, we want to emphasize that these
parameters could be further ﬁne tuned for all the algorithms and speciﬁc experiments to improve the
results. Nevertheless, with exception of several experiments for the OT-ICNN solver, we observed
a good convergence behavior. We present selected convergence graphs in appendix B. Furthermore
we demonstrate the scalabilty of our algorithm by performing the same experiment in dimensions 2,
3, 5, 8, 15 and 30."
VALIDATION AND COMPARISON TO OTHER METHODS,0.29949238578680204,"We do not present comparisons of our approach to more traditional OMT algorithms such as the
Sinkhorn algorithm (Cuturi, 2013b) or the linear programming approaches (Peyr´e et al., 2019), as
these frameworks, although they approximate the OMT distances, do not compute the continuous
optimal transport map, which is essential for the proposed density estimation. While ﬁnite element
or ﬁnite difference based Monge-Ampere solvers, see e.g. (Benamou & Duval, 2019; Jacobs &
L´eger, 2020; Benamou & Brenier, 2000), calculate the continuous OMT map, they are not suitable
in dimensions greater than two or three."
VALIDATION AND COMPARISON TO OTHER METHODS,0.30456852791878175,"To quantify the quality of an estimated transport plan T, we calculate the L2-UVP unexplained vari-
ance percentage (UVP), which is given by L2- UVP(T) = 100 · ∥T −T ∗∥2
L2(µ)/ Var(ν), where
T ∗denotes the (analytic) optimal transport plan. In appendix D we present in addition the percent-
age error between the analytic Wasserstein distance and the approximated distance, which can be
viewed as second, albeit coarser, measure of quality. Our ﬁrst series of experiments is the same as"
VALIDATION AND COMPARISON TO OTHER METHODS,0.3096446700507614,Under review as a conference paper at ICLR 2022
VALIDATION AND COMPARISON TO OTHER METHODS,0.3147208121827411,L2- Unexplained Variance Percentage (L2-UVP)
VALIDATION AND COMPARISON TO OTHER METHODS,0.3197969543147208,"Experiment
Method"
VALIDATION AND COMPARISON TO OTHER METHODS,0.3248730964467005,"Dimensions
2d
3d
5d
8d
15d
30d"
VALIDATION AND COMPARISON TO OTHER METHODS,0.3299492385786802,Random Cvx Function
VALIDATION AND COMPARISON TO OTHER METHODS,0.3350253807106599,"PICANNs
0.004
0.007
0.021
0.034
0.144
0.38
OT-ICNN
0.043
0.052
0.145
0.276
0.746
3.98
W2GEN
0.040
0.043
0.046
0.052
0.150
0.60"
VALIDATION AND COMPARISON TO OTHER METHODS,0.3401015228426396,Random Gaussian
VALIDATION AND COMPARISON TO OTHER METHODS,0.34517766497461927,"PICANNs
0.33
0.15
0.15
0.28
0.30
1.13
OT-ICNN
0.28
0.71
0.86
2.38
2.84
2.24
W2GEN
0.17
0.14
0.16
0.23
0.37
0.67"
VALIDATION AND COMPARISON TO OTHER METHODS,0.350253807106599,Annulus
VALIDATION AND COMPARISON TO OTHER METHODS,0.3553299492385787,"PICANNs
0.29
0.43
0.63
1.61
7.53
21.71
OT-ICNN
23.84
9.98
28.21
43.87
44.52
2725.15
W2GEN
1.33
6.86
18.31
20.50
23.28
34.19"
VALIDATION AND COMPARISON TO OTHER METHODS,0.3604060913705584,"Table 1: We present a comparison between our PICANN approach and OT-ICNN from Makkuva
et al. (2020) and W2Gen from Korotin et al. (2019). We present the L2-UVP for three experiments.
In all these experiments, the source density is the unit Gaussian. The target density in the case of
the ”Random Cvx Function” experiment is the unit Gaussian deformed by the gradient of a random
convex function. In the case ”Random Gaussian”, the target density is another Gaussian with a
randomly sampled mean and co-variance matrix. In the third experiment, the target density is the
annulus distribution. The results in the ﬁrst two experiments are averages of 20 realizations."
VALIDATION AND COMPARISON TO OTHER METHODS,0.36548223350253806,"in the benchmark paper (Korotin et al., 2021): we use the gradient of a random convex function to
transport the unit Gaussian to a random density. By Brenier’s Theorem, as the optimal transport
map is the gradient of a convex function, this map is the optimal transport map. In each dimen-
sion, we repeated this experiment 20 times to compute the error statistics, which are presented in
Table 1. Wheras all three algorithms seem to work well for this experiment, PICANNs consistently
outperform the other algorithms. As already observed in Korotin et al. (2021), this experiment is
favoring the ICNN architecture, as the true solution was chosen to be of the same nature, which
explains the nearly perfect performance of all three algorithms. Next, we turn to cases where the
analytical solution is known in closed form. The ﬁrst is the special case where both densities are
from a family of Gaussians distributions. In that case, the OMT map is simply given by an afﬁne
transform and the OMT distance is again given in closed form. We again repeat this experiment
20 times for each dimension, where we generate Gaussian distributions with a random mean and
covariances. Here the means are sampled from a uniform distribution on [−1, 1]. To construct the
random covariance matrices, we recall that we need to enforce the matrix to be positive deﬁnite and
symmetric. Therefore, we generate a random matrix A of dimension d × 3d, where d is the dimen-
sion of the space and where the entries are i.i.d. chosen from a uniform distribution on [0, 0.75].
Then, a random covariance matrix can be constructed by letting Σ = AAT (the particular form of
Σ almost surely guarantees positive deﬁniteness). The results and comparisons with the other two
methods are again presented in Table 1. In general, all three algorithms still lead to a good ap-
proximation, where one can see that W2Gen and PICANNs are performing signiﬁcantly better than
OT-ICNN. To further validate our algorithm, we choose a more challenging problem: an annulus
density for which we know the transport map in closed form. The annulus distribution is given by
a push forward of the Gaussian distribution by a gradient of a radially symmetric convex function
and is given by f = g((XT X)X) · 3(XT X)d where g is the unit Gaussian and d is the number of
dimensions. The transport map X 7→(XT X)X is the gradient of the convex function 1"
VALIDATION AND COMPARISON TO OTHER METHODS,0.37055837563451777,"4(XT X)2
and thus by Brenier’s theorem the optimal transport map. The results in Table 1 clearly demonstrates
that, in particular for this more challenging problem, the proposed PICANNs outperform the other
algorithms by orders of magnitudes."
VALIDATION AND COMPARISON TO OTHER METHODS,0.3756345177664975,"Density estimation comparisons: Next we consider the problem of estimating a continuous density
from discrete ﬁnite samples. In this part we will compare our method in addition to two methods
from the normalized ﬂow family, namely RealNVP and iResNet. For both these methods we choose
the standard network structure as used in the examples provided by the developers, i.e., for RealNVP
we used a fully connected network with 3 hidden layers with 256 neurons in each layer and for
iResNets we used a fully connected network with 4 hidden layers with 128 neurons in each layer."
VALIDATION AND COMPARISON TO OTHER METHODS,0.38071065989847713,Under review as a conference paper at ICLR 2022
VALIDATION AND COMPARISON TO OTHER METHODS,0.38578680203045684,"We want to emphasize that NODE and normalized ﬂows do not solve the OMT problem and con-
sequently there is no physically motivated optimality conditions of the transport map generated by
these methods. This is also reﬂected in our experiments: in the OMT approach we search for the
map that is as close as possible to the identity among all transformations that satisfy the required
density matching constraint (note, that there is an inﬁnite dimensional set of transformations that sat-
isfy that satisfy this constraints). Consequently the obtained transformation maps for OMT are more
regular as compared to the maps obtained with other density estimation methods such as iResNet or
RealNVP, cf. Figures 1,2,7 and 8."
VALIDATION AND COMPARISON TO OTHER METHODS,0.39086294416243655,"As a ﬁrst example, we consider again the annulus density, that we introduced in the last experiment
of the previous section. Figure 1 shows the comparison of ﬁve different methods for estimating the
density as well as the transport maps. To quantify the ability to approximate the true density we also
report the KL-divergence between the estimated density (i.e., the push-forward of the unit gaussian
by the estimated transport map) and the given samples. While W2Gen, RealNVP and iResNet all
showed a decent estimation of the annulus density, the quality of their approximations is clearly
inferior to our PICANNs approach. Even after extensive parameter tuning we could not get the OT-
ICNN algorithm to converge to the true density. Finally, we note that RealNVP and iResNet do not
aim to compute the L2 optimal transport map. This is exempliﬁed in Figure 1, where one can see that
the estimated maps are very far from the optimal transport map for these two algorithms. A second
example in which we consider a nonsymmetric distribution that has been constructed in Bauer et al.
(2017) can be seen in Figure 2. Similar as for the annulus density our PICANNs approach produces
the best result. Further examples and comparisons can be found in Appendix C."
VALIDATION AND COMPARISON TO OTHER METHODS,0.39593908629441626,"Ground Truth
PICANNs
W2GEN
OT-ICNN
Real NVP
iResNet"
VALIDATION AND COMPARISON TO OTHER METHODS,0.4010152284263959,"Figure 1: Comparison of different density estimation frameworks. First line: the estimated densities.
Second line: the corresponding transport map."
VALIDATION AND COMPARISON TO OTHER METHODS,0.40609137055837563,"Ground Truth
PICANNs
W2GEN
OT-ICNN
Real NVP
iResNet"
VALIDATION AND COMPARISON TO OTHER METHODS,0.41116751269035534,"Figure 2: Comparison of different density estimation frameworks. First line: the estimated densities.
Second line: the corresponding transport map. Note, that we do not have access to the analytic
optimal transport map in this example."
VALIDATION AND COMPARISON TO OTHER METHODS,0.41624365482233505,Under review as a conference paper at ICLR 2022
GENERATIVE MODELING WITH PICANNS,0.4213197969543147,"3.3
GENERATIVE MODELING WITH PICANNS"
GENERATIVE MODELING WITH PICANNS,0.4263959390862944,"Finally, we present preliminary results to demonstrate the application of the PICANN approach to
develop a generative model. It has been well established that for most real world image modeling
applications the support of the target probability distribution is a lower dimensional manifold Gerber
et al. (2009); Lee & Verleysen (2007). Autoencoders are very effective in non-linear dimensionality
reduction and map high-dimensional data to a latent space of lower dimensions, which then can
be transformed back to the original space using the ”decoder” part of the network. One of the
major shortcoming of an autoencoder is that the distribution of data mapped in to the latent space
is not know and cannot be assumed to be Gaussian. In Appendix A, Fig 5 we summarizes how the
autoencoder can be naturally included in our PICANN approach to obtain an efﬁcient generative
model for high-dimensional data."
GENERATIVE MODELING WITH PICANNS,0.43147208121827413,"To demonstrate the generative algorithm in a toy example, we consider the MNIST dataset encoded
using a simple fully connected autoencoder to a latent space of dimension 2. Figure 3 shows the
encoded data points with each class assigned a unique color. We train a PICANN network of 5
hidden layers with 128 neurons in each layer, to learn the forward and the inverse mapping from a
Gaussian with the mean and covariance of the encoded data to the unknown ”latent MNIST distribu-
tion”. Figure 3 presents comparisons to a similar setup with the four other methods discussed in the
previous section. The results show signiﬁcant differences in the quality of the generated samples,
where PICANNs and RealNVP visually lead to the best results. In future work we plan to continue
these investigations, by applying the setup to more challenging image sets and carefully comparing
the results using state-of-the-art quality metrics."
GENERATIVE MODELING WITH PICANNS,0.4365482233502538,"Ground Truth
PICANNs
W2GEN
OT-ICNN
Real NVP
iResNet"
GENERATIVE MODELING WITH PICANNS,0.4416243654822335,"Figure 3: Generative model: this ﬁgure details the application of our generative framework to the
MNIST dataset. Five different algorithms were trained to estimate this distribution and learn the
forward and inverse transport map between the encoded ’MNIST Distribution’ and a Gaussian. The
estimated density can be seen in the ﬁrst line. The second line shows 25 samples passed through the
decoder part of the autoencoder for each of the methods."
CONCLUSION,0.4467005076142132,"4
CONCLUSION"
CONCLUSION,0.4517766497461929,"In this paper, we use the L2-Wasserstein metric and optimal mass transport (OMT) theory to for-
mulate a density estimation and generative modeling framework. We develop a new deep learning-
based solver for the continuous OMT problem, which is rooted in Brenier’s celebrated theorem.
This theorem allows us to formulate the density estimation problem as a solution to a nonlinear PDE
– a Monge-Ampere equation. Recent developments in deep learning for PDEs, namely PINNS and
ICNNs, allow us to develop an efﬁcient solver. We demonstrate the accuracy of our framework by
comparing our results to analytic Wasserstein distances. To further quantify the quality of our re-
sults we compare them to the results obtained with the two best performing algorithms of the recent
benchmark paper for deep learning based OMT solvers (Korotin et al., 2021). Our experiments show
that our approach signiﬁcantly outperforms these methods in term of accuracy. Finally, we present
examples of diffeomorphic density estimation within our framework and showcase an example of a
generative model."
CONCLUSION,0.45685279187817257,Under review as a conference paper at ICLR 2022
REFERENCES,0.4619289340101523,REFERENCES
REFERENCES,0.467005076142132,"Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Confer-
ence on Machine Learning, pp. 146–155. PMLR, 2017."
REFERENCES,0.4720812182741117,"Sigurd Angenent, Steven Haker, and Allen Tannenbaum.
Minimizing ﬂows for the monge–
kantorovich problem. SIAM journal on mathematical analysis, 35(1):61–97, 2003."
REFERENCES,0.47715736040609136,"Ilya J Bakelman. Variational problems and elliptic monge-ampere equations. Journal of differential
geometry, 18(4):669–699, 1983."
REFERENCES,0.48223350253807107,"Martin Bauer, Sarang Joshi, and Klas Modin. Diffeomorphic random sampling using optimal infor-
mation transport. In International Conference on Geometric Science of Information, pp. 135–142.
Springer, 2017."
REFERENCES,0.4873096446700508,"Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and J¨orn-Henrik Jacobsen.
Invertible residual networks. In International Conference on Machine Learning, pp. 573–582.
PMLR, 2019."
REFERENCES,0.49238578680203043,"Jean-David Benamou and Yann Brenier. A computational ﬂuid mechanics solution to the monge-
kantorovich mass transfer problem. Numerische Mathematik, 84(3):375–393, 2000."
REFERENCES,0.49746192893401014,"Jean-David Benamou and Vincent Duval. Minimal convex extensions and ﬁnite difference discreti-
sation of the quadratic monge–kantorovich problem. European Journal of Applied Mathematics,
30(6):1041–1078, 2019."
REFERENCES,0.5025380710659898,"Jean-David Benamou, Brittany D Froese, and Adam M Oberman. Two numerical methods for the
elliptic monge-ampere equation. ESAIM: Mathematical Modelling and Numerical Analysis, 44
(4):737–758, 2010."
REFERENCES,0.5076142131979695,"Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Commu-
nications on pure and applied mathematics, 44(4):375–417, 1991."
REFERENCES,0.5126903553299492,"Guillaume Carlier, Alfred Galichon, and Filippo Santambrogio. From knothe’s transport to brenier’s
map and a continuation method for optimal transport. SIAM Journal on Mathematical Analysis,
41(6):2554–2576, 2010."
REFERENCES,0.5177664974619289,"Marco Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In NIPS, volume 2,
pp. 4, 2013a."
REFERENCES,0.5228426395939086,"Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural
information processing systems, 26:2292–2300, 2013b."
REFERENCES,0.5279187817258884,"Guido De Philippis and Alessio Figalli. W 2, 1 regularity for solutions of the monge–amp`ere equa-
tion. Inventiones mathematicae, 192(1):55–69, 2013."
REFERENCES,0.5329949238578681,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016."
REFERENCES,0.5380710659898477,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.
net/forum?id=HkpbnH9lx."
REFERENCES,0.5431472081218274,"Evelyn Fix and Joseph Lawson Hodges. Nonparametric discrimination: Consistency properties.
Randolph Field, Texas, Project, pp. 21–49, 1951."
REFERENCES,0.5482233502538071,"Liliana Forzani and Diego Maldonado. Properties of the solutions to the monge–amp`ere equation.
Nonlinear Analysis: Theory, Methods & Applications, 57(5-6):815–829, 2004."
REFERENCES,0.5532994923857868,"Samuel Gerber, Tolga Tasdizen, Sarang Joshi, and Ross Whitaker. On the manifold structure of the
space of brain images. In International conference on medical image computing and computer-
assisted intervention, pp. 305–312. Springer, 2009."
REFERENCES,0.5583756345177665,Under review as a conference paper at ICLR 2022
REFERENCES,0.5634517766497462,"Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. stat, 1050:10, 2014."
REFERENCES,0.5685279187817259,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017."
REFERENCES,0.5736040609137056,"Insu Han, Haim Avron, and Jinwoo Shin. Stochastic chebyshev gradient descent for spectral opti-
mization. In 32nd Conference on Neural Information Processing Systems (NIPS). Neural Infor-
mation Processing Systems Foundation, 2018."
REFERENCES,0.5786802030456852,W Keith Hastings. Monte carlo sampling methods using markov chains and their applications. 1970.
REFERENCES,0.583756345177665,"Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving ﬂow-
based generative models with variational dequantization and architecture design. In International
Conference on Machine Learning, pp. 2722–2730. PMLR, 2019."
REFERENCES,0.5888324873096447,"Matt Jacobs and Flavien L´eger. A fast approach to optimal transport: The back-and-forth method.
Numerische Mathematik, 146(3):513–544, 2020."
REFERENCES,0.5939086294416244,"Xiaowei Jin, Shengze Cai, Hui Li, and George Em Karniadakis. Nsfnets (navier-stokes ﬂow nets):
Physics-informed neural networks for the incompressible navier-stokes equations. Journal of
Computational Physics, 426:109951, 2021."
REFERENCES,0.5989847715736041,"Diederik P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions.
arXiv preprint arXiv:1807.03039, 2018."
REFERENCES,0.6040609137055838,"Herbert Knothe. Contributions to the theory of convex bodies. Michigan Mathematical Journal, 4
(1):39–52, 1957."
REFERENCES,0.6091370558375635,"Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing ﬂows: An introduction and review
of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020."
REFERENCES,0.6142131979695431,"Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Saﬁn, and Evgeny Burnaev.
Wasserstein-2 generative networks. arXiv preprint arXiv:1909.13082, 2019."
REFERENCES,0.6192893401015228,"Alexander Korotin, Lingxiao Li, Aude Genevay, Justin Solomon, Alexander Filippov, and Evgeny
Burnaev. Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark. arXiv
preprint arXiv:2106.01954, 2021."
REFERENCES,0.6243654822335025,"John A Lee and Michel Verleysen. Nonlinear dimensionality reduction. Springer Science & Busi-
ness Media, 2007."
REFERENCES,0.6294416243654822,"Huidong Liu, Xianfeng Gu, and Dimitris Samaras. Wasserstein gan with quadratic transport cost.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4832–4841,
2019."
REFERENCES,0.6345177664974619,"Clive R Loader et al. Bandwidth selection: classical or plug-in? The Annals of Statistics, 27(2):
415–438, 1999."
REFERENCES,0.6395939086294417,"Gr´egoire Loeper and Francesca Rapetti. Numerical solution of the monge–amp`ere equation by a
newton’s algorithm. Comptes Rendus Mathematique, 340(4):319–324, 2005."
REFERENCES,0.6446700507614214,"Ashok Makkuva, Amirhossein Taghvaei, Sewoong Oh, and Jason Lee. Optimal transport mapping
via input convex neural networks. In International Conference on Machine Learning, pp. 6672–
6681. PMLR, 2020."
REFERENCES,0.649746192893401,"Anton Mallasto, Jes Frellsen, Wouter Boomsma, and Aasa Feragen. (q, p)-wasserstein gans: Com-
paring ground metrics for wasserstein gans. arXiv preprint arXiv:1902.03642, 2019."
REFERENCES,0.6548223350253807,"Youssef Marzouk, Tarek Moselhy, Matthew Parno, and Alessio Spantini. Sampling via measure
transport: An introduction. Handbook of uncertainty quantiﬁcation, pp. 1–41, 2016."
REFERENCES,0.6598984771573604,"Levi McClenny and Ulisses Braga-Neto. Self-adaptive physics-informed neural networks using a
soft attention mechanism. arXiv preprint arXiv:2009.04544, 2020."
REFERENCES,0.6649746192893401,Under review as a conference paper at ICLR 2022
REFERENCES,0.6700507614213198,"Geoffrey J McLachlan and Kaye E Basford. Mixture models: Inference and applications to cluster-
ing, volume 38. M. Dekker New York, 1988."
REFERENCES,0.6751269035532995,"Gaspard Monge. M´emoire sur la th´eorie des d´eblais et des remblais. Histoire de l’Acad´emie Royale
des Sciences de Paris, 1781."
REFERENCES,0.6802030456852792,"Quan Hoang Nhan Dam, Trung Le, Tu Dinh Nguyen, Hung Bui, and Dinh Phung. Threeplayer
wasserstein gan via amortised duality. In Proc. of the 28th Int. Joint Conf. on Artiﬁcial Intelligence
(IJCAI), 2019."
REFERENCES,0.6852791878172588,"Nicolas Papadakis. Optimal transport for image processing. PhD thesis, Universit´e de Bordeaux;
Habilitation thesis, 2015."
REFERENCES,0.6903553299492385,"George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive ﬂow for density
estimation. arXiv preprint arXiv:1705.07057, 2017."
REFERENCES,0.6954314720812182,"Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathe-
matical statistics, 33(3):1065–1076, 1962."
REFERENCES,0.700507614213198,"Gabriel Peyr´e, Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019."
REFERENCES,0.7055837563451777,"Maziar Raissi, Paris Perdikaris, and George Em Karniadakis.
Physics informed deep learn-
ing (part i): Data-driven solutions of nonlinear partial differential equations.
arXiv preprint
arXiv:1711.10561, 2017."
REFERENCES,0.7106598984771574,"Murray Rosenblatt. Remarks on a multivariate transformation. The annals of mathematical statis-
tics, 23(3):470–472, 1952a."
REFERENCES,0.7157360406091371,"Murray Rosenblatt. Remarks on a multivariate transformation. The annals of mathematical statis-
tics, 23(3):470–472, 1952b."
REFERENCES,0.7208121827411168,"Vivien Seguy, Bharath Bhushan Damodaran, R´emi Flamary, Nicolas Courty, Antoine Rolet, and
Mathieu Blondel.
Large-scale optimal transport and mapping estimation.
arXiv preprint
arXiv:1711.02283, 2017."
REFERENCES,0.7258883248730964,"Yeonjong Shin, Jerome Darbon, and George Em Karniadakis. On the convergence and generaliza-
tion of physics informed neural networks. arXiv preprint arXiv:2004.01806, 2020."
REFERENCES,0.7309644670050761,"Bernard W Silverman and M Christopher Jones. E. ﬁx and jl hodges (1951): An important con-
tribution to nonparametric discriminant analysis and density estimation: Commentary on ﬁx and
hodges (1951). International Statistical Review/Revue Internationale de Statistique, pp. 233–238,
1989."
REFERENCES,0.7360406091370558,"Sarath Sivaprasad, Ankur Singh, Naresh Manwani, and Vineet Gandhi. The curious case of convex
neural networks. In Nuria Oliver, Fernando P´erez-Cruz, Stefan Kramer, Jesse Read, and Jose A.
Lozano (eds.), Machine Learning and Knowledge Discovery in Databases. Research Track, pp.
738–754, Cham, 2021. Springer International Publishing."
REFERENCES,0.7411167512690355,"Amirhossein Taghvaei and Amin Jalali. 2-wasserstein approximation via restricted convex potentials
with application to improved training for gans. arXiv preprint arXiv:1902.07197, 2019."
REFERENCES,0.7461928934010152,"C´edric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003."
REFERENCES,0.751269035532995,"C´edric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008."
REFERENCES,0.7563451776649747,"Laurent Younes. Diffeomorphic learning. Journal of Machine Learning Research, 21(220):1–28,
2020. URL http://jmlr.org/papers/v21/18-415.html."
REFERENCES,0.7614213197969543,"Dongkun Zhang, Ling Guo, and George Em Karniadakis. Learning in modal space: Solving time-
dependent stochastic pdes using physics-informed neural networks. SIAM Journal on Scientiﬁc
Computing, 42(2):A639–A665, 2020."
REFERENCES,0.766497461928934,Under review as a conference paper at ICLR 2022
REFERENCES,0.7715736040609137,"A
NETWORK STRUCTURES"
REFERENCES,0.7766497461928934,"In this section we present schematics of our PICANNs approach (Figure 4) and how it can be in-
cluded in a generative model (Figure 5)."
REFERENCES,0.7817258883248731,"ICNN (
; 
)"
REFERENCES,0.7868020304568528,ICNN (x; ) W0(x) W1(x)
REFERENCES,0.7918781725888325,Wk-1(x) x
REFERENCES,0.7969543147208121,"W1(z)
z1
z2
zk W0(x) W1(x)"
REFERENCES,0.8020304568527918,Wk-1(x)
REFERENCES,0.8071065989847716,"W1(z)
z1
z2
zk
x e"
REFERENCES,0.8121827411167513,"Figure 4: PICANN architecture. We present how a combination of two ICNN networks can be used
to learn the forward and the inverse map between two distributions. Both these networks are trained
independently with their respective loss functions. The inverse network uses the gradient of the
output of the ﬁrst network as its input."
REFERENCES,0.817258883248731,Under review as a conference paper at ICLR 2022
REFERENCES,0.8223350253807107,Autoencoder
REFERENCES,0.8274111675126904,"ICNN (
; 
)"
REFERENCES,0.8324873096446701,ICNN (x; ) W0(x) W1(x)
REFERENCES,0.8375634517766497,Wk-1(x) x
REFERENCES,0.8426395939086294,"W1(z)
z1
z2
zk W0(x) W1(x)"
REFERENCES,0.8477157360406091,Wk-1(x)
REFERENCES,0.8527918781725888,"W1(z)
z1
z2
zk
x"
REFERENCES,0.8578680203045685,"a2
a3
e
d1
d2"
REFERENCES,0.8629441624365483,"Reshape
I
a1"
REFERENCES,0.868020304568528,"Reshape
d3
I' e"
REFERENCES,0.8730964467005076,"Figure 5: How a combination of an autoencoder with our PICANN approach can be used to develop
a generative model. Note how the latent space of the autoencoder becomes the input to the PICANN
network. In such a setting, the PICANN estimates the latent space density and samples from the
estimated distribution. Using random samples from this distribution, one can pass them through the
decoder to generate new samples."
REFERENCES,0.8781725888324873,"B
CONVERGENCE OF EXPERIMENTS"
REFERENCES,0.883248730964467,"In this appendix (Figure 6) we present the convergence plots for a several experiments of Table 1. It
should be noted that PICANNs and W2GEN are pure minimisation algorithms while OT-ICNN is a
min-max algorithm and hence the covergence plots for OT-ICNN differ from the convergence plots
for the other two algorithms. These convergence plots show that PICANNs and W2GEN converge
for all the experiments presented here while the OT-ICNN, as remarked in Section 3.2, does not
converge in the case of the annulus experiments. This is also the case for the annulus experiments in
higher dimensions (not shown here). In table 2 we also present the average timing for 100 Epochs
for these three algorithms in different dimensions."
REFERENCES,0.8883248730964467,"Method
2d
3d
5d
8d
15d
30d
PICANNs
90s
95s
130s
165s
233s
310s
W2Gen
110s
110s
112s
115s
118s
120s
OT-ICNN
180s
180s
185s
186s
197s
210s"
REFERENCES,0.8934010152284264,Table 2: Computational cost for 100 epochs in various dimensions.
REFERENCES,0.8984771573604061,Under review as a conference paper at ICLR 2022
REFERENCES,0.9035532994923858,"(a) PICANNs - Annulus in 2d
(b) W2GEN - Annulus in 2d
(c) OT-ICNN - Annulus in 2d"
REFERENCES,0.9086294416243654,"(d) PICANNs - RG in 2d
(e) W2GEN - RG in 2d
(f) OT-ICNN - RG in 2d"
REFERENCES,0.9137055837563451,"Figure 6: In this ﬁgure we present the convergence plots for PICANNs, W2GEN and OT-ICNN.
In row 1 we present convergence plots for the annulus experiment in 2d for PICANNs, W2GEN
and OT-ICNN in Panels (a), (b) and (c) respectively. In row 2 we present convergence plots for the
gaussian experiments performed using again PICANNs, W2GEN and OT-ICNN in Panels (d), (e)
and (f) respectively. As the OT-ICNN algorithm is a min-max algorithm these convergence plots
have to be read slightly differently. For the gaussian experiment, the loss starts with small negative
values and climbs up to a small positive quantity but around the 50 epoch mark it has converged
to about 0 and just bounces in that neighbourhood thereafter. The OT-ICNN algorithm does not
converge for the annulus experiment, as remarked in Section 3.2."
REFERENCES,0.9187817258883249,"C
FURTHER DENSITY ESTIMATION EXAMPLES AND COMPARISONS"
REFERENCES,0.9238578680203046,"Ground Truth
PICANNs
W2GEN
OT-ICNN
Real NVP
iResNet"
REFERENCES,0.9289340101522843,"Figure 7: Comparison of different density estimation frameworks for a Gaussian mixture model.
First line: the estimated densities. Second line: the corresponding transport map. Note, that we do
not have access to the analytic optimal transport map in this example."
REFERENCES,0.934010152284264,Under review as a conference paper at ICLR 2022
REFERENCES,0.9390862944162437,"Ground Truth
PICANNs
W2GEN
OT-ICNN
Real NVP
iResNet"
REFERENCES,0.9441624365482234,"Figure 8: Comparison of different density estimation frameworks. First line: the estimated densities.
Second line: the corresponding transport map. Note, that we do not have access to the analytic
optimal transport map in this example."
REFERENCES,0.949238578680203,"D
ERROR BETWEEN TRUE AND APPROXIMATED WASSERSTEIN DISTANCE"
REFERENCES,0.9543147208121827,"In this appendix we present the percentage error between the theoretical and the approximated
Wasserstein metric for three different algorithms: PICANNs, OTICNN from Makkuva et al. (2020)
and W2Gen from Korotin et al. (2019). The exact experimental setup and the network details are
described in section 3.2."
REFERENCES,0.9593908629441624,Avg % error between true and approximated Wasserstein distance
REFERENCES,0.9644670050761421,Random Cvx Function
REFERENCES,0.9695431472081218,"PICANNs
0.12
0.05
0.03
0.03
0.02
0.04
OT-ICNN
0.10
0.10
0.08
0.07
0.10
0.09
W2GEN
0.09
0.067
0.03
0.04
0.06
0.52"
REFERENCES,0.9746192893401016,Random Gaussian
REFERENCES,0.9796954314720813,"PICANNs
1.56
0.88
0.35
0.21
0.19
0.15
OT-ICNN
1.66
1.40
0.93
0.95
0.27
0.31
W2GEN
1.59
0.75
0.41
0.25
0.35
0.19"
REFERENCES,0.9847715736040609,Annulus
REFERENCES,0.9898477157360406,"PICANNs
5.37
1.25
1.81
1.56
7.44
5.07
OT-ICNN
6.54
25.89
33.50
20.88
2.03
36.13
W2GEN
12.36
19.39
8.10
3.34
0.96
0.66"
REFERENCES,0.9949238578680203,"Table 3: We present a comparison between our PICANN approach and Makkuva et al. (2020) and
Korotin et al. (2019). In this table, we present the L2-UVP and the percentage error between the
theoretical W2 metric and the approximated W2 metric for three experiments. In all these experi-
ments, the source density is the unit Gaussian. The target density in the case of the ”Random Cvx
Function” experiment is the unit Gaussian deformed by the gradient of a random convex function.
In the case ”Random Gaussian”, the target density is another Gaussian with a randomly sampled
mean and co-variance matrix. In the third experiment, the target density is the annulus distribution.
The results in the ﬁrst two experiments are averages of 20 realizations."
