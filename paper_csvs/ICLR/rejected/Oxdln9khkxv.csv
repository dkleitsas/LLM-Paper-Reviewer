Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.008849557522123894,"Imitation learning is one of the methods for reproducing expert demonstrations
adaptively by learning a mapping between observations and actions. However,
behavior styles such as motion trajectory and driving habit depend largely on the
dataset of human maneuvers, and settle down to an average behavior style in most
imitation learning algorithms. In this study, we propose a method named style
behavior cloning (Style BC), which can not only infer the latent representation of
behavior styles automatically, but also imitate different style policies from expert
demonstrations. Our method is inspired by the word2vec algorithm and we con-
struct a behavior-style to action mapping which is similar to the word-embedding
to context mapping in word2vec. Empirical results on popular benchmark envi-
ronments show that Style BC outperforms standard behavior cloning in prediction
accuracy and expected reward signiﬁcantly. Furthermore, compared with various
baselines, our policy inﬂuenced by its assigned style embedding can better repro-
duce the expert behavior styles, especially in the complex environments or the
number of the behavior styles is large."
INTRODUCTION,0.017699115044247787,"1
INTRODUCTION"
INTRODUCTION,0.02654867256637168,"Deep reinforcement learning (DRL), which combines reinforcement learning and deep neural net-
works, has achieved rapid development in the last few years. Research institutions as well as indus-
trial companies have applied this technology to many domains, such as games (Atari games and Go)
(Silver et al., 2016; Van Hasselt et al., 2016), robotics control (Zhang et al., 2015) and autonomous
driving (Gonz´alez et al., 2016). However, as the DRL algorithms optimize the models according to
the reward function, it may not always generate desirable behaviors due to the inferior local con-
vergence or inappropriate reward design, especially in the real world problems where the reward
function is difﬁcult to clearly deﬁne."
INTRODUCTION,0.035398230088495575,"Imitation learning (IL) methods have the potential to cover this shortage, with which an agent learns
to perform a task from expert demonstrations by mimicking a mapping between observations and
actions instead of maximizing the accumulated reward (Osa et al., 2018). In these methods, the
behavior style such as the motion trajectory and operation habit is determined depending on the
dataset of expert demonstrations. In practise, the real world problems such as self-driving vehicles
(Munoz et al., 2009), assistive robots (Ikemoto et al., 2012) and human-computer interaction (Ross
& Bagnell, 2010) may need a large amount of demonstrations and these demonstrations are usually
collected from multiple human experts who may have different behavior styles. For the conventional
IL algorithms, the imitated behavior tends to converge into an average of the multiple behavior styles
in the demonstrations when the imitation model is trained. However, demonstrations from different
experts can show signiﬁcant variability because different human experts typically employ different
policies according to their own styles and habits, and learning an average behavior style maybe
unacceptable. For instance, in a driving scenario with a leading vehicle, aggressive drivers may
make the decision to change lanes. While some drivers are relatively conservative and will maintain
a stable following distance with the front vehicle by controlling the velocity, as illustrated in Figure
1. In these situations, the average behavior reproduced by imitation learning may not express any
kind of expert styles, or even worse results in an non-convergent policy (e.g., crashing the leading
car in the scenario of Figure 1)."
INTRODUCTION,0.04424778761061947,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05309734513274336,"(a)
(b)"
INTRODUCTION,0.061946902654867256,Figure 1: Driving behaviors of human: (a) Lane-changing; (b) Car-following.
INTRODUCTION,0.07079646017699115,"There are some algorithms considering behavior style in imitation learning. A classical way to learn
from demonstration is Dynamic Bayesian Network (Eppner et al., 2009), which does not use deep
learning. Another way is the constrained methods, such as Probabilistic Principle Component Anal-
ysis (Perico et al., 2019), which can take into account additional constraints such as speed limits
and obstacles. Although these algorithms can reproduce the behavior style from a small number
of demonstrations, due to a number of assumptions about the structure of sensory and control sig-
nals, such traditional methods are not scalable to more multi-dimensional and multi-modal systems,
such as systems involving tactile sensors and raw images. Alternatively, methods based on Genera-
tive Adversarial Imitation Learning (GAIL) combine supervised and reinforcement learning (Ho &
Ermon, 2016), which produces similar behaviors to the expert demonstrations in a simulation envi-
ronment. Based on this, Information Maximizing GAIL (Info-GAIL) (Li et al., 2017), which can
embed differences in behavior style in latent space using mutual information, has been used to pro-
duce different driver models in an autonomous driving environment. However, InfoGAIL involves
sampling a latent code at the beginning of each trial. While when the simulated surrounding ve-
hicles are initialized with the real driving environment information (such as velocity and position),
the random sampling of latent codes may not ensure consistency between the policy’s subsequent
actions and the driver’s behaviors. To address this limitation, a Burn-InfoGAIL algorithm (Kueﬂer
& Kochenderfer, 2017), built on Info-GAIL, draws latent features directly from a learned, inference
distribution. In addition, this burn-in policy must take over from the point at which a speciﬁc expert
demonstration ends."
INTRODUCTION,0.07964601769911504,"Building upon the above researches, this paper proposes a novel imitation learning framework named
Style Behavior Clone (Style BC), which can learn the representation of various behavior styles from
expert demonstrations. Not only can the model automatically learn a style embedding that is mean-
ingful to represent the behavior styles, but also accurately reproduces different expert behaviors.
Our approach is an extension of behavior cloning (BC) (Bojarski et al., 2016) and different from
current style involved imitation learning algorithms which try to infer the style embeddings through
inputting the behavior sequences, our method try to solve this problem in a more direct way: the
correct embeddings should improve the ﬁnal performance of BC and thus designing a framework
which can adjust the style embeddings and the policy network simultaneously according the ﬁnal
training performance."
INTRODUCTION,0.08849557522123894,"To be speciﬁc, this paper introduces a hypothesis that each policy has a latent feature c, i.e. style
embedding, which can determine its behavior style. Therefore, in order to improve the prediction
accuracy of the actions belonging to different styles, we need an accurate c to guide the behavior.
Our method is inspired by the word2vec (Mikolov et al., 2013) algorithm which is used to embed
the words in natural language processing (NLP). The word2vec algorithm try to compute the word
embedding through optimizing the model to predict the contexts accurately, and correspondingly,
Style BC tries to compute the behavior style embedding through optimize the model to predict the
action accurately. Finally, it is worth noting that our algorithm updates style embedding by making
the distinction of behaviors more clear, while Info-GAIL and related deformations disentangle the
latent c by inferring from the sequences of state-actions pairs."
INTRODUCTION,0.09734513274336283,"The main contributions of this paper are summarized as follows: (i) we propose a novel style behav-
ior cloning approach that learns the various representation of behavior styles from expert demon-
strations; (ii) the style embedding which can provide valuable guidance for the generation of style
policies is obtained automatically; (iii) ﬁnally, we empirically demonstrate the effectiveness of our
research on a Grid-World environment, cartpole control tasks from OpenAI gym, and a collection"
INTRODUCTION,0.10619469026548672,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.11504424778761062,"of environment for decision-making in autonomous driving named highway-env. We show that our
model can learn to represent different kinds of behavior styles by exploring the style embedding."
PROBLEM FORMULATION,0.12389380530973451,"2
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.13274336283185842,"In this section, we start with a brief introduction to the background including Markov Decision
Process (MDP) and style Markov decision process (style MDP). Then we provide a basic objective
function of behavior cloning to learn human behaviors from demonstrations. However, due to the
expert demonstrations may collect a variety of behaviors, the simple supervised policy from ofﬂine
demonstration may not have the ability to reproduce these motion styles."
BACKGROUND,0.1415929203539823,"2.1
BACKGROUND"
BACKGROUND,0.1504424778761062,"RL problems are often modeled as Markov Decision Processes (MDPs) which are deﬁned as a tuple
M = (S, A, P, R, γ). S and A represent states and actions respectively. P is a state transition
probability function, which can also be represented as P(s′|s, a). Further R :
S × A →R
is a reward function measuring the performance of agents and γ is a discount factor for future
rewards. Given an MDP M, the goal of the agent is to maximize the expectation of the discounted
accumulative reward JR = E [P∞
t=0 γtr(st, at, st+1)] automatically by ﬁnding an optimal policy
π∗."
BACKGROUND,0.1592920353982301,"In
this
paper,
we
assume
that
the
behavior
policy
is
a
mixture
of
behavior
styles
Q"
BACKGROUND,0.168141592920354,"E= {πB1 (.|s, c1) ,πB2 (.|s, c2) . . . πBn (.|s, cn) }, where c is a style embedding that can deter-
mine the behavior style of policy. The objective is to reproduce a policy π(a|s, c) as an approxima-
tion of πBi, and we use the tuple (S, A, C, P, R, γ) to deﬁne this process, named style MDP."
BEHAVIORAL CLONING,0.17699115044247787,"2.2
BEHAVIORAL CLONING"
BEHAVIORAL CLONING,0.18584070796460178,"BC as a representative imitation learning method seeks the best policy that can minimize the action
prediction error in demonstrations. Traditional BC method learns a policy through supervised learn-
ing and the only requirements are pairs of input sensory observations associated with expert actions.
The objective is to minimize the distribution discrepancy between the expert demonstrations and the
policy, which can be formulated as follows:"
BEHAVIORAL CLONING,0.19469026548672566,"min
π D [ρπ(·|s) || ρE(·|s)] ,
(1)"
BEHAVIORAL CLONING,0.20353982300884957,"where s represents the environment state and D is the discrepancy measure like KL divergence which
is a common objective function in imitation learning. In addition, ρπ and ρE depict the state action
distribution generated by policy π and human demonstration E, respectively."
BEHAVIORAL CLONING,0.21238938053097345,"Human demonstrations can show signiﬁcant variability because these demonstrations might be col-
lected from users with different skills and habits. Since the policy is tied up to the demonstration
in this objective function, in training the BC tends to converge into an average of various human
demonstrations or a ﬁxed behavior style when some behavior style dominates the overall demon-
strations. However, sometimes we are more concerned with how to reproduce the variation of human
behavior styles."
BEHAVIORAL CLONING,0.22123893805309736,"Therefore, inspired by style MDP, we try to improve the prediction accuracy of the model by learning
the style embedding automatically, and leverage this encode to explore the variation of behavior
styles."
PROPOSED METHOD,0.23008849557522124,"3
PROPOSED METHOD"
PROPOSED METHOD,0.23893805309734514,"To learn the style embedding that captures the semantics of human behavior, our main insight is
that the representation of styles can be reﬂected by their motions on the environment. In this paper,
we assume that different style episodes in expert demonstrations are generated by a speciﬁc style
policy πB (a|s, c). And what Style BC tries to do is to ﬁnd the right behavior style embedding c for
each episode in demonstration as well as learn a policy that can output the same action conditioning"
PROPOSED METHOD,0.24778761061946902,Under review as a conference paper at ICLR 2022
PROPOSED METHOD,0.25663716814159293,Figure 2: Overview of the framework of our proposed method.
PROPOSED METHOD,0.26548672566371684,"on the that c and the state as πB (a|s, c) does. Figure 2 illustrates the overall framework of our
method, which can be divided into two parts: learning the behavior style embedding and learning
the imitation policy based on that embedding. The ﬁrst part is implemented through the backward
propagation process in the bottom of Figure 2. And for the learning of the imitation policy based
on the embedding vector, we introduce an additional target policy to learn the representation of the
state in the demonstration, which can also make the optimization process of the imitation policy
more stable. An outline for the Style BC is shown in Algorithm 1.."
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.2743362831858407,"3.1
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING"
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.2831858407079646,"Firstly, in order to initialize the behavior style vectors C, we have to determine the number of
the behavior styles N before training. We can determine it in two ways: 1) if the number of the
episodes in the demonstrations is not large, we can set N to be the number of the episodes in the
demonstrations; 2) otherwise if the number of the episodes in the demonstration is large, we can
determine a proper value of N according to human domain knowledge, and we will introduce a
method to select a right behavior embedding for each episode in the demonstrations Section 3.3.
We take N as a hyper-parameter and this is the only domain knowledge related parameter in our
framework."
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.2920353982300885,"After selecting the style vector c for each episode, we have to optimize this vector to help the model
predicting the actions in this episodes more accurately. We leverage the widely used supervised"
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.3008849557522124,"Algorithm 1: Style BC
Input: N: the number of the predetermined styles, E: the expert demonstration set, θtarget and
ωtarget: the network weights of the shared layers and the policy layers in target
network, θstyle and ωstyle: the network weights of the shared layers and the policy
layers in style network"
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.30973451327433627,"1 Initialize θtarget, ωtarget, θstyle and ωstyle, random initialize N behavior style vectors."
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.3185840707964602,"2 Select an expert episode e from E, and determine the style vector of this episode according to
Eq. 6"
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.3274336283185841,3 Update the target network on e according to Eq. 4
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.336283185840708,"4 Update the style vector c and the policy layers ωstyle of style policy according to Eq. 2 and Eq.
3."
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.34513274336283184,5 Update the shared layers θstyle of style policy according to Eq. 5
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.35398230088495575,Under review as a conference paper at ICLR 2022
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.36283185840707965,"learning process for this purpose. As illustrated in Figure 2, the network named style policy is
responsible for this process and it can be divided into two parts: the shared layers parameterized
with θstyle and the policy layers parameterized with ωstyle . We introduce the imitation learning
loss to update c and the optimization can be formulated as:
min
c
D

π(θstyle,ωstyle)(·|s, c) || πB (·|s, c)

,
(2)"
THE LEARNING OF THE BEHAVIOR STYLE EMBEDDING,0.37168141592920356,"where πB (·|s, c) is the action distribution calculated from the episode e with behavior style vector
c."
THE LEARNING OF THE IMITATION POLICY,0.3805309734513274,"3.2
THE LEARNING OF THE IMITATION POLICY"
THE LEARNING OF THE IMITATION POLICY,0.3893805309734513,"Next, we will introduce the process of optimizing the imitation policy, which is denoted as style pol-
icy in Figure 2. As mentioned above, the two network modules of it named shared layers and policy
layers are parameterized with θstyle, and ωstyle respectively. For θstyle, the optimization process is
similar to the process in optimizing the behavior style embedding c, which can be formulated as:
min
ωstyle D

π(θstyle,ωstyle)(·|s, c) || πB (·|s, c)

,
(3)"
THE LEARNING OF THE IMITATION POLICY,0.39823008849557523,"This optimization is the same as in the common behavior clone algorithm and it is intuitive to
introduce it here as we want the style policy to mimic the expert policy. For θstyle, we introduce
a target policy to update it, the target policy is a common behavior clone network with the same
network construction as the style policy, except for that its input includes the state only and will not
contain the style embedding. The shared layers and the policy layers of it are paremeterized with
θtarget and ωtarget respectively. In training, the parameters of the shared layers in style policy are
copied from the ones in target policy. We use the target policy here because we take the outputs
of the shared layers of the target network as a high level representation of the state and copy it to
the style policy can make the optimization process of the style policy faster and more stable. The
optimization process of θstyle, θtarget and ωtarget can be formatted as:"
THE LEARNING OF THE IMITATION POLICY,0.40707964601769914,"min
(θtarget,ωtarget) D

π(θtarget,ωtarget)(·|s) || πB (·|s)

,
(4)"
THE LEARNING OF THE IMITATION POLICY,0.415929203539823,"θstyle = k ∗θstyle + (1 −k) ∗θtarget
(5)
where k is the hyper-parameter used in the soft update of θstyle, and π(θtarget,ωtarget) is the target
policy network parameterized by θtarget and ωtarget. Note that the input of the target policy network
in Eq. 4 is only the state s, which is different from the input (including both the state s and the
behavior style vector c) of the style policy network in Eq. 3"
SELECTING THE BEHAVIOR STYLE VECTOR FOR EACH EPISODE,0.4247787610619469,"3.3
SELECTING THE BEHAVIOR STYLE VECTOR FOR EACH EPISODE"
SELECTING THE BEHAVIOR STYLE VECTOR FOR EACH EPISODE,0.4336283185840708,"In this section, we will explain the method for selecting the behavior style vector when the number
of the episodes is large and thus we have to predetermined the number of the behavior styles before
training. After selecting an episode from the expert demonstrations, we will compute the prediction
accuracy of the actions in this demonstrated episode with the style policy based on each behavior
style vector c in C, and select the one with the highest prediction accuracy as the behavior style
vector of this episode. To be speciﬁc, the behavior style vector ce for episode e is determined
through Eq. 6, where Pπ(θstyle,ωstyle)(e|c) is the prediction accuracy of style policy on the data in
episode e taking the behavior style vector c as input."
SELECTING THE BEHAVIOR STYLE VECTOR FOR EACH EPISODE,0.4424778761061947,"ce = arg max
c∈C Pπ(θstyle,ωstyle)(e|c)
(6)"
EXPERIMENTS,0.45132743362831856,"4
EXPERIMENTS"
EXPERIMENTS,0.46017699115044247,"In this section, we aim at investigating (i) whether Style BC can reproduce the different behavior
styles from expert demonstrations, and (ii) whether style embedding can guide and determine the
generation of a speciﬁc style policy. To evaluate our approach, we conduct extensive experiments
on several widely used environments: a navigation task in Grid-World, one classic control environ-
ment in OpenAI Gym (Kumar, 2020), and ﬁnally, a minimalist environment for decision-making in
autonomous driving named highway-env (Leurent, 2018)."
EXPERIMENTS,0.4690265486725664,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.4778761061946903,Figure 3: The human demonstrations in the Grid-World environment.
EXPERIMENTS,0.48672566371681414,"(a)
(b)"
EXPERIMENTS,0.49557522123893805,"Figure 4: Experiment results of Style BC and BC in the Grid-world environment. (a) acc and (b)
reward."
GRID-WORLD,0.504424778761062,"4.1
GRID-WORLD"
EXPERIMENTAL SETUP,0.5132743362831859,"4.1.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.5221238938053098,"We use a 15*15 Grid-World environment as shown in Figure 3, the agent (yellow point) starts from
the bottom left corner and is required to arrive at any of the ﬁve goal grid (green squares) placed
at the right sideline (+20 reward). Besides, there is a wall (black squares) in the vertical direction
and the agent can only pass through it from the ﬁve doors. We collected trajectories passing through
different doors and reaching different ﬁve goals, as shown in Figure 3. Based on this, we conduct
a set of experiments and intend to verify the following issues: (i) Whether the accuracy of behavior
cloning can be improved after introducing style embedding? (ii) Whether the proposed Style BC
method can reproduce different style policies? (iii) Is there an exact style embedding vector that can
determine a unique style policy?"
EXPERIMENTAL SETUP,0.5309734513274337,"For the ﬁrst issue, by comparing Style BC and traditional BC method in the above environment, we
observe the differences in accuracy and reward. On this foundation, we introduce two popular imi-"
EXPERIMENTAL SETUP,0.5398230088495575,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.5486725663716814,"tation learning algorithms, Gail and info-GAIL, and count the number of behavior styles displayed
by these four algorithms. As for the last issue, several explicit style embedding is given to learn
whether different styles of paths are produced."
EXPERIMENTAL ANALYSIS,0.5575221238938053,"4.1.2
EXPERIMENTAL ANALYSIS"
EXPERIMENTAL ANALYSIS,0.5663716814159292,"First of all, from the results in Figure 4, we can ﬁnd that the accuracy of Style BC is much higher
than traditional BC, which proves that the introduction of style embedding will improve the pre-
diction accuracy of behavior cloning. Moreover, the advantage of adopting style embedding is also
reﬂected in reward. In the later stage of training, the reward of Style BC tends to be stable. On the
contrary, the traditional BC’s reward ﬂuctuates greatly because this method is sensitive to predic-
tion errors. Figure 5 shows the number of behavior styles displayed by the four algorithms. The
demonstration path is a collection of trajectories that pass through ﬁve gates and reach different
goals, as shown in Figure 3. Therefore, we consider that there are 25 motion strategies in this ex-
pert demonstration. Style BC method proposed in this paper reproduces 22 different motion paths,
which is the most among the four comparison algorithms; Gail and BC ﬁnally converge to only one
trajectory, which reﬂects that the traditional imitation learning algorithm may not be able to show
a variety of behavior styles; Info-GAIL represents four different motion trajectories, while we ﬁnd
that some different latent factors correspond to the same behavior style, resulting in only four style
paths. In particular, although our method does not completely reproduce the 25 style trajectories, the
number of styles reﬂected is much higher than the four in info-GAIL. Finally, we randomly select
three vectors from the style embedding dataset generated by automatic learning as the test style. By
interacting with the Grid-world environment respectively, three different motion trajectories can be
obtained, as shown in Figure 6. Therefore, under the determined style embedding, only one style of
behavior policy can be generated."
CARTPOLE,0.5752212389380531,"4.2
CARTPOLE"
EXPERIMENT SETUP,0.584070796460177,"4.2.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.5929203539823009,"This set of experiments are based on the classic control tasks named CartPole-v1 in OpenAI Gym. In
this research, we assume that keeping the pendulum upright and ﬁnally staying at different speciﬁc
positions (0, ±0.5, ±1.5 distance to center in CartPole) represents various behavior styles, shown in
Figure 7. The expert demonstrations collect the trajectories where the end point stays at these ﬁve
points. In the following experiments, Style BC is trained with the demonstrations and is hoped to
reproduce different behavior styles. Similarly, we also introduce BC, GAIL and info-GAIL as the
comparison algorithms."
EXPERIMENTAL ANALYSIS,0.6017699115044248,"4.2.2
EXPERIMENTAL ANALYSIS"
EXPERIMENTAL ANALYSIS,0.6106194690265486,"The results of CartPole-v1 are demonstrated in Table 1. It’s obvious that Style BC and info-GAIL
are much higher in style diversity than the other two models. BC learns the behavior style with the
end point near -1.5 (-1.46), while algorithm GAIL generates a path with the end stable point near
0 (-0.02). Both behavior styles are from expert demonstrations. Furthermore, we can see that the"
EXPERIMENTAL ANALYSIS,0.6194690265486725,"Figure 5: The number of behavior styles.
Figure 6: Trajectories of the executed policies."
EXPERIMENTAL ANALYSIS,0.6283185840707964,Under review as a conference paper at ICLR 2022
EXPERIMENTAL ANALYSIS,0.6371681415929203,Figure 7: The Cartpole environment.
EXPERIMENTAL ANALYSIS,0.6460176991150443,"trajectory end point shown by Style BC is stable at 5 different positons, of which four positions
stay in our expected position range. This shows that the Style BC model can replicate the four style
representations shown by expert demonstrations. In particular, the end point of one of the tracks
generated by Style BC is stable at 0.25. And we believe that the stable point does not fall in the
desirable position (0.5) due to the deviation of style embedding. As for Info-GAIL model, although
the end of the trajectories it produces also falls in ﬁve different positions, these end stable points
are inconsistent with the expected position. This does not meet the requirements of reproducing
different behavior styles."
HIGHWAY-ENV,0.6548672566371682,"4.3
HIGHWAY-ENV"
EXPERIMENT SETUP,0.6637168141592921,"4.3.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.672566371681416,"To demonstrate the effectiveness of Style BC in complex driving environment, the last experi-
ment is carried out on Highway-env, which is a minimalist environment for decision-making in
autonomous driving. More details about the environment can be seen at https://github.
com/eleurent/highway-env."
EXPERIMENT SETUP,0.6814159292035398,"In this experiment, we focus on the style representation of different drivers in the car-following
scenario, in which there is no lane-changing behavior. The car-following scenario in the highway-
env environment is shown in Figure 1(b). In this environment, the initial position and velocity of
surrounding vehicles are random. Moreover, the acceleration of the surrounding agents is given by
the Intelligent Driver Model (IDM) (Treiber et al., 2000), and the discrete lane change decisions
are given by the Minimizing Overall Braking Induced by Lane change (MOBIL) model (Kesting
et al., 2007). Some literatures point out that the traditional car following style can be divided into
aggressive and conservative. While in this paper, we also introduce a car-following style, that is,
the subject vehicle always keeps the minimum driving speed allowed on the highway (20 m/s) for
car-following maneuver. It is assumed that the initial lane of the target vehicle is the middle lane and
the initial speed is 25m/s. We collect some demonstration data by a policy trained with PPO and
reward shaping that can reﬂect the above different driving styles. Furthermore, the average velocity,
the average distance between the ego-vehicle and the leading vehicle and the total driving distance
can be used to identify the different driving styles. Table 2 presents the descriptive statistics of
car-following behaviors from the expert demonstrations."
EXPERIMENT SETUP,0.6902654867256637,Table 1: Experiment results on Cartpole
EXPERIMENT SETUP,0.6991150442477876,"ALGORITHM
STYLE NUM
FINAL POS"
EXPERIMENT SETUP,0.7079646017699115,"BC
1
[-1.46]
Style BC
5
[0.48; 1.46; -1.46; 0.25; -0.08]
GAIL
1
[-0.02]
info-GAIL
5
[-2.41; -0.74; -1.38; -0.11; -0.73]"
EXPERIMENT SETUP,0.7168141592920354,Under review as a conference paper at ICLR 2022
EXPERIMENT SETUP,0.7256637168141593,Table 2: Descriptive statistics for different driving styles in car-following manuevers
EXPERIMENT SETUP,0.7345132743362832,"DRIVING STYLE
AVERAGE VELOCITY
MEAN GAP
TOTAL DISTANCE"
EXPERIMENT SETUP,0.7433628318584071,"Aggressive
26.29m/s
35.05m
527.48m
Conservative
25.25m/s
65.83m
505.47m
Keep speed
20.05m/s
106.58m
403.01m"
EXPERIMENT SETUP,0.7522123893805309,"(a)
(b)
(c)"
EXPERIMENT SETUP,0.7610619469026548,"Figure 8: Experiment results on car-following scenario. (a) average speed and (b) mean gap (c) total
distance."
EXPERIMENTAL ANALYSIS,0.7699115044247787,"4.3.2
EXPERIMENTAL ANALYSIS"
EXPERIMENTAL ANALYSIS,0.7787610619469026,"We use the Style BC method to learn three style embedding, and each discrete index is used to
interact with the highway environment to generate a set of speciﬁc style trajectory. Results are
provided in Figure 8, where the horizontal axis represents the number of time steps and the ver-
tical axis represents the average speed, mean gap and total distance, respectively. The solid blue
line represents aggressive driving style, the dotted red line means conservative style and the green
point line expresses maintaining the lowest velocity. In Figure 8(a), the velocity of aggressive
driving style ﬂuctuates more violently than the other two models. Accordingly, the distance be-
tween the following and the leading vehicle is relatively small in aggressive driving style. The
following distance of the conservative driving model is stable at 60m, while the driving total dis-
tance of the lowest speed driving style is also the shortest. Furthermore, from Figure 8, we can
also ﬁnd that the three driving styles reproduced are also clearly distinguished in various indica-
tors, which shows that the style embedding can guide the generation of different driving strate-
gies. We recorded the driving process of Style BC with a video which can be found at https:
//www.bilibili.com/video/BV1Sf4y1F72P?spm_id_from=444.41.0.0. During
the interaction between the model and the highway environment, these car-following strategies for
different driving style can safely complete the whole journey. While the traditional BC, GAIL and
info-GAIL methods will collide with surrounding vehicles in the test stage, so they are not included
in the comparative experiment."
CONCLUSION,0.7876106194690266,"5
CONCLUSION"
CONCLUSION,0.7964601769911505,"In this paper, we present a method to infer the latent representation of behavior styles automatically
while imitating different style policies from expert demonstrations. The novel Style BC method
introduces style embedding and proves that it can improve the accuracy of behavior cloning. Our
experimental results in some popular benchmark environments show that our methods can reproduce
more behavior styles, while an exact style embedding vector can determine a unique style policy. In
future work, we seek to explain the performance of different style embedding and ﬁnd the poten-
tial relationship between these style indexes. Besides, extending our method with popular inverse
reinforcement learning is also worth researching in the future."
CONCLUSION,0.8053097345132744,Under review as a conference paper at ICLR 2022
REFERENCES,0.8141592920353983,REFERENCES
REFERENCES,0.8230088495575221,"Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning
for self-driving cars. arXiv preprint arXiv:1604.07316, 2016."
REFERENCES,0.831858407079646,"Clemens Eppner, Jurgen Sturm, Maren Bennewitz, Cyrill Stachniss, and Wolfram Burgard. Imitation
learning with generalized task descriptions. In 2009 IEEE International Conference on Robotics
and Automation, pp. 3968–3974. IEEE, 2009."
REFERENCES,0.8407079646017699,"David Sierra Gonz´alez, Jilles Steeve Dibangoye, and Christian Laugier. High-speed highway scene
prediction based on driver models learned from demonstrations. In 2016 IEEE 19th International
Conference on Intelligent Transportation Systems (ITSC), pp. 149–155, 2016. doi: 10.1109/ITSC.
2016.7795546."
REFERENCES,0.8495575221238938,"Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural
information processing systems, 29:4565–4573, 2016."
REFERENCES,0.8584070796460177,"Shuhei Ikemoto, Heni Ben Amor, Takashi Minato, Bernhard Jung, and Hiroshi Ishiguro. Physical
human-robot interaction: Mutual learning and adaptation. IEEE robotics & automation magazine,
19(4):24–35, 2012."
REFERENCES,0.8672566371681416,"Arne Kesting, Martin Treiber, and Dirk Helbing.
General lane-changing model mobil for car-
following models. Transportation Research Record, 1999(1):86–94, 2007."
REFERENCES,0.8761061946902655,"Alex Kueﬂer and Mykel J Kochenderfer. Burn-in demonstrations for multi-modal imitation learning.
arXiv preprint arXiv:1710.05090, 2017."
REFERENCES,0.8849557522123894,"Swagat Kumar. Balancing a cartpole system with reinforcement learning–a tutorial. arXiv preprint
arXiv:2006.04938, 2020."
REFERENCES,0.8938053097345132,"Edouard Leurent. An environment for autonomous driving decision-making. https://github.
com/eleurent/highway-env, 2018."
REFERENCES,0.9026548672566371,"Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from vi-
sual demonstrations. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, pp. 3815–3825, 2017."
REFERENCES,0.911504424778761,"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word represen-
tations in vector space, 2013."
REFERENCES,0.9203539823008849,"Jorge Munoz, German Gutierrez, and Araceli Sanchis. Controller for torcs created by imitation.
In 2009 IEEE Symposium on Computational Intelligence and Games, pp. 271–278, 2009. doi:
10.1109/CIG.2009.5286464."
REFERENCES,0.9292035398230089,"Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, and Jan Peters.
An algorithmic perspective on imitation learning. arXiv preprint arXiv:1811.06711, 2018."
REFERENCES,0.9380530973451328,"Cristian Alejandro Vergara Perico, Joris De Schutter, and Erwin Aertbeli¨en. Combining imitation
learning with constraint-based task speciﬁcation and control. IEEE Robotics and Automation
Letters, 4(2):1892–1899, 2019."
REFERENCES,0.9469026548672567,"St´ephane Ross and Drew Bagnell. Efﬁcient reductions for imitation learning. In Proceedings of the
thirteenth international conference on artiﬁcial intelligence and statistics, pp. 661–668. JMLR
Workshop and Conference Proceedings, 2010."
REFERENCES,0.9557522123893806,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.9646017699115044,"Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested trafﬁc states in empirical observa-
tions and microscopic simulations. Physical review E, 62(2):1805, 2000."
REFERENCES,0.9734513274336283,"Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 30, 2016."
REFERENCES,0.9823008849557522,Under review as a conference paper at ICLR 2022
REFERENCES,0.9911504424778761,"Fangyi Zhang, J¨urgen Leitner, Michael Milford, Ben Upcroft, and Peter Corke. Towards vision-
based deep reinforcement learning for robotic motion control. arXiv preprint arXiv:1511.03791,
2015."
