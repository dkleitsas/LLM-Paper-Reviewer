Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0024154589371980675,"In this study, we introduce a novel, probabilistic viewpoint on adversarial examples,
1"
ABSTRACT,0.004830917874396135,"achieved through box-constrained Langevin Monte Carlo (LMC). Proceeding from
2"
ABSTRACT,0.007246376811594203,"this perspective, we develop an innovative approach for generating semantics-aware
3"
ABSTRACT,0.00966183574879227,"adversarial examples in a principled manner. This methodology transcends the
4"
ABSTRACT,0.012077294685990338,"restriction imposed by geometric distance, instead opting for semantic constraints.
5"
ABSTRACT,0.014492753623188406,"Our approach empowers individuals to incorporate their personal comprehension
6"
ABSTRACT,0.016908212560386472,"of semantics into the model. Through human evaluation, we validate that our
7"
ABSTRACT,0.01932367149758454,"semantics-aware adversarial examples maintain their inherent meaning. Experi-
8"
ABSTRACT,0.021739130434782608,"mental findings on the MNIST and SVHN datasets demonstrate that our semantics-
9"
ABSTRACT,0.024154589371980676,"aware adversarial examples can effectively circumvent robust adversarial training
10"
ABSTRACT,0.026570048309178744,"methods tailored for traditional adversarial attacks.
11"
INTRODUCTION,0.028985507246376812,"1
Introduction
12"
INTRODUCTION,0.03140096618357488,"The purpose of generating adversarial examples is to deceive a classifier by making minimal changes
13"
INTRODUCTION,0.033816425120772944,"to the original data’s meaning. In image classification, most existing adversarial techniques ensure the
14"
INTRODUCTION,0.036231884057971016,"preservation of adversarial example semantics by limiting their geometric distance from the original
15"
INTRODUCTION,0.03864734299516908,"image [18, 6, 2, 12]. These methods are able to deceive classifiers with a very small geometric based
16"
INTRODUCTION,0.04106280193236715,"perturbation. However, when targeting robust classifiers trained using adversarial methods, an attack
17"
INTRODUCTION,0.043478260869565216,"involving a relatively large geometric distance may be necessary. Unfortunately, these considerable
18"
INTRODUCTION,0.04589371980676329,"distances can be so vast that they ultimately undermine the original image’s semantics, going against
19"
INTRODUCTION,0.04830917874396135,"the core objective of creating adversarial examples. As illustrated in the left portion of Figure 1, when
20"
INTRODUCTION,0.050724637681159424,"applying the PGD attack [12] constrained by L2 norm on a robust classifier, the attacked images that
21"
INTRODUCTION,0.05314009661835749,"successfully deceive the classifier consistently lose their original meaning, which is undesirable.
22"
INTRODUCTION,0.05555555555555555,"To counter this problem, we propose an innovative approach for generating semantics-aware adver-
23"
INTRODUCTION,0.057971014492753624,"sarial examples. Instead of being limited by geometric distance, our approach hinges on a proposed
24"
INTRODUCTION,0.06038647342995169,"semantic divergence. Specifically, we treat generating adversarial examples as a box-constrained
25"
INTRODUCTION,0.06280193236714976,"non-convex optimization problem. We employ box-constrained Langevin Monte Carlo (LMC) to
26"
INTRODUCTION,0.06521739130434782,"find near-optimal solutions for this complex problem. As LMC samples converge to a stationary
27"
INTRODUCTION,0.06763285024154589,"distribution, we gain a probabilistic understanding of the adversarial attack. Within this probabilistic
28"
INTRODUCTION,0.07004830917874397,"perspective, the geometric constraint of the adversarial attack can be viewed as a distribution. By
29"
INTRODUCTION,0.07246376811594203,"replacing this geometric-based distribution with a semantic-based distribution, we can define a
30"
INTRODUCTION,0.0748792270531401,"semantics-aware adversarial attack in a principled manner. The corresponding divergence induced by
31"
INTRODUCTION,0.07729468599033816,"the semantic-based distribution is called semantic divergence. Our semantics-aware adversarial attack
32"
INTRODUCTION,0.07971014492753623,"is capable of deceiving robust classifiers while preserving most of the original image’s semantics, as
33"
INTRODUCTION,0.0821256038647343,"demonstrated in the right section of Figure 1.
34"
INTRODUCTION,0.08454106280193237,Source
INTRODUCTION,0.08695652173913043,"PGD with L2 norm
Our method
Target
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9"
INTRODUCTION,0.0893719806763285,"Target
0
1
2
3
4
5
6
7
8
9"
INTRODUCTION,0.09178743961352658,Source
INTRODUCTION,0.09420289855072464,"0
1
2
3
4
5
6
7
8
9"
INTRODUCTION,0.0966183574879227,"Figure 1: Top left: Targeted attack on an adversarially trained MadryNet [12] for MNIST using
Projected Gradient Descent (PGD) with L2 norm. To ensure successful targeted attacks in most
cases, we increased the ϵ to 5. Bottom left: Targeted attack on an adversarially trained ResNet18
[8] for SVHN using PGD with L2 norm and ϵ = 5. Top right & Bottom right: Our proposed
method applied to targeted attacks on the same MadryNet and ResNet18 for MNIST and SVHN,
respectively. A green border signifies a successful deception of the victim classifier, while a red
border indicates failure. Notably, with PGD, a successful attack often results in the alteration of
the source image’s semantics, which is undesirable. Additional PGD attack examples are provided
in Appendix E."
PRELIMINARIES,0.09903381642512077,"2
Preliminaries
35"
ADVERSARIAL EXAMPLES,0.10144927536231885,"2.1
Adversarial examples
36"
ADVERSARIAL EXAMPLES,0.10386473429951691,"The notion of adversarial examples was first introduced by Szegedy et al. [18]. Let’s assume we have
37"
ADVERSARIAL EXAMPLES,0.10628019323671498,"a classifier C : [0, 1]n →Y, where n represents the dimension of the input space and Y denotes the
38"
ADVERSARIAL EXAMPLES,0.10869565217391304,"label space. Given an image xori ∈[0, 1]n and a target label ytar ∈Y, the optimization problem for
39"
ADVERSARIAL EXAMPLES,0.1111111111111111,"finding an adversarial instance for xori can be formulated as follows:
40"
ADVERSARIAL EXAMPLES,0.11352657004830918,"minimize D(xori, xadv)
such that C(xadv) = ytar and xadv ∈[0, 1]n"
ADVERSARIAL EXAMPLES,0.11594202898550725,"Here, D is a distance metric employed to assess the difference between the original and perturbed
41"
ADVERSARIAL EXAMPLES,0.11835748792270531,"images. This distance metric typically relies on geometric distance, which can be represented by L0,
42"
ADVERSARIAL EXAMPLES,0.12077294685990338,"L2, or L∞norms.
43"
ADVERSARIAL EXAMPLES,0.12318840579710146,"However, solving this problem is challenging. As a result, Szegedy et al. [18] propose a relaxation of
44"
ADVERSARIAL EXAMPLES,0.12560386473429952,"the problem:
45"
ADVERSARIAL EXAMPLES,0.1280193236714976,"minimize L(xadv, ytar) := c1 · D(xori, xadv) + c2 · f(xadv, ytar)
such that xadv ∈[0, 1]n
(1)"
ADVERSARIAL EXAMPLES,0.13043478260869565,"where c1, c2 are constants, and f is an objective function closely tied to the classifier’s prediction. For
46"
ADVERSARIAL EXAMPLES,0.13285024154589373,"example, in [18], f is the cross-entropy loss function, while Carlini and Wagner [2] suggest several
47"
ADVERSARIAL EXAMPLES,0.13526570048309178,"different choices for f. Szegedy et al. [18] recommend solving (1) using box-constrained L-BFGS.
48"
ADVERSARIAL TRAINING,0.13768115942028986,"2.2
Adversarial training
49"
ADVERSARIAL TRAINING,0.14009661835748793,"Adversarial training, a widely acknowledged method for boosting adversarial robustness in deep
50"
ADVERSARIAL TRAINING,0.14251207729468598,"learning models, has been extensively studied [18, 6, 10, 12]. This technique uses adversarial samples
51"
ADVERSARIAL TRAINING,0.14492753623188406,"as (part of) the training data, originating from Szegedy et al. [18], and has evolved into numerous
52"
ADVERSARIAL TRAINING,0.1473429951690821,"variations. In this paper, we apply the min-max problem formulation by Madry et al. [12] to determine
53"
ADVERSARIAL TRAINING,0.1497584541062802,"neural network weights, denoted as θ. They propose choosing θ to solve:
54"
ADVERSARIAL TRAINING,0.15217391304347827,"min
θ
E(x,y)∼pdata"
ADVERSARIAL TRAINING,0.15458937198067632,"
max
∥δ∥p≤ϵ LCE(θ, x + δ, y)

(2)"
ADVERSARIAL TRAINING,0.1570048309178744,"where pdata represents the data distribution, LCE is the cross-entropy loss, ∥·∥p denotes the Lp norm,
55"
ADVERSARIAL TRAINING,0.15942028985507245,"and ϵ specifies the radius of the corresponding Lp ball. In what follows, we will use the term “robust
56"
ADVERSARIAL TRAINING,0.16183574879227053,"classifier” to refer to classifiers that have undergone adversarial training.
57"
ADVERSARIAL TRAINING,0.1642512077294686,"2.3
Energy-based models (EBMs)
58"
ADVERSARIAL TRAINING,0.16666666666666666,"An Energy-based Model (EBM) [9, 4] involves a non-linear regression function, represented by
59"
ADVERSARIAL TRAINING,0.16908212560386474,"Eθ, with a parameter θ. This function is known as the energy function. Given a data point, x, the
60"
ADVERSARIAL TRAINING,0.17149758454106281,"probability density function (PDF) is given by:
61"
ADVERSARIAL TRAINING,0.17391304347826086,pθ(x) = exp(−Eθ(x))
ADVERSARIAL TRAINING,0.17632850241545894,"Zθ
(3)"
ADVERSARIAL TRAINING,0.178743961352657,"where Zθ =
R
exp(−Eθ(x))dx is the normalizing constant that ensures the PDF integrates to 1.
62"
ADVERSARIAL TRAINING,0.18115942028985507,"2.4
Langevin Monte Carlo (LMC)
63"
ADVERSARIAL TRAINING,0.18357487922705315,"Langevin Monte Carlo (also known as Langevin dynamics) is an iterative method that could be used
64"
ADVERSARIAL TRAINING,0.1859903381642512,"to find near-minimal points of a non-convex function g [13, 25, 20, 14]. It involves updating the
65"
ADVERSARIAL TRAINING,0.18840579710144928,"function as follows:
66"
ADVERSARIAL TRAINING,0.19082125603864733,"x0 ∼p0,
xt+1 = xt −ϵ2"
ADVERSARIAL TRAINING,0.1932367149758454,"2 ∇xg(xt) + ϵzt,
zt ∼N(0, I)
(4)"
ADVERSARIAL TRAINING,0.1956521739130435,"where p0 could be a uniform distribution. Under certain conditions on the drift coefficient ∇xg, it
67"
ADVERSARIAL TRAINING,0.19806763285024154,"has been demonstrated that the distribution of xt in (4) converges to its stationary distribution [3, 14],
68"
ADVERSARIAL TRAINING,0.20048309178743962,"also referred to as the Gibbs distribution p(x) ∝exp(g(x)). This distribution concentrates around
69"
ADVERSARIAL TRAINING,0.2028985507246377,"the global minimum of g[5, 24, 14]. If we choose g to be −Eθ, then the stationary distribution
70"
ADVERSARIAL TRAINING,0.20531400966183574,"corresponds exactly to the EBM’s distribution defined in (3). As a result, we can draw samples from
71"
ADVERSARIAL TRAINING,0.20772946859903382,"the EBM using LMC. By replacing the exact gradient with a stochastic gradient, we obtain Stochastic
72"
ADVERSARIAL TRAINING,0.21014492753623187,"Gradient Langevin Dynamics (SGLD) [23, 19].
73"
TRAINING EBM,0.21256038647342995,"2.5
Training EBM
74"
TRAINING EBM,0.21497584541062803,"To train an EBM, we aim to minimize the minus expected log-likelihood of the data , represented by
75"
TRAINING EBM,0.21739130434782608,LEBM = EX∼pd[−log pθ(X)] = EX∼pd[Eθ(X)] −log Zθ
TRAINING EBM,0.21980676328502416,"where pd is the data distribution. The gradient is
76"
TRAINING EBM,0.2222222222222222,"∇θLEBM = EX∼pd[∇θEθ(X)] −∇θ log Zθ = EX∼pd[∇θEθ(X)] −EX∼pθ[∇θEθ(X)]
(5)"
TRAINING EBM,0.2246376811594203,"(see [16] for derivation). The first term of ∇θLEBM can be easily calculated as pd is the distribution
77"
TRAINING EBM,0.22705314009661837,"of the training set. For the second term, we can use LMC to sample from pθ [9].
78"
TRAINING EBM,0.22946859903381642,"Effective training of an energy-based model (EBM) typically requires the use of techniques such as
79"
TRAINING EBM,0.2318840579710145,"sample buffering and regularization. For more information, refer to the work of Du and Mordatch [4].
80"
TRAINING EBM,0.23429951690821257,"(a)
(b)
(c)
(d)"
TRAINING EBM,0.23671497584541062,"Figure 2: (a) and (b) display samples drawn from pvic(·; ytar) with the victim classifier being non-
adversarially trained and adversarially trained, respectively. (c) showcases samples from pdis(·; xori)
when D is the square of L2 norm. (d) illustrates t(xori) for t ∼T , where T represents a distribution
of transformations, including TPS (see Section 4.2), scaling, rotation, and cropping. The xoris in (c)
and (d) consist of the first 36 images from the MNIST test set."
GENERATING SEMANTICS-AWARE ADVERSARIAL EXAMPLES,0.2391304347826087,"3
Generating semantics-aware adversarial examples
81"
GENERATING SEMANTICS-AWARE ADVERSARIAL EXAMPLES,0.24154589371980675,"In this section, we introduce a probabilistic approach to understanding adversarial examples. Through
82"
GENERATING SEMANTICS-AWARE ADVERSARIAL EXAMPLES,0.24396135265700483,"this lens, we establish the concept of semantic divergence, offering an alternative to conventional
83"
GENERATING SEMANTICS-AWARE ADVERSARIAL EXAMPLES,0.2463768115942029,"geometric distance. This concept of semantic divergence enables individuals to integrate their unique
84"
GENERATING SEMANTICS-AWARE ADVERSARIAL EXAMPLES,0.24879227053140096,"understanding of semantics into the model, thereby facilitating the creation of semantics-aware
85"
GENERATING SEMANTICS-AWARE ADVERSARIAL EXAMPLES,0.25120772946859904,"adversarial examples.
86"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2536231884057971,"3.1
A probabilistic perspective on adversarial examples
87"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2560386473429952,"LMC and SGLD are not directly applicable to the optimization problem presented in (1) due to their
88"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2584541062801932,"incompatibility with box-constrained optimization problems. To overcome this limitation, Lamperski
89"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2608695652173913,"[11] proposed Projected Stochastic Gradient Langevin Algorithms (PSGLA). By employing PSGLA
90"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2632850241545894,"to generate samples near the solution of the optimization problem specified in (1), we obtain the
91"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.26570048309178745,"subsequent update rule:
92"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.26811594202898553,"x0 ∼p0,
xt+1 = Π[0,1]n

xt −ϵ2"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.27053140096618356,"2 ∇xL(xt, ytar) + ϵzt"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.27294685990338163,"
,
zt ∼N(0, I)
(6)"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2753623188405797,"where Π[0, 1]n is a clamp projection that enforces the constraints within the [0, 1]n interval.
93"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2777777777777778,"We refer to the stationary distribution of PSGLA as the adversarial distribution padv(x; ytar) ∝
94"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.28019323671497587,"exp(−L(x, ytar)), since samples drawn from this distribution are in close proximity to the optimal
95"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2826086956521739,"value of the optimization problem presented in (1).
96"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.28502415458937197,"Then by definition of L, the adversarial distribution can be represented as a product of expert
97"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.28743961352657005,"distributions [9]:
98"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2898550724637681,"padv(xadv; xori, ytar) ∝pvic(xadv; ytar)pdis(xadv; xori)
(7)
where pvic(xadv; ytar) ∝exp(−c2 · f(xadv, ytar)) denote the victim distribution and pdis(xadv; xori) ∝
99"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2922705314009662,"exp(−c1 · D(xori, xadv)) represent the distance distribution.
100"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2946859903381642,"The victim distribution pvic is dependent on the victim classifier. As suggested by Szegedy et al.
101"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2971014492753623,"[18], f could be the cross-entropy loss of the classifier. We can sample from this distribution using
102"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.2995169082125604,"Langevin dynamics. Figure 2(a) presents samples drawn from pvic when the victim classifier is
103"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.30193236714975846,"subjected to standard training, exhibiting somewhat indistinct shapes of the digits. This implies
104"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.30434782608695654,"that the classifier has learned the semantics of the digits to a certain degree, but not thoroughly.
105"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.30676328502415456,"In contrast, Figure 2(b) displays samples drawn from pvic when the victim classifier undergoes
106"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.30917874396135264,"adversarial training. In this scenario, the shapes of the digits are clearly discernible. This observation
107"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3115942028985507,"suggests that we can obtain meaningful samples from adversarially trained classifiers, indicating that
108"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3140096618357488,"such classifiers depend more on semantics, which corresponds to the fact that an adversarially trained
109"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3164251207729469,"classifier is more difficult to attack. A similar observation concerning the generation of images from
110"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3188405797101449,"an adversarially trained classifier has been reported by Santurkar et al. [15].
111"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.321256038647343,"The distance distribution pdis relies on D(xori, xadv), representing the distance between xadv and
112"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.32367149758454106,"xori. By its nature, samples that are closer to xori may yield a higher padv, which is consistent with
113"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.32608695652173914,"(a)
(b)"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3285024154589372,"78.3
79.6
95.4
84.1
72.9
96.0"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3309178743961353,"78.0
80.8
83.8
83.8
79.0
81.1"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3333333333333333,"76.1
79.4
97.0
77.2
73.8
81.4"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3357487922705314,"77.7
74.5
82.4
79.3
82.2
79.7"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.33816425120772947,"75.6
82.4
78.8
80.6
81.1
95.5"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.34057971014492755,"85.0
92.2
83.6
71.2
79.7
80.9"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.34299516908212563,"(c)
(d)"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.34541062801932365,"38.9
27.9
94.6
17.1
23.2
95.7"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.34782608695652173,"28.6
24.0
20.9
27.0
17.4
34.0"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3502415458937198,"21.9
16.4
96.5
20.7
22.5
36.2"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3526570048309179,"27.9
25.3
31.1
21.1
40.4
30.4"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.35507246376811596,"26.4
16.2
33.6
27.5
18.7
94.8"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.357487922705314,"40.9
90.6
38.7
17.0
43.6
38.9"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.35990338164251207,"Figure 3: (a): Samples from padv(·; xori, ytar) ∝exp(−c1 · D(xori, xadv)) exp(−c2 · f(xadv, ytar)),
where D is the L2 norm, f is the cross-entropy fCE, xori are the first 36 images from the MNIST test
set, ytar are set to 1, c1 is 10−3, and c2 is 10−2. (c): Similar to (a), but with f replaced by fCW, as
introduced in section 4.1. Essentially, this case applies the L2 CW attack [2] using LMC instead of
Adam optimization. A green border indicates successful deception of the victim classifier, while a
red border signifies failure. (b) & (d): the predictive probability (softmax probability) of the target
class, corresponding to each digit of Figures (a) and (c) on a one-to-one basis."
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.36231884057971014,"the objective of generating adversarial samples. Moreover, if D represents the square of the L2
114"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3647342995169082,"norm, then pdis becomes a Gaussian distribution with a mean of xori and a variance determined by
115"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3671497584541063,"c1. Figure 2(c) portrays samples drawn from pdis when D is the square of the L2 distance. The
116"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3695652173913043,"samples closely resemble the original images, xoris, from the MNIST testset, because each sample is
117"
A PROBABILISTIC PERSPECTIVE ON ADVERSARIAL EXAMPLES,0.3719806763285024,"positioned near an optimal point, and these optimal points are the original images, xoris.
118"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.3743961352657005,"3.2
From Geometric Distance to Semantic Divergence
119"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.37681159420289856,"Based on the probabilistic perspective, we propose a semantic divergence, denoted by a non-symmetric
120"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.37922705314009664,"divergence Dsem(xadv, xori) := E(xadv; xori), where E(·; xori) represents the energy of an energy-
121"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.38164251207729466,"based model trained on a dataset consisting of {t1(xori), t2(xori), . . . }. Here, ti ∼T , and T is
122"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.38405797101449274,"a distribution of transformations that do not alter the original image’s semantics. In practice, the
123"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.3864734299516908,"choice of T depends on human subjectivity related to the dataset. Individuals are able to incorporate
124"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.3888888888888889,"their personal comprehension of semantics into the model by designing their own T . For instance,
125"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.391304347826087,"in the case of the MNIST dataset, the transformations could include scaling, rotation, distortion,
126"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.39371980676328505,"and cropping, as illustrated in Figure 2(d). We assume that such transformations do not affect the
127"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.3961352657004831,"semantics of the digits in the MNIST dataset. Consequently, our proposed semantic divergence
128"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.39855072463768115,"induces the corresponding distance distribution pdis(xadv; xori) ∝exp(−c1 · E(xadv; xori)).
129"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.40096618357487923,"We claim that, given an appropriate T , semantic divergence can surpass geometric distance. Empiri-
130"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4033816425120773,"cally, maintaining the semantics of the original image by limiting the geometric distance between the
131"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4057971014492754,"adversarial image and the original image when deceiving a robust classifier is challenging: as shown
132"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4082125603864734,"in Figure 1 and Figure 3, it is difficult to preserve the semantics of the original images. The attacked
133"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4106280193236715,"images either display a ‘shadow’ of the target digits or reveal conspicuous tampering traces, such as
134"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.41304347826086957,"in Figure 3(c), where the attacked digit turns gray. This phenomenon was empirically observed and
135"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.41545893719806765,"tested by Song et al. [17] through an A/B test. Conversely, as depicted in Figure 4, the samples from
136"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4178743961352657,"padv neither exhibit the ‘shadow’ of the target digits nor any obvious traces indicating adversarial
137"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.42028985507246375,"attack. While semantic divergence can’t entirely prevent the generation of a sample resembling
138"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4227053140096618,"the target class, as shown in Figure 4(a), we discuss certain techniques to mitigate this issue in
139"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4251207729468599,"Section 4.1.
140"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.427536231884058,"A plausible explanation for this is that the utilization of geometric distance causes pdis(·, xori) to
141"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.42995169082125606,"overly focus on xori. However, when applying semantic divergence induced by a suitable T , the
142"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4323671497584541,"density of the distance distribution pdis(·, xori) spreads out relatively more, resulting in a higher
143"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.43478260869565216,"overlap between pdis(·, xori) and pvic. This, in turn, provides more opportunities for their product padv
144"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.43719806763285024,"to reach a higher value.
145"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4396135265700483,"(a)
(b)"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4420289855072464,"75.0
80.5
78.7
78.6
79.2
80.5"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4444444444444444,"77.9
81.3
78.2
75.5
78.4
80.3"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4468599033816425,"77.7
77.5
79.3
73.7
78.1
73.1"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4492753623188406,"82.1
69.6
86.0
63.5
76.4
80.7"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.45169082125603865,"77.4
79.9
74.3
83.9
74.6
79.6"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.45410628019323673,"78.7
79.2
81.5
84.2
77.0
77.6"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.45652173913043476,"(c)
(d)"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.45893719806763283,"22.7
36.9
23.9
38.5
38.6
35.6"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.4613526570048309,"35.3
40.5
35.8
24.5
38.6
21.5"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.463768115942029,"26.3
21.9
32.9
43.0
19.9
14.6"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.46618357487922707,"30.1
23.6
23.8
16.6
40.3
42.6"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.46859903381642515,"22.9
22.9
40.5
24.3
24.1
34.0"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.47101449275362317,"35.3
29.1
21.8
35.8
22.2
38.8"
FROM GEOMETRIC DISTANCE TO SEMANTIC DIVERGENCE,0.47342995169082125,"Figure 4: (a) & (c): Samples from padv(·; xori, ytar) ∝exp(−c1·D(xori, xadv)) exp(−c2·f(xadv, ytar)),
where xori refers to the original image of digit “7” shown in Figure 1 and ytar refers to class 9. D
represents our proposed semantic divergence. In (a), f is the cross-entropy fCE, while in (c), f is fCW.
Constants are set as c1 = 1.0 and c2 = 10−2. A green border indicates successful deception of the
victim classifier, whereas a red border denotes failure. (b) & (d): The predictive probability (softmax
probability) of the target class, corresponding to each digit in Figures (a) and (c) on a one-to-one
basis."
DECEIVING ROBUST CLASSIFIERS,0.4758454106280193,"4
Deceiving robust classifiers
146"
DECEIVING ROBUST CLASSIFIERS,0.4782608695652174,"In this section, we present several techniques that enhance the performance of our proposed method
147"
DECEIVING ROBUST CLASSIFIERS,0.4806763285024155,"in generating high-quality adversarial examples.
148"
VICTIM DISTRIBUTIONS,0.4830917874396135,"4.1
Victim distributions
149"
VICTIM DISTRIBUTIONS,0.4855072463768116,"The victim distribution pvic ∝exp(c2 · f(xadv, ytar)) is influenced by the choice of function f. Let
150"
VICTIM DISTRIBUTIONS,0.48792270531400966,"gϕ : [0, 1]n →R|Y| be a classifier that produces logits as output with ϕ representing the neural
151"
VICTIM DISTRIBUTIONS,0.49033816425120774,"network parameters, n denoting the dimensions of the input, and Y being the set of labels (the output
152"
VICTIM DISTRIBUTIONS,0.4927536231884058,"of gϕ are logits). Szegedy et al. [18] suggested using cross-entropy as the function f, which can be
153"
VICTIM DISTRIBUTIONS,0.49516908212560384,"expressed as
154"
VICTIM DISTRIBUTIONS,0.4975845410628019,"fCE(x, ytar) := −gϕ(x)[ytar] + log
X"
VICTIM DISTRIBUTIONS,0.5,"y
exp(gϕ(x)[y]) = −log σ(gϕ(x))[ytar]"
VICTIM DISTRIBUTIONS,0.5024154589371981,"where σ denotes the softmax function.
155"
VICTIM DISTRIBUTIONS,0.5048309178743962,"Carlini and Wagner [2] explored and compared multiple options for f. They found that, empirically,
156"
VICTIM DISTRIBUTIONS,0.5072463768115942,"the most efficient choice of their proposed fs is:
157"
VICTIM DISTRIBUTIONS,0.5096618357487923,"fCW(x, ytar) := max(max
y̸=ytar gϕ(x)[y] −gϕ(x)[ytar], 0)."
VICTIM DISTRIBUTIONS,0.5120772946859904,"From Figure 3 and Figure 4, we observe that fCW outperforms fCE when the pdis depends on either
158"
VICTIM DISTRIBUTIONS,0.5144927536231884,"geometric distance or semantic divergence. A potential explanation for this phenomenon is that,
159"
VICTIM DISTRIBUTIONS,0.5169082125603864,"according to its definition, fCW becomes 0 if the classifier is successfully deceived during the iteration
160"
VICTIM DISTRIBUTIONS,0.5193236714975845,"process. This setting ensures that the generator does not strive for a relatively high softmax probability
161"
VICTIM DISTRIBUTIONS,0.5217391304347826,"for the target class; it simply needs to reach a point where the victim classifier perceives the image as
162"
VICTIM DISTRIBUTIONS,0.5241545893719807,"belonging to the target class. Consequently, after the iteration, the victim classifier assigns a relatively
163"
VICTIM DISTRIBUTIONS,0.5265700483091788,"low predictive probability to the target class σ(gϕ(xadv))[ytar], as demonstrated in Figure 3(d) and
164"
VICTIM DISTRIBUTIONS,0.5289855072463768,"Figure 4(d).
165"
VICTIM DISTRIBUTIONS,0.5314009661835749,"In this study, we introduce two additional choices for the function f. Although these alternatives are
166"
VICTIM DISTRIBUTIONS,0.533816425120773,"not as effective as fCW, we present them in Appendix C for further exploration.
167"
VICTIM DISTRIBUTIONS,0.5362318840579711,"4.2
Data Augmentation by Thin Plate Splines (TPS) Deformation
168"
VICTIM DISTRIBUTIONS,0.538647342995169,"Thin-plate-spline (TPS) [1] is a commonly used image deforming method. Given a pair of control
169"
VICTIM DISTRIBUTIONS,0.5410628019323671,"points and target points, TPS computes a smooth transformation that maps the control points to the
170"
VICTIM DISTRIBUTIONS,0.5434782608695652,"target points, minimizing the bending energy of the transformation. This process results in localized
171"
VICTIM DISTRIBUTIONS,0.5458937198067633,"deformations while preserving the overall structure of the image, making TPS a valuable tool for data
172"
VICTIM DISTRIBUTIONS,0.5483091787439613,"augmentation.
173"
VICTIM DISTRIBUTIONS,0.5507246376811594,"Psou
Ptar"
VICTIM DISTRIBUTIONS,0.5531400966183575,"xori
tTPS(xori, Psou, Ptar)"
VICTIM DISTRIBUTIONS,0.5555555555555556,"Figure 5: TPS as a data augmentation.
Left: The original image xori superim-
posed with a 5 × 5 grid of source control
points Psou. Right: The transformed im-
age overlaid with a grid of target control
points Ptar."
VICTIM DISTRIBUTIONS,0.5579710144927537,"As introduced in Section 3.2, we aim to train an energy-
174"
VICTIM DISTRIBUTIONS,0.5603864734299517,"based model on transformations of a single image xori.
175"
VICTIM DISTRIBUTIONS,0.5628019323671497,"In practice, if the diversity of the augmentations of xori,
176"
VICTIM DISTRIBUTIONS,0.5652173913043478,"represented as t(xori), is insufficient, the training of the
177"
VICTIM DISTRIBUTIONS,0.5676328502415459,"probabilistic generative model is prone to overfitting.
178"
VICTIM DISTRIBUTIONS,0.5700483091787439,"To address this issue, we use TPS as a data augmen-
179"
VICTIM DISTRIBUTIONS,0.572463768115942,"tation method to increase the diversity of t(xori). For
180"
VICTIM DISTRIBUTIONS,0.5748792270531401,"each xori, we set a 5 × 5 grid of source control points,
181"
VICTIM DISTRIBUTIONS,0.5772946859903382,"Psou = {(x(i), y(i))}5×5
i=1 , and defining the target points as
182"
VICTIM DISTRIBUTIONS,0.5797101449275363,"Ptar = {(x(i) + ϵ(i)
x , y(i) + ϵ(i)
y )}5×5
i=1 , where ϵ(i)
x , ϵ(i)
y
∼
183"
VICTIM DISTRIBUTIONS,0.5821256038647343,"N(0, σ2) are random noise added to the source control
184"
VICTIM DISTRIBUTIONS,0.5845410628019324,"points. We then apply TPS transformation to xori with
185"
VICTIM DISTRIBUTIONS,0.5869565217391305,"Psou and Ptar as its parameters. This procedure is depicted
186"
VICTIM DISTRIBUTIONS,0.5893719806763285,"in Figure 5. By setting an appropriate σ, we can substan-
187"
VICTIM DISTRIBUTIONS,0.5917874396135265,"tially increase the diversity of the one-image dataset while
188"
VICTIM DISTRIBUTIONS,0.5942028985507246,"maintaining its semantic content.
189"
REJECTION SAMPLING,0.5966183574879227,"4.3
Rejection Sampling
190"
REJECTION SAMPLING,0.5990338164251208,"Directly sampling from padv(·; xori, ytar) does not guarantee the generation of samples capable of effec-
191"
REJECTION SAMPLING,0.6014492753623188,"tively deceiving the classifier. To overcome this issue, we adopt rejection sampling [22], which elimi-
192"
REJECTION SAMPLING,0.6038647342995169,"nates unsuccessful samples and ultimately yields samples from padv(xadv| arg maxy gϕ(xadv)[y] =
193"
REJECTION SAMPLING,0.606280193236715,"ytar; xori, ytar).
194"
SAMPLE REFINEMENT,0.6086956521739131,"4.4
Sample Refinement
195"
SAMPLE REFINEMENT,0.6111111111111112,"After rejection sampling, the samples are confirmed to successfully deceive the classifier. However,
196"
SAMPLE REFINEMENT,0.6135265700483091,"not all of them possess high visual quality, as demonstrated in Figure 4(c). To automatically obtain N
197"
SAMPLE REFINEMENT,0.6159420289855072,"semantically valid samples1, we first generate M samples from the adversarial distribution. Following
198"
SAMPLE REFINEMENT,0.6183574879227053,"rejection sampling, we sort the remaining samples and select the top κ percent based on the softmax
199"
SAMPLE REFINEMENT,0.6207729468599034,"probability of the original image’s class, as determined by an auxiliary classifier. Finally, we choose
200"
SAMPLE REFINEMENT,0.6231884057971014,"the top N samples with the lowest energy E, meaning they have the highest likelihood according to
201"
SAMPLE REFINEMENT,0.6256038647342995,"the energy-based model.
202"
SAMPLE REFINEMENT,0.6280193236714976,"The auxiliary classifier is trained on the data-augmented training set. We do not use the energy of
203"
SAMPLE REFINEMENT,0.6304347826086957,"the samples as the sole criterion for selection because some low-visual quality samples may also
204"
SAMPLE REFINEMENT,0.6328502415458938,"have a high likelihood. This occurrence is further explained and examined in Appendix D. The entire
205"
SAMPLE REFINEMENT,0.6352657004830918,"process of rejection sampling and sample refinement is portrayed in Algorithm 1.
206"
SAMPLE REFINEMENT,0.6376811594202898,Algorithm 1 Rejection Sampling and Sample Refinement
SAMPLE REFINEMENT,0.6400966183574879,"Input: A trained energy based model E(·; xori) based on the original image xori, the victim classifier
gϕ, an auxiliary classifier gψ, number of initial samples M, number of final samples N, the
percentage κ.
Output: N adversarial samples x."
SAMPLE REFINEMENT,0.642512077294686,"x = ∅
for 0 ≤i < M do"
SAMPLE REFINEMENT,0.644927536231884,"xadv ∼padv(·; xori, ytar)
▷Sample from the adversarial distribution.
if arg maxy gϕ(xadv)[y] = ytar then
▷Accept if xadv deceive the classifier.
x = x ∪{xadv}
end if
end for
Sort x by σ(gψ(xi))[yori] for i ∈{1, . . . , |x|} in descent order
x = (xi)⌊κ|x|⌋
i=1
▷Select the first κ percent elements from x.
Sort x by E(xi; xori) for i ∈{1, . . . , |x|} in ascent order
x = (xi)N
i=1
▷Select the first N elements from x."
SAMPLE REFINEMENT,0.6473429951690821,"1In practice, we could select adversarial samples by hand, but we focus on automatic selection here."
SAMPLE REFINEMENT,0.6497584541062802,Source
SAMPLE REFINEMENT,0.6521739130434783,"99
100 100 100 100 100 100 100 100"
SAMPLE REFINEMENT,0.6545893719806763,"100
100 100 100 100 100 100 100 100"
SAMPLE REFINEMENT,0.6570048309178744,"100 100
100 100
97
100 100 100 100"
SAMPLE REFINEMENT,0.6594202898550725,"84
100 100
100 100 100 100 100 100"
SAMPLE REFINEMENT,0.6618357487922706,"98
96
97
88
80
99
97
94
93"
SAMPLE REFINEMENT,0.6642512077294686,"75
87
99
96
89
98
94
89
83"
SAMPLE REFINEMENT,0.6666666666666666,"99
100
97
100 100 100
95
100
97"
SAMPLE REFINEMENT,0.6690821256038647,"98
100 100 100 100 100 100
100 100"
SAMPLE REFINEMENT,0.6714975845410628,"71
94
99
98
96
69
87
99
98"
SAMPLE REFINEMENT,0.6739130434782609,"100
93
93
76
99
71
96
100
99"
SAMPLE REFINEMENT,0.6763285024154589,Target MNIST
SAMPLE REFINEMENT,0.678743961352657,"0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9"
SAMPLE REFINEMENT,0.6811594202898551,"22
65
66
48
19
79
64
57
73"
SAMPLE REFINEMENT,0.6835748792270532,"58
98
95
96
99
87
60
100
31"
SAMPLE REFINEMENT,0.6859903381642513,"97
98
99
99
98
97
100
98
100"
SAMPLE REFINEMENT,0.6884057971014492,"95
99
100
100
99
97
99
97
100"
SAMPLE REFINEMENT,0.6908212560386473,"87
99
94
99
98
93
100
97
89"
SAMPLE REFINEMENT,0.6932367149758454,"100 100 100 100 100
100 100 100 100"
SAMPLE REFINEMENT,0.6956521739130435,"98
94
91
62
97
99
91
96
92"
SAMPLE REFINEMENT,0.6980676328502415,"50
100
4
49
93
54
78
34
7"
SAMPLE REFINEMENT,0.7004830917874396,"91
92
100 100
99
98
97
99
100"
SAMPLE REFINEMENT,0.7028985507246377,"96
97
98
99
100 100
76
99
96"
SAMPLE REFINEMENT,0.7053140096618358,Target SVHN
SAMPLE REFINEMENT,0.7077294685990339,"0
1
2
3
4
5
6
7
8
9"
SAMPLE REFINEMENT,0.7101449275362319,"Figure 6: The success rates (%) of our targeted unrestricted adversarial attack. Corresponding sample
examples for each grid are depicted in the top right and bottom right sections of Figure 1. Refer to
Table 1 for overall success rate."
EXPERIMENT,0.7125603864734299,"5
Experiment
207"
IMPLEMENTATION,0.714975845410628,"5.1
Implementation
208"
IMPLEMENTATION,0.717391304347826,"We implemented our proposed semantics-aware adversarial attack on two datasets: MNIST and
209"
IMPLEMENTATION,0.7198067632850241,"SVHN. For the MNIST dataset, the victim classifier we used was an adversarially trained MadryNet
210"
IMPLEMENTATION,0.7222222222222222,"[12]. For the SVHN dataset, we utilized an adversarially trained ResNet18, in accordance with
211"
IMPLEMENTATION,0.7246376811594203,"the methodology outlined by Song et al. [17]. On the distance distribution side, for every original
212"
IMPLEMENTATION,0.7270531400966184,"image denoted as xori, we trained an energy-based model on the training set, which is represented
213"
IMPLEMENTATION,0.7294685990338164,"as {t1(xori), t2(xori), . . . }. In this case, ti follows a distribution of transformations, T , that do not
214"
IMPLEMENTATION,0.7318840579710145,"change the semantics of xori. For the MNIST dataset, we characterized TMNIST as including Thin
215"
IMPLEMENTATION,0.7342995169082126,"Plate Spline (TPS) transformations, scaling, and rotation. For the SVHN dataset, we defined TSVHN as
216"
IMPLEMENTATION,0.7367149758454107,"comprising Thin Plate Spline (TPS) transformations and alterations in brightness and hue. Detailed
217"
IMPLEMENTATION,0.7391304347826086,"specifics related to our implementation can be found in Appendix A.
218"
EVALUATION,0.7415458937198067,"5.2
Evaluation
219"
EVALUATION,0.7439613526570048,"Our method generates adversarial samples that can deceive classifiers, but it does not guarantee the
220"
EVALUATION,0.7463768115942029,"preservation of the original label’s semantic meaning. As such, we consider an adversarial example
221"
EVALUATION,0.748792270531401,"successful if human annotators perceive it as having the same meaning as the original label, in line
222"
EVALUATION,0.751207729468599,"with the approach by Song et al. [17]. To enhance the signal-to-noise ratio, we assign the same image
223"
EVALUATION,0.7536231884057971,"to five different annotators and use the majority vote as the human decision, as done in [17]. The
224"
EVALUATION,0.7560386473429952,"screenshot of the annotator’s interface is in Appendix B.
225"
EVALUATION,0.7584541062801933,"In detail, we begin with an original image xori, its label yori, and a target class ytar. We draw
226"
EVALUATION,0.7608695652173914,"M = 2000 samples from padv(·; xori, ytar), rejecting those that fail to deceive the victim classifier.
227"
EVALUATION,0.7632850241545893,"After sample refinement, we obtain N = 100 adversarial examples, x(i)
adv for i ∈{1, . . . , N}. We
228"
EVALUATION,0.7657004830917874,"express the human annotators’ decision as function h and derive the human decision y(i)
hum = h(x(i)
adv).
229"
EVALUATION,0.7681159420289855,"As previously mentioned, an adversarial example x(i)
adv is considered successful if y(i)
hum is equal to yori.
230"
EVALUATION,0.7705314009661836,"We then compute the success rate s as follows:
231"
EVALUATION,0.7729468599033816,"s =
PN
i=1 1(y(i)
hum = yori)
N
where 1 represents the indicator function.
232"
EVALUATION,0.7753623188405797,"We randomly select 10 digits, each representing a different class, from the MNIST/SVHN test set
233"
EVALUATION,0.7777777777777778,"to serve as the original image xori. These are depicted on the left side of Figure 1. For each xori,
234"
EVALUATION,0.7801932367149759,"we iterate through the target class ytar ranging from 0 to 9, excluding the class yori that signifies
235"
EVALUATION,0.782608695652174,"Table 1: Success rate comparison between the method proposed by Song et al. [17] and ours. The
results presented in this table are for reference only, as Song’s results are taken directly from their
paper, and we did not use the same group of annotators for our evaluation."
EVALUATION,0.785024154589372,"Robust Classifier
Success Rate of
Our Success Rate
Song et al. [17]"
EVALUATION,0.7874396135265701,"MadryNet [12] on MNIST
85.2
96.2
ResNet18 [8] (adv-trained) on SVHN
84.2
86.3"
EVALUATION,0.7898550724637681,"the ground-truth label of xori. As previously described, for every pair of xori and ytar, we generate
236"
EVALUATION,0.7922705314009661,"N = 100 adversarial examples post sample refinement. The result of each pair is illustrated in
237"
EVALUATION,0.7946859903381642,"Figure 6. The overall success rate is illustrated in Figure 1.
238"
RESULTS,0.7971014492753623,"5.3
Results
239"
RESULTS,0.7995169082125604,"As depicted in Figure 6 and Table 1, our proposed method often succeeds in fooling robust classifiers,
240"
RESULTS,0.8019323671497585,"all the while preserving the original semantics of the input. It should be noted, however, that this
241"
RESULTS,0.8043478260869565,"does not occur in every instance.
242"
RELATED WORK,0.8067632850241546,"6
Related work
243"
RELATED WORK,0.8091787439613527,"Unrestricted adversarial examples
Song et al. [17] proposed generating unrestricted adversarial
244"
RELATED WORK,0.8115942028985508,"examples from scratch using conditional generative models. In their work, the term “unrestricted”
245"
RELATED WORK,0.8140096618357487,"indicates that the generated adversarial samples, xadv, are not restricted by a geometric distance
246"
RELATED WORK,0.8164251207729468,"such as the L2 norm or L∞norm. The key difference between their approach and ours is that their
247"
RELATED WORK,0.8188405797101449,"adversarial examples xadv are independent of any specific xori, while our model generates xadv based
248"
RELATED WORK,0.821256038647343,"on a given xori. By slightly modifying (7), we can easily incorporate Song’s “unrestricted adversarial
249"
RELATED WORK,0.8236714975845411,"examples” into our probabilistic perspective:
250"
RELATED WORK,0.8260869565217391,"padv(xadv; ysou, ytar) := pvic(xadv; ytar)pdis(xadv; ysou)
(8)"
RELATED WORK,0.8285024154589372,"where ysou is the source class. It becomes evident that the adversarial examples generated by our
251"
RELATED WORK,0.8309178743961353,"padv(·; xori, ytar) adhere to Song’s definition when xori is labeled as ysou.
252"
RELATED WORK,0.8333333333333334,"TPS as a Data Augmentation Technique
To the best of our knowledge, Vinker et al. [21] were
253"
RELATED WORK,0.8357487922705314,"the first to employ TPS as a data augmentation method. They utilized TPS as a data augmentation
254"
RELATED WORK,0.8381642512077294,"strategy in their generative model for conditional image manipulation based on a single image.
255"
LIMITATION,0.8405797101449275,"7
Limitation
256"
LIMITATION,0.8429951690821256,"This work’s foremost limitation pertains to the inherent difficulties in training energy-based models
257"
LIMITATION,0.8454106280193237,"(EBMs), as underscored in the earlier studies by Du and Mordatch [4] and Grathwohl et al. [7]. The
258"
LIMITATION,0.8478260869565217,"EBM training process is notoriously challenging, and a notable gap persists between the generation
259"
LIMITATION,0.8502415458937198,"quality of EBMs and that of other widely-used probabilistic generative models, such as variational
260"
LIMITATION,0.8526570048309179,"autoencoders and diffusion models. Consequently, we are currently unable to generate adversarial
261"
LIMITATION,0.855072463768116,"samples for images with higher resolution.
262"
CONCLUSION,0.857487922705314,"8
Conclusion
263"
CONCLUSION,0.8599033816425121,"In this work, we present a probabilistic perspective on adversarial examples by employing Langevin
264"
CONCLUSION,0.8623188405797102,"Monte Carlo. Building on this probabilistic perspective, we introduce semantic divergence as an
265"
CONCLUSION,0.8647342995169082,"alternative to the commonly used geometric distance. We also propose corresponding techniques for
266"
CONCLUSION,0.8671497584541062,"generating semantically-aware adversarial examples. Human participation experiments indicate that
267"
CONCLUSION,0.8695652173913043,"our proposed method can often deceive robust classifiers while maintaining the original semantics of
268"
CONCLUSION,0.8719806763285024,"the input, although not in all cases.
269"
REFERENCES,0.8743961352657005,"References
270"
REFERENCES,0.8768115942028986,"[1] F. L. Bookstein. Principal warps: Thin-plate splines and the decomposition of deformations. IEEE
271"
REFERENCES,0.8792270531400966,"Transactions on pattern analysis and machine intelligence, 11(6):567–585, 1989.
272"
REFERENCES,0.8816425120772947,"[2] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee symposium
273"
REFERENCES,0.8840579710144928,"on security and privacy (sp), pages 39–57. Ieee, 2017.
274"
REFERENCES,0.8864734299516909,"[3] T.-S. Chiang, C.-R. Hwang, and S. J. Sheu. Diffusion for global optimization in rˆn. SIAM Journal on
275"
REFERENCES,0.8888888888888888,"Control and Optimization, 25(3):737–753, 1987.
276"
REFERENCES,0.8913043478260869,"[4] Y. Du and I. Mordatch. Implicit generation and generalization in energy-based models. arXiv preprint
277"
REFERENCES,0.893719806763285,"arXiv:1903.08689, 2019.
278"
REFERENCES,0.8961352657004831,"[5] S. B. Gelfand and S. K. Mitter. Recursive stochastic algorithms for global optimization in rˆd. SIAM
279"
REFERENCES,0.8985507246376812,"Journal on Control and Optimization, 29(5):999–1018, 1991.
280"
REFERENCES,0.9009661835748792,"[6] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv
281"
REFERENCES,0.9033816425120773,"preprint arXiv:1412.6572, 2014.
282"
REFERENCES,0.9057971014492754,"[7] W. Grathwohl, K.-C. Wang, J.-H. Jacobsen, D. Duvenaud, M. Norouzi, and K. Swersky. Your classifier is
283"
REFERENCES,0.9082125603864735,"secretly an energy based model and you should treat it like one. arXiv preprint arXiv:1912.03263, 2019.
284"
REFERENCES,0.9106280193236715,"[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the
285"
REFERENCES,0.9130434782608695,"IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
286"
REFERENCES,0.9154589371980676,"[9] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14
287"
REFERENCES,0.9178743961352657,"(8):1771–1800, 2002.
288"
REFERENCES,0.9202898550724637,"[10] R. Huang, B. Xu, D. Schuurmans, and C. Szepesvári. Learning with a strong adversary. arXiv preprint
289"
REFERENCES,0.9227053140096618,"arXiv:1511.03034, 2015.
290"
REFERENCES,0.9251207729468599,"[11] A. Lamperski. Projected stochastic gradient langevin algorithms for constrained sampling and non-convex
291"
REFERENCES,0.927536231884058,"learning. In Conference on Learning Theory, pages 2891–2937. PMLR, 2021.
292"
REFERENCES,0.9299516908212561,"[12] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to
293"
REFERENCES,0.9323671497584541,"adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
294"
REFERENCES,0.9347826086956522,"[13] M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient langevin
295"
REFERENCES,0.9371980676328503,"dynamics: a nonasymptotic analysis. In Conference on Learning Theory, pages 1674–1703. PMLR, 2017.
296"
REFERENCES,0.9396135265700483,"[14] G. O. Roberts and R. L. Tweedie. Exponential convergence of langevin distributions and their discrete
297"
REFERENCES,0.9420289855072463,"approximations. Bernoulli, pages 341–363, 1996.
298"
REFERENCES,0.9444444444444444,"[15] S. Santurkar, A. Ilyas, D. Tsipras, L. Engstrom, B. Tran, and A. Madry. Image synthesis with a single
299"
REFERENCES,0.9468599033816425,"(robust) classifier. Advances in Neural Information Processing Systems, 32, 2019.
300"
REFERENCES,0.9492753623188406,"[16] Y. Song and D. P. Kingma. How to train your energy-based models. arXiv preprint arXiv:2101.03288,
301"
REFERENCES,0.9516908212560387,"2021.
302"
REFERENCES,0.9541062801932367,"[17] Y. Song, R. Shu, N. Kushman, and S. Ermon. Constructing unrestricted adversarial examples with
303"
REFERENCES,0.9565217391304348,"generative models. Advances in Neural Information Processing Systems, 31, 2018.
304"
REFERENCES,0.9589371980676329,"[18] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
305"
REFERENCES,0.961352657004831,"properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
306"
REFERENCES,0.9637681159420289,"[19] Y. W. Teh, A. H. Thiery, and S. J. Vollmer. Consistency and fluctuations for stochastic gradient langevin
307"
REFERENCES,0.966183574879227,"dynamics. Journal of Machine Learning Research, 17, 2016.
308"
REFERENCES,0.9685990338164251,"[20] B. Tzen, T. Liang, and M. Raginsky. Local optimality and generalization guarantees for the langevin
309"
REFERENCES,0.9710144927536232,"algorithm via empirical metastability. In Conference On Learning Theory, pages 857–875. PMLR, 2018.
310"
REFERENCES,0.9734299516908212,"[21] Y. Vinker, E. Horwitz, N. Zabari, and Y. Hoshen. Image shape manipulation from a single augmented
311"
REFERENCES,0.9758454106280193,"training sample. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
312"
REFERENCES,0.9782608695652174,"13769–13778, 2021.
313"
REFERENCES,0.9806763285024155,"[22] J. Von Neumann. 13. various techniques used in connection with random digits. Appl. Math Ser, 12(36-38):
314"
REFERENCES,0.9830917874396136,"3, 1951.
315"
REFERENCES,0.9855072463768116,"[23] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of
316"
REFERENCES,0.9879227053140096,"the 28th international conference on machine learning (ICML-11), pages 681–688, 2011.
317"
REFERENCES,0.9903381642512077,"[24] P. Xu, J. Chen, D. Zou, and Q. Gu. Global convergence of langevin dynamics based algorithms for
318"
REFERENCES,0.9927536231884058,"nonconvex optimization. Advances in Neural Information Processing Systems, 31, 2018.
319"
REFERENCES,0.9951690821256038,"[25] Y. Zhang, P. Liang, and M. Charikar. A hitting time analysis of stochastic gradient langevin dynamics. In
320"
REFERENCES,0.9975845410628019,"Conference on Learning Theory, pages 1980–2022. PMLR, 2017.
321"
