Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002053388090349076,"We propose a principled method to learn a set of human-readable logic rules to
explain temporal point processes. We assume that the generative mechanisms
underlying the temporal point processes are governed by a set of first-order tem-
poral logic rules, as a compact representation of domain knowledge. Our method
formulates the rule discovery process from noisy event data as a maximum likeli-
hood problem, and designs an efficient and tractable branch-and-price algorithm
to progressively search for new rules and expand existing rules. The proposed
algorithm alternates between the rule generation stage and the rule evaluation
stage, and uncovers the most important collection of logic rules within a fixed time
limit for both synthetic and real event data. In a real healthcare application, we
also had human experts (i.e., doctors) verify the learned temporal logic rules and
provide further improvements. These expert-revised interpretable rules lead to a
point process model which outperforms previous state-of-the-arts for symptom
prediction, both in their occurrence times and types. 1"
INTRODUCTION,0.004106776180698152,"1
INTRODUCTION"
INTRODUCTION,0.006160164271047228,"Event sequences with irregular time intervals are ubiquitous. The inter-event times usually convey
rich information regarding the underlying dynamics such as disease progression (Liu et al., 2015).
It is useful to understand events’ generating mechanisms, as well as the occurrence reason and
time. In systems where domain knowledge is rich, events generating can usually be governed by a
few first-order temporal logic rules, as a compact representation of knowledge. In healthcare, the
knowledge “if a sudden fall in blood pressure is observed, vasopressors are required to be applied
to patients immediately; and then the blood pressure may return to normal afterwards"" may explain
why the event “blood pressure returns to normal from the abnormally low level "" is observed after the
event “vasopressors are used"". This domain knowledge can be summarized as a collection of logic
rules with temporal relation constraints, such as “A happens before B”, “If A happens, and after 5
mins, B can happen”, and “If A and B happen simultaneously, then at the same time C can happen”.
However, temporal logic alone is not an ideal temporal model, as hard constraints will be too strict to
model the recurrent noisy event data.
Meanwhile, a large amount of literature has been devoted to modeling event data, among which
temporal point process (TPP) models provide an elegant framework without the need to discretize the
time horizon into bins and to compute the count of events within each bin. TPP models treat the inter-
event time as random variables and directly model the intensity function (i.e., occurrence rate) of the
events. However, most TPP models lack interpretability, and they can not represent domain knowledge
in a human-readable form. Recently, Li et al. (2020) proposed a unified framework to marry point"
INTRODUCTION,0.008213552361396304,"∗Equal contribution.
†Shuang Li is with the School of Data Science, The Chinese University of Hong Kong, Shenzhen, Shenzhen
518172, China, and the Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen 518129,
China (corresponding author, e-mail: lishuang@cuhk.edu.cn).
‡Mingquan Feng and Junchi Yan are with Dept. of CSE, MoE Key Lab of Artificial Intelligence, and
SJTU-Yale Joint Center for Biostatistics and Data Science, National Center for Translational Medicine, SJTU.
§Yufeng Cao is with the Antai College of Economics and Management and the Data-Driven Management
Decision Making Lab, SJTU.
1Code is available at https://github.com/FengMingquan-sjtu/Logic_Point_Processes_ICLR"
INTRODUCTION,0.01026694045174538,Published as a conference paper at ICLR 2022
INTRODUCTION,0.012320328542094456,"process intensity functions with temporal logic rules. This method employs a set of pre-specified
temporal logic rules to design the intensity functions to incorporate domain knowledge. The resulting
so-called temporal logic point process models are inherently interpretable and expressive. Using such
logic-informed intensity functions, the model can capture nonlinear dependencies, interactions, and
various temporal relations between events. It was also shown that, by specifying one or two simple
temporal logic rules, many existing parametric models, e.g. the Hawkes process (i.e. self-exciting
processes) (Hawkes, 1971a) and the self-correcting process (Isham & Westcott, 1979) are special
cases of the proposed temporal logic point process.
However, in (Li et al., 2020) the temporal logic rules are required to be specified by the model builder,
but they may not be known beforehand. Models constructed in such a way may suffer from model
misspecification when the pre-specified logic rules are incorrect or missing.
Can we automatically discover the temporal logic rules governing the event dynamics based on
historical event data alone? It is challenging as the space of possible temporal logic formulas is huge,
comprising of combinations of massive discrete logic variables and all kinds of temporal relations.
Previous inductive logic programming approaches focused mostly on deriving the relations between
discrete logic variables, but have largely ignored the crucial temporal information (Dash et al., 2018;
Wei et al., 2019). A generalization of these methods by further considering temporal relations in
logic variables is largely missing and is in pressing needs due to the increasing availability of event
data. To address the aforementioned challenges, we propose the TEmporal Logic rule LearnER
(TELLER) algorithm for learning temporal logic rules from event sequences. TELLER is inspired by
the branch-and-price algorithm (Barnhart et al., 1998), a column generation (CG) algorithm for linear
programming (LP) problems where the number of variables is too large to be considered explicitly.
In the CG algorithm framework, the original LP problem is solved via two alternating procedures:
the master problem and the subproblem, where the master problem is the original problem with only
a subset of variables being considered and the subproblem is a new problem created to identify a
new variable to be added. Similarly, TELLER also alternates between solving a master problem
and a subproblem, where the master problem aims to evaluate the current rules by maximizing the
likelihood and reweighting these rules as in (Li et al., 2020), and the subproblem is to search and
construct a new temporal logic rule (by extending a current rule or adding a new short rule). It repeats
until the likelihood can no longer be improved by adding new candidate rules. In this way, TELLER
searches through the vast space of potential temporal logic rules and learns the importance weights of
the discovered rules to hedge against noise in the data. Fig. 1 shows the flow."
INTRODUCTION,0.014373716632443531,"Figure 1: The flow of TELLER. It
alternates between a rule evaluation
stage (master problem) and a rule
proposal stage (subproblem)."
INTRODUCTION,0.01642710472279261,"Specifically, TELLER is computationally efficient and with guar-
antees. It includes the candidate rules one by one in a tractable
and progressive fashion, and is able to find a near-optimal set of
rules. The hypothesized logic rules are in disjunctive normal form
(DNF, OR-of-ANDs) with temporal relation constraints, and can
be of various lengths. Given the temporal logic point process mod-
eling framework, we show that the resulting objective function,
i.e. likelihood, is convex, which guarantees optimal performance
for our search algorithm. A set of logic rules and their importance
weights are jointly learned by maximizing the likelihood. The
most important collection of logic rules are guaranteed to be discovered within a fixed time limit.
The uncovered rules by TELLER will not only shed light on when and what events would happen,
but also why some events would happen at a specific time. Our method facilitates the human-readable
knowledge exchange between experts and point process models. On one hand, the mined temporal
logic rules may supplement or refine the existing knowledge; on the other hand, human experts can
easily provide feedback to modify the learned models via logic rules, as a way to add the safety of
using these models in high-stake tasks like healthcare and autonomous driving. In our real experiment
related to medicine, TELLER is used to learn the explanatory temporal logic rules regarding the
choice and arrangement of drugs in improving patients’ health status. Doctors were asked to justify
the rules and they confirmed that these discovered logic rules are consistent with the pathogenesis
and have captured the most important factors in affecting patients’ health status.
2
BACKGROUND
2.1
FIRST-ORDER TEMPORAL LOGIC RULES
First-order temporal logic is a form of symbolized reasoning in which each statement is a composition
of temporal predicates and their relations. We formally define the interval-based temporal logic below."
INTRODUCTION,0.018480492813141684,Published as a conference paper at ICLR 2022
INTRODUCTION,0.02053388090349076,"This type of temporal logic rule is often suitable for reasoning about events with duration, which are
better modeled if the underlying temporal ontology uses time intervals. It fits well with the temporal
point process models for event sequences, where the time intervals of events are explicitly modeled.
Refer to (Goranko & Rumberg, 2021) for a comprehensive survey of various temporal logic models.
Temporal predicate. First define a set of static predicates X = {X1, . . . , Xd}, where each static
predicate Xi(c) is a logic random variable that defines the property or relation of entities, such
as Smokes(c) or Friend(c, c′), where c and c′ are the entities. By adding a temporal dimension to
predicate Xi, we obtain a temporal predicate {Xi(c, t)}t≥0, which can be viewed as a continuous-time
stochastic process. Given observations, each temporal predicate will be grounded as a list of ordered
0-1 events, which take True (1) and False (0) in an alternating way with the state transition times
recorded. For example, a temporal predicate {NormalBloodPressure(c, t)}t≥0, where entity c is
referring to the patient, will take value 1 or 0 at any time t to indicate whether the patient’s blood
pressure is normal or not, with stochastic state transition time (see Fig. 2)."
INTRODUCTION,0.022587268993839837,"time
0
0"
INTRODUCTION,0.024640657084188913,"1
0
0
1
0 1
1
0 ! time ℋ("
TIME,0.026694045174537988,"0
time
1
0
1
1
0
0"
TIME,0.028747433264887063,""""" #
#$%"
TIME,0.030800821355236138,"""& #
#$%"
TIME,0.03285420944558522,"$ #
#$%"
TIME,0.03490759753593429,"%! & =
("
TIME,0.03696098562628337,"""!""∈ℋ! "" ("
TIME,0.039014373716632446,"""!#∈ℋ!#"
TIME,0.04106776180698152,)! (+)
TIME,0.043121149897330596,Given repeated events
TIME,0.045174537987679675,Feature:
TIME,0.04722792607802875,"!!'
!!'"
TIME,0.049281314168377825,"!!&
!!&
!!&"
VALID COMBINATIONS,0.0513347022587269,"6 valid combinations
Figure 2:
Feature constructions of
TLPP using a simple logic formula
f : Y ←A∧B, as a template to gather
combinations of the body predicate his-
tory events. A has 2 events and B has 3
events, leading to 6 valid combinations."
VALID COMBINATIONS,0.053388090349075976,"To simplify the notation, we will temporally drop the depen-
dency of predicates on entities. We will write X(c, t) as X(t)
instead and the grounded temporal predicate as {x(t)}t≥0."
TEMPORAL LOGIC POINT PROCESS,0.055441478439425054,"2.2
TEMPORAL LOGIC POINT PROCESS
Temporal Point Process (TPP) provides an elegant tool to cap-
ture the dynamics of the event sequences. It is characterized by
conditional intensity function, denoted by λ(t|Ht), where his-
tory Ht is the knowledge of times of all events. By definition,
we have λ(t|Ht)dt = E[N([t, t+dt])|Ht], where N([t, t+dt])
denotes the number of points falling in an interval [t, t + dt].
Here we aim to use TPP to model these 0-1 events and use the
intensity function to capture the 0-1 transition rate.
Temporal relation. We will use the interval-based temporal
relations originally introduced in Allen’s seminal paper (Allen,
1990) to describe the temporal relations that can exist in any
two temporal predicates. There are 13 types of possible temporal relations, including Before, Meets,
Overlap and so on (see Appendix B for a comprehensive illustration), and we denote this set by
R. To evaluate these temporal relations, we need to define the time interval using the state transit
in/out times. For example, at time tA, the temporal predicate {XA(t)}t≥0 takes state xA = 0
or 1, we define ItA = (tA1, tA2], tA ∈(tA1, tA2], as a time interval where tA1 is the transition
time that this predicate enters the state xA and tA2 is the transition time that this predicate leaves
the state xA. For any two temporal predicates, given their time intervals, ItA = (tA1, tA2] and
ItB = (tB1, tB2], their temporal relation function, denoted as RA type B(ItA, ItB) ∈R, is a logic
function defined over the time intervals and can be evaluated by plugging in the specific time intervals.
For example, RA Before B = 1{tB1 > tA2} and RA Equals B = 1{tA1 = tB1}1{tA2 = tB2}. All
these definitions of temporal relations are also given in Appendix B.
Temporal logic formula. A first-order temporal logic rule f is defined as logical connectives of
temporal predicates and their temporal relations. The generic form is:
f : (¬)Y (ty) ←
^"
TEMPORAL LOGIC POINT PROCESS,0.057494866529774126,"Xu∈X +
f
Xu(tu)
^"
TEMPORAL LOGIC POINT PROCESS,0.059548254620123205,"Xv∈X −
f
¬Xv(tv)
^"
TEMPORAL LOGIC POINT PROCESS,0.061601642710472276,"Xu,Xv∈Xf Ru type v(Itu, Itv)
(1)"
TEMPORAL LOGIC POINT PROCESS,0.06365503080082136,"where each temporal predicate and the temporal relations can be evaluated at ty, tu and tv along
with their corresponding intervals Itu and Itv. In the above formula, negation (¬) means the head
predicate Y can take a negative sign; Xf = X +
f ∪X −
f is the set of predicates defined in f, where
X −
f is the set of predicates as negation in the formula f, and X +
f = Xf \ X −
f . We assume ←has a
causal direction, and the body part of the formula indicate the evidence to be gathered from history to
deduce the state of the head predicate. By this assumption, it is only valid to consider ty ≥tu, tv.
Given observed sequences of data, each predicate in X will be grounded as a list of ordered 0-1
events. The main idea of temporal logic point process (TLPP) (Li et al., 2020) is to use the temporal
logic rules (1) as templates to selectively choose combinations of events from history as evidence to
infer the transition rate of these 0-1 events. In this way, the structure of the intensity function will be
informed by the pre-specified temporal logic rules. Below we provide more details.
Logic-informed intensity function. Let Ht include all the historical trajectories of the grounded
predicates in X up to t. TLPP leverages the rule set to model the state transition dynamics of the
head predicates. Define λ∗(t) := λ(t|Ht) as the transition intensity for {Y (t)}t≥0 to transit from 0 to"
TEMPORAL LOGIC POINT PROCESS,0.06570841889117043,Published as a conference paper at ICLR 2022
TEMPORAL LOGIC POINT PROCESS,0.06776180698151951,"1, given history up to t; and µ∗(t) := µ(t|Ht) vice versa. Assume the generating process of the 0-1
events are governed by the set of temporal logic rules F = {f1, f2, . . . }.
First consider one formula f, to incorporate its knowledge in intensity construction, we define a
formula effect (FE), which aims to gather only the effective combinations of the historical predicate
events defined by f as evidence to reason about the transition rate of the head predicate. To ease the
notation, define the predicate index set of Xf as U, then the formula effect of f is computed as
δf (t | y(t), {xu(tu), Itu}u∈U ∈Ht) := f (1 −y(t), {xu(tu), Itu}u∈U) −f (y(t), {xu(tu), Itu}u∈U)
(2)
where y(t) is the observed head predicate state at t, 1 −y(t) is its counterfactual state,
{xu(tu), Itu}u∈U ∈Ht is one historical combination of the body predicates (including their states
and time intervals) in f, and f(·) is the clausal form of (1), which is an alternative expression to ease
the evaluation of the rule. For example, Y ←X is logically equivalent to its clausal form ¬Y ∨X.
FE, which is essentially a difference between the what-if scenario and the true scenario, answers the
question such as “should Y (t) transit its state given logic formula f is true"".
One can check that the sign of FE can only be 1, -1 or 0, which can be interpreted as: sign(FE) = 1
indicates a positive effect for the head to transit, -1 indicates a negative effect, and 0 means no effect.
Only the non-zero FE will refer to an effective combination. Let Hu
t be the historical trajectory
specific to predicate u up to t, and one can aggregate all the valid (i.e., non-zero) FEs from history by
a summation over all combinations of the temporal predicate states and their intervals, i.e.,
ϕf(t) =
X"
TEMPORAL LOGIC POINT PROCESS,0.06981519507186858,"{(xu(tu),Itu)∈Hu
t }u∈U δf(t | y(t), {xu(tu), Itu}u∈U),
(3)"
TEMPORAL LOGIC POINT PROCESS,0.07186858316221766,"where ϕf(t) is the feature informed by logic formula f. One can refer to Fig. 2 for an illustration.
For each f ∈F, one can build the rule-informed and history-dependent features ϕf(t) as above, and
assumes the rules are connected in disjunctive normal form (OR-of-ANDs) to deduce Y . Then the
transition of {Y (t)}t≥0 are modeled as monotonically increasing and non-negative functions of the
weighted sum of the features:"
TEMPORAL LOGIC POINT PROCESS,0.07392197125256673,"λ∗(t) = exp

b0 +
X"
TEMPORAL LOGIC POINT PROCESS,0.07597535934291581,"f∈F wf · ϕf(t)

,
µ∗(t) = exp

b1 +
X"
TEMPORAL LOGIC POINT PROCESS,0.07802874743326489,"f∈F wf · ϕf(t)

(4)
where w = [wf]f∈F ≥0 are the weight parameters associated with each rule f ∈F, and b0, b1
are the spontaneous intensity, which are distinct for the dual intensities. Weight parameters will be
shared by the dual intensities, but the calculated features ϕf through (2) and (3) will always have
opposite signs. One can think of the formula weight wf as the confidence level put on f. The higher
the weight, the more influence that the formula will have on the intensity. The rationale of TLPP is
that the intensity functions are constructed in a way that the yielding 0-1 event sequences will enable
the set of logic rules to be more satisfied.
3
THE PROPOSED METHOD: TELLER Yes No"
TEMPORAL LOGIC POINT PROCESS,0.08008213552361396,Restricted master problem
TEMPORAL LOGIC POINT PROCESS,0.08213552361396304,(RMP) (Eq.(7))
TEMPORAL LOGIC POINT PROCESS,0.08418891170431211,"Evaluate current 
dual price (Eq.(8))"
TEMPORAL LOGIC POINT PROCESS,0.08624229979466119,Solve Subproblem to
TEMPORAL LOGIC POINT PROCESS,0.08829568788501027,identify a new rule.
TEMPORAL LOGIC POINT PROCESS,0.09034907597535935,Is the increased
TEMPORAL LOGIC POINT PROCESS,0.09240246406570841,gain negative?
TEMPORAL LOGIC POINT PROCESS,0.0944558521560575,"End
(Rule set is found)"
TEMPORAL LOGIC POINT PROCESS,0.09650924024640657,Add rule to RMP
TEMPORAL LOGIC POINT PROCESS,0.09856262833675565,"Temporal logic rule 
learning problem (Eq.(6))"
TEMPORAL LOGIC POINT PROCESS,0.10061601642710473,"Construct Subproblem 
from current dual price"
TEMPORAL LOGIC POINT PROCESS,0.1026694045174538,(Eq.(10))
TEMPORAL LOGIC POINT PROCESS,0.10472279260780287,Figure 3: TELLER
TEMPORAL LOGIC POINT PROCESS,0.10677618069815195,"Likelihood function and its convexity. Given {Ht}0<t<T , one can
write out the likelihood w.r.t. the intensity λ∗(t) and µ∗(t). Suppose
a realization of {Y (t)}0<t<T = {Y (0) = 0, Y (t1) = 1, . . . , Y (tn) =
1, tn < T < tn+1}, the (log) likelihood are (see proof in Appendix C):
ℓ(w, b0, b1) = log L(w, b0, b1), where:"
TEMPORAL LOGIC POINT PROCESS,0.10882956878850103,"L(w, b0, b1) =λ∗(t1) exp

−
Z t1"
TEMPORAL LOGIC POINT PROCESS,0.11088295687885011,"0
λ∗(s)ds

· µ∗(t2)
(5)"
TEMPORAL LOGIC POINT PROCESS,0.11293634496919917,"exp

−
Z t2"
TEMPORAL LOGIC POINT PROCESS,0.11498973305954825,"t1
µ∗(s)ds

· · · exp

−
Z T"
TEMPORAL LOGIC POINT PROCESS,0.11704312114989733,"tn
µ∗(s)ds

,"
TEMPORAL LOGIC POINT PROCESS,0.11909650924024641,"Note that −ℓis convex w.r.t w, b0 and b1. This is true as the intensity
function has a functional form of Eq. (4), and this turns the log-likelihood
function into a generalized linear model (GLM) with Poisson observations
and log link (Fahrmeir & Tutz, 2013). It is well-known that the negative
log-likelihood of a GLM is convex w.r.t the model parameters. This
convexity property leads to a convergence guarantee for TELLER.
TELLER uses TLPP as the backbone to evaluate the likelihood of the
grounded 0-1 temporal predicate sequences. The likelihood will be maxi-
mized to jointly learn the set of temporal logic formulas and their weights.
The problem is essentially combinatorial and requires enumerating an exponentially large set of
combinations of the predicates and their signs. Our problem is even more challenging compared
to traditional inductive logic programming due to the sequential properties of data with temporal
information. TELLER solves this problem using the procedures as shown in Fig. 3. We will start our
exposition with the formulation of the original, or the master rule learning problem."
TEMPORAL LOGIC POINT PROCESS,0.12114989733059549,Published as a conference paper at ICLR 2022
TEMPORAL LOGIC POINT PROCESS,0.12320328542094455,"3.1
TEMPORAL LOGIC RULE LEARNING PROBLEM (MASTER PROBLEM)"
TEMPORAL LOGIC POINT PROCESS,0.12525667351129363,"We aim to uncover the set of temporal logic rules F based via optimizing:
PMaster : w∗, b∗
0, b∗
1 = argmin
w,b0,b1
−ℓ(w, b0, b1) + λ0
X"
TEMPORAL LOGIC POINT PROCESS,0.1273100616016427,"f∈¯
F cfwf;
s.t.
wf ≥0,
f ∈¯F
(6)"
TEMPORAL LOGIC POINT PROCESS,0.1293634496919918,"where ¯F is the complete and exponentially large collection of all possible temporal logic formulas
that can be created by the set of the pre-specified temporal predicates and their temporal relations.
The algorithm needs to determine: 1) the assignment Xf, X +
f , and X −
f ; 2) the important temporal
relations defined over the involved predicates in f; and 3) the weight parameter wf as in (4). The
regularization penalties related to the rule complexity are incorporated into the objective to force
sparsity and to trade off accuracy against rule simplicity. Rule complexity coefficient cf can be the
rule length if we want to learn both the sparse and short rules. Any affine function e.g. λ0
P"
TEMPORAL LOGIC POINT PROCESS,0.13141683778234087,"f∈¯
F cfwf
with cf ≥0, λ0 ≥0 should work here and will not change the convexity. One can further tune λ0 to
balance the negative likelihood and the complexity penalty via cross-validation. The sparsity of w∗
will indicate the collection of rules learned and these non-zero weight rules will compose F.
Key Idea: Although we can write the rule learning problem as a regularized convex optimization
above, the set of variables (wf) is exponentially large and can not be optimized simultaneously in a
tractable way. The idea of TELLER is to start from a restricted master problem (RMP), where the
search space is smaller and the solution is tractable but suboptimal. TELLER will gradually improve
this solution by iteratively expanding the search space until the solution approximates the optimum.
The idea is inspired by the fact that most of the candidate rules in ¯F will not be in F and will lead to
a zero value weight in the optimal solution. We do not want to waste resources by including these
redundant rules when we expand the search space to optimize (6). TELLER will always generate the
rule that has the potential to improve the objective function, which is an any-time algorithm with the
guarantee that the most important rules have been considered.
Our progressive algorithm gives rise to two questions: 1) the criterion to determine the new rule to
add; 2) How to construct new candidate temporal logic rules, as will be tackled in the following."
CRITERION TO ADD RULES,0.13347022587268995,"3.2
CRITERION TO ADD RULES"
CRITERION TO ADD RULES,0.13552361396303902,"Restricted master problem (RMP). TELLER first replaces the original rule set ¯F by a small subset
F0 ⊂¯F and gradually expands this subset to produce a nested sequence of subsets F0 ⊂F1 ⊂
· · · ⊂Fk ⊂· · · . It does so by adding candidate rules identified by subproblems (we will elaborate
on this later). Note that the initial rule set F0 can be an empty set or any pre-defined small set.
For each Fk, k = 0, 1, . . . , TELLER solves the restricted master problem by replacing ¯F with Fk:
PRestricted : w∗
(k), b∗
0,(k), b∗
1,(k) = argmin
w,b0,b1
−ℓ(w, b0, b1) +
X"
CRITERION TO ADD RULES,0.1375770020533881,"f∈Fk
cfwf; s.t. wf ≥0, f ∈Fk. (7)"
CRITERION TO ADD RULES,0.13963039014373715,"This can be regarded as the rule evaluation stage, where all rules in the current set will be
reweighed.
Note that an optimal solution to the restricted master problem above can be ex-
tended to a feasible solution to the original problem by setting the weights of all missing
f ∈¯F \ Fk to zero. The optimality of this extended solution can be verified by leveraging the
convexity property of the original problem (6). Let the Lagrangian of the original problem be
L(w, b0, b1, ν) = −ℓ(w, b0, b1) + P"
CRITERION TO ADD RULES,0.14168377823408623,"f∈¯
F cfwf −P"
CRITERION TO ADD RULES,0.1437371663244353,"f∈¯
F νfwf, where νf ≥0 is the Lagrange
multiplier associated with the non-negativity constraints of wf. Being a convex problem, strong
duality holds under mild conditions. Suppose w∗, b∗
0, b∗
1 is primal optimal, and ν∗is dual optimal,
then −ℓ(w∗, b∗
0, b∗
1) = infw,b0,b1 L(w, b0, b1, ν∗). This implies the complementary slackness, which
will lead to the following condition.
Evaluate current dual price. Obtain the dual price for each constraint in the original problem (6):"
CRITERION TO ADD RULES,0.1457905544147844,"νf,(k) = −∂ℓ(w, b0, b1) ∂wf"
CRITERION TO ADD RULES,0.14784394250513347,"w∗
(k),b∗
0,(k),b∗
1,(k)
+ cf,
(8)"
CRITERION TO ADD RULES,0.14989733059548255,"where subscript (k) means the price depends on the current optimal solution of the RMP, and subscript
f refers to the constraint wf ≥0. The optimality condition says: if the extended solution of w∗
(k) is
optimal to the original problem, it must satisfy the following condition:
w∗
f,(k) > 0 ⇒νf,(k) = 0;
w∗
f,(k) = 0 ⇒νf,(k) ≥0
∀f ∈¯F,
(9)
where νf,(k) is computed via (8). For the missing rules, we have wf,(k) = 0 automatically, and if
they are the optimal solution to the original problem, the computed νf,(k) ≥0 must hold. A more
detailed description of complementary slackness and optimality condition is given in Appendix D."
CRITERION TO ADD RULES,0.15195071868583163,Published as a conference paper at ICLR 2022
CRITERION TO ADD RULES,0.1540041067761807,"Therefore, the rule adding criterion is to search over the missing rules and find the rule that most
violates the condition, i.e., leads to the most negative νf by (8). Adding it to Fk+1 will most reduce
the objective value. Assuming the computed minimal νf ≥0, it is guaranteed that the current
solution to the restricted problem is also optimal to the original one. A subproblem is constructed
and optimized to identify a new rule, as will be discussed next."
PROPOSE A NEW TEMPORAL LOGIC RULE,0.15605749486652978,"3.3
PROPOSE A NEW TEMPORAL LOGIC RULE
Construct a subproblem from the current dual price. A subproblem is formulated to propose a
new (or missing) temporal logic rule which can potentially improve the optimal value of the RMP,
i.e. having a negative increased gain. The increased gain of a missing rule is defined as the possible
change in the objective per unit when it is included in the rule set, and it is computed by taking
the partial derivative of the objective function w.r.t the rule weight. Note the subproblem itself is a
minimization, to find the most negative increased gain.
Given the solution w∗
(k), b∗
0,(k), b∗
1,(k) for the restricted master problem (7), a subproblem is con-
structed by taking the partial derivative w.r.t the weights. The log likelihood can be regarded as a
function of the dual conditional intensities: ℓ(λ∗, µ∗). By the chain rule, the subproblem optimizes:"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.15811088295687886,"min
ϕf ∈Φ ν(ϕf|w∗
(k), b∗
0,(k), b∗
1,(k)) = −
∂ℓ(λ∗, µ∗)"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.1601642710472279,"∂λ∗
∂λ∗"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.162217659137577,"∂wf + ∂ℓ(λ∗, µ∗)"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.16427104722792607,"∂µ∗
∂µ∗ ∂wf"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.16632443531827515,"
w∗
(k),b∗
0,(k),b∗
1,(k)
+ cf, (10)"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.16837782340862423,"where ϕf ∈Φ is the rule-based feature, λ∗and µ∗are functions of ϕf, and the conditional intensities
are time and history-dependent (see Eq. (4)). In our formulation, this subproblem objective happens
to be the current dual price. The objective can be hard to express in a closed-form but will be
easily evaluated numerically. Minimizing over the feature space Φ is essentially searching through a
combinatorial space of the temporal predicates and their temporal relations to determine a rule f, so
that its yielding feature will minimize the objective function.
Solve subproblem to identify a new rule. As discussed above, if the optimal value of the subproblem
is negative, we have identified a feature (i.e., rule) to enter the rule set to construct Fk+1; if the
optimal value of the above problem is non-negative, we have proven that the current solution to the
RMP is also optimal to the original problem. Note that the subproblem would not generate the same
feature or rule more than once, since the optimal Lagrangian multipliers in the RMP are non-negative.
However, explicitly solving subproblem (10) requires enumerating all possible conjunctions of the
input predicates, their signs, and all possible pairwise temporal relations. The construction of feature
ϕf requires evaluating all valid combinations of predicate events from history.
Speedup subproblem. To make the subproblem tractable, we adopt a couple of principles. 1)
Theoretically, the optimality of the subproblem can be sacrificed. In fact, any solution to (10) that has
a negative objective will generate a rule with negative increased gain and can be added to the rule set.
This provides a performance guarantee for speedup. 2) Practically, prior knowledge in rule structures
can be leveraged. Our prior knowledge has 3 parts: sparsity, heredity, and expert’s preference in rule
templates. sparsity means the uncovered rule set is small and the rule lengths are generally short. In
our algorithm, the maximal rule length is pre-specified as H. It means we do not need to consider
rules with lengths exceeding H. By heredity, we mean if the conjunction of the input predicates is
important, at least one of the involved input predicates will show some significance. Heredity justifies
that we can gradually grow rules from the existing short segments in the current set.
Search algorithms. With the prior knowledge above, we can design search algorithms in a way
like depth-first search (DFS) or breadth-first search (BFS). In our setting, they are named as rule-
extension-first-search (REFS) and rule-addition-first-search (RAFS), respectively. REFS will always
extend the existing rules until the length reaches H, while RAFS will first add all possible rules with
length 1, and then consider the rules with length 2 and so on. Here we describe details of REFS,
and leave RAFS for Appendix F. In Fig. 4, a REFS is used to construct a new temporal logic rule.
Specifically, it starts by constructing a rule with length one (i.e., one body predicate). This is achieved
by scanning the predicate set X and temporal relation set R, then enumerating the signs and possible
temporal relations with the head predicate. The subproblem objective is used to score these segments
and select the most negative one. The process continues to extend current segments in the same way,
until the rule length reaches H, or the minimal subproblem objective becomes positive. In this way,
the complexity of our subproblem is O(d), where d is the number of predicates. Once new rules
are added to the rule set, the model will be re-fitted and the model parameters will be re-weighted
according to Eq. (7). Longer rules might outweigh their short segments after the rule evaluation stage
and thus the length of an important rule can be determined."
PROPOSE A NEW TEMPORAL LOGIC RULE,0.1704312114989733,Published as a conference paper at ICLR 2022
PROPOSE A NEW TEMPORAL LOGIC RULE,0.17248459958932238,"In addition, we can use expert’s preference in rule templates to solve the subproblems. Human experts
provide preferred rule templates (rather than a rule set), which reduce the rule searching space, since
it masks out many irrelevant predicates and their combinations. For example, doctors are interested
in how a treatment affects the evolution of patient health status. To reflect the doctor’s conjectures,
the rule template can be defined as “symptoms ←drugs ∧symptoms”.
3.4
DISCUSSION ON SCALABILITY OF THE ALGORITHM"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.17453798767967146,"Temporal 
Relation Set:
!""#$%:
&""""':
()*+,:"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.17659137577002054,
PROPOSE A NEW TEMPORAL LOGIC RULE,0.17864476386036962,"Subproblem: Search New Rule via Rule Extension First Search (REFS)
1.1"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.1806981519507187,Master Problem: Update Rule Weights by MLE *M
PROPOSE A NEW TEMPORAL LOGIC RULE,0.18275154004106775,∗+ = exp /O + + 1 PQR S
PROPOSE A NEW TEMPORAL LOGIC RULE,0.18480492813141683,"2P 3 4P , 5M"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.1868583162217659,∗+ = exp /O + + 1 PQR S
PROPOSE A NEW TEMPORAL LOGIC RULE,0.188911704312115,2P 3 4P
PROPOSE A NEW TEMPORAL LOGIC RULE,0.19096509240246407,Learned
PROPOSE A NEW TEMPORAL LOGIC RULE,0.19301848049281314,"rules
New rule
Rule set"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.19507186858316222,"!0, !1, … , !230
G"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.1971252566735113,"2R, 2V, … , 2S3R
$"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.19917864476386038,"!2
2S = &"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.20123203285420946,"$∗, /O"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.2032854209445585,"∗, /R∗= argmin"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.2053388090349076,"B,T#,T$"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.20739219712525667,"−=CH=>?@A>ℎCCD $, /O, /R + 1 U∈ℱ"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.20944558521560575,EU2U ;
PROPOSE A NEW TEMPORAL LOGIC RULE,0.21149897330595482,"Sub Gradient:
4!, ¬ 4!, 4"", ¬4""
4#, ¬4#, 4$, ¬4$"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.2135523613963039,"…
8, ¬8 ''9 '( '
' '( REFS"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.21560574948665298,New rule !!:
PROPOSE A NEW TEMPORAL LOGIC RULE,0.21765913757700206,"8 ←4""⋀¬4$⋀4!⋀(4"" !""#$%"" 8) ⋀(¬4$ -.""%,+/ 4"")⋀(¬4$ !""#$%"" 4"")⋀(4! ()*+,¬4$)"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.21971252566735114,Intensity Function Calculation
PROPOSE A NEW TEMPORAL LOGIC RULE,0.22176591375770022,"tim
e
0
0
1
1
0
! ""
!""#
Event of 
Interest"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.22381930184804927,"Evaluate "" by MLE … …"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.22587268993839835,"REFS
!""#$%"""
PROPOSE A NEW TEMPORAL LOGIC RULE,0.22792607802874743,"¬'>
'9"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.2299794661190965,"-.""%,+/"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.23203285420944558,"!""#$%""
!""#$%"" ''? ()*+, REFS '
' '( '9"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.23408624229979466,"-.""%,+/"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.23613963039014374,"!""#$%""
!""#$%"" ¬'> 1.2 2"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.23819301848049282,"-.""%,+/: …"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.2402464065708419,"Inner Loop
Outer Loop"
PROPOSE A NEW TEMPORAL LOGIC RULE,0.24229979466119098,"Figure 4: Overall flow of TELLER. It alternates be-
tween rule evaluation stage and rule proposal stage. The
logic rules and their weights are jointly learned."
PROPOSE A NEW TEMPORAL LOGIC RULE,0.24435318275154005,"The previous discussion provides a theoretical
framework to find an (near-) optimal temporal
logic rule set based on the point process model.
For large problems with thousands of event se-
quences and hundreds of predicates, this type of
progressive algorithm tends to be hard to termi-
nate, and obtaining the optimality condition is al-
most impossible; this is the so-called tailing-off
effect (Lübbecke & Desrosiers, 2005; Savels-
bergh, 2002). Therefore, we need other tech-
niques to make the TELLER scale with the data
and the predicates. For example, we may con-
dition an early termination to the algorithm if
a fixed time limit is exceeded. In practice, set-
ting the time limit is proven to work well and
the most important rules are likely to be discov-
ered. For large problems, we may also choose to
take a random subset of sequences and a random
subset of predicates for evaluation. When solving for the RMP, we can use a stochastic gradient
descent-type of optimization to approximate the gradient using a batch of sequences. By doing so,
we enable the RMP to scale with the data (sequences). Furthermore, we can warm-start the model
parameters for consecutive RMPs, i.e. we initialize w(k+1), b0,(k+1), b1,(k+1) using the optimal
solution w∗
(k), b∗
0,(k), b∗
1,(k). When solving the subproblems, besides the efficient search scheme, we
may also choose to take a subset of the predicates in searching for further speedup.
When the domain knowledge is not accessible, a rule of thumb to initialize F0 is to start with an
""empty"" set, which refers to there being only a base term in the intensity function, i.e., λ∗(t) =
exp (b0) and the initial estimate can be learned by MLE (i.e., restricted master problem)."
EXPERIMENTS,0.2464065708418891,"4
EXPERIMENTS
We evaluate TELLER on synthetic and real event data. For real data, we consider healthcare treatment
understanding and crime pattern learning (crime results are in Appendix H & I).
4.1
BASELINES
We considered the following SOTA baselines: 1) Recurrent Marked Temporal Point Processes
(RMTPP) (Du et al., 2016b), the first neural point process (NPP) model, where the intensity function
is modeled by a Recurrent Neural Network (RNN); 2) Neural Hawkes Process (NHP) (Mei & Eisner,
2016), an improved variant of RMTPP by constructing a continuous-time LSTM; 3) Transformer
Hawkes Process (THP) (Zuo et al., 2020), an NPP model with a self-attention mechanism; 4)
Tree-Regularized GRU (TR-GRU) (Wu et al., 2018a), a deep time-series model with a designed
tree regularizer to add model interpretability; 5) Recurrent Point Process Network (RPPN) (Xiao
et al., 2019), where the intensity is modeled by two interleaved RNNs with attention mechanism;
6) CAUSE (Zhang et al., 2020b), an NPP model with Granger causality statistic learned. We also
considered other widely used parametric/nonparametric TPPs: 1) Hawkes Process with an exponential
kernel (HExp) (Lewis & Mohler, 2011); 8) Inhomogeneous Poisson Process (IPP), where the intensity
is a sum of k weighted Gaussian kernels: ˆλ(t) = Pk
i=1 αi
 
2πσ2
i
−1/2 exp
 
−(t −ci)2 /σ2
i
, where ci
and σi are fixed center and standard deviation, and αi is the weight for kernel i."
EXPERIMENTS,0.24845995893223818,Table 1: Model properties and interpretability comparison of all baselines and TELLER
EXPERIMENTS,0.25051334702258726,"Model
RMTPP
NHP
THP
TR-GRU
RPPN
CAUSE
HExp
IPP
TELLER
Interpretable
✓
✓
✓
✓
✓
Flexible
✓
✓
✓
✓
✓
✓
✓
Parsimonious
✓
✓
✓
Pairwise interaction
✓
✓
✓
Higher-order interaction
✓
In Tab. 1, we compared TELLER with these baselines by the model properties: 1) Interpretable:
model parameters and prediction are understandable to humans; 2) Flexible: model is expressive to"
EXPERIMENTS,0.25256673511293637,Published as a conference paper at ICLR 2022 3 Truth
EXPERIMENTS,0.2546201232032854,Learned
EXPERIMENTS,0.25667351129363447,"Dataset-1
 Jaccard = 1.00 1
3 Truth"
EXPERIMENTS,0.2587268993839836,Learned
EXPERIMENTS,0.26078028747433263,"Dataset-2
 Jaccard = 0.75 3 Truth"
EXPERIMENTS,0.26283367556468173,Learned
EXPERIMENTS,0.2648870636550308,"Dataset-3
 Jaccard = 1.00 2
3 Truth"
EXPERIMENTS,0.2669404517453799,Learned
EXPERIMENTS,0.26899383983572894,"Dataset-4
 Jaccard = 0.60 1
4 Truth"
EXPERIMENTS,0.27104722792607805,Learned
EXPERIMENTS,0.2731006160164271,"Dataset-5
 Jaccard = 0.80 1
3 Truth"
EXPERIMENTS,0.2751540041067762,Learned
EXPERIMENTS,0.27720739219712526,"Dataset-6
 Jaccard = 0.75"
EXPERIMENTS,0.2792607802874743,"w0
w1
w2
Rule Weight 0 1 2 Value"
EXPERIMENTS,0.2813141683778234,MAE = 0.16
EXPERIMENTS,0.28336755646817247,"Truth
Learned"
EXPERIMENTS,0.28542094455852157,w0 w1 w2 w3
EXPERIMENTS,0.2874743326488706,Rule Weight Value
EXPERIMENTS,0.28952772073921973,MAE = 0.11
EXPERIMENTS,0.2915811088295688,"Truth
Learned"
EXPERIMENTS,0.2936344969199179,"w0
w1
w2
Rule Weight Value"
EXPERIMENTS,0.29568788501026694,MAE = 0.07
EXPERIMENTS,0.29774127310061604,"Truth
Learned"
EXPERIMENTS,0.2997946611909651,w0 w1 w2 w3 w4
EXPERIMENTS,0.30184804928131415,Rule Weight Value
EXPERIMENTS,0.30390143737166325,MAE = 0.32
EXPERIMENTS,0.3059548254620123,"Truth
Learned"
EXPERIMENTS,0.3080082135523614,w0 w1 w2 w3 w4
EXPERIMENTS,0.31006160164271046,Rule Weight Value
EXPERIMENTS,0.31211498973305957,MAE = 0.08
EXPERIMENTS,0.3141683778234086,"Truth
Learned"
EXPERIMENTS,0.3162217659137577,w0 w1 w2 w3
EXPERIMENTS,0.3182751540041068,Rule Weight Value
EXPERIMENTS,0.3203285420944558,MAE = 0.07
EXPERIMENTS,0.32238193018480493,"Truth
Learned"
EXPERIMENTS,0.324435318275154,"Figure 5: Rule discovery and weight learning results of TELLER-REFS on 6 synthetic datasets (2.4K seqs).
capture sophisticated non-linear dependencies; 3) Parsimonious: model parameters are of small or
medium size; 4) Pairwise interaction: can identify the mutual influence of the events; 5) Higher-order
interaction: can identify the composite effect of a subset of events to another event."
SYNTHETIC DATA,0.3264887063655031,"4.2
SYNTHETIC DATA"
SYNTHETIC DATA,0.32854209445585214,"We verify TELLER’s rule discovery ability on synthetic datasets with ground truth. The syn-
thetic events are generated from TLPPs with a known set of rules and weights. We prepared 12
synthetic datasets with various rule lengths, weights, and temporal relations. See Appendix G
for details. Both the REFS and RAFS subproblem search schemes are tested on all 12 datasets.
Table 2: Learned Rules on Synthetic Dataset1."
SYNTHETIC DATA,0.33059548254620125,"True Rules (# 3) v.s. Learned Rules (REFS)
2400 Sequences:
E ←A ∧(A Before E)
E ←B ∧C ∧(B Before E) ∧(C Before E)
E ←C ∧D ∧(C Before D) ∧(D Equal E)
600 Sequences:
E ←A ∧(A Before E)
E ←A ∧D ∧(A Equal D) ∧(A Before E)
E ←A ∧¬B ∧(A Before E) ∧(¬B Before A)
E ←B ∧C ∧(B Before E) ∧(C Before E)
E ←B ∧C ∧(B Before E) ∧(C Before B)
E ←A ∧C ∧(A Before E) ∧(C Before E)
E ←C ∧A ∧(A Equal C) ∧(A Before E)
E ←A ∧¬C ∧(A Before ¬C) ∧(¬C Before E)"
SYNTHETIC DATA,0.3326488706365503,"Consistent results show an accurate performance of our
algorithm in terms of both the rule discovery and param-
eter learning. We list all the discovered rules based on
dataset-1 using 2400 and 600 sequences respectively in
Tab. 2, where the correct rules are marked in blue. For
dataset-1, the body predicate set is {A,B,C,D}, and the
head predicate is E. Given 600 sequences, TELLER
successfully discovered 2 out of 3 truth rules along
with several noise rules; whereas given 2400 sequences,
TELLER accurately uncovered all 3 ground truth rules
without any noise rules. The results indicate TELLER is
capable of mining rules from noisy event data, and that performance improves with more samples.
More datasests’ results are reported in Fig. 5, where 6 synthetic datasets with 2400 sequences are used
for evaluation. Each plot in the top row uses a Venn diagram to show the true rule set and the learned
rule set, from which the Jaccard similarity score (area of the intersection divided by the area of their
union) is calculated. TELLER discovered almost all the true rules. Each plot in the bottom compares
the true rule weights with the learned rule weights, with the Mean Absolute Error (MAE) reported.
Almost all truth rule weights are accurately learned and the included noise rule weights are relatively
small. In dataset-4, we crafted a long and complex rule with 3 body predicates to challenge TELLER.
It is still able to correctly discover this rule, while the weight is slightly underestimated. The existence
of long logic rules will require more data and stronger signals (i.e., higher rule weights) to have an
accurate recovery. More information about the experiment settings can be found in Appendix G."
SYNTHETIC DATA,0.3347022587268994,"0
2000
4000
6000
8000
Run time (second) 610 605 600 595 590 585"
SYNTHETIC DATA,0.33675564681724846,Log-likelihood
SYNTHETIC DATA,0.33880903490759756,"A-->E
A Before E"
SYNTHETIC DATA,0.3408624229979466,"B^C-->E
B Before E
C Before E"
SYNTHETIC DATA,0.34291581108829566,"C^D-->E
C Before E
D Equal E"
SYNTHETIC DATA,0.34496919917864477,TELLER
SYNTHETIC DATA,0.3470225872689938,Full Model
SYNTHETIC DATA,0.3490759753593429,Figure 6: Training process.
SYNTHETIC DATA,0.351129363449692,"We also compared TELLER with the brute-force method, where
we enumerated all possible temporal logic rules and built a full
model to learn the rule weights. We showed the training process
for dataset-1 in Fig. 6, where the evolving likelihood versus the
run time is displayed. For TELLER, we also marked the time point
when the true rule is discovered (gray dashed vertical lines). The
key observations are: 1) For TELLER, the likelihood increases
very fast and we can see evident jumps whenever a true rule is discovered. As a comparison, the
likelihood for the brute-force method increases slowly. 2) All the true rules can be discovered
efficiently by TELLER and this verifies that we can set a time limit for early stopping. We can stop
TELLER when the objective function seems to start centering around some level. This experiment
verifies that our method is computationally efficient compared to the brute-force methods.
4.3
REAL DATA: TREATMENT ON MIMIC-III
MIMIC-III contains electronic health records of patients admitted to the intensive care unit
(ICU) (Johnson et al., 2016). We focused on patients diagnosed with sepsis (Saria, 2018; Raghu et al.,
2017; Peng et al., 2018), as one of the major causes of mortality in ICU. Evidence suggests that the
treatment strategy remains uncertain – it is unclear how to use intravenous fluids and vasopressors
to support the circulatory system. There also exists clinical controversy about when and how to
use these two groups of drugs to reduce the side effect. TELLER is aimed to learn the explanatory
temporal logic rules regarding these two groups of drugs as well as their importance weights."
SYNTHETIC DATA,0.3531827515400411,Published as a conference paper at ICLR 2022
SYNTHETIC DATA,0.35523613963039014,"Predicates and dataset statistics. We define 62 predicates, including two groups of drugs (i.e.,
intravenous fluids and vasopressors) and lab measurements. The variables involved are similar
to (Saria, 2018). We define two head predicates: 1) LowUrine and 2) Survival. We treat real-
time urine as a head predicate since low urine is the direct indicator of bad circulatory systems and is
an important signal for septic shock. A complete table of predicates can be found in Appendix H. In
our experiment, lab measurement variables are converted to binary values (according to the normal
range used in medicine) with the transition times recorded. For drug predicates, they are recorded
as 1 when they were applied to the patient. We extracted 4298 patient sequences, and randomly
chose 80% of them for training and the remaining for testing. The average time horizon is 392.69
hours and the average #events per sequence is 79.03. Detailed setup can be found in Appendix H.2.
Table
3:
MIMIC-III:
Event prediction results."
SYNTHETIC DATA,0.35728952772073924,"Method
LowUrine
(MAE)
Survival
(ACC)"
SYNTHETIC DATA,0.3593429158110883,"RMTPP
1.983
0.796
NHP
1.684
0.802
THP
1.545
0.824
TR-GRU
2.666
0.811
RPPN
1.640
0.767
CAUSE
1.712
0.740
HExp
2.578
0.882
IPP
2.472
0.784
TELLER
1.266
0.930"
SYNTHETIC DATA,0.3613963039014374,"Evaluation metrics and results. We use two evaluation metrics: 1)
LowUrine is evaluated by predicting the time of transitions from state 0
to state 1 , for which the performance is measured by Mean Absolute Error
(MAE). 2) Survival is evaluated by predicting whether the patient is
survived at the end of treatment, for which the performance is measured
by Accuracy(ACC). Comparison results are in Tab. 3.
Discovered temporal logic rules. Tab. 4 shows the uncovered explanatory
temporal logic rules and weights learned by TELLER. We use LowUrine
as the head predicate and NormalUrine as its negation. For each discov-
ered rule, the sign of the head predicate will be automatically determined
by the algorithm. Human experts confirmed that these discovered logic rules are consistent with
the pathogenesis of sepsis, and have captured the most important factors in affecting real-time urine
output and survival. Experts found that Rule 1 to Rule 4 capture the major lab measurements that
usually emerge together with extremely low urine. Rule 5 seemed hard to interpret in the beginning
by experts, since they thought it was slightly counter-intuitive. They later concluded that, in some
extreme cases, even after using Crystalloid ( intravenous fluids) as the treatment, the urine can remain
low, and thus Rule 5 is still likely to be observed. Rules 6-9 shed light on drug selection. For example,
in Rule 7, using Crystalloid and Phenylephrine (vasopressors) together will play an evident role in
triggering the urine to a normal level. In Rules 6, 8, and 9, using Crystalloid yields a weight of 4.14,
using Colloid (a type of intravenous fluids) yields a weight of 1.97, and using the combinations of the
two yields a weight of 1.83, which can be regarded as an incremental contribution.
Table 4: Learned Rules with LowUrine as the head predicate."
SYNTHETIC DATA,0.36344969199178645,"Weight
Rule
1.07
Rule 1:LowUrine ←LowSysBP ∧(LowSysBP Before LowUrine)
0.89
Rule 2:LowUrine ←LowSodim ∧(LowSodim Before LowUrine)"
SYNTHETIC DATA,0.3655030800821355,"1.16
Rule 3:LowUrine ←HighCreatinine
∧Equal(HighCreatinine, LowUrine)
1.98
Rule 4:LowUrine ←HighBUN ∧(HighBUN Before LowUrine) 1.10"
SYNTHETIC DATA,0.3675564681724846,"Rule 5:LowUrine←HighBUN ∧Crystalloid
∧(HighBUN Before LowUrine)
∧(Crystalloid Before LowUrine) 4.14"
SYNTHETIC DATA,0.36960985626283366,"Rule 6:NormalUrine ←LowUrine ∧Crystalloid
∧(LowUrine Equal NormalUrine)
∧(Crystalloid Equal NormalUrine) 3.13"
SYNTHETIC DATA,0.37166324435318276,"Rule 7:NormalUrine ←LowUrine ∧Phenylephrine
∧(LowUrine Equal NormalUrine)
∧(Phenylephrine Equal NormalUrine) 1.83"
SYNTHETIC DATA,0.3737166324435318,"Rule 8:NormalUrine ←LowUrine ∧Crystalloid ∧Colloid
∧(Colloid Equal NormalUrine)
∧(Crystalloid Equal NormalUrine)
∧(LowUrine Equal NormalUrine) 1.97"
SYNTHETIC DATA,0.3757700205338809,"Rule 9:NormalUrine ←LowUrine ∧Colloid
∧(LowUrine Equal NormalUrine)
∧(Colloid Equal NormalUrine)"
SYNTHETIC DATA,0.37782340862423,"Expert-Model interaction. Experts are
involved in TELLER’s learning loop: 1)
Experts can provide a predicate set, their
interested head predicates, an initial rule
set, and desired rule templates to the
model; 2) Run TELLER to obtain pre-
liminary results for experts to check. Ex-
perts can delete unreasonable or noise
rules and give the refined rule set to the
model before continuing learning. Ex-
perts provided advice for predicates def-
inition and the desired rule templates.
We ran TELLER from an empty set and
generated 12 rules. Experts verified the
rules’ correctness and deleted the following ones: LowUrine ←HighSpO2; LowUrine ←
NormalSodium; LowUrine ←NormalSodium ∧Water. We were informed that the first
rule is contradictory with Pathophysiology common sense, while the other two rules are correct yet
irrelevant to the treatment strategy. Then TELLER is re-implemented to refine the result, as in Tab. 4."
CONCLUSION,0.3798767967145791,"5
CONCLUSION
In this paper, we have proposed a new algorithm TELLER to learn interpretable temporal logic rules
to explain point processes, with promising results on both synthetic and real-world data. To our best
knowledge, this is the first work that automates the discovery of the temporal logic rules based on
point process models. Our method adds transparency and interpretability to event models."
CONCLUSION,0.38193018480492813,Published as a conference paper at ICLR 2022
CONCLUSION,0.3839835728952772,ACKNOWLEDGMENTS
CONCLUSION,0.3860369609856263,"This work was in part supported by Shanghai Municipal Science and Technology Major Project
(2021SHZDZX0102), Neil Shen’s SJTU Medical Research Fund and NSFC (61972250, U19B2035).
Shuang Li’s research was in part supported by the Start-up Fund UDF01002191 of The Chinese
University of Hong Kong, Shenzhen, Shenzhen Institute of Artificial Intelligence and Robotics for
Society, and Shenzhen Science and Technology Program JCYJ20210324120011032"
REFERENCES,0.38809034907597534,REFERENCES
REFERENCES,0.39014373716632444,"James F Allen. Maintaining knowledge about temporal intervals. In Readings in qualitative reasoning
about physical systems, pp. 361–372. Elsevier, 1990."
REFERENCES,0.3921971252566735,"Cynthia Barnhart, Ellis L Johnson, George L Nemhauser, Martin WP Savelsbergh, and Pamela H
Vance. Branch-and-price: Column generation for solving huge integer programs. Operations
research, 46(3):316–329, 1998."
REFERENCES,0.3942505133470226,"William Brendel, Alan Fern, and Sinisa Todorovic. Probabilistic event logic for interval-based event
recognition. In CVPR 2011, pp. 3329–3336. IEEE, 2011."
REFERENCES,0.39630390143737165,"William W Cohen. Tensorlog: A differentiable deductive database. arXiv preprint arXiv:1605.06523,
2016."
REFERENCES,0.39835728952772076,"Sanjeeb Dash, Oktay Gunluk, and Dennis Wei. Boolean decision rules via column generation.
Advances in Neural Information Processing Systems, 31:4655–4665, 2018."
REFERENCES,0.4004106776180698,"N. Du, H. Dai, R. Trivedi, U. Upadhyay, M. Gomez-Rodriguez, and L. Song. Recurrent marked
temporal point processes: Embedding event history to vector. In KDD, 2016a."
REFERENCES,0.4024640657084189,"Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song.
Recurrent marked temporal point processes: Embedding event history to vector. In Proceedings of
the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp.
1555–1564, 2016b."
REFERENCES,0.40451745379876797,"Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal of
Artificial Intelligence Research, 61:1–64, 2018."
REFERENCES,0.406570841889117,"Ludwig Fahrmeir and Gerhard Tutz. Multivariate statistical modelling based on generalized linear
models. Springer Science & Business Media, 2013."
REFERENCES,0.4086242299794661,"Bernd Finkbeiner, Christopher Hahn, Markus N Rabe, and Frederik Schmitt. Teaching temporal
logics to neural networks. arXiv preprint arXiv:2003.04218, 2020."
REFERENCES,0.4106776180698152,"Valentin Goranko and Antje Rumberg. Temporal Logic. In Edward N. Zalta (ed.), The Stanford
Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Fall 2021 edition,
2021."
REFERENCES,0.4127310061601643,"A. Hawkes. Point spectra of some mutually exciting point processes. Journal of the Royal Statistical
Society. Series B (Methodological), pp. 438–443, 1971a."
REFERENCES,0.41478439425051333,"Alan G Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 58
(1):83–90, 1971b."
REFERENCES,0.41683778234086244,"V. Isham and M. Westcott. A self-correcting point process. Stochastic Processes and Their Applica-
tions, 8(3):335–347, 1979."
REFERENCES,0.4188911704312115,"Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-Wei, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a
freely accessible critical care database. Scientific data, 3(1):1–9, 2016."
REFERENCES,0.4209445585215606,"E. Lewis and E. Mohler. A nonparametric em algorithm for multiscale hawkes processes. Journal of
Nonparametric Statistics, 2011."
REFERENCES,0.42299794661190965,Published as a conference paper at ICLR 2022
REFERENCES,0.42505133470225875,"Shuang Li, Lu Wang, Ruizhi Zhang, Xiaofu Chang, Xuqin Liu, Yao Xie, Yuan Qi, and Le Song.
Temporal logic point processes. In International Conference on Machine Learning, pp. 5990–6000.
PMLR, 2020."
REFERENCES,0.4271047227926078,"Xin Liu, Junchi Yan, Shuai Xiao, Xiangfeng Wang, Hongyuan Zha, and Stephen Chu. On predic-
tive patent valuation: Forecasting patent citations and their types. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 31, 2017."
REFERENCES,0.42915811088295686,"Yu-Ying Liu, Shuang Li, Fuxin Li, Le Song, and James M Rehg. Efficient learning of continuous-time
hidden markov models for disease progression. Advances in neural information processing systems,
28:3599, 2015."
REFERENCES,0.43121149897330596,"Marco E Lübbecke and Jacques Desrosiers. Selected topics in column generation. Operations
research, 53(6):1007–1023, 2005."
REFERENCES,0.433264887063655,"Hongyuan Mei and Jason Eisner. The neural hawkes process: A neurally self-modulating multivariate
point process. arXiv preprint arXiv:1612.09328, 2016."
REFERENCES,0.4353182751540041,"Boston
Department
of
Innovation
and
Technology.
Crime
Incident
Reports.
https://data.boston.gov/dataset/crime-incident-reports-august-2015-to-date-source-
new-system, 2015. [Online; accessed 03-May-2021]."
REFERENCES,0.43737166324435317,"Tivadar Papai, Henry Kautz, and Daniel Stefankovic. Slice normalized dynamic markov logic
networks. Advances in Neural Information Processing Systems, 25:1907–1915, 2012."
REFERENCES,0.4394250513347023,"Xuefeng Peng, Yi Ding, David Wihl, Omer Gottesman, Matthieu Komorowski, Li-wei H Lehman,
Andrew Ross, Aldo Faisal, and Finale Doshi-Velez. Improving sepsis treatment strategies by com-
bining deep and kernel-based reinforcement learning. In AMIA Annual Symposium Proceedings,
volume 2018, pp. 887. American Medical Informatics Association, 2018."
REFERENCES,0.4414784394250513,"Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh
Ghassemi. Deep reinforcement learning for sepsis treatment. arXiv preprint arXiv:1711.09602,
2017."
REFERENCES,0.44353182751540043,"Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 62(1-2):
107–136, 2006."
REFERENCES,0.4455852156057495,"Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang. Drum: End-to-end
differentiable rule mining on knowledge graphs. arXiv preprint arXiv:1911.00055, 2019."
REFERENCES,0.44763860369609854,"Suchi Saria. Individualized sepsis treatment using reinforcement learning. Nature medicine, 24(11):
1641–1642, 2018."
REFERENCES,0.44969199178644764,"Martin WP Savelsbergh. Branch-and-price: Integer programming with column generation, bp. 2002."
REFERENCES,0.4517453798767967,"Parag Singla and Pedro M Domingos. Lifted first-order belief propagation. In AAAI, volume 8, pp.
1094–1099, 2008."
REFERENCES,0.4537987679671458,"Son D Tran and Larry S Davis. Event modeling and recognition using markov logic networks. In
European Conference on Computer Vision, pp. 610–623. Springer, 2008."
REFERENCES,0.45585215605749485,"Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and logical
reasoning using a differentiable satisfiability solver. In International Conference on Machine
Learning, pp. 6545–6554. PMLR, 2019a."
REFERENCES,0.45790554414784396,"Po-Wei Wang, Daria Stepanova, Csaba Domokos, and J Zico Kolter. Differentiable learning of
numerical rules in knowledge graphs. In International Conference on Learning Representations,
2019b."
REFERENCES,0.459958932238193,"Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and Perry MacNeille. A
bayesian framework for learning rule sets for interpretable classification. The Journal of Machine
Learning Research, 18(1):2357–2393, 2017."
REFERENCES,0.4620123203285421,"Dennis Wei, Sanjeeb Dash, Tian Gao, and Oktay Gunluk. Generalized linear rule models. In
International Conference on Machine Learning, pp. 6687–6696. PMLR, 2019."
REFERENCES,0.46406570841889117,Published as a conference paper at ICLR 2022
REFERENCES,0.46611909650924027,"Mike Wu, Michael Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, and Finale Doshi-Velez.
Beyond sparsity: Tree regularization of deep models for interpretability. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 32, 2018a."
REFERENCES,0.4681724845995893,"Weichang Wu, Junchi Yan, Xiaokang Yang, and Hongyuan Zha. Decoupled learning for factorial
marked temporal point processes.
In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pp. 2516–2525, 2018b."
REFERENCES,0.4702258726899384,"Shuai Xiao, Junchi Yan, Mehrdad Farajtabar, Le Song, Xiaokang Yang, and Hongyuan Zha. Learning
time series associated event sequences with recurrent point process networks. IEEE transactions
on neural networks and learning systems, 30(10):3124–3136, 2019."
REFERENCES,0.4722792607802875,"Junchi Yan, Shuai Xiao, Changsheng Li, Bo Jin, Xiangfeng Wang, Bin Ke, Xiaokang Yang, and
Hongyuan Zha. Modeling contagious merger and acquisition via point processes with a profile
regression prior. In IJCAI, pp. 2690–2696, 2016."
REFERENCES,0.47433264887063653,"Junchi Yan, Hongteng Xu, and Liangda Li. Modeling and applications for temporal point processes.
In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pp. 3227–3228, 2019."
REFERENCES,0.47638603696098564,"Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge
base reasoning. arXiv preprint arXiv:1702.08367, 2017."
REFERENCES,0.4784394250513347,"Yuan Yang and Le Song.
Learn to explain efficiently via neural logic inductive learning.
In
International Conference on Learning Representations, 2019."
REFERENCES,0.4804928131416838,"Qiang Zhang, Aldo Lipani, Omer Kirnap, and Emine Yilmaz. Self-attentive hawkes process. In
International Conference on Machine Learning, pp. 11183–11193. PMLR, 2020a."
REFERENCES,0.48254620123203285,"Qiang Zhang, Aldo Lipani, and Emine Yilmaz. Learning neural point processes with latent graphs.
In Proceedings of the Web Conference 2021, pp. 1495–1505, 2021."
REFERENCES,0.48459958932238195,"Wei Zhang, Thomas Panum, Somesh Jha, Prasad Chalasani, and David Page. Cause: Learning
granger causality from event sequences using attribution methods. In International Conference on
Machine Learning, pp. 11235–11245. PMLR, 2020b."
REFERENCES,0.486652977412731,"Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and Hongyuan Zha. Transformer hawkes process.
In International Conference on Machine Learning, pp. 11692–11702. PMLR, 2020."
REFERENCES,0.4887063655030801,Published as a conference paper at ICLR 2022
REFERENCES,0.49075975359342916,"A
RELATED WORK
Our work adds the interpretability to temporal point process models and extends the logic learning
method.
Temporal point process models have been an elegant tool for event data learning, either for
future prediction or (quasi-) causality discovery (Yan et al., 2019; 2016; Liu et al., 2017; Wu et al.,
2018b). Traditional parametric models such as Hawkes process (Hawkes, 1971b) are built on simple
assumptions such that past events will boost the occurrence of future events. To capture more
complicated dynamics, Du et al. (2016a) proposed the first neural point process (NPP) model, where
the intensity function is modeled by a Recurrent Neural Network (RNN). Mei & Eisner (2016)
improved RMTPP by constructing a continuous-time RNN. Zuo et al. (2020); Zhang et al. (2020a)
further leveraged the self-attention mechanism to capture the long-term dependencies of events and
meanwhile enhance the computational efficiency. While these neural-based point process models are
flexible and excel at event prediction, they are hard to interpret. To add transparency to the black-box
models, recently Zhang et al. (2021) introduced Granger causality as a latent graph to explain point
processes and the structures are jointly learned via gradient descent. However, Granger causality
is still limited to the mutual triggering patterns of events. To incorporate more sophisticated and
interpretable event patterns Li et al. (2020) proposed a Temporal Logic Point Process (TLPP), which
constructs intensity function by pre-specified temporal logic rules. In fact, such temporal logic rule
guided symbolic reasoning process will require highly expressive models such as Transformer to have
a good approximate to their solutions (Finkbeiner et al., 2020). However, TLPP can not automatically
discover rules and our TELLER overcomes this major drawback.
Probabilistic logic model dates back to Markov Logic Networks (MLN) (Richardson & Domingos,
2006; Singla & Domingos, 2008), where a Markov random field is used to model the first-order
logic rules and their weights will be learned. Enhanced methods (Papai et al., 2012; Tran & Davis,
2008; Brendel et al., 2011) are generally based on probabilistic graphical models, which allow for
partial and noisy observations yet at the cost of intensive computation for inference. TLPP (Li et al.,
2020) and our TELLER simplify their setting with two assumptions, i.e. causal direction and fully
observed states, and then the temporal logic model can be formulated as a continuous-time point
process, which bridges the first-order temporal logic rules to point process.
Logic learning methods include SATNet (Wang et al., 2019a), which transforms rule mining into
an SDP-relaxed MaxSAT problem, and attention-based methods (Yang & Song, 2019). Some
works (Wang et al., 2017; Evans & Grefenstette, 2018; Wei et al., 2019) formulated logic learning as
learning an explanatory binary classifier. For example, Evans & Grefenstette (2018) constructed a
differentiable model that predicts the conditional probability of the outcome label for a ground atom.
Neural-LP (Yang et al., 2017) provided the first fully differentiable rule mining method based on
TensorLog (Cohen, 2016), and Wang et al. (2019b) extended Neural-LP to learn rules with numerical
values via dynamic programming and cumulative sum operations. In addition, DRUM (Sadeghian
et al., 2019) connected learning rule confidence scores with low-rank tensor approximation. Among
all these methods, Dash et al. (2018) and Wei et al. (2019) first introduced the Column Generation
algorithm in logic learning, which is the closest to our learning framework. However, all these
methods are restricted to the static setting and cannot be directly implemented on event sequences.
B
ALLEN’S THIRTEEN TEMPORAL RELATION"
REFERENCES,0.4928131416837782,"Allen’s original paper (Allen, 1990) defined 13 types of temporal relations between two time intervals.
Specifically, define the time intervals for predicate xA and predicate xB as ItA = (tA1, tA2] and
ItB = (tB1, tB2] respectively, where tA1 and tB1 are the transition times that the predicate enters
the state, and and tA2 and tB2 are the time that the predicate leaves the state. The temporal relation,
denoted as RA type B(ItA, ItB), is a logic function defined over the time intervals.
See below Table 5 for an illustration. As shown in this table, the temporal relation can be mathemati-
cally evaluated by a step function
g(s) := 1(s ≥0)
and an indicator function
κ(s) := 1(s = 0).
Considering the inverses of the listed relations plus the symmetric relation “Equal”, there are a total
of 13 relations.
In practice, to tolerate noise, i.e., the imprecisely recorded time information, it makes sense to
introduce softened approximation functions for the step function g(s) and the delta function κ(s) in"
REFERENCES,0.4948665297741273,Published as a conference paper at ICLR 2022
REFERENCES,0.49691991786447637,"Table 5: Interval-based temporal relations.
Temporal Relation
Logic Function RA type B(ItA, ItB)
Illustration"
REFERENCES,0.4989733059548255,"A Before B
g(tB1 −tA2)"
REFERENCES,0.5010266940451745,"!""#
!""%
!$#
!$%"
REFERENCES,0.5030800821355236,"A Meets B
κ(tA2 −tB1)"
REFERENCES,0.5051334702258727,"!""#
!$%
!""# = !$#"
REFERENCES,0.5071868583162218,"A Overlaps B
g(tB1 −tA1) · g(tB1 −tA2) · g(tB2 −tA2)"
REFERENCES,0.5092402464065708,"!""#
!$%
!$#
!""%
A Starts B
κ(tA1 −tB1) · g(tB2 −tA2)"
REFERENCES,0.5112936344969199,"!""#
!$%
= !$# !""%
A Contains B
g(tB1 −tA1) · g(tA2 −tB2)"
REFERENCES,0.5133470225872689,"!""#
!$%
!$#
!""%"
REFERENCES,0.5154004106776181,"A Finished-by B
g(tB1 −tA1) · κ(tA2 −tB2)"
REFERENCES,0.5174537987679672,"!""#
!$%
!$#
= !""%"
REFERENCES,0.5195071868583162,"A Equals B
κ(tA1 −tB1) · κ(tA2 −tB2)"
REFERENCES,0.5215605749486653,"!""#
!$%
= !$#
= !""%
replacement of those used in the definitions of temporal relations in Table 5. Step function g(s) can
be softened as, e.g., a triangular function or a logistic function, i.e.,
g(s) = min(1, max(0, βs + 1 2)),"
REFERENCES,0.5236139630390144,"or g(s) =
1
1 + exp(−βs).
(11)"
REFERENCES,0.5256673511293635,"Delta function κ(s) can be softened as a triangular function or a Laplace density function, i.e.,
κ(s) = max(0, min( s"
REFERENCES,0.5277207392197125,γ2 + 1
REFERENCES,0.5297741273100616,"γ , −s"
REFERENCES,0.5318275154004107,"γ2 + 1 γ )),"
REFERENCES,0.5338809034907598,or κ(s) = exp(−|s|/γ)
REFERENCES,0.5359342915811088,"γ
.
(12)"
REFERENCES,0.5379876796714579,"Decaying parameters β and γ ≥1 can be pre-specified or treated as unknown parameters, which can
be learned from data by maximizing the likelihood. In this paper, we pre-specify these decaying
parameters and make them frozen in the learning process (i.e., both the master and the subproblem)."
REFERENCES,0.5400410677618069,"C
PROOF OF THE LIKELIHOOD"
REFERENCES,0.5420944558521561,"Given a realization of all predicates {H(t)}0<t<T , one can write out the likelihood function in terms
of the intensity function as follows.
For the head predicate Y , denote the dual conditional intensity function as λ∗(t) and µ∗(t). Let
p(tn+1|Htn, y(tn) = 0) and p(tn+1|Htn, y(tn) = 1) be the conditional density function of the next
event time tn+1 given history and y(tn) = 0, y(tn) = 1, respectively. Let F(t|Htn, y(tn) = 0) and
F(t|Htn, y(tn) = 1) be the corresponding cumulative distribution function for any t > tn.
Based on the definition of the conditional transition intensity (or called hazard function), we have"
REFERENCES,0.5441478439425051,"λ∗(t) =
p(t|Htn, y(tn) = 0)
1 −F(t|Ht, y(tn) = 0),"
REFERENCES,0.5462012320328542,"and µ∗(t) =
p(t|Htn, y(tn) = 1)
1 −F(t|Htn, y(tn) = 1).
(13)"
REFERENCES,0.5482546201232033,"From (13), we have"
REFERENCES,0.5503080082135524,λ∗(t) = −d
REFERENCES,0.5523613963039015,"dt log(1 −F(t|Ht, y(tn) = 0)),"
REFERENCES,0.5544147843942505,µ∗(t) = −d
REFERENCES,0.5564681724845996,"dt log(1 −F(t|Ht, y(tn) = 1)).
Integrating both sides, we can get the conditional density and the cumulative distribution function,"
REFERENCES,0.5585215605749486,"p(t|Htn, y(tn) = 0) = λ∗(t) exp

−
Z t"
REFERENCES,0.5605749486652978,"tn
λ∗(s)ds

,"
REFERENCES,0.5626283367556468,"F(t|Htn, y(tn) = 0) = 1 −exp

−
Z t"
REFERENCES,0.5646817248459959,"tn
λ∗(s)ds

,"
REFERENCES,0.5667351129363449,"p(t|Htn, y(tn) = 1) = µ∗(t) exp

−
Z t"
REFERENCES,0.5687885010266941,"tn
µ∗(s)ds

,"
REFERENCES,0.5708418891170431,"F(t|Htn, y(tn) = 1) = 1 −exp

−
Z t"
REFERENCES,0.5728952772073922,"tn
µ∗(s)ds

."
REFERENCES,0.5749486652977412,Published as a conference paper at ICLR 2022
REFERENCES,0.5770020533880903,"Let t0 = 0. Given the transition times (t1, t2, . . . , tn), and suppose y(t0) = 0, y(tn) = 1 and the head
predicate is still in state 1 at time T, the likelihood function can be factorized into all the conditional
densities of each points given all points before it, i.e., the likelihood function is
L = p(t1|Ht0, y(t0) = 0)p(t2|Ht1, y(t1) = 1) · · · p(tn|Htn−1, y(tn−1) = 0)(1 −F(t|Htn, y(tn) = 1)).
Plugging in the conditional density function and the cumulative distribution function, the likelihood
is expressed as,"
REFERENCES,0.5790554414784395,"L
= λ∗(t1) exp

−
R t1
0 λ∗(s)ds

· µ∗(t2) exp

−
R t2
t1 µ∗(s)ds
"
REFERENCES,0.5811088295687885,"· · · λ∗(tn) exp

−
R tn
tn−1 λ∗(s)ds

· exp

−
R t
tn µ∗(s)ds

,
which completes the proof."
REFERENCES,0.5831622176591376,"D
OPTIMALITY CONDITION AND COMPLEMENTARY SLACKNESS"
REFERENCES,0.5852156057494866,"We will provide more descriptions on the optimality condition and the complementary slackness,
which provides a sound guarantee to our learning algorithm.
Given the original restricted convex problem,
PMaster :
w∗, b∗
0, b∗
1 = argmin
w,b0,b1
−ℓ(w, b0, b1) +
X"
REFERENCES,0.5872689938398358,"f∈¯
F cfwf;
s.t.
wf ≥0,
f ∈¯F
(14)"
REFERENCES,0.5893223819301848,"where parameter cf depends on the complexity of rule f, such as the number of predicates involved
in f (i.e., rule length).
The Lagrangian of the original master problem is
L(w, b0, b1, ν) = −ℓ(w, b0, b1) +
X"
REFERENCES,0.5913757700205339,"f∈¯
F
cfwf −
X"
REFERENCES,0.5934291581108829,"f∈¯
F
νfwf,
(15)"
REFERENCES,0.5954825462012321,"where νf ≥0 is the Lagrange multiplier associated with the non-negativity constraints of wf. As it
is a convex problem and strong duality holds under mild conditions. Define w∗, b∗
0, b∗
1 as the primal
optimal, and ν∗as the dual optimal, then:
−ℓ(w∗, b∗
0, b∗
1) =
inf
w,b0,b1 L(w, b0, b1, ν∗)
(strong duality)"
REFERENCES,0.5975359342915811,"=
inf
w,b0,b1 "
REFERENCES,0.5995893223819302,"−ℓ(w, b0, b1) +
X"
REFERENCES,0.6016427104722792,"f∈¯
F
cfwf −
X"
REFERENCES,0.6036960985626283,"f∈¯
F
ν∗
fwf  "
REFERENCES,0.6057494866529775,"≤−ℓ(w∗, b∗
0, b∗
1) +
X"
REFERENCES,0.6078028747433265,"f∈¯
F
λfw∗
f −
X"
REFERENCES,0.6098562628336756,"f∈¯
F
ν∗
fw∗
f"
REFERENCES,0.6119096509240246,"≤−ℓ(w∗, b∗
0, b∗
1) +
X"
REFERENCES,0.6139630390143738,"f∈¯
F
λfw∗
f. (16)"
REFERENCES,0.6160164271047228,"Therefore, P"
REFERENCES,0.6180698151950719,"f∈¯
F ν∗
fw∗
f = 0, for f ∈¯F. This implies the complementary slackness, i.e.,
w∗
f = 0 ⇒ν∗
f ≥0,
w∗
f > 0 ⇒ν∗
f = 0
(17)
Given the Karush-Kuhn-Tucker (KKT) conditions, gradient of Lagrangian L(w∗, b∗
0, b∗
1, ν∗) w.r.t.
w∗, b∗
0, b∗
1 vanishes, i.e.,"
REFERENCES,0.6201232032854209,"ν∗
f := −∂ℓ(w, b0, b1) ∂wf"
REFERENCES,0.62217659137577,"w∗,b∗
0,b∗
1
+ cf.
(18)"
REFERENCES,0.6242299794661191,"In summary, combining conditions (17) and (18), we obtain the optimalitiy condition,"
REFERENCES,0.6262833675564682,"1. if w∗
f > 0, then ν∗
f = 0;"
REFERENCES,0.6283367556468172,"2. if w∗
f = 0, then ν∗
f ≥0,"
REFERENCES,0.6303901437371663,"where the gradient ν∗
f can be computed via (18)."
REFERENCES,0.6324435318275154,Published as a conference paper at ICLR 2022
REFERENCES,0.6344969199178645,"E
ALGORITHM BOX"
REFERENCES,0.6365503080082136,"Our TELLER alternates between solving a restricted master problem and a subproblem. We summa-
rize the algorithm in Algorithm 1 and Algorithm 2. RMP indicates the Restricted Master Problem
used to update model parameters. SP refers to the Sub-Problem used to construct a new rule. Here
we use RAFS as our search scheme."
REFERENCES,0.6386036960985626,"Algorithm 1: TELLER (RAFS)
Input: TimeLimit, MaxRuleLen
Output: ruleSet"
REFERENCES,0.6406570841889117,1 stack ←empty stack;
REFERENCES,0.6427104722792608,2 ruleSet ←empty set;
REFERENCES,0.6447638603696099,3 b ←0;
REFERENCES,0.6468172484599589,4 w ←0;
REFERENCES,0.648870636550308,"5 b, w ←RMP(b, w, ruleSet); // Initialize weights and bases."
REFERENCES,0.6509240246406571,6 while RunTime≤TimeLimit do
REFERENCES,0.6529774127310062,"7
if stack.isEmpty() then"
REFERENCES,0.6550308008213552,"8
NewRule ←SP(b, w, ruleSet, None); // Search simple rule with length
= 1."
IF NEWRULE IS NONE THEN,0.6570841889117043,"9
if NewRule is None then"
IF NEWRULE IS NONE THEN,0.6591375770020534,"10
break; // If simple rule does not exist, algorithm ends."
ELSE,0.6611909650924025,"11
else"
ELSE,0.6632443531827515,"12
RuleToExtend ←stack.top();"
ELSE,0.6652977412731006,"13
NewRule ←SP(b, w, ruleSet, RuleToExtend);// Try to extend this rule."
IF NEWRULE IS NONE THEN,0.6673511293634496,"14
if NewRule is None then"
IF NEWRULE IS NONE THEN,0.6694045174537988,"15
stack ←empty stack;// If this rule can not be extended, do
not revisit it."
IF NEWRULE IS NONE THEN,0.6714579055441479,"16
if len(NewRule)=MaxRuleLen then"
IF NEWRULE IS NONE THEN,0.6735112936344969,"17
stack ←empty stack;// If this rule reaches the maximum rule
length, stop extending it."
IF NEWRULE THEN,0.675564681724846,"18
if NewRule then"
IF NEWRULE THEN,0.6776180698151951,"19
ruleSet.add(NewRule);"
IF NEWRULE THEN,0.6796714579055442,"20
stack.push(NewRule);"
IF NEWRULE THEN,0.6817248459958932,"21
b, w ←RMP(b, w, ruleSet);// After adding new rule, update
weights and bases"
RETURN RULESET,0.6837782340862423,22 return ruleSet
RETURN RULESET,0.6858316221765913,Published as a conference paper at ICLR 2022
RETURN RULESET,0.6878850102669405,"Algorithm 2: SubProblem (SP)
Input: b, w, ruleSet, RuleToExtend
Output: NewRule"
RETURN RULESET,0.6899383983572895,"1 TempRelSet ←{Rbe, Req, Rme, . . . , Null}; // Thirteen types of temporal
relation and Null (i.e., no temporal relation constraint)."
RETURN RULESET,0.6919917864476386,"2 bodyPredSet, Y ←defined by dataset;"
RETURN RULESET,0.6940451745379876,3 candRuleSet ←empty set;
IF RULETOEXTEND IS NONE THEN,0.6960985626283368,4 if RuleToExtend is None then
IF RULETOEXTEND IS NONE THEN,0.6981519507186859,// Search simple rule with length = 1.
IF RULETOEXTEND IS NONE THEN,0.7002053388090349,"5
for sign(Y ) in {+, −} do"
IF RULETOEXTEND IS NONE THEN,0.702258726899384,"6
for X in bodyPredSet and sign(X) in {+, −} do"
IF RULETOEXTEND IS NONE THEN,0.704312114989733,"7
for RX,Y in TempRelSet do"
IF RULETOEXTEND IS NONE THEN,0.7063655030800822,"8
candRuleSet.add((¬)Y ←(¬)X ∧RX,Y );"
ELSE,0.7084188911704312,9 else
ELSE,0.7104722792607803,// Try to extend input rule.
ELSE,0.7125256673511293,"10
for X in bodyPredSet.difference(RuleToExtend) and for sign(X) in {+, −} do"
ELSE,0.7145790554414785,"11
for X′ in RuleToExtend do"
ELSE,0.7166324435318275,"12
for RX,X′ in TempRelSet do"
ELSE,0.7186858316221766,"13
candRuleSet.add((¬)Y ←RuleToExtend ∧(¬)X (∧X′in RuleToExtendRX,X′));"
ELSE,0.7207392197125256,"14 optValue, optRule ←Evaluate the subproblem objective and choose the smallest one from the
candRuleSet;"
ELSE,0.7227926078028748,15 if optValue < 0 then
ELSE,0.7248459958932238,"// This rule is a valid rule, i.e.
it can improve RMP."
ELSE,0.7268993839835729,"16
NewRule ←optRule;"
ELSE,0.728952772073922,17 else
ELSE,0.731006160164271,"18
NewRule ←None;"
RETURN NEWRULE,0.7330595482546202,19 return NewRule
RETURN NEWRULE,0.7351129363449692,"F
SEARCH SCHEME FOR SUBPROBLEM: RULE-ADDITION-FIRST SEARCH"
RETURN NEWRULE,0.7371663244353183,"The search scheme can also be the Rule-Addition-First Search (RAFS). For the RAFS, the algorithm
starts with an empty rule set and first focuses on discovering all rules with only one body predicate.
Then assign each of them a score that equals the objective function of the subproblem (pricing
problem) applied to the rule. To expand the rules from length l to l + 1, we do the following: we
process all generated rules that have l predicates in increasing order of their score (since we aim to
find a negative reduced cost), and for each such rule, we create new rules by appending an additional
predicate together with the associated temporal relations. Whenever we find a rule with a negative
reduced cost, we add it to the current rule list. When our enumeration terminates, we return the best
rules generated by the heuristic before proceeding to the next value of l. When none of the rules with
length l can be extended to l + 1, we proceed to expand rule from length l + 1 to l + 2. We may also
pre-specify a maximum rule length and set a time limit to the algorithm."
RETURN NEWRULE,0.7392197125256673,Published as a conference paper at ICLR 2022
RETURN NEWRULE,0.7412731006160165,"G
SYNTHETIC EXPERIMENT RESULTS"
RETURN NEWRULE,0.7433264887063655,"Dataset description: For synthetic experiment, we systematically considered 12 settings, where
settings-{1, 7, 9, 10, 11, 12} are reported as dataset-{1, 2, 3, 4, 5, 6} in main text Section 4.2. Each
setting corresponds to different rule weights, rule length and number, type of temporal relation, and
intensity of free predicates.
To verify the similarity between the ground truth rules and the generated rules, we further utilize the
Jaccard coefficient to measure the degree of consistency between the generated rules and the truth
rules. For the i-th ground truth rule data, let ˆUi be the set of rules in the i-th ground truth rule and Ui
be the i-th rules generated by our method. The Jaccard is defined as |Ui∩ˆUi|"
RETURN NEWRULE,0.7453798767967146,|Ui∪ˆUi|.
RETURN NEWRULE,0.7474332648870636,"The Jaccard similarities (range from 0 to 1) are reported in Table 6 and Fig. 7. We explore how the
sample size (600, 1200, 2400) and the search method (REFS and RAFS) will impact the performance.
We plot the Jaccard of our method in different sample sizes and search methods on 12 settings
(see Table 7 for descriptions). The results are summarized in Fig. 7. We find that the performance
gradually improves with the increase of sample size, which verifies that sufficient data can benefit the
learning performance.
As mentioned in Section 3.3, REFS will always extend the longest existing rule, and RAFS is always
extending the shortest existing rule. As shown in Fig. 7, REFS achieves similar performance with
RAFS on most settings."
RETURN NEWRULE,0.7494866529774127,Table 6: Synthetic Data: Jaccard similarity with the Ground Truth Rules.
RETURN NEWRULE,0.7515400410677618,"Setting
REFS-600
REFS-1200
REFS-2400
RAFS-600
RAFS-1200
RAFS-2400"
RETURN NEWRULE,0.7535934291581109,"Setting-1
0.222
0.972
1.000
0.200
1.000
0.500
Setting-2
0.059
0.000
0.250
0.200
0.200
0.222
Setting-3
0.182
0.286
0.750
0.286
0.375
0.750
Setting-4
0.111
0.429
0.600
0.117
0.428
0.600
Setting-5
0.200
0.571
1.000
0.125
0.500
1.000
Setting-6
0.250
0.667
0.750
0.222
0.333
0.600
Setting-7
0.250
0.750
0.750
0.090
0.750
0.600
Setting-8
0.167
0.000
0.286
0.071
0.00
0.200
Setting-9
0.750
0.600
1.000
0.600
0.750
1.000
Setting-10
0.059
0.231
0.600
0.111
0.154
0.750
Setting-11
0.444
0.571
0.800
0.364
0.444
0.800
Setting-12
0.375
0.429
0.750
0.091
0.429
1.000"
RETURN NEWRULE,0.75564681724846,"600
1200
2400 0.2 0.4 0.6 0.8 1.0"
RETURN NEWRULE,0.757700205338809,Jaccard
RETURN NEWRULE,0.7597535934291582,Setting-1
RETURN NEWRULE,0.7618069815195072,"REFS
RAFS"
RETURN NEWRULE,0.7638603696098563,"600
1200
2400 0.00 0.05 0.10 0.15 0.20 0.25"
RETURN NEWRULE,0.7659137577002053,Setting-2
RETURN NEWRULE,0.7679671457905544,"REFS
RAFS"
RETURN NEWRULE,0.7700205338809035,"600
1200
2400 0.2 0.3 0.4 0.5 0.6 0.7"
RETURN NEWRULE,0.7720739219712526,Setting-3
RETURN NEWRULE,0.7741273100616016,"REFS
RAFS"
RETURN NEWRULE,0.7761806981519507,"600
1200
2400
0.1 0.2 0.3 0.4 0.5 0.6"
RETURN NEWRULE,0.7782340862422998,Setting-4
RETURN NEWRULE,0.7802874743326489,"REFS
RAFS"
RETURN NEWRULE,0.7823408624229979,"600
1200
2400 0.2 0.4 0.6 0.8 1.0"
RETURN NEWRULE,0.784394250513347,Jaccard
RETURN NEWRULE,0.7864476386036962,Setting-5
RETURN NEWRULE,0.7885010266940452,"REFS
RAFS"
RETURN NEWRULE,0.7905544147843943,"600
1200
2400
0.2 0.3 0.4 0.5 0.6 0.7"
RETURN NEWRULE,0.7926078028747433,Setting-6
RETURN NEWRULE,0.7946611909650924,"REFS
RAFS"
RETURN NEWRULE,0.7967145790554415,"600
1200
2400 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
RETURN NEWRULE,0.7987679671457906,Setting-7
RETURN NEWRULE,0.8008213552361396,"REFS
RAFS"
RETURN NEWRULE,0.8028747433264887,"600
1200
2400 0.00 0.05 0.10 0.15 0.20 0.25"
RETURN NEWRULE,0.8049281314168378,"0.30
Setting-8"
RETURN NEWRULE,0.8069815195071869,"REFS
RAFS"
RETURN NEWRULE,0.8090349075975359,"600
1200
2400
Sample Size 0.6 0.7 0.8 0.9 1.0"
RETURN NEWRULE,0.811088295687885,Jaccard
RETURN NEWRULE,0.813141683778234,Setting-9
RETURN NEWRULE,0.8151950718685832,"REFS
RAFS"
RETURN NEWRULE,0.8172484599589322,"600
1200
2400
Sample Size 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
RETURN NEWRULE,0.8193018480492813,Setting-10
RETURN NEWRULE,0.8213552361396304,"REFS
RAFS"
RETURN NEWRULE,0.8234086242299795,"600
1200
2400
Sample Size 0.4 0.5 0.6 0.7 0.8"
RETURN NEWRULE,0.8254620123203286,Setting-11
RETURN NEWRULE,0.8275154004106776,"REFS
RAFS"
RETURN NEWRULE,0.8295687885010267,"600
1200
2400
Sample Size 0.2 0.4 0.6 0.8 1.0"
RETURN NEWRULE,0.8316221765913757,Setting-12
RETURN NEWRULE,0.8336755646817249,"REFS
RAFS"
RETURN NEWRULE,0.8357289527720739,Figure 7: Jaccard similarity on Synthetic Data: with REFS Results in Blue and RAFS Results in Red.
RETURN NEWRULE,0.837782340862423,Published as a conference paper at ICLR 2022
RETURN NEWRULE,0.839835728952772,"Table 7: Synthetic Data Setting and Observations.
Dataset
Observation
Setting-1: Same body predicate intensity
w = [1, 1, 1]
λ = [1, 1, 1, 1]"
RETURN NEWRULE,0.8418891170431212,"Hard scenario since all body predicates have
the same occurrence rate. TELLER can still
recover the rules (Table 2) and their weights.
Setting-2: Low important weights
w = [0.5, 0.5, 0.5]
λ = [1, 1, 1, 1]"
RETURN NEWRULE,0.8439425051334702,"Lowering the important weights enhances
the challenges in learning."
RETURN NEWRULE,0.8459958932238193,"Setting-3: High important weights
w = [1.5, 1.5, 1.5]
λ = [1, 1, 1, 1]"
RETURN NEWRULE,0.8480492813141683,"For relatively high rule weights, TELLER
can accurately recover the rules and their weights."
RETURN NEWRULE,0.8501026694045175,"Setting-4: Add rule length
w = [1, 1, 2]
λ = [1, 1, 1, 1]"
RETURN NEWRULE,0.8521560574948666,"Adding rule length enhances
the challenges in learning."
RETURN NEWRULE,0.8542094455852156,"Setting-5: Add one rule
w = [1, 1, 1, 1]
λ = [1, 1, 1, 1]"
RETURN NEWRULE,0.8562628336755647,"Adding one rule does not harm
the rule recovery and learning performance."
RETURN NEWRULE,0.8583162217659137,"Setting-6: Add one dummy predicate
w = [1, 1, 1, 1]
λ = [1, 1, 1, 1, 0.2]"
RETURN NEWRULE,0.8603696098562629,"Adding dummy predicates enhances
the challenges in learning."
RETURN NEWRULE,0.8624229979466119,"Setting-7: Different body predicate intensities
w = [1, 1, 1]
λ = [0.6, 0.8, 1.2, 1.4]"
RETURN NEWRULE,0.864476386036961,"Body predicates have different occurrence rates.
The MAE results outperform Setting-1"
RETURN NEWRULE,0.86652977412731,"Setting-8: Different intensities and low weights
w = [0.5, 0.5, 0.5]
λ = [0.6, 0.8, 1.2, 1.4]"
RETURN NEWRULE,0.8685831622176592,"Lowering the important weights enhances
the challenges in learning,
but different intensities lower the challenges.
Setting-9: Different intensities and high weights
w = [1.5, 1.5, 1.5]
λ = [0.6, 0.8, 1.2, 1.4]"
RETURN NEWRULE,0.8706365503080082,"Both high weights and different intensities
lower the challenges."
RETURN NEWRULE,0.8726899383983573,"Setting-10: Different intensities and add rule length
w = [1, 1, 2]
λ = [0.6, 0.8, 1.2, 1.4]"
RETURN NEWRULE,0.8747433264887063,"Both adding rule length and different intensities
enhance the challenges."
RETURN NEWRULE,0.8767967145790554,"Setting-11: Different intensities and add one rule
w = [1, 1, 1, 1]
λ = [0.6, 0.8, 1.2, 1.4]"
RETURN NEWRULE,0.8788501026694046,"Both adding one rule and different intensities
lower the challenges."
RETURN NEWRULE,0.8809034907597536,"Setting-12: Different intensities and add one dummy predicate
w = [1, 1, 1]
λ = [0.6, 0.8, 1.2, 1.4, 0.2]"
RETURN NEWRULE,0.8829568788501027,"Adding dummy predicate enhances
the challenges in learning,
but different intensities lower the challenges."
RETURN NEWRULE,0.8850102669404517,Published as a conference paper at ICLR 2022
RETURN NEWRULE,0.8870636550308009,"H
REAL EXPERIMENT: MIMIC"
RETURN NEWRULE,0.8891170431211499,"H.1
SUPPLEMENTAL INFORMATION"
RETURN NEWRULE,0.891170431211499,"MIMIC-III is a dataset released under PhysioNet Credentialed Health Data License 1.5.02. It was
approved by the Institutional Review Boards of Beth Israel Deaconess Medical Center (Boston, MA)
and the Massachusetts Institute of Technology (Cambridge, MA). The requirement for individual
patient consent was waived because all the patient health information was deidentified. We manually
checked that this data do not contain personally identifiable information or offensive content."
RETURN NEWRULE,0.893223819301848,"H.2
HYPERPARAMETERS AND EXPERIMENT ENVIRONMENT"
RETURN NEWRULE,0.8952772073921971,"For the MIMIC dataset, we limit the maximum rule length to be 3, and the maximum #rules to be
20. The learning rate in solving the restricted master problem is ×10−4. The master problem is
optimized by the SGD type of algorithm and we choose to use the projected gradient descent to
take care of the weight constraints. The batch size is 64. Each time we solve the subproblem, we
randomly selected 50% of the training data (i.e., patient sequences) to evaluate the subproblem
objective. To exclude noisy and irrelevant rules, we clip the learned weights and discard these rules
with weights smaller than is 10−2 in solving the restricted master problem. To further reduce the
number of candidate rules, we set a threshold to the subproblem gain as 5 × 10−3, i.e., we only
include the candidate rules with negative cost smaller than −5 × 10−3. Our model is trained and
evaluated using 16 processes in parallel, on a server with a Xeon W-3175X CPU."
RETURN NEWRULE,0.8973305954825462,"For the neural-based baselines, we set: 1) TR-GRU with 4 hidden states and 4009 trainable weights,
2) RPPN with 64 hidden states, 64 embedding dimensions, and 33277 trainable weights, 3) CAUSE
with 16 hidden states and 19312 trainable weights, 4) NHP with 8 hidden states, 814 trainable weights,
5)THP with 8 hidden states, 3608 trainable weights, 6) RMTPP with 32 hidden states, 5440 trainable
weights, and 7) IPP with 3 Gaussian kernels, c = [1, 1, 1], σ = [1, 0.8, 0.5]."
RETURN NEWRULE,0.8993839835728953,"H.3
PREDICATE DEFINITION IN MIMIC"
RETURN NEWRULE,0.9014373716632443,"H.4
DISCOVERED TEMPORAL LOGIC RULES"
RETURN NEWRULE,0.9034907597535934,"The discovered rules to explain the real-time urine have been reported in Section 4 Table 4. We list
the discovered rules relate to the survival condition here in Table 9."
RETURN NEWRULE,0.9055441478439425,2https://physionet.org/content/mimiciii/view-license/1.4/
RETURN NEWRULE,0.9075975359342916,Published as a conference paper at ICLR 2022
RETURN NEWRULE,0.9096509240246407,"Table 8: Defined Predicates in Our MIMIC-III Experiment.
Lab Measurements
Low/Normal/High-SysBP
Low/Normal/High-SpO2SaO2
Low/Normal/High-CVP
Low/Normal/High-SVR
Low/Normal/High-Potassium
Low/Normal/High-Sodium
Low/Normal/High-Chloride
Low/Normal/High-BUN
Low/Normal/High-Creatinine
Low/Normal/High-CRP
Low/Normal/High-RBCcount
Low/Normal/High-WBCcount
Low/Normal/High-ArterialpH
Low/Normal/High-ArterialBE
Low/Normal/High-ArterialLactete
Low/Normal/High-HCO3
Low/Normal/High-SvO2ScvO2"
RETURN NEWRULE,0.9117043121149897,"Output
LowUrine"
RETURN NEWRULE,0.9137577002053389,"Input
Colloid, Crystalloid, Water"
RETURN NEWRULE,0.9158110882956879,"Drugs
Norepinephrine, Epinephrine,
Dobutamine, Dopamine, Phenylephrine"
RETURN NEWRULE,0.917864476386037,"Suvival Condition
Survival"
RETURN NEWRULE,0.919917864476386,"Temporal Relation Type
Before, Equal"
RETURN NEWRULE,0.9219712525667351,"Table 9: Learned Rules with Survival head predicate for Sepsis Patients in MIMIC-III
Weight
Rule 0.95"
RETURN NEWRULE,0.9240246406570842,"Rule 1:NotSurvival ←NormalSVR ∧Epinephrine
∧(Epinephrine Before NotSurvival)
∧(NormalSVR Before NotSurvival)"
RETURN NEWRULE,0.9260780287474333,"0.91
Rule 2:NotSurvival ←HighArterialBe
∧(HighArterialBe Before NotSurvival) 0.82"
RETURN NEWRULE,0.9281314168377823,"Rule 3:NotSurvival ←∧HighBUN ∧Phenylephrine
∧(Phenylephrine Before NotSurvival)
∧(HighBUN Before NotSurvival)"
RETURN NEWRULE,0.9301848049281314,"0.51
Rule 4:NotSurvival ←HighSodium
∧(HighSodium Before NotSurvival) 0.53"
RETURN NEWRULE,0.9322381930184805,"Rule 5:NotSurvival ←HighSodium ∧Norepinephrine
∧(HighSodium Before NotSurvival)
∧(Norepinephrine Before NotSurvival)"
RETURN NEWRULE,0.9342915811088296,"0.89
Rule 6:Survival ←NormalAterialPH
∧(NormalAterialPH Before Survival)"
RETURN NEWRULE,0.9363449691991786,"0.55
Rule 7:NotSurvival ←HighPotassium
∧(HighPotassium Before NotSurvival) 1.19"
RETURN NEWRULE,0.9383983572895277,"Rule 8:NotSurvival ←∧HighPotassium ∧Colloid
∧(HighPotassium Before NotSurvival)
∧(Colloid Before NotSurvival) 1.00"
RETURN NEWRULE,0.9404517453798767,"Rule 9:NotSurvival ←HighlAterialPH ∧UseNorepinephrine
∧(HighlAterialPH Before NotSurvival)
∧(HighlAterialPH Before NotSurvival)
0.61
Rule 10:NotSurvival ←HighHCO3 ∧(HighHCO3 Before Survival)"
RETURN NEWRULE,0.9425051334702259,Published as a conference paper at ICLR 2022
RETURN NEWRULE,0.944558521560575,"I
REAL EXPERIMENT: CRIME"
RETURN NEWRULE,0.946611909650924,"Crime Incident Reports are provided by Boston Police Department to document the type of each
incident as well as when and where it occurred (of Innovation & Technology, 2015)."
RETURN NEWRULE,0.9486652977412731,"I.1
SUPPLEMENTAL INFORMATION"
RETURN NEWRULE,0.9507186858316222,"This dataset is released by the Boston Department of Innovation and Technology under Open Data
Commons Public Domain Dedication and License (PDDL). The publisher does not discuss how the
data was collected and whether consent was obtained. We manually checked that this data does not
contain personally identifiable information or offensive content.
Predicate Definition and Dataset statistics. We are interested in the top four most frequent crime
types: vandalism, theft from motor vehicles, assault, and shoplifting. Another ten predicates are
defined to describe the occurrence time properties, such as whether it is in the morning or the
afternoon, on weekdays or weekends, in spring or winter, etc. The set of defined predicates is
displayed in Table 10. We consider all the crime reports from June 2015 to May 2021 and split the
data into 1879 sequences according to days. We randomly choose 80% of these sequences as training
data and the remaining as testing data. On average, each sequence contains 46.03 events."
RETURN NEWRULE,0.9527720739219713,"Table 10: Defined Predicates for Crime.
Period of Crime
Spring, Summer, Autumn, Winter, Weekday, Weekend,
Morning, Afternoon, Evening, Night"
RETURN NEWRULE,0.9548254620123203,"Crime Types
Vandalism, TheftFromMV, Assault, Shoplifting"
RETURN NEWRULE,0.9568788501026694,"Temporal Relation Type
Before, Equal"
RETURN NEWRULE,0.9589322381930184,"I.2
HYPERPARAMETERS AND EXPERIMENT ENVIRONMENT."
RETURN NEWRULE,0.9609856262833676,"Our model is trained and evaluated using 16 processes in parallel, on a server with a Xeon W-3175X
CPU. For this Crime dataset, we limit the maximum rule length to be 2, and the maximum #rules to
be 20. The learning rate used in updating model parameters in the restricted master problem is 10−4.
The master problem is optimized by SGD with a batch size of 64. The subproblem objective function
is evaluated on the entire training data. To exclude noisy and irrelevant rules, we clip the learned
weights and discard these rules with weights smaller than is 10−2 in solving the restricted master
problem. To further reduce the number of candidate rules, we set a threshold to the subproblem gain
as ×10−2, i.e., we only include the candidate rules with negative cost smaller than −× 10−2.
For the non-parametric baselines, we set: 1) TR-GRU with 4 hidden states and 1129 trainable weights,
2) RPPN with 64 hidden states, 64 embedding dimensions, and 27037 trainable weights, 3) CAUSE
with 16 hidden states and 5872 trainable weights, 4) NHP with 8 hidden states, 382 trainable weights,
5)THP with 8 hidden states, 2408 trainable weights, 6)RMTPP with 32 hidden states, 2240 trainable
weights, and 7)IPP with 3 Gaussian kernels, c = [1, 1, 1], σ = [1, 0.8, 0.5]."
RETURN NEWRULE,0.9630390143737166,"I.3
PREDICTION ACCURACY"
RETURN NEWRULE,0.9650924024640657,Table 11: Crime: MAE of Event Time Prediction.
RETURN NEWRULE,0.9671457905544147,"Method
Vandalism
Larceny
TheftFromMV
Assault
Larceny
Shoplifting"
RETURN NEWRULE,0.9691991786447639,"RPPN
0.881
1.137
1.185
0.777
HExp
0.761
0.949
1.912
0.704
TR-GRU
0.759
1.351
1.400
1.092
CAUSE
0.962
1.127
1.206
0.892
NHP
0.613
1.300
1.887
1.269
THP
0.973
1.043
0.957
0.939
RMTPP
0.874
1.021
1.059
0.763
IPP
0.908
1.274
1.508
1.179
TELLER
0.770
0.826
1.465
0.710"
RETURN NEWRULE,0.971252566735113,Published as a conference paper at ICLR 2022
RETURN NEWRULE,0.973305954825462,"Table 12: Temporal Logic Rules Discovered for Four Types of Crime Events.
Weight
Rule"
RETURN NEWRULE,0.9753593429158111,"3.84
Rule 1:Vandalism ←Shoplifting
∧(Shoplifting Before Vandalism)"
RETURN NEWRULE,0.9774127310061602,"1.83
Rule 2:Vandalism ←TheftFromMV
∧(TheftFromMV Before Vandalism) 0.43"
RETURN NEWRULE,0.9794661190965093,"Rule 3:Vandalism ←TheftFromMV ∧Shoplifting
∧(Shoplifting Before Vandalism)
∧(TheftFromMV Before Vandalism)"
RETURN NEWRULE,0.9815195071868583,"2.46
Rule 4:TheftFromMV ←Shoplifting∧
(Shoplifting Before TheftFromMV) 1.42"
RETURN NEWRULE,0.9835728952772074,"Rule 5:TheftFromMV ←TheftFromMV ∧Summer
∧(TheftFromMV Equal Summer)
∧(TheftFromMV Before TheftFromMV) 0.40"
RETURN NEWRULE,0.9856262833675564,"Rule 6:Assault ←Assault ∧Weekend
∧(Weekend Equal Assault)
∧(Assault Equal Assault) 3.62"
RETURN NEWRULE,0.9876796714579056,"Rule 7:Assault ←Shoplifting ∧Assault
∧(Shoplifting Before Assault)
∧(Assault Before Assault)"
RETURN NEWRULE,0.9897330595482546,"1.67
Rule 8:Shoplifting ←TheftFromMV
∧(TheftFromMV Before Shoplifting) 1.14"
RETURN NEWRULE,0.9917864476386037,"Rule 9:Shoplifting ←Shoplifting ∧Vandalism
∧(Vandalism Before Shoplifting)
∧(Shoplifting Before Shoplifting) 1.30"
RETURN NEWRULE,0.9938398357289527,"Rule 10:Shoplifting ←Shoplifting ∧Weekday
∧(Shoplifting Before Shoplifting)
∧(Weekday Equal Shoplifting)
The mean absolute error (MAE) of the predicted event times are displayed in Table 11. Our model
outperforms other models in predicting TheftFromMV. It has a comparable performance with the
Hawkes baseline on the remaining three tasks. We can think of Hawkes process as a special case of
our model and this is especially true if we restrict our model to learn short rules."
RETURN NEWRULE,0.9958932238193019,"I.4
DISCOVERED RULES"
RETURN NEWRULE,0.997946611909651,"We displayed the discovered important rules in Table 12, from which one can observe the following
crime patterns. Shoplifting and TheftFromMV may trigger Vandalism. One explanatory reason is
that larceny may involve break-in and destruction of security devices (Rule 1, 2, and 3). Shoplifting
triggers TheftFromMV. These two events are special types of larceny, and thus they may exhibit
similar crime patterns (Rule 4). TheftFromMV exhibits self-exciting (i.e., clustering) patterns in
summer. This is may due to that winter is extremely cold in Boston, and summer, however, is
a nice period of the year for outdoor activities and thus motor vehicles may pour into the area,
which potentially increases the likelihood of theft (Rule 5). Assault shows a self-exciting pattern on
weekends. People tend to have more social activities on weekends, which increases the possibility
of domestic violence and affray (Rule 6). Shoplifting triggers Assault. This might be explained by
the fact that if shoplifting is spotted at the scene, the thief may have a physical conflict with the
security guard (Rule 7). TheftFromMV and Vandalism will work together to trigger Shoplifting,
which is consistent with our previous observations 1 and 2 (Rule 8 and 9). Shoplifting is self-exiting
on weekdays. Stores are busier on weekends than on weekdays, and thus weekdays might be a better
choice for shoplifters (Rule 10)."
