Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024630541871921183,"A key goal of unsupervised representation learning is “inverting” a data generating
process to recover its latent properties. Existing work that provably achieves this
goal relies on strong assumptions on relationships between the latent variables
(e.g., independence conditional on auxiliary information). In this paper, we take a
very different perspective on the problem and ask, “Can we instead identify latent
properties by leveraging knowledge of the mechanisms that govern their evolution?”
We provide a complete characterization of the sources of non-identiﬁability as
we vary knowledge about a set of possible mechanisms. In particular, we prove
that if we know the exact mechanisms under which the latent properties evolve,
then identiﬁcation can be achieved up to any equivariances that are shared by the
underlying mechanisms. We generalize this characterization to settings where we
only know some hypothesis class over possible mechanisms, as well as settings
where the mechanisms are stochastic. We demonstrate the power of this mechanism-
based perspective by showing that we can leverage our results to generalize existing
identiﬁable representation learning results.These results suggest that by exploiting
inductive biases on mechanisms, it is possible to design a range of new identiﬁable
representation learning approaches."
INTRODUCTION,0.0049261083743842365,"1
INTRODUCTION"
INTRODUCTION,0.007389162561576354,"Modern unsupervised learning techniques can generate images of our world with intricate detail (e.g.
Karras et al., 2019; Song et al., 2020; Razavi et al., 2019), and yet, the latent representations from
which these images are generated remain entangled and challenging to interpret (Sch¨olkopf et al.,
2021; Locatello et al., 2019). At the same time, the success of pre-trained transformers (Radford
et al., 2021; Brown et al., 2020) shows that advances in our ability to extract latent representations
can lead to dramatic improvements in the sample complexity of downstream tasks (Bengio & LeCun,
2007; Bengio et al., 2013). In order to consistently replicate this success, we need methods that can
reliably invert the data generating process into its underlying generative factors. This is a challenging
task because unsupervised representation learning with independent and identically distributed data is
hopelessly unidentiﬁed: even in the nice case in which observations, x, are generated with independent
latent variables, p(z) = Q"
INTRODUCTION,0.009852216748768473,"i p(zi), there exists an inﬁnitely many distributions with independent
latents ˜p(z) that are consistent with the observed marginal distribution, p(x), and only one of them
corresponds to the true latent distribution p(z) (Locatello et al., 2019; Khemakhem et al., 2020a)."
INTRODUCTION,0.012315270935960592,"If we want to build systems which are able to provably identify 1 the true generative factors z that
generated our observed data x = g(z) for some observation model g, then we need to exploit
structural assumptions that constrain this set of possible distributions. Most of the prior work that
can provide such guarantees was developed in the independent component analysis (ICA) literature.
The key ICA assumption is that the latent factors are (conditionally) independent and non-Gaussian.
Then, if the observation model, g : Z →X is linear and invertible, z = g−1(x) is identiﬁed (Comon,"
INTRODUCTION,0.014778325123152709,"∗equal contribution, author order selected randomly.
1A problem is identiﬁed if the true generative factors are a unique solution in the inﬁnite data limit."
INTRODUCTION,0.017241379310344827,Published as a conference paper at ICLR 2022
INTRODUCTION,0.019704433497536946,"Encoder
Decoder"
INTRODUCTION,0.022167487684729065,"Mechanism
Latent  
representation"
INTRODUCTION,0.024630541871921183,"m1(z1)
m2(z2)"
INTRODUCTION,0.027093596059113302,˜g ∘m1 ∘˜g−1(x1)
INTRODUCTION,0.029556650246305417,"g−1(x1)
g(z1)
g−1(x2)
g(z2)
g(z3)"
INTRODUCTION,0.03201970443349754,m2(˜z2) ˜g−1
INTRODUCTION,0.034482758620689655,"1 (x2)
˜g1 ∘m ∘(˜z2) = x3"
INTRODUCTION,0.03694581280788178,m2(˜z2) ˜g−1
INTRODUCTION,0.03940886699507389,2 (x2)
INTRODUCTION,0.04187192118226601,"˜g2 ∘m ∘(˜z2) ≠x3
Observation"
INTRODUCTION,0.04433497536945813,"identity
x1
x2
x3"
INTRODUCTION,0.046798029556650245,"Figure 1: This simple data generating process illustrates that if we know the set of mechanisms that
govern the evolution of an environment, this constrains the set of possible representations to any
equivariances of these mechanisms. At each time step, we observe an environment of bouncing balls
in pixel space as images, xt. These images are produced by some rendering engine, g, as a function
of the true latent representation zt, which in this case gives positions and velocities of each ball
(illustrated by the location and length of the arrows in the latent representation). At each time step, the
state evolves according to a mechanism, mt. Any candidate model that is consistent with the observed
data has to satisfy the observation identity. If an encoder produces either the true representation
(shown in green), or a representation transformed by some equivariance of the mechanism (e.g. the
blue rotated representation) then the observation identity is satisﬁed. However, models that produce
representations that are arbitrary transformations of the true representation (e.g. the pink warped
representation) can be discarded as they are not consistent with the observation identity."
INTRODUCTION,0.04926108374384237,"1994), and for nonlinear g there are a number of recent approaches that leverage non-stationarity in
the distributions over z to identify g−1 (Hyvarinen & Morioka, 2016; 2017; Hyvarinen et al., 2019;
Khemakhem et al., 2020a). These results give a tantalizing demonstration that representation learning
with identiﬁcation guarantees is possible, but the requirement that the latent factors are statistically
(conditionally) independent is limiting2 (Higgins et al., 2018; Sch¨olkopf et al., 2021)."
INTRODUCTION,0.05172413793103448,"In this paper we take a very different approach. We study how the mechanisms that govern an
environment’s evolution constrain the set of possible latent representations that are consistent with
the data. As a simple concrete example, consider the bouncing ball environment shown in Figure
1. The latent state can be completely described by a vector, z, containing the position, velocity and
acceleration of the balls3, and given this latent state, the images, x, shown in Figure 1, are produced
via some rendering engine g : Z →X. Our task is to leverage sequences of observations x1, . . . , xT
and knowledge of m to recover z1, . . . , zT . If we can show that this task has a unique solution that is
consistent with the observations and mechanisms, then the problem is identiﬁed."
INTRODUCTION,0.054187192118226604,"Our main result is that when we know the true mechanism, m, the system is identiﬁed up to any
equivariances of the mechanism; or equivalently, in Section 2.2 we prove that we can identify z
up to any invertible transformation a : Z →Z that commutes with m, such that the composition
m◦a = a◦m. For example, in the bouncing balls environment shown in Figure 1, the laws of physics
are equivariant with respect to your choice of units of measurement—changing from representing z in
meters to inches leaves the output of the mechanism unchanged up to a corresponding unit change—
and hence we can only hope to identify z up to some scaling function a(z) which corresponds to an
arbitrary choice of units of measurement. Interestingly, when the environment evolves according to
multiple known mechanisms, the sources of non-identiﬁability are even further constrained: such a
system is identiﬁed up to equivariances that are shared by all n mechanisms."
INTRODUCTION,0.05665024630541872,"2As a simple example of dependence between latent variables, assume the bouncing balls shown in Figure 1
have different masses indicated by their colors. If the initial conditions were such that all the balls have the same
momentum, then mass and velocity will be inversely correlated.
3A complete generative model of these images would also need to track the colors and shapes of the elements;
we will return to this issue in the discussion of the results."
INTRODUCTION,0.059113300492610835,Published as a conference paper at ICLR 2022
INTRODUCTION,0.06157635467980296,"Perfectly knowing the true mechanisms and when they are applied is unlikely, but in Section 2.3 we
show that we can relax that assumption to a setting where we instead know a hypothesis class of
possible mechanisms that could have been applied. This weaker assumption leads to an additional
source of non-identiﬁability: the mechanisms in our hypothesis class can imitate each other if there
exists an invertible transformation a such that for any two mechanisms m1 and m2 in our hypothesis
class, m2 = a−1 ◦m1 ◦a. For example, if we are in a setting where our hypothesis class includes
both a product mechanism, m1(z) = Q"
INTRODUCTION,0.06403940886699508,"i zi, and a sum mechanism, m2(z) = P"
INTRODUCTION,0.0665024630541872,"i zi, then m1 can
imitate m2 if a(z) = exp(z), since P"
INTRODUCTION,0.06896551724137931,i zi = log(Q
INTRODUCTION,0.07142857142857142,"i exp(zi)). As before, this result is complete, in
the sense that these two sources of non-identiﬁability—equivariance and imitation—are the only
sources of non-identiﬁability in such systems. This gives us a natural way of thinking about the way
knowledge of deterministic mechanisms constrains a representation learning task: with complete
knowledge, the only source of non-identiﬁability is any equivariances inherent in the mechanism, but
by allowing a hypothesis class of possible mechanisms, we introduce potential non-identiﬁability
via imitation. That said, the relationship between the size of the hypothesis class and the size of the
set of a’s that commute via imitation is not necessarily monotonic: for environments governed by
multiple mechanisms, a larger hypothesis class can lead to fewer imitations."
INTRODUCTION,0.07389162561576355,"Section 3, shows that we can derive analogous results for stochastic mechanisms, m(z, U), where
the mechanism deﬁnes a conditional distribution p(zt+1|zt). This generalization gives us a way of
comparing our mechanism-based perspective with existing identiﬁability results. We demonstrate
this in Section 4 by showing that it is possible to view the distributional assumptions made in Klindt
et al. (2020) as a particular choice of mechanism, and by doing so, we can leverage our theory to give
alternative proofs of these results. This strategy required weaker distributional assumptions, thereby
generalizing their result. Finally, we give a mechanism-based perspective on the related work in
Section 5 and Section 6 concludes with a discussion of the open problems that need to be addressed
in order to reliably leverage this approach in practice."
MECHANISM BASED IDENTIFICATION,0.07635467980295567,"2
MECHANISM BASED IDENTIFICATION"
DATA GENERATION PROCESS,0.07881773399014778,"2.1
DATA GENERATION PROCESS"
DATA GENERATION PROCESS,0.0812807881773399,"The state of the system at time t ∈{1, · · · , T} is given by zt ∈Z ⊆Rd. At each time t, we observe
g(zt) = xt ∈X ⊆Rn, which is some transformation of the latent zt. We can think of g : Z →X
as a function that transforms the (typically low dimensional) state variables to the (typically high
dimensional) observed variables; for example, in the bouncing ball environment described in the
introduction, g is the rendering engine that produces the images shown in Figure 1. We assume g is
injective 4 with respect to Rn—i.e. g(z1) = g(z2) implies z1 = z2; or equivalently, any change to
the underlying state, z, is reﬂected in some pixel-level change to the observation x—and we make g
bijective by restricting its inverse g−1 to any x on the data manifold which we denote X (i.e. X is
the image of g). The state transition from time t to t + 1 is governed by a mechanism mt : Z →Z.
There may be multiple mechanisms in a given environment. For example in Figure 1 the transition
from z1 to z2 does not involve any collisions so the state evolves according to Newton’s ﬁrst law of
motion (Newton, 1687); the transition from z2 to z3 involves a collision between two of the balls that
is described by Newton’s third law. Together mt and g describe the data-generation from time t to
t + 1 as follows,
xt ←g(zt),
zt+1 ←mt(zt).
(1)
If we ﬁx the initial conditions, z0, this data generation process is deterministic. In Section 2.2, we
provide results when the underlying mechanism is known and in Section 2.3, we we extend those
results to the case when the mechanisms are not known. In Section 3, we extend our results to
stochastic mechanisms, m(Zt, U) that take samples from some distribution U ∼Uniform(0, 1) as
input. These stochastic mechanisms can represent any conditional distribution P(Zt+1|Zt) (Austin
(2015, Lemma 3.1))"
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.08374384236453201,"2.2
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN"
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.08620689655172414,"We begin in the simplest version of the system described by equation (1): assume that at each time t
the same mechanism m : Z →Z is used, and we know this mechanism. From these assumptions we"
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.08866995073891626,"4for ball collision example, if Z equals position and velocity, we can achieve injectivity by frame stacking."
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.09113300492610837,Published as a conference paper at ICLR 2022
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.09359605911330049,"can derive an identity that describes how the observations xt and xt+1 are related,"
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.0960591133004926,"zt+1 = m(zt),
g−1(xt+1) = m ◦g−1(xt),
xt+1 = g ◦m ◦g−1(xt).
(2)"
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.09852216748768473,"It may be helpful to think of this identity as describing an autoencoder where we require that the
encoder g−1(xt) inverts the data generating process to produce some latent zt from xt, and that the
decoder has to reproduce xt+1 from m(zt); i.e. the representation transformed by the mechanism.
Importantly, this identity describes the true data generating process, rather than some model of it. Our
hypothesis class over the possible encoder / decoder functions is the set, G, of all bijective functions
from Z →X. By assuming that bijectivity we are essentially assuming that the reconstuction task
is solved:5 G is the set of all autoencoders that perfectly reproduce the data, such that for any x on
the data manifold X, and any encoder / decoder pair (˜g−1, ˜g) with ˜g ∈G, their composition is the
identity function, x = ˜g ◦˜g−1 ◦x. Because we assumed that the true g is bijective, we know that it
is in our search space, G. We can constrain this set using our knowledge of the mechanism by only
considering solutions that also satisfy the identity given in equation 2, such that for every pair of
observations (xt, xt+1), an analogous identity holds,"
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.10098522167487685,"xt+1 = ˜g ◦m ◦˜g−1(xt)
(3)"
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.10344827586206896,"Now suppose that we have access to observations xt from the entire set X, then the above identities
have to hold for all xt, ∈X with corresponding xt+1 from equation 2, so we can conclude that the
following functions are equal,
g ◦m ◦g−1 = ˜g ◦m ◦˜g−1
(4)
This relationship will hold for any of the possible decoders ˜g (and corresponding encoders ˜g−1)
that are observationally equivalent given our assumptions. We denote the set of all such decoders,
Gid = {˜g | ˜g ∈G, g ◦m ◦g−1 = ˜g ◦m ◦˜g−1}. If Gid = {g} then we have shown that the problem is
exactly identiﬁed, which means that if we manage to ﬁnd a ˜g that satisﬁes equation 2, then ˜g = g; but
if Gid, also includes other functions ˜g ̸= g, then these functions are the sources of non-identiﬁability."
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.10591133004926108,"Equivariant mechanisms
To see an example of a setting where the problem is not exactly iden-
tiﬁed, consider a mechanism which is equivariant with respect to some bijective transformation
a : Z →Z. A mechanism m is said to be equivariant w.r.t a if a ◦m = m ◦a. For example,
a mechanism derived from Newtonian mechanics may be equivariant with respect to your choice
of units of measurement, such that m(cz) = cm(z) for some scaling constant c. Similarly, if a
mechanism transforms sets of items, any permutation of z would lead to a corresponding permutation
of the mechanism’s output."
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.10837438423645321,"If we have a mechanism that is equivariant with respect to some transformation a (where a is not
identity map), that implies that there exists a function ˜g ̸= g in Gid, so the problem is not exactly
identiﬁed. We can see this as follows,"
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.11083743842364532,g ◦m ◦g−1 = g ◦a−1 ◦a ◦m ◦g−1 = g ◦a−1 ◦m ◦a ◦g−1 = ˜g ◦m ◦˜g−1
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.11330049261083744,"where the ﬁrst equality uses the fact that a−1 ◦a is the identity function, the second applies the
deﬁnition of equivariance, and the ﬁnal equality deﬁnes ˜g := g ◦a−1 and ˜g−1 := a ◦g−1. This
shows that if the mechanism is equivariant, an encoder, ˜g−1, can output a transformed ˜z = a(z),
and the decoder, ˜g, inverts this transformation before producing its output, thereby leaving the
observed variables, x, unchanged. If we denote the set of all equivariances of the mechanism
E = {a | a is a bijection, a ◦m = m ◦a}, then we can deﬁne the set of all such sources of
non-identiﬁcation as, Geq = {˜g | ˜g = g ◦a−1, a ∈E}. This is a natural source of non-identiﬁcation:
given that we are relying on the mechanism to constrain the encoder ˜g−1, it is unsurprising we cannot
prevent transformations that are not affected by the mechanism. The more interesting observation,
which we will show below, is that this set is the only source of non-identiﬁcation when the mechanism
is known, and hence we recover the true z up to equivariances in the mechanism."
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.11576354679802955,"To state this theorem, we ﬁrst deﬁne the notion of identiﬁability up to an equivalence class deﬁned by
a family of bijections A, where a ∈A is a map a : Z →Z.
Deﬁnition 1. Identiﬁability up to A. If the learned encoder ˜g−1 and the true encoder g−1 are
related by some bijection a ∈A, such that ˜g−1 = a ◦g−1 (or equivalently ˜g = g ◦a−1), then ˜g−1 is
said to learn g−1 up to bijections in A. We denote this ˜g−1 ∼A g−1."
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.11822660098522167,"5This is obviously a strong assumption—learning autoencoders that perfectly reconstructed the data is not at
all easy—but it focuses the discussion on the identiﬁcation issues that remain after reconstruction is solved."
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.1206896551724138,Published as a conference paper at ICLR 2022
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.12315270935960591,"Suppose, for example, A is a family of a permutations. Identiﬁability up to A implies that the true
latent variables will be recovered, but that they would not necessarily be ordered in the same way as
they were in the original data generation process. In this setting where the mechanism, m, is known,
the following theorem shows that A is just E, the set of equivariances of m.
Theorem 1. If the data generation process follows equation 2, the encoders that solve the observation
identity in equation 4 identify true encoder g−1 up to equivariances of m (˜g−1 ∼E g−1)."
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.12561576354679804,"To prove the above theorem, we need to establish that the set of solutions to the identity in equation 4
and the set of maps derived from equivariances are equal Gid = Geq. The proof is given in Section
A.1 of the appendix. From Theorem 1, we can derive a number of observations. First, notice that if
we have a standard autoencoder6 then the mechanism is the identity map, m(z) = z, and its set of
equivariances E is any invertible function a, and hence Theorem 1 shows that the encoder is essentially
unconstrained. However, if the mechanism is any non-trivial function, m(z′) ̸= z′ for some z′, then
the space of possible encoders is signiﬁcantly reduced to just those invertible transformations that
commute with m. If the system involves multiple known mechanisms, {m1, . . . , mT }, where at
each time zt+1 = mt(zt), then the encoder is even further constrained. Deﬁne the set of all the
mechanisms that are used at least once in the evolution of the system as M∗= ∪T
t=1{mt}. Suppose
Ei denotes the equivariances of mi ∈M∗. Deﬁne the equivariances that are shared across all the
mechanisms as E∗= ∩iEi;
Corollary 1. If the data generation process follows equation 1, then the encoders that satisfy
observation identity in equation 4 for all m ∈M∗identify true encoder g−1 up to the equivariances
shared across all the mechanisms in M∗, E∗(˜g−1 ∼E∗g−1)."
IDENTIFYING ENCODERS WHEN THE UNDERLYING MECHANISM IS KNOWN,0.12807881773399016,"The proof of the above claim is in Section A.3. This corollary implies a blessing that comes with more
complex environments: if an object is transformed by multiple mechanisms which are diverse (in the
sense that they share few equivariances), then it becomes easier to identify. Given access to inputs
and outputs of a mechanism, if we cannot tell apart whether some transformation was applied to the
input or the output, then the mechanism is equivariant with respect to the transformation. Together
Theorem 1 and Corollary 1, state that we can learn to invert the data generation process but we cannot
distinguish latents that were transformed by equivariances shared across the mechanisms."
IDENTIFYING ENCODERS FOR AFFINE MECHANISMS,0.13054187192118227,"2.2.1
IDENTIFYING ENCODERS FOR AFFINE MECHANISMS"
IDENTIFYING ENCODERS FOR AFFINE MECHANISMS,0.1330049261083744,"Theorem 1 shows us that an encoder ˜g−1 is identiﬁed up to any equivariances of the known mechanism,
but given some mechanism, it does not tell us what equivariances it may exhibit. This section gives
an example of how one might go about ﬁnding all sources of equivariance for a given mechanism.
We derive the equivariances for afﬁne mechanisms, and in doing so we show conditions under which
afﬁne mechanisms lead to identiﬁcation up to some ﬁxed offset. Afﬁne mechanisms are broadly
applicable because with a sufﬁciently short time interval, they approximate a wide variety of physical
systems as the Euler discretization of some linear ordinary differential equation. In such systems,
the mechanism is given by m(z) = Mzt + bt where M ∈Rd×d is an invertible diagonalizable
matrix (with eigendecomposition given as M = SΛS−1, where S is the matrix of eigenvectors and
Λ is a diagonal matrix of eigenvalues), bt ∈Rd is the offset parameter at time t, and the analog of
equation 2 is,
xt+1 = g(Mg−1(xt) + bt).
We search for some encoder ˜g−1 such that this relationship holds for all x and t. Deﬁne an offset
function o(z) = z + p, where the offset function shifts the latent by a vector p. Deﬁne O to be the set
of all the offset functions. We show that the encoder is identiﬁed up to O when we have at least two
distinct offset terms bt and a regularity condition (Assumption 2).7"
IDENTIFYING ENCODERS FOR AFFINE MECHANISMS,0.1354679802955665,"Assumption 1. In the data generation process in equation 1, we set m(zt) = Mzt + bt, where M is
invertible and diagonalizable. We assume that the offset bt takes at least d + 1 distinct values, which
we denote by {b1, · · · , bd+1}. The set {b2 −b1, · · · , bd+1 −b1} of vectors is linearly independent.
Assumption 2. a : Z →Z is analytic and satisﬁes the following assumption. For each component
i ∈{1, · · · , d} of ai(z) and each b ∈Rd, deﬁne the set Sij = {θ | ∇jai(z + b) = ∇jai(z) +
∇2
jai(θ)b, z ∈Rd}. Each set Sij has a non-zero Lebesgue measure in Rd."
IDENTIFYING ENCODERS FOR AFFINE MECHANISMS,0.13793103448275862,"6With the constraint that the encoder is the inverse of the decoder such that ˜g−1 is bijective.
7We conjecture that the regularity condition holds for all analytic functions and is thus not needed. Since we
do not have a proof of this claim, we include it as an assumption."
IDENTIFYING ENCODERS FOR AFFINE MECHANISMS,0.14039408866995073,Published as a conference paper at ICLR 2022
IDENTIFYING ENCODERS FOR AFFINE MECHANISMS,0.14285714285714285,"Theorem 2. If the data generation process follows equation 1 with afﬁne mechanisms, m(z) =
Mzt + bt, Assumptions 1, 2 hold, the eigenvalues of the mechanism M are all distinct, and each
component of S−1(bi −bj) is non-zero for some i ̸= j, then the encoders that solve the observation
identity in equation 4 identify true encoder g−1 up to offsets O such that ˜g−1 ∼O g−1."
IDENTIFYING ENCODERS FOR AFFINE MECHANISMS,0.14532019704433496,"The proof is given in Section A.4 in the Appendix. The above theorem shows the power of using
multiple mechanisms. It can be shown that if there is only one mechanism, then we cannot do better
than linear identiﬁcation. However, if we use two mechanisms as is the case in the above theorem, the
constraint of shared equivariances (Theorem 1 and Corollary 1) enforces almost exact identiﬁability
(only offset-based errors remain)."
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.1477832512315271,"2.3
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN"
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.15024630541871922,"We have seen in the previous section that with complete knowledge of the mechanisms under which a
system evolves, we can learn an encoder up to equivariances. In practice, however, we are unlikely
to have such complete knowledge. In this section, the system still evolves according to some
deterministic mechanism, zt+1 ←mt(zt), but we assume that you only know some hypothesis class
M of possible mechanisms which could have been used, without knowing which mt ∈M is used at
every time step."
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.15270935960591134,"A candidate solution now needs to propose both an encoder ˜g−1 and a mechanism ˜mt ∈M for every
(xt, xt+1) pair such that,
xt+1 = ˜g ◦˜mt ◦˜g−1(xt).
(5)"
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.15517241379310345,"As before, this relationship holds for all xt ∈X, where xt+1 is generated from mt, so any candidate
solution that is consistent with the x’s that we observe must satisfy"
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.15763546798029557,"g ◦mt ◦g−1 = ˜g ◦˜mt ◦˜g−1.
(6)"
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.16009852216748768,"We partition the hypothesis class of mechanisms, M = M∗∪M
′, into the mechanisms that are
used at least once in the evolution of the system, M∗(M∗= ∪T
t=1{mt}), and mechanisms which
are hypothesized but not used, M
′. We deﬁne the set of all decoders ˜g (with corresponding encoders
˜g−1) that solve equation 6 across all the time steps as as ˜Gid = {˜g | ˜g is a bijection , for each mt ∈
M∗, ∃˜mt ∈M, such that g ◦mt ◦g−1 = ˜g ◦˜mt ◦˜g−1}. This set looks very much like the set Gid
that we deﬁned in Section 2.2, but the fact that we have to select ˜m ∈M rather than knowing the
true m implies a new source of non-identiﬁability: imitator mechanisms."
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.1625615763546798,"Deﬁnition 2. Equivariances and imitators w.r.t M. Deﬁne a set of functions ˜E that satisfy com-
mutativity w.r.t the set of mechanisms M in the following sense. The set ˜E comprises of all the
bijections, a(·), that satisfy the following condition. If for each m1 ∈M∗, ∃m2 ∈M such that
a ◦m1 = m2 ◦a, then a ∈˜E."
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.16502463054187191,"The set ˜E consists of two types of elements. We illustrate this through an example of set M =
M∗= {m1, m2}. If a is a bijection that commutes with both m1 and m2, i.e., a ◦m1 = m1 ◦a and
a ◦m2 = m2 ◦a, then a ∈˜E. From this we can see that ˜E consists of elements in the intersection
of the equivariances of the respective mechanisms. Alternatively, if a satisﬁes a ◦m1 = m2 ◦a
and a ◦m2 = m1 ◦a, then we say m2 “imitates” m1 and vice-versa because you can produce m1’s
output from m2 for any z using the following relationship m1 = a−1 ◦m2 ◦a. Further simplifcation
of this yields that a2 = a ◦a is an equivariance of both m1 and m2. This example shows that when
we know the list of mechanisms M = M∗but do not know which mechanism is used, the set ˜E can
be expressed in terms of the equivariances of the mechanisms. For further details see the Appendix
Section A.11. Deﬁne the set of maps that are identiﬁed up to ˜E as ˜Geq = {˜g | ˜g = g ◦a−1, a ∈˜E}."
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.16748768472906403,"Theorem 3. If the data generation process follows equation 1, then the set of all the encoders
that satisfy equation 6 identify true encoders g−1 up to equivariances and imitators w.r.t M, ˜E
(˜g−1 ∼˜E g−1)."
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.16995073891625614,"To prove the above theorem, we follow a similar strategy as in Theorem 1 and establish ˜Gid = ˜Geq.
The proof of the above is in Section A.5 of the Appendix. Equivariances and imitators play similar
roles in the way that they relax constraints on the encoder ˜g−1—any bijection that commutes with"
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.1724137931034483,Published as a conference paper at ICLR 2022
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.1748768472906404,"either is a source of non-identiﬁability—but they are different from the perspective of how we should
think about designing representation learning algorithms. Recall that M is composed of two sets of
mechanisms, M = M∗∪M
′, mechanisms in M∗that are used at least once in the evolution of the
environment and those mechanisms in M
′ which are hypothesized but never used. Equivariances are
dictated only by M∗, which characterizes the evolution of the environment. Any increases to the
number of distinct mechanisms in M∗will potentially decrease the number of equivariances shared
by all mechanisms. This can only be achieved by modifying the environment in some way, either
through an explicit intervention that modiﬁes its mechanisms or by collecting data from multiple
environments with diverse set of mechanisms. For example, in the bouncing balls example given in
Figure 1, one could change the environment by varying the mass of the balls or observing it under
different gravity conditions; or one could intervene by, say, changing the shape of balls in the system
such that you get a different bouncing mechanism."
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.17733990147783252,"Imitators, by contrast, are a function of both the mechanisms M∗that were used and those that were
hypothesized, M
′. An imitator is just some mechanism that can imitate another via some bijection,
so one would expect that as we grow the number of mechanisms in M, the size of the set of imitators
can only grow; but interestingly, this is not always the case for mechanisms from M∗. Recall that
any a ∈˜E produces an encoder of the form ˜g−1 = a ◦g−1, so the same transformation has to be used
for all imitations and equivariances among the mechanisms in M∗. Because of this, it is possible that
increasing the size of M∗reduces the number of imitators. For example, if there is some mechanism,
mi, that does not commute with any non-trivial a in ˜E (either by imitation or equivariance), then
adding mi to M∗will make the problem exactly identiﬁed. On the other hand, growing the size
of the set of unused mechanisms, M
′, can result in signiﬁcant non-identiﬁability. For example, if
M consisted of a ﬂexible class of functions (e.g. a multi-layer perceptron) then it would be easy to
construct imitators of the form m1 = a−1 ◦m2 ◦a."
IDENTIFYING ENCODERS WHEN THE MECHANISMS ARE NOT KNOWN,0.17980295566502463,"Illustrating Theorem 3. We consider the same setting as in Theorem 2. For each t ∈{1, · · · , d+1},
m(zt) = Mzt + bt and xt = g(zt). We only know that the mechanism is afﬁne and only the offset
bt is changing, but parameters M and bt are not known. Let us construct the set ˜E corresponding to
the above setting. We can show that the set ˜E consists of afﬁne functions (See the Appendix A.6 for
details). From Theorem 3, we can thus conclude that for this data generation process even with very
little knowledge of the mechanism, we get linear identiﬁability. This is weaker than the offset based
identiﬁability in Theorem 2, but there we were required to know the entire afﬁne mechanism."
STOCHASTIC MECHANISMS,0.18226600985221675,"3
STOCHASTIC MECHANISMS"
STOCHASTIC MECHANISMS,0.18472906403940886,"The results thus far relied on the assumption that the evolution of a system could be described in
terms of deterministic mechanisms. This deterministic approach models settings where the full latent
state is observable (via some unknown encoder g−1) at a short enough time interval that there is no
uncertainty about the system’s evolution. To generalize to cases where there is some uncertainty
about the latent state’s evolution, we now develop analogous identiﬁcation results for stochastic
mechanisms that induce conditional distributions over latent states.The systems evolves as
Xt ←g(Zt),
Zt+1 ←mt(Zt, Ut),
(7)
where each Ut is noise with each component sampled independently from standard uniform distribu-
tion Uniform[0, 1], Z1 ∼PZ, mt : Z×[0, 1]d →Z, and the decoder g : Z →X is a diffeomorphism
(i.e. a smooth bijection with an invertible Jacobian, see deﬁnition A.1 (Kass & Vos, 2011))."
STOCHASTIC MECHANISMS,0.18719211822660098,"In this section, we will focus on the case where the true mechanism is unknown; the case where
mechanism is known is a special case with no imitator, i.e., ˜mt = mt for all t. The goal is to search for
an encoder ˜g−1, which is a diffeomorphism, that generates ˆXt+1 = ˜g ◦˜mt(˜g−1(xt), ˆUt), where ˆUt is
a random vector with each component from Uniform[0, 1]. In the deterministic case, we had required
that any candidate encoder, ˜g−1, was point-wise consistent with the pairs of observations (xt+1, xt).
Here, encoders are only required to match the observed conditional distributions. An encoder that
is consistent with the observed data can be used to generate ˆXt+1 such that the distribution of
ˆXt+1|Xt = xt matches the distribution of Xt+1|Xt = xt for all xt ∈X,"
STOCHASTIC MECHANISMS,0.1896551724137931,"Xt+1|Xt = xt
d= ˆXt+1|Xt = xt"
STOCHASTIC MECHANISMS,0.1921182266009852,"g ◦mt
 
g−1(xt), Ut
 d= ˜g ◦˜mt
 
˜g−1(xt), ˆUt

(8)"
STOCHASTIC MECHANISMS,0.19458128078817735,Published as a conference paper at ICLR 2022
STOCHASTIC MECHANISMS,0.19704433497536947,"Now, deﬁne the set of all candidate decoders ˜g (with corresponding encoders ˜g−1) that solve the
above equation 8 as Gs
id. We can extend the notion of equivariance and imitation to the stochastic
case by replacing equality in value by equality in distribution, and show that, as before, these are the
only sources of non-identiﬁability. In the special case where mt is known, ˜Es, deﬁned below, only
contains maps that result from equivariance. We continue to use the M∗– set of mechanisms that are
used at least once in the evolution of the system and M – hypothesis class of all the mechanisms.
Deﬁnition 3. Equivariance and imitators in distribution w.r.t M. Deﬁne a set of functions Es that
satisfy commutativity w.r.t the set of distributions M in the following sense. The set Es comprises all
the diffeomorphisms a that satisfy the following condition: a ∈Es if and only if for each m ∈M∗,
∃m
′ ∈M such that for all z ∈Z, a ◦m(z, U)
d= m
′(a(z), U), where each component of U is
sampled independently from Uniform[0, 1] ."
STOCHASTIC MECHANISMS,0.19950738916256158,"Deﬁne the set of maps that identify true g up to Es as Gs
eq = {˜g|˜g = g ◦a−1, a ∈Es}
Theorem 4. If the data generation process follows equation 7, then the set of encoders that solve
stochastic observation identity equation 8 identify the true g−1 up to the equivariances and imitators
in distribution w.r.t M, Es(˜g−1 ∼Es g−1)."
STOCHASTIC MECHANISMS,0.2019704433497537,"To prove the above theorem, we follow a similar strategy as in Theorem 1 and establish Gs
id = Gs
eq.
The proof of the above is provided in Section A.7 in the Appendix. Theorem 4 is consistent with
the results that we developed for the deterministic case (Theorem 3), but because the mechanism is
allowed to be stochastic, it allows us to generalize known results based on distributional assumptions;
we give an example of this in the next section."
A MECHANISM-BASED PERSPECTIVE ON EXISTING RESULTS,0.2044334975369458,"4
A MECHANISM-BASED PERSPECTIVE ON EXISTING RESULTS"
A MECHANISM-BASED PERSPECTIVE ON EXISTING RESULTS,0.20689655172413793,"Our primary motivation for understanding how mechanistic knowledge can aid identiﬁcation, is to
develop methods that do not require independence assumptions over the latent variables. However,
independence assumptions are not incompatible with the mechanism-based perspective: they simply
deﬁne a particular kind of mechanism, which then implies identiﬁcation up to the mechanism’s
associated equivariances and imitators. We demonstrate this by re-deriving recent identiﬁcation
results from Klindt et al. (2020) using the mechanisms implied by their respective distributional
assumptions. We begin by describing the data generation process used by Klindt et al. (2020) as a
stochastic mechanism of the form of equation 7. For each t ∈{1, · · · , T}"
A MECHANISM-BASED PERSPECTIVE ON EXISTING RESULTS,0.20935960591133004,"Zt+1 = Zt + Vt,
Vt = f(Ut),
Ut ∼Uniform[0, 1]d
(9)"
A MECHANISM-BASED PERSPECTIVE ON EXISTING RESULTS,0.21182266009852216,"where f is an inverse CDF such that each component of Vt ∈Rd is sampled from the generalized
Laplace distribution centred at zero with norm parameter α ̸= 2, Z1 ∼PZ. Next, we want to use
Theorem 4 to derive all the solutions to the observation identity in equation 8.
Theorem 5. If the data generation process follows equation 9, and PZ and the parameters deﬁning f
are known (same assumption as in Klindt et al. (2020)), then the solution to the stochastic observation
identity equation 8 leads to identifying the true representations up to permutation, sign-ﬂip and offset."
A MECHANISM-BASED PERSPECTIVE ON EXISTING RESULTS,0.21428571428571427,"The proof is given in Section A.8 in the Appendix. The above theorem generalizes Theorem 1 from
Klindt et al. (2020) as we do not require α < 2 and rather we work with α ̸= 2. Alternatively,
analogous results to those in Klindt et al. (2020); Hyvarinen & Morioka (2017) can be derived
in settings where we do not know the distribution of Vt but instead assume that we observe Xt
often enough that the difference between Zt and Zt+1 is small. In particular, suppose that the data
generation process follows equation 9 except each component of Vt is an i.i.d. draw from a non-
Gaussian with zero mean and |Vt| < δ. Then as δ →0 the true latent is identiﬁed up to permutation,
sign-ﬂip and offset. See Section A.9 for details."
RELATED WORKS,0.21674876847290642,"5
RELATED WORKS"
RELATED WORKS,0.21921182266009853,"Non-linear independent component analysis (ICA) is a highly unidentiﬁed problem; several works
(Hyv¨arinen & Pajunen, 1999; Locatello et al., 2019) have shown that it is impossible to invert the
data generation process without placing restrictions on the data and models. In recent years, a lot of
progress has been made on the problem of non-linear identiﬁcation. Hyvarinen & Morioka (2016;"
RELATED WORKS,0.22167487684729065,Published as a conference paper at ICLR 2022
RELATED WORKS,0.22413793103448276,"2017) provide the ﬁrst proofs for identiﬁcation in non-linear ICA. Hyvarinen & Morioka (2016)
showed that if the latent variables are mutually independent, with each component evolving in time
following a non-stationary time series without temporal dependence, then non-linear identiﬁcation
is possible. Hyvarinen & Morioka (2017) showed that non-linear identiﬁcation is also possible
if the latent variables are mutually independent, with each component evolving in time following
a stationary time series with temporal dependence. In H¨alv¨a & Hyvarinen (2020), the authors
combine non-stationarity (Hyvarinen & Morioka, 2016) and temporal dependency (Hyvarinen &
Morioka, 2017) and extend identiﬁability guarantees in somewhat more general settings. Khemakhem
et al. (2020a); Hyvarinen et al. (2019); Khemakhem et al. (2020b) further generalized the previous
results; in these works instead of using time the authors require observation of auxiliary information.
Klindt et al. (2020) departs from other non-linear ICA works as it explicitly exploits the sparsity
in the transitions of the latent variables (further details on Klindt et al. (2020) can be found in the
previous section.). In Zimmermann et al. (2021), the authors show that minimizing contrastive
losses commonly used in self-supervised learning can also guarantee identiﬁcation provided the
data (contrastive pairs) follow a speciﬁc choice of data generation process (e.g., contrastive pair is
generated from a von Mises-Fisher distribution). In another line of work Locatello et al. (2020); Shu
et al. (2019), the authors study the role of weak supervision in assisting disetanglement. In a recent
work, Gresele et al. (2021), propose to add new form of constraints to non-linear ICA. The constraint
is based on the observation that the decoder g that gives rise to the image x is composed of simpler
functions that are mutually algorithmically independent; the authors exploit this inductive bias on the
structure of g to invert the data generation process. In another recent work von K¨ugelgen et al. (2021),
the authors consider models where the latent variables are divided into two blocks – content and style
block, where the latents are not necessarily independent. Using data augmentation on the style latents,
the authors show block-wise identiﬁcation for content block. In Section 4 we argued the existing
distributional assumptions could be interpreted as particular choices of stochastic mechanisms; for
more details, see Table 1 in the Appendix, where we describe the form of the respective mechanisms.
In short, prior work has focused on identiﬁcation guarantees under assumptions on the dependence
between the different random variables, which is in sharp contrast to our approach, which focuses on
identiﬁcation under varying degrees of the knowledge of mechanisms that govern latent dynamics."
RELATED WORKS,0.22660098522167488,"Equivariance There is signiﬁcant recent interest in leveraging equivariance assumptions to design
more efﬁcient deep network architectures; for a recent survey, see Bronstein et al. (2021). The
general recipe of this line of work is to design functions (deep network architectures) that enforce
equivariances. Our setting inverts this recipe, in that we have some known function, m(z), and
we are interested in ﬁnding all of its equivariances. The relationship between distributions and
their equivariances has a long history in the statistics literature (see e.g. Eaton, 1989, and the
references therein). Our characterization of stochastic equivariances was inspired by the functional
representations given in Bloem-Reddy & Teh (2020). Finally, the importance of group symmetries in
representation learning was discussed in Higgins et al. (2018). Higgins et al. focus on the relationship
between the symmetries of the environment and a model’s representations, whereas we focus on
how symmetries in the environment’s transition function constrain the representation. The two
perspectives are complementary, and in future work we hope to unify them."
DISCUSSION AND FUTURE WORK,0.229064039408867,"6
DISCUSSION AND FUTURE WORK"
DISCUSSION AND FUTURE WORK,0.2315270935960591,"This paper has presented the ﬁrst systematic study of how mechanisms governing the dynamics of
high-level variables can be used to identify these variables from low-level observations, and up to what
equivariances, which depend on the mechanisms. We show that this perspective is both powerful—
yielding signiﬁcant constraints in the space of possible representation—and that it generalizes many
known approaches. Moving forward, a natural direction is to build methods founded on this theory. We
describe two natural losses based on the identity ˜g◦m◦˜g−1(xt) = xt+1. i) Loss using observations:"
DISCUSSION AND FUTURE WORK,0.23399014778325122,"We minimize next observation prediction error min˜g∈H,˜h∈H, ˜m∈M
P"
DISCUSSION AND FUTURE WORK,0.23645320197044334,"t E
h
∥Xt+1 −˜g ◦˜m ◦˜h(Xt)∥2i"
DISCUSSION AND FUTURE WORK,0.23891625615763548,"where H, M is the hypothesis class of functions for decoder and mechanisms respectively. ii) Loss us-
ing latents: Alternatively, one could re-write the identity as g−1◦xt+1 = m◦g−1◦xt and use square
error or contrastive loss given as min˜g∈H, ˜m∈M
P"
DISCUSSION AND FUTURE WORK,0.2413793103448276,"t E
h
−log

˜g(Xt+1)T ˜m˜g(Xt)
˜g(Xt+1)T ˜m˜g(Xt)+P"
DISCUSSION AND FUTURE WORK,0.2438423645320197,"τ ˜g(Xτ )T ˜m˜g(Xt)
i"
DISCUSSION AND FUTURE WORK,0.24630541871921183,"where τ represents other time instances, i.e., τ ̸= t + 1. The positive pair in the contrastive loss is
formed by the adjacent time instances and the negative pair is formed by non-adjacent time instances.
In Appendix Section A.12 we share our initial results for the contrastive approach applied to 3d
identiﬁcation dataset (Zimmermann et al., 2021) ."
DISCUSSION AND FUTURE WORK,0.24876847290640394,Published as a conference paper at ICLR 2022
REFERENCES,0.2512315270935961,REFERENCES
REFERENCES,0.2536945812807882,"Tim Austin. Exchangeable random measures. Annales de l’Institut Henri Poincar´e, Probabilit´es
et Statistiques, 51(3), Aug 2015. ISSN 0246-0203. doi: 10.1214/13-aihp584. URL http:
//dx.doi.org/10.1214/13-AIHP584."
REFERENCES,0.2561576354679803,"Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel
Machines. MIT Press, 2007."
REFERENCES,0.25862068965517243,"Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828,
2013."
REFERENCES,0.26108374384236455,"Benjamin Bloem-Reddy and Yee Whye Teh. Probabilistic symmetries and invariant neural networks.
J. Mach. Learn. Res., 21:90–1, 2020."
REFERENCES,0.26354679802955666,"Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges. CoRR, abs/2104.13478, 2021. URL https:
//arxiv.org/abs/2104.13478."
REFERENCES,0.2660098522167488,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165,
2020. URL https://arxiv.org/abs/2005.14165."
REFERENCES,0.2684729064039409,"Pierre Comon. Independent component analysis, a new concept? Signal processing, 36(3):287–314,
1994."
REFERENCES,0.270935960591133,"Morris H DeGroot. Probability and statistics. Pearson, 2012."
REFERENCES,0.2733990147783251,"Morris L Eaton. Group invariance applications in statistics. In Regional conference series in
Probability and Statistics, pp. i–133. JSTOR, 1989."
REFERENCES,0.27586206896551724,"Luigi Gresele, Paul K Rubenstein, Arash Mehrjou, Francesco Locatello, and Bernhard Sch¨olkopf.
The incomplete rosetta stone problem: Identiﬁability results for multi-view nonlinear ica. In
Uncertainty in Artiﬁcial Intelligence, pp. 217–227. PMLR, 2020."
REFERENCES,0.27832512315270935,"Luigi Gresele, Julius von K¨ugelgen, Vincent Stimper, Bernhard Sch¨olkopf, and Michel Besserve.
Independent mechanism analysis, a new concept? arXiv preprint arXiv:2106.05200, 2021."
REFERENCES,0.28078817733990147,"Hermanni H¨alv¨a and Aapo Hyvarinen. Hidden markov nonlinear ica: Unsupervised learning from
nonstationary time series. In Conference on Uncertainty in Artiﬁcial Intelligence, pp. 939–948.
PMLR, 2020."
REFERENCES,0.2832512315270936,"Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,
and Alexander Lerchner. Towards a deﬁnition of disentangled representations. arXiv preprint
arXiv:1812.02230, 2018."
REFERENCES,0.2857142857142857,"Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning
and nonlinear ica. Advances in Neural Information Processing Systems, 29:3765–3773, 2016."
REFERENCES,0.2881773399014778,"Aapo Hyvarinen and Hiroshi Morioka. Nonlinear ica of temporally dependent stationary sources. In
Artiﬁcial Intelligence and Statistics, pp. 460–469. PMLR, 2017."
REFERENCES,0.29064039408866993,"Aapo Hyv¨arinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and
uniqueness results. Neural networks, 12(3):429–439, 1999."
REFERENCES,0.29310344827586204,"Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and
generalized contrastive learning. In The 22nd International Conference on Artiﬁcial Intelligence
and Statistics, pp. 859–868. PMLR, 2019."
REFERENCES,0.2955665024630542,Published as a conference paper at ICLR 2022
REFERENCES,0.29802955665024633,"Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 4401–4410, 2019."
REFERENCES,0.30049261083743845,"Robert E Kass and Paul W Vos. Geometrical foundations of asymptotic inference, volume 908. John
Wiley & Sons, 2011."
REFERENCES,0.30295566502463056,"Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders
and nonlinear ica: A unifying framework. In International Conference on Artiﬁcial Intelligence
and Statistics, pp. 2207–2217. PMLR, 2020a."
REFERENCES,0.3054187192118227,"Ilyes Khemakhem, Ricardo Pio Monti, Diederik P Kingma, and Aapo Hyv¨arinen.
Ice-beem:
Identiﬁable conditional energy-based deep models based on nonlinear ica.
arXiv preprint
arXiv:2002.11537, 2020b."
REFERENCES,0.3078817733990148,"David Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge,
and Dylan Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding.
arXiv preprint arXiv:2007.10930, 2020."
REFERENCES,0.3103448275862069,"Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Sch¨olkopf,
and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentan-
gled representations. In International Conference on Machine Learning, pp. 4114–4124. PMLR,
2019."
REFERENCES,0.312807881773399,"Francesco Locatello, Ben Poole, Gunnar R¨atsch, Bernhard Sch¨olkopf, Olivier Bachem, and Michael
Tschannen. Weakly-supervised disentanglement without compromises. In International Conference
on Machine Learning, pp. 6348–6359. PMLR, 2020."
REFERENCES,0.31527093596059114,"Boris Mityagin. The zero set of a real analytic function. arXiv preprint arXiv:1512.07276, 2015."
REFERENCES,0.31773399014778325,Isaac Newton. Newton’s Principia : the Mathematical Principles of Natural Philosophy. 1687.
REFERENCES,0.32019704433497537,"Bogdan Nica. The mazur-ulam theorem. arXiv preprint arXiv:1306.2380, 2013."
REFERENCES,0.3226600985221675,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International Conference on Machine Learning, pp.
8748–8763. PMLR, 2021."
REFERENCES,0.3251231527093596,"Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images with
vq-vae-2. In Advances in neural information processing systems, pp. 14866–14876, 2019."
REFERENCES,0.3275862068965517,"Bernhard Sch¨olkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the
IEEE, 109(5):612–634, 2021. doi: 10.1109/JPROC.2021.3058954."
REFERENCES,0.33004926108374383,"Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised
disentanglement with guarantees. arXiv preprint arXiv:1910.09772, 2019."
REFERENCES,0.33251231527093594,"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In International
Conference on Learning Representations, 2020."
REFERENCES,0.33497536945812806,"Julius von K¨ugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Sch¨olkopf, Michel
Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably
isolates content from style. arXiv preprint arXiv:2106.04619, 2021."
REFERENCES,0.3374384236453202,"Roland S Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel.
Contrastive learning inverts the data generating process. arXiv preprint arXiv:2102.08850, 2021."
REFERENCES,0.3399014778325123,Published as a conference paper at ICLR 2022
REFERENCES,0.34236453201970446,"Approach
Assumptions"
REFERENCES,0.3448275862068966,"Time contrastive (Hyvarinen & Morioka,
2016)
Zt ←Ut, each U j
t is independent and non-stationary"
REFERENCES,0.3472906403940887,"Permutation
contrastive
(Hyvarinen
&
Morioka, 2017)
Each Zj
t+1 ←m(Zj
t , U j
t ), stationary and not quasi-Gaussian"
REFERENCES,0.3497536945812808,"Slow VAE (Klindt et al., 2020)
Zt+1 ←Zt + f(Ut), each f(U i
t) is independent and gener-
alized Laplace
Conditional VAE (Khemakhem et al., 2020a;
Hyvarinen et al., 2019)
Z ←m(O, U), all components of Z are independent condi-
tional on O
Independently modulated component analysis
(Khemakhem et al., 2020b)
Z ←m(O, U), m has a special structures allowing to relax
conditional independence
Contrastive learning (Zimmermann et al.,
2021)
˜Z ←m(Z, U), m is such that ˜Z ∼conditional von Mises-
Fisher
Multi-view ICA (Gresele et al., 2020)
Z1 ←m(Z0, U), X0 ←g0(Z0), X1 ←g1(Z1), all compo-
nents of Z0 and U are independent"
REFERENCES,0.3522167487684729,Table 1: Table comparing different works and the assumptions made for identiﬁability.
REFERENCES,0.35467980295566504,"A
APPENDIX"
REFERENCES,0.35714285714285715,"A.1
PROOF OF THEOREM 1"
REFERENCES,0.35960591133004927,Proof. First we show that Gid ⊆Geq.
REFERENCES,0.3620689655172414,Consider a ˜g ∈Gid. For each x ∈X
REFERENCES,0.3645320197044335,g ◦m ◦g−1(x) = ˜g ◦m ◦˜g−1(x)
REFERENCES,0.3669950738916256,"˜g−1 ◦

g ◦m ◦g−1(x)

= ˜g−1 ◦

˜g ◦m ◦˜g−1(x)

(Compose ˜g−1 with both sides)"
REFERENCES,0.3694581280788177,"˜g−1 ◦

g ◦m ◦g−1(x)

= m ◦˜g−1(x)
(˜g−1 ◦˜g(z) = z)"
REFERENCES,0.37192118226600984,"Since g is invertible we can substitute x in the above equation with x = g(z) and obtain for each
z ∈Z

˜g−1 ◦g

◦m ◦

g−1 ◦g(z)

= m ◦

˜g−1 ◦g(z)
"
REFERENCES,0.37438423645320196,"
˜g−1 ◦g

◦m(z) = m ◦

˜g−1 ◦g

(z)
(10)"
REFERENCES,0.3768472906403941,"Deﬁne ˜g−1 ◦g = a. Observe that a is invertible and from equation 10 we gather that a ∈E. Also,
since ˜g = g ◦a−1, we can conclude that ˜g ∈Geq, which proves the ﬁrst part of the claim. For the
second part, we show that Geq ⊆Gid."
REFERENCES,0.3793103448275862,"Consider a ˜g ∈Geq = {˜g | ˜g = g ◦a−1, a ∈E}. By deﬁnition, can express ˜g = g ◦a−1. For each
x ∈X we write"
REFERENCES,0.3817733990147783,"˜g ◦m ◦˜g−1(x) =
 
g ◦a−1
◦m ◦
 
g ◦a−1−1(x)"
REFERENCES,0.3842364532019704,"=
 
g ◦a−1
◦m ◦
 
a ◦g−1
(x)"
REFERENCES,0.3866995073891626,"=
 
g ◦a−1
◦a ◦m ◦g−1(x)"
REFERENCES,0.3891625615763547,= g ◦m ◦g−1(x)
REFERENCES,0.3916256157635468,"Observe that ˜g is both a bijection and satisﬁes the identity in equation 4. Therefore, ˜g ∈Gid. This
proves the second part of the claim. Therefore, Gid = Geq."
REFERENCES,0.39408866995073893,"A.2
LINEAR MECHANISM M AND LINEAR G."
REFERENCES,0.39655172413793105,"The focus of the main text is on nonlinear identiﬁcation, but in this section we show how the analysis
for general functions g and m applies to the special case when g and m are linear and afﬁne maps
respectively. This special case is useful both as a concrete example, and as a stepping stone to"
REFERENCES,0.39901477832512317,Published as a conference paper at ICLR 2022
REFERENCES,0.4014778325123153,"Theorem 2 (nonlinear g with afﬁne m) which will reuse some of the same proof strategies. We write
the data generation process as follows."
REFERENCES,0.4039408866995074,"zt+1 = Mzt + b,
xt = Gzt,
(11)"
REFERENCES,0.4064039408866995,"where M ∈Rd×d is a diagonalizable matrix describing the mechanism, b ∈Rd is the offset parameter,
G ∈Rd×d is an invertible matrix determining how the data transforms from latent space to the
observable space. We write the eigendecomposition of M as follows M = SΛS−1, where S is
the matrix of eigenvectors and Λ is a diagonal matrix of the eigenvalues. On the same lines as the
equation 2, we can obtain an identity between xt and xt+1 as follows."
REFERENCES,0.4088669950738916,zt+1 = Mzt + b
REFERENCES,0.41133004926108374,G−1xt+1 = MG−1xt + b
REFERENCES,0.41379310344827586,xt+1 = GMG−1xt + Gb (12)
REFERENCES,0.41625615763546797,"If the learner knows M and b, it tries to solve for an invertible ˜G that satisﬁes"
REFERENCES,0.4187192118226601,"xt+1 = ˜GM ˜G−1xt + ˜Gb
(13)"
REFERENCES,0.4211822660098522,"Theorem 6. If the data generation process follows equation 11 and the eigenvalues of the mechanism
M are all distinct and each component of the vector S−1b is non-zero, then the only solution to the
identity in equation 13 is the true mechanism G."
REFERENCES,0.4236453201970443,"Proof. We take the difference of the equations 12 and 13 to get the following condition. For each
xt ∈Rd
(GMG−1 −˜GM ˜G−1)xt + (G −˜G)b = 0
(14)"
REFERENCES,0.42610837438423643,"Because, equations 12 and 13 hold for all xt, we can substitute xt = 0 in the above to get"
REFERENCES,0.42857142857142855,"(G −˜G)b = 0
(15)"
REFERENCES,0.43103448275862066,"We plug the above condition in equation 15 back into equation 14 to get the following condition. For
each xt ∈Rd
(GMG−1 −˜GM ˜G−1)xt = 0
(16)"
REFERENCES,0.43349753694581283,"If equation 16 holds for d linearly independent vectors xt ∈Rd, then we can conclude that,"
REFERENCES,0.43596059113300495,GMG−1 −˜GM ˜G−1 = 0
REFERENCES,0.43842364532019706,"G−1
GMG−1 −˜GM ˜G−1
= 0"
REFERENCES,0.4408866995073892,"MG−1 −G−1 ˜GM ˜G−1 = 0

MG−1 −G−1 ˜GM ˜G−1
˜G = 0"
REFERENCES,0.4433497536945813,"MG−1 ˜G = G−1 ˜GM
(17)"
REFERENCES,0.4458128078817734,"Let A = G−1 ˜G. We substitute A and the eigendecomposition of M (M = SΛS−1, where Λ =
diag(λ1, · · · , λd)) in equation 17 to get"
REFERENCES,0.4482758620689655,M = AMA−1
REFERENCES,0.45073891625615764,SΛS−1 = ASΛS−1A−1
REFERENCES,0.45320197044334976,"Λ =

S−1AS

Λ

S−1A−1S
"
REFERENCES,0.45566502463054187,"Λ = CΛC−1
(where C = S−1AS)
ΛC = CΛ
(18)"
REFERENCES,0.458128078817734,Published as a conference paper at ICLR 2022
REFERENCES,0.4605911330049261,"We further simplify equation 18 and compare each element of the matrix ΛC and CΛ to get the
following condition. For all i, j ∈{1, · · · , d}"
REFERENCES,0.4630541871921182,"[ΛC]ij = Cijλi, [CΛ]ij = Cijλj
Cij(λi −λj) = 0
(19)"
REFERENCES,0.46551724137931033,Consider the above equation 19 for i ̸= j to get
REFERENCES,0.46798029556650245,"Cij(λi −λj) = 0 =⇒Cij = 0 (we use the assumption that λi ̸= λj)
(20)"
REFERENCES,0.47044334975369456,"From the above, it follows that C is a diagonal matrix. We obtain an expression for A in terms of C
matrix below."
REFERENCES,0.4729064039408867,S−1AS = C
REFERENCES,0.4753694581280788,"A = SCS−1
(21)"
REFERENCES,0.47783251231527096,From equation 15 we get that
REFERENCES,0.4802955665024631,"(G −˜G)b = 0
˜
G=GA
=⇒(I −A)b = 0
Eqn. (21)
=⇒
(I −SCS−1)b = 0"
REFERENCES,0.4827586206896552,"=⇒
| {z }
Left multiply S−1
S−1b −CS−1b =⇒(C −I)S−1b = 0
(22)"
REFERENCES,0.4852216748768473,"Since C is a diagonal matrix, we can simplify the above condition further to get (cii −1)(S−1b)i = 0.
Since (S−1b)i ̸= 0 =⇒cii = 1, we obtain C = I =⇒G−1 ˜G = I =⇒G = ˜G."
REFERENCES,0.4876847290640394,"A.3
PROOF OF COROLLARY 1"
REFERENCES,0.49014778325123154,"Proof. This proof follows essentially the same strategy as the proof of Theorem 1 with a set of
mechanisms, M∗instead of a single mechanism m; we include it for completeness. First we show
that G∗
id ⊆G∗
eq."
REFERENCES,0.49261083743842365,"Consider a ˜g ∈G∗
id. For each x ∈X and for each m ∈M∗"
REFERENCES,0.49507389162561577,"g ◦m ◦g−1(x) = ˜g ◦m ◦˜g−1(x),
(23)"
REFERENCES,0.4975369458128079,"and by following the same steps as the proof of Theorem 1, we can show that,

˜g−1 ◦g

◦m(z) = m ◦

˜g−1 ◦g

(z)
(24)"
REFERENCES,0.5,"As before, deﬁne ˜g−1 ◦g = a. Observe that a is invertible and from equation 24 we gather that
a ∈E∗. Also, since ˜g = g ◦a−1, we can conclude that ˜g ∈G∗
eq, which proves the ﬁrst part of the
claim. In the second part, we need to show that G∗
eq ⊆G∗
id."
REFERENCES,0.5024630541871922,"Consider a ˜g ∈G∗
eq = {˜g | ˜g = g ◦a−1, a ∈E∗}. By deﬁnition, can express ˜g = g ◦a−1. For each
x ∈X and for each m ∈M∗we write,"
REFERENCES,0.5049261083743842,"˜g ◦m ◦˜g−1(x) =
 
g ◦a−1
◦m ◦
 
a ◦g−1
(x) =
 
g ◦a−1
◦a ◦m
 
g−1(x)

(since a commutes with each m ∈M) ="
REFERENCES,0.5073891625615764,"g ◦m ◦g−1(x)
for all m (25)"
REFERENCES,0.5098522167487685,"Observe that ˜g is both a bijection and satisﬁes the observation identity in equation 4. Therefore,
˜g ∈G∗
id. This proves the second part of the claim. Therefore, G∗
id = G∗
eq."
REFERENCES,0.5123152709359606,"A.4
PROOF OF THEOREM 2"
REFERENCES,0.5147783251231527,"Proof. We showed in Theorem 1 that the only source of non-identiﬁability are the bijections, a, in
the set E; our task here is to explicitly ﬁnd all of these bijections for afﬁne mechanisms. If a ∈E,
then it satisﬁes a ◦m = m ◦a. We replace m with the afﬁne mechanism to obtain the following
condition. For each z ∈Rd"
REFERENCES,0.5172413793103449,Published as a conference paper at ICLR 2022
REFERENCES,0.5197044334975369,"a(Mz + b) = Ma(z) + b
(26)"
REFERENCES,0.5221674876847291,"Next, recall that a : Rd →Rd; we take gradient of the function in the LHS and RHS of the above
equation 26 separately w.r.t z. Consider the jth component of a(Mz + b) denoted as aj(Mz + b).
We ﬁrst take the gradient of aj(Mz + b) w.r.t z"
REFERENCES,0.5246305418719212,"∇zaj(Mz + b) =
dy dz"
REFERENCES,0.5270935960591133,"T
∇yaj(y),
(27)"
REFERENCES,0.5295566502463054,"where y = Mz + b, ∇yaj(y) is the gradient of aj w.r.t y and dy"
REFERENCES,0.5320197044334976,"dz denotes the Jacobian of y w.r.t z.
We simplify the above further to get"
REFERENCES,0.5344827586206896,"∇zaj(Mz + b) = M T∇yaj(y) = M T∇yaj(Mz + b)
(28)"
REFERENCES,0.5369458128078818,We can write the above for each component of a as follows.
REFERENCES,0.5394088669950738,"
∇za1(Mz + b), · · · , ∇zad(Mz + b)

=

M T∇ya1(Mz + b), · · · , M T∇yad(Mz + b)
"
REFERENCES,0.541871921182266,"= M T[∇ya1(Mz + b), · · · , ∇yad(Mz + b)] = M TJT(Mz + b),
(29)"
REFERENCES,0.5443349753694581,"where J(Mz + b) is the Jacobian of a computed at Mz + b. Next, we take the gradient of the jth
component of the RHS in equation 26 and let mj denote the jth column of M,"
REFERENCES,0.5467980295566502,"∇z

mT
j a(z) + bj] =
X"
REFERENCES,0.5492610837438424,"i
mji∇zai(z) =

∇a1(z), · · · , ∇ad(z)
  "
REFERENCES,0.5517241379310345,"mj1
mj2
...
mjd "
REFERENCES,0.5541871921182266,"
(30)"
REFERENCES,0.5566502463054187,We can write the above for each component in the RHS of equation 26 as follows
REFERENCES,0.5591133004926109,"h
∇z

mT
1 a(z)+b1

, · · · , ∇z

mT
da(z)+bd
i
=

∇a1(z), · · · , ∇ad(z)
  "
REFERENCES,0.5615763546798029,"m11, · · · , md1
m12, · · · , md2
...
...
m1d, · · · , mdd "
REFERENCES,0.5640394088669951,= JT(z)M T (31)
REFERENCES,0.5665024630541872,"We equate the gradient of LHS and RHS in equation 26 using the expressions derived in equation 29
and equation 31 to obtain"
REFERENCES,0.5689655172413793,"a(Mz + b) = Ma(z) + b =⇒M TJT(Mz + b) −JT(z)M T = 0
(32)"
REFERENCES,0.5714285714285714,"We write the same expression at another offset b
′ ̸= b below"
REFERENCES,0.5738916256157636,"a(Mz + b
′) = Ma(z) + b =⇒M TJT(Mz + b
′) −JT(z)M T = 0
(33)"
REFERENCES,0.5763546798029556,"Taking the difference of equation 32 and equation 33 we get M TJT(Mz + b) = M TJT(Mz + b
′).
Since M is invertible, we get J(Mz + b) = J(Mz + b
′). Consider row j of this identity. For each
z ∈Rd"
REFERENCES,0.5788177339901478,"∇aj(Mz+b)−∇aj(Mz+b
′) = 0 =⇒∇aj(˜z)−∇aj(˜z+b
′−b) = 0 =⇒  "
REFERENCES,0.5812807881773399,"∇2
1aj(θ1)
∇2
2aj(θ2)
...
∇2
daj(θd) "
REFERENCES,0.583743842364532,"(b−b
′) = 0"
REFERENCES,0.5862068965517241,"(34)
where ∇2aj is the Hessian of aj and ∇2
kaj(θk) corresponds to the kth row of the Hessian matrix.
Note that in the above expansion there is a different θk for each row (mean value theorem applied"
REFERENCES,0.5886699507389163,Published as a conference paper at ICLR 2022
REFERENCES,0.5911330049261084,"to each component of ∇aj yields a different point θk on the line joinining ˜z and ˜z + b −b
′. From
Assumption 2 and based on the fact that M is invertible, it follows that ∇2
kaj(θk)(b −b
′) = 0 over a
measurable set. Since aj is analytic ∇2
kaj(z)(b −b
′) is also analytic. Therefore, from (Mityagin,
2015), we can conclude that ∇2
kaj(z)(b −b
′) = 0 for all z. We can make the same argument for
each component k and conclude that ∇2aj(z)(b −b
′) = 0. From Assumption 1, it follows that
∇2aj(z)(bj −b1) = 0 for all j ∈{2, · · · , d + 1} and since the set {b2 −b1, · · · , bd+1 −b1} is
linearly independent ∇2aj(z) = 0 for all z. This implies a(z) = Az + p. Plug a(z) = Az + p into
a(Mz + b) = Ma(z) + b to get
A(Mz + b) + p = MA(z + p) + b =⇒(AM −MA)z + (A −I)b + (I −M)p = 0
(35)"
REFERENCES,0.5935960591133005,"We write the same expression for offset b
′"
REFERENCES,0.5960591133004927,"A(Mz + b
′) + p = MA(z + p) + b
′ =⇒(AM −MA)z + (A −I)b
′ + (I −M)p = 0
(36)
We take the difference of equation 35 and equation 36 to get"
REFERENCES,0.5985221674876847,"(A −I)(b −b
′) = 0
(37)
Substitute z = 0 in equation 35 to get
(A −I)b + (I −M)p = 0
(38)
Substitute the above condition in equation 38 into equation 35 to get the following. For each z
(AM −MA)z = 0
(39)
We can now leverage the proofs from the linear setting in Section A.2. The above equation 39 is
the same as equation 17 and the equation 37 is the same as equation 22, with b replaced by b −b
′.
Following the same analysis as before, we get that latent variables are exactly identiﬁed; we show
all the steps below for completeness. By choosing d linearly independent z and substituting in
equation 39 we get the following,
MA = AM"
REFERENCES,0.6009852216748769,"M = AMA−1,"
REFERENCES,0.603448275862069,"SΛS−1 = ASΛS−1A−1,"
REFERENCES,0.6059113300492611,"Λ =

S−1AS

Λ

S−1A−1S

,"
REFERENCES,0.6083743842364532,"Λ = CΛC−1,
ΛC = CΛ, (40)"
REFERENCES,0.6108374384236454,"where C = S−1AS, M = SΛS−1, Λ = diag

λ1, · · · , λd

. We further simplify equation 18
and compare each element of the matrix ΛC and CΛ to get the following condition. For all
i, j ∈{1, · · · , d}
[ΛC]ij = Cijλi, [CΛ]ij = Cijλj
Cij(λi −λj) = 0
(41)"
REFERENCES,0.6133004926108374,"Consider the above equation 41 for i ̸= j to get
Cij(λi −λj) = 0 =⇒Cij = 0 (we use the assumption that λi ̸= λj)
(42)
From the above, it follows that C is a diagonal matrix. We obtain an expression for A in terms of C
matrix below.
S−1AS = C"
REFERENCES,0.6157635467980296,"A = SCS−1
(43)"
REFERENCES,0.6182266009852216,From equation 37 we get that
REFERENCES,0.6206896551724138,"(G −˜G)(b −b
′) = 0
˜
G=GA
=⇒(I −A)(b −b
′) = 0 =⇒(I −SCS−1)b = 0"
REFERENCES,0.6231527093596059,"=⇒
| {z }
Left multiply S−1
S−1(b −b
′) −CS−1(b −b
′) =⇒(C −I)S−1(b −b
′) = 0
(44)"
REFERENCES,0.625615763546798,"Since C is a diagonal matrix, we can simplify the above condition further to get (cii −1)(S−1(b −
b
′))i = 0. Since (S−1(b−b
′))i ̸= 0 =⇒cii = 1, we obtain C = I =⇒A = I =⇒a(z) = z+p.
This proves that the latents are identiﬁed up to an offset."
REFERENCES,0.6280788177339901,Published as a conference paper at ICLR 2022
REFERENCES,0.6305418719211823,"A.5
PROOF OF THEOREM 3"
REFERENCES,0.6330049261083743,Proof. We ﬁrst show that ˜Gid ⊆˜Geq. Consider a ˜g ∈˜Gid. We rewrite equation 6 below. For all x ∈X
REFERENCES,0.6354679802955665,g ◦mt ◦g−1(x) = ˜g ◦˜mt ◦˜g−1(x)
REFERENCES,0.6379310344827587,"(˜g−1 ◦g) ◦(mt ◦g−1(x)) = ˜mt ◦˜g−1(x)
(45)"
REFERENCES,0.6403940886699507,"Since g is bijective, we can write x = g(z) to get"
REFERENCES,0.6428571428571429,"(˜g−1 ◦g) ◦mt(z) = ˜mt ◦˜g−1 ◦g(z)
(46)"
REFERENCES,0.645320197044335,Since the above equality holds for all z ∈Z we can conclude that
REFERENCES,0.6477832512315271,"(˜g−1 ◦g) ◦mt = ˜mt ◦(˜g−1 ◦g),
a ◦mt = ˜mt ◦a,
(47)"
REFERENCES,0.6502463054187192,"The above conclusion in equation 47 holds for all mt ∈M∗. Therefore, a in equation 47 and
{ ˜mt}T
t=1 (where ˜mt ∈M) together satisfy the condition that for all m ∈M∗, a◦m = ˜m◦a, where
˜m ∈M. We can rewrite ˜g−1 ◦g = a as ˜g = g ◦a−1. From this it follows that ˜g ∈˜Geq. This proves
the ﬁrst part of the theorem."
REFERENCES,0.6527093596059114,"Now let us consider the second part of the theorem. Consider a ˜g ∈˜Geq. We can write ˜g = g ◦a−1,
where a ∈˜E. At time t, some mechanism mt ∈M∗is used to transform the latents. Since a ∈˜E,
select the mechanism ˜mt ∈M for which a ◦mt = ˜mt ◦a and as a consequence"
REFERENCES,0.6551724137931034,"g ◦mt ◦g−1 = g ◦a−1 ◦˜mt ◦a ◦g−1 = ˜g ◦˜mt ◦˜g−1
(48)"
REFERENCES,0.6576354679802956,"In the ﬁrst equality above, we use a ◦mt = ˜mt ◦a and in the second equality we use the deﬁnition
of ˜g. We can repeat the above exercise for all t and corresponding mt using the same a. Therefore, ˜g
is in ˜Gid. This shows the second part of the theorem, i.e., ˜Geq ⊆˜Gid."
REFERENCES,0.6600985221674877,"A.6
LEVERAGING THEOREM 3 WHEN MECHANISM IS LINEAR AND g IS NON-LINEAR"
REFERENCES,0.6625615763546798,"We write the data generation process as follows. For each t ∈{1, · · · , d + 1}"
REFERENCES,0.6650246305418719,"zt+1 = Mtzt + bt,
xt = g(zt),
(49)"
REFERENCES,0.6674876847290641,"Let us construct the set ˜E corresponding to the above setting. For each z ∈Rd,"
REFERENCES,0.6699507389162561,"a(Mz + b) = M
′a(z) + ˜b =⇒M TJT(Mz + b) −JT(z)M
′,T = 0,"
REFERENCES,0.6724137931034483,"a(Mz + b
′) = M
′a(z) + ˜b
′ =⇒M TJT(Mz + b
′) −JT(z)M
′,T = 0,
(50)"
REFERENCES,0.6748768472906403,"where (M, b) and (M, b
′) are the true mechanisms and (M
′, b
′) and (M ′,˜b
′) are the imtitating
mechanisms chosen by the learner, J is the Jacobian of a. Note here the learner only exploits the
knowledge that b changes to ˜b, which is why it keeps M
′ ﬁxed and only changes the offset. We take
the difference of the RHS in the above two equations to get"
REFERENCES,0.6773399014778325,"M TJT(Mz + b) = M TJT(Mz + b
′)
(51)"
REFERENCES,0.6798029556650246,"Since M is invertible we get J(Mz + b) −J(Mz + b
′) = 0 for all z. We can follow the same
justiﬁcation as was used in equation 33 to conclude that J(z) is constant and a is thus an afﬁne map.
We substitue the afﬁne map a(z) = Az + p back into equation 50 to get the following. For all z"
REFERENCES,0.6822660098522167,"AMz + Abj + p = M
′Az + b
′,j + M
′p
(52)"
REFERENCES,0.6847290640394089,"Substitute z = 0 to get b
′,j = Abj + p −M
′p. Substitute this condition back into the above equation,
we get AM = M
′A =⇒M
′ = AMA−1."
REFERENCES,0.687192118226601,Published as a conference paper at ICLR 2022
REFERENCES,0.6896551724137931,"A.7
PROOF OF THEOREM 4"
REFERENCES,0.6921182266009852,"Before stating the proof of Theorem 4, we state two existing results that we use."
REFERENCES,0.6945812807881774,"Result 1. (Change of variables formula (DeGroot, 2012)) Given a continuous random variable
X ∈Rd with pdf pX and its transformation Y = f(X), where f : Rd →Rd is a diffeomorphism,8
then pY (f(x))|det(Jf(x))| = pX(x), where Jf is the Jacobian of f computed at x."
REFERENCES,0.6970443349753694,"Lemma 1. If X and Y are two continuous random variables that take values in Rd that are equal in
distribution, i.e., X
d= Y . If f : Rd →Rd is a diffeomorphism, then f(X)
d= f(Y )."
REFERENCES,0.6995073891625616,"Proof. Since X and Y are equal in distribution, they have the same pdfs, i.e. pX(x) = pY (x) for all
x ∈Rd. We can use the change of variables formula in Resut 1 above to get the following. Let W =
f(X), pX(f −1(w))|det(Jf −1(w))| = pW (w) and let V = f(Y ), pY (f −1(v))|det(Jf −1(v))| =
pV (v). Comparing the two expressions when w = v we get pW (w) = pV (w). This proves the
result."
REFERENCES,0.7019704433497537,"We stated Result 1 and Lemma 1 for continuous random variables. When the random variables are
discrete, Lemma 1 holds for any function f.
Lemma 2. (Kass & Vos, 2011) If f : Z →Z and g : Z →Z are diffeomorphisms, then f ◦g is a
diffeomorphism."
REFERENCES,0.7044334975369458,"Proof. We ﬁrst show that Gs
id ⊆Gs
eq."
REFERENCES,0.7068965517241379,"From the observation identity in equation 8 we get that ˜g, { ˜mt}T
t=1 satisfy the following for all
t ∈{1, · · · , T}"
REFERENCES,0.7093596059113301,"g ◦mt
 
g−1(xt), Ut
 d= ˜g ◦˜mt
 
˜g−1(xt), ˆUt
"
REFERENCES,0.7118226600985221,"
˜g−1 ◦g

◦mt
 
g−1(xt), Ut
 d= ˜mt
 
˜g−1(xt), ˆUt

(53)"
REFERENCES,0.7142857142857143,"In the second step in the above equation, we transformed the random variables in the ﬁrst step using
the same transform ˜g−1. ˜g−1 is a diffeomorphism; we compose both sides of the ﬁrst step LHS and
RHS with ˜g−1. We use Lemma 1 to get from the ﬁrst step to the second step in the above equation
equation 53. In the above equation xt is a ﬁxed value and the only source of randomness is from
Ut in LHS and ˆUt in the RHS. We substitute xt = g(zt) to further simplify the above expression in
equation 53"
REFERENCES,0.7167487684729064,"
˜g−1 ◦g

◦mt
 
g−1 ◦g(zt), Ut
 d= ˜mt
 
˜g−1 ◦g(zt), ˆUt
"
REFERENCES,0.7192118226600985,"
˜g−1 ◦g

◦mt
 
zt, ut
 d= ˜mt
 
˜g−1 ◦g(zt), ˆUt

(54)"
REFERENCES,0.7216748768472906,Substitute a = ˜g−1 ◦g in the above to get the following
REFERENCES,0.7241379310344828,"a ◦mt
 
zt, Ut
 d= ˜mt
 
a(zt), ˆUt
"
REFERENCES,0.7266009852216748,"a ◦mt
 
zt, Ut
 d= ˜mt
 
a(zt), Ut

(55)"
REFERENCES,0.729064039408867,"From Lemma 2 it follows that a in the above is a diffeomorphism. Since we assume that ∪T
t=1{mt} =
M∗it follows that a in equation 47 and { ˜mt}T
t=1 (where ˜mt ∈M) together satisfy the condition for
membership in Es. Since ˜g = g ◦a−1 we obtain that ˜g ∈Gs
eq."
REFERENCES,0.7315270935960592,"We now show that Gs
eq ⊆Gs
id. Consider a ˜g ∈Gs
eq. We use ˜g = g ◦a−1, where a ∈Es to simplify the
following random variable"
REFERENCES,0.7339901477832512,"8http://math.mit.edu/˜larsh/teaching/F2007/handouts/changeofvariables.
pdf"
REFERENCES,0.7364532019704434,Published as a conference paper at ICLR 2022
REFERENCES,0.7389162561576355,"g ◦mt(g−1(xt), Ut) =

g ◦a−1 ◦a

◦mt(g−1(xt), Ut) = ˜g ◦a ◦mt(g−1(xt), Ut)
(56)"
REFERENCES,0.7413793103448276,"Since a ∈Es we have
a ◦mt(g−1(t), Ut
 d= ˜mt
 
a ◦g−1(xt), Ut
"
REFERENCES,0.7438423645320197,From Lemma 2 it follows that ˜g is a diffeomorphism. From Lemma 1 it follows that
REFERENCES,0.7463054187192119,"˜g ◦a ◦mt(g−1(xt), Ut)
d= ˜g ◦˜mt(a ◦g−1(xt), Ut) = ˜g ◦˜mt(˜g−1(xt), Ut)
(57)"
REFERENCES,0.7487684729064039,"Combining equation 56 and equation 57 and using the fact that ˆUt
d= Ut we get"
REFERENCES,0.7512315270935961,"g ◦mt(g−1(xt), Ut)
d= ˜g ◦˜mt(˜g−1(xt), Ut)
d= ˜g ◦˜mt(˜g−1(xt), ˆUt)
(58)"
REFERENCES,0.7536945812807881,"From the deﬁnition of Es it follows with the same choice of a the condition continues to hold for all
mt ∈M∗. Therefore, ˜g ∈Gs
id. This proves the second part of the theorem."
REFERENCES,0.7561576354679803,"A.8
PROOF OF THEOREM 5"
REFERENCES,0.7586206896551724,"Proof. In Theorem 4, we showed that all the solutions to the observation identity in equation 8 can
be characterized in terms of the equivariances in distribution deﬁned by the set Es. Let us analyze the
set Es for the class of mechanisms considered in Klindt et al. (2020). Consider a a ∈Es. For each
z ∈Rd"
REFERENCES,0.7610837438423645,"a(z + V )
d= a(z) + ˆV ,
(59)"
REFERENCES,0.7635467980295566,"Deﬁne ˆY = a(z) + ˆV . Since ˆV
d= V we write the probability density function (pdf) of ˆY as"
REFERENCES,0.7660098522167488,"f ˆY (y) = fV (y −a(z))
(60)"
REFERENCES,0.7684729064039408,"Deﬁne Y = a(z + V ). a : Rd →Rd is a diffeomorphism. We use the change of variables result
(Result 1) to write the pdf Y as follows. For each y ∈Rd"
REFERENCES,0.770935960591133,"fY (y) =
1
det
 
J
 
a−1(y)

fV
 
a−1(y) −z

,
(61)"
REFERENCES,0.7733990147783252,"where J(a−1(y)) is the Jacobian of a computed at a−1(y), and det is the determinant."
REFERENCES,0.7758620689655172,"We substitute equation 60 and equation 61 in the equivariance condition in equation 59 to obtain the
following. For each y ∈Rd"
REFERENCES,0.7783251231527094,"Y
d= ˆY"
REFERENCES,0.7807881773399015,fV (y −a(z)) = fV (a−1(y) −z)
REFERENCES,0.7832512315270936,|det(J(a−1(y)))|
REFERENCES,0.7857142857142857,fV (a(w) −a(z)) = fV (w −z)
REFERENCES,0.7881773399014779,"|det(J(w))|, (62)"
REFERENCES,0.7906403940886699,"where w = a−1(y). In the above we equated the conditionals for each z, we now equate the
marginals."
REFERENCES,0.7931034482758621,"g ◦(Z + V )
d= ˜g ◦( ˆZ + ˆV )"
REFERENCES,0.7955665024630542,"a ◦(Z + V )
d= ˆZ + ˆV
(63)"
REFERENCES,0.7980295566502463,Published as a conference paper at ICLR 2022
REFERENCES,0.8004926108374384,"We follow Klindt et al. (2020) and assume ˆZ
d= Z and ˆV
d= V . Therefore, Z + V
d= ˆZ + ˆV . We use
this condition to restate equation 63 as"
REFERENCES,0.8029556650246306,"a ◦(Z + V )
d= Z + V =⇒a(W)
d= W,
(64)
where W = Z + V . We translate equation 64 into the condition on the pdfs as follows. For each
w ∈Rd"
REFERENCES,0.8054187192118226,"fW (w)|det(J(w))| = fW (w) =⇒|det(J(w))| = 1
(65)"
REFERENCES,0.8078817733990148,Substituting the above equation equation 65 into equation 62 we get
REFERENCES,0.8103448275862069,fV (a(w) −a(z)) = fV (w −z)
REFERENCES,0.812807881773399,|det(J(w))| =⇒fV (a(w) −a(z)) = fV (w −z)
REFERENCES,0.8152709359605911,"∥a(w) −a(z)∥α = ∥w −z∥α,
(66)"
REFERENCES,0.8177339901477833,"where in the last condition in the above expression we exploit the fact that fV is a generalized
Laplacian distribution. From Mazur-Ulam theorem Nica (2013) it follows that a is afﬁne. We now
write a as a matrix A with offset vector q and simplify the condition in equation 59."
REFERENCES,0.8201970443349754,For each z ∈Rd we have
REFERENCES,0.8226600985221675,"A(z + V ) + q
d= Az + ˆV + q"
REFERENCES,0.8251231527093597,"=⇒AV
d= ˆV"
REFERENCES,0.8275862068965517,=⇒E[AV V TAT] = E[ ˆV ˆV T] =⇒AAT = I (67)
REFERENCES,0.8300492610837439,"Since A is a square matrix and AAT it follows that ATA = I. Therefore, A is an orthonormal matrix.
Observe that all the elements of ˆV are independent. Since AV
d= ˆV it follows that all the elements
of AV are also independent. Deﬁne AV = Q. Observe that A is an orthonormal matrix that is
multiplied with a vector V with all independent elements (each of which is non-Gaussian as α ̸= 2)
and outputs a vector that has all independent components. From Theorem 11 in Comon (1994) we
get that A is a composition of permutation and scaling. Since A is also orthonormal, each term in the
diagonal scaling matrix can only be 1 or −1. Therefore, A = ΠΛ, where Π is a permutation matrix
and Λ is a diagonal matrix with +1, −1 elements. Finally, a(z) = ΠΛz + q."
REFERENCES,0.8325123152709359,"A.9
ALTERNATIVE IDENTIFICATION RESULT FOR SMALL TRANSITIONS"
REFERENCES,0.8349753694581281,"In this section, we analyze time series models similar to one in Hyvarinen & Morioka (2017) under
the condition that the transitions are small in magnitude to arrive at permutation and scaling based
identiﬁcation. Let us analyze the set Es for this class of mechanisms. We assume that the learner
knows that the mechanism is additive, and that the noise components are all independent. In the
analysis below we consider bijections that are analytic (each component of the bijection is an analytic
function). Consider an a ∈Es"
REFERENCES,0.8374384236453202,"a(z + V )
d= a(z) + ˆV .
(68)"
REFERENCES,0.8399014778325123,We write the ﬁrst-order approximation of the above identity below
REFERENCES,0.8423645320197044,"a(z) + J(z)V
d= a(z) + ˆV"
REFERENCES,0.8448275862068966,"J(z)V
d= ˆV
(69)"
REFERENCES,0.8472906403940886,"where V
d
̸= ˆV . Note that the set of solutions a to equation 68 and equation 69 become equal in
the limit of δ →0, where δ is the bound on each component of |V |. We analyze the solution to
equation 69 below."
REFERENCES,0.8497536945812808,E[(J(z)V )(J(z)V )T] = J(z)E[V V T]J(z)T = σ2J(z)J(z)T
REFERENCES,0.8522167487684729,σ2E[ ˆV ˆV T] = σ2I
REFERENCES,0.854679802955665,σ2J(z)J(z)T = σ2I =⇒J(z)J(z)T = I =⇒J(z)TJ(z) = I (70)
REFERENCES,0.8571428571428571,Published as a conference paper at ICLR 2022
REFERENCES,0.8596059113300493,"Since J(z)V
d= ˆV and each component of ˆV is independent, we can deduce that all the components
of J(z)V are independent as well. From Theorem 11 in Comon (1994), we can deduce that J(z) is
composed of permutation times a diagonal matrix. Since the matrix is orthonormal, each scaling
component can only be ±1. We can apply this same analysis at another point ˜z in the neighborhood of
z and continue to ﬁnd that the jacobian matrix is a permutation times diagonal matrix (that describes
sign ﬂips). Note that the permutation matrix times scaling used to express the Jacobian cannot change
between the points ˜z and z (if it does change then that violates the Jacobian’s continuity). Since the
Jacobian is equal to a ﬁxed permutation times a ﬁxed scaling matrix over a neighborhood, we can
extend this to the entire space (here we use the fact that the a is analytic and Mityagin (2015)). As a
result, a is of the form ΠΛz + q, where Π is a permutation matrix, Λ is a diagonal matrix."
REFERENCES,0.8620689655172413,"A.10
ANALYZING AUXILIARY INFORMATION MODELS"
REFERENCES,0.8645320197044335,"We now discuss how our machinery can be used in models when auxiliary information is available
(Khemakhem et al. (2020a)) to arrive at permutation and scaling based identiﬁcation. We deﬁne
the data generation process compatible with Khemakhem et al. (2020a). Suppose the latent Z is
generated from a mechanism m : O × [0, 1]d that takes as input some observed auxiliary information
O and uniform independent noise vector U ∈[0, 1]d:
Z ←m(O, U),
X ←g(Z),
(71)"
REFERENCES,0.8669950738916257,"where g : Z →X is a bijection. Suppose the learner knows the mechanism. The learner selects ˜g and
outputs ˆX = ˜g ◦m(o, ˜U), where ˜U is a random vector with each component sampled independently
from Uniform[0, 1]. The learner’s goal is to match"
REFERENCES,0.8694581280788177,"ˆX|O = o
d= X|O = o"
REFERENCES,0.8719211822660099,"g ◦m(o, U)
d= ˜g ◦m(o, ˜U).
(72)"
REFERENCES,0.874384236453202,"for all possible observations o ∈O. Deﬁne the set of solutions to equation equation 72 as Go
id.
Consider bijection a s.t. the following identity holds for all o ∈O"
REFERENCES,0.8768472906403941,"a ◦m(o, U)
d= m(o, ˜U)
(73)"
REFERENCES,0.8793103448275862,"Deﬁne the set of all bijections a that satisfy the condition in equation equation 73 as Eo
eq. The set Eo
eq
consists of intersection of measure preserving transformations of Z|O = o. Deﬁne the set of ˜g that
are identiﬁable up to Eo
eq as Go
eq = {˜g | ˜g = g ◦a−1, a ∈Eo
eq}.
Theorem 7. If the data is generated as described in equation 71, then the set of solutions to the
identity in equation 72 identify true g up to intersection of all the measure preserving maps in Eo
eq
(˜g−1 ∼Eoeq g−1)"
REFERENCES,0.8817733990147784,"Proof. Consider a ˜g ∈Go
id."
REFERENCES,0.8842364532019704,"g ◦m(o, U)
d= ˜g ◦m(o, ˜U)"
REFERENCES,0.8866995073891626,"˜g−1 ◦g ◦m(o, U)
d= m(o, ˜u)"
REFERENCES,0.8891625615763546,"a ◦m(o, U)
d= m(o, ˜U) (74)"
REFERENCES,0.8916256157635468,"From the above it follows that ˜g ∈Go
eq. Consider a ˜g ∈Go
eq."
REFERENCES,0.8940886699507389,"g ◦m(o, U) = g ◦a−1 ◦a ◦m(o, U)
d= ˜g ◦m(o, ˜U)
(75)
From the above it follows that ˜g ∈Go
id. This completes the proof."
REFERENCES,0.896551724137931,"Let us consider additive mechanisms of the form m(o, U) = ¯m(o) + U, where U has independent
components. Suppose the learner knows that the mechanism is additive and the noise has independent
components. The learner solves the following identity."
REFERENCES,0.8990147783251231,"g ◦( ¯m(o) + U)
d= ˜g ◦( ¯m
′(o) + ˜U)"
REFERENCES,0.9014778325123153,"˜g−1 ◦g( ¯m(o) + U)
d= ¯m
′(o) + ˜U"
REFERENCES,0.9039408866995073,"a( ¯m(o) + U)
d= ¯m
′(o) + ˜U (76)"
REFERENCES,0.9064039408866995,Published as a conference paper at ICLR 2022
REFERENCES,0.9088669950738916,"Suppose that the absolute value of each component of U is really small and bounded by δ. We can
use the ﬁrst-order Taylor expansion and obtain"
REFERENCES,0.9113300492610837,"a( ¯m(o)) + J( ¯m(o))U
d= ¯m
′(o) + ˜U,
(77)"
REFERENCES,0.9137931034482759,"where J is the Jacobian of a. Suppose the noise has zero mean. Take the expectation w.r.t. U and
˜U on the two sides respectively to get a( ¯m(o)) = ¯m
′(o). Substitute this back into the equation we
get that J( ¯m(o))U
d= ˜U. We can now follow the analysis that we carried out after equation 69 and
conclude that a is equal to permutation times a scaling matrix along with some offset."
REFERENCES,0.916256157635468,"A.11
ANALYZING ˜E WHEN M = M∗"
REFERENCES,0.9187192118226601,"In this section, we analyze imitators when we know the set of mechanisms that are deployed, we
do not know which mechanism is used when. If a ∈˜E and M = M∗. From the deﬁnition of a, it
follows that for each m ∈M∗, ∃m
′ ∈M∗such that a ◦m = m
′ ◦a. We claim that two distinct
m ∈M∗cannot share the same m
′ (imitator). Suppose there was a common m
′ imitating m and ˜m."
REFERENCES,0.9211822660098522,"a ◦m = m
′ ◦a"
REFERENCES,0.9236453201970444,"a ◦˜m = m
′ ◦a
(78)"
REFERENCES,0.9261083743842364,We take the difference of the above two equations to get
REFERENCES,0.9285714285714286,"a ◦m = a ◦˜m
(79)"
REFERENCES,0.9310344827586207,"Since a is a bijection, we can conclude that m = ˜m, which is a contradiction of the fact that m and
˜m are distinct."
REFERENCES,0.9334975369458128,"This claim implies that for a given a there is an injective map from M∗to M∗. If the set M∗is
ﬁnite, then from Pigeonhole principle it follows that this injective map is a bijection."
REFERENCES,0.9359605911330049,"Let us index the mechanism M∗= {m1, · · · , mn}. We call the bijection map π : M∗→M∗"
REFERENCES,0.9384236453201971,"Consider the element i. We claim ∃l ∈{1, · · · , n} πl(i) = i. We write the chain starting from i as
i →π(i) →π2(i), · · · , πk(i). Since the chain (π(i) →π2(i), · · · , πk(i)) has n steps there have to
be at least two elements that are equal. Suppose p > q and πp(i) = πq(i)"
REFERENCES,0.9408866995073891,"πp(i) = πq(i) =⇒πp−1(i) = πq−1(i) =⇒· · · ..πp−q(i) = i
(80)"
REFERENCES,0.9433497536945813,"In the above at each step we use the fact that π is a bijection and that shows the claim that πl(i) = i.
We now use this observation to carry out the following simpliﬁcation"
REFERENCES,0.9458128078817734,mi = a−1 ◦mπ(i) ◦a
REFERENCES,0.9482758620689655,"mπ(i) = a−1 ◦mπ2(i) ◦a
..."
REFERENCES,0.9507389162561576,mπk(i) = a−1 ◦mi ◦a (81)
REFERENCES,0.9532019704433498,"Substituting the second equation mπ(i) into ﬁrst, and then the third mπ2(i) and so on we get"
REFERENCES,0.9556650246305419,"mi = a−k ◦mi ◦ak
(82)"
REFERENCES,0.958128078817734,"Therefore, for each mi, ∃k such that ak is its equivariance."
REFERENCES,0.9605911330049262,"To summarize, if a ∈˜E and M = M∗, where M is a ﬁnite set, then for each mechanism m ∈M,
∃k ∈{1, · · · , |M|} such that ak is its equivariance."
REFERENCES,0.9630541871921182,"A.12
TRANSLATING IDENTITY INTO LOSS FUNCTIONS AND PRELIMINARY EXPERIMENTS"
REFERENCES,0.9655172413793104,We restate the two loss choices based on our identity ˜g ◦m ◦˜g−1(xt) = xt+1 below.
REFERENCES,0.9679802955665024,Published as a conference paper at ICLR 2022
REFERENCES,0.9704433497536946,"• Loss based on observations. The identity above immediately implies an autoencoder-style
algorithm where one minimizes a reconstruction loss of the form"
REFERENCES,0.9729064039408867,"min
˜g∈H,˜h∈H, ˜m∈M X"
REFERENCES,0.9753694581280788,"t
E
h
∥Xt+1 −˜g ◦˜m ◦˜h(Xt)∥2i
(83)"
REFERENCES,0.9778325123152709,"where H is the hypothesis class of functions for ˜g and M is the hypothesis class of mecha-
nisms.
• Loss based on latents. Alternatively, one could re-write the observation identity as g−1 ◦
xt+1 = m ◦g−1 ◦xt and use a contrastive loss as follows"
REFERENCES,0.9802955665024631,"min
˜g X"
REFERENCES,0.9827586206896551,"t
E
h
−log

˜g(Xt+1)T ˜m˜g(Xt)
˜g(Xt+1)T ˜m˜g(Xt) + P"
REFERENCES,0.9852216748768473,τ ˜g(Xτ)T ˜m˜g(Xt)
REFERENCES,0.9876847290640394,"i
(84)"
REFERENCES,0.9901477832512315,"where τ represents other time instances, i.e., τ ̸= t + 1. The positive pair in the contrastive
loss is formed by the adjacent time instances and the negative pair is formed by non-adjacent
time instances. Identifying which of these two losses works better in practice is an important
empirical question. Note how in both the losses above, we have explicitly not enforced
invertibility for the learned g."
REFERENCES,0.9926108374384236,"We present our initial experiments on 3dIdent dataset from [Zimmerman et al. 2021], using the
contrastive loss described above. With contrastive pairs generated by a (ﬁxed) random orthogonal
matrix U applied to the latents, we obtain the following values for linear disentanglement score (R2
of the predictions of the true representation using a linear model). We report median scores over 10
seeds."
REFERENCES,0.9950738916256158,"• Standard contrastive learning. Linear disentanglement score of 0.29
• Contrastive leveraging with exact mechanism knowledge. Leveraging the true U as the
mechanism in the contrastive loss achieves a median score of 0.76."
REFERENCES,0.9975369458128078,"• Contrastive leveraging with some knowledge of mechanism If we use a random orthog-
onal matrix ˜U ̸= U, that achieves a median score of 0.64. The random matrix can be
interpreted as sampling a random m from the hypothesis class M; it seems likely that better
results are possible by optimizing over M."
