Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002631578947368421,"A determinantal point process (DPP) on a collection of M items is a model,
parameterized by a symmetric kernel matrix, that assigns a probability to every
subset of those items. Recent work shows that removing the kernel symmetry
constraint, yielding nonsymmetric DPPs (NDPPs), can lead to significant predictive
performance gains for machine learning applications. However, existing work
leaves open the question of scalable NDPP sampling. There is only one known
DPP sampling algorithm, based on Cholesky decomposition, that can directly apply
to NDPPs as well. Unfortunately, its runtime is cubic in M, and thus does not
scale to large item collections. In this work, we first note that this algorithm can be
transformed into a linear-time one for kernels with low-rank structure. Furthermore,
we develop a scalable sublinear-time rejection sampling algorithm by constructing a
novel proposal distribution. Additionally, we show that imposing certain structural
constraints on the NDPP kernel enables us to bound the rejection rate in a way that
depends only on the kernel rank. In our experiments we compare the speed of all
of these samplers for a variety of real-world tasks."
INTRODUCTION,0.005263157894736842,"1
INTRODUCTION"
INTRODUCTION,0.007894736842105263,"A determinantal point process (DPP) on M items is a model, parameterized by a symmetric kernel
matrix, that assigns a probability to every subset of those items. DPPs have been applied to a wide
range of machine learning tasks, including stochastic gradient descent (SGD) (Zhang et al., 2017),
reinforcement learning (Osogami & Raymond, 2019; Yang et al., 2020), text summarization (Dupuy
& Bach, 2018), coresets (Tremblay et al., 2019), and more. However, a symmetric kernel can only
capture negative correlations between items. Recent works (Brunel, 2018; Gartrell et al., 2019) have
shown that using a nonsymmetric DPP (NDPP) allows modeling of positive correlations as well,
which can lead to significant predictive performance gains. Gartrell et al. (2021) provides scalable
NDPP kernel learning and MAP inference algorithms, but leaves open the question of scalable
sampling. The only known sampling algorithm for NDPPs is the Cholesky-based approach described
in Poulson (2019), which has a runtime of O(M 3) and thus does not scale to large item collections."
INTRODUCTION,0.010526315789473684,"There is a rich body of work on efficient sampling algorithms for (symmetric) DPPs, including recent
works such as Derezinski et al. (2019); Poulson (2019); Calandriello et al. (2020). Key distinctions
between existing sampling algorithms include whether they are for exact or approximate sampling,
whether they assume the DPP kernel has some low-rank K ≪M, and whether they sample from
the space of all 2M subsets or from the restricted space of size-k subsets, so-called k-DPPs. In
the context of MAP inference, influential work, including Summa et al. (2014); Chen et al. (2018);
Hassani et al. (2019); Ebrahimi et al. (2017); Indyk et al. (2020), proposed efficient algorithms that
the approximate (sub)determinant maximization problem and provide rigorous guarantees. In this
work we focus on exact sampling for low-rank kernels, and provide scalable algorithms for NDPPs.
Our contributions are as follows, with runtime and memory details summarized in Table 1:"
INTRODUCTION,0.013157894736842105,"• Linear-time sampling (Section 3):
We show how to transform the O(M 3) Cholesky-
decomposition-based sampler from Poulson (2019) into an O(MK2) sampler for rank-K kernels."
INTRODUCTION,0.015789473684210527,Published as a conference paper at ICLR 2022
INTRODUCTION,0.018421052631578946,"Table 1: Runtime and memory complexities for sampling algorithms developed in this work. M is
the size of the entire item set (ground set), and K is the rank of the kernel (often K ≪M in practice).
We use k by the size of the sampled set (often k ≪K in practice). ω ∈[0, 1] is a data-dependent
constant (with our specific learning scheme, ω ≪1). The sublinear-time rejection algorithm includes
a one-time preprocessing step, after which each successive sample only requires “sampling time”."
INTRODUCTION,0.021052631578947368,"Sampling algorithm
Preprocessing time
Sampling time
Memory"
INTRODUCTION,0.02368421052631579,"Linear-time Cholesky-based
−
O(MK2)
O(MK)
Sublinear-time rejection
O(MK2)
O((k3 log M + k4 + K)(1 + ω)K) ∗
O(MK2)"
INTRODUCTION,0.02631578947368421,∗This assumes some orthogonality constraint on the kernel.
INTRODUCTION,0.02894736842105263,"• Sublinear-time sampling (Section 4): Using rejection sampling, we show how to leverage existing
sublinear-time samplers for symmetric DPPs to implement a sublinear-time sampler for a subclass of
NDPPs that we call orthogonal NDPPs (ONDPPs)."
INTRODUCTION,0.031578947368421054,"• Learning with orthogonality constraints (Section 5): We show that the scalable NDPP kernel
learning of Gartrell et al. (2021) can be slightly modified to impose an orthogonality constraint,
yielding the ONDPP subclass. The constraint allows us to control the rejection sampling algorithm’s
rejection rate, ensuring its scalability. Experiments suggest that the predictive performance of the
kernels is not degraded by this change."
INTRODUCTION,0.034210526315789476,"For a common large-scale setting where M is 1 million, our sublinear-time sampler results in runtime
that is hundreds of times faster than the linear-time sampler. In the same setting, our linear-time
sampler provides runtime that is millions of times faster than the only previously known NDPP
sampling algorithm, which has cubic time complexity and is thus impractical in this scenario."
BACKGROUND,0.03684210526315789,"2
BACKGROUND"
BACKGROUND,0.039473684210526314,"Notation. We use [M] := {1, . . . , M} to denote the set of items 1 through M. We use IK to denote
the K-by-K identity matrix, and often write I := IM when the dimensionality should be clear from
context. Given L ∈RM×M, we use Li,j to denote the entry in the i-th row and j-th column, and
LA,B ∈R|A|×|B| for the submatrix formed by taking rows A and columns B. We also slightly abuse
notation to denote principal submatrices with a single subscript, LA := LA,A."
BACKGROUND,0.042105263157894736,"Kernels. As discussed earlier, both (symmetric) DPPs and NDPPs define a probability distribution
over all 2M subsets of a ground set [M]. The distribution is parameterized by a kernel matrix
L ∈RM×M and the probability of a subset Y ⊆[M] is defined to be Pr(Y ) ∝det(LY ). For this to
define a valid distribution, it must be the case that det(LY ) ≥0 for all Y . For symmetric DPPs, the
non-negativity requirement is identical to a requirement that L be positive semi-definite (PSD). For
nonsymmetric DPPs, there is no such simple correspondence, but prior work such as Gartrell et al.
(2019; 2021) has focused on PSD matrices for simplicity."
BACKGROUND,0.04473684210526316,"Normalizing and marginalizing. The normalizer of a DPP or NDPP distribution can also be written
as a single determinant: P"
BACKGROUND,0.04736842105263158,"Y ⊆[M] det(LY ) = det(L + I) (Kulesza & Taskar, 2012, Theorem 2.1).
Additionally, the marginal probability of a subset can be written as a determinant: Pr(A ⊆Y ) =
det(KA), for K := I −(L + I)−1 (Kulesza & Taskar, 2012, Theorem 2.2)*, where K is typically
called the marginal kernel."
BACKGROUND,0.05,"Intuition. The diagonal element Ki,i is the probability that item i is included in a set sampled from
the model. The 2-by-2 determinant det(K{i,j}) = Ki,iKj,j −Ki,jKj,j is the probability that both
i and j are included in the sample. A symmetric DPP has a symmetric marginal kernel, meaning
Ki,j = Kj,i, and hence Ki,iKj,j −Ki,jKj,i ≤Ki,iKj,j. This implies that the probability of
including both i and j in the sampled set cannot be greater than the product of their individual inclusion
probabilities. Hence, symmetric DPPs can only encode negative correlations. In contrast, NDPPs can
have Ki,j and Kj,i with differing signs, allowing them to also capture positive correlations."
RELATED WORK,0.05263157894736842,"2.1
RELATED WORK
Learning. Gartrell et al. (2021) proposes a low-rank kernel decomposition for NDPPs that admits
linear-time learning. The decomposition takes the form L := V V ⊤+ B(D −D⊤)B⊤for"
RELATED WORK,0.05526315789473684,"*The proofs in Kulesza & Taskar (2012) typically assume a symmetric kernel, but this particular one does
not rely on the symmetry."
RELATED WORK,0.05789473684210526,Published as a conference paper at ICLR 2022
RELATED WORK,0.060526315789473685,"Algorithm 1 Cholesky-based NDPP sampling (Poulson, 2019, Algorithm 1)"
RELATED WORK,0.06315789473684211,"1: procedure SAMPLECHOLESKY(K)
▷marginal kernel factorization Z, W
2:
Y ←∅
Q ←W
3:
for i = 1 to M do
4:
pi ←Ki,i
pi ←z⊤
i Qzi
5:
u ←uniform(0, 1)
6:
if u ≤pi then Y ←Y ∪{i}
7:
else pi ←pi −1"
RELATED WORK,0.06578947368421052,"8:
KA ←KA −KA,iKi,A"
RELATED WORK,0.06842105263157895,"pi
for A := {i + 1, . . . , M}
Q ←Q −Qziz⊤
i Q
pi
9:
return Y"
RELATED WORK,0.07105263157894737,"V , B ∈RM×K, and D ∈RK×K. The V V ⊤component is a rank-K symmetric matrix, which can
model negative correlations between items. The B(D −D⊤)B⊤component is a rank-K skew-
symmetric matrix, which can model positive correlations between items. For compactness of notation,
we will write L = ZXZ⊤, where Z =

V B

∈RM×2K, and X =
h
IK
0
0
D−D⊤
i
∈R2K×2K.
The marginal kernel in this case also has a rank-2K decomposition, as can be shown via application
of the Woodbury matrix identity:"
RELATED WORK,0.07368421052631578,"K := I −(I + L)−1 = ZX
 
I2K + Z⊤ZX
−1 Z⊤.
(1)"
RELATED WORK,0.07631578947368421,"Note that the matrix to be inverted can be computed from Z and X in O(MK2) time, and the inverse
itself takes O(K3) time. Thus, K can be computed from L in time O(MK2). We will develop
sampling algorithms for this decomposition, as well as an orthogonality-constrained version of it. We
use W := X
 
I2K + Z⊤ZX
−1 in what follows so that we can compactly write K = ZW Z⊤."
RELATED WORK,0.07894736842105263,"Sampling. While there are a number of exact sampling algorithms for DPPs with symmetric kernels,
the only published algorithm that clearly can directly apply to NDPPs is from Poulson (2019) (see
Theorem 2 therein). This algorithm begins with an empty set Y = ∅and iterates through the M items,
deciding for each whether or not to include it in Y based on all of the previous inclusion/exclusion
decisions. Poulson (2019) shows, via the Cholesky decomposition, that the necessary conditional
probabilities can be computed as follows:"
RELATED WORK,0.08157894736842106,"Pr (j ∈Y | i ∈Y ) = Pr({i, j} ⊆Y )"
RELATED WORK,0.08421052631578947,"Pr(i ∈Y )
= Kj,j −(Kj,iKi,j) /Ki,i,
(2)"
RELATED WORK,0.0868421052631579,"Pr (j ∈Y | i /∈Y ) = Pr(j ∈Y ) −Pr({i, j} ⊆Y )"
RELATED WORK,0.08947368421052632,"Pr(i /∈Y )
= Kj,j −(Kj,iKi,j) / (Ki,i −1) .
(3)"
RELATED WORK,0.09210526315789473,"Algorithm 1 (left-hand side) gives pseudocode for this Cholesky-based sampling algorithm†.
There has also been some recent work on approximate sampling for fixed-size k-NDPPs: Alimoham-
madi et al. (2021) provide a Markov chain Monte Carlo (MCMC) algorithm and prove that the overall
runtime to approximate ε-close total variation distance is bounded by O(M 2k3 log(1/(ε Pr(Y0))),
where Pr(Y0) is probability of an initial state Y0. Improving this runtime is an interesting avenue for
future work, but for this paper we focus on exact sampling."
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.09473684210526316,"3
LINEAR-TIME CHOLESKY-BASED SAMPLING"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.09736842105263158,"In this section, we show that the O(M 3) runtime of the Cholesky-based sampler from Poulson
(2019) can be significantly improved when using the low-rank kernel decomposition of Gartrell et al.
(2021). First, note that Line 8 of Algorithm 1, where all marginal probabilities are updated via an
(M −i)-by-(M −i) matrix subtraction, is the most costly part of the algorithm, making overall time
and memory complexities O(M 3) and O(M 2), respectively. However, when the DPP kernel is given
by a low-rank decomposition, we observe that marginal probabilities can be updated by matrix-vector"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.1,"†Cholesky decomposition is defined only for a symmetric positive definite matrix. However, we use the
term “Cholesky” from Poulson (2019) to maintain consistency with this work, although Algorithm 1 is valid for
nonsymmetric matrices."
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.10263157894736842,Published as a conference paper at ICLR 2022
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.10526315789473684,"Algorithm 2 Rejection NDPP sampling
(Tree-based sampling)
1: procedure PREPROCESS(V , B, D)
2:
{(σj, y2j−1, y2j)}K/2
j=1 ←YOULADECOMPOSE(B, D)‡"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.10789473684210527,"3:
ˆ
X ←diag
 
IK, σ1, σ1, . . . , σK/2, σK/2
"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.11052631578947368,"4:
Z ←[V , y1, . . . , yK]
{(λi, zi)}2K
i=1 ←EIGENDECOMPOSE(Zˆ
X1/2)
T ←CONSTRUCTTREE(M, [z1, . . . , z2K]⊤)
5:
return Z,ˆ
X
return T , {(λi, zi)}2K
i=1"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.11315789473684211,"6: procedure SAMPLEREJECT(V , B, D, Z, ˆ
X)
▷tree T , eigen pair {(λi, zi)}2K
i=1 of Zˆ
XZ
7:
while true do
8:
Y ←SAMPLEDPP(Zˆ
XZ⊤)
Y ←SAMPLEDPP(T , {(λi, zi)}2K
i=1)
9:
u ←uniform(0, 1)"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.11578947368421053,"10:
p ←det([V V ⊤+B(D−D⊤)B⊤]Y )"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.11842105263157894,"det([Zˆ
XZ⊤]Y )
11:
if u ≤p then break
12:
return Y"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.12105263157894737,"multiplications of dimension 2K, regardless of M. In more detail, suppose we have the marginal
kernel K = ZW Z⊤as in Eq. (1) and let zj be the j-th row vector in Z. Then, for i ̸= j:"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.12368421052631579,"Pr (j ∈Y | i ∈Y ) = Kj,j −(Kj,iKi,j)/Ki,i = z⊤
j"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.12631578947368421,"
W −(W zi)(z⊤
i W )
z⊤
i W zi"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.12894736842105264,"
zj,
(4)"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.13157894736842105,"Pr (j ∈Y | i /∈Y ) = z⊤
j"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.13421052631578947,"
W −(W zi)(z⊤
i W )
z⊤
i W zi −1"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.1368421052631579,"
zj.
(5)"
LINEAR-TIME CHOLESKY-BASED SAMPLING,0.1394736842105263,"The conditional probabilities in Eqs. (4) and (5) are of bilinear form, and the zj do not change during
sampling. Hence, it is enough to update the 2K-by-2K inner matrix at each iteration, and obtain the
marginal probability by multiplying this matrix by zi. The details are shown on the right-hand side of
Algorithm 1. The overall time and memory complexities are O(MK2) and O(MK), respectively."
SUBLINEAR-TIME REJECTION SAMPLING,0.14210526315789473,"4
SUBLINEAR-TIME REJECTION SAMPLING"
SUBLINEAR-TIME REJECTION SAMPLING,0.14473684210526316,"Although the Cholesky-based sampler runs in time linear in M, even this is too expensive for the
large M that are often encountered in real-world datasets. To improve runtime, we consider rejection
sampling (Von Neumann, 1963). Let p be the target distribution that we aim to sample, and let
q be any distribution whose support corresponds to that of p; we call q the proposal distribution.
Assume that there is a universal constant U such that p(x) ≤Uq(x) for all x. In this setting, rejection
sampling draws a sample x from q and accepts it with probability p(x)/(Uq(x)), repeating until an
acceptance occurs. The distribution of the resulting samples is p. It is important to choose a good
proposal distribution q so that sampling is efficient and the number of rejections is small."
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.14736842105263157,"4.1
PROPOSAL DPP CONSTRUCTION
Our first goal is to find a proposal DPP with symmetric kernel bL that can upper-bound all probabilities
of samples from the NDPP with kernel L within a constant factor. To this end, we expand the
determinant of a principal submatrix, det(LY ), using the spectral decomposition of the NDPP kernel.
Such a decomposition essentially amounts to combining the eigendecomposition of the symmetric
part of L with the Youla decomposition (Youla, 1961) of the skew-symmetric part."
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.15,"Specifically, suppose {(σj, y2j−1, y2j)}K/2
j=1 is the Youla decomposition of B(D −D⊤)B⊤(see
Appendix D for more details), that is,"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.15263157894736842,"B(D −D⊤)B⊤= K/2
X"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.15526315789473685,"j=1
σj
 
y2j−1y⊤
2j −y2jy⊤
2j−1

.
(6)"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.15789473684210525,‡Pseudo-code of YOULADECOMPOSE is provided in Algorithm 4. See Appendix D.
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.16052631578947368,Published as a conference paper at ICLR 2022
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.1631578947368421,"Then we can simply write L = ZXZ⊤, for Z := [V , y1, . . . , yK] ∈RM×2K, and"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.16578947368421051,"X := diag

IK,
 0
σ1
−σ1
0"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.16842105263157894,"
, . . . ,

0
σK/2
−σK/2
0"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.17105263157894737,"
.
(7)"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.1736842105263158,"Now, consider defining a related but symmetric PSD kernel bL
:=
Zˆ
XZ⊤with ˆ
X
:=
diag
 
IK, σ1, σ1, . . . , σK/2, σK/2

. All determinants of the principal submatrices of bL = Zˆ
XZ⊤
upper-bound those of L, as stated below."
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.1763157894736842,"Theorem 1. For every subset Y ⊆[M], it holds that det(LY ) ≤det(bLY ). Moreover, equality holds
when the size of Y is equal to the rank of L."
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.17894736842105263,"Proof sketch: From the Cauchy-Binet formula, the determinants of LY and bLY for all Y ⊆
[M], |Y | ≤2K can be represented as"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.18157894736842106,"det(LY ) =
X"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.18421052631578946,"I⊆[K],|I|=|Y | X"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.1868421052631579,"J⊆[K],|J|=|Y |
det(XI,J) det(ZY,I) det(ZY,J),
(8)"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.18947368421052632,"det(bLY ) =
X"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.19210526315789472,"I⊆[2K],|I|=|Y |
det(ˆ
XI) det(ZY,I)2.
(9)"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.19473684210526315,"Many of the terms in Eq. (8) are actually zero due to the block-diagonal structure of X. For example,
note that if 1 ∈I but 1 /∈J, then there is an all-zeros row in XI,J, making det(XI,J) = 0. We show
that each XI,J with nonzero determinant is a block-diagonal matrix with diagonal entries among"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.19736842105263158,"±σj, or
h
0
σj
−σj
0
i
. With this observation, we can prove that det(XI,J) is upper-bounded by det(ˆ
XI)"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.2,"or det(ˆ
XJ). Then, through application of the rearrangement inequality, we can upper-bound the sum
of the det(XI,J) det(ZY,I) det(ZY,J) in Eq. (8) with a sum over det(ˆ
XI) det(ZY,I)2. Finally, we
show that the number of non-zero terms in Eq. (8) is identical to the number of non-zero terms in
Eq. (9). Combining these gives us the desired inequality det(LY ) ≤det(bLY ). The full proof of
Theorem 1 is in Appendix E.1."
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.2026315789473684,"Now, recall that the normalizer of a DPP (or NDPP) with kernel L is det(L + I). The ratio of
probability of the NDPP with kernel L to that of a DPP with kernel bL is thus:"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.20526315789473684,"PrL(Y )
PrbL(Y ) = det(LY )/ det(L + I)"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.20789473684210527,"det(bLY )/ det(bL + I)
≤det(bL + I)"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.21052631578947367,"det(L + I),"
"PROPOSAL DPP CONSTRUCTION
OUR FIRST GOAL IS TO FIND A PROPOSAL DPP WITH SYMMETRIC KERNEL BL THAT CAN UPPER-BOUND ALL PROBABILITIES",0.2131578947368421,"where the inequality follows from Theorem 1. This gives us the necessary universal constant U
upper-bounding the ratio of the target distribution to the proposal distribution. Hence, given a sample
Y drawn from the DPP with kernel bL, we can use acceptance probability PrL(Y )/(U PrbL(Y )) =
det(LY )/ det(bLY ). Pseudo-codes for proposal construction and rejection sampling are given in
Algorithm 2. Note that to derive bL from L it suffices to run the Youla decomposition of B(D −
D⊤)B⊤, because the difference is only in the skew-symmetric part. This decomposition can run in
O(MK2) time; more details are provided in Appendix D. Since bL is a symmetric PSD matrix, we
can apply existing fast DPP sampling algorithms to sample from it. In particular, in the next section
we combine a fast tree-based method with rejection sampling."
SUBLINEAR-TIME TREE-BASED SAMPLING,0.21578947368421053,"4.2
SUBLINEAR-TIME TREE-BASED SAMPLING
There are several DPP sampling algorithms that run in sublinear time, such as tree-based (Gillenwater
et al., 2019) and intermediate (Derezinski et al., 2019) sampling algorithms. Here, we consider
applying the former, a tree-based approach, to sample from the proposal distribution defined by bL."
SUBLINEAR-TIME TREE-BASED SAMPLING,0.21842105263157896,"We give some details of the sampling procedure, as in the course of applying it we discovered an
optimization that slightly improves on the runtime of prior work. Formally, let {(λi, zi)}2K
i=1 be the
eigendecomposition of bL and Z := [z1, . . . , z2K] ∈RM×2K. As shown in Kulesza & Taskar (2012,
Lemma 2.6), for every Y ⊆[M], |Y | ≤2K, the probability of Y under DPP with bL can be written:"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.22105263157894736,"PrbL(Y ) =
det(bLY )"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.2236842105263158,"det(bL + I)
=
X"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.22631578947368422,"E⊆[2K],|E|=|Y |
det(ZY,EZ⊤
Y,E)
Y i∈E"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.22894736842105262,"λi
λi + 1 Y i/∈E"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.23157894736842105,"1
λi + 1.
(10)"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.23421052631578948,Published as a conference paper at ICLR 2022
SUBLINEAR-TIME TREE-BASED SAMPLING,0.23684210526315788,"Algorithm 3 Tree-based DPP sampling (Gillenwater et al., 2019)"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.2394736842105263,"1: procedure BRANCH(A, Z)
2:
if A = {j} then
3:
T .A ←{j}, T .Σ ←Z⊤
j,:Zj,:
4:
return T
5:
Aℓ, Ar ←Split A in half
6:
T .left ←BRANCH(Aℓ, Z)
7:
T .right ←BRANCH(Ar, Z)
8:
T .Σ ←T .left.Σ + T .right.Σ
9:
return T"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.24210526315789474,"10: procedure CONSTRUCTTREE(M, Z)
11:
return BRANCH([M], Z)"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.24473684210526317,"12: procedure SAMPLEDPP(T , Z, {λi}K
i=1)
13:
E ←∅, Y ←∅, QY ←0
14:
for i = 1, . . . , K do
15:
E ←E ∪{i} w.p. λi/(λi + 1)
16:
for k = 1, . . . , |E| do
17:
j ←SAMPLEITEM(T , QY , E)
18:
Y ←Y ∪{j}"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.24736842105263157,"19:
QY ←I|E|−Z⊤
Y,E
 
ZY,EZ⊤
Y,E
−1 ZY,E"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.25,"20:
return Y"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.25263157894736843,"21: procedure SAMPLEITEM(T , QY , E)
22:
if T is a leaf then return T .A
23:
pℓ←

T .left.ΣE, QY"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.25526315789473686,"24:
pr ←

T .right.ΣE, QY"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.2578947368421053,"25:
u ←uniform(0, 1)
26:
if u ≤
pℓ
pℓ+pr then"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.26052631578947366,"27:
return SAMPLEITEM(T .left, QY , E)
28:
else
29:
return SAMPLEITEM(T .right, QY , E)"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.2631578947368421,"A matrix of the form Z:,EZ⊤
:,E can be a valid marginal kernel for a special type of DPP, called an
elementary DPP. Hence, Eq. (10) can be thought of as DPP probabilities expressed as a mixture of
elementary DPPs. Based on this mixture view, DPP sampling can be done in two steps: (1) choose
an elementary DPP according to its mixture weight, and then (2) sample a subset from the selected
elementary DPP. Step (1) can be performed by 2K independent random coin tossings, while step
(2) involves computational overhead. The key idea of tree-based sampling is that step (2) can be
accelerated by traversing a binary tree structure, which can be done in time logarithmic in M."
SUBLINEAR-TIME TREE-BASED SAMPLING,0.2657894736842105,"More specifically, given the marginal kernel K = Z:,EZ⊤
:,E, where E is obtained from step (1), we
start from the empty set Y = ∅and repeatedly add an item j to Y with probability:"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.26842105263157895,"Pr(j ∈S | Y ⊆S) = Kj,j −Kj,Y (KY )−1KY,j = Zj,EQY Z⊤
j,E =

QY , (Z⊤
j,:Zj,:)E

, (11)"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.2710526315789474,"where S is some final selected subset, and QY := I|E| −Z⊤
Y,E
 
ZY,EZ⊤
Y,E
−1 ZY,E. Consider a
binary tree whose root includes a ground set [M]. Every non-leaf node contains a subset A ⊆[M] and
stores a 2K-by-2K matrix P"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.2736842105263158,"j∈A Z⊤
j,:Zj,:. A partition Aℓand Ar, such that Aℓ∪Ar = A, Aℓ∩Ar =
∅, are passed to its left and right subtree, respectively. The resulting tree has M leaves and each has
exactly a single item. Then, one can sample a single item by recursively moving down to the left
node with probability:"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.27631578947368424,"pℓ=
⟨QY , P"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.2789473684210526,"j∈Aℓ(Z⊤
j,:Zj,:)E⟩"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.28157894736842104,"⟨QY , P"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.28421052631578947,"j∈A(Zj,:Z⊤
j,:)E⟩,
(12)"
SUBLINEAR-TIME TREE-BASED SAMPLING,0.2868421052631579,"or to the right node with probability 1 −pℓ, until reaching a leaf node. An item in the leaf node is
chosen with probability according to Eq. (11). Since every subset in the support of an elementary DPP
with a rank-k kernel has exactly k items, this process is repeated for |E| iterations. Full descriptions
of tree construction and sampling are provided in Algorithm 3. The proposed tree-based rejection
sampling for an NDPP is outlined on the right-side of Algorithm 2. The one-time pre-processing
step of constructing the tree (CONSTRUCTTREE) requires O(MK2) time. After pre-processing, the
procedure SAMPLEDPP involves |E| traversals of a tree of depth O(log M), where in each node a
O(|E|2) operation is required. The overall runtime is summarized in Proposition 1 and the proof can
be found in Appendix E.2.
Proposition 1. The tree-based sampling procedure SAMPLEDPP in Algorithm 3 runs in time
O(K + k3 log M + k4), where k is the size of the sampled set§."
SUBLINEAR-TIME TREE-BASED SAMPLING,0.2894736842105263,§Computing pℓvia Eq. (12) improves on Gillenwater et al. (2019)’s O(k4 log M) runtime for this step.
SUBLINEAR-TIME TREE-BASED SAMPLING,0.29210526315789476,Published as a conference paper at ICLR 2022
AVERAGE NUMBER OF REJECTIONS,0.29473684210526313,"4.3
AVERAGE NUMBER OF REJECTIONS
We now return to rejection sampling and focus on the expected number of rejections. The number
of rejections of Algorithm 2 is known to be a geometric random variable with mean equal to the
constant U used to upper-bound the ratio of the target distribution to the proposal distribution:
det(bL + I)/ det(L + I). If all columns in V and B are orthogonal, which we denote V ⊥B, then
the expected number of rejections depends only on the eigenvalues of the skew-symmetric part of the
NDPP kernel.
Theorem 2. Given an NDPP kernel L = V V ⊤+ B(D −D⊤)B⊤for V , B ∈RM×K, D ∈
RK×K, consider the proposal kernel bL as proposed in Section 4.1. Let {σj}K/2
j=1 be the positive"
AVERAGE NUMBER OF REJECTIONS,0.29736842105263156,"eigenvalues obtained from the Youla decomposition of B(D−D⊤)B⊤. If V ⊥B, then det(bL+I)"
AVERAGE NUMBER OF REJECTIONS,0.3,"det(L+I) =
QK/2
j=1

1 +
2σj
σ2
j +1

≤(1 + ω)K/2, where ω = 2"
AVERAGE NUMBER OF REJECTIONS,0.3026315789473684,"K
PK/2
j=1
2σj
σ2
j +1 ∈(0, 1]."
AVERAGE NUMBER OF REJECTIONS,0.30526315789473685,"Proof sketch: Orthogonality between V and B allows det(L + I) to be expressed just in terms of the
eigenvalues of V V ⊤and B(D −D⊤)B⊤. Since both L and bL share the symmetric part V V ⊤,
the ratio of determinants only depends on the skew-symmetric part. A more formal proof appears in
Appendix E.3."
AVERAGE NUMBER OF REJECTIONS,0.3078947368421053,"Assuming we have a kernel where V ⊥B, we can combine Theorem 2 with the tree-based rejection
sampling algorithm (right-side in Algorithm 2) to sample in time O((K +k3 log M +k4)(1+ω)K/2).
Hence, we have a sampling algorithm that is sublinear in M, and can be much faster than the Cholesky-
based algorithm when (1 + ω)K/2 ≪M. In the next section, we introduce a learning scheme with
the V ⊥B constraint, as well as regularization to ensure that ω is small."
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.3105263157894737,"5
LEARNING WITH ORTHOGONALITY CONSTRAINTS"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.3131578947368421,"We aim to learn a NDPP that provides both good predictive performance and a low rejection rate. We
parameterize our NDPP kernel matrix L = V V ⊤+ B(D −D⊤)B⊤by"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.3157894736842105,"D = diag
0
σ1
0
0"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.31842105263157894,"
, . . . ,
0
σK/2
0
0"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.32105263157894737,"
(13)"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.3236842105263158,"for σj ≥0, B⊤B = I, and, motivated by Theorem 2, require V ⊤B = 0¶. We call such
orthogonality-constrained NDPPs “ONDPPs”. Notice that if V ⊥B, then L has the full rank
of 2K, since the intersection of the column spaces spanned by V and by B is empty, and thus the
full rank available for modeling can be used. Thus, this constraint can also be thought of as simply
ensuring that ONDPPs use the full rank available to them."
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.3263157894736842,"Given example subsets {Y1, . . . , Yn} as training data, learning is done by minimizing the regularized
negative log-likelihood:"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.32894736842105265,"min
V ,B,{σj}K/2
j=1
−1 n n
X"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.33157894736842103,"i=1
log
 det(LYi)"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.33421052631578946,"det(L + I) 
+ α M
X i=1"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.3368421052631579,"∥vi∥2
2
µi
+ β M
X i=1"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.3394736842105263,"∥bi∥2
2
µi
+ γ K/2
X"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.34210526315789475,"j=1
log "
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.3447368421052632,"1 +
2σj
σ2
j + 1 !"
LEARNING WITH ORTHOGONALITY CONSTRAINTS,0.3473684210526316,"(14)
where α, β, γ > 0 are hyperparameters, µi is the frequency of item i in the training data, and vi
and bi represent the rows of V and B, respectively. This objective is very similar to that of Gartrell
et al. (2021), except for the orthogonality constraint and the final regularization term. Note that this
regularization term corresponds exactly to the logarithm of the average rejection rate, and therefore
should help to control the number of rejections."
EXPERIMENTS,0.35,"6
EXPERIMENTS"
EXPERIMENTS,0.3526315789473684,"We first show that the orthogonality constraint from Section 5 does not degrade the predictive
performance of learned kernels. We then compare the speed of our proposed sampling algorithms."
EXPERIMENTS,0.35526315789473684,"¶Technical details: To learn NDPP models with the constraint V ⊤B = 0, we project V according
to: V ←V −B(B⊤B)−1(B⊤V ). For the B⊤B = I constraint, we apply QR decomposition on B.
Note that both operations require O(MK2) time. (Constrained learning and sampling code is provided at
https://github.com/insuhan/nonsymmetric-dpp-sampling. We use Pytorch’s linalg.solve to avoid the
expense of explicitly computing the (B⊤B)−1 inverse.) Hence, our learning time complexity is identical to
that of Gartrell et al. (2021)."
EXPERIMENTS,0.35789473684210527,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.3605263157894737,"Table 2: Average MPR and AUC, with 95% confidence estimates obtained via bootstrapping, test
log-likelihood, and the number of rejections for NDPP models. Bold values indicate the best MPR,
outside of the confidence intervals of the two baseline methods."
EXPERIMENTS,0.3631578947368421,"Low-rank DPP Models
Metric
UK Retail
M=3,941
Recipe
M=7,993
Instacart
M=49,677
Million Song
M=371,410
Book
M=1,059,437"
EXPERIMENTS,0.36578947368421055,"Symmetric DPP
(Gartrell et al., 2017)"
EXPERIMENTS,0.3684210526315789,"MPR
76.42 ± 0.97
95.04 ± 0.69
93.06 ± 0.92
90.00 ± 1.18
72.54 ± 2.03
AUC
0.74 ± 0.01
0.99 ± 0.01
0.86 ± 0.01
0.77 ± 0.01
0.70 ± 0.01
Log-Likelihood
-104.89
-44.63
-73.22
-310.14
-149.76"
EXPERIMENTS,0.37105263157894736,"NDPP
(Gartrell et al., 2021)"
EXPERIMENTS,0.3736842105263158,"MPR
77.09 ± 1.10
95.17 ± 0.67
92.40 ± 1.05
89.00 ± 1.11
72.98 ± 1.46
AUC
0.74 ± 0.01
0.99 ± 0.00
0.87 ± 0.01
0.80 ± 0.01
0.74 ± 0.01
Log-Likelihood
-99.09
-44.72
-74.94
-314.12
-149.93
# of Rejections
4.136 ×1010
78.95
6.806 ×103
3.907 ×1010
9.245×106"
EXPERIMENTS,0.3763157894736842,"ONDPP
without regularization"
EXPERIMENTS,0.37894736842105264,"MPR
78.43 ± 0.95
95.40 ± 0.62
92.80 ± 0.99
93.02 ± 0.83
75.35 ± 1.83
AUC
0.71 ± 0.00
0.99 ± 0.01
0.83 ± 0.01
0.77 ± 0.01
0.64 ± 0.01
Log-Likelihood
-99.45
-44.60
-72.69
-302.64
-140.53
# of Rejections
1.818 ×109
103.81
128.96
5.563 ×107
682.22"
EXPERIMENTS,0.3815789473684211,"ONDPP
with regularization"
EXPERIMENTS,0.38421052631578945,"MPR
77.12 ± 0.98
95.50 ± 0.59
92.99 ± 0.95
92.86 ± 0.80
75.73 ± 1.84
AUC
0.72 ± 0.01
0.99 ± 0.01
0.83 ± 0.01
0.77 ± 0.01
0.64 ± 0.01
Log-Likelihood
-103.83
-44.56
-72.72
-305.66
-140.67
# of Rejections
26.09
21.59
79.74
45.42
61.10"
PREDICTIVE PERFORMANCE RESULTS FOR NDPP LEARNING,0.3868421052631579,"6.1
PREDICTIVE PERFORMANCE RESULTS FOR NDPP LEARNING
We benchmark various DPP models, including symmetric (Gartrell et al., 2017), nonsymmetric for
scalable learning (Gartrell et al., 2021), as well as our ONDPP kernels with and without rejection rate
regularization. We use the scalable NDPP models (Gartrell et al., 2021) as a baseline||. The kernel
components of each model are learned using five real-world recommendation datasets, which have
ground set sizes that range from 3,941 to 1,059,437 items (see Appendix A for more details)."
PREDICTIVE PERFORMANCE RESULTS FOR NDPP LEARNING,0.3894736842105263,"Our experimental setup and metrics mirror those of Gartrell et al. (2021). We report the mean
percentile rank (MPR) metric for a next-item prediction task, the AUC metric for subset discrimination,
and the log-likelihood of the test set; see Appendix B for more details on the experiments and metrics.
For all metrics, higher numbers are better. For NDPP models, we additionally report the average
rejection rates when they apply to rejection sampling."
PREDICTIVE PERFORMANCE RESULTS FOR NDPP LEARNING,0.39210526315789473,"In Table 2, we observe that the predictive performance of our ONDPP models generally match or
sometimes exceed the baseline. This is likely because the orthogonality constraint enables more
effective use of the full rank-2K feature space. Moreover, imposing the regularization on rejection
rate, as shown in Eq. (14), often leads to dramatically smaller rejection rates, while the impact on
predictive performance is generally marginal. These results justify the ONDPP and regularization
for fast sampling. Finally, we observe that the learning time of our ONDPP models is typically a bit
longer than that of the NDPP models, but still quite reasonable (e.g., the time per iteration for the
NDPP takes 27 seconds for the Book dataset, while our ONDPP takes 49.7 seconds)."
PREDICTIVE PERFORMANCE RESULTS FOR NDPP LEARNING,0.39473684210526316,"Fig. 1 shows how the regularizer γ affects the test log-likelihood and the average number of rejections.
We see that γ degrades predictive performance and reduces the rejection rate when set above a certain
threshold; this behavior is seen for many datasets. However, for the Recipe dataset we observed that
the test log-likelihood is not very sensitive to γ, likely because all models in our experiments achieve
very high performance on this dataset. In general, we observe that γ can be set to a value that results
in a small rejection rate, while having minimal impact on predictive performance."
SAMPLING TIME COMPARISON,0.3973684210526316,"6.2
SAMPLING TIME COMPARISON
We benchmark the Cholesky-based sampling algorithm (Algorithm 1) and tree-based rejection
sampling algorithm (Algorithm 2) on ONDPPs with both synthetic and real-world data."
SAMPLING TIME COMPARISON,0.4,"||We use the code from https://github.com/cgartrel/scalable-nonsymmetric-DPPs for the NDPP
baseline, which is made available under the MIT license. To simplify learning and MAP inference, Gartrell
et al. (2021) set B = V in their experiments. However, since we have the V ⊥B constraint in our ONDPP
approach, we cannot set B = V . Hence, for a fair comparison, we do not set B = V for the NDPP baseline in
our experiments, and thus the results in Table 2 differ slightly from those published in Gartrell et al. (2021)."
SAMPLING TIME COMPARISON,0.4026315789473684,Published as a conference paper at ICLR 2022
SAMPLING TIME COMPARISON,0.4052631578947368,"10−8 10−6 10−4 10−2
100"
SAMPLING TIME COMPARISON,0.40789473684210525,regularizer γ 101 104 107 1010
SAMPLING TIME COMPARISON,0.4105263157894737,average # of rejections (a)
SAMPLING TIME COMPARISON,0.4131578947368421,"10−6 10−4 10−2
100"
SAMPLING TIME COMPARISON,0.41578947368421054,regularizer γ −106 −104 −102 −100 −98
SAMPLING TIME COMPARISON,0.41842105263157897,test log-likelihood
SAMPLING TIME COMPARISON,0.42105263157894735,"(b)
Figure 1: Average number of rejections and test
log-likelihood with different values of the regu-
larizer γ for ONDPPs trained on the UK Retail
dataset. Shaded regions are 95% confidence inter-
vals of 10 independent trials."
SAMPLING TIME COMPARISON,0.4236842105263158,"212
214
216
218
220"
SAMPLING TIME COMPARISON,0.4263157894736842,ground set size M 101 102 103
SAMPLING TIME COMPARISON,0.42894736842105263,sampling time (sec)
SAMPLING TIME COMPARISON,0.43157894736842106,"Cholesky-based
Rejection (a)"
SAMPLING TIME COMPARISON,0.4342105263157895,"212
214
216
218
220"
SAMPLING TIME COMPARISON,0.4368421052631579,ground set size M 10−1 101 103
SAMPLING TIME COMPARISON,0.4394736842105263,preprocessing time (sec)
SAMPLING TIME COMPARISON,0.4421052631578947,"Tree construction
Spectral decomposition"
SAMPLING TIME COMPARISON,0.44473684210526315,"(b)
Figure 2: Wall-clock time (sec) for synthetic data
for (a) NDPP sampling algorithms and (b) prepro-
cessing steps for the rejection sampling. Shaded
regions are 95% confidence intervals from 100
independent trials."
SAMPLING TIME COMPARISON,0.4473684210526316,"Table 3: Wall-clock time (sec) for preprocessing and sampling ONDPPs trained on real-world data,
and speedup of the tree-based sampler over the Cholesky-based one. We set K = 100 and provide
average times with 95% confidence intervals from 10 independent trials for the Cholesky-based
algorithm and 100 trials for the rejection algorithm. Memory usage for the tree is also reported."
SAMPLING TIME COMPARISON,0.45,"UK Retail
M=3,941
Recipe
M=7,993
Instacart
M=49,677
Million Song
M=371,410
Book
M=1,059,437"
SAMPLING TIME COMPARISON,0.45263157894736844,"Spectral decomposition
0.209
0.226
0.505
2.639
7.482
Tree construction
0.997
1.998
12.65
119.0
340.1"
SAMPLING TIME COMPARISON,0.45526315789473687,"Cholesky-based sampling
5.572 ± 0.056
11.36 ± 0.098
71.82 ± 1.087
545.8 ± 8.776
1,631 ± 11.84
Tree-based rejection sampling
2.463 ± 0.417
1.331 ± 0.241
5.962 ± 1.049
14.72 ± 2.620
6.627 ± 1.294
(Speedup)
(×2.262)
(×8.535)
(×12.05)
(× 37.08)
(×246.1)"
SAMPLING TIME COMPARISON,0.45789473684210524,"Tree memory usage
630.5 MB
1.279 GB
7.948 GB
59.43 GB
169.5 GB"
SAMPLING TIME COMPARISON,0.4605263157894737,"Synthetic datasets. We generate non-uniform random features for V , B as done by (Han &
Gillenwater, 2020). In particular, we first sample x1, . . . , x100 from N(0, I2K/(2K)), and integers
t1, . . . , t100 from Poisson distribution with mean 5, rescaling the integers such that P
i ti = M. Next,
we draw ti random vectors from N(xi, I2K), and assign the first K-dimensional vectors as the row
vectors of V and the latter vectors as those of B. Each entry of D is sampled from N(0, 1). We
choose K = 100 and vary M from 212 to 220."
SAMPLING TIME COMPARISON,0.4631578947368421,"Fig. 2(a) illustrates the runtimes of Algorithms 1 and 2. We verify that the rejection sampling
time tends to increase sub-linearly with the ground set size M, while the Cholesky-based sampler
runs in linear time. In Fig. 2(b), the runtimes of the preprocessing steps for Algorithm 2 (i.e.,
spectral decomposition and tree construction) are reported. Although the rejection sampler requires
these additional processes, they are one-time steps and run much faster than a single run of the
Choleksy-based method for M = 220."
SAMPLING TIME COMPARISON,0.46578947368421053,"Real-world datasets. In Table 3, we report the runtimes and speedup of NDPP sampling algorithms
for real-world datasets. All NDPP kernels are obtained using learning with orthogonality constraints,
with rejection rate regularization as reported in Section 6.1. We observe that the tree-based rejection
sampling runs up to 246 times faster than the Cholesky-based algorithm. For larger datasets, we
expect that this gap would significantly increase. As with the synthetic experiments, we see that the
tree construction pre-processing time is comparable to the time required to draw a single sample via
the other methods, and thus the tree-based method is often the best choice for repeated sampling**."
CONCLUSION,0.46842105263157896,"7
CONCLUSION"
CONCLUSION,0.4710526315789474,"In this work we developed scalable sampling methods for NDPPs. One limitation of our rejection
sampler is its practical restriction to the ONDPP subclass. Other opportunities for future work
include the extension of our rejection sampling approach to the generation of fixed-size samples
(from k-NDPPs), the development of approximate sampling techniques, and the extension of DPP
samplers along the lines of Derezinski et al. (2019); Calandriello et al. (2020) to NDPPs. Scalable
sampling also opens the door to using NDPPs as building blocks in probabilistic models."
CONCLUSION,0.47368421052631576,"**We note that the tree can consume substantial memory, e.g., 169.5 GB for the Book dataset with K = 100.
For settings where this scale of memory use is unacceptable, we suggest use of the intermediate sampling
algorithm (Calandriello et al., 2020) in place of tree-based sampling. The resulting sampling algorithm may be
slower, but the O(M + K) memory cost is substantially lower."
CONCLUSION,0.4763157894736842,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.4789473684210526,"8
ETHICS STATEMENT"
ETHICS STATEMENT,0.48157894736842105,"In general, our work moves in a positive direction by substantially decreasing the computational
costs of NDPP sampling. When using our constrained learning method to learn kernels from user
data, we recommend employing a technique such as differentially-private SGD (Abadi et al., 2016)
to help prevent user data leaks, and adjusting the weights on training examples to balance the impact
of sub-groups of users so as to make the final kernel as fair as possible. As far as we are aware, the
datasets used in this work do not contain personally identifiable information or offensive content.
We were not able to determine if user consent was explicitly obtained by the organizations that
constructed these datasets."
REPRODUCIBILITY STATEMENT,0.4842105263157895,"9
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.4868421052631579,"We have made extensive effort to ensure that all algorithmic, theoretical, and experimental contribu-
tions described in this work are reproducible. All of the code implementing our constrained learning
and sampling algorithms is publicly available ††. The proofs for our theoretical contributions are
available in Appendix E. For our experiments, all dataset processing steps, experimental procedures,
and hyperparameter settings are described in Appendices A, B, and C, respectively."
ACKNOWLEDGEMENTS,0.48947368421052634,"10
ACKNOWLEDGEMENTS"
ACKNOWLEDGEMENTS,0.4921052631578947,"Amin Karbasi acknowledges funding in direct support of this work from NSF (IIS-1845032) and
ONR (N00014-19-1-2406)."
REFERENCES,0.49473684210526314,REFERENCES
REFERENCES,0.49736842105263157,"Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar,
and Li Zhang.
Deep Learning with Differential Privacy.
In Conference on Computer and
Communications Security, 2016."
REFERENCES,0.5,"Yeganeh Alimohammadi, Nima Anari, Kirankumar Shiragur, and Thuy-Duong Vuong. Fractionally
log-concave and sector-stable polynomials: counting planar matchings and more. In Symposium
on the Theory of Computing (STOC), 2021."
REFERENCES,0.5026315789473684,"Victor-Emmanuel Brunel. Learning Signed Determinantal Point Processes through the Principal
Minor Assignment Problem. In Neural Information Processing Systems (NeurIPS), 2018."
REFERENCES,0.5052631578947369,"Daniele Calandriello, Michal Derezinski, and Michal Valko. Sampling from a k-DPP without looking
at all items. In Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.5078947368421053,"Daqing Chen, Sai Laing Sain, and Kun Guo. Data mining for the online retail industry: A case study
of RFM model-based customer segmentation using data mining. Journal of Database Marketing
& Customer Strategy Management, 2012."
REFERENCES,0.5105263157894737,"Laming Chen, Guoxin Zhang, and Eric Zhou.
Fast greedy MAP inference for Determinantal
Point Process to improve recommendation diversity. In Neural Information Processing Systems
(NeurIPS), 2018."
REFERENCES,0.5131578947368421,"Michal Derezinski, Daniele Calandriello, and Michal Valko. Exact sampling of determinantal point
processes with sublinear time preprocessing. Neural Information Processing Systems (NeurIPS),
2019."
REFERENCES,0.5157894736842106,"Christophe Dupuy and Francis Bach. Learning determinantal point processes in sublinear time. In
Conference on Artificial Intelligence and Statistics (AISTATS), 2018."
REFERENCES,0.5184210526315789,"Javad Ebrahimi, Damian Straszak, and Nisheeth Vishnoi. Subdeterminant maximization via noncon-
vex relaxations and anti-concentration. Foundations of Computer Science (FOCS), 2017."
REFERENCES,0.5210526315789473,††https://github.com/insuhan/nonsymmetric-dpp-sampling
REFERENCES,0.5236842105263158,Published as a conference paper at ICLR 2022
REFERENCES,0.5263157894736842,"Mike Gartrell, Ulrich Paquet, and Noam Koenigstein. Low-Rank Factorization of Determinantal
Point Processes. In Conference on Artificial Intelligence (AAAI), 2017."
REFERENCES,0.5289473684210526,"Mike Gartrell, Victor-Emmanuel Brunel, Elvis Dohmatob, and Syrine Krichene. Learning Non-
symmetric Determinantal Point Processes. In Neural Information Processing Systems (NeurIPS),
2019."
REFERENCES,0.531578947368421,"Mike Gartrell, Insu Han, Elvis Dohmatob, Jennifer Gillenwater, and Victor-Emmanuel Brunel."
REFERENCES,0.5342105263157895,"Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes. In
International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.5368421052631579,"Jennifer Gillenwater, Alex Kulesza, Zelda Mariet, and Sergei Vassilvtiskii. A Tree-Based Method
for Fast Repeated Sampling of Determinantal Point Processes. In International Conference on
Machine Learning (ICML), 2019."
REFERENCES,0.5394736842105263,"Insu Han and Jennifer Gillenwater. MAP Inference for Customized Determinantal Point Processes via
Maximum Inner Product Search. In Conference on Artificial Intelligence and Statistics (AISTATS),
2020."
REFERENCES,0.5421052631578948,"Hamed Hassani, Amin Karbasi, Aryan Mokhtari, and Zebang Shen. Stochastic conditional gradient++.
SIAM Journal on Optimization (SIOPT), 2019."
REFERENCES,0.5447368421052632,"Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative Filtering for Implicit Feedback Datasets.
In International Conference on Data Mining (ICDM), 2008."
REFERENCES,0.5473684210526316,"Piotr Indyk, Sepideh Mahabadi, Shayan Oveis Gharan, and Alireza Rezaei. Composable core-sets for
determinant maximization problems via spectral spanners. In Symposium on Discrete Algorithms
(SODA), 2020."
REFERENCES,0.55,"Instacart. The Instacart Online Grocery Shopping Dataset, 2017. URL https://www.instacart.
com/datasets/grocery-shopping-2017. Accessed May 2020."
REFERENCES,0.5526315789473685,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015."
REFERENCES,0.5552631578947368,"Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Foundations
and Trends® in Machine Learning, 2012."
REFERENCES,0.5578947368421052,"Yanen Li, Jia Hu, ChengXiang Zhai, and Ye Chen. Improving One-class Collaborative Filtering by
Incorporating Rich User Information. In Conference on Information and Knowledge Management
(CIKM), 2010."
REFERENCES,0.5605263157894737,"Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, and Julian J McAuley. Generating Person-
alized Recipes from Historical User Preferences. In Empirical Methods in Natural Language
Processing (EMNLP), 2019."
REFERENCES,0.5631578947368421,"Brian McFee, Thierry Bertin-Mahieux, Daniel PW Ellis, and Gert RG Lanckriet. The million song
dataset challenge. In International Conference on the World Wide Web (WWW), 2012."
REFERENCES,0.5657894736842105,"Yuji Nakatsukasa. The low-rank eigenvalue problem. arXiv preprint arXiv:1905.11490, 2019."
REFERENCES,0.5684210526315789,"Takayuki Osogami and Rudy Raymond. Determinantal reinforcement learning. In Conference on
Artificial Intelligence (AAAI), 2019."
REFERENCES,0.5710526315789474,"Jack Poulson. High-performance sampling of generic Determinantal Point Processes. arXiv preprint
arXiv:1905.00165, 2019."
REFERENCES,0.5736842105263158,"Jocelyn Quaintance. Combinatorial Identities: Table I: Intermediate Techniques for Summing Finite
Series, volume 3. 2010."
REFERENCES,0.5763157894736842,"Marco Di Summa, Friedrich Eisenbrand, Yuri Faenza, and Carsten Moldenhauer. On largest volume
simplices and sub-determinants. In Symposium on Discrete Algorithms (SODA), 2014."
REFERENCES,0.5789473684210527,"Nicolas Tremblay, Simon Barthelmé, and Pierre-Olivier Amblard. Determinantal Point Processes for
Coresets. Journal of Machine Learning Research (JMLR), 2019."
REFERENCES,0.5815789473684211,Published as a conference paper at ICLR 2022
REFERENCES,0.5842105263157895,"John Von Neumann. Various techniques used in connection with random digits. John von Neumann,
Collected Works, 1963."
REFERENCES,0.5868421052631579,"Mengting Wan and Julian McAuley. Item recommendation on monotonic behavior chains. In
Conference on Recommender Systems (RecSys), 2018."
REFERENCES,0.5894736842105263,"Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang."
REFERENCES,0.5921052631578947,"Multi-agent determinantal q-learning. In International Conference on Machine Learning (ICML),
2020."
REFERENCES,0.5947368421052631,"DC Youla. A normal form for a matrix under the unitary congruence group. Canadian Journal of
Mathematics, 1961."
REFERENCES,0.5973684210526315,"Cheng Zhang, Hedvig Kjellström, and Stephan Mandt. Determinantal Point Processes for Mini-Batch
Diversification. In Conference on Uncertainty in Artificial Intelligence (UAI), 2017."
REFERENCES,0.6,Published as a conference paper at ICLR 2022
REFERENCES,0.6026315789473684,"A
FULL DETAILS ON DATASETS"
REFERENCES,0.6052631578947368,We perform experiments on several real-world public datasets composed of subsets:
REFERENCES,0.6078947368421053,"• UK Retail: This dataset (Chen et al., 2012) contains baskets representing transactions from an
online retail company that sells all-occasion gifts. We omit baskets with more than 100 items, leaving
us with a dataset containing 19,762 baskets drawn from a catalog of M = 3,941 products. Baskets
containing more than 100 items are in the long tail of the basket-size distribution, so omitting these is
reasonable, and allows us to use a low-rank factorization of the NDPP with K = 100."
REFERENCES,0.6105263157894737,"• Recipe: This dataset (Majumder et al., 2019) contains recipes and food reviews from Food.com
(formerly Genius Kitchen)‡‡. Each recipe (“basket”) is composed of a collection of ingredients,
resulting in 178,265 recipes and a catalog of 7,993 ingredients.
• Instacart: This dataset (Instacart, 2017) contains baskets purchased by Instacart users§§. We omit
baskets with more than 100 items, resulting in 3.2 million baskets and a catalog of 49,677 products."
REFERENCES,0.6131578947368421,"• Million Song: This dataset (McFee et al., 2012) contains playlists (“baskets”) of songs from Echo
Nest users¶¶. We trim playlists with more than 100 items, leaving 968,674 playlists and a catalog of
371,410 songs."
REFERENCES,0.6157894736842106,"• Book: This dataset (Wan & McAuley, 2018) contains reviews from the Goodreads book review
website, including a variety of attributes describing the items***. For each user we build a subset
(“basket”) containing the books reviewed by that user. We trim subsets with more than 100 books,
resulting in 430,563 subsets and a catalog of 1,059,437 books."
REFERENCES,0.618421052631579,"As far as we are aware, these datasets do not contain personally identifiable information or offensive
content. While the UK Retail dataset is publicly available, we were unable to find a license for it.
Also, we were not able to determine if user consent was explicitly obtained by the organizations that
constructed these datasets."
REFERENCES,0.6210526315789474,"B
FULL DETAILS ON EXPERIMENTAL SETUP AND METRICS"
REFERENCES,0.6236842105263158,"We use 300 randomly-selected baskets as a held-out validation set, for tracking convergence during
training and for tuning hyperparameters. Another 2000 random baskets are used for testing, and
the rest are used for training. Convergence is reached during training when the relative change in
validation log-likelihood is below a predetermined threshold. We use PyTorch with Adam (Kingma &
Ba, 2015) for optimization. We initialize D from the standard Gaussian distribution N(0, 1), while
V and B are initialized from the uniform(0, 1) distribution."
REFERENCES,0.6263157894736842,"Subset expansion task. We use greedy conditioning to do next-item prediction (Gartrell et al., 2021,
Section 4.2). We compare methods using a standard recommender system metric: mean percentile
rank (MPR) (Hu et al., 2008; Li et al., 2010). MPR of 50 is equivalent to random selection; MPR
of 100 means that the model perfectly predicts the next item. See Appendix B.1 for a complete
description of the MPR metric."
REFERENCES,0.6289473684210526,"Subset discrimination task. We also test the ability of a model to discriminate observed subsets
from randomly generated ones. For each subset in the test set, we generate a subset of the same
length by drawing items uniformly at random (and we ensure that the same item is not drawn more
than once for a subset). We compute the AUC for the model on these observed and random subsets,
where the score for each subset is the log-likelihood that the model assigns to the subset."
REFERENCES,0.631578947368421,"‡‡See https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions for the li-
cense for this public dataset.
§§This public dataset is available for non-commercial use; see https://www.instacart.com/datasets/
grocery-shopping-2017 for the license.
¶¶See http://millionsongdataset.com/faq/ for the license for this public dataset.
***This public dataset is available for academic use only; see https://sites.google.com/eng.ucsd.edu/
ucsdbookgraph/home for the license."
REFERENCES,0.6342105263157894,Published as a conference paper at ICLR 2022
REFERENCES,0.6368421052631579,"B.1
MEAN PERCENTILE RANK"
REFERENCES,0.6394736842105263,"We begin our definition of MPR by defining percentile rank (PR). First, given a set J, let pi,J =
Pr(J ∪{i} | J). The percentile rank of an item i given a set J is defined as"
REFERENCES,0.6421052631578947,"PRi,J ="
REFERENCES,0.6447368421052632,"P
i′̸∈J 1(pi,J ≥pi′,J)"
REFERENCES,0.6473684210526316,"|Y\J|
× 100%"
REFERENCES,0.65,where Y\J indicates those elements in the ground set Y that are not found in J.
REFERENCES,0.6526315789473685,"For our evaluation, given a test set Y , we select a random element i ∈Y and compute PRi,Y \{i}. We
then average over the set of all test instances T to compute the mean percentile rank (MPR):"
REFERENCES,0.6552631578947369,"MPR =
1
|T | X"
REFERENCES,0.6578947368421053,"Y ∈T
PRi,Y \{i}."
REFERENCES,0.6605263157894737,"C
HYPERPARAMETERS FOR EXPERIMENTS"
REFERENCES,0.6631578947368421,"Preventing numerical instabilities: The det(LYi) in Eq. (14) will be zero whenever |Yi| > K,
where Yi is an observed subset. To address this in practice we set K to the size of the largest subset
observed in the data, K′, as in Gartrell et al. (2017). However, this does not entirely fix the issue,
as there is still a chance that the term will be zero even when |Yi| ≤K. In this case though, we
know that we are not at a maximum, since the value of the objective function is −∞. Numerically,
to prevent such singularities, in our implementation we add a small ϵI correction to each LYi when
optimizing Eq. (14) (ϵ = 10−5 in our experiments)."
REFERENCES,0.6657894736842105,"We perform a grid search using a held-out validation set to select the best-performing hyperparameters
for each model and dataset. The hyperparameter settings used for each model and dataset are described
below."
REFERENCES,0.6684210526315789,"Symmetric low-rank DPP (Gartrell et al., 2017). For this model, we use K for the number of item
feature dimensions for the symmetric component V , and α for the regularization hyperparameter for
V . We use the following hyperparameter settings:"
REFERENCES,0.6710526315789473,"• UK Retail dataset: K = 100, α = 1.
• Recipe dataset: K = 100, α = 0.01
• Instacart dataset: K = 100, α = 0.001.
• Million Song dataset: K = 100, α = 0.0001.
• Book dataset: K = 100, α = 0.001"
REFERENCES,0.6736842105263158,"Scalable NDPP (Gartrell et al., 2021). As described in Section 2.1, we use K to denote the number of
item feature dimensions for the symmetric component V and the dimensionality of the nonsymmetric
component D. α and β are the regularization hyperparameters. We use the following hyperparameter
settings:"
REFERENCES,0.6763157894736842,"• UK dataset: K = 100, α = 0.01.
• Recipe dataset: K = 100, α = β = 0.01.
• Instacart dataset: K = 100, α = 0.001.
• Million Song dataset: K = 100, α = 0.01.
• Book dataset: K = 100, α = β = 0.1"
REFERENCES,0.6789473684210526,"ONDPP. As described in Section 5, we use K to denote the number of item feature dimensions for
the symmetric component V and the dimensionality of the nonsymmetric component C. α, β, and γ
are the regularization hyperparameters. We use the following hyperparameter settings:"
REFERENCES,0.6815789473684211,"• UK dataset: K = 100, α = β = 0.01, γ = 0.5.
• Recipe dataset: K = 100, α = β = 0.01, γ = 0.1.
• Instacart dataset: K = 100, α = β = 0.001, γ = 0.001.
• Million Song dataset: K = 100, α = β = 0.01, γ = 0.2."
REFERENCES,0.6842105263157895,Published as a conference paper at ICLR 2022
REFERENCES,0.6868421052631579,"• Book dataset: K = 100, α = β = 0.01, γ = 0.1."
REFERENCES,0.6894736842105263,"For all of the above model configurations and datasets, we use a batch size of 800 during training."
REFERENCES,0.6921052631578948,"D
YOULA DECOMPOSITION: SPECTRAL DECOMPOSITION FOR
SKEW-SYMMETRIC MATRIX"
REFERENCES,0.6947368421052632,"We provide some basic facts on the spectral decomposition of a skew-symmetric matrix, and introduce
an efficient algorithm for this decomposition when it is given by a low-rank factorization. We write
i := √−1 and vH as the conjugate transpose of v ∈CM, and denote Re(z) and Im(z) by the real
and imaginary parts of a complex number z, respectively."
REFERENCES,0.6973684210526315,"Given B ∈RM×K and D ∈RK×K, consider a rank-K skew-symmetric matrix B(D −D⊤)B⊤.
Note that all nonzero eigenvalues of a real-valued skew-symmetric matrix are purely imaginary.
Denote iσ1, −iσ1, . . . , iσK/2, −iσK/2 by its nonzero eigenvalues where each of σj is real, and
a1 + ib1, a1 −ib1, . . . aK/2 + ibK/2, aK/2 −ibK/2 by the corresponding eigenvectors for aj, bj ∈
RM, which come in conjugate pairs. Then, we can write"
REFERENCES,0.7,"B(D −D⊤)B⊤= K/2
X"
REFERENCES,0.7026315789473684,"j=1
iσj(aj + ibj)(aj + ibj)H −iσj(aj −ibj)(aj −ibj)H
(15) = K/2
X"
REFERENCES,0.7052631578947368,"j=1
2σj(ajb⊤
j −bja⊤
j )
(16) = K/2
X j=1"
REFERENCES,0.7078947368421052,"aj −bj aj + bj
 
0
σj
−σj
0"
REFERENCES,0.7105263157894737," a⊤
j −b⊤
j
a⊤
j + b⊤
j"
REFERENCES,0.7131578947368421,"
.
(17)"
REFERENCES,0.7157894736842105,"Note
that
a1 ± b1, . . . , aK/2 ± bK/2
are
real-valued
orthonormal
vectors,
because
a1, b1, . . . , aK/2, bK/2 are orthogonal to each other and ∥aj ± bj∥2
2 = ∥aj∥2
2 + ∥bj∥2
2 = 1 for"
REFERENCES,0.718421052631579,"all j. The pair {(σj, aj −bj, aj + bj)}K/2
j=1 is often called the Youla decomposition (Youla, 1961) of
B(D −D⊤)B⊤. To efficiently compute the Youla decomposition of a rank-K matrix, we use the
following result."
REFERENCES,0.7210526315789474,"Proposition 2 (Proposition 1, Nakatsukasa (2019)). Given A, B ∈CM×K, the nonzero eigenvalues
of AB⊤∈CM×M and B⊤A ∈CK×K are identical. In addition, if (λ, v) is an eigenpair of B⊤A
with λ ̸= 0, then (λ, Av/ ∥Av∥2) is an eigenpair of AB⊤."
REFERENCES,0.7236842105263158,"From the above proposition, one can first compute (D −D⊤)B⊤B and then apply the eigende-
composition to that K-by-K matrix. Taking the imaginary part of the obtained eigenvalues gives
us the σj’s, and multiplying B by the eigenvectors gives us the eigenvectors of B(D −D⊤)B⊤.
In addition, this can be done in O(MK2 + K3) time; when M > K it runs much faster than the
eigendecomposition of B(D −D⊤)B⊤, which requires O(M 3) time. The pseudo-code of the
Youla decomposition is provided in Algorithm 4."
REFERENCES,0.7263157894736842,Algorithm 4 Youla decomposition of low-rank skew-symmetric matrix
REFERENCES,0.7289473684210527,"1: procedure YOULADECOMPOSITION(B, D)
2:
{(ηj, zj), (ηj, zj)}K/2
j=1 ←eigendecomposition of (D −D⊤)B⊤B
3:
for j = 1, . . . , K/2 do
4:
σj ←Im(ηj) for j = 1, . . . , K/2
5:
y2j−1 ←B (Re(zj) −Im(zj))
6:
y2j ←B (Re(zj) + Im(zj))"
REFERENCES,0.7315789473684211,"7:
yj ←yj/ ∥yj∥for j = 1, . . . , K"
REFERENCES,0.7342105263157894,"8:
return {(σj, y2j−1, y2j)}K/2
j=1"
REFERENCES,0.7368421052631579,Published as a conference paper at ICLR 2022
REFERENCES,0.7394736842105263,"E
PROOFS"
REFERENCES,0.7421052631578947,"E.1
PROOF OF THEOREM 1"
REFERENCES,0.7447368421052631,"Theorem 1. For every subset Y ⊆[M], it holds that det(LY ) ≤det(bLY ). Moreover, equality holds
when the size of Y is equal to the rank of L."
REFERENCES,0.7473684210526316,"Proof of Theorem 1. It is enough to fix Y ⊆[M] such that 1 ≤|Y | ≤2K, because the rank of both
L and bL is up to 2K. Denote k := |Y | and
 [2K]
k
 := {I ⊆[2K]; |I| = k} for k ≤2K. We recall
the definition of bL: given V , B, D such that L = V V ⊤+ B(D −D⊤)B⊤, let {(ρi, vi)}K
i=1
be the eigendecomposition of V V ⊤and {(σj, y2j−1, y2j)}K/2
j=1 be the Youla decomposition of
B(D −D⊤)B⊤. Denote Z := [v1, . . . , vK, y1, . . . , yK] ∈RM×2K and"
REFERENCES,0.75,"X := diag

ρ, . . . , ρK,

0
σ1
−σ1
0"
REFERENCES,0.7526315789473684,"
, . . . ,

0
σK/2
−σK/2
0 
,"
REFERENCES,0.7552631578947369,"ˆ
X := diag

ρ1, . . . , ρK,

σ1
0
0
σ1"
REFERENCES,0.7578947368421053,"
, . . . ,

σK/2
0
0
σK/2 
,"
REFERENCES,0.7605263157894737,"so that L = ZXZ⊤and bL = Zˆ
XZ⊤. Applying the Cauchy-Binet formula twice, we can write the
determinant of the principal submatrices of both L and bL:"
REFERENCES,0.7631578947368421,"det(LY ) =
X"
REFERENCES,0.7657894736842106,"I∈(
[2K]
k ) X"
REFERENCES,0.7684210526315789,"J∈(
[2K]
k )
det(XI,J) det(ZY,I) det(ZY,J),
(18)"
REFERENCES,0.7710526315789473,"det(bLY ) =
X"
REFERENCES,0.7736842105263158,"I∈(
[2K]
k ) X"
REFERENCES,0.7763157894736842,"J∈(
[2K]
k )
det(ˆ
XI,J) det(ZY,I) det(ZY,J) =
X"
REFERENCES,0.7789473684210526,"I∈(
[2K]
k )
det(ˆ
XI) det(ZY,I)2,
(19)"
REFERENCES,0.781578947368421,"where Eq. (19) follows from the fact that ˆ
X is diagonal, which means that det(ˆ
XI,J) = 0 for I ̸= J."
REFERENCES,0.7842105263157895,"When the size of Y is equal to the rank of L (i.e., k = 2K), the summations in Eqs. (18) and (19)
simplify to single terms: det(LY ) = det(X) det(ZY,:)2 and det(bLY ) = det(ˆ
X) det(ZY,:)2. Now,
observe that the determinants of the full X and ˆ
X matrices are identical: det(X) = det(ˆ
X) =
QK
i=1 ρi
QK/2
j=1 σ2
j . Hence, it holds that det(LY ) = det(bLY ). This proves the second statement of
the theorem."
REFERENCES,0.7868421052631579,"To prove that det(LY ) ≤det(bLY ) for smaller subsets Y , we will use the following:"
REFERENCES,0.7894736842105263,"Claim 1. For every I, J ∈
 [2K]
k

such that det(XI,J) ̸= 0, there exists a (nonempty) collection of
subset pairs S(I, J) ⊆
 [2K]
k

×
 [2K]
k

such that
X"
REFERENCES,0.7921052631578948,"(I′,J′)∈S(I,J)
det(XI,J) det(ZY,I) det(ZY,J) ≤
X"
REFERENCES,0.7947368421052632,"(I′,J′)∈S(I,J)
det(ˆ
XI,I) det(ZY,I)2.
(20)"
REFERENCES,0.7973684210526316,Claim 2. The number of nonzero terms in Eq. (18) is identical to that in Eq. (19).
REFERENCES,0.8,Combining Claim 1 with Claim 2 yields
REFERENCES,0.8026315789473685,"det(LY ) =
X"
REFERENCES,0.8052631578947368,"I,J∈(
[2K]
k )
det(XI,J) det(ZY,I) det(ZY,J) ≤
X"
REFERENCES,0.8078947368421052,"I∈(
[2K]
k )
det(ˆ
XI,I) det(ZY,I)2 = det(bLY )."
REFERENCES,0.8105263157894737,We conclude the proof of Theorem 1. Below we provide proofs for Claim 1 and Claim 2.
REFERENCES,0.8131578947368421,"Proof of Claim 1. Recall that X is a block-diagonal matrix, where each block is of size either
1-by-1, containing ρi, or 2-by-2, containing both σj and −σj in the form
h
0
σj
−σj
0
i
. A submatrix"
REFERENCES,0.8157894736842105,"XI,J ∈Rk×k with rows I and columns J will only have a nonzero determinant if it contains no"
REFERENCES,0.8184210526315789,Published as a conference paper at ICLR 2022
REFERENCES,0.8210526315789474,"all-zero row or column. Hence, any XI,J with nonzero determinant will have the following form (or
some permutation of this block-diagonal):"
REFERENCES,0.8236842105263158,"XI,J = "
REFERENCES,0.8263157894736842,
REFERENCES,0.8289473684210527,"ρp1
· · ·
0
...
...
...
0
0
. . .
ρp|P I,J |
±σq1
· · ·
0
...
...
...
0
· · ·
±σq|QI,J |
0
σr1
−σr1
0
...
0
0
σr|RI,J |
−σr|RI,J |
0 "
REFERENCES,0.8315789473684211, (21)
REFERENCES,0.8342105263157895,"and we denote P I,J := {p1, . . . , p|P I,J|}, QI,J := {q1, . . . , q|QI,J|}, and RI,J := {r1, . . . , r|RI,J|}.
Indices p ∈P I,J yield a diagonal matrix with entries ρp. For such p, both I and J must contain index
p. Indices r ∈RI,J yield a block-diagonal matrix of the form

0
σr
−σr
0

. For such r, both I and J
must contain a pair of indices, (K + 2r −1, K + 2r). Finally, indices q ∈QI,J yield a diagonal
matrix with entries of ±σq (the sign can be + or −). For such q, I contains K + 2q −1 or K + 2q,
and J must contain the other. Note that there is no intersection between QI,J and RI,J."
REFERENCES,0.8368421052631579,"If QI,J is an empty set (i.e., I = J), then det(XI,J) = det(ˆ
XI,J) and"
REFERENCES,0.8394736842105263,"det(XI,J) det(ZY,I) det(ZY,J) = det(ˆ
XI) det(ZY,I)2.
(22)"
REFERENCES,0.8421052631578947,"Thus, the terms in Eq. (18) in this case appear in Eq. (19). Now assume that QI,J ̸= ∅and consider
the following set of pairs:"
REFERENCES,0.8447368421052631,"S(I, J) := {(I′, J′) : P I,J = P I′,J′, QI,J = QI′,J′, RI,J = RI′,J′}."
REFERENCES,0.8473684210526315,"In other words, for (I′, J′) ∈S(I, J), the diagonal XI′,J′ contains ρp,

0
σr
−σr
0

exactly as in XI,J.
However, the signs of the σr’s may differ from XI,J. Combining this observation with the definition
of ˆ
X,"
REFERENCES,0.85,"|det(XI′,J′)| = |det(XI,J)| = det(ˆ
XI) = det(ˆ
XI′) = det(ˆ
XJ) = det(ˆ
XJ′).
(23)"
REFERENCES,0.8526315789473684,"Therefore,
X"
REFERENCES,0.8552631578947368,"(I′,J′)∈S(I,J)
det(XI′,J′) det(ZY,I′) det(ZY,J′)
(24) ≤
X"
REFERENCES,0.8578947368421053,"(I′,J′)∈S(I,J)
|det(XI′,J′)| det(ZY,I′) det(ZY,J′)
(25)"
REFERENCES,0.8605263157894737,"= det(ˆ
XI)
X"
REFERENCES,0.8631578947368421,"(I′,J′)∈S(I,J)
det(ZY,I′) det(ZY,J′)
(26)"
REFERENCES,0.8657894736842106,"≤det(ˆ
XI)
X"
REFERENCES,0.868421052631579,"(I′,∗)∈S(I,J)
det(ZY,I′)2
(27) =
X"
REFERENCES,0.8710526315789474,"(I′,∗)∈S(I,J)
det(ˆ
XI′) det(ZY,I′)2
(28)"
REFERENCES,0.8736842105263158,"where the third line comes from Eq. (23) and the fourth line follows from the rearrangement inequality.
Note that application of this inequality does not change the number of terms in the sum. This
completes the proof of Claim 1."
REFERENCES,0.8763157894736842,Published as a conference paper at ICLR 2022
REFERENCES,0.8789473684210526,"Proof of Claim 2. In Eq. (19), observe that det(ˆ
XI) det(ZY,I)2 ̸= 0 if and only if det(ˆ
XI) ̸= 0.
Since all ρi’s and σj’s are positive, the number of I ⊆[2K], |I| = k such that det(ˆ
XI) ̸= 0 is
equal to
 2K
k

. Similarly, the number of nonzero terms in Eq. (18) equals the number of possible
choices of I, J ∈
 [2K]
k

such that det(XI,J) ̸= 0. This can be counted as follows: first choose i"
REFERENCES,0.881578947368421,"items in {ρ1, . . . , ρK} for i = 0, . . . , k; then, choose j items in
n
0
σ1
−σ1 0

, . . . ,
h
0
σK/2
−σK/2
0
io
for"
REFERENCES,0.8842105263157894,"j = 0, . . . , ⌊k−i"
REFERENCES,0.8868421052631579,"2 ⌋; lastly, choose k −i −2j of {±σq; q /∈RI,J}, then choose the sign for each of
these (σq or −σq). Combining all of these choices, the total number of nonzero terms is: k
X i=0 K
i "
REFERENCES,0.8894736842105263,"| {z }
choice of ρp ⌊k−i 2 ⌋
X j=0"
REFERENCES,0.8921052631578947,"K/2
j "
REFERENCES,0.8947368421052632,| {z }
REFERENCES,0.8973684210526316,"choice of
h
0
σr
−σr
0 i"
REFERENCES,0.9," K/2 −j
k −i −2j"
REFERENCES,0.9026315789473685,"
2k−i−2j"
REFERENCES,0.9052631578947369,"|
{z
}
choice of ±σq (29) = k
X i=0 K
i"
REFERENCES,0.9078947368421053,"  K
k −i"
REFERENCES,0.9105263157894737,"
(30)"
REFERENCES,0.9131578947368421,"=
2K
k"
REFERENCES,0.9157894736842105,"
(31)"
REFERENCES,0.9184210526315789,"where the second line comes from the fact that
 2n
m

= P⌊m"
REFERENCES,0.9210526315789473,"2 ⌋
j=0
 n
j
  n−j
m−2j

2m−2j for any integers
n, m ∈N such that m ≤2n (see (1.69) in Quaintance (2010)), and the third line follows from the fact
that Pr
i=0
 m
i
  n
r−i

=
 n+m
r

for n, m, r ∈N (Vandermonde’s identity). Hence, both the number
of nonzero terms in Eqs. (18) and (19) is equal to
 2K
k

. This completes the proof of Claim 2."
REFERENCES,0.9236842105263158,"E.2
PROOF OF PROPOSITION 1"
REFERENCES,0.9263157894736842,"Proposition 1. The tree-based sampling procedure SAMPLEDPP in Algorithm 3 runs in time
O(K + k3 log M + k4), where k is the size of the sampled set†††."
REFERENCES,0.9289473684210526,"Proof of Proposition 1. Since computing pℓtakes O(k2) from Eq. (12), and since the binary tree
has depth O(log M), SAMPLEITEM in Algorithm 3 runs in O(k2 log M) time. Moreover, the query
matrix QY can be updated in O(k3) time as it only requires a k-by-k matrix inversion. Therefore,
the overall runtime of the tree-based elementary DPP sampling algorithm (after pre-processing) is
O(k3 log M + k4). This improves the previous O(k4 log M) runtime studied in Gillenwater et al.
(2019). Combining this with elementary DPP selection (Line 15 in Algorithm 3), we can sample a set
in O(K + k3 log M + k4) time. This completes the proof of Proposition 1."
REFERENCES,0.9315789473684211,"E.3
PROOF OF THEOREM 2"
REFERENCES,0.9342105263157895,"Theorem 2. Given an NDPP kernel L = V V ⊤+ B(D −D⊤)B⊤for V , B ∈RM×K, D ∈
RK×K, consider the proposal kernel bL as proposed in Section 4.1. Let {σj}K/2
j=1 be the positive"
REFERENCES,0.9368421052631579,"eigenvalues obtained from the Youla decomposition of B(D−D⊤)B⊤. If V ⊥B, then det(bL+I)"
REFERENCES,0.9394736842105263,"det(L+I) =
QK/2
j=1

1 +
2σj
σ2
j +1

≤(1 + ω)K/2, where ω = 2"
REFERENCES,0.9421052631578948,"K
PK/2
j=1
2σj
σ2
j +1 ∈(0, 1]."
REFERENCES,0.9447368421052632,"Proof of Theorem 2. Since the column spaces of V and B are orthogonal, the corresponding
eigenvectors are also orthogonal, i.e., Z⊤Z = I2K. Then,"
REFERENCES,0.9473684210526315,"det(L + I) = det(ZXZ⊤+ I) = det(XZ⊤Z + I2K) = det(X + I2K)
(32) = K
Y"
REFERENCES,0.95,"i=1
(ρi + 1) K/2
Y"
REFERENCES,0.9526315789473684,"j=1
det

1
σj
−σj
1"
REFERENCES,0.9552631578947368,"
(33) = K
Y"
REFERENCES,0.9578947368421052,"i=1
(ρi + 1) K/2
Y"
REFERENCES,0.9605263157894737,"j=1
(σ2
j + 1)
(34)"
REFERENCES,0.9631578947368421,†††Computing pℓvia Eq. (12) improves on Gillenwater et al. (2019)’s O(k4 log M) runtime for this step.
REFERENCES,0.9657894736842105,Published as a conference paper at ICLR 2022
REFERENCES,0.968421052631579,and similarly
REFERENCES,0.9710526315789474,"det(bL + I) = K
Y"
REFERENCES,0.9736842105263158,"i=1
(ρi + 1) K/2
Y"
REFERENCES,0.9763157894736842,"j=1
(σj + 1)2.
(35)"
REFERENCES,0.9789473684210527,"Combining Eqs. (34) and (35), we have that"
REFERENCES,0.9815789473684211,"det(bL + I)
det(L + I) = K/2
Y j=1"
REFERENCES,0.9842105263157894,(σj + 1)2
REFERENCES,0.9868421052631579,"(σ2
j + 1) = K/2
Y j=1 "
REFERENCES,0.9894736842105263,"1 +
2σj
σ2
j + 1 ! ≤ "
REFERENCES,0.9921052631578947,"1 + 2 K K/2
X j=1"
REFERENCES,0.9947368421052631,"2σj
σ2
j + 1   K/2 (36)"
REFERENCES,0.9973684210526316,where the inequality holds from the Jensen’s inequality. This completes the proof of Theorem 2.
