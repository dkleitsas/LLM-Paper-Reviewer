Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0026109660574412533,"In specific domains such as autonomous driving, quantitative trading, and health-
1"
ABSTRACT,0.005221932114882507,"care, explainability is crucial for developing ethical, responsible, and trustworthy
2"
ABSTRACT,0.007832898172323759,"reinforcement learning (RL) models. Although many deep RL algorithms have
3"
ABSTRACT,0.010443864229765013,"attained remarkable performance, the resulting policies are often neural networks
4"
ABSTRACT,0.013054830287206266,"that lack explainability, rendering them unsuitable for real-world deployment. To
5"
ABSTRACT,0.015665796344647518,"tackle this challenge, we introduce a novel semi-parametric reinforcement learning
6"
ABSTRACT,0.018276762402088774,"framework, dubbed ANQ (Approximate Nearest Neighbor Q-Learning), which cap-
7"
ABSTRACT,0.020887728459530026,"italizes on neural networks as encoders for high performance and memory-based
8"
ABSTRACT,0.02349869451697128,"structures for explainability. Furthermore, we propose the Sim-Encoder contrastive
9"
ABSTRACT,0.02610966057441253,"learning as a component of ANQ for state representation. Our evaluations on Mu-
10"
ABSTRACT,0.028720626631853787,"JoCo continuous control tasks validate the efficacy of ANQ in solving continuous
11"
ABSTRACT,0.031331592689295036,"tasks while offering an explainable decision-making process.
12"
INTRODUCTION,0.033942558746736295,"1
Introduction
13"
INTRODUCTION,0.03655352480417755,Figure 1: Overall Architecture of Our Approach
INTRODUCTION,0.0391644908616188,"In recent years, parametric reinforcement learning methods featuring end-to-end training, such as
14"
INTRODUCTION,0.04177545691906005,"Proximal Policy Optimization (PPO) [Schulman et al., 2017], Soft Actor-Critic (SAC) [Haarnoja
15"
INTRODUCTION,0.044386422976501305,"et al., 2018], and Deep Deterministic Policy Gradient (DDPG) [Lillicrap et al., 2015], have gar-
16"
INTRODUCTION,0.04699738903394256,"nered significant attention within the reinforcement learning community. These approaches have
17"
INTRODUCTION,0.04960835509138381,"demonstrated remarkable success in addressing decision-making challenges across diverse domains,
18"
INTRODUCTION,0.05221932114882506,"including robotics [Hwangbo et al., 2019], video games [Mnih et al., 2015], and board games [Schrit-
19"
INTRODUCTION,0.05483028720626632,"twieser et al., 2020]. Nevertheless, the incorporation of deep neural networks in these methods
20"
INTRODUCTION,0.057441253263707574,"presents a major obstacle to interpreting the underlying rationale of their decision-making processes.
21"
INTRODUCTION,0.06005221932114883,"This limitation hampers the application of such methods to numerous real-world scenarios, such
22"
INTRODUCTION,0.06266318537859007,"as autonomous driving [Kiran et al., 2021], quantitative trading [Zhang et al., 2020], and beyond.
23"
INTRODUCTION,0.06527415143603134,"Consequently, further investigation is necessary to enhance the interpretability and practical utility of
24"
INTRODUCTION,0.06788511749347259,"these reinforcement learning techniques in complex, real-world contexts.
25"
INTRODUCTION,0.07049608355091384,"This issue calls for the research of explainable reinforcement learning (XRL) which aims at obtaining
26"
INTRODUCTION,0.0731070496083551,"RL models that are both explainable and of high performance. Fidelity is one of the major objectives
27"
INTRODUCTION,0.07571801566579635,"in XRL [Milani et al., 2022] which measures to what extent the model makes decisions following its
28"
INTRODUCTION,0.0783289817232376,"explanation. Among different XRL algorithms, white-box algorithms (i.e., making decisions directly
29"
INTRODUCTION,0.08093994778067885,"using explainable models such as linear models or decision trees) enjoys high fidelity than the others.
30"
INTRODUCTION,0.0835509138381201,"(We defer the introduction of other XRL algorithms to Section 5.3.)
31"
INTRODUCTION,0.08616187989556136,"Memory-based reinforcement learning, following the non-parametric paradigm, is a popular class of
32"
INTRODUCTION,0.08877284595300261,"white-box algorithm and differs from widely researched parametric methods in deep reinforcement
33"
INTRODUCTION,0.09138381201044386,"learning. The approximation function in memory-based reinforcement learning is determined directly
34"
INTRODUCTION,0.09399477806788512,"by the training samples, rather than relying on a gradually-updated parameterized function. Prominent
35"
INTRODUCTION,0.09660574412532637,"memory-based methods include EC [Blundell et al., 2016], NEC [Pritzel et al., 2017], and EMDQN
36"
INTRODUCTION,0.09921671018276762,"[Lin et al., 2018] (see more in Ramani [2019]). Memory-based reinforcement learning has several
37"
INTRODUCTION,0.10182767624020887,"benefits, including being able to approximate a universal class of functions, the ability to directly
38"
INTRODUCTION,0.10443864229765012,"impact the policy with newly accumulated data without back-propagation updates Blundell et al.
39"
INTRODUCTION,0.10704960835509138,"[2016], the mitigation of the curse of dimensionality in global estimation Sutton and Barto [1998],
40"
INTRODUCTION,0.10966057441253264,"and higher data sampling efficiency and faster learning Lin et al. [2018]. Most importantly, memory-
41"
INTRODUCTION,0.1122715404699739,"based reinforcement learning possesses the advantage of improved explainability due to its human-
42"
INTRODUCTION,0.11488250652741515,"understandable decision making system (i.e., the memory consists of pre-collected samples).
43"
INTRODUCTION,0.1174934725848564,"Despite its potential for self-explainability through white-box decision-making, the utilization of
44"
INTRODUCTION,0.12010443864229765,"memory-based reinforcement learning for enhancing explainability remains relatively unexplored.
45"
INTRODUCTION,0.1227154046997389,"Existing studies investigating the use of episodic memory for explanations, such as Cruz et al. [2019],
46"
INTRODUCTION,0.12532637075718014,"Pritzel et al. [2017], Blundell et al. [2016], have been limited to grid world environments or discrete
47"
INTRODUCTION,0.1279373368146214,"tasks. In contrast, our work aims to expand this research scope to encompass continuous robotics
48"
INTRODUCTION,0.13054830287206268,"tasks in Mujoco by proposing a comprehensive memory-based self-explainable framework.
49"
INTRODUCTION,0.13315926892950392,"Efficiently retrieving relevant data from extensive databases presents a significant challenge in de-
50"
INTRODUCTION,0.13577023498694518,"veloping an effective memory-based reinforcement learning algorithm, particularly in continuous
51"
INTRODUCTION,0.13838120104438642,"control tasks as emphasized by Sutton and Barto [1998]. However, recent advancements in approxi-
52"
INTRODUCTION,0.1409921671018277,"mate nearest-neighbor searching algorithms, such as Hierarchical Navigable Small World (HNSW)
53"
INTRODUCTION,0.14360313315926893,"Malkov and Yashunin [2018], have demonstrated their effectiveness in swiftly retrieving pertinent
54"
INTRODUCTION,0.1462140992167102,"information from billions of records in natural language processing (NLP) tasks. Such methods have
55"
INTRODUCTION,0.14882506527415143,"been successfully applied to question-answering Kassner and Schütze [2020] and text generation
56"
INTRODUCTION,0.1514360313315927,"Borgeaud et al. [2022] tasks. In addition to NLP applications, retrieval-based systems have been
57"
INTRODUCTION,0.15404699738903394,"integrated with deep reinforcement learning algorithms, resulting in enhanced sample efficiency
58"
INTRODUCTION,0.1566579634464752,"Goyal et al. [2022], Humphreys et al. [2022].
59"
INTRODUCTION,0.15926892950391644,"The contributions of our paper are summarized as follows:
60"
INTRODUCTION,0.1618798955613577,"• We introduce a novel framework, ANQ, which offers efficient control in continuous domains
61"
INTRODUCTION,0.16449086161879894,"across a wide range of Mujoco experiments, while maintaining high explainability through
62"
INTRODUCTION,0.1671018276762402,"its ""data is policy"" design principle.
63"
INTRODUCTION,0.16971279373368145,"• We present the Sim-Encoder, a nearest neighbor contrastive learning approach for state
64"
INTRODUCTION,0.17232375979112272,"representation, which demonstrates its effectiveness in memory retrieval learning tasks.
65"
PRENIMINARIES,0.17493472584856398,"2
Preniminaries
66"
PRENIMINARIES,0.17754569190600522,"We first introduce notations and summarize the conventional episodic control method.
67"
NOTATION,0.1801566579634465,"2.1
Notation
68"
NOTATION,0.18276762402088773,"In this work, we study policy learning in continuous action space A and observation space S. We
69"
NOTATION,0.185378590078329,"consider a Markov decision process with transition st+1 ∼p(st+1|st, at). After performing an
70"
NOTATION,0.18798955613577023,"action, the agent receives a reward, and the ultimate goal is to optimize the policy to maximize
71"
NOTATION,0.1906005221932115,"returns.
72"
NOTATION,0.19321148825065274,"A key-value-based dataset D stores the key as the state embedding e. The database consists of rows
73"
NOTATION,0.195822454308094,"of {k, et, st, at, rt, st+1, qt} ∈D and columns of {K, E, St, A, R, St+1, Q} ∈D. K represents the
74"
NOTATION,0.19843342036553524,"set of all record IDs. The maximum number of rows is M. The observation Sim-Encoder network is
75"
NOTATION,0.2010443864229765,"denoted as Gθ parameterized by the network parameters θ.
76"
NOTATION,0.20365535248041775,"For database operating, in total, six operations are defined in the memory module: APPEND, TRIM,
77"
NOTATION,0.206266318537859,"GET, UPDATE, SEARCH, and INDEX. More corresponding explanations for these operations can
78"
NOTATION,0.20887728459530025,"be found in Sec.3.2.
79"
EPISODIC CONTROL,0.21148825065274152,"2.2
Episodic Control
80"
EPISODIC CONTROL,0.21409921671018275,"Episodic control methods enhance sampling efficiency and episodic returns by using an external
81"
EPISODIC CONTROL,0.21671018276762402,"memory database for interactions such as writing, reading, and updating. The concept was first
82"
EPISODIC CONTROL,0.2193211488250653,"introduced in Blundell et al. [2016], which resolved complex sequential decision tasks.
83"
EPISODIC CONTROL,0.22193211488250653,"This method is defined for discrete spaces. It proposes the following Q table update mechanism:
84"
EPISODIC CONTROL,0.2245430809399478,"QEC(s, a) = max(QEC(s, a), R)
(1)"
EPISODIC CONTROL,0.22715404699738903,"After the update, it generates an effective Q Table. During the policy execution phase, if an
85"
EPISODIC CONTROL,0.2297650130548303,"observation-action pair exists in memory, the Q value is retrieved directly from the table. However, if
86"
EPISODIC CONTROL,0.23237597911227154,"the pair is not found in memory, an approximation matching and estimation process is required. The
87"
EPISODIC CONTROL,0.2349869451697128,"agent queries the Q Table using the following approach to obtain the Q value.
88"
EPISODIC CONTROL,0.23759791122715404,"ˆQEC(s, a) = 1 N n=N
X"
EPISODIC CONTROL,0.2402088772845953,"n=1
Q(sn, a)
(2)"
EPISODIC CONTROL,0.24281984334203655,"The objective of episodic control is to accelerate learning speed and improve decision quality. An
89"
EPISODIC CONTROL,0.2454308093994778,"external memory module can then compensate for drawbacks such as low sample efficiency and slow
90"
EPISODIC CONTROL,0.24804177545691905,"gradient updates.
91"
EPISODIC CONTROL,0.2506527415143603,"In previous discussions, the Episodic Control (EC) method has been investigated under both discrete
92"
EPISODIC CONTROL,0.25326370757180156,"actions and continuous actions (Li et al. [2023], Kuznetsov and Filchenkov [2021]). However, the
93"
EPISODIC CONTROL,0.2558746736292428,"explainability of EC in continuous action spaces suffers from low fidelity due to the utilization of a
94"
EPISODIC CONTROL,0.2584856396866841,"policy network. In this paper, we set out to achieve two objectives concurrently. First, we explore
95"
EPISODIC CONTROL,0.26109660574412535,"how Episodic Control can be effectively applied in continuous action spaces. Second, we strive to
96"
EPISODIC CONTROL,0.26370757180156656,"leverage the memory of Episodic Control to attain explainability benefits.
97"
METHOD,0.26631853785900783,"3
Method
98"
METHOD,0.2689295039164491,"The complete algorithm is presented in Algorithm 1, and the illustration of the inference pipeline
99"
METHOD,0.27154046997389036,"can be observed (cf. Fig.1). The proposed method involves generating an embedding vector et from
100"
METHOD,0.2741514360313316,"the observation using the Sim-Encoder. Subsequently, we employ the HNSW algorithm Malkov
101"
METHOD,0.27676240208877284,"and Yashunin [2018] to search for the nearest neighbor set en within the memory. Each neighbor is
102"
METHOD,0.2793733681462141,"associated with an action and a Q value, and the action with the highest Q value is selected as the
103"
METHOD,0.2819843342036554,"policy output. It is worth noting that this action is continuous, which distinguishes it from previous
104"
METHOD,0.2845953002610966,"EC work Blundell et al. [2016].
105"
METHOD,0.28720626631853785,"First, the Sim-Encoder in embedding observations into a cosine space is augmented via One-Step-
106"
METHOD,0.2898172323759791,"Away State Encoding Contrastive Learning. This approach employs adjacent states as positive
107"
METHOD,0.2924281984334204,"samples for contrastive learning, with experimental outcomes demonstrating that the implementation
108"
METHOD,0.2950391644908616,"of the Sim-Encoder considerably enhances performance.
109"
METHOD,0.29765013054830286,Algorithm 1 ANQ Algorithm
METHOD,0.3002610966057441,"Input:
Database D
with each row notated as {k, et, st, at, rt, st+1, qt} ∈D
with each column notated as {K, E, St, A, R, St+1, Q} ∈D
Observation Sim-Encoder network Gθ
Contrastive learning function CL
Gaussian distribution for action noise N(µ, σ)
for each iteration do"
METHOD,0.3028720626631854,for each environment step do
METHOD,0.30548302872062666,"et = Gθ(st)
k1..kn =SEARCH(et)
(a1, q1)..(an, qn) =GET(k1..kn)
nq = argmaxn(q1..qn)
at = anq + N(µ, σ)
st+1 ∼p(st+1|st, at)
APPEND ( et, st, at, rt, st+1)
end for
for sampled minibatch {st, st+1} do"
METHOD,0.30809399477806787,"Lθ = CL(st, st+1)
update networks Gθ to minimize L
end for
K1..Kn =SEARCH(E)
for each learning step do"
METHOD,0.31070496083550914,Q1..Qn = GET(K1..Kn)
METHOD,0.3133159268929504,ˆQ = Rt + γ 1
METHOD,0.31592689295039167,"N
Pn=N
n=1 Qn"
METHOD,0.3185378590078329,"UPDATE( Q, ˆQ)
end for
TRIM()
INDEX()
end for"
METHOD,0.32114882506527415,"Subsequently, in order to acquire a comprehensive Q-table, we employ in-memory learning, which
110"
METHOD,0.3237597911227154,"involves the batch computation of all Q-value estimations and Q-learning updates for each state
111"
METHOD,0.3263707571801567,"stored in memory. The training process undergoes iterative cycles until the global Q-value converges.
112"
EMBEDDING MODULE,0.3289817232375979,"3.1
Embedding Module
113"
EMBEDDING MODULE,0.33159268929503916,"We introduce our novel approach, the ""One-Step-Away State Encoding Contrastive Learning."" The
114"
EMBEDDING MODULE,0.3342036553524804,"reason for using a one-step-away state as a positive sample is that the most informative actions and
115"
EMBEDDING MODULE,0.3368146214099217,"q-values for the current state are derived from a scenario that is most similar to it (Blundell et al.
116"
EMBEDDING MODULE,0.3394255874673629,"[2016]).
117"
EMBEDDING MODULE,0.34203655352480417,"et = Gθ(st)
(3)"
EMBEDDING MODULE,0.34464751958224543,"This method aims to effectively represent the state with contrastive learning. Specifically, we utilize
118"
EMBEDDING MODULE,0.3472584856396867,"positive samples that consist of a state pair st, st+1 that are one step away. The resulting state
119"
EMBEDDING MODULE,0.34986945169712796,"representation is designed such that the nearest neighbor of each state is reachable within one step.
120"
EMBEDDING MODULE,0.3524804177545692,"We adopt a similar objective to SimCLR Chen et al. [2020], aiming to maximize the similarity
121"
EMBEDDING MODULE,0.35509138381201044,"between two vectors as measured by cosine similarity sim(u, v) = uT v/(|u||v|). The Sim-Encoder is
122"
EMBEDDING MODULE,0.3577023498694517,"a standalone component trained to maximize the similarity of embedding, without reward information
123"
EMBEDDING MODULE,0.360313315926893,"but only state transition tuples.
124"
EMBEDDING MODULE,0.3629242819843342,"θ = argmaxθ E(st,st+1)∼D[sim(Gθ(st), Gθ(st+1))]
(4)"
EMBEDDING MODULE,0.36553524804177545,Table 1: Memory Operations
EMBEDDING MODULE,0.3681462140992167,"Operation
Description"
EMBEDDING MODULE,0.370757180156658,"APPEND
Add a new row to the database
INDEX
Construct an HNSW index using Sim-Encoder embeddings for efficient
approximate nearest neighbor search
SEARCH
Given an embedding vector e, return the corresponding
row IDs k1..kn, seeing Malkov and Yashunin [2018]
GET
Given a row ID k, retrieve relevant data values, such as actions, Q values, etc.
TRIM
Remove historical data to maintain a database size of up to M rows
UPDATE
Given a column of data, update the corresponding column in database"
MEMORY MODULE,0.3733681462140992,"3.2
Memory Module
125"
MEMORY MODULE,0.37597911227154046,"The explainable memory module is in the form of a key-value database. And the keys in the database
126"
MEMORY MODULE,0.3785900783289817,"correspond to the observation embedding vectors obtained via the Sim-Encoder, and each key is
127"
MEMORY MODULE,0.381201044386423,"associated with a corresponding value that includes information such as the current step’s observation,
128"
MEMORY MODULE,0.3838120104438642,"action, reward, and all of other relevant data. To manage this database, we have defined 6 standard
129"
MEMORY MODULE,0.38642297650130547,"operations, namely APPEND, TRIM, GET, UPDATE, INDEX, and SEARCH, which are detailed in
130"
MEMORY MODULE,0.38903394255874674,"Algorithm 1. and Table.1.
131"
MEMORY MODULE,0.391644908616188,"The GET operation requires a target state’s embedding as the key and returns the corresponding
132"
MEMORY MODULE,0.39425587467362927,"values. To prevent the database from becoming excessively large, we define a TRIM operation that
133"
MEMORY MODULE,0.3968668407310705,"automatically removes older data, retaining only the most recent M records. This design enables
134"
MEMORY MODULE,0.39947780678851175,"efficient storage and retrieval of data while ensuring that the database remains manageable and
135"
MEMORY MODULE,0.402088772845953,"up-to-date.
136"
MEMORY MODULE,0.4046997389033943,"In our approach for effective memory retrieval, Approximate K-Nearest Neighbors Search (AKNN)
137"
MEMORY MODULE,0.4073107049608355,"plays a crucial role. We introduce a SEARCH operation that takes a state embedding as input and
138"
MEMORY MODULE,0.40992167101827676,"returns the corresponding key(s) of the nearest neighbor(s) in the database. Additionally, we define
139"
MEMORY MODULE,0.412532637075718,"an INDEX operation, activated when the database undergoes modifications, seeing Malkov and
140"
MEMORY MODULE,0.4151436031331593,"Yashunin [2018]. This operation reorganizes the HNSW index to align with the updated database,
141"
MEMORY MODULE,0.4177545691906005,"ensuring that subsequent KNN searches remain both fast and accurate.
142"
POLICY EVALUATION,0.42036553524804177,"3.3
Policy Evaluation
143"
POLICY EVALUATION,0.42297650130548303,"We introduce the Approximate Nearest Neighbor Search Q-Learning method. In contrast to conven-
144"
POLICY EVALUATION,0.4255874673629243,"tional tabular Q-Learning, we employ a novel form of state value estimation, ˆV (st), by aggregating
145"
POLICY EVALUATION,0.4281984334203655,"the Q-values from the nearest neighbors of the state (cf. Fig.2). The Q-value of each state-action pair
146"
POLICY EVALUATION,0.4308093994778068,"is updated following the Bellman equation, incorporating a decay factor, γ.
147"
POLICY EVALUATION,0.43342036553524804,"During the practical training process, we adopt a batch updating strategy wherein we simultaneously
148"
POLICY EVALUATION,0.4360313315926893,"compute the labels for all neighbors of each state and estimate the values of all states in memory.
149"
POLICY EVALUATION,0.4386422976501306,"Subsequently, we update all Q-values in the table accordingly. The learning iteration persists until the
150"
POLICY EVALUATION,0.4412532637075718,"maximum change in Q-values falls below a specified threshold.
151"
POLICY EVALUATION,0.44386422976501305,"q1..qN = GETq(SEARCH(Gθ(st)))
(5)"
POLICY EVALUATION,0.4464751958224543,"ˆv(st) = 1 N n=N
X"
POLICY EVALUATION,0.4490861618798956,"n=1
qn
(6)"
POLICY EVALUATION,0.4516971279373368,"ˆq(st, at) = rt + γˆv(st+1)
(7)"
POLICY IMPROVEMENT,0.45430809399477806,"3.4
Policy Improvement
152"
POLICY IMPROVEMENT,0.45691906005221933,"For policy improvement, our proposed method directly selects the action with the maximum Q-value
153"
POLICY IMPROVEMENT,0.4595300261096606,"from the neighbors (en ∈E, an ∈A, qn ∈Q), as shown in Equation 10. Using the embedding et
154"
POLICY IMPROVEMENT,0.4621409921671018,Figure 2: Policy evaluation in memory (N=3)
POLICY IMPROVEMENT,0.46475195822454307,Figure 3: Performance on Continuous Control Tasks vs Conventional RL
POLICY IMPROVEMENT,0.46736292428198434,"generated by the contrastively learned Sim-Encoder, candidates are retrieved from the memory
155"
POLICY IMPROVEMENT,0.4699738903394256,"module via the nearest neighbor search. To encourage exploration, action noise following a Gaussian
156"
POLICY IMPROVEMENT,0.4725848563968668,"distribution N(µ, σ) is added.
157"
POLICY IMPROVEMENT,0.4751958224543081,"a1, q1..an, qn = GETaq(SEARCH(et))
(8)"
POLICY IMPROVEMENT,0.47780678851174935,"nq = argmaxn(q1..qn)
(9)"
POLICY IMPROVEMENT,0.4804177545691906,"at = anq + N(µ, σ)
(10)"
POLICY IMPROVEMENT,0.4830287206266319,"In contrast to employing a black-box network as an actor, we have devised a data-driven, self-
158"
POLICY IMPROVEMENT,0.4856396866840731,"explaining actor that seamlessly integrates the results of model search and generates decisions directly
159"
POLICY IMPROVEMENT,0.48825065274151436,"using a rule-based approach.
160"
EXPERIMENTS,0.4908616187989556,"4
Experiments
161"
EXPERIMENTS,0.4934725848563969,"In these experiments, we aimed to evaluate the performance of our ANQ approach in solving
162"
EXPERIMENTS,0.4960835509138381,"continuous control tasks in Mujoco, provide action explainability, and investigate the significance of
163"
EXPERIMENTS,0.49869451697127937,"the Sim-Encoder module in the ANQ framework.
164"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5013054830287206,"4.1
Solving Continuous Control Task in Mujoco
165"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5039164490861618,"First of all, our approach is evaluated on several continuous control tasks in the MuJoCo physics
166"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5065274151436031,"engine. Specifically, we compare our method with state-of-the-art reinforcement learning (RL)
167"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5091383812010444,"algorithms, including SAC-1M, PPO-1M, and TRPO-1M, on the Walker2d-v3, Ant-v3, HalfCheetah-
168"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5117493472584856,"v3, and Hopper-v3 environments. We use the benchmark performance reported by stable-baselines3
169"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5143603133159269,"Raffin et al. [2021].
170"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5169712793733682,"The results (cf. Fig.3) show that our method slightly outperforms A2C-1M on the Walker2d-v3
171"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5195822454308094,"task and PPO-1M on the Ant-v3 task while achieving comparable performance to TRPO-1M on the
172"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5221932114882507,Figure 4: Explainable Action
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5248041775456919,"HalfCheetah-v3 task. Furthermore, we analyze the performance of our method on the Hopper-v3
173"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5274151436031331,"task by examining the game replay. We find that the agent fails to take the second step and falls after
174"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5300261096605744,"the first step. This indicates that our method may currently lack exploration capability. This will be
175"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5326370757180157,"addressed in future research in order to surpass the performance of traditional RL methods. Overall,
176"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5352480417754569,"our approach successfully and stably converges on the MuJoCo continuous control tasks, but further
177"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5378590078328982,"improvement is necessary to achieve better performance, seeing the discussion in Sec.6.
178"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5404699738903395,"For the hyperparameters, we utilized a 4-layer MLP network with layer normalization as the encoder.
179"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5430809399477807,"The learning rate was set to 0.0003, the batch size was 512, and the Adam optimizer was used. The
180"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5456919060052219,"size of the explainable memory was limited to 500,000, and old data were discarded once this limit
181"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5483028720626631,"was exceeded. We set the parameters of HNSW to M=16 and ef=10. The total number of training
182"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5509138381201044,"steps was 10 million, and the agent performed ANQ learning every 40,000 environment interactions.
183"
SOLVING CONTINUOUS CONTROL TASK IN MUJOCO,0.5535248041775457,"We set the number of neighboring actions sampled during each action selection to 10.
184"
ACTION EXPLAINABILITY,0.556135770234987,"4.2
Action explainability
185"
ACTION EXPLAINABILITY,0.5587467362924282,"In this explainability experiment, we designed a question-and-answer (QA) case (cf. Fig.4) to
186"
ACTION EXPLAINABILITY,0.5613577023498695,"simulate a scenario where humans need to double-check the correctness of the robot’s decision during
187"
ACTION EXPLAINABILITY,0.5639686684073107,"human-robot collaboration. Specifically, humans ask ""why"" questions to query the basis of the robot’s
188"
ACTION EXPLAINABILITY,0.566579634464752,"action, and the robot responds with the policy that it has chosen, as well as the evidence supporting
189"
ACTION EXPLAINABILITY,0.5691906005221932,"its decision.
190"
ACTION EXPLAINABILITY,0.5718015665796344,"To provide a convincing explanation, the robot searches its memory for similar states and explains to
191"
ACTION EXPLAINABILITY,0.5744125326370757,"the human the actions that it had taken in the past in similar scenarios, as well as the corresponding
192"
ACTION EXPLAINABILITY,0.577023498694517,"returns. By providing such detailed explanations, the robot is able to offer valuable insights to humans
193"
ACTION EXPLAINABILITY,0.5796344647519582,"and effectively bridge the gap in understanding between human and machine decision-making
194"
ACTION EXPLAINABILITY,0.5822454308093995,"processes, for ensuring safe and reliable human-robot collaboration.
195"
SIM-ENCODER,0.5848563968668408,"4.3
Sim-Encoder
196"
SIM-ENCODER,0.587467362924282,"We conducted experiments to investigate the significance of the Sim-Encoder module within the
197"
SIM-ENCODER,0.5900783289817232,"ANQ framework. We have illustrated the retrieved samples (cf. Fig.5). Without the Sim-Encoder,
198"
SIM-ENCODER,0.5926892950391645,"semantically similar states do not share relevant information in cosine space, as discussed in Su et al.
199"
SIM-ENCODER,0.5953002610966057,"[2021]. Our ablation study (cf. Fig.6) demonstrated that the Sim-Encoder led to substantial perfor-
200"
SIM-ENCODER,0.597911227154047,"mance improvements across all four tested tasks, as it effectively retrieves and embeds temporally
201"
SIM-ENCODER,0.6005221932114883,Figure 5: Retrieved Results using Sim-Encoder and Approximate Nearest Neighbor Search
SIM-ENCODER,0.6031331592689295,Figure 6: Ablation Study of Embedding Module Sim-encoder
SIM-ENCODER,0.6057441253263708,"proximate states into a space with adjacent cosine distances. Overall, the Sim-Encoder is an essential
202"
SIM-ENCODER,0.608355091383812,"component of the ANQ framework and holds potential for use in other RL algorithms.
203"
RELATED WORK,0.6109660574412533,"5
Related Work
204"
EPISODIC CONTROL,0.6135770234986945,"5.1
Episodic Control
205"
EPISODIC CONTROL,0.6161879895561357,"The idea of episodic control (EC) was bio-inspired by the mechanism of the hippocampus Lengyel
206"
EPISODIC CONTROL,0.618798955613577,"and Dayan [2007]. EC, as a non-parametric approach, possesses virtues including rapid assimilation
207"
EPISODIC CONTROL,0.6214099216710183,"of past experiences and a solution for sparse-reward situations. Notable works like MFEC Blundell
208"
EPISODIC CONTROL,0.6240208877284595,"et al. [2016] and NEC Pritzel et al. [2017] employed kNN search to acquire the value for the current
209"
EPISODIC CONTROL,0.6266318537859008,"state derived from similar states. The value function is in tabular form and updated using the classical
210"
EPISODIC CONTROL,0.6292428198433421,"Q-learning method. While MFEC adopts random projection Johnson [1984] and VAE Kingma and
211"
EPISODIC CONTROL,0.6318537859007833,"Welling [2013] as state embedding methods, NEC employs a differentiable CNN encoder instead.
212"
EPISODIC CONTROL,0.6344647519582245,"Beyond that, Lin et al. proposed EMDQN Lin et al. [2018], which is a synergy of EC and DQN.
213"
EPISODIC CONTROL,0.6370757180156658,"Their approach combined the merits of both algorithms, i.e., fast learning at an early stage and good
214"
EPISODIC CONTROL,0.639686684073107,"final performance. ERLAM then further promoted the efficacy by introducing an associative memory
215"
EPISODIC CONTROL,0.6422976501305483,"graph Zhu et al. [2020].
216"
RETRIEVAL-BASED LEARNING,0.6449086161879896,"5.2
Retrieval-based Learning
217"
RETRIEVAL-BASED LEARNING,0.6475195822454308,"The retrieval-based learning and inference architecture provides a viable solution for managing an
218"
RETRIEVAL-BASED LEARNING,0.6501305483028721,"explainable and extensible knowledge base. One prominent instantiation of this architecture is the
219"
RETRIEVAL-BASED LEARNING,0.6527415143603134,"retriever-reader model Zhu et al. [2021], which has gained traction in the open domain question
220"
RETRIEVAL-BASED LEARNING,0.6553524804177546,"answering (openQA) research community. The retriever component returns a set of relevant articles,
221"
RETRIEVAL-BASED LEARNING,0.6579634464751958,"while the reader extracts the answer from the retrieved documents. Numerous natural language
222"
RETRIEVAL-BASED LEARNING,0.660574412532637,"processing (NLP) algorithms, including kNN-LM Khandelwal et al. [2019], RAG Lewis et al. [2020]
223"
RETRIEVAL-BASED LEARNING,0.6631853785900783,"and RETRO Borgeaud et al. [2022], leverage a retrieval-based approach to enhance their performance
224"
RETRIEVAL-BASED LEARNING,0.6657963446475196,"and efficiency. These techniques have proven to be effective in the domain of NLP and continue to be
225"
RETRIEVAL-BASED LEARNING,0.6684073107049608,"an active area of future NLP research Liu et al. [2023].
226"
EXPLAINABLE REINFORCEMENT LEARNING,0.6710182767624021,"5.3
Explainable Reinforcement Learning
227"
EXPLAINABLE REINFORCEMENT LEARNING,0.6736292428198434,"The methods for explainability in reinforcement learning can be broadly categorized into three
228"
EXPLAINABLE REINFORCEMENT LEARNING,0.6762402088772846,"groups, as discussed in Milani et al. [2022]: Feature Importance (FI), Learning Processing and
229"
EXPLAINABLE REINFORCEMENT LEARNING,0.6788511749347258,"Markov Decision Process (LPM), and Policy-Level (PL). FI methods involve utilizing decision tree
230"
EXPLAINABLE REINFORCEMENT LEARNING,0.6814621409921671,"models for explainability, learning an explainable surrogate network through expert and learner
231"
EXPLAINABLE REINFORCEMENT LEARNING,0.6840731070496083,"frameworks, or directly generating explanations through natural language or saliency maps. LPM
232"
EXPLAINABLE REINFORCEMENT LEARNING,0.6866840731070496,"addresses explainable transition models to answer ""what-if"" questions, interpretation of Q values,
233"
EXPLAINABLE REINFORCEMENT LEARNING,0.6892950391644909,"and identification of key training points. PL provides an understanding of long-term behavior and
234"
EXPLAINABLE REINFORCEMENT LEARNING,0.6919060052219321,"summarizes the policy. However, many existing explainable reinforcement learning methods require
235"
EXPLAINABLE REINFORCEMENT LEARNING,0.6945169712793734,"additional network training Guo et al. [2021] or the use of decision trees Silva et al. [2019]. These
236"
EXPLAINABLE REINFORCEMENT LEARNING,0.6971279373368147,"methods can also impose a cognitive burden on users to understand the model’s behavior Dodge et al.
237"
EXPLAINABLE REINFORCEMENT LEARNING,0.6997389033942559,"[2021]. In contrast, the memory-based reinforcement learning algorithm, ANQ, presented in this
238"
EXPLAINABLE REINFORCEMENT LEARNING,0.7023498694516971,"paper provides self-explainability without additional explanation specifically training.
239"
LIMITATION,0.7049608355091384,"6
Limitation
240"
LIMITATION,0.7075718015665796,"In this study, we present an innovative and explainable architecture, termed ANQ, which, despite
241"
LIMITATION,0.7101827676240209,"its novelty, does not significantly outperform state-of-the-art benchmarks. Our primary aim is
242"
LIMITATION,0.7127937336814621,"to demonstrate the efficacy of ANQ with its highly interpretable policy. We acknowledge this
243"
LIMITATION,0.7154046997389034,"performance gap and recognize that our method has not yet incorporated the latest techniques, such
244"
LIMITATION,0.7180156657963447,"as maximum entropy learning from SAC Haarnoja et al. [2018], etc. These refinements will be
245"
LIMITATION,0.720626631853786,"addressed in future work, rather than here. Moreover, we have not compared our approach with
246"
LIMITATION,0.7232375979112271,"other contrastive learning methods for representation learning. Since we proposed the Sim-Encoder,
247"
LIMITATION,0.7258485639686684,"a thorough comparison with alternative methods and further study will also be included in future
248"
LIMITATION,0.7284595300261096,"research.
249"
CONCLUSION,0.7310704960835509,"7
Conclusion
250"
CONCLUSION,0.7336814621409922,"Explainability is crucial in specific domains of reinforcement learning, such as autonomous driving,
251"
CONCLUSION,0.7362924281984334,"quantitative trading, and healthcare. To address this challenge, we propose ANQ, a novel semi-
252"
CONCLUSION,0.7389033942558747,"parametric reinforcement learning framework that combines the high performance of neural networks
253"
CONCLUSION,0.741514360313316,"with the explainability of a memory-based structure. Additionally, we validate the effectiveness of
254"
CONCLUSION,0.7441253263707572,"Sim-Encoder, a key module of ANQ, in state representation and learning efficiency enhancement.
255"
CONCLUSION,0.7467362924281984,"Empirical evaluations demonstrate ANQ’s effectiveness in solving continuous tasks and providing
256"
CONCLUSION,0.7493472584856397,"explainable decision-making. Our contributions include proposing a framework that achieves both
257"
CONCLUSION,0.7519582245430809,"efficient control and robust explainability. While further improvements are necessary for superior
258"
CONCLUSION,0.7545691906005222,"performance, our results indicate that ANQ is a promising approach for developing explainable and
259"
CONCLUSION,0.7571801566579635,"trustworthy RL models in critical applications.
260"
REFERENCE,0.7597911227154047,"8
Reference
261"
REFERENCES,0.762402088772846,"References
262"
REFERENCES,0.7650130548302873,"Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo, Jack Rae, Daan
263"
REFERENCES,0.7676240208877284,"Wierstra, and Demis Hassabis. Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016.
264"
REFERENCES,0.7702349869451697,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm
265"
REFERENCES,0.7728459530026109,"Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models
266"
REFERENCES,0.7754569190600522,"by retrieving from trillions of tokens. In International conference on machine learning, pages 2206–2240.
267"
REFERENCES,0.7780678851174935,"PMLR, 2022.
268"
REFERENCES,0.7806788511749347,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
269"
REFERENCES,0.783289817232376,"learning of visual representations. In International conference on machine learning, pages 1597–1607. PMLR,
270"
REFERENCES,0.7859007832898173,"2020.
271"
REFERENCES,0.7885117493472585,"Francisco Cruz, Richard Dazeley, and Peter Vamplew. Memory-based explainable reinforcement learning. In
272"
REFERENCES,0.7911227154046997,"AI 2019: Advances in Artificial Intelligence: 32nd Australasian Joint Conference, Adelaide, SA, Australia,
273"
REFERENCES,0.793733681462141,"December 2–5, 2019, Proceedings 32, pages 66–77. Springer, 2019.
274"
REFERENCES,0.7963446475195822,"Jonathan Dodge, Andrew Anderson, Roli Khanna, Jed Irvine, Rupika Dikkala, Kin-Ho Lam, Delyar Tabatabai,
275"
REFERENCES,0.7989556135770235,"Anita Ruangrotsakun, Zeyad Shureih, Minsuk Kahng, et al. From “no clear winner” to an effective explainable
276"
REFERENCES,0.8015665796344648,"artificial intelligence process: An empirical journey. Applied AI Letters, 2(4):e36, 2021.
277"
REFERENCES,0.804177545691906,"Anirudh Goyal, Abram Friesen, Andrea Banino, Theophane Weber, Nan Rosemary Ke, Adria Puigdomenech
278"
REFERENCES,0.8067885117493473,"Badia, Arthur Guez, Mehdi Mirza, Peter C Humphreys, Ksenia Konyushova, et al. Retrieval-augmented
279"
REFERENCES,0.8093994778067886,"reinforcement learning. In International Conference on Machine Learning, pages 7740–7765. PMLR, 2022.
280"
REFERENCES,0.8120104438642297,"Wenbo Guo, Xian Wu, Usmann Khan, and Xinyu Xing. Edge: Explaining deep reinforcement learning policies.
281"
REFERENCES,0.814621409921671,"Advances in Neural Information Processing Systems, 34:12222–12236, 2021.
282"
REFERENCES,0.8172323759791122,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
283"
REFERENCES,0.8198433420365535,"entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors,
284"
REFERENCES,0.8224543080939948,"Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
285"
REFERENCES,0.825065274151436,"Learning Research, pages 1861–1870. PMLR, 10–15 Jul 2018.
286"
REFERENCES,0.8276762402088773,"Peter C Humphreys, Arthur Guez, Olivier Tieleman, Laurent Sifre, Théophane Weber, and Timothy Lillicrap.
287"
REFERENCES,0.8302872062663186,"Large-scale retrieval for reinforcement learning. arXiv preprint arXiv:2206.05314, 2022.
288"
REFERENCES,0.8328981723237598,"Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and
289"
REFERENCES,0.835509138381201,"Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science Robotics, 4(26):eaau5872,
290"
REFERENCES,0.8381201044386423,"2019.
291"
REFERENCES,0.8407310704960835,"William B Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26:189–206, 1984.
292"
REFERENCES,0.8433420365535248,"Nora Kassner and Hinrich Schütze. Bert-knn: Adding a knn search component to pretrained language models
293"
REFERENCES,0.8459530026109661,"for better qa. arXiv preprint arXiv:2005.00766, 2020.
294"
REFERENCES,0.8485639686684073,"Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through
295"
REFERENCES,0.8511749347258486,"memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.
296"
REFERENCES,0.8537859007832899,"Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
297"
REFERENCES,0.856396866840731,"B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and
298"
REFERENCES,0.8590078328981723,"Patrick Pérez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on
299"
REFERENCES,0.8616187989556136,"Intelligent Transportation Systems, 23(6):4909–4926, 2021.
300"
REFERENCES,0.8642297650130548,"Igor Kuznetsov and Andrey Filchenkov. Solving continuous control with episodic memory. arXiv preprint
301"
REFERENCES,0.8668407310704961,"arXiv:2106.08832, 2021.
302"
REFERENCES,0.8694516971279374,"Máté Lengyel and Peter Dayan. Hippocampal contributions to control: the third way. Advances in neural
303"
REFERENCES,0.8720626631853786,"information processing systems, 20, 2007.
304"
REFERENCES,0.8746736292428199,"Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
305"
REFERENCES,0.8772845953002611,"Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-
306"
REFERENCES,0.8798955613577023,"intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.
307"
REFERENCES,0.8825065274151436,"Zhuo Li, Derui Zhu, Yujing Hu, Xiaofei Xie, Lei Ma, Yan Zheng, Yan Song, Yingfeng Chen, and Jianjun Zhao.
308"
REFERENCES,0.8851174934725848,"Neural episodic control with state abstraction. arXiv preprint arXiv:2301.11490, 2023.
309"
REFERENCES,0.8877284595300261,"Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,
310"
REFERENCES,0.8903394255874674,"and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971,
311"
REFERENCES,0.8929503916449086,"2015.
312"
REFERENCES,0.8955613577023499,"Zichuan Lin, Tianqi Zhao, Guangwen Yang, and Lintao Zhang. Episodic memory deep q-networks. arXiv
313"
REFERENCES,0.8981723237597912,"preprint arXiv:1805.07603, 2018.
314"
REFERENCES,0.9007832898172323,"Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen
315"
REFERENCES,0.9033942558746736,"He, Zhengliang Liu, Zihao Wu, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, and Bao
316"
REFERENCES,0.9060052219321149,"Ge. Summary of chatgpt/gpt-4 research and perspective towards the future of large language models, 2023.
317"
REFERENCES,0.9086161879895561,"Yu A Malkov and Dmitry A Yashunin.
Efficient and robust approximate nearest neighbor search using
318"
REFERENCES,0.9112271540469974,"hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence,
319"
REFERENCES,0.9138381201044387,"42(4):824–836, 2018.
320"
REFERENCES,0.9164490861618799,"Stephanie Milani, Nicholay Topin, Manuela Veloso, and Fei Fang. A survey of explainable reinforcement
321"
REFERENCES,0.9190600522193212,"learning, 2022. URL https://arxiv.org/abs/2202.08434.
322"
REFERENCES,0.9216710182767625,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
323"
REFERENCES,0.9242819843342036,"Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep
324"
REFERENCES,0.9268929503916449,"reinforcement learning. nature, 518(7540):529–533, 2015.
325"
REFERENCES,0.9295039164490861,"Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol Vinyals, Demis Hassabis,
326"
REFERENCES,0.9321148825065274,"Daan Wierstra, and Charles Blundell. Neural episodic control. In International Conference on Machine
327"
REFERENCES,0.9347258485639687,"Learning, pages 2827–2836. PMLR, 2017.
328"
REFERENCES,0.9373368146214099,"Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-
329"
REFERENCES,0.9399477806788512,"baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22
330"
REFERENCES,0.9425587467362925,"(268):1–8, 2021. URL http://jmlr.org/papers/v22/20-1364.html.
331"
REFERENCES,0.9451697127937336,"Dhruv Ramani. A short survey on memory based reinforcement learning. arXiv preprint arXiv:1904.06736,
332"
REFERENCES,0.9477806788511749,"2019.
333"
REFERENCES,0.9503916449086162,"Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt,
334"
REFERENCES,0.9530026109660574,"Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi
335"
REFERENCES,0.9556135770234987,"by planning with a learned model. Nature, 588(7839):604–609, 2020.
336"
REFERENCES,0.95822454308094,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
337"
REFERENCES,0.9608355091383812,"algorithms. arXiv preprint arXiv:1707.06347, 2017.
338"
REFERENCES,0.9634464751958225,"Andrew Silva, Taylor Killian, Ivan Dario Jimenez Rodriguez, Sung-Hyun Son, and Matthew Gombolay. Op-
339"
REFERENCES,0.9660574412532638,"timization methods for interpretable differentiable decision trees in reinforcement learning. arXiv preprint
340"
REFERENCES,0.9686684073107049,"arXiv:1903.09338, 2019.
341"
REFERENCES,0.9712793733681462,"Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou. Whitening sentence representations for better semantics
342"
REFERENCES,0.9738903394255874,"and faster retrieval. arXiv preprint arXiv:2103.15316, 2021.
343"
REFERENCES,0.9765013054830287,"R. Sutton and A. Barto. Reinforcement Learning:An Introduction. Reinforcement Learning:An Introduction,
344"
REFERENCES,0.97911227154047,"1998.
345"
REFERENCES,0.9817232375979112,"Zihao Zhang, Stefan Zohren, and Stephen Roberts. Deep reinforcement learning for trading. The Journal of
346"
REFERENCES,0.9843342036553525,"Financial Data Science, 2(2):25–40, 2020.
347"
REFERENCES,0.9869451697127938,"Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. Retrieving and
348"
REFERENCES,0.9895561357702349,"reading: A comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774,
349"
REFERENCES,0.9921671018276762,"2021.
350"
REFERENCES,0.9947780678851175,"Guangxiang Zhu, Zichuan Lin, Guangwen Yang, and Chongjie Zhang. Episodic reinforcement learning with
351"
REFERENCES,0.9973890339425587,"associative memory. 2020.
352"
