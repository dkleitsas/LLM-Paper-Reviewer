Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003703703703703704,"Supervised learning models have been increasingly used in various domains such
as lending, college admission, natural language processing, face recognition, etc.
These models may inherit pre-existing biases from training datasets and exhibit
discrimination against protected social groups. Various fairness notions have been
introduced to address fairness issues. In general, ﬁnding a fair predictor leads to
a constrained optimization problem, and depending on the fairness notion, it may
be non-convex. In this work, we focus on Equalized Loss (EL), a fairness notion
that requires the prediction error/loss to be equalized across different demographic
groups. Imposing this constraint to the learning process leads to a non-convex op-
timization problem even if the loss function is convex. We introduce algorithms
that can leverage off-the-shelf convex programming tools and efﬁciently ﬁnd the
global optimum of this non-convex problem. In particular, we ﬁrst propose the
ELminimizer algorithm, which ﬁnds the optimal EL fair predictor by reducing
the non-convex optimization problem to a sequence of convex constrained opti-
mizations. We then propose a simple algorithm that is computationally more efﬁ-
cient compared to ELminimizer and ﬁnds a sub-optimal EL fair predictor using
unconstrained convex programming tools. Experiments on real-world data show
the effectiveness of our algorithms."
INTRODUCTION,0.007407407407407408,"1
INTRODUCTION"
INTRODUCTION,0.011111111111111112,"As machine learning (ML) algorithms are increasingly being used in applications such as education,
lending, recruitment, healthcare, criminal justice, etc., there is a growing concern that the algorithms
may exhibit discrimination against protected population groups. For example, speech recognition
products such as Google Home and Amazon Alexa were shown to have accent bias (Harwell, 2018).
The COMPAS recidivism prediction tool, used by courts in the US in parole decisions, has been
shown to have a substantially higher false positive rate for African Americans compared to the
general population (Dressel & Farid, 2018). Amazon had been using automated software since 2014
to assess applicants’ resumes, which were found to be biased against women (Dastin, 2018)."
INTRODUCTION,0.014814814814814815,"Various fairness notions have been proposed in the literature to measure and remedy the biases in
ML systems; they can be roughly classiﬁed into two classes: 1) individual fairness focuses on the
equity at individual level and it requires the similar individuals to be treated similarly (Dwork et al.,
2012; Biega et al., 2018; Jung et al., 2019; Gupta & Kamble, 2019); 2) group fairness requires
certain statistical measures to be (approximately) equalized across different groups distinguished by
some sensitive attributes. Their suitability for use is often application dependent, and many of them
are incompatible with each other (Zhang et al., 2019; Hardt et al., 2016; Conitzer et al., 2019; Zhang
et al., 2020; Khalili et al., 2020). Extensive approaches have been developed to satisfying a given
deﬁnition of fairness and they generally fall under three categories: pre-processing, by modifying
the original dataset such as removing certain features and reweighing, e.g., (Kamiran & Calders,
2012; Celis et al., 2020); in-processing, by modifying the algorithms such as imposing fairness
constraints or changing objective functions, e.g., (Zhang et al., 2018; Agarwal et al., 2018; 2019;
Reimers et al., 2021; Calmon et al., 2017); post-processing, by adjusting the output of the algorithms
based on sensitive attributes, e.g., (Hardt et al., 2016)."
INTRODUCTION,0.018518518518518517,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.022222222222222223,"In this paper, we focus on group fairness and we aim to mitigate unfairness issues in supervised
learning using in-processing approaches. The problem can be cast as a constrained optimization
problem where a fair predictor can be found by minimizing the prediction error (i.e., loss) subject to
certain group fairness constraint. In Section 2.1, we present a number of deﬁnitions of commonly
used group fairness notions, namely, statistical parity (Dwork et al., 2012), equal opportunity (Hardt
et al., 2016), equalized loss (Zhang et al., 2019), and bounded group loss (Agarwal et al., 2019).
Here we are particularly interested in equalized loss which requires the expected loss to be equalized
across different groups."
INTRODUCTION,0.025925925925925925,"Constrained optimization problems for ﬁnding a fair predictor have been studied in the iterature. In
general, imposing a fairness criterion to the optimization problem may lead to a non-convex opti-
mization problem. Existing works have proposed various approaches to solving such a non-convex
optimization in different settings. For example, Komiyama et al. (2018) studied the non-convex op-
timization for regression problems under the coefﬁcient of determination constraint. Agarwal et al.
(2019) proposed an approach to ﬁnding a fair regression model under bounded group loss and statis-
tical parity fairness constraints. Agarwal et al. (2018) studied classiﬁcation problems and aimed at
ﬁnding fair classiﬁers under various fairness notions including statistical parity and equal opportu-
nity. In particular, they considered zero-one loss as the objective function and trained a randomized
fair classiﬁer over a ﬁnite hypothesis space; this problem was reduced to a problem of ﬁnding the
saddle point of a linear Lagrangian function in (Agarwal et al., 2018). Zhang et al. (2018) proposed
an adversarial debasing technique to ﬁnd a fair classiﬁer under equalized odd, equal opportunity,
and statistical parity. However, there is no guarantee that this technique ﬁnds the global optimal
solution. The main difference between the present work and the existing in-processing approaches
are as follows: 1) we consider a non-convex problem for ﬁnding a fair predictor satisfying Equalized
Loss fairness notion, which has not been studied in the literature to the best of our knowledge. 2) We
propose algorithms for ﬁnding the global optimal solution to this non-convex problem efﬁciently.
3) Our algorithms are easy to implement and are applicable to both regression and classiﬁcation
problems. 4) Unlike (Agarwal et al., 2018), our algorithms are not limited to ﬁnite hypothesis space."
INTRODUCTION,0.02962962962962963,"Non-convex optimization problems have also been studied in other contexts such as learning over-
parametrized models. For example, deep neural networks are typically trained by solving uncon-
strained, non-convex problems, and methods such as gradient descent may not be suitable as they
are likely to ﬁnd saddle points but not optimums. To address this issue, approaches have been pro-
posed in recent works by incorporating the higher order derivatives (Celis et al., 2020; Anandkumar
& Ge, 2016) or noisy gradients (Ge et al., 2015). However, these methods only ﬁnd a local minimum
(not a global minimum) and are not applicable to our problem with a non-convex constraint."
INTRODUCTION,0.03333333333333333,"In this work, we develop novel algorithms that ﬁnd the fair (sub-)optimal solutions under Equalized
Loss fairness constraint efﬁciently. Note that while our approach and algorithms are presented in
the context of fair machine learning, they are applicable to any problem that can be formulated as a
constrained optimization problem in the form of minw
w
w L0(www)+αL1(www) s.t. |L0(www)−L1(www)| < γ,
where α is a constant"
INTRODUCTION,0.037037037037037035,"Our main contributions and ﬁndings are as follows.
1. We study the relationship between Equalized Loss (EL) and Bounded Group Loss (BGL) fairness
notions. We show that given the existence of feasible solutions satisfying (approximate) BGL
fairness, imposing (approximate) EL fairness constraint never increase losses of both groups
simultaneously (Theorems 1 and 2 in Section 2.1). These results help policy makers to have a
better understanding of these two fairness notions.
2. We develop an algorithm (ELminimizer) to solve a non-convex constrained optimization prob-
lem that ﬁnds the optimal (approximate) EL fair solution. We show that such non-convex opti-
mization can be reduced to a sequence of convex constrained optimizations and the convergence
property of the algorithm is analyzed (Theorems 3 and 4, Section 3).
3. We develop a simple algorithm for ﬁnding a sub-optimal (approximate) EL fair solution. We
show that a sub-optimal solution is a linear combination of optimal solutions to two uncon-
strained optimizations and it can be found efﬁciently without solving constrained optimizations
(Theorem 5, Section 4).
4. We conduct sample complexity analysis and provide the guarantee on generalization performance
(Theorem 7, Section 5).
5. We validate the theoretical results by conducting experiments on real-world data (Section 6)."
INTRODUCTION,0.040740740740740744,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.044444444444444446,"2
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.04814814814814815,"Consider a supervised learning problem where the training dataset consists of triples (X
X
X, A, Y )
from two social groups. Random variable X
X
X ∈X ⊂Rdx is the feature vector (in form of a column
vector), A ∈{0, 1} is the sensitive attribute (e.g., race, gender) indicating the group membership,
and Y ∈Y ⊂R is the label. The feature vector X
X
X may or may not include sensitive attribute
A. Label Y can be either discrete or continuous depending on the given problem: if Y is discrete
(resp. continuous), then the problem is a classiﬁcation (resp. regression) problem. Let F be a set
of predictors fw
ww : X →R parameterized by weight vector www ∈Rdw.1 Consider loss function l :
Y ×X →R where l(Y, fw
ww(X
X
X)) measures the error of fw
ww in predicting label Y . Denote the expected
loss with respect to the joint probability distribution of (X
X
X, Y ) by L(www) := E{l(Y, fww
w(X
X
X))}. Then,
La(www) := E{l(Y, fw
ww(X
X
X))|A = a} denotes the expected loss of the group with attribute A = a."
PROBLEM FORMULATION,0.05185185185185185,"A predictor that minimizes the total expected loss, i.e., arg minww
w L(www), can be biased against certain
groups. To mitigate the risk of unfairness, various fairness notions have been proposed in the litera-
ture. Some of the most commonly used notions of group fairness are as follows: 1) Statistical Parity
(SP) (Dwork et al., 2012) implies that the predictor and the sensitive attribute should be indepen-
dent, i.e., fw
ww(X
X
X) ⊥A; 2) Equal Opportunity (EqOpt) (Hardt et al., 2016) requires that conditional
on Y = 1, prediction and sensitive attribute are independent, i.e., fww
w(X
X
X) ⊥A|Y = 1; 3) Equalized
Odds (EO) (Hardt et al., 2016) requires the conditional independence between prediction and sen-
sitive attribute given Y , i.e., fw
ww(X
X
X) ⊥A|Y ; 4) Equalized Loss (EL) (Zhang et al., 2019; Berk et al.,
2021) requires that the losses experienced by different groups are equalized, i.e., L0(www) = L1(www);
5) Bounded Group Loss (BGL) (Agarwal et al., 2019) requires that the loss experienced by each
group is bounded."
PROBLEM FORMULATION,0.05555555555555555,"With fairness consideration, the goal is to ﬁnd weight vector www that minimizes total expected loss in
predicting Y given X
X
X, subject to certain fairness condition, i.e., minw
w
w L(www) s.t. fairness constraint.
This is a typical formulation in fair machine learning literature, and above method of ﬁnding a fair
predictor belongs to in-processing approaches. Because such constrained optimization can be non-
convex, ﬁnding the optimal solution efﬁciently can be challenging. In this work, we develop novel
algorithms that solves such an optimization problem udder EL fairness constraint."
PROBLEM FORMULATION,0.05925925925925926,"2.1
EQUALIZED LOSS (EL) AND BOUNDED GROUP LOSS (BGL)"
PROBLEM FORMULATION,0.06296296296296296,"As mentioned in Section 2, various fairness notions have been introduced in the literature. Among
them, Statistical Parity (SP), Equal Opportunity (EqOpt), Equalized Odds (EO), and Bounded
Group Loss (BGL) have been studied extensively in the literature, and both in-processing and post-
processing approaches have been developed to satisfy these constraints (Dwork et al., 2012; Agarwal
et al., 2018; Hardt et al., 2016; Zafar et al., 2019; Fitzsimons et al., 2019). Note that different fairness
notions may be conﬂict with each other and which one to adopt is application and context dependent.
In this work, we are interested in Equalized Loss (EL) fairness notion (Zhang et al., 2019; Berk et al.,
2021) which implies that the prediction error should be the same across different groups,2 and Group
Bounded Loss (BGL) fairness notion (Agarwal et al., 2019) which requires the prediction error of
every group to be bounded. We consider a relaxed version of EL fairness deﬁned as follows."
PROBLEM FORMULATION,0.06666666666666667,"Deﬁnition 1 (γ-EL) A predictor f satisﬁes γ-EL if the expected losses experienced by different
demographic groups satisfy the following,"
PROBLEM FORMULATION,0.07037037037037037,"−γ ≤L0(www) −L1(www) ≤γ.
(1)"
PROBLEM FORMULATION,0.07407407407407407,"Parameter γ controls the degree of fairness; the smaller γ implies the stronger fairness. When γ = 0,
the exact EL fairness is attained. We say a group is disadvantaged if it experiences a larger loss.
Similarly, Group Bounded Loss (BGL) fairness notion is formally deﬁned as follows."
PROBLEM FORMULATION,0.07777777777777778,"Deﬁnition 2 (γ-BGL) A predictor f satisﬁes γ-BGL if the expected loss of each demographic group
is bounded by γ, i.e.,
La(www) ≤γ, ∀a ∈{0, 1}.
(2)"
PROBLEM FORMULATION,0.08148148148148149,"1Predictive models such as logistic regression, linear regression, deep learning models, etc., are parameter-
ized by a weight vector.
2EL has also been referred to as Overall Accuracy Equality in (Berk et al., 2021; Agarwal et al., 2019)."
PROBLEM FORMULATION,0.08518518518518518,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.08888888888888889,"2.2
RELATIONS BETWEEN γ-EL AND γ-BGL"
PROBLEM FORMULATION,0.09259259259259259,"In this section, we formally study the relations between γ-EL and γ-BGL fairness notions. Under
γ-EL fairness constraint, ﬁnding a fair predictor is equivalent to solving the following constrained
optimization problem:
minw
ww L(www) s.t. |L0(www) −L1(www)| ≤γ.
(3)"
PROBLEM FORMULATION,0.0962962962962963,"Let www∗be denoted as the solution to (3) and fw
ww∗is the optimal γ-EL fair predictor. Theorem 1 below
shows that given the existence of a feasible point satisfying γ-BGL fairness, it’s impossible for both
groups experiencing loss larger than γ from the optimal γ-EL fair predictor."
PROBLEM FORMULATION,0.1,"Theorem 1 Consider the following optimization for ﬁnding the optimal γ-BGL fair predictor,"
PROBLEM FORMULATION,0.1037037037037037,"minw
ww L(www) s.t. La(www) ≤γ, ∀a ∈{0, 1}.
(4)"
PROBLEM FORMULATION,0.10740740740740741,"If L0(www∗) > γ and L1(www∗) > γ, then optimization problem (4) does not have a feasible point."
PROBLEM FORMULATION,0.1111111111111111,"Proof 1 We prove by contradiction. Assume ˜w˜w˜w is a feasible point of optimization (4). Note that ˜w˜w˜w
is a feasible point for optimization problem (3) as well. Since both L0(www∗) and L1(www∗) are larger
than γ, we have,"
PROBLEM FORMULATION,0.11481481481481481,"E{l(Y, fw
ww∗)}
=
Pr{A = 0}L0(www∗) + Pr{A = 1}L1(www∗) > γ,
E{l(Y, f ˜
w˜
w˜
w)}
=
Pr{A = 0}L0( ˜w˜w˜w) + Pr{A = 1}L1( ˜w˜w˜w) ≤γ."
PROBLEM FORMULATION,0.11851851851851852,"Therefore, www∗can not be the solution to (3). This contradiction proves that the optimization problem
(4) cannot have a feasible point."
PROBLEM FORMULATION,0.12222222222222222,"Theorem 1 implies that if γ-EL notion leads to an increase of the loss of every demographic group,
then there is no optimal predictor under γ-BGL.3 The next theorem further shows that for any pre-
dictor satisfying γ-EL, it must satisfy 2γ-BGL."
PROBLEM FORMULATION,0.1259259259259259,"Theorem 2 Assume optimization problem (4) has at least one feasible point. Then, we have,"
PROBLEM FORMULATION,0.12962962962962962,"min{L0(www∗), L1(www∗)} ≤γ and max{L0(www∗), L1(www∗)} ≤2γ."
PROBLEM FORMULATION,0.13333333333333333,"Proof 2 Let ˜w˜w˜w be a feasible point of optimization problem (4), then ˜w˜w˜w is also a feasible point to (3). If
min{L0(www∗), L1(www∗)} > γ, then L(www∗) > γ ≥L( ˜w˜w˜w) must hold. This is a contradiction because it
implies that www∗is not an optimal solution to (3). Therefore, min{L0(www∗), L1(www∗)} ≤γ. Similarly,
we can prove max{L0(www∗), L1(www∗)} ≤2γ by contradiction. Assume max{L0(www∗), L1(www∗)} >
2γ. Then, max{L0(www∗), L1(www∗)} −min{L0(www∗), L1(www∗)} > γ which shows that www∗is not a
feasible point for (3). This is a contradiction. Therefore, max{L0(www∗), L1(www∗)} ≤2γ."
PROBLEM FORMULATION,0.13703703703703704,"Theorems 1 and 2 investigated the relations between EL and BGL fairness notions. Since γ-EL
implies 2γ-BGL and it additionally requires the approximate equality across different groups, we
will focus on γ-EL fairness notion in the rest of the paper. Because optimization problem (3) is a
non-convex optimization, ﬁnding the optimal fair γ-EL solution efﬁciently can be challenging. In
the next sections, we propose a number of algorithms that are easy to implement and can solve the
optimization (3) efﬁciently."
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.14074074074074075,"3
OPTIMAL FAIR MODEL UNDER EL FAIRNESS"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.14444444444444443,"In this section, we consider the optimization problem (3) under the EL fairness constraint. Note
that this optimization problem is non-convex and ﬁnding the global optimal solution is difﬁcult.
However, we propose an algorithm which is able to ﬁnd the solution to non-convex optimization (3)
by solving a sequence of convex optimization problems. Before presenting the algorithm, we need
to introduce two assumptions."
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.14814814814814814,"Assumption 1 L0(www), L1(www), and L(www) are strictly convex functions in www."
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.15185185185185185,"3Theorem 1 is related to (Agarwal et al., 2019). In particular, they considered γ-BGL fairness and mentioned
that the equalized loss fairness notion may increase the loss of both groups."
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.15555555555555556,Under review as a conference paper at ICLR 2022
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.15925925925925927,Algorithm 1: Function ELminimizer
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.16296296296296298,"1 ELminimizer(wwwG0,wwwG1, ϵ, γ):"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.16666666666666666,"2
λ0
start = L0(wwwG0)"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.17037037037037037,"3
λ0
end = L0(wwwG1)"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.17407407407407408,"4
Deﬁne ˜L1(www) = L1(www) + γ"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.17777777777777778,"5
i = 0"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.1814814814814815,"6
while λ(i)
end −λ(i)
start > ϵ do"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.18518518518518517,"7
λ(i)
mid = (λ(i)
end + λ(i)
start)/2;"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.18888888888888888,"8
Solve the following convex optimization problem,"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.1925925925925926,"www∗
i = arg min
w
ww
˜L1(www) s.t. L0(www) ≤λ(i)
mid
(5)"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.1962962962962963,"9
λ(i) = ˜L1(www∗
i );"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.2,"10
if λ(i) ≥λ(i)
mid then"
OPTIMAL FAIR MODEL UNDER EL FAIRNESS,0.2037037037037037,"11
λ(i+1)
start = λ(i)
mid; λ(i+1)
end
= λ(i)
end;"
END,0.2074074074074074,"12
end"
ELSE,0.2111111111111111,"13
else"
ELSE,0.21481481481481482,"14
λ(i+1)
end
= λ(i)
mid; λ(i+1)
start = λ(i)
start;"
END,0.21851851851851853,"15
end"
END,0.2222222222222222,"16
i = i + 1;"
END,0.22592592592592592,"17
end"
END,0.22962962962962963,"18
Return www∗
i"
END,0.23333333333333334,"Example 1 Consider a linear classiﬁer fw
w
w(X
X
X) = wwwTX
X
X with squared loss l(Y, fww
w(X
X
X)) = (wwwTX
X
X −
Y )2. In this example, E{l(Y, fw
ww(X
X
X))} = wwwT E{XXT }www −2E{Y X
X
XT }www + E{Y 2} is strictly
convex in www if covariance matrix E{XXT } is positive deﬁnite. Similarly, La(www) is strictly convex
if E{XXT |A = a} is positive deﬁnite."
END,0.23703703703703705,"Let wwwGa be the weight vector minimizing the loss associated with group A = a. That is,"
END,0.24074074074074073,"wwwGa = arg min
ww
w La(www).
(6)"
END,0.24444444444444444,"Since optimization problem (6) is an unconstrained convex optimization problem, wwwGa can be found
efﬁciently by the ﬁrst order condition or the gradient descent. We make the following assumption."
END,0.24814814814814815,"Assumption 2 We assume that the following holds,"
END,0.2518518518518518,L0(wwwG0) ≤L1(wwwG0) and L1(wwwG1) ≤L0(wwwG1).
END,0.25555555555555554,"Algorithm 2: Solving Optimization Problem (3)
Input: wwwG0, wwwG1,ϵ,γ"
END,0.25925925925925924,"1 wwwγ = ELminimizer(wwwG0,wwwG1, ϵ, γ);"
END,0.26296296296296295,"2 www−γ = ELminimizer(wwwG0,wwwG1, ϵ, −γ);"
END,0.26666666666666666,3 if L(wwwγ) ≤L(www−γ) then
END,0.27037037037037037,"4
www∗= wwwγ;"
END,0.2740740740740741,5 end
ELSE,0.2777777777777778,6 else
ELSE,0.2814814814814815,"7
www∗= www−γ;"
END,0.2851851851851852,8 end
END,0.28888888888888886,Output: www∗
END,0.29259259259259257,"Assumption 2 implies that when a group ex-
periences its lowest possible loss, it should
not be the disadvantaged group.
Under
Assumption 2, given wwwG0 and wwwG1, Al-
gorithm 1 with γ
=
0 (i.e., function
ELminimizer(wwwG0,wwwG1, ϵ, 0)) ﬁnds the
optimal 0-EL fair solution, where parame-
ter ϵ > 0 speciﬁes the stopping criterion; as
ϵ →0, the output approaches to the opti-
mal solution. Intuitively, Algorithm 1 solves
non-convex optimization (3) by solving a se-
quence of convex and constrained optimiza-
tion problems. If γ > 0, Algorithm 2 ﬁnds
the optimal predictor under γ-EL using func-
tion ELminimizer.
The convergence of Algorithm 1 for ﬁnding the optimal 0-EL fair solution, and convergence of
Algorithm 2 for ﬁnding the optimal γ-EL fair solution are proved in the following theorems."
END,0.2962962962962963,Under review as a conference paper at ICLR 2022
END,0.3,"Theorem 3 Consider sequences {λ(i)
mid|i = 1, 2, . . .} and {www∗
i |i = 1, 2, . . .} generated by Algo-
rithm 1 when γ = 0, i.e., ELminimizer(wwwG0,wwwG1, ϵ →0, 0). Under Assumptions 1 and 2, we
have,
lim
i→∞www∗
i = www∗and lim
i→∞λ(i)
mid = E{L(Y, fw
w
w∗(X))}"
END,0.3037037037037037,"where fw
ww∗is the optimal 0-EL fair predictor."
END,0.3074074074074074,"Similarly, we can prove the convergence for the approximate EL fairness when γ ̸= 0."
END,0.3111111111111111,"Theorem 4 Assume that L0(wwwG0) −L1(wwwG0) < −γ and L0(wwwG1) −L1(wwwG1) > γ. Then, as
ϵ →0, the output of Algorithm 2 goes to the optimal γ-EL fair solution www∗."
END,0.3148148148148148,"Complexity Analysis: The While loop in Algorithm 1 is executed for O(log(1/ϵ)) times. There-
fore, Algorithm 1 needs to solve a constrained convex optimization problem for O(log(1/ϵ)) times.
Note that constrained convex optimization problems can be efﬁciently solved via sub-gradient meth-
ods (Nedi´c & Ozdaglar, 2009), brier methods (Wright, 2001), stochastic gradient descent with one
projection (Mahdavi et al., 2012), etc. For instance, Nedi´c & Ozdaglar (2009) introduces a sub-
gradient method that ﬁnds the saddle point of the Lagrangian function corresponding to (5) and it
converges at the rate of O(1/k) (k is the number of iterations). Therefore, if ϵ is the maximum error
tolerance for (5), the total time complexity of Algorithm 2 is O(1/ϵ log(1/ϵ))."
END,0.31851851851851853,"4
SUB-OPTIMAL FAIR MODEL UNDER γ-EL"
END,0.32222222222222224,"In Section 3, we have shown that non-convex optimization problem (3) can be reduced to a sequence
of convex constrained optimizations (5), and based on this we proposed an algorithm (Algorithm 2)
that ﬁnds the optimal γ-EL fair predictor. However, the proposed algorithm still requires solving
a convex constrained optimization in each iteration. In this section, we propose another algorithm
which ﬁnds a sub-optimal solution to optimization (3) without solving constrained optimization in
each iteration."
END,0.32592592592592595,"The algorithm consists of two phases in sequence: (1) ﬁnding two weight vectors by solving two
unconstrained convex optimization problems; (2) generating a new weight vector satisfying γ-EL
fairness with the two weight vectors found in the ﬁrst phase. Because of the convexity, two uncon-
strained convex optimization problems in the ﬁrst phase can be solved efﬁciently."
END,0.3296296296296296,"Phase 1: Unconstrained optimization.
In this phase, we remove EL fairness constraint and ﬁrst
solve the following uncontrained optimization problem,"
END,0.3333333333333333,"wwwO = arg min
w
w
w L(www)
(7)"
END,0.337037037037037,"Because L(www) is strictly convex in www, the above optimization problem can be solved efﬁciently using
the gradient descent method. Predictor fww
wO is the optimal predictor without fairness constraint, and
L(wwwO) is the smallest overall expected loss that is attainable. Let ˆa = arg maxa∈{0,1} La(wwwO), i.e.,
group ˆa is the group that is disadvantaged under predictor fw
wwO. Then, for the disadvantaged group
ˆa, we ﬁnd wwwGˆa by solving unconstrained optimization problem (6)."
END,0.34074074074074073,"Phase 2: Binary search to ﬁnd the fair predictor.
For β ∈[0, 1], we deﬁne the followings,"
END,0.34444444444444444,"g(β)
=
Lˆa
 
(1 −β)wwwO + βwwwGˆa

−L1−ˆa
 
(1 −β)wwwO + βwwwGˆa

;"
END,0.34814814814814815,"h(β)
=
L
 
(1 −β)wwwO + βwwwGˆa

,"
END,0.35185185185185186,"where function g(β) can be interpreted as loss disparity between two demographic group under
predictor f(1−β)w
w
wO+βww
wGˆa , and h(β) is the corresponding overall expected loss. Some properties of
functions g(.) and h(.) are summarized in the following theorem."
END,0.35555555555555557,"Theorem 5 Under Assumptions 1 and 2, the followings hold,"
END,0.3592592592592593,"1. There exists β0 ∈[0, 1] such that g(β0) = 0."
END,0.362962962962963,"2. h(β) is strictly increasing in β ∈[0, 1]; g(β) is strictly decreasing in β ∈[0, 1]."
END,0.36666666666666664,Under review as a conference paper at ICLR 2022
END,0.37037037037037035,"Theorem 5 implies that in a dw dimensional space, if we start from wwwO and move toward wwwGˆa along
a straight line, the overall loss increases and the disparity between two groups decreases until we
reach (1 −β0)wwwO + β0wwwGˆa, at which 0-EL fairness is satisﬁed. Note that β0 is the unique root
of g. Since g(β) is a strictly decreasing function, β0 can be found using binary search. For the
approximate γ-EL fairness, there are multiple values of β such that (1 −β)wwwO + βwwwGˆa satisﬁes γ-
EL. Since h(β) is strictly increasing in β, among all β that satisﬁes γ-EL fairness, we would choose
the smallest one. The method for ﬁnding a sub-optimal solution to optimization (3) is described in
Algorithm 3."
END,0.37407407407407406,Algorithm 3: Sub-optimal solution to optimization problem (3)
END,0.37777777777777777,"1 Input: wwwGˆa, wwwO, ϵ, γ"
END,0.3814814814814815,"2 Initialization: gγ(β) = g(β) −γ, i = 0, β(0)
start = 0, β(0)
end = 1"
END,0.3851851851851852,3 if gγ(0) ≤0 then
END,0.3888888888888889,"4
www = wwwO, and go to line 16;"
END,0.3925925925925926,5 end
END,0.3962962962962963,"6 while β(i)
end −β(i)
start > ϵ do"
END,0.4,"7
β(i)
mid = (β(i)
start + β(i)
end)/2;"
END,0.40370370370370373,"8
if gγ(β(i)
mid) ≥0 then"
END,0.4074074074074074,"9
β(i+1)
start = β(i)
mid, β(i+1)
end
= β(i)
end;"
END,0.4111111111111111,"10
end"
ELSE,0.4148148148148148,"11
else"
ELSE,0.4185185185185185,"12
β(i+1)
start = β(i)
start, β(i+1)
end
= β(i)
mid;"
END,0.4222222222222222,"13
end"
END,0.42592592592592593,14 end
END,0.42962962962962964,"15 www = (1 −β(i)
mid)wwwO + β(i)
midwwwGˆa;"
END,0.43333333333333335,16 Output: www
END,0.43703703703703706,"Note that while loop in Algorithm 3 is repeated for O(log(1/ϵ)) times. Since the time complexity
of operations in each loop is O(1), the total time complexity of Algorithm 3 is O(log(1/ϵ)). We can
formally prove that the output returned by Algorithm 3 satisﬁes γ-EL fairness constraint."
END,0.44074074074074077,"Theorem 6 Assume that Assumption 1 holds. If gγ(0) ≤0, then wwwO satisﬁes the γ-EL fairness; if
gγ(0) > 0, then limi→∞β(i)
mid = β(∞)
mid exists, and (1 −β(∞)
mid)wwwO + β(∞)
midwwwGˆa satisﬁes the γ-EL
fairness constraint."
END,0.4444444444444444,"It is worth mentioning, since h(β) is incrasing, we are intrested in ﬁnding the smallest possible β
that (1 −β)wwwO + βwwwGˆa satisﬁes γ-EL. Here, β(∞)
mid is the smallest possible β under which (1 −
β)wwwO + βwwwGˆa satisﬁes γ-EL."
GENERALIZATION PERFORMANCE,0.44814814814814813,"5
GENERALIZATION PERFORMANCE"
GENERALIZATION PERFORMANCE,0.45185185185185184,"So far we proposed algorithms for solving optimization (3). In practice, the joint probability distribu-
tion of (X
X
X, A, Y ) is often unknown and the expected loss needs to be estimated using the empirical
loss. Speciﬁcally, given n samples (X
X
Xi, Ai, Yi), i = 1, . . . , n and predictor fw
ww, the empirical losses
of entire population and each group are deﬁned as follows,"
GENERALIZATION PERFORMANCE,0.45555555555555555,"ˆL(www) = 1 n n
X"
GENERALIZATION PERFORMANCE,0.45925925925925926,"i=1
l(Yi, fw
w
w(X
X
Xi)); ˆLa(www) = 1 na X"
GENERALIZATION PERFORMANCE,0.46296296296296297,"i:Ai=a
l(Yi, fw
w
w(X
X
Xi)),
(8)"
GENERALIZATION PERFORMANCE,0.4666666666666667,"where na = |{i|Ai = a}|. Because γ-EL fairness constraint is deﬁned in terms of expected loss, the
optimization problem of ﬁnding an optimal γ-EL fair predictor using empirical losses is as follows,"
GENERALIZATION PERFORMANCE,0.4703703703703704,"ˆwˆwˆw = arg min
w
ww
ˆL(www) s.t. |ˆL0(www) −ˆL1(www)| ≤ˆγ.
(9)"
GENERALIZATION PERFORMANCE,0.4740740740740741,"Note that ˆγ ̸= γ and one goal in this section is to ﬁnd relation between ˆγ and γ. We aim to investigate
how to determine ˆγ so that with high probability the predictor found by solving problem (9) satisﬁes"
GENERALIZATION PERFORMANCE,0.4777777777777778,Under review as a conference paper at ICLR 2022
GENERALIZATION PERFORMANCE,0.48148148148148145,"γ-EL fairness, and meanwhile ˆwˆwˆw is a good estimate of www∗. To present our result, we make the
following assumption."
GENERALIZATION PERFORMANCE,0.48518518518518516,"Assumption 3 With probability 1 −δ, we have the following,
sup
fw
w
w∈F
|L(www) −ˆL(www)| ≤B(δ, n, F),"
GENERALIZATION PERFORMANCE,0.4888888888888889,"where B(δ, n, F) is a bound that goes to zero as n goes to inﬁnity."
GENERALIZATION PERFORMANCE,0.4925925925925926,"Note that if the class F is learnable with respect to loss function l, then there exists such a bound
B(δ, n, F) that goes to zero as n goes to inﬁnity (Shalev-Shwartz & Ben-David, 2014).4"
GENERALIZATION PERFORMANCE,0.4962962962962963,"Theorem 7 Let F be a set of learnable functions, and let fˆww
w and fw
ww∗be the solution to (9) and
(3) respectively with ˆγ = γ + P"
GENERALIZATION PERFORMANCE,0.5,"a∈{0,1} B(δ, na, F). Then, with probability at least 1 −6δ the
followings hold,
L( ˆwˆwˆw) −L(www∗) ≤2B(δ, n, F) and |L0( ˆwˆwˆw) −L1( ˆwˆwˆw)| ≤γ + 2B(δ, n0, F) + 2B(δ, n1, F)."
GENERALIZATION PERFORMANCE,0.5037037037037037,"Theorem 7 shows that as n0, n1 go to inﬁnity, ˆγ →γ, and both empirical loss and expected loss
satisfy γ-EL. In addition, as n goes to inﬁnity, the expected loss at ˆwww goes to the minimum possible
expected loss. Therefore, solving (9) using empirical loss is equivalent to solving (3) if the number
of data points from each group is sufﬁciently large."
EXPERIMENTS,0.5074074074074074,"6
EXPERIMENTS"
EXPERIMENTS,0.5111111111111111,"6.1
EXPERIMENT 1: QUADRATIC FUNCTIONS"
EXPERIMENTS,0.5148148148148148,"First, we solve optimization problem (3) given the following quadratic functions,
L0(www)
=
(w1 + 5)2 + (w2 + 2)2 + (w3 + 1)2 + 4w1 · w3,"
EXPERIMENTS,0.5185185185185185,"L1(www)
=
(w1 −9)2 + (w2 −9)2 + (w3 −9)2 + w1 · w2 + w2 · w3 + w1 · w3 + 1,
L(www)
=
L0(www) + L1(www).
By the ﬁrst order condition, we obtain wwwG0,wwwG1,wwwO as follows,
wwwG0 = [1, −2, −3]T , wwwG1 = [4.5, 4.5, 4.5]T , wwwO = [24.53, 3.0, 26.53]T"
EXPERIMENTS,0.5222222222222223,"We use Algorithm 1 to ﬁnd the optimal solution to (3) and run Algorithm 3 to ﬁnd a sub-optimal so-
lution. In particular, we adopt the penalty method (Ben-Tal & Zibulevsky, 1997) to solve constrained
convex optimization (5), i.e., by solving the following unconstrained optimization,"
EXPERIMENTS,0.5259259259259259,"min
w
ww L1(www) + t · max{0, (L0(www) −λ(i)
mid)}2,
(10)"
EXPERIMENTS,0.5296296296296297,"where t is the penalty parameter. We solve the optimization problem (10) using gradient descent
with learning rate 0.001 and 10000 iterations. We set penalty parameter t = 0.5 and increase t by
0.1 after every 250 iterations. Note that optimization (5) is convex and the penalty method for a
constrained convex optimization converges to the optimal solution (Ben-Tal & Zibulevsky, 1997)."
EXPERIMENTS,0.5333333333333333,"We compare the our algorithms with a baseline: the solution to optimization problem (3) found
using the penalty method, i.e., by solving the following unconstrained optimization,
min
ww
w L0(www) + L1(www) + t ·

max{0, (L0(www) −L1(www) −γ)}2 + max{0, (L1(www) −L0(www) −γ)}2
.
(11)
When solving the optimization problem (11), we use learning rate 0.001. We set penalty parameter
t = 0.5 and increase it by 0.1 every 250 iterations. Figure 1a illustrates the overall loss L(www) at the
(sub-) optimal points obtained from Algorithms 2 and 3 and the baseline. x-axis represents fairness
parameter γ. Since Algorithm 2 converges to the optimal solution, it achieves the smallest loss.
Figure 1b illustrates the distance of the optimal point www∗from the sub-optimal solutions obtained
by Algorithm 3 and the baseline penalty method. It shows that when γ is sufﬁciently large (less
strict fairness constraint), a sub-optimal solution generated by Algorithm 3 is closer to the optimal
solution than the solution found using the baseline method."
EXPERIMENTS,0.5370370370370371,"4As an example, if F is a compact subset of linear predictors in Reproducing Kernel Hilbert Space (RKHS)
and loss l(y, f(x)) is Lipschitz in f(x) (second argument), then Assumption 3 can be satisﬁed (Bartlett &
Mendelson, 2002). Vast majority of linear predictors such as support vector machine and logistic regression
can be deﬁned in RKHS."
EXPERIMENTS,0.5407407407407407,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.5444444444444444,"0
10
20
30
40
50
γ 235 240 245 250 255 260 265 270 L(w)"
EXPERIMENTS,0.5481481481481482,"Algorithm2
Algorithm3
baseline (a)"
EXPERIMENTS,0.5518518518518518,"0
10
20
30
40
50
γ 0.8 1.0 1.2 1.4 1.6"
EXPERIMENTS,0.5555555555555556,||w −w * ||
EXPERIMENTS,0.5592592592592592,"Algorithm3 vs Algorithm2
baseline vs Algorithm2 (b)"
EXPERIMENTS,0.562962962962963,"0.0
0.1
0.2
0.3
γ 0.36 0.38 0.40 0.42 0.44 0.46 0.48 L(w)"
EXPERIMENTS,0.5666666666666667,"Algorithm2
Algorithm3
baseline (c)"
EXPERIMENTS,0.5703703703703704,"0.0
0.1
0.2
0.3
γ 0.7 0.8 0.9 1.0 1.1 1.2"
EXPERIMENTS,0.5740740740740741,||w −w * ||
EXPERIMENTS,0.5777777777777777,"Algorithm3 vs Algorithm2
baseline vs Algorithm2 (d)"
EXPERIMENTS,0.5814814814814815,"Figure 1: a) Experiment 1: loss as a function of fairness parameter γ. Algorithm 2 and Algorithm
3 signiﬁcantly improve the loss compared to the baseline. b) Experiment 1: distance between the
sub-optimal solution and the optimal solution. Algorithm 3 generates a sub-optimal solution closer
to the optimal solution compared to the baseline. c) Experiment 2: loss as a function of fairness
parameter γ. Both Algorithm 2 and Algorithm 3 outperform the baseline. d) Experiment 2: distance
between the sub-optimal solution and the optimal solution."
EXPERIMENTS,0.5851851851851851,"6.2
EXPERIMENT 2: LOGISTIC REGRESSION AND THE ADULT INCOME DATASET"
EXPERIMENTS,0.5888888888888889,"The adult income dataset is a public dataset containing the information of 48,842 individuals (Ko-
havi, 1996). Each data point includes 14 features including age, education, race, etc. Consider race
(White or Black) as the sensitive attribute, we denote White demographic group by A = 0 and Black
group by A = 1."
EXPERIMENTS,0.5925925925925926,"We ﬁrst pre-process the dataset by removing the data points with a missing value or with the race
other than Black and White and obtain 41,961 data points. Among these data points, 4585 belong
to Black demographic group. For each data point, we convert all the categorical features to one-hot
vectors and result in dx = 110 dimensional features. We then normalize the feature vectors such
that they have zero mean value and unit variance. Our goal is to ﬁnd a logistic regression model
satisfying γ-EL to predict whether the income of an individual is above $50K or not."
EXPERIMENTS,0.5962962962962963,"We use Algorithm 2 and Algorithm 3 with ϵ = 0.01 to ﬁnd the optimal logistic regression model
under EL. We use the penalty method described in equation (11) as the baseline. Similar to Experi-
ment 1, we set learning rate as 0.001 for solving (10) and (11). Penalty parameter t is set to be 0.5
and increases by 0.1 every 250 iterations. Figure 1c illustrates the loss of logistic regression model
trained by Algorithm 2, Algorithm 3, and the baseline. It shows that Algorithm 2 outperforms the
baseline; this is because that the baseline only ﬁnds a sub-optimal solution while Algorithm 2 ﬁnds
the global optimal solution. As mentioned in Section 4, Algorithm 3 ﬁnds a sub-optimal solution
that satisﬁes γ-EL, and its performance can vary from case to case. Even though Algorithm 3 has a
good performance in Experiment 1, it does not outperform the baseline in Experiment 2. Figure 1d
illustrates the distances from the optimal point www∗to the sub-optimal solutions obtained by Algo-
rithm 3 and the baseline penalty method. It shows that the distance from www∗to the solution obtained
under Algorithm 3 is slightly larger than that from www∗to the solution obtained under the baseline."
CONCLUSION,0.6,"7
CONCLUSION"
CONCLUSION,0.6037037037037037,"In this work, we studied the problem of fair supervised learning under the Equalized Loss (EL)
fairness notion which requires the prediction error/loss to be the same across different demographic
groups. By imposing EL constraint, the learning problem can be formulated as a non-convex op-
timization problem. We introduce a number of algorithms that ﬁnd the global optimal solution to
this non-convex optimization problem. In particular, we showed that the optimal solution to such
a non-convex problem can be found by solving a sequence of convex constrained optimizations.
We also introduced a simple algorithm for ﬁnding a sub-optimal solution to the non-convex problem
without solving constrained convex optimization problems. In addition to the theoretical guarantees,
we demonstrated the performance of the proposed algorithm through numerical experiments."
CONCLUSION,0.6074074074074074,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.6111111111111112,"8
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.6148148148148148,"Regarding the theoretical results: This paper includes six Theorems. The proof of Theorem 1 and
Theorem 2 have been provided in the main text. Due to the page limit, the proofs of the other
theorems have been provided in the appendix."
REPRODUCIBILITY STATEMENT,0.6185185185185185,"Regarding the numerical examples: the ﬁrst experiment does not use any dataset, and we study
the performance of our proposed method on quadratic objective functions. The values for hyper-
parameters (including learning and penalty parameter) have been explicitly mentioned in section 6.
In the second numerical example, we used the adult income dataset which is a well-known public
dataset in our community. We explained the data pre-processing procedure in Section 6.2 in details."
ETHICS STATEMENT,0.6222222222222222,"9
ETHICS STATEMENT"
ETHICS STATEMENT,0.6259259259259259,"In this work, we proposed algorithms to ﬁnd fair predictors under the EL fairness notion. We want
to emphasize that selecting a right fairness notion depends on the application and the authors do not
make any suggestions to policy/law makers about choosing or avoiding this fairness notion."
REFERENCES,0.6296296296296297,REFERENCES
REFERENCES,0.6333333333333333,"Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, John Langford, and Hanna Wallach. A re-
ductions approach to fair classiﬁcation. In International Conference on Machine Learning, pp.
60–69. PMLR, 2018."
REFERENCES,0.6370370370370371,"Alekh Agarwal, Miroslav Dudik, and Zhiwei Steven Wu. Fair regression: Quantitative deﬁnitions
and reduction-based algorithms. In International Conference on Machine Learning, pp. 120–129.
PMLR, 2019."
REFERENCES,0.6407407407407407,"Animashree Anandkumar and Rong Ge. Efﬁcient approaches for escaping higher order saddle points
in non-convex optimization. In Conference on learning theory, pp. 81–102. PMLR, 2016."
REFERENCES,0.6444444444444445,"Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002."
REFERENCES,0.6481481481481481,"Aharon Ben-Tal and Michael Zibulevsky. Penalty/barrier multiplier methods for convex program-
ming problems. SIAM Journal on Optimization, 7(2):347–366, 1997."
REFERENCES,0.6518518518518519,"Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal
justice risk assessments: The state of the art. Sociological Methods & Research, 50(1):3–44,
2021."
REFERENCES,0.6555555555555556,"Asia J Biega, Krishna P Gummadi, and Gerhard Weikum. Equity of attention: Amortizing individual
fairness in rankings. In The 41st international acm sigir conference on research & development
in information retrieval, pp. 405–414, 2018."
REFERENCES,0.6592592592592592,"Flavio P Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and
Kush R Varshney. Optimized pre-processing for discrimination prevention. In Proceedings of
the 31st International Conference on Neural Information Processing Systems, pp. 3995–4004,
2017."
REFERENCES,0.662962962962963,"L Elisa Celis, Vijay Keswani, and Nisheeth Vishnoi. Data preprocessing to mitigate bias: A maxi-
mum entropy based approach. In International Conference on Machine Learning, pp. 1349–1359.
PMLR, 2020."
REFERENCES,0.6666666666666666,"Vincent Conitzer, Rupert Freeman, Nisarg Shah, and Jennifer Wortman Vaughan. Group fairness
for the allocation of indivisible goods. In Proceedings of the AAAI Conference on Artiﬁcial Intel-
ligence, volume 33, pp. 1853–1860, 2019."
REFERENCES,0.6703703703703704,"Jeffrey Dastin. Amazon scraps secret ai recruiting tool that showed bias against women. http:
//reut.rs/2MXzkly, 2018."
REFERENCES,0.674074074074074,Under review as a conference paper at ICLR 2022
REFERENCES,0.6777777777777778,"Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. Science
advances, 4(1):eaao5580, 2018."
REFERENCES,0.6814814814814815,"Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.
Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science confer-
ence, pp. 214–226, 2012."
REFERENCES,0.6851851851851852,"Jack Fitzsimons, AbdulRahman Al Ali, Michael Osborne, and Stephen Roberts. A general frame-
work for fair regression. Entropy, 21(8):741, 2019."
REFERENCES,0.6888888888888889,"Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on learning theory, pp. 797–842. PMLR, 2015."
REFERENCES,0.6925925925925925,"Swati Gupta and Vijay Kamble. Individual fairness in hindsight. In Proceedings of the 2019 ACM
Conference on Economics and Computation, pp. 805–806, 2019."
REFERENCES,0.6962962962962963,"Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances
in neural information processing systems, 29:3315–3323, 2016."
REFERENCES,0.7,"Drew Harwell. The accent gap. http://wapo.st/3pUqZ0S, 2018."
REFERENCES,0.7037037037037037,"Christopher Jung, Michael Kearns, Seth Neel, Aaron Roth, Logan Stapleton, and Zhiwei Steven Wu.
Eliciting and enforcing subjective individual fairness. arXiv preprint arXiv:1905.10660, 2019."
REFERENCES,0.7074074074074074,"Faisal Kamiran and Toon Calders. Data preprocessing techniques for classiﬁcation without discrim-
ination. Knowledge and Information Systems, 33(1):1–33, 2012."
REFERENCES,0.7111111111111111,"Mohammad Mahdi Khalili, Xueru Zhang, Mahed Abroshan, and Somayeh Sojoudi. Improving
fairness and privacy in selection problems. arXiv preprint arXiv:2012.03812, 2020."
REFERENCES,0.7148148148148148,"Ron Kohavi. Scaling up the accuracy of naive-bayes classiﬁers: A decision-tree hybrid. In Kdd,
volume 96, pp. 202–207, 1996."
REFERENCES,0.7185185185185186,"Junpei Komiyama, Akiko Takeda, Junya Honda, and Hajime Shimao. Nonconvex optimization for
regression with fairness constraints. In International conference on machine learning, pp. 2737–
2746. PMLR, 2018."
REFERENCES,0.7222222222222222,"Mehrdad Mahdavi, Tianbao Yang, Rong Jin, Shenghuo Zhu, and Jinfeng Yi. Stochastic gradient
descent with only one projection. Advances in neural information processing systems, 25:494–
502, 2012."
REFERENCES,0.725925925925926,"Angelia Nedi´c and Asuman Ozdaglar. Subgradient methods for saddle-point problems. Journal of
optimization theory and applications, 142(1):205–228, 2009."
REFERENCES,0.7296296296296296,"Christian Reimers, Paul Bodesheim, Jakob Runge, and Joachim Denzler.
Towards learning
an unbiased classiﬁer from biased data via conditional adversarial debiasing.
arXiv preprint
arXiv:2103.06179, 2021."
REFERENCES,0.7333333333333333,"Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014."
REFERENCES,0.737037037037037,"Stephen J Wright. On the convergence of the newton/log-barrier method. Mathematical program-
ming, 90(1):71–100, 2001."
REFERENCES,0.7407407407407407,"Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P Gummadi. Fair-
ness constraints: A ﬂexible approach for fair classiﬁcation. The Journal of Machine Learning
Research, 20(1):2737–2778, 2019."
REFERENCES,0.7444444444444445,"Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-
sarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp.
335–340, 2018."
REFERENCES,0.7481481481481481,"Xueru Zhang, Mohammadmahdi Khaliligarekani, Cem Tekin, et al. Group retention when using ma-
chine learning in sequential decision making: the interplay between user dynamics and fairness.
Advances in Neural Information Processing Systems, 32:15269–15278, 2019."
REFERENCES,0.7518518518518519,"Xueru Zhang, Mohammad Mahdi Khalili, and Mingyan Liu. Long-term impacts of fair machine
learning. Ergonomics in Design, 28(3):7–11, 2020."
REFERENCES,0.7555555555555555,Under review as a conference paper at ICLR 2022
REFERENCES,0.7592592592592593,APPENDIX
REFERENCES,0.762962962962963,PROOFS
REFERENCES,0.7666666666666667,"In order to prove Theorem 3, we ﬁrst introduce two lemmas."
REFERENCES,0.7703703703703704,"Lemma 1 Under assumption 2, there exists www ∈Rdw such that L0(www) = L1(www) = L(www) and
λ(1)
start ≤L(www) ≤λ(1)
end."
REFERENCES,0.774074074074074,"Proof. Let h0(β) = L0((1 −β)wwwG0 + βwwwG1) and h1(β) = L1((1 −β)wwwG0 + βwwwG1), and
h(β) = h0(β) −h1(β), β ∈[0, 1]. Note that ∇ww
wLa(wwwGa) = 0 because wwwGa is the minimizer of
La(www). Moreover, ∇2
w
wwLa(www) is positive semi-deﬁnit because La(.) is a strictly convex function."
REFERENCES,0.7777777777777778,"First, we show that L0((1 −β)wwwG0 + βwwwG1) is an increasing function in β, and L1((1 −β)wwwG0 +
βwwwG1) is a decreasing function in β. Note that h′
0(0) = (wwwG1 −wwwG0)T ∇w
wwL0(wwwG0) = 0, and
h′′
0(0) = (wwwG1 −wwwG0)T ∇2
w
wwL0(wwwG0)(wwwG1 −wwwG0) ≥0. This implies that h′
0(β) ≥0, ∀β ∈[0, 1].
Similarly, we can show that h′
1(β) ≤0, ∀β ∈[0, 1]."
REFERENCES,0.7814814814814814,"Note that under Assumption (2), h(0) < 0 and h(1) > 0. Therefore, by the intermediate value
theorem, the exists β ∈(0, 1) such that h(β) = 0. Deﬁne www = (1 −β)wwwG0 + βwwwG1. We have,"
REFERENCES,0.7851851851851852,"h(β) = 0 =⇒L0(www) = L1(www) = L(www)
(12)"
REFERENCES,0.7888888888888889,"wwwG0is minimizer of L0 =⇒L(www) = L0(www) ≥λ(1)
start
(13)"
REFERENCES,0.7925925925925926,"h′
0(β) ≥0, ∀β ∈[0, 1] =⇒h0(1) ≥h0(β) =⇒λ(1)
end ≥L0(www) = L(www)
(14)"
REFERENCES,0.7962962962962963,"Lemma 2 L0(www∗
i ) = λ(i)
mid, where www∗
i is the solution to (5)."
REFERENCES,0.8,"Proof. We proceed by contradiction. Assume that L0(www∗
i ) < λ(i)
mid. Since wwwG1 is not in the feasible
set of (5), ∇ww
wL1(www∗
i ) ̸= 0. This is a contradiction because www∗
i is an interior point of the feasible set
of a convex optimization and cannot be optimal if ∇w
wwL1(www∗
i ) is equal to zero."
REFERENCES,0.8037037037037037,Proof [Theorem 3]
REFERENCES,0.8074074074074075,"Let Ii = [λ(i)
start, λ(i)
end] be a sequence of intervals. It is easy to see that I1 ⊇I2 ⊇· · · and
λ(i)
end−λ(i)
start →0 as i →∞. Therefore, by the Nested Interval Theorem, ∩∞
i=1Ii consists of exactly"
REFERENCES,0.8111111111111111,"one real number λ∗, and both λ(i)
start and λ(i)
end converge to λ∗. Because λ(i)
mid = λ(i)
start+λ(i)
start
2
, λ(i)
mid
also converges to λ∗."
REFERENCES,0.8148148148148148,"Now, we show that L(www∗) ∈Ii for all i. Note that L(www∗) = L0(www∗) ≥λ(1)
start because wwwG0 is the
minimizer of L0. Moreover, λ(1)
end ≥L(www∗) otherwise L(www) < L(www∗) (www is deﬁned in Lemma 1)
and www∗is not optimal solution under 0-EL. Therefore, L(www∗) ∈I1."
REFERENCES,0.8185185185185185,"Now we proceed by induction. Suppose L(www∗) ∈Ii. We show that L(www∗) ∈Ii+1 as well. We
consider two cases."
REFERENCES,0.8222222222222222,"• L(www∗) ≤λ(i)
mid. In this case www∗is a feasible point for (5), and λ(i) ≤L(www∗) ≤λ(i)
mid.
Therefore, L(www∗) ∈Ii+1."
REFERENCES,0.825925925925926,"• L(www∗) < λ(i)
mid. In this case, we proceed by contradiction to show that λ(i) ≥λ(i)
mid.
Assume that λ(i) < λ(i)
mid. Deﬁne g(β) = g0(β)−g1(β), where gi(β) = Li((1−β)wwwG0 +
βwww∗
i ). Note that λ(i) = g1(1) By Lemma 2, g0(1) = λ(i)
mid. Therefore, g(1) = λ(i)
mid −
λ(i) > 0. Moreover, under Assumption 2, g(0) < 0. Therefore, by the intermediate value
theorem, there exists β ∈(0, 1) such that g(β) = 0. Similar to the proof of Lemma 1,
we can show that g0(β) in an increasing function for all β ∈[0, 1]. As a result g0(β) <"
REFERENCES,0.8296296296296296,Under review as a conference paper at ICLR 2022
REFERENCES,0.8333333333333334,"g0(1) = λ(i)
mid. Deﬁne www = (1 −β)wwwG0 + βwww∗
i . We have,"
REFERENCES,0.837037037037037,"g0(β) = L0(www) = L1(www) = L(www) < λ(i)
mid
(15)"
REFERENCES,0.8407407407407408,"L(www∗) < λ(i)
mid
(16)"
REFERENCES,0.8444444444444444,"The last two equations imply that www∗is not an optimal fair solution under 0-EL fairness
constraint. This is a contradiction. Therefore, if L(www∗) > λ(i)
mid, then λ(i) ≥λ(i)
mid. As a
result, L(www∗) ∈Ii+1"
REFERENCES,0.8481481481481481,"By two above cases and the nested interval theorem, we conclude that,"
REFERENCES,0.8518518518518519,"L(www∗) ∈∩∞
i=1Ii,
lim
i→∞λ(i)
mid = L(www∗)"
REFERENCES,0.8555555555555555,"For the second part of the theorem, consider the following,"
REFERENCES,0.8592592592592593,"www∗
∞= arg min
w
w
w L1(www)s.t., L0(www) ≤λ∞
mid = L(www∗)"
REFERENCES,0.8629629629629629,"lim
i→∞www∗
i = www∗
∞"
REFERENCES,0.8666666666666667,"In order to show that www∗
∞is equal to www∗, we proceed by contradiction. Suppose www∗
∞̸= www∗. As a
result, L1(www∗
∞) < L(www∗). Deﬁne η(β) = η0(β) −η1(β), where ηi(β) = Li((1 −β)wwwG0 + βwww∗
∞).
Note that L1(www∗
∞) = η1(1). By Lemma 2, the condition in (5) is binding and η0(1) = L(www∗).
Therefore, η(1) = L(www∗) −L1(www∗
∞) > 0. Moreover, under Assumption 2, η(0) < 0. Therefore,
by the intermediate value theorem, there exists β ∈(0, 1) such that η(β) = 0. Similar to the
proof of Lemma 1, we can show that η0(β) is an increasing function for all β ∈[0, 1]. As a result
η0(β) < η0(1) = L(www∗). Deﬁne www = (1 −β)wwwG0 + βwww∗
∞. We have,"
REFERENCES,0.8703703703703703,"η0(β) = L0(www) = L1(www) = L(www) < L(www∗)
(17)"
REFERENCES,0.8740740740740741,"The last equation implies that www∗is not an optimal fair solution under 0-EL fairness constraint. This
a contradiction. As a result, www∗
∞= ˆwˆwˆw."
REFERENCES,0.8777777777777778,Proof [Theorem 4 ]
REFERENCES,0.8814814814814815,Let www∗be the optimal weight vector under γ-EL.
REFERENCES,0.8851851851851852,"Step 1. we show that one of the following holds,"
REFERENCES,0.8888888888888888,"L0(www∗) −L1(www∗) = γ
(18)
L0(www∗) −L1(www∗) = −γ
(19)"
REFERENCES,0.8925925925925926,"Proof by contradiction. Assume −γ < L0(www∗) −L1(www∗) < γ. This implies that www∗is an inte-
rior point of the feasible set of optimization problem (3). Since www∗̸= www∗
O, then ∇L(www∗) ̸= 0.
As a result, object function of (3) can be improved at www∗by moving toward −∇L(www∗). This a
contradiction. Therefore, |L0(www∗) −L1(www∗)| = γ."
REFERENCES,0.8962962962962963,"Step 2. Function wwwγ = ELminimizer(wwwG0,wwwG0, ϵ, γ) is the solution to the following optimiza-
tion problem,"
REFERENCES,0.9,"min
w
ww Pr{A = 0}L0(www) + Pr{A = 1}L1(www), s.t., L0(www∗) −L1(www∗) = γ
(20)"
REFERENCES,0.9037037037037037,"To show the above claim, notice that the solution to optimization problem (20) is the same as the
following,"
REFERENCES,0.9074074074074074,"min
ww
w Pr{A = 0}L0(www) + Pr{A = 1}˜L1(www), s.t., L0(www∗) −˜L1(www∗) = 0,
(21)"
REFERENCES,0.9111111111111111,"where ˜L1(www) = L1(www) + γ. Since L0(wwwG0) −˜L1(wwwG0) < 0 and L0(wwwG1) −˜L1(wwwG1) > 0, by
Theorem 3, we know that wwwγ = ELminimizer(wwwG0,wwwG0, ϵ, γ) ﬁnd the solution to (21)."
REFERENCES,0.9148148148148149,Under review as a conference paper at ICLR 2022
REFERENCES,0.9185185185185185,"Lastly, because |L0(www∗) −L1(www∗)| = γ, we have,"
REFERENCES,0.9222222222222223,"www∗=

wwwγ
if L(wwwγ) ≤L(www−γ)
www−γ
o.w.
(22)"
REFERENCES,0.9259259259259259,"Thus, Algorithm 2 ﬁnds the solution to (3)."
REFERENCES,0.9296296296296296,Proof [Theorem 5]
REFERENCES,0.9333333333333333,"1. Under Assumption 2, g(1) < 0. Moreover, g(0) ≥0. Therefore, by the intermediate value
theorem, there exists β0 ∈[0, 1] such that g(β0) = 0.
2. Since wwwO is the minimizer of L(www), h′(0) = 0. Moreover, since L(www) is strictly convex,
h′′(0) > 0. As a result, h′(β) > 0 for β > 0.
3. SincewwwGˆa is the minimizer of Lˆa(www), and Lˆa(www) is strictly convex, Lˆa((1−β)wwwO+βwwwGˆa)
is strictly decreasing function.
Note that since h(β) = Pr{A = ˆa}Lˆa((1 −β)wwwO + βwwwGˆa) + Pr{A = 1 −ˆa}L1−ˆa((1 −
β)wwwO + βwwwGˆa) is strictly increasing and Lˆa((1 −β)wwwO + βwwwGˆa) is strictly decreasing,
we conclude that L1−ˆa((1 −β)wwwO + βwwwGˆa) is strictly increasing. As a result, g should be
strictly decreasing."
REFERENCES,0.937037037037037,"Proof [Theorem 6] First, we show that if gγ(0) ≤0, then wwwO satisﬁes γ-EL."
REFERENCES,0.9407407407407408,gγ(0) ≤0 =⇒g(β) −γ ≤0 =⇒Lˆa(wwwO) −L1−ˆa(wwwO) ≤γ
REFERENCES,0.9444444444444444,"Moreover, Lˆa(wwwO) −L1−ˆa(wwwO) ≥0 because ˆa = arg maxa La(wwwO). Therefore, γ-EL is satisﬁed."
REFERENCES,0.9481481481481482,"Secondly, assume that gγ(0) > 0. Under Assumption 1, gγ(1) = Lˆa(wwwGˆa) −L1−ˆa(wwwGˆa) −γ < 0.
Therefore, by the intermediate value there exists β0 such that gγ(β0) = 0. Moreover, gγ is a
strictly decreasing function. Therefore, the binary search proposed in Algorithm 3 converges to root
of gγ(β). As a result, (1 −β(∞)
mid)wwwO + β(∞)
midwwwGˆa satisﬁes satisﬁes γ-EL. Moreover, Lˆa(wwwO) −
L1−ˆa(wwwO) ≥0 because ˆa = arg maxa La(wwwO). Note that since g(β) is decreasing, β(∞)
mid is the
smallest possible β under which (1 −β)wwwO + βwwwGˆa γ-EL. Since h is increasing, the smallest
possible β gives us a better accuracy."
REFERENCES,0.9518518518518518,Proof [Theorem 7]
REFERENCES,0.9555555555555556,"By the triangle inequality, the following holds,"
REFERENCES,0.9592592592592593,"sup
fw
w
w∈F
||L0(www) −L1(www)| −|ˆL0(www) −ˆL1(www)|| ≤sup
fw
w
w∈F
|L0(www) −ˆL0(www)| + sup
fw
w
w∈F
|L1(www) −ˆL1(www)|."
REFERENCES,0.9629629629629629,"(23)
Therefore, with probability at least 1 −2δ we have,"
REFERENCES,0.9666666666666667,"sup
fw
w
w∈F
||L0(www) −L1(www)| −|ˆL0(www) −ˆL1(www)|| ≤B(δ, n0, F) + B(δ, n1, F)
(24)"
REFERENCES,0.9703703703703703,"As a result, with probability 1 −2δ holds,"
REFERENCES,0.9740740740740741,"{www|fw
ww ∈F, |L0(www) −L1(www)| ≤γ} ⊆{www|fww
w ∈F, |ˆL0(www) −ˆL1(www)| ≤ˆγ}
(25)"
REFERENCES,0.9777777777777777,"Now consider the following,"
REFERENCES,0.9814814814814815,"L( ˆwˆwˆw) −L(www∗) = L( ˆwˆwˆw) −ˆL( ˆwˆwˆw) + ˆL( ˆwˆwˆw) −ˆL(www∗) + ˆL(www∗) −L(www∗)
(26)"
REFERENCES,0.9851851851851852,"By (25), ˆL( ˆwˆwˆw)−ˆL(www∗) ≤0 with probability 1−2δ. Thus, with probability at least 1−2δ, we have,"
REFERENCES,0.9888888888888889,"L( ˆwˆwˆw) −L(www∗) ≤L( ˆwˆwˆw) −ˆL( ˆwˆwˆw) + ˆL(www∗) −L(www∗).
(27)
Therefore, under assumption 3, we can conclude with probability at least 1 −6δ, L( ˆwˆwˆw) −L(www∗) ≤
2B(δ, n, F). In addition, by (24), with probability at least 1 −2δ, we have,"
REFERENCES,0.9925925925925926,Under review as a conference paper at ICLR 2022
REFERENCES,0.9962962962962963,"|L0( ˆwˆwˆw) −L1( ˆwˆwˆw)|
≤
B(δ, n0, F) + B(δ, n1, F) + |ˆL0(www) −ˆL1(www)|
≤
ˆγ + B(δ, n0, F) + B(δ, n1, F) = γ + 2B(δ, n0, F) + 2B(δ, n1, F)"
