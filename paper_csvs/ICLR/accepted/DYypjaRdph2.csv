Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00425531914893617,"Human decision making is well known to be imperfect and the ability to analyse
such processes individually is crucial when attempting to aid or improve a decision-
maker‚Äôs ability to perform a task, e.g. to alert them to potential biases or oversights
on their part. To do so, it is necessary to develop interpretable representations of
how agents make decisions and how this process changes over time as the agent
learns online in reaction to the accrued experience. To then understand the decision-
making processes underlying a set of observed trajectories, we cast the policy
inference problem as the inverse to this online learning problem. By interpreting
actions within a potential outcomes framework, we introduce a meaningful mapping
based on agents choosing an action they believe to have the greatest treatment
effect. We introduce a practical algorithm for retrospectively estimating such
perceived effects, alongside the process through which agents update them, using
a novel architecture built upon an expressive family of deep state-space models.
Through application to the analysis of UNOS organ donation acceptance decisions,
we demonstrate that our approach can bring valuable insights into the factors that
govern decision processes and how they change over time."
INTRODUCTION,0.00851063829787234,"1
INTRODUCTION"
INTRODUCTION,0.01276595744680851,"Decision modelling is often viewed through the lens of policy learning (Bain & Sammut, 1995;
Abbeel & Ng, 2004), where the aim is to learn some imitation policy that captures the actions (and
thus decisions) of an agent in some structured way (H√ºy√ºk et al., 2021a). Unfortunately, this usually
implicitly relies on the assumption that the actions taken are already optimal with respect to some
objective, and hence do not change over time. While this might be appropriate to approximate the
stationary policy of an autonomous agent, such assumptions often break down when applying them
to more Ô¨Çexible agents that might be learning on-the-Ô¨Çy. In particular, we may want to model the
decision making process of a human decision-maker, whose actions we may see executed over time
- this is clearly of great use in a range of Ô¨Åelds including economics and medicine (Hunink et al.,
2014; Zavadskas & Turskis, 2011). In the context of machine learning, they would be considered
to be undergoing a process of online learning (Hoi et al., 2018); training while interacting with
the environment, where it is to be expected that beliefs and strategies are constantly adapted based
on accumulating experience (Elio & Pelletier, 1997). Additionally, such belief updates are often
imperfect; for example, people are known to a priori over-weight the potential impact of rare events
when they Ô¨Årst start a task, before proceeding to neglect them when they do not occur in their
experience (Hertwig & Erev, 2009). If our goal is to analyse human decision-making, it is thus
important to be able to model non-stationary policies that are potentially imperfect in two senses:
both marginally at each time-step, and in the way that they become adapted to new information."
INTRODUCTION,0.01702127659574468,"In this work, instead of relying on the assumption that an agent follows a stationary optimal policy,
we consider their actions to be consistent within an online leaning framework where they learn from
an incoming sequence of examples (Hoi et al., 2018) and update their policies accordingly, as in
Figure 1. Thus, instead of assuming that actions are globally optimal, we assume that at any exact
time the agent thought that the action was better than the alternatives. As a natural consequence, we
frame the problem of decision modelling as that of inverse online learning; that is, we aim to uncover"
INTRODUCTION,0.02127659574468085,Published as a conference paper at ICLR 2022
INTRODUCTION,0.02553191489361702,"t = 1
t = 2
t = 3 ùëø ùíÄ(ùíÇ) ùëø
ùëø"
INTRODUCTION,0.029787234042553193,Observations
INTRODUCTION,0.03404255319148936,"True Response 
Surfaces
Perceived 
Response Surfaces"
INTRODUCTION,0.03829787234042553,"Perceived 
Treatment Effect
ùíÄ(ùíÇ)
ùíÄ(ùíÇ)"
INTRODUCTION,0.0425531914893617,"Figure 1: Capturing online learning behaviour. We assume the agent models the response surfaces
in their mind, and that their belief changes each time step in response to the actions they take and the
consequent observations they make. The response surface they model need not converge on the true
surface since the update method of the agent may be biased and not appropriately use the information
available to them."
INTRODUCTION,0.04680851063829787,"the process through which an agent is learning online. Note that this does not involve coming up
with a new way to do online learning itself, rather we recover how it appears a given agent may have
learnt. We do this by modelling the (non-stationary) policy of the agent using a deep state-space
model (Krishnan et al., 2017), allowing us to track the memory of the agent while allowing policies to
evolve in a non-parametric and non-prescribed fashion. We construct an interpretable mapping from
the accrued memory to policies by framing the decision-making process within a potential-outcome
setting (Rubin, 2005); letting actions correspond to interventions (or treatments), and assuming that
agents make decisions by choosing an action which they perceive to have the highest (treatment) effect."
INTRODUCTION,0.05106382978723404,"Contributions. In this work we make a number of contributions: First, we formalise the inverse
online learning problem and a connection between decision modelling, online learning and po-
tential outcomes, highlighting how they complement each other to result in an interpretable un-
derstanding of policies (Section 2); Second, we introduce a practical method to estimate the non-
stationary policy and update method of an agent given demonstrations (Section 4); And third,
we demonstrate how we can uncover useful practical insights into the decision making process
of agents with a case study on the acceptance of liver donation offers (Section 5).
Code is
made available at https://github.com/XanderJC/inverse-online, along with the
group codebase at https://github.com/vanderschaarlab/mlforhealthlabpub."
PROBLEM FORMALISATION,0.05531914893617021,"2
PROBLEM FORMALISATION"
PROBLEM FORMALISATION,0.059574468085106386,"Preliminaries.
Assume we observe a data trajectory1 of the form D = {(Xt, At, Yt)}T
t=1.
Here, Xt
‚ààX
‚äÇRd, denotes a context vector of possible confounders; At
‚ààA =
{0, 1}, a binary action or intervention; and Yt
‚àà
Y, a binary or continuous outcome
of interest.
The subscript t indicates a time-ordering of observed triplets; any time-step
t is generated by (i) arrival of context x, (ii) an intervention a being performed by an
agent according to some policy œÄt(x) and (iii) a corresponding outcome y being observed. Time"
PROBLEM FORMALISATION,0.06382978723404255,Perceived Effect ùëå(ùëé) ùê¥= 1 ùê¥= 0
PROBLEM FORMALISATION,0.06808510638297872,"Figure 2: SimpliÔ¨Åed inverse process. Our task
is to infer the evolving perceived effect over time
(the blue line) given only realisations of the agent‚Äôs
behaviour."
PROBLEM FORMALISATION,0.07234042553191489,"Goal.
We are interested in recovering the
agent‚Äôs non-stationary policy œÄt(x) = P(A =
1|X
=
x, Ht
=
ht), which depends on
the observed context x and can change over
time due to the observed history Ht
=
{(Xk, Ak, Yk)}t‚àí1
k=1. We make the key assump-
tion that the agent acts with the intention to
choose an intervention a leading to the best
(largest) potential outcome Y (a), but that the
perception of the optimal intervention may
change over time throughout the agent‚Äôs learn-
ing process about their environment. This prob-
lem formalisation naturally places us at the in-
tersection of treatment effect inference within the Neyman-Rubin potential outcomes (PO) framework"
PROBLEM FORMALISATION,0.07659574468085106,"1We assume with a single trajectory here, though learning and inference can be applied over a dataset of
multiple trajectories"
PROBLEM FORMALISATION,0.08085106382978724,Published as a conference paper at ICLR 2022
PROBLEM FORMALISATION,0.0851063829787234,"(Rubin, 2005) with the classical (inverse) reinforcement learning (RL) paradigm (Abbeel & Ng,
2004). More speciÔ¨Åcally though the RL-subproblem of logged contextual bandits (Swaminathan &
Joachims, 2015; Atan et al., 2018), where the setting is ofÔ¨Çine and the contexts arrive independently."
PROBLEM FORMALISATION,0.08936170212765958,"Perception Versus Reality. As is standard within the PO framework, we assume that any observation
has two potential outcomes Y (a), associated with each intervention (treatment-level), which are
generated from some true distribution inherent to the environment, i.e. Y (0), Y (1) ‚àºPe(¬∑|X = x),
yet only the potential outcome associated with the chosen treatment is revealed to the agent. If
the agent had full knowledge of the environment, its optimal policy would be to choose an action
as arg maxa ¬µe
a(x) where ¬µe
a(x) = EPe[Y (a)|X = x], or conversely to choose to perform the
intervention only when its expected conditional average treatment effect (CATE) œÑ e(x) = ¬µe
1(x) ‚àí
¬µe
0(x) is positive. In general, we assume that such perfect knowledge of the environment is not
available to the agent in many scenarios of practical interest. Instead, we assume that at any
time t, the agent acts according to some perceived model ÀúP of its environment, which induces
the perceived expected potential outcomes Àú¬µa(x; t) and associated treatment effect ÀúœÑ(x; t). Here,
Àú¬µa(x; t) = EÀúP[Y (a)|X = x, Ht = ht], is a function not only of the observed context but also
the history, such that the agent can update its beliefs according to observed outcomes. Note that
we allow the agent‚Äôs model of the environment to be misspeciÔ¨Åed; in this case Àú¬µa(x; t) would
not converge to ¬µe
a(x) even for very long trajectories. We consider this an important feature, as
we assume that human decision-makers generally need to make simpliÔ¨Åcations (GrifÔ¨Åths et al.,
2015). Formally, we make the following assumption about the agent‚Äôs decision making process:"
PROBLEM FORMALISATION,0.09361702127659574,"Assumption 1 Mutual Observability: The observable space X contains only and all the information
available to the agent at the point of assigning an action."
PROBLEM FORMALISATION,0.09787234042553192,"Assumption 2 Perceived Optimality: An agent assigns the action they think will maximise the
outcome. For a deterministic policy œÄt, the agent believes œÄt(x) = 1 ‚áê‚áí
Àú¬µ1(x; t) > Àú¬µ0(x; t).
Given a stochastic agent, the policy œÄt(x) = P(At = 1|x) is a monotonic increasing function of the
agent‚Äôs belief over the treatment effect ÀúœÑ(x; t)."
PROBLEM FORMALISATION,0.10212765957446808,"Assumption 3 Continual Adaptation: The agent may continually adjust their strategy based on
experience. The index in the dataset D represents a temporal ordering which œÄ is non-stationary with
respect to, i.e. œÄi(x) may not equal œÄj(x), i Ã∏= j ‚â•1 where œÄi represents the policy at time step i."
PROBLEM FORMALISATION,0.10638297872340426,"Assumptions 1 and 2 are generally unveriÔ¨Åable in practice, yet, they are necessary: there would be
little opportunity to learn anything meaningful about a non-stationary policy without them. Mutual
observability is crucial, as assuming that different information streams are available to the observer and
the agent would make it impossible to accurately describe the decision-making process. Assumption
2, albeit untestable, appears very reasonable: in the medical setting, for example, a violation would
imply malicious intent ‚Äì i.e. that a clinician purposefully chooses the intervention with a sub-optimal
expected outcome. Nonetheless, this does mean that we do not consider exploratory agents that are
routinely willing to sacriÔ¨Åce outcomes for knowledge, a constraint we do not consider too restrictive
within the medical applications we have in mind. Assumption 3, on the other hand, is not at all
restrictive; on the contrary, it explicitly speciÔ¨Åes a more Ô¨Çexible framework than is usual in imitation
or inverse reinforcement learning as the set of policies it describes contains stationary policies."
RELATED WORK,0.11063829787234042,"3
RELATED WORK"
RELATED WORK,0.1148936170212766,"Our problem setting is related to, but distinct from, a number of problems considered within related
literature. Effectively, we assume that we observe logged data generated by a learning agent that acts
according to its evolving belief over effects of its actions, and aim to solve the inverse problem by mak-
ing inferences over said belief given the data. Below, we discuss our relationship to work studying both
the forward and the inverse problem of decision modelling, with main points summarised in Table 1."
RELATED WORK,0.11914893617021277,"The Forward Problem: Inferring Prescriptive Models for Behaviour.
The agent, whose (non-
stationary) policy we aim to understand, actively learns online through repeated interaction with an
environment and thus effectively solves an online learning problem (Hoi et al., 2018). The most
prominent example of such an agent would be a contextual bandit (Agarwal et al., 2014), which
learns to assign sequentially arriving contexts to (treatment) arms. Designing agents that update"
RELATED WORK,0.12340425531914893,Published as a conference paper at ICLR 2022
RELATED WORK,0.1276595744680851,"policies online is thus the inverse of the problem we consider here. Another seemingly related
problem within the RL context is learning optimal policies from logged bandit data (Swaminathan
& Joachims, 2015); this is different from our setting as it is prescriptive rather than descriptive."
RELATED WORK,0.13191489361702127,"If the goal was to infer the true œÑ e(x) from the (logged) observational data D (instead of the effect
perceived by the agent), this would constitute a standard static CATE estimation problem, for
which many solutions have been proposed in the recent ML literature (Alaa & van der Schaar,
2018; Shalit et al., 2017; K√ºnzel et al., 2019). Note that within this literature, the treatment-
assignment policy (the so-called propensity score œÄ(x) = P(A = 1|X = x)) is usually assumed to
be stationary and considered a nuisance parameter that is not of primary interest. Estimating CATE
can also be a pretext task to the problem of developing optimal treatment rules (OTR) (Zhang et al.,
2012a;b; Zhao et al., 2012), as g(x) = 1(œÑ(x) > 0) is such an optimal rule (Zhang et al., 2012b)."
RELATED WORK,0.13617021276595745,"The Backward Problem: Obtaining a Descriptive Summary of an Agent.
When no outcomes
are observed (unlike in the bandit problem) the task of learning a policy becomes imitation learning
(IL), itself a large Ô¨Åeld that aims to match the policy of a demonstrator (Bain & Sammut, 1995; Ho
& Ermon, 2016). Note that, compared to the bandit problem, there is a subtle shift from obtaining
a policy that is optimal with respect to some true notion of outcome to one that minimises some
divergence from the demonstrator. While IL is often used in the forward problem to Ô¨Ånd an optimal
policy ‚Äì thereby implicitly assuming that the demonstrator is themselves acting optimally ‚Äì, it
can also, if done interpretably, be used in the backward problem to very effectively reason about
the goals and preferences of the agent (H√ºy√ºk et al., 2021a). One way to achieve this is through
inverse reinforcement learning (IRL), which aims to recover the reward function that is seemingly
maximised by the agent (Ziebart et al., 2008; Fu et al., 2018; Chan & van der Schaar, 2021). This
need not be the true reward function, and can be interpreted as a potentially more compact way
to describe a policy, which is also more easily portable given shifts in environment dynamics."
RELATED WORK,0.14042553191489363,"Our problem formalisation is conceptually closely related to the counterfactual inverse reinforcement
learning (CIRL) work of Bica et al. (2021). There the authors similarly aim to explain decisions
based on the PO framework, speciÔ¨Åcally by augmenting a max-margin IRL (Abbeel & Ng, 2004)
approach to parameterise the learnt reward as a weighted sum of counterfactual outcomes. How-
ever, CIRL involves a pre-processing counterfactual estimation step that focuses on estimating
the true treatment effects, and implicitly assumes that these are identical to the ones perceived
by the agent (i.e. it assumes that the agent has perfect knowledge of the environment dynamics
generating the potential outcomes), marking a signiÔ¨Åcant departure from our (much weaker) as-
sumptions. Without the focus on treatment effects, our method could be seen as a generalised
non-parametric extension of the inverse contextual bandits of H√ºy√ºk et al. (2021b). In general,
the backward problem is hard to evaluate empirically since information about the true beliefs of
agents is not normally available in the data, thus relying on simulation to validate (Chan et al., 2021)."
RELATED WORK,0.14468085106382977,"Table 1: Problems considered in related work, noting the relationship between both input and output."
RELATED WORK,0.14893617021276595,"Problem
Input
Target quantity"
RELATED WORK,0.15319148936170213,"Online Learning of Optimal Polices (Agarwal et al., 2014)
(Xt, At, Yt)
œÄopt
t
(x)
Learning Optimal Policy from Logged Bandits (Swaminathan & Joachims, 2015)
(Xi, Ai, Yi)
œÄopt(x)
Estimating Heterogeneous Treatment Effects (Alaa & van der Schaar, 2018)
(Xi, Ai, Yi)
œÑ e(x)
Learning Imitator Policies via Imitation Learning (Bain & Sammut, 1995)
(Xi, Ai)
œÄobs(x)
Learning Reward Functions via IRL (Abbeel & Ng, 2004)
(Xi, Ai)
Robs(x)
Learning Reward Functions via CIRL (Bica et al., 2021)
(Xi, Ai, Yi)
Robs(x)"
RELATED WORK,0.1574468085106383,"Modelling Dynamic Policies via Inverse Online Learning
(Xt, At, Yt)
œÄobs
t
(x)"
INFERRING THE ONLINE LEARNING PROCESS,0.16170212765957448,"4
INFERRING THE ONLINE LEARNING PROCESS"
INFERRING THE ONLINE LEARNING PROCESS,0.16595744680851063,"Preliminaries: The Forward Problem. In the forward context, the problem we consider amounts
to an agent attempting to, at each time-step, take the action they believe will maximise the out-
come, without being aware of the true effect of their intervention. We assume that the agent
believes the potential outcomes are a linear combination of the contextual features such that
Àú¬µa(Xt; t) = ‚ü®Xt, œât
a‚ü©, with œât
a ‚àà‚Ñ¶a set of weights for the action a at time t, and ‚ü®¬∑, ¬∑‚ü©de-
noting the inner product. Normally we may consider a linear model an oversimpliÔ¨Åcation, but"
INFERRING THE ONLINE LEARNING PROCESS,0.1702127659574468,"Published as a conference paper at ICLR 2022 ùë¥"""
INFERRING THE ONLINE LEARNING PROCESS,0.17446808510638298,"ùê¥""
ùëã""
ùëå"" ùùâ"" ùë¥'"
INFERRING THE ONLINE LEARNING PROCESS,0.17872340425531916,"ùê¥'
ùëã'
ùëå' ùùâ' ùë¥("
INFERRING THE ONLINE LEARNING PROCESS,0.1829787234042553,"ùê¥(
ùëã(
ùëå( ùùâ( Agent"
INFERRING THE ONLINE LEARNING PROCESS,0.18723404255319148,"Environment
‚Ñé""
‚Ñé'
‚Ñé* ùúã, ùíÆ, ùí™,"
INFERRING THE ONLINE LEARNING PROCESS,0.19148936170212766,"Figure 3: Model. Solid black lines indicate single time step dependencies. Given covariates Xi and
an observed history then treatment effects are estimated in the mind of the agent. An action Ai is
selected based on these effect and an outcome Yi is observed. The dashed grey lines then indicate
information Ô¨Çow over time."
INFERRING THE ONLINE LEARNING PROCESS,0.19574468085106383,"when modelling the thought process of an agent it appears much more reasonable - for example,
does a doctor consider higher order terms when weighing up potential outcomes of a patient? It
seems unlikely, especially as clinical guidelines are often given as linear cutoffs (Burgers et al.,
2003). The considered forward problem thus proceeds as follows for every given time step t:"
INFERRING THE ONLINE LEARNING PROCESS,0.2,"1. A context Xt ‚àºPX arrives independently over time. We make no assumptions on the form of
PX , in particular noting that there is no need for any level of stationarity.
2. Given Xt and the current set of weights {œât
1, œât
0}, the agent predicts their perceived potential
outcomes Àú¬µ1(Xt; t) and Àú¬µ0(Xt; t).
3. Action At is assigned by some probabilistic optimal treatment rule œÄ : Y2 7‚Üí[0, 1] such that
P(At = 1|Xt) = œÄ(Àú¬µ1(Xt; t), Àú¬µ0(Xt; t)), before the associated true outcome is observed.
4. Based on this new information the agent (potentially stochastically) updates their belief over the
response surface for the taken action according to some update function f : ‚Ñ¶2 √ó X √ó A √ó Y 7‚Üí
‚ñ≥(‚Ñ¶2) such that œât+1
1
, œât+1
0
‚àºf(œât
1, œât
0, Xt, At, Yt)."
INFERRING THE ONLINE LEARNING PROCESS,0.20425531914893616,"To maximise outcomes it is crucial to employ an update function f that maximally captures
the available information at each step of the process.
This then allows for an appropriate
determination of œÄ that aims to maximise the total (potentially future discounted) outcomes."
A MODEL OF INVERSE ONLINE LEARNING,0.20851063829787234,"4.1
A MODEL OF INVERSE ONLINE LEARNING"
A MODEL OF INVERSE ONLINE LEARNING,0.2127659574468085,"Here, we focus on the inverse to the problem outlined above, which, to the best of our knowl-
edge, has received little to no attention in related work.
That is, after observing an agent‚Äôs
actions, can we recover how they arrived at their decisions, and how this changed over time?
In the following, we use vector notation ‚Éóxt:t‚Ä≤ = {xi}t‚Ä≤
i=t with ‚Éóx = ‚Éóx1:T for all realised vari-
ables, with histories ‚Éóht:t‚Ä≤ = {(xi, ai, yi)}t‚Ä≤
i=t .
Note that the previously described forward
model assumes a factorisation of the full generative distribution of the observed data given by:"
A MODEL OF INVERSE ONLINE LEARNING,0.2170212765957447,"p(‚Éóx,‚Éóa, ‚Éóy) = T
Y"
A MODEL OF INVERSE ONLINE LEARNING,0.22127659574468084,"t=1
p(yt|at, xt) √ó p(at|‚Éóx1:t‚àí1,‚Éóa1:t‚àí1, ‚Éóy1:t‚àí1) √ó p(xt).
(1)"
A MODEL OF INVERSE ONLINE LEARNING,0.225531914893617,"We assume that p(xt) and p(yt|at, xt) are governed by some true environment dynam-
ics, independent from the perception of the agent and thus not part of our modelling
problem.
This seems trivially true,
as the perception of the agent will neither af-
fect the contexts that arrive nor the true outcome conditional on the action being taken."
A MODEL OF INVERSE ONLINE LEARNING,0.2297872340425532,"Our model for the evolving beliefs of an agent with respect to their interaction with the environment
revolves around a specialised deep state-space model. Crucially, this takes the form of a latent
random variable Mt ‚ààRd, which captures the memory of the agent at time t, and can evolve in
a Ô¨Çexible way based on the observed history. We consider the memory to be an efÔ¨Åcient sum-
mary of the history, which can be interpreted as a sufÔ¨Åcient statistic for the beliefs of the agent
at a given time. Naturally, this memory is unobserved and must be learnt in an unsupervised"
A MODEL OF INVERSE ONLINE LEARNING,0.23404255319148937,Published as a conference paper at ICLR 2022
A MODEL OF INVERSE ONLINE LEARNING,0.23829787234042554,"manner. The introduction of this latent variable leads to an extended probabilistic model with a
more structured factorisation of the conditional distribution of the actions, and a model given by:"
A MODEL OF INVERSE ONLINE LEARNING,0.2425531914893617,"pŒ∏(‚Éóa|‚Éóx, ‚Éóy) = T
Y"
A MODEL OF INVERSE ONLINE LEARNING,0.24680851063829787,"t=1
œÄŒ∏(at|ÀúœÑt)
|
{z
}
Treatment Rule"
A MODEL OF INVERSE ONLINE LEARNING,0.251063829787234,"√ó OŒ∏(ÀúœÑt|xt, mt)
|
{z
}
Outcome Estimation"
A MODEL OF INVERSE ONLINE LEARNING,0.2553191489361702,"√ó SŒ∏(mt|‚Éóx1:t‚àí1,‚Éóa1:t‚àí1, ‚Éóy1:t‚àí1)
|
{z
}
Memory Aggregation .
(2)"
A MODEL OF INVERSE ONLINE LEARNING,0.25957446808510637,"This model consists of three components: a memory summarisation network SŒ∏ : H 7‚Üí‚ñ≥(M),
which maps the history into a (distribution over) memory; a (perceived) potential outcome predictor
network OŒ∏ : X √ó M 7‚ÜíR2, that takes the memory and a context and predicts the perceived
potential outcomes; and a treatment rule œÄŒ∏ : Y2 7‚Üí[0, 1], that given the potential outcomes
(summarised by the perceived treatment effect) outputs a distribution over actions. Throughout,
Œ∏ denotes the full set of parameters of the generative model and notation is shared across compo-
nents to reÔ¨Çect that they can all be optimised jointly. Below, we discuss each component in turn."
A MODEL OF INVERSE ONLINE LEARNING,0.26382978723404255,"Memory
Aggregation.
We
deÔ¨Åne
the
base
structure
of
SŒ∏
to
be
relatively
sim-
ple and recursive, and assume the memory at a given time step to be distributed as:
Mt ‚àºN
 
¬µt, Œ£t

with
¬µt, Œ£t = MLP([Mt‚àí1, Xt‚àí1, At‚àí1, Yt‚àí1]),
(3)
where ¬µt and Œ£t are two output heads of the memory network S given the history ht‚àí1. It is
important to note that the memory network does not get the t-th step information (including the
context) when predicting the memory at time t - this means the network cannot provide any spe-
ciÔ¨Åc predictive power for a given context and is forced to model the general response surfaces."
A MODEL OF INVERSE ONLINE LEARNING,0.2680851063829787,"In many models of behaviour the update function f of the agent is modelled as perfect Bayesian
inference (Kaelbling et al., 1998; Colombo & Hartmann, 2017). While this may seems natural
on the surface given the logical consistency of Bayesian inference, it seems unlikely in prac-
tice that agents (and especially humans) will be capable of it. Not least because in all but the
simplest cases it will become intractable for computational agents and humans themselves are
well known to make simplifying approximations when dealing with past histories (Genewein
et al., 2015). By employing a Ô¨Çexible neural architecture, we remove this relatively restrictive
assumption on the memory update process. Our model then learns exactly how the agent appears
to use previous information to update their predictions - allowing us to model common cogni-
tive biases such as overly weighting recent or largely negative events (Hertwig & Erev, 2009)."
A MODEL OF INVERSE ONLINE LEARNING,0.2723404255319149,"Outcome Estimation.
Having obtained a memory state mt,
the second stage in-
volves
predicting
the
perceived
potential
outcomes
for
a
context
xt
with
the
net-
work OŒ∏(ÀúœÑt|xt, mt).
Given the forward model where the potential outcomes are con-
sidered a linear combination of the features this then proceeds again in three steps:
œât
1, œât
0 = MLP(mt),
(4)"
A MODEL OF INVERSE ONLINE LEARNING,0.2765957446808511,"Àú¬µ1(xt; t) = ‚ü®xt, œât
1‚ü©,
Àú¬µ0(xt; t) = ‚ü®xt, œât
0‚ü©
(5)
ÀúœÑt = Àú¬µ1(xt; t) ‚àíÀú¬µ0(xt; t).
(6)
First the memory mt is decoded into a set of weights for each action using some standard feed-
forward multi-layer perceptron (MLP), before the potential outcomes are predicted by taking the
inner product with the context. This is preferable to, for example, concatenating and passing both xt
and mt through a network since, while it may be potentially more expressive, it loses the connection
to the forward model as well as the interpretability that is given by linearity. As it is, while our
model of the potential outcomes is linear at each time step, the memory allows this to Ô¨Çexibly
change throughout the course of the history. The perceived treatment effect is then calculated as the
difference between potential outcomes. However, given the linear nature of the predictions, there is
no non-degenerate solution for the œâs. Thus, to ensure identiÔ¨Åability, we set œât
0 = 0 as a baseline."
A MODEL OF INVERSE ONLINE LEARNING,0.28085106382978725,"Treatment Rule.
Under Assumption 2, the agent should be expected to take an action
that maximises the outcome.
There are different possible options for the exact treatment
rule, the most obvious being œÄ(at|ÀúœÑt) = 1(ÀúœÑt > 0); assuming that as long as the treat-
ment effect is positive then the intervention will be taken.
However, in order to main-
tain differentiability, as well as to allow for more modelling Ô¨Çexibility to capture the stoch-
asitcity of agents, we parameterise the policy as a soft version of the indicator function:"
A MODEL OF INVERSE ONLINE LEARNING,0.2851063829787234,"œÄ(at|ÀúœÑt) =
1
1 + exp(‚àíŒ±(ÀúœÑt ‚àíŒ≤)).
(7)"
A MODEL OF INVERSE ONLINE LEARNING,0.28936170212765955,Published as a conference paper at ICLR 2022
A MODEL OF INVERSE ONLINE LEARNING,0.2936170212765957,"Assuming positive Œ±,
this is clearly monotonically increasing,
satisfying Assumption
2.
These parameters are learnt by the model and thus allow us to model a Ô¨Çexible
threshold with Œ≤ as well as aleatoric uncertainty in the actions of the agent with Œ±."
LEARNING WITH AN INFERENCE NETWORK,0.2978723404255319,"4.2
LEARNING WITH AN INFERENCE NETWORK"
LEARNING WITH AN INFERENCE NETWORK,0.3021276595744681,"As with all Ô¨Çavours of deep state space models, exact posterior inference is analytically intractable,
making optimisation of the marginal likelihood equally so. As such, we aim to maximise an Evidence
Lower BOund (ELBO) on this likelihood of the data following standardised variational principles
(Blei et al., 2017; Hoffman et al., 2013). This is achieved by positing a parameterised approxi-
mate posterior distribution qœÜ(‚Éóm|‚Éóh), and with a simple application of Jensen‚Äôs inequality arrive at:"
LEARNING WITH AN INFERENCE NETWORK,0.30638297872340425,"log pŒ∏(‚Éóh) ‚â•EqœÜ

log pŒ∏(‚Éóm,‚Éóh) ‚àílog qœÜ(‚Éóm|‚Éóh)

.
(8)
Maximising this bound is not only useful in terms of the marginal likelihood but also
equivalently minimises the Kullback-Leibler divergence between the approximate and true
posterior DKL[qœÜ(‚Éóm|‚Éóh)||p(‚Éóm|‚Éóh)],
giving us more conÔ¨Ådence in our inferred posterior."
LEARNING WITH AN INFERENCE NETWORK,0.31063829787234043,"ùê¥""#$
ùëã""#$
ùëå""#$
ùê¥""#'
ùëã""#'
ùëå""#'"
LEARNING WITH AN INFERENCE NETWORK,0.3148936170212766,"ùë¥""#)
ùë¥""#$
ùë¥""#'
Memory Aggregation"
LEARNING WITH AN INFERENCE NETWORK,0.3191489361702128,"ùê¥""
ùëã""
ùëå""
ùê¥""#)
ùëã""#)
ùëå""#)
ùê¥""*)
ùëã""*)
ùëå""*)"
LEARNING WITH AN INFERENCE NETWORK,0.32340425531914896,"ùíÉ""#)
ùíÉ""
ùíÉ""*)
Backwards LSTM"
LEARNING WITH AN INFERENCE NETWORK,0.3276595744680851,"MLP
ùëû-(ùëÄ""; ùùÅ2"" , ùúÆ5"")"
LEARNING WITH AN INFERENCE NETWORK,0.33191489361702126,"Figure 4: Structure of the approximate poste-
rior inference network."
LEARNING WITH AN INFERENCE NETWORK,0.33617021276595743,"Factorising the Posterior for EfÔ¨Åcient Inference.
In order to accelerate learning and make infer-
ence quick, we factorise the approximate pos-
terior in the same way as the true posterior:"
LEARNING WITH AN INFERENCE NETWORK,0.3404255319148936,"qœÜ(‚Éóm|‚Éóh) = qœÜ(m1|‚Éóh) T
Y"
LEARNING WITH AN INFERENCE NETWORK,0.3446808510638298,"t=2
qœÜ(mt|mt‚àí1,‚Éóht‚àí1:T )"
LEARNING WITH AN INFERENCE NETWORK,0.34893617021276596,"(9)
Thus to estimate the approximate posterior,we use
a backwards LSTM to create a summary bt
=
LSTM(ht:T ). This is then concatenated with the
previous memory at time step t ‚àí1 before being
passed through a fully connected layer to estimate pa-
rameters. Given that the memory can be seen as
a sufÔ¨Åcient statistic of the past within the model,
it is well known that given this factorisation, conditioning on the memory leads to a Rao-
Blackwellization of the more general network that would be conditioned on all variables (Kr-
ishnan et al., 2017; Alaa & van der Schaar, 2019). This accelerates learning not only by param-
eter sharing between œÜ and Œ∏ but also by reducing the variance of the gradient estimates dur-
ing training. With the speciÔ¨Åed inference network qœÜ, this leads to an optimisation objective:"
LEARNING WITH AN INFERENCE NETWORK,0.35319148936170214,"F(œÜ, Œ∏) = EqœÜ(‚Éóm|‚Éóh)

pŒ∏(‚Éóa|‚Éóh)

‚àíDKL
 
qœÜ(m1|‚Éóx)||pŒ∏(m1)

(10) ‚àí T
X"
LEARNING WITH AN INFERENCE NETWORK,0.3574468085106383,"t=2
EqœÜ(mt‚àí1|‚Éóh)

DKL
 
qœÜ(mt|mt‚àí1,‚Éóht‚àí1:T )||pŒ∏(mt|mt‚àí1)
"
LEARNING WITH AN INFERENCE NETWORK,0.3617021276595745,"This can optimised in a straightforward manner using stochastic variational inference and re-
parameterised Monte Carlo samples from the approximate posterior distribution.
At each
step a sample is drawn from the posterior through which gradients can Ô¨Çow and the ELBO
is evaluated.
This can be backpropagated through to get gradients with respect to œÜ and
Œ∏ which can then be updated as appropriate using an optimiser of the practitioner‚Äôs choice."
LEARNING WITH AN INFERENCE NETWORK,0.3659574468085106,"5
CASE STUDY: LIVER TRANSPLANTATION ACCEPTANCE DECISIONS"
LEARNING WITH AN INFERENCE NETWORK,0.3702127659574468,"In this section we explore the explainability beneÔ¨Åts of our method for evaluating real deci-
sion making.
Given space constraints, we relegate validation of the method on synthetic ex-
amples to the appendix, and focus here on a real medical example of accepting liver donation
offers for transplantation.
This is an important area where much has been done to improve
decision support for clinicians (Volk et al., 2015; Berrevoets et al., 2020), but which often fo-
cuses on simply suggesting theoretically optimal actions. Here, by trying to understand better
the decision making process, we hope to be able to provide more bespoke support in the future."
LEARNING WITH AN INFERENCE NETWORK,0.37446808510638296,Published as a conference paper at ICLR 2022 Age
LEARNING WITH AN INFERENCE NETWORK,0.37872340425531914,Gender
LEARNING WITH AN INFERENCE NETWORK,0.3829787234042553,Height
LEARNING WITH AN INFERENCE NETWORK,0.3872340425531915,"Weight
ABO BMI"
LEARNING WITH AN INFERENCE NETWORK,0.39148936170212767,Creatinine INR
LEARNING WITH AN INFERENCE NETWORK,0.39574468085106385,Total Bilirubin
LEARNING WITH AN INFERENCE NETWORK,0.4,MELD Score
LEARNING WITH AN INFERENCE NETWORK,0.40425531914893614,Serum Sodium
LEARNING WITH AN INFERENCE NETWORK,0.4085106382978723,DIAL_TX
LEARNING WITH AN INFERENCE NETWORK,0.4127659574468085,MELD Status
LEARNING WITH AN INFERENCE NETWORK,0.41702127659574467,Status 1
LEARNING WITH AN INFERENCE NETWORK,0.42127659574468085,Albumin
LEARNING WITH AN INFERENCE NETWORK,0.425531914893617,"‚àí0.15
‚àí0.1
‚àí0.05
0
0.05
0.1 Age"
LEARNING WITH AN INFERENCE NETWORK,0.4297872340425532,Gender
LEARNING WITH AN INFERENCE NETWORK,0.4340425531914894,Height
LEARNING WITH AN INFERENCE NETWORK,0.43829787234042555,"Weight
ABO BMI"
LEARNING WITH AN INFERENCE NETWORK,0.4425531914893617,Creatinine INR
LEARNING WITH AN INFERENCE NETWORK,0.44680851063829785,Total Bilirubin
LEARNING WITH AN INFERENCE NETWORK,0.451063829787234,MELD Score
LEARNING WITH AN INFERENCE NETWORK,0.4553191489361702,Serum Sodium
LEARNING WITH AN INFERENCE NETWORK,0.4595744680851064,DIAL_TX
LEARNING WITH AN INFERENCE NETWORK,0.46382978723404256,MELD Status
LEARNING WITH AN INFERENCE NETWORK,0.46808510638297873,Status 1
LEARNING WITH AN INFERENCE NETWORK,0.4723404255319149,Albumin
LEARNING WITH AN INFERENCE NETWORK,0.4765957446808511,"‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4 Age"
LEARNING WITH AN INFERENCE NETWORK,0.4808510638297872,Gender
LEARNING WITH AN INFERENCE NETWORK,0.4851063829787234,Height
LEARNING WITH AN INFERENCE NETWORK,0.48936170212765956,"Weight
ABO BMI"
LEARNING WITH AN INFERENCE NETWORK,0.49361702127659574,Creatinine INR
LEARNING WITH AN INFERENCE NETWORK,0.4978723404255319,Total Bilirubin
LEARNING WITH AN INFERENCE NETWORK,0.502127659574468,MELD Score
LEARNING WITH AN INFERENCE NETWORK,0.5063829787234042,Serum Sodium
LEARNING WITH AN INFERENCE NETWORK,0.5106382978723404,DIAL_TX
LEARNING WITH AN INFERENCE NETWORK,0.5148936170212766,MELD Status
LEARNING WITH AN INFERENCE NETWORK,0.5191489361702127,Status 1
LEARNING WITH AN INFERENCE NETWORK,0.5234042553191489,Albumin ‚àí0.5 ‚àí0.4 ‚àí0.3 ‚àí0.2 ‚àí0.1 0 0.1 0.2 0.3 0.4
LEARNING WITH AN INFERENCE NETWORK,0.5276595744680851,"2003
2005
2007"
LEARNING WITH AN INFERENCE NETWORK,0.5319148936170213,Figure 5: Changing Focuses. Predicted weights of different covariates at two-year time intervals.
LEARNING WITH AN INFERENCE NETWORK,0.5361702127659574,"Organ transplantation is an extreme medical procedure, often considered the last resort for critically
ill patients (Merion et al., 2005). Despite the signiÔ¨Åcant risks associated with major surgery, trans-
plantation can offer the chance of a relatively normal life (Starzl et al., 1982) and as a result demand
for donor organs far outstrips supply (Wynn & Alexander, 2011). Yet, perhaps surprisingly, a large
proportion of organ offers are turned down by clinicians (in consultation with the receiving patients)
for various reasons including donor quality or perceived poor compatibility with the recipient (Volk
et al., 2015). This decision faced by clinicians can be (under some simpliÔ¨Åcation) described by the
process: 1) A donor organ becomes available and is offered to a patient; 2) The clinician considers
the state of the organ, the state of the patient, as well as other information such as the previous
reject history of the organ (this is the context X); 3) The clinician makes the binary decision to
accept/reject the organ (the intervention A); 4) The patient receives/does not receive the organ and
a measure of their outcome is recorded - we measure their survival time as the main outcome of
interest (the outcome Y ). Full details of the dataset, preprocessing steps, and experimental setup
can be found in the appendix. Note that in this example we are not trying to capture the policy of
any individual doctor, but rather the general policy of a treatment centre over time and how what
they Ô¨Ånd important changes - this is due to the form of the data available, but the method could
equally be applied to individual clinicians if that information was made available. Additionally,
while survival time will not be recorded immediately, the most powerful signal in this case is usually
a death that follows very soon after the decision, which will be effectively captured in the data."
LEARNING WITH AN INFERENCE NETWORK,0.5404255319148936,"‚àí2.0
‚àí1.5
‚àí1.0
‚àí0.5
0.0
0.5
1.0
1.5
2.0
Policy Shift 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
LEARNING WITH AN INFERENCE NETWORK,0.5446808510638298,Density
LEARNING WITH AN INFERENCE NETWORK,0.548936170212766,Outcome Effect
LEARNING WITH AN INFERENCE NETWORK,0.5531914893617021,"Y -ive, A = 0
Y +ive, A = 0"
LEARNING WITH AN INFERENCE NETWORK,0.5574468085106383,"Figure 6: One-step policy shift densities after
observing an outcome."
LEARNING WITH AN INFERENCE NETWORK,0.5617021276595745,"Evolving Relative Importances. First, we now ex-
amine exactly how our method can explain a non-
stationary policy over time. In Figure 5 we plot
the relative weights for covariates in terms of the
treatment effect for accepting the organ offer as they
change at two-year periods. The Model for End-
Stage Liver Disease (MELD) score (Kamath & Kim,
2007) was introduced in 2002 to give an overall rat-
ing of how sick a patient is. On introduction, it
was the main consideration when making an accep-
tance decision, as can be seen clearly in Figure 5.
However, over time this became seen as less im-
portant as clinicians started to not want to rely on
a single score, with some clinicians even turning
down offers for patients with high scores in hopes
of a better match, with one study showing that 84
% of the patients who died while waiting for an organ had MELD scores ‚â•15 and had previ-
ously declined at least one offer (Lai et al., 2012). This can also be seen reÔ¨Çected in Figure 5
as the importance of the MELD score in accepting the offer decreases and the policy stabilises."
LEARNING WITH AN INFERENCE NETWORK,0.5659574468085107,"Reactionary Policies The most important aspect of our method is that we can model the reac-
tion of the agent to their experience on the Ô¨Çy. For example we can see how observing various
outcomes change the opinion of the agent moving forward. This is demonstrated in Figure 6
which plots density estimates of the policy shift (measured as the distance the decision bound-
ary moves) after rejecting the offer and measuring a positive/negative outcome. Here it can be"
LEARNING WITH AN INFERENCE NETWORK,0.5702127659574469,Published as a conference paper at ICLR 2022
LEARNING WITH AN INFERENCE NETWORK,0.574468085106383,"seen that when an offer is rejected then a negative outcome shifts the decision boundary down-
wards more than a positive outcome, making it less likely to reject an offer in the next time-step."
LEARNING WITH AN INFERENCE NETWORK,0.5787234042553191,"Matching Clinical Decisions. While it should be emphasised that our method is (to the best of our
knowledge) the only method with the aim or capability to model the evolving decision process of the
agent, as mentioned in Section 3 there are a variety of methods that are tangentially related. First,
we consider a few standard approaches to imitation learning with behavioural cloning (BC) (Bain
& Sammut, 1995), where we consider both a linear (BC-Linear) and deep (BC-Deep) function-
approximator, representing both ends of the interpretability spectrum for BC. Continuing in imitation
learning, we also compare alongside reward-regularised classiÔ¨Åcation for apprenticeship learning
(RCAL) (Piot et al., 2014), a method that regularises the reward implied by the learnt Q-values. We
additionally consider an adaptation of counterfactual inverse reinforcement learning (CIRL) (Bica
et al., 2021) to the bandit setting where we Ô¨Årst estimate the true CATE of the interventions and
thus the true potential outcomes. We use these as the reward and apply an optimal policy, which in
this one-step case is trivially achieved by choosing the action that maximises the potential outcome."
LEARNING WITH AN INFERENCE NETWORK,0.5829787234042553,"Table 2: Action prediction performance. Com-
parison of methods on transplant offer acceptances.
Performance of the methods is evaluated on the
quality of action matching against a held out test
set of demonstrations. We report the area under
the receiving operator characteristic curve (AUC)
and the average precision score (APS)."
LEARNING WITH AN INFERENCE NETWORK,0.5872340425531914,"Method
AUC ‚Üë
APS ‚Üë"
LEARNING WITH AN INFERENCE NETWORK,0.5914893617021276,"BC-Linear
0.798 ¬± 0.002
0.647 ¬± 0.001
BC-Deep
0.803 ¬± 0.002
0.655 ¬± 0.001
RCAL
0.797 ¬± 0.008
0.662 ¬± 0.003
CIRL
0.553 ¬± 0.008
0.510 ¬± 0.002"
LEARNING WITH AN INFERENCE NETWORK,0.5957446808510638,"IOL (Ours)
0.824 ¬± 0.006
0.677 ¬± 0.011"
LEARNING WITH AN INFERENCE NETWORK,0.6,"In Table 2 we compare the predictive power of
our method and all benchmarks on the task of
imitation learning ‚Äì i.e. matching the actions
of the real demonstrator in some held-out test
set. This is the only task for which we can
make meaningful comparison with the exist-
ing literature, given our divergent goals. De-
spite the fact that at each step our method only
considers a linear decision boundary, we are
still able to outperform the deep network ar-
chitectures used in BC and RCAL in terms of
AUC and APS. This can be put down to the
Ô¨Çexible way in which the policy can change
over time to allow for adaptation. Less sur-
prisingly, we also outperform the stationary lin-
ear classiÔ¨Åer as well as the version of CIRL,
the poor performance of which is probably ex-
plained by the unrealistic nature of the causal assumptions in this setting. Further experimen-
tal results on action matching in on multiple additional datasets are provided in the appendix.
6
DISCUSSION"
LEARNING WITH AN INFERENCE NETWORK,0.6042553191489362,"Limitations. By employing a deep architecture our method is able to learn a non-parametric and
accurate form of both the non-stationary policy and update function, making far fewer assump-
tions on their true form than is normal in the literature. This does mean, however, that it works
best when training data is abundant, and can otherwise be prone to over-Ô¨Åtting as is common
with deep neural networks. Additionally, the assumption that agents are always taking the action
they believe will maximise the outcome does mean our method is out-of-the-box unable to ac-
count for strategies that deliberately pick actions that could be sub-optimal but may yield useful
information (i.e. exploratory strategies). An interesting future direction would include explor-
ing this, potentially by augmenting the outcome to include a measure of expected information."
LEARNING WITH AN INFERENCE NETWORK,0.6085106382978723,"Societal Impact and Ethics.
Being able to better understand decision making processes
could be highly useful in combating biases or correcting mistakes made by human decision-
makers.
Nonetheless, it should be noted that any method that aims to analyse and ex-
plain observed behaviour has the potential to be misused, for example to unfairly attribute
incorrect intentions to someone.
As such, it is important to emphasise that, as is the na-
ture of inverse problems, we are only giving one plausible explanation for the behaviour
of an agent and cannot suggest that this is exactly what has gone on in their mind."
LEARNING WITH AN INFERENCE NETWORK,0.6127659574468085,"Conclusions. In this paper we have tackled the problem of inverse online learning ‚Äì that is de-
scriptively modelling the process through which an observed agent adapts their policy over time ‚Äì
by interpreting actions as targeted interventions, or treatments. We drew together policy learning
and treatment effect estimation to present a practical algorithm for solving this problem in an inter-
pretable and meaningful manner, before demonstrating how this could be using in the medical domain
to analyse the decisions of clinicians when choosing whether to accept offers for transplantation."
LEARNING WITH AN INFERENCE NETWORK,0.6170212765957447,Published as a conference paper at ICLR 2022
LEARNING WITH AN INFERENCE NETWORK,0.6212765957446809,ACKNOWLEDGEMENTS
LEARNING WITH AN INFERENCE NETWORK,0.625531914893617,"AJC would like to acknowledge and thank Microsoft Research for its support through its
PhD Scholarship Program with the EPSRC. AC gratefully acknowledges funding from As-
traZeneca.
This work was additionally supported by the OfÔ¨Åce of Naval Research (ONR)
and the NSF (Grant number: 1722516).
We would like to thank all of the anonymous re-
viewers on OpenReview, alongside the many members of the van der Schaar lab, for their in-
put, comments, and suggestions at various stages that have ultimately improved the manuscript."
REFERENCES,0.6297872340425532,REFERENCES
REFERENCES,0.6340425531914894,"Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-Ô¨Årst international conference on Machine learning, pp. 1, 2004."
REFERENCES,0.6382978723404256,"Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning, pp. 1638‚Äì1646. PMLR, 2014."
REFERENCES,0.6425531914893617,"Ahmed Alaa and Mihaela van der Schaar. Limits of estimating heterogeneous treatment effects:
Guidelines for practical algorithm design. In International Conference on Machine Learning, pp.
129‚Äì138, 2018."
REFERENCES,0.6468085106382979,"Ahmed Alaa and Mihaela van der Schaar. Attentive state-space modeling of disease progression.
Proceedings of the 33rd Conference on Neural Information Processing Systems, 2019."
REFERENCES,0.6510638297872341,"Onur Atan, James Jordon, and Mihaela van der Schaar. Deep-treat: Learning optimal personalized
treatments from observational data using neural networks. In Proceedings of the AAAI Conference
on ArtiÔ¨Åcial Intelligence, volume 32, 2018."
REFERENCES,0.6553191489361702,"Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelligence
15, pp. 103‚Äì129, 1995."
REFERENCES,0.6595744680851063,"Jeroen Berrevoets, James Jordon, Ioana Bica, Alexander Gimson, and Mihaela van der Schaar.
Organite:
Optimal transplant donor organ offering using an individual treatment effect.
https://proceedings. neurips. cc/paper/2020, 33, 2020."
REFERENCES,0.6638297872340425,"Ioana Bica, Daniel Jarrett, Alihan H√ºy√ºk, and Mihaela van der Schaar. Learning ‚Äùwhat-if‚Äù explana-
tions for sequential decision-making. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=h0de3QWtGG."
REFERENCES,0.6680851063829787,"David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
Journal of the American statistical Association, 112(518):859‚Äì877, 2017."
REFERENCES,0.6723404255319149,"Jako S Burgers, Richard PTM Grol, Joost OM Zaat, Teun H Spies, Akke K van der Bij, and Henk GA
Mokkink. Characteristics of effective clinical guidelines for general practice. British journal of
general practice, 53(486):15‚Äì19, 2003."
REFERENCES,0.676595744680851,"Alex James Chan and Mihaela van der Schaar. Scalable bayesian inverse reinforcement learning. In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=4qR3coiNaIv."
REFERENCES,0.6808510638297872,"Alex James Chan, Ioana Bica, Alihan H√ºy√ºk, Daniel Jarrett, and Mihaela van der Schaar. The
medkit-learn (ing) environment: Medical decision modelling through simulation. In Thirty-Ô¨Åfth
Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021."
REFERENCES,0.6851063829787234,"Matteo Colombo and Stephan Hartmann. Bayesian cognitive science, uniÔ¨Åcation, and explanation.
The British Journal for the Philosophy of Science, 68(2):451‚Äì484, 2017."
REFERENCES,0.6893617021276596,"Ren√©e Elio and Francis Jeffry Pelletier. Belief change as propositional update. Cognitive Science, 21
(4):419‚Äì460, 1997."
REFERENCES,0.6936170212765957,"Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse reinforce-
ment learning. In International Conference on Learning Representations, 2018."
REFERENCES,0.6978723404255319,Published as a conference paper at ICLR 2022
REFERENCES,0.7021276595744681,"Tim Genewein, Felix Leibfried, Jordi Grau-Moya, and Daniel Alexander Braun. Bounded rationality,
abstraction, and hierarchical decision-making: An information-theoretic optimality principle.
Frontiers in Robotics and AI, 2:27, 2015."
REFERENCES,0.7063829787234043,"Thomas L GrifÔ¨Åths, Falk Lieder, and Noah D Goodman. Rational use of cognitive resources: Levels
of analysis between the computational and the algorithmic. Topics in cognitive science, 7(2):
217‚Äì229, 2015."
REFERENCES,0.7106382978723405,"Ralph Hertwig and Ido Erev. The description‚Äìexperience gap in risky choice. Trends in cognitive
sciences, 13(12):517‚Äì523, 2009."
REFERENCES,0.7148936170212766,"Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Proceedings of the
30th International Conference on Neural Information Processing Systems, pp. 4572‚Äì4580, 2016."
REFERENCES,0.7191489361702128,"Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference.
Journal of Machine Learning Research, 14(5), 2013."
REFERENCES,0.723404255319149,"Steven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. Online learning: A comprehensive survey.
arXiv preprint arXiv:1802.02871, 2018."
REFERENCES,0.7276595744680852,"MG Myriam Hunink, Milton C Weinstein, Eve Wittenberg, Michael F Drummond, Joseph S Pliskin,
John B Wong, and Paul P Glasziou. Decision making in health and medicine: integrating evidence
and values. Cambridge university press, 2014."
REFERENCES,0.7319148936170212,"Alihan H√ºy√ºk, Daniel Jarrett, Cem Tekin, and Mihaela van der Schaar. Explaining by imitating:
Understanding decisions by interpretable policy learning. In International Conference on Learning
Representations, 2021a. URL https://openreview.net/forum?id=unI5ucw_Jk."
REFERENCES,0.7361702127659574,"Alihan H√ºy√ºk, Daniel Jarrett, and Mihaela van der Schaar. Inverse contextual bandits: Learning how
behavior evolves over time. arXiv preprint arXiv:2107.06317, 2021b."
REFERENCES,0.7404255319148936,"Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially
observable stochastic domains. ArtiÔ¨Åcial intelligence, 101(1-2):99‚Äì134, 1998."
REFERENCES,0.7446808510638298,"Patrick S Kamath and W Ray Kim. The model for end-stage liver disease (meld). Hepatology, 45(3):
797‚Äì805, 2007."
REFERENCES,0.7489361702127659,"Rahul Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state
space models. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 31, 2017."
REFERENCES,0.7531914893617021,"S√∂ren R K√ºnzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for estimating heteroge-
neous treatment effects using machine learning. Proceedings of the national academy of sciences,
116(10):4156‚Äì4165, 2019."
REFERENCES,0.7574468085106383,"Jennifer Cindy Lai, Sandy Feng, and John Paul Roberts. An examination of liver offers to candidates
on the liver transplant wait-list. Gastroenterology, 143(5):1261‚Äì1265, 2012."
REFERENCES,0.7617021276595745,"Robert M Merion, Douglas E Schaubel, Dawn M Dykstra, Richard B Freeman, Friedrich K Port, and
Robert A Wolfe. The survival beneÔ¨Åt of liver transplantation. American Journal of Transplantation,
5(2):307‚Äì313, 2005."
REFERENCES,0.7659574468085106,"Bilal Piot, Matthieu Geist, and Olivier Pietquin. Boosted and reward-regularized classiÔ¨Åcation for
apprenticeship learning. In Proceedings of the 2014 international conference on Autonomous
agents and multi-agent systems, pp. 1249‚Äì1256. International Foundation for Autonomous Agents
and Multiagent Systems, 2014."
REFERENCES,0.7702127659574468,"Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions. Journal
of the American Statistical Association, 100(469):322‚Äì331, 2005."
REFERENCES,0.774468085106383,"Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: general-
ization bounds and algorithms. In International Conference on Machine Learning, pp. 3076‚Äì3085.
PMLR, 2017."
REFERENCES,0.7787234042553192,Published as a conference paper at ICLR 2022
REFERENCES,0.7829787234042553,"Thomas E Starzl, Shunzaburo Iwatsuki, David H Van Thiel, J Carlton Gartner, Basil J Zitelli,
J Jeffrey Malatack, Robert R Schade, Byers W Shaw Jr, Thomas R Hakala, J Thomas Rosenthal,
et al. Evolution of liver transplantation. Hepatology, 2(5):614S‚Äì636S, 1982."
REFERENCES,0.7872340425531915,"Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback through
counterfactual risk minimization. The Journal of Machine Learning Research, 16(1):1731‚Äì1755,
2015."
REFERENCES,0.7914893617021277,"Michael L Volk, Nathan Goodrich, Jennifer C Lai, Christopher Sonnenday, and Kerby Shedden.
Decision support for organ offers in liver transplantation. Liver transplantation, 21(6):784‚Äì791,
2015."
REFERENCES,0.7957446808510639,"James J Wynn and Charles E Alexander. Increasing organ donation and transplantation: the us
experience over the past decade. Transplant International, 24(4):324‚Äì332, 2011."
REFERENCES,0.8,"Edmundas Kazimieras Zavadskas and Zenonas Turskis. Multiple criteria decision making (mcdm)
methods in economics: an overview. Technological and economic development of economy, 17(2):
397‚Äì427, 2011."
REFERENCES,0.8042553191489362,"Baqun Zhang, Anastasios A Tsiatis, Marie Davidian, Min Zhang, and Eric Laber. Estimating optimal
treatment regimes from a classiÔ¨Åcation perspective. Stat, 1(1):103‚Äì114, 2012a."
REFERENCES,0.8085106382978723,"Baqun Zhang, Anastasios A Tsiatis, Eric B Laber, and Marie Davidian. A robust method for
estimating optimal treatment regimes. Biometrics, 68(4):1010‚Äì1018, 2012b."
REFERENCES,0.8127659574468085,"Yingqi Zhao, Donglin Zeng, A John Rush, and Michael R Kosorok. Estimating individualized
treatment rules using outcome weighted learning. Journal of the American Statistical Association,
107(499):1106‚Äì1118, 2012."
REFERENCES,0.8170212765957446,"Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pp. 1433‚Äì1438. Chicago, IL, USA, 2008."
REFERENCES,0.8212765957446808,Published as a conference paper at ICLR 2022
REFERENCES,0.825531914893617,APPENDIX
REFERENCES,0.8297872340425532,"Algorithm 1: Inverse Online Learning (IOL)
Result: Parameters œÜ of variational distribution and Œ∏ of generative model
Input: D, X, A, Y, learning rate Œ∑;
Initialise œÜ, Œ∏;
while not converged do"
REFERENCES,0.8340425531914893,"Evaluate pŒ∏(mt|mt‚àí1) ;
‚ñ∑Calculate forward model next step priors
Evaluate qœÜ(mt|mt‚àí1,‚Éóh) ;
‚ñ∑Inference network to get posterior
Sample ÀÜ‚Éóm ‚àºqœÜ(‚Éóm|‚Éóh) ;
‚ñ∑Straight-through MC sample
NLLloss = EqœÜ(‚Éóm|‚Éóh)

pŒ∏(‚Éóa|‚Éóm)

;"
REFERENCES,0.8382978723404255,"KLloss = DKL
 
qœÜ(m1|‚Éóx)||pŒ∏(m1)

+ PT
t=2 E

DKL
 
qœÜ(mt|mt‚àí1,‚Éóh)||pŒ∏(mt|mt‚àí1)

;
F(œÜ, Œ∏) = NLLloss + KLloss;
(œÜ‚Ä≤, Œ∏‚Ä≤) ‚Üê(œÜ, Œ∏) ‚àíŒ∑‚àáœÜ,Œ∏F(œÜ, Œ∏) ;
‚ñ∑Gradient step for œÜ, Œ∏
œÜ, Œ∏ ‚ÜêœÜ‚Ä≤, Œ∏‚Ä≤
end
Return: œÜ, Œ∏"
REFERENCES,0.8425531914893617,"A
EXPERIMENTAL DETAILS"
REFERENCES,0.8468085106382979,"All
experiments
were
performed
on
a
2016
MacBook
Pro,
using
a
2.9
GHz
Dual-Core
Intel
Core
i5
with
8GB
of
LPDDR3
RAM
and
no
GPU
acceleration."
REFERENCES,0.851063829787234,"Code was written in PyTorch Paszke et al. (2019) and no external code was used
in
the
implementation
of
either
the
model
nor
the
benchmarks.
Hyperparame-
ters
were
selected
through
grid
search
over
a
validation
fold
of
the
training
data."
REFERENCES,0.8553191489361702,"All
medical
data
used
has
undergone
a
de-identiÔ¨Åcation
process
and
consent
was
obtained
by
the
relevant
curators
for
their
data
to
be
publicly
released."
REFERENCES,0.8595744680851064,"A.1
UNOS DATA"
REFERENCES,0.8638297872340426,"Our data focuses on data from the United Network for Organ Sharing (UNOS), the US non-proÔ¨Åt that
manages their national waiting list and maintains a database and record of transplantations that occur."
REFERENCES,0.8680851063829788,"We focus on liver-transplantations, where the donor is deceased. The data is made up of patient details
from various centres around the United States. Of the multiple million records of individual transplan-
tations available, once those with missing information on either the donor or recipient are removed we
are left with around 500,000 patients spread across 31 centres. Available variables are listed in Table 3"
REFERENCES,0.8723404255319149,"In our action matching experiments we split the data into Train/Validation/Test folds by separating
the data by centre. In particular we use centre CTR23901 with 114,314 patients for training;
CTR124 with 21,067 patients for validation; and Ô¨Ånally CTR279 with 14,295 patients for testing."
REFERENCES,0.8765957446808511,"For
the
more
exploratory
experiments
who‚Äôs
focus
is
on
inference
over
some
data
and
not
extrapolated
prediction,
we
simply
train
on
centre
CTR23901,
and
examine
what
is
inferred
about
the
treatment
of
those
speciÔ¨Åc
patients"
REFERENCES,0.8808510638297873,"B
ADDITIONAL VALIDATION"
REFERENCES,0.8851063829787233,"While we focused on an important case study in the main paper, we appreciate the need
to provide additional evidence that our model can work outside of one speciÔ¨Åc case exam-
ple.
As such, we now report results on some experiments focusing Ô¨Årst on a toy (but rel-
evant) synthetic example, followed by performance results on multiple further real datasets."
REFERENCES,0.8893617021276595,Published as a conference paper at ICLR 2022
REFERENCES,0.8936170212765957,Table 3: UNOS Variables
REFERENCES,0.8978723404255319,"AGE
Recipient‚Äôs age at transplant
AGE_DON
Donor‚Äôs age at transplant
ALBUMIN_TX
Recipient‚Äôs albumin concentration at transplant
ASCITES_TX
Recipient had ascites at the time of transplant
BMI_CALC
Recipient‚Äôs BMI
COD_CAD_DON
Cause of death of the donor
COLD_ISCH
Cold ischemia time - length of time from the donor being
recovered until it is transplanted
CREAT_DON
Donor‚Äôs creatinine concentration
CREAT_TX
Recipient‚Äôs creatinine concentration at transplant
DIAL_TX
Whether recipient was on dialysis before transplant
ETHCAT
Recipient‚Äôs ethnicity
ETHCAT_DON
Donor‚Äôs ethnicity
EXC_HCC
Whether the recipient received a hepatocellular carcinoma
exception point
FUNC_STAT_TRR
Recipient‚Äôs functional status at the time of transplant
HCV_SEROSTATUS
Whether the recipient has hepatitis C
INR_TX
Recipient‚Äôs INR value at the time transplant
LIFE_SUP_TRR
Whether the recipient was on life support at the time of
transplant
MED_COND_TRR
Whether the recipient was in an icu, hospital bed, or came
from home before transplant
MELD_PELD_LAB_SCORE
Recipient MELD score
NON_HRT_DON
Whether the donor was a DCD (donation after cardiac death)
NUM_PREV_TX
Number of previous liver transplants the recipient has had
ON_VENT_TRR
Whether the recipient was on a mechanical ventilator before
transplant
PORTAL_VEIN_TRR
Whether the recipient had a portal vein thrombosis at the
time of transplant
PREV_AB_SURG_TRR
Whether the recipient had prior abdominal surgery before
the liver transplant
PREV_TX
Whether the recipient had any previous transplant
SHARE_TY
Whether the donor is local, regional, or national
TBILI_TX
Recipient‚Äôs bilirubin at the time of transplant
TXLIV
Whether the graft is whole or segmental
death_mech_don_group
Donor mechanism of death
deathcirc
Donor circumstance of death
diag1
Recipient‚Äôs primary cause of liver disease
meldstat
Whether the recipient is status 1a
statushcc
Whether the recipient has hcc
status1
Whether the recipient is status 1a"
REFERENCES,0.902127659574468,Published as a conference paper at ICLR 2022
REFERENCES,0.9063829787234042,"B.1
SYNTHETIC EXPERIMENT"
REFERENCES,0.9106382978723404,"We
consider
a
toy
example
of
an
agent
that
is
learning
online
about
the
treatment
effect
of
the
interventions
they
are
performing."
REFERENCES,0.9148936170212766,"We let X
=
R5, A
=
[0, 1], andY
=
R, with the true outcomes a linear combina-
tion of the features given the action (with weights stochastically samples).
From a sam-
pled prior set of weights in the agents mind, the agent then interacts, estimating the po-
tential outcomes and choosing an action stochastically according to equation 7 with Œ± =
1, Œ≤ = 0.
After every context, action, outcome triple (x, a, y), the agent adjusts it‚Äôs beliefs
over the weights of the potential outcome functions using an online gradient descent method:"
REFERENCES,0.9191489361702128,"œâa = œâa ‚àíŒª((Àúy ‚àíy) √ó x),
(11)"
REFERENCES,0.9234042553191489,"with
Œª
some
learning
rate.
Clearly
then
the
policy
of
the
agent
is
non-
stationary
and
changes
over
time
in
reaction
to
the
observed
outcomes."
REFERENCES,0.9276595744680851,"Results. We simulate 10000 trajectories of length 50. When applying IOL to this simulated
data, we can achieve an accuracy of 99%, meaning 99% of the time we can accurately de-
termine which of the potential outcomes the agent thins will be higher.
This is compared
to a stationary estimate of the CATE which can only reach 52% given the fact that the pol-
icy is detached from the truth of the treatment effect.
Clearly this is an extreme example
designed to fail the existing approaches but it does serve to show how stark the contrast is."
REFERENCES,0.9319148936170213,"B.2
FURTHER PERFORMANCE ON REAL DATA"
REFERENCES,0.9361702127659575,"Alongside the synthetic example above, we also provide further results on real medical data,
demonstrating improved performance across a variety of domains. These datasets do not con-
tain a recorded outcome, consequently so we do not include CIRL, and the next state is used
as a substitute in IOL. Results are given in Table 2, and a description of the datasets follows:"
REFERENCES,0.9404255319148936,"Intensive Care Unit. The ICU data covers the treatment of 23,106 of patients in the intensive care
unit from Amsterdam UMC Elbers (2019). These patients are in a more critical state than those on the
general wards while suffering similarly from a variety of conditions and consequently are monitored
more frequently, with the database containing around 1 billion clinical observations at varying
timesteps down to minute by minute recordings. The data is aggregated into one hour timesteps
and models the treatment of the prescription of antibiotics. Included are 24 series features focusing
on vital signs including heart rate, blood pressure, and various chemical blood concentration levels."
REFERENCES,0.9446808510638298,"Cystic Fibrosis. The CF data considers patients enrolled in the UK Cystic Fibrosis Registry Taylor-
Robinson et al. (2018), coviering around 5,800 patients over the course of multiple years. Measure-
ments were taken on average every 6 months, and the 79 features cover common comorbidities, test
results, and infections that arise. The intervention corresponds to the prescription of cortico steroids."
REFERENCES,0.948936170212766,"Hospital Wards. The Wards data is based on the care of 6,321 patients at the Ronald Reagan
UCLA Medical Center in California who were treated on the general medicine Ô¨Çoor between
2013-2016 Alaa et al. (2017). These patients were treated for a variety of conditions including
pneumonia, sepsis, and fevers were in general stable and deterioration that required ICU care was
rare. Measurements were taken roughly every 4 hours, with average stays lasting 9 days, and
include common vital signs such as pulse and blood pressure alongside lab tests and results and
35 temporal features. The action space is taken as a choice of application of oxygen therapy."
REFERENCES,0.9531914893617022,Published as a conference paper at ICLR 2022
REFERENCES,0.9574468085106383,"Table 4: Medical examples performance. Comparison of methods on three different medical
datasets. Performance of the policy is evaluated on the quality of action matching against a held out
test set of demonstrations. We report the area under the receiving operator characteristic curve (AUC)
and average precision score (APS)."
REFERENCES,0.9617021276595744,"ICU
CF
Wards"
REFERENCES,0.9659574468085106,"Metric
AUC
APS
AUC
APS
AUC
APS"
REFERENCES,0.9702127659574468,"BC-L
0.681 ¬± 0.001 0.655 ¬± 0.000 0.830 ¬± 0.010
0.797 ¬± 0.009
0.896 ¬± 0.006
0.853 ¬± 0.002
BC-D 0.668 ¬± 0.003
0.654 ¬± 0.001 0.952 ¬± 0.010 0.834 ¬± 0.003
0.845 ¬± 0.010
0.811 ¬± 0.013
RCAL 0.396 ¬± 0.018
0.563 ¬± 0.007
0.689 ¬± 0.022
0.745 ¬± 0.008
0.942 ¬± 0.003
0.920 ¬± 0.004"
REFERENCES,0.9744680851063829,"IOL
0.686 ¬± 0.092 0.631 ¬± 0.065
0.942 ¬± 0.031 0.943 ¬± 0.026 0.965 ¬± 0.031 0.941 ¬± 0.055"
REFERENCES,0.9787234042553191,REFERENCES
REFERENCES,0.9829787234042553,"Alaa, A. M., Yoon, J., Hu, S., and Van der Schaar, M. (2017). Personalized risk scoring for critical care
prognosis using mixtures of gaussian processes. IEEE Transactions on Biomedical Engineering,
65(1):207‚Äì218."
REFERENCES,0.9872340425531915,"Elbers, P. W. G. (2019). AmsterdamUMCdb v1.0.2 ICU database."
REFERENCES,0.9914893617021276,"Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library.
In Advances in neural information processing systems, pages 8026‚Äì8037."
REFERENCES,0.9957446808510638,"Taylor-Robinson, D., Archangelidi, O., Carr, S. B., Cosgriff, R., Gunn, E., Keogh, R. H., MacDougall,
A., Newsome, S., Schl√ºter, D. K., Stanojevic, S., et al. (2018). Data resource proÔ¨Åle: the uk cystic
Ô¨Åbrosis registry. International journal of epidemiology, 47(1):9‚Äì10e."
