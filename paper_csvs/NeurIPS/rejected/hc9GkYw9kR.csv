Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009225092250922509,"Many-objective optimization (MOO) simultaneously optimizes many conﬂicting
1"
ABSTRACT,0.0018450184501845018,"objectives to identify the Pareto front - a set of diverse solutions that represent
2"
ABSTRACT,0.0027675276752767526,"different optimal balances between conﬂicting objectives. For expensive MOO
3"
ABSTRACT,0.0036900369003690036,"problems, due to their costly function evaluations, computationally cheap surrogates
4"
ABSTRACT,0.004612546125461255,"have been widely used in MOO to save evaluation budget. However, as the number
5"
ABSTRACT,0.005535055350553505,"of objectives increases, the cost of learning and surrogation, as well as the difﬁculty
6"
ABSTRACT,0.006457564575645757,"of maintaining solution diversity, increases rapidly. In this paper, we propose
7"
ABSTRACT,0.007380073800738007,"LORA-MOO, a surrogate-assisted MOO algorithm that learns surrogates from
8"
ABSTRACT,0.008302583025830259,"spherical coordinates. This includes an ordinal-regression-based surrogate for
9"
ABSTRACT,0.00922509225092251,"convergence and M −1 regression-based surrogates for diversity. M is the number
10"
ABSTRACT,0.01014760147601476,"of objectives. Such a surrogate modeling method makes it possible to use a
11"
ABSTRACT,0.01107011070110701,"single ordinal surrogate to do the surrogate-assisted search, and the remaining
12"
ABSTRACT,0.011992619926199263,"surrogates are used to select solution for expensive evaluations, which enhances the
13"
ABSTRACT,0.012915129151291513,"optimization efﬁciency. The ordinal regression surrogate is developed to predict
14"
ABSTRACT,0.013837638376383764,"ordinal relation values as radial coordinates, estimating how desirable the candidate
15"
ABSTRACT,0.014760147601476014,"solutions are in terms of convergence. The solution diversity is maintained via
16"
ABSTRACT,0.015682656826568265,"angles between solutions, which is a parameter-free. Experimental results show
17"
ABSTRACT,0.016605166051660517,"that LORA-MOO signiﬁcantly outperforms other surrogate-assisted MOO methods
18"
ABSTRACT,0.017527675276752766,"on most MOO benchmark problems and real-world applications.
19"
INTRODUCTION,0.01845018450184502,"1
Introduction
20"
INTRODUCTION,0.01937269372693727,"Many-objective optimization problems (MOOPs) are widely exist in many real-world applications,
21"
INTRODUCTION,0.02029520295202952,"such as production scheduling [26], trafﬁc signal control [33], and water resource engineering [21].
22"
INTRODUCTION,0.021217712177121772,"These MOOPs have conﬂicting objectives to optimize, and thus all objectives cannot reach their
23"
INTRODUCTION,0.02214022140221402,"optimum simultaneously. As a result the optimum of MOOPs is the Pareto front (PF): A set of
24"
INTRODUCTION,0.023062730627306273,"non-dominated solutions that represent different optimal balance between conﬂicting objectives.
25"
INTRODUCTION,0.023985239852398525,"Multi-/many-objective optimization (MOO) 1 aims to ﬁnd non-dominated solutions that are close to
26"
INTRODUCTION,0.024907749077490774,"the PF and also well distributed along the PF, indicating that MOO should consider both convergence
27"
INTRODUCTION,0.025830258302583026,"and diversity.
28"
INTRODUCTION,0.026752767527675275,"Various evolutionary optimization algorithms have been proposed to solve MOOPs [10]. These
29"
INTRODUCTION,0.027675276752767528,"optimization algorithms usually require plenty of solution samplings and evaluations to ﬁnd converged
30"
INTRODUCTION,0.02859778597785978,"and diverse non-dominated solutions. However, in many real-world MOOPs, the evaluation of solution
31"
INTRODUCTION,0.02952029520295203,"performance could be expensive [41]. In these expensive MOOPs, the evaluation budget only allows
32"
INTRODUCTION,0.03044280442804428,"a limited number of solutions to be evaluated on the expensive objective functions. To address
33"
INTRODUCTION,0.03136531365313653,"expensive MOOPs, evolutionary optimization algorithms are combined with computationally cheap
34"
INTRODUCTION,0.03228782287822878,"1Multi-objective optimization has 2 or 3 objectives, many-objective optimization has 4 or more objectives."
INTRODUCTION,0.033210332103321034,"surrogates to enhance sampling efﬁciency and save evaluations, which are known as surrogate-assisted
35"
INTRODUCTION,0.03413284132841329,"evolutionary algorithms (SAEAs).
36"
INTRODUCTION,0.03505535055350553,"Yet, it is a perennial challenge to use surrogates in a more effective and efﬁcient way for SAEAs,
37"
INTRODUCTION,0.035977859778597784,"especially when optimization problems have many objectives. For example, conventional SAEAs
38"
INTRODUCTION,0.03690036900369004,"usually use regression-based surrogates to approximate each objective function separately [5, 34].
39"
INTRODUCTION,0.03782287822878229,"For MOOPs, many objectives indicate maintaining many surrogates for surrogate-assisted search and
40"
INTRODUCTION,0.03874538745387454,"selection, which results in a low efﬁciency of SAEAs. In addition, it is difﬁcult to maintain solution
41"
INTRODUCTION,0.03966789667896679,"diversity in high-dimensional objective space. Some SAEAs [24, 43, 5] need to investigate proper
42"
INTRODUCTION,0.04059040590405904,"parametric strategies to generate reference vectors or divide objective space into subspaces. Recently,
43"
INTRODUCTION,0.04151291512915129,"a family of classiﬁcation-based SAEAs [31, 17] attempted to use a single surrogate to learn pairwise
44"
INTRODUCTION,0.042435424354243544,"dominance relations. However, the training with pairwise relations implies an exponential increase in
45"
INTRODUCTION,0.043357933579335796,"the size of training dataset. Therefore, a natural question is that whether we can reduce the cost of
46"
INTRODUCTION,0.04428044280442804,"maintaining many surrogates without increasing the cost of training a single surrogate. Furthermore,
47"
INTRODUCTION,0.045202952029520294,"whether we can use an non-parametric diversity maintenance strategy to handle the objective space of
48"
INTRODUCTION,0.046125461254612546,"MOOPs, instead of designing complex reference vectors or points?
49"
INTRODUCTION,0.0470479704797048,"In this paper, we propose a different way to implement surrogate-assisted evolutionary optimization
50"
INTRODUCTION,0.04797047970479705,"for expensive MOOPs, named LORA-MOO, where a single surrogate is developed to learn ordinal
51"
INTRODUCTION,0.048892988929889296,"relations for convergence purpose, and several angular surrogates are generated from spherical
52"
INTRODUCTION,0.04981549815498155,"coordinates to maintain diversity. Our major contributions are summarized as follows:
53"
INTRODUCTION,0.0507380073800738,"• We develop a novel ordinal-regression-based model to approximate the ordinal landscape of
54"
INTRODUCTION,0.05166051660516605,"expensive MOOPs. Our ordinal surrogate is able to handle many objectives simultaneously
55"
INTRODUCTION,0.052583025830258305,"and assist MOO algorithms to complete the model-based search. Artiﬁcial ordinal relations
56"
INTRODUCTION,0.05350553505535055,"are generated via a clustering method to improve the learning quality of ordinal relations for
57"
INTRODUCTION,0.0544280442804428,"many objectives. Unlike the pairwise relations learned through classiﬁcation, the ordinal
58"
INTRODUCTION,0.055350553505535055,"relations would not increase the size of training dataset, hence high efﬁciency.
59"
INTRODUCTION,0.05627306273062731,"• We introduce the idea of spherical coordinates approximation into surrogate-assisted evo-
60"
INTRODUCTION,0.05719557195571956,"lutionary optimization and proposed LORA-MOO to solve expensive MOOPs. Different
61"
INTRODUCTION,0.058118081180811805,"from existing SAEAs which learn approximation models from Cartesian coordinates, we ﬁt
62"
INTRODUCTION,0.05904059040590406,"several regression-based surrogates to approximate angular coordinates, while our ordinal
63"
INTRODUCTION,0.05996309963099631,"surrogate can be treated as a radial coordinate. An non-parametric approach is developed to
64"
INTRODUCTION,0.06088560885608856,"select diverse solutions for expensive evaluations via our angular coordinate surrogates.
65"
INTRODUCTION,0.061808118081180814,"• Extensive experiments on benchmark and real-world optimization problems are conducted
66"
INTRODUCTION,0.06273062730627306,"under a range of scales and numbers of objectives. Empirical results show that our LORA-
67"
INTRODUCTION,0.06365313653136531,"MOO is effective. It is able to obtain a well-distributed solution set that outperforms the
68"
INTRODUCTION,0.06457564575645756,"state-of-the-arts.
69"
RELATED WORK,0.06549815498154982,"2
Related Work
70"
RELATED WORK,0.06642066420664207,"2.1
Multi-/Many-Objective Surrogate-Assisted Evolutionary Algorithms
71"
RELATED WORK,0.06734317343173432,"Regression-based SAEAs. Regression-based SAEAs employ regression-based surrogates such as
72"
RELATED WORK,0.06826568265682657,"Kriging [36, 39] to approximate either the objective values of solutions or the objective functions
73"
RELATED WORK,0.06918819188191883,"of expensive problems [22]. To maintain solution diversity, ParEGO [24] employs a Kriging model
74"
RELATED WORK,0.07011070110701106,"to iteratively approximate an aggregate objective function which aggregates all objectives into one
75"
RELATED WORK,0.07103321033210332,"via a set of pre-deﬁned scale vectors. In MOEA/D-EGO [43], plenty of scale vectors are generated
76"
RELATED WORK,0.07195571955719557,"uniformly to decompose the target MOOP into many single-objective subproblems. K-RVEA [5] also
77"
RELATED WORK,0.07287822878228782,"designs a set of scale vectors as reference vectors to maintain solution diversity. Similarity or density
78"
RELATED WORK,0.07380073800738007,"estimation is an alternative option for maintaining diversity. For instance, KTA2 [34] estimates the
79"
RELATED WORK,0.07472324723247233,"distribution status of non-dominated solutions by deﬁning a similarity or density indicator.
80"
RELATED WORK,0.07564575645756458,"Classiﬁcation-based SAEAs. In model-based optimization, the optimization is guided by the relation
81"
RELATED WORK,0.07656826568265683,"between solutions rather than accurate objective values. Therefore, there is a tendency for recently
82"
RELATED WORK,0.07749077490774908,"proposed SAEAs to use classiﬁcation-based surrogates to learn the relation between solutions directly.
83"
RELATED WORK,0.07841328413284133,"CSEA [31] trains a neural network to justify whether candidate solutions can be dominated by given
84"
RELATED WORK,0.07933579335793357,"reference points or not. θ-DEA-DP [42] uses two neural networks to predict the Pareto dominance
85"
RELATED WORK,0.08025830258302583,"relation and θ-dominance relation between two solutions, respectively. REMO [17] employs a
86"
RELATED WORK,0.08118081180811808,"neural network to ﬁt a ternary classiﬁer, which is able to learn the dominance relation between
87"
RELATED WORK,0.08210332103321033,"pairs of solutions. Compared with regression-based SAEAs, although classiﬁcation-based SAEAs
88"
RELATED WORK,0.08302583025830258,"take advantage of learning solution relations directly, their drawbacks are also clear: The prediction
89"
RELATED WORK,0.08394833948339483,"of solution relations lacks the information of how solutions are distributed in the objective space,
90"
RELATED WORK,0.08487084870848709,"making it difﬁcult for classiﬁcation-based SAEAs to maintain solution diversity. In [31, 17], a radial
91"
RELATED WORK,0.08579335793357934,"projection selection approach is adapted to select diverse reference points. However, its effect on
92"
RELATED WORK,0.08671586715867159,"diversity maintenance is limited. In addition, although classiﬁcation-based SAEAs maintain only one
93"
RELATED WORK,0.08763837638376384,"surrogate, the cost of learning pairwise relations from large datasets is inevitably increased.
94"
RELATED WORK,0.08856088560885608,"SAEAs based on Other Surrogates. HSMEA [15] uses an ensemble of multiple surrogates in the
95"
RELATED WORK,0.08948339483394833,"optimization. In addition, a new category of surrogates, namely ordinal regression surrogate [40] or
96"
RELATED WORK,0.09040590405904059,"level-based classiﬁcation surrogate [28], is proposed recently to combine regression-based surrogates
97"
RELATED WORK,0.09132841328413284,"with classiﬁcation-based surrogates. However, the shortcoming remains the same as these surrogates
98"
RELATED WORK,0.09225092250922509,"lack the information of solution distribution, especially when the number of objectives is large.
99"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.09317343173431734,"2.2
Multi-Objective Bayesian Optimization
100"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.0940959409594096,"MOBO. Bayesian Optimization (BO) [35, 18] is also a typical model-based optimization method
101"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.09501845018450185,"for expensive optimization, while multi-objective BO (MOBO) methods are designed for expensive
102"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.0959409594095941,"MOOPs [7, 8, 27, 1]. Some MOBO generalizes the acquisition functions such as upper conﬁdence
103"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.09686346863468635,"bound (UCB) [46], expected improvement (EI) [14], Thompson sampling [3], to solve expensive
104"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.09778597785977859,"MOOPs. In addition, entropy search methods have also been employed in MOBO [2, 37]. To
105"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.09870848708487084,"maintain solution diversity, the EI of a multi-objective performance indicator, Hypervolume (HV)
106"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.0996309963099631,"[45], was used as the acquisition function in recent MOBO [6, 27]. Based on the Hypervolume
107"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.10055350553505535,"improvement (HVI), PSL [27] proposes a learning method to approximate the whole Pareto set for
108"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.1014760147601476,"MOBO, and PDBO [1] automatically selects the best acquisition function for objective functions
109"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.10239852398523985,"in each iteration. However, the time complexity of computing HV increases exponentially with the
110"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.1033210332103321,"number of objectives, which may limit the application of MOBO methods on optimization problems
111"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.10424354243542436,"with many objectives.
112"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.10516605166051661,"Connection to SAEAs. Both SAEAs and MOBO are model-based optimization methods. A SAEA
113"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.10608856088560886,"is also a MOBO if it uses probability models as surrogates, and a MOBO is also a SAEA if it searches
114"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.1070110701107011,"candidate solutions with evolutionary search algorithms. Therefore, some model-based optimization
115"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.10793357933579335,"methods belong to both SAEAs and MOBO [24, 14, 43].
116"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.1088560885608856,"3
LORA-MOO: Optimization via Learning Ordinal Relations and Angles
117"
MULTI-OBJECTIVE BAYESIAN OPTIMIZATION,0.10977859778597786,"This section ﬁrst introduces the LORA-MOO framework, followed by detailed algorithm descriptions.
118"
LORA-MOO FRAMEWORK,0.11070110701107011,"3.1
LORA-MOO Framework
119"
LORA-MOO FRAMEWORK,0.11162361623616236,"The pseudocode of LORA-MOO is depicted in Alg. 1, it consists of four phases:
120"
LORA-MOO FRAMEWORK,0.11254612546125461,"1. Initialization: An initial dataset of size 11D - 1 (As suggested in the literature [24]) are
121"
LORA-MOO FRAMEWORK,0.11346863468634687,"sampled from the decision space using the Latin hypercube sampling (LHS) [30] (line 1),
122"
LORA-MOO FRAMEWORK,0.11439114391143912,"where D is the dimensionality of decision variables. The sampled solutions are evaluated on
123"
LORA-MOO FRAMEWORK,0.11531365313653137,"objective functions f and then saved in an archive SA (line 2).
124"
LORA-MOO FRAMEWORK,0.11623616236162361,"2. Surrogate modeling: For all solutions x ∈SA, quantify their ordinal values (line 4) and
125"
LORA-MOO FRAMEWORK,0.11715867158671586,"calculate their angular coordinates (line 9). The set of ordinal values So is used to train
126"
LORA-MOO FRAMEWORK,0.11808118081180811,"the ordinal surrogate ho (line 5). The angular coordinates are used to ﬁt M −1 angular
127"
LORA-MOO FRAMEWORK,0.11900369003690037,"surrogates hai separately (line 10).
128"
LORA-MOO FRAMEWORK,0.11992619926199262,"3. Sampling (Search and Selection): Run an optimizer on surrogate ho to generate a population
129"
LORA-MOO FRAMEWORK,0.12084870848708487,"of candidate solutions P (line 6). Select optimal candidate solutions x∗
1, x∗
2 from P based
130"
LORA-MOO FRAMEWORK,0.12177121771217712,"on surrogates ho, hai, respectively (lines 7 and 11).
131"
LORA-MOO FRAMEWORK,0.12269372693726938,"4. Update: Evaluate new optimal candidate solutions x∗
1, x∗
2 on expensive objective functions
132"
LORA-MOO FRAMEWORK,0.12361623616236163,"f, update archive SA and the number of used function evaluations FE (lines 8 and 12). The
133"
LORA-MOO FRAMEWORK,0.12453874538745388,"algorithm will go to phase 2 until the evaluation budget FEmax has run out.
134"
LORA-MOO FRAMEWORK,0.12546125461254612,"Algorithm 1 LORA-MOO framework
Input: M objective functions of the optimization problem f(x) = (f1(x), . . . , fM(x));
Evaluation budget: The number of allowed function evaluations FEmax.
Procedure:"
LORA-MOO FRAMEWORK,0.12638376383763839,"1: Sample a set of solutions {x1, . . . , x11D−1} and evaluate them on f.
2: Save all evaluated solutions (x, f(x)) in an archive SA. Set the number of used function
evaluations FE = |SA|.
3: while FE < FEmax do
4:
Ordinal training set So ←Quantify ordinal values for all xi ∈SA (Alg. 2).
5:
Ordinal surrogate ho ←Train Kriging(SA, So).
6:
Population of candidate solutions P ←Run an optimizer on ho (Alg. 3).
7:
x∗
1 ←Use the ordinal surrogate to select a solution from P by convergence criterion.
8:
Evaluate x∗
1 and update SA = SA ∪{(x∗
1, f(x∗
1))}, FE = FE + 1.
9:
Angular training set Sa ←Calculate angular coordinates for all xi ∈SA.
10:
M-1 angular surrogates hai ←Train Kriging (SA, Sa), i = 1, . . . , M −1.
11:
x∗
2 ←Use angular surrogates to select a solution from P by diversity criterion (Alg. 4).
12:
Evaluate x∗
2 and update SA = SA ∪{(x∗
2, f(x∗
2))}, FE = FE + 1.
13: end while
Output: Non-dominated solutions in archive SA."
SURROGATE MODELING,0.12730627306273062,"3.2
Surrogate Modeling
135"
SURROGATE MODELING,0.1282287822878229,"The ordinal surrogate ho is mainly trained on dominance-based ordinal relations, additional clustering-
136"
SURROGATE MODELING,0.12915129151291513,"based artiﬁcial ordinal relations will be introduced for training if the number of objectives M is
137"
SURROGATE MODELING,0.13007380073800737,"large. In addition, for an M-objective problem, M-1 angular surrogates hai are trained on angular
138"
SURROGATE MODELING,0.13099630996309963,"coordinates. These surrogates are used in the selection procedure for solution diversity but are idle in
139"
SURROGATE MODELING,0.13191881918819187,"the search procedure.
140"
SURROGATE MODELING,0.13284132841328414,"3.2.1
Learning dominance-based ordinal relations.
141"
SURROGATE MODELING,0.13376383763837638,"In LORA-MOO, the concept of ordinal regression [40] is adapted to learn dominance-based ordinal
142"
SURROGATE MODELING,0.13468634686346864,"relations. Clearly, the dominance-based ordinal relation between a set of reference points SRP and a
143"
SURROGATE MODELING,0.13560885608856088,"given solution x is quantiﬁed as a relation value. Such a relation value is a numerical value that used
144"
SURROGATE MODELING,0.13653136531365315,"for training the ordinal-regression surrogate ho. The quantiﬁcation of relation values consists of two
145"
SURROGATE MODELING,0.13745387453874539,"steps: The selection of reference points SRP and the computation of relation values.
146"
SURROGATE MODELING,0.13837638376383765,"Selection of Reference Points. We propose the deﬁnition of λ-dominance relationship to simplify
147"
SURROGATE MODELING,0.1392988929889299,"the selection of reference points.
148"
SURROGATE MODELING,0.14022140221402213,"Deﬁnition 1. (λ-Dominance Relationship)
149"
SURROGATE MODELING,0.1411439114391144,"A solution x1 is said to λ-dominate another solution x2 (denoted by x1 ≺λ x2) if and only if:
150"
SURROGATE MODELING,0.14206642066420663,"gλ(x1) ≺gλ(x2),
(1)
where λ ≥0 is the dominance coefﬁcient and gλ is a smooth objective function deﬁned as:
151"
SURROGATE MODELING,0.1429889298892989,"fin(x) = fi(x) −z∗
i
|znad
i
−z∗
i |,
(2) 152"
SURROGATE MODELING,0.14391143911439114,"gλ,i(x) = fin(x) + λmax(fjn(x)), j ∈{1, . . . , M},
(3)
where fin denotes a normalized objective function, z∗= {z∗
1, . . . , z∗
M}, znad = {znad
1
, . . . , znad
M }
153"
SURROGATE MODELING,0.1448339483394834,"are ideal point and nadir point for the current non-dominated solutions, respectively.
154"
SURROGATE MODELING,0.14575645756457564,"More detailed deﬁnitions about the background of MOO are available in Appendix A. All non-λ-
155"
SURROGATE MODELING,0.1466789667896679,"dominated solutions in SA are selected as reference points SRP . There are two reasons to introduce
156"
SURROGATE MODELING,0.14760147601476015,"the deﬁnition of λ-dominance:
157"
SURROGATE MODELING,0.14852398523985239,"• The λ-dominance can smoothen the original PF by excluding dominance resistant solutions
158"
SURROGATE MODELING,0.14944649446494465,"(DRSs) [16, 38]. DRSs are solutions that are best or close to best on one or several objectives
159"
SURROGATE MODELING,0.1503690036900369,"but extremely poor on at least one of the remaining objectives. Such a solution is apparently
160"
SURROGATE MODELING,0.15129151291512916,"not desirable but may be regarded as one of the best solutions since there may not exist any
161"
SURROGATE MODELING,0.1522140221402214,"other solutions dominating it in the solution set.
162"
SURROGATE MODELING,0.15313653136531366,"• Second, λ-dominance can eliminate some similar non-dominated solutions from the Pareto
163"
SURROGATE MODELING,0.1540590405904059,"set, which can be used to adjust the size of Pareto set. When the number of objectives M
164"
SURROGATE MODELING,0.15498154981549817,"is large, it is possible that a majority of past evaluated samples are non-dominated to each
165"
SURROGATE MODELING,0.1559040590405904,"other. To balance the number of reference points and remaining samples, we introduce the
166"
SURROGATE MODELING,0.15682656826568267,"dominance coefﬁcient λ to sightly reduce the ratio of reference points in SA. This alleviates
167"
SURROGATE MODELING,0.1577490774907749,"the situation of extreme imbalance of samples in different ordinal levels (see the division of
168"
SURROGATE MODELING,0.15867158671586715,"ordinal levels below).
169"
SURROGATE MODELING,0.1595940959409594,"Computation of Relation Values. To quantify ordinal relation values, we ﬁrst calculate extension
170"
SURROGATE MODELING,0.16051660516605165,"coefﬁcients ec(x) for each x ∈SA. ec(x) is deﬁned as the minimal coefﬁcient ec ≥1 to make a
171"
SURROGATE MODELING,0.16143911439114392,"solution x non-λ-dominated to all solutions x′ in the extended reference:
172"
SURROGATE MODELING,0.16236162361623616,"ec(x) = arg min
ec≥1 ∄x′ ∈SRP : (x′ ∗ec) ≺λ x.
(4)"
SURROGATE MODELING,0.16328413284132842,"Although extension coefﬁcient ec(x) quantiﬁes the distance between a solution x and reference SRP ,
173"
SURROGATE MODELING,0.16420664206642066,"it has not been used to train the ordinal regression-based surrogate directly. To generate a stable
174"
SURROGATE MODELING,0.16512915129151293,"ordinal regression-based surrogate, solutions in SA are divided into No = max(no, |SA|/|SRP |)
175"
SURROGATE MODELING,0.16605166051660517,"ordinal levels, where no is a pre-deﬁned parameter denoting the minimal number of ordinal levels.
176"
SURROGATE MODELING,0.1669741697416974,"The solutions in SRP are classiﬁed into the non-dominated ordinal level, thus the relation value v1 =
177"
SURROGATE MODELING,0.16789667896678967,"1.0 is assigned to them. Remaining solutions in SA are sorted by their extension coefﬁcients ec(x)
178"
SURROGATE MODELING,0.1688191881918819,"and then divided into No-1 ordinal levels uniformly. The relation value vi = 1 −
i−1
No−1 will be
179"
SURROGATE MODELING,0.16974169741697417,"assigned to the solutions x in the ith ordinal level. Lastly, relation values serve as radial coordinates
180"
SURROGATE MODELING,0.1706642066420664,"and a Kriging model is employed to approximate them.
181"
SURROGATE MODELING,0.17158671586715868,"3.2.2
Artiﬁcial clustering-based ordinal relations.
182"
SURROGATE MODELING,0.17250922509225092,"When the number of objectives M is large, most evaluated solutions in archive SA could be non-
183"
SURROGATE MODELING,0.17343173431734318,"dominated solutions, indicating that these solutions will be divided into the same non-dominated
184"
SURROGATE MODELING,0.17435424354243542,"ordinal level and thus treated as reference points SRP . This is harmful to the ordinal surrogate
185"
SURROGATE MODELING,0.1752767527675277,"modeling due to the extreme imbalance between the numbers of training samples in different ordinal
186"
SURROGATE MODELING,0.17619926199261993,"levels. To reduce the ratio of SRP , we use a clustering method to generate n_clusters clusters
187"
SURROGATE MODELING,0.17712177121771217,"for SRP , where n_clusters is the half of the size of SRP . All solutions x ∈SRP are mapped to
188"
SURROGATE MODELING,0.17804428044280443,"the closest cluster centers. The solutions with the shortest projection on each cluster center will be
189"
SURROGATE MODELING,0.17896678966789667,"selected as the new SRP , while the remaining solutions will be moved to the next ordinal level. Such
190"
SURROGATE MODELING,0.17988929889298894,"artiﬁcial ordinal relations greatly reduce the ratio of SRP in SA. In LORA-MOO, we set a ratio
191"
SURROGATE MODELING,0.18081180811808117,"threshold rp_ratio for SRP , once the ratio of SRP is larger than rp_ratio, artiﬁcial ordinal relations
192"
SURROGATE MODELING,0.18173431734317344,"will be generated for surrogate modeling. Details are available in Appendix C, Alg. 2 and Fig. 5.
193"
SURROGATE MODELING,0.18265682656826568,"3.2.3
Surrogates for Angular Coordinates.
194"
SURROGATE MODELING,0.18357933579335795,"Given a solution x ∈SA with Cartesian coordinates (f1(x), . . . , fM(x)), The angular coordinates
195"
SURROGATE MODELING,0.18450184501845018,"of solution x are transformed with the following rules:
196"
SURROGATE MODELING,0.18542435424354242,"ϕi = arccos
fi(x) −z∗
i
p"
SURROGATE MODELING,0.1863468634686347,"(fi(x) −z∗
i )2 + · · · + (fM(x) −z∗
M)2 , i = 1, . . . , M −1,
(5)"
SURROGATE MODELING,0.18726937269372693,"where z∗is the ideal point. The resulting angular coordinates (ϕ1, . . . , ϕM−1) are used to ﬁt M −1
197"
SURROGATE MODELING,0.1881918819188192,"regression-based surrogates separately. In LORA-MOO, we use the Kriging model to approximate
198"
SURROGATE MODELING,0.18911439114391143,"angular coordinates. The introduction and usage of Kriging model is given in Appendix B.
199"
SURROGATE MODELING,0.1900369003690037,"3.3
Sampling: Search and Selection
200"
SURROGATE MODELING,0.19095940959409594,"In this subsection, we describe how to use surrogate ho to search for candidate solutions and how to
201"
SURROGATE MODELING,0.1918819188191882,"use surrogates ho and hai to select optimal ones from candidate solutions for expensive evaluations.
202"
SURROGATE MODELING,0.19280442804428044,"3.3.1
Search: Generation of Candidate Solutions.
203"
SURROGATE MODELING,0.1937269372693727,"An advantage of LORA-MOO is that it searches for candidate solutions on ordinal surrogate ho
204"
SURROGATE MODELING,0.19464944649446494,"only, leaving all angular surrogates hai idle in this search procedure. This saves a lot of time from
205"
SURROGATE MODELING,0.19557195571955718,"predicting with all surrogates. LORA-MOO employs an optimizer (e.g. PSO [13]) to generate a
206"
SURROGATE MODELING,0.19649446494464945,"population of candidate solutions P (Detailed pseudo-code is available in Appendix C, Alg. 3). The
207"
SURROGATE MODELING,0.1974169741697417,"initial population for optimization search consists of two parts. The ﬁrst half initial solutions are
208"
SURROGATE MODELING,0.19833948339483395,"generated randomly from the decision space, while the remaining initial solutions are mutants of
209"
SURROGATE MODELING,0.1992619926199262,"current reference points SRP . To ensure the diversity of initial candidate solutions, a KNN clustering
210"
SURROGATE MODELING,0.20018450184501846,"method is applied to divide SRP into several different clusters, from each cluster, an equal number of
211"
SURROGATE MODELING,0.2011070110701107,"mutants are generated as initial candidate solutions. The global optimal population P produced by
212"
SURROGATE MODELING,0.20202952029520296,"PSO is the candidate solutions for further environmental selection.
213"
SURROGATE MODELING,0.2029520295202952,"3.3.2
Selection Criteria.
214"
SURROGATE MODELING,0.20387453874538744,"To take both convergence and diversity into consideration, in each iteration, LORA-MOO selects two
215"
SURROGATE MODELING,0.2047970479704797,"optimal candidate solutions x∗
1, x∗
2 from P for objective function evaluations. x∗
1, x∗
2 are sampled on
216"
SURROGATE MODELING,0.20571955719557194,"the basis of convergence and diversity, respectively.
217"
SURROGATE MODELING,0.2066420664206642,"Convergence Criterion for environmental selection is the expected improvement (EI) [14] of ordinal
218"
SURROGATE MODELING,0.20756457564575645,"values, which is similar to many MOBO methods [24, 43]. Since the output of our ordinal surrogate
219"
SURROGATE MODELING,0.20848708487084872,"ho(x) is an 1-D numerical value, the solution with maximal 1-D EI in P is selected as x∗
1.
220"
SURROGATE MODELING,0.20940959409594095,"Diversity Criterion to sample x∗
2 from P is deﬁned as angles dang between candidate solutions
221"
SURROGATE MODELING,0.21033210332103322,"and reference points SRP . Firstly, the minimal degree between each candidate solution and SRP is
222"
SURROGATE MODELING,0.21125461254612546,"measured. Among these minimal degrees mdang, the solution with max(mdang) is selected as x∗
2
223"
SURROGATE MODELING,0.21217712177121772,"(Detailed pseudo-code is available in Appendix C, Alg. 4).
224"
EXPERIMENTS,0.21309963099630996,"4
Experiments
225"
EXPERIMENTS,0.2140221402214022,"To evaluate the optimization performance of LORA-MOO on expensive MOOPs, we conduct
226"
EXPERIMENTS,0.21494464944649447,"experiments to compare LORA-MOO with other SAEAs on different MOOPs, including a series
227"
EXPERIMENTS,0.2158671586715867,"of scalable multi-/many-objective benchmark optimization problems DTLZ [11], WFG [19], and a
228"
EXPERIMENTS,0.21678966789667897,"real-world network architecture search (NAS) problem.
229"
EXPERIMENTAL SETUPS,0.2177121771217712,"4.1
Experimental Setups
230"
EXPERIMENTAL SETUPS,0.21863468634686348,"Optimization Problem Setup. To ensure a fair comparison, the following optimization problem
231"
EXPERIMENTAL SETUPS,0.21955719557195572,"setup is the same as the setup that has been widely used in the literature [5, 31, 34, 17]. In our
232"
EXPERIMENTAL SETUPS,0.22047970479704798,"experiments, initial datasets of size FEinit = 11 D - 1 are used to initialize surrogates, while the
233"
EXPERIMENTAL SETUPS,0.22140221402214022,"maximum number of allowed evaluations FEmax is 300. The statistical results are obtained from 30
234"
EXPERIMENTAL SETUPS,0.22232472324723246,"independent runs. For each run, different comparison algorithms share the same initial dataset.
235"
EXPERIMENTAL SETUPS,0.22324723247232472,"Comparison Algorithms. We compare LORA-MOO with 6 state-of-the-art SAEAs, some of them
236"
EXPERIMENTAL SETUPS,0.22416974169741696,"also known as MOBO methods. These comparison algorithms can be classiﬁed into three categories:
237"
EXPERIMENTAL SETUPS,0.22509225092250923,"• Regression-based MOO methods: ParEGO [24], K-RVEA [5], and KTA2 [34]. ParEGO is a
238"
EXPERIMENTAL SETUPS,0.22601476014760147,"classic regression-based SAEA and also a MOBO, which serves as a baseline. K-RVEA is a
239"
EXPERIMENTAL SETUPS,0.22693726937269373,"typical SAEA which uses reference vector to guide the diversity maintenance. KTA2 is a
240"
EXPERIMENTAL SETUPS,0.22785977859778597,"newly proposed algorithm to use an independent archive to keep solution diversity.
241"
EXPERIMENTAL SETUPS,0.22878228782287824,"• Classiﬁcation-based MOO methods:
CSEA [31], REMO [17].
CSEA is a classic
242"
EXPERIMENTAL SETUPS,0.22970479704797048,"classiﬁcation-based SAEA which serves as a baseline. REMO is a newly proposed SAEA
243"
EXPERIMENTAL SETUPS,0.23062730627306274,"which represents the state-of-the-art performance of classiﬁcation-based SAEAs.
244"
EXPERIMENTAL SETUPS,0.23154981549815498,"• Ordinal-regression-based MOO method: OREA [40] is a new category of SAEA that is
245"
EXPERIMENTAL SETUPS,0.23247232472324722,"different from common regression-based and classiﬁcation-based SAEAs. We compare with
246"
EXPERIMENTAL SETUPS,0.2333948339483395,"it since it is directly related to our radial surrogate.
247"
EXPERIMENTAL SETUPS,0.23431734317343172,"Note that some classic SAEAs and MOBO methods such as MOEA/D-EGO [43] and CPS-MOEA
248"
EXPERIMENTAL SETUPS,0.235239852398524,"[44] are not compared in our experiments as they failed to outperform other comparison algorithms
249"
EXPERIMENTAL SETUPS,0.23616236162361623,"on any DTLZ problem [17]. Some HV-based MOBO methods are not compared as they are failed to
250"
EXPERIMENTAL SETUPS,0.2370848708487085,"solve many objectives.
251"
EXPERIMENTAL SETUPS,0.23800738007380073,"Parameter Setup. For the surrogate modeling, the Kriging models used in all comparison algorithms
252"
EXPERIMENTAL SETUPS,0.238929889298893,"are implemented using DACE [32], just as [24] suggested. For regression-based Kriging surrogates,
253"
EXPERIMENTAL SETUPS,0.23985239852398524,"the range of hyper-parameter θ ∈[10−5, 100]. And for the neural networks in CSEA and REMO, the
254"
EXPERIMENTAL SETUPS,0.24077490774907748,"parameters are the same as suggested in the literature. In the sampling strategy, the mutation operator
255"
EXPERIMENTAL SETUPS,0.24169741697416974,"used to initialize candidate solutions is polynomial mutation [9], the mutation probability pm = 1/d
256"
EXPERIMENTAL SETUPS,0.24261992619926198,"(a) 10 variables and 3 objectives.
(b) 10 variables and 10 objectives."
EXPERIMENTAL SETUPS,0.24354243542435425,"Figure 1: IGD curves averaged over 15 runs on the WFG5 problem instances for LORA-MOO with
different parameter setups (shaded area is ± std of the mean)."
EXPERIMENTAL SETUPS,0.2444649446494465,"and mutation index ηm = 20, as recommended in [34, 17]. The size of offspring population is 100.
257"
EXPERIMENTAL SETUPS,0.24538745387453875,"The settings of the PSO optimizer are the range of hyper-parameter in the ordinal-regression-based
258"
EXPERIMENTAL SETUPS,0.246309963099631,"surrogate are the same as suggested in [40].
259"
EXPERIMENTAL SETUPS,0.24723247232472326,"For the speciﬁc parameters exist in LORA-MOO, such as the dominance coefﬁcient λ and the
260"
EXPERIMENTAL SETUPS,0.2481549815498155,"threshold ratio of reference points to introduce clustering-based ordinal relations rp_ratio. As there
261"
EXPERIMENTAL SETUPS,0.24907749077490776,"is no relevant study in the literature for their setups, we conducted ablation studies to investigate
262"
EXPERIMENTAL SETUPS,0.25,"the effect of these parameters on the performance of LORA-MOO. The results are summarized in
263"
EXPERIMENTAL SETUPS,0.25092250922509224,"Section 4.2 and reported in Appendix F. The source code of LORA-MOO 2 will be available online.
264"
EXPERIMENTAL SETUPS,0.2518450184501845,"Performance Indicator. To have a comprehensive estimation of optimization performance, we use
265"
EXPERIMENTAL SETUPS,0.25276752767527677,"three different performance indicators in our experiments: The inverted generational distance (IGD)
266"
EXPERIMENTAL SETUPS,0.253690036900369,"[4], the inverted generational distance plus (IGD+) [20], and the Hypervolume (HV) [45]. IGD and
267"
EXPERIMENTAL SETUPS,0.25461254612546125,"IGD+ use a set of truth Pareto front to measure the quality of a set of non-dominated solutions in
268"
EXPERIMENTAL SETUPS,0.2555350553505535,"terms of convergence and diversity. A smaller IGD or IGD+ value indicates better MOO performance.
269"
EXPERIMENTAL SETUPS,0.2564575645756458,"HV use a reference point to calculate the area covered by a set of non-dominated solutions, a large
270"
EXPERIMENTAL SETUPS,0.257380073800738,"HV value is preferable to MOO. See Appendix D for details and setups about performance indicators.
271"
ABLATION STUDIES,0.25830258302583026,"4.2
Ablation Studies
272"
ABLATION STUDIES,0.2592250922509225,"We conduct ablation studies on DTLZ and WFG benchmark problems with D = 10 variables and
273"
ABLATION STUDIES,0.26014760147601473,"M={3, 6, 10} objectives. LHS [30] is used to sample initial dataset. The effects of four parameters
274"
ABLATION STUDIES,0.261070110701107,"are investigated: They are the minimal number of ordinal levels no, the dominance coefﬁcient λ, the
275"
ABLATION STUDIES,0.26199261992619927,"ratio threshold of reference points rp_ratio, and the clustering number for reproduction nc. Three
276"
ABLATION STUDIES,0.2629151291512915,"representative results obtained on the WFG5 problem with 3 and 10 objectives are depicted in Fig. 1.
277"
ABLATION STUDIES,0.26383763837638374,"Complete results and statistical analysis of ablation studies are reported in Appendix F.
278"
ABLATION STUDIES,0.26476014760147604,"As shown in Fig. 1 (left), when M = 10, a large no results in poor optimization performance. This is
279"
ABLATION STUDIES,0.2656826568265683,"because the ratio of non-dominated solutions in the archive tends to be large when M is large, hence,
280"
ABLATION STUDIES,0.2666051660516605,"setting a large no will lead to a lack of training samples in each dominated ordinal levels, which is
281"
ABLATION STUDIES,0.26752767527675275,"detrimental to the performance of surrogate modeling. As such, no in LORA-MOO is set to 4.
282"
ABLATION STUDIES,0.26845018450184505,"The result in Fig. 1 (middle) shows that using λ-dominance to sightly modify the original dominance
283"
ABLATION STUDIES,0.2693726937269373,"relations is beneﬁcial to the effectiveness of LORA-MOO. When λ = 0, no λ-dominance would be
284"
ABLATION STUDIES,0.2702952029520295,"used and the corresponding LORA-MOO variant has the worst performance among all the variants. In
285"
ABLATION STUDIES,0.27121771217712176,"addition, setting a large λ could cause severe damage to the original dominance relations. Therefore,
286"
ABLATION STUDIES,0.272140221402214,"we set λ to 0.2.
287"
ABLATION STUDIES,0.2730627306273063,"The effect of introducing artiﬁcial ordinal relations via clustering is demonstrated in Fig. 1 (right).
288"
ABLATION STUDIES,0.27398523985239853,"When the ratio threshold of reference points rp_ratio is 1 and M = 10, no artiﬁcial ordinal relations
289"
ABLATION STUDIES,0.27490774907749077,"are introduced to further divide ordinal levels for plenty of non-dominated solutions in the archive.
290"
ABLATION STUDIES,0.275830258302583,"Consequently, the imbalance of sample numbers in different ordinal levels leads to poor optimization
291"
ABLATION STUDIES,0.2767527675276753,"performance. However, dominance relations are preferable to artiﬁcial ordinal relations when M = 3
292"
ABLATION STUDIES,0.27767527675276754,"and the size of ordinal levels are well balanced. Hence, we set rp_ratio = 0.5.
293"
OPTIMIZATION ON BENCHMARK PROBLEMS,0.2785977859778598,"4.3
Optimization on Benchmark Problems
294"
OPTIMIZATION ON BENCHMARK PROBLEMS,0.279520295202952,"The optimization performance of LORA-MOO is evaluated on DTLZ and WFG benchmark problems
295"
OPTIMIZATION ON BENCHMARK PROBLEMS,0.28044280442804426,"with D = 10 variables and M={3, 4, 6, 8, 10} objectives. The IGD values obtained on DTLZ
296"
OPTIMIZATION ON BENCHMARK PROBLEMS,0.28136531365313655,2The link of code and data will be released here once the paper is accepted.
OPTIMIZATION ON BENCHMARK PROBLEMS,0.2822878228782288,"Table 1: Statistical results of the IGD value obtained by the comparison algorithms on the 35 DTLZ
optimization problems over 30 runs. Symbols ‘+’, ‘≈’, ‘−’ denote LORA-MOO is statistically
signiﬁcantly superior to, equivalent to, and inferior to the compared algorithms in the Wilcoxon rank
sum test (signiﬁcance level is 0.05), respectively. The last three rows are the total win/tie/loss results
on DTLZ, WFG, and both of them, respectively."
OPTIMIZATION ON BENCHMARK PROBLEMS,0.283210332103321,"Problems
M
ParEGO
KRVEA
KTA2
CSEA
REMO
OREA
LORA-MOO (ours)
DTLZ1
3
5.98e+1(3.81e+0)+
8.88e+1(2.16e+1)+
4.75e+1(1.55e+1)≈
6.30e+1(1.69e+1)+
5.06e+1(1.49e+1)+
4.44e+1(1.38e+1)≈
4.35e+1(1.80e+1)
4
4.68e+1(3.71e+0)+
6.45e+1(1.47e+1)+
4.08e+1(1.60e+1)≈
3.69e+1(1.08e+1)≈
3.92e+1(1.11e+1)≈
3.80e+1(1.23e+1)≈
4.06e+1(1.34e+1)
6
3.04e+1(2.74e+0)+
3.22e+1(7.66e+0)+
2.03e+1(8.12e+0)+
1.56e+1(4.96e+0)≈
1.22e+1(4.65e+0)−
1.74e+1(3.98e+0)≈
1.58e+1(6.17e+0)
8
1.23e+1(2.99e+0)+
8.52e+0(2.97e+0)+
4.54e+0(2.66e+0)≈
5.08e+0(2.47e+0)≈
3.33e+0(1.93e+0)≈
5.87e+0(2.91e+0)+
3.83e+0(2.35e+0)
10
4.37e-1(1.63e-1)+
3.32e-1(9.91e-2)+
3.00e-1(8.76e-2)+
2.90e-1(7.13e-2)+
2.42e-1(6.97e-2)≈
2.58e-1(6.33e-2)≈
2.31e-1(3.89e-2)
DTLZ2
3
3.38e-1(2.84e-2)+
1.32e-1(2.77e-2)+
6.17e-2(3.13e-3)≈
2.26e-1(2.61e-2)+
1.65e-1(2.18e-2)+
8.59e-2(8.51e-3)+
6.19e-2(3.48e-3)
4
4.23e-1(2.79e-2)+
2.06e-1(2.95e-2)+
1.41e-1(5.45e-3)≈
2.92e-1(1.89e-2)+
2.43e-1(2.33e-2)+
1.83e-1(1.37e-2)+
1.38e-1(9.86e-3)
6
5.53e-1(2.17e-2)+
3.40e-1(1.20e-2)+
3.24e-1(2.63e-2)+
4.42e-1(3.37e-2)+
3.77e-1(3.16e-2)+
3.96e-1(2.57e-2)+
2.67e-1(8.78e-3)
8
6.53e-1(1.86e-2)+
4.19e-1(2.65e-2)+
4.44e-1(1.86e-2)+
5.95e-1(2.77e-2)+
5.10e-1(3.90e-2)+
5.56e-1(2.19e-2)+
3.80e-1(1.46e-2)
10
6.95e-1(2.23e-2)+
5.92e-1(4.25e-2)+
4.50e-1(1.00e-2)≈
6.76e-1(2.52e-2)+
5.85e-1(3.72e-2)+
6.55e-1(2.66e-2)+
4.54e-1(1.41e-2)
DTLZ3
3
1.66e+2(1.31e+1)+
2.43e+2(4.61e+1)+
1.52e+2(4.73e+1)≈
1.62e+2(4.84e+1)≈
1.49e+2(3.88e+1)≈
1.26e+2(3.18e+1)−
1.57e+2(3.83e+1)
4
1.42e+2(1.57e+1)+
1.83e+2(4.00e+1)+
1.18e+2(3.49e+1)≈
1.29e+2(3.58e+1)≈
1.16e+2(3.00e+1)≈
1.22e+2(4.13e+1)≈
1.25e+2(4.20e+1)
6
9.17e+1(1.59e+1)+
1.06e+2(2.96e+1)+
6.65e+1(2.63e+1)≈
5.27e+1(1.56e+1)≈
5.23e+1(1.71e+1)≈
5.24e+1(1.68e+1)≈
5.96e+1(2.05e+1)
8
4.13e+1(9.84e+0)+
2.96e+1(1.15e+1)+
1.74e+1(1.10e+1)≈
1.60e+1(9.76e+0)≈
1.60e+1(7.70e+0)≈
1.50e+1(6.27e+0)≈
1.27e+1(8.33e+0)
10
1.36e+0(3.15e-1)+
1.23e+0(4.27e-1)+
9.95e-1(2.25e-1)+
1.01e+0(2.45e-1)+
9.53e-1(2.74e-1)+
8.77e-1(1.08e-1)+
8.14e-1(1.33e-1)
DTLZ4
3
6.70e-1(7.61e-2)+
3.32e-1(1.11e-1)+
3.49e-1(1.09e-1)+
4.62e-1(1.36e-1)+
2.31e-1(1.15e-1)+
2.39e-1(1.65e-1)+
1.89e-1(2.34e-1)
4
7.18e-1(6.40e-2)+
4.07e-1(8.73e-2)+
4.77e-1(9.70e-2)+
4.31e-1(6.36e-2)+
3.36e-1(7.02e-2)≈
3.45e-1(1.52e-1)≈
3.48e-1(1.60e-1)
6
7.06e-1(3.07e-2)+
5.04e-1(5.42e-2)+
6.05e-1(8.43e-2)+
4.94e-1(4.55e-2)+
4.97e-1(4.95e-2)+
4.47e-1(4.89e-2)≈
4.55e-1(6.53e-2)
8
6.81e-1(1.48e-2)+
5.49e-1(3.42e-2)+
6.24e-1(5.48e-2)+
5.85e-1(4.20e-2)+
6.16e-1(4.03e-2)+
5.29e-1(3.79e-2)≈
5.32e-1(2.38e-2)
10
6.77e-1(1.26e-2)+
6.07e-1(2.42e-2)+
6.36e-1(3.58e-2)+
6.38e-1(2.38e-2)+
6.71e-1(2.69e-2)+
5.90e-1(1.94e-2)≈
5.90e-1(2.51e-2)
DTLZ5
3
2.16e-1(4.45e-2)+
1.19e-1(3.38e-2)+
1.34e-2(2.83e-3)≈
1.18e-1(2.56e-2)+
7.36e-2(2.03e-2)+
2.02e-2(4.77e-3)+
1.26e-2(2.55e-3)
4
1.89e-1(3.70e-2)+
7.05e-2(2.25e-2)+
4.24e-2(8.84e-3)+
1.16e-1(2.23e-2)+
9.02e-2(2.48e-2)+
3.48e-2(7.82e-3)+
2.85e-2(9.37e-3)
6
1.41e-1(2.32e-2)+
3.53e-2(1.02e-2)−
8.87e-2(1.91e-2)+
7.72e-2(2.57e-2)+
5.53e-2(1.90e-2)+
4.62e-2(1.50e-2)≈
4.26e-2(1.11e-2)
8
7.72e-2(1.22e-2)+
1.99e-2(4.92e-3)−
6.43e-2(8.60e-3)+
3.81e-2(1.03e-2)+
3.10e-2(7.33e-3)≈
2.59e-2(6.96e-3)−
2.84e-2(4.88e-3)
10
2.25e-2(1.87e-3)+
1.25e-2(1.90e-3)+
2.04e-2(2.55e-3)+
1.27e-2(1.46e-3)+
9.35e-3(2.00e-3)−
1.03e-2(1.62e-3)≈
1.06e-2(2.36e-3)
DTLZ6
3
3.15e-1(1.62e-1)+
3.06e+0(5.21e-1)+
1.83e+0(4.37e-1)+
4.86e+0(6.30e-1)+
4.27e+0(5.49e-1)+
3.09e-1(3.99e-1)+
1.18e-1(1.57e-1)
4
3.56e-1(2.12e-1)≈
2.46e+0(3.84e-1)+
1.85e+0(5.06e-1)+
5.13e+0(4.23e-1)+
4.08e+0(6.16e-1)+
1.43e+0(8.89e-1)+
3.29e-1(2.22e-1)
6
2.66e-1(1.37e-1)−
1.36e+0(2.73e-1)+
1.51e+0(5.85e-1)+
3.15e+0(4.35e-1)+
2.33e+0(5.70e-1)+
2.05e+0(6.16e-1)+
9.89e-1(1.02e+0)
8
1.61e-1(6.17e-2)≈
5.28e-1(1.50e-1)+
8.64e-1(3.88e-1)+
1.56e+0(4.28e-1)+
9.64e-1(4.38e-1)+
1.06e+0(3.95e-1)+
3.56e-1(4.31e-1)
10
1.72e-1(1.45e-1)+
7.73e-2(3.13e-2)≈
1.01e-1(4.97e-2)+
2.09e-1(2.28e-1)+
7.91e-2(1.11e-1)≈
1.50e-1(7.37e-2)+
7.05e-2(3.25e-2)
DTLZ7
3
2.45e-1(4.80e-2)+
1.35e-1(2.37e-2)≈
2.19e-1(2.40e-1)−
1.75e+0(6.32e-1)+
1.27e+0(5.65e-1)+
2.73e-1(1.58e-1)+
2.01e-1(1.93e-1)
4
6.59e-1(1.02e-1)+
3.38e-1(7.61e-2)≈
3.73e-1(1.68e-1)≈
2.94e+0(6.59e-1)+
2.06e+0(7.31e-1)+
8.92e-1(4.27e-1)+
4.20e-1(2.21e-1)
6
1.21e+0(1.58e-1)−
6.04e-1(4.57e-2)−
6.46e-1(1.68e-1)−
4.92e+0(9.92e-1)+
3.09e+0(6.71e-1)+
4.03e+0(1.84e+0)+
1.71e+0(6.54e-1)
8
1.45e+0(1.24e-1)−
8.71e-1(7.01e-2)−
1.02e+0(1.65e-1)−
6.12e+0(1.85e+0)+
3.82e+0(5.39e-1)+
4.55e+0(2.63e+0)+
2.44e+0(6.78e-1)
10
1.67e+0(1.24e-1)+
1.12e+0(4.25e-2)−
1.30e+0(2.04e-1)≈
1.99e+0(3.05e-1)+
1.99e+0(3.36e-1)+
1.63e+0(2.42e-1)+
1.34e+0(9.19e-2)
+/ ≈/−
on DTLZ
30/2/3
27/3/5
19/13/3
28/7/0
23/10/2
20/13/2
+/ ≈/−
on WFG
39/4/2
21/10/14
23/6/16
41/1/3
38/3/4
43/1/1
+/ ≈/−
on both
69/6/5
48/13/19
42/19/19
69/8/3
61/13/6
63/14/3"
OPTIMIZATION ON BENCHMARK PROBLEMS,0.28413284132841327,"problems with different M are reported in Table 1. It shows that LORA-MOO achieves the best
297"
OPTIMIZATION ON BENCHMARK PROBLEMS,0.28505535055350556,"optimization results among all the comparison algorithms in terms of IGD values, followed by KTA2
298"
OPTIMIZATION ON BENCHMARK PROBLEMS,0.2859778597785978,"and KRVEA. The IGD values obtained on the WFG problems, the IGD+ and HV results, and the
299"
OPTIMIZATION ON BENCHMARK PROBLEMS,0.28690036900369004,"results obtained under different scales (D= 5 or 20) are reported in Appendix H. A consistent result
300"
OPTIMIZATION ON BENCHMARK PROBLEMS,0.2878228782287823,"can be concluded from the IGD+ and HV values. The results on the 3- and 10-objective problems are
301"
OPTIMIZATION ON BENCHMARK PROBLEMS,0.2887453874538745,plotted in Fig. 2.
OPTIMIZATION ON BENCHMARK PROBLEMS,0.2896678966789668,"Figure 2: IGD(log) curves averaged over 30 runs on the DTLZ problems for the comparison
algorithms (shaded area is ± std of the mean). Top: 10 variables and 3 objectives. Bottom: 10
variables and 10 objectives. More ﬁgures are displayed in Appendices G and H. 302"
REAL-WORLD NETWORK ARCHITECTURE SEARCH PROBLEM,0.29059040590405905,"4.4
Real-World Network Architecture Search Problem
303"
REAL-WORLD NETWORK ARCHITECTURE SEARCH PROBLEM,0.2915129151291513,"Further comparison is conducted on a real-world network architecture search (NAS) problem, the
best three algorithms listed in Table 1 are compared: LORA-MOO, KTA2, and KRVEA. The NAS
problem tested is the NASbench201 implemented in EvoXBench [29], it has 6 variables and 5
objectives. Details of this NAS problem is provided in Appendix E. Considering NASbench201
is a real-world application and we do not know its exact PF, we use HV to evaluate optimization
performance since HV can be calculated without the exact PF. In practice, log(HVdiff) is employed
to amplify the visual difference of the obtained HV values:
log(HVdiff) = log(HVmax −HV )
where HVmax is the maximal HV value on this problem that is provided in EvoXBench.
304"
REAL-WORLD NETWORK ARCHITECTURE SEARCH PROBLEM,0.2924354243542435,"Fig. 3 plots the result. As can be seen in
the ﬁgure, LORA-MOO outperforms KTA2
and KRVEA on this NAS problem. Although
KTA2 and KRVEA have quicker convergence
rate than LORA-MOO at the beginning of the
optimization, both of them slow down their
convergence speed as the number of evalua-
tions increases. Particularly, KTA2 is trapped
on local optima and thus fails to reach better
results. In comparison, LORA-MOO reaches
better NAS results when the evaluation num-
ber is larger than 250.
Figure 3: Log(HVdiff) curves averaged over 30 runs
on the NAS problem for the comparison algorithms. 305"
RUNTIME COMPARISON,0.2933579335793358,"4.5
Runtime Comparison
306"
RUNTIME COMPARISON,0.29428044280442806,"We compare the runtime on benchmark problems for all the comparison algorithms to in-
307"
RUNTIME COMPARISON,0.2952029520295203,"vestigate the relation between their optimization efﬁciency and the number of objectives M.
308"
RUNTIME COMPARISON,0.29612546125461253,"Fig. 4 illustrates how the runtime of each
comparison algorithm varies as the M in-
creases. It can be observed that the runtime
of KTA2 increases exactly in the same rate as
M increases. In comparison, the runtime of
LORA-MOO increases slightly when M in-
creases. This demonstrates that using angular
surrogates only at the end of environmental
selection process is beneﬁcial to the optimiza-
tion efﬁciency of LORA-MOO. In addition,
the runtimes of ParEGO, CSEA, REMO, and
OREA do not increase signiﬁcantly with M
since they do not maintain speciﬁc surrogates
to manage the diversity of non-dominated so-
lutions. Consequently, their overall perfor-
mance reported in Table 1 is not desirable.
Overall, LORA-MOO ﬁnds a good trade-off
between optimization efﬁciency and optimiza-
tion results."
RUNTIME COMPARISON,0.29704797047970477,"Figure 4: Comparison of runtime averaged over
30 runs on benchmark problems D = 10 variables
and M = 3, 4, 6, 8, and 10 objectives for the com-
parison algorithms. For each algorithm, its run-
times are normalized by the runtime it costed on
3-objective problems. 309"
CONCLUSION,0.29797047970479706,"5
Conclusion
310"
CONCLUSION,0.2988929889298893,"In this paper, we propose an efﬁcient MOO method, LORA-MOO, to solve expensive MOOPs.
311"
CONCLUSION,0.29981549815498154,"Different from existing surrogate modeling approaches, our LORA-MOO learns surrogate models
312"
CONCLUSION,0.3007380073800738,"from ordinal relations and spherical coordinates. Only one ordinal surrogate is used in the model-
313"
CONCLUSION,0.3016605166051661,"based search, which hugely improve the efﬁciency of optimization. Our empirical studies have
314"
CONCLUSION,0.3025830258302583,"demonstrated that our LORA-MOO signiﬁcantly outperforms other state-of-the-art efﬁcient MOO
315"
CONCLUSION,0.30350553505535055,"methods, including SAEAs and MOBO methods.
316"
REFERENCES,0.3044280442804428,"References
317"
REFERENCES,0.3053505535055351,"[1] Alaleh Ahmadianshalchi, Syrine Belakaria, and Janardhan Rao Doppa. Pareto front-diverse batch
318"
REFERENCES,0.3062730627306273,"multi-objective Bayesian optimization. In Proceedings of the 38th AAAI Conference on Artiﬁcial
319"
REFERENCES,0.30719557195571956,"Intelligence (AAAI’24), pages 10784–10794, 2024.
320"
REFERENCES,0.3081180811808118,"[2] Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search for
321"
REFERENCES,0.30904059040590404,"multi-objective Bayesian optimization. In Advances in Neural Information Processing Systems 32
322"
REFERENCES,0.30996309963099633,"(NeurIPS’19), pages 7825–7835, 2019.
323"
REFERENCES,0.31088560885608857,"[3] Syrine Belakaria, Aryan Deshwal, Nitthilan Kannappan Jayakodi, and Janardhan Rao Doppa.
324"
REFERENCES,0.3118081180811808,"Uncertainty-aware search framework for multi-objective Bayesian optimization. In Proceedings
325"
REFERENCES,0.31273062730627305,"of the 34th AAAI Conference on Artiﬁcial Intelligence (AAAI’20), pages 10044–10052, 2020.
326"
REFERENCES,0.31365313653136534,"[4] Peter AN Bosman and Dirk Thierens. The balance between proximity and diversity in multiob-
327"
REFERENCES,0.3145756457564576,"jective evolutionary algorithms. IEEE Transactions on Evolutionary Computation, 7(2):174–188,
328"
REFERENCES,0.3154981549815498,"2003.
329"
REFERENCES,0.31642066420664205,"[5] Tinkle Chugh, Yaochu Jin, Kaisa Miettinen, Jussi Hakanen, and Karthik Sindhya. A surrogate-
330"
REFERENCES,0.3173431734317343,"assisted reference vector guided evolutionary algorithm for computationally expensive many-
331"
REFERENCES,0.3182656826568266,"objective optimization. IEEE Transactions on Evolutionary Computation, 22(1):129–142, 2016.
332"
REFERENCES,0.3191881918819188,"[6] Samuel Daulton, Maximilian Balandat, and Eytan Bakshy. Differentiable expected hypervol-
333"
REFERENCES,0.32011070110701106,"ume improvement for parallel multi-objective Bayesian optimization. In Advances in Neural
334"
REFERENCES,0.3210332103321033,"Information Processing Systems 33 (NeurIPS’20), pages 9851–9864, 2020.
335"
REFERENCES,0.3219557195571956,"[7] Samuel Daulton, Maximilian Balandat, and Eytan Bakshy. Parallel Bayesian optimization
336"
REFERENCES,0.32287822878228783,"of multiple noisy objectives with expected hypervolume improvement. In Advances in Neural
337"
REFERENCES,0.3238007380073801,"Information Processing Systems 34 (NeurIPS’21), pages 2187–2200, 2021.
338"
REFERENCES,0.3247232472324723,"[8] Samuel Daulton, David Eriksson, Maximilian Balandat, and Eytan Bakshy. Multi-objective
339"
REFERENCES,0.32564575645756455,"Bayesian optimization over high-dimensional search spaces. In Proceedings of the 38th Conference
340"
REFERENCES,0.32656826568265684,"on Uncertainty in Artiﬁcial Intelligence (UAI’22), pages 507–517, 2022.
341"
REFERENCES,0.3274907749077491,"[9] Kalyanmoy Deb and Mayank Goyal. A combined genetic adaptive search (GeneAS) for engi-
342"
REFERENCES,0.3284132841328413,"neering design. Computer Science and Informatics, 26(4):30–45, 1996.
343"
REFERENCES,0.32933579335793356,"[10] Kalyanmoy Deb and Himanshu Jain. An evolutionary many-objective optimization algorithm
344"
REFERENCES,0.33025830258302585,"using reference-point-based nondominated sorting approach, part I: solving problems with box
345"
REFERENCES,0.3311808118081181,"constraints. IEEE Transactions on Evolutionary Computation, 18(4):577–601, 2013.
346"
REFERENCES,0.33210332103321033,"[11] Kalyanmoy Deb, Lothar Thiele, Marco Laumanns, and Eckart Zitzler. Scalable test problems
347"
REFERENCES,0.33302583025830257,"for evolutionary multiobjective optimization. In Evolutionary Multiobjective Optimization, pages
348"
REFERENCES,0.3339483394833948,"105–145. Springer, London, U.K., 2005.
349"
REFERENCES,0.3348708487084871,"[12] Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural archi-
350"
REFERENCES,0.33579335793357934,"tecture search. In Proceedings of the 8th International Conference on Learning Representations
351"
REFERENCES,0.3367158671586716,"(ICLR’20), 2020.
352"
REFERENCES,0.3376383763837638,"[13] Russell Eberhart and James Kennedy. Particle swarm optimization. In Proceedings of the 1995
353"
REFERENCES,0.3385608856088561,"IEEE International Conference on Neural Networks (ICNN’95), pages 1942–1948, 1995.
354"
REFERENCES,0.33948339483394835,"[14] Michael TM Emmerich, Kyriakos C Giannakoglou, and Boris Naujoks. Single-and multiobjec-
355"
REFERENCES,0.3404059040590406,"tive evolutionary optimization assisted by Gaussian random ﬁeld metamodels. IEEE Transactions
356"
REFERENCES,0.3413284132841328,"on Evolutionary Computation, 10(4):421–439, 2006.
357"
REFERENCES,0.3422509225092251,"[15] Ahsanul Habib, Hemant Kumar Singh, Tinkle Chugh, Tapabrata Ray, and Kaisa Miettinen. A
358"
REFERENCES,0.34317343173431736,"multiple surrogate assisted decomposition-based evolutionary algorithm for expensive multi/many-
359"
REFERENCES,0.3440959409594096,"objective optimization. IEEE Transactions on Evolutionary Computation, 23(6):1000–1014,
360"
REFERENCES,0.34501845018450183,"2019.
361"
REFERENCES,0.3459409594095941,"[16] Thomas Hanne. On the convergence of multiobjective evolutionary algorithms. European
362"
REFERENCES,0.34686346863468637,"Journal of Operational Research, 117(3):553–564, 1999.
363"
REFERENCES,0.3477859778597786,"[17] Hao Hao, Aimin Zhou, Hong Qian, and Hu Zhang. Expensive multiobjective optimization by
364"
REFERENCES,0.34870848708487084,"relation learning and prediction. IEEE Transactions on Evolutionary Computation, 26(5):1157–
365"
REFERENCES,0.3496309963099631,"1170, 2022.
366"
REFERENCES,0.3505535055350554,"[18] Xiaobin Huang, Lei Song, Ke Xue, and Chao Qian. Stochastic Bayesian optimization with
367"
REFERENCES,0.3514760147601476,"unknown continuous context distribution via kernel density estimation. In Proceedings of the 38th
368"
REFERENCES,0.35239852398523985,"AAAI Conference on Artiﬁcial Intelligence (AAAI’24), pages 12635–12643, 2024.
369"
REFERENCES,0.3533210332103321,"[19] Simon Huband, Philip Hingston, Luigi Barone, and Lyndon While. A review of multiobjective
370"
REFERENCES,0.35424354243542433,"test problems and a scalable test problem toolkit. IEEE Transactions on Evolutionary Computation,
371"
REFERENCES,0.3551660516605166,"10(5):477–506, 2006.
372"
REFERENCES,0.35608856088560886,"[20] Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, and Yusuke Nojima. Modiﬁed distance
373"
REFERENCES,0.3570110701107011,"calculation in generational distance and inverted generational distance. In Proceedings of the
374"
REFERENCES,0.35793357933579334,"8th International Conference on Evolutionary Multi-criterion Optimization (EMO’15), pages
375"
REFERENCES,0.35885608856088563,"110–125, 2015.
376"
REFERENCES,0.35977859778597787,"[21] M. Janga Reddy and D. Nagesh Kumar. Evolutionary algorithms, swarm intelligence methods,
377"
REFERENCES,0.3607011070110701,"and their applications in water resources engineering: A state-of-the-art review. H2Open Journal,
378"
REFERENCES,0.36162361623616235,"3(1):135–188, 2021.
379"
REFERENCES,0.3625461254612546,"[22] Yaochu Jin. A comprehensive survey of ﬁtness approximation in evolutionary computation.
380"
REFERENCES,0.3634686346863469,"Soft Computing, 9(1):3–12, 2005.
381"
REFERENCES,0.3643911439114391,"[23] Donald R. Jones, Matthias Schonlau, and William J. Welch. Efﬁcient global optimization of
382"
REFERENCES,0.36531365313653136,"expensive black-box functions. Journal of Global Optimization, 13(4):455–492, 1998.
383"
REFERENCES,0.3662361623616236,"[24] Joshua Knowles. ParEGO: A hybrid algorithm with on-line landscape approximation for
384"
REFERENCES,0.3671586715867159,"expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation,
385"
REFERENCES,0.36808118081180813,"10(1):50–66, 2006.
386"
REFERENCES,0.36900369003690037,"[25] Ke Li, Kalyanmoy Deb, Qingfu Zhang, and Sam Kwong. An evolutionary many-objective opti-
387"
REFERENCES,0.3699261992619926,"mization algorithm based on dominance and decomposition. IEEE Transactions on Evolutionary
388"
REFERENCES,0.37084870848708484,"Computation, 19(5):694–716, 2014.
389"
REFERENCES,0.37177121771217714,"[26] Lin Lin and Mitsuo Gen. Hybrid evolutionary optimisation with learning for production
390"
REFERENCES,0.3726937269372694,"scheduling: State-of-the-art survey on algorithms and applications. International Journal of
391"
REFERENCES,0.3736162361623616,"Production Research, 56(1-2):193–223, 2018.
392"
REFERENCES,0.37453874538745385,"[27] Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, and Qingfu Zhang. Pareto set learning for expen-
393"
REFERENCES,0.37546125461254615,"sive multi-objective optimization. In Advances in Neural Information Processing Systems 35
394"
REFERENCES,0.3763837638376384,"(NeurIPS’22), pages 19231–19247, 2022.
395"
REFERENCES,0.3773062730627306,"[28] Zhuo Liu, Xiaolin Xiao, Feng-Feng Wei, and Wei-Neng Chen. A classiﬁcation-assisted level-
396"
REFERENCES,0.37822878228782286,"based learning evolutionary algorithm for expensive multiobjective optimization problems. In Pro-
397"
REFERENCES,0.37915129151291516,"ceedings of the 24th Annual Conference on Genetic and Evolutionary Computation (GECCO’22),
398"
REFERENCES,0.3800738007380074,"pages 547–555, 2022.
399"
REFERENCES,0.38099630996309963,"[29] Zhichao Lu, Ran Cheng, Yaochu Jin, Kay Chen Tan, and Kalyanmoy Deb. Neural architec-
400"
REFERENCES,0.38191881918819187,"ture search as multiobjective optimization benchmarks: Problem formulation and performance
401"
REFERENCES,0.3828413284132841,"assessment. IEEE Transactions on Evolutionary Computation (Early Access), 2023.
402"
REFERENCES,0.3837638376383764,"[30] Michael D. McKay, Richard J. Beckman, and William J. Conover. A comparison of three
403"
REFERENCES,0.38468634686346864,"methods for selecting values of input variables in the analysis of output from a computer code.
404"
REFERENCES,0.3856088560885609,"Technometrics, 42(1):55–61, 2000.
405"
REFERENCES,0.3865313653136531,"[31] Linqiang Pan, Cheng He, Ye Tian, Handing Wang, Xingyi Zhang, and Yaochu Jin.
A
406"
REFERENCES,0.3874538745387454,"classiﬁcation-based surrogate-assisted evolutionary algorithm for expensive many-objective opti-
407"
REFERENCES,0.38837638376383765,"mization. IEEE Transactions on Evolutionary Computation, 23(1):74–88, 2018.
408"
REFERENCES,0.3892988929889299,"[32] Jerome Sacks, William J. Welch, Toby J. Mitchell, and Henry P. Wynn. Design and analysis of
409"
REFERENCES,0.39022140221402213,"computer experiments. Statistical Science, 4(4):409–423, 1989.
410"
REFERENCES,0.39114391143911437,"[33] Palwasha W. Shaikh, Mohammed El-Abd, Mounib Khanafer, and Kaizhou Gao. A review on
411"
REFERENCES,0.39206642066420666,"swarm intelligence and evolutionary algorithms for solving the trafﬁc signal control problem.
412"
REFERENCES,0.3929889298892989,"IEEE Transactions on Intelligent Transportation Systems, 23(1):48–63, 2020.
413"
REFERENCES,0.39391143911439114,"[34] Zhenshou Song, Handing Wang, Cheng He, and Yaochu Jin. A Kriging-assisted two-archive
414"
REFERENCES,0.3948339483394834,"evolutionary algorithm for expensive many-objective optimization. IEEE Transactions on Evolu-
415"
REFERENCES,0.39575645756457567,"tionary Computation, 25(6):1013–1027, 2021.
416"
REFERENCES,0.3966789667896679,"[35] Lei Song, Ke Xue, Xiaobin Huang, and Chao Qian. Monte Carlo tree search based variable se-
417"
REFERENCES,0.39760147601476015,"lection for high dimensional Bayesian optimization. In Advances in Neural Information Processing
418"
REFERENCES,0.3985239852398524,"Systems 35 (NeurIPS’22), pages 28488–28501, 2022.
419"
REFERENCES,0.3994464944649446,"[36] Michael L. Stein. Interpolation of Spatial Data: Some Theory for Kriging. Springer Science &
420"
REFERENCES,0.4003690036900369,"Business Media, New York, NY, 1999.
421"
REFERENCES,0.40129151291512916,"[37] Shinya Suzuki, Shion Takeno, Tomoyuki Tamura, Kazuki Shitara, and Masayuki Karasuyama.
422"
REFERENCES,0.4022140221402214,"Multi-objective Bayesian optimization using Pareto-frontier entropy. In Proceedings of the 37th
423"
REFERENCES,0.40313653136531363,"International Conference on Machine Learning (ICML’20), pages 9279–9288, 2020.
424"
REFERENCES,0.4040590405904059,"[38] Zhenkun Wang, Yew-Soon Ong, and Hisao Ishibuchi. On scalable multiobjective test problems
425"
REFERENCES,0.40498154981549817,"with hardly dominated boundaries. IEEE Transactions on Evolutionary Computation, 23(2):217–
426"
REFERENCES,0.4059040590405904,"231, 2018.
427"
REFERENCES,0.40682656826568264,"[39] Christopher KI Williams and Carl Edward Rasmussen. Gaussian Processes for Machine
428"
REFERENCES,0.4077490774907749,"Learning. MIT press, Cambridge, MA, 2006.
429"
REFERENCES,0.4086715867158672,"[40] Xunzhao Yu, Xin Yao, Yan Wang, Ling Zhu, and Dimitar Filev. Domination-based ordinal
430"
REFERENCES,0.4095940959409594,"regression for expensive multi-objective optimization. In Proceedings of the 2019 IEEE Symposium
431"
REFERENCES,0.41051660516605165,"Series on Computational Intelligence (SSCI’19), pages 2058–2065, 2019.
432"
REFERENCES,0.4114391143911439,"[41] Xunzhao Yu, Ling Zhu, Yan Wang, Dimitar Filev, and Xin Yao. Internal combustion engine
433"
REFERENCES,0.4123616236162362,"calibration using optimization algorithms. Applied Energy, 305:117894, 2022.
434"
REFERENCES,0.4132841328413284,"[42] Yuan Yuan and Wolfgang Banzhaf. Expensive multi-objective evolutionary optimization assisted
435"
REFERENCES,0.41420664206642066,"by dominance prediction. IEEE Transactions on Evolutionary Computation, 26(1):159–173, 2022.
436"
REFERENCES,0.4151291512915129,"[43] Qingfu Zhang, Wudong Liu, Edward Tsang, and Botond Virginas. Expensive multiobjective
437"
REFERENCES,0.4160516605166052,"optimization by MOEA/D with gaussian process model. IEEE Transactions on Evolutionary
438"
REFERENCES,0.41697416974169743,"Computation, 14(3):456–474, 2010.
439"
REFERENCES,0.41789667896678967,"[44] Jinyuan Zhang, Aimin Zhou, and Guixu Zhang. A classiﬁcation and pareto domination based
440"
REFERENCES,0.4188191881918819,"multiobjective evolutionary algorithm. In Proceedings of the 17th IEEE Congress on Evolutionary
441"
REFERENCES,0.41974169741697415,"Computation (CEC’15), pages 2883–2890, 2015.
442"
REFERENCES,0.42066420664206644,"[45] Eckart Zitzler and Lothar Thiele. Multiobjective optimization using evolutionary algorithms - a
443"
REFERENCES,0.4215867158671587,"comparative case study. In Proceedings of the 5th International Conference on Parallel Problem
444"
REFERENCES,0.4225092250922509,"Solving from Nature (PPSN V), pages 292–301, 1998.
445"
REFERENCES,0.42343173431734316,"[46] Marcela Zuluaga, Andreas Krause, et al. ϵ-pal: An active learning approach to the multi-
446"
REFERENCES,0.42435424354243545,"objective optimization problem. Journal of Machine Learning Research, 17(104):1–32, 2016.
447"
REFERENCES,0.4252767527675277,"A
Background of Many-Objective Optimization
448"
REFERENCES,0.4261992619926199,"We consider minimization problems and many-objective optimization problems (MOOPs) can be
449"
REFERENCES,0.42712177121771217,"formulated as follows:
450"
REFERENCES,0.4280442804428044,"Deﬁnition 2. (Expensive Many-Objective Optimization Problem)
Given M expensive objective functions f1, . . . , fM and an evaluation budget FEmax, obtain the
Pareto set for the following many-objective optimization problem:"
REFERENCES,0.4289667896678967,"argmin
x∈X
f(x) = (f1(x), . . . , fM(x))"
REFERENCES,0.42988929889298894,"where X ⊆RD is the decision space of the problem.
451"
REFERENCES,0.4308118081180812,"The Pareto set is deﬁned through the following deﬁnitions: Pareto set and Pareto front are deﬁned as
452"
REFERENCES,0.4317343173431734,"follows:
453"
REFERENCES,0.4326568265682657,"Deﬁnition 3. Pareto dominance:
A solution x1 is said to dominate another solution x2 (denoted by x1 ≺x2) if and only if:"
REFERENCES,0.43357933579335795,"∀k ∈{1, 2, . . . , M} : fk(x1) ≤fk(x2)∧"
REFERENCES,0.4345018450184502,"∃k ∈{1, 2, . . . , M} : fk(x1) < fk(x2)
Deﬁnition 4. Non-dominated solution:
A non-dominated solution x⋆in the decision space X is a solution that cannot be dominated by any
other solutions in X:
∄x ∈X : x ≺x⋆"
REFERENCES,0.4354243542435424,"Deﬁnition 5. Pareto set:
Pareto set Sps is the set of all non-dominated solutions in the decision space X:"
REFERENCES,0.43634686346863466,Sps = {x⋆∈X|∄x ∈X : x ≺x⋆}
REFERENCES,0.43726937269372695,"Deﬁnition 6. Pareto front:
Pareto front Spf is the corresponding unique set of the Pareto set in the objective space:"
REFERENCES,0.4381918819188192,Spf = {f(x)|x ∈Sps}
REFERENCES,0.43911439114391143,"B
Kriging Model
454"
REFERENCES,0.44003690036900367,"Kriging model, also known as Gaussian process model [23] or design and analysis of computer
455"
REFERENCES,0.44095940959409596,"experiments (DACE) model [32], is a stochastic process model used to approximate an unknown
456"
REFERENCES,0.4418819188191882,"objective function. LORA-MOO uses Kriging models to implement angular surrogates and the radial
457"
REFERENCES,0.44280442804428044,"surrogate, to avoid potential confusion and help the understanding of our algorithm, the working
458"
REFERENCES,0.4437269372693727,"mechanism of the Kriging model is described below.
459"
REFERENCES,0.4446494464944649,"A common way to approximate an unknown objective function with n observations is linear regression:
460 461"
REFERENCES,0.4455719557195572,"y(xi) = N
X"
REFERENCES,0.44649446494464945,"k=1
βkfk(xi) + ϵi,
(6)"
REFERENCES,0.4474169741697417,"where xi is the ith sample point observed from the objective function. fk(xi), βk are a linear or
462"
REFERENCES,0.4483394833948339,"nonlinear function of xi and its coefﬁcient, respectively. N is the number of functions f(x). ϵi is an
463"
REFERENCES,0.4492619926199262,"independent error term, which is normally distributed with mean zero and variance σ2.
464"
REFERENCES,0.45018450184501846,"However, a stochastic process model such as Kriging does not assume that the error terms ϵ are
465"
REFERENCES,0.4511070110701107,"independent. Hence, an error term ϵi is rewritten as ϵ(xi). Moreover, these error terms are assumed
466"
REFERENCES,0.45202952029520294,"to be related or correlated to each other. The correlation between two error terms ϵ(xi) and ϵ(xj) is
467"
REFERENCES,0.45295202952029523,"inversely proportional to the distance between the corresponding points [23]. The correlation function
468"
REFERENCES,0.45387453874538747,"in the Kriging model is deﬁned as:
469"
REFERENCES,0.4547970479704797,"Corr(ϵ(xi), ϵ(xj)) = exp[−dis(xi, xj)],
(7)"
REFERENCES,0.45571955719557194,"where the distance between two points xi and xj are measured using the special weighted distance
470"
REFERENCES,0.4566420664206642,"formula shown below:
471"
REFERENCES,0.4575645756457565,"dis(xi, xj) = D
X"
REFERENCES,0.4584870848708487,"k=1
θi|xi
k −xj
k|pk,
(8)"
REFERENCES,0.45940959409594095,"where D is the number of decision variables, θθθ ∈RD
≥0 and p ∈[1, 2]D are parameters of the Kriging
472"
REFERENCES,0.4603321033210332,"model. It can be seen from Eq.(7) that the correlation is ranged within (0, 1] and is increasing as the
473"
REFERENCES,0.4612546125461255,"distance between two points decreases. Particularly, in Eq.(8), the parameter θk can be explained as
474"
REFERENCES,0.4621771217712177,"the importance of the decision variable xk, and the parameter pk can be interpreted as the smoothness
475"
REFERENCES,0.46309963099630996,"of the correlation function in the kth coordinate direction.
476"
REFERENCES,0.4640221402214022,"Due to the effectiveness of correlation modelling, the regression model in Eq.(6) can be simpliﬁed
477"
REFERENCES,0.46494464944649444,"without degrading modelling performance [23]. Clearly, all regression terms are replaced with a
478"
REFERENCES,0.46586715867158673,"constant term, thus the Kriging regression model can be rewritten as follows:
479"
REFERENCES,0.466789667896679,"y(xi) = µ + ϵ(xi),
(9)
where µ is the mean of this stochastic process, ϵ(xi) ∼N(0, σ2).
480"
REFERENCES,0.4677121771217712,"B.1
Training the Kriging model
481"
REFERENCES,0.46863468634686345,"To train the Kriging model and estimate the parameters θθθ, p in Eq.(8), the following likelihood
482"
REFERENCES,0.46955719557195574,"function is maximised:
483"
REFERENCES,0.470479704797048,"1
(2π)n/2(σ2)n/2|R|1/2 exp[−(y −1µ)T R−1(y −1µ)"
REFERENCES,0.4714022140221402,"2σ2
],
(10)"
REFERENCES,0.47232472324723246,"where |R| is the determinant of the correlation matrix, each element in the matrix is obtained using
484"
REFERENCES,0.4732472324723247,"Eq.(7). y is the n-dimensional vector of dependent variables that observed from the objective function.
485"
REFERENCES,0.474169741697417,"The mean value µ and variance σ2 in Eq.(9) and Eq.(10) can be estimated by:
486"
REFERENCES,0.47509225092250923,ˆµ = 1T R−1y
REFERENCES,0.47601476014760147,"1T R−11
,
(11) 487"
REFERENCES,0.4769372693726937,ˆσ = 1
REFERENCES,0.477859778597786,"n(y −1ˆµ)T R−1(y −1ˆµ).
(12)"
REFERENCES,0.47878228782287824,"B.2
Prediction with the Kriging model
488"
REFERENCES,0.4797047970479705,"For a new solution x∗, the Kriging model predicts the approximation of ˆy(x∗) and the uncertainty
489"
REFERENCES,0.4806273062730627,"ˆs2(x∗) as follows:
490"
REFERENCES,0.48154981549815495,"ˆy(x∗) = ˆµ + r′R−1(y −1ˆµ),
(13) 491"
REFERENCES,0.48247232472324725,"ˆs2(x∗) = ˆσ2(1 −r′R−1r),
(14)
where r is a n-dimensional vector of correlations between ϵ(x∗) and the error terms at the training
492"
REFERENCES,0.4833948339483395,"data, which can be calculated via Eq.(7).
493"
REFERENCES,0.4843173431734317,"Further details and a comprehensive description of the Kriging model and Gaussian Process can be
494"
REFERENCES,0.48523985239852396,"found in [39]. In this paper, all regression-based Kriging models have θθθ ∈[10−5, 100]D, p = 2D.
495"
REFERENCES,0.48616236162361626,"C
Additional Description of LORA-MOO
496"
REFERENCES,0.4870848708487085,"This section describes LORA-MOO with more details.
497"
REFERENCES,0.48800738007380073,"C.1
Quantiﬁcation of Ordinal Relations
498"
REFERENCES,0.488929889298893,"In order to learn the ordinal landscape of MOOPs, we need to quantify the ordinal relations between
499"
REFERENCES,0.48985239852398527,"solutions into numerical values. Alg. 2 illustrates the pseudocode of quantifying ordinal relations3,
500"
REFERENCES,0.4907749077490775,"it describes line 4 in Alg. 1 of the main ﬁle. It can be seen that Alg. 2 is mainly working on the
501"
REFERENCES,0.49169741697416974,"quantiﬁcation of dominance-based ordinal relations. Artiﬁcial ordinal relations will not be added
502"
REFERENCES,0.492619926199262,"unless the ratio of reference points is larger than ratio threshold rpratio (line 5).
503"
REFERENCES,0.4935424354243542,"An illustration of artiﬁcial clustering-based ordinal relations is given in Fig. 5. By using clustering
504"
REFERENCES,0.4944649446494465,"methods, artiﬁcial ordinal relations are generated for training ordinal regression surrogates. Picking
505"
REFERENCES,0.49538745387453875,"one solution from each cluster ensures the diversity of non-dominated solutions in the ﬁrst ordinal
506"
REFERENCES,0.496309963099631,"level L1. Meanwhile, the selection within each cluster is based on the projection length on cluster
507"
REFERENCES,0.49723247232472323,"center, which is beneﬁcial to the convergence of non-dominated solutions.
508"
REFERENCES,0.4981549815498155,"3Symbol ‘←’ indicates the result of a function, Symbol ‘=’ indicates an assignment operation."
REFERENCES,0.49907749077490776,"Algorithm 2 Quantify Ordinal Relations for LORA-MOO
Input:
SA: Archive of evaluated solutions;
rp_ratio: Ratio threshold of reference points in SA;
no: Minimal number of ordinal levels.
Procedure:"
REFERENCES,0.5,"1: SRP ←Non-dominated solutions in SA that are non-λ-dominated to any other solution in SA.
2: Non-dominated level (The ﬁrst ordinal level) L1 ←SRP .
3: The number of non-dominated ordinal levels nndl = 1.
4: Ratio of reference points ratio = |SRP |"
REFERENCES,0.5009225092250923,"|SA| .
5: if ratio > rpratio then
6:
nndl = nndl + 1.
/* Add Artiﬁcial Ordinal Relations. */
7:
Divide SRP into |SRP |"
REFERENCES,0.5018450184501845,"2
clusters via KNN clustering.
8:
For x in each cluster, calculate the projection length of x on the corresponding cluster center.
9:
L1 ←Solutions x with the shortest projection on each cluster.
10:
L2 ←Remaining |SRP |"
REFERENCES,0.5027675276752768,"2
solutions in SRP .
11: end if
12: Calculate extension coefﬁcient ec(x) for all x ∈SA."
REFERENCES,0.503690036900369,"13: The number of ordinal levels No = max(no, |SA|"
REFERENCES,0.5046125461254612,|SRP |).
REFERENCES,0.5055350553505535,"14: Li ←According to the order of ec(x), uniformly divide solutions x ∈(SA −SRP ) into No -
nndl levels.
15: Ordinal relation value vi = 1 −
i−1
No−1 for x ∈Li.
Output: An ordinal training set So consisting of ordinal relation values vi."
REFERENCES,0.5064575645756457,"Figure 5: Illustration of artiﬁcial clustering-based ordinal relations. Left: Non-dominated solutions
without artiﬁcial ordinal relations. Right: Non-dominated solutions with artiﬁcial ordinal relations.
Red solutions are new non-dominated solutions in L1, remaining blue solutions are moved to next
ordinal level L2. Dash circles are clusters, green vectors are cluster centers."
REFERENCES,0.507380073800738,"C.2
Generation of candidate solutions
509"
REFERENCES,0.5083025830258303,"Algo. 3 gives the pseudocode of generating candidate solutions, it is the implementation of line 6 in
510"
REFERENCES,0.5092250922509225,"Alg. 1 of the main ﬁle. In lines 1-9, a population P0 is generated. Since reference points SRP are the
511"
REFERENCES,0.5101476014760148,"optimal solutions in SA in terms of convergence, a half initial solutions are generated from SRP (lines
512"
REFERENCES,0.511070110701107,"2-8). To obtain a diverse subset of SRP , LORA-MOO divides SRP into nc clusters before sampling
513"
REFERENCES,0.5119926199261993,"solutions (line 2). Once population initialization is completed (line 9), a normal PSO is conducted to
514"
REFERENCES,0.5129151291512916,"produce candidate solutions (lines 11-16). Please be noted that, although we are solving expensive
515"
REFERENCES,0.5138376383763837,"MOOPs, only a single ordinal surrogate ho is used in the reproduction process (line 14). This is a
516"
REFERENCES,0.514760147601476,"great advantage of LORA-MOO since existing regression-based SAEAs involve all M surrogates in
517"
REFERENCES,0.5156826568265682,"the reproduction process. Hence, LORA-MOO is more efﬁcient than these regression-based SAEAs.
518"
REFERENCES,0.5166051660516605,"C.3
Angle-Based Diversity Selection
519"
REFERENCES,0.5175276752767528,"Alg. 4 gives the pseudocode of selecting the second optimal solution x∗
2 from P via our angle-based
520"
REFERENCES,0.518450184501845,"diversity criterion, it is the implementation of line 11 in Alg. 1 of the main ﬁle. This angle-based
521"
REFERENCES,0.5193726937269373,"Algorithm 3 Generation of candidate solutions in LORA-MOO
Input:
SRP : Reference points used in the ordinal regression;
ho: Ordinal regression surrogate;
nc: The number of clusters to initialize population P;
|P|: The size of population P;
Gmax: The number of generations for reproduction.
Procedure:"
REFERENCES,0.5202952029520295,1: Pr ←Randomly sample |P |
REFERENCES,0.5212177121771218,"2 solutions from the decision space.
2: Divide SRP into nc clusters via KNN clustering.
3: Pc = ∅.
4: for i = 1 to nc do
5:
Pci ←Randomly sample |P |"
REFERENCES,0.522140221402214,"2nc solutions from ith cluster.
6:
Pci ←Mutation ( Pci).
7:
Pc = Pc ∪Pci.
8: end for
9: Initial population P0 = Pr ∪Pc.
10: ho(P0) ←Evaluate P0 on ordinal surrogate ho.
11: Global Optimal Population Pglobal = P0.
12: for i = 1 to Gmax do
13:
Pi ←PSO operation on Pi−1 and Pglobal.
14:
ho(Pi) ←Evaluate Pi on ordinal surrogate ho.
15:
Update Pglobal using ho(Pi) and ho(Pi−1).
16: end for
Output: A generation of candidate solutions P = Pglobal."
REFERENCES,0.5230627306273062,"Algorithm 4 Angle-Based Diversity Selection in LORA-MOO
Input:
SRP : Reference points used in the ordinal regression;
P: Population of candidate solutions;
ha1, . . . , ha(M−1): M-1 angular surrogates;"
REFERENCES,0.5239852398523985,Procedure:
REFERENCES,0.5249077490774908,"1: h(ai)(P) ←Evaluate P on angular surrogates hai, i = 1, . . . , M −1.
2: for j = 2 to |P| do
3:
xj ←The jth solution in P. /* Assume the ﬁrst solution in P is selected as x∗
1 already. */
4:
dang ←Calculate the angles between xj and all reference points in SRP .
5:
mdang ←The angle between xj and its nearest reference point.
6: end for
7: x∗
2 ←The candidate solution in P with maximal mdang.
Output: The second candidate solution x∗
2."
REFERENCES,0.525830258302583,"diversity selection does not require extra parameters for generating guidance vectors, it selects the
522"
REFERENCES,0.5267527675276753,"candidate solution that is mostly deviate from solutions in SRP . Note that all angular surrogates are
523"
REFERENCES,0.5276752767527675,"only used to evaluate one population P during the whole reproduction and environmental selection
524"
REFERENCES,0.5285977859778598,"procedures. Therefore, although LORA-MOO ﬁts M surrogates in total (one ordinal surrogate and
525"
REFERENCES,0.5295202952029521,"M-1 angular surrogates), its runtime cost is less than other SAEAs which ﬁt M surrogates from
526"
REFERENCES,0.5304428044280443,"Cartesian coordinates.
527"
REFERENCES,0.5313653136531366,"D
Details of Performance Indicators Used in Our Experiments
528"
REFERENCES,0.5322878228782287,"In our experiments, we use IGD [4], IGD+ [20], and HV [45] to measure the performance of many
529"
REFERENCES,0.533210332103321,"objective optimization. Both IGD and IGD+ require a subset of Pareto front as reference points. In
530"
REFERENCES,0.5341328413284133,"our experiments, the number of IGD/IGD+ reference points is set to 5000 for 3-, 4-, and 6-objective
531"
REFERENCES,0.5350553505535055,"optimization problems, as widely used in the literature [40]. Considering the large objective space,
532"
REFERENCES,0.5359778597785978,Table 2: The HV reference points for all problems in this work.
REFERENCES,0.5369003690036901,"Problem
Reference Points
DTLZ
(1,0, . . . , 1.0) ∈RM"
REFERENCES,0.5378228782287823,"WFG
(1,0, . . . , 1.0) ∈RM
NASBench201
(1.0, 1.0, 1.0, 1.0, 1.0)"
REFERENCES,0.5387453874538746,"we set the number of IGD/IGD+ reference points to 10000 for 8- and 10-objective optimization
533"
REFERENCES,0.5396678966789668,"problems to achieve a more accurate estimation of optimization performance. The method proposed
534"
REFERENCES,0.540590405904059,"in [25] is employed to generate well-distributed IGD/IGD+ reference points.
535"
REFERENCES,0.5415129151291513,"In comparison, the calculation of HV values does not require a subset of Pareto front as reference
536"
REFERENCES,0.5424354243542435,"points. For a set of non-dominated solutions, its HV is the volume in the objective space it dominates
537"
REFERENCES,0.5433579335793358,"from the set to a single reference point. Table 2 lists the reference point used for calculating HV
538"
REFERENCES,0.544280442804428,"values. All HV values are calculated using the reference point and the normalized solutions. A
539"
REFERENCES,0.5452029520295203,"solution x is normalize by the upper bound and lower bound of Pareto front:
540"
REFERENCES,0.5461254612546126,"x −lbpf
ubpf −lbpf
,
(15)"
REFERENCES,0.5470479704797048,"where ubpf, lbpf are the upper bound and lower bound of Pareto front, respectively.
541"
REFERENCES,0.5479704797047971,"E
Details of the NASbench201 Problem
542"
REFERENCES,0.5488929889298892,"NASbench201 [12] are discrete optimization problems that aim to identify the optimal architecture
543"
REFERENCES,0.5498154981549815,"for neural networks. The search space is deﬁned by a cell with 4 nodes inside, forming a directed
544"
REFERENCES,0.5507380073800738,"acyclic graph as illustrated in Fig. 6. The decision variables are 6 edges, each edge is associated"
REFERENCES,0.551660516605166,Figure 6: Diagram of a network architecture in NASbench201. 545
REFERENCES,0.5525830258302583,"with an operation selected from a predeﬁned operation set {zeroize, skip-connect, 1x1 convolution,
546"
REFERENCES,0.5535055350553506,"3x3 convolution, 3x3 average pool}. Therefore, a network architecture can be encoded into a 6-
547"
REFERENCES,0.5544280442804428,"dimensional decision vector with 5 discrete numbers. In total, there are 56=15,625 different candidates
548"
REFERENCES,0.5553505535055351,"for neural architecture search.
549"
REFERENCES,0.5562730627306273,"The optimization objectives in NASbench201 varies in different optimization problems. In this
550"
REFERENCES,0.5571955719557196,"paper, our NASbench201 problem consider 5 objectives, including the accuracy in CI-FAR10 dataset,
551"
REFERENCES,0.5581180811808119,"groundtruth ﬂoating point operations (FLOPs), the number of parameters, latency, and energy cost.
552"
REFERENCES,0.559040590405904,"All these objectives are normalized to [0, 1] in the optimization. The optimization problem can be
553"
REFERENCES,0.5599630996309963,"formulated as
554"
REFERENCES,0.5608856088560885,"F(x) = {facc(x), fF LOP s(x), fparam(x), flatency(x), fenergy(x)},
(16)"
REFERENCES,0.5618081180811808,"where decision vector x ∈{0, 1, 2, 3, 4}6.
555"
REFERENCES,0.5627306273062731,"F
Complete Results of Ablation Studies
556"
REFERENCES,0.5636531365313653,"In this section, we report complete results of our ablation studies that are not displayed in the main
557"
REFERENCES,0.5645756457564576,"paper. We conduct four ablation studies to investigate the effect of the following four parameters on
558"
REFERENCES,0.5654981549815498,"the optimization performance of LORA-MOO.
559"
REFERENCES,0.566420664206642,"1. no: The minimal number of ordinal levels. A parameter in the modeling of our ordinal-
560"
REFERENCES,0.5673431734317343,"regression-based surrogate ho.
561"
REFERENCES,0.5682656826568265,"2. λ: The dominance coefﬁcient. A parameter in the modeling of our ordinal-regression-based
562"
REFERENCES,0.5691881918819188,"surrogate ho.
563"
REFERENCES,0.5701107011070111,"3. rpratio: The ratio threshold of reference points SRP . A parameter to determine whether to
564"
REFERENCES,0.5710332103321033,"introduce artiﬁcial ordinal relations via clustering.
565"
REFERENCES,0.5719557195571956,"4. nc: The number of clusters generated from reference points SRP to initialize PSO population.
566"
REFERENCES,0.5728782287822878,"A parameter in the generation of candidate solutions.
567"
REFERENCES,0.5738007380073801,"Setup of Ablation Studies. Our ablation studies are conducted on 7 DTLZ and 9 WFG benchmark
568"
REFERENCES,0.5747232472324724,"optimization problems. These benchmark problems have different features, such as unimodal, multi-
569"
REFERENCES,0.5756457564575646,"modal, scaled, degenerated, and discontinuous. Therefore, the effect of four parameters can be
570"
REFERENCES,0.5765682656826568,"investigated comprehensively. Considering our paper focuses on many-objective optimization instead
571"
REFERENCES,0.577490774907749,"of scalable optimization, we are interested in the optimization performance under different numbers
572"
REFERENCES,0.5784132841328413,"of objectives M rather than the performance under different numbers of decision variables D. Hence,
573"
REFERENCES,0.5793357933579336,"we set D = 10 for all benchmark optimization problems, as suggested in literature [5, 31, 34, 17]. In
574"
REFERENCES,0.5802583025830258,"comparison, we set M = {3, 6, 10} to observe the optimization performance with different objectives.
575"
REFERENCES,0.5811808118081181,"Other setups are the same as described in Section 4.1 of the main ﬁle.
576"
REFERENCES,0.5821033210332104,"F.1
Inﬂuence of Minimal Number of Ordinal Levels no.
577"
REFERENCES,0.5830258302583026,"This subsection investigates the inﬂuence of minimal number of ordinal levels no on the optimization
578"
REFERENCES,0.5839483394833949,"performance. We set no = {10, 8, 6, 4, 3} to generate ﬁve LORA-MOO variants. For all variants, in
579"
REFERENCES,0.584870848708487,"this ablation study, we tentatively set λ = 0.2, rpratio = 2/3, nc = 5 for a fair comparison. The IGD+
580"
REFERENCES,0.5857933579335793,"values obtained by ﬁve LORA-MOO variants with different no are reported in Table 3.
581"
REFERENCES,0.5867158671586716,"In the last ﬁve rows of Table 3, the summary of statistical test results shows that no = 4 is the optimal
582"
REFERENCES,0.5876383763837638,"parameter setup for LORA-MOO, because it is the only variant that is signiﬁcantly superior to or
583"
REFERENCES,0.5885608856088561,"equivalent to all other variants. In comparison, the LORA-MOO variant with no = 10, 8, 6, 3 are
584"
REFERENCES,0.5894833948339483,"signiﬁcantly inferior to other 4, 1, 1, 2 LORA-MOO variants, respectively.
585"
REFERENCES,0.5904059040590406,"F.2
Inﬂuence of Dominance Coefﬁcient λ.
586"
REFERENCES,0.5913284132841329,"In this subsection, we analyze the inﬂuence of λ-dominance coefﬁcient λ on the optimization
587"
REFERENCES,0.5922509225092251,"performance. We set λ = {0, 0.1, 0.2, 0.3} to generate four LORA-MOO variants. As determined in
588"
REFERENCES,0.5931734317343174,"the previous ablation study, we set no = 4 for all variants. The remaining two parameters rpratio and
589"
REFERENCES,0.5940959409594095,"nc are set to 2/3 and 5, respectively. The IGD+ values obtained by four LORA-MOO variants with
590"
REFERENCES,0.5950184501845018,"different λ are reported in Table 4.
591"
REFERENCES,0.5959409594095941,"The last four rows of Table 4 shows that λ = 0.2 is the optimal parameter setup for LORA-MOO.
592"
REFERENCES,0.5968634686346863,"The variant of λ = 0.2 is signiﬁcantly superior to both the variants of λ = 0 and λ = 0.1, and it is
593"
REFERENCES,0.5977859778597786,"equivalent to the variant of λ = 0.3. We note that the variant of λ = 0.3 is also signiﬁcantly superior
594"
REFERENCES,0.5987084870848709,"to both the variants of λ = 0 and λ = 0.1. However, this variant wins/ties/losses 30/105/9 statistical
595"
REFERENCES,0.5996309963099631,"tests in total, while the variant of λ = 0.2 wins/ties/losses 32/109/3 statistical tests in total. Therefore,
596"
REFERENCES,0.6005535055350554,"setting λ = 0.2 is preferable to setting λ = 0.3.
597"
REFERENCES,0.6014760147601476,"Note that all other LORA-MOO variants outperform the variant of λ = 0, this implies that excluding
598"
REFERENCES,0.6023985239852399,"some samples from the set of non-dominated solutions is beneﬁcial to the performance of ordinal
599"
REFERENCES,0.6033210332103321,"regression. The effectiveness of using our λ-dominance approach in LORA-MOO is demonstrated.
600"
REFERENCES,0.6042435424354243,"F.3
Inﬂuence of Ratio Threshold rpratio.
601"
REFERENCES,0.6051660516605166,"In this subsection, we investigate the inﬂuence of ratio threshold rpratio on the optimization perfor-
602"
REFERENCES,0.6060885608856088,"mance. rpratio is the threshold to determine when to add artiﬁcial ordinal relations for the training
603"
REFERENCES,0.6070110701107011,"of ordinal surrogate ho. We set rpratio = {1, 2/3, 1/2, 1/3} to generate four LORA-MOO variants.
604"
REFERENCES,0.6079335793357934,"For all variants, we set no, λ to 4, 0.2, respectively, which are consistent with our conclusions in
605"
REFERENCES,0.6088560885608856,"previous ablation studies. Parameter nc is tentatively set to 5. The IGD+ values obtained by four
606"
REFERENCES,0.6097785977859779,"LORA-MOO variants with different rpratio are reported in Table 5. It should be noted that, when the
607"
REFERENCES,0.6107011070110702,"number of objectives M = 3, the results of rpratio = 1 are the same as the results of rpratio = 2/3,
608"
REFERENCES,0.6116236162361623,"because the ratio of reference points in archive SA is always lower than 2/3. Consequently, when M
609"
REFERENCES,0.6125461254612546,"= 3, setting ratio threshold rpratio to either 1 or 2/3 makes no difference to the optimization process
610"
REFERENCES,0.6134686346863468,"of LORA-MOO. Similarly, the results of rpratio = 1/3 on some problems are the same as the results
611"
REFERENCES,0.6143911439114391,"Table 3: Statistical results of the IGD+ value obtained by LORA-MOO with different no on 48
benchmark optimization problems over 15 runs. The last ﬁve rows count the total results of Wilcoxon
rank sum tests (signiﬁcance level is 0.05). ‘+’, ‘≈’, and ‘−’ denote the corresponding LORA-MOO
variant is statistically signiﬁcantly superior to, almost equivalent to, and inferior to the compared
variants in Wilcoxon tests, respectively."
REFERENCES,0.6153136531365314,"Problems
M
no=10
no=8
no=6
no=4
no=3
DTLZ1
3
4.63e+1(1.60e+1)
4.64e+1(1.23e+1)
5.61e+1(2.04e+1)
4.84e+1(1.34e+1)
4.58e+1(1.85e+1)
6
1.35e+1(7.10e+0)
1.77e+1(5.08e+0)
1.87e+1(6.85e+0)
1.64e+1(3.24e+0)
1.50e+1(7.84e+0)
10
1.56e-1(3.58e-2)
1.60e-1(3.60e-2)
1.63e-1(6.95e-2)
1.60e-1(2.67e-2)
1.63e-1(3.51e-2)
DTLZ2
3
4.50e-2(3.90e-3)
4.54e-2(4.16e-3)
4.38e-2(2.61e-3)
4.45e-2(4.72e-3)
4.39e-2(3.88e-3)
6
2.67e-1(1.47e-2)
2.73e-1(1.93e-2)
2.64e-1(1.67e-2)
2.57e-1(1.91e-2)
2.51e-1(2.20e-2)
10
3.04e-1(1.55e-2)
2.97e-1(1.63e-2)
2.94e-1(1.24e-2)
3.00e-1(1.31e-2)
3.11e-1(1.78e-2)
DTLZ3
3
1.50e+2(4.72e+1)
1.60e+2(4.92e+1)
1.55e+2(5.03e+1)
1.48e+2(4.92e+1)
1.45e+2(4.10e+1)
6
5.43e+1(1.85e+1)
5.65e+1(1.99e+1)
6.92e+1(2.39e+1)
6.68e+1(1.64e+1)
6.24e+1(2.34e+1)
10
4.51e-1(4.40e-2)
4.68e-1(6.10e-2)
4.35e-1(3.71e-2)
4.72e-1(5.45e-2)
4.85e-1(7.87e-2)
DTLZ4
3
1.03e-1(1.28e-1)
8.77e-2(1.30e-1)
9.16e-2(1.25e-1)
1.05e-1(1.27e-1)
1.15e-1(1.33e-1)
6
1.74e-1(3.63e-2)
1.60e-1(3.35e-2)
1.84e-1(3.79e-2)
1.75e-1(3.57e-2)
1.68e-1(2.11e-2)
10
2.29e-1(1.05e-2)
2.29e-1(9.43e-3)
2.36e-1(1.27e-2)
2.38e-1(1.35e-2)
2.42e-1(1.71e-2)
DTLZ5
3
8.65e-3(1.39e-3)
8.76e-3(1.53e-3)
9.03e-3(1.67e-3)
9.26e-3(1.22e-3)
9.26e-3(2.23e-3)
6
3.43e-2(7.07e-3)
3.28e-2(7.74e-3)
3.24e-2(7.73e-3)
3.25e-2(8.25e-3)
3.33e-2(9.38e-3)
10
4.06e-3(6.52e-4)
3.99e-3(4.47e-4)
3.94e-3(4.04e-4)
3.97e-3(9.34e-4)
4.02e-3(1.10e-3)
DTLZ6
3
5.09e-2(5.72e-2)
1.05e-1(2.57e-1)
2.45e-2(8.80e-3)
4.67e-2(4.92e-2)
3.12e-2(1.58e-2)
6
9.45e-1(1.13e+0)
5.16e-1(6.72e-1)
5.42e-1(8.28e-1)
7.52e-1(9.50e-1)
1.34e+0(1.04e+0)
10
4.48e-2(3.90e-2)
2.50e-2(7.37e-3)
5.14e-2(4.26e-2)
4.18e-2(4.66e-2)
4.72e-2(4.57e-2)
DTLZ7
3
1.19e-1(1.00e-1)
9.47e-2(1.15e-1)
1.16e-1(7.80e-2)
1.61e-1(2.77e-1)
1.46e-1(1.27e-1)
6
1.90e+0(9.89e-1)
1.72e+0(6.52e-1)
1.77e+0(7.63e-1)
1.25e+0(4.72e-1)
1.54e+0(8.80e-1)
10
1.19e+0(9.00e-2)
1.18e+0(9.13e-2)
1.17e+0(8.41e-2)
1.17e+0(8.97e-2)
1.22e+0(1.13e-1)
WFG1
3
1.65e+0(5.78e-2)
1.65e+0(3.73e-2)
1.64e+0(3.86e-2)
1.67e+0(4.67e-2)
1.65e+0(5.96e-2)
6
2.24e+0(5.47e-2)
2.20e+0(6.93e-2)
2.23e+0(4.37e-2)
2.22e+0(6.80e-2)
2.21e+0(5.52e-2)
10
2.62e+0(8.72e-2)
2.58e+0(7.39e-2)
2.59e+0(7.81e-2)
2.62e+0(8.93e-2)
2.58e+0(1.16e-1)
WFG2
3
2.39e-1(3.16e-2)
2.49e-1(4.94e-2)
2.68e-1(4.81e-2)
2.52e-1(4.94e-2)
2.66e-1(4.58e-2)
6
5.91e-1(1.79e-1)
5.85e-1(9.10e-2)
5.61e-1(1.29e-1)
5.43e-1(1.51e-1)
5.67e-1(1.07e-1)
10
1.50e+0(3.53e-1)
1.41e+0(2.62e-1)
1.42e+0(3.21e-1)
1.47e+0(4.49e-1)
1.39e+0(2.82e-1)
WFG3
3
2.42e-1(4.10e-2)
2.66e-1(3.75e-2)
2.57e-1(3.28e-2)
2.41e-1(3.21e-2)
2.56e-1(5.04e-2)
6
6.19e-1(8.08e-2)
6.28e-1(6.58e-2)
6.15e-1(9.32e-2)
5.92e-1(7.43e-2)
6.19e-1(1.22e-1)
10
6.24e-1(9.78e-2)
6.07e-1(8.67e-2)
6.18e-1(8.74e-2)
6.60e-1(8.00e-2)
6.61e-1(8.80e-2)
WFG4
3
2.62e-1(5.18e-2)
2.52e-1(1.99e-2)
2.51e-1(1.27e-2)
2.48e-1(1.04e-2)
2.38e-1(8.69e-3)
6
1.41e+0(2.17e-1)
1.34e+0(1.96e-1)
1.27e+0(2.31e-1)
1.30e+0(2.41e-1)
1.58e+0(4.08e-1)
10
4.12e+0(5.64e-1)
3.63e+0(6.43e-1)
3.55e+0(5.77e-1)
3.99e+0(7.21e-1)
4.08e+0(7.57e-1)
WFG5
3
2.93e-1(4.46e-2)
2.89e-1(5.58e-2)
3.01e-1(9.11e-2)
3.10e-1(5.46e-2)
3.19e-1(9.97e-2)
6
1.69e+0(8.33e-2)
1.72e+0(8.16e-2)
1.66e+0(9.57e-2)
1.69e+0(1.53e-1)
1.83e+0(1.34e-1)
10
4.76e+0(2.87e-1)
4.57e+0(3.19e-1)
4.10e+0(3.07e-1)
3.71e+0(3.87e-1)
3.71e+0(4.39e-1)
WFG6
3
4.66e-1(4.13e-2)
4.91e-1(4.44e-2)
4.51e-1(4.36e-2)
4.76e-1(6.61e-2)
4.58e-1(8.29e-2)
6
1.70e+0(1.48e-1)
1.65e+0(9.89e-2)
1.61e+0(1.10e-1)
1.67e+0(1.35e-1)
1.81e+0(2.71e-1)
10
3.88e+0(6.68e-1)
3.60e+0(3.51e-1)
3.64e+0(2.96e-1)
3.45e+0(4.44e-1)
3.72e+0(5.21e-1)
WFG7
3
3.12e-1(2.16e-2)
3.02e-1(2.17e-2)
3.00e-1(2.68e-2)
3.02e-1(2.75e-2)
2.99e-1(2.96e-2)
6
1.78e+0(1.05e-1)
1.69e+0(1.27e-1)
1.73e+0(1.38e-1)
1.67e+0(1.85e-1)
1.74e+0(2.32e-1)
10
5.15e+0(3.94e-1)
5.11e+0(2.97e-1)
4.89e+0(2.62e-1)
4.97e+0(3.07e-1)
4.94e+0(4.00e-1)
WFG8
3
5.84e-1(5.34e-2)
6.09e-1(5.54e-2)
6.07e-1(4.89e-2)
5.68e-1(4.78e-2)
5.70e-1(4.15e-2)
6
2.19e+0(1.08e-1)
2.11e+0(9.97e-2)
2.15e+0(1.22e-1)
2.25e+0(1.12e-1)
2.37e+0(1.76e-1)
10
5.22e+0(4.43e-1)
5.31e+0(3.08e-1)
4.99e+0(3.75e-1)
5.16e+0(5.37e-1)
5.37e+0(4.82e-1)
WFG9
3
3.79e-1(7.28e-2)
3.85e-1(1.20e-1)
3.73e-1(8.90e-2)
4.12e-1(1.17e-1)
4.17e-1(1.11e-1)
6
1.87e+0(1.95e-1)
1.73e+0(2.02e-1)
1.78e+0(2.45e-1)
1.77e+0(2.57e-1)
1.76e+0(1.35e-1)
10
5.03e+0(2.28e-1)
4.63e+0(4.11e-1)
4.44e+0(4.68e-1)
3.96e+0(3.83e-1)
3.73e+0(2.50e-1)
+/ ≈/−
no=10
-/-/-
1/41/6
2/40/6
0/44/4
3/41/4
+/ ≈/−
no=8
6/41/1
-/-/-
2/43/3
3/42/3
4/40/4
+/ ≈/−
no=6
6/40/2
3/43/2
-/-/-
3/41/4
7/38/3
+/ ≈/−
no=4
4/44/0
3/42/3
4/41/3
-/-/-
2/45/1
+/ ≈/−
no=3
4/41/3
4/40/4
3/38/7
1/45/2
-/-/-"
REFERENCES,0.6162361623616236,"obtained by setting rpratio to 1/2, because on these problems, the ratio of reference points in SA is
612"
REFERENCES,0.6171586715867159,"always higher than 1/2.
613"
REFERENCES,0.6180811808118081,"As shown in Table 5, the variant of rpratio = 1/2 outperforms other variants and achieves the optimal
614"
REFERENCES,0.6190036900369004,"behavior. Therefore, we set rpratio = 1/2 for LORA-MOO. In comparison, the variants of rpratio
615"
REFERENCES,0.6199261992619927,"= 2/3 and rpratio = 1/3 have competitive performance, both of them are inferior to the variant of
616"
REFERENCES,0.6208487084870848,"rpratio = 1/2 but signiﬁcantly superior to the variant of rpratio = 1.
617"
REFERENCES,0.6217712177121771,"Setting rpratio = 1 indicates this LORA-MOO variant will never introduce artiﬁcial ordinal relations
618"
REFERENCES,0.6226937269372693,"for the learning of the ordinal surrogate. The ordinal surrogate in this variant is trained completely on
619"
REFERENCES,0.6236162361623616,"Table 4: Statistical results of the IGD+ value obtained by LORA-MOO with different λ on 48
benchmark optimization problems over 15 runs. The last four rows count the total results of Wilcoxon
rank sum tests (signiﬁcance level is 0.05). ‘+’, ‘≈’, and ‘−’ denote the corresponding LORA-MOO
variant is statistically signiﬁcantly superior to, almost equivalent to, and inferior to the compared
variants in Wilcoxon tests, respectively."
REFERENCES,0.6245387453874539,"Problems
M
λ = 0
λ = 0.1
λ = 0.2
λ = 0.3
DTLZ1
3
7.51e+1(1.74e+1)
6.88e+1(1.28e+1)
4.84e+1(1.34e+1)
4.96e+1(1.56e+1)
6
2.74e+1(5.30e+0)
1.73e+1(3.80e+0)
1.64e+1(3.24e+0)
1.41e+1(7.02e+0)
10
1.62e-1(5.15e-2)
1.43e-1(2.33e-2)
1.60e-1(2.67e-2)
1.53e-1(2.28e-2)
DTLZ2
3
4.95e-2(3.32e-3)
4.89e-2(5.80e-3)
4.45e-2(4.72e-3)
4.81e-2(4.10e-3)
6
2.51e-1(2.91e-2)
2.56e-1(2.48e-2)
2.57e-1(1.91e-2)
2.67e-1(1.34e-2)
10
2.97e-1(1.72e-2)
2.94e-1(1.54e-2)
3.00e-1(1.31e-2)
2.92e-1(1.35e-2)
DTLZ3
3
1.91e+2(6.02e+1)
1.80e+2(2.31e+1)
1.48e+2(4.92e+1)
1.57e+2(4.54e+1)
6
9.01e+1(3.13e+1)
8.06e+1(2.18e+1)
6.68e+1(1.64e+1)
6.05e+1(2.03e+1)
10
5.74e-1(2.57e-1)
4.60e-1(5.69e-2)
4.72e-1(5.45e-2)
4.48e-1(4.14e-2)
DTLZ4
3
9.37e-2(1.30e-1)
1.16e-1(1.35e-1)
1.05e-1(1.27e-1)
1.02e-1(1.28e-1)
6
1.72e-1(2.91e-2)
1.63e-1(3.51e-2)
1.75e-1(3.57e-2)
1.61e-1(1.96e-2)
10
2.36e-1(1.29e-2)
2.37e-1(1.77e-2)
2.38e-1(1.35e-2)
2.28e-1(1.05e-2)
DTLZ5
3
1.40e-2(2.50e-3)
1.13e-2(3.34e-3)
9.26e-3(1.22e-3)
7.96e-3(1.58e-3)
6
5.00e-2(9.20e-3)
4.52e-2(1.60e-2)
3.25e-2(8.25e-3)
3.48e-2(5.12e-3)
10
5.16e-3(9.20e-4)
4.44e-3(1.43e-3)
3.97e-3(9.34e-4)
4.10e-3(3.97e-4)
DTLZ6
3
1.54e-1(1.65e-1)
4.14e-2(1.61e-2)
4.67e-2(4.92e-2)
4.13e-2(2.30e-2)
6
1.72e+0(7.66e-1)
1.52e+0(1.08e+0)
7.52e-1(9.50e-1)
2.45e-1(4.79e-1)
10
9.60e-2(7.76e-2)
6.08e-2(5.26e-2)
4.18e-2(4.66e-2)
2.99e-2(9.13e-3)
DTLZ7
3
6.57e-2(1.85e-2)
1.25e-1(1.06e-1)
1.61e-1(2.77e-1)
1.05e-1(1.80e-1)
6
2.74e+0(1.22e+0)
1.53e+0(8.21e-1)
1.25e+0(4.72e-1)
1.66e+0(1.06e+0)
10
1.19e+0(9.70e-2)
1.18e+0(8.58e-2)
1.17e+0(8.97e-2)
1.27e+0(1.61e-1)
WFG1
3
1.74e+0(4.92e-2)
1.67e+0(4.82e-2)
1.67e+0(4.67e-2)
1.64e+0(3.52e-2)
6
2.30e+0(3.54e-2)
2.22e+0(8.09e-2)
2.22e+0(6.80e-2)
2.23e+0(7.54e-2)
10
2.71e+0(6.98e-2)
2.63e+0(7.80e-2)
2.62e+0(8.93e-2)
2.63e+0(7.71e-2)
WFG2
3
2.94e-1(5.47e-2)
2.69e-1(5.46e-2)
2.52e-1(4.94e-2)
2.55e-1(3.46e-2)
6
6.84e-1(1.47e-1)
5.38e-1(1.05e-1)
5.43e-1(1.51e-1)
6.65e-1(2.55e-1)
10
1.67e+0(5.02e-1)
1.27e+0(2.80e-1)
1.47e+0(4.49e-1)
1.37e+0(3.46e-1)
WFG3
3
4.08e-1(4.84e-2)
3.25e-1(3.53e-2)
2.41e-1(3.21e-2)
2.70e-1(5.19e-2)
6
8.23e-1(6.96e-2)
7.51e-1(9.15e-2)
5.92e-1(7.43e-2)
4.94e-1(6.55e-2)
10
7.58e-1(7.71e-2)
7.71e-1(1.08e-1)
6.60e-1(8.00e-2)
6.35e-1(1.04e-1)
WFG4
3
2.55e-1(1.63e-2)
2.56e-1(1.48e-2)
2.48e-1(1.04e-2)
2.57e-1(1.44e-2)
6
1.28e+0(2.24e-1)
1.31e+0(2.39e-1)
1.30e+0(2.41e-1)
1.37e+0(2.50e-1)
10
3.85e+0(5.45e-1)
3.84e+0(5.48e-1)
3.99e+0(7.21e-1)
3.79e+0(4.91e-1)
WFG5
3
3.84e-1(1.18e-1)
2.89e-1(6.47e-2)
3.10e-1(5.46e-2)
3.11e-1(6.94e-2)
6
1.77e+0(1.36e-1)
1.72e+0(1.43e-1)
1.69e+0(1.53e-1)
1.72e+0(1.20e-1)
10
3.70e+0(4.80e-1)
3.58e+0(2.79e-1)
3.71e+0(3.87e-1)
4.38e+0(2.67e-1)
WFG6
3
4.78e-1(7.23e-2)
4.63e-1(5.50e-2)
4.76e-1(6.61e-2)
4.74e-1(4.87e-2)
6
1.62e+0(1.67e-1)
1.59e+0(1.21e-1)
1.67e+0(1.35e-1)
1.60e+0(1.52e-1)
10
3.48e+0(2.80e-1)
3.43e+0(3.18e-1)
3.45e+0(4.44e-1)
3.70e+0(3.85e-1)
WFG7
3
3.16e-1(2.20e-2)
3.13e-1(3.79e-2)
3.02e-1(2.75e-2)
3.17e-1(4.42e-2)
6
1.62e+0(1.57e-1)
1.68e+0(1.80e-1)
1.67e+0(1.85e-1)
1.69e+0(1.88e-1)
10
4.88e+0(4.14e-1)
4.99e+0(3.94e-1)
4.97e+0(3.07e-1)
4.98e+0(2.87e-1)
WFG8
3
5.96e-1(4.58e-2)
6.09e-1(3.63e-2)
5.68e-1(4.78e-2)
5.96e-1(3.58e-2)
6
2.21e+0(1.49e-1)
2.20e+0(1.18e-1)
2.25e+0(1.12e-1)
2.20e+0(7.76e-2)
10
5.07e+0(4.48e-1)
4.96e+0(4.84e-1)
5.16e+0(5.37e-1)
5.09e+0(3.92e-1)
WFG9
3
3.72e-1(3.91e-2)
3.82e-1(9.02e-2)
4.12e-1(1.17e-1)
3.80e-1(1.00e-1)
6
1.76e+0(2.07e-1)
1.67e+0(1.86e-1)
1.77e+0(2.57e-1)
1.81e+0(1.69e-1)
10
3.87e+0(3.66e-1)
4.13e+0(3.55e-1)
3.96e+0(3.83e-1)
4.76e+0(2.31e-1)
+/ ≈/−
λ=0
-/-/-
0/35/13
0/29/19
3/27/18
+/ ≈/−
λ=0.1
13/35/0
-/-/-
0/38/10
3/36/9
+/ ≈/−
λ=0.2
19/29/0
10/38/0
-/-/-
3/42/3
+/ ≈/−
λ=0.3
18/27/3
9/36/3
3/42/3
-/-/-"
REFERENCES,0.6254612546125461,"the basis of dominance ordinal relations. When the number of objectives M is large, a majority of
620"
REFERENCES,0.6263837638376384,"evaluated solutions in archive SA are non-dominated, leading to a large ratio of reference points SRP
621"
REFERENCES,0.6273062730627307,"in SA. As a result, there would be a signiﬁcant imbalance between the number of evaluated solutions
622"
REFERENCES,0.6282287822878229,"in each ordinal level, which causes a poor performance on ordinal surrogate and LORA-MOO. In
623"
REFERENCES,0.6291512915129152,"particular, on most 10-objective WFG problems, the variant of rpratio = 1 performs worse than all
624"
REFERENCES,0.6300738007380073,"other variants. This observation shows the detrimental effect of imbalance solutions in ordinal levels
625"
REFERENCES,0.6309963099630996,"on the optimization performance, which also demonstrates the effectiveness of using artiﬁcial ordinal
626"
REFERENCES,0.6319188191881919,"relations in LORA-MOO to address many-objective optimization problems.
627"
REFERENCES,0.6328413284132841,"Table 5: Statistical results of the IGD+ value obtained by LORA-MOO with different rpratio on 48
benchmark optimization problems over 15 runs. The last four rows count the total results of Wilcoxon
rank sum tests (signiﬁcance level is 0.05). ‘+’, ‘≈’, and ‘−’ denote the corresponding LORA-MOO
variant is statistically signiﬁcantly superior to, almost equivalent to, and inferior to the compared
variants in Wilcoxon tests, respectively."
REFERENCES,0.6337638376383764,"Problems
M
rpratio=1
rpratio=2/3
rpratio=1/2
rpratio=1/3
DTLZ1
3
4.84e+1(1.34e+1)
4.84e+1(1.34e+1)
4.75e+1(1.54e+1)
4.75e+1(1.54e+1)
6
1.83e+1(1.06e+1)
1.64e+1(3.24e+0)
1.35e+1(6.23e+0)
1.35e+1(6.23e+0)
10
1.63e-1(2.74e-2)
1.60e-1(2.67e-2)
1.58e-1(2.81e-2)
1.58e-1(2.81e-2)
DTLZ2
3
4.45e-2(4.72e-3)
4.45e-2(4.72e-3)
4.37e-2(3.41e-3)
3.60e-2(3.69e-3)
6
2.57e-1(1.93e-2)
2.57e-1(1.91e-2)
1.80e-1(1.17e-2)
1.80e-1(7.34e-3)
10
3.74e-1(8.09e-3)
3.00e-1(1.31e-2)
2.87e-1(1.71e-2)
2.87e-1(1.71e-2)
DTLZ3
3
1.48e+2(4.92e+1)
1.48e+2(4.92e+1)
1.54e+2(4.89e+1)
1.54e+2(4.89e+1)
6
6.52e+1(2.87e+1)
6.68e+1(1.64e+1)
6.01e+1(2.61e+1)
6.01e+1(2.61e+1)
10
4.23e-1(5.63e-2)
4.72e-1(5.45e-2)
4.84e-1(5.71e-2)
4.84e-1(5.71e-2)
DTLZ4
3
1.05e-1(1.27e-1)
1.05e-1(1.27e-1)
1.06e-1(1.32e-1)
1.06e-1(1.32e-1)
6
1.70e-1(3.56e-2)
1.75e-1(3.57e-2)
1.79e-1(4.06e-2)
1.79e-1(4.06e-2)
10
2.33e-1(1.26e-2)
2.38e-1(1.35e-2)
2.38e-1(1.56e-2)
2.49e-1(1.46e-2)
DTLZ5
3
9.26e-3(1.22e-3)
9.26e-3(1.22e-3)
8.98e-3(1.67e-3)
8.71e-3(1.89e-3)
6
3.40e-2(9.35e-3)
3.25e-2(8.25e-3)
3.31e-2(7.84e-3)
2.81e-2(1.15e-2)
10
3.83e-3(6.08e-4)
3.97e-3(9.34e-4)
4.85e-3(1.78e-3)
4.92e-3(1.54e-3)
DTLZ6
3
4.67e-2(4.92e-2)
4.67e-2(4.92e-2)
6.38e-2(7.62e-2)
2.56e-2(6.58e-3)
6
4.70e-1(7.64e-1)
7.52e-1(9.50e-1)
7.28e-1(1.00e+0)
1.25e+0(1.13e+0)
10
3.38e-2(1.18e-2)
4.18e-2(4.66e-2)
3.92e-2(3.62e-2)
3.27e-2(2.08e-2)
DTLZ7
3
1.61e-1(2.77e-1)
1.61e-1(2.77e-1)
1.36e-1(1.32e-1)
7.58e-2(2.50e-2)
6
1.41e+0(9.24e-1)
1.25e+0(4.72e-1)
1.21e+0(7.32e-1)
1.28e+0(6.69e-1)
10
1.17e+0(8.28e-2)
1.17e+0(8.97e-2)
1.23e+0(1.33e-1)
1.23e+0(1.33e-1)
WFG1
3
1.67e+0(4.67e-2)
1.67e+0(4.67e-2)
1.67e+0(4.86e-2)
1.67e+0(4.86e-2)
6
2.20e+0(6.03e-2)
2.22e+0(6.80e-2)
2.21e+0(5.69e-2)
2.21e+0(5.69e-2)
10
2.61e+0(1.15e-1)
2.62e+0(8.93e-2)
2.55e+0(1.15e-1)
2.55e+0(1.15e-1)
WFG2
3
2.52e-1(4.94e-2)
2.52e-1(4.94e-2)
2.48e-1(5.57e-2)
2.48e-1(5.57e-2)
6
5.73e-1(1.75e-1)
5.43e-1(1.51e-1)
5.35e-1(9.94e-2)
5.35e-1(9.94e-2)
10
1.37e+0(3.08e-1)
1.47e+0(4.49e-1)
1.36e+0(3.13e-1)
1.25e+0(3.81e-1)
WFG3
3
2.41e-1(3.21e-2)
2.41e-1(3.21e-2)
2.51e-1(3.82e-2)
2.51e-1(3.26e-2)
6
5.82e-1(4.97e-2)
5.92e-1(7.43e-2)
5.83e-1(8.20e-2)
6.05e-1(9.65e-2)
10
6.09e-1(4.65e-2)
6.60e-1(8.00e-2)
6.93e-1(1.22e-1)
6.63e-1(1.05e-1)
WFG4
3
2.48e-1(1.04e-2)
2.48e-1(1.04e-2)
2.49e-1(2.61e-2)
2.96e-1(9.20e-2)
6
2.06e+0(4.21e-1)
1.30e+0(2.41e-1)
1.35e+0(3.15e-1)
1.35e+0(3.15e-1)
10
5.51e+0(6.14e-1)
3.99e+0(7.21e-1)
3.86e+0(6.03e-1)
3.86e+0(6.03e-1)
WFG5
3
3.10e-1(5.46e-2)
3.10e-1(5.46e-2)
3.06e-1(1.05e-1)
4.28e-1(1.46e-1)
6
1.93e+0(1.20e-1)
1.69e+0(1.53e-1)
1.72e+0(1.26e-1)
1.72e+0(1.26e-1)
10
5.50e+0(3.80e-1)
3.71e+0(3.87e-1)
3.63e+0(4.80e-1)
3.63e+0(4.80e-1)
WFG6
3
4.76e-1(6.61e-2)
4.76e-1(6.61e-2)
4.87e-1(1.00e-1)
6.26e-1(1.19e-1)
6
2.21e+0(2.26e-1)
1.67e+0(1.35e-1)
1.62e+0(1.85e-1)
1.62e+0(1.85e-1)
10
5.43e+0(4.78e-1)
3.45e+0(4.44e-1)
3.19e+0(2.14e-1)
3.19e+0(2.14e-1)
WFG7
3
3.02e-1(2.75e-2)
3.02e-1(2.75e-2)
2.95e-1(2.76e-2)
2.98e-1(3.12e-2)
6
2.10e+0(2.12e-1)
1.67e+0(1.85e-1)
1.58e+0(1.47e-1)
1.58e+0(1.47e-1)
10
5.85e+0(5.16e-1)
4.97e+0(3.07e-1)
4.76e+0(4.89e-1)
4.76e+0(4.89e-1)
WFG8
3
5.68e-1(4.78e-2)
5.68e-1(4.78e-2)
5.71e-1(4.02e-2)
5.83e-1(4.65e-2)
6
2.61e+0(2.09e-1)
2.25e+0(1.12e-1)
2.21e+0(1.21e-1)
2.21e+0(1.21e-1)
10
6.41e+0(4.20e-1)
5.16e+0(5.37e-1)
5.06e+0(5.80e-1)
5.06e+0(5.80e-1)
WFG9
3
4.12e-1(1.17e-1)
4.12e-1(1.17e-1)
3.81e-1(1.02e-1)
3.66e-1(8.95e-2)
6
1.86e+0(2.00e-1)
1.77e+0(2.57e-1)
1.48e+0(2.27e-1)
1.45e+0(1.77e-1)
10
5.57e+0(2.73e-1)
3.96e+0(3.83e-1)
4.02e+0(4.62e-1)
4.02e+0(4.62e-1)
+/ ≈/−
rpratio=1
-/-/-
2/34/12
2/32/14
5/28/15
+/ ≈/−
rpratio=2/3
12/34/2
-/-/-
0/46/2
3/42/3
+/ ≈/−
rpratio=1/2
14/32/2
2/46/0
-/-/-
2/45/1
+/ ≈/−
rpratio=1/3
15/28/5
3/42/3
1/45/2
-/-/-"
REFERENCES,0.6346863468634686,"F.4
Inﬂuence of Clustering Number for Reproduction nc.
628"
REFERENCES,0.6356088560885609,"This subsection analyzes the inﬂuence of clustering number nc on the optimization performance. nc
629"
REFERENCES,0.6365313653136532,"is used in the reproduction process to initialize the PSO population. We set nc = {1, 3, 5, 7, 10} to
630"
REFERENCES,0.6374538745387454,"generate ﬁve LORA-MOO variants. According to the conclusions of previous ablation studies, in this
631"
REFERENCES,0.6383763837638377,"ablation study, we set no = 4, λ = 0.2, rpratio = 1/2 for all variants. The IGD+ values obtained by
632"
REFERENCES,0.6392988929889298,"ﬁve LORA-MOO variants with different nc are reported in Table 6.
633"
REFERENCES,0.6402214022140221,"It can be observed that both the variants of nc = 5 and nc = 7 outperform three other variants and are
634"
REFERENCES,0.6411439114391144,"inferior to one variant, showing the optimal performance over other variants in this ablation study.
635"
REFERENCES,0.6420664206642066,"In comparison, the variants of nc = 3 and nc = 10 are signiﬁcantly superior to two variants but are
636"
REFERENCES,0.6429889298892989,"Table 6: Statistical results of the IGD+ value obtained by LORA-MOO with different nc on 48
benchmark optimization problems over 15 runs. The last ﬁve rows count the total results of Wilcoxon
rank sum tests (signiﬁcance level is 0.05). ‘+’, ‘≈’, and ‘−’ denote the corresponding LORA-MOO
variant is statistically signiﬁcantly superior to, almost equivalent to, and inferior to the compared
variants in Wilcoxon tests, respectively."
REFERENCES,0.6439114391143912,"Problems
M
nc=1
nc=3
nc=5
nc=7
nc=10
DTLZ1
3
6.45e+1(1.31e+1)
5.77e+1(2.13e+1)
4.75e+1(1.54e+1)
4.02e+1(1.46e+1)
3.91e+1(1.53e+1)
6
2.22e+1(5.99e+0)
1.67e+1(4.35e+0)
1.35e+1(6.23e+0)
1.55e+1(5.29e+0)
1.56e+1(7.51e+0)
10
1.52e-1(3.01e-2)
1.67e-1(4.03e-2)
1.58e-1(2.81e-2)
1.58e-1(3.11e-2)
1.64e-1(3.19e-2)
DTLZ2
3
4.40e-2(3.06e-3)
4.38e-2(4.17e-3)
4.37e-2(3.41e-3)
4.48e-2(3.51e-3)
4.29e-2(4.38e-3)
6
1.84e-1(1.50e-2)
1.79e-1(1.02e-2)
1.80e-1(1.17e-2)
1.79e-1(9.20e-3)
1.80e-1(1.49e-2)
10
2.89e-1(1.00e-2)
2.97e-1(1.40e-2)
2.87e-1(1.71e-2)
2.90e-1(1.22e-2)
2.85e-1(1.09e-2)
DTLZ3
3
1.89e+2(4.68e+1)
1.61e+2(3.71e+1)
1.54e+2(4.89e+1)
1.58e+2(3.45e+1)
1.57e+2(3.17e+1)
6
7.44e+1(2.34e+1)
6.06e+1(1.32e+1)
6.01e+1(2.61e+1)
6.65e+1(2.14e+1)
6.44e+1(2.63e+1)
10
4.65e-1(1.12e-1)
4.70e-1(8.67e-2)
4.84e-1(5.71e-2)
4.92e-1(1.38e-1)
4.61e-1(4.94e-2)
DTLZ4
3
8.66e-2(1.25e-1)
1.35e-1(1.64e-1)
1.06e-1(1.32e-1)
8.82e-2(1.26e-1)
1.04e-1(1.28e-1)
6
1.69e-1(2.20e-2)
1.80e-1(3.27e-2)
1.79e-1(4.06e-2)
1.81e-1(4.77e-2)
1.79e-1(2.78e-2)
10
2.29e-1(1.15e-2)
2.30e-1(1.06e-2)
2.38e-1(1.56e-2)
2.37e-1(2.00e-2)
2.37e-1(1.88e-2)
DTLZ5
3
9.75e-3(2.19e-3)
8.93e-3(1.67e-3)
8.98e-3(1.67e-3)
9.15e-3(1.58e-3)
8.80e-3(1.44e-3)
6
3.12e-2(9.30e-3)
2.98e-2(1.02e-2)
3.31e-2(7.84e-3)
2.72e-2(7.30e-3)
3.00e-2(1.05e-2)
10
5.60e-3(1.76e-3)
3.92e-3(6.78e-4)
4.85e-3(1.78e-3)
5.65e-3(2.12e-3)
6.02e-3(1.70e-3)
DTLZ6
3
4.87e-2(2.65e-2)
4.28e-2(2.73e-2)
6.38e-2(7.62e-2)
9.93e-2(2.14e-1)
5.04e-2(3.71e-2)
6
1.09e+0(1.19e+0)
1.11e+0(1.07e+0)
7.28e-1(1.00e+0)
1.01e+0(1.13e+0)
8.36e-1(1.16e+0)
10
2.25e-2(7.14e-3)
6.20e-2(5.11e-2)
3.92e-2(3.62e-2)
3.51e-2(3.23e-2)
4.42e-2(4.00e-2)
DTLZ7
3
6.96e-2(3.03e-2)
7.83e-2(5.28e-2)
1.36e-1(1.32e-1)
1.28e-1(1.31e-1)
9.71e-2(5.24e-2)
6
6.96e-1(2.65e-1)
1.68e+0(8.29e-1)
1.21e+0(7.32e-1)
1.16e+0(6.33e-1)
1.74e+0(8.02e-1)
10
1.24e+0(1.54e-1)
1.20e+0(9.84e-2)
1.23e+0(1.33e-1)
1.20e+0(8.92e-2)
1.25e+0(1.08e-1)
WFG1
3
1.67e+0(4.91e-2)
1.64e+0(5.90e-2)
1.67e+0(4.86e-2)
1.62e+0(3.43e-2)
1.61e+0(4.98e-2)
6
2.27e+0(5.70e-2)
2.24e+0(5.05e-2)
2.21e+0(5.69e-2)
2.21e+0(7.43e-2)
2.20e+0(6.16e-2)
10
2.67e+0(8.46e-2)
2.56e+0(1.07e-1)
2.55e+0(1.15e-1)
2.64e+0(7.62e-2)
2.61e+0(8.36e-2)
WFG2
3
2.63e-1(3.41e-2)
2.63e-1(3.89e-2)
2.48e-1(5.57e-2)
2.47e-1(4.40e-2)
2.44e-1(5.40e-2)
6
5.17e-1(1.03e-1)
5.43e-1(1.35e-1)
5.35e-1(9.94e-2)
5.24e-1(1.26e-1)
5.09e-1(1.49e-1)
10
1.39e+0(4.37e-1)
1.39e+0(3.77e-1)
1.36e+0(3.13e-1)
1.40e+0(2.71e-1)
1.38e+0(3.83e-1)
WFG3
3
2.57e-1(3.61e-2)
2.64e-1(7.85e-2)
2.51e-1(3.82e-2)
2.78e-1(5.66e-2)
2.48e-1(2.96e-2)
6
6.25e-1(1.13e-1)
5.89e-1(6.72e-2)
5.83e-1(8.20e-2)
5.80e-1(7.49e-2)
6.56e-1(1.04e-1)
10
6.67e-1(8.95e-2)
6.93e-1(9.45e-2)
6.93e-1(1.22e-1)
7.03e-1(9.06e-2)
7.47e-1(8.54e-2)
WFG4
3
2.56e-1(3.27e-2)
2.49e-1(2.04e-2)
2.49e-1(2.61e-2)
2.48e-1(1.75e-2)
2.41e-1(1.77e-2)
6
1.30e+0(1.91e-1)
1.34e+0(2.28e-1)
1.35e+0(3.15e-1)
1.20e+0(2.23e-1)
1.38e+0(2.88e-1)
10
3.68e+0(6.78e-1)
3.87e+0(7.96e-1)
3.86e+0(6.03e-1)
3.83e+0(7.38e-1)
3.65e+0(3.90e-1)
WFG5
3
3.17e-1(1.22e-1)
3.50e-1(1.07e-1)
3.06e-1(1.05e-1)
3.12e-1(1.25e-1)
2.92e-1(1.28e-1)
6
1.78e+0(9.49e-2)
1.76e+0(1.11e-1)
1.72e+0(1.26e-1)
1.73e+0(9.61e-2)
1.74e+0(1.33e-1)
10
3.79e+0(2.92e-1)
3.59e+0(2.81e-1)
3.63e+0(4.80e-1)
3.87e+0(3.19e-1)
3.79e+0(2.71e-1)
WFG6
3
4.48e-1(1.00e-1)
5.24e-1(1.08e-1)
4.87e-1(1.00e-1)
4.86e-1(9.23e-2)
4.64e-1(9.08e-2)
6
1.65e+0(1.84e-1)
1.63e+0(8.15e-2)
1.62e+0(1.85e-1)
1.61e+0(1.48e-1)
1.59e+0(2.47e-1)
10
3.35e+0(4.95e-1)
3.51e+0(3.14e-1)
3.19e+0(2.14e-1)
3.33e+0(3.76e-1)
3.14e+0(5.76e-1)
WFG7
3
2.90e-1(3.37e-2)
3.14e-1(3.26e-2)
2.95e-1(2.76e-2)
2.95e-1(2.68e-2)
2.90e-1(3.27e-2)
6
1.62e+0(2.02e-1)
1.72e+0(1.37e-1)
1.58e+0(1.47e-1)
1.61e+0(1.63e-1)
1.64e+0(1.85e-1)
10
4.55e+0(3.72e-1)
4.81e+0(3.13e-1)
4.76e+0(4.89e-1)
4.82e+0(3.93e-1)
4.51e+0(2.58e-1)
WFG8
3
5.91e-1(6.73e-2)
6.06e-1(5.44e-2)
5.71e-1(4.02e-2)
5.77e-1(3.92e-2)
5.61e-1(3.98e-2)
6
2.20e+0(1.50e-1)
2.20e+0(1.48e-1)
2.21e+0(1.21e-1)
2.24e+0(1.57e-1)
2.16e+0(1.06e-1)
10
4.99e+0(4.45e-1)
5.15e+0(4.48e-1)
5.06e+0(5.80e-1)
5.00e+0(3.93e-1)
4.90e+0(5.04e-1)
WFG9
3
3.68e-1(1.03e-1)
4.43e-1(1.41e-1)
3.81e-1(1.02e-1)
3.85e-1(9.50e-2)
3.56e-1(6.48e-2)
6
1.54e+0(1.81e-1)
1.51e+0(1.73e-1)
1.48e+0(2.27e-1)
1.45e+0(1.19e-1)
1.48e+0(1.75e-1)
10
4.02e+0(2.34e-1)
3.97e+0(4.11e-1)
4.02e+0(4.62e-1)
3.94e+0(3.94e-1)
3.96e+0(3.20e-1)
+/ ≈/−
nc=1
-/-/-
2/43/3
1/41/6
1/42/5
3/41/4
+/ ≈/−
nc=3
3/43/2
-/-/-
0/46/2
2/45/1
1/41/6
+/ ≈/−
nc=5
6/41/1
2/46/0
-/-/-
1/45/2
2/45/1
+/ ≈/−
nc=7
5/42/1
1/45/2
2/45/1
-/-/-
2/45/1
+/ ≈/−
nc=10
4/41/3
6/41/1
1/45/2
1/45/2
-/-/-"
REFERENCES,0.6448339483394834,"also signiﬁcantly inferior to two other variants. The variant of nc = 1 reaches the worst optimization
637"
REFERENCES,0.6457564575645757,"results as it is signiﬁcantly inferior to all other variants. In addition, considering that the variant of nc
638"
REFERENCES,0.6466789667896679,"= 7 wins/ties/losses 2/45/1 statistical tests when compared with the variant of nc = 5, we set nc = 7
639"
REFERENCES,0.6476014760147601,"for LORA-MOO.
640"
REFERENCES,0.6485239852398524,"The result of this ablation study demonstrates the inﬂuence of population initialization on the
641"
REFERENCES,0.6494464944649446,"optimization results. By clustering the evaluated solutions into several clusters and sampling the same
642"
REFERENCES,0.6503690036900369,"amount of initial solutions from each cluster, the solutions in the initial population are distributed
643"
REFERENCES,0.6512915129151291,"in a more diverse way than the solutions sampled from the set of reference points SRP directly.
644"
REFERENCES,0.6522140221402214,"Figure 7: Distribution of obtained non-dominated solutions on DTLZ2 with 10 variables and 3
objectives."
REFERENCES,0.6531365313653137,"Figure 8: Distribution of obtained non-dominated solutions on DTLZ4 with 10 variables and 3
objectives."
REFERENCES,0.6540590405904059,"Consequently, all variants of nc > 1 have achieved better optimization results than the variant of nc
645"
REFERENCES,0.6549815498154982,"= 1.
646"
REFERENCES,0.6559040590405905,"G
Solution Distribution
647"
REFERENCES,0.6568265682656826,"The solution distribution we obtained on some 3-objective DTLZ problems are plotted.
648"
REFERENCES,0.6577490774907749,"H
Complete Results of Benchmark Optimization
649"
REFERENCES,0.6586715867158671,"In Section 4.3 of the main ﬁle, we display the optimization results of comparison algorithms on
650"
REFERENCES,0.6595940959409594,"DTLZ problems in terms of IGD values. In this section, we provide detailed IGD results on WFG
651"
REFERENCES,0.6605166051660517,"problems and more results on IGD+ and HV values. In addition, the optimization results on DTLZ
652"
REFERENCES,0.6614391143911439,"problems with different scales, such as D = 5 and 20, are reported.
653"
REFERENCES,0.6623616236162362,"H.1
IGD Results on WFG Optimization Problems
654"
REFERENCES,0.6632841328413284,"Table 7 shows the optimization results on WFG problems in terms of IGD values. The last row
655"
REFERENCES,0.6642066420664207,"summarizes the results of statistical tests, which has reported at the end of Table 1 in the main ﬁle.
656"
REFERENCES,0.665129151291513,"It can be seen that LORA-MOO outperforms all comparison algorithms, followed by KTA2 and
657"
REFERENCES,0.6660516605166051,"Figure 9: Distribution of obtained non-dominated solutions on DTLZ6 with 10 variables and 3
objectives."
REFERENCES,0.6669741697416974,"Table 7: Statistical results of the IGD value obtained by comparison algorithms on 45 WFG optimiza-
tion problems over 30 runs. Symbols ‘+’, ‘≈’, ‘−’ denote LORA-MOO is statistically signiﬁcantly
superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum
test (signiﬁcance level is 0.05), respectively. The last row counts the total win/tie/loss results."
REFERENCES,0.6678966789667896,"Problems
M
ParEGO
KRVEA
KTA2
CSEA
REMO
OREA
LORA-MOO
WFG1
3
1.65e+0(8.08e-2)−
1.74e+0(9.91e-2)≈
1.87e+0(1.27e-1)+
1.74e+0(8.60e-2)≈
1.73e+0(1.12e-1)≈
2.03e+0(1.16e-1)+
1.71e+0(9.26e-2)
4
1.94e+0(7.04e-2)≈
2.07e+0(9.03e-2)+
2.18e+0(1.43e-1)+
2.05e+0(1.05e-1)+
1.96e+0(8.19e-2)≈
2.22e+0(9.54e-2)+
1.95e+0(7.52e-2)
6
2.38e+0(5.53e-2)≈
2.49e+0(6.57e-2)+
2.56e+0(9.95e-2)+
2.52e+0(9.89e-2)+
2.42e+0(5.34e-2)+
2.53e+0(1.04e-1)+
2.36e+0(5.07e-2)
8
2.75e+0(5.21e-2)+
2.86e+0(7.05e-2)+
2.85e+0(1.06e-1)+
2.89e+0(5.19e-2)+
2.80e+0(7.44e-2)+
2.82e+0(7.56e-2)+
2.72e+0(6.21e-2)
10
3.08e+0(5.70e-2)+
3.11e+0(9.16e-2)+
2.99e+0(9.77e-2)+
3.09e+0(1.03e-1)+
3.04e+0(1.12e-1)+
3.10e+0(9.11e-2)+
2.93e+0(6.20e-2)
WFG2
3
7.66e-1(7.11e-2)+
3.61e-1(3.87e-2)≈
4.24e-1(6.65e-2)+
5.48e-1(3.75e-2)+
5.22e-1(7.67e-2)+
4.88e-1(6.53e-2)+
3.72e-1(4.87e-2)
4
1.05e+0(1.40e-1)+
5.00e-1(3.97e-2)−
5.66e-1(3.80e-2)+
7.61e-1(1.21e-1)+
7.48e-1(1.23e-1)+
7.45e-1(1.45e-1)+
5.46e-1(3.53e-2)
6
1.90e+0(3.51e-1)+
7.77e-1(5.25e-2)−
9.00e-1(5.39e-2)+
1.28e+0(4.02e-1)+
1.28e+0(3.75e-1)+
1.49e+0(3.76e-1)+
8.55e-1(7.00e-2)
8
2.74e+0(6.68e-1)+
1.06e+0(5.98e-2)−
1.18e+0(1.14e-1)−
2.10e+0(6.97e-1)+
1.90e+0(5.25e-1)+
2.06e+0(4.58e-1)+
1.24e+0(1.23e-1)
10
3.73e+0(9.41e-1)+
1.18e+0(9.32e-2)−
1.37e+0(1.03e-1)−
2.84e+0(8.61e-1)+
2.59e+0(9.91e-1)+
2.95e+0(7.55e-1)+
1.83e+0(2.27e-1)
WFG3
3
5.82e-1(3.86e-2)+
5.39e-1(5.81e-2)+
3.29e-1(5.99e-2)+
5.04e-1(6.26e-2)+
4.60e-1(5.94e-2)+
3.85e-1(4.76e-2)+
2.83e-1(5.99e-2)
4
7.30e-1(6.25e-2)+
6.66e-1(7.02e-2)+
5.63e-1(6.47e-2)+
6.05e-1(7.26e-2)+
5.64e-1(6.43e-2)+
5.68e-1(5.92e-2)+
4.13e-1(5.98e-2)
6
7.75e-1(9.36e-2)+
6.76e-1(1.32e-1)≈
7.94e-1(6.73e-2)+
7.41e-1(8.33e-2)+
6.37e-1(9.55e-2)≈
7.96e-1(6.68e-2)+
6.51e-1(9.20e-2)
8
8.38e-1(1.63e-1)≈
8.27e-1(9.79e-2)≈
9.45e-1(7.42e-2)+
7.63e-1(1.06e-1)−
6.25e-1(1.18e-1)−
8.92e-1(9.90e-2)≈
8.54e-1(9.98e-2)
10
6.85e-1(1.02e-1)−
6.87e-1(8.79e-2)−
9.16e-1(8.20e-2)+
5.91e-1(9.34e-2)−
5.19e-1(1.04e-1)−
7.28e-1(1.10e-1)−
8.23e-1(1.14e-1)
WFG4
3
6.21e-1(3.68e-2)+
4.67e-1(2.33e-2)+
4.21e-1(2.21e-2)+
4.57e-1(2.88e-2)+
4.23e-1(2.53e-2)+
4.34e-1(5.63e-2)+
3.36e-1(2.95e-2)
4
1.11e+0(3.45e-2)+
7.86e-1(2.45e-2)+
7.78e-1(4.50e-2)+
9.83e-1(1.22e-1)+
8.46e-1(8.32e-2)+
1.07e+0(1.18e-1)+
6.82e-1(4.97e-2)
6
2.75e+0(2.36e-1)+
1.87e+0(8.92e-2)≈
1.78e+0(7.66e-2)−
3.13e+0(3.86e-1)+
2.69e+0(3.61e-1)+
2.92e+0(3.04e-1)+
1.86e+0(1.30e-1)
8
5.09e+0(9.78e-1)+
3.47e+0(2.96e-1)−
3.26e+0(1.67e-1)−
5.81e+0(5.38e-1)+
4.99e+0(4.67e-1)+
5.76e+0(4.34e-1)+
3.62e+0(3.31e-1)
10
7.18e+0(1.21e+0)+
5.60e+0(6.92e-1)≈
4.97e+0(1.72e-1)−
8.58e+0(8.39e-1)+
7.78e+0(8.13e-1)+
8.03e+0(5.03e-1)+
5.47e+0(4.14e-1)
WFG5
3
4.21e-1(3.05e-2)+
3.91e-1(4.22e-2)≈
3.30e-1(9.56e-2)−
5.50e-1(3.05e-2)+
5.30e-1(4.46e-2)+
4.51e-1(6.51e-2)+
4.21e-1(1.35e-1)
4
9.98e-1(8.09e-2)≈
7.65e-1(2.86e-2)−
7.20e-1(6.23e-2)−
8.87e-1(3.98e-2)−
8.61e-1(4.68e-2)−
1.02e+0(4.57e-2)+
9.81e-1(5.76e-2)
6
2.82e+0(1.65e-1)+
1.78e+0(6.23e-2)−
1.92e+0(1.03e-1)−
2.35e+0(1.86e-1)+
2.04e+0(1.29e-1)−
2.44e+0(1.08e-1)+
2.11e+0(9.10e-2)
8
5.25e+0(2.55e-1)+
3.30e+0(2.61e-1)−
3.62e+0(2.64e-1)≈
4.75e+0(3.77e-1)+
3.95e+0(2.83e-1)+
4.57e+0(1.82e-1)+
3.66e+0(9.43e-2)
10
7.64e+0(3.23e-1)+
4.67e+0(4.78e-1)−
4.76e+0(1.99e-1)−
6.88e+0(4.23e-1)+
6.11e+0(4.62e-1)+
6.68e+0(3.49e-1)+
4.98e+0(1.57e-1)
WFG6
3
7.96e-1(5.50e-2)+
7.05e-1(5.10e-2)+
6.22e-1(8.49e-2)+
7.19e-1(4.80e-2)+
7.09e-1(4.61e-2)+
5.79e-1(4.68e-2)+
5.67e-1(1.09e-1)
4
1.14e+0(3.47e-2)+
1.02e+0(4.90e-2)+
9.62e-1(4.46e-2)≈
1.08e+0(4.82e-2)+
1.04e+0(4.53e-2)+
1.17e+0(4.94e-2)+
9.51e-1(9.85e-2)
6
2.81e+0(2.60e-1)+
2.18e+0(7.41e-2)+
1.96e+0(4.17e-2)−
2.56e+0(2.16e-1)+
2.20e+0(1.61e-1)+
2.77e+0(1.81e-1)+
2.04e+0(9.86e-2)
8
4.70e+0(5.78e-1)+
3.60e+0(1.17e-1)+
3.54e+0(1.85e-1)≈
4.70e+0(5.18e-1)+
4.13e+0(3.06e-1)+
5.06e+0(3.20e-1)+
3.52e+0(1.52e-1)
10
7.66e+0(5.36e-1)+
5.00e+0(1.33e-1)+
5.09e+0(1.58e-1)+
6.73e+0(5.98e-1)+
5.83e+0(4.69e-1)+
7.00e+0(4.90e-1)+
4.76e+0(1.94e-1)
WFG7
3
6.69e-1(2.70e-2)+
6.28e-1(2.45e-2)+
5.73e-1(2.76e-2)+
5.78e-1(3.23e-2)+
5.38e-1(3.58e-2)+
4.43e-1(4.15e-2)+
3.52e-1(2.22e-2)
4
1.13e+0(4.94e-2)+
9.48e-1(2.66e-2)+
9.04e-1(2.51e-2)+
9.92e-1(8.75e-2)+
8.81e-1(3.49e-2)+
9.72e-1(7.29e-2)+
7.07e-1(4.29e-2)
6
3.17e+0(2.89e-1)+
2.00e+0(5.61e-2)≈
1.96e+0(5.97e-2)≈
2.71e+0(3.18e-1)+
2.18e+0(1.49e-1)+
2.71e+0(1.91e-1)+
1.96e+0(1.06e-1)
8
5.93e+0(3.95e-1)+
3.64e+0(1.23e-1)−
3.37e+0(1.16e-1)−
5.19e+0(5.20e-1)+
4.28e+0(4.59e-1)+
5.19e+0(3.07e-1)+
3.82e+0(1.63e-1)
10
8.78e+0(4.70e-1)+
5.31e+0(3.01e-1)−
4.88e+0(1.76e-1)−
8.07e+0(5.07e-1)+
6.77e+0(5.93e-1)+
7.57e+0(4.12e-1)+
5.73e+0(3.07e-1)
WFG8
3
8.45e-1(2.87e-2)+
6.42e-1(2.49e-2)+
5.09e-1(4.39e-2)−
7.49e-1(4.33e-2)+
7.13e-1(3.87e-2)+
7.01e-1(4.35e-2)+
6.02e-1(3.64e-2)
4
1.33e+0(4.61e-2)+
1.14e+0(3.89e-2)≈
1.02e+0(3.96e-2)−
1.26e+0(6.23e-2)+
1.20e+0(5.28e-2)+
1.36e+0(6.94e-2)+
1.13e+0(7.12e-2)
6
3.11e+0(2.82e-1)+
2.43e+0(7.15e-2)≈
2.28e+0(5.05e-2)−
3.00e+0(1.53e-1)+
2.80e+0(1.90e-1)+
3.07e+0(1.74e-1)+
2.45e+0(9.73e-2)
8
5.74e+0(3.56e-1)+
4.01e+0(2.28e-1)−
3.92e+0(1.28e-1)−
5.56e+0(3.24e-1)+
5.11e+0(4.10e-1)+
5.34e+0(2.72e-1)+
4.22e+0(2.75e-1)
10
8.30e+0(4.83e-1)+
5.56e+0(5.40e-1)−
5.71e+0(3.80e-1)≈
7.81e+0(4.74e-1)+
7.32e+0(3.46e-1)+
7.54e+0(4.88e-1)+
5.82e+0(2.95e-1)
WFG9
3
7.14e-1(5.09e-2)+
6.75e-1(6.73e-2)+
6.37e-1(8.35e-2)+
6.74e-1(8.53e-2)+
6.11e-1(9.76e-2)+
5.12e-1(7.74e-2)+
4.34e-1(8.18e-2)
4
1.24e+0(1.41e-1)+
1.06e+0(8.72e-2)+
1.07e+0(9.28e-2)+
1.16e+0(1.18e-1)+
1.05e+0(1.61e-1)+
1.02e+0(7.89e-2)+
8.43e-1(9.25e-2)
6
3.14e+0(2.96e-1)+
2.22e+0(1.94e-1)+
2.19e+0(1.52e-1)+
2.83e+0(2.36e-1)+
2.30e+0(1.82e-1)+
2.55e+0(1.21e-1)+
1.97e+0(9.18e-2)
8
5.78e+0(4.51e-1)+
3.93e+0(3.00e-1)+
3.77e+0(2.23e-1)+
5.43e+0(3.68e-1)+
4.60e+0(3.92e-1)+
4.73e+0(3.07e-1)+
3.61e+0(2.05e-1)
10
8.41e+0(4.80e-1)+
5.69e+0(6.42e-1)+
5.26e+0(3.13e-1)≈
7.77e+0(5.05e-1)+
6.48e+0(5.60e-1)+
6.74e+0(4.17e-1)+
5.16e+0(2.60e-1)
+/ ≈/−
39/4/2
21/10/14
23/6/16
41/1/3
38/3/4
43/1/1"
REFERENCES,0.6688191881918819,"KRVEA. This is consistent with the results we observed from Table 1. The results on six 3- and
658"
REFERENCES,0.6697416974169742,"10-objective WFG problems are plotted in Fig. 10.
659"
REFERENCES,0.6706642066420664,"Figure 10: Log (IGD) curves averaged over 30 runs on six WFG problems for comparison algorithms
(shaded area is ± std of the mean). Top: 10 variables and 3 objectives. Bottom: 10 variables and 10
objectives."
REFERENCES,0.6715867158671587,"Table 8: Statistical results of the IGD+ value obtained by comparison algorithms on 35 DTLZ
optimization problems over 30 runs. Symbols ‘+’, ‘≈’, ‘−’ denote LORA-MOO is statistically
signiﬁcantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon
rank sum test (signiﬁcance level is 0.05), respectively. The last row counts the total win/tie/loss
results."
REFERENCES,0.672509225092251,"Problems
M
ParEGO
KRVEA
KTA2
CSEA
REMO
OREA
LORA-MOO
DTLZ1
3
5.98e+1(3.81e+0)+
8.88e+1(2.16e+1)+
4.75e+1(1.55e+1)≈
6.30e+1(1.69e+1)+
5.06e+1(1.49e+1)+
4.44e+1(1.38e+1)≈
4.35e+1(1.80e+1)
4
4.68e+1(3.71e+0)+
6.45e+1(1.47e+1)+
4.08e+1(1.60e+1)≈
3.69e+1(1.08e+1)≈
3.92e+1(1.11e+1)≈
3.80e+1(1.23e+1)≈
4.06e+1(1.34e+1)
6
3.04e+1(2.74e+0)+
3.22e+1(7.66e+0)+
2.03e+1(8.12e+0)+
1.56e+1(4.96e+0)≈
1.22e+1(4.65e+0)−
1.74e+1(3.98e+0)≈
1.58e+1(6.17e+0)
8
1.23e+1(2.99e+0)+
8.52e+0(2.98e+0)+
4.54e+0(2.66e+0)≈
5.08e+0(2.47e+0)≈
3.33e+0(1.93e+0)≈
5.87e+0(2.91e+0)+
3.82e+0(2.35e+0)
10
3.82e-1(1.79e-1)+
2.76e-1(1.14e-1)+
2.33e-1(9.65e-2)+
2.22e-1(8.29e-2)+
1.75e-1(7.84e-2)≈
1.83e-1(6.73e-2)≈
1.56e-1(3.41e-2)
DTLZ2
3
2.61e-1(3.63e-2)+
9.22e-2(2.57e-2)+
3.82e-2(3.29e-3)−
1.60e-1(2.76e-2)+
1.01e-1(1.75e-2)+
5.86e-2(8.28e-3)+
4.47e-2(3.35e-3)
4
3.55e-1(4.11e-2)+
1.30e-1(3.08e-2)+
9.05e-2(6.95e-3)−
2.05e-1(2.43e-2)+
1.60e-1(3.01e-2)+
1.37e-1(1.61e-2)+
9.74e-2(1.14e-2)
6
4.47e-1(2.32e-2)+
1.82e-1(1.49e-2)≈
2.36e-1(3.71e-2)+
3.15e-1(4.24e-2)+
2.64e-1(3.18e-2)+
3.21e-1(2.78e-2)+
1.82e-1(1.15e-2)
8
4.68e-1(1.49e-2)+
2.34e-1(1.90e-2)−
3.43e-1(2.37e-2)+
3.95e-1(2.66e-2)+
3.42e-1(2.91e-2)+
4.19e-1(1.86e-2)+
2.58e-1(1.88e-2)
10
4.33e-1(2.26e-2)+
2.92e-1(3.09e-2)≈
3.15e-1(1.47e-2)+
4.17e-1(2.03e-2)+
3.61e-1(2.70e-2)+
4.28e-1(1.61e-2)+
2.88e-1(1.27e-2)
DTLZ3
3
1.66e+2(1.31e+1)+
2.43e+2(4.61e+1)+
1.52e+2(4.73e+1)≈
1.62e+2(4.84e+1)≈
1.49e+2(3.88e+1)≈
1.26e+2(3.18e+1)−
1.57e+2(3.83e+1)
4
1.42e+2(1.57e+1)+
1.83e+2(4.00e+1)+
1.18e+2(3.49e+1)≈
1.29e+2(3.58e+1)≈
1.16e+2(3.00e+1)≈
1.22e+2(4.13e+1)≈
1.25e+2(4.20e+1)
6
9.17e+1(1.59e+1)+
1.06e+2(2.96e+1)+
6.65e+1(2.63e+1)≈
5.27e+1(1.56e+1)≈
5.23e+1(1.71e+1)≈
5.24e+1(1.68e+1)≈
5.96e+1(2.05e+1)
8
4.13e+1(9.84e+0)+
2.96e+1(1.15e+1)+
1.73e+1(1.10e+1)≈
1.59e+1(9.77e+0)≈
1.60e+1(7.71e+0)≈
1.49e+1(6.28e+0)≈
1.26e+1(8.35e+0)
10
1.08e+0(3.73e-1)+
9.96e-1(4.96e-1)+
7.29e-1(2.75e-1)+
6.94e-1(2.89e-1)+
6.89e-1(3.18e-1)+
5.27e-1(6.34e-2)+
4.75e-1(1.13e-1)
DTLZ4
3
4.57e-1(7.52e-2)+
2.66e-1(1.02e-1)+
2.33e-1(8.36e-2)+
2.34e-1(7.76e-2)+
1.32e-1(6.41e-2)+
1.07e-1(9.68e-2)+
8.96e-2(1.25e-1)
4
4.86e-1(5.76e-2)+
2.84e-1(7.44e-2)+
2.95e-1(6.34e-2)+
2.03e-1(3.78e-2)+
1.66e-1(3.40e-2)+
1.35e-1(9.87e-2)≈
1.37e-1(9.79e-2)
6
4.24e-1(4.26e-2)+
2.94e-1(5.11e-2)+
3.61e-1(7.84e-2)+
2.41e-1(3.82e-2)+
2.27e-1(3.26e-2)+
1.67e-1(2.62e-2)≈
1.78e-1(4.02e-2)
8
3.53e-1(2.66e-2)+
2.67e-1(3.51e-2)+
3.33e-1(4.56e-2)+
2.78e-1(3.65e-2)+
2.93e-1(3.63e-2)+
2.09e-1(2.55e-2)≈
2.08e-1(1.89e-2)
10
2.86e-1(1.61e-2)+
2.58e-1(2.11e-2)+
2.88e-1(3.27e-2)+
2.92e-1(2.16e-2)+
3.06e-1(2.71e-2)+
2.29e-1(1.41e-2)≈
2.30e-1(1.70e-2)
DTLZ5
3
1.60e-1(4.40e-2)+
9.18e-2(2.76e-2)+
8.66e-3(1.96e-3)≈
9.58e-2(2.60e-2)+
5.78e-2(1.81e-2)+
1.59e-2(5.12e-3)+
9.40e-3(1.93e-3)
4
1.47e-1(3.58e-2)+
4.96e-2(1.98e-2)+
3.25e-2(9.50e-3)+
9.78e-2(2.16e-2)+
7.51e-2(2.55e-2)+
2.88e-2(7.46e-3)+
2.21e-2(7.30e-3)
6
1.08e-1(2.44e-2)+
2.24e-2(7.50e-3)−
8.02e-2(2.16e-2)+
6.16e-2(2.49e-2)+
4.14e-2(1.76e-2)+
3.89e-2(1.47e-2)≈
3.20e-2(1.14e-2)
8
5.11e-2(7.70e-3)+
1.44e-2(5.17e-3)−
5.35e-2(1.14e-2)+
2.49e-2(6.87e-3)+
2.01e-2(5.56e-3)≈
1.89e-2(5.87e-3)≈
1.87e-2(3.21e-3)
10
1.19e-2(1.01e-3)+
6.26e-3(9.09e-4)+
1.19e-2(1.80e-3)+
7.45e-3(9.85e-4)+
4.80e-3(1.09e-3)−
5.48e-3(9.49e-4)≈
5.62e-3(1.75e-3)
DTLZ6
3
2.42e-1(1.07e-1)+
3.05e+0(5.23e-1)+
1.82e+0(4.48e-1)+
4.85e+0(6.38e-1)+
4.27e+0(5.48e-1)+
2.35e-1(4.14e-1)+
6.74e-2(1.55e-1)
4
2.64e-1(1.83e-1)+
2.44e+0(3.90e-1)+
1.84e+0(5.17e-1)+
5.12e+0(4.31e-1)+
4.07e+0(6.25e-1)+
1.35e+0(9.45e-1)+
2.07e-1(2.06e-1)
6
1.78e-1(1.07e-1)−
1.33e+0(2.80e-1)+
1.49e+0(5.98e-1)+
3.14e+0(4.44e-1)+
2.32e+0(5.72e-1)+
2.04e+0(6.34e-1)+
9.00e-1(1.07e+0)
8
8.31e-2(2.90e-2)≈
4.48e-1(1.88e-1)+
8.28e-1(4.14e-1)+
1.53e+0(4.64e-1)+
9.18e-1(4.68e-1)+
1.03e+0(4.26e-1)+
2.96e-1(4.46e-1)
10
8.21e-2(9.39e-2)+
3.08e-2(1.03e-2)≈
6.59e-2(5.61e-2)+
1.63e-1(2.40e-1)+
5.12e-2(1.09e-1)≈
1.15e-1(7.35e-2)+
3.30e-2(2.86e-2)
DTLZ7
3
1.10e-1(3.57e-2)+
7.39e-2(1.52e-2)≈
1.54e-1(1.97e-1)−
1.65e+0(6.43e-1)+
1.20e+0(5.73e-1)+
1.79e-1(1.20e-1)+
1.38e-1(1.53e-1)
4
4.98e-1(1.02e-1)+
2.20e-1(5.76e-2)≈
2.31e-1(1.27e-1)≈
2.82e+0(6.75e-1)+
1.96e+0(7.49e-1)+
7.18e-1(4.34e-1)+
2.80e-1(1.73e-1)
6
1.07e+0(1.62e-1)≈
4.31e-1(3.82e-2)−
4.39e-1(1.48e-1)−
4.80e+0(1.01e+0)+
2.93e+0(7.01e-1)+
3.96e+0(1.88e+0)+
1.46e+0(6.89e-1)
8
1.28e+0(1.27e-1)−
6.29e-1(7.74e-2)−
7.72e-1(1.53e-1)−
6.03e+0(1.87e+0)+
3.63e+0(5.55e-1)+
4.40e+0(2.74e+0)+
2.25e+0(6.88e-1)
10
1.51e+0(1.37e-1)+
9.42e-1(4.54e-2)−
1.11e+0(1.99e-1)−
1.80e+0(3.39e-1)+
1.79e+0(3.78e-1)+
1.46e+0(2.55e-1)+
1.19e+0(8.31e-2)
+/ ≈/−
31/2/2
24/5/6
20/9/6
28/7/0
24/9/2
20/14/1"
REFERENCES,0.6734317343173432,"H.2
IGD+ Results on DTLZ and WFG Optimization Problems
660"
REFERENCES,0.6743542435424354,"Tables 8 and 9 display the IGD+ optimization results of comparison algorithms on DTLZ and WFG
661"
REFERENCES,0.6752767527675276,"optimization problems, respectively. Different from IGD results, although LORA-MOO achieves the
662"
REFERENCES,0.6761992619926199,"smallest IGD+ values on most DTLZ problems, its perform is competitive to KRVEA and KTA2 on
663"
REFERENCES,0.6771217712177122,"WFG problems. However, from the perspective of overall performance, we can still conclude that our
664"
REFERENCES,0.6780442804428044,"LORA-MOO outperforms all comparison algorithms on benchmark optimization problems in terms
665"
REFERENCES,0.6789667896678967,"of IGD+ values. Such a observation is consistent with the results we observed from IGD values.
666"
REFERENCES,0.6798892988929889,"Table 9: Statistical results of the IGD+ value obtained by comparison algorithms on 45 WFG
optimization problems over 30 runs. Symbols ‘+’, ‘≈’, ‘−’ denote LORA-MOO is statistically
signiﬁcantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon
rank sum test (signiﬁcance level is 0.05), respectively. The last row counts the total win/tie/loss
results."
REFERENCES,0.6808118081180812,"Problems
M
ParEGO
KRVEA
KTA2
CSEA
REMO
OREA
LORA-MOO
WFG1
3
1.62e+0(3.90e-2)≈
1.68e+0(9.09e-2)+
1.78e+0(1.38e-1)+
1.68e+0(7.59e-2)+
1.69e+0(1.08e-1)+
1.92e+0(1.27e-1)+
1.63e+0(3.69e-2)
4
1.90e+0(6.54e-2)+
1.99e+0(1.02e-1)+
2.07e+0(1.47e-1)+
1.98e+0(1.06e-1)+
1.90e+0(8.14e-2)+
2.12e+0(8.95e-2)+
1.85e+0(7.27e-2)
6
2.30e+0(4.35e-2)+
2.36e+0(7.09e-2)+
2.41e+0(1.08e-1)+
2.37e+0(9.06e-2)+
2.29e+0(7.24e-2)+
2.39e+0(8.81e-2)+
2.22e+0(6.71e-2)
8
2.64e+0(4.48e-2)+
2.66e+0(7.65e-2)+
2.60e+0(1.15e-1)+
2.62e+0(6.34e-2)+
2.55e+0(6.82e-2)+
2.59e+0(4.96e-2)+
2.49e+0(7.00e-2)
10
2.88e+0(6.44e-2)+
2.78e+0(9.91e-2)+
2.65e+0(1.26e-1)≈
2.71e+0(1.27e-1)+
2.71e+0(1.22e-1)+
2.78e+0(1.04e-1)+
2.62e+0(7.81e-2)
WFG2
3
6.99e-1(9.48e-2)+
2.58e-1(4.09e-2)≈
2.39e-1(7.01e-2)≈
4.68e-1(5.12e-2)+
4.30e-1(9.29e-2)+
3.95e-1(7.73e-2)+
2.47e-1(4.89e-2)
4
9.74e-1(1.65e-1)+
3.21e-1(4.70e-2)−
3.52e-1(5.16e-2)≈
6.27e-1(1.42e-1)+
6.22e-1(1.45e-1)+
6.23e-1(1.69e-1)+
3.52e-1(5.74e-2)
6
1.77e+0(4.19e-1)+
3.84e-1(7.38e-2)−
5.75e-1(1.00e-1)≈
1.02e+0(4.94e-1)+
1.01e+0(4.70e-1)+
1.33e+0(4.17e-1)+
5.29e-1(1.26e-1)
8
2.55e+0(7.48e-1)+
4.09e-1(1.34e-1)−
6.82e-1(1.43e-1)−
1.77e+0(8.24e-1)+
1.52e+0(6.54e-1)+
1.84e+0(4.86e-1)+
8.28e-1(1.52e-1)
10
3.49e+0(1.01e+0)+
4.18e-1(1.81e-1)−
8.19e-1(1.39e-1)−
2.49e+0(9.71e-1)+
2.19e+0(1.13e+0)+
2.67e+0(8.17e-1)+
1.40e+0(2.64e-1)
WFG3
3
5.65e-1(4.14e-2)+
5.26e-1(5.99e-2)+
3.05e-1(6.02e-2)+
4.87e-1(6.70e-2)+
4.42e-1(6.58e-2)+
3.67e-1(4.79e-2)+
2.65e-1(5.63e-2)
4
7.12e-1(6.70e-2)+
6.35e-1(6.90e-2)+
5.33e-1(6.42e-2)+
5.75e-1(7.97e-2)+
5.24e-1(7.33e-2)+
5.47e-1(6.00e-2)+
3.88e-1(6.09e-2)
6
7.42e-1(9.98e-2)+
6.24e-1(1.35e-1)≈
7.25e-1(7.13e-2)+
6.91e-1(8.44e-2)+
5.60e-1(9.53e-2)≈
7.62e-1(6.68e-2)+
6.04e-1(8.95e-2)
8
7.74e-1(1.66e-1)≈
7.26e-1(1.06e-1)≈
8.46e-1(7.67e-2)+
6.83e-1(1.06e-1)−
5.18e-1(1.13e-1)−
8.26e-1(1.01e-1)+
7.58e-1(9.00e-2)
10
5.78e-1(9.80e-2)−
5.54e-1(8.05e-2)−
7.80e-1(8.72e-2)+
4.91e-1(8.69e-2)−
4.07e-1(9.40e-2)−
6.44e-1(1.04e-1)≈
6.92e-1(1.07e-1)
WFG4
3
4.74e-1(4.21e-2)+
3.78e-1(2.17e-2)+
3.42e-1(2.35e-2)+
3.49e-1(3.80e-2)+
3.04e-1(2.99e-2)+
3.66e-1(6.70e-2)+
2.55e-1(3.20e-2)
4
8.04e-1(5.34e-2)+
5.86e-1(3.17e-2)+
6.00e-1(6.42e-2)+
7.81e-1(1.78e-1)+
6.15e-1(1.13e-1)+
9.50e-1(1.50e-1)+
4.85e-1(6.14e-2)
6
1.83e+0(3.74e-1)+
1.20e+0(1.52e-1)≈
1.12e+0(1.55e-1)≈
2.78e+0(4.35e-1)+
2.26e+0(4.42e-1)+
2.56e+0(4.05e-1)+
1.21e+0(2.18e-1)
8
3.39e+0(1.48e+0)≈
2.33e+0(5.25e-1)≈
2.15e+0(3.46e-1)−
5.15e+0(5.66e-1)+
4.22e+0(5.32e-1)+
5.19e+0(4.73e-1)+
2.55e+0(5.66e-1)
10
3.27e+0(2.29e+0)−
4.00e+0(9.92e-1)≈
3.45e+0(3.75e-1)−
7.46e+0(8.64e-1)+
6.61e+0(8.48e-1)+
7.03e+0(6.17e-1)+
3.92e+0(7.04e-1)
WFG5
3
2.07e-1(1.28e-2)−
3.01e-1(3.82e-2)≈
2.38e-1(7.04e-2)−
3.98e-1(3.16e-2)+
3.93e-1(5.70e-2)+
3.60e-1(7.41e-2)+
3.49e-1(1.55e-1)
4
7.09e-1(1.49e-1)−
5.32e-1(4.45e-2)−
4.97e-1(4.53e-2)−
6.09e-1(6.70e-2)−
6.13e-1(5.55e-2)−
9.11e-1(6.00e-2)≈
8.68e-1(7.81e-2)
6
2.38e+0(2.47e-1)+
1.07e+0(1.36e-1)−
1.38e+0(1.64e-1)−
1.89e+0(2.56e-1)+
1.52e+0(2.17e-1)−
2.13e+0(1.77e-1)+
1.71e+0(1.09e-1)
8
4.63e+0(2.89e-1)+
2.11e+0(5.15e-1)−
2.74e+0(4.81e-1)≈
4.13e+0(4.55e-1)+
3.26e+0(4.42e-1)+
4.08e+0(2.55e-1)+
2.88e+0(2.00e-1)
10
6.67e+0(3.78e-1)+
2.48e+0(9.46e-1)−
3.13e+0(5.04e-1)−
5.90e+0(5.30e-1)+
5.16e+0(5.38e-1)+
5.84e+0(5.37e-1)+
3.87e+0(3.50e-1)
WFG6
3
5.52e-1(4.95e-2)+
6.19e-1(6.81e-2)+
5.70e-1(8.76e-2)+
5.71e-1(5.32e-2)+
5.65e-1(5.43e-2)+
5.09e-1(5.01e-2)≈
5.21e-1(1.15e-1)
4
8.09e-1(7.65e-2)≈
7.62e-1(9.60e-2)≈
8.14e-1(6.51e-2)≈
8.33e-1(7.44e-2)≈
7.87e-1(7.30e-2)≈
1.07e+0(7.09e-2)+
8.09e-1(1.12e-1)
6
2.25e+0(5.29e-1)+
1.28e+0(1.52e-1)−
1.52e+0(9.93e-2)≈
2.17e+0(3.22e-1)+
1.74e+0(2.70e-1)+
2.52e+0(2.20e-1)+
1.60e+0(1.59e-1)
8
3.63e+0(9.69e-1)+
1.50e+0(2.46e-1)−
2.66e+0(3.17e-1)≈
3.96e+0(7.85e-1)+
3.41e+0(4.65e-1)+
4.60e+0(3.93e-1)+
2.72e+0(2.95e-1)
10
6.42e+0(8.39e-1)+
1.27e+0(1.06e-1)−
3.67e+0(3.06e-1)+
5.61e+0(7.46e-1)+
4.68e+0(6.46e-1)+
6.05e+0(7.21e-1)+
3.38e+0(4.60e-1)
WFG7
3
5.47e-1(3.21e-2)+
5.38e-1(3.52e-2)+
4.97e-1(3.13e-2)+
4.36e-1(3.98e-2)+
3.94e-1(4.46e-2)+
3.65e-1(5.17e-2)+
2.92e-1(2.42e-2)
4
9.25e-1(9.05e-2)+
7.42e-1(3.50e-2)+
7.47e-1(3.15e-2)+
7.74e-1(1.39e-1)+
6.29e-1(5.40e-2)+
8.46e-1(1.05e-1)+
5.38e-1(5.32e-2)
6
2.85e+0(3.54e-1)+
1.41e+0(1.08e-1)−
1.41e+0(1.36e-1)−
2.29e+0(4.59e-1)+
1.74e+0(2.09e-1)+
2.45e+0(2.22e-1)+
1.61e+0(1.56e-1)
8
5.37e+0(4.28e-1)+
2.59e+0(2.47e-1)−
2.40e+0(3.16e-1)−
4.51e+0(6.31e-1)+
3.62e+0(5.07e-1)+
4.68e+0(3.37e-1)+
3.28e+0(2.02e-1)
10
7.77e+0(5.41e-1)+
3.50e+0(4.76e-1)−
3.47e+0(3.98e-1)−
6.92e+0(5.90e-1)+
5.72e+0(6.38e-1)+
6.70e+0(4.31e-1)+
4.85e+0(3.42e-1)
WFG8
3
7.23e-1(3.76e-2)+
5.89e-1(2.95e-2)≈
4.72e-1(4.57e-2)−
6.59e-1(5.09e-2)+
6.21e-1(4.47e-2)+
6.77e-1(4.74e-2)+
5.79e-1(4.03e-2)
4
1.19e+0(6.76e-2)+
1.01e+0(5.20e-2)−
9.25e-1(5.15e-2)−
1.14e+0(8.61e-2)+
1.07e+0(7.07e-2)≈
1.30e+0(7.86e-2)+
1.07e+0(7.91e-2)
6
2.80e+0(3.88e-1)+
1.82e+0(1.29e-1)−
1.96e+0(1.02e-1)−
2.77e+0(1.80e-1)+
2.58e+0(2.23e-1)+
2.90e+0(2.21e-1)+
2.22e+0(1.47e-1)
8
5.23e+0(4.86e-1)+
2.93e+0(4.96e-1)−
3.31e+0(2.44e-1)−
5.13e+0(3.86e-1)+
4.69e+0(4.63e-1)+
4.98e+0(3.05e-1)+
3.78e+0(3.27e-1)
10
7.43e+0(5.62e-1)+
2.74e+0(1.25e+0)−
4.75e+0(5.99e-1)−
7.03e+0(5.46e-1)+
6.52e+0(3.98e-1)+
6.74e+0(5.72e-1)+
5.03e+0(3.92e-1)
WFG9
3
5.82e-1(7.28e-2)+
5.83e-1(7.77e-2)+
5.56e-1(9.06e-2)+
6.10e-1(1.00e-1)+
5.32e-1(1.12e-1)+
4.51e-1(8.67e-2)+
3.82e-1(8.04e-2)
4
1.00e+0(1.88e-1)+
8.56e-1(1.30e-1)+
8.76e-1(1.43e-1)+
1.00e+0(1.56e-1)+
8.59e-1(2.01e-1)+
8.50e-1(1.15e-1)+
6.77e-1(9.61e-2)
6
2.72e+0(3.83e-1)+
1.72e+0(2.90e-1)+
1.66e+0(2.48e-1)+
2.44e+0(3.25e-1)+
1.87e+0(2.59e-1)+
2.17e+0(1.80e-1)+
1.45e+0(1.42e-1)
8
5.14e+0(5.22e-1)+
3.05e+0(4.65e-1)+
2.82e+0(2.91e-1)≈
4.80e+0(4.05e-1)+
3.95e+0(4.95e-1)+
4.17e+0(3.83e-1)+
2.76e+0(3.72e-1)
10
7.30e+0(5.37e-1)+
4.30e+0(8.61e-1)≈
3.81e+0(4.78e-1)≈
6.66e+0(5.44e-1)+
5.47e+0(6.11e-1)+
5.75e+0(4.84e-1)+
3.98e+0(4.51e-1)
+/ ≈/−
37/4/4
16/10/19
18/11/16
41/1/3
38/3/4
42/3/0"
REFERENCES,0.6817343173431735,"H.3
HV Results on DTLZ and WFG Optimization Problems
667"
REFERENCES,0.6826568265682657,"Tables 10 and 11 report the HV optimization results of comparison algorithms on DTLZ and WFG
668"
REFERENCES,0.683579335793358,"optimization problems, respectively. Since the calculation of HV values on 8- and 10-obj optimization
669"
REFERENCES,0.6845018450184502,"problems is very time-consuming, only the results obtained on 3-, 4-, and 6-objective optimization
670"
REFERENCES,0.6854243542435424,"problems are displayed. Consistent with the IGD an IGD+ results obtained on 3-, 4-, and 6-objectives,
671"
REFERENCES,0.6863468634686347,"our LORA-MOO achieves the best overall performance over all comparison algorithms, showing the
672"
REFERENCES,0.6872693726937269,"effectiveness of LORA-MOO on addressing expensive many-objective optimization problems.
673"
REFERENCES,0.6881918819188192,"H.4
Problems with Different Scales
674"
REFERENCES,0.6891143911439115,"In this subsection, we investigate the optimization performance of LORA-MOO when the number
675"
REFERENCES,0.6900369003690037,"of decision variables D is different. The experimental setups for all comparison algorithms are the
676"
REFERENCES,0.690959409594096,"same as the setups used in previous benchmark optimization problems, but the setup for optimization
677"
REFERENCES,0.6918819188191881,"problems is different:
678"
REFERENCES,0.6928044280442804,"• The optimization problems have D = {5, 10, 20} decision variables and M = 3 objectives.
679"
REFERENCES,0.6937269372693727,"• When D = 5 or 10, a dataset of size 11 D - 1 is used for surrogate initialization. When D
680"
REFERENCES,0.6946494464944649,"= 20, since 11 D - 1 would be greater than our evaluation budget (300), the size of initial
681"
REFERENCES,0.6955719557195572,"dataset is set to 100.
682"
REFERENCES,0.6964944649446494,"Tables 12, 13, and 14 report the obtained IGD, IGD+, and HV values on benchmark optimization
683"
REFERENCES,0.6974169741697417,"problems with different numbers of decision variables D, respectively. It can be seen from Table 12
684"
REFERENCES,0.698339483394834,"Table 10: Statistical results of the HV value obtained by comparison algorithms on 21 DTLZ
optimization problems over 30 runs. Symbols ‘+’, ‘≈’, ‘−’ denote LORA-MOO is statistically
signiﬁcantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon
rank sum test (signiﬁcance level is 0.05), respectively. The last row counts the total win/tie/loss
results."
REFERENCES,0.6992619926199262,"Problems
M
ParEGO
KRVEA
KTA2
CSEA
REMO
OREA
LORA-MOO
DTLZ1
3
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
4
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
6
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
DTLZ2
3
4.53e-2(2.22e-2)+
2.61e-1(4.46e-2)+
3.87e-1(6.59e-3)−
1.55e-1(3.85e-2)+
2.49e-1(3.32e-2)+
3.49e-1(1.33e-2)+
3.77e-1(6.75e-3)
4
6.06e-2(2.65e-2)+
3.71e-1(6.43e-2)+
4.80e-1(1.34e-2)≈
1.95e-1(3.26e-2)+
3.09e-1(4.54e-2)+
3.87e-1(3.31e-2)+
4.75e-1(2.34e-2)
6
1.26e-1(1.87e-2)+
4.85e-1(4.22e-2)+
4.48e-1(7.23e-2)+
2.86e-1(4.80e-2)+
4.00e-1(4.15e-2)+
3.66e-1(3.09e-2)+
6.09e-1(2.27e-2)
DTLZ3
3
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
4
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
6
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
DTLZ4
3
4.20e-4(2.03e-3)+
6.42e-2(5.54e-2)+
8.85e-2(7.53e-2)+
6.53e-2(3.42e-2)+
1.99e-1(6.05e-2)+
2.52e-1(6.75e-2)+
3.24e-1(9.98e-2)
4
3.27e-3(6.73e-3)+
8.79e-2(6.62e-2)+
8.14e-2(5.85e-2)+
1.46e-1(5.25e-2)+
2.52e-1(6.25e-2)+
3.66e-1(8.97e-2)≈
3.93e-1(9.18e-2)
6
2.14e-2(2.69e-2)+
2.05e-1(9.66e-2)+
1.44e-1(8.78e-2)+
3.16e-1(6.50e-2)+
3.53e-1(7.16e-2)+
5.12e-1(5.37e-2)≈
5.17e-1(4.93e-2)
DTLZ5
3
7.49e-3(1.04e-2)+
2.60e-2(1.04e-2)+
8.60e-2(1.99e-3)≈
2.54e-2(9.46e-3)+
4.66e-2(1.02e-2)+
8.48e-2(1.78e-3)≈
8.53e-2(2.03e-3)
4
4.12e-3(5.91e-3)+
2.35e-2(7.10e-3)+
3.31e-2(4.30e-3)+
1.10e-2(4.90e-3)+
1.65e-2(7.08e-3)+
3.55e-2(4.96e-3)≈
3.73e-2(3.97e-3)
6
1.75e-3(1.88e-3)+
1.28e-2(2.87e-3)−
8.26e-3(2.88e-3)≈
5.75e-3(3.24e-3)+
8.48e-3(3.87e-3)≈
9.99e-3(3.78e-3)≈
9.23e-3(3.37e-3)
DTLZ6
3
3.91e-3(7.22e-3)+
0.00e+0(0.00e+0)+
0.00e+0(0.00e+0)+
0.00e+0(0.00e+0)+
0.00e+0(0.00e+0)+
3.52e-2(2.51e-2)+
4.91e-2(2.38e-2)
4
1.78e-3(2.86e-3)+
0.00e+0(0.00e+0)+
2.07e-5(1.11e-4)+
0.00e+0(0.00e+0)+
0.00e+0(0.00e+0)+
2.60e-4(9.64e-4)+
7.45e-3(9.93e-3)
6
1.28e-3(2.18e-3)≈
0.00e+0(0.00e+0)+
1.10e-5(5.88e-5)+
0.00e+0(0.00e+0)+
0.00e+0(0.00e+0)+
1.21e-0(6.50e-0)+
7.42e-4(2.53e-3)
DTLZ7
3
1.81e-1(4.40e-2)+
2.53e-1(9.02e-3)≈
2.81e-1(3.28e-2)−
1.44e-2(2.31e-2)+
2.11e-2(2.95e-2)+
2.23e-1(3.95e-2)+
2.47e-1(3.63e-2)
4
9.45e-2(3.19e-2)+
1.95e-1(1.73e-2)≈
2.36e-1(8.48e-3)−
4.80e-4(2.04e-3)+
1.20e-2(2.15e-2)+
1.04e-1(4.79e-2)+
1.88e-1(3.33e-2)
6
3.12e-2(1.83e-2)+
1.02e-1(1.04e-2)≈
1.57e-1(1.62e-2)−
5.56e-4(2.99e-3)+
1.55e-2(1.81e-2)+
8.81e-4(1.91e-3)+
1.05e-1(2.61e-2)
+/ ≈/−
14/7/0
11/9/1
8/9/4
15/6/0
14/7/0
10/11/0"
REFERENCES,0.7001845018450185,"Table 11: Statistical results of the HV value obtained by comparison algorithms on 27 WFG optimiza-
tion problems over 30 runs. Symbols ‘+’, ‘≈’, ‘−’ denote LORA-MOO is statistically signiﬁcantly
superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum
test (signiﬁcance level is 0.05), respectively. The last row counts the total win/tie/loss results."
REFERENCES,0.7011070110701108,"Problems
M
ParEGO
KRVEA
KTA2
CSEA
REMO
OREA
LORA-MOO
WFG1
3
1.92e-1(2.65e-2)−
1.09e-1(3.15e-2)≈
6.25e-2(3.98e-2)+
8.61e-2(4.91e-2)≈
1.02e-1(4.70e-2)≈
1.57e-2(2.69e-2)+
1.07e-1(3.15e-2)
4
2.07e-1(2.96e-2)−
1.14e-1(5.44e-2)+
7.27e-2(5.18e-2)+
1.17e-1(5.34e-2)+
1.66e-1(3.54e-2)≈
2.84e-2(3.66e-2)+
1.70e-1(4.15e-2)
6
2.16e-1(8.50e-3)≈
1.46e-1(2.93e-2)+
1.11e-1(4.99e-2)+
1.23e-1(5.25e-2)+
1.76e-1(2.54e-2)+
1.12e-1(5.80e-2)+
2.11e-1(2.75e-2)
WFG2
3
5.76e-1(3.88e-2)+
7.46e-1(2.87e-2)≈
7.11e-1(3.38e-2)+
6.57e-1(2.85e-2)+
6.65e-1(4.44e-2)+
6.92e-1(2.96e-2)+
7.42e-1(3.11e-2)
4
6.14e-1(3.28e-2)+
8.20e-1(3.33e-2)−
7.36e-1(3.33e-2)+
7.23e-1(4.35e-2)+
7.06e-1(4.68e-2)+
7.21e-1(3.81e-2)+
7.79e-1(3.30e-2)
6
6.46e-1(5.10e-2)+
8.51e-1(3.38e-2)≈
8.26e-1(3.84e-2)≈
7.80e-1(5.00e-2)+
7.73e-1(5.46e-2)+
7.29e-1(4.17e-2)+
8.39e-1(3.76e-2)
WFG3
3
1.04e-1(1.96e-2)+
1.13e-1(1.80e-2)+
1.90e-1(2.71e-2)≈
1.20e-1(1.90e-2)+
1.27e-1(2.01e-2)+
1.62e-1(2.11e-2)+
1.91e-1(2.20e-2)
4
3.10e-2(2.15e-2)+
3.48e-2(1.41e-2)+
2.73e-2(1.70e-2)+
3.65e-2(2.01e-2)+
4.07e-2(1.92e-2)+
3.10e-2(2.15e-2)+
5.57e-2(1.56e-2)
6
1.10e-2(1.26e-2)−
1.39e-3(2.87e-3)−
0.00e+0(0.00e+0)≈
6.59e-5(2.13e-4)≈
2.96e-3(8.32e-3)−
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
WFG4
3
1.74e-1(1.18e-2)+
2.18e-1(1.10e-2)+
2.44e-1(1.30e-2)+
2.37e-1(1.46e-2)+
2.55e-1(1.52e-2)+
2.66e-1(2.01e-2)+
2.98e-1(1.58e-2)
4
2.12e-1(9.87e-3)+
2.97e-1(1.52e-2)+
3.18e-1(2.01e-2)+
2.96e-1(2.19e-2)+
3.33e-1(2.24e-2)+
2.97e-1(1.89e-2)+
3.91e-1(1.96e-2)
6
2.50e-1(1.18e-2)+
4.09e-1(3.09e-2)+
4.38e-1(2.23e-2)+
3.16e-1(2.50e-2)+
3.78e-1(2.82e-2)+
3.19e-1(2.08e-2)+
4.78e-1(2.39e-2)
WFG5
3
2.98e-1(1.33e-2)−
2.55e-1(2.28e-2)≈
2.98e-1(4.75e-2)−
2.03e-1(1.32e-2)+
2.08e-1(2.74e-2)+
2.45e-1(3.49e-2)+
2.51e-1(6.54e-2)
4
3.19e-1(2.64e-2)−
3.21e-1(2.50e-2)−
3.63e-1(3.37e-2)−
2.92e-1(2.21e-2)−
2.83e-1(2.44e-2)−
2.16e-1(1.31e-2)−
2.05e-1(3.01e-2)
6
3.39e-1(2.37e-2)−
4.17e-1(3.07e-2)−
3.72e-1(3.17e-2)−
3.46e-1(2.51e-2)−
3.53e-1(2.43e-2)−
2.78e-1(1.48e-2)−
2.66e-1(2.60e-2)
WFG6
3
1.15e-1(2.24e-2)+
1.20e-1(2.10e-2)+
1.59e-1(3.72e-2)+
1.29e-1(2.01e-2)+
1.31e-1(1.90e-2)+
1.87e-1(1.98e-2)≈
1.85e-1(4.25e-2)
4
1.83e-1(1.87e-2)+
2.18e-1(3.46e-2)≈
2.17e-1(2.49e-2)≈
1.87e-1(2.16e-2)+
2.05e-1(2.17e-2)≈
1.96e-1(1.60e-2)+
2.33e-1(5.01e-2)
6
2.30e-1(2.14e-2)+
2.75e-1(4.76e-2)+
3.15e-1(2.12e-2)≈
2.49e-1(1.89e-2)+
2.93e-1(3.03e-2)+
2.42e-1(1.28e-2)+
3.11e-1(2.91e-2)
WFG7
3
1.43e-1(8.60e-3)+
1.44e-1(1.11e-2)+
1.75e-1(1.26e-2)+
1.91e-1(1.74e-2)+
2.13e-1(2.05e-2)+
2.53e-1(1.32e-2)+
2.87e-1(1.30e-2)
4
1.91e-1(1.45e-2)+
2.22e-1(1.23e-2)+
2.36e-1(1.09e-2)+
2.42e-1(1.97e-2)+
2.90e-1(2.08e-2)+
2.83e-1(1.74e-2)+
3.66e-1(2.21e-2)
6
2.25e-1(1.42e-2)+
3.24e-1(2.49e-2)+
3.38e-1(2.89e-2)+
3.16e-1(3.37e-2)+
3.77e-1(2.50e-2)+
3.07e-1(1.80e-2)+
4.06e-1(2.28e-2)
WFG8
3
9.39e-2(1.01e-2)+
1.48e-1(9.46e-3)+
2.14e-1(1.61e-2)−
1.24e-1(1.35e-2)+
1.32e-1(1.24e-2)+
1.60e-1(1.44e-2)+
1.84e-1(9.51e-3)
4
1.32e-1(1.22e-2)+
2.03e-1(1.81e-2)≈
2.17e-1(1.76e-2)−
1.57e-1(1.81e-2)+
1.79e-1(1.75e-2)+
1.80e-1(1.38e-2)+
1.95e-1(2.50e-2)
6
1.81e-1(1.26e-2)+
2.59e-1(2.37e-2)−
2.58e-1(1.13e-2)−
2.18e-1(2.14e-2)+
2.62e-1(2.31e-2)−
2.17e-1(1.19e-2)+
2.40e-1(2.32e-2)
WFG9
3
1.22e-1(1.94e-2)+
1.28e-1(2.33e-2)+
1.50e-1(3.21e-2)+
1.39e-1(2.58e-2)+
1.67e-1(3.64e-2)+
2.23e-1(2.82e-2)+
2.46e-1(3.68e-2)
4
1.74e-1(3.27e-2)+
2.08e-1(3.51e-2)+
2.04e-1(2.90e-2)+
1.87e-1(3.11e-2)+
2.35e-1(4.04e-2)+
2.63e-1(2.48e-2)+
3.06e-1(4.82e-2)
6
2.14e-1(2.85e-2)+
3.31e-1(5.50e-2)+
3.65e-1(5.25e-2)≈
2.76e-1(3.85e-2)+
3.62e-1(3.76e-2)+
2.90e-1(2.96e-2)+
3.89e-1(3.60e-2)
+/ ≈/−
20/1/6
16/6/5
15/6/6
23/2/2
20/3/4
23/2/2"
REFERENCES,0.7020295202952029,"that LORA-MOO outperforms all comparison algorithms on DTLZ optimization problems when D
685"
REFERENCES,0.7029520295202952,"= 5, 10, and 20. In addition, KTA2 reaches competitive optimization results on many optimization
686"
REFERENCES,0.7038745387453874,"problems. The observations from Tables 13 and 14 have demonstrated consistent conclusions.
687"
REFERENCES,0.7047970479704797,"Table 12: Statistical results of the IGD value obtained by comparison algorithms on 5D, 10D, and
20D DTLZ optimization problems over 30 runs. Symbols ‘+’, ‘≈’, ‘−’ denote LORA-MOO is
statistically signiﬁcantly superior to, almost equivalent to, and inferior to the compared algorithms in
the Wilcoxon rank sum test (signiﬁcance level is 0.05), respectively. The last row counts the total
win/tie/loss results."
REFERENCES,0.705719557195572,"Problems
D
ParEGO
KRVEA
KTA2
CSEA
REMO
OREA
LORA-MOO
DTLZ1
5
1.24e+1(4.40e+0)+
7.19e+0(3.77e+0)+
4.00e+0(2.28e+0)≈
5.71e+0(2.66e+0)≈
5.97e+0(2.98e+0)≈
2.27e+0(1.45e+0)−
4.78e+0(2.80e+0)
10
5.98e+1(3.81e+0)+
8.88e+1(2.16e+1)+
4.75e+1(1.55e+1)≈
6.30e+1(1.69e+1)+
5.06e+1(1.49e+1)+
4.44e+1(1.38e+1)≈
4.35e+1(1.80e+1)
20
1.59e+2(1.56e+1)−
3.12e+2(3.79e+1)≈
2.48e+2(3.66e+1)−
2.35e+2(3.47e+1)−
2.01e+2(3.95e+1)−
2.94e+2(3.78e+1)≈
2.91e+2(3.98e+1)
DTLZ2
5
1.81e-1(1.26e-2)+
6.06e-2(2.40e-3)+
4.39e-2(1.11e-3)≈
1.03e-1(7.78e-3)+
7.94e-2(7.71e-3)+
6.55e-2(6.87e-3)+
4.36e-2(2.15e-3)
10
3.38e-1(2.84e-2)+
1.32e-1(2.77e-2)+
6.17e-2(3.13e-3)≈
2.26e-1(2.61e-2)+
1.65e-1(2.18e-2)+
8.59e-2(8.51e-3)+
6.19e-2(3.48e-3)
20
7.15e-1(1.21e-1)+
6.66e-1(7.34e-2)+
2.85e-1(5.83e-2)+
5.17e-1(6.66e-2)+
4.00e-1(7.02e-2)+
1.62e-1(3.35e-2)+
1.02e-1(1.36e-2)
DTLZ3
5
3.17e+1(1.17e+1)+
1.91e+1(9.12e+0)≈
1.17e+1(6.12e+0)≈
1.58e+1(7.60e+0)≈
1.61e+1(9.16e+0)≈
6.78e+0(4.79e+0)−
1.51e+1(9.40e+0)
10
1.66e+2(1.31e+1)+
2.43e+2(4.61e+1)+
1.52e+2(4.73e+1)≈
1.62e+2(4.84e+1)≈
1.49e+2(3.88e+1)≈
1.26e+2(3.18e+1)−
1.57e+2(3.83e+1)
20
4.32e+2(1.78e+1)−
9.11e+2(8.72e+1)≈
7.23e+2(1.38e+2)−
7.12e+2(1.10e+2)−
5.86e+2(1.18e+2)−
7.81e+2(1.20e+2)−
8.58e+2(1.31e+2)
DTLZ4
5
4.33e-1(5.55e-2)≈
1.35e-1(6.05e-2)≈
1.68e-1(1.22e-1)≈
4.33e-1(1.54e-1)+
1.60e-1(6.12e-2)≈
2.91e-1(2.44e-1)≈
3.96e-1(3.71e-1)
10
6.70e-1(7.61e-2)+
3.32e-1(1.11e-1)+
3.49e-1(1.09e-1)+
4.62e-1(1.36e-1)+
2.31e-1(1.15e-1)+
2.39e-1(1.65e-1)+
1.89e-1(2.34e-1)
20
1.02e+0(1.04e-1)+
8.32e-1(1.36e-1)+
7.76e-1(1.29e-1)+
7.11e-1(1.74e-1)+
5.51e-1(1.18e-1)+
5.27e-1(2.75e-1)+
4.01e-1(3.28e-1)
DTLZ5
5
4.16e-2(9.61e-3)+
2.31e-2(3.02e-3)+
3.57e-3(2.35e-4)−
2.18e-2(3.22e-3)+
1.49e-2(3.28e-3)+
1.12e-2(5.73e-3)+
4.20e-3(6.92e-4)
10
2.16e-1(4.45e-2)+
1.19e-1(3.38e-2)+
1.34e-2(2.83e-3)≈
1.18e-1(2.56e-2)+
7.36e-2(2.03e-2)+
2.02e-2(4.77e-3)+
1.26e-2(2.55e-3)
20
6.05e-1(1.43e-1)+
6.16e-1(7.41e-2)+
2.13e-1(5.07e-2)+
4.84e-1(8.14e-2)+
3.60e-1(8.07e-2)+
8.11e-2(3.39e-2)+
4.32e-2(1.45e-2)
DTLZ6
5
4.57e-2(1.11e-2)+
4.69e-1(1.54e-1)+
2.68e-1(1.01e-1)+
7.65e-1(4.09e-1)+
4.08e-1(2.59e-1)+
2.57e-2(2.92e-2)≈
2.98e-2(3.53e-2)
10
3.15e-1(1.62e-1)+
3.06e+0(5.21e-1)+
1.83e+0(4.37e-1)+
4.86e+0(6.30e-1)+
4.27e+0(5.49e-1)+
3.09e-1(3.99e-1)+
1.18e-1(1.57e-1)
20
3.54e+0(1.04e+0)≈
1.10e+1(7.15e-1)+
8.72e+0(1.01e+0)≈
1.33e+1(8.48e-1)+
1.23e+1(7.84e-1)+
7.06e+0(3.05e+0)≈
6.81e+0(5.11e+0)
DTLZ7
5
1.87e-1(2.40e-2)+
1.07e-1(1.50e-2)+
6.66e-2(4.28e-2)−
5.67e-1(2.78e-1)+
2.30e-1(1.07e-1)+
3.05e-1(2.01e-1)+
1.41e-1(1.50e-1)
10
2.45e-1(4.80e-2)+
1.35e-1(2.37e-2)≈
2.19e-1(2.40e-1)−
1.75e+0(6.32e-1)+
1.27e+0(5.65e-1)+
2.73e-1(1.58e-1)+
2.01e-1(1.93e-1)
20
2.67e-1(4.98e-2)≈
4.17e-1(2.04e-1)+
4.69e-1(2.56e-1)+
3.69e+0(9.09e-1)+
2.62e+0(7.33e-1)+
4.77e-1(2.53e-1)+
2.99e-1(2.51e-1)
+/ ≈/−
16/3/2
16/5/0
7/9/5
16/3/2
15/4/2
12/5/4"
REFERENCES,0.7066420664206642,"Table 13: Statistical results of the IGD+ value obtained by comparison algorithms on 5D, 10D, and
20D DTLZ optimization problems over 30 runs. Symbols ‘+’, ‘≈’, ‘−’ denote LORA-MOO is
statistically signiﬁcantly superior to, almost equivalent to, and inferior to the compared algorithms in
the Wilcoxon rank sum test (signiﬁcance level is 0.05), respectively. The last row counts the total
win/tie/loss results."
REFERENCES,0.7075645756457565,"Problems
D
ParEGO
KRVEA
KTA2
CSEA
REMO
OREA
LORA-MOO
DTLZ1
5
1.24e+1(4.40e+0)+
7.19e+0(3.77e+0)+
4.00e+0(2.28e+0)≈
5.70e+0(2.67e+0)≈
5.97e+0(2.98e+0)≈
2.27e+0(1.45e+0)−
4.78e+0(2.81e+0)
10
5.98e+1(3.81e+0)+
8.88e+1(2.16e+1)+
4.75e+1(1.55e+1)≈
6.30e+1(1.69e+1)+
5.06e+1(1.49e+1)+
4.44e+1(1.38e+1)≈
4.35e+1(1.80e+1)
20
1.59e+2(1.56e+1)−
3.12e+2(3.79e+1)≈
2.48e+2(3.66e+1)−
2.35e+2(3.47e+1)−
2.01e+2(3.95e+1)−
2.94e+2(3.78e+1)≈
2.91e+2(3.98e+1)
DTLZ2
5
1.01e-1(7.98e-3)+
2.86e-2(9.66e-4)+
1.94e-2(6.20e-4)−
5.24e-2(6.84e-3)+
3.83e-2(4.18e-3)+
3.92e-2(5.96e-3)+
2.30e-2(2.07e-3)
10
2.61e-1(3.63e-2)+
9.22e-2(2.57e-2)+
3.82e-2(3.29e-3)−
1.60e-1(2.76e-2)+
1.01e-1(1.75e-2)+
5.86e-2(8.28e-3)+
4.47e-2(3.35e-3)
20
6.51e-1(1.39e-1)+
6.36e-1(7.19e-2)+
2.61e-1(5.87e-2)+
4.69e-1(6.69e-2)+
3.56e-1(8.04e-2)+
1.39e-1(3.02e-2)+
8.36e-2(1.22e-2)
DTLZ3
5
3.17e+1(1.17e+1)+
1.91e+1(9.13e+0)≈
1.17e+1(6.15e+0)≈
1.58e+1(7.61e+0)≈
1.61e+1(9.16e+0)≈
6.77e+0(4.80e+0)−
1.51e+1(9.41e+0)
10
1.66e+2(1.31e+1)+
2.43e+2(4.61e+1)+
1.52e+2(4.73e+1)≈
1.62e+2(4.84e+1)≈
1.49e+2(3.88e+1)≈
1.26e+2(3.18e+1)−
1.57e+2(3.83e+1)
20
4.32e+2(1.78e+1)−
9.11e+2(8.72e+1)≈
7.23e+2(1.38e+2)−
7.12e+2(1.10e+2)−
5.86e+2(1.18e+2)−
7.81e+2(1.20e+2)−
8.58e+2(1.31e+2)
DTLZ4
5
1.88e-1(3.03e-2)≈
7.41e-2(4.55e-2)≈
7.39e-2(5.63e-2)≈
1.80e-1(7.75e-2)+
6.02e-2(2.08e-2)≈
1.24e-1(1.32e-1)≈
1.96e-1(2.08e-1)
10
4.57e-1(7.52e-2)+
2.66e-1(1.02e-1)+
2.33e-1(8.36e-2)+
2.34e-1(7.76e-2)+
1.32e-1(6.41e-2)+
1.07e-1(9.68e-2)+
8.96e-2(1.25e-1)
20
6.79e-1(1.38e-1)+
7.74e-1(1.34e-1)+
6.65e-1(1.18e-1)+
5.50e-1(1.44e-1)+
4.63e-1(8.22e-2)+
3.16e-1(1.90e-1)+
2.27e-1(2.02e-1)
DTLZ5
5
2.37e-2(3.64e-3)+
1.30e-2(1.76e-3)+
1.65e-3(1.03e-4)−
1.26e-2(2.08e-3)+
7.74e-3(1.49e-3)+
6.37e-3(2.67e-3)+
2.48e-3(5.73e-4)
10
1.60e-1(4.40e-2)+
9.18e-2(2.76e-2)+
8.66e-3(1.96e-3)≈
9.58e-2(2.60e-2)+
5.78e-2(1.81e-2)+
1.59e-2(5.12e-3)+
9.40e-3(1.93e-3)
20
5.52e-1(1.50e-1)+
5.91e-1(7.98e-2)+
2.01e-1(5.29e-2)+
4.67e-1(8.41e-2)+
3.49e-1(8.31e-2)+
7.69e-2(3.31e-2)+
3.93e-2(1.41e-2)
DTLZ6
5
2.47e-2(6.71e-3)+
3.89e-1(1.88e-1)+
2.13e-1(1.02e-1)+
7.13e-1(4.42e-1)+
3.64e-1(2.75e-1)+
9.09e-3(9.88e-3)≈
1.17e-2(1.30e-2)
10
2.42e-1(1.07e-1)+
3.05e+0(5.23e-1)+
1.82e+0(4.48e-1)+
4.85e+0(6.38e-1)+
4.27e+0(5.48e-1)+
2.35e-1(4.14e-1)+
6.74e-2(1.55e-1)
20
3.49e+0(1.06e+0)≈
1.10e+1(7.14e-1)+
8.71e+0(1.01e+0)≈
1.33e+1(8.47e-1)+
1.23e+1(7.85e-1)+
7.04e+0(3.06e+0)≈
6.77e+0(5.15e+0)
DTLZ7
5
7.68e-2(1.31e-2)+
4.68e-2(4.64e-3)+
3.52e-2(2.90e-2)≈
4.46e-1(2.65e-1)+
1.55e-1(8.32e-2)+
2.04e-1(1.80e-1)+
8.42e-2(1.14e-1)
10
1.10e-1(3.57e-2)+
7.39e-2(1.52e-2)≈
1.54e-1(1.97e-1)−
1.65e+0(6.43e-1)+
1.20e+0(5.73e-1)+
1.79e-1(1.20e-1)+
1.38e-1(1.53e-1)
20
1.38e-1(4.67e-2)≈
3.30e-1(1.80e-1)+
3.60e-1(2.27e-1)+
3.65e+0(9.08e-1)+
2.61e+0(7.28e-1)+
4.15e-1(2.30e-1)+
2.28e-1(2.10e-1)
+/ ≈/−
16/3/2
16/5/0
7/8/6
16/3/2
15/4/2
12/5/4 688"
REFERENCES,0.7084870848708487,"Table 14: Statistical results of the HV value obtained by comparison algorithms on 5D, 10D, and
20D DTLZ optimization problems over 30 runs. Symbols ‘+’, ‘≈’, ‘−’ denote LORA-MOO is
statistically signiﬁcantly superior to, almost equivalent to, and inferior to the compared algorithms in
the Wilcoxon rank sum test (signiﬁcance level is 0.05), respectively. The last row counts the total
win/tie/loss results."
REFERENCES,0.709409594095941,"Problems
D
ParEGO
KRVEA
KTA2
CSEA
REMO
OREA
LORA-MOO
DTLZ1
5
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
6.38e-4(3.44e-3)≈
1.10e-2(5.92e-2)
10
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
20
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
DTLZ2
5
2.15e-1(1.98e-2)+
4.00e-1(2.88e-3)+
4.26e-1(1.70e-3)−
3.39e-1(1.61e-2)+
3.78e-1(1.08e-2)+
3.83e-1(1.22e-2)+
4.21e-1(4.35e-3)
10
4.53e-2(2.22e-2)+
2.61e-1(4.46e-2)+
3.87e-1(6.59e-3)−
1.55e-1(3.85e-2)+
2.49e-1(3.32e-2)+
3.49e-1(1.33e-2)+
3.77e-1(6.75e-3)
20
1.02e-3(3.44e-3)+
7.41e-5(3.74e-4)+
8.31e-2(4.46e-2)+
5.91e-3(9.22e-3)+
3.81e-2(2.47e-2)+
2.38e-1(2.81e-2)+
3.01e-1(2.25e-2)
DTLZ3
5
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
10
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
20
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)
DTLZ4
5
2.28e-2(2.65e-2)+
2.93e-1(7.80e-2)≈
3.02e-1(8.32e-2)≈
1.87e-1(5.36e-2)+
3.07e-1(5.76e-2)≈
2.65e-1(1.11e-1)≈
2.49e-1(1.66e-1)
10
4.20e-4(2.03e-3)+
6.42e-2(5.54e-2)+
8.85e-2(7.53e-2)+
6.53e-2(3.42e-2)+
1.99e-1(6.05e-2)+
2.52e-1(6.75e-2)+
3.24e-1(9.98e-2)
20
0.00e+0(0.00e+0)+
0.00e+0(0.00e+0)+
8.09e-4(2.67e-3)+
1.20e-3(5.76e-3)+
6.38e-3(8.46e-3)+
8.86e-2(6.97e-2)+
1.97e-1(1.08e-1)
DTLZ5
5
7.09e-2(2.85e-3)+
7.93e-2(2.59e-3)+
9.36e-2(1.60e-4)−
8.00e-2(2.29e-3)+
8.58e-2(2.49e-3)+
9.14e-2(6.46e-4)+
9.27e-2(5.11e-4)
10
7.49e-3(1.04e-2)+
2.60e-2(1.04e-2)+
8.60e-2(1.99e-3)≈
2.54e-2(9.46e-3)+
4.66e-2(1.02e-2)+
8.48e-2(1.78e-3)≈
8.53e-2(2.03e-3)
20
4.12e-5(2.22e-4)+
0.00e+0(0.00e+0)+
1.00e-2(1.02e-2)+
0.00e+0(0.00e+0)+
9.09e-4(2.11e-3)+
5.09e-2(7.32e-3)+
6.15e-2(7.35e-3)
DTLZ6
5
6.52e-2(7.55e-3)+
6.06e-3(1.28e-2)+
3.10e-2(1.98e-2)+
3.56e-3(1.03e-2)+
1.93e-2(2.10e-2)+
8.70e-2(8.64e-3)−
7.68e-2(1.94e-2)
10
3.91e-3(7.22e-3)+
0.00e+0(0.00e+0)+
0.00e+0(0.00e+0)+
0.00e+0(0.00e+0)+
0.00e+0(0.00e+0)+
3.52e-2(2.51e-2)+
4.91e-2(2.38e-2)
20
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
0.00e+0(0.00e+0)≈
2.06e-3(7.33e-3)
DTLZ7
5
2.29e-1(2.23e-2)+
2.82e-1(5.98e-3)+
3.08e-1(7.28e-3)−
1.90e-1(3.80e-2)+
2.24e-1(2.41e-2)+
2.49e-1(4.23e-2)+
2.84e-1(3.96e-2)
10
1.81e-1(4.40e-2)+
2.53e-1(9.02e-3)≈
2.81e-1(3.28e-2)−
1.44e-2(2.31e-2)+
2.11e-2(2.95e-2)+
2.23e-1(3.95e-2)+
2.47e-1(3.63e-2)
20
1.59e-1(4.85e-2)+
1.56e-1(4.53e-2)+
2.21e-1(3.02e-2)≈
0.00e+0(0.00e+0)+
1.56e-6(8.40e-6)+
1.15e-1(4.03e-2)+
2.03e-1(4.17e-2)
+/ ≈/−
14/7/0
12/9/0
6/10/5
14/7/0
13/8/0
11/9/1"
REFERENCES,0.7103321033210332,"NeurIPS Paper Checklist
689"
CLAIMS,0.7112546125461254,"1. Claims
690"
CLAIMS,0.7121771217712177,"Question: Do the main claims made in the abstract and introduction accurately reﬂect the
691"
CLAIMS,0.7130996309963099,"paper’s contributions and scope?
692"
CLAIMS,0.7140221402214022,"Answer: [Yes]
693"
CLAIMS,0.7149446494464945,"Justiﬁcation: Claims we made accurately reﬂect the paper’s contributions and scope.
694"
CLAIMS,0.7158671586715867,"Guidelines:
695"
CLAIMS,0.716789667896679,"• The answer NA means that the abstract and introduction do not include the claims
696"
CLAIMS,0.7177121771217713,"made in the paper.
697"
CLAIMS,0.7186346863468634,"• The abstract and/or introduction should clearly state the claims made, including the
698"
CLAIMS,0.7195571955719557,"contributions made in the paper and important assumptions and limitations. A No or
699"
CLAIMS,0.7204797047970479,"NA answer to this question will not be perceived well by the reviewers.
700"
CLAIMS,0.7214022140221402,"• The claims made should match theoretical and experimental results, and reﬂect how
701"
CLAIMS,0.7223247232472325,"much the results can be expected to generalize to other settings.
702"
CLAIMS,0.7232472324723247,"• It is ﬁne to include aspirational goals as motivation as long as it is clear that these goals
703"
CLAIMS,0.724169741697417,"are not attained by the paper.
704"
LIMITATIONS,0.7250922509225092,"2. Limitations
705"
LIMITATIONS,0.7260147601476015,"Question: Does the paper discuss the limitations of the work performed by the authors?
706"
LIMITATIONS,0.7269372693726938,"Answer: [Yes]
707"
LIMITATIONS,0.727859778597786,"Justiﬁcation: When the number of objectives is large, there would be many non-dominated
708"
LIMITATIONS,0.7287822878228782,"solutions in the archive, however, we have introduced artiﬁcial ordinal relations in our
709"
LIMITATIONS,0.7297047970479705,"surrogate modeling procedure to alleviate this limitation.
710"
LIMITATIONS,0.7306273062730627,"Guidelines:
711"
LIMITATIONS,0.731549815498155,"• The answer NA means that the paper has no limitation while the answer No means that
712"
LIMITATIONS,0.7324723247232472,"the paper has limitations, but those are not discussed in the paper.
713"
LIMITATIONS,0.7333948339483395,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
714"
LIMITATIONS,0.7343173431734318,"• The paper should point out any strong assumptions and how robust the results are to
715"
LIMITATIONS,0.735239852398524,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
716"
LIMITATIONS,0.7361623616236163,"model well-speciﬁcation, asymptotic approximations only holding locally). The authors
717"
LIMITATIONS,0.7370848708487084,"should reﬂect on how these assumptions might be violated in practice and what the
718"
LIMITATIONS,0.7380073800738007,"implications would be.
719"
LIMITATIONS,0.738929889298893,"• The authors should reﬂect on the scope of the claims made, e.g., if the approach was
720"
LIMITATIONS,0.7398523985239852,"only tested on a few datasets or with a few runs. In general, empirical results often
721"
LIMITATIONS,0.7407749077490775,"depend on implicit assumptions, which should be articulated.
722"
LIMITATIONS,0.7416974169741697,"• The authors should reﬂect on the factors that inﬂuence the performance of the approach.
723"
LIMITATIONS,0.742619926199262,"For example, a facial recognition algorithm may perform poorly when image resolution
724"
LIMITATIONS,0.7435424354243543,"is low or images are taken in low lighting. Or a speech-to-text system might not be
725"
LIMITATIONS,0.7444649446494465,"used reliably to provide closed captions for online lectures because it fails to handle
726"
LIMITATIONS,0.7453874538745388,"technical jargon.
727"
LIMITATIONS,0.746309963099631,"• The authors should discuss the computational efﬁciency of the proposed algorithms
728"
LIMITATIONS,0.7472324723247232,"and how they scale with dataset size.
729"
LIMITATIONS,0.7481549815498155,"• If applicable, the authors should discuss possible limitations of their approach to
730"
LIMITATIONS,0.7490774907749077,"address problems of privacy and fairness.
731"
LIMITATIONS,0.75,"• While the authors might fear that complete honesty about limitations might be used by
732"
LIMITATIONS,0.7509225092250923,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
733"
LIMITATIONS,0.7518450184501845,"limitations that aren’t acknowledged in the paper. The authors should use their best
734"
LIMITATIONS,0.7527675276752768,"judgment and recognize that individual actions in favor of transparency play an impor-
735"
LIMITATIONS,0.753690036900369,"tant role in developing norms that preserve the integrity of the community. Reviewers
736"
LIMITATIONS,0.7546125461254612,"will be speciﬁcally instructed to not penalize honesty concerning limitations.
737"
THEORY ASSUMPTIONS AND PROOFS,0.7555350553505535,"3. Theory Assumptions and Proofs
738"
THEORY ASSUMPTIONS AND PROOFS,0.7564575645756457,"Question: For each theoretical result, does the paper provide the full set of assumptions and
739"
THEORY ASSUMPTIONS AND PROOFS,0.757380073800738,"a complete (and correct) proof?
740"
THEORY ASSUMPTIONS AND PROOFS,0.7583025830258303,"Answer: [NA]
741"
THEORY ASSUMPTIONS AND PROOFS,0.7592250922509225,"Justiﬁcation: Not applicable.
742"
THEORY ASSUMPTIONS AND PROOFS,0.7601476014760148,"Guidelines:
743"
THEORY ASSUMPTIONS AND PROOFS,0.761070110701107,"• The answer NA means that the paper does not include theoretical results.
744"
THEORY ASSUMPTIONS AND PROOFS,0.7619926199261993,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
745"
THEORY ASSUMPTIONS AND PROOFS,0.7629151291512916,"referenced.
746"
THEORY ASSUMPTIONS AND PROOFS,0.7638376383763837,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
747"
THEORY ASSUMPTIONS AND PROOFS,0.764760147601476,"• The proofs can either appear in the main paper or the supplemental material, but if
748"
THEORY ASSUMPTIONS AND PROOFS,0.7656826568265682,"they appear in the supplemental material, the authors are encouraged to provide a short
749"
THEORY ASSUMPTIONS AND PROOFS,0.7666051660516605,"proof sketch to provide intuition.
750"
THEORY ASSUMPTIONS AND PROOFS,0.7675276752767528,"• Inversely, any informal proof provided in the core of the paper should be complemented
751"
THEORY ASSUMPTIONS AND PROOFS,0.768450184501845,"by formal proofs provided in appendix or supplemental material.
752"
THEORY ASSUMPTIONS AND PROOFS,0.7693726937269373,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7702952029520295,"4. Experimental Result Reproducibility
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7712177121771218,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.772140221402214,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
756"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7730627306273062,"of the paper (regardless of whether the code and data are provided or not)?
757"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7739852398523985,"Answer: [Yes]
758"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7749077490774908,"Justiﬁcation: Experimental setups are described in detail.
759"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.775830258302583,"Guidelines:
760"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7767527675276753,"• The answer NA means that the paper does not include experiments.
761"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7776752767527675,"• If the paper includes experiments, a No answer to this question will not be perceived
762"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7785977859778598,"well by the reviewers: Making the paper reproducible is important, regardless of
763"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7795202952029521,"whether the code and data are provided or not.
764"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7804428044280443,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
765"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7813653136531366,"to make their results reproducible or veriﬁable.
766"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7822878228782287,"• Depending on the contribution, reproducibility can be accomplished in various ways.
767"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.783210332103321,"For example, if the contribution is a novel architecture, describing the architecture fully
768"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7841328413284133,"might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation, it may
769"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7850553505535055,"be necessary to either make it possible for others to replicate the model with the same
770"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7859778597785978,"dataset, or provide access to the model. In general. releasing code and data is often
771"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7869003690036901,"one good way to accomplish this, but reproducibility can also be provided via detailed
772"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7878228782287823,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
773"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7887453874538746,"of a large language model), releasing of a model checkpoint, or other means that are
774"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7896678966789668,"appropriate to the research performed.
775"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.790590405904059,"• While NeurIPS does not require releasing code, the conference does require all submis-
776"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7915129151291513,"sions to provide some reasonable avenue for reproducibility, which may depend on the
777"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7924354243542435,"nature of the contribution. For example
778"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7933579335793358,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
779"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.794280442804428,"to reproduce that algorithm.
780"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7952029520295203,"(b) If the contribution is primarily a new model architecture, the paper should describe
781"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7961254612546126,"the architecture clearly and fully.
782"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7970479704797048,"(c) If the contribution is a new model (e.g., a large language model), then there should
783"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7979704797047971,"either be a way to access this model for reproducing the results or a way to reproduce
784"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7988929889298892,"the model (e.g., with an open-source dataset or instructions for how to construct
785"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7998154981549815,"the dataset).
786"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8007380073800738,"(d) We recognize that reproducibility may be tricky in some cases, in which case
787"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.801660516605166,"authors are welcome to describe the particular way they provide for reproducibility.
788"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8025830258302583,"In the case of closed-source models, it may be that access to the model is limited in
789"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8035055350553506,"some way (e.g., to registered users), but it should be possible for other researchers
790"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8044280442804428,"to have some path to reproducing or verifying the results.
791"
OPEN ACCESS TO DATA AND CODE,0.8053505535055351,"5. Open access to data and code
792"
OPEN ACCESS TO DATA AND CODE,0.8062730627306273,"Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
793"
OPEN ACCESS TO DATA AND CODE,0.8071955719557196,"tions to faithfully reproduce the main experimental results, as described in supplemental
794"
OPEN ACCESS TO DATA AND CODE,0.8081180811808119,"material?
795"
OPEN ACCESS TO DATA AND CODE,0.809040590405904,"Answer: [No]
796"
OPEN ACCESS TO DATA AND CODE,0.8099630996309963,"Justiﬁcation: Will release our code after acceptation, or we can provide the code if any
797"
OPEN ACCESS TO DATA AND CODE,0.8108856088560885,"reviewers are interested in it during the review process. Anyway, the details about the code
798"
OPEN ACCESS TO DATA AND CODE,0.8118081180811808,"have already described in the paper.
799"
OPEN ACCESS TO DATA AND CODE,0.8127306273062731,"Guidelines:
800"
OPEN ACCESS TO DATA AND CODE,0.8136531365313653,"• The answer NA means that paper does not include experiments requiring code.
801"
OPEN ACCESS TO DATA AND CODE,0.8145756457564576,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
802"
OPEN ACCESS TO DATA AND CODE,0.8154981549815498,"public/guides/CodeSubmissionPolicy) for more details.
803"
OPEN ACCESS TO DATA AND CODE,0.816420664206642,"• While we encourage the release of code and data, we understand that this might not be
804"
OPEN ACCESS TO DATA AND CODE,0.8173431734317343,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
805"
OPEN ACCESS TO DATA AND CODE,0.8182656826568265,"including code, unless this is central to the contribution (e.g., for a new open-source
806"
OPEN ACCESS TO DATA AND CODE,0.8191881918819188,"benchmark).
807"
OPEN ACCESS TO DATA AND CODE,0.8201107011070111,"• The instructions should contain the exact command and environment needed to run to
808"
OPEN ACCESS TO DATA AND CODE,0.8210332103321033,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
809"
OPEN ACCESS TO DATA AND CODE,0.8219557195571956,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
810"
OPEN ACCESS TO DATA AND CODE,0.8228782287822878,"• The authors should provide instructions on data access and preparation, including how
811"
OPEN ACCESS TO DATA AND CODE,0.8238007380073801,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
812"
OPEN ACCESS TO DATA AND CODE,0.8247232472324724,"• The authors should provide scripts to reproduce all experimental results for the new
813"
OPEN ACCESS TO DATA AND CODE,0.8256457564575646,"proposed method and baselines. If only a subset of experiments are reproducible, they
814"
OPEN ACCESS TO DATA AND CODE,0.8265682656826568,"should state which ones are omitted from the script and why.
815"
OPEN ACCESS TO DATA AND CODE,0.827490774907749,"• At submission time, to preserve anonymity, the authors should release anonymized
816"
OPEN ACCESS TO DATA AND CODE,0.8284132841328413,"versions (if applicable).
817"
OPEN ACCESS TO DATA AND CODE,0.8293357933579336,"• Providing as much information as possible in supplemental material (appended to the
818"
OPEN ACCESS TO DATA AND CODE,0.8302583025830258,"paper) is recommended, but including URLs to data and code is permitted.
819"
OPEN ACCESS TO DATA AND CODE,0.8311808118081181,"6. Experimental Setting/Details
820"
OPEN ACCESS TO DATA AND CODE,0.8321033210332104,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
821"
OPEN ACCESS TO DATA AND CODE,0.8330258302583026,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
822"
OPEN ACCESS TO DATA AND CODE,0.8339483394833949,"results?
823"
OPEN ACCESS TO DATA AND CODE,0.834870848708487,"Answer: [Yes]
824"
OPEN ACCESS TO DATA AND CODE,0.8357933579335793,"Justiﬁcation: We have described all the details about of experiments.
825"
OPEN ACCESS TO DATA AND CODE,0.8367158671586716,"Guidelines:
826"
OPEN ACCESS TO DATA AND CODE,0.8376383763837638,"• The answer NA means that the paper does not include experiments.
827"
OPEN ACCESS TO DATA AND CODE,0.8385608856088561,"• The experimental setting should be presented in the core of the paper to a level of detail
828"
OPEN ACCESS TO DATA AND CODE,0.8394833948339483,"that is necessary to appreciate the results and make sense of them.
829"
OPEN ACCESS TO DATA AND CODE,0.8404059040590406,"• The full details can be provided either with the code, in appendix, or as supplemental
830"
OPEN ACCESS TO DATA AND CODE,0.8413284132841329,"material.
831"
OPEN ACCESS TO DATA AND CODE,0.8422509225092251,"7. Experiment Statistical Signiﬁcance
832"
OPEN ACCESS TO DATA AND CODE,0.8431734317343174,"Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate
833"
OPEN ACCESS TO DATA AND CODE,0.8440959409594095,"information about the statistical signiﬁcance of the experiments?
834"
OPEN ACCESS TO DATA AND CODE,0.8450184501845018,"Answer: [Yes]
835"
OPEN ACCESS TO DATA AND CODE,0.8459409594095941,"Justiﬁcation: We have conducted statistical tests in our experiments, error bars are plotted in
836"
OPEN ACCESS TO DATA AND CODE,0.8468634686346863,"ﬁgures.
837"
OPEN ACCESS TO DATA AND CODE,0.8477859778597786,"Guidelines:
838"
OPEN ACCESS TO DATA AND CODE,0.8487084870848709,"• The answer NA means that the paper does not include experiments.
839"
OPEN ACCESS TO DATA AND CODE,0.8496309963099631,"• The authors should answer ""Yes"" if the results are accompanied by error bars, conﬁ-
840"
OPEN ACCESS TO DATA AND CODE,0.8505535055350554,"dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
841"
OPEN ACCESS TO DATA AND CODE,0.8514760147601476,"the main claims of the paper.
842"
OPEN ACCESS TO DATA AND CODE,0.8523985239852399,"• The factors of variability that the error bars are capturing should be clearly stated (for
843"
OPEN ACCESS TO DATA AND CODE,0.8533210332103321,"example, train/test split, initialization, random drawing of some parameter, or overall
844"
OPEN ACCESS TO DATA AND CODE,0.8542435424354243,"run with given experimental conditions).
845"
OPEN ACCESS TO DATA AND CODE,0.8551660516605166,"• The method for calculating the error bars should be explained (closed form formula,
846"
OPEN ACCESS TO DATA AND CODE,0.8560885608856088,"call to a library function, bootstrap, etc.)
847"
OPEN ACCESS TO DATA AND CODE,0.8570110701107011,"• The assumptions made should be given (e.g., Normally distributed errors).
848"
OPEN ACCESS TO DATA AND CODE,0.8579335793357934,"• It should be clear whether the error bar is the standard deviation or the standard error
849"
OPEN ACCESS TO DATA AND CODE,0.8588560885608856,"of the mean.
850"
OPEN ACCESS TO DATA AND CODE,0.8597785977859779,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
851"
OPEN ACCESS TO DATA AND CODE,0.8607011070110702,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
852"
OPEN ACCESS TO DATA AND CODE,0.8616236162361623,"of Normality of errors is not veriﬁed.
853"
OPEN ACCESS TO DATA AND CODE,0.8625461254612546,"• For asymmetric distributions, the authors should be careful not to show in tables or
854"
OPEN ACCESS TO DATA AND CODE,0.8634686346863468,"ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
855"
OPEN ACCESS TO DATA AND CODE,0.8643911439114391,"error rates).
856"
OPEN ACCESS TO DATA AND CODE,0.8653136531365314,"• If error bars are reported in tables or plots, The authors should explain in the text how
857"
OPEN ACCESS TO DATA AND CODE,0.8662361623616236,"they were calculated and reference the corresponding ﬁgures or tables in the text.
858"
EXPERIMENTS COMPUTE RESOURCES,0.8671586715867159,"8. Experiments Compute Resources
859"
EXPERIMENTS COMPUTE RESOURCES,0.8680811808118081,"Question: For each experiment, does the paper provide sufﬁcient information on the com-
860"
EXPERIMENTS COMPUTE RESOURCES,0.8690036900369004,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
861"
EXPERIMENTS COMPUTE RESOURCES,0.8699261992619927,"the experiments?
862"
EXPERIMENTS COMPUTE RESOURCES,0.8708487084870848,"Answer: [Yes]
863"
EXPERIMENTS COMPUTE RESOURCES,0.8717712177121771,"Justiﬁcation: A runtime comparison experiment is reported in the end of our experiment
864"
EXPERIMENTS COMPUTE RESOURCES,0.8726937269372693,"section. We did not provide information about compute workers and memory since our
865"
EXPERIMENTS COMPUTE RESOURCES,0.8736162361623616,"experiments do not have speciﬁc requirements on memory or other computation resource.
866"
EXPERIMENTS COMPUTE RESOURCES,0.8745387453874539,"Guidelines:
867"
EXPERIMENTS COMPUTE RESOURCES,0.8754612546125461,"• The answer NA means that the paper does not include experiments.
868"
EXPERIMENTS COMPUTE RESOURCES,0.8763837638376384,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
869"
EXPERIMENTS COMPUTE RESOURCES,0.8773062730627307,"or cloud provider, including relevant memory and storage.
870"
EXPERIMENTS COMPUTE RESOURCES,0.8782287822878229,"• The paper should provide the amount of compute required for each of the individual
871"
EXPERIMENTS COMPUTE RESOURCES,0.8791512915129152,"experimental runs as well as estimate the total compute.
872"
EXPERIMENTS COMPUTE RESOURCES,0.8800738007380073,"• The paper should disclose whether the full research project required more compute
873"
EXPERIMENTS COMPUTE RESOURCES,0.8809963099630996,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
874"
EXPERIMENTS COMPUTE RESOURCES,0.8819188191881919,"didn’t make it into the paper).
875"
CODE OF ETHICS,0.8828413284132841,"9. Code Of Ethics
876"
CODE OF ETHICS,0.8837638376383764,"Question: Does the research conducted in the paper conform, in every respect, with the
877"
CODE OF ETHICS,0.8846863468634686,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
878"
CODE OF ETHICS,0.8856088560885609,"Answer: [NA]
879"
CODE OF ETHICS,0.8865313653136532,"Justiﬁcation: Not applicable.
880"
CODE OF ETHICS,0.8874538745387454,"Guidelines:
881"
CODE OF ETHICS,0.8883763837638377,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
882"
CODE OF ETHICS,0.8892988929889298,"• If the authors answer No, they should explain the special circumstances that require a
883"
CODE OF ETHICS,0.8902214022140221,"deviation from the Code of Ethics.
884"
CODE OF ETHICS,0.8911439114391144,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
885"
CODE OF ETHICS,0.8920664206642066,"eration due to laws or regulations in their jurisdiction).
886"
BROADER IMPACTS,0.8929889298892989,"10. Broader Impacts
887"
BROADER IMPACTS,0.8939114391143912,"Question: Does the paper discuss both potential positive societal impacts and negative
888"
BROADER IMPACTS,0.8948339483394834,"societal impacts of the work performed?
889"
BROADER IMPACTS,0.8957564575645757,"Answer: [No]
890"
BROADER IMPACTS,0.8966789667896679,"Justiﬁcation: Our algorithm has no potential negative social impacts.
891"
BROADER IMPACTS,0.8976014760147601,"Guidelines:
892"
BROADER IMPACTS,0.8985239852398524,"• The answer NA means that there is no societal impact of the work performed.
893"
BROADER IMPACTS,0.8994464944649446,"• If the authors answer NA or No, they should explain why their work has no societal
894"
BROADER IMPACTS,0.9003690036900369,"impact or why the paper does not address societal impact.
895"
BROADER IMPACTS,0.9012915129151291,"• Examples of negative societal impacts include potential malicious or unintended uses
896"
BROADER IMPACTS,0.9022140221402214,"(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
897"
BROADER IMPACTS,0.9031365313653137,"(e.g., deployment of technologies that could make decisions that unfairly impact speciﬁc
898"
BROADER IMPACTS,0.9040590405904059,"groups), privacy considerations, and security considerations.
899"
BROADER IMPACTS,0.9049815498154982,"• The conference expects that many papers will be foundational research and not tied
900"
BROADER IMPACTS,0.9059040590405905,"to particular applications, let alone deployments. However, if there is a direct path to
901"
BROADER IMPACTS,0.9068265682656826,"any negative applications, the authors should point it out. For example, it is legitimate
902"
BROADER IMPACTS,0.9077490774907749,"to point out that an improvement in the quality of generative models could be used to
903"
BROADER IMPACTS,0.9086715867158671,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
904"
BROADER IMPACTS,0.9095940959409594,"that a generic algorithm for optimizing neural networks could enable people to train
905"
BROADER IMPACTS,0.9105166051660517,"models that generate Deepfakes faster.
906"
BROADER IMPACTS,0.9114391143911439,"• The authors should consider possible harms that could arise when the technology is
907"
BROADER IMPACTS,0.9123616236162362,"being used as intended and functioning correctly, harms that could arise when the
908"
BROADER IMPACTS,0.9132841328413284,"technology is being used as intended but gives incorrect results, and harms following
909"
BROADER IMPACTS,0.9142066420664207,"from (intentional or unintentional) misuse of the technology.
910"
BROADER IMPACTS,0.915129151291513,"• If there are negative societal impacts, the authors could also discuss possible mitigation
911"
BROADER IMPACTS,0.9160516605166051,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
912"
BROADER IMPACTS,0.9169741697416974,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
913"
BROADER IMPACTS,0.9178966789667896,"feedback over time, improving the efﬁciency and accessibility of ML).
914"
SAFEGUARDS,0.9188191881918819,"11. Safeguards
915"
SAFEGUARDS,0.9197416974169742,"Question: Does the paper describe safeguards that have been put in place for responsible
916"
SAFEGUARDS,0.9206642066420664,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
917"
SAFEGUARDS,0.9215867158671587,"image generators, or scraped datasets)?
918"
SAFEGUARDS,0.922509225092251,"Answer: [No]
919"
SAFEGUARDS,0.9234317343173432,"Justiﬁcation: Code will be released after acceptation, it would be open access, no safeguards
920"
SAFEGUARDS,0.9243542435424354,"are required.
921"
SAFEGUARDS,0.9252767527675276,"Guidelines:
922"
SAFEGUARDS,0.9261992619926199,"• The answer NA means that the paper poses no such risks.
923"
SAFEGUARDS,0.9271217712177122,"• Released models that have a high risk for misuse or dual-use should be released with
924"
SAFEGUARDS,0.9280442804428044,"necessary safeguards to allow for controlled use of the model, for example by requiring
925"
SAFEGUARDS,0.9289667896678967,"that users adhere to usage guidelines or restrictions to access the model or implementing
926"
SAFEGUARDS,0.9298892988929889,"safety ﬁlters.
927"
SAFEGUARDS,0.9308118081180812,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
928"
SAFEGUARDS,0.9317343173431735,"should describe how they avoided releasing unsafe images.
929"
SAFEGUARDS,0.9326568265682657,"• We recognize that providing effective safeguards is challenging, and many papers do
930"
SAFEGUARDS,0.933579335793358,"not require this, but we encourage authors to take this into account and make a best
931"
SAFEGUARDS,0.9345018450184502,"faith effort.
932"
LICENSES FOR EXISTING ASSETS,0.9354243542435424,"12. Licenses for existing assets
933"
LICENSES FOR EXISTING ASSETS,0.9363468634686347,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
934"
LICENSES FOR EXISTING ASSETS,0.9372693726937269,"the paper, properly credited and are the license and terms of use explicitly mentioned and
935"
LICENSES FOR EXISTING ASSETS,0.9381918819188192,"properly respected?
936"
LICENSES FOR EXISTING ASSETS,0.9391143911439115,"Answer: [Yes]
937"
LICENSES FOR EXISTING ASSETS,0.9400369003690037,"Justiﬁcation: We have cited the existing assets we used in our paper.
938"
LICENSES FOR EXISTING ASSETS,0.940959409594096,"Guidelines:
939"
LICENSES FOR EXISTING ASSETS,0.9418819188191881,"• The answer NA means that the paper does not use existing assets.
940"
LICENSES FOR EXISTING ASSETS,0.9428044280442804,"• The authors should cite the original paper that produced the code package or dataset.
941"
LICENSES FOR EXISTING ASSETS,0.9437269372693727,"• The authors should state which version of the asset is used and, if possible, include a
942"
LICENSES FOR EXISTING ASSETS,0.9446494464944649,"URL.
943"
LICENSES FOR EXISTING ASSETS,0.9455719557195572,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
944"
LICENSES FOR EXISTING ASSETS,0.9464944649446494,"• For scraped data from a particular source (e.g., website), the copyright and terms of
945"
LICENSES FOR EXISTING ASSETS,0.9474169741697417,"service of that source should be provided.
946"
LICENSES FOR EXISTING ASSETS,0.948339483394834,"• If assets are released, the license, copyright information, and terms of use in the
947"
LICENSES FOR EXISTING ASSETS,0.9492619926199262,"package should be provided. For popular datasets, paperswithcode.com/datasets
948"
LICENSES FOR EXISTING ASSETS,0.9501845018450185,"has curated licenses for some datasets. Their licensing guide can help determine the
949"
LICENSES FOR EXISTING ASSETS,0.9511070110701108,"license of a dataset.
950"
LICENSES FOR EXISTING ASSETS,0.9520295202952029,"• For existing datasets that are re-packaged, both the original license and the license of
951"
LICENSES FOR EXISTING ASSETS,0.9529520295202952,"the derived asset (if it has changed) should be provided.
952"
LICENSES FOR EXISTING ASSETS,0.9538745387453874,"• If this information is not available online, the authors are encouraged to reach out to
953"
LICENSES FOR EXISTING ASSETS,0.9547970479704797,"the asset’s creators.
954"
NEW ASSETS,0.955719557195572,"13. New Assets
955"
NEW ASSETS,0.9566420664206642,"Question: Are new assets introduced in the paper well documented and is the documentation
956"
NEW ASSETS,0.9575645756457565,"provided alongside the assets?
957"
NEW ASSETS,0.9584870848708487,"Answer: [NA]
958"
NEW ASSETS,0.959409594095941,"Justiﬁcation: We did not introduce any new assets.
959"
NEW ASSETS,0.9603321033210332,"Guidelines:
960"
NEW ASSETS,0.9612546125461254,"• The answer NA means that the paper does not release new assets.
961"
NEW ASSETS,0.9621771217712177,"• Researchers should communicate the details of the dataset/code/model as part of their
962"
NEW ASSETS,0.9630996309963099,"submissions via structured templates. This includes details about training, license,
963"
NEW ASSETS,0.9640221402214022,"limitations, etc.
964"
NEW ASSETS,0.9649446494464945,"• The paper should discuss whether and how consent was obtained from people whose
965"
NEW ASSETS,0.9658671586715867,"asset is used.
966"
NEW ASSETS,0.966789667896679,"• At submission time, remember to anonymize your assets (if applicable). You can either
967"
NEW ASSETS,0.9677121771217713,"create an anonymized URL or include an anonymized zip ﬁle.
968"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9686346863468634,"14. Crowdsourcing and Research with Human Subjects
969"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9695571955719557,"Question: For crowdsourcing experiments and research with human subjects, does the paper
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9704797047970479,"include the full text of instructions given to participants and screenshots, if applicable, as
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714022140221402,"well as details about compensation (if any)?
972"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9723247232472325,"Answer: [NA]
973"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9732472324723247,"Justiﬁcation: We do not have any experiments or research with human subjects.
974"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974169741697417,"Guidelines:
975"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9750922509225092,"• The answer NA means that the paper does not involve crowdsourcing nor research with
976"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760147601476015,"human subjects.
977"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769372693726938,"• Including this information in the supplemental material is ﬁne, but if the main contribu-
978"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977859778597786,"tion of the paper involves human subjects, then as much detail as possible should be
979"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787822878228782,"included in the main paper.
980"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9797047970479705,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
981"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806273062730627,"or other labor should be paid at least the minimum wage in the country of the data
982"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981549815498155,"collector.
983"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824723247232472,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
984"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833948339483395,"Subjects
985"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9843173431734318,"Question: Does the paper describe potential risks incurred by study participants, whether
986"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985239852398524,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
987"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861623616236163,"approvals (or an equivalent approval/review based on the requirements of your country or
988"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9870848708487084,"institution) were obtained?
989"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880073800738007,"Answer: [NA]
990"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988929889298893,"Justiﬁcation: We do not have any experiments or research with human subjects.
991"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9898523985239852,"Guidelines:
992"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907749077490775,"• The answer NA means that the paper does not involve crowdsourcing nor research with
993"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916974169741697,"human subjects.
994"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992619926199262,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
995"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935424354243543,"may be required for any human subjects research. If you obtained IRB approval, you
996"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944649446494465,"should clearly state this in the paper.
997"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953874538745388,"• We recognize that the procedures for this may vary signiﬁcantly between institutions
998"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996309963099631,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
999"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972324723247232,"guidelines for their institution.
1000"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9981549815498155,"• For initial submissions, do not include any information that would break anonymity (if
1001"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990774907749077,"applicable), such as the institution conducting the review.
1002"
