Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015698587127158557,"We argue to use Differentially-Private Local Stochastic Gradient Descent (DP-
1"
ABSTRACT,0.0031397174254317113,"LSGD) in both centralized and distributed setups, and explain why DP-LSGD
2"
ABSTRACT,0.004709576138147566,"enjoys higher clipping efﬁciency and produces less clipping bias compared to clas-
3"
ABSTRACT,0.006279434850863423,"sic Differentially-Private Stochastic Gradient Descent (DP-SGD). For both convex
4"
ABSTRACT,0.007849293563579277,"and non-convex optimization, we present generic analysis on noisy synchronized-
5"
ABSTRACT,0.009419152276295133,"only iterates in LSGD, the building block of federated learning, and study its
6"
ABSTRACT,0.01098901098901099,"applications to differentially-private gradient methods with clipping-based sen-
7"
ABSTRACT,0.012558869701726845,"sitivity control. We point out that given the current decompose-then-compose
8"
ABSTRACT,0.0141287284144427,"framework, there is no essential gap between the privacy analysis of centralized
9"
ABSTRACT,0.015698587127158554,"and distributed learning, and DP-SGD is a special case of DP-LSGD. We thus build
10"
ABSTRACT,0.01726844583987441,"a uniﬁed framework to characterize the clipping bias via the second moment of
11"
ABSTRACT,0.018838304552590265,"local updates, which initiates a direction to systematically instruct DP optimization
12"
ABSTRACT,0.02040816326530612,"by variance reduction. We show DP-LSGD with multiple local iterations can
13"
ABSTRACT,0.02197802197802198,"produce more concentrated local updates and then enables a more efﬁcient exploita-
14"
ABSTRACT,0.023547880690737835,"tion of the clipping budget with a better utility-privacy tradeoff. In addition, we
15"
ABSTRACT,0.02511773940345369,"prove that DP-LSGD can converge faster to a small neighborhood of global/local
16"
ABSTRACT,0.026687598116169546,"optimum compared to regular DP-SGD. Thorough experiments on practical deep
17"
ABSTRACT,0.0282574568288854,"learning tasks are provided to support our developed theory.
18"
INTRODUCTION,0.029827315541601257,"1
Introduction
19"
INTRODUCTION,0.03139717425431711,"Local Stochastic Gradient Descent (LSGD) [1, 2] and (Local/Client-Level) Differential Privacy (DP)
20"
INTRODUCTION,0.03296703296703297,"[3, 4, 5] are two popular methods to address the issues of communication efﬁciency and data privacy,
21"
INTRODUCTION,0.03453689167974882,"respectively. Rooted in the FedAvg framework ﬁrst proposed in [6], instead of communicating and
22"
INTRODUCTION,0.03610675039246468,"synchronizing on the local updates from each user at each iteration, LSGD [1] randomly samples
23"
INTRODUCTION,0.03767660910518053,"participants to perform gradient descent on their local data in parallel and only aggregates their local
24"
INTRODUCTION,0.03924646781789639,"updates periodically. Though LSGD is a simple generalization of SGD to a distributed setup with a
25"
INTRODUCTION,0.04081632653061224,"lower synchronization frequency, empirically it is known to produce promising performance, with
26"
INTRODUCTION,0.0423861852433281,"regard to both communication efﬁciency and convergence rate [7]. When each user holds i.i.d. data,
27"
INTRODUCTION,0.04395604395604396,"LSGD provably achieves a linear speedup in the number of users with also asymptotic improvements
28"
INTRODUCTION,0.04552590266875981,"on the communication overhead over regular distributed SGD to produce equivalent accuracy [1, 2].
29"
INTRODUCTION,0.04709576138147567,"As for privacy preservation, DP [3, 8] provides a semantically precise way to quantify the data leakage
30"
INTRODUCTION,0.04866562009419152,"from any processing. At a high level, DP is an input-independent guarantee which ensures that an ad-
31"
INTRODUCTION,0.05023547880690738,"versary cannot infer the participation of an individual datapoint easily from the release. For example,
32"
INTRODUCTION,0.05180533751962323,"the classic (✏, δ)-DP with small security parameters ✏and δ implies a large Type I or Type II error for
33"
INTRODUCTION,0.05337519623233909,"an adversarial hypothesis testing to guess whether an arbitrary individual is involved in the processing
34"
INTRODUCTION,0.054945054945054944,"[9]. In DP research, one key problem is to determine the sensitivity, the worst-case inﬂuence/change
35"
INTRODUCTION,0.0565149136577708,"on the output of the objective processing after arbitrarily replacing an individual in an input set. Only
36"
INTRODUCTION,0.058084772370486655,"with tractable sensitivity, one can then apply proper randomization/perturbation such as the Gaussian
37"
INTRODUCTION,0.059654631083202514,"or Laplace mechanism [10] to produce required security parameters. Unfortunately, sensitivity is
38"
INTRODUCTION,0.061224489795918366,"in general NP-hard to compute [11]. To this end, in practice, a commonly-applied alternative is the
39"
INTRODUCTION,0.06279434850863422,"decompose-then-compose framework: a complicated processing is ﬁrst (approximately) decomposed
40"
INTRODUCTION,0.06436420722135008,"into several simpler (possibly adaptive) subroutines such as mean estimation, each of whose sen-
41"
INTRODUCTION,0.06593406593406594,"sitivity is controllable. A white-box adversary is then assumed who can observe the intermediate
42"
INTRODUCTION,0.06750392464678179,"computations, and an upper bound on the privacy loss is derived by the composition of the leakage
43"
INTRODUCTION,0.06907378335949764,"from the virtual release in each step [12].
44"
INTRODUCTION,0.0706436420722135,"In the applications of machine learning, where the processing function returns a model trained on
45"
INTRODUCTION,0.07221350078492936,"possibly sensitive data, arguably the most popular and generic DP privatization method is DP-SGD
46"
INTRODUCTION,0.07378335949764521,"[13, 14]. As a representative of the above-mentioned decompose-then-compose framework, DP-SGD
47"
INTRODUCTION,0.07535321821036106,"views the SGD as a sequence of adaptive gradient mean estimations. To ensure a bounded sensitivity
48"
INTRODUCTION,0.07692307692307693,"guarantee, each per-sample gradient is clipped, usually, in l2-norm [14] to some constant c, which is
49"
INTRODUCTION,0.07849293563579278,"essentially a projection to an l2-norm ball of radius c. Noise, which is determined by both the number
50"
INTRODUCTION,0.08006279434850863,"of iterations T and the clipping threshold c (sensitivity bound), is then added to the clipped stochastic
51"
INTRODUCTION,0.08163265306122448,"gradient in each iteration to produce satisﬁed DP parameters (✏, δ) under T-fold composition. A wider
52"
INTRODUCTION,0.08320251177394035,"dimension and a longer convergence time T will consequently require a larger DP noise. Though the
53"
INTRODUCTION,0.0847723704866562,"implementation of DP-SGD does not require any additional assumptions on either model or training
54"
INTRODUCTION,0.08634222919937205,"data, it is notorious for heavy utility loss, especially for deep learning. Moreover, the understanding
55"
INTRODUCTION,0.08791208791208792,"of the clipping bias from this artiﬁcial sensitivity control remains limited. In general, due to the bias,
56"
INTRODUCTION,0.08948194662480377,"clipped SGD will not converge even without noise perturbation [15, 16].
57"
INTRODUCTION,0.09105180533751962,"Given the artiﬁcial assumption that DP-SGD releases the intermediate computations, there is no
58"
INTRODUCTION,0.09262166405023547,"essential gap between the privacy analysis of the centralized and local SGD, except that in the
59"
INTRODUCTION,0.09419152276295134,"distributed setup one may apply different DP metrics such as Local DP (LDP) [4] or client-level DP
60"
INTRODUCTION,0.09576138147566719,"[5] to consider the privacy preservation for each user’s local data. More interestingly, it is worth
61"
INTRODUCTION,0.09733124018838304,"noting the connection among different problems in federated learning and DP-SGD that are essentially
62"
INTRODUCTION,0.0989010989010989,"equivalent. First, it is not hard to see that DP-SGD is a special case of DP-LSGD. DP-SGD can
63"
INTRODUCTION,0.10047095761381476,"be viewed as: n nodes, each holds a sample, and a virtual server collects the clipped stochastic
64"
INTRODUCTION,0.10204081632653061,"gradient from a subset of sampled nodes in every iteration, and publishes a noisy gradient descent.
65"
INTRODUCTION,0.10361067503924647,"DP-LSGD can be similarly deﬁned where the only difference is that the server may not synchronize
66"
INTRODUCTION,0.10518053375196232,"on each iteration, but clips and aggregates a linear combinations of local gradients, periodically.
67"
INTRODUCTION,0.10675039246467818,"Thus, as a primary concern in federated learning, a smaller communication overhead in a lower
68"
INTRODUCTION,0.10832025117739404,"synchronization/aggregation frequency would also imply less leakage and a smaller composition
69"
INTRODUCTION,0.10989010989010989,"bound of privacy loss. On the other hand, the study on the utility loss by perturbation and artiﬁcial
70"
INTRODUCTION,0.11145996860282574,"sensitivity control (clipping) could also be used to analyze federated learning with compressed
71"
INTRODUCTION,0.1130298273155416,"communication [17] where there exists quantiﬁcation error in broadcasted local updates. Therefore,
72"
INTRODUCTION,0.11459968602825746,"in this paper, we aim to provide a uniﬁed analysis for both noisy LSGD and DP-LSGD/SGD to get
73"
INTRODUCTION,0.11616954474097331,"new insights. Before we can build useful theory to capture these concerns from different perspectives,
74"
INTRODUCTION,0.11773940345368916,"several technical challenges need to be addressed.
75"
INTRODUCTION,0.11930926216640503,"Utility of ""Synchronized/Published"" Iterate Only: Many existing convergence results [2, 18, 19,
76"
INTRODUCTION,0.12087912087912088,"20, 21] on non-private LSGD are developed on the (weighted) average of all iterates. These include
77"
INTRODUCTION,0.12244897959183673,"the intermediate iterates produced during the local updates from each user/node, which will not be
78"
INTRODUCTION,0.12401883830455258,"exposed or shared. To properly characterize the effect of perturbation, a more appropriate and realistic
79"
INTRODUCTION,0.12558869701726844,"convergence guarantee is to measure the performance of synchronized (shared) iterates only. This is
80"
INTRODUCTION,0.1271585557299843,"also important to help understand the practical performance of LSGD as neither the server nor users
81"
INTRODUCTION,0.12872841444270017,"have access to all intermediate computations. Such measurement is especially necessary when we
82"
INTRODUCTION,0.130298273155416,"apply LSGD in a private version: the utility of concern is only with respect to the released outputs,
83"
INTRODUCTION,0.13186813186813187,"and anything assumed to be published would incur privacy loss and increase the scale of DP noise.
84"
INTRODUCTION,0.13343799058084774,"Clipping Bias and Data Heterogeneity: In practice, tight sensitivity of many data processing
85"
INTRODUCTION,0.13500784929356358,"algorithms is intractable and thus a very popular but artiﬁcial control is clipping. However, clipping
86"
INTRODUCTION,0.13657770800627944,"could also bring non-negligible bias. In general, there is no convergence guarantee for clipped SGD
87"
INTRODUCTION,0.13814756671899528,"if we only assume the stochastic gradient is of bounded variance [15], though under more restrictive
88"
INTRODUCTION,0.13971742543171115,"assumptions, for example, when the stochastic gradient is in a symmetric [15] or light-tailed [22]
89"
INTRODUCTION,0.141287284144427,"distribution, or provided generalized smoothness [23], some (near) convergence results are known. A
90"
INTRODUCTION,0.14285714285714285,"concise characterization of such clipping bias still largely remains open, especially for deep learning.
91"
INTRODUCTION,0.14442700156985872,"The bias is even more complicated in the more general DP-LSGD. To provide meaningful theory
92"
INTRODUCTION,0.14599686028257458,"to instruct systematic bias reduction, we do not want to assume Lipschitz continuity or bounded
93"
INTRODUCTION,0.14756671899529042,"gradient, which may make the analysis trivial and impractical. Thus, the desired analysis essentially
94"
INTRODUCTION,0.14913657770800628,"captures the scenario given heavy data heterogeneity, and the results should not require a bounded
95"
INTRODUCTION,0.15070643642072212,"difference among the local updates.
96"
INTRODUCTION,0.152276295133438,"In this paper, through tackling the above-mentioned challenges, we aim to provide useful and intuitive
97"
INTRODUCTION,0.15384615384615385,"theory to understand practical performance of LSGD and instruct optimization with DP guarantees.
98"
INTRODUCTION,0.1554160125588697,"In particular, we want to explain how DP-LSGD out-performs regular DP-SGD. We summarize our
99"
INTRODUCTION,0.15698587127158556,"contributions as follows.
100"
INTRODUCTION,0.15855572998430142,"1. With only a mild assumption that the stochastic gradient is of bounded variance, we present
101"
INTRODUCTION,0.16012558869701726,"the convergence analysis on the released-only iterates of LSGD under perturbation for both
102"
INTRODUCTION,0.16169544740973313,"convex and non-convex smooth optimization in Theorem 3.1 and 3.2. In particular, for the
103"
INTRODUCTION,0.16326530612244897,"general convex case, we show more powerful last iterate convergence, which could be of
104"
INTRODUCTION,0.16483516483516483,"independent interest in developing generic last-iterate analysis with unbounded gradients.
105"
INTRODUCTION,0.1664050235478807,"2. We then generalize our results to study the utility of DP-LSGD, where DP-SGD becomes
106"
INTRODUCTION,0.16797488226059654,"a special case. In particular, we use the incremental norm of local update (see Deﬁnition
107"
INTRODUCTION,0.1695447409733124,"4.1) to characterize the clipping bias and show DP-LSGD has a faster convergence rate to a
108"
INTRODUCTION,0.17111459968602827,"small neighborhood of global/local optimum as compared to DP-SGD.
109"
INTRODUCTION,0.1726844583987441,"3. We further show LSGD behaves as an efﬁcient variance reduction of local update, where
110"
INTRODUCTION,0.17425431711145997,"multiple local GDs with a small learning rate cancel out substantial sampling noise, and
111"
INTRODUCTION,0.17582417582417584,"enable more efﬁcient clipping compared to DP-SGD. Thorough experiments show that
112"
INTRODUCTION,0.17739403453689168,"DP-LSGD produces a much sharpened utility-privacy tradeoff in practical deep learning.
113"
RELATED WORKS,0.17896389324960754,"1.1
Related Works
114"
RELATED WORKS,0.18053375196232338,"Convergence Analysis of LSGD: With the increasing scale of both training data and models,
115"
RELATED WORKS,0.18210361067503925,"federated learning has become an important paradigm in modern machine learning, where LSGD and
116"
RELATED WORKS,0.1836734693877551,"its variants form the building block. Though the idea of LSGD can be traced back to earlier works
117"
RELATED WORKS,0.18524332810047095,"[24, 25], the theoretical convergence analysis has only been proved recently. A common strategy to
118"
RELATED WORKS,0.18681318681318682,"show convergence is to consider a virtual average of all the intermediate iterates produced by each
119"
RELATED WORKS,0.18838304552590268,"user, and keep track of the divergence (dissimilarity) between the virtual average and the local iterate.
120"
RELATED WORKS,0.18995290423861852,"In the setup where each user holds i.i.d. data, Stich in [1] studied strongly-convex optimization with
121"
RELATED WORKS,0.19152276295133439,"LSGD and showed a linear speedup in the number of users/nodes. [26] presented non-convex analysis
122"
RELATED WORKS,0.19309262166405022,"under the Lipschitz continuity assumption where the divergence of local update is also bounded.
123"
RELATED WORKS,0.1946624803767661,"For the more general applications with heterogeneous data, [27] studied the convex case with local
124"
RELATED WORKS,0.19623233908948196,"GD (without sampling on either users or users’ local data) but still under Lipschitz continuity. [2]
125"
RELATED WORKS,0.1978021978021978,"presented more generic and tighter analysis for LSGD without assumptions on bounded gradient for
126"
RELATED WORKS,0.19937205651491366,"both strongly and general convex optimization. Further generalization of LSGD to the decentralized
127"
RELATED WORKS,0.20094191522762953,"setup under arbitrary network topology was considered in [19, 28, 29]. However, many existing
128"
RELATED WORKS,0.20251177394034536,"works [2, 19, 28] only showed the convergence rate relying on all the intermediate averages. To our
129"
RELATED WORKS,0.20408163265306123,"knowledge, the ﬁrst generic analysis for synchronized-only iterates was shown in [30]. [30] proposed
130"
RELATED WORKS,0.20565149136577707,"Scaffold, a generalized LSGD with careful correction on the client-drift caused by data heterogeneity.
131"
RELATED WORKS,0.20722135007849293,"Compared to existing works, in this paper, we prove more powerful last-iterate analysis for general
132"
RELATED WORKS,0.2087912087912088,"convex optimization with clipping and perturbation for privacy. It is also worth mentioning that with
133"
RELATED WORKS,0.21036106750392464,"a different motivation, there is another line of works also studying noisy LSGD to capture the effect
134"
RELATED WORKS,0.2119309262166405,"of compressed local updates to further save the communication cost. But, in most existing related
135"
RELATED WORKS,0.21350078492935637,"works [17, 31], the compression error is assumed to be independent with zero-mean. As we need to
136"
RELATED WORKS,0.2150706436420722,"study DP-LSGD with clipped local update, which introduces bias in the local update generation, in
137"
RELATED WORKS,0.21664050235478807,"this paper we present more involved analysis to handle such adaptive and biased perturbation.
138"
RELATED WORKS,0.21821036106750394,"Convergence Analysis of DP-SGD and DP-LSGD: Asymptotically, under Lipschitz continuity, DP-
139"
RELATED WORKS,0.21978021978021978,"SGD is known to produce a tight utility-privacy tradeoff [32, 33], where no bias is produced given a
140"
RELATED WORKS,0.22135007849293564,"clipping threshold larger than the Lipschitz constant. However, without Lipschitz continuity, practical
141"
RELATED WORKS,0.22291993720565148,"understanding of DP-SGD remains limited. On one hand, negative examples are shown in [15, 16]
142"
RELATED WORKS,0.22448979591836735,"where clipped-SGD in general will not converge, and in practice clipped-SGD does produce bias
143"
RELATED WORKS,0.2260596546310832,"and has a lower convergence rate, especially in deep learning applications compared to regular SGD
144"
RELATED WORKS,0.22762951334379905,"[16]. On the other hand, under more restrictive assumptions on the stochastic gradient distribution,
145"
RELATED WORKS,0.22919937205651492,"clipped-SGD can be shown to (nearly) converge [15, 22, 23]. A generic characterization on the
146"
RELATED WORKS,0.23076923076923078,"clipping bias still largely remains open. As a consequence, there is little known meaningful theory to
147"
RELATED WORKS,0.23233908948194662,"systematically instruct optimization algorithms with DP guarantees, and most existing private deep
148"
RELATED WORKS,0.23390894819466249,"learning works are empirical, which aim to search for the optimal model and hyperparameters for
149"
RELATED WORKS,0.23547880690737832,"objective training data [34, 35, 36]. As for DP-LSGD, to our knowledge the only known theoretical
150"
RELATED WORKS,0.2370486656200942,"result that captures the clipping bias is [16]. However, [16] still assumes globally bounded gradient
151"
RELATED WORKS,0.23861852433281006,"compared to bounded second moment as assumed in our results, and its main motivation is to study
152"
RELATED WORKS,0.2401883830455259,"the clipping effect in client-level DP. In this paper, we show more intuitive and generic analysis of
153"
RELATED WORKS,0.24175824175824176,"DP-LSGD for both convex and non-convex optimization, and our motivations are also very different.
154"
RELATED WORKS,0.24332810047095763,"We set out to provide usable quantiﬁcation on the utility loss due to clipping and we argue to apply
155"
RELATED WORKS,0.24489795918367346,"DP-LSGD both in the centralized and distributed setup, since DP-LSGD can signiﬁcantly reduce the
156"
RELATED WORKS,0.24646781789638933,"clipping bias with a faster convergence rate.
157"
PRELIMINARIES,0.24803767660910517,"2
Preliminaries
158"
PRELIMINARIES,0.24960753532182103,"We focus on the classic Empirical Risk Minimization (ERM) problem. Given a dataset D =
159"
PRELIMINARIES,0.25117739403453687,"{(xi, yi), i = 1, 2, · · · , n}, the loss function is deﬁned as F(w) =
1
n · Pn i=1 f """
PRELIMINARIES,0.25274725274725274,"w, xi, yi #"
PRELIMINARIES,0.2543171114599686,"=
1
n ·
160
Pn"
PRELIMINARIES,0.25588697017268447,"i=1 fi(w). We will consider the cases where the loss function fi(w) : W ! R+ is convex or
161"
PRELIMINARIES,0.25745682888540034,"non-convex. w⇤= arg minw F(w) represents the global optimum. Some formal deﬁnitions about
162"
PRELIMINARIES,0.25902668759811615,"the properties of the objective loss function are deﬁned as follows.
163"
PRELIMINARIES,0.260596546310832,"Deﬁnition 2.1 (Smoothness). A function f is β-smooth on W if the gradient rf(w) is β-Lipschitz
164"
PRELIMINARIES,0.2621664050235479,"such that for all w, w0 2 W, krf(w) −rf(w0)k βkw0 −wk.
165"
PRELIMINARIES,0.26373626373626374,"Deﬁnition 2.2 (Convexity and Strong Convexity). A function f(w) is λ-convex on W if for all
166"
PRELIMINARIES,0.2653061224489796,"w, w0 2 W, λ"
PRELIMINARIES,0.2668759811616955,"2 kw −w0k2 f(w) −f(w0) −hrf(w0), w −w0i. We call f(w) general convex if
167"
PRELIMINARIES,0.2684458398744113,"λ = 0, and f(w) is strongly convex if λ > 0.
168"
PRELIMINARIES,0.27001569858712715,"Assumption 2.1 (Bounded Variance of Stochastic Gradient). For any w 2 W and an index i that is
169"
PRELIMINARIES,0.271585557299843,"randomly selected from {1, 2, · · · , n}, there exists ⌧> 0 such that E[krF(w) −rfi(w)k2] ⌧.
170"
PRELIMINARIES,0.2731554160125589,"Assumption 2.1 is the only additional assumption we need for the analysis of non-private LSGD
171"
PRELIMINARIES,0.27472527472527475,"without clipping. We formally present the non-private LSGD algorithm in Algorithm 1 which uses
172"
PRELIMINARIES,0.27629513343799056,"non-clipped local update (3). The whole process is formed of T phases. In each phase, by q-Poisson
173"
PRELIMINARIES,0.2778649921507064,"sampling, in expectation (nq) many users will be selected to perform K local gradient descents
174"
PRELIMINARIES,0.2794348508634223,"on their local data before broadcasting the local update. To match the DP-LSGD where the local
175"
PRELIMINARIES,0.28100470957613816,"function fi(w) held by each user may only be determined by a single datapoint, we do not consider
176"
PRELIMINARIES,0.282574568288854,"an additional stochastic gradient oracle on the local function in Algorithm 1, but only assume random
177"
PRELIMINARIES,0.28414442700156983,"sampling on the user level at each phase. However, our results can be easily generalized to the
178"
PRELIMINARIES,0.2857142857142857,"scenario with stochastic local gradient. Moreover, we assume Poisson sampling in Algorithm 1 so as
179"
PRELIMINARIES,0.28728414442700156,"to match the setup of DP-LSGD, since given current studies on privacy ampliﬁcation by sampling,
180"
PRELIMINARIES,0.28885400313971743,"Poisson sampling can produce the tightest results [37] (and has become the most popular option in
181"
PRELIMINARIES,0.2904238618524333,"practice [36, 38]). In the following, we introduce the deﬁnition of DP.
182"
PRELIMINARIES,0.29199372056514916,"Deﬁnition 2.3 (Differential Privacy [38]). Given a universe X ⇤, we say that two datasets X, X0 ✓X ⇤
183"
PRELIMINARIES,0.29356357927786497,"are adjacent, denoted as X ⇠X0, if X = X0 [ x or X0 = X [ x for some additional datapoint
184"
PRELIMINARIES,0.29513343799058084,"x 2 X. A randomized algorithm M is said to be (✏, δ)-differentially-private (DP) if for any pair of
185"
PRELIMINARIES,0.2967032967032967,"adjacent datasets X, X0 and any event set O in the output domain of M, it holds that
186"
PRELIMINARIES,0.29827315541601257,P(M(X) 2 O) e✏· P(M(X0) 2 O) + δ.
PRELIMINARIES,0.29984301412872844,"In Deﬁnition 2.3, we apply the unbounded DP deﬁnition as adopted in most existing DP-SGD works
187"
PRELIMINARIES,0.30141287284144425,"[16, 35, 38], where the two adjacent datasets are deﬁned to differ in one datapoint. One may also
188"
PRELIMINARIES,0.3029827315541601,"apply the bounded DP deﬁnition [8] by deﬁning the adjacent datasets as arbitrarily replacing a
189"
PRELIMINARIES,0.304552590266876,"datapoint. However, as a stronger deﬁnition, bounded DP will also face a larger sensitivity bound.
190"
PRELIMINARIES,0.30612244897959184,"We can now formally describe DP-LSGD and DP-SGD. In (2) of Algorithm 1, a clipping operation
191"
PRELIMINARIES,0.3076923076923077,"on a vector v with threshold c is deﬁned as CP(v, c) = v · min{1, c/kvk}, which ensures a bounded
192"
PRELIMINARIES,0.3092621664050236,"sensitivity up to c. Using the clipped local update (2), by selecting Q(t) to be proper DP noise,
193"
PRELIMINARIES,0.3108320251177394,"Algorithm 1 captures DP-SGD when K = 1 and DP-LSGD for general K ≥1. DP-LSGD (SGD) is
194"
PRELIMINARIES,0.31240188383045525,"essentially an LSGD (SGD) with clipped local update (per-sample gradient) and additional DP noise.
195"
PRELIMINARIES,0.3139717425431711,"Running for T iterations with a total privacy budget (✏, δ), one may select Q(t) ⇠N(0, σ2 · Id)
196"
PRELIMINARIES,0.315541601255887,where σ = ˜O(qc p
PRELIMINARIES,0.31711145996860285,"T log(1/δ)/✏) by the composition bound [38]. The privacy analysis and the noise
197"
PRELIMINARIES,0.31868131868131866,"bound are identical for both DP-LSGD and DP-SGD given the same clipping threshold c.
198"
PRELIMINARIES,0.3202511773940345,Algorithm 1 (Differentially Private) Local SGD with Noisy (Clipped) Periodic Averaging
PRELIMINARIES,0.3218210361067504,"1: Input: A system of n workers where each holds a local loss function F(w) = fi(w), sampling"
PRELIMINARIES,0.32339089481946626,"rate q, update step size ⌘, local update length K and global synchronization number T, clipping
threshold c, and initialization ¯w(0) with synchronization noise Q(1:T ).
2: for t = 1, 2, · · · , T do
3:
Implement i.i.d. sampling to select an index batch S(t) = %"
PRELIMINARIES,0.3249607535321821,"[1], · · · , [Bt] "
PRELIMINARIES,0.32653061224489793,"from {1, 2, · · · , n}
of size Bt.
4:
for i = 1, 2, · · · , Bt in parallel do"
PRELIMINARIES,0.3281004709576138,"5:
w(t,0)"
PRELIMINARIES,0.32967032967032966,"[i]
= ¯w(t−1).
6:
for k = 1, 2, · · · , K do
7:"
PRELIMINARIES,0.33124018838304553,"w(t,k)"
PRELIMINARIES,0.3328100470957614,"[i]
= w(t,k−1)"
PRELIMINARIES,0.33437990580847726,"[i]
−⌘rf[i](w(t,k−1)"
PRELIMINARIES,0.3359497645211931,"[i]
).
(1)"
PRELIMINARIES,0.33751962323390894,"8:
end for
9:
Clip the local update as ∆w(t)"
PRELIMINARIES,0.3390894819466248,"[i] = CP(w(t,K)"
PRELIMINARIES,0.34065934065934067,"[i]
−¯w(t−1), c)
10:
end for
11:
if to ensure Differential Privacy with clipping then
12:"
PRELIMINARIES,0.34222919937205654,¯w(t) = ¯w(t−1) + 1
PRELIMINARIES,0.34379905808477235,"nq · ( Bt
X i=1 ∆w(t)"
PRELIMINARIES,0.3453689167974882,"[i] ) + Q(t)
(2)"
PRELIMINARIES,0.3469387755102041,"13:
else
14:"
PRELIMINARIES,0.34850863422291994,¯w(t) = 1
PRELIMINARIES,0.3500784929356358,"nq · ( Bt
X i=1"
PRELIMINARIES,0.3516483516483517,"w(t,K)"
PRELIMINARIES,0.3532182103610675,"[i]
) + Q(t).
(3)"
PRELIMINARIES,0.35478806907378335,"15:
end if
16: end for
17: Output: ¯w(t) for t = 1, 2, · · · , T."
PRELIMINARIES,0.3563579277864992,"We want to stress again that our motivation to study DP-LSGD is not because we only focus on the
199"
PRELIMINARIES,0.3579277864992151,"federated setup, but to provide a uniﬁed analysis of the clipping bias and argue for using DP-LSGD
200"
PRELIMINARIES,0.35949764521193095,"even in the centralized setup. Our results are straightforwardly applicable to distributed learning with
201"
PRELIMINARIES,0.36106750392464676,"local DP [4] or client-level DP [5], where the only difference is that we may add a larger noise Q(t)
202"
PRELIMINARIES,0.3626373626373626,"determined by the number of local datapoints or the users involved, respectively, for these stronger
203"
PRELIMINARIES,0.3642072213500785,"DP deﬁnitions. As for the possible communication restriction where we need to add discrete noise of
204"
PRELIMINARIES,0.36577708006279436,"ﬁnite precision, one may replace the Gaussian noise by the Binomial mechanism [39].
205"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.3673469387755102,"3
Convergence of Synchronized-Only Iterate in Noisy Non-Clipped LSGD
206"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.36891679748822603,"In this section, we will study the convergence analysis of LSGD in Algorithm 1 using the non-clipped
207"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.3704866562009419,"local update (3) for both convex and non-convex optimization.
208"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.37205651491365777,"Theorem 3.1 (Last-iterate Convergence of Noisy LSGD in General Convex Optimization). For an
209"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.37362637362637363,"objective function F(w) =
1
n · Pn"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.3751962323390895,"i=1 fi(w) where fi(w) is convex and β-smooth with variance-
210"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.37676609105180536,"bounded gradient (Assumption 2.1), when ⌘< min{
β
p"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.3783359497645212,"24K , 1"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.37990580847723704,"β ,
1
2β+3Kβ/(nq)}, log(TK) ≥2, and
211"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.3814756671899529,"Q(t) is an independent noise such that E[Q(t)] = 0 and E[kQ(t)k2] ¯Q, for some parameter ¯Q for
212"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.38304552590266877,"t = 1, 2, · · · , T, Algorithm 1 with (3) ensures
213"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.38461538461538464,E[F( ¯w(T ))] (k ¯w(0) −w⇤k2
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.38618524332810045,"⌘(TK + 1)
+ log(TK + 1) """
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.3877551020408163,6⌘⌧/(nq) + 8K2β⌧⌘2 + ¯Q/⌘ #
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.3893249607535322,"+ 5⌘β2(log(TK) + 1) """
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.39089481946624804,"k ¯w(0) −w⇤k2 + T """
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.3924646781789639,8β⌘3K3⌧+ 12K3β2⌘4⌧+ 3K2⌘2⌧
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.3940345368916798,"nq
+ ¯Q ##"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.3956043956043956,"= ˜O(k ¯w(0) −w⇤k2 p TK +
⌧
p TKnq + K⌧ T
+ p"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.39717425431711145,"TK ¯Q), if ⌘= O(1/ p TK). 214"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.3987441130298273,"The proof can be found in Appendix A. To prove Theorem 3.1, with a careful analysis on k ¯w(t)−w⇤k2,
215"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4003139717425432,"we develop a new last-iterate analysis framework, different from existing works [40, 41, 42] which
216"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.40188383045525905,"must count on the assumption of bounded gradient. In Theorem 3.1, we need to assume the noise
217"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.40345368916797486,"Q to be independent and of zero-mean. Because we do not assume Lipschitz continuity of F(w),
218"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4050235478806907,"we cannot provide a meaningful upper bound of the deviation between F(w) and F(w + Q) for
219"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4065934065934066,"arbitrary w and Q in general. However, provided the Lipschitz assumption, Theorem 3.1 can be
220"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.40816326530612246,"easily generalized to handle biased perturbation. In Section 4, with an additional assumption on the
221"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4097331240188383,"similarity of the local functions (Assumption 4.2), we will show how to handle the clipping bias as a
222"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.41130298273155413,"special biased noise. When there is no noise ¯Q = 0, provided that K = O(T 1/3/(nq)2/3), we show
223"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.41287284144427,"LSGD achieves ˜O( k ¯
w(0)−w⇤k2+⌧/(nq)2/3 p"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.41444270015698587,"T K
) last-iterate convergence in general-convex optimization.
224"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.41601255886970173,"We now study the non-convex scenario.
225"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4175824175824176,"Theorem 3.2 (Synchronized-only Iterate Convergence of Noisy LSGD in Non-convex Optimization).
226"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.41915227629513346,For an arbitrary objective function F(w) = 1
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4207221350078493,n · Pn
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.42229199372056514,"i=1 fi(w), where fi(w) is β-smooth and satisﬁes
227"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.423861852433281,"Assumption 2.1, and for arbitrary perturbation (not necessarily independent or of zero mean) where
228"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.42543171114599687,"E[kQ(t)k2] ¯Q, when ⌘< min{
β
p"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.42700156985871274,"24K ,
1
4βK }, Algorithm 1 with (3) ensures that
229 E[ PT"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.42857142857142855,t=1 krF( ¯w(t−1))k2
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4301412872841444,"T
] 4F( ¯w(0))"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4317111459968603,"TK⌘
+ 16⌘2⌧β2K2"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.43328100470957615,"nq
+ 4(1 + β⌘) PT"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.434850863422292,t=1 E[kQ(t)
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4364207221350079,"i k2]
⌘2KT"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4379905808477237,"= O(
⌧1/3"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.43956043956043955,T 2/3(nq)1/3 + T 2/3⌧2/3K ¯Q
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4411302982731554,"(nq)2/3
), (4)"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4427001569858713,"when we select ⌘= O(
(nq)1/3"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.44427001569858715,"T 1/3K⌧1/3 ). In particular, when Q(t) is independent and E[Q(t)] = 0, and
⌘= ⇥(1/K), then E[ PT"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.44583987441130296,t=1 krF( ¯w(t−1))k2
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4474097331240188,"T
] O"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4489795918367347,"""F( ¯w(0))"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.45054945054945056,"⌘TK
+ ⌧+ PT"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4521193092621664,t=1 βE[kQ(t)k2] ⌘TK #
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.45368916797488223,= O( 1
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4552590266875981,T + ⌧+ ¯Q). 230
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.45682888540031397,"The proof can be found in Appendix B. In Theorem 3.2, we provide an analysis on the effect of generic
231"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.45839874411302983,"perturbation, which can also be used to capture the clipping bias in DP-LSGD. When there is no
232"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4599686028257457,"perturbation, Theorem 3.2 has two implications. First, we show to ensure min E[krF( ¯w(t))k2] ,
233"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.46153846153846156,we need T = O( p
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4631083202511774,"⌧/(nq)
3/2
), which is tighter than the state-of-the-art results O( ⌧/(nq) 2
+"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.46467817896389324,"p⌧
3/2 ) in
234"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4662480376766091,"[30]. Second, compared to O(1/T 2/3), we also show that LSGD can converge faster in O(1/T)
235"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.46781789638932497,"to a ⌧-neighborhood of a saddle point. This is helpful to understand the practical performance of
236"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.46938775510204084,"DP-LSGD with bias, as discussed in Section 4.2.
237"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.47095761381475665,"As a ﬁnal remark, we want to mention it is possible to improve the convergence rate from O(1/T 2/3)
238"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4725274725274725,"to O(1/T) via careful variance reduction or error feedback mechanism, such as Scaffold [30] or
239"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4740973312401884,"FedLin [43]. However, the proper implementation of those advanced tricks in DP-LSGD with
240"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.47566718995290425,"additional sensitivity control is not clear. As a ﬁrst step to systematically study the generic clipping
241"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.4772370486656201,"bias, in this paper we only focus on the regular LSGD. We will explain and discuss possible
242"
CONVERGENCE OF SYNCHRONIZED-ONLY ITERATE IN NOISY NON-CLIPPED LSGD,0.478806907378336,"generalizations in Section 6.
243"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.4803767660910518,"4
Utility and Clipping Bias of DP-LSGD and DP-SGD
244"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.48194662480376765,"In this section, we move to study DP-LSGD with clipped local update (2) in Algorithm 1. To have
245"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.4835164835164835,"a clear comparison with DP-SGD, we still consider the centralized setup and F(w) = 1/n · fi(w)
246"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.4850863422291994,"where each local function fi(w) is determined by a single sample. To capture the clipping bias, we
247"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.48665620094191525,"need to introduce a new term, termed incremental norm.
248"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.48822605965463106,"Deﬁnition 4.1 (Incremental Norm). Consider applying the private and clipping version of Algorithm 1
249"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.4897959183673469,with (2) on F(w) = Pn
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.4913657770800628,"i=1 fi(w). In the t-th phase, we deﬁne  (t) i
= 1 """
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.49293563579277866,k∆w(t)
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.4945054945054945,i k > c #
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.49607535321821034,·(k∆w(t)
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.4976452119309262,"i k−c)
250"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.49921507064364207,"as the incremental norm of the local update from fi(w) compared to the clipping threshold c, for
251"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5007849293563579,"t = 1, 2, · · · , T.
252"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5023547880690737,"In Deﬁnition 4.1, the incremental norm  (t)"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5039246467817896,"i
simply quantiﬁes the difference between the norm of
253"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5054945054945055,"the local update and its clipped version from fi(w). In the following, we will always assume the DP
254"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5070643642072213,"noise injected E[kQ(t)k2] = σ2d, following the classic privacy analysis of DP-SGD [38].
255"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5086342229199372,"It is not hard to observe that the clipped local update is essentially a scaled version of the original
256"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5102040816326531,"update, and thus virtually one may view DP-LSGD as a generalization of noisy LSGD but each local
257"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5117739403453689,"update applies a different and adaptively-selected learning rate. To show meaningful characterization
258"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5133437990580848,"on the difference among those learning rates, we need the following assumption as a generalization
259"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5149136577708007,"of bounded-variance stochastic gradient.
260"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5164835164835165,"Assumption 4.1 (Incremental norm of Bounded Second Moment). When applying the clipped version
261"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5180533751962323,of Algorithm 1 via (2) on an objective function F(w) = 1
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5196232339089482,"n · fi(w), E ⇥"" Pn"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.521193092621664,i=1( (t) i )2# /n ⇤
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5227629513343799,"is upper
262"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5243328100470958,"bounded by B2, for some global parameter B for t = 1, 2, · · · , T.
263"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5259026687598116,"Assumption 4.1 basically states that in expectation the square of l2-norm of each local update is
264"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5274725274725275,"bounded. Assumption 4.1 also suggests that E ⇥"" Pn"
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5290423861852434,i=1  (t) i # /n ⇤
UTILITY AND CLIPPING BIAS OF DP-LSGD AND DP-SGD,0.5306122448979592,"B.
265"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5321821036106751,"4.1
Utility of DP-LSGD in Convex Optimization
266"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.533751962323391,"Another assumption we need for the anlysis of DP-LSGD on general convex optimization is the
267"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5353218210361067,"similarity among the local functions.
268"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5368916797488226,Assumption 4.2 (γ Similarity). For F(w) = 1/n·Pn
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5384615384615384,"i=1 fi(w), local functions fi are of γ-similarity
269"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5400313971742543,"to F such that for any w 2 W, |fi(w) −F(w)| γ, for some constant γ > 0.
270"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5416012558869702,"The main reason why we need this additional Assumption 4.2 is because we do not assume Lipschitz
271"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.543171114599686,"continuity of F(w). Thus, we alternatively consider to use the similarity among local functions to
272"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5447409733124019,"characterize the deviation of the evaluation of F(·) on biased iterates.
273"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5463108320251178,"Theorem 4.1 (Last-iterate of DP-LSGD in General Convex Optimization). For an arbitrary objective
274"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5478806907378336,function F(w) = 1
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5494505494505495,n · Pn
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5510204081632653,"i=1 fi(w) where fi(w) is convex and β-smooth, and under Assumptions 2.1,
275"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5525902668759811,"4.1 and 4.2, when ⌘= O(1/ p"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.554160125588697,"TK) and Q(t) is independent DP noise such that E[Q(t)] = 0 and
276"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5557299843014128,"E[kQ(t)k2] = σ2d, t = 1, 2, · · · , T, then DP-LSGD with clipping threshold c ensures that
277"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5572998430141287,"c
c + B · E[F( ¯w(T )) −F(w⇤)] = ˜O "" (
1
p TK + K"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5588697017268446,nT )k ¯w(0) −w⇤k2 + ( K
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5604395604395604,"nT +
1
p TK"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5620094191522763,)(1 + K3/2 p T + K
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5635792778649922,nq )⌧+ (K3/2 p Tn
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.565149136577708,+ 1) γB
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5667189952904239,c + B + p TKσ2d # . (5)
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5682888540031397,"When K = O(nq) and K = O(T), and for (✏, δ)-DP, where σ = ˜O( cp"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5698587127158555,T log(1/δ)
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5714285714285714,"n✏
), we have that
278"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5729984301412873,E[F( ¯w(T )) −F(w⇤)] = ˜O
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5745682888540031,""" c + B c
·"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.576138147566719,"""k ¯w(0) −w⇤k2 p TK"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5777080062794349,"+ (
1
p TK + K T )⌧ #"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5792778649921507,"|
{z
}
(A) + γB"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5808477237048666,"c
|{z} (B)"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5824175824175825,+ c + B
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5839874411302983,"c
· T 3/2K1/2 log(1/δ)dc2"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5855572998430141,"n2✏2
|
{z
}
(C) # . 279"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5871271585557299,"The proof can be found in Appendix C. We focus on a practical scenario where B = O(c), i.e., the
280"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5886970172684458,"incremental norm of local updates is in the same order of the clipping threshold c selected, and thus
281"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5902668759811617,"(c + B)/c = O(1). From Theorem 4.1, we show the last-iterate utility of DP-LSGD is captured by
282"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5918367346938775,"three terms: (A) a similar convergence rate as regular LSGD, (B) a clipping bias, and (C) the DP noise
283"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5934065934065934,"variance. First, ignoring the bias and noise, DP-LSGD still enjoys a convergence rate ˜O( k ¯
w(0)−w⇤k2 p"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5949764521193093,"T K
+
284 (
1
p"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5965463108320251,T K + K
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.598116169544741,"T )⌧), which is slightly worse compared to Theorem 3.2 with ˜O( k ¯
w(0)−w⇤k2 p"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.5996860282574569,"T K
+ (
1
p"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6012558869701727,"T Knq +
285 K"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6028257456828885,"T )⌧) as a consequence of clipping which essentially applies different learning rates in each local
286"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6043956043956044,"update. Second, the clipping bias is captured by (γB)/c. This matches our intuition that a larger
287"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6059654631083202,"incremental norm B combined with a smaller clipping threshold c will imply a more signiﬁcant change
288"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6075353218210361,"on the local update and thus a larger bias. The last accumulated perturbation term is determined by
289"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.609105180533752,the noise injected across each phase with an effect of ˜O( T 3/2K1/2 log(1/δ)dc2
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6106750392464678,"n2✏2
) for (✏, δ)-DP under
290"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6122448979591837,"T-fold composition.
291"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6138147566718996,"As we consider the very generic setup with non-trival clipping, Theorem 3.2 cannot be directly com-
292"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6153846153846154,"pared to the classic DP-utility tradeoff [32] given Lipschitz continuity, where a utility loss ˜⇥( p"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6169544740973313,"d/n✏)
293"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6185243328100472,"is tight for convex optimization under (✏, δ)-DP. However, we have the following interesting observa-
294"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6200941915227629,"tions. First, when we take the clipping threshold c = O(⌘) = O(1/ p"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6216640502354788,"TK) and K = O(T ·d/(n2✏2)),
295"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6232339089481946,DP-LSGD achieves the same optimal rate ˜O( p
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6248037676609105,"d/n✏) [33] ignoring the clipping bias. Second and
296"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6263736263736264,"more important, when the stochastic gradient variance ⌧is in the same order of the clipping bias
297"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6279434850863422,"O(γB/c), then by selecting c = ⇥(⌘) and K = ⇥(T), Theorem 4.1 suggests that DP-LSGD will
298"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6295133437990581,"converge in O(1/T) to an O(γB/c +
d
n2✏2 ) neighborhood of the global optimum. As a comparison,
299"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.631083202511774,"when we select K = 1 in Theorem 4.1, it becomes the analysis of DP-SGD but the convergence
300"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6326530612244898,"rate to the neighborhood of global optimum in the same scale O(γB/c +
d
n2✏2 ) is only O(1/ p"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6342229199372057,"T).
301"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6357927786499215,"Moreover, as we will show in the next section, the local update bound B in DP-SGD with K = 1
302"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6373626373626373,"in practice would be much larger than that of DP-LSGD with a relatively larger K. As a simple
303"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.6389324960753532,"generalization, we also include an analysis of DP-LSGD on strongly-convex functions in Appendix
304"
UTILITY OF DP-LSGD IN CONVEX OPTIMIZATION,0.640502354788069,"D, and we move our focus to the non-convex optimization in the following.
305"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6420722135007849,"4.2
Utility of DP-LSGD in Non-convex Optimization
306"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6436420722135008,Theorem 4.2 (DP-LSGD in Non-convex Optimization). For F(w) = 1
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6452119309262166,n · Pn
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6467817896389325,"i=1 fi(w) where fi(w)
307"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6483516483516484,"is β-smooth and satisﬁes Assumptions 2.1 and 4.1, when ⌘= O(1/K), DP-LSGD ensures that
308 E[ PT"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6499215070643642,t=1 krF( ¯w(t−1))k2
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6514913657770801,"T
] 4F( ¯w(0))"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6530612244897959,"TK⌘
+ 16⌘2⌧β2K2"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6546310832025117,"nq
+ 4(1 + β⌘) """
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6562009419152276,B2/q + σ2d #
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6577708006279435,"⌘2K
.
(6)"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6593406593406593,"When we select ⌘= O(
1
p"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6609105180533752,"T K ) and K = ⇥(T), for (✏, δ)-DP we have that
309 E[ PT"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6624803767660911,t=1 krF( ¯w(t−1))k2
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6640502354788069,"T
] = ˜O(F( ¯w(0)) T
+ ⌧"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6656200941915228,nq + B2T
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6671899529042387,"q
+
d
n2✏2 ).
(7) 310"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6687598116169545,"The proof can be found in Appendix E. For the analysis of DP-LSGD in non-convex optimization,
we do not need Assumption 4.2 on the similarity among local functions and Theorem 4.2 is simply
obtained by substituting the clipping error from each phase into Theorem 3.2. To have a more clear
picture, we still consider a practical scenario when B = B0 · ⌘for some constant B0 and the variance
⌧is also some constant. Then, from (7) we have that E[ PT"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6703296703296703,t=1 krF( ¯w(t−1))k2
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6718995290423861,"T
] = O"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.673469387755102,"""F( ¯w(0)) T
+ 1"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6750392464678179,nq + B2
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6766091051805337,"0
q +
d
n2✏2 # = ˜O "" 1 T + 1"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6781789638932496,"q +
d
n2✏2 # ."
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6797488226059655,"In other words, similar to the convex case, DP-LSGD will converge at a rate of O(1/T) to an
311"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6813186813186813,"˜O(1 + d/(n2✏2)) neighborhood of a saddle point given some constant sampling rate q. As a
312"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6828885400313972,"comparison, for DP-SGD when K = 1, from Theorem 3.2 we can only ensure an O(1/ p"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6844583987441131,"T)
313"
UTILITY OF DP-LSGD IN NON-CONVEX OPTIMIZATION,0.6860282574568289,"convergence rate to a same ˜O(1 + d/(n2✏2)) neighborhood.
314"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.6875981161695447,"5
Why DP-LSGD Produces Less Bias and Better SNR
315"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.6891679748822606,"Throughout the previous section, we showed that asymptotically DP-LSGD enjoys a faster conver-
316"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.6907378335949764,"gence rate to a neighborhood of (global/local) optimum compared to DP-SGD. We characterized
317"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.6923076923076923,"the clipping bias mainly based on the second moment upper bound B2 of the incremental norm
318 (t)"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.6938775510204082,"i
of local updates. In this section, we proceed to empirically study the  (t)"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.695447409733124,"i , and the tradeoff
319"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.6970172684458399,"between clipping bias and DP (Gaussian) noise in practical deep learning tasks. We will explain why
320"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.6985871271585558,"DP-LSGD could produce smaller bias and enable more efﬁcient clipping compared to DP-SGD.
321"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7001569858712716,"To produce good utility-privacy tradeoff, a proper selection of the clipping threshold c is important.
322"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7017268445839875,"Many existing works are devoted to optimizing the selection of c by either grid searching [35] or
323"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7032967032967034,"adaptive ﬁne-tuning [44]. A smaller c requires less DP noise. But, as a tradeoff shown in Theorem
324"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7048665620094191,"4.1 and 4.2, a smaller c and a consequently a larger B will also lead to a heavier clipping bias. Thus,
325"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.706436420722135,"from the perspective of signal-to-noise ratio (SNR), an ideal scenario is that the l2-norm of each
326"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7080062794348508,"local update is concentrated such that we can maximize the efﬁciency of the clipping power c with
327"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7095761381475667,"a small clipping effect for most local updates. Interpreted via our developed theory of clipping
328"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7111459968602826,"bias, it is expected that given the clipping threshold c, the incremental norm  (t)"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7127158555729984,"i
would be small,
329"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7142857142857143,"captured by B in (5) and (7). In Fig. 1 (a,b), we plot various statistics of the incremental norm  (t) i
330"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7158555729984302,"0
500
1000
1500
0 5 10 15 20 25"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.717425431711146,"0
500
1000
1500
0 10 20 30 40 50 60 70 80"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7189952904238619,"0
500
1000
1500
35 40 45 50 55 60 65 70"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7205651491365777,"Figure 1: Training ResNet 20 on CIFAR10 with DP-LSGD (K = 10, ⌘= 0.025, c = 1) and
DP-SGD (K = 1, ⌘= 1, c = 1) under (✏= 2, δ = 10−5)-DP, with expected batch size 1000."
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7221350078492935,"for DP-LSGD and DP-SGD, respectively, on training CIFAR10 [45]. By our analysis, DP-LSGD
331"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7237048665620094,"usually should apply a smaller learning rate ⌘. To have a fair comparison, we consider the normalized
332"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7252747252747253,incremental norm  (t)
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7268445839874411,"i /⌘. Given the same clipping threshold, comparing Fig. 1 (a) and (b), the mean
333"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.728414442700157,"of normalized incremental norm, captured by B/⌘in our theorems, of DP-LSGD is only around 32%
334"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7299843014128728,"compared to that of DP-SGD. The corresponding standard deviation is around only 40% compared to
335"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7315541601255887,"that of DP-SGD. One may also compare the 25% and 75% quantiles, which suggest that more local
336"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7331240188383046,"updates bear less clipping inﬂuence in DP-LSGD and thus enjoying a higher clipping efﬁciency. We
337"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7346938775510204,"also report the comparison when training ResNet20 [46] on SVHN [47] in Fig. 2 in Appendix F with
338"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7362637362637363,"similar observations. Details of experiment setups and the anonymous GitHub code link can be found
339"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7378335949764521,"in Appendix F.
340"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7394034536891679,"Dataset and Method \ ✏
1.5
2.0
2.5
3.0
3.5
4.0"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7409733124018838,"CIFAR10, DP-LSGD (K = 10)
59.4(±0.5)
64.0(±0.3)
66.2(±0.4)
67.7(±0.3)
68.7(±0.2)
69.9(±0.3)
CIFAR10, DP-SGD (K = 1)
49.8(±1.2)
58.7(±1.0)
59.9(±1.2)
60.6(±0.8)
62.1(±0.6)
62.8(±0.6)
SVHN, DP-LSGD (K = 10)
83.2(±0.4)
84.4(±0.5)
85.7(±0.5)
85.4(±0.4)
86.1(±0.4)
86.5(±0.3)
SVHN, DP-SGD (K = 1)
74.5(±0.8)
78.2(±0.6)
79.8(±0.6)
80.3(±1.0)
81.7(±0.4)
82.2(±0.5)"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7425431711145997,"Table 1: Test Accuracy of ResNet20 on CIFAR10 and SVHN via DP-LSGD and DP-SGD under
various ✏and ﬁxed δ = 10−5, with expected batch size 1000."
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7441130298273155,"In Fig.1 (c), we record the performance of DP-LSGD and DP-SGD, which coincides with our theory
341"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7456828885400314,"that DP-LSGD has a smaller clipping bias and a faster convergence rate. The smaller incremental
342"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7472527472527473,"norm in DP-LSGD is not surprising. With relatively larger K, for each individual function fi(w),
343"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7488226059654631,"though the K local gradients are correlated and essentially determined by a single sample, the
344"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.750392464678179,"aggregation of them still averages out substantial sampling noise and makes the l2-norm of local
345"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7519623233908949,"updates more concentrated. In Table 1, we include additional comparison between their performance
346"
WHY DP-LSGD PRODUCES LESS BIAS AND BETTER SNR,0.7535321821036107,"on CIFAR10 [45] and SVHN [47]; DP-LSGD produces signiﬁcant improvements.
347"
CONCLUSION AND PROSPECTS,0.7551020408163265,"6
Conclusion and Prospects
348"
CONCLUSION AND PROSPECTS,0.7566718995290423,"In this paper, via LSGD, we provide a uniﬁed analysis of the clipping bias and the utility loss in
349"
CONCLUSION AND PROSPECTS,0.7582417582417582,"privacy-preserving gradient methods for both centralized and distributed setups. Provided the generic
350"
CONCLUSION AND PROSPECTS,0.7598116169544741,"analysis, we develop the connections between the bias and the second moment of local updates.
351"
CONCLUSION AND PROSPECTS,0.7613814756671899,"This initializes a new direction to systematically instruct private learning by connecting the research
352"
CONCLUSION AND PROSPECTS,0.7629513343799058,"of variance reduction in distributed optimization. In this paper we only focus on regular LSGD
353"
CONCLUSION AND PROSPECTS,0.7645211930926217,"to show its advantage over DP-SGD, but advanced acceleration methods [30, 31, 43] are known
354"
CONCLUSION AND PROSPECTS,0.7660910518053375,"in non-private federated learning to further reduce the “local-update drift” caused by (per-sample)
355"
CONCLUSION AND PROSPECTS,0.7676609105180534,"data heterogeneity. This could then further reduce the clipping bias given local updates of smaller
356"
CONCLUSION AND PROSPECTS,0.7692307692307693,"variance. Thus, a promising future direction is to understand and incorporate those techniques
357"
CONCLUSION AND PROSPECTS,0.7708006279434851,"within the sensitivity control framework. Another important issue we have not fully explored is the
358"
CONCLUSION AND PROSPECTS,0.7723704866562009,"software implementation of DP-LSGD in the centralized case. For DP-SGD, many PyTorch libraries
359"
CONCLUSION AND PROSPECTS,0.7739403453689168,"with fast per-sample gradient computation in low memory overhead have been developed, such as
360"
CONCLUSION AND PROSPECTS,0.7755102040816326,"Opacus [48]. However, in all above-presented experiments, we simulate DP-LSGD in a distributed
361"
CONCLUSION AND PROSPECTS,0.7770800627943485,"environment and compute each local update in parallel at a cost of large memory. Given limited
362"
CONCLUSION AND PROSPECTS,0.7786499215070644,"hardware resources, this restricts the application of larger batchsize (tens of thousands) and deploying
363"
CONCLUSION AND PROSPECTS,0.7802197802197802,"deeper neural networks, which are known to produce much better utility-privacy tradeoffs [36, 49].
364"
CONCLUSION AND PROSPECTS,0.7817896389324961,"We leave empirical efﬁciency improvement to future work.
365"
REFERENCES,0.783359497645212,"References
366"
REFERENCES,0.7849293563579278,"[1] Sebastian Urban Stich. Local sgd converges fast and communicates little. In ICLR 2019-
367"
REFERENCES,0.7864992150706437,"International Conference on Learning Representations, number CONF, 2019.
368"
REFERENCES,0.7880690737833596,"[2] Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter theory for local sgd on
369"
REFERENCES,0.7896389324960753,"identical and heterogeneous data. In International Conference on Artiﬁcial Intelligence and
370"
REFERENCES,0.7912087912087912,"Statistics, pages 4519–4529. PMLR, 2020.
371"
REFERENCES,0.792778649921507,"[3] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to
372"
REFERENCES,0.7943485086342229,"sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography
373"
REFERENCES,0.7959183673469388,"Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265–284.
374"
REFERENCES,0.7974882260596546,"Springer, 2006.
375"
REFERENCES,0.7990580847723705,"[4] Graham Cormode, Somesh Jha, Tejas Kulkarni, Ninghui Li, Divesh Srivastava, and Tianhao
376"
REFERENCES,0.8006279434850864,"Wang. Privacy at scale: Local differential privacy in practice. In Proceedings of the 2018
377"
REFERENCES,0.8021978021978022,"International Conference on Management of Data, pages 1655–1658, 2018.
378"
REFERENCES,0.8037676609105181,"[5] Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A
379"
REFERENCES,0.8053375196232339,"client level perspective. arXiv preprint arXiv:1712.07557, 2017.
380"
REFERENCES,0.8069073783359497,"[6] Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh,
381"
REFERENCES,0.8084772370486656,"and Dave Bacon. Federated learning: Strategies for improving communication efﬁciency. arXiv
382"
REFERENCES,0.8100470957613815,"preprint arXiv:1610.05492, 2016.
383"
REFERENCES,0.8116169544740973,"[7] Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
384"
REFERENCES,0.8131868131868132,"use local sgd. In International Conference on Learning Representations, 2020.
385"
REFERENCES,0.814756671899529,"[8] Cynthia Dwork. Differential privacy. In Automata, Languages and Programming: 33rd
386"
REFERENCES,0.8163265306122449,"International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II 33,
387"
REFERENCES,0.8178963893249608,"pages 1–12. Springer, 2006.
388"
REFERENCES,0.8194662480376766,"[9] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. Journal of the Royal
389"
REFERENCES,0.8210361067503925,"Statistical Society Series B: Statistical Methodology, 84(1):3–37, 2022.
390"
REFERENCES,0.8226059654631083,"[10] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Founda-
391"
REFERENCES,0.8241758241758241,"tions and Trends® in Theoretical Computer Science, 9(3–4):211–407, 2014.
392"
REFERENCES,0.82574568288854,"[11] Xiaokui Xiao and Yufei Tao. Output perturbation with query relaxation. Proceedings of the
393"
REFERENCES,0.8273155416012559,"VLDB Endowment, 1(1):857–869, 2008.
394"
REFERENCES,0.8288854003139717,"[12] Cynthia Dwork, Guy N Rothblum, and Salil Vadhan. Boosting and differential privacy. In 2010
395"
REFERENCES,0.8304552590266876,"IEEE 51st Annual Symposium on Foundations of Computer Science, pages 51–60. IEEE, 2010.
396"
REFERENCES,0.8320251177394035,"[13] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with
397"
REFERENCES,0.8335949764521193,"differentially private updates. In 2013 IEEE global conference on signal and information
398"
REFERENCES,0.8351648351648352,"processing, pages 245–248. IEEE, 2013.
399"
REFERENCES,0.8367346938775511,"[14] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar,
400"
REFERENCES,0.8383045525902669,"and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
401"
REFERENCES,0.8398744113029827,"conference on computer and communications security, pages 308–318, 2016.
402"
REFERENCES,0.8414442700156985,"[15] Xiangyi Chen, Steven Z Wu, and Mingyi Hong. Understanding gradient clipping in private sgd:
403"
REFERENCES,0.8430141287284144,"A geometric perspective. Advances in Neural Information Processing Systems, 33:13773–13782,
404"
REFERENCES,0.8445839874411303,"2020.
405"
REFERENCES,0.8461538461538461,"[16] Xinwei Zhang, Xiangyi Chen, Mingyi Hong, Zhiwei Steven Wu, and Jinfeng Yi. Under-
406"
REFERENCES,0.847723704866562,"standing clipping for federated learning: Convergence and client-level differential privacy. In
407"
REFERENCES,0.8492935635792779,"International Conference on Machine Learning, ICML 2022, 2022.
408"
REFERENCES,0.8508634222919937,"[17] Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-sgd: Distributed
409"
REFERENCES,0.8524332810047096,"sgd with quantization, sparsiﬁcation and local computations. Advances in Neural Information
410"
REFERENCES,0.8540031397174255,"Processing Systems, 32, 2019.
411"
REFERENCES,0.8555729984301413,"[18] Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efﬁcient
412"
REFERENCES,0.8571428571428571,"momentum sgd for distributed non-convex optimization. In International Conference on
413"
REFERENCES,0.858712715855573,"Machine Learning, pages 7184–7193. PMLR, 2019.
414"
REFERENCES,0.8602825745682888,"[19] Jianyu Wang and Gauri Joshi. Cooperative sgd: A uniﬁed framework for the design and analysis
415"
REFERENCES,0.8618524332810047,"of local-update sgd algorithms. The Journal of Machine Learning Research, 22(1):9709–9758,
416"
REFERENCES,0.8634222919937206,"2021.
417"
REFERENCES,0.8649921507064364,"[20] Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in
418"
REFERENCES,0.8665620094191523,"federated learning. arXiv preprint arXiv:1910.14425, 2019.
419"
REFERENCES,0.8681318681318682,"[21] Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan
420"
REFERENCES,0.869701726844584,"Mcmahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd?
In
421"
REFERENCES,0.8712715855572999,"International Conference on Machine Learning, pages 10334–10343. PMLR, 2020.
422"
REFERENCES,0.8728414442700158,"[22] Huang Fang, Xiaoyun Li, Chenglin Fan, and Ping Li. Improved convergence of differential
423"
REFERENCES,0.8744113029827315,"private sgd with gradient clipping. In International Conference on Learning Representations
424"
REFERENCES,0.8759811616954474,"2023.
425"
REFERENCES,0.8775510204081632,"[23] Xiaodong Yang, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Normalized/clipped sgd with per-
426"
REFERENCES,0.8791208791208791,"turbation for differentially private non-convex optimization. arXiv preprint arXiv:2206.13033,
427"
REFERENCES,0.880690737833595,"2022.
428"
REFERENCES,0.8822605965463108,"[24] LO Mangasarian. Parallel gradient distribution in unconstrained optimization. SIAM Journal
429"
REFERENCES,0.8838304552590267,"on Control and Optimization, 33(6):1916–1925, 1995.
430"
REFERENCES,0.8854003139717426,"[25] Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for the structured
431"
REFERENCES,0.8869701726844584,"perceptron. In Human language technologies: The 2010 annual conference of the North
432"
REFERENCES,0.8885400313971743,"American chapter of the association for computational linguistics, pages 456–464, 2010.
433"
REFERENCES,0.8901098901098901,"[26] Fan Zhou and Guojing Cong. On the convergence properties of a k-step averaging stochastic
434"
REFERENCES,0.8916797488226059,"gradient descent algorithm for nonconvex optimization. arXiv preprint arXiv:1708.01012, 2017.
435"
REFERENCES,0.8932496075353218,"[27] Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting
436"
REFERENCES,0.8948194662480377,"He, and Kevin Chan. When edge meets learning: Adaptive control for resource-constrained
437"
REFERENCES,0.8963893249607535,"distributed machine learning. In IEEE INFOCOM 2018-IEEE conference on computer commu-
438"
REFERENCES,0.8979591836734694,"nications, pages 63–71. IEEE, 2018.
439"
REFERENCES,0.8995290423861853,"[28] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A
440"
REFERENCES,0.9010989010989011,"uniﬁed theory of decentralized sgd with changing topology and local updates. In International
441"
REFERENCES,0.902668759811617,"Conference on Machine Learning, pages 5381–5393. PMLR, 2020.
442"
REFERENCES,0.9042386185243328,"[29] Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons. The non-iid data quagmire
443"
REFERENCES,0.9058084772370487,"of decentralized machine learning. In International Conference on Machine Learning, pages
444"
REFERENCES,0.9073783359497645,"4387–4398. PMLR, 2020.
445"
REFERENCES,0.9089481946624803,"[30] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
446"
REFERENCES,0.9105180533751962,"Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
447"
REFERENCES,0.9120879120879121,"International Conference on Machine Learning, pages 5132–5143. PMLR, 2020.
448"
REFERENCES,0.9136577708006279,"[31] Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi.
449"
REFERENCES,0.9152276295133438,"Federated learning with compression: Uniﬁed analysis and sharp guarantees. In International
450"
REFERENCES,0.9167974882260597,"Conference on Artiﬁcial Intelligence and Statistics, pages 2350–2358. PMLR, 2021.
451"
REFERENCES,0.9183673469387755,"[32] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization:
452"
REFERENCES,0.9199372056514914,"Efﬁcient algorithms and tight error bounds. In 2014 IEEE 55th annual symposium on foundations
453"
REFERENCES,0.9215070643642073,"of computer science, pages 464–473. IEEE, 2014.
454"
REFERENCES,0.9230769230769231,"[33] Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic
455"
REFERENCES,0.9246467817896389,"convex optimization with optimal rates. Advances in neural information processing systems, 32,
456"
REFERENCES,0.9262166405023547,"2019.
457"
REFERENCES,0.9277864992150706,"[34] Nicolas Papernot, Abhradeep Thakurta, Shuang Song, Steve Chien, and Úlfar Erlingsson.
458"
REFERENCES,0.9293563579277865,"Tempered sigmoid activations for deep learning with differential privacy. In Proceedings of the
459"
REFERENCES,0.9309262166405023,"AAAI Conference on Artiﬁcial Intelligence, volume 35, pages 9312–9321, 2021.
460"
REFERENCES,0.9324960753532182,"[35] Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much
461"
REFERENCES,0.9340659340659341,"more data). In International Conference on Learning Representations, 2021.
462"
REFERENCES,0.9356357927786499,"[36] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle.
Unlock-
463"
REFERENCES,0.9372056514913658,"ing high-accuracy differentially private image classiﬁcation through scale. arXiv preprint
464"
REFERENCES,0.9387755102040817,"arXiv:2204.13650, 2022.
465"
REFERENCES,0.9403453689167975,"[37] Yuqing Zhu and Yu-Xiang Wang. Poission subsampled rényi differential privacy. In Interna-
466"
REFERENCES,0.9419152276295133,"tional Conference on Machine Learning, pages 7634–7642. PMLR, 2019.
467"
REFERENCES,0.9434850863422292,"[38] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar,
468"
REFERENCES,0.945054945054945,"and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
469"
REFERENCES,0.9466248037676609,"conference on computer and communications security, pages 308–318, 2016.
470"
REFERENCES,0.9481946624803768,"[39] Naman Agarwal, Ananda Theertha Suresh, Felix Xinnan X Yu, Sanjiv Kumar, and Brendan
471"
REFERENCES,0.9497645211930926,"McMahan. cpsgd: Communication-efﬁcient and differentially-private distributed sgd. Advances
472"
REFERENCES,0.9513343799058085,"in Neural Information Processing Systems, 31, 2018.
473"
REFERENCES,0.9529042386185244,"[40] Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent
474"
REFERENCES,0.9544740973312402,"algorithms. In Proceedings of the twenty-ﬁrst international conference on Machine learning,
475"
REFERENCES,0.9560439560439561,"page 116, 2004.
476"
REFERENCES,0.957613814756672,"[41] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization:
477"
REFERENCES,0.9591836734693877,"Convergence results and optimal averaging schemes. In International conference on machine
478"
REFERENCES,0.9607535321821036,"learning, pages 71–79. PMLR, 2013.
479"
REFERENCES,0.9623233908948194,"[42] Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with
480"
REFERENCES,0.9638932496075353,"adaptive stepsizes. In The 22nd international conference on artiﬁcial intelligence and statistics,
481"
REFERENCES,0.9654631083202512,"pages 983–992. PMLR, 2019.
482"
REFERENCES,0.967032967032967,"[43] Aritra Mitra, Rayana Jaafar, George J Pappas, and Hamed Hassani. Linear convergence in
483"
REFERENCES,0.9686028257456829,"federated learning: Tackling client heterogeneity and sparse gradients. Advances in Neural
484"
REFERENCES,0.9701726844583988,"Information Processing Systems, 34:14606–14619, 2021.
485"
REFERENCES,0.9717425431711146,"[44] Galen Andrew, Om Thakkar, Brendan McMahan, and Swaroop Ramaswamy. Differentially
486"
REFERENCES,0.9733124018838305,"private learning with adaptive clipping. Advances in Neural Information Processing Systems,
487"
REFERENCES,0.9748822605965463,"34:17455–17466, 2021.
488"
REFERENCES,0.9764521193092621,"[45] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
489"
REFERENCES,0.978021978021978,"2009.
490"
REFERENCES,0.9795918367346939,"[46] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
491"
REFERENCES,0.9811616954474097,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
492"
REFERENCES,0.9827315541601256,"pages 770–778, 2016.
493"
REFERENCES,0.9843014128728415,"[47] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng.
494"
REFERENCES,0.9858712715855573,"Reading digits in natural images with unsupervised feature learning. 2011.
495"
REFERENCES,0.9874411302982732,"[48] Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad,
496"
REFERENCES,0.989010989010989,"Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, et al. Opacus:
497"
REFERENCES,0.9905808477237049,"User-friendly differential privacy library in pytorch. arXiv preprint arXiv:2109.12298, 2021.
498"
REFERENCES,0.9921507064364207,"[49] Florian A Hölzl, Daniel Rueckert, and Georgios Kaissis. Equivariant differentially private deep
499"
REFERENCES,0.9937205651491365,"learning. arXiv preprint arXiv:2301.13104, 2023.
500"
REFERENCES,0.9952904238618524,"[50] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of
501"
REFERENCES,0.9968602825745683,"stochastic gradient descent. In International conference on machine learning, pages 1225–1234.
502"
REFERENCES,0.9984301412872841,"PMLR, 2016.
503"
